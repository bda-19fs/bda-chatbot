{"id": "657211", "url": "https://de.wikipedia.org/wiki?curid=657211", "title": "SELinux", "text": "SELinux\n\nSELinux (Security-Enhanced Linux; engl. „sicherheitsverbessertes Linux“) ist eine Erweiterung des Linux-Kernels, die den ersten Versuch darstellt, das FLASK-Konzept des US-amerikanischen Geheimdienstes NSA umzusetzen. Es implementiert die Zugriffskontrollen auf Ressourcen im Sinne von Mandatory Access Control.\nSELinux wird maßgeblich von der NSA und von dem Linux-Distributor Red Hat entwickelt. Unternehmen wie Network Associates, Secure Computing Corporation, und Tresys sind bzw. waren ebenfalls an der Arbeit an SELinux beteiligt, besonders Tresys übernimmt vermehrt Aufgaben am Projekt.\n\nSELinux ist Open-Source-Software und setzt sich aus einem Kernel-Patch und aus zahlreichen Erweiterungen für Systemprogramme zusammen. Für das Festlegen der Regeln gibt es eine sogenannte Policy, die momentan von Tresys herausgegeben wird. Die meisten Distributionen bieten spezielle SELinux-Policy-Pakete für ihre Programme an, die die Policy um das jeweilige Programm erweitern.\n\n\"SELinux\" ist seit Linux-2.6.x im Kernel integriert. Die Linux-Distribution \"Fedora\" (ein durch \"Red Hat\" gesponsertes Projekt) war die erste Distribution, die von Haus aus SELinux-Unterstützung mitliefert. \"Fedora Core 3\" und \"Red Hat Enterprise Linux 4\" wurden als erste Distributionen mit voller SELinux-Unterstützung ausgeliefert. Mittlerweile ist es ebenfalls fester Bestandteil von \"CentOS\", \"Hardened Gentoo\" und \"openSUSE\". Unter Ubuntu sowie Debian ist dieses über die Paketverwaltung nachinstallierbar. Die Implementierung für \"Slackware\" ist noch in Arbeit. Mit Einführung von Android 4.3 wurde auch offiziell der auf dem Linux-Kernel basierende Android-Kernel um SELinux erweitert. Zuvor haben bereits Hersteller wie HTC und Samsung in ihren Smartphone-Modellen die Kernelerweiterung eingesetzt, um erweiterte Sicherheitsfunktionen zu implementieren.\n\nNeben den offiziellen SELinux-Werkzeugen existieren zahlreiche nützliche Werkzeuge, die das Arbeiten mit SELinux erleichtern.\n\n\"Setroubleshoot\" benachrichtigt über ein Task-Symbol über durchgesetzte Beschränkungen von Programmen und stellt auf Anfrage zusätzliche Informationen wie auch mögliche Lösungsvorschläge bereit, um das Problem zu beheben. \"SLIDE\" ist eine IDE für die Entwicklung der Richtlinie, die in Form einer Eclipse-Erweiterung veröffentlicht wird. Das Programm \"apol\" ist für die Analyse von Richtlinien zuständig.\n\n\n"}
{"id": "657326", "url": "https://de.wikipedia.org/wiki?curid=657326", "title": "Adam7", "text": "Adam7\n\nAdam7 ist ein Algorithmus zum allmählichen Bildaufbau beim Transfer von Bilddaten über eine langsame Verbindung, wie etwa schmalbandiges Internet. Das Ziel ist es, dem Benutzer anstatt eines Bildes, das sich von oben nach unten langsam aufbaut, eine grobe Version des Bildes zu zeigen, die sich nach und nach verfeinert. Adam7 ist nach seinem Erfinder Adam M. Costello benannt und wird unter anderem im Bildformat PNG angewandt.\n\nDas Gesamtbild wird in insgesamt sieben Durchläufen aufgebaut. Dazu ist es zunächst notwendig, das Ausgangsbild in Blöcke von 8×8 Pixeln zu zerlegen. Jedes dieser Pixel hat eine bestimmte Wertigkeit von 1 bis 7. In der Bilddatei werden nacheinander die sieben Durchläufe abgespeichert. Jeder Durchlauf besteht aus den Pixeln mit der entsprechenden Wertigkeit. Beim Lesen der Datei wiederum werden die entsprechenden Durchläufe nacheinander übertragen.\n\nDie Anzahl der Pixel nimmt bei jedem Durchlauf zu oder bleibt gleich. Die Wertigkeiten sind wie folgt definiert:\n\nEs ist gebräuchlich, für die bereits vorhandenen Pixel über den restlichen, noch nicht übertragenen Bildblock Rechtecke zu zeichnen, sodass der Eindruck einer allmählichen „Verfeinerung“ entsteht.\n\nAdam7 hat gegenüber dem in GIF verwendeten Schema, bei dem nur jeweils vollständige Zeilen übertragen werden, den Vorteil, dass der erste Durchlauf weniger Pixel betrifft und somit ein Ausgangsbild achtmal so schnell präsentiert werden kann. Zudem hilft die Verfeinerung von Spalten zusätzlich zu Zeilen, Text schneller lesbar zu machen.\n\nAllerdings ist Adam7 komplizierter zu implementieren. Insbesondere muss darauf geachtet werden, dass auch Bilder, die kleiner als ein Block sind, korrekt ausgewertet werden. Komprimierte Bilddateien, die Adam7 verwenden, sind außerdem in der Regel etwas größer als normale Bilddateien, da die natürliche Reihenfolge der Bilddaten nicht beibehalten wird. Deshalb sollte Adam7 nur bei größeren Bildern, die ins Internet gestellt werden oder auf einem langsamen Speichermedium lagern, verwendet werden.\n\n"}
{"id": "657792", "url": "https://de.wikipedia.org/wiki?curid=657792", "title": "MSXML", "text": "MSXML\n\nMicrosoft XML Core Services (MSXML) ist eine Programmierschnittstelle, die die Entwicklung von nativen Windows-Programmen mit XML-Unterstützung in den Programmierumgebungen JScript, VBScript und anderen Microsoft-Entwicklungsumgebungen erlaubt. Sie unterstützt XML in der Version 1.0, DOM, SAX, XSLT, XML Schemata in XSD und XDR, sowie andere zu XML gehörende Technologien.\n\nMSXML ist eine Menge eigenständiger Produkte, jeweils veröffentlicht und unterstützt von Microsoft:\n\n\n"}
{"id": "658070", "url": "https://de.wikipedia.org/wiki?curid=658070", "title": "PowerCD", "text": "PowerCD\n\nDas PowerCD ist ein tragbarer CD-Spieler von Apple, der im Sommer 1993 vorgestellt wurde. Mittlerweile ist es nicht mehr offiziell zu kaufen, da es nicht den erhofften Erfolg brachte. CD-ROM-Laufwerke wurden zu diesem Zeitpunkt schon in den Apple-Computern serienmäßig integriert, weshalb sich diese Gerätschaft überflüssig machte.\n\nUm Audio-CDs abzuspielen, gibt es einen Anschluss, über den das Gerät mit einer Stereoanlage verbunden wird. Das PowerCD-Laufwerk kann neben Audio-CDs auch Kodak-Photo-CDs abspielen. Die Bilder von Photo-CDs können über ein FBAS-Kabel auf einem Fernseher angezeigt werden. Das PowerCD kann über SCSI mit einem Computer verbunden werden und dadurch Daten-CDs zugänglich machen. \n\nDas PowerCD wiegt 1,4 Kilogramm und kann entweder über ein Netzteil oder über acht Batterien vom Typ AA mit Strom versorgt werden, so dass man das Gerät auch unterwegs einsetzen kann.\nEs ist nahezu baugleich mit dem Philips CDF 100 und dem Kodak PCD 880, die sich aber in den unterschiedlichen jeweiligen Firmenlogos unterscheiden und zudem über keine SCSI-Schnittstelle verfügen.\n\nDas Gerät bietet ein orange hintergrundbeleuchtetes LCD sowie sechs Tasten an der Vorderseite (Stop, Pause, Play, Next, Prev, Mode). Auf der Oberseite befinden sich der Netzschalter und der CD-Auswurf.\n\nMit einer Infrarotfernbedienung mit 38 Tasten kann die Audio-CD- bzw. Photo-CD-Funktion bedient werden.\n\n"}
{"id": "661954", "url": "https://de.wikipedia.org/wiki?curid=661954", "title": "System/34", "text": "System/34\n\nDas System/34 (auch S/34) war eine Minicomputer-Architektur der Firma IBM. Es wurde im April 1977 als preiswerte Alternative zu den Großrechnern der Serie S/370 vorgestellt. Es war der Nachfolger des System/32 und der Vorläufer des System/36 sowie das kleinere Schwestersystem der S/38. Die S/34 diente als Mehrplatz-Rechner für Abteilungen und kleinere Unternehmen und wurde bis Februar 1985 produziert. \n\nDie Hauptspeicher-Kapazität betrug maximal 256 Kilobyte RAM. Die S/34 konnte in der Grundausstattung mit zwei und per Erweiterungsmodul mit vier Plattenlaufwerken mit jeweils 64 Megabyte betrieben werden. Zum Betrieb erforderlich war ein angeschlossener und betriebsbereiter Kettendrucker als Systemdrucker. Sein Fehlen machte den Betrieb der S/34 zwar nicht unmöglich, wurde beim Start des Systems aber mit einer antwortpflichtigen Fehlermeldung quittiert. Die Datensicherung erfolgte ausschließlich über Disketten zu 8″. Je nach Ausführung der S/34 gab es drei Einzelzuführungen oder zusätzlich zwei Schächte für Magazine mit jeweils zehn Disketten.\n\nAn das System konnten maximal 14 Terminals vom Typ 5250 mit Twinaxialkabeln angeschlossen werden. An einem Strang konnten bis 7 Geräte angeschlossen werden, die seriell nacheinander geschaltet wurden. Dazu hatte jedes Terminal einen Ein- und Ausgang. Das Endgerät musste terminiert werden. Das Terminal am ersten Strang in der ersten Position mit der logischen Adresse 0/0 war die Steuer- oder Systemkonsole für Operator und Administratoren, an der die Fehlermeldungen des Betriebssystems SSP, die sogenannten System Reference Codes (SRC) abgelesen und ggf. beantwortet werden konnten. Hier erfolgte auch die gesamte sonstige Steuerung der S/34 incl. der Überwachung des Systemstarts (\"Initial Program Loading\", \"IPL\") und der Reorganisation der Platten (\"Compress\").\n\nDas System gliederte sich in vier Teile:\n\n\nDas proprietäre Betriebssystem \"SSP\" erschien in neun Versionen. Es verfügte über eine generelle Menüsteuerung, die die S/34 trotz der im Vergleich zur S/38 deutlich geringeren Funktionen wesentlich bedienerfreundlicher machte. So konnte man Befehle aus einer Menüstruktur auswählen, was nicht so versierten Administratoren und besonders Anwendern komplizierte Funktionen erleichterte. Programmiert wurde das System/34 über OCL (Operating Control Language) als Steuersprache und RPG oder seltener Cobol als Programmiersprache.\n\nHierfür waren die Hilfsmittel SEU (Source Entry Utility) als Einzelzeileneditor für den Quellcode und SDA (Screen Design Aid) als Entwurfswerkzeug für Bildschirmmasken verfügbar. Des Weiteren wurde eine als POP (Programmer and Operator Productivity Aid) bezeichnete Hilfe angeboten, die einen Ganzbildeditor (Full Screen Editor, FSE) umfasste. Sie ermöglichte es, viele für Entwickler des Systems nötige Funktionen über besondere Menüs und Steuerprogramme aufzurufen. Ferner standen Generatoren zur Verfügung, die die einfache Dialogbearbeitung oder den schnellen Ausdruck von Datendateien ermöglichten.\n\nIn der Praxis war es öfter erforderlich, zur Reorganisation der Platten einen \"Compress\" durchzuführen, ähnlich dem heutigen Defragmentieren bei den Windows-PCs. Hierbei wurden frei gewordene Bereiche der Festplatte zu größeren Blöcken zusammengefasst, damit das Betriebssystem wieder über diesen Platz verfügen konnte, da neu zu schreibende Daten nur in passende Blöcke geschrieben werden konnten. Dies dauerte oft mehrere Stunden.\n\n\nIBM System/34\n"}
{"id": "663204", "url": "https://de.wikipedia.org/wiki?curid=663204", "title": "Visualisierung", "text": "Visualisierung\n\nMit Visualisierung oder Veranschaulichung (Sichtbarmachen) meint man im Allgemeinen, abstrakte Daten (z. B. Texte) und Zusammenhänge in eine graphische bzw. visuell erfassbare Form zu bringen. Dazu gehört etwa die Umsetzung eines Marketingkonzepts durch einen Werbespot, die Entwicklung eines Drehbuchs aus einem Drama, oder die gestenreiche Darstellung eines Sachverhalts bei einem Vortrag oder die Prozessvisualisierung im technischen Bereich. Im Speziellen bezeichnet Visualisierung den Prozess, sprachlich oder logisch nur schwer formulierbare Zusammenhänge in visuelle Medien zu übersetzen, um sie damit verständlich zu machen. Weiterhin wird Visualisierung eingesetzt, um einen bestimmten Zusammenhang deutlich zu machen, der sich aus einem gegebenen Datenbestand ergibt, der aber nicht unmittelbar deutlich wird.\n\nDabei werden Details der Ausgangsdaten weggelassen, die im Kontext der gewünschten Aussage vernachlässigbar sind. Zudem sind stets gestalterische Entscheidungen zu treffen, welche visuelle Umsetzung geeignet ist und welcher Zusammenhang gegebenenfalls betont werden soll. Visualisierungen implizieren daher stets eine Interpretation der Ausgangsdaten, werden aber auch durch textliche oder sprachliche Angaben ergänzt, um eine bestimmte Interpretation zu kommunizieren. Schließlich wird Visualisierung auch rein illustrativ benutzt, um etwa ein Gegengewicht zum Textkörper zu bilden, ohne eine eigene Aussage zu transportieren.\n\nAls Medien für Visualisierung kommen zum Beispiel manuelle, gedruckte und Computergrafik, Datentabellen, Film- und Computeranimation zur Anwendung.\n\n\nIm industriellen und technischen Bereich gibt es für die Visualisierung von Prozessabläufen spezielle Software, sogenannte Visualisierungssysteme.\n\nFür Medienplayer bezeichnen Visualisierungen Techniken zur Darstellung von abgespielter Musik in Form von bewegten Bildern.\n\nDie Wissenschaft von der Visualisierung von Daten nutzt Kenntnisse über\naus, um anwendungsbezogen visuelle Metaphern zum korrekten, effizienten und umfassenden Erkennen von Datenmustern systematisch herzuleiten. Die Aktivität des Visualisierens ist ein davon zunächst abgetrennter Teil, der durch die Visualisierungs-Pipeline schematisiert wird.\n\nDie Visualisierungs-Pipeline spezifiziert die Prozesskette mittels derer Daten in Bilder überführt werden. Sie besteht aus in Reihe geschalteten Funktionen zum Generieren, Filtern und Bereinigen von Daten, zum Abbilden der Daten auf Geometrien und Materialien, zum Rendern dieser Objekte und zum Darstellen des gerenderten Bildes. Das Paradigma der erweiterten Visualisierungs-Pipeline schließt die interaktive Ausführung oder Steuerung durch mindestens einen Zuschauer ein.\n\nNicht zwingend notwendig, jedoch vermehrt impliziert, ist die Implementierung der Visualisierungspipeline als Visualisierungsprogramm auf einem Computer. Dessen Verwendung komplementiert dann das automatisierte Finden und Bewerten von Datenmustern als Bestandteil des Data-Minings.\n\nVerbreitet ist das folgende Prinzip:\n\nDaten(-gewinnung) → Filter(-bereinigung) → Konvertierung (auf Geometrie und Attribute) → Darstellung (je nach Perspektive, Anzeigetechnik)\n\nDie \"wissenschaftliche Visualisierung\" bezeichnet die Wissenschaft und die Methodik der Visualisierung von gemessenen Daten oder Simulationsergebnissen denen unmittelbar physikalische Prozesse zugeordnet werden. Anwendungsfelder stammen dabei aus den Ingenieurs- und Naturwissenschaften. \nEine wissenschaftliche Visualisierung muss dabei drei Kriterien entsprechen: \nAls Spezialgebiet der \"wissenschaftlichen Visualisierung\" umfasst die \"medizinische Visualisierung\" die Erforschung und Anwendung von Methoden zur Visualisierung von Lebewesen zum Zweck der medizinischen Diagnose.\n\nDie \"Informationsvisualisierung\" ist die Visualisierung von abstrakten Daten, die nicht unmittelbar mit physikalischen Zuständen und Prozessen assoziiert werden. Dieses sind zum Beispiel Dokumente, Börsenergebnisse und Demografiedaten. Ähnlich verhält es sich auch bei der Visualisierung von Kennzahlen, Analysewerten und Berichten. Diese basieren in der Regel auf Zahlen und Zeichen die häufig in tabellarischer Form vorliegen.\n\nEin individualistisches Beispiel von Visualisierung in der bildenden Kunst liefert uns der bekannte deutsche Landschaftsmaler Caspar David Friedrich mit seiner Aussage:\n\nIn der Architektur und Innenarchitektur bezeichnet der Begriff Visualisierung die bildliche Darstellung eines geplanten Bauwerks oder einer städtebaulichen Situation. Die Visualisierung tritt anstelle von Technische Zeichnungen, die für Laien oft schwer lesbar sind.\n\nIn Logos werden Informationen oder Ideen grafisch verdichtet. Sie können rechtlich geschützt sein. Beispiel: WWF (World Wide Fund For Nature)\n\nIn Labels werden Konzepte oder Marken z. B. mittels Symbolen dargestellt. Sie können rechtlich geschützt sein. Beispiel: Grüner Punkt\n\nIn Cartoons und Comics werden politische, soziale, persönliche Erfahrungen und Ideen optisch verdichtet. Beispiel: Mordillo, Walt Disney\n\nIn Porträts sind Persönlichkeiten und ihre Lebensgeschichten enthalten. Beispiel: Einstein (hier sogar in einer Formel)\n\nDiagramme machen Zusammenhänge visuell erfahrbar. Beispiel: Energiekonzept\n\nAllgemein eine auf das Wesentliche beschränkte Darstellung eines Sachverhaltes, siehe z. B. R&I-Fließschema.\n\nEin Piktogramm ersetzt schriftliche Kurz-Hinweise. Beispiel: Fluchtweg\n\nEin Foto macht Inhalte unmittelbar visuell erfahrbar. Beispiel: Foto von Mare Frisium\n\nPinnwandmoderation visualisiert Ideen, Entwicklungen, Gruppenprozesse.\n\nDie Visualisierung ist eine Querschnitts-Wissenschaft, die – neben Anwendungsgebiet der Computergrafik – je nach Betrachtungsweise auch aufgefasst werden kann als:\n\nJemand, der eine Visualisierung eines Sachverhalts erzeugen möchte, muss\n\nDie technische Produktvisualisierung ist die 3D-Darstellung einer Produktgeometrie. Da der Empfänger meist nicht alle Informationen, die in einem CAD-Datenmodell vorhanden sind, benötigt, kann dieses vereinfacht werden.\n\nVisualisierungen können heute ganz einfach am Computer mittels Visualisierungsprogrammen erstellt werden. Für die unterschiedlichsten Aufgabenstellungen gibt es ein breitgefächertes Spektrum an Programmen. 3D-Visualisierungen gewinnen in Unternehmen zunehmend an Bedeutung und werden nicht mehr ausschließlich für Werbezwecke eingesetzt. Im Verkauf, im Vertrieb für interne Präsentationen, für die Produktentwicklung und in der PR- und Öffentlichkeitsarbeit werden Visualisierungen immer wichtiger. Um die Visualisierungen dem gesamten Unternehmen und auch den externen Partnern und Zulieferern zur Verfügung zu stellen, werden die einmal erstellten Medien für die vielfältigen Einsatzmöglichkeiten (Print, Web, Video, CD-ROM usw.) aufbereitet und in einer zentralen Datenbank verwaltet. Und das alles ist bereits möglich, während die Produkte noch in der Planung oder Produktion sind. Bildelemente oder ganze Bildwelten werden dazu künstlich erzeugt oder in reale Bilder integriert. Klassisch gefilmtes oder auf Videobändern aufgezeichnetes Bildmaterial wird digitalisiert, bearbeitet und mit visuellen Effekten versehen. Bewegte oder stehende Bilder werden retuschiert, korrigiert oder miteinander zu neuen, absolut real wirkenden Bildern kombiniert.\n\n\n\n"}
{"id": "663868", "url": "https://de.wikipedia.org/wiki?curid=663868", "title": "Elektronisches Handelssystem", "text": "Elektronisches Handelssystem\n\nEin elektronisches Handelssystem (auch Computerbörse) ist ein automatisiertes, elektronisches System zum Abschluss von Börsengeschäften, d. h., es führt Angebot und Nachfrage nach Wertpapieren zusammen. Im Gegensatz zum Parketthandel ist keine visuelle Interaktion und Kommunikation zwischen den Handelsteilnehmern nötig. Das elektronische Handelssystem führt Transaktionen von Wertpapieren, Währungen oder Waren automatisch aus, wenn passende Kauf- und Verkaufangebote vorliegen. Computerbörsen sind daher auch nicht ortsgebunden.\n\nDie erste vollelektronische Börse Deutschlands war die 1990 in Betrieb genommene Deutsche Terminbörse, auch DTB genannt. In Deutschland existieren die elektronischen Handelsplattformen Xetra, Xontro und Tradegate für den Kassamarkt sowie Eurex für den Terminmarkt.\n\nDie an die NASDAQ angeschlossenen, sekundären elektronischen Handelssysteme bezeichnet man als Electronic Communication Networks (ECNs). Weiterhin existiert in Amerika noch das außerbörsliche OTC Bulletin Board.\n\n"}
{"id": "665260", "url": "https://de.wikipedia.org/wiki?curid=665260", "title": "Applixware", "text": "Applixware\n\nApplixware ist der Name einer Office-Suite von Vistasource für verschiedene Betriebssysteme. Das Office-Paket wurde ursprünglich von Applix für Unix-Betriebssysteme entwickelt.\n\nWegen der starken Verbreitung des kostenlosen Konkurrenzprodukts OpenOffice unter Unix verschob sich Schwerpunkt von Applixware weg von einem gewöhnlichen Office-Paket hin zu einer komplexen Datenverarbeitungslösung für Konzerne. Das Programmpaket läuft auf unterschiedlichen Plattformen und Betriebssystemen, wie Linux, Windows und auf vielen Unix-Derivaten.\n\nApplixware bietet u. a. folgende Module:\n\nUnter Linux und Solaris kann der Anyware Server eingerichtet werden, dessen Client-Funktion unter Linux, Solaris und Windows genutzt werden kann.\n\nApplixware ist kostenpflichtig, steht aber privaten Linux-Nutzern in einer abgespeckten Version kostenfrei zur Verfügung.\n\n\n"}
{"id": "671627", "url": "https://de.wikipedia.org/wiki?curid=671627", "title": "Avant Browser", "text": "Avant Browser\n\nAvant Browser ist ein Webbrowser, der nach individuellem Wunsch des Nutzers auf die HTML-Rendering-Engine Trident des Internet Explorers, Gecko von Firefox oder WebKit von Chrome aufbaut. Die grafische Benutzeroberfläche hat Opera (in der Anwenderfreundlichkeit bis Version 12) als Vorbild. Der Avant Browser wollte ursprünglich die Vorteile des Internet Explorers und von Opera vereinen, weshalb Avant zu Beginn \"IEOpera\" hieß. Die erste Version wurde am 30. Januar 2004 veröffentlicht. Ab Oktober 2011 wurden Gecko und Webkit als zusätzliche frei wählbare Engine hinzugefügt.\n\nAvant Browser ist Freeware eines chinesischen Programmierers, der unter dem Namen Anderson Che auftritt. Auf der Kontaktseite des Internetauftritts von Avant Browser werden nur Forums- und E-Mail-Adressen ohne weitere Angaben genannt.\n\nDer Browser fügt in der „kleinen Version“ dem Internet Explorer zahlreiche Merkmale hinzu. Diese können in Komfortmerkmale, die das Surfen mit Avant gegenüber dem IE erleichtern und in Sicherheitsmerkmale, die das Surfen sicherer machen, unterteilt werden. Einige dieser Merkmale sind im mittlerweile erschienenen Internet Explorer 7 / 8 sowie 9 bereits integriert.\n\nDie Ultimate Version des „Avant Browsers“ vereint wie der Browser Lunascape drei unterschiedliche Web-Engines in einem Browser. Der Anwender kann auf Trident des Internet Explorer, Gecko des Firefox und Webkit von Google Chrome zugreifen. Nutzer können damit durch einfache Einstellung per Mausklick wählen, welche Web-Engine für die Darstellung der Webseiten verwendet werden soll.\n\n\n\n\n"}
{"id": "671647", "url": "https://de.wikipedia.org/wiki?curid=671647", "title": "Cray T3E", "text": "Cray T3E\n\nDer Cray T3E ist ein massiv-paralleler Mehrprozessor-Rechner mit verteiltem Speicher (Distributed Memory MIMD), der in den 1990er Jahren von der Firma Cray entworfen wurde und 1995 das Vorgängermodell Cray T3D ablöste. Der T3E bestand aus 8 bis 2176 sog. \"Processing Elements\" (PEs), jedes bestehend aus einem mit 300 MHz getakteten DEC Alpha 21164 (EV5) Mikroprozessor, zwischen 64 MB und 2 GB RAM sowie einem 6-Wege-Switch, der über eine maximale Bandbreite von 480 MB/Sekunde in beide Richtungen verfügte. Wie beim T3D wurden die PEs so verbunden, dass sie einen dreidimensionalen Toruskörper formten.\n\nEiner der größten Unterschiede zum T3D (und vielen anderen MPP-Systemen) lag darin, dass der T3E keinen Hostrechner mehr benötigte und selbst unter dem UNICOS/mk-Betriebssystem lief (Dessen einzelne Server wurden (wie bei MACH) auf verschiedenen PEs ausgeführt). Das I/O-Subsystem, Cray's \"GigaRing\", das die Netzwerk-, Disk- und Tape-I/O bereitstellte, wurde direkt in den Torus integriert.\n\nVom T3E existierten mehrere Versionen:\nBeginnend mit dem T3E-900 wurde für die PEs nicht mehr der 21164 (EV5) verwendet, sondern der schnellere 21164A (EV56).\n\nAlle Versionen wurden sowohl luftgekühlt (\"Air Cooled\", \"AC\") als auch flüssiggekühlt (\"Liquid Cooled\", \"LC\") angeboten. Abhängig davon konnten 16 bis 128 PEs (luftgekühlt) oder 64 bis 2048 PEs (flüssiggekühlt) eingesetzt werden.\n\nEin T3E war der erste Supercomputer, der bei der Ausführung einer wissenschaftlichen Anwendung eine Leistung von über 1 TFLOPS erreichte; die Anwendung wurde 1998 auf einem T3E-1200 mit einer Konfiguration von 1480 PEs ausgeführt (siehe externer Link).\n\n"}
{"id": "675013", "url": "https://de.wikipedia.org/wiki?curid=675013", "title": "Vegas Pro", "text": "Vegas Pro\n\nVegas Pro (Eigenschreibweise: VEGAS Pro) ist ein kommerzielles Videoschnittprogramm des Unternehmens Magix Software GmbH. Die Bearbeitungssoftware eignet sich für Video, Highdefinition, Audio und Compositing. Entstanden ist es aus einer Audiosoftware des Unternehmens Sonic Foundry. Vegas wird ausschließlich für Windows-PCs vertrieben. Die Software wurde bis Mai 2016 von Sony Creative Software entwickelt und gehört seitdem zu Magix.\n\nBesondere Kennzeichen sind die Möglichkeiten zur Workflow-Individualisierung durch Scripting und die Bedienung: Viele Bearbeitungsschritte geschehen direkt auf der Timeline. Windows-Standards wie Drag’n’Drop sorgen für einen einfachen Workflow. Auch externe Controller wie z. B. die MackieControlUniversal oder das Contour JogShuttle arbeiten mit der Software zusammen. Die Materialorganisation von Vegas umfasst Media-Manager, flexible Ordnerstruktur und Suchfunktion. Die verschiedenen integrierten Titelmodule können bei Bedarf z. B. mit Heroglyph von proDAD, Graffiti von BorisFX o. a. ergänzt werden. An Audiostandards unterstützt Vegas ASIO, VST und DirectX. Möglich sind auch Voiceover, CD-Mastering und Mehrspuraufnahmen. Die Effekt- und Compositingmöglichkeiten beinhalten eine Farbkorrektur, Chroma-Keying, weiche Zeitlupen – auch mit dynamischen Beschleunigungs- und Abbremsphasen –, Frameratenkonvertierung, Animationen im 3D-Raum, Parent-Child-Spurverknüpfungen mit Vererbung von Eigenschaften sowie diverse Import- und Exportfunktionen.\n\nAls Programme lassen sich mit Vegas Pro verbinden: Hitfilm als Compositing-Software, Cubase SX z. B. über MidiTimeCode, die After-Effects-Plug-ins z. B. über BorisRed oder WaveLab über „Send to external Audio-Editor“. Vegas Pro kann daher neben dem Schnitt auch zum Zusammenführen weiterer Produktionsschritte einer Videoproduktion eingesetzt werden. Eine hohe Kompatibilität besteht durch Unterstützung von Windows-Standards, unter anderem OpenDML, DirectX, .NET-Scripting und Windows Media Video. Zusätzlich zu installierende Playback-Codecs sind für den Einsatz der Software nicht notwendig.\n\nAb Version 11 basiert die überwiegende Anzahl der internen Filter auf dem OFX-Standard, so dass beispielsweise das Keyframing jedes einzelnen Filterparameters auch per Bézierkurven steuerbar ist. Zudem wird nun die OpenCL-Architektur genutzt, um den Grafikprozessor sowohl zum Rendern als auch für verschiedene Bearbeitungsprozesse zu nutzen, um beispielsweise eine bessere Wiedergabeleistung bei Einsatz von Effekten zu erreichen.\n\nVegas Pro unterstützt neben DV und MPEG-2 HD-Formate wie XAVC, P2 AVC-Intra, HDCAM SR, HDV, AVCHD und XDCAM HD 422. Seit September 2008 unterstützt Vegas Pro in der Version 8.1 auf Basis von Windows Vista 64 zusätzlich 64-Bit-Systeme. Vegas Pro 9.0 wird von Grund auf als 32-Bit- und 64-Bit-Version ausgeliefert und unterstützt die 4K-Auflösung. Aktuelle Versionen unterstützen ausschließlich 64-Bit-Plattformen. Ab Version 10 wird auch die Bearbeitung von 3D-Material unterstützt. Version 14 unterstützt erstmals den Im- und Export von HEVC-Videos.\n\nVegas Movie Studio ist die Einsteiger-Variante von Vegas Pro. Diese Software enthält nicht dieselben professionellen Features wie Vegas Pro: SDI-Unterstützung, Remote-Controller, Scripting oder weitreichende Compositing-Möglichkeiten fehlen beispielsweise. Beibehalten wurde die vorkonfigurierte Vegas-Oberfläche. Das Movie-Studio-Projektformat kann auch mit Vegas Pro geöffnet werden.\n\n\n"}
{"id": "676240", "url": "https://de.wikipedia.org/wiki?curid=676240", "title": "IBM 704", "text": "IBM 704\n\nDer IBM 704 der 700/7000 series war der erste in kleiner Stückzahl hergestellte Großrechner, der Gleitkommaarithmetik beherrschte. IBM stellte den von Gene Amdahl mitentwickelten Rechner im April 1954 der Öffentlichkeit vor.\n\nZu seinem Vorgänger, der 1953 erbaute IBM 701, auch \"Defense Calculator\" genannt, dessen überwiegende Produktion an das US-Verteidigungsministerium und die Militärflugzeugindustrie ging, war die IBM 704 nicht kompatibel. Der Rechner hatte eine verbesserte Rechnerarchitektur, drei zusätzliche Indexregister und Kernspeicher statt Williamsröhren. Der neue Befehlssatz wurde zur Grundlage der IBM 700/7000-Großrechnerserie.\n\nLaut Angaben des Herstellers, der bis 1960 123 Anlagen verkaufte, konnte die IBM 704 bis zu 40.000 Befehle pro Sekunde ausführen. Die Programmiersprachen Fortran und LISP wurden als erstes für diesen Rechner entwickelt. Die Benennung der primitivsten Lisp-Befehle für den Umgang mit Listen (car, cdr) gehen bis heute auf die Benennung der Register der IBM 704 zurück. Die Buchstaben in dem Wort \"car\" beziehen sich auf die englische Beschreibung \"\"c\"ontents of the \"a\"ddress part of \"r\"egister\" (Inhalt des Registeradressteils) und die in \"cdr\" auf die englische Beschreibung \"\"c\"ontents of the \"d\"ecrement part of \"r\"egister\" (Inhalt des Registerdekrementteils).\n\nDer IBM 704 war 1961 der erste Rechner, der bei den Bell Labs per Sprachsynthese ein Lied wiedergeben konnte. Dabei handelte es sich um \"Daisy Bell\" von Harry Dacre. Später wurde die Idee in Stanley Kubricks übernommen.\n\n\n"}
{"id": "676608", "url": "https://de.wikipedia.org/wiki?curid=676608", "title": "OS-Fingerprinting", "text": "OS-Fingerprinting\n\nUnter dem Begriff OS-Fingerprinting (englisch für „Betriebssystem-Fingerabdruck“), spezieller auch TCP/IP-Stack-Fingerprinting (englisch für „TCP/IP-Protokollstapel-Fingerabdruck“), versteht man die Erkennung von Betriebssystemen durch die Beobachtung diverser Reaktionsweisen der Systeme im Netzwerk aus der Ferne. Zur Erkennung des Betriebssystems können sowohl aktive als auch passive Methoden Verwendung finden.\n\nÜber den TCP/IP-Protokollstapel das Betriebssystem zu ermitteln ist bei beiden Methoden möglich; zusätzlich kann bei der aktiven Variante noch das Banner eines Programmes analysiert werden.\n\nEine weitverbreitete Methode der Betriebssystem-Analyse ist jene mittels TCP/IP-Protokollstapel. Dabei wird die Eigenschaft genutzt, dass jedes Betriebssystem seine eigene TCP/IP-Protokollstapel-Implementierung hat, dessen Einstellungen sich im Header von Netzwerkpaketen finden und die sich von denen anderer Betriebssysteme unterscheiden. Folgende Felder variieren innerhalb verschiedener Implementierungen:\n\nZusammen ergeben die Daten eine 67-Bit-Signatur.\n\nDen TCP/IP-Protokollstapel zu analysieren ist aber nicht immer erfolgreich, weil sich bei vielen Betriebssystemen die obigen Felder konfigurieren lassen, wodurch man sich sogar als ein anderes Betriebssystem ausgeben kann, als man eigentlich nutzt.\n\nPassive Methoden zeichnen sich dadurch aus, dass sie absolut latent durchgeführt werden können. Bei diesen Methoden wird ausschließlich der ablaufende Datenverkehr zwischen dem Beobachter und dem Zielsystem bewertet und analysiert. So kann beispielsweise eine einfache Websitzung durch die gleichzeitige Analyse durch ein passives OS-Fingerprinting detaillierte Informationen zu einem Zielsystem offerieren.\n\nAktive Methoden zeichnen sich dadurch aus, dass sie die Initiative ergreifen und Daten zum Zielhost übertragen, in der Hoffnung, dass aus der daraus resultierenden Antwort eine Analyse möglich ist. Daher ist diese Methode aggressiver Natur und wird nicht immer gerne gesehen. Ebenfalls ist es für Intrusion Detection Systems (IDS) möglich, laufende aktive Fingerprintings zu identifizieren.\n\nNeben der oben erwähnten Methode mittels TCP/IP-Protokollstapel ist es auch möglich, das Betriebssystem mittels des sogenannten Banners herauszufinden. Banner sind Textzeilen, mit denen sich beispielsweise HTTP- oder FTP-Dienste beim Verbindungsaufbau zu erkennen geben. Ein Banner enthält im günstigsten Fall sowohl Informationen über den entsprechenden Dienst als auch über das Betriebssystem. Diese Technik wird auch als \"Banner Grabbing\" bezeichnet und steht auch manchen Portscannern zur Verfügung.\n\nBeispiel mit telnet und FTP:\n\nBeispiel mit netcat:\nEin Banner-Grabbing ist aber nicht immer erfolgreich, weil viele Programme entweder die Möglichkeit besitzen, das Banner zu deaktivieren oder zu editieren, wodurch man entweder keine Informationen erhält oder ein anderes Betriebssystem suggeriert wird als tatsächlich benutzt wird.\n\n\n"}
{"id": "681140", "url": "https://de.wikipedia.org/wiki?curid=681140", "title": "Ipchains", "text": "Ipchains\n\nIPChains ist eine ältere Paketfilter-Software. Sie wurde 1999 von Rusty Russell auf Basis von Ipfwadm entwickelt und wurde in den Linux-Kernel in der 2.2er Versionsserie eingebaut, in der Version 2.4 wurde es durch den Nachfolger Netfilter/Iptables ersetzt.\n\nIPChains ähnelt den Cisco Access Control Lists, es kontrolliert welche Pakete in das System hinein und herauskommen können. Obwohl es primär als Firewall benutzt wird, kann man es auch verwenden, um ein Linux-Betriebssystem zusätzlich zu härten.\n\n"}
{"id": "681305", "url": "https://de.wikipedia.org/wiki?curid=681305", "title": "Handheld-Konsole", "text": "Handheld-Konsole\n\nEine Handheld-(Spiel-)Konsole ist ein tragbares elektronisches Gerät, welches in erster Linie zum Spielen von Videospielen entwickelt wird. Anders als bei stationären (Spiel-)Konsolen sind Bedienelemente, Bildschirm und gegebenenfalls Lautsprecher bereits fest in der Konsole eingebaut. Im Laufe der 1960er- und 1970er-Jahre brachten erstmals verschiedene Hersteller, darunter Mattel und Milton Bradley, tragbare Tabletop- und andere (LCD-) Konsolen auf den Markt (Tabletop-Konsolen werden nicht zu den Handheld-Konsolen gezählt, da sie nicht zum Mitnehmen gedacht sind). Die erste Handheld-Konsole war Mattels Auto Race aus dem Jahr 1976. Die erste kommerziell erfolgreiche Handheld-Konsole war Merlin – der elektronische Zauberer aus dem Jahr 1978. Die erste Handheld-Konsole mit austauschbaren Spielmodulen war schließlich das Microvision, das 1979 von der Milton Bradley Company veröffentlicht wurde. Mit der Markteinführung des Game Boy im Jahre 1989 eroberte Nintendo bis heute die Marktführerschaft auf dem Handheld-Sektor und trug zur Popularisierung des Handheld-Konzeptes bei.\nDie erste Handheld-Konsole mit austauschbaren Spielmodulen, das Microvision, wurde 1979 von Smith Engineering entwickelt und von Milton Bradley (kurz: MB) vertrieben. Aufgrund des recht kleinen LC-Displays und einer Spielauswahl von nur 13 Titeln zeigte es sich jedoch nicht als langfristigen Erfolg und die Produktion wurde bereits zwei Jahre später eingestellt. Die Bedienelemente konnten leicht beschädigt werden und die LCDs der späten 1970er waren von mangelhafter Qualität, sodass sie oft undicht oder dunkel wurden, weshalb funktionierende Geräte des Microvision zu einer Seltenheit geworden sind.\n\n1983 wurde in den USA von Palmtex das Home-Computer-Software-Super-Micro-Cartridge-System veröffentlicht.\n\nFünf Jahre nach dem Microvision, im Jahre 1984, erschien ausschließlich in Japan der Game Pocket Computer von Epoch-sha, die erste programmierbare Handheld-Konsole. Lediglich fünf Spiele wurden offiziell für das System veröffentlicht.\n\n1989 veröffentlichte Nintendo den Game Boy. Das Entwicklerteam unter Leitung von Gunpei Yokoi zeigte sich bereits für die Nintendo Game & Watch-Reihe, das Nintendo Entertainment System sowie die Spiele \"Metroid\" und \"Kid Icarus\" verantwortlich. Kritiker aus der Spieleindustrie standen dem Gerät aufgrund seines Schwarz-weiß-Bildschirms und der geringen Prozessorleistung zunächst skeptisch gegenüber. Das Designerteam war jedoch der Meinung, dass geringe Herstellungskosten und ein sparsamer Batterieverbrauch bei einer Handheld-Konsole wichtiger als die Grafik waren. Verglichen mit dem Microvision war der Game Boy vor allem in dieser Hinsicht ein großer Schritt nach vorne.\n\nYokoi erkannte, dass der Game Boy eine „Killerapplikation“ brauchte, also mindestens ein Spiel, das stellvertretend für die Konsole stehen und die Kunden überzeugen würde, sie zu kaufen. Im Juni 1988 sah Minoru Arakawa, CEO von Nintendo of America, eine Demonstration des Spiels \"Tetris\" auf einer Handelsmesse. Nintendo erwarb die Rechte am Spiel und verkaufte es im Bundle mit dem Game Boy. Der Erfolg ließ nicht lange auf sich warten und gegen Ende des Jahres 1989 hatte man bereits über eine Million Einheiten verkauft. Gegen 1992 betrugen die Verkaufszahl etwa 25 Millionen. Mit knapp 120 Millionen verkauften Einheiten (Game Boy Pocket und Game Boy Color miteingerechnet) ist der Game Boy eine der meistverkauften Spielkonsolen überhaupt.\n\nIm Verlauf der 1990er versuchten verschiedene Hersteller vergeblich, mit der Veröffentlichung neuer Handheld-Konsolen Nintendos Marktführerschaft anzufechten. Der Atari Lynx erschien beispielsweise im selben Jahr wie der Game Boy als erster Handheld mit Farbbildschirm. Das Gerät verfügte neben weiteren speziellen Merkmalen über eine Hintergrundbeleuchtung und konnte für Linkshänder gedreht werden. Aufgrund seines hohen Preises, immensem Batterieverbrauch, Produktionsengpässen, einem Mangel an fesselnden Spielen und Nintendos aggressiver Marketingkampagne verkaufte sich der Atari Lynx trotz der fortschrittlicheren Technik und der Veröffentlichung einer überarbeiteten Version im Jahre 1991 zu keinem Zeitpunkt besonders gut.\n\nAls Reaktion auf den anhaltenden Erfolg des Game Boy wurden mehrere Handhelds entwickelt, die seine größte Schwäche, die geringe Grafikleistung, ausnutzen sollten. Der Sega Game Gear beispielsweise erschien gegen Ende 1990 und verfügte wie der Lynx über ein hintergrundbeleuchtetes Farbdisplay. Die innere Architektur des Game Gear ähnelte der des Sega Master Systems, so dass Sega innerhalb von kurzer Zeit eine große Auswahl an Spielen für den Game Gear anbieten konnte, die ursprünglich für das Master System entwickelt worden waren. Allerdings besaß der Game Gear die gleichen Schwächen wie der Lynx und, obwohl er erfolgreicher als dieser war, gelang es auch dem Game Gear nicht, die Marktherrschaft des Game Boy anzufechten.\n\nIm Laufe der 1990er erschienen weitere Handheld-Konsolen, wie z. B. NECs PC Engine GT, das Watara Supervision oder das Neo Geo Pocket. Trotz der technischen Überlegenheit der meisten dieser Konsolen wurde keine von ihnen zu einer ernsthaften Konkurrenz für den Game Boy.\n\nNeun Jahre nach seiner Veröffentlichung erhielt der Game Boy 1998 erstmals mit dem Game Boy Coloreine Modell mit Farb-Display. Er entsprach in seinen Abmessungen in etwa dem kleineren und leichteren Game Boy Pocket, verfügte jedoch außerdem über einen Farbbildschirm, welcher 32.000 verschiedene Farben darstellen konnte und eine Infrarotschnittstelle, die als Multiplayer-Verbindung diente. Das Gerät war abwärtskompatibel zum Game Boy und konnte somit sowohl mit den vergleichsweise wenigen exklusiven Game Boy Color-Spielen als auch mit allen alten Game Boy-Spielen verwendet werden, wobei die zusätzliche Rechenleistung des Game Boy Color nur sehr gering war.\n\nAm 21. März 2001 veröffentlichte Nintendo mit dem Game Boy Advance einen Nachfolger des Game Boy Color mit zusätzlichen Schultertasten, einem größerem Bildschirm und deutlich mehr Rechenleistung. Zwei Jahre später erschien 2003 mit dem Game Boy Advance SP eine kompakt zusammenklappbare Version mit hinterleuchtetem Display und eingebautem Lithium-Ionen-Akku des Game Boy Advance. Der N-Gage von Nokia aus dem Jahre 2003 konnte Nintendos Game Boy Advance keine Konkurrenz bereiten.\n\nDer Nintendo DS erschien am 21. November 2004 in Nordamerika, wenig später in Japan und 2005 auch in Australien und Europa. Der DS bedeutete die Abkehr von Nintendos bisheriger Vorgehensweise, den vorhandenen Game Boy weiter auszubauen. Das Gerät verfügt über zwei LC-Bildschirme, von denen der untere auf Berührungen reagiert und dem Spieler so eine intuitivere Steuerung von Menüs und Spielfiguren ermöglicht. Der DS verfügt außerdem über eine Sprachsteuerung, ist mit Game Boy Advance-Spielen kompatibel und ermöglicht kabelloses Spielen über eine WLAN-Verbindung mit bis zu 16 Spielern – und auch zu Nintendos Heimkonsole Wii. Dies ist bei einigen Spielen, wie z. B. Mario Kart DS, auch über das Internet möglich.\n\nSonys PlayStation Portable (kurz: PSP) erschien Ende 2004 in Japan, Anfang 2005 in Nordamerika und am 1. September 2005 in Europa. Die PSP verfügt ebenfalls über kabellose Mehrspielerunterstützung und ist die erste Handheld-Konsole, deren Softwaretitel auf optischen Datenträgern, sogenannten UMDs gespeichert sind, was größere Datenmengen zulässt, und somit das Abspielen von Filmen, ähnlich einer DVD, zulässt. Spielstände können auf den beiliegenden Memory Sticks gespeichert werden. Dem Nintendo DS in Bildschirmgröße, Bildschirmqualität und Grafikleistung überlegen, ermöglicht die PSP unter anderem auch das Abspielen von Musik und Filmen und das Betrachten von Bildern. Jedoch war die PlayStation Portable teurer und verfügte über eine geringere Akkulaufzeit als der DS, was dazuführte, dass auch diese Konsole dem DS nicht das Wasser reichen konnte.\n\nDer GP2X von Gamepark Holdings erschien am 10. November 2005 in Deutschland. Für den GP2X kann man dank des vom Hersteller freigegebenen Software Development Kits Spiele und Anwendungen (sogenanntes Homebrew) selbst erstellen. Der GP2X arbeitet mit Linux und ist ein vollwertiger Portable Media Player, der durch Emulatoren auch Spielkonsolen wie das Super Nintendo Entertainment System emulieren kann. Dank eines Speicherkartenschachtes für SD-Karten mit bis zu 4 Gigabyte, quelloffenen Betriebssystems, der Unterstützung von USB-Geräten und Standardbatterien kann auch heute noch mit einigen Entwicklungen seitens des Herstellers und der Community gerechnet werden.\n\nIm Jahr 2010 erschien zuerst in geringer Stückzahl die Pandora, die von Teilen der GP2X-Community entwickelt wurde. Die auf einem quelloffenen Linux basierende Handheld-Konsole fokussierte sich stark auf die Homebrew-Entwicklung und Emulation. 2011 erschienen die Nachfolger der Handhelds von Nintendo und Sony. Nintendo brachte mit dem Nintendo 3DS einen Handheld mit 3D-Bildschirm auf den Markt, während Sony mit der PlayStation Vita vor allem auf gesteigerte Hardwareleistung und ein Touch-Interface auf der Rückseite der Konsole setzte.\n\nIn letzter Zeit bekamen die traditionellen Handhelds zunehmend Konkurrenz durch die technisch aufschließenden Smartphones und Tablets. Diese stellen nämlich mittlerweile ebenfalls ansehnliche 3D-Grafiken dar und bieten mobiles Spielen ohne die zusätzlichen Anschaffungskosten eines eigenen Gerätes sowie günstigere Spielepreise. Weiterhin erscheinen jedoch diverse Handheld-Konsolen aus chinesischer Produktion.\n\nAm 3. März 2017 brachte Nintendo seine neue Spielekonsole Nintendo Switch heraus. Sie verfolgt erstmals ein hybrides Hardware-Konzept und fungiert sowohl als stationäre Konsole an einem Fernseher, als auch als Handheld-Konsole mit abnehmbaren Bedienelementen namens \"Joy-Cons\".\n\nHier eine kleine Auswahl von bekannten Handheld-Konsolen aus der Vergangenheit (mehr Konsolen siehe \"Chronologie\"):\n\n\nEine ausführliche Liste aller (Handheld-)Konsolen kann unter dem Wikipedia-Eintrag \"Liste von Videospielkonsolen\" eingesehen werden.\n\n"}
{"id": "682120", "url": "https://de.wikipedia.org/wiki?curid=682120", "title": "LyX", "text": "LyX\n\nLyX ist ein grafisches Textverarbeitungssystem, das zur Ausgabe das Textsatzsystem LaTeX nutzt. Es kombiniert eine anschauliche grafische Benutzeroberfläche mit typografisch hochwertiger Ausgabe.\n\nDie Benutzeroberfläche sieht auf den ersten Blick ähnlich aus wie bei den grafischen Textverarbeitungssystemen von LibreOffice oder Microsoft Office. Das Konzept unterscheidet sich jedoch erheblich.\n\nBei LyX erfolgt die Formatierung des Dokuments erst bei der Ausgabe. Zunächst wird der Text erfasst. Dabei teilt der Benutzer dem System lediglich mit, welche Funktion wie \"Überschrift\", \"Titel\" oder \"Normaler Text\" der betreffende Text im fertig formatierten Dokument einnehmen soll. Zur Ausgabe wird die Makrosprache LaTeX des Textsatzprogramms TeX eingesetzt. Deshalb gewährleistet LyX qualitativ hochwertigen Satz. Weitere Stärken liegen in der Darstellung mathematischer Formeln und der Verwaltung von Fußnoten und Stichwortverzeichnissen.\n\nBeim Editieren des Dokuments ist die Ansicht optimiert für die Darstellung am Bildschirm. Ähnlich wie bei HTML orientiert sich die Länge der Zeilen an der Breite des Fensters, und ein Seitenumbruch erfolgt nicht am Bildschirm. Für strukturelle Elemente des Dokuments verfolgt LyX eine Strategie des WYSIWYM.\n\nStrukturelle Elemente wie Überschriften oder Aufzählungen werden so formatiert, dass sie als solche erkennbar sind, jedoch nicht notwendigerweise so, wie sie im gedruckten Dokument erscheinen. So kann zum Beispiel für die Überschriften eine für den Bildschirm optimierte Schrift gewählt werden, ohne dass das Einfluss auf das gedruckte Ergebnis nimmt.\n\nFür die Erfassung des Dokuments stehen bei den meisten Elementen sowohl ein grafischer Zugang über die Maus als auch einer über die Tastatur zur Verfügung. Insbesondere werden viele Befehle der LaTeX-Syntax erkannt und automatisch in eine passende grafische Darstellung umgesetzt.\n\nNicht erkannte Befehle werden bei der Ausgabe unverändert an LaTeX weitergegeben. Auf diese Weise ist es möglich, sämtliche Möglichkeiten von LaTeX zu nutzen. Im Editierfenster werden diese Befehle in roter Schrift angezeigt. Diese Anzeige läuft allerdings dem Projektziel entgegen, im Editierfenster keine Befehle anzuzeigen. Daraus entwickelte sich projekt-intern, halb scherzhaft, die Bezeichnung \"Evil Red Text\", \"böser roter Text\", abgekürzt ERT.\n\nLyX wurde 1995 vom damaligen Informatikstudenten Matthias Ettrich im Rahmen seiner Studienarbeit mit dem GUI-Toolkit Motif entwickelt. Als er nach Beendigung der Studienarbeit im Internet nach Mitstreitern suchte, um LyX gemeinsam weiterzuentwickeln, musste er feststellen, dass nur wenige Erfahrungen oder Zugang zu Motif hatten. Daher suchte er nach einem neuen, freien GUI-Toolkit. Die Wahl fiel schließlich auf das Toolkit XForms. In späteren Jahren wurde das \"GUI Independence Framework\" eingeführt, um zu erreichen, dass neben XForms auch noch weitere GUI-Toolkits (Qt, Gtk) verwendet werden können. In aktuellen Versionen wird nur noch das GUI-Toolkit Qt unterstützt.\n\n\n"}
{"id": "682903", "url": "https://de.wikipedia.org/wiki?curid=682903", "title": "Argus (Prozessrechner)", "text": "Argus (Prozessrechner)\n\nArgus ist die Bezeichnung einer Prozessrechnerfamilie des britischen Herstellers Ferranti Limited. \n\nDie Argus Rechner wurden etwa von 1960 bis 1995 gebaut und sind teilweise bis zum heutigen Tag in Betrieb. Die Argus Rechner wurden mit der Bezeichnung Argus 100, Argus 200, etc. bis zum Argus 700 gebaut. Das bekannteste Modell ist zweifellos der Argus 700 der bereits Ende der 1970er Jahre als Multiprozessorsystem installiert wurde. Das OSC245 Betriebssystem der Argus Rechner unterstützt Multiuser, Multitasking und Multiprozessor Eigenschaften.\n\nArgus 700 Prozessrechner wurden stetig weiter entwickelt. Angefangen bei den E/T Typen in TTL-Technik ging es über den ARGUS 700S in ECL-Technik weiter zu dem G/GC-Typ. Letzterer unterstützte bereits den Einsatz von CPU-Caches. Mit den GL, GX und zwei unterschiedlich schnellen GZ-Typen erschienen immer schnellere Prozessoren im Laufe der 1980er und frühen 1990er Jahre. Nicht zu vergessen ist ein weiterer Vertreter der ARGUS 700 Serie, ausgelegt für den VME-Bus, der ARGUS 700GV. Vom Argus 700 wurden mehr als 1500 Systeme weltweit installiert.\n\n"}
{"id": "683576", "url": "https://de.wikipedia.org/wiki?curid=683576", "title": "Computer-aided office", "text": "Computer-aided office\n\nCAO (von engl. computer-aided office) steht für computerunterstützte Verwaltung und ist ein Teil des \"computer-integrated manufacturing\".\n\n\nUm die erwähnten Aufgaben erfüllen zu können, muss die computergestützte Verwaltung (CAO) mit den einzelnen Bereichen der computergestützten Produktion (CIM) zusammenarbeiten.\n"}
{"id": "684551", "url": "https://de.wikipedia.org/wiki?curid=684551", "title": "CORDIC", "text": "CORDIC\n\nDer CORDIC-Algorithmus () ist ein effizienter iterativer Algorithmus, mit dessen Hilfe sich viele Funktionen implementieren lassen, wie z. B. trigonometrische Funktionen, Exponentialfunktion und Logarithmen sowie auch die einfache Multiplikation oder Division.\n\nIn der Rechentechnik, vornehmlich in der digitalen Signalverarbeitung, benötigt man schnelle Verfahren für die Berechnung von bspw. trigonometrischen Funktionen. Herkömmliche Reihenentwicklungen wie z. B. die Taylorreihe zeigen oft nur mittelmäßige (d. h. langsame, oder gar von den Argumenten abhängige) Konvergenz und schlechte numerische Stabilität. Eine Reihenentwicklung besteht außerdem hauptsächlich aus einer Summe von Produkten, die nur aufwendig zu berechnen sind.\n\nDer CORDIC-Algorithmus wurde 1959 von Jack E. Volder präsentiert. In der ursprünglichen Version war es damit möglich, trigonometrische Funktionen wie Sinus, Cosinus und Tangens sowie die Multiplikation und Division von Zahlen allein durch die in digitalen Schaltungen einfach realisierbaren Additionen und Schiebeoperationen (engl. ) zu bilden. Schiebeoperationen zur Zahlenbasis 2 sind in digitalen Schaltungen sehr leicht durch entsprechende Verschaltung realisierbar.\n\nVolders Motivation war der Ersatz der üblichen und fehleranfälligen analogen Navigationsrechner in Convair-B-58-Bombern durch digitale Rechner zur genauen Positionsbestimmung. Die Anforderung war die Positionsberechnung der mit Überschallgeschwindigkeit fliegenden Bomber in Echtzeit über einer vereinfacht als kugelförmig angenommenen Erdoberfläche.\n\nMitte der 1960er Jahre wurde der CORDIC-Algorithmus auch in zivilen Anwendungen eingesetzt. Vorläufer der heutigen Taschenrechner wie der Tischrechner 9100 von Hewlett-Packard aus dem Jahr 1968 setzten ihn zur Berechnung der trigonometrischen Funktionen ein.\n\nErst 1971 jedoch wurde der CORDIC-Algorithmus von J. S. Walther auf die heute übliche Form erweitert und damit auch die effiziente Berechnung von Logarithmen, der Exponentialfunktion und der Quadratwurzel in digitalen Schaltungen möglich.\n\nCORDIC-Algorithmen werden zur Berechnung der wichtigsten Elementarfunktionen in Mikrocontroller-Rechenwerken wie Taschenrechnern eingesetzt. So findet sich auch in arithmetischen x87-Koprozessoren von Intel der CORDIC-Algorithmus zur Berechnung mathematischer Operationen. Weitere Anwendungsbeispiele liegen in der Nachrichtenübertragung. Damit lassen sich beispielsweise effizient Betrag und Phase eines komplexen Signals bestimmen.\n\nDa Multiplizierwerke vor allem in digitalen Schaltungen umfangreich und damit teuer zu realisieren sind, wird CORDIC oft genau da eingesetzt, wo Multiplizierer nicht effizient verfügbar sind. Dies umfasst vor allem den Bereich der digitalen Schaltungstechniken wie FPGAs oder ASICs.\n\nCORDIC ist zwar nicht der schnellste Algorithmus, wird aber wegen seiner Einfachheit und Vielseitigkeit oft eingesetzt.\n\nCORDIC kann man im formula_1, aber auch nur in der zweidimensionalen Ebene betrachten. Im Folgenden umfasst die Beschreibung den einfacheren, zweidimensionalen Fall.\n\nDreht man ein Koordinatensystem um den Winkel formula_2, erscheint der Vektor formula_3 um den Winkel formula_2 gedreht; sein Endpunkt liegt im neuen System bei formula_5 und formula_6.\n\nDie Rotation um den Winkel formula_2 entspricht dem Matrix-Vektor-Produkt:\n\nD. h., um auf den eigentlichen Funktionswert zu kommen, muss der Einheitsvektor formula_3 um formula_2 gedreht werden. Dies lässt sich leichter bewerkstelligen, wenn innerhalb der Transformationsmatrix nur noch eine Abhängigkeit von \"einer\" Winkelfunktion, z. B. formula_11 besteht:\n\nDie Drehung um formula_2 wird trickreich realisiert als Linearkombination von Teildrehungen um geschickt gewählte Teilwinkel formula_14.\n\nEine zu weite Drehung im Schritt formula_16 wird kompensiert durch einen Vorzeichenwechsel formula_17. Das gezeigte Verfahren konvergiert und ist numerisch stabil für alle formula_2, die sich aus obiger Summe ergeben können. Man führt nun noch eine Hilfsvariable formula_19 ein, die für den Drehsinn Verantwortung trägt:\n\nWenn nur einfachste Bauteile verwendet werden sollen und daher keine Multiplizierer vorhanden sind, muss man alles über Schiebe- und Addieroperationen bewerkstelligen. Dieses wird erreicht durch den Ansatz\n\nMan erhält damit den folgenden Algorithmus:\n\nmit dem Skalierungsfaktor formula_24 0,60725... für formula_25, der während der Initialisierungsphase implizit berechnet wird.\n\nVorweg wird eine Tabelle formula_29 fester Länge formula_30 angelegt mit formula_31, wobei formula_32 ist. Die folgenden Werte sind: formula_33 mit formula_34. (Die Werte des Arcustangens lassen sich mit der hier gut konvergierenden Potenzreihenentwicklung bestimmen.)\n\nDie Länge formula_30 der Tabelle bestimmt die erreichbare Genauigkeit. Führt man alle Drehungen eines Einheitsvektors formula_36 mit den so berechneten Werten hintereinander in gleichem Drehsinn aus, erzielt man eine Gesamtdrehung von etwas mehr als formula_37. Der Skalenfaktor formula_38 wird mit einem Aufruf im Vektormodus (s. u.) berechnet, indem man die Verlängerung des Einheitsvektors formula_39 ohne Skalierung berechnet.\n\nDer Ausgangsvektor formula_40 wird in jedem der Schritte so gedreht, dass der Winkel formula_41 gegen Null geht. Es werden stets alle formula_30 Teildrehungen ausgeführt, mit ggf. wechselndem Vorzeichen. Da der Kosinus eine gerade Funktion ist, spielt das Vorzeichen bei der Skalierung keine Rolle. Nach Reskalierung sind die Komponenten des erhaltenen Endvektors formula_43 und formula_44. Der Konvergenzbereich ergibt sich zu formula_45, also bei genügend großem formula_46 etwa zu formula_47, d. h., er erstreckt sich über mehr als den vierten und ersten Quadranten.\n\nDer vorgegebene Vektor, dessen Polarkoordinaten gesucht werden, wird immer so gedreht, dass sich der Betrag seiner formula_48-Komponente verringert. Die Drehwinkel formula_2 werden dabei vorzeichenrichtig addiert. Die formula_50-Komponente des Endvektors ist nach Reskalierung der Betrag des Ausgangsvektors. Dieser Modus wird auch benutzt zur Berechnung des Arcustangens aus zwei Argumenten, Start mit formula_51. Der Konvergenzbereich ist derselbe wie oben.\nAus formula_52 lassen sich die Funktionen formula_53 und formula_54 unter Zuhilfenahme von formula_55 leicht ableiten.\n\nDer Startvektor formula_39 bzw. formula_57 entspricht einer Vorwegdrehung von formula_58 bzw. formula_59 (für den Rotationsmodus). Bei einem Startvektor mit negativer formula_50-Komponente im Vektormodus bewirkt man entsprechende Drehungen durch Vertauschen der Komponenten und Änderungen der Vorzeichen.\n\nDie oben benutzten Iterationsformeln\nsind ein Sonderfall der allgemeineren Vorschrift\nmit formula_63 und formula_64 sowie formula_65.\n\nFür formula_66, formula_67 und formula_68 erhält man\nwomit sich Multiplikation und Division durchführen lassen. Eine Tabelle formula_70 erübrigt sich hier.\n\nformula_71, formula_72 ergibt im Rotationsmodus (formula_19 gegen 0) formula_74 für alle formula_75, formula_76 ergibt im Vektormodus (formula_48 gegen 0) formula_78 für alle formula_79 werden die Hyperbelfunktionen, ihre Umkehrungen (Areafunktionen), Exponentialfunktion und Logarithmus sowie die Quadratwurzel berechenbar. Einheitskreis bzw. -hyperbel werden durch formula_80 mit formula_81 bzw. formula_82 beschrieben. Das zu einem Vektor formula_83 gehörende Winkel- bzw. Areaargument ist durch formula_84 gegeben, also\n\nformula_82, hyperbolische Fkt.:\n, hier formula_89; formula_90; und wegen formula_91 auch formula_92.\n\nDas Verfahren ist analog zu dem eingangs gezeigten für die Winkelfunktionen. Erforderlich sind nur eine weitere Tabelle mit formula_93, formula_94 und die einmalige Berechnung des Skalenfaktors formula_95.\n\nDie Iterationen formula_97 müssen immer wiederholt werden, da der Areatangens hyperbolicus nicht die Bedingung formula_98 erfüllt, das somit für die Reihe formula_99 nicht konvergieren würde.\n\nRotation mit formula_100 liefert:\n_formula_101,\n\ndavon abgeleitet:\n\nVektormodus mit formula_104 berechnet:\nund den hyperbolischen Betrag\n\ndavon abgeleitet:\nsowie\naus dem Betrag des Startvektors formula_109\n\nDer Konvergenzbereich ist in beiden Modi beschränkt durch die maximal mögliche Änderung von formula_19. Alle mathematisch erlaubten Argumente können jedoch durch einfache Umstellungen und Shift-Operationen auf ihn abgebildet werden.\n\nAls Alternativen kommen hauptsächlich schnelle Tablelookup-Verfahren, wie z. B. in DSPs, und Bitalgorithmen, die mit einem ähnlichen Ansatz wie CORDIC die Berechnung vornehmen, in Frage.\n\n\n"}
{"id": "686747", "url": "https://de.wikipedia.org/wiki?curid=686747", "title": "Dashboard (Apple)", "text": "Dashboard (Apple)\n\nDashboard (\"engl.\" für Armaturenbrett) ist eine Technik, mit welcher sich Informationen übersichtlich darstellen lassen und die von Apple seit der Version Mac OS X Tiger des Betriebssystems Mac OS X genutzt wird, um Miniprogramme, genannt Widgets, darzustellen.\n\nDas Programm Dashboard kann mit einer Taste (üblicherweise ) ein- und ausgeblendet werden. Widgets können meist kostenlos aus dem Internet heruntergeladen werden. Dashboard ließ sich bis OS X Yosemite normalerweise nur ausblenden, nicht ausschalten. Über zusätzliche Hilfsprogramme jedoch konnte man es auch ganz deaktivieren.\n\nSeit Mac OS X Lion wird das Dashboard standardmäßig als eigener \"Space\" in Mission Control angezeigt. Jedoch lässt es sich optional wie in früheren Versionen als Überlagerung über dem aktuellen Bildschirminhalt einblenden.\n\nSeit der Einführung von Widgets für die Mitteilungszentrale in OS X Yosemite ist das Dashboard standardmäßig deaktiviert, jedoch kann man es in den Einstellungen aktivieren.\n\nEs gibt drei Möglichkeiten, Widgets zu programmieren. Die am weitesten verbreitete Form basiert auf HTML, CSS und JavaScript. Mit Mac OS X Leopard wurde Dashcode eingeführt, ein Programm, welches das Erstellen von Widgets erheblich erleichtern soll. Weniger verbreitet ist die Möglichkeit, Widgets mittels Cocoa zu erstellen, welche dem Entwickler mehr Eingriffsmöglichkeiten ins System eröffnet. Es gibt z. B. Kalender, Rechner, Wörterbücher, Wettervorhersagen und Spiele.\n\n\n"}
{"id": "689653", "url": "https://de.wikipedia.org/wiki?curid=689653", "title": "Hough-Transformation", "text": "Hough-Transformation\n\nDie Hough-Transformation (Sprechweise []) ist ein robustes globales Verfahren zur Erkennung von Geraden, Kreisen oder beliebigen anderen parametrisierbaren geometrischen Figuren in einem binären Gradientenbild, also einem Schwarz-Weiß-Bild, nach einer Kantenerkennung. Das Verfahren wurde 1962 von Paul V. C. Hough unter dem Namen „Method and Means for Recognizing Complex Patterns“ patentiert.\n\nZur Erkennung von geometrischen Objekten wird ein Dualraum erschaffen (speziell: Parameterraum, Hough-Raum), in den für jeden Punkt im Bild, der auf einer Kante liegt, alle möglichen Parameter der zu findenden Figur im Dualraum eingetragen werden. Jeder Punkt im Dualraum entspricht damit einem geometrischen Objekt im Bildraum. Bei der Geraden kann das z. B. die Steigung und der y-Achsen-Abschnitt sein, beim Kreis der Mittelpunkt und Radius.\nDanach wertet man den Dualraum aus, indem man nach Häufungen sucht, die dann der gesuchten Figur entsprechen.\n\nBei der Geradenerkennung mittels der Hough-Transformation muss man zuerst geeignete Parameter für eine Gerade finden. Steigung und y-Achsen-Abschnitt eignen sich nur bedingt, da zur y-Achse parallele Geraden eine \"unendliche\" Steigung haben und daher im (für die Berechnung zwangsläufig) \"endlichen\" Parameterraum nicht mehr abgebildet werden können. Dieses Problem kann man umgehen, wenn man eine zweite Hough-Transformation auf dem um 90° gedrehten Bildraum durchführt, was aber recht umständlich ist. In der neueren Literatur überwiegt daher der Ansatz, Geraden durch ihre hessesche Normalform zu repräsentieren. Als Parameter wählt man den Winkel formula_1 und den (euklidischen) Abstand d, wobei formula_1 der Winkel zwischen der Normalen der Gerade (= Lot) und der x-Achse ist, und formula_3 den Abstand vom Ursprung zum Lotfußpunkt auf der Gerade bezeichnet.\n\nDamit haben wir die Parametergleichung formula_4, mit der wir für alle Punkte auf Kanten im Bild die entsprechende Kurve im Dualraum einzeichnen. Dabei bezeichnen formula_1 und formula_3 die Variablen, während x und y jetzt zu Parametern umfunktioniert wurden. x und y sind die Koordinaten der vorher detektierten Kantenpunkte. Das Ausgangsbild wird zunächst einem Kantendetektor-Algorithmus unterzogen (z. B. Canny- oder Sobel-Filter) und dadurch der zu untersuchende Punktraum auf mögliche Kanten eingeschränkt.\n\nDer Dualraum wird nun also von formula_1 und formula_3 aufgespannt. Zu jedem errechneten Wert formula_3 wird jetzt im Dualraum (repräsentiert als Matrix) an der Stelle formula_10 der Wert um 1 erhöht, also quasi für die dadurch repräsentierte Gerade „gevotet“. Deshalb nennt man die Matrix auch oft „Voting-Matrix“.\n\nDer nächste Schritt besteht in der Analyse des Dualraums, bei der man nach Häufungspunkten in der Voting-Matrix sucht. Diese Häufungspunkte im Dualraum repräsentieren mögliche Geraden im Bildraum, da sie offensichtlich unter dem gleichen Winkel formula_1 mit der gleichen Entfernung formula_3 vom Ursprung repräsentiert werden.\n\nAufgrund der Unabhängigkeit der einzelnen Zellen des Parameterraumes zueinander bei der Berechnung der Häufungspunkte ist die Hough-Transformation leicht parallelisierbar.\n\nEin einfacher Algorithmus, um eine Hough-Akkumulation durchzuführen, könnte etwa so aussehen:\nDas Resultat der Akkumulation ist ein zweidimensionales Array, welches die Häufigkeit des Auftretens für jede Kombination aus formula_1 und formula_3 enthält. Weil die Geraden nicht gerichtet sind, muss nur der Bereich von 0 bis formula_16 ausgewertet werden, da sich danach die Geraden mit den bereits berechneten decken. In der Praxis wird häufig die Bildmitte als Koordinatenursprung verwendet.\n\nWie oben erwähnt kann die Hough-Transformation – in abgewandelter Weise – nicht nur für das Detektieren von Geraden, sondern z. B. auch von Kreisen eingesetzt werden. Ausgehend vom Kantenbild wird jedes Kantenpixel als von Kreisen eines bestimmten Radius erzeugt angesehen. Die Transformation in den Hough-Raum funktioniert so, dass man im Akkumulator alle Kreismittelpunkte einträgt, die zu diesem Kantenpixel führen könnten (jedes Akkumulatormittelpunktpixel um 1 erhöhen). Wenn nun die Punkte im Kantenbild einen Kreis repräsentieren, ist in der Akkumulatormatrix ein besonders hoher Wert an der dazugehörigen Stelle des Kreismittelpunkts eingetragen, da dort sehr viele Kantenpixel des Kreises für den Mittelpunkt abgestimmt haben. Die Maxima im Hough-Raum repräsentieren also die Kreismittelpunkte.\n\nDie ersten zwei Dimensionen des Hough-Raums entsprechen in diesem Fall denen des Bildraums, da die (x,y)-Koordinaten in beiden Fällen die Lage des Kreismittelpunktes beschreiben. Zusätzlich dazu ist laut der Kreisgleichung formula_23 der Radius r der dritte Parameter, der beachtet werden muss. Man spricht bei Kreisen daher von einem 3-dimensionalen Hough-Raum (xc, yc, r). Die Wertegrenzen für den Radius müssen fest vorgegeben werden (z. B. mittels A-priori-Wissen).\n\nFür Ellipsen und jede andere durch eine parametrische Gleichung darstellbare Form kann dieses Verfahren ebenfalls angewendet werden. Jedoch steigt die Dimension des Hough-Raums mit der Variablenzahl (bei Ellipsen: 5), was den Rechenaufwand deutlich steigert.\n\nEs ist ebenfalls möglich, eine nicht durch parametrische Repräsentation darstellbare Struktur zu finden. Dieses Verfahren wird generalisierte Hough-Transformation (GHT) genannt. So können beliebige Formen in einem Bild gefunden werden. Das Prinzip hierbei ist, dass man eine formabhängige Lookup-Tabelle errechnet. In dieser sind die möglichen Vektoren zu einem Referenzpunkt den unterschiedlichen Gradientenrichtungen zugeordnet. Durch einige Umformungen der Tabelle kann man auch rotierte bzw. skalierte Versionen der gesuchten Formen finden, was zu einer hohen Robustheit führt. Mittels der Lookup-Tabelle kann man nun das Kantenbild in den Hough-Raum überführen; Maxima stellen die gefundenen Referenzpunkte dar.\n\n\nIn freien Software-Bibliotheken zur Bildverarbeitung wie Scikit-image, Dlib und OpenCV ist die Hough-Transformation enthalten.\n\n\n\nApplets zur Visualisierung der Hough-Transformation:\n"}
{"id": "690836", "url": "https://de.wikipedia.org/wiki?curid=690836", "title": "PC Conectado", "text": "PC Conectado\n\nDer PC Conectado (port. \"verbundener PC\") ist ein Computer, der in großem Stil in die Haushalte Brasiliens eingeführt wird. Besonders ärmere Haushalte sind Ziel dieses langfristig angelegten Programmes.\n\nSeit dem Jahr 2004 gibt es in der brasilianischen Regierung Pläne, den Anteil der Bevölkerung mit Zugang zum Internet deutlich zu erhöhen. Dabei sind gewisse Eckpunkte festgesetzt worden: Es soll die Piraterie von Software gesenkt werden, die lokale Hardwareindustrie muss eingebunden werden, und die technische Ausbildung sowohl bei Nutzern als auch potentiellen Betreuern soll erhöht werden.\n\nAm 12. Mai 2005 wurde schließlich die Vorabentscheidung bekanntgegeben. Die Einführung wird größtenteils in der zweiten Hälfte des Jahres ablaufen.\n\nDer PC Conectado ist als Programm im Rahmen der digitalen Einbindung anzusehen, die zusammen mit der sozialen Einbindung eines der wichtigsten Regierungsprogramme überhaupt repräsentiert, vergleichbar vielleicht mit der Agenda 2010 in Deutschland.\n\nEs werden über eine Million PC Conectado produziert werden. Die Preise werden maximal 1400 R$ betragen, was etwa 400 Euro entspricht, allerdings eher als Vielfaches des staatlich garantierten Minimallohnes gesehen werden sollte.\n\nUm Anbietern die Möglichkeit zu geben, sich als Produzent des PC Conectado vorzubereiten, wurden im Vorfeld die technischen Merkmale festgelegt.\n\nEs wurde beschlossen, dass der Computer einen x86-Prozessor mit 1,5 GHz besitzen soll, dazu 128 MB Arbeitsspeicher und eine Festplatte von 40 GB. Modem, Netzwerkkarte und Grafikkarte gehören ebenfalls zur Ausstattung. Die Monitorgröße beträgt 15 Zoll. Zur Einführung gehören etliche Freistunden für den Internetzugang.\n\nEs sind 26 Applikationen als notwendig erachtet worden. Des Weiteren wurde entschieden, dass ausschließlich freie Software die Bedürfnisse der Nutzer und der lokalen Wirtschaft erfüllt.\nDie Softwareliste besteht aus den folgenden Einträgen:\n\nEine technologische Umwälzung großer Teile der Gesellschaft, wie sie im Falle des PC Conectado gedacht ist, läuft nicht ohne Diskussionen ab.\n\nAuch wenn die Ausschreibung nicht öffentlich lief, gab es doch einige Informationen über potentielle Teilnehmer.\nMicrosoft versuchte, über eine spezielle Version des Betriebssystems Windows den Zuschlag zu bekommen. Diese Version sollte sehr preiswert angeboten werden, wäre aber von der Funktionalität her völlig unbrauchbar gewesen, da es künstliche Beschränkungen auferlegte.\nSun Microsystems bot an, ein Konzept auf Basis von Thin Clients auszuarbeiten. Dies hätte eine größere Wahlfreiheit im Bereich der Software gelassen, aber auch wenn die Hardwareplattform die Verbreitung von Schwarzkopien sehr eingeschränkt hätte, wäre die Firma selbst und nicht die lokalen Hardware-Hersteller im Vorteil gewesen.\nDer dritte Weg, der auch eingeschlagen wurde, lag in der Verwendung von quelloffener Software auf Basis von preiswerten Standard-Computern.\n\nZur Unterstützung des dritten Weges hat sich eine Gruppe namens \"PC Livre\" gebildet, welche aus Mitgliedern des brasilianischen Projektes ASL (Associação Software Livre) hervorging. Diese stellt sich die Aufgabe, Informationen und Dokumentation zu den eingesetzten freien Programmen für die anvisierte Zielgruppe aufzubereiten.\n\nBereits im Vorfeld kamen Befürchtungen auf, dass proprietäre Softwarehersteller die Wahl torpedieren würden. Die Einführung des Rechners wird also begleitet von Beobachtungen zur Verhinderung von Lobbyismus seitens der Firmen.\n\nTechnisch ist vielen die Auswahl von 128 MB Arbeitsspeicher ein Dorn im Auge, sind doch die gewünschten Desktop-Umgebungen wie KDE oder GNOME, die die Anforderungen an Mehrsprachigkeit und Benutzerfreundlichkeit erfüllen, erst mit etwa der doppelten Speichermenge gut nutzbar.\n\n"}
{"id": "693192", "url": "https://de.wikipedia.org/wiki?curid=693192", "title": "Indizierte Farben", "text": "Indizierte Farben\n\nIn der Computergrafik bezeichnet man mit indizierten Farben eine Methode zur Speicherung einer Rastergrafik. Bei indizierten Farben enthält die Datenstruktur jedes Pixels nicht direkt die einzelnen Farbwerte, sondern nur einen Index auf einen Eintrag einer sogenannten Farbtabelle oder Farbpalette, die die im Bild verwendeten Farben auflistet. Ein Pixel speichert hierbei – anstelle des Farbwerts – die Nummer des Tabelleneintrags, die diesen Farbwert enthält. Bilder, die nur wenige unterschiedliche Farben enthalten, sparen durch die geringe Größe des Farbindex Speicherplatz ein. Insbesondere einfache Grafiken oder Diagramme lassen sich so platzsparend speichern. Indizierte Farben finden sowohl bei diversen Grafikformaten als auch bei alten Grafikstandards, etwa EGA und VGA, Verwendung.\n\nIn der Farbtabelle werden alle im Bild verwendeten Farben aufgelistet, wobei jeder Eintrag einen Farbwert enthält. Die Farbtabelle wird von den Pixeldaten getrennt gespeichert. Der Speicherplatz, den der Index eines Pixels benötigt, wird in Bit pro Pixel (bpp) angegeben und ist ein Maß für die so genannte Farbtiefe des Bildes. Die Farbtiefe begrenzt die maximale Anzahl der verwendbaren Tabelleneinträge; bei einem Bild mit durch formula_1 bpp indizierte Farben ergibt sich eine maximale Farbtabellengröße von formula_2 Einträgen.\n\nIn der Praxis werden nur maximal 8 bpp verwendet, entsprechend einer Tabellengröße von 256 Farben. Theoretisch sind auch Farbtabellen mit mehr als 8 bpp möglich. Allerdings nimmt die Speicherersparnis mit zunehmender Größe der Farbentabelle ab; ab einer bestimmten Größe wird der Speicherbedarf insgesamt sogar größer als mit direkt angegebenen Farben.\n\nDie Anzahl der Tabelleneinträge ist unabhängig vom Format, mit dem die Farbwerte letztlich in den Tabelleneinträgen repräsentiert werden; für diese sind beliebige Farbräume denkbar. In der Praxis werden entweder 8-Bit-Graustufenwerte oder je 8 Bit für Rot, Grün und Blau (RGB), eventuell zusätzlich noch ein Alpha-Wert zur Angabe der Transparenz, verwendet. Gelegentlich wird mit dem Begriff „Farbtiefe“ auch der von einem Farbwert eingenommene Speicherplatz bezeichnet; diese Bedeutung ist von der Farbtiefe im Sinne des für einen Farbindex verwendeten Speicherplatzes zu unterscheiden.\n\nOft wird die Farbanzahl einer Rastergrafik künstlich auf 256 oder weniger Farben verringert, um vom geringeren Speicheraufwand der indizierten Farben zu profitieren. Diese Methode, bei der die für das Bild repräsentativsten Farben ermittelt werden müssen, nennt man Farbreduktion.\n\nDas obige Schema stellt die Funktionsweise der Farbtabelle am Beispiel eines 5×5 Pixel großen Bildes mit 2 bpp, entsprechend 2=4 Tabelleneinträgen, dar. Jedes Pixel enthält einen Index auf die Farbtabelle, die den jeweiligen Farbwert definiert. Auf der rechten Seite ist das gleiche Bild ohne Farbtabelle zu sehen; die Farbwerte sind hier direkt in den Pixeln gespeichert.\n\nWenn man davon ausgeht, dass für dieses Bild eine Farbe durch drei RGB-Werte zu je 8 Bit repräsentiert wird,\n\n\n\nAls Color Lookup Table (CLUT, siehe auch Lookup-Tabelle) bezeichnet man den Teil der Grafik-Hardware, der die indizierten Farben des Framebuffers in normale Farbwerte umwandelt, um sie auf einem Bildschirm ausgeben zu können. Die CLUT ist somit eine mittels Hardware realisierte Farbtabelle. Bei den ersten Grafikkarten war die CLUT noch ROM-artig fest verdrahtet, heute handelt es sich dabei normalerweise um einen schnellen Speicher oder Registersatz in der Grafikhardware, der die aktuell verwendete Farbtabelle enthält.\n\nDie meisten Grafikformate, die das Format einer gespeicherten Grafikdatei definieren, unterstützen verschiedene Farbtiefen, darunter sowohl solche ohne Palette als auch solche mit Palette. In letzteren gibt es dann einen eigenen Abschnitt für die Farbtabelle, der dann gelegentlich auch „(Color) Look-Up Table“ oder ähnlich genannt wird.\n\nDa die aktuelle Farbtabelle einer Hardware-CLUT verändert werden kann, sind durch bewegte Farbverläufe sehr schnelle und einfache Grafikeffekte möglich \"(Farbtabellenrotation, Color Cycling).\" Dabei wird typischerweise ein Unterbereich der Farbpalette mit einer konstanten Wiederholrate zyklisch umbelegt, d. h. „durchgerollt“. Dadurch lassen sich insbesondere Effekte wie etwa bewegtes Wasser erzielen. Der Aufwand für den Prozessor ist dabei sehr gering, da er von einer zur nächsten Phase nur die Inhalte von wenigen Farbregistern in der Grafikhardware umbelegen muss und auf den viel größeren eigentlichen Grafikspeicher überhaupt nicht zuzugreifen braucht. Entsprechend war dieser Ansatz besonders in der Frühzeit der Heimcomputer verbreitet, als noch keine so große Rechenleistung zur Verfügung stand.\n\nDie Farbtabellenrotation war bereits in SuperPaint, einem Mitte der 1970er Jahre entwickelten Malprogramm, möglich. Das Amiga-IFF-Dateiformat ist besonders geeignet für die Speicherung komplexer Farbtabellenrotations-Effekte innerhalb von Grafikdateien und Animationen. Die dynamisch wechselnden Palettenbereiche sowie die Zeitparameter sind in eigenen Daten-„Chunks“ (CCRT, CRNG oder DRNG) definiert. Diese Chunks unterstützen die asynchrone Rotation mehrerer Palettenbereiche in unterschiedlichen Geschwindigkeiten und Richtungen. Diverse Atari-Malprogramme wie zum Beispiel NeoChrome erlauben Farbtabellenrotations-Effekte für einen bestimmten Palettenbereich. Die Startbildschirme von Microsoft Windows 95 und 98 verwendeten ebenfalls Farbtabellenrotation. Hier war das Bild in einer Datei im BMP-Format abgelegt; ein Eintrag im Header bestimmte die Anzahl der zu rotierenden Paletteneinträge.\n\nAuf heute üblichen Rechnern werden derartige Möglichkeiten aber kaum genutzt, da moderne Grafikkarten im Echtfarbenmodus arbeiten und schnell genug sind, die gleichen Effekte auch in dieser Darstellungsart zu erzielen.\n\n"}
{"id": "694135", "url": "https://de.wikipedia.org/wiki?curid=694135", "title": "Hewlett-Packard 3000 Serie", "text": "Hewlett-Packard 3000 Serie\n\nDie Hewlett-Packard-3000-Serie ist eine Familie von Minicomputern. Markteinführung war nach einer schwierigen Entwicklungsphase Ende 1972; das erste Modell wurde in Amerika wieder zurückgezogen, Ende 1973 nach Überarbeitungen wieder freigegeben und Ende 1974 durch einen Nachfolger ersetzt. Über den genauen Zeitablauf gibt es unterschiedliche Informationen.\n\nDie HP3000 sollte der erste Minicomputer mit einem vollständigen Time-Sharing-Betriebssystem werden und an den großen Erfolg der Nur-Basic-Timesharingsysteme anknüpfen. Die Architektur beruhte im Gegensatz zu den üblichen Register-Architekturen auf einer Stack-Architektur.\n\nDie ersten Rechner gehörten zum System der Mittleren Datentechnik und basierten auf einem 16-Bit-CISC-Prozessor. Ab 1988 wurde die PA-RISC-Architektur mit 32-Bit-Adressierung eingeführt. Die Hardware ist identisch mit der HP-9000-HP-UX-Familie.\n\nDas Betriebssystem der HP3000 heißt MPE, mit Einführung der PA-RISC Architektur MPE XL und später MPE/iX um die Interoperabilität mit Unix anzuzeigen. Programme, die auf den ersten Modellen mit 16-Bit-CISC-Prozessor kompiliert wurden, sind auch auf den neuesten Modellen PA-RISC-Architektur im \"Copatability Mode\" unverändert lauffähig. MPE beinhaltet das Datenbanksystem TurboIMAGE, das eigentliche Erfolgsrezept der HP3000, das Maskensystem Vplus sowie COBOL, Fortran, Pascal, Basic und C-Compiler. \nIm Jahr 1999, mit der Implementierung von WEB-Funktionalitäten, wurde das System von „HP3000“ auf „HPe3000“ umbenannt.\nIm November 2001 wurde das System/der Support zunächst zum Ende 2006 vom Hersteller Hewlett-Packard abgekündigt (End-of-Support), dann aber nochmals bis Ende 2008 verlängert. Die Produktion und der Verkauf von Neusystemen wurde Ende 2003 eingestellt.\n\nDie Wartung wurde bis zum 31. Dezember 2012 fortgesetzt.\n\nAm 1. Januar 2027 werden die Bits im Datenformat des Kalenders auf den Rechnern der 3000er-Serie von Hewlett-Packard aufgebraucht sein. Seit Dezember 2015 besteht keine Unterstützung des Herstellers mehr.\n\n"}
{"id": "696052", "url": "https://de.wikipedia.org/wiki?curid=696052", "title": "Xojo", "text": "Xojo\n\nXojo (früher REALbasic und Real Studio) ist eine objektbasierte Programmiersprache mit Visuellen Entwicklungsumgebungen für macOS, Linux und Windows und basiert in ihren Befehlen und der Syntax auf der Programmiersprache BASIC.\n\nXojo ähnelt sehr stark Visual Basic 6, so dass es seither sehr beliebt bei Visual-Basic-6-Anhängern ist, die nicht zu den .NET-Versionen wechseln wollen. Xojo wird mit einer großen Bibliothek an Elementen ausgeliefert. Eine eigene Version der Entwicklungsumgebung für Linux ist seit 2005 verfügbar.\n\nXojo-Programmcode kann ohne Änderungen auch auf den jeweils anderen Plattformen (speziell Windows, Linux, OS X und Web) kompiliert werden und erzeugt dabei Stand-Alone-Anwendungen, Konsolen-Anwendungen oder auch Web-Anwendungen für das jeweilige x86- oder x64-System. Dabei wird keine zusätzliche Laufzeitumgebung benötigt, lediglich unter x64-Linux muss unter Umständen eine Reihe zusätzlicher Bibliotheken installiert sein, um Xojo und die damit kompilierten Programme lauffähig zu machen. Besagte Webanwendung basiert dabei auf CGI, hingegen die Standalone auf dem Cocoa/Mono-Framework. Für jeweils plattformspezifische Änderungen sind Direktiven implementiert, die es erlauben, den Quellcode entsprechend anzupassen. Bis zur Version 5.5.5 lief die IDE noch nativ auf Mac OS 9, nachfolgende Versionen konnten bis 2007 Release 3 noch Mac-OS-9-Programme kompilieren.\n\nAb der Version 2014r3 kann Xojo auch zur Entwicklung nativer iOS-Apps verwendet werden. Die Entwicklung dafür muss zwingend auf Mac OS X erfolgen, da zum Debuggen der iOS Simulator von Apple verwendet wird, der nur unter OS X zur Verfügung steht. Ab der Version 2015r1 erzeugt Xojo iOS-Apps als Universal Binary. Mit der Version 2015r3 ist es möglich, auch Anwendungen für den Raspberry Pi 2 Model B+ und ähnliche Einplatinencomputer mit 32 Bit-ARMv7-CPU zu kompilieren. Durch Integration des LLVM ist ebenfalls das Erzeugen von Stand-Alone-Programmen für 64 Bit-Systeme der unterstützten Desktop- und Webplattformen möglich geworden. Die Xojo-IDE selbst ist in Xojo programmiert.\n\nVersion 2017r1 brachte in erster Linie Debugging unter 64 bit für macOS und Linux und führte neu einen Remote Debugger für den Raspberry Pi ein, mit dem das Entwickeln von einem Desktop-Rechner und automatisches Hochladen und Debugging der Debug-Applikation auf dem Raspberry Pi vom Entwicklungsrechner aus möglich sind.\n\nSeit Version 2017r3 liegt die IDE als 64 Bit-Anwendung vor.\n\nSeit 11. März 2014 bietet Xojo die Xojo Cloud an, einen Xojo-eigenen integrierten Hosting-Service. Dieser Dienst ermöglicht Deployment Xojo-erstellter Web-Anwendungen auf einen Knopfdruck aus der IDE heraus. Das Deployment erfolgt dabei konfigurations- und wartungsfrei auf sicherheitsverbesserte Linux-Server des Providers Rackspace. Mehrstufige Sicherheitsmechanismen einschließlich intelligenter Firewall, Einbruchserkennung (Intrusion Detection System (IDS)) und Mandatory Access Controls (MAC) sowie Inter-Server-Kommunikation, um sich bei erfolgtem unerlaubtem Zugriff über die Art des Einbruchs zu informieren, schützen dabei sowohl Web-Anwendung als auch Server-Betriebssystem. Zur Benutzung der Xojo Cloud wird eine Xojo-Pro- oder -Web-Lizenz benötigt sowie der Abschluss eines Xojo Cloud-Subskriptionsplans.\n\nRealbasic wurde ursprünglich von dem US-Amerikaner Andrew Barry unter dem Namen CrossBasic entwickelt, damals jedoch als reine Macintosh-Software. Da CrossBasic einerseits sehr einfach zu bedienen war, andererseits aber durchaus professionelle Programme damit entwickelt werden konnten, entstand schnell eine große Nutzergemeinde. Aus dem ursprünglichen Hobby-Projekt (Barry war hauptberuflich Spiele-Entwickler) wurde Ende der 1990er Jahre eine kommerzielle Software. Der heutige Besitzer und CEO, Geoff Perlman, kaufte das Projekt und gründete in Austin (Texas) die Real Software Inc. Trotz einiger Anlaufschwierigkeiten ist Realbasic heute die führende BASIC-Entwicklungsumgebung auf dem Mac.\n\nSeit dem 4. Juni 2013 heißt die Entwicklungsumgebung und Programmiersprache Xojo. Die Entscheidung dazu erfolgte aus Marketinggründen, um jegliche Assoziative zu älteren Dialekten und deren Nachteile zu vermeiden.\n\nIm Januar 2016 erhielt Xojo den BIG Innovation Award der Business Intelligence Group für herausragende technologische Innovationsleistungen.\n\n\n\n"}
{"id": "698930", "url": "https://de.wikipedia.org/wiki?curid=698930", "title": "Windows Hardware Engineering Conference", "text": "Windows Hardware Engineering Conference\n\nDie Windows Hardware Engineering Conference (WinHEC) ist eine jährlich veranstaltete Messe und Wirtschaftskonferenz, auf der Microsoft seine Hardware-Pläne für die Windows-Plattform ausführt. Üblicherweise gehören Reden von Personen wie Bill Gates sowie Sponsoren wie z. B. Intel, HP oder Samsung zu den Veranstaltungen.\n\nAuf der WinHEC stellt Microsoft den Teilnehmern an jeweils mehreren Tagen in verschiedenen Vorträgen mit wechselnden Sprechern aktuelle und zukünftige Entwicklungen und Änderungen für das Windows-Betriebssystem vor, um gemeinsam die Bereitstellung neuer hardwaretechnischer Standards und Geräte voranzutreiben sowie deren Kompatibilität zu Windows zu ermöglichen.\n\nAls Hausmesse kann die WinHEC auch als Teil von Microsofts Marketingstrategie für die Windows-Plattform betrachtet werden. Den Sponsoren und weiteren Ausstellern wird die Möglichkeit gegeben, durch ihre Präsenz für ihr Unternehmen und ihre Produkte zu werben.\n\nZur Zielgruppe der WinHEC gehören laut Microsoft Vertreter von Hardwareherstellern für Windows-Computer, die ihre Geräte an die Windows-Architektur anpassen müssen, Entwickler und Tester von Treibersoftware, die an den Windows-Schnittstellen und Entwicklerwerkzeugen für Treiberprogrammierung interessiert sind, sowie Manager, die über technische Entwicklungen und die Unternehmensstrategie informiert werden.\n\n\n"}
{"id": "700587", "url": "https://de.wikipedia.org/wiki?curid=700587", "title": "AppleShare", "text": "AppleShare\n\nAppleShare war ein Produkt von Apple, das es Clientanwendungen eines Computers in einem Netz erlaubt, mit einem Server Dateien auszutauschen und von diesem angebotene Dienste zu nutzen. AppleShare kann im LAN über DDP verwendet werden, TCP/IP war mit dem Nachfolger AppleShare IP möglich. Mit AppleShare kann ein Nutzer auf Dateien, Anwendungen, Drucker und andere Ressourcen auf einem entfernten Server zugreifen.\n\nAlle Macintosh- und Mac-OS-Klone verfügen über Client-Unterstützung für AppleShare. Eine stark abgespeckte Version findet sich ab System 7.0 in Form des Personal File Sharing, das deutlich langsamer arbeitet als AppleShare und auch maximal 10 gleichzeitige Verbindungen erlaubt. Microsoft Windows NT Server und Novell NetWare bieten beide AppleShare-Server-Unterstützung, wenn auch zurzeit nur über AppleTalk. Client/Server-Unterstützung für AppleShare von anderen Anbietern gibt es für Windows for Workgroups, Windows 95 und Windows NT, sowie für Unix-Systeme.\n\nEin Client oder Server kann verschiedene AppleShare-Protokollvarianten implementieren. Das tatsächlich verwendete Protokoll wird vor einer Sitzung verhandelt.\n\nNach der Veröffentlichung von Mac OS X Server, das über die Funktionalität von AppleShare hinausgeht, wurde die Weiterentwicklung von AppleShare eingestellt.\n\n"}
{"id": "700637", "url": "https://de.wikipedia.org/wiki?curid=700637", "title": "Commodore SX-64", "text": "Commodore SX-64\n\nDer SX-64, auch als Executive 64 bezeichnet, war eine tragbare Version des C64. Er enthielt einen eingebauten 5\" (127 mm) großen Farbbildschirm und ein zur VC1541 kompatibles 5¼\"-Diskettenlaufwerk. Der Computer enthielt keinen Akku, sondern wurde am normalen Stromnetz betrieben, wog 10,5 kg und hatte laut Werbetext unter einem Flugzeugsitz Platz.\n\nDer SX-64 wurde ab Dezember 1983 zum Preis von 995,- US$ verkauft, in Deutschland bot man das Gerät ab Frühjahr 1984 für knapp 3.000 DM an. Das Gerät wurde lediglich im Zeitraum Frühjahr 1984 bis 1986 vertrieben. Entgegen hartnäckigen Gerüchten wurden weltweit mehr als 49.000 Exemplare produziert und vertrieben, was man an der fortlaufenden Seriennummer leicht erkennen kann: In der SN-Database auf sx64.net findet man einen SX-64 mit der Endnummer 049807.\nUrsprünglich plante Commodore ein Doppel-Diskettenlaufwerk für das Gerät ein, auf der Summer CES (Consumer Electronics Show) 1983 in Las Vegas stellte man daher einen entsprechend ausgestatteten Prototyp vor. Aus der anfangs gewählten Bezeichnung Commodore Double Drive Executive 64 entstand kurz DX-64. Außer diesem Prototyp konnte man noch kein verkaufbares Produkt nachweisen. \nAuf der Winter CES wurde ein fast serienreifer Prototyp vorgestellt, der nur noch ein Diskettenlaufwerk aufwies und den dementsprechend geänderten Namen SX-64 (Single Drive Executive 64) bekam. Das zweite Laufwerk, welches wahrscheinlich aus Kostengründen dem Rotstift zum Opfer fiel, sollte aber optional mitbestellt werden können. Es wurde ein weiterer Prototyp vorgestellt, der SX-100, ein weitestgehend gleiches Gerät, lediglich mit einem Schwarz-Weiß-Röhrenmonitor ausgestattet. Dieser kam aber über das Prototyp-Stadium nicht hinaus. \n\nDer DX-64 wird in dem beigefügten Handbuch zwar gelegentlich erwähnt, auch sieht man in einer Gerätebeschreibung eine Zeichnung eines Computers mit Doppellaufwerk, jedoch ist ein „echtes“ Serienmodell des DX-64 bis heute nicht belegt, so dass der Double Drive Executive 64 es wohl nie zur Marktreife gebracht hat, lediglich Eigenbauten lassen sich im Internet finden, leicht daran zu erkennen, dass das obere Laufwerk auf der Frontplatte die Bezeichnung STORAGE aufweist.\n\nTatsächlich wurde der SX-64 seinerzeit als „Vertreter-Laptop“ genutzt. Dank des Farbmonitors waren die Zeiten der Darstellung von einfarbigem Text vorbei, man konnte dem Außendienstmitarbeiter nun auch aufwändigere Grafiken mitgeben. Programme wie Multidata64, CalcResult64, Adressen64, Mitgliederverwaltung64 und Text64 zielten auf Büro-Anwendungen, Lagerverwaltungen und ähnliche Einsätze in Unternehmen.\nEs gab so manche professionelle Einsatzmöglichkeiten für das Gerät. Die Ampelanlagenfirma SILA nutzte den SX-64 zur Steuerung und Programmierung ihrer mobilen Einsatzampeln an Baustellen. Die Firma CIL Microsystems aus Sussex/England bot einen modifizierten SX-64 als Mess- und Steuercomputer für den technisch-wissenschaftlichen Bereich an, wobei der Schacht oberhalb des Laufwerks durch ein Messdateninterface (ADS-Interface) ausgetauscht wurde, das vier Analog-Eingänge, zwei Analog-Ausgänge, vier Digital-TTL- und vier Relais-Ausgänge beinhaltete. Die Firma Tesa schickte ihre Handelsvertreter mit speziell umgerüsteten Geräten namens \"Tesa Etikettendrucker 6240\" zu den Kunden. Der Monitor wurde gegen einen Grünmonitor getauscht, der SID, der im SX-64 für den Sound verantwortlich ist, wurde entfernt, ebenso die Regler für Lautstärke, Helligkeit, Kontrast und Farbe. Das ROM enthielt eine spezielle Tesa-Software.\n\nDie Tastatur des SX-64 war abnehmbar, hatte 66 Tasten in QWERTY-Anordnung und diente auch als Deckel des Computergehäuses. Sie wurde über ein Kabel mit zwei 25poligen, D-Sub-ähnlichen Steckern angeschlossen.\nDa man das Diskettenlaufwerk fest ins Gerät eingeplant hatte, ließen die Ingenieure den Datassettenport weg. Ebenso entfielen die nun nicht mehr benötigten Befehlsroutinen für das Tape im ROM. Das zog allerdings technische Probleme mit sich: Externe Peripherie, wie zum Beispiel das hauseigene parallele Centronics-Druckerinterface, das diesen Port als Spannungsversorgung nutzte, konnte hier nicht verwendet werden. Auch verweigerten einige Programme ihren Dienst, weil sie die fehlenden Datasetten-Routinen im Kernal als Fehler des Computers interpretierten. Ein weiteres Problem brachte das Netzteil mit sich. In frühen Versionen des SX-64 war es so knapp ausgelegt, dass Commodores eigene RAM Expansion Unit (REU) ihren Dienst verweigerte, weil sie nicht genug Energie vom Netzteil erhielt. Das schwache Netzteil ist auch der wahrscheinliche Grund, warum das zweite Laufwerk des ursprünglich vorgestellten Double Drive Executive 64 wegfiel: Die Energieversorgung würde zusammenbrechen, wenn beide Laufwerke gleichzeitig angesprochen werden. Allerdings wurde das Netzteil-Problem in den ersten Geräte-Revisionen korrigiert.\nGenauso wie der C64 verfügte der SX-64 über 64 KB RAM. Die Standardfarben für Schrift wurden in Blau auf weißem Grund geändert, um die Lesbarkeit auf dem kleinen Monitor zu verbessern.\n\nDer SX-64 hat auffallende Ähnlichkeit zum Osborne 1, der 2 Jahre vorher erschienen ist, und gewann dennoch einen Preis für die Industrieform.\nDer Begriff „Tragbarer Computer“ hat sich durch solche Geräte zum ersten Mal manifestiert.\nAuch wenn die batterielosen Computer im heutigen Sinne als nicht portabel gelten, waren sie die ersten mobilen Computer.\n\n\n\n"}
{"id": "700728", "url": "https://de.wikipedia.org/wiki?curid=700728", "title": "PC Professionell", "text": "PC Professionell\n\nDie PC Professionell (im allgemeinen Sprachgebrauch auch \"PCpro\" genannt) war eine monatlich in München bei VNU Business Publications Deutschland erscheinende Computerzeitschrift. Sie wurde mit der Ausgabe 6/2007 vom neuen Eigentümer 3i eingestellt. Durch den WEKA-Verlag wurde versucht, den Abonnenten einen neuen Vertrag mit einer anderen Zeitschrift unterzuschieben. Die Internetseiten wurden von NetMediaEurope fortgeführt, sind aber inzwischen in ITespresso.de aufgegangen.\n\nHandelte es sich in der Erstausgabe 4/1991 im Untertitel noch allein um \"Das Magazin für professionelle PC-Anwender\", so wurde daraus im Laufe der Jahre \"Das Testmagazin\". Damit wurde der Fokus auf unabhängige Tests in den eigenen Labors gelegt.\n\nDie weiteren Schwerpunkte von PC Professionell manifestierten sich in den Rubriken \"Praxis\" und \"Technik\". Dazu gehörten Anleitungen und Tipp-Sammlungen ebenso wie technische Hintergrundartikel.\n\nEgal ob Hauptplatinen, Chipsätze, Grafikkarten, TFT-Monitore, Digitalkameras, Beamer, Drucker und Multifunktionsgeräte, Festplatten oder DVD-Brenner, System-, Security- oder Multimedia-Software, Webservices oder Online-Zugänge und Internet-Hardware – die PCpro-Mitarbeiter testeten fast alles im eigenen Labor.\n\nBesondere Aufmerksamkeit erregte im November 2004 der erste Warflying-Test: Mit einem Kleinflugzeug begaben sich die Tester in den Münchner Luftraum, um zu prüfen, wie sicher die aufgespürten Wireless LANs sind. Die erschreckende Erkenntnis: Nur 124 Netze waren per WPA gut gesichert, 1.849 Netze waren mit leicht knackbarer WEP nur schwach gesichert und 2.049 Netze waren überhaupt nicht gesichert.\n\nPC Professionell erschien zuletzt in drei Varianten: ohne Heft-CD, mit Heft-CD und mit Heft-DVD.\n\nDer veröffentlichende Verlag war ursprünglich \"Redwood Press Verlag\" in Würzburg, der 1991 an \"Ziff-Davis\" verkauft wurde. Online-Holding Ziff Davis Net (ZDNet) wurde in Europa von CNET übernommen und im Jahr 1999 verkaufte der US-amerikanische Medienkonzern den Verlagsbereich, zu dem auch \"PC Professionell\" gehörte, an das US-amerikanische Investmenthaus \"Willis Stein und Partners\". 2000 übernahm VNU Business Publications Deutschland in München die Zeitschrift. Im Februar 2007 übernahm die Investmentgesellschaft 3i alle Anteile an der VNU Business Media Europe, der Muttergesellschaft von VNU Business Media Deutschland. Der bisherige VNU-Konzern benannte sich in \"Nielsen Company\" um. Nach einem missglückten Weiterverkaufsversuch durch die Private Equity Company 3i wurde der Verlag Ende Juni geschlossen und die Zeitschriften eingestellt. Am 2. Juli 2007 informierte die neu gegründete Aktiengesellschaft NetMediaEurope, dass sie die Sites der VNUNET in Frankreich, Deutschland, Spanien und Italien von 3i übernommen hat und die Fortsetzung der Websites VNUNET, Silicon.fr, The Inquirer (Lizenz von Gawker) und Gizmodo (Lizenz von Gawker) plant. Diese Gesellschaft gehört dem bisherigen Management von VNUNET (Dominique Busso, Laure Chaussin, Pierre Mangin) und der VC Firma Truffle Capital.\n\nSeit 2010 wird die Website unter \"ITespresso.de\" weitergeführt.\n\n"}
{"id": "701093", "url": "https://de.wikipedia.org/wiki?curid=701093", "title": "Remote Installation Services", "text": "Remote Installation Services\n\nRemote Installation Services (kurz: RIS; deutsch: \"Remoteinstallationsdienste\") sind ein optionaler Bestandteil von Microsoft Windows 2000 und Microsoft Windows Server 2003, mit dem Speicherabbilder von Betriebssystemen oder vollständigen Computerkonfigurationen einschließlich Einstellungen und Anwendungsprogrammen erstellt damit diese Netzwerk-Clients verteilt werden können, um diese teilweise oder komplett per Remote Service zu installieren.\n\nAuf dem RIS-Server können mehrere Abbilder der gewünschten Betriebssysteme erstellt werden. Die Abbilder werden direkt auf dem Server durch Einlesen der Installations-CD des jeweiligen Betriebssystems erzeugt. Es ist nicht notwendig (aber dennoch möglich), zunächst einen PC zu installieren, um von diesem ein Abbild zu erzeugen. Wie bei einer unbeaufsichtigten Installation wird mit Antwortdateien gearbeitet, die einem Abbild zugeordnet werden. In diesen Dateien sind Informationen wie der Lizenzschlüssel oder der Computername hinterlegt. Antwortdateien sind frei editierbar und können nach Bedarf alle notwendigen Informationen enthalten oder nur Grundinformationen. Dadurch kann die Installation am Client vollständig ohne Benutzereingaben durchgeführt werden, oder der Benutzer muss noch einzelne Angaben wie z. B. den Computernamen während der Installationsprozedur eingeben.\n\nDie Installation erfolgt über PXE-Technologie, der zu installierende Rechner wird über die Netzwerkkarte gebootet und lädt das zu installierende Abbild über das Netzwerk vom RIS-Server. Bei nicht PXE-fähigen Rechnern kann mit einer hierfür erstellten Startdiskette gestartet werden.\n\nRIS kann verwendet werden, um die Erstinstallation des Betriebssystems vorzunehmen oder eine bestehende Installation zu überschreiben. Mit dieser Technologie können keine zusätzlichen Betriebssystemteile wie optionale Dienste zu einem bestehenden System hinzugefügt werden. RIS wird häufig eingesetzt, um eine standardisierte Grundinstallation auf mehreren Computern zu gewährleisten. Grundvoraussetzungen für den Einsatz sind ein konfiguriertes Active Directory und die Installation des Betriebssystems auf Laufwerk C:. Mit RIS kann kein anderes Laufwerk zur Installation ausgewählt werden.\n\nDie drei Hauptkomponenten des RIS sind \"Boot Information Negotiation Layer\" (BINL), Trivial File Transfer Protocol (TFTP) und \"Single Instance Store\" (SIS).\n\nNachfolger der Remote Installation Services wurden mit der Einführung von Microsoft Windows Server 2008 die Windows Deployment Services (WDS).\n\n"}
{"id": "701321", "url": "https://de.wikipedia.org/wiki?curid=701321", "title": "PERM (Computer)", "text": "PERM (Computer)\n\nDie Programmgesteuerte Elektronische Rechenanlage München (PERM) ist ein Röhrenrechner, der zu Beginn der 1950er Jahre an der TU München entwickelt wurde.\n\nScherzhaft wurde der Rechner damals von den Studenten der TH-München (heute TU-München) auch unter Anspielung auf seine Namensabkürzung PERM auch genannt: Piloty Erstes Rechen-Monster.\n\nEr wurde ab 1952 unter der Leitung der Professoren Hans Piloty (Institut für elektrische Nachrichtentechnik und Messtechnik) und Robert Sauer (Mathematik) gebaut und am 7. Mai 1956 in Betrieb genommen, anfangs in einem eingeschränkten Modus und noch ohne Magnetbandspeicher. Unter anderem wurde an der PERM der erste ALGOL-Compiler entwickelt.\n\nDer Hauptspeicher der PERM bestand zunächst aus einem Trommelspeicher von 8192 Worten, der später durch einen Kernspeicher von 2048 Worten ergänzt wurde. Der Kernspeicher hatte zwar erheblich kürzere Zugriffszeiten, aber abgesehen davon war der gesamte Hauptspeicher homogen. Die Speicheradressen 0 bis 8191 (wurde immer dezimal angegeben) gehörten zum Trommelspeicher und von 8192 bis 10239 zum Kernspeicher. Die Befehle von Programmen und die verarbeiteten Daten konnten im gesamten Adressraum liegen, auch über die Grenze von Trommel- und Kernspeicher hinweg.\n\nDie Wortlänge betrug 51 Bit, die als Gleitkommazahl strukturiert waren: 40 Bit Mantisse, 8 Bit Exponent und je ein Bit Vorzeichen. Das 51. Bit, das so genannte Q-Zeichen, hatte eine spezielle Funktion, aber keine Auswirkung auf den Wert von Zahlen oder auf die Wirkung der meisten Befehle. Mit 40 Bit Mantisse lag die Genauigkeit etwa in der Mitte zwischen der von Gleitkommazahlen einfacher und doppelter Genauigkeit nach später üblich gewordenen Maßstäben. Die 10240 Worte Hauptspeicher entsprachen also 40 bis 80 KiB auf anderen Maschinenarchitekturen, je nachdem, ob für die Anwendung einfach genaue Zahlen ausreichen. Da der Speicher wortweise adressiert wurde, belegten Festpunktzahlen unabhängig von ihrem Wertebereich ein ganzes Wort. Der Speicherbedarf von ausführbaren Programmen oder von Texten ist wegen der ganz anderen Wortformate nicht mit anderen Architekturen vergleichbar.\n\nIm Jahr 1974 wurde die PERM abgeschaltet (\"schlafen gelegt\"); sie ist heute im Deutschen Museum in München ausgestellt.\n\nH. Piloty, R. Piloty, H. O. Leilich, W. E. Proebster: Die programmgesteuerte elektronische Rechenanlage München (PERM), Nachrichtentechnische Zeitschrift, 1955, 1. Teil in Heft 11, Seite 603–609 und 2. Teil in Heft 12, Seite 650–658\n\n"}
{"id": "702133", "url": "https://de.wikipedia.org/wiki?curid=702133", "title": "Gittereichtheorie", "text": "Gittereichtheorie\n\nEine Gittereichtheorie ist eine Eichtheorie, die auf einer diskreten Raumzeit definiert wird. Gittereichtheorien gehören zu den wenigen Möglichkeiten, nicht-störungstheoretische Berechnungen in Quantenfeldtheorien anzustellen.\n\nBesondere Bedeutung erlangte die Methode im Rahmen der Quantenchromodynamik (QCD). Weil die Gitterregularisierung eine nicht-störungstheoretische Regularisierung ist, kann man in Gittereichtheorien auch Berechnungen für niedrige Energien durchführen, die für die Störungstheorie nicht zugänglich sind. Dadurch lassen sich u. a. die Massen von Hadronen, d. h. gebundenen Quarkzuständen, von thermodynamischen Größen oder von wichtigen topologischen Anregungen (Monopole, Instantonen und Solitonen) untersuchen.\n\nNeben der QCD werden auch andere Eichtheorien und Spinsysteme auf dem Gitter untersucht, insbesondere solche mit nichtabelscher Eichgruppe (allgemeine Yang-Mills-Theorien analog zur QCD).\n\nDie Grundidee ist, durch Einführen eines minimalen Abstandes in Raum und Zeit die Theorie zu regularisieren, sodass bei hohen Energien keine Divergenzen mehr auftreten. Dieser minimale Abstand entspricht einer Abschneideenergie (engl. Cut-Off) im Impulsraum. Eine stete Verkleinerung des minimalen Gitterabstandes entspricht dem Übergang zur ursprünglichen Theorie im kontinuierlichen Raum durch Entfernen der höchsten Energien im Impulsraum.\n\nUm Simulationen von Gittereichtheorien auf Computern zu ermöglichen, wird in der Regel zusätzlich eine Wick-Rotation ausgeführt, wodurch man zum Euklidischen Raum übergeht. Dann existiert eine Verwandtschaft zur statistischen Physik, und es kann das mächtige Werkzeug der Monte-Carlo-Simulation herangezogen werden.\n\nDie 1974 von Kenneth Wilson eingeführte Gittertheorie der Quantenchromodynamik diskretisiert die Wirkung der QCD auf einem vierdimensionalen kubischen Gitter formula_1 mit Gitterabstand formula_2. Ein wichtiges Prinzip bei Wilsons Konstruktion dieser Gittertheorie ist, dass deren Wirkung auch bei endlichem Gitterabstand explizit eichinvariant ist. Des Weiteren ist die Wilson-Wirkung so gewählt, dass sich im Grenzwert formula_3 die Kontinuumswirkung ergibt. Üblicherweise betrachtet man die Formulierung für den Eichsektor separat von derjenigen für Fermionen, da die Übertragung der chiralen Symmetrie der Fermionfelder auf das Gitter ein eigenes Problemfeld darstellt.\n\nFür die Diskretisierung der Yang-Mills-Wirkung, die die Dynamik der Eichbosonen beschreibt, definiert man sogenannte Linkvariablen formula_4, welche benachbarte Gitterpunkte verbinden,\n\nHierbei sind die Eichfelder formula_6 Elemente der adjungierten Darstellung der Algebra der Eichgruppe der QCD, SU(3). Die Linkvariablen formula_4 selbst Elemente der Eichgruppe, also SU(3)-Matrizen, die jeweils zwei benachbarte Gitterpunkte verbinden. Im Sinne der Differentialgeometrie können sie als endlicher Paralleltransport aufgefasst werden.\n\nDer Eichfeldanteil der Wirkung lässt sich nun als die Spur über geschlossene Schleifen von Linkvariablen darstellen. Jede Spur über solche Wilson-Loops ist eichinvariant. Eine einfache Eichwirkung kann daher wie folgt geschrieben werden:\n\nHierbei sind die formula_9 (die sogenannten Plaketten-Variablen) definiert als die zu kleinsten geschlossenen Rechteck-Schleifen gehörigen Größen\n\nanalog zur Geometrie eines Quadrats, das z. B. bei positivem Umlaufsinn durch die vier Zahlen 1, 2, 3 und 4 indiziert wird.\n\nStatt der Kopplungskonstante formula_11 benutzt man häufig die inverse Eichkopplung formula_12\n\nDa die Form der Wirkung nur durch den Kontinuumslimes, formula_13, festgelegt ist, ist die obige angegebene Eichwirkung, die sogenannte Wilson- oder Plakett-Wirkung, nicht eindeutig, sondern kann durch Terme, die im Kontinuumslimes verschwinden, modifiziert werden. Diese Beobachtung wird verwendet, um verbesserte Wirkungen mit einer schnelleren Kontinuumsannäherung zu konstruieren.\n\nWährend die Linkvariablen jeweils zwei Gitterpunkte verbinden, sind die Fermionfelder auf diesen Punkten definiert. Dadurch lassen sich eichinvariante Kombinationen der Form formula_14 mit formula_15 bilden, die als Bausteine der diskretisierten kovarianten Ableitung verwendet werden können.\n\nErsetzt man nun die Ableitungen in der Dirac-Wirkung durch endliche Differenzen, erhält man eine naive Diskretisierung der Theorie, die nicht nur ein einzelnes Fermion beschreibt, sondern sechzehn (formula_16 mit formula_17 für die Anzahl der Dimensionen). Dieses Phänomen ist als Dopplerproblem bekannt und hängt mit der Realisierung der chiralen Symmetrie auf dem Gitter zusammen. In der Tat besagt das Nielsen-Ninomiya-Theorem, dass auf dem Gitter kein Dirac-Operator mit korrektem Kontinuumslimes gleichzeitig dopplerfrei, lokal, translationsinvariant und chiral-symmetrisch sein kann. Um das Dopplerproblem physikalisch korrekt zu berücksichtigen, werden verschiedene Arten der Fermiondiskretisierung verwendet, die im Folgenden beschrieben werden.\n\nZur Beseitigung der Doppler kann man weitere Terme in die Wirkung einbinden, die den unphysikalischen Fermionmoden eine zusätzliche Masse verleihen. Beim Bilden des Kontinuumslimes entkoppeln die so entstehenden \"Dopplermoden\" von der Theorie, da ihre Masse formula_18 divergiert. Dies ist der Ansatz der Wilson-Fermionwirkung:\n\nwobei \"f\"  den Flavour-Freiheitsgrad der Fermionen bezeichnet und \"r\" als Vorfaktor vor dem neu eingeführten Wilson-Term frei gewählt werden kann. Für \"r=0\" erhält man die ursprünglichen naiv diskretisierten Fermionen mit Dopplern, während für die übliche Wahl \"r=1\" die Doppler wie oben beschrieben beseitigt werden.\n\nBei endlichem \"a\" ist jedoch durch den Wilson-Term die chirale Symmetrie explizit gebrochen und wird erst im Kontinuumslimes wieder hergestellt. Eine praktische Konsequenz ist, dass die Gitterartefakte anders als für andere Wirkungen schon in linearer Ordnung des Gitterabstandes auftreten. Um dieses Problem zu beheben, werden in numerischen Simulationen fast ausschließlich sogenannte verbesserte Wirkungen verwendet. Am weitesten verbreitet sind hierbei die sogenannten \"clover fermions\", für die ein weiterer Term zur Wirkung hinzugefügt wird, dessen freier Parameter so gewählt werden kann, dass die führenden Gitterartefakte eliminiert werden. Daneben finden auch Wilson-Fermionen mit einem modifizierten Massenterm unter dem Namen \"twisted mass fermions\" Verwendung.\n\nNeben den Wilson-Fermionen werden insbesondere die sogenannten Staggered-Fermionen (engl. \"staggered fermions\") verwendet. Diese nutzen eine Spindiagonalisierung um die Anzahl der Doppler um einen Faktor 4 zu reduzieren. Um eine Theorie mit genau einer Fermionart zu beschreiben, muss ein theoretisch umstrittenes Verfahren, das als \"rooting\" bekannt ist, angewandt werden.\n\nIn der Kontinuumstheorie erfüllt der Dirac-Operator \"D\" einer chiral-symmetrischen Theorie die Beziehung formula_20 wobei \"γ\", wie auch \"D\", aus der Dirac-Theorie als bekannt vorausgesetzt werden sollen. Der Wilson-Term bricht diese Symmetrie explizit. Dies lässt sich jedoch durch eine abgeschwächte Definition chiraler Symmetrie auf dem Gitter umgehen,\n\nwobei \"R\" ein lokaler Gitteroperator ist. Das Verhalten ~ a führt zu einer effektiven Glättung des störenden Terms ~ 4/a des Wilson'schen Funktionals.\n\nDurch diese Ersetzung erhält man den sogenannten Overlapoperator, und aus der Wilson-Gleichung entsteht die Ginzparg-Wilson-Gleichung. Neben exakten Lösungen gibt es auch eine Reihe gebräuchlicher Fermionen, deren Diracoperator die Ginsparg-Wilson-Gleichung nur näherungsweise erfüllt. Die bekanntesten sind die Domain-Wall-Fermionen, die (im Falle unendlicher Ausdehnung einer fünften Dimension) dem Overlapoperator entsprechen. In praktischen Simulationen bleibt diese Dimension jedoch stets endlich.\n\nDie zuvor genannten Diskretisierungen stellen die am häufigsten verwendeten Methoden dar, Fermionen auf dem Gitter zu behandeln. Daneben gibt es weitere, wie etwa die Fermionen mit minimaler Dopplung (\"minimally doubled fermions\"), die über eine Modifizierung der Gittergeometrie eine Minimierung des Dopplerproblems erreichen. Eine weitere Variante ist die Brechung der Translationsinvarianz durch die Einführung einer zusätzlichen Dimension, wie es bei den Domain-Wall-Fermionen geschieht.\n\nIn einer Gitter-QCD Simulation kann man eine Reihe von Parametern einstellen: die Anzahl der Gitterpunkte in räumliche und zeitliche Richtung, die Gitterkopplung formula_22 und ggfs. Quarkmassenparameter und Parameter die zur theoretischen Verbesserung des Kontinuumsverhaltens führen. Um ein solches \"Setup\" aus dimensionslosen Zahlenangaben in physikalische Einheiten zu übersetzen, d. h. um dimensionsbehaftete Größen, wie den Gitterabstand (in fm) oder Hadronmassen (in MeV/c) zu erhalten, müssen ausgewählte physikalische Objekte (wie die Masse oder Zerfallskonstante des Pions) zur Setzung der Skala fixiert werden. Alle weiteren berechneten Größen sind dann Vorhersagen der Gitter-QCD zu den gegebenen Parametern.\n\nDabei wächst die benötigte Computerleistung mit sinkender Masse, so dass das Erreichen physikalischer Quarkmassen ohne weitere Extrapolation nach wie vor einen enormen Aufwand bedeutet und nicht mit allen Fermiondiskretisierungen erreicht worden ist. Darüber hinaus gilt es, die systematischen Effekte, die durch die Extrapolation zu verschwindendem Gitterabstand und unendlichem Volumen bedingt sind, unter Kontrolle zu halten.\n\nDie asymptotische Freiheit der QCD stellt mit ihrem Fixpunkt im Fluss der Kopplungskonstante sicher, dass der Kontinuumslimes formula_13 für verschwindende Kopplung (formula_24 bzw. formula_25) erreicht wird.\n\nAls eigentliche Geburtsstunde der Gitter-QCD gilt heute die Veröffentlichung der Arbeit  des Physikers Kenneth Wilson im Jahre 1974, die sehr bald zum Kernbereich des damaligen Forschungsstandes zählte und eine rapide Entwicklung der Methode auslöste.\n\nDie Methode der Gitter-QCD ist analog zu speziellen Spinmodellen, die 1971 in festkörpertheoretischem Zusammenhang von Franz Wegner aufgestellt wurden. Diese Gitter-Spinmodelle zeichnen sich, wie in der QCD, durch eine lokale Eichinvarianz und durch einen zur Eichfeldenergie analogen Term aus. Heutzutage ist jedoch der festkörpertheoretische Zusammenhang der Theorie weitgehend vergessen (siehe aber die Arbeit von J. Kogut).\n\nObwohl die Quantenchromodynamik ein Hauptanwendungsgebiet der Gittereichtheorie ist, gibt es selbst in der Hochenergiephysik Untersuchungen mit Gittermethoden, die über die QCD hinausgehen, z. B. zum Higgs-Mechanismus.\n\nFür die QCD selbst hat sich u. a. ein an der Universität Regensburg zentrierter größerer Verbund gebildet, der Aktivitäten vieler in Deutschland (und Norditalien) führender Forschungsplätze bündelt und mit einem speziellen Hochleistungsrechner, QPACE, auf vorhandene Erfahrungen (siehe QCDOC in der englischen Wikipedia) und ein zukunftsweisendes Konzept zurückgreifen kann.\n\nEin Vorteil von Simulationen von Gittereichtheorien ist, dass vor allem  \"eichinvariante Größen\"  zugänglich sind. Dies führte zu einer Berechnung aller Meson- und Baryon- Grundzustände, die Up-, Down- oder Strange-Quarks enthalten. Ein typisches Resultat (siehe die Grafik) zeigt, dass in einem Meson nicht nur die Teilchen, Quarks und Antiquarks, sondern auch die „Flussschläuche“ der Gluonen-Felder wichtig sind.\n\nMit solchen Rechnungen lassen sich auch \"kollektive Effekte\" studieren, die mit dem Confinement-Phänomen in Zusammenhang stehen könnten. Dies können z. B. topologische Anregungen wie Instantonen, Monopole und Solitonen, oder Perkolationseffekte des Zentrums der Eichgruppe sein.\n\nEs lassen sich auch Berechnungen der QCD bei hohen Temperaturen durchführen um den \"Übergang in das Quark-Gluon-Plasma\" zu studieren, der in Experimenten an Teilchenbeschleunigern oberhalb von etwa 1,2×10 Kelvin gemessen wurde.\n\n"}
{"id": "702496", "url": "https://de.wikipedia.org/wiki?curid=702496", "title": "Ableton Live", "text": "Ableton Live\n\nAbleton Live ist ein für macOS und Windows verfügbarer Sequenzer der Berliner Softwarefirma Ableton und ein Werkzeug zur Musikproduktion, das sich sowohl an die Zielgruppe Live-Musiker/DJs richtet, die ihre Musik in Echtzeit auf der Bühne darbieten, als auch an Produzenten, die mit Hilfe dieser Software musikalische Arrangements erstellen möchten.\n\nDie Hauptstärken von \"Live\" liegen in der Echtzeitbearbeitung von Audioquellen (Samples), die je nach Belieben im Tempo und unabhängig davon in der Tonhöhe geändert (Time-Stretching), mit Effekten moduliert oder mit internen Instrumenten zusammengemischt werden können. Ebenso lassen sich Audioquellen in Echtzeit aufnehmen und virtuelle Instrumente oder Effekte (VST, ReWire etc.) ansteuern. Darüber hinaus ist \"Live\" in der Lage, MIDI-Geräte in die Performance einzubinden. Somit erlaubt \"Live\" das Komponieren / Improvisieren von Musik in Echtzeit, wobei die Software auch über MIDI-Controller (z. B. Tastatur, MIDI-Keyboard, Mischpult etc.) gesteuert werden kann. Die Fernsteuerung über MIDI erlaubt Musikern verschiedene Manipulationen zur selben Zeit, während dieses Feature mit einer Maus eher eingeschränkt ist. Für die MIDI-Steuerung wurde von Ableton in Kooperation mit Akai eine eigens auf Live abgestimmte Serie von MIDI-Controllern entworfen. Unter dem Label von Akai wurden die APC20, APC40 und als Nachfolger die APC40mk2 veröffentlicht, später dann der ebenfalls von Akai gebaute, aber unter der Marke \"Ableton\" verkaufte Push. Die Entwicklung und Fertigung des Push2 wurde losgelōst von Akai durch Ableton ūbernommen. Als Sequenzer kann \"Live\" auch bei Aufnahmen im Studio verwendet werden.\n\nAb Version 5 verfügt \"Ableton Live\" über eine Funktion mit der Bezeichnung „Clip Freeze“, die dazu dient, die CPU-Auslastung zu reduzieren. Eingefrorene Tracks können nicht bearbeitet werden, belasten aber auch die CPU nicht mehr sonderlich. Des Weiteren wird eine Vielzahl an neuen und qualitativ hochwertigen Effekten (sechs neue/22 insgesamt) mitgeliefert. Die Effekte Phaser, Flanger, Auto Pan, Saturator, Arpeggiator und Beat Repeat sind seit der Version 5 fester Bestandteil von \"Live\". Neben WAV- und AIFF- wird nun auch der Import von MP3-Dateien unterstützt.\n\nMit \"Live 5.2\" integrierte Ableton die native Unterstützung für Intel-basierte Apple-Macintosh-Computer.\n\nAb Version 6 halten weitere Verbesserungen Einzug. Neben dem neuen Instrument „Sampler“, dem Effekt „Dynamic Tube“ und der Erweiterung vorhandener Instrumente und Effekte, ist \"Live\" nun auch in der Lage, eingefrorene Tracks zu bearbeiten, ohne sie vorher aufzutauen („Deep Freeze“). \"Live 6\" bietet die Möglichkeit, eingebaute und externe Instrumente und Effekte nicht nur in den \"Device-Groups\" zusammenzufassen, sondern zusätzlich auch in virtuellen Racks zu organisieren und dadurch zentral zu kontrollieren. Unter den angekündigten Neuerungen befinden sich unter anderem: Multi-CPU-/Multi-Core Unterstützung, die Möglichkeit, Filme im QuickTime-Format zu importieren und somit zu vertonen, Tonspuren als einzelne Dateien zu exportieren und die direkte Unterstützung zahlreicher MIDI-Controller.\nVersion 6 wird auf Wunsch mit einer umfangreichen Bibliothek gesampelter Instrumentenklänge, die \"Essential Instrument Collection\" (EIC), ausgeliefert.\n\nIm November 2007 erschien die Version 7, am 2. April 2009 die Version 8. Version 8 bietet Verbesserungen im Warping, eine neue Groove-Funktion, die Möglichkeit, Spuren zu Gruppen zusammenzufassen, die neuen Effekte Vocoder, Looper, Limiter, Multiband Dynamics, Overdrive und Frequency Shifter sowie viele weitere kleinere Änderungen. Weiterhin ist ein Werkzeug für Online-Zusammenarbeit integriert, aber noch (Stand: Juli 2009) nicht freigeschaltet (im geschlossenen Beta-Test). In der Suite oder gegen Aufpreis gibt es das neue virtuelle Instrument Collision für perkussive Klänge und die Sample-Library Latin Percussion.\n\nDie Benutzeroberfläche von \"Live\" wurde in verschiedenen Sprachen lokalisiert, darunter befindet sich auch eine deutsche Version.\n\nSeit Version 8.2.6 ist Live kompatibel zum Betriebssystem Mac OS X 10.7 Lion und unterstützt die Controller-Serie „Impulse“ von Novation. Anfang April 2012 wurde Version 8.3 veröffentlicht; ab dieser Version aktualisiert sich das Programm automatisch. Audiodateien können nun direkt aus dem Programm auf einen eigenen SoundCloud-Account hochgeladen werden. Mit dem Update auf 8.3.1 wurde die Kompatibilität zum Betriebssystem OS X Mountain Lion berücksichtigt.\n\nVersion 9 bringt vor allem die Unterstützung des hauseigenen Hardwarecontrollers Push, die Möglichkeit Clip-Automationskurven aufzunehmen, einen überarbeiteten Browser, Multi-Monitor Unterstützung, den neuen Glue Compressor und die Möglichkeit, Audio-Clips in MIDI umzuwandeln.\n\nDie am 6. Februar 2018 erschienene Version Live 10 enthält einen Wavetable-Synthesizer, zwei neue Effekte sowie eine verbesserte Zusammenarbeit mit Push. Dieser bietet nun die Möglichkeit Effekte und Midinoten zu visualisieren.\n\n\n"}
{"id": "703108", "url": "https://de.wikipedia.org/wiki?curid=703108", "title": "FTPFS", "text": "FTPFS\n\nFTPFS ist ein virtuelles Dateisystem für Linux, welches auf dem File Transfer Protocol (FTP) basiert.\n\nFTPFS wurde ursprünglich als eigenständiges Linux-Kernel-Modul entwickelt, später aber in die LUFS-Suite integriert. LUFS wurde von FUSE abgelöst, wo es mit Stand April 2007 als curlftpfs, basierend auf der universellen HTTP- und FTP-Bibliothek libcurl weiterentwickelt wird.\n\nIn aktuellen Linux-Distributionen wie Debian (ab Version 4.0 Etch), Fedora, openSUSE (in Version 10.3) oder Gentoo Linux ist curlftpfs bereits enthalten.\n\nFTP unterstützt keinen wahlfreien Zugriff auf Dateien. Das bisher letzte Release von CurlFtpFs (0.9.1) hält eine zu schreibende Datei daher solange im Speicher, bis sie geschlossen wird. Erst beim Schließen wird sie dann komplett übertragen. Bei großen Dateien kann das dazu führen, dass der verfügbare Arbeitsspeicher des Clients nicht mehr ausreicht.\n\n\n"}
{"id": "704377", "url": "https://de.wikipedia.org/wiki?curid=704377", "title": "GOCR", "text": "GOCR\n\nGOCR ist eine freie Texterkennungs-Software (OCR) von Jörg Schulenburg, die durch Kommandozeilen-Befehle bedient wird.\n\nGOCR erkennt verschiedene Druckerschriftarten, ohne auf eine Datenbank zurückgreifen zu müssen, was die Anwendung besonders einfach macht. Neben reinem Text kann das Programm auch gängige eindimensionale Strichcodes dekodieren.\n\nDas Programm wird als freie Software auch im Quelltext unter den Bedingungen der GNU General Public License (GPL) verbreitet. Ursprünglich erschien das Programm für Linux, mittlerweile sind auch Binärdateien für Windows und OS/2 verfügbar, die von den externen Programmierern Peter B. L. Meijer und Franz Bakan zur Verfügung gestellt werden. Das Programm ist in allen gängigen Linux-Distributionen enthalten. Es dient zum Beispiel Kooka, einer Scannersoftware unter KDE, als Back-end.\n\nDie erste Version (0.1) wurde im Herbst 1998 erstellt. Die erste offizielle Veröffentlichung erfolgte mit Version 0.2.1 im März 2000 über Freshmeat. Seit Mitte 2000 ist das Projekt auf SourceForge gehostet.\nDa bei der Veröffentlichung des Quellcodes auf SourceForge.net festgestellt wurde, dass bereits ein Projekt mit dem Namen \"GOCR\" existierte, führte die Projektleitung auch den Namen \"JOCR\" für das Programm ein. Um 2000/2001 wurde es in die Linux-Distributionen SuSE und Debian aufgenommen. Bis zur Freigabe von Tesseract im Jahr 2005 (2006?) war es neben Ocrad die beste und gebräuchlichste Texterkennungs-Software aus den Bereichen der freien Software und Linux-Software.\n\n\n"}
{"id": "704462", "url": "https://de.wikipedia.org/wiki?curid=704462", "title": "CPC 5512", "text": "CPC 5512\n\nDer Heimcomputer CPC 5512 war ein Scherz des französischen Computermagazins „Hebdogiciel“ aus dem Jahre 1985, der zwar in der nächsten Ausgabe widerrufen wurde, aber ein gerichtliches Nachspiel nach sich zog.\n\n\n\n\nDas Modell wurde aus Computerteilen und etwas Papier zusammengebaut und anschließend fotografiert. Es war ohne Funktion und wurde natürlich nie produziert.\n\nAmstrad verkaufte zeitgleich sein CPC 6128-Modell und arbeitete an seinen späteren IBM-kompatiblen Modellen. In Erwartung des CPC 5512 gingen die Verkaufszahlen des CPC 6128 auf dem französischen Markt zurück, worüber Amstrad und dessen Handelspartner nicht erfreut waren. Man strengte die Gerichte an, vom Ausgang des Verfahrens ist jedoch nichts bekannt. \n\nDie Auswirkungen des ansonsten gelungenen Artikels hätten sich mit einer Richtigstellung in derselben Ausgabe vermeiden lassen, sie erfolgte aber erst in der nächsten Ausgabe.\n\nUnabhängig von dem französischen Aprilscherz hat die deutsche Computerzeitschrift c't im Oktober 1987 eine Umbauanleitung zur Aufrüstung eines CPC 6128 auf 512 KB RAM veröffentlicht. Das Resultat, der „CPC 6512“, war Thema mehrerer Artikel.\n\n"}
{"id": "707357", "url": "https://de.wikipedia.org/wiki?curid=707357", "title": "Footprint Assembly", "text": "Footprint Assembly\n\nFootprint Assembly ist ein Algorithmus zum anisotropen Filtern von Texturen. Die Texturverzerrung kann dabei in jeder beliebigen Richtung berechnet werden. Außerdem lässt sich der Algorithmus gut mit Techniken wie MIP-Mapping, bilinearem oder trilinearem Filtern oder auch Integralbildern (Summed Area Tables) kombinieren.\n\nEin \"Footprint\" ist die Projektion eines Pixels in das Koordinatensystem der Textur. Es wird angenommen, dass die Textur eine flache Oberfläche bespannt (dies ist bei Dreiecken immer der Fall). Der \"Footprint\" ist dann ein konvexes Viereck. Je genauer der Farbwert des Footprints berechnet werden kann, desto besser wird das resultierende Bild. Um Rechenaufwand zu sparen, wird der Footprint beim 'Footprint Assembly' nur angenähert.\n\nBeim Footprint Assembly wird der \"Footprint\" vereinfachend als Parallelogramm angenommen. Hierzu wird der Pixelmittelpunkt in die Texturkoordinaten projiziert und gibt den Mittelpunkt des Parallelogramms an. Die Vektoren, die das Pixel entlang der beiden Hauptachsen aufspannen, werden ebenfalls in das Texturkoordinatensystem projiziert. Sie spannen nun das Parallelogramm auf.\n\nVielfach wird auch das Parallelogramm als Footprint bezeichnet.\n\nEs wird das Parallelogramm berechnet, das den Footprint annähert. Der Mittelpunkt heiße p.\n\nDer kürzere der beiden Kantenvektoren gibt die Kantenlänge von Quadraten an, aus denen der Footprint berechnet werden soll. Die Quadrate lassen sich mit isotropen Filtern berechnen (meist MIP-Mapping mit bilinearem Filtern). \nDer längere der beiden Vektoren gibt einen Pfad an, entlang dem die quadratischen Flächen aufsummiert werden sollen. \n\nDie Anzahl formula_1 der Quadrate wird durch den Quotienten aus den Vektorlängen des längeren durch den kürzeren Vektor bestimmt und auf die nächste Zweierpotenz gerundet. Die Zweierpotenz ermöglicht ein leichteres dividieren der Summe mittels Rechts-Shifts.\nDann wird ein Schrittvektor formula_2 konstruiert, der sich aus dem längeren Vektor ergibt, wenn man ihn mit formula_3 skaliert.\nDie Punkte formula_4 an denen abgetastet werden soll, ergeben sich aus formula_5 mit formula_6. An diesen Punkten als Mittelpunkt wird jeweils der Farbwert eines der Quadrate berechnet. Die Farbwerte werden summiert und durch die Anzahl formula_1 geteilt. Das Ergebnis ist die Annäherung des Footprint-Farbwertes und der Texturwert für das Pixel.\n\n"}
{"id": "708644", "url": "https://de.wikipedia.org/wiki?curid=708644", "title": "Tripping the Rift", "text": "Tripping the Rift\n\nTripping the Rift ist eine animierte Science-Fiction-Serie, die ursprünglich auf dem animierten Kurzfilm \"Love and Darph\" von Chris Moeller und Chuck Austen basiert. Dieser sechs Minuten lange Kurzfilm geistert seit 2000 durch das Internet. Ein kurzer Teaser für die geplante zweite Folge ist erschienen, die Folge selbst wurde jedoch nie veröffentlicht.\n\nDie Idee des Kurzfilms wurde dann vom amerikanischen SciFi-Channel aufgegriffen. Im März 2004 wurde die erste Folge der Serie in den USA ausgestrahlt. Es gibt drei Staffeln mit je 13 Folgen. Außer in den USA lief die Serie auch noch in Großbritannien und in Frankreich im Fernsehen. Seit dem 20. März 2009 wird die Serie auf DMAX ausgestrahlt.\n\nAufgrund ihres mitunter ziemlich vulgären und morbiden Humors ist \"Tripping the Rift\" für ein erwachsenes Publikum konzipiert und wird entsprechend vermarktet. Vom Konzept her ist die Serie humoristische Science-Fiction, die bevorzugt Elemente aus bekannten Sagen wie Star Trek und Star Wars verarbeitet, aber auch zahlreiche Anspielungen auf andere Filme und Serien enthält.\n\n\n\n\n\n\n\n\n1. Staffel\n\nAuf einer Erholungsreise zum Ursprung der Zeit mit dem Weltraum-Schmugglerschiff Jupiter 42 bringen Kapitän Chode und Bordingenieur Gus versehentlich Gott um. Zurück in ihrer Heimat stellen die beiden fest: Die Welt ohne Gott ist ein absoluter Alptraum. Ein verrückter Plan, der natürlich ein paar paradoxe Zeitreisenphänomene beinhaltet, muss her. \n\nDen Superstar der Mutilation-Ball-Liga zu entführen und ihn dazu bringen, ein letztes Spiel gegen die Finsterclowns zu bestreiten ist keine leichte Aufgabe für die Crew der Jupiter 42. Und als dann auch noch ein echter, aber wesentlicher Mord ins Spiel kommt, wird es richtig eng für die durchgeknallten Freaks um Captain Chode. Die Konföderation erwartet schließlich einen Sieg.\n\nCaptain Chode meldet die Androidin Six of Nine bei einer Schönheitskonkurrenz an, um das Preisgeld einzuheimsen. Allerdings ist auch der Bösewicht Darph Bobo hinter dem Geld her und schickt seine Tochter Babette in den Zweikampf, der natürlich nicht ohne List und Tücke ausgetragen wird…\n\nEin mysteriöser anonymer Auftrag führt den Captain und seine Crew auf den Planeten Kubrickia: Sie sollen dort einen gigantischen Monolithen abliefern. Schnell wird allerdings klar, dass hinter der Geschichte wiederum Darph Bobo steckt, der das dortige Volk versklaven und ausbeuten will. Six überzeugt Chode davon, dass man etwas dagegen unternehmen muss.\n\nUm ein dringend benötigtes Ersatzteil zu besorgen, muss die Besatzung der Jupiter 42 auf einem echten Alptraumplaneten landen: Snoozle, der nicht nur moralisch blitzsaubersten Welt im ganzen bekannten Universum. Es passiert, was passieren muss, Chode wird schließlich gar zum Tode verurteilt und ein Ausweg muss her. \n\nIrgendwie gerät der Draufgänger Chode immer wieder in diese Situationen: Während einer Galaxis-Kreuzfahrt entschließt er sich kurzerhand zum Besuch des Party-Planeten Florida 7 und vergisst dabei, dass er dort seit einiger Zeit polizeilich gesucht wird. Das muss schiefgehen: Die Crew wandert gleich mit in den Knast und es gibt nur einen Weg, rauszukommen, der ist aber ziemlich abgefahren…\n\nSchon wieder blankes Chaos auf der Jupiter 42: Der zuverlässige Bordingenieur Gus wird von einem futuristischen Ersatzroboter abgelöst, der sich als böse Falle eines hinterhältigen Space-Gurus entpuppt und zu allem Überfluss kommt auch noch Captain Chodes ziemlich anstrengender Opa Benito zu Besuch. \n\nAuf der Flucht vor einem Angriff der Finsterclowns gerät die Crew auf den Planeten Moldavia 5, auf dem der König Philbrick gerade ein rauschendes Fest feiert. Auf der Party gelandet stellt sich plötzlich heraus: Chode und der Herrscher des Planeten sind Zwillingsbrüder, die bei der Geburt getrennt wurden. Dass da natürlich ein Rollentausch drin sein muss, ist völlig klar. Und dass dabei einiges schiefgeht, noch klarer.\n\nEine Notlandung auf dem Planeten Harmonia 7 ist die Folge eines Ausfalls im Steuerelement von Bob, der künstlichen Intelligenz der Jupiter 42. Die Suche nach dem Ersatzteil gestaltet sich schwieriger als erwartet, zumal das letzte Geld auch noch von ein paar Vorschulkindern gestohlen wird. Der Regierungschef höchstpersönlich scheint die letzte Rettung für Chode und Co. Aber der hat so seine eigenen Probleme, die erst gelöst werden wollen.\n\nMit Höchstgeschwindigkeit zum nächsten Party-Planeten: Die Besatzung sucht dringend nach der perfekten Freizeitgestaltung im Weltall. Als ihnen jedoch zu Ohren kommt, dass gerade der intergalaktische Friedenspreis mit einer Million dotiert wurde, schmiedet Chode einen verrückten Plan, der die Verkupplung von Darph Bobos Tochter mit dem Sohn von Commander Adam beinhaltet. Wenn das mal nicht schiefgeht.\n\nDer Computer Bob hat einen kurzen Aussetzer und schon befindet sich die Jupiter 42 auf Kollisionskurs mit einem schwarzen Loch! Da hilft nur noch eine Lösung, die man immer in solchen Situationen anwenden kann: Schnell einen Pakt mit dem Teufel schmieden. Dass dieser allerdings seine Haken hat ist klar und deswegen muss er anschließend in der Vergangenheit angefochten werden. Logisch, oder?\n\nDer Oberbösewicht Darph Bobo scheint am Ziel aller seiner finsteren Pläne: Er entführt den hilflosen Bordassistenten Whip und lockt Captain Chode in einen Hinterhalt. Ein Laserschwert-Duell führt nicht nur zu einem Verlust eines lila Tentakels bei dem, sondern auch zum Verlust jeglicher Lust – besorgniserregend. Eine Lösung ist dringend gefragt. Ach ja, und Rache ist natürlich ebenfalls eine Option.\n\nBeim Besuche eines Strip-Clubs trifft die schöne Six einen alten Liebhaber wieder: Mit dem Sexdroiden Ten ist sie damals aus der Sklaverei der Finsterclowns entflohen. Die beiden brennen auch flugs miteinander durch und der wütende Chode braucht einen Ersatz: In dem Cheerleader Angel glaubt er, diesen gefunden zu haben, diese erweist sich aber als längst nicht so willig wie seine ehemalige Gespielin Six.\n\n"}
{"id": "709555", "url": "https://de.wikipedia.org/wiki?curid=709555", "title": "LocalTalk", "text": "LocalTalk\n\nLocalTalk ist ein Begriff aus der Informatik und bezeichnet ein veraltetes, proprietäres Netzwerksystem von Apple.\n\nGenauer handelt es sich um eine spezielle Implementierung der Bitübertragungsschicht (nach OSI-Modell auf Level 1) für die AppleTalk-Protokollfamilie.\n\nLocalTalk spezifiziert eine Zweidraht-Leitung, die den RS-422-Anschluss der älteren Apple Macintosh nutzt, der mit 230,4 kbit/s arbeitet und eine maximale Paketgröße von 603 Bytes zulässt; ohne Datenpräambel und ohne Trailer. Die Sende- und Empfangsleitungen der RS-422 Schnittstelle sind im LocalTalk-Betrieb gebrückt, elektrisch handelt es sich also wie bei Ethernet um ein Bussystem, zur Zugriffssteuerung wird CSMA/CD eingesetzt.\n\nZur Verdrahtung wird eine Splitter-Box eingesetzt, sie enthält zum einen galvanische Trennelemente, um Ausgleichsströme zwischen den einzelnen Stationen bei räumlich ausgedehnteren Netzwerken zu vermeiden; zum anderen stellt sie zwei Steckverbindungen zu Verfügung, um mehrere Arbeitsstationen zu einem Rechnernetz zusammenzuschalten. Die Enden des dadurch gebildeten Netzwerksegments wurden automatisch terminiert.\n\nEin einzelnes LocalTalk-Netzwerksegment kann bis zu 300 Meter Länge umfassen. Empfohlen wird der Betrieb von maximal 32 Geräten pro Segment.\n\nUrsprünglich hieß LocalTalk \"AppleTalk Personal Network\" und arbeitete mit einem geschirmten Twisted-Pair-Kabel und einem Drei-Pin-Mini-DIN-Stecker. Die Geräte wurden ähnlich Thin Ethernet in einer Kette (Daisy Chain) verbunden.\n\nAuch die Drucker von Apple waren mindestens optional mit einer Mac-kompatiblen, nur für LocalTalk gedachten RS-422-Schnittstelle ausgerüstet. Um in diesem Marktsegment ebenfalls erfolgreich sein zu können, haben auch andere Hersteller ihre Drucker mit passenden Schnittstellen ausgerüstet.\n\nBei Dateiübertragungen über AFP ergibt sich eine Übertragungsrate von etwa 22 KBytes/s.\n\nMit der Zeit wurde LocalTalk nicht zuletzt wegen\nvon anderen Techniken wie Ethernet oder Token Ring in Bezug auf Geschwindigkeit und damit Komfort übertroffen.\n\nMit der Einführung des iMac 1998 wurde die Unterstützung von LocalTalk seitens Apple eingestellt.\n\nEine Abwandlung von LocalTalk wurde von der Firma \"Farallon Computing\" unter dem Namen \"PhoneNet\" eingeführt. Dabei verwendete man einen einfachen RJ-11-Anschluss statt der aufwendigen Mini-DIN-Stecker und einfache Telefonkabel statt der teuren Twisted-Pair-Kabel. Die geringeren Kosten der Kabelverbindungen verschafften PhoneNet einen besseren Stand als LocalTalk und verdrängten es in kleineren Netzen sogar gänzlich.\n\nPhonenet erlaubt bei einfachen Daisy Chains Längen von 600 Meter pro Strang, in speziellen Konstellationen bis zu 1200 Meter.\n\nEin weiterer Punkt zur Verbreitung von PhoneNet ist die Tatsache, dass vorhandene Telefoninstallationen in den USA die gleichen Steckverbinder benutzen und somit raumübergreifende Netzwerke ohne Kabelverlegearbeiten gebildet werden können – daher der Name “PhoneNet”. In der Folgezeit wurden von weiteren Herstellern Produkte auf RJ11- Basis veröffentlicht, um Kosten zu sparen teilweise mit manuell zu installierenden RJ11-Abschlusswiderständen statt der nutzerfreundlichen automatischen Terminierung.\n\nMit der steigenden Bedeutung von TCP/IP in Rechnernetzen bot Apple eine Software namens \"MacIP Gateway\" an, die IP-Pakete in AppleTalk-Pakete einbettet. MacTCP bzw. OpenTransport auf Clientseite besitzen in ihren Einstellungen einen Modus, um diese Pakete wieder zu dekapsulieren und so IP auch über das eigentlich nur für AppleTalk geeignete LocalTalk zu nutzen. Auch Cisco hat in IOS entsprechende Funktionen eingebaut, auch wenn LocalTalk von den Routern selbst nicht unterstützt wird.\n\nDurch die geringe maximale Größe von LocalTalk Paketen beträgt die MTU solchermaßen angebundener Rechner lediglich 576 Bytes.\n\nMit der steigenden Verbreitung von Ethernet-Netzwerken in den 1990er Jahren bestand die Notwendigkeit, LocalTalk-Komponenten wie Drucker oder Rechner ohne Ethernet-Schnittstelle zu integrieren (Investitionsschutz).\n\nApple bot hierzu die Software \"LocalTalk Bridge\" an, mit der ein Macintosh mit beiden Schnittstellen als Software-Brücke fungiert. Diese Funktion kann auch mit dem \"Apple Internet Router\" abgebildet werden, wobei diese Software einen vollwertigen AppleTalk-Router für viele Schnittstellentypen abbildet.\n\nVerschiedene Hersteller boten hardwarebasierende Ethernet-LocalTalk-Brücken an, deren Vorteile unter anderem\nwaren.\n\nEinige Hersteller lieferten auch Brücken von Token Ring nach LocalTalk.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "710166", "url": "https://de.wikipedia.org/wiki?curid=710166", "title": "Tonwertspreizung", "text": "Tonwertspreizung\n\nUnter Tonwertspreizung versteht man in der Fotografie die Skalierung des Kontrastumfangs. Dabei werden die Eigenschaften einer Helligkeitsverteilung auf eine größere Skala übertragen.\nDas Gegenteil einer Tonwertspreizung ist die Tonwertreduktion.\n\nJeder Helligkeitswert eines digitalen Bildes wird auf einer bestimmten Skala gespeichert. Beispielsweise besitzt das durchschnittliche Bild einer Digitalkamera eine Skala von 8 bit pro Farbkanal – das entspricht 256 Helligkeitsabstufungen (pro Farbkanal). Jeder Helligkeitswert muss also auf einer dieser 256 Helligkeitsstufen gespeichert werden – Zwischenstufen sind nicht möglich.Die Summe aller Helligkeitswerte eines Bildes können in einem Histogramm dargestellt werden. In der Regel wird horizontal das Spektrum aller Helligkeitswerte angezeigt, während vertikal die Häufigkeit dieser Werte dargestellt werden.\n\nIn der oben gezeigten Original-Bildvorlage (Mitte) wird das Tonwertspektrum (mit 256 Helligkeitsabstufungen) nur ungefähr zur Hälfte ausgenutzt (ein typisches High Key - Foto).Mit Hilfe einer geeigneten (Bildbearbeitungs-) Software kann diese Helligkeitsverteilung gespreizt werden. Die originale Helligkeitsverteilung umfasst ein Spektrum von ungefähr 100 Abstufungen, durch die Spreizung wird diese Helligkeitsverteilung auf ein Spektrum von 256 Abstufungen übertragen (Foto rechts). Dabei entsteht ein neuer Helligkeitseindruck des Bildes.\n\nDurch eine Tonwertspreizung wird immer der Kontrastumfang eines Fotos vergrößert. Dabei entsteht ein höherer Schärfeeindruck – die tatsächliche Schärfe des Bildes verringert sich (durch Interpolationsverluste).\n\nIn der klassischen Fotobearbeitung (im Fotolabor) kann eine Tonwertspreizung durch die Verwendung geeigneter Techniken erreicht werden. Exemplarisch lassen sich nennen:\n\nTonwertspreizungen dienen ausschließlich der Erhöhung des Schärfeeindrucks. Durch den Schärfeverlust muss die Anwendung dieses Verfahrens mit Bedacht gewählt werden.\n\nDie Tonwertreduktion (das Gegenteil der Tonwertspreizung) wird in der klassischen Fotografie als Abschwächen bezeichnet (beispielsweise mit Farmerschen Abschwächern). Dadurch lassen sich Bilder in ihrem Kontrastumfang verringern. Das ist notwendig, wenn der große Tonwertumfang eines bestimmten Mediums gar nicht wiedergegeben werden kann. Typisch dafür ist die Vergrößerung eines Dias auf Fotopapier. Dias haben einen deutlich höheren Kontrastumfang als Fotos (Dia: bis 1:1000; Fotos: max. 1:40).\n"}
{"id": "710618", "url": "https://de.wikipedia.org/wiki?curid=710618", "title": "SheepShaver", "text": "SheepShaver\n\nSheepShaver ist ein freier Apple-Power-Macintosh-Emulator für Unix mit X11 (Linux und BSD), macOS (PowerPC und Intel), Windows und BeOS als Wirtsystem. Als PowerPC-Emulator unterstützt SheepShaver gegenwärtig Mac OS 7.5.2 bis 9.0.4 als Gastsysteme.\n\nDa ShapeShifter und dessen Weiterentwicklung Basilisk II als 68k-Macintosh-Emulator ausgelegt sind können damit nur Mac-OS-Betriebssysteme für 68k-Macintosh-Rechner als Gast ausgeführt werden. Das letzte Mac OS, das diese Rechnerarchitektur unterstützt, ist Mac OS 8.1. Um auch neuere Versionen von Mac OS emulieren zu können bedurfte es daher eines PowerPC-Emulators. Zusammen mit Teilen des Programmcodes von Basilisk II entstand 1998 SheepShaver für BeOS zuerst als Shareware, später als Teil von BeDepot.\n\nDer Name SheepShaver ist eine Anspielung auf ShapeShifter, den 68K-Macintosh-Emulator für den Amiga. Bereits ShapeShifter wie auch Basilisk I und Basilisk II stammen in großen Teilen von Christian Bauer.\n\nNachdem BeOS nicht erfolgreich war, wurde SheepShaver 2002 als Open Source unter der GPL veröffentlicht. Gwenolé Beauchesne, ein Entwickler bei Mandriva Linux, der bereits 1999 zu Basilisk II beigetragen hatte, erweiterte den PowerPC-G4-Emulator ab 2003 um und portierte den Emulator von BeOS nach Linux, Mac OS X und Windows. 2008 stellte er die Arbeit an SheepShaver ein.\n\nSheepShaver war der erste und bis Ende 2005 der einzige verfügbare PowerPC-Emulator mit G4-Prozessor- und AltiVec-Unterstützung. Am 20. Dezember wurde PearPC in Version 0.4.0 ebenfalls mit AltiVec-Unterstützung freigegeben.\n\nMit dem Entfernen der Classic-Umgebung in Mac OS X 10.5 „Leopard“ hat Apple die eingebaute Möglichkeit, alte Mac-Programme auf neuer Apple-Hardware zu nutzen, auch auf PowerPC-Macs genommen. Auf Intel-basierten Macs gab es bereits in der ersten Mac OS X-Version für diese Architektur, 10.4 „Tiger“, kein \"Classic Environment\" mehr. SheepShaver schließt diese Lücke: es ist derzeit (Stand: 2014) die einzige Möglichkeit, für Mac OS geschriebene Programme auf Intel-basierten Macs sowie auf allen Macs, auf denen Mac OS X 10.5 und neuer läuft, zu nutzen. Dabei ist es sowohl auf PowerPC- als auch auf x86-Rechnern stabil und ausreichend schnell.\n\nAls stabil gelten Mac OS 7.5.2 bis 8.6, jedoch wird auch Mac OS 9.0.4 experimentell unterstützt. Die neueren Mac-OS-Betriebssysteme Mac OS 9.1 bis 9.2.2 sind nicht lauffähig, da diese eine MMU voraussetzen, die von der PowerPC-Emulation nicht bereitgestellt wird. macOS ist nicht lauffähig.\n\nZur Ausführung von Mac OS benötigt SheepShaver ein Abbild von einem \"Mac OS ROM\", wobei das verwendete ROM die Kompatibilität zu einer bestimmten Mac-OS-Version festlegt. Bei älteren Macintosh-Computern (später als bezeichnet) war das ROM in seiner Gesamtheit noch fest in einem ROM-Chip auf der Hauptplatine des Computers untergebracht. Dieses kann auf der originalen Hardware ausgelesen und in eine Datei gespeichert werden, die ShapeShifter, Basilisk II und SheepShaver als ROM-Datei verwenden können. Bei neueren Macs (von Apple bezeichnet) wurde eine Trennung zwischen Boot-ROM ( ROM) und “Mac OS ROM” (“ ROM”) eingeführt. Diese ROM-Dateien werden von SheepShaver benötigt, um ein damit unterstütztes Mac OS starten zu können. Das Mac-OS-ROM (“ ROM”) ist ab Mac OS 8.5 für -Macs bereits Bestandteil des Betriebssystems. Das noch benötigte Boot-ROM (“ ROM”) könnte zwar von einem existierenden Power Mac ausgelesen werden, jedoch bietet Apple auch offizielle ROM-Updates an, aus denen das jeweilige ROM als Abbild ebenfalls entnommen werden kann.\n\nSheepShaver emuliert generische PowerPC-Hardware; um zu Mac OS kompatibel zu sein werden die originalen Macintosh-ROMs benötigt. Damit bietet SheepShaver folgende Funktionen:\n\n\nDurch die JIT-Emulation erreicht SheepShaver auf Nicht-PowerPC-Systemen eine annehmbare Geschwindigkeit von in etwa einem Achtel des Wirtssystems. Verglichen mit einem Power Mac G4 mit 400 MHz läuft Mac OS auf einem x86-System mit 3,2 GHz sogar etwas schneller.\n\nDie ursprünglichen Entwickler, Christian Bauer und Marc Hellwig, stellen auf ihrer Homepage nur sehr alte Versionen zur Verfügung. Auch auf der Homepage von Gwenolé Beauchesne, die inzwischen nicht mehr erreichbar ist, ist die letzte Version aus dem Jahr 2008.\n\nAuf E-Maculation gibt es eine aktive Entwicklergemeinde, die aktuelle Builds für verschiedene Betriebssysteme bereitstellt. Mit Stand 2014 wird sogar OS X 10.9 als Wirtsystem unterstützt.\n\nDa SheepShaver unter der GPL steht, können viele Entwickler zum Fortbestand des Emulators beitragen. Derzeit wird Basilisk II und SheepShaver (die Teile des Quelltextes gemeinsam verwenden) auf GitHub bereitgestellt.\n\nNeben den Builds für Linux (PowerPC sowie x86 32-Bit und 64-Bit), Mac OS X (PowerPC und Intel 32-Bit) und Windows (32-Bit) gibt es auch Binaries für Darwin, FreeBSD, Mac OS X (Intel 64-Bit), NetBSD und Windows (64-Bit).\n\n\n"}
{"id": "711692", "url": "https://de.wikipedia.org/wiki?curid=711692", "title": "Microsoft OneNote", "text": "Microsoft OneNote\n\nMicrosoft OneNote ist eine Software von Microsoft, die den PC als digitalen Notizblock nutzt.\n\nDie Software wird unter macOS zusammen mit dem Office-Paket oder als kostenloser Download vertrieben. Unter Windows wird OneNote als Windows-10-App (UWP) vertrieben, es löste die ehemalige, im Office-Paket mitgelieferte Win32-Anwendung ab.\n\nMicrosoft warb damit, dass mit OneNote elektronische Notizen geordnet und leicht wiedergefunden werden können. Der OneNote-„Notizblock“ ist in Abschnitte mit Unterabschnitten gegliedert, die jeweils mehrere Seiten enthalten können. Innerhalb einer Seite können die Seitenelemente verschoben werden. Text kann nicht nur eingetippt, sondern mit einem Tablet-PC auch handschriftlich eingegeben werden. OneNote integriert sich in verschiedene Office-Programme von Microsoft, wodurch Notizen, z. B. in Microsoft Word, oder Kontaktinformationen in Microsoft Outlook leicht aus OneNote übernommen werden können.\n\nNotizen können dabei untereinander verknüpft werden. Eingefügte Inhalte werden dabei automatisch mit ihrer Herkunft verlinkt. So werden z. B. eingefügte Textabschnitte aus Webseiten automatisch mit der entsprechenden URL versehen. In OneNote können Bilder, Videodateien und Texte abgelegt werden. Durch die integrierte Text- und Spracherkennung können Texte in eingefügten Bildern (z. B. von Screenshots) und Sprachdateien (Diktate) nach Stichwörtern durchsucht werden.\n\n\"Sammlungen\" in OneNote 2016 vom Office-Paket bestehen aus einer Datei- und Ordnerstruktur, die sich auf lokale Datenträger und Netzwerkfreigaben speichern lässt. Letztere erlauben ein gemeinsames Bearbeiten mit anderen Nutzern oder den Zugriff von unterschiedlichen Geräten aus. Zusätzlich – unter macOS ausschließlich – lassen sich „Notizbücher“ auch auf SharePoint und in Microsofts Cloud-Dienst OneDrive ablegen, was ebenfalls eine Bearbeitung mit mehreren Teilnehmern und die Synchronisation der Inhalte über verschiedene Geräte aller unterstützten Plattformen erlaubt.\n\nMit der Version \"OneNote 2003\" kam das Produkt im November 2003 erstmals auf den Markt. Es gab drei Service Packs bis im Januar 2007 \"OneNote 2007\" veröffentlicht wurde. Im Juli 2010 folgte \"OneNote 2010\" und im Januar 2013 \"OneNote 2013\". Die aktuelle Desktop-Version wird seit September 2015 als \"OneNote 2016\" in Verbindung mit Microsoft Office (MS Office 2016, auch in der Abonnement-Version Office 365) und als \"OneNote\" in abgewandelter Form mit Windows 10 verbreitet.\n\nSeit Januar 2011 ist von Microsoft eine Apple-iOS-App über den App Store sowie seit Februar 2012 eine Android-Version über Google Play zum kostenlosen Download erhältlich. Seit dem 17. März 2014 ist OneNote auch für Mac OS X erhältlich. Damit ist auch die Synchronisation von Notizen auf einem Windows-PC oder einem Mac und einem Smartphone mit Windows-Phone-, Android- oder iOS-basierten Geräten über OneDrive-Konten möglich.\n\nSeit Mitte März 2014 ist OneNote auf allen Plattformen kostenlos nutzbar, wobei Benutzer gekaufter Office-Versionen zusätzliche Funktionen bekommen.\n\nNachdem unter Windows jedoch schon lange keine Featureupdates für OneNote erschienen ist, ist in Office 2019 für Windows kein OneNote mehr enthalten. In dessen Folge wird auch mit dem Microsoft Office 365 Build 1812 kein OneNote mehr mitgeliefert. Eine \"Standalone\"-Variante für x86 und x64 wird von Microsoft derzeit separat angeboten. Die als Nachfolger empfohlene UWP-App unterstützt keine lokalen Notizbücher mehr (wie unter macOS nur noch OneDrive) und ist für Tablets und die Stift-Nutzung optimiert. Unter macOS wird OneNote wie bisher weiterentwickelt und mit Office 2019 für Mac mitgeliefert.\n\n\n"}
{"id": "712337", "url": "https://de.wikipedia.org/wiki?curid=712337", "title": "Media Composer", "text": "Media Composer\n\nMedia Composer bezeichnet ein professionelles, soft- und hardwarebasierendes Programm der Firma Avid für die Nachbearbeitung, insbesondere das Schneiden von Videoaufnahmen in der Fernseh- und Videoproduktion. Es ermöglicht nicht-linearen Videoschnitt (NLE) und ist für die Fernsehnormen PAL und NTSC ausgelegt. Die Schwester-Applikation für die Spielfilm-Nachbearbeitung heißt \"Film Composer\", die nativ mit 24-Hertz-Material umgehen kann. Beide Systeme sind weltweiter Industriestandard.\n\nDas Wort \"Avid\" wird sehr oft als eine Synonymie für Produkte der Media Composer-Familie gebraucht. \n\nAnfang der 1990er Jahre wurde das erste non-lineare System auf Basis der Apple Macintosh-Plattform und TrueVisions NuVista-Hardware entwickelt. Diese Lösung ermöglichte es erstmals, effektiv Video- und Audiomaterial auf einem Computer nachzubearbeiten. Zu dieser Zeit setzte jedoch die technische Entwicklung der Hardware die Grenzen. Um dennoch eine Bearbeitung zu ermöglichen, setzte man auf verlustbehaftete Kompression der Videodaten. Daher war zunächst nur Offline-Editing möglich. Das Online-Editing erfolgte anschließend an linearen Systemen oder am klassischen Schneidetisch mit Hilfe einer Edit Decision List (EDL).\n\nMit Version 6.1 wurde das Avid Broadcast Video Board (ABVB) eingeführt. Die damit erreichbare beste Auflösung \"AVR 77\" wurde von Fernsehsendern allgemein akzeptiert, obwohl sie dynamisch komprimiert und maximal den Faktor 2/1 erreicht hat.\n\nMit der Version 8 wurde die Meridien-Hardware eingeführt, die das Bearbeiten von unkomprimierten SD-Signalen ermöglichte.\n\n2003 wurde die Meridien-Hardware durch die DNA-Hardware (DNA = Digital Nonlinear Accelerator) abgelöst, die über FireWire angeschlossen wird, und dazu die Version 1 des Media Composer HD vorgestellt. Damit wurde auch die eingeschränkte Bearbeitung von HD-Formaten möglich.\n\n2008 wurde mit der Version 3 der Media Composer HD Software die DX-Hardware vorgestellt, welche über ein PCIe Host Interface Board angeschlossen wird. Mit dieser Hardware ist auch die Bearbeitung von unkomprimierten HD-Formaten möglich. Im Sommer 2010 erschien die Version 5, die entweder mit der im unteren Preissegment angesiedelten Mojo-Hardware oder der professionellen, aber erheblich teureren Nitris-DX-Hardware betrieben wird. Der Media Composer 5 unterstützt den Import aller aktuellen hochauflösenden Videoformate, als systemeigener Codec kommt Avid DNxHD zum Einsatz.\n\nEs wird oft eine spezielle Tastatur mit farbigen Tasten benutzt, wobei verschiedene Farben die einzelnen Funktionsgruppen unterscheiden. Die Bild- und Tondaten sind im System so organisiert, dass auch bei großen Datenmengen ein sicheres Arbeiten möglich ist. Für den Einsatz in Rundfunkanstalten stehen Netzwerk-Lösungen zur Verfügung, die es ermöglichen, auf das Videomaterial auch von mehreren Arbeitsplätzen aus zuzugreifen. Das effektive Arbeiten mit einem Schnittsystem von \"Avid\" ist in der Regel erst nach einer Einarbeitungszeit gegeben, deren Dauer von den Vorkenntnissen des Nutzers abhängig ist. Mehrere Drittanbieter stellen Plugins zur Verfügung, mit denen die Möglichkeiten im Effekt- und Compositingbereich erweitert werden.\n\n\n"}
{"id": "712390", "url": "https://de.wikipedia.org/wiki?curid=712390", "title": "Schachserver", "text": "Schachserver\n\nEin Schachserver (engl. \"Internet chess server\") ermöglicht das Schachspielen mit räumlich entfernten Gegnern über das Internet. Technisch gesehen sind Schachserver eine Untergruppe von Spieleservern. Teilweise kann man direkt im Webbrowser spielen, teilweise muss man eine eigene Zugangssoftware installieren.\n\nEs gibt eine Vielzahl an freien und kommerziellen Schachservern, die sich in ihrer Funktionsvielfalt und Bedienung stark unterscheiden. Als eine Sonderform kann man jedoch Fernschachserver abgrenzen, die im Fernschach die Partieverwaltung sehr erleichtern.\n\nEin Schachserver ermöglicht das Spielen einer Partie, in der beide Gegner während der gesamten Partie angemeldet bleiben. Es lassen sich verschiedene Zeitkontrollen und teilweise auch Schachvarianten wählen. Viele Schachserver bieten sogenannte 1-Klick-Eingaben von Zügen an, mit denen sich Züge sehr schnell ausführen lassen bzw. schon als Antwortzug auf einen beliebigen Gegnerzug festlegen lassen. Damit sind sehr schnelle Zeitkontrollen möglich (eine Minute pro Partie), die als Bullet bezeichnet werden. Durch die ständige Verfügbarkeit von Gegnern haben Schachserver teilweise zu einer Popularisierung seltener Schachvarianten geführt, wie z. B. Einsetzschach oder Tandemschach. Auf Schachservern kann man auch Schachprogramme gegeneinander antreten lassen. Einige Autoren von Schachprogrammen nutzen Schachserver, um ihr Programm gegen menschliche Gegner und andere Programme zu testen.\n\nEin großes Problem bei Schachservern ist der Betrug durch Computerunterstützung während der Partien. Einige Schachserver haben deshalb Algorithmen entwickelt, mit denen sie die Partien auf Computerunterstützung untersuchen. Die Funktionsweise dieser Algorithmen ist umstritten und geheim, dennoch gehen zumindest die kommerziellen Schachserverbetreiber rigoros gegen Betrüger vor, um Beschwerden aus dem Nutzerkreis zu begegnen.\n\nEin Fernschachserver ermöglicht das Spielen von Partien mit sehr langen Bedenkzeiten (mehrere Tage pro Zug), in der sich die Gegner nur für die Zugabgabe anmelden und danach wieder abmelden. Auf einem Fernschachserver führt man die Züge auf einem anklickbaren Brett auf einer Webseite aus und bestätigt nach einer Vorschau den Zug. Damit wechselt das Zugrecht, und die Bedenkzeit des Gegners fängt an zu laufen. Der Gegner wird über den erfolgten Zug per E-Mail informiert. Fernschachserver erfordern in der Regel keine eigene Zugangssoftware, sondern werden direkt im Browser bedient.\n\nFernschachserver haben in den letzten Jahren unter Fernschachspielern sehr an Popularität gewonnen. Das Übermitteln der Züge im Fernschach stellte immer eine technische und organisatorische Herausforderung dar. Neben langen Postlaufzeiten kam es auch zu Missverständnissen und Fehlern in der Übertragung sowie zu Auseinandersetzungen über die verbrauchte Bedenkzeit. Durch Fernschachserver ist einerseits eine Übermittlung der Züge ohne Zeitverlust möglich, und zum anderen werden der Partieverlauf und die Bedenkzeit auf dem Server verwaltet, so dass es zu keinen Missverständnissen über den Partieverlauf kommen kann. Die meisten Fernschachserver bieten darüber hinaus Zugvalidierung und Verwaltung der Urlaubszeiten in einem Turnier an.\n\nLichess ist ein werbefreier, kostenloser Schachserver, der komplett mit Freier Software läuft. Getragen wird die Serverinfrastruktur durch Spenden. Als Wertungssystem kommt das Glicko-2-System zur Anwendung. Es lassen sich dort auch u. a. die Schachvarianten Chess960, Einsetzschach (Crazyhouse) und Atomschach spielen. Die Analyse läuft verteilt unter anderem auch auf Rechnern der Anwender. So ist es möglich auch Rechenleistung zu spenden.\n\nEin gut besuchter Server mit kostenloser Vollmitgliedschaft ist der sogenannte Free Internet Chess Server.\n\nDer Server wird durch freiwillige Helfer gewartet. Freiwillige spielen in Echtzeit Meisterspiele, die im Fernsehen übertragen wurden, auf dem Server nach, so dass von jedem Internet-Anschluss aus mit Gleichgesinnten über die Partie diskutiert werden kann.\n\nRegistrierte Spieler erhalten Wertungspunkte, die nach jedem Spiel auf den neusten Stand gebracht werden. Die Spielstärke reicht vom absoluten Anfängerniveau bis zu gehobenem Vereinsniveau, aber auch einige Spieler mit Meisterniveau sind regelmäßig online. Einzigartig ist, dass das Rating mit dem zusätzlichen Indikator RD – \"ratings deviation\" (engl. für „Spielstärkeabweichung“) – verfeinert wird. Spielt ein Spieler über lange Zeit mehrmals täglich, dann kann man seine Spielleistung zuverlässiger bestimmen, und er hat zum Beispiel bei einer Wertung von 1500 etwa einen „wahren“ Wert zwischen 1485 und 1515; hat er mehrere Monate nicht mehr gespielt, einen wahren Wert zwischen 1150 und 1800.\n\nDer Server bietet neben einigen freien Zugangsprogrammen wie beispielsweise dem Programm \"Winboard\" (für Unix: xboard oder eboard) auch die Möglichkeit, sich per Java-Client einzuloggen. Ein besonderer Service ist die sogenannte \"Teaching Ladder\", auf der Spieler gegenseitig das Spiel der anderen Spieler analysieren können.\n\nObwohl nicht unbedingt Pflicht, bietet sich die Anmeldung auf dem Server an. Da unregistrierte Spieler keine Spielstärkebewertung haben, lehnen viele registrierte Spieler das Spiel gegen nicht registrierte Gegner ab.\n\nchessmail.de ist ein Schachserver, der per Browser Schachspiele gegen reale Spieler und auch Schach-Engines ermöglicht. Es können sowohl Turniere als auch Einzelpartien gespielt werden. chessmail ist seit Mai 2003 online und bisher kostenlos. Die Mitgliederzahl der Community betrug im April 2014 über 6000 aktive Schachspieler.\n\nAuf chessmail wird u. a. eine moderne Variante von Fernschach (Briefschach) gespielt. Das heißt, die Schachzüge werden asynchron ausgeführt, es müssen nicht beide Spieler gleichzeitig online sein. Die Bedenkzeit pro Zug kann zwischen 24 Stunden und 30 Tagen eingestellt werden. Eine Urlaubsregelung erlaubt das Pausieren, also eine definierte Überschreitung der Bedenkzeit.\n\nMit einem an die Elo-Zahl angelehnten Wertungssystem wird die Spielstärke der Nutzer ermittelt. \n\nÜber chessmail kann man an Schachturnieren teilnehmen oder auch selbst Turniere organisieren. Halbjährlich findet das chessmail Pokalturnier statt, bei dem in den drei Sektionen Hobbyspieler, Fortgeschrittene und Experten ein virtueller Pokal gewonnen werden kann. Zusätzlich zu den vollautomatischen, durch chessmail verwalteten Turnieren werden von den chessmail Nutzern Turniere mit Hilfe des chessmail Wikis organisiert.\n\nNutzer können auf chessmail gegen einen \"online Schachcomputer\" mit unterschiedlicher Spielstärke antreten. Die Züge werden von der GNU Chess Engine berechnet. chessmail ist damit der einzige Schachserver, der Spiele gegen den unter Linux ältesten Schachcomputer GNU Chess erlaubt. Die Engine-Spiele können, wie Spiele gegen reelle Personen, über mehrere Tage gespielt werden.\n\nRegistrierte Mitglieder können einem \"online Schachclub\" beitreten oder einen eigenen Club gründen. In chessmail sind inzwischen (Stand April 2014) ca. 50 solcher virtuellen Schachclubs enthalten, die auch teilweise als Internet-Präsenz von echten Schachvereinen genutzt werden. Ein chessmail Schachclub hat einen eigenen Bereich im chessmail Wiki und ein eigenes Forum.\n\nVon den chessmail-Nutzern wird das \"chessmail Wiki\" gepflegt. Es ist insbesondere eine Plattform für die Organisation von Turnieren, deren Durchführung über die Möglichkeiten der automatischen chessmail Turniere hinausgeht. Außerdem befinden sich im chessmail Wiki noch weitere Informationen über Schach und chessmail selbst.\n\nchessmail.de ist seit Januar 2011 zusätzlich zur Desktop-Version auch als mobile Variante für Smartphones und Tablet-PCs verfügbar.\n\n\n\nDer bereits seit 1997 aktive Schachserver freechess.de war früher der größte Browser-basierte Schachserver für Fernschach, inzwischen ist seine Bedeutung rückläufig, da andere Angebote technisch und schachlich (Angebote von Schachvarianten etc.) attraktiver sind.\nEs gibt Einzel- und Mannschaftsturniere, Klassen- und Ranglistenturniere, Cups für Frauen, Senioren und Jugendliche, eine Meisterschaft, ein Wanderpokal und vieles mehr. freechess.de betreibt ein eigenes Rating-System: Die Wertungszahl FCR (frechess-Rating) ist kompatibel mit der DWZ-Zahl des Deutschen Schachbundes, nicht jedoch mit den eher relevanten Rating des BdF (FWZ) oder der ICCF.\n\nSeit 2000 ist remoteSchach.de online. Bis September 2008 trug der Deutsche Fernschachbund seine Turniere auf diesem Gastserver aus, bis er einen eigenen Fernschachserver in Betrieb nahm. Der Verein Chess in Friendship trägt seine Turniere weiterhin dort aus. remoteSchach bietet Einzel- und Mannschaftsturniere verschiedener Arten, umfangreiche Statistiken und Werkzeuge, die Spielstärke wird in RS-Elo gemessen, und ist kompatibel zu ICCF-Wertungszahlen. Des Weiteren verfügt der Server neben dem Webfrontend über eine Windows-Software, mit dem die Partien auch offline verwaltet werden können und man nur zum Übertragen der Züge online sein muss. remoteSchach.de ist Mitveranstalter des \"1. Deutschen Fernschach-Server-Open\", zusammen mit dem Deutschen Fernschachbund und weiteren Schachservern.\n\nSeit September 2008 ist der Fernschachserver des Deutschen Fernschachbundes e. V. am Netz. Auf ihm werden neben den Einzel- und Mannschaftsturnieren nach den herkömmlichen Schachregeln auch Turniere in Chess960 ausgetragen. Weiterhin ermöglicht der BdF-Schachserver die Austragung von Rapid-Fernturnieren, in denen mit einer stark verkürzten Bedenkzeit gespielt wird.\n\n\n\n"}
{"id": "712785", "url": "https://de.wikipedia.org/wiki?curid=712785", "title": "Atanasoff-Berry-Computer", "text": "Atanasoff-Berry-Computer\n\nDer Atanasoff-Berry-Computer („ABC“) war einer der ersten elektronischen Digitalrechner und wurde von John Atanasoff und Clifford Berry in den Jahren 1937–1941 am Iowa State College gebaut. Die Maschine basierte auf dem binären Zahlensystem. Ihr einziger Zweck war das Lösen großer linearer Gleichungssysteme. Ein Computer im modernen Sinne war der ABC nicht, da er nicht frei programmierbar war.\n\nDer 1937 konzipierte Röhrencomputer war in der Lage, gleichzeitig 29 lineare Gleichungen zu verarbeiten und wurde erfolgreich getestet. Als seine Erfinder 1942 aufgrund von Kriegseinsätzen die Universität verließen, war der Mechanismus allerdings immer noch unzuverlässig und störanfällig. Viele wichtige Elemente der modernen Computertechnik wurden im ABC das erste Mal eingesetzt, etwa Bausteine für binäre Arithmetik.\n\nJohn Atanasoffs Arbeiten wurden erst in den 1960er Jahren einer breiten Öffentlichkeit bekannt. Zu diesem Zeitpunkt ging es in einem Patentstreit um die Frage, wer den ersten automatischen elektronischen Digitalrechner in den Vereinigten Staaten von Amerika entwickelt hatte. Als solcher wird zumeist der ENIAC gehandelt, allerdings entschied ein US-Bezirksgericht 1973 in einem umstrittenen, aber unangefochtenen Urteil, dass die ENIAC-Patente unwirksam wären, da der ABC als erster Computer zu betrachten sei.\n\nAtanasoff wurde im Rahmen einer Zeremonie im Weißen Haus durch US-Präsident George H. W. Bush am 13. November 1990 die National Medal of Technology verliehen.\n\nAuch wenn noch nicht alle Kriterien, die moderne Computer auszeichnen, erfüllt waren, so wurden doch drei wichtige Aspekte realisiert:\n\nZur Datenhaltung wurden Trommelspeicher eingesetzt, die eine Speicherkapazität von insgesamt ca. 3000 Bits hatten. Pro Sekunde konnten ca. 30 Rechenoperationen (Additionen/Subtraktionen) durchgeführt werden.\n\nBenutzereingaben erfolgten per Lochkarte, zur Ausgabe gab es eine Anzeige an der Vorderseite. Als Zwischenspeicher dienten Papierbahnen, die das Gerät beschreiben und wieder einlesen konnte, eine fehleranfällige Speichertechnik, denn da man noch keine Paritätsbits kannte, konnte man nicht erkennen, wann ein geschriebener Wert fehlerhaft wieder eingelesen wurde. Die Fehlerrate lag bei einem Fehler pro 100.000 Operationen.\n\nDer Original-ABC wurde entsorgt, als die Universität von Iowa den Keller, in dem er aufgebaut war, in Unterrichtsräume umwandelte. Alles, was übrig blieb, war eine einzelne Speichertrommel. 1997 gelang es einem Forscherteam des Ames Laboratory, welches sich auf dem Campus der Iowa State University befindet, einen funktionsfähigen Nachbau fertigzustellen. Mit dieser Replik wurden letzte Zweifel an ihrer Funktionsfähigkeit ausgeräumt. Die Kosten dieses Projektes beliefen sich auf 350.000 US-Dollar. Der neue ABC ist nun im Durham Center for Computation and Communication an der Iowa State University im Rahmen einer Dauerausstellung für die Öffentlichkeit zugänglich.\n"}
{"id": "713699", "url": "https://de.wikipedia.org/wiki?curid=713699", "title": "Dynabook", "text": "Dynabook\n\nDas Dynabook war ein konzeptionelles Computer-System, das in den frühen 70er Jahren von Alan Kay am Xerox PARC entwickelt wurde. Es sollte die Zusammenführung von intuitiver Benutzbarkeit und Programmierung mit einer hochwertigen grafischen Ausgabe und einer leistungsfähigen, aber preiswerten Hardware sein.\n\nGeplant war, dass die Eingabe mit Hilfe einer integrierten Tastatur erfolgt. Allerdings sollten sich die Eingabemöglichkeiten nicht darauf beschränken, da auch angedacht war, sensomotorische Fähigkeiten des Benutzers zu berücksichtigen, wie es bei vielen modernen Geräten bereits Realität ist. Der Bildschirm sollte direkt im Gerät integriert sein. Auch war eine grafische Benutzeroberfläche geplant. Mit einer Größe von 12\" × 9\" × 0,75\" (was in etwa 30 × 23 × 2 cm entspricht) sollte das Gerät nicht größer sein als ein Notizbuch, um es immer dabei haben zu können. Laut Alan Kay war geplant, den Verkaufspreis möglichst niedrig anzusetzen (maximal $ 500), um einer großen Anzahl von Menschen den Zugang zu ermöglichen.\n\nKay lernte 1968 Seymour Papert kennen, der am MIT die Möglichkeiten des Computers bei der Erziehung von Kindern untersucht und dazu die Programmiersprache Logo entwickelt hatte, mit der die Kinder einer Grundschule das Programmieren erlernten. Über Papert lernte Kay die Lerntheorien von Jerome Bruner und Jean Piaget kennen. Sie waren Pioniere einer Entwicklungs- und Erziehungstheorie, die sich auf die kognitive Psychologie stützte.\n\nNach Kay sollte ein Computer bedingungslos an die Fähigkeiten und Bedürfnisse des Menschen angepasst werden und er folgerte, dass ein Dynabook daher nicht nur auf der symbolischen Ebene zu bedienen sein durfte, sondern auch die sensomotorischen und ikonischen Fähigkeiten des Bedieners unterstützen musste (Zitat Alan Kay: “Doing with Images makes Symbols”).\n\nBei Xerox PARC begann Kay mit dem Entwurf eines Prototyps für das Dynabook mit dem Namen miniCOM. Er wurde ab 1972 zum Ausgangspunkt für die Konstruktion eines kleinen interaktiven Computers. Die Ideen hinter dem Dynabook-Konzept führten zur Entwicklung des Xerox-Alto-Computers, eines Prototyps, der schon im Jahr 1972 alle Elemente der grafische Benutzeroberfläche (GUI) aufwies. Dabei wurde auch die richtungsweisende Programmiersprache Smalltalk entwickelt.\n\nDas Dynabook-Konzept prägte das heutige Verständnis eines tragbaren Computers.\n\n"}
{"id": "716026", "url": "https://de.wikipedia.org/wiki?curid=716026", "title": "Fahrradcomputer", "text": "Fahrradcomputer\n\nEin Fahrradcomputer (schweizerisch Velocomputer) oder Fahrradtacho ist ein elektronisches Gerät zur kontinuierlichen Messung von Geschwindigkeit und der zurückgelegten Wegstrecke beim Fahrradfahren. Fast alle Fahrradcomputer haben weitere Funktionen, beispielsweise die Anzeige der Uhrzeit und Trittfrequenz.\n\nSchon beim Aufkommen des Radsports entstand der Wunsch, insbesondere beim Training Daten über gefahrene Distanzen und erreichte Geschwindigkeiten zu erhalten. Einfache mechanische Lösungen, die daraufhin entwickelt wurden, waren Kilometerzähler und Fahrradtachometer.\n\nBeim Kilometerzähler wurde durch einen in den Speichen befestigten Mitnehmer bei jeder Umdrehung ein Zählwerk um einen Zählbetrag erhöht. Da der mit jeder Radumdrehung zurückgelegte Weg gleich dem Umfang des Rades ist, wurden für unterschiedliche Felgendurchmesser jeweils passende Kilometerzähler angeboten. Eine Ungenauigkeit ergab sich dadurch, dass auf Felgen mit dem gleichen Durchmesser Reifen mit unterschiedlichen Abrollumfängen montiert werden können. Meist wurden Kilometerzähler direkt an der Vordergabel des Fahrrades befestigt. Trotz des großen Abstands zum Gesicht des Fahrers waren die meisten Kilometerzähler während der Fahrt ablesbar. Diese mechanischen Zähler wurden gelegentlich auch Zyklometer genannt.\n\nEine Weiterentwicklung stellten mechanische Tachometer dar, die zusätzlich die Geschwindigkeit anzeigten. Hier war ein Aufnehmer an der Achse des Vorderrades angebracht, der mit der üblicherweise am Lenker angebrachten Anzeigeeinheit durch eine biegsame Welle mit dem Aufnehmer verbunden war. Auch diese Tachometer wurden passend zur Felgengröße angeboten. Die Anzeigeeinheit war für meist für alle Radgrößen einheitlich, lediglich die Übersetzung des Antriebes bzw. Aufnehmers wurde der Radgröße angepasst. Einige Modelle hatten neben dem Kilometerzähler einen Tageskilometerzähler. Mechanisch angetriebene Tachometer sind für gängige Reifengrößen auch heute noch als „Retro“-Artikel in verschiedenen Ausführungen erhältlich.\n\nMit der Entwicklung der Mikroelektronik und LC-Displays konnten ab den 1980er Jahren Kleinstrechner mit geringer Energieaufnahme für den mobilen Einsatz hergestellt werden. Zu dieser Zeit kamen die ersten Fahrradcomputer auf den Markt. Das Funktionsprinzip ist bis heute in den meisten Fällen gleich geblieben: Ein kleiner Magnet an einer Speiche induziert in einer an der Gabel befestigten Spule einen Spannungsimpuls; alternativ löst der Speichenmagnet einen Magnetschalter aus. Dieses Signal wird zur Anzeigeeinheit, dem eigentlichen Computer, weitergeleitet. Früher geschah das ausschließlich mit Hilfe eines Kabels, seit ca. 1995 werden (mit steigender Tendenz) auch drahtlose Systeme angeboten. Die neueste Generation von Fahrradcomputern verwendet dazu codierte digitale Signale, die kaum noch anfällig für elektromagnetische Störungen sind. Die Impulse werden elektronisch gezählt, weiterverarbeitet und schließlich in den gewünschten Einheiten angezeigt. Die Geräte brauchen nicht mehr für eine bestimmte Radgröße produziert zu werden, da der Radumfang millimetergenau eingestellt werden kann.\n\nDie fortschreitende Entwicklung der Elektronik ermöglichte es in den 1990er Jahren, zahlreiche zusätzliche Funktionen einzubauen. Mittlerweile Standard sind Durchschnittsgeschwindigkeit und Höchstgeschwindigkeit.\n\nWeitere verbreitete Funktionen sind:\n\nDer Abrollumfang hängt nicht nur von der angegebenen Felgengröße (z. B. 26 oder 28 Zoll ab), sondern auch von der Reifenbreite, denn je breiter ein Reifen ist, desto höher sind seine Seitenwände. Folgende Werte können dabei als Orientierung gelten:\n\nSeit etwa 2002 gibt es GPS-Geräte, die nur wenig größer als Armbanduhren sind. Die regelmäßige Bestimmung der Position ermöglicht die Anzeige der grundlegenden Daten, also der Geschwindigkeit und der zurückgelegten Strecke. Oft wird auch die Höhe angezeigt, prinzipbedingt jedoch recht ungenau. Bessere Modelle besitzen einen präziseren barometrischen Höhenmesser. Eine Aufzeichnung dient der späteren Auswertung der Fahrt. Manche Outdoor-GPS-Geräte können außerdem Daten von unterschiedlichen Sensoren (Geschwindigkeit, Trittfrequenz, Herzfrequenz) empfangen, anzeigen und aufzeichnen.\nFalls durch Gebäude, Baumbewuchs, Tunnel usw. nicht hinreichend freie Sicht zum Himmel besteht, kann der GPS-Empfang erheblich beeinträchtigt und damit Daten verfälscht oder nicht vorhanden sein. GPS-Geräte mit hinterlegten Karten gleichen GPS-Daten mit den Kartendaten ab und erreichen so hinreichende Genauigkeit.\n\nAuch einige Smartphones und PDAs eignen sich mit entsprechender Software, Ausstattung (insbesondere GPS) und optional mit Trittfrequenzsensor als Fahrradcomputer. Allerdings sind diese Geräte in der Regel nicht vor Regen geschützt und haben oftmals nur eine kurze Akku-Betriebsdauer. Es sollte daher bei der Auswahl der Software auf Stromsparfunktionen geachtet werden. Durch die Mitnahme von zusätzlichen oder externen Akkus kann die Laufzeit verlängert werden. Zur sicheren und auch vor Regen geschützten Befestigung am Fahrrad werden entsprechende Halterungen im Fachhandel angeboten. Als zusätzliche Funktion gegenüber dedizierten Fahrradcomputern bieten einige Apps für Smartphones die Anzeige des Fahrtverlaufs auf Karten an, es kann eine Navigation entlang empfohlener Fahrradrouten erfolgen, und die gefahrenen Routen lassen sich aufzeichnen. Für die Pulsaufzeichnung und die Berechnung des Energieumsatzes ist bei Bedarf ein Trainingscomputer oder eine Pulsuhr zusätzlich mitzuführen.\n\n"}
{"id": "716994", "url": "https://de.wikipedia.org/wiki?curid=716994", "title": "PE Builder", "text": "PE Builder\n\nPE Builder ist ein Programm von Bart Lagerweij, das mit Hilfe einer Windows-XP- oder Windows-Server-2003-CD eine Live-CD oder -DVD erstellen kann, die ein bootbares „Mini-Windows“ namens BartPE enthält.\n\nBis etwa 2002 gab es nur auf DOS basierende Boot-Disketten und -CDs. Da damit nur eingeschränkter Schreib- und Lesezugriff für das Dateisystem NTFS möglich war, entwickelte Bart Lagerweij den PE Builder, um eine Windows-PE-ähnliche, aber erweiterte Umgebung zu erstellen.\n\nDer PE Builder bringt bereits einige Plugins mit, welche sich in das „Mini-Windows“ integrieren lassen. Darüber hinaus kann man auch weitere selbst erstellte oder von anderen Autoren bereitgestellte Plugins für nahezu jede Software hinzufügen. \nDie erstellten Daten können entweder sofort auf CD/DVD gebrannt oder in ein ISO-Abbild geschrieben werden, das sich mit einer Brennsoftware auf CD/DVD brennen lässt.\nDas Programm kann auch mittels Slipstreaming aus Windows-CDs ohne Service Pack eine Bart-PE-CD mit Service Pack erstellen. Dazu muss vorher das entsprechende Service Pack heruntergeladen werden.\n\n"}
{"id": "721815", "url": "https://de.wikipedia.org/wiki?curid=721815", "title": "QuickTime Broadcaster", "text": "QuickTime Broadcaster\n\nDer QuickTime Broadcaster ist ein vom Unternehmen Apple entwickeltes kostenloses Programm, das Audio- und Video-Signale in Echtzeit komprimiert und als entsprechenden Live-Stream bereitstellt.\n\nDer QuickTime Broadcaster verwendet das QuickTime-Framework, um damit MPEG-4-, 3GPP- und QuickTime-kompatible Live-Streams zu erzeugen. Diese Streams können direkt oder mit einem Streaming-Server über das Internet oder ein Netzwerk bereitgestellt werden.\n\nProgramme oder Geräte, wie der QuickTime Player, der Video LAN Client, die PlayStation Portable oder moderne Mobiltelefone, können diese Audio- oder Video-Streams live wiedergegeben.\n\n"}
{"id": "722360", "url": "https://de.wikipedia.org/wiki?curid=722360", "title": "GEOS (8-Bit-Betriebssystem)", "text": "GEOS (8-Bit-Betriebssystem)\n\nGEOS (\"Graphic Environment Operating System\") ist ein Betriebssystem mit grafischer Benutzeroberfläche für Homecomputer, das zusätzlich Büroanwendungen besitzt. Der Hersteller war Berkeley Softworks, der sich später in GeoWorks Corporation umbenannte.\n\nGEOS wurde für die 8-Bit-Computer C64, C128 sowie Apple II 128k und Apple IIc/e veröffentlicht. Eine Crackergruppe veröffentlichte inoffiziell eine Commodore-Plus/4- und C16-Version. Die 8-Bit-Varianten von GEOS nennen sich den Computermodellen entsprechend GEOS 64, GEOS 128, GEOS PLUS/4 sowie Apple II GEOS.\n\nMit Einführung der zweiten Version des C64, C64C in einem flachen Gehäuse, lag diesem Homecomputer ab Ende 1986 zunächst für einige Zeit mit GEOS 1.2 ein Betriebssystem mit einer grafischen Benutzeroberfläche mit integrierten Anwendungsprogrammen (Text- und Bildverarbeitung \"geoWrite\" und \"geoPaint\") bei, die derjenigen des Apple Macintosh ähnelte. Auf dem deutschsprachigen Markt war diese Version, die ausschließlich in englischer Sprache vorlag, als einzelnes Produkt offiziell nicht erhältlich. Mit Erscheinen der ersten auch für den deutschsprachigen Markt lokalisierten Version 1.3 und später auch 2.0 konnten interessierte Anwender beim deutschen Distributor Markt & Technik diese einzelne Diskette nach Entrichtung eines speziellen Updatepreises (für GEOS 1.3: 39 DM statt 69 DM) gegen die jeweils aktuelle Version austauschen.\n\nDie Version 1.3 enthielt eine ins Deutsche übersetzte Oberfläche. Neben anderen kleinen Verbesserungen bot diese dem Anwender erstmals unter GEOS ein deutsches Tastaturlayout und ermöglichte die Verwendung von Umlauten. Des Weiteren wurden die von Commodore für C64/128 angebotenen RAM-Erweiterungen unterstützt. Sie konnten unter GEOS als eigene Laufwerke, sogenannte RAM-Disk, angesprochen werden und beschleunigten auch die Benutzung existierender Diskettenlaufwerke.\nGEOS 1.3 wurde auf zwei beidseitig bespielten 5,25 Zoll-Disketten ausgeliefert. Die erste Diskette enthielt auf der Vorderseite die sogenannte Systemdisk, die zum Booten von GEOS verwendet wurde. Da die GEOS-Disketten aufgrund eines leicht veränderten Dateisystems mit herkömmlichen C64-Kopierprogrammen nicht kopiert werden konnten und über einen Kopierschutz verfügten, der erst einige Zeit später überwunden werden konnte, enthielt die zweite Diskette als Sicherungsmaßnahme den Inhalt der Systemdisk noch einmal, diese wurde „Sicherheitssystem“ genannt. Sie diente zum Starten des Systems für den Fall, dass die Systemdiskette beschädigt wird.\nDie Rückseite der ersten Diskette (genannt „Anwendungen“) enthielt nun die bereits bekannten Anwendungsprogramme \"geoWrite\" und \"geoPaint\" in neueren lokalisierten Versionen mit kleineren Verbesserungen sowie diverse Hilfsprogramme und Schriftartdateien. Auf der Rückseite der zweiten Diskette (genannt „Treiber“) befanden sich dementsprechend Druckertreiberdateien.\n\nMit GEOS 64 1.3 erschienen auf dem Softwaremarkt parallel weitere Anwendungen, die zusätzlich käuflich erworben werden konnten: unter anderem die Tabellenkalkulation \"geoCalc\", die Datenbank \"geoFile\", das Desktop-Publishing-Programm \"geoPublish\", eine Diskette mit einer erweiterten geoWrite-Version 2.1 (\"geoWrite Workshop\"), die auch ein Programm zur Erstellung von Serienbriefen beinhaltete (geoMerge) und mit geoLaser die Möglichkeit Laserdrucker mittels PostScript anzusteuern, eine Diskette mit kleineren Zusatzprogrammen wie einem Kalender (Calendar), einer Adressverwaltung (\"geoDex\") und einem Kartenspiel (BlackJack) (DeskPack), einer Diskette mit vielen zusätzlichen Schriftarten (FontPack) und eines speziellen Assembler-Entwicklungspaketes (geoProgrammer) und einer BASIC-Implementierung (geoBasic). Jedoch wurde die Menüführung dieser Zusatzprogramme nicht eingedeutscht.\n\nDarüber hinaus erschien auch zeitgleich GEOS 128 1.3, eine spezielle Version, die die erweiterten Fähigkeiten des C128 ausnutzen konnte (siehe Abschnitt GEOS 128 für den Commodore C128).\n\n1988 erschien die nächste Version von GEOS 64, 2.0. Diese präsentierte sich dem Anwender mit einer erweiterten, leicht überarbeiteten Desktop-Oberfläche. Integriert wurden in diese Version, die auf insgesamt vier Disketten ausgeliefert wurde, die Programme des GeoWrite Workshop, das Rechtschreibprüfungsprogramm GeoSpell sowie weitere Druckertreiber. Zeitgleich erschien entsprechend eine GEOS128-Version.\n\nFür den Anwender mit einem damals typischen Ein-Diskettenlaufwerksystem ohne RAM-Erweiterung gestaltete sich die Arbeit mit GEOS folgendermaßen: Er musste, nachdem er GEOS gebootet hatte, in das Diskettenlaufwerk eine von ihm selbst zusammengestellte Arbeitsdiskette einlegen, auf die er das benötigte Anwendungsprogramm, Hilfsprogramme, den Druckertreiber und zu benutzende Schriftartdateien kopiert hatte. Der noch freie Speicherplatz auf der Diskette konnte für Projektdateien verwendet werden. GEOS 64 war aufgrund des begrenzten Arbeitsspeichers nicht in der Lage, Hilfsprogramme, Druckertreiber und Schriftartdateien im RAM zu halten. Daher mussten diese, sofern sie beim Arbeiten benötigt wurden, auf der Arbeitsdiskette verfügbar sein. Bei Verwendung mehrerer Diskettenlaufwerke und/oder einer RAM-Erweiterung konnte die ständige Bereitstellung dieser Zusatzdateien für den Anwender vereinfacht werden, in dem diese in der RAMdisk oder auf der Diskette im anderen Laufwerk vorlagen.\n\nDa Anfang der 1990er Jahre Commodore die Produktion und den Vertrieb der RAM-Erweiterungen einstellte, erschien auf Initiative von Berkeley Softworks das Ersatzprodukt GeoRAM, das in GEOS eine ähnliche Funktionalität aufwies, aber außerhalb von GEOS kaum verwendet wurde. Ab Ende der 80er Jahre entwickelte sich zunächst in Nordamerika eine Public-Domain-Software/Shareware/Freeware-Szene, vor allem in den Foren der Onlineanbieter Quantum Link und Compuserve, die für ein Erscheinen vieler zusätzlicher GEOS-Programme und Tools sorgte. Diese Software war in Europa vor allem über Versandhäuser, die sich auf den Vertrieb von PD-Software spezialisiert hatten, erhältlich. Zudem wurde 1989 in Deutschland der GEOS-User-Club (GUC) als Interessensverein privater GEOS-Anwender gegründet. Der Markt&Technik-Verlag veröffentlichte zwischen 1988 und 1994 im Rahmen von sieben 64’er-Sonderheften, sowie auch als einzelne Titel, weitere GEOS-Programme, so das Terminalprogramm GeoTerm und das alternative Assemblerentwicklungspaket MegaAssembler.\n\n1993 stellte der deutsche Distributor Markt & Technik in Zusammenarbeit mit dem Geos-User-Club (GUC) eine neue Version 2.5 zusammen. Diese umfasste zusätzlich zur bisherigen Version 2.0 eine weitere Diskette, die die alternative Oberfläche Topdesk, die Rechtschreibprüfung Spellchecker und weitere Hilfsprogramme enthielt, die zuvor separat beim GUC erhältlich gewesen waren. Diese Aktualisierung, die eigentlich nur eine Erweiterung war, wurde nur für das deutsche GEOS 64 vorgenommen, nicht für die amerikanische Version und auch nicht für GEOS 128.\n\nIn den 90er Jahren bemühten sich andere Hersteller mit der Veröffentlichung alternativer Desktop-Oberflächen und Systemerweiterungen das Arbeiten unter GEOS komfortabler zu gestalten und vor allem die Peripheriegeräte, die der amerikanische Hardwarehersteller Creative Micro Designs (CMD) für C64/128 bis dahin entwickelt hatte, für GEOS besser nutzbar zu machen. Zu nennen sind der von CMD 1993 selbst vorgestellte alternative Desktop Gateway, das 1999 veröffentlichte Upgrade MegaPatch, dessen Oberfläche grafisch an Windows 95 orientiert war und nur in deutscher Sprache vorlag, sowie das etwa zeitgleich erschienene Wheels von Maurice Randall, das den Anspruch hatte, den Look'n Feel des an die frühen MacOS-Versionen orientierten originalen Desktops fortzuführen.\n\nDie Vertriebs- und Produktionsrechte des Betriebssystems lagen bei Berkeley Softworks, die sich später in GeoWorks umbenannten und mit PC/GEOS, sowie darauffolgendem PEN/GEOS eine starke, jedoch zum Vorgänger inkompatible Weiterentwicklung für x86 DOS-PCs, -PDAs, -MiniPCs und -Smartphones veröffentlichten.\n\nDie Commodore-Produktlinie wurde 1993 von Creative Micro Designs (CMD) übernommen. 2001 beendete CMD seinerseits alle Commodore-spezifischen Aktivitäten und übergab am 21. Juli 2001 alle Rechte an \"Click Here Software Company\", die von Maurice Randall betrieben wird.\n\nParallel zum C64 versuchte \"Commodore Business Machines\" mit dem Plus/4 einen preiswerten Homecomputer zu etablieren. 1987 wurde GEOS 64 von Oluf Heinrichsen inoffiziell auf den Plus/4 und C16 portiert und \"GEOS 3.5\" (in Anlehnung der Versionsnummer des auf dem Plus/4 eingesetzten Commodore-BASIC-Dialektes) genannt. Auf beiden Homecomputern benötigt diese GEOS-Version 64 KiB RAM, weshalb der C16 aufzurüsten ist. Außerdem funktioniert GEOS 3.5 ausschließlich mit dem seltenen Plus/4- und C16-Diskettenlaufwerk 1551, das über ein spezielles Parallelkabel angeschlossen wird. Ein Betrieb mit den weit verbreiteten C64/128-Diskettenlaufwerken 1541 und 1571, die an den Rechnern der Plus/4-Reihe ebenfalls üblich waren, ist nicht möglich. GEOS 3.5 besitzt keinen Maussupport, der Mauszeiger muss mittels Joystick gesteuert werden. Hierbei ist zu beachten, dass erstens GEOS 3.5 für den Plus/4 von GEOS 64 1.2 abgeleitet worden war, das zu diesem Zeitpunkt auch am C64/128 für die Verwendung einer Computermaus noch nicht ausgelegt war, und zweitens die Joystick-Anschlüsse des Plus/4 und C16 anders als die des C64 nicht die Unterstützung für Paddles und Analog-Joysticks bieten, die beim C64 später für die Verwendung einer Maus zweckentfremdet wurde.\n\nNach Veröffentlichung des GEOS 1.3 für den C64 entschied sich der damalige Hersteller Berkeley Softworks GEOS auch für den professionellen Commodore 128 auf den Markt zu bringen. Dort lief GEOS 128 1.3 auch erstmals in einem 80-Zeichen-Modus (höhere Auflösung als im C64) und war hierbei durch Nutzung der doppelten Prozessortaktfrequenz von 2 MHz dem C64-GEOS überlegen. Mit Erscheinen von GEOS 64 2.0 wurde auch eine 2.0-Version von GEOS 128 veröffentlicht.\n\nGEOS 128 war zu beinahe allen GEOS-64-Applikationen direkt kompatibel, ohne dass in einen C64-Modus umgeschaltet werden musste – anders als bei normaler C64-Software; allerdings beschränkte sich das auf den Betrieb der Anwendung im 40-Zeichen-Modus, da eine GEOS-Anwendung auf einen Modus ausgelegt sein musste, um lauffähig zu sein. Der Modus konnte im Betrieb umgeschaltet werden, was bei Starten einer Anwendung des anderen Modus auch automatisch getan wurde.\n\nGEOS 128 hatte für den Anwender auch den Vorteil, dass es aufgrund des größeren Arbeitsspeichers des C128, im Gegensatz zu GEOS 64 wichtige Zusatzdateien wie den Druckertreiber ständig im RAM halten konnte und ermöglichte so ein komfortableres Arbeiten.\n\nAndere Besonderheiten:\n\nIn den 90er Jahren erschienen auch für GEOS 128 spezielle Versionen der alternativen Desktop-Oberflächen \"TopDesk\" und Gateway sowie des Upgrades \"Wheels\".\n\nZur allgemeinen Geschichte von GEOS siehe Abschnitt \"GEOS 64 für den Commodore C64\".\n\nNach dem Erfolg des UrGEOS am C64 wurde Apple II GEOS von Berkeley Softworks 1988 für die 1982 und 1985 erschienenen Apple-II-Nachfolger Apple //e und Apples ersten tragbaren Computer, den Apple //c, veröffentlicht. 1989 erschien auch eine GEOS-Version für den Ur-Apple II mit 128 KByte RAM (die entsprechende Speichererweiterung eines anderen Herstellers war allerdings nicht sehr verbreitet, mit Apple-eigenen Komponenten lag das RAM-Limit des alten Apple II bei 64 KB und GEOS war nicht nutzbar). Dieser Version lag auch eine Demodiskette von GEOS für den Apple-II-Clone \"Sears Laser 128Kbyte\" Computer bei. Ob für den Sears Laser 128PC auch eine GEOS Vollversion erschien, konnte bisher nicht ermittelt werden.\n\nDa 1988/89 die Apple-II-Linie bereits betagt war, Apples eigener, aufwärtskompatibler Apple IIgs mit einem wesentlich leistungsfähigeren GUI ausgeliefert wurde, und der Apple Macintosh seinen Siegeszug angetreten hatte, wurde Apple-GEOS kein Verkaufsrenner. Es wurden auch keine Entwicklungswerkzeuge angeboten, die das Softwareangebot hätten erweitern können. Die letzten Apple //e wurden 1993 gefertigt. 1995 wurde der Verkauf und Kundendienst für APPLE II GEOS von Seiten des inzwischen unter der Bezeichnung \"GeoWorks Corporation\" firmierenden Herstellers eingestellt.\n\nInformationen und Bilder:\n\nDownloads:\n"}
{"id": "722774", "url": "https://de.wikipedia.org/wiki?curid=722774", "title": "Daemon Tools", "text": "Daemon Tools\n\nDaemon Tools (offiziell als „DAEMON Tools“ bezeichnet, kurz auch „Daemon“ oder „DT“) ist ein Windows-Programm zum Emulieren optischer Laufwerke.\nDie Daemon Tools gibt es in verschiedenen Ausführungen. Für Privatanwender sind sie kostenlos nutzbar. Die kommerzielle Nutzung hingegen erfordert eine kostenpflichtige Registrierung. Daemon Tools ist proprietäre Software.\n\nWie andere Emulatoren dieser Art werden durch Daemon Tools optische Laufwerke (z. B. für CDs oder DVDs) virtualisiert. Dabei werden zuvor sogenannte Speicherabbilder der optischen Medien erstellt, um diese dann von der Festplatte in einem virtuellen Laufwerk wiederzugeben.\n\nDas virtuelle Laufwerk wird dabei so in das Betriebssystem eingebunden, dass es scheint, als ob sich ein zusätzliches Laufwerk im verwendeten Rechner befände. Für den Nutzer unterscheidet es sich in der Bedienung nicht von physischen Laufwerken.\n\nWie andere Emulatoren dieser Art, erspart Daemon Tools das Wechseln von Datenträgern, um verschiedene Programme stattdessen von einem anderen (größeren) Massenspeicher laufen zu lassen oder diese zu installieren, was zusätzlich die Abnutzung der Datenträger (z. B. Kratzer) verhindert.\nAußerdem kann das Programm verwendet werden, um Software, die in Form eines Abbildes vertrieben wird, zu laden. In Daemon Tools Lite ist es möglich, bis zu vier virtuelle Laufwerke zu erstellen und somit mehrere Abbilder gleichzeitig zu laden. Daemon Tools kann mit Hilfe von Rootkittechnologien viele aktuelle Kopierschutz-Verfahren, wie SafeDisc, SecuROM, Tagès oder StarForce, umgehen. Um dem entgegenzuwirken, verweigern einige Programme den Start auf Computern, auf denen Daemon Tools installiert ist.\n\nDurch Plugins kann Unterstützung für weitere Formate hinzugefügt werden.\n\n\"Daemon Tools Lite\" enthielt von Version 4 bis 4.12.4 Adware von \"WhenU.com\". Während der Installation bestand die Möglichkeit, zu entscheiden, ob die Adware installiert werden sollte, wobei diese Option standardmäßig eingeschaltet war.\nAb Version 5 ist wieder Adware enthalten, wenn man eine kostenlose Lizenz nutzen möchte.\n\n\"Daemon Tools Pro Basic\" enthält diese Adware immer noch, wobei die Installation nicht freiwillig, sondern zwingend ist.\n\nDie \"Lite\"-Ausführung (zu deutsch: „leicht“) der Daemon Tools ist kostenlose Freeware und hat einen eingeschränkten Funktionsumfang. Sie kann, über ein Symbol in der Taskleiste und dessen Kontextmenü bis zu vier optische Laufwerke emulieren. Zudem können noch diverse Einstellungen über ein eigenes Fenster vorgenommen werden.\n\nDas Flaggschiff von DT Soft Ltd. ist \"Daemon Tools Pro\" (für „professionell“, auch „DTPro“ genannt). Der auffallendste Unterschied zur Lite-Ausführung ist die erweiterte grafische Benutzeroberfläche. Mit diesem Programm ist es auch möglich, Abbilder von beliebigen CDs und DVDs im \"Media Descriptor Image\"-Format zu erstellen. Diese können komprimiert und mit einem Passwort verschlüsselt werden.\n\nDaemon Tools Pro kann bis zu 32 virtuelle Laufwerke verwalten. Ein hervorstechendes Feature ist der vIDE-Adapter. Dadurch können nicht mehr nur, wie normalerweise üblich, virtuelle SCSI-Laufwerke, sondern auch bis zu vier virtuelle IDE-Laufwerke (auch \"vIDE\" genannt) angelegt werden. Damit wird das Blacklisting-Problem durch diverse Kopierschutzverfahren der vSCSI-Laufwerke behoben.\n\nEin weiteres Merkmal ist ein Hilfsprogramm, mit dem bereits bestehende Abbilder ins \"MDS\"-Format konvertiert und nachträglich verschlüsselt oder komprimiert werden können.\n\nDaemon Tools Pro erscheint in vier verschiedenen Versionen: \"Basic\" (Adware oder Trial), \"Standard\" und \"Advanced\". Diese unterscheiden sich in Funktionalität und Preis.\n\nDaemon Tools Pro kann dieselben Abbild-Dateitypen einbinden wie Daemon Tools Lite. Erstellt werden die Abbilder jedoch immer im \"Media-Descriptor-File\"-Format.\n\nDaemon Tools Pro ist in der Lage, Abbilder von sehr vielen Spiele-CDs und -DVDs, die mit neuesten Kopierschutzverfahren geschützt sind, zu erstellen. Eine der wichtigsten Neuerungen ist der so genannte \"Tagès Dumper\", der es ermöglicht, die Twin-Daten von Tagès-Datenträgern auszulesen.\n\nMit Daemon Tools Pro Advanced ist es möglich Images zu brennen, womit die Nutzung eines zusätzlichen Brennprogramms entfällt.\n\nUm Daemon Tools Pro zu aktivieren, ist eine Internetverbindung notwendig. Die Lizenz ist ausschließlich für eine bestimmte Hardwarekonfiguration nutzbar, die mittels eines Hardwareschlüssels erkannt wird. Damit das Programm nicht deaktiviert wird, muss es mindestens einmal im Monat auf das Internet zugreifen können. Falls der Benutzer eine neue Hardware-Konfiguration hat, muss dieser seine alte Lizenz widerrufen, um so eine für das neue System zu bekommen.\n\nDaemon Tools wird dafür kritisiert, das Nutzerverhalten bezüglich der verwendeten Images zu protokollieren und über Dritte auszuwerten. Selbst wenn bei der Installation explizit angegeben wurde, dass man der Überwachung des Nutzungsverhaltens und der Protokollierung der Programmaktivitäten nicht zustimmt, werden dennoch alle Daten an den internen Statistikdienst MountSpace weitergeleitet. Dieser Statistik-Dienst wurde mit der Version DAEMON Tools Lite 4.40.1 eingeführt , ältere Versionen sollten also nicht betroffen sein. Die gesammelten, persönlichen Daten enthalten beispielsweise Image-Hash, Imagedateiname, Datenträgername sowie Anzahl der Einbindungen, die den IP-Adressen der Nutzer zugeordnet sind. Von Datenschutz kann keine Rede sein, ggf. können derlei Logdaten auch für die Benutzer strafrechtlich relevante Folgen nach sich ziehen.\n\nRechtliches\n\nÄhnliche Programme\n\n"}
{"id": "723634", "url": "https://de.wikipedia.org/wiki?curid=723634", "title": "Target 3001", "text": "Target 3001\n\nTarget 3001! ist ein Programm zum Entwurf von Schaltplänen und zur Leiterplattenentflechtung bei der Entwicklung von elektronischen Schaltungen und Leiterplatten (Platinen). Die Software wird seit über 25 Jahren durch das \"Ing.-Büro Friedrich\" in Deutschland entwickelt und läuft unter dem Betriebssystem Microsoft Windows. Unter Linux kann sie mit Hilfe von Wine mit der Standardkonfiguration des Pakets genutzt werden (getestet mit Ubuntu 11.04, 64-Bit).\n\nFür den nicht-kommerziellen Einsatz gibt es eine kostenlose, auf 250 Anschlüsse (Pins) begrenzte Version zum Download. Die Benutzeroberfläche der Software ist wahlweise in deutscher, englischer oder französischer Sprache verfügbar. Außerdem bietet der Leiterplattenhersteller PCB-Pool eine Vollversion an, die ausschließlich Fertigungsdaten zur Produktion bei dem Unternehmen erzeugen kann.\n\nVorgänger von Target 3001 ist die 1989 entwickelte Leiterplatten-Software \"Rule\", die unter DOS lief. Deren Name war ein Initialwort, das für Rechner Unterstützter Leiterplatten Entwurf stand. Nachdem diese Software sich in Bastlerkreisen verbreitet hatte, wurde unter den Benutzern der Ruf nach einer Schaltplaneingabe laut. Ebenso wurde ein Autorouter gewünscht, also eine automatische Leiterbahnentflechtung. 1992 kam die Version \"Target 2.1 für DOS\" auf den Markt, die diesen Wünschen in einem ersten Schritt Rechnung trug. Der Umstieg nach Windows gestaltete sich schwierig, die ersten Versionen von \"Target V3 für Windows\" stürzten häufig ab. Mit steigender Stabilität und Leistungsfähigkeit des Gesamtpaketes wurde die Software außer im Hobby- und Ausbildungsbereich auch im professionellen Umfeld eingesetzt. In den weiteren Versionen V7 bis V19 wurde das Paket ausgebaut und ein EMV-Modul, Symbol- und Gehäuse-Generatoren, sowie eine (P)SPICE-kompatible Simulation ergänzten den Leistungsumfang. Der Name „TARGET“ wurde zunächst auf „TARGET 2001!“ geändert und später als „TARGET 3001!“ als Marke beim Deutschen Patentamt eingetragen.\n\nEs wurde der Software eine Funktion hinzugefügt, auch Frontplatten direkt in Verbindung mit den Leiterplatten zu entwerfen. Zudem wurde eine 3D-Ansicht (mit STEP 3D-Import) ergänzt, die einen visuellen Eindruck der entworfenen Leiterplatte vermittelt.\n\nNach einer Umfrage der Zeitschrift Elektor (Heft 05/2004, S. 73) war die Software unter der Leserschaft am zweithäufigsten verbreitet.\n\nDas Softwarepaket fasst mehrere verschiedene Funktionsblöcke unter einer einheitlichen Benutzeroberfläche zusammen. Der Designprozess startet mit dem Entwurf eines Schaltplanes, das Endprodukt ist ein Layout für eine Platine. Schaltplan und Layout werden dabei in einer gemeinsamen Datei gespeichert. Beim Entwurf hält die automatische Vorwärts- und Rückwärtsannotation Schaltung und Layout konsistent.\n\nEine wichtige Komponente sind die mitgelieferten Baustein-Bibliotheken. Das Bibliothekswesen in Target hat sich mehrfach geändert:\nEin Bauteil besteht in der Schaltplandarstellung aus einem Schaltsymbol und im Layout aus einem Anschlussbild, das zum Gehäuse des Bauteils passt. Zu den Bauteilen können Links zu Bauteil-Datenblättern und -Lieferanten gespeichert werden. Sowohl Bauteil–Symbole als auch Bauteilgehäuse können frei entworfen oder im Gehäuse-Generator, anhand von Parametern, generiert (und sofern benötigt überarbeitet) werden. Zu den Gehäusen können 3D-Modelle (auch als STEP-3D) angelegt werden.\n\nMit der SPICE-kompatiblen Schaltplan-Simulation kann die Funktion der Schaltung vorab automatisch durchgerechnet werden. Dazu sind zu vielen Bauteilen elektrische Simulationsmodelle in der Bibliothek hinterlegt.\n\nZum Entwurf des Layouts stehen ein Autoplatzierer und ein Autorouter zur automatischen Entflechtung der Leiterplatte zur Verfügung. Der \"Design Rule Check\" prüft den Entwurf anschließend auf Kurzschlüsse und weitere Verstöße gegen vorher eingestellte Designregeln, etwa Abstände und Mindest-Bohrdurchmesser. \n\nTarget unterstützt gängige Ausgabeformate für die Leiterplattenherstellung und -bestückung, etwa Gerber-Daten.\n\n"}
{"id": "725076", "url": "https://de.wikipedia.org/wiki?curid=725076", "title": "Differentialanalysator", "text": "Differentialanalysator\n\nDer Differentialanalysator () war ein elektromechanischer Analogrechner, der mehrere Differentialgleichungen gleichzeitig handhaben konnte. Die Ausgabe erfolgte mittels automatisch gedruckter Schaubilder. Er wurde in den Jahren 1928 bis 1932 am Massachusetts Institute of Technology (MIT) unter der Leitung von Vannevar Bush und H. L. Hazen entwickelt.\n\nSpäter wurden so genannte (DDAs) entwickelt, die die Zahlen in Registern zwischenspeicherten. Sie stellten einen Übergangsschritt zwischen analogen Rechenmaschinen und digitalen Computern dar. In Analogie dazu werden in der Computergrafik bestimmte einfache Algorithmen zur Rasterung von Linien ebenfalls DDA genannt.\n\n\n\n"}
{"id": "725083", "url": "https://de.wikipedia.org/wiki?curid=725083", "title": "Rockefeller Differential Analyzer", "text": "Rockefeller Differential Analyzer\n\nDer war ein Analogrechner, der in den 1930er Jahren mit finanzieller Unterstützung der Rockefeller-Stiftung von Vannevar Bush und Samuel H. Caldwell entwickelt wurde.\n\nDer Rockefeller Differential Analyzer stellt eine Weiterentwicklung des Differentialanalysators dar. Dabei sollten die mechanischen Bauteile dieses Analogrechners durch elektronische ersetzt werden. Die Dateneingabe erfolgte ab 1935 über Lochstreifen.\n\nDie Rechenmaschine wog 100 Tonnen, in ihr wurden 2000 Röhren, über 300 km Kabel, 150 Motoren und Tausende von Relais verwendet.\n\nSie war ab 1942 einsatzbereit, wurde aber wegen des Krieges erst nach 1945 publik gemacht. Der \"Rockefeller Differential Analyzer\" war bis zum Ende des Zweiten Weltkriegs die leistungsfähigste Rechenmaschine.\n"}
{"id": "725159", "url": "https://de.wikipedia.org/wiki?curid=725159", "title": "Zuse Z4", "text": "Zuse Z4\n\nDie Zuse Z4 ist ein vom Zuse Ingenieurbüro und Apparatebau entwickelter Digitalrechner, der aus 2200 Relais gebaut ist. Sie hat einen elektromechanischen Speicher, der 64 Zahlen aufnehmen kann.\n\nDie Z4 wurde von 1942 bis 1945 als Weiterentwicklung der Zuse Z3 in Berlin gebaut. Um ihr von der Programmierseite her mehr Flexibilität zu geben, wurde sie für die Anbindung mehrerer \"Abtaster\" (Lochstreifenleser) und \"Locher\" (Lochstreifenstanzer) vorgesehen. Lochstreifen waren neben Tasten und Lampen das Ein- und Ausgabemedium dieses Rechners. Kurz vor Fertigstellung im Frühjahr 1945 wurde die Z4 nach Göttingen in die Aerodynamische Versuchsanstalt des KWI für Strömungsforschung verlegt. Dort wurde sie fertiggestellt, und die ersten programmgesteuerten Rechnungen konnten durchgeführt werden. Anfang April 1945 wurde sie nach Süddeutschland gebracht, sie überstand die Kriegswirren zunächst in einem Schuppen in Hinterstein im Allgäu, später in einem Mehllager in Hopferau.\n\n1950 war die Z4 der einzige funktionierende Computer in Kontinentaleuropa. Das Institut für Angewandte Mathematik von Professor Eduard Stiefel an der ETH Zürich holte im Jahre 1950 die von der Zuse KG instandgesetzte Z4 mietweise nach Zürich. Damit war die Z4 der erste kommerzielle Computer weltweit. Sie wurde einige Monate früher als die UNIVAC installiert.\n\nIn ihren frühen Ausbaustufen hatte die Z4 keine bedingte Sprunganweisung, sie war somit anfangs kein Turing-mächtiger universeller Computer. Auf Anforderung der Numeriker an der ETH Zürich wurde diese Funktion um 1950 eingebaut.\n\nDie Z4 diente von 1950 bis 1955 als zentraler Rechner der ETH Zürich und brachte Stiefel auch Erkenntnisse für den Bau seines eigenen Rechners ERMETH. Ihr beschränkter Speicher für Zwischenwerte hatte gewissen Einfluss auf Details der danach am Institut entwickelten Algorithmen.\n\nAnschließend wurde die Z4 1955 in der Nähe von Weil am Rhein an ein französisches Rüstungsforschungsinstitut bei Basel verkauft, wo sie 1957 einen relaisgesteuerten Ferritkernspeicher erhielt, der eine logische Information pro Ferritkern speichern konnte.\n\nGemäß derselben Quelle wurde die Z4 1960 dem Deutschen Museum in München überlassen und gehört seit 1988 zur Ausstellung über Informatik im Deutschen Museum.<br>\nGemäß einer anderen Quelle kam die Z4 von Finnland mit zerschnittenen Kabelsträngen in den Keller des Siemens-Museums in München, von dort 1980 nach Bad Hersfeld und danach wieder nach München, ins Deutsche Museum.\n\n\n"}
{"id": "725457", "url": "https://de.wikipedia.org/wiki?curid=725457", "title": "PDP-11", "text": "PDP-11\n\nDie PDP-11 ist ein in den 1970er Jahren weit verbreiteter 16-Bit-Computer, der von Digital Equipment Corporation angeboten wurde. Obwohl nicht explizit als Nachfolger konzipiert, hat er bei vielen Prozessrechenanwendungen den vorher dominierenden 12-Bit-Computer PDP-8 aus der Programmed-Data-Processor-Reihe abgelöst. Eingeführt wurde die PDP-11-Baureihe im Jahr 1970.\n\nDas technische Konzept der PDP-11-Serie war einfach gehalten. Ein standardisiertes „Universelles Bus-System“ (Unibus), über den Zentraleinheit, Arbeitsspeicher und Ein-/Ausgabe-Geräte miteinander kommunizierten, ermöglichte die Auf- und Umrüstung für eine Vielzahl von Prozessanwendungen. Deswegen wurde die PDP-11 häufig im experimentellen Wissenschafts- und Forschungsbereich eingesetzt.\n\nIm Unterschied zu vielen vorherigen Rechnern kannte die PDP-11 keine speziellen Ein-/Ausgabe-Befehle. Da Peripheriegeräte am Unibus wie Arbeitsspeicher adressiert wurden, konnte die Peripherie mit „normalen“ Rechnerbefehlen gesteuert werden. Auch bei der Steuerung von Kraftwerken, Verkehrswegen und Telefonnetzen wurde die PDP-11 verwendet. Das offene Bus-System ermöglichte es auch Fremdanbietern, kostengünstige und leistungsstarke Peripheriegeräte zum Anschluss an die PDP-11 auf den Markt zu bringen.\n\nDie PDP-11-Rechner können nach dem verwendeten Peripheriebus eingeteilt werden. Mit der MicroPDP-11 wurde in den 1980er Jahren ein Tischrechner verwendet.\n\nDie folgenden Modelle nutzten den ursprünglichen 18 bit breiten Unibus:\n\nDie folgenden Modelle nutzten den später eingeführten preiswerteren Q-Bus, bei dem Adress- und Datenleitungen zusammengelegt waren:\n\n\nDiese Modelle hatten nur den 16 bit breiten Prozessorbus und dienten als intelligente Terminals. Die Serien PDT-11/110 und PDT-11/130 nutzten ein VT100-Terminal-Gehäuse.\n\n\nDie PDP-11 wurde wegen ihrer technischen Bedeutung auch in der Sowjetunion und ihren verbündeten Staaten ohne Lizenz nachgebaut. Beispiele dafür sind:\n\nVerschiedenste Betriebssysteme waren für die PDP-11 verfügbar:\n\nVon DEC:\n\nVon Drittanbietern:\n\nDie PDP-11 hat eine Wortbreite von 16 bit. Es werden Einadressbefehle, Zweiadressbefehle und Sprünge unterschieden. Die Adressierung erfolgt jeweils über sechs Bit, wobei die ersten drei Bit für die acht Adressierungsmodi verwendet werden und die letzten drei für die Auswahl eines der acht Register (R0 bis R7). Viele Befehle gibt es als Wortbefehle und als Bytebefehle, das heißt, sie operieren mit 16-Bit- bzw. 8-Bit-Einheiten. Die Byteversionen der „doppelt“ vorhandenen Befehle sind in den folgenden Aufstellungen in Klammern angegeben.\n\nDie wichtigsten 1-Adress-Befehle sind:\n\n2-Adress-Befehle folgen immer dem Muster „Befehl–Quelle–Ziel“. Beim Befehl \"ADD R1, R2\" wird also gerechnet \"R2 = R1 + R2\".\n\nDie wichtigsten 2-Adress-Befehle sind:\n\nBei Sprüngen wird immer ein 8-Bit-Offset angegeben. Dies gibt die Anzahl der Worte an, um die gesprungen wird.\n\nDie PDP-11 bringt sehr viele Sprungbefehle mit, insgesamt 18 verschiedene.\n\nDie Adressierungsmodi unterscheiden sich bei der PDP-11 abhängig davon, ob als Register R0 bis R5 (Allzweckregister), R6 (Stapelzeiger bzw. Stackpointer, SP) oder R7 (Programmzähler, PC) verwendet wird.\n\nEine Adressierungseinheit ist „1“ für Byte-Befehle und „2“ für Wort-Befehle.\n\nR6 ist ein Zeiger auf den Stapelspeicher, der vom Prozessor bei Interrupts zur Zwischenspeicherung des aktuellen Maschinenzustands verwendet wird. Der Stapelzeiger dient der Verwaltung des Stapelspeichers, er muss grundsätzlich eine Wortadresse, das heißt eine gerade Adresse enthalten. Deshalb wird im Unterschied zu den Allzweckregistern bei den Adressmodes Autoinkrement bzw. Autodekrement das Register R6 immer um 2 erhöht oder erniedrigt, unabhängig ob es sich um einen Byte- oder Wortbefehl handelt.\n\nIn der Einrichtung HASYLAB am Deutschen Elektronen-Synchrotron in Hamburg diente von 1981 bis zum 20. Oktober 2012 eine PDP-11/23 an der Beamline E1 zur Steuerung von FLIPPER II, einer Anlage zur Messung von Photoelektronenspektroskopie mit Synchrotronstrahlung.\n\nDie PDP-11 wird in Kernkraftwerken von General Electric noch verwendet. Das soll bis 2050 so bleiben.\n\nIm Film \"23 – Nichts ist so wie es scheint\" wird die PDP-11 mehrmals erwähnt. Im Film wird fälschlicherweise behauptet, eine PDP-11 benötige zwangsläufig einen Dreiphasenwechselstromanschluss mit 380 V. Obwohl es einige „große“ PDP-11-Modelle gibt, die tatsächlich Dreiphasenwechselstrom benötigen, kommt doch die überwiegende Mehrheit der PDP-11-Rechner mit einphasiger Wechselspannung von 110 V bzw. 220 V aus. Da jedoch im Film ein Einphasenstecker in 32-Ampere-Ausführung zu sehen ist, könnte auch gemeint sein, dass sich die Maschine nicht aus einer normalen Steckdose versorgen lässt.\nTatsächlich ist das im Film gezeigte Gerät jedoch keine PDP-11, sondern ein IBM AS/400.\n\n\nDerzeit gibt es vier Emulatoren für PDP-11-Server.\n\n"}
{"id": "725777", "url": "https://de.wikipedia.org/wiki?curid=725777", "title": "NetBIOS over TCP/IP", "text": "NetBIOS over TCP/IP\n\nNetBIOS over TCP/IP (kurz \"NetBT\" oder \"NBT\") ist ein Netzwerkprotokoll, das es ermöglicht, auf der Programmierschnittstelle NetBIOS aufbauende Programme über das Netzwerkprotokoll TCP/IP zu verwenden.\n\nNBT ist in den RFC-Dokumenten RFC 1001 (Übersicht) und RFC 1002 (Details) definiert.\n\nDie NetBIOS-Namensauflösung wird über UDP auf Port 137, der Datagram Service über UDP auf Port 138 und der Session Service über TCP auf Port 139 abgewickelt. Die UDP-Pakete bzw. TCP-Verbindungen tragen am Anfang jeweils Header mit Informationen wie den NetBIOS-Namen von Sender und Empfänger.\n\nÄltere Windows-Versionen (bis zu Windows ME) verwendeten NBT, um die höheren Netzwerkfunktionen (Server Message Block (SMB)) zu realisieren. Neuere Windows-Versionen (ab Windows 2000) wickeln die SMB-Kommunikation direkt über TCP-Port 445 ab. \n\n"}
{"id": "727694", "url": "https://de.wikipedia.org/wiki?curid=727694", "title": "Macintosh User Group", "text": "Macintosh User Group\n\nDer Ausdruck Macintosh User Group (Abkürzung: MUG) steht für Gruppen von Anwendern, die mit Macintosh-Computern der Firma Apple arbeiten.\n\nMan trifft sich zum Erfahrungsaustausch, gibt Kenntnisse weiter, hilft bei Problemen oder vermittelt Kaufberatung.\nDaneben gibt es noch sog. Mac-Foren, die diese Aufgabe im Internet wahrnehmen und die fallweise auch Treffen veranstalten, um sich gegenseitig kennenzulernen und Erfahrungen auszutauschen. Solche Benutzergruppen erfüllen eine wichtige Aufgabe hinsichtlich einer inoffiziellen und für alle offenen Vertretung der Kunden, die Macintosh-Computer verwenden oder verwenden wollen, fördern die zweckgemäße Verwendung der Computer und bieten schnelle Hilfe, die aus wirtschaftlicher Sicht vom Hersteller oder dem Handel nicht geboten werden kann. Sie kanalisieren auch Benutzerwünsche, die eine Weiterentwicklung im Benutzersinne ermöglicht.\n\nUrsprünglich gab es MUG nur in Amerika, doch mit zunehmender Verbreitung der Mac-Computer formierten sich weltweit MUG.\n\nIn Deutschland sind die bekanntesten MUG:\n\n"}
{"id": "727701", "url": "https://de.wikipedia.org/wiki?curid=727701", "title": "Yahoo Messenger", "text": "Yahoo Messenger\n\nDer Yahoo Messenger (eigene Schreibung \"Yahoo! Messenger\", kurz auch \"Y!M\", \"YIM\" oder \"Yim\") war ein kostenloser Instant-Messaging-Dienst des Unternehmens Yahoo. Die zugehörige Software wurde kostenlos angeboten und konnte mit einem gültigen Yahoo-Zugang heruntergeladen und installiert werden. Die Funktionen des \"Yahoo Messenger\" waren denen von ICQ, dem \"AOL Instant Messenger\" und dem \"Windows Live Messenger\" sehr ähnlich, sie waren zueinander aber nicht kompatibel. Microsoft und Yahoo hatten sich zwischenzeitlich dazu entschlossen, ihre IM-Dienste zusammenzuschließen. Ab Version 8 von \"Yahoo Messenger\" konnten zumindest Textnachrichten mit dem \"Windows Live Messenger\" ausgetauscht werden, bis diese Kooperation im Zuge der Umstellung von \"Windows Live Messenger\" auf \"Skype\" 2012 wieder abgeschaltet wurde.\n\nDer \"Yahoo Messenger\" war für die Betriebssysteme Windows, macOS, Android und iOS verfügbar. Nur noch veraltete Versionen gibt es für Linux und Solaris. Alternativ konnte für Linux ein Client namens \"Gyachi\" genutzt werden, der allerdings seit dem Jahr 2007 nicht mehr aktualisiert worden ist.\n\nDie Software bot neben Textnachrichten auch Sprachanrufe, Webcams, Spiele und Fotofreigaben. Einige Funktionen wurden allerdings nur von der Windows-Version unterstützt.\n\nAm 8. Juni 2018 gab Yahoo bekannt, dass der Messenger-Dienst wegen der sich stetig verändernden Kommunikationslandschaft zum 17. Juli eingestellt werden würde. Einen direkten Nachfolger gab es nicht. User können bis Ende November 2018 auf Antrag ihren gesammelten Chat-Verlauf zum Download erhalten.\n\nLaut Nielsen NetRatings von Januar 2006 lag die Zahl der europäischen Yahoo-Messenger-Nutzer bei 4,65 Millionen. Der Service wurde durchschnittlich 13,6-mal pro Monat genutzt. Durchschnittlich verbrachten die europäischen Nutzer zwei Stunden und 15 Minuten im Monat mit dem \"Yahoo Messenger\".\n\n\n"}
{"id": "730438", "url": "https://de.wikipedia.org/wiki?curid=730438", "title": "Microsoft Reader", "text": "Microsoft Reader\n\nDer Microsoft Reader ist ein proprietäres Computerprogramm zur Darstellung elektronischer Bücher (E-Books), die im Microsoft-eigenen DRM-geschützten Lit-Format vorliegen. Es läuft ausschließlich auf Windows-Betriebssystemen und ist 2012 eingestellt worden.\n\nUrsprünglich war der Microsoft Reader wohl als Teil eines Gesamtkonzeptes von Microsoft zum Thema E-Books geplant. Nach der Präsentation eines Prototyps für ein entsprechendes Lesegerät im Jahr 2000 wurde das Produkt jedoch nicht auf den Markt gebracht. Durch die starke Verbreitung des Windows-Betriebssystems etablierte sich der Reader relativ schnell. Da er aber für die damals weit verbreiteten Palm-PDAs nicht angeboten wurde, ließ er ein wichtiges Marktsegment unbearbeitet. Im August 2011 kündigte Microsoft die Einstellung des Projekts an. Ab 8. November 2011 wird es keine neuen E-Books im .lit-Format mehr geben. Das vollständige Aus für die Microsoft Reader App soll am 30. August 2012 erfolgen.\n\nDie erste Version des Microsoft Readers erschien im Jahr 2000. Die Version war nicht vollends ausgereift, insbesondere besaß sie noch nicht das für die kommerzielle Vermarktung wichtige digitale Rechtemanagement, sodass bald darauf im Jahre 2001 die ausgereifte Version 2.0 mit neuen Funktionen veröffentlicht wurde. Neben den Versionen für Windows-PCs und Pocket-PCs wird zudem eine Anpassung an Tabletcomputer angeboten.\n\nDie 2003 erschienene Version 2.1.1 ist die bisher letzte des Readers. Seitdem wird das Programm bis auf einige wenige Bugfixes nicht mehr weiterentwickelt.\n\nVersion 2.1 hatte folgende Systemanforderungen\n\nNeben den für E-Book-Reader typischen Funktionen wie Lesezeichen, Navigationshilfe, persönlicher Bibliothek und benutzerdefinierte Anmerkungen bietet der Microsoft Reader, neben einer Interaktivität für Wörterbücher und Lexika (Wörter können markiert werden und eine Übersetzung bzw. Erklärung kann angezeigt werden, wenn entsprechende Werke installiert sind), erstmals eine freie Paginierung. Die Buchseiten werden je nach verwendetem Anzeigebildschirm beim Öffnen automatisch angepasst. Zusätzlich bietet der Reader eine ClearType-Unterstützung, die die Anzeigequalität auf Bildschirmen erheblich verbessert. Als wichtige Funktion für Sehbehinderte besitzt der Reader eine Text-to-Speech-Ausgabe.\n\nDie Versionen für Pocket- und Tablet-PCs bieten darüber hinaus die Möglichkeit, zwischen Hoch- und Querformat zu wechseln und Bilder heranzuzoomen. Die Zoomfunktion prädestiniert den Reader auch als Anzeigeprogramm für Karten.\n\nEin Software Development Kit für das Microsoft-eigene Lit-Format, das externen Entwicklern die Möglichkeit bietet, Konvertierungsprogramme für Lit-E-Books zu entwickeln und zu publizieren, ist frei verfügbar. Endkunden haben die Möglichkeit, mit angebotenen Konvertierungsprogrammen ihre eigenen E-Books aus HTML- oder Textdateien herzustellen. Zudem existieren Konvertierungs-Plug-ins für die Textverarbeitung Microsoft Word und das Layoutprogramm QuarkXPress.\n\n"}
{"id": "731106", "url": "https://de.wikipedia.org/wiki?curid=731106", "title": "RatDVD", "text": "RatDVD\n\nratDVD ist eine Freeware für Windows, die Video-DVDs vollständig oder teilweise in ein eigenes Containerformat kopiert, und dabei Video- sowie Tonspuren verlustbehaftet verkleinert. Die entstandene Containerdatei kann mit kompatiblen Mediaplayern abgespielt werden und verfügt -je nach Auswahl- über die gleiche Struktur (Menüs, Bonusmaterial, Untertitel etc.) wie die original DVDs.\n\nAuch bietet das Programm die Möglichkeit, aus den Containern heraus wieder konventionelle Video-DVDs zu brennen. 1:1 Kopien können wegen der reduzierten Bild- und Tonqualität jedoch nicht erstellt werden.\n\nEine mit ratDVD erstellte Kopie ist nach Angaben der Programmierer üblicherweise zwischen 1 und 2 GByte groß. Soll nur der Hauptfilm kopiert werden, so ergeben sich Dateigrößen um die 750 MB. Wird auf maximale Platzersparnis Wert gelegt, so lassen sich 90-minütige Spielfilme auf 200 bis 300 MB reduzieren. Bild- und Tonqualität leiden entsprechend. Zur Komprimierung des Videomaterials wird ein eigenes Kompressionsformat, der \"XEB-Codec\", verwendet.\nVorhandenes LPCM- und AC3-5.1-Material wird in \"AC3 Virtual Surround\" transkodiert. Audiospuren im DTS-Format mit den Voreinstellung automatisch entfernt.\n\nPer CSS kopiergeschützte DVDs kann ratDVD selbst nicht verwenden.\n\nNach Informationen von Heise wurde ratDVD von den deutschen Kopierschutzspezialisten ACE GmbH zu fluxDVD erweitert und wird für das download- und brennbare DVD-Format von T-Online verwendet.\n\nDieser Codec ist das Ergebnis eines experimentellen Projekts. Es wurde dabei kein Code von anderen Codecs wie z. B. Xvid oder x264 übernommen. Die Eigenschaften dieses Codecs sind:\n\n\nratDVD besitzt die Option, beide GPL-Module, mit denen es ausgeliefert wird, nicht zu installieren; das Abspielen von ratDVD-Dateien funktioniert dann aber nicht. ratDVD benutzt anscheinend libdvdnav zum Abspielen, das unter der GPL-Lizenz veröffentlicht wird. Die GPL besagt: \"Linking ABC statically or dynamically with other modules is making a combined work based on ABC. Thus, the terms and conditions of the GNU General Public License cover the whole combination.\" (Kurz: Wenn ein Programm A gegen das Modul B statisch oder dynamisch gelinkt ist, und Modul B unter der GPL steht, muss auch Programm A unter der GPL stehen). Der Quellcode von ratDVD wird aber von den Autoren geheim gehalten, was eine Verletzung der GPL darstellt.\n\n\n"}
{"id": "731998", "url": "https://de.wikipedia.org/wiki?curid=731998", "title": "Xtree", "text": "Xtree\n\nXTree ist ein „baum“-orientierter Dateimanager, für DOS. Die erste Version wurde am 1. April 1985 auf der San Francisco „West Coast Computer Faire“ der Öffentlichkeit vorgestellt (programmiert seit 1983). Im November 1985 vergab die Computerzeitung PC Magazin den Preis „Editors Choice Award“ an Xtree. 1987 wurde die Version XtreePro und 1990 XtreeProGold veröffentlicht. Bei beiden handelt es sich um Weiterentwicklungen.\n\n1993 kaufte Central Point die Firma Executive Systems Inc. (ESI) auf und damit auch die Software XTree. Das Programm sollte Bestandteil der PC-Tools für Windows 3.0 werden. Durch den Verkauf der Firma an Symantec erschien nur noch eine Einzelversion. Die Version 4.0 für Windows ließ sich im normalen Dateimanager- und im XTree-Modus betreiben. \n\nEinzelne Features, wie das Erkennen des Dateiformates im Betrachter auch bei Verwendung einer falschen Dateiendung und die optionale Behandlung von Archiven als Verzeichnis, sind auch heute noch nur vereinzelt bei Dateimanagern zu finden.\n\n\"PC-Tools für Windows 3.0 Dateimanager\" / XTree für Windows 4.0 verfügt über ein eigenes SFX-Datenkompressionsformat.\nEs gibt die als Shareware vertriebenen Nachentwicklungen \"XFile\" und \"ZTreeWin\". Unter Linux existiert eine XTree-Umsetzung mit Namen \"Ytree\".\n\n\n"}
{"id": "734008", "url": "https://de.wikipedia.org/wiki?curid=734008", "title": "No23 Recorder", "text": "No23 Recorder\n\nNo23 Recorder ist ein Audiorekorder mit integriertem CD-Ripper, der wie ein digitaler Kassettenrekorder funktioniert.\n\nDie mit einem beliebigen Format und Audioplayer abgespielten Audiostreams werden mit der Rekord-Funktion manuell und am Stück aufgenommen. Über die Anschlüsse der Soundkarte (Line In, AUX-Eingang, Mikrofon etc.) kann auch von externen Geräten aufgenommen werden.\n\nDer integrierte CD-Ripper ermöglicht zudem ein schnelles Konvertieren von Musik-CDs nach einer Digital Audio Extraction. Durch die automatisch und selbständig arbeitende Jitter-Korrekturfunktion werden Störgeräusche wie Knackser nahezu vollständig beseitigt. So sind auch ältere CDs problemlos konvertierbar. No23 speichert die Dateien direkt im WAV-Format oder aber im platzsparenden MP3- oder Ogg-Vorbis-Format ab.\n\n"}
{"id": "734881", "url": "https://de.wikipedia.org/wiki?curid=734881", "title": "Weta Digital", "text": "Weta Digital\n\nWeta Digital Ltd. ist ein Unternehmen für visuelle Effekte aus Wellington, Neuseeland. Es entstand 1993 als Zweig von Weta Workshop und befasst sich mit digitalen Spezialeffekten.\n\nNeben Industrial Light and Magic, der Effektschmiede von George Lucas, und Sony Pictures Imageworks zählt Weta Digital zu den wichtigsten Unternehmen der Welt für Spezialeffekte.\n\n\n"}
{"id": "734998", "url": "https://de.wikipedia.org/wiki?curid=734998", "title": "Rainlendar", "text": "Rainlendar\n\nRainlendar ist ein Kalender-Projekt für Windows, MacOS X und Linux (ab Rainlendar 2 Beta). Es ist keine freie Software mehr. Darüber hinaus kann das Programm auch als Plugin unter LiteStep eingesetzt werden. Rainlendar zeichnet sich durch eine leicht anpassbare Benutzer-Oberfläche (mittels Skins) aus. Der Kalender kann sich transparent auf den Windows-Desktop legen und ist (per rechter Maustaste) u. a. aus der Windows-Notification-Area zu verwalten. Er verfügt über gängige Funktionen wie Aufgaben-Liste und Erinnerungs-Alarm. Unterschiedliche Ereignistypen können dabei mit verschiedenen Symbolen dargestellt werden. Die Kalendersoftware ist darüber hinaus standardmäßig in der Lage, gebräuchliche Formate wie Outlook- und iCal-Dateien per Plugin zu importieren bzw. zu synchronisieren.\n\nBis zur letzten freien Version enthielt Rainlendar, sowohl für Windows als auch Linux, einen eingebauten Server, über welchen verteilte Rainlendar-Einzelanwendungen (Clients) synchronisiert werden können. In den aktuellen Versionen ist diese Funktion nicht mehr verfügbar. Die Pro-Version unterstützt allerdings WebDAV sowie die Synchronisation über Google Calendar.\n\nEs gibt derzeit Rainlendar in etwa 39 Sprachen (via Language Packs) und tausende Skins mit individuellen Oberflächen-Designs.\n\nAlternativen zu Rainlendar sind zum Beispiel Korganizer, GNOME-Kalender (beide für Linux) oder Mozilla Sunbird.\n\n"}
{"id": "736362", "url": "https://de.wikipedia.org/wiki?curid=736362", "title": "LiMux", "text": "LiMux\n\nLiMux – Die IT-Evolution war ein Projekt der Stadtverwaltung München, die rund 15.000 Arbeitsplatzrechner der städtischen Mitarbeiter mit freier Software zu betreiben. Das Kofferwort LiMux setzt sich aus Linux und München zusammen. Der Zusatz „Die IT-Evolution“ soll den Gegensatz zur „Revolution“, die einen scharfen Schnitt bedeutet, hervorheben. „LiMux – Die IT-Evolution“ sollte eine langsame, aber kontinuierliche Entwicklung sein, die als Ziel eine modernere, den Anforderungen an den Arbeitsplatzrechner besser angepasste IT-Landschaft hatte.\n\nAlle Arbeitsplätze nutzen seit 2009 OpenOffice.org und WollMux. Zweites Ziel war, bis 2013 bei über 80 % der Verwaltungs-PCs das Betriebssystem auf den LiMux Client umzustellen. Aufgrund der für die öffentliche Hand neuartigen Ausrichtung der Software-Beschaffung auf Open Source ist LiMux weltweit sporadisch in den Medien präsent und wird von der proprietären Softwareindustrie und den Befürwortern freier Software gleichermaßen beobachtet.\n\nBis Anfang 2012 hatte die Stadt München nach eigenen Angaben mit dem LiMux-Projekt etwa 25 % der Kosten gegenüber einer Windows-Installation eingespart, des Weiteren hatte sich auch die Fehleranzahl verringert. Eine veröffentlichte Vergleichsrechnung vom November 2012 gab eine Einsparung von mehr als 10 Mio. Euro gegenüber einer vergleichbaren Microsoft-Lösung an. Im Dezember 2013 wurde die Umstellung erfolgreich abgeschlossen. Neben den Kostenersparnissen von über 11 Millionen Euro konnte die Stadt auch mehr Freiheit in der Softwareauswahl gewinnen.\n\n2014 wurde eine Prüfung der gesamten IT-Infrastruktur eingeleitet, um festzustellen, welche Software die Kriterien der Stadt weitestgehend erfülle. Auslöser war die E-Mail-Anbindung eines Smartphones. Das LiMux-Projekt geriet in die Schlagzeilen, nachdem das Linux-basierte E-Mail-System der Stadt München ein komplettes Wochenende lang durch eine einzige E-Mail mit einer überlangen Betreff-Zeile lahmgelegt worden war. Als Ursache für den Ausfall wird ein Fehler in der eingesetzten kommerziellen Spamschutzsoftware UCEPROTECT vermutet, was der Hersteller UCEPROTECT-Orga aber vehement dementierte.\n\nEnde November 2017 wurde vom Stadtrat beschlossen, das Projekt zu beenden und alle Rechner bis zum Jahr 2020 auf Windows umzustellen.\n\nAnlass für die Migration war das Ende des Supports für Windows NT 4 durch Microsoft Ende 2003; dadurch war eine Ablösung des bis dahin genutzten Windows NT 4 nötig. Vor diesem Hintergrund ließ München in einer Vorstudie (Clientstudie) fünf mögliche Konfigurationen der Verwaltungsdesktops unter drei Gesichtspunkten (Wirtschaftlichkeit, Technik, Strategie) untersuchen, von reinen Microsoft-basierten Lösungen bis hin zu reinen Open-Source-Lösungen.\n\nDie Studie ergab letztendlich einen Gleichstand zweier Alternativen. Die Mehrheit des Münchner Stadtrates entschied sich dann für die Lösung, die im Bereich „Strategie“ die vorteilhaftere ist, um damit die auf Herstellerunabhängigkeit ausgelegte IT-Strategie der Stadtverwaltung zu stützen und langfristig den Mittelabfluss selbst bestimmen zu können (= Kostenreduzierung). Der Beschluss besagte nicht, dass fortan ausschließlich Open-Source-Software eingesetzt, sondern nur, dass diese bevorzugt werde. Als wesentlichen Punkt enthielt der Beschluss die Maßgabe, dass zukünftig zu entwickelnde oder öffentlich auszuschreibende Fachverfahren webbasiert implementiert werden sollen. Gerade dies soll eine zu starke Kopplung von Betriebssystem, Officesuite und Fachsoftware verhindern.\n\nIm Winter 2003 reiste Steve Ballmer, Chef von Microsoft, nach München, um mit Oberbürgermeister Christian Ude über das Angebot seiner Firma in Höhe von 36,6 Millionen US-Dollar und die Nachteile eines Abschieds von Microsofts Betriebssystem zu sprechen. Er senkte den Preis zunächst auf 31,9 und dann auf 23,7 Millionen – sein Angebot wurde jedoch abgelehnt.\n\nDie anfängliche Testphase von LiMux ging im September 2006 zu Ende, worauf ab dem 19. September 2006 damit begonnen wurde, die Windows-Arbeitsplätze im Kernbereich der Stadtverwaltung durch ein angepasstes Debian mit K Desktop Environment 3 und OpenOffice.org abzulösen. Die vielfache Linux-Installation läuft dabei durch die freie Software FAI automatisiert ab und wird durch die freie Software GOsa² konfiguriert.\n\nAm 16. Mai 2007 nahm Bürgermeisterin Christine Strobl für das IT-Projekt vom TÜV IT das Zertifikat „Gebrauchstauglicher Basisclient“ entgegen. Der TÜV bestätigte nach einem umfangreichen Zertifizierungsprozess die Gebrauchstauglichkeit des LiMux-Basisclients als Benutzerschnittstelle für interaktive Computersysteme nach der ISO Norm 9241–110. „Entscheidend für diese Einschätzung war, dass mit der neu gestalteten und auf KDE 3 basierenden Oberfläche und den enthaltenen Zusatzprogrammen (u. a. […]) eine effektive, effiziente und zufriedenstellende Arbeit für die Mitarbeiterinnen und Mitarbeiter der Stadtverwaltung möglich ist“.\n\nEine nur in ihrer Zusammenfassung veröffentlichte und daher nicht nachprüfbare Studie von HP vom Januar 2013, die von Microsoft in Auftrag gegeben wurde, kommt dagegen auf Mehrkosten von 43,7 Mio. Euro durch den Systemwechsel weg von Windows. Diese Studie wurde allerdings von der Fachpresse aufgrund falscher Annahmen und fehlender Nachvollziehbarkeit kritisiert. Die Stadt München selbst widerspricht der Studie und weist auf zahlreiche Mängel hin, die bereits in der Zusammenfassung ablesbar sind, ohne selbst die Details der Studie zu kennen. Die Stadt bewertet die Studie als nicht wissenschaftlich fundiert.\n\nAuf dem LinuxTag im Mai 2013, circa zehn Jahre nach Projektbeginn, verkündete der Projektleiter von LiMux die Fertigstellung.\n\nIm Februar 2014 wurde LiMux zu einem Thema bei der Münchener Kommunalwahl. Sabine Nallinger, die Kandidatin der Grünen zur Oberbürgermeisterwahl, sprach sich gegen den Einsatz von Linux in der Münchener Stadtverwaltung aus. Sie behauptete, die Migration zu Linux hätte zu vielen Problemen in der Stadtverwaltung geführt, weshalb sie nun „eine Lösung, die funktioniert, egal mit welcher Software“ wolle. Einen Tag später relativierte die Wahlkandidatin ihre eigenen Aussagen bereits wieder.\n\nDer seit 1. Mai 2014 amtierende Oberbürgermeister Dieter Reiter gilt als Microsoft-Fan und war als Wirtschaftsreferent involviert, als Microsoft Deutschland 2013 entschied, seine Zentrale von Unterschleißheim zukünftig (2016) nach München zu verlegen. Im August 2014 gab Reiter eine Untersuchung zur IT-Infrastruktur der Stadtverwaltung in Auftrag. Auch der zweite Bürgermeister Josef Schmid erklärte, eine Rückkehr zu Microsoft sei nicht ausgeschlossen. Karl-Heinz Schneider, Leiter des internen IT-Dienstleisters it@M erklärte, ihm seien keine Beschwerden oder Störungen bekannt, die über das normal zu erwartende Maß in einer Verwaltung dieser Größenordnung hinausgingen.\n\nEnde Juli 2015 kritisierten die Stadträtin und Diplominformatikerin Sabine Pfeiler (CSU) und der seit 1982 mit Schwerpunkt Softwareentwicklung und IT-Beratung selbstständige Stadtrat Otto Seidl (CSU) in einem gemeinsamen Antrag an den Münchner Stadtrat die mangelnde Alltagstauglichkeit der 2014 für die Stadträte neu angeschafften Notebooks mit vorinstalliertem LiMux. Umständliche Bedienung, Inkompatibilitäten und fehlende Benutzerrechte seien der Grund, warum diese nur sehr eingeschränkt nutzbar seien. Konkret wird bemängelt, dass Programme wie Skype nicht selbst installiert werden können, was einen „normalen Gebrauch“ verhindere, weshalb ein großer Teil der angeschafften Geräte „ungenutzt vor sich hin“ altere. Die beiden Stadträte beantragten, Windows-Lizenzen und Office-Pakete für die Notebooks anzuschaffen und „die Stadträte auch mit den nötigen Benutzerrechten auszustatten“.\n\nIm Februar 2017 beschloss der Stadtrat, bis Ende 2020 anstatt der Open-Source-Lösung einen Windows-Basis-Client mit „marktüblichen Standardprodukten“ zu entwickeln. Die Open-Source-Lösung hinke einerseits im Funktionsumfang mitunter den kommerziellen Produkten aus dem Hause Microsoft hinterher. Andererseits sei sie mit Systemen außerhalb der Verwaltung nicht in dem gewünschten Maße kompatibel.\n\nEnde September 2017 wurde das Informationsangebot zu LiMux von der Münchner Website genommen, obwohl der Münchner Stadtrat das Aus für LiMux noch nicht offiziell beschlossen hatte. In einer Selbstkritik warnte die FSFE davor, einseitig Freie Software oder die Geschäftsgebaren der Firma Microsoft für das Scheitern verantwortlich zu machen. Das Problem in München war vielmehr die starke Fragmentierung der IT und ein schlechtes Projektmanagement sowie unzureichende interne Kommunikation innerhalb der bayerischen Behördenlandschaft.\n\nEnde November 2017 hat der Stadtrat mit den Stimmen der großen Koalition beschlossen, bis 2020 auf Windows umzustellen, es sollten für Bürotätigkeiten und Internetanwendungen „marktübliche Standardprodukte“ eingesetzt werden. Die Opposition kritisierte den „völligen Roll-Back zu Microsoft“ und bezeichnete es als „großes Beispiel für Geldverschwendung“. Ob auch LibreOffice auf Microsoft Office umgestellt werden soll, wurde noch nicht beschlossen.\n\n2003 bis 2004 wurde ein Feinkonzept für die Migration erarbeitet, die im Juni 2004 begann und Kosten in derselben Größenordnung wie eine Microsoft-Lösung haben würde. Die Migration wurde im Sommer 2004 unterbrochen, weil die Stadt die rechtlichen Auswirkungen von Softwarepatenten untersuchen wollte. Ab Ende 2006 startete die eigentliche Migration der Desktops.\n\nIm Mai 2009 waren 1800 Arbeitsplätze auf Linux umgestellt, 12.000 nutzten OpenOffice. Im November 2010 waren 4000 Arbeitsplätze umgestellt, bis Ende 2010 waren es 5000. Mitte April 2011 war die Hälfte, Mitte August 7600, Mitte Dezember 2011 drei Viertel, April 2012 mehr als 10.000, Juni 2012 mehr als 11.000 und November 2012 11.700 der geplanten 12.000 PCs umgestellt. Die Stadt München hatte bis Oktober 2013 über 15.000 Desktop-PCs (von ca. 18.000 Desktops) von Windows NT 4.0 bzw. Windows 2000 und Microsoft Office auf das Betriebssystem Linux und OpenOffice.org umgestellt.\n\nDie \"AG Usability\" der Projektgruppe befragte regelmäßig die Nutzer, um eine gute Anpassung an die Bedürfnisse der Mitarbeiter zu erreichen. Geplant ist ein möglichst einfacher Umgang mit der Software.\n\nLaut einem Artikel im deutschsprachigen Linux-Magazin aus dem Jahr 2014, welches seinerseits die Münchner Stadtverwaltung selbst zitiert, wiesen 10 Jahre vorher (also etwa 2004) 50 % der Arbeitsplatzrechner CPUs mit weniger als 500 MHz auf und lediglich wenige Maschinen haben mehr als 256 MB Arbeitsspeicher. Diese Hardware würde demnach nicht den Anforderungen des aktuellen Microsoft-Betriebssystems Windows 7 mit 1-GHz-CPUs und 1 GB Arbeitsspeicher entsprechen.\n\nDie Umstellung zu OpenOffice.org, teilweise bereits vorab unter Windows, wird durch ein eigens entwickeltes Tool, den \"Eierlegenden WollMux\" (oder kurz WollMux), flankiert. Diese in Java geschriebene Applikation kommuniziert mit OpenOffice.org über die UNO-Schnittstelle. Der WollMux löst einige in München zusammen mit Microsoft Office eingesetzte Applikationen ab. Seine Hauptfunktionen sind:\n\n\n2007 waren über 8000 Nutzer mit dem Programm OpenOffice.org ausgestattet. Laut einem Sprecher des LiMux-Projektes sei die Zufriedenheit der Nutzer damit groß gewesen. Ab Mitte 2010 nutzten alle Büroarbeitsplätze OpenOffice.\n\nWollMux ist seit Ende Mai 2008 als freie Software öffentlich verfügbar.\n\nDer LiMux-Client 4.1, vom August 2012, basiert wie der LiMux-Client 4.0 auf \"Ubuntu 10.04 LTS\" und \"KDE-Desktop 3.5\". Er beinhaltet neben OpenOffice.org, Mozilla Thunderbird und Mozilla Firefox weitere Freie-Software-Produkte.\n\nDie im November 2014 fertiggestellte Version 5.0 basiert auf \"Ubuntu 12.04 LTS\" und dem Desktop \"KDE SC 4.12\".\nAls Office-Anwendung wird LibreOffice in Version 4.1 ausgeliefert. Diese Version wurde mit über 300 Patches aktualisiert. Viele dieser Patches sind auch in neueren LibreOffice-Versionen eingeflossen. Zudem werden Mozilla Firefox und Mozilla Thunderbird in der ESR-Version ausgeliefert. Beide Programme wurden mit KDE-Integrationsmodulen ausgestattet.\n\n\n"}
{"id": "737389", "url": "https://de.wikipedia.org/wiki?curid=737389", "title": "GCompris", "text": "GCompris\n\nGCompris ( \"j'ai compris,\" \"ich habe verstanden\") ist eine freie Lernsoftware für Grundschulen. Die Zielgruppe sind Kinder im Alter von 2 bis 10 Jahren.\n\nDie Suite GCompris ist in mehreren Sprachen und für Plattformen erhältlich und wurde unter der Lizenz \"GNU GPLv3\" veröffentlicht. Ursprünglich wurde das System für GNU/Linux entwickelt. Die Windows-Version stellt nur eine eingeschränkte Anzahl der umfangreichen Funktionen bereit. Diese Beschränkung kann allerdings durch eine kostenpflichtige Aktivierung aufgehoben werden.\n\nGCompris bietet über 100 sogenannte Aktivitäten aus folgenden Bereichen:\n\n\n"}
{"id": "738112", "url": "https://de.wikipedia.org/wiki?curid=738112", "title": "Harvard Graphics", "text": "Harvard Graphics\n\nHarvard Graphics gehört zu den ersten kommerziell verfügbaren Präsentationsprogrammen. Die erste Version erschien 1986 für DOS-basierte Personal Computer; zu Beginn noch unter dem Namen Harvard Presentation Graphics. Benutzer des Programms konnten textbasierte Folien, Organigramme und verschiedene Diagramme erstellen. Die Folien konnten später am Bildschirm vorgeführt oder über Drucker oder Plotter ausgegeben werden, etwa auf Folien zur Vorführung über einen Tageslichtprojektor.\n\nSpätere Versionen von Harvard Graphics boten zusätzliche Diagramme für Folien und die Möglichkeit, über Vorlagen allen Folien einer Präsentation ein einheitliches Aussehen zu geben. 1990 wurde eine Version für OS/2 vorgestellt, 1991 auch eine für Microsoft Windows 3.0.\n\nBis 1990 war Harvard Graphics Marktführer auf DOS-basierten PCs. Ab 1992 begannen jedoch die Hersteller Microsoft, Lotus und WordPerfect sogenannte Office-Pakete auf den Markt zu bringen, die neben Textverarbeitung und Tabellenkalkulation auch je ein Präsentationsprogramm enthielten. Der Marktanteil des als Einzelprodukt verkauften Harvard Graphics ging infolgedessen stark zurück.\n\n2001 wurde Harvard Graphics von \"Serif Incorporated\" gekauft, die es seitdem als \"Pro Presentations\" und \"Advanced Presentations\" vertreibt.\n"}
{"id": "740530", "url": "https://de.wikipedia.org/wiki?curid=740530", "title": "Smoothed Particle Hydrodynamics", "text": "Smoothed Particle Hydrodynamics\n\nSmoothed-particle hydrodynamics (SPH; deutsch: \"geglättete Teilchen-Hydrodynamik\") ist eine numerische Methode, um die Hydrodynamischen Gleichungen zu lösen. Sie wird unter anderem in der Astrophysik, der Ballistik und bei Tsunami-Berechnungen eingesetzt. SPH ist eine Lagrange-Methode, d. h. die benutzten Koordinaten bewegen sich mit dem Fluid mit. SPH ist eine besonders einfach zu implementierende und robuste Methode.\n\nIn Smoothed Particle Hydrodynamics wird die zu simulierende Flüssigkeit in Elemente aufgeteilt. Dabei werden, ähnlich den Monte-Carlo-Methoden, die Elemente zufällig über die Flüssigkeit verteilt. Dies minimiert den zu erwartenden Fehler. Der mittlere Abstand dieser Elemente wird durch die Smoothing Length (Glättlänge) formula_1 repräsentiert. Sie ist der wichtigste Parameter der Methode. Zwischen den Teilchen wird das Fluid durch den Kernel geglättet, daher der Name.\nJede Größe (z. B. die Dichte formula_2) wird durch Summation über alle Teilchen berechnet. Jedes einzelne Teilchen erhält einen Anteil, in Form eines Skalars an dieser Größe. Dadurch werden aus den partiellen Differentialgleichungen der Hydrodynamik gewöhnliche Differentialgleichungen, was die Berechnungen sehr vereinfacht.\nSPH ist eine sehr empirische Methode. Das bedeutet, dass viele Dinge gemacht werden, weil sie funktionieren, nicht, weil es eine strenge mathematische Herleitung gibt.\n\nDie formale Herleitung läuft entweder über eine Lagrange-Funktion oder über eine Integralinterpolation.\nBei der Integralinterpolation für eine Größe formula_3 geht man von einer Identität aus, wobei formula_4 die Diracsche Deltadistribution bezeichnet:\n\nDann wird die formula_6-Distribution durch einen Kern formula_7 angenähert, wobei formula_8 die Glättungslänge ist. Damit die Näherung im Grenzfall gültig bleibt, kann man Normierung und Identität mit der formula_6-Distribution im Grenzwert für \"h\" → 0 fordern:\n\nTatsächlich ist dies bei den meisten verwendeten Kernen nicht mehr der Fall.\nUm daraus die Aufteilung in Massenelemente zu erhalten, erweitert man mit der Dichte formula_12 und belässt formula_1 größer als 0. Für den Fall unendlich vieler, unendlich kleiner Teilchen geht die Summe in das Integral über. Numerisch wird man sich immer mit endlich vielen Teilchen zufriedengeben müssen:\n\nDabei ist formula_15 die Masse des Teilchens b und formula_16 die Dichte am Ort des Teilchens b:\n\nDamit haben wir die Grundgleichung der Smoothed Particle Hydrodynamics hergeleitet (rechter Teil). Die Größe A wird durch eine Summe über alle Teilchen berechnet. Man sieht, dass aus der von r abhängigen Größe formula_18 ein Skalar formula_19 multipliziert mit dem Kernel geworden ist. Dies führt zu einer starken Vereinfachung von Differentialgleichungen, da eine Ableitung nun nicht mehr auf die Größe, sondern nur noch auf den Kernel wirkt:\n\nDer wohl wichtigste Parameter der SPH ist die Glättungslänge formula_21. Sie legt die Auflösung der Methode fest und hat damit starken Einfluss auf Genauigkeit und Rechenaufwand bei Simulationen. Bei entsprechender Wahl des Kernels (siehe unten) legt sie auch die Anzahl der bei Berechnung mit einzubeziehenden Nachbarn fest. Üblich sind bis zu einige zehn Teilchen pro Größe.\nFür gute Ergebnisse orientiert man sich an der mittleren Dichte des Fluids :\n\nmit formula_23 Teilchen, formula_24 Dimensionen und\n\nIn modernen Codes wählt man formula_26 zeitabhängig. Mit\n\nnutzt man dann in Gebieten großer Dichten eine höhere Auflösung, während in Bereichen geringer Dichten die Smoothing Length größer wird. Dadurch lässt sich der Rechenaufwand bei gleich bleibender Genauigkeit verringern.\n\nDer Kern ist die wohl wichtigste Struktur der SPH-Methode. Verschiedene Kerne entsprechen verschiedenen Differenzenschemata in Gittermethoden.\nZur Interpretation von SPH-Gleichungen ist es vorteilhaft, einen Kern in Form einer gaußschen Kurve zu verwenden:\n\nNumerisch ist dieser Ansatz allerdings nicht sehr geeignet, da man in diesem Fall oft auf ein klares Verhalten bezüglich der Reichweite des Kerns Wert legt. D. h. man wählt einen Kern, der ab einem gewissen formula_29 null ist, um die Anzahl der Nachbarn, die bei der Berechnung mit einbezogen werden, klar festlegen zu können. Damit kann man den benötigten Rechenaufwand eingrenzen.\nWie bereits erwähnt, ist SPH eine sehr empirische Methode, d. h. für unterschiedliche Anwendungen werden sehr unterschiedliche Kerne benötigt. Die genaue Wahl ist Erfahrungssache und erfolgt oft nach dem Versuch-und-Irrtum-Prinzip. Da ein Kern oft in einer eigenen Funktion implementiert wird, ist der Aufwand ihn auszutauschen oder zu verändern minimal.\nOft werden Kerne auf Basis von Splines verwendet:\n\nMit formula_31, einer Normierungskonstante formula_32 und der Anzahl der Dimensionen formula_33.\nHier werden nur Teilchen bis zum übernächsten Nachbarn in die Berechnung mit einbezogen. Außerdem ist die 2. Ableitung dieses Kerns nicht konstant, weshalb er nicht von der Unordnung der Teilchen abhängt.\n\nBei der Herleitung über Integralinterpolationsfunktionen wurden zwei Näherungen gemacht. Erstens wurde formula_34 angenommen, und die Summation erfolgt nur über eine endliche Zahl von Teilchen.\nDamit ist man bei Simulationen mit SPH immer auf den Vergleich mit anderen Simulationen angewiesen, zumindest für eine Fehlereinschätzung. Einige Veröffentlichungen erwähnen, dass die Fehler meist deutlich unter denen einer\nMonte-Carlo-Simulation liegen, auch dies ist Erfahrungssache.\nGenerell neigt SPH zur Ausschmierung von Diskontinuitäten, ist also gerade im Falle von Simulationen mit wenigen Teilchen lokal recht ungenau. Für große Teilchenzahlen wird das Verhalten aber deutlich besser. Allerdings ist das globale Verhalten schon bei geringen Teilchenzahlen, was geringem Rechenaufwand entspricht, sehr gut. D. h., globale Größen wie die Energie sind gut wiedergegeben. Oft lässt sich mit SPH eine global gute Simulation mit wenig Aufwand programmieren, die in akzeptabler Zeit auf Workstations gerechnet werden kann.\n\nVorteile:\n\n\nNachteile:\n\n\nUm die Hydrodynamik in SPH zu formulieren, ist der scheinbar einfachste Ansatz die Grundgleichung in die hydrodynamischen Gleichungen wie z. B. die Navier-Stokes-Gleichung einzusetzen. Die daraus resultierenden Gleichungen sind allerdings nicht symmetrisch gegenüber Teilchenvertauschung. Deshalb gelten in diesem Fall viele Erhaltungssätze für Energie, Drehimpuls, etc., nicht mehr.\nOft ist es allerdings möglich, diese zu retten, indem man die Dichte in den jeweiligen Differentialoperator herein schreibt und die Produktregel nutzt:\n\nOft lassen sich so symmetrische Gleichungen herleiten. All dies geschieht nicht streng formal, sondern nur, weil es bessere Ergebnisse liefert.\n\nDie einfachste Möglichkeit ist die Verwendung der Definition der Geschwindigkeit:\n\nDabei ist die Bewegung eines Teilchens nicht an die der anderen gekoppelt, was oft zu Problemen führen kann. Deshalb hat man die XSPH-Methode (\"Extended SPH\") entwickelt:\n\nmit einer gemittelten Dichte:\nund einem Kopplungsparameter ε. Damit wird die Ordnung der Teilchen besser erhalten ohne dass zusätzlich Viskosität eingeführt werden muss.\n\nSetzen wir die Dichte in die Grundgleichung ein, so erhalten wir\n\nfür ein Teilchen a. Daraus lässt sich die SPH-Kontinuitätsgleichung ausrechnen\n\nFür die Euler-Gleichung ergibt sich:\n\nDiese Gleichung ist nicht symmetrisch gegenüber Teilchenaustausch: Impuls und Drehmoment sind nicht erhalten. Deswegen verwenden wir den oben angedeuteten Trick für den Druckgradienten:\n\nWoraus wir die gewünschte symmetrische Gleichung erhalten:\n\nSetzen wir einen Gauß-Funktion ein ergibt sich eine Zentralkraft, die auf beide Teilchen gleich stark wirkt:\n\nWie fast jede numerische Methode erzeugt auch SPH durch Rechenungenauigkeiten Viskosität. Zur Modellierung ist diese oftmals aber nicht ausreichend. Deswegen führt man, ähnlich wie beim Übergang von der Euler-Gleichung zur Navier-Stokes-Gleichung, einen Viskositätstensor ein. Die genaue Wahl dieses Tensors hängt stark vom Modell ab.\n\nSPH findet in vielen verschiedenen Bereichen wie der Astrophysik Anwendung. Es existieren auch relativistische und magnetische SPH-Methoden:\n\n\n\n\n"}
{"id": "742453", "url": "https://de.wikipedia.org/wiki?curid=742453", "title": "Global File System", "text": "Global File System\n\nDas Global File System (GFS) ist ein Cluster-Dateisystem, das es mehreren Rechnern ermöglicht, gleichzeitig auf gemeinsamen Speicher zuzugreifen und das die Konsistenz der gespeicherten Daten gewährleistet. Häufig handelt es sich bei dem gemeinsam genutzten Speicher um ein Storage Area Network (SAN). Das Sperren von Dateien (), ohne das ein Cluster-Dateisystem nicht funktioniert, übernimmt ein Locking-Modul von GFS.\n\nGFS ist Teil des Linux-Kernels und wird in dessen Rahmen entwickelt. Treibende Kraft ist dabei die Firma Red Hat.\n\nGFS ist ursprünglich als Teil einer Machbarkeitsstudie an der Universität von Minnesota entwickelt worden. Später wurde es von Sistina Software übernommen, die es als Open-Source-Projekt weiterentwickelte. 2001 entschied Sistina Software, GFS als proprietäre Software weiterzuentwickeln.\n2003 kaufte Red Hat Sistina Software und veröffentlichte GFS 2004 mit anderen Cluster-Infrastruktur-Programmen unter der GNU General Public License.\n\nZum Mounten von GFS wird das GFS-Kernel-Modul und eine Cluster-Konfiguration benötigt. Zusätzlich sind weitere Kernel-Module und Dienste für das Management (beispielsweise CMAN und ccsd), Locking (zum Beispiel lock_gulm und lock_dlm) und Fencing (zum Beispiel fenced) für das Ausschließen ausgefallener Knoten erforderlich.\n\nDie gemeinsam genutzten Blockspeichergeräte werden häufig via \"Global Network Block Device (GNBD)\" über ein Netzwerk zur Verfügung gestellt. Die Blockgeräte können mit \"gnbd_import\" auf dem Cluster-Knoten importiert und anschließend gemountet werden.\n\nGFS kann verschiedene Locking-Mechanismen verwenden. Empfohlen wird die Verwendung von \"Distributed Lock Manager (DLM)\". Red Hat Enterprise Linux vor Version 5 beinhaltete außerdem \"Grand Unified Lock Manager (GULM)\", von dessen Verwendung abgeraten wird. Für lokale Dateisysteme, auf die exklusiv zugegriffen werden kann, kann das Platzhalter-Modul \"Nolock\" verwendet werden.\n\nMit GFS als Grundlage ist es möglich, einen Diskless Shared Root Cluster aufzusetzen.\n\n\"Global Network Block Device (GNBD)\" ist ein Dienst, der ein Blockgerät über das Netzwerk bereitstellt. So exportierte Geräte können von mehreren Rechnern gleichzeitig importiert werden. Diese mehrfach importierten Blockgeräte bilden oft die Grundlage für GFS. Blockgeräte, die GNBD exportiert, sind beispielsweise Festplatten, Partitionen und \"Logical Volumes\" des Logical Volume Managers (LVM).\n\nGNDB besteht aus Kernel-Modulen und Dienstprogrammen. Es setzt ein funktionierendes (engl. für \"einzäunen\", \"abgrenzen\") voraus. Fencing verhindert E/A-Operationen von Knoten, die aus Sicht des Clusters ein Fehlverhalten aufweisen. Meist kommt dazu der Dienst \"fenced\" zum Einsatz.\n\nEin verbreiteter Irrtum ist die Annahme, Cluster-Dateisysteme könnten verwendet werden, um Daten über ein Netzwerk zu exportieren. Das Dateisystem verwendet Blockspeichergeräte (engl. ) um Daten zu speichern. Im Gegensatz zu konventionellen Dateisystemen gehen Cluster-Dateisysteme \"nicht\" von einer exklusiven Nutzung des Geräts aus. Auf welche Art und Weise mehrere Systeme gleichzeitig auf ein Blockspeichergerät zugreifen, ist für GFS unerheblich. Mögliche Techniken zur gemeinsamen Nutzung von Blockgeräten sind beispielsweise iSCSI und Serial Attached SCSI mit mehreren Initiatoren.\n\n\n"}
{"id": "742507", "url": "https://de.wikipedia.org/wiki?curid=742507", "title": "Hackers on Planet Earth", "text": "Hackers on Planet Earth\n\nHackers on Planet Earth (deutsch: \"Hacker auf dem Planeten Erde\") oder H.O.P.E. (deutsch: Hoffnung) ist eine Reihe von Konferenzen, die durch die Hacker-Zeitschrift 2600 gefördert werden. Emmanuel Goldstein (Pseudonym von Eric Corley), der Chefredakteur der Zeitschrift, gilt als Leiter der Veranstaltung. Seit 1994 fanden zehn Veranstaltungen statt.\n\n\n"}
{"id": "742698", "url": "https://de.wikipedia.org/wiki?curid=742698", "title": "ANSYS (Software)", "text": "ANSYS (Software)\n\nANSYS (Kurzform für \"ANalysis SYStem\") ist eine Finite-Elemente-Software, die von John Swanson entwickelt wurde. Seine 1970 gegründete Firma SASI (Swanson Analysis Systems Inc.) entwickelte die ersten Versionen von ANSYS bis zur Version 5.1. Nach dem Verkauf im Jahre 1994 wurde die Firma in Ansys Inc. umbenannt.\n\nDas Programm ANSYS dient zur Lösung von linearen und nichtlinearen Problemen aus der Strukturmechanik, Fluidmechanik, Akustik, Thermodynamik, Piezoelektrizität, Elektromagnetismus sowie von kombinierten Aufgabenstellungen (Multiphysik). Es besitzt eine Vielzahl von Elementtypen für 1-, 2-, und 3-dimensionale Aufgaben. Das Produkt ANSYS liegt momentan in zwei unterschiedlichen Versionen vor. Hierbei gibt es die klassische Version („ANSYS Classic“) und eine Version mit weitgehend graphischer Bedienerführung („ANSYS Workbench“).\n\nANSYS Classic verfügt über einen grafischen Pre- und Postprozessor, programmiert in Tcl/Tk, zur Definition des Berechnungsproblems. Die meisten Benutzer des klassischen ANSYS benutzen aber auch APDL (Ansys Parametric Design Language), eine eigene Skriptsprache, die zur Eingabe und zur Automatisierung verwendet wird.\n\nANSYS Workbench soll dem Benutzer die Eingabe von Berechnungsproblemen vereinfachen. Neben einer modernen GUI verfügt ANSYS Workbench über verbesserte Algorithmen zur Kontaktfindung, Vernetzung sowie Schnittstellen zu diversen CAD-Systemen. Allerdings enthält ANSYS Workbench noch nicht den vollen Funktionsumfang von ANSYS Classic. Mittels Befehlen in APDL können Funktionen aus ANSYS Classic auch im Workbench verwendet werden. Über die Skriptsprache JScript lässt sich ANSYS Workbench automatisieren und steuern.\n\nANSYS Mechanical ist ein Finite-Elemente-Analyse (FEA) Tool, das komplexe Produktarchitekturen analysiert, um schwierige mechanische Probleme zu lösen. So können mit ANSYS Mechanical reale Komponenten und Teilsysteme simuliert sowie Designvarianten schnell und genau getestet werden.\n\nANSYS besitzt eine Reihe von Schnittstellen, um mit anderen Programmen zu kommunizieren und Daten oder Befehle auszutauschen. Neben den ANSYS-eigenen Sprachen APDL (ANSYS Parametric Design Language) sowie UIDL (User Interface Design Language) besitzt ANSYS Schnittstellen zu Python, C#, Fortran, C, Tcl/Tk und zu LS-DYNA.\n\nANSYS Workbench kann aus den gängigen 3D-CAD-Programmen direkt die Geometrie-Daten einlesen (entweder im Dateiformat des CAD-Programms oder im herstellerunabhängigen IGES-Dateiformat).\n\n"}
{"id": "743984", "url": "https://de.wikipedia.org/wiki?curid=743984", "title": "Kleincomputer", "text": "Kleincomputer\n\nDie Bezeichnung Kleincomputer ist\n\n\n\n"}
{"id": "745372", "url": "https://de.wikipedia.org/wiki?curid=745372", "title": "3Lux", "text": "3Lux\n\n3Lux ist eine dreiteilige Grafik- und Musikvideoproduktion und war Anfang der 1990er Jahre der erste Versuch junger Grafikdesigner um Rainer Remake, die noch junge und neuartige Technomusik mit beweglichen Bildern zu untermalen. Das Projekt gilt als Meilenstein sowohl in der Technogeschichte als auch in der Computerkunst. Die Veröffentlichungen erschienen auf dem Berliner Label STUD!O K7 und sind Vorgänger der X-Mix-Reihe.\n\nMit dem Aufkommen von Techno und House und dessen zunehmender Präsenz in den Medien entstand im Musikvideo-Markt die Forderung nach Clips, mit denen die Musik verkauft werden konnte. Die Reihe basierte auf der Idee die mittels Computern geschaffene Musik entsprechend durch Computeranimationen und neue digitale Technologien mit Bildern und Animationen visuell zu verbinden und dabei die Aufbruchstimmung und das Lebensgefühl der jungen Szene widerzuspiegeln. 3Lux visualisierte dabei nicht einzelne Tracks, sondern ganze Mixe der damals angesagten Künstler wie Visions of Shiva, Aphex Twin, Cosmic Baby und Dave Angel aus den Bereichen Ambient, Trance, House und Acid Techno. So entstanden virtuelle Landschaften, bunte Collagen, psychedelische Traumbilder und geometrische Figuren, die zum Rhythmus der Musik pulsierten.\n\nDie finnischen Gruppen Jumalauta und nrgnation kreierten ein Tribut anlässlich der 3Lux Serie für Mac OS X aus dem Jahr 2005, mit dem Namen \"3LUX 2006 Preview\".\n\n\n"}
{"id": "748050", "url": "https://de.wikipedia.org/wiki?curid=748050", "title": "Störungstheorie (Quantenmechanik)", "text": "Störungstheorie (Quantenmechanik)\n\nDie Störungstheorie ist eine wichtige Methode der theoretischen Physik, die Auswirkungen einer kleinen Störung auf ein analytisch lösbares System untersucht. Vor der Erfindung des Computers war es nur durch solche Methoden möglich, Näherungslösungen für analytisch nicht geschlossen lösbare Probleme zu finden. Entwickelt wurden ihre Methoden in der klassischen Physik (siehe Störungstheorie (Klassische Physik)) zunächst vor allem im Rahmen der Himmelsmechanik, bei der die Abweichungen der Planetenbahnen von der exakten Lösung des Zweikörperproblems, also den Ellipsen, durch Wechselwirkung mit anderen Himmelskörpern untersucht wurden.\n\nWie auch in der klassischen Mechanik wird in der Quantenmechanik die Störungstheorie dazu verwendet, Probleme zu lösen, bei denen ein exakt lösbares Grundsystem einer kleinen Störung ausgesetzt ist. Das kann ein äußeres Feld sein oder die Wechselwirkung mit einem anderen System, Beispiele hierfür sind das Heliumatom und andere einfache Mehrkörperprobleme. Allerdings dienen die hier vorgestellten Methoden nicht dazu, \"echte\" Mehrteilchenprobleme (im Sinne einer großen Teilchenzahl) zu lösen – dazu verwendet man Verfahren wie die Hartree-Fock-Methode oder die Dichtefunktionaltheorie. Außerdem können einfache Störungen durch zeitabhängige Felder beschrieben werden, deren korrekte Beschreibung jedoch erst durch eine Quantenfeldtheorie erfolgt.\n\nDie stationäre (oder zeitunabhängige) Störungstheorie kann bei Systemen angewendet werden, bei denen der Hamiltonoperator aus einem diagonalisierbaren Anteil und genau einer Störung besteht, die beide zeitunabhängig sind:\n\nDabei soll der reelle Parameter formula_2 so klein sein, dass die Störung das Spektrum von formula_3 nicht zu sehr verändert. Für die Konvergenz der Störungsreihe gibt es allerdings keine genauen Regeln; sie muss im konkreten Fall explizit nachgeprüft werden. Im Folgenden seien zum ungestörten Hamiltonoperator formula_3 die orthonormalen Eigenvektoren formula_5 und Eigenwerte formula_6 bekannt. Zusätzlich sollen die Eigenwerte des ungestörten Problems nicht entartet sein.\n\nDer Ansatz zur Lösung des kompletten Eigenwertproblems besteht in einer Potenzreihe in formula_2 für die gestörten Eigenwerte und -zustände eine Potenzreihe im Parameter formula_2 an. \n\nMan nennt die formula_11 und formula_12 die Korrekturen formula_13-ter Ordnung des Systems. Konvergiert die Reihe, so erhält man auf diese Weise den Eigenzustand formula_14 des gestörten Systems mit Hamilton-Operator formula_15 und dessen Energie formula_16,\nbzw. durch Abbruch der Reihe eine Approximation der entsprechenden Ordnung an diese.\n\nEinsetzen der Potenzreihen liefert mit der Konvention formula_18\nbeziehungsweise durch Koeffizientenvergleich die Folge von Gleichungen\n\nDiese Gleichungen können iterativ nach formula_12 und formula_11 aufgelöst werden. Dabei ist die Lösung nicht eindeutig bestimmt, denn aus der Gleichung für formula_23 ist erkennbar, dass jede Linearkombination von formula_24 und formula_25 eine gültige Lösung ist. Eine geeignete zusätzliche Annahme zur eindeutigen Bestimmung der Störterme ist die Forderung nach der Normiertheit der Zustände:\n\nDa der Zustand formula_14 ebenfalls normiert sein soll, folgt mit\ninsbesondere für das Verhältnis zwischen dem Störterm erster Ordnung und dem ungestörten Zustand\nDas Skalarprodukt zwischen ungestörtem Zustand und der ersten Korrektur ist also rein imaginär. Mittels einer geeigneten Wahl der Phase von formula_14, also einer Eichtransformation, kann erreicht werden, dass ebenfalls der Imaginärteil verschwindet, denn es gilt:\nund somit \nAufgrund der freien Wahl der Phase formula_33 folgt formula_34. Dadurch, dass die Zustände formula_35 und formula_36 orthogonal sind, erhält man in erster Ordnung die Korrekturen\n\nund für die Korrektur der Energie in zweiter Ordnung\n\nDie Zustände formula_40 lassen sich nach den orthonormalen Eigenzuständen des ungestörten Problems aufgrund deren Vollständigkeit entwickeln. Bei dieser Darstellung der Korrekturen ist jedoch nur der Projektor auf den zu formula_25 orthogonalen Unterraum zu verwenden:\n\nDie Gleichung erster Ordnung lautet:\n\nMultipliziert man von links formula_45 und nutzt dabei die Bra-Eigenwertgleichung formula_46 des ungestörten Hamiltonoperators sowie die Orthonormalität formula_47 aus\n\nerhält man die \"Energiekorrektur erster Ordnung\":\n\nDie Gleichung erster Ordnung mit entwickeltem formula_24 lautet:\n\nNun multipliziert man von links formula_52 und erhält\n\nDas ergibt die Entwicklungskoeffizienten formula_54\n\nund eingesetzt in obige Entwicklung nach den Eigenzuständen des ungestörten Problems erhält man die \"Zustandskorrektur erster Ordnung\":\n\nDie Gleichung zweiter Ordnung ist\n\nMultipliziert man von links formula_45 und nutzt dabei die Bra-Eigenwertgleichung formula_46 des ungestörten Hamiltonoperators sowie die Orthonormalität formula_47 aus, so erhält man\n\nSo ergibt sich die \"Energiekorrektur zweiter Ordnung\", wobei man formula_24 aus erster Ordnung einsetzt:\n\nDie Gleichung zweiter Ordnung mit entwickeltem formula_24 und formula_65 lautet:\n\nNun multipliziert man von links mit formula_52 und erhält\n\nSo erhält man die Entwicklungskoeffizienten zweiter Ordnung, formula_69:\n\nMit formula_71 und formula_72 sowie formula_73 erhält man schließlich:\n\nDie \"Zustandskorrektur zweiter Ordnung\", entwickelt nach den Eigenzuständen des ungestörten Problems, ist somit:\n\nDie Energiekorrektur formula_76-ter Ordnung lässt sich allgemein angeben:\n\nZur Berechnung muss allerdings die Zustandskorrektur formula_78-ter Ordnung, formula_79 bekannt sein.\n\nEine notwendige Bedingung für die Konvergenz einer störungstheoretischen Entwicklung ist, dass die Beiträge der Wellenfunktionen höherer Ordnung klein gegenüber denen niedrigerer Ordnung sind. Terme höherer Ordnung unterscheiden sich um Faktoren der Größenordnung formula_80 von denen niedrigerer Ordnung. Somit folgt die Bedingung:\n\nIm Allgemeinen ist diese Bedingung jedoch nicht hinreichend. Allerdings ist es bei divergierenden Reihen möglich, dass die Näherungen niedriger Ordnung die exakte Lösung gut approximieren (asymptotische Konvergenz).\n\nAn dem Ergebnis für formula_83 ist das Vorzeichen bemerkenswert: Bei Verschwinden der Effekte erster Ordnung wird die \"Grundzustandsenergie\" formula_84 durch die Störung stets energetisch \"erniedrigt\" gegenüber formula_85, und zwar durch Beimischung höherer angeregter Zustände (siehe formula_36, Energie-Erniedrigung durch „Polarisation“).\n\nZur Konvergenz ist noch zu bemerken, dass man mit der Frage nach ihrer Gültigkeit auf sehr tiefliegende Probleme geführt wird. Selbst ein scheinbar so einfaches Beispiel wie ein „gestörter harmonischer Oszillator“ mit dem Hamilton-Operator formula_89 ist \"nichtkonvergent\", selbst für formula_90 Denn bei Konvergenz wäre das System sogar holomorph („analytisch“) bezüglich formula_2 und besäße somit sogar einen positiven Konvergenzradius formula_92 Dies stünde im Widerspruch zu der Tatsache, dass für kleine negative Werte des Störparameters formula_2, d. h. noch innerhalb des Konvergenzkreises, der Hamiltonoperator sogar nach unten \"unbeschränkt\" wäre und folglich gar kein diskretes Spektrum besitzen könnte.\n\nAn diesem nur scheinbar einfachen Beispiel, das in vielen Veranstaltungen als Standardaufgabe für den Formalismus der Störungsrechnung dient, sieht man, wie tiefliegend die Probleme eigentlich sind, und dass man sich von Anfang an damit begnügen sollte, dass die Störungsreihe in allen Fällen, selbst bei Nichtkonvergenz, als „asymptotische Näherung“ einen Sinn ergibt, in den meisten Fällen „nur“ als asymptotische Näherung. Man sollte aber auf jeden Fall erkennen, dass sie auch unter diesen Umständen wertvoll bleibt. In konkreten Fällen ist es darüber hinaus möglich, Gültigkeitsbereiche für die Näherungen anzugeben.\n\nDie formula_94 sind die Eigenfunktionen zum ungestörten Operator formula_3 mit den entsprechenden Eigenwerten formula_96.\nHier erkennt man auch das Problem bei der Behandlung von entarteten Zuständen in der Störungstheorie, da die Nenner verschwinden würden. Um dieses Problem zu lösen, muss eine unitäre Transformation durchgeführt werden, um in den entarteten Eigenräumen formula_97 und formula_98 zu diagonalisieren. Danach treten die problematischen nichtdiagonalen Quadrate nicht mehr auf.\n\nEs liege jetzt ohne Störung Entartung vor (z. B. formula_99). Dann erhält man die (nicht notwendig verschiedenen) Energiewerte formula_100 , für formula_101, und die zugehörigen Eigenvektoren formula_102 durch Diagonalisierung der hermiteschen formula_103-Matrix formula_104, für formula_105. Die auf diese Weise erhaltenen Zustandsvektoren formula_106 nennt man „die richtigen Linearkombinationen“ nullter Näherung (formula_107).\n\nZeitabhängige Störungstheorie findet ihre Anwendung zur Beschreibung von einfachen Problemen, wie der inkohärenten Bestrahlung von Atomen durch Photonen oder bietet ein Verständnis für induzierte Absorption bzw. Emission von Photonen. Zur vollständigen Beschreibung sind jedoch die weitaus komplizierteren Quantenfeldtheorien nötig. Außerdem lassen sich wichtige Gesetze wie Fermis Goldene Regel ableiten.\n\nIn der Quantenmechanik wird die Zeitentwicklung eines Zustandes durch die Schrödingergleichung bestimmt. formula_108 beschreibt eine Familie von Hamiltonoperatoren. Gewöhnlich sind diese allerdings nicht zeitabhängig.\n\nAuch jetzt können die Systeme scheinbar \"separat\" behandelt werden:\n\nDie Gleichung wird formal durch einen Zeitentwicklungsoperator formula_110 gelöst, der die Zustände zu verschieden Zeiten verbindet und folgende Eigenschaften hat.\n\nDie allgemeine Lösung zu einer Anfangsbedingung wie formula_112 ist damit\n\nAus der Schrödingergleichung für den Zeitentwicklungsoperator lässt sich durch einfache Integration eine entsprechende Integralgleichung ableiten\n\nDurch Iteration, indem immer wieder die Gleichung in sich selbst eingesetzt wird, entsteht die sogenannte Dyson-Reihe\n\nSchließlich kann man diesen Ausdruck noch weiter formalisieren durch die Einführung des Zeitordnungsoperators formula_116. Dieser wirkt auf einen zeitabhängigen Operator formula_117 in der Weise, dass\nAndernfalls werden die Argumente entsprechend vertauscht. Durch Anwendung auf die Integranden in der Dyson-Reihe kann nun bei jeder Integration bis formula_119 integriert werden, welches mit dem Faktor formula_120 ausgeglichen wird. Die Reihe bekommt damit die formale Form der Taylorreihe der Exponentialfunktion.\nDamit ist das Zeitentwicklungsproblem für jeden Hamiltonoperator gelöst. Die Dyson-Reihe ist eine Neumann-Reihe.\n\nBetrachtet man einen allgemeinen Hamiltonoperator formula_108, so lässt sich dieser in den freien Hamiltonoperator formula_123 und einen Wechselwirkungsterm formula_124 zerlegen. Wir wechseln nun in der Anschauung vom hier verwendeten Schrödingerbild hin zum Dirac-Bild (bzw. Wechselwirkungsbild; siehe auch Mathematische Struktur der Quantenmechanik#Zeitliche Entwicklung). Im Wechselwirkungsbild wird die Zeitentwicklung, die auf dem zeitunabhängigen Hamiltonoperator formula_3 beruht von den Zuständen auf die Operatoren \"gezogen\". formula_123 lässt dies unberührt: formula_127, wobei formula_128 der Zeitentwicklungsoperator für formula_123 ist. Für den Wechselwirkungsteil entsteht der neue Operator formula_130\nDer Zeitentwicklungsoperator formula_132 für formula_133 ist durch die sogenannte Dyson-Reihe gegeben:\nDie Zeitentwicklung des gesamten Hamiltonoperators ist damit gegeben durch\n\nBetrachtet man nun die \"Übergangsraten\" formula_136 (physikalische Dimension: Zahl der erfolgreichen Übergangsversuche/(Zahl der Übergangsversuche \"mal Zeitdauer\") ) zwischen Eigenzuständen formula_137 des ungestörten Hamiltonoperators, so ist es möglich nur mit der Zeitentwicklung von formula_130 auszukommen, das heißt\n\nBemerkenswerterweise geht hier nur das Betragsquadrat des Matrixelements ein. Nichtdiagonale Matrixelemente treten \"nicht\" auf (was dagegen bei \"kohärenten\" Prozessen der Fall wäre, z. B. beim Laser), weil die freie Zeitentwicklung der Eigenzustände lediglich eine komplexe Zahl mit Betrag 1 ist. Man muss dabei nur berücksichtigen, dass die Wellenfunktionen im Schrödingerbild aus denen im Wechselwirkungsbild durch Multiplikation mit e-Funktionen der Art formula_140 hervorgehen.\n\nformula_141 kann mit Hilfe der Dyson-Reihe genähert werden. In der ersten Ordnung wird nur der erste Term dieser Reihe berücksichtigt\n\nDie Übergangsrate ergibt sich dann nach Rechnung zu folgendem Ausdruck, wobei formula_143 und formula_144 die entsprechenden Eigenenergien sind und formula_130 wieder wie oben ersetzt wurde:\n\n(Die Exponentialfaktoren entstehen durch Einsetzen der \"U\"-Operatoren).\n\nNimmt man an, dass die Störung nur von zeitlich begrenzter Dauer ist, dann kann man den Startpunkt unendlich weit zurückschieben und den Zielpunkt unendlich weit in die Zukunft legen:\n\nDadurch entsteht die Fouriertransformierte des Betragsquadrates des Skalarproduktes. Das ergibt das Betragsquadrat der Fouriertransformierten, die meistens geschrieben wird als formula_148 multipliziert mit einer Deltafunktion formula_149 welche einerseits als Fouriertransformierte der reellen Achse interpretiert werden kann (also im Wesentlichen das „pro Zeiteinheit“ in der Definition „Übergangsrate = Übergangswahrscheinlichkeit pro Zeiteinheit=Ableitung der Übergangswahrscheinlichkeit nach der Zeit“ repräsentiert) und andererseits die Energieerhaltung explizit macht.\nMit den Abkürzungen formula_150 und dem Energieausdruck formula_151 schreibt sich die Übergangsrate schließlich:\n\nZur Sicherheit überprüft man die Dimensionen: formula_153 hat die Dimension 1/Zeit, wie man es für eine Übergangsrate erwartet. Die rechte Seite ergibt ebenfalls diese Dimension, weil die Deltafunktion die Dimension 1/E hat, während die Dimension von formula_154 gleich formula_155 ist und formula_156 die Dimension (Zeit • Energie) hat.\n\nMan kann hier bei gegebenem formula_13 über die Endzustände formula_158 integrieren, formula_159; oder umgekehrt, formula_160; oder bei festem formula_13 und formula_158 über die Frequenz formula_163 eines zugeschalteten Feldes („induzierte Absorption“) und erhält dann (z. B. für ein Kontinuum von Endzuständen) Formeln der Art\n\nDiese Formel, oder die vorangegangene Beziehung, ist auch als Fermis Goldene Regel bekannt.\n\nHierbei ist formula_165 die Energiedichte der Endzustände (physikalische Dimension: 1/E) und der Querstrich auf der rechten Seite über dem Matrixelement bezeichnet eine Mittelung. Durch die Integration ist jetzt die Deltafunktion verschwunden. In der Dimensionsanalyse ersetzt die Energiedichte formula_165  (Dimension: 1/E) eine Summation (bzw. Integration) über die Deltafunktion.\n\nDie Mittelung in diesem Falle ist „quadratisch“, also als \"inkohärent\" zu bezeichnen (Nichtdiagonalelemente gehen nicht ein). An dieser Stelle, d. h. durch diese Näherung, die ungültig wird, wenn man wie beim Laser \"kohärent\" mitteln muss, befindet sich die „Bruchstelle“ zwischen der (reversiblen) Quantenmechanik und der (in wesentlichen Teilen irreversiblen) Statistischen Physik.\n\nIm Folgenden wird eine wenig formale, fast „elementar“ zu nennende Darstellung gegeben, die auf ein bekanntes Buch von Siegfried Flügge zurückgeht:\n\nEs sei \"V(t)=V e\"\"+V e\" oder gleich einer Summe bzw. einem Integral solcher Terme mit verschiedenen Kreisfrequenzen \"ω\", wobei die Operatoren wegen der Hermitizität von \"V(t)\" stets \"V\" formula_167 \"V\" erfüllen müssen (d. h. die beiden Operatoren \"V\" und \"V\" müssen zueinander \"adjungiert\" sein).\n\nEs wird nun zunächst der Operator \"H\" diagonalisiert: Der Einfachheit wird ein vollständig diskretes Eigenfunktionssystem \"ψ\" mit den nicht entarteten zugehörigen Eigenwerten \"E\" (=formula_168) angenommen, und es wird zusätzlich angenommen, dass ein beliebiger Zustand des Systems \" \"H+V(t)\" \" erhalten werden kann, indem man die Zustände \"ψ\" mit zeitabhängigen komplexen Funktionen \"c(t)\" multipliziert (d. h. die aus dem Schrödingerbild bekannten Entwicklungskoeffizienten c werden jetzt zeitabhängige Funktionen).\n\nMan startet zur Zeit \"t\" mit einem Zustand \"c=1\", \"c=0\" sonst. Die Übergangsrate ist jetzt einfach der Limes \"|c(t)|/(t-t\"), genommen im doppelten Limes \"t → ∞\", \"t → -∞\", und man erhält die angegebenen Ergebnisse.\n\n\nKlassische Literatur\n\nAktuelle Literatur\n"}
{"id": "748458", "url": "https://de.wikipedia.org/wiki?curid=748458", "title": "Netcat", "text": "Netcat\n\nNetcat, auch nc genannt, ist ein einfaches Werkzeug, um Daten von der Standardein- oder -ausgabe über Netzwerkverbindungen zu transportieren. Es arbeitet als Server oder Client mit den Protokollen TCP und UDP. Die Manpage bezeichnet es als \"TCP/IP swiss army knife\" (Schweizer Taschenmesser für TCP/IP).\n\nDas ursprüngliche Programm wurde 1996 von einer unbekannten Person mit dem Pseudonym \"Hobbit\" für die UNIX-Plattform geschrieben und ist inzwischen auf praktisch alle Plattformen portiert worden.\n\nNetcat ist ein typisches Unix-Programm, das die grundlegende Unix-Philosophie implementiert. Insbesondere arbeitet Netcat mit der \"universellen Schnittstelle\" von Datenströmen, ohne dabei deren Inhalt weiter klassifizieren zu müssen.\nDa Netcat ganz abstrakt mit sämtlichen Datenströmen arbeiten kann, lassen sich beliebig komplexe Arbeitsabläufe mit Netcat realisieren, vom einfachen Kopieren von Dateien über Streamen von Datenbeständen bis hin zu komplexen Proxy- oder Gateway-Diensten über Netzwerkgrenzen hinweg.\n\nEs gibt unterschiedliche Implementierungen von netcat, welche sich unter anderem in der Syntax unterscheiden. Im Folgenden ist die ursprüngliche Variante von \"Hobbit\" beschrieben.\n\nGrundsätzlich unterscheidet Netcat zwischen zwei verschiedenen Modi:\n\nIn beiden Fällen gibt Netcat über Netzwerk einkommende Daten auf der Standardausgabe aus, während per Standardeingabe eingelesene Daten über das Netzwerk an den Kommunikationspartner geschickt werden. Werden diese Ein- und Ausgaben nicht umgeleitet, kann der Anwender diese eingeben und lesen, d. h. hier fungieren die zwei Netcat-Aufrufe als ein einfaches Chat-Programm.\nDurch Umleitung oder Verwendung von Pipes oder FIFOs kann Netcat in vielen Fällen Netzwerkkommunikationsfähigkeiten ermöglichen, wo sie nicht implementiert sind, zum Beispiel in Shells. So ist die oben rechts dargestellte Illustration allgemein gültig, da \"stdin\" und \"stdout\" beliebig angepasst werden können.\n\nDie Datei mit dem Namen codice_3 vom Computer \"start\" soll unter dem Namen codice_4 auf einem Computer \"ziel\" abgelegt werden, wobei der Transfer via TCP-Port 2000 abgewickelt wird. Auf einer Shell am Computer \"ziel\" wird dafür Netcat im Server-Modus gestartet. Die Standardausgabe wird mit dem Umleitungsoperator in die Datei codice_4 umgeleitet:\n\n$ netcat -l 2000 > kopie\n\nNachdem der Server auf Computer \"ziel\" läuft, kann auf Computer \"start\" in einer Shell Netcat im Client-Modus gestartet werden. Mithilfe eines Umleitungsoperators liest die Shell den Inhalt der Datei codice_3 aus und schreibt ihn in die Standardeingabe des aufgerufenen \"netcat\"-Prozesses:\n\n$ netcat ziel 2000 < original\n\nIm fehlerlosen Fall erzeugt weder die Netcat-Instanz auf Computer \"start\", noch auf Computer \"ziel\" irgendwelche Ausgaben auf der Shell. Sie beenden sich nach Fertigstellung des Transfers (da die Shell durch den '<'-Operator bedingt automatisch ein \"EOF\"-Zeichen sendet).\nAuf weitere eingehende Daten wartet die Instanz auf dem Computer \"ziel\", wenn sie mit dem Parameter \"-k\" gestartet wird. In diesem Fall muss sie zum Beispiel per Tastenkombination + beendet werden.\n\nNach demselben Schema laufen die meisten Dateiübertragungsszenarien, die sich Netcat zunutze machen. Eine alltäglichere Abwandlung dieses Szenarios ist die zusätzliche Verwendung des Packerprogramms tar, mit welchem komplette Verzeichnisstrukturen über Netzwerk kopiert werden können. Typischerweise kommt es in diesen Szenarien zu regelrechten Kaskaden von Programmen, die mittels Pipes miteinander verbunden sind, zum Beispiel in Anlehnung an das obige Beispiel ein Kommando auf dem Client-Rechner:\n\n$ tar vc * | gzip | netcat ziel 2000\n\nsowie dazugehörig ein analoges Kommando auf dem Zielrechner:\n\n$ netcat -l 2000 | gunzip | tar vx\n\nIn diesem Beispiel werden alle Dateien im aktuellen Arbeitsverzeichnis von codice_7 zu einem Stream gepackt, der über die Standardausgabe ausgegeben wird, von dem Kompressionsprogramm gzip komprimiert wird und dann über Netcat an eine Netcat-Serverinstanz auf dem Zielrechner geschickt wird, von gzip wieder dekomprimiert wird und von tar wieder zu einer Verzeichnisstruktur entpackt wird.\n\nStatt eines Netcat-zu-Netcat Transfers kann Netcat auch die Sprache höherer Protokolle sprechen, wie zum Beispiel HTTP. Folgendes Kommando startet einen Webserver auf dem lokalen Computer, welcher dem ersten Webbrowser die Datei \"hallo.txt\" präsentiert.\n\n$ ( echo \"HTTP/1.0 200 Ok\"; echo; cat hallo.txt; ) | netcat -l 8090\n\nZum Abrufen wird im Webbrowser die URL codice_8 eingegeben. Dieser zeigt dann einen Speicherdialog oder die Datei direkt an.\n\nNetcat lässt sich ähnlich leicht dazu verwenden, Ports umzuleiten und so beispielsweise gezielt unzureichende Firewalls zu umgehen. Solche Anwendungsszenarien können schnell in eine rechtliche Grauzone gelangen, siehe dazu Missbrauch durch Netcat.\n\nDie obige Grafik illustriert die Verwendung von Netcat als offenen Proxyserver, in Kombination mit inetd. Wenn beispielsweise der mit \"Client\" beschriftete Host keine direkte Verbindung mit einem speziellen TCP-Port des mit \"Server\" beschrifteten Hosts aufbauen kann, so kann er sich den obigen Aufbau zunutze machen, in dem er statt der direkten Verbindung zum Server eine Verbindung zu einem speziellen TCP-Port des mit \"Proxy\" beschrifteten Host aufbaut, an dem \"inetd\" lauscht. Dieser ruft daraufhin eine Netcat-Clientinstanz auf, die ihrerseits (voreingestellt) eine Verbindung zu dem TCP-Port des eigentlichen Servers aufbaut. Nun werden alle Ausgaben des eigentlichen Servers unverändert über Netcat und Inetd an den Client durchgereicht und umgekehrt. Auf diese Weise kann der Client völlig transparent mit dem eigentlichen Server kommunizieren, als wäre der Proxy gar nicht vorhanden.\n\nNetcat wird oft mit sicherheitsrelevanten Fragen in Verbindung gebracht. Hierbei treten üblicherweise zwei essentielle Fragestellungen auf.\n\nNetcat ändert grundsätzlich nicht den Datenstrom. Daher verschlüsselt Netcat auch die zu übertragenden Daten nicht, bevor sie den Computer verlassen. Eine Verschlüsselung lässt sich im Anwendungsszenario beispielsweise durch Pipes vorschalten (siehe oben) oder es muss eine der Netcat-Weiterentwicklungen verwendet werden. Alternativ muss von vorneherein zu Programmen gegriffen werden, bei denen Verschlüsselung ein zentraler Bestandteil ist, zum Beispiel der OpenSSL-Client/Server als SSL-Implementierung oder SSH bzw. Secure Copy (SCP) zum sicheren Streamen oder Kopieren von Daten.\n\nDurch seine Universalität kann Netcat auch verwendet werden, um beispielsweise Backdoors auf einem System einzurichten. So stufen diverse Antivirenprogramme, unter anderem McAfee VirusScan, Ikarus, Avira AntiVir, AVG-Antivirus, Norton AntiVirus, Kaspersky Anti-Virus, Sophos AntiVirus und G Data Antivirus, das Programm codice_9 als Werkzeug zur Sicherheitsüberprüfung oder als potentiell unerwünschtes Programm ein und verhindern dessen Ausführung.\n\nChris Wysopal hat Netcat auf Windows portiert. \"GNU Netcat\" ist eine komplette Reimplementierung und vollständig POSIX-kompatibel und wird von Giovanni Giacobbi betreut. \"OpenBSD Netcat\" ist eine weitere Reimplementierung, die aber nicht komplett kompatibel zum Ur-netcat ist, da die Syntax an OpenBSD-Standards angepasst wurde. Andreas Bischoff hat die Windows-Version auf \"Windows CE (Pocket PC\" und \"Handheld PC)\" portiert.\n\nCryptcat ist eine Weiterentwicklung von Netcat, die Verschlüsselung implementiert. Socat ist eine Reimplementation und kann außer TCP und UDP auch SCTP verwenden, über Proxyserver arbeiten und unterstützt ebenfalls Verschlüsselung. Eine weitere nicht vollständig kompatible Variante ist Netcat6, das auch IPv6 unterstützt und diverse Performance-Optimierungen enthält, unter anderem den Nagle-Algorithmus.\n\nAuch der Portscanner Nmap liefert eine um viele Features ergänzte Netcat-Reimplementierung namens Ncat mit. Diese kann auch über IPv6, SCTP und \"Unix Domain Sockets\" kommunizieren und HTTP- und SOCKS-Proxys verwenden (ersteres auch serverseitig). An Sicherheits-Features unterstützt Ncat SSL für Verschlüsselung und Authentifizierung sowie Zugangsbeschränkungen auf Hostname-Basis. Des Weiteren sind ein zum Datenaustausch zwischen mehreren Clients dienender \"Connection-Brokering\"-Modus und ein darauf aufbauender einfacher Chat-Server enthalten.\n\n"}
{"id": "748504", "url": "https://de.wikipedia.org/wiki?curid=748504", "title": "Beowulf (Cluster)", "text": "Beowulf (Cluster)\n\nBeowulf ist ein Cluster, der unter dem freien Betriebssystem Linux oder BSD läuft. Mittlerweile gibt es auch spezielle Linux-Versionen für Cluster wie beispielsweise ClusterKnoppix.\n\n\"Beowulf\" ist nicht die Bezeichnung für eine bestimmte Software, sondern bezeichnet vielmehr den Aufbau des Clusters, der aus vernetzten handelsüblichen Personal Computern (Class I Cluster, siehe auch Off-The-Shelf-Computer) oder auch aus spezialisierter Hardware besteht (Class 2 Cluster). Die einzelnen Rechner kommunizieren über IP. Dabei ermöglicht es Software wie Message Passing Interface und PVM, Rechenoperationen aufzuteilen. Die Leistungsfähigkeit lässt sich durch das Hinzufügen weiterer Rechner steigern.\n\nDas „Beowulf-Projekt“ wurde 1994 von Donald Becker und Thomas Sterling ins Leben gerufen, um eine sowohl kostengünstige als auch leistungsfähige Alternative zu Supercomputern zu entwickeln. Heute werden Beowulf-Rechner z. B. zur Forschung an Universitäten eingesetzt. So unterhielt z. B. das Rechenzentrum der TU Chemnitz den Beowulf-Cluster „CLIC“, der von der Chemnitzer Megware im Jahr 2000 installiert wurde. Dieser ging im Sommer 2007 außer Betrieb, da er durch einen neuen Hochleistungsrechner ersetzt wurde.\n\n"}
{"id": "749372", "url": "https://de.wikipedia.org/wiki?curid=749372", "title": "Pixeltakt", "text": "Pixeltakt\n\nDer Pixeltakt (Dot Clock) formula_1 gibt an, wie viele Pixel pro Sekunde an einen Monitor gesendet werden. Dabei werden auch die inaktiven (also nicht sichtbaren) Pixel innerhalb der horizontalen und vertikalen Austastlücke mitgezählt. Er errechnet sich aus dem Produkt der Anzahl der übertragenen Pixel pro Bild formula_2 und der Bildwiederholrate formula_3, bzw. aus dem Produkt der totalen horizontalen Pixelzahl und der Zeilenfrequenz.\n\nBeispiel:\n\nUnter der Annahme, dass die sichtbaren Pixel 80 % der übertragenen Pixel ausmachen gilt:\nund der Pixeltakt formula_8 berechnet sich zu:\n"}
{"id": "750762", "url": "https://de.wikipedia.org/wiki?curid=750762", "title": "Columbia (Supercomputer)", "text": "Columbia (Supercomputer)\n\nColumbia ist der Name eines Supercomputers, der im Jahr 2004 von der amerikanischen Raumfahrtbehörde NASA in Betrieb genommen wurde. Er wird vor allem für Simulationen verwendet, beispielsweise um das Verhalten von Raketen oder des Space Shuttles während des Fluges zu berechnen.\n\nDer SGI-Altix-Computer befindet sich im Ames Research Center in Mountain View, Kalifornien und war zum Zeitpunkt seiner Inbetriebnahme mit 42,7 TFLOPS der schnellste Computer der Welt. In der momentanen Ausbaustufe besteht er aus 27 Maschinen mit je 512 Intel Itanium 2 Prozessoren, insgesamt also 13.824 Prozessoren mit einer Leistung von 66,57 TFLOPS. Als Betriebssystem wird SUSE Linux Enterprise Server verwendet.\n\nIm Juni 2005 belegte er in der TOP500-Liste der 500 schnellsten Computersysteme Platz vier, in der Liste von Juni 2009 Platz 58 und in der vom November 2011 nur mehr Rang 268.\n\nSeinen Namen hat er in Erinnerung und Ehrung des am 1. Februar 2003 verunglückten Space Shuttles Columbia erhalten.\n\n"}
{"id": "750765", "url": "https://de.wikipedia.org/wiki?curid=750765", "title": "FTP Voyager", "text": "FTP Voyager\n\nFTP Voyager ist ein FTP-Client für das Betriebssystem Windows. FTP Voyager wird von Mark P. Peterson entwickelt und im deutschsprachigen Raum von kapper.net übersetzt und verlegt.\n\nSeit 1997 existiert FTP Voyager als einer der ersten graphischen FTP-Clients für Windows. Mark Peterson, der Erfinder der Software, kommt aus der Modemschmiede U.S. Robotics und erkannte früh den Wert graphischer Datentransfertools und entwickelte aus diesem Grund diese Software. Im Laufe der Jahre entwickelte sich ein ursprünglich noch rudimentärer FTP-Client zum Standard für viele FTP-Anwendungen. Nach Herstellerangaben ist FTP Voyager der einzige Client, der jeden FTP-Server-Dialekt versteht, was bei der bestehenden Zahl von mehreren tausend verschiedener Dialekte kein immer einfaches Unterfangen ist.\n\nNeben den Grundfunktionen bietet FTP Voyager heute die vollverschlüsselte Datenübertragung über SSL-FTP (auch FTP-TLS/FTPS) oder SSH File Transfer Protocol, dazu außergewöhnliche Synchronisationsmöglichkeiten zwischen Client und Server und Zeitplanfunktionen zum gesteuerten Ausführen verschiedener Datentransfers.\n\nEine besondere Funktion dieses FTP-Client ist die sogenannte Synchrone-Navigation, eine patentierte Navigation, welche automatisch beim Wechsel eines Verzeichnisses im Server-Fenster auch im Client-Fenster das gleichlautende Verzeichnis anwählt und umgekehrt. Ein Feature, das Webdesigner besonders schätzen.\n\nGemeinsam mit dem Schwesterprodukt Serv-U bietet der FTP Voyager eine einfache Kommunikationsplattform für firmeninterne Dateiübertragungen ebenso wie für den Austausch sensibler Daten zwischen Unternehmen, Mitarbeitern, Filialen und viele weitere Anwendungen.\n\n"}
{"id": "750907", "url": "https://de.wikipedia.org/wiki?curid=750907", "title": "ACDSee", "text": "ACDSee\n\nACDSee bezeichnet Softwareprodukte des Unternehmens ACD Systems und meint meistens den ursprünglichen Foto-Manager ACDSee. Daneben gibt es die „ACDSee-Pro“-Reihe mit abweichender Versionierung; daraus können sich Missverständnisse ergeben, da der Leistungsumfang zum Beispiel des Bildbetrachters ACDSee 4 aus dem Jahr 2002 erheblich vom Foto-Manager ACDSee Pro 4 aus dem Jahr 2011 abweicht. Zuletzt kam noch die „ACDSee-Ultimate“-Edition heraus, durch welche die Pro-Variante mit einem Ebeneneditor ergänzt wurde, hier entspricht die Versionsnummer der Pro-Variante.\n\nAls Bildbetrachter gehört das Programm in den frühen Versionen zu den am weitesten verbreiteten seiner Art und bot bereits eine Vielzahl von Möglichkeiten zur Bildbetrachtung, -organisation und -verwaltung. Die Grundfunktionalitäten wurden seitdem in zwei Hauptbereiche aufgeteilt, den Übersichtmodus (Browser) und den Einzelbildmodus (Viewer). Version 1.0 erschien 1994 und lief bereits unter dem Betriebssystem Windows 3.1 (16-Bit nativ). Ab Version 3.0 ging die Entwicklung weg vom reinen Bildbetrachter in Richtung Bildverwaltung, was sich am Funktionsumfang und an der Dateigröße bemerkbar machte.\n\nIn den neueren Versionen sind als weitere Funktionalitäten zum Beispiel ein Bildeditor, verschiedene Ausgabeprozessoren (HTML, Powerpoint, Drucklayout etc.) sowie Datenbankfunktionen hinzugekommen.\n\nIm September 2007 etablierte sich „ACDSee Pro“ als Professional-Variante 2 einerseits durch wesentliche Erweiterungen der Bildbearbeitung im Bildeditor, Farbmanagement, Stapelverarbeitung, IPTC- und Exif-Bearbeitung usw. besonders jedoch durch den hier implementierten RAW-Konverter.\nAb Version ACDSee Pro 2.5 (September 2008) unterstützt das Programm auch XMP.\n\nBedeutsamste Änderung in der Version Pro 3.0 war die neue Benutzeroberfläche mit den dahinter liegenden Bearbeitungsmodi \"Verwalten–Ansicht–Verarbeiten–Online\". Im Verarbeitungs-Modus können dabei zum Beispiel Bilder im JPEG-Format mit den gleichen Werkzeugen und ebenso nondestruktiv bearbeitet werden, wie solche in einem Rohdatenformat.\n\nIn der Version 4.0 waren die Einführung einer Geotagging-Funktionalität sowie neben der Verbesserung des Rohdaten-Konverters die Möglichkeit, chromatische Aberrationen und Farbsäume zu korrigieren und ein geändertes Management der Metadaten wesentlich.\n\nMit der Version 5.0 wurden einige Verbesserungen des Rohdaten-Konverters (Default-Justagen, Schärfemaske), ein Stapelexportwerkzeug, Organisationsetiketten, ein Zeichenwerkzeug sowie einige zusätzliche Spezialeffekte hinzugefügt. ACDSee unterstützt Farbmanagement, allerdings keine CMYK-Farbprofile.\n\nDie Version 6.0 führt neben zahlreichen Ergänzungen und Verbesserungen der Entwicklungs- sowie Verarbeitungsfunktionen (Entwicklungspinsel, Korrekturpinsel, Rauschunterdrückung, Klarheit, RAW-SW-Modus, Cross Process etc.) ein hierarchisches Modell der Stichwortdatenbank sowie eine eigenständige 64-Bit-Variante ein. Mit der neuen Farbmanagement-Architektur lassen sich Microsoft ICM oder LittleCMS als Farbmanagement-Engine auswählen.\n\nIn der Version 7.0 stehen im Entwicklungsmodus viele weitere Werkzeuge für die Entwicklungspinsel zur Verfügung, zudem werden viele neue nicht-destruktive Korrekturpinsel zur Verfügung gestellt. Außerdem werden Multi-Monitor-Setups nun sehr gut unterstützt.\n\nVersion 8 bietet im Wesentlichen einige Erweiterungen im Bearbeitenmodus. Wenige Wochen nach Erscheinen der \"ACDSee Pro 8\" wurde mit \"ACDSee Ultimate 8\" eine Variante der Pro 8 mit der Erweiterung des Bearbeitenmodus um Ebenenbearbeitung angeboten.\n\nDurch die Implementation der „Lensfun“-Bibliothek stellen ACDSee Pro v9 und ACDSee Ultimate v9 Funktionen zur Entzerrung von Abbildungsfehlern von Objektiven bereit.\n\nSeit 2011 wird ACDSee Pro auch in einer eigenständigen Variante für macOS angeboten.\n\n\"ACDSee Standard\" ist die letzte verbliebene Version, die noch für 32Bit-Windows bereit gestellt wird. \"ACDSee Ultimate\" und \"ACDSee Pro\" gibt es seit 2018 nur noch für 64Bit-Windows.\n\nFür den deutschen Sprachraum sind, bis auf \"ACDSee Pro (Mac)\", alle Produkte für Windows verfügbar.\n\n\"ACDSee Standard\" wendet sich an Heim- und Büroanwender. Es verfügt über Funktionen zum Ordnen von Bildern, zur Bildbearbeitung (mit Bildeffekten) und zum Online-Sharing. Es unterstützt keine Rohdatenentwicklung wie die Pro-Variante und verfügt dieser gegenüber allgemein über eine reduzierte Funktionalität.\n\nEs bietet folgende Hauptmerkmale:\n\n\n\"ACDSee Pro\" stellt als wesentliche Erweiterung eine Umgebung für die Bildbearbeitung und Bildverwaltung mit Rohdatenentwicklung und -bearbeitung zur Verfügung. Die Software bietet dem Anwender folgende Haupt-Benutzerschnittstellen an:\n\n\nIm Jahr 2014 erschien eine Ultimate-Variante, die zusätzlich zu den Funktionen der Pro-Variante Ebenentechnik für den „Bearbeiten“-Modus bereitstellt. Bilder mit Ebenen können ausschließlich im proprietären .acdc-Dateiformat gespeichert werden.\n\nDie Benutzeroberfläche der macOS-Variante, die ab Mac OS X Leopard läuft, ist stark an die Windows-Variante angelehnt. Der Funktionsumfang ist in \"ACDSee Pro 3.7 (Mac)\" (Build 201) kleiner.\n\nDer \"ACDSee Video Converter 4\" erlaubt die Umwandlung von diversen Videoformaten in andere Formate.\n\nDer \"ACDSee Video Converter Pro 4\" bietet zusätzlich zur Funktionalität des \"ACDSee Video Converter 4\" auch Werkzeuge zum digitalen Auslesen („Rippen“) und Brennen von Video-DVDs sowie einfachem Videoschnitt.\n\n"}
{"id": "751726", "url": "https://de.wikipedia.org/wiki?curid=751726", "title": "Lotus Word Pro", "text": "Lotus Word Pro\n\nLotus Word Pro ist eine Textverarbeitungssoftware der Firma Lotus Development Corporation (heute ein Unternehmen von IBM). Word Pro ist der Nachfolger von Ami Pro.\n\nWord Pro wurde im Paket mit der Lotus Smartsuite verkauft, die noch bis 2013 vermarktet wurde. Die letzte Version von Word Pro war 9.8 Fixpack 6 von 2007 und läuft auf allen Windows-Versionen ab Windows 95 bis Windows 7. Frühere Versionen gab es auch für OS/2. \n\nDie mit Word Pro erstellten Dateien erhalten die Endung .lwp (in älteren Versionen und Ami Pro auch .sam). Vorlagendateien (so genannte SmartMaster) haben die Dateiendung .mwp. WordPro aus der SmartSuite 9.8 kann das Urformat SAM aus AmiPro nicht mehr lesen.\n\nDas Dateiformat selbst ist proprietär und kann nicht ohne weiteres von anderen Programmen gelesen werden. Mit Word Pro kann man jedoch viele Fremdformate (z. B. Microsoft Word) lesen, bearbeiten und speichern. Es gibt einen Betrachter für Windows, mit dem die Dateien angezeigt werden können.\n\n\n"}
{"id": "751812", "url": "https://de.wikipedia.org/wiki?curid=751812", "title": "Lotus Approach", "text": "Lotus Approach\n\nLotus Approach ist eine Datenbanksoftware des Unternehmens Lotus Development Corporation (heute ein Unternehmen von IBM).\n\nMit Approach lassen sich sogenannte Ansichtsdateien für Datenbanken erstellen, mit denen der Anwender Zugriff auf die zugrunde liegende Datenbank und deren Daten nehmen kann. Approach kann mit einer Vielzahl von Datenbankformaten umgehen und auch auf viele ODBC-Datenquellen zugreifen.\n\nDie Ansichtsdatei enthält die Definition sämtlicher Datenbankverbindungen, Formulare, Berichte, Diagramme und Tabellen, die der Anwender in der Datenbank benötigt (und die der Entwickler definiert hat).\nDie eigentlichen Daten sind nicht in der Ansichtsdatei gespeichert, sondern in der zugrunde liegenden Datenbank. Approach verwendet standardmäßig Datenbankdateien im dBASE-IV-Format um die Daten zu speichern. Es können aber auch andere Datenbankformate wie z. B. IBM DB2, Oracle, Paradox oder auch MySQL ausgewählt werden. Hierbei gelten für Approach jeweils die Beschränkungen der ausgewählten Datenbank (z. B. bei dBASE IV beträgt die maximale Dateigröße 2 Gigabyte).\n\nDie Ansichtsdateien sind portabel. Das bedeutet, dass eine mit Approach erstellte Ansichtsdatei auch auf einem anderen Rechner mit installiertem Approach nutzbar ist. Approach lässt sich mittels der enthaltenen Script-Sprache Lotus Script (ähnlich Visual Basic) programmieren und in seinem Funktionsumfang erweitern.\n\nAnders als z. B. mit Microsoft Access lassen sich mit Approach keine selbstständig ablaufenden Anwendungen erstellen, da keine entsprechende Laufzeitbibliothek existiert. Das bedeutet, dass Approach beim jeweiligen Anwender lokal (oder auf einem Terminalserver) installiert sein muss.\n\nApproach wurde als Einzelanwendung oder im Paket mit der Lotus Smartsuite verkauft. Die jüngste Version ist 9.8.6 (Stand 2008).\n\nDas Dateiformat der Ansichtsdateien ist proprietär und kann nicht ohne weiteres von anderen Programmen gelesen werden. Die Approach-Ansichtsdateien enden auf:\n\nDie Datenbankdateien sind (je nach gewähltem Format) auch von anderen Datenbankanwendungen nutzbar. Das Standarddateiformat für die Datenbankdateien ist dBASE IV. Die Dateitypen, die hier von Approach benutzt werden, sind:\n"}
{"id": "751933", "url": "https://de.wikipedia.org/wiki?curid=751933", "title": "FlashFXP", "text": "FlashFXP\n\nFlashFXP ist ein FTP-Client für Microsoft Windows von der Firma Inicom Networks. Die Software stellt eine grafische Benutzeroberfläche für das \"File Transfer Protocol\" (FTP) bereit. Sowohl gängige Client-zu-Server-Übertragungen als auch Server-zu-Server-Übertragungen (FXP) werden unterstützt.\n\nLaut Herstellerangaben ist es das verbreitetste Programm seiner Art. Es beherrschte ursprünglich als erstes Programm das File Exchange Protocol.\n\nFolgende Funktionen werden unterstützt:\n\nIm April 1998 unterhielt sich der Gründer Charles DeWeese mit Freunden mittels Internet Relay Chat, welche ihm vorschlugen, einen eigenen FXP-Client zu programmieren, da damalige Clients ihrer Meinung nach nicht wirklich effektiv arbeiteten.\n\nNachdem er sich einige Wochen weigerte, ließ er sich dann doch überreden, und startete die Arbeit an \"MyFXP\" mit Visual Basic 4. Er merkte jedoch bald, dass er mit Visual Basic schnell an die Grenzen des Machbaren stoßen würde. Einer seiner Freunde, ironischerweise der Erfinder von LeapFTP, schlug ihm deshalb Borland Delphi 4 vor, womit er dann seine Arbeit fortführte. Er benannte MyFXP noch im Juni des Jahres in FlashFXP um, und konnte Version 1.0 bereits am 23. Juli 1998 bereitstellen.\n\nVersion 1.2 war die erste Shareware-Version von FlashFXP.\n\nVersion 2.0 war mit der Implementierung von SSL/TLS-Unterstützung ein Meilenstein für den Entwickler.\n\nCirca 2003 erlangte FlashFXP eine derart große Bekanntheit, dass es DeWeese nicht mehr möglich war, die Weiterentwicklung, den Kundendienst und die Verkäufe alleine abzuwickeln, wodurch er einen Exklusivvertrag mit iniCom Networks, Inc. abschloss, und Version 3.0 veröffentlichte.\n\nGegen Ende 2009 wurde FlashFXP Teil der neu geformten Opensight Software, LLC., welche in Besitz von DeWeese ist. Verkäufe werden mittlerweile vollautomatisch mittels Share-it, PayPal und der FlashFXP-Webseite durchgeführt. Kundendienst wird in direktem Kontakt mit den Entwicklern und Freiwilligen im Forum geleistet.\n\nVersion 4.0 bietet einige Fehlerkorrekturen, neue Funktionen und eine überarbeitete Oberfläche.\n\n"}
{"id": "753191", "url": "https://de.wikipedia.org/wiki?curid=753191", "title": "Pfeiltaste", "text": "Pfeiltaste\n\nAls Pfeiltasten, auch Cursortasten, werden die vier auf Computertastaturen meist in einem eigenen Block angeordneten Tasten mit Pfeilaufdruck bezeichnet. Sie befinden sich auf nahezu allen tastenorientierten Eingabegeräten. Meistens sind genau vier Pfeile für nach oben (aufwärts), unten (abwärts), links und rechts wie auf einem Kompass oder wie ein Siegertreppchen angeordnet. Selten stehen weniger oder mehr Pfeile zur Verfügung.\n\nDie vier Standard-Pfeiltasten befinden sich normalerweise zwischen dem Haupttasten- und Ziffernblock in Form eines auf dem Kopf stehenden Ts. Dies sind:\n\nMeist sind als Zweitbelegung auch Pfeiltasten im rechts benachbarten Ziffernblock auf den Tasten (links), (aufwärts), (rechts) und (abwärts) vorhanden, die nach Abschalten der \"Num\"-Funktion mittels Num-Taste benutzt werden können. Manchmal gibt es zusätzlich im Pfeiltastenblock auch noch Diagonalpfeilasten.\n\nDie erste Tastatur, die Pfeiltasten im heutigen Layout hatte, war \"LK201\" (siehe Weblinks) von DEC, die ab 1982 produziert wurde.\n\nPfeiltasten dienen dazu, den Cursor zu bewegen. Sie finden bei jeder Art von Texteingabe Verwendung. Außerdem sind sie hilfreich, in menügestützten Anzeigen schnell den gewünschten Menüpunkt auswählen zu können. In Videospielen wird häufig eine Spielfigur mittels Pfeiltasten durch die Welt bewegt. Es steht dazu oft ein speziell für solche Spiele ausgelegter Controller zur Verfügung, bzw. ein Joystick, der sich auf die Pfeiltasten der Tastatur aufstecken lässt. Die Funktion der Steuerung der Figur erledigt inzwischen zum größten Teil das WASD Steuerkreuz. Bei der Bewältigung von kreativen Aufgaben wie Webdesign, 3D-Modellierung oder Bildbearbeitung sind Pfeiltasten nahezu unerlässlich, um Objekte zu verschieben, skalieren oder rotieren. In C++- oder C-Programmen können die Pfeiltasten mittels getch() mit = 75, = 72, = 77 und = 80 abgefragt werden.\n"}
{"id": "756348", "url": "https://de.wikipedia.org/wiki?curid=756348", "title": "PowerBook 5300", "text": "PowerBook 5300\n\nDie PowerBook 5300 Serie war die erste Generation tragbarer Rechner der Firma Apple mit einem PowerPC-Prozessor und erschien im August 1995. Trotz ihrer attraktiven Eigenschaften hat sie sich durch eine Pannenserie der ersten Modelle den Ruf als eines der schlechtesten Apple-Produkte aller Zeiten erworben.\n\nEs gab vier Modelle des 5300, vom günstigen Graustufenmodell 5300 bis zum hochauflösenden TFT-Modell 5300ce:\nAls Gehäuse diente das kompakte und werkzeuglos erweiterbare Gehäuse des PowerBook 190, allen gemeinsam war die hohe Erweiterbarkeit mit bis zu 64 MB RAM, zwei PCMCIA-Slots, SCSI-, ADB- und Infrarotschnittstellen, einem Modulschacht (es gab u. a. Module mit Floppy, ZIP-Laufwerk, MO-Laufwerk oder integriertem Netzteil) und einem internen Erweiterungsschacht, für den es etwa ein Modul mit Ethernet und einer Videokarte für ein externes Display im Zwei-Monitor-Betrieb gab.\n\nDie technische Fortschrittlichkeit (andere Notebooks hatten damals Pentium-Prozessoren mit maximal 75 MHz oder gar 486-CPUs; Wechselmodule, hohe RAM-Ausstattungen und TFT-Auflösungen über 640 × 480 waren kaum bezahlbare Seltenheiten) dieser PowerBook-Serie wurde aber von den Pannen der ersten Geräte überschattet. Die ursprünglich vorgesehenen, von Sony gelieferten Lithium-Ionen-Akkus, gerieten zum Teil in Brand und mussten durch NiMH-Akkus ersetzt werden; viele Geräte gingen bereits auf dem Transport kaputt (\"Dead On Arrival\"), und es gab Probleme mit diversen Komponenten, die Apple durch ein weit über die Garantiezeit verlängertes Reparaturprogramm beheben musste.\n\nObwohl die Nachbesserungen alle Probleme der 5300-Serie beheben konnten und viele Geräte dank ihrer Erweiterbarkeit oft noch bis weit über das Jahr 2000 im Einsatz blieben, hat sich der Ruf der Serie nie mehr erholt. Bereits im Oktober 1996 führte Apple mit der 1400-Serie Nachfolger ein, obwohl diese in fast keiner Hinsicht über die von der 5300-Serie gesetzten Maßstäbe hinausgingen (am bemerkenswertesten ist wohl das CD-ROM-Laufwerk). Die Serie 3400 vom Februar 1997 nahm die Kompatibilität mit den 5300-Modulen wieder auf, hatte außer DMA-Fähigkeit und leicht schnelleren CPUs aber immer noch keine wesentlichen Neuerungen zu bieten.\n\nDamit kann das PowerBook 5300 als Beispiel dafür dienen, wie Pannen bei der Einführung den Ruf eines Produktes (und teilweise der gesamten Firma) bis weit über ihre Beseitigung hinaus beeinträchtigen können.\n"}
{"id": "758465", "url": "https://de.wikipedia.org/wiki?curid=758465", "title": "ResEdit", "text": "ResEdit\n\nResEdit (\"Resource Editor\") ist ein Computerprogramm der Firma Apple, um die sogenannten \"Resource forks\" von Computerdateien zu bearbeiten. ResEdit gehört zu den Standardwerkzeugen von Programmentwicklern unter Mac OS (bis 9), dem klassischen Betriebssystem des Macintosh.\n\nIm HFS Dateisystem des Macintosh bestehen die Computerdateien aus einer sogenannten \"data fork\" und einer \"resource fork\". Die resource fork dient dazu, Töne, Bilder, Texte und andere Elemente aufzunehmen. Das hat den Vorteil, dass beispielsweise bei einer Lokalisierung eines Computerprogrammes zur Übersetzung nicht der eigentliche Programmcode, sondern nur die resource fork verändert werden muss.\n"}
{"id": "759034", "url": "https://de.wikipedia.org/wiki?curid=759034", "title": "Symbolische Mathematik", "text": "Symbolische Mathematik\n\nIn Computeralgebrasystemen (CAS) bedeutet der Ausdruck symbolische Mathematik, dass Operation und Kalkulation von mathematischen Ausdrücken mit Variablen auf Computern ausgeführt werden. Umgangssprachlich wird dies als „Rechnen mit Buchstaben“ bezeichnet.\n\nViele CAS und einige programmierbare Taschenrechner (z. B. der HP-48) sind in der Lage, symbolische Formelmanipulationen durchzuführen. Damit sind Umformungen und Berechnungen von Termen und Gleichungen gemeint wie z. B. faktorisieren, ausmultiplizieren, die Polynomdivision sowie das Lösen von Gleichungen. Im Gegensatz zur numerischen Mathematik wird in der symbolischen Mathematik ausschließlich mit exakten Ausdrücken gearbeitet. Vorteil der symbolischen Mathematik ist die universelle Einsetzbarkeit des Ergebnisses. Ein Nachteil ist, dass bei vielen Problemen eine symbolische Lösung sehr aufwändig zu berechnen oder gar nicht möglich ist.\n\nEin Beispiel des CAS Maple zeigt das Lösen eines symbolischen Ausdrucks der sog. Mitternachtsformel: \n\n\n"}
{"id": "760138", "url": "https://de.wikipedia.org/wiki?curid=760138", "title": "SubSeven", "text": "SubSeven\n\nSubSeven (\"auch:\" Sub7) ist ein Fernwartungsprogramm für Microsoft Windows, das illegal eingesetzt werden kann und welches die Möglichkeit bereitstellt, die Programmdatei mit einer anderen Datei zu verbinden, so dass sich der gefährliche Code hinter harmlos scheinendem Code versteckt (sogenannte Trojanische Pferde). SubSeven lässt sich auf dem installierten Rechner während der Laufzeit aktualisieren. Der Verfasser dieses Programmes agiert unter dem Pseudonym \"mobman\" und arbeitet inzwischen für Unternehmen in der IT-Branche. SubSeven kann unter anderem auch Webcams steuern, um das Opfer zu beobachten oder den Computer des Opfers einfach herunterfahren.\n\nSubSeven wird seit April 2005 nicht weiterentwickelt. Die letzten veröffentlichten Versionen waren nicht von mobman erstellte Neuerungen, sondern meist nur von der Sub7crew (Gruppe von sympathisierenden Anwendern) für aktuelle Betriebssysteme angepasste Varianten. Im Januar 2014 wurde sub7 immer noch von dieser Gruppe weiterentwickelt, die nach eigenen Angaben die Software regelmäßig pflegt.\n\nWeniger bekannt als Back Orifice, waren zeitweise weitaus höhere Anzahlen von Systemen mit SubSeven infiziert. Scans bestimmter AOL-IP-Bereiche ergaben Infektionsraten von etwa 10 %.\n\n\n"}
{"id": "761732", "url": "https://de.wikipedia.org/wiki?curid=761732", "title": "Hydra (Schachcomputer)", "text": "Hydra (Schachcomputer)\n\nHydra ist ein Schachcomputer, der von dem Österreicher Christian „Chrilly“ Donninger, den Deutschen Ulf Lorenz und Christopher Lutz sowie der Firma PAL Computer Systems aus Abu Dhabi entwickelt wurde.\n\nEin Vorläufer des Programms hieß \"Brutus\" und wurde von der Firma ChessBase in Auftrag gegeben. Nach dem enttäuschenden Abschneiden einer frühen Version bei der Computerschach-Weltmeisterschaft 2003 in Graz verfolgte ChessBase das Projekt nicht weiter, weil keine kommerzielle Verwertbarkeit gesehen wurde. Dem Hydra-Team gelang es daraufhin, mit der Firma \"PAL Computer Systems\" aus den Vereinigten Arabischen Emiraten einen neuen Geldgeber zu finden.\n\nDas teilweise neue, teilweise weiterentwickelte Programm Hydra schlug 2004 den mehrfachen Computerschachweltmeister Shredder in einem Wettkampf in Abu Dhabi mit 5,5:2,5. Im selben Jahr erzielte das Programm bei einem Vergleichskampf gegen Spieler der Weltspitze in Bilbao 3,5 Punkte aus vier Partien. Im Juni 2005 gelang Hydra in London mit 5,5-0,5 ein überraschend hoher Wettkampfsieg über den englischen Weltklassespieler Michael Adams. Hydra wurde bislang noch nie von einem menschlichen Schachspieler in einer Turnierpartie geschlagen (Stand August 2006). Lediglich im Fernschach verlor das Programm 2005 ein Match gegen den Berliner Fernschach-Großmeister Arno Nickel mit 0,5:2,5. Ebenfalls zum Einsatz kam Hydra im Freistil-Schach, einer Online-Schachart, bei der Analyseunterstützung durch Computer erlaubt ist. Sein Besitzer aus Abu Dhabi, der unter dem Pseudonym \"Zorchamp\" auftritt, spielte mit Hydra bei drei Turnieren in den Jahren 2005 und 2006 und erzielte dabei unterschiedliche Erfolge. Zuletzt verfehlte er im Juni 2006 die Endrundenqualifikation, nachdem er im April 2006 das 2. \"PAL/CSS Freestyle Tournament\" gewonnen hatte. Die Entwickler gaben bereits 2005 eine Spielstärke von über 3000 Elo-Punkten an.\n\nHydra besteht aus einem unter Linux laufenden Computercluster von derzeit 32 Intel Xeon-Prozessoren, die den Suchbaum generieren, und 32 FPGA-Karten, welche die Bewertung der Stellungen vornehmen und an das Programm zurückmelden. \nDer empfindlichste Teil von Hydras Suchalgorithmus läuft verteilt auf den erwähnten Xeon-Prozessoren, welche über ein Myrinet Hochgeschwindigkeitsnetzwerk miteinander verbunden sind. Jeder Prozessor bedient seine eigene FPGA-Karte, und der Ablauf des verteilten Algorithmus ist im Grunde wie folgt: Einer der Prozessoren bekommt die aktuelle Schachstellung und beginnt eine sequentielle Alpha-Beta-Suche. Die anderen Prozessoren senden Arbeitsanfragen zufällig im Netz herum, und wenn ein Prozessor, der bereits sinnvoll am aktuellen Schachproblem mitarbeitet, solch eine Anfrage einfängt, gibt er ein Teilproblem seines eigenen (Teil-)Problems ab. Mit Hilfe eines trickreichen Nachrichtensystems zwischen den Prozessoren wird die zu verrichtende Arbeit dynamisch ausgeglichen und es entsteht nur geringer Suchoverhead. Nach einer Weile haben dann alle Prozessoren etwas Sinnvolles zu tun, und jeder einzelne Prozessor bewertet Schachstellungen in seinem Suchbaum mit Hilfe seines FPGA-„Koprozessors“. Ein Hydra Prozessor erzeugt ca. 100.000 kleine Suchen auf einer FPGA-Karte pro Sekunde.\n\nDer Vorteil der FPGA-Karte selbst ist, dass eine aufwendige wissensbasierte Bewertungsfunktion implementiert werden kann, ohne die Suchgeschwindigkeit nennenswert zu beeinträchtigen. Derzeit kann das Programm etwa 200 Millionen Stellungen pro Sekunde durchsuchen und bewerten. Damit ist es etwa 100 mal schneller als Schachsoftware auf einem Standard-PC. Das folgende Bild zeigt eine schematische Darstellung der Hydrahardware.\n\nFür das Eröffnungsbuch von Hydra ist der deutsche Großmeister Christopher Lutz zuständig. Im Gegensatz zu anderen Programmen, deren Eröffnungsrepertoire auf von Schachmeistern gespielten Partien beruht, ist es nicht sehr umfangreich, da das Programm oft bessere Züge selbstständig errechnen kann.\n\n"}
{"id": "762106", "url": "https://de.wikipedia.org/wiki?curid=762106", "title": "Robotron Z 9001, Robotron KC 85/1, Robotron KC 87", "text": "Robotron Z 9001, Robotron KC 85/1, Robotron KC 87\n\nDer Robotron Z 9001 ist ein auf dem U880-Mikroprozessor basierender Heimcomputer des VEB Robotron aus der Deutschen Demokratischen Republik.\n\nDer pultförmige Computer wurde ab 1983 zunächst zur Versorgung von Privathaushalten, aber auch zum Einsatz in Bildungseinrichtungen entwickelt. Die staatlichen Vorgaben sahen dabei geringstmögliche Herstellungskosten unter Verwendung von ausschließlich in den RGW-Staaten produzierten Bauteilen vor. Die ersten Geräte kamen Ende 1984 in den Handel, ergänzt um separat zu erwerbende Erweiterungsbaugruppen wie Arbeitsspeicherzusätze und verschiedene zunächst nur von Kompaktkassette ladbare Programmiersprachen.\n\nZwischenzeitlich geänderte Planungsvorgaben verlagerten ab 1985 den Einsatzschwerpunkt des nun „Kleincomputer“ genannten Geräts hin zu Bildungseinrichtungen und zur Produktion. Dem wurde neben geringfügigen technischen Überarbeitungen auch durch die Umbenennung in Robotron KC 85/1 Rechnung getragen.\n\nZusätzliche Verbesserungen für eine effizientere Produktion und der Einbau der Programmiersprache BASIC führten schließlich 1987 zu einer weiteren, abwärtskompatiblen Gerätegeneration. Die wiederum an die Jahreszahl angelehnte neue Bezeichnung Robotron KC 87 diente zudem zur Kenntlichmachung der Inkompatibilität zu den konkurrierenden Computermodellen KC 85/2, KC 85/3 und KC 85/4 des VEB Mikroelektronik aus Mühlhausen. Mit beginnender Produktion des offiziellen Nachfolgemodells, des Bildungscomputers A 5105, stellte man im Frühjahr 1989 die Fertigung des KC 87 ein.\n\nVon Z 9001, KC 85/1 und KC 87 wurden zusammen insgesamt etwa 30.000 Geräte ausgeliefert.\n\nIn der Zeit des Kalten Krieges waren den RGW-Staaten der Zugang und die Einfuhr von Hochtechnologie, wozu zunächst Rechentechnik im Allgemeinen und später die Mikroelektronik im Speziellen zählte, durch das CoCom-Embargo weitestgehend verwehrt. Den vorhandenen Bedarf deckte man kurzerhand durch Nachentwicklung illegal beschaffter Technik mittels Reverse Engineering. Dank dieser und weiterer Anstrengungen verfügte die DDR ab den späten 1960er Jahren über eigenentwickelte elektronische Großrechentechnik in Form des Robotron 300. Der erste in der DDR gefertigte Mikroprozessor, ein Nachbau des zu diesem Zeitpunkt bereits fünf Jahre alten Intel 8008, kam mit dem U808 im Herbst 1977 hinzu.\n\nZur Befriedigung der ab Anfang der 1980er Jahre auch im Bildungs- und Privatsektor aufgekommenen Computernachfrage beschloss die Staatsführung eine verstärkte Ausweitung von Entwicklungs- und Produktionsaktivitäten auf den Konsumgüterbereich. Im Gegensatz zu bereits existierender Unterhaltungselektronik wie dem seit 1980 produzierten Bildschirmspiel-Gerät BSS 01 und dem seit 1982 erhältlichen Schachcomputer SC 2 sollten die Neuentwicklungen vor allem die Möglichkeit zur Programmierung durch den Benutzer bieten.\n\nMitarbeiter etablierter Elektronikhersteller wie dem Dresdner VEB Robotron und dem Mühlhäuser VEB Mikroelektronik arbeiteten ohnehin bereits an Machbarkeitsstudien für Heimcomputer. Daher griffen sie die staatlichen Direktiven zur Umsetzung eines solchen Prestigeobjektes nur allzu bereitwillig auf. So wurde das industriell geprägte „Erzeugnisprogramm Dezentrale Datentechnik“ des VEB Robotron ab Ende 1982 innerhalb kürzester Zeit um entsprechende Kapazitäten erweitert. Das „Realisierungskonzept Heimcomputer auf Basis U 880“ startete nur wenig später im Januar 1983 im hauseigenen Zentrum für Forschung und Technik in Dresden. Zur selben Zeit nahmen die Verantwortlichen des VEB Mikroelektronik in Mühlhausen ebenfalls die Gelegenheit wahr und riefen ein ähnliches, jedoch von den Dresdener Bemühungen unabhängiges Jugendprojekt „Videocomputer“ ins Leben. Dieses unkoordinierte Vorgehen beider Betriebe sorgte durch die damit hervorgerufene, in der DDR-Planwirtschaft nicht gern gesehene Konkurrenzsituation für Verstimmungen bis hinauf in höchste politische Kreise. Schlussendlich blieb es jedoch bei beiden Vorhaben, wohl in der Annahme, dass ein Hersteller allein den riesigen Bedarf an Heimcomputern in der DDR nicht würde decken können. Der von beiden Einrichtungen jeweils zu entwickelnde Heimcomputer sollte, den Anweisungen des Ministeriums für Elektrotechnik und Elektronik folgend, erschwinglich und robust mit kompakten Abmessungen sein.\n\nDie staatlichen Planungsvorgaben für die zumeist jungen Ingenieure und Mitarbeiter der entsprechenden Entwicklergruppe („Jugendforscherkollektiv“) vom Zentrum für Forschung und Technik des VEB Robotron in Dresden sahen ein erweiterungsfähiges Kompaktgerät mit integrierter Tastatur und möglichst geringen Material- und Herstellungskosten vor. Die üblicherweise in den DDR-Privathaushalten vorhandene Heimelektronik wie Fernseher und Kassettenrekorder musste durch den Rechner verwendet werden können.\n\nUm die Produktionsabläufe möglichst effizient zu halten, sollten bei der Fertigung einfachste Bauteile und -gruppen Verwendung finden. Aufgrund des CoCom-Embargos musste bei der Konstruktion auf integrierte Schaltkreise ausschließlich aus DDR- bzw. RGW-Produktion zurückgegriffen werden. Die engen Vorgaben bezüglich der geringen Herstellungskosten bei gleichzeitig geforderter Robustheit im Alltagseinsatz waren dabei nur durch eine Systemarchitektur realisierbar, die auf dem preisgünstigen und einsatzerprobten 8-Bit-Mikroprozessor U880 nebst standardisierten elektronischen Beschaltungsbausteinen basierte. Hochaufgelöste Rastergrafik („Vollgrafik“) und Anschlüsse für spezielle Peripheriegeräte fielen dem Kostendruck und fehlendem Platz im materialsparenden und daher klein zu dimensionierenden Gehäuse zum Opfer. Die Konzeption des Computers als modulares System mit Erweiterungsschächten („Bus-Expansionsinterface“) sah jedoch eine einfache Nachrüstbarkeit vor. Die im Zentrum für Forschung und Technik erarbeiteten Vorschläge wurden ab Mitte 1983 anhand von Prototypen im Dresdner Werk VEB Robotron-Meßelektronik, dem späteren Hersteller, überprüft.\n\nDie ersten drei funktionell gleichen Prototypen mit der am Namen einer Mitarbeiterin angelehnten internen Bezeichnung \"SHAFY\" wurden am 1. Juli \"(Modell 01/83)\", am 9. September \"(Modell 02/83)\" und am 16. September 1983 \"(Modell 03/83)\" fertiggestellt. Diese teilweise noch erhaltenen Muster basieren auf handverdrahteten Lochrasterplatinen mit 2 Kilobyte (KB) Arbeitsspeicher, 4 KB Festwertspeicher nebst CP/M-orientierter Systemsoftware, Zeichengenerator und einer vollwertigen Schreibmaschinentastatur. Die Modelle 01/83 und 03/83 wurden zur Entwicklung von Software und für ausführliche Tests genutzt, um zukünftige kostenintensive Reklamationen und Reparaturen zu vermeiden. Das Modell 02/83 diente überwiegend Demonstrationszwecken.\n\nNachdem sämtliche Tests erfolgreich verlaufen waren und die Produktion wirtschaftlich effizient umsetzbar schien, wurde ab Ende 1983 die Serienproduktion geplant. Das „Entwicklung, Überleitung und Produktion des Heimcomputers Z 9001“ genannte Projekt wurde auch von politischer Seite unterstützt („Initiativthema“ der SED) und durch Bereitstellung zusätzlicher Mitarbeiter aus der FDJ gefördert. Bürokratische Hindernisse wurden abgebaut, sodass der Prozess nach elf Monaten abgeschlossen werden konnte. Eine der wenigen der Kostenoptimierung für die Serienproduktion zum Opfer gefallenen Prototypenkomponenten war die Schreibmaschinentastatur. Sie wurde durch eine preisgünstige Elastomer-Matte ersetzt, wie sie in kleineren Abmessungen auch bei Taschenrechnern zum Einsatz kam. Daneben wurde die Kapazität des ab Werk zu verbauenden Arbeitsspeichers unter anderem durch zwischenzeitlich aufgekommene preisgünstigere Speicherbausteine von 2 auf 16 KB erhöht.\n\nWährend der gesamten Entwicklungszeit vereinbarten und implementierten die Ingenieure der Computerprojekte in Dresden wie auch in Mühlhausen gemeinsame Standards zum einfachen Softwareaustausch zwischen ihren beiden Heimcomputersystemen. Dies betraf in erster Linie die Benutzung ein und desselben BASIC-Interpreters sowie ein standardisiertes Aufzeichnungsverfahren für die anzuschließenden Kassettenrekorder.\n\nDer serienreife, fortan Z 9001 genannte Computer wurde – wie das inzwischen fertiggestellte Konkurrenzprodukt HC 900 aus Mühlhausen auch – 1984 unter großem Aufsehen auf der internationalen Leipziger Frühjahrsmesse der Weltöffentlichkeit vorgestellt.\n\nDa Privathaushalte und Bildungseinrichtungen Mitte der 1980er Jahre in der DDR nicht flächendeckend mit Farbfernsehgeräten ausgestattet waren, erschien der Z 9001 in zwei Varianten. Die preiswerteren Grundmodelle mit der Bezeichnung Z 9001.10 verfügten lediglich über eine Schwarzweiß-Ausgabe, die der Reihe Z 9001.11 über Farbausgabe. Die Schwarzweißgeräte konnten durch einen später erhältlichen Aufrüstsatz durch Fachwerkstätten auf Farbausgabe umgestellt werden.\n\nDie Herstellung der Computer erfolgte in mehreren Produktionsbereichen. Die von der Robotron-Niederlassung in Riesa gelieferten, vollständig bestückten Leiterplatten wurden in Radebeul (Werk I) und Pockau (Werk II) mit den dort gefertigten Gehäusen und Tastaturen zum Endprodukt zusammengesetzt, geprüft und ausgeliefert. Die dem Computer auf Kompaktkassette beigelegte wie auch die zusätzliche separat bestellbare Software wurde durch den VEB Deutsche Schallplatten (Amiga) bereitgestellt.\n\nDie erste Serie ging anlässlich des 35. Jahrestages der Gründung der DDR im September 1984 in Produktion. Davon gelangten lediglich etwa 50 Exemplare in den freien Handel. Die restlichen der ersten 100 bis Dezember 1984 produzierten Geräte wurden an das Schülerrechenzentrum in Dresden und an die Heinrich-Hertz-Spezialschule in Berlin geliefert. Ab 1985 wurden die Geräte mit geringfügig überarbeiteter Leiterplatte \"(Serie 85)\" ausgeliefert und die Produktionszahlen erhöht. Das Sortiment der ab Computermarkteinführung erhältlichen Speichererweiterungsmodule wurde um Druckerschnittstellen und eine verbesserte, externe Grafikbaugruppe ergänzt.\n\nStaatliche Entscheidungen verlagerten 1985 den Einsatzschwerpunkt der für den Privatgebrauch entwickelten Computer verstärkt auf den Bereich der Bildung und Wirtschaft. Damit einhergehend erfolgte die Umbenennung des Heimcomputers Z 9001 in \"Kleincomputer Robotron KC 85/1\" (kurz \"KC 85/1\"), das Mühlhäuser Konkurrenzprodukt HC 900 erhielt den neuen Namen \"KC 85/2\". Die technisch weitestgehend unveränderten KC-85/1-Geräte wurden erstmals auf der Leipziger Frühjahrsmesse vorgestellt und ab März 1985 in größeren Stückzahlen produziert und an Bildungseinrichtungen geliefert. Eine Kompatibilität zum ähnlich benannten, aber wesentlich teureren Gerät KC 85/2 des Herstellers VEB Mikroelektronik Mühlhausen besteht trotz des gemeinsamen BASIC-Dialekts und Datenspeicherformates nicht.\n\nBeim Z 9001 und KC 85/1 war die Programmiersprache BASIC nicht im Festwertspeicher des Computers enthalten, sondern musste von Kassette in den Arbeitsspeicher geladen werden. Dadurch standen in der Grundversion ohne Speichererweiterung nach dem Laden lediglich etwa 5 KB Arbeitsspeicher zur freien Verfügung, was die Einsatzmöglichkeiten der Rechner erheblich einschränkte. Die nötige Weiterentwicklung zur gemeinsamen Integration von Betriebssystem und BASIC im Festwertspeicher sowie die damit verbundene Überarbeitung der Leiterplatte begannen im September 1985 mit dem Projekt „Z 9002“.\n\nNach diversen ab 1985 im Rahmen des Projektes „Z 9002“ eingeleiteten technischen Verbesserungen erhielt das bis 1987 aktualisierte Gerät ob des großen Änderungsumfangs die Neubezeichnung \"Kleincomputer Robotron KC 87\". Die im Gerätenamen verwendete Zahl deutet dabei auf den angedachten Produktionsbeginn 1987 hin. Erste Muster waren der Öffentlichkeit bereits 1986 auf Messen in Dresden und Leipzig zugänglich. Bis zum Abschluss der Entwicklungsarbeiten im März 1987 wurden hauptsächlich Vorserienmodelle (KC 87.10 und KC 87.11) hergestellt und in erster Linie zu Entwicklungs-, Test- und Demonstrationszwecken eingesetzt.\n\nDie reguläre Serienproduktion begann im April 1987. Die Varianten KC 87.20 und KC 87.21 verfügen über Konfigurationsmöglichkeiten des eingebauten BASIC zur Ansteuerung der separat erhältlichen Vollgrafikbaugruppe oder entsprechender Plotter. Sämtliche im Jahr 1987 produzierte Geräte waren ausschließlich für Bildungseinrichtungen und Betriebe gedacht. Ab 1988 gelangte von den jährlich 8.000 hergestellten Computern erstmals ein Teil in den regulären Einzelhandel zur Versorgung der Bevölkerung. Die Auslieferung erfolgte in RFT-Fachfilialen und Centrum-Warenhäuser über zuvor hinterlegte „Kundenbedarfslisten“. Zur besseren Unterscheidung wurden die für den Vertrieb an „gesellschaftliche Bedarfsträger“ wie Bildungseinrichtungen und Betriebe gedachten Geräte fortan mit der Modellbezeichnung KC 87.30 bzw. KC 87.31 versehen. Die Produktion des KC 87 lief im März 1989 planmäßig zugunsten des \"Bildungscomputers Robotron A 5105\" (abgekürzt \"BIC A 5105\") aus. Von Z 9001, KC 85/1 und KC 87 wurden zusammen insgesamt etwa 30.000 Geräte ausgeliefert.\n\nDie Grundgeräte enthalten jeweils die elektronischen Baugruppen Rechnereinheit mit Hauptprozessor (englisch \"Central Processing Unit\" kurz \"CPU\"), Speichereinheit mit Arbeits- und Festwertspeicher, Tastatur, Bildschirmansteuerung, Peripherieanschlüsse und Stromversorgung. Die Rechner verfügen über vier Modulsteckplätze (herausgeführter Parallelbus), wobei der Stromverbrauch von eingesteckten Modulen bei der Dimensionierung des Computernetzteiles berücksichtigt wurde. Zum Lieferumfang gehörten neben dem Grundgerät eine Programmkassette, ein Netzkabel, eine Netzsicherung, ein Antennenkabel bzw. RGB-Kabel zum Anschluss eines Fernsehgeräts und die aus Bedienungsanleitung, Programmierhandbuch sowie einem Anhang zum Programmierhandbuch bestehende Dokumentation.\n\nDie Systemarchitektur basiert auf einem mit 2,5 MHz getakteten U880-Mikroprozessor, der in fast allen zeitgenössischen DDR-Computern eingesetzt wurde. Dieser nicht autorisierte Nachbau des Z80-Mikroprozessors von Zilog kann auf einen Adressraum von 65.536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobyte (KB) festlegt. Aus praktischen Gründen ist es üblich, für Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Dieser wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65.535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer vom Hauptprozessor benutzbare Adressraum unterteilt sich bei allen Geräten in Bereiche für das Betriebssystem, Arbeitsspeicher, Festwertspeicher, Erweiterungen und den Grafikspeicher.\n\nDas 4 KB umfassende, an CP/M-80 orientierte Betriebssystem \"Z9001-OS\" befindet sich im obersten Speicherbereich von $F000 bis $FFFF. Es ist beim Z 9001 in zwei EPROM-Bausteinen untergebracht, beim KC 87 dagegen in einem ROM-Baustein. Zum Vorhalten von Systemvariablen nutzt das Betriebssystem den untersten von $0000 bis $021F reichenden Bereich des ab Werk 16 KB umfassenden Arbeitsspeichers.\n\nFür den Anwender stehen etwa 15 KB RAM von $0220 bis $3FFF zur freien Verfügung. Bei Verwendung der maximal möglichen zwei RAM-Erweiterungsmodule à 16 KB im Speichersegment von $4000 bis $BFFF erhöht sich die Kapazität des nutzbaren Arbeitsspeichers auf 47 KB. Beim Z 9001 und KC 85/1 muss zur Programmierung mit BASIC die etwa 10 KB umfassende Programmiersprache von Kassette in den RAM-Speicher von $0300 bis $3FFF geladen werden. Ohne RAM-Erweiterung stehen damit lediglich 5 KB RAM für eigene Programme zur Verfügung. Wird das BASIC dagegen durch ein Steckmodul bereitgestellt, bleibt es bei etwa 15 KB nutzbarem Arbeitsspeicher.\n\nBeim KC 87 ist der BASIC-Interpreter bereits ab Werk im Festwertspeicher (ROM) des Computers und bei Z 9001 sowie KC 85/1 bei gestecktem ROM-Modul jeweils unter den Adressen $C000 bis $E7FF zu finden. Der zur Textdarstellung benötigte Bildspeicher reicht bei Z 9001 und KC 85/1 von $EC00 bis $EFFF, im Falle von Farbausgabe ergänzt um entsprechenden Farbspeicher im Bereich von $E800 bis $EBFF.\n\nDie Computer verfügen in der Grundausstattung lediglich über einen Zeichengenerator mit einem Textmodus von wahlweise 40 × 20 oder 40 × 24 Zeichen à 8 × 8 Bildpunkte. Der nicht änderbare Zeichensatz stellt 128 alphanumerische und Steuerzeichen sowie 128 Grafiksymbole für sogenannte Quasigrafiken bereit. Die Verwendung der Grafiksymbole erlaubt nach Angaben des Herstellers eine für viele Anwendungen ausreichende Darstellung. Ein hochauflösender Rastergrafikmodus („Vollgrafik“) steht nicht zur Verfügung, kann jedoch extern nachgerüstet werden. Die Schwarzweiß-Bildausgabe der Grundversionen KC 87.10, KC 87.20 und KC 87.30 erfolgt über den koaxialen HF-Antennenanschluss an einem handelsüblichen Fernsehgerät. Die Varianten KC 87.11, KC 87.21 und KC 87.31 mit einer ab Werk verbauten „Farbkarte“ ermöglichen über einen RGB-Anschluss die Darstellung von je acht Vorder- und Hintergrundfarben.\n\nDie beiden in den Rechnern enthaltenen Ein- und Ausgabeschaltkreise mit der Modellnummer U855 (englisch \"Parallel Input Output\" kurz \"PIO\") ermöglichen den Betrieb der Tastatur und die Benutzung von Joysticks („Spielhebel“), die Ansteuerung des Kassettenrekorders und eine programmierbare Tonerzeugung (einstimmig, mono). Die Tonausgabe erfolgt entweder über den im Computer eingebauten Lautsprecher oder einen externen Verstärker.\n\nZum Anschluss von Peripherie verfügen die Rechner über verschiedene Schnittstellen, die vom verbauten U855 oder U857 (englisch \"Counter Timer Circuit\" kurz \"CTC\") angesteuert werden. Dazu zählen die Buchse mit digitalen Ein- und Ausgabekanälen für spezielle Anwendungen und ein Joystick-Anschluss mit fünfpoliger DIN-Buchse für die von Robotron produzierten Joysticks. Des Weiteren stehen Steckplätze im Modulschacht für bis zu vier Erweiterungen bereit.\n\nIn Zusammenhang mit vor allem westlichen Heimcomputern der 1980er Jahre kamen als Massenspeicher hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die preisgünstigste Variante der Datenaufzeichnung durch Audiokassetten hat den Nachteil geringer Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung oder im Falle der DDR kaum erhältlich waren. Bei Erscheinen des Z 9001 standen zur Datenaufzeichnung lediglich Kassettenrekorder und Tonbandsysteme zur Verfügung. Diskettensysteme kamen erst einige Zeit nach Veröffentlichung des KC 87 ab Ende des Jahres 1988 und auch nur in Kleinstserien hinzu.\n\nAlle Robotron-Kleincomputer verfügen über einen Anschluss zur Speicherung von Programmen und Daten auf Kompaktkassetten durch handelsübliche Kassettenrekorder. Speziell zum Gebrauch mit Computern wurden Geräte kleinerer Abmessungen, wie etwa die Typen \"Geracord\", \"Datacord\" und später \"LCR-C DATA\" des Herstellers VEB Elektronik Gera, zum Kauf angeboten. Die mittlere Datenübertragungsrate durch die Schnittstelle beträgt etwa 1.000 Bit/s. Auf einer doppelseitig bespielten 60-Minuten-Audiokassette können 300 bis 360 KB Daten gespeichert werden.\n\nGegen Ende des Jahres 1988 wurde für die KC-Computer ein vom Zentralinstitut für Kernforschung in Rossendorf bei Dresden entwickeltes Diskettensystem der Öffentlichkeit vorgestellt. Das System umfasst verschiedene Komponenten zum Anschluss an den Computer und ein Beistellgerät, das zwei 5¼-Zoll-Laufwerksmechaniken beispielsweise vom Typ \"Diskettenspeicher K5601\" enthalten kann. Die Anbindung zum Computer erfolgt über ein Steckmodul, das sämtliche Ansteuerungselektronik und ein 26-poliges Kabel zum Verbinden mit dem Diskettengerät enthält. Es erlaubt den Betrieb von maximal zwei Laufwerken mit Speicherkapazitäten von bis zu 800 KB pro Diskette.\n\nAls Diskettenbetriebssystem wurde das CP/M-kompatible \"SCP\" mitgeliefert, das einen auf 64 KB ausgebauten Arbeitsspeicher voraussetzt. Durch Speicherbankumschaltung („Schatten-RAM“) erlaubt SCP das schnelle Zwischenspeichern von variablen Systemdaten, was zu einer verringerten Anzahl von mechanischen Diskettenzugriffen und damit zu kürzeren Ladezeiten führt.\n\nDie integrierte alphanumerische Elastomer-Tastatur enthält 65 Tasten in schreibmaschinenähnlicher QWERTZ-Anordnung. Sie ist aufgrund der kleinen, schwergängigen und nicht ergonomisch geformten Tasten sowie des fehlenden Druckpunktes für längeres Arbeiten kaum geeignet und wurde von vielen Anwendern, insbesondere im industriellen Bereich, durch komfortablere Varianten ersetzt. In das Tastaturfeld sind zwei Kontroll-Leuchtdioden (LED) eingelassen; die rote LED auf der rechten Seite leuchtet nach dem Einschalten des Rechners, die grüne LED auf der linken Seite zeigt die Umschaltung auf die Sonderzeichenbelegung der Tasten an. Die Tastatur wurde vielfach als sehr unergonomisch empfunden und beispielsweise in Eigenregie durch Schreibmaschinentastaturen ersetzt.\n\nDie ausgelieferten Computer bieten lediglich eine Minimalausstattung an Hardware. Damit ist zwar ein eigenständiger Betrieb möglich, viele Aufgabenstellungen erfordern jedoch eine Aufrüstung. Abgesehen vom Bausatz zum Umrüsten der Computer-Grundvarianten auf Farbausgabe werden fast alle erhältlichen Erweiterungen am Expansionssteckplatz angeschlossen. Dabei stehen insgesamt vier Steckplätze für entsprechende Erweiterungsmodule zur Verfügung. Aufgrund bestehender Inkompatibilitäten zwischen manchen Erweiterungen können nicht in allen Fällen die vier Steckplätze gleichzeitig belegt werden.\n\nIm Folgenden sollen nur die wichtigsten und häufig eingesetzten Erweiterungsmodule ausführlicher beschrieben werden. Im Anschluss folgt eine tabellarische Auflistung aller von Robotron produzierten Erweiterungen mit einer kurzen Funktionsbeschreibung.\n\nZur Vergrößerung des Arbeitsspeichers stehen verschiedene, unter anderem von Robotron produzierte Erweiterungsmodule zur Verfügung. Mit Erscheinen des Z 9001 beschränkte sich aufgrund der hohen Herstellungskosten die Auswahl auf solche mit einer Speicherkapazität von lediglich 16 KB RAM und batteriegepufferten Versionen mit noch weniger, nämlich nur 4 KB jedoch statischen RAMs („SRAM“). Der SRAM diente hauptsächlich dem Zwischenspeichern variabler Daten, die auch nach dem Abschalten der Computer beispielsweise durch Netzausfälle noch zur Verfügung stehen sollten. Nach Entnahme des SRAM-Moduls aus dem Schacht ist ein Transferieren der darin enthaltenen Daten auch nach mehreren Wochen Lagerzeit auch auf andere Computer möglich. Ab 1989 war ein verbessertes SRAM-Modul mit einer Speicherkapazität von 10 KB erhältlich. Bei Verwendung von Speichererweiterungen muss ihnen der Benutzer manuell mithilfe von DIP-Schaltern jeweils einen zu belegenden Adressbereich (entweder von $4000 bis $7FFF oder von $8000 bis $BFFF) zuteilen.\n\nDaneben existieren weitere RAM-Module mit höherer Speicherkapazität von Drittherstellern oder Bastlern, die jedoch erst nach dem Sinken der Preise gegen Ende 1988 aufkamen. Das Diskettensystem von Rossendorf beispielsweise enthält ein RAM-Modul mit einer Speicherkapazität von 64 KB.\n\nDiese Erweiterung ergänzt die Darstellungsmöglichkeiten des Computers um einen hochaufgelösten monochromen Pixelgrafikmodus mit 256 × 192 Bildpunkten („Vollgrafik“). Die Baugruppe besteht aus der in einem externen Gehäuse verbauten Elektronik mit RGB-Bildsignalerzeugung und eigenem Videospeicher sowie einem Flachbandkabel zum Anschluss an einen der vier Erweiterungsteckplätze. Das Baugruppenchassis ist derart konstruiert, dass der darauf abzustellende Computer durch entsprechende Haltezapfen nicht verrutschen kann. Ein mechanischer Umschalter ermöglicht wahlweise das Anzeigen der Vollgrafik oder der Zeichensatzmodi des Computers am angeschlossenen Bildausgabegerät. Zum Betrieb werden zusätzliche 32 KB Arbeitsspeicher in Form zweier 16-KB-RAM-Module und auf Kassette mitgelieferte Treiberprogramme benötigt. Der Einsatz mit den von KC 87.20 und 87.21 verschiedenen Computern erfordert zudem das Plotter-Modul, das die Ansteuerung der Pixelgrafik mithilfe von BASIC-Befehlen ermöglicht. Soll die Baugruppe mit den nicht farbfähigen Computervarianten betrieben werden, so sind einige modifizierende Handgriffe an der Erweiterung auszuführen.\n\nDie Computer verfügen ab Werk über keine Möglichkeiten zum Ansteuern eines Druckers. Vielmehr müssen je nach Druckertyp entweder Treiberprogramme geladen oder entsprechende Erweiterungsmodule nachgerüstet werden. So existieren Zusätze zum Betrieb der vom Büromaschinenwerk Sömmerda produzierten 9-Nadeldrucker mit den Bezeichnungen \"K6303\", \"K6311\" und \"K6312\" beziehungsweise für den Thermodrucker \"K6304\". Für den Betrieb der in der DDR erhältlichen tschechoslowakischen Plotter der Typen \"XY4131\" und \"XY4141\" war ebenfalls eine Erweiterung erhältlich. Dieses Plotter-Modul ergänzt zudem den BASIC-Standardbefehlssatz der Computer KC 87.10 und KC 87.11 und nach kleineren Anpassungen auch den von Z 9001 und KC 85/1 um entsprechende Vollgrafikbefehle.\n\nViele in der DDR weitverbreitete, mit einem Typenraddruckwerk ausgestattete elektronische Schreibmaschinen verfügen über die Möglichkeit, extern eingespeiste Daten drucken zu können. Sie wurden daher oft als preiswerte Ausgabesysteme insbesondere für Heimcomputersysteme eingesetzt. Zu den unterstützten Schreibmaschinen zählen die Baureihen \"S3000\", \"Erika 3004\", \"Erika 3005\", \"Erika 3006\", \"Erika 3015\" und \"Erika 3016\" vom Hersteller VEB Robotron Optima Büromaschinenwerk Erfurt sowie das Modell \"Erika 6005\" vom VEB Mikroelektronik Erfurt. Diese Geräte erlauben bei Benutzung des Schreibmaschinen-Moduls zudem den Betrieb als komfortable Ersatztastatur für den Computer.\n\nNeben dem Einsatz im Bildungswesen wurden die Computer mangels Alternativen häufig auch zur Automatisierung in der Produktion eingesetzt. Die Anwendungen beschränkten sich dabei auf einfache Regelungsaufgaben beispielsweise in Gewächshäusern oder in der Robotik. Die dabei zu regelnden physikalischen Größen wie etwa Temperatur und Druck müssen vor der Auswertung in eine für den Computer verarbeitbare Form gebracht, d. h., das analoge Signal des Messfühlers muss in ein digitales umgewandelt werden. Die dafür benötigte Analog-Digital-Umsetzer-Erweiterung („ADU-Modul“) wurde häufig zusammen mit dem Eingabe-Ausgabe-Modul („E/A-Modul“) zum Ansteuern beispielsweise von externen Stellgliedern eingesetzt. Daneben dienten Computer mit ADU-Modul aber auch als Digital-Oszilloskop, das heißt zum Visualisieren sich zeitlich ändernder Messgrößen.\n\nBei der existierenden Software handelt es sich überwiegend um Eigenentwicklungen aus der DDR. Umsetzungen von Programmen westlicher Z80-basierter Heimcomputersysteme waren aufgrund technischer Unterschiede in der Regel sehr aufwändig und wurden lediglich von den in ihren Grafikmöglichkeiten ebenfalls sehr eingeschränkten Rechnern ZX80 und ZX81 vorgenommen. Die für den Z 9001 bzw. KC 87 erstellten höheren Programmiersprachen waren ebenso nicht mit denen westlicher Systeme kompatibel, da der Befehlssatz größtenteils auf die Eigenheiten der DDR-Computer optimiert wurde. Am einfachsten ist der Programmaustausch und die entsprechende Anpassung von Software mit den Rechnern der Reihe KC 85/2 bis KC 85/4, die über eine ähnliche Systemarchitektur verfügen und sich im Datenspeicherformat und BASIC-Dialekt gleichen. Bei speziellen Aufgabenstellungen war es oftmals wirtschaftlicher, entsprechende Software von Grund auf neu zu entwickeln.\n\nWie bei anderen Heimcomputern auch erfolgte der Vertrieb von Software auf verschiedenen Datenträgern. Die preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Bei den in der Herstellung vielfach teureren ROM-Modulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen wie etwa Programmiersprachen von großem Vorteil war. Neben den mit fest verbauter Software ausgelieferten Modulen existiert zudem ein frei bestückbares ROM-Modul (Typenbezeichnung 690 002.7). Auf den darin befindlichen Sockeln finden bis zu fünf EPROMs à 2 KB Platz, die zuvor häufig mit dem ebenfalls von Robotron erhältlichen EPROM-Programmiergerät mit Daten versehen wurden.\n\nDie Verbreitung von Software sowie der Austausch von Erfahrungen erfolgten vor allem durch private Kontakte sowie über Zeitungsanzeigen, bei Messen, durch Abdruck von Programmen in Zeitschriften und durch Ausstrahlung im Rundfunk, wie beispielsweise in der Sendung \"Rem\". Von staatlicher Seite wurde die Erstellung von Software beispielsweise über die Gesellschaft für Sport und Technik (GST) mit ihrer Sektion Computersport gefördert. Zu deren Aktivitäten gehörte auch das Organisieren und Austragen von öffentlichen Wettkämpfen, den „Programmierolympiaden“.\n\nBeschränkungen der Weitergabe durch Urheberrechtsschutz und damit verbunden Kopierschutzmechanismen existierten nicht. Der Verkauf im Handel spielte nur für die durch Robotron entwickelten Programme eine, wenn auch untergeordnete, Rolle. Ein kommerzielles Softwareangebot vergleichbar mit dem Heimcomputer-Markt in Westeuropa oder Nordamerika existierte weder für Anwendungssoftware noch im Spielebereich.\n\nZur Konfiguration der Computer-Hardware dient das im Festwertspeicher enthaltene Betriebssystem Z9001-OS, je nach Computertyp in geringfügig voneinander verschiedenen Versionen. Es basiert auf dem von Digital Research 1974 für Intel-8080- und Zilog-Z80-Referenzsysteme vorgestellten Betriebssystem CP/M-80. Vom Original unterscheidet es sich durch einige von den Robotron-Ingenieuren vorgenommene Modifikationen, wie etwa die Implementierung der Kassettenschnittstelle und die veränderte Speicherbelegung.\n\nZum Betrieb des Rossendorf-Diskettensystems wurde 1988 eine eigens angepasste Version von CP/M 2.2 mit dem Namen \"SCP\" bereitgestellt, die mit dem Z9001-OS gemeinsam betrieben werden kann. Sein interner Aufbau und Befehlsumfang entspricht im Wesentlichen dem von CP/M. Durch diese weitestgehende Kompatibilität steht prinzipiell auch die umfangreiche CP/M-basierte Programmbibliothek für die Computer zur Verfügung. Viele dieser Programme wie z. B. Wordstar sind jedoch durch die eingeschränkten Grafik- und Tastaturmöglichkeiten von Z 9001 bzw. KC 87 nicht lauffähig, andere wie beispielsweise Turbo-Pascal erfordern zum einwandfreien Betrieb entsprechende Modifikationen. Ein weiterer Vorteil von SCP sind die mitgelieferten Druckertreiber, die einen Einsatz des User-Ports als softwareseitige Druckerschnittstelle ermöglichen. Damit entfällt die Benutzung eines Druckermoduls, wodurch ein Steckplatz des durch das Diskettensystem ohnehin fast vollständig belegten Erweiterungsschachtes für weitere Peripherie frei bleibt. Neben der eigentlichen Systemsoftware enthält der SCP-Datenträger zudem das um Diskettenzugriffsbefehle erweiterte kompatible \"ZBASIC\".\n\nAufbauend auf der Systemsoftware kam dem benutzerspezifischen Einsatz der Computer in unterschiedlichsten Anwendungsgebieten wie in Bildungseinrichtungen, aber auch in der Wirtschaft, große Bedeutung zu. Aufgrund eines praktisch nicht vorhandenen Softwaremarktes in der DDR mussten anfänglich nahezu alle Themengebiete durch eigenentwickelte oder anzupassende Software abgedeckt werden. Für die Z-9001- und KC-85/1-Computer standen bei Erscheinen lediglich das von Kassette zu ladende BASIC und Assemblersprache zur Verfügung. Weitere höhere Programmiersprachen kamen später hinzu, mit Aufkommen des Rossendorf-Diskettensystems auch leistungsfähige CP/M-basierte Compilersprachen wie Turbo-Pascal.\n\nDie mäßige Ausstattung der Computer, die Bearbeitung zeitkritischer Probleme („Echtzeitanwendungen“) oder das Einbinden eigenentwickelter und damit nicht standardmäßig unterstützter Hardware erforderte in vielen Fällen speichereffizientes und hardwarenahes Programmieren. Dies war bei U880-basierten Geräten ausschließlich durch die Verwendung von Assemblersprache mit entsprechenden Übersetzerprogrammen, den Assemblern, möglich. Zur Eingabe der Programmanweisungen („Sourcecode“) dient der häufig mitgelieferte Editor. Ebenfalls erhältliche Debugger vereinfachen die Fehleranalyse.\n\nAnfänglich war lediglich der wenig komfortable \"SYPS-K-1520-Editor/Assembler\" (EDAS) auf Programmkassette mit der Typenbezeichnung R0121 bzw. 690 026.0 oder als Steckmodul erhältlich. Der später bei Erscheinen des KC 87 hinzugekommene \"Interpretative Dialogassembler\" (IDAS) erlaubte neben der üblichen Assemblierung des gesamten Quellcodes in einem Stück auch – einem Interpreter gleich – ein zeilenweises Abarbeiten. Diese Technik ist bei der ansonsten schwer zu beherrschenden Assemblerprogrammierung insbesondere für eine effiziente Fehlerdiagnose von großem Vorteil. Auch IDAS wurde mit zugehörigem Editor und einem Maschinensprachemonitor auf Kompaktkassette mit der Typenbezeichnung R0122 oder als Steckmodul ausgeliefert. Für den Einsatz von IDAS mit Z 9001 und KC 85/1 waren kleinere Änderungen an der Platine des Steckmoduls vorzunehmen.\n\nWeniger zeitkritische und hardwarenahe Anwendungen konnten mit den wesentlich übersichtlicheren und einfacher zu bedienenden, dafür aber in der Ausführung langsameren und weniger flexiblen Hochsprachen programmiert werden.\n\nMit dem Erscheinen des Z 9001 war gleichzeitig eine angepasste Version des K-1520-BASIC erhältlich. Diese vom Landwirtschaftlichen Institutes Dummerstorf entwickelte BASIC-Variante basiert wiederum auf dem bereits in den späten 1970er Jahren für westliche Computer veröffentlichten Extended Microsoft BASIC.\n\nDie Programmiersprache musste nach dem Start des Computers zunächst von Kassette in den mit 16 KB ohnehin nur spärlich vorhandenen Arbeitsspeicher eingelesen werden, womit die Nutzungsmöglichkeiten auf kleinere Programmierprojekte beschränkt blieben. Später wurde das BASIC auch als Steckmodul angeboten, was in Zusammenhang mit der maximal möglichen Speicheraufrüstung die Realisierung auch umfangreicherer Programmieraufgaben beispielsweise im Produktionseinsatz ermöglichte.\n\nDas vom niederländischen Rundfunk entwickelte internationale Projekt BASICODE, das eine Vereinheitlichung der BASIC-Dialekte verschiedener Heimcomputer anstrebte, fand auch in einem entsprechenden Zusatzprogramm für die KC-Rechner von Robotron seinen Niederschlag.\n\nNeben dem einsteigerfreundlichen BASIC waren im Bereich der Software-Entwicklung auch anspruchsvollere Compiler-basierte höhere Programmiersprachen wie KC-Pascal (als Steckmodul PASMOD), Pretty C und Forth verfügbar. Die Vorteile dieser Sprachen liegen in der Geschwindigkeit der von ihnen erzeugten ausführbaren Programme, allerdings um den Preis erhöhter Hardwareanforderungen. Mit Pretty C erstellte Anwendungen erzielen beispielsweise für bestimmte Spezialfälle bis zu 30-fach höhere Ausführungsgeschwindigkeiten als vergleichbare Programme in BASIC, erfordern jedoch auch – damals kostenintensive – Aufrüstungen des Arbeitsspeichers auf mindestens 32 KB.\n\nStand dem Anwender ein Diskettensystem mit CP/M-kompatiblem Betriebssystem SCP zur Verfügung, konnte nach diversen Modifikationen ein Großteil der CP/M-basierten Programmiersprachen wie ZBASIC oder Turbo Pascal genutzt werden.\n\nDer Hersteller Robotron bot vor allem einfache Spiele sowie Programme für den Bildungsbereich an und vertrieb sie durch den VEB Robotron-Vertrieb Berlin, Abt. VD. Mit \"Script\" war eine hinsichtlich ihrer Funktionalität für damalige Verhältnisse sehr umfangreiche Textverarbeitung erhältlich. Insbesondere durch Hobby-Programmierer entstand eine Vielzahl von Adaptionen und Portierungen von Arcade-Spieleklassikern wie beispielsweise \"Centipede\", \"Mazogs\", \"Tetris\", \"Pac-Man\" und \"Boulder Dash\", aber auch von bekannten Brett- und Kartenspielen wie Schach, Skat, Poker und Monopoly.\n\nSpezielle Zeitschriften für den KC 87 oder für alle DDR-Kleincomputer gab es nicht. Die Zeitschriften \"Funkamateur\", \"Jugend + Technik\", \"Mikroprozessortechnik\" und \"Practic\" veröffentlichten regelmäßig Neuigkeiten, Berichte, Bastelanleitungen zum Selbstbau von Zusatzhardware oder die Auf- und Umrüstung der Rechner sowie Programme zum Abtippen.\n\nAuch nach der Deutschen Wiedervereinigung wurde innerhalb der Anhängerschaft von DDR-Rechentechnik der Interessenaustausch in privaten Publikationen und ab den späten 1990er Jahren zudem in Internetforen weiter gepflegt. Am bekanntesten ist die vierteljährlich erscheinende Zeitschrift \"KC-News\" des 1991 gegründeten \"KC-Clubs\". Die Internetseite des Clubs bietet eine Anlaufstelle für Probleme und Fragen rund um die Computer der KC-Baureihe, die bei den seit 1995 jährlich deutschlandweit stattfindenden Treffen vertieft werden können.\n\nNach dem Ende der Heimcomputerära Anfang der 1990er Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Mitte der 1990er Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripherie entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reicht mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit unter anderem ein verstärktes Transferieren von sonst möglicherweise verlorengegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird. \n\nZur Emulation von Z 9001, KC 85/1 und KC 87 wurde der unter Windows und Linux lauffähige \"KCemu\" entwickelt.\n\nBei der erstmaligen Vorstellung der Rechner auf der Leipziger Frühjahrsmesse 1984 stießen sowohl HC 900 als auch Z 9001 auf reges Interesse. Die positive Aufnahme durch das Messepublikum fand kurz darauf auch in euphorischen Zeitungsreportagen ihren Niederschlag. Staatlich kontrollierte Zeitschriften wie \"Jugend + Technik\" und \"Funkamateur\" feierten den Z 9001 als ausbaufähige „leistungsfähige Datenverarbeitungsanlage“ zum „Steuern von Geräten der Heim- und Hobbytechnik“, als „frei programmierbare Erfassungs-, Verarbeitungs- und Steuerzentrale für Versuchs- und Demonstrationsanordnungen“ und als Voraussetzung für „vielseitige Bildschirmspiele zur kreativen Unterhaltung“.\n\nNach Vorstellung des KC 87 einige Jahre später charakterisierte die DDR-Presse den Computer wesentlich nüchterner als „gut zur Heranführung praktisch aller Bevölkerungsgruppen an Probleme der Anwendung der Computertechnik“ und als durch „einfache Modifizierbarkeit für verschiedene Aufgaben durch Steckmodule“ vielseitig einsetzbar, wobei „durch die Begrenzungen des Speicherplatzes und der Rechengeschwindigkeit natürlich Grenzen gesetzt sind“.\n\nVon westlicher Seite beurteilte man die Technik der auf der Leipziger Frühjahrsmesse 1984 vorgestellten Geräte etwas zurückhaltender und ging eher auf wirtschaftliche Aspekte gestützt auf Befragungen von interessierten Messebesuchern ein. In diesen Interviews stuften Besucher aus der DDR den Rechner durchweg als schwer zu beschaffen und als zu teuer ein; eine zukünftige Verbreitung in Privathaushalten wurde bezweifelt.\n\nIn jüngerer Zeit werden die in der DDR entwickelten und produzierten Rechner, darunter insbesondere Kleincomputer und Videospielautomaten, wieder verstärkt in den Medien, allen voran im Internet, wahrgenommen und auch in speziellen Museen ausgestellt. Dabei werden Z 9001, KC 85/1, KC 87 und die Mühlhäuser Rechner KC 85/2 bis 85/4 als vollständige Eigenentwicklungen charakterisiert, obwohl es sich bei vielen elektronischen Einzelkomponenten wie etwa dem U880-Mikroprozessor und bei der Systemsoftware um Kopien westlicher Vorbilder wie dem Z80-Mikroprozessor von Zilog mit seinem CP/M-Betriebssystem handelt. Darüber hinaus wird den Konstrukteuren der DDR-Computer technische und planerische Weitsicht bescheinigt. Trotz „komplizierter ökonomischer Bedingungen“ und „konkreter Materialbedingungen“ seien die Geräte technisch zuverlässig konstruiert und durch den Benutzer leicht beherrschbar, was „besonders von jungen Leuten mit viel Begeisterung aufgenommen und dabei sehr schöpferisch genutzt wurde.“ Gleichzeitig besteht kein Zweifel daran, dass der technologische Rückstand der Computer gegenüber den Produkten westlicher Industrieländer zum Zeitpunkt ihres Erscheinens stets etwa drei bis fünf Jahre betrug: Als die Produktion des KC 85/1 in der DDR aufgenommen wurde, waren im westlichen Ausland bereits wesentlich leistungsfähigere Computer für Privathaushalte erhältlich. Im direkten Vergleich zu den westlichen Z80-basierten Computern wie etwa dem Sinclair ZX Spectrum wird den meisten DDR-Kleincomputern hinsichtlich „Anwendungsbreite, Verarbeitungsleistung und Anschlussmöglichkeiten“ mindestens Gleichwertigkeit bescheinigt. Diese Möglichkeiten hätten jedoch durch im Handel nur „selten käuflich erwerbbare Peripherie, ungeeignete Tastaturergonomie, teilweise fehlende Graphikfähigkeit und begrenzte Software“ nicht voll ausgeschöpft werden können. Das nach der Wende kurzfristig besiegelte Produktionsende für DDR-Kleincomputer wird von allen Autoren einhellig auf die fehlende Konkurrenzfähigkeit durch nicht aufholbaren hard- und softwareseitigen Rückstand zurückgeführt.\n\n"}
{"id": "762119", "url": "https://de.wikipedia.org/wiki?curid=762119", "title": "Windows Server Update Services", "text": "Windows Server Update Services\n\nWindows Server Update Services (WSUS) ist eine Softwarekomponente des Microsoft Windows Server ab Version 2003, die für Patches und Aktualisierungen zuständig ist. Sie ist die Nachfolgeversion der Softwarekomponente Software Update Services.\n\nWindows Server Update Services ist ein Client-Server-System. Kern der Softwarekomponente ist die SQL-Datenbank (entweder über einen bereits vorhandenen SQL-Server 2005–2017 oder der mitgelieferten Windows Internal Database) des Servers, die die Versionsdaten der Aktualisierungen und die Berichte der Clients verwaltet. Die Clients liefern einen Statusbericht, Versionsdaten liegen von den verfügbaren Aktualisierungen vor. Zu einem vom Administrator bestimmbaren Zeitpunkt baut der Server eine Verbindung mit dem \"Windows-Update-Server\" auf und lädt die für seine Clients benötigten Aktualisierungen herunter. Diese werden dann über das lokale Rechnernetzwerk den Clients zugewiesen. Die Aktualisierungen werden über einen Pull-Mechanismus verteilt. Allerdings ist einstellbar, dass besonders wichtige Pakete (zum Beispiel kritische Sicherheitsaktualisierungen) durch eine Fristsetzung installiert werden.\n\nDie in die Betriebssysteme Windows 2000 mit SP3, Windows XP, Windows Vista, Windows 7, Windows 8 und Windows 10, Windows Server 2003, Windows Server 2008, Windows Server 2012 und Windows Server 2016 integrierte Clientkomponente ermöglicht Server- und Clientcomputern Aktualisierungen von Microsoft Update oder von einem Server, auf dem die Update Services ausgeführt werden, zu beziehen. Windows 95/98/ME/NT4 werden beim Aktualisierungsprozess per WSUS nicht berücksichtigt.\n\nDer Vorteil dieses Systems liegt in der Administration. Der Administrator muss nicht auf jedem Client die Aktualisierung aufspielen, sondern kann dies zentral vom Server aus an verschiedene Computergruppen zuweisen. Dies kann besonders dann sinnvoll sein, wenn durch Updates Kompatibilitätsprobleme mit einer installierten Applikation zu erwarten sind. Dann wird das entsprechende Update erst an die Clients ausgeliefert, wenn durch Tests sichergestellt ist, dass es keine Inkompatibilität zu vorhandenen Applikationen gibt.\nDownload und Installation der Aktualisierungen kann jeweils manuell und automatisch per Regel gesteuert werden. Das kann per Richtlinie für jede Gruppe oder auch lokal per Registry Key am Client konfiguriert werden. Am Client verwendet man entweder „gpedit.msc“ oder auch die gewohnte GUI.\n\nDer im Betriebssystem des Clients integrierte Dienst „BITS“ („Background Intelligent Transfer Service“ oder auch „Intelligenter Hintergrundübertragungsdienst“) sorgt dafür, dass nur dann Aktualisierungen auf die Clients übertragen werden, wenn die Bandbreite des Netzwerks es zulässt.\n\nJedoch bedingt dies eine konsistente Nutzung der Produkte von Microsoft (IIS, Datenbank, …). Außergewöhnliche Situationen wie ein per Modem angebundener Client eines Außendienstmitarbeiters sind dadurch nur schwer abdeckbar.\n\nMicrosoft stellt Windows Server Update Services auf der Microsoft-Website kostenlos als Download zur Verfügung. Seit September 2018 finden Microsoft-Nutzer auf der Website auch erstmals die sogenannten \"Microsoft Security Servicing Criteria for Windows\". Damit legt Microsoft offen, nach welchen Kriterien gemeldete Schwachstellen klassifiziert werden und welchen Sicherheitslücken ein Update zugewiesen wird und welchen nicht. \n\nTools\n\nWeitere Links\n"}
{"id": "763484", "url": "https://de.wikipedia.org/wiki?curid=763484", "title": "Magix Video deluxe", "text": "Magix Video deluxe\n\nVideo deluxe ist eine kostenpflichtige, für Heimanwender konzipierte Videoschnittsoftware des Unternehmens Magix. Das Programm wurde im Jahr 2001 erstveröffentlicht und gilt als europaweit meistgekaufte Video-Software. Die aktuelle Version heißt Magix Video deluxe 2017, wobei die Jahreszahl nicht mehr offizieller Teil des Programmnamens ist. Video deluxe gibt es in mehr als 10 Sprachen, u. a. Deutsch, Englisch, Französisch, Spanisch, Italienisch, Niederländisch, Polnisch, Türkisch, Finnisch, Schwedisch, Japanisch, Russisch, Griechisch und Portugiesisch, in einigen dieser Sprachen liegt jedoch nicht die aktuelle Version vor.\n\nEs wird in drei Versionsvarianten mit unterschiedlichem Funktionsumfang angeboten:\nVideo deluxe, Video deluxe Plus und Video deluxe Premium. Für professionelle Anforderungen gibt es zudem die auf der gleichen Plattform basierende Software Video Pro X.\n\nDie 2010 veröffentlichte Version 17 des Programms (in den Varianten Plus und Premium) gilt als weltweit erstes Videoschnittprogramm für Konsumenten, das 3D-Aufnahmen verarbeiten kann.\n\nMit der Version Magix Video deluxe 2015 lassen sich außerdem 4K-Videos und 3D-Videos (Plus- und Premium-Versionen) schneiden. Ab der Version 2016 kann man auch 360° Videos bearbeiten.\n\nIm englischen Sprachraum trägt die Software den Namen \"Magix Movie Edit Pro\".\n\n\nProgrammfremde Audio-Plug-ins lassen sich einbinden (VST). Bei ausreichend schneller Hardware kann man in Echtzeit vorhören. Außerdem verfügt Video Deluxe Plus über einen Dolby Digital 5.1 Creator, mit dem sich der Ton beliebig im dreidimensionalen Raum (Surround-Sound) positionieren lässt. Dieser Vorgang lässt sich auch dynamisch über die Zeit und auch in Echtzeit steuern und abspeichern.\n\nVideo deluxe benutzt bis zur Version 2006 den Ligos-Encoder (MPEG-Encoder). Der Encoder wurde verschiedentlich in Fachzeitschriften (u. a. c´t) getestet und hat befriedigende bis gute Noten erhalten. Zwischen Version 2006/2007 und 2014 fand der Encoder der Firma „MainConcept“ Verwendung. Ab Version 2015 stammt der Standard-Codec in Video deluxe von Intel.\n\nHDV-Material kann ab Version 2006/2007 direkt von HDV-Kameras aufgenommen und verarbeitet werden.\n\nDas fertige Video kann exportiert oder mit einem interaktiven Menu auf DVD oder Blu-ray Disc gebrannt werden. Surround-Ton im Format 5.1 kann dabei mit einem Dolby Digital Encoder integriert werden, mehrere Tonspuren mit Auswahlmenü sind möglich.\n\nDer Preis für Video deluxe beträgt je nach Ausführung zwischen 69,99 Euro und 129,99 Euro im eigenen Onlineshop. Die Toolbar von ask.com und das Programm „simplicheck“ werden ab der Version 2015 nicht mehr standardmäßig mitinstalliert. Die „Pro X“-Version ist für rund 400 Euro erhältlich.\n\n\n\nUnterstützte Betriebssysteme:\n\nMinimal:\n\nEmpfohlen:\n\nFür 4K-/HD-Bearbeitung:\n\nFür 3D-Bearbeitung:\n\nMPEG-2 und Dolby Digital werden erst nach einer kostenfreien online Registrierung bei Magix freigeschaltet, MPEG-4 ist kostenpflichtig als Plug-in erhältlich. MP3-Export ist nur möglich, wenn Windows Media Player (ab Version 10) installiert ist.\n\n\n"}
{"id": "763826", "url": "https://de.wikipedia.org/wiki?curid=763826", "title": "SpeedCommander", "text": "SpeedCommander\n\nSpeedCommander ist ein Dateimanager für Windows und wird zu den Norton-Commander-Klonen gezählt. Typisch für diese Art von Dateimanagern ist, dass sie vollständig über die Tastatur bedient werden können, wobei sich ein Quasi-Standard der Tastenbelegungen herausgebildet hat, der vom MS-DOS-Original abgeleitet ist.\n\nDie erste Version wurde im Mai 1993 fertiggestellt und gelangte im September 1993 in den Verkauf. In den neueren Versionen lässt sich die Oberfläche und Bedienung so weit verändern, dass bei weitestgehender Beibehaltung der erweiterten Leistungsfähigkeit das Look and Feel des Windows-Explorers nachempfunden werden kann.\nNeben dem Dateimanager und der Möglichkeit der Einbindung weiterer Programme bietet SpeedCommander einige integrierte Tools, darunter einen Texteditor (SpeedEdit), einen Multiformat-Betrachter, einen Dateipacker und andere gängige Werkzeuge.\n\nZu den Funktionen der Standard-Variante gehören:\n\n\nSpeedCommander Pro\n\nSeit Version 15 wird auch eine Pro-Variante des SpeedCommander angeboten. Zusätzlich zum Funktionsumfang der Standard-Variante gibt es für einen Aufpreis von 20 € unter anderem Brennfunktionen und den Zugriff auf Cloudspeicher.\n\nSpeedCommander bietet vollständige Unicode-Unterstützung und läuft auf allen Windows-Versionen ab Windows 7. Es gibt Installationspakete für 32- und für 64-Bit-Systeme.\n\n"}
{"id": "764467", "url": "https://de.wikipedia.org/wiki?curid=764467", "title": "Finder (Mac)", "text": "Finder (Mac)\n\nDer Finder ist der Standard-Dateimanager des aktuellen Apple-Betriebssystems macOS (OS X, Mac OS X; ab 2001) und des klassischen (1984–2001).\n\nDer Finder, der ab System 6 von 1991 die gleiche Versionsnummer wie das Betriebssystem hat, bietet eine grafische Sicht des Dateisystems und der genutzten Geräte. Dateien aller Art lassen sich hiermit Verwalten und Sortieren. Der Schreibtisch-Metapher folgend werden Dokumente mit entsprechendem Symbol dargestellt; auch angeschlossene Geräte wie der MP3-Player iPod, Festplatten und USB-Sticks oder eingelegte CDs erscheinen mit entsprechendem Symbol auf dem Schreibtisch (sofern gewünscht). Dokumente und Programme werden durch Doppelklick geöffnet. Das Löschen von Dateien bewirkt, dass diese in den Papierkorb gelegt werden, von dem aus eine Wiederherstellung möglich ist. Erst das Leeren des Papierkorbes löscht die darin enthaltenen Dateien unwiderruflich.\n\nDer Finder bietet eine gute Übersichtlichkeit, wenn mit relativ wenig Dateien und Ordnern hantiert wird; er wird oft dafür kritisiert, dass er beim Umgang mit sehr vielen Dateien unpraktisch ist. Alternative Dateimanager ergänzen den Finder an dieser Stelle. Unter macOS kann der Standard-Dateimanager beliebig gesetzt werden, sodass andere Dateimanager den Finder gänzlich ersetzen können; beispielsweise \"Path Finder\".\n\nDer Finder war bereits Bestandteil des einfach „System“ genannten Betriebssystems des ersten Macintosh von 1984. Er wurde von Bruce Horn entwickelt und gemeinsam mit Steve Capps geschrieben. Der Dateimanager war zum Großteil von dem inspiriert, was bereits bei der Apple Lisa zu sehen war. Das Bedienkonzept von Lisa OS war stark von dem grafischen Benutzerinterface des Xerox Alto inspiriert, doch bei Lisa OS wurden erstmals Dropdown-Menüs eingesetzt. Bei der Entwicklung des Macintosh wurde 1981 der Xerox Star (Nachfolger des teuren Xerox Alto) besichtigt und zusammen mit den Ideen von Lisa OS entstand so auch der Finder.\n\nFür System 1 gab es zahlreiche Aktualisierungen für den Finder, der seine eigene Versionsnummer führte. So war in System 2 (April 1985) bereits der Finder 4.1 enthalten, der erstmals das Macintosh-typische eines Disketten-Symbols in den Papierkorb einführte, um eine Diskette auszuwerfen. \n\nAuf dem Macintosh waren zentrale Funktionen des Betriebssystems, aber auch des Finder, im Macintosh-ROM, genannt Macintosh-Baukasten (), ausgelagert. Das auf Diskette geladene Programm konnte daher sehr schlank sein und benötigte auch wenig Arbeitsspeicher, da viele der Funktionen im ROM aufgerufen wurden. Dies sollte sich erst in den 1990er Jahren ändern, als RAM billiger wurde. Die wurde daraufhin beim Betriebssystemstart von einer Datei in den schnellen RAM geladen. Neuere Versionen des Finder wurden jedoch komplexer und mit der Entwicklung von Mac OS X ab 1998 (das auf OPENSTEP basiert und ein BSD-Unix-System ist) entfällt die Abstützung auf den Macintosh-Baukasten zur Gänze.\n\n1988 wurde der Finder auch mit GS/OS 4.0 für den Apple IIgs ausgeliefert. Die Namensgleichheit ist kein Zufall, da die Bedienung mit der des Macintosh identisch ist, jedoch war diese Version für den Apple II neu programmiert worden.\n\nMit der Macintosh System Software 5 von 1987 war erstmals ein eingeschränkt multi-tasking-fähiger Finder verfügbar: Der „MultiFinder“ erlaubte kooperatives Multitasking und konnte wahlweise statt des bisherigen Finders aktiviert werden, für den Wechsel war allerdings ein Neustart erforderlich. Der alte Finder blieb jedoch vorerst die Standardeinstellung, denn mit dem MultiFinder liefen nicht mehr alle Programme – da dieser im Speicher erhalten blieb stand für andere Programme weniger des ohnehin knappen Arbeitsspeichers zur Verfügung. Ab System 7 ersetzte der MultiFinder den bisherigen Finder vollständig.\n\nIm Projekt Copland wurde der Finder mit Multithreading-Fähigkeiten ausgestattet. Nach dem Ende des Projekts wurden diese in Mac OS 8 übernommen.\n\nAb 1996 wurde an einer Nachfolge für das klassische Mac OS gearbeitet: Rhapsody. Darin wurde der aus NeXTStep/OPENSTEP als Dateimanager und Schreibtisch verwendet. Erst mit dem Startschuss für Mac OS X, das ab 1998 entwickelt wurde, wurde auch der Finder vom klassischen Mac OS (damals Mac OS 8) auf das neue Betriebssystem portiert. Möglich machte das die Entwicklung der großteils Macintosh-kompatiblen Programmierschnittstelle (API) Carbon. Dabei wurde der mit Teilen des Finders kombiniert.\n\nMit Mac OS X Snow Leopard (10.6, 2009) wurde der Finder im moderneren Cocoa-API komplett neu geschrieben und damit auch zur 64-Bit-Anwendung.\n\nDie Symbole (Icons) des originalen Macintosh von 1984 stammten von Grafikdesignerin Susan Kare, die auch das Finder-Icon erstellte. Es zeigte bis Mac OS 9 den originalen Macintosh in stilisierter Form. Im Start-Bildschirm des Betriebssystems („“ bzw. „“) hingegen wurde eine abstrakte Darstellung des originalen Macintosh verwendet, die von Tom Hughes und John Casado entworfen worden war. In der Presse wurde das bis einschließlich System 7.5 (Ende 1994) verwendete Logo als „Picasso-Stil“ bezeichnet, obwohl es laut John Casado der französische Maler Henri Matisse war, der als Inspiration diente.\n\nAb System 7.5.1 (ab 1995) wird ein neues Symbol beim Start-Bildschirm angezeigt, das über dem Schriftzug „Mac OS“ zu sehen ist. Dieses Symbol wurde 1994 von der kalifornischen Design-Firma AlbenFaris nach einem Auftrag von Apple entworfen. Es wurde auch zur Kennzeichnung von Macintosh-Software, z. B. auf Verpackungen von Programmen oder Spielen, verwendet. Unter Mac OS X findet sich das Symbol als das Programmsymbol für den Finder wieder – außer einer dreidimensionaleren Gestaltung wurde bis OS X Mavericks (10.9, 2013) nichts verändert. Erst mit OS X Yosemite (10.10, 2014) wurde das Symbol modernisiert und „freundlicher.“ Viele Mutmaßungen zur Entstehung des neuen Logos sehen es von Picassos Gemälde „“ (deutsch „Zwei Personen“) inspiriert. Tatsächlich solle es nach Angaben von AlbenFaris das Spiel zwischen dem Computer(monitor) und dem Gesicht des Anwenders stilisieren.\n\nDer Finder ist die zentrale Anwendung des Betriebssystems. Er ist nicht nur Dateimanager, sondern auch der verantwortliche Prozess für die Darstellung, Verwaltung und Interaktion mit dem Schreibtisch.\n\nAb System 7 kann der Benutzer jedem Dokument Metadaten zuweisen, wie etwa das Etikett und Kommentare, und eine Listenansicht mit ausklappbaren Ordnern wurde eingeführt. Ab System 7 sind auch Dateiverknüpfungen möglich, die als „Alias“ bezeichnet werden. Dabei wird eine Datei angelegt, die vom Betriebssystem (bzw. vom Finder) verarbeitet wird und somit als Verweis auf eine andere Datei interpretiert wird.\n\nAb Mac OS 8 können dank Multithreading mehrere Kopier- und Löschvorgänge gleichzeitig ablaufen. Eingeführt wurden außerdem Popup-Ordnerfenster, die als Reiter am Bildschirmrand „angedockt“ sind, sowie eine spezielle „Tasten-Ansicht“, bei der jedes Symbol als Button im Fenster angezeigt wird, das durch einfachen Klick gestartet wird.\n\nMit Mac OS 9 wurden „“ eingeführt. D. h. wenn man beim Ziehen eines Dateisymbols über einen Ordner ging und einen Moment wartete, öffnete sich dieser Ordner automatisch, und man konnte so beliebig tief durch die Ordnerhierarchie navigieren. Weiterhin konnte man für bestimmte Ereignisse Töne einstellen, mit Hilfe sogenannter „“.\n\nMit Mac OS X 10.0 wurde ein neu entwickelter Finder eingeführt, der Konzepte des NeXTStep Workspace Manager und des Finders aus Mac OS 9 vereinen sollte. Er war in C++ mit dem Carbon-API geschrieben, im Gegensatz zum Workspace Manager, der in Objective-C mit dem Cocoa-Framework geschrieben war.\n\nDer Finder erhielt zusätzlich die von NeXTStep übernommene „Spaltenansicht“ für Ordnerinhalte und wurde an die neue Benutzeroberfläche Aqua angepasst. Noch unterstützt dieser Finder weder Etiketten noch die „Suche nach Metadaten“ des früheren Finders.\n\nDer Finder 10.3 beherrscht wieder die Funktionen \"Etiketten vergeben\" und \"Suche nach Metadaten\".\n\nHier wurde Spotlight eingeführt, eine Index-basierte, sehr schnelle systemweite Suche. In jedem Finder-Fenster ist ein Spotlight-Suchfeld integriert, Suchanfragen lassen sich als \"intelligente Ordner\" in die in 10.3 eingeführte Seitenleiste speichern. Damit hatte man die Ergebnisse einer bestimmten Suche immer parat, z. B. alle Dokumente, die innerhalb der letzten drei Tage geöffnet wurden. Der Finder 10.4 beherrschte auch wieder die Funktion \"Ereignisgeräusche\" des Finder 9.0.\n\nFolgende Neuerungen wurden mit dem Finder von Leopard (10.5, 2007) eingeführt:\n\n\nIn Snow Leopard (10.6, 2009) wurde der Finder auf der Basis von Objective-C und Cocoa als 64-Bit-Anwendung neu geschrieben. Neue Funktionen kamen nicht hinzu, außer der Unterstützung von Grand Central Dispatch (Multithreading).\n\nAb Mavericks (10.9) unterstützt der Finder sogenannte Tabs: ein Finder-Fenster kann mehrere Ordner darstellen, zwischen denen mittels Reiter () umgeschaltet wird. Die Registernavigation erspart zu viele offene Finder-Fenster wenn mehrere Ordner und Dateien parallel verwaltet werden sollen und reduziert somit die Anzahl von Fenstern in der Taskleiste (dem Dock).\n\nMit Yosemite wurde der Reiter „iCloud Drive“ zur Seitenleiste hinzugefügt, über den auf Dokumente in der iCloud zugegriffen werden kann.\n\nDas Menü „Alle meine Dateien“ in der Seitenleiste wurde mit der Veröffentlichung von High Sierra zu „Zuletzt benutzt“ umbenannt. Wie der Name impliziert, kann darüber nicht mehr auf alle Dateien auf dem Mac, sondern nur noch auf die in letzter Zeit Geöffneten zugegriffen werden. Außerdem wurde das neue Dateisystem APFS hinzugefügt, was den Speicherplatz der Dateien reduzieren soll und das Betriebssystem und den Finder grundsätzlich schneller macht. Des Weiteren ist es jetzt möglich, alle Dateien aus iCloud Drive direkt mithilfe eines Links zu teilen.\n\nMit der neusten Version des Betriebssystems macOS Mojave unterstützt der Finder einen Dunkelmodus und der Schreibtischhintergrund lässt sich so einstellen, dass er sich dynamisch dem Tagesverlauf anpasst. Des Weiteren kann man Dateien mithilfe von „Stapeln“ einfacher aufräumen und die direkte Bearbeitung von bestimmten Dateien aus dem Finder heraus ist nun möglich.\n\nUm die Fensterposition und -größe sowie die Anordnung der Dateisymbole für einen Ordner zu speichern, legt der Finder versteckte Dateien mit dem Namen codice_1 in jedem Ordner an. Lokale Dateien, deren Namen mit einem Punkt beginnen, werden am Mac selbst standardmäßig nicht angezeigt. In diesen codice_1-Dateien werden sämtliche Ansichtsoptionen eines Ordners gespeichert, sie sind vergleichbar mit den codice_3 unter Windows von Microsoft. Dieses Verfahren irritiert manche Anwender anderer Betriebssysteme, die über ein Netzwerk auf solche Verzeichnisse zugreifen, da andere Betriebssysteme diese Informationen nicht interpretieren können und bei Doppelklick darauf Fehlermeldungen anzeigen. Entsprechend kann man auf Netzwerklaufwerken solche Unix-artigen „DOT-Dateien“ ausblenden lassen oder das Erstellen dieser Dateien von vornherein deaktivieren, was durch Konfigurationsprogramme von Drittanbietern oder durch manuelles Editieren einer Konfigurationsdatei erreicht werden kann.\n\nAbgesehen vom Netzwerkbetrieb irritiert die Existenz dieser Metadaten auch auf Wechseldatenträgern, wenn ein solcher auf einem Computer mit einem anderen Betriebssystem aktiviert wird (neudeutsch „ge\"mount\"et“ von ). Verwendet man etwa einen USB-Stick an einem Mac, so erstellt der Finder zunächst codice_1-Dateien und einen Papierkorb (codice_5). Löscht nun der Mac-User Dateien auf dem USB-Stick, so werden sie in den Papierkorb verschoben. Da der Papierkorb ein verstecktes Verzeichnis ist (d. h. ein Ordner mit dem Namen codice_5), können die Dateien unter anderen Betriebssystemen in Standard-Konfiguration nicht gesehen werden, belegen aber dennoch Speicherplatz, falls der Papierkorb am Mac nicht geleert wurde. Zusätzlich zum Papierkorb und den codice_1-Dateien erstellt der Finder für jede Datei, auf die er zugreift, eine versteckte dot-Datei gleichen Namens. Kopiert also ein Mac-Anwender eine einzige Datei auf einen leeren USB-Stick, so findet man hinterher drei Dateien und den Papierkorb darauf. Anders als im Netzwerk lässt sich diese Finder-Funktion nicht deaktivieren. Es gibt aber die Möglichkeit, den USB-Stick mit Hilfe von Programmen wie z. B. FinderCleaner so abzumelden, dass dabei alle versteckten Dateien gelöscht werden. Unsichtbare Dateien und Systemverzeichnisse können im Finder bei Bedarf über das Terminal oder z. B. mit TinkerTool sichtbar gemacht werden.\n\nWenn unter macOS auf das FUSE-Dateisystem NTFS-3G zurückgegriffen wird, kann mit der Mount-Option codice_8 den sog. Punkt-Dateien () gleichzeitig das Versteckt-Attribut zugewiesen werden; so werden diese Dateien unter Windows ebenfalls als versteckt erkannt. Die Dateien werden dann nur angezeigt, wenn die Option „Ausgeblendete Dateien, Ordner und Laufwerke anzeigen“ in den Windows-Explorer-Einstellungen aktiviert ist.\n\n"}
{"id": "766040", "url": "https://de.wikipedia.org/wiki?curid=766040", "title": "Dm-crypt", "text": "Dm-crypt\n\ndm-crypt ist ein Kryptographie-Modul des Device Mappers im Linux-Kernel. Man kann mit \"dm-crypt\" Daten mit verschiedenen Algorithmen ver- und entschlüsseln, dies kann auf beliebige Gerätedateien (englisch: Devices) angewandt werden, in den meisten Fällen Partitionen, Festplatten oder logische Laufwerke (LVM). Es wird hier also eine zusätzliche Schicht zwischen (verschlüsselten) (Roh-)Daten und dem Dateisystem aufgebaut. Für den Benutzer geschieht dies vollkommen transparent. \"dm-crypt\" eignet sich so zur Festplattenverschlüsselung (Partitionen, ganze Festplatten, aber auch alle anderen blockorientierten Geräte wie etwa logische Laufwerke (LVM) oder loop devices).\ndm-crypt unterstützt eine Vielzahl von Verschlüsselungsalgorithmen, da es die Crypto API des Linuxkernels nutzt.\n\nEinen anderen Ansatz verfolgt die (transparente) Dateiverschlüsselung, bei der das Dateisystem für die Ver- und Entschlüsselung zuständig ist.\n\n\ndm-crypt unterstützt verschiedene Verschlüsselungs-Algorithmen und -Betriebsmodi. Sie werden in einem speziellen Format angegeben (optionale Teile sind in eckigen Klammern angegeben):\n\nDie einzelnen Felder bedeuten:\n\n\nBeispiele:\n\nEine gängige Erweiterung ist \"LUKS\" („Linux Unified Key Setup“), welche die verschlüsselten Daten um einen Header erweitert, in dem Metadaten sowie bis zu acht Schlüssel gespeichert werden. Vorteile gegenüber „reinem“ dm-crypt sind: ein standardisiertes Format, Informationen über die Art der Verschlüsselung im Header, Vergabe von bis zu acht Schlüsseln sowie die Änderung und Löschung von Schlüsseln ohne Umschreiben der verschlüsselten Daten.\n\nDa der Header, den LUKS in den Container schreibt, eine Klartext-Kennung, den verwendeten Verschlüsselungs- und Hash-Algorithmus und die Größe des Masterschlüssels enthält, sind eine automatische Erkennung und einfache Verwaltung von LUKS-Containern möglich. Es macht die Verschlüsselung aber auch gegenüber Dritten und Angriffsprogrammen erkennbar. Damit wird eine glaubhafte Abstreitbarkeit schwierig bis unmöglich. Der LUKS-Header inkl. Schlüsseldaten verkleinert außerdem den nutzbaren Speicherplatz auf dem Medium um 1028 KiB (Standardeinstellung). Im Gegensatz zu den zentralen Metadaten verschiedener Dateisysteme, wie z. B. dem Superblock bei ext2, werden diese für den Betrieb des Datenträgers wichtigen Daten nicht auf dem Medium verteilt repliziert gespeichert. Wenn sie überschrieben werden oder aufgrund eines Hardwaredefektes nicht mehr ausgelesen werden können, sind die Nutzdaten auf dem Medium ohne ein Backup des Headers (das das Verwaltungsprogramm \"cryptsetup\" ermöglicht) nicht mehr zu entschlüsseln.\n\nEine mit LUKS verschlüsselte Festplattenpartition besitzt folgenden Header (Mehrbytewerte sind dabei im Big-Endian-Format abgespeichert, Klartext-Bezeichner sind dabei mit Nullbytes aufgefüllt, wenn sie kürzer als der vorgesehene Speicherplatz sind):\n\nJeder der acht Keyslots besitzt dabei folgendes Format:\n\nDie nachfolgende Auflistung erhebt keinen Anspruch auf Vollständigkeit. Je nach Einsatzzweck variiert außerdem die Relevanz der einzelnen Eigenschaften, so dass diese Auflistung keine allgemein gültige Wertung von LUKS ermöglicht.\n\n\n\n\nBedingt durch den zusätzlichen Rechenaufwand der Verschlüsselungsalgorithmen können, wie bei jeder in Software ausgeführten Festplattenverschlüsselung, Performanceeinbußen entstehen: der Datendurchsatz sinkt gegenüber unverschlüsselten Datenträgern. Eine Verbesserung kann durch schnellere Prozessoren, Mehrkernprozessoren, der Optimierung der Algorithmen auf die jeweilige Architektur oder einer Implementierung als Hardwareverschlüsselung erreicht werden.\n\nAuf mit dm-crypt verschlüsselte Daten sind teilweise kryptographische Angriffe denkbar:\n\nMit FreeOTFE existierte bis 2013 eine zu LUKS kompatible Implementierung für Microsoft Windows. Der Quellcode ist in DoxBox übergegangen, welches 2015 seit der Version 6.2ß in LibreCrypt umbenannt wurde. LibreCrypt läuft unter Microsoft Windows 10 und der Quellcode kann auf GitHub heruntergeladen werden.\n\nEin vom Funktionsumfang annähernd vergleichbares alternatives Produkt für Windows und Linux ist VeraCrypt.\n\n\n"}
{"id": "766185", "url": "https://de.wikipedia.org/wiki?curid=766185", "title": "Datagram Delivery Protocol", "text": "Datagram Delivery Protocol\n\nDatagram Delivery Protocol (DDP) ist ein Begriff aus der Informatik.\n\nDas Datagram Delivery Protocol ist das Datenübertragungsprotokoll innerhalb von AppleTalk. Das DDP ist vergleichbar mit dem Internet Protocol (IP).\n\nName Binding Protocol (NBP), Routing Table Maintenance Protocol und Zone Information Protocol (ZIP) nutzen DDP (Datagram Delivery Protocol).\n\nDas Protokoll gehört zur Vermittlungsschicht.\n\nDDP-Adressen bestehen aus einer 2-Byte Netzwerknummer (also im Bereich 0…65535) und einer 1-Byte Knoten-ID (also im Bereich von 0…255). Die Adressierung der einzelnen Knoten wird komplett dynamisch ausgehandelt. Ein im lokalen Netzwerksegment vorhandener Router dient dabei als Vergabeeinheit für die Netzwerknummer(n).\n\nIn der ersten Version der AppleTalk Protokollfamilie konnte ein Segment exakt eine Netzwerknummer zugeteilt erhalten, wenn ein Router vorhanden war; bzw. die Nummer 0 ohne Router. Die Adressierung wurde also lediglich durch die Knoten-ID ermöglicht. Die IDs 0 und 255 haben spezielle Bedeutungen, sodass in einem Netzwerksegment 254 Knoten adressierbar waren. Diesen Modus bezeichnet man als Phase1 bzw. nonextended Netzwerk. Eine weitere Limitierung war, dass pro Netzwerksegment lediglich eine \"Zone\" zugewiesen werden konnte.\n\nDiese Limitierungen bedeuteten auf dem ursprünglich als einzigem Übertragungsmedium gedachten LocalTalk keine wesentliche Einschränkung, da ein Betrieb mit mehr als 32 Knoten durch die zunehmenden Datenkollisionen keinen brauchbaren Datendurchsatz mehr ermöglicht.\n\nUm diese Limitierungen zu umgehen, wurde AppleTalk Phase2 geschaffen. Hier ist pro Segment ein ganzer Bereich von Netzwerknummern zuteilbar (\"Cable-Range\"). Die Knoten-IDs 0, 254 und 255 sind reserviert, daher ergibt sich die maximale Knotenanzahl pro Segment aus der Anzahl der zugewiesenen Netzwerknummern multipliziert mit 253. In Phase2-Netzwerken können pro Netzwerksegment mehrere Zonen definiert werden. Zonen können auch über Segmentgrenzen gleiche Namen aufweisen.\n\nDDP kennt zwei Pakettypen:\n\nOptional können auch in nonextended-Netzwerken extended-Header benutzt werden.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "766846", "url": "https://de.wikipedia.org/wiki?curid=766846", "title": "Chroot", "text": "Chroot\n\nchroot steht für „change root“ und ist eine Funktion unter Unix-Systemen, um das Rootverzeichnis zu ändern. Sie wirkt sich nur auf den aktuellen Prozess und seine Kindprozesse aus. „chroot“ selbst kann sich sowohl auf den Systemaufruf chroot(2) als auch auf das Dienstprogramm chroot(8) beziehen.\n\nEin Programm, das auf ein Verzeichnis „gerootet“ wurde und keine offenen Dateideskriptoren in den Bereich außerhalb des virtuellen Root-Verzeichnisses besitzt, kann (bei korrekter Implementierung des Betriebssystemkerns) nicht mehr auf Dateien außerhalb dieses Verzeichnisses zugreifen. chroot bietet somit eine einfache Möglichkeit, nicht vertrauenswürdige, Test- oder sonst wie gefährliche Programme in eine Sandbox zu versetzen. Es ist ein einfacher Jail-Mechanismus, aus dem aber durchaus leicht wieder ausgebrochen werden kann.\n\nchroot wurde nicht als Sicherheitsfeature entworfen, sondern primär für das Aufsetzen virtueller Umgebungen verwendet. Die erste größere bekannte Anwendung war in Network Software Engineering (NSE) auf SunOS im Jahr 1986. Dort war ein Verlassen der Umgebung mit codice_1 möglich und dokumentiert.\n\nIn der Praxis wird „Chrooting“ dadurch erschwert, dass Programme beim Start erwarten, Platz für temporäre Dateien, Konfigurationsdateien, Gerätedateien und Programmbibliotheken an bestimmten festen Orten vorzufinden. Um diese Programme innerhalb des chroot-Verzeichnisses laufen zu lassen, muss das Verzeichnis mit diesen notwendigen Dateien ausgestattet werden.\n\nOb chroot-Umgebungen ein Sicherheitsfeature sind, um einzelne Computerprogramme gegenüber dem Gesamtrechner abzuschotten, hängt stark von der Ansicht der Schöpfer des jeweiligen Betriebssystems ab:\n\n\n\n\nNur der Benutzer root kann chroot ausführen. Dies soll normale Benutzer davon abhalten, ein setuid-Programm innerhalb einer speziell angefertigten Chroot-Umgebung zu platzieren (z. B. mit einer falschen /etc/passwd Datei), welche es dazu bringen würde, Rechte zu vergeben. Es hindert jedoch auch Nicht-Root-Benutzer an der Verwendung des chroot-Mechanismus, um eine eigene Sandbox zu erstellen.\n\n„schroot“ erlaubt Nutzern ein chroot, „openroot“ stellt viele erweiterte Funktionen zur Verfügung wie etwa X11 Forwarding für GUI-Programme.\n\nDer chroot-Mechanismus selbst ist nicht gänzlich sicher. Wenn ein Programm in einer chroot-Umgebung Root-Rechte besitzt, kann es (unter Linux oder Solaris) eine verschachtelte chroot-Umgebung verwenden, um aus der ersten auszubrechen.\n\nDa die meisten Unix-Systeme nicht komplett dateisystemorientiert sind, bleiben potenziell gefährliche Funktionalitäten wie die Kontrolle über Netzwerk und Prozesse durch Systemaufrufe einem gechrooteten Programm verfügbar.\n\nDer chroot-Mechanismus selbst verhängt auch keine Einschränkungen über Ressourcen wie I/O-Bandbreite, Plattenplatz oder CPU-Zeit.\n\n\n"}
{"id": "766993", "url": "https://de.wikipedia.org/wiki?curid=766993", "title": "Deskmodding", "text": "Deskmodding\n\nAls Deskmodding oder Desktop Styling bezeichnet man, analog zum Begriff Case Modding, das Verändern, Verschönern oder Verbessern der grafischen Benutzeroberfläche (GUI) des Betriebssystems eines Computers. Der Arbeitsplatz bekommt so ein individuelles Aussehen, aber auch die Ergonomie kann verbessert werden.\n\nIm Internet haben sich viele Communitys gebildet, die sich mit dem Themengebiet \"Deskmodding / Desktop Styling\" befassen. Hier finden sich unter anderem Anlaufstellen zum Präsentieren der selbst gestalteten Benutzeroberflächen, Foren zu speziellen Modding-Programmen und auch allgemeine Foren mit dem Ziel, Anfängern einen umfassenden Einblick in die Materie zu geben.\n\nDeskmodding beginnt mit dem Ändern des Hintergrundbildes (engl. \"Wallpaper\"). Dies ist laut einer Untersuchung von Microsoft die unter Windows 7 am meisten genutzte Art, den Desktop zu verändern.\n\nDas Verändern des visuellen Stils (engl. \"Visual Style\", als Bezeichnung für ein Set von Grafiken und/oder Darstellungsparametern) ist bei Microsoft Windows von Haus aus nur eingeschränkt möglich, da das Betriebssystem standardmäßig den Einsatz von Visual Styles verweigert, die nicht von Microsoft digital signiert wurden. Diese Beschränkung kann allerdings durch einen Eingriff in das System aufgehoben werden, der entweder manuell oder unterstützt durch spezielle Programme wie den \"UXTheme Patch\" (für Windows XP, Windows Vista, Windows 7, Windows 8, Windows 10) oder TuneUp Utilities (für Windows XP, Windows Vista, Windows 7, Windows 8, Windows 8.1) durchgeführt werden kann.\n\nDer visuelle Stil ist neben dem Wallpaper und den Icons der Grundbaustein beim Verändern des Desktops. So lässt sich mithilfe eines entsprechenden visuellen Stils etwa das Aussehen von anderen Betriebssystemen nachbilden.\n\nDie meisten Desktopumgebungen und Window Manager für Linux bzw. Unix unterstützen von Haus aus (auch fremde) visuelle Stile. Mac OS X bietet keine visuellen Stile an, es gibt aber Programme, die zumindest die Fensterdekoration leicht transparent machen.\n\nDie Symbole (engl. \"Icons\") sind genauso wie der visuelle Stil und das Hintergrundbild ein beliebtes und einfach zu änderndes Element beim Deskmodding.\n\nDies kann unter Windows zum einen durch einen Rechtsklick auf die betreffende Datei und die Auswahl von „Eigenschaften“ über das Feld „Symbol“ erfolgen, über das eine beliebige *.ico-Symboldatei oder Symbole enthaltende *.exe- oder *.dll-Dateien ausgewählt werden können, zum anderen aber auch mit verschiedenen (teilweise Freeware-)Programmen. Diese Programme tauschen die Icons aus, bieten aber auch eine Funktion zum Wiederherstellen der Original-Windows Icons.\nDie Icons können außerdem in einer Windows-Konfigurationsdatei getauscht werden. Diese Datei heißt „Shell32.dll“ und befindet sich je nach Laufwerksbezeichnung unter C:\\Windows\\system32\\. Änderungen in dieser Datei können mit einer Kombination aus drei Programmen (Reshacker, Replacer und Batchmod) vorgenommen werden. Da Änderungen nicht rückgängig gemacht werden können, wird empfohlen, vorher immer eine Sicherungskopie zu erstellen.\n\nUnter Mac OS X können einzelne Icons per „copy and paste“ ersetzt werden. Dazu klickt man das Symbol an, welches man verwenden möchte und klickt auf Ablage → Informationen. In der linken oberen Ecke befindet sich nun das Icon welches man anklickt und über Bearbeiten → kopieren in die Zwischenablage kopiert. Als Nächstes klickt man das zu ändernde Icon an, geht wieder unter Ablage → Informationen, klickt das linke obere Icon an und tauscht es mit Bearbeiten → Einfügen aus.\nKomplette Systemiconsätze können mithilfe von Candybar getauscht werden. Zudem bietet Pixadex eine direkte Verwaltung der Icons ähnlich wie iPhoto für Bilder. Aus Pixadex lassen sich auch Icons direkt in Candybar übernehmen, wodurch man seine eigenen Iconsets erstellen kann.\n\nAuch das Austauschen von Systemklängen (engl. \"Sounds\"), die etwa beim Starten oder Herunterfahren des Rechners oder bei Warnmeldungen wiedergeben werden, gehört in den Bereich des Deskmoddings.\n\n"}
{"id": "769112", "url": "https://de.wikipedia.org/wiki?curid=769112", "title": "Linux Mark Institute", "text": "Linux Mark Institute\n\nDas Linux Mark Institute (LMI) – auch: Linux Trademark Institute – ist eine Organisation mit Sitz in Oregon (USA), die gegründet wurde, um das Markenrecht für den Namen \"Linux\" durchzusetzen und Lizenzen für die die Benutzung des Namens zu vergeben. \n\nIn Abgrenzung zur Linux Foundation, die sich im Wesentlichen um die Koordination der technischen Entwicklung des Linux-Kernels kümmert, dient das \"Linux Trademark Institute\" zur Vermarktung und rechtlichen Absicherung des Namens \"Linux\".\n\nIn den Jahren 1994 und 1995 hatten diverse Personen und Unternehmen in verschiedenen Ländern – teilweise erfolgreich – den Markennamen „Linux“ für sich eintragen lassen. Aufgrund dieser Eintragungen und den damit verbundenen Rechten wollten sie von Firmen, die „Linux“ im Namen führten, Lizenzzahlungen einfordern.\nDa viele Entwickler und Anhänger der Linux-Gemeinschaft damit nicht einverstanden waren, ging Linus Torvalds mit Hilfe von Linux international gegen diese Eintragungen vor und bekam die Marke Linux zugesprochen (in Deutschland und den USA).\nNach Erhalt der Markenrechte verwaltete Linux International diese. Später wurde hierfür das Linux Mark Institute gegründet.\n\nIm Jahr 2000 legte Linus Torvalds die Grundregeln für die Vergabe von den Lizenzen fest. Diese besagen, dass jeder, der ein Produkt oder eine Dienstleistung mit dem Namen Linux anbietet, eine Lizenz dafür besitzen und somit kaufen muss. Ausnahmen bilden nicht-kommerzielle Verwendungen, die eine kostenlose Lizenz erhalten oder keine benötigen. Mittlerweile sind jedoch alle Lizenzen kostenlos und zeitlich unbeschränkt gültig. Die Vergabe einer Lizenz ist an die Bedingung geknüpft, dass die Lizenznehmer Linus Torvalds' Eigentümerschaft an dem Namen anerkennen, diesen Umstand rechtlich nicht anfechten und die Urheberschaft Torvalds an entsprechender Stelle kennzeichnen. \n\n"}
{"id": "769189", "url": "https://de.wikipedia.org/wiki?curid=769189", "title": "Sichere Inter-Netzwerk Architektur", "text": "Sichere Inter-Netzwerk Architektur\n\nDie Sichere Inter-Netzwerk Architektur (SINA, ursprünglich für \"Sichere Netz-Anbindung\") ist eine Hard- und Software-Architektur, die vom deutschen Bundesamt für Sicherheit in der Informationstechnik zusammen mit der secunet Security Networks AG auf Basis des freien Betriebssystems Linux als gehärtetes SINA-Linux zur Verarbeitung von sensiblem Datenmaterial in unsicheren Netzen entwickelt wurde.\n\nDie Einheit aus den drei Komponenten SINA-Client, SINA-Box und SINA-Management zielt darauf ab, die Kommunikation zwischen Behörden oder Unternehmen abzusichern. Die durch SINA erreichte Sicherheitsstufe für SINA-Box S ist Verschlusssache/Vertraulich für deutsche Behörden und für SINA-Box H sogar bis \"Streng Geheim\".\n\nSINA-Boxen dienen der sicheren Übertragung von Daten im behördlichen und militärischen Umfeld. Folglich gibt es auch verschiedene Varianten der SINA-Box. Bei der SINA-Box S wird die Verschlüsselung ausschließlich in der Kryptosoftware Chiasmus implementiert, wohingegen bei der SINA-Box H das dort eingesetzte geheime Verschlüsselungsverfahren Libelle im Kryptoprozessor Pluto auf einer zusätzlichen PCI-Steckkarte implementiert ist.\n\nSINA-Boxen werden u. a. auch zur sicheren Übertragung von Daten eingesetzt, die bei einer Überwachung der Telekommunikation gemäß TKÜV anfallen. Ihre Aufgabe dabei ist nicht die Überwachung von Verbindungen selbst, sondern der Schutz der Verbindung gegen Mithören durch unbefugte Dritte, sobald die Daten von bestimmten Personen an die Strafverfolgungsbehörden übermittelt werden, indem bei einem Internet Service Provider die betroffenen Daten über die SINA-Box durch ein Virtual Private Network weitergeleitet werden.\n\nSie müssen nach dem Telekommunikationsgesetz ( Abs. 5 Satz 2 TKG) bei jedem Internet-Provider in Deutschland installiert sein, der mehr als 100.000 Kunden hat.\n\n"}
{"id": "770129", "url": "https://de.wikipedia.org/wiki?curid=770129", "title": "What The Hack", "text": "What The Hack\n\nWhat The Hack (als Verballhornung des englischen \"What the heck?\", zu Deutsch „Was zum Teufel?“) war die fünfte Veranstaltung einer Folge von Freiluft-Hacker-Konferenzen, die im vierjährlichen Turnus in den Niederlanden stattfinden, in diesem Fall vom 28. bis 31. Juli 2005 in Liempde (Gemeinde Boxtel, Nordbrabant). Ihre Vorgänger waren 1989 die Galactic Hacker Party, 1993 Hacking at the End of the Universe, 1997 Hacking In Progress und 2001 Hackers At Large.\n\nIns Leben gerufen wurden diese Camps von Aktivisten des kleinen Hackermagazins Hack-Tic. Als Veranstalter trat die von diesen mit Privatmitteln gegründete Stiftung \"Stichting HAL2001\" auf. Co-Veranstalter war der Chaos Computer Club, der bislang drei Chaos Communication Camps organisiert hat.\n\nNach zwischenzeitlichen Schwierigkeiten mit der notwendigen Genehmigung konnte die Konferenz wie geplant stattfinden.\n\nZur viertägigen \"What The Hack\" kamen circa 3000 Teilnehmer aus fast allen europäischen Ländern und Übersee.\n\nDas Camp selbst war in thematische „Dörfer“ unterteilt, so dass sich Gleichgesinnte leichter finden konnten, um sich zu den jeweiligen Themen austauschen zu können oder gemeinsam an Projekten zu arbeiten.\n\nEs gab rund 50 \"Villages\", darunter waren:\n\n\nJeder Teilnehmer konnte ein \"Village\" gründen, ohne dass es Vorgaben zu Thema und Größe gab. Daher existierten auch viele kleinere \"Villages\", welche Sammelpunkt für bestimmte Gruppen oder Nationalitäten waren (wie z. B. „British Embassy“, „Belgian Embassy“).\n\nNeben \"Village\"-Workshops gab es ein Vortragsprogramm mit Themen wie zum Beispiel Computersicherheit, Software-Patenten, Informationsfreiheit und Biometrie.\n\nDarüber hinaus waren Bereiche zum Feiern oder Ausspannen aufgebaut worden. So gab es beispielsweise einen sonnengeschützten Bereich mit Hängematten sowie ein „Megabit“-Zelt mit verschiedenen Arcade-Geräten, Flipper und einer Tanzfläche. In den Abendstunden wurde man dort mit bekannten Songs aus den 1970er und 1980er Jahren beschallt.\n\nWährend der Veranstaltung war für eine schnelle Internetanbindung (ein Gigabit über drei Kilometer Glasfaser) und WLAN (IEEE 802.11a/b/g) gesorgt.\n\nAm 1. April 1998 wurde die auf Hacking in Progress erstmals angewandte Methode Peg DHCP, mit IP-Adressen beschriftete Wäscheklammern zu verwenden, als RFC 2322 veröffentlicht.\n\n"}
{"id": "771529", "url": "https://de.wikipedia.org/wiki?curid=771529", "title": "PowerShell", "text": "PowerShell\n\nPowerShell (auch Windows PowerShell und PowerShell Core) ist ein plattformübergreifendes Framework von Microsoft zur Automatisierung, Konfiguration und Verwaltung von Systemen, bestehend aus einem Kommandozeileninterpreter sowie einer Skriptsprache.\n\nPowerShell Core (auch PSCore) basiert auf der .NET Core Common Language Runtime (CoreCLR) und ist seit 2016 als plattformübergreifendes Open-Source-Projekt unter der MIT-Lizenz für Linux, macOS und Windows verfügbar.\n\nWindows PowerShell basiert auf der Common Language Runtime (CLR) des .NET Frameworks und wird mit Windows als Teil des Windows Management Frameworks (WMF) unter einer proprietären Lizenz ausgeliefert.\nSeit 2016 gibt es auch die Windows PowerShell als \"Core Edition\", welche wie PowerShell Core auf .NET Core basiert und als Teil von Windows Nano Server und Windows IoT ausgeliefert wird.\n\nJede veröffentlichte Version von MS-DOS und Microsoft Windows enthielt eine Kommandozeile oder Eingabeaufforderung (siehe auch Kommandozeileninterpreter). Diese sind codice_1 (in DOS bzw. DOS-basierten Betriebssystemen wie Windows 9x) und codice_2 (in Betriebssystemen der Windows-NT-Familie). Die Eingabeaufforderung bietet einige Standardbefehle und ermöglicht das Ausführen weiterer Konsolenanwendungen. Darüber hinaus gibt es eine Skriptsprache, um Aufgaben zu automatisieren (siehe BAT-Datei).\nDa jedoch nicht alle Funktionalitäten der Grafischen Benutzeroberfläche über die Eingabeaufforderung erreichbar sind, können nicht alle Aufgaben automatisiert werden und auch die Skriptsprache ist aufgrund ihrer Einfachheit in der Funktionalität beschränkt.\nMit Windows Server 2003 waren die meisten Funktionen auch per Eingabeaufforderung verfügbar, dennoch bleiben die Limitierung der Skriptsprache sowie Inkonsistenzen in der Bedienung unterschiedlicher Konsolenanwendungen.\n\nMicrosoft versuchte bereits 1998 einige dieser Limitierungen mit der Einführung von Windows Script Host (codice_3) und dessen Unterstützung von JScript und VBScript in Windows 98 zu überwinden. Mit Windows Script Host ist es möglich, alle COM-Komponenten anzusprechen, was die Automatisierung von weiten Teilen des Betriebssystems ermöglicht.\nJedoch richtet sich Windows Script Host eher an Skriptentwickler und eignet sich weniger für die (ad-hoc) Administration, die Dokumentation war nicht leicht zugänglich und es war anfällig für Missbrauch mittels Computerviren.\nDarüber hinaus gibt es in unterschiedlichen Windows-Versionen weitere Kommandozeileninterpreter für spezielle Einsatzgebiete (z. B. netsh zur Netzwerkkonfiguration oder das Windows Management Instrumentation Command-line (WMIC)).\n\nJeffrey Snover – ein Verfechter von Kommandozeilen, der 1999 bei Microsoft anfing – unterstützte zunächst Microsoft Windows Services for UNIX (SFU), um Unix-Tools unter Windows nutzen zu können. Da Windows aber nicht wie Unix dokumentenorientiert, sondern API-orientiert arbeitet, konnte es letztlich nicht umfassend verwaltet werden (z. B. Setzen von Registry-Werten). Snover entwickelte außerdem die Windows Management Instrumentation Command-line (WMIC). 2002 beschrieb er einen neuen Ansatz für die Automatisierung in Windows als Monad Manifest. Darin beschreibt Snover Monad als mächtige, konsistente, erweiterbare und nützliche „Plattform der nächsten Generation“ auf Basis von .NET, mit der der Aufwand für Administratoren reduziert und das Leben von Nicht-Programmierern deutlich vereinfacht werden könne. Dies sollte unter anderem durch folgende Punkte erreicht werden:\nDie Idee von Monad wurde weiterentwickelt. 2003 bei der \"Professional Development Conference\" in Los Angeles wurde erstmals die Monad Shell (MSH) gezeigt. 2005 wurden Beta-Versionen veröffentlicht.\n\n2006 verkündete Microsoft, dass Monad in PowerShell umbenannt werde. Gleichzeitig wurde verkündet, dass Exchange Server 2007 per PowerShell administriert werde und auch die GUI auf PowerShell aufsetze. Noch im gleichen Jahr wurde PowerShell 1.0 veröffentlicht und zum Download angeboten.\n\nDurch die Unterstützung der Exchange- und Datacenter-Verantwortlichen bei Microsoft konnte die PowerShell im Konzern etabliert werden. 2007 wurde PowerShell Teil von Microsofts \"Common Engineering Criteria\" für 2009, einer Liste von Kriterien, der jedes Microsoft Server-Produkt entsprechen soll. Das bedeutete effektiv, dass ab 2009 jedes Server-Produkt von Microsoft PowerShell unterstützen sollte. Dies war der endgültige Durchbruch für PowerShell als zentrale Verwaltungs- und Automatisierungseinheit in Windows.\n\n2008 wurde Windows Server 2008 das erste Betriebssystem, das mit PowerShell ausgeliefert wird – jedoch noch als optionale Komponente.\n\n2009 wurde PowerShell 2.0 als fester Bestandteil von Windows 7 und Windows Server 2008 R2 veröffentlicht. Anfang 2010 stand PowerShell 2.0 auch für ältere Betriebssysteme als optionales Update per Windows Update bereit.\n\n2016 verkündet Microsoft, dass PowerShell unter der MIT-Lizenz Open Source und plattformübergreifend entwickelt wird. Gleichzeitig wird ein GitHub Repository erstellt und die Alpha-Version von PowerShell Version 6.0 für Windows, Linux und macOS veröffentlicht. Der Hersteller begründet diesen Schritt in seiner Cloud-Strategie. In Microsoft Azure werden nicht nur Windows-, sondern auch eine Vielzahl von Linux-Systemen bereitgestellt. Microsoft möchte PowerShell als universelles Werkzeug für alle Administratoren anbieten, ganz gleich welches Betriebssystem oder welche Anwendung administriert werden soll.\n\nPowerShell wurde speziell für die Systemverwaltung und -automatisierung entworfen\n\nSie verbindet die aus Unix-Shells bekannte Philosophie von Pipes und Filtern mit dem Paradigma der objektorientierten Programmierung. Der Benutzer kann wie bisher einfache Befehle an einer Kommandozeile ausführen und miteinander verknüpfen oder aber auch komplexe Skript-Programme mit der eigens dafür entwickelten \"PowerShell Scripting Language\" schreiben.\n\nDie PowerShell erlaubt Zugriff auf WMI-Klassen, COM-Objekte sowie auf das gesamte .NET Framework.\n\nDie PowerShell Engine (auch Shell, PowerShell Class oder PowerShell Runtime) ist der Kommandozeileninterpreter, der die Eingaben verarbeitet und damit das Herz der PowerShell. Die Engine ist eine Sammlung von .NET-Klassen, die in einer DLL (codice_4) gespeichert sind.\n\nDer PowerShell Host ist die Benutzerschnittstelle zur PowerShell Engine. In Windows steht standardmäßig die \"Windows PowerShell\" (auch \"Konsole\") (codice_5) und \"Windows PowerShell ISE\" (\"Integrated Script Environment\", kurz \"ISE\") (codice_6) zur Verfügung. Die \"ISE\" ist ein modernes Eingabefenster mit unter anderem integriertem Skripteditor, Debugger und IntelliSense. Beide Hosts erlauben es auch, herkömmliche Kommandozeilenanwendungen auszuführen, wie beispielsweise codice_7. Jedoch erlaubt die ISE keine Interaktion, wie zum Beispiel bei codice_8. Die ISE kann durch Add-ons erweitert werden; eine bekannte Erweiterung ist \"ISESteroids\".\nAuch Verwaltungskonsolen, wie etwa die Exchange Management Console (EMC) seit Exchange Server 2007, sind ein Host. Darüber hinaus gibt es auf dem Markt weitere Hosts, wie \"PowerShell Plus\" von Idera.\nZwar nutzen alle Hosts dieselbe Engine, doch da die Implementierung unterschiedlich sein kann, ist es auch möglich, dass sich die Hosts untereinander unterschiedlich verhalten.\n\nDie PowerShell Scripting Language ist die Sprache um Skripte für die PowerShell Engine zu entwickeln. Ab Version 2.0 kann die Skriptsprache auch verwendet werden, um eigene Cmdlets zu erstellen. Mit Version 5 wurde die Skriptsprache um Klassen erweitert.\n\nAnders als bei bisher existierenden objektorientierten Skript-Interpretern (BeanShell, Smalltalk, Python Shell) ist die Syntax der PowerShell-Skriptsprache, welche sich unter anderem Anleihen bei Perl, Unix-Shells, SQL und C# nimmt, darauf ausgelegt, auch für den täglichen interaktiven Gebrauch als Shell für administrative Aufgaben wie etwa Dateiverwaltung geeignet zu sein.\n\nCmdlets (gesprochen \"Commandlets\") werden die Befehle in einer PowerShell-Umgebung genannt. Der Begriff soll verdeutlichen, dass es sich um sehr kleine, spezielle Befehle handelt, etwa wie in \"Befehlchen\". Im Gegensatz zu herkömmlichen Befehlen sind Cmdlets keine Stand-Alone-Anwendungen, das heißt, sie können nicht ohne die PowerShell ausgeführt werden. Cmdlets können .NET-Klassen oder PowerShell-Skripte sein und mit wenigen Zeilen Programmcode geschrieben werden. Cmdlets parsen Eingaben in der Regel nicht selber, stellen Fehler nicht selbst dar und geben Ergebnisse unformatiert als Objekt wieder. Die PowerShell-Engine bzw. Standard-Cmdlets nehmen Cmdlet-Entwicklern Arbeit von Standardaufgaben ab und vereinfachen und beschleunigen die Entwicklung. Gleichzeitig wird die Nutzbarkeit und Lesbarkeit von Skripten und Befehlen durch sehr strikte und ausführliche Namenskonventionen gewährleistet.\n\nDie Anzahl der vorinstallierten Cmdlets steigt mit jeder Version. Sie liegt in der Version 5.1 der Legacy-PowerShell bei 489 Cmdlets und 766 mitgelieferten Funktionen und in der Core-Version 6.1 bei 287 Cmdlets und 145 mitgelieferten Funktionen im Kernpaket und 1900 Cmdlets mit zusätzlichen Modulen.\n\nCmdlets folgen dem Namensschema \"Verb-Substantiv\", also beispielsweise codice_9 oder codice_10. Cmdlets sind durch das vorangestellte Verb \"aufgabenorientiert\", sollen genau eine spezifische Aufgabe erfüllen und nur einen Objekttyp ausgeben, wie zum Beispiel codice_11, das alle laufenden Prozesse auflistet und als Objekt vom Typ codice_12 ausgibt. Das Beenden von Prozessen ist eine eigene Aufgabe und ist daher ein eigenes Cmdlet (codice_13). Das Auflisten von Diensten gibt andere Eigenschaften als bei Prozessen aus und hat daher ein anderes Substantiv (codice_14). Die resultierenden Objekte können sodann weiterverarbeitet werden. So ist es möglich, Objekte zu filtern (codice_15, codice_16), zu konvertieren (codice_17) oder auszugeben (codice_18, codice_19).\n\nMicrosoft hat eine Liste mit erlaubten Verben veröffentlicht. Die Einhaltung der Verben wird nicht technisch erzwungen. Sie wird jedoch empfohlen, um die Bedienung zu vereinheitlichen. Die Liste der freigegebenen Verben kann mit Get-Verb abgerufen werden.\n\nFür Cmdlets können Aliase hinterlegt werden, etwa um Kompatibilität zu alten Skripten herzustellen, es Umsteigern von anderen Systemen leichter zu machen oder einfach Befehle mit weniger Buchstaben ausführen zu können. Einige dieser Aliase werden bereits durch Module vordefiniert. So wird beispielsweise das Cmdlet codice_10 verwendet, um das aktuelle Verzeichnis des Hosts zu definieren, es sind jedoch auch die Aliase codice_21, codice_22 und codice_23 verwendbar.\n\nDie folgende Tabelle stellt eine Auswahl von Standard-Cmdlets mit Aliasen und vergleichbaren Befehlen anderer Kommandozeileninterpreter gegenüber.\n\nParameter werden nach dem Cmdlet mit codice_24 angegeben. Ein Parameter kann unterschiedliche Eigenschaften besitzen, so kann er beispielsweise einen Standardwert haben, erforderlich oder positionsbezogen sein oder Werte aus der Pipe akzeptieren. Welche Parameter für ein Cmdlet verfügbar sind und welche Eigenschaften sie besitzen, kann mit codice_9 geprüft werden (z. B. codice_26).\nZusätzlich zu Cmdlet-spezifischen Parametern gibt es auch sogenannte \"Common Parameter\", also allgemein verfügbare Parameter. Dazu zählt codice_27 zur Ausgabe von Detailinformationen, codice_28, um festzulegen, wie bei einem Fehler verfahren werden soll oder codice_29, um sich ausgeben zu lassen, was passieren \"würde\", wenn man den Befehl ausführt, ohne dass tatsächlich Änderungen gemacht werden.\n\nModule sind die bevorzugte Art, um Cmdlets zu veröffentlichen und in PowerShell zu laden. Module bestehen aus dem Cmdlet-Programmcode (als PowerShell-Skript oder .NET-Klasse) und einem \"Manifest\", das den Inhalt des Moduls beschreibt. In PowerShell 1.0 war es ausschließlich über PSSnapins (Snap-ins) möglich, Cmdlets zu laden. Dies wurde jedoch durch die Funktion der Module ersetzt.\n\nPowerShell Provider (auch PSProvider) bieten dem Zugriff auf Daten und Komponenten (Namensräume), die sonst nicht einfach über den Host abrufbar wären, und präsentieren die Daten in einem konsistenten Format als \"Laufwerke\". Das bekannteste Beispiel dafür ist das Dateisystem, welches eine Partition als Laufwerk \"C\" darstellt. Es sind jedoch auch weitere Provider möglich, wie etwa Variablen, die Registry, Active Directory und der Zertifikatsspeicher. In allen Laufwerken (zum Beispiel codice_30, codice_31, codice_32, codice_33 oder codice_34) kann gleichermaßen Navigiert oder Elemente verarbeitet werden (codice_10, codice_36, codice_37, …).\n\nFolgende Liste zeigt einige der Standardprovider in PowerShell.\n\nDie erste Version von PowerShell erscheint 2006 als ein Grundgerüst mit den wichtigsten Funktionen der PowerShell. Insbesondere der Kommandozeileninterpreter ist sehr mächtig, so gibt es bereits die objektbasierte Pipe, eine eigene Skriptsprache und ein Sicherheitsmodel. PowerShell kann sogenannte Namensräume wie das Dateisystem, den Zertifikatsspeicher oder die Registry ansprechen. Cmdlets können ausschließlich in .NET geschrieben werden, Skripte können auf entfernten Computern nur per WMI ausgeführt werden, die verfügbare Konsole ist sehr einfach gehalten.\n\nMit der PowerShell 2.0 wird die Erstellung von neuen Cmdlets drastisch vereinfacht. Cmdlets können erstmals auch als Skripte geschrieben werden. Auch die Hilfe, basierend auf Kommentaren in Cmdlets, wird eingeführt. Cmdlets können als Module zusammen gefasst werden. Außerdem wird die \"PowerShell ISE\" eingeführt, eine leistungsfähige Konsole mit eingebautem Skripteditor. Zur Verwaltung von entfernten Computern wird \"PowerShell Remoting\" eingeführt. Zusätzlich kann PowerShell nun Jobs erstellen, die im Hintergrund abgearbeitet werden.\nPowerShell 3.0 wird ausgereifter und benutzerfreundlicher. \"Workflows\" auf Basis der Windows Workflow Foundation werden eingeführt, womit es mit geringen Mitteln möglich ist, Aufgaben parallel erledigen zu lassen. Erstmals wird CIM und JSON unterstützt. Im System verfügbare Module werden automatisch geladen und stehen dem Administrator somit sofort zur Verfügung.\n\nMit PowerShell 4.0 wird Desired State Configuration (DSC) eingeführt, eine deklarative Sprache zur Konfiguration von Systemen.\n\nIn PowerShell 5.0 wird Desired State Configuration (DSC) deutlich erweitert und verbessert. Außerdem werden Klassen, welche es erlauben Skripte formaler entwickeln zu lassen, und \"Oneget\", eine Paketverwaltung für PowerShell, eingeführt. In der \"PowerShell ISE\" können Skripte nun auch in Sitzungen auf entfernten Computern geändert werden.\n\nPowerShell 5.1 ist die aktuellste und letzte \"volle\" PowerShell Version (im Gegensatz zur reduzierten \"Core\" Version). Sie wird nicht mehr weiterentwickelt im Hinblick auf neue Features, aber mit Sicherheitsupdates und Bugfixes weiterhin gepflegt. PowerShell 5.1 bleibt auch auf Windows Server 2019 die vorinstallierte PowerShell-Version.\n\nPowerShell 5.0 ist die erste Core Version der PowerShell und läuft nur auf Windows Nano Server 2016.\n\nPowerShell 6.0 ist die erste plattformunabhängige, quelloffene Version der PowerShell und läuft auf Windows, macOS and Linux. Sie basiert auf .Net Core 2.0 und hat einen stark eingeschränkten Funktionsumfang gegenüber der .Net-Version 5.1.\n\nIn PowerShell 6.1 basiert auf .Net Core 2.1 und erweitert mit der Unterstützung vieler existierende Module den Funktionsumfang beträchtlich. Auf Windows werden nun 432 Cmdlets und mitgelieferte Funktionen unterstützt (PowerShell 5.1: 1264 Cmdlets und Funktionen), sowie weitere 1468 Cmdlets über externe Module.\n\nAzure PowerShell ist sowohl der Oberbegriff für alle PowerShell-Ressourcen die Microsoft Azure betreffen, als auch der Name für die PowerShell-Version der im Browser ausführbaren Azure Cloud Shell\nDie Azure Cloud Shell lässt sich darüber hinaus auch als Linux BASH konfigurieren.\nAzure PowerShell Az ist die Bezeichnung eines PowerShell-Moduls zur Remote-Administration von Azure-Ressourcen, das lokal ab PowerShell 5.1 und PowerShell Core 6.0 installiert werden kann und der Nachfolger des Azure RM (RessourceManager)-Moduls ist. Im Browser ausgeführt enthält die Azure Cloud Shell alle Cmdlets des Az-Moduls und eine Untermenge der ansonsten in einer lokalen PowerShell verfügbaren Befehle.\n\nAndere Kommandozeileninterpreter wie Cmd.exe, Bash oder DCL sind textbasiert, wohingegen PowerShell mit Objekten arbeitet. Objekte können Eigenschaften (\"Properties\"), Methoden (\"Methods\") und Ereignisse (\"Events\") enthalten.\ncodice_38 zeigt den Typ sowie die Eigenschaften und Methoden von Objekten.\n\nFür die Systemverwaltung/Administration bedeutet dies, dass Ergebnisse von Befehlen nicht mehr als Text (unstrukturierte Daten), sondern Objekte (strukturierte Daten) vorliegen.\n\nPowerShell arbeitet dabei mit einer dynamischen Typisierung. Dabei kommt ein erweitertes Typsystem (englisch: \"\", \"ETS\") zum Einsatz, bei dem .NET-Objekte in der Klasse PSObject gekapselt werden. PSObject stellt hierbei die Basisklasse für PowerShell-Objekte dar. PSObject entspricht damit einer Monade.\n\nPowerShell bietet implizite Typkonvertierung. Dabei kommen Typkonverter zum Einsatz, von denen einige vorgegeben sind:\n\nDie Tatsache, dass PowerShell objektorientiert ist, wirkt sich auch auf die Pipe aus. Die Pipe in textbasierten Kommandozeileninterpretern verbindet die Standardausgabe codice_39 mit der Standardeingabe codice_40 des darauffolgenden Befehls. In PowerShell werden Daten in der Pipe abgelegt und die Pipe muss die Daten an das darauffolgende Cmdlet binden. Ob und wie Eingaben von einer Pipe erlaubt sind, entscheidet jedes Cmdlet für jeden Parameter selbst. Das heißt, die Pipe muss prüfen, ob der Folgebefehl das Binden an einen Parameter erlaubt. Dabei wird zwischen zwei Arten von Bindungen unterschieden: codice_41 oder codice_42. Bei codice_41 muss der Objekttyp in der Pipe mit dem geforderten Objekttyp übereinstimmen. Bei codice_42 muss der Name einer Eigenschaft des Objekts mit dem Namen des Parameter des Folgebefehls übereinstimmen. codice_41 wird codice_42 gegenüber bevorzugt.\n\nArbeitet man mit herkömmlichen Konsolenanwendungen in einem PowerShell Host wird der Standardoutput codice_39 in ein Objekt des Typs codice_48 umgewandelt.\n\nDieses Model ist anfangs komplex, jedoch ergeben sich dadurch in der Praxis auch Vorteile beim Filtern und Verarbeiten von Informationen, wie folgende Beispiele verdeutlichen sollen.\n\nBeispiel 1: Es sollen alle gestoppten Dienste eines entfernten Computers gestartet werden, mit einer Ausgabe welche Dienste gestartet wurden.\n\nGet-Service -ComputerName Server01 | Where-Object -Property Status -EQ -Value Stopped | Start-Service -PassThru | Select-Object -Property MachineName,DisplayName,Status\nBeispiel 2: Auf dem lokalen Computer sollen die 10 Prozesse von Microsoft mit der höchsten Arbeitsspeichernutzung (in Megabyte) in absteigender Reihenfolge in einer CSV-Datei abgespeichert werden.\n\nGet-Process | Where-Object -Property Company -EQ -Value 'Microsoft Corporation' | Sort-Object -Property WorkingSet64 -Descending | Select-Object ProcessName,@{Name='MemoryUsageMB';Expression={[math]::round($PSItem.WorkingSet64/1MB,0)}} | Select-Object -First 10 | ConvertTo-Csv | Out-File -FilePath $env:TEMP\\TopProcesses.csv\nPowerShell Skripte können in einer Skript-Datei zusammengefasst werden. Skript-Dateien enthalten Funktionen mit zugehöriger Beschreibung in einem Kommentarblock, sowie Variablen und Befehle. Skript-Dateien haben die Endung codice_49 und werden mit dem Befehl codice_50, gefolgt vom vollständigen Namen der Datei geladen und verarbeitet.\n\n<#\n.SYNOPSIS\n\n.DESCRIPTION\n\n.NOTES\n\n.EXAMPLE\n\n.EXAMPLE\n\n.EXAMPLE\n\n.EXAMPLE\n\n.EXAMPLE\n\n.EXAMPLE\n\n.ALIAS\n\n.LINK\n\n.INPUTTYPE\n\n.RETURNVALUE\n\n.PARAMETER Name\n\nfunction Write-Hello\n\nSet-Alias greet Write-Hello\nGib die Zeichenfolge \"Hallo Welt!\" auf der Konsole aus:\n\nBeende alle Prozesse, deren Namen mit dem Buchstaben „p“ beginnen:\n\nSuche alle Prozesse, die mehr als 10 MB Hauptspeicher verwenden, und beende sie:\n\nBerechne die Gesamtanzahl der Bytes aller Dateien in einem Verzeichnis:\n\nWarte, bis ein bestimmter Prozess beendet wurde:\n\nÄndere eine Zeichenkette von Kleinbuchstaben in Großbuchstaben:\n\nFüge die Zeichenfolge „ABC“ nach dem ersten Buchstaben des Wortes „string“ ein, um als Ergebnis „sABCtring“ zu erhalten:\n\nLade einen bestimmten RSS-Web-Feed herunter und zeige die Überschriften der acht neuesten Einträge:\n\nLösche die komplette Festplatte ohne Rückfrage, Äquivalent zu rm -rf / unter Unix:\n\n\nUnterstützt werden unter anderem folgende Systeme:\n\nDeutsch\n\nEnglisch\n\nSchnellreferenzen\n\nEinführung\n\nBlogs\n\nCmdlets und Module\n\nPortierungen\n\nBDD und Unit Testing Frameworks\n\nAutomation, Delegation & Management mit PowerShell Skripten\n\nSonstiges\n"}
{"id": "774885", "url": "https://de.wikipedia.org/wiki?curid=774885", "title": "Dig (Software)", "text": "Dig (Software)\n\ndig ist ein Befehl in verschiedenen Betriebssystemen, mit dem die Server des weltweiten Domain Name Systems (DNS) abgefragt werden können.\n\nDer Name ist die Abkürzung von „domain information groper“ (wörtlich aus dem Englischen übersetzt: „Abtaster für Domain-Informationen“, aber auch ein Wortspiel mit \"to dig\", „ausgraben“).\n\n\"dig\" wird als modernere Alternative zu nslookup angesehen. Es verzichtet auf den interaktiven Betriebsmodus von \"nslookup\", bietet aber, insbesondere als Bestandteil der quelloffenen BIND v9.x-Distribution des ISC (BIND-tools), umfassendere Abfragemöglichkeiten für praktisch alle in Frage kommenden DNS-Informationen. Damit ist es ein mächtiges Werkzeug für Test und Fehlersuche an DNS-Installationen, kann aber auch flexibel als Kommandozeilen-Tool und in Shell-Scripts verwendet werden. Ein besonderer Vorzug von (ISC-BIND-)\"dig\" ist dabei die weitestgehend identische Syntax seiner Befehlszeilenparameter beim Einsatz unter allen unterstützten Betriebssystemen.\n\nDas folgende Beispiel zeigt einen Aufruf von \"dig\" für die Domain „de.wikipedia.org“. Die Ausgabe informiert unter anderem darüber, dass die abgefragte Domäne ein anderer Name für „rr.wikimedia.org“ ist, was wiederum für „rr.knams.wikimedia.org“ steht und die IP-Adresse „145.97.39.155“ hat. Es ist außerdem zu erkennen, dass die Abfrage über den Nameserver „212.185.252.73“ lief.\n"}
{"id": "777132", "url": "https://de.wikipedia.org/wiki?curid=777132", "title": "Microsoft Update", "text": "Microsoft Update\n\nDer Microsoft Update Service ist ein Online-Dienst mit angegliederter Webapplikation. Er dient als zentraler Einstiegspunkt für Softwareaktualisierungen () der gängigen Microsoft-Produktpalette, wie Windows, Office, SQL Server und sonstigen Anwendungen. Die \"wichtigen\" Updates enthalten in erster Linie sicherheitskritische Nachbesserungen (engl. patches), oder zeitlich prioritäre Installationen, obwohl sie mitunter auch Lösungen für Programmfehler (engl. bugs) oder erweiterte Funktionalität (sogenannte Upgrades) bereitstellen. Die beiden letztgenannten Aufgaben sind dennoch Schwerpunkte der \"optionalen\" Updates.\n\nDiese Updates können entweder vollautomatisch über den „Automatische Updates“-Dienst oder manuell direkt von \"Microsoft Update\" heruntergeladen und installiert werden (siehe Abschnitt \"Weblinks\" am Ende des Artikels). Darüber hinaus stehen Programmierschnittstellen zur Verfügung, um alternative Aktualisierungsmethoden zu implementieren.\n\nAlle genannten Varianten verwenden den \"Windows Update Agent\" und können kombiniert werden.\n\nAktuell steht Microsoft Update für die Version 6 des Windows-Update-Systems.\nIm Unterschied zum herkömmlichen Windows Update, welches Software-Aktualisierungen für verschiedene Betriebssysteme bereitstellt, weitet diese Website das Angebot auf eine ganze Reihe von verbreiteten Microsoft-Produkte aus. Was über Standardsoftware hinausgeht oder ein älteres Erscheinungsdatum hat, muss wie gehabt von einzelnen Websites, wie beispielsweise „Microsoft Download Center“, „Windows Update-Katalog“ oder „Microsoft Office Online“, besorgt werden.\n\nMicrosoft Update erschien erstmals als Beta (Testversion) im März 2005. Eine weitere Version folgte Ende Juli 2005, gleichzeitig mit der Beta 1 von Windows Vista. Zugrundeliegende Idee war es, Aktualisierungen für Standardsoftware (z. B. Windows und Microsoft Office) an einer zentralen Stelle bereitzustellen, so dass die Kunden sie nicht mehr von verschiedenen Microsoft-Produktwebseiten herunterladen müssen.\nSeit 2006 hat Microsoft außerdem die Verteilung von Anti-Spyware-Definitions-Updates für Windows Defender zu Microsoft-Update hinzugefügt.\n\nWindows Update ist ein webbasierter Software-Aktualisierungsdienst für Windows-Betriebssysteme. Der Dienst offeriert herunterladbare Updates für Systemkomponenten, Service Packs, Patches und freie Upgrades für ausgewählte Windows-Komponenten. Die automatische Erkennungsroutine stellt die Hardware des Clients fest und stellt Gerätetreiber-Updates bereit, falls verfügbar. Optional werden Beta-Versionen einiger Microsoft-Programme angeboten.\n\nDie meisten neuen Patches werden, kurz nachdem sie herausgegeben wurden, auf der Windows-Update-Seite zum Herunterladen bereitgestellt. Alternativ können Updates manuell von der Microsoft-Webseite heruntergeladen werden. Firmen, die in großem Umfang Netzwerk- und Servertechnologie von Microsoft Windows einsetzen, verwenden praktischerweise Windows Server Update Services. Dieses Programm vereinfacht erheblich das Abrufen von Patches und die Verteilung an die einzelnen Client-Geräte.\n\nDie Windows-Update-Webseite erfordert Internet Explorer oder einen Internet-Explorer-Shell-Ersatz, einschließlich ActiveX-Control. Das Programm greift auf die Skriptsprachen Visual Basic Script und JavaScript zurück. Es ist kompatibel mit Netscape Navigator 8.0, der für diese spezielle Website die Internet Explorer Rendering-Engine verwendet. Firefox kann die Windows-Update-Seite nicht direkt nutzen, sondern nur mit Hilfe des Plugins „IETab“, welches den Internet Explorer in einem Firefox-Tab darstellt.\n\nDer Windows-Update-Katalog dagegen kann seit Herbst 2016 nicht mehr nur mit Internet Explorer, sondern mit jedem beliebigen Browser genutzt werden.\n\nDie Windows-Update-Website startete gleichzeitig mit Windows 98. Seitdem durchlief sie eine Reihe von Änderungen.\n\nEnde 2004 präsentierte Microsoft Version 5 von Windows-Updates für Windows XP. Es enthielt Service-Pack 2 (SP2) und etliche größere Veränderungen des Update-Programms. Nutzer ohne Breitband-Internetzugang konnten eine CD des XP-Service-Packs 2 von der Microsoft-Webseite bestellen. Während diese CD seinerzeit kostenlos war, erhob Microsoft später geringe Gebühren für den Versand. Nach abgeschlossener Installation von SP2 empfiehlt es sich, insbesondere für Internet-Benutzer, von der Windows-Update-Website die neuesten Sicherheitsupdates nachzuladen.\n\nSeit Juli 2005 (Windows XP und Windows Server 2003) verwendet Microsoft das Windows-Genuine-Advantage-Programm (WGA), um die Lizenzvereinbarung (EULA) zum Aktualisierungszeitpunkt geltend zu machen. Es überprüft, ob Windows mit einer gültigen Lizenz betrieben wird. Ist dies nicht der Fall, können nicht-kritische Sicherheitsupdates nicht mehr abgerufen werden.\n\nSeit \"Vista\" ist die gesamte Funktionalität der Windows-Update-Webseite in ein neues Systemsteuerungs-Panel integriert. Darüber hinaus ist Windows Update eine Voraussetzung für den Empfang von Definitions-Updates für Vistas Anti-Spyware-Produkt Windows Defender, genauso wie von Updates für den Spam-Mail-Filter von Windows Mail. Über Windows Update wird außerdem zusätzliche Software für Nutzer der \"Ultimate-Edition\" von Windows Vista und Windows 7 angeboten.\n\nBehalten die Nutzer die Standardeinstellungen von Windows Update bei, so haben sie das automatische Update eingeschaltet, führen die aktuelle Version des Internet Explorers aus und haben die neuesten Patches ihres Betriebssystems installiert.\n\nMit Windows 10 hat Microsoft die Update-Funktionalität in der Form umgerüstet, dass der Benutzer nicht mehr jedes einzelne Update zur Installation auswählen oder ablehnen kann. Stattdessen kann er nur noch den Installationszeitpunkt bestimmen. Außerdem veröffentlicht Microsoft für Windows 10 nicht mehr die einzelnen Updates, sondern nur noch ein einziges, monatliches \"Update-Rollup\". Dieses enthält alle Updates, die seit Veröffentlichung des Betriebssystems erschienen sind. Auf diese Art wird der Fragmentierung der unterschiedlichen Versionsstände entgegengewirkt, was den Wartungsaufwand für den Hersteller verringert und so zudem eine höhere Codequalität ermöglicht. Je nach vorigem Update-Stand des Systems ist der Download des aktuellen Update-Rollups kleiner oder größer.\n\nSeit Herbst 2016 verwendet Microsoft nicht mehr nur für Windows 10, sondern auch für Windows 7 und Windows 8 das Modell der monatlichen Rollups. Dies hat für den Nutzer u. a. den Vorteil, dass Updates schneller installiert werden: Nach einer Neuinstallation müssen jetzt nicht mehr hunderte einzelne Updates installiert werden. Stattdessen ist nur noch das aktuelle Rollup zu installieren.\n\nSeinen früheren Ankündigungen entsprechend veröffentlicht Microsoft seit Herbst 2016 neben den monatlichen Rollups, die am zweiten Dienstag jedes Monats erscheinen, ebenfalls sog. \"Rollup-Vorschauen\". Diese am dritten Dienstag des Monats erscheinenden Updates sind als optional eingestufte Beta-Versionen der für den nächsten Monat geplanten Rollups.\n\nOffizielle Seiten von Microsoft\n\nSonstiges\n"}
{"id": "777178", "url": "https://de.wikipedia.org/wiki?curid=777178", "title": "Systemsteuerung", "text": "Systemsteuerung\n\nDie Systemsteuerung ist bei Computern mit dem Betriebssystem Microsoft Windows ein zentrales Hilfsmittel, um die Einstellungen des Computers den persönlichen Bedürfnissen anzupassen. Die Systemsteuerung ist im Explorer ein Ordner (eine Art „virtueller Dateiordner“), der vom Betriebssystem aus einer Reihe von \"Control-Panels\" zusammengestellt wird. Diese sind in Form von \"CPL-Dateien\" im Systemverzeichnis codice_1 oder codice_2 (also codice_3) vorhanden.\n\nIn der Systemsteuerung kann man im Wesentlichen folgende Änderungen machen:\n\nDes Weiteren finden sich noch einige andere Module für die Konfiguration von Geräten wie Maus, Telefon, Scanner und ähnlichem.\n\nDaneben kann sich auch Software von Drittanbietern mit einer passenden CPL-Datei in der Systemsteuerung eintragen, diese Technik verwenden etwa Java, QuickTime oder diverse Virenscanner.\n\nEine CPL-Datei ist inhaltlich nichts anderes als eine umbenannte DLL-Datei. Durch die so veränderte Dateiendung wird es möglich, automatisch alle Dialogboxen in der Übersichtsdarstellung „Systemsteuerung“ des Explorers zusammenzustellen – Näheres siehe unter Technische Details.\n\nDateien mit der Endung CPL könnten der Anwendung codice_19 zugeordnet werden, wodurch zumindest das Standard-Applet der CPL-Datei gestartet würde. Von diesem Konzept aus den 1990ern ist Microsoft in diesem Zusammenhang aber schon seit Langem abgekommen – die CPL-Dateien enthalten eine Reihe weiterer Dialogboxen, welche über diese Zuordnung nicht zugänglich sind. Tatsächlich erfolgt der Aufruf inzwischen in der Form codice_20, wobei es völlig unerheblich ist, ob die Endung der \"PanelDatei\" codice_21 oder codice_22 lautet. Unter rundll32.exe sind auch Beispiele aufgeführt. \n\nCPL-Dateien sollten genauso wie andere DLL- und EXE-Dateien gegen Manipulationen (Malware) geschützt werden, indem nur Administratoren (und noch nicht einmal jedem Administrator) Schreibrecht gestattet wird.\n\nCPL/DLL verfügen alle über den Einstiegspunkt codice_23. Das codice_23 ist von denjenigen zu programmieren, die die CPL-Datei bereitstellen. Dies kann jeder Software- und vor allem Hardware-Anbieter sein, der sein Produkt komfortabel über die Systemsteuerung bedienen lassen möchte. Es erfolgt jeweils eine ganze Serie von Aufrufen des codice_23.\n\nAuf eine bestimmte „Abfrage“-Aufrufserie von codice_23 wird von dieser DLL eine Liste der von ihr bereitgestellten Hauptdialog-Codes mit Verweisnummern auf deren jeweiligen landessprachlichen Klarnamen, Beschreibungsdetails und Icon zurückgegeben (CPLINFO / NEWCPLINFO).\n\ncodice_27 geht (genauso wie heutzutage jedes Mal mit Öffnen von „Systemsteuerung“) durch alle %SystemRoot%\"\\system32\"\\*.CPL durch, sammelt diese Darstellungsinformationen sowie deren zugehörige Datei einschließlich Aufrufcode. Bis etwa XP wurde das Ergebnis auch im Registrierungs-Schlüssel codice_28 in dem Feld codice_29 zur beschleunigten Darstellung gespeichert. Da sich jedoch Icons und Textelemente situationsabhängig mit jedem Aufruf ändern dürfen, wird der Zwischenspeicher auf den heutigen schnellen Maschinen aber (standardmäßig) nicht mehr benutzt, der Schlüssel ist jedoch auf Benutzerebene (codice_30) weiterhin vorhanden.\n\ncodice_31 \"(ohne Parameter)\" öffnet ein spezielles Explorer-Fenster \"Systemsteuerung\" und stellt das (aktualisierte) Gesamtergebnis zur Auswahl bereit. Währenddessen sind alle aufgelisteten CPL-Dateien geöffnet. Beim Schließen der Systemsteuerung werden alle codice_23 erneut mehrfach aufgerufen.\n\nCPL-Dateien, die als „versteckt“ markiert sind, bleiben anscheinend dauerhaft aus der Systemsteuerung ausgeblendet. CPL-Dateien, auf die nicht \"Jeder\" oder \"Benutzer\" Leserechte hat, sondern die lediglich Administratoren lesen und ausführen können, verraten ihren Icon und Titel nicht und erscheinen deshalb für Normalbenutzer nicht in der Systemsteuerung. Das hilft dann der Übersichtlichkeit, wenn normale Benutzer ohnehin die Steuerungsoptionen nicht ändern könnten und die aktuellen Einstellungen auch gar nicht verstehen möchten.\n\nDurch interaktives Anklicken eines Eintrags im Fenster \"Systemsteuerung\" oder codice_33 oder selbstgeschriebenen direkten Aufruf erfolgt die entsprechende Aufrufserie von codice_23 im Modus \"Ausführen\". Als Argument wird codice_23 der Code der gewünschten Hauptdialogbox übergeben, ein weiterer Code mag beispielsweise einen Registerkarten-Tab bezeichnen.\n\nDer landessprachliche Klarname lässt sich anscheinend ebenfalls als Aufrufparameter von codice_36 verwenden.\n\nEs erfolgt der jeweilige Benutzerdialog und daraufhin die entsprechende Speicherung und Umsetzung der gewünschten Einstellungen – wie auch immer.\n\nCONTROL.EXE macht heute nichts anderes mehr als codice_37 (und durchgereichte Parameter) aufzurufen. In den 1990ern waren darin zusätzlich noch einige Standard-CPL wie MAIN, DESK, INTL, PORTS, TIMEDATE mit englischen Schlüsselwörtern der einzelnen Hauptdialoge voreingestellt.\n\nGod Mode (deutsch: „Gott-Modus“; auch als \"God-Mode\" oder \"GodMode\") ist eine dem Computerspieler-Jargon entlehnte inoffizielle Bezeichnung für eine Funktion unter Windows Vista und folgenden Versionen, alle Einstellungen der Systemsteuerung zu sehen. Es ist eine Auflistung aller erreichbaren Funktionen zusätzlich zu den Haupt-Systemelementen. Obwohl seit Windows Vista verfügbar und in den Grundzügen von Microsoft selbst dokumentiert, wurde diese Möglichkeit den meisten Nutzern erst kurz nach Erscheinen von Windows 7 bekannt. Unter Namen, die eine der God-Mode-Variationen enthalten, kursieren diverse Programm, die diese Funktion bereitstellen wollen. Von Microsoft selbst wird die Funktionalität als „Using File System Folders as Junction Points“ (deutsch „Verwendung von Dateisystemordnern als Verknüpfungspunkte“) bezeichnet.\n\nDie Wirkungsweise beruht auf einer speziellen Klassen-ID (, kurz \"CLSID\"). Microsoft veröffentlichte die CLSID’s für die einzelnen Funktionen der Systemsteuerung, nicht jedoch die CLSID für den sogenannten Gott-Modus, so dass sich diese unangekündigt ändern könnte. Bis inklusive Windows 10 ist sie aber identisch und wirksam. Diese CLSID ist codice_38 (ob Groß- oder Kleinschreibung, ist egal).\n\n\n\n"}
{"id": "777336", "url": "https://de.wikipedia.org/wiki?curid=777336", "title": "Personal System/2", "text": "Personal System/2\n\nDas Personal System/2 ist eine von IBM 1987 eingeführte Personal-Computer-Reihe, die das bisherige System des IBM-PCs ablösen sollte.\n\nHerausragende Merkmale waren der 32-Bit-basierende Systembus Microchannel, der den ISA-Bus ablösen sollte, der Intel-80386-Prozessor, neue Möglichkeiten im grafischen Bereich (VGA, 8514/A und XGA) und das „intelligente“ Netzteil. IBM verbaute zum ersten Mal in einem PC-kompatiblen System 3,5″-Diskettenlaufwerke mit 1,44 MB Kapazität (später mit bis zu 2,88 MB), was zunächst für Verwirrung sorgte, weil auf den Vorgängermodellen (IBM-PC) 5,25″-Disketten üblich waren, für die PS/2-Modelle aber kein 5,25″-Laufwerk angeboten wurde. Der ESDI- und SCSI-Festplattenanschluss ersetzten die frühe ST506-Schnittstelle des IBM-PC XT und IBM-PC AT mit deutlich besseren Massenspeichermöglichkeiten.\n\nDas neue System bot eine Menge Vorteile, war aber z. B. nicht kompatibel zu alten Erweiterungskarten oder Disketten und äußerst teuer. Als Betriebssysteme konnten PC-DOS, das neue OS/2 oder gar das Unix-Derivat AIX gewählt werden. IBM ließ sich den \"Microchannel\" sowie weite Teile des Systems patentieren und verlangte hohe Lizenzzahlungen von Herstellern, die selbst ein PS/2-kompatibles System anbieten wollten, um zu verhindern, dass die eigenen Rechner von Nachbauten vom Markt verdrängt würden, wie es beim PC geschehen war.\n\nDie Konkurrenz unter Führung von Compaq rebellierte; dieses Unternehmen hatte bereits ein System auf Basis des Intel 386 im Angebot, das jedoch mit den alten ISA-Steckplätzen versehen war. Das PS/2 konnten sich wegen nur mäßigen Erfolgs nicht durchsetzen. Die nötige Geschwindigkeit für den IO-Bereich wurde bei den PC-Nachbauten durch neue, offene Schnittstellen (EISA, VESA Local Bus und später PCI) nachgerüstet. Der Microchannel verschwand 1995 mit den letzten PS/2 vollständig vom Markt.\n\nDer überschätzte Erfolg hatte Konsequenzen, IBM hatte seine Marktmacht im PC-Bereich verloren, fortan galt bedingungslose Abwärtskompatibilität auch auf Hardware-Ebene als oberstes Gebot im PC-Bereich. Nach den PS/2 Modellen brachte IBM die Reihe PS/ValuePoint auf den Markt – diese Rechner waren u. a. mit AT-Bus deutlich kompatibler zu den ursprünglichen IBM-PC Modellen. Letztendlich gab IBM 2005 das Segment aber an Lenovo ab.\n\nAn die PS/2-Systeme erinnern heute noch die gleichnamigen PS/2-Anschlüsse für Eingabegeräte, die aus Gründen der Kompatibilität lange Zeit auf jeder Hauptplatine zu finden waren, aber mittlerweile durch USB verdrängt wurden. Ebenso die Bezeichnung eines Arbeitsspeicher-Sockels „PS/2 SIMM“, welcher in den meisten 486er- und Pentium-Systemen Verwendung fand, sowie der lange Zeit übliche (durch HDMI und DVI abgelöste) VGA-Anschluss für den Monitor und das 3,5″-Diskettenformat.\n\n\n\n\n\n\n"}
{"id": "777358", "url": "https://de.wikipedia.org/wiki?curid=777358", "title": "CrossCrypt", "text": "CrossCrypt\n\nCrossCrypt ist eine Freie Software (unter GPL) zur transparenten Ver- und Entschlüsselung von Festplatten und anderen Datenträgern für Microsoft Windows XP und 2000 mit Kompatibilität zu Linux. CrossCrypt bietet ähnlich wie TrueCrypt die Verschlüsselung sogenannter \"Container-Dateien\", welche unter Windows als virtuelle Laufwerke eingebunden werden können. Die Software unterstützt die Algorithmen Twofish sowie AES in den Varianten AES-128, AES-192 und AES-256.\n\nCrossCrypt ist kompatibel zu loop-aes und damit neben FreeOTFE und TrueCrypt (ab Version 4) eines der wenigen Verschlüsselungsprogramme, die unter Linux und Windows laufen.\n\nCrossCrypt besitzt im Vergleich zu FreeOTFE und TrueCrypt folgende Vor- und Nachteile:\n\n"}
{"id": "777530", "url": "https://de.wikipedia.org/wiki?curid=777530", "title": "Rasterzeileninterrupt", "text": "Rasterzeileninterrupt\n\nDer Rasterzeileninterrupt ist ein Hardwareinterrupt, der ausgelöst wird, wenn der Videochip eines Computers die Darstellung einer bestimmten Zeile auf dem Bildschirm beginnt.\n\nDie Rasterzeileninterruptprogrammierung wurde intensiv auf den Heimcomputern der 1980er Jahre angewendet. Sie wurde bald zu einem festen Bestandteil vieler für 8-Bit-Heimcomputer geschriebener Computerspiele.\n\nDer Bildaufbau auf Bildschirmen findet zeilenweise statt. Der Videochip, der die Bilddaten ausgibt, enthält dazu unter anderem einen Zähler für die momentan auszugebende Bildzeile, die sogenannte Rasterzeile. Unterstützt der Videochip einen Rasterzeileninterrupt, kann vom Programm eine Zeile festgelegt werden, bei der dieser Interrupt ausgelöst werden soll. Erreicht der Rasterzeilenzähler diesen Wert, signalisiert der Videochip eine Interruptanforderung an den Prozessor. Dieser unterbricht das laufende Programm und führt eine Unterbrechungsroutine (Interrupt Handler) aus. Am Ende der Unterbrechungsroutine fährt der Prozessor mit dem unterbrochenen Programm fort.\n\nDer Rasterzeileninterrupt vereinfacht es, Programmteile beim Erreichen einer bestimmten Bildschirmzeile auszuführen, also mit dem Bildaufbau zu synchronisieren. Das Programm muss nicht aktiv auf das Erreichen dieser Position warten, sondern wird durch die vom Videochip signalisierte Unterbrechungsanforderung (Interrupt) informiert. So lassen sich beispielsweise sehr einfach während des Bildaufbaus Grafikmodi wechseln (Split Screen) oder normalerweise für das Gesamtbild geltende Farben umschalten.\nAuch die gleichzeitige Darstellung einer größeren Anzahl hardwaregenerierter Sprites als ursprünglich vom System vorgesehen durch Änderung der Bildschirmpositionen nicht mehr benötigter Sprites aus dem bereits überstrichenen in den noch vom Elektronenstrahl zu zeichnenden Bildschirmbereich wird durch den Rasterinterrupt vereinfacht (z. B. bei Spielen für den Commodore 64).\n\nEin bekannter klassischer Grafikchip, bei dem der Hersteller die Mechanismen für einen Rasterzeileninterrupt eingebaut hat, ist der VIC-II (MOS Technology 6569 und ähnliche Varianten). Dieser wurde unter anderem im Commodore 64 verbaut. Zeitgenössische 8-Bit-Hardware wie der Atari 800, Homecomputer nach dem MSX-Standard oder der Schneider CPC beherrschten ihn ebenfalls. Aber auch jüngere Hardware unterstützt Rasterzeileninterrupts, wie z. B. das Megadrive, das Super Nintendo, der Game Boy Advance oder der GameCube, wo die Technik oft in Spielen für diverse optische Effekte eingesetzt wurde.\n\n\n"}
{"id": "779354", "url": "https://de.wikipedia.org/wiki?curid=779354", "title": "Puppy Linux", "text": "Puppy Linux\n\nPuppy Linux ist eine platzsparende und schnelle Linux-Distribution, die u. a. direkt von einer Live-CD betrieben werden kann. Aus Quelltext kompiliert, basiert Puppy auf keiner anderen Linux-Distribution.\nDer Name leitet sich von dem Chihuahua \"Puppy\" (dt.: Welpe) des australischen Projektgründers Barry Kauler ab.\n\nEin Ziel des Betriebssystems ist es, auch von Benutzern ohne Linuxkenntnisse sofort genutzt werden zu können. Die Entwickler versuchen, dies durch eine unkomplizierte und benutzerfreundliche Bedienung, grafische Einrichtungs-Assistenten sowie eine breite Hardwareunterstützung zu erreichen.\n\nPuppy Linux wurde im Jahr 2002 von dem Australier Barry Kauler entwickelt. Die Version 0.1 veröffentlichte er am 18. Juni 2003. Seitdem wurde das System mit zunehmender Unterstützung und Geschwindigkeit weiter entwickelt.\n\nIm Oktober 2008 zog sich Barry Kauler als Leiter der Entwicklung von Puppy Linux zurück und konzentrierte sich mehr auf verwandte Projekte wie \"Woof\", mit dem distributions-fremde Pakete in das Paketformat \"PET\" (Puppy’s Extra Treats) für Puppy Linux konvertiert werden können. Puppy 4.1.2 war die bis dahin letzte Version von ihm. Die Community pflegte die Distribution weiter zu Puppy 4.2 „Deep Thought“, erschienen im März 2009.\n\nErneut koordiniert von Barry Kauler folgte im September 2009 Puppy Linux 4.3, aufbauend auf Woof. Auf Basis von Woof führt Barry Kauler zwei Entwicklungslinien von Puppy mit \"Quirky\" und \"Wary\" weiter.\n\nDie Entwicklung von Puppy aus der Community heraus hat zu \"Lucid Puppy\" und \"Slacko Puppy\" geführt, mit zusätzlichen Ideen und Programmen.\n\nDie etwa 205–330 MB (je nach Software-Auswahl) große Distribution startet schnell, auch wenn sie nicht auf der Festplatte eines Computers installiert wurde. Bei Installation auf Festplatte verbraucht es nur wenig Speicherplatz. Dies ist besonders auf Netbooks mit Solid-State-Festplatten von Vorteil. Es stellt geringe Anforderungen an die Systemressourcen, sodass es auch auf alten Geräten flüssig läuft. Das System wird komplett in den Arbeitsspeicher geladen und erreicht dadurch eine hohe Betriebsgeschwindigkeit. Der Live-Betrieb ohne Installation ist je nach BIOS von fast allen Wechselmedien möglich.\nDie Nutzung durch Anwender, die an das Betriebssystem Windows gewöhnt sind, ist ohne große Schwierigkeiten zu erlernen.\n\nIm Gegensatz zu den meisten Linuxdistributionen, die als Multiuser-System ausgelegt sind, startet Puppy automatisch den einzig verfügbaren User Root, der ohne Passworteingabe sämtliche Systemveränderungen vornehmen kann. Den PC und das Internet als Rootuser mit allen Rechten zu nutzen birgt Gefahren, daher wurde dieses Konzept kritisiert.\n\nSeit Wary 5.1.3 gibt es einige auf Puppy basierende Distributionen, die es experimentell ermöglichen, Puppy auch ohne Superuser-Rechte zu nutzen.\n\n\n\n\nDarüber hinaus gibt es so genannte Puplets (inoffizielle Versionen) von Mitgliedern der Community. Einflüsse zwischen den unterschiedlichen Projekten finden sich in neueren Entwicklungen wieder.\n\nDie aktuellen (2012) Versionen von Puppy setzen ein Minimum von 128 MB Arbeitsspeicher voraus, um das Betriebssystem vollständig und ohne swap-Partition in den Arbeitsspeicher zu laden. So wird die Distribution mit ihren diversen Programmen angemessen schnell ausgeführt. Empfohlen werden heute grundsätzlich 256 MB RAM plus eine Swap-Partition von 512 MB.\nWill man auch auf älterer Hardware ein Puppy Linux mit aktuellen Programmen einsetzen, ist Puppy 2.14x besonders geeignet.\n\nInstallieren lässt sich Puppy auf verschiedene Arten:\n\nAls Fenstermanager verwendet Puppy JWM (Joe’s Window Manager) und auch Openbox.\n\nPuppy umfasst trotz der geringen Größe eine umfangreiche Ausstattung an freien Programmen für viele Anwendungen. Kompakte Programme mit geringen Leistungsanforderungen bestimmen die Auswahl. Codecs und Browser-Plug-ins sind integriert.\n\nROX ist der Standard Dateimanager.\nWeitere Anwendungen sind unter anderem das Textverarbeitungsprogramm AbiWord, Gnumeric als Tabellenkalkulation so wie Sylpheed als E-Mail-Programm.\n\nNicht enthaltene populäre Programme können leicht nachinstalliert werden.\nAls Webbrowser bietet Lucid Puppy in einer Auswahlmaske Firefox, SeaMonkey, Chrome und Opera an. SeaMonkey ist die Browser-Suite in Wary.\n\n„Puplets“ sind von den Mitgliedern der Puppy Community remasterte und für verschiedene Zwecke zugeschnittene Puppy Varianten.\n\n\n\n\n\n\n\nBeim ersten Start von Puppy Linux kann man verschiedene Einstellungen vornehmen: Sprache, Tastatur-Layout, Grafik-Einstellungen, wie zum Beispiel Bildschirm-Auflösung und Farbtiefe.\n\nDanach findet der Nutzer einen Desktop mit den Icons für die wichtigsten Programme vor. Puppy Linux wird ähnlich wie Microsoft Windows über das „Start-Menü“ (links unten, siehe Bild) bedient. Umsteiger von Windows müssen sich erst daran gewöhnen, dass man in der Standardeinstellung nur einmal mit der linken Maustaste auf die Icons klickt. Dies kann aber auch im ROX-Filemanager geändert werden. Das Aussehen der Programme erinnert stark an die Programme des Gnome-Desktops.\n\nBeim Beenden von Puppy Linux hat man die Option, alle Einstellungen, Dokumente und Tabellen, E-Mails, Browser-Einstellungen usw. abzuspeichern. Puppy Linux legt dazu eine, von Windows aus gesehen, einzige große Datei auf einer Windows FAT32-Partition an, in der alles gespeichert wird. Als Speichermedium kann sowohl die Festplatte, eine CD-RW oder auch eine Speicherkarte (zum Beispiel eine Secure-Digital-Speicherkarte mit USB-Cardreader oder ähnliches) dienen. Damit erhält man beim nächsten Start wieder genau dieselbe Umgebung mit allen Einstellungen und Dateien, wie beim vorhergehenden Beenden. Die Daten werden dabei in einer Datei \"pup_save.2fs\" (oder auch \"pup_save.3fs\") abgelegt, die intern eine ext-Dateisystem-Struktur hat und beim Starten von Puppy Linux entsprechend automatisch erkannt und in das Linux-Dateisystem gemountet wird.\n\n\n"}
{"id": "779397", "url": "https://de.wikipedia.org/wiki?curid=779397", "title": "MMS-Protokoll", "text": "MMS-Protokoll\n\nDas MMS-Protokoll (Microsoft Media Server Protocol) ist ein von Microsoft entwickeltes Protokoll der Anwendungsschicht, das der Übertragung von Multimedia-Streams dient.\n\nDie Abkürzung MMS steht für \"Microsoft Media Server\". Das Protokoll wurde also nach der zugehörigen Server-Software benannt, deren vollständige Bezeichnung \"Microsoft Windows Media Server\" lautet und Teil des Softwarepakets Windows Media Services ist (früher \"Microsoft NetShow Services\"). Als Client-Software kommt typischerweise der Windows Media Player zum Einsatz. Zusammenfassend spricht man von den sogenannten \"Windows Media Technologies\". Der Computer, auf dem die Server-Software läuft, heißt Streaming Media Server.\n\nDas MMS-Protokoll wird von einem Client gestartet, der einen URL mit dem Präfix „mms://“ verarbeiten will. Er baut zunächst eine TCP-Verbindung zu Port 1755 des Servers auf, um seine IP-Adresse und einen selbst gewählten UDP-Port zu übermitteln. Daraufhin erzeugt der Server einen UDP-Socket und verbindet ihn mit dem gewünschten Port des Clients. Die Übertragung der Multimediadaten erfolgt dann über diese UDP-Verbindung (MMSU), während die TCP-Verbindung für Steuerungsbefehle genutzt wird. Es kann jedoch vorkommen, dass die UDP-Verbindung von einer Firewall verhindert wird. In diesem Fall kann der Client die Übertragung der Multimediadaten ebenfalls über die für diesen Zweck jedoch weniger gut geeignete TCP-Verbindung anfordern (MMST). Wenn auch das fehlschlägt, muss die dritte Möglichkeit genutzt werden, bei der die Kommunikation über HTTP erfolgt. Dieser Vorgang, bei dem die passende Protokollvariante ausgehandelt wird, bezeichnet man auch als \"„protocol rollover“\". Aufgrund der besseren Echtzeiteigenschaften wird die MMSU-Variante dabei stets bevorzugt, hier besteht auch die Möglichkeit, verlorene Pakete erneut anzufordern, falls genug Zeit vorhanden ist. Die ersten MMS-Pakete bei Sitzungsaufbau enthalten an Offset=12 die Kennung „MMS“.\n\n\nDer Client einigt sich mit dem Server zunächst auf ein Qualitätsniveau der Übertragung, das zur vorhandenen Bandbreite passt. Voraussetzung dafür ist die Verwendung einer ASF-Datei, die den Stream zu unterschiedlichen Datenraten kodiert bereithält. Falls später die Bandbreite nachlässt, kann der Client zusätzlich den Stream dynamisch ausdünnen, wobei eine kontinuierliche Anpassung von \"„full frames“\" bis hinunter zu \"„key-frame only“\" möglich ist. Bei solcherart reduzierter Bandbreite hat dann stets der Ton die Priorität vor dem Bild. Verbessern sich die Bandbreitenbedingungen anschließend, so kann die Video-Bitrate wieder bis zum Optimum aufgestockt werden. Diese Möglichkeiten, den Datenstrom an eine veränderliche Bandbreite anzupassen, werden unter dem Begriff \"„Smart Streaming“\" zusammengefasst.\n\nWird der Server im Unicast-Modus betrieben, so ist für jeden Empfänger eine eigene Verbindung nötig, und die Daten müssen jedes Mal erneut gesendet werden. Der Client kann den Datenstrom dann \"„on-demand“\" empfangen, hat also die Kontrolle über den Abspielvorgang (zum Beispiel Start, Stopp, Pause oder Suchlauf), vergleichbar mit einem Videorekorder. Multicasting dagegen bedeutet, dass viele Empfänger gleichzeitig den einmal gesendeten Datenstrom empfangen können. Dazu muss das Netzwerk multicast-fähig sein und der Server im Multicast-Modus betrieben werden. Man spricht in diesem Fall auch von Broadcast-Empfang, vergleichbar mit dem Empfang eines Fernsehprogramms, das heißt, der Empfänger kann den Abspielvorgang nicht kontrollieren. Der Vorteil besteht dann in einer erheblich geringeren Beanspruchung des Netzwerks.\n\nBeim HTTP-Streaming kann das URL-Präfix zwar ebenfalls „mms://“ lauten, es handelt sich aber im Gegensatz zu dem in diesem Artikel beschriebenen MMS-Streaming um eine grundsätzlich andere Technologie, bei der man anstelle der \"„Windows Media technologies“\" HTTP bzw. einen HTTP-Server verwendet, der um einige Funktionen erweitert wurde, um bessere Echtzeiteigenschaften zu erhalten. Dies ist aber nicht zu verwechseln mit der oben beschriebenen Methode des MMS-Streamings über HTTP.\n\nNormalerweise ist das dauerhafte Speichern von den über das MMS-Protokoll empfangenen Multimedia-Dateien nicht möglich und auch nicht erwünscht, daher wurde die genaue Spezifikation des MMS-Protokolls auch lange Zeit von Microsoft geheim gehalten (Veröffentlichung der Protokollspezifikation am 8. Februar 2008, Newseintrag von SDP Multimedia vom 27. März 2008). Dennoch existieren neben dem \"SDP Downloader\" und einigen anderen wie beispielsweise Nettransport oder auch dem Player VLC diverse freie Programme, mit denen das Speichern von Streams möglich ist.\n\n\n"}
{"id": "783578", "url": "https://de.wikipedia.org/wiki?curid=783578", "title": "Microsoft Baseline Security Analyzer", "text": "Microsoft Baseline Security Analyzer\n\nMBSA (Microsoft Baseline Security Analyzer) ist ein kostenloses Werkzeug, das Windows auf Sicherheitslücken untersucht. Es soll etwa typische sicherheitsrelevante Fehlkonfigurationen in Microsoft-Produkten und Windows ausfindig machen. Außerdem überprüft es, ob alle aktuellen Sicherheitsupdates vorhanden sind. Das Programm kann ohne Gültigkeitsprüfung bei Microsoft heruntergeladen werden.\n\nMBSA 2.0 läuft auf den Betriebssystemen Windows 2000, Windows XP und den Windows Server 2003. Er analysiert den Webserver IIS in der Version 5.0 und 6.0, den SQL-Server in Version 7.0 und 2000, den Internet Explorer ab Version 5.01 und Microsoft Office 2000, 2002 und 2003. Version 2.1 unterstützt auch Windows Vista, Windows Server 2008 und alle x64-Versionen. Es ist auch ein Befehlszeileninterpreter integriert.\nEs werden Sicherheitsupdates für Microsoft Office XP überprüft und Bewertungen angegeben, wie schwerwiegend ermittelte Sicherheitsprobleme sind.\nWeitere Bereiche, die von MBSA überprüft werden können, sind:\n\n\nDer MBSA ausführende Benutzer benötigt auf dem zu scannenden Computer lokale Administratorenrechte, außerdem müssen die administrativen Freigaben aktiviert sein.\n\nMBSA 2.1 unterstützt zusätzlich Windows Vista sowie Windows Server 2008 und die 64-Bit-Versionen der Betriebssysteme. Mit der Version wurde die Unterstützung für Windows XP und SQL Server 2005 erhöht. Des Weiteren gibt es zwei neue Befehlszeilenoptionen für die Ausgabe von Berichten in einem angegebenen Ordner und für die Nutzung der Grafikoberfläche statt der Eingabeaufforderung während der Prüfung. MBSA ist nun mit WSUS 2.0 und 3.0 kompatibel. Auch diese MBSA-Version kann nur von Administratoren bei aktivierten administrativen Freigaben genutzt werden.\n\nMBSA 2.2 unterstützt auch Windows 7 32Bit als auch 64Bit Versionen. Auch sind Verbesserungen von Microsoft eingearbeitet worden.\n\nMBSA 2.3 unterstützt weiterhin die schon genannten älteren Betriebssysteme von Microsoft ab Windows XP und zudem Windows 8 sowie 8.1, Windows Server 2008 R2, Windows Server 2012 sowie Server 2012 R2; unter Windows 10 ist es nicht mehr lauffähig. Des Weiteren sind Verbesserungen von Microsoft eingearbeitet worden.\n\n"}
{"id": "784713", "url": "https://de.wikipedia.org/wiki?curid=784713", "title": "Apple Remote Desktop", "text": "Apple Remote Desktop\n\nApple Remote Desktop (ARD) ist eine Software von Apple zur Verwaltung ferner Computer mit dem Betriebssystem macOS über Rechnernetze hinweg. \n\nARD unterstützt Automatisierung und Inventarisierung mehrerer Computer zugleich, sowie Softwareverteilung und Protokollierung des Benutzerverhaltens. \n\nIm Unterschied zur Secure Shell von macOS weist ARD eine grafische Benutzeroberfläche auf. Das VNC von macOS ist in ARD integriert und in dessen Version 3 um die Möglichkeit ergänzt, den fernen Benutzern den Einblick zu verwehren. Mittels ARD lassen sich alle VNC-fähigen Computer (Virtual Network Computing), einschließlich Windows, Linux und UNIX-Systemen steuern.\n\nIm Gegensatz zur früheren Box-Version, wird die Software kostenpflichtig über den Mac App Store vertrieben , mit dieser Version lassen sich unbegrenzt viele verwaltete Systeme mit einer beliebigen Anzahl von Client-Computern verwalten, eine Gebühr pro verwaltetem Client-Computer gibt es nicht. Sie stützt sich auf SQLite.\n"}
{"id": "785267", "url": "https://de.wikipedia.org/wiki?curid=785267", "title": "Raycasting", "text": "Raycasting\n\nRaycasting (in englischer Schreibweise meist \"ray casting\") ist ein Begriff aus der Computergrafik. Er bezeichnet Techniken zur schnellen Darstellung (Rendern) einer dreidimensionalen Szene, wird aber inzwischen hauptsächlich im Kontext der Volumenvisualisierung verwendet. Die genaue Definition des Begriffs variiert kontextabhängig.\n\nRaycasting bezeichnet eine Methode, skalare Funktionen in einem dreidimensionalen Volumen, die in vielen wissenschaftlichen Anwendungen auftreten, zu visualisieren. Im medizinischen Bereich sind Beispiele hierfür: Computertomographie (CT), Magnetresonanztomographie (MRT) oder Positronen-Emissions-Tomographie (PET). Im Bereich der numerischen Simulation, bei einer Finite-Elemente-Methode (FEM) für Computational Fluid Dynamics (CFD), bei der das Strömungsverhalten von Gasen und Flüssigkeiten berechnet wird. Die hier gewonnenen skalaren Daten, zum Beispiel die Dichte oder Temperatur, können mit verschiedenen Verfahren visualisiert werden, hierzu zählt auch das Raycasting. Hierbei unterscheidet man zwischen direkten und indirekten Verfahren. Indirekte Verfahren visualisieren das Volumen mit Hilfe einer polygonalen Zwischenrepräsentation. Marching Cubes zählt zu diesen indirekten Verfahren. Direkte Verfahren visualisieren das Volumen ohne die Erzeugung solcher Zwischendaten. Zu diesen Verfahren zählen Ray Casting und Splatting. Weiter unterscheidet man noch zwischen bildraumorientierten Verfahren (Image-Order) und objektraumorientierten Verfahren (Object-Order).\n\nGrundlegende Idee ist, wie Volumendaten mit Hilfe des Raycasting-Verfahrens visualisiert werden können. Die theoretische Grundlage ist die Volumen-Rendering-Gleichung, eine Zusammensetzung aus Emission und Absorption. Raycasting löst (approximiert) dieses Problem.\nRaycasting schickt für jedes Pixel des Betrachters (des zu berechnenden Bilds) einen Sehstrahl (Primärstrahlen) durch das Volumen. Der Strahl wird innerhalb des Volumens verfolgt und die Farb- und Opazitätswerte in regelmäßigen Abständen an den Abtast-Punkten auf dem Strahl bestimmt. Es wird ebenfalls die Schattierung an allen Abtastpunkten, für die Farbwerte, berechnet. Der so erhaltene Vektor, für den Sehstrahl, enthält die geordneten Abtast-Werte (Farb-, Opazitätswerte), wobei die Farbwerte dem Quellterm und die Opazitätswerte dem Extinktionskoeffizienten entsprechen. In einem letzten Schritt, dem Compositing, werden dann die Farb- und Opazitätswerte kombiniert und das aus dem Sehstrahl resultierende Pixel in der Bildebene errechnet.\n\nRaycasting bezeichnet oftmals eine einfache Form des Raytracings, eines bekannten Renderverfahrens. Die dreidimensionale Szene wird entsprechend festgelegter Vorgaben wie Betrachterstandpunkt und Perspektive regelmäßig abgetastet, sodass eine zweidimensionale Abbildung eines Ausschnitts entsteht. Im Gegensatz zu erweiterten Raytracing-Varianten ist das Abtasten eines Strahls mit dem Aufeinandertreffen von Strahl und Objekt beendet, es findet also lediglich eine Verdeckungsberechnung statt. Die an diesem Schnittpunkt festgestellte Farbe bildet den Bildpunktfarbwert. Spiegelungen, Brechungen und Transmissionen des Objekts werden nicht beachtet. Diese Technik ermöglicht eine sehr schnelle Vorschau auf eine Szene.\n\nGelegentlich wird Raycasting auch synonym zu Raytracing verwendet.\n\nIn der Computerspielentwicklung bezeichnet der Begriff \"Raycasting\" das auf einer zweidimensionalen Karte basierte Berechnen einer Pseudo-3D-Ansicht. Auf Basis der Entfernung zu einem Objekt, den ein „Sichtstrahl“ trifft, wird zum einen die Objektfarbe vertikal zentriert dargestellt und zum anderen der Anteil an Decke oder Boden der entsprechenden Pixel-Spalte berechnet. Im Gegensatz zur normalen Raytracing-Technik wird hier nur eine einzelne Bildzeile abgetastet, um das gesamte Bild zu berechnen; die Verdeckungsberechnung findet also nur in einer Ebene und nicht im Raum statt. Diese Art des Raycasting findet zum Beispiel im Computerspiel Wolfenstein 3D Anwendung.\nDa diese Technik keinem echten 3D entspricht, unterliegt sie diversen Einschränkungen: Es können keine dreidimensionalen Objekte wie Personen und Gegenstände dargestellt werden, Boden und Decke sind immer gleich hoch und Schrägen sind nicht möglich.\n\nEs wurden diverse Umgehungslösungen gefunden, die den Eindruck der Dreidimensionalität herstellen sollen. So werden zweidimensionale Grafiken, so genannte Sprites, für beliebige Objekte verwendet, die skaliert in das berechnete Bild eingefügt werden. Diese wurden bei fortgeschrittenen Spielen wie Duke Nukem 3D winkelabhängig ausgewählt, sodass ein Objekt von vorne anders aussieht als von hinten.\n\nTexturen für Wände, Böden und Himmel wurden eingebaut, die dreidimensionale Strukturen abbilden. Himmel bzw. Decke und Boden wurden für Abschnitte einer Karte abhängig einstellbar gemacht, sodass Treppen, Durchgänge und Ähnliches möglich wurden (so etwa in Doom). Bei Duke Nukem 3D wurden schließlich sogar schräge Böden bzw. Himmel bzw. Decken ermöglicht und eine vertikale Rotation des Sichtpunkts, sodass man nach oben oder unten schauen konnte. Allerdings verzerrt sich die Darstellung damit stark und unnatürlich. Auch bei den besten Erweiterungen sind keine übereinander liegenden dreidimensionalen Objekte wie Brücken möglich. Darstellungen derselben sind immer mit Hilfe von Sprites oder anderen Tricks „gefälscht“.\n\nPopulär wurde das Raycasting durch die frühen Ego-Shooter Catacomb, Wolfenstein 3D, Doom, Heretic und Duke Nukem 3D, da es erheblich weniger Berechnungszeit benötigt als echtes 3D.\n\nVerwandt mit dem Raycasting ist der mit dem Spiel Comanche eingeführte VoxelSpace-Algorithmus zur Visualisierung von Höhenfeldern. Darauf basierende Grafik-Engines werden oft schlicht als Voxel-Engines bezeichnet, obwohl keine Voxel visualisiert werden.\n\n"}
{"id": "786045", "url": "https://de.wikipedia.org/wiki?curid=786045", "title": "Hartree-Fock-Methode", "text": "Hartree-Fock-Methode\n\nUnter Hartree-Fock-Rechnung (beziehungsweise Hartree-Fock-Methode, nach Douglas Rayner Hartree und Wladimir Alexandrowitsch Fock), versteht man eine Methode der Quantenmechanik, in der Probleme mit mehreren gleichartigen Teilchen in Mean-Field-Näherung behandelt werden. Sie kommt zum Beispiel in der Atomphysik und Theoretischen Chemie (Systeme von Elektronen) oder Kernphysik (Systeme von Protonen und Neutronen) zur Anwendung.\n\nSie ermöglicht es, Orbitalenergien und Wellenfunktionen von quantenmechanischen Vielteilchensystemen näherungsweise zu berechnen und ist eine so genannte Ab-initio-Methode, d. h. sie kommt ohne empirische Parameter aus und benötigt nur Naturkonstanten.\n\nDie Hartree-Fock-Methode ist die Basis der Molekülorbitaltheorie.\n\nSie kann sowohl bei Vielteilchensystemen von Fermionen (wie Elektronen, Protonen, Neutronen) als auch bei Bosonen angewandt werden – bei Fermionen ist sie ein antisymmetrisches Produkt der Einteilchenwellenfunktionen (Slater-Determinante), bei Bosonen ein symmetrisches (Hartree-) Produkt. Im Folgenden wird der zum Beispiel für die Chemie wichtige Fall von Elektronen behandelt.\n\nBei der Aufstellung der Hartree-Fock-Gleichung wird die Wellenfunktion bei Elektronen näherungsweise als antisymmetrisiertes Produkt (Slater-Determinante) von Einelektronen-Wellenfunktionen (den Orbitalen, genauer gesagt den Spinorbitalen) angesetzt und darauf das Rayleigh-Ritz-Prinzip angewendet. Dieses besagt, dass die Energie, die mit einer beliebigen Wellenfunktion eines Systems als Erwartungswert über den Hamiltonoperator dieses Systems berechnet werden kann, immer über der Grundzustandsenergie dieses Systems liegt. Folglich werden die Orbitale so variiert, dass die Energie minimal wird. Es ist aber zu beachten, dass mit der Verringerung der berechneten Energie nicht notwendigerweise auch eine entsprechende qualitative Verbesserung der Wellenfunktion verbunden ist. Bei manchen (open-shell) Molekülen wird statt einer einzigen Slater-Determinante eine Linearkombination mehrerer Slater-Determinanten angesetzt, deren Koeffizienten aber durch die (Spin-)Symmetrie des Systems festgelegt sind.\n\nDie Hartree-Fock-Gleichung ist ein nichtlineares Eigenwertproblem mit einem nichtlokalen Integrodifferentialoperator. Sie lautet\n\nmit dem Fock-Operator formula_1\nwobei formula_4 der Einteilchenanteil des Hamiltonoperators ist und formula_5 der Anteil der Zweiteilchenwechselwirkung, wie oben erwähnt für den Spezialfall der Molekülphysik von Elektronen mit Coulombwechselwirkung untereinander und in atomaren Einheiten.\n\nDie Summe formula_6 erfolgt hierbei über die besetzten elektronischen Zustände, also die mit den formula_7 niedrigsten Eigenwerten, wobei formula_7 die Zahl der Elektronen angibt. Die Summe formula_9 erfolgt über die Kerne.\nÜblicherweise geht man nun in die Matrixdarstellung der Gleichung über, indem man formula_10 in der Basis formula_11 darstellt, sodass formula_12. Diese Basis ist typischerweise nicht orthogonal.\nNach Multiplikation mit formula_14 ergibt sich das verallgemeinerte Eigenwertproblem\nmit der Fockmatrix formula_16, der Überlappmatrix formula_17 und den Koeffizientenvektoren formula_18. Dieses kann mit Hilfe z. B. von Löwdins symmetrischer Orthogonalisierung in ein einfaches Eigenwertproblem umgewandelt werden. Diese Gleichung ist auch als Roothaan-Hall-Gleichung bekannt. Als Lösung erhält man formula_19 Eigenwerte und Eigenvektoren, wovon man die formula_7 niedrigsten Eigenwerte und zugehörigen Eigenvektoren als besetzte Zustände ansieht. Als Basisfunktionen kommen in vielen Fällen Linearkombinationen von Gaussian Type Orbitals (GTO) oder Slater Type Orbitals (STO) zum Einsatz. Für Berechnungen an einzelnen Atomen und zweiatomigen oder linearen Molekülen können die Hartree-Fock-Gleichungen auch auf einem Gitter gelöst werden.\n\nUm die Hartree-Fock-Gleichung zu lösen, muss von den oben verwendeten Spinorbitalen formula_21 noch die Spinwellenfunktion\nformula_22 abgespalten werden, sodass formula_23\nmit der reinen Ortswellenfunktion formula_24 gilt.\nBei dem Geschlossene-Schalen-Hartree-Fock Ansatz (engl. Restricted Hartree Fock) werden alle Spins als gepaart angenommen,\nwas natürlich nur bei einer geraden Anzahl von Elektronen möglich ist. Der Grundzustand wird somit als Spin-Singulett angenommen.\nFür die Wellenfunktionen folgt somit\nSetzt man dies in die Hartree-Fock-Gleichung ein, folgt\nDie Coulombwechselwirkung tritt somit zwischen allen Elektronen auf, die Austauschwechselwirkung hingegen nur zwischen Elektronen\nmit gleichem Spin. Wegen der Symmetrie zwischen Spin up und down ist die HF-Gleichung für beide Spinkonfigurationen gleich,\nsodass weiterhin nur eine Eigenwertgleichung gelöst werden muss, wobei nun allerdings nur noch die niedrigsten formula_28 Eigenwerte\nund Eigenvektoren verwendet werden müssen.\n\nBei dem Offene-Schalen-Hartree-Fock-Ansatz (engl. Unrestricted Hartree Fock) wird im Vergleich zum Geschlossene-Schalen-Ansatz (RHF) die Forderung fallengelassen, dass gleich viele Elektronen im Zustand formula_29, wie im Zustand\nformula_30 sein müssen. Die Spinorbitale werden demnach angesetzt als\nNach Einsetzen in die ursprüngliche Hartree-Fock-Gleichung ergeben sich zwei verschiedene Gleichungen für formula_29\nund formula_33.\nDie Gleichung für formula_33 folgt aus der Ersetzung formula_36 und formula_37.\nHierbei sieht man wieder, dass Elektronen mit gleichem Spin Coulomb- und Austauschwechselwirkung besitzen, Elektronen mit\nunterschiedlichem Spin wechselwirken hingegen nur über den Coulombterm. Da die Austauschwechselwirkung die Gesamtenergie\nstets verringert kann somit, im Rahmen von Hartree-Fock, die zweite Hundsche Regel erklärt werden.\nDiese besagt, dass bei sonstiger Entartung oder Quasientartung die Spins zweier Elektronen möglichst parallel ausgerichtet sind.\n\nZur Herleitung der Hartree-Fock Gleichungen geht man zunächst von der stationären Schrödingergleichung aus. Hier wird der Spezialfall eines Hamiltonoperators mit Coulombwechselwirkung in der Born-Oppenheimer-Näherung betrachtet, wie er zum Beispiel für Elektronen in der Molekülphysik auftritt. Das heißt\nformula_39 bezeichnet hierbei die elektronischen Koordinaten, formula_7 die Anzahl der Elektronen, formula_41 und formula_42 die Ladung und festen\nKoordinaten der Kerne. formula_43 ist nun ein Einteilchenoperator und besteht aus der kinetischen Energie und der Wechselwirkung mit allen Kernen des formula_44-ten Elektrons. formula_45 ist hingegen ein Zweiteilchenoperator und stellt die Coulombwechselwirkung des mit dem formula_46-ten Elektron dar. Die stationäre Schrödingergleichung lautet nun\nAls Näherung für Hartree-Fock schreibt man nun formula_48 als Slater-Determinante von\nEinteilchenwellenfunktionen formula_49. Die Näherung besteht darin, dass man für die exakte Lösung\nüber alle möglichen Slater-Determinanten summieren müsste, z. B. indem man formula_50 durch formula_51 ersetzt. Somit gilt\nund die Energie des Systems lautet\nDies kann man nun, indem man die Orthogonalität der formula_54 ausnutzt, zu\n\numformen. Nun wird das Ritzsche Variationsprinzip verwendet und formula_58 als Funktional nach formula_54 variiert. Um die Orthogonalität der Einteilchenfunktionen zu erhalten wird allerdings nicht direkt formula_58 minimiert, sondern nach der Methode der Lagrange-Multiplikatoren\ndas Funktional formula_61\nMan kann nun in die Basis formula_63 wechseln, in der formula_64 diagonal ist, also formula_65.\nDie Tilde wird im Weiteren weggelassen. Nun kann formula_61 bezüglich formula_68 minimiert werden.\nDa der Summand mit formula_72 gleich Null ist, kann er hinzugenommen werden, wodurch alle formula_7 Gleichungen identisch sind und somit der Index formula_74 weggelassen werden kann.\nSomit folgt\ndie Hartree-Fock-Gleichung mit dem Fockoperator formula_77. Hierbei besitzen die beiden ersten Terme ein klassisches Analogon. formula_4 enthält die kinetische Energie und die Coulombwechselwirkung mit den Kernen. Der zweite Term kann als mittleres Coulombpotential aller anderen Elektronen auf das formula_74-te Elektron interpretiert werden. Die instantane Korrelation der Teilchen wird jedoch vernachlässigt. Die Hartree-Fock-Methode ist daher ein Mean-Field-Ansatz. Der Austauschterm besitzt kein klassisches Analagon. Der Fockoperator für das formula_74-te Elektron enthält die Wellenfunktionen aller anderer Elektronen, wodurch die Fockgleichungen meist nur mit der Methode der selbstkonsistenten Felder, d. h. iterativ mittels Fixpunktiteration, gelöst werden kann. Zur Konvergenzbeschleunigung kommt hierzu häufig das DIIS-Verfahren<ref name=\"DOI10.1016/0009-2614(80)80396-4\"></ref> zum Einsatz.\n\nEine direkte numerische Lösung der Hartree-Fock-Gleichung als Differentialgleichung ist bei Atomen und linearen Molekülen möglich. In der Regel werden die Orbitale aber analytisch als Linearkombinationen von Basisfunktionen angesetzt (Basissatz), was wiederum eine Näherung darstellt, die umso besser wird, je größer und intelligenter der Basissatz gewählt wird. Typischerweise bringt jedes Atom im Molekül nun eine vom entsprechenden Basissatz festgelegte Anzahl von Basisfunktionen, die auf ihm zentriert sind, mit. Als grober Ausgangspunkt zur Erstellung solcher Basissätze dienen die analytischen Lösungen des Wasserstoffatoms, welche ein formula_81-Verhalten für große Kernabstände formula_82 zeigen. Ansätze dieses Typs nennt man Slater Type Orbital (STO). Meist haben sie die Form\nEin formula_84-Orbital besitzt z. B. die Form\nDer große Nachteil der Slater-Type-Orbitale ist jedoch, dass die erforderlichen Matrixelemente formula_86 nicht im Allgemeinen analytisch berechenbar sind. Deshalb benutzt man fast ausschließlich Gaussian Type Orbitals, d. h. Basisfunktion der Form\nHierbei können die Matrixelemente analytisch berechnet werden. Dabei wird u. a. das Gaussian Product Theorem ausgenutzt, d. h., dass das Produkt zweier Gaußfunktionen wieder eine Gaußfunktion ist. Um die STOs besser zu approximieren, besteht typischerweise eine Basisfunktion aus mehreren Gaußfunktionen mit festen, vom Basissatz festgelegten Parametern („Contraction“). Ein einfacher Basissatz ist z. B. der sog. STO-NG, welcher Slater Type Orbitale mit formula_7 Gaußfunktionen annähert. Damit wird die Lösung der Differentialgleichung reduziert auf die analytische Berechnung von Integralen über diese Basisfunktionen und die iterative Lösung des verallgemeinerten Eigenwertproblems mit den Koeffizienten der Basisfunktionen als zu bestimmende Parameter.\n\nDie mit der Hartree-Fock-Methode errechnete Energie erreicht nie den exakten Wert, selbst wenn ein unendlich großer Basissatz verwendet werden würde. Bei diesem Grenzfall wird das sogenannte Hartree-Fock-Limit erreicht. Der Grund dafür ist, dass durch die Verwendung des gemittelten Potenzials die Elektronenkorrelation, also die genaue Wechselwirkung der Elektronen untereinander, nicht erfasst wird. Um diesen Makel zu beseitigen, wurden Methoden entwickelt, die in der Lage sind, zumindest einen Teil der Elektronenkorrelation zu erfassen (siehe Artikel Korrelierte Rechnungen). Von Bedeutung sind insbesondere Coupled-Cluster-Methoden und die Møller-Plesset-Störungstheorie, die auf der Lösung des Hartree-Fock-Verfahrens aufbauen. Eine andere sehr bedeutende Methode ist die Dichtefunktionaltheorie mit Hybridfunktionalen, bei der der Hartree-Fock-Austausch anteilig in den Austausch-Korrelations-Teil des Dichtefunktionals eingeht.\n\nDie Hartree-Fock-Methode erlaubt aber bei sehr vielen Molekülen eine gute Bestimmung ihrer „groben“ elektronischen Struktur. Sie liefert im Regelfall elektronische Gesamtenergien, die bis auf 0,5 % mit den korrekten elektronischen Energien übereinstimmen (zur Berechnung von Energiedifferenzen, wie z. B. Reaktionsenergien, ist sie aber nicht brauchbar), Dipolmomente, die auf 20 % mit den wirklichen Dipolmomenten übereinstimmen, und sehr genaue Verteilungen der Elektronendichte im Molekül. Aufgrund dieser Eigenschaften werden Hartree-Fock-Rechnungen häufig als Ausgangspunkt für die oben genannten genaueren Rechnungen verwendet.\n\nEin weiterer Vorteil der Hartree-Fock-Methode ist, dass die erhaltene Energie gemäß dem Variationsprinzip eine obere Schranke für die exakte Grundzustandsenergie darstellt. Durch die Wahl umfangreicherer Basissätze kann die berechnete Wellenfunktion systematisch bis zum sogenannten \"Hartree-Fock Limit\" verbessert werden. Eine derartige systematische Betrachtungsweise ist bei Dichtefunktionalmethoden nicht möglich.\n\n\n"}
{"id": "786957", "url": "https://de.wikipedia.org/wiki?curid=786957", "title": "Artpack", "text": "Artpack\n\nEin Artpack (szenesprachlich auch kurz „Pack“) ist eine periodische Zusammenstellung von digitaler Kunst – meist ASCII-, ANSI- und Pixel-Art, digitale Musik sowie Softwarekunst –, die von den Mitgliedern einer digitalen Künstlergruppe (szenesprachlich auch „Artgroup“ genannt) in einem bestimmten Zeitraum erstellt wurden. Artpacks werden in der Regel als komprimierte Archivdateien im ZIP- oder anderen Archivformaten veröffentlicht.\n\nMit der Veröffentlichung als Artpack erlauben die Künstler ganz bewusst kein schnelles und damit tendenziell oberflächliches Betrachten im Webbrowser. Ein Artpack muss heruntergeladen, „offline“ ausgepackt und häufig auch manuell per Dateimanager erforscht werden. Auf diese Weise werden sowohl die Kunstwerke in einen Kontext zueinander gestellt als auch der Betrachter bewusster in diesen Kontext einbezogen.\n\nAndere Formen der Veröffentlichung digitaler Kunst, die von Artgroups genutzt werden, sind Diskmags sowie Music Disks, die jeweils eigene grafische Oberflächen an die Stelle des Dateimanagers setzen.\n\nArtgroups verstehen sich meist als Vereinigung von Digitalkünstlern zum Zwecke der nichtkommerziellen Verbreitung ihrer Kunst. Wurden Artgroups in den 1980er und 1990er Jahren als Untergruppen (szenesprachlich „Subdivision“) von Cracker-Gruppen gegründet, arbeiten sie heute überwiegend als freie Nachfragegruppen.\n\nInternational bekannte US-amerikanische Artgroups sind ACiD Productions (seit 1990) und iCE Advertisements (seit 1991); eine bekannte deutsche Artgroup ist Black Maiden (seit 1985).\n"}
{"id": "787113", "url": "https://de.wikipedia.org/wiki?curid=787113", "title": "Norton Personal Firewall", "text": "Norton Personal Firewall\n\nDie Norton Personal Firewall ist bis zur Version 2006 als eigenständiges kommerzielles Produkt erhältlich. Außerdem ist die Personal Firewall Bestandteil des Softwarepakets \"Norton Internet Security\". Dieses enthält neben der Desktop-Firewall ein Antivirenprogramm und einen Spamfilter. \"Norton Internet Security 2007\" ist auf Windows Vista (32 und 64-Bit) und Windows XP (32-Bit) lauffähig und benötigt laut Hersteller mindestens einen mit 300 MHz getakteten Prozessor, 256 MB Arbeitsspeicher und 350 MB freien Speicherplatz auf der Festplatte. Versionen der \"Norton Personal Firewall\" existieren für Windows 2000 SP3 und XP (Norton Personal Firewall 2006), für Windows 98 und ME (Norton Personal Firewall 2005) sowie für Mac OS 9 und OS X (Norton Personal Firewall 3.0. for Macintosh).\n\nHistorischer Vorläufer der Norton Personal Firewall ist die Personal Firewall AtGuard von WRQ, die als Freeware zur Verfügung stand. Es bot dem Anwender neben einem durch dialoggeführte Regelerstellung selbstlernenden TCP/IP-Filter, der sich auf Anwendung, Port und Zieladresse abstimmen lässt, zusätzliche Werbe- und Cookie-Filter für das Browsen im World Wide Web. So sind zahlreiche werbeverdächtige HTML-Strings bereits nach der Installation von AtGuard blockiert. Außerdem lassen sich aktive Inhalte wie Scripting, Java und ActiveX domänenspezifisch unterbinden.\n\nDie Anwendung von AtGuard setzt wegen der vielen Feineinstellungsmöglichkeiten Grundkenntnisse im TCP/IP-Protokoll und eine gewisse Einarbeitungszeit voraus, bot aber richtig konfiguriert einen wirksamen Schutz vor Werbung, unerwünschten Eindringlingen und nach Hause telefonierenden Programmen.\n\nSämtliche Firewall-Regeln und Benutzereinstellungen wurden im Registry-Hive codice_1 abgelegt und sind somit auf andere Installationen übertragbar.\n\nObwohl die Rechte durch den Aufkauf nunmehr bei Symantec liegen, finden sich im Netz noch vereinzelt Downloadmöglichkeiten für die anfangs kostenlose Software. Der ursprüngliche Entwickler hat den MD5 Hash (6dea78ce53e564c49e22552f63b16c2e) der letzten von ihm veröffentlichten Version auf seiner inoffiziellen AtGuard Homepage veröffentlicht. Der Dateiname des dazugehörigen Downloads war \"atgd322u.exe\".\n\nSymantec kaufte 1999 AtGuard von WRQ und nannte es in \"Norton\" um, welcher von Symantec als Konsumentenmarke benutzt wird. Er geht auf den Hersteller des Norton Commanders, die Firma Peter Norton Computing, zurück, die 1990 von Symantec gekauft wurde.\n\n"}
{"id": "787337", "url": "https://de.wikipedia.org/wiki?curid=787337", "title": "Entfernen (Taste)", "text": "Entfernen (Taste)\n\nDie Entfernen-Taste (kurz Entf-Taste auch Löschtaste; englisch: -Taste, für ‚‘) auf der PC-Standard-Tastatur, oft über den Pfeiltasten angeordnet, löscht beim Bearbeiten von Schriften (Texten) das Zeichen an der momentanen Cursor-Position und bewegt alle nachfolgenden Zeichen um eine Stelle je Tastendruck nach links (auch „Vorwärtslöschung“ genannt), anders als die Rücklöschtaste, welche das links vom Cursor stehende Zeichen löscht (auch „Rückwärtslöschung“ genannt).\n\nAuf englischen Tastaturen wird die Entf-Taste wie auf Schweizer Tastaturen mit „Delete“ oder „Del“ \"(löschen)\" beschriftet. Verwirrenderweise wurde zeitweise auf englischen Apple-Computern und Notebooks auch die Rücklöschtaste mit „Del“ beschriftet, und nur die traditionelle Position an der rechten oberen Ecke des Tastenfeldes wies auf die für jene typische Rücklöschfunktion hin.\n\nDas ASCII-Steuerzeichenwert für die Entfernen-Taste ist 0x7F (0127).\n\nViele Betriebssysteme und Programme interpretieren die Entfernen-Taste als allgemeinen Befehl zum Löschen von Daten.\n\nDie Tastenkombination ++ („Klammergriff“) löst in vielen Betriebssystemen einen wichtigen Systembefehl aus:\n"}
{"id": "788609", "url": "https://de.wikipedia.org/wiki?curid=788609", "title": "Simulationsbasierte Optimierung", "text": "Simulationsbasierte Optimierung\n\nDie Idee der simulationsbasierten Optimierung (SBO) besteht darin, mit Simulationsmodellen eine Optimierungskomponente zu verbinden, die bestimmte Variablen eines Simulationsmodells zur Minimierung oder Maximierung einer Zielfunktion variiert.\n\nSimulationsmodelle dienen zur Prognose komplexer, realer Systeme, die zufälligen Einflüssen unterliegen. Typischerweise nutzt man Simulationsmodelle, um die Auswirkungen einzelner Handlungsalternativen zu untersuchen, ohne diese tatsächlich umzusetzen und mögliche negative Effekte auf das reale System zu verursachen. Man beschränkt sich i. d. R. auf eine relativ kleine Zahl von Handlungsalternativen, spielt diese mit Hilfe von Simulationssoftware durch, und wählt anschließend die nach einer bestimmten Zielsetzung beste Handlungsalternative aus.\nDie klassische Methode zur Auswahl einer Handlungsalternative besteht darin, die Simulationen mit entsprechender Software durchzuführen, die Auswahl dann jedoch weitgehend manuell zu treffen. Die simulationsbasierte Optimierung bezweckt hingegen, diese Auswahl der „besten“ Handlungsalternative automatisiert durch einen Algorithmus vorzunehmen, der ein zugrunde liegendes Simulationsmodell verwendet. Die Grundidee der SBO besteht darin, Handlungsalternativen durch Variablen eines Simulationsmodells zu beschreiben. Ein SBO-Algorithmus variiert diese Variablen, bewertet wiederholt die aus der Wahl von Variablenwerten resultierende Lösung durch Simulationsläufe und liefert dann die beste gefundene Lösung zurück. Dabei kann zum einen der Optimierer den Simulator steuern und umgekehrt. \n\nIn Analogie zur „klassischen“ Optimierung entsprechen also bei der SBO das Simulationsergebnis, welches z. B. ein Kostenwert sein kann, der Zielfunktion eines Optimierungsproblems, und die Variablen eines Simulationsmodells den Variablen eines Optimierungsmodells. Ein wesentlicher Unterschied zu „üblichen“ Optimierungsproblemen besteht jedoch darin, dass die „Zielfunktion“ bei der SBO stochastisch ist, d. h., sie unterliegt zufälligen Schwankungen, je nachdem welches eintretende Szenario in einem Simulationslauf betrachtet wird.\n\nMögliche Zielsetzungen, aus denen konkrete SBO-Probleme abgeleitet werden können, sind das Finden\n\n\nAuf dem Markt für Simulationssoftware existiert eine kaum überblickbare Vielfalt von Produkten. Diverse Simulationssoftwarepakete wurden inzwischen um Komponenten zur SBO erweitert:\n\n\nEs lässt sich folgenden Einteilung der SBO-Algorithmen in Klassen vornehmen, die sich an den SBO-Forschungsrichtungen orientiert:\n\n"}
{"id": "790122", "url": "https://de.wikipedia.org/wiki?curid=790122", "title": "3Dc", "text": "3Dc\n\n3Dc (auch als \"DXN\", \"BC5\" oder \"Block Compression 5\" bezeichnet) ist ein von ATI Technologies für 3D-Computergrafik entwickeltes verlustbehaftetes Texturkomprimierungssystem für Normalmaps. Die FourCC-Kennung ist ATI2.\n\n\"3Dc\" ist ein sogenannter Offener Standard, der sowohl von ATI als auch Nvidia verwendet wird. Er baut auf dem DXT5-Algorithmus auf, komprimiert Normalmaps aber deutlich effizienter und fehlerfreier. Der maximal mögliche Komprimierungsfaktor beträgt laut ATI 4:1, d. h. in dem Grafikspeicher können viermal so viele Texturdetails gespeichert werden wie ohne Komprimierung.\n\n"}
{"id": "790514", "url": "https://de.wikipedia.org/wiki?curid=790514", "title": "PowerStack", "text": "PowerStack\n\nPowerStack ist Motorolas Implementierung der von Apple, IBM und Motorola (AIM-Allianz) entwickelten Referenzplattform für die damals neuen PowerPC-CPUs. Der PowerStack besteht aus einem kompakten schwarzen Basisgehäuse, das einen Workstation-Computer mit 3,5\"-Diskettenlaufwerk, CD-ROM-Laufwerk, Festplatte und Multimediafähigkeiten beinhaltet. Erweiterungsgehäuse (beispielsweise für weitere Festplatten) können auf das Computergehäuse aufgestapelt werden (Stack = engl. für Stapel).\n\nPowerStacks gab es in verschiedenen Ausführungen. Die ersten Modelle waren mit einem PPC601-Prozessor bei 66 MHz ausgestattet und entsprachen dem Prep-Standard. Spätere Modelle mit einem PPC604e mit bis zu 300 MHz weichen insofern von diesem Standard ab, als sie eine OpenFirmware enthalten, jedoch trotzdem noch Prep- und nicht CHRP-kompatibel sind. Des Weiteren wurde bei diesen Modellen auf ein üblicheres Gehäuse zurückgegriffen. Diese verfügen über Schächte für ein 1,44\"-Diskettenlaufwerk und drei optische Laufwerke und sind in einem klassischen hellen Farbton gehalten.\n\nPowerstacks wurden mit IBMs AIX oder Microsofts Windows NT als Betriebssystem ausgeliefert. Linux unterstützt Powerstack-PreP bzw. CHRP-Varianten seit Mitte der 1990er.\n\n"}
{"id": "792381", "url": "https://de.wikipedia.org/wiki?curid=792381", "title": "TAP (Zeitschrift)", "text": "TAP (Zeitschrift)\n\nTAP – The Youth International Party Line (YIPL) ist ein amerikanisches Untergrund-Magazin, das seit Juni 1971 Leser mit Informationen zu Phreaking, Hacken, DFÜ, Mailboxen sowie technischen Umbauten an den Telefonanlagen und anderen teilweise illegalen Themen versorgte.\n\nDie TAP dürfte die älteste Hackerzeitung sein und erschien bis 1984. Sie war Vorbild für eine ganze Reihe weiterer Hackzeitschriften wie dem \"Phrack\", dem Magazin \"\" und der deutschsprachigen \"Datenschleuder\".\n\n\nYouth International Party\n\n"}
{"id": "794712", "url": "https://de.wikipedia.org/wiki?curid=794712", "title": "Apple Developer Connection", "text": "Apple Developer Connection\n\nDie Apple Developer Connection (ADC) ist ein Internetportal des Unternehmens Apple zur Softwareentwicklung auf der Macintosh-Plattform. Neben der Macintosh-Plattform kamen im Zuge des iPods auch technische Artikel über den Musikbereich hinzu. Die Seiten sollen Entwicklern helfen, mit Apples Technologie-Plattform (Hardware sowie Software) umzugehen, indem Lehrmaterial wie Tutorien, technische Artikel und umfassende Dokumentation bereitgestellt wird. Das meiste Material auf den Seiten ist in Englisch abgefasst. Allerdings gibt es nationale ADCs, welche Inhalte in der jeweiligen Landessprache zur Verfügung stellen.\n\nDie Apple Reference Library ist eine Sammlung des Lehrmaterials, welcher innerhalb der ADC bereitgestellt wird. Jeder eingetragene Entwickler (mit Ausnahme der kostenlosen Variante) bekommt monatlich ein Päckchen mit der aktuellen Fassung als CD oder DVD per Post zugeschickt. Seit Dezember 2007 hat Apple den Vertrieb der DVDs eingeschränkt, so dass man die DVDs nur noch bekommt, wenn man explizit widersprochen hat, sich die monatlichen DVDs auf der ADC-Webseite herunterzuladen.\n\nDie WWDC ist die jährlich in Kalifornien stattfindende Worldwide Developers Conference der Firma Apple, auf der neue Versionen des Betriebssystems macOS und anderen Apple-Produkte vorgestellt werden. Die Konferenz beginnt, wie auch bis 2009 üblich die Macworld, mit einer Keynote, die seit 1998 von Steve Jobs (damals CEO) gehalten wurde. Aufgrund einer Krankheit wurde sie 2009 von Apples Senior Vice President for Marketing Phil Schiller gehalten. Die erste WWDC fand 1983 in Monterey statt. Mittlerweile findet sie im Moscone Center in San Francisco statt.\n\nAlle Teilnehmer müssen eine Vertraulichkeitsvereinbarung (NDA - „\"Non Disclosure Agreement\"“) unterzeichnen. Das galt bisher auch für die Keynote, jedoch bieten viele Websites einen Liveticker an, der beinahe zeitgleich per Text über das Geschehen berichten. Zirka einen Tag nach dem Event wird außerdem immer eine Videoaufzeichnung auf der Apple Website zur Verfügung gestellt. In der Vergangenheit wurde normalerweise nur Software präsentiert, im Laufe der letzten Jahre wurde Hardware jedoch der gleiche Platz spendiert. So wurden in den Jahren 2008 und 2009 die jeweils neueste iPhone-Generation vorgestellt. Auch widmet sich die Keynote seit geraumer Zeit immer den kommenden Versionen des Betriebssystems macOS. 2005 wurde dort auch der Übergang zur Intel-Plattform bekanntgegeben.\n\nDas ADC bietet mehrere, zum Teil kostenpflichtige, Mitgliedschaften.\n\nDie Online-Mitgliedschaft ist für jeden offengestellt und kostenlos. Neben dem ohnehin schon freien Zugang zu Dokumentationen etc. bietet diese Mitgliedschaft den Zugang zum \"Download\"-Bereich, in dem man sich beispielsweise Apples Entwicklungsumgebung Xcode direkt herunterladen kann (sowohl alte als auch neuere Versionen; im Mac App Store ist immer nur die neueste Version zu finden).\n\nDie Mitgliedschaft im iOS Developer Program kostet 99 US-Dollar jährlich, ermöglicht den Zugriff auf Vorabversionen von iOS und erlaubt es, Applikationen im App Store anzubieten. Außerdem können Entwickler ihre Programme direkt auf ihrem Gerät testen und an Tester verteilen.\n\nFür Unternehmen bietet Apple das 299 US-Dollar pro Jahr teure „iOS Developer Enterprise Program“ an. Dieses richtet sich an größere Unternehmen, die eigene Programme über alternative Distributionswege an die Mitarbeiter verteilen möchten, bspw. über das iPhone Configuration Utility oder den Apple Configurator.\n\nDas Universitäts-Programm richtet sich an Studenten, die in der Entwicklung von iOS-Programmen unterrichtet werden. Es ermöglicht, zusätzlich zu den Vorteilen der kostenfreien Mitgliedschaft, nur das Testen eigener Programme auf einem iPhone, iPod oder iPad.\n\nDie Mitgliedschaft im \"Mac Developer Program\" kostet ebenfalls 99 US-Dollar jährlich, ermöglicht den Zugriff auf Vorabversionen von macOS und erlaubt es, Applikationen im Mac App Store anzubieten.\n\n"}
{"id": "796953", "url": "https://de.wikipedia.org/wiki?curid=796953", "title": "Roland TR-606", "text": "Roland TR-606\n\nDie TR-606 (manchmal auch \"Drumatix\" genannt) ist eine Drummachine der japanischen Firma Roland. Sie wurde unter anderem von Mr. Oizo in seinem Hit \"Flat Beat\", berühmt durch die Levi’s-Werbung, verwendet.\n\nDas TR-606 (Transistor Rhythm) wurde 1982 als Rhythmusgerät zum optisch sehr ähnlichen Typ TB-303 (Transistor Bassline) veröffentlicht. Beide Geräte fanden nur einen kleinen Abnehmerkreis, und die Produktion wurde bald eingestellt. Heute ist das TR-606 ein beliebtes Vintage-Instrument und wird von Künstlern wie Aphex Twin, Mr. Oizo, Moby, Astral Projection, Nine Inch Nails, OMD, Sneaker Pimps, Big Black und vielen anderen verwendet.\n\nDie TR-606 ist ein pattern-basierter analoger Drumcomputer, dessen Klänge (Bass Drum, Snare Drum, Hi Tom, Low Tom, Closed Hi Hat, Open Hi Hat, Cymbal) nur in der Lautstärke verändert werden können. Die Maschine verfügt über eine Accent-Funktion, welche den Rhythmen zusätzliche Dynamik verleihen kann. Über den Roland DIN sync-Anschluss kann die Drummachine mit zum Beispiel der TB-303 oder TR-808 synchronisiert werden – eine hilfreiche Funktion, da zu jenen Zeiten sich MIDI noch nicht als Studiokommunikations-Standard durchgesetzt hatte.\n\n\n"}
{"id": "797459", "url": "https://de.wikipedia.org/wiki?curid=797459", "title": "FreeMind", "text": "FreeMind\n\nFreeMind ist ein freies Computerprogramm zum Visualisieren und Strukturieren von Inhalten (Begriffe, Ideen, Lernstoff, Sitzungsergebnisse usw.). Diese Methode der Wissensdarstellung wird Business Mapping genannt, die entstehenden Dokumente heißen Mindmaps. Mindmaps sind zum Beispiel als Hilfsmittel für das Projektmanagement sinnvoll, unterstützend bei der Strukturierung von Lerninhalten oder bei der Zusammenfassung von Ergebnissen eines Brainstormings. Aktiv weiterentwickelt wird derzeit der Fork Freeplane.\n\nIn die erzeugten Mindmaps lassen sich Hyperlinks, Symbole oder Bilder einfügen, auch grafische Verbindungen zwischen den einzelnen „Zweigen“ können dargestellt werden. Mit Exportfunktionen lassen sich die Ergebnisse unter anderem als XHTML oder im OpenOffice.org-Writer-Format abspeichern. Zudem ist die Integration interaktiver Mindmaps mittels Java-Applet und Flash in Webseiten und speziell in verschiedene Wikis möglich, z. B. MediaWiki und WikkaWiki.\n\nDas Programm FreeMind wurde in der Programmiersprache Java für Windows, Mac und Linux entwickelt. Zum Ausführen ist die Java Runtime Environment ab Version 1.5 erforderlich.\n\nDas XML-basierte Datenformat (Dateiendung *.mm) erlaubt das Schreiben eigener Konvertierroutinen mittels XSLT.\n\nDie Funktionalität des Tools orientiert sich an den Kernaufgaben des Mindmappings. Die erhältlichen kommerziellen Lösungen erweitern die Mindmapping-Funktionalität oft um kooperative Aspekte und zentral verwaltete MindMap-Informationsdatenbanken im Firmenumfeld, was FreeMind nicht leistet.\n\nDie Versionen der 0.9.0-Reihe weisen unter anderem erweiterte Exportfunktionen auf, so ist beispielsweise das Erstellen von PDF-Dokumenten oder SVG-Grafiken möglich. Zudem bieten diese den Einsatz von Filtern und Attributen, sowie die Verschlüsselung einzelner Zweige. Eine Kalenderfunktion mit Wiedervorlage ist integriert.\n\nMit Version 1.0 erhielt die Software Kartenfunktionalität sowie Möglichkeiten zur besseren kollaborativen Arbeit. Die Beta-Versionen 1.1 erweitert die Funktionalität u. a. um die Unterstützung hochauflösender Displays.\n\nDerzeit gibt es nur noch einen aktiven Fork von FreeMind, \"Freeplane\", und dieser ist besonders auf Benutzerfreundlichkeit ausgerichtet. Der Fork SciPlore MindMapping wurde inzwischen zugunsten des Nachfolgeprojekts Docear eingestellt, das gleichfalls Literaturverwaltung für den akademischen Bedarf bietet, aber Freeplane statt FreeMind als Basis verwendet.\n\n\n"}
{"id": "799269", "url": "https://de.wikipedia.org/wiki?curid=799269", "title": "NTLM", "text": "NTLM\n\nNTLM (kurz für \"NT LAN Manager\") ist ein Authentifizierungsverfahren für Rechnernetze. Es verwendet eine Challenge-Response-Authentifizierung.\n\nDurch den Einsatz von NTLM über HTTP ist ein Single Sign-on auf Webservern oder Proxyservern unter Verwendung des Berechtigungsnachweises \"(Credentials)\" der Windows-Benutzeranmeldung möglich.\n\nNTLM war ursprünglich ein proprietäres Protokoll des Unternehmens Microsoft und daher fast ausschließlich in Produkten dieses Herstellers implementiert. Dank Reverse Engineering unterstützen jedoch beispielsweise auch Samba, Squid, Mozilla Firefox, cURL, Opera und der Apache HTTP Server dieses Protokoll. Anfang 2007 hat Microsoft seine Spezifikation auf Druck der Vereinigten Staaten und der Europäischen Union veröffentlicht.\n\nDer Vorgänger des NTLM-Protokolls ist LM (LAN Manager), das schon im Betriebssystem OS/2 zum Einsatz kam. NTLM behob das Problem, dass lange Passwörter angreifbarer als kurze Passwörter sein konnten. Aufgrund weiterer Sicherheitsprobleme wurde NTLMv2 entwickelt und die frühere Version fortan NTLMv1 genannt. Auch in NTLMv2 sind Sicherheitsprobleme bekannt: Responses können abgefangen werden, um Replay-Angriffe auf den Server und Reflection-Angriffe auf den Client durchzuführen.\n\nEine Authentifizierung über NTLM beginnt damit, dass der Client den Benutzernamen an den Server sendet. Der Server sendet daraufhin als \"Challenge\" eine Zufallszahl an den Client. Der Client sendet als \"Response\" die mit dem Hashwert des Benutzerpassworts verschlüsselte Zufallszahl zurück. Der Server verschlüsselt ebenfalls die Zufallszahl mit dem Hashwert des Benutzerpassworts, das er in seiner Datenbank oder vom Domain Controller hat, vergleicht die beiden Ergebnisse und bestätigt bei Übereinstimmung die Authentifizierung. Das Benutzerpasswort wird also nicht über das unsichere Medium gesendet.\n\nEine Alternative zu NTLM ist das Protokoll Kerberos, das auch unter Windows seit der Einführung des Active Directory mit Windows 2000 standardmäßig zum Einsatz kommt. Wenn eine Authentifizierung mittels Kerberos nicht möglich ist, wird aber automatisch NTLM verwendet. Den Port für NTLM wählt Windows in der Grundeinstellung dynamisch.\n\n\"Secure Password Authentication\", kurz SPA, nennt Microsoft die Authentifizierung über NTLM für Microsoft Exchange Server.\n\n"}
{"id": "799446", "url": "https://de.wikipedia.org/wiki?curid=799446", "title": "Microsoft Project", "text": "Microsoft Project\n\nMicrosoft Project ist eine Software zum Planen, Steuern und Überwachen von Projekten. Die derzeit aktuelle deutschsprachige Version ist Project 2019.\n\nNeben der Einzelplatzversion \"Microsoft Project Standard\" gibt es für die Anbindung an den Microsoft-Project-Server die Version \"Microsoft Project Professional\".\n\nDurch die Anbindung an den Microsoft-Project-Server wird auf eine Vielzahl von Enterprise-Projektmanagement-Anforderungen (EPM-Anforderungen) eingegangen.\n\nDazu gehören Anforderungsmanagement, Portfoliomanagement, Multiprojektmanagement, Zeiterfassung, zentrale Ressourcenplanung, Risikomanagement, Dokumentenmanagement, Zugriffsverwaltung, Reporting oder aber auch der Zugriff mittels eines Webclients.\n\nDer Microsoft-Project-Server nutzt die Microsoft-SQL-Datenbank zur zentralen Datenhaltung.\n\nMicrosoft Project ist Bestandteil der Office-Familie und lässt sich sowohl server- als auch clientseitig mit Software von Drittanbietern in andere IT-Systeme integrieren, z. B. in ERP-Software wie SAP PS oder Groupware wie Lotus Domino oder Exchange. Für den Servereinsatz werden allerdings noch der Microsoft SQL Server und die Windows SharePoint Services benötigt, um dem Project Web Access zu ermöglichen.\n\nIn Microsoft Project Client werden die Projekte dezentral verwaltet. Für eine zentrale Verwaltung der Projekte und Ressourcen ist Microsoft Project Server oder eine andere Integrationsplattform erforderlich. Eine zentrale Verwaltung hat den Vorteil, dass Abhängigkeiten und Auswertungen über mehrere Projekte zur Verfügung stehen. Ein weiterer Vorteil ist, dass die Projektteams eingebunden werden können. Damit vereinfacht sich die Informationsverteilung und das Rückmelden wie Fortschrittstatus und Ist-Zeiten. Für die Teameinbindung kann Web Access oder andere Tools verwendet werden.\n\nGrundlage von MS Project ist die Netzplantechnik. Die „Vorgänge“ in MS Project sind ein flexibler Begriff. Ein Vorgang kann eine einzelne Aufgabe aus einem Arbeitspaket, eine Zusammenfassung von Aufgaben oder gar ein eigenes Projekt abbilden. Je nach Ebene bzw. Bedeutung des Vorgangs können diese in Sammelvorgängen gegliedert werden. Meilensteine (signifikante Kontrollpunkte) sind Vorgänge ohne zeitliche Ausdehnung, d. h. bereits in der Planung festgelegte Termine, zu denen definierte, bedeutende Teilziele erreicht werden sollen.\n\nAls Ergebnis der Terminplanung liefert Microsoft Project auf Basis der definierten Vorgänge Termine. Möglich sind Endtermine (auf Basis des Starttermins) und Starttermine (auf Basis des Endtermins). Weitere Determinanten können sein: Stichtage, Termin- oder Ressourceneinschränkungen, Feiertage, Pufferzeiten.\nDarüber hinaus sind spezielle Terminberechnungen möglich:\n\nNeben dem Terminmanagement bietet MS Project auch den sehr umfangreichen Bereich „Ressourcenmanagement“. Hier wird einerseits der Blick auf die entstehenden Kosten eines Projektes geworfen und andererseits die Frage der Auslastung dargestellt. ‚Ressourcen‘ werden bei Microsoft Project eingeteilt in „Arbeit“, „Material“ oder „Kosten“. Die ‚Arbeits-Ressourcen‘ (= i. d. R. die Mitarbeiter im Projekt) ergeben durch die Zuordnung zu den entsprechenden Vorgängen den zu leistenden Aufwand (in Stunden) und damit die entstehenden Kosten. Mit ‚Material-Ressourcen‘ kann den einzelnen Vorgängen differenziert ihr jeweiliger Verbrauch zugewiesen werden. Mit ‚Kosten-Ressourcen‘ (z. B. Reisekosten) können den einzelnen Vorgängen Einzelkosten zugewiesen werden.\nAus der gesamten Kombination sieht der Projektleiter welche Kosten zu welchem Zeitpunkt entstehen und zu welchem Zeitpunkt bei einzelnen Mitarbeitern (Ressourcen) Überlastungen vorhanden sind.\n\nNach der Planungsphase wird in Microsoft Project ein sogenannter „Basisplan“ gespeichert, zu dem dann während des laufenden Projekts ein Soll-Ist-Vergleich erfolgt. So überwacht z. B. der Projektmanager den Fertigstellungswert durch die Eingabe der verbrauchten Zeiten in Microsoft Project. Ziel ist es, dem Projektleiter einen Überblick über entstandene Abweichungen und deren Auswirkungen auf das Projektziel zu ermöglichen.\nFür die Dokumentation und Präsentation stehen neben den grafischen Möglichkeiten Gantt-Charts, Netzplan und Kalender-Darstellungen eine große Anzahl an vorgefertigten Berichtsvorlagen zur Verfügung, die der Nutzer auch abändern und erweitern kann. Durch die Funktion „Grafische Berichte“ mit dem Export der Daten in die Microsoft-Produkte Microsoft Visio und Microsoft Excel erhält man die Möglichkeit, deren Funktionsumfang für weitere präsentierfähige Dokumentationen zu nutzen.\n\n\n\n"}
{"id": "799591", "url": "https://de.wikipedia.org/wiki?curid=799591", "title": "WinSCP", "text": "WinSCP\n\nWinSCP (\"Windows Secure Copy\") ist eine freie SFTP- und FTP-Client-Software für Windows. WinSCP kopiert Dateien zwischen lokalem und entferntem Computer mit diversen Protokollen: FTP, FTPS, SCP, SFTP, WebDAV oder S3. Dem Benutzer steht dabei je nach Präferenz eine graphische Benutzeroberfläche zur Verfügung, die entweder Norton Commander oder Windows-Explorer ähnelt. Daneben können erfahrene Benutzer die gesamten Funktionen von WinSCP mit .NET assembly oder einfachem Batch-Skripting automatisieren. \n\nWinSCP ist in C++ programmiert.\n\nWinSCP steht unter der GNU General Public License und ist damit freie Software. Der SSH- und SCP-Programmcode basiert auf PuTTY, der FTP-Programmcode basiert auf FileZilla.\n\nZusätzlich gibt es WinSCP als Plugin für die Filemanager Altap Salamander und FAR Manager. Auf dem FAR-Plugin basierend wird das um WebDAV erweiterte Plugin NetBox entwickelt.\n\n\n"}
{"id": "800339", "url": "https://de.wikipedia.org/wiki?curid=800339", "title": "IBM Personal Computer XT", "text": "IBM Personal Computer XT\n\nDer IBM Personal Computer XT (Typ 5160) oder einfach \"XT\" (für eXtended Technology), vorgestellt am 8. März 1983, ist eine geringfügige Weiterentwicklung des IBM PC. Beide Geräte, PC und PC XT, werden oft unter dem Kürzel PC/XT zusammengefasst und so vom technisch deutlich veränderten Nachfolgemodell IBM PC AT unterschieden.\n\nEr besitzt im Gegensatz zum originalen PC eine Festplatte (MFM) mit einer Kapazität von 10 MB, ein dementsprechend größeres Netzteil, einen ST-506-kompatiblen Festplattencontroller sowie mehr Steckplätze für Arbeitsspeicher und XT-Bus-Erweiterungskarten auf der Hauptplatine. Er wurde außerdem mit dem neueren PC-DOS 2.0 ausgeliefert, das erstmals die bei der Arbeit mit Festplatten fast unverzichtbaren Unterverzeichnisse unterstützte.\n\nWeiterhin gab es den XT als \"IBM 3270 PC\" mit Hardware zur Emulation eines Terminals vom Typ IBM 3270.\n\nDer \"IBM PC/370\" beinhaltete auf zwei Steckkarten eine /370-kompatible CPU. Der XT diente dabei nur als Ein-/Ausgabeeinheit. Der PC/370 wurde erfolgreich zur Programmentwicklung für IBM-Großrechner eingesetzt.\n\nEin spätes Modell des \"PC XT\", der \"PC XT 286\" (\"Typ 5162\") wurde erst am 2. September 1986 vorgestellt. Anstelle der 8088- besitzt er eine 80286-CPU, was ihn zu einer günstigeren Alternative zum bereits seit 1984 verfügbaren IBM PC AT machte. Obwohl beim \"XT 286\" die 80286-CPU nur mit 6 MHz statt 8 MHz bei \"PC AT\" getaktet war, erreicht er wegen seines schnelleren Arbeitsspeichers eine höhere Geschwindigkeit. Und er besitzt auch das bereits beim \"PC AT\" eingeführte 5,25″-Diskettenlaufwerk mit einer Speicherkapazität von 1,2 MByte mit halber Bauhöhe (1.75″).\n\nIBM fertigte außerdem den EMR (ElectroMagnetically Resistant) XT für militärische und diplomatische Einsatzgebiete. Er war durch sorgfältige Abschirmungsmaßnahmen gegen das „Abhören“ der elektromagnetischen Strahlung nach amerikanischem TEMPEST-Standard gesichert.\n\nDie PCs XT und XT 286 ließen sich mit der \"Expansion Unit Model 5161\" (Erweiterungseinheit) um ein identisches Gehäuse mit Platz für zusätzliche Laufwerke und Erweiterungskarten vergrößern. Dazu wurde in den XT eine Karte mit Treiberbausteinen eingesteckt, die den Bus des XT mit dem der Erweiterungseinheit über ein Kabel verband. Für den ursprünglichen PC gab es dieselbe Option.\n\n"}
{"id": "800370", "url": "https://de.wikipedia.org/wiki?curid=800370", "title": "IBM 3270 PC", "text": "IBM 3270 PC\n\nDer IBM 3270 PC (Typ 5271), vorgestellt im Oktober 1983, war ein IBM PC XT, der die zum Zeitpunkt der Veröffentlichung noch nötige Zusatzhardware zur Emulation eines Terminals vom Typ IBM 3270 beinhaltete. So konnte der 3270 PC sowohl als gewöhnlicher Einzelplatzrechner, als auch als Terminal im Host-Terminal-System eingesetzt werden.\n\nAußerdem war der \"3270 AT\" (Typ 5281) verfügbar. Er entspricht dem 3270 PC, jedoch auf Basis des IBM PC AT anstelle des PC XT als Grundmodell. Für eine Erläuterung der technischen Unterschiede zwischen den beiden Geräten, siehe IBM PC AT.\n\nDie Zusatzhardware belegte fast den kompletten Platz im Rechner, der für Erweiterungen vorgesehen war. Sie umfasste eine mehrteilige, programmierbare Grafikkarten-Erweiterung aus TTL-Bausteinen. Um den höheren Grafikanforderungen eines 3270 zu genügen, wurde eine CGA-kompatible Grafikkarte mit geringfügig höherer Auflösung verbaut. Eine weitere Erweiterungskarte fing die Signale der mitgelieferten PC/3270-Tastatur ab, verarbeitete die 3270-Tastencodes, und leitete die PC-Tastencodes zum normalen Tastaturanschluss weiter. Für die Verbindung zum Host-Rechner – meist ein IBM-Mainframe – sorgte eine weitere Steckkarte, welche eine 3270-kompatible serielle Schnittstelle mit BNC-Steckverbinder bereitstellte.\n\n\n\n"}
{"id": "800394", "url": "https://de.wikipedia.org/wiki?curid=800394", "title": "IBM Personal Computer/AT", "text": "IBM Personal Computer/AT\n\nDer IBM Personal Computer/AT (Typ 5170) oder einfach nur \"IBM AT\" (für Advanced Technology) oder \"PC AT\" oder \"PC/AT\" ist die dritte Generation von PCs aus dem Hause IBM. Er war der Nachfolger des IBM PC XT und IBM PC. Das System wurde am 14. August 1984 als Modell 5170 mit integrierter Festplatte und 6-MHz-CPU der Öffentlichkeit vorgestellt. Im Unterschied zu seinen beiden Vorgänger-Serien \"IBM PC\" und \"IBM PC XT\" verwendete IBM erstmals die 80286-Architektur von Intel und PC DOS 3.0 – das eigens für den AT entwickelt worden war. Da der \"PC AT\" über zahlreiche Neuerungen verfügte – er hatte einen Hauptprozessor mit Protected-Mode-Unterstützung, einen neuen 16-Bit-Systembus und erstmals ein nichtflüchtiges CMOS-RAM – war der Ausdruck „Advanced Technology“ damals sehr wohl gerechtfertigt.\n\nDer Formfaktor \"PC AT\" war für lange Zeit Standard für Mainboards und Gehäuse, egal ob es sich um einen IBM-PC oder einem kompatiblen Nachbau handelte. Das AT-Format wurde erst um die Jahrtausendwende durch das von Intel 1996 lizenzierte ATX-Format ersetzt bzw. an die heutigen Anforderungen angepasst.\n\nDer AT verwendete als CPU den Intel 80286 mit anfangs 6 MHz und später 8 MHz, der über den neuen Protected Mode der CPU eine Speichererweiterung bis auf 16 MB, erstmals bei einem IBM PC, ermöglichte. Im sogenannten Real Mode verhielt sich diese CPU abwärtskompatibel wie eine originale 8086-CPU.\n\nDie meisten anderen Hauptplatinen-Komponenten blieben die gleichen wie im Vorgängermodell IBM PC XT. Allerdings wurden jeweils zwei Interrupt- und DMA-Controller eingesetzt, statt nur je einem wie im XT. Dadurch stieg die Anzahl der Steckkarten, die konfliktfrei gleichzeitig betrieben werden konnten.\n\nDie Tastatur des AT bot im Wesentlichen drei Neuerungen: einen vom Rest der Tastatur deutlich abgesetzten Cursortasten/Zehnerblock, drei Leuchtdioden (LED), die den aktuellen Zustand der drei Umschalttasten Feststelltaste, Num-Taste und Rollen-Taste anzeigten sowie die neue Systemabfrage-Taste (international \"SysRq\", deutsch \"S-Abf\" beschriftet). Dieses AT-Tastaturlayout wurde dann bei IBM im Jahr 1987 durch die MF-II-Tastatur mit vom Zehnerblock abgetrennten Cursortasten ersetzt, andere Hersteller folgten etwas später. Die AT-Tastatur besaß zwar den gleichen 5-pol. DIN-Stecker, war aber elektrisch nicht kompatibel zur XT-Tastatur, u. a. da sie ganz andere Scancodes benutzte und zudem nun bidirektional arbeitete, um die Zustands-LEDs ansteuern zu können. PC-Kompatible Tastaturen anderer Hersteller besaßen vom Baujahr 1984 bis ca. 1990 oft einen XT-/AT-Modusumschalter auf der Unterseite.\n\nEine weitere Neuerung war der 16-Bit breite Systembus. Er benutzte den damals neu definierten 16-Bit-ISA-Bus und hob sich somit von der bisherigen PC-Architektur ab. Da IBM den Steckplatz einfach um einige Kontakte verlängerte, blieb der neue Bus logisch, elektrisch und mechanisch abwärtskompatibel und ermöglichte so auch den Betrieb der bisherigen 8-Bit-Karten.\n\nEbenfalls neu war das überarbeitete, flachere (nur halbe Bauhöhe, d. h. 1¾ Zoll) und anfangs sehr unzuverlässige 5¼″-Diskettenlaufwerk, erstmals mit hoher Kapazität (HD 1,2 MB), aber zugleich abwärtskompatibel zum alten 160/320- bzw. 180/360-kB-Standard.\n\nNeu waren auch die batteriegepufferte Uhr und das CMOS-RAM, erstmals konnten Konfigurationen direkt auf der Hauptplatine batteriegepuffert abgespeichert werden. Hiervon wurde vor allem bei Festplatten Gebrauch gemacht. Sie konnten nun ihre Geometrie-Daten permanent in der Hardware speichern, was die Festplatten-Controller einfacher und preisgünstiger machte. Versagte allerdings die Pufferbatterie, war in Konsequenz neben fehlender Uhrzeit (wie bisher) auch der Festplattenzugriff gestört. Diese Zugriffsinformationen waren unbedingt erforderlich, da die damaligen Festplatten keinerlei Auskunft über ihre Größe und Geometrie geben konnten, aber die Ansteuerung direkt, analog zur Plattenmechanik, über Spuren, Köpfe und Sektoren erfolgte. Aus Kompatibilitätsgründen gibt es diese Tabellen noch immer, jedoch werden bei heutigen Platten die Sektoren einfach linear über das (LBA) adressiert, das Produkt aus Spuren, Köpfen und Sektoren gibt beim Starten des Systems Auskunft über die eingesetzte Plattengröße.\n\nDer AT legte außerdem den Grundstein für fast alle nachfolgenden PC-AT-Klone anderer Hersteller, die als \"AT-kompatibel\" oder auch nur als \"IBM-kompatibel\" bezeichnet wurden. Selbst die heutigen als PC bekannten Rechner sind zumindest softwareseitig noch AT-kompatibel.\n\nWie alle IBM-PCs wurde auch der AT ursprünglich zu recht hohen Preisen verkauft. Da IBM kein Monopol auf die verwendeten Komponenten hatte (mit Ausnahme des BIOS), konnte Compaq, in Zusammenarbeit mit Microsoft, die das Betriebssystem \"Compaq-DOS\" lieferten, bereits 1983 den ersten zum IBM-PC kompatiblen Computer auf den Markt bringen. Da Microsoft das Betriebssystem PC-DOS nicht exklusiv für IBM entwickelt hatte und vertraglich auch nicht weiter gebunden war, lizenzierte man das fast identische MS-DOS frei an beliebige Kunden und Hersteller. Diese Chance ergriffen weltweit, und besonders in Asien, zahlreiche Hersteller. Beim AT wollte IBM nun gegensteuern und versuchte, sich nach der Markteinführung des \"IBM PC AT\" auch den Begriff \"AT\" schützen zu lassen. Da dies nicht gelang, nutzten bald alle Hersteller den Begriff \"AT\" als Standard für PCs mit 80286 oder schnelleren Prozessoren, die mit einem zum IBM-PC-DOS kompatiblen Betriebssystem (hauptsächlich MS-DOS) ausgeliefert wurden. Auch Intel veröffentlichte schon 1985 eine eigene AT-Hauptplatine (\"engl.\" ) mitsamt Gehäuse und Netzteil, das neben dem originalen \"IBM PC AT\" für viele Nachbauer, bis auf das letzte Schräubchen, als Vorlage diente. Vor allem in Südost-Asien schufen Unternehmen zahlreiche Nachbauten. Der sich so entwickelnde Markt führte durch den Konkurrenzkampf zu sinkenden Preisen und verstärkter Innovation.\n\nAb 1987 verabschiedete sich IBM vom AT-Standard und versuchte in einem zweiten Anlauf, mit den für die damalige Zeit technisch sehr hoch entwickelten PS/2-Systemen und dem Betriebssystem OS/2 wieder die Marktführerschaft zurückzuerlangen. Hierzu versuchte man bei IBM, mit proprietären Konzepten (u. a. die Micro Channel Architecture und OS/2) den Markt gegen Mitbewerber besser abzuschotten, allerdings zu Lasten der eigenen Kompatibilität. Die Konkurrenz unter Führung von Compaq, die bereits ein System auf Basis des i386 einen \"AT-386\" – der damals noch mit den alten ISA-Steckplätzen versehen war – im Angebot hatte, rebellierte. Gemeinsam entwickelten Hersteller wie Compaq, HP, Intel und Microsoft konkurrierende Konzepte und herstellerübergreifende Hardware- und Softwarestandards (z. B. EISA oder Windows) und konnten mit ihrer Marktmacht diese – im Gegensatz zu IBM – auch als Industriestandards durchsetzen. IBM vermochte seine PS/2-Modelle nur mit mäßigem Erfolg zu etablieren. In Konsequenz verschwand der überteuerte sowie inkompatible \"Micro Channel\" 1995 mit den letzten PS/2-Systemen vollständig vom Markt.\n\n"}
{"id": "801548", "url": "https://de.wikipedia.org/wiki?curid=801548", "title": "IBM PCjr", "text": "IBM PCjr\n\nDer IBM PCjr (Typ 4860) war eine Homecomputer-ähnliche und preiswertere Variante des IBM PC.\n\nEr wurde von IBM entwickelt, nachdem sich der vergleichsweise teure PC nicht als Rechner für den durchschnittlichen Heimanwender durchgesetzt hatte. Der am 1. November 1983 der Öffentlichkeit vorgestellte Rechner wurde jedoch aufgrund seines beschränkten Leistungsumfanges und immer noch hohen Preises ein Misserfolg, den IBM später vergeblich mit dem IBM PC JX wiedergutzumachen versuchte.\n\nEr verfügte wie der PC über einen Intel 8088 als CPU (zum Teil wurde auch ein Nachbau in Gestalt eines AMD D8088 verbaut), welcher mit 4,77 MHz getaktet war. Die Speicherausstattung betrug ebenfalls 64 KB, war jedoch (offiziell) nur auf 128 KB vergrößerbar. Der DMA-Controller wurde gegenüber dem PC eingespart, wodurch einige hardwarenah programmierte Anwendungen für den IBM PC auf dem PCjr nicht laufen. Auch gab es keine offizielle Möglichkeit zur Nachrüstung eines zweiten Diskettenlaufwerks oder zum Einbau einer Festplatte.\n\nDie Möglichkeit, dem Rechner mittels Erweiterungskarten neue Funktionen hinzuzufügen, beschränkte sich auf einen sogenannten \"Sidecar\"-Anschluss (engl. für Beiwagen), für den auch Speichererweiterungen bis insgesamt 512 KB verfügbar waren. Sidecar-Erweiterungen waren flache Gehäuse, die einfach an der rechten Seite an den Computer angesteckt werden konnten. Programme konnten wie beim PC über das 5,25\"-Diskettenlaufwerk oder einen Kassettenrecorder-Eingang geladen werden. Zusätzlich gab es an der Front zwei Steckplätze für Cartridges, die beispielsweise eine erweiterte Version des integrierten Microsoft BASIC oder Spiele enthalten konnten.\n\nEine weitere Besonderheit war die Tastatur, die kabellos über eine Infrarot-Schnittstelle an der Front mit dem Rechner verbunden wurde. Aus Kostengründen wurden dabei nicht die Tasten, sondern der Raum oberhalb der Tasten beschriftet – was die Nutzung erschwerte. Weitere Kritikpunkte waren die oft unzuverlässige Infrarotverbindung und der unpräzise Anschlag.\n\nDas Grafiksystem war Onboard auf der Hauptplatine integriert und ließ sich daher nicht aufrüsten. Es ist zum CGA-Standard kompatibel, verfügt zur besseren Positionierung auf dem Heimanwendermarkt allerdings zusätzlich über zwei weitere Grafikmodi, welche 16 Farben bei 320×200 Pixel sowie vier Farben bei einer Auflösung von 640×200 Pixel bereitstellten, allerdings auf Kosten des für Programme verfügbaren Arbeitsspeichers. Der PCjr wurde komplett mit Monitor geliefert. Ein weiteres Zugeständnis an den Heimanwendermarkt war das integrierte Soundsystem, welches auf einem TI 76496-Chip von Texas Instruments basierte, drei Stimmen gleichzeitig wiedergeben konnte und eine enorme Verbesserung gegenüber dem Systemlautsprecher des IBM-PC und PC XTs darstellte.\n\nDer PCjr kam in zwei Varianten: Dem \"Model 4\", welches den ursprünglichen PCjr darstellt, und dem \"Model 67\", einer erweiterten Version mit u. a. einem zusätzlichen Diskettenlaufwerk.\n\n"}
{"id": "802992", "url": "https://de.wikipedia.org/wiki?curid=802992", "title": "Binäre Exponentiation", "text": "Binäre Exponentiation\n\nDie binäre Exponentiation (auch Square-and-Multiply genannt) ist eine effiziente Methode zur Berechnung von natürlichen Potenzen, also Ausdrücken der Form formula_1 mit einer natürlichen Zahl formula_2.\n\nDieser Algorithmus wurde bereits um ca. 200 v. Chr. in Indien entdeckt und ist in einem Werk namens \"Chandah-sûtra\" niedergeschrieben.\n\nUm formula_3 zu berechnen, kann man entweder formula_4 ausrechnen (drei Multiplikationen) oder formula_5, formula_6 (zwei Multiplikationen), also formula_7.\n\nEbenso können auch andere ganzzahlige Potenzen durch „fortgesetztes Quadrieren und gelegentliches Multiplizieren“ effizient berechnet werden.\n\nDieses Einsparen von Multiplikationen funktioniert sowohl für reelle Zahlen als auch für reellwertige Matrizen, elliptische Kurven und beliebige andere Halbgruppen.\n\n\nDa die Binärdarstellung von formula_11 immer mit der Ziffer 1 beginnt – und so ebenfalls die Anweisung mit QM beginnt –, ergibt sich für die erste Anweisung QM in jedem Fall das Zwischenergebnis formula_12. Aus diesem Grund ergibt sich eine leicht vereinfachte Variante, bei der die erste Anweisung QM durch formula_10 ersetzt wird.\n\nSei k = 23. Die Binärdarstellung von 23 lautet \"10111\". Daraus ergibt sich nach den Ersetzungen \"QM Q QM QM QM\". Nach dem Streichen des führenden \"QM\"-Paares hat man \"Q QM QM QM\". Daraus können wir nun ablesen, dass der Rechenvorgang folgendermaßen auszusehen hat: „quadriere, quadriere, multipliziere mit formula_10, quadriere, multipliziere mit formula_10, quadriere, multipliziere mit formula_10“.\n\nFormal sieht das Ganze so aus:\nformula_17 bzw. sukzessive geschrieben:\n\nMan sieht am Beispiel, dass man sich mit Hilfe der binären Exponentiation einige Rechenschritte sparen kann.\nAnstatt von 22 Multiplikationen werden nur noch 7 benötigt, indem man viermal quadriert und dreimal mit formula_10 multipliziert.\n\nDer Algorithmus ist in zwei Varianten dargestellt. Variante 1 verwendet eine if-Bedingung, um an den entsprechenden Stellen zu multiplizieren. Variante 2 baut die if-Bedingung implizit in den arithmetischen Ausdruck ein.\n\nMan kann das Verfahren für eine Berechnung von Hand auch so gestalten, dass man zunächst die Basis oft genug quadriert und anschließend die richtigen Zahlen miteinander multipliziert. Dann ähnelt es der Russischen Bauernmultiplikation, welche die Multiplikation zweier Zahlen auf das Halbieren, Verdoppeln und Addieren von Zahlen zurückführt.\n\nDazu schreibt man den Exponenten links und die Basis rechts. Der Exponent wird schrittweise halbiert (das Ergebnis wird abgerundet) und die Basis schrittweise quadriert. Man streicht die Zeilen mit geradem Exponenten. Das Produkt der nichtgestrichenen rechten Zahlen ist die gesuchte Potenz.\n\nAnders als bei dem vorherigen Ansatz werden hier die benötigten Potenzen von formula_10 direkt aufmultipliziert. Diese Variante bietet sich an, wenn der Exponent formula_2 nicht explizit in Binärdarstellung vorliegt. Zur Veranschaulichung kann die Gleichheit formula_22 betrachtet werden.\n\nBestimmt werden soll formula_1, ohne formula_2 in Binärdarstellung vorliegen zu haben.\n\nJede natürliche Zahl formula_25 lässt sich eindeutig in formula_26 zerlegen, wobei formula_27. Aufgrund der Potenzgesetze ergibt sich\n\nDer letzte Ausdruck beinhaltet lediglich\n\nEine direkte Implementation in Haskell sähe wie folgt aus:\n\nDie Funktion codice_1, die hier definiert wird, stützt sich also auf ein vorhandenes codice_2 für die Multiplikation, codice_3 für die Abspaltung der untersten Binärstelle des Exponenten und, rekursiv, sich selbst.\n\nGeringfügige Optimierungen, wie etwa die Umwandlung in eine endrekursive Variante, führen im Wesentlichen zu oben genannten iterativen Algorithmen.\n\nBeim Rechnen modulo einer natürlichen Zahl ist eine leichte Modifikation anwendbar, die verhindert, dass die berechneten Zahlen zu groß werden: Man bildet nach jedem Quadrieren und Multiplizieren den Rest. Die zuvor vorgestellten Algorithmen können leicht durch diese Moduloperationen erweitert werden.\nDieses Verfahren wird beispielsweise bei RSA-Verschlüsselung angewendet.\n\nBei der einfachen und langsamen Potenzierung von formula_1 werden formula_32 Multiplikationen benötigt. Bei der binären Exponentiation wird die Schleife lediglich formula_33-mal durchlaufen (formula_33 entspricht hierbei ungefähr der Länge der Zahl formula_2 in der Binärdarstellung). In jedem Schleifendurchlauf kommt es zu einer Quadrierung (wobei die erste Quadrierung vernachlässigt werden kann) und eventuell einer Multiplikation. Asymptotisch werden formula_36 Operationen (eventuell Langzahloperationen) benötigt, wogegen formula_37 Operationen bei der einfachen Potenzierung zu Buche schlagen. formula_38 bezeichnet eine asymptotische obere Schranke für das Laufzeitverhalten des Algorithmus. Wie man leicht einsieht, ist die binäre Exponentiation sehr viel effizienter als das einfache Verfahren. Dieser verringerte Anspruch an die Rechenleistung ist bei großen Basen und Exponenten enorm.\n\nDie binäre Exponentiation muss nicht notwendig die multiplikationssparendste Berechnungsart einer Potenz sein. Um beispielsweise formula_39 zu berechnen, kann man entweder gemäß binärer Exponentiation\nberechnen (6 Multiplikationen), oder aber\n(insgesamt 5 Multiplikationen).\n\nEs wird formula_43 verwendet, wie in der Funktion codice_4 aus der STL. Statt codice_5 kann jeder beschränkte \"vorzeichenlose\" ganzzahlige Datentyp verwendet werden, falls nötig. Wird ein eigener Typ für Langzahlarithmetik (LZA) benutzt, so lässt sich codice_6 nicht wie hier bestimmen. Wichtig ist die Stelle codice_7, d. h. eine Bitmaske, die das höchstwertige Bit maskiert. Ob und wie dies effektiv möglich ist, hängt speziell vom LZA-Typ ab. Immer möglich ist das Kopieren des Wertes und das anschließende Löschen gesetzter Bits von hinten, bis die Kopie Null ist; oder ein anderes lineares Verfahren.\n// b: basis\n// e: Exponent\n// Annahmen: Typ T besitzt T operator*(T, T) und eine 1.\ntemplate<typename T>\nT bin_exp(T b, unsigned e)\n\n"}
{"id": "803893", "url": "https://de.wikipedia.org/wiki?curid=803893", "title": "Microsoft Operations Manager", "text": "Microsoft Operations Manager\n\nDer Microsoft Operations Manager (MOM oder SCOM) ist ein Server-Produkt von Microsoft. Es bietet Leistungsüberwachung, Ereignisanzeigen und -Auswertungen und applikationsspezifische Protokolldateien für Hard- und Software. Die derzeitige Version ist der \"Microsoft System Center Operations Manager 2016\" und wurde im September 2016 veröffentlicht. Schwerpunkte liegen in der Überwachung von Microsoft-Produkten. Mit der Version 2012 wurden auch Systeme wie Linux und Solaris eingebunden.\n\nDie Software ist in die System-Center-Produktfamilie des Herstellers eingebunden und verfügt unter anderem über Schnittstellen zu \"System Center Configuration Manager 2012\" (SCCM) und \"System Center Virtual Machine Manager 2012\".\n\nNetzwerkmanagementsysteme wie die System Center Produkte gehören zu den komplexeren Produkten im Bereich Software. Entsprechend aufwändig kann die Einführung der Lösungen ausfallen. Schulungen diverser Anbieter können dabei eine Hilfe sein. \n\nEin zentraler Punkt in SCOM sind die \"Management Packs\" (MPs). Diese werden von Microsoft und anderen Unternehmen vorkonfiguriert geliefert und enthalten ein Regelwerk mit Grenzwerten, wann die jeweils überwachte Software oder das System kritische Werte erreicht. Diese können auf verschiedenen Wegen an den Administrator berichtet werden. MPs liefern zu den berichteten Problemen auch Hinweise auf deren mögliche Ursachen und Lösungen sowie Vorschläge für Werkzeuge, die zu deren Bearbeitung geeignet sein könnten.\n\n\n"}
{"id": "804178", "url": "https://de.wikipedia.org/wiki?curid=804178", "title": "Speech Application Programming Interface", "text": "Speech Application Programming Interface\n\nDas Speech Application Programming Interface (SAPI) ist eine Programmierschnittstelle zur Sprachsynthese und Spracherkennung unter dem Betriebssystem Windows.\n\nDas SAPI-Paket gehört in den englischen, chinesischen und japanischen Windows-Versionen (ab Windows 2000) zum Lieferumfang, jedoch ist eine nachträgliche Installation auf allen Windows-Systemen möglich. Dazu bieten verschiedene Unternehmen unterschiedliche Erweiterungen an.\n\nDie aktuelle Hauptversion ist SAPI 5. Der Unterschied zu der älteren SAPI-4-Version sind aus Benutzersicht die geringere Anzahl von kostenlosen Sprachen für SAPI 5.\n\nMicrosoft stellt mit dem Microsoft Reader und dessen „Text zu Sprache“-Erweiterungen kostenlos folgende Sprachen für SAPI 5 zur Verfügung:\nDiese Erweiterung der SAPI setzt das Microsoft Speech SDK voraus.\n"}
{"id": "805798", "url": "https://de.wikipedia.org/wiki?curid=805798", "title": "Madagascar", "text": "Madagascar\n\nMadagascar ist ein computeranimierter Trickfilm. Er entstand unter dem Arbeitstitel \"„Wild Life“\" in den DreamWorks Animation Studios, die vor allem mit der \"Shrek\"-Reihe und dem Film \"Große Haie – Kleine Fische\" Erfolge feierte. 240 Mitarbeiter arbeiteten vier Jahre lang an der Vollendung des Filmes. In Deutschland sahen ihn mehr als 6,5 Mio. Kinozuschauer.\n\nVier New Yorker Zootiere – das abenteuerliebende Zebra Marty, der eitle Löwe Alex, die hypochondrische Giraffe Melman und die divenhafte Nilpferddame Gloria – machen nach Martys zehntem Geburtstag die Straßen außerhalb des Central Park Zoos unsicher. Erst kurz zuvor war der vorlaute Marty von den vier Pinguinen Skipper, Private, Kowalski und Rico zu diesem Fluchtplan überredet worden, denn eigentlich stehen Gloria, Melman und Alex, alle im Zoo verwöhnt, diesem Vorhaben sehr skeptisch gegenüber. Nach einer langen Nacht, die sie vor allem auf der Flucht vor der Polizei verbringen, finden sich die vier Freunde sowie die Gruppe der Pinguine und die Schimpansen Mason und Phil in Holzkisten auf einem Schiff in Richtung Kenias Nationalpark wieder.\n\nNachdem die verschwörerischen Pinguine die Kontrolle über das Schiff übernommen hatten und die Holzkisten der gefangenen Tiere vom Schiff gefallen sind, stranden Alex, Marty, Melman und Gloria auf Madagaskar. Bald merken die verwöhnten Tiere, wie es wirklich ist, in der Wildnis zu leben. Sie treffen auf eine Gruppe Lemuren, die zuerst ängstlich sind, vor allem wegen des Raubtiers Alex, doch schnell merken, dass sie von den Zootieren nichts zu befürchten haben. Julien, der König der Lemuren, will die \"„New York Giants“\" sogar als Verteidigung gegen die Fressfeinde der Lemuren, die Fossas, einsetzen.\n\nAls Alex hungriger und hungriger wird, erinnert er sich an seinen Jagdtrieb und fantasiert sogar davon, seinen besten Freund Marty zu fressen. Am liebsten möchte er aber einfach wieder nach Hause in den Zoo, wo er regelmäßig gefüttert wird und seinen Starruhm genießen kann.\n\nAm Ende gelingt es den Freunden, sowohl Alex’ Fresstrieb mit Fisch in den Griff zu bekommen als auch den Fossas zu entkommen. Da die Pinguine inzwischen wieder aus der Antarktis zurückgekehrt sind, bereitet sich die Truppe auf das Auslaufen Richtung New York vor. Allerdings ist der Tank des Schiffes leer.\n\nDie deutsche Synchronisation des Filmes übernahm die Berliner Synchron AG unter der Dialogregie und nach einem Synchronbuch von Michael Nowka.\nEs existieren deutsche Trailer mit anderen Synchronsprechern. Auffällig ist die Übereinstimmung der Synchronstimme von Ben Stiller (Oliver Rohrbeck) und dass die Pinguine nicht von den Fantastischen Vier gesprochen werden. Der im Fernsehen ausgestrahlte Trailer (Fernsehspot) stimmt mit den für den Film verwendeten Synchronsprechern überein.\n\nRico Pfirstinger schreibt für das Kinomagazin \"Cinema\": \"„Madagascar“ ist kein neuer „Findet Nemo!“ und verzichtet weitgehend auf die in diesem Genre üblichen Moralbotschaften. Kein großer Film – aber ein Riesenspaß.\"\n\nVon der Filmbewertungsstelle Wiesbaden (FBW) bekam der Film das Prädikat \"Besonders wertvoll\". In ihrer Begründung nannte die Bewertungstelle den Film „technisch perfekt“ und sieht in ihm „überzeugend gestaltete Familienunterhaltung mit Einfallsreichtum und Witz“.\n\nDer Film enthält drei mehrmals gespielte Musiktitel. Der eine ist \"Theme from New York, New York\", den die beiden Freunde Alex und Marty sowohl im Zoo (beide) als auch auf der Insel (nur Marty) singen. In der Szene des ersten Wiedersehens von Marty und Alex auf Madagaskar wird die Titelmelodie aus Die Stunde des Siegers von dem griechischen Komponisten Vangelis gespielt. Ein anderer bekannter Song ist \"I Like To Move It\" von Reel 2 Real aus dem Jahr 1994. Dies ist die Partyhymne der Lemuren und wird außerdem im Abspann gespielt. Der dritte Titel ist der des 1966 entstandenen Naturfilms \"Born Free – Frei geboren\". Er wird zu Anfang des Films in Martys Traumsequenz mit Gesang präsentiert. Anschließend wird es mehrmals orchestral angespielt. Ebenso wird die Titelmusik der Sendung \"National Geographic\" kurzfristig eingespielt, auch \"Boogie Wonderland\" von Earth, Wind and Fire und \"Staying alive\" von den Bee Gees. Des Weiteren wird \"What a wounderful world\" von Louis Armstrong als Original gespielt. Bei Martys Ankunft auf der Insel ist das Titellied von \"Hawaii Fünf-Null\" zu hören.\n\nDie Fortsetzung von \"Madagascar\" mit dem Titel \"Madagascar 2\" lief am 7. November 2008 in den USA und am 4. Dezember 2008 in Deutschland an. Außerdem ist der Film \"\" (auch in 3D) am 2. Oktober 2012 in Deutschland veröffentlicht worden.\n\nEin Spielfilm mit den Pinguinen aus \"Madagascar\" als Protagonisten kam 2014 unter dem Titel Die Pinguine aus Madagascar in die Kinos.\n\nAb dem 13. Oktober 2005 lief ein 11-minütiger Kurzfilm namens \"Die Madagascar-Pinguine in vorweihnachtlicher Mission\" als Vorfilm von \"Wallace & Gromit – Auf der Jagd nach dem Riesenkaninchen\" im Kino. Dieser Kurzfilm ist auch auf der \"Madagascar\"-DVD vorhanden. 2009 wurde ein weiteres Weihnachtsspecial mit dem Titel \"Fröhliches Madagascar\" produziert.\n\nEine eigene Serie der Madagascar-Pinguine läuft im deutschen Fernsehen seit April 2009 auf Nickelodeon unter dem Titel \"Die Pinguine aus Madagascar\" und spielt nicht im gleichen Universum wie dem der Filmreihe. Ein weiterer Ableger ist die seit 2014 produzierte animierte Fernsehserie \"King Julien\", die zeitlich vor dem ersten Kinofilm spielt und sich vor allem um den Lemuren Julien dreht.\n\n2005 erschien ein Action-Adventure zum Film.\n\n"}
{"id": "807465", "url": "https://de.wikipedia.org/wiki?curid=807465", "title": "X-Mix", "text": "X-Mix\n\nX-Mix ist eine zwischen 1993 und 1998 erschienene 10-teilige Compilation-Reihe des Berliner Labels STUD!O K7 und stellt die Nachfolgereihe der dreiteiligen 3Lux-Veröffentlichungen dar die sich erstmals der Visualisierung elektronischer Musik widmeten.\nDas Konzept bestand darin, elektronische Musik, vergleichbar mit einem Musikvideo, mittels Computeranimationen visuell darzustellen.\nWährend sich bei der 3Lux-Reihe der Fokus noch auf die Visuals richtete, konzentrierten sich X-Mix-Veröffentlichungen in erster Linie auf die Musikmixe die von namhaften DJs wie Laurent Garnier, John Acquaviva, Ken Ishii oder Dave Clarke in den Bereichen Acid, Techno, Trance und Electro produziert wurden. Somit wurde die Reihe neben VHS auch auf den Formaten CD und Vinyl veröffentlicht und später auch auf DVD.\n\n\n\n"}
{"id": "807530", "url": "https://de.wikipedia.org/wiki?curid=807530", "title": "Adobe InCopy", "text": "Adobe InCopy\n\nInCopy ist ein Texteditor-Programm von Adobe Systems, das ganz auf die Zusammenarbeit mit Adobe InDesign ausgelegt ist. Ein Großteil des Programmcodes ist identisch mit dem von InDesign, so dass ein Redakteur oder Texter absolut zeilenverbindlich Texte schreiben kann. Dabei kann der Redakteur Inhalte einzelner Textrahmen verändern, während gleichzeitig ein Layouter in InDesign die Seitengestaltung bearbeitet.\n\nAdobe InCopy ist das direkte Konkurrenzprodukt zu QuarkCopyDesk der Firma Quark, das seit 1991 auf dem Markt ist.\n\nInCopy funktioniert ähnlich wie andere Texteditor-Programme. Es stehen erweiterte Funktionen zur Verfügung wie zum Beispiel eine Änderungsverfolgung oder Notizfunktionen. Mit InCopy kann man InDesign-Dateien öffnen, so dass man genau den Zustand des Layouts sehen kann, während man Texte bearbeitet. Allerdings kann man die Seitengestaltung, zum Beispiel die Größe eines Textrahmens, nicht verändern. So wird eine strikte Aufgabenverteilung zwischen Textbearbeitung und Seitengestaltung in Arbeitsgruppen in Verlagen und Agenturen erreicht.\n\nInCopy kann in kleinen Arbeitsgruppen direkt mit InDesign genutzt werden. Für größere Arbeitsgruppen oder bei besonderen Anforderungen werden InDesign und InCopy häufig in Verbindung mit einem Redaktionssystem eingesetzt, das eine Steuerung des redaktionellen Workflow ermöglicht.\n\n\n"}
{"id": "808906", "url": "https://de.wikipedia.org/wiki?curid=808906", "title": "IBM 5100", "text": "IBM 5100\n\nIBM 5100 war der Name der ersten Serie von Mikrocomputern, die IBM 1975 vorstellte. Dazu gehörten sowohl (für damalige Zeiten) kompakte Systemeinheiten, sowie eine Auswahl an praktischer Peripherie. Die 5100er waren zwar die Vorgänger des IBM-PC (Typ 5150), ähneln diesem technisch jedoch in keiner Weise. Trotz des revolutionären Konzepts, das sehr praktisch war, wurde die Serie ein Flop.\n\nDer SCAMP (Special Computer, APL Machine Portable) stellte den Prototyp des 5100 Portable Computer (siehe unten) dar. Er bestand aus einem Gehäuse, das dem der damals üblichen Tischrechner ähnelte. Eingebaut war ein kleiner 5″-Monitor, eine komplette Tastatur mit Nummernfeld, sowie ein Bandlaufwerk. Als Prozessor diente eine eigens für diesen Rechner entwickelte Mehrchip-Lösung namens PALM. Als Software bzw. Betriebssystem waren ausschließlich die Programmiersprachen APL, BASIC, oder – per Schalter wählbar – beide, verfügbar.\n\nErstes frei verkäufliches Modell war der \"IBM Portable Computer\" (Typ 5100), der am 9. September 1975 mit der PALM-CPU auf den Markt kam. Erst bei den wesentlich späteren Modellen hatte die IBM auch Mikroprozessoren von Fremdherstellern benutzt, so beispielsweise den Intel 8085 beim System/23.\n\nDer 5100 entsprach weitestgehend dem SCAMP (siehe oben), besaß jedoch ein QIC-Bandlaufwerk für DC300-Kassetten mit 204 kB Kapazität. Je nach Ausstattung von Arbeitsspeicher und Programmiersprache schlug der 5100 mit $9.000 bis $20.000 zu Buche.\n\nAls Beispiel hier ein Codefragment für einen \"Internal Machine Fix\" (IMF) für den IBM 5100:\n\nIm Jahre 1978 kam die verbesserte Variante vom Typ 5110 Model 1 auf den Markt. Es wurden unter anderem die neuen externen Diskettenlaufwerke (siehe unten) und der damals bei IBM ansonsten gebräuchliche Zeichencode EBCDIC unterstützt; der 5100 verwendete noch einen eigenen Zeichencode.\n\nDer 5110 Model 2 entsprach dem Model 1, beinhaltete jedoch kein Bandlaufwerk und auch keine Anschlussmöglichkeit dafür.\n\nBeim 5110 Model 3 handelt es sich um die einzeln nicht erhältliche Systemeinheit des 5120, ein Paket aus eben genanntem Rechner und einem passenden Drucker. In die Systemeinheit waren ein größerer Monitor und zwei 8″-Diskettenlaufwerke integriert. Die Tastatur war fest vor dem Gerät angebracht. Mit fast 50 kg Gewicht ist der 5110 Model 3 der wohl schwerste Schreibtischrechner. Aus dem 5110 Model 3 wurde später das System/23 entwickelt, welches ein sehr ähnliches Gehäuse besitzt.\n\nPeripherie ließ sich an die Systemeinheiten über eine Kombination von drei Kabeln anschließen, die den Systembus repräsentieren. Die Geräte ließen sich kaskadieren. Beispiel: Systemeinheit ← Drucker ← Bandlaufwerk.\n\nDer Matrixdrucker 5103 war sowohl einzeln als auch als Bestandteil des Paketes 5120 in Ausführungen mit 80 oder 120 Zeichen pro Sekunde erhältlich.\n\nDie Diskettenstation war ein voluminöses Turmgehäuse mit zwei 8\"-Diskettenlaufwerken. Sie unterstützte unter anderem das Format IBM 3740 (welches bis heute bei diversen Computersystemen beibehalten wurde) und erlaubte das Speichern von bis zu 1,1 MB pro Diskette. Die 3740-Kompatibilität ermöglichte den Datenaustausch mit anderen Rechnern von IBM und sorgte zusammen mit der – beim Bandlaufwerk nicht vorhandenen – Möglichkeit, schnell und gezielt auf Daten zuzugreifen, dafür, dass sich die 5100er-Rechner nun auch für den täglichen Einsatz im Büro mit mehreren Arbeitsplätzen eigneten.\n\nDas Bandlaufwerk entspricht dem des 5100 bzw. 5110 Model 1, jedoch in eigenem Gehäuse.\n\nDes Weiteren war ein IEEE-488-Interface verfügbar, was eine breite Palette von bereits für andere Rechnersysteme verfügbaren Zubehör und vor allem diversen Geräten aus der Messtechnik für die Verwendung an Geräten der 5100er-Serie ermöglichte. Zusammen mit der Portabilität der Systemeinheiten wurde so eine Vielzahl von Feldanwendungen ermöglicht.\n\n"}
{"id": "808950", "url": "https://de.wikipedia.org/wiki?curid=808950", "title": "Subnotebook", "text": "Subnotebook\n\nSubnotebook ist die Bezeichnung für ein besonders kleines und leichtes Notebook. Gängige Bildschirmdiagonalen reichen bei diesen ultraportablen Notebooks von 10,6″ (27 cm) über 12,1″ (31 cm) bis hin zu maximal 13,37″ (34 cm). Displaygrößen kleiner als 10,6″ werden derzeit meist den Kleinstcomputern, z. B. Ultra-Mobile PCs (UMPCs) zugeordnet, größere ab 14,1″ (36 cm) den am häufigsten auftretenden Notebooks in den Bereichen Büroanwendung, Multimedia und PC-Spiele. Das Gewicht von derzeit marktfähigen Geräten liegt bei knapp unter einem bis hin zu maximal zwei Kilogramm. Die Ausstattung reicht häufig an ein vollwertiges Notebook heran, allerdings wird in der Regel auf optische Laufwerke verzichtet und die Tastatur verkleinert. Teilweise sind weniger Anschlüsse vorhanden.\n\nEs gibt Ausnahmen, die mit einer vollwertigen Tastatur oder auch mit einem optischen Laufwerk ausgestattet sind, diese sind dann jedoch etwas schwerer und/oder größer.\n\nDie Grenze zu den ebenfalls tastaturbetriebenen, jedoch noch kleineren Ultra-Mobile PCs ist fließend; In der Vergangenheit diente als Abgrenzungskriterium – neben der Displaygröße und damit dem Formfaktor – häufig auch das Vorhandensein eines vollwertigen Betriebssystems, wie es auch bei Desktop-Computern zum Einsatz kam. Physikalisch sowohl in der Größe, Gewicht wie auch der Rechenkraft grenzen sie abwärts an Tablet PCs sowie Smartphones. Diese sind jedoch typischerweise nicht wie die Subnotebooks in AMD64-Architektur konstruiert, sondern in ARM-Architektur.\n\nHäufig werden \"Subnotebooks\" aufgrund des kleineren Absatzmarktes im Vergleich zu Standard-Notebooks mit Bildschirmdiagonalen von z. B. 14″ (35,5 cm) oder größer zunächst nur in einer geringen Stückzahl hergestellt.\n\nNachteile dieser handlichen Geräte sind der kleine Bildschirm (vor allem geringere Bildschirmauflösungen), verkleinerte Tasten mit geringem Hub und teilweise – aufgrund der Größe und um Gewicht zu sparen – kleine Akkus, die die Laufzeit ohne Netzanschluss gegenüber vollwertigen Notebooks verringern. Es gibt jedoch auch Modelle, wo durch Kombination besonders sparsamer Prozessoren und teilweise bewusst weniger leistungsfähigen Komponenten eine sehr hohe Akkulaufzeit erzielt wird, die teilweise über 10 Stunden liegt. Der Kaufpreis ist in vielen Fällen entweder aufgrund der kompakten Bauweise oder der hohen Laufzeit deutlich höher als bei einem vergleichbaren Standard-Notebook.\n\nDer Compaq LTE, erschienen 1989, war der erste relativ bekannte Computer, der \"Notebook Computer\" genannt wurde, aufgrund seiner relativ geringen Größe von 27,9 × 21,68 × 4,88 cm, was in etwa der Größe einer A4-Seite entspricht. Nach Compaq startete IBM die ThinkPad-Reihe, deren Modelle einen 10,4″-Bildschirm nutzten und 29,7 × 21 × 5,6 cm groß waren. Kleinere portable Computer als diese beiden wurden relativ schnell \"Subnotebook\" genannt. Das NEC UltraLite erschien 1988 und wurde aufgrund seiner Größe von 29,8 × 21 × 3,5 cm beispielsweise schon so benannt. Kleinere Computer auf DOS-Basis wie z. B. der Poqet PC und das Atari Portfolio, beide 1989 erschienen, wurden hingegen \"Pocket PC\" oder \"Handheld\" genannt. \n\nEin 1992 veröffentlichtes Subnotebook, das Gateway Handbook (24,6 × 15,0 × 4,1 cm, 1,4 kg), nutzte gegen Ende 1993 einen 486er Prozessor. Bei einem anderen Subnotebook aus dem Jahr 1993, dem HP OmniBook 300, konnte die standardmäßig eingebaute Festplatte gegen Aufpreis durch eine optionale Solid State Disk ersetzt werden. Das Unternehmen Toshiba, das seit 1980 portable Computer baut, startete seine ultra-portable Portégé-Reihe im Jahr 1993 mit dem Portege T3400, welches einen monochromen 8,4″-Bildschirm besaß. Das Libretto 20 war hingegen noch kleiner (etwa so groß wie eine VHS-Videokassette) und hatte einen 6,1″-Bildschirm mit einer Festplattenkapazität von 270 MB. Für Aufsehen sorgte 1992 auch der Olivetti Quaderno, ein vollwertiger PC mit MS-DOS im Format heutiger Netbooks.\n\nApple brachte 1997 das PowerBook 2400C mit 10,4″-Bildschirm heraus. Dieses Notebook wurde von IBM co-designed, um das alte Powerbook Duo zu ersetzen. Danach führte Apple sein Subnotebook-Seqment durch das iBook G3 mit 12″-Monitor fort.\nIBM startete mit dem ThinkPad 240 (1,3 kg), der speziell für Geschäftsreisende gedacht war, in den Subnotebook-Markt. Dieses Subnotebook wurde durch die X-Reihe der ThinkPads ersetzt, die einen 12,1″-Bildschirm besaßen.\n1996, 1997 und 1998 erschien dann die \"Libretto\"-Subnotebook-Serie von Toshiba (210 × 165 × 33,4 mm). Diese Serie wurde erst ab 2005 wieder fortgesetzt.\n\nEines der für den heutigen Markt bemerkenswertesten Subnotebooks war Sonys auf einem Transmeta-Prozessor basierendes Vaio PCG-C1 VE, auch PictureBook genannt, mit einer Größe von 24,9 × 15,4 × 2,5 cm. Das Besondere an diesem Modell war damals, dass eine Kamera im Displaydeckel angebracht war, was bei vielen aktuellen Laptops wieder zu finden ist.\n2001 stellte Dell das Latitude L400 vor. Die Besonderheit an diesem Notebook war der veränderliche Takt der Intel-CPU.\n2005 wurde das FlyBook als Tablet PC vorgestellt, der einen lediglich 8,9″ großen Bildschirm besaß. Dieses Subnotebook war in vielen verschiedenen Farben erhältlich und wurde selbst in nicht-PC-Zeitschriften vorgestellt wie z. B. FHM, GQ usw.\nDas Samsung Q1 war ein UMPC, der eine sehr kleine Version eines Tablet PCs darstellte und mit Windows XP Tablet PC Edition 2005 betrieben wurde.\n\nIm August 2007 stellte Toshiba mit dem Portégé R500 ein vollwertiges Subnotebook mit einem Gewicht von unter 800 g vor. Den 12″-Bildschirm dieses Modells soll es auch in einer flexiblen und biegsamen Variante geben. In Verbindung mit einer 64-GB-Solid-State-Disk (SSD) kann dieses Modell starke Belastungen und Erschütterungen verkraften.\nApple stellte im Januar 2008 mit dem MacBook Air ein ultraportables Subnotebook mit einer Bauhöhe von lediglich 0,4 bis 1,94 cm vor (Maße: 32,5 × 22,7 × 1,94 cm, 1,36 kg). Es besitzt ein 13,3″-Display, einen integrierten Akku, fest integriertes RAM und kann ebenfalls mit einer 64-GB-Solid-State-Disk geordert werden. An Anschlüssen ist jedoch lediglich ein USB-Port, ein Kopfhöreranschluss und ein Mini-DVI-Anschluss vorhanden. Die Konnektivität muss per WLAN, Bluetooth oder einem USB-Ethernet-Adapter hergestellt werden.\n\nSeit 2007 gibt es zudem die Unterklasse der Netbooks, bei denen es sich um besonders preisgünstige und minimalistisch ausgestattete Subnotebooks handelt. Ein Beispiel dafür ist der Eee PC, den Asus Ende 2007 vorstellte. Auf diesem Rechner ist die Linux-Distribution Xandros installiert, er ist aber auch mit Windows lauffähig. Dieses Netbook führte schnell die Verkaufsranglisten von großen Internetkaufhäusern an. Allerdings hat es einen im Verhältnis zum Gehäuse sehr kleinen Bildschirm mit 7″ und einer Auflösung von nur 800 × 480 Pixeln.\n\nDer chinesische Hersteller Lenovo stellte im Frühjahr 2008 das Thinkpad x300 vor, das auf der Technologie der von IBM erworbenen Thinkpad-Patente beruht. Es wird ausschließlich mit einer 64-GB-Solid-State-Disk ausgeliefert, was den Einstiegspreis stark in die Höhe treibt, jedoch auch zu verbesserter Laufzeit im Akkubetrieb sowie verringerter Anfälligkeit gegenüber physischen Belastungen führt. Es bietet deutlich mehr Anschlussmöglichkeiten als z. B. das Mac Book Air und kann durch Tausch des optischen Laufwerks gegen einen zusätzlichen Akku sehr lange Betriebszeiten erreichen.\n\nAnfang 2008 kamen auch einige „Mittelklasse“-Subnotebooks auf den Markt, die die Lücke zwischen den knapp ausgestatteten Netbooks und den teuren High-End-Geräten zu schließen versuchen. Hier stehen besonders die osteuropäischen Marken wie die polnischen Hersteller NTT und Aristo in harter Konkurrenz zu asiatischen Herstellern wie Asus.\n\nIn einer Zeit, in der der Subnotebookmarkt stark expandierte und die Nachfrage von speziellen Bauteilen rasant stieg (siehe Intels ULV-CPUs), sah insbesondere Intel eine Chance, das Segment zu besetzen, und entwickelte eine spezielle Plattform. Der Marketingname hierzu ist „Ultrabook“ (AMD zog mit den „Ultrathin“ nach). Besondere Betonung des Marketings ist die schmale Bauform und die lange Akkulaufzeit.\n\nAnfang 2015 stellte LG den bisher leichtesten 14\"-Laptop vor, der mit seinen 0,98 kg leichter als ein 11.6\" MacBook Air ist.\n\n"}
{"id": "809798", "url": "https://de.wikipedia.org/wiki?curid=809798", "title": "Cel Shading", "text": "Cel Shading\n\nCel Shading oder Cel-shaded Animation (gelegentlich Toon-Shading) ist eine Technik zum nicht-fotorealistischen Rendern von 3D-Computergrafiken. Dabei erhalten Bilder oder Animationen, die auf einem digitalen 3D-Modell basieren, ein Aussehen, als wären sie von Hand im Stil eines Comics oder Zeichentrickfilms erstellt. Die Technik ist eine vergleichsweise neue Entwicklung im Gebiet der Computergrafik (seit 1997 im Film, seit 2000 bei Spielen), die bei der Produktion von Zeichentrickfilmen und in Computerspielen verwendet wird. Der englische Begriff \"cel\" kommt von \"celuloid\" (deutsch: Zelluloid), während \"toon\" vom englischsprachigen \"Cartoon\" abgeleitet ist. Beides sind Kunstworte.\n\nObwohl das Resultat von Cel Shading recht simpel wirkt, ist der Prozess der Entwicklung komplex.\n\nDer Prozess des Cel Shading beginnt mit einem typischen 3D-Modell. Der Unterschied zu traditionellen Render-Methoden besteht in der Art, wie diese Modelle auf dem Bildschirm dargestellt werden. Statt die Schattierungen auf dem Objekt weich verlaufen zu lassen, werden nur drei oder vier Helligkeitsstufen verwendet (in der Regel weiß, hellgrau und dunkelgrau, aber kein schwarz) und bei Oberflächen wird fast immer auf Texturen verzichtet und es werden stattdessen nur einzelne Farbtöne verwendet. Zur effizienten Implementierung werden die Graustufen in einer 1D-Textur gespeichert und dann wird der benötigte Grauwert über den Winkel zwischen dem Normalenvektor des Polygons und dem Vektor vom Polygon zur Lichtquelle berechnet. Der Kosinus dieses Winkels ist dann der gewünschte Punkt auf der Textur (1=Hell, 0=dunkel, bei Werten <0 wird die Textur von hinten beleuchtet und der Wert wird auf 0 gesetzt). Der Kosinus des Winkels lässt sich direkt über das Skalarprodukt der beiden Vektoren berechnen. Bei moderneren Grafikkarten können diese Berechnungen auch direkt in den Shadern durchgeführt werden.\n\nUm schwarze Linien darzustellen, die Konturen eines Objekts zu zeichnen, invertiert man Backface Culling, um Dreiecke, die eigentlich perspektivisch nach hinten sichtbar sind, schwarz darzustellen. Diese Linien müssen mehrmals mit leichten Variationen in der Invertierung gezeichnet werden, um dicke Striche zu erzeugen. Das Back Face Culling wird dann in den Normalzustand versetzt, um die Farbtöne und zusätzliche Texturen für das Objekt darzustellen. Abschließend wird das Gesamtbild über einen Z-Buffer zusammengesetzt. Das Resultat ist ein Objekt, das mit einer schwarzen Umrandung und sogar Konturen innerhalb der Objektoberfläche dargestellt wird.\n\nFilme und Animationen mit Cel Shading ermöglichen im Vergleich zu handgezeichneten Cartoons wesentlich aufwändigere Effekte wie Kameraschwenks oder eine Vielzahl bewegter Objekte, da diese (relativ günstig) gestaltet und berechnet und nicht (aufwändig) per Hand gezeichnet werden müssen. Auffällig ist dies etwa bei bewegten Hintergründen, wie z. B. beim Start des Raumschiffs in \"Futurama\". Hier wirkt die Bewegung des Hintergrunds deutlich realistischer als beispielsweise bei den \"Simpsons\", wo die Kamerafahrten in der Regel nur aus einem von rechts nach links bewegten Hintergrund bestehen und ein Objekt auf dem Vordergrund an der mehr oder minder gleichen Stelle bleibt. Dennoch wirkt Cel Shading oft etwas steril, weswegen Serien wie die \"Simpsons\" immer noch per Hand gezeichnet werden.\n\nPrinzipiell kommt bei allen Zeichentrick-Produktionen, die 3D-Modelle verwenden und die Ästhetik eines klassischen Zeichentrickfilms haben, Cel Shading zum Einsatz. Eine Auswahl:\n\n\nDie Filme \"Waking Life\" und \"A Scanner Darkly – Der dunkle Schirm\" wurden zunächst auf Film gedreht und anschließend mit dem Programm Rotoshop von Bob Sabiston digitalisiert und durch Rotoskopie verfremdet. Dadurch wirken diese Filme, als ob sie mit Cel Shading generiert worden wären.\n\nBei Computerspielen ist es eine künstlerische Frage, ob Cel Shading in der Grafik-Engine eingesetzt wird. Als die Technik neu war, gab es eine ganze Welle von Spielen (Modeerscheinung). Inzwischen findet Cel-Shading nur gezielt Verwendung. Für \"Ōkami\" von Clover Studios gibt es frühe Demo-Videos, die Spielszenen in der Ästhetik japanischer Malerei zeigen. Dies war für die PS2 zu rechenaufwändig. Im schließlich veröffentlichten Spiel ist die Cel-Shading-Grafik wesentlich konventioneller.\n\nEinige bekannte Computerspiele, die Cel Shading einsetzen:\n"}
{"id": "811963", "url": "https://de.wikipedia.org/wiki?curid=811963", "title": "Tastaturmaus", "text": "Tastaturmaus\n\nEine Tastaturmaus ist eine Eingabehilfe für die Computertastatur, die in verschiedenen Betriebssystemen verwendet wird, um mit dem Ziffernblock der Tastatur eine Computermaus zu emulieren. Mittels einer speziellen Tastenkombination kann die Tastaturmaus ein- und ausgeschaltet werden. Dabei wird der Mauszeiger mit den Pfeiltasten auf dem Ziffernblock gesteuert. Andere Tasten ersetzen die Klicks mit den verschiedenen Maustasten.\n\nUnter Microsoft Windows und Linux (KDE, Gnome) wird die Tastaturmaus mit der Tastenkombination \" + linke + \" aktiviert bzw. deaktiviert. (Ausnahme: Gnome 3.0 bietet diese Option unter einem ständig am Desktop sichtbaren Symbol für \"barrierefrei\".) Unter Microsoft Windows 10 ist jedoch eine einmalige Einrichtung in der Systemsteuerung unter dem Center für erleichterte Bedienung erforderlich.\n\nUnter Windows XP und Linux (KDE, Gnome) werden Klicks über die Taste auf dem Ziffernblock abgesetzt. Durch Drücken der Taste wird eine gehaltene Maustaste emuliert, die durch Drücken der Taste wieder aufgehoben wird. Welche Maustaste als geklickt bzw. gehalten emuliert wird, wird durch folgende Tasten bestimmt (und unter Windows XP – sofern konfiguriert – über ein Icon im Benachrichtigungsfeld dargestellt):\n\nGrafische Benutzeroberflächen sind zwar zu einem großen Teil auch per Tastatur bedienbar, aber prinzipiell auf den Gebrauch einer Maus o. ä. (Trackball, Touchpad) ausgelegt.\n\nGründe für den Einsatz von Tastaturmäusen auf Seiten des Systems:\n\nGründe auf Seiten des Nutzers: \n\n"}
{"id": "812500", "url": "https://de.wikipedia.org/wiki?curid=812500", "title": "Wrapper (Informationsextraktion)", "text": "Wrapper (Informationsextraktion)\n\nAls Wrapper bezeichnet man im Informatik-Teilbereich der Informationsextraktion eine Gruppe von speziellen Prozeduren zur automatischen Extrahierung von (semi-)strukturierten Daten aus einer bestimmten Datenquelle (Text). Dabei werden je nach Art der zu extrahierenden Datensätze unterschiedliche Wrapper benötigt. Im Zusammenhang mit Feature Subset Selection existieren zudem unterschiedliche Ansätze zur Auswahl einer optimalen Menge von Feature Subsets aus den Datensätzen.\n\nEin LR-Wrapper besteht aus formula_1 abgrenzenden Paaren formula_2\n\nforeach formula_3\n\nEinschränkungen:\n\n\n\n\nFolgende einfache Möglichkeiten der Auswahl bestehen:\n\n\nUm diese Probleme zu lösen, müssen andere Algorithmen zur Informationsextraktion verwendet werden, etwa ein nicht-deterministischer, adaptiver Mealy-Automat (z. B. \"SoftMealy\"), der diese Einschränkungen nicht besitzt.\n\n"}
{"id": "812532", "url": "https://de.wikipedia.org/wiki?curid=812532", "title": "VivaDesigner", "text": "VivaDesigner\n\nVivaDesigner (ehemals VivaPress) ist ein Satzprogramm von Viva Technology für Linux-, Mac-OS-X- und Windows-Betriebssysteme.\n\nEs gilt als ein gut gepflegtes und funktionsreiches DTP-Programm. Unter anderem als einzige (kommerzielle) Alternative zum jüngeren Scribus genießt die Software einige Bekanntheit. Auf anderen Plattformen positioniert es sich in Konkurrenz zu Adobe InDesign und QuarkXPress und bietet einen ähnlichen Funktionsumfang mit einer unterschiedlichen Benutzeroberfläche, die eher mit der von Corel Ventura 10 vergleichbar ist.\n\nEs gibt eine eingeschränkte (unter anderem ohne Rechtschreibprüfung, Silbentrennung, Pantone-Farbpaletten, PDF-Im- und -Export, Pantone- und HKS-Farbmodell) Version zur kostenlosen Nutzung für Privatzwecke und verschiedene kommerzielle Lizenzen.\nDie Benutzeroberfläche basiert auf Qt.\n\nDie Software entstand um 1990 als „VivaPress“ für das klassische Mac OS im Auftrag von Linotype, die es jedoch nicht abnahm, weswegen das Programm von Viva bis zur Marktreife weiterentwickelt und anschließend selbst vermarktet wurde. 1991 trat Viva mit VivaPress an die Öffentlichkeit.\n\nDas Programm bot Techniken, die in anderen Programmen bis dahin unbekannt waren, konnte aber gegen die etablierte Konkurrenz von PageMaker und QuarkXPress nur einen vergleichsweise geringen Marktanteil erobern.\n\nDurch die strategische Entscheidung, Belichtungsstudios eine kostenlose Version zur Verfügung zu stellen, wurde trotzdem sichergestellt, dass mit VivaPress angefertigte Satzdateien auch bei den meisten Belichtungsstudios verarbeitet werden konnten.\n\nDie aktuelle Version von VivaPress ist VivaPress Professional 3. Das Produkt konnte sich gegen die Marktmacht von QuarkXPress jedoch nicht international durchsetzen und wurde eingestellt, wenngleich die Satztechnologie kontinuierlich weiterentwickelt wurde, da das Unternehmen mit Ausschießsoftware, Lösungen für den Großdruck und für automatisierte Medienproduktionen (Publishing-Server) weiter im DTP-Markt aktiv war.\n\nIm Dezember 2004 kam die Software als VivaDesigner (zunächst als öffentliche Beta-Version) wieder für Windows, Linux und Mac OS X auf den Markt und ist inzwischen in der Version 9 verfügbar. Der VivaDesigner 9 wurde in vielen Bereichen verbessert und erweitert. Neben neuen Funktionen wurde insbesondere die Geschwindigkeit sowie die Dateikompatiblität mit Adobe InDesign über das offene IDML-Austauschformat von Adobe verbessert.\n\n"}
{"id": "813291", "url": "https://de.wikipedia.org/wiki?curid=813291", "title": "Microsoft Paint", "text": "Microsoft Paint\n\nMicrosoft Paint (MSPaint.exe, auch Paint genannt) ist eine in Microsoft Windows integrierte Grafiksoftware, die eine einfache Erstellung und Bearbeitung von Rastergrafiken erlaubt.\n\nUnter dem jetzigen Namen gibt es Paint seit Windows 95, davor hieß es \"Paintbrush\", hatte aber nur unwesentliche Unterschiede. Paintbrush ist nicht zu verwechseln mit dem eigenständigen Programm \"PC Paintbrush\" der Firma \"ZSoft\" (siehe auch PCX-Grafikformat).\n\nIm Juli 2017 erklärte Microsoft das Programm für veraltet. Die Integration in künftige Updates von Windows 10 und gar eventuelle neue Versionen von Windows ist fraglich.\nEs soll aber im Windows Store weiter angeboten werden.\n\n\n\nDas Programm beherrscht das Laden und Speichern der Grafikformate Bitmap (.bmp), JPG, PNG, GIF und TIFF. Die alten Formate MSP, PCX und RLE werden in neueren Paint-Versionen nicht mehr unterstützt.\n\nBitmaps können monochrom, mit 16 oder 256 Farben oder mit 24-Bit-Farbtiefe sein und als JPG, GIF oder PNG gespeichert werden.\n\nAn Zeichen-Werkzeugen stehen ein Zeichenstift, ein Füllwerkzeug, ein Radiergummi, eine Linienfunktion, eine Kurve mit zweimaliger Krümmung (kubische Bézierkurve), ein Ausschnittwerkzeug (rechteckig oder freihändig), eine Pipette zum Aufnehmen von Farbwerten, eine Lupe zur Vergrößerung oder Verkleinerung der Anzeige (seit Windows Vista zusätzlich in Form eines Schiebereglers, davor mithilfe wählbarer Vergrößerungs-Faktoren), ein Pinsel mit mehreren Optionen zur Strich-Ausführung, eine Sprühdose, ein Texteingabe-Werkzeug und verschiedene geometrische Figuren (Rechteck, Ellipse, Pfeile, Sterne) zur Verfügung.\n\nDie Bestimmung der Breite von Linien und Umrissen erfolgt, indem vor dem Anwenden eines Werkzeugs in einer separaten Auswahl die Strichstärke eingestellt wird.\n\nEs kann gewählt werden, ob mit geometrischen Werkzeugen erzeugte Flächen umrandet und mit Füllfarbe versehen werden oder nicht. Die Ränder und Füllungen können mit verschiedenen Texturen (Buntstift, Textmarker, Ölfarbe, Bleistift, Wasserfarbe) ausgestattet werden. Des Weiteren steht eine Farbpalette zur Verfügung, aus der mit der Maus die Vorder- und Hintergrundfarbe (1. und 2. Farbe) für das Werkzeug bestimmt werden kann. Beim Verschieben und Einfügen von Ausschnitten kann die 2. Farbe als transparente Farbe verwendet werden.\n\nPaint erlaubt das Führen der Werkzeuge auch mit der rechten Maustaste, sodass die 2. Farbe als Vordergrund- und die 1. Farbe als Hintergrundfarbe verwendet wird. Vor Windows 7 konnte die Hintergrundfarbe noch mit der rechten Maustaste aus der Palette ausgewählt werden.\n\nEs können Bildattribute eingestellt werden (Anzahl horizontaler und vertikaler Pixel), die in der Farbpalette vorgegebenen Farben können verändert, markierte Bildausschnitte gedreht werden (90 Grad-Winkel, kein freies Drehen). Stauchen und Strecken markierter Bereiche ist möglich, wobei beim Stauchen von Pixelgrafiken Details verloren gehen, während beim Strecken nicht nur das gewählte Detail, sondern auch die systembedingten umgebenden Darstellungsfehler vergrößert werden – es kommt deutlich zum Treppeneffekt.\n\nBeim Abspeichern kann zwischen Bitmap-Formaten verschiedener Farbtiefe gewählt werden. Das Abspeichern in einer geringeren Farbtiefe führt oftmals zu ungewollten Farbverlusten, da beim Umwandeln ins 16- und 256-Farb-Format nur in Standardfarben konvertiert wird, ungeachtet der tatsächlich verwendeten Farben. Alternativ definierte Farbpaletten werden beim Speichern beibehalten, können aber in Paint nicht definiert werden. Beim Abspeichern im GIF-Format, auch von bestehenden Grafiken, werden in neueren Versionen nur wiederum andere Standardfarben unterstützt, einfarbige Flächen in anderen Farben (darunter alle von Paint vorgegebenen Farben) werden in (mit dem Füllwerkzeug nicht bearbeitbare) Punktraster umgewandelt, was auch zu deutlich größeren Dateien führt. Das PNG-Format hat dieses Problem nicht.\n\nIn frühen Versionen (Paintbrush) war es möglich, Objekte mit den Pfeiltasten der Tastatur pixelweise zu verschieben, ohne diese Möglichkeit wird eine pixelgenaue Ansteuerung nur mit einer entsprechend sensiblen Maus erreicht.\n\nPaint behandelt im Bild befindliche Gebilde nicht als einzelne Objekte (Kreise, Rechtecke oder auch Texte), sondern – ähnlich einem Mosaik – als Ansammlung von Bildpunkten zu einer gesamten (Raster-)Grafik. Einmal angelegte Details können daher nicht mehr separat markiert und nachträglich verändert werden, sondern müssen – wie sonst eher von der Arbeit mit Papier und Bleistift vertraut – mit Werkzeugen wie dem Radiergummi oder durch Abdecken mit neuen Farbschichten beseitigt werden. Bei jedem Einsatz eines Werkzeugs speichert Paint zur Sicherheit den vorherigen Zustand der Grafik intern ab. Wird eine Operation durch den Nutzer rückgängig gemacht („Undo“), verwirft das Programm die Änderung und greift auf die Sicherung zurück. Die Sicherung erfolgte vor der Einführung von Windows Vista in Form des ganzen Bildes. Diese Methode unterscheidet sich von modernen Verfahren, welche nur Änderungen vermerken und gegebenenfalls zurücksetzen, und ist sehr speicherplatzintensiv, sodass höchstens drei aufeinander folgende Rückgangsschritte möglich waren. In neueren Versionen sind je nach Werkzeugeinsatz unterschiedlich viele Versionen abrufbar.\n\n"}
{"id": "813880", "url": "https://de.wikipedia.org/wiki?curid=813880", "title": "Video for Windows", "text": "Video for Windows\n\nVideo for Windows (VfW) ist eine Programmierschnittstelle von Microsoft Windows, die es erlaubt, Videosignale zu kodieren und dekodieren sowie von Aufnahmegeräten einzulesen. Es handelt sich dabei um die Standard-Schnittstelle für AVI.\n\nVideo for Windows wurde 1992 für Windows 3.1 eingeführt und war ursprünglich eine käuflich zu erwerbende Software zum Erstellen und Schneiden von Videos. Die Runtime-Version zum Abspielen der Videos wurde als kostenloses Add-on zur Verfügung gestellt und in den Folgeversionen von Windows standardmäßig integriert.\n\nViele neuere Mediaplayer benutzen an Stelle von VfW die DirectShow-Schnittstelle zum Abspielen von Mediendateien, da DirectShow die Mediensignale automatisch (mit Hilfe installierter Codecs, z. B. ffdshow) dekodiert. Dies ist vor allem dann von Vorteil, wenn – insbesondere neue – Codecs die (ältere) VfW-Schnittstelle nicht mehr unterstützen. Im Allgemeinen wird Video for Windows mehr und mehr von DirectShow und – auf Seiten der Aufnahme – von WDM-Treibern verdrängt.\n\n"}
{"id": "814412", "url": "https://de.wikipedia.org/wiki?curid=814412", "title": "E/OS", "text": "E/OS\n\nE/OS (\"Emulative Operating System\") ist eine freie, nicht mehr weiterentwickelte Linux-Distribution und verfolgte das Ziel, einmal alle Programme namhafter Betriebssysteme durch Emulation ausführen zu können.\n\nE/OS basiert auf einem Linux-Kernel und sollte als Ersatz, bzw. Alternative für gleich mehrere Betriebssysteme verwendbar werden, darunter Microsoft Windows, Mac OS, BeOS, OS/2, DOS und Linux. Durch die Lizenzierung durch die GNU General Public License wäre es möglich gewesen, eine unabhängige Alternative zu den genannten Systemen zu erhalten.\n\nSomit würde ein einziges Betriebssystem alle allgemein verbreiteten Programme ausführen können. Hierbei liegt auch die größte Herausforderung, da die Originale – bis auf das ebenfalls freie Linux – nicht öffentlich dokumentiert sind und somit nachempfunden werden müssen.\n\nDas Projekt wurde 1995 mit dem Ziel eines freien DOS gestartet. Es basierte bis 1998 alleinig auf FreeDOS, später auch auf Quelltext von Seal System.\n\nIm Jahr 2000 kam es bei dem E/OS-Projekt zu einem größeren Wechsel, der Fokus lag anschließend auf der Windows- und Unix-Emulation. Die Quellen umfassten nun Linux, FreeBSD, ReactOS, XFree86 und Wine.\n\nSeit dem 1. Juli 2008 wird das Projekt nicht mehr weiter entwickelt. Vor der Einstellung befand es sich in der Beta-Phase.\n\n"}
{"id": "814474", "url": "https://de.wikipedia.org/wiki?curid=814474", "title": "Calmira", "text": "Calmira\n\nCalmira ist eine quelloffene Windows-95-ähnliche Desktop-Umgebung für Windows (für Workgroups) 3.1x und für Windows NT 3.1/3.5x (Server).\n\nWährend der Vermarktung von Windows 95 gab es Bestrebungen, auch Windows 3.x mit einer modernen Oberfläche auszustatten. 1997 erschien die erste Version von Calmira. Dieses Programm ähnelt, wie auch sein Startmenü, der Oberfläche von Windows 95. Es wird vollständig über Initialisierungsdateien (statt über Einträge in der Systemregistrierung) verwaltet, die meisten Einstellungen sind direkt über die Optionen \"Einstellungen\" zugänglich. Calmira benötigt keine Installation, lediglich einen Starteintrag in der Datei „SYSTEM.INI“ im Systemverzeichnis. Beim ersten Start importiert es auf Wunsch die Programmgruppen und Einstellungen vom Programm-Manager. Es kann aber auch zu Testzwecken ohne Systemintegration parallel zum Programm-Manager verwendet werden.\n\nCalmira kann mit Delphi 1.0 weiterentwickelt und übersetzt werden, es bleibt dabei kostenlos.\n\nDie Entwicklung begann mit der erstmaligen Freigabe des Quellcodes im Februar 1997. November 1998 wurde es in Calmira Online II umbenannt. \nDie aktuelle Version von Calmira ist Calmira LFN 3.3, die im Unterschied zu früheren Versionen jetzt Unterstützung (im \"Calmira Explorer\") für lange Dateinamen bietet.\nCalmira war und ist heute noch sehr beliebt bei Windows 3.1x-Nutzern. \n\n\n\n\n\nDiese Versionsgeschichte stammt von der offiziellen Homepage von Calmira und wurde hier übersetzt.\n\n\n\nNeben Calmira sind auch weitere Oberflächen bekannt, von denen zumindest die PC Tools eine umfangreich konfigurierbare Systemoberfläche mit Taskleiste und Startmenü zur Verfügung stellen. Weiterentwickelt wurde das Programm nach 1995 noch von Symantec und als NT-Tools verkauft, danach wurde die Entwicklung eingestellt. Windows 95 und Windows 2000 und deren Nachfolger werden offiziell nicht mehr unterstützt.\n\n"}
{"id": "816161", "url": "https://de.wikipedia.org/wiki?curid=816161", "title": "DEF CON", "text": "DEF CON\n\nDEFCON (auch \"Def Con\") ist eine der größten Veranstaltungen für Hacker weltweit. Im Jahr 2007 hatte die Konferenz etwa 6 000 Besucher, 2017 konnte bereits circa 25.000 Besucher verzeichnet werden. Sie findet seit 1993 jährlich in Las Vegas, Nevada, statt. Innerhalb von drei Tagen wird über Neuigkeiten in der Computersicherheit berichtet. Die Veranstaltung besteht aus Vorträgen, Workshops und Treffen verschiedener Personen. In der Regel werden auch Wettbewerbe abgehalten. Der Bekannteste ist Capture the Flag (CTF). Hier geht es darum, eigens aufgebaute Computer oder Netzwerke zu verteidigen oder zu erobern. Gegründet wurde die DEFCON von Jeff Moss – auch bekannt als \"The Dark Tangent\" – welcher ebenfalls Black Hat Briefings gründete.\n\nMittlerweile gibt es viele internationale DEFCON-Gruppen, die in ihren Ländern das Wissen weitergeben und neue Ideen fördern.\n\nIm Jahr 2001 wurde der russische Programmierer Dmitri Witaljewitsch Skljarow auf der Konferenz verhaftet. Adobe warf ihm vor, Software zu verteilen, die Adobes E-Book-Format entschlüsseln kann.\n\nDer Reporterin Michelle Madigan wurde 2007 vorgeworfen, ohne Presseausweis zu filmen bzw. Tonaufnahmen zu machen.\n\nIm Jahr 2009 wurde eine Person wegen unerlaubten Waffenbesitzes in Las Vegas verhaftet.\nEine weitere Person knackte das Türschloss auf dem Dach des Riviera Hotels und wurde kurz darauf verhaftet.\n\nIn Folge des PRISM-Skandals war die Anwesenheit von Regierungsmitarbeitern zur DEF CON 2013 zum ersten Mal nicht erwünscht.\n\n\n"}
{"id": "816644", "url": "https://de.wikipedia.org/wiki?curid=816644", "title": "TeXnicCenter", "text": "TeXnicCenter\n\nTeXnicCenter \"(TXC)\" ist ein freier Texteditor für LaTeX-Dokumente für Windows.\n\nIntegrierte Funktionen erleichtern unter anderem die Strukturierung, Formatierung und Texthervorhebung der Dokumente – Einstellungen, die bei LaTeX als Markup-Befehle direkt in den Text geschrieben werden. Trotzdem ist TeXnicCenter kein WYSIWYG-Editor. Es wird weiterhin nur der LaTeX-Quelltext bearbeitet, allerdings mit integrierter Syntaxhervorhebung. TeXnicCenter bietet außerdem eine einfache Installation und Konfiguration, setzt jedoch ein bereits installiertes LaTeX-Programmpaket (z. B. MiKTeX) voraus.\n\nTeXnicCenter wurde in der Programmiersprache C++ geschrieben. Es steht unter der GNU General Public License (GPL).\n\n1999 begann Sven Wiegand die Entwicklung von TeXnicCenter und Ende 1999 erschien die erste Beta. Weitere Beta-Versionen wurden in unregelmäßigen Abständen veröffentlicht, Ende 2008 gab es den Release Candidate einer ersten Stable-Version.\n\nEinhergehend mit der neuen Version ergaben sich auch neue Verantwortlichkeiten für das Projekt. So ist Tino Weinkauf nun der offizielle Verwalter des Projektes, wohingegen Sven Wiegand die Funktion des Webmasters übernimmt.\n\nDie nächste Veröffentlichung erfolgte erst Ende 2012 mit einer Betaversion. Die daraus resultierende Version 2.0 Stable kam 2013 heraus.\n\nTeXnicCenter ist ein Texteditor-Programm, das zusätzliche Funktionen für das Arbeiten mit LaTeX anbietet und ist mit anderen integrierten Entwicklungsumgebungen für andere Programmiersprachen vergleichbar. Es hält LaTeX-Bausteine als Icons oder Tastenkombinationen bereit, sodass ein Eintippen der manchmal langen Bausteine überflüssig wird (z. B. für mathematische Zeichen, Textausrichtung, Schriftgrößen etc.). Weiterhin wird eine umfangreiche Projektverwaltung geboten, und externe Programme, wie BibTeX oder MakeIndex können leicht eingebunden werden. Das Übersetzen der LaTeX-Quelltexte in DVI, PostScript oder PDF wird ebenfalls per Tastenkombination oder Icon gesteuert. Treten Fehler auf, so kann per Mausklick direkt in den Quelltext zum Fehler gesprungen werden. TeXnicCenter unterstützt erst ab der Version 2.0 Alpha 1 Unicode als Zeichenkodierung.\n\nDas Programm wurde für die Zusammenarbeit mit der MiKTeX-Distribution entworfen. Bei der Installation wird MiKTeX erkannt und automatisch eingebunden. TeXnicCenter kann aber auch mit anderen TeX-Distributionen (proTeXt, TeX Live) verwendet werden. Ebenso wird das jeweilige unter Windows mit der Dateiendung *.pdf registrierte Standardprogramm selbstständig als Betrachter für die erstellten PDF-Dokumente ausgewählt.\n\nTeXnicCenter kann für die automatisierte Verwendung des eingestellten PDF-Viewers zur Anzeige von neu kompilierten PDF-Dateien eingerichtet werden. Über sogenannte DDE-Aufrufe können die PDF-Dateien sowohl im Viewer geöffnet als auch geschlossen werden. Letzteres ist nötig, um die Dateien neu kompilieren zu können. Die DDE-Aufrufe lassen sich bspw. einrichten für Adobe Reader oder PDF-XChange Viewer.\n\nMit dem Sumatra-PDF-Betrachter ist auch die Realisierung der sog. „forward/inverse search“ möglich. Dabei kann gezielt von einer Position im Quelltext zur entsprechenden Position in der PDF-Ausgabe gesprungen werden. Auch in umgekehrter Richtung ist die Navigation vom PDF-Dokument an die entsprechende Stelle im Quelltext möglich.\n\n\n"}
{"id": "816750", "url": "https://de.wikipedia.org/wiki?curid=816750", "title": "KVIrc", "text": "KVIrc\n\nKVIrc ist ein freier, grafischer IRC-Client für Windows, Linux und andere Unix-Derivate sowie Mac OS. Das \"K\" in \"K Visual IRC\" stand für die Abhängigkeit vom KDE, die allerdings ab Version 2.0.0 nicht mehr zwingend gegeben ist. Das Projekt startete 1998.\n\nKVIrc unterstützt mehrere simultane Verbindungen zu Servern (wahlweise mit SSL und/oder IPv6). Neben Unicode und den ISO-8859-*-Codierungen unterstützt KVIrc auch asiatische und die nativen Windows-Zeichensätze. Eine Besonderheit dabei ist die intelligente Kodierung, die es erlaubt Unicode (Linux/Unix-Standard) zusammen mit einem der anderen Zeichensätze wie Windows-1252 zu verwenden.\n\nZusätzlich zu den verbreiteten mIRC-Formatierungscodes für fette, unterstrichene oder farbige Schrift verfügt KVIrc über (abschaltbare) grafische Emoticons sowie die Möglichkeit, einen Avatar festzulegen, den alle kompatiblen IRC-Clients anfordern und anzeigen können.\n\nKVIrc lässt sich mit der bordeigenen Skriptsprache \"KVS\", basierend auf C++, sh und Perl ähnlich der mIRC-Skriptsprache MSL erweitern. Weiterhin kann man für Channel-, Query-, oder DCC-Verbindungen eine Verschlüsselung mittels Mircryption (kompatibel zu Blowfish) oder Rijndael aktivieren.\n\nMit dem \"split window mode\" lässt sich das Chatfenster in zwei einzelne unterteilen. Das eine Fenster enthält dann nur die Gespräche während das andere die Vorgänge im Channel – z. B. das Betreten und Verlassen von Benutzern – anzeigt. Die Fenstergröße ist dabei variabel.\n\nEine Socketüberwachung unterstützt den Entwickler und Benutzer zusätzlich dadurch, dass sie auf Wunsch den unaufbereiteten Datenaustausch mit dem IRC-Server anzeigt, womit man leicht Fehler und/oder Abweichung vom RFC feststellen kann.\n\nStabile Releases werden eher selten veröffentlicht. Allerdings werden auf der Website regelmäßig sogenannte Snapshots bereitgestellt, die neuere Funktionen erhalten. Anregungen der Benutzer, die über den offiziellen Chat kleinere Wünsche äußern, werden mitunter direkt umgesetzt und veröffentlicht. Seit August 2015 findet die Entwicklung via GitHub statt.\n\nMit der Veröffentlichung von KVIrc 4.0.0, Codename Insomnia, das Qt 4 nutzt, wurde der Support für KVIrc3 komplett eingestellt; die zurzeit in der Entwicklung befindliche Version KVIrc 5.0.0 unterstützt Qt 5.\n\n"}
{"id": "817438", "url": "https://de.wikipedia.org/wiki?curid=817438", "title": "Sabre (CRS)", "text": "Sabre (CRS)\n\nSabre (Akronym für \"Semi-Automatic Business Research Environment\"; IATA-Code \"1S\") ist eines von drei (Sabre, Travelport, Amadeus) bedeutenden Computerreservierungssystemen (CRS). Über ein Terminal angeschlossen, lassen sich die weltweite Verfügbarkeit von Flügen, Hotelbetten, Zugfahrkarten und anderer Dienstleistungen prüfen und Buchungen durchführen.\n\nDie Idee zu einem solchen System hatte 1953 Cyrus Rowlett Smith, damaliger CEO von American Airlines, auf einem Transkontinentalflug von Los Angeles nach New York als er neben einem Mitarbeiter von IBM saß, der in das SAGE-Projekt zur Abwehr feindlicher Bomberangriffe auf Nordamerika involviert war.\n\nAus der Erfahrung des Projekts und den Ansprüchen eines elektronischen Buchungssystems entstand bis 1960 in Briarcliff Manor im US-Bundesstaat New York auf Basis von zwei IBM 7090-Mainframes das erste computergesteuerte Projekt mit einer Kapazität von 84.000 Telefonanrufen pro Tag. Bis 1964 hatte American Airlines über tausend Teilnehmer über das Telefonnetz in 60 Städten der USA angeschlossen. Es war einer der größten privaten Datenverarbeiter der Welt und wurde zu einer Art Prototyp für andere Fluggesellschaften, die ihre Reservierungssysteme ebenfalls umstellten.\n\nDie Großrechner wurden 1972 gegen neuere IBM System/360 ausgetauscht und nach Tulsa, Oklahoma unter die Erde verlegt. Ab 1976 schloss der Konkurrent United Airlines mit \"Apollo\", aus dem später Galileo hervorging, erstmals Reisebüros an ihr System an. Noch im selben Jahr folgte Sabre. 1983 wurden Agenturen in Kanada angeschlossen, drei Jahre später folgte mit Großbritannien das erste Land in Europa.\n\nEinige Eisenbahngesellschaften, wie Amtrak nutzen die Technik von Sabre. Der Betreiber des Eurotunnels, die Eurostar Group, stellte auf das System von Amadeus (CRS) um. \n\nAn dem japanischen Marktführer AXESS, einer Tochtergesellschaft von Japan Airlines, beteiligte sich das amerikanische Unternehmen 1995 mit 25 Prozent und tauschte zum Beispiel Passagierinformationen aus. Das Reservierungssystem \"FANTASIA\", von der australischen Fluggesellschaft Qantas Airways 1989 gegründet, wurde angeschlossen. Mit Abacus, im Mai 1988 von Cathay Pacific Airways und Singapore Airlines gegründet mit 11.000 angebundenen Reisebüros, hat sich ein weiterer Anbieter, der vorwiegend im pazifischen Raum aktiv ist, für eine Zusammenarbeit mit Sabre entschieden.\n\nIm Mai 1998 schaltete die Fluggesellschaft US Airways ihre Computer ab und verlagerte alle ihre Flugdaten in den Rechner nach Oklahoma. Mitte November 2000 wurden die Daten des INFINI, einer Tochter von Abacus und All Nippon Airways mit über 6000 angeschlossenen Reiseagenturen, erfolgreich in den Großrechner integriert. In Europa ist die italienische Alitalia einer der größten Anwender von Sabre.\n\nUm eine Verbindung zu dem Computersystem aufzubauen, benötigten die Benutzer eine gesicherte Leitung, zum Beispiel über eine Standleitung mit Nortel-VPN-Box oder eine verschlüsselte Internetverbindung. Jeder Agentur wird ein vierstelliger alphanumerischer \"Pseudo City Code\" (PCC) zugewiesen, der einen Pool von Adressen für Endgeräte enthält, wie Computer oder Drucker für ATB2- oder E-Flugtickets. Über einen Emulator können angemeldete Teilnehmer mit Befehlen (\"Format\") Informationen abfragen, wie zum Beispiel Fluginformationen, Preise oder das Wetter in der Zielregion. \n\nAls Beispiel eine Verfügbarkeitsabfrage im Java-Terminal: \n\nFlugdetails: Thai Airways\nFür jeden Fluggast wird ein Passenger Name Record erstellt, der im System gespeichert bleibt. \n\nIn Deutschland gibt es mit der Java-Anwendung \"Merlin\" als touristisches Reservierungssystem eine einfache Möglichkeit Reisen zu buchen, die ohne Kommandos und Sonderzeichen auskommt. Hiermit können Benutzer über eine grafische Oberfläche verschiedene Reiseveranstalter per Maus anwählen.\n\nHeute sind über 53.000 Reiseagenturen, 400 Fluggesellschaften, tausende von Hotelbetten, sogar Taxis und Kreuzfahrtschiffe über das Reservierungssystem miteinander verbunden. Eigentümerin ist die Sabre Holding. Neben dem europäischen Wettbewerber Amadeus ist der eigentliche Betreiber, die Tochtergesellschaft \"Sabre Travel Network\", einer der beiden größten Anbieter auf diesem Markt.\n\n"}
{"id": "817951", "url": "https://de.wikipedia.org/wiki?curid=817951", "title": "Histogramm-Differenz", "text": "Histogramm-Differenz\n\nDie Histogramm-Differenz (Abkürzung HD) ist eine positive Zahl, die durch Bildung der Differenz zweier Histogramme entsteht. Sie dient als Maß für die Unterschiedlichkeit zweier Histogramme und findet Anwendung in der Schnitterkennung und der Bildverarbeitung.\n\nMan unterscheidet zahlreiche Formen von Histogramm-Differenzen, wobei die folgenden einer hohen Verbreitung unterliegen: die absolute Histogramm-Differenz formula_1, die quadrierte Histogramm-Differenz formula_2 und die absolute Differenz der kumulierten Histogramme formula_3. Die formula_3 ist dabei die Implementierung Earth Mover's Distance für eindimensionale Diagramme. Spricht man von „der“ Histogramm-Differenz, so ist stets die absolute Histogramm-Differenz gemeint. Weitere Formen der Histogramm-Differenz werden aus der Differenz der kontinuierlichen Wahrscheinlichkeitsdichtefunktionen abgeleitet, wie die Kullback-Leibler-Divergenz oder die Jensen-Shannon-Divergenz.\n\nDie unterschiedlichen Formen der Histogramm-Differenz unterscheiden sich hinsichtlich ihrer Empfindlichkeit gegenüber Unterschieden zwischen den Histogrammen.\n\nHistogramme sind Abbildungen von einer Definitionsmenge in eine Wertemenge formula_5.\n\nEs entspricht dabei einer diskretisierten Wahrscheinlichkeitsdichtefunktion.\n\nEin kumuliertes Histogramm ist eine alternative Darstellungsform eines Histogramms. Definitionsmenge und Wertebereich bleiben gleich, das kumulierte Histogramm ergibt sich aus der Formel:\n\n\nAlle drei Differenzen sind positiv semidefinit, also stets formula_9.\n\nBilder und Histogramme werden in der Informatik im Allgemeinen durch die folgenden Datentypen repräsentiert:\n\nHierbei bezeichnet w die Breite und h die Höhe des Bildes in Pixeln, während d die Anzahl aller möglichen Farbwerte der Bilder bezeichnet.\n\nDer Algorithmus zur Ermittlung der Histogramm-Differenz setzt sich dann aus den folgenden Teilen zusammen:\n\nDer Algorithmus hat eine Komplexität von formula_10, wobei b die Breite und h die Höhe der Bilder in Pixeln und f die Anzahl der verschiedenen Farben bezeichnet.\n"}
{"id": "819066", "url": "https://de.wikipedia.org/wiki?curid=819066", "title": "DVD Decrypter", "text": "DVD Decrypter\n\nDVD Decrypter war ein Freeware-Computerprogramm, das von \"LIGHTNING UK!\" entwickelt wurde. Es ermöglicht, Inhalte einer Film-DVD auf der Festplatte zu speichern.\n\nDie erste Version von DVD Decrypter erschien im November 2000. Ab der am 2. September 2001 veröffentlichten, völlig neu erstellten Version 3.0 wurde das Programm mehrmals komplett überarbeitet.\n\nDa das Programm mit der Möglichkeit auf Umgehung mehrerer DVD-Verschlüsselungstechniken gegen ein im Oktober 2003 verabschiedetes Kopierschutzgesetz verstößt, wurde \"LIGHTNING UK!\" im Juni 2005 gezwungen, die Entwicklung unverzüglich einzustellen. Infolgedessen erwarb der Kopierschutzhersteller Macrovision Corporation sämtliche Rechte an der Software und ließ diese von den meisten Mirrors entfernen.\n\nSeitdem entwickelt \"LIGHTNING UK!\" die Freeware ImgBurn, die zwar „in Bezug auf die Brennfunktion“ des \"DVD Decrypter\"s als dessen inoffizieller Nachfolger gilt, aber z. B. keine Kopierschutz-Umgehungsmechanismen mehr beinhaltet.\n"}
{"id": "819320", "url": "https://de.wikipedia.org/wiki?curid=819320", "title": "Image Warping", "text": "Image Warping\n\nWarping (von englisch \"warp\" = verformen, verzerren) von Bildern gehört in der Computergrafik zu den bildbasierten Techniken. Falls zu einem Bild die dazugehörigen Tiefenwerte existieren, ist es mittels der Warping-Gleichung möglich, das Bild von einem anderen Blickpunkt zu betrachten. Das Verfahren ist echtzeitfähig, bringt jedoch einige Artefakte wie beispielsweise Aufdeck- oder Verdeckungsfehler mit sich.\n\n\n"}
{"id": "819595", "url": "https://de.wikipedia.org/wiki?curid=819595", "title": "PicMaster", "text": "PicMaster\n\nPicMaster ist eine Grafiksoftware, die nach dem Shareware-Prinzip vertrieben wird. Die Software vereint mehrere Anwendungsfelder für den Grafikbereich. So finden sich in PicMaster neben grundlegenden Bildbearbeitungsfunktionen (mit Pinsel malen, rote Augen entfernen, Text einfügen etc.) auch erweiterte Funktionen wie die Darstellung von stereoskopen Bildern, Morphing und eine Webcam-Integration.\n\nFür die Erstellung eines 3D-Bildes lassen sich mit einer Digitalkamera zwei leicht versetzte Fotos schießen, die mit Hilfe der Software und einer Farbbrille so präpariert werden können, dass ein 3D-Eindruck entsteht.\n\nBeim Morphen lassen sich Fotos von Personen in andere Personen verwandeln. Dazu werden die Personen fotografiert und deren Kontur mit Hilfe der Software ausgeschnitten. Anschließend können gemeinsame Stützpunkte auf den markanten Stellen (Augen, Nase, Mund, etc.) definiert werden. Sie Software berechnet die Zwischenbilder und erstellt ein Video der Verwandlung.\n\nPicMaster bietet die Möglichkeit, selbständig Bilder von einer Kamera (z. B. Webcam) einzufangen, um sie dann über das Internet zu übertragen (per FTP, oder die Software bietet die Bilder selbst als Webserver an). So lässt sich das System beispielsweise zur Überwachung oder als Alarmanlage verwenden.\n\nDie Software besitzt bereits 300 Grafikfiltereffekte, die in Form von Formeln beschrieben werden. Durch den integrierten Formeleditor und Parser ist es möglich, eigene Bildeffekte schnell zu erstellen. Für weitere Filter lassen sich auch Photoshop-Plug-ins in PicMaster einbinden.\n\nMit der Mosaikfunktion lässt sich ein Bild aus Einzelbildern erstellen. Aus den Einzelbildern wird jeweils das am besten geeignete Bild herausgesucht, so dass von weitem betrachtet die vielen Einzelbilder ein neues Gesamtbild ergeben. Mit der Posterdruckfunktion kann das große Gesamtbild über mehrere Seiten ausgedruckt werden.\n\nFür das Scannen von Bildern bietet das Programm mehrere Hilfsfunktionen an. So lassen sich beispielsweise mehrere Fotos auf einen Scanner legen und mit einem Knopfdruck automatisch\n\n\nZudem können die einzelnen Arbeitsschritte als Makro aufgenommen und wieder abgespielt werden. Dazu lassen sich die Aktionen in einer Textdatei abspeichern und zu einem späteren Zeitpunkt in Stapelverarbeitung auf mehrere Bilder anwenden. \n\nWeitere Funktionen sind:\n"}
{"id": "819731", "url": "https://de.wikipedia.org/wiki?curid=819731", "title": "QuarkCopyDesk", "text": "QuarkCopyDesk\n\nQuarkCopyDesk (oft auch nur \"CopyDesk\") ist ein Texteditor-Programm der Firma Quark, das auf die Zusammenarbeit mit QuarkXPress ausgelegt ist.\n\nMit CopyDesk kann ein Redakteur entweder ein neues Textdokument mit Stilvorlagen aus QuarkXPress erzeugen oder ein bestehendes Layout aus QuarkXPress mit Texten oder Bildern füllen. Da CopyDesk und QuarkXPress vom gleichen Quelltext erzeugt werden und somit auch die identische Textengine besitzen, sieht der Redakteur nicht nur das Layout wie in QuarkXPress, sondern auch den Text zeilenverbindlich. Umbruch, Länge und Typografie des Textes sieht zu 100 % so aus, wie es später in QuarkXPress aussehen wird.\n\nMeist wird CopyDesk im Zusammenhang mit Redaktionssystemen benutzt, wie z. B. QPS. Seit CopyDesk 2.2 kann man das Programm auch einzeln erwerben und (ohne Redaktionssystem im Zusammenhang mit QuarkXPress) nutzen.\n\nDirekte Konkurrenz zu CopyDesk ist seit Ende 1999 Adobe InCopy.\n\nCopyDesk funktioniert ähnlich wie andere Texteditor-Programme, mit dem Unterschied, dass Texte genauso umbrochen werden wie in QuarkXPress.\n\nCopyDesk hat drei Ansichtsmodi, eine WYSIWYG-Ansicht, eine Spaltenansicht und eine Textansicht, zwischen denen der Redakteur umschalten kann. Da CopyDesk mehrere Ansichtsfenster erlaubt, kann man auch zwei oder drei Ansichtsmodi parallel darstellen.\n\nDer Textmodus sieht aus wie bei jedem anderen Texteditor, hier wird nur der Text im Fenster dargestellt. Im Spaltenmodus sieht der Redakteur die Textfahne, einen Zeilenzähler sowie den gleichen Umbruch wie in QuarkXPress. Somit können unschöne Trennungen beurteilt werden und vor allem, ob der Text ins Layout passt. Im WYSIWYG-Modus zeigt CopyDesk das Layout wie in QuarkXPress (ohne dabei Zugriff auf das Original QuarkXPress-Dokument haben zu müssen), der Redakteur kann jedoch nicht die Geometrien der Rahmen oder das Layout ändern.\n\nCopyDesk erlaubt es auch, Änderungen zu verfolgen und visuell anzuzeigen, so dass Versionsänderungen am Bildschirm sichtbar gemacht werden können. Zusätzlich können Notizen eingefügt werden, die auch in QuarkXPress sichtbar werden.\n\nCopyDesk erlaubt es, den Text mittels Komponenten strukturiert anzulegen, also z. B. eine Komponente als Überschrift, eine als Fließtext und eine dritte als Bildunterschrift zu kennzeichnen, die dann getrennt oder gemeinsam ins Layout einfließen können.\n\nAb CopyDesk 7 kann der Redakteur auch Bilder in Rahmen einladen, eine visuelle Kennzeichnung in der Maßpalette zeigt dabei, ob die Qualität der Bilder ausreicht.\n\nGenerell gilt in CopyDesk, dass ein Redakteur nur Texte bearbeiten und Bilder einladen kann, somit wird eine strikte Aufgabenverteilung zwischen Textbearbeitung und Seitengestaltung erreicht. Diese getrennte Arbeitsweise ist oft in RedaktionsWorkflows gewünscht. \n\n\n"}
{"id": "820366", "url": "https://de.wikipedia.org/wiki?curid=820366", "title": "Worldspan (CRS)", "text": "Worldspan (CRS)\n\nWorldspan ist ein amerikanisches Unternehmen, das am 7. Februar 1990 von den Fluggesellschaften Northwest Airlines, Delta Air Lines und der ehemaligen Trans World Airlines mit Hauptsitz in Atlanta gegründet wurde.\n\nEs entstand auf Grundlage einer Vereinbarung der Fluggesellschaften ihre bisherigen Computerreservierungssysteme \"DATAS II\" von Delta Air Lines und \"PARS\" vom Trans World Airlines zu einem System zu vereinen. 1993 war die Einsatzzentrale in Atlanta voll einsatzbereit. Der dortige Großrechner hat eine Größe von zwei Fußballfeldern und verarbeitet die Anfragen in Echtzeit. \n\nInzwischen hat das Unternehmen über zweitausend Mitarbeiter, unterhält Außenstellen in Südamerika und Kanada. In Frankfurt am Main befindet sich die deutsche Niederlassung. Worldspan, eines der vier großen, weltweiten Reservierungssysteme (neben Amadeus, Sabre und Galileo), wurde am 30. Juni 2003 an eine US-amerikanische Investmentgesellschaft verkauft. Am 21. August 2007 hat Travelport Inc., zu der auch der Worldspan-Mitbewerber Galileo gehört, Worldspan übernommen und begonnen, die beiden Reservierungssysteme zusammenzuführen.\n\nOnline-Reisebüros wie Expedia oder Orbitz nutzen das Computerreservierungssystem zur Buchung von Flügen, Hotels oder anderer Dienstleistungen.\n\nUm eine Verbindung zu dem Computersystem aufzubauen, benötigten die Benutzer eine gesicherte Leitung, zum Beispiel über eine Standleitung oder eine verschlüsselte Internetverbindung. Da Standleitungen mit nicht unerheblichen Kosten und Aufwand verbunden sind, geht der Trend vermehrt zu verschlüsselter Kommunikation über das Internet. Jeder Agentur wird ein dreistelliger alphanumerischer Code (genannt SID) zugewiesen, der einen Pool von Adressen für Endgeräte enthält, wie Computer oder Drucker für ATB2- oder E-Flugtickets. Über einen Emulator können angemeldete Teilnehmer mit Befehlen (Format) Informationen abfragen, wie zum Beispiel Fluginformationen, Preise oder das Wetter in der Zielregion.\n\nFür jeden Fluggast wird ein Passenger Name Record mit erstellt, der im System gespeichert bleibt.\n\nBeispiel: Ergebnis der Verfügbarkeitsanfrage für einen Flug von Frankfurt nach New York am 15. Dezember 2006.\n\n"}
{"id": "821309", "url": "https://de.wikipedia.org/wiki?curid=821309", "title": "Scientific Linux", "text": "Scientific Linux\n\nScientific Linux (SL) ist eine Linux-Distribution, die auf der Distribution \"Red Hat Enterprise Linux\" (RHEL) des Unternehmens Red Hat basiert und zu dieser binärkompatibel ist. Die Distribution wird hauptsächlich von Entwicklern am Fermilab, am CERN, an der ETH Zürich und am DESY weiterentwickelt.\n\nDie kommerzielle Linux-Distribution RHEL kann nur im Zusammenhang mit Supportverträgen erworben werden. Die Firma Red Hat stellt aber alle Quellpakete von RHEL im Netz bereit, um die Anforderungen unterschiedlicher Lizenzen von – in RHEL enthaltener – freier Software zu erfüllen, und ermöglicht es so, auf dieser Basis eine zu RHEL binärkompatible Linux-Distribution zu entwickeln. Durch die Binärkompatibilität ermöglicht Scientific Linux, Computer mit einer RHEL-kompatiblen Linux-Distribution zu nutzen, ohne einen Supportvertrag mit Red Hat abschließen zu müssen. Neben finanziellen Ersparnissen ergibt sich auch der Vorteil, dass alle Software, die für RHEL angeboten wird, direkt und ohne Einschränkungen unter Scientific Linux genutzt werden kann.\n\nScientific Linux entstand im Umfeld verschiedener Forschungslabore und Universitäten und wurde geschaffen, um verschiedene Ansprüche dieser Institutionen zu erfüllen:\n\nScientific Linux ist binärkompatibel zu RHEL und ist daher ebenfalls ein Enterprise-Betriebssystem, also ein Betriebssystem, das auf die Bedürfnisse großer Unternehmen und staatlicher Organisationen ausgerichtet ist. Als Enterprise-Betriebssystem ist es deshalb auf Stabilität und lange Wartungszyklen ausgelegt. Man kann Scientific Linux über zehn Jahre nutzen, ohne Pakete bzw. Softwareversionen migrieren zu müssen, weshalb es für den kommerziellen Einsatz geeignet ist. Für RHEL bieten große Softwarehäuser wie Oracle oder SAP Zertifikate an, die garantieren, dass deren Software auf RHEL problemlos funktioniert, was analog für große Serverhersteller gilt. Enterprise-Betriebssysteme findet man daher meist auf Workstations und Servern, wo ein extrem stabiler Betrieb verlangt wird (z. B. Wissenschaft, Forschung, Börse, Militär oder Raumfahrt). Im Gegensatz zu RHEL gibt es für Scientific Linux von den meisten Software- und Hardware-Herstellern weder Zertifikate noch Support. Aufgrund der Binärkompatibilität zu RHEL kann es aber oft von den Voraussetzungen, die für RHEL geschaffen werden, direkt profitieren.\n\nScientific Linux entstand im Umfeld verschiedener Universitäten und Forschungsinstitute wie CERN und Fermilab. Es baute am Anfang auf der Linux-Distribution „Fermi Linux LTS 3.0.1“ (Codename „Feynman“) auf, die um einige Programme und Updates erweitert worden war. Danach entschied man sich aber, die Quellen von RHEL zu nutzen, um binärkompatibel zu dieser Linux-Distribution zu werden, da für RHEL die meiste Enterprise-Linux-Software angeboten wird.\n\nScientific Linux wird neben RHEL und Debian auf der Internationalen Raumstation (ISS) eingesetzt, wie die NASA bekannt gab.\n\nDer Codename der jeweiligen Versionen von Scientific Linux folgt aus dem wissenschaftlichen Namen des chemischen Elements, dessen Ordnungszahl der Versionsnummer entspricht.\n\n\n"}
{"id": "823268", "url": "https://de.wikipedia.org/wiki?curid=823268", "title": "SmartFTP", "text": "SmartFTP\n\nSmartFTP ist ein FTP- und SFTP-Client für Microsoft Windows. Die Software stellt eine grafische Benutzeroberfläche für das File Transfer Protocol dar. Es werden sowohl die gewöhnlichen Client-Server-Verbindungen als auch Server-zu-Server-Transfers über das File Exchange Protocol (FXP) unterstützt.\n\nSmartFTP ist bis zur Versionsnummer v3.0.1016.xx für den nicht kommerziellen Privatgebrauch und zu Bildungszwecken kostenlos – kommerzielle Anwender dürfen das Tool 30 Tage lang testen. Ab Versionsnummer v3.0.1017.x läuft SmartFTP als 30-tägige Testversion. SmartFTP kann mehrere gleichzeitige Client-Server-Verbindungen (über FTP) und Server-Server-Verbindung (über FXP) verwalten. Außerdem können Verbindungen über SSL und TLS vor unautorisiertem Zugriff geschützt werden.\n\nDie Oberfläche von SmartFTP unterstützt standardmäßig nur die englische Sprache, kann jedoch über eines von 20 separat erhältlichen „Language Packs“ (engl. Sprachpakete) um andere Sprachen erweitert werden, unter anderem auch um Deutsch. Seit Version 4.0 besitzt SmartFTP einen integrierten Texteditor mit Syntax-Hervorhebung für HTML, PHP und andere Programmiersprachen. Ferner verfügt SmartFTP über einen SSH-Terminal, um entfernte Rechner zu steuern, wobei auch das Drucken von Inhalten des SSH-Servers auf dem lokalen Gerät möglich ist.\n\n"}
{"id": "823772", "url": "https://de.wikipedia.org/wiki?curid=823772", "title": "Macintosh-Baukasten", "text": "Macintosh-Baukasten\n\nDer Macintosh-Baukasten () ist eine Sammlung von Ressourcen, Treibern, Routinen und Programmierschnittstellen, die sich im ROM früher Modelle von Macintosh-Computern befindet. Diese wurden später als „“-Modelle bezeichnet. Der Macintosh-Baukasten wird vom Betriebssystem für Macintosh-Computer, der Macintosh System Software, die später als klassisches Mac OS bezeichnet wurde, genutzt – umgekehrt ist Mac OS ohne den Macintosh-Baukasten nicht ausführbar.\n\nUm Arbeits- und Diskettenspeicherplatz zu sparen, waren seit dem ersten Macintosh mit 128 KB RAM zahlreiche Komponenten des Betriebssystems in einem ROM bereitgestellt worden. Arbeitsspeicher (RAM) war zu dieser Zeit doppelt so teuer wie die gleiche Speichermenge ROM, und 128 KB Arbeitsspeicher stellten bei der Produktion bereits einen wesentlichen Kostenfaktor dar. Außerdem bot diese Maßnahme einen gewissen Geschwindigkeitsvorteil, da ROM seinerzeit schneller auszulesen war als die Kombination aus RAM und Daten von Disketten, denn die Daten hätten jeweils von einer Diskette in den RAM geladen werden müssen. Diskettenlaufwerke waren aber weitaus langsamer als der ROM. Zusätzlich waren die Routinen im ROM ohnehin jederzeit verfügbar – ohne wertvollen Speicherplatz auf Disketten zu belegen, der für die Programme und deren Daten wichtig war. Ebenso war beim originalen Macintosh von 1984, retronym als „Macintosh 128k“ bezeichnet, der spärliche Arbeitsspeicher für die Programme sehr wertvoll.\n\nDa der Inhalt des ROMs mit der Auslieferung des Rechners festgelegt war, werden alle Inhalte des Baukastens über eine Tabelle angesprungen. Um den Macintosh-Baukasten erweiterbar zu machen, kann eine neuere Routine aus dem Betriebssystem (von Diskette) in den Arbeitsspeicher (RAM) geladen werden. Die Sprungadresse in der Tabelle wird dabei auf die nun im RAM vorhandene Funktion umgeleitet. So wird automatisch die neue statt der jeweils veralteten Funktion des ROM verwendet. Dies erlaubte z. B. Fehler zu bereinigen und Funktionen zu erweitern, was dann jedoch ein wenig Arbeitsspeicher kostete.\n\nEnde der 1980er Jahre kehrte sich das Preisverhältnis von ROM und RAM um, ROM war nun deutlich teurer als die gleiche Menge RAM und zudem langsamer. Auch wurde ab 1992 in der Apple–IBM–Motorola-Allianz (AIM) ein neuer auf IBMs POWER-Architektur aufbauender RISC-Prozessor entwickelt, der die Motorola-68000er-Familie ablösen sollte. Die ersten Modelle mit diesem PowerPC getauften Prozessor, dem PowerPC 601, kamen 1995 auf den Markt. Dabei wurde im System-ROM ein Nanokernel geladen, der 68k-Programmcode transparent emuliert ausführen konnte. Der Macintosh-Baukasten wurde ursprünglich in Pascal entwickelt und dann aus Geschwindigkeits- und Platzgründen in Motorola-68000-Assembler umgesetzt. Auf dem PowerPC wurde anfänglich der Macintosh-ROM beinahe unverändert in 68k-Maschinencode wie bisher im ROM vorgehalten, was die bestehende Firmware weiter nutzbar machte und auch für Software für die nötige Kompatibilität sorgte, da über den Nanokernel jegliche Software die transparente 68k-Emulation nutzen konnte. Diese Vorgehensweise ermöglichte Apple zwar einen sanften Übergang von einer Prozessorarchitektur auf eine völlig andere eigentlich inkompatible Befehlssatzarchitektur, jedoch führte es auch zu Einbußen bei der Ausführungsgeschwindigkeit. Das hatte zur Folge, dass der eigentlich schnellere PowerPC in einem Macintosh-Computer unter System 7 (später Mac OS 7.6) real nicht viel schneller war als der 68k-Prozessor, den er – wegen der Geschwindigkeit – ersetzen sollte.\n\nNach und nach wurde der Macintosh-Baukasten bzw. Macintosh-ROM in die Programmiersprache C und C++ übersetzt und dabei in nativen PowerPC-Maschinencode umgewandelt. Auch wurde die Firmware des Computers auf die von Sun Microsystems entwickelte auf FORTH basierte Open Firmware umgestellt. Diese Firmware sollte ein schnelles Initialisieren der nötigen Hardwarekomponenten bieten und schließlich die Kontrolle an den Mac-OS-Bootloader übergeben. Diese ersten Macintosh-Computer mit Open Firmware erhielten nach 1998 die Bezeichnung OldWorld (Deutsch: „alte Welt“), da sie den Mac-OS-ROM, der Teile des Macintosh-Baukastens enthält, als codice_1 im Gerätebaum der Open Firmware bereitstellen. Der Macintosh-Baukasten ist somit ein Teil der Firmware und wie gehabt ein Teil des ROM, weshalb er auch weiterhin oft als Macintosh ROM bezeichnet wurde.\n\nAls Apple Zugang zum Quelltext der Open Firmware erhielt, flossen zahlreiche Macintosh-spezifische Erweiterungen darin ein. Das Ergebnis war Open Firmware 3.0, das bereits ELF-Objekte laden und auf die Dateisysteme HFS und ext2 zugreifen konnte. Diese Art Apple-Mac-Computer wurde von Apple als NewWorld (Deutsch: „neue Welt“) bezeichnet. Zahlreiche Funktionen, die zum Starten eines Betriebssystems notwendig waren, wurden in die Open Firmware verlagert – und im Gegenzug das Mac-OS-ROM ausgelagert. Dies wurde als „ROM-in-RAM“ bezeichnet, da der Macintosh-Baukasten nun nicht mehr im ROM abrufbar war, sondern beim Betriebssystemstart aus einer Datei von der Festplatte in den schnelleren RAM geladen wurde. Der erste NewWorld-Mac war der 1998 vorgestellte iMac „Bondi,“ 1999 folgten das PowerBook G3 „Lombard“ und der Blau-Weiße Power Mac G3.\n\nWichtige Bestandteile des Macintosh-Baukastens umfassen:\n\nAnfang der 1990er-Jahre war Apple mit dem als veraltet geltenden Betriebssystem „Macintosh System Software“ (ab 1996 in Mac OS umbenannt) nicht mehr konkurrenzfähig. Microsoft und IBM arbeiteten an einem grafischen Betriebssystem, das so einfach zu bedienen sein sollte wie das Apple-Betriebssystem, zusätzlich jedoch modere Funktionen wie kooperatives Multitasking und Speicherschutz bot. Während der Entwicklung von System 7 wurde der Macintosh-Baukasten von 68k-Assembler auf C neu implementiert und mit dem Projekt „Star Trek“ (System 7 auf einem IBM-PC-kompatiblen Computer) sogar auf die x86-Architektur portiert.\n\nAm Ende scheiterte Apple jedoch bei dem Versuch, sein Betriebssystem entweder zu erweitern oder durch eine modere Neuentwicklung zu ersetzen. Projekte wie Pink (Taligent) und Copland wurden nie fertiggestellt während Microsoft mit Windows 95 große Erfolge feierte und IBM mit OS/2 und Microsoft mit Windows NT stabile und moderne Betriebssysteme entwickelt hatten.\n\nEnde 1996 erhielt Apple durch die Übernahme von NeXT ein modernes Unix-basiertes Betriebssystem, das eine nicht-kompatible objektorientierte Programmierschnittstelle (API) namen OpenStep enthielt. Diese neue API wurde von Apple im Betriebssystemprojekt Rhapsody als Yellow Box und mit Mac OS X schließlich als Cocoa weiterentwickelt.\n\nDa jedoch viele Anbieter von für Apple unverzichtbarer Anwendersoftware ein mit dem ursprünglichen Macintosh-API kompatibles System forderten, wurde sowohl unter Mac OS ab Version 8.1 als auch unter Mac OS X eine neue Programmierschnittstelle implementiert, die größtmögliche -Kompatibilität mit dem Macintosh-Baukasten bieten sollte. Damit wurde der Portierungsaufwand für bestehende Anwendungen gering gehalten und Softwarehersteller konnten ihre Programme relativ schnell auf das neue Betriebssystem Mac OS X portieren, das ab 2002 das ältere klassische Mac OS vollständig ersetzte. Diese kompatible Programmierschnittstelle erhielt von Apple den Namen Carbon und ist in jeder Version von Mac OS X (ab 2012 in OS X und ab 2016 in macOS umbenannt) enthalten, wird jedoch seit 2007 nicht mehr weiterentwickelt und wurde auch nicht mehr auf 64-Bit portiert, sodass nur 32-Bit-Anwendungen unter Mac OS X die Funktionen der Carbon-Programmierschnittstelle nutzen können.\n\nBei „“-Macintosh-Computern war der Macintosh-Baukasten noch im ROM gespeichert. Die Speicherkapazität des verbauten ROM wurde seit 1984 von 64 kB auf 4 MiB von 1994 bis 1998 erhöht.\n\nBei „“-Macs sind die folgenden Versionen des codice_2 bekannt:\n\n"}
{"id": "824545", "url": "https://de.wikipedia.org/wiki?curid=824545", "title": "Crack (Passwortüberprüfungsprogramm)", "text": "Crack (Passwortüberprüfungsprogramm)\n\nCrack ist ein Programm, das zum Erraten von Passwörtern auf UNIX und UNIX ähnlichen Systemen verwendet wird. Es wurde von Alec Muffett 1991 geschrieben und liegt heute in der Version 5.0 vor.\n\nCrack verwendet eine im Verzeichnis /etc enthaltene passwd-Datei, in der normalerweise die Passwörter verschlüsselt enthalten waren. Crack errät die Passwörter und sucht nach Loginnamen, die ein schwaches Passwort verwenden. Dabei können auch Wörterbücher eingesetzt werden, um z. B. Wörter aus bestimmten Themengebieten zu verwenden (Wörterbuchangriff).\n\nDieser Angriff kann dadurch erfolgen, da für die Loginshell die Datei passwd auslesbar sein muss.\n\nDa auf den meisten UNIX-Systemen heute Passwort-Shadowing eingesetzt wird, hat dieses Programm an Wirkung in diesem Bereich eingebüßt, da die shadow-Datei nicht für alle Benutzer lesbar ist und damit nicht kopiert werden kann.\n\nDie shadow-Datei liegt ebenfalls in /etc, kann aber von der Loginshell nicht ausgelesen werden, da die entsprechenden Rechte nicht bestehen. Das Shadowing wird durch eine Suite von Programmen möglich gemacht, die das zum Benutzernamen gehörige Passwort aus der shadow-Datei lesen und es an die Loginshell übergeben. Dadurch wird die Sicherheit beim Login erhöht.\n\nProgramme wie Crack werden auch von Administratoren eingesetzt, um beim Ändern von Benutzerpasswörtern schwache Passwörter zu vermeiden. Dabei läuft dieses Programm im Hintergrund und überprüft die neu eingegebenen Passwörter, also ob sie z. B. in einem Wörterbuch auftauchen. Sollte dies der Fall sein, so wird die Eingabe eines anderen Passwortes verlangt.\n\n"}
{"id": "825265", "url": "https://de.wikipedia.org/wiki?curid=825265", "title": "ROBODOC", "text": "ROBODOC\n\nROBODOC ist ein Gerät für die rechnergestützte Fräsung und Implantation von Hüftgelenksprothesen zur Behandlung von Coxarthrose und anderer Erkrankungen und Verletzungen die eine Ersetzung des Hüftgelenks erforderlich machen. Es wurde von der amerikanischen Firma Integrated Surgical Systems (ISS) entwickelt und hergestellt, und später von der Firma Curexo Technology, die 2014 in THINK Surgical umbenannt wurde, weiterentwickelt und vermarktet.\n\nDas System ROBODOC besteht aus einem angepassten Industrieroboter, der rechnergestützt die zur Implantation von künstlichen Hüftgelenken notwendigen Fräsungen des Knochens vornimmt. Diese Fräsungen werden bei der herkömmlichen Methode vom operierenden Chirurgen von Hand mit Fräsgeräten und Feilen vorgenommen.\n\nAlle weiteren Operationsschritte vor und nach dem Fräsvorgang werden bei der ROBODOC-Methode ebenso wie bei der herkömmlichen Methode vom operierenden Chirurgen von Hand ausgeführt.\n\nFür die Behandlung mit ROBODOC ist eine ausführliche und genaue Planung der Operation notwendig, um Daten der Knochengeometrie des Patienten zur Steuerung des Gerätes in ausreichender Genauigkeit zur Verfügung zu haben. Dabei ist die Realitätstreue und Detailauflösung des zur Planung verwendeten bildgebenden Verfahrens (z. B. Röntgen, Computertomographie oder Kernspintomographie) ebenso wichtig wie die Erfahrung des planenden Chirurgen, der die abgebildeten Strukturen den Strukturen im Körper zuordnen muss.\n\nNachdem das System 1990 erstmals an Hunden erprobt wurde, kam es in Deutschland in 1992 erstmals zum Einsatz. Für den europäischen Markt wurde ROBODOC in Deutschland vom TÜV Rheinland auf technische Sicherheit geprüft und erhielt mit dieser Prüfung die europaweite Zulassung. Im Jahr 2008 erhielt das System die Zulassung in den USA durch die FDA.\n\nDer RoboDoc kann (bei entsprechend genauer Planung) Fräsungen wesentlich genauer und gleichmäßiger erstellen und genau auf die Abmessungen des Implantats abstimmen. Dadurch werden die Passgenauigkeit und die Einheilung der Prothese verbessert, was zu kürzeren Nachbehandlungsphasen und schnellerer Belastbarkeit der Prothese führt.\n\nNach der Einführung des Gerätes im normalen Krankenhausbetrieb hat sich eine höhere Komplikationsrate als bei der herkömmlichen Methode eingestellt.\n\nDie ROBODOC-Methode erfordert eine größere Öffnung im mittleren Gesäßmuskel als die herkömmliche Methode. Dadurch entstehen im Verhältnis häufiger als bei der herkömmlichen Methode dauerhafte Schädigungen des mittleren und großen Gesäßmuskels und sogar der dazugehörigen Nerven. Betroffene einer solchen Schädigung haben oft äußerlich sichtbar tiefe Dellen im Gesäß.\n\nUngenauigkeiten bei der Planung vor der Operation können dazu führen, dass die Steuerdaten des ROBODOC-Gerätes von der Realität abweichen. Dadurch kann es bei der Operation zu Schädigungen des Knochens (z. B. Durchstoß der seitlichen Knochenwand) kommen, was das ROBODOC-Gerät nicht erkennen kann. Ein menschlicher Chirurg kann solche Abweichungen von der Operationsplanung oft während der Operation durch „Fingerspitzengefühl“ sensorisch wahrnehmen und erkennen und kurzfristig die Operation anpassen.\n\nLangfristige medizinische Studien zur Zuverlässigkeit und Komplikationsanfälligkeit der ROBODOC-Methode waren zur TÜV-Zulassung nicht notwendig, und wurden auch nicht durchgeführt. Eine Studie des medizinischen Dienstes der Krankenkassen im Auftrag der Barmer Ersatzkasse zu den Risiken des ROBODOC-Verfahrens aus dem Jahr 2004 stellt die Zuverlässigkeit und Tauglichkeit des Verfahrens in Frage, und verpflichtet Kassenärzte bei der Aufklärung vor der Operation den Patienten ausdrücklich darauf hinzuweisen, dass es sich beim ROBODOC-Verfahren um ein experimentelles Verfahren handelt, mit dem höhere Risiken verbunden sind.\n\nAm 13. Juni 2006 wurden die Klage auf Schmerzensgeld einer Patientin, die nach einer Operation mit der ROBODOC-Methode Komplikationen und dauerhafte Schädigungen erlitten hatte vom Bundesgerichtshof letztinstanzlich abgewiesen. Dieser Fall ist aber nicht repräsentativ für die mehreren hundert in der Geschädigten-Initiative „Forum ROBODOC“ zusammengeschlossenen Opfer, denn die Schäden dieser Patientin waren laut Gericht nicht für das ROBODOC-Verfahren spezifisch. Zum Stand Oktober 2006 sind mehrere Dutzend Klagen vor verschiedenen Landgerichten in Deutschland anhängig sowie mehr als 100 Beweisverfahren.\n\n\n\n"}
{"id": "825317", "url": "https://de.wikipedia.org/wiki?curid=825317", "title": "Cp (Unix)", "text": "Cp (Unix)\n\ncodice_1 (für \"copy\") ist ein \"interner Befehl\" einer Unix-Shell zum Kopieren von Dateien bei Unix- oder unixoiden Betriebssystemen auf Computern.\n\nDie Syntax ist sehr einfach gehalten; üblicherweise wird eine Datei von \"quelldatei\" zu \"zieldatei\" kopiert, in dem man\neingibt. Mehrere Dateien können kopiert werden, indem alle Dateinamen angegeben werden, anschließend das Zielverzeichnis:\nDamit ist z. B. die Verwendung von Wildcards möglich, welche moderne Unix-Shells üblicherweise zur Verfügung stellen. So kopiert\nz. B. die Dateien \"bild1.jpg\", \"bild234.jpg\" oder \"bild_abc.jpg\" in das Verzeichnis \"ziel\", wenn die verwendete Shell das Wildcard \"*\" expandiert.\n\nAus dem traditionellen Unix-Werkzeug haben sich einige Programme entwickelt, die Dateien über das Computernetzwerk von einem Unix-Rechner zum nächsten kopieren und sich dabei in ihrer syntaktischen Verwendung stark an das Programm codice_1 anlehnen.\n\nEine Erweiterung des cp-Programms ist das rcp-Programm, welches für die Verwendung in Netzwerken (\"remote copy\") geschaffen wurde.\n\nDie Syntax ist:\n\nAufgrund prinzipbedingter Sicherheitsprobleme – die Dateien werden unverschlüsselt übertragen – wird von rcp dringendst abgeraten und stattdessen die Verwendung des Nachfolgers codice_3 aus der SSH-Familie empfohlen.\n\nWenn es darum geht, zwei Verzeichnisse abzugleichen und nur Änderungen zu kopieren, ist rsync ein weit verbreiteter Ersatz für den Befehl cp. Er kann sowohl lokal als auch über das Netzwerk benutzt werden. Beim Kopieren über das Netzwerk kann eine Verschlüsselung mit SSH durchgeführt werden.\n\n\n"}
{"id": "827362", "url": "https://de.wikipedia.org/wiki?curid=827362", "title": "Wings 3D", "text": "Wings 3D\n\nWings 3D oder auch \"Wings\" ist freie Software zur einfachen Erstellung von 3D-Modellen. Vorlage war Nendo und Mirai von Izware, die ebenfalls die Winged edge Datenstruktur benutzen. Wings 3D ist in Erlang geschrieben und für mehrere Plattformen wie Windows, Linux und Mac OS X PowerPC/Intel verfügbar.\n\nDas Programm bietet Neulingen und Fortgeschrittenen im Bereich der 3D-Modellierung eine einfach zu erlernende Benutzeroberfläche. Zur Bedienung ist eine 3-Tasten-Maus zu empfehlen, da die meisten Menüs über die Maus angesteuert werden. Wings 3D eignet sich gut für die Texturierung und Erstellung von Polygon-Netzen niedriger bis mittlerer Dichte, es präsentiert die Szene dabei in einer Art Vorschau, die einen Kompromiss zwischen Drahtgittermodell und echtem Rendering versucht. Um fotorealistische Bilder zu erhalten, muss ein externer Renderer benutzt werden. Dazu können Szenen oder Teile davon in standardisierte Formate exportiert werden. In Programmen wie POV-Ray, Blender oder Cinema 4D können solche Szenen geladen oder integriert und gerendert werden.\nWings 3D bietet zur Steuerung der „Kamera“ verschiedene in anderer 3D-Grafiksoftware verwendete Modi an.\n\n"}
{"id": "827572", "url": "https://de.wikipedia.org/wiki?curid=827572", "title": "Nautilus (Dateimanager)", "text": "Nautilus (Dateimanager)\n\nNautilus ist ein freier Dateimanager für unixoide Systeme.\nEs ist der Standard-Dateimanager der Desktop-Umgebung Gnome und wird auch einfach \"Dateien\" genannt.\nSein Name ist eine Anspielung auf die Schale der Perlboote. Nautilus wurde von der Firma Eazel entwickelt.\n\nDer Quelltext des Nautilus Dateimanagers ist im Gegensatz zum Finder auf dem Mac oder dem Windows-Explorer gemäß der GNU General Public License (GPL) frei verfügbar.\n\nNautilus löste den Dateimanager gmc ab und ist seit Gnome 1.4 fester Bestandteil des Gnome-Projekts. Nautilus ist konfigurierbar, bietet Funktionen zur Systemkontrolle und unterstützt verschiedene Dateitypen.\n\nZu heftigen Kontroversen führte die mit der Version 2.6 eingeführte Umstellung, dass neue Verzeichnisse in eigenen Fenstern geöffnet werden, der sogenannte räumliche Modus (\"spatial mode\"), anstatt wie vorher im selben Fenster (Browser-Modus bzw. \"navigational mode\" genannt). Diese Umstellung wurde mit Version 2.30 wieder rückgängig gemacht.\n\nUrsprünglich geht Nautilus auf den langjährigen Apple-Entwickler Andy Hertzfeld zurück. Zur Entwicklung des Pakets gründete er eigens die Softwarefirma eazel, Inc. Nach seinen Plänen sollte der Dateimanager zwar selbst kostenlos und freie Software sein, jedoch kostenpflichtige Dienste integrieren, die das Einkommen des Unternehmens sicherstellen sollten. Da der bis dahin im Gnome-Desktop eingesetzte Dateimanager \"gmc\", eine grafische Variante des Midnight Commander, allgemein als zu unmodern und nicht konkurrenzfähig mit dem Konqueror des konkurrierenden K Desktop Environment angesehen wurde, wurden eazels Pläne, eine moderne Schaltzentrale für den freien Desktop zu schaffen, überwiegend begrüßt. Im Zuge der dotcom-Blase gelang es Hertzfeld, etwa 15 Millionen US-Dollar an Startkapital für sein Geschäftsmodell zu akquirieren und für die Entwicklung von Nautilus einzusetzen. In den Ankündigungen des Unternehmens wurde Nautilus dabei nicht als „Dateimanager“, sondern als „grafische Shell“ bezeichnet. Die genaue Bedeutung dieses Schlagwortes blieb zwar ungeklärt, jedoch bedeutete es in der Praxis die Einbettung umfangreicher Funktionalitäten in den Dateimanager, die teilweise die Funktionalität anderer Bestandteile des Desktops duplizierten, so etwa die Einbettung von Mozilla als – funktionell sehr eingeschränkter – Browser, die Möglichkeit, Web-Feeds zu abonnieren, Hilfeseiten anzuzeigen und MP3-Audiodaten abzuspielen. Für die beschleunigte Dateisuche produzierte eazel zudem einen eigenen Indizierungsdienst namens \"Medusa\". Selbst zur Glättung der Bildschirmschriften (Antialiasing) auf dem Desktop brachte Nautilus eigene Komponenten mit, da Antialiasing im zugrundeliegenden Toolkit Gtk 1.2 noch nicht implementiert war.\n\nEazels \"Software catalogue\" sollte zudem das Aktualisieren installierter Programmpakete aus dem Dateimanager heraus ermöglichen, womit jedoch die Funktionalität der jeweiligen distributionseigenen Paketverwaltung dupliziert wurde.\n\nZusätzlich wurde eine Anbindung an die geplanten \"eazel services\" integriert, von denen jedoch nur ein kostenloser Datenspeicher im Internet, ähnlich Apples iDisk, realisiert wurde. Noch bevor die Firma dazu übergehen konnte, die Version 1.0 des Programms herauszubringen und kostenpflichtige Dienste anzubieten, musste das Unternehmen Konkurs anmelden und stellte am 15. Mai 2001 den Geschäftsbetrieb ein. Die Fortentwicklung des unter der GPL stehenden Programms wurde anschließend von Freiwilligen der Gnome-Entwickler-Community übernommen.\n\n\n\n"}
{"id": "828039", "url": "https://de.wikipedia.org/wiki?curid=828039", "title": "Apache Flex", "text": "Apache Flex\n\nApache Flex, vorher Adobe Flex, ist ein Software Development Kit (SDK) zum Entwickeln von Rich Internet Applications (RIAs) auf der technischen Basis der Flash-Plattform. Neben dem als Open Source veröffentlichten SDK besteht das Framework aus dem kostenpflichtigen \"Flash Builder\" (ab Version 4, davor hieß dieser \"Flex Builder\"), den LiveCycle Data Services und den Flex Charting-Komponenten. Obwohl es schon vor der Veröffentlichung von Flex möglich war, mit Adobe Flash (früher Macromedia) reichhaltige Anwendungen zu erstellen, bot Flash als Werkzeug nicht das von Software-Entwicklern von einer IDE geforderte Funktionsspektrum, sondern wurde weiterhin hauptsächlich von Webdesignern und Grafikern eingesetzt. Flex soll deshalb gezielt Software-Entwickler ansprechen.\n\nFlex hat mehrere Bestandteile, die als Framework auf die Technik der Adobe Flash-Plattform aufsetzen.\n\nEine Flex-Anwendung wird mit Hilfe der Auszeichnungssprache MXML für die Benutzeroberfläche und der Programmiersprache ActionScript für die Anwendungslogik entwickelt. MXML basiert auf XML, mit der ein Entwickler die Zusammensetzung der Benutzeroberfläche aus sichtbaren und unsichtbaren Komponenten beschreibt. Der Flex-Compiler übersetzt in einem Zwischenschritt die MXML-Dateien in ActionScript-Quelldateien, die im weiteren zu einer Flashdatei kompiliert werden. Der Entwickler kann die Benutzeroberfläche daher auch ohne MXML teilweise oder komplett in ActionScript imperativ beschreiben. Änderungen an der Zusammensetzung der Benutzeroberfläche sind deshalb auch während der Laufzeit möglich.\n\nDie \"LiveCycle Data Services\" (vormals \"Flex Data Services\") sind die Serverkomponente aus der Flex-2-Produktfamilie. Über diesen Dienst kann die Flexanwendung andere Serveranwendungen (zum Beispiel Java-Anwendungen) ansprechen. Teile davon sind unter der Bezeichnung \"BlazeDS Open Source\" bekannt.\n\nDer \"Flash Builder\" ist die integrierte Entwicklungsumgebung von Flex und basiert auf Eclipse. Mit ihr können vollständige und komplexe ActionScript-Projekte, sowohl für den Einsatz auf Websites als auch für den Desktop entwickelt werden. Grafische Oberflächen werden in Flash Builder mit der XML-basierten Sprache MXML beschrieben, wobei der Flash Builder auch eine \"Design View\" von MXML-Code zulässt, mit der die mausunterstützte Komposition von Benutzeroberflächen möglich ist. Der Flash Builder ist kostenpflichtig und ist in einer Standard und einer Professional Version verfügbar, die sich durch den Umfang der verfügbaren Komponenten zur Datenverarbeitung und durch das Vorhandensein bestimmter Profiling-Werkzeuge unterscheiden. Die Professional Version ist für Schulen jedoch kostenlos (\"Adobe Flash Builder Pro for Education\").\n\nAdobe hat angekündigt, dass im Rahmen der Übergabe von Flex an die Apache Software Foundation die Design View des Flash Builders nicht mehr weiterentwickelt wird.\n\nDie Flex-Charting-Komponenten (\"Data Visualization\") sind eine Erweiterung des Flex SDK. Mit diesen Komponenten können in einfacher und optisch ansprechender Weise Diagramme dargestellt werden. Die Charting-Komponenten sind ab der Version 4 enthalten (vorher nur mit Flex Builder 3 Professional).\n\nDer Flex Builder 1.5 baute noch auf Dreamweaver MX 2004 auf. Ab Version 2.0 basiert der Flex Builder auf Eclipse 3.1. In dieser Version ist es nicht mehr erforderlich, die Flex-Anwendungen von einem Server kompilieren zu lassen. Die Kompilierung übernimmt nun die integrierte Entwicklungsumgebung oder der Kommandozeilen-Compiler des Flex SDK. Mit Flex 2 wurde zudem ActionScript 3 eingeführt.\n\nAm 25. Februar 2008 veröffentlichte Adobe Flex 3. Das SDK selbst ist nun als kostenloser Download verfügbar. In Flex 3 neu hinzugekommen sind hauptsächlich die Unterstützung von Adobe AIR sowie Funktionen für Profiling und Refactoring.\n\nAdobe hat Flex 4 (Codename Gumbo) im März 2010 veröffentlicht. Neu ist die „Spark“ genannte Komponentenarchitektur und die Unterstützung des Austauschformats FXG, die zum Release von Adobe Flash Catalyst einen integrierten Arbeitsfluss ermöglichen soll. Mit „Spark“ wurde das Layoutmodell erneuert und erlaubt nun die Anwendung von 2D- und 3D-basierten Animationen auf Komponenten sowie die automatische Umkehrung von Übergängen, den Einsatz von \"Pixel Bender\"-Filtern und keyframebasierte Animationen. Das \"Status\"-Feature (\"Viewstates\"), das verschiedene Zustände der Anwendung ermöglicht, wurde verbessert. Die Compilerleistung wurde optimiert. Das ASDoc-Werkzeug zur Dokumentation von Anwendungen unterstützt nun ASDoc in MXML-Dateien. Die DataBinding-Funktion unterstützt jetzt die bidirektionale Kommunikation und Veränderung von Daten. Die mit Flash Player 10 verbesserte Text-Engine wurde in Flex integriert. Die HTML-Vorlagen zur Einbettung von Flex-Anwendungen in HTML-Code basieren nun auf der Open Source-Methode \"SWFObject\".\n\nAdobe hat das Flex SDK inklusive der \"Spark\" genannten Komponenten und des neuen \"Falcon\"-Compilers an die Apache Software Foundation abgegeben, wo es aktiv weiterentwickelt wird. Die Weiterentwicklung des MXML-Designers im Flash Builder, des in der Creative Suite enthaltenen Tools Adobe Flash Catalyst sowie der \"Introspektion\" genannten datenzentrierten Entwicklerfeatures wurde eingestellt. Darüber hinaus will Adobe die eigenen \"Runtime Shared Libraries\", die das Flex-Framework enthalten, nicht mehr signieren. Im Unterschied zu \"RSL\"s von Drittentwicklern waren die RSLs von Adobe vorkompiliert und digital signiert, da der Adobe Flash Player sie so allen gleichzeitig laufenden Anwendungen, unabhängig von ihrer Domain, zur Verfügung stellen kann. Eigene RSLs konnten nur von Anwendungen der gleichen Ursprungsdomain gleichzeitig verwendet werden.\n\n\n\n\n"}
{"id": "828242", "url": "https://de.wikipedia.org/wiki?curid=828242", "title": "Hugin (Software)", "text": "Hugin (Software)\n\nHugin [] ist eine freie Stitching-Software, die ursprünglich von Pablo d’Angelo entwickelt wurde. Sie ist ein GUI für die Panorama Tools (auch bekannt als \"PanoTools\") und andere Werkzeuge.\n\nDank intelligenter Entzerr-Algorithmen sind nicht nur lineare Panoramabilder aus nebeneinander gereihten Fotos möglich, sondern auch das Erstellen hochauflösender Fotos, wobei die Fotos auch übereinander positioniert sein können. Hugin kann aus Exif-Daten der Digitalkameras automatisch die Brennweiten von Aufnahmen feststellen, oder man gibt sie von Hand vor. Ebenso kann man für die eigenen Objektive Profile erstellen. Ab der Version 0.6 besteht die Möglichkeit zur Korrektur von Vignettierung in den Ausgangsbildern, mit Version 0.7 kann Hugin die Korrekturparameter dafür selbst ermitteln, Version 0.8.0 bringt neben einem neuen Vorschaufenster viele andere Änderungen. Weiterhin besteht mit verschiedenen Projektionsmethoden die Möglichkeit, dieselben Ausgangsbilder völlig unterschiedlich abzubilden.\n\nUm vor dem Zusammenfügen von Einzelbildern deren relative Position zueinander (und weitere Parameter) optimieren zu können, benötigt Hugin Kontrollpunkte, die identische Stellen auf zwei oder mehr sich überlappenden Einzelbildern markieren. Normalerweise müssen diese Kontrollpunkte vom Anwender selbst gesetzt werden, die Programme \"Autopano-sift\" bzw. \"Autopano\", für beide muss unter Mac OS X und Linux die Mono-Umgebung und für \"Autopano-sift\" unter Windows .NET installiert sein, können solche Punkte allerdings auch automatisch markieren. Seit Oktober 2009 ist \"autopano-sift-C\" 2.5.1 auf der Hugin-Webseite veröffentlicht, was das Installieren von Mono bzw. .NET erübrigt, da es in reinem C geschrieben ist. \"Autopano-sift\" und \"autopano-sift-C\" verwenden für das Auffinden von Kontrollpunkten den SIFT-Algorithmus. Ab Version 2010.4.0 besitzt Hugin mit \"cpfind\" einen eigenen Kontrollpunkt-Generator, der nicht auf dem patentbehafteten SIFT-Algorithmus basiert.\n\nBeim Überblenden der Einzelbilder kann es zu unschönen abrupten Helligkeitsübergängen kommen, die beispielsweise durch unterschiedliche Belichtungs- oder Blendeneinstellungen ausgelöst werden können. Ebenso sind Doppelbilder möglich, wenn die Aufnahmen nicht genau um den oft als Nodal- oder Knotenpunkt bezeichneten Punkt der Kamera herum erfolgten, an dem keine Parallaxefehler auftreten, oder sich bewegte Objekte in den Aufnahmen befinden. Um diese Probleme zu vermeiden oder zumindest zu mindern, können zum Überblenden die Programme \"Enblend\" (inzwischen auch Version 4.0), \"Enfuse\" oder \"Smartblend\" verwendet werden.\n\nFür die Erzeugung der Parameter gibt es Assistenten.\n\nEinige der Änderungen der letzten Jahre entstanden im Rahmen des Google Summer of Code in den Jahren 2007, 2008 und 2009.\n\nDer Name „hugin“ wurde von Nils Lagerkvist vorgeschlagen. Hugin war einer der beiden Raben Odins (Hugin und Munin) und bedeutet übersetzt in etwa so viel wie „Gedanke“ oder „Sinn“. Es gibt auch eine Software namens Munin.\n\n"}
{"id": "829070", "url": "https://de.wikipedia.org/wiki?curid=829070", "title": "Mapping", "text": "Mapping\n\nDer Begriff Mapping (zu deutsch Abbildung oder Kartierung, wörtlich \"eine Karte machen\") hat in den letzten Jahrzehnten eine Reihe zusätzlicher Bedeutungen erhalten. Zum ursprünglichen Inhalt – ein begrenztes Gebiet kartografisch erfassen; erhobene Daten in eine Landkarte eintragen – kamen Bedeutungen aus der Computergrafik und -technik hinzu, aus der allgemeinen Technik, Meteorologie, Medizin sowie Inhalte aus den Planungsmethoden.\n\n\n\n\n\n\n"}
{"id": "830031", "url": "https://de.wikipedia.org/wiki?curid=830031", "title": "Große Haie – Kleine Fische", "text": "Große Haie – Kleine Fische\n\nGroße Haie – Kleine Fische (Originaltitel: \"Shark Tale; auf dt.: Haifisch-Geschichte\") ist ein Computeranimationsfilm aus dem Jahr 2004 der Regisseure Vicky Jenson, Rob Latterman und Bibo Bergeron. \n\nOscar ist nur ein kleiner Putzerlippfisch in der Unterwasserwelt des heimischen Southside Riffs. Er arbeitet in der Walwäscherei des Kugelfisches Sykes und träumt von Ruhm, Respekt und Geld, doch er sieht keine Chance, all das zu erreichen. Unglücklich wie er mit seinem Leben ist, bemerkt er nicht, dass der Engelfisch Angie heimlich in ihn verliebt ist.\n\nZu allem Unglück hat sich Oscar bei Sykes hoch verschuldet und da er das Geld nicht zurückzahlen kann, beschließt Sykes, Oscar 24 Stunden zum bezahlen zu geben. Aber als Angie ihm für das Geld Hilfe gibt, setzt er es auf das Rennseepferdchen, Lucky Day. Aber als dieser im Rennen stürzt und nicht mehr gewinnen kann, wird Sykes wütend und beschließt, Oscar von Ernie und Bernie, zwei „Rasta-Quallen“, beseitigen zu lassen.\n\nZufällig wird an der Stelle, an der Oscar beseitigt werden soll, Frankie, der älteste Sohn und ganze Stolz von Don Lino, dem Hai-Mafiaboss des Riffs, von einem Anker erschlagen. Geschickt nutzt Oscar die Situation und präsentiert sich als Haikiller und Riffretter.\n\nWährend Oscar die Annehmlichkeiten seines Stardaseins, inklusive der Bekanntschaft mit der attraktiven Rotfeuerfischdame Lola, genießt, bereitet die Hai-Mafia einen Gegenschlag vor.\n\nLenny, der jüngste Sohn Don Linos, ist Vegetarier und absolut sanftmütig, was seinem Vater Sorgen macht, da sein ältester Sohn nun tot ist und deshalb nicht mehr das Familiengeschäft übernehmen kann. Lenny soll deshalb den „Haikiller“ töten und so einerseits den Ruf der Familie wiederherstellen und andererseits beweisen, dass er ein richtiger Hai ist.\n\nDoch auch Oscar und Lenny, die einander bei dem tragischen Tod von Frankie kennengelernt haben, schmieden Pläne. Da Lenny aus dem Mafiageschäft seiner Familie aussteigen möchte, täuscht er mit Oskar einen Todeskampf vor, bei dem er in die Tiefe stürzt. Oscar hat den Riffbewohnern nun bewiesen, dass er wirklich einen Hai töten kann und Lenny kann nun, getarnt als Delfin, eine Stelle bei der Walwäscherei annehmen.\n\nNatürlich kann die Hai-Mafia, angeführt von Don Lino, diese Schande nicht auf sich sitzen lassen und plant, Oscar und seine Freunde in eine Falle zu locken, was auch gelingt. Nachdem ein anfänglicher Bluff aufflog, entgehen sie nur knapp dem Tod durch Gefressenwerden, werden aber von Don Lino bis ins Riff und in die Walwaschanlage verfolgt. Dort gelingt es Oscar durch geschickten Einsatz der Waschstraßentechnik, Don Lino und Lenny vis-a-vis zu arretieren. Oscar nutzt die Gelegenheit, um Vater und Sohn auszusöhnen und die Mafia zu einer Vereinbarung zu bewegen, das Riff künftig zu verschonen. Er selbst offenbart der versammelten Presse seinen Schwindel und gesteht Angie vor laufenden Kameras seine Liebe.\n\nZu guter Letzt wird Oscar Miteigentümer und Manager der Walwäscherei.\n\n\nKomponist Hans Zimmer stellte den Soundtrack zu \"Große Haie – Kleine Fische\" zusammen.\n\nBill Damaschke, der Produzent von \"Große Haie – Kleine Fische\", war 2005 für den Oscar in der Kategorie \"Bester animierter Spielfilm\" nominiert, konnte sich aber nicht gegen \"Die Unglaublichen – The Incredibles\" durchsetzen.\n\nHans Zimmer gewann den ASCAP Film and Television Music Award für die Musik zu \"Große Haie – Kleine Fische\" und Will Smith erhielt den \"Kid's Choice Award\" für die \"beliebteste Stimme in einem Animationsfilm\".\n\nAußerdem wurde der Film für den Saturn Award als \"Bester Animationsfilm\" nominiert und für den Annie Award in den Kategorien \"Animationseffekte\", \"Charakteranimation\" und \"Charakterdesign\" sowie \"Drehbuch für einen Animationsfilm\" vorgeschlagen.\n\nRenée Zellweger und Ken Duncan waren für die Visual Effects Society Awards nominiert für die \"außergewöhnliche Darstellung eines animierten Charakter\".\n\n"}
{"id": "830862", "url": "https://de.wikipedia.org/wiki?curid=830862", "title": "UNIVAC 1108", "text": "UNIVAC 1108\n\nDie UNIVAC 1108 war die zweite Produktlinie in der Serie der UNIVAC 1100/2200, die 1964 vorgestellt wurde. Im Gegensatz zur UNIVAC 1107 wurde bei der UNIVAC 1108 der magnetische Speicher für die Prozessorregister auf Basis des Thin Film Memory durch integrierte Schaltkreise ersetzt. Zudem wurden kleinere und schnellere Kernspeicher für den Hauptspeicher eingesetzt.\n\nZusätzlich wurden zwei weitere Architekturänderungen eingeführt:\n\n\nAls die erste UNIVAC 1108 im Jahre 1965 ausgeliefert wurde, wurde gleichzeitig die UNIVAC 1108 II, auch unter dem Namen UNIVAC 1108A bekannt, vertrieben. Im Unterschied zur UNIVAC 1108 war die UNIVAC 1108A ein Mehrprozessorsystem.\n\nObwohl 1964 eine interne Studie prophezeite, dass nur ungefähr 43 Stück verkauft werden könnten, wurden im Ganzen 296 Maschinen produziert.\n\nAls die Sperry Corporation den Hauptspeicher als Halbleiterspeicher auslieferte, wurde die gleiche Maschine unter dem Namen UNIVAC 1100/20 verkauft.\n\n\nDamit konnte die UNIVAC 1108 drei Programme in den Prozessoren und zwei I/O-Programme in den Ein-/Ausgabecontrollern gleichzeitig ausführen.\n\n"}
{"id": "833544", "url": "https://de.wikipedia.org/wiki?curid=833544", "title": "AppleTalk Transaction Protocol", "text": "AppleTalk Transaction Protocol\n\nAppleTalk Transaction Protocol (ATP) ist ein Begriff aus der Informatik.\n\nDas AppleTalk Transaction Protocol, oder kurz ATP, ist ein Protokoll zum ungesicherten Datentransport in einem AppleTalk-Netz.\n\nDas Protokoll gehört zur Transportschicht.\n\nRFC 1742 definiert die MIB für ATP.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "833554", "url": "https://de.wikipedia.org/wiki?curid=833554", "title": "AppleTalk Echo Protocol", "text": "AppleTalk Echo Protocol\n\nAppleTalk Echo Protocol (AEP) ist ein Begriff aus der Informatik.\n\nDas AppleTalk Echo Protocol, oder kurz AEP, ist ein Protokoll zum Testen für Komponenten in einem AppleTalk-Netz, vergleichbar mit Ping in IP-Netzen.\n\nEs ist definiert in RFC 1742.\n\nDas Protokoll gehört zur Transportschicht.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "833557", "url": "https://de.wikipedia.org/wiki?curid=833557", "title": "Name Binding Protocol", "text": "Name Binding Protocol\n\nName Binding Protocol (NBP) ist ein Begriff aus der Informatik.\n\nDas Name Binding Protocol, oder kurz NBP, ist als Teil der AppleTalk-Protokoll-Suite für die Zuordnung von Geräte-Namen und Diensten zu AppleTalk-Adressen und AppleTalk-Sockets in einem lokalen Netzwerk zuständig. Da AppleTalk-Adressen nicht von zentralen Instanzen vergeben werden, sondern die AppleTalk-Geräte sich diese per AARP automatisch selbst zuteilen, besteht die Notwendigkeit, feststehende Gerätenamen auf die sich stetig wechseln könnenden Adressen umzusetzen. Dieser Aufgabenstellung nimmt sich das NBP an, indem es unter Berücksichtigung von Namenskonflikten den dynamischen Adressen fixe Namen zuweist.\n\nNicht nur Gerätenamen, sondern auch angebotene Dienste werden per NBP abgedeckt. Je AppleTalk-Gerät können auf einem bis 254 verschiedenen AppleTalk-Sockets Dienste bereitgestellt werden (das IP-Äquivalent zu den Sockets wären die TCP-Ports). Ist ein Drucker via AppleTalk per PAP als \"Farblaser\" erreichbar und bietet ein Server Dateidienste per AFP an, so werden per NBP sowohl \"Farblaser:LaserWriter\" als auch \"Big Mac:AFPServer\" im AppleTalk-Netzwerk registriert.\n\nDas Name Binding Protokoll ist kein offen standardisiertes Protokoll aber seitens Apple auf veröffentlicht.\n\nDas Protokoll gehört nicht zur Transportschicht, sondern ist zwischen dieser und der Anwendungsschicht angeordnet (die AppleTalk-Protokolle passen nicht exakt in das ISO-OSI-Schichten-Korsett). Historisch gesehen gehört NBP wie die anderen Protokolle der AppleTalk-Suite zu den aussterbenden Sorten. In TCP/IP-Netzwerken wird seit Verbreitung des Zeroconf-Ansatzes die NBP-Funktionalität durch das Multicast DNS Protokoll fortgeführt.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "833589", "url": "https://de.wikipedia.org/wiki?curid=833589", "title": "AppleTalk Address Resolution Protocol", "text": "AppleTalk Address Resolution Protocol\n\nAppleTalk Address Resolution Protocol (AARP) ist ein Begriff aus der Informatik.\n\nDas AppleTalk Address Resolution Protocol, oder kurz AARP, ist ein Protokoll, dass die Adressen dynamisch von der Sicherungsschicht auf höhere Schichten abbildet. AARP benutzt die physikalische Token-Ring- oder Ethernet-Adresse zur Abbildung der Knoten in einem AppleTalk-Netz. Als Bestandteil der AppleTalk MIB ist die AARP-Tabelle in der Lage die Address Mapping-Tabelle zu verwalten.\n\nEs ist definiert in RFC 1742.\n\nDas Protokoll gehört zur Sicherungsschicht.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "833601", "url": "https://de.wikipedia.org/wiki?curid=833601", "title": "AppleTalk Session Protocol", "text": "AppleTalk Session Protocol\n\nAppleTalk Session Protocol (ASP) ist ein Begriff aus der Informatik.\n\nDas AppleTalk Session Protocol, oder kurz ASP, ist ein Protokoll, das einen gesicherten Datentransport in einem AppleTalk-Netz ermöglicht. ASP ist eine Erweiterung des AppleTalk Transaction Protocols und erlaubt zwei Prozessen den Austausch von Transaktionen und Befehlen.\n\nEs ist definiert in RFC 1742.\n\nDas Protokoll gehört zur Sitzungsschicht.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "833609", "url": "https://de.wikipedia.org/wiki?curid=833609", "title": "AppleTalk Data Stream Protocol", "text": "AppleTalk Data Stream Protocol\n\nAppleTalk Data Stream Protocol (ADSP) ist ein Begriff aus der Informatik.\n\nDas AppleTalk Data Stream Protocol, oder kurz ADSP ist ein symmetrisches, verbindungsorientiertes Protokoll in einem AppleTalk-Netz, über das zwei Rechner eine virtuelle Datenverbindung aufbauen und über diese in beiden Richtungen kommunizieren können. \n\nEs ist definiert in RFC 1742.\n\nDas Protokoll gehört zur Sitzungsschicht.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\n"}
{"id": "835065", "url": "https://de.wikipedia.org/wiki?curid=835065", "title": "Algorithmus von Cohen-Sutherland", "text": "Algorithmus von Cohen-Sutherland\n\nDer Algorithmus von Cohen-Sutherland ist ein Algorithmus zum Abschneiden (Clipping) von Linien an einem Rechteck. Er ist nach seinen Erfindern Danny Cohen und Ivan Sutherland benannt. Der Algorithmus gilt als populärster, wenn auch nicht effizientester für seine Zwecke. Er eignet sich besonders für Fälle, in denen ein hoher Anteil der zu clippenden Linien inner- oder außerhalb des Rechtecks liegt.\n\nZuerst werden jeweils für die beiden Endpunkte der zu zeichnenden Linie vier Flags ermittelt, die gesetzt werden, wenn sich der Endpunkt links des Rechtecks, rechts davon, darunter oder darüber befindet. Ist keines der Flags gesetzt, dann liegen beide Endpunkte innerhalb des Rechtecks, es ist kein Clipping notwendig und die Linie kann einfach gezeichnet werden.\n\nWenn dieser triviale Fall nicht eintritt, werden im nächsten Schritt die einander entsprechenden Flags beider Endpunkte angesehen. Ist mindestens eines dieser Flags bei beiden Endpunkten gesetzt, so befindet sich die gesamte Linie außerhalb des Rechtecks und die Linie braucht nicht gezeichnet zu werden.\n\nWenn auch dieser einfache Fall nicht auftritt, wird der Schnittpunkt einer (beliebigen) Rechteck-Seite mit dem Liniensegment berechnet und der überlappende Teil erneut getestet (und eventuell gekürzt), bis schließlich beide Punkte innerhalb des Rechtecks liegen.\n\nDas folgende Programm gibt ein Beispiel einer Implementierung des Cohen-Sutherland-Algorithmus in C++, anhand dessen man die Funktionsweise gut nachvollziehen kann.\n\n"}
{"id": "836220", "url": "https://de.wikipedia.org/wiki?curid=836220", "title": "Personal Store", "text": "Personal Store\n\nDer Personal Store (PST) des E-Mail-Programmes Microsoft Outlook für Windows ist eine proprietäre Container-Datei, in der Aufgaben, Notizen, E-Mails und der Kalender gespeichert werden. Das Dateiformat PST wurde von Microsoft im Rahmen seiner \"Open Specification Promise\" veröffentlicht.\n\nNeben einer Standarddatei kann Outlook beliebig viele weitere PST-Dateien verwalten. Für eine Datensicherung kann eine Kopie der PST-Dateien erstellt werden. Weitere Outlookdaten, die außerhalb der PST-Dateien verwaltet werden (z. B. E-Mailkonten, Signaturen, Programmeinstellungen usw.), bleiben hierbei jedoch unberücksichtigt.\n\nDie maximale Größe dieses Dateityps betrug 2 Gigabyte für die Windows-Outlook-Versionen Outlook 97 bis Outlook 2002.\n\nDas Format der Outlook-PST-Datei hat sich mit der Version von Outlook 2003 geändert. Ab Outlook 2003 wird eine Unicode-PST-Datei verwendet, deren maximale Speichergröße bei 20 Gigabyte liegt. Mit Office 2010 wurde diese Grenze auf 50 Gigabyte angehoben. Nach Änderung eines Schlüssels in der Windows-Registrierungsdatenbank kann diese Größe allerdings wieder beschränkt werden.\n\nWährend Entourage nicht mit PST-Dateien umgehen konnte, kann Outlook für Mac 2011 nur Unicode-PST-Dateien importieren, ältere ANSI-PST-Dateien aus Outlook 2002 und ältere können nicht importiert werden. Der Export einer OLM-Datei, dem Containerformat des Outlook für Mac, in eine PST-Datei ist hingegen nicht möglich.\n\nDas Format kann in Windows nur dann in andere Mailprogramme exportiert werden, wenn Outlook ebenfalls installiert ist (Verwendung der MAPI). Alternativ gibt es das \"libpst\"-Library, eine Anwendung dessen ist das Kommandozeilenprogramm \"readpst\", welches in der Lage ist, das Format zu lesen. \n\nEine gemeinsame Verwendung von PST-Dateien als Groupware ist mit Outlook nicht möglich. Die Groupware für Outlook ist der Microsoft Exchange Server. Es gibt wenige Alternativen zum Exchange Server, die auf Basis von vernetzten PST-Dateien als MAPI-Server funktionieren, z. B. Public ShareFolder oder OLfolders.\n\n\n"}
{"id": "838380", "url": "https://de.wikipedia.org/wiki?curid=838380", "title": "Electronic Design Automation", "text": "Electronic Design Automation\n\nDie Aufgabe der EDA ist die Entwurfsautomatisierung auf unterschiedlichen Ebenen:\n\n\nAlphabetisch sortierte Auswahl von kommerziellen EDA-Anbietern für den Entwurf von ICs, ASICs und FPGAs (nur universelle EDA-Werkzeughersteller aufgelistet, die nicht spezifisch für nur einen Halbleiterhersteller sind):\n\nHersteller von CPLDs und FPGAs bieten ihren Kunden oftmals eigene EDA-Tools zu sehr günstigen Konditionen an, da diese Firmen nicht von den Softwarelizenzen leben, sondern diese eher als Marketing nutzen, um ihre ICs besser verkaufen zu können. Wichtige Anbieter derartiger EDA-Tools sind alle namhaften Anbieter wie die Firmen Altera, Xilinx und Actel. In Teilen bestehen deren Tools auch aus eingeschränkten Tools der oben genannten Anbieter.\n\nAlphabetisch sortierte Auswahl von Programmen für das Leiterplatten-Design:\n\n"}
{"id": "839654", "url": "https://de.wikipedia.org/wiki?curid=839654", "title": "Online-Demonstration", "text": "Online-Demonstration\n\nEine Online-Demonstration oder ein Virtuelles Sit-In ist eine politische Aktionsform im World Wide Web. Durch wiederholtes Aufrufen einer bestimmten Homepage von zahlreichen Computern aus und innerhalb einer festgelegten Zeitspanne wird eine Blockade des Servers beabsichtigt, über den die betreffende Homepage erreichbar ist. Bei einem technischen Erfolg ist die entsprechende Webseite unerreichbar oder nur stark verlangsamt abrufbar.\n\nOnline-Demonstrationen tragen den Charakter einer Blockade oder eines Sit-Ins. Sie richten sich wie herkömmliche Demonstrationen gegen die Politik von Staaten, Konzernen, Organisationen usw. und zielen auf die Lahmlegung der Webseite durch Zugriffe zahlreicher Demonstranten.\n\nTechnisch gesehen sind Online-Demonstrationen eine bestimmte Form von DDoS-Attacken. Bei Online-Demonstrationen wird sich jedoch keiner fremder Rechner – zum Beispiel in Form eines Bot-Netzwerkes – bedient. Um technisch zum Erfolg zu kommen, ist daher eine große Teilnehmerzahl notwendig.\n\nDie erste dokumentierte Aktion im Rahmen dieses Konzeptes fand am 21. Dezember 1995 statt. Die Gruppe \"Strano Network\" veranstaltete ein Virtuelles Sit-In auf verschiedenen Seiten der französischen Regierung, um gegen die Atomtests auf dem Pazifikatoll Mururoa zu protestieren. Internetnutzer waren dazu aufgerufen, diese Seiten für eine Stunde immer wieder aufzurufen. Dies hatte wenig Auswirkungen, da das Internet damals noch nicht die Popularität von heute besaß und deshalb wenig Resonanz und Teilnahme zu verzeichnen war. Erst drei Jahre später, am 29. Januar 1998, gab es die nächste direkte Aktion im Internet. Eine Stunde wurden verschiedene Seiten von mexikanischen Finanzinstituten blockiert. Eine \"Anonymous Digital Coalition\" hatte dazu aufgerufen mit dem Ziel, auf den Krieg zwischen der mexikanischen Armee und der EZLN in der Provinz Chiapas aufmerksam zu machen.\n\nDas \"Electronic Disturbance Theater\" (EDT) um Ricardo Dominguez hatte damals das \"Zapatista Floodnet Tool\" entwickelt, ein inzwischen legendäres Script, das automatisiert immer wieder die anzugreifenden Internetseiten neu lud, um den betreffenden Server zu „fluten“. Im Dezember 1999 war \"EDT\" am sogenannten toywar beteiligt, einer wochenlangen Netzschlacht zwischen der Spielzeugfirma eToys und der Künstlergruppe etoy. Das \"EDT\" gilt heute als Vorreiter des elektronischen Widerstandes. Dominguez sieht Online-Demonstrationen in der Tradition der amerikanischen Bürgerrechtsbewegung der 1960er Jahre und nennt sie „Elektronischen Zivilen Ungehorsam“.\n\nDie erste Online-Demonstration in Deutschland fand am 29. Juni 2000, zwischen 21:15 und 22:00 Uhr statt. Im Rahmen des Projekts ActiveLink der Merz Akademie in Stuttgart organisierte der Internet-Aktivist Alvar Freude einen Online-Protest gegen die Gesetzgebung und Haftungsregeln bezüglich Hyperlinks. Anlass waren Massenabmahnungen des Anwalts Günter Freiherr von Gravenreuth wegen Links auf fremde Webseiten. Die Online-Demo sollte der Startschuss für die Gründung einer Online-Demonstrations-Plattform sein. Die Plattform wird heute als ODEM.org weitergeführt, nutzte aber nicht mehr das Demonstrationsformat einer Online-Demonstration. Freude war später federführend in den Kampagnen gegen die Sperrungen ausländischer Internet-Seiten in Nordrhein-Westfalen und gegen das Zugangserschwerungsgesetz sowie bei der Gründung des Arbeitskreises gegen Internetsperren und Zensur involviert und führte somit die Ziele der ersten Online-Demo mit anderen Mitteln weiter. Er sah virtuelle Sit-Ins als Protestform dann als legitim an, wenn es sich um ein Online-Thema handelt und andere Protestformen nicht oder nur schwer umsetzbar sind.\n\nFälschlicherweise wird als erste Online-Demonstration häufig eine Aktion der Initiativen Libertad! und kein mensch ist illegal im Rahmen der antirassistischen \"Deportation.class\"-Kampagne, die die Beteiligung von Fluggesellschaften an staatlichen Abschiebungen durch vielfältige Aktionsformen kritisierte, bezeichnet. Die Organisationen riefen dazu auf, am Tag der Lufthansa-Aktionärsversammlung, die am 20. Juni 2001 in Köln stattfand, die Homepage der Lufthansa für zwei Stunden zu blockieren. Die Demonstration wurde beim Kölner Ordnungsamt angemeldet. Als Versammlungsort gaben die Initiatoren „www.lufthansa.com“ an.\n\nSchon vor ihrem Start hatte die Demonstration einen ersten Erfolg. Die Online-Demonstration wurde von vielen deutschen Print- und Internetmedien aufgegriffen und hatte auch weltweite Medienresonanz. In der Berichterstattung wurde auch über ihr Anliegen informiert und damit das Abschiebegeschäft der Lufthansa thematisiert. Über 13.000 Menschen beteiligten sich schließlich an der Online-Demonstration gegen Lufthansa. Zum Einsatz kam dabei auch eine Protest-Software, deren Quellcode inzwischen veröffentlicht ist. Die Webseite des Konzerns war innerhalb der zweistündigen Blockade knapp zehn Minuten weltweit nicht erreichbar und auch in der übrigen Zeit nicht oder nur schwer aufrufbar.\n\nLufthansa stellte nach der Demonstration Strafanzeige. Im Rahmen der Ermittlungen kam es im Oktober 2001 zu Hausdurchsuchungen und zahlreichen Beschlagnahmungen in Privat- und Geschäftsräumen von \"Libertad!\". Im Frühsommer 2005 fand am Amtsgericht Frankfurt am Main ein Prozess gegen den Domaininhaber von „www.libertad.de“ wegen Nötigung und Aufruf zu Straftaten statt. Der Angeklagte wurde im Mai 2006 vom Oberlandesgericht Frankfurt am Main im Revisionsverfahren freigesprochen. Das Gericht hat in der Urteilsbegründung festgestellt, dass eine Online-Demonstration „weder das Tatbestandsmerkmal der Gewalt, noch das der Drohung mit einem empfindlichen Übel“ erfüllt.\n\nWährend des Prozesses gegen \"Libertad!\" wurde als Solidaritätsaktion zu einer Online-Demonstration gegen den Ausbau des Frankfurter Flughafens auf der Webseite des Flughafenbetreibers Fraport aufgerufen.\n\nNur in Deutschland kam es bisher zu Strafverfahren gegen Online-Demonstrationen. Die Frankfurter Staatsanwaltschaft interpretierte die Online-Demo gegen Lufthansa als Nötigung der Lufthansa-Kunden sowie der Lufthansa selbst. Nach einem zweitägigen Prozess schloss sich das Gericht dieser Auffassung an. Nötigung sei sowohl wegen Anwendung von Gewalt als auch wegen Drohung mit einem empfindlichen Übel gegeben. Das Urteil war unter Juristen umstritten. Nicht wenige sehen Online-Demonstrationen durch das Grundrecht auf freie Meinungs- und Versammlungsfreiheit gedeckt. Gegen das Urteil des Frankfurter Amtsgericht wurde im Juli 2005 Revision eingelegt. Das Oberlandesgericht Frankfurt am Main sprach im Mai 2006 die Initiatoren der Online-Demo frei und erklärte, dass eine Online-Demonstration „weder das Tatbestandsmerkmal der Gewalt, noch das der Drohung mit einem empfindlichen Übel“ beinhalte (Aktenzeichen: 1 Ss 319/05).\n\nMittlerweile gibt es zur Frage, ob Online-Demonstrationen eine Versammlung im verfassungsrechtlichen Sinne darstellen, auch eine Stellungnahme der Bundesregierung: In ihrer Antwort auf eine sogenannte Kleine Anfrage der Fraktion Die Linke stellt die Bundesregierung fest, dass „virtuelle Versammlungen mangels Körperlichkeit“ keine Versammlungen im Sinne von Artikel 8 GG seien. Damit können sich die Initiatoren dieser „Online-Demonstrationen“ nicht auf das Versammlungsgrundrecht berufen.\n\n\n"}
{"id": "840277", "url": "https://de.wikipedia.org/wiki?curid=840277", "title": "Douglas-Peucker-Algorithmus", "text": "Douglas-Peucker-Algorithmus\n\nDer Douglas-Peucker-Algorithmus (auch Ramer-Douglas-Peucker-Algorithmus) ist ein Algorithmus zur Kurvenglättung im Bereich der Vektorgrafik und Generalisierung von Karten. Das Ziel ist, einen durch eine Folge von Punkten gegebenen Streckenzug durch Weglassen einzelner Punkte (engl. weeding) so zu vereinfachen, dass die grobe Gestalt erhalten bleibt. Der Grad der Vergröberung wird gesteuert durch Vorgabe des maximalen Abstands zwischen den ursprünglichen Punkten und dem approximierenden Streckenzug. Die Ausgangsform des Algorithmus wurde von Urs Ramer und (unabhängig) von David Douglas und Thomas Peucker angegeben.\n\nDer Algorithmus betrachtet den Streckenzug als ganzes (globaler Ansatz) und schreitet zu feineren Approximationen fort. Dazu wird die Ausgangsfolge geeignet in zwei Abschnitte geteilt, die dann ihrerseits den Algorithmus durchlaufen (siehe Rekursion). Der Algorithmus realisiert damit einen Ansatz nach dem Prinzip Teile und Herrsche.\nGegeben ist der Ausgangsstreckenzug (Bild 0) als Folge von \"n\" Punkten\nsowie die Toleranz formula_2.\n\nAls Approximation von \"K\" wird die Strecke formula_3 aus erstem und letztem Punkt betrachtet, a in Bild 1. Um zu prüfen, ob diese Approximation ausreicht, wird unter den formula_4 inneren Punkten von \"K\" derjenige Punkt formula_5 gesucht, welcher den größten Abstand von dieser Strecke hat:\n\nIn Bild 1 ist dies der Punkt c mit dem Abstand b. Ist formula_7 oder formula_8, so ist die Approximation ausreichend und die inneren Punkte werden ggf. verworfen. Andernfalls wird die Approximation zu formula_9 verfeinert und die beiden Teilfolgen\nwerden ihrerseits daraufhin überprüft, ob ihre inneren Punkte verworfen werden können (Bild 2 und 3).\n\nDas Ergebnis des Algorithmus ist der durch die Folge der nicht verworfenen Punkte definierte Streckenzug, blau in Bild 4. Keiner der verworfenen Punkte, grau in Bild 4, hat zum Ergebnis einen Abstand größer als formula_12.\n\n function DouglasPeucker(PointList[], epsilon)\n\nLiegt der Streckenzug (zumindest in guter Näherung) in einer Ebene, so lassen sich die Abstände formula_13 effizient berechnen, indem man vor der Iteration über die inneren Punkte formula_14 einen in der Ebene liegenden Normaleneinheitsvektor zur Geraden durch formula_15 und formula_16 ermittelt und diesen dann jeweils mit den Verschiebungsvektoren formula_17 skalar multipliziert. In mehr als zwei Dimensionen berechnet man zuerst den Fußpunkt des Lotes.\n\nDie Autoren Ramer bzw. Douglas und Peucker hatten die Möglichkeit nicht berücksichtigt, dass der Fußpunkt des Lotes nicht auf der Verbindungslinie liegt, sondern außerhalb, auf ihrer Verlängerung. Dadurch können Punkte wegfallen, die vom Endergebnis einen größeren als den zugesicherten Abstand haben.\n\n"}
{"id": "841494", "url": "https://de.wikipedia.org/wiki?curid=841494", "title": "Final Gathering", "text": "Final Gathering\n\nFinal Gathering (auch Final Gather) ist ein Algorithmus der Bildsynthese in der Computergrafik, der auf Raytracing basiert.\n\nDabei wird es sehr oft in Verbindung mit globaler Beleuchtung (\"global illumination\") eingesetzt um optimale Beleuchtungs- und Schatteneffekte zu erzeugen.\nBesonders für diffuse Schattierungen und realistische Außenszenen wird dieser Algorithmus verwendet.\n\nFinal Gathering war ursprünglich als performanceschonende Caching-Technik gedacht, die globale Beleuchtung in Geschwindigkeit und Qualität verbessern und ergänzen sollte. Durch die Ermittlung der ungefähren lokalen Strahlungsintensität nach dem Photon Tracing kann in Verbindung mit globaler Beleuchtung eine schnellere Berechnung der Beleuchtung erfolgen, da die globale Beleuchtung nicht so exakt berechnet werden muss.\nWeiterhin verringert sich der „dunkle-Ecken-Effekt“ und das niederfrequente Rauschen.\n\nDie Berechnung von Final Gathering läuft grob nach folgendem Schema ab und hängt von diversen Einstellungen ab:\n\n\nVon der Kamera ausgehend werden sogenannte Eye Rays in die Szene ausgesandt. An den Stellen, an denen es zu einem Schnitt mit der Szenengeometrie kommt, werden Final Gathering Rays von diesem Schnittpunkt aus in die in Normalenrichtung orientierte Hemisphäre ausgesandt.\nDabei passt sich diese Art Raster aus Final Gather Points der Szene an. Weniger Final Gather Points werden zum Beispiel auf großen Flächen benötigt, mehr hingegen in detailreichen Regionen und Ecken; somit wird eine schnelle und akkurate Berechnung gewährleistet.\n\nDiese Final Gathering Rays laufen so lange bis sie auf Geometrie treffen und geben dann ihre Informationen über Farbe und Helligkeit jenes Schnittpunktes mit der Geometrie wieder.\n\nAll diese Informationen über die Beleuchtung der Szene werden in einer 3D Final Gathering Map im Arbeitsspeicher oder für spätere Berechnungszwecke auf Festplatte abgespeichert. Weiteres zur Final Gathering Map siehe unter \"Animationen\".\n\nDie Berechnung von Final Gathering Points in der Presampling-Phase (Vorberechnung) ist sehr rechenaufwendig, so dass zur Ermittlung der lokalen Strahlungsintensität benachbarte Punkte herangezogen werden.\nAbhängig ist dies von den Einstellungen für den maximalen und minimalen Radius:\n\nFür den Benutzer sind diese Werte von Bedeutung, da diese die Geschwindigkeit der Berechnung und Qualität des Bildes stark beeinflussen.\nEin Beispiel wäre eine Vertiefung in Form einer feinen Linie in der Geometrie. Bei einem zu großen minimalen Radius würde diese Linie lückenhaft und artefaktbehaftet erscheinen.\n\nDie Final Gathering Rays legen je nach Einstellung nicht nur die eine Strecke zurück bis sie auf Geometrie treffen, sondern werden gegebenenfalls reflektiert oder gebrochen.\nVom ersten Schnittpunkt mit der Geometrie sucht der FG Ray dann weiter, bis er wiederholt auf Geometrie trifft oder nicht.\nWie oft ein FG Ray reflektiert und/oder gebrochen werden darf legt die Trace Depth fest. Auch diffuse Materialien reflektieren Licht!\n\nNormal ist ein Trace Depth Wert von 1–3. Dabei ist zu beachten, dass die Anzahl der zulässigen Reflexionen und Brechungen separat gesteuert werden kann, aber ihre Summe nicht größer sein kann als der Trace Depth Wert. Dies ist so zu verstehen, dass zum Beispiel ein FG Ray erst einmal gebrochen wird (von möglichen drei) und dann zweimal reflektiert wird, danach hört die Berechnung für diesen Strahl auf, da der Trace Depth von hier drei erreicht wurde.\n\nDieser Wert beeinflusst das berechnete Bild bedeutend, da die Berechnung bei Hohen Trace Depth Werten weitaus aufwendiger und genauer ist.\nEin Raum mit nur einem Fenster als Lichtquelle würde bei einem niedrigen Trace Depth (1–2) mitunter zu dunkel erscheinen, da nur FG Rays die direkt das Fenster treffen die Szene beleuchten. Bei höheren Werten (4–8) erscheint der Raum heller, weil es durch die zusätzlichen Reflexionen zu einer viel stärkeren indirekten Beleuchtung gekommen ist. Ähnlich wie bei der globalen Beleuchtung.\n\nNormalerweise laufen die FG Rays so lange, bis sie auf Geometrie stoßen. Der falloff-Wert bestimmt diese Strecke.\nDas heißt, solle ein FG Ray sein \"Lebensende\" (maximal zurückgelegte Strecke – der falloff-Wert) erreicht haben und keine Geometrie gefunden haben, so stoppt dieser die Suche und gibt eine vorher eingestellte Hintergrundfarbe wieder.\nDadurch wird der Final Gathering Prozess beschleunigt – kann aber auch bei einem zu niedrigem Wert (deutlich) ungenauer werden.\n\nFinal Gathering wird von allen großen unterstützt, da in ihnen Mental Ray als Renderer integriert ist.\nIn der Film-Industrie ersetzt Final Gathering teilweise die globale Beleuchtung.\nFinal Gathering kann alleine eingesetzt werden oder in Kombination mit globaler Beleuchtung.\nBei dieser Kombination ergänzen sich beide Techniken, so dass man die Qualitätseinstellungen des einzelnen Verfahrens teilweise halbieren kann.\nDiese Kombination bietet ein Höchstmaß an visueller Qualität wie Realitätsnähe – in produktionsfähigen Renderzeiten.\n\nWie auch beim alleinigen Rendern mit Final Gathering setzt eine effektive Nutzung ein umfassendes Verständnis für die Einstellmöglichkeiten und Erfahrung voraus.\n\nEin Nachteil bei Final Gathering ist der nicht problemlose Einsatz für Animationen. Dabei kann es zu einem Flackern (z. B. Helligkeitsschwankungen im Bild) kommen.\nUm dies zu verhindern, wird die Final Gathering 3D-Map nicht für jedes Bild erneuert (wie es standardmäßig der Fall ist), sondern man friert sie ein oder berechnet nur die Bereiche, die noch nicht von der Final Gathering Map erfasst wurden.\n\nEine weitere Möglichkeit besteht darin die globale Beleuchtung in Kombination mit Final Gathering einzusetzen. Das Flackern wird dadurch vermieden.\n"}
{"id": "841978", "url": "https://de.wikipedia.org/wiki?curid=841978", "title": "Informationsvisualisierung", "text": "Informationsvisualisierung\n\nInformationsvisualisierung ist ein Forschungsgebiet, das sich mit computergestützten Methoden zur grafischen Darstellung abstrakter Daten beschäftigt. Abstrakte Daten werden hierbei als numerische, relationale oder textuelle Daten verstanden, für die eine räumliche Darstellung nicht unmittelbar gegeben ist. Im Gegensatz dazu beschäftigt sich die Wissenschaftliche Visualisierung mit der Darstellung räumlicher Daten.\n\nDie bildlichen Darstellungsmethoden sollen helfen, die Daten auszuwerten und aus ihnen neue Erkenntnisse zu gewinnen. Ebenso unterstützen Informationsvisualisierungen die Kommunikation und Veranschaulichung dieser Erkenntnisse.\n\nGemeinhin wird Information begrifflich von Wissen unterschieden. So wird auch von der Informationsvisualisierung selbst unterschieden die \"Wissensvisualisierung\" () als ein Prozess der Wissensübermittlung \"()\", welcher sich grafischer Mittel jeder Art bedient, auch nicht computer-gebundener. Grundlage für die Wissensvisualisierung sind die lernpsychologischen Vorzüge der visuellen Kommunikationsform.\n\nDie Informationsvisualisierung ist ein relativ neu entstandenes interdisziplinäres Gebiet, das unter anderem Methoden und Erkenntnisse der Informatik, Statistik, Data-Mining sowie der Kognitionswissenschaft verwendet. Ziel ist hierbei auch, die Mensch-Computer-Interaktion zu verbessern.\n\nAufgabenstellung der Informationsvisualisierung ist grundsätzlich die \"expressive und dabei effektive Darstellung\" der Datenmuster und der darin enthaltenen Informationen. Expressiv bedeutet, dass alle Daten und nur die Daten in die Visualisierung einfließen. Effektivität besagt, dass sich der Betrachter einer Visualisierung möglichst schnell einen Überblick über die in den Daten enthaltenen Informationen verschaffen können soll. Dabei ist Wahrnehmungseffekten, etwa wie optische Täuschungen, Rechnung zu tragen.\n\nVisualisierungen können sowohl zur Präsentation von Informationen dienen oder zur explorativen Datenanalyse eingesetzt werden. Bei einer explorativen Analyse handelt es sich um einen offenen, erforschenden Prozess, bei dem ein genaues Analyseziel zunächst nicht formuliert werden muss.\n\nDie bedeutendste wissenschaftliche Konferenz für Informationsvisualisierung ist die \"IEEE Information Visualization (InfoVis)\", die jährlich im Rahmen der \"IEEE VIS\" stattfindet. Weitere bedeutende wissenschaftliche Konferenzen mit ähnlichem Schwerpunkt sind die \"EG/VGTC Conference on Visualization (EuroVis)\" sowie das \"IEEE Pacific Visualization Symposium (PacificVis).\"\n\nIn seinem Artikel \"Searching for intellectual turning points\" stellt Chaomei Chen eine Methode für die Analyse von Zitationsräumen dar. Auf seiner Homepage ist ein Tool zur Analyse des Web of Science vorhanden. Darin können nicht nur bibliografische Daten, sondern auch die Zitationen von Autoren durch andere Autoren nachverfolgt werden. Die Untersuchung von Kozitation ist eine seit langem bekannte informationstechnische Methode. Neu ist bei Chen die Anwendung von Progressive Pathfinder Network Scaling auf das Problem Cocitation. Diese Analyse arbeitet mit \"Pathfinder Associative Networks\". Der Algorithmus stammt von dem Psychologen Roger W. Schvaneveldt und gehört in den Bereich der künstlichen Intelligenz.\n\nDas Tool ist sehr flexibel und bietet die Möglichkeit, andere Analysen wie z. B. autoren- oder publikationsbezogene Clusteranalyse durchzuführen.\n\nFür den Ontologie-Editor \"Protégé-2000\" existieren verschiedene Visualisierungs-Plug-ins, die eine Darstellung von Ontologien, bzw. Semantischen Netzwerken im Graphenformat mit variablem Layout ermöglichen.\nDes Weiteren existieren erweiterte Ansätze zur Visualisierung von Semantik, die über reine formale Ontologien hinausgehen und mit analytischen Ansätzen Semantik generieren.\n\nJe nach Datentyp können unterschiedliche Visualisierungstechniken verwendet werden. Für häufig auftretende Datentypen haben sich gewisse visuelle Darstellungsformen und Diagramme als Standards etabliert.\n\nZeitliche Veränderungen einer numerischen Variable werden häufig als Balken-, Säulen- oder Liniendiagramm dargestellt, wobei eine der beiden Achsen des verwendeten zweidimensionalen Koordinatensystems die jeweilige zeitliche Abfolge abbildet. Vergleicht man mehrere Objekte hinsichtlich einer numerischen Variable (univariate Daten), können ähnliche Diagramme zum Einsatz kommen – die Zeitachse wird durch eine Liste der zu vergleichenden Objekte ersetzt. Zudem eignen sich Kreisdiagramme zum Vergleich numerischer Werte, insbesondere wenn die Summe der Werte eine interpretierbare Gesamtheit ergibt (z. B. die Zusammensetzung von Stimmanteilen).\n\nMultivariate Daten beschreiben eine Menge von Objekte anhand mehrerer numerischer Variablen. Einfache Streudiagramme erlauben den Vergleich zweier Variablen durch Markierung der Objekte als Punkte in einem zweidimensionalen Koordinatensystem. Eine Streudiagrammmatrix stellt darauf aufbauend alle paarweisen Kombinationen von Variablen vergleichend dar. Als weiterer Diagrammtyp setzen Parallele Koordinaten die Variablen als parallel Achsen um und zeichnen jedes Objekt als Polylinie ein, die alle Achsen entsprechend der Variablenwerte des Objekts verbindet.\n\nNetzwerke (in der Informatik auch \"Graph\" genannt) stellen Beziehungen zwischen Objekten dar. In Knoten-Kanten-Diagramme repräsentieren Knoten (meist Kreise oder Rechtecke) die Objekte, die mit geraden oder geschwungenen Linien verbunden werden. Techniken zur Darstellung solcher Diagramme, insbesondere zur Berechnung der Knoten und Kantenpositionen bezeichnet man als Graphzeichnen. Alternativ können Netzwerke auch als Adjazenzmatrix visualisiert werden.\n\nHierarchien (in der Informatik auch \"Baum\" genannt) sind ein Spezialfall von Netzwerken, bei denen ein Objekt jeweils nur einem übergeordneten Objekt zugewiesen wird. Zur Darstellung von Hierarchien werden ebenfalls Knoten-Kanten-Diagramme verwendet. Aber auch alternative Darstellungen wie Tree Maps sind üblich, die eine Fläche rekursiv in Teilflächen anhand der Hierarchiestruktur unterteilen.\n\n\nInformationsvisualisierungen können in einer Vielzahl unterschiedlicher Anwendungen zum Einsatz kommen. Bedeutende Anwendungsgebiete sind unter anderen:\n\n\n\n\n\n"}
{"id": "842518", "url": "https://de.wikipedia.org/wiki?curid=842518", "title": "Einfache Genauigkeit", "text": "Einfache Genauigkeit\n\nIn der Mathematik und Informatik ist einfache Genauigkeit ( oder auch nur ) eine Bezeichnung für ein Gleitkommaformat, das eine Speichereinheit im Rechner belegt. Damit sind die genauen Details abhängig vom Rechnerhersteller und dessen Speicherarchitektur. Speziell für Mikroprozessoren mit byteweisem Speicher wurde der IEEE 754 Standard entworfen, der 4 Byte (32 Bit) für dieses Zahlenformat vorschreibt. Die Bezeichnung ist nicht Gleitkommazahlen vorbehalten; sie ist auch für ganze Zahlenformate anwendbar.\n\nEine IEEE-754-Zahl hat im Dezimalsystem eine Genauigkeit zwischen 7 und 8 geltenden Ziffern.\n\nOft besteht die Notwendigkeit, Ergebnisse mit höherer Genauigkeit zu berechnen, dafür gibt es das Zahlenformat doppelte Genauigkeit.\n\nDa Programmiersprachen wie C einfache Genauigkeit grundsätzlich zweitrangig behandeln, der doppelte Speicherverbrauch heute nur noch eine geringe Rolle spielt und auch die höhere Rechenzeit kaum ins Gewicht fällt, entsteht für doppelte Genauigkeit dasselbe numerische Problem wie für die einfache. Daher wurden in der Revision IEEE 754-2008 vierfach genaue Zahlenformate eingeführt.\n\nFür spezielle numerische Aufgaben, z. B. in der Computergrafik und für didaktische Zwecke, existieren auch kürzere Zahlenformate als einfache Genauigkeit.\n\n"}
{"id": "842532", "url": "https://de.wikipedia.org/wiki?curid=842532", "title": "Doppelte Genauigkeit", "text": "Doppelte Genauigkeit\n\nDoppelte Genauigkeit ( oder auch ) steht in der Computerarithmetik für ein Gleitkomma-Zahlenformat, bei dem eine Zahl 8 Byte (also 64 Bit) belegt. Es belegt damit doppelt so viel Speicher wie Gleitkommazahlen einfacher Genauigkeit. Bei der Gleitkommadarstellung werden 11 Bit für den Exponenten verwendet und ein Bit für das Vorzeichen, die restlichen 52 Bit stehen für die eigentliche Zahlendarstellung (Mantisse) zur Verfügung. Eine Präzision von 53 Bit bedeutet umgerechnet ins Dezimalsystem eine ungefähre Genauigkeit auf 16 Stellen im Dezimalsystem (formula_1).\n\nDie Bezeichnung ist nicht nur für Gleitkommazahlen vorbehalten, sondern auch für Ganz-Zahl-Formate anwendbar.\n\nDie Darstellung von solchen Gleitkommazahlen ist seit dem Jahr 1985 durch das Institute of Electrical and Electronics Engineers in der Norm 754 spezifiziert.\n\n"}
{"id": "842560", "url": "https://de.wikipedia.org/wiki?curid=842560", "title": "Vierfache Genauigkeit", "text": "Vierfache Genauigkeit\n\nIn der Informatik ist vierfache Genauigkeit () eine Bezeichnung für ein Zahlenformat, das vier Speichereinheiten im Rechner belegt. Damit sind die genauen Details abhängig vom Rechnerhersteller und dessen Speicherarchitektur. In der aktuellen Fassung des IEEE 754-2008 Standards ist der in der folgenden Graphik gezeigte 128-Bit-Datentyp beschrieben:\n\nDie Bezeichnung ist nicht nur für Gleitkommazahlen vorbehalten, sondern auch für Ganzzahlformate anwendbar (Double Quadword).\n\n"}
{"id": "844989", "url": "https://de.wikipedia.org/wiki?curid=844989", "title": "Fraktalkunst", "text": "Fraktalkunst\n\nDie Fraktalkunst (engl. „Fractal Art“) ist eine relativ weit verbreitete Unterform der digitalen Kunst und beschäftigt sich mit der Erschaffung von digitalen Bildern, die im Wesentlichen aus einem oder mehreren Fraktalen bestehen. Als eigenständige Kunstform ist die Fraktalkunst selbst unter den Künstlern umstritten, da einige argumentieren, man bilde lediglich Teile einer mathematisch-geometrischen Form als Kunstwerk ab, was nicht die eigenständige Schöpfungshöhe erreicht. Es gibt diverse Programme, die zum Erschaffen von Fraktalkunst verwendet werden, zu ihnen gehören die Freeware-Programme Apophysis bzw. Apo7x und Fractal Explorer, sowie Ultra-Fractal.\n\n\n"}
{"id": "846958", "url": "https://de.wikipedia.org/wiki?curid=846958", "title": "Star Seven", "text": "Star Seven\n\nDer Star Seven (im Original: *7) war ein im Rahmen des „Green Project“ bei Sun Microsystems entwickelter, portabler Mikrocomputer, der zur Steuerung von Haushaltsgeräten dienen sollte. Nach seiner internen Vorstellung im Herbst 1992 wurde das Unternehmen \"First Person, Inc.\" gegründet, welches die Vermarktung übernehmen sollte. Das Gerät kam allerdings nie auf den Markt.\n\nPatrick Naughton startete im Dezember 1990 das Projekt \"The Green Project\", das ursprünglich der Entwicklung einer Software für interaktives Fernsehen und andere Geräte der Konsumelektronik dienen sollte. James Gosling und Mike Sheridan waren ebenfalls beteiligt. Im August 1991 führte der von Gosling entwickelte \"Oak\"-Interpreter (der später in „Java“ umbenannt wurde) auf dem „Green-OS“ genannten Betriebssystem die ersten Programme aus, und Bill Joy hatte mit der Implementierung einer grafischen Oberfläche begonnen. Als Plattform diente ein eigens entwickelter, portabler Mikrocomputer – der \"*7\".\n\nIm Herbst 1992 wurde das Gesamtkonzept bei Sun Microsystems intern vorgestellt. Sun-Chef Scott McNealy zeigte sich von den Ergebnissen so beeindruckt, dass ein eigenes Unternehmen gegründet wurde, um Star Seven zu vermarkten – \"die First Person, Inc.\" war geboren. Das Unternehmen unterlief mehrere Richtungswechsel, unter anderem in Richtung Set-Top-Boxen, der Star Seven kam nie auf den Markt. Bereits 1994 galt die Hardware als obsolet, und man machte sich daran die vielversprechendsten Komponenten (Java) zu eigenständigen Produkten weiterzuentwickeln. Spätestens 1996, als die verbliebenen Mitglieder des Green-Teams das Unternehmen \"JavaSoft\" gründeten, war der Star Seven endgültig vom Tisch.\n\nBestandteil des Systems waren das Betriebssystem Green-OS, der \"Oak\"-Interpreter (später Java) und einige Hardwarekomponenten. Der Benutzer interagierte mit dem System über eine grafische Benutzeroberfläche, die mit Hilfe von Assistenten durch die Dialoge führte. Einer dieser Assistenten war Duke, der sich heute als Maskottchen von Java wiederfindet.\n"}
{"id": "850497", "url": "https://de.wikipedia.org/wiki?curid=850497", "title": "Audiograbber", "text": "Audiograbber\n\nAudiograbber ist eine CD-Ripper-Software zum Rippen und Konvertieren von Audiosignalen. Autor Jackie Franck war zuvor für die Programmierung des \"AudioCatalyst\" von RealNetworks verantwortlich.\n\nDas Programm wurde anfänglich als Shareware vermarktet. Derzeit wird es entweder als Freeware (ohne MP3-Kodierung) im Bündel mit schwer deinstallierbarer Adware angeboten, oder als Kaufsoftware. Im Falle der Freeware-Version werden neben Adware auch noch weitere Softwareprodukte mitinstalliert, die ggf. unerwünscht sein können.\n\nAudiograbber dient primär dem Auslesen von Audio-CDs, ein Vorgang der als Digital Audio Extraction bekannt ist, und ist damit ein CD-Ripper. Die extrahierten Audiodaten können in Formaten wie WAV, WMA oder Vorbis codiert und auf der Festplatte abgelegt werden. Ferner kann auch jeder weitere externe Encoder eingebunden werden, der über eine Kommandozeilen-Funktionalität verfügt. So ist es beispielsweise auch möglich, den weit verbreiteten LAME-Codec mit Audiograbber zu nutzen, um hochqualitative MP3-Dateien zu erstellen. Der beiliegende Fraunhofer-MP3-Encoder kann hingegen aus lizenzrechtlichen Gründen nur in sehr niedrigen Bitraten verwendet werden.\n\nAudiograbber beherrscht das Normalisieren der Lautstärke, das Schreiben von ID3-Tags ohne Verwendung von Unicode unter der fakultativen Nutzung der freedb, das Auslesen von CD-Text und Karaoke-CDs, das Entfernen von Stillepassagen in Songtiteln und das Berechnen einer Prüfsumme. Eine Aufnahmefunktion ermöglicht auch das Einlesen von analogen Tonträgern wie Schallplatten oder Kassetten. Während erfahrenen Benutzern eine Vielzahl an weiteren Einstellungsmöglichkeiten zur Verfügung steht, können Einsteiger auf die automatisch vorgenommenen Standard-Einstellungen zurückgreifen.\n\nDas Programm bietet Benutzerführungen in verschiedenen Sprachen, darunter Englisch, Deutsch, Französisch, Spanisch und Italienisch. Weitere Sprachen können nachinstalliert werden. Zwei deutschsprachige Einsteiger-Anleitungen im PDF-Format sind Audiograbber beigelegt.\n\nDie Website Heise online warnt vor ungefragt zusätzlich installierter Software, die potenziell schädlich sein kann.\n\n"}
{"id": "851272", "url": "https://de.wikipedia.org/wiki?curid=851272", "title": "IEEE 854", "text": "IEEE 854\n\nDie Norm IEEE 854 (ANSI/IEEE Std 854-1987, \"Standard for Radix-Independent Floating-Point Arithmetic\") definiert Standarddarstellungen für basis-unabhängige Gleitkommazahlen in Computern und legt genaue Verfahren für die Durchführung mathematischer Operationen, insbesondere für Rundungen, fest. Als Basis ist nur einer der beiden Werte 2 oder 10 erlaubt.\n\nDie Norm \"IEEE 854\" existiert nicht mehr eigenständig, sondern ist in der IEEE 754-2008, der Überarbeitung der Norm IEEE 754-1989, aufgegangen.\n\n"}
{"id": "851306", "url": "https://de.wikipedia.org/wiki?curid=851306", "title": "Grml", "text": "Grml\n\ngrml (sprich: grummel) ist eine seit Januar 2005 existierende, auf Debian basierende Linux-Distribution und läuft vorrangig als Live-System. Grml wurde als kleines Rettungssystem mit flexiblem Startprozess entworfen. Ursprünglich auf Knoppix basierend, hat sich grml inzwischen zu einem eigenen Debian-GNU/Linux-Derivat weiterentwickelt, das \"als Rettungssystem für Systemadminstrationen\" bestimmt ist. Bis zur Version 2014.11, basierte Grml auf Debian Wheezy. Mit Erscheinen von Version 2017.05 erfolgte ein Wechsel zu systemd und Debian Stretch als Basis. Maintainer ist Michael Prokop, ein Debian-Entwickler aus Österreich.\n\nGrml ist als Live-System ausgelegt. Die Möglichkeit, es mit eigenen Partitionen fest auf die Festplatte zu installieren, haben die Entwickler mittlerweile verworfen. Wer die Vorzüge von Grml dauerhaft nutzen möchte, übernimmt sie idealerweise in ein frisch installiertes Debian. Mit dem Tool grml-debootstrap kann dieses von Grml aus auf der Festplatte installiert werden. Mittels grml2usb kann man Grml aber auf einem Flashspeicher (z. B. USB-Stick) installieren und dann von dort aus booten. Ursprünglich war grml nur für x86 (32-bit) verfügbar. Seit das Projekt die Version 1.0 erreicht hat, gibt es grml auch für die AMD64-Architektur.\n\nBemerkenswert ist zudem der flexible Startprozess, in den durch Steuerdateien auf dem Startmedium schon frühzeitig eingegriffen werden kann. Das ermöglicht die Erstellung spezialisierter Rettungssysteme mit geringem Aufwand. Startparameter als Vorgabe für die Netzwerkkonfiguration, die Installation weiterer Pakete und die Ausführung eigener Skripts lassen sich speichern. WLAN mit WPA wird ebenfalls unterstützt.\n\nGrml ist für Systemadministratoren und Benutzer von textbasierten Werkzeugen prädestiniert. Die Fähigkeiten von grml als Rettungssystem für den Einsatz durch ebendiese Nutzergruppen zeigen sich insbesondere bei der Datenwiederherstellung unter ext3-Dateisystemen, da keine Änderungen am Dateisystem vorgenommen werden. So kann beispielsweise mit dem Programm ext3rminator, das Teil der Live-CD ist, ein Großteil aller Dateien geringerer Dateigröße wiederhergestellt werden. Neben dem mitgelieferten ext3rminator können auch externe Tools wie extundelete und ext3grep zur Datenwiederherstellung von ext3-Dateisystemen unter grml eingesetzt werden. Diese Tools arbeiten auf Basis des Grep-Befehls, mit Hilfe dessen Dateifragmente kopiert und wieder zusammengesetzt werden. Sie bieten sich daher insbesondere zur Rettung von textbasierten Anwendungsdateien wie Textdateien und Datenbanken an. Problematisch ist hierbei jedoch, dass die Fragmente im Anschluss an den Kopiervorgang wieder in das korrekte Dateiformat umgewandelt werden müssen.\n\nUm das System kompakt zu halten, wird auf die großen WIMP-Desktop-Umgebungen KDE und Gnome verzichtet. Stattdessen kommen kleine, schnelle Fenstermanager wie fluxbox, openbox und wmii zum Einsatz.\n\nAus Platzgründen verzichtet grml auf anspruchsvolle Pakete wie Apache OpenOffice oder GIMP und stellt dafür eine Vielzahl flexibler Programme bereit, darunter auch solche, die einige andere Live-CDs nicht mitliefern. Als interaktive Shell kommt die zsh zum Einsatz, alternativ stehen aber auch Bash, ksh und Dash zur Verfügung.\n\nVon Februar 2008 (Grml 1.1) bis 2011.05 existierte grml-medium, was zwischen der Standardversion und grml-small angesiedelt war. Zwar wird ein X-Server und der Fenstermanager Fluxbox mitgeliefert, aber der Umfang war immer noch kleiner als bei Grml-full. Grml Medium war als ISO-Image in der ersten Version kleiner als 200 MB und später ungefähr 210 MB groß.\n\nGrml-small ist ein minimalistisches Rettungssystem, das als ISO-Datei etwa 150 MB groß ist. Es ist gedacht für die Reparatur beschädigter Systeme. Während auf anwendungsorientierte Pakete wie grafische Benutzeroberfläche, Manpages etc. verzichtet wird, enthält grml-small die wichtigsten Dienstprogramme zur Diagnose und Behebung von Netz- und Massenspeicherproblemen. Durch Schnelldekompression steht Software mit ca. 300 MB Originalgröße bei circa 150 MB komprimierter Image-Größe zur Verfügung. Damit passt das System beispielsweise auf kleine 256 MB USB-Speicher (USB-Stick) oder Visitenkarten-CD-ROMs.\n\nDas Standard-Grml hat ab der Version 2012.05 das erste Mal eine eigene Bezeichnung grml-full erhalten. Es ist als ISO-Datei etwa 350 MB groß. Es besitzt alle Features. Es ist eine grafische Oberfläche (Fluxbox) und beispielsweise Firefox (bzw. Iceweasel) installiert. Anstatt drei sind sechs ttys aktiviert. Außerdem sind einige weitere Programme für die Konsole installiert, die auf der abgespeckten Version keinen Platz mehr hatten. Bis zur Version 2011.05 war diese Version noch etwa 700 MB groß; aufgrund der dann eingeführten \"grml-96\"-Variante, die die 32- und 64-Bit-Version enthält, steht für jede der beiden Architekturen nur noch die Hälfte einer CD-ROM zur Verfügung.\n\nSowohl grml-small als auch grml-full werden für die Architekturen i686 (32 Bit) und x86_64 (64 Bit) angeboten. Außerdem gibt es beide Varianten in einer kombinierten (und damit doppelt so großen) Version, die aus beiden Architekturen besteht und beim Booten automatisch erkennt, ob die CPU den 64-Bit-Modus beherrscht und dann das passende System startet. Diese Variante hieß früher \"Out for both\" und heute grml-96 und wurde zum ersten Mal mit der Version 2011.12 veröffentlicht.\n\n"}
{"id": "851332", "url": "https://de.wikipedia.org/wiki?curid=851332", "title": "Basilisk II", "text": "Basilisk II\n\nBasilisk II ist ein freier 680x0-Macintosh-Emulator, der auf einer Reihe von Betriebssystemen lauffähig ist, darunter BeOS, Linux, AmigaOS, Windows NT und macOS. Die aktuelle Version läuft auf Mac OS X für PowerPC- und Intel-Prozessoren.\n\nBasilisk II unterstützt nur Versionen von Mac OS, die auf 68K-Macintoshs laufen, als neueste Version also Mac OS 8.1. Neuere Versionen benötigen einen PowerPC-Prozessor, der von Basilisk II nicht emuliert wird.\n\nMacintoshs mit PowerPC-Prozessoren können mit SheepShaver oder PearPC emuliert werden.\n\nSeit 2006 gab es von Seiten des Projektes keine offizielle Veröffentlichung mehr, der Emulator wurde jedoch bis 2008 weiter entwickelt, seitdem wurden ausschließlich Bugfixes in die Codebasis eingepflegt. Der aktuelle Sourcecode ist über das CVS-Repository des Projektes verfügbar, von dritter Seite werden in unregelmäßigen Abständen Builds zusammengestellt und angeboten.\n\n\n"}
{"id": "852890", "url": "https://de.wikipedia.org/wiki?curid=852890", "title": "Alien (Software)", "text": "Alien (Software)\n\nalien ist ein Unix-Programm, das zur Konvertierung verschiedener Paket-Dateiformate benutzt wird.\nAlien steht unter der GNU General Public License (GPL, Version ≥2) als freie Software zur Verfügung. Es wurde ursprünglich von dem Debian-Entwickler \"Christoph Lameter\" geschrieben.\nDer Code zum Konvertieren von RPM-Format zum Debian-Format entstammt einer Software namens \"Martian\" (englisch für \"Mars-Bewohner\"), an die auch der name \"Alien\" (englisch für \"fremd, Fremdling, Außerirdischer\") erinnert.\n\nEs beherrscht Konvertierungen von und in Red Hats RPM-, Debians deb-, Stampedes slp-, Slackwares tgz- und Solaris’ pkg-Format. Pakete können nach der Erzeugung auch gleich automatisch installiert werden. Auch in den Paketen eingebettete Installationsskripte können umgewandelt werden, was jedoch fehleranfällig ist.\nDie Formatkonvertierung ist im Allgemeinen nicht verlustfrei oder kann gelegentlich zu fehlerhaften Paketen führen, weswegen eine genauere Prüfung der erzeugten Pakete nach jedem Aufruf von alien zu empfehlen ist.\n\n"}
{"id": "853868", "url": "https://de.wikipedia.org/wiki?curid=853868", "title": "Cray-2", "text": "Cray-2\n\nDie Cray-2 war ein Vektorsupercomputer, der von Cray Research 1985 angeboten wurde. Sie war die Weiterentwicklung der Cray-1 und bei ihrem Erscheinen der weltweit schnellste Rechner.\n\nDie Cray-2 wurde im Vergleich zur Cray-1 auf 135 cm maximale Höhe und 115 cm Durchmesser verkleinert, und die Zykluszeit auf 4,1 Nanosekunden verkürzt. Die Kühlung geschah mittels Fluorinert, einer von 3M entwickelten Mischung aus Fluorcarbonen. Das Kühlmittel wog ein Drittel des 2,5-Tonnen-Rechners.\n\nJeder Prozessor hatte bei 244 MHz eine Leistung von maximal 488 MFLOPS, wodurch sich beim Maximalausbau von vier Prozessoren die Leistung auf 1,951 GFLOPS summierte. Zum Vergleich: Die Apple Watch aus 2015 hat nahezu die doppelte Rechenleistung der Cray-2.\n\nDie Cray-2 besitzt für I/O-Handling, Synchronisation der genannten Hintergrundprozessoren und zum Ausführen von Systemaufgaben einen Vordergrundprozessor.\nDie Prozessoren bestehen aus Silizium. Seymour Cray versuchte zwar, Prozessoren aus Galliumarsenid herzustellen, allerdings waren diese Prozessoren noch nicht einsatzreif, als die Cray-2 entwickelt wurde.\n\nDie Adressregister wurden auf 32 Bit erweitert, und eine Cray-2 hatte in der Grundausführung 4 Gigabyte DRAM-Speicher mit 56 ns Zugriffszeit. Dabei wurde Interleaving verwendet, bei dem 64 oder 128 Speichermodule nacheinander abgefragt wurden. Spätere Versionen erweiterten den Adressraum, so dass insgesamt 32 Gigabyte adressiert werden konnten.\n\nDas Laden und Speichern von 64 Vektordaten pro Prozessor dauerte 63 Takte, während eine Berechnung nur 22 bis 23 Takte benötigte. Um dieses Problem einzuschränken, war jeder Prozessor mit zusätzlichen 128 KByte Speicher mit nur vier Takten Zugriffszeit verbunden. Dies war jedoch kein echter Cache, da statt Arbeitsspeicher lediglich Register gecacht wurden.\n\nMehrere Cray-2 konnten miteinander über ein spezielles 1,6-GBit/s-Netzwerk verbunden werden. Die Cray-2 hatte hausinterne Konkurrenz von der X-MP- und Y-MP-Serie, die mit mehr Prozessoren dieselbe Rechenleistung erreichten.\n\nDie Cray-2 wurde in erster Linie vom Verteidigungsministerium und vom Energieministerium der Vereinigten Staaten eingesetzt. Sie fand jedoch auch weitere Abnehmer weltweit. Unter anderem wurde 1986 vom deutschen Bundesland Baden-Württemberg eine Cray-2 für die Universität Stuttgart erworben, wo sie bis zum April 1994 im Einsatz war. Es handelte sich dabei um die erste Cray-2 in Europa.\n\n"}
{"id": "854075", "url": "https://de.wikipedia.org/wiki?curid=854075", "title": "LinuxTLE", "text": "LinuxTLE\n\nLinuxTLE (ausgesprochen: Linux Talee) ist eine thailändisch lokalisierte Linux-Distribution, die von der thailändischen Regierungsbehörde NECTEC, einer Abteilung des thailändischen \"Ministry of Science and Technology\", erstellt wird. \"TLE\" steht für \"Thai Language Extension\". Die Aussprache „Talee“ ist ein Anklang an das thailändische Wort (das Meer). Dementsprechend ist das Logo der Distribution ein aus dem Meer springender Delfin. Wie bei anderen Linuxdistributionen üblich gibt es in Form von OpenTLE ebenfalls eine Live-CD-Ausgabe, mit der das System vor der Installation ausgiebig getestet werden kann und welche ebenfalls vom NECTEC vertrieben wird.\n\nLinuxTLE basierte ursprünglich auf Red Hat, mittlerweile wird jedoch Ubuntu als Basis verwendet. Die Live-CD OpenTLE 7.10 basiert auf Knoppix und \"Berry Linux\". Als Desktop wird GNOME verwendet. Ein Großteil der enthaltenen Programme sind auf Thai übersetzt. Linux/OpenTLE 5.5 vom April 2004 enthält neben den üblichen Web-, Grafik- und Officeprogrammen unter Linux insbesondere Thai TrueType Fonts, ArnThai 2.0 (eine Texterkennung für Thai), LEXiTRON 2.0 pre2 (Wörterbuch Thai – Englisch) und ThaiLatex.\n\n"}
{"id": "855883", "url": "https://de.wikipedia.org/wiki?curid=855883", "title": "Constructive Solid Geometry", "text": "Constructive Solid Geometry\n\nConstructive Solid Geometry (CSG) oder Konstruktive Festkörpergeometrie ist eine Technik zum Modellieren von Körpern, die u. a. in der 3D-Computergrafik und bei CAD-Programmen genutzt wird. Constructive Solid Geometry ermöglicht einem Designer einen komplex geformten Körper zu erzeugen, indem er boolesche Operatoren zur Kombination von Grundkörpern zu einen neuen Körper benutzt. Aus der CSG hervorgegangene Körper wirken oft sehr komplex, sind aber in Wirklichkeit nichts anderes als geschickt verknüpfte Objekte.\n\nDie Basisobjekte, aus denen CSG-Körper hervorgehen, nennt man Primitive (vgl. Grafisches Primitiv). Typischerweise handelt es sich dabei um Körper, deren Oberfläche mittels einer relativ einfachen mathematischen Formel beschrieben werden kann, wie z. B. Würfel, Zylinder, Prismen, Pyramiden, Kugeln oder Ringe. Die Menge der möglichen Primitive wird gewöhnlich von der verwendeten Software begrenzt. Einige Software-Pakete erlauben CSG auf gekrümmten Objekten (prozedurale oder parametrische Oberflächen), während andere nur auf polygonalen Meshes (Dreiecksnetze) arbeiten. Der prozedurale oder parametrische Ansatz erlaubt eine mathematisch exakte Berechnung und Repräsentation der Körper, während Meshes immer nur eine mehr oder weniger ungenaue Annäherung an die Wirklichkeit sind.\n\nWie bereits erwähnt, wird ein komplexer Körper von Primitiven erzeugt, die durch Operationen verknüpft sind. Gewöhnlich handelt es sich dabei um boolesche Operationen auf Mengen: Vereinigung (Union, formula_1), Differenz (Difference, formula_2) und Schnitt (Intersection, formula_3). Folgende Abbildung zeigt die Wirkung der Operatoren exemplarisch an der Verknüpfung von Würfel mit Kugel:\n\nCSG ist unter Designern sehr beliebt, da man mit einer Anzahl relativ einfacher Körper komplexe Geometrien formen kann. Der Designer kann (bei den meisten Programmen) die Geometrie auch im Nachhinein noch ändern, indem er die Position (bzw. Transformation) der einzelnen Objekte oder den booleschen Operator ändert, mit dem die Objekte verbunden sind. Der Designer kann also sein Modell interaktiv und intuitiv, durch Versuch-und-Irrtum, kreieren.\n\nCSG wird aber auch von diversen Programmen \"unter der Haube\" benutzt, d. h. ohne dass der Benutzer etwas von der Existenz der CSG-Operationen mitbekommt. So benutzen z. B. die Game-Engines von Unreal und Quake das CSG-Verfahren. Simulationsprogramme, die die Abläufe an Werkzeugmaschinen simulieren, verwenden i. d. R. ebenfalls CSG.\n\n Da mehrere hintereinander ausgeführte CSG-Operationen im Allgemeinen nicht kommutativ sind, lassen sie sich hierarchisch ordnen und in einen CSG-Baum überführen. Jedes Blatt entspricht dabei einem Primitiv, jeder Knoten einer CSG-Operation (bzw. dem Zwischenergebnis aus einer CSG-Operation, welches wieder transformiert werden kann). Die Wurzel des Baumes ist das Endergebnis.\n\nDie regularisierten Mengen des n-dimensionalen euklidischen Raumes bilden mit den Verknüpfungen formula_1, formula_3, formula_6(Komplementärmenge) eine boolesche Algebra, für die auch das Kommutativgesetz gilt. Der formula_7-Operator ist nicht Teil dieser booleschen Algebra. Allerdings lassen sich der formula_6- und formula_3-Operator als Ersatz für den formula_7-Operator verwenden:\n\nDas Beispielobjekt lässt sich von daher auch ohne Klammerung berechnen (formula_12 = Würfel, formula_13 = Kugel, formula_14 = Zylinder):\n\n\n\n"}
{"id": "856772", "url": "https://de.wikipedia.org/wiki?curid=856772", "title": "Adobe GoLive", "text": "Adobe GoLive\n\nAdobe GoLive ist ein HTML-Editor des Unternehmens Adobe Systems. Er ist ein WYSIWYG-Editor für Mac OS und Microsoft Windows. Das Erstellen und Bearbeiten von Webseiten wird durch Drag and Drop, vorgefertigte Seiten und Syntaxhighlighting vereinfacht.\n\nEntwickelt wurde das Programm von dem Hamburger Unternehmen \"GoNet communications\" 1996 als \"GoLive CyberStudio\" und \"GoLive Systems\". 1999 wurde das Unternehmen von Adobe gekauft. Die erste Adobe GoLive-Version erschien 1999. In den Versionen 2 und 2.3 der \"Adobe Creative Suite\" sind sowohl GoLive 8 als auch Dreamweaver enthalten. Die letzte Version \"Adobe GoLive 9\" trug zwar den Zusatz „CS2“, wurde aber nur noch separat angeboten und verwendet für das Layout-Rendering die Opera-Engine (wie auch Dreamweaver bis zur Version CS3). Mit dieser Version wurde die Entwicklung und im April 2008 auch der Vertrieb von GoLive zugunsten von Dreamweaver CS3 eingestellt.\n\nDie CS-Versionen von GoLive müssen nach der Installation durch eine Produktaktivierung freigeschaltet werden, um lauffähig zu bleiben (in CS2-Version inzwischen deaktiviert, siehe CS2-Version).\n\n\n"}
{"id": "857071", "url": "https://de.wikipedia.org/wiki?curid=857071", "title": "GNU Common Lisp", "text": "GNU Common Lisp\n\nGNU Common Lisp (GCL) ist eine freie Implementierung der Programmiersprache Common Lisp. Der Quelltext steht dabei teilweise unter der GPL, teilweise unter der LGPL, die Binär-Pakete ausschließlich unter der GPL.\n\nUrsprung ist das Kyoto Common Lisp System, das 1984 von Taiichi Yuasa und Masami Hagiya geschrieben wurde. 1987 nahm William Schelter seine Arbeit daran auf, der Name war nun AKCL (Austin Kyoto Common Lisp). AKCL wurde 1994 als GNU Common Lisp veröffentlicht. Nach dem Tod von William Schelter 2001 wird das Projekt von anderen Entwicklern weitergeführt.\n\nGCL läuft derzeit auf 11 GNU/Linux-Architekturen (x86, PowerPC, S/390, SPARC, ARM, Alpha, IA-64, PA-RISC, M68k, MIPS, MIPSEL), Windows, SPARC Solaris und FreeBSD.\n\nUrsprünglich sollte GCL lediglich den CLtL1-Standard erfüllen, inzwischen ist jedoch erklärtes Ziel, den Common-Lisp-ANSI-Standard zu erfüllen.\n\n\n"}
{"id": "860387", "url": "https://de.wikipedia.org/wiki?curid=860387", "title": "MovieJack", "text": "MovieJack\n\nMovieJack war früher eine Software zum Konvertieren von Video-DVDs, mittlerweile ein Video Downloader. Als erste Software im kommerziellen Markt ebnete diese den Weg für viele heute erhältliche ähnlich aufgebaute sogenannte Ripper.\n\nEntwickelt von der Firma Engelmann Software und vertrieben von der Firma S.A.D. wurde im Zuge des neuen Urheberrechtsgesetzes 2003 die Funktion zum Knacken des CSS-Kopierschutzes entfernt und danach konnten nur noch Video-DVDs ohne Kopierschutz konvertiert werden.\nDer Hersteller von MovieJack musste seinerzeit aufgrund einer einstweiligen Verfügung der Kanzlei Waldorf sämtliche Produkte vom Markt nehmen.\n\nIm Laufe der Jahre wurde das MovieJack Produktspektrum immer weiter ausgebaut. Neben dem ursprünglichen Ripper zur Umwandlung in VCDs bzw. SVCD konnten auch Video-DVDs transkodiert oder gesplittet werden.\n\nMit MovieJack Mobile wurde auch der Markt der mobilen Endgeräte bedient und DVDs bzw. alle möglichen Arten von Videodateien werden u. a. für Handys (im 3GPP Format) oder die Playstation Portable konvertiert. Eingesetzt wurde dabei u. a. der Videocodec HDX4.\n\nSeit 2016 ist MovieJack ein Video Downloader für Portale wie YouTube, Clipfish, Dailymotion, Vevo, Vimeo und einigen Mediatheken.\n\n"}
{"id": "860403", "url": "https://de.wikipedia.org/wiki?curid=860403", "title": "Datei- und Druckerfreigabe", "text": "Datei- und Druckerfreigabe\n\nEine Datei- und Druckerfreigabe, auch Ordnerfreigabe oder \"Netzwerk-Freigabe\" genannt, ermöglicht unter Windows den Zugriff auf Dateien und Drucker im Netzwerk.\n\nDie konkrete Realisierung übernimmt dabei die Windows-Komponente „Datei- und Druckerfreigabe für Microsoft Netzwerke“ (engl. „\"\"“). Diese Komponente nutzt – zumindest in Heimnetzwerken – das „Server-Message-Block-Protokoll“ (SMB) von Microsoft zur Kommunikation mit anderen Rechnern.\n\nSofern auf eine Dateifreigabe im Netzwerk eine permanente Verbindung eingerichtet wird, entsteht ein \"Netzlaufwerk\", das als \"virtuelles Laufwerk\" die Ordner und Dateien eines Servers auf dem Client wie gewohnt anzeigt.\n\nAlternativ können Dateifreigaben auch ohne Netzlaufwerk benutzt werden, indem Verzeichnisse oder Dateien über die Uniform Naming Convention, d. h. in der Form codice_1, direkt angesprochen werden.\n\nDie Begriffe „Datei- und Druckerfreigabe“ bzw. „Datei- und Druckdienste“ werden auch im Zusammenhang mit anderen Betriebssystemen verwendet, insbesondere wenn bei ihnen ebenfalls das Netzwerkprotokoll SMB zur Anwendung kommt. Ein Beispiel ist die Verwendung von SMB durch Linux mit Hilfe von Samba.\n\n\n\n"}
{"id": "860468", "url": "https://de.wikipedia.org/wiki?curid=860468", "title": "Windows Hardware Quality Labs", "text": "Windows Hardware Quality Labs\n\nDie Windows Hardware Quality Labs (WHQL) sind eine Einrichtung von Microsoft zur Zertifizierung von Gerätetreibern und Komplettsystemen. Zur Zertifizierung testet ein Gerätehersteller sein Produkt oder seinen Gerätetreiber nach Vorgaben von Microsoft und reicht anschließend das Testprotokoll ein. Gegebenenfalls werden dann von Microsoft noch weitere Tests durchgeführt.\n\nNach erfolgreicher Absolvierung der Tests darf der Gerätehersteller sein Produkt mit einem Logo versehen. Außerdem erhält der Gerätetreiber eine digitale Signatur von Microsoft. Microsoft Windows warnt beim Installieren von Gerätetreibern ohne diese digitale Signatur vor möglichen Risiken. Außerdem werden zertifizierte Gerätetreiber über Windows Update und den Microsoft Update Catalog verteilt.\n\nGerätetreiber sind tief im Betriebssystem verankert und können bei fehlerhafter Programmierung leicht Instabilitäten und Systemabstürze auslösen. Microsoft beabsichtigt daher mit der WHQL-Zertifizierung die Qualität von Gerätetreibern zu verbessern, um eine bessere Stabilität des Betriebssystems Windows zu erreichen. \n\nAuch Computer-Komplettsysteme und Peripheriegeräte können zertifiziert werden. Kunden sollen dann Geräte mit Zertifizierung als qualitativ höherwertig empfinden. \nSeit 1. Juni 2007 wird für den Erhalt eines Zertifikats für Computersysteme für Windows Vista ein Audio-Fidelity-Test mit der speziellen Messausrüstung \"Audio Precision SYS-2722-A-M\" vorausgesetzt. Damit sollen auch die Ansprüche an die Qualität der Tonwiedergabe sichergestellt werden.\n\nDerzeit gibt es zwei Stufen im \"Windows Logo Program 3.0\": \"Basic\" und \"Premium\".\n\nKritiker sehen in dem Programm den Versuch einer Beeinträchtigung der Marktfreiheit.\n\n"}
{"id": "864628", "url": "https://de.wikipedia.org/wiki?curid=864628", "title": "Windows Movie Maker", "text": "Windows Movie Maker\n\nWindows Movie Maker (zwischenzeitlich \"Windows Live Movie Maker\") ist eine Videoschnittsoftware von Microsoft. Das Programm, das grundlegende Funktionen zum Erstellen von Filmen und Musik bietet, kann als Teil des Windows-Essentials-Pakets über die Microsoft-Website kostenlos bezogen werden. In älteren Windows-Versionen war es Bestandteil des Betriebssystems.\n\nMovie Maker wurde zum 10. Januar 2017 offiziell eingestellt. Sein Ersatz ist \"Windows Story Remix\", das Teil von Microsoft Photos in Windows 10 ist.\n\nWindows Movie Maker debütierte mit dem Betriebssystem Windows ME. Es wurde vielfach wegen seiner sehr dürftigen Funktionalität kritisiert. In Windows 2000, das im selben Jahr wie ME erschien, war der Movie Maker allerdings nicht enthalten, da er lediglich für die Ansprüche der Endanwender reichte, für professionelle Anwender hingegen zu simpel war.\nAllerdings ist es möglich, den Movie Maker aus Windows ME durch das Kopieren der entsprechenden Dateien auch problemlos unter Windows 2000 zu betreiben.\n\nMit Windows XP lag der Movie Maker in Version 1.1 vor und unterstützte DV-AVI- und WMV8-Dateien. Ein Jahr später, im November 2002, erschien die Version 2.0 als kostenloses Update und fügte eine Reihe von Funktionen hinzu, darunter Dutzende von Überblendeffekten und jede Menge anderer Videoeffekte. Es war möglich Videoclips zu tönen, zu raffen oder zu schärfen. Zusätzlich zur Zeitachse wurde, für eine einfachere Orientierung, ein Storyboard hinzugefügt.\nMit dem Erscheinen von Windows XP Service Pack 2 wurde die leicht verbesserte Version 2.1 bereitgestellt. Mit dem Erscheinen der Windows XP Media Center Edition im Jahre 2005 kam Version 2.5 zum Einsatz, die mehr Übergangseffekte und zusätzlich DVD-Brennen ermöglichte. Es war auch eine Funktion vorhanden, mit welcher die erstellte Datei direkt bei YouTube hochgeladen werden kann.\n\nZuerst sollte der Movie Maker von Vista auf der WPF aufbauen, die Idee wurde allerdings schnell verworfen. Mit Vista machte der Movie Maker einen Sprung auf Version 6.0 und bringt eine Reihe weiterer neuer Effekte und Übergänge mit, weiterhin sind Unterstützung für den DVR-MS-Dateityp zu finden, der im Windows Media Center zum Einsatz kommt.\n\nAufgrund der höheren Systemanforderungen unter Vista, darunter auch das Fordern von hardwareseitiger Videobeschleunigung der Grafikkarte, war es nicht allen Windows-Vista-Anwendern möglich, das Programm auszuführen. Daher hat Microsoft die Version 2.6 bereitgestellt, die der Version 2.1 von Windows XP ähnelt.\n\nMit Windows 7 entfernte Microsoft den Movie Maker aus dem Betriebssystem und integrierte ihn in das Windows-Live-Essentials-Paket.\n\nUnter Windows 8/8.1 und Windows 10 war der Movie Maker nur über die \"Windows Essentials\" verfügbar und bot einige exklusive Funktionen, beispielsweise eine Videostabilisierung.\n\nAb Windows 7 ist der Windows Movie Maker nicht mehr im Betriebssystem auffindbar. Stattdessen wurde er in \"Windows Live Movie Maker\" (\"Codename\" Sundance) umbenannt und ist als Teil des Windows Live Essentials Pakets kostenlos über das Internet beziehbar.\nIm Vergleich zu älteren Versionen wurde der Windows Live Movie Maker einer umfangreichen Überarbeitung unterzogen. So wurde die Oberfläche grundlegend erneuert und nach dem Vorbild der Ribbon-Oberfläche (bekannt aus Microsoft Office ab Version 2007 oder diversen Windows-7-Applikationen, wie Paint oder WordPad) ausgerichtet. Neben der neuen Oberfläche werden neue Möglichkeiten eröffnet, wie zum Beispiel das bearbeitete Video auf YouTube zu veröffentlichen oder HD-Videos zu erstellen.\n\nKritisiert wurde das Entfernen vieler Funktionen, wodurch lediglich Basisfunktionen zur Verfügung stehen. Beispielsweise wurden Zeit- und Audioachse ganz aufgegeben. Im Vergleich zum Vorgänger war Movie Maker 14 damit lediglich ein besseres Präsentationsprogramm für Fotos. Die für Heimanwender relativ professionelle Videobearbeitung war verschwunden.\n\nMit der Version 16 (2012) können der Videodienst Vimeo und andere Portale und Archive in die Programmnutzung eingebunden werden. Eine Bildstabilisierung, nur nutzbar ab Windows 8/8.1, soll in der Bearbeitung eigener Videoaufnahmen Verwacklungen automatisch korrigieren.\n\nSeit dem Ende des Supports für die Windows Essentials 2012 am 10. Januar 2017 ist der Windows Movie Maker nicht mehr im Downloadportal von Microsoft zu finden.\n\nIm Folgenden eine Übersicht über alle bisher veröffentlichten Versionen des Videoschnittprogramms:\n\n\n"}
{"id": "865478", "url": "https://de.wikipedia.org/wiki?curid=865478", "title": "GNU Multiple Precision Arithmetic Library", "text": "GNU Multiple Precision Arithmetic Library\n\nDie GNU Multiple Precision Arithmetic Library (GMP) ist eine Programmierbibliothek, die arithmetische Funktionen für beliebig genaue/große Zahlen implementiert. Die erste Version von GMP erschien 1991. Seitdem wird die Bibliothek ständig erweitert und verbessert und in einem jährlichen Release herausgegeben. GMP ist offizieller Teil des GNU-Projekts, steht unter der LGPL und ist somit freie Software. Computeralgebrasysteme, die GMP verwenden, sind beispielsweise Maple und Mathematica.\n\nDie Möglichkeiten von GMP in Bezug auf die Größe der Zahlen sind einzig und allein durch den im Computer verfügbaren Arbeitsspeicher bzw. virtuellen Speicher begrenzt. Trotz der Emulation der Hardwareberechnungen in Form von Softwarealgorithmen bleibt GMP verhältnismäßig schnell, da an vielen Stellen mit Hilfe von Assembleranweisungen optimiert wurde.\n\nDer Funktionsumfang von GMP ist in sieben Kategorien unterteilt.\n\n\nDie GMP besitzt drei Hauptdatentypen: codice_1 für beliebig große Integer, codice_2 für beliebig große Gleitkommazahlen mit änderbarer, auch extrem großer Präzision und codice_3 für die Darstellung von Zahlen als Bruch. Den GMP-Variablen können nicht einfach Werte zugewiesen werden wie bei normalen Datentypen, sondern es müssen spezielle Funktionen aufgerufen werden (siehe Listing). Der folgende Quelltext veranschaulicht den grundlegenden Gebrauch der GMP:\n\nint main(void)\n\nWichtige Funktionen der GMP sind unter anderem:\n\n\nint main(void)\n\n"}
{"id": "866146", "url": "https://de.wikipedia.org/wiki?curid=866146", "title": "Streamripper", "text": "Streamripper\n\nStreamripper ist eine plattformunabhängige Software zum Abspeichern (Szenebegriff: Rippen) von \"SHOUTcast\"- und \"Icecast\"-Internetradiosendungen. Das Projekt wird Open Source entwickelt und unter der GNU General Public License veröffentlicht.\n\nWie ein streaming-fähiger Player verbindet sich Streamripper mit dem Server und empfängt von diesem den Datenstrom paketweise. Unterstützt werden die Formate MP3 und Vorbis. Anstatt das empfangene Material wiederzugeben und danach zu verwerfen, wird es zusammengefügt und auf der Festplatte belassen. Anhand der mitgesendeten Metadaten lässt sich erkennen, wann ein Musikstück endet und ein neues beginnt, so dass die Stücke in einzelne Dateien aufgeteilt und anhand der mitgesendeten Tag-Information entsprechend benannt werden können. Streamripper selbst ist nicht in der Lage, das empfangene Material wiederzugeben, kann es aber lokal zu einem streaming-fähigen Audioplayer als „Relay Server“ weiterleiten, so dass das Material gleichzeitig wiedergegeben werden kann.\n\nStreamripper selbst wird per Kommandozeile bedient, ohne grafische Benutzeroberfläche, so dass der Ressourcenverbrauch des Computers gering ist. Von anderen Programmierern sind Frontends erschienen, welche den Umgang komfortabler machen. Weiterhin existiert ein Winamp-Plug-in für die Versionen 2, 3 und 5. Hier wird Streamripper direkt in Winamp integriert und über eine gesonderte Oberfläche bedient. Auch existieren einige Abwandlungen, welche beispielsweise von Haus aus über eine Oberfläche verfügen, diese werden jedoch größtenteils nicht mehr weiterentwickelt.\n\nIm Jahre 2001 wurde der Internetradio-Anbieter \"Live365\" auf die Software aufmerksam und mahnte den Entwickler Jon Clegg, seine Aktivitäten einzustellen. Als Clegg das ignorierte, begannen Live365-Stationen, Streamripper-Clients gezielt mit einem manipulierten Stream abstürzen zu lassen. Es handelte sich dabei nicht um die erwartete Musik, sondern um eine aufgeblähte HTML-Datei, welche immer wieder die Zeile \"DEFINITION clegg n – large swift fly the female of which sucks blood of various animals [syn horsefly, cleg, horse fly]\" (dt. „Clegg ist eine große weibliche Mücke, die das Blut von Tieren saugt.“) enthielt. Clegg gab daraufhin nach und entfernte die Unterstützung für Live365-Stationen.\n\nDie Rechtslage bezüglich des Angebots der Software und deren Nutzung ist von Fall zu Fall zu beurteilen. Der Rechtsausschuss des Bundestages hat empfohlen, im Rahmen eines möglichen „Dritten Korbes“ der Urheberrechtsreform die Nutzung von Recording-Software (wie dem Streamripper) explizit gesetzlich zu regeln.\n\nDerzeit ist darauf abzustellen, ob die Aufnahme von einem rechtmäßig oder (etwa wegen fehlender GEMA- oder GVL-Anmeldung) unrechtmäßig sendenden Internetradio oder Videoportal erfolgt und ob es sich um einen interaktiven Radiosender handelt oder nicht. Die Aufnahme von einem nicht interaktiven Piratensender kann nach Abs. 2 UrhG unrechtmäßig sein. Allerdings ist kaum von strafrechtlichen oder zivilrechtlichen Folgen auszugehen, weil es beim Nutzer in der Regel an der Kenntnis der Unzulässigkeit und damit am Vorsatz (bei fehlender Kenntnis der Unzulässigkeit) bzw. an der Fahrlässigkeit fehlt (wenn der Nutzer die Unzulässigkeit nicht hätte erkennen können).\n\nDie Stiftung Warentest hat im Juni 2014 die Ansicht vertreten, beim Rippen von MP3-Dateien aus dem Livestream des Videoportals YouTube durch ein darauf spezialisiertes Portal handele es sich grundsätzlich um zulässige Privatkopien, die der Verbraucher für sich selbst auf seinen eigenen Geräten nutzen dürfe, solange das Portal auf rechtmäßig auf YouTube hochgeladene Videos zurückgreife. Das seien beispielsweise Videos aus offiziellen YouTube-Kanälen der Künstler oder aus Kanälen, bei denen es offensichtlich ist, dass die Rechteinhaber mit der Verwendung einverstanden sind, wie etwa der Kanal des Eurovision Song Contest. Eine Weiterverwendung der Audio-Aufnahme – etwa durch die erneute Veröffentlichung auf einem Portal – sei dagegen nicht zulässig, weil sie vom Tatbestand der Privatkopie nicht gedeckt sei.\n"}
{"id": "866879", "url": "https://de.wikipedia.org/wiki?curid=866879", "title": "Lawrence University", "text": "Lawrence University\n\nDie Lawrence University ist eine kleine Universität in Appleton, Wisconsin.\n\n1847 wurde das \"Lawrence Institute\" von den Methodistenpredigern William Harkness Sampson, Henry Root Colman und Reeder Smith gegründet und nach dem Händler Amos A. Lawrence benannt, der 10.000 Dollar für den Aufbau eines Colleges zur Verfügung stellte. 1913 wurde es zum \"Lawrence College\" und 1964 zur \"Lawrence University\". Die Universität unterrichtet heute etwa 1.500 Studenten. Zur Universität gehört auch das Lawrence Konservatorium. Lawrence gilt als eines der besten kleinen amerikanischen „liberal arts colleges“.\n\n\n\n"}
{"id": "867017", "url": "https://de.wikipedia.org/wiki?curid=867017", "title": "Liste von Linux-Distributionen", "text": "Liste von Linux-Distributionen\n\nDiese Liste von Linux-Distributionen enthält eine nach Derivaten alphabetisch gegliederte Auswahl von Linux-Distributionen. Eine stichwortartige Beschreibung gibt nur die wichtigsten Alleinstellungsmerkmale wieder. Mehr Informationen, wie Verbreitung, Versionen, Sprachen oder Lizenzierung zu den einzelnen Distributionen sind im jeweiligen Hauptartikel zu finden.\n\nEine Linux-Distribution ist eine Zusammenstellung von Software auf Basis des Linux-Kernels. Für andere Unix-Betriebssysteme wie beispielsweise BSD oder Solaris gibt es separate Distributionslisten. Da immer wieder zahlreiche unbedeutende Derivate und Neuzusammenstellungen von bekannten Distributionen gegründet werden, werden hier nur etablierte Distributionen genannt. Unterschiedliche Ausgaben der gleichen Distribution – mit beispielsweise anderem Installationsverfahren, einer anderen Desktop-Umgebung oder Optimierung für andere Hardware – werden nur einmal genannt, auch wenn sie einen anderen Namen tragen. Bei Distributionen, die „Linux“ als Namenszusatz führen, wird dieser weggelassen.\n\nArch ist eine Linux-Distribution mit Rolling Releases, d. h., Releases entsprechen dem aktuellen Stand der Paketarchive. Da das Entwicklerteam dem KISS-Prinzip („“) folgt und die Distribution somit auf grafische Einrichtungswerkzeuge verzichtet, ist Arch eher bei fortgeschrittenen Linux-Benutzern zu finden. Einige Arch-Derivate bieten eine grafische Installations- und Benutzeroberfläche an und eignen sich daher auch für Anfänger. Die meisten Arch-Derivate nutzen \"pacman\" als Paketverwaltungssoftware.\n\nDebian ist eine Distribution mit einer großen Softwareauswahl und unterstützt mit insgesamt zwölf Prozessor-Architekturen mehr als jede andere binäre Linux-Distribution.\n\nKnoppix war die erste Live-Distribution, die eine große Popularität erlangte. Heute weist sie, für eine Live-Distribution, einen großen Softwareumfang auf. So sind beispielsweise sowohl die Gnome- als auch die KDE-Desktop-Umgebung auf der DVD enthalten.\n\nUbuntu ist ein Debian-Derivat, das für Heimrechner optimiert wurde. In Ubuntu können proprietäre Treiber nachinstalliert werden. Die Ubuntu-Entwicklern veröffentlichen zahlreiche Neuzusammenstellungen mit teilweise fantasievollen Namen. Schlussendlich beruhen alle diese Neuzusammenstellungen nur auf einer unterschiedlichen Auswahl der bei der Erstinstallation enthaltenen Pakete. Basissystem, Installationsprogramm und Repositories sind hingegen identisch. Somit ist es auch möglich, eine Distribution durch Nachinstallieren um den Funktionsumfang eines anderen Neuzusammenstellung zu erweitern. Daneben gibt es noch diverse, teilweise stark spezielle Varianten, die von Canonical Ltd. veröffentlicht und oft auch als „\"inoffizielle Ubuntu-Derivate\"“ bezeichnet werden. Die von den Ubuntu-Entwicklern veröffentlichen Ubuntu-Neuzusammenstellungen sind im Hauptartikel zu finden.\n\nGentoo ist eine quellbasierte Linux-Distribution für fortgeschrittene Linux-Benutzer, die ihr System komplett individuell einrichten möchten. Auf Gentoo basierende Distributionen verwenden oft den von Gentoo entwickelten Paketmanager „Portage“.\n\nRed Hat Linux (RHL, nicht zu verwechseln mit RHEL) wurde von 1994 bis ins Jahr 2003 von der Firma Red Hat entwickelt. Obwohl diese Distribution heute nicht mehr weiterentwickelt wird, basieren diverse noch heute gepflegte Distributionen auf ihr. Als direkter Nachfolger von RHL kann Fedora gesehen werden. Der weit verbreitete RPM Package Manager, der heute das Gemeinsame an vielen Red Hat-Derivaten ist, stammt ursprünglich aus \"Red Hat Linux\".\n\nFedora kann als einziger direkter Nachfolger von Red Hat Linux (RHL) gesehen werden. Es verwendet nur Inhalte mit vollständig freien Lizenzen. Viele ehemalige RHL-Derivate basieren heute auf Fedora.\n\nRed Hat Enterprise Linux (RHEL, nicht zu verwechseln mit RHL) ist eine kommerzielle, kostenpflichtige Distribution der Firma Red Hat. RHEL wird hauptsächlich auf Servern und Workstations eingesetzt. RHEL-Versionen werden jeweils aus einer aktuellen Fedora-Version abgeleitet.\n\nMandriva (ehemals Mandrake Linux) war eine kommerzielle, aber kostenlose Distribution (auch kostenpflichtige Versionen erhältlich). Es war anwenderfreundlich und basierte ursprünglich auf RHL.\n\nSlackware ist die älteste noch heute existierende Distribution. Sie hält sich strikt an die UNIX-Prinzipien und ist im hohen Grad konfigurierbar.\n\nopenSUSE (einst SuSE) wird in Nürnberg von SUSE, einem unabhängigen Geschäftsbereich der Firma \"The Attachmate Group\" (früher von Novell), entwickelt. Die Distribution kann komplett über das Werkzeug YaST administriert werden.\n\nEinige Linux-Distributionen wurden von Grund auf neu erstellt und können deshalb nicht als Derivat einer anderen Distribution angesehen werden. Andere übernahmen Teile aus diversen anderen Distributionen, weshalb eine klare Zuordnung unmöglich ist.\n\nEs gibt zahlreiche Linux-Distributionen, deren Entwicklung eingestellt wurde, die aber einen großen Einfluss auf die Geschichte von Linux selbst und/oder die nachfolgenden Distributionen hatten. Distributionen, die länger als drei Jahre von den Entwicklern nicht mehr gepflegt wurden, oder wenn die Entwicklung offiziell eingestellt wurde, gelten als historisch.\n\n\n"}
{"id": "869930", "url": "https://de.wikipedia.org/wiki?curid=869930", "title": "Appleton (Wisconsin)", "text": "Appleton (Wisconsin)\n\nAppleton ist eine Stadt (mit dem Status „City“) und Verwaltungssitz des Outagamie County im US-amerikanischen Bundesstaat Wisconsin. Im Süden erstreckt sich das Stadtgebiet bis in das Calumet und das Winnebago County. Im Jahr 2010 hatte Appleton 72.623 Einwohner, deren Zahl sich bis 2016 auf 74.370 erhöhte.\n\nAppleton ist neben Oshkosh eine der beiden Kernstädte der \"Fox Cities\" genannten Metropolregion.\n\nAppleton liegt im Osten Wisconsins, beiderseits des Fox River, rund 50 km unterhalb von Green Bay. Dort mündet der Fluss in die Green Bay, einen Teil des Michigansees. Die geografischen Koordinaten von Appleton sind 44°15′44″ nördlicher Breite und 88°24′54″ westlicher Länge. Das Stadtgebiet erstreckt sich über eine Fläche von 55,3 km², die sich auf 54,1 km² Land- und 1,2 km² Wasserfläche verteilen.\n\nBenachbarte Orte von Appleton sind Little Chute und Kimberly (an der östlichen Stadtgrenze), Harrison (an der südöstlichen Stadtgrenze), Menasha (an der südöstlichen Stadtgrenze), Greenville (12,4 km nordwestlich).\n\nDie neben Green Bay nächstgelegenen größeren Städte sind Milwaukee (173 km südlich), Chicago (315 km in der gleichen Richtung), Wisconsins Hauptstadt Madison (171 km südwestlich), Eau Claire (294 km westlich), die Twin Cities in Minnesota (441 km in der gleichen Richtung) und Duluth am Oberen See in Minnesota (528 km nordwestlich).\n\nDie größten Arbeitgeber in Appleton waren 2017:\nWeiter befindet sich in Appleton eine Papiermühle von Neenah Paper.\n\nDer Fox River ist durch Stauwerke und Schleusen für Binnenschiffe befahrbar, die zwischen dem Hafen von Green Bay über den das Stromgebiet des Mississippi erreichen können.\n\nDer vierspurig ausgebaute U.S. Highway 41 und der ebenfalls vierspurig ausgebaute Wisconsin State Highway 441 bildet eine ringförmige Umgehungsstraße um das Zentrum von Appleton. Im Süden wird das Stadtgebiet vom U.S. Highway 10 gestreift. Weiterhin treffen in der Stadt die Wisconsin State Highways 47, 96 und 125 zusammen. Alle weiteren Straßen sind untergeordnete Landstraßen, teils unbefestigte Fahrwege sowie innerstädtische Verbindungsstraßen.\n\nIn Appleton treffen mehrere Eisenbahnstrecken der heute zur Canadian National Railway gehörenden Wisconsin Central zusammen.\n\nMit dem Outagamie County Regional Airport befindet sich 9,1 km westlich der nach dem Passagieraufkommen viertgrößte Verkehrsflughafen Wisconsins. Die nächsten Großflughäfen sind der Milwaukee Mitchell International Airport in Milwaukee (183 km südlich), der O’Hare International Airport in Chicago (295 km in der gleichen Richtung) und der Minneapolis-Saint Paul International Airport (440 km westlich).\n\nNach der Volkszählung im Jahr 2010 lebten in Appleton 72.623 Menschen in 28.874 Haushalten. Die Bevölkerungsdichte betrug 1342,4 Einwohner pro Quadratkilometer. In den 28.874 Haushalten lebten statistisch je 2,43 Personen.\n\nEthnisch betrachtet setzte sich die Bevölkerung zusammen aus 87,5 Prozent Weißen, 1,7 Prozent Afroamerikanern, 0,7 Prozent amerikanischen Ureinwohnern, 5,9 Prozent Asiaten sowie 2,2 Prozent aus anderen ethnischen Gruppen; 2,0 Prozent stammten von zwei oder mehr Ethnien ab. Unabhängig von der ethnischen Zugehörigkeit waren 5,0 Prozent der Bevölkerung spanischer oder lateinamerikanischer Abstammung.\n\n25,0 Prozent der Bevölkerung waren unter 18 Jahre alt, 63,7 Prozent waren zwischen 18 und 64 und 11,3 Prozent waren 65 Jahre oder älter. 50,5 Prozent der Bevölkerung war weiblich.\n\nDas mittlere jährliche Einkommen eines Haushalts lag bei 52.183 USD. Das Pro-Kopf-Einkommen betrug 27.179 USD. 10,3 Prozent der Einwohner lebten unterhalb der Armutsgrenze.\n\n\n"}
{"id": "873793", "url": "https://de.wikipedia.org/wiki?curid=873793", "title": "Kalibrierung bei Farbmanagement", "text": "Kalibrierung bei Farbmanagement\n\nEffektives Farbmanagement macht es möglich, digitale Bilder farbecht weiter zu verarbeiten. Eine entsprechende Software ermöglicht eine Farbanpassung zwischen verschiedenen Ein- und Ausgabegeräten sowie zwischen Geräten unterschiedlicher Hersteller. Dies ist notwendig, da jedes der verwendeten Geräte wie Monitor, Scanner oder Drucker Farben anders darstellt, und dadurch verfälschte Farben auf einem Fotoausdruck entstehen können. Ein Farbmanagementsystem kann dieses Problem durch Implementierung eines Color Matching Moduls (CMM) auf Betriebssystemebene beseitigen. CMM korrigiert mit Hilfe zu erstellender ICC-Profile automatisch die Farbfehler zwischen den Geräten. Für die optimale Farbübertragung beim Ausdrucken von Fotos passt das Programm die unterschiedlichen Farbumfänge der einbezogenen Geräte so aneinander an, dass alle Farben in einem Zielfarbraum liegen.\n\nFarbmanagementsysteme werden bisher vor allem von Agenturen und professionellen Fotografen verwendet.\nEin erster Schritt zur Kalibrierung von Digitalkameras ist zum Beispiel das Benutzen einer Graukarte zum Weißabgleich, Scanner werden mit IT8-Targets kalibriert. Für Monitor und Drucker benötigt man ein Colorimeter oder ein Spektralfotometer.\n"}
{"id": "874062", "url": "https://de.wikipedia.org/wiki?curid=874062", "title": "Krawinkel und Eckstein", "text": "Krawinkel und Eckstein\n\nKrawinkel und Eckstein (\"original: Keepvogel\") ist ein Kinderbuch und eine niederländische Computeranimationsserie von Wouter van Reek. \n\nKrawinkel ist eine flugunfähige Krähe, mit einem roten Cape mit Kapuze bekleidet. Sein bester Freund und Hausgenosse ist der Hund Lupus Wolfram von Eckstein (\"niederländisch: Lupus Wolfram Tungsten\"). Sie leben zusammen in einem einsamen Haus auf einer leeren Ebene.\n\nKrawinkels Beschäftigungen sind Erfindungen, Beerenpflücken, Kleben, Pfannkuchenbacken und Anfeuern des Kanonenofens. Er ist unternehmungslustig und neigt dazu, bei seinen Beschäftigungen nicht zum Ende zu kommen. Möbel stapelt er auf dem Dach aufeinander, bis er hinaufklettern und an den Wolken kratzen kann. Eckstein ist der eher verzagte, vorsichtige Konterpart, der Krawinkel wieder auf den Teppich bringt. Die meisten Folgen enden mit dem Sample „Sag ich doch!“, der meist von Eckstein geäußert wird.\n\nDas Buch \"Krawinkel & Eckstein Die Rettungsaktion\" von Wouter van Reek wurde 2005 von Uitgeverij Leopold BV in den Niederlanden veröffentlicht. 2006 erschien es beim Patmos Verlagshaus auf Deutsch. \n\nDie Serie besteht aus einer Mischung aus Zeichentrick und Computeranimation. Ein Markenzeichen ist, dass die Umrisse belebter Wesen dauernd in Bewegung sind, was sie sehr gut von unbelebten Gegenständen abhebt. Die Folgen waren zunächst fünf, später zehn Minuten lang. Die deutsche Version wird vom Duo Badesalz synchronisiert. Einige Folgen waren bis heute in der \"Sendung mit der Maus\" zu sehen.\n\n\n"}
{"id": "875016", "url": "https://de.wikipedia.org/wiki?curid=875016", "title": "XploRe", "text": "XploRe\n\nXploRe ist eine ehemals kommerziellen Statistik-Software, die vom Berliner Unternehmen MD*Tech (Wolfgang Härdle) entwickelt wurde, die heute aber frei zugänglich ist. Die Software samt zugehörigem Quellcode kann auf der Website bezogen werden. Die aktuelle Versionsnummer ist 4.8, jedoch wird die Software nicht mehr weiterentwickelt. Der Benutzer interagiert mit der Software über die XploRe-Sprache, eine an C angelehnte Programmiersprache. Einzelne XploRe-Programme, die sogenannten \"Quantlets\", werden in Bibliotheken (\"Quantlibs\") zusammengefasst.\n\nAbgesehen von Standardfunktionen zur ein- und mehrdimensionalen Datenanalyse liegt der Fokus der von XploRe bereitgestellten Algorithmen in der nicht- und semiparametrischen Statistik sowie der Finanzmarktstatistik. So bietet XploRe Funktionen für\n\n\nNeben einer lokalen Installation gibt es mit dem \"XploRe Quantlet Client\" die Möglichkeit, XploRe als Java-Applet im Browser ausführen zu lassen. Das Applet reicht die Nutzereingaben über ein TCP/IP-basiertes Kommunikationsprotokoll an einen \"XploRe Quantlet Server\" weiter, der die notwendigen Berechnungen ausführt und an den Client zurückschickt.\n\n\n"}
{"id": "875083", "url": "https://de.wikipedia.org/wiki?curid=875083", "title": "Malicious Code", "text": "Malicious Code\n\nMalicious Code () oder \"Malware\" ist ein Sammelbegriff für Computeranwendungen, die unerwünschte Effekte erzeugen, wie Trojanische Pferde, Computerviren oder Computerwürmer.\nMalicious Codes werden in Web-Applikationen injiziert (z. B. als JavaScript-Code), um die Kontrolle über Online-Applikationen (z. B. im Bereich Online-Banking) zu erlangen. Mit Malicious Codes kann auch versucht werden, ein Session-Cookie zu erlangen, das es erlaubt, eine Session zu hijacken.\n\n\n"}
{"id": "880399", "url": "https://de.wikipedia.org/wiki?curid=880399", "title": "Aptitude", "text": "Aptitude\n\nAptitude ist ein Frontend für das Advanced Packaging Tool (APT). Das Programm zeigt eine Liste von Software-Paketen an und erlaubt dem Benutzer, interaktiv Pakete zu verwalten. Ursprünglich wurde es für die Debian-Distribution erstellt, kam abgesehen von ihren vielen Varianten aber auch in RPM-basierten Distributionen zum Einsatz.\n\nAptitude zeichnet sich durch eine auf ncurses basierende zeichenorientierte Benutzerschnittstelle aus, was im Vergleich mit dem reinen Kommandozeilentool apt-get eine etwas komfortablere Interaktion mit APT erlaubt.\n\nWie apt-get verfügt Aptitude über eine zusätzliche Integritätsschicht, in der Paketzustände (\"package states\") gespeichert werden. Dort registriert die Software, welche Pakete infolge von Abhängigkeiten automatisch installiert werden und ist dadurch in der Lage, nicht mehr benötigte (verwaiste) Pakete ebenso automatisch zur Deinstallation vorzuschlagen. Weiterhin führt Aptitude ein von APT unabhängiges, eigenes Log über die gesamte Installationsgeschichte und mögliche Konflikte.\n\nDie Entwicklung von Aptitude wurde im Jahr 1999 begonnen. Zu dieser Zeit waren zwei weitere konsolenbasierte APT-Frontends verfügbar: Das dselect-Programm, welches zur Installation von Debian benutzt wurde, bevor APT existierte, sowie console-apt, ein Projekt, das als der direkte Nachfolger von dselect eingestuft wurde. Ursprünglich wurde das Aptitude-Projekt begonnen, um mit einem, im Vergleich zu console-apt stärker objektorientierten Design zu experimentieren. Ziel war ein flexibleres Programm mit größerem Funktionsumfang.\n\nDie erste Version 0.0.1, veröffentlicht am 18. November 1999, war in ihrem Funktionsumfang noch stark beschränkt: Es konnte eine Liste der verfügbaren Pakete angezeigt werden, doch weder das Herunterladen noch das Installieren waren möglich. Mit der in Debian 2.2 (Codename Potato) enthaltenen Version 0.0.4a wurden u. a. diese Fähigkeiten hinzugefügt.\n\nGegen Ende des Jahres 2000 wurde das gesamte Schnittstellen-Modul neu geschrieben; die neue Architektur basiert auf der Callback-Bibliothek libsigc++ und Konzepten moderner GUI-Toolkits wie GTK+ und Qt. Dies ermöglichte es, die Schnittstelle mit Funktionen wie Drop-Down-Menüs und Pop-Up-Dialogen GUI-ähnlicher zu gestalten als bisher. Das erste offizielle Aptitude-Release war 0.2.0. Aptitude 0.2.11.1 wurde zusammen mit Debian 3.0 (Codename \"Woody\") veröffentlicht. Zu dieser Zeit wurde console-apt von seinen Entwicklern aufgegeben und aus dem Repository entfernt.\n\nSeit Debian 3.1 (Codename \"Sarge\") ist Aptitude die bevorzugte konsolenbasierte Installationsmethode und ersetzt das vormals aus Gründen der Abwärtskompatibilität gepflegte \"dselect\".\n"}
{"id": "880809", "url": "https://de.wikipedia.org/wiki?curid=880809", "title": "Microsoft InfoPath", "text": "Microsoft InfoPath\n\nBei Microsoft InfoPath handelt es sich um ein Windows-Anwendungsprogramm der Firma Microsoft, das sowohl die Gestaltung von XML-basierten Formularen als auch deren späteres Befüllen mit Informationen erlaubt. Das den InfoPath-Dateien zugrundeliegende XML-basierte Dateiformat verfolgt einen mit dem W3C-Standard XForms vergleichbaren Ansatz, Formularfelder und Steuerelemente auf Dateninstanzen im XML-Format abzubilden. Ab der Version „InfoPath 2010“ ist InfoPath Teil der Microsoft Office Professional Plus-Suite.\n\nEingeführt wurde InfoPath zusammen mit Microsoft Office 2003 und wurde zuvor unter dem Codenamen \"XDocs\" entwickelt. InfoPath entstand als Reaktion auf die von Adobe eingeführten PDF-Forms, die das „papierlose Büro“ möglich machen sollten.\n\nAm 31. Januar 2014 gab Microsoft bekannt, dass InfoPath 2013 die letzte Version dieser Software sein wird und es InfoPath in kommenden Versionen von Microsoft Office nicht mehr geben wird.\n\nDie erstellten Formulare können über ein Netzwerk verteilt werden, sodass Anwender-Informationen einfach und benutzerfreundlich gesammelt und gespeichert werden können. InfoPath kann in Verbindung mit Windows SharePoint Services genutzt werden, um Anwendern Formulare für typische Situationen wie Status-Reports, Reisekostenabrechnung etc. zur Verfügung zu stellen.\n\nDa unter InfoPath 2003 Anwender zum Entwerfen wie auch Ausfüllen der Formulare auf die kostenpflichtige Software angewiesen sind, beschränkte sich die Zielgruppe im Wesentlichen auf Unternehmen mit homogenen Arbeitsumgebungen. Diese sollen in die Lage versetzt werden, Formulare in Workflows zu integrieren. Seit der Version InfoPath 2007 ist es auch möglich in Verbindung mit dem Office SharePoint Server 2007 mit Hilfe der „Forms Services“ Formulare ins Web zu stellen. Diese können dann auch ohne InfoPath-Installation auf dem Clientrechner ausgefüllt und abgeschickt werden.\n\nInfoPath benutzt die Dateierweiterung XSN. XSN ist ein CAB-Archiv mit XML-, GIF-, und JS- bzw. VBS-Dateien.\n\n"}
{"id": "881969", "url": "https://de.wikipedia.org/wiki?curid=881969", "title": "Wipe", "text": "Wipe\n\nWipe (vom englischen für „wischen“ oder „putzen“) ist eine Eraser-Software, die zum sicheren Löschen von Dateien unter Linux und Windows dient.\nWird eine Datei mit Wipe gelöscht, so überschreibt es diese mehrmals mit Nullen, speziellen Bit-Mustern und/oder Zufallsdaten. Dadurch soll gewährleistet werden, dass die gelöschten Daten nicht mehr durch forensische Analyse von magnetischen Datenträgern (wie Festplatten oder Disketten) rekonstruiert werden können.\n\nDas Löschen einer Datei mit Wipe dauert länger als das normale Löschen, da zum einen die Datei mehrmals komplett überschrieben wird und zum anderen da benötigte Zufallsdaten nicht immer verfügbar sind. Das einmalige Überschreiben von Daten mit Nullen reicht entgegen früheren Meinungen aus, um eine Wiederherstellung praktisch unmöglich zu machen.\n\nDas Löschen von Dateien geschieht normalerweise durch den Aufruf der Operation codice_1 des Betriebssystems. Dieser Aufruf erfolgt normalerweise durch das Kommando codice_2 (z. B. unter Unix) oder codice_3 (z. B. unter DOS oder Windows) oder durch eine entsprechende Auswahl in der grafischen Benutzeroberfläche. codice_1 entfernt im Normalfall nur den Eintrag im Inhaltsverzeichnis des betreffenden Dateisystems, d. h. letztlich nur den Inode. Die Nutzdaten sind dadurch immer noch vorhanden und mit einfachsten Mitteln auszulesen. Sollen erneut Daten im Dateisystem abgelegt werden, steht dieser freigegebene Speicherplatz zwar zur Verfügung, ist aber mit aufwändigen Methoden immer noch rekonstruierbar.\n\nDie Benutzung von Wipe wird immer dann empfohlen, wenn sensible Daten nicht in die Hände anderer fallen sollen. Die Manpage von Wipe empfiehlt sogar, zur Reparatur abgegebene Notebooks vorher mittels Wipe von allen privaten Daten zu säubern. Selbiges gilt für den Verkauf von Festplatten mit ehemals wichtigem Inhalt (dabei kann aber die Festplatte als ganzes mehrfach mit Daten überschrieben werden, z. B. durch Benutzung des Unix-Kommandos codice_5).\n\nWipe hängt entscheidend von der Struktur des Dateisystems ab. Ältere Systeme, wie etwa das unter Linux verwendete Ext2 oder das von Windows/DOS bekannte FAT bzw. FAT32 machen im Zusammenhang mit Wipe keine Probleme, da sie kein Journal über getätigte Dateisystemtransaktionen führen. Sogenannte journalisierende Dateisysteme, wie ReiserFS oder Ext3, führen jedoch Buch über getätigte Schreiboperationen, um im Fehlerfall ein aufwändiges Suchen von Fehlern im Dateisystem zu unterbinden. In diesem Journal können allerdings Daten zu den zu löschenden Dateien gespeichert sein. Wipe ist jedoch nicht in der Lage, dieses Journal zu verändern, weil dies eines tiefgreifenden Eingriffs in die Treiberstruktur des Dateisystems auf Kernebene bedarf. Deshalb ist die Arbeitsweise von Wipe bei journalisierenden Dateisystemen nur durch den Dateisystemtreiber realisierbar.\n\nEin weiteres Problem sind die Reserve-Blöcke, die vom Speichercontroller verwendet werden, wenn in genutzten Speicher-Blöcken die Fehlerrate einen kritischen Grenzwert übersteigt: Der Speichercontroller kopiert dann die Daten aus dem betreffenden Block in einen Reserve-Block und verwendet danach nur diesen statt des ursprünglichen, allerdings – bei IDE-Festplatten – transparent für die Anwendungssoftware, so dass nicht einmal Schnittstellentreiber zur Hardware über diesen Vorgang Bescheid wissen. Die so geretteten Daten im ursprünglichen Block können daher nicht überschrieben werden. Dies ist nicht nur bei Festplatten so, sondern auch bei vielen anderen Speichermedien; beispielsweise bei der Multimedia Card (MMC). Bei Festplatten kann man dies durch Auslesen der Anzahl der defekten Blöcke überwachen, beispielsweise unter Linux wie unter Cygwin/MS-Windows bei der ersten HDD/SSD mittels\n\nsmartctl -a /dev/sda | awk '/Reallocated_S/{ print $10 }'\n\naber nicht verhindern.\n\nIn jüngerer Zeit immer relevanter wird weiterhin das Problem, dass auf Datenspeichern wie den meisten Flash-Speichern, sogenannte Wear-Leveling-Algorithmen dafür sorgen, dass beim Überschreiben vorhandener Daten diese an einer anderen physischen Adresse des Speichers abgelegt werden. Damit wird Programmen wie Wipe die Möglichkeit entzogen, gezielt Daten auf dem Speicher physikalisch überschreiben zu können. Auch auf Flash-Speichern mit z. B. FAT32-Dateisystem ist so ein sicheres Löschen nicht mehr möglich.\n\nEin sehr ähnliches Programm mit vergleichbarem Funktionsumfang, welches jedoch keine Verzeichnisse überschreiben kann, dafür aber als Mitglied der GNU Core Utilities bei jeder Linux-Distribution bereits vorinstalliert ist, ist Shred.\n\n"}
{"id": "882178", "url": "https://de.wikipedia.org/wiki?curid=882178", "title": "GraphicConverter", "text": "GraphicConverter\n\nGraphicConverter (anfangs Grafikkonverter) ist ein Konvertierungs- und Bildbearbeitungsprogramm und Bildbrowser für Pixelgrafikdaten (Rastergrafik) unter Mac OS von Thorsten Lemke. Es konvertiert – auch in Stapelverarbeitung – zwischen Hunderten von Grafikformaten. Für Windows ist mit IrfanView ein vergleichbares Programm entstanden, das aber eher auf Bildbetrachtung ausgerichtet ist.\n\nThorsten Lemke entwickelte 1992 die Software, ursprünglich für System 7, um auf seinem alten Atari-System vorhandene Grafikdateien weiter nutzen zu können. Mit dem Anwachsen des Internets setzte er mehr und mehr Anfragen um, weitere Grafikkonvertierungen zu implementieren. Die Software ist mittlerweile in vielen Sprachen, unter anderem in Deutsch, Englisch, Französisch, Dänisch, Schwedisch, Italienisch, Spanisch, Russisch, Tschechisch, Chinesisch und Japanisch erhältlich. Das Programm unterstützt AppleScript.\n\nDie Software benötigt ab Version 9 mindestens Mac OS X Lion; für ältere Rechnersysteme, auch für Power Macs und sogar bis zum „klassischen“ Mac OS, sind entsprechend ältere Programmversionen offiziell verfügbar.\n\nIn Version 10 wurde u. a. Gesichtsererkennung, Import von BigTIFF und Export von Animated PNG ergänzt.\n\nDie Software wurde von Apple mit den damals aktuellen Power Mac G5 als OEM-Version mitgeliefert.\n\n"}
{"id": "883042", "url": "https://de.wikipedia.org/wiki?curid=883042", "title": "PICT", "text": "PICT\n\nPICT (als Dateiendung wird auch \"pct\" benutzt) ist ein vorwiegend internes Grafikformat (\"metafile\") der Apple-Betriebssysteme bis Mac OS 9. Es gehört zur Bildschirmbeschreibungssprache QuickDraw. Es kann Pixel-, Vektor- und Textinformation enthalten. Unter Mac OS X wurde es von PDF abgelöst, wird aber auch von aktuellen QuickTime-Versionen (Mac und Windows, Stand: November 2012) weiterhin unterstützt. Es entspricht Windows Metafile (WMF bzw. EMF) unter Windows.\n"}
{"id": "884476", "url": "https://de.wikipedia.org/wiki?curid=884476", "title": "Statistiklabor", "text": "Statistiklabor\n\nDas Computerprogramm Statistiklabor (Statistical Lab) ist ein explorativer und interaktiver Werkzeugkasten zur statistischen Analyse und Visualisierung von Daten. Es unterstützt durch einen sehr anschaulichen Ansatz und durch eine Reihe von Funktionen insbesondere die Ausbildung im Fach Statistik in den Wirtschafts- und Sozialwissenschaften. Das Programm wurde vom Center für Digitale Systeme der Freien Universität Berlin entwickelt. Seit dem 27. September 2006 und der Version 3.5 steht das Statistiklabor als freie Software im Quellcode unter GPL zur Verfügung. Vorherige Versionen waren Freeware für nicht-kommerzielle Anwendungen. Statistiklabor wird seit 2011 nicht mehr weiterentwickelt.\n\nEinfache oder auch komplexe statistische Probleme können mit Hilfe des Statistiklabors simuliert, bearbeitet und individuell gelöst werden. Das Programm kann durch externe Bibliotheken erweitert werden. Über diese Bibliotheken ist darüber hinaus auch eine Anpassung an individuelle und lokale Anforderungen wie beispielsweise einzelne Statistik-Kurse für bestimmte Anwendergruppen möglich. Die vielfältigen grafischen Darstellungsvarianten ermöglichen anschauliche Visualisierungen der zugrundeliegenden Daten.\n\nDas Statistiklabor ist der Nachfolger von \"Statistik interaktiv!\" Im Gegensatz zum kommerziellen SPSS ist das Statistiklabor didaktisch motiviert. Es richtet sich vorwiegend an statistische Laien. Um die Einstiegshürden für Studierende in die Statistik so niedrig wie möglich zu halten, verbindet das Statistiklabor Datensätze, Häufigkeitstabellen, Zufallszahlen, Matrizen etc. in einem anwenderfreundlichen virtuellen \"Arbeitsblatt\", das den Nutzern ermöglicht, explorativ und spielerisch Berechnungen, Analysen, Simulationen und Datenmanipulationen durchzuführen.\n\nFür die mathematischen Berechnungen verwendet das Statistiklabor die Programmiersprache R, eine freie Implementierung der von den Bell Laboratories entwickelten Sprache S zur Verarbeitung statistischer Daten. Das R-Projekt wird ständig von der R-Community, einer weltweiten Gemeinschaft von freiwilligen Entwicklern, weiterentwickelt.\n\n\n"}
{"id": "885374", "url": "https://de.wikipedia.org/wiki?curid=885374", "title": "Windows-1254", "text": "Windows-1254\n\nWindows-1254 ist eine 8-Bit-Zeichenkodierung des Windows-Betriebssystems, die die meisten westeuropäischen Sprachen und Türkisch unterstützt. Sie baut auf ISO 8859-9 (Latin-5) auf.\n\nWindows-1254 ist ebenfalls bei der IANA registriert und akzeptiert u. a. CP1254 als Synonym.\nDies entspricht dem Unterschied zwischen ISO 8859-9 und ISO 8859-1, nur dass dort jeweils der Bereich 0x80–9F, also samt Ž und ž, unbelegt ist. Die verlorengegangenen Zeichen sind u. a. für Isländisch nötig.\n\n\n"}
{"id": "887456", "url": "https://de.wikipedia.org/wiki?curid=887456", "title": "Generative Kunst", "text": "Generative Kunst\n\nDie Generative Kunst oder engl. Generative Art ist eine zeitgenössische Form des künstlerischen Schaffens, wobei nicht unbedingt das Kunstwerk oder Endprodukt im Zentrum steht, sondern der Entstehungsprozess und die ihm zugrunde liegenden Ideen. \n\nDas Werk oder Produkt entsteht durch das Abarbeiten einer prozessualen Erfindung, das heißt, eines vom Künstler geschaffenen Regelsatzes bzw. eines Programmes, das beispielsweise in Form natürlicher Sprache, musikalischer Sprache, eines binären Codes, oder eines Mechanismus festgehalten wird. \n\nDas Abarbeiten geschieht selbstorganisierend, in Form eines relativ autonomen Prozesses, etwa durch Handlungen, die – wie in der Partitur zu einem Happening – nach vorliegenden Anweisungen vorgenommen werden, durch ein Computerprogramm, das Anweisungen, Bildinformationen oder andere Konzepte abarbeitet, oder durch andere Medien und Hilfsmittel. Unter unterschiedlichen Produktionsbedingungen läuft der Prozess jeweils anders ab. Das Ergebnis bewegt sich in mehr oder weniger gegebenen Grenzen, ist darin jedoch unvorhersehbar. \n\nGenerative Kunst dient Künstlern oft als Mittel, Intentionalität zu vermeiden.\n\nSiehe auch: Prozesskunst\n\n"}
{"id": "888820", "url": "https://de.wikipedia.org/wiki?curid=888820", "title": "PDFCreator", "text": "PDFCreator\n\nPDFCreator ist ein Anwendungsprogramm zur Erstellung von PDF-Dateien (und Bildern wie JPG, PNG etc.) aus jeder beliebigen Anwendung. Lizenziert ist das Programm teilweise unter AGPL, teilweise unter anderen proprietären Lizenzen. In der Basisvariante \"PDFCreator Free\" ist es kostenfrei.\n\nVon PDFCreator zu unterscheiden sind die kostenpflichtigen Programme ähnlichen Namens \"PDF-Creator\" und \"Jaws PDF Creator\" sowie die Freeware-Programme \"PDF24 Creator\" und \"PDF Creator Pilot\".\n\nBei der Installation von PDFCreator wird ein neuer Windows-Druckertreiber angelegt. Dadurch können PDF-Dateien direkt aus jeder Anwendung erstellt werden, die eine Druckfunktion bereitstellt. Die an den vermeintlichen Drucker gesendeten Befehle werden verwendet, um eine PDF-Datei zu erstellen. Zur Erzeugung der PDF-Dateien baut \"PDFCreator\" auf der API der integrierten ebenfalls freien Software Ghostscript auf. Einige Dokumenttypen wie Postscript, JPEG und BMP kann die Software auch ohne den Umweg über die Druckfunktion in PDF-Dateien konvertieren.\n\n\"PDFCreator\" liegt für mehrere Sprachen lokalisiert vor. Standardmäßig sind im Installationsprogramm der Anwendung mindestens eine deutsche und eine englische Sprachdatei enthalten; durch zusätzlich erhältliche Übersetzungsdateien können 26 weitere Sprachen verwendet werden.\n\nDas Programm unterstützt die gängigen Verfahren der 128-Bit-Verschlüsselung von PDF-Dokumenten.\nEs kann nicht nur PDF-Dateien aus beliebigen Dokumenten erstellen, sondern auch Bilddateien in mehreren gängigen Formaten (JPEG, TIFF, PNG, PCX, BMP) sowie auch PostScript- und Encapsulated-PostScript-Dateien.\n\nDarüber hinaus besteht die Möglichkeit, verschiedene Quelldateien in einem PDF-Dokument zusammenzufassen, indem man diese zunächst über den \"PDFCreator\" druckt und dann in der Druckerwarteschlange zu einem Dokument zusammenfasst.\n\nDie Entwicklung der Version 0.9 wurde stark auf den Geschäftskunden-Sektor hin ausgerichtet, indem die Autoren eine Terminalservereinbindung und die Serveranbindung im Netzwerk implementierten.\n\nAb Version 0.9.5 ist es möglich, PDF-Dokumente in den Formaten PDF/A (1b) und PDF/X (X-3:2002, X-3:2003 and X-4) zu erstellen. Damit können auch PDF-Dokumente zur Langzeitarchivierung bzw. für die Druckvorstufe erzeugt werden.\nAuch das Signieren von PDF-Dateien ist möglich. Die notwendigen Parameter können dazu über den Einstellungen-Dialog festgelegt werden. Das Erstellen von PDF/A und PDF/X-Dokumenten ist weiter verbessert. Zusätzlich können jetzt die Grafikformate JPEG und BMP direkt in PDF-Dokumente konvertiert werden, ohne sie zunächst drucken zu müssen.\n\nAb Version 0.9.6 ist das Signieren von PDF-Dateien möglich.\n\nVersion 0.9.7 erstellt kleinere PDF/A-Dokumente und unterstützt die Beta-Version von Windows 7. Die mitgelieferte \"pdfforge\"-Bibliothek kann jetzt auch aus Bildern direkt PDF-Dateien erzeugen und PDFs im Broschüren-Format erstellen.\n\nDruckerprofile und mehrere unterschiedliche Drucker werden ab Version 0.9.9 unterstützt. Somit lassen sich zusätzlich zu einem PDF-Drucker auch ein PDF/A-Drucker oder/und ein TIFF-Drucker einrichten.\n\nIndividuelle Komprimierungsstufen für eingebettete Farb- und Graustufenbilder, um das bestmögliche Verhältnis von Qualität und Dateigröße zu erreichen, lassen sich ab Version 1.0 festlegen.\n\nAES-Verschlüsselung ist ab Version 1.1 integriert.\n\nInterne Verknüpfungen, wie sie beispielsweise in Inhaltsverzeichnissen häufig verwendet werden, unterstützt PDFCreator nicht. Weblinks werden hingegen in das PDF-Dokument integriert.\n\nDurch die Integration von Ghostscript beträgt die Größe des Downloads bei Version 1.2.3 rund 18 MB.\n\nAb Version 1.3 ist \"PDFArchitect\" enthalten; ein kleines Tool zum Drehen, Einfügen und Entfernen von Seiten aus PDF-Dokumenten.\n\nAb Version 1.6 ist \"PDFArchitect\" ein eigenes Programm, welches im Setup von \"PDFCreator\" zwar ausgewählt werden kann, dann jedoch extra heruntergeladen wird. Alternativ kann man \"PDFArchitect\" auch selbst gesondert herunterladen und installieren. Weiter sind nicht mehr alle Funktionen des \"PDFArchitect\" frei verfügbar, sondern nur mehr einige rudimentäre Funktionen gratis, alles andere ist kostenpflichtig.\n\n\"PDFCreator\" ist unter allen Microsoft-Windows-Betriebssystemen ab Windows Vista lauffähig. Seit Version 0.9.6 wird Windows Vista unterstützt, seit Version 0.9.7 Windows 7. Spätestens ab Version 2 werden Windows 8 und Windows 10 unterstützt.\n\nDas Installationsprogramm von \"PDFCreator Free\" enthält regelmäßig wechselnde Adware. Je nach Version ist die Adware vorausgewählt und die Option zur Abwahl leicht übersehbar. Die \"Plus\"- oder \"Business\"-Variante von \"PDFCreator\" sind werbefrei.\n\n"}
{"id": "890883", "url": "https://de.wikipedia.org/wiki?curid=890883", "title": "Doppelpufferung", "text": "Doppelpufferung\n\nDoppelpufferung (englisch \"double buffering\") beschreibt ein Konzept in der Computergrafik, bei dem der Framebuffer (Bildspeicher) des Video-RAM bei Grafikkarten in zwei Bereiche unterteilt wird. Ziel des Verfahrens ist die Gewährleistung einer kontinuierlichen Bildfrequenz ohne Flackern.\n\nBevor Doppelpufferung eingeführt wurde, unterteilte man den Framebuffer nicht. Bildberechnung und Monitorausgabe fanden also parallel statt. Damit schwankte die Bildfrequenz je nach Szenenkomplexität und das Bild flackerte. Dies entsteht, wenn während der Ausgabe noch in den gleichen Framebuffer geschrieben wird.\n\nDer Framebuffer des Grafikkartenspeichers wird in Front- und Backbuffer unterteilt. Während durch den RAMDAC der Frontbuffer ausgelesen und auf den Bildschirm dargestellt wird, berechnet die GPU im Backbuffer das nächstfolgende Bild. Nach Abschluss dieser Berechnung ist die Ausführung des \"Swap\"-Befehls – das ist die Vertauschung der Speicheradressen von Front- und Backbuffer (\"Page Flip\") – abhängig von VSync.\n\n\nNach Ausführung des \"Swap\"-Befehls (dem Tausch von Front- und Backbuffer) beginnt der ganze Zyklus wieder von vorn.\n\n\n\nWenn in Microsoft Windows das Windows Aero Skin ausgewählt wurde, nutzt der Desktop Window Manager standardmäßig die doppelte Pufferung um Tearing zu verhindern.\n\nIn anderen Bereichen (vor allem in Echtzeit-Anwendungen) wird die Doppelpufferung als Wechselpuffertechnik bezeichnet.\nEin Wechselpuffer entspricht einem Ringpuffer mit zwei Plätzen (Erzeuger-Verbraucher-Problem).\n"}
{"id": "892887", "url": "https://de.wikipedia.org/wiki?curid=892887", "title": "Geri’s Game", "text": "Geri’s Game\n\nGeri’s Game ist ein vierminütiger, animierter Kurzfilm, der 1997 von Pixar produziert wurde.\n\nEs ist Herbst, die Blätter fallen und in einem Park baut ein alter Mann namens Geri ein Schachbrett auf. Er spielt gegen sich selbst, indem er die Tischseiten wechselt und bei weißen Zügen seine Brille auf- und bei den schwarzen Zügen absetzt. Nach komplettem Figurenverlust von Geri Weiß täuscht dieser einen Anfall vor, um dem Spiel doch noch eine Wende zu geben, er dreht das Schachbrett um 180° und stellt damit Geri Schwarz vor die ausweglose Situation.\n\nAls Vorlage des Geri diente Jonathan Harris, der durch die Serie \"Verschollen zwischen fremden Welten\" bekannt wurde.\n\nAußer Grummeln, Lachen und Gemurmel wird nicht gesprochen, daher gibt es auch keine Synchronisation.\n\nGeri ist später in \"Toy Story 2\" noch einmal zu sehen: Er tritt als Puppenrestaurator auf, wobei sein Charakter unverkennbar der alte geblieben ist.\n\nDer Film ist als Extra auf der DVD und dem Video \"Das große Krabbeln\" enthalten sowie in \"Pixars komplette Kurzfilm Collection\".\n\n"}
{"id": "893702", "url": "https://de.wikipedia.org/wiki?curid=893702", "title": "LIRC", "text": "LIRC\n\nLIRC ist die Abkürzung für \"Linux Infrared Remote Control\". Es handelt sich dabei um ein Programm für Linux, mit dessen Hilfe man Befehle von IR-Fernbedienungen in Programmbefehle umsetzen kann. Nötig ist dafür lediglich ein Infrarot-Empfänger. Es gibt verschiedene Typen von unterstützten Empfängern, darunter USB-Empfänger oder solche, die an die serielle Schnittstelle angeschlossen werden. Auf der Webseite des Projekts ist eine Bauanleitung für einen seriellen Empfänger verfügbar. Außer speziellen Empfängern werden auch in TV-Karten oder Soundkarten eingebaute Empfänger unterstützt.\n\nUm die Signale von Fernbedienungen korrekt zu decodieren, benötigt man eine Konfigurationsdatei, die auf die jeweilige Fernbedienung abgestimmt ist. Man kann diese entweder selbst erzeugen, oder eine passende Datei aus dem Internet herunterladen. Unter KDE stellt KDELirc die Verbindung zwischen LIRC und dem Desktop dar.\n\nDas Programm bietet die Möglichkeit, mit dem lircmd-Daemon eine Maus zu simulieren.\n\nNeben der Linux-Version von LIRC gibt es einen Port für FreeBSD. Zusätzlich existiert eine Windows-Portierung namens WinLIRC, die allerdings im Funktionsumfang etwas eingeschränkt ist.\n\n"}
{"id": "895379", "url": "https://de.wikipedia.org/wiki?curid=895379", "title": "SharePoint", "text": "SharePoint\n\nSharePoint ist eine Webanwendung von Microsoft, die unter anderem folgende Anwendungsgebiete abdeckt:\n\n\nMicrosoft nennt sechs Funktionsbereiche von SharePoint:\n\nZentrales Element in SharePoint sind Websites. In ihnen werden alle Inhalte strukturiert und dargestellt. Eine SharePoint-Website besteht aus beliebig vielen einzelnen Webseiten. Diese Einzelseiten nehmen die eigentlichen Inhalte auf. Dies können sein:\n\nSharePoint-Websites können ihrerseits Unter-Websites enthalten, die sich hierarchisch schachteln lassen. Mehrere Websites, deren Berechtigungen gemeinsam verwaltet werden, werden in einer Websitesammlung zusammengefasst.\n\nNeue Websites lassen sich, entsprechende Rechte vorausgesetzt, mittels Vorlagen einfach erstellen. Im Lieferumfang der SharePoint Foundation sind enthalten:\nMit SharePoint Server kommen eine Reihe weiterer Vorlagen hinzu, darunter:\nDiesen Vorlagenkatalog können Entwickler oder Anwender mit entsprechenden Rechten um eigene Vorlagen erweitern. Ferner können Anwender, entsprechende Rechte vorausgesetzt, vorgegebene Websites umgestalten:\n\nFür die Organisation der Inhalte auf einer Seite sind drei Grundtypen von Seiten vorgesehen:\n\nUnter dem Begriff \"Communities\" fasst Microsoft Funktionen zusammen, die die Kommunikation zwischen Personen erleichtern und soziale Netzwerke in Unternehmen unterstützen. Darunter fallen Wikis, Blogs, soziales Tagging, persönliche Websites und Nutzerprofile.\n\nDie Anwendungen \"Meine Website\" und \"Mein Profil\" sind die zentralen Dienste für diesen Bereich. Unter \"Meine Website\" kann jeder Nutzer persönliche Informationen ablegen, beispielsweise der eigene Kalender oder das eigene Postfach, und eigene Dokumente, Bilder und Medien ablegen und anderen zur Verfügung stellen. Auch einen persönlichen Blog und Unter-Websites kann der Benutzer anlegen. Ein Benutzerprofil ergänzt diese Angaben. Daten wie Telefon, E-Mail und Abteilung werden meist aus dem Active Directory importiert. Der Nutzer kann diese Angaben beispielsweise um seine Fachkenntnisse oder ein Foto ergänzen.\n\nAus diesen Informationen kann sich der Nutzer sein persönliches soziales Netzwerk innerhalb seiner Organisation zusammenstellen. Über einen Newsfeed kann er sich über Aktivitäten in diesem Netzwerk auf dem Laufenden halten. Ein Organisations-Browser zeigt ihm die Position jeder Person im Organigramm der Organisation. Mit einer Pinnwand-Funktion kann er kurze Notizen und Fragen auf den Profilen anderer hinterlassen.\n\nWie in einem klassischen Dokumentenmanagementsystem können in SharePoint Dokumente versioniert und mit Metadaten angereichert werden. Sie können ein- und ausgecheckt werden, und Sichtungs- und Freigabeprozesse für Dokumente können eingerichtet werden. Über Web-Feeds oder E-Mail-Benachrichtigung können sich Nutzer informieren lassen, sobald Inhalte einer Bibliothek, einer Liste oder eines Dokuments geändert werden.\n\nWenn ein Nutzer ein Dokument auscheckt, dann bietet SharePoint ihm an, das Dokument in seinem Entwurfsordner auf seiner lokalen Festplatte zu speichern. So kann der Nutzer das Dokument bearbeiten, auch wenn er nicht mit dem SharePoint-Server verbunden ist. Noch weiter gehende Möglichkeiten, Dokumente offline zu halten und zu synchronisieren, bietet \"SharePoint Workspace 2010\". Dieses Produkt ermöglicht, komplette Bibliotheken und Listen lokal zu halten und mit SharePoint zu synchronisieren. \"SharePoint Workspace 2010\" ist das Nachfolgeprodukt von \"Microsoft Office Groove\". Es ist in \"Microsoft Office 2010 Professional Plus\" enthalten, kann aber auch separat erworben werden. Für mobile Endgeräte ist \"SharePoint Workspace Mobile\" vorgesehen. Damit können SharePoint-Inhalte vom Smartphone aus durchsucht, mit den Office-Mobile-Programmen bearbeitet und, sobald wieder eine SharePoint-Verbindung vorhanden ist, synchronisiert werden.\n\nZentral verwaltete \"Inhaltstypen\" unterstützen die Klassifizierung der Dokumente. Jeder Inhaltstyp kann mit einer Vorlage verbunden werden. Zu jeder Dokumentenbibliothek können die Inhaltstypen aus dem Gesamtkatalog ausgewählt werden, die in dieser Bibliothek sinnvoll verwendbar sind. Beim Erstellen eines neuen Dokuments werden automatisch die verfügbaren Vorlagen angeboten.\n\nAlle diese Funktionen sind direkt aus den Microsoft-Office-Anwendungen heraus verfügbar.\n\nIn der Server-Edition sind weitere Funktionen verfügbar, die regelkonformes Lenken von Dokumenten unterstützen. Aufbewahrungspflichtige Dokumente können als \"Datensatz\" deklariert werden. Ab diesem Moment ist sichergestellt, dass – im Sinne der ISO 15489 – innerhalb der gesetzlichen Frist keine Änderung oder Löschung dieser Dokumente stattfindet.\n\nMittels Informationsverwaltungsrichtlinien können Aufbewahrungsfristen definiert werden. So kann beispielsweise festgelegt werden, dass ein Dokument einen Monat nach Erstellung automatisch als Datensatz deklariert und zehn Jahre später automatisch gelöscht wird.\n\nAuch als Web-Content-Management-System lässt sich SharePoint einsetzen. SharePoint Server erfüllt folgende Anforderungen:\n\nIn der Grundeinstellung enthalten SharePoint-Seiten ein einfaches Suchfeld rechts oben. Gibt man dort einen Suchbegriff ein, so wird eine Volltextsuche durchgeführt.\n\nBereits in der SharePoint Foundation werden die Elemente indiziert, und es können eigene Server für die Suche bereitgestellt werden. Die Inhalte können ohne großen Aufwand indiziert und rasch gefunden werden. Die Suche ist jedoch auf die Websitesammlung beschränkt, in der der Benutzer sich bewegt.\n\nIn der Server-Edition kommt eine Reihe weiterer Möglichkeiten hinzu:\n\nMit dem FAST-Server für SharePoint kann Suchleistung und -Komfort noch weiter verbessert werden. Für kleine Organisationen, die nur die SharePoint Foundation einsetzen, bietet der Microsoft Search Server eine Möglichkeit, die Suchleistung zu steigern. In Verbindung mit SharePoint Server ist der Search Server nicht geeignet, da SharePoint Server alle Funktionen von Search Server umfasst.\n\nUnter diesem Sammelbegriff fasst Microsoft die Möglichkeiten zusammen, Anwendungen in SharePoint zu entwickeln, ohne dass dazu Code in einer Programmiersprache wie C# oder Visual Basic geschrieben werden muss:\n\nUnter dem Begriff \"Insights\" fasst Microsoft die Business Intelligence-Komponenten von SharePoint zusammen. Dazu gehören:\n\nDie erste Version der unter dem Codenamen ‚Tahoe‘ entwickelten Software wurde 2001 angeboten. Nach eigenen Angaben hatte die Software im Jahr 2009 mehr als 100 Millionen Benutzer. Das Produkt hat in verschiedenen Versionen Namenszusätze wie ‚Portal Server‘, ‚Services‘ oder ‚Foundation‘ und ist auch als kostenlose Draufgabe zu Windows Server erhältlich.\n\nVolle Browserunterstützung garantierte Microsoft per 2012 nur für die 32-Bit-Versionen von Microsoft Internet Explorer ab Version 7 sowie (eingeschränkt) für die Windows-Versionen von Mozilla Firefox und Google Chrome. Andere Browser funktionieren in den meisten Fällen, dies wird jedoch von Microsoft nicht gewährleistet.\n\nAuch mit Firefox und Chrome gab es jedoch Einschränkungen, darunter:\n\nDies liegt daran, dass diese Funktionen an ActiveX-Steuerelemente gebunden sind, die nur in den 32-Bit-Versionen des Internet Explorers verfügbar sind. Auch die Einschränkungen in den 64-Bit-Versionen des Internet Explorers liegen an fehlenden ActiveX-Steuerelementen. Auch auf 64-Bit-Installationen von Windows wird jedoch die 32-Bit-Version des Internet Explorers zur Verfügung gestellt und kann alternativ zur 64-Bit-Version verwendet werden.\n\nSeitens Microsoft werden – ausgenommen von Funktionen, die noch auf ActiveX basieren – folgende Browser unterstützt:\n\nDie kleinstmögliche Installation für SharePoint Server 2010 besteht aus einer einzigen Servermaschine. Auf dieser werden sämtliche SharePoint-Funktionen, die 64-Bit-Version des Server-Betriebssystems Microsoft Windows Server 2008/R2 und ein Microsoft SQL Server bereitgestellt. Mit einer solchen Installation lassen sich Evaluierungen durchführen und nicht-kritische Lösungen für bis zu ca. 100 Anwender betreiben.\n\nAls Entwicklungssystem lässt sich SharePoint Foundation 2010 sogar auf einem Arbeitsplatzrechner unter den 64-Bit Versionen von Windows 7 oder Vista SP1/SP2 betreiben. Microsoft nennt dafür als Mindestanforderungen einen 64-Bit-Dual-Core-Prozessor mit 3 GHz Taktfrequenz sowie 4 GB Arbeitsspeicher. Für SharePoint Foundation 2013 ist ein Server erforderlich. Mittels Hyper-V kann dieser virtuell auch auf einem Arbeitsplatzrechner betrieben werden, sofern dieser ausreichend ausgestattet ist.\n\nFür größere, skalierbare Installationen muss eine Serverfarm eingerichtet werden. Man gruppiert dazu die benötigten Dienste in Schichten: eine Datenbank-, eine Anwendungs- und eine Front-End-Schicht. Abhängig von der erwarteten oder gemessenen Last werden die benötigten Dienste und Anwendungen auf die Server verteilt. Mit solchen Farmlösungen können\n\nFür noch höhere Anforderungen gruppiert man die Dienste in geeigneter Weise, beispielsweise nach Applikationen, Publikationsdiensten, Kollaborationsdiensten und Abteilungsdiensten, und richtet für jeden dieser Dienste eine eigene Serverfarm ein. Diese Einzelfarmen gruppiert man dann zu einer Gesamtinstallation.\n\nBerechtigungen werden in SharePoint über Gruppen zugeteilt. Einzelpersonen bekommen ihre Rechte im Regelfall über die Mitgliedschaft in einer geeigneten Gruppe. In der Grundeinstellung besitzt jede SharePoint-Website drei Gruppen: Besitzer, Mitglieder, und Besucher. Besitzer einer Website haben alle administrativen Rechte über die Site. Mitglieder können Inhalte verfassen und ändern. Besucher haben nur Lesezugriff. Daneben stellt SharePoint noch einige andere administrative Gruppen zur Verfügung; zudem können weitere Gruppen mit differenzierteren Rechten eingerichtet werden. Die Berechtigungsgruppen gelten jeweils nur innerhalb einer Websitesammlung.\n\nDen Berechtigungsgruppen übergeordnet sind folgende administrative Rollen:\n\nBerechtigungen werden innerhalb der Hierarchie der Inhalte vererbt: Einer Unterwebsite sind in der Grundeinstellung dieselben Berechtigungsgruppen zugeordnet wie ihrer übergeordneten Website. Diese Berechtigung vererbt sich weiter über Bibliotheken und Listen bis zu den einzelnen Dokumenten und Inhaltselementen. Die Vererbung kann jedoch unterbrochen werden, wenn für ein Element auf einer unteren Stufe eingeschränkte oder differenziertere Berechtigungen erforderlich sind. Auch das Recht, Rechte zu verwalten, kann so von einer übergeordneten Website auf eine untergeordnete Ebene delegiert werden. Für die Sicherheit der Inhalte einer solchen dezentral administrierten Website sind die jeweiligen Websiteadministratoren verantwortlich.\n\nDieses Prinzip der Vererbung und Delegation kann jedoch zu schwer durchschaubaren Abhängigkeiten führen. Dies wird dadurch erschwert, dass unter SharePoint weniger Instrumente zur automatischen Verwaltung von Rechten zur Verfügung stehen als beispielsweise im Windows-Dateisystem. Es kann im Einzelfall mühsam werden, festzustellen, welche Personen welche Berechtigungen an einem bestimmten Objekt haben. Wenn eine Person die Rolle im Unternehmen wechselt, sollten die Rechte, die mit ihrer alten Rolle verbunden sind, erlöschen. Dazu kann es notwendig werden, sie aus einer Vielzahl von Gruppen, verteilt über mehrere Websitesammlungen, zu entfernen. Mit den Bordmitteln von SharePoint ist dies sehr mühsam. Abhilfe schaffen hier Zusatzprodukte von Drittanbietern, beispielsweise \"Control Point\" von Axceler.\n\nSharePoint unterstützt folgende Authentifizierungsverfahren:\n\nFerner unterstützt SharePoint anonyme Benutzer. Es ist möglich, in einer SharePoint-Installation unterschiedliche Authentifizierungsverfahren für unterschiedliche Zonen einzurichten. So kann beispielsweise erreicht werden, dass im Intranet die Nutzer mittels Windows-Authentifizierung in Verbindung mit Kerberos identifiziert werden, während sie sich im Internet mittels Digestauthentifizierung ausweisen müssen.\nIm SharePoint ist eine Website namens \"SharePoint-Zentraladministration\" enthalten, mittels der ein Farmadministrator interaktiv alle Verwaltungs- und Überwachungsarbeiten erledigen kann. Sie umfasst:\n\nAlternativ dazu können Administrationsaufgaben mithilfe der Windows PowerShell wahrgenommen und automatisiert werden. SharePoint enthält Bibliotheken, die diese Befehlsshell um spezifische Anweisungen erweitert, die das Verwalten einer SharePoint-Serverfarm erleichtern.\n\nDas Erscheinungsbild von SharePoint lässt sich umfassend verändern – so weitgehend, dass Anwender nicht mehr sehen, dass es sich um SharePoint handelt. Die Gestaltung kann den Vorgaben eines Unternehmens-Designs angepasst werden.\n\nAllerdings beruht das Erscheinungsbild von SharePoint auf dem Zusammenwirken mehrerer Komponenten. Die Abhängigkeiten sind komplex. Für tiefgehende Änderungen sind Zeit und Wissen erforderlich.\n\nFolgende Komponenten wirken zusammen:\n\nDesignvorlagen bieten die einfachste Möglichkeit, Farben und Schriftbild zu konfigurieren. Im SharePoint ist eine Reihe von vorgegebenen Vorlagen enthalten. Dieser Vorlagenkatalog kann um eigene Vorlagen erweitert werden. Design-Vorlagen können mit PowerPoint (sic) erstellt und modifiziert werden. Sie werden in Microsoft-Office-Design-Dateien mit der Erweiterung .thmx gespeichert.\nMasterseiten bestimmen die Anordnung der Komponenten auf einer SharePoint-Seite. Sie sind nichts SharePoint-spezifisches, sondern werden generell in .NET verwendet. CSS-Dateien bestimmen das Aussehen und teilweise auch das Verhalten dieser Komponenten. Die Masterdatei von SharePoint 2010 im Auslieferungszustand heißt \"v4.master\". Die CSS-Core-Datei, die alle CSS-Vorgaben enthält, heißt im Auslieferungszustand \"corev4.css\". SharePoint-Experten raten davon ab, diese Dateien direkt zu modifizieren, denn:\n\nDeshalb legt man sich eine Kopie der \"v4.master\" an, fügt diese dem Katalog der verfügbaren Masterdateien hinzu, und wählt sie als Mastervorlage für die betroffene Website oder Websitesammlung aus. Diese Kopie modifiziert man mit dem SharePoint Designer.\n\nDie \"corev4.css\" ist mithilfe einer Anweisung (Direktive) in die Masterdatei eingebunden. Es ist möglich, diese Datei zu kopieren, zu modifizieren, und die Direktive in der Masterdatei so zu ändern, dass sie auf die neue CSS-Datei verweist. Die \"corev4.css\" ist jedoch sehr umfangreich; zur besseren Übersicht rät der Fachautor Ulrich Boddenberg dazu, eigene kleinere CSS-Dateien anzulegen, in der nur die Elemente enthalten sind, die modifiziert werden müssen. Diese Dateien können über zusätzliche Direktiven so in die Masterdatei eingebunden werden, dass die Definitionen dort ihre Entsprechungen aus der \"corev4.css\" ersetzen.\n\nAus Sicht des Entwicklers ist SharePoint eine Sammlung von Bibliotheken, Klassen, Steuerelementen und Werkzeugen im Rahmen von ASP.NET. SharePoint erweitert ASP.NET 3.5 SP1; ASP.NET 4.0 wird nicht unterstützt.\n\nHäufige Arbeitsgebiete der SharePoint-Programmierung sind:\n\nUnter SharePoint Foundation 2010 und 2013 ist es möglich, Anwendungen als Dienste zu gestalten. Diese Dienste können innerhalb einer SharePoint-Farm oder sogar über mehrere Farmen hinweg Funktionen und Ressourcen zur Verfügung stellen.\nDie wesentlichen Klassen des SharePoint-Server-Objektmodells liegen in den Namensräumen, die mit \"Microsoft.SharePoint.*\" oder \"Microsoft.Office.*\" beginnen. SharePoint-Klassen haben meist einen Namen, der mit \"SP\" beginnt. Für Webpart-Anwendungen enthalten die Namensräume \"System.Web.UI.Web.WebControls\" und \"System.Web.UI.Web.WebControls.WebParts\" einschlägige Klassen. Die zentralen Klassen und ihre Hierarchie sind im Diagramm rechts dargestellt.\n\nZur Programmierung in SharePoint können die meisten .Net-Sprachen verwendet werden. Entwicklungswerkzeug der Wahl ist Microsoft Visual Studio ab Version 2010. Dort stehen Vorlagen für unterschiedliche Typen von SharePoint-Projekten sowie Vorlagen für unterschiedliche Inhaltselemente zur Verfügung. Unabhängig davon, mit welcher Vorlage begonnen wird, kann das Projekt durch Hinzufügen weiterer Elemente beliebig erweitert werden.\n\nDer programmtechnische Zugriff auf SharePoint-Listen und ihre Elemente ist mittels CAML-Abfragen möglich. \"LINQ to SharePoint\" bietet eine alternative, typensichere Methode für denselben Zweck. Zudem bietet LINQ den Vorteil, dass aus der Programmiersprache heraus mit einheitlichen Methoden auf unterschiedliche Datenquellen zugegriffen wird – seien es SharePoint-Listen, XML-Dateien, SQL-Tabellen, Excel-Tabellen und vieles mehr. Damit man auf SharePoint-Listen mittels LINQ zugreifen kann, muss man zuvor mithilfe des mit SharePoint gelieferten Programms \"SPMetal.exe\" ein Objektmodell dieser Listen erstellen und in das betreffende SharePoint-Projekt einbinden.\n\nCAML kann auch dazu verwendet werden, um Felder in Webparts und Formularen darzustellen (zu \"rendern\"). Diese Möglichkeit besteht in SharePoint 2010 hauptsächlich zwecks Kompatibilität mit früheren SharePoint-Versionen. Die Methode der Wahl, um Felder in SharePoint 2010 zu rendern, ist die Verwendung von XSLT. SharePoint unterstützt spezialisiertes Feldrendering für mobile Geräte; neben der Möglichkeit, dies selbst zu gestalten, stellt SharePoint eine Reihe von vorgefertigten Rendervorlagen für mobile Geräte zur Verfügung.\n\nAls generelles Werkzeug für die Entwicklung von Workflows in .NET dient die Windows Workflow Foundation. SharePoint 2010 nutzt dieses Werkzeug; allerdings nicht die aktuelle (Stand 2012) Version 4.0, sondern Version 3.5. Somit kann der SharePoint-Entwickler den Funktionsumfang der Workflow Foundation nutzen; unter anderem kann er:\n\n\nAls Alternative zur Programmierung in Visual Studio steht im SharePoint Designer der \"Workflow Designer\" zur Verfügung, mit dem sich Workflows grafisch modellieren lassen. In der Enterprise-Edition von SharePoint kann der Workflow-Entwickler zudem Microsoft Visio verwenden, um sich Workflows und deren Status anzeigen lassen. Mit der Visio 2010 Premium Edition kann er Workflows für den SharePoint entwerfen und vorhandene SharePoint-Workflows modifizieren.\n\nSharePoint Server 2010 bietet sieben vordefinierte Workflows, die als Vorlage oder direkt ohne Anpassung verwendet werden können. Die SharePoint Foundation bietet lediglich eine Vorlage für einen einfachen sequenziellen Workflow mit drei Zuständen.\n\nZusätzliche Funktionalität beim grafischen Entwurf von Workflows bietet der Drittanbieter Nintex mit seinem Produkt \"Nintex Workflow\".\n\nBis zur Version 2007 wurden SharePoint-Anwendungen stets auf der Ebene der Serverfarm bereitgestellt. Die Anwendungen haben somit Zugriff auf die Serverobjekte der obersten Ebene. Eine fehlerhaft programmierte Anwendung ist in der Lage, einen kompletten SharePoint-Betrieb lahmzulegen, indem sie ein kritisches Serverobjekt funktionsunfähig macht. Auch durch übermäßigen Ressourcenverbrauch kann eine Anwendung auf Serverebene den Betrieb außer Gefecht setzen.\n\nMit SharePoint Foundation 2010 führte Microsoft deshalb sogenannte \"Sandkastenlösungen\" ein. Diese werden nicht mehr auf der Ebene der Serverfarm, sondern auf Websitesammlungen implementiert. Solche Anwendungen können nur noch die Websitesammlung außer Betrieb setzen, aber nicht mehr die gesamte Farm. Auch dem übermäßigen Ressourcenverbrauch kann begegnet werden, weil es möglich ist, genau festzulegen, welche Ressourcen eine Websitesammlung maximal in Beschlag nehmen darf.\n\nMit SharePoint 2013 wurden serverseitige Sandkastenlösungen wieder abgekündigt. Stattdessen empfiehlt Microsoft das neue App-Modell von SharePoint 2013.\nMittels Add-ins können Entwickler SharePoint-Erweiterungen bereitstellen, die mit eingeschränkten Rechten auf die Serverumgebung zugreifen. Gleichzeitig fügen sie sich in die Cloud-Strategie von Microsoft. Es gibt sie in drei Ausprägungen:\n\n\nDrei Hostingmodelle stehen zur Verfügung:\n\nAdd-ins kommunizieren mit dem SharePoint über dessen Client-Objektmodell und über eine neue REST-API, die mit SharePoint 2013 zur Verfügung gestellt wurde. Solche Programme können in vielen Programmiersprachen geschrieben werden; die Bandbreite umfasst .NET-Sprachen, aber auch gängige Web-Sprachen wie PHP und Java, die nicht von Microsoft entwickelt wurden.\n\nSharePoint Apps wurden von Microsoft in SharePoint-Add-ins umbenannt.\n\nVor SharePoint 2010 war clientseitige Programmierung von SharePoint-Anwendungen mühsam, weil Microsoft dafür nur die sperrigen WebDAV- und ASMX-Webdienste zur Verfügung stellte. Clientseitige Programmierung bietet jedoch hohe Flexibilität in der Gestaltung der Benutzeroberfläche. Zudem sind oft die Antwortzeiten besser, weil weniger Daten zwischen Client und Server übertragen werden müssen. Ab SharePoint 2010 stellt Microsoft eine umfangreiche Klassenbibliothek für den clientseitigen Zugriff auf das Server-Objektmodell in einem WCF-Dienst zur Verfügung.\n\nDie Methoden des Clientmodells nehmen XML-Anforderungen entgegen und schicken JSON-Antworten zurück. Die Architektur des Modells ist in SharePoint 2013 im Wesentlichen die gleiche wie in SharePoint 2010. Es kann nicht nur in SharePoint, sondern in jeder verwalteten .NET-Anwendung eingesetzt werden, so auch unter Silverlight.\n\n\n"}
{"id": "895682", "url": "https://de.wikipedia.org/wiki?curid=895682", "title": "Xgrid", "text": "Xgrid\n\nXgrid ist ein Konzept des Grid-Computings und verteilten Rechnens, welches von der Firma Apple Inc. für das Betriebssystem Mac OS X entwickelt wurde. Die Lösung zeichnet sich durch einfache Konfigurierbarkeit und vollständig implementierten Support durch das Betriebssystem aus. Inzwischen gibt es auch Lösungen, mit denen sich auch andere Systeme in Xgrid integrieren lassen (z. B. Windows oder Linux)\n\nXgrid benutzt, wie beim Grid-Computing üblich, drei Komponenten:\nDer Agent läuft auf jedem Knoten im Cluster, also jedem Rechner, der seine Ressourcen zur Verfügung stellt, und nimmt die Aufträge entgegen. Der Controller steuert das Grid als zentrale Verwaltungseinheit. Der Client ist schließlich das Programm, welches die Grid-Ressourcen anfordert. Agenten lassen sich auf zahlreichen Betriebssystemen aufsetzen und in das Apple-Xgrid einbinden. Dazu existieren Updates für ältere Mac-OS-X-Systeme, Lösungen für Unix und Linux bzw. Java-Lösungen, die Betriebssystem-unabhängig sind und daher auch auf Microsoft-Windows-Betriebssystemen laufen.\n\nPraktisch gesehen benötigt man an Hardware nichts weiter als einen Computer mit einer Netzwerkverbindung. Auf diesen Grid-Computern übernimmt eine Software das Lösen einer Teilaufgabe, die ein – in der Regel zentraler – Server zur Verfügung stellt. Dieser Server benutzt Software, die eine große Aufgabe in eine Anzahl von Teilaufgaben für alle Knoten im Grid aufspalten kann und die Teilergebnisse wieder zusammenfasst.\n\nDie erste Implementierung dieser Idee erfolgte in OPENSTEP mit der Demoapplikation „zilla.app“. Alle notwendigen Komponenten sind in Mac OS 10.4 (Tiger) und 10.5 (Leopard) bereits vollständig im System integriert und müssen nur noch aktiviert werden (Agenten und Controller). Für ältere Mac-OS-X-Versionen (Panther) gibt es Updates für Xgrid. Alternative Betriebssysteme werden seit einiger Zeit durch Software von Drittanbietern unterstützt.\n\n\n"}
{"id": "900006", "url": "https://de.wikipedia.org/wiki?curid=900006", "title": "Ipconfig", "text": "Ipconfig\n\nipconfig ist ein Kommandozeilenbefehl, der in Microsoft Windows ab Windows for Workgroups 3.11 mit nachinstalliertem TCP/IP-Stack sowie allen 32- und 64-Bit-Windows-Ausgaben enthalten ist. Es zeigt die Hardwareadressen bzw. die IP-Adressen der im lokalen Netzwerk verwendeten Geräte an. \n\nMit dem Befehl codice_1 werden die Adressdaten des lokalen IP-Netzwerkes abgerufen. Die Adressen lassen sich auch über die Systemsteuerung unter Netzwerkeinstellungen einsehen. Die Anzeige über den Befehl codice_1 hat den Vorteil, dass die Daten übersichtlich dargestellt werden.\n\ncodice_1 kann per Windows 7 folgende allgemeine Informationen liefern:\n\n\nMit codice_4 kann man sich folgende Informationen ausgeben lassen:\n\n\nDazu werden Informationen zu allen Netzwerkadaptern inklusive Modems und ISDN-Karten geliefert:\n\n\nMit codice_5 wird die IPv4-Adresse für den Angegebenen Adapter freigegeben. Wird kein Adapter angegeben werden die IPv4-Adressen aller Adapter freigegeben.\n\nMit codice_6 können die IPv4-Adressen für einen oder für alle Adapter erneuert werden. \n\ncodice_7 löscht den DNS Cache\n\nWird am Ende des jeweiligen Befehls eine 6 geschrieben, betrifft der Befehl die IPv6-Adressen.\n\nAußerdem kann mit codice_8 eine Hilfe mit allen Optionen und Beispielen ausgegeben werden.\n\nDas Sichern und Wiederherstellen eines bestehenden IP-Netzwerks ermöglicht das Tool netsh.\n\nUnter Unix und unixoiden Systemen gibt der Befehl codice_9 ähnliche Informationen aus.\n"}
{"id": "900680", "url": "https://de.wikipedia.org/wiki?curid=900680", "title": "BOB (Computergrafik)", "text": "BOB (Computergrafik)\n\nEin BOB ist ein vom Amiga-Computer bekanntes grafisches Objekt, das auf dem Bildschirm ähnlich einem Sprite bewegt werden kann. Der Name \"BOB\" leitet sich aus \"blitter object\" ab.\n\nBobs unterscheiden sich von Sprites dadurch, dass Bobs tatsächlich in den Bildspeicher kopiert werden, während Hardware-Sprites in eigenen Registern oder Speicherbereichen stehen und erst zum Anzeigezeitpunkt in den Datenstrom geschaltet werden. Im Gegensatz zu Sprites sind Bobs nicht in Größe und Anzahl begrenzt. Es gibt auch den Begriff \"Software-Sprite\", welcher jedoch nicht mehr als ein in Software realisierter Bob ist. Software-Sprites nennt man auch Shape.\n\nEin Blitter ist zuständig für das äußerst schnelle Verschieben oder Kopieren von großen Speicherbereichen innerhalb eines bestimmten Adressraumes, das heißt auch innerhalb des Bereiches für die Bildwiedergabe. Um ein bewegtes Objekt darzustellen, wird zunächst der Bereich des Hintergrundes, auf dem das Objekt dargestellt werden soll, in einem Puffer als Rastergrafik kopiert und dort gesichert. Dann wird eine Grafik des zu bewegenden Objektes aus einem anderen Puffer auf den Hintergrund kopiert und somit dargestellt. Um das Objekt zu bewegen, werden immer abwechselnd gesicherter Teil-Hintergrund und das darzustellende Objekt auf den Hintergrund kopiert. Bei jedem Kopieren werden die Koordinaten entsprechend der gewünschten Bewegung angepasst. Bobs benötigen daher erheblich mehr Rechenleistung als Sprites, da jeder Kopiervorgang einen Speicherdirektzugriff zum Kopieren der Grafikdaten in den Bildschirmspeicher erfordert. Ein Blitter ist jedoch in der Lage, solche Vorgänge äußerst schnell und unabhängig vom Hauptprozessor durchzuführen.\n\nEine Blitter-Einheit ist in der Regel direkt in den ICs der Speicherverwaltung integriert, wie zum Beispiel beim Agnus des Amiga von Commodore oder beim geplanten aber nie in Serie gebauten C65, von dem nur einige Prototypen existieren.\n\n"}
{"id": "900873", "url": "https://de.wikipedia.org/wiki?curid=900873", "title": "Shape (Computergrafik)", "text": "Shape (Computergrafik)\n\nEin Shape ist ein grafisches Objekt, welches auf dem Computerbildschirm dargestellt werden kann. Von der Funktion her ähnelt es zwar einem Sprite, wird jedoch nicht autonom von der Grafikhardware dargestellt. Vielmehr ist für die Darstellung von Shapes die CPU des Computers zuständig, die das Shape an der passenden Stelle in die Bitmap hineinkopiert, was zu Lasten der Systemleistung geht. Auch wenn Shapes keine echte Alternative zu hardwarebeschleunigten Sprites oder BOBs sind, relativieren heute schnelle Prozessoren die Nachteile.\n"}
{"id": "903331", "url": "https://de.wikipedia.org/wiki?curid=903331", "title": "Peer Name Resolution Protocol", "text": "Peer Name Resolution Protocol\n\nDas Peer Name Resolution Protocol (PNRP) ist ein Netzwerkprotokoll zur Namensauflösung nach dem Peer-to-Peer-Modell. \n\nEs soll die Veröffentlichung von Namen und deren Auflösung in Adressen ohne Server ermöglichen, wie sie im Domain Name System notwendig sind. \n\nDas Protokoll wurde von Microsoft entwickelt und erstmals in Windows Vista implementiert, wo es NetBEUI ablöst. In Microsoft Windows XP wurde es mit Service Pack 3 eingeführt, muss dort aber explizit aktiviert werden. Es ist nur unter IPv6 oder unter Verwendung von Teredo nutzbar und in Vista standardmäßig deaktiviert. Eine Namensauflösung ist aber möglich, da der Dienst dynamisch gestartet wird.\n\nEs besitzt Ähnlichkeiten mit dem Peer-to-Peer Netz Pastry und nutzt eine Abwandlung des Präfix-Routings.\n\nUnter Microsoft Windows 7 wird PNRP in Zusammenspiel mit dem Remote Desktop Protocol für die Remoteunterstützung genutzt, und ermöglicht dort ein unkompliziertes Überbrücken einer Network Address Translation.\n\nMicrosoft stellt die Spezifikation des Protokolls als Open Specification zur Verfügung. \n\n"}
{"id": "903670", "url": "https://de.wikipedia.org/wiki?curid=903670", "title": "Abgesicherter Modus", "text": "Abgesicherter Modus\n\nDer Abgesicherte Modus ist ein spezieller Startvorgang eines Betriebssystems, den es unter Windows, Apple Mac OS sowie Google Android OS gibt.\n\nDer Abgesicherte Modus (auch \"eingeschränkter Modus\") bezeichnet eine besondere Art des Startens der Windows-Betriebssysteme von Microsoft, die sich vom regulären Normalstart in einigen Punkten unterscheidet:\n\n\nDen abgesicherten Modus erreicht man, indem man während des Starts des Computers im richtigen Augenblick die F8-Taste drückt. Dies ist der Moment kurz bevor das Windows-Logo erscheint (Es hat sich als praktikabel erwiesen, während des Bootvorgangs die F8-Taste einfach wiederholt zu drücken, wobei einige BIOS dann zunächst ein Auswahlmenü des Bootmediums präsentieren). Neben anderen Startoptionen erscheint ein Menü mit der Auswahl:\n\n\nBei wenigen Windows-Versionen kann man statt der F8-Taste die F5-Taste drücken. Windows NT sowie Windows 98 und höher unterstützen standardmäßig F8. In bestimmten Situationen gelangt man ausschließlich über den Umweg der Windows Starthilfe in den Abgesicherten Modus: Starthilfe wählen --> automatische Systemwiederherstellung unterbrechen --> \"Erweiterte Optionen\" wählen --> \"Neu Starten\" wählen --> jetzt sofort F5 drücken (ggf. mehrmals) bis Windows Start Manager erscheint --> jetzt gelangt man mit F8 in den Abgesicherten Modus.\n\nSinn und Zweck des abgesicherten Modus ist die Möglichkeit, einen Reparaturversuch durchführen zu können, wenn sich Windows regulär nicht mehr starten lässt. Häufig, aber nicht immer, lässt sich Windows im abgesicherten Modus starten, während der normale Start zu einem Bluescreen führt. Stellt Windows beim Normalstart Probleme fest, wird beim nächsten Starten von Windows oft obiges Auswahlmenü mit dem abgesicherten Modus auch ohne vorheriges Drücken der F8-Taste eingeblendet.\n\nWeil im abgesicherten Modus die meisten sonst im Hintergrund arbeitenden Prozesse (Wächter von Antivirensoftware, Netzwerkkomponenten, erweiterter Grafiktreiber, aber auch viele Computerviren uvm.) nicht mitstarten, ist der abgesicherte Modus auch besonders geeignet, um nach Fehlern zu suchen oder Deinstallationen vorzunehmen.\n\nAuch Mac OS kann in einem abgesicherten Modus (\"Safe Boot\" im Apple-Vokabular) gestartet werden. Unter neueren Systemversionen sind unterschiedliche Ebenen des abgesicherten Modus möglich.\n\nDurch Drücken der Maustaste während des Startvorgangs werden die vorhandenen Systemerweiterungen deaktiviert. Außerdem wird eine ggf. eingelegte Diskette ausgeworfen.\n\nWenn während des Startvorgangs die Shift-Taste (Umschalttaste) gedrückt wird, dann werden die vorhandenen Systemerweiterungen deaktiviert. Durch Drücken der Leertaste wird in einer frühen Startphase (auf jeden Fall vor dem Erscheinen des Login-Dialogs) das Kontrollfeld \"Systemerweiterungen Ein/Aus\" aufgerufen.\n\nDrückt man während des Startvorgangs die Shift-Taste bei einem Computer mit macOS, so wird dieser im abgesicherten Modus gestartet. Das bewirkt, dass das Dateisystem repariert wird, nur die unbedingt nötigen Treiber und die Systemschriften geladen werden, und verschiedene Caches geleert werden.\n\nDurch Drücken von cmd+S startet Mac OS X im Single User Mode. Es werden nur die nötigen Treiber geladen, und statt der grafischen Oberfläche wird eine Root-Shell angezeigt, mithilfe derer der Nutzer das System reparieren kann.\n\nWenn während des Startens cmd+V (verbose, engl. \"ausführlich\") gedrückt wird, wird während des Bootvorgangs eine Konsole angezeigt, in die das System Log-Einträge schreibt. Das System wird allerdings vollständig geladen und die grafische Oberfläche erscheint auch; von da her ist das kein abgesicherter Modus, bietet aber einen Einblick in das, was während des Bootens geschieht.\n\n\n"}
{"id": "905928", "url": "https://de.wikipedia.org/wiki?curid=905928", "title": "Stellarium", "text": "Stellarium\n\nStellarium ist ein freies Astronomieprogramm unter GNU General Public License zur Simulation eines Planetariums. Die Software läuft unter Microsoft Windows (9x, NT, 2000, XP, Vista, 7, 8, 8.1, 10), macOS und GNU/Linux.\n\nMittels OpenGL wird ein fotorealistischer Sternenhimmel mit über 600.000 Sternen in Echtzeit dargestellt, wobei Ort und Zeit sowie der Raumwinkel frei gewählt werden können. Gezeigt werden Sterne und weitere astronomische Objekte, die mit dem bloßen Auge, einem Fernglas oder einem Teleskop sichtbar sind. Der Sternenkatalog kann bis zur 18. Größenklasse und somit auf über 210 Millionen Sterne ergänzt werden.\n\nEinige herausragende Merkmale sind ein fast fotorealistischer Sonnenaufgang und -untergang, Mond- sowie Sonnenfinsternisse, Meteore und Grafiken von Planeten, Nebeln und Galaxien. Auch zeigt Stellarium die Veränderung der Form der Sternbilder über Jahrtausende hinweg an. Es lassen sich auch Animationen erzeugen. Dafür lassen sich Skripte definieren (Beispielskripte sind enthalten).\n\nProgrammiert wurde Stellarium von dem Franzosen Fabien Chéreau. Viele Grafiken und Bilder im Programm stammen von anderen Autoren.\n\n"}
{"id": "907696", "url": "https://de.wikipedia.org/wiki?curid=907696", "title": "Ulead PhotoImpact", "text": "Ulead PhotoImpact\n\nUlead PhotoImpact ist ein Bildbearbeitungsprogramm, das unter Windows-Betriebssystemen läuft und auf den Heimanwender zugeschnitten ist.\n\nDer Benutzer kann mit PhotoImpact digitalisierte Bilder bearbeiten und verfremden. Über TWAIN-Schnittstellen ist es möglich, einzelne Bilder direkt vom Scanner oder der Digitalkamera abzurufen. In PhotoImpact lassen sich Filter zur Bildverfremdung, die im Photoshop-Format vorliegen, einbinden, um die Möglichkeiten der Bildbearbeitung zu erhöhen. Mit einem Assistenten kann der Benutzer eine eigene Homepage erstellen.\n\nPhotoImpact ist im gleichen Marktsegment wie PaintShop Pro angesiedelt und bedient die gleiche Zielgruppe. Bei Tests in Computerzeitschriften werden diese beiden Programme und die Möglichkeiten, die sie für den Anwender bieten, direkt verglichen.\n\nDie Firma Ulead wurde von InterVideo und im Herbst 2006 anschließend vom Unternehmen Corel übernommen, vorgeblich um dessen Produktpalette abzurunden.\n\nLaut Aussage der Fa. Corel war Version X3 die letzte Version, die es von PhotoImpact gab. Vor Ulead hatte Corel im Jahr 2004 bereits Jasc Software und damit auch Paint Shop Pro übernommen. Nach der Übernahme von Ulead wurde PhotoImpact zur Konkurrenz aus eigenem Hause. Diese Zweigleisigkeit wurde im Jahr 2009 beendet.\n\nEinige Elemente von PhotoImpact finden sich in Paint Shop Pro X3 wieder.\n\n"}
{"id": "908962", "url": "https://de.wikipedia.org/wiki?curid=908962", "title": "WordPad", "text": "WordPad\n\nWordPad ist ein kompaktes Textverarbeitungsprogramm von Microsoft mit einigen Formatierungsmöglichkeiten. Es enthält allerdings bei weitem nicht so viele Funktionen wie andere Textverarbeitungsprogramme aus Office-Paketen.\n\nSeit Windows 95 wird WordPad mit allen Varianten des Betriebssystems Microsoft Windows ausgeliefert. WordPad ist der Nachfolger von Microsoft Write, das in früheren Versionen von Windows enthalten war. Als Überbleibsel dieser Zeit lässt sich auch in aktuellen Windows-Versionen WordPad mit dem Befehl „write“ aufrufen.\n\nWordPad wurde kurz vor der Auslieferung von Windows 95 als MFC-3.2-Beispiel als Quelltext veröffentlicht der heute noch öffentlich zugänglich ist.\n\nVon Windows 95 bis Windows Vista wurden die Engines RichEdit 1.0, 2.0, 3.0 und 3.1 verwendet, seit Windows XP SP2 und auch für Vista-Updates wurde RichEdit 4.1 verwendet.\n\nIn der Windows-7-Version wurde es komplett überarbeitet. So wurde die aus Office 2007 bekannte Ribbon-Oberfläche eingebaut. Sie ähnelt der Oberfläche von Microsoft Office. Außerdem wurde WordPad um einige Funktionen erweitert, es werden zudem weitere Dateiformate (siehe unten) unterstützt.\n\nNeben reinen Textdateien in ANSI (Windows-Format / ISO 8859) sowie ASCII („MS-DOS-Format“, im Gegensatz zum Notepad) kann WordPad Dateien im universell verbreiteten Rich Text Format (RTF) verarbeiten, seinem Hauptformat. Seit Windows XP wird Unicode unterstützt. Dabei schreiben beispielsweise Windows Server 2000 UTF-16BE und Windows 7 UTF-16LE. In Windows 7 wird eine UTF-8-formatierte Textdatei korrekt gelesen, bei einem einfachen „Speichern“ aber als „Textdatei“ (=ANSI) geschrieben. Mit „Speichern unter“ und der Auswahl „Unicode“ wird eine UTF-16LE-Datei geschrieben.\n\nAb Windows XP Service Pack 2 und Windows Server 2003 SP2 wurde die Lese-Unterstützung für das programmeigene Dateiformat seines Vorgängers Microsoft Write (.wri-Dateien) sowie für Microsoft-Word-6-Dateien aus Sicherheitsgründen abgeschaltet. Sie kann notfalls über die Registry wieder aktiviert werden, dann sollen aber nur Dateien aus vertrauenswürdigen Quellen geöffnet werden.\n\nAb der Version in Windows 7 ist es in der Lage, Office Open XML (.docx) sowie OpenDocument (.odt) zu lesen und zu schreiben. Dokumente in anderen Formaten werden als Nur-Text-Dokumente geöffnet und möglicherweise nicht wie erwartet dargestellt.\n"}
{"id": "908994", "url": "https://de.wikipedia.org/wiki?curid=908994", "title": "Esprit (CAD/CAM)", "text": "Esprit (CAD/CAM)\n\nDas Programmiersystem ESPRIT des amerikanischen Herstellers DP Technology dient der Programmierung folgender Technologien:\n\n\nDas System ist modular aufgebaut, das heißt jeder kann sich gleich einem Baukasten das System so zusammenstellen, wie er es benötigt.\n\nIn ESPRIT sind ca. 450 Postprozessoren enthalten für viele gängige Maschinen. Kundenspezifische Postprozessoren können erstellt und angepasst werden.\n\nESPRIT verfügt über eine 100 %-Windows-native Oberfläche, ist dadurch leicht zu erlernen und kann den Bedürfnissen des Anwenders problemlos angepasst werden. Außerdem verfügt es über eine integrierte VBA-Umgebung (Visual Basic for Applications), so dass sich jeder Anwender Makros schreiben kann und das System entsprechend seinen Bedürfnissen erweitern kann.\n\nDie Simulation verfügt über komplette komplexe Maschinenumgebungen, das heißt jeder Anwender kann seine Maschine 100 % realistisch simulieren, um sicherzustellen, dass in der Werkstatt während der Bearbeitung keine Kollisionen auftreten.\n"}
{"id": "912526", "url": "https://de.wikipedia.org/wiki?curid=912526", "title": "Media Transfer Protocol", "text": "Media Transfer Protocol\n\nDas Media Transfer Protocol (MTP) ist ein Netzwerkprotokoll zur Übertragung von Dateien zwischen mobilen Endgeräten (wie z. B. Smartphones) und PCs. Es ist eine Weiterentwicklung des Picture Transfer Protocol.\n\nVorgestellt wurde MTP im Herbst 2004 von den Unternehmen Microsoft und Canon. Es soll die Möglichkeiten des Picture Transfer Protocol so erweitern, dass die Kommunikation auch mit anderen Geräten – neben den bisher unterstützten Digitalkameras – erweitert wird. So ermöglicht eine spezielle UMS-Software, Audiodaten ohne zusätzliche Gerätetreiber auf MTP-fähige MP3-Player zu laden. Mittlerweile wird MTP auch von vielen Smartphones verwendet.\n\nZur Datenübertragung mit einem MTP-Gerät wird eine MTP-fähige Software benötigt. Einige MP3-Player lassen sich mit Hilfe einer Firmware-Aktualisierung als UMS-Gerät (\"USB-Massenspeicher\") betreiben; UMS-Geräte werden von allen gängigen Betriebssystemen ohne spezielle Treiber unterstützt.\n\n\n\n\n"}
{"id": "916757", "url": "https://de.wikipedia.org/wiki?curid=916757", "title": "KTorrent", "text": "KTorrent\n\nKTorrent ist ein freies, plattformübergreifendes BitTorrent-Clientprogramm für KDE, das neben dem Übertragen von Dateien über das BitTorrent-Netzwerk viele zusätzliche Verwaltungs- und Protokollfunktionen bietet und mit einer vergleichsweise effizienten und sparsamen Speicherverwaltung aufwarten kann.\n\nKTorrent wird als Freie Software auch im Quelltext unter den Bedingungen der GNU General Public License (GPL) veröffentlicht. Es ist KDE-typisch in C++ geschrieben und nutzt das GUI-Toolkit Qt.\n\nEin Herunterladen einzelner Dateien aus BitTorrent-Archiven, die Begrenzung der Übertragungsraten und eines Verteilungsverhältnisses sind möglich. Weiterhin werden Protokollverschlüsselung und trackerlose Torrents (mittels verteilter Hashtabellen) unterstützt.\n\nNeben den Basisfunktionen des Hauptprogrammes stellen Zusatzmodule, die auch einzeln deaktiviert werden können, weitere Funktionen bereit. Unter anderem werden von vornherein Module mitgeliefert für\n\nVersion 1.0 kam am 12. Juli 2005 heraus, schon am 30. November wurde Version 2 veröffentlicht.\n\nAb Version 2 unterstützt KTorrent den trackerlosen Betrieb und als erster Linux-Client außer dem Java-basierten Azureus (heute Vuze) die Protokollverschlüsselung.\n\nIn Version 2.1 wurde die Oberfläche überarbeitet, Torrents lassen sich nun gruppieren, um bei vielen Übertragungsvorgängen die Übersicht zu behalten und die Zeroconf-Erweiterung kam dazu.\n\nAb Version 3.0 wurde KTorrent auf die Bibliotheken der KDE-Version 4 umgestellt. Ab Version 3.1 ist es auch unter Windows möglich, die Software zu nutzen.\n\n\n"}
{"id": "920735", "url": "https://de.wikipedia.org/wiki?curid=920735", "title": "Most Frequently Used", "text": "Most Frequently Used\n\nAls Most Frequently Used (MFU) wird eine Liste bezeichnet, die Dateien nach der Häufigkeit der Aufrufe speichert. Diese Liste wird der Reihenfolge nach angezeigt. Die Programme, die häufiger aufgerufen wurden, werden weiter oben angezeigt und Programme, die weniger oft aufgerufen wurden, weiter unten.\n\nDie MFU ist eine Erweiterung, die bei Windows XP das erste Mal aufgetreten ist. Zu finden ist sie üblicherweise nach dem Klicken des bei Windows XP vorhandenen \"Start-Buttons\".\n\n"}
{"id": "922602", "url": "https://de.wikipedia.org/wiki?curid=922602", "title": "DirectX Media Objects", "text": "DirectX Media Objects\n\nDirectX Media Objects (DMOs) sind streamingfähige Komponenten, die auf COM basieren.\nSie arbeiten ähnlich wie DirectShow Filter, indem sie Eingabedaten entgegennehmen und Ausgabedaten produzieren.\nAllerdings ist die DMO-API einfacher zu benutzen als die entsprechende DirectShow-API.\nSo sind DMOs einfacher zu erstellen, testen und benutzen.\nUm einfache Encoder, Decoder, oder Effekte zu schreiben, sollte daher DMOs der Vorzug zu DirectShow Filtern gegeben werden.\n\nMögliche Einsatzgebiete:\n\n\n\n\n\"Siehe auch: DirectX\"\n"}
{"id": "922637", "url": "https://de.wikipedia.org/wiki?curid=922637", "title": "DScaler", "text": "DScaler\n\nDScaler ist eine freie Software zur Echtzeit-Wiedergabe digitaler Video-Quellen (vor allem analoge TV-Karten). Ziel bei der Entwicklung von DScaler war es, die bestmögliche Bildqualität speziell auf Geräten mit hoher Bildauflösung zu erzielen. Eine Besonderheit sind die diversen Deinterlace-Filter, die eine deutlich bessere Qualität liefern als die meisten (auch kommerziellen) Programme zur TV-Wiedergabe. Zusätzlich bietet DScaler auch viele weitere Filter um das Video-Signal zu verbessern, zum Beispiel Rauschunterdrückung, Schärfung und Gamma-Korrektur.\n\nDScaler benötigt zur Nutzung von analogen TV-Karten keine Treiber, da es direkt auf die Hardware der verwendeten Chipsätze der Karte zugreift. Das ist zwar nicht Programmkonform, aber erhöht die Leistung und Verwendbarkeit dieser Software.\n\nFolgende Chips werden derzeit unterstützt:\n\nEbenfalls werden viele Tuner der Analog-Karten unterstützt, in der Regel werden diese auch automatisch erkannt. Darüber hinaus auch diverse Video (S-Video) Eingänge der Karten.\nDie Version 5 der Software befindet sich „in einem frühen Entwicklungsstadium“ (bisher kann nur der MPEG-Filter getestet werden). In dieser Version soll die Software dann nicht mehr direkt auf die Hardware zugreifen, sondern die Treiber verwenden, um den Prozessor weniger zu belasten.\n\n"}
{"id": "924029", "url": "https://de.wikipedia.org/wiki?curid=924029", "title": "Atari-Heimcomputer", "text": "Atari-Heimcomputer\n\nBei den Atari-Heimcomputern handelt es sich um eine Serie früher, untereinander weitgehend kompatibler 8-Bit-Heimcomputer der Firmen Atari, Inc. und Atari Corporation aus Sunnyvale, Kalifornien, Vereinigte Staaten. Allen Rechnern der Atari-Heimcomputer-Serie ist gemeinsam, dass sie auf dem Ende der 1970er Jahre weit verbreiteten Hauptprozessor MOS Technology 6502 basieren und zusätzlich vom Hersteller unter der Leitung von Jay Miner eigenentwickelte Custom Chips zur Grafik- und Tonerzeugung verwenden. Diese elektronischen Spezialbausteine verhalfen den Rechnern zu einer bei Erscheinen der Computer überdurchschnittlichen Leistungsfähigkeit. Sie waren damit die ersten Heimcomputer, die mit speziellen Co-Prozessor-Chips entwickelt wurden.\n\nDie Markteinführung erfolgte im Jahr 1979. Im Dezember 1991 wurde die Produktion eingestellt und im Folgejahr der Support. Die verschiedenen Modelle der Atari-8-Bit-Heimcomputer-Serie fanden große Verbreitung, insbesondere in Nordamerika und Europa. Die Serien 400/800 (ab 1979), XL (ab 1983) und XE (ab 1985) unterscheiden sich hauptsächlich durch ihre Gehäuseform und das Betriebssystem, aber auch durch kleinere technische Details wie RAM-Konfiguration, die Anzahl der Joystickanschlüsse und die Art der Tastatur.\n\nAb Ende 1977 entwickelte die bis dato nur in den Branchen Videospiele und Arcade-Automaten tätige Firma Atari ihre eigene 6502-CPU-basierte Heimcomputer-Modellreihe: das Einsteigermodell Atari 400 mit Folientastatur und zunächst lediglich 8 kByte RAM (später 16 kByte RAM standardmäßig) und den für gehobenere Ansprüche gedachten, aufrüstbaren Atari 800 mit bis zu 48 kByte RAM und Schreibmaschinentastatur. Im Gegensatz zu den Konkurrenzprodukten von Apple, Tandy und Commodore wurden die ab Ende 1979 erhältlichen Atari-Rechner mit leistungsfähigeren und leichter zu programmierenden elektronischen Spezialbausteinen (ANTIC, CTIA bzw. GTIA bei neueren Geräten, POKEY) ausgestattet. Die Geräte enthalten zudem vier Anschlüsse für Joysticks im Gegensatz zu den Modellen der Konkurrenz, die erst kostenintensiv nachgerüstet werden mussten. Die Atari 400/800-Heimcomputer waren die ersten Geräte im Heimbereich, die über eine „intelligente“ Anbindung der Peripheriegeräte, den so genannten Atari SIO-Bus, verfügten. \n\nEine gut abgestimmte Palette an Zubehör, Spielen, Anwendungsprogrammen, Programmiersprachen, ausgezeichnete Kundenbetreuung, die Einbindung anderer Anbieter und die gezielte Platzierung der Computer in Bildungseinrichtungen trugen maßgeblich zum Erfolg dieser Baureihe vom Erstverkauf im Jahre 1979 an bei. Die Produktion wurde 1983 eingestellt.\n\nAtari bot zwei Floppy-Modelle für seine Computer an. Für deren Betrieb ist ein Minimum von 16 kByte RAM erforderlich. Es können bis zu vier Diskettenstationen angeschlossen werden (IDs 0-3), die Einstellung der Laufwerks-ID erfolgt über einen schwarzen und weißen Schiebeschalter, die von der Rückseite aus mit einem spitzen Gegenstand betätigt werden können.\n\nDie Diskettenstation 810 kann bis zu 88.375 Byte für Daten und Programme pro Diskettenseite speichern. Ein eigener Mikroprozessor 6507 steuert den Lese- und Schreibbetrieb. Schaltet man die Betriebsspannung des Atari 800 aus und dann wieder ein, wird automatisch die Diskette auf 0 gesetzt und der Ladevorgang beginnt. Die 5¼\"-Diskette ist in 720 Sektoren unterteilt. Es wird ein CRC (cyclic redundancy check) durchgeführt, der 13 Sektoren auf der Diskette für sich beansprucht.\n\nDie Diskettenstation 815 beinhaltet zwei Systeme für 5¼\"-Disketten. Hier lassen sich auf jeder 178 kByte an Daten und Programmen abspeichern. Wie bei dem 810-Modell steuert auch hier ein Mikroprozessor den gesamten Ablauf des Lese- und Schreibvorganges.\n\nDie 810 und die 815 sind nicht kompatibel im Aufzeichnungsformat, keine von ihnen kann Disketten lesen, die mit dem anderen Laufwerk aufgezeichnet wurden.\n\nBeide Laufwerke beschreiben nur eine Diskettenseite. Die Verwendung der Rückseite ist durch Wenden möglich.\n\nDieser Drucker erhält von dem Computer oder von dem Schnittstellenmodul 850 seine Daten seriell. Im Drucker befindet sich ein Mikroprozessor vom Typ 6507, einem verkleinerten 6502. Ein weiterer Baustein 6532 hat ein internes RAM mit 128 Byte und 16 Ein- und Ausgängen. Die Steuerung der Software übernimmt ein 2K-ROM. Auf einer Zeile werden maximal 40 Symbole pro Sekunde von einer 5×7-Drucker-Matrix auf das Papier gebracht. Das Papier ist Standard und auf einer Rolle untergebracht.\nDieser Drucker erhält seine Daten seriell vom Computer oder Schnittstellenmodul. Die Druckgeschwindigkeit liegt bei 37 Symbolen pro Sekunde. Die Breite pro Symbol beträgt 2,53 mm, und es werden 40 Symbole pro Zelle gedruckt. Neben den Symbolen ist auch ein Grafikdruck möglich. Die Symbole sind durch den ASCII-Satz auf 96 Zeichen festgelegt. Der Druck erfolgt mittels einer 5×7-Matrix auf Thermopapier, da der Druck thermisch erfolgt. Durch den internen Mikroprozessor lässt sich das Papier vor- und rückwärts transportieren. Für den Hobbyanwender dürfte dies der richtige Drucker sein.\nDieser Drucker ist speziell für die Textverarbeitung. Die Schreibbreite beträgt etwa 100 mm und hat vier verschiedene Betriebsarten:\n\nDer Druck wird durch einen Mikroprozessor im Gerät gesteuert. Der Druckkopf ist mit einer 7×8-Matrix ausgestattet, wobei eine Normal- und Schmalschrift entsteht. Pro Zeile ergeben sich dadurch 80 oder 132 Zeichen. Die einzelnen Symbole sind nach dem ASCII-Standard festgelegt, und es stehen 96 Charakter zur Verfügung. Die Druckgeschwindigkeit liegt bei 50 Zeichen pro Sekunde. Die eingehenden ASCII-Zeichen werden in einem Datenbuffer eingeschrieben und zwischengespeichert. Insgesamt lassen sich etwa 1200 Zeichen speichern. Der Mikroprozessor steuert den gesamten Ablauf.\n\nDer Drucker kann nur in Verbindung mit dem Schnittstellenmodul 850 arbeiten, der den geeigneten 7-Bit ASCII-Code mit den entsprechenden Leerzeichen erstellen kann.\n\nFür die Datenübertragung per Telefon wurde dieser Koppler entwickelt. Der Akustik-Koppler wird an dem Schnittstellenmodul angeschlossen und erhält seriell seine Daten. Dieser setzt die Daten nach der Frequenzumtast-Methode (FSK = frequency shift keying) um. Die Übertragungsrate liegt bei 300 bit pro Sekunde. Mit dem Koppler werden Daten auf das Telefon gegeben und empfangen. Die Übertragung ist kompatibel zu der Bell-Serie 103 und 113.\n\nBei der Übertragung eines 1-Signales, einer Marke (mark), ertönt ein 1270-Hz-Ton und bei einem 0-Signal, einem Leerzeichen (space), ein 1070-Hz-Ton. Die Antwort beträgt 2225 Hz bei einem 1-Signal und 2025 bei einem 0-Signal.\n\nAls Kontrollfunktionen steht „FULL“ für Voll-Duplex, „HALF“ für Halb-Duplex, „TEST“ für den Tontest, „ANS“ für den Antwort-Betrieb, „OFF“ für das MODEM-Ein und „ORIG“ für den Ursprung-Betrieb.\n\n\nAn der Vorderseite dieses Gerätes befinden sich zwei Verbindungsbuchsen (I/O-Connectors), mit denen der Computer zu verbinden ist. An der Rückseite stehen dann dem Anwender vier serielle Schnittstellen zur Verfügung. Auf der rechten Seite ist eine parallele Schnittstelle.\n\nDie seriellen Schnittstellen entsprechen der EIA-RS-232C-Norm. Dadurch können bis zu vier Geräte parallel betrieben werden. Dies entspricht vier 20 mA-Stromschleifen. Die parallele Schnittstelle ist 8 Bit breit und entspricht den Centronics-Typ. Hier wird auch der Drucker 825 angeschlossen.\n\nDurch dieses Schnittstellenmodul ist ein Voll-Duplex-Betrieb möglich. Die Baudraten sind programmierbar und liegen bei 75, 110, 134,5, 150, 300, 600, 1200, 1800, 2400, 4800 und 9600 Bit pro Sekunde. Die Baudot-Geschwindigkeit ist 60, 66, 75 und 100 Worte pro Minute.\n\nBei der seriellen Übertragung von Daten ergibt sich ein standardmäßig asynchrones Start/Stopp-Bit. Die Übertragung ist ASCII-Standard und die Baudot-Unterstützung für RTTY (radio teletype) ausgelegt.\n\nDie Stopp-Bit sind programmierbar, entweder ein oder zwei Bit. Durch eine Prüfbitsummenbildung mit einer geraden oder ungeraden Parität wird die Datenübertragung auf Wunsch noch sicherer. Die maximale Übertragungslänge auf Kabeln liegt bei etwa 15 Metern.\n\n\n\n\n\nAls Antwort auf die rasch wachsende Konkurrenz im Heimcomputerbereich insbesondere durch den Commodore 64 und die Spectrum-Computer brachte Atari 1982 das erste Modell der XL-Baureihe (aus dem englischen „e\"X\"tended \"L\"ine“ gebildetes Akronym) auf den Markt: den Atari 1200XL. Dieses Modell wurde wegen Kompatibilitätsproblemen mit der alten Baureihe alsbald wieder vom Markt genommen und 1983 durch die Modelle 600XL (16 kByte RAM) und 800XL (64 kByte RAM) ersetzt.\n\nDie elektronische Architektur der XL-Computer unterscheidet sich nur geringfügig von der der 400/800-Baureihe. Es kamen lediglich einige Veränderungen in der Speicherverwaltung (OS ausblendbar), die nun ab Werk eingebaute Programmiersprache Atari-BASIC und ein nach außen geführtes Parallel Bus Interface (PBI), um Erweiterungen effizienter anbinden zu können, hinzu. Zudem wurde die Anzahl gleichzeitig anschließbarer Joysticks auf nur noch zwei beschränkt. Die XL-Computer und deren Peripherie weisen deutlich verschlankte und weniger verspielte Gehäuse auf. \n\nDie Entwicklung der XL-Baureihe wurde bis 1984 weitergeführt. Während dieser Zeit entstand eine Reihe von Konzeptstudien und Prototypen (die Bürocomputer 1400XL und 1450XLD, das Erweiterungsmodul 1090XL, diverse Diskettenstationen auch für Disketten im 3½\"-Format, CP/M-Module usw.), die sich durch Details wie integrierte elektronische Spracherzeugung oder auch für damalige Verhältnisse sehr schnelle Modems auszeichneten. Diese und weitere Entwicklungsprojekte wie der 1650XLD (mit 80186-Emulation), der 1850XLD (Codename „Lorraine“; in Zusammenarbeit mit Amiga Inc., die wenig später von Commodore gekauft wurde) und der über einen Musiksynthesizer verfügende 900XLA brachten es aufgrund von Änderungen in Ataris Entwicklungsphilosophie nie zur Marktreife. \n\n\n\n\n\n\nAus der Atari, Inc. ging nach dem Video Game Crash 1984 die Heimcomputersparte als Atari Corporation hervor. Im Rahmen der Einführung der ST-Baureihe wurde 1985 auch die 6502-CPU-basierte Produktpalette aufgefrischt – Atari lieferte nun die XE-Modelle (Akronym aus e\"X\"tended line \"E\"nhanced\"), die mit einer moderneren, an der ST-Baureihe orientierten Gestaltung der Gehäuse aufwarten konnten. Die technischen Änderungen gegenüber der XL-Baureihe beschränkten sich im Wesentlichen auf einen weiteren Ausbau der RAM-Minimalkonfiguration (64 kByte beim 65XE und 800XE; 128 kByte beim 130XE) und einige Veränderungen zur Senkung der Produktionskosten. Dabei wurde der nach außen geführte Parallelbus durch einen schlankeren, aber nicht kompatiblen Expansionsport ersetzt und qualitativ minderwertige Tastaturen eingebaut. Dies erschwerte den effizienten Einsatz im Heimanwenderbereich und führte zur Abwanderung einer großen Kundengruppe hin zur ST-Baureihe oder zur Konkurrenz. Dem Kostendruck zum Opfer fielen auch vielversprechende Projekte wie eine portable Version des 65XE (65XEP) sowie eine mit dem Synthesizer-Spezialbaustein \"AMY\" bestückte Variante (65XEM), weiterhin Peripheriegeräte wie 3½\"-Diskettenlaufwerke (XF351), Monitore (XC1411) und Farbdrucker (XTM201, XTC201).\n\nEbenfalls zur Produktpalette der XE-Baureihe ist das durch eine externe Tastatureinheit zu einem vollwertigen XE-Computer erweiterbare Game-System (XEGS) zu zählen. Das XEGS gilt als Versuch Ataris, Nintendos NES und Segas Master System Marktanteile abzunehmen.\n\nDie allgemein nachlassende Nachfrage im Bereich des 8-Bit-Computersegments in den USA, schlechte Bewerbung der Produkte in Europa, fehlende leistungsstarke Peripherie, schlechte Verarbeitung und mangelnde Unterstützung durch Fremdhersteller – insbesondere im Spielebereich – führte zu im Vergleich zur 400/800- und XL-Baureihe geringen Verkaufszahlen, wobei der für den damaligen Ostblock produzierte Atari 800XE eine Ausnahme bildete. Die Produktion der XE-Baureihe wurde im Jahr 1992 eingestellt. \n\n\n\n\n\nAuf modernen Computern kann vorhandene Software in Form von Disketten-, Kassetten- und Modulabbildern sowie Atari-Executables auf Emulatoren wie dem Atari800 ausgeführt werden.\n\n\n"}
{"id": "926247", "url": "https://de.wikipedia.org/wiki?curid=926247", "title": "Texas Instruments TI-99/4", "text": "Texas Instruments TI-99/4\n\nBeim Texas Instruments TI-99/4 (kurz TI-99/4) handelt es sich um einen Heimcomputer des US-amerikanischen Technologiekonzerns und Ende der 1970er Jahre weltweit führenden Halbleiterherstellers Texas Instruments (TI). Der mit einem für damalige Verhältnisse sehr leistungsstarken 16-Bit-Hauptprozessor, 16 Kilobyte Arbeitsspeicher (RAM), 31 Kilobyte Festspeicher (ROM) sowie Spezialbausteinen für die Bild- und Tonausgabe ausgestattete Rechner wurde im Juni 1979 auf der Summer Consumer Electronics Show in Chicago erstmals der Weltöffentlichkeit vorgestellt.\n\nDer TI-99/4 war aufgrund von Problemen bei der Herstellung sowie der Funkentstörung erst ab Anfang 1980 auf dem nordamerikanischen Markt erhältlich. In Europa gelangte der Rechner erst in der zweiten Hälfte des Jahres 1980 zur Marktreife und war zunächst ausschließlich in einer für die dort üblichen PAL-Fernsehgeräte untauglichen NTSC-Version zu erwerben. Später wurde auch eine PAL-fähige Version in Europa angeboten. Durch die späte Markteinführung der PAL-Version des TI-99/4 hat das Nachfolgemodell TI-99/4A einen erheblich höheren Bekanntheits- und Verbreitungsgrad erlangt als der TI-99/4.\n\nDie technikgeschichtliche Relevanz des TI-99/4 besteht in der Tatsache, dass es sich um den ersten für Privatanwender konzipierten 16-Bit-Heimcomputer handelt. Aufgrund des hohen Preises sowie einiger technischer Defizite verkaufte sich der Rechner aber nur schleppend. Insgesamt wurden ca. 20.000 Einheiten des ersten Heimcomputers von Texas Instruments verkauft. Im Sommer 1981 wurde der TI-99/4 durch eine technisch modifizierte, leistungsstärkere Version mit der Modellbezeichnung TI-99/4A abgelöst.\n\nZwischen dem TI-99/4 und seinem Nachfolger TI-99/4A gab es im Wesentlichen folgende Unterschiede:\n\nAnsonsten waren der TI-99/4 und TI-99/4A weitgehend identisch aufgebaut, so dass sich weitere Details zum TI-99/4 dem Artikel zum TI-99/4A entnehmen lassen.\n\nTexas Instruments produzierte selbst ab 1980 eine ganze Reihe von Peripheriegeräten, die für den TI-99/4 gedacht waren, aber auch an das Nachfolgemodell angeschlossen werden können. Diese sog. \"Sidecars\" besitzen ein eigenes externes Netzteil, um Hitzeentwicklung im Innern des Gehäuses und eine Überlastung der Stromversorgung des Rechners zu vermeiden. Ausnahmen sind das Modem für den Austausch von Daten mit anderen Computersystemen sowie der \"Solid State Speech Synthesizer\". Darüber hinaus verfügen sämtliche Peripheriegeräte der ersten Generation über einen durchgeschleiften Platinenstecker, sodass bis zu sechs \"Sidecars\" im Daisy-Chain-Verfahren gleichzeitig an die Konsole angeschlossen werden können.\n\nDie folgende Auflistung gibt einen Überblick über die von Texas Instruments angebotenen Peripheriegeräte, deren Design an das äußere Erscheinungsbild des TI-99/4 angepasst wurde:\n\n\nDas PHP1850 ist in einem rechteckigen Gehäuse aus schwarzem Kunststoff untergebracht, das hinten mit einem Anschluss für das Verbindungskabel und Lüftungsschlitzen zur Kühlung der Elektronik ausgestattet ist. Das Laufwerk verfügt auf der Vorderseite über einen Klappverschluss zwecks Fixierung der eingelegten Disketten. Es verwendet den Floppy-Disk-Controller FD1771 von Western Digital und besitzt einen magnetischen Schreib-Lese-Kopf mit einer mittleren Zugriffszeit von 463 ms. Das Laufwerk verwendet handelsübliche 5¼-Zoll-Disketten mit maximal 40 Spuren. Es gestattet das Abspeichern eines Datenvolumens von bis zu 89 KB auf einer Diskettenseite in einfacher Dichte bei variabler Sektorengröße.\n\nDas PHP1850 lässt sich nur in Verbindung mit einer Steuereinheit des Typs PHP1800 betreiben, die einerseits die Positionierung des Schreib-Lese-Kopfes regelt, andererseits das Inhaltsverzeichnis der Diskette verwaltet. Darüber hinaus wird das \"Disk Manager\"-Steckmodul benötigt, auf welchem sich das Diskettenbetriebssystem TI-DOS befindet.\n\nDer Neupreis des PHP1850 lag 1982 bei 499,95 US-Dollar, während für den PHP1800 \"Disk Drive Controller\" nebst dem im Lieferumfang enthaltenen \"Disk Manager\"-Steckmodul weitere 299,95 US-Dollar aufgebracht werden mussten.\n\n"}
{"id": "926349", "url": "https://de.wikipedia.org/wiki?curid=926349", "title": "C-One", "text": "C-One\n\nDer C-One ist ein Nachbau des 8-Bit-Heimcomputers C64 von Commodore aus den 1980er Jahren. Die Amerikanerin Jeri Ellsworth begann im Jahr 2000 an einem kompletten Re-Design des C64, die in Zusammenarbeit mit \"Individual Computers\", 2003 in der Auslieferung der ersten C-One-Boards im ATX-Format mündete. Mittlerweile ist Jeri Ellsworth nicht mehr in die Entwicklung involviert.\n\nFür das Design finden Standardbauteile Verwendung. Als Prozessor kommt eine zum MOS 6502 kompatible 65C816-CPU mit einer Taktfrequenz von etwa 20 MHz, 24-Bit-Adressraum und einem erweiterten Befehlssatz für den Zugriff auf Speicher oberhalb von 64 KB zum Einsatz. Für zeitrichtiges Ausführen alter C64-Software sorgt eine programmierte Bremse. Der C-One hat außerdem einen Prozessor-Slot, über den mittels einer Adapterplatine fast jeder beliebige 8-Bit-Prozessor nachgerüstet werden kann, so zum Beispiel ein echter MOS 6502 oder ein Zilog Z80.\n\nDer C-One gilt als erster rekonfigurierbarer Computer: Die Funktionen der Custom-Chips des C64 sind im C-One nicht festverdrahtet (\"hardwired\"), sondern können umprogrammiert (rekonfiguriert) werden. Dafür verwendet der C-One zwei Field Programmable Gate Arrays (FPGA). Somit kann der C-One auch andere 8-Bit-Computer wie etwa den Schneider/Amstrad CPC hardwaretechnisch nachahmen. Inzwischen existiert auch das Projekt \"NatAmi\", in dessen erstem Prototyp ein C-One verwendet wird, um einen Amiga mit 32-Bit-CPU 68060 nachzubilden. In diesem Projekt ist aber für später die Entwicklung eigener Hardware geplant.\n\n"}
{"id": "927080", "url": "https://de.wikipedia.org/wiki?curid=927080", "title": "Microsoft Management Console", "text": "Microsoft Management Console\n\nDie Microsoft Management Console (MMC) ist eine grafische Benutzeroberfläche zur Verwaltung von Computern unter Microsoft Windows. Die MMC selbst führt keine Verwaltungsaufgaben aus, sondern fasst Verwaltungsprogramme zusammen. Ein solches Programm für die MMC wird \"Snap-In\" genannt und trägt die Dateinamenserweiterung \"msc\". \n\nMit der MMC werden die meisten Werkzeuge ausgeführt, die der Benutzer unterhalb des Systemsteuerungseintrags „Verwaltung“ findet, darunter die Computerverwaltung mit dem Gerätemanager und der Datenträgerverwaltung sowie die Dienststeuerung. Die Hauptansicht der Management Console zeigt links einen Baum, in dem das gewünschte Snap-in ausgewählt wird. Rechts erscheinen dann die zugehörigen Optionen. \n\nDie MMC wurde Ende 1997 mit dem \"Option Pack\" für Windows NT 4.0 eingeführt. Version 1.2 ist Bestandteil von Windows 2000. Sie ist auch unter Windows 9x nachrüstbar. Mit Windows XP folgte Version 2.0 und mit Windows Vista Version 3.0 der MMC. Version 3.0 ist auch im Service Pack 3 für Windows XP enthalten. Alle neueren Versionen des Betriebssystems beinhalten ebenfalls diese Version der MMC.\n"}
{"id": "927110", "url": "https://de.wikipedia.org/wiki?curid=927110", "title": "Windows Management Instrumentation", "text": "Windows Management Instrumentation\n\nWindows Management Instrumentation (WMI) ist Microsofts Implementierung – und Erweiterung – des Common Information Models (CIM), einer Kernfunktionalität des standardisierten Web Based Enterprise Managements (WBEM), für Windows.\n\nÜber WMI kann lesend und schreibend, lokal oder vom Netzwerk aus, auf nahezu alle Einstellungen eines Windows-Computers zugegriffen werden. WMI ist daher unter Windows eine der wichtigsten Schnittstellen für die Administration und Fernwartung von Workstations und Servern mittels Skriptsprachen wie Windows PowerShell und VBScript.\n\nEine weitere Möglichkeit, WMI interaktiv oder innerhalb von Batch-Dateien zu verwenden, ist das Microsoft Kommandozeilenprogramm \"Windows Management Instrumentation Command-line\" (WMIC).\n\nBasierend auf COM und DCOM ist WMI integraler Bestandteil von Windows 2000, Windows XP, Windows Server 2003 und allen Nachfolgeversionen. Für Windows 9x und NT4 sind Add-ons erhältlich. Die Windows Firewall funktioniert nicht bei nicht laufendem WMI-Dienst (Winmgmt.exe). Erst ab Version NT 5.1 (XP, 2003, …) erzwingt Windows ein Kennwort für die Anmeldung auf dem DCOM-Port (135/TCP) zur Nutzung von WMI.\n\nBei der Abfrage von WMI über das Netzwerk werden Daten über den dynamischen Portbereich 1025-5000 (Windows 2000, Windows XP und Windows Server 2003) bzw. 49152-65535 (Windows Server 2008, Windows Vista und höhere Versionen) zurückgegeben, es sei denn, dass ein statischer WMI-Port eingerichtet wurde.\n\n\n"}
{"id": "929504", "url": "https://de.wikipedia.org/wiki?curid=929504", "title": "Xbox-Linux", "text": "Xbox-Linux\n\nXbox-Linux ist eine Linux-Distribution für die Spielkonsole Xbox. Der Begriff wird auch als Oberbegriff für diverse weitere auf der Xbox lauffähigen Linux-Distributionen verwendet. Die entsprechenden Distributionen wurden an die speziellen Hardwarebedingungen der Spielkonsole Xbox angepasst. Entsprechend angepasste Linux-Distributionen können nach geringfügigen Modifikationen auf jeder Xbox installiert werden.\n\nEnde 2001 von der deutschen Entwicklergruppe H Zero Seven als Projekt gestartet, konnte Mitte 2002 trotz TCPA-ähnlicher Sicherheitsarchitektur das Xbox-Linux mit Hilfe eines Xbox-Modchips auf der Xbox zum Laufen gebracht werden. Hierdurch wurde aus der Spielekonsole ein vollwertiger Computer. Durch eine Sicherheitslücke ist seit August 2003 sogar ein sogenannter Softmod möglich, also das Benutzen des Betriebssystems ohne einen solchen Chip. Der Softmod betrifft sowohl eine Modifikation der Festplatte als auch der Flash-Speicher, der überschrieben werden kann.\n\nZu einem Medienwirbel kam es, als 2002 Michael Robertson, der Gründer von \"mp3.com\" und \"Lindows.com\", demjenigen 200.000 US-$ versprach, der als erster Linux auf der Xbox zum laufen bekäme, woraufhin die ursprünglichen Gründer wegen des kommerziellen Hintergrundes und diverser GPL-Verletzungen durch Robertsons Firma das Projekt verließen. Der Co-Gründer Michael Steil führte das Projekt bis zum endgültigen Erfolg weiter.\n\nMit Hilfe von USB-Adaptern werden Tastatur und Maus angeschlossen.\n\nUnter Verwendung des Programms Wine können Microsoft Office sowie andere Windows-Programme genutzt werden. Mit dem Emulationsprogramm VMware ist es möglich, Windowsversionen auf der Xbox laufen zu lassen. Die Xbox mit 128 MB Arbeitsspeicher ist hierbei im Vorteil gegenüber der 64-MB-Version.\nDa die Hardware der Xbox mit der eines PC identisch ist, lässt sich die Xbox-Hardware in gewissem Maße aufrüsten. Besonders oft wird die vorhandene Festplatte gegen eine größere ausgetauscht.\n\n\"Gentoox\" ist eine speziell für die Xbox angepasste Version von Gentoo. Im Gegensatz zu der normalen Installation von Gentoo, die etwas fundiertere Kenntnisse über Linux erfordert, kann Gentoox sogar \"out of the box\" installiert werden. Das erspart eine Menge Zeit, da die Pakete nicht mehr kompiliert und konfiguriert, sondern eigentlich nur noch übertragen werden müssen.\n\n\"Gentoo for Xbox\" wurde im Gegensatz zu Gentoox nicht speziell angepasst, sondern enthält nur ein erweitertes Profil, welches auf die Hardwareanforderungen der Xbox zugeschnitten ist. Die Pakete werden von der Xbox kompiliert und die Konfiguration der Anwendungen erfolgt wie bei einem Gentoo auf dem PC.\n\n\"Xebian\" ist eine spezielle Debian-Version für die Xbox. Es kann auf eine freie Partition oder auf die Partition der Spielstände installiert werden. Xebian kann auch als Live-CD/DVD eingesetzt werden.\n\nDie Abwandlung von Ubuntu für die Xbox heißt XUbuntu. Dieser Name wird inzwischen aber auch für Ubuntu mit XFCE verwendet.\n\nDarüber hinaus gibt es angepasste Versionen von Fedora Core sowie Slackware und vielen weiteren.\n\nAuf dem 22C3 des CCC wurde ein Vortrag über die Sicherheit der Xbox und der Xbox 360 gehalten. Unter anderem wurden 17 Sicherheitsmängel entdeckt, die es erst ermöglicht haben, Linux auf der Xbox zu installieren.\n\n"}
{"id": "929814", "url": "https://de.wikipedia.org/wiki?curid=929814", "title": "Cinelerra", "text": "Cinelerra\n\nCinelerra ist ein freies, nichtlineares Videobearbeitungsprogramm für das Betriebssystem GNU/Linux. Es wird von Heroine Virtual Ltd. entwickelt und wird unter der GNU General Public License veröffentlicht. Cinelerra beinhaltet auch eine \"Video-Compositing Engine\", die dem Benutzer die grundlegenden Compositing-Funktionen wie Keying und Maskieren erlaubt.\n\nCinelerra wurde am 1. August 2002 veröffentlicht und basierte zu dem Zeitpunkt auf dem Vorgängerprodukt \"Broadcast 2000\".\n\n\nAlle Merkmale sind auf der offiziellen Webseite aufgelistet.\n\nGerade beim Umgang mit HD-Material, bei Anwendung mehrerer Effekte auf Video- und Audiospuren, sowie für das Rendering des fertigen Films ist entsprechend leistungsfähige Hardware erforderlich.\n\nCinelerras Benutzeroberfläche gleicht der von anderen nichtlinearen Videobearbeitungssystemen wie Adobe Premiere Pro oder Apples Final Cut; da Cinelerra gleichzeitig aber auch ein Video-Compositing-Programm ist, gleicht es auch Adobe After Effects oder Apple Shake.\n\nDem Benutzer werden vier Arbeitsbereiche zur Verfügung gestellt:\nCinelerra ist im Umfeld professioneller Linux-Videoproduktion entstanden und wird dort genutzt. Unterstützung gibt es dabei auch von Firmen wie \"Linux Media Arts\", die sich auf den Bereich spezialisiert haben.\n\nCinelerra kann jedoch ohne Einschränkung auch für den privaten oder heimischen Videoschnitt eingesetzt werden.\n\nCinelerra wurde auf der NAB Show der National Association of Broadcasters 2004 gezeigt, der weltgrößten Messe für elektronische Medien, und wurde zu diesem Anlass vom (heute nicht mehr existierenden) Online-Magazin The Cut neben anderen Videobearbeitungsprogrammen mit dem \"Bob Turner’s „Making THE CUT Award“\" ausgezeichnet. Die Auszeichnung wurde „den besten und erstaunlichsten Postproduktions-Programmen verliehen, die auf der Ausstellung zu sehen waren“.\n\nDie freie Lizenzierung von Cinelerra hat es immer wieder einigen Projekten ermöglicht, eigene Projekte zu forken, die auf dem Code von Cinelerra aufbauen. Das bekannteste davon ist \"Cinelerra CV\", das die Zielsetzung verfolgt, eine mehr auf den Normalverbraucher ausgerichtete Version des Programms bereitzustellen.\n\nCinelerra CV hat die letzte Version 2.0 übernommen, verfügt aber im Vergleich zu ihr über einige Verbesserungen. Dazu gehört das Editieren von DV-Rohdaten, wie sie von dvgrab --format raw ausgegeben werden. DV kann zwar auch von Cinelerra 2.0 gelesen werden, jedoch nur nach Umpacken des Datenstroms in ein DV-AVI mittels: dvgrab --stdin --format dv2 < raw.dv\nAuch wurde Cinelerra CV hinsichtlich der Performance verbessert, um die ruckelfreie Wiedergabe von DV-Material beim Editieren zu verbessern und das Abreißen der Audioausgabe zu minimieren.\n\nUrsprünglich als Code-Rewrite von Cinelerra CV entstanden, ist mit Lumiera eine weitere Abspaltung entstanden, die allerdings noch nicht benutzbar ist.\n\n"}
{"id": "930281", "url": "https://de.wikipedia.org/wiki?curid=930281", "title": "Kuckucksei (Clifford Stoll)", "text": "Kuckucksei (Clifford Stoll)\n\nKuckucksei. Die Jagd auf die deutschen Hacker, die das Pentagon knackten. ist ein 1989 erschienenes Sachbuch von Clifford Stoll. Es handelt von seiner Jagd auf den Hacker Markus Hess, der im Rahmen des KGB-Hack von Hannover aus in Militärcomputer in den USA einbrach.\n\nDer amerikanische Original-Titel ist \"The Cuckoo’s Egg\". Die deutsche Übersetzung von Gabriele Herbst wurde 1989 im Krüger-Verlag veröffentlicht (heute Imprint der Verlagsgruppe S. Fischer Verlag). Mittlerweile sind mehrere Auflagen gedruckt worden – auch aktualisierte Neuausgaben mit einem aktuellen Nachwort von Clifford Stoll.\n\nClifford Stoll arbeitet am Lawrence Berkeley National Laboratory (LBNL) als Astronom, wird jedoch in Ermangelung von Arbeit in die Computerabteilung versetzt, wo er Programme für seine ehemaligen Kollegen schreiben soll. Als ein Abrechnungsfehler von 75 Cent festgestellt wird, soll Stoll das aufklären, um sich in die Materie einzuarbeiten. Tatsächlich schafft Stoll es, einen Hacker im Netz des LBNL aufzuspüren und dessen Sitzungen jeweils mittels eines Druckers zu protokollieren. So wird er Zeuge erfolgreicher und erfolgloser Computereinbrüche in zahlreiche Militärcomputer. Da das FBI kein Interesse an dem Fall hat, schaltet Stoll die CIA ein, die jedoch nicht zuständig ist, und auch die NSA zeigt sich offiziell nur mäßig interessiert.\n\nAls Stoll klar wird, dass er die Verbindung des Hackers zurückverfolgen kann, startet er \"Operation Showerhead:\" Das LBNL ist angeblich verantwortlich für \"SDINET,\" ein Netzwerk über die Strategic Defense Initiative, nach welcher der Hacker häufig sucht. Stoll legt riesige Dateien an, die sich der Hacker regelmäßig herunterlädt. Diese Langzeitverbindungen verfolgt Stoll mit Hilfe von Steve White zurück, einem Mitarbeiter von \"Tymnet\", einem Unternehmen, über dessen Leitungen der Hacker den Atlantik überquert. In Deutschland hilft Wolfgang Hoffmann von der Deutschen Bundespost bei der Verfolgung. Ein großes Problem ist dabei die relativ alte Vermittlungstechnik in Deutschland. Weil in den USA die meisten Vermittlungsstellen bereits digitalisiert sind, kann dort eine „Malicious Call Identification“ in nur wenigen Sekunden einen Anrufer ermitteln. In Deutschland jedoch muss noch eine spezielle analoge Fangschaltung in der betreffenden Vermittlungsstelle eingerichtet werden. Die Ermittlung des Anrufers dauert so viele Minuten, da die Ursprungsschaltung einmal durch die gesamte Vermittlungsstelle von einem Techniker durchgemessen werden muss. Erst nachdem Clifford Stoll von ihm manipulierte Dateien mit großem Datenvolumen bereitstellt, welche aus bürokratischen Anordnungen seiner Universität bestehen in denen er die akademischen Titel bzw. Anreden in militärische umgetauscht hat (Dr. wird zu Colonel usw.), reicht die Zeit für die Bundespost aus, um die Schaltung zurückzuverfolgen und den Anrufer so zu ermitteln.\n\nDer Titel des Buches rührt aus der an ein Kuckucksei erinnernden Tatsache, dass der Hacker, der auf verschiedenen Rechnern Zugriff auf Benutzerkonten durch das systematische Erraten von Passwörtern erlangt, mit einem Trick Superuser-Rechte auf Root-Ebene erhält. Er nutzt einen Konfigurationsfehler im Programm Emacs und ersetzte damit kurzfristig ein Systemprogramm, welches in regelmäßigen Abständen bestimmte Dateien verarbeitet. Dieses Programm änderte er so ab, dass er Root-Rechte erlangt, sobald die Datei erneut verarbeitet wird. Diesen Vorgang beschreibt Stoll mit „Ausbrüten des Kuckuckseis“.\n\n\n\n\n"}
{"id": "930803", "url": "https://de.wikipedia.org/wiki?curid=930803", "title": "Subpixel-Rendering", "text": "Subpixel-Rendering\n\nSubpixel-Rendering (engl.) bedeutet übersetzt \"Teilbildpunktwiedergabe\" und steht für Algorithmen, die Subpixel eines Farbbildschirmen zusätzlich zur Erhöhung der dargestellten Auflösung von Text und Bildern benutzen. Da Subpixel meist horizontal angeordnet sind, wird die horizontale Auflösung erhöht.\n\nDie Verbesserung in der Darstellung ist am deutlichsten auf Bildschirmen, bei denen die Subpixel deutlich getrennt sind, wie es bei LCD- oder AMOLED-Bildschirmen der Fall ist. Je nach Qualität des Geräts kann aber auch die Lesbarkeit bei Röhrenbildschirmen erhöht werden.\n\nDie Technik wurde im Jahre 1988 von IBM entwickelt, um die Schriftdarstellung auf Farb-LCDs wie Laptop-, Mobilfunk- oder Flachbildschirmen zu verbessern.\n\nIn größerem Stil wurde das Verfahren von Apple bei Mac OS 9 und von Microsoft bei Windows XP als ClearType angeboten. Bei beiden Betriebssystemen war es in der Grundeinstellung jedoch nicht aktiviert. Dies änderte sich erst mit macOS und Windows Vista. Auch das bei BSD-Systemen und Linux verbreitete XFree bzw. dessen Folgeprodukte ermöglichen Subpixel-Rendering.\n\n Durch Ansteuerung eines \"einzelnen Subpixels\" anstelle eines \"vollständigen\" (d. h. aus mehreren Subpixeln bestehenden) Pixels erhöht sich die nutzbare Auflösung des Bildschirms in einer Richtung um den \"Faktor drei\". Die Richtung ist dabei sowohl vom internen Aufbau des Bildschirms abhängig als auch von dessen Ausrichtung, etwa bei Nutzung der Pivot-Funktion. Die höhere Auflösung erlaubt prinzipiell eine feinere Darstellung von Details.\n\nDie Nutzung von Subpixel-Rendering wird aber stets mit \"Farbsäumen\" erkauft, denn bei der additiven Farbmischung werden pro Pixel (Bildpunkt) drei Subpixel in den \"Grundfarben\" \"Rot\", \"Grün\" und \"Blau\" benötigt, um die Farbe \"Weiß\" auf dem Bildschirm darzustellen. Dies ist bei Pixeln, die im Bereich des Übergangs zwischen Schrift und Hintergrund liegen, aber nicht immer gewährleistet, da deren Subpixel die Farbe Schwarz anzeigen, soweit sie in die Darstellung der Schriftzeichen einbezogen sind. Die Grundfarben der übrigen, außerhalb der Schriftzeichen liegenden Subpixel ergeben in ihrer Mischung dann andere Farben als Weiß.\nSofern Subpixel-Rendering mit \"Antialiasing\", d. h. \"Schriftglättung\", verbunden wird, tritt als weiterer Effekt eine \"Kontrastverminderung\" hinzu. Die Subpixel der Schriftzeichen werden dabei mit unterschiedlich hellen Subpixeln umgeben, so dass für das Auge die Pixeltreppen weiter aufgelöst werden.\n\nBeide Effekte können individuell als angenehm oder störend empfunden werden. Grundsätzlich basieren Subpixel-Rendering und vergleichbare Techniken auf der Tatsache, dass das menschliche Auge Helligkeitskontraste wesentlich besser wahrnehmen kann als Farbunterschiede.\n\nDas nebenstehende Bild macht Subpixel-Rendering im Vergleich zur konventionellen Schriftdarstellung deutlich. Oben wird das herkömmliche Schriftbild gezeigt, unten dasjenige mit ClearType. Links sieht man die normale Darstellung, in der Mitte und rechts die um ein Vielfaches vergrößerte. Hierbei kann der Bildschirm der beiden rechten Teilbilder Subpixel darstellen im Gegensatz zu dem der beiden mittleren.\n\nSubpixel-Verfahren wie ClearType müssen für jedes Anzeigegerät individuell abgestimmt werden. Insbesondere ist für die Berechnung der Farbabweichungen die Anordnung der Subpixel zueinander entscheidend. Bei den meisten Flachbildschirmen (LCD oder AMOLED) sind die Subpixel horizontal in der Reihenfolge RGB (Rot-Grün-Blau) angeordnet. In selteneren Fällen kommt die Reihenfolge BGR zur Anwendung. Es gibt auch Bildschirme, die gänzlich andere Anordnungen verwenden, wie z. B. vertikale oder oktogonale Anordnungen (ähnlich der Anordnung von Bildpunkten in den meisten Röhrenbildschirmen), mehr grüne als rote und blaue oder gar zusätzliche weiße Subpixel.\n\nDer für das Subpixel-Rendering verwendeten Software muss die Anordnung der Subpixel bekannt sein. Andernfalls werden die falschen Subpixel abgeschwächt, und die Kante erscheint unschärfer als zuvor. Außerdem sind manche Monitore mit Streufolien ausgestattet, die die Helligkeit der Subpixel auf das ganze logische Pixel verteilen. Viele Desktop-Umgebungen bieten daher die Möglichkeit, dem System die tatsächliche physikalische Anordnung der Pixel mitzuteilen.\n\nUnter Windows kann Cleartype auf folgende Art und Weise konfiguriert werden:\n\nAnsonsten sind noch folgende Dinge zu beachten:\n\n\"ClearType Text Tuner\" stellt eine 9 Bildschirmpunkte hohe Schrift dar, die intern mit einer Auflösung von 1/6 Bildpunkt × 1 Bildpunkt gerendert wurde. Die Strichstärke beträgt horizontal 1 Bildpunkte, vertikal 1 1/3 Bildpunkte.\n\nDie Bedeutung der Dialoge bleibt etwas im Dunkeln, daher wird diese hier erläutert.\n\nSubpixel-Rendering wird von Mac OS 9 und macOS sowie diversen X-Window-Systemen, der Grafikumgebung für unixartige Systeme, unterstützt. Ab macOS 10.14 Mojave steht Subpixel-Rendering nicht mehr zur Verfügung, da die Rendering-Pipeline des Betriebssystems dies nicht mehr unterstützt. \n\nClearType ist für Microsoft Windows XP, Windows Server 2003, Windows Vista und Windows Mobile seit 2003 verfügbar.\n\nBeim Drehen von Bildschirmen von der üblichen Breitformatdarstellung in die Hochformatdarstellung ändert sich die Pixelanordnung. Windows XP,\nWindows Vista und Windows 7 berücksichtigen dies nicht und führen dadurch zu einer schlecht lesbaren, mit deutlichen Farbrändern versehenen Darstellung. Sie beherrschen nur horizontale RGB- und BGR-Anordnungen. Bei Windows Mobile wird Cleartype deaktiviert, wenn das Gerät gedreht wird.\nIn Internetforen und Blogs werden Nachteile von Subpixel-Rendering diskutiert. Webdesigner stellen fest, dass sich die Schrift in einem vorgegebenen Layout um einige Pixel verschieben und dann ein anderes Layout notwendig machen kann. Dieses Problem kann auch aus anderen Gründen auftreten – beispielsweise bei Verwendung einer anderen Schriftart, der Inanspruchnahme der Browser-Zoom-Funktion oder der Nutzung einer wesentlich anderen Bildschirmauflösung –, so dass als Ursache nicht das Subpixel-Rendering angesehen werden sollte, sondern der Wunsch der Webdesigner nach pixelgenauem Seitendesign. Dafür werden teilweise zusätzliche browserabhängige CSS-Stilbeschreibungen angelegt, die man mit neuen Webbrowsern und aktuellen W3C-Standards reduzieren wollte.\n\nBei senkrechten Kontrastlinien (z. B. „I“-Strich bei Schwarz-auf-Weiß-Darstellung) sind durch das subpixelspezifische Rendering von Helligkeitswerten manchmal deutliche Farbschatten zu erkennen, die sich ähnlich wie unsaubere Farbüberdrucke im Papierdruck bemerkbar machen. Das Problem tritt aber in etwas schwächerer Form auch ohne ClearType auf, denn Farben werden auf Flüssigkristallbildschirmen immer durch waagerecht nebeneinander liegende Farbpixel aufgelöst. Demzufolge befinden sich die einzelnen Subpixel an unterschiedlichen Orten. Die unscharfe Darstellung kann sich auf die Texterkennung von Bildschirmsoftware auswirken; so ist beispielsweise die Zeichenerkennung von Babylon Translator signifikant eingeschränkt. Dies ist allerdings ein generelles Problem bei jeder Form des Antialiasing.\n\nWeiterhin wird die Darstellung von vielen Benutzern als unscharf empfunden. Das unbewusste Bemühen des Auges, das künstlich unscharfe Schriftbild zu fokussieren, kann zu Ermüdungserscheinungen und Kopfschmerzen führen.\n\n"}
{"id": "931387", "url": "https://de.wikipedia.org/wiki?curid=931387", "title": "Krusader", "text": "Krusader\n\nDer Krusader ist ein Dateimanager mit vielen Extras für KDE. Er arbeitet mit einer zweispaltigen Ansicht nach Vorbild des Midnight Commanders (ähnlich dem Windows-Programm Total Commander). Krusader unterstützt Linux, Solaris und andere Unix-Systeme. Das Programm wird unter der GNU General Public License verteilt und ist bislang vom offiziellen Server über 811.000 mal heruntergeladen worden, dazu gehört es zum Paketumfang praktisch jeder namhaften Linux-Distribution.\n\nKrusader bietet alle üblichen Dateimanagerfunktionen und zeigt Ordnerinhalte als Dateiliste in zwei Spalten an. Jede Spaltenseite unterstützt dabei mehrere Tabs und kann Dateien als detaillierte oder mehrspaltige Kurzansicht darstellen. Es werden alle KIO-Protokolle (FTP, SMB, NFS usw.) unterstützt. Außerdem können Archive mit diversen Formaten (etwa tar, ZIP, bzip2, gzip, RAR, ACE, ARJ, LHA, RPM) transparent geöffnet und verändert werden. Eine Dateivorschau für Bilder, Dokumente, Textdateien, etc. kann im Hauptfenster angezeigt werden. Krusader enthält auch einen integrierten Betrachter und Editor. Dateioperationen (Kopieren, Verschieben) sind durch Abbrechen, Pausieren, Fortsetzen und Rückgängigmachung kontrollierbar.\n\nDarüber hinaus werden erweiterte Operationen für fortgeschrittene Benutzer unterstützt:\n\nDurch Definition von eigenen Aktionen (engl. User Actions) kann die Funktionalität des Dateimanagers zusätzlich erweitert werden.\n\nDas Projekt \"Krusader\" wurde im Mai 2000 von Shie Erlich und Rafi Yanai begründet, um eine Art Total Commander für Linux zu schaffen. Shie und Rafi entschieden sich dafür, das Programm mit dem Qt-Toolkit unter KDevelop zu schreiben. \"Krusader-M1\" (Milestone 1) war der erste Vorschauversion für KDE2 (Kleopatra 1.91). Mitte 2005 zählte das Krusader-Entwicklungsteam („Krusader Krew“) rund zehn Mitglieder. Mit der Veröffentlichung von Version 2.5 im Oktober 2016 wurde Krusader auf Qt 5 und KDE Frameworks 5 portiert.\n\n"}
{"id": "931679", "url": "https://de.wikipedia.org/wiki?curid=931679", "title": "Apple Network Server", "text": "Apple Network Server\n\nNetwork Server (Codename \"Shiner\") war eine Serie von Netzwerkservern der Firma Apple, auf Basis von PowerPC 604 bzw. 604e Prozessoren, die mit einer angepassten Version von IBMs Betriebssystem AIX betrieben wurden (das Betriebssystem gehörte nicht zum Lieferumfang, sondern musste zusätzlich erworben werden). Die Network Server verfügten über sechs PCI-Steckplätze, eine Netzwerkschnittstelle (10 Mbit/s) und drei SCSI-Kanäle (zwei intern, einer extern). Die Network Server konnten auch in ein Rack eingebaut werden.\n\nDie Network Server wurden 1996 vorgestellt und waren in drei Varianten erhältlich. Entweder mit 32 MB Arbeitsspeicher und PowerPC 604-Prozessor mit 132 (Network Server 500) oder 150 MHz (Network Server 700) oder mit 48 MB Arbeitsspeicher und PowerPC 604e mit 200 MHz (Network Server 700). Bei allen Modellen war der Ausbau auf bis zu 512 MB Arbeitsspeicher möglich. Ebenso waren alle Modelle mit einem CD-ROM-Laufwerk ausgestattet.\n\nDie Produktion der Network Server wurde 1997 eingestellt. Auf den Network Servern läuft neben dem \"AIX for Apple Network Server\" auch Linux/PPC.\n\n"}
{"id": "932163", "url": "https://de.wikipedia.org/wiki?curid=932163", "title": "Initialisierungsdatei", "text": "Initialisierungsdatei\n\nEine Initialisierungsdatei (kurz INI-Datei) ist eine Textdatei, die Schlüssel-Wert-Paare enthält, die ggf. durch Sektionen gegliedert werden. Initialisierungsdateien werden typischerweise von Microsoft-Windows-Anwendungen als Konfigurationsdatei genutzt. Bis zur Einführung der Registrierungsdatenbank mit Microsoft Windows NT 3.1 war das INI-Format das einzige Dateiformat zur Speicherung von Programm-Konfigurationen, das durch die WinAPI unterstützt wurde. Als einfach aufgebautes und zugleich von Menschen leicht lesbares Format ist es auch betriebssystemübergreifend verbreitet, um Einstellungen von Programmen dauerhaft zu speichern.\n\nDie Initialisierungsdaten werden zeilenweise abgelegt: ein Gleichheitszeichen trennt den Schlüssel von seinem Wert. \n\nSchlüssel=Wert\nUm Schlüssel in sog. Sektionen zu gruppieren, ist eine (pro Datei eindeutige) Bezeichnung in eckigen Klammern über der jeweiligen Sektion anzugeben. Benannte Sektionen sind nicht verpflichtend, je Sektion müssen die Schlüssel eindeutig sein. \n[Sektion1]\nSchlüssel=Wert\n[Sektion2]\nSchlüssel=Wert\nSchlüssel2=Wert\nAußerdem erlaubt das Dateiformat Kommentarzeilen, diese beginnen mit einem Semikolon.\nBeim Erstellen einer INI-Datei sind folgende Regeln zu beachten:\n\nEs gibt Dateien mit nahezu demselben Aufbau, die sich lediglich durch folgende Merkmale unterscheiden:\n\nEin Beispiel für eine solche Variation findet sich in den Konfigurationsdateien für Subversion-Repositories.\n\nIn früheren Windows-Ausgaben (mit der Einführung von NTFS) existierten zwei alternative/konkurrierende Philosophien für den Standard-Dateipfad zur INI-Ablage, wobei auf der einen Seite die Benutzerfreundlichkeit und auf der anderen Seite die Sicherheit im Vordergrund stand.\n\nIm Folgenden am Beispiel von Windows XP:\n\n\nAb Windows Vista wird die Veränderung des Programme-Ordners durch Benutzer mit eingeschränkten Rechten (oder \"Nicht-Administratoren\") standardmäßig von der Benutzerkontensteuerung blockiert, wovon auch dort gespeicherte INI-Dateien betroffen sind. Manche Programme vergeben daher bei der Installation sämtliche Rechte für die installierten Dateien an den Benutzer „Jeder“. In diesen Windows-Versionen existieren ggf. zudem automatisch erzeugte Kopien der INI-Dateien im Schattenverzeichnis codice_3.\n\nAnwendungen, die ihre Einstellungen in ihrem eigenen Programmordner ablegen (beispielsweise codice_4 oder codice_5), bieten den Vorteil, dass sie leichter auf einen anderen Rechner übertragbar sind, haben aber den Nachteil, dass sich alle Benutzer der Anwendung auf einem Rechner diese Einstellungen teilen. Solche Anwendungen werden auch als \"portabel\" bezeichnet. Voraussetzung zum Ändern dieser Einstellungen ist, dass der Benutzer Schreibrechte auf die INI-Datei (und den Programmordner) besitzt.\n\n"}
{"id": "934051", "url": "https://de.wikipedia.org/wiki?curid=934051", "title": "Cray-3", "text": "Cray-3\n\nDer Supercomputer Cray-3 ist Crays Nachfolger des Cray-2. Das System sollte die erste Anwendung von Galliumarsenid-Halbleitern (auch GaAs-Halbleiter genannt) in der elektronischen Datenverarbeitung sein, was bereits beim Vorgängermodell versucht worden war. Das Projekt wurde allerdings ein riesiger Flop, da der Cray-3 aufgrund seiner Galliumarsenid-Schaltungen sehr teuer war und deshalb nur ein Exemplar ausgeliefert wurde. \n\nGleich nach der Fertigstellung der Cray-2 machte sich Seymour Cray an die Entwicklung der Cray-3. Mit diesem Rechner verließ er Cray Research und gründete seine dritte Firma Cray Computer Corporation (kurz: CCC). Es war ein friedliches Auseinandergehen. Der Grund: Durch die zweijährige Verzögerung der Cray-2, die schon 1982 erscheinen sollte, war es nötig, einen Nachfolger der Cray-1 zu etablieren – schließlich bringt auch die Konkurrenz neue Modelle heraus. Die Wahl fiel auf eine Weiterentwicklung der Cray-1 auf 2 Prozessoren, die Cray X-MP. Es war möglich, diese weiter zu entwickeln (später entstand daraus die Cray Y-MP mit 4 oder 8 Prozessoren und die Cray T90 mit 16 Prozessoren), aber keine Firma kann es sich leisten, praktisch zwei Supercomputerlinien zu fahren: Solche Rechner werden einzeln verkauft und von einem Modell selten mehr als 100 Stück.\n\nDas Problem, das schon die Entwicklung der Cray-2 verzögerte, waren die Zykluszeiten: Langsam kam man an Grenzen, die von der Physik vorgegeben wurden. Das Licht und damit auch Taktsignale bewegen sich in einer Nanosekunde nur noch 30 cm weit. Transistoren schalten in Bereichen, die zwar noch unter 1 ns liegen, aber bei der Verarbeitung von Daten durchlaufen die Daten einige Dutzend Gatter, so dass sich die Schaltzeiten addieren.\n\nSeymour Cray sah die Lösung in einem anderen Halbleitermaterial: Galliumarsenid. Silizium ist zwar das allgemein eingesetzte Material und auch das preiswerteste. Doch es ist von den elektrischen Eigenschaften das schlechteste. Sowohl Germanium-Transistoren als auch Halbleiter aus Elementen der dritten und fünften Hauptgruppe wie Galliumarsenid schalten erheblich schneller, weil die Elektronen eine höhere Beweglichkeit haben. Germanium kommt aus preislichen und praktischen Erwägungen für hochintegrierte Schaltungen nicht in Frage, blieb noch Galliumarsenid.\n\nWas Seymour Cray unterschätzt hat, war die Komplexität der Aufgabe: Einen neuen Rechner zu entwickeln ist schon schwierig. Aber dazu noch neue Schaltungen auf der Basis eines nicht allgemein eingesetzten Materials zu konstruieren, war zu schwierig. Die Entwicklung verzögerte sich von 1988/1989 auf 1993. Abgesichert durch einen Auftrag und staatliche Unterstützung, konnte er mit einem Aufwand von 120 Mill. USD 1993 einen Prozessor der Cray-3 vorstellen und auch im NCAR installieren. Das gesamte System sollte aus 16 Prozessoren bestehen in einem nur 90 × 90 cm großen Kubus, 4 GB Speicher und direkten Verbindungen mit 8 GB/s zwischen den Prozessoren. Bei einer Zykluszeit von 2,11 ns erreichte ein Prozessor eine Peakperformance von 0,948 GFLOP, der gesamte Rechner 15,17 GFLOP. Die Leistungsaufnahme des kleinen Würfels betrug 88 kW.\n\nDoch nachdem die Cray-3 produktionsreif war, wollte sie niemand haben – sie war einfach zu teuer. Ähnlich erging es auch dem CM-2 von Convex, einem anderen Supercomputer-Hersteller, der Galliumarsenid einsetzte.\n\nCray Research hatte die Architektur der Cray-1 weiter entwickelt und mit der Cray C90 ein 16-Prozessorensystem mit einer Performance von 1 GFLOP pro Prozessor – bei nur 4,2 ns Zykluszeit und herkömmlicher Technologie. 1995 ging Crays Firma CCC in Konkurs. Sie hatte nicht ein Cray-3-System verkaufen können. In der Zwischenzeit arbeitete jedoch Seymour Cray schon an der Cray-4 – ebenfalls mit Galliumarsenid und 1 ns Zugriffszeit. Am 22. September 1996 starb er an den Folgen eines Verkehrsunfalls, bevor er die Cray-4 vollenden konnte.\n"}
{"id": "934075", "url": "https://de.wikipedia.org/wiki?curid=934075", "title": "Cray-4", "text": "Cray-4\n\nDie Cray-4 ist ein Supercomputer, der von der amerikanischen Firma Cray, unter der Leitung von Seymour Cray, konstruiert wurde, der allerdings vor Abschluss der Arbeiten bei einem Autounfall ums Leben kam. Die Cray-4 ist die Weiterentwicklung des kommerziellen Fehlschlags Cray-3, eines Supercomputers mit Gallium-Arsenid-Schaltungen. Der 1994 vorgestellte Supercomputer basiert auf einem Vektorprozessor mit Shared Memory.\n\nEin Standard-System mit 16 Prozessoren und 1.024 Megaword (8 GiByte) Speicher, das 32 Gigaflops leistete, wurde 1995 für 11 Mio. USD angeboten.\n"}
{"id": "936846", "url": "https://de.wikipedia.org/wiki?curid=936846", "title": "Flickerfixer", "text": "Flickerfixer\n\nEin Flickerfixer (engl. etwa \"Entflackerer\") ist ein Zusatzmodul für Amiga-Computer.\nEr verbessert die Bildqualität und die Monitoranschlussmöglichkeiten von Amiga-Computern, indem er eine flimmerfreie Darstellung von Bildschirmmodi, die das Zeilensprungverfahren (engl. \"Interlace\") verwenden, erreicht.\n\nAls Alternative zu RGB- oder Videomonitoren und TV-Geräten können an einen Flickerfixer VGA-kompatible Anzeigegeräte angeschlossen werden,\nsofern der Flickerfixer Ablenkfrequenzen liefert, die im zulässigen Frequenzbereich des verwendeten Monitors liegen. Hierfür verfügen viele Flickerfixer über einen eingebauten „Scandoubler“.\n\nInterne Geräte müssen als Hardware-Erweiterung oder als Steckkarte für den Video-Slot (je nach Amiga-Modell verfügbar) eingebaut werden und bieten eine bessere Bildqualität als externe Varianten. Externe Geräte haben den Vorteil, dass sie lediglich am RGB-Port angesteckt werden müssen, den jedes Amiga-Modell bereitstellt, führen aber intern eine erneute (verlustbehaftete) A/D-Wandlung des Videosignals durch. Ein Beispiel dafür ist der von Commodore selbst angebotene „Hedley-Monitor“ A2024, der ein 4-Bit-HiRes-Bild in ein 2-Bit-Monochrombild (4 Graustufen) mit verdoppelter Auflösung oder wahlweise ein Schwarz/weiß-Bild in vierfache Auflösung umwandeln kann.\n\nDer A3000(T) hat als Besonderheit einen Flickerfixer in Gestalt des \"Amber\"-Chips serienmäßig integriert. Für ältere Amigas (A500 & A2000) gab es z. B. Flickerfixer der „MultiVision“-Serie von 3-state und Produkte anderer Firmen, für den A2000 von Commodore selbst die Steckkarte A2320 mit dem genannten Amber-Chip. In Verbindung mit Grafikkarten existieren Add-on-Lösungen oder separate Geräte.\n\nEs gibt auch Geräte mit S-Video-Ausgang und dem eigentlich unnötigen Feature, alle AGA-Modi neben dem üblichen PAL oder NTSC entflimmern zu können (CompServ: AGA Flickerfixer Scandoubler II). Es existiert auch eine abgespeckte Version des „Flickermagic“ getauften Flickerfixers (DCE: Scanmagic), der nur die Scandoubler-Funktion bereitstellt.\n\nDie zwei wesentlichen Funktionen eines Flickerfixers sind wie folgt zu beschreiben:\n\nIm klassischen Fall (Amiga mit OCS-Chipsatz) wird die Frequenz des PAL-Modus von ca. 15,6 kHz auf ca. 31,2 kHz verdoppelt. Diese Funktion alleine ermöglicht schon den Betrieb eines Standard-VGA-Monitors am Amiga, weil ein VGA-Monitor mit den Amiga-typischen 15,6 kHz noch nicht arbeiten würde. Dabei muss aber auch die Vertikalfrequenz, bei PAL sind dies 50 Hz, vom Monitor unterstützt werden. CRT-Monitore machen dabei selten Probleme, aber etliche TFT-Monitore arbeiten erst ab ca. 60 Hz.\n\nEs ergibt sich dabei ein Effekt, der die schwarzen Zwischenzeilen des Videobildes in ihrer Höhe halbiert oder subjektiv sogar aufhebt: Ein Videobild wird auf digitaler Ebene aus Pixeln zusammengesetzt, diese erscheinen dann auf dem Monitor schön quadratisch, und die Pixelzeilen sind weniger streifig. Um eine Brücke zu modernen Grafikkarten zu schlagen, die (insbesondere in kleineren Bildauflösungen) den sog. Doublescan-Modus verwenden können, wird der beschriebene Effekt nochmals verdoppelt (dieser Modus könnte als das Gegenteil zum Interlaced-Modus angesehen werden).\n\nDas Problem ist folgendes: Um eine Verdopplung der vertikalen Auflösung zu erreichen (z. B. von 256 auf 512 Zeilen), muss beim Amiga der Interlaced-Modus verwendet werden. Dabei werden nur noch 2 mal 25 \"Halbbilder\" pro Sekunde ausgegeben (anstatt 1 Vollbild 50 mal pro Sekunde), die jeweils abwechselnd auf normaler Höhe und danach um eine \"Halbzeile\" vertikal verschoben sind. Die vertikale Auflösung wird verdoppelt, allerdings auf Kosten einer effektiv auf 25 Hz halbierten vertikalen Frequenz. Ein Amiga im Interlaced-Modus flimmert also kräftig. Bei Fernsehern ohne 100-Hertz-Technik (also mit ebenso niedriger Vertikalfrequenz) wird das Flimmern durch länger nachleuchtende Phosphore und andere technische analoge Zusammenhänge vermindert. Bei einem Bildschirm mit Kathodenstrahlröhre erscheint das Bild stabiler, je höher die Vertikalfrequenz ist, weil der Elektronenstrahl das Bild schneller aufbaut. Es gibt Studien, die bei CRTs eine als besonders angenehm empfundene Vertikalfrequenz ermittelt haben, die zwischen 100 und 130 Hz liegt. Bei TFT-Bildschirmen hat dieser Wert keine direkten analogen Auswirkungen mehr, sondern stellt einen Faktor der Taktfrequenz dar, mit der die interne Videohardware des TFT-Bildschirms arbeiten kann.\n\nZur Erklärung der Begriffe: Ein \"Halbbild\" ist ein in der vertikalen Auflösung halbiertes Vollbild, von denen es zwei Arten gibt: Das erste Halbbild enthält alle geraden Zeilen und das zweite Halbbild alle ungeraden Zeilen des Vollbildes. Wenn man sie nun per Zeilensprung-Verfahren nacheinander wiedergibt, erhält man wieder die komplette Auflösung des Vollbildes. Dabei wird die Trägheit des menschlichen Auges ausgenutzt, weil die Halbbilder in schneller Folge wiedergegeben werden. Eine \"Halbzeile\" zu Beginn des einen Halbbilds wird durch die Videohardware geschaffen und befindet sich zwischen zwei normalen (non-interlaced) Zeilen.\n\nNun kommt der Flickerfixer (Deinterlacer) ins Spiel: Der Flickerfixer „greift“ sich 2 Halbbilder und gibt diese gemeinsam aus. Dadurch wird das „Halbzeilen-Geflimmere“ gepuffert und man erhält eine flimmerfreie Darstellung.\n\nNatürlich hat dieses – „Weave-Deinterlacing“ (engl. \"verweben\") getaufte – Verfahren auch einen großen Nachteil:\nEs funktioniert nur bei Standbildern gut – bei bewegten Objekten entstehen sogenannte Ghost-Effects (auch Bewegungs- oder Kammartefakte genannt), d. h. ein Nachbild der vorherigen Position des bewegten Bildes oder Objektes bleibt sichtbar.\nDas Nachbild entsteht dadurch, dass die Halbbilder ursprünglich eine fünfzigstel Sekunde zeitversetzt generiert wurden, und nun als Paar gleichzeitig wiedergegeben werden.\n(Anmerkungen: Standbilder, die aus Bewegungssequenzen von Fernsehunternehmen erzeugt werden, zeigen oftmals nur ein Halbbild. Dabei wird jede Zeile des Halbbildes doppelt dargestellt , um das Problem mit den Ghost-Effects zu vermeiden. Auch Videorekorder zeigen beim Standbild nur die halbe vertikale Auflösung.\nBei progressiven Videoquellen werden nur noch Vollbilder verwendet, daher ist kein Deinterlacer mehr notwendig.)\n\nDer in der Einleitung erwähnte \"Scanmagic\" verzichtet ganz auf die Deinterlace-Funktion,\nwas den Vorteil hat, dass bewegte Bilder mit Interlace ohne Ghost-Effects auf VGA-kompatiblen Monitoren dargestellt werden können (dabei ist das Flimmern aber eben wieder vorhanden).\nMittlerweile gibt es auf Hardwareebene programmierbare Flickerfixer mit -Technologie (Indivision ECS/AGA), die dynamisch gespeist werden können und programmierbar sind, und dadurch für einen zweiten Bildschirm (stackable) oder zu einer Erweiterung des Farbraumes genutzt werden können.\n\nZu Zeiten des Amiga 500 war die Anschaffung eines Flickerfixers in Verbindung mit einem VGA-kompatiblen Monitor eine relativ teure, aber auch die einzige Möglichkeit, ein höherwertiges Computerbild zu erhalten.\n\nIm Amigabereich gab es einige Spezial-Entwicklungen bei Monitoren, um den möglichen Frequenzumfang des zuletzt entwickelten AGA-Chipsatzes voll auszunutzen. Mit der Einführung des ECS-Chipsatzes waren höhere Ablenkfrequenzen als bei PAL und NTSC möglich, was zum Kauf eines Mehrfrequenz-Monitors reizte. Diese Monitore unterstützen aber im Normalfall die für PAL und NTSC niedrige vertikale Ablenkfrequenz nicht und damit eine große Reihe Amiga-Software, die hauptsächlich auf PAL basiert. Daher ist die Verwendung eines Flickerfixers oder Scandoublers heute noch gängige Praxis, um diese Software auf dem Monitor wiederzugeben (dies sind meist Spiele und Demos – nicht Programme, die sich an die AmigaOS-Richtlinien zur Grafikprogrammierung halten). Viele der heutigen Amiga-User profitieren von der schnellen und hochauflösenden Grafik „echter“ Grafikkarten \"und\" der Möglichkeiten und der Vielfalt des nativen Amiga-Chipsatzes. Eine Grafikkarte, die PicassoIV von Village Tronic mit integriertem, programmierbarem Scandoubler/Flickerfixer, liest die native Amiga-Grafik ein und gibt sie in relativ frei konfigurierbaren Frequenzbereichen wieder aus. So ist die Darstellung alter Software sogar auf den meisten TFT-Monitoren möglich. Dabei ist zu beachten, dass sich die Vertikalfrequenz möglichst aus ganzen Teilern der ursprünglichen Vertikalfrequenz zusammensetzt, z. B. 75, 100 oder gar 150 Hz bei PAL, damit die Timings zusammenfallen und eine ruckfreie Darstellung gewährleistet ist.\n\nDie Entwicklung von Amiga-Flickerfixern ist auf dem Niveau von Weave-Deinterlacing (s. o.) stehen geblieben. Moderne Grafikkarten und Fernseher verwenden höherentwickelte Verfahren, um eine flimmerfreie und saubere Darstellung von (bewegten) Videobildern zu erreichen. Bei digital aufbereiteten Videobildern – etwa bei 100-Hertz-Fernsehern – wird u. a. das vorhergehende (ältere) Halbbild mit eingerechnet, was zu einer gegenüber Weave stark verbesserten Darstellung führt. Manche TV-Hersteller verwenden ganz spezielle Verfahren, um aus der veralteten Fernsehnorm ein Maximum herauszuholen.\nBei der Wiedergabe von Videodateien mit Zeilensprungverfahren auf dem Computer kann z. B. der VLC media player verwendet werden. Dieser unterstützt verschiedene Algorithmen für das Deinterlacing.\n\n"}
{"id": "937395", "url": "https://de.wikipedia.org/wiki?curid=937395", "title": "Windows Small Business Server 2003", "text": "Windows Small Business Server 2003\n\nMicrosoft Windows Small Business Server 2003 ist ein Netzwerkbetriebssystem des Softwareherstellers Microsoft. Es wurde speziell auf die Anforderungen kleiner Unternehmen abgestimmt. Durch die Kombination mehrerer Softwareprodukte zu einem vergleichsweise günstigen Preis sollen insbesondere kleine Unternehmen als Käufer angesprochen werden.\n\nMicrosoft Windows Small Business Server 2003 R2 wird in einer Standard- und einer Premium-Edition angeboten.\n\nDie \"Standard Edition\" enthält folgende Microsoft-Einzelprodukte:\n\nDie \"Premium Edition\" enthält alle Komponenten der Standard Edition und beinhaltet ab der Version R2 folgende weitere Technologien:\n\nEinige Microsoft-Serverprodukte wurden im ersten Halbjahr 2006 um einige Funktionen verfeinert. Diese tragen die Bezeichnung „R2“ hinter dem eigentlichen Produktnamen. Die Bezeichnung „R1“ wird mitunter verwendet, um die „R2“-Versionen explizit von ihren Vorgängern zu unterscheiden; sie war nie Bestandteil der offiziellen Produktbezeichnungen.\n\n\nUnter Verwendung des Microsoft Small Business Server 2003 R1 gilt:\n\n\nUnter Verwendung des Microsoft Small Business Server 2003 R2 gilt:\n\n\nIm Lieferumfang befinden sich fünf sogenannte CALs (Client Access License). Das bedeutet, dass fünf Benutzer oder Computer Zugriff auf die vom Server zur Verfügung gestellten Ressourcen haben. Gängig ist es, die Lizenzierung pro Benutzer und nicht pro Gerät zu betreiben. In Betrieben, die Schichtbetrieb betreiben, kann jedoch die Lizenzierung pro Gerät von finanziellem Vorteil sein.\n\nEine CAL für einen Benutzer erlaubt es, jede der im Lieferumfang enthaltenen Technologien des Small-Business-Servers 2003 in vollem Umfang zu nutzen. Da der Small-Business-Server auf kleine und mittelgroße Betriebe ausgelegt ist, beinhaltet die Software eine Beschränkung auf 75 CALs.\n\nBei mehr als 5 CALs sollte die empfohlene Systemanforderung eingehalten werden, da sonst eine Verlangsamung des Servers auftreten kann.\n\nMit Hilfe des Transitionpacks kann der Small-Business-Server 2003 in seine Einzelkomponenten überführt werden. Nach Anwendung des Transitionpacks besitzt man sowohl technisch als auch lizenzrechtlich die Basiskomponenten des SBS2003 in seiner jeweiligen Version. Die Beschränkungen des SBS2003 gegenüber den Basiskomponenten sind dann aufgehoben. Das betrifft vor allem die Lizenzanzahlbeschränkung auf 75 CAL und die Rollenbeschränkung des Servers. Auch die Terminalservices stehen danach zur Verfügung, man erhält aber durch das Transitionpack keine Lizenz (TSCAL) dafür.\n\n\n\n\n"}
{"id": "938999", "url": "https://de.wikipedia.org/wiki?curid=938999", "title": "Back Orifice", "text": "Back Orifice\n\nBack Orifice (oft abgekürzt BO) ist ein Fernwartungstool für Microsoft Windows.\n\nEntwickelt wurde es von der 1984 gegründeten Hackergruppe Cult of the Dead Cow und ab 1998 verbreitet. Insbesondere weil es versteckt eingesetzt und sein Programmname beliebig gewählt werden kann, wird es häufig als illegales Hintertür-Programm angewendet. Es wird weder durch ein Symbol angezeigt, dass das Programm läuft, noch taucht es in der Task-Liste auf. Es gilt (nicht ganz treffend) auch als trojanisches Pferd. Sobald Back Orifice auf einem „aus der Ferne zu wartenden“ (oder angegriffenen) Rechner installiert ist, kann der „Fernwartungstechniker“ (oder Angreifer) diesen Rechner über das Internet oder das lokale Netzwerk unbemerkt unter seine Kontrolle bringen. Da das Programm seine Existenz und vor allem seine Ausführung nicht anzeigt, ist sein Einsatz illegal, wenn nicht der jeweilige Nutzer des ferngewarteten (ferngesteuerten) PCs dem Einsatz zustimmt. Deshalb erkennen die meisten Antivirenprogramme Back Orifice auch als Malware.\n\nSein Name, der übersetzt etwa „hintere Körperöffnung“ bedeutet, ist eine Persiflage auf die mittlerweile nicht mehr unter dieser Bezeichnung angebotenen Produktlinie „Back Office“ des Anbieters Microsoft.\n\nMittels Back Orifice kann man auf einem entfernten Rechner:\n\n\n"}
{"id": "943825", "url": "https://de.wikipedia.org/wiki?curid=943825", "title": "NeXTstation", "text": "NeXTstation\n\nDie NeXTstation war ein Computer des Herstellers NeXT, der 1990 vorgestellt wurde und die Nachfolge des NeXTcube antrat.\n\nDie NeXTstation verfügte über einen Motorola-68040-Prozessor mit 25 MHz in der normalen oder 33 MHz in der \"Turbo\"-Variante. Die beiden Gerätevarianten verfügten standardmäßig über Schwarzweiß-Grafik, es gab jedoch auch die Möglichkeit, die Geräte mit Farbgrafik (als \"Color\"-Modell) zu bekommen, somit gab es insgesamt vier Modellvarianten. Des Weiteren verfügten alle Modelle über 8 MB Arbeitsspeicher, der auf bis zu 32 MB ausgebaut werden konnte (bzw. 128 MB in der \"Turbo\"-Variante), eine SCSI-Festplatte mit einer Kapazität von 105 MB sowie ein Diskettenlaufwerk und einen Ethernetanschluss (10BASE-T und 10BASE2).\n\nDie normale Monochromversion der NeXTstation kostete bei Erscheinen, mit 17-Zoll-Monitor, etwa 10.000 DM (nach heutiger Kaufkraft ca. US-Dollar) bzw. 4.995 Euro.\n\nAusgeliefert wurde die NeXTstation mit NeXTStep 2.0; es liefen jedoch auch die späteren Versionen wie 3.0 oder 3.3 problemlos darauf. Auch der NeXTStep-Nachfolger OPENSTEP lief bis Version 4.2 auf der NeXTstation. Als Alternative kann auf der NeXTstation auch NetBSD installiert werden.\n\n"}
{"id": "943958", "url": "https://de.wikipedia.org/wiki?curid=943958", "title": "WinHelp", "text": "WinHelp\n\nWinHelp war ein Hilfeformat, das erstmals im Jahr 1990 mit Windows 3.0 eingeführt wurde und noch bis Windows XP unterstützt wurde. Das Format ist nicht öffentlich dokumentiert, konnte aber inzwischen erfolgreich mittels Reverse Engineering entschlüsselt werden.\n\nEs gab vier Versionen von WinHelp:\n\n\nWinHelp basierte auf dem Dateiformat RTF. Hilfedateien konnten somit mit einem beliebigen Textverarbeitungsprogramm (z. B. Microsoft Word) erstellt werden. Spezielle Anweisungen für den Compiler wurden in geschweiften Klammern festgehalten, zur abschnittsweisen Unterteilung der Hilfedatei dienten Fußnoten. Hinzu kam noch eine Datei mit der Dateiendung codice_5. Diese entsprach von der Funktion her den Makefiles gängiger Programmiersprachen und enthielt u. a. Verweise auf sämtliche Dateien, die Bestandteil der kompilierten Hilfedatei werden sollten.\n\nBilder konnten entweder direkt in das RTF eingefügt werden (waren dann aber auf eine Größe von 32 KB beschränkt) oder als Referenz eingebettet werden, wobei das Bild erst bei Laufzeit eingefügt wird. Unterstützt wurden zunächst ausschließlich Bitmaps, ab Windows 3.1 WMF-Dateien sowie zwei proprietäre Bildformate.\n\nMithilfe von Makros konnten entweder Funktionen über eine mitgelieferte DLL-Datei ausgeführt werden, oder auch unmittelbar aus der Hilfedatei heraus auf die Windows-API zugegriffen werden. Insbesondere letzteres galt später als eklatantes Sicherheitsrisiko.\n\nOptional konnte eine Hilfedatei Schlüsselwörter definieren, die dann über einen Index durchsucht werden konnten. Ab Windows 95 gab es dann auch eine echte Volltextsuche innerhalb von Hilfedateien.\n\nDie Compiler waren noch bis Windows 3.1 MS-DOS-Programme und somit auf die 640 KB konventioneller Speicher begrenzt, auch wenn Microsoft später eine Version des Compilers veröffentlichte, die DOS-Extender unterstützte und somit nicht mehr von dieser Beschränkung betroffen war. Ab Windows 95 gab es dann den grafischen Help Compiler, der auch kostenlos zum Download von der Microsoft-Webseite angeboten wurde.\n\nMit Windows 98 wurde WinHelp durch das HTML-Help-Format ersetzt, WinHelp wurde aber aus Gründen der Abwärtskompatibilität weiter unterstützt.\n\nWindows Vista enthält keine Unterstützung für WinHelp mehr. Der Betrachter kann aber bei Bedarf von der Microsoft-Webseite heruntergeladen werden. Auch für Windows 7 und Windows 8 kann ein Betrachter für WinHelp noch von der Microsoft-Webseite heruntergeladen werden. Spätere Versionen unterstützten das WinHelp-Format nicht mehr.\n"}
{"id": "947419", "url": "https://de.wikipedia.org/wiki?curid=947419", "title": "COMMAND.COM", "text": "COMMAND.COM\n\nCOMMAND.COM ist die Betriebssystem-Shell der Betriebssysteme MS-DOS und PC-DOS sowie der darauf basierenden Systeme Microsoft Windows 95, 98, 98 SE und ME.\n\ncodice_1 wurde durch Tim Paterson in 86-DOS eingeführt, welches von Microsoft übernommen worden ist. Neben der Variante von Microsoft gab oder gibt es auch Entsprechungen in den MS-DOS-kompatiblen Betriebssystemen, darunter etwa das bis heute weiterentwickelte FreeCOM aus dem quelloffenen Projekt FreeDOS oder der Kommandozeileninterpreter, den der ehemalige Microsoft-Konkurrent Digital Research mit seinem DR-DOS mitlieferte. Es gibt auch Varianten wie 4DOS, die nicht Bestandteil eines Betriebssystems sind, sondern den Interpreter eines Systems ersetzen und erweiterte Funktionen bieten.\n\nDie Funktion der codice_1 in MS-DOS und Kompatiblen ist, die grundlegende – und früher einzige im Betriebssystem enthaltene – Schnittstelle zum Benutzer zu bilden. codice_1 interpretiert die Eingabe des Benutzers und führt daraufhin Befehle aus oder versucht, ein anderes Programm aufzurufen. Ebenfalls von codice_1 wird deswegen die für DOS typische Eingabeaufforderung (eng. \"Prompt\") angezeigt – für gewöhnlich der aktuelle Verzeichnispfad, gefolgt von einem Größer-als-Zeichen und im Standard-Textmodus Weiß auf Schwarz.\n\nDa die codice_1 \"nicht\" Teil des Kernels, sondern als eigenes Programm ausgeführt ist, könnte ein DOS-System theoretisch auch ohne codice_1 oder Entsprechung genutzt werden; ohne jede Eingabemöglichkeit oder ausreichende Automatisierung hätte dies aber keinen sinnvollen Nutzen. Teilweise boten neuere DOS-Versionen direkt die Möglichkeit, eine Konfiguration ohne (Standard-)Interpreter zu wählen; beispielsweise, indem in der von codice_7 (MS-DOS), codice_8 (DR-DOS, PC DOS) oder codice_9 (Windows 9x) verarbeiteten Datei codice_10 die Option codice_11 angegeben wurde. Eine andere Variante, die codice_1 zu ersetzen, besteht darin, lediglich dem Alternativprogramm diesen Namen zu geben – das System führt dann die gewünschte Alternative aus, ohne den Unterschied zu merken. Da die Ausführung einer codice_13-Datei und einer codice_14-Datei sich nicht unterscheiden, stört es nicht, wenn der Name des anderen Programms ursprünglich die Dateiendung codice_15 hatte.\n\ncodice_1 führt bei ihrem ersten Aufruf codice_17 aus.\n\nBei den für codice_1 verfügbaren Befehlen wird zwischen internen und externen Befehlen unterschieden. Als interne Befehle werden die im Kommandozeileninterpreter selbst enthaltenen Befehle bezeichnet; externe Befehle hingegen sind eigenständige Programme, die aus eigenen Programmdateien (ausführbare codice_19- oder codice_15-Dateien) bestehen.\n\ncodice_1 von MS-DOS 5.0, eingeführt im Sommer 1991, kennt 29 interne Befehle.\n\nAußerdem gibt es noch sogenannte Stapelverarbeitungsdateien, auch Batchdateien oder Batches genannt, die einen „Stapel“ von nacheinander abzuarbeitenden – nach Belieben internen oder externen – Befehlen enthalten. Stapelverarbeitungsdateien können sich wie ein echtes Programm oder ein echter Befehl verhalten und werden oft auch so aufgerufen; sie sind aber den Skripten zuzuordnen.\n\nEin Befehl wird aufgerufen, indem der Name dieses Befehls eingegeben wird. Sobald die Befehlseingabe mit der Eingabetaste (\"Enter\" oder \"Return\") beendet wurde, sucht codice_1 nach einem passenden internen Befehl. Wird kein interner Befehl gefunden, so wird zunächst im aktuellen Verzeichnis nach ausführbaren Dateien gesucht, die den eingegebenen Befehl als Dateinamen tragen. Falls so kein passendes Programm gefunden wurde, wird zuletzt jeder Ordner im „Pfad“ durchsucht. Das erste gegebenenfalls gefundene Programm wird ausgeführt, andernfalls wird eine Fehlermeldung angezeigt, und die Eingabeaufforderung erscheint wieder.\n\nBefehle können entweder einzeln oder gefolgt von (auch als \"Argumente\" bekannten) sogenannten Parametern aufgerufen werden. Diese Parameter werden je nach Befehl oder Programm unterschiedlich interpretiert und können verschiedenste Optionen festlegen. Ein Parameter (manchmal auch Schalter genannt), den fast jeder Befehl in DOS kennt, und der für gewöhnlich eine kurze Direkthilfe ausgibt, lautet beispielsweise \"/?\". Im Gegensatz zu Programmen, die sich nur über Parameter steuern lassen, gibt es auch solche, die ohne Parameter aufgerufen werden und dem Benutzer danach selbst die nötigen Eingabemöglichkeiten bieten. Dazu zählen auch die verschiedenen grafischen Benutzeroberflächen, die von der DOS-Kommandozeilenoberfläche gestartet werden können, zum Beispiel Microsoft Windows 1.x bis 4.x oder die Versionen von GEM für DOS.\n\nUnter MS-DOS 6.22 beträgt die maximale Länge eines Befehls in der Eingabeaufforderung 127 Zeichen.\n\nUnter den inzwischen veralteten DOS-basierten Windows-Versionen wird die codice_1 aus dem sowieso benötigten DOS-System (welches ab Windows 95 auch enthalten ist) dazu benutzt, eine Eingabeaufforderung in einem „Fenster“ der Benutzeroberfläche bereitzustellen. Zwar ist dem zugrundeliegenden DOS-Betriebssystem kein Alleinzugriff auf die Hardware mehr möglich, es können aber noch alle Funktionen des Interpreters uneingeschränkt genutzt werden.\n\nMicrosoft nannte diese Kommandozeilenoberfläche damals \"MS-DOS-Eingabeaufforderung\", allerdings ist der Begriff fälschlicherweise auch heute noch vielen Anwendern ein Synonym für textbasierte Eingaben in Windows-Betriebssystemen.\n\nWindows NT und alle Nachfolger bis hin zum aktuellen Microsoft-Betriebssystem Windows 10 setzen einen neuen, leistungsfähigeren und weitgehend abwärtskompatiblen Kommandozeileninterpreter namens codice_24 ein. Viele in der ursprünglichen codice_1 fehlende Funktionen, die früher nur durch Programme von Drittherstellern bereitgestellt werden konnten, sind in codice_24 enthalten. So können zum Beispiel einfache Berechnungen durchgeführt werden, und das Entfernen bestimmter Zeichen aus einer Variable ist möglich. Zudem wird die codice_24 mit der fortschreitenden Entwicklung von Windows-NT-basierten Betriebssystemen ebenfalls noch weiterentwickelt, während die codice_1 von Microsoft (zumindest als Standard-Kommandozeileninterpreter) in Windows ME ihr endgültig letztes Auftreten hatte. Im Gegensatz zu codice_1 bei MS-DOS und Kompatiblen ist codice_24 \"kein\" wichtiger Bestandteil des Betriebssystems und wird nur benötigt, um eine Kommandozeilenoberfläche (Eingabeaufforderung) bereitzustellen und Stapelverarbeitungsdateien auszuführen.\n\nNeben dem Standard-Kommandozeileninterpreter codice_24 bieten Windows NT und Nachfolger – sofern auf 32-Bit-Basis implementiert – allerdings auch noch einen kompatibleren Ersatz-Interpreter, der ebenfalls codice_1 heißt. Zwar erfüllt er die gleiche Funktion wie die codice_1 aus MS-DOS und älteren Windows-Versionen, ist aber eine richtige Windows-Anwendung. Alle 16-Bit-Funktionen werden durch codice_1 und die virtuelle DOS-Umgebung (eng. \"Virtual DOS Machine\", abgekürzt VDM) lediglich bereitgestellt. Nachteilig wirkt sich aus, dass die codice_1 aus Windows NT dem Interpreter aus den MS-DOS-Versionen 5 und 6 mehr entspricht, als dem, der in Windows 95, 98 und Me zum Einsatz kam. Damit gehen auch einige Verbesserungen verloren, zum Beispiel die mögliche Nutzung von langen Dateinamen mit mehr als acht Zeichen und Dateiendungen mit mehr als drei Zeichen.\n\nIm Betriebssystem FreeDOS ist der Kommandozeileninterpreter FreeCOM enthalten. FreeCOM und FreeDOS werden – im Gegensatz zu MS-DOS, DR-DOS und den meisten anderen DOS-Betriebssystemen und/oder -Kommandozeileninterpretern – noch immer weiterentwickelt. Ähnlich codice_24 bietet auch FreeCOM einige Erweiterungen zum MS-DOS-Interpreter, ist dabei aber mehr auf Kompatibilität ausgerichtet. So werden beispielsweise Stapelverarbeitungsdateien aus MS-DOS meist auch ohne Anpassungen korrekt von FreeCOM interpretiert.\n\n"}
{"id": "955374", "url": "https://de.wikipedia.org/wiki?curid=955374", "title": "Arithmetischer Überlauf", "text": "Arithmetischer Überlauf\n\nDer Arithmetische Überlauf () oder Zählerüberlauf (engl. ) ist ein Begriff aus der Informatik. Solch ein Überlauf tritt auf, wenn das Ergebnis einer Berechnung für den gültigen Zahlenbereich zu groß ist, um noch richtig interpretiert werden zu können.\n\nZumeist wird man dem Überlauf beim Rechnen mit Zweierkomplementzahlen begegnen. So kann es passieren, dass bei der Addition zweier Zahlen mit gleichem Vorzeichen eine Zahl mit anderem Vorzeichen entsteht. In diesem Fall setzt der Prozessor das Überlaufbit. Mit einigen Prozessoren und Programmiersprachen kann ein Überlauf durch einen Laufzeitfehler oder eine Ausnahmebehandlung (Exception) aufgefangen werden.\n\nDer Überlauf hängt immer von der verwendeten Zahlendarstellung ab. Er ist keinesfalls mit dem Übertrag (engl. ) zu verwechseln.\n\nEin Ganzzahlüberlauf (engl. ) tritt auf, wenn ein Computer Berechnungen mit \"begrenzter Stellenzahl\" durchführt und das Rechenergebnis zur Darstellung mehr Stellen erfordert.\n\nDie Stellenanzahl und damit der Wertebereich ist durch das Rechenwerk begrenzt. Das Rechenwerk heutiger Computer ist meist für 32 oder 64 Binärstellen ausgelegt. Tritt hier ein Ganzzahlüberlauf auf, wird das im Statusregister des Prozessors registriert; dieser Fall kann vom Programmierer festgestellt werden.\n\nEin anderer Fall liegt vor, wenn ein Rechenergebnis in einer Variablen gespeichert wird, die weniger Stellen als das Rechenwerk aufweist. Dieser Fall wird vom Prozessor nicht automatisch erkannt, die Variable erhält einen falschen Wert.\n\nNur durch die Verwendung von Funktionsbibliotheken ist es möglich, Berechnungen mit Millionen von Stellen durchzuführen ohne einen Ganzzahlüberlauf zu erreichen.\n\nEin Beispiel aus der Programmiersprache C:\nDer Datentyp \"unsigned char\" umfasst in der Regel 8 Bit und hat einen Wertebereich von 0 bis 255.\n\nDie zugehörige duale Rechnung veranschaulicht den Ganzzahlüberlauf:\n\nDie vordere Eins, das neunte Bit, ist nicht in den 8 Bit des gewählten Datentyps enthalten. Betrachtet man nur diese letzten 8 Bit, so erhält man \"00000001\", also 1 und nicht 257. Selbst wenn die Zahlenwerte bei der Übersetzung des Programmcodes schon feststehen, ignorieren manche C-Compiler diese Überläufe, was zu falschen Ergebnissen führt. Daher sollte der Datentyp immer ausreichend groß gewählt werden.\n\nBei plattformunabhängiger Programmierung sollte der \"Ganzzahlüberlauf\" nicht absichtlich benutzt werden, da der Wertebereich der Datentypen und damit der Punkt des Überlaufs auf den Zielsystemen unterschiedlich sein kann.\n\nDer auf 32-Bit-Prozessoren häufig verwendete Ganzzahl-Datentyp Integer kann im Zweierkomplement die Werte –2 = –2.147.483.648 bis +(2)–1 = +2.147.483.647 darstellen. Wird nun zu +2.147.483.647 (Binär 01111111 11111111 11111111 11111111) eins dazugezählt, erhält man nicht wie erwartet +2.147.483.648, sondern –2.147.483.648, da der Binärwert 10000000 00000000 00000000 00000000 als negative Zahl interpretiert wird. Ein solcher Überlauf ist auch die Ursache für das Jahr-2038-Problem.\n\nIm Zweierkomplement sind positive und negative Zahlen darstellbar, sodass die Subtraktion auf die Addition zurückgeführt werden kann. Es sind 3 Fälle zu betrachten:\n\nMan muss sich stets die letzten beiden Überträge anschauen, hier formula_1 und formula_2 genannt. Sind diese ungleich, dann ist das Ergebnis falsch, als Resultat eines Überlaufes. Bei der Addition einer positiven und einer negativen Zahl kann dies nie der Fall sein.\n\nIn einer 4-Bit-Architektur, die mit dem Zweierkomplement arbeitet, ist z. B. die Dezimalzahl 10 nicht dual abbildbar.\n\nManche Prozessoren können einen Überlauf durch ein Überlaufbit registrieren.\n\nGanzzahlüberläufe können indirekt ein sicherheitsrelevantes Problem darstellen, wenn sie Teil eines Programmfehlers sind. Insbesondere wenn die fehlerhafte Berechnung zur Bestimmung der Größe eines Puffers genutzt wird oder die Adressierung eines Feldes betrifft. Dann können daraus Pufferüberläufe resultieren oder es einem Angreifer ermöglichen den Stack zu überschreiben.\n\nEinen Spezialfall stellt der sogenannte \"signedness bug\" dar. Er tritt auf, wenn eine vorzeichenbehaftete Ganzzahl (\"signed\") als nichtnegative Zahl (\"unsigned\") interpretiert wird.\n\nDie Tragweite von Ganzzahlüberläufen liegt oft auch darin, dass sie nicht erkannt werden können, nachdem sie erfolgt sind. Derart fehlerbehaftete Stellen sind im Programmcode deshalb nur schwer zu finden.\n\n\n"}
{"id": "955713", "url": "https://de.wikipedia.org/wiki?curid=955713", "title": "Scanner Access Now Easy", "text": "Scanner Access Now Easy\n\nScanner Access Now Easy, kurz SANE, ist eine freie Programmierschnittstelle (API) für den Zugriff auf bildgebende Geräte wie Scanner, Digitalkameras, Videokameras und andere über verschiedene Schnittstellen (USB, SCSI etc.). Es findet hauptsächlich unter Linux Verwendung, mit Portierungen auf OS/2 bzw. eComStation, macOS und Windows. Es ist eine Alternative zu TWAIN als Programmierschnittstelle.\n\nDie zur Distribution gehörenden Anwendungs- und Hilfsprogramme stehen unter der GNU GPL, die Schnittstellenbeschreibung selbst ist jedoch gemeinfrei.\n\nIm Gegensatz zu TWAIN sind bei SANE die Funktionalitäten von \"Frontend\" und \"Backend\" strikt getrennt.\n\nAufgabe der \"Backends\" ist die Kommunikation mit dem jeweiligen Bilderfassungsgerät über eine der zahlreichen unterstützten Schnittstellen (USB, FireWire, SCSI, Parallele Schnittstelle oder Serielle Schnittstelle). Daneben existieren auch Backends ohne eine physische Bildquelle, z. B. zum Generieren von Weißem Rauschen oder beliebigen anderen Bilddaten aus einem Prozess oder einer Bilddatei. Dazu kommt das spezielle \"net\"-Backend (siehe unten). Unabhängig von ihrer Implementierung stellen alle Backends eine einheitliche Schnittstelle zum Ansprechen der von ihnen unterstützten Quellen zur Verfügung, die den Kern der SANE-Definition bildet.\n\nDie \"Frontends\" stellen den zum Benutzer gerichteten Teil des Arbeitsablaufes dar und sind normalerweise in ein Anwendungsprogramm integriert. Durch die standardisierte Schnittstelle können alle Frontends mit allen Backends ohne Probleme verwendet werden, die verfügbaren Optionen werden lediglich durch die tatsächlichen Fähigkeiten der Bildquelle (Auflösung, Farbtiefe, mit oder ohne Einzelblatteinzug, Diawechsler usw.) beschränkt. Eine besondere Form eines Frontends stellt der zum Projekt gehörende \"saned\" dar (siehe unten). Die SANE-Distribution enthält bereits einige fertige Frontends, wie z. B. das Programm \"scanimage\", mit dem sich in einem Skript automatisch Bilder einlesen lassen, ohne dafür eine grafische Benutzeroberfläche zu benötigen.\n\nFrontends können sowohl auf das reine Bildeinlesen beschränkt sein, als auch weitergehende Funktionen bieten.\n\n\nDie Aufgabenteilung zwischen Backend und Frontend macht es verhältnismäßig einfach, via SANE einen im Netzwerk verfügbaren Scanner zu betreiben. Der mitgelieferte Daemon \"saned\" greift als spezielles Frontend auf das zum Scanner gehörende Backend zu und stellt seine Dienste via TCP/IP (auf Port 6566) zur Verfügung. Am anderen Ende des Netzes agiert das \"net\"-Backend gegenüber dem lokalen Frontend und sorgt für die Weiterleitung aller Daten zum Daemon. Folge ist, dass alle anderen Frontends und damit auch Anwendungsprogramme keine Anpassungen benötigen, um einen (oder mehrere) Netzwerkscanner zu verwenden.\n\nSANE bietet keinen Schutz des Zugriffes auf den Scanner, es erfolgt keine Authentifizierung oder Autorisierung, alle Daten werden unverschlüsselt übertragen. Mittels TCP-Wrapper lässt sich die Verwendung begrenzen oder über SSH durch einen gesicherten Tunnel führen. Dank des \"net\"-Backends sind dafür nur Änderungen an dessen Konfiguration, nicht aber den eingesetzten Programmen nötig.\n\n\n"}
{"id": "955932", "url": "https://de.wikipedia.org/wiki?curid=955932", "title": "Porter-Duff Composition", "text": "Porter-Duff Composition\n\n\"Porter-Duff Composition\" ist ein Verfahren zur Überlagerung digitaler Bilder. Es wurde 1984 von Thomas Porter und Tom Duff im Rahmen des Computer Graphics Project der Lucasfilm Ltd. beschrieben .\n\nBei der Überlagerung von digitalen Bildern muss definiert sein, welchen Einfluss die Überlagerung auf das einzelne Pixel hat. Dabei muss neben den drei Farbkanälen (rot, grün, blau) noch die Transparenz (Alphakanal) berücksichtigt werden, wodurch pro Pixel definiert werden kann, wie es bei Überlagerung mit einem darunterliegenden Pixel dargestellt werden soll bzw. wie stark die Farbe des unten liegenden Pixel auf dem überlagernden Pixel durchscheint.\n\nDas Verfahren unterscheidet für die Überlagerung von zwei Pixeln (A und B) zwischen 12 verschiedenen Methoden. In den Beispielen 1 und 3 ist eine Transparenz von 0 % bzw. eine Opazität von 100 % definiert, d. h. die Pixel sind nicht transparent. Bei Beispiel 2 hat sowohl Pixel A als auch Pixel B eine Transparenz von 50 %.\n\nAlle modernen Grafiksysteme unterstützen die Porter-Duff-Verfahren. So z. B. in Quartz, der Grafikschicht des Betriebssystems Mac OS X.\n\nAuch die Programmiersprache Java unterstützt standardmäßig diese Verfahren.\n\n"}
{"id": "960525", "url": "https://de.wikipedia.org/wiki?curid=960525", "title": "Produktregistrierung", "text": "Produktregistrierung\n\nBei der Produktregistrierung handelt es sich um eine in aller Regel über das Internet stattfindende Registrierung, bei manchen Produkten, wie z. B. Windows-Betriebssystemen, ist dies jedoch auch alternativ über das Telefon möglich. Ggf. ist für die volle Benutzung der Software eine Registrierung nötig, dann handelt es sich dabei um eine Produktaktivierung.\n\nOft ist eine zeitliche oder funktionale eingeschränkte Benutzung eines solchen aktivierungspflichtigen Programms auch ohne Registrierung möglich.\n\nBei vielen Software-Produkten wird auch zwischen Produktaktivierung und Produktregistrierung unterschieden.\nIn dem Fall handelt es sich bei der Produktaktivierung um eine Registrierung des Produkts, bei der keine persönlichen Daten abgefragt werden, sondern nur Produktkey und ein für den Computer individuell errechneter Schlüssel an den Hersteller gesendet werden, zumindest aber weniger Daten versendet werden als bei dem dann als Produktregistrierung bezeichneten Vorgang. Sinn, Zweck und Funktionsweise sind unter Produktaktivierung genauer erklärt.\nMit Produktregistrierung ist dann eine weitergehende Produktregistrierung bei der auch persönliche Daten wie z. B. E-Mail-Adresse abgefragt werden und für die oft zusätzlicher Service angeboten wird, welche jedoch für die Benutzung des Programms nicht zwangsläufig nötig ist, gemeint.\n"}
{"id": "963546", "url": "https://de.wikipedia.org/wiki?curid=963546", "title": "Kiosksystem", "text": "Kiosksystem\n\nKiosksysteme sind interaktive Computeranlagen, die im öffentlichen Raum oder an halböffentlichen Standorten, etwa in Ladenzonen, genutzt werden. Im Gegensatz zu mobilen Computergeräten bekommen sie einen Standort fest zugeteilt. Sie stellen den Nutzern Informationen zur Verfügung. Manche bieten daneben auch die Möglichkeit zur Geschäftsanbahnung oder zum Geschäftsabschluss. Kiosksysteme sind Selbstbedienungsterminals. Durch ihren Einsatz kann Personal eingespart werden.\n\nVon den Kiosksystemen zu unterscheiden sind Computeranlagen, die ebenfalls in der Öffentlichkeit genutzt werden und einen Zugang zu der thematisch unbegrenzten Welt des Internets bieten. Diese werden als \"Internet-Terminals\" oder als \"Surf-\" oder \"MultimediaStationen\" bezeichnet.\n\nKiosksysteme bieten meistens nur Zugriff auf jene Anwendungsprogramme, mit denen der Aufsteller/Inhaber Informationen verbreiten will. Sie bieten Informationen über Zeitschriften, Zeitungen, Unternehmen, Fahrpläne oder historische Gebäude. Man findet sie in der normalen Öffentlichkeit oder in geschlossener, aber anonymer Öffentlichkeit, z. B. auf Messen.\n\nDie ersten Kiosksysteme wurden als POI-Systeme (POI: Point of Interest oder Point of Information) oder POS-Systeme (POS: Point of Sale oder Point of Service) bezeichnet. Sie stellten dem Nutzer auf Flughäfen oder Bahnhöfen Informationen bereit, verkauften Fahr- oder Eintrittskarten und boten dem Nutzer einen hohen Grad an Selbstbedienung.\n\nWenn man den Kiosksystem-Begriff weit fasst, können auch Geldautomaten zu den Kiosksystemen gezählt werden.\n\nDie Nutzung der Systeme kann anonym, d. h. ohne persönliche Identifikation, erfolgen. Manche Systeme sehen allerdings die Eingabe einer Kennung vor (Benutzerkennung, Passwort, Personalnummer, Kundennummer, u. a.) und sind dann imstande, ein personalisiertes Informationsangebot zu bieten.\n\nEs gibt unzählige Anwendungen für geschlossene Benutzer-Gruppen (CUG – Closed User Groups) auf dem Markt, etwa zur Personalzeiterfassung als Stempeluhr.\n\nDatenausgabe über Kiosksysteme gibt es in Firmenbereichen, die üblicherweise über keine Computerarbeitsplätze verfügen, insbesondere in Werkhallen und unter freiem Himmel. Weiter gibt es sie auch in Bereichen, die nur von Personen mit besonderer Berechtigung betreten werden können, also in Bereichen, in denen die Identifizierung über Clubausweise, Mitgliederausweise, Kundenkarten oder dergleichen erfolgt.\n\nDie Nutzungskonzepte von Kiosksystemen sehen eingeschränkte Benutzeroberflächen vor. Es gibt nur Zugriff auf die unbedingt erforderlichen Programme. Wenn Webbrowser zum Einsatz kommen, werden diese so konfiguriert, dass es eine eingeschränkte Nutzungsbreite gibt. Man erreicht mit diesen Maßnahmen eine erhöhte Betriebssicherheit und -stabilität im Dauerbetrieb.\n\nBei einem einfachen Kiosksystem bietet das Hypertext-System nur eine geringe Zahl an Verzweigungsmöglichkeiten und einen begrenzten Umfang an aufrufbaren Seiten. Es handelt sich um ein in der hierarchischen Tiefe und Komplexität reduziertes System.\n\nDas Bedienkonzept eines Kiosksystems kann die Verwendung im Stehen oder Sitzen vorsehen. Was jeweils angebracht ist, ergibt sich aus der anvisierten Zielgruppe und der optimalen Verweildauer. Für Informationssysteme im Gemeindewesen ist Barrierefreiheit vorgesehen. Sie müssen also für Personen im Rollstuhl in gleicher Weise nutzbar sein, wie für Personen ohne Einschränkungen.\n\nKiosksysteme stehen den Nutzern häufig rund um die Uhr zur Verfügung. Unter solchen Umständen lässt sich nicht präzise vorhersagen, welche Nutzergruppen sich einstellen werden und welchen Kenntnisstand in der Anwendung von Computerarbeitsmitteln sie mitbringen. Kioskterminals sollten daher über eine einfach zugängliche, intuitiv erfassbare und leicht bedienbare Benutzeroberfläche verfügen.\n\nWährend sich die Nutzer ein leicht verständliches Bedienkonzept wünschen, kann es auf der anderen Seite aber Geräte-, Standort- und Inhaltsanbieter geben, die sich von einem schlechten Bedienkonzept eine längere Verweildauer am Gerät und damit einen erhöhten Werbenutzen versprechen.\n\nEiner der größten Vorteile der Kiosksysteme gegenüber anderen Informationsangeboten liegt darin, dass der Dienstleister kein Personal am Ort haben muss und dass die Systeme auch abseits von den üblichen Arbeitszeiten genutzt werden können. Außerdem müssen Produkte, die vorgestellt werden sollen, nicht physisch anwesend sein.\n\nMit Kiosksystemen lassen sich neuartige Konzepte für die Informationsvermittlung realisieren. Ein Beispiel sind Systeme in den Verkaufszonen von Einzelhandelsgeschäften. Sie bieten dem Kunden die Möglichkeit, sich selbständig über die zum Verkauf angebotenen Produkte zu informieren. Der Ablauf kann dabei so aussehen: Der Kunde trägt das Produkt, für das er sich interessiert, zum Kiosksystem; dort wird über einen Scanner die EAN-Nummer eingelesen. Es können dann alle Informationen angezeigt werden, für die auf dem Klebetikett der Platz nicht gereicht hat. Trägt der Kunde beispielsweise eine Flasche Wein zum Kiosksystem, kann das System die optimale Genussreife und Lagerfähigkeit angeben, dazu die ideale Trinktemperatur, und es kann auch die Frage beantwortet werden, zu welchen Speisen der Wein passt.\n\nInformationen können an Orten angeboten werden, an denen der Anbieter im Regelfall über keine örtliche Präsenz verfügt, sogenannten Drittstandorten. Beispielsweise kann ein Konzertveranstalter in einer Verkaufsstelle für Eintrittskarten ein Gerät aufstellen, mit dem sich Konzerttermine abfragen lassen. Bei so einer Kooperation profitiert die Verkaufsstelle, weil für die Kunden zusätzliche Gründe entstehen, ihre Örtlichkeiten aufzusuchen, und der Konzertveranstalter profitiert, weil er mit seinem Informationsangebot an ein interessiertes Publikum gerät.\n\nInzwischen werden Kiosksysteme jedoch nicht mehr nur für Informationszwecke eingesetzt, sondern auch um einen stets präsenten und offenen Feedbackkanal im Prozess des Beschwerdemanagements zur Verfügung zu stellen. Das Feedback kann dann dank des Kiosksystems direkt digital ausgewertet werden. Im Einsatz sind Kiosksysteme mit einer solchen Feedbackfunktion vor allem in der Gastronomie und Hotellerie.\n\nEs gibt Kiosksysteme, die sich durch Werbung finanzieren lassen. Umso besser es bei diesen Systemen gelingt, werbewirksame Wiedererkennungseffekte zu schaffen, umso mehr Geld steht zur Verfügung, um mit aufwendigen und kostspieligen Funktionen den Nutzeffekt für die Kunden zu erhöhen.\n\nAufgrund von kulturellen Unterschieden sieht die gesellschaftliche Akzeptanz von Kiosksystemen in den verschiedenen Ländern unterschiedlich aus. Die Verbreitung und Akzeptanz ist in den angelsächsischen Ländern sehr viel weiter fortgeschritten als in den deutschsprachigen Ländern. In öffentlichen Bibliotheken stehen Kiosksysteme nicht nur zur selbsttätigen Suche und zur Ausleihe und Rückgabe zur Verfügung, sondern zählen mit kostenlosem oder auch kostenpflichtigem unbeschränkten Internetzugang zum etablierten Bild. In den deutschsprachigen Ländern befindet sich die Verbreitung von Kiosksystemen dagegen noch im Anfangsstadium.\n\n\n"}
{"id": "968004", "url": "https://de.wikipedia.org/wiki?curid=968004", "title": "TRADIC", "text": "TRADIC\n\nTRADIC (engl. Abkürzung für \"TR\"ansistorized \"A\"irborne \"DI\"gital \"C\"omputer) war der weltweit erste Computer auf der Grundlage von Transistoren – und leitete wegen seiner Störungs- und Ausfallsicherheit und der höheren Geschwindigkeit von Transistoren gegenüber Röhren und nicht zuletzt wegen seiner geringeren Baugröße damit den Siegeszug der Computer auf Transistorbasis gegenüber den zur damaligen Zeit üblichen Röhrencomputern ein. TRADIC schaffte eine Million logische Operationen pro Sekunde.\n\nVon den Bell-Forschungslaboratorien für die United States Air Force entwickelt, wurde im Januar 1954 fertiggestellt. Er bestand aus 10.358 Germanium-Dioden sowie 684 Transistoren und hatte eine Leistungsaufnahme von ca. 90 Watt.\n\nEs gibt unterschiedliche Angaben über den Entwicklungsbeginn von TRADIC und TX-0. Manche Quellen sprechen von 1953 für beide Computer. Vermutlich war der TRADIC aber der erste vollständig auf Transistoren basierende Computer.\n\n"}
{"id": "968362", "url": "https://de.wikipedia.org/wiki?curid=968362", "title": "Paint.NET", "text": "Paint.NET\n\nPaint.NET ist eine Bildbearbeitungssoftware für Microsoft Windows, die von der Washington State University und Microsoft entwickelt wurde.\n\nUrsprünglich als eine einfache, kostenlose Alternative zu Microsoft Paint gedacht, bietet die Software mittlerweile auch anspruchsvollere Funktionen, wie zum Beispiel das Arbeiten mit Ebenen. Die Bedienung von \"Paint.NET\" orientiert sich am Marktführer Adobe Photoshop.\n\n\"Paint.NET\" wurde 2004 als Open-Source-Software unter der MIT-Lizenz veröffentlicht. Seit Version 3.5 ist Paint.NET nicht mehr Open Source, sondern Freeware.\n\nUrsprünglich war Paint.NET als kleines Studentenprojekt für den Frühling 2004 geplant. Rick Brewster, einer der Hauptentwickler, schreibt in seinem Blog, dass die Version 1.0 innerhalb von vier Monaten aus ungefähr 36.000 Zeilen C#-Code geschrieben wurde.\n\nIm Herbstsemester 2004 konnten die Versionen 1.1 und 2.0 veröffentlicht werden. Die Entwicklung wird heute von drei Programmierern, welche inzwischen für Microsoft arbeiten, fortgeführt. Zwei davon sind Rick Brewster und Tom Jackson, welche bereits als Studenten an der Washington State University an Paint.NET gearbeitet haben.\n\nDie Paint.NET-Version 2.72, die am 31. August 2006 veröffentlicht wurde, aber auch frühere Versionen, sind auch noch unter Windows 2000 lauffähig.\n\nAm 26. Januar 2007 wurde die Version 3.0 veröffentlicht, diese unterstützt neben der deutschen und englischen Sprache, die schon in Version 2.72 vorhanden waren, Chinesisch, Französisch, Koreanisch, Japanisch, Portugiesisch und Spanisch. Einen Monat später, am 26. Februar, wurde Version 3.01 veröffentlicht, in der einige Fehler aus Version 3.0 behoben wurden.\n\nAm 29. März erschien eine neue Version. Mit Version 3.05 wurden zwei neue Funktionen eingeführt: Unter Windows Vista lässt sich das Aussehen des Programms verändern und es gibt einen neuen Effekt namens „Bleistiftskizze“. Version 3.07, die am 8. Mai 2007 veröffentlicht wurde, bietet neue Funktionen im Bereich der Linien, wodurch das Erstellen von simplen Pfeilen oder ähnlichem vereinfacht wurde. Am 23. August 2007 wurde die Version 3.10 herausgegeben.\n\nVersion 3.20 wurde am 12. Dezember 2007 veröffentlicht. Sie fasst die „Effekte“ im Programm besser und übersichtlicher zusammen, wodurch Paint.NET sehr Adobe Photoshop ähnelt. Zudem gibt es auch einige neue Effekte und die Funktionen zum Markieren von bestimmten Bereichen/Stellen wurden erweitert.\n\nAm 29. Februar 2008 wurde die Paint.NET-Version 3.30 Beta 1 veröffentlicht. Sie umfasst einige neue Funktionen. Dateien im PNG-Format lassen sich jetzt wahlweise im 8-, 24- oder 32-Bit-Modus speichern. Auch bei Dateien vom Typ BMP kann man jetzt zwischen dem 8- und 24-Bit-Modus wählen. Bei den Dateiformaten PNG, BMP und TGA steht außerdem die Option der automatischen Auswahl zur Verfügung. So wählt das Programm selbst die beste Option zur Speicherung der Dateien. Der Effekt „Polar Inversion“ wurde verbessert. Außerdem gibt es Paint.NET jetzt in italienischer Sprache. Die finale Version stand ab dem 10. April zum Download bereit. Bis Version 3.36 wurden kleinere Fehler aus Version 3.30 behoben und einige Features wurden verändert oder weiter verbessert.\n\nAm 30. Dezember 2008 wurde, zunächst ohne nähere Erläuterung, bekanntgegeben, dass der Quellcode nicht mehr zur Verfügung stehe. In einem Blogeintrag wurde das am 6. November 2009 damit begründet, dass der Quellcode oft kopiert und das Programm unter einem neuen Namen vertrieben wurde, oft auch kommerziell.\n\nAm 7. November 2009 wurde die nicht eingeplante Version 3.5 als Übergang zu Version 4.0 veröffentlicht. Diese bietet neben verbesserter Leistung einen verbesserten Updater, sowie einige neue Effekte. Außerdem ist die Optik für Windows Vista und Windows 7 optimiert worden. Erstmals ist .NET Framework 3.5 SP1 für die Installation zwingend vorgeschrieben.\n\nAb Version 3.5.5 wird mindestens Windows XP SP3 vorausgesetzt, ab Version 4.0 mindestens Windows 7 SP1 und .NET Framework 4.5. Ab Version 4.0.7 wird das .NET Framework 4.6 vorausgesetzt, ab Version 4.0.20 das .NET Framework 4.7.\n\n\"Paint.NET\" beherrscht Ebenenbearbeitung und bietet außerdem eine „unendliche History“ (Undo), welche es ermöglicht, alle Arbeitsschritte jederzeit wieder rückgängig zu machen, sowie weitere Spezialeffekte.\n\nEs gibt eine Anzahl Plug-ins, die zur Verfügung gestellt wurden. Sie können im offiziellen Forum (Bestandteil der Website) heruntergeladen werden. Diese fügen neue Effekte oder die Fähigkeit, neue Dateiformate zu verwenden, hinzu. Zum Beispiel haben Benutzer nach der Installation eines Plug-ins die Möglichkeit, Adobe-Photoshop-Dateien (PSD) zu verwenden (bearbeiten) und zu speichern.\n\nEs gibt eine Reihe von Tutorials, die im Forum von Benutzern oder Entwicklern zur Verfügung gestellt werden.\n\nPaint.NET speichert Bilder standardmäßig im PDN-Format (PaintDotNet). Dies ist ein proprietäres Dateiformat ohne Komprimierung und Datenverlust, die angelegten Ebenen einer Paint.NET-Grafik bleiben erhalten. Das PDN-Format wird von anderen Bildverarbeitungsprogrammen nicht unterstützt, es ist vor allem für weitere Bearbeitung der Grafiken und Bilder gedacht. Der Bildbetrachter IrfanView kann Bilder im PDN-Format anzeigen (Kompositbild und einzelne Ebenen).\n\nDie Software unterstützt des Weiteren folgende Bildformate: BMP, JPG, GIF, TIF, PNG und TGA. Seit Version 3.10 wird das Format DirectDraw Surface (DDS) unterstützt. Über Plug-ins können noch weitere Dateiformate gelesen und geschrieben werden (zum Beispiel PSD).\n\nPaint.NET bekam von dem britischen Web-Magazin „Webuser“ den „Webuser Gold Award 2006“. In allen Teilen der Bewertung erhielt die Software fünf von fünf Sternen. Ein Zitat aus der Bewertung lautet: \n\nPaint.NET belegte den 19. Platz unter den Top-100-Produkten 2007 im Computer-Magazin \"PC World\".\nEin Kritikpunkt von Paint.NET ist, dass keine Offlinehilfe vorhanden ist. Die Hilfe gibt es nur online und ausschließlich auf Englisch. Das Programm hat außerdem keine eigene Funktion zum Drucken der Dokumente, sondern benutzt die Druckhilfe von Windows. Paint.NET „vergisst“ zudem jegliche Farbinformationen für Pixel, die in Transparenz unterstützenden Formaten mit voller Transparenz bemalt wurden. Die Farbkanäle werden in dem Fall auf (0, 0, 0) (schwarz) eingestellt. Man sieht bei der normalen Bildbearbeitung keinen Unterschied, jedoch lassen sich bei Texturen in 3D-Grafik, welche die Transparenz nutzen (zum Beispiel Schrift-Glyphen), aufgrund der schnellen Interpolationsmethoden moderner Hardware schwarze Ränder erkennen.\n\n\nDie auf dem .NET-Framework basierende Software ist nur unter Windows lauffähig. Die Initiative \"Paint Mono\" strebt daher Kompatibilität zur freien .NET-Implementierung Mono an, welche neben Windows auf anderen Betriebssystemen wie Linux und macOS lauffähig ist. Seit März 2009 wurden allerdings keine Updates für \"Paint Mono\" mehr veröffentlicht.\n\n"}
{"id": "968634", "url": "https://de.wikipedia.org/wiki?curid=968634", "title": "AnyDVD", "text": "AnyDVD\n\nAnyDVD ist eine Software für das Betriebssystem Windows. Sie befreit Video-DVDs unter anderem von Kopierschutzmechanismen wie zum Beispiel ARccOS oder Macrovision, Regionalcodes und weiteren Nutzungseinschränkungen. Weitere Fähigkeiten des Programms sind die automatische Drehzahldrosselung von DVD-Laufwerken nach Einlegen einer Video-DVD zwecks Lärmreduzierung, die halbtransparente Darstellung von Untertiteln, die Umwandlung von Zwangsuntertiteln in optionale Untertitel und das Wechseln der Bildwiederholfrequenz abhängig vom Videostandard (PAL oder NTSC) eines eingelegten Mediums. Auch kann das Programm vorgeschaltete Trailer und Hinweise auf einem Medium automatisch überspringen und wahlweise direkt zum Navigationsmenü oder zum Hauptfilm springen.\n\nAnyDVD wurde bis Februar 2016 von dem auf Antigua und Barbuda ansässigen Unternehmen SlySoft entwickelt. Am 24. Februar 2016 gab Slysoft bekannt, die Entwicklung und den Vertrieb des Programms aus rechtlichen Gründen einzustellen. Das Unternehmen befand sich viele Jahre lang im Rechtsstreit mit dem Lizenzverwalter AACS LA. Weiterhin hatte dieser Anfang Februar 2016 einen Antrag an die \"United States Trade Representative\" (USTR) gestellt, Antigua und Barbuda, dem Hauptsitz von Slysoft, auf die \"Copyright Priority Watch List\" zu setzen. Genaue Gründe für die Einstellung der Aktivitäten wurden von Slysoft jedoch nicht mitgeteilt.\n\nAnschließend übernahm das (nach dem Programm-Logo benannte) Unternehmen RedFox mit Sitz in Belize die Weiterentwicklung der Software. Seit März 2016 ist das Programm daher erneut in verschiedenen Nachfolgeversionen auf der RedFox-Seite verfügbar, bei deren erster, Version 7.6.9.0 (02/16), für nichtregistrierte Benutzer eine 21-tägige Testphase, jedoch ohne Zugriff auf die Online-Datenbank möglich war. Bei Version 7.6.9.1 (03/16) dagegen entfiel die Testphasen-Option, dafür war nun wieder der Zugriff auf die Online-Datenbank möglich, doch nur für Inhaber einer gültigen AnyDVD-HD-Lizenz.\n\nIn der Folge hat RedFox den Verkauf der \"AnyDVD\"-Software gänzlich eingestellt und stellt Käufern nur noch \"AnyDVD HD\" zum Download bereit, wobei Version 7.6.9.5 die letzte noch mit einer Slysoft-Lizenz nutzbare Programmversion ist – für alle nachfolgenden Programmversionen muss eine neue RedFox-AnyDVD-HD-Lizenz erworben werden.\n\nDas Programm arbeitet als Gerätetreiber (Filtertreiber) im Hintergrund und entfernt einen eventuell vorhandenen Kopierschutz einer Video-DVD vor der Übergabe des Signals an Abspiel- oder Verarbeitungssoftware. Das Betriebssystem erkennt das Video somit als ungeschützt an und ermöglicht anderen Anwendungen dessen entsprechende Nutzung. Seit der im Februar 2007 erschienenen Version 6.1.2.3 wird das Programm auch mit einer optionalen, kostenpflichtigen Erweiterung namens \"AnyDVD HD\" angeboten. Diese ermöglicht zusätzlich das Umgehen des HDCP- bzw. AACS-Kopierschutzes von HD DVDs. Eine Unterstützung von Blu-ray Discs wurde Anfang März 2007 mit der Version 6.1.3.0 nachgerüstet. Ab der Version 6.1.8.4 verfügt AnyDVD über ein sogenanntes KI-Modul, wobei eine künstliche Intelligenz in der Software den Kopierschutz selbstständig analysiert und entscheidet, wie er zu umgehen ist. Mit Veröffentlichung der Version 6.4.0.0 war AnyDVD HD das erste und für über ein Jahr einzige Programm, das den Blu-ray-Schutzmechanismus BD+ vollständig umgehen konnte. Eine spätere modifizierte Version von BD+ kann ab Version 6.5.0.2 umgangen werden, welche Ende des Jahres 2008 erschienen ist.\n\nAb Version 7.0.0.0 kann AnyDVD HD beim Einlegen einer Blu-ray Disc auf Wunsch ein sogenanntes \"Speed Menu\" erstellen, welches anstelle des regulären Menüs sofort (ohne jegliche vorgeschaltete Trailer etc.) angezeigt und automatisch aus den auf der Disc enthaltenen Kapitelinformationen generiert wird. Über dieses Menü kann wahlweise der Hauptfilm, ein bestimmtes Kapitel oder jeder beliebige Track auf der Disc direkt gestartet werden. AnyDVD HD unterstützt das Integrieren dieses Sondermenüs in eine Kopie, indem es die veränderten Menüinformationen an die kopierende Anwendung durchreicht.\n\nMit Version 7.1.6.0 wurde die Möglichkeit eingeführt, den auf einem in die Tonspur eingewobenen digitalen Wasserzeichen basierenden Kopierschutz Cinavia beim Abspielen auf unterstützender Abspielsoftware zu deaktivieren. Der Schutz an sich wird dabei derzeit nicht entfernt, das Abspielen einer mit AnyDVD HD erzeugten Kopie auf einem Hardware-Player mit Cinavia-Unterstützung ist daher nicht möglich. Sofern AnyDVD HD auf dem abspielenden System aktiv ist, wird das Wasserzeichen jedoch so verfälscht, dass Abspielsoftware es nicht mehr erkennt und somit keine Stummschaltung der Tonspur nach ca. 20 Minuten erfolgt.\n\nAnyDVD ermöglicht es auch, den Kopierschutz einiger Audio-CDs zu umgehen – bei den meisten Audio-CDs hängt die Wirksamkeit des Kopierschutzes jedoch ausschließlich vom verwendeten CD- oder DVD-Laufwerk bzw. davon ab, ob die automatische Wiedergabe eventuell auf den Medien enthaltener Programme unterdrückt wird.\n\nSeit April 2015 ist AnyDVD HD in Kombination mit CloneBD dazu in der Lage, Cinavia unter Qualitätsverlust vollständig zu umgehen.\n\nDie häufig zu hörende pauschalisierende Aussage, dass das Programm an sich verboten sei, entspricht nicht den Tatsachen. In vielen europäischen Ländern, auch in Deutschland und Österreich, darf das Programm zwar weder beworben noch verkauft oder verliehen werden, der Besitz selbst ist jedoch unter anderem in Deutschland nicht strafbar. Der Gebrauch ist strafbar, sofern dabei eine technische Schutzvorrichtung umgangen wird und dies nicht ausschließlich zum eigenen Privatgebrauch geschieht.\n\nDer Heise-Verlag, der in einer Meldung vom 19. Januar 2005 in seinem Nachrichtenportal heise online einen Hyperlink auf die Entwicklerwebsite von AnyDVD gesetzt hatte, wurde von mehreren Unternehmen der Musikindustrie verklagt und vom Landgericht München I zur Entfernung des Links verurteilt. Dieses Urteil wurde vom Oberlandesgericht (OLG) München am 28. Juli 2005 bestätigt. Der Heise-Verlag legte daraufhin beim Bundesverfassungsgericht Beschwerde wegen Verletzung der Pressefreiheit ein. Nach deren Zurückweisung aus formalen Gründen hat der Verlag das Hauptsacheverfahren angestrengt. Mit Urteil vom 14. November 2007 (21 O 6742/07) hat das Landgericht München I seine bereits im einstweiligen Rechtsschutzverfahren vertretene Rechtsauffassung nochmals bestätigt. Das OLG München hat die hiergegen gerichtete Berufung am 23. Oktober 2008 (29 U 5696/07) zurückgewiesen. Dem Gericht zufolge war der Eingriff in die Medienfreiheit (Art. 5 Abs. 1 Satz 2 GG), der darin lag, dass dem Heise-Verlag die Setzung des Hyperlinks verboten wurde, durch § 95a Abs. 3 UrhG und die Grundsätze der Teilnehmerhaftung gerechtfertigt. Die Richter begründen das damit, dass dem beklagten Heise-Verlag die Rechtswidrigkeit des Internetauftritts von AnyDVD bei der Linksetzung bekannt war.\n\nAm 14. Oktober 2010 hob der Bundesgerichtshof nach dem Revisionsverfahren das Urteil des OLG München auf und wies die Klage der Musikindustrie vollumfänglich ab. In der Einführung stellte der Senat klar, dass die konkrete Funktion der Linksetzung bewertet werden müsse, da die Verlinkung als Mittel der Berichterstattung grundsätzlich zulässig sei. Eine Anbringung des Links als fußnotenäquivalente Möglichkeit zur reinen Informationsbeschaffung spreche für seine Zulässigkeit. Das Urteil ist rechtskräftig, die Urteilsbegründung wurde am 19. April 2011 vorgelegt. Eine Verfassungsbeschwerde der Musikindustrie, die die Überprüfung des BGH-Urteils zum Ziel hatte, wurde per Beschluss vom 15. Dezember 2011 nicht zur Entscheidung angenommen. In der unanfechtbaren Ablehnungsbegründung bekräftigte das Bundesverfassungsgericht die Auffassung des Bundesgerichtshofs.\n\n"}
{"id": "972268", "url": "https://de.wikipedia.org/wiki?curid=972268", "title": "Berkeley DB", "text": "Berkeley DB\n\nDie Berkeley-Datenbank (Berkeley DB) ist eine eingebettete Datenbank-Bibliothek mit Programmierschnittstellen zu C, C++, Java, Perl, Python, Tcl und weiteren Programmiersprachen.\n\nDie Berkeley DB entstand ab dem Jahr 1991 an der University of California, Berkeley, und wurde im Jahr 1992 als Berkeley DB 1.85 mit der Berkeley Software Distribution (BSD) veröffentlicht. Im Jahr 1996 folgte die Berkeley DB 1.86 aus Harvard für Kerberos, ein Vertrag der ursprünglichen Entwickler mit Netscape, und deren Gründung von Sleepycat Software. 2006 wurde Sleepycat durch Oracle aufgekauft.\n\nBerkeley DB läuft auf einer großen Anzahl von Betriebssystemen, unter anderem auf den meisten Unix-artigen und Windows-Systemen und auch Echtzeitbetriebssystemen.\n\nDie Berkeley DB enthält Kompatibilitätsschnittstellen für einige historische Unix-Datenbankbibliotheken wie \"dbm\", \"ndbm\" und \"hsearch\".\n\nFür die Verwendung in Java-Systemen wird eine Berkeley DB Java Edition angeboten, die als einzelne JAR-Datei in einer virtuellen Java-Maschine eingebunden werden kann. Sie bietet dieselben Funktionen wie die in C geschriebene Berkeley DB.\n\nBerkeley DB XML ist eine Schnittstelle, die die Speicherung von XML-Daten in der in C geschriebenen Berkeley DB unterstützt. Dadurch können XML-Dokumente geparst werden, und die Abfrage-Sprachen XPath und XQuery für Datenzugriffe genutzt werden.\n\nDie Berkeley DB speichert Datensätze bestehend aus einem Schlüssel- und einem Datenteil. Eine weitere Strukturierung der Daten als einzelne Tabellenspalten mit bestimmten Datentypen wird nicht unterstützt.\n\nAb der Version 11G gibt es eine SQLite-kompatible SQL-Schnittstelle, mit der auf die gespeicherten Daten zugegriffen werden kann. Berkeley DB ist ausschließlich für die Verwendung als Eingebettetes Datenbanksystem konzipiert. Programme können die Datenbank nur durch prozessinterne API-Aufrufe verwenden, vergleichbar mit Zugriffen auf ein Dateisystem. Dadurch unterliegen die Programme, die die Berkeley DB als Datenspeicher verwenden, keinen Einschränkungen, in welcher Weise die Daten in einem Datensatz abgelegt werden. Ein Datensatz und sein zugehöriger Schlüssel kann bis zu vier Gigabyte groß sein. Eine Tabelle kann bis zu 256 Terabyte Speicher belegen.\n\nIm Gegensatz zu einem Dateisystem bietet die Berkeley DB viele Funktionen, die ein Datenbanksystem charakterisieren. Sie bietet simultane Threads zum Manipulieren der Daten. Sie bietet Transaktionssicherheit für lesende und schreibende Zugriffe, Lock-Mechanismen, eine XA-Schnittstelle, Backups zur Laufzeit (Hot-Backup) und Replikation.\n\nDie Berkeley DB benutzt denselben Adressraum, den auch die Anwendung verwendet, in die die Datenbank eingebettet ist. Das bedeutet einen Performance-Vorteil gegenüber großen Server-DBMS, die die angeforderten Daten vom permanenten Speichermedium erst in den eigenen Arbeitsspeicher lesen müssen und dann an den Adressraum der Anwendung übergeben muss. Mit einer Größe des Maschinencodes von weniger als 500 kB eignet sich die Berkeley DB auch gut für den Einsatz in Systemen mit einer schwachen Rechnerleistung.\n\nBerkeley DB bietet zwar selbst keine Schnittstellen für Netzwerk-Zugriffe, besitzt aber Replikations-Funktionen für den Einsatz z. B. auf Bladeservern. Ein Blade-Computer fungiert dabei als Master, der Datenänderungen entgegennimmt und diese auf die Replika-Blades verteilt. Dadurch entsteht eine hohe Ausfallsicherheit des Gesamtsystems. Nach Angaben des Herstellers kann durch die Replikation eine Verfügbarkeit von 99,999 % des Gesamtsystems erreicht werden.\n\nWeil die Berkeley DB nicht den Overhead einer großen Server-DB hat, ist auch die erforderliche Administration sehr einfach. Die Datenbank kann sehr variabel konfiguriert werden. Sie ist besonders gut geeignet für den Einsatz in geschlossenen Systemen, die überhaupt keine Administration erfordern bzw. ermöglichen. Bei einem Fehler des Systems startet sich das System selbständig neu und die Betriebsfähigkeit ist in den meisten Fällen wiederhergestellt.\n\nBerkeley DB wird nach Angaben von Oracle mehr als 200 Millionen Mal eingesetzt, unter anderem von namhaften Telekommunikations-, Netzwerk- und Hardwareanbietern:\n\nBerkeley DB wird häufig in folgenden Systemen eingesetzt:\n\nNachfolgend eine Liste von beachtenswerten Programmen, die Berkeley DB zur Datenspeicherung verwenden:\n\n\nDie folgenden Programme haben Berkeley DB in der Vergangenheit zur Datenspeicherung verwendet. Die Unterstützung soll jedoch in künftigen Versionen aufgegeben werden oder wurde bereits aufgegeben:\n\n\nDie Versionen 2.0 und höher von Berkeley DB sind unter einer Duallizenz verfügbar. Man hat die Wahl zwischen einer kommerziellen Lizenz und der Sleepycat, einer Open-Source-Lizenz. Nutzer, die die DB mit proprietärer Software ausliefern wollen, müssen sich lizenzieren lassen. Die Kosten belaufen sich dabei bei lebenslangen Lizenzen je nach Version pro Prozessor zwischen 180 USD und 13.800 USD (Stand September 2014) und enthalten lebenslange Updates sowie ein Jahr Support.\n\nAb der Version 6.0 lizenziert Oracle alle Produkte der Berkeley DB Reihe unter der GNU AGPL v3.\n\nDie Versionen vor 2.0 stehen unter der BSD-Lizenz, womit sie auch kommerziell frei genutzt werden können.\n\n"}
{"id": "972790", "url": "https://de.wikipedia.org/wiki?curid=972790", "title": "TOP500", "text": "TOP500\n\nTOP500 ist eine Liste der 500 schnellsten Computersysteme und ihrer Kenndaten. Die Liste wird nach dem Rmax-Wert des jeweiligen Computers bei Verwendung der High-Performance LINPACK Benchmark sortiert und stellt damit eine Rangfolge der leistungsfähigsten Maschinen zur Lösung linearer Gleichungssysteme dar. Seit Juni 2008 wird auch der Energieverbrauch gelistet.\n\nHervorgegangen ist die TOP500 aus Hans-Werner Meuers von 1986 bis 1992 jährlich publizierter \"Mannheimer Supercomputer-Statistik\". Darin wurden nur die in den USA, Japan und Europa installierten Vektorcomputer-Systeme gezählt. Die Zahlen basierten dabei auf Angaben der Hersteller. Die schwierige Datenlage vor allem in Japan, die zunehmende Verbreitung massiv paralleler Systeme und von Hochleistungsrechnern allgemein machten eine Neuorganisation nötig.\n\nUm die Liste auf eine bessere und überprüfbare Basis zu stellen, übernahm danach die Organisation TOP500, die von den Universitäten Mannheim und Tennessee sowie dem \"National Energy Research Scientific Computing Center\" repräsentiert wird, die Zusammenstellung der Liste. Die Festlegung auf 500 Einträge erfolgte, weil einerseits die letzte Mannheimer Supercomputerliste 530 Einträge hatte, andererseits mit Bezug auf die Forbes-500-Liste der erfolgreichsten Unternehmen. Seit Juni 1993 wird die TOP500 zweimal jährlich erarbeitet und abwechselnd auf der in Deutschland stattfindenden Internationalen Supercomputer-Konferenz und der in den USA stattfindenden \"Supercomputer Conference\" vorgestellt.\n\nDie schnellsten Computer weltweit (Stand: November 2018):\n\nDie sechs schnellsten Supercomputer in der EU – mit nur noch 28 (zuvor 30) unter den ersten Hundert\n\nDie sechs schnellsten Supercomputer in Deutschland – bei neun unter den ersten hundert – sind (Stand: November 2018):\n\nDie zwei schnellsten Computer in der Schweiz sind (Stand: November 2018):\n\nDer schnellste Supercomputer Österreichs, der VSC-3, kommt mit 596 TFLOPS nicht unter die 500 Schnellsten.\n\nIm Juni 2013 lag der Vorgänger VSC-2 mit 20.776 Opteron-Kernen und 152,9 TFLOPS auf Platz 238. VSC-2 und VSC-3 sind die zweite und dritte Ausbaustufe des \"Vienna Scientific Cluster\", eines gemeinsamen Projekts der Technischen Universität Wien, der Universität Wien und der Universität für Bodenkultur Wien.\n\nAlle Supercomputer dieser Liste werden mit Linux-Derivaten betrieben (Stand: Nov. 2017).\n\nDie Green500 sind eine seit 2009 veröffentlichte alternative Reihung der TOP500-Liste. Sie sortiert die TOP500-Supercomputer nach ihrer Energie-Effizienz, gemessen in GFLOPS pro Watt. Der Energiebedarf des Kühlsystems bleibt dabei unberücksichtigt.\n\nAktuelle Listen zeigen, dass massiv parallele Prozessoren mit mehreren Tausend kleinen Rechenkernen energieeffizienter arbeiten als Allzweck-Rechenkerne wie Intels Xeon-Reihe: Im Juni 2017 basierten die zehn energieeffizientesten Green500-Systeme auf hochparallelen Grafikprozessoren (Nvidia Tesla). Im November 2017 basierten die drei besten Systeme auf Prozessoren mit 2048 Rechenkernen je Chip (PEZY-SC2) – bei einer Energie-Effizienz von rund 17 GFLOPS/Watt.\n\n\n\n"}
{"id": "974751", "url": "https://de.wikipedia.org/wiki?curid=974751", "title": "Shake (Software)", "text": "Shake (Software)\n\nShake war ein Compositing-Programm, das in der Postproduktion zur Verarbeitung von digitalen Filmaufnahmen und computergenerierten Bildern mit Bildverarbeitungseffekten eingesetzt wurde. Das Programm arbeitet mit einem Node-basierten Workflow und unterstützt eine Farbtiefe von bis zu 32 Bit pro Kanal.\n\nUrsprünglich wurde Shake von der Firma Nothing Real als reines Kommandozeilen-Werkzeug, also ohne grafische Benutzeroberfläche, entwickelt. Auf der Broadcast-Messe NAB Show in Las Vegas wurde das Programm für Windows und Linux 1999 zum ersten Mal vorgestellt. Im Februar 2002 kaufte Apple Nothing Real auf. Mit der Vorstellung der Version 4.1 für macOS im Juni 2006 beendete Apple offiziell die Entwicklung von Shake. Seit 30. Juli 2009 leitet der Link zu Shake auf der Apple Homepage nur mehr auf die Seite von Final Cut Pro weiter. Alternativen zu Shake sind Nuke von The Foundry, After Effects von Adobe, Combustion von Autodesk, sowie Natron oder Blender als Open-Source-Alternative.\n\n\nSieben Jahre in Folge ging der Oscar für Visuelle Effekte an Filme, bei denen neben Produkten von Avid, discreet, Alias, eyeon und zahlreichen anderen Herstellern auch mit Hilfe von Shake produziert wurde.\n\n\n"}
{"id": "976911", "url": "https://de.wikipedia.org/wiki?curid=976911", "title": "Windows on Windows", "text": "Windows on Windows\n\nWindows on Windows, allgemein bekannt als WOW oder WoW, ist ein Subsystem von Microsoft Windows, welches das Ausführen von 16-Bit-Programmen für Windows 3.x und älter auf 32-Bit-Windows-Systemen erlaubt. \n\n"}
{"id": "978885", "url": "https://de.wikipedia.org/wiki?curid=978885", "title": "Connect (Zeitschrift)", "text": "Connect (Zeitschrift)\n\nConnect (Eigenschreibung connect) ist eine Zeitschrift für Telekommunikation. Sie gehört zum Verlag WEKA Media Publishing GmbH der \"WEKA Holding\". \n\nThemen der seit 1992 erscheinenden Zeitschrift sind vor allem Hardwaretests von Mobiltelefonen, Notebooks, Routern, Navigationssystemen, Freisprecheinrichtungen, PDAs, Streaming-Clients und Ähnlichem. Neben der Hauptzeitschrift erscheint zusätzlich alle zwei Monate das Magazin NAVIconnect, das ursprünglich aus einem Sonderheft der Autohifi hervorgegangen ist und sich hauptsächlich Navigationssystemen und den dazugehörigen Anwendungen und Komponenten widmet.\n\nChefredakteur ist seit 2004 Dirk Waasen, der in dieser Funktion auch das 2013 eingestellte Magazin Autohifi leitete. Connect erscheint in verschiedenen europäischen Ländern und China als Lizenzausgabe.\n\nDer Redaktionssitz befand sich bis 2015 in Stuttgart, seitdem ist er Haar bei München.\n\nIm vierten Quartal 2014 lag die durchschnittliche verbreitete Auflage nach IVW bei 79.436 Exemplaren. Das sind 7.055 Exemplare pro Ausgabe weniger (–8,16 %) als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 2.608 Abonnenten auf durchschnittlich 11.810 pro Ausgabe ab (–18,08 %); damit bezogen rund 14,87 % der Leser die Zeitschrift im Abo. Die steigende Verbreitung im vierten Quartal 2016, gegen den allgemeinen Trend, wird durch eine starke Erhöhung von Bordexemplaren um 58 % zum Vorjahresquartal erreicht. Das sind 20 % der verbreiteten Exemplare.\n\n"}
{"id": "979981", "url": "https://de.wikipedia.org/wiki?curid=979981", "title": "A- und B-Strategie", "text": "A- und B-Strategie\n\nA- und B-Strategie sind Begriffe aus der Programmierung von Strategiespielen, insbesondere dem Computerschach.\n\nEin Programm zum Spielen eines Strategiespiels betrachtet üblicherweise einen Suchbaum, der ein Teil des Spielbaums ist.\n\nA-Strategie bezeichnet nach Claude Shannon ein Verfahren, welches zur Bestimmung des besten Zuges alle möglichen Kombinationen von Zügen und Gegenzügen durchrechnet, bis zu einer bestimmten Tiefe (Zahl der aufeinanderfolgenden Züge), die durch Rechenleistung und verfügbare Zeit begrenzt wird. Die erreichten Stellungen werden heuristisch bewertet, und die Züge, die dazu geführt haben, werden nach dem Minimax-Prinzip bewertet. Die A-Strategie bezeichnet man auch als \"Brute-Force-Methode\".\n\nIm Gegensatz zur A-Strategie spielt ein Schachprogramm gemäß der B-Strategie, wenn es nur plausible – und nicht alle – Zugfolgen bei der Analyse einer Stellung durchsucht. Die Züge werden heuristisch bewertet, und nur solche mit hohem Wert werden in den Suchbaum aufgenommen. Die B-Strategie wird mitunter als Versuch verstanden, den menschlichen Denkprozess bei der Analyse von Varianten nachzubilden. Der Mensch berechnet auch nicht alle in einer Stellung legalen Züge, sondern erkennt bestimmte Merkmale der Stellung und wählt dann Züge zur genaueren Betrachtung aus, die im Hinblick auf die Stellungsmerkmale aussichtsreich sind.\n\nWeil die Zahl der Varianten mit der Tiefe langsamer wächst als bei der A-Strategie, können die Varianten entsprechend bis zu einer größeren Tiefe berechnet werden. Dadurch kann das Programm taktische Kombinationen erkennen, die jenseits des Horizonts der A-Strategie liegen. Dafür übersieht es aber manchmal eine Wendung, indem es einen guten Zug niedrig bewertet und dadurch von vornherein aus dem Suchbaum ausschließt.\n\nDer erste Versuch, ein solches Schachprogramm zu schreiben, wurde 1955 bis 1958 von Allen Newell, John Shaw und Herbert A. Simon unternommen. Er schlug praktisch fehl, und man begann zu verstehen, dass die Realisierung eines solchen Programms weit schwieriger ist, als man zunächst angenommen hatte.\nModerne Schachprogramme verwenden überwiegend modifizierte Formen der A-Strategie, die einzelne aussichtsreiche oder taktisch kritische Varianten tiefer als andere verfolgen.\n\nAnders als im Computerschach stoßen beim Brettspiel Go die Brute-Force-Methoden an ihre Grenzen. Hier ist eine Forschung in Richtung intelligenter Lösungsansätze (B-Strategie) anders als im Schach noch in vollem Gange.\n"}
{"id": "981923", "url": "https://de.wikipedia.org/wiki?curid=981923", "title": "Atari Portfolio", "text": "Atari Portfolio\n\nDer Atari Portfolio ist ein tragbarer 16-Bit-Personal-Computer, der 1989 von der Firma Atari vorgestellt wurde. Mit etwa der Größe einer VHS-Videokassette und einem Gewicht von ca. 500 Gramm kann er als ein Vorgänger späterer PDAs bezeichnet werden. Der Portfolio ist im Wesentlichen kompatibel zum IBM-PC, was ihn zu einem universell einsetzbaren Computer machte. Mit eingebauten Programmen, die sich über Funktionstasten starten lassen, kann u. a. Adress- und Terminverwaltung, Texteingabe und Tabellenkalkulation betrieben werden. Mit geeigneter Software ist er aber auch zum Programmieren, Spielen und zum Steuern und Regeln geeignet.\n\nDas Gerät kostete in der Grundausstattung in den USA etwa 400 US-Dollar, in Deutschland wurde der „Intelligente Bleistift“ (Atari-Werbeslogan für den Pofo) anfangs für 999 DM angeboten.\n\nDer Portfolio besitzt eine Schreibmaschinen-ähnliche Tastatur, die sich trotz ihrer kleinen Tasten recht gut bedienen lässt. Der monochrome Flüssigkristallbildschirm hat eine Auflösung von 240×64 Bildpunkten, und ist CGA-kompatibel, wobei die Darstellung in rein Monochrom gewandelt wird. Als CPU wird eine mit 4,92 MHz getaktete, stromsparende Variante des Intel 8088 mit der Bezeichnung 80C88 verwendet, die auf einen 128 KB großen Arbeitsspeicher und 256 KB Festwertspeicher zurückgreifen kann.\n\nAls Energiequelle für unterwegs dienen drei Batterien der Bauform AA. Stationär ist auch der Betrieb über ein Netzteil möglich.\n\nAls Wechselmedium bot Atari spezielle Speicherkarten (Bee-Karten) an, die mit Kapazitäten von 32 KB bis 128 KB verfügbar waren. Mit externen Adaptern lässt sich der Portfolio mit serieller und paralleler Schnittstelle, aber auch mit weiteren 512 KB Arbeitsspeicher ausrüsten. Alle Erweiterungen werden an einer Schnittstelle an der rechten Geräteseite angesteckt, worunter die Flexibilität des Gerätes bei mobilen Anwendungen etwas leidet.\n\nDas eingebaute DIP-DOS-(MS-DOS 2.11)-kompatible Betriebssystem macht das Gerät kompatibel zu einer Reihe verbreiteter MS-DOS-Anwendungen. Das DIP-DOS beinhaltet auch einige interne Anwendungen, die durch den Befehl \"app\" oder das Drücken der Tastenkombination ATARI+A gestartet werden konnten. Das Auswahlmenü bietet eine einfache Textverarbeitung, letztendlich nichts anderes als ein einfacher Texteditor, eine zu Lotus 1-2-3 kompatible Tabellenkalkulation, eine einfache Adressverwaltung, einen Terminkalender mit Weckfunktion und einen Taschenrechner. Die Adressverwaltung hat eine Besonderheit, sie kann eingetragene Telefonnummern als Töne abspielen. Wenn man das Mikrofon eines Telefons an den Lautsprecher hielt, kann dies zur komfortablen Tonwahl verwendet werden. Das Auswahlmenü beinhaltet auch eine Funktion zur Datenübertragung zum PC und ein System-Menü, über das man interne Einstellungen des Systems anpassen kann.\n\nDie Datenübertragung zum „Pofo“, wie er von seinen Anhängern liebevoll genannt wird, erfolgte zum PC und Atari ST wahlweise über ein Terminalprogramm über die als Zubehör erhältliche serielle Schnittstelle, über ein spezielles Parallelport-Kabel und spezielle Software, die für PCs mitgeliefert wurde und im Portfolio schon eingebaut ist, oder über Speicherkarten, die der Portfolio als Laufwerk A: und B: ansprechen konnte. Das Laufwerk B: war ein weiterer Kartenslot in der 512 KB-Erweiterung. Für den PC gab es ein passendes externes Kartenlaufwerk, das über eine spezielle ISA-Karte angesteuert wurde. Findige Programmierer schufen allerdings auch einen Treiber für den Portfolio, mit dem man ein ZIP-Laufwerk (100 MB) an der Parallelport-Erweiterung betreiben konnte. Auf eine ZIP-Diskette passte dann locker die gesamte freie Software, die es speziell für den Rechner gab.\nMittlerweile gibt es sogar einen Adapter, um CompactFlash-Karten daran zu betreiben; dieser ist allerdings recht selten und dementsprechend teuer.\n\nDer Arbeitsspeicher war bereits für die späten 1980er Jahre sehr knapp bemessen und auch die ungewöhnliche Auflösung des Displays bereitete vielen Anwendungen Probleme. Trotzdem fand der Portfolio viele Anhänger, die ihm zum Teil bis heute treu geblieben sind.\n\nIm Spielfilm \"Terminator 2 – Tag der Abrechnung\" wird von John Connor ein Atari Portfolio zur Manipulation von Geldausgabeautomaten oder Sicherheitszugängen benutzt. Im Film sind sämtliche Produktmerkmale und Typenschilder zu erkennen (Produktplatzierung).\n\n"}
{"id": "982489", "url": "https://de.wikipedia.org/wiki?curid=982489", "title": "IEEE 754-2008", "text": "IEEE 754-2008\n\nDer Standard IEEE 754-2008, der frühere Arbeitstitel lautete IEEE 754r, ist eine notwendig gewordene Revision des 1985 verabschiedeten Gleitkommastandards IEEE 754. Der alte Standard war sehr erfolgreich und wurde in zahlreichen Prozessoren und Programmiersprachen übernommen. Die Diskussion über die Revision begann im Jahr 2001; im Juni 2008 wurde der Standard angenommen und im August 2008 verabschiedet.\n\nDie Hauptziele des verabschiedeten Standards konnten aufgeteilt werden in\n\nDer Standard soll Formate und Methoden für Gleitkommaarithmetik sowie eine Mindestqualität definieren.\n\nFormate umfassen Gleitkommazahlen mit halber (16 Bit), einfacher (32 Bit), doppelter (64 Bit) sowie vierfacher (128 Bit) Genauigkeit. Das Halbformat stellt ein standardisiertes Minifloat dar.\nErgänzt werden die Grundformate durch erweiterte (extended) und erweiterbare (neu!) Langzahl-Formate. Ebenfalls neu aufgenommen wurden Datenaustauschformate.\nNeben der 16/32/64/128-Bit-Darstellungen sind Darstellungen mit einem Vielfachen von 32 Bits definiert.\n\nDicht gepackte Dezimalformate (DFP, 3 Ziffern in 10 Bit) sind ebenfalls dazugekommen. Sie weichen von klassischen einzelzifferbasierten BCD-Formaten folgendermaßen ab:\n\nSignaling NaNs wurden zur Streichung vorgeschlagen (3. Februar 2003), später aber wieder in den Vorschlag aufgenommen (21. Februar 2003).\nEine Signaling NaN ist eine NaN mit gesetztem Bit 7.\nDarstellungen von formula_2 existieren und sind leicht erkennbar.\n\nZu den vier alten IEEE-754-Rundungen kommt eine zusätzliche hinzu, so dass folgende Rundungen gefordert werden:\n\n\nDie IEEE 754-Rundung (next even) wurde schon von Carl Friedrich Gauß vorgeschlagen und vermeidet ein statistisches Ungleichgewicht bei längeren Rechnungen zu größeren Zahlen hin.\n\nIn der Diskussion um den neuen Standard wird diese Erkenntnis offensichtlich wieder verworfen und die „Handrechnungsrundung“ (to next) wieder eingeführt.\n\nAusnahmebedingungen und Ausnahmebehandlung werden spezifiziert.\n\nNeue Funktionen sind Prädikatfunktionen (größer gleich) und Operatoren für Maximum und Minimum. Hier wird vor allem über die Ergebnisse bei den Sonderwerten (NaN, Inf) diskutiert.\n\nDie primäre Idee hinter der dicht gepackten Dezimaldarstellung ist, dass diese mit extrem wenig (Gatter-)Aufwand in eine klassische BCD-Darstellung für die Mantisse sowie einen binären Exponenten umkodiert werden kann, aber gleichzeitig den Speicherplatz so effizient wie möglich ausnutzt. Die eigentliche Verarbeitung findet dann im klassischen BCD-Format statt, nur beim Lesen und Schreiben von Registern ist eine Umkodierung erforderlich.\n\nDie Kodierung von 32-bit-, 64-bit- und 128-bit-dezimalkodierten Zahlen erfolgt nach folgendem Schema.\nFür längere Dezimalkodierungen werden für jedes weitere 32-bit-Wort dem Exponenten 2 bit und der Mantisse 30 bit (3× 10 bit) zugeschlagen, so dass unter Beibehaltung des 5-bit-Kombinationsfeldes der Wertebereich des Exponenten sich vervierfacht und die Mantisse weitere neun Ziffern erhält.\n\nDie Zahl besteht aus\n\nZur Dekodierung und Kodierung werden folgende Kodiertabellen benötigt:\n\n\nDie Absichten hinter dezimalen Gleitkommazahlen sind:\n\nDie Probleme sind:\n\nDie Ergebnisse sind:\n\nSinnvoll sind sie:\n\nHier prallen zwei gegensätzliche Standpunkte aufeinander.\n\nManche Experten gehen sogar soweit, zu behaupten, dass duale Arithmetik in Zukunft kaum noch eine Rolle spielen wird. Ein zugegeben polemisches Zitat zu diesem Thema stammt vom „Gleitkomma-Altmeister“ William Kahan:\n\nEr übersieht dabei aber, dass\n\n"}
{"id": "982850", "url": "https://de.wikipedia.org/wiki?curid=982850", "title": "Autodesk Inventor", "text": "Autodesk Inventor\n\nAutodesk Inventor ist eine auf Modellierungselementen aufbauende, parametrische 3D-CAD-Software, die von dem Unternehmen Autodesk entwickelt und vertrieben wird. „3D“ heißt, dass damit räumliche Modelle erzeugt werden. Dabei werden sämtliche Modellierschritte (Elemente) sowie alle zugehörigen Maße (Parameter) einzeln und zugeordnet gespeichert, das heißt Modelle sind auch nachträglich durch Veränderung der Eingabewerte gezielt und kontrolliert beeinflussbar. Da dieses Prinzip auch für die Baugruppen gilt, können damit auch mechanische Bewegungsabläufe ohne weitere Hilfsmittel als Videosequenz dargestellt werden.\n\nDie Erstellung der nötigen Zeichnungen von Einzelteilen und Baugruppen ist ein gesonderter Arbeitsschritt, bei dem die vorher erstellten 3D-Modelle lediglich in druckbarer Form dargestellt und mit ergänzenden Kommentaren versehen werden. Die Zeichnungen werden dabei assoziativ mit den Modellen verknüpft, das heißt bei allen Änderungen an den Modellen (3D) werden die Zeichnungen (2D) automatisch nachgeführt.\n\nAutodesk-Inventor ist neu entwickelt und basiert nicht auf AutoCAD. Es wird in der Anwendung auch grundsätzlich anders gehandhabt.\nAls Geometrie-Kern wird der Autodesk-eigene ShapeManager genutzt, der auch in anderen Autodesk-Anwendungen wie beispielsweise AutoCAD verwendet wird. ShapeManager wurde vom ACIS-Geometrie-Kern abgeleitet.\n\nEine Modellierungsmethode besteht darin, sämtliche Einzelteile als eigene Teile zu modellieren. Dann fügt man sie in einer Baugruppe unter Zuhilfenahme sogenannter Abhängigkeiten zusammen. Abhängigkeiten können Flächen, Achsen, Punkte und Linien miteinander verknüpfen und somit Bewegung und Position zueinander einschränken oder fixieren. Ändert sich die Geometrie eines Teiles der Baugruppe, so ändern sich auch alle von diesem Teil abhängig gemachten Teile. Neben Position und Bewegung eines Bauteils kann dabei auch seine Geometrie von einem anderen Teil beeinflusst werden. Zudem ist es auch möglich, Bauteile direkt in einer Baugruppe zu modellieren.\n\nEine andere Methode, die Bauraum-Methode, geht zuerst von einer vereinfachten Grundform aus. Diese Grundform kann eine 2D-Skizze oder ein 3D-Modell sein, in das Platzhalter für die zu verbauenden Teile eingetragen sind. Aus der Grundform werden dann mittels Datenverknüpfung die einzelnen Bauteilmodelle passend herausgeschnitten oder aufmodelliert und mit den erforderlichen Details ergänzt. Man erhält so automatisch zueinander passende Bauteilmodelle, die als Abkömmlinge des gemeinsamen Mastermodells alle an diesem vorgenommenen Maßänderungen an ihren eigenen entsprechenden Stellen übernehmen.\n\nAutodesk Inventor wurde speziell für die mechanische Konstruktion konzipiert und findet insbesondere Verwendung in Maschinenbau, Werkzeugbau, Blechverarbeitung und Anlagenbau.\n\nDas Paket besteht aus mehreren Komponenten. Die eigentliche Autodesk-Inventor-Software ist ein 3D-Modellierpaket mit der Möglichkeit, parametrische 3D-Modelle und -Baugruppen zu erstellen. Davon können 2D-Zeichnungen abgeleitet und Animationen erstellt werden. Erweiterungen stellen für spezielle Anwendungen und Branchen leistungsfähige Extra-Funktionen zur Verfügung.\nMit Inventor Studio lassen sich fotorealistische 3D-Darstellungen ableiten.\n\nAutodesk Inventor wird zwar alleine angeboten, aber das 3D CAD-Programm wird meist in Paketen (engl.: Bundles) zusammen mit anderen Autodesk Lösungen angeboten, die sich ergänzen. Ein solches Paket wird von Autodesk als „Suite“ bezeichnet. Diese Suiten werden meist in drei Ausbaustufen angeboten (im Umfang ansteigend): Standard, Premium und Ultimate. In den Paketen „Standard“ und „Premium“ sind meist Inventor „Grundpakete“ beinhaltet. In den „Ultimate“-Suiten wird die „Inventor Professional“ Version mit umfangreichen Erweiterungen und Berechnungsmodulen angeboten. Häufige Suiten sind: „Product Design Suite“, „Factory Design Suite“ und weitere. Die Vielzahl von angebotenen Suiten und deren bis zu drei Ausbaustufen bietet für viele Anwendungsfälle eine Lösung, ist aber in Summe dadurch nicht sehr übersichtlich.\n\n\nZusätzlich zum Grundpaket gehören weiterhin:\n\nEine preislich günstigere, in der Funktionalität eingeschränkte Version ist die „Autodesk Inventor LT“ -Version: sie ist als ergänzender Arbeitsplatz in einem Konstruktionsteam konzipiert und auf das Erstellen von Einzelteilen und deren 2D-Ableitungen beschränkt. Baugruppen können nicht bearbeitet werden. Dieses Produkt wird einzeln oder wiederum als Bundle, also als \"Suite\", zusammen mit AutoCAD LT angeboten.\n\n\nAktuell ist die Version Autodesk Inventor 2017, die zusammen mit AutoCAD 2017 und AutoCAD Mechanical 2017 im April 2016 erschienen ist. Autodesk vergibt im Frühjahr eines Jahres die Versionsnummer des Folgejahres.\n\nErsterscheinung der InventorversionenBei den Versionen 1 bis 10 standen bekannte Autonamen Pate für die interne Bezeichnung.Ab der Version 11 (\"Faraday\") sind es berühmte Pioniere, deren Namen mit der internen Bezeichnung geehrt wird.\nAutodesk verkauft, so wie fast alle anderen Softwarehersteller, keine Software. Es wird nur die Nutzung der Software an den Endkunden „\"lizenziert\"“. Der Endkunde kauft also eine „Lizenz zur Nutzung“, aber der Besitzer der Software bleibt dabei immer der Hersteller. Diese Lizenzen können wie folgt erworben werden:\n\nKritik: ein Kunde mit mehreren SLM-Lizenzen kann sehr schnell, auch ohne besondere böse Absicht, zu viele Lizenzen aktivieren. Eine saubere Buchführung (modern: „Lizenzmanagement“) über die eingesetzten Versionen auf den jeweiligen PCs ist daher empfehlenswert!\n\nAutodesk bietet, wie fast alle namhaften Softwarehersteller, für den Einsatz zur Forschung und Lehre an Schulen (Haupt-, Real-, Gymnasial-, Fach- und Berufsschulen), Universitäten, zum Studium und kursbegleitet zu Schulungen kostenlose Schulversionen an. Denen gemeinsam ist immer jeglicher Ausschluss einer kommerziellen, also gewinnbringenden Tätigkeit. Vom Funktionsumfang sind diese identisch mit der jeweils größten kommerziellen Vollversion, aber in der Laufzeit begrenzt. Früher wurde beim Speichern und beim Ausdrucken eine (für den Schulbetrieb vollkommen unerhebliche) nicht entfernbare Kennung in die Datei und in die ausgedruckte Zeichnung eingefügt. Ab der Version 2013 verzichtet Autodesk auf diese Kennzeichnung. Trotzdem wird dringend davon abgeraten, 3D-Einzelteile in kommerzielle 3D-Zusammenbauten einzufügen. Bezugsberechtigt sind Schüler an oben genannten öffentlichen Schulen, Studenten, Auszubildende, Teilnehmer an IHK-, HK-Meisterkursen, Volkshochschulen (VHS) und Dozenten. Diese können die Software (Umfang ca. 6–20 GB) beim Hersteller kostenlos herunterladen. Die Software muss über das Internet persönlich registriert und auch aktiviert werden. Die Laufzeit ist auch hier auf (typisch) 36 Monate begrenzt.\n\nAnerkannte Schuleinrichtungen in staatlicher Trägerschaft können Schullizenzen (in 125er Lizenzschrittgrößen) als Standalone (SLM: auf den PC gebunden) oder auch als NLM (Netzwerklizenz) im Unterricht kostenlos einsetzen. Auch diese Schullizenzen ist jeweils auf drei Jahre Laufzeit begrenzt.\n\nDirekt aus Inventor heraus kann auf die umfangreiche Onlinehilfe zugegriffen werden. Ebenso gibt es auf Grund der guten Verbreitung von Inventor von vielen namhaften Verlagen eine Vielzahl von Bücher unter den Schlagworten \"Autodesk Inventor\". Bei bekannten Videostreaming-Diensten und anderen Bezugsquellen gibt es für Anwender, die nicht gerne lesen, auch viele kürzere oder längere kostenlose und käufliche zeitgemäße Videotrainings.\n"}
{"id": "984128", "url": "https://de.wikipedia.org/wiki?curid=984128", "title": "CCCeBIT", "text": "CCCeBIT\n\nDer CCCeBIT-Award war ein Negativpreis des Chaos Computer Clubs und reichte auf eine lange Tradition von CeBIT-Treffen zurück. In den frühen 1990er Jahren wurde jährlich eine Demonstration auf dem Messestand der Deutschen Telekom veranstaltet. Ab 2001 wurde die Kundgebung durch die Verleihung der Auszeichnung an andere Organisationen und Unternehmen ersetzt, die in den Augen der Veranstalter soziale Grundsätze wie Meinungsfreiheit, Datenschutz und Informationelle Selbstbestimmung verletzen. Seit 2008 findet keine Preisverleihung mehr statt.\n\n"}
{"id": "985173", "url": "https://de.wikipedia.org/wiki?curid=985173", "title": "Universal Chess Interface", "text": "Universal Chess Interface\n\nDas Universal Chess Interface (UCI) ist ein offenes Schach-Kommunikationsprotokoll, welches von Schachengines genutzt wird, um mit der grafischen Benutzeroberfläche (GUI) zu kommunizieren.\n\nEs wurde im November 2000 von Rudolf Huber und Stefan Meyer-Kahlen, dem Autor von Shredder, entwickelt und ist nicht kompatibel zum älteren und weit verbreiteten XBoard-Protokoll. Beide Protokolle sind ohne Lizenzgebühren nutzbar.\n\nNachdem ChessBase das \"Universal Chess Interface\" im Jahr 2002 in seine Software integriert hatte, fand das Protokoll in der Folge weite Verbreitung. Eine Vielzahl von Schachprogrammen und GUIs unterstützen das UCI, so beispielsweise Shredder, Fritz, Chess Assistant, Chess Partner und Arena.\n\n"}
{"id": "988478", "url": "https://de.wikipedia.org/wiki?curid=988478", "title": "Chemnitzer Linux-Tage", "text": "Chemnitzer Linux-Tage\n\nDie Chemnitzer Linux-Tage (CLT) ist eine große Veranstaltung rund um das Thema Linux und freie Software in Deutschland. Sie findet im zentralen Hörsaal- und Seminar-Gebäude der Technischen Universität Chemnitz statt, auf der unter anderem Vorträge, Projektstände, Workshops, Installparty und Technik-Ecke (\"Praxis Dr. Tux\") etc. angeboten werden.\n\n1999 fand erstmals ein Chemnitzer Linux-Tag statt – eine eintägige Veranstaltung mit Vorträgen und einer Installationsparty, die von 700 Besuchern besucht wurde. Seit 2000 haben sich die Chemnitzer Linux-Tage als jährliche zweitägige Veranstaltung an einem Wochenende im März etabliert.\n\nOrganisiert wird die Veranstaltung im Wesentlichen von (ehemaligen) Studenten und Mitarbeitern der TU Chemnitz. Des Weiteren sind die Chemnitzer Linux User Group des IN Chemnitz e.V., auf deren Mitglieder die Initiative zur erstmaligen Durchführung der CLT zurückgeht, die Fakultät für Informatik sowie das Rechenzentrum der TU Chemnitz an der Organisation beteiligt.\n\nFür eine Community-Veranstaltung typisch ist die relativ lockere Organisationsstruktur. Alle Aufgaben wechseln regelmäßig und werden ständig von neuen Personen übernommen. Im Kernteam kümmern sich sechs bis acht Personen das gesamte Jahr über mit wechselnder Intensität um alle grundsätzlichen Belange. Zwischen September und März wächst dieses Team zum Organisations-Team von rund 30 Personen an, welche sich um unterschiedlich große Aufgabengebiete kümmern. An den CLT selbst wirken rund 450 bis 500 Aktive mit. Neben den offensichtlich präsentationsbezogenen Aufgaben (Vorträge, Projekte etc.) werden auch sehr viele Helfer für Infrastrukturaufgaben wie Logistik, Catering, Informationsstand, Kinderspielecke oder Ordnerdienste eingesetzt.\n\nDie Chemnitzer Linux-Tage bieten ein abwechslungsreiches Programm für alle Besuchergruppen. Neben einem umfangreichen Vortragsprogramm für Laien, Anwender und Experten kann man sein Wissen in zahlreichen Workshops intensiv erweitern, im Ausstellungsbereich mit Experten auf den verschiedensten Gebieten diskutieren und natürlich den eigenen Rechner mitbringen, um unter Anleitung Linux zu installieren oder Probleme im System zu beheben. \n\nDas Publikum der Chemnitzer Linux-Tage ist bunt gemischt: Schüler und Studenten sind genauso vertreten wie Linux-interessierte Heim-PC-Nutzer und Mitarbeiter von Unternehmen. Um den Interessen der Besucher – vom Linux-Neuling bis zum Open-Source-Experten ist alles dabei – gerecht zu werden, präsentiert sich das Veranstaltungsprogramm entsprechend vielfältig.\n\nDie Vorträge werden seit 2005 per Audio und seit 2009 ebenfalls per Video-Stream live aus den Vortragsräumen übertragen. Die Chemnitzer Linux-Tage werden dabei vom Radio UNiCC und der Professur Medieninformatik unterstützt.\n\nMit einigen speziell auf Kinder und Jugendliche zugeschnittenen Programmpunkten soll außerdem die Neugier und Freude an eigener schöpferischer Tätigkeit bei der jüngeren Generation gefördert werden.\n\n\n"}
{"id": "989243", "url": "https://de.wikipedia.org/wiki?curid=989243", "title": "Translation Memory eXchange", "text": "Translation Memory eXchange\n\nDie Arbeit an TMX begann im Jahr 1997, seit Oktober 2004 liegt es in der Version 1.4b vor. Heute unterstützen praktisch alle Anbieter von Übersetzungssoftware das Format zumindest für Im- und Export in ihre proprietären Formate.\n\nVerwaltet wurde TMX von der \"OSCAR\" (), die für die Organisation (LISA) an der Erarbeitung offener Standards für die Übersetzungsbranche arbeitet. Da die LISA im März 2011 insolvent ging, ist der Standard heute unter der Creative Commons Attribution 3.0 License öffentlich. LISA hat die European Telecommunications Standards Institute (ETSI) Localization Industry Standards (LIS) Industry Specification Group (ISG) als Nachfolger für die Verwaltung des Standards bestimmt.\n\nBeispiel einer TMX-Datei mit einem Eintrag:\n<tmx version=\"1.4b\">\n</tmx>\n"}
{"id": "989769", "url": "https://de.wikipedia.org/wiki?curid=989769", "title": "Microsoft Works", "text": "Microsoft Works\n\nMicrosoft Works war ein Office-Paket, das folgende Aufgaben erledigte: Textverarbeitung, Tabellenkalkulation, Datenbankverwaltung und Terminplanung. Im Gegensatz zu \"Microsoft Office\" verfügten die Einzelkomponenten jedoch nicht über fortgeschrittene Eigenschaften, so dass beispielsweise ein Tabellenkalkulationsdokument nicht mehr als eine Tabelle gleichzeitig enthalten konnte, die Textverarbeitung keine benannten Absatzformatvorlagen beherrschte, eine Datenbank nur eine Tabelle gleichzeitig verwalten konnte usw.\nMit Erscheinen von Microsoft Office 2010 wurde Works eingestellt und durch Microsoft Office Starter ersetzt.\n\nZielgruppe von Microsoft Works war der Privatanwender zu Hause. Mit Microsoft Works wurde der Benutzer mit Hilfe von Assistenten zu dem entsprechenden Dokument geführt. Ziel von Works war es nicht, umfangreiche Dokumentationen, Tabellen und Datenbanken zu erstellen. Es richtete sich nur an den gelegentlichen Hausgebrauch, wie z. B. gelegentlicher Briefwechsel, CD/DVD-Datenbank oder ähnlichem.\n\nAus ökonomischer Sicht versuchte Microsoft das Budget-Segment unterhalb von Microsoft Office abzudecken. Es war besonders im OEM/SB/DSP-Vertrieb gemeinsam mit einem Computer sehr verbreitet, obwohl die Konkurrenz durch andere proprietäre Produkte wie Lotus SmartSuite oder umfangreichere, kostenlose und freie Produkte wie OpenOffice.org sowie webbasierte Online-Office-Angebote wie Google Drive groß war.\n\nWorks versuchte mit Hilfe von Assistenten den Anwender bei seiner täglichen Arbeit zu unterstützen. Dies war vor allem für ungeübte Gelegenheitsnutzer von Vorteil, welche mit den vielen Funktionen eines Office-Paketes überfordert gewesen wären.\n\nDie MS-DOS Versionen v1.05 und höher enthielten die Möglichkeit der Datenübertragung via Modem.\n\nIm Programmpaket integriert war schon ab der ersten Version eine Works-Datenbank enthalten, die stetig im Lauf der Jahre weiter aktualisiert wurde. Es ließen sich grundlegende Datenbankoperationen durchführen und Datenbestände jeglicher Art verwalten, in Verbindung mit der Works-Textverarbeitung auch mit Serienbrieffunktion.\n\nMit Hilfe der Works-Tabellenkalkulation konnten Berechnungen in Tabellen angestellt werden. Vergleichbar war dieses Teilprogramm mit \"Microsoft Excel\". Works unterstützte jedoch viele aus Excel bekannte Funktionen nicht und die Automatisierung mit Hilfe der Visual Basic for Applications fehlt.\n\nMit der Works-Textverarbeitung ließen sich einfache Texte wie Briefe oder Aufsätze verfassen.\n\nDas Programm war vergleichbar mit \"Microsoft Word\", jedoch sind viele von Word bekannte Funktionen weggelassen worden. Mathematische Formeln konnten wie in Word als Embedded-Objekt eingegeben und dargestellt werden.\n\nMit Works erstellte Texte wurden in einem eigenen Dateiformat mit der Endung *.wps gespeichert.\n\nDie Microsoft Works Suite war nahezu identisch zur jeweiligen Microsoft-Works-Ausgabe. Der Hauptunterschied lag vor allem darin, dass neben dem Textverarbeitungsprogramm von Works eine aktuelle Vollversion von Microsoft Word mit enthalten war. Die Works Suiten bis Works 2006 enthielten zudem noch weitere nützliche Zusatzprogramme für den Privatanwender (siehe unten).\n\n\"Works Plus 2008\" enthielt Works 9 mit den traditionellen Funktionen Textverarbeitung, Tabellenkalkulation, Datenbank, Kalender, E-Mail- und Internet-Werkzeugen. Die beiden wichtigsten Neuerungen waren das Works-Wörterbuch mit Thesaurus und der PowerPoint Viewer. Nebenbei wurde die Dokumentensicherheit durch eine „Auto-Save“-Funktion erhöht. Darüber hinaus war in Works Plus 2008 – gegenüber Works 9.0 – zusätzlich erstmals \"Microsoft Word 2003\" enthalten. Diese „Word/Works-Kombination“ entsprach zwar der bisherigen \"Works Suite\", allerdings ohne die Programme \"Encarta\", \"AutoRoute\" und \"Foto Suite\".\n\nAb Works Plus 2008 nicht mehr enthalten:\n\n\n\n\n\nMit dem Erscheinen von Office 2010 wurde die Entwicklung von Works eingestellt. Office Starter 2010 besteht aus in ihrem Funktionsumfang eingeschränkten Versionen der Programme Word 2010 und Microsoft Excel 2010. Es nutzt dieselben Dateiformate *.docx und *.xlsx wie die Vollversionen dieser Programme. Vielfach wird \"Office Starter 2010\" vorinstalliert mit neuen Rechnern ausgeliefert. Es wird kostenlos angeboten, enthält allerdings Werbeeinblendungen.\n\nDie Produktunterstützung auch für Programmaktualisierungen für Works 9 endete am 9. Oktober 2012, für Works Plus 2008 am 8. Januar 2013. Erweiterte Unterstützung („Extended Support“) z. B. mit Sicherheitsaktualisierungen wurde nicht betrieben.\n\nAktuelle Officeversionen von Microsoft bieten beim Öffnen keine Dateiendung WKS (Works Tabellen) oder WPS (Works Texte) an. Die von Microsoft zum Download angebotenen Konverter unterstützen MS-Works 6 bis 9, aber keine älteren Versionen und diese auch nur als Plugin für ältere MS-Office 2000 bis 2003. Wer auf alte Dokumente zugreifen muss, kann diese mit LibreOffice (beispielsweise 4.1.3) öffnen und dann als XLS, XLSX, DOC oder DOCX-Format in aktuellen Microsoft-Office-Versionen wieder weiterverwenden. Ein Verlust einzelner Formatierungen ist nicht ausgeschlossen und LibreOffice gibt auch einen entsprechenden Hinweis dazu. Auch das Konvertierungsprogramm \"FileMerlin\" (vormals \"WordPort\") unterstützt Works in den Versionen 1.0/2.0 (DOS) und 3.x/4.x (Windows).\n\n"}
{"id": "990386", "url": "https://de.wikipedia.org/wiki?curid=990386", "title": "Starry Night", "text": "Starry Night\n\nStarry Night ist eine Reihe von kommerziell ausgelegten Astronomie-Software der Firma Imaginova Corp. für Hobby-Astronomen, allen voran die verschiedenen Planetarien-Programme. Es zeigt den Himmel über der Erde von jedem beliebigen Ort aus, sowie von anderen Himmelskörpern des Sonnensystems. Umfangreichere Produkte der Reihe der Planetarien-Programme bieten zusätzlich die Möglichkeit an, den Himmel von jedem beliebigen Punkt in der Lokalen Gruppe zu simulieren und zu betrachten.\n\nDie Software bietet die Möglichkeit, von mehr als 200.000 in der einfachen Ausführung des Programms, bis zu über 2,5 Millionen Sterne mit zusätzlichen Informationen in der umfangreichsten Version der Reihe zu erkunden und sich mit fotorealistischem Bildmaterial als künstlichem Himmel anzeigen zu lassen. Zusätzlich dazu sind Deep Sky-Objekte wie beispielsweise Galaxien ebenfalls fotorealistisch implementiert und runden den Anblick des Himmels ab. Eines der vielen Extras ist noch dazu die vielen verschiedenen kleinen Himmelskörper in unseren Sonnensystem wie Asteroiden oder Kometen, die alle möglichst wirklichkeitsgetreu simuliert werden.\n\nDie Simulation ist sowohl in Echtzeit möglich, als auch im Zeitraffer in beide Richtungen der Zeitachse. Die Möglichkeit, zu jedem beliebigen Datum zu springen, macht es möglich, den Himmel zu Christi Geburt zu sehen oder einen Blick von dem Kometen Hale-Bopp auf das Sonnensystem zu werfen.\n\nMit einem zusätzlichen Plug-in, das in umfangreicheren Ausführungen bereits enthalten ist, kann man Teleskope, die mit einem dafür kompatiblen Motor ausgestattet sind, steuern und auf in der Software ausgewählte Punkte ausrichten und diese mit bloßem Auge betrachten.\n\n\n"}
{"id": "990970", "url": "https://de.wikipedia.org/wiki?curid=990970", "title": "GStreamer", "text": "GStreamer\n\nGStreamer ist ein freies Multimedia-Framework, lizenziert unter der LGPL. Es ist in der Programmiersprache C geschrieben und verwendet die Programmbibliothek GObject. GStreamer ist eines der Projekte von freedesktop.org, das sich darum bemüht, die Zusammenarbeit und den Datenaustausch zwischen den Projekten durch offene Standards zu erhöhen.\n\nDiese Bibliothek dient dazu, grundlegende Funktionalität zur Verarbeitung von Multimedia-Datenströmen bereitzustellen, und kann so als Basis von Multimedia-Anwendungen wie beispielsweise Mediaplayern, Videoschnittsoftware oder Audio-Mixern dienen. Sie kann mit Hilfe von Plug-ins einfach erweitert werden.\n\nDer Vorteil von GStreamer (und generell von Frameworks) ist, den Softwareentwicklern ein möglichst leicht bedienbares Grundgerüst bereitstellen zu können. So muss weniger Zeit in die Entwicklung von grundlegenden Funktionen investiert werden, und es bleibt mehr Zeit für die individuellen Programmteile übrig, welche die neue Funktionalität ausmachen sollen. Installierte Plug-ins stehen automatisch allen GStreamer-basierenden Anwendungen zur Verfügung und profitieren automatisch von den neuen Fähigkeiten.\n\nEs gibt Bindings zu Perl, Python, Java, Vala, C#, Guile und Ruby, welche die Entwicklung einer Applikation in einer dieser Programmiersprachen ermöglichen.\n\nDa der Kern von GStreamer relativ kompakt ist, wird es auch im Bereich eingebetteter Systeme eingesetzt, etwa in Maemo oder HP webOS. Auch die beliebten Linuxreceiver mit der Benutzeroberfläche Enigma2, z. B. Dreambox, verwenden GStreamer.\n\nEnde 1999 wurde das Projekt GStreamer ein öffentliches Projekt. GStreamer wurde und wird hauptsächlich für Unix/Linux entwickelt. Mittlerweile basieren zahlreiche Programme auf GStreamer, darunter ab der Version 2.2 auch die Desktop-Umgebung Gnome sowie dazu passende Musik- und Videoprogramme wie z. B. Rhythmbox oder Cheese. Auch die seit KDE 4 eingesetzte Multimedia-API Phonon kann GStreamer als Backend einsetzen.\n\n\n"}
{"id": "991614", "url": "https://de.wikipedia.org/wiki?curid=991614", "title": "Orwell Dev-C++", "text": "Orwell Dev-C++\n\nOrwell Dev-C++ (früher Dev-C++, so wie es auch noch verkürzt genannt wird) ist eine freie, quelloffene Entwicklungsumgebung ursprünglich von \"Bloodshed Software\" für C und C++. Sie läuft unter Windows und nutzt MinGW als Compiler, eine Windows-Portierung der GNU Compiler Collection. Dev-C++ kann auch in Verbindung mit Cygwin oder anderen Compilern genutzt werden, die auf GCC basieren.\nDev-C++ wurde von Colin Laplace begonnen, die Entwicklungsumgebung wurde in Delphi geschrieben.\n\nMit sogenannten \"DevPaks\" kann die Dev-C++-Entwicklungsumgebung erweitert werden, beispielsweise um neue Bibliotheken oder Werkzeuge. DevPaks können auch mit verwendet werden.\n\nDev-C++ wird als Projekt auf SourceForge gespeichert und verwaltet.\n\nNachdem die Entwicklung von Dev-C++ ab März 2005 stillgestanden hatte, erschien am 30. Juni 2011 eine neue Version, der weitere folgten. Die aktuelle Version enthält einen neuen GCC (Version 4.8.1) und bringt auch alle erforderlichen Ressourcen zum Programmieren von DirectX und Win32 mit. Viele Programmfehler und Stabilitätsprobleme wurden behoben.\n\nVon Bloodshed wird noch immer die veraltete Version 4.9.9.2 (März 2005) als „aktuell“ bereitgestellt, neuere Versionen erschienen sämtlich unter dem Namen \"Orwell Dev-C++\".\n\nwxDev-C++ ist eine Erweiterung von Dev-C++ und wird ebenfalls aktiv weiterentwickelt. Es bietet eine RAD-Umgebung für wxWidgets.\n\n"}
{"id": "992496", "url": "https://de.wikipedia.org/wiki?curid=992496", "title": "CBM-3000-Serie", "text": "CBM-3000-Serie\n\nDie 3000er-Serie von Commodore war der Nachfolger des PET 2001. Diese Computer-Serie besaß neben einer richtigen, schreibmaschinenähnlichen Tastatur eine Reihe von Detailverbesserungen, die das eingebaute Basic und das Betriebssystem betrafen. Aufgrund der nun gewachsenen Tastatur wurde die eingebaute Datasette (Programm- und Datenspeicher) als Zusatzgerät ausgelagert.\n\nDie Hauptplatine war beim Modell CBM 3008 noch praktisch identisch mit der des Vorläufers PET 2001 mit seinen statischen RAM-Chips. Bei den Modellen 3016 und 3032 wurde das RAM und damit die komplette Hauptplatine aber auf dynamische Chips umgestellt, daher im angelsächsischen Bereich auch der Spitzname \"Dynamic PET\". Wie beim Vorgänger arbeitete ein MOS Technology 6502 mit 1 MHz Taktfrequenz als CPU.\n\nDer Name PET (Personal Electronic Transactor) musste in Europa aus markenrechtlichen Gründen in CBM (Commodore Business Machines) geändert werden.\n\nMit diesen Geräten gelang Commodore ein erster Erfolg im Bürobereich sowie in Universitäten und Schulen. Aber erst die Nachfolgemodelle 4000er und 8000er führten zum eigentlichen Durchbruch.\n\nEs gab folgende Modelle:\n\n\nZu diesen Rechnern gehörte noch eine passende Palette an Peripheriegeräten, insbesondere Diskettenlaufwerke und Drucker, siehe die Commodore-Produktübersicht und dort die 3000er-Modelle.\n\n\n"}
{"id": "992523", "url": "https://de.wikipedia.org/wiki?curid=992523", "title": "CBM-4000-Serie", "text": "CBM-4000-Serie\n\nDie 4000er-Serie ist die Nachfolgerserie des PET 2001 bzw. der CBM-3000-Serie von Commodore. Wie dort arbeitet ein MOS Technology 6502 mit 1 MHz Taktfrequenz als CPU.\n\nDer Computer verfügte über die BASIC-Version 4.0, die im Gegensatz zu seinem Vorgänger um einige Diskettenbefehle erweitert worden ist.\n\nEs gab von jedem Modell, außer dem 4064, eine THIN- und FAT-Version. Der Unterschied bestand darin, dass die THIN-Version nicht erweiterbar war und nur einen 9\"- statt eines 12\"-Monitors integriert hatte. Die FAT-Version konnte durch einiges Umlöten (über 20 Löt-Jumper plus Video-RAM-Vergrößerung) auf 80 Zeichen erweitert werden und wurde dadurch praktisch zu einem 8000er. \n\nDie THIN-Version war also hardwaremäßig identisch mit der Vorgängerversion 3000er, hatte aber schon das neue BASIC; die FAT-Version verfügte außerdem schon über die Platine der späteren 8000er, und deren Monitor, Letzterer wurde aber mit 40 Zeichen betrieben.\n\nEs gab folgende Modelle:\n\n\nDer 4064 war als Schul-Version des C64 gedacht, daher auch manchmal als C4064 bezeichnet, und hatte dessen Platine und BASIC, allerdings auf Monochrom-Ausgabe umgestellt und auch diverse Software-Unterschiede, die ihn inkompatibel sowohl zum C64 als auch zu den anderen 4000ern machten. Er war ein Misserfolg.\n\nAn Software standen u. a. die Textverarbeitung Wordpro und die Tabellenkalkulation Visicalc zur Verfügung. Neben dem CBM-BASIC Version 4.0 ließ sich zur Programmierung der Pascal-Compiler TCL-Pascal einsetzen. Außerdem war wie bei der CBM-3000-Serie eine Programmierung des Prozessors 6502 in Assembler leicht möglich. Zum Commodore-BASIC gab es Erweiterungen mit vielen nützlichen Befehlen, die in einem oder zwei ROMs (maximal 8 KByte) geliefert und in freie Stecksockel auf der Platine eingesetzt wurden. \n\nZu diesen Rechnern gehörte noch eine passende Palette an Peripheriegeräten, insbesondere Diskettenlaufwerke und Drucker.\n\n\n"}
{"id": "992598", "url": "https://de.wikipedia.org/wiki?curid=992598", "title": "CBM-8000-Serie", "text": "CBM-8000-Serie\n\nBei der CBM (\"Commodore Business Machines\") 8000er-Serie von Commodore handelt es sich um einen Computer der 4000er-Serie (FAT-Version), aber mit einem größeren Monitorteil mit 80 Zeichen. Unverändert arbeitete ein MOS Technology 6502 mit 1 MHz Taktfrequenz als CPU. Dazu passend gab es ein 5¼-Zoll-Diskettenlaufwerk Modell CBM 8050 mit 500 KB Kapazität (später CBM 8250 mit 1 MB Kapazität) und einen bidirektionalen 132-Zeichen-Drucker mit 160 cps. An Software standen u. a. das Datenbankprogramm Ozz, die Textverarbeitung Wordpro und die Tabellenkalkulation Visicalc zur Verfügung. Der Computer war mit CBM-BASIC Version 4.0 programmierbar, es gab aber auch die Möglichkeit, andere Programmiersprachen in den Hauptspeicher zu laden und dann in diesen zu programmieren; relativ weit verbreitet war der Pascal-Compiler TCL-Pascal.\n\n\nTechnisch nicht in die Reihe gehörten Ausführungen des Commodore 64 in den Gehäusen der 8000er-Serie:\n\nDer 8032 wurde in drei verschiedenen Gehäuse-Ausführung hergestellt: In der Standardversion, der 32B-Version, die über ein größeres Gehäuse verfügte, in das man ein Laufwerk einbauen konnte, und der ergonomischen SK-Version, die über einen schwenkbaren Monitor und eine abnehmbare Tastatur verfügte. Vom 8064 gab es auch eine SK-Version.\n\nAb dem 8096er wurde das Betriebssystem LOS-96 zusätzlich mitgeliefert, welches im Gegensatz zum CBM-Basic den gesamte Hauptspeicher ansprechen konnte. Beim 8296, der ein später aber direkter Nachfolger war, wurde eine D-Version produziert, die zwei Slim-Line-Laufwerke (Modell CBM 8250LP) eingebaut hatte, die bis zu einem MB abspeichern konnten. \n\nDer Name MMF des Modells MMF 9000 stand für \"Micro Mainframe\". Der Rechner kam mit Compilern für Pascal und weitere fortschrittlichere Sprachen, war aber extrem langsam und wirtschaftlich kein Erfolg.\n\nAufgrund ihrer Beliebtheit wurde die 8000er-Serie bis Mitte der 1980er-Jahre produziert und kommt z. T. heute noch in manchen Unternehmen zum Einsatz. Bevor der IBM-PC den Markt dominierte, war Commodore mit diesen Geräten in Deutschland und wahrscheinlich einigen Ländern mehr Marktführer bei kommerziellen Büroanwendungen und dank des IEEE-488-Busses auch in Fertigungsautomatisierungen und in Universitäten, dort sowohl zum Numbercrunching, als auch zur Messwerterfassung und Textverarbeitung. Auch das „Vernetzen“ über den IEEE-488-Bus war für den kommerziellen Erfolg ein starkes Argument. In den frühen 1980er-Jahren wurden schon Stückzahlen von deutlich über 100.000 Exemplaren erreicht. Ende 1983 war Commodore mit dieser Modellserie Marktführer in Deutschland bei Bürocomputern, ausführliche Zahlen dazu siehe bei Personal Computer.\n\nDie Hauptplatine der 8000er (bis zum 8096) fasste dynamisches RAM bis zu 32 KB sowie einen Videoteil, der erstmals mit einem eigenen Videochip 6545 (identisch mit 6845, wie er auch in der IBM-CGA-Karte benutzt wurde und auch noch in heutigen Grafikchips als Teilfunktionalität enthalten ist) ausgeführt war. Per Jumper konnte das Video-RAM zwischen 40 und 80-Zeichen Breite umkonfiguriert werden, so dass diese Platine auch in den FAT-Versionen der 4000er-Serie benutzt werden konnte. Ansonsten war die Hauptplatine noch weitgehend mit der der 3000er identisch. Beim 8296 gab es eine neue Hauptplatine, auf der von Haus aus 128 KB RAM verbaut waren sowie ein Teil der Funktionalität in selbst entwickelten Custom-Chips zusammengefasst wurde.\n\nDer 8096-SK war eine Zwischenlösung, weil der 8296 nicht so schnell wie geplant fertig wurde. In das neue Gehäuse passte die alte 8032-Platine nur hinein, indem man sie um 90° drehte. Danach lagen aber alle Peripherieanschlüsse in Gestalt von Platinensteckern an den falschen Stellen, so dass innerhalb des Gehäuses extra Kabel von den Platinensteckern zu den von außen zugänglichen Steckkontakten geführt werden mussten.\n\nIn der gleichen Grundarchitektur sollte diese Rechnerreihe mit den Modellen CBM 500, CBM 600 und CBM 700 weitergeführt werden, siehe dort.\n\nFür diese Rechnerserie gab es ein breites Angebot an Peripheriegeräten:\n\n"}
{"id": "993959", "url": "https://de.wikipedia.org/wiki?curid=993959", "title": "Maple (Software)", "text": "Maple (Software)\n\nMaple (mathematical manipulation language) ist ein englischsprachiges Computeralgebrasystem (CAS) für Algebra, Analysis, diskrete Mathematik, Numerik und viele andere Teilgebiete der Mathematik. Es stellt ferner eine Umgebung für die Entwicklung mathematischer Programme zur Verfügung und ermöglicht die Visualisierung mathematischer Strukturen.\n\nDie erste Version von Maple wurde 1980 von Keith O. Geddes, Gaston H. Gonnet und deren Mitarbeitern von der \"Symbolic Computation Group\" an der Universität von Waterloo in der kanadischen Stadt Waterloo (Ontario) programmiert. Ende 1987 gab es Maple bereits in der Version 4.2.\n\nSeit 1988 wird Maple von \"Maplesoft\", einer Abteilung der Firma Waterloo Maple, weiterentwickelt und vermarktet. \"Waterloo Maple\" gehört seit September 2009 zur in Japan ansässigen Firma \"Cybernet Systems Co., Ltd\".\n\nBei der wissenschaftlichen Unterstützung des Maple-Projektes ging und geht es darum, schnelle und effiziente Algorithmen für symbolische Berechnungen zu entwickeln und in das Programm zu integrieren. An diesen Arbeiten sind bzw. waren neben dem \"Ontario Research Centre for Computer Algebra\" (ORCCA), bestehend aus der \"Maple Symbolic Computation Group\" (Universität von Waterloo) und dem \"Symbolic Computation Laboratory\" (Universität von Western Ontario), auch Wissenschaftler an der ETH Zürich, dem Institut national de recherche en informatique et en automatique (INRIA) und vielen anderen Labors weltweit beteiligt.\n\nSeit 1998 gibt es eine Zusammenarbeit zwischen \"Maplesoft\" und der \"Numerical Algorithms Group\" (NAG). NAG-Komponenten fanden sich erstmals in Maple Release 6 aus dem Jahre 2000. Diese Komponenten führten insbesondere zu einer erheblichen Verbesserung der Rechengeschwindigkeit und der Rechengenauigkeit auf dem Gebiet der Linearen Algebra.\n\n2003 gab es Maple für Windows CE 2.0 für den mobilen Einsatz auf dem Handheld Cassiopeia A-23g. Dieser wurde oft in diversen Oberstufen eingesetzt.\n\n2005 wurde mit Maple 10 ein neuer Dokument-Modus („document mode“) innerhalb der Standardversion von Maple eingeführt. Seither ist es möglich, Maple-Inputs in normaler mathematischer Schreibweise zu editieren. Hierbei lassen sich Texte und mathematische Symbole in derselben Eingabezeile miteinander kombinieren.\n\nMaple 13 bot unter anderem erhebliche Verbesserungen bei der Ausgabe von 3D-Grafiken, neue Prozeduren und neue interaktive Tutoren.\n\nAb der Version 14 von Maple ist es möglich, zusammen mit anderen Maple-Nutzern auf Worksheets gemeinsam zuzugreifen (\"MapleCloud\"). Hierbei kann man eigene Ressourcen allen Maple-Nutzern weltweit oder aber nur den Mitgliedern bestimmter Arbeitsgruppen zur Verfügung stellen.\n\nMaple 15 unterschied sich von den Vorgängerversionen insbesondere durch eine erhebliche Vergrößerung der Rechengeschwindigkeit bei Rechnern, die mit mehreren Prozessoren ausgestattet sind.\n\nNeben neuen Packages (wie zum Beispiel \"GroupTheory\"), neuen Rechenbefehlen sowie zahlreichen Verbesserungen bietet Maple ab der Version 17 einen Editor, der die Entwicklung von Quelltext mit Syntax Highlighting und weiteren Funktionalitäten unterstützt. Außerdem werden Funktionsaufrufe jetzt durch die Verwendung von hardwareunterstützten Algorithmen erheblich schneller abgearbeitet als früher.\n\nHauptkomponente der grafischen Benutzeroberfläche\nvon Maple ist das jeweilige Worksheet, in dem interaktiv gearbeitet wird. Es erscheint als\nFenster, in das Rechenanweisungen (\"Maple-Inputs\") eingetragen werden. Die Maple-Engine\ninterpretiert diese Anweisungen und liefert entsprechende Ausgaben (\"Maple-Outputs\") zurück.\n\nTypische Maple-Outputs sind Zahlenwerte, Terme, Funktionen, Tabellen,\n2- und 3-dimensionale Grafiken, Animationsobjekte und Diagramme. Es ist möglich,\ndie von Maple erzeugten Objekte bzw. Ausdrücke über kontext-sensitive Menüs zu bearbeiten.\n\nDas Einfügen mathematischer Symbole, Ausdrücke, Vektoren und Matrizen in\nRechenanweisungen wird erleichtert durch die Benutzung von \"Paletten\".\nDiese bestehen aus für verschiedene Aufgaben vorgefertigten Code-Schnipseln,\ndie per Mausklick dem Worksheet hinzugefügt werden können.\n\nVon der Version 9 an gibt es neben dem \"Classic Worksheet Maple\" eine Java-basierte Version\n\"Maple Standard\". Die Standardversion von Maple bietet eine komfortablere Oberfläche, ist\naber andererseits deutlich langsamer als die klassische Variante. Aufgrund dieser zwei Varianten\ngibt es auch zwei unterschiedliche Arten, Worksheets zu speichern. Man unterscheidet\nStandard-Worksheets (Dateiendung: mw) und\nClassic-Worksheets (Dateiendung: mws; kompatibel zu älteren Maple-Versionen).\n\nEin fertig bearbeitetes Worksheet kann man bei Bedarf exportieren als PDF-, HTML-, LaTeX- oder als RTF-Dokument.\n\nMaple umfasst einen Kern häufig benutzter Standard-Rechenanweisungen\n(\"main library\") und zusätzliche, zur Laufzeit mit dem with-Befehl ladbare Pakete (\"packages\").\nIm Folgenden sind einige der wichtigsten dieser insgesamt über hundert Pakete aufgelistet:\n\n\nMaple besitzt Schnittstellen zu Matlab, Fortran, C, C#, Java, Visual Basic, Python, Perl, R, JavaScript, Julia und Swift, die Maple-Code in diese Zielsprachen übersetzen. Umgekehrt lassen sich Fortran-, C-, C#- oder Java-Routinen in Maple einbinden, und Maple lässt sich aus C, Java und VisualBasic aufrufen (OpenMaple-API).\n\nEs folgen einfache Beispiele für Rechenanweisungen in typischer Maple-Notation.\n\n RootofTwo:= evalf[21](sqrt(2));\n solve(3*x^2+b*x=7, x);\n\n f:= x -> tan(x)*sqrt(x):\n\n int(sin(x)^2, x);\n DGL:= diff(y(x),x,x) - 3*y(x) = x:\n\n with(geom3d):\n with(plots):\n with(plots):\n\n"}
{"id": "994377", "url": "https://de.wikipedia.org/wiki?curid=994377", "title": "Commodore PC-1", "text": "Commodore PC-1\n\nDer Commodore PC-1 war als günstiger Einsteiger-PC für den Heimgebrauch, aber auch als Terminal für größere Rechner der mittleren Datentechnik gedacht und wurde von Februar 1987 bis Ende des Jahres 1988 produziert. \n\nDer Commodore PC-1 war ein Modell in einer Reihe von MS-DOS-kompatiblen Computern von Commodore International, mit deren Bau das damalige Endfertigungswerk der deutschen \"Commodore Büromaschinen GmbH\" in Braunschweig beauftragt war. In seiner ultrakompakten Form war er auch als Konkurrenz für den \"PC-I\" von Atari gedacht. Der Einführungspreis lag damals bei ca. 900 DM. Der Preis wurde sehr schnell reduziert bis auf unter 700 DM (in Aktionen über Kaufhäuser und spezialisierte Computer-Handelsketten). Nach weniger als zwei Jahren wurde die Produktion des Commodore PC-1 eingestellt und sich auf den Commodore PC-10 und seine Nachfolgermodelle konzentriert.\n\n\n\nObwohl einige Zeit am Markt und durch Ketten wie Vobis beworben und vertrieben, konnte sich dieser Rechner nie erfolgreich durchsetzen. Hauptgrund dafür war die für die damalige Zeit zwar durchaus beachtenswerte, ultrakompakte, aber sehr einschränkende Bauform. Dadurch fehlten elegante Erweiterungsmöglichkeiten im eigenen Gehäuse, wie sie etwa im sehr erfolgreichen Commodore PC-10 und seinen Nachfolgermodellen gegeben waren. Weiterhin war es auch nicht möglich, im Gehäuse ISA-kompatible Karten zu verbauen, da keine ISA-Steckplätze vorhanden waren und diese über die Expansionsbox erst nachgerüstet werden mussten. Ein weiterer Grund war die große Anzahl an IBM-kompatiblen Rechnern, die auf den Markt drängten und günstiger u. a. in Ostasien gefertigt werden konnten. Einige, jedoch nie durchdringende Verbreitung fand der Rechner als Terminal in Rechenzentren.\n\n"}
{"id": "994417", "url": "https://de.wikipedia.org/wiki?curid=994417", "title": "Commodore PC-10 bis PC-60", "text": "Commodore PC-10 bis PC-60\n\nBei den Commodore PC-10 bis PC-60 handelt es sich um eine Serie von IBM-kompatiblen PCs von Commodore, die 1984 bis Mitte der 1990er Jahre verkauft wurde. Sie wurden in der Braunschweiger Entwicklungsabteilung konstruiert und im dortigen Endfertigungswerk teilweise auch montiert.\n\nMit dieser Rechnerfamilie lieferte sich Commodore ein paar Jahre lang ein Kopf-an-Kopf-Rennen mit IBM um die Marktführerschaft im PC-Bereich in Deutschland, typischerweise wechselte die Führung von Monat zu Monat. Als Höhepunkt erreichte man es, dass nach der Wende sämtliche Büros der Deutschen Reichsbahn in Ostdeutschland mit Commodore-PCs ausgerüstet wurden.\n\nDiese Computer waren günstige Alternativen zum IBM-PC und wurden ab 1984 produziert. Der PC-10 wurde Mitte 1984 zu einem Preis von 4.950 DM ohne Mehrwertsteuer angeboten. Sie zeichneten sich durch eine gute Ausstattung, Robustheit, Kompatibilität und Erweiterbarkeit aus. Sie waren mit einem Intel 8088-Prozessor ausgestattet, der wie in den meisten frühen PCs mit 4,77 MHz getaktet war und mit einem mathematischen Coprozessor Intel 8087 erweitert werden konnte; dazu kamen 256 kB RAM (erweiterbar auf 640 kB), ein 5¼″-Diskettenlaufwerk mit 360 kB Kapazität und ohne Uhrmodul, erst im PC-20 war eine 10-MB-Festplatte verbaut. Weiter konnte der Computer aufgrund der fünf vorhandenen XT-Bus-Steckplätze gut erweitert werden. Die sogenannte AGA-Grafik der Commodore PCs konnte sowohl CGA- als auch MDA- und Hercules-Grafikmodi anzeigen, wodurch die große Mehrzahl der damaligen PC-Programme abgedeckt war (EGA-Grafikkarten waren 1985 noch sehr teuer und wurden daher selten zwingend vorausgesetzt). Vor allem in Europa verkauften sich diese Modelle sehr gut.\n\nDa die Braunschweiger Entwickler zuerst einen tragbaren PC entwickeln sollten, wurde aus Platzgründen und damit die Entwicklung schneller vorangeht, das Motherboard in zwei gestapelte Platinen geteilt, einem CPU-Board mit dem RAM-Hauptspeicher sowie dem I/O-Board mit allen Schnittstellen für die Peripheriegeräte. Im Frühjahr 1984 wurde diese Entscheidung revidiert. Es sollte nun ein Desktop PC entwickelt werden. Der Grund dafür war, dass Commodore USA eine Vertriebslizenz mit dem kanadischen Hersteller Dynalogic für deren fertigen Portable-PC abgeschlossen hatte.\n\nWährend bei den meisten damaligen PCs, einschließlich der IBM-Modelle, der Tastatur-Controller der einzige Schnittstellenbaustein direkt auf der Hauptplatine („Motherboard“) war, beherbergte die Platine der Commodore-PCs daneben auch die Controller für zwei Diskettenlaufwerke, eine Centronics-Schnittstelle und eine RS232-Schnittstelle. Damit wurden zwei Steckplätze eingespart, da fast alle PCs diese Komponenten ohnehin benötigten.\n\nDer PC-30 war eine Version des PC-20 mit 20-MB-Festplatte.\n\nPraktisch die gleichen Geräte, nur mit einer überarbeiteten, durchgängigen Hauptplatine (\"Combined Board\") und beim PC-20 II mit 20-MB-Festplatte, kein PC-30 mehr.\n\nDie Antwort von Commodore auf den PC1 von Atari: Ein PC-10 in einem ultrakompakten Gehäuse mit einem Diskettenlaufwerk, für eine Festplatte und/oder zwei ISA-Steckplätze brauchte man ein zweites, gleich großes Gehäuse mit einer externen Kabelverbindung. Nicht sehr erfolgreich und bald wieder eingestellt.\n\nDie gleiche Architektur, nur wesentlich kompakter gebaut.\nDas Motherboard beherbergte neben den damals üblichen und schon oben genannten Komponenten auch einen Controller für zwei XT-Bus-Platten, die Grafikkarte (umschaltbar zwischen Hercules und CGA) sowie eine (Amiga-)Mausschnittstelle. Die Taktrate konnte über eine Tastenkombination oder über ein Programm auf 9,54 MHz verdoppelt werden; auch eine Steigerung auf 7,16 MHz war möglich. Die Rechner kamen mit einer eingebauten Echtzeituhr und 640 kB Arbeitsspeicher, der PC-10 III besaß zwei Diskettenlaufwerke in 5,25\" Größe mit je 360 kB Speicherkapazität. Der PC-20 III wurde mit einem 360 kB Diskettenlaufwert und einem 20 MB Festplattenlaufwerk ausgeliefert.\n\nHierbei handelt es sich um IBM-kompatible PCs mit 286er, 386er und 486ern, ab PC-60 im Tower-Gehäuse und mit einer zugekauften Hauptplatine, die keine Besonderheit aufweisen.\n\nNoch später wurden nur noch komplette Fremdfabrikate, meist aus Taiwan, als Commodore-PCs verkauft. Bei ihnen war angesichts des enger werdenden Marktes und der wachsenden Konkurrenz durch Discounter wie Vobis und Escom der Ertrag nicht mehr so hoch wie vorher.\n\nIn Deutschland wurden u. a. auch Modelle des Commodore PC mit einem 3½″- und einem 5¼″-Diskettenlaufwerk verkauft.\n"}
{"id": "994441", "url": "https://de.wikipedia.org/wiki?curid=994441", "title": "Commodore x86LT", "text": "Commodore x86LT\n\nCommodore 286LT, 386LT und 486LT sind eine Serie von Laptops von Commodore, die in den späten 1980ern und frühen 1990er Jahren produziert wurden.\n\nDer Commodore 286LT war mit 9\"-LC-Monitor, einem sparsamen 80286-Prozessor, 1 MB RAM und einem 3,5\"-Laufwerk ausgestattet. Er verfügte über einen Akku. Die Festplatte (Conner) hatte 20 MB.\n\nDer Bildschirm konnte 32 Graustufen anzeigen, ein VGA-Monitor konnte angeschlossen werden.\n\nDieser Laptop war baugleich mit dem 286LT, nur dass er mit einem Intel 80386 SX und einer größeren Festplatte ausgestattet war.\n\nDieses Modell war bis auf den Prozessor (Intel 80486 SX) baugleich mit den Vorgängermodellen.\n\n"}
{"id": "994514", "url": "https://de.wikipedia.org/wiki?curid=994514", "title": "Rauheit (Bildbearbeitung)", "text": "Rauheit (Bildbearbeitung)\n\nDie Rauheit bezeichnet in der Bildbearbeitung eine Eigenschaft von Texturen. Die Rauheit kann mit mehreren verschiedenen Methoden und Algorithmen festgestellt werden, deren Genauigkeit vom vorhandenen Bildmaterial abhängen kann.\n\nZu den Methoden zur Erkennung von Rauheit zählen:\n\n"}
{"id": "995657", "url": "https://de.wikipedia.org/wiki?curid=995657", "title": "Informationsextraktion", "text": "Informationsextraktion\n\nUnter Informationsextraktion (engl. \"Information Extraction\", IE) versteht man die ingenieursmäßige Anwendung von Verfahren aus der praktischen Informatik, der künstlichen Intelligenz und der Computerlinguistik auf das Problem der automatischen maschinellen Verarbeitung von unstrukturierter Information mit dem Ziel, Wissen bezüglich einer im Vorhinein definierten Domäne zu gewinnen. Ein typisches Beispiel ist die Extraktion von Informationen über Firmenzusammenschlüsse (engl. \"merger events\"), wobei etwa aus Online-Nachrichten Instanzen der Relation merge(Firma1, Firma2, Datum) extrahiert werden. Der Informationsextraktion kommt eine große Bedeutung zu, da viele Informationen in unstrukturierter (nicht relational modellierter) Form vorliegen, zum Beispiel im Internet, und dieses Wissen durch Informationsextraktion besser erschließbar wird.\n\nInformationsextraktion kann aus zwei verschiedenen Perspektiven betrachtet werden. Einerseits als das Erkennen von bestimmten Informationen – so bezeichnet etwa Grishman IE als ”the automatic identification of selected types of entities, relations, or events in free text” (Grishman 2003) –, andererseits als das Entfernen der Informationen, die nicht gesucht werden. Letztere Sichtweise drückt etwa eine Definition von Cardie aus: ”An IE system takes as input a text and ’summarizes’ the text with respect to a prespecified topic or domain of interest” (Cardie 1997). In diesem Sinne könnte man Informationsextraktion auch als gezielte Text-Extraction bezeichnen (vgl. Euler 2001a, 2001b). Informationsextraktionssysteme sind also immer zumindest auf ein spezielles Fachgebiet, meist sogar auf bestimmte Interessengebiete (Szenarios) innerhalb eines allgemeineren Fachgebietes (Domäne) ausgerichtet. So wäre etwa in der Domäne ’Wirtschaftsnachrichten’ ein mögliches Szenario ’Personalwechsel in einer Managementposition’. Eine weitergehende Einschränkung macht Neumann, wenn er schreibt, dass das Ziel der IE ”die Konstruktion von Systemen” sei, ”die gezielt domänenspezifische Informationen aus freien Texten aufspüren \"und strukturieren\" können [...]” (Neumann 2001, Hervorhebung hinzugefügt). In diesem Zusammenhang ist zu beachten, dass eine solche Einschränkung Konsequenzen für die technische Realisierung eines Informationsextraktionssystems hat.\n\nAbzugrenzen ist das eigenständige Forschungsgebiet der Informationsextraktion von verwandten Gebieten: Text-Extraction hat eine umfassende Zusammenfassung des Inhaltes eines Textes zum Ziel (die umfassende automatische Textzusammenfassung ist insofern problematisch, als dass auch menschliche Leser bei der Aufgabe, das Wichtigste eines Textes zusammenzufassen, nie völlige Übereinstimmung erzielen werden, wenn nicht spezifiziert wurde, \"inwiefern\" die Informationen wichtig sein sollen). Textclustering bedeutet das selbstständige Gruppieren von Texten, Textklassifikation das Einordnen von Texten in vorgegebene Gruppen. Mit Information Retrieval kann die Suche nach Dokumenten in einer Dokumentenmenge (Volltextsuche) oder auch – entsprechend der wörtlichen Bedeutung – die allgemeiner formulierte Aufgabe des Abrufs von Informationen gemeint sein (vgl. Strube et al. 2001). Data-Mining bezeichnet ganz allgemein den “Prozess, Muster in Daten zu erkennen” (Witten 2000:3).\n\nGenerell lassen sich zwei Arten der Anwendung von Informationsextraktion unterscheiden: Zum einen können die extrahierten Daten sofort für einen menschlichen Betrachter gedacht sein. In diesen Anwendungsbereich fällt etwa das von Euler (2001a) zu Testzwecken entwickelte System, das aus E-Mails extrahierte Informationen als SMS weiterleitet, oder ein System, das in einer Suchmaschine zu den Treffern extrahierte Informationen anzeigt, etwa die angebotenen Positionen in Stellenanzeigen.\n\nZum anderen können die Daten für die maschinelle Weiterverarbeitung gedacht sein, sei es zur Speicherung in Datenbanken, zur Textkategorisierung oder -klassifikation oder als Ausgangspunkt für eine umfassende Text-Extraction. Bestehen die gesuchten Informationen aus mehreren Einzelinformationen, bestimmt das Anwendungsgebiet gewisse Ansprüche an das Informationsextraktionssystem. So müssen zu einer maschinellen Weiterverarbeitung die Informationen strukturiert vorliegen, während für eine Weiterverarbeitung direkt durch den Menschen auch ein unstrukturiertes Ergebnis genügen kann.\n\nWenn die gesuchten Informationen nicht aus weiteren Einzelinformationen bestehen, wie bei der Erkennung von Eigennamen, ist eine solche Unterscheidung überflüssig.\n\nZur Bewertung (Evaluation) von Informationsextraktionssystemen werden die im Information Retrieval gebräuchlichen Kriterien Vollständigkeit und Präzision (Recall und Precision) bzw. das aus diesen Werten ermittelte F-Maß verwendet. Ein weiteres Kriterium zur Bewertung der Güte des Extraktes ist der Anteil der unerwünschten Informationen (Fall-out).\n\nDie Entwicklung auf dem noch recht jungen Forschungsgebiet der Informationsextraktion wurde maßgeblich durch die Message Understanding Conferences (MUC) vorangetrieben. Die sieben MUC wurden von 1987 bis 1997 von der ’Defense Advanced Research Projects Agency’ (DARPA) – der zentralen Forschungs- und Entwicklungseinrichtung des Verteidigungsministeriums der Vereinigten Staaten – veranstaltet. Vorgegebene Szenarios waren Nachrichten über nautische Operationen (MUC-1 1987 und MUC-2 1989), über terroristische Aktivitäten (MUC-3 1991 und MUC-4 1992), Joint Ventures und Mikroelektronik (MUC-5 1993), Personalwechsel in der Wirtschaft (MUC-6 1995), sowie über Raumfahrzeuge und Raketenstarts (MUC-7 1997) (Appelt und Israel 1999). Da zur gemeinsamen Evaluation ein standardisiertes Ausgabeformat notwendig war, verwendete man ab der zweiten MUC eine gemeinsame Ausgabeschablone (Template), weshalb nahezu alle Informationsextraktionssysteme eine strukturierte Ausgabe der extrahierten Informationen leisten, eine Ausnahme hierzu bildet Euler (2001a, 2001b, 2002).\n\nInformationsextraktionssysteme können für verschiedene Aufgabenbereiche von der automatischen Analyse von Stellenanzeigen bis zur Vorbereitung einer allgemeinen Text-Extraction eingesetzt werden. Entsprechend diesen Anforderungen können die Systeme strukturierte oder unstrukturierte Ergebnisse liefern. Weiter können die Systeme völlig unterschiedliche linguistische Tiefe aufweisen, von der Extraktion durch gezielte Zusammenfassung (Euler 2001a, 2001b, 2002) mit reiner Satzfilterung, wo lediglich semantische Orientierung in Form der Wortliste gegeben ist, bis hin zu Systemen mit Analysemodulen für sämtliche Ebenen der Sprache (Phonologie, Morphologie, Syntax, Semantik, ev. auch Pragmatik). In einigen Bereichen führt unser mangelndes Verständnis für die Funktionsweise natürlicher Sprache zu einer Stagnation der Entwicklung, doch da Informationsextraktion eine eingeschränktere Aufgabe als ein komplettes Textverständnis darstellt, sind vielfach im Sinne eines ”appropriate language engineering” (Grishman 2003) den Anforderungen angemessene Lösungen (vielleicht auch gerade in Verbindung mit den Nachbargebieten) möglich. Als Beispiel hierfür möge das von Euler (2001a, 2001b, 2002) entworfene Verfahren dienen, das im Unterschied zu den die IE dominierenden Systemen lediglich unstrukturierte Ergebnisse liefert. Dafür erreicht es hohe Leistung nach F-Maß und verlangt lediglich einen geringen oder gar minimalen Annotierungsaufwand des Trainingskorpus, was eine hohe Portabilität auf neue Domänen und Szenarios bedeuten könnte, etwa in Form einer Erstellung von Wortlisten en passant bei einer Textklassifikation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "996220", "url": "https://de.wikipedia.org/wiki?curid=996220", "title": "Commodore LCD", "text": "Commodore LCD\n\nDer Commodore LCD oder auch CBM LCD war ein Laptop von Commodore, der im Januar 1985 vorgestellt, aber nie verkauft wurde.\n\nEr war ausgestattet mit einem MOS 65C102-Prozessor mit 1 MHz Taktfrequenz von MOS Technology, 32 KB RAM (erweiterbar auf 128 KB), 96 KB CMOS-ROM, einem Modem mit 300 bps, einer Tastatur mit 72 Tasten (Maschinenschreibtastatur mit 8 Funktions- und 4 Cursor-Tasten) und einem LCD-Monitor, der 80 × 16 Zeichen und monochrome Grafiken mit einer Auflösung von 640 × 128 Pixel darstellen konnte. Aufgrund des Prozessors konnte das System mit vier 1,5 V NiCd-Akkus betrieben werden.\n\nDas CMOS-ROM enthielt neben dem C64-kompatiblen BASIC 3.6 noch einen Notizblock, eine Textverarbeitung, Datenbank, Kalender, Taschenrechner und einen Dateimanager. Das BASIC enthielt keine Befehle zur Programmierung der Farbausgabe, da diese von der Hardware nicht unterstützt wurde.\n\nAn das Gerät konnte neben dem gesamten Zubehör des C64 auch ein HP-kompatibler Barcode-Scanner angeschlossen werden. Die Erweiterungen konnte man mittels eines speziellen Ports anschließen.\n\nFür dieses Gerät war sogar ein spezielles Laufwerk, das 3,5\"-Laufwerk 1561 gedacht.\n\n"}
{"id": "997374", "url": "https://de.wikipedia.org/wiki?curid=997374", "title": "Aperture (Software)", "text": "Aperture (Software)\n\nAperture (engl. „Blende“, „Blendenöffnung“) war eine Software von Apple für macOS. Sie wurde speziell für Fotografen entwickelt und soll einen schnellen und unkomplizierten Umgang mit Bilddateien bieten. Mit der Betriebssystemversion OS X Yosemite 10.10.3 wurde Aperture zusammen mit iPhoto zugunsten seines Nachfolgers Apple Fotos eingestellt.\n\nAperture wurde am 19. Oktober 2005 auf der PhotoPlus Expo in New York vorgestellt. Hervorgehoben wurde die Optimierung des Arbeitsablaufs eines Fotografen bei der Nachverarbeitung (Digital Workflow). Apple stellt Aperture als Post-Production-Werkzeug dar, das auf die Verwaltung und effiziente Korrektur von digitalen Negativen spezialisiert ist. Alle Korrekturen, die an den Daten vorgenommen werden, basieren auf den Originaldaten (RAW oder Standardbildformat) der Kamera.\n\nZiel ist es, die Handhabung von RAW-Dateien zu vereinfachen. Die aktuelle Version unterstützt neben gängigen RAW-Formaten (.ARW, .CR2, .CRW, .MOS, .NEF, .RAF, .RAW, .SRW, .TIF, .OLY, .FFF, .3FR und .DNG) auch gängige Standardbildformate (JPEG, GIF, TIFF, PNG, PDF und PSD).\n\nZudem kommt für alle Effekte das in Mac OS X integrierte Core Image zur Anwendung. Dieses ermöglicht ab der Rechnergeneration Power-Mac-G5 sehr schnelle Bildberechnungen, da sie von der Grafikkarte übernommen werden und nicht wie gewöhnlich von der CPU.\n\nDie Software arbeitet nichtdestruktiv, d. h., dass die Originaldatei nicht verändert wird und nur die Bearbeitungsschritte getrennt von der Ursprungsdatei aufgezeichnet werden. Änderungen können daher schnell rückgängig gemacht werden, und die Gefahr, die Originaldatei durch versehentliches Speichern zu überschreiben, wird verringert. Die Version 1.5 lässt neben der Verwaltung der Bilder in einer eigenen Datenbank auch die referenzierte Verwaltung im Dateisystem und auf anderen Medien zu.\n\nAm 12. Februar 2008 wurde die zweite Version von Aperture vorgestellt. Zu den Neuerungen zählten unter anderem eine übersichtlichere Oberfläche, eine höhere Geschwindigkeit und weitere Möglichkeiten zur Bildbearbeitung.\n\nAm 9. Februar 2010 wurde die dritte Version von Aperture vorgestellt. Neuerungen waren unter anderem eine automatische Erkennung von Gesichtern, das Lokalisieren und Markieren von Fotos auf Landkarten (Geotagging) sowie die verlustfreie Bearbeitung von Fotos mit Pinseln und umfangreiche neue Filter- und Bildbearbeitungsfunktionen. Außerdem unterstützte die neue Version 64-Bit-Betriebssysteme, was vor allem auf Mehrkern-Prozessoren mit größerem Arbeitsspeicher ein effizienteres Arbeiten ermöglicht.\n\nMit dem Wechsel des Softwarevertriebs von Kartonware auf Download über den Mac App Store am 6. Januar 2011 reduzierte Apple den Preis von Aperture von 199 Euro auf 63 Euro. Kunden, welche bereits die Karton-Lizenz gekauft hatten, erhielten 2013 kostenlos automatisch auch eine Mac App Store-Lizenz, welche die gleichzeitige Installation auf bis zu fünf Macs erlaubte.\n\nSeit 22. Oktober 2013 steht die Version 3.5 von Aperture bereit. Neu sind die iCloud-Fotofreigabe, Integration von SmugMug und die Unterstützung der iOS 7-Kamerafilter.\n\nAm 28. Juni 2014 informierte Apple über Umwege die Öffentlichkeit bezüglich der Einstellung der weiteren Entwicklung der zwei Apple-Fotoverwaltungslösungen Aperture und iPhoto zu Gunsten eines gemeinsamen Nachfolgerproduktes Apple Fotos, das zusammen mit OS X Yosemite erscheinen soll. Schon kurze Zeit danach ertüchtigten die Aperture-Wettbewerber wie Adobe Photoshop Lightroom und Capture One ihre Migrationssoftware, damit Neukunden möglichst bequem eine Aperture-Bibliothek mit Fotos und Metadaten importieren können.\n\nDas letzte Update von Aperture war die Version 3.6 am 16. Oktober 2014 und kam zeitgleich mit der Veröffentlichung von OS X 10.10 Yosemite heraus zur Sicherstellung der Kompatibilität mit der neuen Betriebssystemversion.\n\nDie Entwicklung des Nachfolgers Apple Fotos verzögerte sich, so dass diese erst mit dem Update Mac OS 10.10.3 am 8. April 2015 herauskam. Ein paar Tage später wurde der Verkauf von Aperture eingestellt. Der Nachfolger Apple Fotos wird mit dem Update des Betriebssystems installiert, so dass die Benutzern die Migration von großen Bibliotheken in Ruhe zu einen späteren Zeitpunkt vornehmen können. Bei der Migration werden die Fotos nicht in die neue Bibliothek kopiert und somit doppelt vorgehalten, sondern diese werden durch Links in das Nachfolgeprodukt eingebunden. Auch nach dem Import zu Apple Fotos lässt sich Aperture starten, um bspw. durch Vergleiche die ordnungsgemäße Migration zu kontrollieren.\n\nDie Software Aperture ist auch unter macOS High Sierra noch lauffähig. In Aperture erstellte Fotobücher ließen sich bis zum 31. März 2016 über den eingebauten Onlineshop als Printprodukt mit Versand beauftragen. Mittlerweile lassen sich Aperture Fotobücher nur als PDF-Datei oder Fotodateien exportieren und ausdrucken.\n\nAb Version 2.1 unterstützt Aperture auch Plug-ins von Fremdherstellern und bringt selbst ein Plug-in mit: Ab Aperture 2.1 kann man bei Fotos über das Plug-in „Nachbelichten & Abwedeln“ ähnlich wie in einer Dunkelkammer u. a. bestimmte Bildbereiche aufhellen bzw. abdunkeln. Weitere Plug-ins bietet u. a. die Firma Nik mit den Grafikfiltern Silver Efex Pro, Dfine, Viveza, Color Efex Pro, Sharpener Pro und HDR Efex Pro.\n\n"}
{"id": "1000575", "url": "https://de.wikipedia.org/wiki?curid=1000575", "title": "Hackermanifest", "text": "Hackermanifest\n\nDas Hackermanifest (Original: \"The Conscience of a Hacker\") ist ein 1986 veröffentlichter Artikel von Loyd Blankenship, der zu einem der wichtigsten Texte der Netzwerk- und Computersicherheits-Hackerkultur wurde.\n\nDer Artikel wurde am 8. Januar 1986 verfasst. Er erschien unter dem Pseudonym \"The Mentor\" in Phrack, dem bekanntesten Untergrund-Hacker-Magazin der USA. In der Folge erreichte es eine große Verbreitung innerhalb der Hackerkultur und sein Autor entwickelte sich zu einer ihrer Leitfiguren. Dementsprechend ist es mittlerweile auf zahlreichen Internetseiten in vielen Übersetzungen verfügbar.\n\nInhaltlich verteidigt das \"Hackermanifest\" ethische Grundsätze und vermittelt die Aufbruchsstimmung, die damals meist unter jungen Computerenthusiasten oder Hackern herrschte. Der Artikel kritisiert die Gesellschaft, die laut Blankenship versuche, die bestehenden Machtverhältnisse durch die Kontrolle und wirtschaftliche Ausbeutung von Information und Wissen zu erhalten. Dem stellt der Autor den Hacker im eigentlichen Sinne gegenüber, ein von Neugier angetriebenes Individuum, das einen freien, gleichberechtigten Zugriff auf Information anstrebe. Blankenship klagt die Gesellschaft, die Kriege, Morde und Massenvernichtungswaffen zulasse, dieses Streben nach Wissen jedoch kriminalisiere, als heuchlerisch und verlogen an.\n\n\n"}
{"id": "1001450", "url": "https://de.wikipedia.org/wiki?curid=1001450", "title": "AcceleRacers", "text": "AcceleRacers\n\nAcceleRacers ist eine computeranimierte Zeichentrickserie, die von der kanadischen Firma Mainframe Entertainment im Auftrag von Mattel produziert in Deutschland bei Super RTL ausgestrahlt wurde. Zu der Serie wurden zahlreiche Spielzeuge, in erster Linie Fahrzeugmodelle ebenfalls von Mattel angeboten. Die Serie stellt einen Nachfolger zum Film Hot Wheels Highway 35 dar.\n\nDie Serie handelt von Rennen in fiktiven Umgebungen, diese zählen zu dem \"Reich der Rennen\". Dieses wurde durch einen Wissenschaftler, Dr. Peter Tezla entdeckt. Den Zugang dazu stellt das \"Rad der Macht\" dar. Dies ist eine sehr leistungsstarke Energiequelle, die von den Erbauern des Reichs der Rennen entwickelt und auf der Erde zurückgelassen wurde. Dieses Rad gelangt in den Besitz der \"Renndrohnen\", Roboter, deren Anführerin das Ziel hat, die Weltherrschaft zu übernehmen. Dies soll mithilfe des \"Rads der Macht\" und Energiequellen, die im Reich der Rennen versteckt sind, den \"Accelechargern\". Um dies zu verhindern rekrutiert Dr. Tezla zwei Rennteams, die in das Reich der Rennen eindringen und die Accelecharger sichern sollen, bevor die Drohen diese erlangen können. Das Ende der Geschichte ist offen.\n\nNachdem Dr. Tezlas Fahrer den \"Highway 35\" erfolgreich beendet haben und das Rad der Macht in \"HotWheels City\" deponiert haben, entdeckte und mobilisierte Gelorum, die Antagonistin aus \"HotWheels Highway 35\", die Renndrohnen, die sich an einem unbekannten Ort auf der Erde deaktiviert aufhielten. Mit den Drohnen stahl sie das Rad der Macht aus HotWheels City und errichtete eine Festung. Das Rad der Macht nutzte sie fortan, um ihre Drohnen in das Reich der Rennen zu schicken. Dieses besteht aus zahlreichen einzelnen Rennstrecken, die teilweise sehr unterschiedlich aufgebaut sind und meist unter Extrembedingungen (extreme Witterungen, besonders harte Hindernisse etc.) befahren werden müssen.\n\nNachdem Dr. Tezla knapp den Angriff der Drohnen auf HotWheels City überleben konnte, versucht er, Rennfahrer zu sammeln, die ihm helfen würden, die Drohnen zu besiegen. Zuerst nimmt er einige Fahrer, die am Highway 35 teilgenommen haben, doch diese schaffen es nicht, die Drohen zu besiegen und gingen nacheinander in verschiedenen Renngebieten verschollen.\n\nAnschließend entsendet er seinen persönlichen Roboter-Gehilfen, um weitere Fahrer aufzuspüren. Dieser trifft auf zwei Straßenrennteams, die \"Teku \"und die \"Metal Maniacs\", die gerade ein Rennen gegeneinander fahren. Die Teams, deren Fahrer zum Teil aus Teilnehmern am Highway 35 bestehen, akzeptieren das Angebot und treffen sich mit einem weiteren Fahrer Tezlas in dessen alten Stützpunkt. Nach einem kurzen Treffen führt er sie anschließend zu Tezlas neuer getarnter Basis. Kurz nach ihrem Eintreffen wird gleich eine Rennstrecke zugänglich, die sie betreten. Bei ihrem ersten Kontakt mit den Drohen sind sie ihnen unterlegen und verlieren einige Fahrzeuge sowie das Rennen.\n\nNach Aufrüstung ihrer Fahrzeuge und Analyse des Fahrverhaltens der Drohnen gelingt es den Fahrern, den Drohnen ebenbürtig zu sein und sie sogar in mehreren Gebieten zu schlagen.\n\nBei einem späteren Rennen im Stadtgebiet, eine Rennstrecke durch eine große Stadt, machen die Fahrer Kontakt mit einer neuen Fraktion, den \"Stummen Fahrern. \"Diese Seite gibt sich sehr geheimnisvoll und verfügt über eine den anderen Teams weit überlegene Technologie. So können sie ihre Fahrzeuge tarnen und haben einen EMP-Generator an Bord ihrer Fahrzeuge. Ab diesem Zeitpunkt werden die meisten Siege und damit die meisten AcceleCharger von diesem Team geholt. Den Renndrohnen gelingt inzwischen, trotz Entwicklung neuer Fahrzeuge wie dem \"Bahnräumer, \"ein Fahrzeug mit der Größe eines Schwerlasters, das dafür gebaut wurde, gegnerische Fahrzeuge zu finden, zu fangen und zu zerlegen, kaum noch ein Sieg.\n\nIm Schrottgebiet, eine Strecke durch einen riesigen Schrottplatz, verunfallt ein Fahrer der Metal Maniacs. Die Drohen nehmen diesen gefangen und erfahren von ihm, wo sich Dr. Tezlas Stützpunkt befindet. Sie finden ihn, dringen dort ein und besetzen ihn. Die Fahrer befinden sich inzwischen auf einer Rettungsmission, um ihren verlorenen Kameraden zu retten. Sie übernahmen dafür die Kontrolle über einen Bahnräumer, damit sie nach dem Rennen im Portal der Drohen und nicht in ihrem eigenen herauskommen, Es gelingt ihnen, den Fahrer zu finden und zu befreien. Anschließend kehren sie ohne ihre Autos, da sie nur dank dem `NRE-Knopf` (NRE: NotfallRettungsEinrichtung) aus `HotWheels-City` fliehen konnten, in ihren eigenen Stützpunkt zurück und werden in einen Kampf mit den Drohnen verwickelt. Einer von ihnen, Vert Wheeler, der Gewinner des Highway 35, tritt zu einem Rennen gegen die Anführerin der Drohen an und besiegt sie. Währenddessen fliehen die Fahrer, die in enormer numerischer Unterlegenheit gegenüber den Drohnen sind und sprengen den Stützpunkt in die Luft. Dabei werden alle Drohnen vernichtet. Wheeler gewinnt das Rennen gegen Gelorum und will anschließend zur Basis zurückkehren, um seine Freunde im Kampf gegen die Drohen zu unterstützen. Da die Basis und damit das Portal aber inzwischen zerstört wurde und auch das Portal der Drohnen nicht mehr einsatzbereit ist, da ihre Anführerin, die die Energiequelle des Portals kontrollierte, gefallen ist, landet er in der Basis der Stummen Fahrer. Dort tritt der Anführer an ihn heran, der seinen Helm abnimmt und sich als Verts Vater zu erkennen gibt. Mit diesem offenen Ende endet der letzte Film.\n\nDies ist ein recht junges Team, deren Schwerpunkt auf das Tuning ihrer Fahrzeuge liegt. Außerdem legen sie auch Wert auf möglichst viel Hightech in ihren Fahrzeugen. Angeführt wird es von Nolo Pasaro. Er hat das Team von seinem Bruder Tone Pasaro übernommen, weil dieser bei einem Wettrennen mit dem Anführer der Metal Maniacs, Tork Maddox, ums Leben kam. Die Farbe der Fahrzeuge ist orange, blau und weiß.\nDas Team \"Metal Maniacs\" hat seine Fahrzeuge selbst zusammen gebastelt. Schnelle, rasante, wenn auch zerstörerische Rennen lieben sie. Sie zählen, obwohl sie das Team \"Teku\" hassen, eigentlich zu den Guten in der Serie. Denn auch sie kämpfen gegen die \"Racing Drones\". Aufgrund ihrer Haltung sind ihre Fahrzeuge recht aggressiv und in recht düsteren Farben lackiert. Ihr Anführer heißt Tork Maddox.\n\"Racing Drones\" ist das Team der \"Königsdrohne\" Gelorum. Das Team besteht aus Gelorum und ihren beiden Assistenten RD-L1 und RD-S1, welche schwer bewaffnete Kampfroboter sind, wobei RD-L1 die \"rechte Hand\" von Gelorum ist. Gelorum selbst ist ein hochentwickelte Drohne. Die Farbe der Fahrzeuge ist grün-schwarz. Ihr Schwerpunkt liegt auf Zerstörung und Betrügen.\nDie „Spezialität“ der \"Silencerz\" ist das lautlose Ausschalten. Man kann nicht zwangsläufig sagen, dass sie böse sind, da sie auch die Daten der Racing Drones gehackt haben. Die Fahrzeuge sind in silber und einem dunklen Lila gehalten. In der Serie werden nach und nach die wichtigsten Geheimnisse dieser Organisation aufgedeckt.\n\nIn Deutschland sind vier Trickfilme erschienen. Jeder Film wurde für die Ausstrahlung im Fernsehen in drei Episoden mit je 20 Minuten Länge unterteilt. Die Erstausstrahlung erfolgte in den USA am 8. Januar 2005. Alle vier Filme wurden später auf DVD von Warner Bros. Television veröffentlicht. Eine Besonderheit bei der deutschen Erstausstrahlung ist, dass nur der erste Teil komplett (d. h. alle drei Episoden) an einem Tag ausgestrahlt wurde, die folgenden wurden auf mehrere Tage aufgeteilt:\n"}
{"id": "1002891", "url": "https://de.wikipedia.org/wiki?curid=1002891", "title": "Computersabotage", "text": "Computersabotage\n\nComputersabotage ist in Deutschland gemäß des Strafgesetzbuches (StGB) ein Vergehen, welches mit Freiheitsstrafe bis zu drei Jahren, in besonders schweren Fällen bis zu zehn Jahren, oder Geldstrafe bestraft wird.\n\nComputersabotage im Sinne des deutschen Strafrechts ist das Stören einer fremden Datenverarbeitungsanlage, die für einen anderen von wesentlicher Bedeutung ist. Im Gegensatz zur vorherigen Rechtslage erfasst der neue § 303b StGB nicht nur den betrieblichen Bereich, sondern auch Datenverarbeitungen, die den privaten Bereich betreffen. Die Änderung des § 303b StGB und die Neufassung des Delikts der Computersabotage erfolgte in Umsetzung des Übereinkommens des Europarates über Computerkriminalität und der Umsetzung des Rahmenbeschlusses 2005/222/JI des Rates vom 24. Februar 2005 über Angriffe auf Informationssysteme (ABl. EU Nr. L 69 S. 67).\n\n§ 303b StGB lautet nach der letzten Änderung im Rahmen des 41. Strafrechtsänderungsgesetzes zur Bekämpfung der Computerkriminalität in der Fassung vom 7. August 2007 wie folgt:\n\n§ 303b Computersabotage\n\n(1) Wer eine Datenverarbeitung, die für einen anderen von wesentlicher Bedeutung ist, dadurch erheblich stört, dass er\nwird mit Freiheitsstrafe bis zu drei Jahren oder mit Geldstrafe bestraft.\n\n(2) Handelt es sich um eine Datenverarbeitung, die für einen fremden Betrieb, ein fremdes Unternehmen oder eine Behörde von wesentlicher Bedeutung ist, ist die Strafe Freiheitsstrafe bis zu fünf Jahren oder Geldstrafe.\n\n(3) Der Versuch ist strafbar.\n\n(4) In besonders schweren Fällen des Absatzes 2 ist die Strafe Freiheitsstrafe von sechs Monaten bis zu zehn Jahren. Ein besonders schwerer Fall liegt in der Regel vor, wenn der Täter\n\n(5) Für die Vorbereitung einer Straftat nach Absatz 1 gilt § 202c entsprechend.\n\nDenial of Service-Angriffe (DoS, DDoS) werden grundsätzlich als Tathandlungen nach § 303b Abs. 1 Nr. 2 StGB angesehen. Wenn der Angriff mittels IP-spoofing ausgelöst wird, spricht man von einer Distributed-Reflected-Denial-of-Service-Attacke (DRDoS).\nDie Nutzung von Malware unterschiedlicher Art (Viren, Würmer, Trojaner und dgl.) kann regelmäßig die Tatbestandsalternative des § 303b Abs. 1 Nr. 1 StGB erfüllen.\n\nGemäß StGB wird die einfache Computersabotage Tat nur auf Antrag verfolgt, es sei denn, dass die Strafverfolgungsbehörde wegen des besonderen öffentlichen Interesses an der Strafverfolgung ein Einschreiten von Amts wegen für geboten hält. Wird ein Antrag gestellt, erhebt die Strafverfolgungsbehörde allerdings auch nur dann Anklage, wenn sie ein (einfaches) öffentliches Interesse bejaht ( StPO). Andernfalls hat der Verletzte die Möglichkeit, Privatklage zu erheben ( Abs. 2 StPO).\n\nIn der deutschen Polizeilichen Kriminalstatistik (PKS) wurden 2008 insgesamt 2.207 Delikte von Datenveränderung bzw. Computersabotage erfasst. Die Fallzahlen der letzten Jahre können dem Diagramm (PKS 2003–2008) entnommen werden. 2009 verzeichnete die PKS 2.276 Fälle, was einem im Vergleich mit Wachstum der Fälle von Computerkriminalität in der PKS von 17,7 % eher moderaten Zuwachs von 3,1 % entspricht.\n\nBei den Computerstraftaten überwiegen männliche erwachsene Tatverdächtige ab 21 Jahren.\n\nAnhand von Statistiken (Polizeiliche Kriminalstatistik, Verurteiltenstatistik usw.) lässt sich das genaue Ausmaß der Delikte allerdings nicht ermitteln. Wegen unterschiedlicher Erfassungszeiträume/-daten, zwischenzeitlichen Änderungen der Rechtslage und anderen Einflussfaktoren sind diese Statistiken in Deutschland nicht immer direkt vergleichbar.\n\n\n\n"}
{"id": "1004206", "url": "https://de.wikipedia.org/wiki?curid=1004206", "title": "Varkon", "text": "Varkon\n\nVarkon ist ein CAD-Computerprogramm, mit dem man Zeichnungen und dreidimensionale Konstruktionen erstellen kann.\n\nUrsprünglich wurde Varkon 1984 bis 1986 an der Universität von Linköping in Schweden unter der Leitung von Johan Kjellander entwickelt. Dieser übernahm später die Geschäftsleitung der Firma Microform AB, die seitdem die Weiterentwicklung und das Markenrecht an Varkon übernommen hat. Die UNIX-Version wurde im Jahr 2000 unter der GNU General Public License freigegeben. Die Windows-Version kostet 875 US-Dollar.\n\n"}
{"id": "1006967", "url": "https://de.wikipedia.org/wiki?curid=1006967", "title": "KSTEM", "text": "KSTEM\n\nDer KSTEM-Algorithmus ist ein Algorithmus aus dem Bereich der Computerlinguistik zum automatischen Zurückführen von Wörtern auf ihren Wortstamm (Stemming). Der von Robert Krovetz entwickelte Algorithmus basiert auf morphologischen Regeln und einem Stammlexikon, mit dessen Hilfe er versucht, ein fehlerhaftes Stemming zu vermeiden. KSTEM entfernt Suffixe solange von einem Wort, bis es die durch Regeln reduzierte Wortform im Lexikon findet. Nur wenige Suffixe werden entfernt, wenn das neu zu stemmende Wort nicht im Wörterbuch steht. Wortformen, die im Lexikon gefunden werden, werden nicht gestemmt, da man annimmt, dass sie nicht weiter ableitbar sind.\n\n"}
{"id": "1008594", "url": "https://de.wikipedia.org/wiki?curid=1008594", "title": "Just for Fun (Buch)", "text": "Just for Fun (Buch)\n\nJust for Fun – Wie ein Freak die Computerwelt revolutionierte (Originaltitel: \"Just for Fun – The story of an accidental revolutionary\") ist ein Buch von Linus Torvalds und David Diamond über die Geschichte und Entwicklung des Linux-Kernels und ist gleichzeitig eine Art Biografie von Torvalds. Die englische Originalausgabe erschien 2001. Die deutsche Ausgabe stand auf der Bestsellerliste der ZEIT.\n\nDas Buch beginnt mit den ersten Erfahrungen mit Computern, die Torvalds als Kind machte, berichtet über die ersten Kontakte mit Unix und sehr detailliert über die ersten Schritte auf dem Weg zu dem neuen Betriebssystem. Dabei werden viele Fragen beantwortet, unter anderem wie es zu dem Namen Linux kam oder wieso Torvalds sich entschied, das System unter der GPL zu veröffentlichen, so dass Zitate aus diesem Buch oft zu finden sind, wenn diese Themen diskutiert werden (Siehe auch Geschichte von Linux).\n\nUnterbrochen wird der Erzählfluss von unterschiedlich langen Abschnitten über Gespräche zwischen den Autoren oder Aktivitäten während des Schreibens, die im Gegensatz zum Rest des Buches aus der Sicht von Diamond geschrieben sind. Er versucht so ein Bild von dem Privatmann Torvalds und seiner Familie zu zeichnen.\n\nEs gibt Ausgaben in vielen verschiedenen Sprachen (darunter auch Finnisch und Schwedisch).\n\n\nIngrid Hielle von der Frankfurter Allgemeinen Zeitung bewertet die Absicht von Linus Torvalds, „ein „Spaß-Buch“ zu gestalten, ein Buch, das ihm selbst beim Abfassen und dem Leser bei der Lektüre Spaß machen sollte“, als „gelungen“.\n\nFür Markus Mathys von Spektrum der Wissenschaft ist das Buch „beileibe nicht für Computerfreaks maßgeschneidert, sondern für den interessierten Laien“. Mathys lobt, dass ein „guter Index“ vorhanden sei, bemängelt jedoch, dass ein Literaturverzeichnis fehle.\n\n"}
{"id": "1009057", "url": "https://de.wikipedia.org/wiki?curid=1009057", "title": "Allplan", "text": "Allplan\n\nAllplan ist ein BIM/CAD-Programm des Unternehmens Allplan GmbH, einer Tochter der Nemetschek SE, für Architekten, Bauingenieure und Bauausführende. Allplan wird seit 1984 angeboten. Die Software ist in 20 Sprachen verfügbar und bei über 240.000 Anwendern im Einsatz.\n\nDas CAD-System Allplan unterstützt 2D-Konstruktion, 3D-Modellierung bis zum bauteilorientierten Gebäudemodell mit Mengen- und Kostenermittlung (\"4D BIM\").\n\nAllplan besitzt Schnittstellen zur Visualisierungssoftware CINEMA 4D sowie zu traditionellen Austauschformaten (u. a. DXF, DWG, DGN) und unterstützt zusätzlich die Formate IFC und PDF (auch PDF/A und 3D-PDF). Auch der Import und Export zu SketchUp und Rhino ist möglich. Ferner werden Austauschformate wie STL für Rapid Prototyping und der bidirektionale Austausch für Google SketchUp und Google Earth unterstützt.\n\nSeit 2011 gibt es das Online-Portal \"Allplan Connect\", das Funktionen wie Anwenderforen, Downloadbibliotheken, Weiterbildungsangebote, technischen Support sowie Softwareupdates und -upgrades bietet.\n\n\n\n"}
{"id": "1010094", "url": "https://de.wikipedia.org/wiki?curid=1010094", "title": "SUSE (Unternehmen)", "text": "SUSE (Unternehmen)\n\nDie SUSE LINUX GmbH ist ein international tätiges Softwareunternehmen mit Hauptsitz in Nürnberg, dessen Hauptprodukte die gleichnamigen Linuxdistributionen sowie der Kundendienst derselbigen sind. Nach mehreren Umfirmierungen handelt es sich wieder um eine eigenständige Geschäftseinheit, die der schwedische Finanzinvestor EQT Partners von Micro Focus übernahm.\n\nIm September 1992 gründeten Roland Dyroff, Burchard Steinbild, Hubert Mantel und Thomas Fehr die „Gesellschaft für Software und Systementwicklung mbH“. Der Name S.u.S.E. stand dabei als Akronym für Software- und System-Entwicklung. Als erstes eigenes Linux-Produkt wurde dabei eine Erweiterung der Linux-Distribution Slackware vertrieben, die auf 40 Disketten ausgeliefert wurde. Das Unternehmen übersetzte die Distribution in einer Kooperation mit dem Slackware-Gründer Patrick Volkerding ins Deutsche. Der Kern der Distribution blieb jedoch Slackware, bis SuSE im Mai 1996 die erste eigene Distribution, basierend auf der Jurix-Distribution von Florian La Roche, veröffentlichte.\n\n1997 eröffnete S.u.S.E. ein Büro in Oakland.\n1998 wurde der Hauptsitz von Fürth nach Nürnberg verlegt; im Dezember 1998 wurde der Unternehmensname (Firma) von \"S.u.S.E.\" in \"SuSE\" geändert. Im Laufe der folgenden Zeit eröffnete SuSE dann insgesamt sechs nationale und vier internationale (USA, Tschechien, Großbritannien und Italien) Niederlassungen. Am 25. November 2002 wurde Richard Seibt Geschäftsführer von SuSE.\n\nAm 4. November 2003 gab Novell die Übernahme der \"SuSE LINUX AG\" zum Preis von 210 Millionen US-Dollar bekannt. Die Transaktion, durch die Novell wieder stärker auf dem Markt der Betriebssysteme Fuß fassen wollte, wurde am 13. Januar 2004 abgeschlossen und ging einher mit der Umfirmierung der \"SuSE LINUX AG\" in die Gesellschaftsform \"SUSE Linux GmbH\" und \"SUSE Linux Products GmbH\". Die \"SUSE Linux Products GmbH\" führt alle Entwicklungsarbeiten durch und wurde von Markus Rex geleitet, der gleichzeitig auch für das gesamte Linux-Geschäft bei Novell weltweit verantwortlich war. Bei der Übernahme wurden sowohl die Partner- als auch die Vertriebsorganisationen in Novell integriert. Richard Seibt wurde EMEA-Chef von Novell. Er verließ zum 9. Mai 2005 ohne Angabe von Gründen das Unternehmen.\n\nIm August 2005 wurde mit dem Launch des openSUSE-Community-Projekts begonnen, die Weiterentwicklung von SUSE Linux für externe Benutzer und Entwickler zu öffnen. Novell schlug damit einen ähnlichen Weg wie Red Hat mit dem Fedora-Projekt ein. Novells zukünftige Linux-Varianten werden dann mit Hilfe der openSUSE-Community entwickelt.\n\nDer Firmensitz von SUSE wurde nach der Übernahme durch Novell von Nürnberg nach Massachusetts in den USA verlagert. Im Zuge der Übernahme Novells durch die Attachmate Group im Jahr 2011 wurde der Firmensitz zunächst nach Nürnberg zurückverlegt, nach der Übernahme durch Micro Focus 2014 dann aber erneut nach Cambridge (Massachusetts) in die USA verlegt. Inzwischen listet die Firma auf ihrer Website aber wieder Nürnberg als Sitz des Hauptquartiers.\n\nDie SUSE Linux GmbH war ein Tochterunternehmen von Novell, das organisatorisch dem Novell-Hauptsitz in den USA zugeordnet war und nicht zur deutschen Novell GmbH. Der Grund dafür ist, dass bei SUSE Linux mit Stand Dezember 2005 in Nürnberg unter anderem 250 Entwickler beschäftigt waren, die in die weltweite Produktentwicklung eingebunden sind, die von den USA aus gesteuert wird. Die deutsche Novell GmbH ist dagegen für den Vertrieb und die Vermarktung der Novell-Produkte, darunter auch SUSE-Linux-Produkte, in Deutschland, Österreich und der Schweiz zuständig. Im Zuge der Übernahme von Novell durch Attachmate im Mai 2011 ist SUSE von Novell getrennt und als eigene Geschäftseinheit SUSE wieder ausgegründet worden.\n\nNach der Übernahme von Attachmate durch Micro Focus International im Jahr 2014 blieb die SUSE LLC unter dem neuen Eigentümer weiterhin eigenständig.\n\nIm Juli 2018 wurde SUSE an die schwedische Investitionsgruppe EQT Partners AB für 2,5 Milliarden US-Dollar weiterverkauft (Zustimmungen von Aktionären und Behörden stehen noch aus). Formaler Eigentümer ist hierbei eine neu gegründete Gesellschaft „Blitz 18-679 GmbH“ mit Sitz in München.\n\nFür Privatkunden und Entwickler ist die SUSE-Linux-Version des openSUSE-Projekts gedacht, die entweder von den Projektseiten frei heruntergeladen oder per torrent bezogen werden kann.\n\nZu den Produkten für Geschäftskunden zählen die \"Server-Familien\" mit dem SUSE Linux Enterprise Server (kurz SLES). Dieser wird derzeit in drei Versionen gepflegt: Der ältere SLES 9 mit einem Linux 2.6.5 Kernel, der SLES 10 Service Pack 4 mit einem 2.6.16 Kernel und der SLES 11 Service Pack 3 mit dem 3.0 Kernel. SLES 12 wurde im Oktober 2014 fertiggestellt und nutzt derzeit Kernel 4.4. Das 2018 erschienene SLES 15 nutzt die Kernel-Version 4.12. Alle Varianten sind für mehrere Prozessorarchitekturen verfügbar, unter anderem Intel x86, AMD x86-64, IBM Power, IBM S/390 und zSeries, sowie Intel Itanium.\n\nIn der Desktop-Familie ist der Nachfolger des SUSE Linux Desktop der Novell Linux Desktop (kurz NLD). Ab der im Sommer 2006 erschienenen Version 10 basieren Server und Desktop auf einer identischen Codebasis, SUSE Linux Enterprise. Das spiegelt sich auch im Namen wider. Aus dem Novell Linux Desktop wird der SUSE Linux Enterprise Desktop („SLED“).\n\nAls Anwendung im Groupware-Segment gab es bis vor einiger Zeit den SUSE Linux Openexchange Server. Linux-Lösungen wurden durch die SUSE Framework Solutions angeboten.\n"}
{"id": "1010454", "url": "https://de.wikipedia.org/wiki?curid=1010454", "title": "Ulead MediaStudio Pro", "text": "Ulead MediaStudio Pro\n\nMediaStudio Pro ist eine Videobearbeitungssoftware von Ulead Systems, konzipiert sowohl für Hobby- als auch für Profi-Anwender.\n\nDie aktuelle Version 8 des Ulead MediaStudio Pro verfügt über eine neue Benutzeroberfläche, bietet die Möglichkeit, HDV (High Definition Video)-Material zu bearbeiten, und unterstützt Dolby Digital 5.1 Surround-Audiospuren mit Kanalsteuerung. Außerdem stehen dem Anwender integrierte Werkzeuge wie der Smart-Compositor (für Titelsequenzen und Übergänge) und Smart-Proxy (für mobile Echtzeitbearbeitung von HDV) zur Verfügung.\n\nDie Version 8 ist in Deutschland seit Ende Oktober 2005 im Handel erhältlich.\n\nMediaStudio Pro 8 ist laut Angabe des Herstellers nicht kompatibel zu Windows Vista, eine Aktualisierung ist nicht geplant.\n\nDie Software wird mittlerweile nicht mehr weiterentwickelt, allerdings gibt es eine Nachfolge-Software von Corel, VideoStudio Pro.\n\n"}
{"id": "1017787", "url": "https://de.wikipedia.org/wiki?curid=1017787", "title": "Loadlin", "text": "Loadlin\n\nLoadlin ist ein freies Betriebssystem-Startprogramm (Bootloader) mit dessen Hilfe von Windows-9x- oder MS-DOS-Systemen ein Linux-Kernel gestartet werden kann.\n\nLoadlin kann aus einem laufenden MS-DOS- oder Windows-9x-System heraus verwendet werden, um ein Linux zu starten. Dies war vor allem früher nötig, da Inkompatibilitäten zwischen Festplatten und dem BIOS eines Systems es nicht erlaubten, einen Bootloader in den Startsektor einer Festplatte zu schreiben oder auf diesen zuzugreifen. Gelegentlich wurde für gewisse Soundkarten DOS-basierte zusätzliche Anwendung benötigt, um sie überhaupt verwenden zu können.\n\nHeutzutage ist dieser Bootloader seltener anzutreffen, da er ein DOS-basiertes System (wie die Windows-9x-Linie) voraussetzt.\n\nTechnisch ist Loadlin ein DOS-Programm, welches den Kernel von Linux und eine RAM-Disk erstellen kann. Diese müssen dazu als Image-Datei unter DOS oder Windows verfügbar sein, und werden nach dem Start vom Linux-System nicht mehr benötigt.\n\nLoadlin wird als Freie Software unter der GNU General Public License (GPL, ≥2) lizenziert.\n\n"}
{"id": "1018757", "url": "https://de.wikipedia.org/wiki?curid=1018757", "title": "Cypherpunk", "text": "Cypherpunk\n\nCypherpunks (gebildet aus \"Cipher\" (engl. für: Chiffre), \"Cyber\" und \"Punk\") bezeichnet eine Gruppe von technisch versierten Menschen, die sich für die weitere Verbreitung des Datenschutzes in der Elektronischen Datenverarbeitung einsetzen. Dieses Ziel soll durch Verschlüsselung von Daten und Kommunikationskanälen realisiert werden.\nDie Kommunikation innerhalb der Gruppe findet, wenn nicht öffentlich in Diskussionsforen des Usenet, meist in Mailinglisten statt.\n\nDer Begriff des Cypherpunks wurde ursprünglich von Jude Milhon geprägt. Milhon wollte damit Cyberpunks beschreiben, welche Kryptographie nutzen.\n\nDie Cypherpunks stehen politisch dem Libertarismus nahe und fordern das strikte Offenlegen von öffentlichem Wissen (Wissen über die Gesellschaft, Politik, Wirtschaft, Wissenschaft) und Verbergen von privatem Wissen (Wissen über ein Individuum, persönliches Wissen). Daraus folgt, dass jede Form von privatem Wissen der Öffentlichkeit verborgen, das öffentliche Wissen aber für jeden zugänglich sein soll. Dieses Ideal wird von Datenschützern sehr befürwortet, führt aber häufig in den Interessenkonflikt mit Staat und Wirtschaft.\n\nCypherpunks schaffen und nutzen bei der Umsetzung ihrer Ziele verschiedene Werkzeuge. Das sind beispielsweise Web-Anonymisierer, Verschlüsselungsprogramme und Remailer. Damit soll ein gewisses Maß an Anonymität im Internet erreicht werden.\n\n\n"}
{"id": "1019136", "url": "https://de.wikipedia.org/wiki?curid=1019136", "title": "IsoBuster", "text": "IsoBuster\n\nIsoBuster (aus dem englischen \"Iso\" für die Internationale Organisation für Normung (ISO), die den ISO 9660-Standard der ISO-Abbilder verabschiedet hat, und \"Buster\" für „Knacker“ / „Cracker“) vom Unternehmen „Smart Projects“ ist ein Shareware-Windows-Anwendungsprogramm, mit dessen Hilfe Daten von beschädigten Datenträgern wiederhergestellt werden können.\nIsoBuster kann viele CD- und DVD-Formate auslesen und Dateien, zumindest teilweise, wiederherstellen. Kombinationen von mehreren beschädigten Kopien können sogar die vollständige Wiederherstellung des Speicherabbilds möglich machen.\nDer Grad der Wiederherstellung ist stark vom Ausmaß von Beschädigungen und Verschmutzungen abhängig. Mit dem Programm sind oft auch Datenträger auslesbar, die von Windows nicht erkannt werden. Das Programm kann auch bootbare ISO-Abbilder erstellen und viele Abbild-Dateiformate entpacken, selbst wenn sie mit einem anderen Betriebssystem erstellt wurden. \n\nSeit Version 1.9 (Juli 2006) von IsoBuster lassen sich bequem Verwaltete Abbilder von allen möglichen Datenspeichermedien erstellen.\nDiese Funktion steht selbst dann zur Verfügung, wenn man keine Personal-/Pro-Lizenz für IsoBuster erworben hat.\n\nDie Funktion der Verwalteten IBQ/IBP-Abbilder erweist sich als nützlich, da man damit Abbilder von Datenspeichermedien erstellen kann, welche das exakte Dateisystem inklusive der Reihenfolge der Defragmentierung und aller Partitionen des Datenspeichermediums enthält. Dementsprechend lassen sich auch verlorene Daten von solchen Abbildern wiederherstellen, ohne dass man während der Wiederherstellung auf das ursprüngliche Speichermedium angewiesen ist.\n\nSolche Abbilder sind im Alltag der Datenwiederherstellung sehr tauglich, da sie alles genau so enthalten, wie es beim Datenspeicher ist, inkl. Attributeninformationen, Position der Datei auf dem Speichermedium.\n\nMan kann solche Abbilder auch zum Vergleich mit dem originalen Medium verwenden.\n\nDas Erstellen eines Verwalteten IBQ/IBP-Abbildes kann jederzeit abgebrochen und beliebig fortgesetzt werden. Sollten einzelne Sektoren beim Erstellen des Abbilds aufgrund Lesefehler des Mediums ausgelassen gewesen sein, lassen sie sich jederzeit nachträglich ergänzen.\n\nMit IBQ und IBP-Abbildern lassen sich bekratzte Disks gut retten, indem Disk-Laufwerke gegenseitig Lücken im Abbild schließen.\n\nSollte ein Laufwerk dazu in der Lage sein, ein noch unvollständiges Abbild zu erstellen, kann ein anderes Disklaufwerk andere Teile der Disk besser lesen, welche das erste Laufwerk nicht gut lesen kann.\n\nSo ergänzen sich die Laufwerke gegenseitig und können mit IsoBuster ein möglichst vollständiges Abbild von dem Datenspeichermedium erstellen.\n\n\n"}
{"id": "1019339", "url": "https://de.wikipedia.org/wiki?curid=1019339", "title": "Microsoft Expression", "text": "Microsoft Expression\n\nMicrosoft Expression ist eine Softwarefamilie von Microsoft zur Entwicklung und Gestaltung von Bildern, Videos, HTML- und ASPX-Seiten sowie Benutzeroberflächen für Windows und plattformübergreifende Webanwendungen. Sie besteht aus vier Komponenten:\n\nVersion 3 von Expression Studio erschien am 1. Oktober 2009. Die englische Version von Expression Studio 4 ist am 7. Juni 2010 veröffentlicht worden, die deutsche Version ist seit 17. August 2010 erhältlich. Im Dezember 2012 stellte Microsoft die Weiterentwicklung der Software ein. Teile des Programmpakets (darunter Expression Web und Design) sind seitdem kostenlos als Download verfügbar.\n\nMit \"Expression\" versucht Microsoft in Marktbereiche vorzudringen, die das Softwarehaus Adobe Systems unter anderem durch den Zukauf der ehemaligen Firma Macromedia dominierte. Macromedia entwickelte und vertrieb unter anderem die nun zum Adobe-Angebot gehörende Software Dreamweaver und Flash. Mit „Expression Blend“ wurde ein Design-Werkzeug geschaffen, welches Benutzeroberflächen erzeugt, die von der (WPF) Gebrauch machen.\n\nBei \"Expression Web\" handelt es sich um eine Web-Entwicklungsumgebung, die sich insbesondere auch zur Entwicklung von Webprojekten mit Microsofts ASP.NET-Technik für dynamische Webinhalte eignet. „Expression Web“ ist somit der Nachfolger von Microsoft FrontPage, basiert aber auf einer neuen Codebasis. Ein grafischer WYSIWYG-Editor und ein Quellcodeeditor existieren gleichberechtigt nebeneinander. Änderungen im grafischen Editor werden parallel im Quellcodeeditor vollzogen und umgekehrt.\n\nMicrosoft hat mit Expression Web auf die Bindung der erstellten Seiten zum Internet Explorer und dem Internet Information Server IIS verzichtet. Im Vorläuferprodukt Frontpage waren zahlreiche Funktionen und Webbausteine enthalten, die das Erstellen einer persönlichen, gegliederten Seite unter anderem mit einem Gästebuch, einer Volltextsuche, einem Zugriffszähler, vorgefertigten Fotoalben, Top-10-Listen usw. erleichterten. Diese Webbausteine waren jedoch in den meisten Fällen weder standardkonform, noch für andere Server außer dem Microsoft IIS lauffähig. Deshalb entschloss man sich zu einem Werkzeug, welches sich mehr an Programmen wie Adobe Dreamweaver orientiert.\n\nBesonderes Augenmerk wurde bei der Entwicklung darauf gelegt, aktuelle W3C-Standards zu unterstützen. Expression Web ist unter anderem in der Lage, valides XHTML 1.0 Strict/1.1 und CSS zu generieren. Auch Funktionen wie der \"\" sollen dem Designer dabei helfen, barrierefreie Webseiten zu erstellen.\n\nMit „SuperPreview“ liefert Microsoft eine Funktion, Webseiten während der Entwicklung in verschiedenen Browsern gleichzeitig darzustellen und die Ergebnisse übereinander darzustellen, um evtl. vorhandene Rendering-Unterschiede zu erkennen.\n\n\"Expression Blend\" ist ein Designwerkzeug für interaktive Benutzeroberflächen, welche die Windows Presentation Foundation zur Ausführung benutzen. Er fungiert als WYSIWYG-Editor für XAML, eine in XML formulierte Sprache zur Beschreibung und Erstellung von solchen Oberflächen. Vektorgrafiken, Rastergrafiken, 3D-Objekte aus einigen marktführenden 3D-Programmen, Videos, Sound und Text lassen sich verwenden und mit Hilfe einer Zeitleiste animieren und in Aktion bringen.\n\nAb Version 2 mit Service Pack 1 unterstützt das Programm das plattformunabhängige Browser-Plug-in Silverlight und die Kooperation mit \"Visual Studio 2008\".\n\nAb Version 4 unterstützt das Programm die Entwicklung für Silverlight 4 und .NET 4 sowie die Kooperation mit Visual Studio 2010.\n\n\"Expression Design\" ist das Bildbearbeitungsprogramm der Expression-Familie. Es basiert auf dem Vektorprogramm „Expression“ des 2003 von Microsoft übernommenen Softwareherstellers Creature House. Das Programm ermöglicht sowohl die Bearbeitung von vektor- als auch von rasterbasierten Grafiken. Im Gegensatz zu anderen Programmen wird in Expression Design zwischen Vektor- und Pixelebenen unterschieden, wobei je nach Ebenenart die Arbeitsumgebung und die Toolpalette angepasst werden.\n\nDie Grafiken können nicht nur in die Adobe-eigenen Formate für Photoshop (.PSD), Illustrator (.AI) und Acrobat (.PDF) exportiert werden, sondern auch in XAML, welches in Expression Blend oder auch direkt in WPF-basierten Anwendungen Verwendung findet.\n\n\"Expression Encoder\" dient zur Codierung von Audio- und Videomedien in die Videocodecs H.264 und VC-1, welche auch als Microsoft IIS-Smooth Streaming-Dateien für das Online- und Live-Streaming mit Microsoft Silverlight-Client ausgegeben werden können. Über das mitgelieferte Zusatzprogramm \"Microsoft Expression Encoder Screen Capture\" können Bildschirmaufnahmen erstellt werden. \n\n"}
{"id": "1019383", "url": "https://de.wikipedia.org/wiki?curid=1019383", "title": "Lens Flare", "text": "Lens Flare\n\nLens Flare ( = ‚Linsen-Lichtschein‘) ist das helle Bild der Lichtquelle in Form der Irisblende, das durch Reflexionen an einer oder mehreren Linsen im Objektiv entsteht. Die deutsche Bezeichnung ist Blendenflecke oder Linsenreflexion.\nDie Anzahl der Lens Flares im Bild entsprechen, außer in wenigen Ausnahmen, der Anzahl der Linsen des Objektives. Ihre Form entspricht der Form der Irisblende und kann somit, je nach Anzahl der Blendenlamellen, drei- oder mehreckig und je nach Form der Lamellen ring- oder sternförmig sein.\nDie Blendenflecke können um ein Vielfaches heller sein als das aufgenommene Motiv und führen somit zu einer Kontrastabnahme. Die Farbe der Lens Flares wird maßgeblich durch die Antireflexbeschichtungen der Linsen beeinflusst.\n\nIn der klassischen Fotografie werden Blendenflecke durch geeignete Hilfsmittel wie Streulichtblenden und/oder Polarisationsfilter, zugunsten einer besseren Bildqualität unterdrückt. Filter neigen jedoch in der Regel dazu, zusätzliche Blendenflecke zu erzeugen. Bei künstlerischen Aufnahmen werden mitunter Blendenflecke bewusst genutzt, um die Helligkeit einer Lichtquelle für den Betrachter spürbar zu machen.\n\nIn der Fotografie sowie beim Film werden Lens Flares eingesetzt, um eine bestimmte Stimmung zu erzeugen. Da die Helligkeit der Kinoleinwand oder einer Fotografie weit unter der Helligkeit einer Lichtquelle wie der Sonne liegt, muss der Eindruck von extremer Helligkeit durch den Einsatz von Lens Flares vermittelt werden. Bei Weitwinkel- oder anamorphotischen Objektiven spielen die Blendenflecke eine besonders große Rolle, da auch Lichtquellen außerhalb des Bildes häufig zu Lensflares führen.\nEin gutes Beispiel für den Einsatz von Lens Flares ist der Film \"Gravity\" von 2013, in dem die großen Helligkeitsunterschiede zwischen dem tiefschwarzen Weltraum und gleißendem Sonnenlicht dargestellt werden.\nIn den Filmen \"Star Trek\" (2009) und \"Total Recall\" (2012) wurde ebenfalls sehr oft von dem Effekt Gebrauch gemacht – teilweise so massiv, dass dies sogar negativ in vielen Kritiken erwähnt wurde.\n\nTeils werden Bildstörungen dieser Art absichtlich in computergenerierte (gerenderte) Bilder oder Filmsequenzen eingerechnet, um diese realistischer wirken zu lassen. Aus diesem Grund generieren auch viele Computerspiele Blendenflecke.\n\n"}
{"id": "1020669", "url": "https://de.wikipedia.org/wiki?curid=1020669", "title": "KIM-1", "text": "KIM-1\n\nDer KIM-1 ist ein auf der 6502-CPU basierender Heimcomputer des US-amerikanischen Herstellers Commodore International, der ab 1976 käuflich erhältlich war.\n\nDer KIM-1 (Akronym aus dem englischen \"Keyboard Input Monitor\") ist ein gehäuseloser Einplatinencomputer. Seine elektronischen Hauptbestandteile sind der mit 1 MHz getaktete 6502-Mikroprozessor, zwei Spezialbausteine (insgesamt 2 KB Festwertspeicher mit dem Betriebssystem, 64 Byte RAM, diverse Eingabe-/Ausgabe-Kanäle) und 1 KB weiteren Arbeitsspeicher. Daneben enthält der KIM-1 eine sechsstellige 7-Segment-LED-Anzeige und 24 Tasten zur direkten Eingabe von Hexadezimal-Code. Einige der Eingabe-/Ausgabe-Kanäle können als serielle Schnittstelle, zum Anschluss eines externen Terminals, eines Druckers (TTY mit 20-mA-Schleife) oder zur Ansteuerung eines Kassettenrekorders eingesetzt werden. Über einen herausgeführten Systembus kann das System um zusätzlichen Speicher oder weitere Peripherie erweitert werden. Eine von Don Lancaster entwickelte Erweiterungskarte ermöglicht so beispielsweise die Darstellung von 32 × 16 Großbuchstaben auf einem Fernseher oder Monitor.\n\nEin Nachbau in Form des MCS Alpha 1 wurde mit Gehäuse nebst interner Stromversorgung und externer Tastatur ausgeliefert. Daneben wies er einige Änderungen auf: es sind acht LEDs verbaut und ein verbessertes Programm zur komfortableren Bedienung (Monitor) ist implementiert.\n\nDas Betriebssystem des KIM-1 bestehend aus \"TIM\" (Akronym für englisch \"Terminal Input Monitor\") und \"KIM\" (Akronym für englisch \"Keyboard Input Monitor\") erlaubt den Betrieb mit einem Kassettenrekorder als Massenspeicher, die Ansteuerung der LED-Anzeige und die Abfrage der Tastatur. Für den Computer existieren unter anderem höhere Programmiersprachen wie etwa Microsoft BASIC, das auf dem KIM-1 debütierte.\n\n"}
{"id": "1025852", "url": "https://de.wikipedia.org/wiki?curid=1025852", "title": "System/36", "text": "System/36\n\nDas System/36, Kurzform /36, ist ein Computermodell der Firma IBM. Das System ist ein Mehrbenutzer- und Multithreadsystem.\n\nAuf der Basis des Systems/34 wurde die /36 entwickelt. Vorgestellt wurde sie im Mai 1983. Es gab Modelle für kleine Firmen (auch als so genannte Abteilungsrechner bezeichnet) bis hin zu Modellen größerer Ausbaustufen. Die einfache Bedienung und die zu damaligen Zeiten hohe Zuverlässigkeit zeichneten die /36 aus. Neben dem System/36 wurde von IBM parallel das System/38 angeboten.\n\nDie /36 wurde als Modell 5360 und 5362 herausgebracht.\nDie Modelle 5360 und 5362 besaßen bis zu 7 MB Hauptspeicher, bis zu 1,4 GB Plattenspeicher und ein Diskettenmagazin-Laufwerk für 20 Disketten mit 8\" Durchmesser (je 10 Disketten in einem Magazin) sowie eine Einzelzuführung für 3 Disketten. Die 14\" Magnetplatten mit einem Gewicht von einigen Dutzend Kilogramm hatten beispielsweise eine Kapazität von 320 MB zu einem Preis Mitte der 80er Jahre von ca. 30.000,- DM. Der Unterschied zwischen der 5360 und der 5362 lagen in einem erweiterten Gehäuse, welches größere Anschlussmöglichkeiten für Magnetplatten bot.\n\nSpäter folgte eine kleinere 5363, an die man lokal bis zu 28 Einheiten anschließen konnte. Dieses Modell wurde später als AS/Entry 5363 vertrieben, um einen gewissen Übergang zum Nachfolgemodell AS/400 darzustellen.\n\nDer Adressraum betrug 16 Bit.\n\nDie S/36 verfügte über eine Steuerkonsole, an der Fehlermeldungen des Betriebssystems (sogenannte SRC=System Reference Codes) abgelesen und ggf. beantwortet werden konnten. Des Weiteren konnten hier spezielle Funktionen ausgeführt werden sowie das IPL (Initial Program Load) durchgeführt werden.\n\nDie Terminals wurden über Twinaxialkabel verbunden. Hier konnten maximal sieben Geräte in einem Strang angeschlossen werden, die seriell nacheinander geschaltet wurden. Dazu hatte jedes Terminal einen Ein- und Ausgang. Das Endgerät musste terminiert werden. An einem Controller konnte man sechs dieser Leitungen anschließen. Maximal konnten 72 lokale Arbeitsplätze angeschlossen werden. DFÜ-Adapter ermöglichten in Verbindung mit den 5294-Steuereinheiten und einer Datenverbindungsleitung die Datenfernverarbeitung.\n\nAls die Ära der PCs begann, wurden Anschlusskarten für diese angeboten, die die sogenannte 5250-Emulation bereitstellten. Damit konnte ein PC als Terminal am System/36 arbeiten.\n\nZur schnelleren Datensicherung wurden ab 1985 Magnetbandkassetten und -geräte angeboten, die von Kapazitäten von wenigen Dutzend bis zu einigen Hundert Mbytes reichten.\n\nDas Betriebssystem SSP verfügte über eine generelle Menüsteuerung, die die /36 im Vergleich zur /38 wesentlich bedienerfreundlicher machte. So konnte man Befehle aus einer Menüstruktur auswählen oder Parameter durch Funktionstasten auswählen, was nicht so versierten Administratoren komplizierte Funktionen erleichterte.\nProgrammiert wurde das System/36 über OCL (Operating Control Language) als Steuersprache und BASIC, Cobol oder RPG als Programmiersprache. Hierfür waren die Hilfsmittel SEU (Source Entry Utility) und SDA (Screen Design Aid) verfügbar.\nDes Weiteren wurde eine als POP (Programmer and Operator Productivity Aid) bezeichnete Hilfe angeboten, die es ermöglichte, alle Funktionen über Menüs aufzurufen.\n\nIn der Praxis war es öfter erforderlich, einen \"Compress\" durchzuführen, ähnlich dem heutigen Defragmentieren bei den Windows-PCs. Hierbei wurden frei gewordene Bereiche der Festplatte zu größeren Blöcken zusammengefasst, damit das Betriebssystem wieder über diesen Platz verfügen konnte, da neu zu schreibende Daten nur in passende Blöcke geschrieben werden konnten. Dies dauerte oft mehrere Stunden.\n\nDie hohen Wartungsgebühren und die begrenzte Erweiterbarkeit der S/36 führten dann zur Entwicklung des Nachfolgemodells AS/400 im Jahre 1988. Das System/36 lief aber noch einige Jahre in vorwiegend mittelständischen Betrieben weiter, hat sogar noch im laufenden Betrieb die Jahrtausendwende überstanden.\n\n"}
{"id": "1027346", "url": "https://de.wikipedia.org/wiki?curid=1027346", "title": "DLL-Konflikt", "text": "DLL-Konflikt\n\nDer Ausdruck DLL-Konflikt (auch DLL Hell, deutsch: „DLL-Hölle“ genannt) bezeichnet ein Problem, das durch die Installation von Dynamic Link Library (DLLs) auf den Betriebssystemen der Windows-Reihe entstehen kann. Vorwiegend sind ältere Windowsversionen betroffen, da diese nur beschränkte Möglichkeiten besitzen, um System-Dateien und DLL-Bibliotheken zu verwalten. Auch bei älteren Versionen von Mac OS treten ähnliche Probleme auf, die als Extension Conflicts (Erweiterungskonflikte) bezeichnet werden. In den verschiedenen Linux-Distributionen werden Bibliothekskonflikte meist durch den distributionseigenen Paketmanager verhindert, jedoch nicht immer.\n\nGrundsätzlich erlauben DLLs Computerprogrammen, auf ihren Programmcode und ihre Ressourcen zuzugreifen, um so identischen Code, den sonst jedes Programm selbst mitbringen müsste, zusammenzufassen. Jedoch bringen neue Programme oft neue Versionen einer bereits vorhandenen DLL (Shared Library) mit. Nun hat das Programm die Wahl, ob es bei der Installation die alte DLL überschreibt (was aber zu Kompatibilitätsproblemen mit anderen Programmen führen kann) oder eine weitere Kopie auf dem System installiert.\n\nVirtueller Speicher ermöglicht der Prozessverwaltung eines Betriebssystems, weite Teile gemeinsam benutzter Bibliotheken in gemeinsam verwendeten Seiten physischen Speichers abzulegen. Wenn viele Programme dieselbe Bibliothek verwenden, wird der gesamte Speicherbedarf damit deutlich kleiner als die Summe aller Prozesse. Dieses Verfahren setzt voraus, dass es sich um dieselbe Datei, nicht nur um einen identischen Inhalt handelt. Mehrere verwendete Kopien der gleichen Bibliothek benötigen daher nicht nur zusätzlichen Festplattenspeicher, sondern auch mehr Arbeitsspeicher.\n\nJe mehr alte und neue Programme gemeinsam verwendet werden, desto höher ist das Risiko für das Auftreten von DLL-Konflikten. Das kann zu einer unüberschaubaren Menge verschiedener DLL-Dateien führen, die zum Teil vom Betriebssystem selbst benötigt werden (und deshalb auf keinen Fall entfernt werden dürfen), zum Teil aber auch unbenötigte Reste gelöschter Installationen darstellen.\n\nAuf modernen Systemen kann zwar davon ausgegangen werden, dass die verfügbare Festplattenkapazität durch redundante DLL-Versionen kaum beeinträchtigt sein wird. Jedoch stellt, ähnlich wie bei verwaisten Einträgen in der Systemregistrierung, alleine die Tatsache, dass das System immer chaotischer wird und somit unbegründet Rechenleistung verbraucht sowie potenzielle Instabilitäten erzeugt, ein grundsätzliches Problem dar.\n\nDLLs werden von verschiedenen Programmen in unterschiedlichen Versionen benötigt, aber in der Regel an zentraler Stelle (im Windows- oder Systemverzeichnis) abgelegt und in der Windows-Registrierungsdatenbank eingetragen. Das spart Speicherplatz und kann die Programmausführung deutlich beschleunigen, da das System weniger Zeit benötigt, um die für das Programm jeweils richtige DLL-Version zu finden. Andererseits kann die Installation eines neuen Programms dazu führen, dass eine neue DLL die alte Version überschreibt. Die neue Version kann eventuell bei der älteren Software aufgrund einer schlechten oder ungenauen Spezifikation der Schnittstelle oder einer falschen Nutzung der Programmierschnittstelle Kompatibilitätsprobleme verursachen. Das ist ein Zeichen mangelhaften Softwaredesigns. Solche Probleme werden oft durch die Nutzung undokumentierter Funktionsaufrufe seitens der Anwendungsentwickler oder unspezifizierte Änderung des Verhaltens einer DLL seitens der Bibliotheksentwickler ausgelöst.\n\nBei Macintosh-Betriebssystemen wird dieser Nachteil vermieden, indem solche Systemdateien nicht an zentraler Stelle, sondern im jeweiligen Programmverzeichnis abgelegt werden. Diese Redundanz führt nicht nur zur Belegung zusätzlicher Festplattenkapazität, sondern kann auch zu Einbußen bei der Rechenleistung eines Systems führen. Aufgrund der inzwischen enorm gestiegenen Festplattenkapazitäten und der hohen Rechenleistungen moderner Prozessoren sind diese Nachteile jedoch in den Hintergrund gerückt.\n\nEs gibt erprobte Methoden, wie sich diese DLL-Konflikte vermeiden lassen. Diese Empfehlungen können jedoch nur wirksam sein, wenn sie in ihrer Gesamtheit umgesetzt werden.\n\n2001 veröffentlichte Microsoft die \".NET\"-Programmierumgebung, die ein eigenes Paketverwaltungssystem, die sogenannten \"Assemblies\", enthält. Diese Umgebung stellt vielverwendete Funktionen in einer Bibliothek bereit. Es wird vor allem Programmcode aus mehreren DLLs in einer Klasse zusammengefasst.\n\nIn \".NET\" kann jedes Programm eigene Bibliotheken verwenden und diese im Stammverzeichnis des Programms ablegen. Alternativ können Assemblies aber auch zentral im \"Global Assembly Cache (GAC)\" abgelegt werden. Dieser ist jedoch im Gegensatz zu früheren Windows-Systemen in der Lage, mehrere Versionen einer Assembly zu verwalten (Side-by-side Assembly, WinSxS), so dass jedes laufende Programm die Version der DLL, mit der es verknüpft ist, zugewiesen bekommt.\n\nDie Idee, verschiedene Versionen einer Datei zu verwalten, wird teilweise als Überrest veralteter Programmiertechniken begriffen.\n\n"}
{"id": "1028915", "url": "https://de.wikipedia.org/wiki?curid=1028915", "title": "Ice Age 2 – Jetzt taut’s", "text": "Ice Age 2 – Jetzt taut’s\n\nIce Age 2 – Jetzt taut’s (englischer Originaltitel: Ice Age: The Meltdown (zu Dt.: Eiszeit: die Schmelzung)) ist ein US-amerikanischer Computeranimationsfilm aus dem Jahr 2006. Er ist die Fortsetzung von \"Ice Age\" aus dem Jahr 2002. Regie bei der kommerziell äußerst erfolgreichen Filmkomödie führte wieder Carlos Saldanha, das Drehbuch schrieb Jon Vitti. Der Film wurde von Blue Sky Studios für 20th Century Fox produziert.\nFortgesetzt wurde die Serie 2009 mit \"Ice Age 3 – Die Dinosaurier sind los\" und 2012 mit \"Ice Age 4 – Voll verschoben\". 2016 folgte \"Ice Age – Kollision voraus!\".\n\nIn dem Film spielen mit dem Mammut Manni, dem Säbelzahntiger Diego und dem Faultier Sid dieselben Charaktere wie im ersten Teil mit, und es kommen noch Ellie, ein weibliches Mammut, die beiden Opossums Crash und Eddie sowie der flotte Tony, ein Gürteltier, hinzu.\n\nDie Eiszeit ist vorüber und das Tal, in dem sie leben, wird in Kürze mit Schmelzwasser überschwemmt werden. Ein Gletscher hat sich an einem Ende des Tals gebildet und verhindert das Abfließen des Wassers.\n\nDiego, Manfred und Sid müssen alle Bewohner des Tals warnen und gemeinsam mit ihnen aus der Heimat fliehen. Als ein Geier der Gruppe von einer Arche am anderen Ende des Tales erzählt, macht sich die Gruppe auf den Weg, die Arche zu erreichen, um mit ihr aus dem Tal zu fliehen.\n\nDas Rattenhörnchen Scrat hat noch andere Probleme: Ein Geier hat seine Eichel gestohlen und sie in seinem Nest versteckt.\n\nWährenddessen wird Manfred von Depressionen geplagt, weil er denkt, das letzte Mammut auf der Erde zu sein – bis er das Mammutweibchen Ellie trifft. Ellie fühlt sich aber als Opossum, weil sie bei Opossums aufgewachsen ist und nur deren Gepflogenheiten kennt. Ellie versucht zum Beispiel, auf einem Ast zu schlafen, was angesichts ihrer Masse zum Scheitern verurteilt ist. So hat Manni ein hartes Stück Arbeit vor sich, um Ellie vom Gegenteil zu überzeugen.\n\nDes Weiteren lernt Sid auch Diegos Geheimnis kennen: Er ist wasserscheu. Aber gerade in den Wassermassen lauern Gefahren wie z. B. der Deinosuchus Cretaceous (ein Urzeitkrokodil) oder der riesige Dunkleosteus Maelstrom (ein Panzerfisch). Diese zwei eigentlich bereits ausgestorbenen Urzeittiere sind von den tauenden Eismassen freigesetzt worden.\n\nErst als Sid und die Opossums in Gefahr geraten, kann Diego seine Wasserscheu überwinden und sie retten. Kurz vor dem Ziel wird ihr Weg von einem lebensgefährlichen Geysirfeld abgeschnitten. Ellie und Manfred geraten in Streit, und so trennt sich ihr Weg. Während Manfred, Sid und Diego das Feld überqueren und die anderen Tiere erreichen, wird Ellie in einer Höhle eingeschlossen und droht zu ertrinken. Crash und Eddie ersuchen Manfred um Hilfe, der Ellie nach einem dramatischen Unterwasserkampf mit Maelstrom und Cretaceous befreien kann.\n\nScrat kann inzwischen die Eichel aus dem Nest der Geier entwenden. Als er sie auf dem Gletscher vergräbt, spaltet sich das Eis. Der Damm ist damit gebrochen, die Wassermassen strömen aus dem Tal und alle Bewohner sind gerettet. Hier treffen Manfred und Ellie auch auf andere Mammuts. Für einen kurzen Moment scheint es, dass sich nun die Wege der beiden trennen, doch schließlich finden sie doch noch zueinander.\n\nScrat wiederum fällt in den Spalt und wird von den Wassermassen überwältigt. Im Himmel kommt er wieder zu sich, wo er von unzähligen Eicheln umgeben ist. Als er jedoch eine riesige Eichel (symbolisiert Gott; eine Szene erinnert an Die Erschaffung Adams von Michelangelo) berühren will, wird er von einem Sog erfasst und zurück zur Erde gebracht: Sid hat ihn vor dem Ertrinken gerettet. Weil Sid ihn aus dem Himmel geholt hat, zeigt Scrat seine schon bei den Piranhas eingesetzten Karatefähigkeiten.\n\nDie Premiere fand am 31. März 2006 statt. Der deutsche Kinostart war am 6. April 2006. Auf der anschließenden Kino-Tour reiste Otto Waalkes, Sprecher der Figur Sid, eine Woche durch zahlreiche deutsche Städte, um jeweils vor bzw. nach dem Film mit dem „Eiszeit-Song“ für Stimmung zu sorgen (unter anderem bei Stefan Raab in TV Total).\n\nDie Feuilletons seriöser deutscher Tageszeitungen sind des Lobes voll für die Figur des Scrat und berichten von begeisterten Reaktionen in Kinosälen, wenn er auf der Leinwand erscheint. In der Süddeutschen wird sogar die Philosophie bemüht: „Wenn die Daseins-Sorge – wie Heidegger bemerkte – das entscheidende Existential aller Lebewesen ist, so steigert sich diese Sorge bei Scrat zur Panik. Scrats Beliebtheit hat gewiss damit zu tun, dass er ein von Katastrophen aufgeschrecktes Panikgefühl bis in die zitternden Spitzen seiner Krallen verkörpert.“ (Rainer Gansera: \"Ein Denkmal für Scrat!\", 6. April 2006)\n\nDas Lexikon des internationalen Films schrieb: „Komplett computergenerierter Animationsfilm, der inhaltlich nur schwer an den Vorgänger anschließen kann, visuell das Niveau aber höher schraubt. Wie im ersten Teil bietet vor allem das Urzeit-Hörnchen Scrat ungetrübtes Vergnügen.“\n\nIn den USA spielte der Film am Startwochenende 68 Millionen US-Dollar an den Kinokassen ein. Dies ist das höchste Einspielergebnis in einem März in den US-Kinos. Der erste Teil hatte im selben Zeitraum „nur“ 46 Millionen Dollar eingespielt. Weltweit hat \"Ice Age 2\" über 651 Millionen Dollar eingespielt.\n\nIn Deutschland sahen den Film am Starttag 440.000 Besucher. Dies war der beste Start für einen Animationsfilm in den deutschen Kinos. Nach nur sieben Tagen erhielt der Film die \"Goldene Leinwand\" für 3,3 Millionen und nach 14 Tagen die \"Goldene Leinwand mit Stern\" für über 6 Millionen Zuschauer. In Folge wurde \"Ice Age 2: Jetzt taut’s\" 2006 mit 8,7 Millionen Besuchern zum erfolgreichsten Animationsfilm in Deutschland und zum erfolgreichsten Film in Deutschland 2006.\n\nIn Österreich eröffnete \"Ice Age 2\" mit 284.793 Zuschauern und damit dem besten Startwochenende bisher. Mit 1,185 Millionen erhielt der Film das einzige Diamond Ticket 2006 und wurde zum erfolgreichsten Film in Österreich in diesem Jahr.\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh dem Film das Prädikat besonders wertvoll.\n\nDas dargestellte Ereignis des sich durch schmelzende Gletscher in einer katastrophalen Flut entleerenden Eisstausees, ist geologisch vor allem auf dem amerikanischen Doppelkontinent häufig nachzuweisen und bereits seit dem 19. Jahrhundert wissenschaftlich erforscht. Genannt seien hier der Missoulasee (Missoula-Fluten) und der Agassizsee. Der plötzliche Abfluss des \"Agassizsees\" hatte vermutlich erhebliche Auswirkungen auf das Erdklima.\n\nDie Geier singen den Song \"Food, Glorious Food\", nachdem Sid sich fragte, was sie wohl denken mögen. Das Lied stammt aus dem Musical Oliver.\n\nDer offizielle gleichnamige Roman zum Film erschien im März 2006 bei Xenos mit dem Titel \"Ice Age 2 – Das Buch zum Film\". Autoren waren Jennifer Frantz und Peter Clarke. Im Februar des Jahres 2006 erschien auch ein Sammelband mit dem Roman und dem Roman zum ersten Teil. Der wurde diesmal von Glenn Dakin geschrieben und erschien auf deutsch im Dorling-Kindersley-Verlag.\n\nDie deutsche Synchronisation entstand nach einem Dialogbuch von Michael Nowka unter seiner Dialogregie im Auftrag der Berliner Synchron.\n\n"}
{"id": "1030696", "url": "https://de.wikipedia.org/wiki?curid=1030696", "title": "Free Standards Group", "text": "Free Standards Group\n\nDie Free Standards Group (FSG) war eine 1998 gegründete, unabhängige und gemeinnützige Organisation mit dem Ziel, die Nutzung freier und quelloffener Software durch die Entwicklung und Verbreitung entsprechender Standards zu stärken. Das bekannteste Projekt der \"Free Standards Group\" war die Arbeitsgruppe der \"Linux Standard Base\".\n\n2007 fusionierte sie mit den \"Open Source Development Labs\" (OSDL) zur Linux Foundation, die die Aufgaben, die Projekte und die Mitglieder sowohl von der \"Free Standards Group\" als auch von den \"Open Source Development Labs\" übernahm.\n\nDie \"Free Standards Group\" konzentrierte sich zuletzt auf folgende Hauptaufgaben:\n\nDie \"Free Standards Group\" wurde dabei von vielen Unternehmen aus dem Linux-Umfeld unterstützt, dazu gehörten 2006 unter anderem \"Red Hat, Novell, Mandriva, Progeny, Turbolinux, Red Flag Software, Miracle Linux, Beijing Co-Create Software Company, Sun Wah Linux, Thiz Linux, IBM, Intel, HP, AMD, Dell, Sun Microsystems, Veritas Software, BakBone Software, Google, Qt Software\" und \"MySQL\".\n\nNeben der Arbeitsgruppe der \"Linux Standard Base\" gab es noch weitere Arbeitsgruppen, die sich mit einer Standardisierung beschäftigten. So arbeitete die Gruppe \"OpenI18n\" an einer Basis für die sprachliche Internationalisierung von Programmen und Distributionen, um diese später wiederum einfacher zu lokalisieren. Die Arbeitsgruppe \"OpenPrinting\" beschäftigte sich mit den Ansprüchen an professionelle Drucker-Lösungen wie Management, Verlässlichkeit, Sicherheit, Skalierbarkeit usw., während die Gruppe \"Accessibility\" Standards definierte, um den Zugang zu Linux-Systemen auch behinderten Menschen zu ermöglichen.\n\nAndere, teilweise kleinere Arbeitsgruppen, beschäftigten sich jeweils mit anderen Teilbereichen zum Gesamtthema Standardisierung von Linux.\n\n"}
{"id": "1031358", "url": "https://de.wikipedia.org/wiki?curid=1031358", "title": "DCC Alliance", "text": "DCC Alliance\n\nDie DCC Alliance (DCC Common Core) war ein Zusammenschluss mehrerer Organisationen und Einzelpersonen mit dem Ziel, einen gemeinsamen Standard für debianbasierte Linux-Distributionen zu schaffen. Dadurch sollte die kommerzielle Anerkennung von Debian-Systemen weltweit beschleunigt und forciert werden. Mitglieder der im Juli 2005 gegründeten und bis 2007 aktiven DCC Alliance waren dabei fast alle damaligen großen debianbasierten Projekte wie Knoppix, Linspire, Progeny, SimplyMEPIS, Xandros, User Linux, gnuLinEx, Sun Wah Linux und credativ.\n\nNach dem Scheitern von United Linux schlossen sich Mandrake, Conectiva, Turbolinux und Progeny Linux Systems am 16. November 2004 zum \"Linux Core Consortium\" (LCC) zusammen mit der Absicht, den Linux Standard Base (LSB 2.01) verstärkt zu unterstützen. Das Konsortium lud Red Hat/Fedora, Novell/SUSE, Hewlett-Packard, Computer Associates und Sun Microsystems zum Beitritt ein, aber alle genannten Firmen verweigerten ihre Unterstützung.\n\nSo blieb es wieder nur bei der Absicht. Weil sich Mandriva und Turbolinux zudem nicht einig waren, zerbrach das LCC wieder. Im Juli 2005 schaute sich daraufhin Progeny als treibende Kraft hinter dem alten Linux Core Consortium nach anderen Partnern um. Es gelang Progeny dabei, die meisten wichtigen debianbasierten Projekte um sich zu scharen und aus dem LCC den DCC zu formen. \n\nDer ursprüngliche Name „Debian Core Consortium“ war in Anlehnung an „Linux Core Consortium“ gewählt worden. Aber um Verwechslungen mit dem Debian-Projekt zu vermeiden, erfolgte eine weitere Umbenennung in \"DCC Common Core\".\n\n\n\n"}
