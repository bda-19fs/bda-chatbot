{"id": "379925", "url": "https://de.wikipedia.org/wiki?curid=379925", "title": "Intervallarithmetik", "text": "Intervallarithmetik\n\nIntervallarithmetik bezeichnet in der Mathematik eine Methodik zur automatisierten Fehlerabschätzung auf Basis abgeschlossener Intervalle.\nDabei werden nicht genau bekannte reelle Größen formula_1 betrachtet, die aber durch zwei Zahlen formula_2 und formula_3 eingegrenzt werden können. Dabei kann formula_1 zwischen formula_2 und formula_3 liegen oder auch einen der beiden Werte annehmen. Dieser Bereich entspricht mathematisch gesehen dem Intervall formula_7. Eine Funktion formula_8, die von einem solchen unsicheren formula_1 abhängt, kann nicht genau ausgewertet werden. Es ist schließlich nicht bekannt, welcher Zahlenwert innerhalb von formula_7 für formula_1 eigentlich eingesetzt werden müsste. Stattdessen wird ein möglichst kleines Intervall formula_12 bestimmt, das gerade die möglichen Funktionswerte formula_13 für alle formula_14 enthält. Durch gezielte Abschätzung der Endpunkte formula_15 und formula_16 erhält man eine neue Funktion, die wiederum Intervalle auf Intervalle abbildet.\n\nDieses Konzept eignet sich unter anderem zur Behandlung von Rundungsfehlern direkt während der Berechnung und falls Unsicherheiten in der Kenntnis der exakten Werte physikalischer und technischer Parameter vorliegen. Letztere ergeben sich oft aus Messfehlern und Bauteil-Toleranzen. Außerdem kann Intervallarithmetik dabei helfen, verlässliche Lösungen von Gleichungen und Optimierungsproblemen zu erhalten.\nAls Beispiel soll hier die Berechnung des Körpermasseindex (BMI von engl. \"Body Mass Index\") betrachtet werden. Der BMI ist die Körpermasse in Kilogramm geteilt durch das Quadrat der Körpergröße in Metern. Zur Illustration soll die Gewichtsbestimmung (eigentlich Massebestimmung) mit Hilfe einer Badezimmerwaage erfolgen, bei der das Gewicht auf ein Kilogramm genau abgelesen werden kann. Es werden also niemals Zwischenwerte bestimmt – etwa 79,6 kg oder 80,3 kg –, sondern auf ganze Zahlen gerundete Angaben. Dabei ist es natürlich sehr unwahrscheinlich, dass man wirklich exakt 80,0 kg wiegt, wenn dies angezeigt wird. Bei üblicher Rundung auf den nächstliegenden Gewichtswert liefert die Waage 80 kg für jedes Gewicht zwischen 79,5 kg und 80,5 kg. Den entsprechenden Bereich aller reellen Zahlen, die größer oder gleich 79,5 und gleichzeitig kleiner oder gleich 80,5 sind, kann einfach als Intervall formula_17 aufgeschrieben werden. Um Verwechslungen zu vermeiden setzt man meistens einen Punkt statt eines Kommas als Dezimaltrennzeichen.\n\nFür einen Menschen, der 80 kg wiegt und 1,80 m groß ist, liegt der \"BMI\" bei ungefähr 24,7. Bei einem Gewicht von 79,5 kg und gleicher Körpergröße müsste aber nur ein Wert von 24,5 angenommen werden, wohingegen 80,5 kg schon fast 24,9 entsprechen. Der tatsächliche \"BMI\" liegt also in dem Bereich formula_18. In diesem Fall kann der Fehler in der Praxis zwar noch vernachlässigt werden, jedoch ist das nicht bei allen Rechnungen der Fall. Beispielsweise schwankt das Gewicht auch im Laufe eines Tages, so dass der \"BMI\" hier durchaus zwischen 24 (noch normalgewichtig) und 25 (schon übergewichtig) variieren kann. Ohne detaillierte Rechnung können aber nicht immer von vornherein Aussagen darüber getroffen werden, ob ein Fehler letztendlich groß genug ist, um maßgeblichen Einfluss zu haben.\n\nIn der Intervallarithmetik wird der Bereich möglicher Ergebnisse ausdrücklich berechnet. Vereinfacht gesagt, rechnet man nicht mehr mit Zahlen, sondern mit Intervallen, die nicht genau bekannte Werte repräsentieren. Ähnlich wie ein Fehlerbalken um einen Messwert drückt ein Intervall das Ausmaß der Unsicherheit bezüglich der zu berechnenden Größe aus.\nHierfür werden einfache Rechenoperationen, wie die Grundrechenarten oder trigonometrische Funktionen, für das Rechnen mit Intervallen neu definiert, um äußere Grenzen eines gesuchten Wertebereiches zu erhalten.\n\nDas Hauptaugenmerk bei der Intervallarithmetik liegt darauf, auf möglichst einfache Art und Weise obere und untere Schranken für den Wertebereich einer Funktion in einer oder mehreren Variablen zu bestimmen. Dabei müssen diese Schranken nicht unbedingt dem Supremum bzw. Infimum entsprechen, da die genaue Berechnung dieser Werte oft zu schwierig ist. (Es lässt sich zeigen, dass diese Aufgabenstellung im Allgemeinen NP-schwer ist.)\n\nÜblicherweise beschränkt man sich auf die Behandlung abgeschlossener, reeller Intervalle, also Mengen der Form\nwobei auch formula_20 und formula_21 zulässig sind. Dabei entsprechen formula_22 und formula_23 den meist halboffen geschriebenen Intervallen, die alle reellen Zahlen kleiner oder gleich formula_3 bzw. größer oder gleich formula_2 umfassen. Entsprechend bezeichnet das Intervall formula_26 die gesamte reelle Achse.\n\nWie beim klassischen Rechnen mit Zahlen muss zunächst einmal definiert werden, wie die arithmetischen Operationen und elementaren Funktionen auf Intervalle anzuwenden sind. Komplexere Funktionen können dann aus diesen Grundelementen zusammengesetzt werden (Lit.: Kulisch, 1989).\n\nZu Erläuterung wird nochmal auf das Beispiel vom Anfang zurückgegriffen. Bei der Bestimmung des Körpermasseindex spielt neben dem Gewicht auch die Körpergröße eine Rolle. Diese wird üblicherweise nur in ganzen Zentimetern gemessen werden: eine Angabe der Körpergröße von 1,80 Meter bedeutet also eigentlich eine Körpergröße irgendwo zwischen 1,795 m und 1,805 m. Diese Ungenauigkeit muss zusätzlich zu der Schwankungsbreite beim Gewicht, das zwischen 79,5 kg und 80,5 kg liegt, eingerechnet werden. Für den \"BMI\" muss nun wie gesagt die Körpermasse in Kilogramm durch das Quadrat der Körpergröße in Metern geteilt werden.\nSowohl für 79,5 kg und 1,795 m als auch für 80,5 kg und 1,805 m ergibt sich dafür ungefähr 24,7. Es muss nun aber auch berücksichtigt werden, dass die fragliche Person möglicherweise nur 1,795 m groß ist bei einem Gewicht von 80,5 kg – oder auch 1,805 m bei 79,5 kg. Auch die Kombinationen aller möglichen Zwischenwerte müssen in die Betrachtung eingehen.\nMit Hilfe der im Folgenden festgelegten Intervallarithmetik kann der intervallwertige \"BMI\"\ntatsächlich ausgerechnet werden.\n\nEine Operation formula_28 zwischen zwei Intervallen, wobei formula_28 beispielsweise für Addition oder Multiplikation steht, muss die Bedingung\n\nerfüllen. Für die vier Grundrechenarten ergibt sich daraus\n\nfalls formula_32 zulässig ist für alle\nformula_33 und formula_34.\n\nFür praktische Anwendungen lässt sich dies noch weiter vereinfachen:\n\nFür die Division durch ein Intervall, das die Null enthält, definiert man zunächst einmal\nFür formula_43 gilt formula_44,\nso dass man eigentlich\nformula_45 setzten müsste. Dadurch verliert man allerdings die Lücke formula_46 und damit wertvolle Informationen. Üblicherweise rechnet man daher mit den Teilmengen formula_47 und formula_48 einzeln weiter.\n\nWeil innerhalb einer Intervallrechnung auch mehrere solcher Aufspaltungen auftreten können, ist es manchmal sinnvoll, das Rechnen mit sogenannten \"Multi-Intervallen\" der Form formula_49 zu systematisieren. Die entsprechende \"Multi-Intervall-Arithmetik\" pflegt dann eine disjunkte Menge von Intervallen und sorgt dann beispielsweise auch dafür, sich überschneidende Intervalle zu vereinigen (Lit.: Dreyer, 2005).\n\nDa man eine Zahl formula_50 als das Intervall formula_51 interpretieren kann, erhält man sofort eine Vorschrift zur Kombination von intervall- und reellwertigen Größen.\n\nMit Hilfe dieser Definitionen lässt sich bereits der Wertebereich einfacher Funktionen,\nwie formula_52 bestimmen.\nSetzt man beispielsweise formula_53, formula_54 und formula_55,\nso ergibt sich\n\nInterpretiert man formula_57 als Funktion einer Variablen\nformula_1 mit intervallwertigen Parametern formula_2 und formula_3, dann lässt sich die Menge aller Nullstellen dieser Funktionenschar leicht bestimmen. Es gilt dann\n\ndie möglichen Nullstellen liegen also im Intervall formula_62.\nWie im obigen Beispiel kann die Multiplikation von Intervallen oft auf die Multiplikation nur zweier Zahlen zurückgeführt werden. Es gilt nämlich\n\nDie Multiplikation lässt sich hier als Flächenbestimmung eines Rechtecks mit variierenden Kantenlängen interpretieren. Das intervallwertige Ergebnis deckt dann alle Werte von der kleinst- bis zu größtmöglichen Fläche ab.\n\nEntsprechendes gilt, wenn eines der beiden Intervalle ganz im nicht-positiven und das andere ganz im nicht-negativen Bereich der reellen Achse liegt. Generell muss bei der Multiplikation noch beachtet werden, dass das Ergebnis sofort auf formula_26 gesetzt werden muss, falls unbestimmte Werte, wie formula_66 auftreten. Dies tritt z. B. bei einer Division auf, bei der Zähler und Nenner beide Null enthalten.\n\nUm intervallwertige Größen leichter in mathematischen Formeln zu erkennen, zweckentfremdet man die eckigen Klammern zur „Markierung“.\n\nDementsprechend bezeichnet formula_67 ein Intervall und die Menge aller reellen Intervalle wird als\nabgekürzt. Für eine Box oder einen Vektor von Intervallen formula_69 verwendet man zusätzlich fetten Schriftschnitt: formula_70.\n\nBei einer derart kompakten Notation ist zu beachten, dass formula_71 nicht mit einem sogenannten \"uneigentlichen\" Intervall formula_72 verwechselt wird, bei dem obere und untere Grenze übereinstimmen.\n\nUm auch Funktionen mit Intervallmethoden behandeln zu können, deren Terme sich nicht aus den Grundrechenarten ergeben, muss man auch noch weitere \"elementare Funktionen\" für Intervalle neu definieren. Dabei nutzt man vorhandene Monotonieeigenschaften aus.\n\nFür monotone Funktionen in einer Variablen lässt sich der Wertebereich ebenfalls leicht bestimmen. Ist formula_73 monoton steigend oder fallend in einem Intervall formula_74, dann gilt für alle Werte formula_75 mit formula_76 die Ungleichung\n\nDen Wertebereich des Intervalls formula_79 erhält man durch Auswertung der Funktion an den Endpunkten formula_80 und formula_81:\n\nDaher lassen sich folgende \"Intervallisierungen\" elementarer Funktionen leicht definieren:\n\nEs ist außerdem noch wichtig, den Wertebereich für gerade Potenzen bestimmen zu können. Im Gegensatz zur üblichen Numerik ist es hier nicht sinnvoll, die Berechnung auf die Multiplikation zurückzuführen.\nBeispielsweise bewegt sich formula_90 für formula_91 innerhalb des Intervalles formula_92, wenn formula_93. Versucht man formula_94 aber durch Multiplikationen der Form formula_95 zu bestimmen, so erhält man in jedem Fall als Ergebnis formula_96.\n\nSinnvoller ist es hier, die Parabel formula_90 als Zusammensetzung einer monoton fallenden (für formula_98) und einer monoton steigenden Funktion (für formula_99) zu betrachten. Es gilt also für gerade formula_89:\n\n\nAllgemeiner kann man sagen, dass es für \"stückweise\" monotone Funktionen ausreicht, diese an den Endpunkten formula_106 eines Intervalls formula_74, sowie an den in formula_74 enthaltenen sogenannten \"kritischen Punkten\" auszurechnen. Die kritischen Punkte entsprechen hierbei den Stellen, an denen sich die Monotonieeigenschaften ändern.\n\nDies lässt sich z. B. auf Sinus und Kosinus anwenden, die zusätzlich an Stellen formula_109 bzw. formula_110 für alle formula_111 ausgewertet werden müssen. Hierbei spielen höchstens fünf Punkte eine Rolle, da man als Ergebnis sofort formula_96 festlegen kann, wenn das Eingangsintervall mindestens eine ganze Periode enthält. Außerdem müssen Sinus und Kosinus lediglich an den Randpunken neu evaluiert werden, da die entsprechenden Werte an den kritischen Stellen – nämlich −1, 0, +1 – vorab abgespeichert werden können.\n\nIm Allgemeinen findet man für beliebige Funktionen keine derart einfache Beschreibung des Wertebereiches. Man kann diese aber oft auf Intervalle ausdehnen.\nWenn formula_113 eine Funktion ist, die einen reellwertigen Vektor auf eine reelle Zahl abbildet, dann nennt man formula_114 eine \"Intervallerweiterung\" von formula_8, wenn gilt\n\nDies definiert die Intervallerweiterung nicht eindeutig. So sind beispielsweise sowohl formula_117 als auch formula_118 zulässige Erweiterungen der Exponentialfunktion. Da möglichst scharfe Erweiterungen gewünscht sind, also solche, die so genau wie möglich den gesuchten Wertebereich approximieren, wird man in diesem Fall eher formula_119 wählen, da sie sogar den exakten Bereich bestimmt.\n\nDie \"natürliche Intervallerweiterung\" erhält man, indem man in der Funktionsvorschrift formula_120 die Grundrechenarten und elementaren Funktionen durch ihre intervallwertigen Äquivalente ersetzt.\n\nDie \"Taylor-Intervallerweiterung\" (vom Grad formula_121) einer formula_122 mal differenzierbaren Funktion formula_8 ist definiert durch\n\nfür ein formula_126,\n\nwobei formula_127 das Differential formula_128-ter Ordnung von formula_8 am Punkt formula_130 und formula_131 eine Intervallerweiterung des \"Taylorrestgliedes\"\n\nbezeichnet.\n\nDa der Vektor formula_133 zwischen formula_134\nund formula_130 mit formula_136 liegt, lässt sich formula_133 ebenfalls durch formula_70 abschätzen.\nÜblicherweise wählt man für formula_130 den Mittelpunkt des Intervallvektors und die natürliche Intervallerweiterung zur Abschätzung des Restgliedes.\n\nDen Spezialfall der Taylor-Intervallerweiterung vom Grad formula_140 bezeichnet man auch als \"Mittelwert\"-Intervallerweiterung.\nFür eine Intervallerweiterung der Jacobi-Matrix formula_141\nerhält man hier\n\nEine nichtlineare Funktion kann so durch lineare Funktionen eingegrenzt werden.\n\nDie Methoden der klassischen Numerik können nicht direkt für die Intervallarithmetik umgesetzt werden, da hierbei Abhängigkeiten meist nicht berücksichtigt werden.\n\nUm effizient mit Intervallen rechnen zu können, muss eine konkrete Implementierung kompatibel zum Rechnen mit Gleitkommazahlen sein. Die oben definierten Operationen basieren auf exakter Arithmetik, die bei schnellen numerischen Lösungsverfahren nicht zur Verfügung steht. Der Wertebereich der Funktion formula_143\nfür formula_144 und formula_145 wäre beispielsweise formula_146. Führt man die gleiche Rechnung mit einstelliger Präzision durch, so würde das Ergebnis üblicherweise zu formula_147 gerundet. Da aber formula_148\nwürde dieser Ansatz den Grundprinzipien der Intervallarithmetik widersprechen, da ein Teil des Wertebereiches von formula_149 verloren geht.\nStattdessen ist hier die \"nach außen gerundete\" Lösung formula_150 vorzuziehen.\n\nDie Norm IEEE 754 definiert neben Standarddarstellungen binärer Gleitkommazahlen auch genaue Verfahren für die Durchführung von Rundungen. Demnach muss ein zu IEEE 754 konformes System dem Programmierer neben dem \"mathematischen\" Runden (zur nächsten Gleitkommazahl) noch weitere Rundungsmodi bereitstellen: \"immer aufrunden\", \"immer abrunden\" und Rundung gegen 0 (Ergebnis betragsmäßig verkleinern).\n\nDas benötigte \"nach außen Runden\" lässt sich also durch entsprechendes Umschalten der Rundungseinstellungen des Prozessors beim Berechnen von oberer und unterer Grenze bewerkstelligen. Alternativ kann dies durch Hinzuaddition eines geeigneten schmalen Intervalls formula_151 erreicht werden.\n\nDas sogenannte \"Abhängigkeitsproblem\" ist ein Haupthindernis bei der Anwendung der Intervallarithmetik.\nObwohl der Wertebereich der elementaren arithmetischen Operationen und Funktionen mit Intervallmethoden sehr genau bestimmt werden kann, gilt dies nicht mehr für zusammengesetzte Funktionen. Falls ein intervallwertiger Parameter mehrfach in einer Rechnung auftritt, wird jedes Auftreten unabhängig voneinander behandelt. Dies führt zu einer ungewollten Aufblähung der resultierenden Intervalle.\nZur Illustration sei eine Funktion formula_8 durch den Ausdruck\nformula_153 gegeben. Der Wertebereich dieser Funktion über dem Intervall formula_154 beträgt eigentlich formula_155. Um die natürliche Intervallerweiterung zu erhalten, rechnet man aber formula_156, was einen etwas größeren Bereich ergibt. In der Tat berechnet man eigentlich Infimum und Supremum der Funktion formula_157 über formula_158.\nHier würde man also besser eine alternative Formulierung für formula_8 verwenden, die die Variable formula_1 nur einmal verwendet. In diesem Fall kann man den Ausdruck formula_153 einfach durch quadratische Ergänzung zu\nformula_162 umformen.\n\nDann liefert die entsprechende Intervallrechnung\nauch den richtigen Wertebereich.\n\nIm Allgemeinen lässt sich zeigen, dass man tatsächlich den genauen Wertebereich erhält, wenn jede Variable nur einmal auftaucht. Allerdings lässt sich nicht jede Funktion geeignet auflösen.\nDie durch das Abhängigkeitsproblem verursachte \"Überschätzung\" des Wertebereiches kann soweit gehen, dass das Resultat einen derart großen Bereich umfasst, der keine sinnvollen Schlüsse mehr zulässt.\n\nEine zusätzliche Vergrößerung des Wertebereichs ergibt sich aus dem Einhüllen von Bereichen, die nicht die Form eines Intervallvektors haben. Die Lösungsmenge des linearen Systems\n\nist genau die Strecke zwischen den Punkten formula_166 und formula_167.\nIntervallmethoden liefern hier aber im besten Fall das Quadrat formula_168, das die tatsächliche Lösung einhüllt (\"Einhüllungs- oder „Wrapping“-Effekt\").\n\nEin lineares Intervallsystem besteht aus einer intervallwertigen Matrix formula_169 und einem Intervallvektor formula_170. Gesucht ist dann eine möglichst schmale Box formula_171, die alle Vektoren\nformula_172 enthält, für die es ein Paar formula_173 mit formula_174 und formula_175 gibt, das die Gleichung\nerfüllt.\n\nFür quadratische Systeme – also für formula_177 – lässt sich ein solcher Intervallvektor formula_70, der alle möglichen Lösungen enthält, sehr einfach mit dem \"Intervall-Gauß-Verfahren\" bestimmen. Hierfür ersetzt man die numerischen Operationen, die bei dem aus der linearen Algebra bekannten gaußschen Eliminationsverfahren auftauchen, durch ihre Intervallversionen. Da allerdings während der Abarbeitung dieser Methode die intervallwertigen Einträge von formula_179 und formula_180 mehrfach in die Rechnung eingehen, leidet dieser Ansatz sehr stark an dem Abhängigkeitsproblem. Folglich bietet sich der Intervall-Gauß nur für grobe erste Abschätzungen an, die zwar die gesamte Lösungsmenge enthalten, aber auch einen sehr großen Bereich außerhalb davon.\n\nEine grobe Lösung formula_70 kann oft durch eine Intervallisierung des \"Gauß-Seidel-Verfahrens\" verbessert werden.\nDiese ist folgendermaßen motiviert:\nDie formula_128-te Zeile der intervallwertigen linearen Gleichung\n\nlässt sich nach der Variablen formula_184 auflösen, falls die Division\nformula_185 erlaubt ist. Es gilt demnach gleichzeitig\nMan kann also nun formula_188 durch\nersetzen, und so den Vektor formula_70 elementweise verbessern.\nDa das Verfahren effizienter für diagonaldominante Matrizen ist, versucht man oft statt des Systems formula_191 die durch Multiplikation mit einer geeigneten reellen Matrix formula_192 entstandene Matrixgleichung\nzu lösen. Wählt man beispielsweise formula_194 für die Mittelpunktsmatrix formula_174, so ist formula_196 eine äußere Näherung der Einheitsmatrix.\n\nFür die oben genannten Methoden gilt allerdings, dass sie nur dann gut funktionieren, wenn die Breite der vorkommenden Intervalle hinreichend klein ist. Für breitere Intervalle kann es sinnvoll sein, ein Intervall-lineares System auf eine endliche (wenn auch große) Anzahl reellwertiger linearer Systeme zurückzuführen. Sind nämlich alle Matrizen formula_174 invertierbar, so ist es vollkommen ausreichend, alle möglichen Kombinationen an (oberen und unteren) Endpunkten der vorkommenden Intervalle zu betrachten. Die resultierenden Teilprobleme können dann mit herkömmlichen numerischen Methoden gelöst werden. Intervallarithmetik wird lediglich noch benutzt, um Rundungsfehler zu bestimmen.\n\nDieser Ansatz ist allerdings nur für Systeme kleinerer Dimension möglich, da bei einer vollbesetzten formula_198 Matrix schon formula_199 reelle Matrizen invertiert werden müssen, mit jeweils formula_200 Vektoren für die rechte Seite. Dieser Ansatz wurde von Jiří Rohn noch weitergeführt und verbessert.\n\nEine Intervallvariante des Newton-Verfahrens zur Bestimmung der Nullstellen in einem Intervallvektorformula_70 lässt sich einfach aus der Mittelwert-Erweiterung ableiten (Lit.: Hansen, 1992). Für einen unbekannten Vektor formula_202 gilt für ein festes formula_203, dass\nFür eine Nullstelle formula_205 ist formula_206, und somit muss\nerfüllt sein. Man erhält also\nformula_208.\nEine äußere Abschätzung von formula_209 kann hierbei durch eines der linearen Verfahren bestimmt werden.\n\nIn jedem \"Newton-Schritt\" wird nun ein grober Startwert formula_210 durch formula_211 ersetzt und so iterativ verbessert. Im Gegensatz zum klassischen Verfahren nähert sich diese Methode von außen den Nullstellen. Daher ist garantiert, dass das Ergebnis immer alle Nullstellen im Startwert enthält. Umgekehrt hat man bewiesen, dass formula_8 keine Nullstelle in formula_70 hat, wenn der Newton-Schritt die leere Menge zurückliefert.\n\nDas Verfahren konvergiert gegen eine Menge, die alle Nullstellen (innerhalb der Startregion) enthält. Durch in diesem Fall vorhandene Divisionen durch Null entstehen oft mehrere Intervallvektoren, die die Nullstellen voneinander trennen. Diese Trennung ist nicht immer vollständig, und kann dann durch Bisektion forciert werden.\n\nAls Beispiel betrachte man die Funktion formula_214, den Startwert formula_215 und den Punkt formula_216. Man hat dann formula_217 und der erste Newton-Schritt ist gegeben durch\nEs gilt also für eine Nullstelle formula_219.\nWeitere Newtonschritte werden dann jeweils auf formula_220 und formula_221 getrennt angewendet. Diese konvergieren zu beliebig kleinen Intervallen um formula_222 und formula_223.\n\nDas Intervall-Newton-Verfahren lässt sich auch ohne weiteres bei \"dicken Funktionen\" anwenden, also Funktionen wie formula_224, die bereits dann Intervalle zurückliefern, wenn man reelle Zahlen einsetzt. Die Lösung besteht dann aus mehreren Intervallen formula_225.\n\nDie verschiedenen Intervallmethoden liefern nur äußerst konservative Abschätzungen eines jeweils gesuchten Bereiches, da Abhängigkeiten zwischen den intervallwertigen Größen nicht ausreichend berücksichtigt werden. Das Abhängigkeitsproblem spielt aber eine desto geringere Rolle, je dünner die Intervalle sind.\n\nÜberdeckt man einen Intervallvektor formula_70 durch kleinere Boxen formula_227 so dass formula_228 dann gilt für den Wertebereich\nformula_229\nFür die oben genannten Intervallerweiterungen gilt dann\nformula_230.\nDa formula_231 oft eine echte Obermenge der rechten Seite ist, erhält man somit meist eine verbesserte Abschätzung.\nEine solche Überdeckung kann zum einen durch Bisektion generiert werden, indem man besonders \"dicke\" Elemente formula_232 des Intervallvektors formula_233 beispielsweise in der Mitte teilt und durch zwei Intervalle formula_234 und formula_235 ersetzt. Sollte das daraus folgende Resultat immer noch nicht geeignet sein, kann sukzessive weiter zerlegt werden. Hierbei gilt allerdings zu beachten, dass durch formula_236 geteilte Vektorelemente eine Überdeckung aus formula_237 Intervallvektoren entsteht, was den Rechenaufwand natürlich stark erhöht.\n\nBei sehr breiten Intervallen kann es sogar sinnvoll sein, alle Intervalle gleich in mehrere Teilintervalle mit (kleiner) konstanter Breite zu zerlegen („Mincing“). Damit spart man die Zwischenrechnung für die einzelnen Bisektionsschritte.\nBeide Herangehensweisen sind allerdings nur für Probleme niedriger Dimension geeignet.\n\nDie Intervallarithmetik kommt auf verschiedenen Gebieten zum Einsatz, um Größen zu behandeln, für die keine genauen Zahlenwerte festgelegt werden können (Lit.: Jaulin u. a., 2001).\n\nDie Intervallarithmetik wird bei der Fehleranalyse angewendet, um Kontrolle über die bei jeder Berechnung auftretenden Rundungsfehler zu bekommen.\nDer Vorteil der Intervallarithmetik liegt darin, dass man nach jeder Operation ein Intervall erhält, welches das Ergebnis sicher einschließt. Aus dem Abstand der Intervallgrenzen kann man den aktuellen Berechnungsfehler direkt ablesen:\nIntervallanalyse bietet hierbei keinen Ersatz für die klassischen Methoden zur Fehlerreduktion, wie Pivotisierung, sondern ergänzt diese lediglich.\n\nBei der Simulation technischer und physikalischer Prozesse treten oft Parameter auf, denen keine exakten Zahlenwerte zugeordnet werden können.\nSo unterliegt der Produktionsprozess technischer Bauteile gewissen Toleranzen, so bestimmte Parameter innerhalb bestimmter Intervalle schwanken können.\nAußerdem können viele Naturkonstanten nicht mit beliebiger Genauigkeit gemessen werden (Lit.: Dreyer, 2005).\n\nWird das Verhalten eines solchen toleranzbehafteten Systems beispielsweise durch eine Gleichung formula_240, für formula_241 und Unbekannten formula_134, beschrieben, dann kann die Menge aller möglichen Lösungen\ndurch Intervallmethoden abgeschätzt werden. Diese stellen hier eine Alternative zur klassischen Fehlerrechnung dar.\nIm Gegensatz zu punktbasierten Methoden, wie der Monte-Carlo-Simulation, stellt die verwendete Methodik sicher, dass keine Teile des Lösungsgebietes übersehen werden.\nAllerdings entspricht das Ergebnis immer einer Worst Case-Analyse für gleichverteilte Fehler, andere Wahrscheinlichkeitsverteilungen sind nicht möglich.\n\nIntervallarithmetik kann auch dazu verwendet werden, beliebige Zugehörigkeitsfunktionen für unscharfe Mengen wie sie in der Fuzzy-Logik benutzt werden anzunähern. Neben den strikten Aussagen formula_244 und formula_245 sind hier auch Zwischenwerte möglich, denen reelle Zahlen formula_246 zugeordnet werden. Dabei entspricht formula_247 der sicheren Zugehörigkeit und formula_248 der Nichtzugehörigkeit. Eine Verteilungsfunktion ordnet jedem dieser Werte einen gewissen Schwankungsbereich zu, den man wieder als Intervall auffassen kann.\n\nFür die \"Fuzzy-Arithmetik\" werden nur endlich viele diskrete Zugehörigkeitsstufen formula_249 betrachtet. Die Form einer solchen Verteilung für einen unscharfen Wert kann dann durch eine Reihe von Intervallen\nangenähert werden. Dabei entspricht das Intervall formula_251 genau dem Schwankungsbereich für die Stufe formula_252.\n\nDie entsprechende Verteilung für eine Funktion formula_120 bezüglich unscharfer Werte\nformula_254 und den entsprechenden Sequenzen\nformula_255 lässt sich dann durch die Intervallsequenz\nformula_256\napproximieren. Die Werte formula_257 sind gegeben durch formula_258 und können durch Intervallverfahren abgeschätzt werden. Dabei entspricht formula_259 dem Ergebnis einer Intervallrechnung.\n\nIntervallarithmetik ist keine völlig neue Erscheinung in der Mathematik und tauchte bereits mehrfach unter verschiedenen Namen im Laufe der Geschichte auf. So berechnete Archimedes bereits im 3. Jahrhundert v. Chr. obere und untere Schranken für die Kreiszahl Pi. Allerdings wurde das eigentliche Rechnen mit Intervallen nie so populär wie andere numerische Techniken, wurde aber nie völlig vergessen.\n\nRegeln für das Rechnen mit Intervallen und anderen Teilmengen der reellen Zahlen finden sich schließlich in einer 1931 veröffentlichten Arbeit von Rosalind Tanner (Rosalind Cecily Young), einer Doktorandin von Ernest William Hobson an der Universität Cambridge. Arbeiten für eine Arithmetik von \"range numbers\" („Bereichszahlen“) in Hinblick auf eine Verbesserung und Zuverlässigkeit digitaler Systeme finden sich dann in einem 1951 veröffentlichten Lehrbuch zur linearen Algebra von Paul S. Dwyer (University of Michigan). Hier werden Intervalle tatsächlich dafür eingesetzt, die Rundungsfehler bei Gleitkommazahlen abzuschätzen.\n\nAls Geburtsstunde der modernen Intervallarithmetik wird das Erscheinen des Buches \"Interval Analysis\" von Ramon E. Moore im Jahr 1966 (Lit.: Moore) angesehen.\nDie Idee dazu hatte er im Frühjahr 1958, und bereits ein knappes Jahr später veröffentlichte er einen Artikel über computerunterstützte Intervallarithmetik. Sein Verdienst ist es, dass aus einem einfachen Prinzip eine allgemeingültige Methode zur automatisierten Fehleranalyse wurde, mit deren Hilfe nicht nur der Einfluss von Rundungen bestimmt werden konnte.\n\nUnabhängig davon hatte Mieczyslaw Warmus zwar schon 1956 Formeln für das Rechnen mit Intervallen vorgeschlagen, bei Moore fanden sich aber neben Implementierungshinweisen auch erste nicht-triviale Anwendungen.\n\nIn Deutschland hatten sich in den 1960er Jahren Forschergruppen um Karl Nickel (Universität Karlsruhe; ab 1976: Universität Freiburg), Ulrich Kulisch (Lit.: Kulisch) (Universität Karlsruhe) und Fritz Krückeberg (Lit.: Krückeberg) (Universität Bonn; ab 1968: Gesellschaft für Mathematik und Datenverarbeitung, Sankt Augustin) etabliert, in denen zahlreiche Diplom- und Doktorarbeiten zu intervallarithmetischen Themen entstanden.\n\nDas erste internationale Symposium über Intervallanalysis (Lit.: Hansen) veranstaltete das Oxford University Computing Laboratory im Januar 1968 in Culham, England. Der Tagungsband wurde von Eldon R. Hansen herausgegeben, der auch später sehr aktiv auf dem Gebiet war (Lit.: Hansen, Walster).\n\nKarl Nickel war die Triebfeder hinter fünf Workshops zur Intervallarithmetik,\ndie 1968–1976 im mathematischen Forschungsinstitut Oberwolfach stattfanden und wo sich deutschsprachige Forscher über ihre Arbeiten austauschten. \nEr organisierte 1975, 1980 und 1985 (Lit.: Nickel) internationale Symposien zur Intervallmathematik, wobei er den Begriff Intervallmathematik prägte. \nEine Intervallbibliothek, in der Software zur Intervallarithmetik systematisch gesammelt wurde, war in seinem Institut angesiedelt. \nVon 1978 bis 1987 gab er die Zeitschrift “Freiburger Intervall-Berichte” heraus. \nEr war Gründer und Vorsitzender des GAMM-Ausschuss Intervallmathematik.\n\nZwei Schüler von Ulrich Kulisch, Götz Alefeld und Jürgen Herzberger, veröffentlichten 1974 das erste deutschsprachige Lehrbuch (Lit.: Alefeld und Herzberger) zur Intervallarithmetik.\n\nSeit den 90ern wird das Journal \"Reliable Computing\" (ursprünglich \"Interval Computations\") herausgegeben, das sich der Zuverlässigkeit computerunterstützter Berechnungen widmet. Als leitender Redakteur hat R. Baker Kearfott neben seinen Arbeiten zur globalen Optimierung wesentlich zur Vereinheitlichung der Notation und Begrifflichkeiten der Intervallarithmetik beigetragen (Web: Kearfott).\n\nIn jüngerer Zeit sind insbesondere die Arbeiten zur Abschätzung des Urbildes parametrisierter Funktionen und zur robusten Kontrolle von der \"COPRIN\"-Arbeitsgruppe des INRIA im französischen Sophia Antipolis zu erwähnen (Web: INRIA).\n\nEiner der wesentlichen Förderer der Intervallarithmetik, G. William Walster von Sun Microsystems, hat in den Jahren 2003/04 – teilweise zusammen mit Ramon E. Moore und Eldon R. Hansen – mehrere Patente im Bereich der Intervallarithmetik beim U.S. Patent and Trademark Office angemeldet. Die Gültigkeit dieser Ansprüche ist jedoch in der Intervallarithmetik-Forschungsgemeinde stark umstritten, da sie möglicherweise lediglich den bisherigen Stand der Technik wiedergeben.\n\nEs gibt viele Softwarepakete, welche die Entwicklung numerischer Anwendungen unter Nutzung der Intervallarithmetik erlauben.\nDiese sind meist in Form von Programmbibliotheken umgesetzt.\nEs gibt allerdings auch C++- und Fortran-Übersetzer, welche Intervall-Datentypen und entsprechend geeignete Operationen als Spracherweiterung besitzen, so dass Intervallarithmetik direkt unterstützt wird.\n\nSeit 1967 entwickelte man zunächst an der Universität Karlsruhe \"XSC\"-Erweiterungen für wissenschaftliches Rechnen („Extensions for Scientific Computation“) für verschiedene Programmiersprachen, darunter C++, Fortran und Pascal. Plattform war zunächst ein Zuse Z 23, für den ein neuer Intervall-Datentyp mit entsprechenden elementaren Operatoren zur Verfügung gestellt wurde.\n\n1976 folgte mit \"Pascal-SC\" eine Pascal-Variante auf einem Zilog Z80, die es ermöglichte, schnell komplexe Routinen für automatisierte Ergebnisverifikation zu schaffen. Es folgte das Fortran 77-basierte \"ACRITH-XSC\" für die System/370-Architektur, das später auch von IBM ausgeliefert wird. Ab 1991 kann man mit \"Pascal-XSC\" dann Code für C-Compiler erzeugen, und ein Jahr später unterstützt die C++-Klassenbibliothek \"C-XSC\" bereits viele verschiedene Computersysteme. 1997 werden alle \"XSC\"-Varianten unter die General Public License gestellt und stehen so frei zu Verfügung. Anfang der 2000er-Jahre wurde \"C-XSC 2.0\" unter Federführung der Arbeitsgruppe für wissenschaftliches Rechnen an der Bergischen Universität Wuppertal neugestaltet, um dem mittlerweile verabschiedeten C++-Standard besser entsprechen zu können.\n\nEine weitere C++-Klassenbibliothek ist das 1993 an der TU Hamburg-Harburg geschaffene \"Profil/BIAS\" („Programmer’s Runtime Optimized Fast Interval Library, Basic Interval Arithmetic“), das die üblichen Intervalloperationen benutzerfreundlich zur Verfügung stellt. Dabei wurde besonderen Wert auf die effiziente Ausnutzung der Hardware, Portabilität und Unabhängigkeit von einer speziellen Intervalldarstellung gelegt.\n\nDie Boost-Sammlung von C++-Bibliotheken enthält ebenfalls eine Template-Klasse für Intervalle. Deren Autoren bemühen sich derzeit um eine Aufnahme der Intervallarithmetik in den C++-Sprachstandard.\n\nHeute können außerdem die gängigen Computeralgebrasysteme, wie Mathematica, Maple und MuPAD, mit Intervallen umgehen. Außerdem gibt es für Matlab die Erweiterung \"Intlab\", die auf BLAS-Routinen aufbaut, sowie die Toolbox \"b4m\", die ein \"Profil/BIAS\"-Interface zur Verfügung stellt.\n\nEin IEEE-Standard für Intervallarithmetik wurde im Juni 2015 veröffentlicht. Es gibt zwei freie Referenzimplementierungen, die von Mitgliedern der Arbeitsgruppe entwickelt worden sind: Die libieeep1788-Bibliothek für C++ und das Intervall-Paket für GNU Octave.\n\nEine vereinfachte Variante des Standards wird noch entwickelt. Diese soll noch einfacher umzusetzen sein und für eine schnellere Verbreitung sorgen.\n\nMehrere internationale Konferenzen und Workshops werden jährlich in der ganzen Welt abgehalten. Die wichtigste Konferenz ist SCAN (International Symposium on Scientific Computing, Computer Arithmetic, and Verified Numerical Computation). Daneben gibt es SWIM (Small Workshop on Interval Methods), PPAM (International Conference on Parallel Processing and Applied Mathematics) und REC (International Workshop on Reliable Engineering Computing).\n\n\n\n"}
{"id": "380188", "url": "https://de.wikipedia.org/wiki?curid=380188", "title": "Filesystem Hierarchy Standard", "text": "Filesystem Hierarchy Standard\n\nDer Filesystem Hierarchy Standard (FHS) ist eine Richtlinie für die Verzeichnisstruktur unter Unix-ähnlichen Betriebssystemen.\n\nDer Standard richtet sich an Softwareentwickler, Systemintegratoren und Systemadministratoren. Er soll die Interoperabilität von Computerprogrammen fördern, indem er die Lage von Verzeichnissen und Dateien vorhersehbar macht.\n\nDie Entwicklung dieser Richtlinie begann im August 1993 und war zunächst nur auf Linux bezogen. Zwischenzeitlich trugen einige Entwickler von FreeBSD dazu bei, einen umfassenden Standard für alle Unix-ähnlichen Systeme zu schaffen. Diese Zusammenarbeit wurde im Mai 2011 offiziell eingestellt, nachdem die Linux Foundation das Projekt übernommen hatte. Derzeit wird dieser Standard nur von Linux-Distributionen verwendet. Die erste Dokumentation solcher Hierarchie erschien in AT&T UNIX Version 7.\n\nDer FHS unterscheidet Dateien unter zwei Aspekten:\n\nAus diesen beiden Aspekten ergeben sich vier Kategorien von Dateien:\nUm Datensicherungen und Bereitstellungen im Rechnernetz effizienter zu gestalten, sieht der FHS vor, keine Dateien unterschiedlicher Kategorie im selben Verzeichnis zu speichern. Historisch gab es diese Trennung nicht.\n\nDie Partition des Stammverzeichnisses muss all jene Dateien enthalten, die zum Hochfahren des Betriebssystems und zum Einbinden weiterer Partitionen notwendig sind. Um ein System reparieren zu können, muss sie auch die dazu notwendigen Hilfsmittel enthalten.\n\nUm die Flexibilität und Zuverlässigkeit zu erhöhen, empfiehlt der FHS, Teile der Verzeichnisstruktur in anderen Partitionen anzulegen. Zusätzliche Partitionen sind unter Unix und ähnlichen Betriebssystemen transparent.\n\n14 Verzeichnisse oder symbolische Verknüpfungen auf Verzeichnisse werden im Stammverzeichnis verlangt:\n\nDie Verzeichnisse /opt, /usr und /var sind so konzipiert, dass sie nicht in der Partition des Stammverzeichnisses liegen müssen.\n\nZusätzliche Verzeichnisse sind erforderlich, wenn entsprechende Untersysteme installiert sind:\n\nAndere Verzeichnisse sollen im Stammverzeichnis nicht angelegt werden. Anwendungsprogramme sollen keine Dateien im Stammverzeichnis fordern oder anlegen.\n\n/bin enthält alle Befehle, die sowohl vom Administrator als auch vom Benutzer aufgerufen werden können und auch dann benötigt werden, wenn keine anderen Dateisysteme eingehängt (mounted) sind, zum Beispiel im Single User Mode. Darüber hinaus können auch Skripte, die solche Befehle verwenden, dort abgelegt werden.\n\nDas Verzeichnis /bin darf keine Unterverzeichnisse enthalten. Folgende Programme oder symbolische Links auf die Unix-Kommandos werden in /bin mindestens benötigt:\n\nFalls benötigt, müssen die folgenden Kommandos (oder auf sie verweisende Verknüpfungen) ebenfalls im /bin-Verzeichnis installiert sein:\n\nAlle weiteren Kommandos, die zur Wiederherstellung benötigt werden, wie beispielsweise ftp, tftp oder diverse Archivierungsprogramme, haben hier ebenfalls ihren Platz.\n\nDieses Verzeichnis enthält alle vom Bootloader für den Bootvorgang benötigten Dateien. Dies beinhaltet z. B. auch gespeicherte Master Boot Records. Auch der Betriebssystem-Kern muss entweder in diesem Verzeichnis oder im Wurzelverzeichnis abgelegt sein.\n\nDas Verzeichnis /dev beinhaltet Spezial-Dateien \"(special file)\" oder Gerätedateien \"(device file)\". Manche der Gerätedateien müssen manuell angelegt werden. In diesem Fall muss das Verzeichnis den Befehl MAKEDEV bzw. MAKEDEV.local enthalten, der diese Gerätedateien anhand der vorhandenen Hardware anlegen kann.\nErwähnenswert sind /dev/null, in der geschrieben werden kann, aber alles verworfen wird, /dev/zero, aus der Nullbytes in beliebiger Menge gelesen werden können, und /dev/random (bzw. /dev/urandom), welche als Hardware-Zufallsgenerator dient.\n\nudev hat seit Kernel 2.6 devfs abgelöst und sorgt nun mittels Konfigurationsdateien (standardmäßig in \"/etc/udev\") im Userspace für die automatische Erstellung der Devices in /dev.\n\n/etc stand ursprünglich für „alles übrige“ (lat. \"et cetera\"). Es hat sich dann aber als Konfigurationsverzeichnis etabliert und wird daher auch häufig als „editable text configuration“ interpretiert. Das Verzeichnis /etc und seine Unterverzeichnisse enthalten jede Art von Konfigurationsdateien. Diese Dateien müssen statische Dateien sein (s. o.). In diesem Verzeichnis dürfen sich keine Binärdateien befinden.\n\nFolgende Unterverzeichnisse können u. a. in /etc vorhanden sein:\nAllgemein liegen Konfigurationen einzelner Dienste unter /etc/<Dienstname>/ und/oder /etc/<Dienstname>.conf (der LDAP unter dem Debianderivat Ubuntu macht z. B. ohne eindeutige Dateibenennung beides). Bei Daemons fehlt oft dann das abschließende \"d\", der NTP-Server z. B. hat den Prozess \"ntpd\", aber die Konfigurationsdatei \"/etc/ntp.conf\".\n\nFolgende Konfigurationsdateien gehören auf jeden Fall (soweit vorhanden) in /etc:\n\nDiese Verzeichnisstruktur dient zur Aufnahme der benutzerspezifischen Daten der einzelnen Benutzer des Systems. Der FHS führt dieses Verzeichnis, obwohl es als Quasi-Standard etabliert ist, nur als optional auf. Alle Benutzer-spezifischen Konfigurationsdateien werden in versteckten Dateien und Verzeichnissen (die einen Punkt als erstes Zeichen des Dateinamens besitzen) unter /home/$USER/ abgelegt. Diese versteckten Dateien und Verzeichnisse im Benutzerverzeichnis werden oft auch \"dot files\" genannt.\n\nDas Verzeichnis /lib beinhaltet die installierten dynamischen Bibliotheken und Kernel-Module, die zum Starten des Systems und für die Programme in /bin und /sbin benötigt werden. Hier befindet sich auch die dynamische C-Standard-Bibliothek \"libc.so.*\" und die Linker-Bibliothek \"ld*\". Das Unterverzeichnis \"modules\" beinhaltet die oben genannten Kernel-Module, falls diese installiert sind.\n\nManche Systeme unterstützen mehrere Binärformate (für unterschiedliche Prozessor-Architekturen bzw. -modi), für die jeweils eigene Versionen derselben Bibliothek vorhanden sind. Dann gibt es beispielsweise /lib32 und /lib64 für die beiden Betriebsmodi (32 Bit und 64 Bit) des x86-Prozessors.\n\nDie einzelnen Unterverzeichnisse in /media dienen als Einhängepunkt für jede Art von Wechseldatenträger. Früher wurden Wechseldatenträger entweder direkt im Wurzelverzeichnis oder im Verzeichnis /mnt gemountet. Zur Verschlankung des Wurzelverzeichnisses wurden die Verzeichnisse in den Ordner /media verschoben. Der Standard sieht folgende Unterverzeichnisse jeweils optional vor:\n\nFalls ein Gerät mehrmals vorhanden ist, wird das Anhängen einer Ziffer an den Gerätetyp vorgeschlagen.\n\nDas Verzeichnis dient zum kurzzeitigen Einhängen von Fremd-Dateisystemen aller Art. Installationsprogrammen ist die Verwendung des Verzeichnisses /mnt für temporäre Dateien ausdrücklich untersagt.\n\nDas Verzeichnis ist für sämtliche optionale, d. h. zusätzlich installierte, Software vorgesehen, welche nicht aus zur Distribution gehörenden Paketquellen stammen. Die Pakete müssen in einem Unterverzeichnis mit Namen <samp>/opt/\"<Paket>\"</samp> oder <samp>/opt/\"<Hersteller>\"</samp> installiert werden, wobei <samp>\"<Paket>\"</samp> ein beschreibender Paketname ist und <samp>\"<Hersteller>\"</samp> der bei der LANANA registrierte Name des Herstellers ist. Die Unterverzeichnisse <samp>/opt/bin</samp>, <samp>/opt/doc</samp>, <samp>/opt/include</samp>, <samp>/opt/info</samp>, <samp>/opt/lib</samp>, und <samp>/opt/man</samp> sind für den lokalen Systemadministrator reserviert. Binärdateien von Softwarepaketen finden sich normalerweise ausschließlich in <samp>/opt/\"<Paket>\"/bin</samp>\n\nDas Verzeichnis kann das Benutzerverzeichnis für den Benutzer root bilden. Dieses Verzeichnis ist nur eine Empfehlung des FHS.\n\nEnde März 2011 wurde durch Entwickler der Linux-Distribution Fedora verkündet, dass zukünftige Fedora-Versionen das Verzeichnis /run enthalten werden. Dieses soll in seiner Funktion das Verzeichnis /var/run ersetzen und das Problem lösen, dass beim Bootvorgang /var/run unter bestimmten Umständen noch nicht verfügbar ist, aber zum Booten benötigt wird. Mehrere Linux-Distributionen sagten ihre Unterstützung zu. /run wird mittlerweile auch vom Filesystem Hierarchy Standard verlangt.\n\nDas Verzeichnis beinhaltet Befehle für die Systemadministration und andere Aufgaben, die nur der Benutzer root ausführen darf. Dies sind im Wesentlichen alle Befehle, die auch im Verzeichnis /bin hätten abgelegt werden können, aber z. B. aus Speicherplatzgründen nicht dort liegen. Programme, die in /sbin erwartet werden: shutdown, fastboot, fasthalt, fdisk, fsck, fsck.*, getty, halt, ifconfig, init, mkfs, mkfs.*, mkswap, reboot, route, swapon, swapoff, update\n\nIn diesem Verzeichnis sollen die Daten zu angebotenen Diensten abgelegt werden. Momentan gibt es noch keine Vorschriften darüber, wie die Verzeichnisstruktur in /srv auszusehen hat. Eine vorgeschlagene Möglichkeit ist die Benennung der Unterverzeichnisse nach dem Dienst, also z. B. www, ftp, mysql usw. Eine andere Möglichkeit ist die Ordnung nach Verwaltungseinheiten wie beispielsweise Fachschaften an Universitäten. Dieses wird momentan nur von SuSE und Arch Linux so gemacht. So existiert beispielsweise unter Debian das Verzeichnis /var/www, hingegen wird bei SuSE /srv/www (Arch: /srv/http) verwendet.\n\nDieses Verzeichnis muss vorhanden sein, weil es Programme gibt, die ihre temporären Dateien in diesem Verzeichnis ablegen. Im FHS wurde dieses Verzeichnis vor allem auch wegen seiner historischen Bedeutung aufgenommen. Das Verzeichnis ist für alle Benutzer zum Schreiben freigegeben, und muss ein Sticky Bit haben.\n\n/usr (engl.: Unix System Resources, ursprünglich: USeR) ist die zweite wichtige Ebene des Dateisystems. Dieser Bereich kann von mehreren Rechnern gemeinsam verwendet werden (shareable) und enthält dementsprechend keine vom lokalen Rechner abhängigen oder zeitlich variable Inhalte. Diese werden an anderen Stellen des Dateisystems hinterlegt.\n\nFolgende Verzeichnisse müssen in /usr vorhanden sein:\n\nDarüber hinaus können optional die nachfolgenden Verzeichnisse existieren:\n\nZur Wahrung der Kompatibilität mit älteren Systemen können symbolische Links für folgende Verzeichnisse angelegt sein:\n\nDas /var-Verzeichnis (englisch \"variable\") enthält variable Daten, welche im Zuge der Abarbeitung entstehen.\nDie folgenden Verzeichnisse, oder symbolische Verknüpfungen zu Verzeichnissen, werden in /var erwartet:\n\nAus „historischen“ Gründen existieren noch bei Bedarf die folgenden Verzeichnisse:\n\nFalls die entsprechenden Anwendungen installiert sind, werden noch folgende Verzeichnisse verwendet:\n\n\n"}
{"id": "381953", "url": "https://de.wikipedia.org/wiki?curid=381953", "title": "Asterisk (Telefonanlage)", "text": "Asterisk (Telefonanlage)\n\nAsterisk ist eine freie Software für Computer aller Art, die Funktionalitäten einer Telefonanlage bietet. Sie unterstützt IP-Telefonie (VoIP) mit unterschiedlichen Netzwerkprotokollen und kann mittels Hardware mit Anschlüssen wie POTS (analoger Telefonanschluss), ISDN-Basisanschluss (BRI) oder -Primärmultiplexanschluss (PRI, E1 oder T1) verbunden werden.\n\nMark Spencer hat Asterisk ursprünglich bei der Firma Digium entwickelt. Mittlerweile haben sich noch weitere Entwickler der Asterisk-Gemeinde angeschlossen und so stammen viele Erweiterungen und Applikationen auch von anderen Entwicklern. Der Name stammt von der Bezeichnung für das Sternsymbol ab.\n\nAsterisk wird unter einer dualen Lizenz zur Verfügung gestellt – der GNU General Public License (GPL) als freier Softwarelizenz und einer proprietären Lizenz, die es den Lizenznehmern gestattet, proprietäre, nichtöffentliche Bestandteile auszuliefern.\n\nAsterisk unterstützt\n\nAsterisk funktioniert auf einer Reihe unterschiedlicher Computersysteme. Neben Personal Computern mit x86-Prozessor läuft Asterisk auch auf Geräten wie dem Raspberry Pi und Routern, die OpenWrt unterstützen.\n\nEin Asterisk-System kann ohne besondere Hardware nur über ein Netzwerk mit (nach intern) VoIP-Endgeräten und (nach extern) VoIP-Telefonprovidern verbunden und betrieben werden.\n\nFür die Anbindung (nach extern) an herkömmliche Telefonnetze oder (nach intern) von normalen Endgeräten wie Telefonen, Faxgeräten etc. benötigt man zusätzliche Steckkarten oder externe Analog-Adapter für analoge POTS oder digitale ISDN-Amtsleitungen (nach extern) oder Nebenstellen (nach intern).\n\nInterne ISDN-Nebenstellen können nur über Schnittstellen realisiert werden, die den NT-Modus beherrschen (ISDN-Karten mit HFC-Chips oder VoIP-Adapter/-Router mit internem ISDN-Anschluss).\n\nDas zentrale Steuerelement von Asterisk ist der Rufnummern- oder besser \"Wählplan\" (engl. \"dial plan\"). Hier wird entschieden:\n\n\nDieser Plan gliedert sich in mehrere Abschnitte (engl. \"sections\"), deren Bezeichnungen in eckigen Klammern gefasst werden. Ein Abschnitt ist bis zur nächsten öffnenden Klammer bzw. bis zum Dateiende gültig. Innerhalb der Abschnitte wird jeweils eine bestimmte Gruppe Nebenstellen (engl. \"extensions\") und damit zusammenhängende Reihenfolgen, Aktionen und Verhalten behandelt. Die Abschnitte „[general]“ und „[globals]“ sind vordefinierte Abschnitte. Alle anderen Abschnitte werden auch als Kontexte bezeichnet. Kontexte bestimmen einen geschlossenen Bereich und somit einen definierten Sicherheitsbereich. Ein Kontext kann den Sicherheitsbereich eines anderen Kontexts erlangen, indem er in diesen Kontext über die Anweisung include eingebunden wird. Ist für eine Nebenstelle kein Kontext definiert, so wird diese über den Standardkontext „[default]“ behandelt.\n\nAsterisk ist lauffähig unter folgenden Betriebssystemen:\n\n\nNeben den vorgenannten Protokollen bietet Asterisk mit den Modulen chan_alsa und chan_mobile auch die Nutzung lokaler Schnittstellen. Damit ist es möglich, eine vorhandene Soundkarte als Spechstelle zu nutzen. Auch kann ein Bluetooth Headset direkt als Nebenstelle angemeldet werden. Ein Handy/Smartphone wird per Bluetooth zum Mobilfunkgateway.\n\nDiese Möglichkeiten werden überwiegend im privaten Umfeld genutzt, da sie naturgemäß nur im unmittelbaren Umfeld des Asteriskservers eingesetzt werden können.\nUnter anderem werden folgende Audio-Codecs unterstützt:\n\nDas AGI Asterisk Gateway Interface bietet unter anderem eine Programmierschnittstelle zu:\n\nGrafische Benutzerschnittstellen ermöglichen das Bearbeiten der Asterisk-Konfiguration via Web-Interface. Die nachfolgenden Asterisk-Server-Distributionen beinhalten Linux oder BSD und sind inzwischen meist kommerzielle Produkte, die jedoch ihre Wurzeln in freien Projekten haben und in den Basisversionen oder für Privatanwender kostenfrei sind.\n\n\nSprachbausteine werden benötigt, um das Asterisk-Voicemailsystem mit lokaler Sprache zu nutzen. Es handelt sich dabei um Sammlungen erforderlicher Buchstaben, Zahlen und Wörter eines Sprechers und \"nicht\" um Text-to-Speech- oder Sprachsynthese-Module.\n\n"}
{"id": "383365", "url": "https://de.wikipedia.org/wiki?curid=383365", "title": "Linux-Magazin", "text": "Linux-Magazin\n\nDas Linux-Magazin ist eine Fachzeitschrift, die von Computec Media herausgegeben wird. Sie wendet sich an Leser, die professionell mit Linux arbeiten. Das Linux-Magazin ist weltweit die zweitälteste Zeitschrift über Linux (und nach eigenen Angaben die älteste Europas). Die erste Ausgabe erschien im Oktober 1994 als Informationsblatt der DELUG. Der Slogan der Zeitschrift lautet „Die Zeitschrift für Linux-Professionals“.\n\nDie gedruckte Auflage war im Jahr 2013 mit 36.000 Exemplaren angegeben. Im Jahr 2017 betrug sie nur noch 19.000 Stück.\n\nDie Zeitschrift erscheint an jedem ersten Donnerstag eines Monats. Eine Ausgabe kostet im Einzelhandel 6,40 € in Deutschland, 7,05 € in Österreich und 12,80 sfr in der Schweiz. Seit der Ausgabe 11/2008 gibt es am Kiosk parallel eine Ausgabe mit der so genannten DELUG-DVD für 8,50 € (9,35 € in Österreich und 17,00 sfr in der Schweiz) frei zu kaufen. Dieses Heft mit Datenträger war zuvor nur in Form eines speziellen Abos zu haben.\n\nVierteljährlich wird ein Sonderheft mit Spezialthemen wie Netzwerk, Sicherheit oder Hardware herausgegeben. Es kostet 9,80 €. Weiterhin existieren auch Ausgaben des Linux-Magazins auf Englisch, Polnisch, Rumänisch, Portugiesisch und Spanisch.\n\nSeit November 2000 Jan Kleinert (Stand: März 2018), zuvor Tom Schwaller und Harald Milz.\n\nDas Linux-Magazin richtet sich vor allem an professionelle Linux-Anwender. Jeden Monat werden Schwerpunktthemen behandelt. Diese stammen aus verschiedenen Bereichen des Linux-Umfeldes: Internet-Anwendungen, Programmierung, Sicherheit usw.\n\nDaneben gibt es noch einige feste Rubriken, die in jedem Heft vertreten sind:\n\nZu erwähnen ist, dass das Linux-Magazin die einzige deutschsprachige Zeitschrift war, die über Nachrichten aus dem Sicherheitsumfeld detailliert (ca. 5 Seiten pro Ausgabe) monatlich berichtet. Autor hierfür war von 1999 bis 2011 Mark Vogelsberger, betreuender Redakteur seit 2009 Nils Magnus, zuvor Achim Leitner. Die Idee für die InSecurity News stammte ursprünglich von Mark Vogelsberger. Mittlerweile haben die InSecurity News auch Einzug in die internationalen Ausgaben des Linux Magazins gehalten. Die InSecurity News wurden im Mai 2011 durch das Insecurity Bulletin als Online-Blog abgelöst. Dieses bietet jetzt detailliertere Artikel und Analysen von aktuellen Sicherheitslücken. Der Internetauftritt des Linux-Magazins stellt ebenfalls ein kostenloses Heft-/Artikelarchiv zur Verfügung, wobei lediglich die Bezahlinhalte der letzten zehn Ausgaben vom freien Zugriff ausgenommen sind (Stand: April 2018).\n\nAus der Rubrik LinuxUser entstand die gleichnamige Zeitschrift, die sich an Endbenutzer wendet.\nVom selben Verlag wird auch die Zeitschrift easyLinux vertrieben, die sich besonders an Linux-Einsteiger ohne Vorkenntnisse richtet.\n\nDie weltweit älteste Zeitschrift (Start: März 1994), die sich fast ausschließlich mit Linux beschäftigt, ist das englischsprachige Linux Journal des Hauses Belltown Media, Inc.\n\n"}
{"id": "385610", "url": "https://de.wikipedia.org/wiki?curid=385610", "title": "TextEdit", "text": "TextEdit\n\nTextEdit ist ein von Apple mit dem Betriebssystem macOS ausgeliefertes Programm zur Bearbeitung von Texten. Ursprünglich war TextEdit Teil von NeXTStep, für GNUstep existiert ebenfalls eine Version.\n\nTextEdit ersetzte bei Mac OS X (heute macOS) das zuvor mit dem klassischen Mac OS ausgelieferte Programm SimpleText, das dazu diente, Begleittexte zu Programmen anzeigen zu können. Im Vergleich zu SimpleText wurde TextEdit ausgebaut, so geht der Funktionsumfang von TextEdit inzwischen deutlich über den von Microsofts WordPad hinaus.\n\nTextEdit bietet Drag and Drop von Text, Bildern und Anhängen, Anzeigen und Bearbeiten von Dokumenten verschiedener Formate.\n\nWie bereits SimpleText erlaubt TextEdit das Vorlesen von Texten; dazu nutzt es die Sprachausgabe des Betriebssystems. Zudem wird die vom Betriebssystem bereitgestellte Rechtschreibprüfung genutzt.\n\nTextEdit unterstützt die Apple Advanced Typography, die aufwendigere typografische Möglichkeiten wie etwa Ligaturen über die Schriftpalette zugänglich macht.\n\n\nDas hochauflösende Icon von TextEdit zeigt von Mac OS X Leopard bis OS X Mavericks den Text von Think Different.\n\n"}
{"id": "387098", "url": "https://de.wikipedia.org/wiki?curid=387098", "title": "Null-Zug-Suche", "text": "Null-Zug-Suche\n\nMit Null-Zug-Suche (\"nullmove pruning\") bezeichnet man eine \"Forward-Pruningtechnik\" in Spielbaumsuchverfahren für Zwei-Personen-Nullsummenspielen mit perfekter Information.\n\nSpeziell in Schachprogrammen hat sich das \"Nullmove Pruning\" bewährt. Diese Technik wird benötigt, um die Ermittlung der Spielstärke möglicher Züge bzw. Spielverläufe zu beschleunigen, indem Züge, welche durch unten beschriebenes Verfahren als zu schwach ermittelt werden, von einer weiteren Berechnung ausgeschlossen werden.\n\nAusgehend von der Annahme, dass das Zugrecht einen Vorteil darstellt, wird beim \"Nullmove Pruning\" in der Baumsuche (Weiterverfolgung von Stellungsmöglichkeiten, die sich aus einem Zug ergeben) einer Seite ermöglicht, zwei Züge auszuführen. Ist der dadurch erzielte Vorteil nicht groß genug, so war wahrscheinlich schon der erste der beiden Züge minderwertig, und der daraus resultierende Ast des Spielbaums (sämtliche mögliche Spielverläufe, die sich aus der aktuellen Stellung ergeben können) braucht nicht weiter untersucht zu werden, er wird abgeschnitten. Hierdurch können minderwertige Varianten gut und schnell erkannt werden und die zur Verfügung stehende Zeit für die Analyse wichtigerer Varianten genutzt werden.\n\nUm insgesamt den Suchaufwand zu reduzieren, muss die Baumsuche, mit der der Null-Zug bewertet wird, mit geringerer Suchtiefe durchgeführt werden, als die Suche zur Bewertung normaler Züge. Eine Reduktion der Suchtiefe um zwei Halbzüge hat sich als vorteilhaft herausgestellt. Manche Programme arbeiten auch mit einer Reduktion um drei Halbzüge, was ein stärkeres Pruning bewirkt, aber taktisch etwas anfälliger ist, da auch vielversprechende Züge mit aussortiert werden können.\n\nDas normale \"Nullmove Pruning\" versagt in Zugzwangstellungen, da hier die Prämisse nicht erfüllt wird. Es kann ein taktisch nachteiliger Zug durch den Zugzwang erforderlich sein.\nDa Zugzwangstellungen beim Schach relativ selten vorkommen (am ehesten in bestimmten Endspielsituationen), ist die Fehlerhäufigkeit eher gering. Einige Schachprogrammierer schalten das Nullmove Pruning im Endspiel auch einfach ganz ab, da gerade am Ende nur noch wenige Zweige des Baumes übrig sind und diese eher Zugzwangsstellungen sein können.\n\nBei Spielen wie Dame (engl. \"checkers\") gehören Zugzwangstellungen zum Normalfall, weshalb bei solchen Spielen diese Technik nicht angewandt wird.\n\nEine verbesserte Technik nennt sich \"Verified Nullmove Pruning\" und umgeht die Probleme in Zugzwangstellungen.\n\n"}
{"id": "387120", "url": "https://de.wikipedia.org/wiki?curid=387120", "title": "Pruning", "text": "Pruning\n\nPruning ist der englische Ausdruck für das Beschneiden (Zurechtstutzen) von Bäumen und Sträuchern. In der Informatik im Umfeld des maschinellen Lernens wird der Ausdruck für das Vereinfachen, Kürzen und Optimieren von Entscheidungsbäumen verwendet.\n\nDie Idee des Pruning entstammt ursprünglich aus dem Versuch, das sog. Overfitting bei Bäumen zu verhindern, die durch induziertes Lernen entstanden sind. Overfitting bezeichnet die unerwünschte Induktion von Noise in einem Baum. Noise bezeichnet falsche Attributwerte oder Klassenzugehörigkeiten, welche Datensets verfälschen und so Entscheidungsbäume unnötig vergrößern. Durch das Pruning der Bäume werden die unnötigen Sub-Bäume wieder gekürzt.\n\nPruningverfahren lassen sich nach zwei Arten teilen (Pre- und Post-Pruning). \n\nPre-Pruning-Verfahren verhindern eine vollständige Induktion des Training-Sets durch Austausch eines Stopp()-Kriteriums im Induktionsalgorithmus (z. B. max. Baumtiefe oder Information Gain(Attr) > minGain). Pre-Pruning-Methoden gelten als effizienter, da dabei nicht ein gesamtes Set induziert wird, sondern Bäume von Beginn an klein bleiben. Prepruning-Methoden haben ein gemeinsames Problem, den Horizont-Effekt. Darunter ist das unerwünschte, vorzeitige Abbrechen der Induktion durch das Stopp()-Kriterium zu verstehen.\n\nPost-Pruning (oder nur Pruning) ist das häufigst eingesetzte Verfahren, Bäume zu vereinfachen. Dabei werden Knoten und Teilbäume durch Blätter ersetzt, um die Komplexität zu verbessern. Durch Pruning lässt sich nicht nur die Größe entscheidend verringern, sondern auch die Klassifizierungsgenauigkeit ungesehener Objekte verbessern. Zwar kann es der Fall sein, dass die Genauigkeit der Zuordnung am Testset schlechter wird, die Treffsicherheit der Klassifizierungseigenschaften des Baumes jedoch insgesamt steigt.\n\nDie Verfahren werden anhand deren Vorgehensweise im Baum (Top-Down bzw. Bottom-Up) unterschieden.\n\nDiese Verfahren starten am letzten Knoten im Baum (an der tiefsten Stelle). Rekursiv nach oben folgend bestimmen sie die Relevanz jedes einzelnen Knotens. Ist die Relevanz für die Klassifizierung nicht gegeben, fällt der Knoten weg bzw. wird durch ein Blatt ersetzt. Der Vorteil ist, dass durch dieses Verfahren keine relevanten Sub-Bäume verloren gehen können.\nZu diesen Verfahren zählt das Reduced Error Pruning (REP), das Minimum Cost-Complexity-Pruning (MCCP) oder das Minimum Error Pruning (MEP).\n\nIm Gegensatz zum Bottom-Up-Verfahren setzt diese Methodik an der Wurzel des Baumes an. Der Struktur nach unten folgend wird ein Relevanz-Check durchgeführt, welcher entscheidet, ob ein Knoten für die Klassifizierung aller n Items relevant ist oder nicht. Durch Beschneiden des Baums an einem inneren Knoten kann es passieren, dass ein gesamter Sub-Baum (ungeachtet dessen Relevanz) wegfällt. Zu diesen Vertretern gehört das Pessimistic Error Pruning (PEP), welches durchaus gute Resultate bei ungesehenen Items bringt.\n\nBei Suchverfahren verwendet man verschiedene Pruning-Methoden zur Vorwärtsabschneidung von Suchbäumen, wenn der Algorithmus auf Grund der bereits gesammelten Daten weiß (bzw. bei spekulativem Pruning davon ausgeht), dass diese Teilbäume das gesuchte Objekt nicht enthalten (angewandt zum Beispiel bei Schachprogrammen). \n\nWichtige Pruning-Techniken für Minimax- oder Alpha-Beta-Suchen, die zur Lösung von Zwei-Personen-Nullsummenspielen mit vollständiger Information (wie zum Beispiel Schach) eingesetzt werden können, sind zum Beispiel:\n\nPruning wird auch in Branch-and-Bound-Algorithmen in der mathematischen Optimierung angewandt. Hier wird ein Teilbaum des Suchbaums nicht betrachtet, falls die Schranke für die beste mögliche Lösung in diesem Teilbaum schlechter ist als eine bereits bekannte Lösung.\n\nBei Forensoftware ist Pruning eine Einstellung, die das automatische Löschen von alten Themen (Topics) bewirkt, um Speicherplatz zu sparen, die CPU-Last zu verringern und dadurch die Schnelligkeit des Forums zu erhöhen.\n\n"}
{"id": "387313", "url": "https://de.wikipedia.org/wiki?curid=387313", "title": "Xcode", "text": "Xcode\n\nXcode ist eine integrierte Entwicklungsumgebung von Apple für macOS. Mit ihr lassen sich Programme für macOS, iOS, tvOS und watchOS entwickeln. Xcode ist für die Programmiersprachen Swift und Objective-C unter Verwendung der Cocoa-Frameworks gedacht. Die Programmiersprachen C, C++ können aber auch verwendet werden. Durch seine Modularität kann man damit aber auch Programme in anderen Sprachen schreiben (etwa in Java, Ruby, Perl oder Pascal).\n\nDie Anwendung Xcode ist Bestandteil des gleichnamigen Software-Entwicklungspakets. Zur besseren Unterscheidung wird die Anwendung von Apple „Xcode IDE“ und das gesamte Entwicklungspaket meist „Xcode Tools“ (seltener „Xcode Package“) genannt.\n\nDas Entwicklungspaket \"Xcode Tools\" besteht aus vielen verschiedenen Programmen (in Version 4.2 sind es ungefähr 40, Kommandozeilenprogramme nicht eingeschlossen); die wichtigsten davon sind:\n\nDaneben gibt es noch eine Vielzahl weiterer Anwendungen, z. B. zum Hochladen von Apps oder zum Erstellen von Installationspaketen oder Icons.\n\nDer direkte Vorgänger von Xcode hieß „Project Builder“, das gesamte Entwicklungspaket „Developer Tools“. Diese Namen stammen noch von NeXTStep, dem Vorgänger von Mac OS X. Diese Abstammung ist noch in Teilen zu erkennen; so beinhalten Xcode-Projektdateien immer eine Datei namens codice_1 (Project Builder X Project), und viele Klassennamen beginnen mit dem Präfix NS (NeXTStep).\n\nApple veröffentlichte auf der WWDC 2003 die \"Xcode 1.0 Developer Preview\". Das hatte zwei Gründe: Zum einen konnte so die Beta-Software ausgiebig von den Entwicklern getestet werden, und zweitens lief Project Builder nicht mehr auf Mac OS X Panther, das auch auf der WWDC verteilt worden war.\n\nXcode 1.0 wurde im Herbst 2003 zusammen mit Mac OS X 10.3 veröffentlicht. Neuerungen gegenüber Project Builder sind unter anderem eine neue Benutzeroberfläche, verteiltes Kompilieren via distcc, und Code-Vervollständigung (\"Code Sense\" genannt). Außerdem wurden Cocoa Bindings (bidirektionale Verknüpfungen zwischen Objekt-Attributen und Benutzeroberflächen-Elementen) eingeführt.\n\nXcode 1.1 und 1.2 behoben größtenteils Fehler der vorhergehenden Versionen; mit Xcode 1.5 wurden hauptsächlich \"Code Sense\" und das Debugging verbessert.\n\nXcode 1.0 und 1.5 können bis heute auf den Seiten des Entwickler-Portals von Apple heruntergeladen werden.\n\nMit Mac OS X Tiger gab Apple auch Xcode 2.0 frei. Bedeutsame Änderungen sind hier Quartz Composer (siehe Abschnitt „Bestandteile der Xcode Tools“), Unterstützung für Core Data (zum persistenten Speichern von Daten), und Unterstützung für Ant. Außerdem neu war das \"Apple Reference Library Tool\" zum Lesen sowohl der Online-Dokumentation auf Apples Website als auch lokal gespeicherter Dokumentation.\n\nXcode 2.1 war die erste Version, die auf PowerPC- und Intel-Prozessoren lief und sogenannte Universal Binaries erstellen konnte, bei denen PPC- und Intel-Code in derselben Binärdatei enthalten waren. Auch neu waren Unit-Testing-Targets, bedingte Haltepunkte und bessere Überprüfung von Abhängigkeiten.\n\nXcode 2.2 bis 2.4 boten nur verschiedene Verbesserungen und Fehlerkorrekturen, insbesondere in den Bereichen Kompilieren, Debuggen, und verteiltes Kompilieren. Xcode 2.5 war auch für Mac OS X Leopard verfügbar, und behob nur einige Fehler von Xcode 2.4.\n\nIm Herbst 2007 gab Apple Xcode 3.0 gemeinsam mit Mac OS X 10.5 frei. Die wichtigsten Änderungen sind Unterstützung für Objective-C 2.0 mit Garbage Collection, DTrace (in Form von Instruments), Refactoring, Snapshots, und die Möglichkeit, bis zu vier Binärdateien zu erstellen (Intel und PowerPC mit je 32 und 64-bit). Auch Dashcode wurde zu den \"Xcode Tools\" hinzugefügt.\n\nZusammen mit dem iPhone SDK 2.0 wurde Xcode 3.1 freigegeben. Damit konnte man iPhone-Anwendungen erstellen, kompilieren, ausführen und debuggen. Als Compiler wurden gcc 4.2 und LLVM-gcc 2.4 ausgeliefert, und Subversion 1.5 wurde auch unterstützt. Xcode 3.1.4 ist die letzte Version für PowerPC.\n\nMit Mac OS X Snow Leopard zusammen wurde Xcode 3.2 freigegeben. Es bietet statische Code-Analyse via clang-Compiler, Support für Grand Central Dispatch (dynamische Thread-Verwaltung) und OpenCL.\n\nXcode 4.0 wurde erstmals auf der WWDC 2010 vorgestellt und befand sich bis März 2011 in der Beta-Phase.\n\nXcode 4 wurde grundlegend überarbeitet. Zu den größten Neuerungen zählen etwa eine neue Benutzeroberfläche, die Integration des \"Interface Builder\", neue Compiler und Debugger (LLVM und LLDB), und Unterstützung für git. Xcode 4.0 wurde für 3,99 Euro im Mac App Store für Mac OS X Snow Leopard (\"Snow Leopard\") zum Herunterladen angeboten.\n\nXcode 4.1 war die erste Version für Mac OS X Lion und war für Lion-Nutzer kostenlos im \"Mac App Store\" verfügbar. Dieselbe Version konnte auch über die Entwickler-Seiten heruntergeladen werden.\n\nMit Xcode 4.2 wurde LLDB 3.0 eingeführt, außerdem wurde Unterstützung für Automatic Reference Counting hinzugefügt. Dabei fügt der Compiler (mittels Code-Analyse) an den nötigen Stellen Codezeilen zur Speicherverwaltung (codice_2 und codice_3) ein; auf diese Art werden die Vorteile von manueller Speicherverwaltung und Garbage Collection miteinander verbunden. Xcode 4.2 ist auch die letzte Version für Snow Leopard.\n\nXcode 4.3 vereinigt die \"Xcode Tools\" in einem Programm, das in codice_4 installiert wird. Damit ist eine einfachere Aktualisierung über den Mac App Store möglich. Das vorher verwendete Verzeichnis (codice_5) wird auf Nachfrage entfernt.\n\nXcode 4.4 erschien am 25. Juli 2012 zusammen mit OS X Mountain Lion. Es beinhaltet LLVM 3.2 und unterstützte neue Versionen von Objective-C und C++.\n\nXcode 4.5 erschien am 19. September 2012 und brachte Unterstützung für iOS 6.0.\n\nXcode 4.6 erschien am 28. Januar 2013 und brachte Unterstützung für iOS 6.1.\n\nXcode 5.0 wurde erstmals auf der WWDC 2013 in Zusammenhang mit der Präsentation von OS X Mavericks vorgestellt und erschien am 18. September 2013. Neu sind die Unterstützung von iOS 7 und OS X 10.9. Es beinhaltet LLVM 3.3 und LLDB. Die Unterstützung für den LLVM-GCC-Compiler und GDB-Debugger wurde entfernt.\n\nXcode 6.0 wurde erstmals auf der WWDC 2014 am 2. Juni 2014 zusammen mit iOS 8.0 vorgestellt und nach der Präsentation als Beta-Version für Entwickler zur Verfügung gestellt. Es unterstützt die von Apple entwickelte Programmiersprache Swift, die laut Apple speziell für die Verwendung mit den \"Cocoa Frameworks\" und den \"Cocoa Touch Frameworks\" konzipiert wurde. Xcode 6.0 bietet sogenanntes \"Live Rendering\", um bereits in Xcode zu beurteilen, wie sich die Benutzeroberfläche des fertigen Programms verhält.\n\nXcode 7.0 wurde erstmals auf der WWDC 2015 am 8. Juni 2015 zusammen mit iOS 9.0, OS X 10.11 El Capitan und watchOS 2.0 vorgestellt und nach der Präsentation als Beta-Version für Entwickler zur Verfügung gestellt. Mit Xcode 7.0 wurde Swift 2.0 eingeführt. Ebenso wurde das sogenannte „Live Rendering“ im Interface Builder erweitert, sodass es jetzt das exakte Aussehen der Benutzeroberfläche des fertigen Programms darstellt.\n\nSeit Xcode 7.0 ist es möglich, auch ohne eine kostenpflichtige Mitgliedschaft in Apples Entwicklerprogramm, Apps auf iOS-Geräten zu testen.\n\nXcode 8.0 wurde erstmals auf der WWDC 2016 am 13. Juni 2016 zusammen mit iOS 10.0, macOS 10.12 Sierra und watchOS 3.0 vorgestellt und nach der Präsentation als Beta-Version für Entwickler zur Verfügung gestellt. Mit Xcode 8.0 wurde Swift 3.0 eingeführt. Ebenso wurde der Interface Builder optimiert, um eine höhere Arbeitsgeschwindigkeit realisieren zu können. Darüber hinaus ist es nun möglich, Erweiterungen für den Xcode-Editor zu installieren, um die Entwicklungsumgebung zu individualisieren.\n\nXcode 9.0 wurde erstmals auf der WWDC 2017 am 5. Juni 2017 zusammen mit iOS 11.0, macOS 10.13 High Sierra, watchOS 4.0 und tvOS 11 vorgestellt und den Entwicklern nach der Präsentation als Beta-Version zur Verfügung gestellt. Mit Xcode 9.0 wurde neben Swift 3.0 die Unterstützung für Swift 4.0 hinzugefügt sowie ab Xcode 9.3 die Unterstützung für Swift 4.1.\n\nXcode 10.0 wurde erstmals auf der WWDC 2018 am 4. Juni 2018 zusammen mit iOS 12.0, macOS 10.14 Mojave, watchOS 5.0 und tvOS 12 vorgestellt und den Entwicklern nach der Präsentation als Beta-Version zur Verfügung gestellt. Zeitgleich mit iOS 12.0 wurde es am 17. September 2018 im App Store veröffentlicht. U. a. enthält Xcode 10 nun die neusten SDKs für iOS 12, watchOS 5, tvOS 12 und MacOS Mojave, sowie die Swift Version 4.2.\n\n"}
{"id": "387943", "url": "https://de.wikipedia.org/wiki?curid=387943", "title": "Newton OS", "text": "Newton OS\n\nNewton OS ist das Betriebssystem des PDAs Newton von Apple. Es ist das erste vollständig in C++ geschriebene Betriebssystem, das auf geringen Speicher- und Prozessorverbrauch hin optimiert wurde.\n\nEbenso wie die ersten Macintosh-Rechner hat der Newton in seinem Festwertspeicher (ROM) zusätzliche Programme, um mehr Arbeitsspeicher (RAM) und Flash-Speicher für Anwenderprogramme übrigzulassen.\n\nAuf Applikations-Ebene besteht keine Multitasking-Fähigkeit. Das Newton OS kann Anwendungen ausführen, die in C++ und NewtonScript geschrieben sind.\n"}
{"id": "388380", "url": "https://de.wikipedia.org/wiki?curid=388380", "title": "Interface Builder", "text": "Interface Builder\n\nDer Interface Builder ist eine Software von Apple, mit der grafische Benutzeroberflächen für Programme für macOS und iOS erstellt werden.\n\nBereits in den Developer Tools für NeXTStep war der Interface Builder enthalten. Auch bei Project Builder (2001–2003) und Xcode (2003 bis heute) war bzw. ist der Interface Builder ein wichtiger Bestandteil der Entwickler-Tools.\nBis einschließlich Xcode 3.x war der Interface Builder eine eigenständige Anwendung, mit Xcode 4.0 wurde er jedoch in die IDE integriert.\n\nDie Dateien, die der Interface Builder erzeugt, haben die Dateiendung .nib (von \"NeXT Interface Builder\"; die Dateien werden von Entwicklern oft \"nib-Dateien\" genannt) oder .xib.\n\nDas GNUstep-Projekt hat einen Klon von Interface Builder namens \"Gorm\" geschrieben.\n\nDie erste Version von Interface Builder erschien 1986 und war in Lisp geschrieben. Der Entwickler des Tools wurde noch im gleichen Jahr von NeXT übernommen, und bereits 1988 war Interface Builder Teil von NeXTStep 0.8.\nEs war die erste kommerzielle Anwendung, mit der man eine Benutzeroberfläche mithilfe einer Maus zusammenstellen konnte.\n\nNeXTStep diente als Basis für Mac OS X, und als Mac OS X 10.0 erschien, veröffentlichte Apple auch eine neue Version der Developer Tools. Teil davon war, neben einer komplett neu geschriebenen Version von Project Builder, auch Interface Builder zum Erstellen von Oberflächen für Carbon- und Cocoa-Anwendungen (in C++ bzw. Objective-C.)\n\nResEdit, das unter Mac OS 9 und davor zum Erstellen von Oberflächen verwendet wurde, konnte unter Mac OS X nicht mehr eingesetzt werden, da das neue System \"Bundles\" (Code und Ressourcen in verschiedenen Dateien, aber zusammen in einem Ordner mit einer bestimmten Struktur) statt \"Code Forks\" und \"Resource Forks\" verwendete.\n\nAb Mac OS X Panther wurde Project Builder nicht mehr unterstützt, stattdessen setzte Apple auf die Xcode Tools mit Xcode als IDE. Interface Builder wurde beibehalten.\nDa das Datenformat nicht geändert wurde, sind Programme und NIB-Dateien, die mit Panther erstellt wurden, auch auf älteren Systemen ausführbar.\n\nMit Panther wurden die Cocoa Bindings eingeführt, die UI-Elemente und Code miteinander verbinden.\n\nInterface Builder wurde mit jedem Betriebssystem-Release verbessert und erweitert, sodass es die neuen UI-Elemente einer neuen Systemversion unterstützte (z. B. die HUD-Panels in 10.5.)\n\nMit Erscheinen des iPhone SDK 2.0 und Xcode 3.1 wurde auch der Interface Builder aktualisiert, um Oberflächen von iPhone-Apps designen zu können.\n\nMit Xcode 4.0 wurde Interface Builder direkt in Xcode integriert. Das bot einige Vorteile: Entwickler mussten nicht mehr zwei Anwendungen geöffnet haben, sie mussten nicht mehr den Code speichern um ihn mit dem UI zu verbinden, und sie konnten per Drag und Drop die UI-Elemente direkt mit dem Quellcode verbinden.\n\nObjekte für eine Oberfläche sind in sogenannten Paletten gruppiert. Im AppKit-Framework existiert eine Palette für die vom System vorgegebenen UI-Elemente wie Fenster, Knöpfe, Listen, Bilder oder Textfelder.\n\nIn der Regel dient als Grundelement ein Fenster (\"NSWindow\") oder eine Fläche (\"NSView\"). Jede .nib-Datei ist mit einer Klasse verbunden, die auch die Outlets und Aktionen deklariert (siehe nächster Abschnitt); diese Klasse ist oft eine Unterklasse von \"NSWindow\" oder \"NSView\", seltener auch \"NSObject\".\n\nAuf diesen Flächen können dann weitere Elemente platziert werden, ihre Eigenschaften können verändert werden, und sie können mit dem Code verbunden werden.\n\nUI-Elemente werden über Outlets mit dem Code verbunden. Die Deklaration erfolgt in der jeweiligen Header-Datei etwa wie folgt:\n\nViele Objekte können bei bestimmten Aktionen (\"Actions\" genannt) (z. B. \"gedrückt\" oder \"verändert\") eine Nachricht an ein Ziel (\"Target\") senden. Die Deklaration im Header sieht z. B. so aus:\n\nIm eigentlichen Quellcode befindet sich eine Methode mit gleicher Deklaration, die dann bei Bedarf ausgeführt wird. Der generische Datentyp id ermöglicht es hierbei, dass die Methode von verschiedenen Objekten aufgerufen werden kann, z. B. von einem Knopf und einem Schieberegler.\n\nObjekte, Outlets, Actions und Targets werden per Drag und Drop miteinander verbunden.\n\nDie fertige Datei mit ihren Verbindungen zum Code wird in einer Property-List-Datei mit der Dateiendung .nib gespeichert (während der Entwicklung meist als XML, im fertigen Produkt als Binärdatei). Wenn das Programm ausgeführt und die NIB-Datei \"aufgeweckt\" wird, wird sie geladen und mit dem Binärcode verbunden.\n\nMit Interface Builder 3.0 wurde ein neues Dateiformat eingeführt, das die Dateiendung .xib trägt. Es hat genau die gleichen Funktionen wie .nib-Dateien, ist jedoch aufgrund seiner Datenstruktur einfacher in Tools wie Versionsverwaltungen und diff zu handhaben.\nDie Dateien werden von den meisten Entwicklern jedoch immer noch nib-Dateien genannt.\n\nEs wird seitens Apple dazu geraten, verschiedene Fenster (sofern sie nicht zu einer Klasse gehören) in verschiedene NIB-Dateien zu packen. Hauptgründe dafür sind Übersichtlichkeit und Effizienz (es dauert länger eine große Datei in den Speicher zu laden und zu verbinden als drei kleine.)\n\nElemente mit Text können entweder per Code oder direkt im Interface Builder lokalisiert werden.\nFür die Lokalisierung werden Ordner mit dem Sprachkürzel und der Endung .lproj angelegt, darin sind dann die lokalisierten NIB-Dateien gespeichert. Bei der Ausführung wird dann nur die Datei, die im Ordner mit der aktuellen Systemsprache liegt, geladen und ausgeführt.\nDie Lokalisierung per Code erfolgt per .strings-Dateien und der Methode codice_1.\n\nEs gibt auch einige Programme von Drittherstellern, mit denen man entweder die String- oder NIB-Dateien lokalisieren kann.\n\nEs ist ohne Weiteres möglich, UIs für Anwendungen nicht per Interface Builder, sondern per Code zu schreiben. Besonders um die Portierbarkeit zu gewährleisten, wird gerade bei Libraries und Frameworks in der Regel mit programmatisch erstellten Views gearbeitet. Ein weiterer Vorteil ergibt sich, wenn mehrere Entwickler an einer App arbeiten. Nur minimale Änderungen der .storyboard- oder .xib-Datei können dazu führen, dass sich die zugrundeliegende XML-Datei deutlich verändert, was relativ schnell zu Konflikten führt, wenn die verschiedenen Änderungen gemerged werden sollen. Bei manuell programmierten Views gibt es dieses Problem nicht, da sich die Struktur der Klasse nicht willkürlich ändern kann. Außerdem sind die einzelnen Teile der App besser gekapselt. Bei einem Storyboard ist in der Regel die komplette Benutzeroberfläche einer App in einer Datei, beim programmatischen Ansatz wird jede View in einer eigenen Klasse erstellt.\n\nProgrammatisch erstellte Views sind zudem performanter, da hier keine zusätzliche XML-Datei vom Speicher geladen und geparst werden muss, und deutlich mächtiger, da beispielsweise nur im Code weitere Eigenschaften wie Schatten oder Rahmen gesetzt werden können. Der Performanceverlust des Interface Builders macht sich zudem nicht nur zur Laufzeit, sondern bereits zur Entwicklungszeit bemerkbar. Gerade bei komplexen Storyboards (hier werden mehrere Views in einer Datei verwaltet) kommt es dazu, dass das Öffnen der Datei im Interface Builder spürbar lange dauert.\n\nNachteilig ist der erhöhte Zeitaufwand beim programmatischen Erstellen der View. Zum einen dauert es länger, die einzelnen Code-Zeilen zu schreiben, zum anderen ist es schwerer, die Elemente genau auszurichten. Da das grafische Ergebnis erst zur Laufzeit – und nicht wie beim Interface Builder zur Übersetzungszeit – betrachtet werden kann, sind Änderungen erst nach dem Kompilieren und Starten der App sichtbar. Das Überprüfen einer jeden Änderung benötigt daher deutlich mehr Zeit als bei einem WYSIWYG-Editor wie dem Interface Builder. Diese erhöhte Komplexität ist sowohl für Neulinge als auch für schnelles Prototyping ungeeignet.\n"}
{"id": "389215", "url": "https://de.wikipedia.org/wiki?curid=389215", "title": "Celestia", "text": "Celestia\n\nCelestia ist ein 3D-Astronomieprogramm für Windows, macOS, FreeBSD und Linux, entwickelt von Chris Laurel. Es ist Open Source und GPL-lizenziert. Seit dem 20. Oktober 2008 gibt es auch eine portable Version von PortableApps. Wie auch für die Standard-Version gibt es auf der deutschen Celestia-Website ein deutsches Sprachpaket für die Portable-Version.\n\nDas Programm erlaubt es Benutzern, Objekte in der Größenordnung der Internationalen Raumstation bis hin zu Galaxien wie der Milchstraße dreidimensional zu simulieren. Das Programm benutzt OpenGL als 3D-Bibliothek. Anders als normalerweise bei Planetariums-Software ist der Benutzer in der Lage, sich frei durch das Universum zu bewegen. In dieser und weiteren Eigenschaften ähnelt es dem Programm Space Engine.\n\nDie NASA und die ESA haben das Potential von Celestia erkannt und es für eigene Demonstrationszwecke benutzt \"(siehe hierzu unter Weblinks)\". Nicht mit Celestia zu verwechseln ist dabei das ESA-eigene Programm Celestia 2000. Die ESA verwendet Celestia inzwischen in einem eigenständigen Programm (Space Trajectory Analysis) zur Analyse, Simulation und Visualisierung von Flugbahnen.\n\nDas Institut für Raumfahrtsysteme der Universität Stuttgart nutzt Celestia als Live-Visualisierung für die in Zusammenarbeit mit EADS Astrium aufgebaute Systemsimulationsinfrastruktur im Rahmen der Satellitenentwicklung. Auch die für die Lehre am Institut genutzte Systemsimulationsinfrastruktur \"OpenSimKit\" stellt die simulierte Raketenoberstufe mit Celestia dar.\n\nAb der Version 1.3.1 wird Celestia auch als deutschsprachige Fassung angeboten.\n\nAnfang August 2016 wurde die Pflege und Wartung von Celestia von einem russischen Celestia-Enthusiasten übernommen und das Benutzerforum reaktiviert.\n\n\n\nÜber 10 GB an Erweiterungen (sogenannten Add-Ons) sind zur Ergänzung des Basisprogramms verfügbar. Diese enorme Menge ist das Ergebnis der Aktivitäten einer kleinen, aber sehr produktiven Benutzergemeinschaft, die das Programm unterstützt. Die größte Sammlung wird von dem Portal „The Celestia Motherlode“ (siehe Weblinks) bereitgestellt. Diese „Add-Ons“ umfassen folgende Bereiche:\n\nDie Texturen der Planeten und anderer Himmelskörper, wie Monde und Kometen, sind durch sehr hochauflösende Texturen (bis zu 128.000 × 64.000 Pixel), Normalmaps oder Bumpmaps, Specularmaps (für Spiegelungen), Wolkentexturen und Nachttexturen austauschbar. Für die Erde beispielsweise werden auch Satellitenaufnahmen, welche über sogenannte „Virtual Textures“ eingebunden werden, zur Verfügung gestellt, wodurch eine sehr detaillierte Ansicht der Planeten ermöglicht wird. Es können 3D-Objekte für besonders hohe Gebirge oder berühmte Bauwerke eingebaut werden, um eine noch realistischere Ansicht zu bekommen. Politische Karten oder wissenschaftliche Texturen, wie die Gravitationsverteilung auf der Planetenoberfläche sind auch verfügbar und können im Programm schnell ausgewählt werden.\n\nEs gibt verschiedene Datenbanken, von Objekten in Größenordnungen von Satelliten über Monde, Asteroiden und Sterne bis hin zu Galaxien. Allein die erweiterte Sternendatenbank besitzt zwei Millionen Sterne und die Galaxiendatenbank fast 2.000, wodurch auch Galaxienhaufen sichtbar werden. Die Städtedatenbank der Erde ist auf 96.000 Orte erweiterbar.\n\nEs besteht die Möglichkeit, Flugbahnen von Satelliten einzubinden. So können die kompletten Missionen (zum Beispiel Cassini-Huygens) nachsimuliert werden.\n\nDie Benutzergemeinschaft hat Skripte entwickelt, die ähnlich wie im Planetarium den Benutzer durch das Sonnensystem führt. Spezielle Dokumentationen im Textformat, \"Educational Activities\" genannt, die zum Beispiel über die Geburt und den Tod der Sterne informieren, sind mit Celestia verknüpft, wodurch solche Themen anschaulich erklärt werden können.\n\nEs wurden auch Erweiterungen für Science-Fiction-Begeisterte entwickelt, welche Universen aus berühmten Filmen wie Star Wars oder Star Trek simulieren, aber auch selbst ausgedachte Planetensysteme ohne Bezug auf einen Film oder ein Buch.\n\nAb Version 1.5.1 unterstützt Celestia auch die Ausführung von Add-Ons, die in der Skriptsprache Lua geschrieben wurden. Dazu greift eine in Celestia enthaltene C-Bibliothek auf den Lua-Interpreter zu.\n\n\n"}
{"id": "389415", "url": "https://de.wikipedia.org/wiki?curid=389415", "title": "GNU Aspell", "text": "GNU Aspell\n\nGNU Aspell oder kurz Aspell ist eine freie Software zur Rechtschreibprüfung für Unix-artige Systeme und Windows. Aspell ist Teil des GNU-Projekts und soll einmal Ispell ersetzen. Unter GNU/Linux ist es bereits recht weit verbreitet. Wie Ispell kann auch Aspell sowohl als eigenständiges Programm wie auch als Programmbibliothek verwendet werden. Ein Vorzug von Aspell ist ein für das Englische recht gut funktionierender Algorithmus zur phonetischen Suche nach falsch geschriebenen Wörtern. Es existieren Wörterbücher für über 70 Sprachen, darunter auch zwei für die deutsche Sprache (alte und neue Rechtschreibung).\n\nIm Rennen um die Ispell-Nachfolge hat Aspell mit Hunspell einen starken Konkurrenten, da die OpenOffice.org-Rechtschreibprüfung Hunspell unter anderem bereits Einzug in die Webbrowser Mozilla Firefox, Google Chrome und Opera sowie in das seit August 2009 verfügbare Apple-Betriebssystem Mac OS X Snow Leopard gefunden hat. \n\n"}
{"id": "392266", "url": "https://de.wikipedia.org/wiki?curid=392266", "title": "Magic Disk 64", "text": "Magic Disk 64\n\n\"Magic Disk 64 – Das C64-Magazin auf Diskette\" war ein von 1987 bis 1993 von der CP Computer Publications GmbH veröffentlichtes Disketten-Magazin.\n\nNeben der \"Game On\" war \"Magic Disk 64\" das bekannteste monatlich erscheinende deutschsprachige C64-Diskmag. Die  Zoll große Diskette war mit einem Menü (ab Ende 1989 mit Hintergrundmusik) ausgestattet, von dem aus die einzelnen Programme angesehen und gestartet werden konnten. Das eigentliche Magazin befand sich ebenfalls auf der Diskette und musste auf einem C64 gestartet werden, um den Inhalt lesen zu können. Er bestand überwiegend aus Programmierkursen und Leserbriefen. Die ersten Ausgaben enthielten auch Spieletests. Auf der zweiten Seite der Diskette wurden Programme, Programmierwerkzeuge und Spiele veröffentlicht. Erhältlich war die \"Magic Disk 64\" bei Zeitschriftenhändlern, an Kiosken und im Abonnement zu einem Preis von 9,80 DM.\n\nAb Anfang November 1987 erschien die \"Magic Disk 64\" im Monatsrhythmus. Ende 1989 wurde ein neues Menüsystem eingeführt, das grafisch aufwändiger und mit Musik unterlegt war. Ab Ausgabe 12/93 verlor die \"Magic Disk 64\" ihre Eigenständigkeit und wurde zusammen mit der \"Game On\" als Diskettenbeilage zur Amiga- und PC-Spielezeitschrift \"Play Time\" verkauft. Mit der Ausgabe 7/95 wurde die \"Magic Disk 64\" und \"Game On\" eingestellt. Danach erschien eine Zeitschrift namens \"Magic Disk Classic\", die im Gegensatz zur \"Magic Disk 64\" jedoch eine gedruckte Zeitschrift mit Diskettenbeilage war. Von der \"Magic Disk Classic\" erschienen nur wenige Ausgaben (8/95 bis 1/96), danach ging sie in der Zeitschrift \"64’er\" auf.\n\n"}
{"id": "393753", "url": "https://de.wikipedia.org/wiki?curid=393753", "title": "Text-Extraction", "text": "Text-Extraction\n\nDie Text-Extraction ( auch ) bzw. Textextrahierung ist eine Methode zur automatischen Zusammenfassung eines Textes mit Hilfe computerlinguistischer Techniken. Dabei werden Teile eines Textes – zum Beispiel Sätze oder ganze Abschnitte – mittels statistischer und/oder heuristischer Methoden bezüglich ihrer Wichtigkeit oder Relevanz bewertet. Diese \"scores of importance\" dienen als Grundlage für die Entscheidung, welche Teile (\"keyphrases\") extrahiert und zu einem kürzeren Text zusammengestellt werden, der dann einen Überblick über die Inhalte des Originaltextes bietet und in der Regel als \"extract\" oder abstract bezeichnet wird.\n\nNach Karen Spärck Jones (1999) haben die mit dieser Methode produzierten Zusammenfassungen den Nachteil, dass sie zumeist wenig kohärent und somit nur schlecht lesbar und unter Umständen sogar unverständlich sind. Andererseits ist diese Methode und ihre Varianten vermutlich einfacher in automatischen Systemen zu modellieren. Beispiele dafür sind die Systeme von Luhn (1959) und Edmundson (1969) und die Ansätze von Rath et al. (1961) und Brandow et al. (1995).\n\n"}
{"id": "393757", "url": "https://de.wikipedia.org/wiki?curid=393757", "title": "Fact-Extraction", "text": "Fact-Extraction\n\nFact-Extraction bzw. Faktenextrahierung ist das Bestreben, bei der automatischen Zusammenfassung von Texten mit Hilfe von Techniken der Computerlinguistik einen kohärenten neuen Text zu schaffen. \n\nSie ist damit Teil der Text-Extraction, eines Spezialgebiets des Data-Mining.\nAnders als bei der \"Text-Extraction\" werden hier nicht aus der Erkenntnis über Schlüsselphrasen (Keyphrases) rein formale, damit u. U. unverständliche Zusammenfassungen ermittelt.\n\nDie mit Hilfe der \"Fact-Extraction\" gewonnenen Erkenntnisse über die Inhalte eines Textes werden als Grundlage für die Generierung eines ganz neuen, wesentlich kürzeren Textes verwendet. Dieser ist dann die Zusammenfassung des Originaltextes und wird in der Regel als Abstract bezeichnet.\n\nDie so produzierten Zusammenfassungen sind zwar meist zusammenhängend, aber dieser Ansatz erfordert umfangreiche Wissensbasen und seine Leistungsfähigkeit ist abhängig von der jeweiligen Textsorte.\n"}
{"id": "393772", "url": "https://de.wikipedia.org/wiki?curid=393772", "title": "Phrack (Magazin)", "text": "Phrack (Magazin)\n\nDas Phrack-Magazin (engl. \"Phrack Magazine\", aus engl. phreak\" und \"hack, dt. \"Begeisterter\" und (Daten)-\"Hacker\") ist ein seit dem 17. November 1985 bestehendes Untergrund-Magazin, das kostenlos über das Internet verbreitet wird. Es war das erste lediglich elektronisch verbreitete Magazin und behandelt vor allem Themen der IT-Sicherheit, des Hackens, Phreaking, Anarchismus und zählt, ähnlich wie das 2600-Magazin, zu den bedeutendsten Magazinen der Hacker-Szene.\n\nIm Sommer 2005 wurde die Nachricht verbreitet, dass \"Phrack\" mit der Ausgabe 63 eingestellt werden sollte. Diese „letzte“ Ausgabe erschien am 29. Juli erstmals als gedrucktes Heft, welches kostenlos auf den Hacker-Veranstaltungen What The Hack und DEF CON verteilt wurde. Laut Angaben im Editorial dieser Ausgabe sollte es sich aber nicht definitiv um die letzte Ausgabe handeln.\n\nIm Mai 2007 erschien Ausgabe 64 mit neuen Herausgebern. Seitdem erschienen in unregelmäßigen Abständen weitere Ausgaben. Die aktuelle Ausgabe 69 wurde am 6. Mai 2016 veröffentlicht.\n\nWeiterhin lassen sich in früheren Ausgaben des Phrack-Magazin Informationen zur Erstellung von einer so genannten Zip Gun, einer Handfeuerwaffe, und anderen Bomben/Waffen finden. Häufig wird auch irreführend behauptet dass in einer Ausgabe die Herstellung der Droge „Crystal Speed“ beschrieben wird. Zwar findet sich in Ausgabe 4 tatsächlich ein Artikel der die angebliche Herstellung der Droge aus einem käuflich zu erwerbendem Erkältungspräparat beschreibt. Diese Anleitung ist aber nicht tatsächlich zur Herstellung der Droge geeignet, da das beschriebene Erkältungspräparat den Wirkstoff nur in einer abgewandelten, zu Rauschzwecken untauglichen Variante enthält.\n\n\n"}
{"id": "394369", "url": "https://de.wikipedia.org/wiki?curid=394369", "title": "Matra Alice", "text": "Matra Alice\n\nDer Matra Alice war ein in Frankreich ab 1983 durch das Unternehmen Matra verkaufter Heimcomputer. Produziert wurde der Rechner durch eine Kooperation zwischen den Unternehmen Matra, Hachette Livre in Frankreich und der Tandy Corporation in den USA. Das augenfälligste Hauptmerkmal war das rote Gehäuse. Intern handelte sich um einen Tandy TRS-80 MC-10 Computer zu dem er folgerichtig auch softwarekompatibel ist. Nachfolgegeräte waren der Matra Alice 32 und der Matra Alice 90.\n\nTechnische Daten:\n\n\n\n"}
{"id": "395494", "url": "https://de.wikipedia.org/wiki?curid=395494", "title": "Datei-Manager (Windows)", "text": "Datei-Manager (Windows)\n\nDer Datei-Manager war der standardmäßig installierte Dateimanager der 16-Bit-Betriebssysteme der Windows-3.x-Reihe sowie der 32-Bit-Betriebssysteme Windows NT 3.1 und Windows NT 3.51. Mit der Veröffentlichung von Windows 95 wurde er durch den Windows-Explorer abgelöst, war aber unter dem Dateinamen „WINFILE.EXE“ noch über Jahre Bestandteil des Systems. 2018 wurde der Quellcode unter der MIT-Lizenz auf GitHub veröffentlicht.\n\nDer Datei-Manager wurde mit Microsoft Windows 3.0 eingeführt, es ersetzte das vorher verwendete \"MS-DOS Executive\" (im deutschsprachigem Windows: \"MS-DOS\"). Er war in allen Windows 3.x-Versionen der Standard-Dateimanager von Windows. Windows 95 bietet in der ursprünglichen A-Version während des Setups die Möglichkeit, statt des neuen Windows-Explorer weiterhin die alte Kombination aus Programm-Manager und Datei-Manager zu verwenden. Der Datei-Manager dieser Windows-Betriebssysteme ist nicht Jahr-2000-kompatibel, es ist aber ein Patch von Microsoft verfügbar, der dieses Problem behebt.\n\nMit der Veröffentlichung von Windows NT 3.1, welches dieselbe Benutzeroberfläche wie Windows 3.1 verwendete, wurde eine 32-Bit-Version des Datei-Managers eingeführt. Mit Windows NT 4.0 wurde der Datei-Manager auch in dieser Betriebssystemreihe durch den Windows-Explorer abgelöst. Als alternativer Dateimanager ist er in der 16-Bit-Version noch bis Windows Me enthalten und die 32-Bit-Version in NT 4.0, wobei die ausführbare Datei bis Windows XP (Home) offenbar vollkommen kompatibel ist.\n\nAm 6. April 2018 veröffentlichte Microsoft den Quellcode und für Windows 10 kompilierte Versionen des Datei-Managers auf GitHub unter der MIT-Lizenz (Open-Source).\n\nDas Programm ist als Verknüpfung über die Hauptgruppe des Programm-Managers aufrufbar. Das Fenster des Datei-Managers ist unterteilt in ein Drop-Down-Menü zum Wechseln zwischen Laufwerken am oberen Rand, einer Ordneransicht (die wesentliche Neuerung im Vergleich zum sonst ähnlichen MS-DOS-Fenster) auf der linken sowie eine Dateiansicht auf der rechten Seite. Neben dem Verwalten von Dateien und Ordnern erlaubte der Datei-Manager das Formatieren von Laufwerken, das Verbinden eines Netzlaufwerks sowie die Zuordnung von Dateierweiterungen.\n\nUnter Windows NT konnten die erweiterten Möglichkeiten des NTFS-Dateisystems verwaltet werden, also Zugriffsrechte und Datenkompression. Diese stehen mit der 32-Bit-Variante auch unter der Home-Edition von Windows XP zur Verfügung, die im Windows-Explorer ausgeblendet werden. Außerdem ist Winfile weniger anfällig gegen verschiedene Schutzverletzungen oder ausgeblendete Dateien und Laufwerke und eignet sich deshalb als grafisches Werkzeug bei bestimmten Reparaturzwecken.\n\n"}
{"id": "395591", "url": "https://de.wikipedia.org/wiki?curid=395591", "title": "Enterprise Objects Framework", "text": "Enterprise Objects Framework\n\nDas Enterprise Objects Framework oder kurz EOF ist ein Framework zur objektrelationalen Abbildung, welches von NeXT, heute Apple zunächst in Objective-C entwickelt, später nach Java portiert wurde und heute als Bestandteil von WebObjects verkauft wird.\n\nHeutzutage existieren diverse Open-Source-Implementierungen von EOF in Objective-C: GDL2, die GNUstep Database Library, sowie eine Implementierung als Bestandteil von Open Groupware.\n"}
{"id": "397404", "url": "https://de.wikipedia.org/wiki?curid=397404", "title": "V8 (JavaScript-Implementierung)", "text": "V8 (JavaScript-Implementierung)\n\nV8 ist eine freie Implementierung der Skriptsprache ECMAScript (JavaScript) nach dem Standard ECMA-262. Sie wird vom Unternehmen Google Inc. unter einer BSD-Lizenz als freie Software veröffentlicht. Ausgeliefert wird sie als Teil des Webbrowsers Google Chrome, kann jedoch auch unabhängig davon verwendet werden. V8 soll die Ausführung von JavaScript-Code beschleunigen.\n\nV8 wird von Googles V8-Team unter der Leitung von Lars Bak hauptsächlich in Googles Forschungslabor in Aarhus (Dänemark) entwickelt, das mit der Universität Aarhus zusammen Forschung an und Entwicklung von virtuellen Maschinen betreibt.\n\nV8 wird in der Programmiersprache C++ entwickelt und unterstützt die x86- und ARM-Hardware-Architekturen und Mehrkernprozessoren.\n\nV8 steigert die Leistung zur Laufzeit, indem der JavaScript-Code bei der Ausführung durch sogenannte Just-in-time-Kompilierung zunächst in nativen Maschinencode übersetzt wird. Weitere Leistungsvorteile ergeben sich aus Optimierungstechniken wie dem Einsatz von Inline Caching, das JavaScript-Objekte versteckt, um geteilte Klassen erweitert, und einer sogenannten „exakten automatischen Speicherbereinigung“, die Speicher schnell und in kleinen Portionen zuweist und wieder freigibt, was hierbei längere Wartezeiten vermeidet.\n\nDie Initialisierung der V8 wird durch Snapshots beschleunigt. Bei der Kompilierung der V8 wird eine JavaScript-Umgebung erstellt, serialisiert und durch den JIT vorkompiliert. Das Kompilat wird als Snapshot in die endgültige V8 integriert und dient zur Laufzeit neuen Instanzen der V8 als Vorlage.\n\nV8 befindet sich seit 2006 in der Entwicklung.\nAm 3. Juli 2008 wurde erstmals Quellcode veröffentlicht.\nAls V8 am 2. September 2008 als Teil von Google Chrome veröffentlicht wurde, erreichte es im Vergleich mit anderen gebräuchlichen Implementierungen deutlich überlegene Ausführungsgeschwindigkeiten.\nIn der Folge zogen die Hersteller der anderen verbreiteten Browser daraufhin bald mit ähnlichen Optimierungen nach (wie Mozilla mit den Entwicklungen der Projekte \"TraceMonkey\" und \"JägerMonkey\"), bis zuletzt auch Microsoft mit der im Internet Explorer 9 enthaltenen überarbeiteten JavaScript-Implementierung zu ähnlichen Ergebnissen kam.\nMit der von Google unter dem Projektnamen \"Crankshaft\" entwickelten Technik zur Optimierung des JavaScript-Codes zur Laufzeit wurden nochmals deutliche Leistungssteigerungen erzielt. Sie wurde im Dezember 2010 erstmals vorgestellt und ist in Google Chrome ab der Version 10 enthalten.\n\n2016 erhielt sie den Programming Languages Software Award von ACM SIGPLAN.\nV8 wird primär für den Einsatz im Browser Google Chrome entwickelt, aber auch in der ereignisgesteuerten Plattform „Node.js“ findet sie Verwendung. Des Weiteren wird V8 im Rahmen des V8CGI-Projektes (mittlerweile umbenannt in TeaJS) als serverseitige Scriptsprache genutzt, und in der neuen Version 5 der Qt-Bibliothek ersetzt V8 die bisher verwendete JavaScript-Implementierung. Das NoSQL-Datenbanksystem MongoDB benutzt V8 als JavaScript-Implementierung. Mit der V8js Extension kann V8 in PHP integriert werden.\n\n\n"}
{"id": "399455", "url": "https://de.wikipedia.org/wiki?curid=399455", "title": "Optische Maus", "text": "Optische Maus\n\nEine optische Maus ist ein Zeigegerät in der Informationstechnologie. Diese Maus erfasst die Bewegung optisch, im Gegensatz zur mechanischen Erfassung per Rollkugel. Die Funktionsweise basiert auf Bildverarbeitung. Ein Sensorchip wirkt als kleine Kamera, kombiniert mit einer in der Maus integrierten Recheneinheit. Der optische Sensor nimmt ständig Bilder von der Oberfläche unter der Maus auf und ermittelt aus der Bildverschiebung die Bewegung der Maus. \n\nDie Mausunterlage wird von einer Leuchtdiode (LED) oder einer Laserdiode (Lasermaus) beleuchtet. Das von der Unterlage reflektierte Bild wird (über eine Linse) von der Minikamera des Sensorchips aufgenommen. Die Auflösung der Minikamera reicht dabei von 16 × 16 bis 30 × 30 Pixel. Die Bildinformation kommt als Graustufenbild in den Digitalen Signal-Prozessor (DSP) des Chips. Dort wird das Bild mit den vorigen Bildern verglichen und zunächst Geschwindigkeiten und Richtungen bestimmt. Dann werden aus den Geschwindigkeiten schließlich die Bewegungsdaten (Δx- und Δy-Werte) errechnet. Als Algorithmus für die Geschwindigkeitsberechnung wird die Bildkorrelation verwendet, welche seit Ende der 1990er Jahre kostengünstig direkt in einem Chip in der Maus realisiert werden kann. Üblich sind Abtastraten um 1500 Bilder pro Sekunde; die Rechenleistung des DSP zur Ermittlung der Bewegungsinformation beträgt ca. 18 MIPS.\n\nBei der ersten Generation optischer Mäuse besteht die Lichtquelle aus einer roten Leuchtdiode. Diese Technik zeigt jedoch Schwierigkeiten bei der Ermittlung der Bewegungsrichtung und -geschwindigkeit auf einfarbigen, transparenten, glatten oder glänzenden Oberflächen. Eine Weiterentwicklung besteht darin, statt einer roten eine blaue Leuchtdiode zu verwenden. Sie soll gegenüber Laser- und herkömmlichen, optischen Mäusen flexibler sein, was den verwendeten Untergrund angeht. Allerdings können auch hier keine Glas- und Spiegelflächen genutzt werden. Diese Technik wird als Bluetrack oder Blue Wave bezeichnet.\n\nNeben der Verwendung von LED wurden Mäuse mit Laser als Lichtquelle entwickelt. Bei der Lasermaus werden die Strukturen der Mausunterlage deutlicher aufgelöst. Dadurch kann die Lasermaus auch bei glatten, reflektierenden Oberflächen verwendet werden, die für LED-Mäuse ungeeignet sind, wie Glas oder Spiegelflächen. Allerdings sind sie auch empfindlicher für Unebenheiten, Unreinheiten und sonstige Oberflächenstörungen.\n\n Ursprünglich entwickelt wurde die Technik der Positionsbestimmung mithilfe einer Kamera von Hewlett Packard für die Navigation von Flugzeugen. Probleme mit instabilen Lichtverhältnissen und sinkende Kosten der satellitenbasierten Navigation haben jedoch eine erfolgreiche Vermarktung verhindert. Der Einsatz als Maussensor hat die Technik letztlich zu einem großen Erfolg gemacht. Die optischen Mäuse haben in wenigen Jahren die mechanische Variante vollständig verdrängt.\n\nBei der späteren Aufsplittung von Hewlett Packard wurden die Maussensoren zunächst von Agilent weitergeführt, mittlerweile gehört die Sparte des HP-Konzernes zu Avago Technologies.\n\nAktuell gibt es nur einen Lizenznehmer für die Technologie der optischen Maus, ST Microelectronics. Allerdings ist Avagotech ständig damit beschäftigt, Plagiate dieser Technik zu verfolgen.\n\nVorläuferformen der derzeitigen optischen Mäuse gab es bereits in den 1980er und frühen 1990er Jahren. Diese benötigten ein spezielles Mauspad, das mit einer Gitterstruktur versehen war. Dabei waren die senkrechten und waagerechten Gitterlinien mit unterschiedlichen Farben gedruckt, und diese Mäuse hatten zwei Reflexlichtschranken, von denen eine mit rotem und die andere mit infrarotem Licht arbeitete. Die Auflösung dieser Mäuse war vergleichsweise gering, aber sie waren sehr exakt benutzbar, und das Fehlen mechanisch bewegter Teile war ein Vorteil für den Nutzer.\n\n"}
{"id": "401223", "url": "https://de.wikipedia.org/wiki?curid=401223", "title": "Text Mining", "text": "Text Mining\n\nText Mining, seltener auch Textmining, Text Data Mining oder Textual Data Mining, ist ein Bündel von Algorithmus-basierten Analyseverfahren zur Entdeckung von Bedeutungsstrukturen aus un- oder schwachstrukturierten Textdaten. Mit statistischen und linguistischen Mitteln erschließt Text-Mining-Software aus Texten Strukturen, die die Benutzer in die Lage versetzen sollen, Kerninformationen der verarbeiteten Texte schnell zu erkennen. Im Optimalfall liefern Text-Mining-Systeme Informationen, von denen die Benutzer zuvor nicht wissen, ob und dass sie in den verarbeiteten Texten enthalten sind. Bei zielgerichteter Anwendung sind Werkzeuge des Text Mining außerdem in der Lage, Hypothesen zu generieren, diese zu überprüfen und schrittweise zu verfeinern.\n\nDas 1995 von Ronen Feldman und Ido Dagan als \"Knowledge Discovery from Text (KDT)\" in die Forschungsterminologie eingeführte Text Mining ist kein klar definierter Begriff. In Analogie zu Data-Mining in der Knowledge Discovery in Databases (\"KDD\") ist Text Mining ein weitgehend automatisierter Prozess der Wissensentdeckung in textuellen Daten, der eine effektive und effiziente Nutzung verfügbarer Textarchive ermöglichen soll. Umfassender kann Text Mining als Prozess der Zusammenstellung und Organisation, der formalen Strukturierung und algorithmischen Analyse großer Dokumentsammlungen zur bedarfsgerechten Extraktion von Informationen und der Entdeckung versteckter inhaltlicher Beziehungen zwischen Texten und Textfragmenten gesehen werden.\n\nDie unterschiedlichen Auffassungen von Text Mining können mittels verschiedener Typologien geordnet werden. Dabei werden Arten des Information Retrieval (\"IR\"), des Dokumenten-Clustering, des Text Data-Mining und des KDD immer wieder als Unterformen des Text Mining genannt.\n\nBeim \"IR\" ist dabei bekannt, dass die Textdaten bestimmte Fakten enthalten, die mittels geeigneter Suchanfragen gefunden werden sollen. In der \"Data-Mining-Perspektive\" wird Text Mining als „Data-Mining auf textuellen Daten“ verstanden, zur Exploration von (interpretationsbedürftigen) Daten aus Texten. Die weitestgehende Art des Text Mining ist das eigentliche \"KDT\", bei der neue, zuvor unbekannte Informationen aus den Texten extrahiert werden sollen.\n\nText Mining ist mit einer Reihe anderer Verfahren verwandt, von denen es wie folgt abgegrenzt werden kann.\n\nAm stärksten ähnelt Text Mining dem Data-Mining. Mit diesem teilt es viele Verfahren, nicht jedoch den Gegenstand: Während Data-Mining zumeist auf stark strukturierte Daten angewandt wird, befasst sich Text Mining mit wesentlich schwächer strukturierten Textdaten. Beim Text Mining werden deshalb in einem ersten Schritt die Primärdaten stärker strukturiert, um ihre Erschließung mit Verfahren des Data-Mining zu ermöglichen. Anders als bei den meisten Aufgaben des Data-Mining sind zudem Mehrfachklassifikationen beim Text Mining meist ausdrücklich erwünscht.\n\nDes Weiteren greift Text Mining auf Verfahren des Information Retrieval zurück, die für die Auffindung derjenigen Textdokumente, die für die Beantwortung einer Suchanfrage relevant sein sollen, konzipiert sind. Im Gegensatz zum Text Mining werden also nicht möglicherweise unbekannte Bedeutungsstrukturen im Gesamttextmaterial erschlossen, sondern anhand von bekannten Schlüsselwörtern eine Menge relevant erhoffter Einzeldokumente identifiziert.\n\nVerfahren der Informationsextraktion zielen darauf ab, aus Texten einzelne Fakten zu extrahieren. Informationsextraktion verwendet oft die gleichen oder ähnliche Verfahrensschritte wie dies im Text Mining getan wird; bisweilen wird Informationsextraktion deshalb als Teilgebiet des Text Mining betrachtet. Im Gegensatz zu (vielen anderen Arten des) Text Mining sind hier aber zumindest die Kategorien bekannt, zu denen Informationen gesucht werden – der Benutzer weiß, was er nicht weiß.\n\nVerfahren des automatischen Zusammenfassens von Texten, der Textextrahierung, erzeugen ein Kondensat eines Textes oder einer Textsammlung; dabei wird jedoch, anders als beim Text Mining, nicht über das in den Texten explizit Vorhandene hinausgegangen.\n\nAls eine Fortsetzung des Text Mining kann das Argumentation Mining betrachtet werden. Hierbei ist es das Ziel, Argumentationsstrukturen zu extrahieren.\n\nWeb Mining, insbesondere \"Web Content Mining\", ist ein wichtiges Anwendungsgebiet für Text Mining. Noch relativ neu sind Versuche, Text Mining als Methode der sozialwissenschaftlichen Inhaltsanalyse zu etablieren, beispielsweise Sentiment Detection zur automatischen Extraktion von Haltungen gegenüber einem Thema.\n\nDie Internetseite \"Wort des Tages\", ein Projekt der Universität Leipzig, zeigt, was Text-Mining-Verfahren leisten können. Sie zeigt an, welche Wörter im Web aktuell häufig verwendet werden. Die Aktualität eines Begriffs ergibt sich dabei aus seiner aktuellen Häufigkeit, verglichen mit seiner durchschnittlichen Häufigkeit über einen längeren Zeitraum hinweg.\n\nText Mining geht in mehreren Standardschritten vor: Zunächst wird ein geeignetes Datenmaterial ausgewählt. In einem zweiten Schritt werden diese Daten so aufbereitet, dass sie im Folgenden mittels verschiedener Verfahren analysiert werden können. Schließlich nimmt die Ergebnispräsentation einen ungewöhnlich wichtigen Teil des Verfahrens ein. Alle Verfahrensschritte werden dabei softwareunterstützt.\n\nText Mining wird auf eine (meist sehr große) Menge von Textdokumenten angewandt, die gewisse Ähnlichkeiten hinsichtlich ihrer Größe, Sprache und Thematik aufweisen. In der Praxis stammen diese Daten meist aus umfangreichen Textdatenbanken wie PubMed oder LexisNexis. Die analysierten Dokumente sind unstrukturiert in dem Sinn, dass sie keine einheitliche Datenstruktur aufweisen, man spricht deshalb auch von „freiem Format“. Trotzdem weisen sie jedoch semantische, syntaktische, oft auch typographische und seltener auch markup-spezifische Strukturmerkmale auf, auf die Text-Mining-Techniken zurückgreifen; man spricht deshalb auch von \"schwachstrukturierten\" oder \"halbstrukturierten\" Textdaten. Meist entstammen die zu analysierenden Dokumente aus einem gewissen Diskursuniversum (\"domain\"), das mehr (z. B. Genomanalyse) oder weniger (z. B. Soziologie) stark abgegrenzt sein kann.\n\nDas eigentliche Text Mining setzt eine computerlinguistische Aufbereitung der Dokumente voraus. Diese basiert typischerweise auf den folgenden, nur zum Teil automatisierbaren Schritten.\nZunächst werden die Dokumente in ein einheitliches Format – heutzutage zumeist XML – überführt.\n\nZur Textrepräsentation werden die Dokumente dann zumeist anhand von Schriftzeichen, Wörtern, Begriffen (\"terms\") und/oder so genannten \"concepts\" tokenisiert. Dabei steigt bei vorstehenden Einheiten die Stärke der semantischen Bedeutung, aber gleichzeitig auch die Komplexität ihrer Operationalisierung, oft werden deshalb Hybridverfahren zur Tokenisierung angewandt.\n\nIn der Folge müssen Worte in den meisten Sprachen lemmatisiert werden, das heißt, auf ihre morphologische Grundform reduziert werden, bei Verben also zum Beispiel der Infinitiv. Dies erfolgt durch Stemming.\n\nZur Lösung einiger Probleme werden digitale Wörterbücher benötigt. Ein Stoppwörterbuch entfernt diejenigen Wörter aus den zu analysierenden Daten, bei denen keine oder kaum Vorhersagekraft erwartet wird, wie dies zum Beispiel oft bei Artikeln wie „der“ oder „eine“ der Fall ist. Um Stoppwörter zu erkennen, werden oft Listen mit den am häufigsten im Textkorpus vorkommenden Wörter erstellt; diese enthalten zumeist neben Stoppwörtern auch die meisten domainspezifischen Ausdrücke, für die normalerweise ebenfalls Wörterbücher erstellt werden. Auch die wichtigen Probleme der Polysemie – die Mehrdeutigkeit von Wörtern – und Synonymie – die Gleichbedeutung verschiedener Worte – werden mittels Wörterbüchern gelöst. (Oft domainspezifische) Thesauri, die das Synonymproblem abschwächen, werden dabei zunehmend in großen Corpora automatisch generiert.\n\nJe nach Analyseart kann es möglich sein, dass Phrasen und Wörter auch durch Part-of-speech-Tagging linguistisch klassifiziert werden, häufig ist dies jedoch für Text Mining nicht notwendig.\n\n\nUm die Semantik der analysierten Textdaten besser bestimmen zu können, wird meist auch auf themenspezifisches Wissen zurückgegriffen.\n\nAuf der Grundlage dieser partiell strukturierten Daten können die eigentlichen Text-Mining-Verfahren aufbauen, die vor allem auf der Entdeckung von Kookkurrenzen, idealiter zwischen \"concepts\", basieren. Diese Verfahren sollen:\n\nKernoperationen der meisten Verfahren sind dabei die Identifizierung von (bedingten) Verteilungen, häufige Mengen und Abhängigkeiten. Eine große Rolle bei der Entwicklung solcher Verfahren spielt maschinelles Lernen, sowohl in seiner überwachten als auch in seiner unüberwachten Variante.\n\nNeben den traditionell am weitesten verbreiteten Clusteranalyseverfahren – formula_1-means und hierarchischen Clustern – werden bei Clusterverfahren auch selbstorganisierende Karten verwendet. Außerdem greifen mehr und mehr Verfahren auf Fuzzylogik zurück.\n\nSehr häufig werden beim Text Mining formula_1-means Cluster gebildet. Der zu diesen Clustern gehörende Algorithmus zielt darauf ab, die Summe der euklidischen Distanzen innerhalb und über alle Cluster zu minimieren. Hauptproblem ist dabei, die Anzahl der zu findenden Cluster zu bestimmen, ein Parameter, der durch den Analysten mit Hilfe seines Vorwissens festgelegt werden muss. Derartige Algorithmen sind sehr effizient, allerdings kann es vorkommen, dass nur lokale Optima gefunden werden.\n\nBei der ebenfalls populären hierarchischen Clusteranalyse werden Dokumente in einem hierarchischen Clusterbaum (\"siehe Abbildung\") ihrer Ähnlichkeit nach gruppiert. Dieses Verfahren ist deutlich rechenaufwändiger als das für formula_1-means Cluster. Theoretisch kann man dabei so vorgehen, dass man die Dokumentenmenge in sukzessiven Schritten teilt oder indem man jedes Dokument zunächst als einen eigenen Cluster auffasst und die ähnlichsten Cluster in der Folge schrittweise aggregiert. In der Praxis führt aber meist nur letzteres Vorgehen zu sinnvollen Ergebnissen. Neben den Runtimeproblemen ist eine weitere Schwäche die Tatsache, dass man für gute Ergebnisse bereits Hintergrundwissen über die zu erwartende Clusterstruktur benötigt. Wie auch bei allen anderen Methoden des Clustering muss letztendlich der menschliche Analyst entscheiden, ob die gefundenen Cluster Sinnstrukturen widerspiegeln.\n\nDer 1982 von Teuvo Kohonen erstmal entwickelte Ansatz der selbstorganisierenden Karten ist ein weiteres weit verbreitetes Konzept zur Clusterbildung im Text Mining. Dabei werden (in der Regel zweidimensionale) künstliche neuronale Netze angelegt. Diese verfügen über eine Eingabeebene, in der jedes zu klassifizierende Textdokument als multidimensionaler Vektor repräsentiert ist und dem ein Neuron als Zentrum zugeteilt wird, und über eine Ausgabeebene, in der die Neuronen gemäß der Reihenfolge des gewählten Distanzmaßes aktiviert werden.\n\nEs werden auch auf Fuzzylogik basierende Clustering-Algorithmen verwendet, da viele – insbesondere deiktische – Sprachentitäten nur vom menschlichen Leser adäquat decodiert werden können und so eine inhärente Unsicherheit bei der computeralgorithmischen Verarbeitung entsteht. Da sie dieser Tatsache Rechnung tragen, bieten Fuzzy Cluster so in der Regel überdurchschnittlich gute Ergebnisse. Typischerweise wird dabei auf Fuzzy C-Means zurückgegriffen. Andere Anwendungen dieser Art greifen auf Koreferenzcluster-Graphen zurück.\n\nEine große Zahl von Text-Mining-Verfahren ist vektorenbasiert. Typischerweise werden dabei die in den untersuchten Dokumenten vorkommenden \"terms\" in einer zweidimensionalen Matrix formula_4 repräsentiert, wobei \"t\" durch die Anzahl der \"terms\" und \"d\" durch die Anzahl der Dokumente definiert ist. Der Wert des Elements formula_5 wird dabei durch die Häufigkeit des \"terms\" formula_6 im Dokument formula_7 bestimmt, oft wird die Häufigkeitszahl dabei transformiert, meist, indem die in den Matrizen-Spalten stehenden Vektoren normiert werden, in dem sie durch ihren Betrag dividiert werden. Der so entstandene hoch-dimensionale Vektorraum wird in der Folge auf einen deutlich nieder-dimensionaleren Vektor abgebildet. Dabei spielt seit 1990 zunehmend die Latent Semantic Analysis (\"LSA\") eine bedeutende Rolle, die traditionell auf Singulärwertzerlegung zurückgreift. Probablistic Latent Semantic Analysis (\"PLSA\") ist dabei ein mehr statistisch formalisierter Ansatz, der auf der Latent Class Analysis basiert und zur Schätzung der Latenzklassenwahrscheinlichkeiten einen EM-Algorithmus verwendet.\n\nAlgorithmen, die auf LSA aufbauen sind allerdings sehr rechenintensiv: Ein normaler Desktop-Computer des Jahrgangs 2004 kann so kaum mehr als einige hunderttausend Dokumente analysieren. Geringfügig schlechtere, aber weniger rechenaufwändige Ergebnisse als LSA erzielen auf Kovarianzanalysen basierende Vektorraumverfahren.\n\nDie Auswertung von Beziehungen zwischen Dokumenten durch solcherartig reduzierte Matrizen ermöglicht es, Dokumente zu ermitteln, die sich auf denselben Sachverhalt beziehen, obwohl ihr Wortlaut verschieden ist. Auswertung von Beziehungen zwischen Termen in dieser Matrix ermöglicht es, assoziative Beziehungen zwischen Termen herzustellen, die oftmals semantischen Beziehungen entsprechen und in einer Ontologie repräsentiert werden können.\n\nEinen ungewöhnlich wichtigen und komplexen Teil des Text Mining nimmt die Präsentation der Ergebnisse ein. Darunter fallen sowohl Werkzeuge zum Browsing als auch zur Visualisierung der Ergebnisse. Oft werden die Ergebnisse dabei auf zweidimensionalen Karten präsentiert.\n\nEine Reihe von Anwendungsprogrammen für Text Mining existieren; oft sind diese auf bestimmte Wissensgebiete spezialisiert. In technischer Hinsicht lassen sich reine Text Miner, Erweiterungen existierender Software – zum Beispiel zum Data-Mining oder zur Inhaltsanalyse – und Programme, die nur Teilschritte oder -bereiche des Text Mining begleiten, unterscheiden.\n\n\n\n\n\n\n\n\n"}
{"id": "404357", "url": "https://de.wikipedia.org/wiki?curid=404357", "title": "Oric Atmos", "text": "Oric Atmos\n\nDer Oric Atmos war ein Heimcomputer und erschien 1984 hauptsächlich in Großbritannien und Frankreich.\nEr basierte auf dem 8-bit-Prozessor 6502 mit 1 MHz Taktfrequenz und verfügte über 16 KB bzw. 48/64 KB RAM (16 KB waren für das DOS reserviert). Es war das Nachfolgemodell des Oric-1, der schon im Jahre 1983 veröffentlicht wurde. Der große Unterschied der beiden war die Tastatur: Beim Oric-1 wurde eine Tastatur mit kleinen Knöpfen aus Hartplastik verwendet (ähnlich die eines Taschenrechners), der Oric Atmos hatte dagegen eine normale Schreibmaschinentastatur.\nBeim Nachfolger Telestrat sind besonders die beiden Modulschächte auffallend.\n\n\n"}
{"id": "405776", "url": "https://de.wikipedia.org/wiki?curid=405776", "title": "Cooperative Linux", "text": "Cooperative Linux\n\nCooperative Linux (kurz coLinux) ist eine Implementierung einer \"Cooperative Virtual Machine\" (CVM), die es erlaubt, Linux ohne zusätzliche Virtualisierung auf Windows zu betreiben.\n\ncoLinux basiert auf dem Konzept einer \"kooperierenden virtuellen Maschine\". Während traditionelle Virtualisierungslösungen wie VMware, Virtual PC und Xen unprivilegiert sind und daher unter der kompletten Kontrolle des Wirtssystem stehen, werden bei coLinux die Kernprozesse beider Betriebssysteme zu zwei großen Koroutinen, die volle Prozessor- und Adressraumkontrolle haben und eigenständig entscheiden, wann sie diese an ihren Partner abgeben.\n\nVorteil dieses Ansatzes ist eine gute Leistung bei vergleichsweise geringer Entwicklungszeit. Nachteilig ist, dass gegebenenfalls Instabilitäten und Sicherheitslücken des Gastsystems sich auf das Wirtssystem übertragen. Stürzt bei einer traditionellen Virtualisierungslösung das Gastsystem ab, kann es über das Wirtssystem beendet und neu gestartet werden. Bei einer Cooperative Virtual Machine kann auch das Wirtssystem abstürzen. Wegen seiner tiefen Eingriffe in das Wirtssystem muss coLinux mit Administratorrechten laufen und dementsprechend hat auch bösartige Software des Gastsystems vollen Zugriff auf das Wirtssystem.\n\nIm Moment wird coLinux hauptsächlich für Windows und Linux entwickelt, prinzipiell sollte der Code aber mit geringen Änderungen mit allen Betriebssystemen für die gleiche Hardwarearchitektur laufen, die Methoden zu Verfügung stellen, die es dem coLinux-Treiber ermöglichen, im CPL0-Mode (ring 0) zu laufen und Speicher zu reservieren. coLinux beansprucht keinen Zugriff auf physische Hardware, sondern arbeitet mit einer virtuellen Hardwareabstraktionsschicht, die Hardware-Zugriffe auf das Windows-API abbildet. Dasselbe Konzept, aber mit Abbildung auf die Linux-API wird bei User Mode Linux verwendet, um einen Linux-Kernel unter Linux zu betreiben.\n\ncoLinux steht als Quelltext und als Windows-Installer-Paket zur Verfügung. Darin enthalten sind ein virtueller Netzwerkkartentreiber und ein modifizierter Linux-Kernel. Zum Betrieb sind ferner eine Festplattenimage-Datei oder eine Linuxpartition auf der Festplatte erforderlich. Um X-Window-System-Programme zu benutzen, wird ein X-Server für Windows benötigt, z. B. Xming. coLinux bietet eine Möglichkeit, mit Linux zu arbeiten, ohne die Festplatte umpartitionieren zu müssen und sich dabei dem Risiko des Datenverlusts auszusetzen.\n\n\"andLinux\" ist eine auf Ubuntu basierende CoLinux-Distribution für Windows. Die native grafische Anzeige stellt Xming als X-Window-Server unter Windows (2000, XP, 2003, Vista, 7; 32 Bit) bereit; PulseAudio die Sound-Ausgabe. Das coLinux File System (CoFS) oder Samba realisieren den Dateizugriff durch den Midnight Commander oder Thunar. Die Linux-Version von Firefox erlaubt den sicheren Internet-Zugang. Auch Xfce oder KDE laufen direkt unter Windows. AndLinux fährt in der Standardeinstellung mit Windows hoch und ist sofort einsatzbereit.\n\nandLinux wird nicht mehr weiterentwickelt.\n\nUrsprünglich \"freetzLinux\" genannt, ist \"speedLinux\" eine angepasste, an Entwickler gerichtete CoLinux-Distribution für Windows. Die native grafische Anzeige stellt Xming als X Window-Server unter Windows (2000, XP, 2003, Vista, 7; 32 Bit) bereit; PulseAudio die Sound-Ausgabe. Das eigentliche Linux-System wird ab Version 3001 auch zum Installieren angeboten, es kann auch ein base.drv System Image aus einer anderen Distribution verwendet werden. Zur Auswahl an vorbereiten LINUX-Systemen stehen momentan Ubuntu 9.04 und Ubuntu 12.04 Develop in zwei Versionen.\nDer Funktionsumfang ist dann ähnlich wie bei andLinux, jedoch angepasst für die Verwendung von Freetz und speed-to-fritz. Es ist wie andLinux nutzbar. Bei Problemen mit andlinux ist es immer einen Versuch wert, da sich einige wenige Teile doch unterscheiden. Xming-Version und coLinux-Version sind unterschiedlich. Netzwerk-Bridge ist aktiviert damit Pings und FTP-Transfer funktionieren.\n\n"}
{"id": "408183", "url": "https://de.wikipedia.org/wiki?curid=408183", "title": "Korg M1", "text": "Korg M1\n\nDer Korg M1 ist ein digitaler Synthesizer des Typs „Workstation“ und wurde zwischen 1988 und 1994 gebaut.\n\nDer Korg M1 ist neben dem Yamaha DX7 der am meisten gebaute Synthesizer und war bei seiner Präsentation der erste eines neuen Typs von erschwinglichen Synthesizern, die man als Music Workstation bezeichnet. Bei diesem Konzept befinden sich alle Funktionen, die für eine einfache Musikproduktion benötigt werden, in einem einzigen Gerät. Im Falle des M1 sind das Sample-ROM-basierter Synthesizer, Effektgerät, Drumcomputer und 8-Spur-MIDI-Sequenzer. Zur Klangerzeugung wird die vom Hersteller so bezeichnete AI-Synthese eingesetzt. Es steht ein Vorrat von 144 synthetischen und von Naturinstrumenten (= Klavier, Streichinstrumente, Gitarren usw.) stammenden gesampelte Wellenformen (4 MB) zur Verfügung, die nach dem Prinzip der subtraktiven Synthese weiterverarbeitet werden können: pro Stimme gibt es einen digitalen Tiefpassfilter (12 dB, ohne Resonanz), 3 vierstufige Hüllkurven und 2 einfache LFOs (hier MGs genannt) zur weiteren Klangformung. Zur klanglichen Verfeinerung lassen sich die so erzeugten Sounds mit Hilfe von zwei identischen Effektprozessoren mit je 33 Effektalgorithmen weiter bearbeiten und verfeinern. Der Korg M1 ist 16-stimmig polyphon und 8-fach multitimbral, es lassen sich somit bis zu acht unterschiedliche Klänge gleichzeitig wiedergeben.\n\nDie Rackversion des M1 ist der Korg M1R. Später entstand der M1R-EX mit erweitertem 8-MB-Wellenformenspeicher. Eine abgespeckte Version des M1R ist der Korg M3R, der mit einem geringeren Wellenformenspeicher von nur 2 MB auskommen muss, ein kleineres Display besitzt und auf einen eingebauten Sequencer verzichtet.\n\nDer M-Serie folgte 1989 die T-Serie, deren Vorrat an Wellenformen der M1R-EX-Variante entspricht und neben einer verbesserten Bedienung mit größerem Display um einige Features erweitert wurde. Die T-Serie (Total Workstation) bietet die Versionen T3 (61 Tasten), T2 (76 Tasten) sowie T1 mit hochwertigem Holzgehäuse und 88 gewichteten Tasten. Zusätzlich wurde ab 1993 der M1 als M1EX angeboten, der mit dem 8-MB-Wellenformspeicher der M1REX/T-Serie ausgestattet ist. Für „normale“ M1 wurde ein entsprechendes Expansion-Kit als Zusatzplatine zum Einbau angeboten.\n\nZu hören ist der Korg M1 in zahllosen zeitgenössischen Musikproduktionen, z. B. auf dem Song „No Ordinary Love“ von Sade. Sehr beliebt sind im House auch die Orgel- und Klavierklänge des M1. Ebenso ist der M1 in „Hiroshima“ von Sandra vertreten, erklingt bei „Innocent“ von Mike Oldfield und ist dort auch im zugehörigen Video zu sehen. Unter anderem verwendet auch die Band Europe diesen populären Synthesizer, dem man aufgrund seines recht charakteristischen Sounds mittlerweile einen gewissen Status als Neo-Klassiker unter den Synthesizern zubilligt.\n"}
{"id": "412105", "url": "https://de.wikipedia.org/wiki?curid=412105", "title": "L4Linux", "text": "L4Linux\n\nLLinux ist eine modifizierte Version des Betriebssystems Linux. LLinux läuft virtualisiert in einer L4-Mikrokernel-Umgebung, gleichberechtigt neben anderen µ-Kernelanwendungen. Es ist binärkompatibel zum normalen Linux/x86-Kern und kann somit mit jeder Linux-Distribution für die X86-Architektur benutzt werden.\n\nEntwickelt wurde LLinux, um innerhalb des Dresden Real-Time Operating System Project (DROPS) Echtzeit- und Time-Sharing-Anwendungen gleichzeitig auf einem Computer ausführen zu können.\n\nMit LLinux lässt sich ebenfalls eine virtualisierte Umgebung, ähnlich wie etwa mit Xen oder OpenVZ, herstellen. Hierbei muss jedoch beachtet werden, dass sich die Projekt-Zielsetzungen und Konzepte von L4 und Xen teils gravierend unterscheiden.\n\nDie Versionierung orientiert sich an der des Linux-Kernels.\n\nBei L4Android handelt es sich um eine Abspaltung von L4Linux, welche die Kernelmodifikationen von L4Linux und jene von Google für Android zusammenführt. Es handelt sich um ein Gemeinschaftsprojekt der Operating Systems Gruppe von der Technischen Universität Dresden und dem \"Chair for Security in Telecommunications\" der Technischen Universität Berlin.\n\n\n"}
{"id": "415938", "url": "https://de.wikipedia.org/wiki?curid=415938", "title": "PDP-10", "text": "PDP-10\n\nDie PDP-10 war die 36-Bit-Rechnerfamilie von DEC. Sie wurde Ende der 1960er-Jahre als abwärtskompatible Weiterentwicklung der PDP-6 auf den Markt gebracht und später als DECSystem-10 oder DECSystem-20 vermarktet. Die Systeme konnten sowohl von ihren Leistungsdaten als auch von ihren äußeren Abmessungen her als Mainframe eingestuft werden.\n\nInnerhalb der Produktreihe gab es vier verschiedene Zentraleinheiten (KA10, KI10, KL10 und KS10). Die Beendigung der Entwicklungsarbeiten an einer weiteren Zentraleinheit („Projekt Jupiter“) im Jahre 1983 war das erste Anzeichen für eine Einstellung der gesamten Produktreihe. \n\nBetriebssysteme waren u. a. TOPS-10 (Time Sharing Operating System 10, ein interaktives Multiuser-Betriebssystem), ITS, TENEX, TOPS-20 (auch als \"TWENEX\" bezeichnet), sowie das auf Basis von TOPS-10 selbstentwickelte Betriebssystem von Compuserve.\n\nPDP-10-Rechner waren besonders im akademischen Umfeld beliebt, weil sie anders als die vorherrschenden IBM- oder CDC-Anlagen anstatt für Batchverarbeitung hauptsächlich für den interaktiven Betrieb vorgesehen waren. Gut ausgebaute Systeme verfügten über 150 oder mehr angeschlossene Terminals, die im Time-Sharing-Betrieb gleichzeitig aktiv sein konnten. \n\nPDP-10en ließen sich unter TOPS-10, Version 7 und höher zu symmetrischen Multiprozessoranlagen von bis zu acht Rechnern zusammenschließen, die ununterbrochenen Betrieb trotz eventueller Hardwareausfälle ermöglichten.\n\nZur normalen Peripherieausrüstung zählten anfangs 16 KByte Kernspeicher, Wechselplattenlaufwerk, DEC-Tape Magnetbandsystem, Falt-Lochstreifen-Einheit, Magnetbandlaufwerk, Walzendrucker und mechanischer Fernschreiber. Zum Anschluss von Realzeit-Steuerungskomponenten war ein frei programmierbares Analog-Digital-Schnittstellensystem verfügbar. Die Laufwerke waren i. d. R. in einem Schrank mit Kühlung untergebracht. Die Kommunikation der interaktiven Nutzer erfolgte in den ersten Jahren ausschließlich über elektro-mechanische Fernschreiber (Typ Teletype) mit einer Geschwindigkeit von 110 Baud.\n\nIm Rahmen der Software zur Simulation historischer Computer SIMH existiert auch ein Modul, das die PDP-10 auf einem Windows- bzw. Unix-artigen-Rechner emuliert. Es wird die CPU KS10 nachgebildet. Mit Hilfe im Internet verfügbarer Kopien der Original-Magnetbänder von DEC kann ein lauffähiges TOPS-10- oder TOPS-20-System aufgebaut werden.\n\n\n"}
{"id": "416529", "url": "https://de.wikipedia.org/wiki?curid=416529", "title": "FileZilla", "text": "FileZilla\n\nFileZilla ist eine freie Server- und Client-Software zur Dateiübertragung mittels FTP und SFTP. FileZilla steht für die Betriebssysteme Windows, macOS und Linux zur Verfügung. Der Quelltext steht unter der GNU General Public License (GPL) und wird über SourceForge bereitgestellt.\n\nMit dem FileZilla Client kann der Benutzer sich mit einem FTP-/SFTP-Server verbinden und daraufhin Dateien hoch- und herunterladen. Zudem können Textdateien im lokalen Texteditor geöffnet werden. FileZilla überwacht daraufhin die geöffnete Datei und bietet bei Änderung das Hochladen der Datei an.\n\n\"FileZilla Portable\" ist eine portable Ausgabe des FileZilla-Clients. Sie ermöglicht den rechnerübergreifenden Einsatz mittels USB-Massenspeicher oder einem anderen tragbaren Datenträger und kann ohne Installation an verschiedenen Rechnern wie beispielsweise abwechselnd auf dem Laptop, Büro- und Heimcomputer eingesetzt werden.\n\nFileZilla Server ermöglicht den Betrieb eines FTP-Servers und ist ein separates Programm für Windows, das vom gleichen Projekt entwickelt wird. Neben einem einfachen Benutzer- und Gruppen-Management bietet die Software SSL-/TLS-Verschlüsselung, Up-/Downloadraten-Beschränkungen global und pro Benutzer sowie die Möglichkeit, Verbindungen auf bestimmte IP-Adressen sowohl auf Client- wie auf Serverseite einzuschränken. Ein sogenanntes Remote-Admin-Interface ermöglicht auch die Wartung von anderen Computern aus. Windows 7 (2008 R2) oder neuer wird benötigt, um FileZilla Server zu verwenden. Die aktuelle Version ist 0.9.60.2 vom 8. Februar 2017.\n\nAb der Version 2.2.23 basiert FileZilla auf Unicode, wodurch es nicht mehr mit den Windows-Versionen vor Windows XP kompatibel ist.\n\nMit der Version 3 ist FileZilla von Grund auf neu geschrieben worden und setzt auf wxWidgets.\n\nAb der Version 3.0.4 ist die Verwendung von FTP-Proxyservern möglich.\n\nBis Version 3.0.9 wurden Windows NT und 2000 unterstützt. Die vorkompilierten Dateien für Mac OS X setzen Mac OS X 10.5 (Leopard) voraus, 10.4 (Tiger) und älter erfordern eine manuelle Kompilierung.\n\nSourceForge hob FileZilla im November 2003 besonders hervor, indem das Programm zum Projekt des Monats gekürt wurde. Außerdem gewann FileZilla den \"SourceForge.net Community Choice Award\" in der Kategorie \"Netzwerk\". Seit Start des Projektes wurde die Software über 414 Millionen Mal bei SourceForge heruntergeladen.\n\nZeitweise enthielt der ausführbare FileZilla-Installer Adware von Drittanbietern, da FileZilla an einem Programm des Hosters SourceForge namens „DevShare“ teilnahm. Der Installer lud bei der Installation ungefragt Drittanbieter-Inhalte von fremden Websites herunter, über die weder SourceForge noch Filezilla-Entwickler Tim Kosse Kontrolle hatten, um zusätzliche Software Dritter zur Installation anzubieten.\n\nDer FileZilla-Client speicherte die Anmeldedaten (Benutzername und Passwort) bis Version 3.26 unverschlüsselt auf der Festplatte, sodass diese leicht durch Dritte oder durch Schadsoftware ausgespäht werden konnten. Dafür wurde FileZilla von Nutzern häufig kritisiert. Mit der Version 3.26 haben die Entwickler auf diese Kritik reagiert und das Programm mit einer Master-Passwort-Funktion nachgerüstet. Diese ist unter Einstellungen zu finden, muss vom Nutzer allerdings in Eigenregie aktiviert werden.\n\n\n"}
{"id": "417346", "url": "https://de.wikipedia.org/wiki?curid=417346", "title": "Urpm", "text": "Urpm\n\nurpm (User RedHat Package Manager) ist ein Paketmanagementsystem, das bei den Linux-Distributionen Mandriva und Mageia die installierte und zu installierende Software verwaltet.\n\nIn der Datei /etc/urpmi/urpmi.cfg werden die Paketquellen gespeichert. Diese Quellen können dabei sowohl Verzeichnisse, als auch CDs oder FTP-Server sein. Wird das Programm nun mit einem Paketnamen aufgerufen, durchsucht es zuerst alle Quellen, ob es Pakete mit dem Namen gibt. Wenn dies zutrifft, werden diese Pakete ausgegeben. Gibt es nur einen Treffer, so werden bei diesem die Abhängigkeiten überprüft und nachinstalliert. Danach wird das Programm selbst installiert.\n\n\nEs stehen mehrere Frontends zur Verfügung, mit denen urpmi verwaltet werden kann. RPMDrake ist in der Grundinstallation von Mandriva Linux bereits enthalten, weitere können nachinstalliert werden. Während RPMDrake und der Smart Package Manager direkt auf urpmi zurückgreifen, können durch APT-RPM auch für apt entwickelte Frontends verwendet werden (beispielsweise Synaptic).\n\n\n"}
{"id": "417533", "url": "https://de.wikipedia.org/wiki?curid=417533", "title": "Personal Internet Communicator", "text": "Personal Internet Communicator\n\nPersonal Internet Communicator (PIC) ist ein von der Firma AMD Ende Oktober 2004 der Öffentlichkeit vorgestellter, lüfterloser PC, mit welchem AMD bis 2015 etwa 50 Prozent der Erdbevölkerung ausstatten wollte. Der PC besaß einen 366 MHz starken Geode-Prozessor mit 1,1 Watt Leistungsaufnahme, eine 10 GB Festplatte, VGA-Anschluss und 4 USB-Ports.\nDie schwache Ausstattung rührt daher, dass das Gerät auf eine sehr niedrige Leistungsaufnahme ausgelegt war, da die Zielgruppe in den ärmsten Regionen der Welt angesiedelt ist. \nDas System war in ein sehr kompaktes und leichtes (3 Pfund) Gehäuse integriert, das besonders auch auf Widerstandsfähigkeit ausgelegt ist.\n\nAls Betriebssystem wurde Windows CE mit dem CE 2D Game Pack eingesetzt. Internet tauglich war es durch den Internet Explorer 6.0 und den Macromedia Flashplayer 6, Textverarbeitung und Tabellenkalkulation wurden von der Firma Softmaker beigesteuert. \nDas System sollte mit dem eigentlichen Rechner, Netzteil, Maus und Tastatur ausgeliefert werden.\n\nDie Herstellung wurde zum 1. Oktober 2006 eingestellt.\n\nIm September 2006 veröffentlichte eine Testgruppe des „Computer and Peripheral Testing Laboratory“, in Thailands Wissenschaftspark nahe der Bangkok University auf der thailändischen Wiki-Seite. des 100-Dollar-Laptop Projekts Informationen über die Lauffähigkeit der Linux Distribution Edubuntu in Verbindung mit der OLPC eigenen Software. Auf einer modifizierten Version mit 256 MB Arbeitsspeicher kann Edubuntu problemlos arbeiten.\nAllerdings ist zu berücksichtigen, dass die Software auf dem 100-Dollar-Laptop auf die ausschließliche Verwendung von Flashspeicher optimiert ist, was in der Folge den Energieverbrauch und den RAM-Bedarf erheblich senkt. Demnach ist ein multimediafähiges Laptopsystem auch ohne die oben genannte Speichererweiterung möglich.\n\nLaut AMD enthält der PIC folgende Komponenten.\n\nDer PIC hat Abmessungen von 140 × 216 × 64 mm, also nur geringfügig größer als die 3.5\"-Festplatte; er wiegt etwa 3 Pfund.\n\n"}
{"id": "418634", "url": "https://de.wikipedia.org/wiki?curid=418634", "title": "Interlacing (Grafiken)", "text": "Interlacing (Grafiken)\n\nAls ' oder ' (englische Aussprache []) wird ein Speicherverfahren für Rastergrafiken bezeichnet, welches beim Laden einer Grafik den schnellen Aufbau eines Übersicht- oder Vorschaubildes (in der vollen Bildgröße) ermöglicht.\n\nUrsprünglich wurde das \"Interlace\"-Verfahren dadurch erreicht, dass am Anfang der automatisch bearbeiteten Grafikdatei nur jede achte Bildzeile und danach die fehlenden Zeilen gespeichert wurden. Dies ähnelt dem Zeilensprungverfahren bei der Darstellung von Fernsehbildern, das im Englischen ebenfalls als \"\" bezeichnet wird. Bei neueren Verfahren (siehe Adam7) wird eine zweidimensionale Auswahl der Bildpunkte abgespeichert.\n\nBeim langsamen Laden eines Bildes, etwa über eine Verbindung mit geringer Datenübertragungsrate, hat ein Betrachter den Eindruck, dass das Bild während des Aufbaus immer schärfer und detailreicher wird. Vorteilhaft ist diese Methode des Abspeicherns, wenn die Ladezeit eines Bildes groß ist. Allerdings wird die Grafikdatei dadurch zusätzlich vergrößert.\n\nDie Grafikformate GIF, PNG, Kodak Photo CD und Progressives JPEG unterstützen Interlacing.\n\nZu beachten ist, dass dieses Verfahren bei JPEG \"progressiv\" genannt wird, obwohl im Englischen die beiden Begriffe ' (für das Vollbildverfahren) und ' (für das Zeilensprungverfahren) Gegensätze sind, wenn damit die Verfahren des Bildaufbaus bei der Fernsehtechnik gemeint sind.\n\n"}
{"id": "419769", "url": "https://de.wikipedia.org/wiki?curid=419769", "title": "Blue Gene", "text": "Blue Gene\n\nBlue Gene ist ein Projekt zum Entwurf und Bau einer High-End-Computertechnik. Diese soll laut IBM sowohl zur Erforschung der Grenzen des Supercomputing in der Computerarchitektur, zur Entwicklung der für die Programmierung und Kontrolle massiv paralleler Systeme nötigen Software und zur Nutzung von Rechenkraft für ein besseres Verständnis biologischer Prozesse wie etwa der Proteinfaltung dienen. Letzteres war auch für die Namensgebung verantwortlich.\n\nIm Dezember 1999 kündigte IBM ein auf fünf Jahre angelegtes Programm an, einen massiv parallelen Computer zu bauen, der bei der Erforschung biomolekularer Phänomene wie der Proteinfaltung helfen soll. Zielvorgabe war dabei, Geschwindigkeiten im Peta-FLOPS-Bereich zu erreichen.\n\nEs handelt sich um ein kooperatives Projekt zwischen dem US-Energieministerium (welches das Projekt teilweise auch finanziert), der Industrie (insbesondere IBM), und den Hochschulen. In der Entwicklung befinden sich fünf Blue Gene-Projekte, darunter Blue Gene/L, Blue Gene/P und Blue Gene/Q (siehe Advanced Simulation and Computing Program).\n\nAls erste Architektur war Blue Gene/L vorgesehen. Die Vorgaben lagen bei einem System mit einer Spitzenleistung von 360 TFLOPS auf 65.536 Nodes und Fertigstellung 2004/2005. Die darauffolgenden Maschinen sollen bis zu 1000 TFLOPS (Blue Gene/P, 2006/2007) beziehungsweise 3000 TFLOPS (Blue Gene/Q, 2007/2008) erreichen. Die Dauerleistung dieser Nachfolgesysteme von Blue Gene/L soll bei 300 TFLOPS beziehungsweise 1000 TFLOPS liegen.\n\nZu den Chef-Architekten des Projekts bei IBM zählten Monty Denneau und Alan Gara.\n\nBei Blue Gene/L handelt es sich um eine Familie sehr gut skalierbarer Supercomputer. Das Projekt wird von IBM gemeinsam mit dem Lawrence Livermore National Laboratory finanziert.\n\nDie Architektur besteht aus einem Basisbaustein (Knoten oder Compute-Chip), der immer wieder wiederholt werden kann, ohne dass Flaschenhälse entstehen. Jeder Knoten des BG/L besteht aus einem ASIC mit zugehörigem DDR-SDRAM-Speicher. Jeder ASIC wiederum enthält zwei 0,7 GHz PowerPC Embedded 440 Prozessorkerne, zwei „Double Hummer“ FPU, ein Cachesubsystem und ein Kommunikationssubsystem.\n\nDie doppelten TFlops-Raten (2,8 bzw. 5,6 TFLOPS) auf verschiedenen Zeichnungen im Netz rühren von der Tatsache her, dass ein ASIC mit zwei Prozessoren in zwei Modi betrieben werden kann, welche entweder beide Prozessoren für Rechenaufgaben verwenden, oder nur einen für Rechenaufgaben und den anderen als Coprozessor für Kommunikationsaufgaben. Für die Kommunikation zwischen den Prozessoren steht ein Hochgeschwindigkeitsnetzwerk mit einer 3D-Torus-Topologie, sowie ein hierarchisches Netzwerk für kollektive Operationen zur Verfügung.\n\nDer Zugriff auf das Torus-Netzwerk erfolgt über speicher-gemappte Netzwerkadapter, um ähnlich wie bei InfiniBand sehr niedrige Latenzzeiten zu erzielen. Für die Kommunikation wurde eine modifizierte MPICH2-Implementierung entwickelt. Auf den Rechenknoten läuft ein speziell hierfür programmierter, sehr kleiner POSIX-Kernel, welcher kein Multitasking unterstützt – das laufende Programm ist also der einzige Prozess auf dem System.\n\nIn der Ausgabe November 2004 der TOP500-Liste übernahm das noch im Aufbau befindliche System Blue Gene/L am Lawrence Livermore National Laboratory mit 16 Racks (16.384 Knoten, entspricht 32.768 Prozessoren) den Spitzenplatz. Seitdem wurde es schrittweise ausgebaut und erreichte am 27. Oktober 2005 mit 65.536 Knoten über 280 TFLOPS, was ihm die Führung in der TOP500 11/2005 einbrachte. Zwar war diese Ausbaustufe ursprünglich als Endausbau deklariert worden, er wurde 2007 jedoch noch einmal erweitert und erbringt seitdem mit 212.992 Prozessoren in 104 Racks über 478 TFLOPS. Damit war er Mitte 2008 das viertschnellste System weltweit, befindet sich mittlerweile jedoch nur noch auf dem 14. Platz.\n\nDie Architektur taugt jedoch auch für andere Installationen wie den Blue Gene Watson (BGW) am IBM-eigenen Thomas J. Watson Research Center (Platz 98 in der TOP500 6/2011), JUGENE (Jülich Blue Gene am Forschungszentrum Jülich, Juni 2011 mit Platz 12 sogar vor seinem Vorgänger) und sechs weiteren Einträgen in den Top 100. Diese fallen alle unter die Bezeichnung \"eServer Blue Gene Solution\".\n\nDie Blue Gene/P-Serie wurde erstmals im Juni 2007 auf der ISC in Dresden vorgestellt.\nZu den Änderungen gegenüber BG/L zählen die Verwendung von mit 850 MHz getakteten PowerPC 450 Kernen von denen jetzt vier in einem Knoten enthalten sind. Auf jeder Compute-Card sitzt jetzt zwar nur noch ein statt zweier solcher Knoten, jedoch enthält eine Node-Card als nächstgrößere Einheit 32 statt 16 solcher Compute-Cards.\n\nAnsonsten sind die Baueinheiten gleich geblieben und ein Rack enthält somit doppelt so viele Prozessoren wie ein BG/L. Bei einer zur Taktratenerhöhung parallelen Leistungssteigerung jedes Prozessors von rund 21 % (jedenfalls beim LINPACK) leistet jedes Rack nun 14 statt 5,6 TFLOPS (jeweils R). Die Speicherbandbreite wuchs im gleichen Maße, die Bandbreite des Torus-Netzwerks wurde von 2,1 GB/s auf 5,1 GB/s mehr als verdoppelt und die Latenzzeiten halbiert. Der Energiebedarf hat sich dabei laut Hersteller nur um 35 % erhöht.\nFür ein aus 72 Racks bestehendes System, das die Peta-FLOPS-Grenze erreichen soll, sind das ca. 2,3 Megawatt.\n\nEine der ersten Auslieferungen ging ins Forschungszentrum Jülich, wird dort unter dem Namen JUGENE betrieben und stand mit 180 TFLOPS in der TOP500-Liste Ende 2008 auf Platz elf. Im November 2008 waren sieben Blue Gene/P-Systeme unter den 100 weltweit schnellsten Systemen vertreten.\n\nAm 26. Mai 2009 wurde eine verbesserte Version von JUGENE (\"Jülich Blue Gene\") eingeweiht, bei der die Anzahl der Prozessoren von 65.536 auf 294.912 erhöht wurde und damit eine Spitzenleistung von 1 Petaflops erreicht wird. Dieser Rechner war damit 2012 einer der schnellsten Rechner in Europa und belegte in der TOP500-Liste vom November 2011 den 13. Platz unter den schnellsten Superrechnern weltweit.\n\nDas neueste Supercomputer-Design der Reihe, Blue Gene/Q, zielte darauf ab, 20 Petaflops im Zeitrahmen bis 2011 zu erreichen. Es ist konzipiert als weitere Verbesserung und Erweiterung der Blue Gene/L- und P-Architekturen mit einer höheren Taktfrequenz bei wesentlich verbesserter Energieeffizienz. Blue Gene/Q weist eine vergleichbare Anzahl von Knoten, aber 16 anstatt 4 Kerne pro Knoten auf (neuer entwickelte POWER CPU A2).\n\nDie Referenzinstallation eines Blue Gene/Q-Systems namens IBM Sequoia erfolgte am Lawrence Livermore National Laboratory im Jahr 2011 als Teil des „Advanced Simulation and Computing Program“; es dient nuklearen Simulationen und anderer fortgeschrittener wissenschaftlicher Forschung.\n\nEin Blue Gene/Q-System namens Mira wurde Anfang 2012 am Argonne National Laboratory installiert. Es besteht aus ca. 50.000 Rechenknoten (16 Rechenkerne pro Knoten), 70 PByte Plattenspeicher (mit 470 GByte/s I/O-Bandbreite) und wird mit Wasser gekühlt.\n\nEbenfalls 2012 ging im Rechenzentrum von CINECA bei Bologna FERMI in Betrieb, eine Installation mit 10.240 Power A2-Sockel mit je 16 Kernen.\n\nNeuere IBM Supercomputer tragen nicht mehr den Namen Blue Gene. Sie basieren auf einer wesentlich weiterentwickelten POWER Prozessor Plattform und sind in der neusten Version (2018) beispielsweise am Lawrence Livermore National Laboratory und am Oak Ridge National Laboratory im Probebetrieb.\n\nNeben klassischen Supercomputern hat IBM mit der Entwicklung und dem Bau neuronaler Supercomputer mit extrem niedrigem Stromverbrauch begonnen.\n\n"}
{"id": "420434", "url": "https://de.wikipedia.org/wiki?curid=420434", "title": "Conectiva", "text": "Conectiva\n\nConectiva war eine Firma und eine gleichnamige Linux-Distribution. Nach einer Fusion mit der französischen Firma Mandrake 2005 wurde die neue gemeinschaftliche Firma Mandriva genannt.\n\nConectiva wurde am 28. August 1995 von Arnaldo Carvalho de Melo gegründet. Es hat seinen Sitz in Curitiba, Paraná (Brasilien). Weitere Niederlassungen befinden sich in Manaus und São Paulo. Zurzeit beschäftigt die Firma ca. 60 Mitarbeiter.\n\nIm Finanzjahr 2004 konnte das Unternehmen einen Umsatz in Höhe von 1,7 Millionen Euro erwirtschaften und erreichte während des zweiten Halbjahres laut eigenen Angaben die Gewinnschwelle. Zu den bekannteren von Conectiva initiierten und finanzierten Softwareprojekten gehören die Anpassung von APT zur Nutzung von RPM-Paketen sowie der Smart Package Manager. Die Linux-Distribution wurde auf der Basis von Red Hat Linux entwickelt und ist in Lateinamerika verbreitet. Bekannt wurde Conectiva in Deutschland durch seinen Beitritt zu United Linux.\n\nAm 24. Februar 2005 wurde bekannt gegeben, dass Conectiva von dem französischen Linux-Distributor Mandrake für 1,79 Millionen Euro aufgekauft wurde. Durch diese Fusion wurde auch ein neuer Name gefunden: Mandriva. Als Ziel der Übernahme wurde vor allen Dingen eine Erweiterung des Bereiches Forschung und Entwicklung genannt. Conectiva verliert damit den Status einer eigenen Distribution und kann als eingestellt betrachtet werden.\n\nDie „Crystal“-Icons für KDE, die von dem Conectiva-Mitarbeiter Esveraldo Coelho gestaltet wurden, und die Portierung „apt4rpm“ der Paketverwaltung von Debian GNU/Linux, wurden von Conectiva entwickelt. Ferner wurde von Conectiva die Grafische Benutzeroberfläche Synaptic entwickelt. Conectiva beschäftigte u. a. Rik van Riel und den Kernel-Maintainer Marcelo Tosatti.\n\nConectiva war eine Linux-Distribution für den lateinamerikanischen Markt. Sie entwickelten eine Reihe von Produkten und zusätzlichen Dienstleistungen, die auf die Nachfrage von Open-Source-Werkzeugen zielen, so in etwa wie Literatur, Handbücher, zusätzliche Software wie Linux-Zusatzprogramme und eingebettete Systeme, OEM-Programme, und das „Revista do Linux“ (Linux-Magazin). Ebenso bot Conectiva Beratung, Schulung und technischen Support in ganz Lateinamerika durch ihre eigenen Servicezentren und zertifizierte Partner an.\n\nConectiva entwickelte auch im Bereich des Linux-Kernels, der Hochverfügbarkeit von Linux und auf dem Gebiet der Treiber. Ferner: XFree86, Netzwerkprotokolle, Firewall, Linux-Cluster, Performanceanalyse und -optimierung, Datei- und Ressourcenmanagement. Seit seiner Gründung hat Conectiva Lösungen für den Unternehmensbereich entwickelte und besaß jetzt eine breite Produktpalette, die unter anderem Software für Geldautomaten und Kassen in Finanzinstituten, infrastrukturelle Lösungen für öffentliche Auftraggeber sowie ein komplettes Linux-Angebot aufwies, das durch große Handelsketten in Brasilien vertrieben wurde.\n\n"}
{"id": "420614", "url": "https://de.wikipedia.org/wiki?curid=420614", "title": "True Color", "text": "True Color\n\nTrue Color (engl. für \"Echtfarben\") ist ein Begriff aus der Computertechnik (Grafikkarten) und bezeichnet eine Farbtiefe von 24 Bit (3×8 Bit, entspricht 2 ≈ 16,78 Millionen Farben). Bilder dieser Farbtiefe erwecken beim menschlichen Betrachter einen natürlichen Eindruck.\n\nDie Darstellung erfolgt im RGB-Farbraum mit je acht Bit (256 Abstufungen) für den Rot-, Grün- und Blauanteil. Nebeneinanderliegende Farben, die nur um eine Intensitätstufe je Farbanteil voneinander abweichen, können vom menschlichen Auge nur schwer unterschieden werden. Bei räumlich ausgedehnten Farbverläufen zwischen sehr ähnlichen Farben sind allerdings Abstufungen mehr oder weniger deutlich zu erkennen.\n\nTrue Color wird oft auch mit einer Farbtiefe von 32 Bit verwendet. Dabei gehen aber auch nur 24 Bit in die eigentliche Bilddarstellung ein, die übrigen 8 Bit dienen entweder als Alphakanal (bei Grafikdateien) oder werden gar nicht verwendet (bei der eigentlichen Bildschirmdarstellung). Diese Anordnung wird häufiger gewählt als die mit 24 Bit, weil 32 als Zweierpotenz eine einfachere Adressierungslogik ermöglicht – alle Rot-Farbwerte befinden sich an durch 4 teilbaren Speicheradressen, die durch Bitverschiebung errechnet werden können. Bei durch 3 teilbaren Adressen, wie sie beim echten 24-Bit-Modus benutzt werden, muss eine langsamere Multiplikation ausgeführt werden und die Speicherzugriffe werden durch Misalignment langsamer.\n\nEine weitere Farbtiefe, die nur 15 oder 16 Bit zur Kodierung der Farbinformationen verwendet, ist High Color. Deep Color wiederum verwendet 10 Bit pro Farbkanal und wird, geeignete Hardware vorausgesetzt, von Windows 7 bzw. OS X El Capitan und neueren Betriebssystemen unterstützt.\n"}
{"id": "421447", "url": "https://de.wikipedia.org/wiki?curid=421447", "title": "AviSynth", "text": "AviSynth\n\nAviSynth ist ein freier Frameserver. Während die gegenwärtig stabile Version 2.6 ausschließlich für Microsoft Windows geschrieben wurde, sollte von Version 3.0 – welche sich im pre-Alpha-Stadium befindet – auch ein Linux-Port erscheinen. Die Entwicklung kam jedoch gänzlich zum Erliegen.\n\n\"AviSynth\" verfügt über eine eigene Skriptsprache und entbehrt dabei einer grafischen Benutzeroberfläche. Die \"AviSynth-Skripte\" mit der Dateiendung „.AVS“ beinhalten Klartext, sie können somit mit einem beliebigen Texteditor bearbeitet werden. Mit Hilfe dieser Skriptsprache ist es dem Benutzer mit vergleichsweise wenig Aufwand möglich, eigene Filter zu erstellen. Zusätzlich finden sich auf der Internetpräsenz des Projektes von Benutzern vorgefertigte Skripte. Ferner kann der Funktionsumfang von \"AviSynth\" mit Hilfe von Plug-ins erweitert werden.\n\n\"AviSynth\" fungiert als \"Frameserver\", welcher Dateien einliest und den Datenstrom an ein anderes Programm mittels einer virtuellen Datei weiterreicht. Wird eine AVS-Skript-Datei mit einem unterstützenden Videobearbeitungsprogramm (beispielsweise VirtualDub) geöffnet, so verhält sich diese dem Zielprogramm gegenüber wie eine AVI-Datei, welche eine rohe Video- und Audiospur beinhalten würde. Auf diese Weise können auch Formate geöffnet werden, welche vom Zielprogramm nicht unterstützt werden oder vorher die Installation eines Codecs benötigen würden. Mit Hilfe einer Reihe von integrierten sowie nachrüstbaren Filtern kann das Bild- und Tonmaterial bereits von \"AviSynth\" bearbeitet werden, bevor es weitergereicht wird. Da keine Komprimierung stattfindet, kommt es nicht zu Generationsverlust.\n\nMitte 2012 startete der Programmierer Fredrik Mellbin das Projekt VapourSynth, welches als moderne Neuauflage von AviSynth gedacht ist. Im Gegensatz zum Vorbild ist VapourSynth plattformunabhängig und beherrscht Multithreading. Die eigene Skriptsprache von AviSynth kommt jedoch nicht mehr zum Einsatz, stattdessen wird Python verwendet.\n\n"}
{"id": "421496", "url": "https://de.wikipedia.org/wiki?curid=421496", "title": "Cmd.exe", "text": "Cmd.exe\n\ncmd.exe (offiziell auch Windows-Eingabeaufforderung genannt) ist die Betriebssystem-Shell von OS/2 und ReactOS sowie der Windows-NT-Linie. In der Shell können DOS-Kommandozeilenbefehle verarbeitet und BAT-Dateien ausgeführt werden (Stapelverarbeitung).\n\ncmd.exe öffnet ein Fenster mit Kommandozeile und Eingabeaufforderung, in dem man Anweisungen nicht grafisch durch die Maus übermittelt, sondern diese direkt über die Tastatur eingibt. Mit diesen können beispielsweise Dateien kopiert, verschoben oder entfernt werden. Die Syntax der Befehle entspricht im Grundsatz der unter MS-DOS/PC DOS mit dessen Kommandozeileninterpreter COMMAND.COM, der jedoch um zahlreiche Funktionen erweitert wurde. Einige interne Befehle sind verbessert worden, beispielsweise \"for\" und \"if\". Außer den internen Befehlen stehen dem Anwender noch zahlreiche weitere Programme, als \"Exe-Dateien\" oder \"Shell-Skripte\", zur Verfügung.\n\nWenn cmd.exe über eine Dateiverknüpfung gestartet wird, ist das Fenster mit dem Namen der Verknüpfung betitelt. Der Name der im Startmenü vorgesehenen Verknüpfung lautet \"Eingabeaufforderung\".\n\nEs handelt sich bei cmd.exe um eine native Win32-Anwendung, daher ist der Name „DOS-Eingabeaufforderung“ irreführend: Es wird zwar eine Kommandozeile für MS-DOS-Befehle zur Verfügung gestellt, die selbst allerdings nicht unter MS-DOS als Betriebssystem läuft.\n\nCmd.exe hat eine Reihe von Vorteilen gegenüber COMMAND.COM, so erzeugt das Zeichen codice_1 in einem Befehl zwei Prozesse sowie eine Pipe dazwischen (wie auf einem Unix-Betriebssystem), und keine Temporärdatei, in die zunächst die gesamte Ausgabe des Befehls links von der Pipe umgeleitet wird.\n\nMit Windows 2000 wurden die sogenannten \"Befehlserweiterungen\" eingeführt, die zahlreiche neue Funktionen zur Kommandozeile hinzufügt:\n\nDiese Neuerungen lassen sich per Parameter deaktivieren, falls Kompatibilitätsprobleme mit älteren Stapelverarbeitungsdateien auftreten.\n\nDer Funktionsumfang von cmd.exe reicht für viele praktische Fälle nicht aus, insbesondere im Vergleich zu seinen Wettbewerbern aus dem UNIX-Umfeld. Microsoft reagierte mit einer kostenlosen Erweiterung namens \"Resource Kit Tools\", aktuell in der Version \"Windows Server 2003 Resource Kit Tools\". Auch wenn der Name Windows Server 2003 aufführt, so sind die meisten Erweiterungen auch in früheren Betriebssystemen lauffähig. Die zugehörigen Dokumentationen listen hier explizit auch Windows 2000 und Windows XP auf. Eine von vielen Funktionserweiterungen ist beispielsweise \"robocopy.exe\", ein Programm für cmd.exe zum Kopieren, Synchronisieren und Überwachen auf Veränderungen von Dateien oder ganzen Verzeichnissen (in Windows Vista, Windows 7 und Windows Server 2008 bereits ohne Resource Kit integriert).\n\n\n"}
{"id": "423149", "url": "https://de.wikipedia.org/wiki?curid=423149", "title": "Parsytec GC", "text": "Parsytec GC\n\nDer GC (für \"Giga Cube\" oder \"Grand Challenge\") ist ein Parallelrechner der deutschen Firma Parsytec und wurde Anfang der 1990er-Jahre hergestellt.\n\nAls Prozessor wurden Transputer T-805 (30 MHz) der Firma Inmos eingesetzt. Die Rechenleistung eines Prozessors betrug 4,4 Megaflops. Jedem Prozessor stand 1 Megabyte lokaler Hauptspeicher zur Verfügung.\nEine vom Hersteller angekündigte Version des GC mit Inmos-T-9000-Transputern konnte niemals realisiert werden, da der Prozessor nicht veröffentlicht wurde.\n\nDie Netzwerkstruktur war ein zweidimensionales Gitter. Die Kommunikationsgeschwindigkeit lag bei 20 MBit/s.\n\nFür die Zeit außergewöhnlich war das modulare Konzept des GCs. Ein Modul enthielt vier Cluster mit jeweils 16 Transputern und eine eigene Stromversorgung, I/O- und Kommunikationsanschlüsse. Durch Kombination von Modulen konnten theoretisch bis zu 16.384 Prozessoren zu einem sehr leistungsfähigen System miteinander verschaltet werden.\n\nDie Bezeichnung \"GC-x\" gibt die Größe des Systems an. Ein \"GC-1\" besitzt 64 Prozessoren, ein \"GC-2\" 256, ein \"GC-3\" 1024, ein \"GC-4\" 4096 und ein \"GC-5\" 16.384 Prozessoren. Während die kleineren Versionen bis zum GC-3 luftgekühlt waren, sollte für die größeren Wasserkühlung eingesetzt werden.\n\nAls Betriebssystem wurde PARIX eingesetzt.\n\nDie Stromaufnahme eines Systems mit 1024 Prozessoren lag bei ca. 27 kW, das Gewicht bei fast einer Tonne. Der Kaufpreis betrug 1992 ca. 1,5 Millionen DM.\n\nDie beiden größten Installationen des GC hatten 1024 Prozessoren (16 Module mit jeweils 64 Transputern) und wurden an Rechenzentren der Universitäten Köln und Paderborn betrieben. Das Paderborner Exemplar des GC ist seit Oktober 2004 im Heinz Nixdorf MuseumsForum zu besichtigen (nicht mehr betriebsfähig).\n\nDer GC mit 1024 Prozessoren erreichte 1992 eine Platzierung in der TOP500-Liste der weltweit schnellsten Supercomputer-Installationen. Deutschlandweit erreichte er Platz 22 der schnellsten Rechner. \n"}
{"id": "424222", "url": "https://de.wikipedia.org/wiki?curid=424222", "title": "Red Flag Linux", "text": "Red Flag Linux\n\nRed Flag Linux ( Linux) ist eine auf chinesische Bedürfnisse abgestimmte Linux-Distribution, die vorerst im asiatischen Raum, später jedoch auch weltweit einen Ersatz für Microsoft Windows darstellen soll. Die Distribution wird unter anderem von der chinesischen Regierung gefördert.\n\nZum ersten Mal erschien Red Flag Linux im August 1999. Entwickelt wurde die Linux-Distribution vom Institut für Software an der chinesischen Akademie der Wissenschaft. Finanzielle Unterstützung kam vom Shanghai NewMargin Venture Capital, welches wiederum Eigentum der chinesischen Regierung ist. Der chinesische Staat unterstützt die Entwicklung und Verbreitung von Red Flag Linux somit seit Beginn auf finanzielle Weise, aber auch politisch auf dem Absatzmarkt Chinas. Weitere Partner des Red-Flag-Projekts waren zu Beginn die Firmen Compaq (wurde später von Hewlett-Packard erworben) und Beijing Founder Electronics.\n\nIm Jahr 2000 wurde dann die Firma Red Flag Software Co., Ltd. (oft einfach nur \"Red Flag\" genannt) gegründet, deren Firmensitz Peking ist. Das Unternehmen entwickelt seither Linuxsysteme und Programme für den Arbeitsplatzrechner- und Server-Einsatz.\n\nIm März 2001 berichtete Bloomberg News, dass CCIDNET Investment, ein Teil des Ministry of Information Industry, zum zweitgrößten Gesellschafter von Red Flag wurde.\n\nLaut gartner.com vergab die Regierung von Peking am 28. Dezember 2001 Aufträge für Büro- und Antiviren-Anwendungen sowie Betriebssoftware an sechs verschiedene lokale Software-Anbieter, darunter Red Flag, lehnte aber Microsoft, den siebten Bewerber, ab.\n\nSeit 2004 arbeiten China, Südkorea und Japan gemeinsam an Plänen um Red Flag Linux zu einer bedeutenden Windows-Alternative zu machen. Laut Medienberichten sollen sich unter anderem die Unternehmen NTT Data, Matsushita Electric, NEC, Hitachi und Fujitsu an den Plänen für die Entwicklung und Nutzung von Open-Source-Software beteiligen. Begründet wurde diese Suche nach einem Ersatz für Windows mit der unerwünschten Abhängigkeit von US-amerikanischen Firmen wie Microsoft, sowie Bedenken beim Schutz vor Angriffen durch Computerviren.\nEin Sprecher des japanischen Handelsministeriums ließ beispielsweise verlauten, dass man ein Open-Source-Betriebssystem entwickeln wolle, da man verstanden habe, dass „eine Marktbeherrschung durch ein einziges Unternehmen nicht gut ist“. Für Linux sprach zudem dessen Stabilität.\n\nIm Jahr 2006 hatte Red Flag Linux in China einen Marktanteil von über 80 % bei Linux-Desktops.\n\nSAP unterstützt einige Versionen seit März 2006 auf RedFlag Linux.\nSo sind SAP ERP 2004 SR1 ECC5.0 sowie CRM 4.0 auf WebAS 6.20 zur Installation freigegeben.\nFolgende Versionen sind erlaubt:\n\nFolgende Datenbanken sind zulässig:\n\nAm 17. Februar 2014 vermeldete die South China Morning Post, dass Red Flag Linux vor dem Aus stehe. Grund dafür sei Missmanagement der geringe Bekanntheitsgrad von Red Flag Linux, der sich gegen die große Konkurrenz wie Red Hat Enterprise Linux und SUSE Linux Enterprise Server nicht richtig durchsetzen konnte. Zudem soll die Firma finanzielle Probleme mit Lohn- und Rechnungszahlung haben. Daher sollen die Verträge zu Donnerstag gekündigt werden und das Projekt eingestellt werden.\n\nZiel von Red Flag Linux soll es nach Vorstellung von Matei Mihalca sein, die marktbeherrschende Stellung von Microsoft Windows auf dem chinesischen Computer-Markt zu verringern, was am besten durch „vollständige Transparenz beim zugrunde liegenden Code“ erreicht werden kann. Auch Kosteneinsparungen spielen eine Rolle.\n\nDie Firma Red Flag Software konzentriert sich auf die Entwicklung und den Vertrieb von Linux-basierten Betriebssystemen und der dazugehörigen Anwendungssoftware für verschiedenste Rechner-Plattformen, um der wachsenden Nachfrage chinesischer Technologie-Anwender gerecht zu werden.\n\nRed-Flag nutzt die Office-Suite StarOffice von Sun Microsystems und verwendet als Desktop-System K Desktop Environment.\n\nDas Red-Flag-Projekt schloss mit Trolltech ein Abkommen, um eine Plattform für Embedded-Linux-Systeme zu schaffen, welche in China hergestellt werden soll. Die Vorgehensweise, die hinter Red Flag Linux steht, führt dazu, dass in den Geschäften neben anderen Linux-Distributionen vor allem auch Red Flag Linux angeboten wird. So werden in China zu einem großen Teil auch PCs mit bereits vorinstalliertem Linux verkauft, während in Europa bei einem PC-Kauf meist Windows als Betriebssystem enthalten ist.\n\nRed Flag arbeitet auf dem chinesischen Markt mit Firmen wie Hewlett-Packard, Oracle, IBM, Dell, Intel sowie BEA Systems, SGI, Sybase, Haier, Great Wall und TCL zusammen. Mit Hilfe von Hewlett-Packard will Red Flag Software in naher Zukunft auch den Weltmarkt erschließen. Da Red Hat nach China expandieren will, kam es 2004 auch zwischen Red Flag und Red Hat zu einer Partnerschaft.\n\nFür die verschiedenen Einsatzgebiete gibt es unterschiedliche Varianten von Red Flag Linux:\n\n\n\n\n"}
{"id": "425354", "url": "https://de.wikipedia.org/wiki?curid=425354", "title": "Dia (Software)", "text": "Dia (Software)\n\nDia ist ein Diagramm-, Zeichen- und Illustrationsprogramm sowie UML-Werkzeug, welches zum Gnome-Projekt gehört und unter der GNU General Public License steht.\n\nDia dient dazu, verschiedene Abläufe und Strukturen – vornehmlich in Form von Diagrammen – visuell darzustellen.\nHierzu können verschiedene Vorlagen mit passenden Werkzeugen und Symbolen, beispielsweise für Programmablaufpläne und Geschäftsprozesse, verwendet und diese per Drag and Drop auf ein Dokument gezogen werden.\nDie Funktionalität von Dia ist bedingt mit dem proprietären Programm Visio vergleichbar.\n\nFür die Erstellung strukturierter Diagramme stehen Standardobjekte zur Verfügung, mit deren Hilfe komplexe Strukturen aufgebaut werden können. So gibt es unter anderem Modi für Flussdiagramme (englisch \"Flowchart\"), Netzwerk- und UML-Diagramme. Es gibt jedoch auch einige generische Funktionen, mit denen einfache Zeichnungen erstellt werden können.\n\nDas Programm verfügt über eine Reihe von Ausgabeformaten, beispielsweise EPS, SVG, CGM und PNG. Die Diagrammtypen sind erweiterbar, da noch nicht unterstützte Objekttypen über XML-Dateien hinzugefügt werden können.\n\nDas programmeigene Dateiformat „.dia“ ist eine mit Gzip gepackte XML-Datei.\n\nIn der neuesten Version ist sowohl unter Windows als auch unter Linux die alte Mehr-Fenster Ansicht durch eine Ansicht mit nur einem Fenster ersetzt worden.\n\nDas im Rahmen eines Gnome-Projektes realisierte Programm verwendet das GTK+ und ist dadurch auch auf Windows lauffähig.\n\n\n"}
{"id": "427561", "url": "https://de.wikipedia.org/wiki?curid=427561", "title": "Punktwolke", "text": "Punktwolke\n\nEine Punktwolke oder ein Punkthaufen () ist eine Menge von Punkten eines Vektorraums, die eine unorganisierte räumliche Struktur („Wolke“) aufweist. Eine Punktwolke ist durch die enthaltenen Punkte beschrieben, die jeweils durch ihre Raumkoordinaten erfasst sind. Punktwolken mit Georeferenzierung enthalten Punkte in einem erdbezogenen Koordinatensystem. Zu den Punkten können zusätzlich Attribute, wie z. B. geometrische Normalen, Farbwerte oder Messgenauigkeit, erfasst sein.\n\nDie Erzeugung kann grundsätzlich über Scanning-Verfahren (z. B. terrestrisches oder flugzeuggestütztes Laserscanning) oder photogrammetrische Verfahren erfolgen sowie allgemein mittels Abtastung von Objektoberflächen durch Systeme wie Koordinatenmessmaschinen oder tastende 3D-Scanner.\nOptische Scanner untergliedert man in Lasertechnologie, die nach dem Triangulationsprinzip arbeiten, und Normallicht-Scanner, die nach dem Streifenlichtverfahren („coded-light“) arbeiten. Einen zusammenfassenden Überblick über die Vielfalt und Leistungsfähigkeit aktueller optischer Scanning-Methoden und die Weiterverarbeitung der resultierenden 3D-Daten/Punktwolke gibt beispielsweise C. Teutsch.\nDurch die mehrfache Erfassung eines räumlichen Ausschnitts zu unterschiedlichen Zeitpunkten lässt sich ein vierdimensionales (zeitvariantes) diskretes räumliches Modell einer Umgebung aufbauen. Jeder Punkt der Wolke wird dabei zeitlich und räumlich (XYZ-Koordniaten) lokalisiert und kann in weiterer Folge auch georeferenziert werden.\n\nAufgrund des enormen Datenvolumens stellen sich Herausforderungen an die Speicherung hinsichtlich des Speicherplatzes und des effizienten Zugriffs auf einzelne Bereiche einer Punktwolke.\nZur Implementierung von Speicherverfahren kommen Multiresolutionsdatenstrukturen zum Einsatz: „Um die Daten effizient verarbeiten und in Echtzeit visualisieren zu können, werden in Software-Implementierungen Out-of-Core-Algorithmen und Level-of-Detail-Strukturen benötigt.“ Bekannte Umsetzungen erfolgen in Form von Octrees. Derzeit wird versucht, die Datenspeicherung von 3D-Punktwolken zu standardisieren, um ein Datenmanagement zu ermöglichen, dass mit anderen Disziplinen kompatibel ist. Neben der Kompatibilität sollen dadurch die großen Datenmengen leichter zu verwalten sein, und interaktive Forschungsansätze leichter ermöglicht bzw. dadurch gefördert werden.\n\nZur Visualisierung massiver Punktwolken sind Out-of-Core-Algorithmen erforderlich, die einen effizienten, auflösungsabhängigen Zugriff des 3D-Renderingsystems auf Punkte einer Punktwolke erlauben. Insbesondere \"Point-based Rendering\" ermöglicht eine differenzierte grafische Darstellung von Punktwolken, z. B. in für Punkte unterschiedlicher Kategorie (z. B. Fassadenpunkte, Dachpunkte, Vegetationspunkte, etc.). Aus Punktwolken mit ausreichend hoher Punktdichte können über 3D-Renderingverfahren kontinuierliche Oberflächen abgeleitet werden, um so eine möglichst geschlossene Visualisierung von Oberflächenbereichen zu erzielen.\n\nEs gibt viele Möglichkeiten um aus einer Punktwolke eine geschlossene 3D-Oberfläche zu erstellen. Einige Herangehensweisen, wie Delaunay-Triangulation, alpha shapes oder ball pivoting, bauen ein Netzwerk von Dreiecken über die Normalvektoren der einzelnen Punkte auf. Andere Herangehensweisen, wie z. B. der Marching-Cubes-Algorithmus, extrahieren ein Polygonnetz über Voxel-basierte Ansätze. Diese spielen vor allem für bildgebende Verfahren in der Medizin eine Rolle. Zu den aktuell bekanntesten open Source Visualisierungsprogrammen für 3D-Punktwolken zählen CloudCompare und MeshLab. Jedoch sind diese Anwendungen in gewissen Bereichen limitiert. Zwar können beide Programme Punktwolken darstellen und erleichtern so den Austausch und die Kommunikation über 3D-Daten, eine Bearbeitung der Punktwolke ist jedoch nur beschränkt möglich. Hinzu kommt, dass die zu verarbeitende Datenmenge für beide Programme limitiert ist.\n\nSeit 2011 ist die freie Programmbibliothek der Point Cloud Library (PCL) verfügbar. Diese bietet zahlreiche Algorithmen zur Verarbeitung n-dimensionaler Punkwolken und dreidimensionaler Geometrien. Die darin enthaltenen Module ermöglichen z. B. die Filterung, Registrierung, Segmentierung, Oberflächenrekonstruktion oder Visualisierung. Die PCL hat für die 3D-Bildverarbeitung einen ähnlichen Status wie OpenCV für die 2D-Bildverarbeitung.\n\nFür geomorphologischen Analysen werden vor allem digitale Höhenmodelle von 3D-Punktwolken abgeleitet. Dadurch werden eine Vielzahl von oberflächenbezogenen Analysen ermöglicht. Bodenerosionsprozesse wurden mittels terrestrischer Laserscanner untersucht. Um den Bodenabtrag zu quantifizieren wurden 3D-Punktwolken zu unterschiedlichen Zeitpunkten gemessen und miteinander verglichen. Dadurch lassen sich Aussagen darüber treffen, von wo Sedimente abgetragen werden bzw. wie diese in weiterer Folge verlagert werden. Im Bereich der Glaziologie werden mittels Punktwolken die Bewegungen und Veränderungen von Gletschern dokumentiert und untersucht. Darüber hinaus beschäftigen sich eine Reihe von fluvialen Forschungsansätzen bzw. Anwendungen mit der Analyse von Punktwolken. So können Veränderungen von Flussläufen über größere Zeiträume beobachtet werden.\n\nViele Entwicklungen von Anwendungen rund um 3D-Punktwolken stammen aus dem Bereich der Archäologie. Durch die Analyse von Oberflächenformen wird dabei auf vergangene Siedlungsstrukturen geschlossen. Dadurch können anthropogen genutzte Flächen erkannt werden und deren Anordnung und Organisation analysiert werden. Punktwolken ermöglichen es in diesem Zusammenhang, dass einerseits die Erdoberfläche visualisiert wird um so Archäologen die Möglichkeit zu bieten gezielt nach bestimmten Strukturen und deren Lage zu suchen, und andererseits bietet sich die Möglichkeit durch automatisierte Abläufe systematisch größere Flächen nach vorab definierten Mustern zu scannen. Eine weitere Einsatzmöglichkeit von 3D-Punktwolken in der Archäologie stellt die Modellierung von historischen Stätten dar.\n\nIn der Agrar- und Forstwirtschaft werden 3D-Punktwolken vor allem für Monitoring-Anwendungen herangezogen. Durch die Verwendung von LiDAR-Daten wird es möglich, großräumig agrarisch genutzte Flächen zu überwachen, ohne dabei direkt vor Ort sein zu müssen. Dadurch wird verhindert, dass durch die Präsenz des Menschen die Nutzpflanzen und deren Umgebung gestört oder zerstört wird. Darüber hinaus kann der Arbeitsaufwand deutlich vermindert werden, da die Agrarflächen großräumig überwacht werden und gezielt Einfluss genommen werden kann. Besonders im Bereich des Precision Farmings kommen punktwolkenbasierte Verfahren zum Einsatz. Hierbei können Aussagen über das Pflanzenwachstum durch die Analyse von aufgenommenen 3D-Punktwolken getroffen werden. Ziel der Analyse ist es, Bereiche einer landwirtschaftlichen Nutzfläche auszumachen, die besondere Wachstumsmuster zeigen, um diese in weiterer Folge dann individuell düngen zu können. Im Bereich der Forstwirtschaft wird z. B. der Zusammenhänge zwischen Baumgesundheit und Borkenkäferbefall untersucht. Aus den 3D-Puntwolkendaten können Unterschiede in den Baumkronenstrukturen bei gesunden bzw. befallenen Bäumen abgeleitet werden. Auf diese Weise werden gezielt befallene Bäume ausgemacht und in weiterer Folge behandelt.\n\nIm Bereich städtebaulicher und raumplanerischer Prozesse kommen Laserscanning-Daten verstärkt zum Einsatz. Mittels Computeralgorithmen ist es möglich, 3D-Punktwolken eines Areales in unterschiedliche Bereiche zu segmentieren. So kann zwischen Vegetation, Gebäuden und unbebauten Flächen unterschieden werden. Informationen aus solchen Analysen können in weiterer Folge bei stadtplanerischen Entscheidungen berücksichtigt werden. Die einzelnen Bereiche werden dabei aufgrund der Anordnung der gescannten Punkte innerhalb der Wolke erkannt. Erfasste Baumkronen zeichnen sich beispielsweise durch eine unregelmäßige Anordnung der Punkte aus wohingegen Gebäude deutliche lineare Strukturen aufweisen. Ein weiterer Algorithmus der regelmäßige Bereiche innerhalb einer Punktwolke filtert wurde dafür entwickelt, potentielle Standorte für Photovoltaikanlagen auszuweisen. Darüber hinaus ermöglichen Punktwolken detaillierte 3D-Modelle eines Stadtgebietes zu erzeugen. Um bauliche Veränderungen innerhalb eines Stadtgebietes erfassen zu können, werden Punktwolken zu unterschiedlichen Zeitpunkten erfasst und die Punktabstände zwischen den einzelnen Erhebungen verglichen.\n\nIm Naturgefahren- und Risikomanagement ermöglichen 3D-Punktwolken eine detaillierte Analyse von Naturereignissen sowie eine gezielte Überwachung von potentiellen Gefahrenbereichen. Dadurch wird die Optimierung von Frühwarnsystemen ermöglicht. Gravitative Massenbewegungen werden mittels Zeitreihen von 3D-Punktwolken überwacht um so Dynamiken frühzeitig zu erkennen und betroffene Personen warnen zu können. In steinschlaggefährdeten Gebieten können Bereiche mit Infrastruktur durch regelmäßiges Scanning und durch die Analyse der daraus generierten 3D-Punktwolken wichtige Informationen für lokales Risikomanagement gewonnen werden. Die Rauigkeit einer Oberfläche ist in der Analyse von Naturgefahren ein wichtiger Parameter. Anhand von 3D-Punktwolken lassen sich Aussagen über die Struktur von Oberflächen treffen. Dadurch wird es möglich, aufgrund der Bodenbeschaffenheit und deren potentielle Eigenschaften, Rückschlüsse auf mögliche Naturgefahren, wie etwa Hochwasser, Steinschlag oder Lawinen zu ziehen. Für Risikomanagementmaßnahmen ist es wichtig, das genaue Ausmaß des Ereignisses zu kennen, um richtig, rechtzeitig und ausreichend mit der Umsetzung von Schutzmaßnahmen zu reagieren. Die Gewinnung von 3D-Punktwolken durch Airborne Laserscanning hat im Vergleich zu traditionellen Methoden der Fernerkundung (z. B. Photogrammetrie) den Vorteil, dass durch das aktive Messsystem Vegetation durchdrungen werden kann und somit Bodenpunkte aufgezeichnet werden.\n\nEine Punktwolke wird verwendet:\n\nIm Bereich CAD werden Punktewolken verwendet, um eingescannte Designobjekte in das CAD-System einzulesen. Bei anspruchsvollen Formen (z. B. Automobilkarosserie) wird nicht selten ein maßstabsgetreues Lehmmodell (\"clay model\") erstellt. Mit Ziehmesser und anderen Handwerkzeugen werden die Formen aus einer Modelliermasse erstellt und danach eingescannt. Bei symmetrischen Bauteilen (z. B. Motorhaube) wird dabei nur eine Seite modelliert. Diese wird dann mit taktilen, messenden oder optischen Scannern eingescannt. Die dabei entstehende 3D-Geometrie besteht zuerst nur aus Punkten im Raum (XYZ-Koordinaten). Diese Punktewolke wird entweder in einer speziellen Software zur Flächenrückführung eingelesen oder in einigen Fällen auch direkt in die CAD-Software eingelesen. Die oft übliche Umwandlung von Punkten zu einfachen Oberflächennetzen, wie sie bei Computerspielen meist ausreichend ist, genügt im Automobilbau nicht. Hier werden Bézier- und NURBS-Flächen durch die Punkte gelegt und dabei mit Filtermethoden mögliche Messfehler ausgeglichen. Die so entstandenen Flächen haben exakte Flächengrenzen und exakt definierte Übergänge zu den angrenzenden Flächen. Das aus dem Lehmmodell entstandene CAD-Modell kann im CAD gespiegelt werden. Es liegt dann ein exakt beschriebenes Modell einer Autokarosserie (oder Teile davon) vor.\n\nIn der Statistik und explorativen Datenanalyse werden Punktwolken zur grafischen Darstellung bivariater Zusammenhänge verwendet (vgl. Streudiagramm, Korrelation). Sie erlauben es, einen einfachen optischen Eindruck von Richtung und Enge des Zusammenhangs zu gewinnen und Ausreißer im Datensatz aufzuspüren.\n\n\n"}
{"id": "429327", "url": "https://de.wikipedia.org/wiki?curid=429327", "title": "Xandros", "text": "Xandros\n\nXandros ist ein Unternehmen, das eine gleichnamige, auf Debian basierende, Linux-Distribution produzierte. Der Name Xandros wurde von dem Begriff „X Window System“ und der griechischen Insel „Andros“ abgeleitet.\n\nXandros wurde 2001 gegründet und hat seinen Sitz in New York und Ottawa, Ontario. Xandros kaufte Corel Linux und entwickelte es weiter, nachdem Corel seine Linuxgeschäfte im August 2001 aufgegeben und den Distributionsmarkt verlassen hatte.\n\nAndreas Typaldos ist Chief Executive Officer von Xandros. Der Mitgründer und Vorsitzende von Xandros, Frederick H. Berenstein, verstarb am 6. September 2005.\n\nXandros ist ein Gründungsmitglied der DCC Alliance und seit 2006 Mitglied der OSDL, wo es sich besonders in der Desktop Working Group (DTL) engagiert.\nNach Novell hat Xandros 2007 ein Kooperationsabkommen mit Microsoft abgeschlossen. Am 30. Juni 2008 gab der ehemalige Linspire-CEO Kevin Carmony in seinem Blog den Verkauf des Unternehmens an Xandros bekannt.\n\n\"Xandros Desktop\" ist in der Lage, Windows-Programme durch die Software CrossOver auszuführen, die Aufrufe von Windows-Bibliotheken auf ähnliche Bibliotheken weiterleitet, die CrossOver mitbringt.\n\nDie Deluxe-Version enthält zusätzlich ein Benutzerhandbuch und 60 Tage im Preis enthaltenen E-Mail-Support.\n\nDie Edition \"Surfside Linux\" enthält das Benutzerhandbuch, 30 Tage E-Mail-Support und Internetsoftware wie Skype mit einer passenden USB-Sprechgarnitur.\n\nAuch das grafische Aussehen von Xandros Desktop ist eng an Windows angelehnt. Ähnliche Produkte sind Lycoris und Linspire.\n\nDer \"Xandros Business Desktop\" enthält Softwareunterstützung für das Active Directory und SMP- und Hyperthreading-Unterstützung, die die Ausführung mancher Programme beschleunigen können. Er enthält auch Werkzeuge zur Administration innerhalb von Betrieben oder Instituten und 90 Tage E-Mail-Support.\n\nPresto ist ein schlankes Schnellstart-Betriebssystem mit der Desktop-Umgebung Xfce, das innerhalb von Windows (ab XP) und dessen Partition installiert werden kann. Es wurde 2009 eingeführt. Es integriert einen Dienst zum Bezug neuer Software über das Internet mit dem mit Linspire aufgekauften Click-and-run-System (CNR) für einfache ein-Klick-Installationen von Softwarepaketen.\n\nSchließlich ist die \"Xandros Open Circulation Edition\" eine freie, weiterverwendbare Ausgabe für den nichtkommerziellen Gebrauch. Sie enthält künstliche Restriktionen, etwa ist im \"Xandros File Manager\" die CD-ROM-Brenn-Geschwindigkeit auf vierfach begrenzt. CrossOver ist nicht enthalten, es gibt keine technischen Unterstützungsdienstleistungen und kein Handbuch. Auch sind einige beigelegte Programme, etwa der Webbrowser \"Mozilla Firefox\", veraltet.\n\nDie derzeitige Betaversion vom \"Xandros Small Business Server\" (xSBS) unterstützt den Apache- und den Sambaserver. Diese Server arbeiten mit Linux- und Windowsumgebungen. Auch wird 2007 ein \"Xandros Desktop Management Server\" (xDMS) verkauft. Seit dem 1. Mai 2006 ist der \"Xandros Server\" erhältlich, der unerfahrene Administratoren ansprechen soll.\n\nDie Xandros-Benutzeroberfläche verwendet eine modifizierte Version des K Desktop Environment: Aus dem Konqueror (dem kombinierten Dateimanager und Browser) entwickelte man den eigenen Xandros File Manager. KDE wurde in Hinblick auf die beabsichtigte Ähnlichkeit zu Microsoft Windows modifiziert.\n\nAuf dem Asus Eee PC ist ein angepasstes Xandros vorinstalliert.\n\nDie Paketverwaltung basiert nach Hersteller-Angaben auf der Debians.\n\nLaut DistroWatch war 4.2 die letzte Version von Xandros. Diese Distribution wird nicht mehr weiterentwickelt, das offizielle Forum und die Webseite wurden eingestellt.\n\n"}
{"id": "429329", "url": "https://de.wikipedia.org/wiki?curid=429329", "title": "Lycoris", "text": "Lycoris\n\nLycoris war eine Linux-Desktop-Umgebung und ein Unternehmen. Lycoris ist griechisch für Dämmerung.\n\nJoseph Cheek arbeitete zuvor bei Microsoft und Linuxcare. 2000 gründete er Redmond Linux. Ende 2001 ging er mit der „Embedded systems“ Firma DeepLinux zusammen und nannte seinen Betrieb in Redmond Linux Corporation um. Das erste Betriebssystem hieß: „Redmond Linux Personal“. Im Januar 2002 firmierte das Unternehmen unter dem Namen Lycoris. Das Privatunternehmen hat seinen Sitz in Maple Valley, Washington, USA. Das bekannteste Produkt ist der Lycoris Desktop/LX, basierend auf „Redmond Linux Personal“. Am 16. Juni 2005 wurde bekannt gegeben, dass Lycoris von Mandriva übernommen wurde. Im Anschluss wurde der Lycoris Desktop/LX in Mandriva eingearbeitet und Lycoris als eigenständige Distribution eingestellt.\n\nLycoris Desktop/LX beruht ursprünglich auf Calderas Workstation 3.1 (Kefk.net: Debian). Desktop/LX bietet Internetzugang, OpenOffice.org-Pakete, Multimedia und mehr. Der Lycoris-Desktop und seine Applikationen sehen absichtlich wie das Microsoft-Produkt Windows XP aus bis hin zum Hintergrundbild (eine Rasenlandschaft mit wolkigem Himmel). Ein Nachteil von Lycoris ist, dass jegliche Entwicklungswerkzeuge grundsätzlich fehlen. Enthalten sind z. B. KDE, ein Foto-Editor, Mediaplayer sowie Mozilla und ein PDA-Synchronisations-Tool für Palm-Geräte. Man vergleiche das Prinzip von Lycoris mit Xandros oder Linspire.\n"}
{"id": "429740", "url": "https://de.wikipedia.org/wiki?curid=429740", "title": "MareNostrum", "text": "MareNostrum\n\nMareNostrum (, eine römische Bezeichnung des Mittelmeeres) ist ein Supercomputer an der Universitat Politècnica de Catalunya in Barcelona. Betrieben wird die Einrichtung vom Barcelona Supercomputer Centre BSC. Der Supercomputer ist für Forschung in den Bereichen Biowissenschaften, Meteorologie, und Umweltwissenschaften vorgesehen und für kommerzielle Anwendungen in den Bereichen Pharmazeutik, Automobil und Aeronautik. Die komplette Technik-Installation wurde in die ehemalige Kapelle Torre Girona eingebaut und wird von 5 m hohen Glaswänden umgeben. Der Supercomputer wurde seit der ersten Inbetriebnahme 2004 mehrfach durch aktuellere Technik ersetzt.\n\nBei Inbetriebnahme im November 2004 erreichte MareNostrum eine Dauerrechenleistung von 20 Teraflops bei Nutzung von 3.564 PPC 970, 2,2 GHz Prozessoren und kam damit im November 2004 auf Platz 4 der TOP500.\n\nAm 13. April 2005 wurde der Rechner zum ersten Mal mit 4.800 PPC 970, 2,2 GHz Prozessoren und Myrinet mit seiner vollen Leistung von 27,9 Teraflops hochgefahren. In dieser neuen Konfiguration erreichte er mit insgesamt 4.812 Prozessoren Platz fünf der Ausgabe Juni 2005 der Vergleichsliste.\n\nNach einer Umrüstung im Jahr 2006 auf PPC 970, 2,3 GHz Prozessoren und Myrinet verfügte MareNostrum 2 über 10.240 Kerne und 20 TB RAM und erreichte damit mit 62,6 Teraflops. Das System hatte ungefähr 300 TB Plattenpeicher. Das reichte im November 2006 für den fünften Platz in der Liste. 2008 kam es mit einer Leistung von 63,8 TFLOPS als Spaniens schnellstes System weltweit auf den 41. Platz. Im Juni 2012 belegte es noch Platz 465. Als das System abgebaut wurde, wurden die verbleibenden Teile zu kleineren Clustern mit 256 und 512 Rechenknoten aufgeteilt, die in verschiedenen spanischen Universitäten und Instituten weiterbetrieben wurden.\n\nFür den Betrieb von MareNostrum 3 wurden umfangreiche Bauarbeiten zur Verstärkung der Stromversorgung und des Kühlsystems erforderlich.\n\nIn der Zeit zwischen 2011 und 2013 waren zwei Systeme in Betrieb. Eines war ein System aus 5.544 Xeon E5649 6C 2,53 GHz Prozessoren, InfiniBand QDR, NVIDIA 2090 Prozessoren und 3024 GB Speicher. Dieses System schaffte 103,2 Teraflops und wurde bis 2013 betrieben.\n\nMareNostrum 3 ging zwischen 2012 und 2013 in Betrieb und verwendete anfänglich 33.664 DX360M4, Xeon E5-2670 8-Core, 2.600GHz Prozessoren und InfiniBand FDR zur Verbindung. Die Leistung war damit 636,9 Teraflops. Der Computer verfügte ab 2013 über 48.896 Intel Sandy Bridge Prozessoren in 3.056 Knoten, dazu 84 Xeon Phi 5110P in 42 Knoten, mit mehr als 115 TB Hauptspeicher und 2 PB an GPFS Diskspeicher. Insgesamt erreichte er damit 925,1 Teraflop und 1,1 Petaflop peak. Im Top500 Ranking erreichte das System im Juni 2013 den Platz 29.\n\nMareNostrum 3 wurde ab Mitte 2017 durch MareNostrum 4 ersetzt. Das neue System übertrifft MareNostrum 3 ungefähr um den Faktor 10 bis 12. Er verfügt über 11,1 Petaflops Peak Rechenkapazität und wird insgesamt 13,7 Petaflops erreichen. Gemäß dem Top 500 Ranking vom 19. Juni 2017 war es der drittstärkste Cluster in Europa und der dreizehnte weltweit. MareNostrum 4 ist verbunden mit den Big Data Einrichtungen des Barcelona Supercomputer Centre BSC, die eine Speicherkapazität von 24,6 Petabytes haben und ist über die RedIris- und GÉANT-Netzwerke mit den europäischen Universitäten verbunden.\n\nInteressant und einzigartig ist die heterogene Architektur. Es gibt den allgemeinen Block, der die Hauptrechenarbeit übernimmt und einen zusätzlichen Block zur Erforschung neu entwickelter Technologien. Fünf Speichereinheiten (Elastic Storage) verwalten 14 Petabytes an Daten, ein Intel-Omni-Path-Hochgeschwindigkeitsnetzwerk und ein Ethernet verbindet die Komponenten.\n\n\nDer Block mit den neuentwickelten Technologien enthält Cluster von drei verschiedenen Technologien, die eingebunden und aktualisiert werden, sowie sie auf dem Markt verfügbar sind. Neue Prozessoren und Software können damit betrieben, getestet und optimiert werden, noch bevor die nächste Rechnergeneration im vollen Umfang aufgebaut wird. Spezialisierte Chips z. B. Grafikprozessoren können entsprechende Aufgaben in besonderem Maß optimieren und beschleunigen. Der Übergang zu künftigen neuen Technologien kann so fließend geschehen.\n\n\n\nMareNostrum erfüllt vielfältige Aufgaben in Wissenschaft, Forschung und Lehre. Die Arbeitsbereiche umfassen Big Data, Bioinformatik, Biomechanik, Klimatologie, Cloud Computing, KognitionswissenschaftRechnerarchitektur und Codedesign, verteilte Systeme, Schulung, Technische Simulationen, Fusionsenergie, Genomforschung, Geophysik, Softwareentwicklung für Supercomputer, Materialwissenschaft, Molekulare Modellierung Operations Infrastruktur, Leistungsanalyse, Programmierungsmodelle, Soziale Simulationen, rechnergestützte Erdwissenschaften, extreme mathematische Probleme und Algorithmen, Quanteninformation.\n\nBSC betreibt noch weitere größere Rechenknoten. Der zweitgrößte Cluster unter dem Namen MinoTauro vereinigt 39 Server mit jeweils \n\n\nInsgesamt erreicht das System 250.94  Tflops Peak, davon 226.98 TFlops von den Grafikprozessoren und 23.96 TFlops von den Hauptprozessoren. Als Betriebssystem kommt Red Hat Enterprise Server zum Einsatz\n\n"}
{"id": "429903", "url": "https://de.wikipedia.org/wiki?curid=429903", "title": "Maxima (Computeralgebrasystem)", "text": "Maxima (Computeralgebrasystem)\n\nMaxima ist ein Computeralgebrasystem, das als Open-Source-Projekt unter der GNU General Public License (GPL) entwickelt wird. \n\nImplementiert ist Maxima in Common Lisp. Es existieren Versionen für Windows, macOS, Linux und Android.\n\nMaxima ist eine Version von Macsyma, einem der ersten Computeralgebrasysteme. Es wurde in den 1960er Jahren im Auftrag des US-Energieministeriums (\"DOE\") am MIT entwickelt. Eine Macsyma-Version (\"DOE Macsyma\") wurde von William Schelter von 1982 bis zu seinem Tod 2001 weiterentwickelt. 1998 erhielt Schelter vom Energieministerium die Genehmigung, seine Version unter der GPL zu veröffentlichen. Diese Version wird nun unter dem Namen Maxima von einer unabhängigen Gruppe von Anwendern und Entwicklern gepflegt.\n\nMit dem Programm wxMaxima ist eine auf wxWidgets basierende grafische Benutzeroberfläche für Maxima verfügbar, die durch Menüs und Dialoge die Nutzung des Programms vereinfacht und eine grafische Formelausgabe besitzt. Von Version 5.10.0b an ist die aktuelle Version von wxMaxima bereits im Installationspaket für Windows integriert.\n\nDer Emacs-Editor enthält mit maxima.el ebenfalls ein Frontend für Maxima. maxima.el leitet die Ausgabe von Maxima in einen Emacs-Buffer um. Mit imaxima gibt es eine Erweiterung, die die Ausgabe von maxima mittels LaTeX im Emacs-Buffer darstellt.\n\nMaxima enthält eine ALGOL-ähnliche Programmiersprache mit Lisp-Semantik und kann unter anderem folgende Aufgabenklassen symbolisch und numerisch (mit frei wählbarer Stellengenauigkeit) lösen:\n\nWeitere Fähigkeiten\n\n\n"}
{"id": "429905", "url": "https://de.wikipedia.org/wiki?curid=429905", "title": "Macsyma", "text": "Macsyma\n\nMacsyma ist ein Computeralgebrasystem, das in der Sprache Lisp implementiert ist. Maxima ist eine Open-Source-Version von Macsyma.\n\nUrsprünglich wurde Macsyma 1968 bis 1982 im Labor für künstliche Intelligenz des MIT als Teil des DARPA finanzierten Project MAC entwickelt. Federführend in der Entwicklung war Joel Moses.\n\n1982 übergab das MIT eine Version von Macsyma an das US-Energieministerium (DOE), einen der Hauptsponsoren des Projekts. Diese Version wurde als DOE Macsyma bezeichnet. Auf ihr beruht die Open-Source-Version Maxima.\n\nSymbolics erwarb 1982 eine Lizenz für die Weiterentwicklung von Macsyma. Symbolics entwickelte Macsyma einige Jahre lang, aber betrachtete es schließlich als nebensächlich für ihr Hauptgeschäft, den Verkauf von Lisp-Maschinen. Durch das mangelnde Interesse von Symbolics Macsyma weiter zu entwickeln und auch auf andere Systeme, wie PCs, zu portieren, verlor Macsyma dramatisch an Marktanteilen. Hatte Macsyma 1987 noch einen Anteil von 70 % am Markt der symbolischen Computer-Algebra-Systeme gehalten, fiel der Anteil bis 1991 auf 1 % zurück.\n\nRichard Petti und Russell Noftsker, der Gründer von Symbolics, gründeten 1992 Macsyma Inc., kauften Symbolics Macsyma ab und führten die Entwicklung einige Jahre lang weiter. Macsyma Inc. schaffte es jedoch nicht mehr einen größeren Marktanteil gegenüber anderen Computer-Algebra-Systemen zu sichern.\n\n1999 wurde Macsyma von Tenedos LLC, einer Holdinggesellschaft, übernommen. Bis jetzt hat Tenedos Macsyma weder neu herausgegeben noch abgestoßen. Macsyma kann immer noch von Symbolics lizenziert werden. Macsyma 2.4 ist für PCs unter Windows erhältlich.\n\n"}
{"id": "431125", "url": "https://de.wikipedia.org/wiki?curid=431125", "title": "MacSOUP", "text": "MacSOUP\n\nMacSOUP ist ein Usenet-Newsreader für Mac OS X und Mac OS Classic. Er arbeitet im Offline-Betrieb. Zusätzlich kann er auch zum Empfang und Versenden von E-Mails eingesetzt werden. \n\nBesonderheiten sind seine graphische Threadanzeige und die Möglichkeit einer Benutzung der Faces-Datenbank, mit welcher für bestimmte Nutzer ein Porträt angezeigt werden kann. Dabei liegen die Bilder lokal auf dem System vor und müssen nicht mit dem Posting übertragen werden. XFaces können aber auch angezeigt werden.\n\nMit der 2016 veröffentlichten Version 2.8.5 änderte der Autor der Software die Lizenz von Shareware zu Freeware.\n\n"}
{"id": "432135", "url": "https://de.wikipedia.org/wiki?curid=432135", "title": "Softlanding Linux System", "text": "Softlanding Linux System\n\nSoftlanding Linux System (SLS) war die vermutlich erste Linux-Distribution, die mehr als nur den Linux-Kernel und Basisapplikationen enthielt: Neben GNU/Linux-Features gab es auch Programme wie das X Window System und TCP/IP. Sie wurde von Peter MacDonald zusammengestellt und am 15. August 1992 zum Download angeboten. Damals hatte sie einen Umfang von 15 Disketten, wuchs jedoch im Laufe der Zeit auf über 30 Disketten an. Interessierte ohne breitbandige Netzanbindung konnten deshalb auch eine CD bestellen.\n\nSLS war zunächst die bekannteste Distribution. Aufgrund der vielen Fehler wurde die Linux-Community aber immer unzufriedener damit. Zwei ihrer größten Kritiker waren Ian Murdock und Patrick Volkerding. Murdocks Frustration ließ ihn das Debian-Projekt aus der Taufe heben, während Volkerding entschied, die Probleme zu beseitigen und SLS zu erweitern. Schließlich gab er der so entstandenen Distribution den Namen Slackware.\n"}
{"id": "432838", "url": "https://de.wikipedia.org/wiki?curid=432838", "title": "Udev", "text": "Udev\n\nudev steht für userspace /dev (\"/dev\" steht für Gerätedatei; siehe engl. \"device\" = Gerät) und ist ein Programm, mit welchem der Linux-Kernel Gerätedateien für die Datenein- und -ausgabe (Input/Output) verwaltet.\n\nudev ersetzt seit dem Kernel 2.6 das früher genutzte devfs-Dateisystem, dessen Aufgaben es damit übernimmt. Genauso wie devfs verwaltet udev das /dev-Verzeichnis, welches die speziellen Gerätedateien enthält, um von Programmen aus auf die vom System zur Verfügung gestellten Geräte zuzugreifen.\n\nMit der Einführung von udev waren sowohl udev als auch devfs im Kernel enthalten. Seit Ende Juni 2006 ist nur noch udev enthalten, devfs wurde vollständig entfernt.\n\nAm 3. April 2012 wurde udev in systemd mit der Begründung aufgenommen, dass Hotplugging ein integraler Bestandteil von systemd sei und sich der administrative Aufwand beider Projekte sowie redundanter Code durch einen Merge reduzieren ließen. udev wurde somit zu einem Bestandteil von systemd 183.\n\nudev überwacht und wertet hotplug-Ereignisse aus. Finden sich dort Informationen über ein neu angeschlossenes Gerät, werden zu diesem Gerät vorhandene zusätzliche Informationen dem \"sysfs\"-Dateisystem entnommen und eine neue Gerätedatei im /dev-Verzeichnis erzeugt. Dabei ist der für die spezielle Datei verwendete Name und die Zugriffsberechtigung frei durch Regeln konfigurierbar.\n\nInstalliert man udev, so findet man die Standardregeln unter codice_1.\nUm die eigenen Regeln anwenden zu können, sollte man diese Datei nicht verändern, sondern eine neuere mit kleinerer Nummer erzeugen, zum Beispiel codice_2. Somit ist gewährleistet, dass diese Regeln zuerst erkannt werden.\nWurde eine Regel gefunden, so wird diese, und auch jede passende folgende, angewandt.\n\nIm Unterschied zu devfs, welches im Linux-Kernel selbst integriert war, arbeitet udev im userspace, wird also als normales Programm gestartet. Die Verwaltung und Namensgebung der angeschlossenen Geräte liegt dadurch nicht mehr beim Kernel und ist somit einfacher konfigurierbar.\n\nWeitere Vorteile sind:\n\nEin bewusst abgewägter Nachteil von udev besteht darin, dass es im Gegensatz zu devfs beim Aufruf eines /dev-Knotens nicht automatisch die entsprechenden Gerätetreiber lädt.\nFunktioniert beim Einstecken eines Gerätes HotPlug nicht richtig und lädt die benötigten Treiber nicht, können diese nicht mit udev nachgeladen werden.\n\n"}
{"id": "433006", "url": "https://de.wikipedia.org/wiki?curid=433006", "title": "Head-driven Phrase Structure Grammar", "text": "Head-driven Phrase Structure Grammar\n\nDie Head-driven Phrase Structure Grammar (HPSG) ist eine Grammatiktheorie, die in den 1980er Jahren auf der Basis der Wiederbelebung der kontextfreien Phrasenstrukturgrammatiken als Generative Grammatiktheorie aus der Familie der Unifikationsgrammatiken entstand. In HPSG werden grammatische Regeln als Beschränkungen formuliert, die korrekte Sätze und Satzglieder erfüllen müssen; Transformationsregeln finden daher keine Anwendung. Die gesamte Information über ein linguistisches Zeichen wird dabei in einer einzigen Merkmalsbeschreibung zusammengefasst. Im Gegensatz zu einigen anderen Grammatiktheorien werden wortspezifische Informationen vollständig im Lexikon angegeben, sodass nur wenige Grammatikregeln nötig sind.\n\nWie alle Phrasenstrukturgrammatiken ist die HPSG eine Konstituentengrammatik. Sie baut also auf dem Prinzip der Konstituenz auf und ist daher keine Dependenzgrammatik, die auf dem Prinzip der Dependenz aufbaut. \n\nDie Head-Driven Phrase Structure Grammar wurde von Carl Pollard und Ivan Sag ab der Mitte der 1980er Jahre entwickelt. Wesentliche Bestandteile wurden von älteren Syntaxtheorien, besonders nicht-derivationellen Ansätzen, angeregt oder übernommen, beispielsweise der Kategorialgrammatik (CG), Generalized Phrase Structure Grammar (GPSG), Arc Pair Grammar (AC), Lexical-Functional Grammar (LFG), aber auch der damals vorherrschenden Rektions- und Bindungstheorie (Government and Binding Theory, GB) Noam Chomskys. Die Darstellung der Semantik beruht zu Teilen auf der Situationssemantik; formale Grundlagen entstammen der Informatik. Die erste umfassende Darstellung der Theorie lieferten Carl Pollard und Ivan Sag mit dem 1987 erschienenen Buch \"Information-Based Syntax and Semantics, Volume I\" (Pollard, Sag 1987); eine überarbeitete Version stellten sie 1994 im Werk \"Head-Driven Phrase Structure Grammar\" (Pollard, Sag 1994) vor. Schon von Beginn an haben auch andere Wissenschaftler die Head-Driven Phrase Structure Grammar aufgegriffen und Modifikationen, Erweiterungen und Anwendungen auf unterschiedliche Sprachen vorgeschlagen. In vielen Fragen existiert somit eine Vielzahl von Ansichten, die von unterschiedlichen Wissenschaftlern zur Beschreibung unterschiedlicher Sprachen geäußert werden. Auch einige Grammatiktheorien, die eine Mittelstellung zwischen HPSG und anderen Theorien einnehmen, wurden entwickelt, beispielsweise die \"Sign-Based Construction Grammar\", die innerhalb das HPSG-Formalismus Ideen der Konstruktionsgrammatik aufgreift.\n\nIn HPSG werden alle Wörter und Phrasen als Zeichen im Sinne von Ferdinand de Saussure modelliert, das heißt als Form-Bedeutungs-Paare. Syntaktische Eigenschaften, die Lautstruktur und die Bedeutung eines Zeichens werden in einer einzigen Attribut-Wert-Matrix dargestellt, weshalb HPSG als monostratal gilt. Die Attribut-Wert-Matrix jedes Zeichens enthält mindestens ein Merkmal PHON, das die Phonemfolge repräsentiert, und einen Wert SYNSEM, der in einer Matrix vom Typ \"synsem\" Informationen über grammatikalische Eigenschaften und die Bedeutung zusammenfasst. Es gibt auch Vorschläge zur formalen Darstellung weiterer Aspekte einer Sprache in Attribut-Wert-Matrizen, beispielsweise der Wortstellung (siehe den Abschnitt :Wortstellung) und der Silbenstruktur.\n\nIm Gegensatz zu vielen anderen Grammatiktheorien ist HPSG deklarativ: die gesamte Grammatik inklusive des Lexikons wird als Beschreibung grammatikalisch richtiger Zeichen formuliert. Daher existieren in HPSG keine Regeln zur Veränderung oder Bewegung von Konstituenten. Stattdessen werden grammatische Regeln ausschließlich in Form von Beschränkungen ausgedrückt, die von wohlgeformten Zeichen erfüllt werden müssen. Ein Beispiel hierfür ist die Kongruenz eines Verbs mit seinem Subjekt. Während in nicht-beschränkungsbasierten Grammatiktheorien beispielsweise die Übertragung eines Merkmales vom Subjekt auf das Verb angenommen wird, besitzen in HPSG Verb und Subjekt entsprechende Merkmale, die nach bestimmten Beschränkungen in beiden Zeichen gleich sein müssen.\n\nHPSG ist außerdem eine in großem Maß lexikalisierte Grammatiktheorie, das heißt, die grammatische Information ist zu einem großen Teil im Lexikon gespeichert, die Grammatik selbst muss nur noch wenige Beschränkungen zur Verarbeitung des Lexikons zur Verfügung stellen. Beispielsweise werden die Argumente eines Verbs in Listen festgelegt, die in der Merkmalsbeschreibung des Verbs enthalten sind; die Grammatik legt dann durch Beschränkungen fest, wie die Argumente realisiert werden.\n\nAlle Informationen über ein Zeichen werden in HPSG in einer hierarchisch aufgebauten Attribut-Wert-Matrix (\"attribute-value matrix\", kurz AVM) angegeben. In jeder Zeile wird dabei zu einem bestimmten Attribut der entsprechende Wert angegeben. Jeder Wert hat dabei einen bestimmten Typ und kann eigene Merkmale haben. Der Typ bestimmt dabei, welche Merkmale ein Objekt hat und welche Typen die entsprechenden Werte haben. Beispielsweise hat im Formalismus von Pollard und Sag 1994 jedes Objekt vom Typ \"synsem\" ein Merkmal LOCAL mit einem Objekt vom Typ \"local\" als Wert und ein Merkmal NONLOC mit einem Wert vom Typ \"nonloc\". Die Typen bilden eine Hierarchie, wobei Subtypen die Merkmale ihrer Obertypen erben. Typen werden in der grafischen Darstellung meist am linken Rand kursiv dargestellt. Das durch die folgende Matrix dargestellte Objekt beispielsweise hat den Typ \"index\" und die Merkmale PERSON, NUMBER und GENDER. Die zugehörigen Werte sind von den Typen \"2\", \"sg\" und \"fem\" und haben hier keine eigenen Merkmale:\n\nformula_1\n\nAls Werte sind auch Listen und Mengen von Objekten zugelassen. So verlangt das Merkmal SUBCAT als Wert eine Liste von Objekten, die den Typ \"synsem\" haben:\n\nformula_2\n\nBei der grafischen Darstellung von Attribut-Wert-Matrizen ist zu berücksichtigen, dass meist nur die für die jeweilige Fragestellung notwendigen Merkmale einer Matrix dargestellt sind. Außerdem werden längere Pfade in der Literatur oft mit „|“ abgekürzt. Daher sind die beiden folgenden Matrizen gleichbedeutend:\n\nformula_3\n\nformula_4\n\nEine HPSG-basierte Beschreibung einer Sprache besitzt mindestens die folgenden formalen Bestandteile:\nDas Lexikon wird entweder durch Beschränkungen auf den Typ \"word\" ausgedrückt oder aber es erhält einen eigenen Status außerhalb der Beschränkungen.\n\nBeschränkungen lassen sich mit Hilfe von unterspezifierten Merkmalsbeschreibungen formulieren, die dann bei der Anwendung der Beschränkung auf eine Merkmalsbeschreibung mit dieser unifizierbar sein muss, damit die Beschränkung erfüllt ist. Als Beispiel diene hier das Kopfmerkmalsprinzip, das festlegt, dass in jedem phrasalen Zeichen, das einen Kopf hat, der HEAD-Wert gleich dem der Kopftochter sein muss. Dies lässt sich als Implikation formulieren (die Verwendung von Quantoren ist in der HPSG-Literatur nicht obligatorisch):\n\nformula_5\n\nDas Lexikon besteht in HPSG aus Beschreibungen für die Wörter einer Sprache, die sogenannten Lexikoneinträge. Hierzu kann eine Disjunktion von Merkmalbeschreibungen für jedes einzelne Wort angewendet werden:\n\nUm Verallgemeinerungen zu ermöglichen, können Wörter in Wortklassen eingeteilt werden, die solche Merkmale erfassen, die allen Wörtern einer Wortklasse gemeinsam sind. So erfüllen Substantive wie \"Frau\", \"Sonne\", \"Katze\" das allgemeinere Schema für feminine Substantive im Singular: in der Analyse von Müller 1999 beispielsweise sind ihre Kopfmerkmale vom Typ \"noun\", sie verlangen einen Artikel als Komplement und haben die gleichen Werte für Person, Numerus und Genus.\n\nEinige syntaktische und vor allem morphologische Phänomene werden durch sogenannte \"Lexikonregeln\" erfasst, die Lexikoneinträge lizenzieren, indem sie sie mit anderen Einträgen in Beziehung setzen. Beispielsweise könnten in einer vereinfachten Grammatik passive Verbformen lizenziert werden, indem zu jedem transitiven Verb ein passives Verb lizenziert wird, dessen Subjekt mit dem Objekt des transitiven Verbs übereinstimmt. In vielen HPSG-basierten Theorien wird dabei der Operator „↦“ angewendet, der die Beschreibung des Ausgangsworts und des durch die Regel lizenzierten Wortes verbindet:\n\nformula_6\nformula_7\n\nLexikonregeln werden dabei − je nach theoretischem Ansatz − entweder als Metaregeln zur Beschreibung des Lexikons oder als Beschränkungen auf Wörter innerhalb des Formalismus von HPSG formuliert.\n\nDie Bedeutung eines Zeichens ist in einem \"synsem\"-Objekt in einer Matrix angegeben, die meist CONTENT genannt wird und meist einen von mehreren Subtypen von \"content\" mit jeweils eigenen Merkmalen hat. Pollard und Sag 1994 beispielsweise sehen die Typen \"psoa\", \"nom-obj\" und \"quant\" vor, Bender, Sag und Wasow 2003 etwa nehmen dagegen eine einheitliche Merkmalsgeometrie für alle CONTENT- (bei ihnen SEM-)Werte an.\n\nZur Darstellung der Situationssemantik wird in den meisten HPSG-Theorien auf so genannte \"parametrisierte Sachverhalte\" (englisch \"parametrized state of affairs\", kurz \"psoa\") zurückgegriffen, die mit Matrizen des Typs \"psoa\" dargestellt werden. Parametrisierte Sachverhalte bestehen aus einer Relation wie \"sehen\", \"schlagen\", \"Buch\", \"Mensch\" und Parametern, die verschiedene semantische Rollen in der Relation angeben. In HPSG werden die parametrisierten Sachverhalte ebenfalls in Attribut-Wert-Matrizen dargestellt. So lässt sich die Relation „der Mann sieht den Hund“ nach Müller 2007 durch folgende Matrizen darstellen::\n\nformula_8\n\nDie erste Matrix ist dabei die \"psoa\"-Darstellung der Relation \"sehen\" mit zwei Argumenten. Die zweite Matrix stellt den CONTENT-Wert der Beschreibung von \"Mann\" dar. Ihr Index ist durch den Tag \"[1]\" mit dem Agens von \"sehen\" identifiziert, die RESTRICTIONS-Menge legt fest, dass es sich bei \"[1]\" um einen Mann handelt. Analog verhält es sich mit der dritten Matrix, die sich in der Beschreibung von \"Hund\" befindet.\n\nDurch Strukturteilung lassen sich die semantischen Rollen auch mit syntaktischen Funktionen verbinden. Der folgende Auszug aus dem LOCAL-Wert von „sehen“ identifiziert die Indizes der beiden Nominalphrasen in der SUBCAT-Liste (siehe den Abschnitt :Komplemente) mit dem Agens beziehungsweise Patiens (Merkmalsgeometrie nach Müller 2007):\nformula_9\n\nIn jüngeren HPSG-basierten Theorien finden auch andere Theorien wie \"Minimal Recursion Semantics\" (MRS) und \"Lexical Resource Semantics\" (LRS) Anwendung, die sich ebenfalls mit Attribut-Wert-Matrizen darstellen lässt.\n\nKontextinformationen werden in einer Matrix vom Typ \"context\" unter dem Pfad SYNSEM | LOC | CONTEXT angegeben, die Merkmale wie BACKGROUND und C-INDICES hat. BACKGROUND ist eine Menge von \"psoa\"-Objekten, die Hintergrundinformationen über das Aussprechen des Satzes liefern. C-INDICES hat mehrere Attribute, die in Form von Indizes Informationen über die Umstände des Sprechens geben, beispielsweise den Sprecher, den Angesprochenen und den Ort.\n\nIn HPSG wird meist angenommen, dass phrasale Zeichen sich aus einer Kopftochter und einer gewissen Anzahl von Nichtkopftöchtern zusammensetzen. Die Zusammensetzung einer solchen Struktur wird vor allem durch Grammatikprinzipien und die Merkmale der Töchter bestimmt. Vor allem die frühe Forschung versuchte, mit einer möglichst geringen Anzahl von sehr allgemeinen Arten von Konstruktionen, so genannten \"ID-Schemata\" (\"Immediate-Dominance-Schemata\") auszukommen. In der Grammatik von Pollard und Sag 1994 gibt es sechs ID-Schemata, darunter beispielsweise eines für die Verbindung von Kopf und Komplement und eines für die Verbindung von Kopf und Adjunkt. Jüngere Versionen, besonders solche, die der Konstruktionsgrammatik nahestehen, enthalten oft sehr zahlreiche und spezifische Konstruktionen. So entwickeln Ginzburg und Sag 2000 23 ID-Schemata. Umstritten ist auch, ob bestimmte Phänomene mit ID-Schemata beschrieben werden sollten, die keine Nichtkopftochter, sondern nur eine Kopftochter besitzen. Solche Vorschläge wurden beispielsweise zur Einführung nichtlokaler Information ohne Verwendung von Spuren gemacht (siehe unter :Nichtlokale Informationen). Der folgende Abschnitt konzentriert sich stärker auf Ansätze, die Pollard und Sag 1994 folgend von einer geringen Anzahl von ID-Schemata und einer starken Lexikalisierung ausgehen.\n\nIn HPSG-Versionen, die Pollard und Sag 1994 folgen, wird angenommen, dass die Semantik einer Phrase in den meisten Fällen identisch ist mit der der Kopftochter. Hingegen soll die Semantik von der Adjunkttochter bestimmt werden, wenn es sich um eine Struktur mit Adjunkt handelt. Für die Kopf-Adjunkt-Struktur „rotes Buch“ und ihre Töchter ergeben sich damit folgende SYNSEM-Werte:\nformula_10\n\nformula_11\n\nformula_12\n\nDas Merkmal HEAD enthält Informationen, die dem Kopf und seinen phrasalen Projektionen gemeinsam sind, beispielsweise Kasus, Numerus und Genus bei Nominalphrasen. Das Kopfmerkmalsprinzip bedingt dabei, dass das HEAD-Merkmal einer Phrase mit der ihres Kopfes identisch ist.\n\nJe nach Ansatz werden verschiedene Arten von Nichtkopftöchtern unterschieden. Dieser Absatz kann daher nur Beispiele liefern.\n\nKomplementtöchter sind von ihrem Kopf lexikalisch im Rahmen der Valenz festgelegt. Informationen über die Valenz eines Zeichens werden in einer oder mehreren Listen, wie dem SUBCAT-Merkmal, unter dem Pfad SYNSEM|LOC gespeichert. Sie enthalten meist die SYNSEM-Objekte von Argumenten des Zeichens, die noch nicht abgebunden wurden. Je nach theoretischem Ansatz ließen sich für ein Verb wie „sehen“ also folgende Ausschnitte einer Merkmalsbeschreibung formulieren:\n\nformula_13\n\nformula_14\n\nEin Grammatikprinzip legt dabei fest, wie Komplemente abgebunden werden. Bei Annahme einer SUBCAT-Liste lässt es sich folgendermaßen formulieren:\n\nNach Pollard und Sag 1994 selegieren Adjunkte ihre Köpfe. Hierzu erhalten sie ein Kopfmerkmal MODIFIED, kurz MOD, das durch Strukturteilung mit dem SYNSEM-Wert des Kopfes identifiziert wird, wie ein ID-Schema festlegt, in dem Kopf-Adjunkt-Strukturen definiert sind. Außerdem wird, wenn die \"Situationssemantik\" (siehe :Semantik und Pragmatik) verwendet wird, angenommen, dass die Semantik der Adjunkttochter identisch ist mit der der Mutter, weshalb diese Beschränkung in das Semantikprinzip aufgenommen ist. Hierdurch wird es ermöglicht, die Semantik von Phrasen mit so genannter kapselnder Modifikation zu erfassen. Eine Reihe von neueren Arbeiten geht dagegen davon aus, dass Adjunkte wie Komplemente von einer eigenen Valenzliste des Kopfes bestimmt werden; dieser Ansatz ist auch unter dem Namen \"Adjunct-as-Complement Approach\" bekannt.\n\nZur Analyse diverser Phänomene wurden zahlreiche weitere Arten von Nichtkopftöchtern eingeführt. So haben Pollard und Sag 1994 für Wörter wie die englische Konjunktion \"that\" eine eigene Wortklasse vorgeschlagen, die sogenannten Markierer (englisch \"Marker\"). Gemäß ihrem Kopf-Markierer-Schema selegieren Markierer über das SPEC-Merkmal den SYNSEM-Wert des Kopfes, zusätzlich hat bei ihnen und bei ihrer Mutter das Merkmal MARKING den Wert \"marked\".\n\nEine formale Besonderheit stellt die von Pollard und Sag 1994 eingeführte Analyse von Quantifikatoren dar. Ihre Skopuseigenschaften werden mit einem \"Cooper-Store\" modelliert, der die Semantik des Quantifikators enthält und der vom Quantifikator aus so lange nach oben weitergereicht wird, bis er aufgrund einer Beschränkung abgebunden wird.\n\nFür Verbindungen zwischen Knoten, die in der Merkmalsbeschreibung der übergeordneten Phrase weiter voneinander entfernt liegen, verwendet HPSG sogenannte \"nichtlokale Merkmale\", die für die Fernverbindung (englisch: \"unbounded dependency\") nötige Information enthalten und von Knoten zu Knoten weitergereicht werden, um die Information so beiden relevanten Knoten zur Verfügung zu stellen. Ein Grammatikprinzip stellt dabei sicher, dass nichtlokale Werte so lange weitergereicht werden, bis sie aus einem bestimmten Grund abgebunden werden.\n\nDies ermöglicht beispielsweise die Analyse der Extraktion von Nominalphrasen. In vielen HPSG-Ansätzen wird wie in anderen Grammatiktheorien angenommen, dass die entsprechende Nominalphrase an ihrer eigentlichen Position eine Spur (englisch \"trace\") hinterlässt, die mit der extrahierten Phrase koindiziert ist und sich von anderen Wörtern dadurch unterscheidet, dass ihr PHON-Wert leer ist. Beispiele aus dem Englischen sind:\nPollard und Sag 1994 und andere haben eine Merkmalsbeschreibung für Spuren vorgeschlagen, nach der ihr LOCAL-Wert, in dem alle lokalen Informationen über Syntax und Semantik gespeichert sind, mit einem Element der nichtlokalen SLASH-Liste identifiziert wird, das dann durch das erwähnte Grammatikprinzip so lange weitergegeben wird, bis es wieder abgebunden wird. Andere Analysen verwenden Phrasen mit nur einer Tochter oder Lexikonregeln, um die nichtlokale Information an einem Knoten einzuführen, ohne ein leeres Zeichen zu benötigen.\n\nDie Bindungstheorie macht Aussagen darüber, ob Nominalphrasen als Reflexivpronomina, als Personalpronomina oder als Nichtpronomina realisiert werden können. Die vorgeschlagenen Bindungstheorien gehen dabei davon aus, dass zwei Bedingungen erfüllt werden müssen, damit eine Nominalphrase anaphorisch, in der klassischen Grammatik also als Reflexivum, erscheint:\nDas Problem der Definition dieser Beziehung wird von den verschiedenen Ansätzen unterschiedlich gelöst, maßgeblich ist in HPSG aber immer, dass die anaphorische Nominalphrase, ihre Mutter, oder deren Projektion obliquer ist als die andere Nominalphrase, mit der sie koindiziert wird.\n\nDie Konstituentenstellung einer Sprache lässt sich in HPSG durch weitere Beschränkungen ausdrücken, die sogenannten \"Linear Precedence Rules\", kurz LP-Regeln, die die Form \nhaben. X < Y bedeutet dabei, dass die Konstituente X vor der Konstituente Y steht, X > Y steht für die umgekehrte Stellung. In Sprachen, in denen Köpfe am Ende einer Phrase stehen, gilt also die Regel\nSprachen mit komplexerer oder freierer Wortstellung wie das Deutsche benötigen kompliziertere Regeln, beispielsweise in Form einer Disjunktion mehrerer LP-Regeln.\n\nFür Sprachen mit besonders freier Wortstellung existieren mehrere Analysevorschläge, die über die Formulierung komplexer LP-Regeln hinausgehen. Eine freie Anordnung der Komplemente wurde von einigen Wissenschaftlern mit Lexikonregeln zur Umordnung der Valenzlisten oder durch Annahme ungeordneter Valenzmengen statt Listen erklärt, auch flache Strukturbäume wurden in Erwägung gezogen. In einer Reihe von Arbeiten wird dagegen vorgeschlagen, Elemente der Valenzlisten nach dem Abbinden nicht zu entfernen, sondern weiter nach oben zu reichen, damit Zeichen von einem anderen Ort aus noch darauf zugreifen können.\n\nEin anderer Ansatz geht davon aus, dass die Wortstellung nicht direkt mit der syntaktischen Struktur zusammenhängt, dass also Konstituenten nicht kontinuierlich sein müssen. Hierzu wird angenommen, dass die Töchter eines phrasalen Zeichens in einer sogenannten \"Linearisierungsdomäne\" im Merkmal DOM gesammelt werden, auf das dann LP-Regeln angewendet werden. Bei der Vereinigung zweier Domänen können sie mit der \"Shuffle\"-Operation, bei der die Stellung der Zeichen aus einer Domäne relativ zueinander erhalten bleibt, zur Domäne der Mutter zusammengefügt werden; eine Domäne kann aber auch kompakt sein, sodass keine fremden Zeichen zwischen Zeichen dieser Domäne stehen können.\n\nSeit Beginn der 1990er Jahre wurden in der Computerlinguistik verschiedene Systeme zur Implementierung von HPSG-Grammatiken entwickelt. Nur ein geringerer Teil der HPSG-basierten Architekturen setzt direkt den Formalismus um, indem es auf jedes linguistische Objekt die in der Theorie formulierten Beschränkungen anwendet. Da sich in solchen Systemen Effizienzprobleme ergeben können, kodieren andere Implementierungen einen Teil der Beschränkungen als Relationen mit linguistischen Objekten als Argumenten, wobei besonders Phrasenstrukturregeln eine wichtige Rolle spielen, die zwar in HPSG nicht vorhanden sind, aber effizientes Parsen erleichtern. Einige Implementierungen ermöglichen auch Parsen mit diskontinuierlichen Konstituenten, die in bestimmten HPSG-Grammatiken eine Rolle spielen (siehe :Wortstellung). HPSG-basierte Systeme spielen eine Rolle bei der Forschung im Bereich des \"Deep Processing\", wo sie auch mit Methoden des \"Shallow Processing\" kombiniert werden können.\n\n\n\n\n"}
{"id": "433046", "url": "https://de.wikipedia.org/wiki?curid=433046", "title": "Supercalc", "text": "Supercalc\n\nSupercalc (Eigenschreibweise \"SuperCalc\") war eine ursprünglich von kalifornischen Firma Sorcim ab 1980 entwickelte Tabellenkalkulation für Mikrorechner mit dem Betriebssystemen CP/M. Sie war auch unter DOS sehr erfolgreich. Von Drittanbietern wurden Erweiterungen für die Finanzkalkulation angeboten. Von G&G Engineering gab es Supersheet zur grafischen Darstellung der Daten.\n\nSeit der Übernahme 1984 durch CA Technologies wurde das Produkt zu Supercalc 4 und 5 weiterentwickelt. Die letzte Version erschien 1993 für Windows 3.1.\n\nIn der DDR wurde ein vergleichbares Programm als „KP“ (Kalkulationsprogramm) vom Kombinat Robotron unter dem Betriebssystem SCP, einem CP/M 2.2-Derivat, auf diversen 8-bit Mikrorechnern angeboten.\n\n\n\n\n\n"}
{"id": "433245", "url": "https://de.wikipedia.org/wiki?curid=433245", "title": "Avira Antivirus", "text": "Avira Antivirus\n\nAvira Antivirus (bis 2011 Avira AntiVir) ist ein Antivirenprogramm des deutschen Software-Herstellers \"Avira\".\n\nEin aufgespannter Regenschirm in verschiedenen Formen ist das traditionelle Zeichen der Firma sowie ihres Hauptproduktes \"Avira Antivirus\" und der Auerbach-Stiftung.\n\nFür Anwender ist der Einsatz der Freeware-Version \"Avira Free Antivirus\" kostenlos, für einen erweiterten Funktionsumfang oder die Anforderungen im kommerziellen Umfeld stehen als kostenpflichtige Varianten für Privatkunden \"Avira Antivirus Pro\" und \"Avira Internet Security Suite\" zur Verfügung. Avira bietet Kunden einige weitere kostenlose Programme im Bereich Computersicherheit an, Geschäftskunden außerdem Funktionen zum Schutz von Personalcomputern sowie Datei-, Mail- und Proxy-Servern für die Betriebssysteme \"Windows\" und \"Linux\". Die Unterstützung der Varianten für PDAs und Geräte mit \"Windows Mobile\" wurde Ende 2011 eingestellt.\n\n1988 brachte das Unternehmen \"H+BEDV Datentechnik\" das Produkt \"AntiVir\" auf den Markt, eines der ersten professionellen Antivirenprogramme überhaupt. Im Juni 2004 wurde in AntiVir ein Schutz gegen Internet-Dialer integriert, den zu dieser Zeit weltweit kein anderes Produkt enthielt. Aufgrund dessen verlieh das Land Baden-Württemberg dem Hersteller im Rahmen einer Mittelstandsinitiative die Auszeichnung „Innovation des Monats“. Im selben Jahr erschien eine AntiVir-Portierung für Linux. Später wurden neben den Windows-Varianten, auch die unixoiden Betriebssysteme \"FreeBSD\", \"OpenBSD\" und \"Solaris\" unterstützt.\n\nMit der Freigabe der Version 2012 benannte Avira seine Produktpalette um. Die zuvor genutzte Marke \"AntiVir\" wurde aufgegeben; aus \"Avira Antivir Personal\", der kostenlosen Variante, wurde \"Avira Free Antivirus\", aus \"Avira AntiVir Premium\" wurde \"Avira Antivirus Premium\" und \"Avira Premium Security Suite\" wurde zu \"Avira Internet Security\". Am 9. April 2013 wurde bekannt gegeben, dass \"Avira Free Antivirus\" nun \"Windows 8\" unterstützt und Microsoft das Produkt entsprechend zertifiziert hat.\n\nAm wurde die Unterstützung für Linux eingestellt. Bestehende Versionen sollen jedoch laut Hersteller noch bis gepflegt werden.\n\nAvira Antivirus wehrt Trojaner, Würmer und Backdoors ab, schützt vor Dialern und Rootkits. Infizierte Dateien können in den meisten Fällen repariert werden. Integriert ist unter anderem ein Virenscanner, der den Datenverkehr im System in Echtzeit scannt und im Notfall Alarm schlägt. Dabei kann das Programm auf eine Heuristik-Funktion zum Schutz vor bislang unbekannten Viren zurückgreifen. Bei Verdacht auf einen Fehlalarm oder bisher unerkannte Malware besteht die Möglichkeit, die betroffenen Dateien beim Hersteller für eine genaue Analyse hochzuladen. Da der Nutzen eines Antivirenprogramms umso höher ist, je aktueller die verwendeten Virendefinitionen sind, stellt Avira mehrmals täglich in unregelmäßigen Abständen neue Virendefinitionsdateien zur Verfügung.\n\nAb Version 7.06 verfügt Avira Antivirus über einen Prozess-Schutz, dessen Ziel es ist, das Beenden des Virenscanners durch ein anderes Programm zu verhindern. Mit Version 8.1 wurde das Programm optisch überarbeitet und modularisiert. Die im März 2009 erschienene Version 9 brachte eine Unterstützung für Dualcore-Prozessoren und in der Freeware-Variante einen Spyware-Schutz sowie die Unterstützung für leistungsschwächere Netbooks.\n\nVersion 10 der Software erschien am 23. März 2010 und enthält die Mustererkennung ProActiv, die unbekannte Malware anhand bestimmter Aktionsmuster erkennen soll. Darüber hinaus wurde die Benutzeroberfläche überarbeitet und die generische Reparatur verbessert. Damit sollen nun infizierte Dateien und die Windows-Registrierungsdatenbank besser von Malware gesäubert werden können. Am 4. Oktober 2011 erschien die Version 2012 von Avira Antivirus. Sie soll ressourcenschonender sein und Viren effektiver entfernen. Zudem wurde die Oberfläche stark überarbeitet.\n\nSeit 25. Juli 2016 wird auch bei den Pro Versionen per Update im Systemtray der bisher nur bei Avira Free obligate Avira Launcher installiert, der für andere Produkte von Avira wirbt.\n\nAntivirus prüft in der kostenfreien Variante den E-Mail-Verkehr nicht direkt beim Versand oder Empfang. Der Virenscanner wird aber bei jedem Dateizugriff des E-Mail-Programms aktiv. Die ab Version 10 vorhandene Mustererkennung ist in der kostenlosen Fassung ebenso nicht enthalten. Seit dem 1. Oktober 2013 zeigt das Programm keine tägliche Werbeeinblendung mehr an, sondern weist regelmäßig auf die verfügbaren Premiumversionen des Programms hin.\n\nKostenlose Varianten für Mobilgeräte mit dem Betriebssystem Android gibt es unter dem Namen \"Avira Antivirus Security\" und für iOS unter dem Namen \"Avira Mobile Security\".\n\n"}
{"id": "437772", "url": "https://de.wikipedia.org/wiki?curid=437772", "title": "Internetverbindungsfreigabe", "text": "Internetverbindungsfreigabe\n\nAls Internetverbindungsfreigabe (engl. Internet Connection Sharing, kurz ICS) bezeichnet man eine Funktionalität des Betriebssystems Microsoft Windows, die es ohne größeren technischen Aufwand ermöglicht, mit einem oder mehreren Windows-Computern eine Internet-Verbindung aufzubauen.\n\nDabei stellt der Computer, der ICS zur Verfügung stellt, die Dienste Network Address Translation (NAT), DHCP-Server (\"Dynamic Host Configuration Protocol\") und DNS-Server (\"Domain Name Service\") zur Verfügung und macht es somit möglich, dass Computer-(Clients), die per LAN angeschlossen sind, dessen Internetanbindung nutzen können, ohne dass weitere Dienste oder Server vorhanden sein müssen. \n\nUnter Windows ist die Funktion seit Windows 98 SE vorhanden. Allerdings müssen dort alle Clients im Subnetz 192.168.0.0/24 sein. Bei Windows 2000/XP/Vista ist dies das Netzwerksegment 192.168.0.x/24, wobei der ICS-Server die 192.168.0.1 bekommt. Diese ist änderbar. Will man mit Windows 98 SE bzw. ME die Internetverbindungsfreigabe eines Windows aus der NT-Reihe verwenden, so muss man, wie in der Hilfe beschrieben, einen besonderen Client verwenden. In der Praxis gestaltet sich die Einrichtung einer Internetverbindungsfreigabe unter Windows 98SE und ME jedoch oft sehr mühsam und war sehr fehleranfällig – ab Windows 2000 Professional ist ICS problemlos möglich. Voraussetzung ist jedoch, dass kein anderes Gerät den Dienst DHCP-Server im selben Netzwerk zur Verfügung stellt, denn dieser darf immer nur einmal pro Netzwerk verfügbar sein, da der Zugriff darauf nicht steuerbar ist. Moderne DSL-Router oder WLAN-Router bieten oft auch einen DHCP-Server-Dienst an. Bei Einsatz von ICS muss der DHCP des Routers abgeschaltet werden, ansonsten kann es zu doppelten IP-Adressen kommen.\n\nMit dem Aufkommen kostengünstiger Hardware-Router (oft auch als WLAN-Variante erhältlich), die zumeist verbilligt mit Online-Verträgen angeboten werden, ist der Gebrauch des Windows-eigenen \"Internet Connection Sharing\" zurückgegangen. Gründe hierfür sind die recht einfache Installation und der größtenteils fehlerfreie Betrieb der Router. Diese beiden Punkte waren insbesondere bei den Betriebssystem-Varianten Windows 98 SE und Windows ME oft ein größeres Problem.\n\nAuch unter Windows XP ist die Einrichtung einer Internetverbindungsfreigabe nach wie vor sehr fehleranfällig, da der Dienst oftmals nicht wie gewünscht aktiviert oder deaktiviert wird. Bei der Zuweisung von IP Adressen nimmt ICS keine Überprüfung auf Konflikte mit statischen Adressen vor, die bereits von Computern im Netzwerk verwendet werden. Standardmäßig bekommt der Rechner, der ICS aktiviert, die IP-Adresse 192.168.0.1. Diese ist jedoch änderbar, so können Probleme durch manuelles umstellen der IP-Adresse beseitigt werden.\n\nAm 30. Oktober 2006 entdeckten Spezialisten die Möglichkeit, eine DoS-Attacke auf den Dienst auszuführen. Die Schwachstelle entsteht durch eine Null-Zeiger-Referenzierung in den Windows NAT-Helper-Komponenten (ipnathlp.dll). Angreifer können über eine manipulierte DNS-Abfrage den Dienst zum Absturz bringen. Zurzeit steht kein Patch bereit.\n\n"}
{"id": "437815", "url": "https://de.wikipedia.org/wiki?curid=437815", "title": "Spooling", "text": "Spooling\n\nSpooling (aus engl. ' für „[auf]spulen“; und dies zur Abkürzung für engl. ' oder \"\") ist ein Vorgang z. B. in Betriebssystemen, bei welchem zu bearbeitende Aufträge (etwa Druckaufträge) in einem Puffer im Speicher oder auf einem externen Datenspeicher gelagert werden, bevor sie der eigentlichen Verarbeitung zugeleitet werden.\n\nDie Abarbeitung der im Puffer gehaltenen Druckaufträge durch das verarbeitende System erfolgt als Stapelverarbeitung. Spooling wird vor allem dann verwendet, wenn die Datenausgabe deutlich schneller erzeugt wird, als das Zielgerät die Verarbeitung vornehmen kann.\n\nDurch diese Trennung von Produktion und Weiterverarbeitung oder Ausgabe der Daten ist es möglich, die Auslastung der Teilsysteme zu verbessern. So können die produzierenden Prozesse ohne Verzögerung weiterarbeiten (solange Speicherplatz im Puffer vorhanden ist), obwohl die Verarbeitung der Ausgabe langsamer erfolgt. Das verarbeitende System erledigt dann zurückliegende Aufträge, während das produzierende System bereits neue Aufgaben erledigt.\n\nTypisches Beispiel ist hier die Druckerwarteschlange, in der Druckaufträge gesammelt und nacheinander abgearbeitet werden. Auch Mailserver sammeln zu versendende Mails üblicherweise in einem Spool-Verzeichnis, von dem aus sie dann verschickt werden.\n\nSpooling bietet mehrere Vorteile:\n\n\nAls Spooler bezeichnet man ein Systemprogramm oder einen Dienst, welcher Aufgaben (z. B. Druckaufträge) von Anwendungsprogrammen in eine Warteschlange setzt und von dort aus an das Ziel (z. B. Drucker) übergibt.\nZum Beispiel werden Druckaufträge nicht direkt an einen Drucker geschickt, sondern an den Spooler. Dieses Systemprogramm nimmt die Aufträge der Benutzer entgegen und wartet, bis das Gerät wieder einen Ausdruck verarbeiten kann. Wenn das zugehörige Gerät frei ist, wird im Regelfall der älteste Druckauftrag verarbeitet, bis schließlich alle wartenden Aufträge abgearbeitet sind. Ein guter Spooler ermöglicht es dem Benutzer die Reihenfolge von Aufträgen in der Warteschlange zu ändern oder Aufträge abzubrechen.\n\nIn der Informationstechnik kennt man sowohl den Druckerspooler als auch den Plotterspooler.\n\n\"Spool\" wird meist als Abkürzung – oder genauer als Initialwort – der englischen Bezeichnung ' angesehen (zu dt. etwa \"Gleichzeitiger Gerätebetrieb während der Verarbeitung\"). In der Tat war auf Großrechnern der Speicherplatz auf Festplatten und im Hauptspeicher wesentlich teurer als der Speicherplatz auf Magnetbändern, sodass Druckaufträge auf ein Band (oder auf \"Spulen\", engl. ') geschrieben wurden, das dann vom Drucksystem gelesen wurde. Der Begriff zielt in seiner Bedeutung jedoch auf die Datenausgabe im Hintergrund, also Aufspulen/Abspulen im übertragenen Sinne, nicht auf physisches Spulen.\n\n"}
{"id": "438427", "url": "https://de.wikipedia.org/wiki?curid=438427", "title": "Devfs", "text": "Devfs\n\ndevfs (engl. Abk. \"Device Filesystem\", übersetzt \"Gerätedateisystem\") ist ein spezielles Dateisystem für viele Unix-artige Betriebssysteme. Es dient der Verwaltung der Gerätedateien. Da die Implementierung von devfs im Linux-Kernel einige Unzulänglichkeiten hat und nicht mehr aktiv weiterentwickelt wird, wurde udev entwickelt. Seit Ende Juni 2006 ist devfs nicht mehr Bestandteil des Linux-Kernels und wurde vollständig durch udev ersetzt.\n\nAllgemein werden auf Unix-Systemen viele I/O-Geräte wie Festplatten, Drucker, virtuelle Terminals und Ähnliches als spezielle Dateien (Gerätedateien) behandelt. Soll nun eine Operation an dem Gerät vorgenommen werden, wird diese Operation an der Gerätedatei vorgenommen – das zugrundeliegende Dateisystem der Gerätedateien wandelt dann die jeweilige Operation auf die Datei in eine entsprechende Operation auf das Gerät um.\n\nZu den Aufgaben von devfs gehört dabei das Erschaffen und Löschen solcher Dateien, wenn beispielsweise neue Geräte (wie USB-Sticks) angebracht und wieder entfernt werden. Ebenso ist devfs auch für die Benutzerrechte der Gerätedateien zuständig, mithin auch für die Rechte, die einzelne Nutzer beim Zugriff auf die Geräte haben.\n\nGegenüber dem statischen /dev-System hat devfs eine Reihe von Vorteilen:\n\n\nTrotz der Vorteile von devfs gegenüber dem klassischen Modell birgt es auch eine Reihe von Nachteilen. Diese sind unter anderem:\n\n\nDiesen Problemen begegnete man beim Linux-Betriebssystem mit der Ersetzung von devfs durch das Softwarepaket udev.\n\nIm Linux-Kernelbaum 2.2 wurde das \"devfs\" eingeführt. Die zugrundeliegende Idee war, dass die Kernelmodule selbst Informationen zu den Namen der Gerätedateien, die sie erzeugen, neben den Minor- und Majornummern sowie dem Typ mit sich führen. Dadurch konnte der Kernel erstmals die Erzeugung der Gerätedateien selbst übernehmen.\n\nDie benötigten bzw. vom Kernel und seinen Modulen gestellten Gerätedateien hat der Kernel anschließend automatisch in dem devfs-Dateisystem mit Hilfe des \"devfsd\"-Daemons erstellt. Das Dateisystem wurde dabei üblicherweise im Verzeichnis /dev gemountet.\n\nDer \"devfsd\"-Daemon (\"Device Filesystem Daemon\") war in der Datei /etc/devfsd.conf konfigurierbar. So konnte man unter anderem die gewünschten Berechtigungen oder Besitzverhältnisse je Gerät oder Gerätegruppe eintragen.\n\n"}
{"id": "438977", "url": "https://de.wikipedia.org/wiki?curid=438977", "title": "Turtle-Grafik", "text": "Turtle-Grafik\n\nMit Turtle-Grafik, auch Igelgrafik, wird eine Bildbeschreibungssprache bezeichnet, bei der man sich vorstellt, dass ein stifttragender Roboter (die Schildkröte, engl. „turtle“) sich auf der Zeichenebene bewegt und mit einfachen Kommandos, wie Stift heben, senken, vorwärts laufen und drehen, gesteuert werden kann. Diese Idee wurde mehrfach realisiert, zum Beispiel als Steuersprache für Stiftplotter (HPGL), als Teil der Programmiersprache für Heimcomputer (BASIC, Pascal auf Amiga, Atari) und als Grundidee der pädagogischen Programmiersprache LOGO.\n\nIm schulischen Bereich werden Turtle-Grafik ähnliche Grafiksysteme weiterhin gerne eingesetzt, weil so ein motivierender, spielerischer Einstieg gefördert und die geometrische Vorstellung besser geschult wird als bei einem Zugang über absolute Koordinaten. Gegenüber der klassischen Turtle-Grafik von LOGO sind diese Systeme teils erheblich erweitert worden, so dass sich teilweise echte GUI-Anwendungen (auf schulischem Niveau) damit erstellen lassen. Dies gilt etwa für die Python-Module xturtle und frog.\n\nIm professionellen Bereich wurde der Turtle-Ansatz zugunsten koordinatenbasierter Grafikbeschreibung wieder fallengelassen und wird heute im Wesentlichen nur noch zur Darstellung von Fraktalen mittels Lindenmayer-Systemen verwendet.\n\nDie Schildkröte besitzt keinen Speicher und führt Kommandos sofort aus.\n\n\nEs wird ein Stack zum Speichern des jeweiligen aktuellen Zustands eingeführt. Ein Zustand besteht aus der aktuellen Position und Richtung der Schildkröte.\n\nHierzu gibt es die Zeichen [ und ] mit folgender Bedeutung:\n\n\nInnerhalb eines Klammerpaars kann also eine Teilfigur, z. B. ein im Leeren endender Zweig, gezeichnet werden.\n\n\n"}
{"id": "440261", "url": "https://de.wikipedia.org/wiki?curid=440261", "title": "VirtualDub", "text": "VirtualDub\n\nVirtualDub ist eine freie Software zum Bearbeiten und Erstellen von Videodateien unter Windows. Vorkompilierte Ausgaben des Programms sind für alle Windows-Versionen ab Windows 98 verfügbar.\n\nAutor Avery Lee wollte mit „VirtualDub“ eine schnelle, lineare Videobearbeitung mit Hilfe einer grafischen Benutzeroberfläche schaffen. Unterstützt wird primär das AVI-Containerformat. Dazu griff VirtualDub lange Zeit einzig auf die Video-for-Windows-Schnittstelle zu, welche wiederum meist einen zuvor installierten, kompatiblen Codec benötigt. Neben AVI werden noch das MPEG-1-Format unterstützt sowie die Grafikformate JPEG, BMP, TGA, PNG und JFIF. In Version 1.7.6 wurde eine Plug-in-Schnittstelle für weitere, nachrüstbare Dateiformate implementiert. Darüber hinaus kann VirtualDub den Datenstrom einer geeigneten Videoquelle (etwa einer Kamera oder TV-Karte) aufzeichnen. Unabhängig vom Quellmaterial kann VirtualDub Videodateien nur im AVI-Format abspeichern.\n\nZum Bearbeiten liefert VirtualDub einige Videofilter mit, weitere können über die Plug-in-Schnittstelle hinzugefügt werden, u. a. Deshaker. Die automatische Bearbeitung mehrerer Videos (Stapelverarbeitung) kann mit Hilfe von Skripten bewerkstelligt werden. Ferner kann VirtualDub auch als Frameserver fungieren.\n\nLange Zeit war die Anzahl der unterstützten Video- und Audioformate von VirtualDub vergleichsweise gering. Beispielsweise konnte das Programm das weit verbreitete MPEG-2-Videoformat (Standard bei digitalem Fernsehen und bei handelsüblichen Video-DVDs) nicht einlesen. Abhilfe schuf hier nur die Verwendung eines Frameservers wie AviSynth, welcher das Material zuerst dekodierte und es dann an VirtualDub weiterreichte. Daher veröffentlichten Drittpersonen Modifikationen von VirtualDub, die um die native Unterstützung anderer Formate ergänzt wurden. Die populärsten Vertreter waren \"Nandub\" (eines der ersten Videoprogramme mit Two-pass encoding auf Basis des DivX-Codec in Version 3.11), \"VirtualDubMod\" (eine Verschmelzung vieler Modifikationen, z. B. für den Support von Matroska oder Ogg Media) und \"VirtualDub-MPEG2\" (einzig durch MPEG2-Unterstützung ergänzt). Die Entwicklung der Modifikationen kam jedoch wieder zum Erliegen, da VirtualDub inzwischen über Plug-ins erweitert werden kann.\n\n"}
{"id": "441221", "url": "https://de.wikipedia.org/wiki?curid=441221", "title": "Homogene Koordinaten", "text": "Homogene Koordinaten\n\nIn der projektiven Geometrie werden homogene Koordinaten verwendet, um Punkte in einem projektiven Raum durch Zahlenwerte darzustellen und damit geometrische Probleme einer rechnerischen Bearbeitung zugänglich zu machen. Im Vergleich zu den normalerweise verwendeten (inhomogenen) Koordinaten, die jeden Punkt eindeutig identifizieren, haben homogene Koordinaten die Eigenschaft, dass sie für einen vorgegebenen Punkt nicht eindeutig bestimmt sind. Der Vorteil homogener Koordinaten liegt in der einheitlichen Darstellung der Elemente eines projektiven Raums, bei der Fernelemente keine Sonderrolle mehr spielen. Zudem lassen sich durch die Verwendung homogener Koordinaten alle Kollineationen, und damit auch Parallelverschiebungen, einheitlich durch lineare Abbildungen und damit durch Matrizen beschreiben. Aus diesem Grund spielen homogene Koordinaten im dreidimensionalen Raum eine wichtige Rolle in der Computergrafik.\n\nHomogene Koordinaten lassen sich am besten am Beispiel der reellen projektiven Ebene verstehen. Die projektive Gerade und höherdimensionale projektive Räume werden analog mit Koordinaten versehen. Im \"homogenen Modell\" der reellen projektiven Ebene entspricht\n\n\nEin Punkt liegt dann auf einer Gerade, falls die zum Punkt gehörige Ursprungsgerade in der zur Gerade gehörenden Ursprungsebene liegt (siehe Bild). Ein Punkt formula_1 der projektiven Ebene kann damit durch einen beliebigen Punkt formula_2 der zugehörigen Ursprungsgerade beschrieben werden. Man schreibt dann\n\nund nennt formula_4 \"homogene Koordinaten\" des Punktes formula_1. Offensichtlich gilt\n\nfür jede Zahl formula_7. Eine Gerade der projektiven Ebene wird dann durch eine (homogene) Ebenengleichung formula_8 beschrieben. In diesem Modell überzeugt man sich leicht von den grundlegenden Inzidenzeigenschaften einer projektiven Ebene:\n\n\nBeim \"inhomogenen Modell\" der reellen projektiven Ebene geht man von der Anschauungsebene aus und ergänzt die Punktmenge durch Fernpunkte so, dass sich je zwei Geraden, also auch parallele Geraden, in genau einem Punkt schneiden.\n\nNach Einführung von kartesischen Koordinaten fügt man jeder Gerade formula_9 mit der Steigung formula_10 üblicherweise den Fernpunkt formula_11 hinzu. Die Geraden formula_12 (Parallelen zur y-Achse) erhalten den Fernpunkt formula_13 (siehe Bild). Da auch je zwei Fernpunkte durch eine Gerade verbunden sein müssen, fasst man alle Fernpunkte zur Ferngerade formula_14 zusammen. Man prüft leicht nach, dass die neue Inzidenzstruktur (erweiterte Anschauungsebene) die wesentlichen Eigenschaften einer projektiven Ebene erfüllt:\n\n\nUm zu zeigen, dass das homogene und das inhomogene Modell der reellen projektiven Ebene isomorph sind, wird das inhomogene Modell derart in den dreidimensionalen Raum eingebettet, dass die Punkte der Anschauungsebene die Gleichung formula_15 erfüllen: formula_16. Damit wird dem Punkt formula_17 des inhomogenen Modells der Punkt formula_18 des homogenen Modells zugeordnet. Ein Punkt formula_19 wird dabei auf denjenigen Punkt formula_20 abgebildet, dessen homogene Koordinaten die Gleichung formula_21 erfüllen. Also kann man dem allen inhomogenen Geraden formula_22 gemeinsamen Fernpunkt formula_11 die allen Ursprungsebenen formula_24 gemeinsame Ursprungsgerade formula_25 zuordnen (siehe Bild).\n\nDer große Vorteil homogener Koordinaten gegenüber den anschaulicheren inhomogenen Koordinaten liegt in der homogenen Darstellung der Punkte und Geraden. Fernpunkte und Ferngerade spielen formal keine Sonderrolle mehr und alle Kollineationen, einschließlich der Translationen, lassen sich einheitlich durch lineare Abbildungen (Matrizen) beschreiben. Letzteres spielt insbesondere in der Computergrafik eine große Rolle.\n\nZusammenfassung:\n\nUmkehrung:\n\nZuordnung der Geraden:\n\nDie Einbettung wird in der Literatur nicht einheitlich dargestellt. So können die homogenen Koordinaten auch mit formula_35 bezeichnet sein oder die Ferngerade die Gleichung formula_36 erfüllen.\n\nJeder Punkt in einem formula_37-dimensionalen projektiven Raum kann durch formula_38 Koordinaten beschrieben werden. Der projektive Raum formula_39 über dem Körper formula_40 ist definiert als der Faktorraum\n\ndes Koordinatenraums formula_42 ohne den Nullvektor formula_43 bezüglich der Äquivalenzrelation\n\nDie homogenen Koordinaten eines Punkts formula_45 des projektiven Raums sind dann formula_46, wobei formula_47 ein beliebiges Element der entsprechenden Äquivalenzklasse ist. Homogene Koordinaten werden häufig durch\n\nnotiert, wobei die Doppelpunkte andeuten sollen, dass die Darstellung nur bis auf Multiplikation mit einer Konstante eindeutig ist.\n\nIm Folgenden werden Beispiele und schließlich alle affinen Abbildungen zunächst im inhomogenen Modell zu Projektivitäten fortgesetzt und dann im homogenen Modell durch Matrizen beschrieben. Es ist aber darauf zu achten, dass die jeweiligen Matrizen (im homogenen Modell) nicht eindeutig bestimmt sind. Denn nicht nur die Einheitsmatrix formula_50, sondern jedes vielfache formula_51 (Skalierungsmatrix im formula_52) lässt jede Ursprungsgerade (projektiver Punkt) invariant. Man kann also die Matrix einer Projektivität mit einer beliebigen Skalierungsmatrix multiplizieren, ohne dass sich die zugehörige Projektivität ändert.\n\nDie Fortsetzungen der Affinitäten liefern nur solche Kollineationen, die die Ferngerade als Ganzes fest lassen. Die zugehörigen Matrizen im homogenen Modell zeichnen sich dadurch aus, dass sie in den ersten beiden Spalten an der 3. Stelle eine 0 haben. Es treten also noch nicht alle Matrizen auf. Aber es gilt:\nZ. B.: Die Matrix formula_54 induziert eine Projektivität, die im inhomogenen Modell die Ferngerade formula_14 mit der y-Achse vertauscht und den Punkt formula_17 mit dem Punkt formula_57. (Die Punkte formula_58 sind Fixpunkte.) Sie ist also keine Fortsetzung einer Affinität.\n\nWill man eine \"beliebige\" Projektivität im inhomogenen Modell darstellen, so ist dies nur mit \"gebrochen\" linearen Ausdrücken möglich. Hier zeigt sich die Stärke des homogenen Modells. Es kommt mit linearen Ausdrücken aus.\n\nHomogene Koordinaten können analog zum ebenen Fall auch im 3-dimensionalen projektiven Raum eingeführt werden. Es gibt dann 4 homogene Koordinaten und die Abbildungsmatrizen der Projektivitäten sind 4×4-Matrizen.\nIn der Computergrafik werden nicht nur Transformationen des Raums in homogenen Koordinaten durch 4×4-Matrizen dargestellt, sondern auch Projektionen des Raumes auf eine Ebene (siehe Grafikpipeline). Da bei solchen Projektionen die Dimension verkleinert wird (von 3 auf 2) haben die zugehörigen Matrizen die Determinante 0. Hier zwei Beispiele von Projektionsmatrizen:\n\nDie erste Matrix beschreibt die \"Zentralprojektion\" vom Augpunkt formula_59 aus auf die x-y-Ebene. Die zweite Matrix bewirkt eine \"Orthogonalprojektion\" auf die x-y-Ebene.\nHomogene Koordinaten werden innerhalb der Geometrie benutzt um\nIn der Computergrafik werden homogene Koordinaten verwendet um\nIn der Robotik lassen sich hintereinanderliegende Achsen durch Verkettung ihrer zugehörigen homogenen Matrizen beschreiben. Hierfür wird als Standardverfahren die Denavit-Hartenberg-Transformation angewandt.\n\n\n"}
{"id": "441375", "url": "https://de.wikipedia.org/wiki?curid=441375", "title": "PC 1715", "text": "PC 1715\n\nDer PC 1715 oder robotron 1715 wurde nach seiner Einführung im Jahr 1985 der Standardcomputer der DDR in den 1980er-Jahren. Darüber hinaus wurde er unter der SKR-Chiffre \"CM 1904\" in die UdSSR und andere RGW-Staaten exportiert.\n\nDem PC 1715 diente ein U880-Chip mit 2,5 MHz als Prozessor, der baugleich mit dem Prozessor Zilog Z80 war. (Ein weiterer U880 fand sich in der zugehörigen Tastatur als deren Controller.) Die weitere Ausstattung umfasste 64 KB RAM, 2 KB ROM sowie zwei Diskettenlaufwerke zu je 5,25 Zoll mit anfangs jeweils 160 KB und bei späteren Geräten – abhängig vom Betriebssystem – bis je 800 KB Speicherkapazität.\n\nDer PC 1715 ist ein Desktop-PC, dessen Gehäuse 50 cm breit, 40 cm tief und 14 cm hoch ist. Gehäuse, Monitor und Tastatur wiegen zusammen 12,8 kg. Die separate Tastatur besteht aus 98 Tasten gemäß der international standardisierten QWERTY-Tastaturbelegung mit einem zusätzlichen Ziffernblock sowie 15 Funktionstasten. Zur visuellen Darstellung dient ein Grünmonitor mit den Auflösungen 64×16 oder 80×24 Zeichen. Es gibt keine Rastergrafik. Der Rechner hat keine Festplatte, dafür jedoch zwei eingebaute Diskettenlaufwerke, die um zwei externe Diskettenlaufwerke erweitert werden konnten. In der Praxis verwenden die Benutzer ein Laufwerk für die Software und das zweite zur Speicherung der Daten. Als Software lieferte der Hersteller eine auf die Hardware angepasste Version des Betriebssystems CP/M unter dem Namen SCP. Alternative Betriebssysteme waren BROS und JAMB zur Kompatibilität mit einem russischen Rechnersystem. Als Textverarbeitung lieferte Robotron TP, ein Duplikat von WordStar. Für die Anwenderprogrammierung standen die Programmiersprachen BASIC, Pascal, Assembler und Fortran sowie das Datenbanksystem Redabas (eine dBASE-II-Kopie) zur Verfügung.\n\nAls Nachfolgemodell des A 5120 war der \"robotron 1715\" anwenderprogrammseitig softwarekompatibel zu seinem Vorgänger. Der Hersteller \"VEB Robotron Büromaschinenwerk „Ernst Thälmann“ Sömmerda\" lieferte ab 1987 den Nachfolger \"PC 1715W\" mit höherer Taktfrequenz (4 MHz), 256 KB RAM und der Version 3.0 des Betriebssystems SCP aus. Der Computer konnte nicht im Einzelhandel, sondern nur von staatlichen Betrieben und Behörden sowie von Hoch- und Fachschulen erworben werden. Bei der Verrechnung veranschlagte der Produzent anfänglich 19.000 Mark. Vom PC 1715 und PC 1715W wurden bis 1990 93.096 Exemplare hergestellt, von denen circa 50.000 Stück in die Sowjetunion exportiert wurden.\n\n\n"}
{"id": "442110", "url": "https://de.wikipedia.org/wiki?curid=442110", "title": "System X", "text": "System X\n\nSystem X ist ein Supercomputer für allgemeine Forschungsaufgaben, der an der Virginia Tech University in den USA steht. Früher auch bekannt als Terascale Cluster oder Big Mac (wegen seiner Bestandteile aus \"PowerMacs\").\n\nDer Computer bestand ursprünglich (Anfang 2004) aus 1100 Rechnern \"Apple PowerMac PowerPC 970 G5 Dual 2,0 GHz\", erzielte eine Rechenleistung von etwa 10 Teraflops und sprang damals direkt auf den dritten Platz der \"TOP500\"-Liste der Supercomputer vom November 2003.\n\nDer Terascale Cluster wurde einige Monate später nach Verhandlungen mit Apple durch 1100 \"Xserve PowerPC 970FX G5 Dual 2,3 GHz\" ersetzt. Er erzielte nun bei einem Benchmark etwa 12,25 TeraFLOPS und erreichte im November des Jahres 2004 Platz sieben, im Juni 2007 stand er auf dem 71. Platz der TOP500-Liste.\n\n"}
{"id": "442804", "url": "https://de.wikipedia.org/wiki?curid=442804", "title": "Tomsrtbt", "text": "Tomsrtbt\n\ntomsrtbt (ausgesprochen: Tom’s Root Boot) ist eine sehr kleine Linux-Distribution. Es ist die Kurzform für „Tom’s Floppy, die ein Root-Dateisystem beinhaltet und bootfähig ist“. Der Autor bewirbt sie als „Das meiste GNU/Linux auf einer Diskette“; es enthält viele gebräuchliche Linuxbefehle für die Systemwiederherstellung (Linux und andere Betriebssysteme) und die Erstellung von Linuxrechnern ohne Festplatte. Es stellt auch Treiber für diverse Hardware und Netzwerkfunktionalität bereit.\n\nDie Distribution kann aus Microsoft Windows oder Linux heraus erstellt werden, indem man eine Standard-1,44-MB-Diskette mit hoher Dichte (1,72 MB) formatiert und das Abbild auf die Diskette schreibt. Es ist in der Lage, Dateisysteme vieler Betriebssysteme zu lesen und zu beschreiben, einschließlich ext2 (in Linux verwendet), FAT (in MS-DOS und einigen Windows-Versionen verwendet), NTFS (in Windows NT, 2000, und XP verwendet), und Minix (im Minix-Betriebssystem verwendet).\n\nDie fertige Diskette kann dann als Rettungs- oder Analysediskette eingesetzt werden. So kann die Hardwarekonfiguration eines Rechners gelistet werden oder auf die Daten des Rechners zugegriffen werden.\n\nDie letzte Version von tomsrtbt erschien im Mai 2002. Seitdem wird es nicht mehr weiter entwickelt. Es gibt einige Nachfolger, viele auch auf CD, da das CD-Laufwerk das Diskettenlaufwerk mittlerweile schon größtenteils verdrängt hat. In den Jahren vor 2002 genoss tomsrtbt einen recht hohen Bekanntheitsgrad.\n\n"}
{"id": "443995", "url": "https://de.wikipedia.org/wiki?curid=443995", "title": "QEMU", "text": "QEMU\n\nQEMU (von englisch „Quick Emulator“) ist eine freie Virtualisierungssoftware, die die gesamte Hardware eines Computers emuliert und durch die dynamische Übersetzung der Prozessorinstruktionen des Gastprozessors () in Instruktionen für den Wirtprozessor () eine sehr gute Ausführungsgeschwindigkeit erreicht.\n\nQEMU emuliert Systeme mit den folgenden Prozessorarchitekturen: x86, x64, PowerPC (32- und 64-Bit), ARM (32- und 64-Bit), Alpha, CRIS, LatticeMico32, m68k bzw. Coldfire, MicroBlaze, MIPS, Moxie, SH-4, S/390, Sparc32/64, TriCore, OpenRISC, Unicore und Xtensa (Stand 2015).\n\nQEMU ist auf den Betriebssystemen GNU/​Linux, Windows, FreeBSD, NetBSD, OpenBSD, OpenSolaris, macOS (Mac OS X, nicht klassisches Mac OS) und Haiku lauffähig. Es kann den gesamten Status einer virtuellen Maschine so speichern, dass diese auf ein anderes Wirtsystem () übertragen werden und dort weiterlaufen kann (Live-Migration).\n\nUnter Linux, BSD und macOS unterstützt QEMU auch die „Userspace“-Emulation. Diese ermöglicht es, dass Programme, die für eine andere Architektur kompiliert wurden, im Userspace betrieben werden können. Dabei werden die Prozessoren x86, PowerPC (32- und 64-Bit), Alpha, ARM (32- und 64-Bit), CRIS, MicroBlaze, 32-Bit-MIPS, S/390, SH-4, Sparc32/64, m68k/ColdFire und Unicore unterstützt.\n\nFür virtuelle x86-Maschinen auf x86-Rechnern stand mit \"kqemu\" ein Zusatzmodul bereit, das durch die native Ausführung von geeigneten Teilen des Maschinencodes einen erheblichen Geschwindigkeitszuwachs bewirkte. Da die Weiterentwicklung seit QEMU 0.12 auf KVM fokussiert ist, kann das Beschleunigermodul \"kqemu\" nur in QEMU bis Version 0.11 verwendet werden.\n\nAktuelle Versionen von QEMU nutzen Hardware-Virtualisierung – auf Linux-Hosts die Kernel-based Virtual Machine (KVM), die Prozessoren mit den Hardware-Virtualisierungstechniken von Intel (Intel VT) oder AMD (AMD-V) unterstützt. Für macOS und Windows steht mit dem Intel Hardware Accelerated Execution Manager (HAXM) eine vergleichbare Lösung zur Verfügung. Damit erreicht QEMU auf allen gängigen Desktop-Betriebssystemen eine sehr gute Geschwindigkeit.\n\nMittels HX DOS Extender war QEMU auch in FreeDOS und DR-DOS lauffähig.\n\nEmuliert wird neben dem Hauptprozessor auch:\n\nDas verwendete PC-BIOS ist \"SeaBIOS\" vom coreboot-Projekt, das das Bochs-BIOS früherer Versionen ersetzt. Für VGA wird das Plex86-BIOS aus dem Bochs-Projekt genutzt.\n\nAls PowerPC-Firmware wird \"Open Hack'Ware\", eine Open-Firmware-kompatible Firmware, verwendet.\n\nQEMU emuliert die folgenden PowerMac-Peripheriegeräte:\n\nDie \" (PReP)\" bezeichnet einen Standard für PowerPC-basierte Computer und soll eine Referenz-Implementierung darstellen. PReP wurde bereits von der \"\" abgelöst.\n\nQEMU emuliert die folgenden PReP-Peripheriegeräte:\n\nSun-SPARC-Architektur = (Scalable Processor ARChitecture)\n\nAls BIOS der JavaStation (sun4m-Architektur) wurde bis Version 0.8.1 \"Proll\", ein PROM-Ersatz, verwendet, in Version 0.8.2 wurde es durch OpenBIOS ersetzt.\n\nQEMU emuliert die folgenden \"sun4m\"-Peripheriegeräte:\n\nÄhnlich wie für andere Virtualisierungslösungen wie VirtualBox, stehen auch für QEMU sogenannte Gasterweiterungen (englisch \"\") für verschiedene Gastbetriebssysteme zur Verfügung. Sie dienen dazu, die Integration zwischen Wirt- und Gastsystem zu erweitern bzw. Funktion und Datendurchsatz zu verbessern. Beispielsweise kann durch die Gasterweiterung im laufenden Betrieb des Gastsystems eine Gesamtsicherung durchgeführt werden – ein Stoppen, Erstellung der Sicherung und danach Neustarten des Gastsystem, um ein konsistentes Abbild zu erzeugen ist damit nicht mehr nötig. Andere Erweiterungen stellen verbesserte Schnittstellen zu virtuellen Hardwareschnittstellen wie speziellen Netzwerkkarten oder virtuelle SATA-Schnittstellen dar. Da dabei nicht herkömmliche Hardware nachgebildet wird, kann durch die speziell gestaltete virtuellen Schnittstelle ein höherer Datendurchsatz erzielt werden. Während Linux und die meisten üblichen Linuxdistributionen diese Gasterweiterungen und Unterstützung für virtuelle Geräte im Gastsystem automatisch mitbringen bzw. die Nachinstallation aus der jeweiligen Distribution erlauben, sind für Gastsysteme wie Windows die sogenannten \"Virtio\"-Treiber extra zu installieren.\n\nBei Verwendung von SPICE als Konsolenschnittstelle – SPICE wird von QEMU seit dem Jahr 2010 direkt unterstützt – erlauben die SPICE-Gasterweiterungen bei grafischen Oberflächen im Gastsystem, dass sich beispielsweise beliebige Grafikauflösungen in der virtuellen Grafikkarte durch die Wahl der Fenstergröße am Ausgabegerät vorgeben lassen.\n\nDas Starten von Live-CD- und Startdisketten-Abbildern ist möglich.\n\nUm QEMU unter Windows zu verwenden, sind keine Administratorrechte notwendig. Auf einem USB-Stick lässt sich QEMU so als portable Software verwenden. Für die Installation eines 64-Bit-Betriebssystems unter Mac OS X ist die Rosetta-Erweiterung von Apple erforderlich.\n\nViele Virtualisierungslösungen (VirtualBox, Xen, FAUmachine, Win4BSD, Win4Solaris, Win4Lin) nutzen Teile des Quelltextes von QEMU.\n\nDank der Quelloffenheit und der Konfiguration mit Startoptionen ist die Entwicklung von Werkzeug- und Hilfsprogrammen für QEMU unproblematisch. Durch den \"QEMU-Manager\" und \"AQEMU\" lässt sich QEMU über eine grafische Benutzeroberfläche bedienen.\n\n\n"}
{"id": "445667", "url": "https://de.wikipedia.org/wiki?curid=445667", "title": "SoundTracker", "text": "SoundTracker\n\nSoundTracker ist ein freies, GPL-lizenziertes Musikprogramm vom Typ Tracker, geschrieben von Michael Krause und lauffähig unter GNU/Linux, BSD, macOS, SunOS und IRIX. Es existieren auch Vorleistungen für einen Windows-Port.\n\nSoundTracker ersetzt die Funktionalität des DOS-Programms Fasttracker II (technische Details siehe dort), die Benutzeroberfläche ist jedoch deutlich übersichtlicher. Die Abspielroutinen wurden vom Open Cubic Player übernommen.\n\nSoundTracker, Schism Tracker und CheeseTracker sind die für unixoiden Plattformen bedeutende Tracker, siehe auch Liste von Trackern.\n\nDie letzte offizielle Version benötigt GTK+ 1.2, welches in den meisten Linux-Distributionen nicht mehr verfügbar ist. Es existiert ein inoffizieller GTK-2-Port, dessen GUI-Animationen jedoch aufgrund der Ineffizienz von GTK 2 extrem viel Rechenzeit fressen.\n\n\n"}
{"id": "447920", "url": "https://de.wikipedia.org/wiki?curid=447920", "title": "CorelDraw Graphics Suite", "text": "CorelDraw Graphics Suite\n\nCorelDraw Graphics Suite (eigene Schreibweise: \"CorelDRAW Graphics Suite\") ist eine insbesondere auf Windows-Systemen verbreitete Grafik- und Bildbearbeitungs-Softwaresammlung der Corel Corporation.\n\nDie Versionsnummern wurden erst bis 12 (2003) hochgezählt, in den nachfolgenden Versionen wurde die „10“ durch die römische Ziffer X ersetzt, somit mit X3 (2006) bis X8 (2016) bezeichnet. Seit 2017 wird als Versionsnummer die Jahreszahl benutzt.\n\nCorelDraw wird als Paket von Anwendungen (\"Graphics Suite\") zur Grafikbearbeitung vertrieben. Es enthält in der Version X8 folgende Komponenten:\n\n\nZudem sind verschiedene zusätzliche „Helferlein“ enthalten, wie BenVISTA PhotoZoom Pro 4 (Plugin zur Vergrößerung digitaler Bilder), ein Barcode-Assistent, Cliparts, Schriften und Sonstiges.\n\nIn den Versionen 10–12 lag zudem noch Corel Rave zur Erstellung einfacher Flash-Animationen bei.\n\nDie erste Version des Programms wurde für Microsoft Windows entwickelt und im Jahr 1989 als Version 1.2 veröffentlicht (die Versionen 1 und 1.1 waren interne Vorabversionen). Später kam eine Version für Apple Macintosh hinzu, wobei Version 11 die letzte Version für Macintosh-Systeme darstellt. Zeitweise gab es auch eine eigenständige Unix-Version (CorelDraw 3), die später durch eine Wine-basierte (CorelDraw 9) nur für Linux ersetzt und schließlich ganz aufgegeben wurde.\n\nDas Besondere an den ersten CorelDraw-Versionen war, dass Vektor-Schriften in einem proprietären Format mitgeliefert wurden. Windows 3.0 unterstützte noch keine TrueType-Schriftarten. Um trotzdem Postscript-Fonts nutzen zu können und die mitgelieferten Corel-Fonts im .wfn-Format auch als Druckerfont auf entsprechende Drucker hochladen zu können, lag diesen Versionen das Dienstprogramm WFN-Boss bei. So war Corel von den PostScript-Schriften der Firma Adobe unabhängig. Das umfangreiche Schriftenpaket, 2010 bestehend aus über tausend OpenType- und PostScript-Schriften, ist bis heute beibehalten worden. Zumindest einzelne ältere Versionen waren auch zusammen mit Ventura erhältlich. Schon die ganz frühen Versionen erlaubten auch den Export von Grafiken in einen Font (zunächst ins Corel-eigene Format), wodurch sich CorelDRAW auch als einfacher Font-Editor einsetzen lässt.\n\nDaneben war es in den ersten Versionen nur möglich, im schwarzweißen Outline-Modus zu arbeiten, die tatsächliche farbige Grafik bekam man dann erst nach Umschalten in den Vorschaumodus zu sehen. Dies war ein Zugeständnis an die damals sehr leistungsschwachen Rechner. Trotzdem dauerte das Rendern des Vorschaubildes nicht selten einige Sekunden bis Minuten, je nach Komplexität des Bildes.\n\nDie neueren Versionen beinhalten zunehmend Funktionen aus dem Desktop-Publishing und der Druckvorstufe. So gibt es eine direkte PDF-Ausgabe, eine Druckvorschau, die komplexe Montagebögen ermöglicht, und eine kalibrierte Farbvorschau, die durch die Verwendung der Trapping-Methode auch Farbmischungen anzeigt.\n\nIn der Version X5 wurde das Farbmanagement vollständig neu programmiert und entspricht nun dem weltweiten Industriestandard, der weitgehend von Adobe geprägt wurde. Das führt allerdings zu teils erheblichen Farbunterschieden beim Bearbeiten von Dateien aus vorherigen Versionen.\n\nDas eigene versionsabhängige Vektorformat hat die Dateiendung CDR. Dank dem Programm libcdr können auch LibreOffice (Draw), Inkscape, sk1 und Scribus dieses Dateiformat öffnen, dies gilt ebenso für CorelDraw Templates (CDT) und CorelDraw Presentations Exchange (CMX) Dateien.\n\n\n"}
{"id": "450723", "url": "https://de.wikipedia.org/wiki?curid=450723", "title": "MEDUSA", "text": "MEDUSA\n\nMEDUSA, (seit 2004 MEDUSA4) ist ein CAD-Programm, das besonders in den Bereichen Maschinen- und Anlagenbau sowie in der Automobil- und Zuliefer-Industrie eingesetzt wird.\n\nMEDUSA basiert auf einem plattformunabhängigen Kern, der, kombiniert mit einer plattformunabhängigen Oberfläche (XML-Konfigurationen für die Administration, Web-basierende Client-Server-Kommunikation in der Datenverwaltung) eine hohe Flexibilität der Betriebssystemstrategie ermöglichen soll. Die Software ist sowohl unter Microsoft Windows als auch unter Sun Solaris und Linux verfügbar.\n\nDie 4. Generation der MEDUSA 2D- und 3D-CAD-Produktfamilie wurde im Sommer 2004 von der Firma CAD Schroer zum kostenlosen privaten Gebrauch freigegeben. MEDUSA4 Drafting plus ist ein 2D-CAD-Programm mit allen Standard- und Automatisierungs-Werkzeugen für die Zeichnungserstellung.\n\nMEDUSA, eines der frühen weitverbreiteten Systeme, hat eine bewegte Geschichte in der CAD-Welt, die in den siebziger Jahren in Cambridge England begann. Im Jahr 1977 gründete der britische Computer-Forscher Dick Newell zusammen mit Tom Sancha die Firma Cambridge Interactive Systems (CIS), die sich vornehmlich der Entwicklung eines 2D-CAD-Systems widmete. CABLOS war eine CAD-Lösung für die Planung von elektrischen Kabelsystemen, die als Erstes im Jahr 1979 von der Firma Dowty Engineering eingesetzt wurde. Der erste Anwender in Deutschland war BMW, die CABLOS für die Planung der Autoelektronik nutzten. Bald wurde CABLOS unter dem Namen MEDUSA weiter vermarktet und weltweit bekannt. In diesem Zeitraum begann auch die Entwicklung eines eigenen 3D-Modellierungskerns für MEDUSA.\n\nIm Jahr 1980 ging CIS eine Partnerschaft mit Prime Computer ein, einem in den USA beheimateten Hersteller von Computer-Hardware. Prime bekam den Zugriff auf den MEDUSA-Quellcode, sollte der Erfolg für CIS ausbleiben. Im Jahr 1983 wurden CIS und die europäische Vertriebsschwester Applied Graphics Systems (AGS) von ComputerVision übernommen.\n\nDie deutsche AGS-Niederlassung forcierte von Anfang an die Lokalisierung, Anpassung an die DIN-Zeichnungsnormen, deutschsprachige Dokumentation und die Entwicklung umfangreicher Symbol- und Normteilbibliotheken für die Mechanikkonstruktion. MEDUSA startete deshalb in den 80er Jahren besonders erfolgreich im deutschsprachigen Maschinenbaumarkt. Die MEDUSA-Umsätze in diesem Markt betrugen Ende der 80er Jahre und Anfang der 90er mehr als die Hälfte des MEDUSA-Umsatzes weltweit. Einer der Erfolgsfaktoren war das 2D-Parametric-System, das 1983 auf den Markt kam. Es wurde zum Vorbild der späteren historienbasierten 3D-Parametric-Systeme. Richard (Dick) Newell war der Entwickler des MEDUSA Parametric-Systems bei CIS, die Brüder Vladimir Geisberg (bei Prime Computer und Computervision) und Samuel Geisberg (als Gründer von PTC) spielten wichtige Rollen bei der Umsetzung des Konzeptes in 3D-Systemen. Das Projekt von Samuel Geisberg, der auf der grünen Wiese ohne historische Altlasten begann, war das erfolgreichere.\n\nMEDUSA war auf den damals neu erschienenen sogenannten 32bit-Supermini-Computern zu erwerben, deren prominenteste Vertreter die Anbieter DEC (VAX) und Prime Computer waren. Ab 1984 gab es eine Abspaltung (Fork) des MEDUSA, als mit der Übernahme eines Teiles der Rechte und des Codes der Rechner-Anbieter Prime Computer mit der Software in eigene Weiter-Entwicklungen ging, die binnen ganz kurzer Zeit korrekt nur noch auf Prime-Rechnern liefen. Daneben wurde auch die ursprüngliche Software bei Computervision weiterentwickelt, sowohl für die DEC-VAX-Nutzer als auch für die Prime-Kunden. Somit gab es über einige Jahre auf Prime-Rechnern zwei sich zunächst auseinanderentwickelnde MEDUSA-Versionen: Prime-Medusa und CV-Medusa.\n\nMitte der 1980er Jahre kostete eine CV-Farbgrafik-Terminalausrüstung für einen MEDUSA-Arbeitsplatz mit einem 19 Zoll-Farbgrafikschirm inkl. der Arbeitsplatz-Lizenzen ca. 145.000 DM. Realistische Kalkulationen sahen anteilig zudem nochmal einen gleich hohen Betrag in der Zentralrechnerausrüstung stecken. Bei solchen Kosten eines Arbeitsplatzes wurde in vielen Unternehmen, die CAD einsetzen, wieder Schichtarbeit im Angestelltenbereich geleistet, um die CAD-Geräte gut auszulasten: die erste Schicht z. B. von morgens sechs Uhr bis mittags um zwei Uhr, und die zweite Schicht von zwei Uhr bis ca. zehn Uhr abends.\n\nDie gespaltene MEDUSA-Entwicklung sollte wieder zusammengeführt werden, als Prime schlussendlich Computervision aufkaufte, mit dem Versprechen an die CV-Kundschaft, keinen VAX-Nutzer zum Umstieg auf die Prime-Hardware nötigen zu wollen. Viele Nutzer trauten aber den Sirenengesängen nicht und wechselten ihr CAD-System, dies zu einer Zeit, als der Erfolg des PC-basierten AutoCAD groß wurde, recht schnell die Basis-Funktionalitäten des 2D-Konstruierens auch am PC verfügbar wurden, und zu einem Bruchteil der Kosten pro Workstation, wie man es vom Supermini und auch noch von SUN-Netzen gewohnt war. Längerfristig blieben nur die Nutzer bei der Prime-CV-Stange, die ihre MEDUSA-Anwendung weit über die 2D-Nutzung hinaus integriert hatten.\n\nAuch die beginnende Ablösung der Zentral-Superminis mit Remote-Workstations wurde noch von MEDUSA-Software begleitet; sehr populär waren Anfang der 1990er Jahre die Unix-Workstations von SUN für den Einsatz des CAD-Paketes.\n\nPrime gliederte sich in die zwei Säulen Prime Hardware, die für die proprietären Rechner zuständig war, sowie Prime Computervision, die für das CAD/CAM-Geschäft mit MEDUSA und CADDS zuständig war. Nach Einstellung der proprietären PrimOS-Rechnerproduktion und Abgabe der Wartungsverpflichtungen an einen anderen Hersteller folgte eine Konzentration auf das CADCAM-Softwaregeschäft mit Umbenennung von Prime Computervision in Computervision.\n\n1998 schließlich wurde CV von der Parametric Technology Corporation (PTC) übernommen. Die über Jahre stagnierende Entwicklung von MEDUSA NG (Next Generation) wurde wieder forciert. Dabei wurde die Entwicklungspartnerschaft zwischen CV und ehemaligen CV-Entwicklern der Firma Quintic in Cambridge, deren Mitarbeiter schon in den CIS-Tagen an der Entwicklung von MEDUSA beteiligt waren, übernommen und verstärkt. In MEDUSA NG wurde die bisher übliche Bedienung über das Tablett durch eine neue icon- und menübasierte Bedienung abgelöst. Trotzdem war es noch möglich, MEDUSA über Tablett zu bedienen.\n\nUnter den Fittichen von PTC wurde von Quintic ein neues Projekt mit dem Codenamen \"Pegasus\" gestartet. Dabei sollte MEDUSA als 2D-Detaillierprogramm für Pro/E-Zeichnungen dienen.\n\nIm Jahr 2001 verkaufte PTC MEDUSA an die Firma CAD Schroer. Damit war MEDUSA im Besitz einer Firma, die mit dem Produkt groß geworden war. Auch die Entwicklungspartnerschaft mit Quintic wurde übernommen. Unter CAD Schroer wurde das Projekt Pegasus unter dem Namen STHENO/PRO im Jahr 2002 an den Markt gebracht und die Benutzeroberfläche in die neue Version MEDUSA4 übernommen. Die vierte Generation der MEDUSA Software wurde im Jahr 2004 als MEDUSA4 veröffentlicht. Sie beinhaltete eine komplette Überarbeitung des Funktionsumfangs, die Entwicklung einer neuen Benutzeroberfläche (GUI) auf QT-Basis, die Möglichkeit des Datenaustauschs mit anderen Systemen, und die Portierung auf Linux.\n\nIm Jahr 2005 wurde Quintic von CAD Schroer übernommen und agiert heute als CAD Schroer UK.\n\nMEDUSA4 wird weiterhin stark entwickelt.\n\n\nMEDUSA4 Personal ist eine kostenlose 2D/3D-CAD Software für den rein privaten Gebrauch.\nDiese voll funktionsfähige Version beinhaltet viele Funktionen des MEDUSA4 ADVANCED-Paketes (z. B. SMART Edit, basic 3D), sowie Zusatzmodule wie Parametrics und MEDRaster Colour für die Farbbild-Integration.\n\nEinschränkungen: Druck mit Wasserzeichen, Eigenständiges Blattformat, Registrierung erforderlich. Die kostenlose Lizenz ist auf 12 Monate beschränkt, eine Verlängerung ist möglich.\n\nIn den 1980er Jahren gab es auch eine sehr aktive Community von MEDUSA-Anwendern in Deutschland. Diese gründete den Benutzer-Verein MED-USER, der zu Fragen der Software-Verwendung, -Weiterentwicklung, der CAD-CAM-Datenkopplung, der Datenbank-Anbindungen einige Konzepte entwickelten und diese gegenüber den Systemanbietern zur Einbindung und Umsetzung einforderte. Noch heute existiert ein harter Kern von MEDUSA-Nutzern, die das System nunmehr über zwei Jahrzehnte in Gebrauch haben.\n\n"}
{"id": "450732", "url": "https://de.wikipedia.org/wiki?curid=450732", "title": "Graphzeichnen", "text": "Graphzeichnen\n\nDas Graphzeichnen (engl. \"Graph Drawing\") ist ein Themengebiet der Informatik und der Diskreten Mathematik, das sich damit beschäftigt, Graphen geometrisch zu realisieren. Eine zentrale Rolle beim Graphzeichnen bilden Algorithmen, die für einen gegebenen Graphen eine 2-dimensionale Einbettung in den Euklidischen Raum berechnen. Die Knoten des Graphen werden in der Regel durch einfache geometrische Objekte wie Punkte, Kreise oder Quadrate realisiert. Gibt es eine Kante zwischen zwei Knoten, wird dies in der \"Zeichnung\" durch eine Jordan-Kurve dargestellt, welche die den Knoten zugeordneten Objekte verbindet.\n\nDas Graphzeichnen ist in zwei Felder unterteilt: Im \"statischen\" Graphzeichnen soll ein Graph dargestellt werden, während im \"dynamischen\" Graphzeichnen ganze Sequenzen von Graphen (meist in einer Animation) visualisiert werden sollen.\n\nIm Graphzeichnen gibt es keine universellen Techniken, die einen Graphen zeichnen können. Je nach Anwendungsgebiet sind unterschiedliche Ansätze notwendig, die den jeweilig zu erzielenden Effekt unterstreichen. Die folgenden Erklärungen gelten sowohl für das statische, als auch für das dynamische Graphzeichnen.\n\nHierbei versucht man aus einem gerichteten Graph eine Hierarchie auszulesen und diese dann angemessen darzustellen. Dazu wird die Knotenmenge in Äquivalenzklassen aufgeteilt, so dass Knoten einer Äquivalenzklasse auf einer Höhe gezeichnet werden. Dadurch entsteht eine Zeichnung, die die im Graph vorherrschende Hierarchie herausstellt.\n\nIn der Geschäftsprozessmodellierung werden hierarchische Graphen u. a. für Wertschöpfungskettendiagramme oder Organigramme verwendet.\n\nDabei wird zwischen allen Start-Knoten (Knoten ohne Vorgänger) und End-Knoten (Knoten ohne Nachfolger) diejenige Kombination ermittelt, bei der der Pfad zwischen Start- und End-Knoten die größte Anzahl dazwischen liegender Knoten aufweist. Dieser längste Pfad wird dann zur Basis für die Ausrichtung aller Knoten und Kanten, wobei die im längsten Pfad liegenden Knoten und Kanten möglichst an einer Gerade ausgerichtet werden die nicht im längsten Pfad liegenden Knoten und Kanten um die Gerade herum angeordnet werden.\n\nIn der Geschäftsprozessmodellierung werden am längsten Pfad ausgerichtete Graphen u. a. für EPKs verwendet.\nDabei ist die Anwendung von Autolayout-Algorithmen zur Berechnung eines am längsten Pfad ausgerichteten Graphen ein Problem mit erheblichem Entwicklungspotential.\n\nIn der Softwaremodellierung kann diese Darstellung in den Notationen BPMN und UML verwendet werden.\n\nDiesem Ansatz liegt das Modell zugrunde, dass auf alle Knoten Kräfte wirken. Diese Kräfte ergeben sich in diesem Modell durch die Kanten. Anschließend bestimmt man die Gesamtkraft, die auf jeden Knoten wirkt und kann so die Positionen der Knoten in der Zeichnung erhalten. Kanten werden bei diesem Ansatz immer durch gerade Linien repräsentiert.\nDarüber hinaus können komplexere mathematische oder pseudo-physikalische Modelle für die Berechnung der Kräfte angewandt werden. So können sich z. B. alle Knoten gegenseitig abstoßen (ähnlich einer elektrostatischen Kraft). Knoten können auch mit unterschiedlicher \"Dichte\" in einem \"flüssigen Medium\" simuliert werden, so dass einzelne Knoten mehr oder weniger \"Auftrieb\" erfahren. Auf diese Weise ergeben sich natürlicher anmutendende und oft intuitiver interpretierbare Zeichnungen des Graphen .\n\nIn diesem Fall liegt bereits eine Skizze eines Graphen vor. Daraus wird dann ein Bild für den Graph erzeugt. Diese Methode findet zum Beispiel in der Kartografie beim Vereinfachen von Karten Anwendung: Bestimmte Orte werden aus der Karte herausgefiltert und anschließend werden Straßen zwischen den ausgewählten Orten durch Kanten repräsentiert. In einer Zeichnung dieses Graphen werden dann alle Knoten an die Positionen gezeichnet, an denen sie schon in der Karte erschienen. Kanten verlaufen dann (meist als gerade Linien) gegenseitig ausgerichtet. Das resultierende Bild kann zum Beispiel als Anfahrtskizze oder für Busfahrpläne benutzt werden.\n\nJe nach gewünschtem Ergebnis teilt man die Art der Zeichnungen in folgende Klassen ein:\n\nKanten sind in orthogonalen Zeichnungen immer als Polygonzüge dargestellt, die an Ecken miteinander verbunden sind. Alle Linienzugsegmente verlaufen dabei innerhalb der Zeichnung vertikal oder horizontal, aber nie diagonal. Beispiele sind u. a. Organigramme.\n\nDie Kanten werden hierbei durch geschwungene Linien repräsentiert, die keine Knicke aufweisen. Dies kann zum Beispiel durch den Einsatz von Bezierkurven oder B-Splinekurven erreicht werden.\n\nBeispiele hierzu siehe \n\nDie Darstellung eines Graphen sollte auf einen Betrachter auf keinen Fall verwirrend wirken, sondern sollte die besonderen Eigenschaften des zugrundeliegenden Graphen betonen. Dabei ist die Auswahl des Algorithmus zur Berechnung der Darstellung ausschlaggebend. Dieser Algorithmus soll eine möglichst ästhetische Darstellung des Graphen realisieren. Was als möglichst ästhetische Darstellung angesehen wird ist jedoch einerseits vom persönlichen Empfinden des Betrachters und andererseits vom beabsichtigten Zweck der Darstellung abhängig.\nEs gibt dennoch messbare Kriterien, nach denen die Eignung der Darstellung für einen beabsichtigten Zweck beurteilt werden kann, wie\n\nEine besondere Eigenschaft von Graphen sind zum Beispiel das Vorhandensein von Quellen (Knoten ohne eingehende Kanten) oder Senken (Knoten ohne ausgehende Kanten). Diese Eigenschaft wird von einem hierarchischen Layoutalgorithmus oder einem Layoutalgorithmus zur Ausrichtung am längsten Pfad besonders hervorgehoben.\n\nIm \"dynamischen\" Graphzeichnen ist zusätzlich wichtig, dass aufeinanderfolgende Graphen nicht zu unterschiedlich gezeichnet werden. Knoten, die beispielsweise von einem Graph zum nächsten beibehalten werden, sollten möglichst ihre Position oder wenigstens ihre relativen Anordnungen (horizontale und vertikale Knotenreihenfolge) behalten. An dieser Stelle geht man davon aus, dass ein Graph eine sogenannte \"Mental Map\" besitzt, die von einem Betrachter meist unterbewusst wahrgenommen wird. Das Ziel ist es nun, die Mental Map über die gesamte Sequenz zu erhalten. Dabei kann der Einfluss der Mental Map davon abhängig auf welche Art ein dynamischer Graph gezeichnet wird. So ist es möglicherweise in einer Animation beispiel leichter, mehr Änderungen zu Folgen als bei einer Folge von Einzelbildern, in denen man aufeinanderfolgende Darstellungen vergleichen muss.\n\nNeben der Erhaltung der Mental Map als weitere Anforderung kann man die Problemstellung beim Zeichnen dynamischen Graphen noch weiter in zwei Fälle unterscheiden. Denn während ein statischer Graph um gezeichnet werden zu können vollständig einem Algorithmus als Eingabe vorliegen muss, so kann es bei einem dynamischen Graph der Fall sein, dass stets nur der nächste zu zeichnende Graph vorliegt. Man spricht in diesem Fall vom interaktiven Graphzeichnen oder einem in diesem Online-Problem. Im Offline-Fall liegt dann die vollständige Sequenz der Graphen vor. \n\nGraphzeichnen findet Anwendung beim automatischen Anordnen von auf Graphen basierenden Diagrammtypen unterschiedlichster Art, etwa bei der Geschäftsprozessmodellierung oder der Softwaremodellierung. \nDie Autolayout-Algorithmen zum Erstellen der Zeichnungen finden sich auch in spezialisierten kommerziellen Software-Bibliotheken. \n\nSoftware Anwendungen wie der Diagrammeditor yEd bieten umfangreiche Unterstützung für hierarchisches, kräfte- und skizzenbasiertes Zeichnen und ermöglichen sowohl statisches als auch dynamisches Graphzeichnen.\n\n"}
{"id": "452213", "url": "https://de.wikipedia.org/wiki?curid=452213", "title": "OQO", "text": "OQO\n\nOQO, Inc. ist der Name eines amerikanischen Startup-Unternehmens, dass von seiner Gründung 2000 bis zum Mai 2009 Subnotebooks hergestellt hat. OQO ist gleichzeitig die Bezeichnung für das von dem Unternehmen vertriebene Subnotebook, auf dem die gesamte Produktlinie aufbaut. Das Handheld erschien bisher in drei Revisionen, mit den Bezeichnungen \"OQO model 01\", \"OQO model 01+\" und \"OQO model 02\". Ein auf der CES 2009 angekündigtes \"OQO model 02+\" kam über den Prototypen-Status nicht hinaus.\n\nJory Bell und Jonathan Betts-LaCroix gründeten im Jahr 2000 das Unternehmen OQO, zusammen mit vier anderen Größen der Computertechnik, mit dem Ziel, die Computerbranche durch den bisher kleinsten vollwertigen Rechner zu revolutionieren.\n\nAm 14. Mai 2009 teilte OQO mit, die Geschäfte nicht weiterführen zu können; das zuletzt angekündigte Model 02+ wird damit nicht mehr den Endverbrauchermarkt erreichen.\n\nEs handelte sich zur Markteinführung Oktober 2004 um den kleinsten PC der Welt. Mit 12,4 × 8,6 × 2,3 cm und 400 g Gewicht erinnert er äußerlich an einen PDA und passt in eine Handfläche. Technisch gesehen ist er jedoch ein vollwertiger PC mit den folgenden Spezifikationen:\n\n\nIm September 2005 wurde OQO model 01+, der Nachfolger des OQO model 01, vorgestellt. Das neue Modell hat bei gleichen Ausmaßen und gleichem Gewicht einige Verbesserungen zu verzeichnen. Dazu gehören:\n\nAm 7. Januar 2007 präsentierte das Unternehmen den OQO model 02, welcher zahlreiche Weiterentwicklungen beinhaltet, dazu zählen der neue VIA-Chipsatz, C7M-Prozessor und DDR2-RAM. Das neue Modell war zunächst in drei Ausführungen (\"good\", \"better\", \"best\") erhältlich, diese unterschieden sich jeweils in der Größe der Festplatte, des RAMs, sowie in der Taktfrequenz der CPU. Als Betriebssystem wird außerdem Windows Vista angeboten.\n\nDie Ausstattung wird stetig an aktuelle technische Entwicklungen angepasst, so sind beim Neukauf inzwischen Solid State Disks mit bis 60 GB Kapazität sowie Module für Edge oder UMTS erhältlich. Ein Modul für WiMAX wurde ebenfalls bereits vorgestellt.\n\nDas aktuelle Modell für den europäischen Markt hat die Bezeichnung \"OQO e2\". Es ist ohne und mit einem zu den europäischen Mobilfunknetzen kompatiblen UMTS-Modul verfügbar.\n\nEine noch auf der CES 2009 angekündigte, auf der Intel Atom Architektur basierende Version konnte nicht mehr in Serie produziert werden.\n\nZum Start waren die Meinungen gemischt. Dies lag zum einen in einer Reihe von kleinen Mängeln, die durch ihre Anzahl an Bedeutung gewannen, aber mit dem model 01+ größtenteils behoben wurden. Zum anderen liegt es noch heute daran, dass die Nutzer noch nicht so recht wissen, was sie damit anzufangen haben.\n\nDer OQO-Rechner sollte folgendes in sich vereinen:\n\nSomit sollte man nur noch diesen einen PC benötigen, wodurch man sich das Synchronisieren mehrerer Systeme sparen könnte. Jedoch war es für viele Nutzer im Jahr 2004 indiskutabel, ihren schnellen Desktop gegen einen Rechner mit nur 1 GHz Takt einzutauschen.\n\nEine Auswahl von speziellem Zubehör für die drei OQO-Modelle ist erhältlich.\n\nAufgrund der zu früh angekündigten Markterscheinung und oft verschobenen Terminen galt der OQO 01 bis zur Veröffentlichung als Vaporware (Produkt-Ankündigung, die wohl nie erfüllt wird). Es wurde auf Platz 4 der Vaporware-\"Hitliste\" 2002 des US-Magazins Wired gewählt.\n\n"}
{"id": "453349", "url": "https://de.wikipedia.org/wiki?curid=453349", "title": "GEDA (Software)", "text": "GEDA (Software)\n\nDas gEDA-Projekt (GPL Electronic Design Automation)\nstellt eine Sammlung (Suite) freier und quelloffener Software (GNU General Public License) für die Entwicklung von elektronischen Schaltungen bereit. Mit den aufeinander abgestimmten Programmen können u. a. Schaltpläne entworfen, Schaltungen simuliert und Platinen-Layouts (Leiterbahnen-Struktur auf Leiterplatten) erstellt werden. Die Funktionalität von gEDA ist mit den proprietären Programmen „TARGET 3001!“ oder „Eagle“ vergleichbar.\n\nObwohl die Entwicklung der Software vorwiegend unter GNU/Linux erfolgt, streben die Autoren Kompatibilität mit anderen UNIX-Varianten an. In vielen Linux-Distributionen ist die gEDA-Suite bereits enthalten, für Linux auf (32-Bit) x86-Systemen wird alternativ eine spezielle ISO-Installations-CD bereitgestellt. Die gEDA-Suite lässt sich auch für Windows kompilieren.\n\ngEDA ist u. a. für folgende Betriebssysteme verfügbar:\n\nDas gEDA-Projekt wurde 1998 von Ales Hvezda gegründet. Heute steht die Bezeichnung gEDA als Oberbegriff für eine Sammlung von freien, quelloffenen Programmen (GPL), die für die Entwicklung elektronischer Schaltungen eingesetzt werden können. Entwickelt werden die einzelnen Komponenten von verschiedenen Programmierern, die vorwiegend über Mailinglisten miteinander kommunizieren. Für den Datenaustausch zwischen den einzelnen Programmen werden offene, gut dokumentierte Dateiformate in Form von lesbaren (ASCII)-Textdateien verwendet. Dies stellt die problemlose Zusammenarbeit der Programme sicher und ermöglicht es, ein Programm durch ein anderes zu ergänzen oder zu ersetzen. (Außerdem können diese Textdateien bei Bedarf auch mit einem beliebigen Texteditor bearbeitet werden.) Da die Entwicklung der Programme in der Anfangsphase weitgehend unabhängig voneinander durchgeführt wurde, gibt es noch keine vollständig einheitliche Benutzeroberfläche.\n\nDer Kern der gEDA-Werkzeuge bestand ursprünglich aus diesen Programmen:\n\nDiese Programme benutzen ein gemeinsames Dateiformat (.sch), teilen eine gemeinsame\nFunktions-Bibliothek (libgeda) und werden als gEDA/gaf bezeichnet. Dabei steht \"gaf\" für „gschem and friends“.\nEbenso bedeutend sind folgende Programme, die prinzipiell \nselbstständig, aber wichtiger Teil des gEDA-Projektes sind:\n\nWeiter existieren noch viele kleinere Anwendungen, etwa die Scripte \"tragesym\" (Python) oder \"djboxsym\" (Perl), mit deren Hilfe man sehr einfach neue Schaltplansymbole erzeugen kann.\n\n"}
{"id": "454073", "url": "https://de.wikipedia.org/wiki?curid=454073", "title": "De-Casteljau-Algorithmus", "text": "De-Casteljau-Algorithmus\n\nDer Algorithmus von de Casteljau ermöglicht die effiziente Berechnung einer beliebig genauen Näherungsdarstellung von Bézierkurven durch einen Polygonzug. Der Algorithmus wurde Anfang der 1960er Jahre von Paul de Faget de Casteljau bei Citroën entwickelt.\n\nDer Algorithmus von de Casteljau beruht darauf, dass eine Bézierkurve geteilt und durch zwei aneinandergesetzte Bézierkurven dargestellt werden kann. Diese Unterteilung kann rekursiv fortgesetzt werden. Das Kontrollpolygon der zusammengesetzten Bézierkurve nähert sich dabei der Originalkurve an. Nach ausreichend vielen Verfeinerungsschritten kann der entstandene Polygonzug als Näherung für die Originalkurve verwendet werden. Ein Verfeinerungsschritt, der das Kontrollpolygon einer Ausgangskurve in ein Kontrollpolygon einer zusammengesetzten Kurve zerlegt, besteht nur aus wenigen einfachen Teilungen von Polygonkanten.\n\nDarüber hinaus ermöglicht der Algorithmus die schnelle Bestimmung jedes einzelnen Punktes auf der Bézierkurve durch seine parametrische Darstellung.\n\nErweiterungen findet der Algorithmus im Blossoming wie auch im fokalen Algorithmus von de Casteljau.\n\nGegeben sind die Kontrollpunkte formula_1 der Ausgangskurve formula_2 mit formula_3.\n\nVon den Kontrollpunkten der Ausgangskurve formula_4 liegen im Allgemeinen nur formula_5 und formula_6 auf der Kurve. Der Algorithmus berechnet im ersten Schritt einen weiteren Punkt formula_7 der Kurve. Dabei kann formula_8 frei gewählt werden. Die Kurve wird im Weiteren an diesem Punkt formula_7 geteilt. Es bietet sich daher die Wahl von formula_10 an, weil dies eine gleichmäßige Aufteilung und damit eine schnelle Annäherung des Kontrollpolygons an die Ausgangskurve gewährleistet.\n\nStatt durch direktes Einsetzen von formula_11 in die Gleichung der Kurve formula_4 geschieht die Berechnung von formula_7 über die Konstruktion von Punkten formula_14 aus den gegebenen Kontrollpunkten formula_15. Die Konstruktion startet mit den Ausgangspunkten formula_16. Aus diesen werden durch Teilen der Verbindungslinien formula_17 im Verhältnis formula_18 neue Punkte formula_19 konstruiert. Der Punkt formula_19 berechnet sich nach der intuitiv einsichtigen Formel:\n\nIn nebenstehender Abbildung sind die Punkte formula_22, die aus den Ausgangspunkten formula_16 hervorgegangen sind, rot eingezeichnet. Durch Fortsetzen der Berechnungsvorschrift werden in gleicher Weise Punkte formula_24 bestimmt. Zur Berechnung von formula_25 werden dazu die blau gestrichelten Verbindungslinien der im ersten Schritt berechneten Punkte formula_26 im selben Verhältnis geteilt usw.\n\nEs gilt die folgende Aussage (siehe Beweis der Punktkonstruktion):\n\nDas heißt, dass der Punkt formula_28, welcher in der formula_29ten Iteration durch fortgesetztes Streckenteilen konstruiert wird, ein Punkt der Kurve ist. Das Teilungsverhältnis formula_11 bestimmt dabei seine Lage auf der Kurve.\n\nIn nebenstehender Abbildung ist die Konstruktion für formula_31 vollständig durchgeführt. Der Punkt formula_32, der durch dreifache Anwendung der Teilungsvorschrift aus den Ausgangspunkten formula_33 hervorgegangen ist, liegt auf der gesuchten Kurve.\n\nDie bei weitem interessantere Aussage ist aber, dass dieser Punkt formula_28 die Kurve formula_4 in zwei Bézierkurven formula_36 und formula_37 teilt und dass die Punkte formula_38 das Kontrollpolygon von formula_36 und die Punkte formula_40 das Kontrollpolygon von formula_37 bilden.\n\nNebenstehende Abbildung zeigt die Zerlegung von formula_4 in formula_36 und formula_37 für formula_45. Dabei bilden die Punkte formula_46, formula_47, formula_48 und formula_32 das Kontrollpolygon von formula_36 und entsprechend die Punkte formula_32, formula_52, formula_53 und formula_54 das Kontrollpolygon von formula_37.\n\nAn der Abbildung erkennt man außerdem, warum in der Regel ein Teilungsverhältnis von formula_10 verwendet wird. Da in diesem Beispiel ein Teilungsverhältnis kleiner ½ verwendet wurde, teilt sich die Kurve formula_4 in einem ungleichen Verhältnis in eine kurze Kurve formula_36 und eine lange Kurve formula_37 auf. Der kürzere Teil ist viel besser an sein Kontrollpolygon angenähert als der längere. Möchte man (bei ungefähr gleich langen Strecken des Ausgangskontrollpolygons) eine gleichmäßige Näherung erreichen, eignet sich dazu das Teilungsverhältnis formula_10.\n\nDie Unterteilung der Kurven wird so lange fortgesetzt, bis sie hinreichend genau durch ihre Kontrollpolygone angenähert sind.\n\nZu Beginn liegen die Punkte des Kontrollpolygons in einem Feld formula_61 vor. Bei gegebenem Parameter formula_11 wird der Punkt formula_7 folgendermaßen berechnet:\n\nDer obige Algorithmus ist insoweit unvollständig, dass nur der Punkt formula_7 bestimmt, aber keine fortgesetzte Unterteilung von formula_4 durchgeführt wird. Der Algorithmus ist nicht speichereffizient, da für alle formula_69 neue Speicherplätze belegt werden.\n\nDie Aussage, dass über formula_29-fach fortgesetzte Streckenteilung ein weiterer Punkt der Kurve konstruiert werden kann, lässt sich über Lösen der Rekursion beweisen, die formula_69 definiert.\n\nDie Rekursionsgleichung definiert die Punkte formula_14 in Abhängigkeit von den in der letzten Iteration berechneten Punkten formula_73. Den Start der Rekursion bilden die Punkte formula_74:\n\nZu beweisen ist die Aussage, dass der Punkt formula_28 ein Punkt der Kurve an der Stelle formula_11 ist:\n\nUm eine Lösung der Rekursion für den speziellen Punkt formula_28 zu finden, wird eine geschlossene Form für alle Punkte formula_14 der Rekursion gesucht und gezeigt, dass diese insbesondere für formula_28 das gesuchte Resultat liefert. Der Beweis für formula_14 wird über vollständige Induktion mit folgender Induktionsannahme geführt:\n\nDer Induktionsschritt ist dann eine gradlinige Rechnung, bei der Aussagen über Binomialkoeffizienten benutzt werden.\n\nMit Hilfe des Algorithmus von de Casteljau ist es möglich, Näherungen von Bézierkurven durch gerade Linien zu errechnen. Dadurch kann eine Bézierkurve effizient mit dem Rechner gezeichnet werden.\n\n"}
{"id": "454908", "url": "https://de.wikipedia.org/wiki?curid=454908", "title": "Logic Pro", "text": "Logic Pro\n\nLogic [] ist eine Digital Audio Workstation (DAW) und ein Software-Audio- und Midisequenzer des Herstellers Apple.\n\nDas Programm unterscheidet zwischen mehreren Spurtypen: unter anderem MIDI, Audio, Film, Plugin- und Mischpult-Automationsspuren. Logic war als Pro- oder als Express-Version erhältlich. Letztere unterschied sich von der Pro-Version in Preis und Funktionsumfang. Derzeit wird von Apple jedoch nur noch die Pro-Version vertrieben.\n\nBis zur Version „Logic 6“ wurde die Software von dem deutschen Unternehmen Emagic entwickelt. Apple übernahm Emagic im Jahr 2002 und stellte die Weiterentwicklung von Logic für das Betriebssystem Windows ein. Seitdem ist Logic ausschließlich für macOS erhältlich. Die letzte Version, die unter Windows betrieben werden kann, ist Logic 5.5.1. Während Version 6 noch den Namen Emagic trug, ist Logic ab Version 7 offiziell ein Apple-Produkt.\n\nAußerdem legte Apple mehr als 20 Emagic-Produkte zusammen, unter anderem alle Instrumente und Effekt-Plug-Ins, die CD-Authoring-Software „Waveburner Pro“ und das \"Pro Tools TDM support package\", und nannte dieses einzelne Produkt „Logic Pro“.\n\nLogic bietet Funktionen zum Aufnehmen, Bearbeiten, Mischen und zur Notation von Musik. Neben den Aufnahmefunktionen für MIDI und Audio findet man auch Audioeffekt-Plugins sowie eine integrierte Sample-Bibliothek, die sogenannten \"Apple Loops\" (ab Version 7) und mehr.\n\nIn Logic können MIDI-, Audio- und Parameter-Daten in Form von Sequenzen (Regionen) aufgenommen und grafisch auf Spuren arrangiert werden. Die Spuren werden mit Hilfe virtueller Mischpulte, die individuell konfigurierbar sind, weiter bearbeitet. Ebenso können MIDI-Audio-Spuren exportiert werden. Im sogenannten \"Environment\" können alle Audio- und MIDI-Geräte konfiguriert und an die Bedürfnisse des Benutzers angepasst werden. Mittels frei konfigurierbarer Tastaturbefehle können fast alle Funktionen des Programms ohne Maus ausgeführt werden.\n\nFür die Audio-Bearbeitung stehen eine ganze Reihe virtueller Effektgeräte (Plug-Ins) zur Verfügung. Neben Modulationseffekten wie z. B. Chorus, Phaser, Flanger und Ensemble, Dynamikprozessoren wie z. B. Kompressor,\nLimiter, Gate und Expander und verschiedenen Delays bietet Logic auch einen Faltungshall (Space Designer) und einen Vocoder (EVOC20). Ebenso bietet Logic diverse virtuelle Synthesizer (ES1, ES2, EFM1, Sculpture), Sampler (EXS24), Emulationen elektromechanischer Instrumente wie Fender Rhodes, Hammond B3 oder Hohner Clavinet D6 (EVP88, EVB3, EVD6), sowie einen virtuellen Drumcomputer mit integriertem Step-Sequencer (Ultrabeat). In Version 7 bzw. 7.2 von Logic Pro sind u. a. eine Gitarren-Verstärker-Simulation (Guitar Amp Pro) sowie eine Bass-Verstärker-Simulation (Bass Amp) neu hinzugekommen. Außerdem können ab Version 7 auch Loops und Software-Instrumente integriert werden, die für Apples Einsteiger-Musikprogramm GarageBand erstellt wurden. Apple hat inzwischen fünf umfangreiche \"Jam Packs\" mit Loops und Samples herausgebracht, die sich somit auch in Logic einsetzen lassen.\nDie im September veröffentlichte Version Logic 8 wurde um ein komplexes Multi-Tap-Delay (\"Delay Designer\") sowie neue Kompressorentypen erweitert. Auch wurde das Programm an die Anforderungen des immer weiter verbreiteten 5.1-Surround-Formats angepasst und mit den entsprechenden Funktionen zur Verräumlichung von Klängen ausgestattet.\n\nLogic kann um zusätzliche Klangerzeuger- und Effekt-Plugins anderer Hersteller erweitert werden. Bis zur letzten Version Logic 6 Pro wurde unter OS 9 die VST-Schnittstelle unterstützt; mit der ersten 6er-Version, die unter OS X lauffähig war, wurde VST durch die AU-Schnittstelle (Audio Unit) abgelöst. Diese Schnittstelle hat sich bei den Plugin-Herstellern mittlerweile durchgesetzt, und AU wird parallel zur VST-Schnittstelle angeboten. Bei Bedarf lassen sich allerdings mit dem Zusatzprogramm des Unternehmens Fxpansion „VST-AU-Adapter“ VST-Plugins in AU konvertieren.\n\nAb Version 7 enthält Logic neben der gleichstufigen Stimmung auch die Möglichkeit, verschiedene fixierte Stimmungen (historische und selbst definierte) und das dynamische Stimmungssystem Hermode Tuning (HMT) für die internen Audio Instrumente zu verwenden.\n\nDateiendungen von Logic sind hauptsächlich \".LSO\"\n\nSeit 2008 kommt Apple mit einem Softwarepaket, bestehend aus Logic 8 / MainStage / Soundtrack Pro sowie einem mittlerweile integrierten \"Waveburner\", auf den Markt. Durch das neue \"One-Window-Konzept\" lässt sich die ganze Software nun von einem Fenster aus bedienen, wobei die alte Arbeitsweise mit mehreren Fenstern (Screensets) weiterhin möglich ist. Etliche Plug-Ins wurden verbessert und um den Stereo- bzw. Surroundeinsatz erweitert. Der „Space Designer“ (Digitaler Hall, basierend auf Impulsantworten) ermöglicht nun True Stereo anstatt bislang nur Mono to Stereo und wurde um eine Bibliothek von Surround-Impulsantworten erweitert. In den Ultrabeat lassen sich komplette Sample Sets laden. In der Mischpultsektion wurden Post-Pan Send-Regler hinzugefügt. Das neue CAF-Format eliminiert die bislang auf 4 GB begrenzte Größe von Dateien. Über den Media Browser lassen sich die mitgelieferten Samples und Instrumente bequem vorab anhören und einbinden.\n\nIm Sommer 2009 veröffentlichte Apple mit Logic 9 das Update auf das oben beschriebene Softwarepaket (erheblich bessere Prozessorlastverteilung bei Multikernprozessoren, verbesserter Node-Betrieb über Ethernet, verbesserter Import von Projekten in „Songs“, „Flex Time Funktion“, „Bounce to Track“, Gitarren-Verstärker Plug-In (Amp Designer) und Pedalboard Plug-In). Seit Version 9.1.1 kann man Logic Pro ohne Einschränkungen im 64-Bit-Modus nutzen.\nZeitgleich wurde bekanntgegeben, dass Logic Pro bis auf weiteres das Pro Tools TDM System der Firma Avid nicht mehr unterstützt.\n\nAm 16. Juli 2013 wurde Logic Pro X von Apple vorgestellt. Laut offizieller Webseite gibt es nun Neuerungen, bei denen unter anderem die Plug-In-Bibliotheken erweitert wurden und neue Plug-Ins hinzugekommen sind. Außerdem hat sich die gesamte Bedienoberfläche verändert, die die Software nun in dunklem Grau erscheinen lässt.\n\n\nSeit 2012 ist Logic nur noch über den AppStore ab 10.6 zu kaufen und kann somit auf bis zu fünf Macs gleichzeitig genutzt werden. Zuvor wurde Logic durch Eingabe einer Seriennummer vor unerlaubter Vervielfältigung (Schwarzkopieren) geschützt und ließ sich, laut Anleitung, auf je einem Desktop und einem mobilen Mac installieren. Ein Dongle wird nicht mehr benötigt. Eine Aktualisierung verlangt im Gegensatz zur Neuinstallation jedoch den Abgleich mit dem Dongle.\n\n\n"}
{"id": "455032", "url": "https://de.wikipedia.org/wiki?curid=455032", "title": "Programmed Data Processor", "text": "Programmed Data Processor\n\nPDP ist die Bezeichnung für viele unterschiedliche Computer des Herstellers Digital Equipment Corporation (DEC, heute HP). PDP steht als Abkürzung für \"Programmable Data Processor\" oder \"Programmed Data Processor\".\n\nUm nicht in direkte Konkurrenz zur mächtigen IBM zu treten, vermied DEC in den 60er Jahren das Wort \"Computer\" in der Bezeichnung ihrer Rechner. Stattdessen nannte die Firma ihre Geräte \"programmierbarer Datenprozessor\". Die PDP-Rechner wurden beginnend bei der kleinen PDP-1 (18 Bit Wortlänge, standardmäßig 4096 Speicherworte) in der Reihenfolge ihrer Entwicklung durchnummeriert (über das Fehlen von PDP-2 und PDP-3 gibt es immer noch Gerüchte). Dadurch haben verwandte Systeme nie aufeinanderfolgende Nummern. Insgesamt gab es zwischen 1959 und 1982 mehr als 60 PDP-Modelle, die sich auf vier Systemfamilien verteilten und die sich teilweise auch untereinander Konkurrenz machten:\n\nVon den besonders erfolgreichen PDP-Rechnern gab es jeweils viele Varianten, z. B. PDP-8: -8/S, -8/I, -8/L, -8/E, -8/M, -8/A.\n\nDie bekannteste und erfolgreichste PDP war die PDP-11.\n\nViele PDPs waren verhältnismäßig preiswert (den PDP-8 gab es für unter 20.000 US$) und fanden so schnell Verbreitung in Universitäten und als Prozessrechner, zum Beispiel in der Computerisierung des amerikanischen Telefonnetzes von AT&T. Am anderen Ende der Preisskala lagen die als Mainframe einzustufenden 36-Bit-Rechner PDP-6 und PDP-10, die bei entsprechender Konfiguration mehrere Millionen US$ kosten konnten.\n\nEine für die Technikgeschichte beispielhafte Anwendung der PDP-8 ist Olympia Multiplex 80 in den 1970er Jahren.\n\nDie PDPs waren zuerst klassisch, d. h. aus diskreten Bauelementen wie z. B. Transistoren aufgebaut, dann aus integrierten TTL-Schaltungen, später ging DEC dazu über, die Prozessoren auf einem IC zu integrieren (T11, J11).\n\nNeben der zunehmenden Integration der Bauelemente erfuhr auch die Verbindungstechnik erhebliche Fortschritte. Die Verbindungsebene zwischen den Printkarten bestand vorerst aus Tausenden von einzelnen Wickelverbindungen (engl. \"Wire Wrap\"). Als bedeutender Fortschritt wurde bei der Version PDP-8/E erstmals der sogenannte \"Omnibus\" (eine spezielle Ausführung eines Bus (Datenverarbeitung)) als Ersatz eingeführt, welcher auch eine flexible, modulare Erweiterung des Systems durch zusätzliche Baugruppen erlaubte. Der Schweizer Projektleiter der Entwicklung des PDP-8/E hat darüber einen Erfahrungsbericht publiziert.\n\nDie wesentlich höhere Packungsdichte ermöglichte es anschließend, eine komplette PDP-11 auf einer Q-Bus-Karte unterzubringen und mit diesen PDP-11s Multiprozessorsysteme aufzubauen. Unter dem Namen MicroPDP brachte DEC eine Reihe von PDP-11s als Desktop-(Personal-)Computer auf den Markt, in denen diese Prozessoren auch eingesetzt wurden. In der Sowjetunion wurde Anfang der 1980er Jahre ein MicroPDP-Nachbau unter der Bezeichnung Elektronika BK-0010 produziert.\n\n\nFolgende Betriebssysteme liefen auf PDP-Rechnern:\n\n\n\n"}
{"id": "456916", "url": "https://de.wikipedia.org/wiki?curid=456916", "title": "Most Recently Used", "text": "Most Recently Used\n\nAls Most Recently Used (MRU) wird eine Funktion des Windows-Betriebssystems bzw. seiner Programme bezeichnet, die es dem Benutzer dieser Programme erleichtern soll, häufig genutzte Einträge, z. B. Kommandozeilen bei \"Start\\Ausführen ...\", zuletzt verwendete Dateien in Microsoft Word, URLs im Browser oder andere häufig genutzte Einträge schneller aufzurufen. Meistens werden diese Listen in Programmen unter \"Datei\" angezeigt.\n\nDazu wird in einer Datei oder der Windows-Registrierungsdatenbank (Registry ...\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Map Network Drive MRU) eine Liste angelegt, die neben den eigentlichen Einträgen, die meist mit den alphabetischen Kleinbuchstaben (abc ...) benannt werden, auch einen Wert (im Register der Standard-Wert) enthält, der die Reihenfolge beim Auslesen der Einträge angibt.\n\n"}
{"id": "458611", "url": "https://de.wikipedia.org/wiki?curid=458611", "title": "Reaktor (Software)", "text": "Reaktor (Software)\n\nReaktor ist eine grafische Entwicklungsumgebung zur Erstellung und Verwendung von Software-Synthesizern, Sequenzern, Samplern und Effektgeräten der Berliner Firma Native Instruments.\n\nZentraler Bestandteil von Reaktor ist eine umfangreiche Sammlung von DSP-Modulen zur Erzeugung und Verarbeitung von Audio- und Event-Datenströmen. Mit Hilfe dieser Module und einer grafischen Oberfläche, auf der die Bauelemente mit virtuellen „Kabeln“ verbunden werden können, lassen sich virtuelle Musikinstrumente, Effektgeräte, Sequenzer, Sampler und Ähnliches erstellen, sogenannte \"Ensembles\". Die Benutzeroberfläche dieser Ensembles lässt sich in großem Umfang frei gestalten, was neben grafischen Spielereien auch die Umsetzung unterschiedlicher Interface-Konzepte ermöglicht.\n\nAus technischer Sicht gibt es 3 Ebenen. Die ursprünglichen Komponenten sind auf der mittleren angesiedelt. Einzelne Oszillatoren, Hüllkurvengeneratoren, Filter, Knöpfe und Anzeigetafeln lassen sich hier zu Makros kombinieren, die wiederum Bausteine von Instrumenten sind, welche zu Ensembles zusammengefasst sind. Das Prinzip erinnert an modulare Synthesizer aus den 1960er und 1970er Jahren. Im Vergleich dazu ist Reaktor flexibler, aber auch komplexer. Mit Version 5 kam eine sogenannte CORE-Ebene hinzu. Sie umfasst Komponenten, die sehr nah an einzelne C++-Befehle angelehnt sind und eine wesentlich effektivere Programmierung erlauben. Diese werden gleichermaßen zu CORE-Macros verbunden, die sich wie normale Macros weiter einsetzen lassen. Das erlaubt Projekte, die vorher wegen zu großer Prozessorlast gescheitert wären. Version 6 führte Blocks ein, die Instrument-Teile darstellen, ähnlich Eurorack-Modulen deren Wertebereiche normiert sind und nicht zwischen Kontroll- oder Audio-Signalen unterschieden. Technisch nicht vorgebildeten Personen ermöglichen Blocks einen leichteren Einstieg, aber auch die Steuerung analoger Hardware. Dadurch lassen sich Modularsysteme um Einheiten im Wert von dutzenden Hundert Euro mit wenigen Klicks erweitern.\n\nUm Anfängern den Einstieg zu erleichtern, wird Reaktor mit einer Vielzahl an fertigen \"Ensembles\" ausgeliefert. Außerdem unterhält Native Instruments auf seiner Webseite ein Archiv, in das Nutzer ihre Ensembles hochladen und so anderen zugänglich machen können. Dort finden sich über 4000 Ensembles, von simplen Nachempfindungen subtraktiver Synthesizer bis hin zu komplexen generativen Ensembles, die auf Basis unterschiedlicher Algorithmen selbsttätig sich ständig verändernde Klänge erzeugen.\n\nReaktor lässt sich als Plug-In in entsprechende Sequenzer-Programme einbinden, läuft aber auch „standalone“. Für Nutzer, die nicht daran interessiert sind, selbst Ensembles zu erstellen, sondern nur die vorhandenen verwenden möchten, gab es mit \"Reaktor Session\" eine abgespeckte Version des Programms. Mit dem Erscheinen der Version 5 von Reaktor wurde \"Reaktor Session\" allerdings von Native Instruments aus dem Programm genommen; mit Reaktor 5 erstellte Ensembles lassen sich mit \"Reaktor Session\" nicht nutzen. Als Äquivalent erschien im Jahr 2010 der kostenlose \"Reaktor Player\", mit dem Ensembles der Version 5.5 gespielt, aber nicht verändert werden können.\n\nMit der Veröffentlichung der Version 6 werden sogenannte Blocks Module angeboten, die den Umgang mit Reaktor vereinfachen können.\n\nDaneben ist Native Instruments dazu übergegangen, einzelne Ensembles als separate Programme zu veröffentlichen, die man verwenden kann, ohne Reaktor zu besitzen. Für die Anwender von Reaktor gibt es keine Möglichkeit, solche selbstständig lauffähigen Ensembles zu erstellen.\n\nZu den Hauptkonkurrenten von Reaktor gehören die Software Max/MSP und das ähnliche, quelloffene Pure Data.\n\n"}
{"id": "459520", "url": "https://de.wikipedia.org/wiki?curid=459520", "title": "Subpixel", "text": "Subpixel\n\nEin Subpixel (etwa „Teilbildpunkt“ oder „Unterpixel“) beschreibt die innere Struktur eines Pixels.\nIn aller Regel ist mit dem Begriff Subpixel eine höhere Auflösung verbunden als man aufgrund eines einzelnen Pixels erwarten kann.\n\nAuch kann ein Pixel in Realität aus mehreren kleineren Pixeln bestehen. Ein Bildschirmpixel kann beispielsweise aus 3 kleineren Pixeln bestehen aus deren individueller Farbe die Farbe des \"großen\" Bildschirmpixels erzeugt wird. Ein gutes Beispiel hierfür sind OLED-Displays.\n\nWerden Fotos mit einer Digitalkamera, beispielsweise einer CCD-Kamera aufgenommen, erhält man als Ergebnis Bilder mit einer bestimmten Anzahl von Pixeln. Eine durchaus übliche Anzahl von Pixeln in Smartphones des Jahres 2018 liegt bei 12 Millionen Pixeln. Damit können Bilder mit 4000 × 3000 Pixeln dargestellt werden.\n\nIst in einem 4000 × 3000 Pixel großen Bild ein Mensch formatfüllend aufgenommen, so ist es mit geeigneter Software möglich den Augenabstand im Gesicht auf eine Genauigkeit besser als ein Pixel zu bestimmen.\nGanz grob entspricht bei einer solchen Aufnahme ein Pixel in etwa 0,5 Millimetern, d. h. ein Mensch mit einer Länge von 1,8 m (1800 mm) nimmt in etwa 3600 Pixel in der Höhe ein. Der mittlere Augenabstand von Menschen liegt um die 65 mm, entsprechend 130 Pixeln.\nDie Größe der Pupille liegt bei Tageslicht zwischen 2 und 4 Millimetern, entsprechend 4 bis 8 Pixeln.\n\nDie Positionsbestimmung der beiden Augenpupillen erfolgt nun im einfachsten Fall durch einen kreisförmigen Fit um die in der Regel schwarzen Augenpupillen. Der geometrische Schwerpunkt einer so gefitteten \"schwarzen\" Augenpupille liegt dann oft nicht genau in der Mitte eines Bildpixels. Somit erhält man eine Positionsgenauigkeit bzw. Abstandsgenauigkeit im Subpixel-Bereich.\n\nAls Subpixel werden ebenfalls die Grundfarben, oft die drei Farben Rot, Grün und Blau von Flüssigkristallbildschirmen, Computermonitoren oder Fernsehgeräten bezeichnet, aus denen sich ein Pixel zusammensetzt. Da die Subpixel bei dieser Art von Geräten in der Regel direkt nebeneinander angeordnet sind, ist es durch gezielte Ansteuerung der Subpixel möglich die horizontale Auflösung zu erhöhen. Für das Auge kann dies beispielsweise zu besser lesbaren Texten auf Bildschirmen führen. Beispiele hierzu finden sich im Hauptartikel Subpixel-Rendering.\n"}
{"id": "460555", "url": "https://de.wikipedia.org/wiki?curid=460555", "title": "DOSBox", "text": "DOSBox\n\nDOSBox ist ein freier x86-Emulator, der PC-kompatibles DOS und die in dessen Ära gebräuchliche Hardware nachbildet. Ziel ist das Ausführen älterer, DOS-basierter Software, die mit modernen Computersystemen nur eingeschränkt oder gar nicht kompatibel ist.\n\nIm Gegensatz zur mitgelieferten Virtual DOS Machine bei Windows-Betriebssystemen oder zu Emulatoren wie DOSEMU unter Linux emuliert DOSBox nicht nur die Hardwareumgebung eines IBM-PC-kompatiblen Computers, sondern auch den Prozessor und das Betriebssystem DOS. Das erlaubt eine bessere Kontrolle über den Ablauf des emulierten Programmes. Als Emulator ist die Software außerdem in der Lage, auch auf Computern mit unterschiedlicher Prozessorarchitektur sowie auf inkompatiblen PCs eine weitgehend zu MS-DOS/​PC DOS kompatible Umgebung zu erschaffen. Zwar kann ein echtes DOS-Betriebssystem (wie z. B. MS-DOS, PC DOS oder DR-DOS) innerhalb von DOSBox ausgeführt werden, notwendig ist das jedoch selten.\n\nAuch Erweiterungsspeicher (EMS) und besondere EXE-Ladeprogramme (etwa bei dem Spiel Jazz Jackrabbit) werden ab Version 0.61 unterstützt. Zudem ist DOSBox \"portabel\" oder \"mobil\", das heißt, es ist keine Installation in das bestehende Betriebssystem nötig, da DOSBox lediglich über eine einfache Textdatei konfiguriert wird.\n\nDie DOSBox-Entwickler haben hauptsächlich das Emulieren einer Plattform für ältere Computerspiele im Sinn, die unter Win32-Betriebssystemen entweder gar nicht oder nur fehlerhaft ausgeführt werden können. DOSBox ermöglicht aufgrund der vollständigen Emulation des x86-Prozessors das Ausführen von DOS-Anwendungen auch unter den 64-Bit-Versionen von Windows, die selbst keine Unterstützung für 16-Bit-Software mehr anbieten. So können auch 16-Bit-Windows-Anwendungen ausgeführt werden, wenn Windows 3.x innerhalb von DOSBox installiert wurde.\n\nDa DOSBox Simple DirectMedia Layer (SDL) verwendet, ist es vergleichsweise einfach, es auf andere Betriebssysteme oder Rechnerarchitekturen zu portieren.\n\nDOSBox benötigt eine hohe Rechenleistung, um das emulierte System in der originalen Geschwindigkeit nachzubilden. Abhängig von der eingesetzten Software sind leistungsstärkere Prozessoren als Pentium oder 80486 erforderlich, um ursprünglich für die Intel-80486- oder Intel-Pentium-Familie geschriebene Programme flüssig ablaufen zu lassen.\n\nFür IA-32 (was eigentlich x86 in seiner Gesamtheit umfasst, daher auch 16-/32-/64-Bit) – spezifisch für Intel 80386 (32-Bit) und x64 (64-Bit; AMD64/​Intel 64, alternative Bezeichnung x86-64) – sowie für ARM existiert ein dynamischer Übersetzer (englisch \"Dynamic Recompilation\") oder auch JIT-Compiler, wodurch der emulierte CPU-Kern von DOSBox den Programmcode direkt vom Prozessor des Hosts ausführen lässt, statt die einzelnen Befehle in Software nachzubilden. Dadurch wird auf diesen Prozessorarchitekturen die Ausführungsgeschwindigkeit innerhalb der emulierten DOS-Umgebung erhöht.\n\nEine weitere Möglichkeit zur teilweise drastischen Beschleunigung von Protected-Mode-Software ist das Ersetzen des häufig verwendeten \"DOS/4GW Protected Mode Memory Extenders\" durch das kompaktere und schnellere Open-Source-Derivat DOS/32A.\n\nIn der offiziellen Version von DOSBox fehlt eine Unterstützung der parallelen Schnittstelle, die von den meisten älteren Druckern und auch manchen Steuergeräten in der produzierenden Industrie verwendet wird. Mittlerweile existiert eine inoffizielle DOSBox-Version, die die Unterstützung der parallelen Schnittstelle beinhaltet.\n\nDie Zeichensätze von eingehängten Partitionen auf Dateisystem-Ebene werden durch den Emulator nicht in den DOS-Zeichensatz\nübersetzt (siehe auch FAT – ohne Unicode-Unterstützung). Das führt dazu, dass Dateien und Ordner (z. B. mit deutschen Umlauten im Namen, die auf NTFS-Partitionen gespeichert sind) im Emulator nicht immer korrekt dargestellt werden und somit derartige Dateien (wie z. B. Bibliotheken) auch nicht von den in DOSBox ausgeführten Programmen verwendet werden können.\n\n\nDOSBox bietet für die Konfiguration keinerlei grafische Benutzeroberfläche, sondern lediglich eine vom Benutzer zu bearbeitende Textdatei. Obwohl diese mit Hinweisen zur Konfiguration versehen ist, sind weniger versierte Anwender damit oftmals überfordert. Einige Programmierer haben hier Abhilfe geschaffen, indem sie eine grafische Oberfläche (Frontend) als externes Programm nachgeliefert haben.\n\nAuch für erfahrene Benutzer können solche Frontends interessant sein, weil damit einige Arbeitsschritte unnötig werden. Ein Beispiel dafür ist das Erstellen von individuellen Konfigurationen für verschiedene Spiele.\n\nInzwischen ist die Mehrzahl der Frontends für die meisten Betriebssysteme erhältlich. Sie werden auch direkt auf der offiziellen Website von DOSBox beworben.\n\nDOSBox wird von den Entwicklern in englischer Sprache (als Voreinstellung) ausgeliefert. Neben Deutsch sind noch weitere Sprachen verfügbar, welche als Sprachmodule von der DOSBox-Webseite (zu finden unter „Translations“) heruntergeladen werden können.\n\nNeben der offiziellen Version von DOSBox gibt es noch zahlreiche Erweiterungen (in Form von Quelltext-Patches), die fehlende Systemfunktionen als Emulation nachrüsten oder bestehende verbessern. Auch finden sich zahlreiche Portierungen auf nicht vom offiziellen DOSBox-Entwicklerteam unterstützte Betriebssysteme und Plattformen.\n\nDiese Erweiterungen beziehen sich in den meisten Fällen auf eine Entwicklerversion (SVN-Version) des Projekts und sind damit auf dem aktuellen Stand der Weiterentwicklung durch die DOSBox-Entwickler. Es gibt jedoch auch verbesserte Versionen, deren Entwicklung vom betreffenden Autor eingestellt wurde und die damit nicht mehr aktuell sind. Auch ist es nicht immer eindeutig, welche Qualität die zusätzlichen Programmteile aufweisen und ganz allgemein, ob sie aus einer vertrauenswürdigen Quelle stammen.\n\nEinige Beispiele für Erweiterungen sind:\n\nMit diversen zusätzlichen Emulationen wird DOSBox zu einer annähernd vollwertigen virtuellen Maschine für DOS-basierte Betriebssysteme – beispielsweise ist es mittels spezieller Erweiterungen sogar möglich, Windows 95 innerhalb von DOSBox auszuführen. Da jedoch eine vollständige Emulation niemals die Intention der DOSBox-Entwickler war, werden diese Erweiterungen vermutlich nie in den offiziellen Entwicklerzweig aufgenommen werden.\n\nAm 23. September 2006 schrieb die englische Palm-Infoseite TamsPalm über die Alpha-Version eines neuen x86-Emulators für Palm OS.\nEntwickelt wurde sie vom Emuboards.com-Forenmitglied \"voda\", der bereits andere Emulatoren auf die Palm-OS-Plattform portiert hatte. Unter anderem wurden QBasic, Microsoft Word 5.5 und DOS Shell erfolgreich getestet. Während die Maus über den Touchscreen des PDAs gesteuert werden konnte, gab es keine Möglichkeit zur Texteingabe. Somit konnte man mit dieser Version die erste direkte x86-Emulation unter Palm OS präsentieren; eine produktive Nutzung war jedoch kaum möglich.\n\nAnfang 2008 stellte der deutsche Softwareentwickler Henk Jonas eine erweiterte Version der PalmDosBox vor. Wichtigste Neuerung war, dass nun die eingebaute Tastatur von Treo-Telefonen zur Eingabe genutzt werden konnte; für andere Geräte stand eine Bildschirmtastatur zur Verfügung. Auch diese aktuelle Version kann nicht als stabil angesehen werden, da sie auf manchen Geräten nur sporadisch funktioniert.\n\nEine weitere Besonderheit bei der Emulation auf Palm-OS-Geräten besteht darin, dass neuere Modelle nur sehr wenig dynamischen Speicher haben und die Emulatoren oft nicht ausgeführt werden können. Programme wie UDMH (Unlimited Dynamic Memory Hack) oder das Open-Source-Tool MMH (More Heap Hack) geben auf Kosten des restlichen Datenspeichers mehr dynamischen Speicher frei und ermöglichen so das Ausführen speicherintensiver Anwendungen.\n\nDie Entwickler der Windows-kompatiblen Laufzeitumgebung Wine begannen bei Version 1.3.12 mit der Integration von DOSBox in Wine.\n\nDOSBox-X ist eine Abspaltung, die besonderen Wert auf eine möglichst akkurate Emulation von Hardware legt. Hierbei wird angestrebt, sämtliche Hardware-Konstellationen der DOS-Ära abzudecken, um damit u. a. eine Testplattform für Programmierer zu bieten. Ferner ist geplant, DOSBox-X als Grundlage für die Windowsversionen 3.x, 95, 98 und ME nutzen zu können, wobei auch Beschleunigungsmechanismen implementiert werden sollen.\n\nUm DOS-basierte Software wie Spiele unter neueren Betriebssystemen lauffähig zu machen, wird DOSBox von Digitaldistributoren mit der Software mitgebündelt. So werden beispielsweise bei Steam und Good Old Games ältere Spiele mittels DOSBox ausgeführt, wobei jedoch nicht immer die neueste Version des Emulators beiliegt.\n\nSeit dem 23. Dezember 2014 verwendet das Internet Archive eine Emscripten-konvertierte DOSBox-Version für Browser-basierte Präsentation von Tausenden archivierter DOS-Computerspiele, ausschließlich für „Schul- und Forschungszwecke“.\n\n\n"}
{"id": "460841", "url": "https://de.wikipedia.org/wiki?curid=460841", "title": "Active Directory", "text": "Active Directory\n\nActive Directory (AD) heißt der Verzeichnisdienst von Microsoft Windows Server, wobei ab der Version Windows Server 2008 der Dienst in fünf Rollen untergliedert und deren Kernkomponente als Active Directory Domain Services (AD DS) bezeichnet wird.\n\nBei einem solchen Verzeichnis () handelt es sich um eine Zuordnungsliste wie zum Beispiel bei einem Telefonbuch, das Telefonnummern den jeweiligen Anschlüssen (Besitzern) zuordnet.\n\nActive Directory ermöglicht es, ein Netzwerk entsprechend der realen Struktur des Unternehmens oder seiner räumlichen Verteilung zu gliedern. Dazu verwaltet es verschiedene Objekte in einem Netzwerk wie beispielsweise Benutzer, Gruppen, Computer, Dienste, Server, Dateifreigaben und andere Geräte wie Drucker und Scanner und deren Eigenschaften. Mit Hilfe von Active Directory kann ein Administrator die Informationen der Objekte organisieren, bereitstellen und überwachen.\n\nDen Benutzern des Netzwerkes können Zugriffsbeschränkungen erteilt werden. So darf zum Beispiel nicht jeder Benutzer jede Datei ansehen oder jeden Drucker verwenden.\n\nSeit Windows Server 2008 sind unter dem Begriff Active Directory fünf verschiedene Serverrollen zusammengefasst:\n\n\nDas LDAP-Verzeichnis stellt beispielsweise Informationen über Benutzer und deren Gruppenzugehörigkeit bereit. Aber auch andere Objekte, wie zum Beispiel die Zertifikate eines Computers, werden in dem Verzeichnis gespeichert.\nLDAP selbst ist kein Verzeichnis, sondern ein Protokoll, mittels dessen es über eine bestimmte Syntax möglich ist, Informationen eines LDAP-Verzeichnisses abzufragen.\n\nKerberos ist ein Protokoll, mit welchem der Benutzer authentifiziert wird, so dass er ein sogenanntes „Ticket Granting Ticket“ (TGT) erhält. Mit diesem ist es möglich, sich Diensttickets für den Zugriff auf einen bestimmten Dienst innerhalb des Netzwerks zu besorgen. Der Benutzer muss dabei nur einmal sein Passwort eingeben, um das TGT zu erhalten. Die Besorgung der Diensttickets erfolgt dann im Hintergrund.\n\nDas CIFS-Protokoll ist für die Ablage von Dateien im Netzwerk vorgesehen. Dabei wird DNS zum Auffinden der einzelnen Computersysteme und Dienstinformationen (SRV Resource Record) genutzt. Es stellt außerdem aufgrund des standardisierten Protokolls eine Möglichkeit zur Anbindung an das Internet dar.\n\nAnders als frühere Windows-Versionen, wie zum Beispiel Windows NT 4.0, welche für die Namensauflösung NetBIOS verwendeten, ist für Active Directory ein eigenes DNS erforderlich. Um voll funktionsfähig zu sein, muss der DNS-Server SRV-Ressourceneinträge unterstützen.\n\nAus Gründen der Kompatibilität sind Windows 2000- oder XP-Clients mit entsprechender Konfiguration auch bei Einsatz von Active Directory weiterhin in der Lage, mit Hilfe von NetBIOS oder WINS Ressourcen im Netzwerk ausfindig zu machen.\n\nActive Directory ist in drei Teile aufgegliedert: Schema, Konfiguration und Domain.\n\n\nDie ersten beiden Teile von Active Directory werden zwischen allen Domain Controllern der Gesamtstruktur repliziert, während die Domain-spezifischen Informationen grundsätzlich nur innerhalb der jeweiligen Domain, also auf ihren jeweiligen Domain Controllern, verfügbar sind. Deshalb existiert in jeder Domain zusätzlich ein sogenannter \"Globaler Katalog\". Er repräsentiert alle Informationen der eigenen Domain und enthält zusätzlich wichtige Teilinformationen der anderen Domain der Gesamtstruktur und ermöglicht damit z. B. Domain-übergreifende Suchoperationen.\n\nActive Directory verwendet zur Speicherung der Informationen über die Netzwerkobjekte eine Jet (Blue)-Datenbank, die Microsoft auch für den Exchange Server einsetzt. Sie ist relational, transaktionsorientiert und benutzt ein „Write-Ahead-Logging“.\nDie Active-Directory-Datenbank ist auf 16 Terabytes begrenzt und jeder Domain Controller kann bis zu 2 Milliarden Objekte anlegen.\n\nDie Datenbankdatei „NTDS.DIT“ enthält drei Haupttabellen: die „schema table“ zur Speicherung der Schemata, die „link table“ zur Speicherung der Objekt-Struktur und die „data table“ zur Speicherung der Daten.\n\nESE (extensible storage engine) ordnet die nach einem relationalen Modell abgespeicherten Active-Directory-Daten nach einem vorgegebenen Schema in einem hierarchischen Modell an.\n\nUnter Windows 2000 benutzt Active Directory die Jet-basierende ESE98-Datenbank.\n\nIm Gegensatz zum objektorientierten Verzeichnissystem eDirectory von NetIQ ist Active Directory eher als objektbasiert – und hierarchisch – zu bezeichnen.\n\nDie Datensätze in der Datenbank werden in Active Directory als „Objekte“ und deren Eigenschaften als „Attribute“ definiert. Die Attribute sind abhängig von ihrem Typ definiert. Objekte werden eindeutig über ihren Namen identifiziert.\n\nDie Gruppenrichtlinien-Einstellungen werden in Gruppenrichtlinien-Objekten gespeichert. Diese sind ebenfalls Domains und Standorten zugeordnet.\n\nObjekte lassen sich in zwei Haupt-Kategorien einteilen:\n\nDie möglicherweise bis zu vielen Millionen Objekte werden in Containern (Organisationseinheiten), auch OUs (Organizational Unit) genannt, abgelegt. Einige Container sind vordefiniert, beliebige weitere Organisationseinheiten können mit Subeinheiten (Unterorganisationseinheiten) erstellt werden. Als objektbasiertes System unterstützt Active Directory die Vererbung von Eigenschaften eines Objektcontainers an untergeordnete Objekte, die auch wieder Container sein können. Dadurch erlaubt es Active Directory, Netzwerke logisch und hierarchisch aufzubauen.\n\nDer Verbund mehrerer zusammengehöriger Domains heißt im englischen Original „forest“, deutsch „Gesamtstruktur“. Die wichtigsten Informationen aller enthaltenen Domains sind zentral im Globalen Katalog abrufbar, außerdem benutzen alle Domains dasselbe Verzeichnis-Schema. Die Verwendung von Sicherheitsinformationen (z. B. Nutzer-Rechte/-Gruppen-Zuordnungen) sowie Schema-Erweiterungen sind so Domain-übergreifend möglich. Die Gesamtstruktur kann verschiedene \"Bäume\" (trees) enthalten, das sind jeweils Domains, die im selben DNS-Namensraum liegen (z. B. buchhaltung.meinefirma.de und meinefirma.de). Auch eine einzelne Domain bildet schon eine Gesamtstruktur, die später um weitere Domains ergänzt werden kann.\n\nEine Organisationseinheit (OU) ist ein Containerobjekt, das zum Gruppieren anderer Objekte im AD dient. Eine OU kann neben Objekten auch andere OUs enthalten. Die frei definierbare Hierarchie der OUs vereinfacht die Administration von Active Directory. In der Regel richtet sie sich nach den Netzwerkstrukturen (Netzwerkverwaltungsmodell) oder nach der Organisationsstruktur des Unternehmens. Die OUs sind die unterste Ebene von Active Directory, in der administrative Rechte aufgeteilt werden können.\n\nEine Möglichkeit der Unterteilung sind Standorte. Diese stellen eine räumliche Gliederung der IP-Unternetze innerhalb der Gesamt-Topologie dar.\n\nDie schnellen Netzwerke (LAN) der Standorte sind meistens durch langsamere Netzwerke (WAN) untereinander verbunden. Die Standort-Bildung ist deshalb wichtig für die Kontrolle des Netzwerkverkehrs, der durch Replikationsvorgänge entsteht. Domains können Standorte enthalten, und Standorte können Domains beinhalten.\n\nEs ist fundamental, die Infrastruktur der Unternehmensinformationen in eine hierarchische Aufteilung in Domains und Organisationseinheiten sorgfältig zu planen. Hierfür haben sich Aufteilungen hinsichtlich geografischer Orte, Aufgaben oder IT-Rollen oder einer Kombination aus diesen Modellen als nützlich erwiesen.\n\nUnter Windows NT gab es pro Domain immer einen ausgezeichneten Controller, den primären Domain Controller (PDC), der Änderungen an der Nutzer- und Computerdatenbank (SAM) ausführen durfte. Alle anderen Domain Controller dienten als Sicherungskopie, die im Bedarfsfall zu einem PDC hochgestuft werden können.\n\nActive Directory nutzt für die Replikation des Verzeichnisses zwischen den Domain Controllern eine sogenannte Multimaster-Replikation. Das hat den Vorteil, dass sich jedes Replikat beschreiben und synchronisieren lässt. Somit ist bei verteilten Implementierungen eine lokale Administration vollständig möglich.\nIm Gegensatz zu NT4- Domains besitzen ab Windows 2000 alle Domain Controller (DC) eine beschreibbare Kopie der Active-Directory-Datenbank. Die Veränderung eines Attributes auf einem der DCs wird in regelmäßigen Intervallen an alle anderen DCs weitergegeben (repliziert). Dadurch sind alle DCs auf demselben Stand. Der Ausfall eines DCs ist für die Active-Directory-Datenbank unerheblich, da keine Informationen verloren gehen. Das Replikationsintervall kann je nach Änderungshäufigkeit auf 15 oder mehr Minuten eingestellt werden. Windows 2000 Server repliziert das AD standardmäßig nach spätestens 5 Minuten, Windows Server 2003 repliziert es standardmäßig nach spätestens 15 Sekunden. Da eine Replikation über höchstens 3 Hops geht, erhält man je nach verwendeter Serverversion 15 Minuten bzw. 45 Sekunden als Replikationsintervall für eine Domain.\n\nActive Directory unterstützt eine Benennung und den Zugriff über UNC/URL- und LDAP-URL-Namen. Intern wird die LDAP-Version X.500 für die Namensstruktur verwendet. Jedes Objekt hat einen vollqualifizierten Namen (distinguished name, DN). Ein Druckobjekt heißt beispielsweise „LaserDrucker3“ in der organisatorischen Einheit „Marketing“ und der Domain „foo.org“. Der voll qualifizierte Name ist somit „CN=LaserDrucker3,OU=Marketing,DC=foo,DC=org“. „CN“ steht hierbei für „common name“. „DC“ ist die Domain-Objekt-Klasse (domain component), die aus sehr vielen Teilen bestehen kann. Die Objekte können auch nach der UNC/URL-Notation bezeichnet werden. Diese zeichnet sich durch eine umgekehrte Reihenfolge der Bezeichner aus, welche durch Schrägstriche voneinander getrennt sind. Das obige Objekt könnte somit auch mit „foo.org/Marketing/LaserDrucker3“ bezeichnet werden. Um Objekte innerhalb der Container anzusprechen, werden relative Namen (relative distinguished names, RDNs) verwendet. Dies wäre für den Laserdrucker „CN=LaserDrucker3“. Jedes Objekt hat neben seinem global eindeutigen Namen eine ebenfalls global eindeutige 128 Bit lange Nummer (globally unique identifier, GUID). Diese wird üblicherweise als Zeichenfolge dargestellt und ändert sich auch beim Umbenennen des Objekts nicht. Weiterhin kann jedes Benutzer- und Computerobjekt auch eindeutig über seinen zugeordneten UPN (User Principal Name) angesprochen werden, der den Aufbau „Objektname“@„Domainname“ hat.\n\nEs existieren neben Active Directory weitere Verzeichnisdienste, die zwar LDAP und Kerberos implementieren, jedoch nicht AD-kompatibel sind. Einige Software-Produkte emulieren jedoch auch ein Active Directory. Damit können Windows- und andere Clients ohne zusätzlich aufgebrachte Software einer Domain beitreten und einen Großteil der Möglichkeiten eines Active Directory, wie z. B. die zentrale Authentifizierung und Verwaltung, nutzen, ohne dass Windows Server eingesetzt wird.\n\nNeben Windows Server kann auch die freie Software Samba für Linux- und Unix-Systeme einen Active-Directory-Verzeichnisdienst zur Verfügung stellen. Die aktuelle Version 4 umfasst eine vollständige Implementierung von Active Directory und kann damit einen Windows-Server in dieser Hinsicht vollständig ersetzen. Dies wurde nicht zuletzt durch die Unterstützung, die das Samba-Projekt von Microsoft direkt erhalten hatte, möglich.\n\nÄhnliche Funktionen wie Active Directory bietet das von NetIQ entwickelte eDirectory. Es ist für Windows sowie für Linux verfügbar und erlaubt anders als Active Directory auch die Verwaltung einer inhomogenen IT-Infrastruktur. Mittels des Aufsatzes \"Domain Services for Windows\" kann eDirectory ein Active Directory emulieren.\n\n\n\n"}
{"id": "461879", "url": "https://de.wikipedia.org/wiki?curid=461879", "title": "Schönhage-Strassen-Algorithmus", "text": "Schönhage-Strassen-Algorithmus\n\nDer Schönhage-Strassen-Algorithmus ist ein Algorithmus zur Multiplikation zweier \"n\"-stelliger ganzer Zahlen. Er wurde 1971 von Arnold Schönhage und Volker Strassen entwickelt. Der Algorithmus basiert auf einer sehr schnellen Variante der diskreten schnellen Fourier-Transformation sowie einem geschickten Wechsel zwischen der Restklassen- und der zyklischen Arithmetik in endlichen Zahlenringen.\n\nDer Schönhage-Strassen-Algorithmus terminiert in formula_1 (siehe Landau-Notation), wenn als Effizienzmaß die Bitkomplexität auf mehrbändigen Turingmaschinen, also die maximale Laufzeit des Algorithmus gemessen als benötigte Bitoperationen in Abhängigkeit von der Bitlänge formula_2 der Eingabegrößen gewählt wird.\nDiese Komplexität stellt eine Verbesserung sowohl gegenüber dem naiven aus der Schule bekannten Algorithmus der Laufzeit formula_3 als auch gegenüber dem 1962 entwickelten Karatsuba-Algorithmus mit einer Laufzeit von formula_4 sowie dessen verbesserter Variante, dem Toom-Cook-Algorithmus mit formula_5 Laufzeit dar.\n\nDer Schönhage-Strassen-Algorithmus war von 1971 bis 2007 der effizienteste bekannte Algorithmus zur Multiplikation großer Zahlen; 2007 veröffentlichte Martin Fürer eine Weiterentwicklung des Algorithmus mit der noch niedrigeren asymptotischen Komplexität formula_6, wobei formula_7 der iterierte Logarithmus von n ist.\nDurch Optimierungen des Algorithmus von Fürer erreichten David Harvey, Joris van der Hoeven und Grégoire Lecerf eine weitere Verbesserung der asymptotischen Laufzeit auf formula_8.\n\nBis 2007 konnte kein effizienterer Algorithmus gefunden werden. Als untere Schranke gibt es für den allgemeinen Fall nur die (triviale) lineare Laufzeit, an die sich der Algorithmus mit wachsender Zahlenlänge annähert. Allerdings haben die Forscher Hinweise dafür gefunden, dass die Schranke formula_9 niemals unterboten werden kann.\nSelbst bei modernen Computern ist diese Methode der Berechnung erst bei Zahlen mit mehreren tausend Stellen effizienter als der Karatsuba-Algorithmus. Dies liegt wohl allerdings weniger am Overhead des Schönhage-Strassen-Algorithmus, sondern vielmehr an der seit Jahrzehnten typischen Designoptimierung der Computerprozessoren, die dem Erreichen schneller Gleitkommaoperationen den Vorzug vor der Arithmetik in endlichen Restklassenringen ganzer Zahlen gibt.\n\nFür die Suche nach den Algorithmen mit der besten (Zeit-) Komplexität in der Computer-Algebra genießt der Schönhage-Strassen-Algorithmus zentrale Bedeutung.\n\nUm zwei ganze Zahlen formula_10 und formula_11 zu multiplizieren, wird im Groben folgendes Schema angewandt:\n\nDie im mittleren Schritt durchzuführenden kleinen Multiplikationen werden im rekursiven Sinne wiederum durch den Schönhage-Strassen-Algorithmus ausgeführt.\n\nUm zu verstehen, warum das Ergebnis das Produkt der Zahlen a und b ist, betrachtet man die Polynome\n\nSetzt man formula_16 ein, so erhält man gerade die Binärdarstellung der Zahlen a und b. Zu berechnen ist formula_17 für das Produktpolynom\n\nWir bestimmen die Fouriertransformierte der Koeffiziententupel von A und B:\n\nAnders gesagt wertet man die beiden Polynome an den Stellen formula_23 aus. Multipliziert man nun diese Funktionswerte, so ergeben sich die entsprechenden Funktionswerte des Produktpolynoms\nUm das Polynom formula_25 selbst zu gewinnen, müssen wir die Transformation rückgängig machen:\nNach Definition der Einheitswurzeln gilt formula_32.\nDiese genügt folgender Identität geometrischer Summen von Einheitswurzeln:\ndenn\n\nSomit gilt:\n\nIm Artikel Diskrete Fourier-Transformation sind die mathematische Grundlagen dieser Transformation weiter ausgeführt. Da bei der Transformation formula_39 Summen mit jeweils formula_39 Termen entstehen, haben wir bei einer klassischen Berechnung der Terme (etwa durch das Horner-Schema) nach wie vor eine quadratische Laufzeit. Mittels der schnellen Fourier-Transformation kann man diese Werte schneller berechnen. Diese Berechnung beruht auf folgendem Teile und herrsche Prinzip:\n\nMan setzt Teillösungen mittels einfacher Operationen (Addition und einfache Multiplikation) zusammen. Damit können die Transformationen in Zeit formula_44 berechnet werden. Durch das Runden der komplexen Einheitswurzeln auf feste Stellenlänge ergeben sich jedoch Rechenfehler. Um diese auszugleichen, muss für ein resultierendes Bit mit mindestens formula_45 Bits gerechnet werden. Daraus ergibt sich eine Gesamtlaufzeit von formula_46. Bei der Schönhage-Strassen-Variante rechnen wir stattdessen in einem Restklassenring und vermeiden damit die Rechenfehler der komplexen Zahlen.\n\nDes Weiteren ist die Multiplikation keine reine Faltung, sondern es kann auch zu Überträgen kommen; nach Durchführen der FT und iFT müssen diese passend behandelt werden.\n\nDie Aufgabe der Multiplikation zweier ganzer Zahlen wird nun wie folgt konkretisiert:\n\nEs seien die zwei zu multiplizierenden Zahlen formula_47 in Binärzifferdarstellung gegeben. Weiter sei formula_48 die maximale Länge (also Binärziffernanzahl) der beiden Zahlen.\n\nNach passender Behandlung der Vorzeichen der beiden Zahlen sowie der trivialen Sonderfälle formula_49 und formula_50 (was mit linearem Aufwand formula_51 machbar ist) darf man davon ausgehen, dass formula_52 natürliche Zahlen sind.\nDer Schönhage-Strassen-Algorithmus löst diese Aufgabe in formula_53.\n\nDie oben angesprochene superschnelle DFT, die das Kernstück des Algorithmus darstellt, muss etwas ausführlicher erläutert werden, da sie hier sehr speziell eingesetzt wird.\n\nEs sei formula_54 ein kommutativer unitärer Ring. In formula_54 sei das Element formula_56 eine Einheit; weiterhin sei formula_57 eine formula_39te Einheitswurzel (also formula_59), die die Gleichheit formula_60 erfüllt. Dann lässt sich die Berechnung der diskreten Fouriertransformation (DFT) im Produktraum formula_61 (dies ist eine Kurznotation für formula_62; der Begriff Vektorraum ist hier nur für den Fall, dass formula_54 ein Körper ist, üblich) wie folgt in einer schnellen Variante (als FFT) durchführen:\n\nZu berechnen ist für formula_64 die Transformierte formula_65 mit\n\nIndem wir die Indizes formula_68 und formula_69 in Binärdarstellung aufschreiben, wobei wir dies bei der Zahl formula_70 in umgekehrter Reihenfolge tun, ist die Transformierte formula_71 wie folgt optimiert berechenbar:\n\nEs seien\nund\n\nDie geschlossene Darstellung für diese Zwischenterme ist\n(Zum Nachrechnen dieser Darstellung beachte man formula_80).\n\nDiese Rekursion liefert die gewünschten Fourierkoeffizienten formula_81.\n\nAufgrund der Eigenschaft formula_82 können wir den Rekursionsschritt etwas berechnungsfreundlicher umformen zu\nund\nmit dem \"gleichen\" Exponenten formula_87.\n\nDie Umkehrtransformation, also die inverse FFT, gelingt, da wir vorausgesetzt haben, dass formula_56 im Ring formula_54 invertierbar ist:\nsowie\nwobei wiederum formula_94 ist.\n\nIn der Anwendung im Schönhage-Strassen-Algorithmus wird tatsächlich nur eine halbierte FFT benötigt; gemeint ist damit folgendes: Beginnen wir im 1. Schritt der Rekursion mit der Berechnung\nnur für formula_96 und schränken wir die weiteren Schritte der Rekursion ebenso auf formula_96 ein, so berechnen wir gerade alle formula_98 für ungerade Werte formula_99. Will man umgekehrt aus diesen formula_98 für ungerade formula_99 (das sind formula_102 Stück) lediglich die Differenzen formula_103 der ursprünglichen formula_104 zurückgewinnen, so genügt auch in der Rückrichtung die halbierte Rekursion.\n\nIm Schönhage-Strassen-Algorithmus wird die geschilderte schnelle Fouriertransformation für endliche Zahlenringe formula_105 mit Fermatzahlen formula_106 benötigt.\n\n\"Hinweis zur Notation:\" Für den Restklassenring formula_107 benutzen wir hier die kürzere Schreibweise formula_108, die lediglich im Kontext der p-adischen Zahlen zu Verwechslungen führen könnte.\n\nAls Einheitswurzel wird im Ring formula_105 die Zahl formula_56 (oder je nach Kontext auch eine geeignete Potenz von 2) zum Einsatz kommen. Die beim FFT-Algorithmus durchzuführenden Multiplikationen sind dann von der Form formula_111; allerdings sind sie nicht als reine Shift-Operationen durchführbar, da das Reduzieren eines größeren Zwischenergebnisses modulo formula_112 noch nachgeschoben werden muss. Hier greift eine der brillanten Ideen von Schönhage und Strassen: Sie betten den Ring (ausgestattet mit der Restklassenarithmetik) passend in einen größeren, mit der \"zyklischen\" Arithmetik ausgestatteten Überring ein. Dieser Überring hat eine 2-Potenz als Ordnung, so dass in ihm die entsprechende Multiplikation tatsächlich als reine Shift-Operation durchführbar ist. Diesen Trick kann man in einem schönen Struktursatz über Restklassen- und zyklische Arithmetik in endlichen Zahlenringen zusammenfassen.\n\nDer Struktursatz über zyklische Arithmetik lässt sich formal wie folgt fassen:\n\nFür eine Zweierpotenz formula_113 mit einer natürlichen Zahl formula_114 gilt\n\nHierbei bezeichnet formula_116 die durch die Repräsentanten formula_117 darstellbaren Restklassen modulo formula_118 ausgestattet mit der Restklassenarithmetik, d. h. mit der Addition und Multiplikation modulo formula_118. Die in diesem Restklassenring vorkommenden Zahlen können mit formula_120 Binärziffern dargestellt werden.\n\nDie auf der rechten Seite vorkommende Struktur formula_121 bezeichnet die Restklassen modulo der Zahl formula_122, die allerdings nicht mit der Restklassenarithmetik, sondern abweichend mit der zyklischen Arithmetik ausgestattet werden. Hierbei werden bei Zwischenergebnissen, die zu groß werden, Überträge aufgehoben und auf das Endergebnis additiv aufgeschlagen. Dies entspricht in Binärzifferdarstellung einer Verschiebung der überständigen Binärziffern (rechtsbündig an die niedrigsten Zifferpositionen gestellt) mit nachfolgender Addition. Beispielsweise ergibt die Addition formula_123 mit formula_124 nicht den Wert formula_125, sondern den Wert formula_126. Aus der so erhaltenen Zahlenstruktur mit zyklischer Arithmetik wird nun noch der Faktorring modulo formula_118 gebildet. Es werden also die Endergebnisse noch modulo formula_118 reduziert.\n\nDamit besagt dieser Struktursatz folgendes: Das modulo-Rechnen in formula_129 kann ebenso ersetzt werden durch das zyklische Rechnen im größeren Zahlenraum formula_130 mit nachfolgendem Reduzieren modulo formula_118.\n\nEntscheidend für das Gelingen der in diesem Struktursatz vorgestellten Einbettung ist die Eigenschaft, dass die größte darstellbare Zahl formula_132 im zyklischen Zahlenraum (hier ist dies die Zahl formula_133) die Zahl formula_134 aus dem Restklassenring formula_129 repräsentiert. Hierfür ist die Bedingung formula_136 notwendig. Damit die zyklische Arithmetik aber überhaupt sinnvoll definiert werden kann, muss andererseits formula_137 eine Zweierpotenz sein. Zusammen ergibt sich, dass formula_138 die optimale Wahl für die Größe des zyklischen Einbettungsraumes darstellt.\n\nDer klassische Restklassenring formula_139 wäre für die Einbettung dagegen nicht geeignet, denn in diesem Ring gilt formula_140, d. h. die Zahl formula_56 ist in diesem Ring ein Nullteiler.\n\nHaben wir die zu multiplizierenden Zahlen formula_142 mit formula_143 Binärziffern vorliegen, so führen wir je nachdem, ob formula_144 gerade oder ungerade ist, unterschiedliche Rekursionsschritte aus, um die Stellenzahl in einem Einzelschritt zu logarithmieren:\n\nDiesen Schritt der Rückführung von formula_145 auf formula_2 führen wir mit der Komplexität formula_147 durch.\n\nEs seien formula_148 mit formula_145 und der Fermatzahl formula_150 zu multiplizieren. Wir werden in diesem Schritt die Rückführung auf die Fermatzahl formula_151 vollziehen.\n\nFür die zu den beiden Fermatzahlen gehörenden Zweierpotenzen führen wir die Abkürzungen\nund\nein. Die halbierte Stellenzahl von formula_154 wird unsere Stückelungsgröße werden, d. h. wir entwickeln formula_10 und formula_11 nach Potenzen von formula_157:\nwobei für die Einzelstücke formula_160 gilt.\nIn Binärdarstellung entspricht diese Zerlegung einer einfachen Gruppierung der Bitfolgen in Stücke der Länge formula_102 Bits.\n\nEine kleine Schwäche des Algorithmus (die allerdings der erreichten Komplexitätsschranke keinen Abbruch tut) offenbart sich jetzt. Um die superschnelle DFT auf die Stückfolgen formula_162 und formula_163 anwenden zu können, müssen diese zur nächsten Zweierpotenzlänge mit Nullen aufgefüllt werden; die Zahlendarstellung wird also künstlich verlängert zu\n\nVermöge des oben erwähnten Struktursatzes zur zyklischen Arithmetik wechseln wir nun vom Restklassenring formula_166 über zum Quotientenraum\nformula_167 mit der zyklischen Arithmetik. In diesem Raum errechnet sich für die Multiplikationsaufgabe\nwobei wir im letzten Schritt die Eigenschaft formula_172 in diesem zyklischen Zahlenraum benutzt haben.\n\nZusammenfassend erhält die Multiplikation also die Form\nmit den Ergebniskoeffizienten\nWir können formula_175 nach oben abschätzen.\n\nNun folgt eine Umschreibung der Summenformel, damit wir uns bei der anzuwendenden FFT auf eine halbierte FFT beschränken können.\n\nEs gilt formula_176,\nalso ist\nmit formula_178 in formula_166.\nDurch passende Addition können wir den Wertebereich ins Positive verschieben, es ist nämlich\nformula_180, und mit der Definition\ngilt\nFür die nichttrivialen formula_183 (Indizes formula_134 bis formula_185) gilt die Abschätzung formula_186. Da die beiden Zahlen formula_187 und formula_112 teilerfremd ist, genügt zur Bestimmung der formula_183 die Berechnung der Reste formula_190 und formula_191.\n\nHat man nämlich die Reste formula_192 und formula_193 bestimmt, so kann man in Komplexität formula_194 wie folgt rechnen: Berechne erst formula_195 und dann formula_196.\n\nHier wenden wir einen für die Computeralgebra sehr typischen Trick an: Wir setzen die Stückfolgen formula_197 und formula_198 durch Einfügen genügend langer Nullsequenzen mit Sicherheitsabständen so zusammen, dass nach Produktbildung die Einzelergebnisse ebenfalls noch ohne Überlappungen in Stücken aneinandergereiht sind. Es seien also formula_199 und formula_200 in formula_201. Wir bilden nun\nund haben dabei formula_204. Das Produkt formula_205 enthält dann in disjunkten Stücken der Bitlänge formula_206 die Summen\nmit formula_208, denn es ist formula_209. Für die Terme formula_210 unserer ursprünglichen Multiplikationsaufgabe formula_211 sehen wir\nFür die zu bestimmenden Reste formula_213 erhalten wir\n\nDer Komplexitätsaufwand für die Bildung aller formula_216 sowie der Extraktion der formula_217 ist formula_218; die Multiplikation formula_205 kostet formula_220, insgesamt ist dies also formula_218.\n\nHier kommt die DFT zum Einsatz. Wir unterziehen die Vektoren formula_222 und formula_223 mit formula_224 der DFT in formula_225 mit formula_226 und der Zahl formula_56 als formula_228-ter Einheitswurzel. Da wir nur die Differenzen formula_229 benötigen, genügt die halbierte DFT:\nDer Komplexitätsaufwand hierfür besteht aus formula_240 Schritten des Einzelaufwands formula_194 für die DFT (gesamt also formula_242); hinzu kommen die Addition von formula_243 sowie die Reduktionen modulo formula_244 für die Gewinnung der formula_245, was in formula_218 bewältigt werden kann.\n\nAuch für diesen Schritt der Rückführung von formula_247 auf formula_2 wird die Komplexität formula_147 erreicht.\n\nEs seien formula_148 mit formula_247 und der Fermatzahl formula_150 zu multiplizieren. Wir werden auch in diesem Schritt die Rückführung auf die Fermatzahl formula_151 vollziehen.\n\nFür die zu den beiden Fermatzahlen gehörenden Zweierpotenzen führen wir analog die Abkürzungen\nund\nein. Wiederum wird die halbierte Stellenzahl von formula_154 unsere Stückelungsgröße werden, d. h. wir entwickeln formula_10 und formula_11 nach Potenzen von formula_157:\nwobei für die Einzelstücke formula_160 gilt.\n\nWie oben verlängern wir die Zahlendarstellung auf Zweierpotenzlänge zu\nund analog für formula_11.\n\nUnter abermaliger Zuhilfenahme des Struktursatzes zur zyklischen Arithmetik wechseln wir nun vom Restklassenring formula_166 über zum Quotientenraum\nformula_167 mit der zyklischen Arithmetik.\n\nDamit können wir wieder\nmit den Ergebniskoeffizienten\ndarstellen.\nDabei können wir formula_269 nach oben abschätzen.\n\nAus formula_270 können wir wieder\nfolgern, und mit\ngilt\nmit formula_274.\nFür die nichttrivialen formula_183 (Indizes formula_134 bis formula_277) gilt die Abschätzung formula_278. Wegen der Teilerfremdheit der beiden Zahlen formula_228 und formula_112 genügt es wieder zur Bestimmung der formula_183, die Reste formula_190 und formula_191 zu berechnen.\n\nWir wenden wieder den Trick der Einfügung von Sicherheitsabständen an: Es seien also formula_284 und formula_285 in formula_286. Wir bilden\nund haben dabei formula_289. Das Produkt formula_205 enthält dann in disjunkten Stücken der Bitlänge formula_291 die Summen\nmit formula_233. Für die gesuchten formula_210 unserer ursprünglichen Multiplikationsaufgabe formula_211 sehen wir\nFür die zu bestimmenden Reste formula_297 erhalten wir\n\nMit formula_226 unterziehen wir wieder die Vektoren formula_301 und formula_302 mit formula_224 der DFT in formula_61, wobei wir diesmal die Zahl formula_305 als formula_39-te Einheitswurzel wählen. Da wir nur die Differenzen formula_307 benötigen, genügt hier wiederum die halbierte DFT:\n\nStartend mit formula_10 und formula_11 mit Ziffernlänge formula_2 wird durch die dargestellte Rekursion eine Komplexität von formula_321 erreicht.\n\nZimmermann und Brent beschreiben eine Variante des Algorithmus, bei der die Laufzeit (in Abhängigkeit von der Länge der Eingabe) keine Sprünge macht, sondern stetiger verläuft. Dies wird erreicht, indem die DFT-Vektoren nicht aus formula_39-stelligen Binärzahlen, sondern Zahlen der passenden Länge gebildet werden. Dadurch muss die Länge der zu transformierenden Vektoren keine Zweierpotenz sein.\n\n\n"}
{"id": "462566", "url": "https://de.wikipedia.org/wiki?curid=462566", "title": "Linux-VServer", "text": "Linux-VServer\n\nVServer ist ein Open-Source-Projekt, das virtuelle Linux-Server auf Basis von Betriebssystemvirtualisierung für den Linux-Kernel implementiert. Das Produkt ist freie Software, die unter der GNU General Public License (GPL) veröffentlicht wird. Es verfolgt damit einen grob vergleichbaren Ansatz wie OpenVZ.\n\nDas Projekt wurde von Jacques Gélinas begonnen und wird jetzt von dem Österreicher Herbert Pötzl betreut. Das Produkt hat nichts mit dem Linux Virtual Server zu tun, der für eine gleichmäßige Lastverteilung (Load Balancing) sorgen soll.\n\nLinux-VServer schafft die Möglichkeit, Teile eines Computersystems wie das Dateisystem, die Prozessorzeit, Netzwerkadressen und den Hauptspeicher so aufzuteilen und sicher voneinander abzugrenzen, dass einzelne Prozesse keine Möglichkeit haben, außerhalb ihrer Partition, des ihnen zugestandenen Arbeitsbereiches, zuzugreifen.\n\nJede Partition bzw. jeder Arbeitsbereich stellt einen sogenannten \"Security Context\" dar, in dem das virtualisierte Computersystem einen \"virtuellen Server\" bildet. Ein dem Chroot-Mechanismus vergleichbares Werkzeug bietet Zugriff auf diesen \"Security Context\". Das Booten dieses \"virtuellen Servers\" beschränkt sich dann auf Durchführen eines Init in diesem \"Security Context\", das Herunterfahren entspricht vergleichbar einem Beenden der in diesem \"Security Context\" ablaufenden Prozesse. Die \"Security Contexte\", also die virtuellen Maschinen, sind leistungsfähig genug, um beispielsweise viele Linux-Distributionen unmodifiziert verwenden und verarbeiten zu können, dazu gehören auch Debian und Fedora.\n\n\"Virtuelle Server\" werden normalerweise von Webhosting-Dienstleistern verwendet, da sie bei gleichzeitiger Zusammenfassung der Computerressourcen eine saubere Trennung verschiedener virtueller Kundenmaschinen sowie eine Abschottung dieser Maschinen untereinander erlauben. Um bei solchen Installationen mit möglichst wenig Plattenplatz auszukommen, wird das Dateisystem jedes \"virtuellen Servers\" als ein Copy-On-Write Hard Link auf ein virtuelles Dateisystem aufgesetzt. Dieser Hard Link besitzt spezielle Dateisystemattribute, die bei einer erkannten Änderung die zu schreibende Datei durch eine sicher und transparent geschriebene reale Kopie der Datei ersetzen.\n\nLinux-VServer existiert in zwei verschiedenen Versionen, einer stabilen Version V.2.2.x und einer Entwicklerversion V.2.3.x für den Linux-Kernel 2.6, sowie einer stabilen Version für den Linux-Kernel 2.4.\n\nAndere vergleichbare Virtualisierungstechniken sind OpenVZ, von FreeBSD das \"FreeBSD Jail\"-Verfahren, \"Solaris Containers\" und \"FreeVPS\" (eine frühe Ableitung (Fork) von Linux-VServer).\n\n\n\n\n"}
{"id": "462801", "url": "https://de.wikipedia.org/wiki?curid=462801", "title": "Digital Mock-Up", "text": "Digital Mock-Up\n\nDer Begriff Digital Mock-Up (DMU) bzw. Digitales Versuchsmodell (D-VM) bezeichnet ein möglichst wirklichkeitsgetreues, computergeneriertes Versuchsmodell, das hauptsächlich verwendet wird, um einen Teil der sehr teuren, realen Produktprüfung durch Computersimulationen zu ersetzen.\n\nDMU-Modelle sind übliche, rein geometrische CAD-Modelle bei denen die geometrischen Volumen mit physikalischen Werkstoffkenngrößen und weiteren Metadaten der geplanten Produktteile so ergänzt bzw. erweitert werden, dass sich der resultierende Datensatz für die Einsetzung in computergestützte physikalische Simulationen bzw. allgemein zur Datenverarbeitung eignen. \n\nZiele des DMUs sind das Ersetzen von physischen Versuchsmodellen (Physical Mock-Up - PMU) und die Bereitstellung verschiedener, aktueller und konsistenter Sichtweisen auf die Gestalt und Funktion eines Produktes.\n\nDie Idee des DMU entstand aus den Überlegungen, wie man die kosten- und zeitintensiven physischen Versuchsträger (PMU) mit Hilfe der Rechnerunterstützung ersetzen könnte. Vorreiter dieser Technik war die Flugzeugindustrie. Die Boeing 777 wurde als erstes Produkt vollständig digital und dreidimensional beschrieben. Mit Hilfe des DMU konnte innerhalb des Projektes die Entwicklungszeit bei gleichzeitiger Verringerung von Änderungen und Fehlern verkürzt werden. Zusätzlich konnte die Passgenauigkeit der Teile und der Systeme verbessert werden. Mittlerweile wird diese Technik auch im Automobilbau und Schiffbau erfolgreich eingesetzt.\n\nDas Digital Mock-Up zählt zu den Bestandteilen der virtuellen Produktentwicklung (VPE) und des Product Lifecycle Managements (PLM). Dies wird als ganzheitlicher Lösungsansatz zur Bewältigung des Zeit- und Kostendruckes sowie dem Handling der Variantenvielfalt gesehen. Hinsichtlich der Abstimmung, Analyse und Konkretisierung von Entwicklungsergebnissen bietet die VPE eine frühzeitige, kontinuierliche, vernetzte und integrierte Unterstützung. Folglich führt eine konsequente Anwendung von DMU Methoden und Prozessen zu einer Reduzierung der Time-to-Market, Steigerung der Qualität und Minimierung der Kosten.\n\nAuf Basis eines DMU können eine Vielzahl von statischen sowie dynamischen Untersuchungen wie Ein- und Ausbauuntersuchungen, Kollisionsprüfungen, Packaging, Bewegungsprüfung flexibler Bauteile und Baubarkeitsprüfungen ausgeführt werden. Ferner profitieren weitere Bereiche wie z. B. CAE/ Simulation, Einkauf, Produktion, Marketing und Aftersales von einem DMU, womit durchgängige Prozesse im Sinne des PLMs von hoher Bedeutung sind. In der Industrie wird derzeit vermehrt an der Integration von Virtual Reality Werkzeugen in den DMU Prozess gearbeitet.\nZur realen Überprüfung der virtuellen Ergebnisse steht am Ende eines DMU-Prozesses meist immer noch ein Physical Mock-Up.\n\n"}
{"id": "462950", "url": "https://de.wikipedia.org/wiki?curid=462950", "title": "Hybridrechner", "text": "Hybridrechner\n\nAls Hybridrechner werden historische Computeranlagen bezeichnet, die eine Kombination aus Elementen von Analog- und Digitalrechnern verwenden und so die Vorteile beider Technologien vereinigen. Gegen Ende der 1960er-Jahre wurden sie vermehrt entwickelt und im technisch-wissenschaftlichen Sektor eingesetzt. Die wichtigsten Bestandteile dieser Elektronik sind Analog-Digital-Umsetzer und deren Gegenstücke, Digital-Analog-Umsetzer.\n\nEinsatzbereiche:\n\nAls letzter Hybridrechner gilt der Dornier 960.\n\n"}
{"id": "464895", "url": "https://de.wikipedia.org/wiki?curid=464895", "title": "Commodore Max", "text": "Commodore Max\n\nDer Commodore MAX, ebenfalls bekannt als Ultimax in den USA und VC10 in Deutschland, war ein Computer, der von Commodore International in Japan entworfen und verkauft wurde. Im Jahr 1982 war er ein Vorgänger des bekannteren C64.\n\nDas Gerät hatte eine Folientastatur und verfügte über 2 kB RAM (2048 mal 8 bit) und 0,5 kB Farbspeicher (1024 mal 4 bit). Der Prozessor war ein MOS Technology 6510 mit 1,02 MHz. Für die Anzeige wurde ein Fernseher verwendet. Die Software wurde ausschließlich von Steckmodulen geladen. Der MAX selbst besaß kein ROM. Chipsatz und CPU (6510) entsprachen dem C64, und MAX-Steckmodule funktionieren auch im C64. Als externer Speicher konnte eine Datasette eingesetzt werden. Anschlüsse für ein Diskettenlaufwerk, einen Drucker oder ein Modem waren nicht vorhanden. Als Programmiersprachen wurden die Steckmodule MAX BASIC (2047 freie Bytes und mit LOAD- und SAVE-Kommandos) und MINI BASIC (510 freie Bytes und keine LOAD- und SAVE-Kommandos) angeboten.\n\nDer Rechner sollte für 200 US-Dollar verkauft werden.\n\nDer MAX hatte bessere Grafik- und Soundfähigkeiten (MOS Technology SID) als der für ungefähr den gleichen Preis erhältliche Commodore VC 20, letzterer war jedoch besser erweiterbar, hatte ein viel größeres Softwareangebot und eine bessere Tastatur. Daher verkaufte sich der MAX nicht gut, und die Produktion wurde schnell eingestellt.\n\nZwar erwähnt die C64-Anleitung den MAX namentlich und deutet an, dass Commodore beabsichtigte, ihn international zu verkaufen, jedoch ist unklar, ob er überhaupt jemals außerhalb Japans verkauft wurde. Auch deutsche Zeitschriftenwerbungen wurden als Vorankündigungen für das Gerät geschaltet, sie nannten als Verkaufsstart den Oktober 1982. Letztlich wurde das Gerät in Deutschland jedoch nie verkauft. Heute gilt es als Rarität.\n\nFür den Max wurden folgende Titel veröffentlicht:\n\n\nDie Modulsoftware, die für diesen Rechner herauskam, war auch auf dem C64 lauffähig. Beim Einschalten des C64 wechselt dieser in den Ultimax-Modus, sofern ein solches Modul eingesteckt ist. Der Wechsel verursacht einige Veränderungen der Speicheraufteilung.\n\n"}
{"id": "465302", "url": "https://de.wikipedia.org/wiki?curid=465302", "title": "Visicalc", "text": "Visicalc\n\nVisicalc bzw. VisiCalc (aus engl. \"visible calculator\") war das erste kommerzielle Tabellenkalkulationsprogramm für Personal Computer und Heimcomputer. Es wurde erstmals 1979 für den Apple II durch das Unternehmen Personal Software auf den Markt gebracht. Das Konzept stammt von Dan Bricklin, die Software wurde gemeinsam mit Bob Frankston erstellt. Das Programm ermöglichte erstmals kaufmännische Berechnungen, ohne Programmierkenntnisse beim Nutzer vorauszusetzen. Später erschienen Portierungen auch für Apple III, für Atari 400 und 800, den Commodore PET, den TRS-80 und schließlich für Intel-8086-basierte Computer von IBM.\n\nMit dem Erscheinen von Lotus 1-2-3 brach der Verkauf von Visicalc rasant ein. Später wurde Visicalc von Lotus 1-2-3 übernommen und kurze Zeit später der Vertrieb eingestellt.\n\nDie Dateiendung von Visicalc-Dateien war codice_1. Der Gründer von Autodesk, John Walker, sagte am 18. September 1982: „Der Dateityp des Drawing Interchange File Formats wurde von DIF in DXF geändert, damit nicht irgendein Gonzo versucht, eine Zeichnung in Visicalc zu laden.“\n\n\n\nDie letzte Version wurde 1985 als \"Paladin FlashCalc\" vermarktet.\n\n\n"}
{"id": "465669", "url": "https://de.wikipedia.org/wiki?curid=465669", "title": "Kachelgrafik", "text": "Kachelgrafik\n\nAls Kachelgrafik (auch kurz Mz. \"Kacheln\" genannt, engl. \"tiles\") wird eine (Computer-)Grafik bezeichnet, die mosaikartig zusammengesetzt ein vielfach größeres Gesamtbild ergibt.\n\nDie Kachel-Technik wird häufig in Computerspielen eingesetzt, da die Kacheln einfacher zu berechnen sind und weniger Arbeitsspeicher verbrauchen, was bis in die 1990er Jahre der Computertechnik – bis zum Aufkommen der 3D-Beschleuniger – ein heikles Thema war. Die Verwendung von Kacheln führt zu einer bestimmten, auch ästhetisch fassbaren, Computerspielform mit Kachel-basiertem Landschaftsaufbau (engl. \"tile-based design\"). Heute werden Kachel-basierte Landschaften eher aus Tradition, wegen der speziellen Optik oder für Flash-Spiele entwickelt, die die 3D-Funktionen der Grafikkarte nicht nutzen.\n\nKacheln fallen dadurch auf, dass sie nahtlos mit weiteren Kacheln kombiniert werden können. Aus vielen Kacheln entsteht so ein Gesamtbild. Die Zusammensetzung und Berechnung dieser Kacheln zu einem Gesamtbild ist ein internes Modul des Spiels, das man Tile-Engine nennt. Der Vorteil der additiven Anordnung ist eine große Speicherersparnis.\n\nNeben quadratischen, draufsichtigen Kachel-Designs wie bei vielen Jump-’n’-Run- und Shoot-’em-up-Spielen gibt es isometrische Kacheln, die einen dreidimensionalen Eindruck vermitteln. Bei ihnen weist die Spielfläche in der Regel – verglichen mit den Längen- und Breitenangaben – eine halbe Höhe auf, das ist hierbei jedoch nicht zwingend festgelegt. Hier übernimmt die – komplexeren – Berechnungen die Iso-Engine. Gewisse Operationen, wie die Prüfung des Z-Index, der die räumliche Tiefenanordnung und damit Überschneidung der Kacheln berechnet, werden z. B. bei ATI-Grafikkarten über eine Kachel berechnet. Es wird entweder die gesamte Kachel verworfen oder die gesamte Kachel dargestellt (Clipping). Traditionelle Kachelgrößen sind 32×32 und 64×64 Pixel.\n\nAuch bei isometrischen Spielen, etwa Simulationen und Rennspielen, ist der Kachelsatz rechteckig oder quadratisch. Dabei sind einzelne Bereiche der Kacheln transparent; durch die übereinandergelegte Anordnung ergibt sich der Eindruck der nahtlosen geneigten Fläche.\n\nDer \"Kachelsatz\" wird in einer Datei gespeichert, die die Anordnung mehrerer unterschiedlicher Kacheln zum letztendlichen Gesamtbild erfasst.\n\nAuch im Webdesign wird zur leichteren Handhabung oder zur Einsparung zu übertragender Datenmengen die Kacheltechnik verwendet. Eine gängige Anwendung ist die Hintergrundgrafik, bei der aus Gründen der Sparsamkeit oder auch aus optischen Gründen (mehr Gleichförmigkeit für weniger Ablenkungseffekt) durch das wiederholte nahtlose Aneinanderfügen einer einzelnen Kachel ein wiederholendes Muster erzeugt wird.\n\nEine andere Anwendung sind Geodaten als sehr große bzw. hochaufgelöste Rastergrafiken, die erwartungsgemäß nicht als Ganzes auf einmal benötigt werden. Diese werden mit Hilfe von Geodiensten in Kacheln zerteilt und die jeweils benötigten Teile des Gesamtbildes übertragen und angezeigt.\n\nWeb Map Tile Service ist ein Standard des Open Geospatial Consortium für Kartenkachel-Server.\n\nEinige Archive benutzen Dienste wie Zoomify und DFG-Viewer für die Bereitstellung von Digitalisaten auf ihrer Website.\n\nEinen Dienst zur Darstellung der gesamten Bilddatei bietet Dezoomify.\n\n"}
{"id": "465839", "url": "https://de.wikipedia.org/wiki?curid=465839", "title": "PC Praxis", "text": "PC Praxis\n\nDie PC Praxis (auch PC Pr@xis) war eine monatlich erscheinende Computerzeitschrift, die sich mit seinem inhaltlichen Themenkonzept – ähnlich der Computer Bild – an Computernutzer mit begrenztem Sachwissen richtete. Die Ressorts gliederten sich in die Bereiche \"Forum\", \"Windows-Praxis\", \"PC Praxis-DVD\", \"Praxis-Test\", \"Hardware-Test\", \"Foto/Video/Musik-Praxis\", \"Mobil-Praxis\" und \"Internet-Praxis\".\n\nSeit 11/1998 lag jedem Heft eine CD bei, seit Ausgabe 4/2006 eine DVD. Diese enthielten Test- und Vollversionen von Programmen, Free- und Shareware, Sicherheitsprogramme und Treiber. Seit Ausgabe 9/2005 waren auf den Heft-DVDs unregelmäßig sogenannte Power-Packs enthalten. Ein Power-Pack war einem Thema oder Artikel im Heft gewidmet und enthielt eine Vollversion und weitere kleinere Programme, die das Thema abdecken. So gab es beispielsweise Power-Packs zu Windows Vista, Videoschnitt und ähnlichen Themen.\n\nDie erste Ausgabe der \"PC Praxis\" erschien im Jahre 1987 als Sonderheft der Computerzeitschrift „DATA WELT“, die ab 1984 als Zeitschrift mit Produkttests und Tipps und Tricks zur Heimcomputerprogrammierung erschien und nach kurzer Zeit bereits eine sechsstellige Auflage erreichte. Nach dem Erscheinen des zweiten Sonderhefts als Januar/Februar-Ausgabe 1988 erschien sie zunächst ab März/April zweimonatlich, bevor sie mit der Einstellung der „DATA-WELT“ im September 1989 als monatliche PC-Zeitschrift erschien.\n\nDie \"PC Praxis\" erschien bis zur Auflösung des Verlages Data Becker in Düsseldorf, der neben Zeitschriften auch Bücher und Software herausgab, monatlich. In den letzten Jahren des Bestehens wurde die aktuelle Ausgabe immer schon zum Beginn des Vormonats verkauft. Die \"PC Praxis\" wurde Ende 2013 eingestellt, das letzte herausgegebene Heft war die Februar-Ausgabe 2014. Die verbliebenen Abonnenten erhielten nahtlos die \"c’t\" des Heise Zeitschriften Verlag.\n\n2007 rügte der Deutsche Presserat das Magazin wegen Verstoßes gegen den Pressekodex. Es hatte über illegale und „halb-legale“ Software berichtet; eine DVD mit „halb-legaler“ Software lag dem Heft bei. Der Presserat sah darin einen Verstoß gegen journalistische Grundsätze; Herausgeber und Journalisten müssten sich ihrer „Verantwortung gegenüber der Öffentlichkeit und ihrer Verpflichtung für das Ansehen der Presse bewusst sein“.\n\nIm vierten Quartal 2012 lag die durchschnittliche monatlich verbreitete Auflage nach IVW bei 65.545 Exemplaren. Das sind 13,16 Prozent (9.933 Hefte) weniger als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 1.594 auf 12.113 Abonnenten ab (−11,63 %). 18,48 Prozent der Leser bezogen die Zeitschrift im Abonnement.\n"}
{"id": "466155", "url": "https://de.wikipedia.org/wiki?curid=466155", "title": "PowerPC G3", "text": "PowerPC G3\n\nG3 ist die Bezeichnung für die Generation 3 der PowerPC-Prozessorfamilie. Sie wurde – wie schon die Vorgängerprozessoren der zweiten Generation (PowerPC 603 und 604) – gemeinsam von Apple, IBM und Motorola entwickelt.\n\nG3 ist ebenfalls die umgangssprachliche Bezeichnung für die \"Power Macintosh G3\"-Rechner von Apple.\n\nEine gegen Strahlung geschützte Variante dieses Prozessors (RAD750) dient als Hauptprozessor der Mars-Reconnaissance-Orbiter-Sonde, und war lange Zeit einer der leistungsfähigsten operierenden Prozessoren außerhalb der Erde oder ihrer Umlaufbahn. Die Raumsonde Gaia verfügt über 21 Stück der 750FX-Prozessoren. Die sieben Videoverarbeitungseinheiten der Sonde arbeiten mit jeweils drei Prozessoren gleichzeitig. Eine Logik vergleicht laufend die Ergebnisse der drei Prozessoren, erkennt einen fehlerhaft arbeitenden Prozessor und startet diesen in einer Millisekunde automatisch neu mit den Werten der anderen beiden Prozessoren, ohne dass der Rechenprozess insgesamt unterbrochen wird. Es handelt sich um den leistungsfähigsten Computer, der bis dahin im Weltraum betrieben wurde.\n\nDie G2-Prozessoren hatten den Nachteil, dass sie zu sehr an den frühen, teilweise noch experimentellen Prozessor PowerPC 601 anlehnten, welcher für eine andere Umgebung mit anderem Hintergrund und mit mangelnder Erfahrung entwickelt wurde. Die G3-Prozessoren jedoch wurden auf den Einsatz als CPUs für Arbeitsplatzrechner optimiert; also für die Power-Macintosh-Modellreihe. Außerdem war nun auch der Betrieb mit zwei Prozessoren gleichzeitig möglich, was allerdings Probleme mit dem L2-Cache verursachte und dementsprechende Leistungseinbußen bei den beiden einzelnen Prozessoren bedeutete.\n\n\nVideospielkonsolen mit PowerPC 750:\n"}
{"id": "466194", "url": "https://de.wikipedia.org/wiki?curid=466194", "title": "Scribus", "text": "Scribus\n\nScribus ist ein freies Desktop-Publishing-Programm für Linux-, Unix-, macOS-, OS/2- bzw. eCS-, Haiku- und Windows-Systeme, das unter der GNU General Public License lizenziert ist.\n\nScribus ist ein Seitenlayoutprogramm, das die flexible Gestaltung von Dokumenten mit freier Platzierung von Text und Bildern in Rahmen ermöglicht. Dabei können Musterseiten und Ebenen erstellt werden.\n\nFür die Textbearbeitung stehen zahlreiche Funktionen zur Formatierung zur Verfügung. Stilvorlagen können für Absatz- und Zeichenstile sowie für Linienstile verwendet werden. Text kann an einem Grundlinienraster ausgerichtet werden (Registerhaltigkeit). Darüber hinaus bietet das Programm einige mikrotypographische Funktionen wie optischen Randausgleich, Unterschneidung oder Grauwert-Optimierung mittels Glyphenskalierung.\n\nAndere ausgefeiltere Umbruchfunktionen wie beispielsweise vertikaler Keil (vertikales Austreiben von Text auf die volle Höhe des Rahmens durch Raumverteilung zwischen Absätzen oder Zeilen), automatisches Zusammenhalten von Absätzen oder die automatische Unterdrückung von Hurenkindern und Schusterjungen, die in den professionellen Programmen InDesign und QuarkXPress seit langem vorhanden sind, fehlten auch noch in der aktuellen stabilen Versionsstufe 1.4.x .\n\nMit Version 1.5.0 wurde vertikales Skalieren und die Unterdrückung von Hurenkindern und Schusterjungen implementiert.\n\nVorgaben für die Silbentrennung lassen sich mit Ausnahme der Spracheinstellung nur global für das ganze Dokument festlegen. Festausschlüsse (Leerzeichen mit fixierter Breite, z. B. Halbgeviert, Viertelgeviert usw.) werden aus Unicode-Zeichensätzen entnommen und sind folglich nur in Unicode-Schriften verfügbar, in denen die entsprechenden Zeichensatzpositionen belegt sind.\n\nDie in den marktführenden Standardanwendungen in den letzten Jahren geschaffenen Möglichkeiten zum Satz von Fußnoten und Marginalien waren ab Version 1.5.1 vorhanden. Die erweiterten Funktionalitäten von OpenType-Schriften wie automatische Substitution von Ligaturen und Mediävalziffern waren ab 1.5.3 nutzbar. Auch die Unterstützung von Complex Text Layout (nötig für Arabisch und Hebräisch) stand seit Version 1.5.3 zur Verfügung.\n\nFormatierter Text kann aus freien Formaten wie ODT oder HTML oder mit vom Benutzer frei definierbaren Filtercodes importiert werden (letztere unterstützen jedoch nur Absatzstilvorlagen, keine Zeichenstile). Der Export von Text ist nur unformatiert möglich.\n\nZu den Besonderheiten von Scribus gehören spezielle Rahmen (Renderrahmen), die den Einsatz externer Software zum Rendern von Grafiken innerhalb von Scribus erlauben; z. B. für Formelsatz mittels LaTeX oder Notensatz mit Hilfe von LilyPond. Gut unterstützt wird die Erstellung von PDF-Formularen.\n\nRastergrafiken können in allen gängigen Formaten, inklusive PSD, geladen werden. Vektorgrafiken können in den Formaten AI, CVG, EPS, FIG, ODG, SXD, PDF, PICT, PostScript, SHAPE, SML, SVG sowie WMF geöffnet oder importiert und bearbeitet werden.\n\nBeginnend mit Scribus 1.5 unterstützte Scribus den Import einiger, größtenteils proprietärer, Dokumentformate wie das von Apple Pages, VivaDesigner und Xara. In Version 1.5.1 kamen RTF und Docx von Microsoft Word hinzu.\n\nScribus kann Dokumente als PDF exportieren. Für die professionelle Druckausgabe unterstützt Scribus die Standards PDF/X-1a, PDF/X-3 und neu PDF/X-4 (ab 1.5.0), CMYK-Farbseparation, Schmuckfarben sowie Farbmanagement (mit LittleCMS). Darüber hinaus enthält das Programm mehr als 300 Farbpaletten kommerzieller Anbieter sowie staatliche bzw. offizielle Farbstandards. Dazu kommt der Export von Microsoft XPS (XPS) und dem standardisierten Open XML Paper (OXPS) ab Version 1.5.0. Beide Formate beherrscht Windows ab Version 8 zum Drucken und Anschauen wie auch der Acrobat Reader ab Version 9.\n\nMit Version 1.5.4 wurde erstmals der neue, anspruchsvolle ISO-Standard CxF3 für Farbpaletten von einem DTP-Programm unterstützt. Dazu wurden einige Limitierungen und Fehler im PDF-Export beseitigt. Neue Importfilter für ZonerDraw Vector und QuarkXPress 3 und 4 erweiterten die Möglichkeiten zum Wechsel nach Scribus. Dazu konnten nun alle bisherigen Filter Dateien mit LAB-Farbsystem laden.\n\nDer von freieFarbe vorgestellte HLC Colour Atlas für reale Druckfarben und Lacke basierend auf dem CIELAB-Farbsystem konnte in Scribus ab Version 1.5.4 benutzt werden.\n\nDas Scribus-Dokumentformat SLA beruht auf XML, sodass eine Transformation in andere Formate relativ einfach möglich ist. Scribus hat eine integrierte Scripting-Engine unter Verwendung von Python. Das Programm ist in 44 Sprachen bzw. Sprachvarianten verfügbar. Mit Version 1.5.3 sind ca. 500 Sprachen und Skripte in der Textbox verfügbar, so dass hier die Vielfalt der Sprachen schon sehr gut unterstützt wird. Dies wird sicherlich noch ausgebaut je mehr Sprachen OpenType zur Verfügung stellt.\n\n\n"}
{"id": "466517", "url": "https://de.wikipedia.org/wiki?curid=466517", "title": "MIRC", "text": "MIRC\n\nmIRC ist ein als Shareware vertriebener, weit verbreiteter IRC-Client für Windows.\n\nmIRC wurde vom jordanischen Softwareentwickler Khaled Mardam-Bey entwickelt und am 28. Februar 1995 zum ersten Mal in der Version 2.1a veröffentlicht. Es ist seitdem zu einem der beliebtesten IRC-Clients für Windows gewachsen. Seine Vielseitigkeit beruht hauptsächlich auf der integrierten Scriptsprache \"mIRC Script\", abgekürzt MSL (mIRC Scripting Language). Diese ist so umfangreich, dass mit ihrer Hilfe bereits MP3-Player, IRC-Spiele, HTTP-Server und -Clients sowie DCC-Dateiserver und IRC-Bots implementiert wurden. Um das Schreiben von Programmen in \"mIRC Script\" hat sich im IRC eine Gemeinschaft von Anhängern gebildet.\n\n\nDas \"m\" in mIRC steht nach Aussage des Entwicklers auf seiner persönlichen Website für „mu“, was dem japanischen Schriftzeichen für „kein Ding“ entspricht.\n\nmIRC ist wohl der am weitesten verbreitete IRC-Client unter Windows. Anfang 2004 hat das größte IRC-Netzwerk QuakeNet eine netzwerkweite Versionsabfrage der verbundenen Benutzer durchgeführt, welche für mIRC eine Verbreitung von 93,71 % nachwies.\n\n\nDie hohe Popularität bringt auch Nachteile mit sich, so grassieren diverse Viren und Trojanische Pferde, die sich über das IRC verbreiten können und fast ausschließlich gegen mIRC-Anwender gerichtet sind. Sie verführen insbesondere gutgläubige Benutzer dazu, codierte Schad- und Verbreitungsroutinen auszuführen. Ab Version 6.17 besteht jedoch die Möglichkeit, einige Bezeichner (u. a. $decode, $dll), welche oft zur Verbreitung von Schadcode genutzt werden, zu deaktivieren.\n\nEin weiterer Kritikpunkt ist die Implementierung sogenannter Color-Codes, der nicht dem IRC-Standard RFC 1459 konform ist. Sie eröffnen geringfügige Textformatierungsmöglichkeiten (Textfarbe, Hintergrundfarbe des Textes, Fettschrift, Unterstrichen) und werden von einigen Benutzern als störend empfunden. Von vielen aktuelleren IRC-Clients werden Color-Codes ignoriert. Klassische IRC-Clients haben, vor allem, wenn sie in einem Terminalfenster ausgeführt werden, jedoch bisweilen Schwierigkeiten mit der Darstellung. Insbesondere kann es hier zu Schriftdarstellung in Hintergrundfarbe oder zu blinkender Schrift kommen, welche wiederum von vielen als lästig empfunden wird. Viele IRC-Server implementieren diese Funktion dennoch.\n\nNach einer langen Entwicklungspause, in der praktisch nur Fehler korrigiert wurden, erschien im Februar 2006 mIRC 6.17, welches neben zahlreichen anderen Änderungen auch Unterstützung für UTF-8 mitbrachte. Im Juli 2006 ist mIRC 6.2 erschienen, die neben einer verbesserten Unterstützung für UTF-8 auch weitere Scripting-Möglichkeiten und mehr Möglichkeiten in der Gestaltung der Benutzerschnittstelle mit sich brachte. Die etwas angestaubt wirkende Oberfläche wurde mit mIRC 6.3 (August 2007) und 6.31 (November 2007) durch neue Symbole und ein neues, moderneres Logo angepasst.\n\nAm 30. Juli 2010 veröffentlichte Khaled Mardam-Bey mit mIRC 7.1 die erste vollständig Unicode-fähige Version von mIRC. Diese bislang größte Änderung in der Geschichte von mIRC – es wurden über 120.000 Codezeilen geändert – führte Unterstützung für Universal Plug and Play und das Protokoll IPv6 ein.\n\nDa mIRC nur die nötigsten Features zum Chatten besitzt, gibt es zahlreiche Erweiterungen, welche diverse Tools und zusätzliche Einstellungsmöglichkeiten bieten. Diese Scripts sind vor allem für Leute, die sich sehr oft im IRC aufhalten, oder für andere bestimmte Gruppen gedacht.\n\nDie bekannteste, noch aktiv entwickelte Erweiterung ist \"Gamers.IRC\" vom Gamers.Interactive-Team.\n\nmIRC bietet auch die Möglichkeit an, einen Dateiserver über das DCC-Protokoll zu betreiben. Für Benutzer, die einen solchen betreiben wollen, sind spezialisierte Scripts wie \"SysReset\" entwickelt worden, die deren Einrichtung erheblich vereinfachen.\n\n"}
{"id": "467570", "url": "https://de.wikipedia.org/wiki?curid=467570", "title": "Offscreen (Computer)", "text": "Offscreen (Computer)\n\nDer Begriff „Offscreen“ bzw. „Offscreen-Buffer“ bezeichnet einen Speicherbereich, in dem ein Bildschirminhalt aufgebaut wird, während man einen anderen Bildschirminhalt sieht.\n\nIst der Bildschirminhalt im Offscreen-Buffer fertig, wird dieser zum sichtbaren Bild und der vorherige Bildspeicher zum Offscreen-Buffer. Nur so kann man durch schnelles Umschalten zwischen den Speicherbereichen den Eindruck flüssiger Bewegungen erzeugen.\n\nVoraussetzung ist, dass der neue Bildschirminhalt in max. der Zeit berechnet werden kann, in der der fertige Bildschirminhalt zu sehen ist. Flüssige Bewegungen entstehen etwa ab 25–30 Bildern pro Sekunde.\n"}
{"id": "468517", "url": "https://de.wikipedia.org/wiki?curid=468517", "title": "VxD", "text": "VxD\n\nVxD bezeichnet einen virtuellen Gerätetreiber für das Betriebssystem Microsoft Windows. \"VxD\" steht für \"virtual x driver\", wobei das \"x\" in der Abkürzung ursprünglich für eine Hardwarekomponente stehen sollte, „d“ steht etwa für \"display\", also Anzeigegeräte. \"c\" hätte für \"communication\", \"p\" für \"printer\" und \"n\" für \"network\" gestanden. Somit wäre eine Datei mit der Endung \"VPD\" ein virtueller Druckertreiber. Da das \"x\" in der Abkürzung jedoch oft nicht durch einen anderen Buchstaben ersetzt wurde, findet man meist nur Dateien mit der Endung \".VXD\". Die Treiber besitzen die Dateiendung \".386\" unter Windows 3.x und \".vxd\" unter Windows 95. Hierbei ist zu beachten, dass die \".386\"- Dateien auch unter Windows 95 benutzt werden können, aber nicht umgekehrt. Dieser virtuelle Gerätetreiber steuert die Systemressourcen wie Hardwarekomponenten (Drucker, interne Controller, Steckkarten usw.) oder installierte Software (z. B das Antivirenprogramm \"AntiVir\"). So können mehrere Anwendungen auf dieselbe Ressource zugreifen. Mit der Einführung des Windows Driver Model mit Windows 98 wurden die VxDs überflüssig, jedoch konnten sie noch bis Windows ME eingesetzt werden. Windows NT und davon abstammende Systeme unterstützen keine VxD's mehr.\n\nDie VxDs sollten nicht mit den NTVDM-spezifischen \"VDD\" (Virtual Device Drivers) verwechselt werden.\n\n"}
{"id": "471402", "url": "https://de.wikipedia.org/wiki?curid=471402", "title": "Lasermaus", "text": "Lasermaus\n\nEine Lasermaus ist eine optische Computermaus, die zur Beleuchtung der Kontaktoberfläche eine Laserdiode mit kohärentem Licht verwendet. Dadurch unterscheidet sie sich von herkömmlichen optischen Mäusen, die eine meist im sichtbaren roten Bereich arbeitende Leuchtdiode (LED) als Lichtquelle verwendet.\n\nDas erste Modell einer Lasermaus, die \"Logitech MX 1000 Laser\", stellte Logitech im Herbst 2004 vor. Diese erreichte eine Bildverarbeitungsgeschwindigkeit von 5,8 Megapixeln/Sekunde bei einer Auflösung von 800 cpi (diese Werte entsprechen denen der Vorgängermäuse MX500 und MX700 mit herkömmlicher optischer Abtastung).\n\nLasermäuse wurden entwickelt, da optische Mäuse mit LED-Lichtquellen Schwierigkeiten bei der Ermittlung der Bewegungsrichtung und -geschwindigkeit auf unstrukturierten, transparenten oder spiegelnden Oberflächen haben. Lasermäuse kommen aufgrund des Speckle-Effektes, welcher bei kohärenten Licht besonders deutlich auftritt, auch mit glatten Oberflächen, wie etwa Metall oder poliertem Holz, zurecht. Der Speckle-Effekt sorgt für einen hohen Kontrast auch auf gering strukturierten Oberflächen.\n\nIn der Lasermaus wird als Laserdiode üblicherweise ein kostengünstiger Oberflächenemitter (VCSEL) eingesetzt, dessen Laserstrahl durch eine optische Linse im Gehäuse aufgeweitet wird und damit eine kleine Fläche von wenigen Millimeter Durchmesser unter der Maus beleuchtet wird. Unmittelbar neben der Laserdiode befindet sich im Gehäuse der optische Empfänger, welcher das von der Oberfläche reflektierte Licht über ein Linsensystem aufnimmt. Je nach Typ werden pro Sekunde einige 1000 Einzelbilder (Frames) detektiert, üblich sind Werte bis 7000 Bilder pro Sekunde, welche durch eine anschließende Signalverarbeitung in einem auf diese Anwendung optimierten digitalen Signalprozessor (DSP) verarbeitet werden. Der DSP berechnet aus der Folge der Einzelbilder und deren Veränderung die Bewegungen in beiden Achsen und übermittelt diese Information an den PC.\n\nDie Auflösung – diese wird in \"Counts per Inch\" (cpi) angegeben und entspricht der Anzahl an Bildpunkten, welche pro 2,54 cm erfasst werden können – ist je nach Typ verschieden und liegt zwischen 200 cpi bis 8000 cpi.\n\nDie meisten Lasermäuse sind mit Sensoren von Avago Technologies aufgebaut, einer ehemaligen Tochtergesellschaft von Hewlett-Packard, die auch viele Schlüsselpatente für optische Mäuse hält. Seit Kurzem bietet auch Cypress Semiconductor einen Chipsatz für Lasermäuse an, der eine andere Verfahrensweise der Bildauswertung verwendet und damit die AvagoTech-Patente umgeht.\n\nBei Lasermäusen wird aus Kosten- und Effizienzgründen eine infrarote Laserdiode eingesetzt. Die Laserstrahlung – üblich sind Wellenlängen um 848 nm – ist für das menschliche Auge nicht wahrnehmbar, was ein Sicherheitsrisiko darstellen kann. Um Augenschäden durch den Laser zu vermeiden – beim nicht sichtbaren infraroten Licht funktioniert der Lidschlussreflex als Schutzfunktion für das Auge nicht – werden nur Laser der Laser-Klasse 1 verwendet, das heißt, die Intensität der Laserstrahlung ist beim Betrachten so gering, dass keine Augenschäden entstehen. Zusätzlich sind in der Elektronik der Maus Vorkehrungen vorhanden, um bei technischen Defekten die Leistung der Laserdiode nach oben hin zu begrenzen und zwangsweise abzuschalten.\n"}
{"id": "472411", "url": "https://de.wikipedia.org/wiki?curid=472411", "title": "Twentieth Anniversary Macintosh", "text": "Twentieth Anniversary Macintosh\n\nDer Twentieth Anniversary Macintosh wurde als Sondermodell zum 20. Firmenjubiläum des kalifornischen Computer-Herstellers Apple im Jahre 1997 produziert. Er wurde ein ganzes Jahr lang in einer begrenzten Auflage hergestellt. Der Preis fiel innerhalb des Produktionszeitraumes von anfangs 10.000 US-Dollar bis zu seiner Einstellung im März 1998 auf 1.999 US-Dollar.\n\nDie Hauptplatine ähnelte dem des PowerMac 5500 und hatte mit dem PowerPC 603ev denselben Prozessor, der mit 250 MHz getaktet wurde.\n\nEr verfügte über einen eingebauten 12,1″-Aktivmatrix-Flüssigkristallbildschirm sowie ein von Bose entwickeltes Klangsystem mit integrierten Lautsprechern und einem separaten Tieftöner. Außerdem besaß er einen integrierten TV-Tuner sowie einen S-Video-Eingang und enthielt noch ein 33,6-kbps-GeoPort-Modem.\n\nZum Lieferumfang gehörte eine Kompakttastatur mit abnehmbarem Touchpad, aber keine Maus. Für den TV-Tuner, die Tonausgabe und die Steuerung von Audio-CDs lag eine Fernbedienung bei. Das Zubehör umfasste außerdem eine CD-Hülle aus Leder sowie ein Kugelschreiberset im Lederetui.\n\n"}
{"id": "472526", "url": "https://de.wikipedia.org/wiki?curid=472526", "title": "Up2date", "text": "Up2date\n\nup2date ist ein \"Update-Agent\", der sowohl über eine textbasierte Schnittstelle als auch über eine grafische Benutzeroberfläche verfügt. Up2date wird mit den Linux-Distributionen \"Red Hat Linux\" (RHL) und \"Fedora Core\" mitgeliefert und kann feststellen, welche Software-Pakete auf dem eigenen Computer oder Server aktualisiert werden müssen.\n\nUp2date kann ebenfalls neue Software-Pakete installieren und Abhängigkeiten dabei selbst auflösen, wenn sie in den angegeben Repositories verfügbar sind.\n\nZur Überprüfung der Echtheit der Software-Pakete wird GPG benutzt.\n\nWährend up2date bis \"Red Hat Linux\" 9 (RHL) und bei \"Red Hat Enterprise Linux\" (RHEL) Version 2 bis 4 auf das anmelde- und kostenpflichtige Red Hat Network zugreift, so benutzt up2date aus \"Fedora Core\" 1 bis 4 direkt die angegebenen Repositories, welche lokale Verzeichnisse, aber auch YUM- und APT-Quellen im Internet oder Intranet sein können.\n\nIn den Versionen 5 von \"Fedora\" und \"RHEL\" wurde up2date durch YUM und später dessen grafische Oberfläche PackageKit abgelöst. Die Entwicklung von up2date wurde eingestellt.\n\n"}
{"id": "472529", "url": "https://de.wikipedia.org/wiki?curid=472529", "title": "Red Hat Network", "text": "Red Hat Network\n\nDas Red Hat Network ist eine kostenpflichtige Systemmanagement-Plattform für Red-Hat-Linux- und Red-Hat-Enterprise-Linux-Systeme.\n\nEs bietet Systemadministratoren Werkzeuge, mit denen sie ihre Systeme in ihrem Netz effizient verwalten können. Ein Merkmal ist das modulare Design. Mit dem Wachstum von Netzwerken können Administratoren beliebig erweiterte Ressourcen für System-Updates, Verwaltung und Provisioning der gesamten Infrastruktur hinzufügen.\n\nDas Red Hat Network wird meistens zusammen mit up2date oder yum bei Red-Hat-Enterprise-Linux-Distributionen eingesetzt.\n"}
{"id": "472552", "url": "https://de.wikipedia.org/wiki?curid=472552", "title": "Red Hat Enterprise Linux", "text": "Red Hat Enterprise Linux\n\nRed Hat Enterprise Linux (RHEL) [] ist eine populäre Linux-Distribution, die von der Firma Red Hat hergestellt wird und auf den Unternehmensmarkt abgestimmt ist. Sie gilt in diesem Bereich als Marktführer unter den Linux-Distributionen und genießt eine große Unterstützung durch unabhängige Software-Hersteller.\n\nRHEL hat sich aus der ehemaligen Linux-Distribution Red Hat Linux (RHL) herausgebildet und wurde das erste Mal am 17. Mai 2002 veröffentlicht. Ziel war, eine speziell für Geschäftskunden ausgerichtete Distribution mit entsprechendem Support- und Schulungsangebot aufzubauen. Als Red Hat im September 2003 das Endkundenprodukt \"Red Hat Linux\" zugunsten des \"Fedora-Projekts\" aufgab, blieb RHEL als einzige von Red Hat vermarktete Linux-Distribution übrig. Red Hat nutzt die Communityarbeit des \"Fedora-Projekts\" (das es maßgeblich sponsert), um diese zu RHEL weiterzuentwickeln. RHEL 7 basiert auf einem Mix von Fedora 19 und Fedora 20.\n\nRHEL existiert in verschiedenen Varianten. Dazu gehören die Server-Varianten mit den Versionen \"Entry Server (ES)\" und \"Advanced Server (AS)\". Die \"Desktop-Familie\" enthält den \"Red Hat Desktop\" (RHD) und \"Red Hat Enterprise Linux WS\" (Workstation). Für diese Produkte ist Update-Support über das Red Hat Network möglich.\n\nRed Hat Enterprise Linux hebt sich durch eine Reihe von Besonderheiten von anderen Distributionen ab:\n\nRHEL ist ein Enterprise-Betriebssystem, also ein Betriebssystem, das auf die Bedürfnisse großer Unternehmen ausgerichtet ist. Als Enterprise-Betriebssystem ist es deshalb auf Stabilität und lange Wartungszyklen ausgelegt. Man kann RHEL-Versionen bis zu 13 Jahre lang nutzen, ohne Pakete bzw. Softwareversionen migrieren zu müssen, weshalb es für den kommerziellen Einsatz geeignet ist. Für RHEL bieten große Softwarehäuser wie Oracle oder SAP Zertifikate an, die garantieren, dass deren Software auf RHEL problemlos funktioniert, was analog für große Serverhersteller gilt. Enterprise-Betriebssysteme findet man daher meist auf Workstations und Servern, wo ein extrem stabiler Betrieb verlangt wird. (Z. B. in der Wissenschaft, Forschung, Börse, Militär oder Raumfahrt.)\n\nDer Lebenszyklus einer RHEL-Version beträgt zehn Jahre. In dieser Zeit wird die Verfügbarkeit von Updates und Patches von Red Hat garantiert. Die ersten drei RHEL-Versionen wiesen anfänglich eine siebenjährige Lebensdauer auf. Durch den Erwerb einer \"Extended Life Cycle Support\"-Lizenz (ELS) erhält man auch für RHEL 3 und 4 eine Versorgung mit Hotfixes für weitere drei Jahre. Für die RHEL-Versionen 5 bis 7 verlängerte Red Hat nachträglich die Lebenszeit ebenfalls um drei Jahre auf insgesamt 13 Jahre, im Rahmen der sogenannten \"Extended Life Phase\" (ELP).\n\nRedHat unterscheidet zwischen drei \"Production\"-Phasen: Während der ersten Phase werden noch neue Funktionen hinzugefügt und die Hardware-Unterstützung optimiert, falls sich diese im \"Fedora\"-Projekt bewährt haben. In der zweiten Phase werden nur noch neue Gerätetreiber hinzugefügt, wenn dafür keine grossen Änderungen notwendig sind. In der dritten Phase empfiehlt RedHat die Virtualisierung falls aktuelle Hardware genutzt werden soll. Während der drauf folgenden dreijährigen ELP wird nur noch ein stark eingeschränkter Support geboten.\n\nRed Hat Enterprise Linux wird mit einem grafischen Installer mit dem Namen \"Anaconda\" installiert, der auch für Einsteiger leicht bedienbar ist. Bei der Softwareverwaltung setzt Red Hat Enterprise Linux auf den Paketmanager RPM und die Software-Verwaltung yum. Anwendungen und System-Teile werden dabei online auf einem Repository-Server gesucht, von dort als RPM-Package heruntergeladen und installiert. Zur allgemeinen Konfiguration des Systems stehen die system-config-*-Programme zur Verfügung, die jeweils auch grafische Benutzeroberflächen haben. Die system-config-*-Werkzeuge sind nach den üblichen Red-Hat- und Fedora-Prinzipien programmiert – diese legen fest, dass „Management-Tools“ (Hilfsprogramme zur Systemverwaltung) jeweils nur gezielt eine einzige Aufgabe erfüllen und keine exklusive Kontrolle über Konfigurationsdateien benötigen. Administratoren eines Systems müssen trotz Verwaltungswerkzeugen in der Lage sein, beliebige Änderungen manuell in den Konfigurationsdateien vorzunehmen.\n\nRepositories anderer Anbieter verfolgen meist andere Ziele oder eine andere Lizenzpolitik als RHEL. Nennenswert sind hier \"Dag Wieers\", \"RPM Fusion\", \"RPMForge\" und \"atrpms\". Diese Quellen sind nicht immer zueinander kompatibel. Darüber hinaus stellen immer mehr Softwareprojekte und Firmen, wie das Mono-Projekt, das GStreamer-Projekt, Skype oder Adobe, eigene Repositories zur Verfügung.\n\nEPEL (Extra Packages for Enterprise Linux) ist ein vom \"Fedora-Projekt\" gepflegtes Repository, das portierte Pakete von Software bereitstellt, die in Fedora selbst, aber nicht in RHEL, CentOS oder Scientific Linux enthalten ist. Weil diese Enterprise-Distributionen auf der Basis von Fedora entwickelt werden, sind meist nur sehr kleine Anpassungen an den Paketen notwendig. EPEL erweitert somit die Enterprise-Distributionen um viele dort nicht enthaltene Anwendungen und Treiber. Da EPEL allein vom Einsatz der Community abhängt, geben \"Red Hat\" und das \"Fedora Projekt\" für EPEL-Pakete keine Garantien, Support oder Zertifizierungen, wie dies für Pakete im offiziellen RHEL-Repository üblich ist.\n\nRHEL integriert vollständig die Kernel-Erweiterung SELinux, um so Mandatory Access Control zu ermöglichen. Neben dem eigentlichen SELinux-Kern wird auch ein grafisches Programm mitgeliefert, mit dem die Aktivitäten von SELinux analysiert und weiter bearbeitet werden können.\n\nZiel der SELinux-Integration ist, dass insbesondere RHEL 5 den Zertifizierungen EAL4+ und Labeled Security Protection Profile nach dem Common Criteria Standard genügen soll.\n\nRed Hat bietet für die RHEL-Distribution eine Versicherung an, die gegen Klagen auf geistiges Eigentum an der Software schützt. Diese sind aufgrund von Softwarepatenten möglich. Zum Beispiel schützte die Versicherung vor möglichen Klagen der Firma SCO, die Teile des geistigen Eigentums von Linux beanspruchte (siehe auch SCO gegen Linux).\n\nAlle RHEL-Versionen wurden durch die Linux Foundation gemäß der Linux Standard Base (LSB) zertifiziert. Diese Zertifizierung stellt sicher, dass RHEL eine Binärkompatibilität zu anderen Linux-Distributionen aufweist, was die Software-Entwicklung und -Migration stark vereinfacht. Während sich ältere RHEL-Versionen nach älteren LSB-Standards richten, ist RHEL 5.6 und 6.0 nach LSB 4.0 zertifiziert.\n\nRed Hat Enterprise Linux wird ab der Version 5.x in zwei Varianten (Server oder Workstation) angeboten.\n\nWorkstation und Server:\n\nWeitere unterstützte Architekturen (Server-Version):\n\nWorkstation und Server:\n\nWeitere unterstützte Architekturen (Server-Version):\n\nWorkstation und Server:\n\nWeitere unterstützte Architekturen (Server-Version):\n\nRed Hat Enterprise Linux 3.x und 4.x wurden in drei Varianten entwickelt: Advanced Server (AS, max. 16 CPUs, bis 64 GB Ram), Enterprise-Server (ES, 2 CPUs, 8 GB Ram) oder Workstation (WS, 2 CPUs, 64 GB Ram). Es werden folgende Architekturen unterstützt:\n\nAS, ES, WS:\n\nAS, WS:\n\nAS:\n\nRed Hat Enterprise Linux 2.x gab es in folgenden Versionen:\n\n\nObwohl Red Hat die Quellpakete der RHEL-Distributionen frei im Internet bereitstellt, gibt es direkt von Red Hat keine frei verfügbaren RHEL-Bootmedien oder -Images, RHEL kann, wie z. B. auch das (nicht RHEL-basierte) Konkurrenzprodukt SUSE Linux Enterprise Server, nur im Zusammenhang mit Supportverträgen erworben werden. Um dennoch ein frei verfügbares, RHEL-kompatibles Linux anbieten zu können, sind Projekte wie CentOS oder Scientific Linux entstanden. Da alle Quellpakete für die RHEL-Distributionen im Internet bereitstehen, können diese Projekte alle frei verfügbaren Pakete kompilieren und auf eigenen Boot-Images und Installationsmedien anbieten. Das Ziel ist dabei, meist mit nur geringfügigen Änderungen, eine Distribution zu erzeugen, die vollständig zu RHEL kompatibel ist und ausschließlich aus frei verfügbarer Software besteht. In der Regel müssen daher aus Lizenzgründen einzelne Pakete (z. B. Adobe Reader, Flash Player) aus der Distribution entfernt werden. Umgekehrt können zusätzliche Pakete hinzugefügt sein, die ebenfalls frei sind und keine Verpflichtung zum Kauf von Lizenzen oder Supportverträgen mit sich bringen.\n\nBeispiele für Distributionen, die auf RHEL basieren, sind:\n\nRHEL wird neben Scientific Linux und Debian auf der Internationalen Raumstation ISS eingesetzt, wie die NASA bekannt gab.\n\nRHEL ist beim Verteidigungsministerium der USA als Standardplattform für serverbasierte Anwendungen, Webdienste, Datenbanken, Netzwerksicherheit und Ähnliches ausgewählt worden. RHEL wird in der US-Armee an zahlreichen Stellen eingesetzt. Das Verteidigungsministerium wurde 2005 zum größten Kunden der Firma Red Hat.\n\nDie nationale Luftfahrtbehörde (FAA) der USA migrierte im Frühling 2006 vollständig zu RHEL. Dadurch konnten öffentliche Mittel in der Höhe von 15 Millionen US-Dollar eingespart werden. Die FAA verwendete zuvor nicht Windows, sondern eine sehr teure proprietäre UNIX-Plattform. Mit RHEL ließen sich Probleme mit der Skalierbarkeit lösen, die Effizienz steigern und die hohe geforderte Verfügbarkeit der Systeme sicherstellen. Die Sicherheit der bis zu 8000 Flugzeuge, die gleichzeitig im von der FAA überwachten Luftraum unterwegs sein können, hängt wesentlich von der Verfügbarkeit der IT-Infrastruktur der FAA ab, die Behörde kann sich keine Fehler oder Ausfälle erlauben.\n\nAnmerkungen\n\nIn regelmäßigen Abständen entwickelt das Unternehmen Red Hat, mit meist nur geringfügigen Änderungen, aus einer Fedora-Version das Produkt Red Hat Enterprise Linux, (RHEL) dessen Versionen im Gegensatz zu Fedora sehr lange gepflegt werden:\n\n"}
{"id": "473064", "url": "https://de.wikipedia.org/wiki?curid=473064", "title": "Winny", "text": "Winny\n\nWinny (alias WinNY) ist ein japanisches Peer-to-Peer-Filesharingprogramm, das behauptet, durch die Design-Prinzipien hinter dem Freenet-Netz inspiriert worden zu sein und Benutzeridentitäten anonym zu halten. Während Freenet in Java implementiert wurde, wurde Winny als C++-Anwendung für Windows implementiert.\n\nDer Name der Software leitet sich von WinMX ab, indem M und X um einen Buchstaben im lateinischen Alphabet zu N und Y angehoben werden. Im September 2003 gab es 250.000 Benutzer des Programms entsprechend der \"Association of Copyright for Computer Software\" in Tokyo. Laut \"P2Pnet\" ist es das populärste Tauschprogramm in Japan, mit WinMX auf dem zweiten Platz.\n\nDie Software wurde von Isamu Kaneko entwickelt, welcher Forschungsassistent in Masterstudiengängen über Technische Informatik an der Universität Tokio in Japan war. Er war auch Forscher am Japan Atomic Energy Research Institute. Kaneko hatte ursprünglich seine Absicht, Winny zu entwickeln, auf dem „Download Software“-Board des populären japanischen Forums 2channel bekannt gegeben. Da 2channel-Nutzer oft auf anonyme Nutzer durch deren Nummer ihres Postings verweisen, wurde Kaneko als \"47-shi\" () bekannt, oder einfach nur als „47“.\n\nAm 28. November 2003 wurden zwei Nutzer von Winny, Yoshihiro Inoue, ein 41-jähriger selbstständiger Geschäftsmann aus Takasaki, Präfektur Gunma, und ein 19 Jahre alter arbeitsloser Mann aus Matsuyama, von der Kyotoer Polizei verhaftet. Sie wurden beschuldigt, urheberrechtlich geschütztes Material via Winny verteilt zu haben und gestanden ihre Taten. Bald nach den Verhaftungen dieser beiden Nutzer wurde die Wohnung von Kaneko durchsucht und der Winny-Quellcode von der Kyotoer Polizei beschlagnahmt. Kaneko wurde unter dem „Verdacht auf Verschwörung mit dem Ziel, Copyright-Verletzungen zu begehen“ verhaftet.\n\nKanekos Verhaftung löste einen Aufruhr in Internet-Communitys einschließlich 2channel aus, die Verhaftung wurde als ungerechtfertigt bezeichnet. Eine Website, die extra dafür geschrieben wurde Geld für seine Verteidigung zu sammeln, hat Spendeneingänge von über 11 Millionen Yen (um die 78.500 Euro) in nur zwei Wochen verzeichnen können.\n\nKaneko wurde am 1. Juni 2004 gegen Kaution aus der Haft entlassen. Die Gerichtsanhörung startete im September 2004 am Kyotoer Bezirksgericht.\n\nAm 13. Dezember 2006 wurde Kaneko wegen „Beihilfe zu Copyright-Verletzungen“ verurteilt und zu einer Geldstrafe von 1,5 Mio. Yen verurteilt. Das Urteil wurde in den japanischen Tageszeitungen heftig diskutiert. Im Oktober 2009 wurde Kaneko in zweiter Instanz freigesprochen.\n\nIsamu Kaneko verstarb am 6. Juli 2013 in Folge eines Herzinfarkts. Er wurde 42 Jahre alt.\n\nNachdem Winnys Entwicklung gestoppt wurde, hat ein anonymer japanischer Entwickler Share entwickelt, um dort weiterzumachen, wo Winny aufgehört hat.\n\nAls damals die beiden Nutzer von Winny verhaftet wurden, hat die Kyotoer Polizei zwar behauptet „Winnys Fähigkeiten zum Anonymisieren analysiert“ zu haben, um die Nutzer aufzuspüren, aber niemals die genaue Methode hierzu preisgegeben. Später, als die Details über die genutzten Methoden an Kanekos erstem Tag der Gerichtsverhandlung genannt wurden, sollte sich herausstellen, dass diese Aussage nicht ganz zutreffend war. Es wurde von der Polizei gezielt in Features des Programms nach Nutzern gesucht, in denen Winny keinerlei Anonymität bietet.\n\nNachdem sie nicht in der Lage waren, Winnys verschlüsselte Kommunikation in dessen File-Sharing-Feature zu knacken, ging die Kyotoer Polizei zu einer anderen Methode über, nämlich Nutzer über Winnys integriertes Forum aufzuspüren. Im Gegensatz zum Filesharing-Feature bot das Forum nur Anonymität für die Besucher von Nachrichten-Threads an, nicht aber für die Ersteller eines Threads. Nutzer, die einen Thread aufriefen, konnten so leicht die IP-Adresse des Threaderstellers herausfinden.\n\nDie Kyotoer Polizei hatte zuerst nach Threads gesucht, deren Ersteller die Dateinamen des urheberrechtlich geschützten Materials angaben und zeichnete deren IP-Adresse auf. Dann hatten sie ihre Firewall dahingehend konfiguriert, dass nur noch Verbindungen von der IP-Adresse dieser Nutzer erlaubt wurden. Schlussendlich bestätigten sie dann, dass sie in der Tat jene Dateien herunterladen konnten, welche die Threadersteller gepostet hatten, um sie zu verteilen.\n\nKritiker von Kaneko behaupten, der Haupteinsatzzweck von Winny sei, das Urheberrecht zu verletzen, im Gegensatz zu Freenet, mit dem Winny oft verglichen wird und das angibt, die freie Meinungsäußerung schützen zu wollen. Diese Kritiker behaupten folgleich, dass 2chs Download Software Board, in dem die Software das erste Mal angekündigt wurde, ein Paradies für Urheberrechtsverletzer sei und zitieren aus mehreren Mitteilungen Kanekos, wonach Winny das Ziel verfolge, eine Welt voller Urheberrechtsverletzungen aufzubauen.\n\nIn einer Mitteilung auf dem 2ch Download Software Board hat „47“ darauf hingewiesen, dass  beta 8.1 <nowiki>[</nowiki>von Winny<nowiki>]</nowiki> eine Sicherheitslücke hat und keine Anonymität bietet. „Tauscht keine illegalen Dateien <nowiki>[</nowiki>damit<nowiki>]</nowiki> aus.“ Kritiker sehen dies als Beweis für Kanekos böswillige Absicht, da „47“ Nutzer angewiesen hat, kein urheberrechtlich geschütztes Material mit der Beta 8.1 auszutauschen, \"weil\" sie keine Anonymität biete und Urheberrechtsverletzer aufgespürt werden könnten.\n\nAndere wiederum bezeichnen Kanekos Handlungen nicht als Straftat, da er selbst keine Urheberrechtsverletzungen begangen, sondern nur eine Software produziert habe, die für diesen Zweck genutzt werden könne. Außerdem seien die Behauptungen von Kanekos Kritikern nicht beweisbar (manche bezeichnen sie sogar als schlichtweg falsch), da seine notierten Aussagen viel zu vage seien, als dass ihm illegale Absichten nachgewiesen werden könnten. Gemäß der Website „Befreit Kaneko“ soll er davor gewarnt haben, urheberrechtlich geschütztes Material mit Hilfe der Software auszutauschen.\n\nWinny wurde von westlichen Nutzern nahezu überrannt. Das Netzwerk ist gespickt mit sog. Fakes (z. B. Pornofilm als Linux-ISO getarnt) sowie mit von Viren und Trojanern verseuchter Dateien. In der Szene heißt es, dass die „Gaijins“ (Japanisch für 'Ausländer') das Winny-Netzwerk zerstört hätten. Trotz der Fakes und verseuchter Dateien ist Winny, vor allem in der westlichen Japan-(Fan-)Szene, immer noch beliebt.\n\nEs wird befürchtet, dass Share, den Nachfolger Winnys, das gleiche Schicksal ereilen wird, falls auch hier die Zahl westlicher Nutzer zunimmt.\n\nAls Nachfolgeprogramm ist Perfect Dark (P2P) entwickelt worden.\n\n\n\n"}
{"id": "474951", "url": "https://de.wikipedia.org/wiki?curid=474951", "title": "CURL", "text": "CURL\n\ncURL (ausgeschrieben Client for URLs oder Curl URL Request Library) ist eine Programmbibliothek und ein Kommandozeilen-Programm zum Übertragen von Dateien in Rechnernetzen. cURL steht unter der offenen MIT-Lizenz und ist u. a. auf folgende Betriebssysteme portiert worden: Solaris, NetBSD, FreeBSD, OpenBSD, Darwin und macOS, HPUX, IRIX, AIX, Tru64, Linux, UnixWare, HURD, Windows, AmigaOS, OS/2, BeOS, Ultrix, QNX, OpenVMS, RISC OS, Novell Netware und DOS. Die zugehörige Programmbibliothek \"libcurl\" wird von zahlreichen Programmen und Programmiersprachen verwendet.\n\nDaniel Stenberg, der Programmierer von cURL, begann 1997 ein Programm zu schreiben, das IRC-Teilnehmern Daten über Wechselkurse zur Verfügung stellen sollte, welche von Webseiten abgerufen werden mussten. Er setzte dabei auf das vorhandene Open-Source-Tool \"httpget\". Nach einer Erweiterung um andere Protokolle wurde das Programm am 20. März 1998 als \"cURL 4\" erstmals veröffentlicht. Ursprünglich stand der Name für \"see URL\" und wurde erst später von Stenberg nach einem besseren Vorschlag zum aktuellen Backronym umgedeutet.\n\nWie der ausgeschriebene Name „Client for URLs“ andeutet, ist es ein Kommandozeilen-Werkzeug zum Herunter- oder Hochladen von Dateien über eine Internetadresse, auch POST-Übertragungen sind möglich. Zu den unterstützten Protokollen gehören u. a. HTTP, HTTPS, FTP, FTPS, DICT, LDAP, RTMP und Gopher.\n\nIm Gegensatz zum älteren wget aus dem Jahre 1995 kann cURL Dateien nicht nur herunter-, sondern auch hochladen.\n\ncURL ist schon länger fest in Linux Systemen implementiert, seit dem April-2018-Update (1803) wurde cURL als Bordmittel in Windows 10 aufgenommen und ist ohne Zusatzsoftware über die Kommandozeile aufrufbar.\n\n"}
{"id": "474989", "url": "https://de.wikipedia.org/wiki?curid=474989", "title": "Kile (Software)", "text": "Kile (Software)\n\nKile ist eine Entwicklungsumgebung für LaTeX. Es ist ein KDE-Programm, das unter der GNU General Public License verfügbar ist.\n\nKile ist sehr ähnlich wie Kate und bietet fast alle Funktionen desselben und darüber hinaus noch die TeX-spezifischen Erweiterungen. Kile erleichtert das Bearbeiten von mehreren Dokumenten, z. B. Hauptdokument und BibTeX-Datenbank oder mehrere Teile eines LaTeX-Dokuments, nebeneinander mithilfe von Tabs und die Ansicht der zum Projekt gehörigen Dateien. Kile besitzt eigene Schaltflächen zum Kompilieren der Dokumente, wobei eventuell vorhandene BibTeX-Datenbanken ebenfalls mitkompiliert werden – es braucht nur ein Klick ausgeführt zu werden, um das gesamte Projekt zu kompilieren. Die LaTeX-IDE unterstützt weiterhin eine Syntaxhervorhebung von LaTeX/BibTeX/TeX sowie allen Sprachen, die auch von Kate unterstützt werden, darunter so exotische wie TI-Basic oder Wesnoth Markup Language. Außerdem vervollständigt Kile LaTeX-Befehle und bietet eine grafische Auswahl von Sonderzeichen (auch aus AMS-Paketen) zum Einfügen als LaTeX-Befehl in den Quelltext an. Eine Rechtschreibprüfung kann konfiguriert werden. Kile bietet auch Hilfen für Anfänger und unerfahrene Nutzer, indem es die grafische Erstellung von Tabellen und anderen Strukturen ermöglicht. Schließlich bietet Kile auch Code-Faltung und das grafische Anzeigen der LaTeX-Struktur des aktuellen Dokuments.\n\n\n"}
{"id": "474999", "url": "https://de.wikipedia.org/wiki?curid=474999", "title": "Kate (KDE)", "text": "Kate (KDE)\n\nKate (Akronym für KDE Advanced Text Editor) ist ein freier Texteditor von KDE.\n\nKate ist vorwiegend für Programmierer und Systemadministratoren, aber auch für erfahrenere Nutzer geeignet. In das Programm sind Syntaxhervorhebung und Code-Faltung für insgesamt über 200 Programmiersprachen wie zum Beispiel C oder C++ und für Auszeichnungssprachen wie HTML oder XML integriert. Es besteht die Möglichkeit, viele Dateien gleichzeitig zu öffnen sowie Projekte zu erstellen und zu verwalten. Außerdem beinhaltet Kate eine eingebaute Konsole, unterstützt eine Vielzahl von Plug-ins und besitzt einen vi-Eingabemodus. Unterstützte Kodierungen sind unter anderem ASCII, ISO 8859-1, UTF-8 und UTF-16.\n\nDie Editorkomponente von Kate, genannt KatePart, ist als ein KPart angelegt, womit es möglich ist, den Editor in eine beliebige andere KDE-Anwendung einzubetten. Kate selbst ist somit eine Anwendung, die die Editorkomponente KatePart um weitere nützliche Funktionen wie zum Beispiel das Suchen und Ersetzen in mehreren Dateien oder die Möglichkeit, mehrere Dateien auf einmal zu öffnen, erweitert. Die integrierte Entwicklungsumgebung KDevelop sowie das Webentwicklungsprogramm Quanta Plus sind zwei bekannte Anwendungen, die ebenfalls KatePart als Editorkomponente verwenden. Auch viele weitere KDE-Anwendungen nutzen KatePart, so zum Beispiel das Archivierungsprogramm Ark zum Anzeigen von Vorschauen für Textdateien. KWrite basiert ebenfalls auf KatePart.\n\nFür Kate steht ein Modus zur Verfügung, der das Bearbeiten von Texten in der Wikisyntax ermöglicht. Mit Hilfe einer XML-Datei werden HTML-Tags und -Entitäten, Wiki-Steuerzeichen, Links, Überschriften, Tabellen sowie <nowiki>-Abschnitte unterschieden.\n\n"}
{"id": "475000", "url": "https://de.wikipedia.org/wiki?curid=475000", "title": "KWrite", "text": "KWrite\n\nKWrite ist ein Texteditor, der von KDE entwickelt wird und als Teil der Software Compilation veröffentlicht wird. Seit der K Desktop Environment 2 basiert er auf dem Text-Editor KatePart. Er ist als Programmiereditor gedacht und kann zusammen zur farbigen Darstellung von Quelltexten verschiedener Programmiersprachen wie Perl, C++, Java und HTML benutzt werden (Syntaxhervorhebung).\n\nFür das gleichzeitige Bearbeiten mehrerer Dateien, Projektverwaltung und für weitergehende Funktionen steht Kate zur Verfügung.\n\n"}
{"id": "475559", "url": "https://de.wikipedia.org/wiki?curid=475559", "title": "KSnapshot", "text": "KSnapshot\n\nKSnapshot ist das Bildschirmfoto-Programm von KDE. Das Programm ist einfach zu bedienen und kann sowohl einzelne Fenster, wie auch einen ganzen Bildschirm, sofort oder mit einstellbarer Verzögerung aufnehmen. Ebenso ist es möglich, die Aufnahme mit der Maus auf einen Bereich eines Bildschirms oder eines sichtbaren Fensters einzugrenzen. Die Bilder können dann entweder gespeichert, in die Zwischenablage kopiert oder (als automatisch erzeugte temporäre Datei) einem auswählbaren Programm zur Weiterverarbeitung übergeben werden.\n\nIm Zuge der Veröffentlichung der KDE Applications 15.12 wurde KSnapshot durch ein neues Programm namens Spectacle ersetzt.\n\n"}
{"id": "475561", "url": "https://de.wikipedia.org/wiki?curid=475561", "title": "KPaint", "text": "KPaint\n\nKPaint ist ein einfaches Zeichenprogramm für die Grafische Benutzeroberfläche KDE. Grundlage des Programmes ist das Canvas Widget von Tcl/Tk. Es besitzt einfache Zeichenwerkzeuge, ermöglicht aber keine komplizierten Manipulationen von Fotos. Neben BMP, GIF, JPG, PNG kann KPaint auch andere Formate lesen. Bestimmte Operationen wie Skalierung, Rotation, Verschiebung, Spiegelung und Deformierung im Sinne einer Neigung können auf mehrere Objekte einer Gruppe angewandt werden. Es ist als das Pendant zu Paintbrush bei Windows anzusehen.\nMit der KDE Version 3.3, die im August 2004 herauskam, wurde KPaint vom neuen Programm KolourPaint abgelöst.\n\n"}
{"id": "475563", "url": "https://de.wikipedia.org/wiki?curid=475563", "title": "DigiKam", "text": "DigiKam\n\ndigiKam ist ein freies Bilderverwaltungs-Programm für Linux, Windows und macOS. Mit Hilfe der Software lassen sich Bilder archivieren, sortieren, bearbeiten und publizieren.\n\ndigiKam wurde zu Beginn auf Basis der KDE Plattform 4 entwickelt. Im Zuge der Umstellung des Quellcodes auf die Qt-5-Bibliotheken löste sich die Anwendung mit der Veröffentlichung von Version 5.0.0 gleichzeitig von vielen KDE-Abhängigkeiten, was eine Portierung auf andere Plattformen erleichtern soll. Für die Metadaten des Archivs verwendet die Software eine Datenbank, mit der es möglich ist, über die Exif-Daten der Bilder, über Stichwörter oder über Bewertungen das gesuchte Bild zu finden. Die Bilder lassen sich dabei anhand selbstgewählter Stichwörter kategorisieren oder in Alben zusammenstellen. Angeboten wird auch eine Ähnlichkeitssuche mit einem Vergleichsbild oder einer Skizze als Vorlage. Eine Funktion zur Gesichtserkennung, mit der Personen automatisch einander zugeordnet werden können, ist ab der Version 2.0 enthalten. Außerdem ist auch ein Wikimedia-Commons-Uploader zum Hochladen von Bildern nach Wikimedia Commons enthalten.\n\nNeben den integrierten Digikam-spezifischen imagePlugins kann das Programm auf die KIPI-Schnittstelle zugreifen, mit denen Bilder einzeln oder im Stapelverarbeitungsmodus behandelt werden können. Eine Hauptfunktion bildet die grafische Schnittstelle zu gPhoto2, mit der digiKam über 1000 Digitalkameramodelle ansprechen kann. Auch Scanner können über SANE genutzt werden.\n\nDie Software unterstützt alle gängigen Rastergrafikformate wie JFIF, JPEG 2000, TIFF, PNG, PGF und über DCRaw auch RAW-Daten und das digitale Negativformat DNG. Standardmäßig erstellt digiKam eine SQLite-Datenbank, kann jedoch ab Version 1.3.0 auch mit lokalen und per Netzwerk angebundenen MySQL-Datenbanken umgehen. Dienstprogramme für die Erstellung von Fotokalendern, HTML-Galerien und eine umfangreiche Stapelverarbeitung für Dateiformate, Bildgröße, Rahmen, Weißabgleich usw. runden das Paket ab. OpenStreetMap wird ab Version 1.0 ebenfalls unterstützt. Gesichtserkennung wird mit Hilfe der Programmbibliothek OpenCV realisiert.\n\nIm Programmpaket von digiKam ist ebenfalls das Bild-Anzeige- und Bearbeitungsprogramm \"ShowFoto\" enthalten. ShowFoto kann wie auch digiKam die Plugin-Struktur KIPI nutzen und stellt unter anderem folgende Bildbearbeitungsfunktionen zur Verfügung:\n\n"}
{"id": "475567", "url": "https://de.wikipedia.org/wiki?curid=475567", "title": "Gwenview", "text": "Gwenview\n\nGwenview ist ein Programm zum Betrachten und zur Verwaltung von Fotos und Bildern für das K Desktop Environment. Es wurde für KDE 3 entwickelt und ist als KDE-Anwendungsprogramm erstmals in KDE SC 4 mit enthalten. Es wird als freie Software unter der GNU General Public License (GPL) verbreitet.\n\nEs unterstützt eine Vielzahl von Bildformaten, bietet Albenfunktionen und Unterstützung für diverse Standards, wie zum Beispiel Exif. Mit Hilfe der KIPI-Plugins können Bilder weiterverarbeitet und unter anderem exportiert werden. Des Weiteren kann es innerhalb des Konqueror als Bild-Anzeige arbeiten, und so erweiterte Betrachtungsmöglichkeiten einbringen.\n\n"}
{"id": "475817", "url": "https://de.wikipedia.org/wiki?curid=475817", "title": "KStars", "text": "KStars\n\nKStars ist ein freies Astronomieprogramm auf Basis der KDE-Plattform 4. Man kann damit für jeden Ort und jedes Datum den Nachthimmel vom Schreibtisch aus erkunden. Es zeichnet die Positionen von Sternen, Sternbildern, Sternhaufen, Nebel, Galaxien und Planeten auf den Bildschirm und beinhaltet über 100 Millionen Sterne, 13.000 Deep-Sky-Objekte, unser komplettes Sonnensystem sowie Tausende von Kometen und Asteroiden. Dabei kann die Anzeige verschoben und vergrößert oder verkleinert werden, man kann auch Objekte identifizieren und verfolgen, wie sie über den Himmel ziehen.\n\nKStars erlaubt umfangreiche Anpassungen. Man kann einstellen, welche Objekte mit welchen Farben angezeigt werden sollen und Bilder von jedem Teil des Himmels können aus Online-Datenbanken heruntergeladen werden. Man kann mit KStars geeignete Teleskope steuern und Aufnahmen mittels CCD-Kameras machen. Es werden die Bedürfnisse der modernen Amateurastronomie erfüllt.\n\nNeben der Version für Desktop-Computer gibt es zur Nutzung auf Smartphones und Tablets eine \"Lite-Version\" für Android.\n\n"}
{"id": "476228", "url": "https://de.wikipedia.org/wiki?curid=476228", "title": "SICOMP", "text": "SICOMP\n\nSICOMP ist eine Bezeichnung der Siemens AG für eine von ihr entwickelte Computerfamilie. Sie unterteilt sich in verschiedene \"SICOMP\"-Systeme. Siemens hat zum 31. März 2008 die komplette Hardware-Produktpalette des Systems SICOMP zum Auslauf erklärt und verweist gleichzeitig auf die Ablöseprodukte Microbox PC, SIMATIC PC und SIMATIC WinAC.\n\nAuf dieser Computerfamilie baut \"Teleperm M\", ein weltweit eingesetztes System zur Prozessautomatisierung, auf. Der erste Einsatz erfolgte ca. 1985 auf der Hardwarebasis eines SICOMP-R-Rechners.\n\nFür die SICOMP-R- und SICOMP-M-Rechner gibt es inzwischen eine Portierung auf eine PC-basierende Plattform unter dem SICOMP Emulator M2000.\n\nDie Entwicklung des Bedien- und Beobachtungssystems dafür ist:\n\n\nAls Betriebssysteme wurden AMBOSS 4 (BS4) und BS-M eingesetzt.\n\n\"SICONFEX\" ist ein Expertensystem zur Konfigurierung von Betriebssystemen für SICOMP-Prozessrechner.\nEs wurde 1985 von Lehmann im Auftrag von Siemens München entwickelt.\n\nSICOMP SMP16 (Siemens Microcomputer Platinensystem) und SICOMP AMS (Advanced Microcomputer System) sind modulare Baugruppensysteme im Einfach- bzw. Doppel-Europaformat mit offener Systemarchitektur. Diese Systeme boten eine Basis für Echtzeitanwendungen mit Standard-Baugruppen mit hoher Rechenleistung (es war auch 2-CPU-Betrieb möglich) und Erweiterbarkeit. Diese Industrie-Microcomputer wurden als PC-Systemplattform für Automatisierungsaufgaben genutzt. Sie waren modular in der Industrie üblichen 19\"-Technik aufgebaut. Sie hatten schnelle zentrale Prozess-Interfaces, z.B. zur Messwerterfassung und Feldbusanschlüsse. \nTypische Einsatzbereiche waren Daten- und rechenintensive Aufgaben in der Automatisierungstechnik, zeitkritische Regelungs- und Steuerungsaufgaben, Bedarf von mehreren technologischen Funktionen in einem System.\n\nTechnische Eckdaten: Betriebssysteme RMOS und Microsoft, Bussysteme Compact PCI, Baugruppen im Europaformat 3U, Einbaurahmen nach ES902C, Betriebstemperaturbereich 0 bis 55 °C. Multibus-I-Spezifikation nach IEC 796.\n\nDiese Kompaktrechner wurden als fertige Bausteine für Maschinen und Anlagen gebaut. In einem Gerät wurden Steuerung und Regelung so wie Bedienen über integrierte Tastatur und Beobachten über ein beleuchtetes LC-Display und LEDs untergebracht. Ausgestattet waren die Geräte mit einem x86-Prozessorkern mit Speicherarchitektur, digitalen und analogen I/O, seriellen Schnittstellen, Gebereingängen und Feldbusanschaltung. Als Echtzeit-Betriebssystem wurde standardmäßig RMOS von Siemens eingesetzt. Programmierbar waren diese Systeme unter anderem mit STEP 5.\n\nIn den 1980er Jahren wurden die Siemens Sicomp-PC auch als Einzelplatz-Arbeitscomputer verkauft, speziell an Weiterbildungseinrichtungen.\nDer x86-Prozessor war Standard bei allen RMOS und DOS Computern.\nMicrosoft hat diese Technik danach völlig vom Markt verdrängt mit MS-DOS und den ersten grafikorientierten Benutzeroberflächen wie WINDOWS /286.\n\nEin Bürorechner, welcher optional mit Btx-Anschluss angeboten wird.\n\nEin Bürorechner mit einem 286er Prozessor von Intel mit 12 MHz. Als Betriebssystem wurde MS-DOS 3.2 oder CDOS XM 6.X eingesetzt. Weiterhin war er ausgestattet mit VGA-Grafikkarte, 1 MB RAM (wahlweise bis 4 MB), 20 MB Festplatte und 3,5\" Diskettenlaufwerk. Der Preis für dieses Gerät betrug 1989 5000 DM.\n\nEin Bürorechner, ausgestattet mit iAPX-286er Prozessor mit 6 MHz und ISA-Steckplätzen. Außerdem 512 kB RAM (aufrüstbar bis 8 MB) und als Betriebssystem MS-DOS 3.1, Concurrent-DOS 5.0 oder FlexOS 286. Installiert war dies auf einer 20 MB Festplatte.\n\nEin Industrie-PC der mit bis zu fünf Laufwerken ausgestattet werden konnte.\n\nEin Bürorechner im Tower-Gehäuse. Ausgestattet mit einem 486er Prozessor mit 33 MHz. Außerdem Zehn ISA-Slots, 16 MB RAM, 5,25\" und 3,5\" Diskettenlaufwerke, ESDI-Festplatte mit 337 MB, SCSI-Festplatte mit 2 GB, VGA-Grafikkarte von EIZO mit 512 kB und 3COM Netzwerkkarte Etherlink II.\n\n"}
{"id": "476234", "url": "https://de.wikipedia.org/wiki?curid=476234", "title": "Kopete", "text": "Kopete\n\nKopete ist ein freier Instant Messenger für verschiedene Protokolle. Das Programm ist Teil der KDE Software Compilation 4. Der Name leitet sich vom chilenischen Wort \"Copete\" ab und bedeutet so viel wie \"„alkoholische Getränke“\". Die zentrale Kommunikationskomponente KDE-Telepathy ist designierter Nachfolger innerhalb von KDE.\n\nKopete unterstützt die verbreiteten Instant-Messaging-Protokolle XMPP/Jabber (Google Talk, Nokia OVI, Facebook Chat, …), OSCAR (AIM, ICQ), MSNP (MSN) und YMP (Yahoo!). Darüber hinaus unterstützt er Gadu-Gadu, Novell GroupWise, Lotus Sametime und SMS.\n\nDabei können auch mehrere Profile eines gleichen Protokolls genutzt werden. Außerdem ist es möglich, verschiedene Profile anderer Nutzer unter einem Meta-Namen zusammenzufügen. Ebenso kann das Profil des Kopete-Nutzers unter einem Gesamt-Profil zusammengefasst werden.\n\nAb der Version 0.11 gibt es ebenfalls die Möglichkeit, Webcams einzusetzen. Unterstützt wird dies zurzeit bei den Protokollen Yahoo! und MSN.\n\nAb Version 0.12 steht eine experimentelle Unterstützung für die XMPP-Audio-Erweiterung Jingle zur Verfügung. Des Weiteren wurde in dieser Version die Theme-Technik des Chat-Fensters gegen eine Adium-kompatible Technik ausgetauscht. Die Änderung ermöglicht es, die gleichen Themes unter beiden Programmen zu nutzen.\n\nAb Version 0.50 basiert Kopete auf KDE 4, bietet eine verbesserte Benutzeroberfläche und unterstützt Dateitransfers mittels ICQ und AIM.\n\nÜber ein mitgeliefertes Modul sind mit Kopete auch per Off-the-Record Messaging verschlüsselte Unterhaltungen möglich. Ein weiteres enthaltenes Modul ermöglicht auch die Verschlüsselung per OpenPGP-Standard, was allerdings keine Abstreitbarkeit und keine Folgenlosigkeit (bei Aufdecken des privaten Schlüssels) bietet.\n\n\n"}
{"id": "477292", "url": "https://de.wikipedia.org/wiki?curid=477292", "title": "KJots", "text": "KJots\n\nKJots ist ein Programm, mit dem man Notizen schreiben und verwalten kann. Es wurde für die graphische Linux-Desktop-Umgebung KDE erstellt und passt sich vollständig in deren Desktop ein.\n\nEine Besonderheit von KJots liegt darin, dass Notizen auch in ganzen Büchern gespeichert und sortiert werden können. Dabei können auch Bücher innerhalb von Büchern angelegt werden. Dies erlaubt eine komplexere Verwaltung.\n\nFür die Textformatierung stehen Werkzeuge wie Fett, Kursiv, Schriftart, Farbe, Einrückung etc. zur Verfügung, Wörter/Textpassagen können mit anderen Büchern oder Weblinks verknüpft werden, die dann per Strg-Klick aufgerufen werden können. Das Einfügen von Bildern ist nicht möglich.\n\nIn KJots erstellte „Bücher“ können als HTML-Seiten exportiert werden, ebenso einzelne Seiten. \n\nErgänzend zur Stand-Alone-Darstellung kann KJots auch innerhalb der PIM-Anwendung Kontact angezeigt werden.\n\n"}
{"id": "477299", "url": "https://de.wikipedia.org/wiki?curid=477299", "title": "KTimer", "text": "KTimer\n\nKTimer ist ein freies Programm, das Sekunden rückwärts zählt und nach Ablauf der eingestellten Zeit ein Programm starten kann.\nEs wurde für die graphische Benutzeroberfläche KDE programmiert und fügt sich nahtlos in deren Umgebung ein.\n\nKTimer kann ohne großen Aufwand Programme zu bestimmten Zeitpunkten starten, ohne dass man dafür eine komplexe Task-Verwaltung benutzen muss.\n\n"}
{"id": "478277", "url": "https://de.wikipedia.org/wiki?curid=478277", "title": "Toy Story 2", "text": "Toy Story 2\n\nToy Story 2 aus dem Jahr 1999 ist die Fortsetzung von \"Toy Story\" von 1995 und wie sein Vorgänger ein komplett computeranimierter Trickfilm. Er wurde 2000 für einen Oscar in der Kategorie Bester Song für „When She Loved Me“ nominiert.\n\nAls Andy ins Ferienlager fährt, sind seine Spielzeugfiguren auf sich gestellt. Woody hätte mitkommen sollen, doch kurz vorher wurde sein Arm beschädigt. Sie beobachten, wie Andys Mutter einen Flohmarkt eröffnet, bei dem sie unter anderem Andys Puppen verkaufen möchte. Unter den Puppen ist auch der Pinguin Wheezy, mit dem sich der Cowboy Woody, Andys Lieblingsspielzeug, während dessen Abwesenheit angefreundet hat. Es folgt eine Rettungsaktion, um den von der Mutter mitgenommenen Wheezy zurückzuholen, bei der Woody sich Andys Hund als Fortbewegungsmittel bedient. Der Plan gelingt, doch als Woody den Pinguin zurück in das Zimmer bringen will, treffen sie auf dem Flohmarkt auf einen Mann namens Al, der gerade zufällig nach einer Figur sucht, die genauso aussieht wie Woody. Nachdem Andys Mutter den Verkauf von Woody unter allen Umständen ablehnt, klaut der Mann ihn einfach.\n\nNun starten die Spielzeuge eine Rettungsaktion, bei der Hindernisse wie eine Straße in der Hauptverkehrszeit oder ein Spielzeugladen zu überwinden sind. Währenddessen erfährt Woody, dass er ein begehrtes Sammlerstück ist und mit drei anderen Wild-West-Spielzeugen an ein Spielzeugmuseum in Tokio verkauft werden soll. Zunächst denkt er an Flucht, beginnt aber zu verstehen, dass Andy ihn vielleicht eines Tages gar nicht mehr braucht. Als er die Chance hat, mit seinen Freunden zu fliehen, entscheidet er sich im letzten Augenblick aber doch noch für ein Leben in Andys Spielzeugzimmer. Doch die Spielzeugpuppe Stinke Pete, mit der Woody hätte verkauft werden sollen, sieht nun seine friedliche Museumszukunft in Gefahr. Er versucht Woody gewaltsam daran zu hindern, zu seinen Freunden zurückzukehren. Schließlich wird er im letzten Augenblick von Buzz Lightyear und den anderen Spielzeugen gerettet, bevor das Flugzeug nach Tokio abhebt. Danach kehren die Spielzeuge nach Hause zurück. Andy spielt wieder mit allen.\n\nWährend der Produktion im Jahre 1998 bewirkte ein fehlerhaft ausgeführter Befehl den Verlust fast sämtlicher Dateien des Projekts. Ein Backup-Stand wurde wiederhergestellt, doch nach einer Woche erwies sich dieser Schritt als Reinfall. Das Backup-Medium war nämlich bereits voll und neuere Dateien hatten ältere überschrieben, was bis dahin unbemerkt blieb. Dank des Heimservers der zu Hause arbeitenden Galyn Susman konnte glücklicherweise ein wochenalter Stand wiederhergestellt werden.\n\nNach der überstandenen Krise wurde der Film im Winter 1998 erneut gelöscht. Dieses Mal allerdings mit Absicht, da die Produzenten mit dem Film nicht zufrieden waren. Innerhalb von neun Monaten entwickelte Pixar den Film von Grund auf neu.\n\nDer Film, vertrieben von Buena Vista Entertainment, kam am 19. November 1999 in den Vereinigten Staaten und Großbritannien in die Kinos. Am 3. Februar 2000 kam der Film in die deutschen Kinos.\n\nObwohl der Film im Format 1:1,85 produziert wurde, enthält die „Special Edition“ DVD-Veröffentlichung lediglich eine 1:1,78-formatige Version.\nDer Film lief auch im amerikanischen und britischen Fernsehen und wurde unter anderem auch ins Französische, Spanische und Italienische übersetzt.\n\nAm 2. Oktober 2009 wurde der Film in den USA in 3D wieder in die Kinos gebracht. Er war ausschließlich in einer Doppelvorführung zusammen mit Toy Story, welcher ebenfalls in 3D konvertiert wurde, zu sehen. Ursprünglich sollten die Vorführungen auf einen Zeitraum von zwei Wochen begrenzt werden. Wegen des großen Erfolgs wurde dies jedoch auf fünf Wochen ausgedehnt.\n\nFür die 3D-Konvertierung wurden die ursprünglichen Computerdaten aufbereitet und eine virtuelle zweite Kamera hinzugefügt, so dass ein stereoskopes Bildmaterial vorlag, was für die Tiefenwahrnehmung unerlässlich ist. Allein dieser Prozess nahm vier Monate in Anspruch. Anschließend brauchte es weitere sechs Monate, um die Filme mit adäquaten 3D-Effekten zu versehen.\n\nDie 3D-Doppelvorführung von Toy Story 1 und 2 spielte innerhalb der fünfwöchigen Laufzeit 30.714.027 US-Dollar (23.686.658 €; 28.924.419 Schw. Fr.; Stand: 17. Juli 2010) ein, davon alleine 12,5 Millionen US-Dollar (9,64 Millionen €; 11,89 Schw. Fr.; Stand 17. Juli 2010) am Eröffnungswochenende.\n\nDie deutsche Synchronisation entstand nach einem Dialogbuch von Hartmut Neugebauer unter seiner Dialogregie im Auftrag der Film- & Fernseh-Synchron in Berlin und München.\nDer Film erhielt überwiegend positive Kritiken: Bei Rotten Tomatoes fallen alle 163 Kritiken für den Film positiv aus – womit er eine Wertung von 100 % besitzt – und bei Metacritic konnte ein Metascore von 88, basierend auf 34 Kritiken, erzielt werden.\n\nDie Filmbewertungsstelle Wiesbaden verlieh der Produktion das Prädikat \"wertvoll.\"\n\nAnnie Awards 2000\n\nOscarverleihung 2000\n\nGolden Globe Awards 2000\n\nSatellite Awards 2000\n\nBox Office Germany Award 2000\n\nZum Film erschien auch ein Videospiel für die Konsolen Playstation, Nintendo 64 und Dreamcast sowie für den PC. In diesem wurden Ausschnitte aus dem Film zur Einleitung der Levels verwendet.\n\nEin weiteres Spiel erschien für den Game Boy Color.\n\n\n"}
{"id": "481106", "url": "https://de.wikipedia.org/wiki?curid=481106", "title": "Die Monster AG", "text": "Die Monster AG\n\nDie Monster AG (Originaltitel: Monsters, Inc.) ist ein 2001 erschienener computeranimierter Kinofilm von den Pixar Animation Studios in Zusammenarbeit mit Disney. Der Film legte in seiner ersten Woche mit einem Einspielergebnis von mehr als 62 Millionen US-Dollar den bis dahin besten Start eines Animationsfilmes hin. Insgesamt brachte der Film mehr als 561 Millionen US-Dollar in die Kassen und ist damit der zehnterfolgreichste Film unter den komplett am Computer animierten Filmen (Stand: Dezember 2015).\n\nJames P. „Sulley“ Sullivan, ein haariger, blauer Hüne, und sein bester Freund Michael „Mike“ Glotzkovski (im Original \"Wazowski\"), ein giftgrüner Augapfel auf zwei Beinen, sind Monster. Sie arbeiten zusammen in der Monster AG, einem Energielieferanten in Monstropolis.\n\nSulley und Mike beherrschen ihr Fach perfekt. Sie erschrecken Menschenkinder in der Nacht, um durch deren markerschütternde Schreie Energie zu erzeugen, die in Behältern aufgefangen wird und der Energieversorgung von Monstropolis zugutekommt. Jedoch wird ein körperlicher Kontakt mit Menschen als gefährlich angesehen. Derart „kontaminierte“ Monster müssen mit harten Dekontaminationsmaßnahmen rechnen.\n\nGerade, als es so aussieht, als würden die beiden erneut den firmeninternen Rekord brechen, dringt durch ein unglückliches Ereignis ein Menschenkind an der Seite von Sulley in Monstropolis ein. Mike und Sulley versuchen, das Kind wieder loszuwerden, ohne Aufmerksamkeit auf sich zu ziehen.\n\nIm Laufe mehrerer vergeblicher Versuche entwickelt Sulley eine herzliche Beziehung zu dem kleinen Mädchen, das er Buh nennt, während Mike rational bleiben möchte. Sulley kann sich schließlich diesbezüglich durchsetzen, sie retten Buh aus einer lebensgefährlichen Lage und bringen sie zurück in ihr Kinderzimmer.\n\nIm Laufe dieser Geschehnisse decken die beiden eine Verschwörung auf, die bis in die oberste Chefetage führt. Das und die guten Erfahrungen mit Buh führen letztlich zu der Reorganisation der Monster AG, die nun Energie aus dem Lachen der Kinder bezieht, welche zehnmal wirksamer ist als die der Schreikraft.\n\nAlle Figuren, Landschaften und Effekte wurden am Computer erstellt. Das Monster Sullivan zum Beispiel hat über 2,3 Millionen Haare. Die Szene in der Türhalle verwendet 35 Millionen individuelle und identifizierbare Türen und eine kilometerlange Beförderungsachterbahn. Während der Produktion am Computer hieß der Film \"Hidden City\" (Arbeitstitel) oder \"Monsters, Incorporated\".\n\nZusätzlich zum Film wurde auch ein Kurzfilm namens \"Mikes neues Auto\" produziert. Er wurde im Kino nach dem Hauptfilm gezeigt und wurde auch auf DVD veröffentlicht. Einen Monat nach der Premiere wurde beim Hauptfilm der Abspann ausgetauscht. Im neuen Abspann gab es einen Blooper Reel mit (absichtlich gerenderten) verpatzten Szenen sowie eine Aufführung des Laiendarsteller-Musicals, das von Mike im Film erwähnt wird. Die Version mit diesem Abspann ist bei der 4:3-Fullscreen-DVD-Version zu finden, sowie als separates Bonusfeature bei der 2-DVD-Special Edition und bei der Blu-ray. Die Fernsehausstrahlung beinhaltet meistens den Originalabspann, allerdings mit Ton des neuen Abspanns.\n\nDer Film wurde in den Vereinigten Staaten am 2. November 2001 veröffentlicht, und kam am 31. Januar 2002 in die deutschen Kinos. Während seiner ursprünglichen Kinoveröffentlichung im Jahr 2001 spielte der Film insgesamt rund 525,4 Mio. US-Dollar ( Mio. Euro; Mio. Schweizer Franken) ein. Davon mit 255,4 Mio. US-Dollar ( Mio. Euro; Mio. Schweizer Franken) mehr als die Hälfte allein in Nordamerika (Vereinigte Staaten und Kanada) sowie 14,8 Mio. US-Dollar ( Mio. Euro; ) in Deutschland und 2,5 Mio. US-Dollar ( Mio. Euro; ) in Österreich. Inflationsbereinigt entspricht das damalige Gesamteinspielergebnis heute etwa einem Gegenwert von Mio. US-Dollar ( Mio. Euro; ).\n\nIm Vorfeld der Fortsetzung, die 2013 in 3D in die Kinos gekommen ist, wurde auch Monster AG 2012 in Nordamerika (Vereinigte Staaten und Kanada) und einigen Auslandsmärkten in 3D konvertiert noch einmal in die Kinos gebracht. Dabei spielte der Film insgesamt weitere 36,3 Mio. US-Dollar ( Mio. Euro; ) ein.\n\nDamit erreicht der Film eine nominelles Gesamteinspielergebnis von rund 561,6 Mio. US-Dollar ( Mio. Euro; Mio. Schweizer Franken).\n\nDie Synchronisation fand bei der FFS Film- & Fernseh-Synchron GmbH unter der Dialogregie von Frank Lenart statt, der auch das Dialogbuch schrieb.\n\nFür den Trailer des Films wurde Hartmut Neugebauer als Sprecher von Sulley gewählt, im Film selbst jedoch sprach ihn Reinhard Brock.\nEin zweiter Film kam unter dem Titel \"Die Monster Uni\" am 20. Juni 2013 in die deutschen Kinos, er feierte in Nordamerika unter dem Originaltitel \"Monsters University\" am 21. Juni 2013 sein Debüt. Der Film erzählt die Vorgeschichte zu „Die Monster AG“ und ist somit das erste Prequel zu einem Pixarfilm. Er schildert das erste Zusammentreffen von Sulley und Mike und das Entstehen ihrer Freundschaft. Regie führte Dan Scanlon (\"Cars\"), das Drehbuch stammt von Andrew Stanton (\"WALL·E – Der Letzte räumt die Erde auf\").\n\n\n"}
{"id": "481168", "url": "https://de.wikipedia.org/wiki?curid=481168", "title": "Die Unglaublichen – The Incredibles", "text": "Die Unglaublichen – The Incredibles\n\nDie Unglaublichen – The Incredibles aus dem Jahr 2004 ist der sechste computeranimierte Kinofilm der Pixar Animation Studios und der Walt Disney Company. Mit weltweiten Einspielergebnissen von über 631 Millionen US-Dollar war \"Die Unglaublichen\" seiner Zeit einer der erfolgreichsten Computer-Animationsfilme. Der Film startete am 9. Dezember 2004 in den deutschen Kinos.\n\nIn der fiktiven Stadt Municiberg kämpft eine Vielzahl von Superhelden im Jahre 1947 gegen das Verbrechen. Eine der führenden Figuren ist der superstarke Mister Incredible. Noch auf dem Weg zur Hochzeit mit seiner Superheldenkollegin Elastigirl, die ihren Körper wie Kaugummi verbiegen und dehnen kann, stellt er sich in den Dienst der Bürger. So verhindert er einen Selbstmord, stellt nach einer wilden Verfolgungsjagd einen Räuber, vereitelt einen Bombenanschlag des Superverbrechers Bomb Voyage und muss sich währenddessen der Zudringlichkeit eines jugendlichen Fans namens Buddy Pine erwehren, der sich Incrediboy nennt und nur zu gerne sein Sidekick werden möchte, was Mr. Incredible jedoch entschieden ablehnt. Kurz darauf verklagt der Lebensmüde Mr. Incredible dafür, dass er ihm das Leben gerettet hat, und fordert Schadensersatz für seine bei dieser Aktion erlittenen leichten Verletzungen. Dies führt zu Nachahmern und zieht eine Klagewelle gegen Superhelden und die von ihnen verursachten Kollateralschäden nach sich. Die Regierung muss für die entstandenen Kosten aufkommen und kann sich den aufkeimenden Unmut in der Bevölkerung nicht mehr länger leisten. Also beschließt man, den Superhelden ihre Einsätze zu verbieten, ihnen eine neue Identität zu verschaffen und sie als normale Menschen in die Gesellschaft zu integrieren.\n\nFünfzehn Jahre später, im Jahre 1962, leben Mr. Incredible und Elastigirl – unter ihren bürgerlichen Namen Robert „Bob“ und Helen Parr – mit ihren drei Kindern in einem kleinbürgerlichen Vorort von Metroville. Ihre pubertierende Tochter Violetta kann sich unsichtbar machen und ein Kraftfeld erzeugen. Der zehnjährige Sohn Robert „Flash“ ist übermenschlich schnell. Das Baby Jack Jack scheint keine übermenschlichen Kräfte zu haben. Ganz glücklich sind die Parrs allerdings nicht mit ihrem Familienleben: Violetta ist so schüchtern und unsicher, dass sie ihre Kräfte meist nur unbewusst einsetzen kann. Flash frustriert es, sich „normal“ verhalten zu müssen, so dass er mithilfe seiner Supergeschwindigkeit seinem Lehrer gerne Streiche spielt. Bob, der enorm zugenommen hat, fristet sein Zivilleben als Versicherungsangestellter unter einem kleingeistigen Chef und sehnt sich nach seinen Tagen als gefeierter Superheld zurück. Zum Ausgleich unternimmt er heimlich zusammen mit seinem Freund Lucius Best – dem Superhelden Frozone – Patrouillen, um in Not geratenen Menschen weiterhin zu helfen.\n\nEines Tages wird Bob wieder einmal von seinem Chef gemaßregelt, dem Bobs Menschenfreundlichkeit missfällt. In einem Wutanfall schleudert Bob ihn mit seiner Superstärke durch die nächsten Wände. Um seine Identität zu wahren, muss er den Job aufgeben, und Agent Dicker, sein Verbindungsmann zum Superhelden-Schutzprogramm, muss wieder aufwändige Arbeit leisten, um den Vorfall zu verdecken. Mitten in dieser Midlife Crisis bekommt Mr. Incredible von einer geheimnisvollen Schönheit namens Mirage ein überraschendes Angebot. Er soll auf eine abgelegene Vulkaninsel reisen und dort einen Kampfroboter namens Omnidroid 9000 einfangen, der eine Fehlfunktion aufweise. Bob kann die Mission erfolgreich abschließen. Dies lässt ihn wieder aufleben; er kümmert sich vermehrt um die Familie, beginnt zu trainieren und abzunehmen.\n\nAls er von Mirage telefonisch einen weiteren Auftrag bekommt, hört seine Frau Helen Teile des Gesprächs mit und vermutet, Bob habe eine Affäre. Bob muss die Wahrheit vor ihr verbergen, da sie vehement darauf besteht, die Regierungsauflagen zu erfüllen und sich unauffällig an die normale Bevölkerung anzupassen.\n\nDer Auftrag führt Mr. Incredible erneut auf die Insel. Das Unternehmen entpuppt sich jedoch als Falle, denn ein verbesserter Omnidroid tötet ihn fast. Der Kampfroboter wird von Mr. Incredibles ehemals größtem Fan Buddy Pine kontrolliert, der sich jetzt „Syndrome“ nennt und der seit seiner Zurückweisung durch Mr. Incredible einen verzehrenden Hass auf alle Superhelden entwickelt hat. Es stellt sich heraus, dass der gerissene Erfinder schon zahlreiche Superhelden auf seine Insel gelockt hat, um sie gegen seine Roboter kämpfen zu lassen und durch die daraus gewonnenen Erkenntnisse die Eigenschaften seines Omnidroiden zu verbessern. Alle Superhelden zuvor haben hierbei den Tod gefunden. Den schließlich voll entwickelten Omnidroiden will er nach Metroville schicken. Dort soll der Kampfroboter die Stadt angreifen, so dass Syndrome sich ihm in den Weg stellen und ihn heimlich durch eine Fernbedienung „besiegen“ kann, um so als neuer und einziger Superheld gefeiert zu werden.\n\nUnterdessen findet Helen Bobs alten, aber geflickten Superhelden-Anzug und wird misstrauisch. Sie besucht ihre langjährige Freundin Edna Mode, eine Designerin, und findet heraus, dass diese für Bob und den Rest der Familie neue Anzüge entworfen hat. Die Anzüge enthalten unter anderem Peilsender, was es Helen ermöglicht, ihren Mann zu orten. Dadurch wird jedoch Bobs Peilsender just in dem Moment aktiviert, als der in Syndromes bestens abgesichertem Geheimversteck herumschleicht. Kanonen mit haftenden und aufblasbaren Geschossen strecken ihn schließlich nieder und er wird gefangen genommen. Daraufhin schlüpft Helen wieder in ihre Rolle als Elastigirl und fliegt zur Insel – mit ihren Kindern Violetta und Robert „Flash“ an Bord, die sich als blinde Passagiere eingeschmuggelt haben. Bei einer abenteuerlichen Flucht vor Syndromes Wachmannschaft und seinen ausgeklügelten Sicherheitssystemen können sie Mr. Incredible schließlich befreien und gemeinsam entkommen. Hierbei hilft ihnen auch Mirage, die sich gegen Syndrome stellt, nachdem dieser in einer Konfrontation mit Mr. Incredible ihr Leben aufs Spiel setzt.\n\nWährenddessen greift der Roboter die Stadt an, und Syndrome erscheint auf der Bildfläche. Doch sein Versuch, sich als Held zu profilieren, scheitert kläglich: Seine Programmierung macht den Roboter so unabhängig, dass er seinen Erbauer als Gefahr ansieht und ihn beschießt. Syndrome verliert dabei seine Fernbedienung, erleidet eine Bruchlandung und fällt in Ohnmacht. Die Incredibles kommen kurz danach an und nehmen mit Frozones Unterstützung den Kampf auf. Der Omnidroid scheint überlegen zu sein, doch als sie Syndromes Fernbedienung finden, gelingt es ihnen, die Kampfmaschine zu zerstören. Zum ersten Mal seit fünfzehn Jahren ernten die Superhelden wieder die Achtung der Bürger.\n\nAls sie zuhause ankommen, müssen die Parrs voller Schrecken feststellen, dass Syndrome ihr Baby Jack Jack entführt hat und mithilfe seiner Düsenstiefel zu seinem Jet unterwegs ist, der oben am Himmel auf ihn wartet. Die Incredibles müssen zunächst untätig zusehen, doch dann erwacht Jack Jack und beginnt zu aller Überraschung, Superkräfte einzusetzen, so dass Syndrome ihn erschreckt fallen lässt. Den Incredibles gelingt es, Jack Jack sicher zur Erde zu bringen. Bob wirft seinen Sportwagen in die Luft und trifft Syndromes Jet, wodurch der Bösewicht fällt und durch sein Cape in die Turbine seines Jets gesaugt wird. Somit wird Ednas Entscheidung bestätigt, beim Entwurf von Mr. Incredibles neuem Anzug auf ein Cape zu verzichten. Als der Jet abstürzt, gelingt es Violetta, die Kontrolle über ihre Kräfte wiederzuerlangen und ein Kraftfeld aufzubauen, das die Familie vor dem herabstürzenden Wrack schützt.\n\nEinige Zeit später haben die Incredibles dank ihres neuen Verständnisses füreinander wieder ein glückliches Familienleben vor sich. Eines Tages erscheint ein neuer Superschurke: der „Tunnelgräber“. Ohne sich lange absprechen zu müssen, schlüpfen die Familienmitglieder in ihre Superheldenidentitäten, um den Kampf mit ihm aufzunehmen.\n\nDer Vorfilm bzw. DVD-Bonustrack war \"Boundin’ – Ein Schaf ist von der Wolle\" aus dem Jahr 2003, der ebenfalls eine Oscarnominierung erhalten hat.\n\nErwähnenswert sind auch die Filmszenen, die in der End-Fassung fehlen, da sie die Gesamtlänge des Filmes gesprengt hätten. Darin zu sehen ist eine Szene, in der Jack zum Feuerteufel wird.\n\n2018 erschien eine Fortsetzung mit dem Titel \"Die Unglaublichen 2\". Der Film lief am 15. Juni in den US-amerikanischen und am 27. September 2018 in den deutschen Kinos an. Neben Brad Bird, der erneut die Regie übernahm, kehrte auch Michael Giacchino als Komponist für die Fortsetzung zurück.\n\n\nDer Film war in den Kinos sehr erfolgreich und spielte weltweit über 631 Millionen US-Dollar ein.\n\n\nOscarverleihung 2005\n\n\nGolden Globe Awards 2005\n\nAnnie Award 2005\n\nHugo Award 2005\n\nDVD Champion 2005\n\n\n"}
{"id": "481424", "url": "https://de.wikipedia.org/wiki?curid=481424", "title": "Jeskola Buzz", "text": "Jeskola Buzz\n\nJeskola Buzz, oder kurz nur Buzz, ist der Name eines kostenlosen Software-Studios, das seit Mitte 1997 von Oskari Tammelin (Pseudonym \"Jeskola\") in Finnland entwickelt wird. Buzz ist ein Plug-in basierter sogenannter \"3rd Generation Tracker,\" also eine Musiksequenzersoftware.\nÜber die Jahre hat sich eine weitreichende Szene um das Programm entwickelt, das auch von populären Künstlern der Elektronik-Szene wie James Holden oder Andreas Tilliander benutzt wird.\n\nBuzz ist ein Plug-in basierter sogenannter \"3rd Generation Tracker,\" für den inzwischen weit mehr als 1000 Plug-ins verfügbar sind. Außerdem bietet Buzz VST(i)-Unterstützung. Es gliedert sich in drei Hauptmodule: Die \"Machineview,\" den \"Pattern Editor\" und den \"Sequencer\". Im Machineview lassen sich die Plug-ins laden und die Signale frei routen. Der Sequencer ist im Gegensatz zu herkömmlichen Trackern nicht linear aufgebaut. Dies bedeutet, dass Parts frei arrangiert werden können wie in üblichen Sequencer-Programmen. Der Pattern-Editor hat sich in den letzten Betaversionen dahingehend entwickelt, dass er nicht mehr zwangsläufig ein solcher ist. Es wurde die Möglichkeit implementiert, dass Programmierer eigene Editoren als Plug-ins beisteuern. Dieses Feature bedeutet, dass die Bezeichnung \"Tracker\" für Buzz immer unzutreffender wird. So arbeitet Oskari Tammelin selbst an einer \"Pianoroll\".\n\n\nObwohl Buzz Tracker-basiert ist, spielt die Wiedergabe und Manipulation von Samples nur eine untergeordnete Rolle im Programm. Neben den klassischen Samplern gibt es in Buzz etliche native Synthesizer (in Buzz als „Generators“ bezeichnet) und Effektgeräte (in Buzz „Effects“ genannt); außerdem ist die Nutzung von VST-Geräten möglich.\nDurch den überwiegenden Verzicht auf eine eigene grafische Benutzeroberfläche wirkt Buzz oftmals abschreckend auf Erstnutzer. Die elementar-funktionale Programmstruktur ermöglicht allerdings erst die große Flexibilität von Buzz, die sich unter anderem darin zeigt, dass das Programm nicht installiert werden muss, sondern mit dem Öffnen der Ausführungsdatei bereits funktionsfähig ist (zumindest in der ursprünglichen Programmversion, neuere Modifikationen erfordern eine Installation). Des Weiteren sind Plug-ins für Buzz für jeden Programmierer mit C++-Kenntnissen leicht zu erstellen und zu implementieren, was einerseits zu einer Vielzahl verschiedenster verfügbarer Plug-ins führt, andererseits aber dazu, dass viele Einstellungen schlecht klingen oder sogar zu Programmabstürzen führen. Dadurch wird das Musizieren selbst oftmals weniger als intuitives Spielen empfunden denn wiederum als Programmieren.\n\nNach der Veröffentlichung der aktuellen Version Beta 1.2 musste Oskari Tammelin die Entwicklung von Buzz einstellen, weil ihm aufgrund eines Festplattendefektes die einzige Kopie des Quelltextes verlorenging. Seitdem gab es verschiedene Versuche, entweder den Binärcode des Original-Programms durch Hacks zu optimieren oder Programme von Grund auf neu zu programmieren, die Buzz nachempfunden sind („Buzz-Clones“, siehe Links). Auch gab es jahrelang Gerüchte über eine offizielle Version 2, nach dem fast vollständigen Rückzug von Oskari Tammelin aus der Szene schien dies jedoch immer unwahrscheinlicher. Am 2. Juli 2008 veröffentlichte Buzzmachines.com die Meldung, dass Oskari Tammelin an einer neuen Version arbeitet, die er teilweise aus alten Backups rekonstruieren, teilweise neu implementieren will. Eine Beta-Version ist bereits verfügbar und um inzwischen zahlreiche grundlegende Funktionen erweitert worden.\n\n"}
{"id": "483609", "url": "https://de.wikipedia.org/wiki?curid=483609", "title": "Nastran", "text": "Nastran\n\nNastran (Kurzform für \"Nasa Structural Analysis System\") ist ein auf der Finite-Elemente-Methode (FEM) basierender Solver ohne die Möglichkeit der graphischen Interaktion und wird meist zusammen mit Pre-/Postprozessoren, mit denen die Anwender die Berechnungsmodelle erstellen, eingesetzt.\n\nDie Eingabedateien sind reine Textdateien und daher auch problemlos zwischen verschiedenen Pre-/Postprozessoren und Betriebssystemen austauschbar. Nastran war der erste kommerzielle FEM-Solver und aufgrund seiner langen Geschichte und der damit verbundenen Reifung gilt Nastran als De-facto-Standard für Finite Elemente Berechnungen in der Luft- und Raumfahrt.\n\nDie Entwicklung dieser Software begann in den 1960er-Jahren durch die US-Raumfahrtbehörde NASA an mehreren Standorten in Hinblick auf die Erschließung des Weltalls. Dies stellte sich als nicht zielführend heraus und daher wurde 1964 die Firma CSC mit der Entwicklung eines einheitlichen FEM-Solvers beauftragt . Der ursprünglich gewählte Name war GPSA (General Purpose Structural Analysis), wurde jedoch von der NASA in Nastran geändert. Die Software wurde zunächst in Fortran geschrieben und zeichnet sich durch eine offene Architektur mit der Möglichkeit, eigene Routinen einzubinden, aus. Der Quellcode wurde von mehreren Softwarefirmen erworben und im Laufe der Jahrzehnte kontinuierlich weiterentwickelt. Besondere Bedeutung hatte dabei die MacNeal-Schwendler Corporation (heute MSC Software).\n\nDie kommerzielle Nutzung begann 1971 und der erste Kunde in Deutschland war 1972 Daimler-Benz. Eine große Verbreitung erfuhr Nastran schon früh in der Luft- und Raumfahrt sowie der Automobilindustrie. Bedingt durch den Preisverfall der Hardware wurde die Software zunehmend auch in anderen Industriebereichen und an Hochschulen eingesetzt. Heute ist Nastran einer der am meisten genutzten FEM-Solver.\n\nFür außerordentliche Verdienste wurde Nastran im Jahre 1988 in die Space Technology Hall of Fame aufgenommen.\n\nIn den 1980er und 1990er Jahren waren neben MSC Nastran auch andere Derivate (UAI-Nastran, CSAR-Nastran) am Markt vertreten. Bedingt durch die Akquisition dieser Mitbewerber hatte MSC Software eine Monopolstellung geschaffen und wurde 2002 von der US-amerikanischen FTC verpflichtet, Nastran für Wettbewerber zugänglich zu machen. Quellcode, Dokumentation und die Rechte zur dauerhaften, lizenzfreien Nutzung wurden von der damalige UGS, heute Siemens PLM, erworben. UGS veröffentlichte im Jahr 2003 NX Nastran 1.0. Die bekanntesten Nastran-Versionen sind heute MSC Nastran, NX Nastran und NEi Nastran. \n\n\n"}
{"id": "485264", "url": "https://de.wikipedia.org/wiki?curid=485264", "title": "CBM 900", "text": "CBM 900\n\nDer CBM 900 war ein Computersystem von Commodore, welches als Server- und Workstationvariante Anfang der 1980er Jahre entwickelt, aber nie in Serie produziert wurde. Als CPU wurde ein Zilog Z8001 mit 10 MHz verwendet, sowie der Hauptspeicher mit 512 KB RAM ausgestattet; wobei dieser auf bis zu 2 MB ausgebaut werden konnte. Als Grafik- oder Videochip wurde der MOS VDC 8563 entwickelt, der später im C128 verwendet wurde. Die Prototypen hatten eine 20-MB-Festplatte und liefen mit dem Betriebssystem Coherent.\n\nDer Computer ging nie in Serie, unter anderem weil Commodore zu dieser Zeit die Firma Amiga übernahm und mit den unter dieser Marke vertriebenen Rechnern, vor allem Amiga 1000 und Amiga 2000, leistungsfähigere Alternativen zum C900 ins Rennen brachte.\nDas Gehäusedesign ist nur augenscheinlich identisch mit dem des Amiga 2000, nicht zuletzt da es etwas größer dimensioniert ist.\n\n\n\n\n\n"}
{"id": "486051", "url": "https://de.wikipedia.org/wiki?curid=486051", "title": "Informix", "text": "Informix\n\nInformix ist der Produktname für ein Datenbankmanagementsystem (DBMS). Die gleichlautende Datenbankfirma wurde 2001 von IBM übernommen, die Software wird seitdem von IBM weiterentwickelt und liegt aktuell in der Version 12.10 vor.\n\nDas ehemalige Unternehmen Informix spielte bei der Verbreitung relationaler Datenbanken (RDBMS) unter Unix eine bedeutende Rolle – als stärkster Wettbewerber von Oracle.\n\nDie zugehörige, ebenfalls auf UNIX und auf Terminalbetrieb ausgerichtete Integrierte Entwicklungsumgebung \"Informix 4GL\" war in den frühen 1990er-Jahren das vermutlich verbreitetste 4GL-Entwicklungswerkzeug überhaupt. In Deutschland wurden die Informix-Produkte zunächst durch den Distributor und Systemintegrator \"Garmhausen und Partner\", dann durch die \"BYTEC GmbH\" sowie Siemens bekanntgemacht und etabliert. \n\nWährend der Datenbankserver \"Informix Dynamic Server (IDS)\" auch heute noch weit verbreitet ist, gelang es dem Unternehmen nicht, ein ähnlich erfolgreiches Nachfolgeprodukt für die inzwischen veraltete 4GL-Entwicklungsumgebung auf den Markt zu bringen.\n\nNeben dem eigentlichen Informix-Datenbankserver hat auch das von Informix entwickelte Datenzugriffstool \"C-ISAM\" (eine C-Schnittstelle für ISAM-Dateien) eine weite Verbreitung (vor allem auf UNIX-Systemen) gefunden. Die neueren Versionen der Informix IDS können mittels des C-ISAM Datablades auf die Daten zugreifen, die im alten C-ISAM Format abgelegt wurden.\n\nMit der Übernahme von \"Garmhausen und Partner\" wurde Mitte der 1990er-Jahre versucht, die Position des Unternehmens in Deutschland weiter zu stärken. In den Folgejahren jedoch verlor Informix, ausgelöst durch einen Bilanzierungsskandal und einige zweifelhafte Akquisitionen, an Börsenwert. Im Jahr 2000 kaufte Informix die Firma Ardent (ETL-Anbieter) dazu und benannte sich wenig später in Ascential um, um zu verdeutlichen, dass das Softwareportfolio nun aus mehr Produkten als nur dem Datenbankserver bestand. Nach einem Managementwechsel wurde die Datenbanksparte Informix inklusive Mitarbeiter an die IBM für 1 Milliarde Dollar verkauft. Mittlerweile wurde auch Ascential für eine weitere Milliarde Dollar von der IBM übernommen. \n\nNach dem Aufkauf durch die IBM gab es kurzzeitig den Plan, Informix mit DB2 zusammenzuführen, was eine gewisse Verunsicherung der Informix-Kunden verursachte. Dieser Plan wurde aber verworfen, und Informix und DB2 werden parallel mit unterschiedlichem Fokus weiterentwickelt, wobei aber häufig neue Technologien in das jeweils andere System einfließen. Der Datenbankserver \"Informix Dynamic Server (IDS)\" wird von IBM als strategisches Datenbanksystem bezeichnet und wieder verstärkt im Markt (vor allem für OLTP, Integrated Applications, embedded systems, spatial applications und SMB) etabliert.\n\nAm 26. März 2013 hat IBM die Informix Version 12.10 in einem Webcast vorgestellt.\n\nDie aktuelle Version von Informix IDS (12.10, Codename: Centaurus) wurde am 26. März 2013 vorgestellt. Die neueste Version ist verfügbar auf AIX, HP-UX, Linux, Mac OS X, Solaris und Windows.\n\nDer Hersteller bewirbt \"Informix\" mit folgenden Eigenschaften:\n\nDie Stärken der Version 12.10 sind\n\nDie Stärken der Version 11 sind:\n\nBei dieser dateibasierenden Version werden alle Tabellen, inklusive Systemtabellen, in einem Unterverzeichnis pro Datenbank in je zwei Dateien gespeichert: eine Datei mit den Daten, eine mit den Indizes. Der Zugriff erfolgt direkt durch die Clients. Dieses System benötigt wenig Wartung, einzelne Tabellen können leicht einzeln gesichert und wieder zurückgesichert werden. Die Datenbanken können mit oder ohne Transaktionslogik angelegt werden.\n\nBei dieser Version werden die Daten auf einem oder mehreren Bereichen (Database-Spaces) mittels Manager verwaltet. Der Zugriff durch die Clients ist nur möglich, wenn die Datenbank online ist, d. h. ein Serverprozess läuft. Für die Sicherung usw. sind spezielle Programme notwendig.\n\nEine Besonderheit ist der Datentyp SERIAL für eine automatisch fortlaufend vergebene Nummer. Je Tabelle kann ein Feld diesen INTEGER-Typ haben. Bei Anlage der Tabelle kann der Startwert angegeben werden. Bei einem Insert des Wertes 0 in dieses Feld, wird die nächste Nummer vergeben. Ein Update ist nicht möglich, es kann jedoch beim Insert eine selbst bestimmte freie Nummer angegeben werden. Ist diese Nummer höher als der aktuelle letzte Wert, wird ab dieser weitergezählt. Das Rücksetzen auf einen niedrigeren Wert – z. B. nach Löschungen – ist nur durch Neuanlage der Tabelle möglich. Dieser Feldtyp ersetzt die bei anderen Datenbanken üblichen Sequenzen und ist nicht SQL-Standard.\n\n\nDurch den verschachtelten Aufruf von Masken und Reports konnten schon kleinere Programme realisiert werden.\n\n\n SELECT Wert FROM Tabelle1 INTO nWert WHERE ID = 1\n\n\nDie Programmmodule wurden zuerst in CE-Code (C embedded, wo die SQL-Kommandos noch nicht aufgelöst sind), danach in C-Code übersetzt und dieser compiliert.\nDaher bestand die Möglichkeit auch CE- und C-Source einzubinden, die Parameterübergabe erfolgte über Stacks.\nDie Objektdateien wurden dann mit Standard-, Informix- und eigenen Bibliotheken gebunden.\nDie Masken wurden in eine spezielles Format für die Programmverwendung umgewandelt.\nFür den Programmablauf benötigte man eine Runtime-Umgebung, die Datenbank, die Programmdateien und die Maskenfiles.\n\nVon der Siemens AG gab es eine OEM-Version für Sinix auf Deutsch.\n\n"}
{"id": "486531", "url": "https://de.wikipedia.org/wiki?curid=486531", "title": "CBM-500-Serie", "text": "CBM-500-Serie\n\nDie Computer von Commodore aus der CBM-500-Serie wurden zum Ende der 1970er-Jahre konzipiert. Sie waren als Nachfolger der erfolgreichen \"PET\"-Computer (\"Personal Electronic Transactor\") gedacht. (Der Begriff \"PC\" / \"Personal Computer\" war zu jener Zeit noch nicht Allgemeingut; er setzte sich erst ab 1981 mit dem IBM-PC durch.)\n\nDie Rechner der Serie CBM 500 waren wie die Vorgänger und Nachfolger mit 8-Bit-Mikroprozessoren ausgestattet.\n\nInnerhalb der CBM-500-Serie (CBM-II-Serie) von Commodore war der P 500 ein Modell mit ausgesprochen futuristischem Design. Es stammte von \"Ira Velinsky\", der unter anderem auch den Prototyp des PET 2001 und den Atari Stacy entwarf. Die CBM-500-Serie war ein erfolgloser Versuch, die PET/CBM-Serie zu ersetzen.\n\nEs war geplant, Computer für Private (P) und für Business-Kunden (B) zu bauen. Das P-Modell wurde, wie später auch der C64, mit dem VIC-II-Chip ausgestattet und konnte 16 Farben und bis zu 320 × 200 Pixel darstellen. Die B-Modelle waren auf den Büroalltag ausgelegt und konnten mit ihrem MOS-6545-Chip 25 Zeilen zu je 80 Zeichen darstellen.\n\nDer P 500 war ein Tastaturcomputer für den Betrieb mit einem externen Monitor. Er wurde mit einem MOS 6509-Prozessor mit 1 MHz ausgestattet. Dieser Prozessor war in der Lage, mittels \"Bankswitching\" bis zu 16 Bänke zu je 64 kB zu adressieren. Dieser Homecomputer besaß im Gegensatz zu den \"B\"-Modellen der 600er- und 700er-Serie zwei Joystick-Ports.\n\nDer P 500 ist offiziell nie in den Handel gekommen. Daher existiert, im Gegensatz zu seinen kurz darauf lancierten „Brüdern“ der Serien 600 und 700, außer einer Verkaufsdemo keine Software.\n\nMit leicht erlernbarer BASIC-Programmiersprache ausgerüstet, waren die Rechner der CBM-Serien bis in die frühen 1990er-Jahre von Bedeutung, u. a. zur Steuerung von Laborausrüstungen und Messgeräten, da sie über eine IEEE-488-Schnittstelle verfügten.\nAuch heute sind sie noch vereinzelt anzutreffen.\n\nDie Modellnummern wurden je nach Speicherbelegung wie folgt vergeben:\n\n\n"}
{"id": "486545", "url": "https://de.wikipedia.org/wiki?curid=486545", "title": "CBM-600-Serie", "text": "CBM-600-Serie\n\nDie Computer der CBM-600-Serie sind Mikrocomputer bzw. Personal Computer aus den frühen 1980er-Jahren, aus der Zeit kurz nach Einführung des IBM-PC. Sie arbeiten intern mit einem 8-Bit-Mikroprozessor – der IBM-PC kam hingegen 1981 als 16-Bit-Rechner heraus.\n\nDie CBM 600-Serie war die professionellere Ausgabe der nie in Serie gegangenen CBM-500-Serie. Im Gegensatz zu den 500ern besaß sie keinen Joystickanschluss mehr. Durch eine Tochterplatine konnten die Rechner mit einem 8088- oder Z80-Prozessor ausgestattet werden, um CP/M- oder MS-DOS-Programme betreiben zu können.\n\nAls Massenspeicher wurden wie bei den Vorgängern der CBM-8000-Serie die CBM-Diskettenlaufwerke verwendet.\n\nDurch die beginnende Konzentration des Marktes auf den IBM-PC und seine Nachbauten war diesen Rechnern kein großer Verkaufserfolg mehr beschert. Mit einer recht bequemen BASIC-Programmiersprache ausgerüstet, waren sie aber punktuell bis in die frühen 1990er-Jahre von Bedeutung, u. a. zur Steuerung bei der Laborautomatisierung und von Messgeräten aufgrund ihrer, bei Modellen anderer Hersteller kaum vorhandenen, IEEE-488-Schnittstelle.\n\n\nDer gesamte Speicher ist in sogenannte Segmente (oder \"Banks\") unterteilt. Jedes dieser Segmente umfasst einen Adressraum von 64 KByte. Es können maximal 16 solcher Segmente verarbeitet werden. Diese Segmente sind von 0 bis 15 durchnummeriert. Jedes Segment hat eine feste Bedeutung, die (zum Teil) vom implementierten Speicherausbau abhängt.\n\nBei CBM 610 mit 128 KB gilt die Aufteilung:\n\nBei Modellen mit 256 KB RAM ist Segment 1 identisch mit den 128-KB-Modellen\n\nBei allen Modellen wird das Segment 15 identisch als \"Systemsegment\" verwendet.\nDarin sind der BASIC-Interpreter, der Editor, der Kernel (Betriebssystem), die E/A-Bausteine sowie die Systeminformation (Zero-Page etc.) enthalten.\n\n\n\n\nIn der englischsprachigen Ausgabe des Bedienerhandbuchs (COMMODORE 500/600/700 SERIES USER'S GUIDE) von 1983 ist zu lesen, dass viele Programme des C-64 für die Commodore-CBM-500-Serie erhältlich sein sollen. Ebenso sollten die Programme der CBM-8000er-Serie für die 600er Serie konvertiert werden. Ferner sollten Assembler und ein BASIC-Compiler erhältlich sein.\n\nFolgende Programme waren geplant:\n\nIm Juli 2016 erschien mit \"Space Chase\" (www.spacechase.de) ein vollständig in Assembler programmiertes Spiel, das die Fähigkeiten der CBM 600 und 700-Serie voll ausnutzt. Auch der in den Computern verbaute SID-Soundchip kommt mit Musik und Soundeffekten vom SID-Komponisten \"Max Hall\" zum Einsatz. Space Chase ist Freeware.\n\nDie Modellnummern lauteten wie folgt:\n\n"}
{"id": "486577", "url": "https://de.wikipedia.org/wiki?curid=486577", "title": "CBM-700-Serie", "text": "CBM-700-Serie\n\nDie Arbeitsplatzrechner der CBM-700-Serie von Commodore wurden Anfang der 1980er-Jahre auf den Markt gebracht. Den Erfolg der Heimcomputer von Commodore (VC20, C64) nach oben abrundend und mit bequemerer BASIC-Programmiersprache ausgerüstet, waren die Rechner der 700er Serie für mehr als ein Jahrzehnt bis in die frühen 1990er-Jahre von Bedeutung, u. a. zur Steuerung bei der Laborautomatisierung aufgrund ihrer IEEE-488-Schnittstelle. \n\nIm Gegensatz zur CBM 500- und CBM-600-Serie wurden die CBM-700er in einem Gehäuse mit integriertem Monochrom-Bildschirm mit 12 Zoll (30,5 cm) Diagonale ausgeliefert. Wie bei diesen wurden als Massenspeicher weiterhin die CBM-Diskettenlaufwerke verwendet.\n\nIm CBM 730 war auf der Tochterplatine ein Intel-8088-Prozessor mit 4,77 MHz integriert.\n\nDie Modellbezeichnungen lauteten:\n\n"}
{"id": "488767", "url": "https://de.wikipedia.org/wiki?curid=488767", "title": "Captive (Dateisystem)", "text": "Captive (Dateisystem)\n\nCaptive ist ein von Jan Kratochvil entwickeltes freies Programmpaket, mit dem es um 2005 erstmals möglich wurde, unter Linux auf das proprietäre Dateisystem NTFS sicher schreibend zuzugreifen. Dies erreichte Captive durch die Einbindung des originalen Treibers \"ntfs.sys\" von Windows XP.\n\nCaptive wird seit Januar 2006 nicht mehr weiterentwickelt und wird dank ausgereifter Alternativen auch nicht mehr benötigt.\n\nNTFS ist das Standard-Dateisystem der Microsoft-Betriebssysteme der NT-Reihe. Die Spezifikationen von NTFS sind nicht öffentlich zugänglich. Obwohl Linux-Entwickler seit 1995 versucht haben, NTFS-Treiber zu schreiben, dauerte es über zehn Jahre, bis dieses Unterfangen mit NTFS-3G erfolgreich verwirklicht wurde. Als dieses Ziel erreicht war, wurde die Entwicklung von Captive-NTFS folgerichtigerweise aufgegeben.\n\nCaptive NTFS stellte zum ersten Mal die Möglichkeit bereit, uneingeschränkt und sicher auf NTFS-Dateisysteme zuzugreifen. Es hatte jedoch einen äußerst hohen Bedarf an Arbeitsspeicher. Die Zugriffe wurden zudem durch die Emulation einer Windowsumgebung erheblich verlangsamt.\n\nFür die Nutzung des Microsoft-Treibers, der nicht Teil von Captive war und nachgeladen werden musste, war wie beim normalen Einsatz von Windows eine Windows-Lizenz erforderlich.\n\nCaptive NTFS umging die Probleme des Linux-eigenen NTFS-Treibers, indem es den originalen Treiber \"ntfs.sys\" aus Microsoft Windows XP in einer Sandbox laufen ließ. Mit Hilfe von Komponenten des freien Windows NT-Nachbaus ReactOS entwickelte Jan Kratochvil eine Kompatibilitätsschicht, die dem Treiber eine Windows-Umgebung vortäuschte und ihn so nutzbar machte. Transparenter Zugriff auf das Dateisystem (also ein aus Sicht von Programmen und Benutzern \"unmittelbarer\" Zugriff, der sich nicht von dem auf native Dateisysteme unterscheidet) wurde ermöglicht, indem diese Software mit Hilfe des Userland-Dateisystems FUSE in den Kernel eingebunden wurde.\n\nIm Laufe des Jahres 2005 hat das Linux-NTFS-Projekt eine weitgehende Unterstützung von NTFS erreicht. So können, mit bestimmten Einschränkungen, mittlerweile Dateien neu angelegt, gelöscht und in ihrer Größe verändert werden. Diese Funktionen werden allerdings nur nach und nach in den Linux-Kernel integriert.\n\nDie weitergehenden Funktionen, die in den vom Linux-NTFS-Projekt herausgegebenen Bibliotheken implementiert sind, lassen sich über die Hilfsprogramme \"ntfsmount\" (Paket \"ntfstools\") sowie NTFS-3G nutzen, die ihrerseits auf FUSE (Filesystem in Userspace) zurückgreifen, um Partitionen im Dateisystem verfügbar zu machen. NTFS-3G ist das neuere der beiden Programme und weist nur noch auf die unterschiedliche Architektur von Windows und Linux bezogene Einschränkungen auf.\n\nEine kommerzielle Alternative ist \"NTFS for Linux\" der Firma \"Paragon\".\n"}
{"id": "492955", "url": "https://de.wikipedia.org/wiki?curid=492955", "title": "Slab allocator", "text": "Slab allocator\n\nDer Slab allocator ist ein Verfahren zur Verwaltung von Arbeitsspeicher, das viele Betriebssysteme und auch Anwendungen verwenden. Der Algorithmus hat zum Ziel, dass bei der häufig vorkommenden Reservierung kleiner Speicherbereiche der vorhandene Arbeitsspeicher möglichst effizient, also mit wenig Verschwendung, genutzt wird.\n\nAnmerkung: Einige Details der vorliegenden technischen Beschreibung des Slab allocators basieren auf der Implementierung im Linux-Kernel 2.6.10-rc2. Diese können sich von anderen Systemen unterscheiden und sind ggf. technisch nicht auf dem aktuellen Stand.\n\nEntwickelt wurde der Slab allocator 1994 von Jeff Bonwick für SunOS 5.4. Da er seine Vorgehensweise öffentlich dokumentiert hat, wurde sie von vielen anderen Betriebssystemen (z. B. Linux) und Anwendungen (z. B. Perl) übernommen. Im Jahr 2001 veröffentlichte Bonwick eine deutlich verbesserte Version, die ebenso öffentlich dokumentiert wurde.\n\n1999 hielt der Slab allocator Einzug in den Linux-Kernel. Die aktuelle Implementierung enthält bereits einige Elemente aus Bonwicks 2001er-Version, diese ist jedoch noch nicht vollständig umgesetzt.\n\nDas Ziel einer Speicherverwaltung sollte es sein, den vorhandenen Speicher möglichst effizient zu nutzen und dabei eine schnelle Freigabe und Reservierung der benötigten Objekte zu ermöglichen. Dabei ist auch das Problem der Fragmentierung nicht zu vernachlässigen: Es sollten im Betrieb möglichst wenig kleine, ungenutzte Speicherbereiche zwischen den benutzten entstehen.\nAus technischen Gründen wird der Speicher in großen (bei IA-32: 4096 Bytes) Speicherseiten organisiert, was jedoch nicht besonders effizient für dynamisch angeforderten Speicher ist.\n\nDie meisten Objekte, die ein Betriebssystem verwendet, sind relativ klein und haben eine begrenzte Lebensdauer, werden also nach kurzer Zeit wieder an das System zurückgegeben. Außerdem werden häufig mehrere Objekte desselben Typs benötigt.\n\nBonwick distanziert sich in seiner Ausarbeitung von der Idee privater Caches, also Zwischenspeicher, die jeder Prozess bzw. jedes Kernelmodul im Betriebssystem selbst verwaltet.\nZwar hat ein Prozess selbst den besten Überblick über den eigenen Speicherbedarf, jedoch hat er fast keinen Überblick über die Speichersituation im System als Ganzes und keinen über den Bedarf anderer Prozesse.\n\nBonwick unterteilt die Speicherverwaltung folglich in zwei Teile. Der Central allocator stellt Funktionen bereit, um den Speicher zu verwalten. Er sorgt für die möglichst effiziente Nutzung.\nDem gegenüber stehen die Clients. Diese beschreiben nur noch die benötigten Objekte und müssen sich nicht um Details kümmern.\n\nFolgender Aufbau wird nun von Bonwick vorgeschlagen:\n\nMit anderen Worten: Der Slab allocator teilt den vom Buddy-System gelieferten Speicher weiter auf.\nDer Central Allocator bietet Schnittstellen (APIs) an, über die die Clients Speicher anfordern können.\n\nDie Caches besitzen drei Aufgaben:\n\nJeder Cache wird durch eine Datenstruktur (unter Linux vom Typ: kmem_cache_s) repräsentiert. Diese enthält drei doppelt verkettete Listen (innerhalb der Unterstruktur list vom Typ kmem_list3), unter der die Slabs aufgereiht werden. Eine Liste enthält die Slabs, die nur mit ungebrauchten Objekten gefüllt sind (\"all free\"), eine mit den vollständig in Benutzung befindlichen Objekten (\"all used\"), und die dritte Liste enthält Slabs mit gebrauchten sowie derzeit nicht gebrauchten Objekten (\"mixed free/used\").\n\nDes Weiteren gibt es (seit der 2001er Version von Bonwicks Ausarbeitung) einen Zwischenspeicher (vom Typ array_cache) für jeden Prozessor im System, der sich die zuletzt freigegebenen Objekte merkt. Diese Objekte werden auch zuerst wieder vergeben, da die Wahrscheinlichkeit sehr hoch ist, sie noch in einem der Prozessorcaches zu finden, die dadurch also besser ausgenutzt werden.\n\nAm Beispiel wird dieses Konzept deutlicher.\nEs gibt unter Linux Laufzeitinformationen zum Slab allocator in der Datei /proc/slabinfo. Die folgende beispielhafte Ausgabe ist stark gekürzt.\n\nBedeutung der Spalten:\n\n\nDer erste Teil der Tabelle zeigt die bereits erwähnten Caches für bestimmte Objekte, der zweite Teil die Caches in allgemeinen Größen, je in einer Variante für DMA-fähigen Speicher und für nicht-DMA-fähigen Speicher.\n\nFragmentierung lässt sich in zwei Arten unterteilen: Interne und externe Fragmentierung.\n\nInterne Fragmentierung definiert Bonwick als „per buffers wasted space“, also den Platz, der pro Slab verschwendet wird. Ungenutzte Lücken entstehen durch:\n\n\nDer Verschnitt am Ende ist kleiner als formula_3 der Slabgröße, er kann also durch die Größe des Slabs beeinflusst werden: Je größer der Slab, desto mehr Objekte enthält er und desto kleiner ist der Verschnitt. Des Weiteren wird dieser Verschnitt für das sogenannte Colouring verwendet, auf das weiter unten eingegangen wird.\n\nExterne Fragmentierung definiert Bonwick als „unused buffers in the freelist“, also ungenutzte Objekte in den Slabs. Wie bereits in der Tabelle weiter oben zu erkennen ist, existiert bei diesem Verfahren externe Fragmentierung. Sie ist jedoch relativ gering.\n\nDadurch, dass gleiche Objekte ähnliche Lebensdauern haben, entstehen wenige nur teilweise genutzte Slabs. Die meisten Slabs sind entweder vollständig oder gar nicht belegt. Die Ausnahme hierbei sind die Caches in bestimmten Größen, diese werden jedoch nicht so häufig verwendet.\n\nDes Weiteren wird der Speicher nicht weiter aufgeteilt, wie es andere Verfahren (z. B. das Buddy-Verfahren) tun. Denn je mehr Objekte ein Slab enthält, desto höher wird die externe Fragmentierung, da die Wahrscheinlichkeit, einen Slab vollkommen leer zu bekommen, sinkt.\n\nDas Colouring versucht, durch unterschiedliche Ausrichtung gleicher Objekte in verschiedenen Slabs die CPU-Caches besser zu nutzen.\n\nDazu wird der Verschnitt am Ende des Slabs genommen und Teile davon vor dem ersten Objekt im Slab eingefügt. Somit liegen die Objekte im Vergleich zu einem anderen Slab „versetzt“. Durch geschicktes Ausrichten an Vielfachen der Cachelinegröße kann dadurch ein gegenseitiges Verdrängen der Objekte aus dem Cache sowie der Adressen aus dem TLB vermindert werden.\n\nAus dem bisherigen ergibt sich der typische Ablauf einer Speicheranforderung:\n\n\nDer negative Einfluss auf die Caches und somit auf die Performance steigt dabei von Punkt zu Punkt.\n\nEin Slab besteht aus\n\n\nDie Verwaltung der freien Objekte verläuft im Linux-Kern über eine Liste von Ganzzahlen. Sie enthält eben so viele Zahlen wie Slab-Objekte. Jedes Zahlenfeld hat nur eine Bedeutung, wenn das entsprechende Objekt nicht benutzt wird, und gibt dann den Index des nächsten freien Objektes an. Durch die Information über das erste freie Objekt im Kopf des Slabs kann somit schnell das nächste ermittelt und zurückgegeben werden.\n\n\n"}
{"id": "496961", "url": "https://de.wikipedia.org/wiki?curid=496961", "title": "Acorn Archimedes", "text": "Acorn Archimedes\n\nDer Acorn Archimedes war eine von 1987 bis Mitte der 1990er Jahre produzierte Computerserie der britischen Firma Acorn.\nAcorn hatte 1983 mit der Entwicklung eines eigenen 32-bit-RISC-Prozessors begonnen, der \"Acorn RISC Machine\". Diese wurde erstmals im Archimedes serienmäßig eingesetzt, welcher dadurch zum ersten für eine breite Käuferschicht zugänglichen Computer mit einer RISC-CPU wurde. Der Prozessor zeichnete sich durch eine für damalige Homecomputer sehr hohe Geschwindigkeit aus.\n\nVon der Arbeitsgeschwindigkeit her übertraf der mit 8 MHz getaktete und damals etwa 3500 DM teure Archimedes die meisten anderen Computersysteme in der Preisklasse bis etwa 20.000 DM.\nDie ersten Archimedes-Modelle (A305 und A310) sowie der A3000 trugen auf der Tastatur neben dem Archimedes-Logo noch den Schriftzug „British Broadcasting Corporation Microcomputer System“ und hatten die für BBC-Computer typischen roten Funktionstasten.\n\nEin PC-Emulator ermöglichte es, PC-Programme laufen zu lassen, und zwar beinahe mit der Geschwindigkeit eines echten PCs.\n\nDer Acorn Archimedes beherrschte auch ohne Interlace höhere Auflösungen bis zu 1152 × 896 bei zwei Farben und 640 × 512 bei 256 Farben. Bei seinem Grafikchip (VIDC) ließen sich Auflösungen und Farbtiefen in weiten Grenzen beliebig programmieren, was jedoch einen entsprechend flexiblen Monitor voraussetzte (sehr beliebt war der Multisync II von NEC). Der Archimedes konnte aus insgesamt 4096 Farben wählen, von denen je nach Modus 2, 4, 16 oder 256 gleichzeitig dargestellt werden konnten.\n\nDer Archimedes beherrschte nur ein Hardware-Sprite (das in erster Linie als Mauszeiger verwendet wurde). Zudem hatte er keine Grafik-Spezialchips, so dass die Entwicklung von Spielen schwierig war. Trotzdem gab es auch beim Archimedes aufwendig gestaltete Spiele, darunter Umsetzungen von Amiga-Spielen, die ihren Originalen entsprachen.\n\nDas Betriebssystem RISC OS unterstützte bereits Vektorschriftarten und viele weitere Funktionen, so dass es für Programmierer relativ einfach war, diese in komplexen Anwendungen zu verwenden. Eindrucksvoll zeigt sich das beim im ROM des Archimedes enthaltenen Programm \"!Draw\", welches sich funktional mit den damaligen Versionen von Corel Draw messen konnte, zusätzlich aber schneller lief und vollständig im ROM des Rechners enthalten war.\n\nDer Archimedes bot, so wie bei heutiger Soundhardware üblich, lediglich einen DMA-Kanal und musste die Audiodaten der einzelnen Kanäle vor der Ausgabe per Software zusammensetzen. Die Ausgabe der Samples war dabei nicht linear realisiert, sondern logarithmisch umgesetzt, was dem menschlichen Gehör näher kommt als die heute üblicherweise genutzte lineare Umsetzung. Dadurch konnte mit den 8-Bit-Samples ein Dynamikumfang von ca. 12 Bit erzielt werden. Das Betriebssystem des Archimedes stellte 8 Stimmen zu Verfügung. 16-stimmige Tracker wie der \"!Coconizer\" waren verfügbar.\n\nDas Betriebssystem RISC OS war im ROM integriert. Neben der gesamten grafischen Oberfläche beinhaltete es bereits zahlreiche Zusatzprogramme wie \"!Paint\" (ein Bitmap-Zeichenprogramm), \"!Draw\" (ein vektororientiertes DTP-Programm), \"!Edit\" (ein Schreibprogramm). Die Programme des Archimedes starteten in der sogenannten Taskleiste, die Microsoft für Windows 95 lizenzierte. Enthalten waren auch der CLI (Commandline Interpreter), eine komplexe, Unix nachempfundene Betriebssystem-Shell, die Stapelverarbeitung ausführen konnte, und das BBC BASIC, das auch eine Inline-Assemblersprache beinhaltete.\n\n\"RISC OS 2\" erlaubte damit den Systemstart ohne Festplatte oder Disketten. Einstellungen wurden in einem batteriegepufferten CMOS-RAM gespeichert. \"RISC OS 3\" konnte ebenfalls ohne Disketten oder Festplatte gebootet werden, jedoch wurden Bootdisketten mitgeliefert, die diverse Erweiterungen nachgeladen haben.\n\nDas Betriebssystem bot kooperatives Multitasking. Kooperativ bedeutet, dass die Anwendungen vom RISC OS aufgerufen wurden und selbständig wieder einen Zyklus beendeten, so dass das RISC OS an die nächste Anwendung übergeben konnte. Der Speichercontroller MEMC des Archimedes konnte den gesamten Speicher umblenden (Memory Mapping), so dass jedes Programm scheinbar im gleichen Speicherbereich ausgeführt wurde und von den anderen Programmen nichts sah.\n\nDie Bedienung geschah fast ausschließlich mit Maus, die mit drei Tasten ausgestattet war. Mit der mittleren Maustaste wurden zu allen Objekten auf dem Desktop und in den Programmen Kontextmenüs geöffnet. Das Betriebssystem unterstützte durchgehend Drag and Drop.\n\nDer Taskmanager des RISC OS listete alle Programme und deren Ressourcenverbrauch auf.\n\nEin Nachteil von kooperativem Multitasking im Vergleich zu präemptivem Multitasking (wie z. B. beim Amiga, Windows NT oder Unix-Derivaten) ist, dass die Programme die Kontrolle selbständig wieder an das Betriebssystem übergeben müssen und somit ein Programmfehler dazu führen kann, dass das gesamte System hängt. Allerdings ist es zumindest bei späteren RiscOS-Versionen möglich, in diesem Fall das hängende Programm mit einer Tastenkombination zu beenden, so dass der Fehler nicht zu einem Absturz des gesamten Betriebssystems führt.\n\nDas geringe Auftreten von Problemen ist auch dadurch begründet, dass es keine Dynamic Link Libraries gab, sondern nur das einheitliche ROM mit den festen und vergleichsweise fehlerarmen Modulen. Ferner verhinderte der MEMC hardwaremäßig Fehler, bei denen Programme den Speicherbereich des Betriebssystems überschreiben; derartige Fehler können bei Systemen ohne vergleichbare Hardwareunterstützung prinzipiell zu einem Absturz des Systems führen.\n\nDie Defaulteinstellung für den Zeichensatz ist ISO 8859-1 mit proprietären Erweiterungen. Die Umstellung auf andere Zeichensätze ist möglich.\n\nDas im ROM eingebaute BBC BASIC unterstützte auch Inline-Assemblersprache. Die grafische Benutzeroberfläche des ersten Archimedes-Betriebssystems, ARTHUR, wurde noch teilweise in BBC BASIC programmiert. Das spätere Betriebssystem RISC OS wurde schließlich vollständig in Assembler geschrieben.\n\nFür die Betriebssystemaufrufe bot der ARM einen eigenen Assembler-Befehl, den SWI (Software Interrupt). So konnten Betriebssystemfunktionen aus dem Assembler in der Form z. B. SWI „OS_WriteC“ (für Write Character) aufgerufen werden. Als Parameter dienten hier die 16 Register des ARMs. Detaillierte Handbücher listeten alle SWIs von RISC OS auf und dokumentierten die Ein- und Ausgänge der Register und die Funktionen.\n\nEigene Betriebssystem-Aufrufe konnten über neue Module programmiert und so das Betriebssystem beliebig erweitert werden.\n\n\nDie nachfolgenden Modellen wurden von Acorn nicht als Archimedes bezeichnet, sind aber technisch weitestgehend mit dem Archimedes identisch:\n\nDie folgenden Modelle benutzen einen ARM250 als Hauptchip. Dabei handelt es sich um einen IC, der die Funktionen CPU (ARM) + Speichercontroller (MEMC) + Video (VIDC) + Ein-/Ausgabesteuerung (IOC) auf einem Stück Silizium vereint und somit hier im Prinzip den kompletten Rechner in einem Chip „bündelt“. Man spricht daher auch von einem System-on-a-Chip.\n\nDie 7000er sind eher formal eine Fortsetzung der Archimedes Geräte. Technisch entsprechen sie mehr dem Nachfolgegerät RiscPC, wobei hier dessen modularer Aufbau und Erweiterbarkeit fehlt. Zudem wird der Videochip durch fehlendes VideoRAM gebremst. Dafür gab es die 7000er auch mit eingebautem FPA (Floating Point Accelerator), d. h. mathematischem Co-Prozessor, was für bestimmte Anwendungen (Tabellenkalkulation etc.) höhere Rechengeschwindigkeiten als mit einem StrongARM RiscPC erlaubt. Auch hier ist wieder im Prinzip der komplette Computer in einem IC vereint.\n\nNachfolger waren die Risc PCs von Acorn.\n\n"}
{"id": "497302", "url": "https://de.wikipedia.org/wiki?curid=497302", "title": "Maxthon", "text": "Maxthon\n\nMaxthon (Eigenname \"MX5 Cloud Browser\") ist ein in China entwickelter Webbrowser, der anfangs vorrangig die Rendering-Engine \"Trident\" des Internet Explorer verwendete, dabei aber mehr Funktionen als dieser bot. Inzwischen benutzt Maxthon auch die Rendering-Engine WebKit. Soll nur diese Verwendung finden, können die Einstellungen entsprechend verändert werden.\n\nDie erste Version (0.1) wurde am 11. Juli 2002 unter der Bezeichnung \"MyIE2\" veröffentlicht und später, nach dem 18. März 2004, in \"Maxthon\" umbenannt und weiterentwickelt.\n\nIm Mai 2006 investierte das US-Unternehmen \"Charles River Ventures\" bis zu 5 Millionen US-Dollar in Maxthon.\n\nSeit dem Jahr 2014 ist Maxthon auch in einer Version für Linux verfügbar.\n\nMaxthon setzte ursprünglich auf Trident und ergänzt diese um Funktionen, welche der Internet Explorer bis zur Version 6 nicht bot, z. B. die Registernavigation. Neben dieser Funktion unterstützt der Webbrowser Mausgesten, verschiedene Designs, Pop-up-Blocker, Werbeblocker und Feedreader. Man konnte zwischen der Rendering-Engine Gecko oder Trident umschalten, die Google-Toolbar verwenden oder durch wenige Mausklicks die Verläufe, Cookies und temporären Internetdateien löschen, um somit seine Privatsphäre besser zu schützen.\n\nDie Mindestvoraussetzungen beim Betriebssystem sind folgende:\n\nMittlerweile ist auch eine portable Ausgabe für die Nutzung auf externen Speichermedien (zum Beispiel externen Festplatten, USB-Sticks oder Speicherkarten) für Windows erhältlich.\n\nMaxthon kann bei jedem Programmstart alle Registerkarten wiederherstellen, die beim letzten Programmende geöffnet waren. Laut Maxthon-Blog aus dem Jahr 2006 war Jeff Chen Urheber und CEO. Schon bis zum Jahr 2012 gab es weltweit mehr als 600 Millionen Downloads aller Maxthon-Versionen. Auf seiner Website erwähnt die Maxthon-Redaktion allein für die Windowsversion mehr als 300 Millionen Downloads.\n\n\n"}
{"id": "497670", "url": "https://de.wikipedia.org/wiki?curid=497670", "title": "ARM Limited", "text": "ARM Limited\n\nDas Unternehmen ARM Limited (früher Advanced RISC Machines Ltd.) ist ein zur japanischen Softbank gehörender Anbieter von IP-Lösungen im Bereich Mikroprozessoren. Es ist Entwickler der ARM-Architektur, die in Lizenz gebaut in den meisten Smartphones und Tabletcomputern zum Einsatz kommt und auch im Bereich der Eingebetteten Systeme sehr weit verbreitet ist.\n\nARM entwickelt das Design von RISC-Prozessoren, deren Fertigung von den Lizenznehmern durchgeführt wird, zu denen Firmen wie AMD, Apple, IBM, 3Com, Infineon, Freescale, Intel, Samsung, Qualcomm, Atmel, Toshiba, Renesas, NXP, Nvidia oder Texas Instruments gehören.\nWährend einige Unternehmen direkt Chipdesigns von ARM lizenzieren, benutzen andere lediglich eine sogenannte ARM-Architekturlizenz. Zu diesen Firmen gehören Apple, Samsung und Qualcomm.\nSiehe z. B. Apples A6 oder Qualcomms Snapdragon.\n\n\"Advanced RISC Machines Ltd.\" entstand 1989 mit der Auslagerung der Prozessorsparte aus dem Unternehmen Acorn. Acorn behielt 43 % Anteil an dem neuen Unternehmen, weitere 43 % hielt Apple, 7 % VLSI Technology (bis dahin einziger Produzent des ARM-Chipsatzes und danach erster Lizenznehmer von ARM) und die restlichen 7 % \"Nippon Investment and Finance\".\n\nMit der Trennung der Prozessorsparte vom Mutterkonzern wurde die vom Unternehmen entwickelte ARM-Architektur von \"Acorn RISC Machines\" in \"Advanced RISC Machines\" umbenannt.\n\n2004 wurde der US-amerikanische Konkurrent Artisan Components für 913 Millionen US-Dollar übernommen. 2005 wurde das deutsch-amerikanische Unternehmen Keil Software übernommen, ein Hersteller von IDEs für eingebettete Systeme.\n\n2016 – einen Monat nach dem Brexit-Votum im Vereinigten Königreich – gab der japanische Telekommunikationskonzern Softbank die Absicht bekannt, ARM für etwa 23,4 Milliarden Pfund (27,95 Milliarden Euro) zu kaufen. Die Übernahme wurde im September 2016 abgeschlossen. ARM soll als eigenständiges Unternehmen bestehen bleiben und personell aufgestockt werden.\n\n"}
{"id": "500899", "url": "https://de.wikipedia.org/wiki?curid=500899", "title": "Wau Holland Stiftung", "text": "Wau Holland Stiftung\n\nDie Wau Holland Stiftung (WHS) ist eine 2003 gegründete und 2004 als gemeinnützig anerkannte Stiftung mit Sitz in Hamburg, die den Nachlass des 2001 verstorbenen Hackers und Journalisten Wau Holland verwalten und der Öffentlichkeit zugänglich machen will. Planungen hierfür begannen schon Ende 2001. Mitbegründer und Mitglied im Stiftungsvorstandes ist u. a. Bernd Fix. Die Stiftung steht dem von Holland mitbegründeten Chaos Computer Club nahe und hat zum Ziel, sein Lebenswerk unter anderem auf den Gebieten der Technikfolgenabschätzung, der Technikgeschichte und der Informationsfreiheit fortzuführen. Konkret fördert sie den Einsatz elektronischer Medien zu Bildungszwecken sowie Veranstaltungen über die gesellschaftlichen Aspekte neuer Techniken.\n\nProjekte der Stiftung, jeweils in Zusammenarbeit mit dem Chaos Computer Club, sind ein „Archiv für Neue Technikgeschichte (Hackerarchiv)“, welches die Geschichte der Szene dokumentieren soll, sowie die Kampagne gegen Wahlcomputer. Des Weiteren nimmt die Stiftung in Europa Spenden für die Unterstützung der Website WikiLeaks an. Ab dem 3. Dezember 2010 wurde das PayPal-Konto der Wau Holland Stiftung – wie das Wikileaks-Konto selbst – wegen „illegaler Aktivitäten“ von PayPal gesperrt. Die Stiftung hatte damit zeitweilig keinen Zugriff auf verbleibende rund 10.000 Euro und kündigte am 7. Dezember rechtliche Schritte gegen PayPal an. PayPal gewährte kurz danach wieder den Zugriff auf die verbliebenen Spenden und wurde später auch wieder als Spendenmöglichkeit auf der Webseite der Stiftung angeboten. Für das Jahr 2010 wurden Spenden in Höhe von 1,33 Millionen Euro an Wikileaks weitergeleitet, im Jahr 2011 waren es 660.523 Euro.\n\nDas Finanzamt Kassel erkannte der Stiftung rückwirkend für das Jahr 2010 die Gemeinnützigkeit ab, da diese keine „ordnungsgemäße[n] Aufzeichnungen“ zum Nachweis der Überwachung der „weisungsgemäße[n] Verwendung der Mittel“ durch Wikileaks vorlegen konnte. Ein Einspruch gegen die Entscheidung wurde vom Finanzamt Hamburg-Nord als unbegründet zurückgewiesen. Ab dem Jahr 2011 wurde die Gemeinnützigkeit der Stiftung durch die Hamburger Behörde wieder anerkannt.\n\nDas verzinsliche angelegte Stiftungskapital beträgt gegenwärtig rund 62.000 Euro (Stand November 2012); weiterer Vermögensbestand ist eine kleinere landwirtschaftliche Fläche, die verpachtet ist.\n\nAuch in Berlin unterhält die Stiftung ein Büro.\n\n"}
{"id": "502512", "url": "https://de.wikipedia.org/wiki?curid=502512", "title": "Diskrete-Elemente-Methode", "text": "Diskrete-Elemente-Methode\n\nDer Begriff Diskrete-Elemente-Methode (engl. Discrete Element Method) oder (DEM) wird heutzutage für zwei numerische Berechnungsverfahren verwendet.\n\nDie häufigste Verwendung findet die von Cundall im Jahre 1971 entwickelte numerische Berechnungsmethode, mit der die Bewegung einer großen Zahl von Teilchen berechnet werden kann. Die Methode wird manchmal auch als \"Distinct Element Method\" bezeichnet. Ursprünglich diente sie für verschiedene Berechnungen der \"Molekulardynamik\" (MD). Seit ihrer Einführung hat sich ihr Einsatzgebiet ausgedehnt, wie z. B. auf die Simulation aus der Partikelverfahrenstechnik, der Geotechnik und des Maschinenbaus. Eine Erweiterung des Verfahrens ist die Erweiterte Diskrete-Elemente-Methode.\n\nAuf der anderen Seite wird der Begriff DEM auch für ein Stabgittermodell verwendet. Diese Betrachtungsweise – Abbildung eines Körpers durch Stäbe – geht auf Arbeiten von E. G. Kirsch aus dem Jahr 1868 zurück und wurde zum Beispiel von Felix Klein und Karl Wieghardt Anfang des 20. Jahrhunderts weiterentwickelt. Heute wird die Stabgittermethode unter anderem zur Simulation des Materialverhaltens von Verbundswerkstoffen insbesondere Gewebestrukturen eingesetzt. Darüber hinaus zeigte sich, dass die DEM sich zur Lebensdauerabschätzung von metallischen und keramischen Werkstoffen eignet. DEM wird auch zur Beschreibung des Materialverhaltens in der Geomechanik angewandt.\n\nDer folgende Text beschränkt sich auf die DEM nach Cundall, da sie derzeit eine höhere Relevanz in der Forschung erfährt.\n\nDie Grundannahme des Verfahrens beruht darauf, dass die zu berechnende Materie (Physik) sich aus einzelnen, abgeschlossenen Elementen zusammensetzt. Diese Elemente können unterschiedliche Formen und Eigenschaften haben. <br>\nAnwendung findet die Methode in folgenden Bereichen:\n\nBei einer DEM-Simulation werden alle Teilchen in einer bestimmten Startgeometrie positioniert und mit einer Anfangsgeschwindigkeit versehen. Aus diesen Anfangsdaten und den physikalischen Gesetzen, die für die Teilchen relevant sind, werden die Kräfte ausgerechnet, die auf jedes Teilchen wirken.\n\nKräfte, die hier in Frage kommen, sind zum Beispiel im makroskopischen Fall:\noder auf molekularer Ebene\n\nAlle diese Kräfte werden aufsummiert und danach mit Hilfe eines numerischen Integrationsverfahren aus der Newtonschen Bewegungsgleichung die Veränderung der Teilchengeschwindigkeit und -position berechnet, die sich in einem gewissen Zeitschritt ergibt. Danach werden mit den veränderten Positionen und Geschwindigkeiten erneut die Kräfte berechnet und diese Schleife so lange wiederholt, bis der Simulationszeitraum beendet ist.\n\nWenn langreichweitige Kräfte (typischerweise Gravitationskräfte oder elektrostatische Kräfte) berücksichtigt werden, so muss grundsätzlich die Wechselwirkung von jedem Teilchen mit allen anderen Teilchen berechnet werden. Die Zahl der Interaktionen und damit auch der Rechenaufwand steigt dann quadratisch mit der Zahl der Teilchen. Bei hohen Teilchenzahlen steigt damit die Rechenzeit inakzeptabel an. Eine Möglichkeit, dies zu vermeiden, besteht darin, mehrere Teilchen, die weit entfernt vom aktuellen Teilchen liegen, zu einem Pseudoteilchen zusammenzufassen und nur eine Interaktion zwischen dem aktuellen Teilchen und dem Pseudoteilchen zu berechnen. Als Beispiel kann die Interaktion zwischen einem Stern und einer weit entfernten Galaxie dienen: Der Fehler, der entsteht, wenn alle Sterne der entfernten Galaxie zu einem einzigen Massepunkt zusammengefasst werden, ist bei normalen Anforderungen vernachlässigbar. Um zu entscheiden, welche Teilchen zu Pseudoteilchen zusammengefasst werden können, werden sogenannte Baumverfahren angewendet. Dabei werden die Teilchen in einem hierarchischen Baum, im zweidimensionalen Fall einem Quadtree, im dreidimensionalen Fall einem Octree angeordnet.\nBei Molekulardynamik-Simulationen wird dagegen der Raum, in dem die Simulation stattfinden soll, in Simulationszellen eingeteilt. Sowohl die Kräfte als auch die Teilchen werden, wenn sie über den Rand der Zelle hinausgehen, einfach auf der anderen Seite der Zelle wieder eingefügt (Periodische Randbedingung). Um zu verhindern, dass ein Teilchen nun sowohl von der eigentlichen Kraft als auch von deren Spiegelbild auf der anderen Seite erfasst wird, wird diese Kraft ab der sogenannten Cutoff-Distanz (normalerweise die halbe Länge der Zelle) nicht mehr berücksichtigt. Um nun die Anzahl der beteiligten Teilchen zu erhöhen, wird einfach die Simulationszelle beliebig vervielfacht.\n\n\n\n\n\n\n"}
{"id": "503033", "url": "https://de.wikipedia.org/wiki?curid=503033", "title": "Smart Package Manager", "text": "Smart Package Manager\n\nDer Smart Package Manager ist eine freie Software, die bei vielen Linux-Distributionen die Verwaltung und Installation von Software-Paketen übernehmen kann.\n\nDa der Smart Package Manager auf dem jeweiligen distributionseigenen Format (rpm, deb, Slackware) und Mechanismus (yum, APT, Urpm) aufbaut, eignet er sich, um eine einheitliche Oberfläche für die Softwareverwaltung quer über viele Distributionen zu präsentieren, ohne die Konfiguration zu verändern.\n\nDer Smart Package Manager zeichnet sich durch eine komfortable Auflösung von Paket-Abhängigkeiten aus. Dies ermöglicht ihm ein effizienteres Vorgehen als viele andere Lösungen. Laut Eigenaussage der Entwickler arbeitet das Computerprogramm dabei besser als die vergleichbaren Programme Urpm, yum und APT. Dabei kann er auch auf Repositories zugreifen, die für andere Paketmanager bereitgestellt werden.\n\n"}
{"id": "503461", "url": "https://de.wikipedia.org/wiki?curid=503461", "title": "Software Update Services", "text": "Software Update Services\n\nSoftware Update Services (SUS) ist ein Programm von Microsoft zur Updateverwaltung in lokalen Netzwerken. Ist SUS auf einem Server installiert, muss er sich zuerst mit dem Microsoft Update-Server synchronisieren, ab dann hält er alle Updates lokal vor und bedient automatisch alle Clients.\n\nDie aktuelle Version wird von Microsoft als Windows Server Update Services (WSUS) bezeichnet (zuvor SUS 2.0; die Betaversion hieß noch WUS). Sie enthält neben dem gleichen Prinzip zahlreiche Neuerungen.\n"}
{"id": "503711", "url": "https://de.wikipedia.org/wiki?curid=503711", "title": "Wortnetz (Computerlinguistik)", "text": "Wortnetz (Computerlinguistik)\n\nEin Wortnetz ist eine Art von semantischem Netz. In einem Wortnetz werden lexikalische Zeichen und Konzepte durch semantische Relationen miteinander verbunden. Der Unterschied zu semantischen Netzen besteht darin,\nBei einem Wortnetz handelt es sich technisch um eine objektrelationale Datenbank.\n\nMitte der 1980er Jahre begann ein Team von Psycholinguisten an der Universität Princeton unter der Leitung von George A. Miller mit dem Aufbau eines Wortnetzes der englischen Sprache.\nDas Hauptmotiv für den Aufbau einer umfangreichen lexikalischen Ressource lag in der Erwartung, mit Hilfe dieser Daten Theorien über das mentale Lexikon umfassender und damit genauer testen zu können.\nDas Wortnetz sollte den Grundwortschatz des Englischen umfassen, sich aber auf die bedeutungstragenden Wortklassen Substantiv, Verb, Adjektiv und Adverb beschränken.\nDas Princeton WordNet hat sich damit zu einer lexikalischen Datenbank entwickelt, deren Struktur auf psycholinguistischen Erkenntnissen über den Aufbau des mentalen Lexikons basiert.\n\nRecht schnell wurde das Princeton WordNet als wertvolle lexikalische Ressource für Anwendungen der Computerlinguistik und Sprachtechnologie entdeckt. In der Folge wurden deshalb Wortnetze für zahlreiche weitere Sprachen aufgebaut. Eine Liste der Sprachen, für die Wortnetze existieren, findet sich auf den Seiten der \"Global Wordnet Association\" über die auch die Arbeit an den Wortnetzen koordiniert wird. Das deutsche Wortnetz \"GermaNet\" wird an der Eberhard-Karls-Universität Tübingen aufgebaut und gepflegt.\n\nDie elementare Beschreibungseinheit in einem Wortnetz ist das Konzept. Konzepte werden durch ein oder mehrere lexikalische Zeichen repräsentiert. Die bedeutungsgleichen bzw. synonymen lexikalischen Zeichen werden in \"synsets\" zusammengefasst. Beispiel:\n\nDie Bedeutung eines lexikalischen Zeichens kann somit aus dessen Zugehörigkeit zu einem Synset sowie durch den Platz dieses Synsets im Netz erschlossen werden. Einige Wortnetze, z. B. GermaNet, spezifizieren die Bedeutung eines Konzepts zusätzlich durch eine Glosse und durch Verwendungsbeispiele.\n\nDie Synsets sind durch die folgenden konzeptuellen Relationen verbunden:\n\n\nEinzelne lexikalische Einheiten sind durch folgende lexikalisch-semantischen Relationen verbunden:\n\n\nDie Relationen der Hyperonymie / Hyponymie und der Holonymie / Meronymie schaffen hierarchische Beziehungen zwischen den vernetzten Synsets, da beide Beziehungstypen transitiv sind. Die hierarchische Organisation von Hyperonym/Hyponym entspricht dem in der Informatik verwendeten Mechanismus der Vererbung: ein Hyponym erbt die semantischen Merkmale aller übergeordneten Konzepte und fügt diesen mindestens ein Merkmal hinzu. In den meisten Wortnetzen wird auch multiple Vererbung modelliert – ein Konzept kann das Hyponym mehrerer Konzepte sein, z. B. 'Banane' → 'Südfrucht', → 'Plantagenpflanze'.\n\nSynonymie, Antonymie und Pertonymie gruppieren die lexikalischen Einheiten.\n\nViele einsprachige Wortnetze sind über eine gemeinsame konzeptuelle Struktur, die durch einen \"Interlingual Index\" repräsentiert wird, miteinander vernetzt. So wird über das gemeinsame Konzept 'DRIVE' der deutsche Ausdruck \"fahren\" mit dem italienischen Ausdruck \"guidare\" und dem spanischen Ausdruck \"conducir\" vernetzt.\n\n\n\n"}
{"id": "504299", "url": "https://de.wikipedia.org/wiki?curid=504299", "title": "Ice Age", "text": "Ice Age\n\nIce Age (engl. für „Eiszeit“) ist ein US-amerikanischer Computeranimationsfilm von Blue Sky Studios aus dem Jahr 2002. Der Film der Regisseure Chris Wedge und Carlos Saldanha handelt vom gemeinsamen Abenteuer eines Mammuts, eines Riesenfaultiers und eines Säbelzahntigers zur Eiszeit. Nach dem finanziellen Erfolg des Films kamen die Fortsetzungen \"Ice Age 2 – Jetzt taut’s\" 2006, \"Ice Age 3 – Die Dinosaurier sind los\" 2009, \"Ice Age 4 – Voll verschoben\" 2012 und \"Ice Age – Kollision voraus!\" 2016 in die Kinos.\n\nDer Film spielt vor etwa 20.000 Jahren in der letzten Eiszeit, und der Winter steht vor der Tür. Deshalb macht sich die ganze eiszeitliche Tierwelt auf die Flucht in den Süden. Nur das einzelgängerische Mammut Manfred (Manni) widersetzt sich dem Gruppendruck und wandert nach Norden.\n\nRiesenfaultier und Nervensäge Sid wird wie jedes Jahr von seiner Familie allein zurückgelassen. Als er von zwei Brontotherien gejagt wird, denen er eine der letzten Löwenzahnblumen vor dem Winter weggefressen hat, wird er von Manfred gerettet. Fortan schließt er sich dem Mammut an, sehr zu dessen Leidwesen.\n\nUnterdessen überfällt ein Rudel Säbelzahntiger unter ihrem Anführer Soto eine Horde Steinzeitmenschen, um aus Rache für erlegte Artgenossen das Baby des Häuptlings zu entführen und zu töten. Doch dessen Mutter kann ihr Kind gerade noch rechtzeitig vor Säbelzahntiger Diego in Sicherheit bringen und mit dem Säugling fliehen. Diego setzt nach.\n\nZufällig entdecken Manni und Sid die Frau, die, von Diego in die Enge getrieben, einen Wasserfall hinunter gesprungen war und nun dem Ertrinken nahe ist. Sie reicht den zwei verdutzten Tieren ihren kleinen Sohn an Land, bevor sie vom Wasserstrom wieder fortgerissen wird.\n\nDiego bekommt von seinem Rudelführer Soto den Auftrag, das Menschenkind zum Fressen zurückzuholen. Unter dem Vorwand, Manfred und Sid helfen zu wollen, das Kind den Artgenossen wiederzugeben, versucht Diego, sich ihnen anzuschließen. Tatsächlich will er sie einem Hinterhalt zuführen, wo seine Kameraden das Kind in Empfang nehmen wollen. Schließlich wird er trotz Mannis Zweifel als Fährtensucher von den sich uneinigen Manfred und Sid aufgenommen.\n\nAuf dem Weg zurück zu den Menschen müssen die drei ungleichen Tiere verschiedene Abenteuer überstehen – sich so beispielsweise gegen die Brontotherien oder Dodos wehren (diese rotten sich durch ungeschicktes und selbstüberschätzendes Verhalten sowie Futterneid selbst aus) und den Tücken von Kälte und Eis, Lava und Feuer trotzen. Dabei müssen Manfred, Sid und Diego Vertrauen zueinander aufbauen, was sich in einer Welt voller Jäger und Gejagter als nicht so einfach herausstellt. Auch die Aussicht auf das Zusammentreffen mit den Menschen, welche offenbar – wie sie es bei Höhlenzeichnungen gesehen haben – extra Waffen zum Töten der Tiere erfunden haben, vereinfacht ihre Situation nicht.\n\nKurz vor dem geplanten Hinterhalt, nachdem Manni ihm zuvor unter Einsatz des eigenen Lebens das Leben gerettet hatte, gesteht Diego seine ursprünglichen verräterischen Absichten und schlägt sich auf die Seite seiner Gefährten. Gemeinsam trotzen sie erfolgreich dem Angriff der anderen Säbelzahntiger. Soto wird beim Versuch, sich das Kind zu greifen, von Manni gegen einen Felsen mit Eiszapfen geworfen, woraufhin diese abstürzen und Soto erstechen.\n\nAm Ende wird das Kind dem ausgiebig nach seiner Familie suchenden Vater zurückgegeben. Die Gefährten Manfred, Diego und Sid ziehen gemeinsam nach Süden.\n\nDer Film war ursprünglich von 20th Century Fox als Zeichentrickfilm geplant. Aufgrund der kommerziellen Flops der mit traditioneller Zeichentricktechnik hergestellten Filme \"Anastasia\" und \"Titan A.E.\" sowie der Erfolge der computeranimierten Pixar-Produktionen wurde \"Ice Age\" dann aber von Blue Sky Studios komplett computeranimiert umgesetzt. Hierfür wurde die Zahl der Mitarbeiter im Produktionsstudio kurzfristig von rund 65 auf mehr als 170 erhöht. Diese arbeiteten fast zwei Jahre an der Umsetzung, bevor einem Großteil wieder gekündigt wurde. Eine Vielzahl verschiedener Trailer und weiterer Werbemaßnahmen für Internet und Kino steigerten die Gesamtproduktionskosten auf 95 Millionen US-Dollar.\n\nBei Scrat handelt es sich um den Vertreter einer fiktiven Spezies, eines „Rattenhörnchens“, eine Mischung von Eichhörnchen und Ratte mit den Zähnen eines Säbelzahntigers, mit großen Augen und buschigem Schwanz. Der Name leitet sich aus den englischen Namen der Gattungen Eichhörnchen (\"squirrel\") und Ratte (\"rat\") ab. Obwohl es sich bei Scrat um ein Fantasiegeschöpf handelt, wurde 2011 ein argentinisches Säugetierfossil präsentiert, das Scrat sehr ähnlich sieht. Das „Säbelzahneichhörnchen“ \"Cronopio dentiacutus\" lebte jedoch nicht in der letzten Eiszeit, sondern vor etwa 100 Millionen Jahren.\n\nUrsprünglich war Scrat nur für den Trailer des Films \"Ice Age\" entworfen worden. Die Publikumsreaktionen machten jedoch schnell deutlich, dass von diesem Tier mehr zu sehen sein sollte. Daraufhin begannen die Designer, einige Szenen mit Scrat zu entwerfen. Schnell wurde jedoch klar, dass es keine Möglichkeiten gab, Scrat in den bereits in der Post-Produktion befindlichen Film als volle Figur zu integrieren. Daher laufen die Szenen, in denen Scrat auftaucht und versucht, eine Eichel zu vergraben, parallel zur Handlung des Films und schneiden sich nur an zwei Stellen, wo er jedoch kaum Einfluss auf die Handlung des Films nimmt.\n\nDennoch bestreitet er die erste und die letzte Szene des Films. In diesem Epilog ist zu sehen, wie ein Eisblock 20.000 Jahre später (also heute) an einer tropischen Insel angeschwemmt wird. In ihm sind eine Eichel und der bei deren Verfolgung eingefrorene Scrat eingeschlossen. Obwohl er – weil festgefroren – es nicht schafft, die Eichel zu retten, bevor sie weggespült wird, wird er doch mit einer riesigen Kokosnuss belohnt, welche er gleich tollpatschigerweise dazu nutzt, die gesamte Insel zu spalten und den darauf befindlichen Vulkan zum Ausbruch zu bringen.\n\nAuf der DVD zum Film haben die Produzenten Scrat noch einmal bedacht. Es wird als Bonusmaterial neben einigen Trailern mit Scrat auch der Kurzfilm \"Scrats neue Abenteuer\" präsentiert – ganz davon abgesehen, dass auch das Bedienungsmenü mit Scrat und seinen Missgeschicken animiert ist.\n\nIm Kurzfilm \"Scrats neue Abenteuer\" erfährt der Zuschauer neue Erkenntnisse über die Kontinentaldrift. Scrat hat in einem hohlen Baumstumpf einen unermesslich großen Vorrat an Eicheln angelegt. Bei dem Versuch, eine weitere Eichel in die Mitte dieser Sammlung einzufügen, kippt das Ganze. Alle Nüsse – und Scrat – fallen einen tiefen Abgrund hinunter. Durch den Aufprall der letzten Eichel wird schließlich die Kontinentaldrift ausgelöst. Schlussendlich steht Scrat ohne eine einzige Eichel da.\nScrat wird von Regisseur Chris Wedge gesprochen.\n\n\n\nDie deutsche Synchronisation entstand nach einem Dialogbuch von Michael Nowka unter seiner Dialogregie im Auftrag der Berliner Synchron AG.\nDie Kritikermeinungen waren größtenteils positiv. Auch die Animationen waren technisch gesehen auf dem Stand der Zeit. Im Vergleich zu Veröffentlichungen aus dem konkurrierenden Hause Pixar wurden die Figuren jedoch eckiger und weniger nach dem Kindchenschema entworfen.\n\nDie FSK-Einschätzung „Freigegeben ohne Altersbeschränkung“ wird teilweise kritisiert, da beispielsweise der gewaltsame Tod der Mutter des Menschenbabys für einen Kinderfilm eher untypisch ist.\n\nDer Film war trotz verschieden lautender Kritiken ein großer Publikumserfolg. Ice Age brachte dem Produktionsteam am ersten Wochenende den bis dahin drittbesten Start eines animierten Kinofilms in den USA ein und spielte weltweit 383,3 Millionen US-Dollar ein.\n\n2003 erhielt der Film eine Oscarnominierung als „bester animierter Film“ sowie je eine Nominierung für den Saturn Award, den Golden Satellite Award und den Young Artist Award in der gleichen Kategorie.\n\nDer Film war so erfolgreich, dass im Frühjahr 2006 eine Fortsetzung, \"Ice Age 2 – Jetzt taut’s\", in die Kinos kam. Am 1. Juli 2009 startete der dritte Teil \"Ice Age 3 – Die Dinosaurier sind los\" im Kino. Der vierte Teil startete am 2. Juli 2012 mit dem Titel \"Ice Age 4 – Voll verschoben\" in den Kinos.\n\"Ice Age – Kollision voraus!\", im Original \"Ice Age: Collision Course\", startete in Deutschland am 30. Juni 2016.\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh dem Film das Prädikat besonders wertvoll.\n\n"}
{"id": "505015", "url": "https://de.wikipedia.org/wiki?curid=505015", "title": "Z22", "text": "Z22\n\nDer Z22-Computer oder kurz Z22 war ein ab 1955 von dem Physiker Lorenz Hanewinkel konstruierter und für die Zuse KG gebauter Computer. Er war der erste Röhrenrechner aus Westdeutschland. In der DDR wurde der vergleichbare D1 von 1950 bis 1956 entwickelt. Als einer der ersten in Serie produzierten Rechner weltweit ermöglichte Z22 deutschen Hochschulen, Universitäten und anderen wissenschaftlichen Instituten nach dem Zweiten Weltkrieg erstmals eine elektronische Datenverarbeitung.\n\nNach den Modellen Z1, Z2, Z3, Z4, Z5 und Z11 war Z22 das siebte Computer-Modell, das unter Konrad Zuse entwickelt wurde. Wie die Vorgänger ab Z4 wurde die Maschine Z22 bereits kommerziell vertrieben, eine im Jahre 1950 in die Schweiz vermietete Z4 war wahrscheinlich der erste jemals kommerziell gehandelte Computer.\n\nDie Entwicklung der Z22 war ca. 1957 abgeschlossen, die ersten der insgesamt 55 Exemplare wurden ab 1958 an die TU Berlin und nach Aachen verkauft. Zu den ersten Abnehmern gehörte auch die Firma Zeiss, welche fortan einen Rechner für ihre optischen Berechnungen besaß. Theodor Fromme, wissenschaftlicher Leiter bei Zuse und ehemaliger Mitarbeiter bei Zeiss, war maßgeblich an der Ausarbeitung der Schaltpläne für diese Rechenmaschine beteiligt.\n\nDie Diebold-Computer-Statistik weist für den 1. Juli 1971 insgesamt 48 Exemplare der Z22 aus.\n\nIn der Standard-Ausführung war die Z22 wie folgt ausgestattet:\n\n\nDie Taktfrequenz der Z22 betrug 3 kHz, das entsprach exakt der Geschwindigkeit des Trommelspeichers.\nDie Eingabe von Daten konnte sowohl über den Lochstreifenleser als auch über eine Direkteingabe von Daten an der Trommelspeichereinheit als auch über Taster zur Direktprogrammierung des Kernspeichers erfolgen.\n\nDie relativ schnelle Ausgabe von Daten konnte über einen Lochstreifenstanzer erfolgen, ein relativ schnelles Einlesen über einen opto-elektrischen Lochstreifenleser. Einige Daten konnten auch über die im Bedienpult eingebauten Glimmlampen angezeigt werden, die wichtige Registerinhalte darstellten.\n\nAls kombiniertes Ein-Ausgabe-Gerät wurden Siemens-Fernschreiber T100 mit angebautem 5-Kanal-Lochstreifen-Leser und -Stanzer benutzt. Damit waren vier „Peripheriegeräte“ in einer Maschine verfügbar: Tastatur-Eingabe, Lochstreifen-Eingabe, Blattschreiber-Ausgabe und Lochstreifen-Ausgabe. Deren Arbeitsgeschwindigkeit betrug zehn Zeichen/Sekunde und war deutlich langsamer als die speziellen Monogeräte Lochkartenstanzer und -leser.\n\nDie Z22 wurde mit dem Ziel entwickelt, einfacher programmierbar zu sein als die Computer der Vorgängergeneration. Er wurde in Maschinencode programmiert; jede Instruktion war 38 Bit breit und in fünf Felder fester Länge aufgeteilt:\n\n\nUm die Programmierung weiter zu vereinfachen, wurde eine Assembler-ähnliche Sprache namens „Freiburger Code“ entwickelt. Wesentliche Elemente dieses Codes waren die Torschaltbits. Jedes dieser Bits schaltete ein Tor von oder zu der im Adressteil des Befehls angesprochenen (Trommel)speicher Zelle von respektive zu dem adressierten Register über eine Schaltkaskade. Jedes der Bits im Befehlsteil sprach ein Tor an – davon existierten je eines für die Auswertung einer Bedingung:\n\n\nDie Operationstore führten zu einer Schaltung\n\n\nDer in späteren Assembler übliche Ladebefehl hatte im Freiburger Code die Codierung:\n\n\nDiese Sprache wurde mit dem Ziel entwickelt, die Implementierung mathematischer Algorithmen zu vereinfachen; dieses Ziel wurde in der Praxis auch erreicht.\n\nDie Hochschule Karlsruhe besitzt ein restauriertes und voll funktionsfähiges Exemplar mit der Seriennummer 13, das 1958 erbaut wurde. Diese Maschine ist dem Zentrum für Kunst und Medientechnologie (ZKM) als Dauerleihgabe am 9. März 2005 übergeben worden. Sie wurde von den beiden Zuse-Experten Hans Baumann und Helmut Kammerer auseinandergenommen und im ZKM wieder aufgebaut. Jener Z22/13 ist der älteste noch funktionierende, originalgetreue Röhrenrechner der Welt und steht unter Denkmalschutz. Eine weitere, voll funktionstüchtige Z22R steht auf dem Campus der Fachhochschule Suderburg (Teil der Ostfalia Hochschule für angewandte Wissenschaften).\n\nDas Konrad-Zuse-Computermuseum in Hoyerswerda (Sachsen) besitzt zwei – allerdings nicht mehr funktionierende – Exemplare der Z22. Ein weiteres für Prof. Hubert Cremer an der RWTH Aachen gebautes Exemplar befand sich im ehemaligen Computermuseum Aachen. Auch dieses ist nicht mehr funktionstüchtig.\n\nIm Kopfgebäude der Universität Linz befindet sich vor den Hörsälen HS9&10 ebenfalls ein nicht mehr funktionierendes Exemplar. Eine weitere, nicht mehr funktionsfähige Z22 befindet sich im Technikmuseum Berlin.\n\nIm Computermuseum der Fachhochschule Kiel steht eine nicht mehr betriebsbereite Anlage, die im Rahmen der normalen Öffnungszeiten des Museums besichtigt werden kann.\n\nEine Z22 befindet sich in der Informatikabteilung des Deutschen Museums in München.\n\nEine Z22R befindet sich auch im Museum wortreich in Bad Hersfeld.\n\nEine restaurierte, lauffähige Z23 aus dem Jahr 1962 gehört zur Informatik-Sammlung Erlangen ISER der Friedrich-Alexander-Universität Erlangen-Nürnberg. Es ist die vermutlich einzige, lauffähige Z23 weltweit.\n\n\n\n"}
{"id": "505373", "url": "https://de.wikipedia.org/wiki?curid=505373", "title": ".nfo", "text": ".nfo\n\nDie Dateinamenserweiterung .nfo wird unter anderem in der Warez- und Demoszene für reine Textdateien (sogenannte NFO-Dateien) verwendet, die Informationen über gemeinsam damit verteilte Veröffentlichungen (wie Demos bzw. Schwarzkopien) enthalten. codice_1 ist dabei eine Abkürzung für „Info“, die aufgrund der 8.3-Beschränkung vieler Dateisysteme zustande kommt.\n\nWeitere Programme, die codice_1 als Dateinamenserweiterung nutzen, sind u. a. Systeminformationen unter Microsoft Windows, das Windows-Programm , das Computerspiel Age of Empires sowie die Multimediacenter-Distribution Kodi (vormals XBMC).\n\nDie erste NFO-Datei wurde von der Warez-Gruppe ' (kurz THG) für das 1990 von ihnen gecrackte Computerspiel ' genutzt. codice_3 ersetzte dabei die bis dahin genutzten Readme-Dateien.\n\nWar zuanfangs noch der Name der Software verwendet worden, so setzte sich nach kurzer Zeit der Name der als Dateiname durch. Diese Gruppe zeichnete sich für den Inhalt des veröffentlichten Pakets (Software wie Computerprogramme oder -spiele, aber auch Filme und ähnliches) verantwortlich. Stammt die Veröffentlichung beispielsweise von THG, so wurde codice_4 als Dateiname verwendet. In der Datei fand sich dann ein einheitliches der Gruppe zugeordnetes ASCII-Art-Logo.\n\nDas Vorhandensein einer NFO-Datei war in der Warez-Szene wie ein Authentizitäts-Zertifikat, das eine gültige Veröffentlichung bescheinigte. Die meisten Hacker-Gruppen konkurrierten um die erste gecrackte Veröffentlichung einer Software, gleichzeitig bestimmte aber auch die Qualität der Arbeit den Ruf der Gruppe. Viele Hacker-Gruppen testeten daher ihren Crack ausgiebig, um sicherzugehen, dass das Umgehen eines Kopierschutzes keine Beeinträchtigung der Software verursachte. Insofern ist die NFO-Datei und die Gruppe dahinter auch eine Art Qualitätsmerkmal.\n\nIn der Datei wurden neben Informationen zur Veröffentlichung, , auch Grüße an andere Hacker oder Gruppen, Kommentare zu deren Veröffentlichung oder zu deren Arbeit, oft in Form von Lob oder Tadel, angeführt. Darüber hinaus gab die Gruppe Kontaktinformationen bekannt, um Mitglieder zu rekrutieren.\n\nBei den Informationen handelte es sich meist um Installationsanweisungen für die gecrackte Software, oft inklusive einer dafür benötigten Seriennummer.\n\nDateien mit der Dateinamenserweiterung codice_5 sind meist generische ASCII-Textdateien, die mit jedem Texteditor betrachtet werden können. Allerdings enthalten die meisten NFO-Dateien zusätzlich ASCII-Art, die ASCII-Erweiterungen und unterschiedliche Zeichensätze benötigen und unter MS-DOS und dazu kompatiblen DOS-Betriebssystemen mittels Zeichensatztabellen (, abgekürzt „CP“) erstellt wurden. Übliche Zeichensatztabellen sind beispielsweise CP850 für Westeuropa und CP866 für Russland. Später wurde auch ANSI-Art verwendet, wofür unter MS-DOS jedoch ein ANSI-Treiber, z. B. ANSI.SYS, zur Verarbeitung der verwendeten ANSI-Escapesequenzen benötigt wird.\n\nUm ASCII-Art und damit eine NFO-Datei richtig darstellen zu können, ist neben dem korrekten Zeichensatz () auch die Verwendung von nichtproportionalen Schriften Pflicht, damit sich die Textgrafik nicht verschiebt. Neben aus der Warez-, Demo- und Shareware-Szene erhältlichen speziellen Anzeigeprogrammen, , können auf modernen Betriebssystemen viele erweiterte Texteditoren mit angepassten Einstellungen eine NFO-Datei korrekt darstellen. Ein Beispiel dafür ist das quelloffene und freie Windows-Programm Notepad++.\n\nFür die Darstellung auf Internetseiten gibt es ebenfalls Entwicklungen, die den Text inklusive ASCII-Art korrekt z. B. in eine PNG-Bilddatei umwandeln können.\n\nAls einfache Textdatei kann der Inhalt in einem entsprechenden Texteditor bearbeitet werden, was bei ASCII-Art entsprechend aufwändiger ist.\n\nIn der Warez-, Demo- und Shareware-Szene gibt es diverse Programme, die das Erstellen von NFO-Dateien mit ASCII-Art erleichtern sollen. Die selbst haben meist eigene Programme, die die schnelle Erstellung einer NFO-Datei erheblich erleichtern und automatisieren.\n\nNeben der NFO-Datei als reine Textdatei gibt es einige Programme, die ebenfalls diese Dateinamenserweiterung nutzen. Diese enthalten inkompatible, meist binäre Daten und werden von spezifischen Programmen genutzt.\n\n\n\n"}
{"id": "506785", "url": "https://de.wikipedia.org/wiki?curid=506785", "title": "PdfTeX", "text": "PdfTeX\n\npdfTeX ist eine Erweiterung des Textsatzprogramms TeX, mit der aus TeX-, LaTeX- und ConTeXt-Quellen unmittelbar PDF-Dateien erzeugt werden können. Es wurde ursprünglich von Hàn Thế Thành im Rahmen seines Promotionsprojekts an der Masaryk-Universität Brünn entwickelt.\n\nDie Bedeutung von pdfTeX ergibt sich vor allem daraus, dass PDF zunehmend die „alten“ TeX-Ausgabeformate DVI und PostScript verdrängt hat.\n\nPdfTeX ist in den neueren TeX-Distributionen (TeX Live und das darauf aufbauende MacTeX, MiKTeX) enthalten und wird dort als Standard-TeX-Engine verwendet. Die Aufnahme von pdfTeX in die TeX-Distribution teTeX 1.0 war für die Verbreitung des Systems von großer Bedeutung.\n\nDie Idee zu dieser TeX-Erweiterung entstand Anfang der 1990er Jahre, als Jiří Zlatuška und Philip Taylor ihre Vorstellungen zur weiteren Entwicklung von TeX mit Donald E. Knuth in Stanford diskutierten. Als Knuth später an die Universität Brünn kam, um einen Ehrendoktortitel von der dortigen Informatik-Fakultät entgegenzunehmen, kam es zu weiteren ermutigenden Gesprächen, diesmal auch mit Hàn Thế Thành. Weitere wichtige Beiträge zur Entwicklung von pdfTeX stammen von Pavel Janík, Heiko Oberdiek, Jiří Osoba, Ricardo Sanchez Carmenes, Robert Schlicht und Martin Schröder.\n\nPdfTeX basiert auf den TeX-Quellen und auf Web2c. Ab Version 1.40 sind auch die ε-TeX-Quellen in pdfTeX enthalten.\n\nDie Entwicklung von pdfTeX ist so gut wie abgeschlossen. Bis zum Release der Version 1.50.0 sollen nur noch Fehler bereinigt werden. Als Nachfolger von pdfTeX gilt LuaTeX.\n\nDer Hauptunterschied zwischen TeX und pdfTeX besteht in der Erzeugung von PDF-Dateien. Das Ausgabeformat von TeX ist DVI. Um hieraus PDF-Dateien für die Druckvorstufe zu erzeugen, muss eine weitere Bearbeitung durch einen Treiber erfolgen. Dieser Schritt entfällt bei der Verwendung von pdfTeX, weil damit das PDF direkt erzeugt werden kann.\n\nHierdurch ist es möglich, PDF-spezifische Features wie Hypertextverweise und ein Inhaltsverzeichnis durch LaTeX-Zusatzpakete wie vor allem \"hyperref\" unmittelbar zu erzeugen. Auch PDF-Formulare können erzeugt werden.\n\nIm Gegenzug funktionieren Pakete, die auf der Konvertierung von DVI zu PostScript aufsetzen, mit pdfTeX nicht (vor allem PSTricks). Das gilt insbesondere auch für das Einbetten von PostScript-Grafiken. Diese müssen zuvor in ein Format umgesetzt werden, das von pdfTeX unmittelbar verarbeitet werden kann (PNG, JPEG, JBIG2 oder PDF). Als Alternative zur Erzeugung von Grafiken mit LaTeX wurde das System PGF/TikZ entwickelt.\n\nWeil pdfTeX die TeX-Quellen enthält, ist es auch möglich, eine DVI-Ausgabe unmittelbar mit pdfTeX zu erzeugen. Sie wird mit derjenigen von TeX identisch sein, sofern die mikrotypographischen Erweiterungen von pdfTeX nicht verwendet worden sind.\n\nDie beiden wichtigsten Fähigkeiten von pdfTeX liegen im mikrotypographischen Bereich. Zum einen wurde das sogenannte \"protruding\" implementiert (der optische Randausgleich, auch \"margin kerning\" oder \"hanging punctuation\" genannt), zum anderen gibt es die sogenannte Schriftstärkenveränderung \"(font expansion),\" womit Arbeiten von Hermann Zapf umgesetzt werden, deren Ziel es war, einen einheitlichen Grauwert der Druckseite zu bewirken. Für LaTeX erleichtert das Paket \"microtype,\" das auch das Nachfolgersystem LuaTeX unterstützt, den Zugriff auf diese Features.\n\nAußerdem bietet pdfTeX eine native Unterstützung von TrueType- und Type-1-Schriften, die unmittelbar in PDF-Dateien eingebettet werden können. OpenType-Schriften können dagegen nur eingeschränkt verwendet werden. Deshalb wurden Nachfolgersysteme wie LuaTeX und XeTeX entwickelt, die einen rein unicode-basierten Produktionsprozess für LaTeX-Quellen bieten.\n\nUm LaTeX-Quelltexte zu verarbeiten, kann unmittelbar das Programm \"pdflatex\" aufgerufen werden. Für ConTeXt-Dokumente wird pdfTeX beim Aufruf von \"texexec\" standardmäßig verwendet.\n\n\n"}
{"id": "508025", "url": "https://de.wikipedia.org/wiki?curid=508025", "title": "Scanline-Algorithmus", "text": "Scanline-Algorithmus\n\nAls Scanline-Algorithmus bzw. Bildzeilenalgorithmus wird in der Computergrafik ein Bildzeile für Bildzeile (englisch \"scan line\") arbeitendes Verfahren zur Verdeckungsberechnung von aus Polygonen aufgebauten 3D-Szenen bezeichnet. Der gesamte Darstellungsprozess wird auch Scanline-Rendering bzw. Bildzeilenrenderung genannt. Scanline-Algorithmen nutzen die Tatsache aus, dass durch die Zeile für Zeile erfolgende Arbeitsweise das Problem der Verdeckungsberechnung von drei auf zwei Dimensionen reduziert wird. Die ersten Scanline-Algorithmen wurden Ende der 1960er Jahre veröffentlicht.\n\nScanline-Algorithmen basieren auf dem Kantenlisten-Algorithmus mit aktiver Kantenliste zum Füllen von Polygonen (siehe Rastern von Polygonen), der manchmal ebenfalls \"Scanline-Algorithmus\" genannt wird. Dieser Algorithmus wird dahingehend erweitert, dass er auch mehrere Polygone auf einmal rastern kann und in Zweifelsfällen entscheidet, welches Polygon vom Betrachter aus sichtbar ist.\n\nDer Scanline-Algorithmus zum Rendern von Polygonen verwendet eine \"Kantentabelle,\" die einen Eintrag für jede Kante eines Polygons enthält. Horizontale Kanten werden ignoriert. Jeder Eintrag der Kantentabelle enthält folgende Informationen:\n\nZusätzlich ist eine \"Polygontabelle\" erforderlich, die für jedes Polygon, zusätzlich zu dessen Nummer, zumindest folgende Informationen enthält:\n\nWeiterhin ist wie beim einfachen Algorithmus zum Füllen von Polygonen eine \"aktive Kantentabelle\" (Tabelle der aktiven Kanten) erforderlich, die alle Kanten enthält, die die aktuell verarbeitete Bildzeile schneiden. Für die im Beispiel angegebenen Bildzeilen enthält die aktive Kantentabelle folgende Werte:\n\nDie Bildzeilen werden von links nach rechts abgearbeitet. Bei der Bildzeile c verfährt der Algorithmus folgendermaßen: Sobald die Kante AB erreicht wird, wird das In-out-Flag des entsprechenden Polygons gesetzt; der Algorithmus ist jetzt „in“ diesem Polygon, und Pixel werden entsprechend der mit dem Polygon verknüpften Shading-Informationen eingefärbt. Sobald die Kante AC erreicht ist, wird das Flag wieder auf \"falsch\" gesetzt. Da die Kante AC die letzte in der aktiven Kantentabelle ist, ist der Vorgang für diese Bildzeile beendet. Bei der Bildzeile b sind zwei Polygone beteiligt, da sie aber hier einander nicht überlappen, betrachtet der Algorithmus genau wie im vorherigen Fall stets nur maximal ein Polygon pro Pixel.\n\nBei der Bildzeile a ist mehr Aufwand erforderlich. Sobald das Dreieck ABC erreicht wird, setzt der Algorithmus das entsprechende Flag in der Polygontabelle. Die Shading-Informationen dieses Polygons werden verwendet, bis die Kante DE erreicht wird. Hier wird nun auch das Flag für das Dreieck DEF gesetzt, der Algorithmus ist jetzt also „in“ zwei Polygonen gleichzeitig. Es muss nun entschieden werden, welches dieser beiden Polygone dem Betrachter näher liegt. Dies geschieht, indem mit Hilfe der gespeicherten Ebenengleichungs-Koeffizienten die \"z\"-Koordinate des Polygons an diesem Punkt berechnet wird. In diesem Beispiel hat ABC die größere \"z\"-Koordinate, also ist es nicht sichtbar. Wenn man annimmt, dass die beiden Polygone einander nicht schneiden, so werden demzufolge die Shading-Parameter des Dreiecks DEF verwendet. Wenn die nächste Kante CB erreicht ist, wird das Flag des Polygons ABC auf \"falsch\" gesetzt und der Algorithmus befindet sich nur noch im Polygon DEF, dessen Shading-Parameter weiterhin verwendet werden.\n\nDer grundlegende Scanline-Algorithmus berechnet die Tiefe der überlappenden Polygone an jedem Pixel. Die Anzahl dieser Berechnungen kann durch das Einteilen der Bildzeile in einzelne Bereiche („Spans“), für die jeweils nur eine Berechnung durchgeführt wird, reduziert werden. Derartige Verfahren, zu denen auch der von Watkins 1970 veröffentlichte Algorithmus gehört, werden \"Spanning-Scanline-Algorithmen\" genannt.\n\nScanline-Algorithmen können auch mit dem Z-Buffer kombiniert werden. Dabei wird ein nur eine Bildzeile umfassender Z-Buffer, ein sogenannter \"Scanline-Z-Buffer,\" für die aktuelle Bildzeile verwendet. Er bietet sich an, wenn nicht ausreichend Speicher für einen vollständigen Z-Buffer vorhanden ist.\n\nGegenüber dem Z-Buffer haben Spanning-Scanline-Algorithmen den Vorteil, dass Shading-Berechnungen nur einmal pro Pixel durchgeführt werden müssen. Allerdings beginnen und enden Spans nicht unbedingt an den Schnittpunkten mit den Kanten des sichtbaren Polygons. Dies erschwert die inkrementelle (schrittweise) Shading-Berechnung, da die Startwerte an einem beliebigen Punkt des Polygons berechnet werden müssen. Der Z-Buffer ist ab einer bestimmten Anzahl von Polygonen effizienter als Spanning-Scanline-Algorithmen, weshalb er für die heute üblichen komplexen Szenen meist vorgezogen wird.\n\n"}
{"id": "508951", "url": "https://de.wikipedia.org/wiki?curid=508951", "title": "Hdparm", "text": "Hdparm\n\nhdparm ist ein Computerprogramm zum Lesen und Setzen von Parametern für ATA-Laufwerke unter Linux und Windows. hdparm steht unter der BSD-Lizenz und ist somit freie Software.\n\nMit hdparm können unter anderem die Einstellungen für DMA und Tagged Command Queuing und der ATAPI-Transfermodus geändert werden. In vielen Fällen kann damit bei EIDE-Festplatten sowie CD-ROM- und DVD-Laufwerken die Geschwindigkeit deutlich erhöht werden, aber hdparm-Befehle können auch zu Datenverlust und sogar zu Beschädigung von Festplatten und Controllern führen. Z. B. sollte ein Verwechseln der Optionen -w mit -W vermieden werden. hdparm wird über die Kommandozeile aufgerufen. Zum Beispiel kann der Benutzer mit\nDMA für das erste IDE-Gerät anschalten.\n\nEine oft genutzte Funktion ist das Abschalten des Power Managements für Notebookfestplatten mit dem „Notebook-Festplatten-Bug“.\n\nWeiterhin können Benchmarks ausgeführt werden und die Eigenschaften und aktuellen Einstellungen des ATA Laufwerkes selbst ausgelesen werden. Einige dieser Einstellungen können mit hdparm geändert werden, beispielsweise Automatic acoustic management (AAM). Diese Funktionen stehen auch in einer Windows-Version von hdparm zur Verfügung.\n\nDas Programm muss mit root-/Administratorrechten laufen, da es sonst nicht auf das Device/Devicefile zugreifen darf.\n\n"}
{"id": "509610", "url": "https://de.wikipedia.org/wiki?curid=509610", "title": "Viterbi-Algorithmus", "text": "Viterbi-Algorithmus\n\nDer Viterbi-Algorithmus ist ein Algorithmus der dynamischen Programmierung zur Bestimmung der wahrscheinlichsten Sequenz von verborgenen Zuständen bei einem gegebenen Hidden Markov Model (HMM) und einer beobachteten Sequenz von Symbolen. Diese Zustandssequenz wird auch als Viterbi-Pfad bezeichnet.\n\nEr wurde von Andrew J. Viterbi 1967 zur Dekodierung von Faltungscodes entwickelt, er fiel quasi als Nebenprodukt bei der Analyse der Fehlerwahrscheinlichkeit von Faltungscodes ab. G. D. Forney leitete daraus 1972 den Optimalempfänger für verzerrte und gestörte Kanäle her. Der Viterbi-Algorithmus wird heutzutage zum Beispiel in Mobiltelefonen oder Wireless LANs zur Fehlerkorrektur der Funkübertragung verwendet, ebenso in Festplatten, da bei der Aufzeichnung auf die Magnetplatten ebenfalls Übertragungsfehler entstehen.\n\nDer Algorithmus ist in der Nachrichtentechnik und Informatik weit verbreitet: Die Informationstheorie, Bioinformatik, Spracherkennung und Computerlinguistik verwenden häufig den Viterbi-Algorithmus.\n\nGegeben sei ein HMM formula_1 mit\n\nSei formula_7 die beobachtete Sequenz von Symbolen.\nEs soll die wahrscheinlichste Zustandsfolge formula_8 berechnet werden.\nAlso diejenige Sequenz von verborgenen Zuständen, die unter allen Folgen formula_9 der Länge formula_10 den Wert von formula_11 maximiert, das ist die Wahrscheinlichkeit, dass das Modell formula_12 bei Erzeugung der Ausgabe formula_13 durch die Zustände formula_9 gelaufen ist.\n\nNach den Rechenregeln für bedingte Wahrscheinlichkeiten gilt:\nDa außerdem formula_16 nicht von formula_9 abhängt, ergibt sich folgender Zusammenhang:\n\nFür die eigentliche Berechnung werden nun zwei verschiedene Arten von Variablen – formula_19 und formula_20 – verwendet:\n\nIn formula_19 ist die maximale Verbundwahrscheinlichkeit gespeichert zum Zeitpunkt formula_22 bei der Beobachtung des Präfixes formula_23 durch eine Zustandsfolge der Länge formula_24 gelaufen zu sein und im Zustand formula_25 zu enden:\nDie Variable formula_20 dagegen merkt sich für jeden Zeitpunkt und jeden Zustand, welcher Vorgängerzustand an der Maximumsbildung beteiligt war.\n\nDie Variablen formula_19 sowie formula_20 lassen sich rekursiv bestimmen:\n\n\n\n\n\nDie Tabelle der formula_19 benötigt formula_35 Speicher, die Matrix der formula_20 ist von gleichem Umfang.\nFür jede Zelle der beiden Matrizen wird über formula_37 Alternativen optimiert, also ist die Laufzeit in formula_38.\n\nUm den Speicherplatz zu halbieren kann der Pfad formula_39 alternativ auch nach der Terminierung durch Backtracking in der Matrix aller formula_19 – also ohne die zusätzlichen Variablen formula_20 – ermittelt werden.\nDa aber in der Praxis die Berechnung von formula_20 keinen Mehraufwand verursacht, verlängert sich die benötigte Rechenzeit bei dem Backtracking-Ansatz geringfügig.\n\nDer Viterbi-Algorithmus ist der optimale Algorithmus zur Dekodierung von Faltungscodes im Sinne der Blockfehlerrate (maximum likelihood sequence estimation). Der im Sinne der Symbolfehlerrate optimale Dekodieralgorithmus ist der BCJR-Algorithmus.\n\nWie man aus der Beschreibung des Algorithmus sieht, kann er fast überall eingesetzt werden, um Muster zu erkennen. Das ist ein weites Feld, da Lebewesen ständig Sinnesreize interpretieren müssen und aus dem bereits Gelernten diese Signale einordnen. Der Viterbi-Algorithmus tut genau das auch und ist somit ein wichtiger Baustein der Künstlichen Intelligenz.\n\nEinen wichtigen Stellenwert nimmt der Algorithmus in der Bioinformatik ein, denn anhand des Viterbi-Algorithmus kann unter anderem von der tatsächlichen Sequenz eines DNA-Abschnitts auf eventuelle versteckte Zustände geschlossen werden. So kann zum Beispiel untersucht werden, ob es sich bei einer vorliegenden Sequenz wahrscheinlich um ein bestimmtes Strukturmotiv handelt (CpG-Insel, Promotor, …) oder nicht. Vorteil dieses rekursiven Algorithmus ist hierbei der linear mit der Sequenzlänge steigende Aufwand im Gegensatz zum exponentiellen Aufwand des zugrundeliegenden Hidden Markov Model.\n\n\n\n"}
{"id": "510353", "url": "https://de.wikipedia.org/wiki?curid=510353", "title": "Fotomanipulation", "text": "Fotomanipulation\n\nUnter einer Fotomanipulation versteht man die Veränderung oder Manipulation einer Fotografie unter Zuhilfenahme technischer Mittel, um einen fremden Sachverhalt vorzutäuschen.\n\nDie Manipulation kann vor, während oder nach der Aufnahme erfolgen.\n\nDie Geschichte der Fotomanipulation ist schon seit je eng mit der der Fotografie selbst verbunden. Bereits im 19. Jahrhundert, als es technisch möglich wurde, Bilder chemisch zu fixieren, versuchten Fotografen, Fotos durch technische oder kompositorische Tricks zu verändern. Ihr Ziel war es, ein Bild einer Wirklichkeit zu erzeugen, die es so nicht gegeben hat. Dabei wurde zu verschiedenen Mitteln gegriffen.\n\nDer Franzose Hippolyte Bayard (1801–1887) gilt als einer der frühesten Fotografen und entwickelte seinen „Fotoapparat“ sogar vor Louis Daguerre. Erste fotografische Versuche startete er bereits im Februar 1839. Das Verfahren, das Bayard im Verlauf dieser Experimente im März 1839 erfand, nannte sich „Direktpositiv-Verfahren“. Er hatte somit eine Methode entwickelt, um die Bilder direkt als Positiv auf Papier zu bringen. Von einem Freund Daguerres, François Arago, wurde er jedoch überredet, mit der Veröffentlichung seiner Arbeit ein Jahr zu warten – so veröffentlichte Daguerre seine Erfindung zuerst, weshalb Bayard nicht als Erfinder der Fotografie gilt.\n\nDaraufhin nahm Bayard ein Selbstporträt auf, das ihn selbst als Ertrunkenen darstellt, ein Protest gegen diese Ungerechtigkeit. Es symbolisiert, dass er über seine Erfindung schweigen musste. Er wird deshalb als Erfinder der modernen Technik des Kombinationsdruckes genannt, da er bereits verschiedene Negative verwendete, die er zu einem Bild kombinierte.\n\nMachthaber nutzen gerne die Tatsache, dass der durchschnittliche Betrachter dazu neigt, die Glaubwürdigkeit von Fotos zu überschätzen. Der Gebrauch von Fotomanipulationen für politische Zwecke ist besonders leicht, wenn die Medien der Kontrolle von Personen unterstehen. Solcher Missbrauch fand und findet in allen Gesellschaften statt, in denen Individualinteressen gegen die Gemeinschaft eingesetzt werden können.\n\n\nSeit den 1980er Jahren stiegen die Möglichkeiten beträchtlich, um Fotos zu verändern. Die Entwicklung in der Elektronischen Bildverarbeitung ermöglichte eine leichtere Bildmanipulation. Die Sofwarehilfsmittel erlauben es meist Veränderungen für das ungeschulte Auge kaum erkennbar auszuführen. Vor dem Computerzeitalter waren Fotografen für solche Vorhaben auf ihre handwerklichen Fähigkeiten zur Retusche angewiesen. Deren Methoden sind jedoch als Vorläufer der neuen Techniken erkennbar. Die Fotomanipulation wurde ein beherrschender Bestandteil der Massenmedien. Neben politischen Gründen ist die Beautyretusche allgegenwärtig. Es gibt (fast) kein veröffentlichtes Porträt ohne Manipulation der gewünschten Schlüsselreize.\n\nManipulationstechniken sind deutlich älter als hundert Jahre. Mittels feiner Pinsel und spezieller Farbe wurden wichtige Bilddetails sowohl auf Fotos, als auch den Originalen (Negative, Glasplatten) nachgezeichnet. In den meisten Fällen diente diese Arbeit der Verbesserung des Schärfeeindrucks. Diese Methode war so erfolgreich, dass sie bis Ende des 20. Jahrhunderts von den meisten Profifotografen für derartige Zwecke benutzt wurde. Eine beliebte Methode, die ohne großen technischen Aufwand auskam, war das Nachstellen von Szenen (Inszenierte Fotografie), das vor allem dann zum Einsatz kam, wenn der Fotograf zum Zeitpunkt des Geschehens nicht anwesend war.\n\nEine weitere Möglichkeit, die noch in ähnlicher Form angewendet wird, war die Fotomontage, bei der ein Bild aus mehreren Negativen zusammengesetzt wurde oder durch Mehrfachbelichtungen erzeugt wurde. Eine Mischung aus diesen beiden Methoden war die Komposografie. Durch das Entfernen oder Hinzufügen von Details und falsche Texterklärungen, kann zwar nicht das Foto selbst, aber der Zusammenhang, in dem das Bild steht, verfälscht werden.\n\nDie Komposografie ist eine retuschierte Bildcollage. Der Amerikaner Harry Grogin gilt als ihr Erfinder, da der Ausdruck erstmals in seiner Zeitschrift „The Graphic“ erwähnt wurde.\nBeim Prozess um das Ehepaar Rhinelander (1925) konnten die Fotografen keine brauchbaren Fotos machen, deshalb verwendete Artdirector Grogin Fotos der im Gerichtssaal anwesenden Personen und stellte die Szene mit Schauspielern nach. Auf deren Körper kopierte er die Köpfe der Dargestellten. Er verwendete insgesamt 20 verschiedene Fotos für ein Bild, das in „The Graphic“ erschien. Die Fälschung war mit dem Hinweis, dass das Bild im Studio entstanden sei, als solche gekennzeichnet.\n\nDie Komposografie existierte bereits länger, schon 1857 fertigte der Fotograf Oscar Gustave Rejlander Bilder mit dokumentarischer Qualität an, die er aus 30 separaten Negativen zu einem Bild zusammengefügt hatte.\n\nDie moderne digitale Bildbearbeitung bietet unzählige, leicht zu realisierende Manipulationsmöglichkeiten. Grenzen der Manipulationen sind praktisch nur noch durch die Fantasie gegeben.\n\nEs entwickelte sich das Bedürfnis einem Foto wieder Beweiskraft zu geben. Versuche, dies auf technischem Wege zu realisieren, sind das digitale Negativ, digitale Wasserzeichen oder digitale Bildforensik. Jedoch erlauben schon einfache Bildgestaltungsmittel, wie beispielsweise die Wahl des Aufnahmestandorts, einige Manipulationsmöglichkeiten:\nDie Arbeit des Fotojournalisten beruht von jeher auf dem Vertrauen der Leser und somit auf dem Maß an Glaubwürdigkeit, das seinen Bildern innewohnt. In den Anfängen der Fotografie war man sogar der Ansicht, dass die Kamera nicht lügen könne und deshalb alles, was sie zeigt, der Wahrheit entspricht. Dieses Vertrauen in die realitätsgetreue Wiedergabe der Umwelt wurde aber im Laufe der Jahre auf eine harte Probe gestellt, denn die Manipulationsfälle häuften sich. Dennoch genießt die Fotografie noch einen gewissen Beweischarakter.\n\nDabei ist das Wort „Manipulation“ ursprünglich nicht unbedingt negativ behaftet. Paul Martin Lester definiert den Begriff in einem seiner Artikel zuerst mit . Erst die dritte und vierte Bedeutung und enthält den schlechten Beigeschmack, den wir mit diesem Wort verbinden. Sie bezieht sich auf die betrügerische Absicht, die hinter der bloßen Veränderung steht, und erst in dieser Erklärung definiert sich ethisch unkorrektes Handeln.\n\nIn die Kategorie der erlaubten Veränderungen fallen die so genannten Bildillustrationen, wie sie etwa auf den Titelseiten von Magazinen zu finden sind. Sie müssen als Fälschungen erkennbar oder als solche gekennzeichnet sein und dürfen keinen Anspruch darauf erheben, Originalfotos zu sein. Andere legale Praktiken sind beispielsweise zoomen, die Änderung des Winkels, die Verwendung anderer Linsen, sowie diverse Dunkelkammertechniken, wie Adjustieren von Kontrast und Graustufen.\n\nDefinitiv nicht legal sind dagegen Fotomontagen oder anders veränderte Bilder, die als Originale ausgegeben werden. Sie werden als „visuelle Lügen“ bezeichnet und verstoßen somit gegen die Sorgfalts- und Wahrheitspflicht der Presse und werden nicht von der Pressefreiheit gedeckt. Eine explizite gesetzliche Regelung, die Fotomanipulationen verbieten würde, existiert jedoch nicht. Es ist fraglich, ob eine abstrakte gesetzliche Regelung auf diesem Gebiet überhaupt möglich ist, denn für die Entscheidung, ob eine legitime Bildbearbeitung bereits eine illegitime (und damit unter Umständen illegale) Fotomanipulation darstellt, muss stets der Einzelfall im entsprechenden Kontext betrachtet werden.\n\nEine übliche Form der Kennzeichnung von Manipulationen ist das Symbol „[M]“ in der Quellenangabe.\n\n\n"}
{"id": "511476", "url": "https://de.wikipedia.org/wiki?curid=511476", "title": "Macintosh TV", "text": "Macintosh TV\n\nDer Macintosh TV (oder Mac TV), der im Oktober 1993 erschien, war Apples erster Versuch, die Fernseh- und Computerwelt zu verbinden. Er besaß das Aussehen eines Macintosh LC 520, jedoch in schwarz, verfügte über einen Kabelfernseh-Tuner und wurde mit einer Fernbedienung ausgeliefert, die ebenfalls mit Fernsehern von Sony kompatibel war. Mit dem Gerät war es jedoch nicht möglich am Macintosh zu arbeiten und gleichzeitig ein TV-Signal zu empfangen. Man konnte auf der 14\" Sony Trinitron Röhre lediglich zwischen TV-Empfang und Macintosh Nutzung hin und her schalten.\n\nDas Gerät basierte auf einem Motorola 68030-Prozessor mit 32 MHz und verfügte über 4 MB Arbeitsspeicher, der auf bis zu 8 MB erweitert werden konnte. Die Festplatte war 160 MB groß. Mitgeliefert wurde System 7.1\n\nDer Macintosh TV kostete 2097$ und wurde in nur 250 Geschäften zum Verkauf angeboten. Nach 10.000 Stück wurde die Produktion schon nach 5 Monaten im Februar 1994 eingestellt.\n\n\nProzessor\n\nVideo\n\nLaufwerke\n\nAnschlüsse\n\nSonstiges\n\nApple TV\n"}
{"id": "511707", "url": "https://de.wikipedia.org/wiki?curid=511707", "title": "U3 (Standard)", "text": "U3 (Standard)\n\nU3 ist ein von SanDisk und M-Systems entwickelter Hard- und Software-Standard, um unter Windows geeignete Programme ohne vorherige Installation von einem USB-Stick auszuführen. U3 wird seit Juni 2010 nicht mehr unterstützt.\n\nAuf den sogenannten U3-Sticks befindet sich eine Software, genannt U3-Launchpad, zur Verwaltung der U3-zertifizierten bzw. U3-kompatiblen Anwendungen. Mit dem U3-Launchpad kann auch ein Passwortschutz für den USB-Stick aktiviert werden.\n\nU3 wurde erstmals am 7. Januar 2005 auf der CES in Las Vegas vorgestellt. Der Name soll für die drei Grundgedanken des Standards, „Simplified for You“, „Smarter about You“ und „As mobile as You“, stehen.\n\nU3-konforme Speichermedien und Programme sind seit Herbst 2005 verfügbar. Die typischen Kapazitäten von U3-Sticks liegen bei 1 bis 64 GB.\n\nDie Website des U3-Konsortiums www.u3.com wurde im Juni 2010 nach Ablauf des End-of-Life-Zeitraumes abgeschaltet. Die letzte Version 1.6.3.9 des Windows-7-fähigen Launchers ist aktuell nur noch über die Update-Funktion des Launchers bei den jeweiligen Herstellern erhältlich, bei Laufwerken des Herstellers SanDisk ist das Update nur noch über dessen Webauftritt zu beziehen.\n\nWindows 2000 (mit Service Pack 4), XP, Vista (ab U3 Launchpad v1.4) und Windows 7 (ab U3 Launchpad v1.6.3.9, nur 32-Bit-Version).\n\nU3 ermöglicht es, einen U3-USB-Stick in einen geeigneten öffentlichen Computer (im Büro, im Internetcafé, Arbeitsräumen an Universitäten, etc.) zu stecken und ein U3-kompatibles Programm ohne vorherige Installation direkt auszuführen. Nachdem der Stick wieder abgezogen wurde, verbleiben keine privaten Daten auf dem Wirtscomputer.\n\nBeispielsweise kann man so auf einem Computer in einem Internetcafé, auf dem nur ein Webbrowser installiert ist, Dokumente mit OpenOffice.org bearbeiten und E-Mails mit Thunderbird schreiben und empfangen, ohne dabei Daten auf der Festplatte zu hinterlassen.\n\nAllerdings hat ein Systemadministrator vielfältige Möglichkeiten, etwa durch eine entsprechende Systemrichtlinie, das Ausführen von Programmen von einem USB-Stick zu verhindern. Außerdem funktioniert die U3-Software nur unter Windows. Dadurch lässt sich bei eingesetzter Verschlüsselung der U3-Stick nur mit 3rd-Party-Tools unter anderen Betriebssystemen nutzen.\n\nEin Entfernen der U3-Software vom USB-Stick ist mit von den Herstellerseiten herunterladbaren Programmen möglich. Diese entfernen die ROM-Partition des USB-Sticks, infolgedessen ist auch U3 komplett gelöscht. Die U3-Software kann jedoch durch die Aktualisierungssoftware des jeweiligen Herstellers wiederhergestellt werden.\n\nDurch Austauschen der U3-Software kann der USB-Stick wie ein USB-CD-Laufwerk benutzt werden, auf welchem sich beliebige Programme befinden können. Dadurch ist es bei aktivierter Autorun-Funktion möglich, nur durch Einstecken des USB-Sticks, ohne Benutzung von Tastatur oder Maus, z. B. auf dem Rechner gespeicherte Passwörter, Favoriten oder Dokumente auf den Speicherstick zu kopieren (siehe Podslurping).\n\n\n\"(Auswahl – nur verfügbare und kostenlose Programme)\"\n\n"}
{"id": "513463", "url": "https://de.wikipedia.org/wiki?curid=513463", "title": "Mac mini", "text": "Mac mini\n\nDer Mac mini ist ein vom kalifornischen Computerhersteller Apple entwickelter Kompaktcomputer. Er wurde am 11. Januar 2005 auf der Macworld Expo in San Francisco der Öffentlichkeit vorgestellt.\n\nIm Laufe seiner Entwicklungsgenerationen verlor das Gerät sein internes DVD-Laufwerk, erhielt jedoch ein integriertes Netzteil (statt des ursprünglichen externen) sowie einen Lüfter hinzu. In der achten Generation hat der Mac mini die Abmessungen 19,7 cm × 19,7 cm × 3,5 cm und wiegt etwa 1,22 kg. Er zeichnet sich durch seine geringe Größe, eine ausgewogene Energieverwaltung und einen niedrigen Geräuschpegel aus. Gefertigt wird der Mac mini von dem taiwanischen Auftragshersteller Foxconn.\n\nDas aktuelle Modell des Mac mini wurde von Apple im Oktober 2018 vorgestellt. Es wird als „ 2018“ bezeichnet und hat wie sein Vorgänger ein Unibody-Gehäuse aus Aluminium, nun jedoch in der Farbe Space-Grau. Die Modellnummer ist nun A1993, der codice_1. Das Gehäuse ist unverändert 19,7 × 19,7 cm (7,7 Zoll) breit und tief sowie 3,6 cm (1,4 Zoll) hoch, jedoch verwendet Apple nun nur wiederverwendetes (Anm.: nicht recycletes) Aluminium aus eigener Produktion. Die Farbe ist identisch mit der des iMac Pro und ist im Vergleich zu den Vormodellen dunkler. Das Gewicht von rund 1,3 kg variiert je nach Konfiguration.\n\nIn der Standardversion verbaut Apple einen Vierkern-Core-i3-Prozessor der achten Generation (Coffee Lake) mit 3,6 GHz Taktfrequenz von Intel. Auf Wunsch lässt sich dieser durch einen Intel-Sechskern-Core-i5-Prozessor mit 3,0 GHz (Turbo Boost bis zu 4,1 GHz) oder einen Core i7 mit 3,2 GHz (Turbo Boost bis zu 4,6 GHz) der achten Generation ersetzen. Standardmäßig werden 8 GB DDR4-Arbeitsspeicher (2666 MHz) verwendet, die sich auf bis zu 64 GB erweitern lassen. Der PCIe-SSD-Festplattenspeicher ist fest mit dem Logicboard verlötet. Als Grafikprozessor kommt die im jeweiligen Intel-Chip integrierte \"UHD Graphics 630\" zum Einsatz.\n\nApples macOS Mojave ist als Betriebssystem vorinstalliert. An Schnittstellen gibt es einen Gigabit-Ethernet-Anschluss (optional 10 Gbit Ethernet), zwei USB-3-Anschlüsse (bis zu 5 Gbit/s), einen HDMI-2.0-Anschluss, einen 3,5-mm-Kopfhöreranschluss und vier USB‑C / Thunderbolt-3-Anschlüsse (bis zu 40 Gbit/s) mit Unterstützung für DisplayPort, USB 3.1 (2. Generation mit bis zu 10 Gbit/s), Thunderbolt 2. DVI- und VGA-Anschlüsse sind mit entsprechenden Adaptern verfügbar. Die Datenübertragung ist zudem über den Wi-Fi-Standard 802.11ac und Bluetooth 5 möglich.\n\nApple verwendet nun auch den selbst entwickelten T2-Chip für einen sicheren Boot-Prozess, bei Apple „“ bezeichnet. Auch das Passwort für die Verschlüsselung per FileVault und die Daten zur Erkennung des Anwenders durch Siri („Hey, Siri!“) werden in der „“ auf dem T2-Chip gespeichert.\n\nBei Tests war der Mac mini 2018 durchwegs leise. Die SSD erreichte eine ausgezeichnete Übertragungsrate von 3.000 MB/s beim Lesen, lässt sich allerdings nicht austauschen. Durchwegs positiv wird die Möglichkeit zum Aufrüsten des Arbeitsspeichers bewertet. Von iFixit erhält der Mac mini „Late 2018“ 6 von 10 Punkten – bemängelt werden die auf dem Logicboard fix verlöteten Bauteile (Prozessor und SSD) und Anschlüsse.\n\nDer Mac Mini ist nicht für grafikintensive Anwendungen wie den professionellen Videoschnitt konzipiert, weshalb die Grafikhardware nicht sonderlich leistungsstark ist. Für „klassische“ Anwendungsbereiche wie Textverarbeitung, Internet und Bildbearbeitungs-Aufgaben ist er ausreichend. Für das flüssige Betrachten von hochauflösenden Filmen (HD) war die Leistung der ersten Modelle nicht immer ausreichend.\n\nDurch den Umstieg von Apple auf die Intel/x86-Architektur wurde die Installation von Microsoft Windows auf Macintosh-Rechnern ohne Leistungseinbußen möglich, wie sie bei der Emulation unumgänglich sind.\n\nApple verhinderte dies durch die alleinige Verwendung des BIOS-Nachfolgers Extensible Firmware Interface (EFI), welches von Windows XP nicht unterstützt wird. Die Betreiber der Website www.onmac.net veranstalteten daher einen Wettbewerb, bei dem der Erste, dem die Installation von Windows auf einem Macintosh-Rechner mit x86-Architektur gelänge (ohne auf virtuelle Umgebungen wie VirtualPC zurückzugreifen), ein Preisgeld erhalten sollte, das durch Spenden auf der Website zusammengekommen war.\n\nDer erste Rechner, auf dem dies gelang, war ein Mac mini. Am 16. März 2006 stand ein Sieger fest, und der Wettbewerb wurde beendet.\n\nKurz danach (Anfang April 2006) veröffentlichte Apple eine Betaversion von Boot Camp, das einen Bootloader und Windows-Treiber für die Hardware bereitstellt. Zudem wurde für alle x86-Macs eine Firmware-Aktualisierung veröffentlicht, in welcher ein BIOS-Kompatibilitätsmodul enthalten ist. Dadurch wurde es möglich, auf diesen Rechnern Windows XP (Service Pack 2) oder Linux zu installieren. Seit Mac OS X 10.5 (Leopard) ist Boot Camp integriert und offiziell unterstützt.\n\nDer PowerPC-basierte Mac mini war niemals für klassisches Mac OS ausgelegt, allerdings kann bis Mac OS X Tiger die Classic-Umgebung verwendet werden, um alte Mac-Applikationen auszuführen. Die letzte Version von Mac OS X für PowerPCs ist Leopard (10.5), die auch auf dem Mac mini G4 läuft.\n\nDas 2005 vorgestellte Gerät verwendete einen auf der Hauptplatine verlöteten PowerPC-G4-Prozessor mit 1,25 oder 1,42 GHz Taktfrequenz in zwei, später drei Ausstattungsvarianten. Die größeren Modelle wurden mit einem DVD-Brenner („SuperDrive“) und einer 80-GB-Festplatte ausgeliefert, die kleineren mit einem CD-RW-Laufwerk („ComboDrive“) und einer 40-GB-Festplatte. Während im kleinsten Modell immer ein 56K-V.92-Modem eingebaut war, gab es dies beim größeren ab Mitte 2005 nur noch auf Bestellung. Ein Bluetooth-Modul und eine Airport-Extreme-Karte waren ab Mitte 2005 im größeren mini ab Werk eingebaut. Als Grafikkarte wurde in allen G4-minis eine Radeon 9200 mit 32 MB VRAM verbaut, im 1,42-GHz-Modell mit 64 MB. Die Grafikkarte hatte einen DVI-I-Ausgang und war nicht Core-Image-fähig.\n\nDer Mac mini G4 arbeitet mit einem geregelten Lüfter; der Geräuschpegel liegt im Leerlauf laut Computermagazin c’t bei 0,3 sone und springt unter Last auf 1,6 sone. Die Leistungsaufnahme liegt bei 18 Watt im Normalbetrieb und maximal 30 Watt unter Volllast. Das mitgeliefte Betriebssystem war Mac OS X Panther (10.3.7).\n\nEs gibt zwei USB-2.0-, einen Firewire-400- und einen 10/100-MBit-Ethernet-Anschlüsse sowie einen Ton-Ausgang („line out“). Es kann ein DDR-SDRAM-Modul (PC-2700) eingebaut werden, dieser Steckplatz ist werkseitig mit einem 256-MB-Modul bestückt. Der Arbeitsspeicher kann somit auf bis zu 1 GB (als ein Modul) erweitert werden. Die Festplatte ist ein 2,5″-ATA-Modell, das eingebaute optische Laufwerk ein ATA-Notebook-Laufwerk mit Einzugsmechanik. Somit ähneln die technischen Spezifikationen des Mac mini jenen des iBook G4.\n\nMit der Einführung des Betriebssystems Mac OS X Tiger (10.4.2) wurde die Modellpalette Mitte 2005 auf drei Varianten aufgestockt und der Arbeitsspeicher auf 512 MB verdoppelt. Beim sogenannten „“ im September 2005 wurde die Taktfrequenzen auf 1,33 GHz bzw. 1,50 GHz erhöht, der Preis blieb unverändert. Der Speicher war nun DDR-SDRAM PC3200U.\n\nDie Prozessoren sind gesockelt, was einen Austausch der 32-Bit-Core-Prozessoren in den Mac-mini-Modellen von 2006 durch 64-Bit-fähige Core-2-Prozessoren ermöglicht, wie sie auch im Modell von 2007 verbaut sind. Aber auch mit Prozessor-Upgrade bleibt den Modellen von 2006 ein 32-Bit-EFI, sie sind daher nicht ohne weiteres für ein 64-Bit-Mac-OS-X geeignet. Ab Snow Leopard (10.6) kann das Betriebssystem auf unterstützten Macs mit einem 64-Bit-Kernel gestartet werden, bis Lion (10.7) steht parallel auch der bisherige 32-Bit-Kernel zur Verfügung. Ab Mountain Lion (10.8) ist 64-Bit schließlich Voraussetzung. Apple stellt jedoch keine 64-Bit-Treiber für die integrierten Grafikchips GMA950 und X3100 bereit, sodass auch das Modell von 2007 mit 64-Bit-EFI nicht mit Mountain Lion und neuer kompatibel ist.\n\nAm 28. Februar 2006 wurde der Mac mini der ersten Generation mit Intel-x86-Prozessor vorgestellt. Äußerlich gegenüber dem ursprünglichen Design mit PowerPC-Architektur nahezu identisch, durch die intern nun genutzte Intel-Core-Architektur innen nun mit entsprechend grundlegend veränderter Technik. Das Betriebssystem und die Anwendungesprogramme mussten dafür von der PowerPC-Architektur auf IA-32 überführt werden, was in der Übergangsphase mit Universal-Apps bewerkstelligt wurde.\n\nIn der ersten Ausstattungsvariante hat der Rechner eine Core-Solo-T1200-CPU mit 1,5 GHz und eine 60-GB-2,5\"-SATA-Festplatte mit 5400 min sowie ein CD-RW-Combo-Laufwerk.\n\nDie zweite Ausstattungsvariante enthält die mit 1,66 GHz getaktete Mehrkern-CPU Core Duo T2300 und eine 9,5 mm hohe 80-GB-2,5\"-SATA-Festplatte mit 5400 min sowie ein DVD-R/DVD-RW-SuperDrive, welches auch zweilagige DVDs beschreiben kann.\n\nBeide Modelle haben 512 MB Arbeitsspeicher, deren zwei 256 MB DDR2-667-Module in SO-DIMM-Steckplätzen stecken. Gegenüber der PowerPC-Generation des Mac mini sind nun zwei statt nur ein Steckplatz für Speichermodule verbaut, und zwar für insgesamt bis zu 2 GB Arbeitsspeicher. Bedingt durch die Chipsatz-Grafik (s. u.), sollten grundsätzlich zwei gleiche Module eingebaut werden, da hierdurch der Dual-Channel-Modus des Speichers genutzt wird und sich so ein Geschwindigkeitsvorteil ergeben soll.\n\nDie Ethernet-Schnittstelle ist nun eine 10/100/1000-MBit-Variante. Gegenüber dem vorangegangenen Modell bietet der Mac mini nun sowohl analoge („line in/out“) als auch optisch-digitale Ton-Ein- und und kann damit auch Raumklang ausgeben. Die Anzahl der USB-Anschlüsse wurde auf vier erweitert, es gibt auch eine Firewire-400-Schnittstelle. Die Grafikeinheit des Mac mini ist ein im Chipsatz integrierter Intel-GMA950-Grafikprozessor („Chipsatzgrafik“), der im Betriebssystem mit „shared RAM“ betrieben wird, der unter Mac OS X mit 64 MB fest eingestellt ist. Das auf dem Mac mini mit \"Boot Camp\" installierbare Windows XP nutzt dynamisch zwischen 16 MB und 224 MB „shared RAM“. Die Grafikkarte kann die Bildsignale über DVI-I ausgeben (der VGA-Adapter liegt wieder bei), neu ist die Unterstützung für S-Video über einen als Zubehör erhältlichen Adapter.\n\nEbenfalls neu ist ein eingebauter Infrarotempfänger, welcher mit der ebenfalls mitgelieferten Fernbedienung \"Apple Remote\" für Front Row zusammenarbeitet. Der Empfänger wurde von Apple direkt im Gehäuseschlitz für das optische Laufwerk integriert. Beide Modelle bringen nun standardmäßig AirPort Extreme (802.11g) und Bluetooth mit. Das Modem der vorangegenen Mac-mini-Generation ist nicht mehr Bestandteil des Lieferumfangs, es kann jedoch als externes USB-Gerät nachgekauft werden.\n\nEine weitere Neuerung ist, dass die CPU des Mac mini nun in einem Prozessorsockel sitzt – Sockel 479 – und damit durch einen kompatiblen leistungsfähigeren Prozessor aufgerüstet werden kann.\n\nDem Mac mini „Early 2006“ liegen Mac OS X Tiger in Version 10.4.5 und iLife '06 bei. Zusätzlich sind die Programme Big Bang Board Games und Comic Life sowie diverse Demos installiert. Die letzte offiziell lauffähige Betriebssystemversion ist Mac OS X Snow Leopard (Version 10.6.8).\n\nAm 6. September 2006 führte Apple eine Produkt-Verbesserung durch, bei der auch die Mac-mini-Serie mit neuen Prozessoren ausgestattet wurde. Der Core-Solo-Prozessor wurde dabei aus dem Programm genommen. Die Ausstattungsvariante mit dem bisherigen Core Duo T2300 mit 1,66 GHz wurde ergänzt durch eine teurere Ausstattungsvariante mit T2400 mit 1,83 GHz. Das mitgelieferte Betriebssystem war nun Tiger in Version 10.4.7.\n\nAm 7. August 2007 wurde angekündigt, den Mac mini mit neueren Prozessoren vom Typ Core 2 Duo auszustatten, wahlweise mit 1,83 GHz T5600 oder 2 GHz T7200. Das Apple-eigene EFI ist nun in 64-Bit implementiert, jedoch wird wegen fehlender 64-Bit-Treiber für den integrierten Grafikprozessor („IGP“, \"\") weiterhin nur ein 32-Bit-Kernel unterstützt. Entgegen der offiziellen Herstellerangabe (maximal 2 GB) können diese Modelle bis zu 4 GB Arbeitsspeicher aufnehmen.\n\nZum Lieferumfang des \"„Mitte-2007“\"-Mac-mini gehört das Betriebssystem Mac OS X 10.4.10 und iLife ’08.\n\nIm Unterschied zu den vorherigen Modellnummer sind die Prozessoren nun auf dem Logicboad verlötet. Die Anzahl der USB-Anschlüsse wurde auf fünf erhöht und der FireWire-400- durch einen FireWire-800-Anschluss ersetzt.\n\nAm 3. März 2009 stellte Apple eine überarbeitete Version des Mac mini vor, ausgestattet mit Core-2-Duo-Prozessoren, wahlweise mit 2,0 oder 2,26 GHz. Im direkten Vergleich mit dem Vorgänger wurde ein leistungsfähigerer integrierter Grafikprozessor Nvidia Geforce 9400M verbaut sowie ein Mini-DisplayPort als Ergänzung zum weiter vorhandenen Mini-DVI-Anschluss. Unverändert wurde der Mac mini weiterhin mit 1 GB Speicher ausgeliefert, kann aber auf bis zu 4 GB bzw. nach Installation des „EFI Firmware Update 1.2“ auf 8 GB erweitert werden. Das drahtlose Netzwerk ist kompatibel mit dem 802.11n-Standardentwurf, außerdem hat der Mini nun fünf USB-Anschlüsse. Statt FireWire-400 ist nun die Variante mit bis zu 800 Mbit/s eingebaut. Die bisher im Lieferumfang enthaltene Apple Remote (Infrarotfernbedienung) ist nicht mehr enthalten.\n\nDas Betriebssystem Mac OS X Leopard ist in Version 10.5.6 im Lieferumfang enthalten. Dank vollständiger 64-Bit-Fähigkeit ist dieser Mac mini nun auch mit Mac OS X Lion (10.7) bis El Capitan (10.11) voll kompatibel.\n\nDie Überarbeitung des Mac mini 2009 wurde gleichzeitig mit dem Mac mini Server, dem iMac und dem MacBook am 20. Oktober 2009 vorgestellt. Standardmäßig werden zwei Varianten zum Preis von 549 Euro und 749 Euro angeboten, wobei die günstigste Ausführung einen Intel Core-2-Duo-Prozessor P7550 mit 2,26 GHz Taktfrequenz, 2 GB DDR3-Arbeitsspeicher und eine 160-GB-Festplatte, die teurere einen Intel Core-2-Duo-Prozessor P8700 mit 2,53 GHz Taktfrequenz, 4 GB DDR3-RAM und einer 320-GB-Festplatte verwendet. Als Aufpreis war auch ein Core-2-Duo-Prozessor P8800 mit 2,66 GHz Taktfrequenz als BTO-Option erhältlich. Die überarbeiteten Modelle \"„Spät-2009“\" sind wie die Vorgänger mit SuperDrive, eingebautem Mono-Lautsprecher, AirPort und mit dem Nvidia-Grafikprozessor Geforce 9400M mit Mini-DVI und Mini-DisplayPort ausgestattet. Im Lieferumfang ist nun Mac OS X Snow Leopard und iLife enthalten.\n\nDer Mac mini Server wurde erstmals am 20. Oktober 2009 vorgestellt. Er unterscheidet sich in der Hardware-Ausstattung kaum vom herkömmlichen Mac mini derselben Serie mit 2,53 GHz (Intel Core 2 Duo P8700), mit der Ausnahme, dass er anstatt eines optischen Laufwerks eine zweite Festplatte enthält; beide Festplatten sind mit 5400 U drehende 2,5-Zoll-SATA-II-Laufwerke mit 500 GB Speicherkapazität. Softwareseitig ist Mac OS X Server 10.6.1 mit einer Lizenz für eine unbegrenzte Anzahl an Clients im Lieferumfang enthalten.\n\nAm 15. Juni 2010 veröffentlichte Apple die optische überarbeitete Mac-mini-Generation mit Unibody-Gehäuse. Dieses misst nun 19,7 × 19,7 cm (entspricht 7,7 Zoll) in der Breite und Tiefe, 3,6 cm (1,4 Zoll) in der Höhe - im Vergleich zum vorherigen Gehäuse ist es deutlich flacher bei größerer Grundfläche. Auf dem Logicboard sitzt nun ein fix verlöteter Intel Core-2-Duo-Prozessor mit 2,4 GHz Taktfrequenz (P8600), zwei DIMM-Speicherbänken mit 2 GB DDR3-Arbeitsspeicher (maximal 16 GB möglich) und eine 320-GB-SATA-Festplatte; Optional war bei Bestellung auch ein P8800 mit 2,66 GHz Taktfrequenz konfigurierbar. Weiterhin hat sich, laut Apple, die Grafikleistung dank des Nvidia-Grafikprozessors GeForce 320M verdoppelt. Der neue Mac mini besitzt im Gegensatz zum Vorgänger ein integriertes Netzteil und nur noch vier USB-Anschlüsse, dafür aber einen HDMI-Anschluss. Über den Mini-DisplayPort kann nun ebenfalls ein Audiosignal ausgegeben werden. Das mitgelieferte Betriebssystem ist Mac OS X Snow Leopard in Version 10.6.4. Der erste Unibody-Mac-mini kostete in Deutschland 709 Euro (Preissenkung zum 2. November 2010, zuvor 809 Euro), in Österreich 699 Euro und in der Schweiz 849 Franken (ca. 660 Euro).\n\nDem ebenfalls am 15. Juni 2010 veröffentlichten Mac mini Server des ansonsten baugleichen Grundmodells fehlt, wie schon dem Vormodell, das optische Laufwerk. Stattdessen sind zwei 500-GB-Festplatten verbaut, was kombiniert somit eine Speicherkapazität von 1 TB ergibt. Als Ausstattungsvariante gab Apple den Intel Core-2-Duo-Prozessor mit 2,66 GHz und 4 GB DDR3-Arbeitsspeicher vor, nur die Festplatten waren per Build-to-Order konfigurierbar. Die Server-Variante verwendet dieselbe Grafikkarte wie das Desktop-Modell, eine Nvidia GeForce 320M. Als Betriebssystem ist Mac OS X Server 10.6.4 mit der Lizenz für eine unbegrenzte Anzahl an Clients im Lieferumfang enthalten. Der Mac mini Server „Mitte 2010“ kostete in Deutschland seit der Preissenkung am 2. November 2010 999 Euro (zuvor 1.149 Euro), in Österreich ebenfalls 999 Euro und in der Schweiz 1.249 Franken (ca. 970 Euro).\n\nIm Zuge der Veröffentlichung von Mac OS X Lion wurde auch der Mac mini im Juli 2011 überarbeitet. Apple verzichtet auf ein optisches Laufwerk und stattet den Mac mini mit Intels Core-i5- (2,3 oder 2,5 GHz Dual-Core) oder i7-Prozessoren (2,7 GHz Dual-Core) aus. Bei der Grafikkarte können Kunden zwischen der Intel HD Graphics 3000 (codice_12) und einer AMD Radeon HD 6630M (codice_13) wählen. Ebenfalls enthalten ist ein Thunderbolt-Anschluss, der auch beim iMac, MacBook Pro und MacBook Air seit Mitte 2011 angeboten wird.\n\nUrsprünglich hätte dieses Modell wohl noch mit Mac OS X Snow Leopard (10.6.7 oder 10.6.8) ausgeliefert werden sollen, da in diesen Versionen des Betriebssystems Treiber speziell für den Mac mini „Mid 2011“ enthalten sind. Mit der Wiederherstellungs-DVD des MacBook „Early 2011“, das eine sehr ähnliche Hardware besitzt, ist es sogar möglich, Snow Leopard auf dem Mac mini codice_13 zu installieren, da darauf die benötigen Treiber für die Radeon-Grafikkarte enthalten sind, die in den regulären Versionen von Mac OS X ab 10.6.7 fehlen. Allgemein ist die Installation nicht trivial, aber möglich. Snow Leopard ist die letzte Version des Betriebssystems mit Rosetta, mit dem PowerPC-basierte Mac-Programme ausgeführt werden können.\n\nDer Mac mini Server 2011 ist identisch mit dem Grundmodell codice_12, bis auf den Core i7-Quad-Core-Prozessor mit 2,0 GHz Taktrate und dem vorinstallierten Betriebssystem Lion Server. Konfigurationsoptionen beinhalteten größere Festplatten ( 750 GB statt  500 GB), SSDs ( 256 GB) oder eine Kombination aus beiden.\n\nIn der im Oktober 2012 vorgestellten Generation wurden mit Intels Core-i5- (2,5 GHz Dual-Core, codice_19) oder i7-Prozessoren (2,3 oder 2,6 GHz Quad-Core, codice_20) schnellere Prozessoren verbaut, erstmals auch Vierkern-CPUs. Der Grafikchip ist nun bei allen Modellen der integrierte Intel HD Graphics 4000, ein dedizierter Grafikchip lässt sich nicht mehr auswählen. Die vier USB-Anschlüsse unterstützen nun das schnellere USB 3.0. Alle Modellvarianten haben mindestens 4 GB mit 1600 MHz getakteten DDR3-RAM. Die Festplattenkapazität erhöhte sich auf maximal 1 TB.\n\nAlle Modellvarianten verteuerten sich in Deutschland dabei gegenüber der 6. Generation um 30 Euro.\n\nEnde 2012 wurde mit dem Mac Mini der siebten Generation der neue Mac Mini Server vorgestellt. Er besitzt einen Intel Core i7-Quadcore-CPU mit 2,3 oder 2,6 GHz. Der Arbeitsspeicher beläuft sich auf 4, 8 oder 16 GB DDR3 mit 1600 MHz. Als Festplatte sind zwei mal 1 TB, 256 GB SSD oder zwei 256 GB SSDs erhältlich. Zudem wurden die USB-2.0-Ports durch USB-3.0-Ports ersetzt. Mac OS X Mountain Lion Server ist werksseitig installiert. Die 4. Generation des Mac mini Server ist die vorerst letzte Version des Mac mini Servers. Mit der Vorstellung des Mac mini der 8. Generation (Late 2014) wurde der Mac mini Server eingestellt.\n\nIm Oktober 2014 stellte Apple die achte Generation vor, mit dem gleichen Gehäuse (19,7 × 19,7 × 3,5 cm. Gewicht zwischen 1,19 und 1,22 kg) wie dem des Vorgängers und ebenfalls ohne optisches Laufwerk.\n\nIn der Standardversion verbaut Apple einen 1,4 GHz (Turbo Boost bist zu 2,7 GHz) Intel Core i5-Zweikernprozessor der vierten Generation (Haswell). Auf Wunsch kann man einen i5-Dual-Core-Prozessor mit 2,6 GHz oder 2,8 GHz (Turbo Boost bis 3,1 GHz oder 3,3 GHz) oder einen Core-i7-Dual-Core-Prozessor mit 3,0 GHz (Turbo Boost bis zu 3,5 GHz) verbauen lassen. Standardmäßig werden 4 GB 1600 MHz LPDDR3-Arbeitsspeicher verwendet, die sich aber auch ab Werk bis zu 16 GB erweitern lassen (dies geht nicht durch den Anwender, da sich die Unterseite nicht, anders als bei der 7. Generation, öffnen lässt und der Arbeitsspeicher selbst festgelötet wurde). Als Grafikchip kommt ein „Intel HD Graphics 5000“ zum Einsatz, optional „Intel Iris Graphics“. Zudem wurde zeitweise keine 2-TB-Fusion-Drive-Variante der Festplatte mehr angeboten. Apple gibt mit Berufung auf die Rangliste der mit dem Energy-Star-Label versehenen Geräte an, der Mac mini sei der „energieeffizienteste Desktop-Computer der Welt“.\n\nDas Apple Desktop-Betriebssystem Mac OS X Yosemite ist vorinstalliert. Zusätzlich steht die iWork- und die iLife-Suite kostenlos zum Herunterladen zur Verfügung. An Schnittstellen gibt es zwei Thunderbolt-2-Ports, vier USB-3.0-Ports, einen HDMI-Port, einen SDXC-Speicherkartensteckplatz, einen Gigabit-Ethernet-Port und einen Audio-Eingang und -Ausgang. Für kabellose Kommunikation zum Rechner stehen Bluetooth 4.0, ein IR-Empfänger und Wi-Fi 802.11ac (kompatibel mit IEEE 802.11a/g/b/n) zur Verfügung.\nIm Lieferumfang sind der Mac mini und ein Netzkabel enthalten.\n\nSiehe Abschnitt „Aktuelles Modell“.\n\n"}
{"id": "513584", "url": "https://de.wikipedia.org/wiki?curid=513584", "title": "Hewlett-Packard 9100A", "text": "Hewlett-Packard 9100A\n\nDer HP-9100A von Hewlett-Packard war ein Computer, der 1968 in einer Werbeanzeige erstmals in der Literatur als \"Personal Computer\" (PC) bezeichnet wurde, obgleich er nicht viel mit dem heutigen Verständnis eines PCs gemeinsam hat.\n\nDennoch stellte er zu einer Zeit, als Zugriff auf Rechenleistung nur \"indirekt\" (durch Übergabe von Lochkarten an Operatoren in einem Rechenzentrum) möglich war, den Aufbruch in ein neues Zeitalter dar (Anzeigentext: \"9100A puts answers just a touch away\"), in welchem Apple I und Apple II, TRS-80, PET und IBM PC, die dem heutigen Verständnis des PCs entsprechen, erst denkbar wurden.\n\nSteve Jobs, Gründer von Apple Computer, sagte im April 1995 in einem Interview mit Daniel Morrow: \"„I saw my first desktop computer at Hewlett-Packard which was called the 9100A. It was the first desktop in the world.“\"\n\nDer HP-9100A verfügte über:\n\n\nDer Preis für das Grundgerät betrug 1968 4.900 US-Dollar oder 19.600 DM, und damit ca. das Doppelte eines durchschnittlichen Bruttojahresgehaltes der damaligen Zeit. Gemessen am heutigen Jahresdurchschnittseinkommen (29.569 Euro) entspricht das einem Preis von ca. 59.000 Euro.\n\nBemerkenswert ist, dass diese Leistung ohne die Verwendung von integrierten Schaltkreisen erbracht wurde. Wenige Jahre später brachte HP 1972 den ersten technisch-wissenschaftlichen Taschenrechner HP-35 heraus, der von William Hewlett als „HP-9100A im Taschenformat“ bezeichnet wurde. Er kostete 395 US-Dollar und wurde ein großer kommerzieller Erfolg, 1974 folgte der weltweit erste programmierbare Taschenrechner HP-65.\n\nHewlett-Packard wurde aufgrund der Verletzung von geistigem Eigentum zur Zahlung von 900.000 $ an Olivetti verurteilt. Bei der Verhandlung wurde festgestellt, dass für den HP-9100 technische Lösungen wie die Magnetkarte und die Architektur von Olivettis Programma 101 kopiert wurden.\n\n\n"}
{"id": "514196", "url": "https://de.wikipedia.org/wiki?curid=514196", "title": "Extra-Halfbright-Modus", "text": "Extra-Halfbright-Modus\n\nDer Extra-Halfbright-Modus (kurz EHB) ist ein Grafikmodus der Amiga-Computer von Commodore mit Ausnahme der ersten Amiga 1000. Er stellt zusätzlich zu den normalen 32 gleichzeitig darstellbaren Farben des Amiga weitere 32 zur Verfügung, die die halbe Helligkeit der ersten 32 besitzen. Dadurch können 64 Farben mit lediglich 32 Farbregistern dargestellt werden.\n\nDie Farbpalette des Amiga umfasst 32 Einträge, die in den sogenannten Farbregistern des Videochips abgelegt sind. Jede Farbe wird durch einen Rot-, Grün- und Blauanteil (siehe RGB-Farbraum) definiert, wobei für jeden Farbanteil vier Bit zur Verfügung stehen. Jede gespeicherte Farbe kann also über einen Index von 0-31 angesprochen werden. Nebenstehende Beispieltabelle verdeutlicht das logische Schema der so genannten Farbpalette.\n\nUm ein Pixel am Bildschirm farbig darzustellen, muss das Programm den Index der gewünschten Farbe angeben, beispielsweise den Wert codice_1 für „Grün“. In Binärdarstellung sind für die Adressierung von 32 Werten 5 Bit erforderlich (2=32).\n\nBeim Extra-Halfbright-Modus wird für die Indizierung der Farbpalette ein Bit mehr verwendet, also insgesamt 6 Bits. Dadurch wäre theoretisch die Darstellung von 64 Farben (2=64) möglich. Da die Farbpalette jedoch nur 32 Farben enthält, können auch nur Indizes von 0 bis 31 angesprochen werden. Das sechste, höchstwertige (also ganz links stehende) Bit wird daher nicht für den Index verwendet, sondern fungiert als so genanntes Flag. Ist dieses Bit nicht gesetzt – hat also den Wert 0 – so bilden die übrigen fünf Bit wieder den normalen Index. Gemäß dem oben stehenden Beispiel würde (0)11111 also zum Index 11111 für die Farbe Grün. Ist das sechste Bit gesetzt (hat also den Wert 1), wird wieder der gleiche Paletteneintrag mit dem Index 11111 verwendet, allerdings werden hier die Bit-Werte der einzelnen Farbanteile um eine Stelle nach rechts verschoben (durch die Hardware im Amiga-Chipsatz, entspricht im Dezimalsystem der Division durch zwei), wodurch sich die dargestellte Farbe ändert. Da die resultierende Farbe etwa die halbe Helligkeit der Basisfarbe hat, spricht man vom Extra-Halfbright-Modus (von englisch \"half\" halb und \"brightness\" Helligkeit). Gemäß dem Beispiel würde der Index(1)11111 also zum Paletteneintrag 11111 der Farbe Grün verweisen, die einzelnen Farbanteile würden um eine Stelle nach rechts verschoben, so dass eine neue Farbe entsteht.\n"}
{"id": "514761", "url": "https://de.wikipedia.org/wiki?curid=514761", "title": "IWork", "text": "IWork\n\niWork ist das Office-Paket von Apple für die Betriebssysteme macOS und iOS. Es enthält das Textverarbeitungsprogramm \"Pages\", das Präsentationsprogramm \"Keynote\" und das Tabellenkalkulationsprogramm \"Numbers\".\n\niWork trat neben das veraltete und nur behelfsmäßig als Carbon-Programm auf Mac OS X portierte AppleWorks, das von Apple nicht mehr unterstützt wird. Ein neues Büropaket als Nachfolger von AppleWorks war zuvor lange erwartet worden. Apple wählte jedoch nicht den gemeinhin erwarteten Namen \"iWorks\", sondern den Namen iWork, was im Englischen wie „I work“ (deutsch: „ich arbeite“) klingt. iWork war vormals der Name eines Programms zur Zeiterfassung und Rechnungserstellung, bis Apple die Rechte an dem Namen erwarb.\n\nDas Paket iWork wird nicht mit Versionsnummern bezeichnet, sondern erhält die zweistellige Jahreszahl als Versionsnummer (bspw. \"iWork ’09\"), da die einzelnen Programme in unterschiedlichen Versionsgenerationen vorliegen. Ab Januar 2009 war iWork auch als Paket mit Mac OS X und iLife vergünstigt erhältlich.\n\nMit \"iWork.com\" stellte Apple darüber hinaus eine internetbasierte Ergänzung für iWork zur Verfügung, mit der man Dokumente gemeinsam mit anderen Personen betrachten und kommentieren konnte. Die Dokumente konnten jedoch nicht online bearbeitet werden.\n\nAnfang 2010 stellte Apple zusammen mit dem iPad auch eine iWork-Version für das Betriebssystem iOS vor. Seit Mai 2011 sind die Programme auch für das iPhone und den iPod touch verfügbar.\n\niWork.com wurde am 31. Juli 2012 im Zuge der Umstellung auf iCloud eingestellt. Ein Jahr später stellte Apple während der WWDC 2013 einen in iCloud integrierten Nachfolgedienst vor, mit der man iWork-kompatible Formate direkt im Webbrowser editieren kann. Dieser befindet sich derzeit (Stand: April 2015) im Beta-Stadium.\n\nAm 22. Oktober 2013 stellte Apple neue Versionen von iWork vor, die eine übersichtlichere Benutzeroberfläche haben und an die neuesten Versionen von OS X (10.9 Mavericks) und iOS (iOS 7) angepasst sind. So wurde zum Beispiel die Zusammenarbeit von mehreren Personen eingeführt. Ein Dokument kann gleichzeitig von mehreren Personen editiert werden. Der Funktionsumfang der Programme wurde allerdings reduziert, so sind z. B. Serienbriefe nicht mehr möglich, verknüpfte Textfelder werden nicht mehr unterstützt und der Funktionsumfang zur Animation bei Präsentationen wurde reduziert.\n\nIm Herbst 2013 kündigte Apple an, dass das iWork-Paket zukünftig beim Kauf eines neuen Macs oder iOS-Gerätes kostenlos erhältlich ist.\n\n"}
{"id": "514794", "url": "https://de.wikipedia.org/wiki?curid=514794", "title": "Carbon (Apple)", "text": "Carbon (Apple)\n\nCarbon ist eine Sammlung von Programmierschnittstellen (APIs) von Apple, die Teile des unter der Macintosh System Software enthaltenen Macintosh-Baukasten () bereitstellt. Es war von vornherein als Brückentechnologie gedacht, um es seinerzeit Programmentwicklern möglichst leicht zu machen, ihre Mac-OS-Applikationen für das später erschienene Mac OS X vorzubereiten. Zum Zeitpunkt der Einführung war Mac OS 8 das aktuelle Betriebssystem von Apple, Mac OS 9 sowie dessen Nachfolger und neue Betriebssystemgeneration Mac OS X befanden sich in Entwicklung.\n\nMac OS X wurde 2012 in OS X umbenannt, ohne „Mac“ im Namen, und 2016 schließlich in macOS. Carbon ist in jeder Version von Mac OS X/​OS X/​macOS enthalten, wird jedoch seit 2007 nicht mehr weiterentwickelt und bleibt somit auf 32-Bit beschränkt.\n\nDie , zu Deutsch auch Macintosh-Baukasten, ist jener Teil an Funktionsaufrufen des originalen Macintosh Systems (1984), die im ROM untergebracht und dadurch fix mit dem jeweiligen Macintosh-Computer verbunden waren. Lange Zeit war es von Apple so dargestellt worden, dass nur ein Apple-Computer die grafischen Funktionen auf diese Weise bereitstellen kann.\n\nDie Toolbox erlaubt es Programmierern, bereitgestellte Funktionen der zu nutzen, um auf einfache Weise vom System erstellte (und somit auch einheitlich aussehende) Fenster, Menüs, Kontrollkästchen und Dialogfelder zu erstellen und Ereignisse zu steuern. Auch Funktionen des Finder, der selbst nicht Teil des Toolbox-ROM ist, lassen sich über dieselbe Art von Funktionsaufrufen nutzen.\n\nIm Laufe der Entwicklung der Macintosh System Software wurde die Programmierschnittstelle (API) stetig erweitert, etwa um QuickDraw und QuickTime. Undokumentierte Funktionen wurden jedoch auch gestrichen. So war mit System 7 (1991) die zuvor in Pascal programmierte API in C neu implementiert worden, was die Portierung des Betriebssystems von der m68k- auf die PowerPC-Architektur erleichterte. Auch wurde im Zuge des Projekts „Star Trek“ an einer Portierung von System 7 und damit auch der enthaltenen Programmierschnittstelle auf x86-PCs gearbeitet, diese wurde aber nicht fertiggestellt. Da viele Macintosh-Anwenderprogramme für System 6 undokumentierte Funktionen nutzten, mussten die Entwickler diese bereits für System 7 anpassen.\n\nAb 1997 versuchte Apple auf eine gänzlich neue Programmierschnittstelle zu wechseln: der von NeXT übernommenen OpenStep-API, die mit dem Betriebssystemprojekt Rhapsody als Yellow Box und mit Mac OS X als Cocoa weiterentwickelt wurde. Dabei hätten jedoch alle existierenden Macintosh-Programme vollkommen neu geschrieben werden müssen. Die Hersteller von für Apple unverzichtbarer Anwendungssoftware gaben nach der WWDC 1997 zu verstehen, dass sie diesen Portierungsaufwand nicht in Kauf nehmen würden, woraufhin Apple notgedrungen einen großteils Quelltext-kompatiblen Ersatz für den Macintosh-Baukasten entwickeln musste. Dieser Ersatz ist Carbon, der mit Mac OS X eingeführt wurde und auch für das klassische Mac OS als CarbonLib verfügbar ist. Damit waren nur geringere Anpassungen am Quelltext nötig damit ein bestehendes Macintosh-Programm für das Carbon-API neu kompiliert werden konnte, das dann auf beiden Betriebssystemen (Mac OS und Mac OS X) lauffähig ist.\n\nCarbon basiert auf den Programmierschnittstellen des Macintosh-Baukasten () der klassischen Macintosh System Software (1984, ab 1996 Mac OS), abzüglich derer, die historisch gewachsen und redundant waren, und solcher, die typischen Mac OS X-Funktionen wie präemptivem Multitasking und Speicherschutz im Weg standen. So wurden von ca. 8.000 Funktionen auf dem klassischen Mac OS rund 2.000 Funktionen nicht in Carbon übernommen. Wurde ein Mac-OS-Programm so geschrieben, dass es die Carbon-Bibliothek nutzte, so konnte es sowohl auf Mac OS als auch auf Mac OS X nativ ausgeführt werden. Voraussetzung war jedoch, dass kein Motorola-68k-Code mehr verwendet wurde, da Carbon unter Mac OS nur auf der PowerPC-Architektur verfügbar war.\n\nMit dem Verwenden der Carbon-Bibliothek ergab sich für die Entwickler der Vorteil, bereits bestehenden Quelltext in großen Teilen weiter verwenden zu können. Der Portierungsaufwand war also überschaubar, denn für die modernere Cocoa-API, die ihren Ursprung im NeXT-Betriebssystem NeXTStep hat, hätte ein sehr großer Anteil des Quelltextes der bestehenden Programme neu geschrieben werden müssen. Viele bekannte Programme waren noch lange Zeit Carbon-Programme, beispielsweise auch Adobe Photoshop bis zur Version CS4 (Ende 2008). Apple selbst nutzte Carbon für einige Bestandteile von Mac OS X, die es auch schon unter Mac OS gab – erst für Mac OS X Snow Leopard (10.6, 2009) wurde beispielsweise der Finder in Cocoa komplett neu geschrieben. Auch die Frontends von iTunes und QuickTime Player für Mac OS 9, Mac OS X und Windows basierten lange Zeit auf Carbon. Neu entwickelte Apple-Programme wie z. B. der Safari-Browser für Mac OS X und Windows basierten hingegen von Beginn an auf dem moderneren Cocoa.\n\nDie Carbon-Bibliothek ist in Mac OS X seit der ersten Version (Public Beta, 2000) Bestandteil des Betriebssystems. Unter Mac OS 8.1 bis 9.2.2 muss die Bibliothek, die gratis per Download verteilt wurde, manuell nachinstalliert werden. Nach Meinung eines Entwicklers kann Carbon allerdings erst ab der in Mac OS X 10.2 („Jaguar,“ 2002) integrierten Version als fertig betrachtet werden. Die letzte Version der CarbonLib ist 1.6 und benötigt mindestens Mac OS 8.6. In der Classic-Umgebung ist CarbonLib 1.6.1 enthalten, das automatisch in das virtualisierte Mac OS 9.1 oder neuer installiert wird.\n\nProgramme, die die Carbon-Bibliothek verwendeten und auf Mac OS und Mac OS X lauffähig waren, hießen zumeist (übersetzt also ‚carbonisierte Programme‘). Auf spätere reine Mac-OS-X-Programme, die die Carbon-Bibliothek verwendeten, wurde der Begriff nicht angewandt.\n\nBereits in Mac OS X Tiger (10.4, 2005) konnten 64-Bit-Konsolenprogramme ausgeführt werden; mit Mac OS X Leopard (10.5, 2007) war Cocoa 64-Bit-tauglich. Seit Mac OS X Snow Leopard (10.6, 2009) konnte entweder ein 32-Bit- oder ein 64-Bit-Kernel gestartet werden. Ab OS X Mountain Lion (10.8, 2012) ist das Betriebssystem voll auf 64-Bit umgestellt. Pläne, auch Carbon 64-Bit-tauglich zu machen, wurden von Apple 2007 fallengelassen. Während sich Mac OS X also schrittweise in ein reines 64-Bit-Betriebssystem verwandelte, wurde die Weiterentwicklung von Carbon nun endgültig eingestellt und alte Programme, die noch auf Carbon setzten, blieben auf 32-Bit beschränkt. Unter anderem deswegen war Adobe Photoshop CS4 unter Windows bereits als 64-Bit-Programm verfügbar, unter Mac OS X jedoch noch immer ein 32-Bit-Programm. Zur Portierung von Photoshop auf Cocoa wurde ein eigenes Entwicklerteam eingesetzt (C2C: Carbon to Cocoa). Ende April 2010 veröffentlichte Adobe schließlich Photoshop CS5 als 32- und 64-Bit-Cocoa-Anwendung.\n\nAuf dem seit 2007 verfügbaren iOS ist Carbon nicht vorhanden.\n\n\n"}
{"id": "515956", "url": "https://de.wikipedia.org/wiki?curid=515956", "title": "StyleWriter", "text": "StyleWriter\n\nStyleWriter bezeichnet eine Serie von Tintendruckern der Firma Apple für die hauseigenen Macintosh-Computer. \n\nDas Druckwerk stammte von Canon, bei späteren Modellen von Hewlett-Packard; fast baugleiche Geräte wurden von diesen Herstellern für IBM-kompatible Computer vertrieben. Entsprechend nutzten die äquivalenten Drucker auch die gleichen Tintenpatronen. \n\nDie StyleWriter lösten die ImageWriter Nadeldrucker ab und wurden parallel zu den professionellen und damals sehr teuren Laserdruckern von Apple angeboten. \n\nDie StyleWriter waren ausnahmslos QuickDraw-Drucker ohne eigenen Hauptprozessor. PostScript-Dateien konnten nur mit Hilfe von Rasterprogrammen ausgegeben werden. Die StyleWriter wurden an der RS-422-Schnittstelle betrieben, über die alle Macintoshs bis einschließlich des platinumgrauen PowerMacintosh G3 verfügten. Die Schnittstellentaktung erfolgte seitens des Druckers, so dass die Daten mit hoher Geschwindigkeit an den Drucker übermittelt werden konnten. Es gab für einige Modelle Adapter, um die Drucker in ein LocalTalk oder Ethernet Netzwerk zu integrieren.\nTypisch für das Apple–Design war der Verzicht auf nahezu alle Knöpfe, die zu der Zeit bei anderen Druckern noch weit verbreitet waren, etwa ein Papiervorlauf. \n\nAndere Hersteller übernahmen Teilweise den Begriff „...Writer“. So bot Hewlett-Packard (HP) seine Macintosh-kompatiblen Drucker unter dem Namen „Deskwriter“ statt Deskjet an. \n\n"}
{"id": "516251", "url": "https://de.wikipedia.org/wiki?curid=516251", "title": "SimplyMEPIS", "text": "SimplyMEPIS\n\nSimplyMEPIS (Managerial, Educational and Personal Information Systems) ist eine auf Debian GNU/Linux basierende Linux-Distribution. Die Distribution wurde von Warren Woodford (aus Morgantown, West Virginia, USA) ab November 2002 eigenständig entwickelt, da er mit den diversen Linux-Distributionen unzufrieden war. Woodfords Ziel ist die Entwicklung einer Distribution, die das tut, was der Anwender möchte. Linux-Distributionen wie SuSE, Red Hat oder Mandriva sind seiner Meinung nach für den einfachen Anwender ohne IT-Kenntnisse zu schwer zu verstehen und zu gebrauchen. Der Name soll auch nach Memphis, Tennessee, USA benannt sein, die Einwohner sprechen ihre Stadt ähnlich wie Mepis aus.\n\nBei der Distribution handelt es sich um eine Live-CD, die ohne Installation ausprobiert werden kann, aber auch eine Installation auf der Festplatte wird ausdrücklich unterstützt. Die Linux-Distribution verfügt über automatische Hardwareerkennung und -konfiguration, sowie die Möglichkeit, die Größen von NTFS-Partitionen zu verändern. Weitere Features sind der ACPI-Stromsparmodus, die WiFi-Unterstützung, Anti-Aliasing-TrueType-Schriftarten sowie eine Firewall. Als grafische Benutzeroberfläche dient das weit verbreitete KDE.\n\nDie Distribution wurde ursprünglich unter dem Namen \"MEPIS\" entwickelt, den heute aber nur noch die entwickelnde Firma trägt. Anfang 2004 wurde MEPIS in zwei Versionen, \"SimplyMEPIS\" und \"ProMEPIS\", aufgespalten. Die Haupt-Distribution wurde in der Zwischenzeit in \"SimplyMEPIS\" umbenannt. Daneben ist eine kommerzielle Variante namens \"ProMEPIS\" für Unternehmen und Entwickler geplant, jedoch bis heute nicht erschienen. Eine Ende 2004 erstmals erschienene, kommerzielle und speziell für Deutschland entwickelte Version unter dem Namen \"SphinxOS\" wurde schon Mitte 2005 wegen zu großer Konkurrenz – insbesondere durch Ubuntu – wieder eingestellt. Eine Gruppe Freiwilliger begann daraufhin, die Distribution als Community-Projekt fortzusetzen, aber auch dieses Vorhaben scheiterte letztendlich. Hintergrund waren markenrechtliche Probleme, da sowohl die Rechte an der Marke \"SphinxOS\" als auch an zum Beispiel dem Installer und dem System-Center der Distribution bei Dritten lag und liegt.\n\nDie Version 6.5 basiert technisch auf Ubuntu, um so von dessen Weiterentwicklungen zu profitieren. Im Juni 2007 kündigte MEPIS-Hauptentwickler Warren Woodford allerdings an, in Zukunft doch wieder auf Debian GNU/Linux zu setzen, da sich seine Erwartungen an Ubuntu nicht erfüllt hätten. Insbesondere werden in alten Ubuntu-Versionen nur Fehler behoben, aber keine neuere Software hinzugefügt. Nach etlichen Verzögerungen wurde SimplyMEPIS 7.0 am 23. Dezember 2007 veröffentlicht. Es enthält einige Neuerungen, unter anderem neuere Pakete wie Kernel 2.6.22.14 oder OpenOffice.org 2.3.\n\nKurz nach Freigabe der MEPIS-Tools unter einer Apache-Lizenz wurden diese durch Mitglieder der weltweit aktiven MEPIS-Community für die Veröffentlichung von Version 8.0 weiter verbessert. SimplyMEPIS 8.0 basiert auf Debian GNU/Linux 5.0 („Lenny“) und enthält den Linux-Kernel 2.6.27-18. Zu der Vielzahl an aktualisierten Softwarepaketen zählen unter anderem KDE 3.5.10, OpenOffice.org 3.0 sowie Firefox 3.0.6.\n\nNeben den \"offiziellen\" Paketquellen von MEPIS werden von der MEPIS-Community eigene Paketquellen unterhalten, die jeweils aktuelle Programmupdates enthalten sowie zusätzliche Softwarepakete.\n\nAlle Mepis-Pakete der Version 8 sind kompatibel zu Debian Lenny und können auch auf einem Rechner mit Debian Lenny installiert werden.\n\nWährend SimplyMEPIS CD-Abbilder als freier Download verfügbar sind, ermutigt MEPIS zufriedene Nutzer, kostenpflichtigen Zugang zu Premium-Servern zu buchen, worin die einzige Einkommensquelle des Unternehmens besteht.\n\nSeit der Veröffentlichung der SimplyMEPIS-Version 6.5 gibt es mit \"MEPIS antiX\" ein schlankes Derivat für ältere Computer ab 64 MB RAM. MEPIS antiX wird von der Community unterstützt und ist kostenlos erhältlich. Als Fenstermanager wird Fluxbox verwendet. Die aktuelle Version antiX-17.3.1 \"Helen Keller\" erschien am 28. Dezember 2018.\n\nEine Community-Version in Zusammenarbeit mit dem antiX-Team und der früheren MEPIS-Gemeinschaft, als Fenstermanager wird Xfce verwendet. Die aktuelle Version MX-18.1 \"Continuum\" ist am 9. Februar 2019 veröffentlicht worden.\n\n\n"}
{"id": "517841", "url": "https://de.wikipedia.org/wiki?curid=517841", "title": "FileMaker (Unternehmen)", "text": "FileMaker (Unternehmen)\n\nFileMaker, Inc. ist eine hundertprozentige Tochtergesellschaft des kalifornischen Computerherstellers Apple, die die Datenbanksoftware FileMaker entwickelt. Die Firma FileMaker entstand 1998 als Nachfolgerin von \"Claris\", die ihrerseits 1987 als Ableger von Apple gegründet worden war.\n\nClaris wurde Anfang 1998 aufgelöst. Das Programm \"FileMaker Pro\" wurde Grundlage des neu gegründeten Unternehmens FileMaker, Inc.\n\nProdukte von Claris waren:\n\nVon 2008 bis 2013 wurde die persönliche Datenbankanwendung \"Bento\" verkauft.\n"}
{"id": "517871", "url": "https://de.wikipedia.org/wiki?curid=517871", "title": "Nisus Writer", "text": "Nisus Writer\n\nNisus Writer ist ein Textverarbeitungsprogramm für Macintosh.\n\nNisus Writer war das erste Textverarbeitungsprogramm für den Macintosh, das WorldScript unterstützte. Damit war es – lange vor der Einführung von Unicode – in der Lage, mehrere Schriftsysteme in einem Textdokument zu mischen, also Hebräisch, Arabisch, Japanisch usw., sofern das entsprechende Apple Language Kit installiert war. Nisus Writer war daher vor allem ein unverzichtbares Werkzeug für Anwender, die mit alten Sprachen zu tun hatten, etwa Theologen und Archäologen.\n\nWeitere Alleinstellungsmerkmale waren mehrteilige Textauswahl und mehrere Zwischenablagen, eine Tonaufnahmefunktion usw.\n\nDie letzte Version ist Nisus Writer 6.5; sie wird nach wie vor vertrieben. Sie läuft unter Mac OS 8.6 bis 9.2.2 sowie unter Mac OS X, allerdings nur in der Classic-Umgebung auf PPC, ab Mac OS X 10.5 nur unter SheepShaver. Die internationalen Nisus Writer Dictionaries, die es auf CD gab, werden nicht mehr vertrieben, so dass man als Käufer der US-Version des Nisus Writer 6.5 nur englische Rechtschreibprüfung und vor allem nur englische Silbentrennung durchführen kann.\n\nEin besonders schlankes Textverarbeitungsprogramm stellte Nisus mit Nisus Writer Compact zur Verfügung. Es war vor allem für den stromsparenden Einsatz auf den 68K-PowerBooks von Apple gedacht.\n\nNisus Writer wurde von seinen Anwendern geschätzt wegen der Zuverlässigkeit und der Leistungsmerkmale, die bei anderen Programmen nicht zu finden waren. Diese Leistungsmerkmale machten es zu einem beliebten Werkzeug für Vielschreiber, insbesondere Buchautoren. Zugleich stand Nisus Writer wegen der sehr freien Interpretation von Apples Benutzerschnittstellenrichtlinien stets in der Kritik.\n\nFür Mac OS X entwickelte Nisus ein von Grund auf neues Produkt, Nisus Writer Express auf Basis von Cocoa, das sich sehr eng an die Benutzerschnittstellenrichtlinien von Apple hält. Es handelt sich um eine sehr schlanke Textverarbeitung mit weitreichender Word-Kompatibilität. Im Jahr 2007 veröffentlichte Nisus ein weiteres Produkt, Nisus Writer Pro, welches die Funktionsmerkmale von Nisus Writer Express um mehrere professionelle Funktionen, wie zum Beispiel dem Erstellen von Inhaltsverzeichnissen und Indices sowie der Unterstützung von Querverweisen, erweiterte. Das Programm baut auf Nisus Writer Express auf und weist daher eine ganze Reihe Ähnlichkeiten mit Letzterem auf, sowohl was die Funktionalität wie auch die Benutzeroberfläche betrifft.\n\nEs gibt Lokalisierungen des Programms in verschiedenen Sprachen wie Dänisch, Französisch, Deutsch, Italienisch, Polnisch und Spanisch.\n\n"}
{"id": "519619", "url": "https://de.wikipedia.org/wiki?curid=519619", "title": "Oracle RAC", "text": "Oracle RAC\n\nOracle RAC (\"Oracle Real Application Clusters\") ist eine zusätzliche Option des Datenbankmanagementsystems der Firma Oracle. Oracle RAC ermöglicht Ausfallsicherheit, indem mehrere Knoten eines Rechnerverbundes (engl. Cluster) auf dieselbe Datenbank zugreifen und für Clientrechner Datenbankdienste zur Verfügung stellen. Fällt einer der Knoten aus, übernehmen die anderen dessen Funktionalität. Zusätzlich kann Oracle Real Application Cluster als Möglichkeit der Skalierung eingesetzt werden: Reicht die Kapazität eines Datenbankservers nicht aus, kann diese durch einen weiteren Rechnerknoten ergänzt werden. Um eine optimale Skalierung im Oracle Datenbankcluster erreichen zu können, muss jedoch das Anwendungsdesign an die Anforderungen paralleler Clusterzugriffe angepasst werden. Das Oracle Real Application Cluster dient als Basis des Oracle Grid im Datenbank-Backend.\n\nBei Nutzung des \"Oracle Real Application Clusters\" sind im Normalbetrieb zwei oder mehr Rechnerknoten in einem Cluster (Rechnerverbund) aktiviert und greifen auf denselben Datenbestand zu. Jeder Knoten betreibt zwar eine eigenständige Datenbankinstanz, die jedoch über die Cache-Fusion-Technologie mit den Datenbankcaches aller am Cluster beteiligten Rechner zusammengeschaltet ist. So macht es für den Nutzer nahezu keinen Unterschied, auf welchem Knoten sein Zugriff tatsächlich ankommt.\n\nDer Vorteil: Fällt ein Rechner aus, so können sich Clients unmittelbar und ohne Wiederanlaufzeit auf einen verbleibenden Rechnerknoten verbinden. Zudem können Lasten auf alle Clusterknoten verteilt werden. Langlaufende Operationen können mittels Parallel-Slave-Prozessen auch über den gesamten Cluster parallelisiert werden. So ist neben der erhöhten Verfügbarkeit auch das Thema Skalierung eine der Stärken des Real Application Clusters. Redundante Hardware wird nicht nur im Fehlerfall eingesetzt, sondern kann – anders als bei Failover-Clustern - im Normalbetrieb genutzt werden.\n\nÄhnlich wie in einem \"Oracle Failover Cluster\" wird auch hier ein Private-Network für den Cluster-Heartbeat sowie ein davon separiertes Public-Network benötigt. Allerdings wird im Oracle Real Application Cluster konkurrierend auf die Datenbank-Dateien eines Shared-Storages zugegriffen. Dies erfordert zusätzlichen Kommunikationsaufwand zwischen den Knoten, der zur Abstimmung von Sperren und veränderten Block-Images nötig ist. Diese Cluster-Kommunikation erfolgt über das Private-Network. In Real-Application-Cluster-Umgebungen ist es daher wichtig, einen möglichst schnellen privaten Cluster-Interconnect zu nutzen. Mindestens Gigabit-Ethernet sollte eingesetzt werden. Besser noch sind spezielle, oft proprietäre Lösungen mit höherer Bandbreite. Beispiele sind Memory-Channel (alphabasierte HP-Cluster), Myrinet (Linux-Systeme), Scalable-Coherent-Interconnect (SUN), Veritas LLT (verschiedene Plattformen) oder HP Hyper-Fabric HMP.\n\nInzwischen gibt es auch Hersteller-Lösungen wie die Oracle Exadata, die auf den Einsatz von RAC abgestimmt sind.\n\nNotwendige Komponenten für den Einsatz eines Oracle Real Application Clusters sind:\n\n\n\nOracle Real Application Cluster wird durch folgende Administrationswerkzeuge unterstützt:\n\n\n\n"}
{"id": "519777", "url": "https://de.wikipedia.org/wiki?curid=519777", "title": "QuickDraw", "text": "QuickDraw\n\nQuickDraw (kurz: QD) war die 2D-Bildschirmbeschreibungssprache des Macintosh bis Mac OS 9.2.2 und ein Bestandteil des Macintosh-Baukastens. In macOS wurde sie von Quartz abgelöst, ist jedoch als Bestandteil der Carbon-Programmierschnittstelle noch verfügbar.\n\nDie erste kommerzielle QuickDraw-Bibliothek wurde von Bill Atkinson für UCSD Pascal auf dem Apple II Computer geschrieben, um eine schnelle Rastergraphik zu ermöglichen. Später arbeitete Atkinson an QuickDraw auf dem Apple Lisa Computer weiter. Schließlich wurde QuickDraw für den Macintosh verwendet, hierzu leistete auch Andy Hertzfeld Beiträge.\n\nDer theoretische Ursprung scheint in der Doktorarbeit von Bill Atkinsons Professor Jef Raskin zu liegen: \"A Hardware-Independent Computer Drawing System Using List-Structured Modeling: The Quick-Draw Graphics System, Pennsylvania State University, 1967\".\n\nMit der Veröffentlichung des MacPaint 1.3 Quelltextes 2010 durch das Computer History Museum, wurde eine historische Variante des QuickDraw Quelltextes ebenfalls verfügbar.\n"}
{"id": "520089", "url": "https://de.wikipedia.org/wiki?curid=520089", "title": "WorldScript", "text": "WorldScript\n\nWorldScript, herausgebracht von Apple in den frühen 1990er Jahren, war die erste Möglichkeit, die Zeichen nichtlateinischer Silbenschriften wie Chinesisch oder Bengalische auf einem preiswerten Rechners darzustellen. WorldScript bildete eine Ebene zwischen Betriebssystem und Anwendung (Texteditor usw.). Für Apple-Nutzer stellt sich WorldScript als Vorläufer von Unicode dar.\n\nAb System 7.1 führte Apple die Implementierungen nichtlateinischer Schriftsysteme in einer Programmierschnittstelle namens WorldScript zusammen. WorldScript I bot Ein-Byte- und WorldScript II Zwei-Byte-Zeichensätze. Die Unterstützung für neue Schriftsysteme wurde durch Installation neuer Apple Language Kits erreicht. Einige solcher Kits wurden mit dem Betriebssystem mitgeliefert, andere mussten separat bei Apple oder anderen Herstellern erworben werden.\n\nDie Unterstützung internationaler Schriftsysteme gab Textverarbeitungen wie Nisus Writer eine Chance sowie Programmen, die die WASTE-Texteinheit (WorldScript-Aware Styled Text Engine) benutzten, da das marktbeherrschende Microsoft Word keine WorldScript-Unterstützung bot.\n\nIn Mac OS 8.5 wurde das System um volle Unicode-Unterstützung erweitert (durch eine Programmierschnittstelle namens Apple Type Services for Unicode Imaging (ATSUI)). Trotzdem blieb WorldScript die vorherrschende Technik für internationale Schriftsysteme bis zur Einführung von Mac OS X, da nicht viele Programme ATSUI unterstützten.\n"}
{"id": "521895", "url": "https://de.wikipedia.org/wiki?curid=521895", "title": "OpenLDAP", "text": "OpenLDAP\n\nOpenLDAP ist eine Implementierung des LDAP, die als freie Software unter der, BSD-Lizenz ähnlichen, OpenLDAP Public License veröffentlicht wird. OpenLDAP ist Bestandteil der meisten aktuellen Linux-Distributionen und läuft auch unter verschiedenen Unix-Varianten, macOS und verschiedenen Windows-Versionen.\n\nDa OpenLDAP den LDAP-Standard verfolgt, ist es mit OpenLDAP möglich, eine zentrale Benutzerdatenverwaltung aufzubauen und zentral zu warten.\n\nDa OpenLDAP die Referenzimplementierung des Protokolls ist, werden Schemadateien sorgfältig auf Protokollkonformität geprüft. Dies führt gelegentlich zu Fehlermeldungen, wenn mangelhafte Schemadateien, die von Directory Server Agents (DSA) anderer Hersteller akzeptiert werden, in ein OpenLDAP System übertragen werden.\n\nDurch die Bereitstellung unterschiedlicher Backends und Overlays lassen sich Protokollerweiterungen und erweiterte Operationen (extended Operations) sehr leicht realisieren.\nDas SQL Backend leitet die Suchergebnisse einer RDBM-Suche an den DSA weiter, so dass der auftraggebende LDAP Client ein protokollgerechtes Datenpaket empfängt.\n\nDas Softwarepaket umfasst neben dem Server auch weitere Werkzeuge zur Konfiguration und benötigte Bibliotheken. Es besteht hauptsächlich aus folgenden Bestandteilen:\nFolgender Dienst wird nicht mehr mit ausgeliefert, da die Replikation jetzt über die Syncrepl ausgeführt wird.\n\nDie Konfigurationsdateien für die OpenLDAP-Clients sind unter Linux (siehe auch die Manual Page codice_6):\n\n\n\nDie OpenLDAP-Server Konfigurationsdateien sind:\n\n\n\n"}
{"id": "522878", "url": "https://de.wikipedia.org/wiki?curid=522878", "title": "Canonical", "text": "Canonical\n\nCanonical ist ein britischer Linux-Distributor. Das Unternehmen gehört dem südafrikanischen Unternehmer Mark Shuttleworth und hat seinen Hauptsitz im 27. Stock des Millbank Tower im Londoner Stadtteil City of Westminster. Im Sommer 2006 eröffnete Canonical ein Büro in Montreal für den globalen Kundendienst und Dienstleistungsbetrieb.\n\nCanonical ist Sponsor der kostenlosen Debian-basierten Linux-Distribution Ubuntu und bietet zusätzlich Unterstützung bei der Installation und Konfiguration von Ubuntu und offiziell unterstützter Derivate an. Canonical bietet außerdem eine Serverversion von Ubuntu mit mehreren Jahren „Long Term Support“ an. Der Kunde bezahlt für die Unterstützung, nicht für die Programme. Bei dem Verwaltungsprogramm „Landscape“ muss man jedoch bereits die kostenpflichtige Betreuung von Canonical abonniert haben, um Zugriff zu erhalten. Zudem bietet es IT-Beratung, Schulungen, Zertifizierungen, Rechtsschutzversicherungen gegen Klagen um geistiges Eigentum sowie Paketierungen von Programmen als Debian-Paket für Ubuntu an und arbeitet mit Erstausrüstern zusammen.\n\nWeitere Projekte sind unter anderem eine Webanwendung zur Unterstützung bei der Entwicklung von freier Software namens Launchpad und eine proprietäre Webanwendung namens Landscape, die IT-Fachleuten bei der Systemadministration behilflich ist. Mit Ubuntu One betrieb Canonical einen Online-Speicherdienst, welcher zum 1. Juni 2014 eingestellt wurde. Das eingestellte OpenCD-Projekt, bei dem freie Programme für das Betriebssystem Windows auf einer CD-ROM zusammengestellt wurden, wird in kleinerem Maßstab auf den Ubuntu-Installationsmedien und der Projektaufspaltung OpenDisc weitergeführt.\n\nAußerdem kündigte das Unternehmen 2013 die Entwicklung eines eigenen Smartphones an, das durch Crowdfunding finanziert wird. Mit der Kampagne wurden zwar Einnahmen in Rekordhöhe erzielt, der notwendige Gesamtbetrag aber dennoch nicht erreicht – damit scheiterte das Projekt.\nTrotz des Scheiterns der Crowdfoundingkampange arbeitet Canonical weiter an dem Projekt. Ziel ist es, Ubuntu Touch und Ubuntu soweit möglich auf dem gleichen Code aufzubauen, sodass auf Desktop und Mobilgeräten die gleichen Programme genutzt werden können. In Zusammenarbeit mit dem spanischen Hersteller BQ wurde das BQ Aquaris E4.5 Ubuntu Edition zum ersten auf dem Markt befindlichen Smartphone mit diesem Mobilbetriebssystem. Somit stellt das Unternehmen das erste GNU/Linux-System, das sowohl als Desktop- als auch Mobilvariante erhältlich ist, zur Verfügung. Am 5. April 2017 wurde von Mark Shuttleworth die Absicht erklärt, die Produkte Convergence und somit Ubuntu für Phones und Tablets nicht fortzuführen. Ebenso soll die distributionseigene Desktopumgebung Unity nicht weiterentwickelt werden, so dass Ubuntu 17.10 mit Gnome Shell als Desktopumgebung ausgeliefert werden soll. Weiterhin wird auch der selbst entwickelte Anzeigeserver Mir zugunsten von Wayland eingestellt. Das Unternehmen wird sich mehr in Richtung Cloud und IoT entwickeln, Sparten, die mehr Gewinn ermöglichen als der Consumer-Bereich.\n\nDas Unternehmen unterstützt die weltweite Veranstaltung „Software Freedom Day“, bei der sich verschiedene regionale Gruppen aus Nutzern und Entwicklern von freier Software treffen. Zudem unterstützt es die GNOME-Entwicklerkonferenz GUADEC.\n\nCanonical wird immer wieder vorgeworfen, die Entwicklung von Ubuntu negativ zu beeinflussen. Nach Meinung eines Teils der Gemeinschaft sei das Betriebssystem sehr stark auf die Bedürfnisse von Umsteigern ausgelegt, die zuvor auf einer anderen Plattform gearbeitet haben. Insbesondere der Wechsel von Gnome zur hauseigenen Unity-Benutzeroberfläche hat in der Entwicklergemeinde eine sehr große Diskussion ausgelöst.\n\nAuch Canonicals Umgang mit anderen Open-Source-Projekten und der Linux-Gemeinschaft wird oft kritisiert. Anfang 2013 kündigte Canonical an, aus dem Projekt Wayland, welches einen Ersatz für den in die Jahre gekommenen X11-Displayserver entwickelt, auszusteigen und stattdessen einen eigenen zu entwickeln, woraus das Projekt Mir entstand. Im April 2017 gab Canonical in einer Erklärung bekannt, dass Unity samt Mir gescheitert sei und ab der Ubuntu-Version 17.10 den GNOME-Desktop samt Wayland als Standard nutzen wird.\n\nDas Debian-Projekt hat Canonical mehrfach für den Umgang mit den eigenen Paketen kritisiert. Canonical bediene sich zwar fleißig bei Debian, führe selbst aber kaum Änderungen (sogenannte Patches) an das Projekt zurück. Als Reaktion auf die monatelange Diskussion wurde im März 2011 eine Austauschplattform gegründet, mit der die Zusammenarbeit zwischen Canonical beziehungsweise Ubuntu und Debian verbessert werden soll. Canonical selbst hat auf die Kritik mit dem Angebot reagiert, einige Mitarbeiter für die freiwillige Arbeit am Debian-Projekt abzustellen.\n\n"}
{"id": "525390", "url": "https://de.wikipedia.org/wiki?curid=525390", "title": "Mittlere Datentechnik", "text": "Mittlere Datentechnik\n\nAls Mittlere Datentechnik (engl. \"\") bezeichnete man in den 1960er bis 90er Jahren im engeren Sinne Computersysteme (kurz \"MDT-Computer\"), im weiteren Sinne eine Komplettlösung des Arbeitsprozesses der Datenverarbeitung im Büro, bestehend aus Minirechner, Software und Support.\n\nLaut IBM geht die Bezeichnung „Mittlere Datentechnik“ auf die Einführung des Satellitenrechners IBM 1401 am 5. Oktober 1959 zurück. Dieser war in Transistortechnik mit Kernspeicher aufgebaut. Als Datenmedium wurden Lochkarten und wahlweise Magnetbänder verwendet. Es war für die damalige Zeit ein vergleichsweise komplettes System, das auch autonom arbeitete und so den ökonomisch günstigen Einstieg in die Datenverarbeitung ermöglichte. Etwa zeitgleich (1959) wurde die Programmiersprache RPG verfügbar, die half, die per Tabelliermaschinen oder Lochkartenmischern erarbeiteten Ergebnisse auf die neue Technologie zu übertragen.\n\nUngefähr ab Mitte der 1960er Jahre wurden MDT-Computer von Nixdorf Computer, Triumph-Adler, Hohner, CTM Computertechnik Müller GmbH Konstanz (Otto Müller und Ilse Müller), Dietz Computersysteme, Kienzle Apparate, Taylorix, Ruf Datensysteme (Didic, Frech und Frieder Jung), SIEMAG Datentechnik, Olivetti und anderen internationalen Unternehmen entwickelt. Diese Entwicklungen liegen zeitlich lange vor CP/M und anderen Mini- und Mikrorechnern. Viele MDT-Computer konnten sich auf Dauer am Markt nicht durchsetzen und werden heute als Exoten betrachtet, wie Wagner Computer, Unicomp Computer Systeme und Saturn.\nZum Industrie-Arbeitskreis Mittlere Datentechnik (AMD) gehörten die Unternehmen Kienzle, Nixdorf, Triumph-Adler, Hohner, Anker und Philips.\n\nMDT-Computer waren zunächst Einzelplatzsysteme mit herstellereigenem Betriebssystem und meist herstellereigenem Prozessor, ausgerichtet auf kommerzielle Anwendungen wie Finanzbuchhaltung, Fakturierung und Lagerbuchhaltung. Anfangs wurden Magnetkonten als Externspeicher verwendet (daher auch die Bezeichnung \"Magnetkonten-Computer\", abgekürzt \"MKC\"), später Magnetkassettenspeicher, Diskettenspeicher und zuletzt Plattenspeicher (siehe Festplatte). Maschinen mit Plattenlaufwerk und Datensichtschirmen (Terminals, Bildschirmarbeitsplatz) wurden als Dialogcomputer bezeichnet. Das erste Mehrplatzsystem war RUF90 von Ruf Datensysteme Karlsruhe, entwickelt von Didic, Frech und Frieder Jung. Danach kam Nixdorf mit einem in den USA gekauften und auf Deutschland angepassten Clone auf den Markt, dem Data General Minicomputer. Keines der Systeme konnte sich als Standard durchsetzen. Mit Aufkommen der integrierten Mikroprozessoren, BASIC als Programmiersprache und CP/M als Betriebssystem verloren die deutschen Entwicklungen der Mittleren Datentechnik immer mehr an Bedeutung und durch den PC endgültig ihren Markt. Einen Eindruck von der Vielfalt der Mittleren Datentechnik vermittelt folgende Liste von Anbietern (Prime-Produkt bzw. Modellnummer in Klammer) aus dem Jahr 1973 :\n\n\nAn der Universität Karlsruhe wurde 1970 auf Initiative von Lutz J. Heinrich unter finanzieller Mitwirkung der MDT-Hersteller Akkord Electronic, RUF Datensysteme, Hohner, Kienzle und Philips der Stiftungslehrstuhl „Organisationstheorie und Datenverarbeitung (Mittlere Datentechnik)“ eingerichtet. Der Inhaber des Lehrstuhls war Lutz J. Heinrich selbst, bis zu seiner Berufung an die Johannes Kepler Universität Linz noch im gleichen Jahr. Nicht zuletzt deshalb führte die Einrichtung dieses Lehrstuhls zu keiner nachhaltigen Zusammenarbeit zwischen Wissenschaft und akademischer Lehre mit den Unternehmen, die Systeme der Mittleren Datentechnik entwickelten und vertrieben.\n\n\n\n\n\n"}
{"id": "525407", "url": "https://de.wikipedia.org/wiki?curid=525407", "title": "K3b", "text": "K3b\n\nK3b ist ein Brennprogramm von KDE zum Erstellen von CDs, DVDs und Blu-ray Discs. Es besitzt eine grafische Benutzeroberfläche, die es dem Anwender ermöglichen soll, alle Aufgaben des CD- und DVD-Brennens möglichst intuitiv zu erledigen. K3b wird seit 1998 entwickelt. Im März 2007 erschien Version 1.0 für KDE 3 und am 27. Juni 2010 ist die erste auf KDE 4 basierende Version 2.0 veröffentlicht worden.\n\nDer Name K3b ist die kurze Schreibweise für KBBB, was für „KDE \"Burn Baby, Burn!\"“ steht.\n\nGenau genommen ist K3b eine graphische Oberfläche für eine Vielzahl von Programmen, welche die Grundfunktionen für das Brennen von DVDs und CDs unter GNU/Linux (oder anderen Unix-artigen Systemen) auf der Kommandozeile zur Verfügung stellen. K3b dient als Front-End und vereinfacht durch das Bündeln der Funktionalitäten dieser Programme die Handhabung erheblich, da der Benutzer mit ihnen nicht mehr direkt in Berührung kommt. Das heißt der Anwender muss die einzelnen Programme mit ihren Optionen nicht kennen und kann mittels grafischer Oberfläche und Maus an sein Ziel gelangen.\n\nDie K3b-Oberfläche unterstützt einfaches Drag and Drop sowohl innerhalb des Programms, als auch aus anderen Programmen heraus (zum Beispiel aus dem Dateimanager Dolphin). Ebenso kann K3b als Dienstprogramm für andere KDE-Programme dienen, wie z. B. dem Medienplayer Amarok. Ermöglicht wird dies durch die Verwendung von D-Bus, welches Kommunikationsfunktionen zwischen Programmen bereitstellt.\n\nDie Funktionen und Fähigkeiten von K3b hängen von den installierten Programmen (und Bibliotheken) ab, auf die K3b auf dem jeweiligen System zurückgreifen kann. Bei vollständiger Installation aller benötigten Programme ergibt sich ein sehr großer Funktionsumfang:\n\nDabei sind eMovix-Datenträger CDs oder DVDs, die neben dem eigentlichen Film auch ein kleines GNU/Linux-basiertes Betriebssystem enthalten. Damit kann ein Computer direkt von CD oder DVD gebootet werden, und erfordert so keine installierte Software zum Abspielen der CD oder DVD.\n\nDie Auswahl an Audio-Formaten, aus denen heraus auf CD gebrannt werden kann, ist ebenfalls abhängig von der installierten Software. Auf einem vollständigen, freien System umfassen sie Ogg Vorbis, FLAC und WAVE (.wav). Optional werden MP3, Musepack und Monkey’s Audio unterstützt.\n\nAb Version 2.0 von K3b, die im Juni 2010 erschien, wird das Beschreiben von Blu-ray Discs unterstützt.\n\n\n"}
{"id": "529070", "url": "https://de.wikipedia.org/wiki?curid=529070", "title": "Knowledge Navigator", "text": "Knowledge Navigator\n\nDer Knowledge Navigator ist ein fiktiver Computer, den der damalige Apple-CEO John Sculley 1987 erstmals in seinem Buch \"Odyssey\" beschrieb. Er war Sculleys Vision des persönlichen Computers für das 21. Jahrhundert.\n\nDas Konzept des Knowledge Navigators war durch die Arbeiten von Vannevar Bush (Memex) und Alan Kay inspiriert. Präzise die Veränderungen durch das World Wide Web vorhersagend, würde der Knowledge Navigator seinen Verwendern die Kommunikation rund um den Globus erlauben. Er wäre mit hypertextuellen Informations-Datenbanken verbunden, die die Verknüpfung einer Idee mit anderen Ideen erlaubten (Sculley zitiert Ted Nelson als Referenz) und künstliche Intelligenz bzw. intelligente Agenten verwenden, um aktiv nach Informationen zu suchen.\nDieser persönliche Computer werde die Technologien und das Interface-Design der Nachfolger der damaligen Unix- und Macintosh-Systeme enthalten.\n\nSculley beschrieb in verschiedenen Reden die fünf Schlüsseltechnologien, die dem Knowledge Navigator zugrunde liegen sollten:\n\n\nNach Sculley würde der Knowledge Navigator große, hochauflösende Flachbildschirme verwenden, auf denen Text, Grafiken und computergenerierte Animationen dargestellt würden. Außerdem wären HiFi-Sound, synthetische Sprache und Spracherkennung vorhanden. Sculley sah für den Knowledge Navigator keine spezifische Form vor; so könne der Knowledge Navigator die Form eines Desktop-Computers haben, ein PDA sein oder sogar in die Kleidung seines Anwenders integriert werden. Das Konzept des Knowledge Navigators wurde 1987 in einem Video mit dem Titel \"The Knowledge Navigator: Technologies to Get Us There and Beyond\" präsentiert und in einem kurzen Werbefilm der breiten Öffentlichkeit vorgestellt.\n\nEinige von Sculleys Ideen für den Knowledge Navigator wurden im Newton, Apples erstem PDA, verwirklicht.\n"}
{"id": "529525", "url": "https://de.wikipedia.org/wiki?curid=529525", "title": "Farbmanagement", "text": "Farbmanagement\n\nMit Farbmanagement (engl. \"colo(u)r management\") soll erreicht werden, dass eine Vorlage, die mit einem beliebigen Eingabegerät erfasst wurde, an einem beliebigen Ausgabegerät möglichst ähnlich wiedergegeben wird.\n\nEine hohe Ähnlichkeit der Farben zwischen der Ein- und Ausgabe eines Bildes wird Farbtreue genannt. Um diese Farbtreue zu erreichen, werden Farbmanagementsysteme (engl. CMS, \"color management system\") verwendet, die allerdings niemals eine 100 % Übereinstimmung liefern können.\n\nFarbmanagementsysteme benutzen\n\nDie Aufgabe eines Farbmanagementsystems besteht darin, die geräteabhängigen Farbbeschreibungen (der Ein- und Ausgabegeräte) mit Hilfe des geräteunabhängigen Austausch-Farb\"raum\"s ineinander zu konvertieren. Dadurch wird erreicht, dass jedes Gerät in einem Farbmanagementsystem die Farben annähernd gleich darstellt.\n\nEin einfaches Beispiel ist der Ausdruck von farbigen Dokumenten, die mit einem Farbmanagementsystem auf dem Monitor und auf dem Ausdruck annähernd identisch aussehen:\n\nNeben dem L*a*b*-Farbraum, auf dem die gängigen CM-Systeme basieren, gibt es auch andere medienneutrale Farbräume wie etwa L*u*v*, welcher im Gegensatz zu L*a*b eher zur Messung von Lichtfarben benutzt wird. Auch XYZ und xyY sind solche physikalischen Räume, denen gemeinsam ist, alle vom menschlichen Auge wahrnehmbaren Farben, also das sichtbare Licht, darstellen zu können.\n\nColor Management wird beispielsweise häufig in der Druck-, Foto- und Werbeindustrie eingesetzt. Die Nachfrage nach Color-Management-Lösungen steigt nicht nur bei den Profis, sondern auch bei Hobbyfotografen und ambitionierten Amateuren stetig an.\n\nWie jeder Mensch individuell Farben wahrnimmt, haben auch Geräte, zumindest Geräteklassen, unterschiedliche Farbräume, in denen sie Farben registrieren oder darstellen. Eine solche Individualität ist durch Konstruktionsunterschiede und Produktionsschwankungen bedingt. <br>Farbprofile können die Farbdaten einer Geräteklasse oder die Individualität eines speziellen Gerätes widerspiegeln.<br>\nDas Standard-Format für Farbprofile wurde vom ICC (engl. International Color Consortium) entwickelt und in der ISO-Norm 15076 international standardisiert. Jedes an der Konvertierung beteiligte Gerät (Monitor, Digitalkamera, Scanner, etc.) braucht sein eigenes Profil. Es enthält Übersetzungstabellen oder Berechnungsparameter, anhand derer die Konvertierung der Farbdaten vom bzw. in den PCS (\"profile connection space\", Verbindungsfarbraum) erfolgt. Als PCS werden überwiegend XYZ und LAB verwendet.\n\nHinsichtlich ihres Verwendungszweckes unterscheidet man Eingabeprofile (RGB → PCS), Ausgabeprofile (PCS → RGB oder CMYK) und Devicelink-Profile, die ein direktes Gamut-Mapping ohne den Umweg über einen PCS zwischen zwei CMYK-Farbräumen gestatten.\n\nHinsichtlich ihres inneren Aufbaues unterscheidet man Matrix-Profile und LUT-Profile (look up table). Matrix-Profile verwendet man vorzugsweise für Geräte, deren Farbverhalten von relativ wenigen Einflüssen abhängig ist und sich deshalb hinreichend gut z. B. in Form einer 3×3-Umrechungsmatrix beschreiben lässt. Die Dateigröße von Matrixprofilen ist relativ klein (wenige Kilobyte). LUT-Profile finden für Geräte Anwendung, deren Farbverhalten von vielen Faktoren abhängt und zu komplex ist, als dass es sich über eine einfache Matrix-Transformation hinreichend genau beschreiben ließe. LUT-Profile können bis zu mehrere Megabyte groß sein.\n<br>Dabei ist zu berücksichtigen, dass ein Profil immer nur für einen bestimmten Zustand des betreffenden Geräts gilt. Wird also zum Beispiel die Papiersorte von einem weißen auf ein gelbliches Papier gewechselt, so führen dieselben CMYK-Werte zu abweichenden Farben. Ähnliches gilt für Monitore, wenn zum Beispiel am Helligkeitsregler gedreht wird.\n\nDie Profilerstellung basiert auf einer Farbmessung. Dabei werden Farben, deren genaue Farbwerte bekannt sind, vom Gerät wiedergegeben (Monitor, Drucker) oder gemessen (Scanner) und dann mit den bekannten Werten verglichen. Daraus ergibt sich unter anderem der Gamut, der die Fähigkeit der Farbwiedergabe eines Gerätes beschreibt. Je nach Gerätetyp erfolgt die Erstellung von Profilen auf unterschiedliche Art und Weise. Profile müssen regelmäßig neu generiert werden, da sich insbesondere Monitore im Laufe der Zeit verändern. Herstellerprofile etwa sind nur für die Serie, nicht jedoch für das spezifische Gerät passend.\n\nZur Erstellung eines Scannerprofils benutzt man eine Vorlage mit vielen kleinen unterschiedlichen Farbfeldern (IT8-Target), das vom Hersteller mit einem Spektralphotometer vermessen wurde. Der Scanner liest diese Vorlage ein und vergleicht gelesene Farbwerte mit den Referenzwerten des Targets. Aus den Unterschieden dieser Werte wird nun das ICC-Profil berechnet und dadurch ist nun der gerätespezifische Farbraum (RGB-Farbraum) mit einem geräteunabhängigen Farbraum (L*a*b*-Farbraum) derart verknüpft, dass die Farbwerte, die der Scanner liest, farbverbindlich wiedergegeben werden können.\n\nZur Monitorprofilerstellung kommen ein Farbmessgerät und eine dazu passende Software zum Einsatz. Das Farbmessgerät ist mit dem Messcomputer und der Software verbunden und wird normalerweise in der Mitte des Monitors positioniert. Nach dem Start des Messlaufes stellt die Software auf dem Monitor nacheinander Farben dar, deren genauer RGB-Wert der Software bekannt ist. Das Farbmessgerät liefert den CIELab-Wert der tatsächlich sichtbaren Farbe an die Software zurück. Nachdem diese Prozedur für alle RGB-Werte durchlaufen worden ist, kann jeder möglichen RGB-Farbe ein CIELab-Wert zugeordnet werden.\nBeispiel: Die Software stellt ein perfektes Rot = RGB (255,0,0) dar. Das Messgerät liefert zurück, dass der Monitor den Wert CIELab (0.73, 0.26) anzeigt. Damit kann jeder Wert aus RGB nach CIELab übersetzt werden.\n\nNicht alle Monitore sind in der Lage, die RGB-Werte vollständig wiederzugeben. Das führt dazu, dass bei diesen Monitoren verschiedenen RGB-Werten gleiche CIELab-Werte zugewiesen werden.\n\nDiese charakteristischen, gerätespezifischen Eigenheiten der Farbwiedergabe sind der Grund, warum Farbmanagement überhaupt zum Einsatz kommt.\n\nAuch wenn alle Drucker letztendlich nach dem Prinzip der subtraktiven Farbmischung mit den Druckfarben CMYK und ggf. auch weiteren Farben arbeiten, so präsentiert sich die übergroße Mehrheit der für den Privat- und Office-Bereich vorgesehenen Geräte als RGB-Device gegenüber dem Betriebssystem. Die erforderliche Farbseparation (Umrechnung v. RGB nach CMYK) wird vom Treiber oder der Druckerhardware vorgenommen, ohne dass der Anwender darauf Einfluss hat. Lediglich Drucker für professionelle Zwecke, z. B. Proofsysteme oder Großformatdrucker (meist per Postscript angesteuert) erscheinen am System als echte CMYK-Devices.<br> \nDie Erzeugung von ICC-Profilen für Drucker erfolgt, indem man ein Testchart mit vielen Farbfeldern ausdruckt, deren Farbwerte bekannt sind. Anschließend werden die L*a*b*-Werte dieser Farbfelder mit einem Spektralfotometer gemessen. Dadurch wird eine Beziehung zwischen den ausgedruckten RGB- bzw. CMYK-Daten und den sichtbaren CIE-L*a*b*-Farbwerten hergestellt. Es ist also bekannt, welcher Farbeindruck (L*a*b*-Wert) entsteht, wenn eine bestimmte Tinten- oder Toner-Kombination auf diesem Drucker ausgegeben wird. In einem Profilerstellungsprogramm werden die gemessenen Daten in eine Form gebracht, die der Spezifikation des ICC (International Color Consortium) entspricht. Es entstehen standardisierte Tabellen, die eine Umrechnung von RGB bzw. CMYK in den PCS (CIELAB oder XYZ) und umgekehrt erlauben.\n<br>Zu beachten ist, dass man für jede Tinten/Toner- und Papierkombination ein eigenes Profil erstellen muss, um berechenbare und korrekte Druckergebnisse zu erhalten.\n\nEine zweite Möglichkeit zur Drucker-Profilierung bietet die Software höherwertiger Scanner von verschiedenen Herstellern mittels einer Standardprofilierung. So ist, ein vorhandener Scanner vorausgesetzt, keine zusätzliche Hardware erforderlich, um eine ICC-Profilierung des Druckers vorzunehmen.\n\nLiegen die Profile für Eingabe- und Ausgabegerät vor, so kann mit Hilfe des Color Management Moduls (CMM) eine Umsetzung der Farbbeschreibungen erfolgen. Das Color Management Modul ist dabei der Farbrechner, der die Werte aus den Tabellen (Farbprofil) liest und falls erforderlich Anpassungen vornimmt. <br>Auf diese Weise kann Bildpunkt für Bildpunkt eine Umsetzung von RGB-Daten in CIELab-Farbwerte und schließlich in CMYK-Werte für das betreffende Ein- und Ausgabegerät erreicht werden.\n\nMit Hilfe des Farbmanagements ist es möglich, beliebige Geräte miteinander zu kombinieren und trotzdem das jeweils bestmögliche Ergebnis (je nach Möglichkeiten des Ausgabegeräts) zu erhalten. Das hier beschriebene Vorgehen ist heute in der Druckpraxis Standard.\n\n\n\n"}
{"id": "529543", "url": "https://de.wikipedia.org/wiki?curid=529543", "title": "HFS (Dateisystem)", "text": "HFS (Dateisystem)\n\nDas Hierarchical File System (engl. Hierarchisches Dateisystem) ist ein Dateisystem, das von Apple für Computer mit der Macintosh System Software (ab 1996 in Mac OS umbenannt) entwickelt wurde. Es ersetzte 1985 mit der System Software 0.5 (System 2.1) das erst ein Jahr zuvor eingeführte Macintosh File System (MFS).\n\nAuch andere Betriebssysteme, wie BeOS und Linux, haben Lese- und Schreibunterstützung für HFS. Obwohl es ursprünglich für Disketten und Festplatten entworfen wurde, kann man es auch auf nur lesbaren Medien wie CD-ROMs finden. HFS ist ein proprietäres Format. Da es aber sehr gut dokumentiert ist, gibt es in den meisten modernen Betriebssystemen Lösungen, um auf HFS-formatierte Medien zugreifen zu können.\n\nEs wurde 1998 mit der Veröffentlichung von Mac OS 8.1 von HFS+ abgelöst.\n\nHFS wurde im Januar 1986 als neues Dateisystem für Apple-Macintosh-Computer vorgestellt. Die Abkürzung HFS steht für , hierarchisches Dateisystem, also ein Dateisystem mit Unterverzeichnissen – im Gegensatz zum zuvor verwendeten Macintosh File System (MFS), welches als „“ keine Unterverzeichnisse unterstützt und nur von den frühesten Macs benutzt wurde.\n\n1998 stellte Apple HFS+ vor, um ineffiziente Zuweisung von Speicherplatz in HFS anzugehen und weitere Verbesserungen hinzuzufügen. Jedoch kann man seit Einführung von Mac OS X nicht mehr von einem HFS-Volume starten, da die maximale Anzahl Dateien (65.536) auf einem HFS-Volume nicht ausreicht. HFS wird noch von Versionen des Mac OS X Leopard (10.5, 2007) voll unterstützt. In Mac OS X Snow Leopard (10.6, 2009) wurde die Schreibunterstützung entfernt. In macOS Sierra (10.12, 2016) ist HFS nicht mehr enthalten.\n\nHFS unterstützt Dateinamen bis zu einer Länge von 31 Zeichen, Mac-spezifische Metadaten sowie Dual-Fork-Dateien. Beim Dual-Fork-Verfahren wird die eigentliche Datei (data fork) um zusätzliche Informationen ergänzt (resource fork), z. B. Icons. Beide Dateiteile können – jeder für sich getrennt – gelesen und geschrieben werden, dabei wird die „data fork“ meist sequentiell, die „resource fork“ dagegen wie eine Datenbank verwendet. Die Aufteilung ist für den Endbenutzer unsichtbar, jedoch für den Programmierer zugänglich.\n\n\n"}
{"id": "529854", "url": "https://de.wikipedia.org/wiki?curid=529854", "title": "KEduca", "text": "KEduca\n\nKEduca ist ein KDE-Programm zur Erstellung und Durchführung von Multiple-Choice-Tests.\n\nMit dem \"KEduca Builder\" werden Mehrfachauswahl-Tests erstellt, das Punktevergabesystem festgelegt und Grafiken eingefügt.\nMit \"KEduca\" kann ein Benutzer dann solche Tests durchführen. Das Ergebnis wird wahlweise nach jeder Frage oder für alle Fragen gemeinsam am Ende des Tests angezeigt.\n\nKEduca eignet sich zur Vorbereitung auf Multiple-Choice-Prüfungen, zum Beispiel für diverse Zertifizierungen im IT-Bereich.\n\n"}
{"id": "531842", "url": "https://de.wikipedia.org/wiki?curid=531842", "title": "Codename: Linux", "text": "Codename: Linux\n\nCodename: Linux (Originaltitel \"The Code\") ist ein 2001 realisierter, 52-minütiger Dokumentarfilm über die Geschichte von GNU/Linux, freier Software und der Open-Source-Bewegung. Bei dieser finnisch-französischen Produktion führte Hannu Puttonen Regie.\n\nDie Dokumentation zeigt unter anderem Interviews mit Linus Torvalds, Richard Stallman, Eric S. Raymond, Alan Cox, Miguel de Icaza, Theodore Ts’o, David S. Miller, Ari Lemmke und vielen anderen mehr. Des Weiteren enthält die Dokumentation einen chronologischen Verlauf der Entwicklung von Unix und Linux. Mit dabei ist der Free Software Song (mit Übersetzung als Untertitel), welcher unter der GNU Free Documentation License veröffentlicht wurde.\n\nIm deutschen Fernsehen lief die Dokumentation auf arte. Auf DVD ist der Film nur in englischer Sprache (also auch ohne deutschen Untertitel) erhältlich, dafür mit Bonusmaterial, unter anderem einige Musikstücke welche in der Dokumentation selbst nur ausschnittsweise zu hören sind.\n\n"}
{"id": "533654", "url": "https://de.wikipedia.org/wiki?curid=533654", "title": "GNU Screen", "text": "GNU Screen\n\nGNU Screen ist ein Terminalmultiplexer zur Verwendung mit textbasierten Eingabefenstern (Textkonsole) und kommt typischerweise auf UNIX-ähnlichen Betriebssystemen zum Einsatz. Hierbei ist es möglich, innerhalb eines einzigen Zugangs (zum Beispiel über ein Terminal oder eine Terminalemulation) verschiedene virtuelle Terminalsitzungen zu erzeugen. Zusätzlich können direkte Verbindungen zu seriellen Schnittstellen geöffnet und über Z-Modem Dateitransfer durchgeführt werden. Insbesondere können Sitzungen in den Hintergrund geschoben und später fortgeführt werden. Screen stellt somit eine interaktive Version von nohup dar.\n\nDie erste Version von Screen wurde 1987 von Oliver Laumann an der TU Berlin entwickelt und zwischen 1993 und 2003 von Jürgen Weigert und Michael Schröder an der Uni Erlangen vollendet und seitdem von dort aus gewartet.\n\nMit Screen ist es möglich, über einen einzigen Zugang verschiedene Programme (zum Beispiel Editoren, Browser, Mailreader, IRC-Client) parallel zu nutzen und frei zwischen diesen hin- und herzuschalten. Es kann auch als Terminalprogramm zur Kommunikation mit Geräten verwendet werden, die an einer seriellen Schnittstelle angeschlossen sind (z. B. ein Modem oder die serielle Konsole eines anderen Computers). Es ist ebenfalls möglich, den Bildschirm zu teilen, um verschiedene Sitzungen parallel anzuzeigen. Zusätzlich kann eine Sitzung auch von mehreren Personen zugleich genutzt werden.\n\nEin weiteres beliebtes Feature von Screen ist das sogenannte Abtrennen (\"detach\") einer Sitzung. Diese läuft im Hintergrund weiter, während das eigentliche Eingabefenster geschlossen werden kann. Zu einem späteren Zeitpunkt kann diese Sitzung fortgesetzt werden. Diese Möglichkeit wird gerne genutzt, um auf einem dauerhaft mit dem Internet verbundenen System einen Prozess (zum Beispiel einen IRC<nowiki>-Client</nowiki> oder Gameserver) zu starten, ohne dass dieser Prozess sich beendet, wenn die eigene Internetverbindung zum Server getrennt wird (für einfache Prozesse ohne Terminal gibt es für diesen Zweck das vorangestellte Kommando codice_1 in Verbindung mit dem codice_2-Operator).\n\nGNU Screen wird durch einfachen Aufruf (codice_3) gestartet. Es startet dann eine Shell. Optional kann auch ein auszuführendes Programm angegeben werden (z. B. codice_4). Innerhalb von Screen wird + in Verbindung mit weiteren Tastenkombinationen genutzt, um dem Programm Befehle zu erteilen. Nützlich ist z. B. +–, welches eine Übersicht der Tastenkürzel anzeigt. Weitere virtuelle Konsolen werden nun durch die Tastenkombination +– erzeugt. In einer neuen virtuellen Konsole wird jeweils eine Shell gestartet, von der aus weitere Anwendungen ausgeführt werden können. Eine Umschaltung zwischen den verschiedenen Konsolen ist u. a. mit den Tastenkombinationen +– oder +–… möglich. Mit der Tastenkombination +– kann die Sitzung abgetrennt werden. Screen wird dann verlassen, läuft aber im Hintergrund weiter. Der Benutzer kann sich nun ausloggen, und (später) nach erneuter Anmeldung mit dem Befehl codice_5 die Sitzung wiederherstellen.\n\n\n"}
{"id": "534348", "url": "https://de.wikipedia.org/wiki?curid=534348", "title": "Air Wisconsin", "text": "Air Wisconsin\n\nAir Wisconsin Airlines Corporation ist eine amerikanische Regionalfluggesellschaft mit Sitz in Appleton (Wisconsin). Hauptbasis der Air Wisconsin ist der Outagamie County Regional Airport in Greenville (Wisconsin) in der Nähe von Appleton. Sie flog für US Airways unter der Marke \"US Airways Express\". Heute werden die Flugzeuge für American Eagle betrieben.\n\nAir Wisconsin wurde 1965 gegründet, um Appleton mit Chicago zu verbinden, später wurde das Streckennetz über die ganzen USA ausgedehnt. Dies geschah durch Fusionen mit den Fluggesellschaften Mississippi Valley Airlines 1985 und Aspen Airways 1991. 1993 wurde Air Wisconsin von privaten Anlegern aufgekauft. \n\nAir Wisconsin leistete Pionierarbeit beim Codesharing mit United Express, betrieb ihre Flotte an Regional-Jets unter dem Namen United Express und wurde die größte regionale Fluggesellschaft in den 1980er-Jahren. Sie flogen als Zubringer für AirTran unter dem Namen \"AirTran JetConnect\", diese Zusammenarbeit wurde im Juli 2004 eingestellt, ebenso wie die Kooperation mit United Airlines im April 2006.\n\nAir Wisconsin fliegt ausschließlich Regional- und Zubringerflüge für US Airways unter der Dachmarke \"US Airways Express\", nachdem Air Wisconsin 175 Mio. US-Dollar in US Airways investiert hatte. Sie betreibt Stationen in Philadelphia, Washington National und Norfolk/Virginia. \n\nInfolge der 2013 eingeleiteten Fusion von US Airways und American Airlines führt das Unternehmen seinen Flugbetrieb mittlerweile im Auftrag von \"American Eagle\" durch. \n\nMit Stand März 2018 besteht die Flotte der Air Wisconsin aus 65 Flugzeugen des Typs Bombardier CRJ200 mit einem Durchschnittsalter von 15,7 Jahren; sie alle werden für \"American Eagle\" betrieben.\n\n\n"}
{"id": "534937", "url": "https://de.wikipedia.org/wiki?curid=534937", "title": "Minivac 601", "text": "Minivac 601\n\nDer Minivac 601 war ein Lerncomputer, der im Jahr 1961 von der Firma Scientific Development Corporation in Massachusetts produziert wurde. Er bestand aus einer Konsole mit Relais, Schaltern, Steckverbindungen und einem motorisierten Drehschalter zur Ein- und Ausgabe von Dezimalzahlen und wurde zum Preis von 85 Dollar angeboten.\n\n"}
{"id": "535018", "url": "https://de.wikipedia.org/wiki?curid=535018", "title": "Alphatronic PC16", "text": "Alphatronic PC16\n\nDer Alphatronic PC16 von Triumph-Adler war ein äußerst kompakter Bürocomputer. Das Gehäuse ähnelte den typischen Heimcomputern wie z. B. von Commodore oder Schneider/Amstrad CPC. Dennoch war er für ernsthafte Anwendungen gedacht und zum Spielen aufgrund seiner mangelnden Grafik- und Soundfähigkeiten denkbar ungeeignet. Er war als eigenentwickelter Nachfolger des Alphatronic PC gedacht, der ein Zukaufteil war. Als Neuheit kam mit dem 8088 ein 16-Bit-Prozessor von Intel zum Einsatz. Dies ermöglichte die Verwendung von MS-DOS 2.11 von Microsoft als Betriebssystem, das aufgrund der besonderen Hardware speziell an den Rechner angepasst war.\nDiese MS-DOS Version war nicht IBM-kompatibel und ein Grund, dass dieses Gerät vom Markt nicht angenommen wurde. \nCP/M(80) Unterstützung war nur durch den Austausch der Original-CPU Intel 8088 durch die CPU NEC V20 möglich. Als weiteres Betriebssystem stand L2 (Betriebssystem) (Eumel) zur Verfügung. Für Bastler bestand die Möglichkeit über die V24-Schnittstelle einen Einplatinenrechner auf Z8-Basis mit eigenem BASIC zu betreiben.\n\nDie kompakte Bauform begrenzte die Erweiterungsmöglichkeiten enorm. Einen ISA-Bus gab es nicht, und so konnten die vielen ISA-Karten anderer Anbieter nicht verwendet werden. Die Grafikkarte war in einem speziellen Modul untergebracht, das seitlich eingeschoben wurde. Es gab jedoch nur das Standardmodul und ein BTX-Modul, mit dem man den Rechner auch als BTX-Dekoder verwenden konnte. Weiter war noch ein PAL-TV-Modulator erhältlich. Die Tastatur hatte ein nicht standardmäßiges Layout, dem einige Tasten (etwa „Backspace“) fehlten. Auch der etwas spärliche Arbeitsspeicher wurde schnell zu einem Problem, da er nur begrenzt erweitert werden konnte. Ein Floppylaufwerk musste wie schon beim Vorgänger extern angeschlossen werden. Das erste Laufwerk enthielt auch den Laufwerkscontroller und das Netzteil und war daher relativ teuer; ein preiswerteres zweites Laufwerk ohne eigenen Controller konnte an dieses angeschlossen werden. Es war auch möglich, an diesen Bus eine Hardisk mit 10 MB bzw. 20 MB anzuschließen. Der hohe Marktpreis führte jedoch zu einer sehr geringen Nachfrage nach dieser Option.\n\n\n"}
{"id": "536132", "url": "https://de.wikipedia.org/wiki?curid=536132", "title": "Dialogpost Manager", "text": "Dialogpost Manager\n\nDer Dialogpost Manager ist die offizielle Software der Deutschen Post AG zur Portooptimierung und Versandvorbereitung von Mailings. Bis 31. Dezember 2015 hieß die Software Infopost Manager.\n\nDie Software bietet Anredengenerierung, Anschriftenüberprüfung, Dublettenabgleich, Portooptimierung auf Grundlage der Bestimmungen der Deutschen Post AG sowie ein Seriendruckmodul. Alle zur Einlieferung erforderlichen Unterlagen werden automatisch erstellt. Die vom Postkunden erbrachte Vorsortierung wird durch ermäßigte Portosätze honoriert.\n\n"}
{"id": "537879", "url": "https://de.wikipedia.org/wiki?curid=537879", "title": "Rdesktop", "text": "Rdesktop\n\nDas Open-Source-Programm rdesktop erlaubt über eine gewöhnliche Netzwerkverbindung den Fernzugriff (Remote Desktop) von einem Unix-artigen System auf einen Rechner, auf dem Microsoft Windows (von NT4 bis Windows Server 2012 R2) läuft. Es holt den Windows-Desktop eines entfernten Windows-Rechners auf den lokalen Rechner, auf dem ein unixähnliches Betriebssystem oder OS/2 läuft.\n\nDie Verbindung wird mittels des Remote Desktop Protokolls (RDP) hergestellt. Auf dem entfernten Rechner muss der Windows Terminal Server gestartet sein. \n\nIm Unterschied zu anderen Methoden wie VNC, wird hier vom entfernten System aus direkt auf den lokalen Desktop geschrieben. Das ergibt einen deutlichen Performance-Vorteil gegenüber Methoden, bei denen die Ausgabe zuerst auf dem (echten oder virtuellen) entfernten Desktop erfolgt, wo sie dann gescannt und dann zum lokalen System weitergeleitet wird.\n\nDas Computerprogramm läuft auf den meisten Unix-artigen Systemen wie beispielsweise FreeBSD, NetBSD, OpenBSD oder GNU/Linux, wenn dort ein X11-Server zur Verfügung steht. Für OS/2 und eComstation stehen Versionen für XFree86 sowie Presentation Manager zur Verfügung.\n\nEs wurde von Matthew Chapman programmiert und steht unter der GNU General Public License (GPL). Die aktuelle Version ist 1.8.3 vom 13. Aug. 2015.\n\nUnterdessen gibt es mit FreeRDP ein Nachfolgeprojekt, das aktuelle RDP-Funktionalitäten wie z. B. \"remoteFX\" implementiert und unter einer Apache-Lizenz steht. Dieses Projekt wird im Rahmen der \"Open Thin Client Alliance\" von der Open Source Business Alliance finanziell gefördert.\n\n\n"}
{"id": "538586", "url": "https://de.wikipedia.org/wiki?curid=538586", "title": "MythTV", "text": "MythTV\n\nMythTV ist eine freie, unter der GPL lizenzierte Festplattenrekorder-Software. Das Projekt wurde im April 2002 von Isaac Richards gestartet. Geschrieben wurde MythTV für die Betriebssysteme Linux, macOS und BSD. Die aktuelle Version ist 30.0.\n\nIm April 2002 wurde das Projekt von Isaac Richards begonnen.\n\nAm 10. September 2004 wurde die Version 0.16 freigegeben, welche unter anderem um das Modul „mythphone“ erweitert wurde.\n\nAm 10. Mai 2010 wurde die Version 0.23 veröffentlicht. Neben den neuen Designs „Arclight“ und „Childish“ haben die Entwickler das Audio-Framework neu geschrieben.\n\nAm 10. November 2010 stellten die Entwickler die Version 0.24 fertig mit einem neuen OSD, HD-Audio und Blu-ray Unterstützung. Über 200 Fehler wurden in der Version 0.24.1 behoben, die am 16. Mai 2011 freigegeben wurde.\n\nAm 10. April 2012 erfolgte die Freigabe der Version 0.25.\n\nAm 3. Oktober 2012 wurde die stabile Version 0.26 offiziell freigegeben.\n\nAm 18. September 2013 wurde die stabile Version 0.27 offiziell freigegeben.\n\nAm 11. April 2016 wurde die stabile Version 0.28 offiziell freigegeben.\n\nDie Software unterstützt analoge TV-Karten, analoge mit Hardware-Encoder und digitale DVB-T/-S/-C-Karten/USB-Geräte, für die Linux-Treiber existieren. MythTV unterstützt den gleichzeitigen Betrieb mehrerer TV-Karten. Bei DVB-Karten können auch mehrere Sendungen, die auf demselben Transponder gesendet werden, zugleich aufgenommen werden.\n\nSendungen können in folgenden Formaten aufgenommen werden:\n\nMedien wie USB-Sticks werden automatisch erkannt und ggf. eingebunden. UPnP wird unterstützt. Werbeblöcke können automatisch erkannt und gelöscht werden.\n\nMythTV unterscheidet sich von vergleichbaren Open-Source-Programmen darin, dass es einem Client-Server-Modell entspricht. So kann ein Rechner die Aufnahmen verarbeiten und speichern, während eine Vielzahl von Rechnern über ein Netzwerk die Aufnahmen abspielen kann. Der Client, unter MythTV Frontend genannt, ist für Linux, Windows (eingeschränkt, externes Projekt) und macOS verfügbar und kann über Vorlagen im Aussehen geändert werden. Selbst mehrere Server mit TV-Karten können zusammengeschaltet werden. Die Clientsoftware nutzt OpenGL oder Qt zur Darstellung der Menüführung und VDPAU zur Wiedergabe über eine Nvidia Grafikkarte.\n\nIm Laufe der Zeit hat sich MythTV zu einem vielseitigen Produkt entwickelt, vor allem dank seiner Plug-in-Struktur. Diese Struktur ermöglicht es, Erweiterungen zu MythTV unabhängig von der Grundfunktionalität zu entwickeln. Programminformationen werden mittels xmltv aus dem Internet geladen oder per DVB bezogen und in der MySQL-Datenbank von MythTV gespeichert.\n\nUnter anderem werden die folgenden Plugins offiziell unterstützt:\n\nDas Basissystem integriert sich reibungslos mit den Modulen. Die Bedienung erfolgt über die Tastatur oder, viel komfortabler, mittels einer Fernbedienung. Zusätzlich kann die Maus mit Mausgesten in den Menüs genutzt werden.\n\nFolgende Linux-Distributionen haben MythTV standardmäßig installiert:\n\n\n\n"}
{"id": "539817", "url": "https://de.wikipedia.org/wiki?curid=539817", "title": "Forsyth-Edwards-Notation", "text": "Forsyth-Edwards-Notation\n\nDie Forsyth-Edwards-Notation (FEN) oder in der erweiterten Form (X-FEN) ist eine Kurznotation, mit der jede beliebige Brettstellung im Schach niedergeschrieben werden kann.\n\nFEN basiert auf einem System, das der schottische Zeitungsjournalist David Forsyth entwickelt hat. Dieses System wurde im 19. Jahrhundert populär. Es wurde durch Steven J. Edwards erweitert, um dadurch auch die Nutzung durch Computer zu unterstützen. FEN ist integraler Bestandteil der Portable Game Notation, in der es für die Eingabe von Startaufstellungen eingesetzt wird, die von der Partieanfangsstellung abweichen.\n\nEine FEN ist in sechs Gruppen aufgeteilt, die durch Leerzeichen voneinander getrennt sind. Diese geben jeweils an:\n\nZum Beispiel wird die Startposition einer Schachpartie in FEN (siehe Diagramm 1) so angegeben:\n\nDie erste Gruppe beschreibt die Positionen der Figuren auf dem Brett. Dies geschieht reihenweise von oben links (Feld a8) bis unten rechts (Feld h1). Jede Reihe wird durch einen Schrägstrich von der nächsten getrennt. Jede Reihe besteht aus Buchstaben für die Figuren und Zahlen für die Anzahl der Leerfelder. Die Summe der Figuren und Leerfelder in einer Reihe muss genau acht ergeben. Weiße Figuren werden in Groß-, schwarze Figuren in Kleinbuchstaben angegeben. Für die Figuren werden die Abkürzungen der englischen Bezeichnungen verwendet („r“ = Rook (Turm), „n“ = Knight (Springer), „b“ = Bishop (Läufer), „q“ = Queen (Dame), „k“ = King (König), „p“ = Pawn (Bauer)). Aufeinanderfolgende leere Felder in einer Reihe werden durch eine Zahl angegeben (mindestens „1“, höchstens „8“).\nZum Beispiel ergibt ein weißer König auf e4 und ein schwarzer König auf c5 folgende erste Gruppe (siehe Diagramm 2):\n\nDie zweite Gruppe gibt den Spieler an, der in dieser Stellung am Zug ist. Mögliche Zeichen sind w für Weiß (engl.: \" white\") und b für Schwarz (engl.: \" black\").\n\nDie dritte Gruppe gibt die noch erlaubten Rochaden an. Hier bedeutet\n\n\nDabei ist die Reihenfolge KQkq einzuhalten. Die Buchstaben, die nicht mehr möglichen Rochaden entsprechen, werden ausgelassen, und der Strich „-“ wird dann und nur dann geschrieben, wenn keine der vier Rochaden mehr möglich ist.\n\nDie vierte Gruppe gibt einen möglichen en-passant-Schlag an. Sofern im letzten Zug ein Bauer zwei Felder vorgerückt ist, wird das übersprungene Feld angegeben, unabhängig davon, ob ein en-passant-Schlag auf dieses Feld tatsächlich möglich ist oder nicht. Sonst wird „-“ angegeben.\n\nBeispiel: Nach Bauer f2–f4 wird in der FEN in der 4. Gruppe „f3“ angegeben.\n\nIn der fünften Gruppe wird die Anzahl der Halbzüge seit dem letzten Bauernzug oder des Schlagens einer Figur angegeben. Dieser Wert ist wichtig, um die 50-Züge-Remisregel zu überwachen. Mögliche Werte sind eine Null oder eine positive ganze Zahl.\n\nIn der sechsten Gruppe wird die Nummer des nächsten Zuges angegeben. In der Ausgangsstellung ist der Wert 1. Nach jedem Zug von Schwarz wird er um 1 erhöht. Damit gibt man die Zahl der zuvor gespielten Züge an, damit die Folgezüge in der Notation richtig nummeriert werden können.\n\nZur Darstellung aller denkbaren Stellungen im Chess960 (8×8-Brett) oder Capablanca-Random-Chess (10×8 -Brett) (CRC) reicht die klassische FEN nicht aus. Deshalb ist hierzu eine abwärtskompatible Erweiterung der herkömmlichen FEN entwickelt worden. Das bedeutet: Alle Stellungen, die bislang bereits mittels FEN kodierbar sind, mussten durch eine Erweiterung in einer 100 % identischen Form kodiert werden. Die von Reinhard Scharnagl 2003 eingeführte \"X-FEN\" leistet dieses. X-FEN (vormals FRC-FEN) ist seit Jahren bewährt.\n\nPartien werden im PGN-Format (Portable Game Notation) dargestellt. Mit Ausnahme herkömmlicher Schachpartien müssen bei einer Speicherung von Chess960- oder CRC-Partien (Capablanca-Random-Chess) auch die jeweiligen Startstellungen mit aufgenommen werden. Das geschieht definitionsgemäß bei traditionellen Schachpartien mittels eines SetUp-Tags und eines FEN-Strings, bei Chess960- und CRC-Partien somit in kompatibler Weise über einen X-FEN-String.\n\nGrundlage der X-FEN ist die herkömmliche FEN. Sie unterscheidet sich nur in der Art, wie Rochade-Tags und das e.p.-Tag verwendet werden. Außerdem unterstützt sie 10×8 Stellungen, die José Raúl Capablancas erweiterten Figurensatz (zusätzliche Figuren Kanzler und Erzbischof) verwenden.\n\nZur Verbesserung der Eindeutigkeit wird das e. p.-Feld dann und nur dann angegeben, wenn ein Bauer, der unmittelbar zuvor einen Doppelschritt ausgeführt hat, neben einem gegnerischen Bauern steht, so dass auch tatsächlich ein pseudolegaler e. p.-Schlag möglich ist. Dabei wird nicht geprüft, ob der Schlag auch legal ist, d. h. nach Ausführung des Schlags der König der schlagenden Partei nicht bedroht wäre.\n\nEs werden die aus der FEN bekannten Rochadetags „KQkq“ verwendet. Wie üblich bedeuten kleine Buchstaben schwarze Rochaderechte und große Buchstaben weiße Rochaderechte. „Kk“ bezeichnen das Recht zur \"g-Rochade\" (bzw. \"i-Rochade\" auf einem 10×8 Brett), „Qq“ jenes zur \"c-Rochade\".\nFür den Fall, dass in der beschriebenen Stellung beide Türme der betreffenden Partei auf der gleichen Seite des Königs und auf der Grundreihe stehen, gilt ein mit k/K/q/Q angegebenes Rochaderecht für den weiter außen, d. h. weiter vom König entfernt stehenden Turm. Sollte jedoch der innere Turm das Rochaderecht besitzen, dann (und nur dann) wird statt einem K/k oder Q/q der Spaltenbuchstabe des betreffenden Turms angegeben, bei Weiß wieder als Groß- und bei Schwarz als Kleinbuchstabe. Dieser Fall kann sich aus der normalen Grundstellung (Türme auf der a- und h-Linie) niemals ergeben, so dass dann die Angabe stets mit k/K/q/Q erfolgt, wodurch X-FEN abwärtskompatibel zur bisherigen FEN ist.\n\nGewöhnlich ist das Rochadeziel des Königs entweder zwei Felder vom linken (weißen) Rand, oder ein Feld vom rechten Rand entfernt. Es gibt jedoch auch Varianten mit symmetrisch verteilten Zielfeldern (z. B. Janusschach), beide je ein Feld vom Rand entfernt. Dann wird dem Rochadeblock ein „s“ vorangestellt. Ein anderes Präfix „m“ bedeutet: moderne Rochade (z. B. Embassy Chess oder Chess480). Hierbei zieht der König eine normale Rochadedistanz (8×8: 2 Schritte, 10×8: 3 Schritte) zur Seite, maximal jedoch bis unmittelbar vor den Rand.\n\nZehn aufeinander folgende freie Felder in einer Reihe werden mit „10“ kodiert, neun mit „9“. Der \"Erzbischof\" (Springer + Läufer) erhält den Buchstaben „A“ (engl. \"Archbishop\"), ein \"Kanzler\" (Springer + Turm) bekommt „C“ (engl. \"Chancellor\"). Schwarze Figuren werden hierbei in gewohnter Weise mit Kleinbuchstaben symbolisiert. 10×8-Beispiel: Capablanca-Random-Chess.\n\nSollte man nur klassische Schachpartien aus einer PGN-Datei berücksichtigen wollen (ein mit dem Shuffle Chess bereits vorhandenes Problem), so ist lediglich darauf zu achten, nur PGN-Einträge ohne vorhandenen FEN-Tag auszuwählen.\n\nBeispiel für X-FEN Bedarf im Chess960. Rochaderecht innerer Turm.\nPGN:\n\nNach 10 Zügen, vor 11. 0–0, besteht das Rochaderecht für Weiß mit dem inneren Turm auf g1, also muss es mit einem G statt K angegeben werden:\n\nX-FEN:\n\nWährend X-FEN den Spagat zwischen FEN-Abwärtskompatibilität und Flexibilität für moderne Schachvarianten bewältigt, und dies mit einer relativ hohen Komplexität erkauft, wurde für die Chess960-Fähigkeit von Schachprogrammen eine weitere „kleine“ Lösung namens Shredder-FEN entwickelt.\n\nDie Shredder-FEN bricht mit der 100%igen Abwärtskompatibilität zu FEN und kodiert in den Rochaderechten einfach die Ausgangslinien der Türme, d. h. beispielsweise HAha statt KQkq, wenn die Türme ursprünglich auf der A- und H-Linie stehen.\n\nDie beiden Führer auf dem Markt der Schachprogramme, ChessBase/Fritz und Shredder, können in ihren aktuellen Versionen nur mit Shredder-FEN umgehen. Andere Schach-Engine-GUIs (z. B. Arena) verstehen beide Formate.\n\n\n"}
{"id": "541493", "url": "https://de.wikipedia.org/wiki?curid=541493", "title": "Failover", "text": "Failover\n\nEin Failover (vom englischen \"\" für die „Ausfallsicherung“) ist der ungeplante Wechsel zwischen zwei oder mehreren Netzwerkdiensten bei einem einseitigen Ausfall. Als Folge können die Dienste trotz des Ausfalls eines der Systeme hochverfügbar gehalten werden.\n\nUnternehmenswichtige Anwendungen wie Datenbanken, Webserver oder E-Mail können auf diese Weise auch bei einem Ausfall eines Rechners weiter zur Verfügung gestellt werden, da das Zweitsystem im Fehlerfall die Aufgaben des Primärsystems übernimmt.\n\nMeist überwachen sich beide Server mit einem Heartbeat gegenseitig.\n\nDa bei einem bewussten Übertragen der Dienste von einem Clusterknoten auf einen anderen nicht von einem Fehler (Failover) gesprochen werden kann, bezeichnet man diesen Vorgang auch als Switchover.\n\nEin Failover-Cluster oder Aktiv/Passiv-Cluster ist ein Verbund von mindestens zwei Computern (Cluster), in dem bei einem Ausfall eines Rechners ein zweiter Rechner dessen Aufgaben übernimmt.\n\nDas aktive System ist das Primärsystem, die wartenden passiven Systeme sind die Backup- oder Standby-Systeme.\n\nDamit der Failover automatisiert und ohne Eingriff eines Administrators erfolgen kann, ist der Einsatz einer Cluster-Manager-Software oder eines Loadbalancer erforderlich.\n\nWeiterer Vorteil eines Failover-Clusters ist die Möglichkeit, ein System zu Wartungszwecken bewusst außer Betrieb zu nehmen, wobei die Redundanz während dieses Zeitraumes bei einem 2-Server-Cluster nicht mehr gegeben ist.\n"}
{"id": "542140", "url": "https://de.wikipedia.org/wiki?curid=542140", "title": "Sibelius (Software)", "text": "Sibelius (Software)\n\nSibelius ist ein Notensatzprogramm, benannt nach dem finnischen Komponisten Jean Sibelius. Herstellung und Vertrieb leistet das Unternehmen Avid Technology, welches das Programm im Jahr 2006 von der Sibelius Software Ltd. der Brüder Ben und Jonathan Finn erworben hat. Es ist für Windows und macOS verfügbar.\n\nSibelius ermöglicht einen professionellen Notensatz. Traditionelle Notation wird ohne Einschränkungen (etwa an Zahl oder Art der Instrumente) voll unterstützt; auch ausgefallene oder nicht allgemein gebräuchliche Notationsformen sind möglich. Beim Erstellen von Orchesterpartituren sind die Einzelstimmen automatisch verfügbar. Die Noteneingabe erfolgt entweder durch Einspielen über eine MIDI-Tastatur, durch Tastaturbefehle direkt auf dem Rechner oder durch Mausklicks. Editierfunktionen können durch selbst erstellbare Plugin-Programme automatisiert werden. Das Programm bietet auch Funktionen zum Abspielen einer Partitur mit Berücksichtigung der Spielanweisungen (Dynamik, Tempo usw.), wobei auf eine umfangreiche Klangsamplebibliothek zurückgegriffen wird. Ferner gehört zum Lieferumfang eine vereinfachte Version des Programms \"PhotoScore\" zum Scannen von gedruckten Notenvorlagen und Umwandeln in das Sibelius-Format.\n\nSibelius-Dateien können im World Wide Web verteilt werden und mit Hilfe des kostenlos für jedermann erhältlichen Webbrowser-Plugins \"Scorch\" auch ohne das eigentliche Sibelius-Programm angezeigt, gedruckt und abgespielt werden. Unter dem Namen \"Avid Scorch\" existiert ebenfalls ein Anzeige-Programm von Sibelius-Dateien für das iPad.\n\nSibelius ist in der Lage, die folgenden Formate zu öffnen:\n\nSibelius ist in der Lage, folgende Formate zu speichern:\n\nIn weitere Formate kann mit Hilfe von Plug-ins exportiert werden.\n\nSibelius ist in folgenden Versionen erhältlich:\n\nDie Arbeit an Sibelius begann 1986 als Freizeitprojekt der britischen Zwillingsbrüder Ben and Jonathan Finn, die während ihres Musikstudiums an der Universität die ersten Zeilen konzipierten und programmierten. Die Implementierung basierte anfangs auf Assembler und wurde später komplett neu in C++ geschrieben.\n\nDie erste offizielle Version erschien 1993 unter dem Namen \"Sibelius 7\" für den Acorn Archimedes. Der Markt war zu dieser Zeit stark von dem Programm Finale des Unternehmens CodaMusic beherrscht. Seitdem wurde die Software stetig zu einem der leistungsfähigsten und meistbenutzten Notation<nowiki></nowiki>sprogramme weiterentwickelt. Der Namenszusatz \"7\" entfiel später. Die an den Namen angehängte Ziffer bezieht sich heute auf die jeweils aktuelle Versionsnummer. Die aktuelle Version \"Sibelius 7\" für Windows und Mac ist hinsichtlich ihrer Namensgebung also doppeldeutig.\n\nIm Jahr 2006 kaufte Avid die sich im Besitz der Finns befindende Firma \"Sibelius Software Inc.\" auf. Das Programmiererteam wurde daraufhin zwar verkleinert, doch verblieb die Weiterentwicklung und Pflege bei derselben Firma, verkörpert vom \"öffentlichen Gesicht\" des seit 1999 involvierten Produktmanagers Daniel Spreadbury. Die Mitarbeiter unterlagen von nun an dem Angestelltenverhältnis bei Avid.\n\nIm Juli 2012 gab Avid bekannt, zukünftig wieder seine Kernkompetenzen fokussieren zu wollen, was mit umfassenden Firmen-Umstrukturierungen und Verkäufen von Tochtergesellschaften einherging. Als Folge dessen wurde auch die UK Sibelius-Zentrale am Finsbury Park in London vollständig geschlossen und ihre Mitarbeiter wurden entlassen. Avid erklärte daraufhin, dass die Weiterentwicklung von Sibelius zukünftig einem Team in der Ukraine übertragen werden solle.\n\nDie Übergangszeit von 2012 bis 2013 koordinierte Bobby Lombardi, Produktmanager von Pro Tools. Währenddessen versuchte eine breit angelegte und von Sibelius-Nutzern organisierte Initiative vergeblich, Avid zum Verkauf der Sibelius-Sparte bzw. zur Wiedereinstellung der Londoner Entwickler zu bewegen. Anfang 2013 wurde bekannt, dass das alte Entwicklerteam von Sibelius nun für Steinberg an einer neuen Notensatzsoftware arbeitet; diese neue Software erschien im Oktober 2016 unter dem Namen \"Dorico\".\n\nDas neue in Kiew wirkende Entwicklerteam um Senior Product Manager Sam Butler und Softwareentwickler Michael Ost wurde Ende 2013 vorgestellt.\n\nEine Installation unter Firefox wird bei Sibelius First nicht unterstützt (keine captcha-Funktion).\n\n\n"}
{"id": "542937", "url": "https://de.wikipedia.org/wiki?curid=542937", "title": "Cray-1", "text": "Cray-1\n\nDie Cray-1 war der erste Supercomputer der Firma Cray, dessen Architektur vom Team um Seymour Cray entwickelt wurde. Seymour Cray war dabei für die Technologie der Vektor-Register zuständig. Die erste Cray-1 wurde 1976 am Los Alamos National Laboratory in Betrieb genommen.\n\nDas Los Alamos National Laboratory erhielt 1976 die erste Cray-1 für 8,8 Millionen US-Dollar (entspricht einem heutigen Wert von ungefähr  $) und nutzte den Supercomputer u. a. für Kernwaffentestberechnungen. Das National Center for Atmospheric Research (NCAR) war im Juli 1977 der erste offizielle kommerzielle Kunde der Firma Cray. NCAR bezahlte 8,86 Millionen US-Dollar, davon 1 Million für den Speicher. Diese Cray-1 wurde erst 1989 außer Betrieb genommen.\n\nIn Europa bekam im Herbst 1978 das \"European Centre for Medium Range Weather Forecasts\" (ECMWF) die erste Cray. Das Zentrum nahm mit diesem \"Number Cruncher\" 1979 mit der ersten 10-tägigen Wettervorhersage seinen Betrieb in Reading bei London auf.\n\nMit Preisen zwischen 5 Mio. und 8 Mio. $ wurden ungefähr 80 Cray-1 weltweit verkauft. Das US-amerikanische Air Force Systems Command (AFSC) nutzte eine Cray-1 Anfang der 1980er Jahre auch zur Berechnung von Laserentwicklungen.\n\n1982 wurde die Cray-1 durch die 500 MFLOPS schnelle Cray X-MP abgelöst, die erste Multiprozessormaschine der Firma Cray. 1985 kam dann die sehr fortgeschrittene Cray-2 auf den Markt. Diese war schon in der Lage, 1,9 GFLOPS in der Spitze zu erreichen. Der Erfolg dieser Variante blieb jedoch aus, sodass eine etwas konservativere Maschine als Nachfolger der Cray-1 und der Cray X-MP entwickelt wurde, die ab 1988 unter dem Namen Cray Y-MP verkauft wurde.\n\nDie Cray-1A wog einschließlich des Freon-Kühlsystems 5,5 Tonnen. Um die Kabellängen innerhalb des Gehäuses kurz zu halten (es gab keine Kabel, die länger als 1,2 Meter waren) hatte sie eine Hufeisenform. Die Cray-1A war ein Vektorrechner und beinhaltete zusätzlich 200.000 spezialisierte ECL-Schaltkreise.\nAusgestattet mit einer für damalige Verhältnisse enormen Taktfrequenz von 80 Megahertz, 64 Vektor-Registern in der Wortbreite von 64 Bit sowie einer Million sehr schnellen 64 Bit breiten Speichern (entsprechend 8 Megabyte RAM), erreichte die Cray-1A über 80 Millionen Gleitkommazahl-Operationen pro Sekunde (FLOPS). Mit einer späteren Variante der Cray-1 wurde dieser Wert sogar auf 133 MFLOPS erhöht. Vor allem durch die eine Million Speicherzellen verbrauchte die Cray-1A einschließlich Stromversorgung 115 kW. Wenn man die Kühlung dazurechnet, wird der Wert annähernd verdoppelt.\n1978 wurde das erste Standardsoftwarepaket für die Cray-1 herausgegeben. Es bestand aus\n\n\n"}
{"id": "543602", "url": "https://de.wikipedia.org/wiki?curid=543602", "title": "KBabel", "text": "KBabel\n\nKBabel ist ein grafisches Tool für KDE zur Bearbeitung und Verwaltung von gettext PO Dateien.\nEs hilft bei Übersetzungen von Dokumentationen und GUIs. Des Weiteren verfügt \"KBabel\" über eine integrierte Rechtschreibprüfung und einen Katalogmanager, der das Arbeiten mit einem CVS erleichtert.\n\nFür KDE SC 4 wurde Lokalize als Nachfolger von KBabel entwickelt\n"}
{"id": "544259", "url": "https://de.wikipedia.org/wiki?curid=544259", "title": "EasyLinux", "text": "EasyLinux\n\nEasyLinux war eine vierteljährlich erscheinende Fachzeitschrift, die sich zum Ziel setzte, den Einstieg in Linux bzw. den Umstieg von Windows zu erläutern. Sie gehörte zur Familie der Linux-Zeitschriften der Computec Media Group und wurde 2003 von Hans-Georg Eßer gegründet, der auch bis zuletzt ihr Chefredakteur war. Die Zeitschrift erschien von Juni 2003 bis Oktober 2018.\n\nIm März und April 2003 wurden vorab zwei sogenannte „Starter Kits“ mit CDs oder DVDs von den zwei damals größten Distributionen SuSE und Red Hat Linux veröffentlicht. Im Juni 2003 erschien dann erstmals das EasyLinux-Magazin selbst, das auf die Starter Kits Bezug nahm. Sowohl Installationsanleitungen für die Starter Kits als auch Anleitungen für den Gebrauch von Linux für Anfänger waren in dem Magazin zu finden. Kurz darauf wurde auch Mandrake als Starter Kit mit aufgenommen.\n\nIm Juni 2005 fand eine Umorientierung statt: Mit jedem Heft wurden CDs mit Open-Source-Programmen und DVDs mit Sicherheitsupdates für die jeweilig berücksichtigte Distribution herausgegeben. Gleichzeitig wurde Fedora Core als Distribution ersatzlos gestrichen und mehr Bezug auf debianbasierte Anwendungen wie Knoppix und Ubuntu genommen. Mit dem Heft 2/2006 wurde sowohl das Layout als auch die inhaltliche Ausrichtung überarbeitet. Am 13. Juli fand erneut eine Umorientierung statt: Neben einem wiederum neuen Layout, wurde die Ausgabenzahl auf 4 Hefte im Jahr begrenzt. Das wurde mit Anzeigen- und Umsatzrückgang begründet.\n\nMit dem Erscheinen von Ausgabe 8 – 10/2018 wurde die Zeitschrift eingestellt. Die Themen werden in der Schwesterzeitschrift LinuxUser weitergeführt. Dort gibt es ab Ausgabe 11/2018 eine „EasyLinux“-Rubrik.\n\nArtikel, die älter als ein Jahr waren, sind über die EasyLinux-Webseite in der Regel kostenlos erhältlich; bei jüngeren Ausgaben trifft das nur auf eine Auswahl von Texten zu. Seit Mitte 2009 erschien zeitgleich mit dem Heft eine Community-Edition der Zeitschrift, die im PDF-Format 32 Seiten aus dem aktuellen Heft mit demselben Layout wie in der gedruckten Version anbietet. Das Archiv neuerer Artikel (ab 2009) liegt (gemeinsam mit dem LinuxUser-Archiv) auf dem Portal Linux-Community.\n\n"}
{"id": "545333", "url": "https://de.wikipedia.org/wiki?curid=545333", "title": "BeBox", "text": "BeBox\n\nDie BeBox war ein Personal Computer der Firma Be Incorporated, der zusammen mit dem Betriebssystem BeOS ausgeliefert wurde. Die BeBox war ein System mit zwei Mikroprozessoren (Dualprozessorsystem) vom Typ PowerPC, das in einem blauen Designgehäuse der Firma frog design steckte. Auffällig dabei waren zwei LED-Reihen an der Gehäusefront, die die Auslastung der Prozessoren anzeigten.\n\nDie BeBox war von Anfang an ein Mehrprozessorsystem. Ursprünglich sollte sie auf zwei Hobbit-Prozessoren von AT&T basieren. Nachdem die Produktion des Hobbit-Prozessors jedoch eingestellt worden war, lange bevor mit der Fertigung der BeBox begonnen wurde, wählte man als Ersatz den PowerPC-Prozessor. Die erste Version der BeBox, die am 3. Oktober 1995 erschien, verwendete zwei Prozessoren vom Typ PowerPC 603 mit jeweils 66 MHz. Ab August 1996 wurden dann zwei Prozessoren vom Typ PowerPC 603e mit 133 MHz Takt verwendet. Am 31. Januar 1997 wurde bekanntgegeben, dass die Produktion der BeBox eingestellt wird. Insgesamt wurden weltweit etwa 1000 BeBoxen mit 66-MHz-Prozessor und etwa 800 mit 133-MHz-Prozessor ausgeliefert. Es wurden auch einige Prototypen mit mehr als zwei Prozessoren hergestellt, die jedoch nie in Serie gingen.\n\nDie BeBox bestand aus zwei PowerPC-Prozessoren, anfangs mit 66 MHz und später mit 133 MHz. Ein Second-Level-Cache war jedoch nicht vorhanden, da der verwendete Prozessor-Controller entweder einen Prozessor und dessen Cache oder zwei Prozessoren verwalten konnte. Für Arbeitsspeicher stellte die BeBox 8 SIMM-Steckplätze zur Verfügung, diese konnten jeweils maximal einen Speicherriegel mit einer Kapazität von 32 MB aufnehmen, was einen Speicherausbau bis auf 256 MB erlaubte. Weiter besaß die BeBox 5 ISA- und 3 PCI-Steckplätze für Erweiterungskarten. Standardmäßig wurde eine S3-Grafikkarte mitgeliefert.\nZum Anschluss der Festplatte und des CD-ROM-Laufwerks diente ein Fast-SCSI-Adapter, zusätzlich war noch ein Diskettenlaufwerks- und ein IDE-Adapter vorhanden.\n\nAn externen Anschlüssen stellte die BeBox einen Anschluss für DIN-Tastaturen, eine PS/2-Schnittstelle für Maus, vier RS-232-Schnittstellen, zwei Joystick-Anschlüsse, zwei MIDI-In/-Out, drei Infrarot-Anschlüsse, einen Parallelport und den \"GeekPort\" zur Verfügung. Der GeekPort war eine 37-polige Schnittstelle, die analoge und digitale Ein-/Ausgänge sowie verschiedene Betriebsspannungen bereitstellte.\nFür Tonbearbeitung stellte die BeBox Cinch-Buchsen für Line-In/-Out sowie eine Klinkenbuchse für Mikrofon und Kopfhörer bereit. Dank des CS4231-Codecs von Crystal war dabei an all diesen Ein- und Ausgängen CD-Qualität möglich.\n\nDer GeekPort ist ein 37-poliger Anschluss, der verschiedene Betriebsspannungen, 16 direkt programmierbare I/O-Leitungen und vier A/D-Paare bietet. Die Abtastrate beträgt 100 kHz.\nAls Zielgruppe waren, wie der Name andeutet, interessierte Bastler angedacht, die ihre Computer mit selbstgebauten Teilen erweitern wollten.\n\nAuf den anfangs mit den Developer Releases von BeOS ausgelieferten BeBoxen war es problemlos möglich, auch spätere Versionen von BeOS zu installieren, jedoch unterstützten erst die BeBoxen ab Revision 6 sämtliche Versionen von BeOS, einschließlich BeOS 5. Auf einer neueren BeBox eine ältere Version von BeOS (vor Release 3) zu installieren, ist jedoch nicht ganz einfach, da bei diesen frühen Versionen häufig das Flash-ROM der BeBox aktualisiert wurde, so dass dieses mit älteren BeOS-Versionen (vor DR7) nicht mehr zurechtkommt.\n\n"}
{"id": "547790", "url": "https://de.wikipedia.org/wiki?curid=547790", "title": "Newton-Fraktal", "text": "Newton-Fraktal\n\nDas Newton-Fraktal zu einer nicht-konstanten meromorphen Funktion formula_1, die die komplexen Zahlen in sich abbildet, ist eine Teilmenge der Menge der komplexen Zahlen. Genauer ist es die Julia-Menge formula_2\nzur Funktion\ndie das Newton-Verfahren zum Auffinden von Nullstellen der Funktion formula_1 beschreibt. Das Newton-Verfahren selbst konstruiert aus einem Startwert formula_5 eine Folge mit der Rekursionsvorschrift formula_6.\n\nAbhängig vom Startwert formula_7 kann der Orbit von formula_8\nganz unterschiedliches Verhalten zeigen.\n\n\"Anmerkung:\" Hier bezieht sich der Exponent auf formula_10 als Funktion, und nicht auf deren Funktionswert. formula_11 bedeutet also die formula_12-fach iterierte Anwendung von formula_10 auf formula_8 (oder das formula_12-te Iterierte von formula_10), formal also formula_17.\n\nFür die Dynamik in einer Umgebung von formula_8 gibt es genau zwei Möglichkeiten:\nDie Punkte im ersten Fall bilden die Fatou-Menge formula_24 von formula_10, die Punkte im zweiten Fall die Julia-Menge formula_2. In der Fatou-Menge kann es insbesondere vorkommen, dass die Folge der Abstände gegen null konvergiert, sich die Orbits von Punkten formula_27 also dem Orbit von formula_8 annähern. Falls formula_1 mindestens drei Nullstellen hat, ist die Julia-Menge formula_2 immer ein „Fraktal“; daher wird formula_2 gelegentlich auch „\"Newton-Fraktal\" von formula_1“ genannt.\n\nLiegt der Startwert der Newton-Iteration nahe an einer einfachen Nullstelle von formula_1, dann konvergiert das Verfahren quadratisch gegen diese Nullstelle (d. h., die Anzahl der korrekten Dezimalziffern verdoppelt sich langfristig in jedem Schritt). Bei einer mehrfachen Nullstelle ist das Newton-Verfahren immerhin noch linear konvergent. Nullstellen liegen immer in der Fatou-Menge.\n\nJe näher der Startwert jedoch an der Julia-Menge liegt, desto unüberschaubarer ist das Resultat des Newton-Verfahrens:\n\nÜberraschenderweise kann die Julia-Menge (das Newton-Fraktal) auch positives Maß in der Ebene haben: das heißt, dass zufällige Startwerte in der Julia-Menge liegen und damit nicht gegen eine Nullstelle (oder einen anderen anziehenden Zyklus von formula_10) konvergieren (dieses Verhalten kommt nur in wenigen Fällen vor und wurde auch erst vor wenigen Jahren bewiesen, ist aber durchaus möglich). Selbst wenn das Newton-Fraktal eine Nullmenge ist, kann es also ganze Gebiete geben, in denen das Verfahren nicht gegen eine Nullstelle konvergiert.\n\nDiese Feststellung gilt auch für reellwertige rationale Funktionen. Wiederum dient das Polynom formula_34 als Beispiel. Weil es reelle Koeffizienten hat, bleiben die Werte der Newton-Iteration für reelle Startwert reellwertig. Da die reelle Achse durch Gebiete der Nichtkonvergenz verläuft, gibt es Intervalle, für die keine Konvergenz vorliegt. Von solchen Intervallen gibt es unendlich viele.\n\nDie Abbildung rechts zeigt das Newton Fraktal (in weiß) zu formula_34, farbcodiert nach Konvergenzgeschwindigkeit und den drei Nullstellen. Startwerte, die in den beige gezeichneten Gebieten liegen, konvergieren gegen die gleiche Nullstelle (im Bild links in hellbeige), analog für das grüne und das blaue Gebiet. Die Nullstellen zum grünen bzw. blauen Gebiet liegen symmetrisch zur waagerechten Symmetrieachse rechts im Bild. Je schneller ein Startwert zu seiner Nullstelle konvergiert, desto heller ist er eingefärbt. Die Werte in den unendlich vielen roten Bereichen konvergieren nicht gegen eine Nullstelle, sondern werden vom anziehenden Zyklus formula_38 eingefangen. Das Newton-Fraktal – im Bild als helle Struktur erkennbar – ist nicht beschränkt. In den drei zu erkennenden Richtungen reicht es bis nach ∞.\n\nDie Abbildung rechts zeigt das Newton-Fraktal zu einem Polynom mit 7 zufällig gewählten Nullstellen (weiße Punkte), der Bereich stellt formula_39 dar. Das Fraktal selbst ist z. B. der Rand des gelben Gebietes. Ebenso ist es der Rand des grünen Gebietes, der Rand des türkisfarbenen Gebietes, etc. Diese Eigenschaft ist allen Julia-Mengen gemein. (Die Farben rot und pink wurden doppelt verwendet; dennoch entsprechen auch die Grenzen des roten bzw. des pinkfarbenen Gebietes dem Newton-Fraktal.)\n\n"}
{"id": "547921", "url": "https://de.wikipedia.org/wiki?curid=547921", "title": "Ceres (Computersystem)", "text": "Ceres (Computersystem)\n\nCeres ist der Name des im Jahre 1986 vom Schweizer Informatiker Niklaus Wirth und seiner Gruppe entwickelten Computersystems. Basis war ein 32-Bit-Prozessor (NS320xx). Dabei wurde auch das dazugehörige Betriebssystem, das , entwickelt, das in der parallel dazu definierten Programmiersprache Oberon programmiert ist. Die Rechner dienten bis etwa 2003 zur Ausbildung der Informatikstudenten an der ETH Zürich. Das ETH Oberon System wurde im Laufe der Zeit auf andere Plattformen wie Windows oder Solaris portiert, um von der alten und inzwischen fehleranfälligen Ceres-Hardware unabhängig zu sein. \n\nDer Name Ceres stammt von der römischen Göttin des Ackerbaus.\n\nWirth schuf auch das Computersystem Lilith und entwickelte für die Programmierung dessen Betriebssystems die Sprache Modula-2.\n\n"}
{"id": "547941", "url": "https://de.wikipedia.org/wiki?curid=547941", "title": "Lilith (Computersystem)", "text": "Lilith (Computersystem)\n\nLilith ist der Name des im Jahre 1980 vom Schweizer Informatiker Niklaus Wirth an der ETH Zürich entwickelten Computersystems. Die Lilith-Workstations dienten Anfang der 1980er Jahre als Plattform für zahlreiche Software-Projekte in der Forschung. Sie gehörten zu den ersten Rechnern mit Bitmap-Display, Maus und einer fensterorientierten Benutzerschnittstelle. Sie wurden in einem lokalen Netzwerk betrieben und ausgedruckt wurde auf den damals ebenfalls neuen Laserdruckern.\n\nAb 1982 wurde versucht, sie zu vermarkten. Kommerziell war sie ein Misserfolg, aber diese futuristische Maschine hat eine Informatikergeneration beeinflusst.\n\nWirth hatte 1977/78 am Forschungszentrum Palo Alto Research Institute von Xerox die zukunftsweisende Architektur der Alto-Workstations kennengelernt, die bereits mit Maus, Grafikbildschirm und Fenstertechnik versehen war. Nach seiner Rückkehr begann Wirth mit seiner Gruppe selbst eine solche Workstation zu entwickeln, wobei sie die Co-Entwicklung von Hardware- und Software betrieben.\n\nAuf diese Weise wurde Modula-2, der Nachfolger von Pascal, als Systemsprache generiert und als Betriebssystem Medos-2. Der Modula-2-Compiler übersetzte dabei in einen kompakten Zwischencode (sog. \"M-Code\"), der direkt auf der Hardware interpretiert werden konnte. Basis war ein aus vierbittigen AMD-Am2900-Bausteinen „bit-sliced“ zusammengesetzter 16-Bit-Prozessor. Dessen Befehlssatz hatte Wirth auf den Zwischencode des Modula-2-Compilers abgestimmt. Bei einem Systemtakt von 7 MHz konnte die Lilith pro Sekunde ein bis zwei Millionen dieser Instruktionen ausführen. Die Ur-Lilith hatte 128 kB Speicher und einen Bildschirm mit 704 × 928 Pixel (Hochformat), spätere Versionen hatten bereits 1024 Pixel Auflösung.\n\nDer Name „Lilith“ geht gemäss Angabe von Niklaus Wirth auf die gleichnamige Göttin in der sumerischen Mythologie zurück.\n\nDas Computersystem Ceres stammt ebenfalls von Wirth und ist für Oberon optimiert.\n\n"}
{"id": "548054", "url": "https://de.wikipedia.org/wiki?curid=548054", "title": "Bildungscomputer robotron A 5105", "text": "Bildungscomputer robotron A 5105\n\nDer Bildungscomputer robotron A 5105 (kurz BIC A 5105) wurde vom VEB Robotron-Meßelektronik „Otto Schön“ Dresden in der DDR ab Januar 1987 entwickelt und ab Juli 1989 serienmäßig hergestellt. Er war gedacht zur Ausbildung im Fach Informatik an den allgemeinbildenden Schulen, in der Berufsausbildung sowie an Hoch- und Fachschulen. Die Entwicklung wurde notwendig, da die Kleincomputer KC 85/2-4 und KC 87 nicht den Ansprüchen für die Ausbildung genügten. Bis zur Einstellung der Produktion im April 1990 wurden ca. 5.000 Stück des Rechners hergestellt. Davon gelangten ca. 3.000 Geräte an die Abnehmer im Bildungswesen zu einem anfänglichen Preis von ca. 11.000 M, der bis Ende der Lieferungen auf 3.000 M sank.\n\nDa der Einsatz des BIC A 5105 im Bildungswesen des vereinigten Deutschlands nicht vorgesehen war, wurden ab Anfang 1990 die Restbestände der produzierten Hard- und Softwarekomponenten unter dem Namen ALBA PC 1505 direkt vom Hersteller als Konsumgut vertrieben. Bis Mitte 1990 wurden so ca. 2.000 ALBA-PCs verkauft. Bemühungen, den ALBA-PC nach der Währungs-, Wirtschafts- und Sozialunion auch im vereinigten Deutschland und im westlichen Ausland zu vermarkten, blieben ohne nennenswerten geschäftlichen Erfolg.\n\nNachdem Ende 1989 die Fertigungsanlagen für das Plastgehäuse des Computergrundgerätes an den VEB Mikroelektronik Mühlhausen für 1 Mio M verkauft wurden, konnte das gleiche Gehäuse mit sehr guter Tastatur (aber nicht gleicher Elektronik) auch für den KC compact und eine optionale Zusatztastatur für den KC 85/2-4 verwendet werden.\n\n\n"}
{"id": "548123", "url": "https://de.wikipedia.org/wiki?curid=548123", "title": "Mikroprozessortechnik", "text": "Mikroprozessortechnik\n\nDie Zeitschrift MP Mikroprozessortechnik (\"Zeitschrift für Mikroelektronik • Computertechnik • Informatik\") erschien monatlich in der DDR von 1987 bis August 1992. Der Preis betrug 5 Mark. Herausgeber war die Kammer der Technik (KDT), Fachverband Elektrotechnik; verlegt wurde die Zeitschrift vom VEB Verlag Technik bzw. nach der Wende von der Verlag Technik GmbH.\n\nDie Zeitschrift befasste sich mit allen Bereichen der Computertechnik. Dazu gehörten Hardwarevorstellungen, Softwarelehrgänge, viele Programmierbeispiele, Tipps und Tricks. Programmbeispiele wurden teilweise in mehrseitiger Minischrift abgedruckt.\n\nNach der Wende druckte die Mikroprozessortechnik auf der Grundlage von Kooperationsvereinbarungen Artikel der westdeutschen Fachzeitschriften \"c’t\" (ab MP 3/1990) und Computer Persönlich (ab MP 5/1990) nach.\n\n"}
{"id": "548167", "url": "https://de.wikipedia.org/wiki?curid=548167", "title": "A 7100", "text": "A 7100\n\nDer A 7100 war der erste in Serie gefertigte 16-Bit-Personal Computer der DDR. Er wurde 1985 vorgestellt und verwendete einen Hauptprozessor vom Typ K1810WM86. Bei diesem handelte es sich um einen exakten Nachbau des Intel 8086. Trotzdem war der A 7100 nur eingeschränkt PC-kompatibel. Das Gerät wurde ab 1986 vom VEB Robotron hergestellt. Als Nachfolgemodell wurde 1988 der A 7150 vorgestellt. \n\nFür diesen Rechner gab es eine Grafikoption, die jedoch mit einem langsamen U-880-Grafikprozessor arbeitete und GKS (vgl. PHIGS) implementierte.\n\nDas Rechnergrundgerät ist 174 mm hoch, 486 mm breit und 451 mm tief. Es wiegt circa 22 kg und verfügt über eine RESET-Taste sowie optische Anzeigen zum Status der Stromversorgung und des Lüfters. Es kann akustische Signale erzeugen. Als Massenspeicher können im Desktop-Gehäuse maximal zwei 5¼″-Diskettenlaufwerke der Typen K5600.20 (einseitig, 80 Spuren einseitig) oder K5601 (doppelseitig, 80 Spuren zweiseitig) verbaut werden. Beide Laufwerksmodelle verwenden das Aufzeichnungsverfahren Modified Frequency Modulation und eine Spurdichte von 96 Spuren pro Zoll. Sie erreichen eine unformatierte Speicherkapazität von 0,5 MByte respektive 1 MByte pro Diskette. Als externe Tastatur waren vom Hersteller die Modelle K7672.01 und K7672.03 mit jeweils 104 Tasten sowie das Modell K7637.91 mit 106 Tasten vorgesehen.\n\nIm Gegensatz zum IBM-PC, bei dem die Zeichen im Bildwiederholspeicher nur als ASCII- und Farb-Code gespeichert wurden, speicherte der A7100 das Bitmuster eines jeden erzeugten Zeichens. Das verlangsamte die einfache Textausgabe sichtlich.\n\nDas hatte aber auch Vorteile. Für das Scrollen gab es einen Soft-Modus, bei dem nicht zeilen-, sondern pixelweise gescrollt wurde. Dadurch waren Texte auch während des Scollens gut lesbar.\nÜber die Umprogrammierung des Zeichengenerators ließen sich im Text-Modus Grafiken erzeugen.\n\nZusätzlich zum pixelorientierten Textspeicher gab es noch den grafischen Bildwiederholspeicher. Zwischen den beiden konnte von oben beginnend zeilenweise umgeschaltet werden, dass eine kombinierte Anzeige von Grafik und Text möglich war.\n\nDie Darstellung oblag dem Controller für das grafische Subsystem, welches durch einen U880 gesteuert wurde. Dieser wurde mit einer speziell zu ladenden Firmware programmiert.\n\nDer Grafikspeicher war nicht über den Systembus erreichbar. Alle Grafikein- und -ausgaben erfolgten über zwei Portadressen, von denen die eine die Daten aufnahm und die andere den Status beinhaltete. Über die Portkombination konnten spezielle Grafik-Kommandos übertragen und ausgeführt werden. Deren Umfang reichte vom Zeichnen von Punkten und Linien mit verschiedenen Linientypen bis zum Zeichnen von komplexen Bitmustern, Linien, Kreisen, Polygonen und deren Füllen mit vordefinierten oder definierbaren Mustern. \n\nEin holzverarbeitender Betrieb (Name ergänzen!) aus dem Bezirk Karl-Marx-Stadt entwickelte für den A7100 einen Emulator, der den KGS (Grafikcontroller) umprogrammierte und mit zeichen- und farbcodeorientierten Interrupt-Routinen versah.\nMit diesem Emulator war der A7100 in der Lage, MS-DOS zu starten. Unter MS-DOS lag dann aber der Textspeicher auf der Adresse 0x7800 anstelle von 0xB800, weshalb Programme, die hart auf den Textspeicher zugriffen, anstatt Interrupts zu benutzen, nur in speziell gepatchten Versionen lauffähig waren. (z. B. Norton Commander, Borland Turbo Pascal 4.0) \n\nBei laufendem Emulator war der A7100 nicht mehr in der Lage, in konventionellen Programmen Grafik auszugeben.\nDas ließ sich dadurch umgehen, dass ein Programm während der Abarbeitung den KGS-Reset auslöste, die Grafik-Firmware programmierte, die Grafiken anzeigte und im Anschluss die Textfirmware wieder zurückspeicherte.\n\n\n\n"}
{"id": "548468", "url": "https://de.wikipedia.org/wiki?curid=548468", "title": "Chemische Graphentheorie", "text": "Chemische Graphentheorie\n\nDie chemische Graphentheorie beschäftigt sich mit der Formalisierung und Anwendung von graphentheoretischen Prinzipien im Bereich der Chemie, speziell der Chemoinformatik. Gegenstand der chemischen Graphentheorie ist die Verarbeitung von Molekülstrukturen.\n\nWichtige Anwendungen sind die Identifizierung von Substrukturen beispielsweise für Gruppenbeitragsmethoden, topologische Indizes, die Kekulisierung von Molekülstrukturen und die Berechnung von charakteristischen Polynomen.\n\nMolekülstrukturen werden zumeist in Matrixform gespeichert, typische Darstellungsformen sind die\n\nEine alternative Darstellung ist die Bindungsliste, die zumeist auch von Grafikprogrammen verwendet wird und Platz für mehr Informationen bietet. Die Bindungsliste enthält zumeist eine Liste von Atomen inkl. bspw. der Koordinaten, Atomtypen, Ladungen etc. und anschließend eine Liste der Bindungen mit der Angabe der verbundenen Atome inkl. der Bindungsordnung und weiterer geometrischer Informationen.\n\n"}
{"id": "548503", "url": "https://de.wikipedia.org/wiki?curid=548503", "title": "Leitstruktur (Pharmakologie)", "text": "Leitstruktur (Pharmakologie)\n\nEine Leitstruktur (engl. \"lead\") ist ein chemischer Stoff, der in der Pharmaforschung im Zuge eines Wirkstoffdesigns als ein Ausgangspunkt für die Entwicklung eines Arzneistoff-Kandidaten untersucht wird. Eine Leitstruktur zeigt in vitro schon eine erwünschte biologische Wirkung, hat aber in der Wirkstärke, Selektivität oder in den pharmakokinetischen Eigenschaften noch nicht die für einen Arzneistoff benötigten Qualitäten.\n\nVon einem beliebigen Treffer \"(Hit)\" aus einem High-throughput screening unterscheidet sich eine Leitstruktur dadurch, dass ihre Struktur die Synthese von analogen Molekülen erlaubt. Eine Leitstruktur kann auch ein Naturstoff sein, der durch chemische Modifikationen in seinen pharmakologischen Eigenschaften verbessert werden muss. \n\nMittels geeigneter chemischer, biochemischer und in silico-Methoden wird in der medizinischen Chemie versucht, die Molekülstruktur und damit die Wirkungsweise der Leitstruktur so weit zu optimieren, dass der Arzneistoff-Kandidat in den nächsten Phasen der Arzneimittelentwicklung getestet werden kann.\n\n\n, Cheminformatik, Bioinformatik, LADME\n"}
{"id": "548686", "url": "https://de.wikipedia.org/wiki?curid=548686", "title": "SuperKaramba", "text": "SuperKaramba\n\nSuperKaramba (Super Karamba) ist ein Computerprogramm, das die Darstellung verschiedenster kleiner Programme (Widgets) und Themes auf dem Hintergrundbild des Desktops ermöglicht.\n\nDabei werden die Ausgaben der Programme meist direkt in den Hintergrund eingebettet und stören so nicht die Übersicht über die \"normalen\" Programme. Die Nutzung von SuperKaramba beschränkt sich zwar nicht auf Oberflächen von KDE, doch sind gewisse Bibliotheken, die der Desktop mit sich bringt, erforderlich. Seit KDE 3.5 wird SuperKaramba bereits mitgeliefert, ist also offizieller Teil des Desktops.\nIn KDE SC 4 stellt die neue Oberfläche Plasma ähnliche Möglichkeiten wie SuperKaramba in Bezug auf die Gestaltung des Desktops bereit. Trotzdem wird SuperKaramba weiterentwickelt. So wurde es bereits nach KDE SC 4 portiert und erlaubt die Einbettung von SuperKaramba-Widgets in Plasma. Die Weiterentwicklung wurde mittlerweile eingestellt, seit 2015 werden auch keine neuen Releases mehr erstellt.\n\nProgrammierer und Grafiker erstellen ihre eigenen Themes oder Textdateien, die das Widget definieren. Danach können noch optional Python-Skripte hinzugefügt werden, um aus dem Widget ein interaktives Programm zu machen. SuperKaramba stellt dazu eine kleine API zur Verfügung.\n\n\n\n"}
{"id": "549351", "url": "https://de.wikipedia.org/wiki?curid=549351", "title": "Eagle (Software)", "text": "Eagle (Software)\n\nEAGLE ist ein EDA-Programm der Firma CadSoft zur Erstellung von Leiterplatten. Der Name ist ein Initialwort, gebildet aus Einfach Anzuwendender Grafischer Layout-Editor (engl.: Easily Applicable Graphical Layout Editor). EAGLE und die Unternehmung CadSoft wurden im September 2009 an Premier Farnell verkauft, einen Lieferanten elektronischer Bauteile. Im Juni 2016 wurde bekannt, dass EAGLE von der US-amerikanischen Firma Autodesk gekauft wurde.\n\nDie Software besteht aus mehreren Komponenten: Layout-Editor, Schaltplan-Editor, Autorouter und einer erweiterbaren Bauteil-Datenbank. Sie ist für die Plattformen Microsoft Windows, Linux und OS X erhältlich.\n\nMit Version 6 wechselte der Hersteller von einem proprietären Datenformat für u. a. Schaltpläne und Layouts hin zu einem XML-Datenformat.\n\nAb Version 8 ist für den Bezug von EAGLE nur noch ein \"subscription\"-Abomodell möglich. Es gibt keine Standalone-Version mehr. Außerdem muss ab Version 8 alle 30 Tage eine Internetverbindung bestehen um die Lizenzierung zu bestätigen.\n\nEagle wird mit Stand von 2017 mit folgenden zeitlich beschränkten Lizenzen vertrieben:\nBis 2017 wurde Eagle als zeitlich unbegrenztes Produkt in Versionen mit unterschiedlichen Eigenschaften vertrieben:\n\nNur für \"EAGLE-Premium\" und \"EAGLE-Ultimate\" waren Mehrbenutzer-Lizenzen verfügbar.\n\n1992 verschickte CadSoft tausende Disketten mit einer Demoversion der Eagle-Software. Neben der Demoversion enthielt diese Diskette ein Spionageprogramm, das urheberrechtswidrig hergestellte Kopien entdecken sollte. Wurden solche vermeintlich entdeckt, wurden die Benutzer dazu animiert, mittels eines vermeintlichen Bestellfomulars für ein Gratis-Handbuch Ihre Anschrift preiszugeben. Daraufhin erhielten diese Post von Anwälten der CadSoft mit dem Vorwurf, Raubkopien einzusetzen.\n\n\n"}
{"id": "549419", "url": "https://de.wikipedia.org/wiki?curid=549419", "title": "PDP-7", "text": "PDP-7\n\nDer PDP-7 ist ein Minirechner, der von der Firma Digital Equipment Corporation (DEC) produziert wurde. Vorgestellt im Jahr 1965, war es der erste Rechner, der DECs \"Flip-Chip\"-Technologie nutzte. Bei einem Preis von 72.000 US-Dollar (entspräche heute etwa US-Dollar) war dieser Computer für seine Rechenleistung sehr günstig. Der PDP-7 hat eine Wortbreite von 18 Bit, der Befehlssatz ist den Prozessoren PDP-4 und PDP-9 ähnlich.\n\nIm Jahr 1969 schrieb der Informatiker Ken Thompson das erste Unix-System in Assemblersprache für einen PDP-7-Prozessor,\ndas damals in Anspielung auf das Betriebssystem \"Multics\" zum Scherz \"Unics\" genannt wurde. Es sollte das Betriebssystem für das Weltraum-Computerspiel \"Space Travel\" werden, welches eine Grafikausgabe benötigte, um die Bewegung der Planeten darzustellen.\n\nEinige wenige PDP-7 sind noch immer in funktionsfähigem Zustand, und es gibt ein Restaurationsprojekt in der norwegischen Hauptstadt Oslo.\n\n"}
{"id": "549462", "url": "https://de.wikipedia.org/wiki?curid=549462", "title": "JuK", "text": "JuK\n\nJuK ist ein Jukebox-Programm aus der KDE Software Compilation 4. Es unterstützt MP3-, (Ogg)Vorbis-, und FLAC-Audiodateien.\n\nJuK wurde von Scott Wheeler im Jahr 2000 ins Leben gerufen und hatte ursprünglich den Namen QTagger. Im Jahr 2002 wurde das Programm in das KDE-CVS aufgenommen, wo es zu einer der führenden Audioanwendungen wurde.\n\nJuK ist hauptsächlich zum Verwalten von Audiodateien gedacht und kann diese Sammlungen abspielen. Die Funktionen des Programms sind:\n\n"}
{"id": "550022", "url": "https://de.wikipedia.org/wiki?curid=550022", "title": "Lattice-Boltzmann-Methode", "text": "Lattice-Boltzmann-Methode\n\nBei der Lattice-Boltzmann-Methode (auch Lattice-Boltzmann-Verfahren oder Gitter-Boltzmann-Methode) handelt es sich um eine Ende der 1980er Jahre entwickelten Methode zur numerischen Strömungssimulation. Wie der Name andeutet, wird der Phasenraum zur numerischen Lösung der Boltzmann-Gleichung durch ein Gitter diskretisiert. Durch das Einarbeiten weiterer Modelle können auch andere physikalische Prozesse in einem Kontinuum wie beispielsweise thermodynamische Vorgänge in Fluiden oder Festkörpern mittels Lattice-Boltzmann-Methoden berechnet werden.\n\nDie Lattice-Boltzmann-Methode basiert auf der Berechnung einer stark vereinfachten Teilchen-Mikrodynamik. Das heißt, es wird eine Simulation auf der Teilchenebene durchgeführt. Aufgrund der internen Struktur (geringer Speicher- und Rechenbedarf je Zelle) eignet sich das Verfahren u. a. zur Berechnung von Strömungen in komplexen Geometrien.\nDas Lattice-Boltzmann-Verfahren hat seine theoretische Basis in der statistischen Physik. Die Wechselwirkung der mikroskopischen Teilchen wird durch die Boltzmann-Gleichung beschrieben.\n\nUm die Boltzmanngleichung zu lösen, wird sie diskretisiert. Die Diskretisierung erfolgt durch Einführung eines Gitters im Ortsraum, wodurch auch die Geschwindigkeitsrichtungen diskretisiert werden. Somit ist der ganze Phasenraum diskretisiert. Ein zweidimensionaler Raum lässt sich beispielsweise mit dem hier gezeigten D2Q9-Modell diskretisieren. In der Abbildung stellen die Punkte Punkte im Ortsraum dar, während die Pfeile darstellen, wie wahrscheinlich die Geschwindigkeit der Teilchen, die einem Punkt zugeordnet sind in die Richtung des Pfeiles am jeweiligen Punkt auftritt. Ein Fluidpartikel kann pro Zeitschritt an gleicher Stelle bleiben oder sich in den jeweils angrenzenden Zellen des quadratischen Gitters bewegen. Er besitzt daher neun mögliche Geschwindigkeiten formula_1, wobei der Index formula_2 die Richtung kennzeichnet.\n\nDer Algorithmus lässt sich in zwei Teilschritte einteilen, deren Reihenfolge fest, aber beliebig ist:\n\nIm Kollisionsschritt werden die Kollisionsregeln angewendet. Diese Regeln müssen die Masse sowie den Impuls erhalten. Zu jeder Phasenraumdichte formula_3 am Ort formula_4 wird ein entsprechend berechneter Kollisionsterm formula_5 addiert:\n\nEin möglicher Kollisionsterm ist der Bhatnagar–Gross–Krook (BGK) Operator\n\nDie Relaxationszeit formula_8 bestimmt, wie schnell sich das Fluid dem Gleichgewicht nähert und hängt somit direkt von der Viskosität des Fluids ab. Der Wert formula_9 ist die lokale Gleichgewichtsfunktion, welche die Boltzmannverteilung approximiert.\n\nBeim Strömungsschritt werden alle Pfeile (gemäß ihrer Richtung) zum nächsten Gitterpunkt verschoben:\n\nDie so verschobenen Pfeile bilden wieder die Ausgangssituation für den nächsten Kollisionsschritt.\n\n"}
{"id": "550083", "url": "https://de.wikipedia.org/wiki?curid=550083", "title": "Konsole (KDE)", "text": "Konsole (KDE)\n\nKonsole ist die Terminalemulation des K Desktop Environment, welches auf unixoiden Betriebssystemen läuft. Es unterstützt Registerkarten, Verlauf, Drucken und Lesezeichen. Einige Anwendungen wie Konqueror und Kate verwenden Konsole als KPart. Konsole war die erste komplett neu geschriebene Terminalemulation und nicht wie die meisten anderen auf xterm oder xvt aufgebaut.\n\nMit der Konsole können innerhalb einer Session mehrere Instanzen in Reitern geladen und das Fenster horizontal wie vertikal geteilt werden, auch mehrfach. Darüber hinaus können je nach Bedarf des Nutzers eine Bildlaufleiste, eine Menüleiste und die Reiterleiste an- und ausgeschaltet werden.\nEbenso lassen sich Zeichensatz, Farbschemata und Tastaturzuordnung einstellen und in Profilen abspeichern.\n\n\n"}
{"id": "550158", "url": "https://de.wikipedia.org/wiki?curid=550158", "title": "Kompare", "text": "Kompare\n\nKompare ist ein freies Programm, welches dem Benutzer hilft, zwei Text-Dateien oder Verzeichnisse miteinander zu vergleichen. Es ist ein Teil des K Desktop Environment (KDE) und wird hauptsächlich auf Linux und anderen Unix-artigen Betriebssystemen eingesetzt. Kompare verwendet diff für die Berechnung der Unterschiede. diff ist ein Kommandozeilen-Programm und gibt die Unterschiede in Textform aus.\n\nWie das Bild oben zeigt, versucht Kompare die zwei Textdateien so nebeneinander darzustellen, dass die zusammengehörigen Zeilen immer nächstmöglich zueinander, abhängig von der Position der Bildlaufleiste angezeigt werden. Zeilen die die beiden Dokumente voneinander unterscheiden werden in beiden Ansichten hervorgehoben. Dazu werden drei verschiedene Farben/Anzeigemöglichkeiten verwendet:\n\nWenn zwei Verzeichnisse anstelle von zwei Text-Dateien miteinander verglichen werden sollen, zeigt Kompare einen Verzeichnisbaum für die beiden ausgewählten Verzeichnisse. \n\nKompare kann Patch-Dateien erstellen, welche nur die Unterschiede zwischen zwei Text-Dateien A und B auflisten. Weiters kann Kompare eine Patch-Datei, die auf diesem Weg erstellt wurde, auf eine Datei A anwenden, auf diese Weise kann der Inhalt von Datei B in Datei A wiederhergestellt werden. Dies ist eine komfortable Möglichkeit, um aus einer alten Version einer Datei eine neuere/verbesserte Version zu machen, da nur die tatsächlichen Änderungen (die Patch-Datei) übernommen werden müssen. Mit diesem System werden bei Unix-artigen Betriebssystemen Patches eingespielt, wenn Programme aus dem Sourcecode kompiliert und nicht über ein Paketmanagement-System installiert wurden.\n\nDie Patches die mittels Kompare erstellt wurden sind kompatibel zu Patch-Dateien die mit dem Kommandozeilen-Programm diff erstellt wurden, weil Kompare hauptsächlich ein graphisches Frontend für die Kommandozeilen-Programme diff und patch ist.\n\nMeld und KDiff3 erlauben das Visualisieren von Unterschieden wie Kompare, zusätzlich ist es dem Benutzer auch möglich, Dateien miteinander zu verschmelzen und Einzelheiten des Textes zu bearbeiten. Kompare seinerseits wurde nicht für das Bearbeiten oder Vereinigen von Texten programmiert.\n\n"}
{"id": "550498", "url": "https://de.wikipedia.org/wiki?curid=550498", "title": "Mathcad", "text": "Mathcad\n\nMathcad ist ein kommerzielles Computeralgebrasystem, das ursprünglich die Firma Mathsoft entwickelt hatte, die 2004 von der Firma Parametric Technology Corporation (PTC) übernommen wurde. Derzeit liegt es in der Version 15.0 vor. Parallel dazu gibt es eine Variante namens \"Prime\". In der aktuellen Version 5 hat \"Prime\" nicht die Funktionalität von Mathcad 15. Mathcad 15 wird nicht weiterentwickelt. Es wurde ursprünglich für rein numerische Rechnungen (etwa die Anwendung von Näherungsmethoden) entwickelt. Entsprechend liegt in diesem Bereich der Unterschied von Mathcad zu ähnlichen Produkten wie Mathematica, Maple und Scilab. Intern nutzt Mathcad seit Version 3 (1991) eine Minimalvariante des Symbolprozessors von Maple.\n\nMathcad ist bei symbolischen Rechnungen weniger leistungsfähig als Maple und Mathematica, ist aber wegen seiner numerischen Rechenfähigkeiten und der Möglichkeit der Erweiterung durch Erweiterungspakete zu verschiedenen Themengebieten bei Technikern, Wirtschafts- und Naturwissenschaftlern verbreitet. Auch an Schulen findet das Programm aufgrund seines Kompromisses aus einfacher Bedienbarkeit und mathematischer Ausdrucksfähigkeit Verwendung.\n\nMathcad stellt alle Berechnungen auf Arbeitsblättern dar. Berechnungen werden durch den Benutzer in mathematischer Standardnotation eingegeben. Dieses Konzept hat den Vorteil, dass Berechnungen und Graphen einfach ausgedruckt werden können. Außerdem nähert sich das Konzept an den normalen Arbeitsablauf von Technikern und Wissenschaftlern an. Allerdings ist der Speicheraufwand größer als etwa bei einer Notation von Berechnungen in einer Programmiersprache, da auch Zusatzinformationen, z. B. die Positionen der Elemente auf dem Blatt, gespeichert werden müssen. Des Weiteren führt das Speichermanagement öfter zu Programmabstürzen.\n\nDas Programm richtet sich hauptsächlich an Nutzer ohne Programmierkenntnisse, bietet aber auch Möglichkeiten zur prozeduralen Programmierung. In C++ oder C geschriebener Code lässt sich durch benutzerdefinierte DLLs einbinden.\n\n\n"}
{"id": "551009", "url": "https://de.wikipedia.org/wiki?curid=551009", "title": "Axiom (Software)", "text": "Axiom (Software)\n\nAxiom ist ein freies Computeralgebrasystem. Es besteht aus einer interaktiven Umgebung (dem Interpreter), einem Compiler und einer Programmbibliothek. Letztere implementiert eine mathematisch korrekte Hierarchie von Typen.\n\nAxiom wurde seit 1971 von Forschern der IBM unter der Leitung von Richard Dimick Jenks entwickelt, ursprünglich unter dem Namen SCRATCHPAD in der Programmiersprache Lisp. In den 1990ern wurde es an die Gruppe NAG verkauft und erhielt seinen jetzigen Namen. Im Jahr 2001 wurde es vom Markt genommen, seit 2002 wird es unter einer modifizierten BSD-Lizenz verbreitet. Seitdem haben sich zwei Projekte davon abgespalten: FriCAS und OpenAxiom.\n\nIn Axiom haben alle Objekte einen Typ. Beispiele für solche Typen sind mathematische Strukturen (wie Ringe, Körper, Polynome) als auch Datenstrukturen aus der Informatik (z. B. Liste, Baum, Heap, Hashtabelle in verschiedenen Formen).\n\nEine Funktion kann einen Typ als Argument haben, und ihr Ergebnis kann ebenso ein Typ sein. Zum Beispiel ist codice_1 eine Funktion, die einen codice_2 als Argument akzeptiert und den Quotientenkörper ihres Arguments zum Ergebnis hat. Als weiteres Beispiel kann man den Ring der formula_1 Matrizen mit rationalen Einträgen als codice_3 konstruieren. Wenn man in diesem Domain arbeitet, wird codice_4 als Einheitsmatrix interpretiert und codice_5 liefert die Inverse der Matrix codice_6, soweit sie existiert.\n\nVerschiedene Funktionen können den gleichen Namen haben. Die Typen der Argumente und der Typ des Resultats werden verwendet um festzustellen, welche Operation tatsächlich gemeint ist, ähnlich wie in der Objektorientierten Programmierung.\n\nDie Erweiterungssprache von Axiom heißt \"SPAD\". Das gesamte mathematische Wissen von Axiom ist in dieser Sprache geschrieben. Der Interpreter akzeptiert ungefähr die gleiche Sprache.\n\nSPAD wurde unter dem Namen \"A#\" und später \"Aldor\" weiterentwickelt. Aldor kann nach wie vor als alternative Erweiterungssprache verwendet werden. Es wird allerdings unter einer anderen Lizenz vertrieben.\n\nInnerhalb des Interpreters benutzt Axiom Typinferenz und einen heuristischen Algorithmus, um explizite Typangaben weitgehend unnötig zu machen.\n\nAxiom beinhaltet 'HyperDoc', eine interaktive, browserähnliche Hilfe, und kann zwei- und dreidimensionale Graphiken darstellen. Die Grafikanzeige ermöglicht es unter anderem auch, die Grafik zu rotieren oder eine Lichtquelle zu setzen.\n\nAxiom ist derzeit das einzige CAS mit einer vollständigen Implementierung des Risch-Algorithmus zur elementaren Integration. Die Implementierung stammt von Manuel Bronstein und Barry Trager.\n\nEs gibt einen Modus für Emacs sowie ein Plugin für den Editor \"TeXmacs\". Eine weitere Möglichkeit besteht darin SAGE als Interface für Axiom zu verwenden. \n\n"}
{"id": "552414", "url": "https://de.wikipedia.org/wiki?curid=552414", "title": "TextMaker", "text": "TextMaker\n\nTextMaker ist ein Textverarbeitungsprogramm, das vom Nürnberger Unternehmen SoftMaker entwickelt wurde und als Bestandteil des Office-Pakets \"SoftMaker Office\" vertrieben wird. Eine funktional eingeschränkte kostenlose Version ist in \"SoftMaker FreeOffice\" enthalten.\n\nDie aktuelle Version ist \"TextMaker 2018\" und Bestandteil von \"SoftMaker Office 2018\". Das Programm ist für folgende Betriebssysteme erhältlich:\n\n\nTextMaker ist das einzige Textverarbeitungsprogramm, das alle diese Systeme unterstützt.\n\nFrühere Versionen gab es auch für die Betriebssysteme DOS (MS-DOS, PC-DOS, DR-DOS, FreeDOS, NW DOS), Windows CE (TextMaker 2010), Windows Mobile (TextMaker 2010), FreeBSD (TextMaker 2006) und Sharp Zaurus (TextMaker 2002). Softmaker Office 2018 ist mittlerweile für Bildungseinrichtungen kostenlos.\n\nDie Entwicklung von TextMaker begann 1987 als Programm für MS-DOS. Ein Schwerpunkt von Textmaker ist neben dem geringen Ressourcenverbrauch die weitestgehende Kompatibilität mit Microsoft Word. Es gibt einige zusätzliche Funktionen z. B.\n\nNeben dem proprietären TextMaker-Format - \".tmd\" liest und schreibt das Programm die Dateiformate\n\nAls Standardformat können die Formate \".tmd\", \".doc\", \".docx\" und \".odt\" verwendet werden. Die Formate .sxw von OpenOffice.org 1.x, WordPerfect (.wpd) und Windows Write (.wri) kann TextMaker lesen, aber nicht schreiben.\n\nFerner enthält TextMaker eine Export-Funktion für das Format PDF und EPub, die ohne Umweg über einen PDF-Druckertreiber und mit einigen zusätzlichen Optionen Dokumente ausgibt.\n\nDie Makro- und Skript-Funktionen von Microsoft (VBA) werden zwar nicht unterstützt, SoftMaker integriert aber in den Windows-Versionen von SoftMaker Office das VBA-ähnliche Scripting-System BasicMaker.\n\nDie Fremdsprachenunterstützung umfasst ab Version 2010 die Langenscheidt-Taschenwörterbücher für Englisch, Französisch, Spanisch und Italienisch, einen Thesaurus für zehn Sprachen, Rechtschreibprüfung für 20 Sprachen, Benutzeroberfläche in 15 Sprachen und Silbentrennung in 33 Sprachen. Ab der Version 2008 sind zudem das Duden Universalwörterbuch und das große Duden Fremdwörterbuch Bestandteil des Programms.\n\nSeit TextMaker 2012 ist eine Professional-Version mit integriertem Duden-Korrektor erhältlich.\n\nTextMaker für Windows lässt sich ab der Version 2006 auch als portable Software von einem USB-Stick verwenden, ohne dass auf dem Gast-PC eine Installation notwendig wäre oder Systemdateien verändert würden.\n\n"}
{"id": "552498", "url": "https://de.wikipedia.org/wiki?curid=552498", "title": "PlanMaker", "text": "PlanMaker\n\nPlanMaker ist ein von der Nürnberger Firma SoftMaker entwickeltes Tabellenkalkulationsprogramm. Es wird als Bestandteil des Office-Pakets \"SoftMaker Office\" vertrieben.\n\nDie aktuelle Version ist \"PlanMaker 2018\" (enthalten in \"SoftMaker Office 2018\"). Das Programm ist für folgende Betriebssysteme erhältlich:\n\nPlanMaker ist das einzige Tabellenkalkulationsprogramm, das alle diese Systeme unterstützt. \n\nPlanMaker für Windows und Linux lässt sich auch auf einen USB-Stick kopieren und so an jedem beliebigen Windows- bzw. Linux-PC verwenden, ohne dass eine Installation notwendig ist oder Systemdateien des PC verändert werden.\n\nPlanMaker ist deutlich am Vorbild von Microsoft Excel orientiert: Die Benutzeroberfläche ist ähnlich, der Funktionsumfang stimmt im Kern überein, größtmögliche Datenkompatibilität zu Excel wird angestrebt. Das Excel-Dateiformat (.xlsx) kann sogar statt des proprietären PlanMaker-Dateiformats (.pmdx) als Standard-Speicherformat für Dateien gewählt werden. PlanMaker ist jedoch durch den Akzent auf leichterer und schnellerer Bedienbarkeit eher auf den Home- und Small-Business-Anwender ausgerichtet.\n\nIn Excel-Dokumenten enthaltene Makros und VBA-Scripts können in PlanMaker nicht ausgeführt werden, bleiben aber beim Speichern im Excel-Format erhalten. Als Bestandteil des SoftMaker Office-Pakets stellt SoftMaker mit BasicMaker eine eigene Scripting-Sprache zur Verfügung, deren Skripts jedoch nicht in PlanMaker-Dokumente eingebettet werden können.\n\n"}
{"id": "552512", "url": "https://de.wikipedia.org/wiki?curid=552512", "title": "DataMaker", "text": "DataMaker\n\nDataMaker ist das Datenbankprogramm im SoftMaker Office. Derzeit liegt DataMaker in der Version 99 vor und läuft unter Microsoft Windows. DataMaker gehört zu dem Office-Paket, wird aber auch einzeln verkauft.\n\nAm 21. Juli 2006 vermeldete SoftMaker im Kunden-Newsletter, dass sich DataMaker 2006 im nicht-öffentlichen Betatest befinde. Die neue Version soll ODBC-basiert sein und somit nicht mehr auf dBASE-Datenbanken beschränkt sein, wie der derzeit verfügbare DataMaker 99. Weiterhin wurde angekündigt, dass der Funktionsumfang erweitert worden sei: Formulare (auch Subformulare), Berichte, Aufkleber, volle Relationalität, Queries, direkte Eingabe von SQL-Befehlen und vieles mehr.\nAm 19. Februar 2007 wird im Kunden-Newsletter \"DataMaker 2007\" erwähnt, welches voraussichtlich die etwas älteren Windows-Versionen 95/98/ME nicht mehr unterstützen wird.\n\n"}
{"id": "552675", "url": "https://de.wikipedia.org/wiki?curid=552675", "title": "Worldwide Developers Conference", "text": "Worldwide Developers Conference\n\nDie Worldwide Developers Conference (oft abgekürzt als WWDC; ) ist eine jährlich von Apple in Kalifornien veranstaltete Konferenz, die sich in erster Linie an Software-Entwickler für MacOS und iOS sowie neuerdings auch an die Entwickler von WatchOS richtet. Im Gegensatz dazu zielte die 2014 eingestellte Macworld auf Endbenutzer ab.\nDie Konferenz beginnt mit einer Keynote, die Apple oft nutzt um zukünftige Produkte vorzustellen, währenddessen auch Vorträge gehalten werden und Workshops statt finden.\n\nWährend die erste WWDC 1983 noch in Monterey stattfand, ist der Veranstaltungsort mittlerweile das Moscone Center in San Francisco.\nAlle Teilnehmer müssen eine Vertraulichkeitsvereinbarung (, \"NDA\") unterzeichnen. Das galt bisher auch für die Keynote, die Apple allerdings in manchen Jahren auch live im Web übertragen hat und inzwischen auch einige Stunden später auf seiner Website als Videostream zur Verfügung stellt.\n\n\nDie WWDC 2001 hatte über 4000 Besucher. Zur Einführung von Mac OS X wurden Lederjacken mit einem großen blauen „X“ auf der Rückseite an die Entwickler verteilt.\n\nZu dieser WWDC erschienen etwa 2990 Besucher. In der Keynote zu Beginn wurde der PowerMac G5 vorgestellt und eine Vorschau auf Mac OS X Panther gezeigt. Die Besucher bekamen eine iSight-Webcam geschenkt. Bühnengäste waren Dr. John E. Kelly III., IBM, Greg Gilley, Adobe Systems, Theo Gray, Wolfram Research, Gerhard Lengeling, Emagic und Brad Peebler, Luxology.\n\nEtwa 3500 Entwickler nahmen teil. Es wurden neue Cinema Displays in 20″-, 23″- und 30″-Widescreen vorgestellt. Es gab eine Vorschau auf Mac OS X Tiger. Alle Besucher bekamen eine Entwicklerversion von Tiger, ein einfaches T-Shirt mit dem Apple-Logo auf der Vorder- und „WWDC 2004“ auf der Rückseite, einen Rucksack, in den ein 17″-Powerbook passt, und ein Exemplar von Apple Remote Desktop 2.0.\n\nBühnengäste:\n\n3800 Entwickler aus 45 Ländern nahmen teil. Es gab 110 „lab sessions“, 95 Vorträge und mehr als 500 Apple-Entwickler waren vor Ort.\n\nDer größte Teil der Veranstaltung beschäftigte sich mit Apples Umstieg auf Intel-Prozessoren und die x86-Plattform. Es kamen mehrere Entwickler auf die Bühne, die ihre Erfahrungen mit der Umstellung ihrer Software schilderten oder die Erweiterung ihrer Programme um Intel-Kompatibilität ankündigten. Unter den Sprechern war auch Paul Otellini, CEO von Intel.\n\nBühnengäste:\n\n4200 Entwickler aus 48 Ländern besuchten die Veranstaltung, was sie zur bis dahin größten WWDC machte. Es gab 140 Vorträge und 100 „hands-on labs“ für Entwickler. Über 1000 Apple-Entwickler waren vor Ort.\n\nIm Rahmen der Keynote wurde der Mac Pro als Nachfolger des Power Mac G5 angekündigt, sowie neue Xserves vorgestellt. Damit wurden die letzten verbliebenen PowerPC-basierten Macs auf Intel-Prozessoren umgestellt.\nZudem wurde erstmals Mac OS X Leopard demonstriert und für das Frühjahr 2007 angekündigt. Zusätzlich zu den neuen Leopard-Features wurde eine größere Neuauflage des Mac OS X Server-Produkts angekündigt. Einige der neuen Features: ein vereinfachter Installationsprozess, „iCal Server“ (basiert auf dem CalDAV-Standard), „Apple Teams“, eine Sammlung webbasierter zusammenarbeitender Dienste, Spotlight Server und Podcast Producer.\n\nBühnengäste: (keine)\n\nDie WWDC 2007 war mit mehr als 5000 Besuchern die größte in Apples Geschichte. Die Mitgliederzahl der Apple Developer Connection stieg im Vergleich zum letzten Jahr um etwa 200.000 auf mehr als 950.000. Es gab 159 Sessions, 95 Hands-on-labs, und es waren 1200 Apple-Entwickler vor Ort.\n\nIn der Keynote wurden zunächst neue Spiele von Electronic Arts sowie eine neue Spieleentwicklungstechnologie von id Software für die Mac-Plattform angekündigt. Anschließend wurden zehn Schlüsselfeatures von Mac OS X Leopard gezeigt. Alle Entwickler bekamen, wie im Vorfeld bekanntgegeben, eine kostenlose Kopie der neuen Beta-Version ausgehändigt.\n\nAls „One more thing…“ gab Steve Jobs bekannt, dass die neue Version 3.0 vom Webbrowser Apple Safari auf Windows portiert wird. Eine Beta-Version für Mac OS X, Windows XP und Windows Vista stand wenig später zum Herunterladen bereit.\n\nAls „One last thing…“ wurde eine im Vorfeld diskutierte Möglichkeit vorgestellt, wie Entwickler eigene Software für das am 29. Juni in den USA erscheinende iPhone entwickeln können. Diese Anwendungen sollten als Webapplikationen mit standardisierten Techniken wie Ajax geschrieben werden und sich nahtlos mit den iPhone-Funktionen integrieren. Scott Forstall zeigte einige Demoapplikationen.\n\nBühnengäste:\n\n\nDie WWDC 2008 fand vom 9. bis 13. Juni im Moscone Center in San Francisco statt. Erstmals war eine WWDC bereits im Vorfeld „ausverkauft“.\n\nAuf der Keynote wurden das iPhone 3G, iPhone OS 2.0, sowie der Dienst MobileMe vorgestellt, der den .Mac-Service ersetzte.\n\nIm Anschluss an die Keynote folgte ein Vortrag über Mac OS X Snow Leopard.\n\nDie WWDC 2009 fand vom 8. bis 12. Juni im Moscone Center in San Francisco statt. Zum zweiten Mal in Folge war sie bereits vor Beginn ausverkauft, in diesem Jahr sogar schon vor Ende des Frühbucherrabatts.\n\nKernthema der Konferenz war für iPhone-Entwickler die Version 3.0 des iPhone SDK und für Mac-Entwickler die vollständige 64-Bit-Architektur, die erweiterte Unterstützung bei der Entwicklung für Rechnern mit mehreren Prozessorkernen, die Nutzung des Grafikprozessor in Anwendungen sowie die neue Version von QuickTime.\nPhillip Schiller stellte Mac OS X Snow Leopard und das neue iPhone 3GS vor, aktualisierte das MacBook Air und die MacBook- bzw. MacBook Pro-Produktlinien.\n\nDie 21. WWDC fand vom 7. bis 11. Juni 2010 im Moscone Center in San Francisco statt. In der Keynote wurden das neue iPhone 4 und die neue Betriebssystemversion iOS 4, sowie Apple Safari 5 vorgestellt. Außerdem kamen die neuen Funktionen FaceTime und iMovie für das neue Gerät hinzu.\nAn der WWDC 2010 nahmen laut Apple mehr als 5200 Entwickler, Journalisten und andere Besucher aus 57 Ländern teil.\n\nDie 22. WWDC fand vom 6. bis 10. Juni 2011 im Moscone Center in San Francisco statt. Auf der Keynote wurden u. a. Mac OS X Lion, iOS 5 sowie iCloud vorgestellt. Die WWDC 2011 war in weniger als 10 Stunden ausgebucht. Es nahmen laut Apple wieder mehr als 5200 Besucher teil. Auf der WWDC 2011 hielt Steve Jobs seine letzte Keynote.\n\nDie 23. WWDC fand vom 11. bis 15. Juni 2012 im Moscone Center in San Francisco statt. Sie war binnen zwei Stunden ausgebucht. Auf der Keynote wurden u. a. OS X Mountain Lion, iOS 6 sowie das neue MacBook Air und das neue MacBook Pro mit Retina-Display vorgestellt.\n\nDie 24. WWDC fand vom 10. bis 14. Juni 2013 im Moscone Center in San Francisco statt.\nDie Tickets für das Event kosteten 1599 US-Dollar und waren binnen 71 Sekunden ausverkauft. Auf der Keynote wurden unter anderem iOS 7, OS X Mavericks, das neue MacBook Air und der neue Mac Pro vorgestellt.\nDie aus 66 verschiedenen Ländern stammenden Teilnehmer der Konferenz erhielten zum Auftakt der Veranstaltung eine Softshell Jacke mit einer großen aufgestickten 13 auf der Rückseite und der aufgestickten Jahreszahl der Veranstaltung in der römischen Zahlschrift (MMXIII).\n\nDie 25. WWDC fand vom 2. bis 6. Juni 2014 im Moscone Center in San Francisco statt. Erstmals nutzte Apple zur Vergabe der Tickets unter den Entwicklern ein Losverfahren. Der Ticketpreis beläuft sich auf 1599 US-Dollar.\nAuf der Keynote wurden unter anderen iOS 8, OS X Yosemite und Swift vorgestellt.\n\nDie 26. WWDC fand im Moscone Center in San Francisco statt. Auf der Keynote wurden OS X El Capitan, iOS 9, watchOS 2, Swift 2, Apple Music und weitere Neuerungen vorgestellt.\n\nDie 27. WWDC fand vom 13. bis 17. Juni in San Francisco statt. Die Veranstaltungen am ersten Konferenztag, sowie der Konferenz-Bash fanden im Bill Graham Civic Auditorium, die restlichen Veranstaltungen während der Konferenzzeit im Moscone Center statt. Auf der Keynote wurden macOS Sierra, iOS 10, watchOS 3, Swift 3 und weitere Neuerungen vorgestellt. Auf dem Bash traten DJ Qbert, sowie die Band Good Charlotte auf.\n\nDie 28. WWDC fand vom 5. bis 9. Juni statt. Dieses Mal wurde sie, das erste Mal seit 2002, in San Jose im \"The McEnery Convention Center\" ausgetragen. Die Keynote beinhaltete die Vorstellung der Betriebssysteme macOS High Sierra, iOS 11, watchOS 4, tvOS 11 und der Programmiersprache Swift 4. Zudem wurden MacBooks und MacBook Pro mit Kaby-Lake-Chips und verbesserte iMacs vorgestellt. Zudem fand hier die Vorankündigung des iMac Pro statt.\n\nDie 29. WWDC fand vom 4. bis 8. Juni 2018 statt. Wie schon im Vorjahr, wurde sie in San Jose im \"The McEnery Convention Center\" durchgeführt. Die Keynote beinhaltete die Vorstellung neuer Versionen der Betriebssysteme macOS Mojave, iOS 12, watchOS 5 und tvOS 12.\n\nFür 2019 wurde angekündigt, dass macOS Programmbibliotheken enthalten wird, die es erlauben werden, Programme so zu entwickeln, dass sie unverändert sowohl auf iOS als auch auf macOS lauffähig sind.\n\nAnders als in vorangegangenen Jahren wurde während der WWDC keine neue Hardware von Apple vorgestellt.\n\n"}
{"id": "552878", "url": "https://de.wikipedia.org/wiki?curid=552878", "title": "EC 1834", "text": "EC 1834\n\nDer Personal Computer EC 1834 wurde ab 1986 in der DDR vom VEB Robotron-Elektronik Dresden, Fachgebiet Geräte E2 Karl-Marx-Stadt als Hauptentwickler gemeinsam mit dem VEB Buchungsmaschinenwerk Karl-Marx-Stadt (jetzt Chemnitz) und dem VEB Robotron-Büromaschinenwerk „Ernst Thälmann“ Sömmerda entwickelt und in beiden Werken hergestellt. Es wurden etwa 34.000 EC 1834 sowie etwa 120 Funktionsmuster des Modells EC 1834.01/EC 1834.M gebaut.\n\nDer EC 1834 stellte einen IBM PC XT-kompatiblen PC dar und wurde mit einer mit 4,9152 MHz getakteten Intel-8086-kompatiblen CPU K1810WM86 ausgestattet. Ein Sockel für einen mathematischen Koprozessor (K1810WM87 oder Intel 8087 waren einsetzbar) war vorhanden. Er verfügte über zwei 5,25 Zoll Diskettenlaufwerke (doppelseitig, je 720 kByte Kapazität), eine Festplatte (meist zu 20 MByte, z. T. aber auch 40 MByte, davon waren aber anfänglich nur 32 MByte nutzbar), und hatte einen Hauptspeicher von 256 kByte. Der Arbeitsspeicher konnte mit einer zusätzlichen Speichererweiterung (Kapazität: 384 kByte) auf 640 kByte – damals Grenze des Konventionellen Speichers – erweitert werden.\n\nDie erste Bauserie wurde z. T. auch ohne Festplatte, dafür aber mit bis zu vier Diskettenlaufwerken ausgeliefert. Ein Teil der Geräte mit Festplatte hatte aber auch nur ein Diskettenlaufwerk. Es soll auch Geräte mit zwei Festplatten gegeben haben. Neben den K 5504.20 / K 5504.50 von Robotron kamen auch bulgarische Festplatten und „West-Importe“ (u. a. MFM-Festplatten der Firma Seagate) zum Einsatz.\n\nIn der ursprünglichen Ausführung hatte der EC 1834 acht Steckplätze mit einem dreireihigen EFS-Steckverbinder. Diese waren zwar elektrisch mit dem XT-Bus kompatibel, jedoch mechanisch verschieden. Eine modernere Variante (EC 1834.01, auch EC 1834.M genannt) hatte auch einige 8 Bit-ISA-Steckplätze, in die dann auch westliche ISA-Karten (z. B. VGA-Grafikkarten oder ähnliche) passten.\n\nDer Grafikcontroller war ein U82720. Optional konnte der EC 1834 mit einem Farbgrafik-Adapter aufgerüstet werden, welcher ebenfalls eine Eigenentwicklung von Robotron war. Dieser war (durch einen Emulationslayer) kompatibel zum Grafikstandard CGA (hardwarenahe Spiele liefen problemlos auf dieser Emulation, im Gegensatz zur IBM-VGA-Karte) und unterstützte folgende Auflösungs-Modi:\n\nAls Betriebssysteme für den EC 1834 kamen entweder DCP in den Versionen 3.20 oder 3.30 oder der UNIX-Klon MUTOS1834 zum Einsatz. Alternativ kann der EC 1834 aber auch mit CP/M-86 oder DOS betrieben werden.\n\nDer EC 1834 sollte bis Anfang der 1990er Jahre die in der Produktion weitaus teureren, materialintensiveren Computer A 7100 (inkompatibel zu IBM-XT) und A 7150 (kompatibel zu IBM XT durch langsamen Grafik-Emulation-Layer) ablösen.\n\n1988 wurde die Entwicklung eines IBM AT-kompatiblen Nachfolgers, dem EC 1835 begonnen, jedoch kam es bei diesem Computer (der u. a. mit einem U80601-Prozessor ausgerüstet werden sollte) nicht mehr zur Serienproduktion. Von diesem Nachfolger wurden nur etwa 20 Funktionsmuster gebaut.\n\n\n"}
{"id": "554811", "url": "https://de.wikipedia.org/wiki?curid=554811", "title": "Virtual DOS Machine", "text": "Virtual DOS Machine\n\nDie Virtual DOS Machine (VDM) ist eine virtuelle Maschine, die in einigen Betriebssystemen der Microsoft-Windows-NT-Familie enthalten ist, um die Kompatibilität mit älterer, für MS-DOS entwickelter Software sicherzustellen.\n\nAusgehend von \"ntvdm.exe\" simuliert eine Virtual DOS Machine weitgehend eine DOS-Umgebung, einige PC-Hardware-Komponenten (z. B. Prozessor in Real Mode, Intel 8253/8254 PIT, CGA/EGA/VGA-/PC-BIOS), sowie gängige DOS-Speicheradressierungsschnittstellen (EMS/XMS/DPMI) für Programme, die nicht für das Win32-API ausgelegt wurden, sondern für die INT-21h-Schnittstelle bzw. direkten Hardwarezugriff programmiert sind.\nÜber diese Emulationsschicht können die meisten DOS-Programme unter Windows ausgeführt werden. Für die Programme scheint es, als ob sie sich auf einer DOS-Maschine befänden, sie haben jedoch keinen direkten Kontakt zu den Ressourcen des Windows-Betriebssystems oder zu der Hardware. Anforderungen an das Betriebssystem (z. B. Dateien lesen oder schreiben) werden durch die Virtual DOS Machine in die entsprechenden Windows-Funktionen übersetzt. Maschineninstruktionen, die unter Windows nicht zulässig sind (diese führen zu Exceptions), aber in DOS ungeschützt wären, werden abgefangen und dann entweder emuliert oder übersetzt (z. B. die \"int\", \"in/out\", \"cli/sti\", \"hlt\" Instruktionen), oder weiterhin unterbunden (z. B. den direkten Zugriff auf Datenträger). Es können mehrere Instanzen einer Virtual DOS Machine ausgeführt werden. Somit können auf einem Windows-Betriebssystem mehrere virtuelle DOS Maschinen gleichzeitig laufen. Über spezielle Wrapper-Funktionen ist es möglich, von Windows aus auf den Inhalt der VDM zurückzugreifen.\n\nDie Virtual DOS Machine macht sich den von x86-Prozessoren zunutze, um DOS-Programme auszuführen. Da jedoch der Virtual-86-Modus nicht mehr funktioniert, wenn der Prozessor bereits in den 64-Bit-Modus geschaltet wurde, wird NTVDM bei 64-Bit-Versionen von Windows nicht mehr mit ausgeliefert. Die für Linux entwickelte virtuelle DOS-Umgebung DOSEMU teilt dieses Schicksal. Alternativen zur NTVDM (wie auch zu DOSEMU) sind entweder Emulatoren, virtuelle Maschinen oder Interpreter.\n\nMithilfe eines Emulators, bei dem ein gesamter PC inklusive Prozessor in Software nachgebildet wird, können DOS-Programme nicht nur auf Windows, sondern auf einer Vielzahl weiterer Betriebssysteme und Rechnerarchitekturen ausgeführt werden. Ein Beispiel dafür ist Bochs, aber auch virtuelle Maschinen wie QEMU, Virtual PC, VirtualBox und VMware können zum Virtualisieren oder Emulieren von DOS genutzt werden. Weil ein vollständiger PC emuliert bzw. virtualisiert wird, muss in jedem Fall zuerst ein DOS-Betriebssystem in der virtualisierten Umgebung installiert werden. FreeDOS steht unter der GPL und ist somit frei (quelloffen und gratis) verfügbar, es kann aber auch ein bereits vorhandenes oder gekauftes MS-DOS oder ein dazu kompatibles Betriebssystem verwendet werden. Zusätzlich sind DOS-Treiber nötig, damit bestimmte Funktionen wie etwa Audio-Ausgabe genutzt werden können. Weil eine Emulation zudem sehr aufwendig ist, läuft ein emuliertes DOS meist um einiges langsamer ab als das nativ gestartete Betriebssystem. Bei einer Virtualisierung werden Teile der emulierten Hardware ebenfalls nativ ausgeführt, was einen Geschwindigkeitsvorteil bringt.\n\nEine für den Anwender weniger aufwendige Alternative zur NTVDM stellt ein Interpreter dar, wie z. B. DOSBox, weil die Installation eines kompatiblen DOS-Betriebssystems entfällt. Außerdem bietet DOSBox eine Vielzahl weiterer Möglichkeiten zur Steuerung der Umgebung, in der ein DOS-Programm abläuft, und benötigt zudem keine Treiber für emulierte Hardware. DOSBox emuliert auf Nicht-x86-Hardware einen Intel-Prozessor (i386, i486 oder Pentium), virtualisiert hingegen die meisten x86-Instruktionen, wenn es auf einem x86-Prozessor läuft. Komponenten wie die Soundkarte werden hingegen in beiden Fällen emuliert. Ein Treiber ist nicht notwendig, da dessen Funktionen von der virtuellen DOS-Umgebung ebenfalls bereitgestellt werden.\n\n\n"}
{"id": "556140", "url": "https://de.wikipedia.org/wiki?curid=556140", "title": "Wienux", "text": "Wienux\n\nWienux (WIENUX) war eine Linux-Distribution für die Mitarbeiter der Wiener Stadtverwaltung. Ziel war die Migration von Proprietärer Software zu Open-Source-Software. Das Kofferwort Wienux setzt sich aus \"Wien\" und \"Linux\" zusammen. Wienux wurde auf Basis der Open-Source-Distribution Debian, dem Unix/Linux-Desktop KDE und einer überarbeiteten Hardwareerkennung von Knoppix entwickelt. Begonnen 2005, wurde das Migrations-Projekt um das Jahr 2009 herum abgebrochen, auch die dazugehörige Distribution wird nicht mehr weiterentwickelt.\n\nAuf etwa 4.800 von insgesamt 18.000 PC-Arbeitsplätzen der Wiener Stadtverwaltung konnte ab Ende Januar 2005 wahlweise Wienux oder Windows 2000 / Office 2000 eingesetzt werden. Wien stellte damit nach München die zweite Großstadt im deutschsprachigen Raum dar, die ihren Mitarbeitern das Arbeiten mit Open-Source-Software ermöglichte.\n\nDas Projekt fußte auf der Studie „Open Source Software am Arbeitsplatz im Magistrat Wien“. Die Studie stellte einen weiteren Beitrag zur Machbarkeit und den Kosten bei der Umstellung von proprietärer Software auf Open-Source-Software dar.\n\nAb 4. Oktober 2005 stand die Wienux-Distribution in Form eines 1 Gigabyte großen Zip-Archives zum Download bereit oder konnte auf DVD angefordert werden. Seit etwa November 2008 ist Wienux nicht mehr zum Download erhältlich.\n\n2008 waren erst 1000 Rechner, davon 200 in der eigentlichen Stadtverwaltung, auf Wienux umgestellt.\nGerüchte, dass die Entwicklung eingestellt werde, erhärteten sich durch die Meldung, dass Arbeitsplätze in den Kindergärten wieder mit Windows Vista ausgerüstet werden. Aufgrund von Kompatibilitätsproblemen mit einer Sprachförderlösung, die nur unter Windows lief, wurden 2008 720 Rechner in den Kindergärten auf Windows XP umgerüstet, so dass nur noch 280 Rechner mit Wienux als Betriebssystem arbeiteten.\n\nIm Juni 2009 wurde wiederum beschlossen, dass die Stadt Wien wieder verstärkt Open Source Software einsetzt, obwohl nicht explizit darauf hingewiesen wurde, ob die Entwicklung von Wienux wiederaufgenommen wird.\nGleichzeitig wurde 2009 beschlossen, bis 2012 für 1 Million Euro Windows-Lizenzen zu beschaffen und Open-Source-Software nur noch auf Mitarbeiterwunsch zu installieren. Begründet wurde das von Vera Layr, Pressesprecherin der IT-Magistratsabteilung 14, damit, dass „es für einige Arbeitsbereiche keine Open-Source-Alternative am Markt gibt“.\n\n2012 wurde das System de facto nicht mehr genutzt, mittlerweile ist auch die Webseite zu Wienux offline. Kritisiert wurde, dass mangelnde Ressourcen und starkes Lobbying von Microsoft wesentlich zum Scheitern des Projekts beigetragen haben.\n\nDie Distribution kommt ohne eigenständiges Partitionierungstool aus. Die erste Festplatte wird komplett gelöscht. Bei Parallelinstallationen mit anderen Systemen sollte Wienux, wie viele ältere Windowsversionen, zuerst installiert werden.\n\nIn den Jahren 2003 und 2004 wurde das Projekt LiMux in München gestartet und teilweise umgesetzt, das eine Umstellung aller ungefähr 14.000 Computer der Stadtverwaltung München auf Open-Source-Software bis 2008 zum Ziel hat. Bis Mai 2013 wurden 14.000 Arbeitsplätze auf den LiMux-Client migriert, was „deutlich über 80 % der Arbeitsplätze“ entspricht.\n\n\n\n\n"}
{"id": "556197", "url": "https://de.wikipedia.org/wiki?curid=556197", "title": "Amiga 3000T", "text": "Amiga 3000T\n\nDen Commodore Amiga 3000T, welcher die Towerversion der Desktopausführung Amiga 3000 darstellte, gab es in zwei Varianten: einmal als den normalen A3000T mit 68030-Prozessor und einmal als A3000T-040. Letzterer war mit einer Commodore-A3640-Turbokarte ausgestattet, welche dem Rechner einen 68040-Prozessor mit 25 MHz zur Seite stellte.\n\nDie Prototypen trugen noch den Namen A3500. Das eigentlich hochgeheime Projekt, das erst einige Zeit nach dem A3000 Desktop fertig wurde, sickerte aber durch, weil im Handbuch-Anhang zum A3000 in den dort abgedruckten Schaltplänen Varianten mit der Beschriftung „A3500“ nicht komplett eliminiert wurden.\n\nBei dem Gehäuse handelte es sich um ein abgewandeltes Modell des Commodore PC-60. Es wurden 1 MB Chip-RAM und 4 MB Fast-RAM verbaut, so dass der Rechner insgesamt über 5 MB Hauptspeicher verfügte. Beim 3000T wurde das ROM auch von Anfang an als Baustein auf der Platine ausgeliefert und nicht als Softkick auf der Festplatte, wie es bei den ersten Versionen der Desktopvariante geschah. Ansonsten war er mit dem 3000D(esktop) identisch, bis auf die Tatsache, dass das Board einen Zorroslot mehr und der Tower drei 5,25-Zoll-Laufwerk<nowiki></nowiki>sschächte besaß. Von diesen waren zwei vertikal eingebaut, was das Betreiben von CD-ROM-Laufwerken in diesen Schächten erschwerte, sofern man kein Laufwerk mit Caddy besaß.\n\nDer Amiga 3000T konnte auch als Amiga 3000T/UX mit einer auf die Amiga-Hardware angepassten Version von AT&T System V Release 4 namens AMIX bestellt werden, das auf einer QIC-Magnetbandkassette ausgeliefert wurde. Hierfür gab es für den Einsatz mit X11 sogar eine Maus mit drei Tasten. Eine Koexistenz von AmigaOS und AMIX auf demselben Rechner war prinzipiell möglich.\n\nVerschiedene Händler haben den Rechner in aufgewerteten Versionen angeboten, ihn zum Beispiel mit neueren Kickstarts und zuletzt gar mit CD-ROM-Laufwerk versehen. Sicherlich auch ein Grund, warum man den 4000er mit gemischten Gefühlen sah. \n\n"}
{"id": "558543", "url": "https://de.wikipedia.org/wiki?curid=558543", "title": "BasicLinux", "text": "BasicLinux\n\nBasicLinux (alternativ auch BasLin, BasLinux oder BL3) ist eine minimalistische Linux-Distribution in englischer Sprache, die speziell für extrem alte Systeme (386er und 486er) mit einer damals üblichen Menge an Arbeitsspeicher gedacht ist.\n\nDas System, welches den Linux-Kernel 2.2.26 nutzt, ist dabei ohne eine grafische Oberfläche bereits ab nur 3 MB RAM einsetzbar – moderne, vollwertige Linux-Systeme brauchen zwingend etwa das 10fache.\n\nDas System selbst hat eine Größe von nur 21,5 MB, wobei unter anderem auf BusyBox und diverse kommandozeilenbasierte Programme wie etwa den Mailclient Mutt zurückgegriffen wird. BasicLinux kann sich auch auf derselben Partition wie ein Windows-Betriebssystem befinden. Eine andere Variante startet von zwei Disketten, benötigt dadurch aber mehr Arbeitsspeicher (12 MB). Als Abwandlung von Slackware 4.0 können unter BasicLinux Pakete von diesem installiert werden.\n\n"}
{"id": "558581", "url": "https://de.wikipedia.org/wiki?curid=558581", "title": "Geexbox", "text": "Geexbox\n\nGeexbox (Eigenschreibweise GeeXboX) ist eine kleine Multimedia-Live-CD, die den Betrieb eines HTPC (Home Theater PC) ermöglicht und für die Wiedergabe von Audio-CDs, DVDs, VCDs aber auch einzelner Audio- und Video-Dateien konzipiert wurde.\n\nDie Multimedia-Distribution lässt sich von einer CD starten und wird in den Arbeitsspeicher geladen. Danach kann die Live-CD entfernt werden, um beispielsweise eine MP3-CD zum Abspielen einlegen zu können. Als Bootoption steht auch die Möglichkeit bereit, Geexbox auf die Festplatte zu installieren, von einem USB-Stick zu booten oder über Netzwerk (PXE) zu booten.\n\nDie ca. 136 MB große Linux-Distribution verwendet zum Abspielen der Multimedia-Dateien das XBMC, das alle gängigen Datei-Formate wiederzugeben vermag.\n\nEine ähnliche Entwicklung ist MoviX.\n\n"}
{"id": "559190", "url": "https://de.wikipedia.org/wiki?curid=559190", "title": "Nessus (Software)", "text": "Nessus (Software)\n\nNessus ist ein Netzwerk- und Vulnerability Scanner für Linux-, Unix-, Windows- und Mac OS X. Er basiert auf dem Client-Server-Prinzip, das heißt, dass auf einem Rechner der Nessusserver (\"nessusd\") gestartet wird und man sich anschließend mit einem oder mehreren Clients entweder vom lokalen oder einem entfernten Computer aus verbinden kann. Abgesichert wird dies durch SSL-Zertifikate und Passwörter.\n\nMit dem Start des Servers werden automatisch die Plug-ins geladen. Mit diesen Plug-ins lassen sich diverse Sicherheitslücken des Betriebssystems bzw. der Dienste, die auf dem zu scannenden Host laufen, finden. Die Plug-ins werden in der Nessus-eigenen Skriptsprache „Nessus Attack Scripting Language“ (NASL) erstellt.\n\nMit Hilfe des Client-Programms verbindet man sich darauf mit dem Server und stellt eine \"Session\" ein, in welcher man die Plug-ins, den Zielhost und andere Einstellungen eintragen oder verändern kann. Wurde der Scan auf einem Host ausgeführt, gibt der Nessus-Client eine Übersicht über die offenen Ports (das Scannen der Ports führt Nessus mit Hilfe von \"nmap\" durch) und eventuell gefundene Sicherheitslücken aus.\n\nIm Oktober 2005 wurde bekannt, dass das vorher unter der GPL stehende Projekt in Zukunft unter einer proprietären Lizenz weiterentwickelt wird. Ab der Version 3.0 steht Nessus damit nicht mehr frei zur Verfügung.\n\nUm weiterhin einen freien Scanner zur Verfügung zu stellen, hat sich das Projekt OpenVAS gebildet, das auf der letzten freien Version (2.2) des Scanners aufsetzt und diesen weiterentwickelt.\n\nDas Bundesamt für Sicherheit in der Informationstechnik hatte eine Open-Source-Software unter dem Namen BOSS (BSI OSS Security Suite) entwickelt, welche im Wesentlichen auf dem Sicherheits-Scanner Nessus aufbaute, stellte dann aber BOSS ein und empfahl OpenVAS.\n\n"}
{"id": "559332", "url": "https://de.wikipedia.org/wiki?curid=559332", "title": "HotSauce", "text": "HotSauce\n\nHotSauce, vorher bekannt als Project X, ist eine Mitte der 1990er Jahre bei Apple entwickelte Software zur dreidimensionalen Visualisierung von Webseiten. Anhand des Datenformats Meta Content Format wurden in einer WWW-Seite enthaltene HTML-Verweise in Baumform in einem dreidimensionalen Guckkasten dargestellt. Durch Klicken auf die Verweise konnte man zu der Seite gelangen, deren Adresse man angeklickt hatte.\n\nHotsauce wurde realisiert über ein Erweiterungsmodul für WWW-Browser, das für 68K-, PowerMacintosh- und Windows-Rechner angeboten wurde.\n\nBald nach der Vorstellung der Technik bekundete Netscape Interesse und sagte Unterstützung des Formats zu. Netscape plante die Unterstützung in seinem eigenen Projekt \"Constellation\". Innerhalb kurzer Zeit stellten hunderte großer Angebote im Internet Meta-Informationen im Meta-Content-Format zur Verfügung, darunter Yahoo!. Trotzdem stellte Apple 1997, kurz nachdem Steve Jobs zu dem Unternehmen zurückgekehrt war, die Weiterentwicklung des Projekts ein.\n\nHotSauce gilt heute als frühe Machbarkeitsstudie für einen 3D-Finder für das Macintosh-Betriebssystem Mac OS.\n\n"}
{"id": "559839", "url": "https://de.wikipedia.org/wiki?curid=559839", "title": "Hochverfügbarkeit", "text": "Hochverfügbarkeit\n\nHochverfügbarkeit (, \"HA\") bezeichnet die Fähigkeit eines Systems, trotz Ausfalls einer seiner Komponenten mit einer hohen Wahrscheinlichkeit (oft 99,99 % oder besser) den Betrieb zu gewährleisten. In Abgrenzung zur Fehlertoleranz kann es bei dem Betrieb im Fehlerfall zu einer Unterbrechung kommen.\n\nEin System wird als verfügbar bezeichnet, wenn es in der Lage ist, die Aufgaben zu erfüllen, für die es vorgesehen ist. Als Verfügbarkeit wird die Wahrscheinlichkeit bezeichnet, dass ein System innerhalb eines spezifizierten Zeitraums funktionstüchtig (verfügbar) ist. Die Verfügbarkeit wird als Verhältnis aus ungeplanter (fehlerbedingter) Stillstandszeit (= Ausfallzeit) und gesamter Produktionszeit eines Systems bemessen: \noder auch:\n\nDie genaue Definition von Hochverfügbarkeit kann variieren. Das \"\" (IEEE) gibt folgende Definition:\n\nEine andere Definition der Hochverfügbarkeit lautet:\n\nDie Frage, ab welcher Verfügbarkeitsklasse ein System als hochverfügbar einzustufen ist, wird je nach Definition der Verfügbarkeit unterschiedlich beantwortet.\n\nEine Verfügbarkeit von 99 % definiert im Allgemeinen keine Hochverfügbarkeit, sie wird allgemein heutzutage als grundlegend oder normal angesehen, zumindest bei qualitativ hochwertigen EDV-Geräten. Folglich wird von Hochverfügbarkeit erst ab 99,9 % oder höher gesprochen. Ob aber bereits 3*9 ausreichen oder erst 4*9 oder 5*9 ein System zum Hochverfügbaren System machen, ist quellen- und herstellerabhängig sowie unter dem jeweiligen Einsatzszenario zu bewerten. Im Allgemeinen kann ein System als hochverfügbar eingestuft werden, wenn seine jährliche Ausfallzeit im Bereich weniger Minuten (~99,999 % bzw. AEC-2) oder darunter liegt.\n\nBerechnet man mit der obigen Formel die Verfügbarkeit im Zeitraum eines Jahres, so entspricht eine Verfügbarkeit von 99,99 % beispielsweise einer Stillstandszeit von 52,6 Minuten. Man benutzt nun üblicherweise die Anzahl der Neunen in der Prozentangabe, um die Verfügbarkeitsklasse zu kennzeichnen: so bedeutet das obige Beispiel mit 99,99 % die Verfügbarkeitsklasse 4.\n\nBei einer gegebenen maximalen Ausfallzeit folgt eine Übersicht der relevanten Klassen 2 bis 6, wobei ein Jahr mit durchschnittlich 365,25 Tagen, der Monat als 1/12 Jahr gerechnet wird:\n\nDie rechnerische Verfügbarkeit bei einer Gesamtausfalldauer von einem Tag pro Jahr würde 99,73 % (fast VK3), von einer Stunde 99,989 % (praktisch VK4), von einer Minute 99,99981 % (fast VK6) und von einer Sekunde 99,9999968 % (VK7) betragen. Dies entspricht ziemlich genau den 3σ-, 4σ-, 5σ- und 6σ-Niveaus der Standardnormalverteilung.\nDie \"Harvard Research Group\" (HRG) teilt Hochverfügbarkeit in ihrer \" (AEC) in sechs Klassen ein.\n\nDie Hochverfügbarkeit wird in Unternehmen häufig im Rahmen von \" (SLA) definiert, und stellt ein wesentliches Bewertungskriterium für IT-Services dar.\n\nViele hochverfügbare Systeme müssen 24 Stunden * 7 Tage online sein, also das ganze Jahr „rund um die Uhr“. Manche dieser Systeme müssen die Eigenschaft der Hochverfügbarkeit jedoch nur für einen bestimmten Zeitausschnitt haben: Handelssysteme der \"Deutschen Börse\" etwa brauchen nachts und an börsenfreien Tagen nicht hochverfügbar zu sein. Die Hochverfügbarkeit bezieht sich bei diesen Systemen damit nur auf die Tageszeit und/oder die Arbeitstage, an denen es benötigt wird.\n\nGenerell streben HA-Systeme danach, so genannte \"\"-Risiken (SPOF) zu eliminieren (ein SPOF ist eine einzelne Komponente, deren Versagen zum Ausfall des gesamten Systems führt).\n\nEin Hersteller eines hochverfügbaren Systems muss dieses mit folgenden Merkmalen ausstatten: \n\nTypische Beispiele für Komponenten, die zum Erreichen einer erhöhten Fehlertoleranz eingesetzt werden, sind unterbrechungsfreie Stromversorgungen (USV; engl. , \"UPS\"), mehrfache Netzteile, ECC-Speicher oder der Einsatz von RAID-Systemen. Weiter kommen Techniken zur Serverspiegelung oder auch redundante Cluster zum Einsatz.\n\nJe höher die geforderte Verfügbarkeit ist, desto mehr Aufwand muss der Betreiber investieren in:\n\nHochspezialisierte Systeme mit höchsten Verfügbarkeiten sind\n\n\n\n"}
{"id": "562258", "url": "https://de.wikipedia.org/wiki?curid=562258", "title": "Fotomosaik", "text": "Fotomosaik\n\nEin Fotomosaik ist ein Bild, meist in Poster- bzw. Plakatgröße, das sich aus vielen kleinen Fotos zusammensetzt. Das Konzept ist angelehnt an die Technik des Mosaiks, bei der durch Zusammenfügen verschiedenfarbiger Teile ein Muster oder Bild entsteht. Von weitem betrachtet, verschmelzen die kleinen Bilder zu einem großen Gesamtbild. Das Erstellen von Fotomosaiks wird mit Computerprogrammen in kurzer Zeit ermöglicht.\n\nUm ein Fotomosaik zu erstellen, sind zahlreiche Aspekte zu beachten. Hauptsächlich gilt es, die Größe der Einzelbilder im Verhältnis zum Gesamtbild zu berücksichtigen. Ein Einzelbild sollte dabei so groß sein, dass es noch gut und scharf erkennbar ist und der Effekt interessant wirkt. Gleichzeitig dürfen es nicht zu wenig, zu große Einzelbilder sein, damit das Hauptmotiv gut erkennbar bleibt.\n\nDas Fotomosaik ist der neuzeitliche Nachfahr des Mosaiks. Das Mosaik ist ein Begriff aus der Kunst, der für die Technik des Zusammenfügens einzelner Teilchen steht. Es handelt sich um eine Flächendekoration. Hierfür werden Wandbilder oder Böden mit einzelnen, unterschiedlich farbigen Stückchen aus Ton, Holz, Stein, Glas oder ähnlichen Materialien verziert. Charakteristisch für das Mosaik ist, das die Anordnung der Einzelteile so erfolgt, das diese wiederum ein neues Bild oder Muster entstehen lassen. Der Begriff selbst leitet sich aus dem Spätlateinischen ab. Das Musaicum war ein den griechischen Musen gewidmetes Werk.\n\nDas ursprüngliche Mosaik ist bereits seit der Antike bekannt. Die Mosaikkunst gehört damit zu den ältesten Kunsttechniken. Vorläufer des Mosaiks wurden bereits 3000 Jahre v. Chr. angefertigt. Etwa ab dem 8. Jahrhundert v. Chr. treten Kieselmosaike auf. Im 2 und 3 Jh. v. Chr. revolutionierten die Griechen das Mosaik, indem sie Figuren und Bilder in ihre Mosaike einbanden. Erstmals wurden farbige Steinchen verwendet, die einheitlich zugeschnitten wurden. Diese geschnittenen Steine werden in der Mosaikkunst Tessera genannt.\n\nEs ist naheliegend, dass sich eine über die Jahrtausende entwickelte und weitverbreitete Kunstrichtung weiterentwickelt. So ermöglichen Fotografie und Bildbearbeitung die Herstellung von Fotomosaiken in den 90er Jahren. \n\nDie Besonderheit der Fotomosaike ist es, das die einzelnen „Steinchen“ selbst Fotos sind und Bildmotive enthalten. Es entsteht ein Bild aus vielen Bildern. Die nebeneinanderliegenden Bilder ergeben zusammen ein Hauptmotiv. Hierdurch entsteht für den Betrachter die Wirkung zweier Bildebenen. Wird das Fotomosaik mit Abstand betrachtet, ist lediglich das Hauptbild zu erkennen. Bei näherem Herantreten wird die zugrunde liegende Bildebene deutlich und die einzelnen Fotokacheln sichtbar.\n\nFür die Erstellung eines Fotomosaiks gibt es aktuell drei Varianten:\n\nFarbtonwert: Jede Fotokachel erhält einen Farbtonwert und wird zu einem Fotomosaik zusammengefügt.\n\nFarbtonübereinstimmung: Die Einzelfotos werden entsprechend ihrer Farbgebung sortiert und zum Hauptbild zusammengesetzt.\n\nEinfärbung: Die Kacheln werden mit der Farbe des Hauptfotos getönt und platziert.\n\nFür die Anordnung der Einzelfotos in den Fotomosaiken gibt es zahlreiche Möglichkeiten. Vielschichtige Mosaikstrukturen sind für die menschliche Wahrnehmung jedoch angenehmer als einfache, klare Strukturen. Ein System aus verworren wirkender Platzierung der Fotokacheln hat sich durchgesetzt. Fotos im Hochformat und im Querformat wechseln sich bei dieser Variante wahllos ab.\nDie nächst einfachere Anordnungsvariante ist die versetzte Struktur, bei die Einzelfotos horizontal verschoben platziert werden.\n\nAusgehend von einem typischen Fotoformat (4:3) und einer Postergröße von zum Beispiel 60 cm × 45 cm ergibt sich folgender Zusammenhang:\n\nEs werden 60 × 60 = 3600 Einzelbilder benötigt, um das Gesamtbild darzustellen. Dabei wird jedes Einzelbild 10 mm × 4,5 mm groß, wenn man auch bei den Einzelbildern von einem Bildverhältnis von 4:3 ausgeht. Verwendet man weniger Einzelbilder, erscheint das Hauptmotiv undeutlicher. Verwendet man ein kleineres Posterformat, werden die 3600 Einzelbilder sehr klein und bei üblichen Ausbelichtungsverfahren mit z. B. 300 ppi nicht mehr erkennbar. Das ist auch im obigen Beispiel des Ballonmosaiks der Fall.\n\nBei Freeware-Software wird im Allgemeinen das quadratische Zentrum des Fotos zugeschnitten, und die Ränder gehen verloren. Das kann bei vielen Motiven nachteilig sein. Sinnvoll sind Mosaikbilder erst ab einer Größe von 80 cm × 60 cm - optimal ab 100 cm × 70 cm o. ä. Dann ergeben sich bei ca. 1000–1200 Mosaiksteinen Einzelbildgrößen von 2 cm × 2 cm bis 3 cm × 3 cm. Diese sind dann auch auf Ausdrucken sehr gut erkennbar.\n\n"}
{"id": "562308", "url": "https://de.wikipedia.org/wiki?curid=562308", "title": "Prozessrechner", "text": "Prozessrechner\n\nProzessrechner sind Rechner, die durch folgende technische Spezifikationen gekennzeichnet sind:\n\nDadurch unterscheiden Prozessrechner sich insbesondere von Arbeitsplatzrechnern, die ihre Eingaben vorwiegend über die Tastatur erhalten.\n\nTypische Beispiele für Prozessrechner sind:\n\nGröße und Leistung von Prozessrechnern reichen vom Ein-Chip-Mikrocontroller bis zum vernetzten Großrechner. Besondere Bedingungen ergeben sich aus der Echtzeitanforderung, die Ausgangsdaten müssen für den laufenden Prozess rechtzeitig bereitstehen, sowie der Verlässlichkeit, da ein Rechnerabsturz etwa in einer großen Fertigungsanlage weitaus größere Schäden verursachen könnte als der Absturz eines Bürorechners.\n"}
{"id": "565470", "url": "https://de.wikipedia.org/wiki?curid=565470", "title": "Apples", "text": "Apples\n\nApples ist eine politische Gemeinde im Distrikt Morges des Kantons Waadt in der Schweiz.\n\nApples liegt auf , 7 km nordwestlich der Bezirkshauptstadt Morges (Luftlinie). Das Haufendorf erstreckt sich auf einer Kuppe am Rand des Hochplateaus, das am Jurafuss liegt, im Waadtländer Mittelland, an aussichtsreicher Lage über dem Tal der Morges.\n\nDie Fläche des 12,9 km² grossen Gemeindegebiets umfasst einen Abschnitt in der westlichen Randzone des Mittellandes. Der östlichste Gemeindeteil liegt im Einzugsgebiet des Flüsschens Morges und des Seitenbaches \"Le Curbit\". Im weitaus grösseren westlichen Gemeindeteil befindet sich eine eiszeitlich überformte Molassehügellandschaft mit ausgedehnten Wäldern und teils moorigen Senken. Dazu gehören der Wald von \"Les Bougeries\" (bis im Südwesten), der \"Bois de Saint-Pierre\" (bis im Norden) sowie der \"Bois de Savoye\" und der \"Bois de Fermens\" im Nordwesten. Der höchste Punkt von Apples wird mit auf der Waldhöhe \"La Chaux-Derrière\" erreicht. Ganz im Westen reicht das Gemeindegebiet an den Rand des Trockentals von \"Grand Marais\", das einst als Schmelzwasserrinne am Rand des eiszeitlichen Rhonegletschers diente. Dieses Tal wird nach Nordosten vom Veyron entwässert. Von der Gemeindefläche entfielen 1997 5 % auf Siedlungen, 48 % auf Wald und Gehölze und 47 % auf Landwirtschaft.\n\nZu Apples gehören die beiden isolierten Neubausiedlungen \"En Lèvremont\" () und \"La Motte\" () an den sanften Hängen des Morges-Tals sowie einige Einzelhöfe. Die Nachbargemeinden von Apples sind im Westen Ballens und Mollens, im Norden Pampigny und Sévery, im Osten Echichens und Clarmont, im Südosten Reverolle und Bussy-Chardonney sowie im Süden Yens.\n\nMit Einwohnern (Stand ) gehört Apples zu den mittelgrossen Gemeinden des Kantons Waadt. Von den Bewohnern sind 86,2 % französischsprachig, 7,3 % deutschsprachig und 1,9 % portugiesischsprachig (Stand 2000). Die Bevölkerungszahl von Apples belief sich 1900 auf 546 Einwohner. Nach 1970 (652 Einwohner) setzte eine rasche Bevölkerungszunahme mit fast einer Verdoppelung der Einwohnerzahl innerhalb von 30 Jahren ein.\n\nApples war bis ins 20. Jahrhundert ein vorwiegend durch die Landwirtschaft geprägtes Dorf. Heute spielt die Landwirtschaft als Erwerbszweig der Bevölkerung nur noch eine untergeordnete Rolle. Sie konzentriert sich auf Ackerbau und Viehzucht respektive Milchwirtschaft. Nördlich des Dorfes entstand eine kleine Industriezone, unter anderem mit der Herstellung von Schaltanlagen und Edelstahlrohren. Weitere Arbeitsplätze sind im Dienstleistungssektor vorhanden. In Apples gibt es eine Reithalle, ein Sport- und Schulzentrum sowie ein regionales Alters- und Pflegeheim. In den letzten Jahrzehnten hat sich das Dorf dank seiner attraktiven Lage zu einer Wohngemeinde entwickelt. Viele Erwerbstätige sind Wegpendler, die vor allem in den Städten Morges und Lausanne arbeiten.\n\n1981 wurde auf einem Bauernhof in Apples das Unternehmen Logitech von Daniel Borel und einigen seiner Studienkollegen der Stanford University gegründet.\n\nDie Gemeinde ist verkehrstechnisch gut erschlossen. Sie liegt an der Hauptstrasse, die von Morges nach Bière führt. Am 1. Juli 1895 wurde die Schmalspurbahn Bière–Apples–Morges (BAM) mit einem Bahnhof in Apples in Betrieb genommen. Am 12. September 1896 folgte die Einweihung der ehemaligen Schmalspurbahn Apples–L’Isle (AL). Auf der Bahnstrecke von Apples nach L'Isle werden infolge des geringen Verkehrs und der weit von den Bahnstationen abgelegenen Dörfer mehrere Zugsverbindungen mit Kleinbussen statt Zügen ausgeführt.\n\nApples kann auf eine lange Siedlungstradition zurückblicken. Auf dem Gemeindegebiet wurden Spuren aus dem Neolithikum (Schalensteine), aus der Bronzezeit und aus der Römerzeit gefunden. Die erste richtige Siedlung wurde im 5. Jahrhundert von den Burgundern gegründet und hiess vermutlich \"Iplingen\". Dieser Name wandelte sich mit der Romanisierung wahrscheinlich über \"Iplens\" in \"Aplis\". Unter dieser Bezeichnung wurde das Dorf im Jahr 1011 erstmals in einer Urkunde erwähnt, als König Rudolf III. die Kirche mit dem Dorf dem Kloster Romainmôtier schenkte. Später erschien die Schreibweise \"Aples\" (1222) und 1328 der heutige Ortsname.\n\nMit der Eroberung der Waadt durch Bern im Jahr 1536 kam Apples als Exklave unter die Verwaltung der Vogtei Romainmôtier. Nach dem Zusammenbruch des Ancien régime gehörte das Dorf von 1798 bis 1803 während der Helvetik zum Kanton Léman, der anschliessend mit der Inkraftsetzung der Mediationsverfassung im Kanton Waadt aufging. 1798 wurde Apples zunächst dem Bezirk Morges, 1803 dem Bezirk Aubonne zugeteilt.\n\nDie mittelalterliche Kirche von Apples steht auf den Fundamenten eines romanischen Vorgängerbaus. Sie wurde 1838 und 1905 umgestaltet.\n\n"}
{"id": "566432", "url": "https://de.wikipedia.org/wiki?curid=566432", "title": "Total Commander", "text": "Total Commander\n\nTotal Commander ist ein zweispaltiger Dateimanager für Windows nach dem Vorbild des Norton Commanders. Es gibt auch kostenlose Varianten für die Smartphone- und Tabletbetriebssysteme Android, Blackberry, Windows Phone und Windows 10 Mobile.\n\nDas Shareware-Programm wird von dem Schweizer Christian Ghisler entwickelt.\n\nDie erste öffentliche Version des Programms wurde in der Programmiersprache Turbo Pascal programmiert und erschien am 29. September 1993 unter dem Namen \"Windows Commander\" für die DOS-Betriebssystemerweiterung Windows. Hintergrund ist, dass damals viele Power-User für größere Dateioperationen den originalen Norton Commander in Verbindung mit diversen Befehlszeilenprogrammen direkt in der DOS-Ebene bevorzugten. Mit einigen Tricks war das auch nach der Einführung von Windows 95 und mit langen Dateinamen möglich. 1998 kam zwar die Windows-Version des Norton Commander heraus, allerdings unterstützte diese einige der unter der MS-DOS-Version gewohnten Tastaturbefehle nicht mehr, war teuer und wirkte technisch überholt.\n\nUnter anderem wegen dieser konzeptionellen Fehler konnte sich der \"Windows Commander\" gegen den Norton Commander durchsetzen, denn er beherrschte nicht nur alle seine Funktionen und Tastenkombinationen, sondern auch noch zusätzliche, die sich als sehr hilfreich erwiesen.\n\nIm Jahr 2002 wurde der Name \"Windows Commander\" in den heutigen Namen \"Total Commander\" geändert, um rechtliche Probleme mit dem von Microsoft eingetragenen Warenzeichen \"Windows\" zu vermeiden.\n\nAb Version 6.50 2004 passte die Installationsdatei nicht mehr auf eine 1,44-MB-Diskette, was Christian Ghisler nicht ohne Wehmut bekannt gab.\n\nInzwischen wird der Total Commander in Lazarus bzw. Free Pascal entwickelt und ist einer der bekanntesten Vertreter der zweispaltigen Dateimanager nach dem Vorbild des Norton Commander.\n\nDas Programm kann auf vielfältige Weise an die Bedürfnisse des Nutzers angepasst werden und lässt sich vollständig wahlweise über die Maus und die Tastatur steuern, wobei sich die Tastenkombinationen am \"Norton Commander\" (für MS-DOS) orientieren.\n\nEr bietet unter anderem folgende Funktionalitäten:\n\nÜber die Plug-in-Unterstützung ist sowohl die Einbindung externer Plugins als auch normaler Programme (zum Beispiel Editoren) möglich. Die integrierte Vorschau nutzt auf Wunsch die Routinen eines installierten IrfanView oder XnView zur Bilddarstellung.\n\nTotal Commander läuft unter allen Windows-Versionen ab Windows 3.11 und NT 3.1. Eine 16-bit-Version für Windows 3.11 wird weiterhin unterstützt. Der portable Betrieb ist durch Anpassung der INI-Datei möglich. Spezielle Installationspakete zur Installation auf USB-Speichersticks oder U3-Sticks sind gleichfalls erhältlich.\n\nAb Version 8.0 beta steht der Total Commander zusätzlich als 64-Bit-Version zur Verfügung. Die bisherigen 32-Bit-Plugins funktionieren damit nicht und werden schrittweise angepasst. Beide Varianten (32- und 64-Bit) können in das gleiche Verzeichnis installiert werden und teilen damit die Einstellungen, wie beispielsweise die Belegung der Button-Bars mit Programmstart-Icons. Seit Mitte Mai 2012 ist die finale Version 8.0 verfügbar.\n\nAb Version 9.0 (RC1) unterstützt der Dateimanager eine abstellbare vertikale Buttonleiste zwischen den beiden Dateifenstern. Neben zahlreichen anderen Verbesserungen und Erweiterungen gibt es neugestaltete Icons für das Hauptmenü und die Buttonbars, die beim ersten Start oder später über Rechtsklick auf den Buttonbar wählbar sind sowie gedimmte Symbole für versteckte Dateien bzw. Verzeichnisse.\n\nAb Version 2.60 PB1 unterstützt der TC auch die Schreibfunktionen von Android 5 auf der externen SD-Karte.\n\nDer Total Commander wird für Windows als Shareware vertrieben. Dabei wird nach dem Programmstart eine Zahlungsaufforderung eingeblendet, die mit wechselnden Ziffern weggeklickt werden muss; eine weitere Einschränkung oder Beschneidung der Funktionen existiert nicht. Mit dem Kauf einer Key-Datei entfällt diese Aufforderung, auch für zukünftige Versionen – seit 1993 besteht ein unbegrenztes Updaterecht.\n\nNach einer längeren Beta-Testphase wird der Total Commander für das Smartphone-Betriebssystem Android angeboten. Seit dem 20. Dezember 2011 konnten Freigabekandidaten () heruntergeladen werden. Ab dem 18. Mai 2012 sind die endgültigen Versionen als Download verfügbar. TC4Android bleibt Freeware und hat eine unbegrenzte Laufzeit.\n\nDie programmtypische Zweifenster-Darstellung wird in der Vertikaldarstellung dadurch realisiert, dass mittels einer horizontalen „Wischgeste“ zwischen dem rechten und linken Fenster gewechselt wird. Befindet sich der Bildschirm im horizontalen Modus, teilt sich die Anzeige in die bekannten zwei Hälften.\n\nDer Dateimanager hat ähnliche Funktionen wie der Total Commander für Windows Mobile, beispielsweise das Kopieren, Verschieben, Löschen von einzelnen oder markierten Dateien und ganzen Verzeichnissen. Textdateien sind erstell-, betracht- und änderbar.\n\nMit Hilfe von Erweiterungen () können zusätzliche Funktionen integriert werden. Eine Erweiterung ermöglicht beispielsweise den Zugriff auf die im Funknetz () befindlichen Netzwerkrechner.\n\nDie Freigabekandidaten waren nicht über den \"Google Play Store\" (früher \"Android Market\"), sondern nur über einen direkten Download von der Herstellerwebsite erhältlich. Version 1.01 sowie alle Nachfolgeversionen einschließlich der Plugins sind auch über den Google Play Store erhältlich.\n\nAb Version 2.01 verfügt der TC4Android über einen internen Audio- und Videoplayer. Mediendateien können nun direkt vom Netzwerkspeicher (bei installiertem LAN-Plugin) oder über das Internet abgespielt werden und müssen nicht erst vor dem Abspielen auf das Android-Gerät geladen werden.\n\nFür verschiedene Cloudspeicher wie beispielsweise Dropbox oder Google Drive lassen sich Plugins nachinstallieren, die einen Zugriff auf die Internetdatenspeicher in der dateimanagertypischen Weise ermöglichen.\n\nDie Variante für Windows Mobile kann den integrierten Dateimanager bezüglich des Funktionsumfangs ersetzen. Zusätzlich zu den Basisfunktionalitäten bietet diese Variante:\n\n\n"}
{"id": "567627", "url": "https://de.wikipedia.org/wiki?curid=567627", "title": "Linux Terminal Server Project", "text": "Linux Terminal Server Project\n\nDas Linux Terminal Server Project (LTSP) ist ein Linux-Programmpaket, das dazu dient, Benutzern von Terminals Zugriff auf den Terminalserver zu gewähren, von dem aus Anwendungen ausgeführt und mittels des Terminals gesteuert werden können. Einsatzgebiete sind beispielsweise Internetcafés oder Büros, bei denen an jedem Arbeitsplatz die gleichen Anwendungen zur Verfügung stehen und die Kosten pro Arbeitsplatz gering gehalten werden sollen. Der Effekt der Kostenminimierung entsteht dadurch, dass für die Terminals keine schnellen Systeme und mitunter keine Festplatten vorgehalten werden müssen. LTSP ist freie Software und seine Veröffentlichung steht unter der Lizenz GPLv2.\n\nDie Terminals können aus alten, leistungsschwachen Rechnern bestehen (z. B. Pentium I mit 90 MHz, ohne Festplatte, 100 MBit Netzwerkkarte, Grafikkarte; ein vorhandener PCI-Bus erleichtert hierbei die Hardwareerkennung) oder aus dedizierten Thin Clients. Als Wirtsbetriebssystem dient eine beliebige Linux-Distribution. Das Booten der Terminals erfolgt über PXE-fähige Netzwerkkarten, mittels Diskette oder CD-ROM.\n\nTechnisch gesehen vereinfacht LTSP die Ausnutzung der Fähigkeiten des grafischen X-Servers, ein beliebiges Linux-Programm in die zwei Teile Programmausführung und Anzeige zu trennen. Die Programmausführung erfolgt auf einem leistungsstarken Server, während die Anzeige zusammen mit der Eingabe über Tastatur oder Maus auf beliebig vielen Terminals erfolgen kann. Die Anzahl der Thin Clients wird hier von der Übertragungsrate des Netzwerkes und der Leistungsfähigkeit des Servers bestimmt.\n\nFür den Einsatz von lokalen Geräten an den Thin Clients verwendet LTSP eine eigene Entwicklung, eine Mischung aus udev und FUSE. Mit dieser ist es möglich, USB-Massenspeicher, CD-Roms und Disketten an die Clients anzuschließen und sie auf dem Server auszulesen.\n\nLTSP hat die gleichen Vor- und Nachteile, welche jede Terminalserver-Installation mit sich bringt. \n\nWeitere spezifische Vorteile:\n\nDa die eigentlichen Anwendungen auf dem Server ausgeführt werden und die Darstellung der Ausgabe auf den Clients über einen \"entfernten\" X-Server auf den Client über eine Ethernetverbindung realisiert wird, kommt es zu Problemen, z. B. beim Einsatz von OpenGL-Technologie. Angepasste Grafikkartentreiber stellen ein DRI (Direct Rendering Interface) bereit, über die Applikationen 3D-Anweisungen direkt über den internen Computerbus an die Grafikkarte leiten können. Ist die Grafikkarte physikalisch von der Anwendung getrennt, also nicht im selben Computersystem, ist diese \"schnelle\" Methode der 3D-Darstellung nicht möglich. Dies ist in LTSP-Systemen der Fall. Trotzdem bietet das X-Server-System die Möglichkeit, 3D-Anweisungen an die Grafikkarte zu leiten, ohne das Ergebnis lokal berechnen zu müssen. Voraussetzung ist natürlich ein X-Server mit Treibern, die 3D-Funktionen unterstützten (bei NVIDIA und ATI proprietär).\n\n3D-Anwendungen sind also durchaus auf den Thinclients abbildbar, jedoch abhängig von der Ethernet-Anbindung, da alle \"OpenGL\"-Befehle über das Netzwerk gesendet werden.\n\nSpiele mit relativ simplen 3D-Anforderungen wie X-Moto sind über eine 100-Mbit-Verbindung durchaus betrachtbar.\n\n3D-Modelling-Anwendungen fallen nicht gänzlich in dieses Feld, da die finale 3D-Berechnung durch Software Renderingengines bewerkstelligt wird. Die OpenGL-Funktionalität des XServers findet allerdings während der Modellierung an sich Verwendung. \nBlender lässt sich via LTSP und vernünftigen Grafikkarten-Treibern beispielsweise gut bedienen.\n\nFür die Videodarstellung gilt ähnliches. Unterstützt der X-Server mit den entsprechenden Treiber-Modulen ein passendes Video-Protokoll wie XVideo (wird etwa vom Standard VESA-Modul nicht unterstützt), ist eine gute Videodarstellung je nach Verbindungsrate zu erwarten. Mit einer 100-MBit-Verbindung ist das Ansehen einer SVCD durchaus möglich, HDTV-Inhalte überfordern jedoch selbst ein Gigabit-Netzwerk.\n\nÄltere LTSP-Versionen stellen durch ihr Projekt eigene Softwarepakete der Linux-Umgebung für die Clients zur Verfügung (Kernel, libraries, x-server etc). Diese sind auf Minimalismus, Größe und Geschwindigkeit optimiert. Ein Nachinstallieren von Programmen auf den Clients (bzw. deren NFS-Umgebung) bereitet schon alleine durch das Fehlen einer Paketverwaltung wie RPM oder APT Schwierigkeiten.\n\nMit der Zeit wuchs nicht nur die Leistung der Server- und Desktop-Rechner, sondern auch die der Thin Clients. Clients mit einer Leistung kleiner 300 MHz sind mittlerweile eher unüblich. So gilt es doch in der Regel zumindest gängige Monitore mit einer Auflösung >= 1280 × 1024 zu bedienen, wofür wiederum eine Grafikkarte mit AGP-Steckplatz gängig ist. \n\nSomit ergeben sich für die Clients auch größere Arbeitsgebiete, wie etwa „Video decoding on client side“, Irda-Ansteuerung (Infrarot-Fernbedienung), Installation proprietärer Grafikkartentreiber (für AIX, TV-Out, OpenGL beispielsweise), Nutzung lokaler Geräte wie TV-Karten etc.\n\nDie aktuellen LTSP-Versionen liefern den Clients eine Umgebung, die einer regulären Linux-Distribution entspricht. Dadurch kann der Benutzer das Linux-System der Thin Clients beliebig erweitern und Pakete nach eigenem Wunsch installieren. So lassen sich die Clients mit besserer Unterstützung für ACPI, Irda usw. erweitern. Da dies wenig mit dem ursprünglichen LTSP-Projekt gemein hat, erhielt es den Codenamen 'muekow'.\n\n\n"}
{"id": "568147", "url": "https://de.wikipedia.org/wiki?curid=568147", "title": "Sun Ray", "text": "Sun Ray\n\nEine Sun-Ray-Infrastruktur dient zur Virtualisierung von Desktopcomputer-Umgebungen und besteht aus mindestens einem Sun-Ray-Client und einem Sun-Ray-Server. Der virtuelle Desktop befindet sich physikalisch am Sun Ray Server oder auf einem anderen Computer.\n\nDer zeitkritische Datenaustausch zwischen dem Sun Ray Client und dem Sun Ray Server basiert auf dem (optional) verschlüsselten \"Appliance Link Protocol (ALP)\", welches (im Unterschied zu anderen Thin Clients) primär auf UDP basiert. TCP findet ebenfalls Verwendung, wird aber nicht zur bandbreitenintensiven Grafikübertragung verwendet.\n\nDer Sun-Ray-Client ist ein (Ultra) Thin Client von Sun Microsystems, welcher erstmals im September 1999 vorgestellt wurde. Der Client ist auf die Idee der älteren JavaStations von Sun Microsystems aus dem Jahr 1996 zurückzuführen. Da Sun Microsystems Anfang 2010 von der Oracle Corporation übernommen wurde, firmieren die Sun-Ray-Geräte nun unter \"Oracle Sun Ray Clients\".\n\nDer Client führt kein eigenes Betriebssystem aus, sondern stellt (unter anderem) die Grafikdaten eines Sun-Ray-Servers dar.\n\nDer Client hat eine CPU und einen Speicher, benutzt diese aber nur, um die Verbindung zum Sun-Ray-Server herzustellen, die Grafikdaten des Servers zwischenzuspeichern und zu rendern sowie die seriellen und USB-Ports an den Server durchzureichen und die Audiodaten (input/output) zu übertragen. Es können direkt am Gerät USB-Geräte (Tastatur, Maus, Massenspeicher) angeschlossen werden. Die USB-Schnittstellen können seit 2009 auch andere Geräte wie z. B. Scanner an den Server durchreichen. Der Client hat keine eigenen Massenspeicher (lediglich einen Flash-Speicher für die Firmware und optional eine lokale, minimale Konfiguration) und keine beweglichen Teile.\n\nDurch den Verzicht auf potente Hardware ist die Leistungsaufnahme der Clients im Betrieb mit 4 Watt (Sun Ray 2) bis 8 Watt (Sun Ray 2FS, Angaben laut Datenblatt) niedriger als die eines durchschnittlichen Desktop-PCs. \n\nWeiterhin gibt es Clients, die in einem Monitor integriert sind. Das aktuelle Modell (2011) Sun Ray 3i entspricht dem Sun Ray 3, wobei die Auflösung des Monitors (1920 × 1080 Pixel) fest vorgegeben ist. Das Vorgängermodell war der Sun Ray 270 Client mit einem 17-Zoll-Monitor mit 1280 × 1024 Pixeln.\n\nDa kein Betriebssystem auf dem Client ausgeführt wird, wird er auch als Ultra Thin Client bezeichnet. Der Client selbst ist durch das Fehlen eines Betriebssystems praktisch unanfällig gegen Hackerangriffe. Auch wenn ein Sun-Ray-Client physikalisch gestohlen wird, sind alle Daten sicher im Rechenzentrum.\n\nAlle Sun Rays enthalten einen eingebauten Lautsprecher und Smartcard-Leser. Der Smartcard-Leser kann sowohl für PKI-Infrastrukturen oder für die Sun-Ray-interne Authentifizierung verwendet werden. Durch die eingebaute Smartcard-Unterstützung können laufende Sitzungen einfach von einem Client zum anderen bewegt werden, ohne laufende Programme beenden oder sich abmelden zu müssen.\n\nIm Jahr 2005 übernahm Sun Microsystems Inc. die Tarantella Inc. und übernahm Programmteile der Secure Global Desktop Software zum Anbinden von Windows-Terminalservern in die Sun-Ray-Server-Software. Dies führte zur großen Verbreitung der Sun Ray zwecks Anbindung an Windows-Terminalserver und VDI-Infrastrukturen.\n\nDie erste Modellgeneration der Sun Rays wurde Mitte 2006 durch drei neue Geräte ersetzt. Die dritte Generation wurde im April 2010 mit der Sun Ray 3 Plus gestartet.\n\nEs gibt auch mehrere Implementierungen des Sun-Ray-Clients in Software:\n\nDie Software des Sun-Ray-Servers setzt das Betriebssystem Linux (X86) oder Solaris (Sun SPARC oder X86) voraus.\n\nIm sogenannten \"Kiosk\"-Modus (früher \"CAM\") kommuniziert der Server mit einem anderen Computer, um dessen virtuellen Desktop an den Client zu übertragen. In der Regel wird dabei das RDP-Protokoll verwendet, um den anderen Computer zu erreichen. Von Sun (jetzt Oracle) wurde eigens dafür ein RDP-Client (codice_1) entwickelt.\n\n\nAm 15. Juli 2013 hat Oracle mitgeteilt, dass die weitere Entwicklung von Sun-Ray-Software und Sun-Ray-Clients eingestellt wird.\n\n"}
{"id": "568994", "url": "https://de.wikipedia.org/wiki?curid=568994", "title": "Halo (Bildbearbeitung)", "text": "Halo (Bildbearbeitung)\n\nEin Halo ist ein (in der Regel ungewollter) Effekt in der digitalen Bildbearbeitung (\"Heiligenschein\" um das Bildobjekt).\nEr tritt auf, wenn in einem Objekt durch Antialiasing ein Farbübergang zum Hintergrund besteht und der Hintergrund ausgetauscht wird. Dies passiert häufig in folgenden Fällen:\n\nDas Problem kann vermieden werden, wenn das Datenformat Antialiasing durch unterschiedliche Transparenzstufen oder einen Alphakanal unterstützt. Arbeitet man mit Vektorgrafik und wandelt das Bild erst nach Austauschen des Hintergrunds in eine Rastergrafik, tritt das Problem ebenfalls nicht auf. Dies ist bei Digitalfotos nicht möglich, da sie nur als Rastergrafik vorliegen.\n"}
{"id": "569784", "url": "https://de.wikipedia.org/wiki?curid=569784", "title": "Quantitative Struktur-Wirkungs-Beziehung", "text": "Quantitative Struktur-Wirkungs-Beziehung\n\nDer Begriff Quantitative Struktur-Wirkungs-Beziehung stammt von dem englischen Quantitative Structure-Activity Relationship\" (QSAR) ab. Er beschreibt die Erstellung einer quantitativen Beziehung zwischen einer pharmakologischen, chemischen, biologischen, physikalischen (z. B. Siedepunkt) Wirkung eines Moleküls mit seiner chemischen Struktur. Teilweise findet man in der Literatur auch die Bezeichnung QSPR, das Acronym für Quantitative Structure Property Relationship\", hier beschränkt man sich darauf die Beziehung zwischen den physikalischen und chemischen Eigenschaften eines Moleküls und seiner Struktur darzustellen. Insbesondere im Bereich der Wirkstoffentwicklung und Chemoinformatik finden diese Prinzipien eine breite Anwendung.\n\nErste Arbeiten in diesem Gebiet sind vermutlich bereits im Jahre 1842 durch Hermann Kopp entstanden, welcher eine lineare Beziehung zwischen den Siedepunkten von Alkanen mit deren Kettenlänge herstellte (formula_1 °C). Ein Pionier auf diesem Gebiet in der Biochemie war Corwin Hansch in den USA.\n\n\n\n"}
{"id": "570222", "url": "https://de.wikipedia.org/wiki?curid=570222", "title": "SolidWorks", "text": "SolidWorks\n\nSolidWorks ist ein 3D-CAD-Programm. Hersteller ist das Softwareunternehmen \"Dassault Systèmes SolidWorks Corp.\", eine Tochtergesellschaft der französischen Dassault Systèmes.\n\n\nSolidWorks ist eine CAD-Software, mit der parametrische Modelle, Baugruppen und Zeichnungen erzeugt werden können. Typisch ist die Arbeitsweise mit Beziehungen (die Ausrichtung von Elementen zueinander) und Bemaßungen (Ausrichten durch Werte). Eine Modularisierung in Einzelschritte (Features) garantiert eine Bearbeitung früherer Arbeitsschritte zu jedem Zeitpunkt. So entstehen Teile, die in Baugruppen verwendet werden und dann in Zeichnungen abgeleitet werden. Die drei Arbeitsbereiche können auch aus den anderen Bereichen zugegriffen werden, wobei jedes Maß über Gleichungen und Tabellen gesteuert werden kann.\n\nDie Programmoberfläche ist Ribbon-ähnlich und gliedert sich thematisch, beispielsweise in Schweiß-, Blech- und Gusskonstruktionen. Zusatzanwendungen wie Rendering und die Simulationsfunktionen können bei Bedarf eingeblendet werden.\n\nIn der günstigen Ausführung \"SolidWorks Standard\" sind bereits einfache FEM- und Flüssigkeits- und Bewegungssimulationen vorhanden.\n\nSolidWorks benutzt den Parasolidkern der UGS in Lizenz. Da SolidWorks auf dem Parasolidkern aufsetzt und sich die beiden Produkte CATIA und SolidWorks als Wettbewerber darstellen, findet derzeit kein gegenseitiger Austausch von Funktionalitäten statt. Während SolidWorks für kleinere und mittlere Maschinenbauer gedacht ist, hat CATIA seinen Fokus eher im Bereich der OEM-Prozessketten, im Produktdesign und generell bei den Zulieferern der Automobil-, Luft- und Raumfahrtindustrie.\n\nDer DWG-Editor ist seit 2011 nicht mehr Bestandteil von SolidWorks. Stattdessen steht kostenfrei das 2D-Editierprogramm DraftSight zur Verfügung, das eine DWG-Schnittstelle bietet.\n\nSolidWorks wird laut Hersteller von mehr als 2,1 Millionen Konstrukteuren und Entwicklern weltweit eingesetzt.\n\n\n\n\n\n„SolidWorks Standard“ ist das Kernprodukt mit folgenden Modulen:\n\nDas „SolidWorks Professional“-Paket besteht aus „SolidWorks Standard“ und folgenden Modulen:\n\nDas „SolidWorks Premium“- Paket besteht aus „SolidWorks Standard“ und „Professional“ sowie folgenden Modulen:\n\n\nDas SolidWorks Student Design Kit ist eine Testversion der SolidWorks Education Edition mit reduzierter Funktionalität.\n\nDie SolidWorks Education Edition ist eine lizenzierte Version von SolidWorks für Schüler. Das Paket umfasst unter anderem die SolidWorks Premium Software sowie SimulationXpress zur Konstruktionsprüfung.\n\nDie SolidWorks Student Edition kann von Studenten außerhalb des Klassenzimmers benutzt werden. Diese Edition ist mit oder ohne SolidWorks SimulationXpress erhältlich und enthält die SolidWorks Standard-Software sowie eDrawings.\n\nSolidWorks ist konzipiert für die mechanische Konstruktion in kleinen und mittleren Unternehmen und findet Verwendung in Maschinenbau, Werkzeugbau, Blechbearbeitung, Industrie- und Konsumgüter-Design, Schiffbau, Medizintechnik und Anlagenbau.\n\n\n"}
{"id": "571695", "url": "https://de.wikipedia.org/wiki?curid=571695", "title": "Adobe Audition", "text": "Adobe Audition\n\nAdobe Audition [] (früher Cool Edit Pro) ist eine professionelle Audioeditor-Software des Unternehmens Adobe Systems zum Bearbeiten von digitalen Audiodateien. Es beherrscht sowohl die direkte als auch die indirekte Bearbeitung von Tonmaterial (destruktive und nicht-destruktive Bearbeitung). Dateien können aus vielen unterschiedlichen Quellen geöffnet bzw. aufgenommen werden und in allen allgemeinen Formaten abgespeichert werden. Bis zur (aktuell kostenlos angebotenen) Version 3.0 bietet Adobe Audition einen MIDI-Sequenzer und ReWire Technologie (Propellerhead Software) zur Einbindung von Drittsoftware. Seit der Version CS5.5 (Version 4.0) war Adobe Audition Bestandteil des Programmpakets der Adobe Creative Suite und ist erstmals auch für OS X erschienen. Mit Version 6.0 wird das Programm als Adobe Audition CC im monatlichen Einzelabonnement sowie im Abonnement der Adobe Creative Cloud angeboten.\n\nDas Programm wurde Mitte der 1990er ursprünglich von dem Unternehmen \"Syntrillium\" unter dem Namen \"Cool Edit\" entwickelt. In der Grundversion wurden Audiodateien aufgrund der seinerzeit geringeren Rechenleistung destruktiv bearbeitet, um ein Arbeiten in Echtzeit zu ermöglichen. In späteren Versionen wurden dann die nicht destruktive Bearbeitung von Audiodateien und die Mehrkanal-Abmischung möglich.\n\nIm Mai 2003 wurde Cool Edit Pro 2.1 von Adobe aufgekauft und als Adobe Audition in die eigene Produktpalette eingegliedert und weiterentwickelt.\n\nDie erste Version wurde am 18. August 2003 veröffentlicht und hatte gegenüber der letzten Veröffentlichung von Cool Edit keine erkennbaren Veränderungen in der Funktion.\n\nIm Mai 2004 veröffentlicht, erfuhr das Programm größere Veränderungen. Es kamen Funktionen wie Tonhöhenkorrektur, Frequenzbereichanalyse, eine Projektansicht für die CD-Erstellung und die grundlegenden Videobearbeitungswerkzeuge aus Adobe Premiere hinzu, außerdem die Integrationsmöglichkeit von Dateien beider Programme in das jeweils andere Programm.\n\nMit Adobe Audition 2.0 wurde am 17. Januar 2006 erstmals eine Version für den professionellen Anwenderbereich veröffentlicht. Neben der neu strukturierten Benutzeroberfläche und neuen Bearbeitungsmöglichkeiten, wie sie in anderen Audioeditoren bereits verfügbar waren, erfolgte die Implementierung von ASIO-Treibern und VST-Plugins, entwickelt jeweils von der Steinberg Media Technologies GmbH. Adobe Audition 2.0 wurde zudem in das Software-Paket Adobe Production Studio eingegliedert, allerdings nicht zusammen mit der bereits im April 2005 herausgebrachten Adobe Creative Suite 2 ausgeliefert, welche seinerzeit das weniger leistungsfähige \"Adobe Soundbooth\" enthielt.\n\nAdobe Audition 3.0 wurde am 8. November 2007 veröffentlicht und nunmehr in die Adobe Creative Suite 2 integriert. Es wurden eine Spektralanalyse und neue Effekte hinzugefügt, die Multitrack-Oberfläche neu gestaltet und die Unterstützung für virtuelle Instrumente (VSTi) erweitert. Adobe Audition 3.0 erfuhr dann zunächst keine weitere Entwicklung und wurde in der Adobe Creative Suite 3 wieder durch Soundbooth CS3, in der Adobe Creative Suite 4 durch Soundbooth CS4 und in der Adobe Creative Suite 5 durch Soundbooth CS5 ersetzt.\n\nAdobe Audition 3.0 kann mittlerweile für jedermann als Vollversion von der Herstellerseite nach Anlegen eines kostenlosen Accounts heruntergeladen werden. Adobe rät hier allerdings vom Ausführen nicht unterstützter und veralteter Software ab. Weiters wird darauf hingewiesen, dass nur Inhaber einer gültigen Lizenz für die jeweiligen Produkte berechtigt sind, die zur Verfügung gestellten Aktivierungscodes zu verwenden. Neben Avids \"Pro Tools\", Apples \"Logic Pro\" oder dessen Vorversion, Emagics \"Logic Audio Platinum\", kann Adobe Audition 3.0 allerdings auf Grund seines MIDI-Sequenzers MIDI-Daten eines E-Pianos oder eines Keyboards aufzeichnen und mit Hilfe von virtuellen Instrumenten wie Native Instruments' \"Kontakt 4\" Sample-Libraries (Klangbibliotheken) wie beispielsweise die \"Vienna Symphonic Library\" laden und MIDI-Dateien orchestrieren. Adobe Audition 3.0 ist daher nach wie vor für Komponisten gleichwie für Tonmeister und Homerecording relevant, wenngleich es auch keinen Noteneditor aufweist.\n\nAdobe Audition CS5.5 wurde am 11. April 2011 im Rahmen der Aktualisierung auf die aktuelle Adobe Creative Suite 5.5 vorgestellt. Seit der Creative-Suite-Version 5.5 wird das in CS3 eingeführte \"Adobe Soundbooth\" durch Adobe Audition in den Versionen „Production Premium“ und „Master Collection“ ersetzt, woraus zu erkennen ist, dass Soundbooth nicht mehr weiterentwickelt werden wird.\n\nAdobe Audition CS6 wurde am 7. Mai 2012 im Rahmen der Adobe Creative Suite 6 veröffentlicht.\n\nAdobe Audition CC (Version 6.0) wurde am 17. Juni 2013 als Teil der Adobe Creative Cloud erstmals sowie ausschließlich mit 64-Bit-Architektur veröffentlicht. Adobe Audition CC kann daher nur über ein Abonnement genutzt und ausschließlich auf 64-Bit Systemen installiert werden, auch können nur mehr 64-bit VST-Plugins implementiert werden. Die leistungsfähigere Architektur ermöglicht beispielsweise ein schnelleres Exportieren oder die Echtzeit-Wiedergabe umfangreicherer Multitrack-Sessions mit mehr gleichzeitigen Spuren und aktiven Effekten, als dies, abhängig von der Ausstattung des Computers, mit 32-Bit Systemen und der damit verbundenen 3-GB-Barriere für den RAM möglich wäre. Die interne Signalverarbeitung erfolgt jedoch nach wie vor in 32 Bit Gleitkommazahlen mit 192 dB Dynamik.\n\nAdobe Audition CC 2014 (Version 7.0), herausgebracht im Juni 2014, weist eine neu gestaltete Benutzeroberfläche auf und bietet den Import von weiteren Videodateiformaten sowie die Unterstützung von hochauflösenden Bildschirmen (HiDPI). Darüber hinaus werden nunmehr 5.1 Dolby Digital und 7.1 Dolby Digital Plus unterstützt und das Lesen von Soundbooth-Dateien ermöglicht. Mit Version 2014.2 (Build 7.2.0.52) aus Dezember 2014 wurde die Stabilität des Programms verbessert.\n\nAdobe Audition CC 2015 (Version 8.0), herausgebracht am 16. Juni 2015, bietet eine höhere Videoauflösung per \"Dynamic Link\" aus Premiere Pro übermittelter Videosequenzen, die Wiedergabe des Videos im Vollbildmodus auf einem zweiten Bildschirm, sowie eine verbesserte Unterstützung für USB-Mikrofone. Adobe Audition CC 2015 in der Version 8.1, herausgebracht im November 2015, bietet darüber hinaus eine synthetisierte Sprachausgabe sowie eine dateiübergreifende Lautstärkeanpassung mehrerer Audiodateien u. a. nach EBU R128, ITU BS1770 und ATSC-Standard.\n\n\nMultitrack-Sessions können wahlweise mit 16, 24 oder 32 Bit Amplituden-Auflösung und Samplerates bis 192 kHz angelegt werden, die interne Signalverarbeitung erfolgt jedoch immer als 32 Bit Floating Point Processing. Die Signalpegel werden von den Pegelmessern in dBFS angezeigt (Digitalpegel mit maximal 0 dB), die Fader hingegen haben eine Skala in dBu (Spannungspegel mit maximal 15 dB). Das Editorfenster ermöglicht in der Ansicht der Spektralanalyse die visuelle Darstellung des Frequenzspektrums im zeitlichen Verlauf und bietet Werkzeuge zur frequenzselektiven Retuschierung von Störgeräuschen. In der Ansicht der Spektraltonhöhenanzeige erfolgt die Darstellung des Frequenzspektrums im Bereich der musikalischen Tonskala.\n\n"}
{"id": "576636", "url": "https://de.wikipedia.org/wiki?curid=576636", "title": "RTAI", "text": "RTAI\n\nRTAI (Real Time Application Interface) ist eine Erweiterung von Linux zu einem Echtzeitbetriebssystem.\nEntworfen wurde RTAI von Paolo Mantegazza vom \"Dipartimento di Ingegneria Aerospaziale\" der Technischen Universität Mailand. RTAI wurde vom Beginn an als Open-Source-Projekt von einer größeren Entwicklergemeinde weiterentwickelt, wobei heute neben dem weiterhin koordinierend tätigen Mantegazza vor allem Philippe Gerum als sehr aktiver Mitarbeiter zu nennen ist.\n\nEs gibt inzwischen auch eine Reihe diverser verwandter oder kooperierender Projekte, wie zum Beispiel RTnet (ein Echtzeit-Netzwerk-Protokoll) und Linux Trace Toolkit.\n\nEin großer Pluspunkt von RTAI ist, dass es mit der Variante LXRT möglich ist, Hard-Realtime-Tasks im Userspace laufen zu lassen und damit die Schutzmechanismen von Linux zu nutzen. Dies erfolgt ohne größere Einbußen im Bereich der Latenzzeiten und ohne großen Overhead. Bei anderen Echtzeit-Systemen, welche ausschließlich im Kernelspace laufen, kann sich ein Fehler im Programmablauf verheerend auswirken.\n\nRTAI wird von einer großen Zahl von Entwicklern in vielen Ländern als Basis für ihre Entwicklungen im Realtime-Bereich verwendet, hat aber ebenso wie RTLinux naturgemäß für den Standard-Büro-Desktop-Computer-Anwender keine direkte Bedeutung.\n\nDie Grundlage von RTAI-Linux ist ein normaler Linux-Kernel, der mit dem RTAI-Patch (Realtime Application Interface) erweitert wird. Wie in der Abbildung zu sehen ist, fügt der Patch einen Echtzeit-Kernel zwischen der Hardware (Prozessor) und dem Linux-Kernel ein. Dieser übernimmt die Interruptverwaltung des Prozessors. Somit kann Software auf der Kernel-Ebene keine Interrupts mehr blockieren oder freigeben. Die dafür verwendeten Befehle cli() und sti() ersetzt RTAI durch Makros und ist somit in der Lage den Kernel-Code zu unterbrechen.\n\nDer Linux-Kernel selbst ist ebenfalls ein Echtzeit-Task. Er besitzt jedoch die kleinste Priorität (Idle-Task) und wird immer nur dann ausgeführt, wenn die Echtzeit-Tasks nichts zu tun haben. Nach dem Ausführen eines Echtzeit-Tasks werden alle Register wiederhergestellt, so dass der Kernel die Unterbrechung nicht bemerkt.\n\nDamit deterministische Interrupt-Latenzzeiten erzielt werden können, muss die Interruptverwaltung an RTAI übergeben werden. Die Umleitung der Interrupt-Kontrolle wird mit Hilfe des Realtime Hardware Abstraction Layers (RTHAL) realisiert. RTHAL wird mit dem RTAI-Patch in den Source-Code des Linux-Kernels integriert.\n\nIn der folgenden Abbildung sind die möglichen Kommunikationswege innerhalb eines modifizierten Kernels dargestellt. Im Fall A ist die Abstraktion transparent, das heißt die Interrupt-Kontrolle liegt nach wie vor beim Linux-Kernel, was der Nutzung eines Standard-Kernels entspricht. Bei B wird dem Linux-Kernel die direkte Kontrolle über die Interrupts entzogen und der Echtzeiterweiterung zugewiesen.\n\nRTAI arbeitet autonom von Linux auf der Hardware. Abgefangene Interrupts werden auch an RTHAL weitergegeben, damit der Kernel darauf entsprechend reagieren kann. RTAI wird durch verschiedene Kernel-Module implementiert. Solange diese Module nicht geladen sind, behält der Linux-Kernel die Interrupt-Kontrolle (Fall A). Erst beim Laden der RTAI-Module wird die direkte Interrupt-Kontrolle an RTAI übertragen (Fall B). So kann die Echtzeiterweiterung während der Laufzeit nach Belieben in den Kernel eingefügt und wieder entfernt werden. Dank dieser modularen Struktur lassen sich Fehlerquellen leichter isolieren. Arbeitet zum Beispiel ein RTAI-System fehlerhaft, kann man einfach die RTAI-Module entfernen, um zu testen, ob der Fehler bei Linux oder RTAI liegt.\n\nRTHAL besteht im Wesentlichen aus einer Struktur von Funktionspointern, welche beim Systemstart auf die Interrupt-Handling-Funktionen des Linux-Kernels zeigen. Beim Laden der RTAI-Module werden die Funktionspointer auf RTAI interne Funktionen umgelenkt. So übernimmt RTAI die Interrupt-Kontrolle, ohne dass der Linux-Kernel etwas davon bemerkt. Nach dem Entfernen der RTAI-Module zeigen die Pointer der Struktur rthal wieder auf die Standard-Kernel-Funktionen.\n\nWenn RTAI die Interrupt-Kontrolle übernimmt, werden interruptspezifische Funktionsaufrufe des Linux-Kernels mit Hilfe von RTHAL an RTAI interne Funktionen umgeleitet. So implementiert RTAI zum Beispiel einen Ersatz für das Funktionspaar codice_1 und codice_2. Diese RTAI-Funktionen setzen Flags in RTAI internen Datenstrukturen, um festzuhalten, ob Linux über eingehende Interrupts informiert werden möchte (codice_3) oder nicht (codice_4). So wird sichergestellt, dass der Kernel keine Interrupts mit Hilfe der Funktion codice_2 deaktivieren kann. RTAI gibt die mit der Funktion codice_1 angeforderten Interrupts nach dem Ausführen der Echtzeit-Interrupt-Handler an den Linux-Kernel weiter.\n\nIn der folgenden Abbildung wird mit Hilfe eines Flussdiagramms dargestellt, wie ein eingehender Interrupt von RTAI verarbeitet wird. Zuerst prüft der RTAI Dispatcher, ob eine Echtzeit-Applikation einen Handler für diesen Interrupt registriert hat. Falls entsprechende Interrupt-Handler vorhanden sind werden diese ausgeführt.\n\nDanach prüft RTAI anhand der internen Datenstrukturen, ob der Linux-Kernel den Interrupt ebenfalls mit sti() aktiviert hat. Bei einem positiven Prüfergebnis wird der Linux Dispatcher gestartet und somit die Verarbeitung des Interrupts auf der Kernel-Ebene eingeleitet. Falls der Linux-Kernel den betreffenden Interrupt nicht aktiviert hat, verlässt RTAI sofort den Interrupt-Kontext und führt das unterbrochene Programm wieder aus.\n\nRTAI unterstützt drei verschiedene Scheduling-Varianten. Diese sind entweder für den Einsatz auf Uni- oder auf Multiprozessor-Systemen spezialisiert. Alle Scheduler können sowohl im sogenannten Oneshot- oder Periodic-Mode betrieben werden. Die verschiedenen Scheduler werden in den Modulen codice_7, codice_8 und codice_9 implementiert. Das entsprechende Scheduler-Modul wird jeweils nach dem RTAI-Modul rtai_hal.ko mit insmod in den Kernel eingefügt.\n\nDieser Scheduler ist für Plattformen mit einem Prozessor vorgesehen, welche den 8254 als Timer benutzen. Der Aufbau des Schedulers ist recht einfach. Er besteht im Wesentlichen aus mehreren Listen mit verschiedenen Prioritäten, welche er linear abarbeitet. Dabei erhält jeweils der Task mit der höchsten Priorität Zugriff auf die CPU. Der Linux-Kernel selbst ist ebenfalls ein Echtzeit-Task, allerdings mit der geringsten Priorität.\n\nDer SMP-Scheduler (Symmetric Multiprocessing) ist für Multiprozessor-Systeme gedacht, die entweder 8254 oder APIC basiert sind. Der APIC ist der sogenannte Advanced Programmable Interrupt Controller in Multiprozessor-Systemen. Dieser hat unter anderem die Aufgabe, die auftretenden Interrupts den einzelnen CPUs zuzuteilen. Tasks können an eine CPU gebunden werden oder symmetrisch auf einem Cluster von CPUs laufen. Der Scheduler kann auch auf Systemen eingesetzt werden, die nur einen Prozessor haben, aber deren Kernel mit SMP-Option kompiliert wurde.\n\nWie es der Name schon sagt, sieht dieser Scheduler ein Multiprozessor-System als eine Ansammlung von mehreren Einzelprozessoren. Dies hat den Vorteil, dass im Gegensatz zum SMP-Scheduler jeder Prozessor seine Timer unabhängig von den anderen programmieren kann. Also können die Timer-Modi Periodic- und Oneshot-Mode abhängig von der CPU verschieden sein.\n\nDie Ausführung von Echtzeit-Tasks in RTAI ist timergesteuert. RTAI bietet die Wahl zwischen den beiden Timer-Modi Periodic- und Oneshot-Mode. Periodisch bedeutet, dass der Timer in regelmäßigen Intervallen ein Interrupt auslöst, der ein Rescheduling veranlasst. Im Gegensatz dazu steht das Oneshot-Verfahren. Hierbei wird der Timer so programmiert, dass er nach einer festgelegten Zeitspanne genau einen Interrupt auslöst, der den Scheduler aufruft. Für die Generierung eines weiteren Interrupts muss der Timer neu programmiert werden, was einen größeren Aufwand bedeutet, als beim periodischen Verfahren. Jedoch sind so auch unterschiedlich lange Intervalle möglich, nach denen ein Rescheduling erfolgen kann.\n\nBei der Initialisierung des Programms muss ein Modus gewählt werden. Dies geschieht indem eine der beiden folgenden Funktionen aufruft:\n\n\nFür die Kommunikation und Synchronisation zwischen Echtzeit-Tasks im Kernel-Space stellt RTAI die für ein Echtzeitbetriebssystem üblichen Mechanismen zur Verfügung. Diese werden in den Kernel-Modulen der Scheduler implementiert:\n\n\nMit Hilfe von Mailboxen ist eine asynchrone Inter-Prozess-Kommunikation möglich. Ein Task kann Nachrichten asynchron an die Mailbox eines anderen Tasks senden. Wenn der Empfänger bereit ist die empfangenen Nachrichten zu bearbeiten kann er sie aus der Mailbox holen. In diesem Fall arbeitet die Mailbox wie eine FIFO (first in first out), deren Funktionalität vollständig vom jeweiligen Task entkoppelt ist und keine Synchronisationsmechanismen benötigt.\n\nHier die wichtigsten RTAI-Funktionen zum Arbeiten mit Mailboxen:\n\n\nEin Semaphor ist eine Art Schlüssel, den ein Task zum Beispiel benötigt, um auf eine gemeinsame Ressource zuzugreifen. Wurde der Semaphor bereits von einem anderen Task geholt, wird der anfragende Task in den Wartezustand gesetzt, bis der aktuelle Besitzer den Semaphor wieder zurückgibt. Ein Semaphor beinhaltet eine geschützte Variable (binär oder counting), welche die noch freien Zugriffe auf eine Ressource angibt. In einer Queue werden die Tasks vermerkt, die auf den Semaphor warten. Wird der Semaphor zurückgegeben erhält ihn der erste Task in der Queue.\n\nFolgende Funktionen stehen zum Arbeiten mit Semaphoren in RTAI zur Verfügung:\n\n\nRTAI stellt mit FIFOs und Shared Memory auch zwei Mechanismen zur Verfügung, die es den Echtzeit-Tasks ermöglicht mit normalen Linux-Prozessen im User-Space zu kommunizieren.\n\nEin FIFO ist ein Puffer-Speicher, über den Daten zwischen einem RTAI-Task und einem normalen Linux-Prozess im User-Space ausgetauscht werden können. Theoretisch ist ein FIFO bidirektional. In der Praxis wird jedoch meistens nur eine Richtung benutzt. Zum gegenseitigen Austausch von Daten verwendet man zwei FIFOs, einen zum Senden von Befehlen und einen weiteren zum Empfangen der entsprechenden Antworten.\n\nLinux-Prozesse können auf einen FIFO wie auf eine normale Datei zugreifen. Anstelle einer Datei öffnet man mit der Funktion codice_20 einen speziellen Device-Node im codice_21-Verzeichnis (codice_22 bis codice_23). Anschließend kann man mit den Funktionen codice_24 und codice_25 Daten lesen und schreiben. Im Kernel-Space stellt die RTAI-API folgende Funktionen zum Arbeiten mit FIFOs für die Echtzeit-Tasks zur Verfügung:\n\n\nShared Memory ist wie es der Name schon sagt, ein Speicherbereich, den sich Linux-Prozess und RTAI-Task teilen. Shared Memory wird hauptsächlich dann eingesetzt, wenn mehrere Linux-Prozesse Zugriff auf die Daten eines RTAI-Task benötigen oder eine große Datenmenge in kurzer Zeit von einem RTAI-Task an einen Linux-Prozess übertragen werden müssen.\n\nUm die Entwicklung von Echtzeit-Tasks zu erleichtern, wurde in RTAI das LXRT-Modul eingeführt. Dieses Modul erlaubt die Entwicklung von Echtzeit-Tasks im User-Space, mit der Möglichkeit, auf die API von RTAI zuzugreifen.\nDies ist eine Besonderheit, die nur in RTAI existiert und die Entwicklung sehr vereinfachen kann, da sich Fehler in einem User-Space Prozess in der Regel nicht auf die Stabilität des Gesamtsystems auswirken. Fehler in Kernel-Modulen können oft zum Absturz des gesamten Systems führen. Zudem kann man im User-Space im Gegensatz zum Kernel-Space mit einem normalen Debugger (zum Beispiel GDB) arbeiten.\nLXRT ist nur als Testwerkzeug gedacht und kann keine Echtzeiteigenschaften garantieren. Deshalb portiert man LXRT-Programme nach der Testphase in normale Kernel-Module. Da das LXRT-Modul die RTAI-API im User-Space zu Verfügung stellt, kann der Code des LXRT-Programms ohne große Änderungen für das Kernel-Modul übernommen werden.\n\nDas RTAI-Lab Projekt erweitert Simulink und Scicos um einen Blocksatz, mit dessen Hilfe sich Echtzeitanwendungen für RTAI grafisch zusammenklicken lassen.\nEr enthält Blöcke zur Behandlung der oben beschriebenen Techniken wie Intertask-Kommunikation, oder Kommunikation mit Linux-Prozessen.\nAußerdem werden analoge und digitale IO-Blöcke bereitgestellt, mit deren Hilfe die Echtzeitanwendung mit der Außenwelt interagieren kann. Dazu greift RTAI-Lab auf die Treiber des Comedi Projekts zurück, unterstützt werden also alle IO-Karten, die von Comedi unterstützt werden.\nAus dem so zusammengestellten Model wird automatisch C-Code generiert, der direkt kompiliert und als Echtzeittask gestartet werden kann.\nZur Benutzerinteraktion mit diesem Task wird eine grafische Benutzeroberfläche mitgeliefert, über die sich Variablen plotten, aber auch verändern lassen.\n\n\n\n\n"}
{"id": "578599", "url": "https://de.wikipedia.org/wiki?curid=578599", "title": "Floodfill", "text": "Floodfill\n\nFloodfill bzw. Flutfüllung ist ein Begriff aus der Computergrafik. Es ist ein einfacher Algorithmus, um Flächen zusammenhängender Pixel einer Farbe in einem digitalen Bild zu erfassen und mit einer neuen Farbe zu füllen.\n\nAusgehend von einem Pixel innerhalb der Fläche werden jeweils dessen Nachbarpixel darauf getestet, ob diese Nachbarpixel auch die alte Farbe enthalten. Jedes gefundene Pixel mit der alten Farbe wird dabei sofort durch die neue Farbe ersetzt.\n\nZwei Varianten des Algorithmus sind gängig:\n\nEs werden jeweils die vier benachbarten Pixel unten, links, oben und rechts vom Ausgangspunkt untersucht (Koordinatenursprung ist links-oben im Eck).\n\nfill4(x, y, alteFarbe, neueFarbe) {\n\nEs werden jeweils die acht benachbarten Pixel oben, unten, links, rechts, oben-links, oben-rechts, unten-links und unten-rechts vom Ausgangspunkt untersucht (Koordinatenursprung ist links-oben im Eck).\nfill8(x, y, alteFarbe, neueFarbe) {\n\nBei gleichen Ausgangsbedingungen füllt die \"8-connected\"-Variante üblicherweise einen größeren Bereich als die \"4-connected\"-Variante, da sie „durch feine Ritzen kriecht“. Dies ist häufig nicht gewünscht.\n\nAufgrund der Einfachheit des Algorithmus eignet er sich nur bedingt für nicht-triviale Anwendungen. Der Algorithmus ist hoch-rekursiv. Daher besteht ein hohes Risiko, dass der Algorithmus zu einem Stack-Überlauf führt. Ebenso benötigen die möglichen vielen Stack-Operationen durch die Rekursionen im Vergleich zu den eigentlichen Operationen des Algorithmus relativ viel Zeit.\n\nNicht zuletzt sind viele Rekursionsaufrufe des Algorithmus unnötig, da dabei unnötigerweise Pixel getestet werden, die kurz zuvor bereits auf die neue Farbe gesetzt wurden. Jede Rekursion trifft mindestens auf ein solches Pixel, jenes Pixel, welches gerade im darüberliegenden Rekursionslevel markiert wurde.\n\nMit Hilfe eines Stapelspeichers oder einer ähnlichen Datenstruktur ist auch eine iterative Implementierung möglich. Dabei werden die Nachbarpixel-Koordinaten gespeichert und dann nacheinander abgearbeitet. Die Reihenfolge, in der die Koordinaten aus der Datenstruktur ausgelesen werden, spielt dabei keine Rolle. Durch das hohe Risiko eines Stack-Überlaufs bei der rekursiven Flutfüllung ist die iterative Version in der Regel die bessere Wahl zur Implementierung des Verfahrens.\nfill4(x, y, alteFarbe, neueFarbe) {\n\nDieses Beispiel entspricht dem rekursiven Algorithmus mit vier Nachbarn. Derjenige mit acht Nachbarn wird entsprechend analog implementiert.\n\nBei teiliterativer Flutfüllung besteht der Algorithmus aus einem iterativen Teil, der immer weiterläuft und sich immer in eine bestimmte Richtung wendet, zum Beispiel immer linksherum. Wenn der iterative Teil keine Pixel mehr zum Einfärben findet, wird der rekursive Teil gestartet, der nun deutlich weniger tief in die Rekursion geht. Somit ist ein Stack-Überlauf weniger wahrscheinlich.\n"}
{"id": "581675", "url": "https://de.wikipedia.org/wiki?curid=581675", "title": "Texas Instruments Graphics Architecture", "text": "Texas Instruments Graphics Architecture\n\nDie Texas Instruments Graphics Architecture (TIGA) ist ein Standard für Grafikkarten, der seit Mitte der 1980er Jahre von Texas Instruments entwickelt wurde.\n\nTIGA-Grafikkarten bauten auf einem universellen digitalen Signalprozessor auf. Bekannte Prozessoren dieses Standards waren der TMS34010 und der TMS34020. Ursprünglich nur in speziellen CAD-Systemen eingesetzt, kamen Anfang der 1990er Jahre auch Produkte für IBM-kompatible Personal Computer auf den Markt.\n\nDiese Karten waren Zusatzkarten, die überwiegend im CAD-Bereich eingesetzt wurden. Da sie nicht vom BIOS IBM-kompatibler Rechner unterstützt wurden, war stets noch eine normale Grafikkarte zum Betrieb erforderlich. Einige Ausführungen integrierten daher einen zusätzlichen VGA-Controller (Frame buffer) auf dem Grafikboard. Für kostengünstige CAD-Zweischirmlösungen wurden TIGA-Karten häufig auch mit einer preisgünstigen Hercules-Grafikkarte und einem entsprechenden Schwarzweiß-Monitor kombiniert.\n\nDer High-End-Standard konnte sich aufgrund der hohen Preise auf dem Massenmarkt nicht durchsetzen. Die für CAD-Anwendungen unter DOS und für Microsoft Windows konzipierten Karten ermöglichten eine Bildauflösung von 1280×1024 Punkten bei einer Farbtiefe von 24 Bit.\n\nBeispieldaten einer TIGA-Grafikkarte:\n\nHercules Superstation XP\n\n"}
{"id": "581717", "url": "https://de.wikipedia.org/wiki?curid=581717", "title": "Xen", "text": "Xen\n\nXen (auch Xen Project, in Abgrenzung zu darauf basierenden kommerziellen Produkten) ist ein Hypervisor, also eine Software, die den Betrieb mehrerer virtueller Maschinen auf einem physischen Computer erlaubt. Sie entstand an der britischen Universität Cambridge und wird heute von dem US-Unternehmen Citrix Systems weiterentwickelt. Der Name leitet sich von der Vorsilbe xeno (, \"xénos\": Fremder) ab.\n\nXen ist ein Typ-1-Hypervisor (auch \"Virtual Machine Monitor\" (VMM) genannt), der direkt auf einer Hardware läuft. Xen kann mehrere Betriebssysteme in virtuellen Maschinen starten, den sogenannten \"Domänen\". Für diese Betriebssysteme sind weder der Hypervisor noch andere Domänen „sichtbar“. Vom Prinzip her ist das Vorgehen vergleichbar mit virtuellem Speicher und Prozessen: Durch den virtuellen Speicher kann jeder Prozess (hier: das virtuelle System) den Speicher so nutzen, als wäre er der einzige Prozess, der vom Betriebssystem ausgeführt wird. Genauer gesagt: Der Hypervisor weist dem virtuellen System Teilbereiche des gesamten Hauptspeichers zu. Diese erscheinen dem virtuellen System als zusammenhängender Adressraum, so wie der physische Speicher einem nicht virtuellen System erscheint. Er kann vom virtuellen System entsprechend und exklusiv genutzt werden.\n\nEine besondere Bedeutung hat die erste Domäne, die von Xen gestartet wird: Diese Domäne ist privilegiert und dient der Interaktion mit dem eigentlichen Hypervisor. Die privilegierte Domäne, \"Dom0\" genannt, kann andere Domänen starten, stoppen und verwalten. Dazu muss diese Verwaltungsfunktionalität in das Betriebssystem integriert werden, das in der Dom0 läuft.\n\nUm völlig transparent für die unprivilegierten Domänen, häufig \"DomU\" genannt, zu sein, benötigt Xen einen Hauptprozessor mit der Befehlssatzerweiterung Secure Virtual Machine, wie beispielsweise Intel VT oder AMD-V. Mit dieser Hardware müssen die Betriebssysteme, die in den Domänen laufen, nicht angepasst werden – sie „bemerken“ nicht, dass sie die Hardware in Wirklichkeit mit anderen Systemen teilen. Diese Betriebsart wird als \"volle Virtualisierung\" oder \"Hardware Virtual Machine\" (HVM) bezeichnet. Wird auf anderer Hardware virtualisiert, haben die jeweiligen Kernels vollen Hardwarezugriff und könnten aufgrund fehlerhaften oder böswilligen Codes auf jeweils fremde Ressourcen (z. B. Hauptspeicher) zugreifen, was aus Gründen der Stabilität und Sicherheit des Ganzen nicht erwünscht ist.\n\nDie Effizienz virtualisierter Systeme kann gesteigert werden, indem Unterstützung für den Betrieb als DomU in das Betriebssystem integriert wird. Dieser Ansatz wird als Paravirtualisierung bezeichnet und erfordert eine Modifikation des Systems, das in einer DomU laufen soll.\n\nDer Linux-Kernel stellt ab Version 2.6.21 die Rahmenbedingungen für den Betrieb unterhalb eines beliebigen Hypervisors in Form von sogenannten \"paravirt ops\" bereit. Ab Version 2.6.23 ist eine eingeschränkte Unterstützung für den Betrieb unter Xen integriert. Dieser Basis-Support unterstützte jedoch zahlreiche Möglichkeiten von Xen nicht, beispielsweise das (dynamische) Durchreichen von PCI-Geräten oder dynamische Speichervergrößerungen. Ab Version 3.0 des Kernels unterstützt dieser Xen vollständig.\n\nIn den Linux-Distributionen openSUSE ist Xen ab Version 9.3 und in Fedora ab Version 4 integriert. Ebenfalls enthalten ist Xen im Novell/SUSE Linux Enterprise Server (SLES) ab Version 10 und im Red Hat Enterprise Linux 5 (RHEL5). Debian enthält ab Version 4 (\"Etch\") einen Xen-Kernel und Ubuntu ab Version 12.04 (\"Precise Pangolin\"). Gentoo Linux und Arch Linux bieten beide Pakete für vollständige Xen-Unterstützung.\n\nBei den Versionen bis Xen 3.x ist für den Betrieb als Dom0 oder als vollwertige native DomU nur der „offizielle“ Linux-Kernel-Quellcode von Xen verwendbar, der lediglich in der Version 2.6.18.8 vorliegt. Durch die aktive Entwicklung am Linux-Kernel lässt sich der Patch für diese Version nicht ohne erheblichen Aufwand auf einen aktuellen Kernel anwenden. So wurde die aufwändige Praxis, die Xen-Patches an aktuellere Versionen anzupassen, beispielsweise von Debian vor kurzem eingestellt.\n\nSeit Xen 4.0 unterstützt Xen per default auch die Standard-Kernel-Option PVOps für seine dom0-Kernel in Kernel-Version 2.6.31.x. Weiter ist eine Long-Term-Support-Version (LTS) unter pvops und dem Linux-Kernel 2.6.32.x für dom0-Kernel verfügbar. Trotzdem bleibt die Kompatibilität zum bisherigen 2.6.18-Kernel auch weiterhin erhalten und es sind weitere Xen-Patches für diese Version sowie auch für einige Kernel mit Forward-Patches (wie dem Kernel von RHEL 5.x) geplant.\n\nNetBSD 2.0 unterstützte Xen 1.2 als Host und Gast sowie Xen 2.0 nur als Gast, die Version NetBSD 3.1 unterstützt Xen 2.0 komplett, also als Host und Gast, sowie Xen 3.0 als Gast. Seit NetBSD 4.0 wird Xen 3.0 vollständig unterstützt.\n\nSun hat Xen Anfang Oktober 2007 mit Nevada build 75 unter dem Namen \"xVM\" vollständig in OpenSolaris integriert, davor existierten Testversionen.\n\nAuf der \"Novell BrainShare Conference\" stellte Novell 2005 eine Portierung von Netware auf Xen vor.\n\nAn einer Portierung von ReactOS auf Xen wird seit 2005 gearbeitet.\n\nZu den Unterstützern von Xen zählen weltweit agierende IT-Konzerne – selbst direkt stark untereinander konkurrierende Unternehmen vereinen sich unter diesem Dach, unter anderem: Microsoft, Oracle, Intel, AMD, IBM, HP, Red Hat und SUSE.\n\nDie Open-Source-Software Xen entstand ursprünglich an der Universität Cambridge. Die Entwickler haben mit XenSource ein Unternehmen gegründet, das Xen zum Industriestandard machen soll. Die Firma XenSource wurde im August 2007 für 500 Millionen US-Dollar durch die Firma Citrix Systems übernommen.\n\nAnfang 2013 wurde Xapi dann zusammen mit Xen.org wieder zurück an das Xen-Project übertragen, das unter dem Dach der Linux-Foundation arbeitet. Xapi ist heute demnach ein Unterprojekt des Xen-Projects.\n\nAuf Xen basieren unter anderem die Citrix Produkte XenServer und XenClient.\n\n\n\n\n\n"}
{"id": "583598", "url": "https://de.wikipedia.org/wiki?curid=583598", "title": "Canon Cat", "text": "Canon Cat\n\nDie Canon Cat ist ein 1987 von Jef Raskin bei Canon entwickeltes, elektronisches Schreibsystem.\n\nEs unterscheidet sich von anderen Computern der 1980er Jahre vor allem durch sein neuartiges Benutzerinterface. Statt auf ein Bedienkonzept auf Basis von Maus, Icons und Kommandozeile zurückzugreifen, wurde die Tastatur um zusätzliche Spezialtasten erweitert.\n\nNach sechs Monaten und 20.000 verkauften Einheiten stellte man den Vertrieb ein. Trotz seines innovativen Konzeptes konnte sich das Schreibsystem nicht gegen die aufkommenden Textverarbeitungsprogramme für den PC durchsetzen.\n\n\n"}
{"id": "586269", "url": "https://de.wikipedia.org/wiki?curid=586269", "title": "IPCop", "text": "IPCop\n\nIPCop war eine freie Linux-Distribution, die in erster Linie als Router und Firewall gedacht war. Darüber hinaus bietet sie noch ausgewählte Server-Dienste an und kann um zusätzliche Funktionen erweitert werden. IPCop basiert bis zur Version 1.3.0 auf der freien GPL-Version von \"Smoothwall\", ab der Version 1.4.0 auf Linux From Scratch (kurz LFS). 2017 wurde das Ende der Entwicklung angekündigt. Die letzte Version erschien 2015. Das Projekt ist Ende 2018 eingestellt worden.<br>\nMit IPFire und Endian Firewall gibt es zwei Abspaltungen von IPCop, die weiterentwickelt werden.\n\nDer IPCop stellt direkt nach der Installation einen Router, eine funktionierende Firewall, einen Proxyserver (Squid), einen DHCP-Server, einen Caching-Nameserver (\"dnsmasq\") sowie ein Intrusion Detection System (Snort) bereit. Weitere Funktionen wie Traffic-Shaping, VPN und Dynamic DNS sind vorhanden.\n\nDie benötigte Rechenleistung des PCs richtet sich nach dem Einsatzbereich. Erforderlich sind 133 MHz mit 32 MByte RAM (besser 64 MByte).\nEs werden mindestens 2 Netzwerkkarten benötigt (PCI, PCMCIA, USB, ISA oder VL-Bus), eine für den Anschluss ans Internet (über DSL oder einen anderen Router), eine zum Anschluss ans LAN.\n\nDie Rechenleistung bei privatem Gebrauch kann bereits ein 486er übernehmen, wenn man Squid und das Intrusion Detection System (IDS) abschaltet.\n\nIPCop unterscheidet zwischen unterschiedlichen Netzwerken, die verschiedenfarbig dargestellt werden. Das grüne Netzwerk stellt das eigene LAN dar, das rote Netzwerk symbolisiert das „ungeschützte“ Internet. Ein eventuell vorhandenes WLAN wird durch die Farbe Blau symbolisiert, während orange die \"DMZ\" (Demilitarized Zone) darstellt. Diese wird für Server verwendet, die aus dem Internet erreichbar sein sollen (Webserver, FTP-Server, etc.). Würde nun dieses Netzwerk erfolgreich angegriffen (kompromittiert), sind die anderen Netzwerke davon unabhängig geschützt.\n\nFür jedes Netzwerk, das verwendet wird, wird eine eigene Netzwerkkarte mit IP-Adresse benötigt. Es ist nicht erforderlich, jedes Netzwerk zu verwenden. Ist kein WLAN vorhanden, existiert einfach kein blaues Netzwerk. Ist kein Webserver (o. ä.) vorhanden, wird keine DMZ, also kein oranges Netzwerk benötigt.\nDie Mindestausstattung mit einem roten und grünen Netzwerk kann durch Add-ons auf bis zu vier weitere Netzwerkkarten und damit Netze – unabhängig von blau und orange – erweitert werden. Jedes dieser Netze ist separat und durch die Firewall geschützt.\n\nKonfiguriert wird der IPCop über eine Webschnittstelle, zu erreichen über (vor Version 2.0.0) \"<nowiki>http://SERVERNAME:81/</nowiki>\" oder über SSL auf \"<nowiki>https://SERVERNAME:445/</nowiki>\" (Standardports - können bzw. sollen aus Sicherheitsgründen geändert werden, da 445 durch viele Provider mittlerweile blockiert wird), alternativ zum Servernamen auch über dessen IP-Adresse. Ab Version 2.0.0 ist der sichere Zugriff nicht mehr über Port 445, sondern (standardmäßig) nur noch über Port 8443 möglich.\n\nÜber dieses Web-Interface können dann Einstellungen wie Port-Weiterleitung, öffnen von Ports (externer Zugang), Proxy- und DHCP-Server, aber auch dynamisches DNS, Traffic-Shaping, IDS und Zeitserver (NTP) konfiguriert werden.\nDes Weiteren erhält man über die Webschnittstelle Zugriff auf die verschiedenen Log-Dateien und deren Auswertungen, die z. T. auch als Grafiken bereitgestellt werden.\n\nAuf die Unix-Shell kann der Benutzer auch zugreifen, um tiefergehende Konfigurationen zu erstellen oder zu ändern. Der Zugriff erfolgt hierbei dann per SSH auf dem Port 8022. Sehr verbreitet und auch ohne Linux-Kenntnisse leicht nutzbar sind WinSCP und PuTTY.\n\nDie Möglichkeiten des IPCop lassen sich über Add-Ons erweitern, wie z. B. mit einem URL-Filter, dem Open-VPN ZERINA oder einem Layer-7-Filter. Die Erweiterungen werden auf der offiziellen Website von IPCop veröffentlicht.\n\nIPCop liefert mit der Basisinstallation viele Dienste und ist darüber hinaus mit Add-ons anpassbar. Doch hier wird ein Kompromiss zwischen Leistungs- bzw. Funktionsumfang und Sicherheit gemacht, da unter steigender Komplexität auch die Sicherheit leiden kann. Bereits mit der Grundinstallation wird ein für die Firewall-Funktionen nicht notwendiger Webserver sowie ein NTP-Server installiert, diese können für Angriffe ausgenutzt werden. Auch diverse Add-ons wie etwa Samba können zusätzliche Angriffsflächen schaffen.\n\nDie Zeitschrift c’t hatte 2005 im Rahmen eines Server-Projekts den \"c’t-Debian-Server\" vorgestellt, in dem IPCop in User Mode Linux (UML), einer virtuellen Maschine unter einem umfangreich ausgestatteten Linux-Home-Server-System mit verschiedenen Netzwerkdiensten läuft. Dieses Vorgehen wird jedoch von vielen Fachleuten als unsicher eingestuft, da ein Angreifer die Kontrolle über den virtuellen Host übernehmen könnte. In der aktuellen Version des Beispielservers wurden diese Risiken durch den Einsatz von Xen und zwei darauf basierenden virtuellen Servern verringert.\n\nIn der neuesten Version fehlt die Unterstützung für IPv6.\n\nLCD4Linux ist eine Erweiterung, die es ermöglicht, Informationen auf einem LC-Display, welches über die serielle Schnittstelle angeschlossen ist, anzeigen zu lassen.\n\n"}
{"id": "586277", "url": "https://de.wikipedia.org/wiki?curid=586277", "title": "BBC Micro", "text": "BBC Micro\n\nDer BBC Micro (Acorn BBC Microcomputer) ist ein auf dem 6502-Prozessor (2 MHz) basierender Heimcomputer der britischen Firma Acorn.\n\nSeine Verbreitung fand er ab 1981 vor allem in britischen Schulen, weil eine Sendung des BBC den Acorn als Lehrmodell nutzte. Daher auch der häufig genutzte Name „BBC Micro“ für die Modelle Acorn A (16 KB RAM), B (32 KB RAM) und B+ (64 KB RAM).\n\nEinzigartig für die damalige Zeit war der offene Systembus des Acorn – der „Tube“. Darüber lassen sich an den sonst eigenständig arbeitenden Rechner zusätzliche CPU-Module anschließen. Möglich waren hier Prozessoren der Bauart 6502, Z80, und ARM1, die den Rechner zur Konsole degradierten, aber dadurch auch die Nutzung anderer Betriebssysteme (CP/M, Unix) erlaubten.\n\nIntern besaß der Acorn in seinem ROM mit BBC BASIC bereits ein leistungsfähiges BASIC. Er bot eine Grafikauflösung bis zu 640×256, 4-Kanal-Tonausgabe, Digital-Analog-Wandler und diverse externe Schnittstellen (Parallel, Seriell, Monitor, TV, User-Port).\n\nWegen des relativ hohen Preises konnte sich der Micro international nie durchsetzen, daher erschien später mit dem Acorn Electron auch eine preisgünstigere Variante des Micro. Der Nachfolger des Micro war der BBC Master.\n\nIn den frühen 1980er-Jahren startete der britische Fernsehsender BBC das Projekt \"BBC Computer Literacy Project\". Das Projekt entstand hauptsächlich als Reaktion auf die damals sehr einflussreiche Fernseh-Dokumentation \"The Mighty Micro\", in der Dr. Christopher Evans vom National Physical Laboratory die bevorstehende Revolution des Microcomputer beschreibt und die sich daraus ergebenden Effekte auf die Wirtschaft, Industrie und Gesellschaft in England.\n\nDie BBC brauchte für das Projekt einen Microcomputer, der in der Lage war, bestimmte Aufgaben zu erfüllen, die man den Zuschauern in der TV Serie \"The Computer Programme\" im Jahre 1982 beibringen wollte. Die Liste der Anforderungen enthielt Programmierung des Rechners, Computergrafik, Töne und Musik, Teletext, Steuerung von externer Hardware und Künstliche Intelligenz. Hierfür wurde eine Spezifikation geschrieben und mit mehreren Computer-Herstellern besprochen. Dazu gehörten Sinclair Research, Newbury Laboratories, Tangerine Computer Systems, Dragon Data und Acorn Computers.\n\nDas Acorn-Team arbeitete zu dieser Zeit schon an dem Nachfolger des \"Acorn Atom\" Computers. Jedoch war das Gerät damals noch in einer frühen Entwicklungsphase, und das Team (dabei waren auch Steve Furber und Sophie Wilson) hatte lediglich vier Tage, um einen funktionierenden Prototyp nach den Spezifikationen der BBC daraus zu machen. Am Ende war Acorn mit seinem Rechner erfolgreich: Nicht nur war der Acorn Computer der einzige, der den Spezifikationen der BBC entsprach, sondern er übertraf die Anforderungen in fast jedem Bereich.\n\n\nÜber die Entstehung des BBC Micro wurde 2009 ein Film gedreht: „Micro Men“, der den Wettstreit zwischen den Firmen Acorn und Sinclair Research zur Produktion des Rechners für die BBC beschreibt. Die Benennung des von der BBC im Jahr 2015 vorgestellten Kleinstcomputers nimmt Bezug auf den BBC Micro.\n\n"}
{"id": "586979", "url": "https://de.wikipedia.org/wiki?curid=586979", "title": "Photon Mapping", "text": "Photon Mapping\n\nPhoton Mapping ist ein von Henrik Wann Jensen 1995 veröffentlichter Algorithmus der Bildsynthese, der vornehmlich als Erweiterung von Raytracing-basierten Verfahren genutzt wird. Das Ziel von Photon Mapping ist es, die globale Beleuchtung einer Szene effizient zu ermitteln und somit realistische Bilder bei geringerem Zeitaufwand zu erzeugen. Photon Mapping zählt zu den Particle-Tracing-Methoden.\n\nBei Photon Mapping werden vor dem eigentlichen Rendervorgang so genannte „Photonen“, die eine bestimmte Leistung (Strahlungsfluss) transportieren, von den Lichtquellen in die Szene „geschossen“. Geometrisch entspricht dies dem Raytracing von der Lichtquelle ausgehend. Ein Photon wird dabei maßgeblich durch seine Position, die Richtung, aus der es eingestrahlt wird, und durch seinen Strahlungsfluss abstrahiert. Trifft ein solches von der Lichtquelle ausgehendes Photon eine Oberfläche, kann es reflektiert, gestreut, gebrochen, absorbiert oder in der \"Photon Map\" gespeichert werden, wenn es auf diffus reflektierende Oberflächen trifft. Dabei wird eventuell die Leistung des Photons verändert. Die Photon Map (selten: „Photonenkarte“) ist eine von der Szenengeometrie unabhängige Datenstruktur (meist ein dreidimensionaler \"k\"d-Baum). Die in ihr gespeicherten Photonen können eine Abschätzung über den an der Position des Photons \"eingehenden\" Strahlungsfluss abgeben. Den gesamten Vorgang nennt man \"Photon Tracing.\"\n\nDer verwendete Begriff „Photon“ stimmt nur insoweit mit dem physikalischen Begriff überein, als Strahlungsfluss transportiert wird. Es handelt sich beim Photon Mapping nicht um eine quantenphysikalisch korrekte Simulation der Lichtausbreitung. Stattdessen abstrahiert ein „Photon“ beim Photon Mapping eine große Anzahl physikalischer Photonen, deren Energie pro Zeiteinheit zur Leistung des „Photons“ aufsummiert wird. Quantenphysikalische Vorgänge werden dabei nicht berücksichtigt, da zur Erzeugung realistisch aussehender Bilder die geometrische Optik meist ausreicht.\n\nDie so erstellte Photon Map kann auf verschiedene Art genutzt werden. Gebräuchlich ist das Verfahren, bei dem beim Rendering an jedem Punkt, an dem ein Strahl während des Raytracings auf eine Oberfläche trifft, die indirekte Beleuchtung durch die Dichte und Leistung der Photonen in der Photon Map in der Nähe des jeweiligen Punktes ermittelt wird. Durch Addition der so ermittelten indirekten Beleuchtung und der durch diffuses Raytracing ermittelten direkten Beleuchtung lässt sich die globale Beleuchtung einer Szene ermitteln. Daneben lässt sich die Photon Map zur Beschleunigung von Path Tracing verwenden. Zur effizienten Simulation von Kaustiken werden diese in einer unabhängigen \"Caustic Map\" gespeichert.\n\nDer Unterschied zu anderen von der Lichtquelle aus arbeitenden Verfahren wie Light Ray Tracing oder früheren Particle-Tracing-Methoden besteht darin, dass die indirekte Beleuchtung nur einmal in einem Arbeitsgang ermittelt und von der Geometrie unabhängig gespeichert wird.\n\nFür Photon Mapping gibt es weitere Anwendungen und Erweiterungen wie Importon Mapping und Volumenstreuung.\n\nPhoton Mapping wird heute von den meisten großen Renderern unterstützt. Der verbreitetste Renderer ist Mental Ray, der in vielen großen 3D-Softwarepaketen standardmäßig integriert ist. Auch andere Renderer wie POV-Ray, YafaRay, V-Ray, Maxwell Render und LuxRender unterstützen Photon Mapping.\n\n\n\n"}
{"id": "588749", "url": "https://de.wikipedia.org/wiki?curid=588749", "title": "Darik’s Boot and Nuke", "text": "Darik’s Boot and Nuke\n\nDarik’s Boot and Nuke (meist abgekürzt mit dem Akronym „DBAN“) ist ein freies, Linux-basiertes Live-System, dessen Aufgabe einzig das unwiderrufliche Löschen von Festplatten oder anderen beschreibbaren Datenspeichern ist. Im Ergebnis können die Dateien selbst mit forensischen Spezialwerkzeugen nicht wiederhergestellt werden. Die Distribution ist auf das Nötigste beschränkt und weist eine vergleichsweise sehr geringe Größe auf. Dank der Tatsache, dass DBAN von Wechselmedien aus gestartet wird, können sämtliche verfügbaren Datenträger (solange sie beschreibbar sind und deren Controllertyp unterstützt wird) gelöscht werden. \n\nDBAN unterstützt die Controllertypen IDE, SATA, SATA2, XT, SCSI und PATA, zur Formatierung werden Algorithmen wie der Standard DoD 5220.22-M, die Gutmann-Methode oder der kanadische Standard RCMP TSSIT OPS-II angeboten.\n\n\nDer Sinn des mehrfachen Überschreibens von Festplatten – wie es auch DBAN anbietet – wird hingegen angezweifelt. Eine Studie aus dem Jahr 2008 kam zu dem Ergebnis, dass bereits nach einmaligem Überschreiben die Daten so gut wie gar nicht mehr hergestellt werden können, selbst dann nicht, wenn sie nur mit Nullen statt mit zufällig generierten Daten überschrieben wurden.\n\n"}
{"id": "588826", "url": "https://de.wikipedia.org/wiki?curid=588826", "title": "Rhapsody (Betriebssystem)", "text": "Rhapsody (Betriebssystem)\n\nRhapsody [] war der Projekt- bzw. Codename von Apples damals neuer Betriebssystem­generation während der Entwicklungsphase zwischen der Übernahme der Firma NeXT Ende 1996 und der öffentlichen Ankündigung von Mac OS X im Jahre 1998, zu dem es letztlich weiterentwickelt wurde. Nach Copland und Pink (später Taligent) war es bereits Apples dritter Versuch, das hauseigene Betriebssystem – ursprünglich die Macintosh System Software (1984–2001), die bereits 1988 als System 6 und 1991 als System 7 als technisch veraltet galt – zu modernisieren. \n\nRhapsody sollte das klassische Mac OS vollständig ersetzen und wurde auf Basis von OPENSTEP (ursprünglich NeXTStep) entwickelt. Abgesehen von zwei Entwicklerversionen ist die einzige veröffentlichte Rhapsody-Version Mac OS X Server 1.0 (1999) bis 1.2v3 (2000). Damit ist es das erste Betriebssystem mit dem Namen „Mac OS X“. \n\nDas zwischen 1998 und 2001 entwickelte Mac OS X Version 10.0 hat Rhapsody, und somit OPENSTEP und NeXTStep, als Basis. Die freien Teile davon, u. a. das spezifische BSD-Userland und der XNU-Kernel, wurden als Darwin im Quelltext unter freien Lizenzen veröffentlicht. Mac OS X, das diese BSD-Basis weiterhin nutzt, wurde 2012 in OS X (ohne „Mac“) und 2016 in macOS umbenannt. Ebenso basiert das ab 2005 entwickelte iOS auf Darwin.\n\nNach der Rückkehr von Steve Jobs 1996 zu Apple, das er 1985 im Streit verlassen hatte, wurde das von NeXT stammende moderne Unix-Betriebssystem OPENSTEP 4.2 im Projekt Rhapsody in ein Nachfolgebetriebssystem für das veraltete System 7 weiterentwickelt. Die ursprünglichen Pläne sahen vor, die objektorientierte OpenStep-Programmierschnittstelle (API) aus dem letzten NeXT-Betriebssystem OPENSTEP 4.2 unter dem neuen Namen ' zu verwenden, die es auch für Windows und Solaris gab. Im Mai 1997 kündigte Apple auf der Worldwide Developers Conference (WWDC) in den USA an, dass sich Software, die unter Rhapsody programmiert werde, problemlos für Windows und weitere Betriebssysteme kompilieren lasse, wenn das zugrundeliegende OPENSTEP-kompatible Framework ' in der Windows-Version installiert sei. Diese Art der Kompatibilität wird als Quelltext-kompatibel bezeichnet. Apple, das seit 1994 mit den Macintosh-Rechnern die PowerPC-Architektur verwendete, musste jedoch zuallererst das zugekaufte OPENSTEP-Betriebssystem, das nur für die Plattformen Intel i486, Motorola 680x0 und Sun SPARC verfügbar gewesen war, auf die eigene Plattform portieren. Die relativ leichte Portierbarkeit war dem Mikrokernel Mach zu verdanken, der bereits in früheren NeXTStep-Versionen zusätzlich auf HP PA-RISC und angeblich im Labor bereits versuchsweise auf PowerPC portiert gewesen war, auf der jedoch die Performance Schwierigkeiten bereitet haben soll. Bei Apple war man bis 1997 noch voll auf eine Multi-Plattform-Strategie ausgerichtet, weshalb Rhapsody anfangs neben dem PowerPC auch auf Intel-i486-PCs lauffähig blieb – von der Rhapsody Developer Release gab es 1997 daher drei Installationsmedien: für Power Macintosh, für den Intel-PC und das \"\"-Framework für Microsoft Windows NT. Die anderen Prozessorarchitekturen (m68k, SPARC) wurden bei Rhapsody nicht weitergeführt.\n\nUm Kompatibilität für das zu ersetzende Macintosh-Betriebssystem, das ab Version 7.6 in „Mac OS“ umbenannt wurde, zu erreichen, wurde mit \"\" eine virtuelle Maschine in Rhapsody eingebaut, sodass auf einem virtualisierten Mac OS 8 alte Macintosh-Software auf Rhapsody für Power Macintosh ausgeführt werden konnte. Auf der Intel-Version war die ' nicht lauffähig. Für neue Programme sollten vor allem die NeXTStep-Bibliotheken, die ', zum Einsatz kommen. Zusätzlich setzte das neue Betriebssystem voll auf Java.\n\nDas moderne Design von Rhapsody hing jedoch gänzlich vom ab und Programme hätten dementsprechend mehr oder minder aufwändig portiert werden müssen. Software-Lieferanten wie Microsoft, Adobe und Macromedia, die mit ihren Anwendungen für das Macintosh-Ökosystem unverzichtbar waren, zeigten sich nach der WWDC 1997 jedoch nicht bereit diesen Aufwand auf sich zu nehmen. Parallel dazu wurde intern daran gearbeitet, die ' noch besser in Rhapsody zu integrieren. Bald war jedoch klar, dass es nicht möglich war, die veraltete Programmierschnittstelle von Mac OS 7.6 in Rhapsody zu integrieren ohne dabei auf die Vorzüge eines modernen Designs mit präemptiven Multitasking und Speicherschutz, welches nur die ' bot, verzichten zu müssen. Auf der MacWorld am 8. Juli 1998 in New York wurde schließlich verkündet, dass Rhapsody als Mac OS X Server 1.0 1999 auf den Markt kommen werde – ohne Intel-Plattform und ohne Windows-Framework. Steve Jobs selbst bezeichnete das Rhapsody-Experiment als gescheitert.\n\nUm doch noch einen sanften Übergang vom klassischen Mac OS hin zum modern von OPENSTEP abstammenden Unix-Betriebssystem Mac OS X zu ermöglichen wurde daher notgedrungen ein neues Framework geschaffen: Carbon. Dieses bestand im Wesentlichen aus den Programmierschnittstellen von Mac OS 8, die historisch bereits mit dem ersten \"System\" des Macintosh von 1984 eingeführt wurden, abzüglich jener Funktionen, die modernem Design wie Speicherschutz und Multitasking im Weg standen und einigen wenig genutzten redundanten Funktionen. Als Brückentechnologie konnte Carbon damit in etwa 75 % des ursprünglichen Macintosh-Baukasten abbilden und wurde sowohl integriert in Mac OS X als auch als CarbonLib unter Mac OS 8 und 9 eingeführt. Die Software-Lieferanten mussten nun nur noch geringe Anpassungen an bestehenden Programmen vornehmen, damit diese sowohl auf Mac OS (mit CarbonLib) als auch auf Mac OS X nativ ausgeführt werden konnten. Sogar Apple selbst konnte dank Carbon einige Programme direkt von Mac OS 8 in Mac OS X überführen, ohne sie aufwändig von Grund auf neu programmieren zu müssen.\n\nFür neue Programme wurde jedoch weitgehend das von NeXTStep übernommene Framework OpenStep bzw. \"\" genutzt, das ab 1998 mit Beginn der Entwicklung von Mac OS X in Cocoa umbenannt wurde.\n\nRhapsody wurde 1998 gestoppt und Mac OS X gestartet – zu Beginn der Entwicklung ist Mac OS X jedoch eine weitere Version von Rhapsody – mit dem Unterschied von Carbon als neuer (alter) Programmierschnittstelle. Während sich jedoch Rhapsody mehr an NeXTStep/OPENSTEP orientierte (z. B. in der Versionsnummer) war Mac OS X als teil-kompatibler Nachfolger und Erhalter der Macintosh-Plattform konzipiert (z. B. sowohl in Versionsnummer und Name: auf Mac OS 9 folgt Mac OS X, in der Bedeutung als römische Zahl 10).\n\nDie grafische Oberfläche von OPENSTEP wurde bei Rhapsody an Mac OS 8 angepasst. Der „Programmleistenblock“ mit „Info“, „Datei“, „Bearbeiten“ etc. von OPENSTEP wurde dabei an den oberen Bildschirmrand in eine Leiste verschoben, wie eben schon bei Mac OS 8. Auch das Design-Thema wurde auf Mac OS getrimmt und eine Programmumschaltliste (oben rechts in der Systemleiste, wie bei Mac OS) hinzugefügt.\n\nOPENSTEP (Version 4.2 von 1996/1997) basierte noch auf dem Unix-Derivat 4.3BSD und verwendete einen Mach-2.5-Kernel. Um dieses System für die Macintosh-Hardware nutzbar zu machen musste es auf die PowerPC-Architektur portiert werden. Gleichzeitig jedoch wurde das System auf 4.4BSD-Lite aktualisiert und der Kernel wurde auf Mach 3, mit Teilen von MkLinux (welches bereits ein funktionierender Mach-3-Kernel war) sowie mit Teilen des FreeBSD-Kernels ebenfalls aktualisiert. Dieses Kernsystem wurde erstmals in der Version von Mac OS X Server 1.0 (Rhapsody 5.3, 1999), mitsamt dem quelloffenen und unter freier Lizenz stehenden Userland, als Darwin-Betriebssystem (Version 0.1, 1999) veröffentlicht. Der Kernel wurde XNU genannt und unter die freie Lizenz APSL gestellt. Unter Rhapsody weist sich der Kernel zwar noch nicht als XNU aus, ist dies aber effektiv, so wie Darwin 0.1 (der Kern von Mac OS X Developer Preview 1 und Darwin OS 0.1, beide von 1999) im Wesentlichen Rhapsody 5.3 entspricht.\n\nWeitere – proprietäre – Bestandteile des Systems waren die \"Yellow Box\" als das weiterentwickelte objektorientierte Framework OpenStep, wie es im Betriebssystem OPENSTEP bis Version 4.2 und in Solaris enthalten war (und aus dem später unter Mac OS X Cocoa-API hervorging), die \"Blue Box\" (die unter Mac OS X als Classic-Umgebung weiterentwickelt wurde) um Anwendungen für Mac OS nutzen zu können (nur in der PowerPC-Version), und eine Java-Umgebung. Carbon vom späteren Mac OS X zur einfachen Portierung von Mac‑OS-Anwendungen war dagegen noch nicht enthalten. Außerdem wurden viele Apple-eigene Technologien wie QuickTime auf Rhapsody (und somit auf \"Yellow Box\" bzw. später Cocoa) portiert.\n\nDie ersten beiden Developer Previews von Mac OS X 10.0 (beide 1999) waren Rhapsody sehr ähnlich, boten aber bereits die (unfertige) zum klassischen Mac OS kompatible Carbon-Programmierschnittstelle. Der Darwin-Systemkern wird seither ständig durch Apple weiterentwickelt und bildet die Basis für alle nachfolgenden auf Mac OS X basierenden Betriebssysteme – von denen Rhapsody (bzw. NeXTStep/​OPENSTEP) somit der Urvater ist.\n\nDa es direkt aus OPENSTEP Version 4.2 entwickelt wurde, startet Rhapsody mit Versionsnummer 5.0. OPENSTEP, bis Version 3.3 noch unter dem Produktnamen NeXTStep, gab es ab Version 4.0 nur für die i486- und die SPARC-Architektur – neben der ursprünglichen Motorola-68k-Architektur der NeXT-Hardware, deren Produktion jedoch bereits 1993 eingestellt werden musste. Da Apple die PowerPC-Architektur für die eigenen Macintosh-Computern nutzte, musste das Betriebssystem erst auf PowerPC portiert werden, wobei die Intel-Version zunächst weitergeführt wurde. Nach der Aufgabe von Rhapsody als gescheitertes Projekt entstand daraus schließlich Mac OS X, welches nur mehr auf PowerPC veröffentlicht wurde, da Apple nun kein weiteres Interesse daran hatte, das Betriebssystem auch für Nicht-Apple-Computer zu vermarkten. Während sich Mac OS X Server 1.0-1.2v3 intern noch als Rhapsody ausweist, wurde jeder Hinweis darauf mit Erscheinen der Public Beta von Mac OS X 10.0 (und zuvor der an Entwickler abgegebenen Developer Previews) entfernt.\n\n"}
{"id": "589382", "url": "https://de.wikipedia.org/wiki?curid=589382", "title": "Heinz Nixdorf MuseumsForum", "text": "Heinz Nixdorf MuseumsForum\n\nDas Heinz Nixdorf MuseumsForum (HNF) in Paderborn ist das größte Computermuseum der Welt (Stand 2018).\n\nBenannt ist es nach dem Paderborner Computerpionier und Wirtschaftsunternehmer Heinz Nixdorf.\n\nIn den Jahren 1992 bis 1996 wurde das HNF in den Räumlichkeiten der ehemaligen Hauptverwaltung der Nixdorf Computer AG von den Berliner Architekten Ludwig Thürmer und Gerhard Diel und einem Wissenschaftsteam um den Mathematiker Norbert Ryska konzipiert und errichtet. Im Beisein des damaligen Bundeskanzlers Helmut Kohl wurde das Haus am 24. Oktober 1996 eröffnet und hat durchschnittlich über 110.000 Besucher jährlich, so dass bereits die 2,4 Mio. Besuchergrenze überschritten wurde. Getragen wird die Einrichtung von der \"Stiftung Westfalen\"; neben der \"Heinz Nixdorf Stiftung\", entstanden aus dem Nachlass von Heinz Nixdorf.\n\nDas Museum präsentiert in seiner Dauerausstellung 5.000 Jahre Geschichte der Informations- und Kommunikationstechnik. In einer historischen Zeitreise wird der Bogen gespannt, von der Entstehung der Schrift in Mesopotamien um etwa 3.000 vor Christus, bis zu aktuellen Themen wie dem Internet, künstlicher Intelligenz und der Robotik. Auf 6.000 Quadratmetern sind mehr als 5.000 Exponate zu sehen, die auf zwei Etagen organisiert sind. Insgesamt verwahrt das Museum etwa 25.000 Objekte. Einige Museumsobjekte sind in museum-digital, einer Online-Datenbank, abrufbar.\n\nIm ersten Obergeschoss findet der Besucher die historischen Themen. Der Hintergrund der Informatik als Lehre von der automatischen Verarbeitung von Information, die in der Erfindung der Computer mündet, wird hier grundsätzlich beleuchtet. Information in Form von Schrift, Sprache und Mathematik wird in allen drei Aspekten aufgegriffen.\nDie Themen sind:\n\nDie beginnende Konvergenz der verschiedenen Technologien wird konsequent in den Ausstellungsstücken zur Büroautomatisierung aufgegriffen; allen voran mit Geräten aus dem Haus des IBM-Vorgängers \"Hollerith\". Es bleibt aber ebenfalls Raum für Exoten wie den „Schachtürken“, eine mechanische Webstuhlsteuerung und die frühen Computer wie ENIAC oder die Systeme von Zuse.\n\nEin besonderer Bereich bleibt hier auch der Kryptografie vorbehalten, in dem von frühen Chiffren aus der Antike bis zur Enigma die Geschichte der Verschlüsselung vorgestellt wird.\n\nIm zweiten Obergeschoss finden sich die moderneren Ausprägungen der Computer. Die Sortierung ist hier weniger zeitlich als thematisch gehalten. Die Themen sind ebenfalls breit gefächert und decken die gesamte Palette vom Heimcomputer und Videospiel bis zum Supercomputer mit vielen ihrer Anwendungen ab.\n\nDer Einstieg wird mit frühen Computern und der Entwicklung von der Röhre über das Relais hin zu Transistor-basierten Systemen gemacht. Eine eigene Abteilung widmet sich später dem Mikroprozessor und seiner Fertigung.\n\nDem PC als wichtigen Schritt hin zur umfassenden Verfügbarkeit als Bürogerät wird ebenso ein Bereich gewidmet, wie den Systemen der mittleren Datentechnik. Der Schritt vom PC zum Heimcomputer wird mit einer kompletten Sammlung von Klassikern wie zum Beispiel dem Altair 8800, Apple Lisa oder Texas Instruments TI-99/4A dokumentiert.\n\nEin besonderes Schmuckstück ist der Supercomputer Cray-2.\n\nBeeindruckend sind die Exponate zu aktuellen Themen wie Robotik, Neuen Medien, Wearable Computing und künstliche neuronale Netze.\n\nDas besondere an der Ausstellung ist, dass zu allen Themen und Zeiten immer wieder, neben den reinen Exponaten und den ausführlichen Erläuterungen, Experimente, Aufgaben und Beispiele in die Ausstellung integriert sind, die dem Besucher die Möglichkeit zu eigenen Erfahrungen und zum Ausprobieren geben.\n\nDas dritte Obergeschoss beherbergt ein Schülerlabor und ein Schülerforschungszentrum. Zudem wird die Fläche für zeitlich begrenzte Ausstellungen und für Seminare und Weiterbildungen genutzt. Bis zum 20. Juli 2008 war dort z. B. die Sonderausstellung „Zahlen, bitte! Die wunderbare Welt von null bis unendlich“ zu sehen. Vom 18. Januar bis 5. Juli 2009 wurde die Sonderausstellung „Computer.Sport“ gezeigt. Die Sonderausstellung „Codes und Clowns. Claude Shannon - Jongleur der Wissenschaft“ war vom 6. November 2009 bis 28. Februar 2010 für die Besucher geöffnet. Das HNF bildete am 18. Oktober 2012 den Startschuss für die weltweite Tournee des „Max Planck Science Tunnels“, der bis zum 24. Februar 2013 gezeigt wurde. Vom 2. September 2015 bis zum 10. Juli 2016 wurde die vielbesprochene Ausstellung „Am Anfang war Ada - Frauen in der Computergeschichte“ präsentiert. Das Jahr 2019 steht im Zeichen der großen Sonderausstellung „Aufbruch ins All – Raumfahrt erleben“.\n\nEin umfangreiches Veranstaltungsangebot ergänzt die Dauerausstellung. Vortragsreihen, Diskussionen und Kongresse thematisieren Fragen der Informationsgesellschaft. In Programmen der Museumspädagogik können Kinder unter anderem mit dem Abakus rechnen, Geheimschriften lernen oder Roboter bauen. Führungen finden zu zahlreichen Themen statt. Man kann sich jederzeit den Film „Zehn Hoch“ ansehen. Bis 2009 war im HNF das Business Forum, eine Weiterbildungs- und Qualifizierungsplattform in den Bereichen Technologietrends, E-Business und Kompetenzentwicklung untergebracht. \n\nIn den Jahren 2000 und 2016 war das HNF Veranstaltungsort des Bundeswettbewerbs „Jugend forscht“. Seit 2010 richtet das HNF als Patenunternehmen jährlich den Regionalwettbewerb \"Jugend forscht – Schüler experimentieren\" aus.\n\nIm Jahr 2014 wurde das zentraleuropäische Finale der First Lego League im HNF ausgetragen. Der jährliche Regionalwettbewerb der FIRST LEGO League findet in Kooperation des HNF mit der Initiative „Paderborn ist Informatik“ statt.\n\nDer WDR Computerclub sendete drei Sondersendungen mit dem Namen \"WDR-ComputerNacht\" live aus dem HNF. Diese Sendungen wurden jeweils an einem Wochenende am Jahresende der Jahre 1998, 1999 und 2001 veranstaltet.\n\nVon 2001 bis 2005 war das HNF jährlicher Austragungsort der „RoboCup German Open“.\n\nSeit 2004 ist das HNF Austragungsort des Schachturniers Paderborner „Schachtürken-Cup“.\n\nDer Avatar Max ist ein virtueller Agent, der als Museumsführer im Heinz Nixdorf MuseumsForum in Paderborn eingesetzt wird. Auf Grundlage einer BDI-Architektur verfügt Max über proaktive Fähigkeiten, die es ihm erlauben, Menschen anzusprechen und in ein Gespräch zu verwickeln.\n\n\"Thürmer, Ludwig / Diel, Gerhard (Hgg.), Die Entstehung des Heinz Nixdorf MuseumsForum. Architektur und Design an der Schnittstelle von Mensch und Technik, Berlin 1996.\"\n\n\n"}
{"id": "589742", "url": "https://de.wikipedia.org/wiki?curid=589742", "title": "Mac OS 9", "text": "Mac OS 9\n\nMac OS 9 ist die letzte Version des ursprünglichen Betriebssystems Mac OS, vormals „System“ oder „System Software“, für Macintosh-Rechner des US-amerikanischen Unternehmens Apple. Es werden nur Macs mit PowerPC-Prozessor unterstützt. Mac OS 9.0 wurde am 23. Oktober 1999 herausgegeben, die letzte Aktualisierung auf Version 9.2.2 wurde am 5. Dezember 2001 veröffentlicht.\n\nMac OS 9 wurde vor seiner Veröffentlichung in Entwicklerversionen als „Mac OS 8.7“ bezeichnet. Dementsprechend sind die Änderungen, die Apple im Halbversionsschritt von 8.1 nach 8.5 vornahm, größer als diejenigen von 8.6 nach 9. Der Sprung in der Versionsnummer ist auch dadurch zu begründen, dass Apple für Mac OS X, die neue Generation des Betriebssystems, der 10 nahekommen wollte. Um den Verkauf zu fördern, führte Apple die zunächst kostenlosen \"iTools\" (später .Mac, dann MobileMe, und schließlich iCloud) ein, die vor allem ein freies E-Mail-Konto beinhalteten, setzte aber Mac OS 9 für die Registrierung voraus.\n\nDie wichtigsten Neuerungen von Mac OS 9 waren Mehrbenutzerunterstützung, die Schlüsselbund-Funktion, mit der ein zentraler Aufbewahrungsort für Passwörter eingeführt wurde, und die \"Software-Aktualisierung\", die verfügbare Updates automatisch aus dem Internet anbot und auf Wunsch installierte. Die Server für ältere Systeme von Apple sind jedoch nicht mehr verfügbar, sodass die \"Software-Aktualisierung\" wie auch die \"iTools\" nun nicht mehr funktionieren.\n\nNachdem bereits im Jahre 2000 die erste Vorschauversion des Nachfolgerebetriebssystems, Mac OS X 10.0 („Cheetah,“ 2001, Public Beta bereits 2000), erschienen war, kündigte Apple-Chef Steve Jobs im Mai 2002 an, die Entwicklung von Mac OS 9 werde nicht mehr fortgeführt. Im Januar 2003 wurde der erste Power Mac verkauft, der nicht mehr Mac OS 9 starten konnte. Nach Protesten von Anwendern wurde im gleichen Jahr nochmals ein Power Mac G4 aufgelegt, der Mac OS 9 nativ booten konnte – die letzten Mac-OS‑9-bootfähigen Macs wurden von Apple offiziell im Sommer 2004 verkauft. Die Ausführung von Mac‑OS-Programmen ist auf allen PowerPC-basierten Macintosh-Computern weiterhin in der Classic-Umgebung von Mac OS X bis Version 10.4 „Tiger“ möglich.\n\nMac OS 9 läuft auf Macintosh-Computern mit PowerPC-Prozessor, 32 MB Hauptspeicher (40 MB inklusive virtuellem Speicher) und bei der empfohlenen Installation zwischen 150 und 250 MB freien Festplattenspeicherplatz, je nach Macintosh-Modell und Formatierung des Zielmediums. Bei Vollinstallation werden bis zu 400 MB an Daten auf das Zielmedium installiert.\n\nNicht mehr unterstützt wird die Blue Box (MacOS.app) von Rhapsody und Mac OS X Server 1.0 bis 1.2v3, die für die Ausführung von Mac OS als virtuelle Maschine innerhalb des als Nachfolger eigenständig entwickelten Betriebssystems gedacht ist. Unter Blue Box kann nur Mac OS 8.5.1 und 8.6 virtualisiert ausgeführt werden.\n\nIn der Weiterentwicklung von Blue Box, der Classic-Umgebung von Mac OS X, kann Mac OS ab Version 9.1 virtualisiert werden, um Programme, die unter Mac OS X nicht laufen, verwenden zu können. Die Classic-Umgebung gibt es in Mac OS X nur auf der PowerPC-Architektur und nur bis Mac OS X Tiger (10.4, 2005) – auf allen seit 2006 von Apple verkauften Macs mit Intel-Prozessor und ab Mac OS X Leopard (10.5, 2007), auch auf PowerPC, wird die Virtualisierung von Mac OS mittels „Classic“ nicht mehr unterstützt. Als Alternative empfiehlt sich SheepShaver, der sowohl als Virtuelle Maschine auf PowerPC-basierten Macs als auch als Emulator auf Intel-basierten Macs Mac OS emulieren kann, allerdings nur bis einschließlich Version 9.0.4.\n\nSeit System 7 mit Einführung der mit bezeichneten Macs hat Apple die im ROM enthaltenen Teile des Betriebssystems auf ein Minimum reduziert. Mit der Einführung von Open Firmware benötigt Mac OS daher eine codice_1 betitelte Datei, die Teile des spezifisch auf den jeweiligen Macintosh-Computer angepassten Macintosh-Baukasten () bereithält. Ohne diese systemspezifischen Funktionen ist Mac OS nicht lauffähig. Bei älteren Systemen, von Apple bezeichnet, war der Macintosh-Baukasten noch im ROM gespeichert. Macs, die vor der Veröffentlichung von Mac OS 9 bereits auf dem Markt waren, werden zwar von der mitgelieferten ROM-Datei unterstützt, nicht jedoch Modelle, die nach Mac OS 9 auf den Markt kamen. Da der Macintosh-Baukasten nicht mehr im ROM neuer Macintosh-Computer enthalten ist benötigen Modelle ab ca. 2000 spezifische Versionen von Mac OS, die auf Installation- oder Wiederherstellungs-Medien mitgeliefert wurden und die spezifische ROM-Datei für das jeweilige Macintosh-Modell beinhalten. Die letzte Veröffentlichung des Mac-OS-ROM für Mac OS 9 ist Version 9.5.1, die beigelegt (, enthalten im vorinstallierten Mac OS 9.2.2 und auf dem Wiederherstellungsmedium) mit dem iMac G4 800 MHz 17″ „Flat Panel“ ab 17. Juli 2002 verkauft wurde. Dasselbe iMac-Modell wurde (bezeichnet als „X only“ – also nur für Mac OS X) am 4. Februar 2003 ohne die Möglichkeit, Mac OS 9.2.2 nativ zu starten und somit ohne Mac-OS-ROM, neu aufgelegt.\n\n"}
{"id": "590437", "url": "https://de.wikipedia.org/wiki?curid=590437", "title": "Die kleine Lampe", "text": "Die kleine Lampe\n\nDie kleine Lampe ist ein zweiminütiger animierter Kurzfilm aus dem Jahre 1986, der von den Pixar Animation Studios produziert wurde und als Meilenstein in der Computeranimation gilt.\n\n2014 wurde der Film ins National Film Registry aufgenommen.\n\nDer Film zeigt, wie zwei Schreibtischlampen (eine größere namens Luxo, von lat. lux : Licht, und eine kleinere namens Luxo Jr.) mit einem Gummiball spielen. Als Luxo Jr. auf den Ball springt und darauf herumhüpft, entweicht die Luft daraus. Die kleine Leuchte schaut etwas verdutzt in die Kamera und schafft dann den platten Ball beschämt zur Seite. Die Größere glaubt zunächst, die Kleinere hätte die Lektion gelernt – kurz darauf spielt sie jedoch schon wieder, diesmal mit einem noch größeren Ball.\n\nDer Film kommt mit zwei Spots und etwas ambientem Licht aus und besteht nur aus wenigen einfachen Objekten, von denen viele Rotationskörper sind. Da beide Spots die Glühlampen (und Augen) der animierten Leuchten sind, kommt es zu vielen realistischen Schattenspielen. Physis und Geräuschkulisse wirken ebenfalls glaubhaft, auch wenn sie sich auf rollende, rutschende und springende Bälle, Kabel und Stabsysteme (die Leuchtenmechanik) beschränken.\n\n\"Die kleine Lampe\" war der erste Film des Animationsstudios Pixar und das Regiedebüt von John Lasseter, der auch für die meisten weiteren Erfolge Pixars verantwortlich zeichnet (u. a. \"Toy Story\", \"Monster AG\", \"Findet Nemo\")\n\n1986 wurde \"Die kleine Lampe\" als erster komplett computeranimierter Film für den Oscar als Bester animierter Kurzfilm nominiert.\nEr legte somit den Grundstein für die wachsende Beliebtheit computeranimierter Filme ab Mitte der 1990er Jahre, durch die der konventionelle Zeichentrickfilm inzwischen fast vollständig vom Markt verdrängt wurde.\n\nSeit Anfang der 1990er Jahre ist die kleine Lampe Luxo Jr. ein Bestandteil des animierten Pixar-Logos, wo sie auf dem Buchstaben I herumhüpft, ihn plattdrückt und anschließend in die Kamera schaut. In fast allen Filmen des Studios sind Verweise auf den Kurzfilm zu finden (Der blaue Ball, mit dem die Lampen spielen, ist beispielsweise in den \"Toy Story\"-Filmen und \"Monster AG\" zu sehen).\n1999 wurde \"Die kleine Lampe\" als Vorfilm der Pixar-Produktion \"Toy Story 2\" auch im Kino gezeigt.\n\n\n\n\n"}
{"id": "592123", "url": "https://de.wikipedia.org/wiki?curid=592123", "title": "E-media", "text": "E-media\n\ne-media ist eine österreichische Zeitschrift für \"smarter leben\" die von der VGN Medien Holding GmbH vertrieben wird. \n\nDie Zeitschrift wendet sich vor allem an technisch interessierte Privatpersonen, stellt neue Produkte vor, bringt Gerätetests, App-Vorstellungen sowie Tipps zu Hard- & Software wie auch Hintergrundgeschichten aus der Welt der IT. Die zum Teil komplexen Themen werden anschaulich und leicht verständlich aufbereitet. e-media erscheint monatlich, umfasst ca. 100 Seiten und kostet aktuell 4,90 Euro pro Einzelheft. Sie ist eine der bedeutendsten Technikzeitschriften Österreichs. \n\nGegründet wurde e-media im Jahr 2000 als Auskoppelung zur österreichischen Fernsehzeitschrift tv-media, die im selben Verlag erscheint. Seither wurde der Technikteil in TV-MEDIA deutlich reduziert. Ursprünglich erschien die Zeitschrift 14-täglich an jedem zweiten Freitag, zuerst zu 2,00 Euro, dann zu 2,50 Euro. Das Heft bot Beiträge in den Rubriken Internet, Computer-Hardware, Computer-Software, Telekom & Handy, Multimedia und Spiele. Auch ein Tipps-&-Tricks-Teil mit Workshops fand sich in jedem Heft. Die Druckauflage betrug bis zu rund 72.000 Exemplare, die Reichweite lag 2011 bei etwa 3,5 Prozent. \n\nSeit Juni 2015 erscheint e-media monatlich, jeweils am letzten Freitag. Chefredakteur von e-media ist Goran Miletić, die wirtschaftlichen Agenden leitet Samira Kurz. Die Rubriken im Heft wurden überdacht und heißen nun: \"Cool Tool\" als Einstieg in das Heft mit Informationen über neue Gadgets. Es folgen \"Test + Tipp\", \"Intelligent\" und schließlich \"Web + App\". Im 2. Halbjahr 2017 wurden von 46.017 gedruckten Heften 48.262 verbreitet, davon 40.728 verkauft. Die Reichweite gibt die Verlagsgruppe News für 2017 mit 2,6 Prozent an.\n\n"}
{"id": "592297", "url": "https://de.wikipedia.org/wiki?curid=592297", "title": "WordPerfect", "text": "WordPerfect\n\nWordPerfect (WP) ist ein Textverarbeitungsprogramm, das zunächst von Satellite Software International und später von der WordPerfect Corporation aus Orem, Utah entwickelt und vermarktet wurde. Bis Anfang der 1990er-Jahre stellte WordPerfect de facto den „Textverarbeitungsstandard“ für DOS-Computer dar. WordPerfect ersetze das zuvor populäre WordStar.\n\nWas WordPerfect von anderen Textverarbeitungsprogrammen grundsätzlich unterscheidet, sind die Steuerzeichen. Jeder Befehl, der innerhalb eines Dokuments vom Nutzer verwendet wird, ist durch ein Steuerzeichen jederzeit erkennbar und auch nachträglich veränderbar. Hierfür lässt sich in das Fenster der Anwendung das Steuerzeichenfenster ein- und ausblenden. Wird beispielsweise ein Wort in der Auszeichnung „fett“ dargestellt, so ist dies im Steuerzeichenfenster durch ein von einer Pfeilform umrandetes Wort „Fett“ vor und nach dem in Fettschrift dargestellten Text sichtbar, wobei die jeweiligen Pfeilspitzen klarmachen, wo der Befehl beginnt und wo er endet. Auch gilt bei WordPerfect der Grundsatz, dass ein Befehl so lange aktiv bleibt, bis er wieder durch den Nutzer aufgehoben bzw. ausgeschaltet wird. Durch diese eindeutige Programmbedienung und Darstellung hat der Nutzer jederzeit den Überblick über das Programmverhalten.\n\nFerner bietet das Programm ein echtes WYSIWYG. Zwar gibt es in Anlehnung an Word von Microsoft noch eine Druckbildvorschau, doch ist diese bei WP überflüssig, da die Bildschirmdarstellung immer exakt das Druckergebnis zeigt. Ebenfalls eine Besonderheit ist die Abwärts- und Aufwärtskompatibilität mit anderen Versionen dieses Programms.\n\nAbwärtskompatibilität bedeutet in diesem Falle, dass Dokumente, mit z. B. WordPerfect X3 abgespeichert, auch ohne Probleme mit WordPerfect 6.0 für Windows geöffnet werden können (nicht bei noch älteren Versionen). Nur die neuen Funktionen von X3 können in Version 6 natürlich nicht gezeigt werden. Speichert man dann unter Wordperfect 6.0 und lädt das Dokument wieder in einer neueren Version, so sind die neueren Funktionen für diese Version immer noch uneingeschränkt vorhanden.\n\nDie Informatiker Bruce Bastian und Alan Ashton schrieben die erste Version von WordPerfect 1979 für einen Minicomputer von Data General. Bastian war seinerzeit Student an der Brigham Young University in Provo (Utah, USA). Sein Programmierstil bei der Entwicklung einer Software, die eine in einem Sportstadion spielende Musikkapelle auf dem Bildschirm zeigen sollte, fiel seinem Professor Alan Ashton auf, was schließlich zur Zusammenarbeit und zur Entwicklung der WordPerfect-Urversion führte. Ein Jahr später gründeten die beiden Satellite Software International (SSI) und verkauften ihr Programm als „SSI*WP“, das aufgrund des Erfolges von WordStar später umbenannt wurde. Im November 1982 erschien schließlich die erste Version von WordPerfect für den IBM-PC. Zum Starten des Programmes, welches auf einer 5,25-Zoll-Diskette geliefert wurde, musste aus Gründen des Kopierschutzes jedes Mal die sogenannte Schlüssel-Diskette, auf der sich auch das Programm selbst befand, in das Diskettenlaufwerk eingelegt werden. Dem Programmpaket lagen drei weitere Disketten bei: eine enthielt das Programm, welches auf einem PC mit einer CGA-Farbgrafikkarte samt Farbmonitor lauffähig war. Unterstreichungen oder kursive Schrift wurden dann im Text auf dem Bildschirm in jeweils einer anderen Farbe und weder unterstrichen noch kursiv dargestellt. Es gab zudem eine „Speller“-Disk (Wörterbuch zur Rechtschreibkorrektur) und eine „Supplementary“-Disk, auf der sich unter anderem ein Programm zum eigenhändigen Modifizieren der mitgelieferten Druckertreiber befand. Der IBM-kompatible PC musste über mindestens 128KB RAM und idealerweise zwei 5,25-Zoll-Diskettenlaufwerke mit jeweils 360KB Lesekapazität verfügen: ein Laufwerk für die KeyDisk, das andere für die Speller-Disk.\n1986 änderte SSI seinen Namen in „WordPerfect Corporation“. Zur gleichen Zeit löste WordPerfect das bis dahin dominierende MicroPro WordStar als das weltweit am weitesten verbreitete Textverarbeitungssystem ab, bevor es selbst ab 1992 von Microsoft Word binnen anderthalb Jahren von der Spitze verdrängt wurde.\n\nZu DOS-Zeiten war WordPerfect quasi Textverarbeitungsstandard und insbesondere an Universitäten aufgrund seines riesigen Funktionsumfangs und seiner weitestgehenden Freiheit von Programmfehlern (Bugs) weitverbreitet. Gesteuert wurde die Software über Kombinationen von , und mit einer Funktionstaste (z.B.  +  für das Druckmenü). Eine Tastaturschablone, die mit dem Programm geliefert wurde und oberhalb der Funktionstasten auf die Tastatur geklebt wurde, half Anfängern, sich schnell mit den Tastaturkürzeln zurechtzufinden. Erst in Version 5 wurde eine Menüleiste eingefügt, die mit der -Taste aktiviert werden konnte. In Version 5.1 war WordPerfect sehr verbreitet. Die Version WordPerfect 6.0 war eine der ersten vollgrafischen Anwendungen unter DOS, und so ließen sich auch Formatierungen und Grafiken direkt anzeigen und bearbeiten. DOS wurde als Betriebssystem jedoch immer unbeliebter und so verkaufte sich die Software immer schlechter. Es gab wohl noch einige populäre Windows-Versionen (z.B. Version 6.1), doch Word dominierte seit Windows 3.1 zusehends den Markt. Ein Grund hierfür war auch, dass auf neuen PCs gemeinsam mit dem Betriebssystem Windows häufig auch schon Word vorhanden war.\n\nNach der Übernahme der WordPerfect Corporation durch Novell im Juni 1994 und dem Weiterverkauf an Corel im Januar 1996 war es lange Zeit fraglich, ob sich WordPerfect überhaupt am Markt halten könnte. Es folgten die Versionen 7, 8 und 9, die sich insgesamt aber nur sehr schlecht verkauften.\n\nAb der Version 11 versucht die Firma Corel nun erneut Marktanteile zu gewinnen, und es erschienen wieder in regelmäßigen Abständen neue Versionen und Updates.\n\nSeit dem Marktstart der Version 12 (Juni 2004) sieht es so aus, als habe Corel sein Produkt WordPerfect Office (enthält die Textverarbeitung WordPerfect, die Tabellenkalkulation Quattro Pro und das Präsentations- und Grafikprogramm Presentations) zumindest als Nischenprodukt sicher etabliert. Version 12 bietet nach Wahl zusätzlich zu der normalen WordPerfect-Bedienoberfläche eine Benutzeroberfläche, die entweder dem augenblicklichen Quasistandardprogramm MS-Word gleicht oder der vor Jahren weitverbreiteten DOS-Version von WordPerfect entspricht.\nNeben der Standard-Version (die mit Upgrade bezeichnet ist, zum Upgrade berechtigt sind sämtliche Produkte von Corel, aber auch Konkurrenzprodukte wie das Microsoft Office) wird besonders bei eBay häufig eine OEM-Version angeboten. Im Gegensatz zur Vollversion fehlen dieser die Zugaben, beispielsweise Cliparts oder TrueType-Schriftarten. Außerdem ist der Support für diese Version eingeschränkt. So ist u.a. die Installation des Service Packs 2 mit der OEM-Version nicht möglich; allerdings hat Corel inzwischen auf die harsche Kritik seitens der Anwender reagiert und ein Service Pack 3 herausgebracht, das sich auch mit OEM-Versionen installieren lässt. Ferner ist eine vollumfängliche Version für Schüler, Studenten und Lehrkräfte zu einem stark ermäßigten Preis (aber ohne gedrucktes Handbuch) erhältlich (sogenannte SSL-Version). Überhaupt werden für Bildungseinrichtungen eine Vielzahl unterschiedlicher Gruppenangebote offeriert, die zudem preislich unterhalb von Microsofts Office-Produkt angeboten werden.\n\nZu WordPerfect 12 wurde die Benutzeroberfläche modernisiert, die jetzt auch Windows-Themes unterstützen soll. Außerdem besteht ab X3 die Möglichkeit, PDF-Dateien sofort zu öffnen. Komplexere Layouts werden dabei jedoch nicht erhalten. Aus den einschlägigen Newsgroups geht allerdings hervor, dass die Benutzer mit der neuen Version nicht zufrieden sind. Unter anderem wird kritisiert, dass Bugs aus Vorversionen nicht behoben wurden und auch die Kompatibilität zu Produkten wie Microsoft Word nicht wesentlich verbessert wurde.\n\nAuch in der im April 2008 erschienenen Version 14 (offiziell X4 genannt, nur in englischer – inzwischen auch französischer – Sprache verfügbar) wurden außer einer Verbesserung der Import- und Export-Funktionen für das Format PDF und der Möglichkeit, nunmehr auch Texte im OpenDocument- und Open-XML-Format einzulesen, aber nicht speichern zu können, keine nennenswerten Neuerungen eingeführt. Die derzeit aktuelle Version ist WordPerfect Office X8.\n\nFür DOS:\n\nFür Amiga\n\nFür Atari ST\n\nFür Apple II:\n\nFür Apple Macintosh:\n\nFür NeXT Computer:\n\nFür Windows:\n\nMit der Version 6.1 von 1994 wurde der Vertrieb von WordPerfect als Einzelprogramm eingestellt. WordPerfect ist nur als Bestandteil einer kompletten Büro-Suite erhältlich. Die Programme in der Suite unterscheiden sich je nach Version, fest enthalten sind aber immer die Textverarbeitung WordPerfect, die Tabellenkalkulation Quattro Pro, das Präsentations- und Grafikprogramm Presentations und ab der Version X3 auch WordPerfect Mail, ein Personal Information Manager mit E-Mail-, Adressbuch- und Kalenderfunktionen ähnlich wie Microsoft Outlook.\n\nFür Linux:\n\n"}
{"id": "593583", "url": "https://de.wikipedia.org/wiki?curid=593583", "title": "Rosegarden", "text": "Rosegarden\n\nRosegarden ist ein Audio- und MIDI-Sequenzer mit Notenschreibfunktion (Digital Audio Workstation). Rosegarden ist eine freie Alternative zu Anwendungen wie Cubase.\n\nRosegarden entstand 1993 an der University of Bath für IRIX unter Verwendung von OSS. Ab 1995 wurde es dann auch auf Linux portiert. Diese Version erhielt später den Namen \"X11 Rosegarden\".\n\nAb April 2000 begann das Entwicklungsteam Rosegarden auf Basis von KDE/Qt3 vollständig neu zu entwickeln. Die Version 1.0 wurde am 14. Februar 2005 veröffentlicht, die letzte Version dieses Entwicklungszweiges, der noch auf Qt3/KDE3 aufbaute war Version 1.7.3 vom 2. Februar 2009. Danach verwendeten die Entwickler Qt4 und stellten die Versionsbezeichnung um. Die Versionsnummer besteht nun aus der zweistelligen Jahreszahl gefolgt von der Monatszahl, also 10.02 für die im Februar 2010 veröffentlichte erste Version dieses Zweiges.\n\n\n\n"}
{"id": "593584", "url": "https://de.wikipedia.org/wiki?curid=593584", "title": "Finale (Software)", "text": "Finale (Software)\n\nFinale ist ein Notensatzprogramm der US-amerikanischen Firma \"MakeMusic\". Es ist für die Betriebssysteme Windows und macOS verfügbar.\n\nDas Programm dient dem Erstellen und Drucken von Partituren bzw. Einzelstimmen und beherrscht alle gängigen Aufgaben des klassischen Notensatzes. Die eingegebene Musik kann über die Soundkarte per MIDI, der mitgelieferten Garritan-Samplingbibliothek oder über die VST-Schnittstelle mit anderen Samplingbibliotheken abgespielt werden. Das Programm verfügt auch über ein \"Filmfenster\", das synchron zur eingegebenen Musik den zu vertonenden Film zeigt. Import- und Exportfunktionen für MP3-, EPUB-, WAV- oder MusicXML-Dateien ergänzen die Möglichkeiten. Im deutschsprachigen Raum wird Finale von Klemm Music Technology vertrieben.\n\nFinale verfügt über verschiedene Arbeitsansichten. Das Notenfenster kann als Blatt, als Rolle oder der sogenannten Studioansicht, welche dem \"Look and Feel\" von Sequenzern nachempfunden ist, angezeigt werden. Zum Setzen von Zeichen oder zum Bearbeiten des Notentextes muss für jeden Schritt aus den Seitenleisten ein entsprechendes Icon ausgewählt werden. Der werkzeugorientierte Workflow, wie man ihn etwa auch von Photoshop kennt, wird von manchen Anwendern besonders auch im Vergleich zu Konkurrenzprodukten als unintuitiv kritisiert. Dem gegenüber steht die große Fülle an Möglichkeiten und Zusatzfunktionen, auch im Bereich Multimedia-Anbindung. Neben einem Mixer und einem Film-Fenster verfügt Finale über zahlreiche Notenfonts und unterstützt den VST-Standard.\n\nJe nach Anspruch und Bedarf gibt es verschiedene Ausgaben von Finale, die jeweils abgestufte Funktionen haben. Aufgelistet sind jeweils einige Merkmale, die sich gegenüber der nächstniedrigeren Version unterscheiden.\n\nDie ursprüngliche und erste Version von Finale erschien 1989. Initialer Programm-Autor und Programmierer war Phil Farrand, der für die Firma \"Coda Music\" arbeitete. Später kaufte \"Net4Music\" die Software auf und nannte sich einige Zeit später in \"MakeMusic\" um. Bis Finale 3.7 folgten die Versionsnummern der Releases der klassischen Bezeichnung, von da an wechselte der Herausgeber zu Jahreszahlen als Identifizierung und brachte jährlich eine neue Ausgabe von Finale heraus, beginnend mit Finale 1997. 2012 wurde bekannt gegeben, dass der jährliche Rhythmus unterbrochen wird und es 2012 keine Version 2013 geben wird.\n\nFinale 2004 ist erstmals auch unter Mac OS X lauffähig, doch erst ab Finale 2005 sollte Finale auch erstmals hybrid für Windows und Mac OS ausgeliefert werden. Im Jahr 2005 erschien zum ersten Mal die abgespeckte Version Finale Allegro.\nFinale NotePad wurde in den Versionen 2009–2011 kostenpflichtig. In der Zwischenzeit bestand mit \"Finale Reader\" eine kostenfreie Möglichkeit zum Anschauen von Finale-Dateien. 2010 wurden die Varianten Notepad plus und Allegro eingestellt.\n\nDie Version \"2012\" wurde im Herbst 2011 veröffentlicht und bietet Neuerungen wie den ScoreManager, Unterstützung von Text im Unicode-Format, direktes Erstellen von PDF-Dateien aus dem Programm heraus, einen aktualisierten Noten-Assistenten (\"Wizard\"), verbessertes Sound-Management sowie eine umfangreichere Garritan Library. Die Version Finale NotePad 2012 wird wieder kostenlos angeboten.\n\nIm Jahr 2012 wies die Firmenbilanz von MakeMusic erstmals rote Zahlen auf, weswegen im Sinne der Shareholder Veränderungen der Firmenstrukturen anvisiert wurden.\nVor diesem Hintergrund wurde das Übernahmeangebot eines Finanzinvestors mit der Summe von 13,5 Millionen $ angenommen. MakeMusic ist seither Tochter der Risikokapital-Gesellschaft LaunchEquity Partners.\n\nIn einem im März 2014 veröffentlichten Posting innerhalb der MusicXML-Community deckte die Informatik-Koryphäe L Peter Deutsch den Umstand auf, dass das neue Dateiformat von Finale 2014 geändert und verschlüsselt worden sei, um Kunden zukünftig an das eigene Produkt zu binden, da für die innerhalb des Major Releases erfolgten internen Umstellungen keine nachvollziehbaren, technischen Beweggründe sprechen würden. Auf die von Deutsch vorgebrachten Kritikpunkte wurde seitens MakeMusic nicht inhaltlich eingegangen. Im Rahmen eines Umzugs der Postings von einer vormals bestehenden Mailingliste auf ein neu eingerichtetes Forum wurden L Peter Deutschs diesbezügliche Beiträge gelöscht.\n\nAm 7. August 2014 veröffentlichte MakeMusic die Pressemitteilung, dass sie mit \"Peaksware\" fusioniere. Peaksware ist ebenfalls Tochter von LaunchEquity und fertigt Software für Ausdauer-Training, die von Sportlern eingesetzt wird.\n\nDas Notensatzprogramm \"Finale\" ist durch Plug-ins erweiterbar. So kann zum Beispiel das Notenscanprogramm \"SmartScore\" über eine Schnittstelle eingebunden werden. Über das Plug-in von \"Recordare\" können MusicXML-Dateien im- und exportiert werden. \"Medieval\" realisiert altertümliche Notationsstile. Die \"TGTools\" ermöglichen vereinfachte Befehle (wie beispielsweise das Setzen von Wiederholungen mit Wiederholtakten). Seit 2008 existiert auch ein Band-in-a-Box-Plugin. Mit dessen Hilfe kann für eine mit Akkorden versehene Melodiestimme automatisch eine Instrumentierung generiert werden.\nWeitere erhältliche Erweiterungen sind \"Braille Music Kit\", \"Classic Transposition\", \"Forza!\", \"Tuplet Copy\", \"Tuplet Mover\", \"Staff Sets\", \"Patterson Beams\" and \"Settings Scrapbook\".\n\nÜber die Plattform Net4Music.com konnten Anwender ihre mit Finale erstellten Dateien direkt vertreiben. Diese, von der Herstellerfirma MakeMusic! betriebene Plattform, wurde am 1. Oktober 2002 abgeschaltet.\n\nSpäter wurde eine andere Möglichkeit zum Austausch von mit Finale erstellten Noten angeboten: das Finale Showcase, deutsch als Finale-Schaufenster bezeichnet. Diese, vom englischen Hersteller MakeMusic auf dessen Produkt-Website betriebene, Plattform wurde Anfang 2013 im Rahmen einer völligen Neugestaltung jener Website abgeschaltet.\n\nFinale diente als Notationssoftware auch als Grundlage für Filmmusik, so unter anderem bei den im Jahr 2008 oscarprämierten Filme wie Ratatouille, Michael Clayton, Sweeney Todd – Der teuflische Barbier aus der Fleet Street, Elizabeth – Das goldene Königreich, Der Goldene Kompass; sowie oscar-nominierten Filmen wie Pirates of the Caribbean – Am Ende der Welt, Todeszug nach Yuma, Der Klang des Herzens, Verwünscht und Transformers.\n\n\n"}
{"id": "595502", "url": "https://de.wikipedia.org/wiki?curid=595502", "title": "Borland Quattro Pro", "text": "Borland Quattro Pro\n\nBorland Quattro Pro ist ein Tabellenkalkulationsprogramm, das von Borland entwickelt wurde und meist als Bestandteil des Officepaketes WordPerfect Office Verbreitung fand. Seit 1996 wird das Produkt von Corel vertrieben. Als das Produkt 1988 auf den Markt kam, hieß es zunächst \"Quattro\" (die italienische Bezeichnung für „vier“ als logische Konsequenz zu Lotus 1-2-3). Borland änderte ab der 1990er Veröffentlichung den Namen zu \"Quattro Pro\".\n\nDas Originalprogramm wurde auf DOS-Ebene von Adam Bosworth und Lajos Frank hauptsächlich in Assembler geschrieben. Zunächst wurde es ein kommerzieller Flop.\n\nDaraufhin wurde ein Ersatzprodukt mit dem Namen „Surpass“ in Modula-2 erworben. Die Hauptentwickler und Programmierer Bob Warfield, Dave Anderson, Weikuo Liaw, Bob Richardson und Todd Landis wurden von Borland eingestellt, um Surpass letztlich unter neuem Namen als \"Quattro Pro\" auf den Markt zu bringen.\n\nDas Vorgängerprodukt von Bob Warfield war „Farsight“ von Farsight Technologies. Es war eines der ersten Kalkulationsprodukte, die „Window-Fenster“ hatten. Es war in Modula-2 geschrieben. Eine Besonderheit war, dass die Druckertreiber direkt bei der Installation kompiliert wurden.\n\nFarsight war sehr populär in den Schulen, da es rund 50 % günstiger war als Lotus oder Excel.\n\nDie 1990 erschienene Version 2.0 bot erstmals 3D-Grafik und einen Solver (Solve For).\n\n\n\n"}
{"id": "595599", "url": "https://de.wikipedia.org/wiki?curid=595599", "title": "IZArc", "text": "IZArc\n\nIZArc ist ein Datenkompressionsprogramm für Microsoft Windows und iOS.\n\nDie grafische Benutzeroberfläche fügt sich per Drag and Drop und Kontextmenüs nahtlos ins Betriebssystem ein. \"IZArc\" beherrscht die AES-Verschlüsselung (Rijndael mit 256 Bit) und kann verteilte sowie selbstentpackende Archive erstellen. Das Programm ist in über 40 Sprachen verfügbar.\n\n\"IZArc\" kann komprimierte Dateien der folgenden Formate bearbeiten: 7z, PAK, BGA (GZA, BZA), BH, BZ2 (TBZ), CAB, JAR, LHA, TAR, tar-gzip, cpio, YZ1, ZIP. Weitere Formate wie RAR können zumindest geöffnet werden. Der Funktionsumfang von \"IZArc\" ähnelt sehr stark dem von TUGZip.\n\nAktuelle Versionen von IZArc enthielten bis vor kurzem noch Adware wie OpenCandy und SweetIM. Nachdem sich dies aber sehr negativ ausgewirkt hatte, hat man sich dazu entschlossen diese wieder rauszunehmen und auf reguläre Spenden zu vertrauen.\n\n\n"}
{"id": "596478", "url": "https://de.wikipedia.org/wiki?curid=596478", "title": "Windows Installer", "text": "Windows Installer\n\nDer Windows Installer (vormals \"Microsoft Installer\") stellt eine Laufzeitumgebung für Installationsroutinen unter Microsoft-Windows-Betriebssystemen bereit. Er besteht aus einem Windows-Systemdienst, der Paketdateien im \"msi\"-Format (Microsoft Software Installation), sowie Dateien mit den Dateiendungen \"mst\" für Transform-Dateien und \"msp\" für Patches interpretieren, entpacken und ausführen kann.\n\nDurch das Tabellenformat der Installationsdateien ist es möglich, diese Pakete mit verschiedenen Programmen zu bearbeiten oder anzupassen.\n\nEin weiterer Vorteil bei der Verwendung der Windows-Installer-Technologie ist die Reparaturfähigkeit. Der Windows-Installer-Dienst überwacht beim Starten eines durch ihn installierten Programmes, ob alle zugehörigen Komponenten vorhanden sind. Fehlt eine dieser Komponenten, versucht der Windows Installer, diese nachzuinstallieren. Dieser Prozess lässt sich auch auslösen, wenn die Setup-Routine erneut ausgeführt wird – der Installer bietet dem Benutzer dann eine Reparatur (sowie eine Deinstallation) an.\n\nMSI ist die Abkürzung für MicroSoft Installer. Es gibt Hersteller, die Editoren für MSI-Dateien bereithalten, wie z. B. Flexera Software mit dem Produkt \"InstallShield\". Weitere Produkte sind Advanced Installer von Caphyon, Wise Installer von Symantec, AKInstallerMSI von AKApplications und InstallAware von InstallAware Software Corporation. Auch die Entwicklungsumgebungen Visual Studio (ab Version 2002) von Microsoft erlauben im begrenzten Umfang die Erstellung von Windows-Installer-Paketen. Die Entwicklungsumgebung Visual InterDev konnte mit dem \"Microsoft Windows 2000 Developers Readiness Kit\" so aufgerüstet werden, dass bereits 1999 die Erstellung von MSI-Installationspaketen in der Entwicklungsumgebung möglich war.\n\nEs gibt auch kostenlose Software wie \"Makemsi\" und WiX von Microsoft zur Erstellung solcher Installationspakete. Beide erstellen MSI-Pakete ausgehend von einer Beschreibung des Paketes in einer Textdatei. \"Makemsi\" benutzt dabei eine eigene Macro-Sprache, WiX eine XML-basierte Beschreibung. Ab Version 3.0 wird WiX im Visual Studio 2010 unterstützt. Es wird sowohl Intelli-Sense für XML-Dateien angeboten als auch die Möglichkeit, zu kompilieren und zu linken. Trotzdem sind tiefergehende Kenntnisse erforderlich, um Installationen mit WiX zu erstellen.\n\nEine Installationslösung, die die Konfiguration der installierten Software mit einem Assistenten vollständig vom Paket aus übernimmt, ist von allen bekannten, aktuellen Betriebssystemen nur in Windows notwendig. Dort existierte bis zum Erscheinen von Microsoft Office 2000 keine betriebssystemeigene Einrichtung zur Konfiguration von Software, so dass das ausgelieferte Softwarepaket sich auf dem Zielrechner erst in eine gebrauchsfertige Version „verwandelt“. Ab Windows Vista, das jeden Eingriff in Betriebssystemkonfigurationen meldet, wird der Installationsprozess durch die Benutzerkontensteuerung unterbrochen, wenn das Setup höhere Rechte benötigt. Unter vorherigen Systemen wurde die Installation in diesem Fall abgebrochen.\n\nMSI-Dateien sind relationale Datenbanken, deren Tabellen Binär/Text-Daten enthalten, die Steueranweisungen für die Installation darstellen. Diese Datenbanken werden dann, zusammen mit den vorgenommenen Einstellungen während der Installation, vom Betriebssystem verwaltet. Das Softwarepaket selbst kann in der Datenbank gespeichert sein, es ist aber ebenso möglich, nur Verweise auf die Dateien abzulegen. Es können keine Abhängigkeiten zu anderen Softwarepaketen ausgedrückt werden. Der Systemdienst wurde mit \"Microsoft Office 2000\" eingeführt, dessen Setup den Dienst erstmals installierte und verwendete. Seit Windows 2000 ist die Technologie vollständig in die Microsoft-Betriebssysteme integriert.\n\nDie betriebssystemgestützte, tabellengesteuerte Installation für Windows konnte sich bei den meisten (kleineren) Anbietern von Software noch nicht in einem akzeptablen Umfang durchsetzen. Während kleinere Anbieter dazu übergehen, kostenlose skriptgesteuerte Programme wie NSIS oder Inno Setup, beides vom Windows Installer unabhängige Programme, zu verwenden, benutzen größere Konzerne den Quasi-Industriestandard InstallShield, der neben der \"Windows-Installer\"-Unterstützung auch seine eigene Laufzeitumgebung mitbringt.\n\nAusgeführt werden MSI-Dateien durch die Kommandozeile mittels codice_1 oder den Doppelklick auf den Dateinamen. Die installierte Version und optionale Parameter des Windows Installers werden mit codice_2(»Start → Ausführen«) aufgerufen. Die dabei ausgeführte Datei \"msiexec.exe\" befindet sich im Windows-Ordner \"System32\" und bei 64-bit Systemen zusätzlich als 32-bit Version im Ordner \"SysWOW64\".\n\n\n"}
{"id": "598844", "url": "https://de.wikipedia.org/wiki?curid=598844", "title": "Jimmy Neutron", "text": "Jimmy Neutron\n\nJimmy Neutron () ist die Hauptfigur der gleichnamigen US-amerikanischen computeranimierten Fernsehserie. Die Serie entstand im Auftrag von Nickelodeon als Ableger des 2001 erschienenen Films \"Jimmy Neutron – Der mutige Erfinder\".\n\nSie wurde im Juli des Jahres 2006 in den USA abgesetzt. In Deutschland wird sie in unregelmäßigen Abständen wiederholt. Ab Herbst 2010 wurde ein Ableger namens \"Planet Max\" produziert. In diesem geht es nicht mehr um Jimmy Neutron selbst, sondern um einen seiner besten Freunde Max, der mit Jimmys Rakete auf einem fremden Planeten bruchgelandet ist.\n\nJames Isaac Neutron, genannt \"Jimmy\", und von seinem Vater \"Jimbo\", ist ein hochintelligenter Junge mit einem großen Kopf im Alter von elf Jahren, der stets die erstaunlichsten Experimente und Erfindungen zustande bringt. Jedoch geht bei den Erfindungen von Jimmy öfter etwas schief, woraufhin er wieder einmal die Welt oder seine Stadt Retroville retten muss. \nDabei ist er sich seines gewaltigen IQs nur zu sehr bewusst und glänzt nicht oft durch Bescheidenheit. Im Allgemeinen ist er sehr rechtschaffen, freundlich und naiv, manchmal tritt er auch egozentrisch, verstockt sowie hitzig auf. In der Schule ist Jimmy nicht sehr beliebt, gilt teilweise als Langweiler (bis man ängstlich seine Hilfe benötigt) und generell als recht ‚uncool‘. Auf den Spott und die Hänseleien seiner Mitschüler reagiert er oft verletzt, kann aber auch ihr offensichtliches Desinteresse an seinen ellenlangen Vorträgen meistens nicht erkennen. Leider wird seine Leidenschaft für die Wissenschaft auch in seinem übrigen Umfeld nur zu oft mit Unverständnis und Langeweile konfrontiert, was ihn einerseits frustrieren kann, andererseits auch sein Ego nur weiter anschwellen lässt, fühlt er sich doch in der Lage, alles und jeden als intellektuell unterlegen anzusehen, mit Ausnahme natürlich seiner großen Idole, allesamt große Männer (oder Frauen im Fall von Marie Curie) der Wissenschaft, denen er ehrliche Bewunderung entgegenbringt.\nEr lässt sich leicht zu lauten Debatten hinreißen, ist von seinen Erfindungen stets überzeugt, blitzgescheit, eifersüchtig, eine wahrhaft fantasielose Niete in allen künstlerischen und sportlichen Bereichen, sieht sich als Anführer seiner Freundesgruppe (meist mit Recht), hat viele Feinde und ist trotz seiner Genialität ein ganz gewöhnlicher Junge mit einem harmonischen Familienleben. \nSeine besten Freunde sind Karl Keucher, Max Estevez, beide deutlich weniger schlau als er und äußerst exzentrisch, aber sehr treu (zumindest meistens), und sein Roboterhund Robbie, den er selbst geschaffen hat. Diese unterstützen ihn bei seinen Rettungsaktionen und sonstigen Abenteuern, wenn auch nicht immer sehr erfolgreich. Jimmys Erzfeindin hingegen ist Cindy Vortex, eine Klassenkameradin, mit der er sich unablässig in den Haaren liegt, zu jeder Zeit und bei jeder Gelegenheit. Ihre Streitereien sind beinahe legendär und fragt man einen Retroviller: „Sind die beiden immer so?“, wird man wohl die Antwort erhalten: „Ja, aber tief in ihrem Inneren hassen sie sich wirklich.“ („Das Ei-perium schlägt zurück.“) Doch im Laufe der Serie wird klar, dass die beiden sich insgeheim sehr mögen und sogar ineinander verliebt sind. Beide streiten das jedoch ab.\n\nJimmys Eltern heißen Frank (engl. \"Hugh\") und Judy Neutron. Der etwas schrullige, aber liebenswerte Frank arbeitet bei einem örtlich ansässigen Automobilkonzern und ist verrückt nach Enten und Kuchen, seinen zwei großen Leidenschaften. Judy ist das handwerklich begabte Talent, die Herrin im Haus und bekannt für ihr köstliches Gebäck. Weiterhin ist Judy Neutron sehr hygienebewusst, beinahe besessen von Sauberkeit und intelligent. Frank Neutron hingegen wirkt naiv, verspielt und nicht allzu schlau, doch ist er sehr liebevoll und fürsorglich, wenn es um Jimmy geht. Seine Frau ist sich Franks geringerer Intelligenz durchaus bewusst, das Paar ist aber trotzdem sehr einträchtig und verliebt.\n\nObwohl lange Zeit angenommen wurde, dass Jimmy seinen Intellekt von seiner Mutter geerbt hätte, stellte sich in der Folge \"Das Krabbel-Genie\" heraus, dass das Genie-Gen in Franks Familie liegt. \n\nRobbie (engl. \"Goddard\"), ist Jimmys selbstgebauter Roboterhund. Er kann alles Mögliche, was normale Hunde nicht können, wie Videos von verschiedenen Situationen aufnehmen und fliegen. Was immer gerade benötigt wird, er hat es dabei. (Insgesamt soll er über 11 Millionen Kunststücke beherrschen.) Wegen eines Programmierfehlers von Jimmy explodiert Robbie jedes Mal bei dem Befehl „Stell dich tot!“, setzt sich danach aber selbstständig wieder zusammen. Als Nahrung nimmt Robbie Aluminium-Dosen zu sich; er hinterlässt Schrauben-Häufchen, die Frank dann wegmachen darf.\n\nAuch wirkt er oft vernünftiger als sein Herrchen, genießt das gute Leben, wenn er es kann, und ist seinem Erfinder gegenüber der verlässlichste und langjährigste Freund. In einer Episode hat er sogar eine kurze Romanze mit einer außerirdischen Hündin namens Roxy, einer grünen Dame mit drei lila Schwänzen.\n\nKarlton Ulysses „Karl“ Keucher (engl. \"Carl Wheezer\") ist ein eher unbeholfener und übergewichtiger Junge mit Asthma, der allerdings in einigen Episoden auch plötzlich sehr undankbar und machtbesessen werden kann. Er ist ebenfalls zehn Jahre alt, Jimmys offiziell bester Freund, bekannt dafür beinah alles verdauen zu können, hat ein Benehmen, das teilweise weit unter seinem Alter liegt, schreit nach seiner Mutter (oder Jimmys) wenn es brenzlig wird und seine Leidenschaft sind Lamas, für die er alle seine Ängste bekämpfen kann. Wie alle Mitglieder seiner Familie ist er ein Hypochonder. Karl ist heimlich in Jimmys Mutter Judy Neutron verliebt und trägt meistens ein bearbeitetes Foto von sich und ihr mit sich herum. Es kommt zu einigen unabsichtlichen Versprechern seinerseits, die seine merkwürdige Zuneigung verraten, diese werden aber meist nur mit einem konsternierten Blick von Jimmys und Max’ Seite erwidert. Trotz der Tatsache, dass Karl so ziemlich vor allem und jedem fürchterliche Angst hat (bzw. allergisch ist), probiert Jimmy seine anderen Erfindungen meistens an ihm aus.\n\nAuch liebt er es zu singen, ohne zu merken, dass seine Lieder im höchsten Grade nervtötend sind.\nIn der Folge \"Die schwedische Brieffreundin\" erhält Karl seine erste Freundin, Elke Elkberg, eine hellblonde Schönheit, sowie seinen ersten Kuss, auf die Beziehung wird aber nie wieder eingegangen und Karl kehrt zu seiner Schwärmerei für Jimmys Mutter zurück.\n\nMaximiliano „Max“ Guevara Estevez (engl. \"Sheen Juarrera Estevez\" – eine Anspielung auf den Schauspieler Charlie Sheen, bürgerlich \"Carlos Irwin Estévez\"), genannt Max, mit mexikanischen Wurzeln, zählt zu Jimmys besten Freunden. Er ist zwölf Jahre alt und in derselben Klasse wie Jimmy und Karl, weil er zweimal in der Schule sitzen geblieben ist und der absolute ‚Spinner‘ der Klasse. Hyperaktiv, nach Süßigkeiten süchtig, mit einer Stimme, die nur Monster zum Einschlafen bringt, hält er sich für einen wirklichen männlichen ‚hombre‘ (Mann) und mit einer unerklärlichen Liebe für elektrische Schläge verehrt Max den Comicsuperhelden „UltraLord“ bis zum bitteren Ende und besitzt alle Actionfiguren UltraLords, jede Sonderedition, sogar die mit falschem Sprachchip. Dank seiner nervtötenden Art und seiner Tendenz alles zu ruinieren, fällt er seinen übrigen Freunden oft auf die Nerven, ist aber im Grunde eine gutmütige Seele, auf den viele der Lacher der Show zurückgehen. \n\nAufgrund eines missglückten Experimentes von Jimmy ist Max in Libby verliebt, doch auch weit nach der Korrektur des Experiments stehen sich die beiden sehr nahe und sind, anfangs insgeheim, ineinander verliebt. Die Zuneigung zwischen den beiden ist jedoch ziemlich entspannt, natürlich und entwickelt sich problemlos, bis Libby schließlich in der Folge \"Der Auserwählte\" tatsächlich seine feste Freundin wird.\n\nMax’ Mutter wird nie in der Serie gezeigt und es ist sehr wahrscheinlich, dass sie nicht mehr am Leben ist.\n\nCynthia Aurora „Cindy“ Vortex spielt das Gegenstück zu Jimmy. Sie ist blond, intelligenter als die große Mehrheit der Retroviller, athletisch, künstlerisch sehr begabt, laut, geldgierig, launisch, ein wahres ‚Großmaul‘, manchmal etwas versnobt und mit großen Erwartungen an sich selbst, für die sie auch andere behindert um selber einen Vorteil zu erringen. Ein Blick in ihr rosarotes Schlafzimmer genügt allerdings um zu ahnen, dass hinter dieser taffen Fassade auch ein sensibleres, ja, romantisches Mädchen steckt. Ihr Stolz und ihre Sturheit kommen ihr jedoch stets in die Quere, wenn sie versucht sich anderen gegenüber zu öffnen; sie hasst es, eine Schwäche einzugestehen oder gar ihrem Erzfeind recht zu geben.\n\nJimmy ist ihr Lieblingsziel für Hänseleien und Streiche, und besonders am Anfang der Serie setzt sie ihm schwer zu. Ihr Hass auf ihn begründet sich in der Tatsache, dass er ihr durch seinen Umzug in die Stadt das Rampenlicht als schlauestes Kind in Retroville gestohlen hat, und außerdem ist \"niemand\" in irgendeiner Hinsicht besser als Cindy Vortex. Sie liebt es ihn zu sabotieren, ihm Namen wie ‚Blödtron‘ oder ‚Elefantenschädel‘ zu geben und auch den schönsten Moment noch in letzter Minute zu verderben. Spätestens ab der 2. Staffel wird es aber offensichtlich, dass ihre ganzen Spötteleien noch einen anderen Grund zu haben scheinen. Es kommt immer wieder zu kleinen, seltsamen Momenten zwischen den beiden, die durchblicken lassen, dass sie sich hinter dem ganzen Streit insgeheim sehr, \"sehr\" gernhaben – eine Tatsache, die nicht nur beide vollkommen bestreiten, aber mit der sich auch keiner von beiden auseinandersetzen möchte, obwohl es dreimal beinahe zum Kuss der beiden kommt und Cindy öfters starke Eifersucht zur Schau stellt, wann immer Jimmy Interesse an einem anderen Mädchen zeigt. Doch als die zwei auf einer einsamen Insel stranden, öffnen sie sich mehr als je zuvor, geben zu, dass ihre Streitereien eigentlich sinnlos sind, und Cindy äußert sogar eine gewisse Trauer, als beide gerettet werden. Obwohl nach dieser Episode das Streiten wieder anfängt, verändert sich Cindys Verhalten langsam und sie scheint sich mit ihren Gefühlen stetig abzufinden, quält Jimmy und seine Freunde nicht mehr ganz so stark wie zuvor. Sie versucht sogar (mit Erfolg) Jimmys Aufmerksamkeit zu erregen, durch andere Kleidung, Parfüm und Make-up und er gibt zu, dass sie ihn ‚ablenkt‘. In den letzten Folgen der Serie ist die Zuneigung zwischen den beiden, trotzdem keiner von ihnen etwas zuzugeben bereit ist, frei offensichtlich. Zweimal (unter extremen Umständen) will sie ihm sogar alles gestehen, fällt jedoch vorher in Ohnmacht oder verhilft Jimmy zu einem Einfall, der sie daraufhin unterbricht (jedoch einen Kuss auf die Wange gibt). In \"Opfer des Gehirnwurms\" schließlich, überrascht Jimmy sie am Ende, als er sie auf dem Nachhauseweg plötzlich bei den Händen nimmt und küsst.\n\nCindys beste Freundin ist Libby, mit der sie eine augenscheinlich tiefe Freundschaft verbindet. In der ersten Staffel, schwärmte sie noch ein wenig für den coolen Nick, was sich aber bald gegeben hat. Über ihre Familie ist nicht viel bekannt, doch ihre Mutter ist oft sehr anmaßend und versnobt, ihr Vater zeigt sich nicht oft an ihrer Seite. \n\nLiberty Danielle „Libby“ Folfax ist die beste Freundin von Cindy Vortex und sehr an Mode, moderner Musik und Lifestyle interessiert. Sie kennt praktisch jedes Lied und jede Band, die es je gegeben hat. Aus einer Folge ging hervor, dass sie von einem ägyptischen Herrschergeschlecht abstammt. Sie ist in gewisser Weise das Gewissen von Cindy, wenn Cindy mal wieder maßlos übertreibt und auch die einzige, die von ihren Gefühlen zu Jimmy weiß, obwohl sie es sich schon selbst zusammenreimen musste. Trotz ihrer entspannten Art, kann sie auch recht furchterregend werden, wenn sie zu viel ‚Macht‘ erlangt. Anfangs verbringt sie nur aufgrund von Cindys Feindseligkeit gegenüber Jimmy Zeit mit den Jungs, später, als auch ihre Freundschaft zu Max tiefer wird, gehören sie und Cindy fest zum Inventar bei den Abenteuern. Sie beginnt eine Beziehung mit Max und ist auch ansonsten in ihn verliebt. Sie ist elf Jahre alt, wird aber in einer Episode zwölf.\n\nKönig Goobot ist ein böser Außerirdischer und König der Yokianer. Er wirft ungebetene Gäste einem Hühner-Dämon namens Poultra zum Essen vor. Sein erster Minister ist sein etwas zurückgebliebener Bruder Ooblar.\n\nNick Dean ist das coolste Kind an der Schule. In Wahrheit ist er aber sehr ängstlich; als er das Hühnchen Poultra das erste Mal sieht, rennt er schreiend davon. Er fährt immer Skateboard und spielt manchmal den DJ. Alle Schülerinnen sind besessen von ihm, wie auch er selbst. Bevor sich Cindy in Jimmy verliebte, schwärmte sie für ihn.\n\nBetty ist eine Mitschülerin von Jimmy und war anfangs seine große Liebe. Sie tritt stets sehr freundlich, mitfühlend und ehrlich auf. Sie ist zudem ein sehr liebenswertes Mädchen. Wenn Jimmy sie sah oder sie ihn nur ansprach, konnte er keinen klaren Gedanken mehr fassen und war wie weggetreten. Obwohl er kleiner ist als sie, schien sie doch ehrliche Gefühle für ihn zu besitzen, wenn auch nicht so tiefe, dass es sie betrübt hätte, als sie das Feld räumte. Denn nach einer heftigen Eifersuchtsattacke von Cindy, die seit Bettys erstem Erscheinen großen Groll gegen sie hegte, versprach sie ihr lächelnd, dass er ‚ihr gehöre‘.\n\nCalamitous ist ein böser Wissenschaftler und, neben Goobot, Jimmys Erzfeind. Er ist bekannt dafür, dass er nie etwas fertig bekommt und deswegen Jimmy braucht, der Calamitous' Arbeit vollenden soll. Er ist meistens in einem gewaltigen Kampfroboter zu sehen, da er zu schwach ist, um Gegner mit eigener Faust besiegen zu können. Er war nebenbei auch ein Schüler von Miss Foul. Zudem hat er eine erwachsene Tochter namens „Graziella Wunderschön“.\n\nJustus Stritch ist der reichste Junge aus Retroville und selbst eigentlich kein Genie. Aber wegen seines Reichtums kann er jederzeit ein Forscherteam anheuern, das für ihn erfindet. Sein einziges Ziel ist es Jimmy bloßzustellen. Ironischerweise versteht sich Frank gut mit Justus' Vater.\n\nMiss Foul ist Jimmys, Cindys, Carls, Max, Libbys, Nicks und Bettys Klassenlehrerin. Sie unterrichtet Jimmys Klasse mit Herz und mag alle ihre Schüler (bis auf Max), und Jimmy und Cindy sind ihre Lieblingsschüler. Sie ist einem Huhn nachempfunden und wird unabsichtlich von Jimmys Erfindungen häufig in Gefahr gebracht.\n\nDirektor Willoughby ist der Direktor der Schule. Er ist energisch und enthusiastisch. Er hat eine Schwester namens Eunice, die in der Folge \"Jimmy neuer Job\" erwähnt wird.\n\nSam ist der Besitzer von Candy Bar. Er ist knurrig und cholerisch. Er spricht oft Sätze, die er mit Ja endet. Er hegt eine Beziehung zu Ms. Foul.\n\nJet Fusion ist ein Actionfilmstar und das einzige Idol von Jimmy, das kein Wissenschaftler ist. Er arbeitet insgeheim an einer Organisation für die Rettung der Welt und ist dort als „Agent X“ bekannt. Er geht zusammen mit Jimmy, Max und Carl gegen Professor Calamitous vor und entwickelt eine Hassliebe zu Calamitous' Tochter Graziella. Er konnte jedoch Calamitous zusammen mit Jimmy besiegen. Zum Leidwesen von Jet wird Graziella mit ihrem Vater abgeführt. In der Episode „Wenn Spione heiraten“ wird Graziella jedoch freigelassen, die beiden werden ein Paar und heiraten schließlich.\n\nBowlbee ist ein Mitschüler von Jimmy und kommt im Laufe der Serie auf die Schule. Er scheint nicht wirklich klug zu sein und spricht häufig in der dritten Person von sich. Dank der Stimme von Karl wird er der neue Klassensprecher. In der Episode „Macbath im All“ wird jedoch deutlich, dass sein unintelligentes Erscheinungsbild nur Fassade ist, da er in perfekter Grammatik und tiefer Stimme vorspricht. In der Trilogie „Ein gefährliches Spiel“ wird er zufällig mit Jimmy, Karl, Max, Robbie, Libby und Cindy ins Weltall gesaugt. Er gehört zu den Lieblingsopfern vom Schulschläger Butch. Sein Name „Bowlbee“ enthält „Bowl“ und spielt somit auf seine rundliche Form an.\n\nButch ist der Schulschläger und sehr aggressiv. Er verprügelt meistens ohne Grund einige Schüler, darunter Karl, Bowlbee und Oleander.\n\nOleander ist ein weiterer Mitschüler von Jimmy. Über ihn ist nahezu nichts bekannt, außer dass er hin und wieder von Butch verprügelt wird. Außerdem verteilt er Informationen und schuldet vielen noch Geld.\n\nBritney ist eine Mitschülerin von Jimmy. Ihre besten Freunde sind Cindy und Libby. Sie hat blonde Haare, die mit zwei Zöpfen zueinander gebunden sind und eine Zahnspange.\n\nKäpt'n Betty ist ein alter Fischer. Er ist häufig zu sehen, spielt aber in der Folge \"Das Seemonster\" eine große Rolle. Er kann seinen Arm abnehmen. Er hilft Jimmy, Max und Carl das Seeungeheuer zu finden. Nachdem sie es gefunden haben, wird er vom Seeungeheuer gefressen, später aber wieder ausgespuckt.\n\nGrandma Taters ist eine ältere Frau, welches in Wahrheit ein Alien ist. Ihr einziges Ziel ist es, die Bürger von Retroville zu hypnotisieren, um aus ihnen glückliche Zombies zu verwandeln. Jimmy und Cindy konnten sie jedoch aufhalten.\n\nBrobot ist ein von Jimmy gebauter Roboter-Bruder. Brobot treibt Jimmy in den Wahnsinn, weswegen Jimmy zwei Roboter-Eltern baut. Brobot brauchte Jimmys Hilfe, als seine Eltern vom Müllmann entführt wurden. Jimmy denkt, dass das ein Scherz von Brobot war, aber Brobot hatte recht. Brobot lenkte den Müllmann ab, damit Jimmy und seine Freunde seine Eltern befreien können.\n\nVOX ist ein von Jimmys installierter Computer. Sie gibt Jimmy Informationen über ihre Erfindungen.\n\nMr. Estevez ist Maxs Vater. Er kann gut Klimaanlagen reparieren. In der Folge \"Mein Vater, der Superheld\" ist er traurig, weil sein Sohn mehr Zeit mit Ultralord als mit ihm verbringt. Er bittet Jimmy um Hilfe, damit er auch ein Superheld sein kann. Er möchte seinen Sohn glücklich machen.\n\nMrs. Vortex ist Cindys Mutter. Ihr richtiger Name lautet Sascha Vortex. Sie wurde in Frankreich geboren und liebt französisches Essen. Sie ist mit den anderen Eltern von den Kindern gut befreundet.\n\nHumprey ist Cindys Hund. Laut Cindy, ist er der beste Hund.\n\nMüllmann ist ein stinkiger Außerirdischer, der überall Müll auf der ganzen Galaxie sammelt. Er hat einen Hund namens Roxy. Er entführte Brobots Eltern um aus ihnen Frisbees zu machen.\n\nSkeet ist ein Mitarbeiter von McSpankys und ist für seine geringe Intelligenz und seinen Arbeitseifer bekannt. Er ist freundlich gegenüber Carl und Max, jedoch nicht gegenüber Jimmy, den er für ein schlichtes Gemüt hält.\n\nSchauplatz der Serie ist die fiktionale Stadt ‚Retroville‘, deren Name auf die Mehrheit der als eher dümmlichen dargestellten Einwohner anspielen mag. Aus verschiedenen Folgen, bei denen sich die Charaktere auf Weltreisen befinden, ging hervor, dass sich dieses Retroville in den USA befindet, genauer gesagt in Texas, wie es von Liebhabern der Show als allgemein gültig anerkannt wird. Obwohl es zahlreiche Beweise für diese Theorie gibt, lassen sich ebenso einige Instanzen während der Serie feststellen, an denen Zweifel auftreten. So sagt die Figur Carl Wheezer (deutsch: Karl Keucher) in der Folge \"Invasion der Hosen\", dass er einen Onkel habe, der in Texas wohnt. Jimmy Neutron selbst erklärt in einer weitaus späteren Folge (\"Der Hollywood-Betrug\"), dass sein Film unter anderem in Texas spiele, Retroville jedoch keinem der Handlungsorte gleiche. \n\nWichtige Orte in Retroville, die während der Show oft eine Rolle spielen sind die Lindbergh Schule, der Schulhof besagter Schule, der Vergnügungspark ‚Retroland‘, die ‚Candy Bar‘, der Park, Jimmys Haus und Nachbarschaft, sowie eine Halle in der Stadt, die der Bürgermeister nutzt um öffentliche Diskussionen zu führen, bzw. Ankündigungen zu machen. \n\nDieses Etablissement, in rosa-roten und grünen Tönen gehalten mit einem riesigen Plasterlutscher auf dem Dach, welches von einem eher barschen Mann namens Sam geführt wird, ist der Anlaufpunkt Nummer Eins für alle Kinder und Jugendlichen der Stadt. Wie es der Name bereits suggeriert, bietet die Candy Bar zahlreiche Sorten von Bonbons, Süßigkeiten und Eisbechern an, die von gutem Geschmack zu sein scheinen. Die Qualität dieser Speisen lässt allerdings etwas zu wünschen übrig: in der Folge \"Opfer des Gehirnwurms\" nämlich wird enthüllt, dass die Hälfte aller Schokoladenstreuseln in Wirklichkeit aus Ameisen besteht. Trotz solcher Kommentare und Hinweise erfreut sich die Candy Bar höchster Beliebtheit und wird immer wieder von den Hauptcharakteren besucht, wenn sie Verträge aushandeln, einen Anlass zum Feiern haben oder einfach mal eine Pause machen wollen.\n\nJimmys Labor ist unterirdisch und befindet sich unter seinem Clubhaus im Garten, einer kleinen Holzhütte. Es gibt mehrere Wege hinein, die in der Serie aufgezeigt werden; einer führt direkt von Jimmys Kinderzimmer dorthin, doch dieser wird nur einmal benutzt, ist er schließlich noch nicht ganz ausgereift. Die herkömmlichste Art und Weise sich Zutritt zu verschaffen, ist durch die Tür des Clubhauses, die von vielen von Jimmys Erfindungen geschützt wird („Wenn du das Sicherheitssystem überlebst, bring mir die Fernbedienung“ – Jimmy Neutron in \"Maternotron\"), unter anderem einem Anti-Mädchen-Alarm. Ein DNA-Scanner mit der Computerstimme Vorx (die die künstliche Intelligenz von Jimmys Computer und Labor ist), der nur auf eine Probe von Jimmys Haaren reagiert, verschärft die Sicherheit zusätzlich. Jimmy selbst hält das Wissen um den Eintritt in sein Labor streng geheim, eingeweiht sind außer ihm allein seine beiden besten Freunde. Nicht einmal seine Eltern sind sich bewusst, \"was\" genau sich da unter ihrem Garten befindet.\n\nDas Labor ist Jimmys Lieblingsort zu Hause. Er verbringt dort die meiste Zeit seiner Freizeit (wenn nicht gerade etwas schiefgelaufen ist und die Welt/Stadt gerettet werden muss), manchmal sogar Nächte. Am Anfang der Serie wurde Mädchen der Zutritt vehement verweigert, es sei denn es war unbedingt nötig, doch diese strikte Regel scheint sich im Laufe der Zeit stückchenweise aufzuheben. In der Folge \"Liga des Bösen\" werden die Mädchen Libby und Cindy sogar vom Erfinder selbst eingeladen, jedoch unter der Bedingung, dass sie sich nicht weiter als bis zu der gelb markierten Linie wagen.\n\nDie Serie wurde von 2002 bis 2004 von DNA Productions und O Entertainment unter der Regie von Keith Alcorn, Mike Gasaway und Kirby Atkins produziert. Die Musik komponierte Charlie Brissette. Die Erstausstrahlung erfolgte in drei Staffeln vom 20. Juli 2002 bis zum 25. November 2006 durch Nickelodeon in den USA. \n\nDie erste Staffel wurde in Deutschland ab dem 9. November 2002 von Super RTL ausgestrahlt. Die anderen Staffeln folgten später bei Super RTL und Nick / Nickelodeon. \"Jimmy Neutron\" wurde unter anderem auch ins Japanische, Niederländische und Französische übersetzt.\n\nTurbine Media bestätigte auf Facebook, dass man die Serie 2014 auf DVD veröffentlichen möchte. Die Serie erschien am 5. Dezember 2014 auf DVD.\n\nBisher wurden acht Fernsehfilme zur Serie produziert. \nDas Serien-Crossover „The Jimmy Timmy Power Hour“ umfasst 3 Teile. In diesen Sonderfolgen geht es um Jimmy Neutron aus der Serie \"Jimmy Neutron\" und Timmy Turner aus der Serie \"Cosmo und Wanda – Wenn Elfen helfen\".\n\n→ Siehe Hauptartikel: Jimmy Neutron vs. Timmy Turner\n\nIm Laufe der Jahre erhielt die Serie fünf Fernseh- und Zuschauerpreise:\n\nZudem wurde sie noch für sieben Preise nominiert:\n\nNach dem Guinness-Buch der Rekorde war die Serie die zehnt-beliebteste des Jahres 2006.\n\n"}
{"id": "599503", "url": "https://de.wikipedia.org/wiki?curid=599503", "title": "Mikrocomputer", "text": "Mikrocomputer\n\nMikrocomputer (Mikrorechner) sind Computer, die kompakter als Großrechner und Minirechner sind und von einer einzelnen Person bedient werden können. Der Begriff ist heute meistens assoziiert mit der ersten Welle der 8-Bit-Heim- und Bürocomputer.\n\nIn einer engeren Definition verstand man darunter Computer einer bestimmten Leistungsklasse, die weniger leistungsfähig als Großrechner und Minirechner waren und lediglich einen einzigen Mikroprozessor als Zentraleinheit aufwiesen. Sie wurden millionenfach als Heim- und Bürocomputer benutzt. Die Ausstattung war auf diese Anwendungen zugeschnitten.\n\nDer Begriff Workstation bezeichnete vielseitig einsetzbare Mikrocomputer für den professionellen Einsatz, die meist wartungsfreundlicher, zuverlässiger und deutlich leistungsfähiger als Heim- oder Bürocomputer waren. Sie wurden vor allem im technisch-wissenschaftlichen Bereich verwendet und waren in der Regel permanent in ein Netzwerk eingebunden.\n\nIn der Alltagssprache hat der Gebrauch des Begriffs \"Mikrocomputer\" seit den 1980er-Jahren deutlich abgenommen und ist kaum noch üblich, obwohl ein immer größerer Kreis von modernen, mikroprozessorgesteuerten Geräten der Definition eines Mikrocomputers entspricht. Er ist daher bei Tischgeräten weitgehend durch den Begriff \"Personal Computer\" (PC) ersetzt worden. Bei anderen Bauformen, wie z. B. eingebetteten Systemen, die sich inzwischen durch einen erhöhten Integrationsgrad bei den Komponenten und einer entsprechend kompakten Ausführung auszeichnen, behält der ursprüngliche Begriff jedoch seine Gültigkeit.\n\n"}
{"id": "599553", "url": "https://de.wikipedia.org/wiki?curid=599553", "title": "Integer (Datentyp)", "text": "Integer (Datentyp)\n\nMit Integer ([], [], für ganze Zahl; von ) wird in der Informatik ein Datentyp bezeichnet, der ganzzahlige Werte speichert. Der Wertebereich ist endlich. Berechnungen mit Integern sind in der Regel exakt. Lediglich ein Überlauf kann durch Überschreiten des zulässigen Wertebereichs auftreten. Als grundlegender arithmetischer Datentyp sind Integer in der Hardware fast aller Rechenanlagen vorhanden und in nahezu jeder Programmiersprache verfügbar. Meist werden sogar mehrere Integerarten bereitgestellt, die sich in der Darstellung, der Länge oder dem Vorhandensein eines Vorzeichens unterscheiden. Die implementierte Arithmetik mit Integern ist bisher nicht genormt und weist oft sprachabhängige (Java, C) oder sogar compilerabhängige (C – Reihenfolge der Auswertung von Ausdrücken) Eigenheiten auf. Ein Normungsversuch liegt mit der “Language Independent Arithmetic” (ISO/IEC 10967) vor.\n\nVon exotischen Darstellungen abgesehen, gibt es drei Möglichkeiten zur Speicherung von Integer-Variablen. Das Vorzeichen – soweit vorhanden – kann man in allen Darstellungen an einer bestimmten Ziffer ablesen.\n\nIn der Betrags-Vorzeichendarstellung werden das Vorzeichen und der Betrag getrennt gespeichert und verarbeitet.\n\nBei \"b\"-Komplementzahlen (Zweierkomplementzahlen) wird genau die halbe Teilmenge der Zahlen mit großem Betrag als negative Zahlen interpretiert, ohne dass die Arithmetik positiver Zahlen wesentlich geändert wird. Das führt zu einfachen Schaltungen und zu einer einfachen Regel für Vorzeichenänderungen (ziffernweises \"b\"-Komplement und anschließende Erhöhung der Zahl). Zwischen der Arithmetik mit Zweierkomplementzahlen und rein positiven Binärzahlen besteht kein Unterschied. Es existiert eine bijektive Abbildung zwischen den Darstellungen und den Werten (kein Wert hat zwei Darstellungen). Man kann \"b\"-Komplementzahlen wie technische Zähler (Kilometerzähler im Auto) interpretieren. Nachteil von \"b\"-Komplementzahlen ist, dass der kleinste negative Wert kein positives Gegenstück in der Darstellung hat.\n\nBei \"(b−1)\"-Komplementzahlen (Einerkomplementzahlen) vereinfacht man dagegen die Regel für Vorzeichenänderungen (die anschließende Erhöhung fällt weg) und muss dafür in der Arithmetik mehr Fallunterscheidungen und vor allem zwei Darstellungen der Null (±0) berücksichtigen.\n\nIn modernen Rechenanlagen ist die Basis \"b\" praktisch ausnahmslos \"b = 2\" und die Darstellung im Zweierkomplement hat sich weitgehend durchgesetzt.\n\nDezimale Äquivalente zum Zweier- und Einerkomplement im Binärsystem wären Zehner- und Neunerkomplementzahlen.\n\nVon manchen Herstellern wird oft unter Berufung auf Kundenwünsche (Banken) noch ein Dezimalformat gepflegt. Hier wird fast ausnahmslos eine Betrags-Vorzeichendarstellung gewählt und der Betrag in der sogenannten BCD-Form (\"binary coded decimal\") gespeichert. Der Wunsch ist begründet, da bei der Umwandelung einer Dezimalzahl in eine Integerzahl oder zurück Rundungsfehler auftreten, die eine exakte kaufmännische Buchhaltung unmöglich macht.\n\n(die Beispielzahlen sind für 9 Bit ausgelegt, da so zweistellige BCD-Zahlen möglich sind, Darstellung MSB → LSB):\nEin Integer besteht in der Regel aus 8, 16, 32, 64 oder 128 Bits (also 1, 2, 4, 8 oder 16 Bytes) – entsprechend der Wortbreite der jeweiligen CPU. Historisch wurden auch andere Werte (12, 48, … Bit) verwendet. In Programmiersprachen sind die Bezeichnungen dieser Zahlen teilweise genormt: In Java werden sie als codice_1 (8), codice_2 (16), codice_3 (32) und codice_4 (64 Bit) bezeichnet. In C gibt es dieselben Bezeichnungen, jedoch sind die Größen architekturabhängig, mit C99 wurden architekturunabhängige Typen definiert (stdint.h). Dafür unterstützt C auch die vorzeichenlose (codice_5) Variante, die von den meisten Mikrocontrollern und Mikroprozessoren in Hardware unterstützt werden.\n\nRechenanlagen verarbeiten Integer meist schneller als Gleitkommazahlen, da oft weniger Bits zu verarbeiten sind (die kleinste IEEE-754-Gleitkommazahl hat 32 Bit) und die Verarbeitung des Exponenten entfällt, was Rechenzeit und Speicherplatz einspart. Zusätzlich bietet eine reine Festkommaarithmetik (Integer basierend) gegenüber Gleitkommaarithmetik den Vorteil der exakten Verarbeitung (innerhalb fester Dynamikgrenzen), datenabhängige Effekte wie Denormalisierung oder Absorption treten nicht auf. Für interne Software von Geldinstituten wird deswegen häufig reine Integerverarbeitung gefordert, wie es z. B. bei GnuCash realisiert ist.\n\nBei der Ablage im Speicher taucht neben der Notwendigkeit, die Bits der Zahlendarstellung überhaupt abzulegen, noch das Problem der Bytereihenfolge und Anordnung auf.\n\nWird einer Integer-Variable ein Wert außerhalb ihres Wertebereiches zugewiesen, führt dies zu einem arithmetischen Überlauf. So wird z. B. bei einer vorzeichenlosen 8-Bit-Integer-Variablen aus 255+1 der Wert 0; bei einer vorzeichenbehafteten im Zweierkomplement hingegen aus 127+1 der Wert −128.\n\n\n"}
{"id": "599946", "url": "https://de.wikipedia.org/wiki?curid=599946", "title": "Yggdrasil Linux", "text": "Yggdrasil Linux\n\nYggdrasil Linux war eine der ersten Linux-Distributionen. Sie wurde von Adam J. Richters Firma Yggdrasil entwickelt und wird als die erste kommerzielle Linux-Distribution für einen breiten Anwenderkreis angesehen.\n\n\"Yggdrasil Linux\" wurde ab November 1992 auf CD-ROM ausgeliefert. Sie konnte sogar als Live-CD gestartet werden. In damaligen Printausgaben von HOWTOs wurde Yggdrasil Linux häufig als „Plug-and-Play-Linux“ aufgeführt, da es über eine heute selbstverständliche Hardware-Erkennung verfügte.\n\nDie 1995 eingestellte Distribution ist immer noch erhältlich. Interessant könnte sie für Personen sein, die Support für alte Hardware benötigen oder generell Interesse an der geschichtlichen Entwicklung von Linux haben.\n\nNeben der Linux-Distribution verlegte das Unternehmen Yggdrasil einige der ersten Bücher zum Thema Linux-Kompilierung und trug maßgeblich zur Entwicklung des Dateisystems und des X Window Systems bei.\n\nAdam J. Richter spielte eine Schlüsselrolle in all diesen Entwicklungen und unterstützte eine Gruppe von kleinen Linux-Firmen und Enthusiasten. Derzeit bietet Richter Software Consulting an und arbeitet immer noch am Linux-Kernel. Außerdem besucht er häufig Linux-Veranstaltungen im nördlichen Kalifornien.\n\nDer Name Yggdrasil steht für die Weltenesche in der germanischen Mythologie.\n"}
{"id": "603146", "url": "https://de.wikipedia.org/wiki?curid=603146", "title": "SeaMonkey", "text": "SeaMonkey\n\nSeaMonkey (englisch für den Salinenkrebs) ist ein aus der Mozilla Application Suite hervorgegangenes freies Internet-Anwendungspaket, das aus Webbrowser, E-Mail-Programm, Chat-Client, HTML-Editor, Adressbuch und weiteren Hilfsprogrammen besteht.\n\nDie Module des Programm-Pakets benutzen wesentliche Programmteile gemeinsam. Dadurch wird weniger Speicherplatz belegt und ein Geschwindigkeitsvorteil erzielt. Im Gegensatz dazu bringt bei Mozillas populären Ablegern Firefox und Thunderbird jedes Programm seine eigenen Bibliotheken mit. Diese sind dann mehrfach im Speicher vorhanden.\n\nAls die Mozilla Foundation gegen Ende 2005 ankündigte, die Entwicklung ihrer \"Mozilla Application Suite\" und deren Unterstützung offiziell einzustellen, um sich künftig vor allem der Entwicklung ihres Webbrowsers Firefox und des E-Mail-Clients Thunderbird zu widmen, führte eine Gruppe Softwareentwickler die \"Mozilla Application Suite\" unter neuem Namen fort.\n\nAls offiziellen Namen erhielt ihr Projekt schließlich den ehemaligen Codenamen des Vorgängerprojekts, der ursprünglich vom Markennamen der fast gleichnamigen US-amerikanischen Salinenkrebs-Zuchtlinie \"Sea-Monkey\" (Artemia nyos) stammt. Am 30. Januar 2006 wurde die erste offizielle Version, 1.0, veröffentlicht.\n\nSeaMonkey 1.x bot durch seine Roaming-Funktion die Möglichkeit, Kennwörter, Lesezeichen, Cookies und andere Benutzerdaten auf einem zentralen Server zu speichern, um sie von verschiedenen Orten aus zu nutzen. Diese Funktion fiel mit SeaMonkey 2.0 weg und wurde durch die Erweiterung Firefox Sync ersetzt. Sync wurde ab SeaMonkey 2.1 direkt integriert. Die parallele Nutzung von Firefox Sync durch SeaMonkey ist seit der mit Firefox 29 eingeführten neuen Sync-Variante nicht mehr möglich.\n\nSeaMonkey als Produkt wird zwar parallel zu Firefox und Thunderbird entwickelt, teilt sich im Hintergrund jedoch den meisten Code mit diesen unter Bewahrung des eigenen Look and Feel. Auch die Entwicklungszyklen sind größtenteils mit den beiden offiziellen Produkten synchronisiert.\n\nManche Erweiterungen für Firefox und Thunderbird sind nicht kompatibel mit SeaMonkey, können aber konvertiert werden.\n\nSeaMonkey ist auch als portable Version mit der Bezeichnung „SeaMonkey, Portable Edition“ erhältlich.\n\nNach einem Namensstreit mit dem Debian-Projekt über die Nutzungsbedingungen für die Warenzeichen von Mozilla bekam das mit Debian mitgelieferte SeaMonkey-Paket den Namen \"Iceape\". Andere Mozilla-Programme erhielten ebenfalls neue Namen, die alle mit \"Ice\" beginnen.\n\nDas Projekt \"SeaMonkey\" wurde ins Leben gerufen, nachdem Mozilla verkündet hatte, die Entwicklung der \"Mozilla Application Suite\" nicht mehr fortzuführen, deren letzte Version, die \"Mozilla Application Suite\" 1.7.13, am 19. April 2006 erschien.\n\n\n"}
{"id": "605597", "url": "https://de.wikipedia.org/wiki?curid=605597", "title": "Keynote (Software)", "text": "Keynote (Software)\n\nKeynote ist ein Präsentationsprogramm des Unternehmens Apple für die Betriebssysteme macOS und iOS. Es dient ähnlich wie PowerPoint des Unternehmens Microsoft oder Impress (aus Apache OpenOffice und LibreOffice) zur Erstellung von Präsentationen. Auf dem Mac ist es im Mac App Store erhältlich. Seit 2010 ist es für das iPad und seit 2011 auch für iPhone und iPod touch erhältlich. Die aktuellen Versionen wurden auf der WWDC 2016 vorgestellt und sind seit September 2016 verfügbar.\n\nDer Name für das Programm geht auf den Begriff Keynote zurück, welcher meist den Eröffnungsvortrag einer Veranstaltung bezeichnet. Speziell unter Nutzern von Apples Macintosh Computern wird unter „Keynote Address“ der Einführungsvortrag von Steve Jobs, dem früheren CEO des Unternehmens Apple, verstanden, den dieser jeweils jährlich auf der Macworld Expo bzw. der WWDC hielt.\n\nKeynote zeichnet sich durch eine einfache Bedienung aus. So werden zum Beispiel automatisch Hilfslinien eingeblendet, wenn man ein Objekt über die Folie zieht, was das exakte Platzieren auf gleicher Höhe oder in „harmonischen“ Abstandsverhältnissen beschleunigt.\n\nWährend es bei Keynote eine recht große Anzahl von Vorlagen gibt, waren die Animationsfunktionen in den ersten beiden Versionen noch recht einfach. Spätestens ab Version 4, Keynote ’08, hat sich das jedoch geändert. Keynote hat inzwischen eine neue Funktion \"Leuchttischansicht\" zum Anordnen der Folien, wie man sie bspw. von Powerpoint kennt. Mit den in Version 4 eingeführten „intelligenten Animationen“ lassen sich sogar anschauliche Diashows auf einer Folie unterbringen. Hinzugekommen ist auch die Möglichkeit, Objekte auf Pfaden (in Linien und Kurven) zu animieren.\n\nAb Version 2 gibt es die Funktion „Moderatormonitor“, der bei Vorträgen auf der Leinwand die momentane Folie zeigt, während am Computer des Vortragenden die momentane Folie, die nächste Folie, Notizen sowie eine Uhr (bzw. ein Countdown für die verbleibende Redezeit) zu sehen sind.\n\nKeynote kann PowerPoint-Dateien importieren (sowohl im PPT- als auch PPTX-Format) und exportiert in die Formate PowerPoint-Präsentation, PDF (Adobe Acrobat), HTML und QuickTime-Video. Mathematische Formeln können nur über andere Programme wie zum Beispiel TeXShop, MathType, LaTeXiT oder das bei macOS mitgelieferte Programm Grapher erstellt und in Keynote übertragen werden.\n\nSeit Keynote ’09 gibt es die Funktion, Bilder, Formen oder Texte, die in zwei aufeinander folgenden Folien enthalten sind, in einem Übergang automatisch animieren zu lassen (Magic Move). Außerdem gibt es ein Programm für iPhone und iPod touch, mit dem man Keynote steuern kann (Keynote Remote).\n\nMit der Einführung von Keynote 6.0 (auch als „Keynote '13“ bezeichnet) wurden zahlreiche Funktionen zugunsten einer besseren Kompatibilität mit der iOS-Version entfernt, was zu Problemen mit Präsentationen führte, die mit Vorgängerversionen erstellt wurden. Während sich Keynote '13 durch eine einfachere Bedienung auszeichnet, kritisierten professionelle Nutzer das Wegfallen von Funktionen und Kompatibilität. Einige Nutzer sind dazu übergegangen, Keynote '09 und Keynote '13 parallel zu installieren, um weiterhin mit älteren Präsentationen zu arbeiten ohne sie an die neue Version anpassen zu müssen.\n\nDer Hauptgrund für das Verschwinden der alten Funktionen war das Schaffen einer „gemeinsamen Basis“ für die Weiterentwicklung der Mac- und iOS-Version zusammen mit der ebenfalls 2013 vorgestellten Browserversion „Keynote for iCloud“. Apple hat mit den bisherigen Updates für Keynote '13 begonnen, nach und nach alte Funktionen wieder einzuführen, wobei diese auf allen Plattformen gleichermaßen implementiert wurden.\n\nSeit 2016 ist es möglich, mehrere Personen mit einem Link dazu einzuladen, gleichzeitig auf verschiedenen Geräten an einer Präsentation zu arbeiten. Diese Funktion befindet sich noch im Beta-Stadium (Stand Oktober 2016).\n\n\n"}
{"id": "607371", "url": "https://de.wikipedia.org/wiki?curid=607371", "title": "UltraEdit", "text": "UltraEdit\n\nUltraEdit ist ein Text- und Hex-Editor für Windows, Linux und Mac OS X.\n\nDas Programm wird von der in Hamilton (Ohio) ansässigen \"IDM Computer Solutions, Inc.\" entwickelt und als Shareware vertrieben.\n\nDie wichtigsten Funktionen von UltraEdit sind:\n\nIDM Computer Solutions, Inc. produziert UltraEdit und andere Software seit 1994. IDM konnte UltraEdit nach eigenen Angaben über 2 Millionen Mal lizenzieren.\n\n"}
{"id": "607863", "url": "https://de.wikipedia.org/wiki?curid=607863", "title": "Kismet (Sniffer)", "text": "Kismet (Sniffer)\n\nKismet ist ein freier passiver WLAN-Sniffer zum Aufspüren von Funknetzwerken.\n\nPassiv bedeutet, dass Kismet keine Anfragen an die Netzwerke sendet, sondern die Pakete, die von diesen geschickt werden, mitliest. Zu diesem Zweck wird die WLAN-Karte in den sog. „Monitormodus“ geschaltet. Dadurch können u. a. auch sogenannte „versteckte“ (E)SSIDs gefunden werden.\n\nEs unterliegt der GPL und läuft unter GNU/Linux, NetBSD, FreeBSD, OpenBSD, macOS und auch Windows. Bei letzterem benötigt man allerdings Cygwin.\nZusammen mit einem GPS-Empfänger und einem Skript, das Kismet beiliegt, können die gefundenen Netzwerke kartographiert werden. Kismet kann auch mit der ebenfalls freien Software GpsDrive kooperieren. Dabei erscheinen die Netzwerke in Echtzeit als Wegpunkte auf der von GpsDrive angezeigten Karte.\n\nKismet wird nicht nur beim Wardriving verwendet, sondern kann auch dazu dienen, sein eigenes WLAN auf Sicherheit zu überprüfen und die Signalstärke festzustellen. Darüber hinaus kann es auch als Wireless Intrusion Detection System (IDS) eingesetzt werden.\n\n\n"}
{"id": "608567", "url": "https://de.wikipedia.org/wiki?curid=608567", "title": "Robots (2005)", "text": "Robots (2005)\n\nRobots ist ein Computeranimationsfilm aus dem Jahr 2005. Bei dem von Blue Sky Studios produzierten Film führten Chris Wedge und Carlos Saldanha Regie.\n\nDas Roboter-Paar Herb und Lydia Copperbottom aus \"Rivet Town\" bekommt einen Sohn: Rodney Copperbottom. Wie bei Robotern üblich, wird der Nachwuchs in Form eines Bausatzes geliefert und dann von den Eltern individuell zusammengesetzt. Nach der erfolgreichen „Geburt“ von Rodney wächst er zu einem jungen Roboter heran. Das Großwerden geschieht bei Robotern durch den Austausch von Einzelteilen, da die Familie aber nicht viel Geld hat, bekommt Rodney statt neuer Bauteile die bereits getragenen Teile von der Verwandtschaft. \n\nRodney beschäftigt sich mit dem Tüfteln an Erfindungen. So erfindet er Wonderbot, einen Roboter der seinem Vater, der als Geschirrspüler im Restaurant von Mr. Gunk tätig ist, bei der Arbeit helfen soll. Rodneys großes Vorbild ist Bigweld, ein großer Erfinder und Gründer der Firma \"Bigweld Industries\". Im Fernsehen stellt sich dieser in der \"Bigweld-Show\" als Chef der Firma dar, wie er Roboter aus dem ganzen Land einlädt, um sich deren Erfindungen persönlich anzusehen. Rodney beschließt, zum Firmensitz nach \"Robot City\" zu fahren, um sein Idol persönlich kennenzulernen und ihm seine Erfindung Wonderbot vorzustellen. \n\nLeider entwickelt es sich dort nicht so, wie sich Rodney das vorgestellt hat. Er muss feststellen, dass Bigweld verschwunden ist. An seiner Stelle leitet nun Ratchet dessen Firma. Der ändert das Firmenmotto und stellt ab sofort keinerlei Ersatzteile mehr für die Roboter her, stattdessen nur noch teure Upgrades, um mehr Geld verdienen zu können. Restliche Ersatzteile und nicht mehr funktionierende Roboter werden in der ganzen Stadt von Kehrfahrzeugen eingesammelt und zur Roboterpresse gebracht und eingeschmolzen. Die Altmetallverwertung wird von Ratchets Mutter Madame Gasket geleitet.\n\nRodney stellt weitere Nachforschungen an, wobei er auf viele skurrile Figuren stößt und Freundschaften schließt. So trifft der bei seiner Suche nach Bigweld auf Roboter, die ihm hilfreich zur Seite stehen. Allen voran der rote Fender, welcher ständig irgendwelche seiner Teile verliert, dessen Schwester Piper, den großen Lug, der leicht pessimistische Crank und Diesel, welcher seit einem Unfall kein Sprachmodul mehr hat und somit stumm ist.\n\nEs stellt sich heraus, dass der idealistische Bigweld im eigenen Unternehmen nichts mehr zu sagen hat und vom geldgierigen Ratchet abgelöst worden ist. Angetrieben durch seine Mutter Madame Gasket will dieser seinen finsteren Plan durchsetzen und alle „minderwertigen“ Roboter zu Altmetall verarbeiten. Einzig Ratchets Assistentin Cappy sympathisiert mit Rodney und unterstützt ihn bei seinem Plan, aus \"Robot City\" wieder eine lebenswerte Stadt für alle Roboter zu machen. Als Ratchet herausfindet, dass Rodney den armen Robotern hilft, indem er sie repariert, schickt er ihm und seinen Freunden seine Sicherheitsroboter auf den Hals. Cappy gelingt es durch eine List, Rodney vor deren Fängen zu retten. \n\nCappy bringt Rodney zum Haus von Bigweld. Nach einem zunächst enttäuschend verlaufenen Versuch, Bigweld für den Kampf gegen Ratchet zu gewinnen, hat es sich dieser später doch anders überlegt und steht Rodney, aus Dankbarkeit für dessen Vertrauen in ihn, tatkräftig zur Seite. In der entscheidenden Schlacht gegen die Roboter-Bösewichte kommen den Freunden viele von Rodney reparierte Altmodelle aus der Stadt zu Hilfe. Ratchet kann besiegt werden und Madame Gasket fliegt in den Schmelzofen. Bigweld kommt zusammen mit Rodney in dessen Heimatstadt \"Rivet Town\", um Rodneys Vater die dringend benötigten Ersatzteile mitzubringen und den sichtlich stolzen Eltern mitzuteilen, Rodney sei von nun an seine rechte Hand und sein zukünftiger Nachfolger der Firmenleitung.\n\n\nFür die Filmmusik wurden auch Musiker der Blue Man Group für Percussion-Effekte engagiert, die dafür bekannt sind, aus Objekten wie Industrierohren, Schläuchen und ähnlichen Gegenständen Musikinstrumente für ihre Darbietungen zu bauen. Sarah Connor steuerte den Titel „From Zero to Hero“ bei, dieser ist jedoch nur in der deutschen Filmfassung im Abspann zu hören und nicht in der englischen Originalversion. Im Film wird auch der Titel „Underground“ von Tom Waits (aus dem Album \"Swordfishtrombones\") gespielt, der nicht auf dem offiziellen Film-Soundtrack veröffentlicht wurde.\n\nDie Produktionskosten wurden auf rund 75 Millionen US-Dollar geschätzt. Der Film spielte in den Kinos weltweit rund 260 Millionen US-Dollar ein, davon rund 128 Millionen US-Dollar in den USA und 10,7 Millionen US-Dollar in Deutschland.\n\nKinostart in den USA war am 11. März 2005, in Deutschland am 17. März 2005.\n\n\n\n"}
{"id": "608603", "url": "https://de.wikipedia.org/wiki?curid=608603", "title": "SCORE (Software)", "text": "SCORE (Software)\n\nScore (nicht zu verwechseln mit Score Perfect) ist ein von Leland Smith, Professor für Musik an der Stanford University, seit 1967 entwickeltes, PostScript basiertes Notensatzprogramm. Die 2012 veröffentlichte aktuelle Version 5 ist unter Windows 98, XP, Vista, 7 und 8 lauffähig. Score war bis zur Version 4 immer noch ein MS-DOS-Programm, was zu zunehmenden Kompatibiltätsproblemen mit neuerer Hard- und Software führte und neue Nutzer wegen der wenig intuitiven Benutzeroberfläche leicht abschreckte. Dennoch konnte sich Score einen festen Nutzerstamm überwiegend professioneller Anwender halten. Dazu haben vor allem drei Dinge beigetragen:\n\n\nBenutzer der Versionen 3 und 4 konnten seit dem 1. Januar 2009 auf eine Beta-Version für Windows (Version 5.0) upgraden. Seit dem Release von Version 5 können diese Nutzergruppen preisgünstigere Upgrade-Versionen erwerben.\n\n\n"}
{"id": "609710", "url": "https://de.wikipedia.org/wiki?curid=609710", "title": "AxCrypt", "text": "AxCrypt\n\nAxCrypt ist eine Software zur fallweisen Verschlüsselung und Entschlüsselung von einzelnen Dateien, die den AES-Algorithmus mit einer Schlüssellänge von 128 oder 256 Bit verwendet. AxCrypt erzeugt bei der Verschlüsselung einer Datei ein Archiv, welches neben den verschlüsselten Daten der Datei weitere Metadaten enthält und löscht anschließend die Originaldatei. Zum Öffnen des Archives kann ein Doppelklick verwendet werden. AxCrypt entschlüsselt daraufhin die enthaltende Datei, schreibt diese ins Dateisystem und ruft anschließend damit die für die Dateierweiterung zuständigen Anwendung auf.\n\nAxCrypt funktioniert mit Windows 95/98/Me/NT/2000/XP/7/8/10 sowie mit macOS und ist freie Software, die unter der GNU GPL steht. Hersteller ist das Unternehmen Axantum Software AB aus Järfälla, Schweden.\n\nDerzeit wird AxCrypt in den Sprachen Englisch, Niederländisch, Französisch, Deutsch, Italienisch, Koreanisch, Portugiesisch, Spanisch, Schwedisch, Türkisch, Russisch und Polnisch angeboten. Das Programm verfügt auch über einen integrierten Dateishredder.\n\nSeit dem 29. August 2009 gibt es auch eine 64-Bit-Version.\n"}
{"id": "612418", "url": "https://de.wikipedia.org/wiki?curid=612418", "title": "Kellfaktor (Technik)", "text": "Kellfaktor (Technik)\n\nDer Kellfaktor, auch als K-Faktor bezeichnet, ist im Bereich der Rastergrafiken, und davon abgeleitet in der Videotechnik und Fernsehtechnik, ein experimentell ermittelter Faktor, der angibt, um wie viel höher ein abgetastetes Bildsignal von einem Bildsensor minimal sein muss, um bei Bildanzeigegeräten wie einem Monitor mit diskreten Bildpunkten (Pixel) Stördarstellungen infolge von Schwebung zu minimieren. Er liegt im Wertebereich zwischen 0,5 und 1. Der Wert 0,5 entspricht genau der Abtastung mit der Nyquist-Frequenz (halben Abtastfrequenz), der obere Grenzwert von 1 entspricht der Abtastfrequenz. Er wird durch praktische Versuche ermittelt und möglichst klein gewählt, um bei gegebener Bandbreite des Bildsignals im diskreten System die nötige Anzahl von Pixel pro Zeiteinheit zu minimieren. Signaltheoretisch beschreibt der Kellfaktor eine Art von Unterabtastung.\n\nDer Kellfaktor ist benannt nach Raymond Davis Kell, der in den 1930er Jahren bei Radio Corporation of America (RCA) erste Experimente dazu durchführte und einen experimentellen Wert von 0,64 ermittelte. Allerdings gelang es Kell dabei nicht, seinen experimentellen Aufbau nachvollziehbar zu beschreiben. Die spätere publizierte Arbeit von Raymond Kell gemeinsam mit seinen Kollegen A. Bedford und G. Fredendall im Jahr 1940 kommt zu einem Faktor von 0,85.\n\nBei Darstellunggeräten mit fixen Pixeln, wie Flüssigkristallanzeigen (LCD-Monitor) und CCD-Sensoren als Bildaufnahmegerät kann ein vergleichsweise hoher Kellfaktor um 0,9 nötig sein, um eine annähernd störungsfreie Darstellung zu erlauben. Digitales High Definition Television (HDTV) arbeitet generell mit einem Kellfaktor von 0,9. Bei Kathodenstrahlröhren, wie sie früher in Fernsehgeräten und bei analogen Fernsehübertragungsverfahren eingesetzt wurden, ist ein Kellfaktor von 0,7 ausreichend.\n\nWird ein Signal mit einer Abtastfrequenz mindestens doppelt so hoch wie die höchsten Spektralanteile im Signal abgetastet, ist zufolge des Nyquist-Shannon-Abtasttheorems immer eine fehlerfreie Rekonstruktion des kontinuierlichen Signalverlaufs aus den diskreten Einzelwerten möglich. Dieser Fall entspräche einem Wert von 1 oder mehr des Kellfaktors, der in diesem Fall aber keine Reduktion der Anzahl der Pixel pro Zeiteinheit entspricht, weshalb Werte von 1 und größer keine Bedeutung haben. Das Ziel ist, die Anzahl der nötigen Pixel pro Zeiteinheit bei gleicher Bandbreite zu minimieren, womit nur Werte kleiner 1 von Bedeutung sind.\n\nBei einem Kellfaktor kleiner als 0,5, dies entspricht einer Abtastfrequenz, die kleiner als die obersten Spektralanteile im Signal ist, kommt es zum Auftreten von Spiegelfrequenzen oder Aliasing, die eine korrekte Rekonstruktion unmöglich machen.\n\nIm Bereich von 0,5 bis 1, dem Bereich, in dem der konkrete Kellfaktor gewählt werden kann, treten Zufolge der Phasenlage des Bildsignals in Relation zur Phasenlage der Abtastfrequenz Überlagerungseffekte auf, die sich als eine Schwebung bemerkbar machen. Je nach Bildinhalt und Art der Darstellungsgeräte sind diese Störungen unterschiedlich. Als Beispiel dient nebenstehendes Bild, das aus abwechselnd weißen und schwarzen vertikalen Linien besteht, somit dessen obere Grenzfrequenz genau der Abtastfrequenz entspricht, was gleich einem Kellfaktor von 0,5 ist. Die vertikalen Linien im Bild sind gegen den rechten Bildrand hin leicht geneigt, wodurch es zu unterschiedlichen Phasenlagen in Bezug zu den exakt vertikalen Abtastpunkten kommt. Durch diese Neigung kommt es zu einer Schwebung, die sich als Mischung, in diesem Fall sind dies graue Bereiche, unterschiedlicher Form ausbilden. Da diese grauen Bereiche nicht Teil des ursprünglichen Bildinhaltes sind, wirken sie als Störung. Bei einem Kellfaktor von 0,66, in zweiter Abbildung dargestellt, sind diese Störungen zwar noch vorhanden, aber deutlich schwächer. Dies entspricht der Abwägung zwischen einer Reduktion der Anzahl der Pixel einerseits und andererseits möglichst geringen Störungen.\n\nDie Bildstörungen zufolge eines zu niedrig gewählten Kellfaktors sind ähnlich wie Bildstörungen bei dem Moiré-Effekt, damit aber nicht zu verwechseln.\n"}
{"id": "613673", "url": "https://de.wikipedia.org/wiki?curid=613673", "title": "Media Control Interface", "text": "Media Control Interface\n\nDas Media Control Interface (MCI) war die erste generische Programmierschnittstelle in Windows zur Steuerung abstrakter Multimedia-Geräte und Ressourcen. Entwickelt wurde es Anfang der 1990er Jahre und fand Einzug in Windows 3.11. MCI bietet identische Funktionen innerhalb einer Geräteklasse und Datentypen und zum Teil auch darüber hinweg. So können Multimedia-Geräte einer Geräteklasse weitestgehend unabhängig von ihren ganz konkreten Eigenschaften benutzt werden, daher spricht man von abstrakten Geräten.\n"}
{"id": "613840", "url": "https://de.wikipedia.org/wiki?curid=613840", "title": "KSysguard", "text": "KSysguard\n\nKSysguard (auch bekannt als KDE System Monitor) übernimmt in KDE SC 4 die Aufgabe der Prozessverwaltung und Leistungsüberwachung.\n\nKSysguard bietet eine Client/Server-Architektur, die das Überwachen von lokalen und Netzwerk-Ressourcen zulässt. Dafür verwendet die graphische Oberfläche so genannte Sensoren, um die Informationen zu bekommen, die auf ihr dargestellt werden. Ein Sensor kann einfache Werte oder auch komplexere Informationen wie Tabellen liefern. Für jede Art von Information werden eine oder mehrere Anzeigen bereitgestellt, die wiederum in Arbeitsblättern zusammengefasst werden, die gespeichert und unabhängig voneinander auch wieder geladen werden können. Damit ist KSysguard nicht nur ein einfacher Prozess-Verwalter, sondern gleichzeitig auch ein sehr mächtiges Werkzeug zur Kontrolle großer Server-Verbände.\n\n\n"}
{"id": "613852", "url": "https://de.wikipedia.org/wiki?curid=613852", "title": "KControl", "text": "KControl\n\nDas KDE-Kontrollzentrum alias KControl war das zentrale Programm zur Konfiguration der K Desktop Environment.\n\nDas Kontrollzentrum besteht aus mehreren Modulen, die separate Anwendungen darstellen, aber vom Kontrollzentrum geordnet und einheitlich dargestellt werden. Der modulare Aufbau ermöglicht dabei, beliebige Erweiterungen in das Kontrollzentrum einzubauen.\n\nDie Einstellungen, die standardmäßig im Kontrollzentrum vorgenommen werden können, beziehen sich auf\n\nAb der Version 4.0 der KDE Software Compilation (ehemals \"K Desktop Environment\") hat das Programm \"System Settings\" („Systemeinstellungen“) KControl ersetzt.\n\n"}
{"id": "613861", "url": "https://de.wikipedia.org/wiki?curid=613861", "title": "KPDF", "text": "KPDF\n\nMit dem KDE-Programm KPDF können PDF-Dokumente unter KDE betrachtet werden. KPDF verwendet die Programmbibliothek Xpdf und kann mit dem Adobe Reader der Firma Adobe verglichen werden. Wie alle übrigen KDE-Programme ist es freie Software und unter der GNU General Public License erhältlich.\n\nDer universelle Dokumentbetrachter Okular ersetzt KPDF seit KDE 4.0.\n\nKPDF gibt dem Nutzer die Möglichkeit, Textpassagen ebenso zu durchsuchen wie zu markieren. Markierte Texte oder auch markierte Bilder können dann in die Zwischenablage kopiert und in anderen Programmen weiterverarbeitet werden. Weiterhin können PDF-Dateien in einem besonderen Präsentationsmodus angezeigt werden, bei dem nur die Inhalte der Dateien ohne den Rest der Desktopumgebung angezeigt werden.\n\nDie enge Verflechtung von KPDF mit KDE äußert sich unter anderem darin, dass die Sprachsynthese-Einheit von KDE genutzt werden kann, um PDF-Dateien vorlesen zu lassen. Ebenso kann KPDF als Plugin innerhalb des Browsers Konqueror geladen werden, um PDF-Dateien in dem Browser anzuzeigen.\n\nKPDF erlaubt (im Gegensatz zum Adobe Reader), dass geöffnete Dateien von anderen Programmen geändert werden. Damit ist es geeignet, um sich von LaTeX generierte Dokumente anzeigen und automatisch aktualisieren zu lassen.\n\nZudem eignet sich KPDF zum Durchsuchen von großen PDF-Dateien. Während der ersten Suche wird ein Index des PDF-Dokuments angelegt, mit dem weitere Suchanfragen stark beschleunigt bearbeitet werden können.\n\n"}
{"id": "613875", "url": "https://de.wikipedia.org/wiki?curid=613875", "title": "KGet", "text": "KGet\n\nKGet ist der Download-Manager der KDE Software Compilation 4.\nEr wird in C++ programmiert und als freie Software einschließlich des Quelltextes unter den Bedingungen von Version 2 der GNU General Public License (GPL) verbreitet.\n\nMit dem Programm lassen sich Downloads komfortabel verwalten: Man kann sie beliebig starten, anhalten und wiederaufnehmen. Während des Downloads zeigt KGet auf Wunsch die geschätzte Restdauer des Vorgangs, die übertragene Datenmenge oder weitere Informationen an.\n\nKGet unterstützt neben HTTP und FTP auch BitTorrent und Metalinks. Die Downloads können verschiedenen Gruppen zugeordnet werden, die z. B. unterschiedliche Ziel-Verzeichnisse und max. Download-Geschwindigkeiten (Datenraten) haben; außerdem kann man die maximale Anzahl der gleichzeitigen Downloads pro Gruppe einstellen.\n\nKGet kann in den Browser Konqueror eingebunden werden, um dort eine Übersicht und Download-Auswahl über alle Links einer Webseite zu geben. Über das Zusatzmodul FlashGot kann KGet auch in Mozilla Firefox eingebunden werden. rekonq delegiert alle Datei-Downloads an KGet. Alternativ kann auch ein Drop-Feld angezeigt werden, auf das von beliebiger Stelle in KDE aus Verweise auf Dateien gezogen werden können, damit diese mit KGet heruntergeladen werden.\nNeben der manuellen Eingabe der Quelladresse oder der Überwachung der Zwischenablage besteht auch die Möglichkeit, via D-Bus zum Beispiel mittels eines Shell-Skriptes neue Downloads anzustoßen. Im Unterschied zur direkten Verwendung von wget und ähnlichem wird ihr Fortschritt dann in der KDE-weiten Statusanzeige integriert.\n\nAls Standard-Download-Manager der KDE Software Compilation dient KGet das Standard-Werkzeug für viele Linux-Distributionen.\n\nDie erste Version für KDE 3 (0.8.0-RC-1) wurde am 26. Mai 2002 veröffentlicht.\nDie Versionen für KDE 3.x unterstützten nur HTTP und FTP.\nFür KDE Software Compilation 4 wurde KGet von Dario Massarin, Manolo Valdes und Urs Wolfer neugeschrieben und Unterstützung für BitTorrent sowie Segmentierung von Downloads und Begrenzung der Übertragungsgeschwindigkeit hinzugefügt. Weiterhin kann nun über eine Programmierschnittstelle per Zusatzmodul Unterstützung für weitere Protokolle hinzugefügt werden. Das Ergebnis wurde als KGet 2 veröffentlicht. Es wurde mit einem eigenen Widget/Applet in die Plasma-Oberfläche integriert.\n\n"}
{"id": "617951", "url": "https://de.wikipedia.org/wiki?curid=617951", "title": "A 7150", "text": "A 7150\n\nDer Robotron A 7150 war ein 16-Bit Personal Computer aus der DDR. Er kam im Januar des Jahres 1988 als Nachfolger des A 7100 auf den Markt. Er wurde unter der Bezeichnung CM 1910 in das System der Kleinrechner des RGW eingegliedert. Die meisten Computer wurden unter diesem Namen vertrieben.\n\nWie der A 7100 hatte der A 7150 als CPU einen sowjetischen Intel 8086-Clone, den K1810WM86. Dank Geschäften mit der Bundesrepublik Deutschland bzw. anderen Staaten des NSW gab es jedoch eine Menge Systeme, die auch mit dem Intel 8086, meistens in Kombination mit dem Koprozessor Intel 8087 vermarktet wurden. Die Hardware wurde so gestaltet, dass der A 7150 – im Gegensatz zum A 7100 – (begrenzt) PC-kompatibel war. Dementsprechend lief auch ein Clone von MS-DOS (DCP). Im Inneren arbeitete, neben den zwei 5,25 Zoll-Diskettenlaufwerken, eine 5,25 Zoll-Festplatte. In der Regel war dies eine MFM-Festplatte mit 20 oder 40 MB Kapazität. In den letzten Versionen des A 7150 war sogar die Installation einer zweiten Festplatte möglich.\n\nDie meisten Modelle des A 7150 besaßen weiterhin einen Anschluss für ein Grafiktablett, mit dem das Übertragen von Punktkoordinaten und Linien in den Rechner erfolgen konnte. Hierfür war ein entsprechender Treiber und ein geeignetes Grafikprogramm notwendig.\n\nAls weitere Betriebssysteme neben DCP kamen auch SCP1700 V3.x (CP/M-86-Klon), MUTOS1700 (UNIX-Klon) oder seltener BOS1810 (Echtzeitbetriebssystem) zum Einsatz.\n\n\n"}
{"id": "619172", "url": "https://de.wikipedia.org/wiki?curid=619172", "title": "GpsDrive", "text": "GpsDrive\n\nGpsDrive ist eine freie Navigationssoftware für GNU/Linux und diverse BSD-Derivate (offiziell wird nur FreeBSD unterstützt). Sie nutzt das Global Positioning System (GPS), um die Position festzustellen. GpsDrive unterliegt der GPL. Das Programm wurde ursprünglich von Fritz Ganter, einem Softwareentwickler und Elektroniker entwickelt. Zurzeit wird das Projekt von Jörg Ostertag geleitet. \n\nGpsDrive arbeitet mit GPS-Empfängern zusammen, die entweder den NMEA-Standard zur Datenübertragung oder den des Herstellers Garmin verwenden. Dabei kann das GPS-Gerät mit dem PC (meist ein Notebook) über eine serielle Schnittstelle, per USB oder Bluetooth verbunden sein.\n\nDie benötigten Karten können in verschiedenen Maßstäben über eine integrierte Downloadfunktion (kostenfrei) vor Antritt oder während der Fahrt bezogen werden. Man kann auch eingescannte Karten über einen Assistenten importieren. Das Kartenmaterial stammt von OpenStreetMap beziehungsweise Expedia. Beim Navigieren kann GpsDrive automatisch die beste (bzw. die mit dem geringsten Maßstab) Karte auswählen und anzeigen. Eine einfache Zoomfunktion für die Karten ist auch integriert.\n\nWegpunkte können entweder in einer Textdatei oder SQL-Datenbank gespeichert, ausgelesen und mit benutzerdefinierten Symbolen belegt werden.\nMit Hilfe des ebenfalls freien Programms festival unterstützt die Software auch Sprachausgabe, was besonders für das Navigieren im Auto nützlich sein kann.\n\nÜber einen Daemon (friendsd) und einer Verbindung zu anderen Computern können andere Personen die eigene Position in Echtzeit sehen und umgekehrt.\nGpsDrive kann auch mit Kismet, einem WLAN-Sniffer, zusammenarbeiten, um die Access-Points, die man beim Wardriving findet, auf der Karte anzuzeigen und zu speichern.\n\nGpsDrive unterstützt zurzeit (Stand: 26. September 2007) keine Fahrzeugnavigation im herkömmlichen Sinne: Das Programm besitzt keinen Routenplaner. Es zeigt lediglich die vom GPS ermittelte Position auf einer Karte an, die es – anders als übliche Navigationshilfen – nicht „versteht“. Es werden keinerlei Abbiegehinweise erteilt; auch kann das System weder Alternativrouten berechnen noch präzise die voraussichtliche Fahrzeit prognostizieren.\n\n\n"}
{"id": "622742", "url": "https://de.wikipedia.org/wiki?curid=622742", "title": "Exact Audio Copy", "text": "Exact Audio Copy\n\nExact Audio Copy (oft auch EAC abgekürzt) ist ein CD-Ripper zum Auslesen (Rippen) von Audio-CDs, ein Vorgang, der auch als Digital Audio Extraction (DAE) bekannt ist. EAC wird seit Mitte 1990 von André Wiethoff für Windows entwickelt. Besonderer Schwerpunkt ist das möglichst exakte, also fehlerfreie Auslesen der Daten, um identische Kopien erstellen zu können, im Zweifelsfalle auf Kosten der Geschwindigkeit.\n\nWiethoff entwickelte EAC, da er bei bis dato verfügbaren CD-Rippern damit unzufrieden war, wie zerkratzte Audio-CDs behandelt wurden. Weder konnten sie zerkratzte Medien fehlerfrei auslesen, noch Lesefehler in Hinsicht auf ihre Existenz und ihre Position innerhalb des Audiomaterials dokumentieren. Daher musste das ausgelesene Audiomaterial vollständig per Hörtest überprüft werden.\n\nEAC wird als natives Windows-Programm entwickelt, es wurde aber auch erfolgreich mit neuen Versionen von Wine unter Linux verwendet.\n\nDas Besondere im Vergleich zu anderen Rippern wie CDex und Audiograbber oder multimedialer Audiosoftware wie iTunes und Windows Media Player ist sein Ausleseverfahren: \n\nIm sogenannten \"Secure Mode\" können CDs auch mit (meist älteren) CD-ROM-Laufwerken, die ohne EAC Leseprobleme zeigen, mit hoher Wahrscheinlichkeit verlustfrei ausgelesen werden, soweit die Ursache der Probleme im Bereich des Laufwerks (z. B. zufällige Lesefehler) und nicht bei der jeweiligen CD liegt (z. B. durch Kratzer, Schmutz oder einen Kopierschutz). Aber auch in letzteren Fällen steigt durch die Benutzung von EAC die Wahrscheinlichkeit eines akzeptablen bis fehlerfreien Ausleseergebnisses.\n\nDurch mehrfaches, häppchenweises Auslesen der CD können fehlerhaft ausgelesene Stellen korrigiert oder zumindest identifiziert werden. Dabei erhält der Benutzer Fehlermeldungen über Leseprobleme und deren Position, auch wenn das Laufwerk trotz Problemen noch keinen Fehler meldet. Die subjektive Audioqualität ist an solchen problematischen Stellen meist besser als bei Benutzung von konventionellen CD-Rippern, die keine entsprechenden Vorkehrungen treffen. Die Meldungen beim Auslesen können in einer Logdatei gespeichert werden, wodurch fehlerhafte Bereiche dokumentiert und vor der Weiterverarbeitung begutachtet werden können.\n\nDie so gewonnenen Dateien werden zunächst als RIFF-WAVE-Dateien abgespeichert und können durch EAC mit einem konfigurierbaren Audio-Codec umgewandelt werden. Darüber hinaus enthält das Programm eine simple CD-Schreib-Funktionalität (mit der Möglichkeit einer Offsetkorrektur), so dass auch CD-Kopien unter ausschließlicher Verwendung von EAC erstellt werden können. \n\nZu den weiteren Eigenschaften zählen:\n\nEAC bietet bei der Installation schrittweise eine Reihe von Optionen an, die mit Standard-Vorschlägen und Erklärungen versehen sind. Dadurch wird Laien sowohl die Installation als auch die Anwendung erleichtert; Experten haben Zugang zu detaillierten Optionen. Ein typischer Anwendungsfall ist, dass die „gerippten“ Dateien zu MP3-Dateien konvertiert werden. Zu dieser Art von Konvertierung wird LAME unterstützt. LAME ist aus lizenzrechtlichen Gründen kein Bestandteil der EAC-Installation. Während der EAC-Installation wird darauf aufmerksam gemacht, eine Bezugsquelle von LAME genannt und erklärt, wie EAC so zu konfigurieren ist, dass LAME als Konverter genutzt werden kann. Standardmäßig wird bei der Installation von EAC der Rechner durchsucht, ob bereits eine LAME-Installation vorliegt. Nach erfolgreicher Installation (oder bereits vorhandener Existenz) von LAME werden in EAC typische Aufruf-Parameter für LAME gesetzt, die vom Benutzer geändert werden können.\n\nLaut Website ist EAC Freeware (Donationware), solange es nicht für geschäftliche Zwecke benutzt wird.\n\n"}
{"id": "622887", "url": "https://de.wikipedia.org/wiki?curid=622887", "title": "Microsoft Windows XP Embedded", "text": "Microsoft Windows XP Embedded\n\nWindows XP Embedded ist ein Betriebssystem des Unternehmens Microsoft. Es ist eine modularisierte Version von Windows XP Professional, die es erlaubt, nur einen Teil der Komponenten auf dem Ziel-PC zu installieren. Hierfür stehen spezielle Installationswerkzeuge zur Verfügung (dazu unten mehr). Verwendungsgebiete sind ausschließlich PC-artige Geräte, die auf einem x86-Prozessor basieren, wobei PCs im engeren Sinne per Lizenz von der Nutzung ausgeschlossen sind. Damit ist es von Windows CE zu unterscheiden, das auch die MIPS- und ARM-Architekturen unterstützt.\n\nWindows XP Embedded ist vollständig modular aufgebaut. Module umfassen unter anderem Windows Media Player (8 oder 10), Internet Explorer 6, eine Firewall, den Windows-Explorer, DirectX 9 und .NET 3.1. XP Embedded kann auf einen Speicherverbrauch auf der Festplatte, Flash oder USB-Stick von minimal 34 MB abgespeckt werden. Übliche Installationen mit grafischer Oberfläche und Netzwerk umfassen etwa 200–500 MB. Die einzelnen Module können mit dem sog. „Target Designer“ zu einem vollwertigen, auf die Anforderungen angepassten, Betriebssystem-Speicherabbild zusammengestellt werden. Außerdem ist es möglich, z. B. selbst entwickelte Software in ein Modul zu kapseln, welches sich dann, wie die mitgelieferten auch, bearbeiten lässt.\n\nMeistens wird es in Kassen im Einzelhandel (z. B. McDonald’s, Aral, Deutsche Bahn) eingesetzt. Andere Einsatzmöglichkeiten umfassen Geldautomaten, Oszilloskope, Spielautomaten, Thin Clients, Digitalkameras, Car-PCs oder Geräte der Unterhaltungselektronik. Als Beispiel kommt Windows XP Embedded auf Packstationen von DHL zum Einsatz. Auch für industrielle Anwendungen ist es aufgrund der Möglichkeit zum bedarfsgerechten Zuschneiden auf Applikationen interessant (z. B. für PC-basierte Steuerungen). Es ist lauffähig auf Geräten ohne Tastatur, Maus, Bildschirm oder Festplatte.\nIm Gegensatz zu Windows 7 Embedded besitzt es noch keine echte onboard Touchscreen Unterstützung. Einzig ELO Software inkl. dessen Monitore lassen sich zu diesem Zweck z. B. einer Visualisierung Nutzen.\n\nEs existiert eine kostenlose 120-Tage-Testversion des Systems, in deren Testperiode ein Anwender den Leistungsumfang des Eingebetteten Systems testen kann. Bei der Testversion wird ein Testversion-Hinweis im Desktop-Hintergrund eingeblendet.\n\nNachfolger des Systems Windows XP Embedded ist Windows Embedded Standard 2009.\n\n"}
{"id": "623800", "url": "https://de.wikipedia.org/wiki?curid=623800", "title": "Tastenbeschriftung", "text": "Tastenbeschriftung\n\nAuf den Standardtasten (Buchstaben-, Zahlen- und OEM-Tasten) von Computertastaturen können mehrere Zeichen abgebildet sein, der Regelfall liegt bei bis zu drei Tastenbelegungen.\nBei den meisten Tasten verhält es sich wie folgt: Links unten (auf Bild Position 1) ist das Zeichen, das durch Drücken der Taste ohne Sondertaste (Alt Gr, Shift, Strg, Alt, Shift Lock) erzeugt wird. Links oben (2) ist das Zeichen abgebildet, das durch Halten von + Drücken der Taste erzeugt wird und \nrechts unten (3) ist das Zeichen, das durch Halten von + Drücken der Taste erzeugt wird. \nEs gibt Ausnahmen, so sind die Umlauttasten auf Schweizer Tastaturen fünffach beschriftet (Als Beispiel das : Position 1: «ö»; Position 2: «é» Position 3: «é» Position 4: «ö»). Falls die Deutschschweizer Tastaturtreiber geladen sind, so sind nur die linken zwei Beschriftungen zu beachten, d. h. ohne Shift wird ein «ö», mit Shift ein «é» ausgegeben. Mit französischschweizer Tastaturtreibern dagegen sind nur die zwei rechten Beschriftungen zu beachten, d. h. ohne Shift wird ein «é», mit Shift ein «ö» ausgegeben. Der Vorteil darin ist, dass nicht separate Tastaturen für die deutsch- und französischsprachige Schweiz produziert werden müssen, sondern für beide Sprachregionen dieselben Tastaturen – nur mit anderen Treibern – verwendet werden können. Daneben kann mit der -Taste noch das «{» erzeugt werden.\n\nDie Position rechts oben (4) wird in manchen anderen Layouts dagegen für das Zeichen verwendet, welches mittels Halten von + + Drücken der Taste realisiert wird.\n\nUnter Windows lassen sich solche Belegungen durch spezielle Tastaturtreiber erreichen; unter X bei Linux sind in den meisten Tastaturlayouts bereits solche Dritt- und Viertbelegungen vordefiniert.\nBeispielsweise lassen sich im deutschen Tastaturlayout Symbole wie ™, ©, Ω, Æ usw. per - oder +-Kombination erzeugen.\n\nDa aber zurzeit keine Norm für diese zusätzlichen Belegungen existiert, sind entsprechend bedruckte Tastaturen auf dem Markt gewöhnlich nicht zu finden.\n"}
{"id": "623964", "url": "https://de.wikipedia.org/wiki?curid=623964", "title": "Autopackage", "text": "Autopackage\n\nAutopackage war ein alternatives Linux-Softwareinstallationssystem. Ihr Zweck war eine einfache Installation von Software, unabhängig von der verwendeten Linux-Distribution. Sie sollte sowohl Nutzern als auch Entwicklern das Leben erleichtern, indem sie es ermöglicht, nur ein einziges Paket zu erzeugen, das sich vom Nutzer auf jedem beliebigen Linux-System mit einem Klick installieren und auch wieder löschen lässt. Durch die mögliche direkte Verbreitung von Software hat der Anwendungsanbieter mehr Kontrolle über Produktupgradezyklen. Es wurde von etablierten Open-Source-Projekten wie AbiWord, Inkscape oder Gajim erfolgreich eingesetzt.\n\nAutopackage selbst ist ein Shellscript, welches das zu installierende Programm beinhaltet. Bis auf die Bash ist kaum zusätzliche Software nötig. Sollte Autopackage auf dem System fehlen, wird das Programm automatisch heruntergeladen und installiert. Die Installation des Programmes ist skriptgesteuert und lässt sich dadurch flexibel anpassen und weiter automatisieren. Der Installationsprozess soll praktisch keine Benutzerinteraktion erfordern. Benutzern kann es auch erlaubt werden Pakete ohne Root-Passwort zu installieren. Abhängigkeiten werden automatisch und unabhängig vom eingesetzten Paketverwaltung des Stammsystems aufgelöst, heruntergeladen und installiert. Zur besseren Integration auf dem Linux-Desktop gibt es grafische Interfaces sowohl für Qt und GTK+, aber auch eine textbasierte Schnittstelle für Terminals.\n\nAutopackage unterliegt jedoch einigen Einschränkungen, besonders im Bereich der Binär-Kompatibilität des Linux-Ökosystems, weswegen seine Verbreitung überschaubar ist. Nach wie vor sind einige Probleme nur teilweise oder noch gar nicht gelöst. Zum Beispiel kann die Verwendung der Versionen 3.4 oder 4.0 des GNU-C++-Compilers zu früheren gcc-Versionen ABI-inkompatiblen Code erzeugen. Dieses Problem betrifft unter anderem das Qt-Toolkit, welches in C++ geschrieben ist.\n\nAuch die Zusammenarbeit mit den Distributions-eigenen Paketverwaltung klappt noch nicht immer reibungsfrei. So können installierte Pakete fälschlicherweise als eigene identifiziert und diese im Zuge von Veränderungen irrtümlich deinstalliert werden, da die jeweils andere Paketverwaltungssystem von dieser Deinstallation jedoch nichts mitbekommt wird angenommen, diese Pakete wären noch vorhanden und so kommt es in beiden Paketverwaltungen zu falschen Paketlisten.\n\nAuch das Umplatzieren (Relocation) von Applikationen zwischen verschiedene Verzeichnissen ist in Linux nicht vorgesehen, Pfade werden typischerweise zur Compilezeit in der Anwendung hart-codiert. Die Autopackage-Bibliothek \"binreloc\" löst dieses Problem jedoch über die Bereitstellung von Funktionalität vergleichbar der Win32-API-Funktion codice_1 wodurch Verzeichnis relokierbare Anwendungen und Bibliotheken möglich werden.\n\nVision des 2002 von Mike Hearn gestarteten Projekts war auch eine Weiterentwicklung von Linux zu einer \"Desktop-Plattform\". Technisch sollte dazu eine binärkompatible Plattform mit stabilen ABIs entstehen, ähnlich Windows oder Mac-OS, hierzu sollte mit der LSB kooperiert werden. Als weiterer Aspekt sollte ein Fokuswechsel stattfinden, weg von den bereits gut ausgebauten \"Corporate-Desktop\" Administrationswerkzeugen und Strukturen, hinzu den Desktopanwendern und deren Bedürfnis nach \"Simplen Lösungen\". Damit einhergehen sollte eine schärfere Differenzierung zwischen Anwendungssoftware und Systemsoftware, wodurch differenziertere Sicherheitsmaßstäbe und Update-Zyklen angelegt werden könnten. Dieser weitgehende Ansatz wurde von vielen aus der Linux-Community kritisiert, besonders aus dem Umfeld der Distributionen.\n\n2006 zeigte sich Hearn in einem Vortrag auf der \"Free Standard Group's Packaging Summit\" Konferenz der Linux Foundation pessimistisch über die Chancen die grossen Distributionen zur Kooperation bewegen zu können. Als Grund nannte er das Konkurrenzdenken der Distributionen untereinander, welches Weiterentwicklung in Richtung übergreifender Standards verhindere. Weiter kritisiert Hearn das vorherrschende Modell des Paketmanagements direkt als \"überholt\" und \"anti-demokratisch\": \n\nObwohl die meisten technischen Probleme gelöst wurden und auch einige namhafte Anwendungen Autopackage verwendeten, gelang es dem Projekt nicht, breiteren Zuspruch zu erringen. 2007 kam der Autor des \"Linux.com\"-Artikel \"Autopackage struggling to gain acceptance\" zu dem Schluss, dass eine mögliche, schmerzhafte Lehre des Autopackage-Projekts die scheinbare Unmöglichkeit größere Änderungen an der Infrastruktur des Linux-Ökosystems zu erreichen, sei.\nProjektinitiator Mike Hearn wechselte schließlich zu Google und gab die Projektführung von Autopackage ab.\n\n2010 wurde das Projekt dann eingestellt, dazu wurde auf der Homepage auf Alternativ-Projekte verwiesen: \"Listaller\", \"Zero Install\", \"portablelinuxapps.org\" (heute \"AppImage.org\") und das \"MojoSetup\". Teile der Codebasis wurden vom \"Listaller\"-Projekt übernommen.\n\n\n\n"}
{"id": "624893", "url": "https://de.wikipedia.org/wiki?curid=624893", "title": "CDex", "text": "CDex\n\nCDex ist ein Windows-Programm, ein sogenannter \"CD-Ripper\", der zum digitalen Auslesen/\"Rippen\" von Audio-CDs verwendet wird. Das Programm war ursprünglich frei unter der GNU General Public License (GPL) erhältlich. Seit Veröffentlichung der Version 1.70 ist jedoch kein Quelltext mehr erhältlich und die Software wird im Bundle mit Adware ausgeliefert.\n\nDie mehrsprachige Software CDex enthält nur rudimentäre Funktionen zur Fehlerkorrektur und Fehlererkennung. Es kann jedoch wahlweise auch das mitgelieferte \"cdparanoia\" verwendet werden, was dann zu hochwertigen und mit EAC vergleichbaren Rips führt. Im Umfang der Software sind bereits viele Codecs für das Zielformat integriert, wie beispielsweise (Ogg-)Vorbis, LAME MP3 und WMA. Zusätzlich ist CDex auch mit externen Audiocodecs erweiterbar. Über die CDDB-Funktion ist es möglich, Titelinformationen der eingelegten CD abzurufen und diese Informationen direkt in die ID3-Tags der erzeugten Audiodateien zu übernehmen.\n\nDie Version 1.51 wurde im September 2003 veröffentlicht. Bis zu diesem Zeitpunkt war Albert L. Faber für das Programm verantwortlich. Da danach lange keine neue Version veröffentlicht wurde, entwickelten andere Programmierer eine inoffizielle Betaversion mit der Versionsnummer 1.60 – in dieser wurden unter anderem Fehler beseitigt und aktuelle Codecs verwendet. Inzwischen wurde die Arbeit an CDex durch Georgy Berdyshev wieder aufgenommen. Unter anderem wurde das CVS-Repository auf SVN umgestellt, in dem mittlerweile einige Änderungen zu verzeichnen sind. Am 23. Juni 2006 erschien eine neue Entwicklerversion (Beta 1.70b2), die neue Funktionen und Fehlerbehebungen enthält.\nAm 30. Juni 2007, einen Tag nach dem Erscheinen von Version 3 der GPL, wurde die Lizenz aktualisiert. Somit war CDex eines der ersten großen und populären Projekte, das die GPL in Version 3 benutzte.\n\nDie Version 1.70 wurde am 29. Juni 2014 veröffentlicht und enthielt erstmals nicht mehr den Quellcode des Programms. Das SVN-Repository, in dem bis Mitte 2015 noch der letzte frei verfügbare Quellcode aus dem Jahr 2008 verfügbar war, ist inzwischen nicht mehr erreichbar.\n\nDie Software enthält die Adware \"OpenCandy\", ein Werbemodul. Die Installation erlaubt eine benutzerdefinierte Installation, bei der nicht benötigte Zusatzangebote (z. B. Skype oder Opera) entfernt werden können.\n\n"}
{"id": "631365", "url": "https://de.wikipedia.org/wiki?curid=631365", "title": "Shared Storage", "text": "Shared Storage\n\nUnter Shared Storage ist in der Informatik ein gemeinsam genutzter Festspeicher zu verstehen, auf den von mehreren Rechnern über ein Netzwerk oder eine sonstige Verbindung gemeinsam und unter Umständen gleichzeitig (konkurrierend) zugegriffen werden kann. Der Festspeicher besteht typischerweise aus einem sogenannten Disk Array.\n\nKlassische Anwendungen von Shared Storage sind Aktiv/Passiv-Cluster oder auch Failover-Cluster. Alle Systeme in diesem Cluster greifen auf denselben Speicher zu. Im Falle eines Failover kann also ein Backup-System sofort das ausgefallene System ersetzen, da beide physikalisch auf den gleichen Datenbestand zugreifen. Ein gleichzeitiger Zugriff ist in einem Aktiv/Passiv-Cluster ausgeschlossen, da immer genau ein System aktiv ist, während die übrigen auf ein Failover warten.\n\nAnders verhält es sich bei einem Aktiv/Aktiv-Cluster: In diesem sind zwei oder mehr Rechner gleichzeitig aktiv und greifen unter Umständen konkurrierend auf die Daten des Shared Storage zu. Ein typisches Beispiel für einen Aktiv/Aktiv-Cluster ist der Oracle Real Application Cluster.\n\nUm Shared Storage zu ermöglichen, kann nicht ohne weiteres eine lokale Festplatte eines Rechners eingesetzt werden. Verwendet werden stattdessen beispielsweise:\n\n\n\nUm doch lokale Festplatten verwenden zu können und zusätzliche Ausfallsicherheit zu erzeugen eignet sich auch das Distributed Replicated Block Device.\n\nIm Zusammenhang mit dem Thema verteilte und parallele DB-Systemen, bedeutet Shared Disk das Vorhandensein eines gemeinsamen Externspeichers. Die DB-Verarbeitung erfolgen auf lose gekoppelten oder lokal oder ortsverteilt angeordneten Rechnern, mit jeweils einem DBMS. \"\" und \"\" sind weitere Ausprägungsformen.\n\n"}
{"id": "631752", "url": "https://de.wikipedia.org/wiki?curid=631752", "title": "Cluster-Dateisystem", "text": "Cluster-Dateisystem\n\nDer Begriff Cluster-Dateisystem beschreibt ein Dateisystem, das in einem Rechnerverbund konkurrierenden Zugriff auf eine Shared Storage gestattet.\n\nAuf ein Cluster-Dateisystem greifen alle im Cluster befindlichen Rechner direkt ohne Vermittlung eines Servers zu. Das Dateisystem muss sich dazu auf einem Speichermedium befinden, das von allen Rechnern direkt erreichbar ist. Dies wird im Allgemeinen durch den Aufbau eines SAN auf der Basis von Fibre Channel oder iSCSI erreicht. Durch den direkten Zugriff ergibt sich eine bessere Performance als bei der Nutzung eines Netzwerk-Dateisystems wie NFS oder CIFS. Insbesondere bei Datenbanken oder Anwendungen, die große Datenmengen manipulieren (Video) ist der Leistungsgewinn erheblich.\n\nWenn mehrere Computer in einem Netzwerk zusammenarbeiten, ist es generell wünschenswert oder sogar notwendig, dass alle auf einen gemeinsamen Datenbestand zugreifen. Dieser Zugriff kann jedoch nicht unkoordiniert ablaufen, denn dies würde zu Problemen führen.\n\nDiesen Zustand nennt man Inkonsistenz.\nDas Auftreten von Inkonsistenz muss zur Vermeidung von Datenverlusten unbedingt vermieden werden.\nUm dies zu erreichen, ist es notwendig, sobald ein Teilnehmer A schreibenden Zugriff auf eine Datei im gemeinsamen Datenbestand erlangt hat, alle weiteren Schreibzugriffe auf dieselbe Datei zu blockieren, bis Teilnehmer A den schreibenden Zugriff wieder aufgegeben hat.\n\nDieses Problem besteht grundsätzlich auch innerhalb eines Rechners mit Multitasking-Betriebssystem, in dem die einzelnen Prozesse um den Schreibzugriff auf einzelne Dateien konkurrieren. Hier obliegt es dem Betriebssystemkern, dafür Sorge zu tragen, dass niemals zwei Prozesse gleichzeitig Schreibzugriff auf ein und dieselbe Datei erhalten. Da es zwischen den Rechnern eines Netzwerkes keine übergeordnete Instanz gibt, der diese Funktion naturgemäß zufällt, sind zusätzliche Maßnahmen erforderlich, um die Konsistenz der Daten zu gewährleisten. Ein Cluster-Dateisystem übernimmt diese Steuerungsaufgabe.\n\n\nUm die Konsistenz der Daten sicherzustellen, müssen die Verwaltungsdaten, d. h. Verzeichnisse, Attribute und Speicherplatzzuweisungen (\"Metadaten\"), koordiniert gespeichert werden. Hierzu wird üblicherweise ein Metadaten-Server eingesetzt, der all diese Daten von den verschiedenen Teilnehmern am Cluster-Dateisystem übermittelt bekommt, üblicherweise über ein Ethernet. Dieser übernimmt auch die Koordinierung der Caches und der Dateisperren (Locks). In manchen Cluster-Dateisystemen kann der Metadaten-Server auch andere Aufgaben übernehmen, bei anderen (CXFS) müssen ein oder mehrere dedizierte Metadatenserver eingesetzt werden um die Betriebssicherheit zu erhöhen.\n\nGelingt den Servern aufgrund einer Störung im Netzwerk die Synchronisation nicht, so besteht die Gefahr von Inkonsistenzen. Üblicherweise wird das betroffene Dateisystem sich dann auf all jenen Servern herunterfahren, die (sich selbst eingerechnet) nur noch maximal 50 % der Gesamtheit an Servern sehen können. Da es nur maximal eine Gruppe geben kann, die mehr als 50 % der Server umfasst, bleibt nur diese aktiv, es können keine Inkonsistenzen entstehen.\nMan sagt auch, das Quorum liegt bei über 50 %.\n\nAus dem Quorum ergibt sich, dass ein so konfiguriertes Dateisystem mindestens drei Server benötigt, wenn Hochverfügbarkeit erwünscht ist. Diese sollten dann konsequenterweise auch in getrennte Infrastrukturen eingebunden sein, das heißt, drei Server-Räume in unterschiedlichen Brandabschnitten, drei unterbrechungsfreie Stromversorgungen usw.\n\nEin Cluster-Dateisystem, das lediglich eine gemeinsame Datenbasis für eine Vielzahl parallel arbeitender Server dient, muss zwar nicht zwangsläufig hoch verfügbar sein, doch wird man eine solche Serverfarm ohnehin aus viel mehr als zwei Rechnern aufbauen. Die Mehr-Raum-Notwendigkeit stellt sich bei solchen Anwendungen zunächst nicht.\n\nIm Gegensatz zu der weitgehend autarken Position der Server in einem Cluster-Dateisystem steht der Zugriff auf Dateien über ein \"Netzwerk\", z. B. über Network File System (NFS) auf Unix-Systeme, über Netware von Novell oder über SMB von Microsoft. Hier „gehört“ der Plattenplatz einem bestimmten Server, der den Datenzugriff vermittelt. Fällt er aus, ist das betroffenen Dateisystem nicht verfügbar.\n\nDie dritte Möglichkeit des verteilten Zugriffs auf Dateien ist die Verwendung von \"Raw Devices\". Hier verzichtet man ganz auf Dateisysteme und überlässt es der Anwendung, den verfügbaren Platz auf dem betreffenden Plattensystem zu verwalten. Die Anwendung muss also gegebenenfalls die Synchronisation zwischen den Servern durchführen und mit Störungen umgehen. Moderne Betriebssysteme erlauben es, auch Anteile eines physischen Plattensystems als Raw Devices zu benutzen, während andere Anteile für Dateisysteme reserviert werden.\n\n"}
{"id": "632917", "url": "https://de.wikipedia.org/wiki?curid=632917", "title": "Cluster Manager", "text": "Cluster Manager\n\nEin Cluster Manager ist eine Software für das Management eines Computerclusters (Verbund aus mehreren Rechnern). Er steht für Verwaltungsvorgänge im Cluster zur Verfügung und automatisiert meist Vorgänge wie den Failover vom Primärsystem zum Standby-System im Fehlerfall sowie Switchover für Wartungszwecke. Als Synonym ist auch der Begriff Clusterware gebräuchlich.\n\nFür die Erkennung eines Ausfalls eines Rechners im Cluster wird meist der sogenannte Cluster Heartbeat in Verbindung bzw. in Ergänzung mit einem Quorum verwendet, ein Signal, das über eine Netzwerkverbindung zwischen den Clusterknoten (einzelne Rechner im Cluster) ausgetauscht wird. Verstummt der Heartbeat eines Rechners, wird ein Ausfall desselben angenommen und ein Failover (eine Übernahme der Dienste wie Datenbanken, Webserver oder Application Server) eingeleitet.\n\nEinige Cluster Manager bieten die Möglichkeit, eigene Agenten zu entwickeln. Diese Variante deckt den Fall ab, dass ein Rechner zwar verfügbar ist und sein Cluster Heartbeat weiter über das Cluster-Netzwerk gesandt wird, jedoch der Applikationsdienst (zum Beispiel ein Datenbankservice) nicht mehr zur Verfügung steht. So kann ein Cluster Agent prüfen, ob eine Datenbankverbindung möglich ist. Sofern diese nicht erfolgreich ist, kann ebenfalls mittels des Agents ein Failover initiiert werden.\n\nTypische Cluster Manager sind\n\n\n\n"}
{"id": "632977", "url": "https://de.wikipedia.org/wiki?curid=632977", "title": "Calimero", "text": "Calimero\n\nCalimero (jap. ) ist eine italienische Zeichentrickfigur, zu der eine japanische Anime-Fernsehserie und in den 2010er Jahren eine Computeranimationsserie produziert wurde. Die Figur eines schwarzen Kükens mit einer Eierschale auf dem Kopf wurde bereits im Juli 1963 als Werbefigur erfunden.\n\nDie Anime-Fernsehserie handelt von Calimero, der in Palermo lebt und mit seiner Freundin Priscilla und weiteren Bewohnern viele Abenteuer erlebt.\n\n1972 produzierte das japanische Studio Toei Animation eine 47-teilige Anime-Fernsehserie mit der Figur Calimero. Dabei führten Kazuya Miyazaki, Takeshi Tamiya und Yugo Serikawa Regie, das Charakter-Design stammt von Shinya Takahashi und die künstlerische Leitung übernahm Hideo Chiba. Die Erstausstrahlung fand vom 6. Januar 1972 bis 6. Juni 1973 durch den Sender TROS in den Niederlanden statt. Die Serie wurde in Japan vom 15. Oktober 1974 bis zum 30. September 1975 von TV Asahi ausgestrahlt. \n\nDer Anime wurde unter anderem auf Französisch auf dem Sender TF1, RTL4 und TROS ausgestrahlt. Die Serie wurde auch auf Italienisch übersetzt. Die deutschsprachige Ausstrahlung fand am 5. Oktober 1972 auf dem Sender ZDF statt.\n\nIm Jahr 1992 wurde eine zweite Staffel mit 52 Folgen von Toei Animation produziert. Diese wurde erstmals vom 15. Oktober 1992 bis zum 9. September 1993 ausgestrahlt. Sie wurde auch auf Französisch auf Teletoon TPS und 1996 auf TF1 gezeigt. \n\nDie Musik der 1. Staffel wurde von Tadashi Kinoshita komponiert. Der Vorspanntitel des Originals, \"Boku wa Calimero\" (, \"Boku wa Kalimero\") und die Abspannlieder \"Suki nano Priscilla\" (, \"Suki nano Purishira\") und \"Kono Kao da are?\" () wurden gesungen von Rina Yamazaki. Die Texte stammen von Taichi Yamada und Takeshi Yoshida.\n\nDie Musik der 2. Staffel stammt von Junnosuke Yamamoto. Als Vorspannlied wurde \"Nē! Calimero\" () und als Abspannlied \"Koi ni Ki o Tsukete\" () verwendet, jeweils gesungen von Raspberry.\n\n"}
{"id": "633130", "url": "https://de.wikipedia.org/wiki?curid=633130", "title": "Oracle Cluster Ready Services", "text": "Oracle Cluster Ready Services\n\nOracle Cluster Ready Services (kurz CRS oder OCRS) ist der Cluster Manager der Firma Oracle. Inzwischen wird das Teilprodukt des Oracle RAC unter dem Namen Oracle Clusterware (kurz OCW) angeboten.\n\nEr steht ab Version 10.2 für Oracle-Datenbanken und Drittanwendungen nach dem System-V-init-Verfahren (start|stop) zur Verfügung, unterstützt einen Aktiv/Aktiv-Cluster unter dem Namen Oracle Real Application Cluster, sowie Services und Applikationen inklusive eines Failovers für einen Aktiv/Passiv-Cluster.\n\n\n\n"}
{"id": "633175", "url": "https://de.wikipedia.org/wiki?curid=633175", "title": "Aktiv/Aktiv-Cluster", "text": "Aktiv/Aktiv-Cluster\n\nEin Aktiv/Aktiv-Cluster ist ein Rechnerverbund, in dem mehrere Rechner (auch als Clusterknoten bzw. Clusternodes bezeichnet) gleichzeitig aktiv sind. Bei Aktiv/Aktiv-Clustern wird zwischen den Architekturen Shared Nothing und Shared All unterschieden. \n\nComputercluster werden neben der Verteilung der Rechenleistung auch u. a. zur Sicherstellung der Verfügbarkeit von diversen Ressourcen wie Netzwerke, Applikationen etc. verwendet.\n\nUnter einer Aktiv/Aktiv-Konfiguration versteht man in diesem Zusammenhang, dass die so gesicherte Ressource, also zum Beispiel eine Datenbank, auf allen Clusterknoten aktiv ist. Wenn ein Knoten ausfällt, übernehmen die übrigen Knoten die Prozesse des ausgefallenen Knotens, es gibt praktisch keinerlei Ausfallzeiten, eventuell jedoch starke Einbußen in der Performance, da die gleiche Last nun von weniger Systemen übernommen werden muss.\n\nKlarer Vorteil dieses Konzeptes ist, dass die Ressourcen nicht redundant vorhanden sein müssen und der Ausfall eines Knotens nur leistungsabhängige Auswirkungen auf die Verfügbarkeit des Clustersystems als Ganzes hat.\n\nHauptnachteil eines solchen Setups ist, dass die Ressource, anders als bei einer Aktiv/Passiv-Konfiguration, entweder direkt eine Aktiv/Aktiv-Konfiguration unterstützen oder mit teils hohem Aufwand an eine solche angepasst werden muss. \nBei einer Aktiv/Passiv-Konfiguration hingegen können nahezu beliebige Ressourcen verfügbar gemacht werden.\n\nEs gibt in Aktiv/Passiv-Clustern ebenfalls die Option des „Hot Standby“. \nDies bedeutet dass die Clusterressource auf allen Knoten aktiv ist und dass sie sofort nach Ausfall des Primärknotens übernommen werden kann, ohne dass der Prozess erst (langwierig) auf dem Sekundärsystem gestartet werden müsste. \nDie Failover-Zeit kann so stark minimiert werden. \nDa das Sekundärsystem hierbei jedoch immer noch redundant ausgelegt ist und keine Lastverteilung oder Ähnliches stattfindet, ist dieses Verfahren kein echtes Aktiv/Aktiv-Clustering.\n\n"}
{"id": "633191", "url": "https://de.wikipedia.org/wiki?curid=633191", "title": "Loogie", "text": "Loogie\n\nLoogie.net ist eine Arbeit des Schweizer Medienkünstlers Marc Lee, der es 2002 als „ersten interaktiven Nachrichten-Fernsehsender“ gründete.\n\nDie Software sucht zu einem frei wählbaren Thema Texte, Bilder und Filmbeiträge aus dem Internet und präsentiert das Material im typischen Stil einer Nachrichten-Website (\"Loogie.net NEWS\") oder wie eine CNN-Nachrichtensendung im Fernsehen (\"Loogie.net TV\"). Da kein menschlicher Eingriff zur Ordnung und Wertung erfolgt, entsteht eine künstlerische Mischung aus nur scheinbar „maßgeschneiderter“ Information und Karikatur der Echtzeitnachrichten im Fernsehen und Internet.\n\nNeben dem zweisprachigen Fernsehprogramm (Deutsch und Englisch) erzeugt Loogie.net zurzeit drei Websites in verschiedenen Sprachen. Die Technik ist zurzeit im Zentrum für Kunst und Medientechnologie Karlsruhe installiert.\n\nLoogie.net erhielt den \"Förderpreis Diplomjahr 2003 HGKZ\". Die Jury würdigte den \"„intelligenten wie kritischen Kommentar auf unser Medien- und Informationszeitalter“\".\n\n"}
{"id": "633995", "url": "https://de.wikipedia.org/wiki?curid=633995", "title": "Windows-Explorer", "text": "Windows-Explorer\n\nDer Windows-Explorer (kurz \"Explorer\" und auch \"Datei-Explorer\" genannt) bildet den voreingestellten Dateimanager und die Desktop-Umgebung in der Windows-Betriebssystemsfamilie seit Windows 95 und ist seitdem integraler Bestandteil dieser Betriebssysteme.\n\nDer „Windows-Explorer“ erfüllt zwei Aufgaben: Zum einen ist er ein Dateiverwaltungsprogramm und wurde erstmals mit Windows 95 eingeführt. Er ist damit der Nachfolger des von Windows 3.x bekannten \"Datei-Managers\", der noch bis Windows ME von Microsoft ausgeliefert wurde. Bereits in der Windows-9x-Serie war dieses vorherige „Datei-Manager“ genannte Dateiverwaltungsprogramm standardmäßig nicht mehr als Verknüpfung über das Windows-Startmenü, sondern lediglich im Windows-System-Verzeichnis aufrufbar.\n\nIn zweiter Funktion ist der sogenannte \"Desktop\" eine Instanz des Windows-Explorers:\n\nAllerdings bot Windows 95 (lediglich die Ur- oder A-Version) dem Benutzer noch während der Installation an, das klassische Duo Programm- und Datei-Manager (aus Windows-3.x) als Hauptbestandteile der grafischen Benutzeroberfläche und Arbeitsumgebung (Shell) zu verwenden anstatt des Windows-Explorers samt Startmenü und Desktop.\n\nDer \"Windows-Explorer\" stellt die (Haupt-)Funktionen für die Arbeitsoberfläche zur Verfügung und gehört damit zu den sogenannten Shell-Programmen. Das heißt, er wird nach dem Start des eigentlichen Betriebssystems gestartet und stellt die Bildschirmelemente zur Verfügung (vergleichbar mit dem \"Panel\" bei GNOME oder \"Plasma\" bei KDE). Seit Windows 95 gehören dazu die Taskleiste (mit dem \"Start\"-Knopf für das Windows-Startmenü) sowie der eigentliche Desktop (ein speziell angepasster System-Ordner), auf dem Verknüpfungen, aber auch andere Dateien liegen können. In Windows Server 2012 und Windows 8 wurde der Oberfläche ein Menüband (englisch \"\" genannt) hinzugefügt.\n\nGrundsätzlich können alle Windows-Betriebssysteme der NT-Linie auch ohne den Explorer als Shell verwendet werden; bei der Windows-9.x-Linie sind wesentliche Betriebssystemfunktionen in das GUI integriert, hier ist daher das Betriebssystem nicht ohne den Desktop betreibbar; stattdessen konnte eine „parallel installierte“ DOS-Version gestartet werden.\n\nDer eigentliche Dateiverwalter ist eine spezielle Fenster-Ansicht innerhalb des Explorers und kein eigenes Programm. Diese Ansicht wird im normalen Betrieb erst dann sichtbar, wenn zuvor die Arbeitsoberfläche gestartet wurde.\n\nNeben dem Windows-Explorer existieren für Microsofts Windows auch Dateiverwaltungs- und Shell-Programme von Fremdherstellern.\n\n\n"}
{"id": "634224", "url": "https://de.wikipedia.org/wiki?curid=634224", "title": "Brüsselator", "text": "Brüsselator\n\nDer Brüsselator ist ein einfaches Modell zur Beschreibung chemischer Oszillatoren. Der Brüsselator wurde von Ilya Prigogine und René Lefever an der Université Libre de Bruxelles in Belgien entwickelt, daher auch der Name.\n\nEs handelt sich um ein System von vier hypothetischen Reaktionsgleichungen, die ein einfaches Modell bilden, das alle Phänomene von chemischen Oszillatoren (wie der Belousov-Zhabotinsky-Reaktion) widerspiegelt. Ein ähnliches Modellsystem wurde 1985 an der Humboldt-Universität zu Berlin durch Vereinfachung aus einem real existierenden Reaktionssystem abgeleitet.\n\nDie Reaktionsgeschwindigkeiten werden durch die Konstante \"k\" bis \"k\" widergespiegelt. Die Konzentrationen von A und B werden immer konstant gehalten und die Produkte C und D werden ständig abgeführt. Die Konzentrationen X und Y reagieren empfindlich auf kleine Störungen und erreichen schnell einen oszillierenden Zustand, wenn die Gesamtreaktion weit vom Gleichgewichtszustand entfernt ist. Man hat also ein thermodynamisch offenes System und kann zwei Ratengleichungen für die Konzentration von X und Y aufstellen:\n\nDiese Differentialgleichungen können numerisch gelöst werden. Nebenstehende Abbildung zeigt einige Lösungen. Je nach Wahl der freien Parameter \"kA\", \"kB\", \"k\" und \"k\" ergibt sich unterschiedliches Verhalten. Im oberen Fall sieht man stabile Oszillationen, während für eine andere Wahl der Parameter die Konzentrationen \"X(t)\" und \"Y(t)\" einem Fixpunkt im Phasenraum zustreben.\n\nWie schon im obigen Bild gezeigt hat der Brüsselator je nach Parametrisierung stabile Oszillationen als Lösung oder strebt im Phasenraum einem Fixpunkt zu. Der Fixpunkt ergibt sich über d\"X\"/d\"t\" = d\"Y\"/d\"t = 0\" zu:\nMit Hilfe der linearen Stabilitätsanalyse lässt sich weiter zeigen, dass dieser Fixpunkt instabil wird, wenn gilt:\nIn diesem Fall streben die Trajektoren \"(X(t), Y(t))\" einem Grenzzyklus im Phasenraum zu und das System führt die gezeigten Oszillationen aus.\n\nMan kann das Modell auch auf ein Reaktions-Diffusions-Modell erweitern und erhält dann bei Wahl der richtigen Parameter Chemische Wellen als Lösung, wie sie in der Animation rechts gezeigt sind.\n\nDie Differentialgleichungen werden um einen Diffusionsanteil formula_6, bzw. formula_7 erweitert und lauten dann:\nHierin ist formula_10 ein Punkt im Raum und formula_11 bezeichnet den Laplace-Operator, also in kartesischen Koordinaten die Summe der zweiten räumlichen Ableitungen.\n\n\n"}
{"id": "634283", "url": "https://de.wikipedia.org/wiki?curid=634283", "title": "Mandriva Linux", "text": "Mandriva Linux\n\nMandriva Linux bzw. Mandrake bezeichnen die vom französischen Unternehmen \"Mandriva\" bzw. \"MandrakeSoft\" mit Sitz in Paris entwickelten GNU/Linux-Distributionen, deren Betriebssystem-Teile wie Microsoft Windows als Betriebssystem auf PCs und Laptops einsetzbar sind. Als erste(s) Release bzw. Version der Mandrake-Distribution, bestehend aus den Betriebssystemteilen und aller darauf laufenden Programme, erschien \"Mandrake 5.1\" im Sommer 1998.\n\nAus der Fusion des französischen Software-Unternehmens MandrakeSoft mit dem brasilianischen Unternehmen Conectiva ging im Februar 2005 die Firma Mandriva S. A. hervor. 2011 erschien die letzte Version der Mandriva-Distribution. Sie wurde von Mandriva sowie zusammen mit dem russischen Partner ROSAlab herausgegeben. Mandriva versuchte nun mit anderen Produkten und Dienstleistungen stärker im Unternehmensumfeld Fuß zu fassen und gab die Entwicklung von Distributionen an die Entwicklergemeinde zurück.\n\nSeit Mai 2015 befindet sich die Firma Mandriva im Verfahren einer gerichtlichen Liquidation (). Am 10. Oktober 2017 wurde das Unternehmen im französischen Unternehmensregister abgemeldet.\n\nAm 23. Juli 1998 veröffentlichte Gaël Duval seine erste, auf Red Hat Linux basierende Mandrake-Distribution. Kurz darauf gründete er zusammen mit Jacques Le Marois und Frédéric Bastok das Unternehmen MandrakeSoft. Der Börsengang folgte am 30. Juli 2001. Knapp eineinhalb Jahre später, am 13. Januar 2003, musste MandrakeSoft einen Antrag auf Gläubigerschutz stellen, da das Unternehmen in die Krise geraten war. Das Unternehmen strukturierte sich in der Folge um. Viele Beitragende aus der Mandrake-Gemeinschaft hielten der Distribution trotz der finanziellen Situation die Treue. MandrakeSoft konnte dadurch am 30. März 2004 den Gläubigerschutz wieder verlassen und den laufenden Betrieb wiederherstellen. Für den Abbau der Schulden wurden maximal 9 Jahre angesetzt.\n\nMandrakeSoft begann zügig mit der Expansion: Am 17. September 2004 übernahm MandrakeSoft den Open-Source-Dienstleister Edge It. Am 24. Februar 2005 folgte der brasilianische Linux-Distributor Conectiva für etwa 1,79 Millionen Euro. Als Ziel der Übernahme wurde vor allem eine Erweiterung des Bereiches Forschung und Entwicklung genannt. 2005 benannte sich das Unternehmen in \"Mandriva\" und ihre Linux-Distribution in \"Mandriva Linux\" um. Dadurch wurde einerseits der Rechtsstreit mit der Hearst-Verlagsgruppe bezüglich eines gleichnamigen Comic-Zauberers umgangen, andererseits wurde der Zusammenschluss mit Conectiva verdeutlicht. Am 16. Juni wurde die Übernahme des Distributors Lycoris bekannt gegeben. Der Streit mit der Hearst-Verlagsgruppe endete am 19. Juli mit einem Vergleich.\n\nNach Mandrake Linux 10.1 erschienen neue Versionen zeitweise nur noch einmal jährlich. Außerdem wurde die bis dahin geläufige Nummerierung der Versionen aufgegeben; die nächste Version hieß nicht wie erwartet 10.2, sondern trug die Bezeichnung Limited Edition 2005. Diese war keine offizielle Verkaufsversion, sondern eine Übergangsveröffentlichung, welche nur als Download angeboten wurde.\n\nNach dem Release Mandrake Linux 10.1 von 2004 erschien im nächsten Jahr eines mit neuem Namen: \"Mandriva\" Linux 2005 LE. Der Grund für den geänderten Veröffentlichungszyklus war hauptsächlich die Übernahme von Conectiva und die damit verbundene Integration in Mandriva Linux.\n\nIm Oktober 2006 übernahm Mandriva das französische Software-Unternehmen Linbox FAS für etwa 1,7 Millionen Euro, welches größtenteils durch Aktien finanziert wurden.\n\nAnfang 2007 kündigte Mandriva – vermutlich auch wegen wachsender Unzufriedenheit in der Community – an, mit einigen kleineren Änderungen zum halbjährlichen Veröffentlichungszyklus zurückzukehren. Während der verkürzte Zyklus der Aktualität der Applikationssoftware zugutekommen soll, sollen Desktop-Komponenten darüber hinaus 12 Monate und das Basissystem 18 Monate lang mit Aktualisierungen versorgt werden.\n\nMandriva SA fand weiterhin mit der russischen Firma ROSAlab einen neuen Partner für die Entwicklung der Distribution Mandriva Linux. Gemeinsam entwickelte man die letzte Version des Mandriva Powerpack 2011. Die letzte Veröffentlichung die im Sommer 2011 erschien trug den Namen Mandriva 2011 Hydrogen.\n\nSeitens Mandriva wurde die Entscheidung getroffen, sich eher auf Enterpriseprodukte zu konzentrieren und neue Produkte und Dienstleistungen wie beispielsweise Mandriva Business Server und Mandriva ServicePlace zu entwickeln.\n\nSeit Mai 2015 befindet sich das Unternehmen in Liquidation.\n\nDie Distribution Mandriva Linux wurde an eine unabhängige Association nach französischen Recht abgegeben. Der Name der Association und der Distribution wurden durch eine Umfrage auf „OpenMandriva Association“ und „OpenMandriva Lx“ bestimmt. Am Freitag, den 22. November 2013 wurde dann die erste Veröffentlichung von „OpenMandriva Lx“ bekannt gegeben.\nOpenMandriva basiert auf der russischen Linux-Distribution ROSA Linux, die ursprünglich im Jahr 2010 als Abspaltung aus Mandriva hervorgegangen war.\n\nIm September 2010 wurde vom Mandriva-Entwickler Jérôme Quelin eine Abspaltung () unter dem Namen Mageia angekündigt. Anlass war der unsichere Fortbestand des Mandriva-Projekts, weil nach der Liquidation der zu Mandriva gehörenden \"Edge-IT\" die für Mandriva-Linux arbeitenden Angestellten entlassen worden waren. Seitdem wird die Weiterentwicklung der Linux-Distribution als ein Community-Projekt von \"Mageia.Org\", ebenfalls eine Vereinigung nach französischen Recht (konstituiert von Jérôme Quelin und 16 weiteren Gründungsmitgliedern), fortgeführt.\n\nSo erschienen von Mageia.Org bisher vier Versionen der Distribution. Ein fünftes Release ist als \"Mageia 5\" am 20. Juni 2015 veröffentlicht worden. Der Nachfolger \"Mageia 6\" wurde im Juli 2017 veröffentlicht und steht seit dem als aktuellste Version zum Download zur Verfügung.\n\nDie Systemkonfiguration wurde bei Mandrake und Mandriva Linux mit eigenen Werkzeugen im Rahmen der \"Drak-Tools\" konfiguriert – hierzu stand eine zentrale grafische Oberfläche (Mandriva-Kontrollzentrum) zur Verfügung.\n\nMandriva Linux verwendete die Paketverwaltung urpmi, welche die Verwaltung von RPM-Paketen automatisiert. Pakete konnten sowohl von lokalen als auch entfernten Quellen installiert und aktualisiert werden. Abhängigkeiten und mögliche Konflikte wurden dabei überwacht und nötigenfalls aufgelöst. Die Softwareverwaltung erfolgte über das Frontend \"RPMDrake\", welches Teil der Drak-Tools ist.\n\nDas System der offiziellen Paketquellen wurde während der zweiten Jahreshälfte 2006 schrittweise umgestellt. Die Quellen \"main\" (Pakete des Basissystems), \"contrib\" (von der Community beigesteuerte Pakete) und \"updates\" (Aktualisierungen) existieren in der alten Form nicht mehr. Stattdessen gliederten sich die Paketquellen nun in die Zweige \"main\" (Basissystem), \"contrib\" (Community-Pakete) und \"nonfree\" (proprietäre Software). Jeder Zweig verfügte über eine \"release\"-, \"updates\"- und \"backports\"-Quelle. Der Vorteil lag darin, dass sich das Betriebssystem sehr viel selektiver aktualisieren ließ.\n\nDa nicht alle Pakete offiziell von Mandriva unterstützt werden, empfahl sich das Einrichten zusätzlicher Paketquellen mit z. B. EasyUrpmi.\n\nMandriva Linux 2007.0 war bei seiner Veröffentlichung die erste Linux-Distribution mit integrierter Unterstützung für 3D-Desktop<nowiki>effekte</nowiki> mittels Beryl.\n\nSeit Mandriva Linux 2007.1 war im \"Mandriva Control Center\", einer Oberfläche zum ändern der Grundeinstellungen, ein Dialog zum Wählen des 3D-Desktops vorhanden. Dort konnte zwischen Beryl, Compiz und dem Mandriva-eigenen Metisse umgeschaltet werden. Außerdem konnte mit Nvidia-Grafikkarten auch zwischen der AIGLX-Erweiterung für X.org und Xgl gewählt werden. Bei weniger leistungsfähiger Hardware waren die 3D-Effekte auch ganz abschaltbar.\n\nAb Mandriva 2009.0 wurde KDE Plasma Desktop als voreingestellte Arbeitsoberfläche bereitgestellt. 3D-Effekte waren in der Standardeinstellung aktiviert. Jedoch konnten weiterhin andere Fenstermanager wie z. B. Compiz genutzt werden.\n\nAnfangs erhielten die Anwender durch die kostenpflichtige Mitgliedschaft im Mandriva Club frühzeitigen Zugriff auf die offiziellen Downloads der Distribution sowie andere Vorteile. Häufig standen die Downloads der CD/DVD-Abbilder Clubmitgliedern einige Tage vor der Veröffentlichung zur Verfügung. Außerdem unterhielt Mandriva eine Sammlung an Programmpaketen, welche nur Clubmitgliedern zur Verfügung steht. Diese beinhalten meist proprietäre Software, für deren Vertrieb Mandriva Lizenzgebühren aufbringen musste.\n\nDie monatlichen Beiträge betrugen ab 5 US-Dollar aufwärts. Dies wurde nun geändert, da viele Mitglieder der Mandriva-Community mit dem Club unzufrieden waren. Der Club wurde von den kommerziellen Angeboten abgekoppelt, um so größere Offenheit zu erreichen, wie es bei Linux-Distributionen wie openSUSE, Fedora und Ubuntu bereits der Fall ist.\n\nEine Mitgliedschaft im Mandriva Club stand später allen registrierten Benutzern offen. Seitens Mandriva SA wurde aber weiterhin die Möglichkeit gegeben, ein 12-monatiges Abonnement für das kommerzielle Powerpack zu erwerben. In der Zeit des Abonnements konnte sich der Kunde das Powerpack über BitTorrent oder FTP-Server herunterladen (inklusive proprietärer Software).\n\nDa es sich bei Mandriva Linux um eine Distribution mit gewerblichem Hintergrund handelte, konnten Anwender auf zwei Wegen technische Unterstützung erhalten.\n\n\nDie Mandriva-Expert-Plattform stellt eine Kombination beider Möglichkeiten dar. Hier können Probleme kostenfrei anderen Anwendern geschildert werden und bei Bedarf auf kommerziellen Support zurückgegriffen werden.\n\nDie Distribution war in mehreren, unterschiedlich ausgerichteten Varianten erhältlich:\n\nMandriva One war eine frei herunterladbare Kombination aus Live-System und Installationsmedium auf einer einzigen CD. Die Installation fand im Live-Betrieb statt. Aufgrund des beschränkten Speicherplatzes befanden sich jedoch nur die wichtigsten Anwendungen auf der CD. Nutzer mit weitergehenden Ansprüchen konnten im Anschluss an die Installation weitere Programme über die Paketverwaltung herunterladen und installieren. Es standen verschiedene Varianten mit KDE-Plasma- oder Gnome-Desktopumgebung bzw. proprietären Treibern zur Verfügung.\n\nMandriva Linux Free nannte sich die kostenlose Download-Version ohne proprietäre bzw. kommerzielle Software. Sie ließ sich entweder als Satz von 4 CDs oder als einzelne DVD herunterladen, wobei die DVD-Ausgabe sowohl 32-Bit- wie auch 64-Bit-Pakete enthielt. Es war bereits eine große Menge an zusätzlicher Software auf den Installationmedien vorhanden. Auch hier ließen sich nach der Installation zusätzliche Paketquellen einrichten, um Programmpakete aus dem Internet nachzuinstallieren.\n\nDas Mandriva PowerPack war auf ambitionierte Computernutzer und Softwareentwickler zugeschnitten; die Installationsmedien enthielten eine entsprechend größere Softwareauswahl. Beispielsweise befanden sich eine DVD-Abspielsoftware sowie Audio-/Videocodecs im Powerpack.\n\nMandriva Business Server wendet sich an kleine und mittelständische Unternehmen und ist in der SOHO-Variante (ohne Support) kostenlos erhältlich. Das Produkt ist vollständig auf Deutsch verfügbar und bietet eine Anbindung an ein Online-Angebot, von dem aus sich Unternehmensanwendungen wie beispielsweise Groupware-Software etc. installieren lassen. Weiterhin ist die Software OwnCloud für die Erstellung von Cloud-Computing Systemen verfügbar.\n\nEs existieren einige auf Mandriva Linux basierende Distributionen. Ein im breiten Einsatz befindliches Derivat ist PCLinuxOS. PCLinuxOS ist vor allem für den Desktop-Einsatz bestimmt, während der Server-Einsatz eine geringere Rolle spielt. Von der deutschen Community wird seit 2009 auch eine Netbookversion erstellt. Sie wird ausschließlich von der deutschen Community betreut und nicht von Mandriva selbst.\n\nAus der Zusammenarbeit zwischen der russischen Firma ROSAlab und Mandriva SA entstand nicht nur die Version Mandriva Linux 2011, sondern auch der sogenannte ROSA Desktop für den russischen Markt. Die Version ROSA Desktop Enterprise konzentriert sich dabei auf Langzeitunterstützung, die Version „Fresh“ hingegen auf die Auslieferung aktueller Software. ROSAlab und die OpenMandriva Association arbeiten dabei eng zusammen, zum Beispiel bei der Paketierung und bezüglich der Infrastruktur (ABF – Automated Build Farm), die weitgehend von ROSAlab getragen wird.\n\nWeiterhin wird ein dem OpenMandriva Projekt freundlich gesinntes Projekt namens „Moondrake“ entwickelt, welches klassische Komponenten wie beispielsweise die klassische Installationsroutine von Mandrake Linux verwendet.\n\n"}
{"id": "634284", "url": "https://de.wikipedia.org/wiki?curid=634284", "title": "Tesseract (Software)", "text": "Tesseract (Software)\n\nTesseract ist eine freie Software zur Texterkennung. Schwerpunkt ist die Erkennung von Textzeichen bzw. Textzeilen, aber auch die Zerlegung eines Textes in Textblöcke (Layoutanalyse) kann Tesseract übernehmen. Zur Verbesserung der Erkennungsraten verwendet Tesseract Sprachmodelle wie beispielsweise Wörterbücher. \n\nFür weit mehr als 100 Sprachen und Sprachvarianten sind bereits Texterkennungsdaten in Zusatzmodulen vorhanden. Tesseract unterstützt dabei nicht nur lateinische Antiqua-Schriften, sondern auch Fraktur-Schrift, Devanagari (indische Schrift), chinesische, arabische, griechische, hebräische, kyrillische und weitere Schriften.\n\nUrsprünglich wurde die Software zwischen 1984 und 1994 von Hewlett-Packard entwickelt.\nAus einem Test der University of Nevada, Las Vegas (UNLV) ging sie 1995 als einer der drei präzisesten Testkandidaten hervor. Nach dem Ausstieg von HP aus dem OCR-Markt lag die Entwicklung weitgehend brach, bis der Code 2005 an das Information Science Research Institute der UNLV übergeben wurde. Hier wurde festgestellt, dass der ehemalige Entwickler Ray Smith mittlerweile bei Google arbeitete. Nach einer Nachfrage bei Google, ob Interesse an dem Code bestünde, nahm sich Google des Quelltextes an, brachte ihn auf einen aktuellen Stand und gab ihn noch im selben Jahr unter der Apache-Lizenz über SourceForge frei.\n\nDies bedeutete in der Welt der freien Software einen großen Qualitätssprung im Bereich der Texterkennung. Das Projekt migrierte von SourceForge auf Googles eigene Software-Entwickler-Plattform Google Code, wo es unter Betreuung von Google weiterentwickelt wurde. Seit 2015 findet die Weiterentwicklung auf GitHub statt.\n\nSeit 2006 wird das Programm als Grundlage von Google Books weiterentwickelt. Ab Version 3.0 vom September 2010 können Ergebnisse direkt in das hOCR-Format ausgegeben werden und es wurde ein neues Modul zur Analyse der Seitengestaltung eingeführt.\n\nIn der Version 3.02 vom 28. Oktober 2012 wurde u. a. die Erkennung arabischer und hebräischer Texte im bidirektionalen Modus eingeführt.\n\nDas Projekt \"tesseractindic\" widmet sich der Aufgabe, das Programm mit Sprachen aus der indischen Sprachenfamilie verwendbar zu machen.\n\nEnde 2016 führte Tesseract ein neuronales Netz zur Texterkennung ein. Version 4 unterstützt diese neue Methode, kann aber auch weiterhin mit dem Mustervergleich der Vorgängerversionen arbeiten.\n\nSeit Dezember 2018 kann Tesseract die OCR-Ergebnisse im standardisierten ALTO-Format ausgeben.\n\nGoogle verwendet nach eigener Aussage Tesseract für die Texterkennung auf mobilen Geräten und in Videos sowie bei der Erkennung von Spam in E-Mail-Bildern.\n\nTesseract wird nach den unter Unix üblichen Konventionen auch unter Windows von der Kommandozeile aus gesteuert und hat folgendes Format:\nTesseract liest das Bild im Tagged Image File Format (TIFF) ein und gibt den Text in die Ausgabedatei weiter. Ältere Versionen von Tesseract hatten keine eigene Layoutanalyse, waren also auf externe Software wie beispielsweise OCRopus angewiesen, um Textspalten auf einzelne Bilddateien zu verteilen. Aktuelle Versionen nutzen die Programmbibliothek Leptonica für die Analyse der Seitengestaltung, aber auch für die direkte Verarbeitung aller gängigen Bildformate.\n\nEine automatisierte Verarbeitung lässt sich zum Beispiel mit ImageMagick verwirklichen.\n\nTesseract kann ab Version 3 die Scan-Ergebnisse im hOCR-Format speichern, wodurch die Seitengestaltung erhalten bleibt. Auch durchsuchbare PDF-Dateien lassen sich mit dieser Version direkt erzeugen.\n\nEs existiert eine Reihe Software, die Tesseract als Backend einbindet.\nTesseract kann als Zeichenerkennungsmodul in OCRopus verwendet werden, das zusätzlich noch Analyse der Dokumentgliederung und statistische Sprachmodelle bietet.\nAllerdings benutzt OCRopus ab Version 0.4 standardmäßig ein eigenes Zeichenerkennungsmodul basierend auf neuronalen Netzen. In früheren Versionen wurde Tesseract als Standardmodul in OCRopus verwendet. Neben weiteren möglichen Backends kann es in der Desktop-OCR-Lösung \"OCRFeeder\" zur Zeichenerkennung genutzt werden. Mittels hocr2pdf dient es zum Beispiel in dem Linux-basierten Dokumentenmanagement-System \"Archivista\" der Erzeugung einer Text-Schicht zu rastergraphischen Abbildern eingescannter Papierdokumente, um diese maschinell durchsuchbar zu machen.\n\nTesseract wird als freie Software auch im Quelltext unter den Bedingungen von Version 2.0 der Apache-Lizenz (Apache Software License, ASL) verbreitet. In praktisch allen gängigen Linux-Distributionen kann es direkt aus den Standard-Paketquellen installiert werden. Installationsprogramme für Windows gibt es von mehreren Anbietern.\n\nTesseract dient u. a. bei folgenden Programmen als Basis der Texterkennung:\n\n\n"}
{"id": "634880", "url": "https://de.wikipedia.org/wiki?curid=634880", "title": "TOAD", "text": "TOAD\n\nToad ist die Bezeichnung mehrerer grafischer Software-Werkzeuge der Firma Quest Software für die Entwicklung und Administration unterschiedlicher SQL-Datenbanksysteme.\n\nDas ursprüngliche Produkt wurde für die komfortable Erstellung und das Debugging von SQL-Skripten sowie PL/SQL-Programmen für Oracle-Datenbanken entwickelt. Aus dieser Zeit stammt das Akronym (). Seit der Entwicklung von Versionen für Datenbanksysteme anderer Hersteller wird der Begriff „Toad“ von Quest Software nicht mehr als Akronym erklärt, sondern als registrierter Name einer Produktfamilie verwendet. TOAD ist inzwischen auch für Datenbanken wie IBM-DB2, Microsoft SQL Server und MySQL erhältlich.\n\nIm Laufe der letzten Jahre wurde die Funktionalität des Produkts erweitert, sodass es heute im Wesentlichen drei Anwendungsbereiche gibt: PL/SQL-Programmentwicklung, Datenbankadministration und Datenanalyse. Für die PL/SQL-Programmentwicklung stehen neben der Debuggingfunktion eine Codeüberprüfung (Code Tester), eine \" sowie diverse Tuning-Hilfen zur Verfügung. \n\nDer Funktionsumfang beinhaltet auch einen Vergleich von Objekten, Schemata oder Datenbanken, ein LogMiner Interface sowie die Möglichkeit in der gleichen oder einer anderen Datenbank Objekte, User, Tablespaces etc. mit identischem Aufbau anzulegen.\nAußerdem enthält TOAD Funktionen für weniger versierte Datenbank-Anwender und Analysten. Dazu gehören neben einem grafischen Query-Builder und der Möglichkeit, Ergebnisfenster nach Microsoft Excel zu exportieren, vor allen Dingen eine eigene \", sodass es möglich ist, aus TOAD heraus Unternehmensreports mit Grafiken etc. zu erstellen.\n\n\n"}
{"id": "637480", "url": "https://de.wikipedia.org/wiki?curid=637480", "title": "PC Games Hardware", "text": "PC Games Hardware\n\nPC Games Hardware (\"PCGH\" oder \"PCG Hardware\") ist eine monatlich erscheinende, deutschsprachige Computerzeitschrift. Sie wird von der in Fürth ansässigen Computec Media herausgegeben und beschäftigt sich hauptsächlich mit Hardware für den PC-Spieler.\n\nDie PCGH ist das Schwestermagazin der PC Games. Zwischen beiden Magazinen gibt es Synergien, das heißt, dass Redakteure der PCGH auch Artikel schreiben, die im Hardwareteil der PC Games erscheinen. Spieletests der PC Games werden häufig durch ausführlichere Tests zu den Hardwareanforderungen in der PCGH ergänzt. Außerdem arbeitet man auch beim Onlineauftritt zusammen.\n\nDie PC Games Hardware richtet sich hauptsächlich an PC-Spieler. Diese Leser besitzen im Allgemeinen ein gewisses Grundwissen über den Umgang mit dem Computer. Auf Grund dessen werden in den Artikeln der PCGH, im Gegensatz zu „Massen-Magazinen“ wie \"Computer Bild,\" banale Vorgehensweisen nicht behandelt. Stattdessen werden auch komplexere Themen rund um den PC behandelt.\n\nIn der PCGH befinden sich redaktionelle Artikel und Interviews vor allem zu folgenden Themen:\n\n\nSeit der Ausgabe 11/2000 ist Thilo Bayer Chefredakteur, zwischenzeitlich unterstützt von Kay Beinroth sowie Christian Gögelein. Er setzte sein Fachgebiet insbesondere auf Grafikkarten, arbeitet aber schon länger nicht mehr als Fachredakteur. Unterstützend als leitende Redakteure stehen ihm seit März 2007 Raffael Vötter (Print) und seit März 2009 Andreas Link (Online) zur Seite. Des Weiteren unterstützten über die Jahre hinweg sowohl zahlreiche freie Journalisten als auch kurzzeitige Redakteure die Redaktion. Das aktuelle Team von PC Games Hardware findet sich auf der Webseite.\n\nDie erste Ausgabe der PC Games Hardware, 11/2000, erschien am 6. Oktober 2000 und kostete 5,- DM. Damit war die PCGH, nach eigener Angabe, Deutschlands erstes Hardware-Magazin für PC-Spieler.\n\nDie Themen der PCGH werden in folgenden Rubriken behandelt: Aktuell, Test, Spiele, Praxis, Wissen und Service.\n\nDer ersten PCGH lag eine CD-ROM bei. Schon wenig später legte man zum ersten Mal zwei CDs bei. Diese CD-Version der PCGH gab es bis zur Ausgabe 06/2006. Dann wurde sie mit dem Verweis auf mangelnde Wirtschaftlichkeit eingestellt. Außerdem sah man die CD-ROM als nicht mehr zeitgemäß an.\n\nSeit dem Sommer 2004 gibt es die PCGH mit einer DVD-ROM zu kaufen. Zuerst war die DVD einseitig (DVD-5). Mit der Ausgabe 01/2005 wurde zum ersten Mal, wie bei der PC Games schon länger üblich, eine doppelseitige DVD (DVD-10) mit einer Kapazität von ca. 9,4 GByte mitgeliefert. Nach zwei weiteren einseitigen DVDs ist die Heft-DVD seit 04/2005 durchgehend zweiseitig (Stand Ausgabe 10/2006).\n\nAuf vielfachen Leserwunsch gibt es seit der Ausgabe 01/2006 zum ersten Mal auch eine günstigere Magazinversion ohne Datenträger.\n\nDie Ausgabe 08/2006 erschien erstmals als Premium-Version. Premium-Versionen gab es bereits kurz zuvor von der PC Games. Sie enthielt zusätzlich zur Standard-DVD-Ausgabe ein 36 DIN-A4-Seiten starkes Kompendium zu World of Warcraft. Die Ausgabe 11/2006 ist ebenfalls als Premium-Ausgabe mit limitierter Auflage erschienen und beschäftigte sich mit dem Prozessorhersteller AMD und seiner aktuellen Produktpalette. Die limitierte Premium-Ausgabe 12/2008 behandelte in einem Beiheft das Spiel Far Cry 2.\n\nDie Premium-Ausgabe wurde zur Ausgabe 12/2011 eingestellt und erschien mit dieser Ausgabe zum letzten Mal. Ihr Titelthema lautete „Aufrüsten“, und es lag ein Schraubendreher mit PCGH-Logo bei. Die Entscheidung zur Einstellung erklärte die Redaktion damit, dass man sich mehr auf das Hauptheft konzentrieren möchte.\n\nDie Ausgabe 09/2006 war die erste Extended-Version der PCGH. Diese enthielt zusätzlich 32 Seiten zu von der PCGH ausgewählten Top-Spielen und Dauerbrennern wie \"\", \"Battlefield 2\", \"\", \"Half-Life 2\" und \"\". Thematisiert wurden Tuning, Spieletipps und Maps & Mods. Die Herausgabe der Extended Version sollte vorerst eine einmalige Aktion sein. Jetzt erscheint sie genau wie die Standardversion der PC Games monatlich. Allerdings wurde die Extended-Ausgabe ab 09/09 eingestellt.\n\nDie Datenträger der PCGH enthalten unter anderem folgende Software:\n\n\nIm vierten Quartal 2014 lag die durchschnittliche monatlich verbreitete Auflage nach IVW bei 31.292 Exemplaren. Das sind 7,4 Prozent (2.500 Hefte) weniger als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 7,95 Prozent auf jetzt 10.761 Abonnenten ab. Damit beziehen 34,38 Prozent der Leser die Zeitschrift im Abonnement.\n\nDie Zeitschrift wurde 2017 aus der IVW-Zählung herausgenommen. Der Verlag gab im Jahr 2017 eine Auflage von 25.300 Exemplaren an.\n\nVon der PCGH werden regelmäßig Sonderhefte produziert. Diese beschäftigen sich zum Beispiel speziell mit Grafikkarten, Infrastruktur oder Windows-Tuning. Auch diese Hefte werden mit Datenträgern geliefert, die in der Regel eine große Menge an Tools, Treibern und Begleitmaterial enthalten.\n\nAuf der Internetseite der PCGH findet man immer aktuelle News rund um die Themen, die auch im Heft behandelt werden. Im Archiv auf der Website kann man sich Sonderhefte oder einzelne Artikel im PDF kaufen und herunterladen. Im Dezember 2006 ist eine überarbeitete Version des Onlineangebots erschienen, mit neuen Features wie Blogs der Redakteure und einem neuen Design.\n\nSowohl in der PCGH als auch in der PC Games werden Computerspiele und PC-Hardware behandelt. Die Schwerpunktsetzung ist unterschiedlich, trotzdem betrieben beide Magazine lange Zeit ihre Foren zusammen, um ein größeres Publikum und vielseitig interessierte Leser zu erreichen. Seit 1. Oktober 2008 hat PCGH ein eigenes Forum, PCGH Extreme genannt. Das Forum startete ursprünglich zur Games Convention 2007 parallel zum alten Forum. Es sollte die Berichterstattung zur GC sicherstellen. Dazu bot das neue Forum auf vBulletin-Basis einige Features, die die bisherige Foren-Software nicht bot, zum Beispiel den Bilderupload. Später erweiterte man die Themen des Forums um Overclocking, Kühlung und Modding. Zum 1. Oktober 2008 wurde die Struktur erneut erweitert, um alle Hardware- und Spielethemen abzudecken.\n\nAußerdem benutzt die PCGH ein Webcode-System. Anstatt komplette URLs abzudrucken werden so genannte Webcodes (vierstellige Kombinationen aus Buchstaben und Ziffern) angegeben. Diese können in ein Eingabefeld auf der PCGH-Seite eingegeben werden, um zum Ziel weitergeleitet zu werden.\n\nSeit dem 2. Juni 2008 gibt es auch eine englische Version der Webseite von PC Games Hardware. Unter der URL www.pcgameshardware.com finden sich einige ausgewählte Artikel der deutschen Webseite auch in englischer Sprache. Ein Forum auf Basis von vBulletin gibt es ebenfalls.\n\nNeben dem Forum PC Games Hardware Extreme gibt es auch eine Sonderpublikation in Heftform seit 19. Dezember 2007. Sie erscheint alle drei Monate für 5,99 Euro und aufgrund des Erfolges ab Heft 02/2009 (25. März 2009) in einem zwei-monatigen Erscheinungsrhythmus. Behandelt werden „extreme“ Themen, wie z. B. Kühlungen mit Flüssigstickstoff oder Kompressoren, Übertakten und Case Modding. Extreme wurde mit Ausgabe 05/2009 eingestellt. Jedes Jahr erscheint aber ein Sonderheft, das sich mit den Themen von PCGH Extreme beschäftigt.\n\nUm die Berichte und Tests auf die Interessen der Leser abstimmen zu können, werden auf der Webseite viele Umfragen angeboten, zum Beispiel zur verbauten Hardware oder zu den Kaufabsichten der Leser. Jeden Monat werden außerdem Bewertungsformulare zur aktuellen Ausgabe angeboten.\n\nProdukte, die in Einzel- oder Vergleichstests der PCGH eine hervorragende Leistung erzielen, erhalten einen PCG-Hardware-Award. Das beste in seiner jeweiligen Kategorie, zum Beispiel Grafikkarten oder Mainboards, erhält die Auszeichnung „Top-Produkt“. Das Produkt, das bei einer akzeptablen Leistung den günstigsten Preis hat, wird als „Spar-Tipp“ ausgezeichnet. Seit Ausgabe 03/2006 wird auch der Award „Top-Technik“ vergeben. Hiermit werden Produkte oder Technologien ausgezeichnet, die eine besondere Bedeutung besitzen. Dieser Award ist deutlich exklusiver und wird nur zu bestimmten Anlässen verliehen. Mit ihm werden, im Gegensatz zu den anderen beiden Awards, auch Grafikkarten- oder Mainboardchipsätze und Prozessoren geehrt.\n\nAußerdem gibt es bei der PCGH jedes Jahr den Publikumspreis „Hardware des Jahres“. Dabei kann jeder PCGH-Leser seinen persönlichen Hersteller des Jahres in den verschiedenen Kategorien wählen. Der Wahlbogen wird üblicherweise in der Januarausgabe (02/xxxx) abgedruckt. In der übernächsten Ausgabe (März – 04/xxxx) werden dann jedes Jahr die ersten drei Plätze jeder Kategorie veröffentlicht.\n\nEs gibt folgende Kategorien:\n\n\n\nBei der PCGH gibt es manchmal auch Lesertests. Hierfür kann sich jeder bewerben, der das jeweilige Gerät benutzen will und der einen Internetanschluss besitzt. Diejenigen, die dann von der PCGH ausgewählt werden, erhalten dann die meist von Hardwareherstellern zur Verfügung gestellten Geräte, die sie dann zu Hause testen können. Im Laufe des Tests fertigen die Tester dann ihren Bericht an, der dann zur Redaktion geschickt wird. In den folgenden Ausgaben werden diese dann ausgewertet.\n\nAußerdem gibt es seit Ausgabe 10/2005 die Aktion Leser-PC bzw. Pimp my PC. Diese Aktion gab es bereits früher. Lag damals der Schwerpunkt noch auf der Behebung von Problemen, mit denen die Leser nicht allein fertig wurden, wird heute Hardware, die als veraltet betrachtet wird, durch höherwertige ersetzt.\n\n"}
{"id": "639848", "url": "https://de.wikipedia.org/wiki?curid=639848", "title": "Libranet", "text": "Libranet\n\nLibranet ist eine Linux-Distribution, die auf Debian basiert. Sie ist ein Produkt der in Vancouver, Kanada ansässigen \"Libra Computer Systems Ltd\". Das Unternehmen entwickelte und verkaufte Unix-Systeme seit 1984. Eine erste Version von Libranet wurde im November 1999 auf debian.org durch den Firmeneigentümer vorgestellt. Die letzte Version wurde als Libranet 3.0 am 25. April 2005 freigegeben. Diese kostete um die 90 USD für neue Nutzer und 65 USD für diejenigen, die bereits eine Vorversion von Libranet nachweisen konnten. Die Vorgängerversion 2.8.1 stand ab dem Erscheinungstermin der Version 3.0 als Software zum kostenlosen Download bereit.\n\nNach dem plötzlichen Tod des Präsidenten und Gründers von Libranet, Jon Danzig am 3. Juni 2005 gab sein Sohn und Nachfolger Tal Danzig am 28. November 2005 bekannt, dass die Entwicklung von Libranet eingestellt wird.\n\nDer Name stammt von “Libra Computer Systems” (\"www.libra.com\") ab. Libranet erhob den Anspruch besonders benutzerfreundlich, desktoptauglich und schnell zu sein, die nunmehr eingestellte Libranet-Website war unter <nowiki>www.libranet.com</nowiki> erreichbar. Libranet wollte damit die Features von klassischen Distributionen wie Debian und besonders einfachen wie SuSE oder Mandrake (jetzt Mandriva Linux) zusammenführen. Libranet ist noch Mitglied der DCC.\n\nAb Version 3.0 verfügt Libranet über eine grafische Installationsroutine und wird mit einer DVD oder 5 CD-ROMs ausgeliefert. Libranets Einstellungs-Tool heißt AdminMenu und ähnelt YaST von SuSE, verwendet aber das typische Debian-Format Deb für die Softwarepakete, die mittels APT, Synaptic oder Gnome-apt installiert werden können. Als Dateisysteme werden ext2, ext3, FAT32 und ReiserFS unterstützt. NTFS kann gelesen werden, der Schreibzugriff ist jedoch noch experimentell. Als Kernel kommt 2.6 zum Einsatz und als Benutzeroberflächen werden Gnome, KDE, IceWM, Xfce und Enlightenment eingerichtet. Ebenso integriert sind die Browser: Mozilla, Opera, Galeon und der Konqueror. WLAN wird erkannt und automatisch installiert. Als Hardwareunterstützung hat es IDE, SATA, SCSI, USB, FireWire (IEEE1394) und PCMCIA.\n\n"}
{"id": "640509", "url": "https://de.wikipedia.org/wiki?curid=640509", "title": "Pathping", "text": "Pathping\n\nPathping ist ein erweiterter Windows-Befehl (ab Windows 2000) zu \"Tracert\" und \"Ping\". Im Gegensatz zu \"Tracert\" liefert \"Pathping\" detaillierte Informationen über die Weiterleitung der Pakete zu den einzelnen Rechnern. So kann zur Fehlerermittlung der spezifische Rechner bzw. Router eingekreist werden. Technisch gesehen werden innerhalb eines festgelegten Zeitabschnittes regelmäßig Ping-Signale an alle Zwischenstationen gesendet. Es werden maximal 30 Abschnitte angezeigt. Anhand der von jeder Zwischenstation zurückgesendeten Datenpakete wird am Ende eine Statistik ausgegeben. Das Erstellen dieser Statistik wird mit Hinweis auf die vermutliche Wartezeit bis zur Bildschirmausgabe angezeigt, die abhängig ist von der Anzahl der Hops bis zum Ziel (25 Sekunden pro Hop).\n\n"}
{"id": "645089", "url": "https://de.wikipedia.org/wiki?curid=645089", "title": "Desinfec’t", "text": "Desinfec’t\n\nDesinfec’t ist eine auf Linux basierende Live-DVD zur Desinfektion von Rechnersystemen nach einem Virenbefall. Sie wird von der c’t-Redaktion als Heftbeigabe zu regulären c’t-Ausgaben wie auch zu c’t-Sonderheften herausgegeben und in regelmäßigen Abständen aktualisiert. Bis 2009 trug die Distribution den Namen \"Knoppicillin\". Anfangs war es eine CD, seit 2011 ist es aufgrund der Datenmenge eine DVD.\n\nDas Konzept, den Virenscanner von einer Boot-CD zu starten, stellt sicher, dass die Betriebssystemumgebung des Scanners nicht verseucht sein kann. Ein Schädling kann so weder den Scanner selbst manipulieren, noch sich mit Funktionalitäten eines Rootkits unsichtbar machen, da sein Programmcode nicht während des Systemstarts von der Festplatte ausgeführt wurde. Ebenso kann er sich beispielsweise nicht in den Arbeitsspeicher kopieren und von dort aus erneut das System infizieren. Außerdem ist es von einer Live-CD auch dann noch möglich, das System zu booten, wenn eine Vireninfektion dies beim normalen Betriebssystem verhindert.\n\nDesinfec’t ist damit ein wirksames Mittel bei einem Befall mit Schädlingen mit Rootkit-Eigenschaften, der mit herkömmlichen Virenscannern nicht aufgedeckt werden kann – bei anderen Rootkits kann es allerdings (im Gegensatz zu herkömmlichen Viren und Trojanern) sinnvoll sein, diese im aktiven System zu suchen, da ihre charakteristischen Eigenschaften nur dann erkannt werden können.\n\nDie modifizierte Linux-Distribution enthält mehrere teils kommerzielle Virenscanner und Treiber für alle gängigen Dateisysteme. Damit ist Desinfec’t wie sein Vorgänger Knoppicillin in der Lage, auch NTFS-Partitionen zu durchsuchen und zu säubern. Eine Aktualisierung der Virensignaturen ist während der laufenden Session automatisch über das Internet oder manuell per Diskette und USB-Stick möglich, oder man erstellt eine Kopie der CD und integriert dabei gleich aktuelle Virensignaturen.\n\nKnoppicillin basierte auf Knoppix als Distribution, woher sich auch der Name ableitet. Bei Desinfec’t wurde zunächst auf Fedora umgestellt, was jedoch von einigen Anwendern nicht gut angenommen wurde. Ab der zweiten Veröffentlichung, Version 2011, setzt Desinfec’t auf die Linux-Distribution Ubuntu auf.\n\nWegen der Lizenzen einiger Virenscanner ist Desinfec’t nur kostenpflichtig erhältlich, während Knoppicillin sich in der recht alten Version 6.0.2 von 2008 nicht komplett herunterladen lässt. Zudem ist bei einigen Virenscannern eine Aktualisierung nur bis zu einem Jahr nach Erscheinen der CD möglich, also bis zum Erscheinen der neuen Desinfec't-Ausgabe.\n\nDas in Ausgabe 20/2004 erschienene Knoppicillin 3 enthält die drei Virenscanner von F-Secure, Sophos und Kaspersky Lab.\nFür den Zugriff auf NTFS-Partitionen sorgt der proprietäre Treiber von Paragon.\n\nBei der Produktion der CDs hatte sich ein Fehler eingeschlichen, der erst kurz vor dem Jahreswechsel 2004/2005 festgestellt wurde und dazu führte, dass die Signaturdateien für den Virenscanner von Sophos nicht mehr aktualisiert werden konnten.\n\nDie c’t-Redaktion darf zwar wegen der enthaltenen proprietären Software kein komplettes ISO-Abbild von Knoppicillin Version 3.1 im Internet zum Download bereitstellen, hat aber eine Aktualisierung erstellt, welche auch gleich aktualisierte Virensignaturen mit dem Stand von Januar 2005 enthält.\n\nEine eigene, auf Version 3.1 aktualisierte Knoppicillin-CD kann mittels der Software Jigdo sowie der Original-CD \"Softwarekollektion 5\" hergestellt werden.\n\nDem Heft 23/2005 wurde das aktualisierte Knoppicillin 4 auf CD beigelegt, das die Virenscanner von \"Kaspersky Lab\" und \"Sophos\" inklusive entsprechender Aktualisierungsberechtigung für ein Jahr enthält. NTFS-Partitionen werden mittels der \"libntfs\"-Bibliothek angesprochen; diese ermöglicht zwar noch keinen vollständigen Schreibzugriff, genügt aber zum sicheren Löschen von infizierten Dateien.\n\nMit Heft 21/2006 erschien das aktualisierte Knoppicillin 5, das die Virenscanner von Bitdefender Antivirus Scanner, F-Secure Anti-Virus und Sophos SAVScan inklusive entsprechender Aktualisierungsberechtigung für ein Jahr enthält. In der c’t spezial „Security“ im März 2007 ist eine aktualisierte Version \"5.2\" enthalten. Nachdem das Datenformat bei Sophos 2007 geändert wurde, ist dieser Scanner nicht mehr mit aktuellen Signaturen benutzbar.\n\nDie Version 5.2 erschien mit dem Sonderheft c’t special 03/2007 mit dem Titel „Security“.\n\nDie Version 6 erschien mit Heft 26/2007, im Dezember 2007. Enthalten sind die Virenscanner von Avira AntiVir und Bitdefender mit Aktualisierungsberechtigung für ein Jahr.\n\nVersion 7 ist mit Ausgabe 26/08 der c’t erschienen. Auf x86-Apple-Computern mit mehreren Boot-Partitionen werden nun auch Daten-Partitionen im Format HFS Plus gemounted und gescannt, sofern man diese nicht explizit im Knoppicillin-Assistenten abwählt. Allerdings berichten einige Benutzer, dass die CD auf ihren Rechnern nicht bootet. Dieses Verhalten wird auf eine Inkompatibilität mit dem Award-BIOS zurückgeführt und lässt sich nur durch Ändern des Inhalts der CD umgehen. Hierfür hat die c’t ein Patchprogramm zur Verfügung gestellt.\n\nMit dem Mitte September 2009 erschienenen Heft „c’t kompakt Security“ gab es eine neuere Version, die aber ebenfalls Knoppicillin 7 heißt.\n\nInhalt:\n\nDer Nachfolger von Version 7 heißt nun Desinfec’t, trägt die Versionsnummer 8 und ist mit Ausgabe 2/10 der c’t erschienen (Erscheinungsdatum: 4. Januar 2010).\n\nBasierte Knoppicillin noch auf einer Knoppix-Live-Distribution, so setzt Desinfec’t auf eine Fedora 12-Live-CD als Unterbau. Wie dem Forum zur offiziellen Projektseite zu entnehmen ist, waren einige Anwender mit diesem Umstieg nicht zufrieden. In einigen Beiträgen geht es darum, dass etliche neue Fehler mit Version 8 auftraten, die in Knoppicillin Version 7 nicht vorhanden waren. Letztmals wird eine CD als Medium benutzt, alle folgenden Versionen müssen aufgrund gewachsener Datenmengen auf DVD erscheinen.\n\nDesinfec’t 2011 liegt c’t-Ausgabe 8/11 bei, die ab dem 26. März 2011 erhältlich ist. Es verwendet nun eine Ubuntu-10.10-Live-CD als Unterbau. Enthalten sind wieder die kommerziellen Viren-Scanner von Avira, Bitdefender und Kaspersky, sowie das freie ClamAV. Erstmals lässt sich die Distribution mit der Desinfec’t-DVD als Ausgangspunkt auf einen USB-Stick übertragen und dieser dann startfähig machen. Damit hat man den Vorteil, ein Boot-Medium zu besitzen, auf dem man die Virus-Definitionen stets aktuell halten und im Bedarfsfall auch wichtige Dateien sichern kann. Allerdings ist das Live-Linux in dieser Form dann kompromittierbar, sofern ein USB-Stick ohne Schreibschutzschalter verwendet wird.\n\nNeben dem eigentlichen Desinfec’t befindet sich auch die Software-Kollektion 2/2011 auf der DVD, welche deshalb 2,1 GB groß ist.\n\nDer c’t-Ausgabe 9/12 mit dem Datum 10. April 2012 liegt Desinfec’t 2012 bei. Gegenüber Desinfec’t 2011 wurde hauptsächlich die Bedienbarkeit verbessert. Wie die Vorversion baut es auf Ubuntu auf, diesmal auf dem Live-System 11.10 . Es enthält Ein-Jahres-Abos für die Viren-Scanner Avira AntiVir, Bitdefender und Kaspersky Anti-Virus, sodass sich deren Virus-Definitionen über eine Internetverbindung innerhalb dieses Zeitraums unentgeltlich aktualisieren lassen. Auch das freie ClamAV ist wieder enthalten. Neu ist die Möglichkeit, Virenfunde mit einem Standardpasswort zu verschlüsseln, damit ein Virus-Scanner die umbenannte Datei nicht fälschlicherweise ein zweites Mal entdeckt. So ist sichergestellt, dass wichtige Inhalte wiederherstellbar sind und gleichzeitig ein System als sauber bestätigt werden kann.\n\nDie als \"Software-Kollektion 2/2012\" betitelte DVD ist 1,7 GB groß.\n\nDesinfec’t 2013 liegt der mit dem Datum 22. April 2013 versehenen c’t-Ausgabe 10/13 bei. Es beruht auf Ubuntu 12.04.02 \"Long Term Support\" und kann daher auch mit UEFI umgehen. Es beinhaltet wieder Ein-Jahres-Abos der Viren-Scanner von Avira, Bitdefender und Kaspersky sowie das freie ClamAV. Als große Neuerung enthält Desinfec’t nun eine startbereite Version von TeamViewer, was die Fernwartung bzw. die Hilfe z. B. für Familienangehörige erheblich erleichtern soll. Auch die Windows-Version von TeamViewer Portable befindet sich auf dem Medium. Truecrypt ist aus lizenzrechtlichen Gründen allerdings nicht mehr Bestandteil der Version 2013, kann aber nachträglich installiert werden. Das Transferieren auf einen USB-Stick ist ebenfalls wieder vorgesehen. Das Starten im UEFI-Modus ist Dank Ubuntu sogar bei eingeschaltetem Secure Boot möglich. In Einzelfällen kann es jedoch passieren, dass man Secure Boot und eventuell sogar den UEFI-Modus abschalten muss, um im BIOS-Modus zu starten, damit es funktioniert. Bei Windows 8 muss das Betriebssystem jedoch vor dem Start von Desinfec’t ganz heruntergefahren werden, da sich Windows 8 sonst normalerweise im Hyperboot-Modus befindet und ein Schreiben auf das NTFS-Dateisystem in diesem Modus zu Datenverlust führen würde. Ein Script zum vollständigen Herunterfahren von Windows 8 befindet sich auf dem Desinfec’t-Medium („codice_1“). Die DVD mit der Beschriftung \"Software-Kollektion 2\" umfasst 1,2 GiB.\n\nDie der c’t-Ausgabe 12 aus 2014 (19. Mai 2014) beiliegende Version von Desinfec’t 2014 wird von der Redaktion im zugehörigen Heft-Artikel als Modellpflege bezeichnet. Somit baut es auf bereits Bewährtem auf und bringt hauptsächlich Aktualisierungen und kleinere Detailverbesserungen. Als Grundlage dient Ubuntu Linux 12.04.4 LTS (Februar 2014), sodass sich gegenüber der Vorversion Desinfec’t 2013 eine bessere Hardware-Unterstützung ergibt; außerdem sind damit gleichzeitig einige Fehler in Ubuntu behoben.\nNeu ist die Funktion, Virenfunde direkt an Virustotal hochzuladen, was die Identifikation und Bewertung von Virenfunden erleichtern soll. Die DVD mit der Beschriftung \"Software-Kollektion 2\" umfasst 1,4 GiB.\n\nIn der Ausgabe 14 von 2015 (13. Juni 2015) lag die Desinfec’t 2015 bei. Sie basiert auf Ubuntu 14.04 LTS und enthält die Virenscanner von Avira, Bitdefender, ClamAV und Kaspersky. Neu hinzugekommen ist der Easy-Scan-Modus, mit dem die Suche noch einfacher vonstattengehen soll. In diesem werden bei einer Internetverbindung automatisch Signaturupdates heruntergeladen. Anschließend werden alle Windows-Laufwerke von dem Scanner Aviras untersucht.\n\nBei einer Installation auf einem USB-Stick wird freier Speicher für Auslagerungsdateien und Log-Dateien verwendet.\n\nIn einem Verzeichnis finden sich „Experten“-Tools, wie z. B. einen Registry-Editor, der Kaspersky Windows Unlocker oder ein Skript zum Entfernen eines Windows-Passwortes.\n\nIn der Ausgabe 12 von 2016 (28. Mai 2016) lag die aktualisierte Version Desinfec't 2016 auf DVD bei. Sie basiert auf Ubuntu 14.04 LTS. Statt Bitdefender ist in dieser Version ESET NOD32 Antivirus als Scanner enthalten.\n\nDas Sonderheft c't Security von 2016-09 (2016-09-19) enthält eine aktualisierte Version vom Desinfec't.\n\nDiese Version ist seit dem 27. Mai 2017 erhältlich. Die enthaltenen Virenscanner sind von Avira, ESET, F-Secure und Sophos. Es basiert wieder auf einer aktuellen Ubuntu-Version.\n\nDiese Version ist seit dem 24. Oktober 2017 erhältlich. Die enthaltenen Virenscanner sind von Avira, ESET, F-Secure und Sophos. Als Basis dient die aktuelle Ubuntu-Version 16.04 LTS.\n\nDiese Version ist seit dem 26. Mai 2018 erhältlich. Die enthaltenen Virenscanner sind von Avira, ESET, F-Secure und Sophos. Als Basis dient die aktuelle Ubuntu-Version 16.04.2 LTS.\n\nDas Konzept, einen Computer zum Zwecke forensischer Untersuchungen von einem sterilen Medium zu booten, ist nicht neu oder erst mit Knoppicillin eingeführt worden. Früher waren dafür schreibgeschützte Disketten üblich, die aber heutigen Anforderungen in Bezug auf Speicherkapazität nicht mehr gerecht werden.\n\nWegen der mit enthaltenen Scannern verbundenen Lizenzen ist Knoppicillin und Desinfec’t an den Erwerb einer Heftausgabe der c’t gebunden, während im Laufe der Zeit neben kommerziellen Produkten auch freie entstanden, die diesem Zweck dienen und auf DOS/Windows oder Linux-Derivaten basierenden bootbaren CDs erschienen wie z. B. UBCD oder Helix. Neben anderen forensischen Tools sind oft auch Virenscanner darauf enthalten, die entweder vollständig frei oder zumindest für den nichtkommerziellen Gebrauch frei sind.\n\nAuch auf verschiedenen Versionen der Knoppix-Live-CD/DVD, auf deren Konzept Knoppicillin beruht, waren bisher Virenscanner enthalten, so z. B. F-Prot von F-Secure (ursprünglich von FRISK Software aus Island) oder auch ClamAV. Zudem bieten einige Hersteller auch eigene Rettungs-CDs zum kostenlosen Download an, z. B. Avira Rescue System (welches ebenfalls auf Ubuntu basiert), Bitdefender Rescue CD, Kaspersky Rescue Disk.\n\n"}
{"id": "645362", "url": "https://de.wikipedia.org/wiki?curid=645362", "title": "Vulnerability Scanner", "text": "Vulnerability Scanner\n\nVulnerability Scanner sind Computerprogramme, die Zielsysteme auf das Vorhandensein von bekannten Sicherheitslücken hin untersuchen. \n\nDer Scanner bedient sich dabei Datenbanken mit Informationen zu diversen Sicherheitsproblemen wie z. B.:\n\nDer Einsatz von Vulnerability Scannern kann auf zwei unterschiedliche Arbeitsweisen erfolgen:\n\nVulnerability Scanner können damit im Gegensatz zu Portscannern nicht nur die am Zielsystem erreichbaren Dienste und ggf. deren Versionen auf Schwachstellen prüfen, sondern ermöglichen auch eine Prüfung auf tatsächlich vorhandene Schwachstellen des verwendeten Betriebssystems und der angebotenen Dienste. \n\nNeben Scannern, die eine gesamtheitliche Prüfung eines Zielsystems auf Schwachstellen durchführen, gibt es eine Reihe von Scannern, die auf Detailaspekte spezialisiert sind. Besonders hervorzuheben sind in diesem Zusammenhang Web-Applikation-Scanner, deren Prüfmethoden auf Applikationen, die webbasierte Methoden einsetzen, optimiert sind.\n\nAufgrund der Komplexität lassen sich mit Vulnerability Scannern False-Positive-Erkennungen nicht vollständig ausschließen.\n\n\n"}
{"id": "645647", "url": "https://de.wikipedia.org/wiki?curid=645647", "title": "DVD Shrink", "text": "DVD Shrink\n\nDVD Shrink ist ein Computerprogramm, das Video- und Audiodaten von einer DVD auf die Festplatte eines PCs transferieren kann. Gleichzeitig können, falls erwünscht, die Videodaten recodiert und dabei komprimiert werden, um ihren Speicherplatzbedarf zu verringern. DVD Shrink ist Freeware und damit frei von Lizenzgebühren erhältlich. Das Programm wird seit Mai 2005 nicht mehr weiterentwickelt. Vom Namen her ähnliche Versionen wie \"DVD Shrink 2010\" sind in der Regel Betrugsversuche, sogenannte scams.\n\nDVD Shrink kann beispielsweise Filme, die auf Double-Layer-DVDs (zweilagigen DVDs) gespeichert sind, auf Wunsch von 8,5 Gigabyte auf 4,7 Gigabyte, so komprimieren, dass sie schließlich auch auf einschichtige DVDs gebrannt werden können. Das geschieht in erster Linie durch Videokomprimierung mit minimalem Qualitätsverlust. Der Qualitätsverlust kann allerdings höher als bei regulären MPEG-2-Encodern sein.\n\nAlternativ bietet DVD Shrink statt Komprimierung auch die \"„Verkürzung“\" der Videodaten an, d. h. dass ausgewählte, nicht-benötigte DVD-Inhalte (wie zum Beispiel Vor- und Abspann) beim Brennen weggelassen werden können. Da das aber nicht die zentrale Funktionalität des Programms ist, ist es hier auch nicht so ausgereift, so dass z. B. die originalen Menüs bei dieser Art der \"„Komprimierung“\" nicht mit ihrer vollen Funktion beibehalten werden können. DVD Shrink bietet in diesem Rahmen standardmäßig eine Funktion an, bei der nur der Hauptfilm einer DVD transferiert wird. Zusätzlich können nicht erwünschte, bspw. fremdsprachige Ton- und Untertitelspuren entfernt werden, um mehr Platz für die Videodaten zu gewinnen.\n\nFür eine ausgefeiltere \"Komprimierung durch Weglassen\" sind aber andere Programme (sog. IFO-Editoren, die die Menü-Dateien bearbeiten) geeigneter.\n\nDVD Shrink ist außerdem in der Lage, mit CSS verschlüsselte DVDs auszulesen und in ungeschützter Form wieder abzuspeichern. DVD Shrink ist jedoch nicht in der Lage, Video-DVDs von Kopierschutzmechanismen wie zum Beispiel ARccOS oder Macrovision zu befreien. Das Umgehen wirksamer technischer Schutzmaßnahmen ist unter anderem in der Europäischen Union nicht zulässig (siehe für Deutschland etwa Urheberrechtsgesetz, für Österreich § 90c Urheberrechtsgesetz.) DVD Shrink wird daher auf der offiziellen Webseite nicht mehr zum Download angeboten, wenn die deutschsprachige Lokalisierung gewählt wurde. Betrachtet man die offizielle Website dagegen in englischer Sprache, ist der Download nach wie vor möglich.\n\nAm 4. Dezember 2004 schob der Autor auf Bitten der Community eine „CSS-free“ benannte, englische EXE nach, deren Funktionalität auf nicht CSS-geschützte DVDs beschränkt ist und somit auch in Ländern gehostet werden kann, in denen die Umgehung eines Kopierschutzes verboten ist. Diese falsche Bezeichnung – schließlich wird DeCSS benötigt um CSS zu umgehen – wurde dann am 31. Mai 2005 mit Erscheinen der offiziellen deutschen Version „DVD Shrink DeCSS-Frei“ korrigiert.\n\n\n"}
{"id": "645757", "url": "https://de.wikipedia.org/wiki?curid=645757", "title": "High Level Architecture", "text": "High Level Architecture\n\nHigh-Level Architecture (HLA) ist eine vom US-amerikanischen Verteidigungsministerium (genauer dem \"Defense Modeling and Simulation Office, DMSO for the U.S. Department of Defense\") definierte Architektur zur integrierten und verteilten Simulation. Dieses Konzept ist im Jahr 2000 zum internationalen Standard geworden (IEEE 1516). Sie basiert auf der Idee, eine Gesamtsimulation in mehrere einzelne, kleine Simulationen aufzuteilen, die untereinander ihre Informationen austauschen. Die Kommunikation geschieht über ein Computernetzwerk. Verwaltet werden die einzelnen Simulationen dabei von einer zentralen Komponente, der sogenannten \"Run-Time-Infrastructure\" (RTI). Diese überwacht den Simulationsablauf und verwaltet die Verteilung der Daten zwischen den Einzelsimulationen (Föderaten). Die Gesamtheit der Einzelsimulationen wird als \"Föderation\" bezeichnet.\n\nDie High-Level-Architecture basiert auf drei großen Definitionsbereichen\n\n\n\n\nDie Daten, die zwischen HLA-Föderaten ausgetauscht werden, können entweder Objekte oder Interaktionen sein. Ein Objekt ist eine Datenstruktur, die in einer teilnehmenden Simulation vorhanden ist und die ihre Daten (Attribute) im Netzwerk verteilt. Eine Interaktion ist mit einem Ereignis gleichzusetzen und besitzt bestimmte Parameter.\nZum Beispiel besitzt ein Objekt \"Flugabwehrpanzer\" in Simulation A die Attribute Position, Geschwindigkeit, Beschleunigung, etc.\nEine Interaktion kann nun z. B. das Abfeuern eines Lenkflugkörpers auf ein Kampfflugzeug in Simulation B sein und als Parameter den Typ des Gefechtskopfes, das beschossene Ziel oder das abfeuernde Objekt besitzen.\nNun wäre Simulation B in der Lage, auf das Abfeuern des Lenkflugkörpers zu reagieren.\n\nDie Daten können als Unicast UDP/TCP oder Multicast übertragen werden. Somit bietet die HLA mehr Flexibilität im Bereich der vernetzten Simulation als das Simulationsprotokoll DIS, das seine Daten ausschließlich per Broadcast austauscht.\n\nDie Schnittstelle zwischen Simulation und RTI ist in der Regel objektorientiert und in einer Programmiersprache wie C++ oder Java implementiert. Die zur Verfügung stehenden Objekte und Funktionen können in folgende Gruppen (service groups) unterteilt werden:\n\n\nDas OMT bildet einen gemeinsamen Rahmen für die Kommunikation zwischen HLA-Föderaten. Es handelt sich um eine standardisierte Schablone für ein Datenmodell und legt fest, welche Daten mit anderen Föderaten ausgetauscht werden können. Das Datenmodell selber kann frei definiert werden. Man unterscheidet zwei Arten von Datenmodellen:\n\n\nDamit ein Datenaustausch innerhalb einer Föderation möglich ist, müssen die Föderaten mit ihrem SOM zumindest eine gemeinsame Schnittmenge des FOM unterstützen.\n\nDie HLA-Regeln beschreiben Anforderungen an Föderaten und Föderationen beim Simulationsablauf. Hier werden u. a. die Kommunikationsgrundlagen definiert.\n\nRegeln für Federations:\n\n\nRegeln für Federates:\n\n\nDie RTI überprüft die Einhaltung der HLA-Regeln, der Implementierungsgrad variiert je nach verwendeter RTI. Insbesondere \"Time Management\" wird nicht von allen RTIs unterstützt.\n\nDie HLA ist seit 2000 im IEEE-Standard 1516 definiert, welcher sich in folgende Anteile gliedert:\n\n\n"}
{"id": "646345", "url": "https://de.wikipedia.org/wiki?curid=646345", "title": "Windows Script Host", "text": "Windows Script Host\n\nDer Windows Script Host (WSH) (früher: Windows Scripting Host) ist in Windows-Betriebssystemen eine COM-basierte Laufzeitumgebung für Skriptsprachen.\n\nDer WSH lässt sich seit Windows 95 und Windows NT 4.0 verwenden, seit Windows 98 SE und Windows 2000 wird er standardmäßig installiert.\n\nDer Script Host wurde entwickelt, um Anwendern und insbesondere Administratoren die Möglichkeit zu geben, häufig wiederkehrende Vorgänge zu automatisieren. Während Unix, OS/2 und andere Systeme schon lange mit ausgereiften Programmiersprachen gesteuert werden konnten, boten die frühen Microsoft-Betriebssysteme (DOS mit und ohne Microsoft Windows 3.0, Microsoft Windows 3.1, Windows NT 3.5x) nur die sehr begrenzte Steuerungsmöglichkeit mittels Batch-Programmierung.\n\nDer WSH kann selbst keine Skripte ausführen, sondern verwendet sogenannte Script Engines. Diese Script Engines lassen sich wiederum in andere Programme bzw. Bibliotheken (z. B. Internet Explorer, Microsoft Office, Active Server Pages, …) einbinden, wodurch diese ebenfalls zu Script Hosts werden. Mit dem WSH werden Script Engines für die Programmiersprachen JScript und VBScript mitgeliefert. Weitere Programmiersprachen für den WSH lassen sich nachinstallieren.\n\n\n"}
{"id": "646886", "url": "https://de.wikipedia.org/wiki?curid=646886", "title": "Path Tracing", "text": "Path Tracing\n\nPath Tracing ist ein Algorithmus zur Bildsynthese, der die Simulation der globalen Beleuchtung ermöglicht.\n\nPfadnachverfolgung (Path Tracing) basiert auf der Erkenntnis, dass die Simulation der globalen Beleuchtung der Lösung der sogenannten Rendergleichung entspricht, die die Strahlungsdichte eines beliebigen, von einem bestimmten Punkt ausgehenden Lichtstrahls angibt.\n\nPath Tracing verwendet rigorose mathematische Verfahren, die aus dem Bereich der mathematischen Statistik stammen. Der Algorithmus verwendet eine so genannte Monte-Carlo-Integration, um die Rendergleichung näherungsweise zu lösen. Daher wird Path Tracing, ebenso wie weitergehende, darauf aufbauende Algorithmen wie Metropolis Light Transport oder Bidirectional Path Tracing, auch als Monte-Carlo-Raytracing bezeichnet.\n\nBeim Path Tracing wird jeder Strahl, der in die Szene geschossen wird, beim Auftreffen auf Oberflächen reflektiert, gebrochen oder absorbiert, wobei jedes Mal (außer im Falle der Absorption) mindestens ein zufälliger Strahl generiert wird, der das Integral der Rendergleichung nähert. Der Anfangsstrahl sucht sich so seinen Weg (\"path\") durch die Szene. Je mehr Anfangsstrahlen man verwendet, desto mehr nähert man sich dem idealen Bild an. Die Fehler der Näherung äußern sich als Varianz, was Bildrauschen entspricht. Techniken wie Importance Sampling tragen zur Verringerung der Varianz bei.\n\nDer Unterschied zum diffusen Raytracing liegt darin, dass beim Path Tracing die vollständige Rendergleichung mittels zufällig generierten Strahlen auf allen – auch auf diffusen – Oberflächen gelöst und somit die globale Beleuchtung (Global Illumination) simuliert wird.\n\nIn der Praxis ist reines Path Tracing meist zu langsam, weshalb es mit Photon Mapping kombiniert werden kann.\n\nDie Idee zum Path Tracing wurde 1986 von James Kajiya zusammen mit der Rendergleichung als SIGGRAPH-Publikation veröffentlicht, damals unter der Bezeichnung \"Integral equation technique.\"\n\n"}
{"id": "646889", "url": "https://de.wikipedia.org/wiki?curid=646889", "title": "Light Ray Tracing", "text": "Light Ray Tracing\n\nDer Begriff Light Ray Tracing, auch \"Backward Ray Tracing,\" \"Forward Ray Tracing\" oder \"Light Tracing\" genannt, bezeichnet einen Algorithmus zur Bildsynthese, der auf Raytracing basiert.\n\nEs handelt sich dabei um eine Umkehrung des normalen Raytracing-Verfahrens. Dabei werden die Strahlen von den Lichtquellen ausgesendet, bis sie ins Leere gehen oder die Bildfläche des Beobachters erreichen. Light Ray Tracing ist – wie auch Path Tracing – in der Lage, die globale Beleuchtung zu ermitteln. Beide Verfahren arbeiten jedoch in unterschiedlichen Situationen unterschiedlich effizient.\n\nLight Ray Tracing hat den Nachteil, dass von den vielen, von den Lichtquellen ausgesandten Strahlen die meisten ins Leere treffen und weniger die Bildebene treffen. Dadurch werden sehr viele Strahlen unnötigerweise losgeschickt. Das Verfahren hat allerdings auch den Vorteil, dass bestimmte Lichteffekte wie etwa Kaustiken einfacher simuliert werden können als beim normalen Raytracing.\n\nDurch Photon Mapping oder Path-Tracing-Erweiterungen wie Bidirectional Path Tracing ist es möglich, die Vorteile der Simulationen vom Beobachter und von den Lichtquellen aus zu vereinen.\n\n"}
{"id": "648593", "url": "https://de.wikipedia.org/wiki?curid=648593", "title": "Treppeneffekt", "text": "Treppeneffekt\n\nAls Treppen(stufen)effekt (auch \"Treppcheneffekt\" oder \"Rasterkonvertierungseffekt\") wird in der Computergrafik das „treppenartige“ Erscheinungsbild an den Kanten gerasterter Figuren bezeichnet.\n\nDer Treppeneffekt ist keine Konsequenz des Raster-Algorithmus selbst, sondern wird durch die endliche Bildauflösung des Grafikgerätes verursacht. Daher wird das Aussehen betroffener Objekte auch als „pixelig“ beschrieben. Besonders irritierend ist er bei Animationen, da sich hier kleine Objekte auffallend ruckartig zu bewegen scheinen. Obwohl der Treppeneffekt in der Fachliteratur oft als Konsequenz des Alias-Effekts beschrieben wird, handelt es sich nicht um einen Alias-Effekt im herkömmlichen Sinn der Signalanalyse, bei dem eine hohe Frequenz als störende niedrigere Frequenz erscheint. Der Treppeneffekt kann durch höhere Bildauflösung und durch Antialiasing, auch Kantenglättung genannt, gemindert werden. Bei einigen Bildschirmen kann der Treppeneffekt zusätzlich durch Subpixel-Rendering verringert werden.\n\nGanz ohne Treppeneffekt lassen sich Figuren nur mit Hilfe von Vektorbildschirmen oder Plottern darstellen.\n"}
{"id": "648706", "url": "https://de.wikipedia.org/wiki?curid=648706", "title": "Tree Adjoining Grammar", "text": "Tree Adjoining Grammar\n\nTree-adjoining grammars (TAG), auch Baumadjunktions-Grammatiken, sind formale Grammatiken, die von Aravind Joshi eingeführt wurden und in der Computerlinguistik für die Beschreibung von natürlichen Sprachen verwendet werden.\n\nTAGs ähneln kontextfreien Grammatiken, verwenden aber Bäume statt Regeln als kleinste Elemente.\n\nTAGs werden als schwach kontextsensitiv (\"mildly context-sensitive\") beschrieben; sie sind also stärker als kontextfreie Grammatiken, aber schwächer als kontextsensitive Grammatiken in der Chomsky-Hierarchie. Daher sind sie vermutlich stark genug, um natürliche Sprachen zu erzeugen, aber auch schwach genug, um noch effizient parsebar zu sein.\n\n"}
{"id": "648996", "url": "https://de.wikipedia.org/wiki?curid=648996", "title": "Com! Das Computer-Magazin", "text": "Com! Das Computer-Magazin\n\ncom! Das Computer-Magazin war eine seit 2003 monatlich erscheinende deutsche Computerzeitschrift. Im August 2014 wurde sie eingestellt. Stattdessen wird nun \"com! professional für IT-Entscheider\" herausgegeben, also für eine andere Zielgruppe.\n\nBis Juli 2014 erschien die Zeitschrift in folgenden Varianten:\n\n\n\"com! Das Computer-Magazin\" erschien im Verlag \"Neue Mediengesellschaft Ulm\" mbH mit Sitz in Ulm.\n\nEin Vorläufer wurde 1980 als „bildschirmtext magazin“ gegründet. Aus dem kostenlosen Kundenmagazin für BTX-Kunden wurde später eine Online-Programmzeitschrift. Seit 2004 schrieb \"com! Das Computer-Magazin\" für technisch interessierte PC-Anwender und versorgte diese mit Workshops, Tests und zahlreichen Tipps & Tricks. Im Juli 2014 wurde \"com! Das Computer-Magazin\" zu Gunsten von \"com! professional für IT-Entscheider\" mit anderem Inhalt für eine andere Zielgruppe eingestellt.\n\nSitz der Redaktion war München. Zu den Redakteuren von \"com! Das Computer-Magazin\" gehörten Andreas Dumont, Oliver Ehm, Andreas Th. Fischer, Stefan Kuhn, Mark Lubkowitz, Konstantin Pfliegl, Johann Scheuerer und Johann Sedlbauer.\n\n\nIm vierten Quartal 2012 lag die durchschnittliche verbreitete Auflage nach IVW bei 128.856 Exemplaren. Das sind 77.803 Exemplare pro Ausgabe weniger (–37,65 %) als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 10.735 Abonnenten auf nun 55.147 pro Ausgabe ab (–16,29 %); damit bezogen rund 42,8 % der Leser die Zeitschrift im Abo.\n\n"}
{"id": "651018", "url": "https://de.wikipedia.org/wiki?curid=651018", "title": "Microsoft Windows Mobile", "text": "Microsoft Windows Mobile\n\nWindows Mobile ist ein kompaktes Betriebssystem von Microsoft und Vorgänger von Windows Phone, kombiniert mit einer Zusammenstellung von Anwendungen für mobile Geräte. Es basiert auf der Win32-API. Geräte, die \"Windows Phone\" bzw. \"Windows Mobile\" benutzen, sind zum Beispiel Pocket PCs, Smartphones, und tragbare Media-Center. Das Betriebssystem ist so gestaltet, dass eine Ähnlichkeit mit den PC-Versionen von Windows erkennbar ist, obwohl die jeweiligen Zielarchitekturen und Codebasen einander kaum ähnlich sind. Hingegen ist Windows Phone komplett neu entwickelt.\n\nWindows Mobile für Pocket PC enthält in den meisten Versionen:\n\n\nWindows Mobile 2002, welches auf Windows CE 3.0 basiert, war die erste Version von Windows CE, die unter dem Namen „Windows Mobile“ vermarktet wurde. Gezielt für tastaturlose Pocket PCs mit QVGA-Auflösung entworfen, war Windows Mobile 2002 (wie die ursprünglichen Pocket PC Versionen) das einzige Betriebssystem von Microsoft für eingebettete Systeme.\n\nDie zweite Version, genannt Windows Mobile 2003, wurde am 23. Juni 2003 veröffentlicht und kam in drei Varianten. Zwei sind sich sehr ähnlich: Windows Mobile 2003 für Pocket PC und Windows Mobile 2003 Pocket PC Phone Edition, das speziell für Pocket PCs mit Telefonfunktion entwickelt wurde, wie zum Beispiel den XDA bzw. MDA.\n\nDie dritte Variante ist eine Edition mit dem Namen Windows Mobile 2003 for Smartphone, die – trotz mehrerer Gemeinsamkeiten mit dem Pocket PC – eine wesentlich andere Plattform darstellt, die auch Software erfordert, die speziell darauf abgestimmt ist: Windows Mobile Smartphones haben keinen Touchscreen, wesentlich geringere Display-Auflösungen, eine normale Telefontastatur und sind speziell für Einhand-Bedienung entworfen.\n\nWindows Mobile 2003 basiert auf Windows CE 4.20.\n\nWindows Mobile 2003 Second Edition, bekannt als Windows Mobile 2003SE, wurde am 24. März 2004 freigegeben. Es enthielt eine Reihe von Verbesserungen gegenüber dem Vorgänger wie:\n\n\nWindows Mobile 2003SE basiert auf Windows CE 4.21.\n\nWindows Mobile 5.0, ursprünglicher Name „Magneto“, wurde am 9. Mai 2005 veröffentlicht. Es basiert auf Windows CE 5.0 und enthält in manchen Distributionen das .NET Compact Framework 2.0, das ansonsten zusätzlich installiert werden kann (kostenfrei). Der enthaltene Internet-Browser heißt nun „Internet Explorer Mobile“ und nicht mehr „Pocket Internet Explorer“.\n\nEnthaltene Fähigkeiten:\n\n\nWindows Mobile 5.0, wurde auf der Microsofts Mobile and Embedded Developers Conference 2005 in Las Vegas veröffentlicht. Es gibt vier verschiedene Varianten, die nur teilweise anwendungskompatibel sind.\n\nSpäter wurde das Mobile Security and Feature Pack (MSFP) nachgereicht, das auf Windows Mobilgeräten mit Telefonfunktion den GPRS-gestützten Abgleich mit Exchange 2003 SP2 und höher ermöglichte, und einige Sicherheitsfunktionen ergänzte, die für mobile Exchangeclients wünschenswert sind. Die Synchronisationstechnik \"Directpush\" ermöglicht eine Echtzeitsynchronisation von Mail, Kontakten, Terminen und Notizen.\n\nWindows Mobile 6 (abgekürzt WM6) wurde am 12. Februar 2007 auf der 3GSM-Messe in Barcelona offiziell vorgestellt. Version 6 basiert auf Windows CE 5.2, welches in großen Teilen dem Kern von Windows Mobile 5.0 gleicht. Das Design der Benutzeroberfläche ist an Windows Vista angelehnt.\n\nFolgende Versionen werden ausgeliefert:\n\n\nWichtige Neuerungen:\n\n\nErweiterte Sicherheits-Funktionen:\n\n\nWindows Mobile 6.1 erschien erstmals auf den Geräten Asus P320, Samsung i900 Omnia, Asus P560, Asus M536, Motorola Q9, Sony Ericsson Xperia X1, HTC Touch Pro und simvalley mobile XP-25. Für HTC TyTN II, HTC Touch Dual und Asus P750 sind offizielle Aktualisierungen verfügbar. Geplant sind ebenfalls Updates auf Windows Mobile 6.1 für Asus P527, Asus M930, Toshiba Portégé G810, Toshiba Portégé G910, HTC Touch, HTC X7510, Motorola MOTO Q 9h global, Samsung ACE, T-Mobile MDA Ameo 16 GB, T-Mobile MDA compact IV, T-Mobile MDA vario IV und weitere nicht in Deutschland erhältliche Geräte. Bei Version 6.1 Standard wurde das Design der Oberfläche noch stärker an Vista angelehnt; so ist die Startleiste jetzt leicht transparent, Hinweise, Meldungen und Kurznachrichten werden in kleinen Infofenstern angezeigt, und es kann nach links und rechts zu weiteren Fenstern gewechselt werden. Die Oberfläche der Professional- und Classic-Version hat sich nicht verändert.\n\nWichtige Neuerungen:\n\n\nTrotz der stärkeren Anlehnung an Windows Vista ist eine Synchronisation mit den Vista-Standard-Programmen Mail und Calendar nicht möglich – nach wie vor funktioniert diese nur mit Outlook.\n\nAm 11. Mai 2009 wurde die Version 6.5 veröffentlicht. Erste Geräte wurden für den 6. Oktober 2009 erwartet.\nAls Neuerung bietet die Version 6.5 eine vorinstallierte Anwendung für den Zugang zum Windows Marketplace for Mobile, welcher Anwendungen zum Download bereithält. Des Weiteren konnten mit dem mittlerweile wieder eingestellten Service My Phone zahlreiche Daten des Smartphones online synchronisiert werden. Die zuerst geplante Honigwaben-Oberfläche, auch Honeycomb genannt, wurde nach Kritik an dieser wieder verworfen. Stattdessen sollen nun größere Icons verwendet werden. Trotz dieser Änderungen an der Oberfläche wird das grundsätzliche Layout beibehalten, wie Microsoft auf der MIX09 bekannt gab. Zudem soll die Ausrichtung von Windows Mobile künftig nicht mehr nur in den Business-Sektor sein, sondern auch noch viel mehr den Consumerbereich.\n\nDie Neuerungen:\n\n\nAm 2. Februar 2010 wurde die Version 6.5.3 durch Microsoft offiziell vorgestellt. Erste Geräte mit dem Betriebssystem erschienen im zweiten Quartal 2010. Hauptaugenmerk wurde auf die Verbesserung der Fingerbedienung gelegt.\n\nNeuerungen:\n\n\nIm Juni 2010 kündigte Microsoft für die zweite Jahreshälfte eine auf Windows Mobile 6.5 basierende neue Betriebssystemvariante unter der (im Moment sowohl für Windows Embedded CE als auch für Windows Mobile genutzten) Markenbezeichnung \"Windows Embedded Handheld\" an. Es sollen in der ersten Version keine neuen Funktionen hinzukommen. Diese Version wurde im Januar 2011 freigegeben. Eine auf Windows 7-Technologien basierende, derzeit Windows Embedded Compact 7 genannte Version mit neuen Funktionen soll dann in der zweiten Hälfte des Jahres 2011 erscheinen.\n\nWindows Mobile 7 sollte eigentlich 2009 erscheinen, mehrere Verzögerungen veranlassten Microsoft aber, Windows Mobile 6.5 als Zwischenschritt zu veröffentlichen. Die Version 7 wurde am 15. Februar 2010 unter der neuen Bezeichnung Windows Phone auf dem Mobile World Congress 2010 in Barcelona angekündigt. Die Entwickler konzentrieren sich besonders auf die intuitivere Benutzeroberfläche, welche Multitouch- und Gestensteuerung unterstützen soll, um mit ihrem Betriebssystem auch Nicht-Business-Kunden für sich zu gewinnen. Auch entscheidend ist die Einteilung in sogenannte „Hubs“ – Themenbereiche – die etwa an die Xbox oder das Windows Media Center erinnern und verschachtelte Menüs verhindern sollen.\n\nWindows Phone 7 basiert auf Windows CE 6.0 R3 und Teilen von Windows Embedded Compact 7.\nEs gilt zu beachten, dass es kein Update von Windows Mobile auf Windows Phone gibt.\nDer Nachfolger Windows Phone 8 basiert nicht mehr auf Windows CE, sondern auf dem Desktop-System Windows NT 6.2.\n\nWährend der Entwicklungsphase unter dem Codenamen Apollo bekannt, ist die derzeit aktuelle Version des Windows Phone-Betriebssystems für Mobiltelefone. Im Gegensatz zum Vorgänger Windows Phone 7 basiert es nicht länger auf Windows CE, sondern demselben Windows NT-Kernel wie Windows 8 und Windows RT. Es wurde offiziell am 20. Juni 2012 auf dem Windows Phone Summit in San Francisco vorgestellt. Da ab dieser Version ein anderer Kernel als in den vorherigen Versionen verwendet wird, wird dieses Betriebssystem in der Regel nicht mehr der Windows Mobile-Linie zugerechnet.\n\nWindows 10 Mobile ist eine speziell für den Einsatz auf Smartphones und kleinen Tablets (Bildschirmgröße < 8 Zoll) angepasste Ausgabe des Betriebssystems Windows 10. Sie läuft auf modernen CPUs der ARM- und der IA-32-Architektur. Diese Version führt das Smartphone-Betriebssystem Windows Phone und das Desktop-/Tablet-Betriebssystem Windows zu einem Betriebssystem zusammen, Apps sollen zum großen Teil überall denselben Quelltext verwenden können. Windows 10 Mobile wurde erstmals im Rahmen einer Veranstaltung am 21. Januar 2015 der Öffentlichkeit präsentiert. Für die meisten Smartphones mit Windows Phone 8.1 sollte es ein kostenloses Update geben, was jedoch für die meisten Geräte widerrufen wurde.\n\nWindows 10 IoT Mobile Enterprise enthält alle Features von Windows 10 Mobile mit zusätzlichen Komponenten aus Windows 10 Enterprise. Es läuft auf X86-, x64- oder ARM-Architektur.\n\n"}
{"id": "652280", "url": "https://de.wikipedia.org/wiki?curid=652280", "title": "Burroughs Corporation", "text": "Burroughs Corporation\n\nDie Burroughs Corporation war ein US-amerikanischer Bürogerätehersteller.\n\nUrsprünglich verkaufte die \"American Arithmometer Company\" in St. Louis von William Seward Burroughs erfundene Addiermaschinen. Nach ihrem Umzug nach Detroit im Jahre 1904 wurde ihr Name nach dem Erfinder in \"Burroughs Adding Machine Company\" geändert. Burroughs starb 1898. Burroughs wurde zum größten Hersteller von Addiermaschinen in den USA, in den 1950er Jahren wurden dann auch Schreibmaschinen und Rechenmaschinen gefertigt. \n\n1953 wurde die \"Burroughs Adding Machine Company\" in \"Burroughs Corporation\" umbenannt und begann, nach dem Kauf der \"ElectroData\" in Pasadena, Kalifornien, mit der Herstellung von Computern für Banken. Das erste Produkt war der Computer \"B205 Tube\". Burroughs hatte weltweit Niederlassungen, in Deutschland die Burroughs GmbH.\n\nAuf der Basis der mechanischen Rechenautomaten wurden in Verbindung mit elektronischen Schaltungen Buchungs- und Abrechnungsautomaten entwickelt, die unter dem Namen \"Sensimatic\" in Deutschland Eingang bei vielen Banken als Belegbearbeitungs- und Belegverarbeitungs-Automaten fanden. Eine besondere Kategorie waren die Magnetkontenbuchungsautomaten.\n\nBurroughs entwickelte drei Mainframe-Rechnerarchitekturen. \n\nDie Burroughs Corporation war von der Größe her – wenngleich nicht technologisch – immer ewiger Zweiter nach IBM. Sie war in den 1960er-Jahren eine der acht wichtigen Computerfirmen der USA (mit IBM – der Größten, Honeywell, Scientific Data Systems, Control Data Corporation, General Electric, RCA und UNIVAC). Später wurde diese Gruppe unter dem Namen BUNCH (Burroughs, UNIVAC, NCR, Control Data Corporation, und Honeywell) bekannt.\n\nIm September 1986 verschmolz die Burroughs Corporation mit der Sperry Corporation unter dem neuen Namen Unisys Corporation.\n"}
{"id": "652949", "url": "https://de.wikipedia.org/wiki?curid=652949", "title": "Git", "text": "Git\n\nGit [] ist eine freie Software zur verteilten Versionsverwaltung von Dateien, die durch Linus Torvalds initiiert wurde.\n\nDurch eine Lizenzänderung des bis dahin genutzten, proprietären BitKeeper-Systems konnten die Linux-Kernel-Entwickler dieses nicht mehr kostenlos verwenden, und somit blieb vielen Entwicklern der Zugang verwehrt. Daher begann Linus Torvalds im April 2005 mit der Entwicklung einer neuen Quellcode-Management-Software und präsentierte bereits wenige Tage nach deren Ankündigung eine erste Version.\n\nTorvalds wünschte sich ein verteiltes System, das wie BitKeeper genutzt werden kann und die folgenden Anforderungen erfüllt:\n\nEin bereits existierendes Projekt namens Monotone entsprach den ersten beiden Anforderungen, das dritte Kriterium wurde jedoch von keinem bestehenden System erfüllt.\n\nTorvalds entschied sich dagegen, Monotone an seine Anforderungen anzupassen, und begann stattdessen, ein eigenes System zu entwickeln. Einer der Hauptgründe für diesen Schritt war die Arbeitsweise, für die Monotone nach Torvalds Ansicht optimiert ist. Torvalds argumentierte, dass einzelne Revisionen von einem anderen Entwickler in den eigenen Entwicklungszweig zu importieren zu Rosinenpickerei und unordentlichen Repositorien führen würde. Wenn hingegen immer ganze Zweige importiert werden, wären Entwickler gezwungen aufzuräumen. Dazu seien Wegwerf-Zweige notwendig.\n\nGits Gestaltung verwendet einige Ideen aus Monotone sowie BitKeeper, aber keinen Quellcode daraus. Es soll ausdrücklich ein eigenständiges Versionsverwaltungssystem sein.\n\nDerzeitiger Maintainer von Git ist Junio Hamano.\n\nDer Name „Git“ bedeutet in der britischen Umgangssprache so viel wie „Blödmann“. Linus Torvalds erklärte seine Wahl des ungewöhnlichen Namens mit einem Witz sowie damit, dass das Wort praktikabel und in der Softwarewelt noch weitgehend unbenutzt war:\n\nDer Name \"Linux\" wurde anfangs nicht von Torvalds selbst propagiert und nur widerwillig akzeptiert.\n\nGit ist ein verteiltes Versionsverwaltungssystem, das sich in einigen Eigenschaften von typischen Versionsverwaltungssystemen unterscheidet:\n\nSowohl das Erstellen neuer Entwicklungszweige (\"branching\") als auch das Verschmelzen zweier oder mehrerer Zweige (\"merging\") sind integrale Bestandteile der Arbeit mit Git und fest in die Git-Werkzeuge eingebaut. Git enthält Programme, mit deren Hilfe sich die nicht-lineare Geschichte eines Projektes einfach visualisieren lässt und mit deren Hilfe man in dieser Geschichte navigieren kann. Verzweigungen (\"branches\") in Git sind (im Gegensatz zu anderen SCMs) sehr effektiv implementiert: Ein Branch stellt nur eine \"Reference\", kurz \"ref\", eine Textdatei mit einer Commit-ID, dar, die in einem Repository im Verzeichnis codice_1 (z. B. codice_2 für den \"master\"-Branch) liegt und auf einen bestimmten Commit verweist. Über dessen \"Parental Commits\", also Eltern-Commits, lässt sich die Branch-Struktur rekonstruieren. Durch diese Eigenschaften lassen sich weiterhin sehr große und effiziente Entwicklungsstrukturen, wie bei Git selbst oder dem Linux-Kernel, realisieren, bei denen jedes Feature und jeder Entwickler einen Branch oder ein eigenes Repository haben, aus dem der Projektverantwortliche (\"maintainer\") dann Commits über Merge oder Cherry-pick (Nutzen einzelner Commits) in den Hauptzweig des Projekts (master) übernehmen kann.\n\nJeder Benutzer besitzt eine lokale Kopie des gesamten Repositorys, inklusive der Versionsgeschichte (\"history\"). So können die meisten Aktionen lokal und ohne Netzwerkzugriff ausgeführt werden. Es wird nicht zwischen lokalen Entwicklungszweigen und Entwicklungszweigen entfernter Repositories unterschieden. Obwohl es keinen technischen Unterschied zwischen verschiedenen Repositories gibt (außer dem zwischen \"normalen\" und \"bare\"-Repositories auf Servern, bei denen kein \"Working-Tree\", also die echten Dateien existiert), gilt die Kopie, auf die von einer Projekt-Homepage aus verwiesen wird, häufig als das offizielle Repository, in das die Revisionen der Entwickler übertragen werden. Es existieren spezielle \"Remote-tracking branches\", das sind Referenzen (siehe \"Nicht-lineare Entwicklung\"), die auf den Stand eines anderen Repositorys zeigen.\n\nDaten können neben dem Übertragen auf Dateisystemebene (codice_3) mit unterschiedlichen Netzwerkprotokollen zwischen Repositories übertragen werden. Git hat ein eigenes, sehr effizientes Protokoll, das den TCP-Port 9418 nutzt (codice_4), welcher allerdings nur zum \"Fetchen\" und \"Clonen\" genutzt werden kann, also dem Lesen eines Repositorys. Ebenso kann der Transfer über SSH (codice_5, das gängigste Protokoll für Schreiboperationen), HTTP (codice_6), HTTPS (codice_7) oder über (weniger effizient) FTP (codice_8) oder rsync (codice_9) erfolgen. Die Übertragung in das offizielle Repository eines Projekts erfolgt häufig in Form von Patches, die via E-Mail an den Entwickler oder eine Entwicklungs-Mailing-Liste geschickt werden. Alternativ kann auch ein Review-System wie Gerrit verwendet werden. Für Projekte, die auf Websites wie GitHub oder Bitbucket gespeichert (\"hosting\") werden, kann eine Änderung einfach durch das \"Pushen\" eines Branches vorgeschlagen werden, der dann bei Bedarf durch ein \"Merge\" in das Projekt integriert wird.\n\nDie Geschichte eines Projektes wird so gespeichert, dass der Hash-Wert einer beliebigen Revision \"(commit)\" auf der vollständigen Geschichte basiert, die zu dieser Revision geführt hat. Dadurch ist es nicht möglich, die Versionsgeschichte nachträglich zu manipulieren, ohne dass sich der Hash-Wert der Revision ändert. In der Kryptographie nennt man dies einen Hash-Baum. Einzelne Revisionen können zusätzlich markiert \"(tagging)\" und optional mit GPG digital signiert werden \"(signed tag)\", beispielsweise um den Zustand zum Zeitpunkt der Veröffentlichung einer neuen Version der Software zu kennzeichnen.\n\nEs gibt Versionsverwaltungssysteme (beispielsweise CVS), die für jede Datei (und jedes Verzeichnis) eigene, von allen anderen Dateien unabhängige Revisionsnummern verwalten. Für eine Datei wird nur dann eine neue Nummer erzeugt, wenn die jeweilige Datei Teil einer Commit-Operation ist. Im Gegensatz dazu weist Git bei jedem Commit allen im Repository verwalteten Dateien (und Verzeichnissen) eine neue, für alle Dateien gleiche Revisionsnummer zu. Das Abspeichern selbst erfolgt, indem im Commit-Objekt ein Verweis auf die Projektwurzel als \"tree\"-Objekt gespeichert wird, das wiederum Verweise auf \"blobs\" (\"binary large objects\", die reinen Inhalte der Dateien ohne Identifizierung) und weitere \"trees\" (Verzeichnisse) enthält. Ein \"tree\"-Objekt verweist (wie ein Verzeichnis-Inode) mit seinen Einträgen auf SHA1-Prüfsummen, die weitere \"trees\" und \"blobs\" identifizieren, ähnlich Inode-Nummern in Dateisystemen. Wenn eine Datei in einem Commit nicht geändert wird, ändert sich auch die Checksumme nicht, und sie braucht nicht nochmals gespeichert zu werden. Die Objekte liegen im Projekt unter codice_10. Über Git-„Bordmittel“ lässt sich jeder beliebige Commit über den zugeordneten Hash-Wert (die Prüfsumme) eindeutig identifizieren, separat auslesen, verschmelzen oder gar als Abzweigungspunkt nutzen – vergleichbar mit den Revisionsnummern in anderen Systemen.\n\nDie Daten gelöschter und zurückgenommener Aktionen und Entwicklungszweige bleiben vorhanden (und können wiederhergestellt werden), bis sie explizit gelöscht werden.\n\nEs gibt Hilfsprogramme, die Interoperabilität zwischen Git und anderen Versionskontrollsystemen herstellen. Solche Hilfsprogramme existieren unter anderem für GNU arch \"(git-archimport)\", CVS \"(git-cvsexportcommit\", \"git-cvsimport\" und \"git-cvsserver)\", Darcs (\"darcs-fastconvert\", \"darcs2git\" und andere), Quilt \"(git-quiltimport)\" und Subversion \"(git-svn)\".\n\nMit Gitweb gibt es eine in Perl geschriebene Weboberfläche. Der Team Foundation Server von Microsoft hat eine Git-Anbindung (\"Git-tf\").\n\nDie aktuelle Version wird produktiv für die Entwicklung vieler Open-Source-Projekte eingesetzt, darunter Amarok, Android, BusyBox, CMake, Debian, DragonFly BSD, Drupal, Eclipse, Erlang, Fedora, FlightGear, Git selbst, Gnome, Joomla, jQuery, JUnit, KDE, LibreOffice, LilyPond, Linux-Kernel, Linux Mint, MediaWiki, node.js, One Laptop per Child, OpenFOAM, OpenStreetMap, Perl 5, Parrot und Rakudo (Perl 6), PHP, phpBB, Plone, PostgreSQL, Python, Qt, Ruby on Rails, Ruby, Samba, Scala, TaskJuggler, TYPO3, VLC media player, Wine, x264 und X.org. Außerdem wird Git von einer Vielzahl weiterer Open-Source-Projekte genutzt, wie sie beispielsweise auf Plattformen wie GitHub, GitLab, oder BitBucket zu finden sind.\n\nGit wird auch in kommerziellen Projekten verwendet. So gab beispielsweise Microsoft im August 2017 bekannt, die Versionsverwaltung von Windows auf Git umgestellt zu haben.\n\nObwohl Git primär zur Versionsverwaltung von Quellcode entwickelt wurde, wird es auch zum Speichern von flach strukturierten (im Gegensatz zu relationalen Strukturen) Datensätzen direkt als Datei genutzt. So können Funktionen wie Versionsverwaltung, Hook, diff, Replikation und Offline-Nutzung auch für Inhalte ohne Datenbank genutzt werden. Die Nutzung ist ähnlich zu NoSQL, auch wenn Git keine Indexstruktur, Abfrage oder Gleichzeitigkeit erlaubt.\n\nDer Einsatz erstreckt sich auch auf inhaltlich einfach strukturierte Systeme wie CMS oder Wikis.\n\nGit läuft auf fast allen modernen unixartigen Systemen wie Linux, Solaris, macOS, FreeBSD, DragonFly BSD, NetBSD, OpenBSD, AIX, IRIX.\n\nEs gibt mehrere Portierungen von Git für Microsoft Windows:\n\n\n\n\n"}
{"id": "655575", "url": "https://de.wikipedia.org/wiki?curid=655575", "title": "Internet Chess Club", "text": "Internet Chess Club\n\nDer Internet Chess Club (ICC) ist ein kommerzieller Schachserver, der Schachspielern aus der ganzen Welt ermöglicht, rund um die Uhr einen adäquaten Spielpartner zu finden. Der Club besteht seit März 1995, Vorgänger war der \"Internet Chess Server\" (ICS). Er hat heute nach eigenen Angaben über 30.000 Mitglieder.\n\nEiner der führenden Entwickler der eingesetzten Software ist Daniel Sleator, Professor für Informatik an der Carnegie Mellon University, der seit 1992 zunächst ehrenamtlich an der Entwicklung beteiligt war, sich den Quellcode dann aber 1994 per Copyright schützen ließ. Die Kommunikation mit dem Server läuft über die proprietäre Client-Software \"BlitzIn\" (derzeit Version 2.52) oder \"ICC Dasher\" (derzeit Version 1.5.4). Auch mehrere Schachvarianten werden unterstützt.\n\nSehr schnell nach offizieller Eröffnung des Clubs wuchs die Mitgliederzahl auf über 10.000, obwohl es einige Forks wie den Free Internet Chess Server (FICS) gab, die weiterhin kostenloses Spielen anboten. Die Attraktivität für die Schachspieler liegt in der Tatsache begründet, dass die weltbesten Spieler Mitglied sind, bzw. in unregelmäßigen Abständen am Spielbetrieb teilnehmen. Vom 13. Weltmeister Garri Kasparow und vom 14. Weltmeister Wladimir Kramnik ist eine Vielzahl gespielter Partien erhalten und publiziert worden. Die Mitgliedschaft ist kostenpflichtig, für Titelträger (GM und IM) ist sie frei.\n\nJedes angemeldete Mitglied darf einen Benutzernamen wählen, der es ihm bei Bedarf erlaubt, seinen wahren Namen geheim zu halten. Überwiegend werden Blitz- oder Bulletpartien gespielt, dabei können Mitglieder eine Wertungszahl erspielen. \n\nDer ICC überträgt Partien von Großmeisterturnieren aus der ganzen Welt. Bei besonders wichtigen Partien kommentieren kompetente Großmeister vor einem großen Publikum die gespielten Züge. Außerdem gibt es Vorträge und Simultanvorstellungen. Online-Training kann mit einer \"Chekel\" genannten Verrechnungseinheit bezahlt werden. \n\n"}
{"id": "656137", "url": "https://de.wikipedia.org/wiki?curid=656137", "title": "TUGZip", "text": "TUGZip\n\nTUGZip ist ein Datenkomprimierungsprogramm für Microsoft Windows.\n\nDas Programm wird von dem Schweden Christian Kindahl entwickelt. Die grafische Benutzeroberfläche fügt sich mittels Drag and Drop und Kontextmenüs nahtlos in Windows ein. Das Programm liest auch Speicherabbilddateien wie ISO-Abbilder, C2D, IMG, ISO und NRG. Externe Plugins können nachinstalliert werden.\n\n\"TUGZip\" beherrscht auch die AES-Verschlüsselung (Rijndael mit 128, 192 bzw. 256 Bit), daneben können Archive mit verschiedenen anderen Algorithmen verschlüsselt werden: Blowfish (128-bit), DES (56-bit) und Triple DES (168-bit).\n\nMit \"TUGZip\" lassen sich komprimierte Dateien der Formate 7z, BH, BZ2 (TBZ), CAB, JAR, LHA, SQX, TAR, YZ1 und ZIP bearbeiten. Darüber hinaus lassen sich alle gängigen Formate öffnen. \"TUGZip\" ist in über 20 Sprachen verfügbar, unter anderem auf Deutsch.\n\n\n"}
{"id": "656466", "url": "https://de.wikipedia.org/wiki?curid=656466", "title": "Adobe Creative Suite", "text": "Adobe Creative Suite\n\nDie Adobe Creative Suite [] ist ein Programmpaket von Design-, Grafik- und Produktionssoftware des US-amerikanischen Unternehmens Adobe Systems. Die erste Version kam im September 2003 mit aktualisierten Versionen der einzelnen Programme auf den Markt.\n\nDie letzte Version der \"Adobe Creative Suite 6\" („CS6“) wurde am 23. April 2012 vorgestellt und am 7. Mai 2012 veröffentlicht.\nNeuere Versionen der Einzelprogramme wurden vom Hersteller über die Vertriebsplattform \"Creative Cloud\" angeboten. Adobe hat den Vertrieb der Creative Suite am 9. Januar 2017 eingestellt.\n\nDie eigenständigen Programme der Sammlung decken jeweils einen speziellen Anwendungsbereich ab:\n\n\nAdobe vertrieb die Creative Suite Anwendungen zuletzt in fünf verschiedenen Kombinationen, den sogenannten „Editionen“:\n\nAdobe bot ebenfalls sogenannte „Student and Teacher“-Editionen aller oben genannten Softwarepakete an. Ab der Version CS5 ist auch eine kommerzielle Nutzung der vergünstigten Schüler- und Studenteneditionen erlaubt.\n\nNachfolgende Tabelle vergleicht die einzelnen Editionen der Adobe Creative Suite 6:\nDie im April 2005 veröffentlichte zweite Auflage der Creative Suite umfasste die Anwendungen Photoshop CS2, Indesign CS2, Illustrator CS2, Golive CS2 sowie Adobe Acrobat 7.0 Professional. Neu kam damals der Designprozess-Manager Version Cue CS2 hinzu, ebenso wie der Dateibrowser Adobe Bridge als Schaltzentrale. Erhältlich war die Suite für Windows (32 bit) und MacOS (PowerPC).\n\nIm Dezember 2012 wurde der Aktivierungsserver „wegen eines technischen Fehlers deaktiviert.“ Kunden werden aufgefordert, aktuelle Versionen dieser Software zu verwenden. Wer die Software dennoch weiterverwenden will, muss die Programme und Standard-Seriennummern als Versionen ohne Aktivierungszwang von der Adobe-Website herunterladen.\n\nAm 27. März 2007 wurde eine neue Auflage der Creative Suite angekündigt. Die Versionen sind vielfältig:\nAllen Versionen gemeinsam sind: Adobe Bridge CS3, Adobe Device Central CS3, Adobe Stock Photos (wurde am 1. April 2008 eingestellt), Adobe Acrobat Connect. Adobe Version Cue CS3 ist nur in Production Premium nicht enthalten.\n\nAm 23. September 2008 kündigte Adobe die Creative Suite 4 an, die seit Mitte November 2008 in Deutschland erhältlich ist. Die Programmpakete wurden leicht verändert. Größere Fortschritte hat Adobe bei der Zusammenarbeit der verschiedenen Programme angekündigt.\nZudem wurden die Benutzeroberflächen vereinheitlicht.\nAm 12. April 2010 kündigte Adobe die Creative Suite 5 an. Dabei wurde die Version „Web Standard“ gestrichen. Die Pakete haben sich dagegen kaum verändert, doch die Preisempfehlungen sind teils deutlich gesenkt worden.\nAm 11. April 2011 kündigte Adobe die Creative Suite 5.5 an. Als wichtigste Neuerung wurde ein neues Lizenzpreismodell auf monatlicher Mietbasis eingeführt. Soundbooth wurde in dieser Veröffentlichung durch Adobe Audition CS5.5 ersetzt.\nDie Adobe Creative Suite 6 konnte ab dem 23. April 2012 vorbestellt werden und war seit dem 7. Mai verfügbar. Photoshop CS 6 konnte bereits davor als Beta-Version kostenlos getestet werden. Die Testperiode endete mit dem Verkaufsstart der Creative Suite 6. Benutzer der Versionen Creative Suite 3 und 4 konnten bis Ende 2012 die normalen Upgrade-Pakete nutzen, um auf Creative Suite 6 upzugraden. Danach verfiel dieses Recht und stand nur noch ab Creative Suite 5.0 zur Verfügung. Adobe kündigte dieses Modell aufgrund des Rhythmuswechsels bei den Versionsveröffentlichungen ausdrücklich als Übergangslösung an. In Zukunft sollten immer nur die beiden vorangegangenen Versionen, inklusive Zwischenversionen wie CS 5.5, upgradefähig gehalten werden.\nMit Einführung der Creative Suite CS6 startete Adobe den Abodienst Adobe Creative Cloud, dessen Nutzer für einen monatlichen Beitrag vollen Zugang zu allen Funktionen der Adobe Creative Suite CS6 erhalten.\nIm Mai 2013 äußerte sich Adobe zu den weiteren Entwicklungsplänen. Demnach werde das Produkt künftig nicht mehr als Kaufsoftware weiterentwickelt, sondern nur noch als Mietsoftware im Cloud-Computing unter dem Namen Adobe Creative Cloud. Für das Abo-Modell sollen rund 60 Euro im Monat fällig werden. Aus diesem Grund soll die Version 6 zwar auch künftig noch vertrieben werden, werde aber nach Aussage von Adobe künftig nicht mehr weiterentwickelt.\n\n\n"}
{"id": "656723", "url": "https://de.wikipedia.org/wiki?curid=656723", "title": "Microsoft Multiplan", "text": "Microsoft Multiplan\n\nMicrosoft Multiplan war ein Tabellenkalkulationsprogramm von Microsoft.\n\nMultiplan wurde 1982 veröffentlicht, zunächst für CP/M und MS-DOS/PC DOS, später auch für das Betriebssystem des Apple II sowie den Apple Macintosh. Außerdem gab es eine Version für den Commodore 64.\n\nDer größte Konkurrent war Lotus 1-2-3, welches den Markt der IBM-PCs und dazu kompatiblen Computern dominierte, da es Berechnungsergebnisse schneller anzeigte und Ergebnisse auch grafisch darstellen konnte.\n\nMicrosoft Excel kann als Nachfolger von Microsoft Multiplan gesehen werden. Es unterstützte auf allen Plattformen die Maus als Eingabegerät, bei Multiplan war das nur bei einigen Plattformen wie dem Macintosh gegeben. Nach der Veröffentlichung von Windows 2.03 und etwa ein Jahr nach dem Apple Macintosh wurde Multiplan nicht mehr weiterentwickelt.\n\nMicrosoft Multiplan hatte in den USA nur mäßigen Erfolg und erzielte dort nicht über 15 Prozent Marktanteil. Durch die schnelle Übersetzung ins Deutsche wurde das Programm im deutschsprachigen Raum mit einem Marktanteil von 50 Prozent zu einem Erfolg. Im Jahre 1990, mit Erscheinen der letzten Version 4.2, wurde die Entwicklung von Multiplan eingestellt, da Microsoft schon seit 1987 Excel parallel auf dem Markt etabliert hatte.\n\nMit dem Programm Microsoft Chart konnten Multiplan-Dokumente grafisch aufbereitet dargestellt werden.\n\nIm Gegensatz zum allerersten Tabellenkalkulationsprogramm Visicalc und später Lotus 1-2-3, bei welchen die Zeilen mit Zahlen und die Spalten mit Buchstaben bezeichnet waren, wurden sie bei Multiplan nach einem anderen System identifiziert. Sie wurden in dem System L1C1 (\"L\"ine 1, \"C\"olumn 1), L2C2 usw. referenziert, auf Deutsch \"Z1S1\", was als alternative Anzeigevariante (Deutsch: Z1S1-Bezugsart) auch Einzug in das spätere Excel gefunden hat. Die Zelle in der 3. Zeile und der 15. Spalte hatte zum Beispiel die \"Koordinate\" Z3S15. Bei Lotus und anderen Tabellenkalkulationsprogrammen würde diese Zelle O3 heißen.\n\n\n\n\n"}
{"id": "657137", "url": "https://de.wikipedia.org/wiki?curid=657137", "title": "Slipstreaming", "text": "Slipstreaming\n\nAls Slipstreaming bezeichnet man das Integrieren eines oder mehrerer Service Packs, Hotfixes oder Patches in das Installationsmedium eines Software-Produkts (meist Windows oder Microsoft Office). Somit wird die direkte Installation einer aktuellen Version der Software ermöglicht, ohne im Anschluss manuell die besagten Service Packs, Bugfixes oder sonstigen Updates installieren zu müssen.\n\nZunächst werden die \"alten\" Setupdateien vom Medium auf die Festplatte kopiert, um die Verbesserungen einzuspielen. Nach erfolgter Aktualisierung und Anpassung verschiedener Parameter in den Windows-Setup-Dateien, zum Beispiel der txtsetup.sif, werden alle Dateien, inklusive der Verbesserungen, in einem neuen Verzeichnis zusammengestellt, welches man dann auf CD brennen und somit ein neues Installationsmedium erstellen kann.\n\nProgramme, die dieses Verfahren automatisch und relativ bedienerfreundlich ausführen, sind zum Beispiel nLite (für XP), NTLite (für Windows 7 bis Windows 10), vLite (für Vista), Win Toolkit (für Windows 7 und 8), xp-Iso-Builder, Office SP Slipstreamer für Office 2003 & 2007 oder WUCD (Windows Unattended CD Creator).\n\nManche Programme liefern die Slipstream-Funktionalität bereits mit. Hier kann man mit bestimmten Kommandozeilenparametern (Schaltern) zum gewünschten Ergebnis kommen. Neben dem Windows-Setup seit Windows 2000 sind dies zum Beispiel MSI-Pakete, die mittels MSP-Updatepaketen aktualisiert werden können, so etwa bei Microsoft-Office-Programmen. Bei anderen Programmen muss man sich verschiedener Hilfsprogramme bedienen, um das Gleiche zu erreichen.\n\n"}
