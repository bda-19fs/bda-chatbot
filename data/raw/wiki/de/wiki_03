{"id": "92079", "url": "https://de.wikipedia.org/wiki?curid=92079", "title": "Ext3", "text": "Ext3\n\n</math> (oder die Anzahl von Datenblöcken, es wird der niedrigere Wert genommen), und die minimale Anzahl von Inodes ist formula_1. Der voreingestellte Wert ist für die meisten Anwendungsszenarien ausreichend.</ref>\nDas ext3 \"(third extended filesystem)\" ist ein Journaling-Dateisystem, das für den Linux-Kernel entwickelt wurde. Es war der Vorläufer von ext4 und wurde bei vielen Linux-Distributionen als Standard-Dateisystem verwendet.\n\nDie von Stephen Tweedie entwickelte Journaling-Erweiterung für ext2 sorgt dafür, dass Metadaten nicht mehr korrumpiert werden können und somit auf einen kompletten Durchlauf der Integritätsprüfung \"e2fsck\" nach einem Rechnerabsturz verzichtet werden kann. Die Kombination von \"ext2\" mit der Journal-Erweiterung wird als \"ext3\" bezeichnet.\n\nDabei ändert sich das Datenformat des Datenträgers bei der Verwendung eines Journals nicht. Die Daten können deshalb mit einem ext2-Dateisystemtreiber gelesen werden. Das Journal ist eine Dateistruktur, in die Metadaten (optional die Nutzdaten) geschrieben werden, bevor sie auf das tatsächliche Dateisystem geschrieben werden. Aus einem ext2- kann daher ein ext3-Dateisystem gemacht werden, ohne irgendwelche Daten konvertieren zu müssen.\n\nAls direkter Nachfolger existiert ext4.\n\nIn Linux 4.3 wurde der Code des nativen Treibers für Ext3 endgültig entfernt. Ext3 wird weiterhin vom Treiber des Nachfolgers Ext4 unterstützt.\n\next3 fügt dem ext2-System folgende Fähigkeiten hinzu:\n\nBei H-Bäumen (englisch \"Htree\") handelt es sich um eine spezielle Form von B-Bäumen, die für ext3 entwickelt wurden.\n\nWenn eine Änderung am Dateisystem (zum Beispiel die Umbenennung einer Datei) durchgeführt wird, wird sie als Transaktion im Journal vermerkt und kann im Fall eines Absturzes entweder abgeschlossen oder noch nicht abgeschlossen sein. Wenn eine Transaktion zum Absturzzeitpunkt abgeschlossen war, ist garantiert, dass alle an dieser Transaktion beteiligten Blöcke einen gültigen Dateisystemstatus repräsentieren. Diese Blöcke werden anschließend ins Dateisystem kopiert. Wenn eine Transaktion zum Absturzzeitpunkt nicht abgeschlossen war, kann nicht garantiert werden, dass die beteiligten Blöcke konsistent sind, daher wird eine solche Transaktion verworfen (das bedeutet, dass die Dateisystemänderung, die diese Transaktion repräsentierte, verlorengeht).\n\nBei abgebrochenen Schreiboperationen kann es passieren, dass ein Teil einer Datei bereits aus den neuen Daten besteht und ein Teil noch aus den alten, was manchmal noch schlimmer sein kann als ein inkonsistentes Dateisystem. ext3 bietet daher einen besonderen Modus, in dem Daten zunächst im Journal abgelegt werden. ext3 schützt nicht davor, dass Daten verlorengehen, die zum Absturzzeitpunkt zwar bereits auf die Platte geschrieben sein sollten, vom Kernel jedoch noch in sogenannten \"schmutzigen Puffern\" gehalten wurden, um sie später zurückzuschreiben. Nach dem Abspielen des Journals ist nur garantiert, dass mit einem konsistenten Datenbestand zu einem gegebenen Zeitpunkt weitergearbeitet werden kann.\n\nDie Linux-Implementierung von ext3 bietet drei Journaling-Stufen:\n\n\nEs gibt verschiedene Treiber, Programme und Plugins, die einen Zugriff auf ext3, durch die Abwärtskompatibilität ebenfalls auf ext2, unter anderen Betriebssystemen außer den Nativen zulassen. Diese sind insbesondere von Nutzen, wenn Laufwerke unter mehreren Betriebssystemen verwendet werden sollen, so zum Beispiel ein USB-Laufwerk in einer gemischten Linux/Windows-Umgebung. Sie stellen eine Alternative zu den Ansätzen dar, in solchen Linux/Windows-Umgebungen das in seinen Fähigkeiten sehr limitierte Dateisystem FAT/FAT32 zu verwenden oder das proprietäre NTFS von Microsoft durch Reverse-Engineering Linux-basierten Betriebssystemen zugänglich zu machen, wie beispielsweise NTFS-3G. Insbesondere in Dual-Boot-Konfigurationen ist es vorteilhaft, auf eine ext3-Linux-Partition zugreifen zu können und diesen Speicherplatz für Windows nutzbar zu machen. Da ext3, wie viele UNIX basierte Dateisysteme, zwischen Groß- und Kleinschreibung unterscheidet, kann es bei der Nutzung unter Betriebssystemen welche Groß- und Kleinschreibung ignorieren, wie z. B. Windows, zu gravierenden Problemen kommen.\n\nExt3 ist langsamer als andere moderne Journaling-Dateisysteme, wie zum Beispiel XFS oder JFS, dafür jedoch relativ robust.\n\nWeiterhin überschreibt ext3 bei Löschvorgängen die Block-Pointer der Inodes mit Nullen. Dies erschwert ein Wiederherstellen gelöschter Dateien, erhöht jedoch die Wahrscheinlichkeit, dass die Integrität des Dateisystems nach einem Programmfehler oder Systemausfall ohne Datenverlust wiederhergestellt werden kann. Ein Wiederherstellen der Daten ist mitunter dennoch möglich.\n\n\n"}
{"id": "92216", "url": "https://de.wikipedia.org/wiki?curid=92216", "title": "Fedora (Linux-Distribution)", "text": "Fedora (Linux-Distribution)\n\nFedora [] ist eine RPM-basierte Linux-Distribution. Ziel der Entwickler der Distribution ist es, Freie Software zu fördern und ein Betriebssystem für eine möglichst vielfältige Zielgruppe zu gestalten. Organisiert wird die Entwicklung in der Online-Community des Fedora-Projekts, das vom Unternehmen Red Hat angeführt wird. Fedora ist der direkte Nachfolger von Red Hat Linux. Das englische Wort Fedora bezeichnet eine spezielle Art des Filzhuts, das Markenzeichen des Unternehmens \"Red Hat\".\n\nFedora wird für den generellen Einsatz auf Server- und Desktop-Systemen entwickelt. Das Fedora-Projekt selbst bezeichnet seine Distribution als geeignet für Einsteiger ebenso wie für erfahrene Benutzer. Trotzdem betonen Kritiker oft, dass Fedora nicht besonders für Linux-Einsteiger geeignet sei, da es in gewissen Bereichen zu kompliziert sei. Diese unterschiedliche Wahrnehmung könnte daraus entstehen, dass die Entwickler stärker das auf der Basis von Fedora entstehende Enterprise-Linux – dessen Nutzer vorwiegend in Unternehmen und staatlichen Organisationen zu finden sind – vor Augen haben als den durchschnittlichen Anwender.\n\nIm Gegensatz zu anderen Linux-Distributionen gibt es keinen Langzeit-Support. Der Lebenszyklus einer Fedora-Version ist auf 13 Monate angelegt, etwa alle sechs Monate erscheint eine neue Version, weshalb Fedora für eine langfristig geplante Verwendung (z. B. auf Embedded-Systemen oder Systemen mit jahrelanger Uptime) ungeeignet ist. Dafür bietet es im Gegensatz zu anderen Distributionen stets sehr aktuelle Software.\n\nDas \"Fedora-Projekt\" formuliert seine Ziele wie folgt:\nDiese Absichten werden ausgedrückt in den vier Prinzipien:\n\nDie „Freiheit“ bezieht sich auf die Freiheit der Software im Gegensatz zu proprietärer Software. Die Förderung solcher FLOSS ist den Entwicklern ein Anliegen, weswegen Fedora gratis, quelloffen und ausdrücklich zur Weiterentwicklung angeboten wird. „Freunde“ repräsentiert die Stärke dieser weltweiten Gemeinschaft (Community) unterschiedlichster Menschen, die gemeinsam an der Entwicklung freier Software arbeiten. „Funktionen“ drückt das Streben nach Vielseitigkeit und Flexibilität aus, die Bedürfnisse möglichst vieler Anwender bestmöglich abzudecken. „Zuerst“ steht für den Innovationswillen und die angestrebte Vorreiterrolle innerhalb der Open-Source-Bewegung.\n\nWie schon beim Vorgänger von Fedora gilt auch hier der Grundsatz, dass nur Computerprogramme mit vollständig freier Lizenz in die Distribution aufgenommen werden dürfen. Die einzige Ausnahme bildet Firmware in binärer Form (Binärblob), die mit einer Lizenz zur Verfügung gestellt wird und die eine freie Nutzung erlaubt. Prominenteste Folge dieser Lizenzpolitik ist, dass Fedora z. B. keine Unterstützung für die MP3- oder DVD-Video-Wiedergabe mit sich bringt, sondern diese aus Drittquellen installiert werden müssen.\n\nZur \"Fedora-Gemeinschaft\" gehören Mitarbeiter des Unternehmens Red Hat ebenso, wie Entwickler anderer Unternehmen, unabhängige ehrenamtliche Entwickler, darunter auch Studenten. Neben Software-Entwicklern gibt es Spezialisten für das Marketing, die linguistische Übersetzung oder die grafische Gestaltung. Um die Zusammenarbeit dieser teilweise sehr inhomogenen Community zu erleichtern, gibt sich das Fedora-Projekt zwei Grundsätze:\n\nDas Fedora-Projekt wird von einem zehnköpfigen sogenannten \"Fedora Board\" (englisch für \"Gremium\") geführt. Neben dem Vorsitzenden – der vom Unternehmen Red Hat bestimmt wird und ein Vetorecht besitzt – besteht das \"Board\" aus vier vom Vorsitzenden ernannten Mitgliedern und fünf durch die \"Fedora Community\" gewählten Mitgliedern. Das \"Board\" entscheidet über alle strategischen und finanziellen Belange des Projekts. Die technische Leitung und damit die Verantwortung für die Entwicklungsarbeit des Projekts liegt beim sogenannten \"Fedora Engineering Steering Committee\" (kurz \"FESCo\", engl. für \"Technisches Steuerungs-Komitee\"). Das \"FESCo\" besteht aus neun durch die \"Fedora Community\" gewählten Mitglieder. Die Diskussionen und Entscheidungen von beiden Gremien sind meist online und öffentlich nachverfolgbar, da die regelmäßigen Sitzungen meist in einem IRC-Kanal stattfinden. Der Vorsitzende des \"Boards\" koordiniert gleichzeitig auch die Umsetzung der Entscheidungen von \"Board\" und \"FESCo\" als amtierender Fedora-Projektleiter \"(FPL)\". Seit Juni 2014 ist \"Matthew Miller\" Vorsitzender und \"FPL\".\n\nDas Fedora-Projekt wird hauptsächlich durch das Unternehmen Red Hat finanziert. Red Hat möchte eine möglichst vollständige und solide Grundlage für ihr kommerzielles Produkt \"Red Hat Enterprise Linux\" (RHEL) erhalten. Das Sponsoring von Red Hat besteht neben der Bereitstellung finanzieller Mittel vorwiegend aus der Mitarbeit zahlreicher Red Hat-Angestellter in der \"Fedora Community\". Daneben stellt Red Hat seine Infrastruktur, wie z. B. Server oder Konferenzräume dem Fedora-Projekt kostenfrei zur Verfügung.\n\nNeben Red Hat und einzelnen privaten Personen sind die Unternehmen \"Arrival Telecom Inc., BodHOST, Colocation America Inc., Dell Inc., Ibiblio, InterNetX GmbH, OSU Open Source Lab, proIO GmbH, ServerBeach, Technomonk Industries, Telia Company AB\" und \"tummy.com ltd.\" als Sponsoren von Fedora bekannt. Einige dieser Unternehmen verkaufen Hardware mit vorinstallierter Fedora-Software oder sie sind Nutzer der Distribution.\n\nDas Fedora-Projekt veranstaltet mehrmals jährlich Tagungen, die \"FUDCon\" (Fedora Users and Developers Conference) genannt werden, zu denen sich Fedora-Benutzer und -Entwickler für mehrere Tage irgendwo auf der Welt treffen. Neben Vorträgen, Podien, Sitzungen und sozialen Anlässen werden an den \"FUDCons\" sogenannte Hackfests organisiert, bei denen eine Gruppe von Entwicklern eine spezifische Fragestellung zusammen angeht und programmiert.\n\nFedora erfreut sich unter anderem als Nachfolger der alten Red-Hat-Linux-Versionen großer Beliebtheit und hat dieses in vielen Unternehmen und Institutionen ersetzt. Die meisten kommerziellen Anwendungen, Daemons und Treiber, die früher für RHL zur Verfügung gestellt wurden, werden mittlerweile ebenfalls für Fedora angeboten, womit diese Distribution eine breitflächige Unterstützung durch Unternehmen und Institutionen bekommt. Laut den Erhebungen von Distrowatch gehört Fedora seit dem Jahr 2004 zu den fünf bedeutendsten Linux-Distributionen der Welt.\n\nBis 2008 wurde die Wikipedia auf \"Red Hat Linux 9\" und unterschiedlichen Fedora-Versionen gehostet. Ende 2008 wurden alle 400 Server der \"Wikimedia Foundation\" zu Ubuntu migriert. Der von IBM gebaute Supercomputer \"Roadrunner\", der am National Nuclear Security Administration (NNSA) betrieben wird, verwendet sowohl \"RHEL\" als auch Fedora als Betriebssystem. Die philippinische Regierung ließ im Jahr 2008 13.000 Computer, auf denen Fedora installiert war, an Schulen verteilen. An der \"ARM TechCon 2013\" stellte Dell den ersten ARM-64-Bit-Server vor, der mit Fedora als Betriebssystem ausgestattet war.\n\nEine Erhebung der Nutzerzahlen ist nur mittels Schätzung möglich, da anders als bei kommerzieller Software keine Verkaufszahlen erhoben werden können und keine Produktaktivierung für die Nutzung notwendig wird. Es können also lediglich die Downloadzahlen der Installationsmedien (ISO-Images) und die Zugriffe auf die Update-Server (Repositories) gezählt werden. Diese Erhebungsmethoden sind mit einer großen Unschärfe behaftet, da das System mit einem einzelnen Installationsmedium (CD-ROM oder DVD) auf vielen Computern installiert werden kann und wiederum oft IP-Adressen dynamisch genutzt werden, was zu einer Mehrfachzählung führen kann.\n\nIn der gesamten Lebenszeit von \"Fedora 16\" wurden insgesamt 2.143.906 Installationsmedien heruntergeladen. Vom Mai 2007 bis im Mai 2012 bezogen durchschnittlich etwa 5 Millionen einzelne IP-Adressen pro Fedora-Version Updates von einem der Repository-Server.\n\nUnter Webhostern war Fedora im Dezember 2005 die am drittstärksten verbreitete Linux-Distribution. Zählt man alle Derivate von Fedora zusammen und bezieht \"Red Hat Enterprise Linux\" und dessen Derivate wie \"CentOS\" oder \"Scientific Linux\" ein, so ergab sich 2005 eine Verbreitung von etwa 50 Prozent.\n\nEnde 2008 erhob das Fedora-Projekt den Anspruch auf die Führung in der Linux-Welt. Dies geschah aufgrund ähnlicher Äußerungen des Ubuntu-Entwicklers Canonical. Angeblich soll es 8 Millionen Ubuntu-Installationen gegeben haben. Das Fedora-Projekt zählte 3,4 Millionen aktive Installationen von \"Fedora 7\", 3,9 Millionen \"Fedora-8\"-Installationen und 1,8 Millionen von \"Fedora 9\" und leitete aus diesen Zahlen seinen Anspruch auf die führende Rolle im Linux-Geschäft ab. Red Hat weist darauf hin, dass die eigenen Zahlen – wie die der Konkurrenz – mit den weiter oben beschriebenen großen Unsicherheiten behaftet seien.\n\nFedora wird, laut Interviews von 2008, 2012 und 2014, von Linus Torvalds verwendet.\n\nFür \"Fedora 19\" wurde ein Prozessor mit einer Taktrate von mindestens 1 GHz, 1 GB Arbeitsspeicher und 10 GB freier Platz auf der Festplatte als minimale Anforderung genannt. Fedora kann auch ohne Grafikhardware betrieben werden, was z. B. auf einem Server sinnvoll sein kann. Für eine grafische Oberfläche wird ein Grafikprozessor, der neuer ist als Intel GMA900, Nvidia GeForce-FX NV30 und ATI-(AMD-)Radeon 9500, empfohlen. Um eine flüssige grafische Darstellung zu erhalten, sind leistungsfähigere Grafikprozessoren notwendig. Die Desktop-Umgebungen \"Xfce\" und besonders \"LXDE\" sind für einen geringen Ressourcen-Verbrauch optimiert und benötigen deshalb weniger leistungsfähige Hardware als \"Gnome 3\" oder \"KDE\".\n\nFedora wird primär für x86- und AMD64-Architekturen entwickelt. Daneben gibt es – meist nach dem offiziellen Release fertiggestellte – Varianten für die ARM-, PowerPC- (64 Bit) und s390x-Architekturen. Da die Bedeutung der ARM-Plattform in den letzten Jahren stark zugenommen hat, wurde geplant, bei der Fedora-Version 20 der ARM-Variante die gleiche Priorität einzuräumen wie den x86- und AMD64-Architekturen. Pidora ist ein vom \"Fedora Projekt\" unterstütztes Remixe für das Raspberry Pi. Für nicht mehr aktuelle Fedora-Versionen gab es teilweise Varianten für die folgenden Architekturen: IA-64, PowerPC (32 Bit), SPARC (32 und 64 Bit) und System/390.\n\nFedora enthält Treiber für den Großteil der aktuell verfügbaren PC-Hardware. Auf den Installations-Medien (CD-ROM oder DVD) ist nur ein Teil der Treiber enthalten, weshalb gewisse Treiber nachträglich nachgeladen und installiert werden müssen. Fedora enthält freie Treiber für AMD- (\"radeon\" und \"radeonhd\") und Nvidia-Grafikprozessoren \"(nouveau)\", die Hardwarebeschleunigung und 3D-Unterstützung erlauben. Fedora enthält aus lizenzrechtlichen Gründen keine proprietären Treiber. Deshalb müssen solche Treiber für AMD- oder Nvidia-Grafikprozessoren aus Software-Repositories von Drittanbietern nachträglich installiert werden.\n\nSeit Fedora 21 gibt es drei sogenannte „Flavors“, (englisch Geschmacksrichtung) die auf die Teilgebiete Workstation, Server und Cloud ausgerichtet sind. Diese „Flavors“ enthalten alle die gleichen, „Base“ genannten Tools, Bibliotheken und APIs, unterscheiden sich aber in der Konfiguration sowie in den darauf aufbauenden Software-Paketen.\n\nDie bevorzugte Desktop-Umgebung ist seit \"Fedora 15\" Gnome 3. Im „Workstation-Flavor“ ist Gnome 3 deshalb enthalten. Neben den „Flavors“ gibt es spezielle sogenannte \"Spins\", die etwa mit der Desktop-Umgebung KDE, Xfce, LXDE, Cinnamon, MATE oder Enlightenment angeboten werden. Zusätzlich zu diesen, vom Fedora-Projekt selbst gepflegten \"Spins\", gibt es weitere, für bestimmte Anwendungen oder Interessen optimierte Spins. Darunter befinden sich \"Spins\" für: Sicherheitsanalysen, Elektronik-Entwicklung, Wissenschaft, Design & Gestaltung, Schüler und Robotik.\n\nFedora-Installationsmedien können in verschiedenen Formaten und Varianten als ISO-Images gratis heruntergeladen werden. Vor der Installation brennt der Benutzer das ISO-Image auf eine beschreibbare CD-ROM oder DVD. Alternativ ist es (z. B. mit dem \"liveusb-creator\" oder dem Befehl dd) auch möglich, ein ISO-Image auf einen USB-Stick oder eine Speicherkarte zu übertragen und diese zu booten.\n\nDas bootbare \"DVD\"-Image umfasst die größte Anzahl von Softwarepaketen. Für die einzelnen Spins gibt es sogenannte \"Live-Medien\", welche ein Live-System enthalten, das aber wie die \"DVD\" auch installiert werden kann. Eine \"Netzwerk-Installations-CD\" enthält nur ein minimales Fedora-System, das es nur gerade erlaubt, einen Computer hochzufahren und die Installation zu starten. Alle zu installierenden Software-Pakete werden nicht von der CD installiert, sondern während der Installation über das Internet heruntergeladen.\n\nEs gibt Händler, die Fedora-Installationsmedien gegen einen kleinen Betrag per Briefpost liefern.\n\nDas Software-Repository für \"Fedora 19\" enthielt über 36.000 verschiedene Pakete, die einen Großteil der für Linux verfügbaren Anwendungen und Tools abdeckten. Auf den Installations-Medien ist nur ein Bruchteil davon enthalten. Die Standard-Installation enthält eine Desktopumgebung und Anwendungen für verbreitete Anwendungsgebiete (E-Mail-Client, Browser, Office-Paket, Bildbearbeitungsprogramm, Mediaplayer usw.). Je nach Spin werden unterschiedliche Anwendungen als Standard-Auswahl installiert, so enthält das KDE-Spin die Calligra Suite während einige andere Spins LibreOffice als Office-Paket enthalten. Nach der Installation kann der Anwender mit der Paketverwaltung jede beliebige Anwendung aus dem Software-Umfang des Repositories oder aus Drittquellen installieren.\n\nDie Installationsroutine \"Anaconda\" ermöglicht eine grafisch geführte Installation eines Fedora-Systems, während das Programm \"Kickstart\" dies automatisieren kann, was vor allem bei der mehrfachen Installation auf Unternehmensrechnern hilfreich ist. Zur Konfiguration des Systems stehen die sogenannten codice_1-Programme zur Verfügung, die grafische Benutzeroberflächen haben. Die codice_1-Werkzeuge sind nach den üblichen Fedora-Prinzipien programmiert. Die Prinzipien fordern, dass „Management-Tools“ (Hilfsprogramme zur Systemverwaltung) nur gezielt eine einzige Aufgabe erfüllen sollen und keine exklusive Kontrolle über Konfigurationsdateien benötigen. Administratoren eines Systems sind dadurch trotz dieser Verwaltungswerkzeuge in der Lage, beliebige Änderungen manuell in Konfigurationsdateien vorzunehmen.\n\nFedora integriert vollständig die Kernel-Erweiterung SELinux, um so Mandatory Access Control systemweit zu erzwingen. SELinux ist nach einer Fedora-Installation standardmäßig aktiviert und schützt somit das System vor einer Vielzahl von Bedrohungen. Während der Installation wird der Firewall-Daemon \"firewalld\" eingerichtet. Die Firewall verhindert alle von außen kommenden Netzwerk-Verbindungen zum System und erlaubt Verbindungen nur vom System nach draußen.\n\nAuf einem neu installierten Fedora-System unterbinden SELinux und die Firewall die Kommunikation aller Serverdienste, zum Beispiel Apache oder Samba. Erst nach der Anpassung der SELinux- und Firewall-Regeln durch einen Administrator können Serverdienste mit anderen Systemen kommunizieren. Für die Konfiguration von SELinux und der Firewall stehen die grafischen Tools codice_3 und codice_4 zur Verfügung.\n\nFedora nutzt die von Red Hat entwickelte Paketverwaltung \"RPM\", um Pakete zu installieren und lokal zu verwalten. Abhängigkeiten zwischen den Paketen werden mit Hilfe von im Internet bereitgestellten Paketsammlungen – sogenannten Repositories – mit dem Programm DNF (vor Fedora 22 YUM) aufgelöst. Auf einem bereits installierten Fedora-System dient DNF dazu, automatisch Patches einzuspielen und das System aktuell zu halten. Der Administrator kann jederzeit mit DNF zusätzliche Software-Pakete installieren oder entfernen. PackageKit und YumExtender (auch bekannt als yumex) sind grafische Benutzeroberflächen für DNF.\n\nIn den Anfangszeiten gab es eine Unterstützung für up2date und APT. Die Unterstützung für Letzteres wurde mit \"Fedora Core 4\" eingestellt, da die Entwicklung von \"APT-RPM\" ruhte. Ab \"Fedora Core 5\" kann \"APT-RPM\" wieder genutzt werden, da dieses seitdem nativ auf DNF-Repositories zugreifen kann.\n\nIn Fedora sind standardmäßig die offiziellen Paketquellen des Fedora-Projekts eingerichtet. Diese enthalten alle durch das Fedora-Projekt gepflegten Pakete. Dies umfasst Repositories für Patches (Updates), experimentelle Pakete, Quellcode-Pakete und Debuginformationen.\n\nNeben der Nutzung der offiziellen Repositories hat der Benutzer die Möglichkeit, weitere Paketquellen von Drittanbietern zu konfigurieren.\n\nDie bekannteste Quelle für zusätzliche Softwarepakete, die aus ideologischen Gründen bezüglich der jeweiligen unfreien Lizenzen nicht in den offiziellen Paketquellen enthalten sind, ist \"RPM Fusion\". Dort finden sich beispielsweise patentrechtlich geschützte Multimedia-Codecs (MP3, MPEG…) und proprietäre Grafiktreiber.\n\nDarüber hinaus stellen immer mehr Softwareprojekte und Unternehmen, wie Google, Skype oder Adobe, eigene Repositories für Fedora zur Verfügung.\n\nBeginnend mit \"Fedora 21\" steht das Werkzeug \"dnf-plugin-system-upgrade\" zum Upgrade auf eine aktuellere Fedora-Version zur Verfügung und ersetzt die zuvor unterstützen Upgrade-Methoden wie \"FedUp,\" \"PreUpgrade\" und \"Anaconda\". Dieses DNF Plugin lädt in einem ersten Schritt zunächst alle Softwarepakete der nächsten Fedora-Version auf die Festplatte und aktualisiert danach das System zu Beginn des nächsten, unter der Kontrolle des Plugins initiierten Neustarts.\n\nEPEL \"(Extra Packages for Enterprise Linux)\" ist ein vom Fedora-Projekt gepflegtes Repository, das portierte Pakete von Software bereitstellt, die in Fedora selbst enthalten sind, nicht aber in RHEL, CentOS oder Scientific Linux. Weil diese Enterprise-Distributionen auf der Basis von Fedora entwickelt werden, sind meist nur sehr kleine Anpassungen an den Paketen notwendig. EPEL erweitert die Enterprise-Distributionen um dort nicht enthaltene Anwendungen und Treiber. Da die Portierung von Paketen von Fedora zum EPEL-Repository allein vom Einsatz der Community abhängt, geben weder Red Hat noch das Fedora-Projekt für solche Pakete eine Garantie, Support oder Zertifizierung, wie dies für Pakete im offiziellen RHEL-Repository üblich ist.\n\nDas \"fedora.us\"-Projekt wurde im November oder Dezember 2002 von Warren Togami gegründet. Ziel war es, ein qualitativ hochwertiges RPM-Repository für \"Red Hat Linux\" zu schaffen, das zusätzliche Anwendungen enthielt, die in RHL nicht enthalten waren. Die Grundidee war: Offizielle Richtlinien, aber offen für jedermann. Zuerst beschäftigte sich das Projekt vorwiegend mit Regeln für die Paketnamen. Es gab große Probleme mit der Koexistenz von originalen \"Red-Hat-Linux\"- und \"fedora.us\"-Paketen. Es entstanden unter anderem die heute noch gültigen \"Fedora Naming Guidelines\". Die Ressourcen des Projekts waren aber bis zur Gründung des Fedora-Projekts zusammen mit dem Unternehmen Red Hat eher bescheiden. Dafür bot \"fedora.us\" aber gut funktionierende Strukturen und Prozesse für eine Entwickler-Community.\n\nAm 22. September 2003 kündigte das Unternehmen Red Hat an, die eigene Consumer-Linux-Distribution zugunsten einer Verschmelzung mit dem \"fedora.us\"-Projekt aufzugeben und sie als Community-Projekt weiterzuführen. Red Hat brachte die Quellcodes von RHL und zahlreiche Arbeitskräfte in das neue gegründete Fedora-Projekt ein. \"fedora.us\" brachte eine funktionierende Online-Entwickler-Gemeinschaft mit in das Fedora-Projekt. Am 5. November 2003 veröffentlichte das Fedora-Projekt die erste stabile Version der neuen Distribution unter dem Namen \"Fedora Core\", die zu jenem Zeitpunkt eine direkte Weiterentwicklung des alten \"Red Hat Linux 9\" war.\n\nRechtlich heikle Softwareteile (Pakete), die aus den Arbeiten von \"fedora.us\" stammten, wurden aus den Fedora-Repositories entfernt. Die restlichen Teile der Distribution wurden in die Unterprojekte \"Fedora Core\" und \"Fedora Extra\" aufgeteilt. \"Fedora Core\" beinhaltete alle Pakete, die auch auf den downloadbaren Medien (ISO-Images) der ersten \"Fedora Core\"-Version enthalten waren. \"Fedora Extra\" beinhaltete Pakete die nicht in \"Fedora Core\" enthalten waren, aber die lizenzrechtlichen Richtlinien des Fedora-Projekts in gleichem Maße erfüllten wie die Pakete in \"Fedora Core\". Das dritte Unterprojekt war \"Fedora Legacy\". Es diente der Pflege von Fedora-Versionen, die mehr als zwei bis drei Monate zuvor von ihrer Nachfolger-Version abgelöst wurden. In \"Fedora Legacy\" wurden Updates nur durch die Eigeninitiative von einzelnen Entwicklern erstellt und getestet. Das Fedora-Projekt selbst stellte nur die Infrastruktur dafür bereit und lehnte jede Verantwortung für die Pakete selbst ab.\n\nIm Jahr 2005 gründete Red Hat mit der \"Fedora Foundation\" eine unabhängige Stiftung, die für das Fedora-Projekt zuständig sein sollte. Ziel des Stiftungsvorhabens war es, mehr Entwickler zur Mitarbeit am Fedora-Projekt zu gewinnen, das weiterhin die Basis für \"Red Hat Enterprise Linux\" bleiben sollte. Red Hat wollte die \"Fedora Foundation\" aus diesem Grund finanziell und technisch unterstützen. Da das Steuerrecht eine Unterstützung einer solchen Stiftung seitens Red Hat nur in gewissen Grenzen erlaubt hätte, gab Red Hat im April 2006 bekannt, dass die Stiftung wieder aufgelöst werden solle. Stattdessen solle die \"Fedora Community\" verstärkt im \"Fedora Board\" eingebunden werden.\n\nIm November 2006 wurden größere Umstrukturierungen im Fedora-Projekt beschlossen: Die verschiedenen Repositories von \"Fedora Extra\" und \"Fedora Core\" wurden zusammengelegt. \"Fedora Legacy\" wurde ersatzlos eingestellt und auf den Namenszusatz \"Core\" wird ab der Version 7 verzichtet. Das zusammengelegte Repository wurde zwischenzeitlich auch \"Fedora Package Universe\" genannt. Im Mai 2008 gab Red Hat die Stimmenmehrheit im \"Fedora Board\" an die \"Fedora Community\" ab, die seither mit 5 von 9 Stimmen die Mehrheit besitzt.\n\nAm 5. November 2013 feierte Fedora sein zehnjähriges Bestehen.\n\nIm August 2013 wurden unter dem Namen \"Fedora.next\" tiefgreifende Umstrukturierungen des Fedora-Projekts begonnen. Dabei wurde unter anderem beschlossen, künftig in fünf sogenannten „Working groups“ zu arbeiten:\nFedora 21 wird die erste Fedora-Version sein, die in der neuen Struktur entstanden ist. Sie wird je einen Spin speziell für Workstations, Server und Cloud Computing enthalten. Daneben gibt es jedoch auch weiterhin Spins z. B. für KDE oder Xfce. Um die Umstrukturierungen ohne Zeitdruck umzusetzen zu können wurde beschlossen, Fedora 21 nicht schon – wie üblich – ein halbes Jahr nach Fedora 20 zu veröffentlichen, sondern erst nach rund einem Jahr.\n\nSchon Red Hat Linux hatte für seine jeweiligen Versionen spezielle Codenamen. Diese Tradition wurde weiter fortgesetzt. Eine Liste dieser Namen und was es mit ihnen auf sich hat, findet sich unter Fedora- und Red-Hat-Versionsnamen. Mit Fedora 20 endete diese Namenserie.\nIn regelmäßigen Abständen entwickelt das Unternehmen Red Hat, mit meist nur geringfügigen Änderungen, aus einer Fedora-Version das Produkt Red Hat Enterprise Linux, (RHEL) dessen Versionen im Gegensatz zu Fedora sehr lange gepflegt werden:\n\nDie Entwicklungsarbeiten für Fedora finden an einer Distribution mit dem Namen \"Rawhide\" (engl: Rohleder) statt. In diese Distribution werden alle Neuerungen für die gerade in Entwicklung befindliche Fedora-Version eingearbeitet und von den Entwicklern getestet. Nähert sich der Entwicklungs-Zyklus dem ersten Alpha-Release, einer kommenden Fedora-Version, wird von der Rawhide ein \"Branched\" (engl: verzweigt) genannter Zweig abgeleitet, über die Alpha- und Beta-Releases-Phase stabilisiert und dann mit dem \"Final Release\" abgeschlossen. Das \"Branched\" trägt bereits die Versions-Nummer der kommenden Fedora-Version. Parallel zu \"Branched\" wird \"Rawhide\" für die übernächste Fedora-Version weiterentwickelt.\n\nDie \"Rawhide\" kennt – im Gegensatz zum \"Branched\" – keine Versionen, sondern nur tägliche Rolling Releases. Täglich erstellt ein Dienst auf einem Server des Fedora-Projekts, aus dem Quellcode der \"Rawhide\" und dem \"Branched\", ein installierbares ISO-Abbild. Da diese \"Nightly live builds\" genannten Releases den gerade vorhandenen Entwicklungsstand abbilden, ist mit groben Instabilitäten zu rechnen. Auch Datenverluste oder die Beschädigung von Hardware können nicht ausgeschlossen werden, weshalb die \"Nightly live builds\" für jegliche Art von Endnutzern ungeeignet sind. Nicht allzu selten kommt es auch vor, dass kein \"Nightly live build\" erstellt werden kann, weil zeitweilige Inkonsistenzen in der \"Rawhide\" dies unmöglich machen.\n\nFür Entwickler von nicht in der Distribution enthaltener Software und Personen, die sich informieren möchten, sind die \"Nightly live builds\" der \"Rawhide\" und \"Branched\" die einfachste Möglichkeit um vor einem Alpha-Release oder zwischen Alpha- und Beta-Releases die zukünftige Fedora zu testen und nutzen.\n\n\"Fedora Core 1\" basierte auf \"Red Hat Linux 9\" (RHL) und stammte damit von diesem ab. \"Red Hat Linux\" selbst ist vollständig in \"Fedora Core\" aufgegangen und wird nicht mehr selbständig weiterentwickelt. Stattdessen vertreibt das Unternehmen Red Hat das Produkt \"Red Hat Enterprise Linux\" (RHEL), das in seinen ersten Versionen auf \"Red Hat Linux\" und bei aktuellen Versionen auf Fedora aufbaut.\n\nEs gibt eine ganze Reihe von Linux-Distributionen, die auf Fedora aufbauen oder noch von der Red-Hat-Distribution abstammen.\n\n\nJede Fedora-Version wird passend zum Codename grafisch gestaltet:\n\n\n"}
{"id": "92512", "url": "https://de.wikipedia.org/wiki?curid=92512", "title": "GRASS GIS", "text": "GRASS GIS\n\nGRASS GIS ist eine hybride, modular aufgebaute Geoinformationssystem-Software mit raster- und vektororientierten Funktionen. \"GRASS\" steht für \"Geographic Resources Analysis Support System\", \"GIS\" für \"Geographical Information System\".\n\nEs steht unter der GNU General Public License und ist damit eine frei verfügbare Software. Das System bietet Raster- und topologische Vektordatenfunktionalität, 3D-Raster-Voxelbearbeitung, Bildverarbeitung, Visualisierungsmöglichkeiten und den Im- und Export verschiedener GIS-Datenformate. Als portable Software läuft es auf verschiedenen Betriebssystemen mit einer graphischen Benutzeroberfläche sowie optional per Kommandozeile.\n\nAb den 1970er Jahren begannen Behörden in den USA damit, geographische Daten digital zu speichern und zu bearbeiten. Ab Mitte der 80er Jahre wurden von verschiedenen Firmen geographische Informationssysteme mit immer nutzerspezifischeren Lösungen entwickelt. Ziel war es dabei, „\"...eine umfassende Sammlung von Werkzeugen für die Erfassung, Speicherung, Bereitstellung im Bedarfsfall, Transformation und Darstellung raumbezogener Daten der realen Welt im Rahmen spezieller Anwendungen\"“ bereitzustellen. Auch GRASS wurde in diesem Zusammenhang ab 1982 von einer Anzahl US-amerikanischer Regierungsbehörden entwickelt. Die Tatsache, dass dies durch Steuermittel finanziert wurde, führte aufgrund der amerikanischen Rechtsprechung zu der (kosten)freien Nutzbarkeit dieses Programms.\n\nAnfang der 1990er Jahre erfolgte die erste Veröffentlichung der GRASS-Version 4.0 im Internet. 1997 gründete sich das GRASS-Development-Team an der Baylor University, Waco (Texas), und gab im November des Jahres eine neue Version 4.2 heraus. Seit 1999 wird GRASS unter der GNU General Public License (GPL) veröffentlicht.\nGRASS richtete sich bisher eher an der Raster- und Bildverarbeitung aus und war vom Leistungsumfang her mit ArcInfo zu vergleichen, das sich bislang eher an der Vektorverarbeitung orientierte. GRASS hat sich nunmehr zu einem leistungsfähigen GIS entwickelt.\n\nDurch den modularen Aufbau (GRASS GIS ist eine unixtypische Sammlung vieler kleiner Programme) lassen sich einzelne Elemente einfach mit eigenen Applikationen unter einer Benutzeroberfläche verbinden.\n\nSchon ab Version 5.2 waren umfangreiche Bio-Statische Applikationen in GRASS integriert. Momentan liegt GRASS in der stabilen Version 7.0 mit erweiterter Funktionalität und neuem Vektormodell vor. Die Entwicklung wird von den USA aus von der Open Source Geospatial Foundation koordiniert, zuvor auch von Italien und Deutschland aus, während sich die Baylor University gänzlich aus der Entwicklung zurückgezogen hat.\n\nIm Jahr 2000 wurde die GRASS-Anwender-Vereinigung e. V. (GAV) gegründet, woraus im September 2008 der Verein FOSSGIS hervorgegangen ist. Der FOSSGIS vertritt die Open Source Geospatial Foundation (OSGeo) im deutschsprachigen Raum.\n\n\n\n"}
{"id": "93156", "url": "https://de.wikipedia.org/wiki?curid=93156", "title": "LinEx", "text": "LinEx\n\nLinEx bzw. GNU/LinEx ist eine Linux-Distribution, die auf Debian basiert. Die Entwicklung wurde von der Regierung der spanischen Region Extremadura initiiert, die im April 2002 beschloss, LinEx auf allen Computern in Schulen und öffentlicher Verwaltung zu verwenden. Das Projekt weist deutliche Parallelen zu Guadalinex der andalusischen Regierung auf.\n\nIm Jahr 2013 wurde Limex 2013, das auf Debian basiert, veröffentlicht.\n\nDie Regionalverwaltung von Extremadura investierte 300.000 Euro in LinEx und konnte dadurch bisher 30 Millionen Euro einsparen. Die Initiative brachte Ende 2003 Extremadura mit zwei Schülern pro Computer europaweit ins Spitzenfeld in diesem Bereich. Bis Mai 2003 wurden 200.000 CDs der Distribution kostenlos in Extremadura verteilt, weitere 70.000 wurden heruntergeladen. Auch die Nutzung in Wirtschaft und privatem Bereich wird aktiv beworben. Die Verwaltung Andalusiens beteiligt sich seit April 2003 am Projekt Guadalinex, nachdem auch dort beschlossen wurde, freie Software in der Verwaltung zu bevorzugen.\n\nAnfang 2012 wurde öffentlich, dass LinEx nicht mehr weiterentwickelt werden sollte. Bis dahin hatte es im Gesundheits- und Bildungsbereich eine große, in der Verwaltung jedoch nur 1 % Verbreitung erreicht. Ende Januar 2012 wurde der Plan bekannt gegeben, alle Verwaltungsarbeitsplätze Extremaduras bis Ende des Jahres auf Linux umzustellen.\n\nAnfang 2013 wurde die auf Debian basierende Version Linex 2013 veröffentlicht, die unter anderem spanischen Verwaltungsbedürfnissen angepasst wurde. Die Unterstützung von Menschen mit Behinderung sowie Anpassungen des Browsers Firefox waren Teile des Projekts. Die Arbeiten an Linex 2013 wurden von der spanischen Firma Emergya durchgeführt.\n\nLinEx ist konzipiert für i386-kompatible Computer, und beinhaltet die aktuelle, stabile Version von Gnome als Desktop-Umgebung, sowie einige Erweiterungen.\n\n\n"}
{"id": "94436", "url": "https://de.wikipedia.org/wiki?curid=94436", "title": "RTLinux", "text": "RTLinux\n\nRTLinux ist eine Erweiterung von Linux zu einem Echtzeitbetriebssystem, die ursprünglich von Victor Yodaiken zusammen mit seinem Studenten Michael Barabanov an der Universität von New Mexico entwickelt wurde. RTLinux wird in einer freien und wurde bis 2011 in einer kommerziellen Version vertrieben.\n\nUrsprünglich wurde RTLinux von der Firma FSMLabs vertrieben, bis 2007 die Rechte von dem Embedded-Linux-Spezialisten Wind River Systems übernommen wurden, der seit 2009 eine 100 %-Tochter von Intel ist.\n\nRTLinux/GPL, die freie Version, ist unter den Bedingungen der GNU GPL erhältlich. RTLinux hat im Bereich der Ansteuerung von Maschinen eine gewisse Bedeutung. Beispielsweise setzt das Werkzeugmaschinenlabor WZL der RWTH Aachen diese freie Betriebssystemvariante ein. Auf dem Massenmarkt für gewöhnliche Anwender ist es dagegen weitgehend unbekannt.\n\n\n"}
{"id": "95040", "url": "https://de.wikipedia.org/wiki?curid=95040", "title": "Grand Unified Bootloader", "text": "Grand Unified Bootloader\n\nGrand Unified Bootloader (kurz GRUB, für \"Großer vereinheitlichter Bootloader\") ist ein freies Bootloader-Programm, das oft zum Starten von unixoiden Betriebssystemen wie z. B. Linux eingesetzt wird.\n\nGRUB wurde innerhalb des GNU-Hurd-Projektes als Bootloader entwickelt und wird unter der GPL bereitgestellt. Aufgrund seiner höheren Flexibilität verdrängte GRUB in vielen Linux-Distributionen den traditionellen Bootloader Linux Loader (LILO). GRUB wird auch in Solaris 10 x86 benutzt.\nDie aktuelle Version, GRUB 2, welche erstmals im Juni 2012 veröffentlicht wurde, stellt eine komplette Überarbeitung der 0.9x-Reihe dar. Diese wird daher als GRUB Legacy bezeichnet (engl. ‚Altlast‘, ‚Erbe‘, ‚Hinterlassenschaft‘).\n\n\nNormalerweise wird der Bootloader von GRUB, die sogenannte Stage 1, in den Master Boot Record (MBR) geschrieben, welcher sich in den ersten 512 Bytes des primären Laufwerkes befindet. Aufgrund des durch die Partitionstabelle zusätzlich beschränkten Platzes kann die Stage 1 nur den ersten Sektor der sogenannten Stage 2 laden. In diesem Sektor befinden sich der Programmcode und eine Blockliste zum Lesen der restlichen Sektoren von Stage 2.\n\nDie Stage 2 kann sich auf einer beliebigen Partition befinden. Unter Unix-Systemen befindet sie sich meistens unter /boot/grub/stage2. Stage 2 enthält die Dateisystemtreiber, den Programmcode für das Auswahlmenü und die GRUB-Kommandozeile sowie die Laderoutine für die Kernel.\n\nNach dem Laden von Stage 2 wird, sofern vorhanden, die Konfigurationsdatei /boot/grub/menu.lst eingelesen und verarbeitet. In dieser Datei sind die Einträge des Auswahlmenüs definiert, welche nun in der Konsole angezeigt werden. Aus dem Menü können nun das zu bootende Betriebssystem ausgewählt oder Befehle über die Kommandozeile direkt an GRUB gesendet werden. Stage 2 stellt somit den eigentlichen Bootloader dar, welcher einen Kernel oder den Bootsektor einer Partition lädt.\n\nDiese zweistufige Aufteilung des Bootloaders hatte den Nachteil, dass der Bootloader nach Verschieben oder Änderungen von Stage 2 nicht mehr bootfähig war. Deswegen wurde zwischen Stage 1 und 2 eine Zwischenstufe, Stage 1.5, eingeführt. Diese liegt auf den Datenblöcken zwischen MBR bzw. Stage 1 und dem ersten Block der ersten Partition und ist in der Lage, genau ein Dateisystem zu lesen. Dabei wird die Variante installiert, welche das Dateisystem jener Partition unterstützt, auf welcher Stage 2 liegt. Zurzeit gibt es Stage 1.5 für die Dateisysteme FAT, Minix, ext2, ext3, JFS, ReiserFS, UFS2, JFS, XFS sowie Joliet. Unterstützung für Reiser4 und ext4 gibt es durch Patches von Drittanbietern.\n\nFür den Nachfolger GRUB 2 wurde ein vollständiges Redesign durchgeführt und auf Rückwärtskompatibilität zu GRUB Legacy verzichtet. Die Stage 2 wurde in einen Kernel (kernel.img) und viele ladbare Module (*.mod) aufgeteilt. Der Kernel enthält nur essentiellen Code mit Dekompression, ELF-Lader für Module, Festplattenzugriff und eine Rettungs-Shell. Bei der Installation werden die Module für das Dateisystem, das die restlichen Komponenten enthält, an den Kernel angehängt und als Datei core.img abgelegt. Hierbei kommt eines der Kompressionsverfahren LZMA oder LZO zum Einsatz, so dass die komprimierte Datei z. B. noch im Bootbereich hinter dem MBR abgelegt werden kann (Bei der Nutzung einer GPT erfolgt diese Ablage in eine eigens dafür vorgesehene BIOS Boot-Partition). Nach dem Laden wird der Code entpackt und die Konfigurationsdatei /boot/grub/grub.cfg geladen. Bei Bedarf werden Module für weitere Dateisysteme, Bootmenü, Bootroutinen für verschiedene Betriebssysteme und GRUB Shell vom Dateisystem nachgeladen. Neben der Shell-ähnlichen Skriptsprache bietet GRUB 2 auch Unterstützung für die Sprache Lua.\n\nDes Weiteren lässt sich GRUB 2 auch als Payload für die freie BIOS-Alternative coreboot verwenden. Dabei muss GRUB nicht wie üblich in den MBR geschrieben werden, sondern wird zusammen mit coreboot direkt in den Flash-Speicher-Baustein („BIOS Chip“) des Systems geschrieben. Beim Bootvorgang übergibt coreboot, nachdem es die Hardware initialisiert hat, die Kontrolle an GRUB, welches anschließend wie üblich ein Menü anzeigt und das Laden eines Kernels erlaubt.\n\nDie unterstützten Plattformen und Architekturen sind neben x86 und AMD64 (auch oft als x86-64, also x86 64-Bit, bezeichnet) nun auch Open-Firmware-basierte PowerPC-Rechner (Power Mac und Pegasos) und ab GRUB 2.02 auch ARM und ARM64 (64-Bit, ab ARMv8).. An der Unterstützung von UltraSparc wird gearbeitet.\n\nGRUB kann über das Dateisystem auf die als normale Dateien gespeicherten Betriebssystemkerne zugreifen. Andere Bootloader wie zum Beispiel LILO waren lange Zeit auf Konfigurationsdaten angewiesen, die angeben, in welchen Datenblöcken der Kernel liegt. Diese Angaben können sich nach einem Kernel-Update ändern, und die entsprechenden Konfigurationsdaten müssen neu geschrieben werden. Dieser Schritt ist bei GRUB dagegen nicht notwendig.\n\nDer Standard-GRUB stellt, wie oben beschrieben, einen eigenen Bootblock zur Verfügung. Das führt dazu, dass man GRUB normalerweise nicht von einem bestehenden Betriebssystem aus starten kann. Die GRUB-Shell ist unter Linux zugänglich, eine Alternative stellt das Projekt GRUB4DOS bereit, welches GRUB so erweitert, dass es als Programm unter DOS bzw. als GRLDR aus dem Windows-XP-/-NT-Bootmenü heraus startbar ist. Letzteres erspart das umständliche Extrahieren des Linux-Bootblocks mittels codice_1 in eine Datei. Jedoch ist Grub4dos nur für DOS und 32-bit Windows-Systeme, die dazu kompatibel sind, verfügbar. Auf 64-bit-Systemen können keine DOS-Programme ausgeführt werden.\n\nMit TrustedGRUB wird derzeit eine Erweiterung von GRUB entwickelt, die Trusted Platform Module (TPM) unterstützt.\n\n\n"}
{"id": "95283", "url": "https://de.wikipedia.org/wiki?curid=95283", "title": "Windows Live Messenger", "text": "Windows Live Messenger\n\nDer Windows Live Messenger war ein Instant-Messaging-Dienst von Microsoft und der Nachfolger des \"MSN Messengers\". Die Software war für verschiedene Betriebssysteme erhältlich, darunter Windows, Xbox 360, macOS, Android und iOS. Neben dem eigenen Netzwerk unterstützte er die dienstübergreifende Kommunikation mit Nutzern des Yahoo Messengers und dem Facebook-Chat. Veröffentlicht wurde der erste Client namens „MSN Messenger“ am 22. Juli 1999. Zum 30. April 2013 wurde der Dienst eingestellt und zu Skype ausgelagert.\n\nAm 6. November 2012 kündigte Microsoft an, den Windows Live Messenger zugunsten von Skype einzustellen. Nach eigenen Angaben wurde die Schließung des Dienstes seit Langem vorbereitet und soll für die Nutzer reibungslos vonstattengehen. So sollte es zum Beispiel möglich sein, sich mit den gewohnten Zugangsdaten bei Skype anzumelden – eine erneute Registrierung war also nicht erforderlich. Außerdem sollten alle Kontakte des Messengers automatisch an Skype übertragen werden. Der Dienst wurde am 30. April 2013 grundsätzlich eingestellt, im weiteren Verlauf desselben Jahres auch für „Mac-Betriebssysteme vor OSX“ (OS X 10.8) und Windows-Betriebssysteme vor Windows XP, auf welchen er weiterhin verfügbar gewesen ist, da kein Update zu Skype möglich war.\nAußerdem erfolgte in China die Einstellung des Dienstes aufgrund seiner hohen Popularität erst am 31. Oktober 2014.\n\nDer Dienst hatte weltweit über 300 Millionen Benutzer, davon sieben Millionen in Deutschland (Stand: 6. April 2008). Mit dem Messenger war es möglich, neben dem üblichen Chatten auch Webcam-Konferenzen abzuhalten, Onlinespiele zu spielen, Dateien auszutauschen oder über das Internet zu telefonieren. Der Messenger unterstützte ab Version 2011 nur die Betriebssysteme Windows Vista und Windows 7. Die letzte Version wurde im Design von Windows 7 gestaltet. Die letzte für Windows XP verfügbare Version war Version 2009.\n\nDer MSN Messenger 5.0 konnte von Benutzern, die die Betriebssysteme Windows 95 und Windows NT 4.0 einsetzen, noch benutzt werden. Die Versionen 6.0 und 6.1 konnten nur noch unter Windows 98 und Windows Me genutzt werden. Die Version 6.2 war seit Januar 2011 nicht mehr nutzbar. Der MSN Messenger 7.0 funktioniert ab Windows 98-Windows Server 2003 R2. Der MSN Messenger 7.5 war die letzte Version vor der Namensänderung: Sie konnte nur noch durch eine Änderung der Kompatibilitätseinstellungen auf Windows 2000 ausgeführt werden.\n\nBei Windows XP wurde die Vorversion des Messengers, der \"Windows Messenger\", bereits mit diesem ausgeliefert. Seit Windows Vista war der Messenger nicht mehr in das Betriebssystem integriert. Stattdessen wurde er Teil der Windows Essentials, in die auch weitere vormals integrierte Funktionen des Windows-Betriebssystems ausgelagert wurden.\n\n\"Windows Live Messenger Lite\" bzw. \"Light\" war eine inoffizielle portable Version von Windows Live Messenger. Trotz des Namenszusatzes „Lite“ war die Version etwa 16 Megabyte groß. Die Lite-Version musste nicht installiert werden und speichert keine Benutzerdaten auf dem verwendeten Computer. Stattdessen wird im Ordner des Anwendungsprogramms ein Sandbox-Ordner erstellt, in dem Messengerdaten gespeichert werden. Weiterhin funktioniert sie ohne Benutzerrechte. Die portable Version war nur auf Englisch verfügbar und nicht mit Messenger Plus 2010! Live kompatibel.\n\nAuf der CEBIT 2009 stellte Microsoft den „Windows Live Messenger für KIDS“ vor. Dieser war speziell an die Bedürfnisse von 8- bis 12-jährigen Kindern angepasst.\n\nBesonderheit gegenüber dem Live-Messenger waren, dass kein Versand von Dateien und Fotos möglich war und Eltern entscheiden konnten, ob ein Kontakt mit dem Kind aufgenommen werden konnte.\n\n\"Messenger für Mac\" bzw. Microsoft Messenger für Mac war eine offizielle und eigenständige Version von dem Messenger für das Apple Betriebssystem macOS. Bis zur Version 7 waren Gesprächs- sowie Videochat nicht möglich. Seit März 2010 gab es eine öffentliche Betaversion, welche Gesprächs- und Videochat unterstützt.\n\nMit dem MSN Mobile Dienst war es auch möglich, den Messenger auf Smartphones, dem Ogo und eingeschränkt über SMS-Textnachrichten zu benutzen. Entwickler wie Agile Mobile bieten mittlerweile Instant-Messaging-Programme für Handys an.\n\nDer Messaging-Dienst konnte zusätzlich zum offiziellen Client auch über den von Microsoft zur Verfügung gestellten \"Webmessenger\" sowie die Anbieter eBuddy und Meebo über jeden modernen Webbrowser verwendet werden.\n\nFür viele Betriebssysteme sind auch Clients anderer Hersteller erhältlich, die allerdings von Microsoft nicht unterstützt, in der Vergangenheit wurde das Protokoll oft in einer Form verändert, die eine Anpassung dieser Clients erforderlich machte. Beispielsweise Adium ist ein für macOS erhältliches All-in-one-Chatprogramm, das auch Messenger unterstützt.\n\nMit der Nutzung von Microsoft-Diensten erlaubte man laut den „Terms of Use“ den Mitschnitt von Nachrichten und E-Mails. Auch die allgemeinen Geschäftsbedingungen (AGB) von ICQ und Skype enthalten ähnliche Klauseln.\n\nAlternativ gab es das kostenlose Closed-Source-Verschlüsselungstool SimpLite für alle Versionen des MSN Messengers, das die Gespräche mittels AES-128-Bit-Verschlüsselung überträgt. Eine weitere Möglichkeit zur Verschlüsselung der Chats stellt Off-the-Record Messaging (OTR) dar, dessen Konzept sich besonders gut für die Absicherung dieser Kommunikationsform eignet. Die Software fordert außerdem bei einer älteren ein Update auf die aktuelle Version, andernfalls blockiert sie die Nutzung des Messengers.\n\n"}
{"id": "95884", "url": "https://de.wikipedia.org/wiki?curid=95884", "title": "Spracherkennung", "text": "Spracherkennung\n\nDie Spracherkennung oder auch automatische Spracherkennung ist ein Teilgebiet der angewandten Informatik, der Ingenieurwissenschaften und der Computerlinguistik. Sie beschäftigt sich mit der Untersuchung und Entwicklung von Verfahren, die Automaten, insbesondere Computern, die gesprochene Sprache der automatischen Datenerfassung zugänglich macht. Die Spracherkennung ist zu unterscheiden von der Stimm- bzw. Sprechererkennung, einem biometrischen Verfahren zur Personenidentifikation. Allerdings ähneln sich die Realisierungen dieser Verfahren.\n\nDie Forschung an Spracherkennungssystemen begann in den 1960er Jahren, verlief damals allerdings weitestgehend erfolglos: Die von privaten Firmen entwickelten Systeme ermöglichten unter Laborbedingungen die Erkennung von einigen Dutzend Einzelwörtern. Dies lag einerseits an dem begrenzten Wissen in diesem neuen Forschungsgebiet, aber auch an den zur damaligen Zeit begrenzten technischen Möglichkeiten.\n\nErst Mitte der 1980er Jahre kam die Entwicklung weiter voran. In dieser Zeit entdeckte man, dass man durch Kontextprüfungen Homophone unterscheiden konnte. Indem man Statistiken über die Häufigkeit bestimmter Wortkombinationen erstellte und auswertete, konnte man bei ähnlich oder gleich klingenden Wörtern entscheiden, welches gemeint war. Diese sogenannten Trigrammstatistiken wurden anschließend ein wichtiger Bestandteil aller Spracherkennungssysteme. 1984 stellte IBM ein erstes Spracherkennungssystem vor, das etwa 5.000 englische Einzelwörter erkennen konnte. Das System brauchte für einen Erkennungsvorgang jedoch mehrere Minuten Rechenzeit auf einem Großrechner. Fortschrittlicher war dagegen ein von Dragon Systems entwickeltes System: Dieses ließ sich auf einem tragbaren PC verwenden.\n\nZwischen 1988 und 1993 demonstrierte das europäische Projekt SUNDIAL auch die Spracherkennung der Zugfahrpläne in deutscher Sprache. SUNDIAL studierte auch Bewertungskennzahlen der Spracherkennungen.\n\n1991 stellte IBM erstmals auf der CeBIT ein Spracherkennungssystem vor, das 20.000 bis 30.000 deutsche Wörter erkennen konnte. Die Präsentation des TANGORA 4 genannten Systems musste jedoch in einem speziell abgeschirmten Raum stattfinden, da der Lärm der Messe das System sonst gestört hätte.\n\nEnde 1993 stellte IBM das erste für den Massenmarkt entwickelte Spracherkennungssystem vor: Das \"IBM Personal Dictation System\" genannte System lief auf normalen PCs und kostete unter 1000 Dollar. Als es unter dem Namen \"IBM VoiceType Diktiersystem\" auf der CeBIT 1994 präsentiert wurde, stieß es auf hohes Interesse seitens der Besucher und der Fachpresse.\n\n1997 erschienen für den PC-Endbenutzer sowohl die Software IBM ViaVoice (Nachfolger von IBM VoiceType) als auch die Version 1.0 der Software \"Dragon NaturallySpeaking\". 1998 brachte Philips Speech Recognition Systems mit FreeSpeech 98 eine Spracherkennung für PC-Endbenutzer auf den Markt, dessen Steuerung auf das hauseigene digitale Diktiergerät SpeechMike angepasst war, stellte die Produktlinie aber nach der zweiten Version FreeSpeech 2000 wieder ein. 2004 gab IBM Teile seiner Spracherkennungsanwendungen als Open Source frei und sorgte damit für Aufsehen. Branchenkenner vermuteten als Grund taktische Maßnahmen gegen die Firma Microsoft, die ebenfalls in diesem Bereich tätig ist und seit 2007 mit Erscheinen von ihrem PC-Betriebssystem Windows Vista als integralen Bestandteil erstmals Spracherkennungsfunktionen für die Steuerung wie auch für das Diktat anbot, die bis heute in Windows 8.1 weiterentwickelt wurden.\n\nWährend die Entwicklung von IBM ViaVoice eingestellt wurde, entwickelte sich \"Dragon NaturallySpeaking\" zur gegenwärtig meistverbreiteten sprecherabhängigen Drittanbieter-Spracherkennungssoftware für Windows-PCs und wird von Nuance Communications seit 2005 hergestellt und vertrieben.\n\nNuance hat 2008 mit dem Erwerb der Philips Speech Recognition Systems, Wien, auch die Rechte an dem Software Development Kit (SDK) SpeechMagic erlangt, welches insbesondere im Gesundheitsbereich Verbreitung gefunden hat. Für iMac-Personal Computer von Apple wurde von dem Unternehmen MacSpeech seit 2006 eine Drittanbieter-Spracherkennungssoftware unter dem Namen iListen vertrieben, die auf Philips-Komponenten basierte. 2008 wurde diese durch MacSpeech Dictate unter Verwendung der Kernkomponenten von Dragon NaturallySpeaking abgelöst und nach dem Erwerb von MacSpeech durch Nuance Communications 2010 in Dragon Dictate (Version 2.0 – seit 2012 wird die Version 3.0 vertrieben) umbenannt.\n\n2007 wurde die Firma Siri Inc. gegründet und im April 2010 von Apple gekauft. Im Oktober 2011 stellte Apple die Spracherkennungssoftware Siri für das iPhone 4s vor, die der Erkennung und Verarbeitung von natürlich gesprochener Sprache (unter Nutzung von Apple Servern) dient und so Funktionen eines persönlichen Assistenten erfüllen soll.\n\nDerzeit kann grob zwischen zwei Arten der Spracherkennung unterschieden werden:\n\nCharakteristisch für die „sprecherunabhängige“ Spracherkennung ist die Eigenschaft, dass der Benutzer ohne eine vorhergehende Trainingsphase sofort mit der Spracherkennung beginnen kann. Der Wortschatz ist jedoch auf einige tausend Wörter begrenzt.\n\n„Sprecherabhängige“ Spracherkenner werden vom Benutzer vor der Verwendung (in neueren Systemen: während der Verwendung) auf die eigenen Besonderheiten der Aussprache trainiert. Ein zentrales Element ist die individuelle Interaktionsmöglichkeit mit dem System, um ein optimales sprecherabhängiges Ergebnis zu erzielen (eigene Begrifflichkeiten, Abkürzungen, Kürzel usw.). Ein Einsatz in Anwendungen mit häufig wechselnden Benutzern (z. B. Call Center) ist damit nicht sinnvoll. Der Wortschatz ist im Vergleich sehr viel größer als der der sprecherunabhängigen Erkenner. So enthalten aktuelle Systeme mehr als 300.000 Wortformen. Zu unterscheiden ist ferner zwischen:\n\nIn \"Front-End-Systemen\" erfolgt die Verarbeitung der Sprache und Umsetzung in Text unmittelbar, so dass er das Ergebnis praktisch ohne nennenswerte Zeitverzögerung ablesen kann. Die Umsetzung kann auf dem Computer des Benutzers oder Cloud-basiert erfolgen. Durch die unmittelbare Interaktion zwischen Benutzer und System wird hier die höchste Erkennungsqualität erzielt. Ebenso sind Steuerungen des Systems über Kommandos und Einbindung weiterer Komponenten wie Echtzeit-Assistenzsysteme möglich. In \"Back-End-Systemen\" wird die Umsetzung hingegen zeitversetzt durchgeführt. Dies geschieht meist auf einem entfernten Server. Der Text steht erst mit Verzögerung zur Verfügung. Solche Systeme sind im medizinischen Bereich noch verbreitet. Da keine unmittelbare Interaktion zwischen dem Sprecher und dem Erkennungsergebnis erfolgt, ist eine herausragende Qualität nur dann zu erwarten, wenn der Nutzer bereits Erfahrung mit Spracherkennung hat.\n\n„Sprecherunabhängige“ Spracherkennung wird bevorzugt im technischen Einsatz verwendet, zum Beispiel in automatischen Dialogsystemen wie etwa einer Fahrplanauskunft. Überall dort, wo nur ein begrenzter Wortschatz verwendet wird, wird die sprecherunabhängige Spracherkennung mit Erfolg praktiziert. So erreichen Systeme zur Erkennung der gesprochenen englischen Ziffern von 0 bis 9 eine nahezu 100-%-Erkennungsquote.\n\nIm Einsatz von „sprecherabhängiger“ Spracherkennung können sehr hohe Erkennungsquoten erreicht werden. Allerdings kann selbst eine Treffsicherheit von 95 Prozent als zu gering empfunden werden, da zu viel nachgebessert werden muss. Entscheidend für den Erfolg „sprecherabhängiger“ Spracherkennung ist die Interaktion zwischen Nutzer und System, die dem Nutzer ermöglicht, direkt oder indirekt Einfluss auf das persönliche Erkennungsergebnis zu nehmen.\n\nZwischenzeitlich erreichen aktuelle Systeme beim Diktat von Fließtexten auf Personal Computern Erkennungsquoten von ca. 99 Prozent und erfüllen damit für viele Einsatzgebiete die Anforderungen der Praxis, z. B. für wissenschaftliche Texte, Geschäftskorrespondenz oder juristische Schriftsätze. An Grenzen stößt der Einsatz dort, wo der jeweilige Autor ständig neue, von der Software zunächst nicht erkennbare Wörter und Wortformen benötigt, deren manuelle Hinzufügung zwar möglich, aber bei nur einmaligem Vorkommen in Texten desselben Sprechers nicht effizient ist. Daher profitieren z. B. Dichter weniger vom Einsatz der Spracherkennung als z. B. Ärzte und Rechtsanwälte.\n\nNeben der Größe und Flexibilität des Wörterbuches spielt auch die Qualität der akustischen Aufnahme eine entscheidende Rolle. Bei Mikrofonen, die direkt vor dem Mund angebracht sind (zum Beispiel bei Headsets oder Telefonen) wird eine signifikant höhere Erkennungsgenauigkeit erreicht als bei weiter entfernten Raummikrofonen.\n\nWesentlichste Einflussfaktoren in der Praxis sind allerdings eine präzise Aussprache und das zusammenhängende flüssig gesprochene Diktat, so dass Wortzusammenhänge und Wortfolgewahrscheinlichkeiten optimal in den Erkennungsprozess einfließen können.\n\nDie Entwicklung bei der Spracherkennung schreitet sehr schnell voran. Heute (Stand 2016) werden Spracherkennungssysteme u. a. in Smartphones eingesetzt z. B. bei Siri, Google Now, Cortana und Samsungs S Voice. Aktuelle Spracherkennungssysteme müssen nicht mehr trainiert werden. Entscheidend für eine hohe Treffsicherheit außerhalb der Alltagssprache ist dabei die Plastizität des Systems. Um hohen Ansprüchen gerecht werden zu können, bieten professionelle Systeme dem Anwender die Möglichkeit, durch Vorschreiben oder Vorsprechen das persönliche Ergebnis zu beeinflussen.\n\nUm die Erkennungsgenauigkeit noch weiter zu erhöhen, wird teils auch versucht, mithilfe einer Videokamera das Gesicht des Sprechers zu filmen und daraus die Lippenbewegungen abzulesen. Indem man diese Ergebnisse mit den Ergebnissen der akustischen Erkennung kombiniert, kann man gerade bei verrauschten Aufnahmen eine signifikant höhere Erkennungsquote erreichen.\n\nDies entspricht Beobachtungen bei der menschlichen Spracherkennung: Harry McGurk hatte 1976 festgestellt, dass auch Menschen aus der Lippenbewegung auf die gesprochene Sprache schließen (McGurk-Effekt).\n\nDa es sich bei Kommunikation mit menschlicher Sprache meist um einen Dialog zwischen zwei Gesprächspartnern handelt, findet man die Spracherkennung häufig in Verbindung mit Sprachsynthese. Auf diesem Weg können dem Benutzer des Systems akustische Rückmeldungen über den Erfolg der Spracherkennung und Hinweise über eventuell ausgeführte Aktionen gegeben werden. Auf die gleiche Weise kann der Benutzer auch zu einer erneuten Spracheingabe aufgefordert werden.\n\nUm zu verstehen, wie ein Spracherkennungssystem arbeitet, muss man sich zuerst über die Herausforderungen klar werden, die zu bewältigen sind.\n\nBei einem Satz in der Alltagssprache werden die einzelnen Wörter ohne wahrnehmbare Pause dazwischen ausgesprochen. Als Mensch kann man sich intuitiv an den Übergängen zwischen den Wörtern orientieren – frühere Spracherkennungssysteme waren dazu nicht in der Lage. Sie erforderten eine diskrete (unterbrochene) Sprache, bei der zwischen den Wörtern künstliche Pausen gemacht werden müssen.\n\nModerne Systeme sind jedoch auch fähig, kontinuierliche (fließende) Sprache zu verstehen.\n\nBei der diskreten Sprache erkennt man deutlich die Pausen zwischen den Wörtern, die länger und deutlicher ausfallen als die Übergänge zwischen den Silben innerhalb des Worts \"Enzyklopädie\".\n\nBei der kontinuierlichen Sprache gehen die einzelnen Wörter ineinander über, es sind keine Pausen erkennbar.\n\nDurch die Flexion, also die Beugung eines Wortes je nach grammatikalischer Funktion, entstehen aus Wortstämmen (Lexemen) eine Vielzahl von Wortformen. Dies ist für die Größe des Wortschatzes von Bedeutung, da alle Wortformen bei der Spracherkennung als eigenständige Wörter betrachtet werden müssen.\n\nDie Größe des Wörterbuchs hängt stark von der Sprache ab. Zum einen haben durchschnittliche deutschsprachige Sprecher mit circa 4000 Wörtern einen deutlich größeren Wortschatz als englischsprachige mit rund 800 Wörtern. Außerdem ergeben sich durch die Flexion in der deutschen Sprache in etwa zehnmal so viele Wortformen, wie in der englischen Sprache, wo nur viermal so viele Wortformen entstehen. \n\nIn vielen Sprachen gibt es Wörter oder Wortformen, die eine unterschiedliche Bedeutung haben, jedoch gleich ausgesprochen werden. So klingen die Wörter „Meer“ und „mehr“ zwar identisch, haben jedoch trotzdem nichts miteinander zu tun. Solche Wörter nennt man Homophone. Da ein Spracherkennungssystem im Gegensatz zum Menschen in der Regel kein Weltwissen hat, kann es die verschiedenen Möglichkeiten nicht anhand der Bedeutung unterscheiden.\n\nDie Frage nach der Groß- oder Kleinschreibung fällt auch in diesen Bereich.\n\nAuf akustischer Ebene spielt insbesondere die Lage der Formanten eine Rolle: Die Frequenzanteile gesprochener Vokale konzentrieren sich typischerweise auf bestimmte unterschiedliche Frequenzen, die Formanten genannt werden. Für die Unterscheidung der Vokale sind insbesondere die zwei tiefsten Formanten von Bedeutung: Die tiefere Frequenz liegt im Bereich von 200 bis 800 Hertz, die höhere im Bereich von 800 bis 2400 Hertz. Über die Lage dieser Frequenzen lassen sich die einzelnen Vokale unterscheiden.\n\nKonsonanten sind vergleichsweise schwierig zu erkennen; einzelne Konsonanten (sogenannte Plosive) sind zum Beispiel nur durch den Übergang zu den benachbarten Lauten feststellbar, wie folgendes Beispiel zeigt:\n\nMan erkennt, dass innerhalb des Wortes \"sprechen\" der Konsonant \"p\" (genauer: die Verschlussphase des Phonems \"p\") faktisch nur Stille ist und lediglich durch die Übergänge zu den anderen Vokalen erkannt wird – das Entfernen bewirkt also keinen hörbaren Unterschied.\n\nAndere Konsonanten sind durchaus an charakteristischen spektralen Mustern erkennbar. So zeichnen sich etwa der Laut \"s\" wie auch der Laut \"f\" (Reibelaute) durch einen hohen Energieanteil in höheren Frequenzbändern aus. Bemerkenswert ist, dass die für die Entscheidung dieser beiden Laute relevanten Informationen größtenteils außerhalb des in Telefonnetzen übertragenen Spektralbereichs (bis zirka 3,4 kHz) liegt. Dadurch ist es zu erklären, dass das Buchstabieren über Telefon ohne Verwendung eines speziellen Buchstabieralphabets auch in der Kommunikation zwischen zwei Menschen ausgesprochen mühselig und fehleranfällig ist.\n\nAuch wenn ein Spracherkennungsprogramm bereits gut auf eine Hochsprache eingestellt ist, bedeutet dies jedoch nicht, dass es jede Ausformung dieser Sprache verstehen kann. Besonders im Fall von Dialekten und Soziolekten stoßen solche Programme häufig an ihre Grenzen. Menschen sind meist in der Lage, sich schnell auf die möglicherweise unbekannte Mundart ihres Gegenübers einzustellen – Erkennungssoftware ist dazu nicht ohne weiteres in der Lage. Dialekte müssen dem Programm hierfür erst in aufwendigen Prozessen beigebracht werden.\n\nZudem muss auch beachtet werden, dass sich gelegentlich und regional abhängig Wortbedeutungen verändern können. So meinen Bayern und Berliner beispielsweise unterschiedliche Süßspeisen, wenn von „Pfannkuchen“ die Rede ist. Ein Mensch kann durch sein kulturelles Hintergrundwissen derartige Missverständnisse leichter vermeiden und aufklären als es eine Software aktuell vermag.\n\nSollte es zu Verständnisproblemen in einer Kommunikation kommen, tendieren Menschen naturgemäß dazu besonders laut zu sprechen oder missverstandene Begriffe ausführlicher zu umschreiben. Dies kann sich jedoch einem Computer gegenüber kontraproduktiv auswirken, da dieser auf normale Gesprächslautstärke trainiert ist und außerdem eher mit Schlüsselwörtern arbeitet als Sinnzusammenhänge zu erfassen.\n\nEin Spracherkennungssystem besteht aus folgenden Bestandteilen: Einer Vorverarbeitung, die die analogen Sprachsignale in die einzelnen Frequenzen zerlegt. Anschließend findet die tatsächliche Erkennung mit Hilfe akustischer Modelle, Wörterbücher und Sprachmodellen statt.\n\nDie Vorverarbeitung besteht im Wesentlichen aus den Schritten Abtastung, Filterung, Transformation des Signals in den Frequenzbereich und Erstellen des Merkmalsvektors.\n\nBei der Abtastung wird das analoge (kontinuierliche) Signal digitalisiert, also in eine elektronisch verarbeitbare Bitfolge zerlegt, um es einfacher weiterverarbeiten zu können.\n\nDie wichtigste Aufgabe des Arbeitsschrittes Filterung ist die Unterscheidung von Umgebungsgeräuschen wie Rauschen oder z. B. Motorengeräuschen und Sprache. Dazu wird zum Beispiel die Energie des Signals oder die Nulldurchgangsrate herangezogen.\n\nFür die Spracherkennung ist nicht das Zeitsignal, sondern das Signal im Frequenzbereich relevant. Dazu wird es mittels FFT transformiert. Aus dem Resultat, dem Frequenzspektrum, lassen sich die im Signal vorhandenen Frequenzanteile ablesen.\n\nZur eigentlichen Spracherkennung wird ein Merkmalsvektor erstellt. Dieser besteht aus voneinander abhängigen oder unabhängigen Merkmalen, die aus dem digitalen Sprachsignal erzeugt werden. Dazu gehört neben dem schon erwähnten Spektrum vor allem das Cepstrum. Merkmalsvektoren lassen sich z. B. mittels einer zuvor zu definierenden \"Metrik\" vergleichen.\n\nDas Cepstrum wird aus dem Spektrum gewonnen, indem die FFT des logarithmierten Betrags-Spektrum gebildet wird. So lassen sich Periodizitäten im Spektrum erkennen. Diese werden im menschlichen Vokaltrakt und durch die Stimmbandanregung erzeugt. Die Periodizitäten durch die Stimmbandanregung überwiegen und sind daher im oberen Teil des Cepstrums zu finden, wohingegen der untere Teil die Stellung des Vokaltraktes abbildet. Dieser ist für die Spracherkennung relevant, daher fließen nur diese unteren Anteile des Cepstrums in den Merkmalsvektor ein. Da sich die Raumübertragungsfunktion – also die Veränderung des Signals z. B. durch Reflexionen an Wänden – zeitlich nicht verändert, lässt diese sich durch den Mittelwert des Cepstrums darstellen. Dieser wird deshalb häufig vom Cepstrum subtrahiert, um Echos zu kompensieren. Ebenso ist zur Kompensation der Raumübertragungsfunktion die erste Ableitung des Cepstrum heranzuziehen, die ebenfalls in den Merkmalsvektor einfließen kann.\n\nIm weiteren Verlauf spielen Hidden-Markov-Modelle (HMM) eine wichtige Rolle.\nDiese ermöglichen es, die Phoneme zu finden, die am besten zu den Eingangssignalen passen. Dazu wird das akustische Modell eines Phonems in verschiedene Teile zerlegt: Den Anfang, je nach Länge unterschiedlich viele Mittelstücke und das Ende. Die Eingangssignale werden mit diesen gespeicherten Teilstücken verglichen und mit Hilfe des Viterbi-Algorithmus mögliche Kombinationen gesucht.\n\nFür die Erkennung von unterbrochener (diskreter) Sprache (bei der nach jedem Wort eine Pause gemacht wird) reichte es aus, jeweils ein Wort zusammen mit einem Pausenmodell innerhalb des HMMs zu berechnen. Da die Rechenkapazität moderner PCs aber deutlich gestiegen ist, kann mittlerweile auch fließende (kontinuierliche) Sprache erkannt werden, indem größere Hidden Markov Modelle gebildet werden, die aus mehreren Wörtern und den Übergängen zwischen ihnen bestehen.\n\nAlternativ wurden auch schon Versuche unternommen, neuronale Netze für das akustische Modell zu verwenden. Mit Time Delay Neural Networks sollten dabei insbesondere die Veränderungen im Frequenzspektrum über den Zeitablauf hinweg zur Erkennung verwendet werden. Die Entwicklung hatte zunächst durchaus positive Ergebnisse gebracht, wurde dann aber zugunsten der HMMs wieder aufgegeben. Erst in den letzten Jahren wurde dieses Konzept im Rahmen von Deep Neural Networks wiederentdeckt. Spracherkennungssysteme, die auf Deep Learning aufsetzen, liefern Erkennungsraten im menschlichen Bereich.\n\nEs gibt aber auch einen hybriden Ansatz, bei dem die aus der Vorverarbeitung gewonnenen Daten durch ein neuronales Netzwerk vor-klassifiziert werden, und die Ausgabe des Netzes als Parameter für die Hidden Markov Modelle genutzt wird. Dies hat den Vorteil, dass man ohne die Komplexität der HMMs zu erhöhen auch Daten von kurz vor und kurz nach dem gerade bearbeiteten Zeitraum nutzen kann. Außerdem kann man so die Klassifizierung der Daten und die kontextsensitive Zusammensetzung (Bildung von sinnvollen Wörtern/Sätzen) voneinander trennen.\n\nDas Sprachmodell versucht anschließend, die Wahrscheinlichkeit bestimmter Wortkombinationen zu bestimmen und dadurch falsche oder unwahrscheinliche Hypothesen auszuschließen. Dazu kann entweder ein Grammatikmodell unter Verwendung Formaler Grammatiken oder ein statistisches Modell mit Hilfe von N-Grammen eingesetzt werden.\n\nEine Bi- oder Trigrammstatistik speichert die Auftrittswahrscheinlichkeit von Wortkombinationen aus zwei oder mehr Wörtern. Diese Statistiken werden aus großen Textkorpora (Beispieltexten) gewonnen. Jede von der Spracherkennung ermittelte Hypothese wird anschließend geprüft und ggf. verworfen, falls ihre Wahrscheinlichkeit zu gering ist. Dadurch können auch Homophone, also unterschiedliche Wörter mit identischer Aussprache unterschieden werden. „Vielen Dank“ wäre also wahrscheinlicher als „Fielen Dank“, obwohl beides gleich ausgesprochen wird.\n\nMit Trigrammen sind im Vergleich zu Bigrammen theoretisch zutreffendere Schätzungen der Auftrittswahrscheinlichkeiten der Wortkombinationen möglich. Allerdings müssen die Beispieltext-Datenbanken, aus denen die Trigramme extrahiert werden, wesentlich größer sein als für Bigramme, denn es müssen sämtliche zulässigen Wortkombinationen aus drei Wörtern in statistisch signifikanter Anzahl darin vorkommen (d. h.: jede wesentlich mehr als einmal). Kombinationen von vier oder mehr Wörtern wurden lange nicht verwendet, weil sich im Allgemeinen keine Beispieltext-Datenbanken mehr finden lassen, die sämtliche Wortkombinationen in genügender Anzahl beinhalten. Eine Ausnahme bildet hier Dragon, welches ab der Version 12 auch Pentagramme verwendet – was die Erkennungsgenauigkeit in diesem System steigert.\n\nWenn Grammatiken verwendet werden, handelt es sich meist um kontextfreie Grammatiken. Dabei muss allerdings jedem Wort seine Funktion innerhalb der Grammatik zugewiesen werden. Deshalb werden solche Systeme meist nur für einen begrenzten Wortschatz und Spezialanwendungen verwendet, nicht aber in der gängigen Spracherkennungssoftware für PCs.\n\nDie Güte eines Spracherkennungssystems lässt sich mit verschiedenen Zahlen angeben. Neben Erkennungsgeschwindigkeit – meist als Echtzeitfaktor (EZF) angegeben – lässt sich die Erkennungsgüte als Wortakkuratheit oder Worterkennungsrate messen.\n\nFür die Integration von professionellen Spracherkennungssystemen gibt es bereits vordefinierte Vokabulare, die die Arbeit mit der Spracherkennung erleichtern sollen. Diese Vokabulare werden etwa im Umfeld von SpeechMagic \"ConText\" und im Bereich von Dragon \"Datapack\" genannt. Je besser das Vokabular auf den vom Sprecher verwendeten Wortschatz und Diktierstil (Häufigkeit der Wortfolgen) angepasst ist, desto höher ist die Erkennungsgenauigkeit. Ein Vokabular beinhaltet neben dem sprecherunabhängigen Lexikon (Fach- und Grundwortschatz) auch ein individuelles Wortfolgemodell (Sprachmodell). Im Vokabular sind alle der Software bekannten Wörter in der Phonetik und Orthografie hinterlegt. Auf diese Weise wird ein gesprochenes Wort an seinem Klang durch das System erkannt. Wenn sich Wörter in Bedeutung und Schreibweise unterscheiden, aber gleich klingen, greift die Software auf das Wortfolgemodell zurück. In ihm ist die Wahrscheinlichkeit definiert, mit der bei einem bestimmten Benutzer ein Wort auf ein anderes folgt. Spracherkennung in Smartphones verwendet die gleichen technischen Konzepte, jedoch ohne dass der Nutzer Einfluss auf das vordefinierte Vokabular hat. Neuere Technologien lösen sich von der Vorstellung einer starren hinterlegten Wortliste, da Komposita gebildet werden können. Allen Systemen ist gemein, dass sie nur durch Korrekturen des jeweiligen Benutzers individuelle Wörter und Wortfolgen lernen.\n\nDie Spracherkennung wird heutzutage u. a. in Smartphones eingesetzt z. B. bei Siri, Google Now, Cortana, Amazons Echo / Alexa und Samsungs S Voice. Mit der nun hohen Zuverlässigkeit in der Alltagssprache (z. B. Smartphones) oder in der Fachsprache (individualisierbare professionelle Systeme) können Sprache in Text gewandelt \"(speech to text)\", Befehle und Steuerungen ausgeführt \"(command and control)\" oder semantische Analysen durchgeführt werden \"(language understanding)\".\n\n\n\n"}
{"id": "96950", "url": "https://de.wikipedia.org/wiki?curid=96950", "title": "Microsoft FrontPage", "text": "Microsoft FrontPage\n\nMicrosoft FrontPage ist ein HTML-Editor von Microsoft für das Betriebssystem Microsoft Windows, der nach dem WYSIWYG-Prinzip arbeitet. Das Programm gehörte zur Familie der Office-Produkte von Microsoft und war in einigen Varianten des Office-Programmpakets enthalten. Entsprechend war es in die Officesuite integriert und unterstützt die Microsoft Windows SharePoint Services. Im Herbst 2007 trat Microsoft Expression Web die Nachfolge von Microsoft FrontPage an.\n\nMicrosoft FrontPage wird als Client-Applikation unter Windows genutzt. In früheren Versionen wurde auch Apples Mac OS Classic unterstützt. Diese Produktlinie wurde jedoch seit 1998 nicht mehr aktualisiert. Der Editor kann auf Websites, im FrontPage-Jargon als \"Webs\" bezeichnet, sowohl im lokalen Dateisystem bzw. freigegebenen Netzwerklaufwerken, als auch über HTTP mittels Benutzung eigenentwickelter serverseitiger „FrontPage Extensions“ zugreifen. Besondere Funktionen bietet Frontpage in den aktuellen Versionen bei der Bearbeitung von Windows SharePoint Services-Webs und ermöglicht so umfangreiche Erweiterungen und Veränderungen von Windows SharePoint Services mit einfachen Mitteln.\n\nFrontpage bietet eine Reihe von speziellen Funktionen, die durch Verwaltungsinformationen in Form von \"vti-Dateien\", so genannten \"Bots\" oder/und XSLT ermöglicht werden. Einige dieser Funktionen ähneln dem Leistungsumfang, den professionelle, web-basierte Content-Management-Systeme besitzen, z. B. das automatische Generieren von Navigationselementen, das rasche und konsistente Wechseln des Erscheinungsbildes (\"Design\") der gesamten Website oder die Volltextsuche über den gesamten Inhalt einer Website. Die Verwaltungsinformationen sind proprietär und werden ausschließlich von Microsoft FrontPage und einigen verwandten Microsoft-Produkten (z. B. Windows Sharepoint Services) genutzt. Auch die \"FrontPage Bots\" sind proprietär. Beide Technologien setzen teilweise eine spezielle Erweiterung auf dem Webserver, die so genannten \"FrontPage Server Extensions\" (FPSE), voraus. Auf entgeltlich gemietetem Webhosting sind diese meist vorhanden. Der von den letzteren Versionen von FrontPage (2002 „\"XP\"“ / 2003) erzeugte HTML-Code hält sich an die Standards des W3C. Dokumente aus Microsoft Office können problemlos importiert werden, um proprietären Quelltext zu entfernen.\n\nMicrosoft FrontPage eignet sich für die Verwaltung von kleineren und mittelgroßen Websites. Die Verwaltung größerer Websites mit einem Datenvolumen von rund 10 GB mit mehreren hunderttausend Objekten ist durch die Untergliederung in \"Webs\", \"Subwebs\" und \"Nested Subwebs\" zwar ebenfalls möglich, jedoch verringert sich dabei der Benutzerkomfort und die Gewährleistung einer Design- und Link-Konsistenz wird zunehmend schwieriger.\n\nDas ursprünglich von \"Vermeer Technologies Inc.\" entwickelte Programm wurde 1995 in der Version \"FrontPage 1.0\" am Markt eingeführt. Zu dieser Zeit gab es noch keine verbreiteten visuellen HTML-Editoren. FrontPage war von Anfang an mehrbenutzerfähig und integrierte die Verwaltung von Websites sowie das Editieren von HTML-Seiten in einem Paket, dabei waren die beiden Funktionen aufgeteilt in zwei Applikationen: dem FrontPage Explorer, der die Verwaltung übernahm, und zum anderen den FrontPage Editor, der dann die Bearbeitung einer einzelnen Seite ermöglicht. Bereits nach kurzer Marktpräsenz wurde das Produkt von Microsoft übernommen und als \"Microsoft FrontPage 1.1\" im Frühjahr 1996 auf den Markt gebracht.\n\nFrontPage 2.0 wurde von Microsoft ebenfalls 1996 unter der Bezeichnung \"Microsoft FrontPage 97\" veröffentlicht. Verbessert wurde die Integration in Microsoft Office, daneben wurden Zusatztools wie das Bildbearbeitungsprogramm \"Image Composer\" und der Webserver \"Microsoft Personal Web Server\" (PWS) hinzugefügt. Seit dieser Version kann FrontPage HTML-Code direkt bearbeiten.\n\nFrontPage Express war eine abgespeckte Variante des HTML-Editors FrontPage 2.0 aus dem Hause Microsoft. Dieser wurde, benannt in Anlehnung an Outlook Express, mit dem Internet Explorer 4.0 und 5.0 vertrieben, danach aber auch wieder eingestellt. Die Variante bestand nur aus dem FrontPage Editor selbst, der FrontPage Explorer fehlte.\n\nFrontPage 3.0 wurde als \"Microsoft FrontPage 98\" im Jahr 1997 veröffentlicht. Es wurden weitere Werkzeuge zur Bildbearbeitung sowie die Möglichkeit der WYSIWYG-Bearbeitung von Tabellen und Frames integriert. In dieser Version wurde auch die grafische Darstellung der Struktur einer Website zur Bearbeitung der automatisch generierten Navigationsleisten, sowie die Unterstützung für die Skriptsprache ASP hinzugefügt.\n\nFrontPage 4.0 erschien 1999 unter der Bezeichnung \"Microsoft FrontPage 2000\", war zum ersten Mal eine einzelne integrierte Applikation, die FrontPage Editor und FrontPage Explorer vereinte und bot erstmals Unterstützung für Cascading Style Sheets und konnte direkt bearbeiteten HTML-Code vor Veränderungen durch das Programm schützen (\"HTML Source Preservation\"). Außerdem wurden zahlreiche Funktionen zur Vereinfachung und Benutzung ergänzt, beispielsweise der \"Database Results Wizard\". Diese Version bot auch eine rudimentäre Unterstützung für DHTML und XML, sowie einen integrierten Mechanismus zur Versionskontrolle (\"Check in\", \"Check out\"). FrontPage \"Webs\" können seit dieser Version weiter verschachtelt werden (\"Nested Subwebs\"). Es gab auch erstmals Unterstützung für einfaches Workflow Management.\n\nFrontPage 5.0 wurde 2001 als \"Microsoft FrontPage 2002\" (auch fälschlicherweise bekannt als \"Frontpage XP\") am Markt eingeführt. Zu den Neuerungen zählen automatisch aktualisierte Webinhalte, die Bearbeitung von SharePoint-basierten Websites über den Browser, eine stärkere Integration der Office-Zwischenablage und die Möglichkeit der gezielten Veröffentlichung einzelner Seiten.\n\nDie Version \"Microsoft FrontPage 2003\" (auch bekannt als \"FrontPage ONG\" oder \"FrontPage 11\" oder \"FrontPage 6.0\") erschien 2003 und verbesserte vor allem die Unterstützung der offenen Web-Standards wie XML und WebDAV. Erstmals stehen seit dieser Version ein Seitenlineal und ein Layoutraster für pixelgenaues HTML-Layout zur Verfügung.\n\nIn Office 2007 ist FrontPage nicht mehr enthalten. Es wird abgelöst von Microsoft Expression Web und dem kostenlosen Visual Web Developer Express zur Webseiten-Erstellung für das Internet und SharePoint-Designer zur Gestaltung von SharePoint-Seiten vornehmlich für das Intranet.\n\nDie Microsoft FrontPage Server Extensions (FPSE) sind Erweiterungen, die den FrontPage-Client um zusätzliche Funktionen ergänzen. Die FPSE werden auf dem Webserver installiert und stehen für die Plattformen Microsoft Windows (Internet Information Services (IIS) bzw. früher Internet Information Server) sowie Unix (PHP, Apache) unter einer proprietären Lizenz zur Verfügung.\n\n"}
{"id": "97099", "url": "https://de.wikipedia.org/wiki?curid=97099", "title": "Plattform (Computer)", "text": "Plattform (Computer)\n\nEine Plattform – auch Schicht oder Ebene genannt – bezeichnet in der Informatik eine einheitliche Grundlage, auf der Anwendungsprogramme ausgeführt und entwickelt werden können. Sie befindet sich zwischen zwei Komponenten eines Rechnersystems. Für die Komponente, welche die Plattform nutzt, ist die Komponente darunter nicht sichtbar. Daher kann dieselbe Komponente über eine Plattform auf verschiedenen „Untergründen“ betrieben werden. Es gibt eine Vielzahl von Plattformen und Plattformkonzepten im Informatikbereich.\n\nDie Idee hinter einer Plattform ist die Abstraktion von komplizierten Details für eine Anwendungssoftware bzw. deren Entwickler.\n\nEinerseits können diese Details unbekannte Eigenschaften der Ausführungsumgebung sein, in der eine Anwendungssoftware zukünftig verwendet wird, die zum Entwicklungszeitpunkt der Anwendung nicht bekannt sind oder sein können. Diese Eigenschaften der Ausführungsumgebung können beispielsweise der genaue Typ und die Leistungsfähigkeit der Hardwarekomponenten sein oder mit welchem Betriebssystem die Anwendung irgendwann einmal vom Anwender betrieben wird.\n\nAndererseits kann die Motivation für eine Abstraktion auch bekannte Komplexität sein (z. B. unstandardisierte Hardware, konkurrierende Hersteller-APIs), die reduziert werden soll, um Entwicklern eine schnellere, günstigere oder einfachere Entwicklung von Anwendungen zu ermöglichen.\n\nErreicht werden kann diese Vereinfachung dadurch, dass dem Anwendungsentwickler ein abstrakteres Funktionsmodell von konkreter Funktionalität zur Verfügung gestellt wird, typischerweise in Form einer Programmierschnittstelle (eng. API), welche darunter liegende Funktionalität einhüllt. Für die resultierende Anwendung geschieht das typischerweise in Form einer dynamisch interpretierten Laufzeitumgebung (z. B. JRE, Browser) oder einer binären ABI zu bekannten Softwarefunktionen (z. B. Win32, DirectX).\n\nEine Qualität, die diese Abstraktionsschichten bieten können, ist Allgemeingültigkeit, üblicherweise als \"Kompatibilität\" bezeichnet. Das kann sich auf die \"Breite\", also die Menge der verschiedenartigen, abstrahierten Details beziehen, wie auch auf die Stabilität der Plattform über die Zeit. Bei der Kompatibilität über die Zeit kann die Sicherstellung der Abwärtskompatibilität bei einer Weiterentwicklung einer Plattform gemeint sein oder auch die Zusicherung des Herstellers, dass mit dem Aufkommen neuer abstrahierbarer „Details“ (z. B. neue Betriebssysteme, neue Hardware) diese in die Plattform integriert werden (Aufwärtskompatibilität).\n\nMögliche Bestandteile einer Plattform sind eine Rechnerarchitektur, Programmiersprache, Bibliotheken und Laufzeitumgebungen.\n\nBei Plattformen kann zwischen Soft- und Hardwareplattformen unterschieden werden. Eine Hardwareplattform, auch Maschinenebene genannt, bezeichnet eine bestimmte Rechner­art oder eine \"[Prozessor]-Familie\". Die Maschinenebene ist hauptsächlich durch eine bestimmte Rechner- oder Prozessorarchitektur gegeben und liegt logisch betrachtet ganz unten – unter der Anwendungsebene.\n\nEine Prozessorarchitektur-Plattform verwendet eine einheitliche Maschinensprache, Datenwort­größe, Byte-Reihenfolge usw. Ein Beispiel dafür ist die weitverbreitete x86-Architektur.\n\nWie die einzelnen Befehle dieser Maschinensprache intern im Mikroprozessor verarbeitet werden (z. B. mit Micro-ops), kann sich aber innerhalb der gleichen Plattform stark unterscheiden. Nur die Endergebnisse, welche die Befehle liefern, bleiben dieselben.\n\nHardwareplattformen können grob in CISC- und RISC-Architekturen eingeteilt werden. Bei aktuellen Prozessorarchitekturen verwischen sich aber die Grenzen zwischen diesen beiden Architekturtypen zusehends.\n\nDie sogenannten \"Software-Plattformen\", auch Anwendungsebene oder -schicht genannt (siehe auch OSI-Modell), werden wie folgt unterschieden.\n\nKompatibilität über die Zeit lässt sich beispielsweise über stabilgehaltene Binärschnittstellen von Funktionsbibliotheken erreichen, mit denen auf die Plattform zugegriffen wird. Bei einer Weiterentwicklung der Plattform muss ausschließlich der Plattformanbieter dafür Sorge tragen, dass die Kompatibilität erhalten bleibt. Dieser muss dann die neue Version seiner Plattformbibliothek verbreiten, Änderungen am Anwendungsprogramm (Neukompilierung oder Anpassung) durch Anwendungsentwickler oder Konfigurationsänderungen durch Anwender sind nicht notwendig.\n\nNeben dem obigen Konzept einer auf Binärkompatibilität basierenden Plattform, welches eine weitergehende Lauffähigkeit von einmal erstellter Software ermöglicht, existiert noch das Konzept der Kompatibilität über die Portierbarkeit des Quellcodes eines Anwendungsprogramms. Hier wird keine langfristige oder breite Lauffähigkeit der Anwendungsprogramm-Kompilate garantiert, sondern eine Kompilierbarkeit mit einer weiten Palette an unterliegender Hardware, Programmbibliotheken und Software-APIs, auch Plattformunabhängigkeit genannt. Nachteile sind, dass der Vorgang des Kompilierens dann häufiger und vor allem durch den Anwender oder Anwendungsentwickler durchgeführt werden muss, ein manchmal komplexer und fehlerträchtiger Vorgang. Auch die Erstellung portabler Software für eine solche Plattform ist ein Problem. Ebenso kann die Notwendigkeit, dass der Quellcode beim Anwender vorliegen muss, ein Hindernis sein, da beispielsweise bei proprietärer Software eine Offenlegung von diesem unüblich ist. Deshalb ist dieses Konzept der Quellcode-basierenden Kompatibilität vor allem im Open-Source-Bereich und bei unixähnlichen Betriebssystemen dominierend, die Binärkompatibilität dagegen beispielsweise bei Windows oder den Mac-Betriebssystemen.\n\nBeispielsweise ermöglicht es eine Softwareplattform – wie die Win32-API und andere ähnliche in Betriebssysteme integrere Schnittstellen – Softwareentwicklern, Anwendungen zu schreiben, die auf veränderlichen, Hardware, wie Prozessoren unterschiedlicher Hersteller, verschiedenen Grafikkarten, verschiedenen Peripheriegeräten usw. funktionsfähig sind. Typischerweise werden solche Anwendungen jedoch zu binären Programmen, bestehend aus Maschinenbefehlen, kompiliert, sind also nur auf einer spezifischen Hardware funktionsfähig, setzen also auf diese Hardwareplattform auf. Dieses Vorgehen kann als Kompromiss aus Effizienz und Abstraktionsgrad betrachtet werden, da dadurch aufwändige Konvertierung zur Laufzeit eingespart werden.\n\nBei dynamisch interpretierten Laufzeitumgebungen wird die Anwendung von der Hardware noch weitergehend abstrahiert. Das bedeutet, dass Befehle und Daten einer Laufzeitumgebung oder einem Dienst übergeben werden und dort erst zur Laufzeit interpretiert oder in die entsprechende Maschinensprache übersetzt werden. Weitergehend können mit einer Laufzeitumgebung (z. B. JRE oder Webbrowser) auch verschiedene unterliegende Betriebssysteme, also andere Softwareplattformen, wegabstrahiert werden.\n\nFür die Werbung werden oft Markennamen in vereinfachender Weise, als technisch betrachtet eigentlich zu differenzierende Plattformen, zusammengefasst. Ein bekanntes Beispiel dafür ist die „Macintosh-Plattform“, deren technische Plattformen sich je nach Generation grundlegend unterscheiden können. Diese vereinfachende Sicht ist teilweise in den Sprachgebrauch und die öffentliche Wahrnehmung übergegangen.\n\nSo wirbt z. B. die Firma Apple mit der „Macintosh“- bzw. „Mac“-Plattform, obwohl über die gesamte Zeit des Bestehens praktisch alle Plattformen, die Macintosh ausmachen, (teilweise mehrfach) ausgetauscht wurden. Aus technischer Sicht besteht und bestand Macintosh aus sehr unterschiedlichen und teilweise inkompatiblen Hard- und Softwareplattformen. (Macintosh nutzte oder nutzt 680x0, POWER und x64. Von Apple-Betriebssystemen verwendete Softwareschnittstellen und Standards sind oder waren Carbon, Cocoa, POSIX, SUS, GNU-Software-Umgebung, JRE etc.). Um den Nutzern einen reibungslosen Wechsel der Plattformen zu gewährleisten, verwendete Apple übergangsweise Ansätze wie Fat Binarys oder Emulatoren. Dadurch wurde die ganze Produktfamilie in der Öffentlichkeit weiter als eine einheitliche Plattform wahrgenommen.\n\nÄhnliches gilt auch für die von der Firma Microsoft beworbene Marke „Windows“. Obwohl die Änderungen nie so umfassend waren wie bei Macintosh, ist auch Windows keine einheitliche Plattform. (Es nutzte oder nutzt die Plattformen x86, x64, ARM, MIPS, PowerPC sowie Alpha und stellte oder stellt die Plattformen DOS, Win16, Win32, Win64, Native API, Windows CE, .NET, POSIX, OS/2 und andere Anwendungen zur Verfügung.) So sind z. B. die Win32- und die Windows-CE-API nur sehr bedingt kompatibel. Alle auf den DOS- oder Windows-NT-Kernel aufbauenden Windows-Produkte enthalten mehrere Plattformen, wodurch für Anwendungen eine Rückwärts-Kompatibilität von teilweise bis zu 30 Jahre (im Fall von Win16) erreicht wurde.\n\nHersteller von Plattformen haben verschiedene Vorgehensweisen bezüglich der Offenheit bzw. Geschlossenheit ihrer Plattformen.\nDies betrifft z. B. das Entwicklungsmodell, Kostenmodell oder den Grad der Offenheit bzw. Freiheit die bei der Verwendung auf verschiedenen Ebenen gewährt wird.\n\nAls Anwendungsschnittstelle kann im Wesentlichen eine durch das Betriebssystem eingeführte oder inkludierte Programmierschnittstelle (, kurz API) bezeichnet werden. Es gibt jedoch auch plattformübergreifende APIs, die auf mehreren Betriebssystemen als Laufzeitumgebung verfügbar sind und oft nachträglich installiert werden müssen.\n\n\n\n\n\n"}
{"id": "97635", "url": "https://de.wikipedia.org/wiki?curid=97635", "title": "Alsa Modular Synth", "text": "Alsa Modular Synth\n\nAlsaModularSynth ist eine Software, die die Emulation analoger Modularsynthesizer gestattet. Die Software ist bereits sehr leistungsfähig, läuft jedoch nur unter unixoiden Betriebssystemen, die als Audiosystem ALSA verwenden.\n\nAlsaModularSynth steht unter der GPL. Das Programm ist ein virtueller analoger Synthesizer und entspricht in seinen Komponenten den echten analogen Hardwaresynthesizern.\nEs eignet sich deshalb für alle, die sich Grundkenntnisse aneignen möchten oder den elektronischen Sound und die elektronisch erzeugten Effekte der analogen Synthesizer lieben und machen möchten.\nMan kann beispielsweise ein MIDI-Keyboard an den Rechner anschließen und dann als Synthesizer verwenden.\n\n"}
{"id": "99169", "url": "https://de.wikipedia.org/wiki?curid=99169", "title": "PC-98", "text": "PC-98\n\nPC-98 ist eine Bezeichnung für auf dem \"NEC\" PC-9800 basierende \"\". PC-98-Rechner waren in Japan sehr weit verbreitet. Der Rechner wurde von 1982 bis 1997 angeboten.\n\nDer PC-98 ist teilweise dem \"IBM PC\" ähnlich, allerdings hat er statt des ISA-Busses den sogenannten \"C-Bus\", einen eigenen 16-Bit-Bus, der auch in der \"PC-88\"-Reihe der Firma \"NEC\" verbaut wurde. Außerdem sind BIOS, I/O-Port-Adressierung, Speicherverwaltung und Grafikausgabe (via NEC µPD7220) anders als bei herkömmlichen PCs.\n\nDie Entwicklung und der Erfolg des PC-98 waren in den Besonderheiten des japanischen Marktes begründet. Dieser war weitestgehend von ausländischen Firmen abgeschottet, sodass japanische Hersteller den Markt dominierten. Während sich im Rest der Welt Personal Computer weitestgehend durchsetzten, hielten die japanischen Hersteller sehr lange an Mainframes fest, was unter anderem in Problemen mit der Unterstützung der japanischen Schrift begründet war.\n\nAls der IBM PC im Rest der Welt einen großen Erfolg verbuchen konnte, entwickelten die japanischen Hersteller verschiedene zueinander inkompatible PC-Standards. Während die meisten japanischen Hersteller PCs lediglich als Terminal für ihre Mainframes vermarkteten, ging NEC, seinerzeit den kleinsten Marktanteil innehaltend, einen anderen Weg und entwickelte mit dem PC-98 einen als Einzelplatzrechner nutzbaren Computer. Da Microsoft nicht bereit war, MS-DOS an den PC-98 anzupassen, entwickelte NEC auch das Betriebssystem selbst. So erschien im Oktober 1982 mit dem PC-9801 das erste Modell der PC-98-Reihe. Vor allem durch den zeitlichen Vorsprung konnte sich NEC einen großen Marktanteil sichern. Durch geschicktes Marketing entwickelte sich eine reichhaltige Softwarebibliothek; im Jahr 1987 gab es 3500 Softwareprodukte für den PC-98. Schon bald entstanden erste Klone des PC-98, unter anderem der erste PC-98-basierte Laptop im Jahr 1987. Im Jahr 1991 hielt der PC-98 einen Marktanteil von 60 Prozent. Dadurch, dass NEC das Urheberrecht an allen relevanten Komponenten hielt, konnte es durch hohe Preise einen großen Gewinn erwirtschaften.\n\nDer Niedergang des PC-98 begann im Jahr 1991 mit der Entwicklung von DOS/V. DOS/V ermöglichte erstmals die Verwendung der japanischen Sprache auch auf gewöhnlichen IBM-kompatiblen PCs, sodass hierfür keine speziellen Rechner wie der PC-98 erworben werden mussten. Im Jahr 1993 folgte dann Windows 3.1. Dieses erschien zwar auch für den PC-98, aber durch die gemeinsame Programmierbibliothek war eine besondere Anpassung von Anwendungsprogrammen auf die verschiedenen Rechnerarchitekturen nicht mehr notwendig. Damit hatte der PC-98 keine Vorteile mehr gegenüber IBM-kompatiblen PCs und konnte sich auf Dauer wegen des hohen Preises nicht durchsetzen. NEC sah sich dadurch gezwungen, 1997 die PC-98-Reihe endgültig einzustellen.\n\n"}
{"id": "99397", "url": "https://de.wikipedia.org/wiki?curid=99397", "title": "Farbtiefe (Computergrafik)", "text": "Farbtiefe (Computergrafik)\n\nDie Farbtiefe bestimmt eine wesentliche Eigenschaft von Raster- und Vektorgrafiken: die Differenzierung aller Helligkeits- und Farbwerte.\nDie Farb- und Helligkeitswerte von digitalen Bildern werden innerhalb der kleinsten Einheit jedes Bildes gespeichert: bei Rastergrafiken innerhalb jedes Pixels, bei Vektorgrafiken innerhalb jedes farbdefinierten Vektors. Jede Bildeinheit enthält eine festgelegte Anzahl der maximal möglichen Abstufungen (beispielsweise beim durchschnittlichen Digitalfoto: 256 Abstufungen pro Farbkanal eines Pixels) sowie die konkrete Farb- und Helligkeitsinformation (auf der Skala dieser festgelegten Abstufungen).\nDie Anzahl der möglichen Abstufungen ist nicht zwangsläufig gleichbedeutend mit der Anzahl der möglichen Farben. Hier wird unterschieden nach Anzahl der Farbkanäle oder nach Umfang der Farbtabelle. Erst der Zusammenhang aus der Art der Farbdefinition (Farbkanäle und ihre Anzahl, Tabellen, …) sowie der Angabe der Abstufungen (in Bit) ergibt die maximal mögliche Farbtiefe.\nDie maximal mögliche Menge an (Farb-)Abstufungen wird in Bits angegeben und benennt damit die Farbtiefe eines Bildes. Diese Abstufungen stellen eine Skala dar, auf der die eigentliche Farbinformation gespeichert wird. Die \"Farbtiefe\" ist also die mathematische Basis der tatsächlichen Farbinformation. In der Praxis besitzt ein Bild niemals die Menge an Farben, die der Umfang dieser Skala (Farbtiefe) zur Verfügung stellt.\n\nEine Farbtiefe von 1 Bit würde bedeuten, dass in jeweils einem Farbkanal (am Computer-Bildschirm meist rot, grün und blau) genau zwei Zustände möglich wären. Als Beispiel wären das für den Farbkanal rot dann \"schwarz und rot\". Bei einer Farbtiefe von 2 Bit wären 4 Zustände möglich, also beispielsweise \"schwarz, dunkelrot, mittleres Rot und hellrot\". Bei der gebräuchlichen Farbtiefe von 8 Bit sind 2 = 256 Zustände und damit ebenso viele einzelne Rot-Töne möglich.\n\nAm gebräuchlichsten ist der RGB-Farbraum mit 8 Bit pro Kanal, entsprechend (2) = 16.777.216 (ca. 16,8 Millionen) theoretisch möglichen Farben. Bei 16 Bit (pro Kanal) resultieren daraus 281.474.976.710.656 (281 Billionen) Farbmöglichkeiten.\n\nBilder mit indizierten Farben stellen eine Sonderform dar: bei ihnen enthält die Datenstruktur eines Pixels nicht die Farben selbst, sondern einen Index auf einen Eintrag der Farbtabelle. Statt von einer Farbtabelle spricht man dabei auch oft von einer Farbpalette. Die Farbtiefe gibt also die maximale Anzahl der verwendbaren Einträge der Farbtabelle an. Praktisch werden Farbtabellen mit 1 bis 8 bpp (= bit pro Pixel) verwendet, entsprechend 2 = 2 bis 2 = 256 gleichzeitig kodierten oder darstellbaren Farben. 1 bpp ist für Bilder, die nur Schwarz und Weiß enthalten, gebräuchlich.\n\nBeispiele:\n\nDigitalfotos besitzen üblicherweise eine Farbtiefe von 24 Bit. In der Praxis gibt es natürlich kein Foto, das wirklich alle Einzelfarben besitzt – der Durchschnitt liegt deutlich darunter. Der Vorteil der 24-Bit-Farbtiefe kommt dennoch bei fast jedem Foto deutlich zum Tragen, wie man anhand eines Vergleichs mit einer 16-Bit-Version desselben Fotos feststellen kann. Das Foto mit 16-Bit-Farbtiefe zeigt erkennbare, oft sehr störende Treppchenmuster bei Farbübergängen, die bei 24-Bit-Farbtiefe nicht mehr sichtbar sind. Der Vorteil der höheren Farbtiefe liegt also weniger in der Maximalzahl der möglichen Farben, sondern vielmehr in der größeren Farbdifferenzierung.\n\nDie meisten Computermonitore können nur 8 Bit pro Kanal darstellen. In der professionellen Fotografie und für medizinische Anwendungen werden auch 16 Bit pro Kanal benötigt. Extreme Helligkeitsbereiche (tiefschwarzer Schatten und gleißendes Licht) können mit 8 Bit nicht gespeichert werden. Hierzu ist eine drastische Reduzierung des Kontrastumfangs und der Kontrastdifferenzierung nötig. Um diese Veränderung des Dynamikumfangs optisch ansprechend zu gestalten, finden \"High Dynamic Range Images (Hochkontrastbilder)\" Anwendung, die per Tone-Mapping-Verfahren zur Darstellung auf 8 Bit heruntergerechnet werden. Dieses Verfahren ist eine spezielle Form der Bildoptimierung.\n\nIm Scan-, Kino-, TV- und Druckbereich kommen auch weitere Farbtiefen mit 30, 32, 36, 40 und 48 Bit häufig vor, z. T. auch nur in der internen Verarbeitung (d. h. beispielsweise arbeitet die Hardware eines Scanners intern mit 30 Bit, um eine Vorlage einzulesen, der Scanner gibt danach aber an den Computer das fertig gescannte Bild nur mit 24 Bit Farbtiefe aus).\n\n"}
{"id": "99716", "url": "https://de.wikipedia.org/wiki?curid=99716", "title": "Backdoor", "text": "Backdoor\n\nEin Beispiel sind Universalpasswörter für ein BIOS oder eine spezielle (meist durch einen Trojaner heimlich installierte) Software, die einen entsprechenden Fernzugriff auf den Computer ermöglicht.\n\nAls Trojanisches Pferd, kurz Trojaner, wird ein Computerprogramm oder Skript bezeichnet, das sich als nützliche Anwendung tarnt, im Hintergrund aber ohne Wissen des Anwenders eine andere Funktion erfüllt. Das einfachste Beispiel dafür ist eine schädigendes Programm, welches Dateien des Benutzers löscht, dessen Dateiname aber auf eine andere Funktion schließen lässt, wie codice_1. Dabei ist es unerheblich, ob der „lustige Bildschirmschoner“ tatsächlich auch einen Bildschirmschoner anzeigt, während er die Daten zerstört, oder ob er einfach nur die Daten zerstört. Die Nutzung des irreführenden Dateinamens genügt, um das Programm als Trojanisches Pferd zu klassifizieren.\n\nTrojaner können auch dazu dienen, Backdoorprogramme zu installieren, müssen diese jedoch nicht notwendigerweise enthalten. Beherbergt und installiert ein Trojaner ein eigenständiges Backdoorprogramm, greift der Eindringling auf das installierte Backdoorprogramm zu und nicht auf den Trojaner. Der Trojaner diente in diesem Fall lediglich als Hilfsprogramm für die heimliche Installation. Der Trojaner kann danach jederzeit gelöscht werden, ohne dass dies einen Einfluss auf die weitere Funktion des Backdoorprogramms hat.\n\nAllerdings hält niemand den Entwickler eines Backdoorprogramms davon ab, sich der Technik eines Trojaners zu bedienen. Bei einem Backdoorprogramm, das sich selbst als nützliche Anwendung tarnt (beispielsweise als Desktopuhr, die heimlich einen Fernzugriff auf den Computer ermöglicht), handelt es sich um eine Mischform zwischen einem Backdoor und einem Trojaner. Wird ein solches Programm beendet oder gar gelöscht, so steht auch die heimliche Backdoorfunktion nicht mehr zur Verfügung.\n\nEine Variante besteht darin, in einem System fest vorgegebene, nur dem Ersteller des Systems bekannte Passwörter oder andere versteckte Funktionen einzubauen, die einen Zugriff ohne die sonst übliche Authentifizierung ermöglichen. Ein bekanntes Beispiel hierfür ist der von \"Award Software\" über mehrere Jahre vergebene Hash-Code, der mit dem BIOS-Universalpasswort „lkwpeter“ bedient wird.\n\nZur Software, die einen Fernzugriff auf den Computer ermöglicht, zählen z. B. Programme wie \"Sub Seven\" und \"Back Orifice\".\n\n1999 wurde eine Variable namens NSAKEY in Windows gefunden und ebenfalls eine Backdoor vermutet.\n\nAuch die Router von Cisco Systems, die weite Teile des Internetverkehrs abwickeln, sind mit Backdoors für US-Geheimdienste versehen.\n\nPublikumswirksam demonstriert wurde der Einsatz einer Hintertür in Kinofilmen wie \"WarGames – Kriegsspiele\" und \"Jurassic Park\".\n\nBei Softwareprodukten ist eine freie Einsicht in deren Quellcode ein Aspekt der Computersicherheit. Dabei gilt es unter anderem die Gefahr zu minimieren, dass ein Produkt Funktionalitäten enthalten kann, von denen der Anwender nichts wissen soll, wie die heimliche Funktion einer Backdoor.\n\nQuelloffene Software lässt sich von der Öffentlichkeit dahingehend überprüfen und darüber hinaus mit rechtlich unbedenklichen Mitteln auf Schwachstellen untersuchen, die auf diese Weise schneller geschlossen werden können.\n\nQuelloffene Software kann zwar durch jeden mit entsprechender Sachkunde selbst auf heimliche Funktionalitäten und Schwachstellen hin untersucht werden, was jedoch nicht bedeutet, dass die bloße Verfügbarkeit des Quelltextes ein Garant dafür ist, dass dieser von den Computernutzern hinreichend überprüft wurde. Über einen langen Zeitraum bestehende Sicherheitslücken in quelloffener Software weisen auf diesen Umstand hin. Zudem ist eine geschickt verbaute Hintertür auch mit fundierten Fachkenntnissen mitunter schwer zu erkennen. Der Zeitaufwand für eine Analyse ist bei komplexen Programmen oft beträchtlich.\n\nOb das von einer externen Quelle bezogene ausführbare Programm tatsächlich mit dem veröffentlichten Quellcode erstellt wurde, oder ob hier nicht zuvor eine Hintertür eingebaut oder eine andere Veränderung vorgenommen wurde, ist für den Anwender oft schwer zu erkennen. Auch hierfür gilt, dass mit entsprechender Sachkunde wenigstens in der Theorie eine Überprüfung möglich ist. Dies gestaltet sich jedoch in der Praxis oft als schwierig, da die beim Kompilieren entstehenden Binärdateien gerade bei größeren Codebasen durch sehr viele Faktoren beeinflusst werden können und es im Allgemeinen keine zuverlässige Möglichkeit gibt herauszufinden unter welchen Bedingungen eine vorliegende ausführbare Datei entstanden ist.\n\nEine Methode, diesen Schritt des Kompilierens abzusichern, ist, „reproducible builds“ zu erstellen. Dabei wird die Software reproduzierbare bzw. deterministisch kompiliert und so kann jeder durch eigene Kompilierung nachprüfen, dass das Kompilat aus dem entsprechenden Quellcode gebaut wurde und während des Build-Prozesses keine Hintertür eingeschleust wurde.\n\n1984 stellte der Computer-Pionier Ken Thompson während seiner Turing-Award-Rede ein Beispiel für eine Hintertür vor, die selbst bei der Verfügbarkeit des Quelltextes schwer aufzuspüren wäre. Die Rede war von einem login-Programm für Unix, das derart verändert wird, dass es zusätzlich zum normalen Passwort auch ein Generalpasswort akzeptiert. Diese Hintertür könne, so Thompson, ein entsprechend manipulierter C-Compiler beim Übersetzen des login-Programms automatisch hinzufügen, wodurch der Quelltext des login-Programms keinen Hinweis auf eine Manipulation liefert. Das Vorgehen ließe sich auf eine weitere Instanz verschieben, die dafür zuständig ist, den C-Compiler selbst in eine ausführbare Datei zu übersetzen, wodurch die Manipulation dann nicht einmal mehr aus dem Quellcode des C-Compilers ersichtlich wäre.\n\n"}
{"id": "99967", "url": "https://de.wikipedia.org/wiki?curid=99967", "title": "Workstation", "text": "Workstation\n\nMit Workstation (deutsch: „Arbeitsstation“) bezeichnet man einen besonders leistungsfähigen Arbeitsplatzrechner für technisch-wissenschaftliche Zwecke oder für die Bearbeitung von Audio- und Videodaten, in Abgrenzung zum handelsüblichen Personal Computer für den Privat- oder Bürogebrauch. Typischerweise werden Workstations in Unternehmen und Forschungseinrichtungen für rechenintensive Anwendungen wie die 3D-Konstruktion, Computersimulationen, Videobearbeitung und animierte 3D-Computergrafik eingesetzt. Üblicherweise erbringen Workstations im Bereich Grafikdarstellung, Rechenleistung, Speicherplatz und Multitasking überdurchschnittliche Ergebnisse, oft können zusätzliche Terminals verwendet werden. Zur Erhöhung der Ausfallsicherheit und Langlebigkeit kommen teilweise auch Technologien und Komponenten aus dem Server-Bereich zum Einsatz.\n\nIn den 1980er und 1990er Jahren war der Markt durch Workstations mit Unix- und VMS-Betriebssystemen dominiert, viele Hersteller produzierten eigene Hardware auf Basis herstellerspezifischer Hochleistungs-RISC-Mikroprozessoren wie der PA-RISC-, MIPS- oder SPARC-Serie. Die Anschaffungskosten betrugen in der Regel ein Vielfaches von denen durchschnittlicher PCs. Mittlerweile hat sich auch im Workstation-Segment weitgehend die Kombination aus den jeweils leistungsfähigsten Intel- oder AMD-Prozessoren (z. B. Xeon oder Opteron) mit Microsoft-Windows- oder Linux-Betriebssystem durchgesetzt, und die meisten Hersteller bieten ihre eigenen Prozessor-Linien für Workstations nicht mehr an oder haben diese ganz eingestellt. Der Markt wird heute von großen PC-Herstellern wie Hewlett-Packard, Dell und Fujitsu dominiert, die meisten früheren Hersteller von RISC/Unix-Workstations haben ihr Geschäftsfeld auf andere Produkte verlagert.\n\nDurch den zunehmenden Computereinsatz in der Produktentwicklung haben Workstations heute eine zentrale Bedeutung im Entstehungsprozess von industriell gefertigten Produkten. Das Einsatzspektrum umfasst unter anderem die Konstruktion mittels CAD-Software, Funktionssimulation, die Erstellung digitaler Prototypen komplexer Produkte und den Entwurf der Werkzeuge und Formen für die Fertigung. Ein stetig wachsendes Anwendungsgebiet ist auch die Erstellung von Computeranimationen für Spielfilme und Fernsehproduktionen. Dabei werden zum Teil viele, über Standorte in mehreren Ländern verteilte Einzelplatz-Workstations eingesetzt, die zur Erhöhung der Render-Rechenleistung zu einem Rechencluster zusammengeschaltet werden.\n\nDer Begriff ist nicht synonym mit \"Arbeitsplatzrechner\". Eine Workstation ist, ebenso wie ein Personal Computer, ein Arbeitsplatzrechner – aber nicht jeder Arbeitsplatzrechner ist auch eine Workstation. Da jedoch Personal Computer heute ebenfalls sehr leistungsfähig sind und zunehmend im technisch-wissenschaftlichen Bereich eingesetzt werden, verwischen die Grenzen zwischen Personal Computer und Workstation immer mehr. Unterstützt wird dieser Trend durch die häufige Praxis im Computer-Marketing, einem Desktop-Computer durch den Begriff \"Workstation\" einen Anstrich besonderer Leistungsfähigkeit zu geben.\n\nInsbesondere für den Einsatz moderner 3D-Computerspiele werden so genannte Gaming-PCs angeboten, die ebenfalls mit schnellen Prozessoren und Grafikkarten sowie großem Arbeitsspeicher ausgestattet sind. Diese werden jedoch nicht zu den hier behandelten Workstations gezählt – unter anderem, da sie technisch und qualitativ nicht auf die für professionellen Arbeitseinsatz notwendige hohe Zuverlässigkeit ausgelegt sind und auch nicht unter diesem Gesichtspunkt vermarktet werden.\n\nHistorisch war die Eigenschaft \"für einen Benutzer\", oder zumindest \"für wenige Benutzer\" eine wichtige Unterscheidung zu den sonst üblichen Mehrbenutzersystemen. Statt wie beim Mehrplatzsystem über ein Terminal (meist seriell und im Textmodus) mit einem Computer verbunden zu sein, dessen Rechenzeit man sich mit vielen Anderen teilen musste, stand dem Ingenieur, dem Wissenschaftler oder einer kleinen Arbeitsgruppe mit einer Workstation praktisch exklusiv ein eigenes Gerät zur Verfügung. Im Unterschied zu den oft langsamen seriellen Terminalverbindungen der klassischen Mehrplatzsysteme verfügen Workstations über direkt angebundene, zum Teil mehrere, leistungsfähigere Grafiksysteme und Monitore, zusätzlich können sie aber auch über schnelle Netzwerkverbindungen angeschlossene leistungsstarke grafische X-Terminals betreiben. Damit eröffneten sich gerade für technisch-wissenschaftliche Anwendungen ganz neue Visualisierungsmöglichkeiten. Vor diesem Hintergrund sind auch die für Workstations und für X-Terminals typischen, hochwertigen, großformatigen Bildschirme zu sehen. Durch die rasanten Entwicklungen bei Standard-PCs, vor allem auch im Bereich CPU, GPU und Betriebssystemen, verschwamm der Unterschied zwischen Workstation und PC immer mehr und führte ab etwa 2000 zum Ausdünnen und in der Folge auch zum Verschwinden der üblicherweise eigenentwickelten Workstation-Architekturen.\n\nWorkstations entwickelten sich in den 1980er-Jahren zu einer eigenständigen Rechnerform, nicht zuletzt durch die großen Workstationhersteller dieser Zeit wie Apollo, DEC, HP, Sun, SGI, und NeXT, denen es gelang, die Vorzüge einer Workstation gegenüber Mehrbenutzersystemen aufzuzeigen. Hinzu kam zu der Zeit die Idee des Client/Server-Computing, bei dem Workstations als Client ebenfalls einen Platz haben. Viele dieser Hersteller sind heute vom Markt verschwunden oder produzieren keine Workstations mehr. Unter anderem auch deshalb, da Personal Computer immer weiter in die traditionellen Anwendungsbereiche von Workstations eindrangen. Heutige Standard-PCs sind wesentlich preisgünstiger als die klassischen Workstations, dennoch sind sie in puncto Rechen- und Grafikleistung den traditionellen Workstation-Architekturen bzw. -Prozessoren (MIPS, PA-RISC, PowerPC, SPARC) ebenbürtig, wenn nicht teilweise sogar überlegen. Die meisten heute als \"Workstations\" angebotenen Systeme sind normale High-End-PCs mit einem x86-Prozessor. Häufig werden dabei Prozessoren aus den Server- und Workstation-Serien wie z. B. Intel Xeon oder AMD Opteron verwendet.\n\nWorkstations sind typischerweise sowohl in Bezug auf ihre Hardware als auch ihre Software besonders robust ausgestattet. Als vergleichsweise teure Systeme waren sie für professionelle Anwendungen ausgelegt, bei denen Ausfallzeiten einen erheblichen Kostenfaktor darstellen. Als Betriebssystem kamen bis Mitte der 1990er-Jahre daher hauptsächlich die kommerziellen UNIX-Versionen der großen Anbieter Sun, HP, IBM und SCO zum Einsatz, aber auch VMS und andere Unix-artige Systeme wie NeXT. Ab 1994 war dann die Entwicklung des Funktionsumfangs der Linux-Distributionen so weit fortgeschritten, dass diese quelloffene Neuimplementierung von Unix die kommerziellen Systeme ersetzen konnte. Etwa zur gleichen Zeit begann sich die graphische Oberfläche mit Windows auch auf PCs durchzusetzen, wo sie bis dahin eine Verbreitung im Wesentlichen nur auf Mac- und Atari-Systemen gefunden hatte, die hauptsächlich von kreativen Anwendern und teilweise im universitären Bereich genutzt wurden. Nur die Unix- und Unix-artigen Systeme boten allerdings die von Workstations gewohnte Zuverlässigkeit. Insbesondere bei Windows 3.1, das ein Aufsatz auf MS-DOS war, aber auch noch bei Windows NT standen regelmäßig Systemabstürze auf der Tagesordnung. In der Folgezeit sind die Unterschiede zwischen Workstations und PCs immer mehr geschwunden. Eine Unterscheidung zwischen einer Workstation und einem mit hochwertigen Komponenten ausgestatteten PC ist im 21. Jahrhundert kaum noch zu machen.\n\nBei Workstations war Ergonomie von Anfang an ein wichtigeres Thema als beim PC. Während der PC-Nutzer bis in die 1990er-Jahre im Allgemeinen mit den sehr eingeschränkten Möglichkeiten von MS-DOS zurechtkommen musste, waren Workstations Multitasking- und Multiuser-fähig und boten eine graphische Benutzeroberfläche, aber auch die deutlich leistungsfähigere Kommandozeilenumgebung eines Unix-Systems. Hier gab es unter anderem eine automatische Befehlszeilenergänzung und eine Historie für die eingegebenen Befehle. Beim Vergleich dieser Systeme mit neueren Computern ist allerdings zu beachten, dass selbst ein durchschnittliches Smartphone aus dem beginnenden 21. Jahrhundert mehr Rechenleistung bietet als eine Workstation aus den 1980er-Jahren. Entsprechend haben sich auch die Ansprüche an die Ergonomie der Systeme geändert. Die klassischen Desktop-Umgebungen der kommerziellen Unix-Systeme wie \"HP-VUE\" und \"CDE\" mögen einem heutigen Mac- oder Windows-Nutzer wenig benutzerfreundlich erscheinen. In ihrer Zeit waren sie jedoch ergonomischer als die gängigen PC-Systeme.\n"}
{"id": "100078", "url": "https://de.wikipedia.org/wiki?curid=100078", "title": "Commodore 65", "text": "Commodore 65\n\nBeim Commodore 65 (kurz C65) handelt es sich um einen projektierten, aber nie zur Serienreife gebrachten 8-Bit-Heimcomputer des US-amerikanischen Herstellers Commodore International, der 1991 als Nachfolger des marktführenden Commodore 64 auf den Markt kommen sollte. Der C65 sollte zum Vorgängermodell softwarekompatibel sein und zu diesem Zweck mit einer Weiterentwicklung des bewährten 8-Bit-Mikroprozessors MOS Technology 6502, 128 KB Arbeitsspeicher (RAM), 128 KB Festspeicher (ROM) sowie mit gegenüber dem C64 verbesserten Spezialbausteinen für die Bild- und Tonausgabe ausgestattet werden. Eine Erweiterung der Funktionalität der Schnittstellen war ebenfalls vorgesehen. Zur Bedienung und Programmierung des Rechners wurde die Entwicklung eines neuen Dialekts der Programmiersprache BASIC namens Commodore BASIC V10.0 in Auftrag gegeben. Außerdem sollte der C65 über ein neues Gehäusedesign inklusive eines integrierten 3½-Zoll-Diskettenlaufwerks verfügen.\n\nUm den eigenen 16-Bit-Heimcomputern, allen voran dem Amiga 500 sowie dem Amiga 500 Plus, keine gewinnmindernde hausinterne Konkurrenz zu schaffen, wurde jedoch schließlich seitens des Herstellers auf eine Markteinführung des intern als \"C64DX Development System\" bezeichneten Rechners verzichtet. Daher ist der C65 nie über das Planungsstadium hinausgekommen und es existieren nur wenige Prototypen, die aufgrund ihrer Seltenheit sowie der Bedeutung des Vorgängermodells für die Geschichte der Heimcomputer heutzutage einen hohen Sammlerwert besitzen.\n\nNach der Insolvenz und Abwicklung von Commodore im Jahr 1994 wurden unter anderem sämtliche bisher gebauten Prototypen des C65 verkauft, welche sich heute ausnahmslos als gesuchte und geschätzte Raritäten in der Hand von Sammlern befinden. Die Angaben über die im Umlauf befindlichen Stückzahlen variieren zwischen 50 und 1000 Stück. Realistisch ist jedoch eine Menge von nicht mehr als 250 Geräten.\n\nNicht ganz klar ist, warum Commodore den C65 entwickelte, da bereits 1986 mit dem Amiga 500 ein sehr ähnliches Gerät existierte. Vermutungen gehen dahin, dass der ungebrochene Erfolg des Commodore 64 einen dedizierten Nachfolger unumgänglich erscheinen ließ, da die Amiga-Serie nicht mit der Soft- und Hardware des 64ers kompatibel war. Es wäre sicher ein Kaufargument für viele Benutzer gewesen, wenn sie die vorhandenen Gerätschaften sowie Programme weiterhin hätten benutzen können.\nCommodore hatte den Amiga jedoch quasi fast fertig entwickelt zugekauft. Der Amiga war zunächst als Spielkonsole gedacht. Commodore änderte das Design bekanntlich in einen vollwertigen Homecomputer, welcher in zueinander kompatiblen Versionen auch für anspruchsvolle Büroanwendungen genügen sollte. Doch aus der Nische des Homecomputers fand der Amiga, mit Ausnahme professioneller Video-Anwendungen, nie wirklich heraus. Für den C64 brauchte man indes mehr eine Weiterentwicklung als einen Nachfolger. Die ersten Pläne zu dessen Entwicklung wurden bereits vor dem Erwerb des Amiga erstellt.\n\nAm 22. April 2015 kündigte das \"Museum of Electronic Games & Art\" in einem Blogpost an, dass man unter dem Namen \"Mega65\" aktuell an einem Clone des Commodore 65 arbeite. Im ersten Halbjahr 2018 sollen 20 vom Projekt produzierte Entwicklungsmaschinen an Entwickler, frühere C65-Ingenieure, Influencer und Journalisten von Retro-Magazinen ausgegeben werden. Die CPU soll 50 Mal schneller sein als das Original.\n\nDie Prototypen des C65 befinden sich allesamt in jeweils unterschiedlichen, sehr frühen Entwicklungsstadien. So ist zum Beispiel das eingebaute Commodore BASIC V10.0 noch sehr fehlerhaft und weist zum Teil noch erhebliche Lücken auf. Auch die vorgesehene Kompatibilität zum C64 ist bei weitem noch nicht erreicht. Eigentlich waren diese Vorserienmodelle auch nur als Muster für Entwickler und die Presse gedacht.\n\n\nAls Prozessor wurde mit dem CSG 4510 eine Weiterentwicklung des MOS 65CE02 eingesetzt, da der seinerzeit aktuelle Motorola 68000 keine Kompatibilität zum C64 gewährleisten konnte. Der Prozessor wird mit 3,54 MHz getaktet und hat zwei CIAs vom Typ 6526 integriert. Der ursprüngliche 8-Bit-Befehlssatz wurde um einige 16-Bit-RMW-Befehle erweitert, und relative Sprünge und Unterprogrammaufrufe können ebenfalls mit einem 16-Bit-Offset adressiert werden. Somit ist der CSG 4510 eine um 16-Bit-Funktionen erweiterte 8-Bit-CPU. Da nach wie vor auch der komplette Befehlssatz aus dem MOS 6502 enthalten ist, ist der C65 weitgehend binärkompatibel zum C64. Lediglich einige bekannte unzulässige Opcodes des MOS 6502 funktionieren hier nicht mehr, dafür hat der CSG 4510 andere undokumentierte Features.\n\nÄhnlich wie der Amiga hat der C65 verschiedene Co-Prozessoren mit besonderen Aufgaben, und ebenso wie beim Amiga hat man diesen eigene Namen gegeben:\n\n\nEin weiteres Highlight ist ein UART mit programmierbarem Baudratengenerator, welcher Geschwindigkeiten bis zum MIDI-Clock erreichen kann. Dies sollte den C65 bereit machen, um Modems mit hohen Datenraten zu betreiben oder ihn als Sequenzer oder gar als Instrument in einem MIDI-Setup zu verwenden.\n\nNative C65-Programme gibt es so gut wie keine. Ähnlich wie der C128 sollte der C65 aber zum C64 voll kompatibel sein. Dies gelang jedoch nur teilweise, und nur etwa 60–70 % der C64-Programme sind auf dem C65 lauffähig. Das liegt daran, dass im Unterschied zum C128, welcher einen vollwertigen C64 in sich enthält, beim C65 ein C64 quasi auf ein und derselben Hardware emuliert wird. Probleme bereiten vor allem Spiele und Demos, die exzessiven Gebrauch von geläufigen Programmiertricks des C64 machen. Dafür können die meisten der neuen Fähigkeiten des C65 auch im C64-Modus genutzt werden. Außerdem steht ebenfalls wie schon beim C128 ein Maschinensprache-Monitor zur Verfügung. Zwischen den einzelnen Betriebsmodi (C65, C64, Monitor) kann jederzeit gewechselt werden, ohne einen Neustart auszuführen. Der eingebaute BASIC-Interpreter des C65-Modus trägt die Version V10.0.\n\nEs gibt eine Handvoll kleiner Demoprogramme, welche nativ auf dem C65 laufen und die für die damalige Zeit fortschrittlichen Grafikfähigkeiten demonstrieren.\n\nDas ROM enthält in seinen 128 KB BASIC 2.2, BASIC 10.0, DOS und einen Monitor. Es wird jeweils bei Bedarf per Bankswitching der entsprechende Speicherbereich im Kernal eingeblendet.\n\nDas Commodore BASIC basiert von jeher auf dem ersten BASIC-Interpreter von Microsoft aus dem Jahr 1977. Der C64 verfügte über die Version 2.0. Diese ist leicht angepasst als BASIC 2.2 im C65 enthalten (die Datasette-Routinen wurden mangels Anschlussmöglichkeit gelöscht, als Standard-Laufwerk war stattdessen das interne 3,5″ festgelegt). Es ist mit 20 KB im 128 KB großen ROM des C65 enthalten. Zwar klingt das im C65 implementierte BASIC 10.0 aufgrund der Versionsnummer nach einer erheblich erweiterten Version, doch leider wurden nie alle Befehle implementiert, und so weist der Befehlssatz funktionell große Lücken auf.\n\nBASIC 10.0 hat viele Gemeinsamkeiten mit BASIC 7.0 auf dem C128. Zusätzlich gibt es noch eine Reihe neuer interessanter Befehle:\n\nFolgende Befehle geben einen ?UNIMPLEMENTED COMMAND ERROR aus:\n\nWie bei BASIC 7.0 sind die Funktionstasten programmierbar. Belegt sind sie standardmäßig wie folgt:\n\nDie Grafik ist vergleichbar mit den Leistungen des ersten Amiga. Neuheit ist der CSG 4567 Grafikprozessor, auch bekannt unter dem Namen „Bill“ oder „VIC III“, mit integrierter MMU. Der Chip kann 256 Farben in 16 Helligkeitsabstufungen darstellen. Das ergibt eine Palette von 4096 verschiedenen Farben, von denen jedoch nur 256 gleichzeitig dargestellt werden können. Die MMU lässt sich in einer ähnlichen Weise wie der Amiga Blitter separat und autark programmieren. Der Blitter selber ist im DMA-Controller „DMAgic“ integriert. Hiermit ergeben sich seinerzeit einzigartige Möglichkeiten in der Programmierung von dynamischen Grafik- und Farbeffekten. Die Auflösung kann bis zu 1280 × 400 Pixel (interlaced) betragen. Die Darstellung von 256 Farben bei 320 × 200 Pixel war einem damals modernen PC ebenbürtig. Selbst der Amiga konnte im Normalmodus nur 16 bzw. 32 Farben darstellen. Die Anzeige kann über einen eingebauten HF-Modulator oder einen Composite-Ausgang auf einen Fernseher ausgegeben werden. Für bessere Monitore steht ein RGB-Ausgang zur Verfügung, was eine wesentlich höhere Bildqualität verspricht.\n\nBei der Übersetzung der komplizierten Adressierung von Koordinaten im Grafikspeicher hilft ein Display Adress Translator (DAT).\n\n\n\nAuch die Tonausgabe (Sound) wurde verbessert. Man spendierte dem C65 gleich zwei SIDs mit je drei, also insgesamt sechs unabhängigen Stimmen. Bei den Prototypen war leider keine Zweikanaltechnik (Stereo) vorgesehen, denn die Signale wurden intern gemixt.\n\nIm Gegensatz zu den meisten bisherigen 8-Bit-Computern von Commodore verfügt der C65 über ein vollständiges DOS, über welches die eingebaute 3,5″-Floppy gesteuert wird. Das Laufwerk ist kompatibel zur VC1581 und deren MFM-Format. Die Disketten haben eine Speicherkapazität von 880 KB. Da dieses Format jedoch bei den damaligen C64-Besitzern nicht sehr verbreitet war, verfügt der C65 auch zusätzlich über den schon bekannten seriellen Port für Commodore-Floppys. Hierüber kann auch eine VC1541, wie sie für den C64 Verwendung findet, am C65 betrieben werden.\n\nDer Programmierer des C65 Betriebssystems Dennis Jarvis verwendete das DOS der alten Commodore-Laufwerke mit IEEE-488-Anschluss als Basis für das DOS des C65. Es kann mit nur zwei Laufwerken umgehen, das interne mitgezählt. Der F011-Controller für Laufwerke kann bis zu sieben externe Laufwerke ansprechen, die dazu je einen IC namens F016 (CSG 4101) enthalten müssen. Da die geplante externe 1565, ein Zusatzlaufwerk für den F011 „Fast Serial“-Port keine weitere Anschlüsse aufweist, ist davon auszugehen, dass der C65 maximal ein externes Laufwerk ansprechen können sollte.\n\nDer C65 verfügt über die üblichen Schnittstellen des C64, zusätzlich existiert ein DMA-Port für die Speichererweiterung. Letztere wird genau wie beim Amiga 500 über eine Klappe im Boden von unten auf die Platine aufgesteckt. Das eingebaute Diskettenlaufwerk ist über einen „Fast Serial“ genannten Port angebunden, serielle Laufwerke von Commodore können über den üblichen IEC-Port angeschlossen werden. Außerdem wurde ein Stecker für ein Genlock vorgesehen. Nur der Port für die Datasette des C64 ist nicht mehr vorhanden, und dem Userport fehlt die 9 Volt Wechselspannung, was zu Problemen mit einigen Erweiterungen führen könnte. Der Expansionsport ist 50-polig und identisch mit jenem des C16. Ein spezieller Adapter, das sogenannte „Widget“-Board, stellte den üblichen 44-poligen C64-Expansion-Port zu Verfügung, womit einfache Spiele und Programm-Module abgespielt werden konnten. Spezielle Erweiterungen wie Freezer waren allerdings nicht lauffähig.\n\nEin Handbuch existiert nicht, lediglich einige Unterlagen aus der Entwicklung mit technischen Beschreibungen („C65 Technical Specification“).\n\nDer Preis sollte zur Markteinführung etwa zwischen dem des C64 (≈300 DM/≈150 Euro) und dem des Amiga 500 (≈1000 DM/≈500 Euro) liegen. Die Prototypen wurden dann für um die 600 DM (≈300 Euro) abverkauft. Es gibt allerdings kaum Software, die direkt für den C65 geschrieben worden war und die Kompatibilität zum C64 lag bei ca. 60–70 %.\n\nObwohl es sich hier um veraltete Prototypen mit einem keinesfalls kompletten Funktionsumfang handelt, werden heute hohe und stetig steigende Sammlerpreise für einen C65 gezahlt: Während im Dezember 2009 ein funktionsbereiter C65 im Online-Auktionshaus eBay noch einen Verkaufspreis von 6.060 € erzielte, wurde ein Gerät mit fehlenden Bausteinen im Oktober 2011 über dieselbe Plattform für über 20.100 USD verkauft. Im April 2013 wurde auf eBay ein C65 für 17.827 € verkauft. Die meisten Eigentümer eines C65 sind Sammler und viele sogar namentlich in der Szene bekannt. Am 15. Februar 2015 wurde wieder ein C65 auf eBay verkauft für 20.050 € und am 18. Oktober 2015 erreichte ein leicht zerkratztes Exemplar sogar 22.827 €. Ein funktionierendes Exemplar mit der Seriennummer 000004 wurde am 6. November 2016 auf eBay für 15.605 € verkauft. Die Auktion eines weiteren voll funktionsfähigen C65 Prototypen mit Speichererweiterung(Seriennummer 000016), endete am 8. November 2017 mit einem neuen Höchstgebot von 81.450 Euro. Die in der Auktion angegebene Seriennummer weist darauf hin, dass es sich laut Liste um Enno Coners C65 handelt.\n\nDer C65 kann vom MESS-Emulator und Hi65 emuliert werden.\n\n\n"}
{"id": "100094", "url": "https://de.wikipedia.org/wiki?curid=100094", "title": "TT-100", "text": "TT-100\n\nDer TT-100 war der erste Versuch innerhalb der Bioelektronik, einen Computer auf der Basis von DNA, also dem genetischen Material der Lebewesen, zu bauen. Er wurde 1994 von Leonard Adleman konstruiert, um die Speicher- und Verarbeitungsmöglichkeiten der DNA zu demonstrieren. Dabei wurde das zu lösende Problem als speziell sequenzierte DNA codiert. Die Lösung war dann das in freier Reaktion synthetisierte Molekül.<ref name=\"Spahl/Deichmann\">Thilo Spahl, Thomas Deichmann: \"Das populäre Lexikon der Gentechnik.\" Eichborn-Verlag, Frankfurt/Main 2001; S. 38. ISBN 3-8218-1697-X</ref>\n\nDer DNA-Computer bestand aus einem Reagenzglas, in dem 100 Mikroliter einer DNA-gesättigten Flüssigkeit enthalten war. Aus diesem Grund nannte er ihn TT-100 (Testtube mit 100 Mikrolitern). Mit Hilfe dieser Erfindung löste er eine einfache Version des Hamiltonschen Wegeproblems.\n"}
{"id": "100957", "url": "https://de.wikipedia.org/wiki?curid=100957", "title": "Farbkanal", "text": "Farbkanal\n\nDie Farbkanäle sind die Grundfarben des Farbraums, der zur Speicherung eines digitalen Farbbildes verwendet wird, oder die Wellenlängen, in denen das Bild aufgenommen wurde.\nDer Farbkanal bietet Informationen über die im Bild enthaltenen Farbkomponenten. Die Anzahl der Farbkanäle in einem Bild ist abhängig vom Modus, das heißt pro Farbauszug gibt es einen Kanal. Daher haben CMYK-Bilder vier Farbkanäle, RGB-Farbbilder nur drei.\n\nDurch passende Verschneidung oder Intensitätsänderung einzelner Farbkanäle können Digitalbilder manipuliert, verbessert und im Kontrast den Erfordernissen angepasst werden.\n\nFür Umweltschutz oder die Fernerkundung der Oberfläche von Erde oder Planeten ist der Spektralbereich von Infrarot wichtig.\nMultispektralscanner von Satelliten nehmen die Erdoberfläche in fünf oder mehr Farbkanälen auf, von denen mindestens zwei im Infrarot-Bereich liegen. Sie dienen dazu den Gesundheitszustand des Pflanzenwuchses einzuschätzen, aber auch dazu Gestein zu beurteilen und für geologische Untersuchungen. \n"}
{"id": "101537", "url": "https://de.wikipedia.org/wiki?curid=101537", "title": "Computerschach", "text": "Computerschach\n\nComputerschach bzw. Rechnerschach bezeichnet das Spielen von Schach gegen einen Computer, das Spielen von Computern untereinander, die Entwicklung von schachspielenden Maschinen (Schachcomputer) sowie die Entwicklung von Schachprogrammen.\n\nDie Idee, eine schachspielende Maschine zu erschaffen, reicht in das 18. Jahrhundert zurück. Weiteres zur Geschichte siehe unter Schachcomputer, zur Funktionsweise unter Schachprogramm.\n\nDie Hauptziele des Computerschachs waren Unterhaltung, Schachanalyse und die Hoffnung auf Einsichten in das menschliche Denken. Seit Mitte der 1960er Jahre wurde Computerschach oft als „Drosophila der künstlichen Intelligenz“ bezeichnet. Während die ersten beiden Ziele innerhalb von 50 Jahren mit Bravour erreicht wurden, wurde die Hoffnung auf Einsichten in das menschliche Denken enttäuscht. Alle Forschungen in diese Richtung (z. B. von Michail Moissejewitsch Botwinnik oder Allen Newell) waren nie von Erfolg gekrönt.\n\nAus diesem Grund ist Computerschach (ebenso wie Scrabble) heutzutage kein Forschungsgegenstand mehr, und wurde größtenteils ersetzt durch Spiele wie Go oder Arimaa, da bei diesen Computerprogramme weniger durch reine Rechenleistung als durch eine wesentlich komplexere Bewertungsfunktion Erfolge erzielen können, und beide Spiele trotzdem relativ leicht von Menschen erlernt und erfolgreich gespielt werden können.\n\nStattdessen hat die anhaltende Miniaturisierung und andauernde Verdopplung der Rechengeschwindigkeit von Computern (Mooresches Gesetz) dem Lager der Brute-Force-Verfechter des Schachs in die Hände gespielt: Schachcomputer für den Hausgebrauch sind heutzutage zu vernachlässigbaren Kosten zu erstehen und es gibt eine Reihe von Schachprogrammen (Open-Source- und Freeware-Programme wie Fruit, Amy, Pepito, Crafty und weitere), die auf handelsüblichen PCs Großmeistern ebenbürtig sind. Spitzenprogramme wie Shredder, Junior oder Fritz schlagen inzwischen sogar die Weltspitze in Turnierbedenkzeiten regelmäßig.\n\nOffen ist, ob die Rechner das Schachspiel in absehbarer Zeit uninteressant machen, da ihre Spielstärke ständig steigt. Allerdings wird argumentiert, dass selbst bei immer besser werdenden Computerprogrammen das Schachspiel interessant bliebe – schließlich würden sich Menschen auch noch im Sprint oder Marathonlauf messen, obwohl jedes motorisierte Gefährt schneller sei. Eine Möglichkeit ist, bei Showkämpfen jeweils nur so starke Hardware einzusetzen, dass die Software mit dem Gegner in etwa gleichauf liegt. Während Deep Blue 1997 auf spezialisierter Hardware bereits ca. 200 Millionen Stellungen pro Sekunde analysieren konnte, vermochte Deep Fritz 2006 auf handelsüblicher Hardware lediglich rund acht bis zehn Millionen Stellungen pro Sekunde zu analysieren. Der Geschwindigkeitsnachteil der Hardware wurde jedoch durch verbesserte Sortier-, Such- und Bewertungsalgorithmen der Software kompensiert.\n\nWährend Menschen längerfristige Pläne entwerfen können, dabei gelegentlich aber kurzfristige Drohungen übersehen, nutzen Computer jeden kleinsten taktischen Fehler aus. Die Programmierer versuchen, ihren Programmen auch immer bessere strategische „Kenntnisse“ beizubringen. Dabei gibt es jedoch insbesondere Probleme dabei, wie eine Position zu bewerten ist. Ein Schachprogramm probiert grob gesagt jeden möglichen Zug (und alle darauf möglichen bis zu einer bestimmten Tiefe) aus und bewertet die entstehenden Stellungen mittels einer Bewertungsfunktion. Viele Positionen sind aber nur schwer mit einer Zahl zu bewerten. Oft haben Merkmale wie Bauernstruktur, offene Linien etc. für beide Seiten Vor- und Nachteile. Menschen, denen die Rechenkraft eines Computers fehlt, können nicht jeden Zug im Kopf durchspielen und die sich ergebenden Stellungen betrachten. Vielmehr ergibt sich im Laufe der Zeit ein Gefühl (Intuition) dafür, welcher Zug in welcher Stellung einen Vorteil ergeben könnte. Diese Züge werden dann genauer betrachtet.\n\nDeutlich überlegen sind Computer dem Menschen bei taktischen Manövern, die innerhalb ihrer Rechentiefe abgeschlossen werden können. Besonders gefährlich ist dabei die Dame, sodass menschliche Spieler oft versuchen, den Computer zu einem Damentausch zu bewegen. Es liegt in der Natur der Sache, dass derartige „Tricks“ – einmal erkannt – von den Programmierern in Nachfolgeversionen bei der Programmierung berücksichtigt werden.\n\nStrategisch muss ein Mensch gegen einen Computer mit langfristig angelegten Manövern operieren, deren Ansatz für den Computer im Rahmen seiner Rechentiefe zunächst nicht erkennbar ist. Kramnik hatte gegen Deep Fritz z. B. Erfolg mit einem langfristig angelegten möglichen Durchmarsch eines Freibauern, der – zunächst noch nicht weit gezogen – von Deep Fritz erst zu spät als ernste Bedrohung erkannt wurde. Somit bestraft der Computer kombinatorische Strategien und erzwingt eine positionelle Spielanlage.\n\nEine weitere Strategie besteht darin, zu Beginn einen unüblichen Zug zu spielen, und den Computer aus seinem Eröffnungsrepertoire herauszudrängen. So muss er damit beginnen, die günstigen Spielzüge zeitintensiv zu berechnen anstelle sie in einer Tabelle nachzuschauen. Für den menschlichen Spieler birgt dies allerdings auch Risiken.\n\nIm Jahre 1968 wettete der schottische Internationale Meister David Levy mit mehreren Informatikern um 1250 englische Pfund, dass es innerhalb der folgenden zehn Jahre kein Computerprogramm schaffen würde, ihn in einem Wettkampf zu besiegen. Im August 1978 kam es in Toronto zum Match gegen das damals beste Programm \"Chess 4.7,\" das Levy mit 3,5:1,5 gewinnen konnte. 1979 spielte er gegen eine verbesserte Version dieses Programms eine Schaupartie, die im ZDF übertragen wurde und mit einem Remis endete. Es kam zu einer zweiten Wette, die nochmals über zehn Jahre lief. Im Jahre 1988 war Levy dann aber gegen das Programm \"Deep Thought\" völlig chancenlos und verlor mit 0:4.\n\nZwischen 1986 und 1997 fanden in Den Haag jährlich Turniere zwischen Schachcomputern und menschlichen Spielern statt, die von der Versicherungsgesellschaft AEGON finanziert wurden. In den ersten Jahren spielten ausschließlich niederländische Amateure gegen die Computer, später wurden auch bekannte Großmeister wie David Bronstein, Jeroen Piket, Vlastimil Hort, John Nunn, Larry Christiansen und Yasser Seirawan eingeladen, um den immer besser werdenden Computern Paroli bieten zu können. Obwohl in allen zwölf Turnieren ein Mensch die Einzelwertung gewann, siegten die Computer 1993 erstmals in der Gesamtwertung.\n\nSeit den 1990er Jahren wurden Schachcomputer auch für Spieler der Weltelite zu ernstzunehmenden Kontrahenten, zunächst aber nur im Blitz- und Schnellschach. Am 31. August 1994 kam es zu einer Sensation, als der Weltmeister Garri Kasparow bei einem Schnellturnier in London gegen das auf einem Pentium laufende Programm \"Chess Genius\" mit 0,5:1,5 verlor.\n\nDie speziell entwickelte Schachmaschine Deep Blue von IBM schlug Kasparow 1997 in einem medienwirksamen Wettkampf über sechs Partien mit langer Bedenkzeit. Da diese Version von Deep Blue allerdings öffentlich nur insgesamt diese sechs Partien gespielt hat, ist über die erreichte Spielstärke nicht viel bekannt. Nach dem Wettkampf äußerte Kasparow den Verdacht, der Sieg der Maschine in der zweiten Wettkampfpartie sei mit menschlicher Hilfe zustande gekommen. Anhaltspunkte, die diese Behauptung stützen würden, wurden nicht gefunden.\n\nIm Jahr 2002 und 2003 hielten neuere Programme remis in Schau-Wettkämpfen gegen zwei der weltbesten Großmeister (Brains in Bahrain 2002 „Deep Fritz“ gegen Wladimir Kramnik, 2003 „Junior“ und wiederum „Deep Fritz“ gegen Kasparow).\n\nIm Jahr 2005 trat der Großmeister Michael Adams (im Juli 2005 die Nummer 7 der Schachweltrangliste) ein Match gegen den Computer Hydra an. Die Rechenleistung des Programms betrug ca. 200 Millionen Stellungen in der Sekunde. Der Rechner entschied das Match mit fünf Siegen und einem Remis klar für sich. Im Turnier bleibt die Maschine von Menschen bisher ungeschlagen. Im Fernschach musste Hydra gegen Fernschach-Großmeister Arno Nickel jedoch schon zwei Niederlagen einstecken.\n\nVom 25. November bis 5. Dezember 2006 spielte Wladimir Kramnik gegen Deep Fritz 10 in Bonn einen Wettkampf über sechs Partien. Dabei galten einige für den Menschen vorteilhafte Bedingungen: Kramnik erhielt vorab die im Wettkampf eingesetzte Programmversion, um sich mit ihrer Spielweise vertraut zu machen. Während der Partien bekam er die im Eröffnungsbuch des Programms gespeicherten Züge angezeigt. Nach 56 Zügen hätte er das Recht gehabt, eine Hängepartie zu beantragen, außerdem hätte er in Stellungen, die von der Endspieldatenbank entsprechend bewertet werden, ein Remis reklamieren können. Dazu kam es im Wettkampf allerdings nicht. Deep Fritz siegte mit 4:2 (2 Siege, 4 Remis).\n\nIn den Jahren 2014 und 2016 versuchte noch einmal der amerikanische Großmeister Hikaru Nakamura (beste Elo-Zahl 2816 im Oktober 2015) vergeblich, „die Ehre der Menschheit“ zu retten. Er unterlag – trotz nicht unerheblicher Vorgaben – 2014 der Schachengine „Stockfish“ mit 1:3 und 2016 der Engine „Komodo“ mit 1,5:2,5 ohne Partiegewinn. Sam Copeland verglich den „tapferen“, aber vergeblichen Widerstand gegen „Komodo“ mit den historischen Schlachten von Alamo und bei den Thermopylen.\n\nNach der Niederlage Kasparows begannen auch die besten menschlichen Spieler systematisch mit der Unterstützung von Schachcomputern zu trainieren. Insbesondere wurde auch versucht, spezielle Strategien für Wettkämpfe gegen Computer zu entwickeln. Heutzutage sind Menschen schon Schachprogrammen auf einem Handy unterlegen. Erschwerend kommt hinzu, dass Computer während der gesamten Partie mit jedem Zug ihr bestes Schach spielen, da sie anders als Menschen nicht ermüden und ihnen keine offensichtlichen Fehlzüge unterlaufen.\n\nEs gab und gibt eine Reihe von Wettkämpfen und Turnieren zwischen Schachcomputern. International bedeutend waren die Nordamerikanische Computerschachmeisterschaft (NACCC) und die Mikrocomputer-Schachweltmeisterschaft (WMCCC). Das wichtigste Computerschachturnier ist die inzwischen jährlich stattfindende Schachcomputerweltmeisterschaft (WCCC). Anfang Juli 2017 fand die 23. WCCC in der niederländischen Universität Leiden statt. Weltmeister wurde wie im Vorjahr \"Komodo.\"\n\nSchachspielende Computer sind auch immer wieder Motive in Filmen, zum Beispiel in \"\".\nIn der Serie \"Raumschiff Enterprise\" bemerkt der erste Offizier (Mr. Spock) des Raumschiffes eine Fehlfunktion des Computers, als dieser im Schach gegen ihn verliert und nicht, wie zu erwarten gewesen wäre, ein Unentschieden gegen den Vulkanier erreicht. In einem der \"Star-Trek\"-Kinofilme \"(Star Trek IV)\" gehört ein durch einen Computer gestelltes Schachproblem im 3D-Schach zu den Aufgaben, mittels deren die mentalen Fähigkeiten Spocks nach einem Unfall überprüft werden.\n\n\n\n"}
{"id": "101617", "url": "https://de.wikipedia.org/wiki?curid=101617", "title": "GNU-C-Bibliothek", "text": "GNU-C-Bibliothek\n\nglibc, die GNU C-Bibliothek, ist eine freie Implementierung der C-Standard-Bibliothek, die vom GNU-Projekt zusammen mit der GNU Compiler Collection entwickelt wird.\n\nDie glibc steht unter der LGPL, was den Einsatz der Bibliothek bei nicht freier Software ermöglicht. Die glibc-Bibliothek gehört zu den fundamentalsten und wichtigsten Bibliotheken von unixoiden Betriebssystemen.\n\nEines der Designziele der glibc ist Portabilität über verschiedene Softwareplattformen, daher ist sie auch für eine Reihe von Betriebssystemen verfügbar. Einige Betriebssysteme, darunter GNU/Linux, benutzen die glibc als ihre offizielle Standard-C-Bibliothek. Die Bibliotheken der glibc sind selbst zum größten Teil auch in C geschrieben, laufzeitkritische Routinen verwenden jedoch Assembler-Code.\n\nDie glibc stellt die in der Single UNIX Specification, POSIX (1c, 1d, und 1j) geforderte Funktionalität bereit, zusätzlich Teile der ISO C99, Berkeley Unix (BSD) Interface, der \"System V Interface Definition (SVID)\" und der \"X/Open Portability Guide (XPG)\", Issue 4.2, mit allen Erweiterungen üblich für XSI-(X/Open System Interface)-konforme Systeme mit allen X/Open-Unix-Erweiterungen.\n\nZusätzlich zu den von den C-Standards geforderten Funktionen bietet sie auch eine Reihe von (nicht standardisierten) Erweiterungen.\n\nIhre Universalität und ihr gleichzeitiger Fokus auf die x86-Hardware-Plattform ist gleichzeitig aber auch der größte Kritikpunkt an der glibc. Durch die Menge des einzubindenden Codes werden gegen die glibc gelinkte Programme unnötig groß und damit potenziell langsam, andere Plattformen werden gar nicht unterstützt. Eine Reihe von Projekten hat sich daher der Idee verschrieben, Alternativen zu glibc zu entwickeln, die bekanntesten sind uClibc und diet libc. Durch Beschränkung auf die – aus Sicht der Kritiker – „wesentlichen Dinge“ sind diese Implementierungen deutlich kleiner für die fertigen Binärprogramme, allerdings lässt sich nicht jedes glibc-Programm auch gegen diese alternativen Bibliotheken linken (z. B. weil sie Funktionen der glibc benutzen, die in den anderen Bibliotheken fehlen), oder es verhält sich während der Ausführung unerwartet. Vor allem für eingebettete Systeme sind die schlanken libc-Implementierungen jedoch sinnvoll.\n\nSeit 2001 wurde das CVS-Repository der glibc bei Red Hat gehostet und fast ausschließlich von Ulrich Drepper gepflegt (Maintainer). Zusätzlich wurden aktuelle Snapshots in den FTP-Archiven und deren Spiegelserver bereitgestellt. Damit kam man der Community entgegen, da man z. B. durch restriktive Firewalls nicht von überall aus per CVS auf das Internet zugreifen kann.\n\nUm das Jahr 2001 wurde ein Lenkungsausschuss für das glibc-Projekt eingerichtet, um welches es öffentlich ausgetragene Kontroversen gab. Ulrich Drepper beschrieb die Vorgänge öffentlich als Versuch einer \"feindlichen Übernahme\" (engl. \"hostile takeover\") durch Richard Stallman, welche fehlgeschlagen war.\n\nSeit Mai 2009 wird die glibc als Git-Repository bei \"Sourceware\" weiter gepflegt.\n\nMit der glibc 2.3 wurde eine Reihe von Verbesserungen integriert, die wichtigste davon ist die Ersetzung der alten Linux-Threading-Erweiterung \"linuxthreads\" durch die Native POSIX Thread Library (NPTL), die ebenso wie die glibc selbst federführend bei Red Hat entwickelt wurde. Die NPTL ermöglicht in Zusammenarbeit ab dem Linux-Kernel 2.6 eine deutliche Leistungssteigerung beim Threading und ist dabei POSIX-konform. Da man abwärtskompatibel sein wollte, steht für Programme, die auf nicht POSIX-konforme Verhaltensweisen der alten Implementation angewiesen sind, auch weiter \"LinuxThreads\" zur Verfügung, man muss es nun aber explizit per Linker-Direktive anfordern (z. B. codice_1). Auch die glibc selbst ist in den wichtigsten Funktionen abwärtskompatibel. Der kleinste gemeinsame Nenner ist dabei die Funktionalität der \"libc6\", weshalb die Bezeichnungen glibc und libc6 auch häufig synonym füreinander verwendet werden (auf Alpha- und IA-64-Architekturen heißen die Bibliotheken aus historischen Gründen libc6.1, bieten jedoch die gleiche Funktionalität).\n\nWegen eines fehlenden Fokus der glibc auf Kompatibilität mit eingebetteten Systemen, besonders ARM-Prozessoren, und Problemen mit dem Umgang des Projektverantwortlichen, Ulrich Drepper, bei Fehlerberichten und eingereichten Korrekturen wurde eine Abspaltung () des Projekts namens EGLIBC erstellt. Nach Selbsteinschätzung der Entwickler handelt es sich bei eglibc jedoch nicht um einen klassischen Fork, vielmehr wollen die Entwickler die Änderungen von glibc übernehmen, aber auch Patches akzeptieren, die keinen Einzug in glibc gefunden haben. Damit verfolgt eglibc das Ziel, einen freundlicheren Umgang mit Entwicklern zu pflegen und Embedded-Prozessoren besser zu unterstützen. Als erste große Linux-Distribution hat Debian auf diese Implementierung umgestellt, wechselte aber im Juni 2014 wieder zurück zu glibc, da das EGLIBC-Projekt seine Mission als erfüllt ansah und sich auflöste. Ubuntu verwendet ab Version 9.10 EGLIBC.\n\nDie Veröffentlichungsdaten wurden, so weit möglich, vom offiziellen FTP-Server übernommen.\n\n\n"}
{"id": "102641", "url": "https://de.wikipedia.org/wiki?curid=102641", "title": "Framebuffer", "text": "Framebuffer\n\nDer Framebuffer oder Bildspeicher (engl. \"frame\" – Einzelbild, \"buffer\" – Zwischenspeicher) ist Teil des Video-RAM von Computern und entspricht einer digitalen Kopie des Monitorbildes. Das heißt, jedem Bildschirmpixel kann genau ein bestimmter Bereich des Framebuffers zugewiesen werden, der dessen digital übersetzten Farbwert enthält. Seit den 1990er-Jahren befindet sich der Framebuffer vorwiegend auf der Grafikkarte.\n\nDie Größe des Framebuffers ist abhängig von zwei Faktoren: der verwendeten Farbtiefe (genauer: Pixelformat) und der verwendeten Bildauflösung.\n\nDie Farbtiefe des Framebuffers definiert die Maximalzahl der gleichzeitig auf dem Bildschirm angezeigten Farben oder Farbnuancen. Im Bereich der IBM-PC-kompatiblen Computer waren und sind die in der folgenden Aufstellung angegebenen Größen üblich. Die angegebenen Pixelformate geben an, wie viele Bits pro Pixel auf die einzelnen Farbkanäle (rot, grün, blau, Alphakanal) vergeben werden – bei Farbmodi, die indizierte Farben (Paletten) benutzen, fehlt diese Angabe, weil sie keinen Sinn ergibt.\nBei Grafikhardware, die mit Bitplanes arbeitet (z. B. Amiga), sind bei indizierten Farben auch 3, 5, 6 und 7 Bit pro Pixel mit dementsprechend 8, 32, 64 bzw. 128 Farben üblich.\n\nIn der 3D-Computergrafik werden auch Framebuffer mit höherer Genauigkeit benutzt. Dort benötigt die Bestimmung der Farbe eines Pixels oftmals mehrere Rechenschritte, wobei bei jedem Zwischenergebnis Rundungsfehler entstehen können, die bei herkömmlichen Framebufferformaten schnell sichtbar sind und störend wirken.\n\nBei diesen genaueren Formaten interpretiert man die Farbkanalwerte als Kommawerte auf einer Skala von 0.0 bis 1.0, damit bei der Verwendung mehrerer Pixelformate die Handhabung vereinfacht wird.\n\nDie Bildauflösung gibt an, aus wie vielen Pixeln der Framebuffer besteht. Üblicherweise gibt man hierbei die horizontale und vertikale Pixelanzahl an, wodurch man auch das Seitenverhältnis direkt berechnen kann, üblich sind hier 4:3, 5:4 und 16:10.\n\nTypische Framebuffer-Auflösung:\n\n\nIn der Übersicht wurde im Fall von TrueColor berücksichtigt, dass Daten intern mit 24 Bit gespeichert werden.\n\nDurch Unzulänglichkeiten in der Kontinuität der Bildfolge, und um die allgemeine Darstellungsqualität weiter zu erhöhen, wurde das Konzept des Framebuffers im Laufe der Zeit überarbeitet. So entspricht ein Framebuffer auf aktuellen Systemen mehreren Pufferspeichern.\n\nDas \"Linux Framebuffer Device\" (kurz \"fbdev\") ist eine hardwareunabhängige Abstraktionsschicht unter Linux, um Grafiken auf der Konsole bzw. mit X-Window (xf86_fbdev) anzuzeigen. Dabei setzt das Framebuffer-Device nicht auf systemspezifischen Bibliotheken wie der SVGALib oder dem X Window System auf und ist somit eine ressourcensparende Alternative zum weiterverbreiteten X-Server, auf dem heute die meisten grafischen Oberflächen für Linux aufbauen. Es ist seit der Linux-Kernelversion 2.1.107 für alle Plattformen im Standardkernel enthalten.\n\nUrsprünglich wurde es für Linux/m68k implementiert, um auf entsprechenden Systemen (Amiga, Atari, Macintosh) mit einer geringen Hardwarebeschleunigung einen Textmodus zu emulieren und wurde erst später auf die IBM-PC-kompatible Plattform erweitert.\n\nHeutzutage kann der Framebuffer direkt von verschiedenen Programmen wie MPlayer und Bibliotheken wie GGI, SDL, GTK+ und Qt Extended benutzt werden. Das ressourcensparende Konzept macht den Einsatz besonders für eingebettete Systeme interessant.\n\nInsbesondere wird es von verschiedenen Distributionen (Ubuntu, openSUSE) verwendet, um schon während des Bootstrappings in Form eines Splash Screens eine grafische Ausgabe zu ermöglichen.\n\nDer am häufigsten verwendete VESA-Framebuffer-Treiber (vesafb) baut auf einheitlichen Spezifikationen von Videostandards auf und erlaubt so einen Zugriff auf Grafikkarten größtenteils unabhängig vom Hersteller. Dadurch ist dann auch eine quelloffene Implementation möglich. Außerdem wurden von diversen Grafikchipherstellern (Nvidia: rivafb, nvidiafb; AMD: radeonfb) proprietäre Treiber auf den Markt gebracht.\n\nBekannt wurde das Framebuffer-Device durch die Möglichkeit, während des Linux-Kernel-Ladevorgangs dem Benutzer ein Tux-Logo anzuzeigen. Dazu muss es aber zunächst im Kernel enthalten sein und beim nächsten Reboot durch den Boot-Loader, der auch das Betriebssystem in den Arbeitsspeicher lädt, durch die Angabe des Parameters codice_1 aktiviert werden.\n\nIm Folgenden werden zwei Beispiele gezeigt, in denen ein AMD-Treiber mit einer Bildauflösung von 1024×768 Bildpunkten bei einer Farbtiefe von 8 Bit pro Pixel und einer Bildwiederholungsfrequenz von 76 Hz geladen wird:\n\n # LILO configuration file\n\n # GRUB configuration file\n\nFür einen Hardwarezugriff auf das Framebuffer-Device muss nicht unbedingt ein Kernelmodul geschrieben werden. Ferner hat die Anwendung die Möglichkeit im User-Mode über die Gerätedatei codice_2 auf das Device zuzugreifen und in den Grafikspeicher zu schreiben. In folgendem Beispiel wird demonstriert, wie mit der Programmiersprache C linear in den Framebuffer geschrieben werden kann. Hier wird der hexadezimale Wert 0x000000FF (Binär: 0b00000000000000000000000011111111) für jedes Pixel gesetzt:\n\n\nint main(int argc, char **argv) {\n\n"}
{"id": "102839", "url": "https://de.wikipedia.org/wiki?curid=102839", "title": "Scriptkiddie", "text": "Scriptkiddie\n\nEin Scriptkiddie (von „script“ und „kid“, manchmal auch als Skiddie oder Scriddie abgekürzt) ist ein Stereotyp, das sich alltagssprachlich auf Personen aus dem Bereich der Computersicherheit bezieht. Der Begriff beschreibt vornehmlich jugendliche Computernutzer, die trotz mangelnder Grundlagenkenntnisse versuchen, in fremde Computersysteme einzudringen oder sonstigen Schaden anzurichten. Erfolgreiche Versuche sind dabei der Anwendung gebrauchsfertiger Lösungen geschuldet, also der Nutzung vorgefertigter Automatismen oder schriftlicher Anleitungen. Die Bezeichnung „Scriptkiddie“ hat Anklänge von unreifem Verhalten und Vandalismus und wird oft abwertend verwendet.\n\nDaneben besteht eine weitere Verwendung im Bereich der Programmierung. Dort nimmt das Wort Bezug auf eine Person, die fremden Quellcode für eigene Projekte zusammenkopiert, um deren Effekte zu nutzen, ohne jedoch den Code zu verstehen.\n\nEin \"Skript\" ist eine Textdatei, die eine Folge von Befehlen enthält, die ein Computer selbständig ausführen kann. Wenn der Computer angewiesen wird, ein solches Skript abzuarbeiten, führt dies zum Aufruf verschiedener Programme gemäß einem Ablaufplan, der darin beschrieben ist.\n\nScriptkiddie ist also ein Sinnbild für einen Jugendlichen, der lediglich mit Hilfe von vorgefertigten Skripten über ein Netzwerk in fremde Computer einbricht oder diesem durch absichtlich verbreitete Viren, Würmer oder Trojaner Schaden zufügt, ohne nachhaltige Kenntnisse aus dem Bereich der Computersicherheit zu haben. Dazu gehört das Klischee, dass es aus der Motivation heraus geschieht, anderen zu imponieren.\n\nEin Scriptkiddie ist abzugrenzen von einem Hacker: Ein Hacker besitzt tiefe Grundlagenkenntnis, ein Scriptkiddie nicht. Innerhalb des Boulevardjournalismus und der Politik werden beide Ausdrücke gewöhnlich nicht unterschieden.\n\n\n\n"}
{"id": "103203", "url": "https://de.wikipedia.org/wiki?curid=103203", "title": "Virtual Address eXtension", "text": "Virtual Address eXtension\n\nDie VAX (\"Virtual Address eXtension\") ist eine Rechnerarchitektur der Digital Equipment Corporation.\n\nDie erste VAX mit der Typenbezeichnung 11/780 kam im Oktober 1977 auf den Markt. Im Februar 1978 wurde ein spezielles Betriebssystem für die VAX mit der Bezeichnung \"VMS\" (Virtual Memory System) fertiggestellt, dessen Entwicklung gleichzeitig begonnen worden war. VAX-Rechner wurden bis zum Jahr 2000 verkauft.\nHeute kommt diese Rechnerarchitektur noch im Militärbereich vor, zum Beispiel bei den Kampfflugzeugen F-15 und F/A-18 des Herstellers McDonnell Douglas (heute Boeing) oder im Minuteman Interkontinentalraketensystem.\n\nHauptarchitekt war William D. Strecker, ein ehemaliger Doktorand von Gordon Bell.\n\nEines der Hauptziele bei der Spezifikation der VAX-Architektur war es, den 16-Bit-Adressraum des Vorgängers PDP-11 auf 32 Bit zu erweitern, also eine Erweiterung von direkt adressierbaren 64 kB auf – für damalige Verhältnisse zukunftssichere – 4 GB. Ursprünglich sollte die VAX-Architektur lediglich eine modifizierte PDP-11-Architektur mit Hardwareerweiterung zur Unterstützung von virtueller Speicherverwaltung sein, daher die Bezeichnung VAX (Erweiterung auf virtuelle Adressen). Im Laufe der Erstentwicklung wurde jedoch entschieden, eine neue Architektur zu schaffen, die im Vergleich zu PDP-11 inkompatible, aber auch zusätzliche Instruktionen, Datentypen und weitere Adressierungsmodi bietet.\n\nBei dem Design von VMS wurde auf Quellcode-Kompatibilität zu älteren Betriebssystemen geachtet, um bestehende Programme und Daten mit wenig Aufwand auf das neue System umsetzen zu können. Optional besaßen einige VAXen einen \"binary compatibility mode\", in dem PDP-11-Programme direkt ausgeführt werden konnten. Da VMS als besonders stabiles und ausgereiftes Betriebssystem gilt, hat es vor allem im Finanzbereich und in der Luftraumüberwachung (zivil und militärisch) eine sehr große Verbreitung gefunden und wird immer noch als OpenVMS (auch für andere Hardwareplattformen) weiterentwickelt. Der aktuelle Lizenzinhaber HP (Hewlett Packard Enterprise) hat am 10. Juni 2013 mitgeteilt, dass er OpenVMS noch bis Ende 2020 unterstützen wird. Im Juli 2014 kündigten HP und \"VMS Software, Inc.\" (VSI) an, dass VSI die Weiterentwicklung von OpenVMS in Lizenz betreiben wird.\n\nDie 32-Bit-VAX-Hauptprozessoren beruhten auf einem CISC-Befehlssatz, der aufgrund des gemeinsamen PDP-11-Vorbildes dem Befehlssatz der m68k-Prozessoren der Firma Motorola ähnelt, jedoch zusätzliche Befehle zur Betriebssystemunterstützung, etwa zur Warteschlangenverwaltung, bot. Die Prozessorfamilie wurde von dem ebenfalls von DEC entwickelten Alpha-Prozessor, einem 64-Bit-RISC-Prozessor abgelöst.\n\nVAX-Computer waren nach den PDP-Rechnern die ersten Rechner, auf die das Betriebssystem Unix portiert wurde. Digital Equipment bot ein eigenes UNIX-Derivat namens ULTRIX neben dem firmeneigenen VMS als Betriebssystem an. Mittlerweile wurden auch die Open-Source-Betriebssysteme NetBSD, OpenBSD und Linux auf VAX-Computer portiert.\n\nUrsprünglich wurden die Rechner aus Standardbauteilen, etwa Bit-Slices und TTL-Logikschaltkreisen, aufgebaut. Später entwickelte Digital eigene VAX-Mikroprozessoren, die sogenannten MicroVAX-Prozessoren wie den MicroVAX 78032 oder CVAX, die jedoch nicht auf dem freien Markt verkauft wurden. Sie waren nur zusammen mit Rechnern der MicroVAX-Familie erhältlich.\n\nDie Software \"SIMH\" kann ein komplettes MicroVAX-3900-System emulieren.\n"}
{"id": "103605", "url": "https://de.wikipedia.org/wiki?curid=103605", "title": "Macintosh SE/30", "text": "Macintosh SE/30\n\nDer Macintosh SE/30 war ein Rechnermodell der Firma Apple. Er wurde im Januar 1989 eingeführt und war bis Oktober 1991 im Programm. Trotz der recht kurzen Produktionszeit verkaufte sich der SE/30 gut.\n\nDer Macintosh SE/30 benutzt als Prozessor einen Motorola 68030 mit 16 MHz und einen Motorola 68882 als FPU-Coprozessor. Das Gehäuse basiert auf dem des Macintosh SE mit eingebautem 9-Zoll-Bildschirm (512 × 342 Pixel, monochrom). Die Hauptplatine war jedoch eine Neuentwicklung und bietet mit acht 30pol SIMM-Sockeln für bis zu 128 Mbyte RAM und den genannten Prozessoren für die Zeit eine beachtliche Leistung.\n\nSchnittstellen zur Außenwelt sind SCSI (für externe Festplatten, CD-ROMs, Scanner, Drucker, Wechselplattenlaufwerke wie das SyQuest SQ555, Belichter und später auch CD-Brenner); außerdem zwei serielle Schnittstellen (für Modems, LocalTalk, serielle Apple-Drucker wie den ImageWriter oder StyleWriter), ADB für Eingabegeräte, ein HD-Diskettenlaufwerk sowie ein Kopfhöreranschluss (8-Bit, Stereo, 22 kHz PCM, vierstimmig). In Mono können Klänge ersatzweise auch über den eingebauten Lautsprecher abgespielt werden.\n\nDie Platine des SE/30 ist praktisch eine des Macintosh IIx, nur ohne NuBus-Steckplätze, dafür aber mit einem SE/30-PDS (Processor Direct Slot) für Ethernetkarten, Grafikkarten, CPU-Karten, GPIB-Karten und Ähnliches ausgestattet. Wegen seiner Leistungsfähigkeit war der SE/30 nicht zuletzt im grafischen Gewerbe, aber auch aufgrund der FPU als Numbercruncher im wissenschaftlichen Bereich sehr beliebt. Für diesen Einsatzzweck wurde er häufig mit einer Farb-Grafikkarte erweitert und mit bis zu 21 Zoll großen Monitoren ausgestattet.\n\nAuf dem SE/30 laufen Mac-OS-Systeme von Version 6.0.3 bis 7.5.5 (mit Hacks auch 8.1), A/UX 2.0 und 3.0.1, Minix, NetBSD und Linux.\n\nNachfolger des SE/30 ist der Macintosh Classic II, der zwar ebenfalls einen Motorola 68030/16 besitzt, aber die Busbandbreite wurde aufgrund der Speicheranbindung von 32 Bit auf 16 Bit gekappt, und der Adressraum für RAM beträgt nur noch 10 Mbyte.\n\n\"Siehe auch:\" Macintosh-Modelle\n\n"}
{"id": "103649", "url": "https://de.wikipedia.org/wiki?curid=103649", "title": "Macintosh SE", "text": "Macintosh SE\n\nDer Macintosh SE war ein populäres Rechnermodell der Firma Apple. Er wurde im März 1986 eingeführt und war bis Oktober 1990 im Programm.\n\nObwohl teilweise parallel zum Macintosh Plus produziert, war er als dessen Nachfolger angetreten. Gegenüber diesem gab es ein neues Gehäusedesign, wobei das Konzept des All-in-one-Gehäuses (Rechner, Monitor, Festplatte und Diskettenlaufwerk in einem Gehäuse, externe Tastatur und Maus) beibehalten wurde. Das Gehäuse war in der neuen Farbe Platinum (einem sehr hellen Grauton) ausgeführt, die mit dem Macintosh Plus eingeführt worden war. Durch die optionale interne Festplatte und die Möglichkeit, Erweiterungskarten einzubauen, wurde jedoch ein Lüfter auf der Rückseite zur Kühlung des Gehäuses durch verstärkte Luftumwälzung nötig.\n\nDer Macintosh SE verfügt wie der Plus über einen Motorola 68000 mit 8 MHz, einen 9 Zoll s/w-Bildschirm mit 512×342 Pixeln, besitzt aber gegenüber dem Macintosh Plus einen schnelleren Arbeitsspeicherzugriff (ca. 15 %), einen SE-PDS Erweiterungssteckplatz für monochrome Grafikkarten für Ganzseitenbildschirme, Ethernetkarten, GPIB- oder CPU-Karten sowie einen internen SCSI-Anschluss. Somit war der Macintosh SE für eine interne SCSI-Festplatte vorbereitet, ab Werk wurden typischerweise 20 oder 40 MB ausgeliefert.\nIm Gegensatz zum ENIAC, dem ersten Computer der nur 3 Quadratwurzeln pro Sekunde schaffte, ist der Macintosh SE 24 mal schneller mit 72 Quadratwurzeln pro Sekunde.\n\nDer Arbeitsspeicherausbau erfolgt über vier 30pol SIMM-Sockel und beträgt maximal 4 MB.\n\nAuf dem Macintosh SE laufen Mac-OS-Systeme von Version 2.0 bis 7.5.5 sowie Minix.\n\nNoch 1989 kostete der Mac SE in der Dual-Floppy-Konfiguration 6400 DM, mit Festplatte gar 7600 DM. Die letzten Restposten wurden als Studentenversion für 1200 DM unters Volk gebracht. Nachfolger des SE war der Macintosh Classic, der ebenfalls einen Motorola 68000/8 besaß.\n\n\"Siehe auch:\" Macintosh-Modelle\n\n"}
{"id": "106227", "url": "https://de.wikipedia.org/wiki?curid=106227", "title": "Vorschussbetrug", "text": "Vorschussbetrug\n\nDer Vorschussbetrug ist die kriminologische Bezeichnung für eine Unterart des Betrugs in Deutschland ( StGB) und in Österreich ( ff. StGB). Die Empfänger werden unter Vorspiegelung falscher Tatsachen (vgl. Social Engineering) dazu bewogen, an Schneeballsystemen teilzunehmen oder in Erwartung zugesagter Vermittlungsprovisionen gegenüber den Absendern (den \"Betrügern\") finanziell in Vorleistung zu treten. Dem Opfer wird zunächst glaubhaft gemacht, ein enormes Vermögen verdienen zu können. Auf diese Gegenleistung des Geschäfts – Geld oder Waren – wartet der Vorschussleistende vergeblich, weil eine Gegenleistung von Anfang an nicht beabsichtigt war.\n\nPotentielle Opfer werden zu diesem Zweck heute oft mit Massen-E-Mails („Spam“) kontaktiert, während vor einiger Zeit noch der Massen-Fax-Versand verbreitet war. Aber auch normale Post-Briefe finden zur Kontaktaufnahme weiterhin Verwendung.\n\nIm Englischen wird der Begriff Scam () als Synonym für den Vorschussbetrug verwendet. Im Allgemeinen bezieht er sich auf eine Leistung, die im Voraus bezahlt aber dann nicht erbracht wird. Er hat sich in der Folge als Anglizismus für Vorschussbetrug auch im deutschen Sprachraum verbreitet.\n\nVorschussbetrug per Briefpost ist schon seit dem 16. Jahrhundert bekannt. Der Betrug ist im Englischen unter dem Begriff \"Spanish Prisoner\", im Französischen unter \"Lettre de Jérusalem\" bekannt. In den 1950er und 60er Jahren, als ein allgemeiner Arbeitskräftemangel herrschte, war die besondere Form des Lohnvorschussbetrugs recht verbreitet, bei der sich Betrüger auf eine Stellenanzeige meldeten und den künftigen Arbeitgeber um Zahlung eines Vorschusses auf den Lohn baten, etwa zur Finanzierung des notwendigen Umzugs. Nach Erhalt dieses Vorschusses meldeten sie sich nicht mehr. \n\nEin Massenphänomen wurde der Vorschussbetrug mit der zunehmenden Verbreitung von Faxgeräten Mitte der 1980er Jahre, als vor allem nigerianische Banden in hunderttausenden von Faxen (und später in E-Mails) potentiellen Opfern in oft fehlerhaftem Englisch hohe Gewinne versprachen. Daher wird diese Art des Betruges inzwischen auch \"four-one-niner\" oder \"419 scam\" (nach dem relevanten § 419 des nigerianischen Strafgesetzbuchs, der sich mit dieser Straftat vor Erlass der Vorauszahlungsverordnung Nr. 13 im Jahre 1995 befasste) oder aber auch \"Nigerianischer Brief\" \"(Nigeria-Connection)\" genannt.\n\nInzwischen agieren die Trickbetrüger auch aus anderen Städten und Staaten. Am 13. Februar 2003 erschoss ein geprellter Tscheche aus Rache den nigerianischen Konsul in Prag, der ihm Hilfe gegen finanzielle Beteiligung angeboten haben soll.\n\nDas System dieser Betrügerei zielt darauf ab, das Opfer zu einer Zahlung für verschiedene fiktive Kosten zu veranlassen, z. B. für einen Rechtsanwalt, damit der Geldtransfer abgeschlossen werden kann, oder als sogenannte Aktivierungsgebühr für angeblich ruhende Konten. Dem deutschen Bundeskriminalamt zufolge handelt es sich um einen schematischen Tatablauf: Bekundet jemand sein Interesse an dem angebotenen „Geschäft“ und antwortet auf das Angebot per E-Mail, erhält er per Telefax zahlreiche offiziell aussehende Schreiben, etwa der „Central Bank of Nigeria CBN“, der „Nigerian National Petroleum Corporation NNPC“ oder anderen fiktiven oder tatsächlichen Behörden oder Banken, wo er als Empfänger einer hohen Summe eingetragen ist. Die angebliche Freigabe der Gelder wird anschließend durch unterschiedliche fiktive Behörden wie „The Presidency – Debt Reconciliation Committee“, „The Foreign Payment Office“, „Debt Management Department“, „Office of the Accountant General“, „Federal Inland Revenue Service“, „Central Bank of Nigeria – Department of Foreign Operation“, „Fund Release Authority“, „International Fund Remittance“ u. ä. bestätigt.\n\nVor der Auszahlung werden jedoch in allen Fällen Provisions-, Verwaltungs- oder Versicherungsgebühren fällig, die von dem „Geschäftspartner“ gefordert werden. Hat dieser dann Vorauszahlungen geleistet, verzögert sich die Auszahlung des Millionenbetrages immer wieder wegen unterschiedlichster „Schwierigkeiten“, die nur durch Zahlung weiterer Beträge beseitigt werden können. Nicht selten werden zur Übergabe des Geldes persönliche Treffen im europäischen Ausland (bevorzugt London, Amsterdam und Madrid) arrangiert.\n\nNeben Nigeria sind in den letzten Jahren viele weitere Staaten als Absenderländer bekannt geworden. Die angebliche Herkunft der Gelder reicht von unterschlagenem Firmenvermögen über unverhofft aufgetauchte Familienschätze, Kriegsbeute, Lotterie- oder Gewinnspiele bis hin zu angeblichen Erbschaften nach plötzlichen Todesfällen.\n\nObwohl die Geschichten, die in den Betrugsbriefen erzählt werden, häufig fantastisch und unglaubwürdig sind, fallen immer wieder einzelne leichtgläubige Personen und sogar staatliche Stellen auf diese Masche herein. So zahlte die Stadt Ennigerloh 2001 einem Sozialhilfeempfänger 145.000 €. Er hatte den Bürgermeister durch einschlägige Dokumente von einem angeblich in Afrika festsitzenden Vermögen von 34 Mio. € überzeugt und versprochen, nahezu den doppelten Betrag des Vorschusses an die Stadt zurückzuzahlen, sobald er an sein Geld gekommen sei. In der Folge dieser Vorkommnisse verlor der Bürgermeister sein Amt und musste sich wegen Veruntreuung öffentlicher Gelder vor Gericht verantworten.\n\nNeben Geld, das man beim Betrug als Vorschussleistung entrichtet und verliert, besteht auch die Gefahr, sich schwerwiegend strafbar zu machen. Manche Betrüger geben vor, dass man aus abstrusen Gründen Geldsummen über ein Zwischenkonto des Betrogenen transferieren müsse, und der Betrogene wird dafür mit einer Provision belohnt. Tatsächlich handelt es sich dabei meistens um Geldwäsche.\n\nDas transferierte Geld stammt aus den Gewinnen krimineller Organisationen, vor allem Rauschgifthandel. Daher wird der Tatbestand der Geldwäsche hart bestraft. In der Regel ist mit Haftstrafen zu rechnen, wodurch der Schaden weit höher ausfallen kann als der Verlust aller Ersparnisse.\n\nManche Betrugsopfer nehmen zudem gutgläubig Kredite auf oder leisten eine Bürgschaft, die sie, wenn sie dann bürgen müssen, nur durch jahrelange Ratenzahlungen abbezahlen können. In Einzelfällen beschaffen sie sich auch größere Summen durch Betrug oder aus dem Bekanntenkreis.\n\nEinige verbreitete Formen des Vorschussbetruges sind unter eigenen Bezeichnungen bekannt und werden in den folgenden Abschnitten beschrieben.\n\nTypischer Vertreter dieser Spielart ist der Nigeria-Scam einer Nigeria-Connection. Hier behaupten die Absender, Kenntnisse von Konten ehemaliger Machthaber oder Großkonzerne in Entwicklungsländern zu besitzen und nun die Hilfe des Mailempfängers zu benötigen, um die Millionensummen ins Ausland zu transferieren. Die dafür in Aussicht gestellten Provisionen im zweistelligen Prozentbereich locken die Opfer, im Vorfeld Gelder – vorgeblich für Gebühren, Bestechungen etc. – zu bezahlen. Oftmals werden täuschend echt gestaltete Webseiten erstellt, die denen von Behörden und Banken sehr ähnlich sehen und von der Seriosität des Angebots überzeugen sollen. Auch unverhoffte Lotteriegewinne, die eingelöst werden müssen, und Treuhandbetrug (mit Hilfe eigener Treuhänder) bei Online-Auktionshäusern gehören zum Repertoire der \"Nigeria-Scammer\". Hierbei werden teure, meist elektronische Artikel erstanden, die dann ins Ausland verschickt werden sollen, und zur Zahlung soll ein vom Käufer ins Spiel gebrachter Treuhänder verwendet werden, der natürlich niemals Geld an den Verkäufer weiterleitet, nachdem die Ware verschickt wurde.\nEs gibt auch angebliche Erbschaften aus dem westlichen Raum. Die Schriftstücke sind mit dem echten Logo einer Bank oder einer Behörde versehen. Die Anschreiben appellieren in mehrfacher Hinsicht an das humanitäre Gefühl der Adressaten: oft in Christi Namen wird eine Erbschaft angekündigt, etliche Millionen Pfund, die der Empfänger zu einem Teil für sich verwenden dürfe, zu einem Teil für einen guten Zweck weiterleiten solle. Dadurch soll das Opfer Hoffnung auf eine sorglose Zukunft schöpfen und sich gleichzeitig als Wohltäter beweisen. Durch die geringere Gebühr, etwa 1.000 Euro, ist eine Einstandssumme gewählt worden, die möglicherweise leichter zum Risiko verlockt.\n\nIn diversen Foren, die sich mit diesem Thema beschäftigen, gibt es Hinweise, wie sich die Unseriosität dieser Angebote auf den zweiten Blick einfach erkennen lassen:\nkleine Veränderungen bei den E-Mail-Adressen der Absender (die meist als seriöse Banken oder Behörden mit deren Logo, Fotos von leitenden Mitarbeitern und Telefonnummern auf dem entsprechenden Briefpapier auftauchen) oder ein zweifelhafter Weg für das Geld (über eine bestimmte Bank im Inland an eine große ausländische Bank zugunsten eines Empfängers, ohne Angabe einer Kontonummer), das zu zahlen ist, um die Erbschaft amtlich zu regeln und auszahlen zu können.\n\nDie Opfer dieser Betrügereien werden von den Betrügern als „Mugu“ bezeichnet, das Wort für „Vollidiot“ im nigerianischen Pidgin.\n\n2013 wurden erstmals Kontaktaufnahmeversuche einer Nigeria-Connection über Dienste wie Skype oder Facebook beobachtet.\n\nHier handelt es sich um eine Art von Vorschussbetrug mittels einer fiktiven Liebesgeschichte mit Hilfe von E-Mail und Chatsystem (Bridescam für Verlobungen, die aus Sicht einer Beteiligten von Anfang an nicht stattfinden sollen). Ausländische Betrüger nehmen in einer Singlebörse Kontakt auf und suggerieren ihren Opfern, sie hätten sich verliebt. Etwas später bittet der Betrüger sein Opfer unter einem Vorwand um Geld:\n\n\nDie Internetseite der Polizeien der Länder und des Bundes zur Kriminalprävention weist darauf hin, dass außerdem Einladungen nach Deutschland und Kopien von Ausweisen erbeten werden. Die Daten werden für Fälschungen von Pässen genutzt.\n\nInternet Romantic Love Scam wird sehr häufig bei Singlebörsen und Partnervermittlungen praktiziert. Aber auch vor sozialen Netzwerken machen diese Betrüger keinen Halt. Dabei bietet heutzutage Google bereits Möglichkeiten zum Identifizieren von möglichem Scam.\n\nWer sein Kraftfahrzeug über Internetbörsen verkaufen will, erlebt oftmals ähnliches. Hier wird dem Verkäufer eine E-Mail geschickt, in der angekündigt wird, den geforderten Preis ohne weiteres zu zahlen, jedoch soll die Zahlung mittels Scheck erfolgen. Dieser ist jedoch auf eine höhere Summe als der Kaufpreis ausgestellt. Der Verkäufer soll dann den Scheck einlösen und den Differenzbetrag dem Abholer des Fahrzeugs mitgeben, um auf diese Weise den Transport des Fahrzeuges zu zahlen.\nOftmals erhält der Verkäufer auch zunächst Bargeld, wenn er den Scheck einlöst. Häufig stellt sich aber heraus, dass der Scheck nicht gedeckt ist, so dass alles zurückgebucht wird. In der Zwischenzeit ist in der Regel aber auch das zu verkaufende Fahrzeug schon abgeholt und ins Ausland verbracht worden. Eine Rückabwicklung des Geschäftes ist im Grunde unmöglich.\n\nAuch beim Kauf eines Kraftfahrzeugs ist Vorsicht geboten, insbesondere wenn Fahrzeuge erheblich unter Marktwert verkauft werden. Hierbei werden oft Gründe wie ein vorausgegangener Umzug nach England vorgeschoben, weswegen der Verkäufer mit der Linkslenkung nicht mehr zurechtkäme. Damit man sich nicht umsonst auf eine größere Reise begibt, bietet der Verkäufer an, dass man sich auf halber Fahrstrecke trifft. Als Nachweis, dass es beide Seiten ernst meinen, wird dann vorgeschlagen, Bargeld per Transfer „an sich selbst“, tatsächlich an die mitreisende Ehefrau oder einen Bekannten zu senden und die Transferbelege per Mailanhang auszutauschen.\n\nDer Täter benötigt von den Kopien nur die Transfernummer des Opfers und die Personalien des Mitreisenden. Mittels eines gefälschten Identitätsdokumentes und mit der bekannten Transfernummer kann er das Geld abholen, während das arglose Opfer noch auf dem Weg zum Treffpunkt ist. Der Transferbeleg des Täters ist dabei, wie alles andere auch, gefälscht. Die Täter horten elektronische Ausweiskopien und Annoncen, um diese in abgewandelter Form immer wieder in eigener Sache zu verwenden.\n\nDiese Form des Vorschussbetrugs richtet sich (derzeit) vor allem an Rechtsanwälte. Es meldet sich die angebliche geschiedene Frau eines Deutschen mit der Bitte, die in der Scheidungsfolgenvereinbarung vereinbarte Summe einzutreiben (in der Regel mehrere hunderttausend Dollar), die sich der ehemalige Partner weigert zu zahlen. Wenig später meldet sich der angebliche Partner selbst aus dem Ausland mit einer Entschuldigung für die Unannehmlichkeiten und einem Auslandsscheck, den der deutsche Empfänger einlösen und das Geld dann an die angebliche Ex-Frau weiterleiten möge. Hier ist der Scheck meist eine sehr gute Totalfälschung, so dass das Geld zunächst in Deutschland gutgeschrieben und hierüber verfügt werden kann, jedoch später eine Rückbuchung erfolgt.\n\nBei dieser Form des Scam wird ein zur Vermietung stehendes Appartement, meist gut ausgestattet und in gehobener Wohnlage bei gleichzeitigem günstigen Mietpreis, als Lockvogel für Opfer auf Wohnungssuche benutzt. Als Vermieter tritt eine Person auf, die aufgrund beruflicher Umstände für mehrere Jahre im Ausland lebt und sich nun entschieden hat, die Wohnung in dieser Zeit zu vermieten. Wegen des Auslandsaufenthaltes sei keine persönliche Besichtigung oder Wohnungsübergabe möglich. Stattdessen wird vorgeschlagen, die erste Miete und Kaution auf ein Treuhandkonto zu überweisen, im Gegenzug erhält man den Wohnungsschlüssel per Post; dieses Verfahren wird beispielsweise als „TNT buyer protection“ beschrieben. Bei Nichtgefallen schickt man den Schlüssel zurück. Tatsächlich hat der Anbieter Zugriff auf das Geld ab Zustellung und wird davon umgehend Gebrauch machen.\n\nPer Flugblatt oder einer anderen anonymen Werbeform bietet vorgeblich ein privat geführtes Hotel oder ein Vermieter von Ferienwohnungen in einer ausländischen Urlaubsregion Aufenthalte zu günstigen Preisen an, oft mit dem Hinweis, man spare so Kosten für Reisebüro und Pauschalreiseanbieter. Auf einer eigens eingerichteten Hotel-Webseite wird das fiktive Hotel vorgestellt und es können Zimmer gebucht werden. Für eine verbindliche Buchung wird die Anzahlung eines erheblichen Teils des Gesamtpreises auf das „Bankkonto des Hotels“ im Zielland verlangt.\n\nPer Telefon, E-Mail oder Briefpost\nerhält das Opfer die Nachricht, einen Preis gewonnen zu haben. Zum Teil handelt es sich nur um eine Marketingmasche in einem Graubereich zwischen Legalität und Kriminalität wie bei Kaffeefahrten und Drückerkolonnen (Ziel etwa Zeitschriften-Abonnement), wobei eine Gewinnzusage eventuell einklagbar ist. Zum Teil zielt der Trick auf Mehrwertdienstmissbrauch. Um Vorschussbetrug handelt es sich, wenn bevorzugt ein angeblicher Rechtsanwalt oder Notar eigene oder andere „Bearbeitungsgebühren“, Zollgebühren, Steuern oder Transport- oder Versicherungskosten als Voraussetzung für die Auszahlung eines Geldgewinns oder anderweitiges Zukommenlassen fordert. Gewinnanrufe kommen aus Callcentern vor allem in der Türkei, verschleiert durch Spoofing.\n\nIm Internet hat sich mit dem Scam Baiting (sinngemäß \"Betrüger ködern\") eine Gegenbewegung zu dieser Form des Betrugs herausgebildet. Hierbei wird in der Regel versucht, die Vorschussbetrüger (Scammer) selbst zu „betrügen“. Dabei geht der Scam Baiter zum Schein auf die Forderung des Scammers ein, erfindet aber selbst eine Geschichte, die den Scammer veranlassen soll, auf seine eigene Gier hereinzufallen. Gute Scam Baiter können die Betrüger sogar dazu überreden, selbst Vorleistungen zu erbringen oder an einem Treffen teilzunehmen. Tatsächlich gelingt dies selten, zum Sport hat es sich jedoch entwickelt, vom Scammer als Beweis, dass es ihn gibt, Selbstporträts zu verlangen, auf denen der Scammer oftmals in einer lächerlichen Situation erscheinen soll oder Schilder mit – ihm meist unverständlichen – Obszönitäten zeigt.\n\nSinn des Scam Baiting ist es zunächst, die Scammer zu ärgern und damit von einer Fortsetzung ihres Tuns abzuhalten. Manchmal können darüber hinaus auch wichtige Informationen an die Ermittlungsbehörden weitergegeben werden. Dabei besteht allerdings das Risiko, selbst Opfer einer Racheaktion zu werden oder sich strafbar zu machen.\n\nBekannt wurde um 2002 ein umfangreicher Mailwechsel, in dem sich ein philippinischer Empfänger einer Scam-Mail nicht nur zum Schein auf den Handel einließ, sondern dem Absender noch weitaus mehr bis hin zur Adoption anbot, worauf dieser eine symbolische Vorauszahlung von 3 Dollar leistete und anschließend bloßgestellt und der Polizei übergeben wurde, da er während seiner Versuche, seinerseits die erwartete Zahlung zu empfangen, seine Mobiltelefonnummer offengelegt hatte.\n\nDas deutsche Bundeskriminalamt rät, derartige E-Mails oder Sonstiges nicht zu beantworten und keine Kontakte mit den Beteiligten aufzunehmen. Weitere Hinweise gibt die Polizei-Beratung. Die Täter setzen darauf, dass ihr Opfer sie aus Scham oder Angst um seinen guten Ruf (z. B. in der Geschäftswelt) nicht anzeigt. Auch schrecken manche Opfer vor einer Anzeige zurück weil sie sich vom Täter zu einem nicht ganz legalen Vorgehen haben verleiten lassen und daher ihrerseits Strafe befürchten. Trotzdem ist eine Anzeige unbedingt anzuraten, auch um weitere Opfer verhindern zu helfen.\n\n\n\n\n"}
{"id": "106371", "url": "https://de.wikipedia.org/wiki?curid=106371", "title": "Antialiasing (Computergrafik)", "text": "Antialiasing (Computergrafik)\n\nAntialiasing (AA, auch \"Anti-Aliasing\" oder Kantenglättung) ist die Verminderung von unerwünschten Effekten, die durch das begrenzte Pixelraster (siehe Bildauflösung und Alias-Effekt) oder durch den Treppeneffekt bei der Erzeugung einer Computergrafik (computergenerierte 2D- oder 3D-Grafiken) entstehen können.\n\nBeim Antialiasing werden die Bildinhalte üblicherweise nicht nur am Pixel, sondern auch an anderen Positionen ausgewertet (abgetastet) und in die Berechnung der Pixelfarbe mit einbezogen. Daneben verringern einige für das Echtzeitrendern entwickelte Techniken den Treppeneffekt durch nachträgliche Filterung oder Nachzeichnung des Bildes.\n\nAntialiasing-Methoden unterscheiden sich durch das zur Verteilung der Abtastpunkte verwendete Schema sowie durch die Wahl des Rekonstruktionsfilters, der bestimmt, wie die an den Abtastpunkten ermittelten Farbwerte gewichtet werden.\n\nDie Erzeugung einer Rastergrafik aus einer Bildbeschreibung per Rasterung oder Bildsynthese besteht letzten Endes darin, jedem diskreten Pixel eine Farbe zuzuweisen. Dieser Vorgang kann im Rahmen der Theorie der Signalverarbeitung als Abtastung eines Signals interpretiert werden. Die Computergrafik ist insofern einzigartig, als hier viele Signale als abstrakte Bildbeschreibungen vorliegen, die nur an einzelnen Punkten algorithmisch ausgewertet werden können. Ein Beispiel sind Bilder, die mittels Raytracing berechnet werden. Derartige Signale können auch als \"prozedurale Signale\" bezeichnet werden.\n\nBeim Rastern ohne Antialiasing wird die Bildbeschreibung ausschließlich an den Pixeln ausgewertet; andere Punkte des Bildes fließen nicht in die Farbe der Pixel mit ein. Ein Problem mit dieser Methode ist, dass kleine Figuren nicht vom Pixelraster erfasst werden und somit im gerasterten Bild nicht auftauchen. Wenn kleine Bilddetails regelmäßig angeordnet sind, interferieren sie mit dem Pixelraster, was zu Alias-Effekten führt. Dies wird durch nebenstehendes computergeneriertes Bild eines unendlich großen Schachbretts illustriert.\n\nDie Periodenlänge des Signals ist hier gleich der Größe zweier projizierter Schachbrettfelder. Sobald die Periodenlänge zwei Pixelabstände unterschreitet, also die Nyquist-Frequenz überschritten wird, wird das Signal unterabgetastet. Bestimmte Schachbrettfelder werden jetzt vom Pixelraster nicht mehr erfasst; das Schachbrettmuster wird zerstört. Gemäß dem Nyquist-Shannon-Abtasttheorem treten Alias-Effekte auf, bei denen sich eine hohe Originalfrequenz in der Abtastung als irreführende niedrige Frequenz äußert: Nahe dem Horizont entsteht der falsche Eindruck, das Originalsignal würde merkwürdig große Felder enthalten. Der im Deutschen gelegentlich verwendete illustrierende Begriff „Lattenzauneffekt“ macht dies deutlich: Man betrachtet das Ortsfrequenzspektrum des Originalbildes durch eine vom Wiedergabegerät, z. B. dem Bildschirm, fest vorgegebene Frequenzauflösung – den Lattenzaun. Diese Art von Alias-Effekten, die ein Resultat der Abtastmethode sind, wird \"Prealiasing\" genannt.\n\nAuch wenn das Bild mit höheren Bildauflösungen und damit höheren Abtastraten berechnet würde, würden störende Aliasing-Effekte nahe dem Horizont auftauchen, allerdings erst ab einer höheren Frequenz. Das liegt daran, dass in der Schachbrettszene die Felder zum Horizont hin immer kleiner werden und somit die Ortsfrequenz unbegrenzt ist.\n\nIn der Signalverarbeitung bezeichnet Rekonstruktion die Umwandlung eines diskreten in ein kontinuierliches Signal, indem zwischen den einzelnen Abtastwerten mittels eines Rekonstruktionsfilters interpoliert wird. In der Computergrafik wird der Begriff etwas anders interpretiert, da hier gar kein kontinuierliches Signal erzeugt wird. Vielmehr bedeutet Rekonstruktion in diesem Zusammenhang, die Farbe eines Pixels aus den Farbwerten in der Nähe des Pixels zu berechnen, zum Beispiel um eine Rastergrafik zu skalieren. Beim dabei verwendeten Rekonstruktionsfilter handelt es sich um eine zweidimensionale Funktion, die über dem zu berechnenden Pixel zentriert ist und angibt, wie Abtastwerte gewichtet werden. So wird bei einem kegelförmigen Filter ein direkt am Pixel ermittelter Farbwert am höchsten gewichtet, während weiter entfernte Werte weniger Einfluss haben. Außerhalb des Trägers des Rekonstruktionsfilters liegende Farbwerte werden ignoriert. Der Farbwert des Pixels ist die Summe der gewichteten Farbwerte. Diese Operation entspricht einer Faltung mit einem Tiefpassfilter von einer Ordnung der gewählten Antialiasing-Stufe.\n\nDas Antialiasing mittels Box-Filter, das den Mittelwert aller Farbwerte innerhalb eines um das Pixel gelegten Quadrates berechnet, wird auch als „ungewichtete Flächenabtastung“ \"(Unweighted Area Sampling)\" bezeichnet. In anderen Fällen spricht man von „gewichteter Flächenabtastung“ \"(Weighted Area Sampling).\" Beispiele für Rekonstruktionsfilter bei gewichteter Flächenabtastung sind die Mitchell-Netravali-Filter (bikubische Filter), das Lanczos-Filter oder das Gauß-Filter.\n\nEin abgetastetes, diskretes Signal ist im Frequenzbereich eine Reihe von frequenzversetzten Kopien des Nutzsignals. Die Rekonstruktion kann als Isolation des originalen Nutzsignals unter Ausschluss der Kopien interpretiert werden. Wenn der Rekonstruktionsfilter nicht nur das Nutzsignal isoliert, sondern auch Teile von dessen Kopien mit einbezieht (siehe Schema rechts), kommt es zu \"Postaliasing.\" Postaliasing entsteht also durch die Wahl eines ungeeigneten Rekonstruktionsfilters und kann selbst dann auftreten, wenn das Originalsignal mit einer ausreichenden Abtastrate und ohne Prealiasing abgetastet wurde.\n\nDer Begriff Treppeneffekt bezeichnet das kantige, „treppenartige“ Erscheinungsbild gerasterter Figuren. In der Fachliteratur wurde oft behauptet, dass der Treppeneffekt eine Folge des Alias-Effekts sei. Dies ist jedoch falsch, wie sich anhand eines Vergleichs der Fourier-Transformationen des idealen und des gerasterten Bildes zeigen lässt. Der Treppeneffekt äußert sich nicht, indem eine hohe Frequenz als störende niedrige Frequenz erscheint, sondern ist eine direkte Konsequenz der begrenzten Auflösung des Ausgabegerätes. Besonders bei Animationen ist der Treppeneffekt auffallend, da sich hier Figuren scheinbar ruckartig bewegen und sehr dünne oder kleine Objekte bei der Bewegung zu flimmern scheinen.\n\nOhne Anwendung von Antialiasing wird für jedes Pixel nur einmalig an einer relativ zum Pixel gleichbleibenden Position abgetastet. Beim Antialiasing wird die Bildbeschreibung an mehreren und/oder relativ zum Pixel unterschiedlichen Positionen ausgewertet. Aus den so ermittelten Werten wird die Farbe des Pixels gemäß einem Rekonstruktionsfilter berechnet. Durch die Wahl eines geeigneten Abtastverfahrens lässt sich Prealiasing, durch die Wahl geeigneter Rekonstruktionsfilter Postaliasing vermindern oder vermeiden. Die Bezeichnung „Antialiasing“ ist insofern irreführend, als Antialiasing nicht nur gegen Alias-Effekte, sondern auch gegen den Treppeneffekt und andere unerwünschte Effekte, etwa durch das Pixelraster fallende kleine Figuren, angewandt wird. Franklin Crow lieferte 1977 die erste Beschreibung des Alias-Effekts als Ursache von Bildartefakten in Computergrafiken.\n\nDie traditionellen Antialiasing-Techniken der Signalverarbeitung sind nicht ohne Weiteres auf die Computergrafik übertragbar. Dies liegt daran, dass vom Standpunkt der Signalverarbeitung betrachtet Computergrafiken im Gegensatz beispielsweise zu Audiosignalen folgende Besonderheiten aufweisen:\n\n\nDamit Antialiasing die beste Wirkung erzielt, ist es unerlässlich, Gammakorrektur anzuwenden. Linien und Polygonkanten, die mit Antialiasing, aber ohne Gammakorrektur gerastert wurden, tendieren zu einem „seilartigen“ Aussehen. Antialiasing lässt sich auch mit Subpixel-Rendering kombinieren, um von der horizontalen Unterteilung eines Bildschirmpixels in nebeneinander liegende Grundfarben zu profitieren. Im Bereich der Pixel-Art geschieht Kantenglättung nicht automatisch, sondern wird vom Grafiker direkt durch das Setzen einzelner Pixel erreicht.\n\nAntialiasing nimmt zusätzliche Rechenleistung in Anspruch, die besonders beim Echtzeitrendern nicht vernachlässigbar ist. Ein weiterer Nachteil ist, dass ein mit Antialiasing erzeugtes Bild als unscharf empfunden werden kann; die Unschärfe hängt ebenso wie andere Mängel vom verwendeten Rekonstruktionsfilter ab. Umgekehrt stellt die bloße nachträgliche Weichzeichnung eines gerasterten Bildes kein konventionelles Antialiasing dar, da hierbei die ursprüngliche Bildbeschreibung nicht ausgewertet wird; dennoch basieren einige Techniken aus dem Echtzeitbereich auf einer solchen Verfahrensweise (siehe Abschnitt Hardware-Implementierung). Bilder, die bereits Prealiasing-Effekte aufweisen, etwa mit niedriger Auflösung gescannte Druckraster, lassen sich zum Teil nachträglich im Frequenzbereich durch Beseitigung der störenden Fourierkomponenten korrigieren. Es existieren außerdem Techniken, um Treppeneffekte bei Bildern, die ohne Antialiasing erzeugt wurden, zu entfernen.\n\nIn der Computergrafik bezeichnet \"Prefiltering\" die Ermittlung der Farbe eines Pixels, ohne dass einzelne Abtastungen vorgenommen werden. Vielmehr wird die Farbe direkt anhand der Bildbeschreibung berechnet. Wie oben beschrieben, entspricht der Farbwert eines Pixels der vom Rekonstruktionsfilter gewichteten Summe aller Farbwerte der Objekte, die vom Träger des Filters überlappt werden. Prefiltering ist nur bei Bildbeschreibungen möglich, deren Faltung mit dem Rekonstruktionsfilter analytisch berechenbar ist, also in Form bekannter Funktionen ausgedrückt werden kann. Hierzu zählen einfache geometrische Objekte wie Linien. Auf prozedurale Signale kann Prefiltering hingegen nicht angewandt werden, da diese nur an individuellen Punkten abgetastet werden können.\n\nEines der ersten Prefiltering-Verfahren für Computergrafiken wurde 1978 von Edwin Catmull beschrieben. Sein Algorithmus verwendet Unweighted Area Sampling. Die Farbe eines Pixels wird berechnet, indem die Polygone, aus denen sich das Bild zusammensetzt, gegen das Pixel geclippt werden und so deren Flächenanteil ermittelt werden kann. Dieses Verfahren war so langsam, dass es nur bei zweidimensionalen Computeranimationen mit einigen großen Polygonen eingesetzt werden konnte. Spätere Methoden versuchten, die Flächenanteile der Objektfragmente mittels Bitmasken zu approximieren – darunter Carpenters A-Buffer, manchmal auch \"Multisampling\" genannt – oder Lookup-Tabellen zu verwenden. Daneben wurden auf die Rasterung von Grundformen wie Linien und Kreisen zugeschnittene Antialiasing-Verfahren entwickelt, siehe Rasterung von Linien und Rasterung von Kreisen. Auch hochwertiges Prefiltering-Antialiasing beliebiger Kurven mit verschiedenen Rekonstruktionsfiltern ist möglich.\n\n\"Postfiltering\" oder \"Supersampling\" wird vornehmlich angewandt, wenn die Bildbeschreibung nur an einzelnen Punkten ausgewertet werden kann. Dazu werden zur Berechnung der Farbe eines jeden Pixels mehrere Abtastwerte herangezogen, die mittels eines Rekonstruktionsfilters gewichtet werden. Mathematisch betrachtet ist Postfiltering eine Methode zur numerischen Annäherung des Faltungsintegrals.\n\nMultisampling-Methoden unterscheiden sich in der Anzahl und Verteilung der Abtastpositionen pro Pixel. Beim Echtzeitrendern wird meist für alle Pixel das gleiche Muster verwendet. Einige Muster legen für die Abtastwerte unterschiedliche Wichtungen fest, weshalb sie auch als \"lineare\" Rekonstruktionsfilter betrachtet werden können. Wenn der Träger eines Rekonstruktionfilters über mehrere Pixel reicht und sich mehrere Pixel einige Abtastwerte teilen, spricht man von \"Sample Sharing.\"\n\nFolgende geordnete Muster sind gebräuchlich:\n\nIm Gegensatz zu den geordneten Mustern variiert bei der \"stochastischen Abtastung\" das Abtastmuster relativ zum Pixel. Dadurch werden Alias-Effekte durch Rauschen ersetzt, das als weniger irritierend wahrgenommen wird. Gleichzeitig ist es jedoch wünschenswert, die Abtastpunkte weit entfernt voneinander zu halten, um eine möglichst repräsentative Abtastung zu gewährleisten. Stochastische Abtastmuster werden vor allem in der realistischen Bildsynthese verwendet.\n\nEine weitere Methode ist \"Adaptive Supersampling.\" Dabei wird die Anzahl der Abtastpunkte über das Bild variiert. Die Entscheidung, ob weitere Abtastungen vorgenommen werden sollen, wird anhand von lokalen Kriterien wie Kontrast gefällt. Derartige Techniken sind allerdings in der Regel nicht erwartungstreu und können zu Artefakten führen.\n\nAnstatt die Abtastpunkte mehr oder weniger gleichmäßig zu verteilen und die dort ermittelten Werte durch einen Rekonstruktionsfilter zu gewichten, kann zur Berechnung der Pixelfarbe Importance Sampling angewandt werden. Hierbei werden die Abtastwerte entsprechend der Form des Rekonstruktionsfilters verteilt (mehr Abtastpunkte nahe beim Pixel) und gleich gewichtet. Diese Methode führt zu einem weniger verrauschten Ergebnis.\n\nZum Echtzeitrendern mittels Grafikkarten kann Antialiasing direkt in Hardware implementiert werden. Zunächst unterstützten nur sehr hochwertige und teure Grafikkarten diese Technik in Grafikschnittstellen wie OpenGL. Mit dem Aufkommen erschwinglicher Desktop-Grafikchips mit hoher Performance wie dem 3dfx VSA-100 oder dem NV10 von Nvidia wurde Echtzeit-Antialiasing auch für den normalen Anwender verfügbar.\n\nDie konzeptuell einfachste Methode ist \"Full-Scene Antialiasing\" (FSAA). Dabei werden die Bilder mit höherer Auflösung gerendert und anschließend heruntergerechnet. Eine andere Methode verwendet den Akkumulationspuffer. Um ein Bild mit vier Abtastpunkten pro Pixel zu rendern, werden nacheinander vier Bilder in den Puffer geschrieben, die jeweils um den Bruchteil eines Pixelabstands in alle Richtungen versetzt sind. Eine Variante des Akkumulationspuffers ist der T-Buffer. Er besteht aus zwei, vier oder mehr Bild- und Z-Buffern, von denen jeder zum Rendern verwendet werden kann. Eine Maske bestimmt, wohin ein Dreieck gesendet wird; am Ende werden alle Puffer kombiniert. Wenn für jeden Puffer individuell ein Bildschirmoffset gesetzt wird, lässt sich Antialiasing erzielen, indem jedes Dreieck parallel an alle Puffer gesendet wird. Der Vorteil des Akkumulationspuffers und des T-Buffers ist, dass nicht nur geordnete Abtastpositionen möglich sind. Häufig wird auch sogenanntes \"Multisampling\" verwendet, das sich insofern von Supersampling unterscheidet, als nicht für jede Abtastposition Shader-Berechnungen durchgeführt werden.\n\nAls Prefiltering-Algorithmus kann auch der A-Buffer in Hardware implementiert werden. Es wurden weitere hardwarebasierte Prefiltering-Algorithmen veröffentlicht, darunter das \"Z³\"-Verfahren, das in ähnlicher Form von Matrox unter dem Namen \"Fragment Antialiasing\" (FAA) eingeführt wurde. Der Vorteil von Prefiltering ist, dass nur auf die Bereiche mit Polygonkanten tatsächlich Antialiasing angewendet wird.\n\nIn der Praxis werden beim Echtzeit-Antialiasing oft nur wenige Abtastwerte pro Pixel und das Box-Filter oder einfache lineare Rekonstruktionsfilter verwendet. Mindestens eine Grafikkarte, die Wildcat von 3DLabs, verwendet mit Jittering eine stochastische Abtastmethode. Das professionelle SAGE-Grafiksystem von Sun Microsystems erlaubt ein über 128×128 Pixel reichendes, programmierbares Abtastmuster mit gewichtetem Rekonstruktionsfilter.\n\nEine weitere, von DirectX unterstützte Methode ist \"Edge Antialiasing.\" Dabei werden nach dem Rendern der Szene die Objektkanten in einem zweiten Durchgang mit geglätteten Linien nachgezeichnet. Beim Deferred Shading, eine in einigen neueren Computerspielen angewandte Technik, wird die Geometrie der Objekte unabhängig von ihrer Beleuchtung berechnet. Da hierbei kein herkömmliches Hardware-Antialiasing anwendbar ist, kann der Treppeneffekt durch Kantendetektion und Weichzeichnung der Objektkanten kaschiert werden. Eine ähnliche Methode ist \"Morphological Antialiasing\" (MLAA). Hierbei bestimmt das Pixelmuster an den Kanten die Art der Weichzeichnung.\n\nDie perspektivische Darstellung von Texturen beim Echtzeitrendern wird gelegentlich als „Textur-Antialiasing“ bezeichnet. Die dabei verwendeten Techniken wie Mip Mapping und anisotropes Filtern führen jedoch kein Antialiasing im üblichen Sinn durch, da keine neuen Bilder aus einer mathematischen Bildbeschreibung generiert werden. Vielmehr werden beim Texture Mapping existierende Rastergrafiken perspektivisch skaliert, außer bei prozeduralen Texturen, die während des Rendervorgangs berechnet werden.\n\n\n"}
{"id": "107113", "url": "https://de.wikipedia.org/wiki?curid=107113", "title": "/dev/null", "text": "/dev/null\n\n/dev/null ist ein Device, das in den meisten UNIX-ähnlichen Betriebssystemen existiert. In echten UNIX-Systemen (also solchen, die den POSIX-Standard einhalten) ist seine Existenz vorgeschrieben. Auch viele andere Betriebssysteme weisen in ihrer Funktion ähnliche sogenannte Nullgeräte auf.\n\nJegliche dorthin geschriebenen Daten werden verworfen, beim Lesezugriff darauf wird ein einzelnes -Zeichen (codice_1) ausgegeben.\n\nDas Verhalten und die Gerätedatei selbst ist durch den POSIX-Standard standardisiert.\n\nDas Device dient – wie alle Implementierungen des Nulldevice – hauptsächlich dazu, Ausgaben zu verwerfen. Darüber hinaus kann das Nulldevice allerdings auch als Pseudo-Adressat dienen. Zum Beispiel kopiert der Befehl\n\ndie angesprochene Datei effektiv nirgendwohin, aber durch das für den Vorgang notwendige Lesen der Datei wird einerseits deren vollständige Lesbarkeit festgestellt (dient also als eine Art \"selektives fsck\"), andererseits wird die Datei durch das Lesen in den Diskcache geladen wodurch eine eventuelle spätere Verwendung beschleunigt wird.\n\nLesezugriffe liefern ausschließlich ein einzelnes End-of-File-Symbol, weshalb es auch zur Erzeugung „leerer Eingaben“ verwendet werden kann.\n\nIn der Netzkultur ist codice_2 ein umgangssprachlich verwendeter Begriff für ein – oft gedankliches – Schwarzes Loch, meist um Desinteresse an oder Verachtung für einen Gegenstand zu bekunden.\n\n"}
{"id": "107260", "url": "https://de.wikipedia.org/wiki?curid=107260", "title": "Rechnerverbund", "text": "Rechnerverbund\n\nEin Rechnerverbund oder Computercluster, meist einfach \"Cluster\" genannt (vom Englischen für „Rechner-Schwarm“, „-Gruppe“ oder „-Haufen“), bezeichnet eine Anzahl von vernetzten Computern. Der Begriff wird zusammenfassend für zwei unterschiedliche Aufgaben verwendet: die Erhöhung der Rechenkapazität (HPC-Cluster, engl. high performance computing – Hochleistungsrechnen) und die Erhöhung der Verfügbarkeit (HA-Cluster, engl. high available – hochverfügbar). Die in einem Cluster befindlichen Computer (auch \"Knoten\", vom englischen \"nodes\" oder Server) werden auch oft als Serverfarm bezeichnet.\n\nDer Begriff \"Cluster\" beschreibt primär die Architektur der einzelnen Bausteine und ihr Zusammenwirken.\nHardware- oder Software-Cluster sind grundsätzlich unterschiedlich. Die einfache Form eines Hardware-Clusters ist als aktiv/passiv bekannt. Andere Varianten sind als cascading bekannt. Dabei muss eine Unterbrechung des Services mit berücksichtigt werden. HP OpenVMS Cluster sind in der Lage, eine Hardware-aktiv/aktiv-Funktionalität zu implementieren.\n\nSoftwarecluster oder Applikationscluster hingegen sind eher in der Lage, einen kontinuierlichen Betrieb zu realisieren (Beispiel: DNS-Server). Es hängt aber vom Client in der Client/Server-Architektur ab, ob er mit der Umschaltung des Dienstes (bzw. Service) umgehen kann.\n\nMan unterscheidet zwischen sogenannten homogenen und heterogenen Clustern. Computer homogener Cluster laufen unter dem gleichen Betriebssystem und gleicher Hardware, beim heterogenen Cluster können unterschiedliche Betriebssysteme oder Hardware eingesetzt werden. Bekannte Linux-Cluster-Software sind z. B. HP Serviceguard, Beowulf und openMosix.\n\nHochverfügbarkeitscluster (engl. High-Availability-Cluster – HA-Cluster) werden zur Steigerung der Verfügbarkeit bzw. für bessere Ausfallsicherheit eingesetzt. Tritt auf einem Knoten des Clusters ein Fehler auf, werden die auf diesem Knoten laufenden Dienste auf einen anderen Knoten migriert. Die meisten HA-Cluster besitzen 2 Knoten. Es existieren Cluster, bei denen ständig auf allen Knoten Dienste laufen. Diese Cluster nennt man aktiv-aktiv bzw. symmetrisch. Sind nicht alle Knoten aktiv, spricht man von aktiv-passiv oder asymmetrisch. Sowohl die Hardware als auch die Software eines HA-Clusters muss frei von Single-Point-of-Failures (Komponenten, die durch einen Fehler das gesamte System zum Ausfall brächten) sein. Anwendung finden solche HA-Cluster in kritischen Umgebungen, in denen Ausfallzeiten von nur wenigen Minuten im Jahr erlaubt sind. Im Rahmen von Katastrophenszenarien müssen kritische Computersysteme abgesichert werden. Dazu werden die Cluster-Knoten oft mehrere Kilometer auseinander in verschiedenen Rechenzentren platziert. Im Katastrophenfall kann der Knoten im nicht betroffenen Rechenzentrum die gesamte Last übernehmen. Diese Art von Clustern nennt man auch „stretched Cluster“.\n\nLoad-Balancing-Cluster werden zum Zweck der Lastverteilung auf mehrere Maschinen aufgebaut. Die Lastverteilung erfolgt in der Regel über eine redundant ausgelegte, zentrale Instanz. Mögliche Einsatzgebiete sind Umgebungen mit hohen Anforderungen an Computerleistung. Der Leistungsbedarf wird hier nicht durch Aufrüstung einzelner Computer abgedeckt, sondern durch das Hinzufügen zusätzlicher Computer. Grund für die Verwendung ist nicht zuletzt der Einsatz von preisgünstigen Standardcomputern (COTS-Komponenten) anstatt von teuren Spezialcomputern.\n\nHigh-Performance-Computing-Cluster (HPC-Cluster) dienen zur Abarbeitung von Rechenaufgaben. Diese Rechenaufgaben werden auf mehrere Knoten aufgeteilt. Entweder werden die Aufgaben in verschiedene Pakete aufgeteilt und parallel auf mehreren Knoten ausgeführt oder die Rechenaufgaben (Jobs genannt) werden auf die einzelnen Knoten verteilt. Die Aufteilung der Jobs übernimmt dabei meistens ein Job Management System. HPC-Cluster finden sich oft im wissenschaftlichen Bereich. In der Regel sind die einzelnen Elemente eines Clusters untereinander über ein schnelles Netzwerk verbunden. Auch die sogenannten Renderfarmen fallen in diese Kategorie.\n\nDas erste im Handel erhältliche Clusterprodukt war ARCNET, welches 1977 von Datapoint entwickelt wurde. Den ersten wirklichen Erfolg hatte das Unternehmen DEC im Jahr 1983 mit der Vorstellung des Produktes VAXCluster für ihr Computersystem VAX. Das Produkt unterstützte nicht nur paralleles Rechnen auf den Clusterknoten, sondern auch die gemeinsame Nutzung von Dateisystemen und Geräten aller beteiligten Knoten. Diese Eigenschaften fehlen noch heute bei vielen freien und kommerziellen Produkten. VAXCluster ist als „VMSCluster“ auch heute noch von der Firma HP für das Betriebssystem OpenVMS und die Prozessoren Alpha und Itanium erhältlich.\n\nDie failover-Funktion wird meist durch das Betriebssystem zur Verfügung gestellt (Servicefailover, IP-Übernahme). Die Übernahme von Diensten kann z. B. durch die automatische Migration von IP-Adressen oder das Verwenden einer Multicastadresse erreicht werden.\n\nGenerell wird zwischen den Architekturen shared nothing und shared all unterschieden.\n\nTypischer Vertreter des „active-active“-Clusters mit shared-nothing-Architektur ist DB2 mit EEE (gesprochen „triple e“). Hier beherbergt jeder Clusterknoten eine eigene Datenpartition. Ein Leistungsgewinn wird durch die Partitionierung der Daten und die damit einhergehende verteilte Verarbeitung erzielt. Ausfallsicherheit wird hiermit nicht gewährleistet.\n\nAnders ist dies beim „shared-all“-Cluster. Diese Architektur gewährleistet durch einen konkurrierenden Zugriff auf Shared Storage, dass alle Clusterknoten auf den gesamten Datenbestand zugreifen können. Neben Skalierung und Leistungssteigerung wird durch diese Architektur auch eine zusätzliche Ausfallsicherheit erreicht. Fällt ein Knoten aus, übernehmen die anderen Knoten seine Aufgabe(n). Ein typischer Vertreter der shared-all-Architektur ist der Oracle Real Application Cluster (RAC).\n\nHA-Computercluster können auch ohne lokale Datenträger direkt aus einem Storage Area Network (SAN) heraus als ein „Single System Image“ booten. Solche Diskless Shared Root Cluster erleichtern den Austausch von Cluster-Knoten, die in einer solchen Konfiguration nur noch ihre Rechenleistung und I/O Bandbreite zur Verfügung stellen.\n\nDienste müssen speziell für den Einsatz auf einem Cluster programmiert sein. Ein Dienst wird als „cluster aware“ bezeichnet, wenn er auf spezielle Ereignisse (wie z. B. den Ausfall eines Clusterknotens) reagiert und diese in geeigneter Weise verarbeitet.\n\nCluster-Software kann in Form von Skripten implementiert oder auch in den Betriebssystemkernen integriert sein.\n\nBei HPC-Clustern wird die zu erledigende Aufgabe, der „Job“, oft mittels eines Decomposition-Programms in kleinere Teile zerlegt und dann auf die Knoten verteilt.\n\nDie Kommunikation zwischen Job-Teilen, die auf verschiedenen Knoten laufen, geschieht in der Regel mittels Message Passing Interface (MPI), da eine schnelle Kommunikation zwischen einzelnen Prozessen gewünscht ist. Dazu koppelt man die Knoten mit einem schnellen Netzwerk wie z. B. InfiniBand.\n\nEine gängige Methode zur Verteilung von Jobs auf einen HPC-Cluster ist ein Job-Scheduling-Programm, welches eine Verteilung nach verschiedenen Kategorien vornehmen kann, wie z. B. Load Sharing Facility (LSF) oder Network Queueing System (NQS).\n\nDie TOP500 der Superrechner sind zu über 90 % Linux-Cluster, nicht zuletzt weil sich auch für anspruchsvolle Rechenaufgaben billige COTS Hardware nutzen lässt.\n\n\n\n\n"}
{"id": "107562", "url": "https://de.wikipedia.org/wiki?curid=107562", "title": "Su (Unix)", "text": "Su (Unix)\n\nDas Programm su (Abkürzung für ) dient auf Unix- und unixoiden Betriebssystemen dazu, den Benutzerkontext zu wechseln und damit Programme unter einer anderen Benutzer-ID auszuführen. Es gehört zu den essentiellen Systemverwaltungswerkzeugen von Unix-Systemen.\n\nDie Syntax ist codice_1. Ohne Angabe eines Benutzernamens wechselt codice_2 nach einer Passwortabfrage zum Benutzer root. Die Befehle codice_3 oder codice_4 simulieren einen kompletten Login-Vorgang, d. h., es werden das komplette Profil des neuen Benutzers eingelesen und die Umgebungsvariablen neu gesetzt. Um Missbrauch zu verhindern, zeichnen die meisten Systeme alle geglückten und missglückten Login-Versuche mit su in den System-Logdateien auf.\n\nMit dem Befehl codice_5 oder durch Senden des -Signals/​-Zeichens (EOF) durch Drücken der Tastenkombination + gelangt man zum ursprünglichen Benutzer und seinen Rechten zurück.\n\nBei einigen Systemen ist das root-Konto standardmäßig deaktiviert; statt codice_2 wird codice_7 empfohlen, das keine eigene Subshell startet, sondern lediglich den mitangegebenen Befehl unter Administratorrechten ausführt.\n\nDie meisten BSD-Varianten einschließlich macOS beschränken den Gebrauch von codice_2 auf Mitglieder der Gruppen \"wheel\" und \"admin\". Aufgrund einer Entscheidung von GNU-Projekt-Gründer Richard Stallman wurde diese Eigenschaft nicht in die GNU-Implementierung des Programms, die David MacKenzie programmierte, übernommen. Stallman argumentierte, dass eine derartige Beschränkung die Macht einiger weniger Systemadministratoren zementiere.\n\nDas Kommando taucht bereits im AT&T UNIX Time-Sharing System auf.\n\n"}
{"id": "107935", "url": "https://de.wikipedia.org/wiki?curid=107935", "title": "WxWidgets", "text": "WxWidgets\n\nwxWidgets (ehemals wxWindows) ist ein quelloffenes GUI-Toolkit zur Entwicklung grafischer Benutzeroberflächen. Es wird in der Programmiersprache C++ entwickelt und unter einer modifizierten LGPL lizenziert, die auch das Verbreiten von abgeleiteten Werken unter eigenen Bedingungen erlaubt.\nObwohl wxWidgets in C++ implementiert ist, existieren Anbindungen für eine Vielzahl weiterer Programmiersprachen. Durch seine Plattformunabhängigkeit ermöglicht wxWidgets, den GUI-spezifischen Code eines Programms bei keiner oder nur geringer Modifikation auf verschiedensten Plattformen zu kompilieren und auszuführen.\n\nwxWidgets wird in einer Reihe bekannter Projekte verwendet, beispielsweise durch den Audioeditor Audacity, das Strategiespiel 0 A.D., den FTP-Client FileZilla und die Entwicklungsumgebung .\n\nDie Programmierschnittstelle ermöglicht die Entwicklung von GUI-Programmen für Windows, Unix, Mac, Symbian OS, Palm OS und Windows CE. wxWidgets abstrahiert plattformabhängige Funktionen, beispielsweise die Interprozesskommunikation (IPC), und bietet zusätzliche Funktionen an, zum Beispiel eine Hashtabelle. Einige Funktionen wie zum Beispiel \"wxMetafile\" oder OLE werden für die jeweilige Plattform einzeln in wxWidgets implementiert. Für bessere Portabilität verzichtet wxWidgets auf Ausnahmen (englisch: \"exceptions\") und Templates. Die API umfasst über 450 Klassen mit über 5000 Funktionen. Wichtigste Funktionalitäten decken folgende Bereiche ab:\n\nIm Jahr 1992 erschien die erste Ausgabe, welche für XView und MFC war.\n\nVon 1993 bis 1995 wurden Motif und Xt-Port, wxPython entwickelt und veröffentlicht. Ab 1998 bis 2002 wurde wxGTK Port eingesetzt, wxMac 2.0 Port, wxX11 Port und OS/2 Port begonnen.\n\n2003 wurde \"wxWidgets Software Foundation\" gegründet, welche nach einigen Monaten wieder aufgelöst wurde. Ein Jahr später wurde wxWindows in wxWidgets auf Bitte und durch Bezahlung von Microsoft umbenannt.\n\nIm Jahr 2005 fand die Veröffentlichung von wxWidgets 2.6.0 statt. Es löste die vorherige „stable“ Version 2.4.2 von Mitte 2003 ab. Ende 2006 ist die Version 2.8.0 erschienen, im September 2009 wurde die Version 2.9.0 veröffentlicht.\n\n\n\n"}
{"id": "108289", "url": "https://de.wikipedia.org/wiki?curid=108289", "title": "IEEE 754", "text": "IEEE 754\n\nDie Norm IEEE 754 (ANSI/IEEE Std 754-1985; IEC-60559:1989 – International version) definiert Standarddarstellungen für binäre Gleitkommazahlen in Computern und legt genaue Verfahren für die Durchführung mathematischer Operationen, insbesondere für Rundungen, fest. Der genaue Name der Norm ist .\n\nDie aktuelle Ausgabe ist unter der Bezeichnung ANSI / IEEE Std 754-2008 im August 2008 veröffentlicht worden und umfasst neben der 754-1985 eine Erweiterung um zusätzlich ein binäres und zwei dezimale Datenformate. Weiter ist die Norm IEEE 854-1987, mit dem engl. Titel , in der IEEE 754-2008 vollständig integriert worden.\n\nIn der Norm IEEE 754-1989 werden zwei Grunddatenformate für binäre Gleitkommazahlen mit 32 Bit (\"single precision\") bzw. 64 Bit (\"double precision\") Speicherbedarf und zwei erweiterte Formate definiert. Die IEEE 754-2008 umfasst die binären Zahlenformate mit 16 Bit als Minifloat, 32 Bit als \"single\", 64 Bit als \"double\" und neu 128 Bit. Zusätzlich kamen noch die dezimale Darstellungen mit 32 Bit als Minifloat, 64 und 128 Bit hinzu.\n\nSchließlich gab es Vorschläge und Implementierungen von weiteren Zahlenformaten, die nach den Prinzipien der IEEE 754-1989 Norm gestaltet sind und deshalb oft als IEEE-Zahlen bezeichnet werden, obwohl sie das nach der alten Definition streng genommen nicht sind. Dazu gehören die in den neuen Ausgaben integrierten Minifloats, die für die Ausbildung gedacht sind. Minifloats mit 16 Bit werden gelegentlich in der Grafikprogrammierung verwendet. Ebenso gehören auch mehrere nicht von IEEE 754-1989 definierte Zahlenformate mit mehr als 64 Bit, etwa das 80-Bit-Format (Extended Precision ), welches die IA-32-Prozessoren intern in ihrer klassischen Gleitkommaeinheit (Floating Point Unit, FPU) verwenden, dazu.\n\nDie Darstellung einer Gleitkommazahl\nbesteht aus:\n\nDas Vorzeichen formula_9 wird in einem Bit formula_10 gespeichert, sodass formula_11 positive Zahlen und formula_12 negative Zahlen markiert.\n\nDer Exponent formula_7 wird als nichtnegative Binärzahl formula_14 (formula_14 wird manchmal auch als Charakteristik oder biased exponent bezeichnet) gespeichert, indem man den festen Biaswert formula_16 addiert:\nformula_17.\nDer Biaswert (engl: Verzerrung) berechnet sich durch formula_18. Der Biaswert formula_16 dient also dazu, dass negative Exponenten durch eine vorzeichenlose Zahl (die Charakteristik formula_14) gespeichert werden können, unter Verzicht auf alternative Kodierungen wie z. B. das Zweierkomplement (vergleiche auch Exzesscode).\n\nSchließlich ist die Mantisse formula_21 ein Wert, der sich aus den formula_4 Mantissenbits mit dem Wert formula_23 als formula_24 berechnet. Einfacher ausgedrückt denkt man sich an das Mantissenbitmuster formula_23 links eine „1,“ angehängt: formula_26.\n\nDieses Verfahren ist möglich, weil durch Normalisierung (s. u.) die Bedingung formula_30 für alle darstellbaren Zahlen immer eingehalten werden kann. Da dann die Mantisse immer links mit „1,“ beginnt, braucht dieses Bit nicht mehr gespeichert zu werden. Damit gewinnt man ein zusätzliches Bit Genauigkeit.\n\nFür Sonderfälle stehen spezielle Bitmuster zur Verfügung. Um diese Sonderfälle zu kodieren, sind zwei Exponentenwerte reserviert, der maximale (formula_31) und die Null (formula_32). Mit dem maximalen Exponentenwert werden die Sonderfälle NaN und ∞ kodiert. Mit Null im Exponenten werden die Gleitkommazahl 0 und alle denormalisierten Werte kodiert.\n\nWerte außerhalb des normalen Wertebereichs (zu große bzw. zu kleine Zahlen) werden durch ∞ bzw. −∞ dargestellt. Diese Erweiterung des Wertebereichs erlaubt auch im Falle eines arithmetischen Überlaufs häufig ein sinnvolles Weiterrechnen. Neben der Zahl 0 existiert noch der Wert −0. Während formula_33 das Ergebnis ∞ liefert, ergibt formula_34 den Wert −∞. Bei Vergleichen wird zwischen 0 und −0 nicht unterschieden.\n\nDie Werte NaN (für engl. „not a number“, „keine Zahl“) werden als Darstellung für undefinierte Werte verwendet. Sie treten z. B. auf als Ergebnisse von Operationen wie formula_35 oder formula_36 auf. NaN werden in Signal-NaN (signalling NaN, NaNs) für Ausnahmebedingungen und stille NaN (quiet NaN, NaNq) unterteilt.\n\nAls letzter Sonderfall füllen denormalisierte Zahlen (in IEEE 754r als subnormale Zahlen bezeichnet) den Bereich zwischen der betragsmäßig kleinsten normalisierten Gleitkommazahl und Null. Sie werden als Festkommazahlen gespeichert und weisen nicht dieselbe Genauigkeit auf wie die normalisierten Zahlen. Konstruktionsbedingt haben die meisten dieser Werte den Kehrwert ∞.\n\nIEEE 754 unterscheidet vier Darstellungen: einfach genaue (single), erweiterte einfach genaue (single extended), doppelt genaue (double) und erweiterte doppelt genaue (double extended) Zahlenformate. Bei den erweiterten Formaten ist nur jeweils eine Mindestbitzahl vorgeschrieben. Die genaue Bitzahl und der Biaswert bleiben dem Implementierer überlassen. Die Grundformate sind vollständig definiert.\n\nDie Anzahl der Exponentenbits legt den Wertebereich der darstellbaren Zahlen fest (s. u.). Die Anzahl der Mantissenbits legt die Genauigkeit dieser Zahlen fest.\n\nDie beiden letzten Beispiele demonstrieren ein minimales erweitertes Format.\n\nFür die angegebenen Formate ergibt sich die folgende Beschränkung des jeweiligen Zahlenbereichs. Die betragsmäßig kleinsten Zahlen sind hierbei nicht normalisiert. Der relative Abstand zweier Gleitkommazahlen ist größer als formula_37 und kleiner gleich formula_38. Konkret ist der Abstand (und in diesem Fall auch der relative Abstand) der Gleitkommazahl formula_39 zur nächstgrößeren Gleitkommazahl gleich formula_38. Dezimalstellen beschreibt die Anzahl der Stellen einer Dezimalzahl, die ohne Genauigkeitsverlust gespeichert werden können. Die Mantisse ist rechnerisch durch das implizite Bit um eins größer als gespeichert.\n\nDie Anordnung der Bits einer \"single\" zeigt die nachfolgende Abbildung. Die bei einer Rechenanlage konkrete Anordnung der Bits im Speicher kann von diesem Bild abweichen und hängt von der jeweiligen Bytereihenfolge (little/big endian) und weiteren Rechnereigenheiten ab.\n\nDie Anordnung mit \"Vorzeichen – Exponent – Mantisse\" in genau dieser Reihenfolge bringt (innerhalb eines Vorzeichenbereiches) die dargestellten Gleitkommawerte in dieselbe Reihenfolge wie die durch dasselbe Bitmuster darstellbaren Ganzzahlwerte. Damit können für die Vergleiche von Gleitkommazahlen dieselben Operationen wie für die Vergleiche von ganzen Zahlen verwendet werden. Kurz: die Gleitkommazahlen können lexikalisch sortiert werden.\n\nHierbei ist jedoch zu beachten, dass für steigende negative Ganzzahlwerte der entsprechende Gleitkommawert gegen minus unendlich geht, die Sortierung also umgekehrt ist.\n\nAuch wenn in diesem Artikel hauptsächlich das Zahlenformat erörtert wird, liegt die Bedeutung der Norm IEEE 754 auch darin, dass für Gleitkommazahlen genaue Vorschriften für\nfestgelegt wurden.\n\nDie Zahl formula_41 soll in eine Gleitkommazahl umgewandelt werden, dabei nutzen wir den Single IEEE-Standard.\n\nNun soll die Gleitkommazahl von oben wieder in eine Dezimalzahl zurück gewandelt werden, gegeben ist also folgende IEEE754-Zahl:\n\nformula_42\n\nDie Interpretation hängt von dem Exponenten ab. Zur Erläuterung wird mit \"S\" der Wert des Vorzeichenbits (0 oder 1), mit \"E\" der Wert des Exponenten als nichtnegative ganze Zahl zwischen 0 und \"E\" = 11…111 = 2−1, mit \"M\" der Wert der Mantisse als nichtnegative Zahl und mit \"B\" der Biaswert bezeichnet. Die Zahlen \"r\" und \"p\" bezeichnen die Anzahl der Exponentenbits und Mantissenbits.\n\nNull repräsentiert die vorzeichenbehaftete Null. Auch Zahlen, die zu klein sind, um dargestellt zu werden (Unterlauf), werden auf Null gerundet. Ihr Vorzeichen bleibt dabei erhalten. Negative kleine Zahlen werden so zu −0,0 gerundet, positive Zahlen zu +0,0. Beim direkten Vergleich werden jedoch +0,0 und −0,0 als gleich angesehen.\n\nDie Mantisse besteht aus den ersten \"n\" wesentlichen Ziffern der Binärdarstellung der noch nicht normalisierten Zahl. Die erste wesentliche Ziffer ist die höchstwertige (d. h. am weitesten links stehende) Ziffer, die von 0 verschieden ist. Da eine von 0 verschiedene Ziffer im Binärsystem nur eine 1 sein kann, muss diese erste 1 nicht explizit abgespeichert werden; gemäß der Norm IEEE 754 werden nur die \"folgenden\" Ziffern gespeichert, die erste Ziffer ist eine implizite Ziffer oder ein implizites Bit (engl. ). Dadurch wird gewissermaßen 1 Bit Speicherplatz „gespart“.\n\nIst eine Zahl zu klein, um in normalisierter Form mit dem kleinsten von Null verschiedenen Exponenten gespeichert zu werden, so wird sie als „denormalisierte Zahl“ gespeichert. Ihre Interpretation ist nicht mehr ±1,\"mantisse\"·2 sondern ±0,\"mantisse\"·2. Dabei ist \"de\" der Wert des kleinsten „normalen“ Exponenten. Damit lässt sich die Lücke zwischen der kleinsten normalisierten Zahl und Null füllen. Denormalisierte Zahlen haben jedoch eine geringere (relative) Genauigkeit als normalisierte Zahlen; die Anzahl der signifikanten Stellen in der Mantisse nimmt zur Null hin ab.\n\nIst das Ergebnis (oder Zwischenergebnis) einer Rechnung kleiner als die kleinste darstellbare Zahl der verwendeten endlichen Arithmetik, so wird es im Allgemeinen auf Null gerundet; das nennt man Unterlauf der Gleitkommaarithmetik, engl. . Da dabei Information verloren geht, versucht man, Unterlauf nach Möglichkeit zu vermeiden. Die denormalisierten Zahlen in IEEE 754 bewirken einen allmählichen Unterlauf (engl. ), indem „um die 0 herum“ 2 (für \"single\") bzw. 2 (für \"double\") Werte eingefügt werden, die alle denselben absoluten Abstand voneinander haben und ohne diese denormalisierten Werte nicht darstellbar wären, sondern zu Unterlauf führen müssten.\n\nProzessorseitig sind denormalisierte Zahlen aufgrund ihres proportional seltenen Auftretens mit wenig Priorität implementiert und führen deswegen zu einer deutlichen Verlangsamung der Ausführung, sobald sie als Operand oder als Ergebnis einer Berechnung auftauchen. Um Abhilfe (z. B. für Computerspiele) zu schaffen, bietet Intel seit SSE2 die nicht-IEEE 754 konforme Funktionalität an, denormalisierte Zahlen vollständig zu deaktivieren. Gleitkommazahlen, die in diesen Bereich gelangen, werden auf 0 gerundet.\n\nDer Gleitkommawert Unendlich repräsentiert Zahlen, deren Betrag zu groß ist, um dargestellt zu werden. Es wird zwischen positiver Unendlichkeit und negativer Unendlichkeit unterschieden. Die Berechnung von 1,0/0,0 ergibt \"per Definition\" die positive Unendlichkeit.\n\nDamit werden ungültige (oder nicht definierte) Ergebnisse dargestellt, z. B. wenn versucht wurde, die Quadratwurzel aus einer negativen Zahl zu berechnen. Einige „unbestimmte Ausdrücke“ haben als Ergebnis „keine Zahl“, zum Beispiel 0,0/0,0 oder „Unendlich“ − „Unendlich“. Außerdem werden \"NaN\"s in verschiedenen Anwendungsbereichen benutzt, um „Kein Wert“ oder „Unbekannter Wert“ darzustellen. Insbesondere der Wert mit dem Bitmuster 111…111 wird oft für eine „nicht initialisierte Gleitkommazahl“ benutzt.\n\nIEEE 754 fordert zwei Arten von Nichtzahlen: stille NaN (NaNq – \"quiet\") und signalisierende NaN (NaNs – \"signalling\"). Beide stellen explizit keine Zahlen dar. Eine signalisierende NaN löst im Gegensatz zu einer stillen NaN eine Ausnahme (Trap) aus, wenn sie als Operand einer arithmetischen Operation auftritt.\n\nIEEE 754 ermöglicht dem Anwender das Deaktivieren dieser Traps. In diesem Falle werden signalisierende NaN wie stille NaN behandelt.\n\nSignalisierende NaN können genutzt werden, um uninitialisierten Rechnerspeicher zu füllen, so dass jedes Verwenden einer uninitialisierten Variable automatisch eine Ausnahme auslöst.\n\nStille NaN ermöglichen den Umgang mit Rechnungen, die kein Ergebnis erzeugen können, etwa weil sie für die angegebenen Operanden nicht definiert sind. Beispiele sind die Division Null durch Null oder der Logarithmus aus einer negativen Zahl.\n\nStille und Signalisierende NaN unterscheiden sich im höchsten Mantissenbit. Bei stillen NaN ist dieses 1, bei signalisierenden NaN 0. Die übrigen Mantissenbits können zusätzliche Informationen enthalten, z. B. die Ursache der NaN. Dies kann bei der Ausnahmebehandlung hilfreich sein. Allerdings schreibt der Standard nicht fest, \"welche\" Informationen in den übrigen Mantissenbits enthalten sind. Die Auswertung dieser Bits ist daher plattformabhängig.\n\nDas Vorzeichenbit hat bei NaN keine Bedeutung. Es ist nicht spezifiziert, welchen Wert das Vorzeichenbit bei zurückgegebenen NaN besitzt.\n\nIEEE 754 unterscheidet zunächst zwischen binären Rundungen und binär-dezimalen Rundungen, bei denen geringere Qualitätsforderungen gelten.\n\nBei binären Rundungen muss zur nächstgelegenen darstellbaren Zahl gerundet werden. Wenn diese nicht eindeutig definiert ist (genau in der Mitte zwischen zwei darstellbaren Zahlen), wird so gerundet, dass das niederwertigste Bit der Mantisse 0 wird. Statistisch wird dabei in 50 % der Fälle auf-, in den anderen 50 % der Fälle abgerundet, so dass die von Knuth beschriebene statistische Drift in längeren Rechnungen vermieden wird.\n\nEine zu IEEE 754 konforme Implementierung muss drei weitere vom Programmierer einstellbare Rundungen bereitstellen: Rundung gegen +Unendlich (immer aufrunden), Rundung gegen −Unendlich (immer abrunden) und Rundung gegen 0 (Ergebnis immer betragsmäßig verkleinern).\n\nZu IEEE 754 konforme Implementierungen müssen Operationen für Arithmetik, Berechnung der Quadratwurzel, Konversionen und Vergleiche bereitstellen. Eine weitere Gruppe von Operationen wird im Anhang empfohlen, jedoch nicht verbindlich vorgeschrieben.\n\nIEEE 754 verlangt von einer (Hardware- oder Software-)Implementierung exakt gerundete Ergebnisse für die Operationen Addition, Subtraktion, Multiplikation und Division zweier Operanden sowie der Operation Quadratwurzel eines Operanden. Das heißt, das ermittelte Ergebnis muss gleich demjenigen sein, das bei einer exakten Ausführung der entsprechenden Operation mit anschließender Rundung entsteht.\n\nWeiter ist die Berechnung des Restes nach einer Division mit ganzzahligem Ergebnis gefordert. Diese Restberechnung ist definiert durch formula_43, formula_44 ganzzahlig, formula_45 oder bei geradem formula_44 auch formula_47. Dieser Rest muss ohne Rundung exakt ermittelt werden.\n\nKonversionen werden zwischen allen unterstützten Gleitkommaformaten gefordert. Bei einer Konversion in ein Gleitkommaformat mit kleinerer Genauigkeit muss wie schon unter Arithmetik beschrieben exakt gerundet werden.\n\nZu IEEE 754 konforme Implementierungen müssen Konversionen zwischen allen unterstützten Gleitkommaformaten und allen unterstützen ganzzahligen Formaten bereitstellen. Die ganzzahligen Formate werden in IEEE 754 jedoch nicht genauer definiert.\n\nZu jedem unterstützten Gleitkommaformat muss eine Operation existieren, die diese Gleitkommazahl in die exakt gerundete ganze Zahl im selben Gleitkommaformat konvertiert.\n\nSchließlich müssen Konversionen zwischen dem binären Gleitkommaformat und einem Dezimalformat existieren, die genau beschriebenen Mindestqualitätsforderungen genügt.\n\nGleitkommazahlen nach IEEE 754 müssen verglichen werden können. Die Norm definiert die notwendigen Vergleichsoperationen und für alle möglichen Sonderfälle (vor allem NaN, Unendlich und 0) die geforderten Ergebnisse. Gegenüber den „schulmathematischen“ Vergleichen (kleiner, gleich oder größer) kommt als mögliches Ergebnis nach IEEE 754 vor allem \"unordered\" („nicht eingeordnet“) hinzu, wenn einer der Vergleichsoperanden NaN ist. Zwei NaN sind prinzipiell verschieden, auch wenn ihre Bitmuster übereinstimmen.\n\nIm Anhang der Norm werden zehn weitere Operationen empfohlen. Da sie in einer Implementierung im Grunde sowieso benötigt werden, läuft diese Empfehlung letztlich darauf hinaus, die Operationen an den Programmierer weiterzugeben. Diese Operationen sind (in C-Schreibweise):\ncopysign (x, y), invertsign (x), scalb (y, n), logb (x), nextafter (x, y), finite (x), isnan (x), x ≠ y, unordered (x, y), class (x). Die Details der Implementierung vor allem wieder bei den Sonderfällen NaN usw. sind ebenfalls vorgeschlagen.\n\nTreten bei der Berechnung Ausnahmen (Exceptions) auf, werden Status-Flags gesetzt. Im Standard wird vorgeschrieben, dass der Benutzer diese Flags lesen und schreiben kann. Die Flags sind „sticky“: werden sie einmal gesetzt, bleiben sie so lange erhalten, bis sie explizit wieder zurückgesetzt werden. Das Überprüfen der Flags ist beispielsweise die einzige Möglichkeit, 1/0 (=Unendlich) von einem Überlauf zu unterscheiden.\n\nDes Weiteren wird im Standard empfohlen, Trap Handler zu ermöglichen: Tritt eine Ausnahme auf, wird der Trap Handler aufgerufen, anstatt das Status-Flag zu setzen. Es liegt in der Verantwortung solcher Trap Handler, das entsprechende Status-Flag zu setzen oder zu löschen.\n\nAusnahmen werden im Standard in 5 Kategorien eingeteilt: Überlauf, Unterlauf, Division durch Null, ungültige Operation und Ungenau. Für jede Klasse steht ein Status-Flag zur Verfügung.\n\nIn den 1960er und frühen 1970er Jahren hatte jeder Prozessor sein eigenes Format für Gleitkommazahlen und seine eigene FPU oder Gleitkommasoftware, mit der das jeweilige Format verarbeitet wurde. Dasselbe Programm konnte auf verschiedenen Rechnern unterschiedliche Resultate liefern. Die Qualität der verschiedenen Gleitkommaarithmetiken war ebenfalls sehr unterschiedlich.\n\nIntel plante um 1976 für seine Mikroprozessoren eine eigene FPU und wollte die bestmögliche Lösung für die zu implementierende Arithmetik. Unter der Federführung der IEEE begannen 1977 Treffen, um FPUs für Gleitkommaarithmetik für Mikroprozessoren zu normieren. Das zweite Treffen fand im November 1977 unter dem Vorsitz von Richard Delp in San Francisco statt. Einer der richtungsweisenden Teilnehmer war William Kahan.\n\nUm 1980 wurde die Anzahl der Vorschläge für die Norm auf zwei reduziert: Der K-C-S Vorschlag (nach seinen Autoren Kahan, Coonen und Stone, 1977) setzte sich letztlich gegen die Alternative von DEC (F-Format, D-Format und G-Format) durch. Ein bedeutender Meilenstein auf dem Weg zur Norm war die Diskussion über die Behandlung des Unterlaufs, der bis dahin von den meisten Programmierern vernachlässigt worden war.\n\nIntel implementierte gleichzeitig mit der Entwicklung der Norm die Normvorschläge weitgehend in der Intel FPU 8087, die als Gleitkomma-Coprozessor zum 8088 Verwendung fand. Die erste Version der Norm wurde 1985 verabschiedet und 2008 erweitert.\n\n\n"}
{"id": "108642", "url": "https://de.wikipedia.org/wiki?curid=108642", "title": "Midgard (Software)", "text": "Midgard (Software)\n\nMidgard ist ein objektbasiertes Software-Framework für datenbankbasierte Web- und Desktop-Anwendungen. Midgard wurde ursprünglich zur Entwicklung von Webanwendungen wie Web-Content-Management-Systemen in PHP konzipiert, bietet mittlerweile aber ebenfalls Sprachanbindungen für die Programmiersprachen Python, Java und C#.\n\nDas Midgard-Projekt nutzt ein an Ubuntu angelehntes Versionsmodell, d. h. zweimal pro Jahr wird eine neue Hauptentwicklungslinie freigegeben. Die Neuerungen der aktuellen 8.09-Serie umfassen unter anderem eine eingebaute Replikations-Schnittstelle und D-Bus-Unterstützung.\n\nEin weiteres Hauptmerkmal des Midgard-Frameworks ist die Datenbank-Abstraktionsschicht \"MgdSchema\", die auf Basis von XML-Beschreibungen die Struktur der Datenbank-Tabellen verwaltet und einzelne Einträge als Objekte in den verschiedenen unterstützten Sprachen zur Verfügung stellt.\n\n\"Midgard\" setzt im Web-Anwendungsfall auf ein LAMP-System auf, und stellt mit dem mitgelieferten CMS-Framework \"MidCOM\" ein Komponentensystem bereit. Durch die mitgelieferten Komponenten können viele Anwendungsfälle wie Blogs, Foren oder Wiki-Systeme sehr einfach integriert werden. Zur Administration wird die Oberfläche \"Asgard\" bereitgestellt.\n\nDurch das eingebaute ACL-System lassen sich verschiedene Rollenkonzepte realisieren, so dass beispielsweise Designer und Seitenautoren unterschiedliche Zugriffsrechte für einzelne Datenbankobjekte haben können.\n\nDie Anwendungs-Logik wird in wiederverwendbaren \"MidCOM Components\" organisiert, deren Datenstruktur über ein System von sog. \"Datamanager Schemas\" weitgehend anpassbar ist. Website-Inhalte werden in \"Topics\" organisiert, die mit der jeweils zuständigen Komponente verbunden sind.\n\nDas Layout wird durch ein System von in PHP, HTML und CSS geschriebenen \"Style Elements\" realisiert, die den Topics zugewiesen werden. Styles und Style Elements sind in einer Baumstruktur organisiert, unterstützen Vererbung und sind mehrfach verwendbar.\n\nDas \"Midgard\"-Framework ist grundsätzlich freie Software; die Lizenzierung der Teilkomponenten der Software unterliegt folgendem Modell:\n\nDie \"Midgard\"-Architektur setzt sich aus folgenden Kern-Komponenten zusammen:\n\n"}
{"id": "108836", "url": "https://de.wikipedia.org/wiki?curid=108836", "title": "Apple Xserve", "text": "Apple Xserve\n\nDer Xserve war ein Server von Apple, der zum Einbau in einen 19″-Rack vorgesehen war. Er wurde nicht mit dem normalen Betriebssystem Mac OS X verkauft, sondern mit Mac OS X Server.\n\nDer erste Xserve wurde im Mai 2002 eingeführt. Im März 2003 führte Apple die Xserve Cluster Nodes ein, die nur über eine einzige Festplatte verfügen und nicht mit Videokarten oder optischen Laufwerken bestückt sind. Einige der schnellsten Supercomputer der Welt waren Xserve-Cluster. Als höchstplatziertes System erreichte die Installation an der Virginia Tech den siebten Platz in der TOP500-Liste von November 2004.\n\n2004 wurde der Xserve G5 eingeführt, der über ein oder zwei Prozessoren vom Typ PowerPC 970 mit 2 GHz verfügt. Aus Gründen der Wärmeableitung können in ihn nur 3 Festplatten eingebaut werden. Der Platz für die vierte Festplatte wurde für die Belüftung benötigt. Zwischen Januar 2005 und August 2006 wurden die Doppelprozessor-Xserve mit einem CPU-Takt von 2,3 GHz ausgeliefert. Auf der WWDC 2006 in San Francisco am 7. August 2006 wurde der neue Xserve mit Intel-Prozessoren vorgestellt. Im Oktober 2006 wurden die Xserve mit einem bis zu 3 GHz starken Quad-Core-64-Bit-„Woodcrest“-Xeon-Prozessor von Intel ausgeliefert. Laut Apple seien diese bis zu 5-mal schneller als die alten Server mit G5-Prozessor.\n\nDer Xserve unterstützte bis zu drei SATA- und SAS-Laufwerke und eine maximale interne Speicherkapazität von bis zu 2,25 TB. Darüber hinaus verfügte er über zwei PCI-Express-Steckplätze mit 8 Lanes und zwei Gigabit-Ethernet-Anschlüsse, die den simultanen Anschluss an mehrere Netzwerke ermöglichen. Wie schon beim Xserve G5 wurde ein internes DVD-ROM/CD-RW-Laufwerk verbaut. Der 256-Bit-Speicher-Controller mit einer Bandbreite von bis zu 21,3 GB/Sek. unterstützt bis zu 32 GB DDR2 Arbeitsspeicher mit 667 MHz.\n\nOptional konnte ein Xserve mit redundantem Netzteil erworben werden. Dies ist ein wichtiges Kriterium für Industrietauglichkeit, z. B. im Bereich von Prozessleitsystemen, welche eine sehr hohe Verfügbarkeit aufweisen müssen. Der Xserve dient häufig als Metadaten-Controller innerhalb einer Xsan-Umgebung in Verbindung mit einem oder mehreren Xserve-RAID. Die Xserve-G5-Cluster-Nodes findet man sehr häufig in Render-Farmen zum Auslagern und Beschleunigen der Rechenleistung im Bereich Videoschnitt, Compositing und 3D.\n\nDie Produktion des Xserve wurde am 31. Januar 2011 eingestellt.\n\nXserve mit PowerPC G4-Prozessoren\n\nXserve mit PowerPC G5-Prozessoren\n\nXserve mit Intel Xeon Core Duo-Prozessoren\nXserve mit Intel Xeon 8-Core Technologie\n"}
{"id": "109711", "url": "https://de.wikipedia.org/wiki?curid=109711", "title": "Z1 (Rechner)", "text": "Z1 (Rechner)\n\nDie Z1 war ein mechanischer Rechner von Konrad Zuse aus dem Jahre 1937. Sie arbeitete als erstes frei programmierbares Rechenwerk mit binären Zahlen und verfügte über viele Rechner-Architekturelemente des späteren Modells Z2, war jedoch wegen mechanischer Probleme unzuverlässig. Ihre Nachfolger, die Zuse Z3, 1941 und Zuse Z4, 1945, waren die ersten universell programmierbaren Computer.\n\nZuse wurde motiviert durch die Idee, zeitaufwändige, aber gut formalisierbare baustatische Berechnungen zu automatisieren.\n\nDie Z1 war privat finanziert. Vor der Umsetzung rief Zuse den Rechenmaschinenfabrikanten Kurt Pannke an und erzählte ihm von seiner Idee. Dieser antwortete: Ab 1935 entwickelte Zuse seine programmgesteuerte Rechenmaschine. Für den Bau belegte er das Wohnzimmer seiner Eltern. Er zog seine Schwester, seinen Freundeskreis und seine Theatergruppe des Akademischen Vereins Motiv für Hilfsarbeiten hinzu. Auch der Vater, Emil Zuse, sägte mit der Laubsäge Blechteile zu. Frei aus dem Kopf montierte Konrad Zuse rund eine Tonne Material zur Maschine zusammen.\n\nDie Umsetzung mittels mechanischer Schaltglieder, die mit einem Staubsaugermotor angetrieben wurden, schien ihm kompakter als mit elektromechanischen Relais. Die Maschine funktionierte im Prinzip wie konzipiert und rechnete korrekt. Allerdings verhakten sich die mechanischen Schaltglieder im Betrieb regelmäßig, so dass die Z1 keine ausreichende Zuverlässigkeit erreichte.\n\nDie Z1 gilt als Vorläufer des modernen Computers, der in einer ähnlichen Form aufgebaut ist. Sie arbeitete als erster Rechner mit binären Zahlen und besaß bereits ein Eingabe-/Ausgabewerk, ein Rechenwerk, ein Speicherwerk und ein Programmwerk, das die Programme von gelochten Filmstreifen ablas.\n\nDie Z1 wurde durch Bombenangriffe auf Berlin 1943/1944 im Zweiten Weltkrieg zerstört. In den Jahren 1986 bis 1989 baute Konrad Zuse die Z1 für das damalige Museum für Verkehr und Technik (heute: Deutsches Technikmuseum Berlin) nach. Auch bei diesem Nachbau verhakten sich wieder die Schaltglieder.\n\n\n\n"}
{"id": "109939", "url": "https://de.wikipedia.org/wiki?curid=109939", "title": "Firebird (Datenbank)", "text": "Firebird (Datenbank)\n\nFirebird ist ein freies Datenbankmanagementsystem (DBMS). Es ist der freie Ableger des weiterhin kommerziell von Embarcadero (ehemals CodeGear, Borland) vertriebenen relationalen Datenbankmanagementsystems \"InterBase\".\n\nDie Abspaltung erfolgte im Jahr 2000, als kurz vor Freigabe der Version 6 des kommerziellen Vorgängers Interbase bei Borland ernsthafte Überlegungen im Gange waren, die Weiterentwicklung einzustellen.\n\nAus Interbase 6.0 wurde Firebird 1.0, wobei dies als eine Bugfix-Version mit nur wenigen Erweiterungen angesehen werden kann. Eine Erweiterung von Firebird 1.0 ist der 64-Bit File I/O, so dass auch Datenbankdateien von mehr als 2 GB Größe erzeugt werden können.\n\nGrundlegende Erweiterungen erfolgten im Firebird-2.0-Zweig. Der gesamte Quellcode von Interbase wurde aus der Sprache C nach C++ übersetzt. Ab Februar 2004 stand die erste Produktiv-Version aus dem Firebird 2.0-Zweig als Firebird 1.5 zur Verfügung. Im November 2006 wurde Firebird 2.0 freigegeben, die aktuelle Version dieses Zweiges ist 2.5.8 vom 5. Januar 2018.\n\nDie Version 3.0 wurde am 19. April 2016 veröffentlicht. Die Hauptziele dieser Version sind die Vereinheitlichung der Server-Architektur sowie die Verbesserung der Unterstützung von Symmetrischen Multiprozessorsystemen (SMP) bzw. von Mehrkernprozessoren.\n\nGrundsätzlich ist der Firebird-Server SQL-92-Entry-Level-1-konform. Firebird 1.5 folgt dem SQL-Standard nach ANSI SQL-99. Firebird 2.0 ist überwiegend konform mit SQL-2003 (17 obligatorische Fähigkeiten fehlen ganz, vor allem Schemata; 9 obligatorische Fähigkeiten werden nur teilweise unterstützt).\n\nIn LibreOffice wird der vorhandene Datenbank Engine durch Firebird SQL ersetzt. In der Version 6.1 von LibreOffice wurde er als Standard Datenbank Engine aktiviert, aber als experimentell deklariert.\n\nDie Firebird-Datenbank gibt es in drei Versionen mit unterschiedlichem Funktionsumfang. Es gibt die SuperServer-, ClassicServer- und EmbeddedServer-Variante. Nicht alle Varianten sind für alle Plattformen erhältlich.\n\nDie SuperServer-Variante ist ein Multithreaded Server-Prozess. Der SuperServer verwaltet alle Benutzeranfragen und Verbindungen mittels voneinander unabhängigen Threads innerhalb eines Prozesses. Unter Windows (ab NT4) kann der SuperServer als Dienst laufen oder generell auch als Applikation.\n\nMit dem ClassicServer werden alle Verbindungen in getrennten Prozessen verwaltet. Jeder Prozess verwaltet dabei seinen eigenen Datenbank-Cache. Durch die Trennung nach Prozessen eignet sich der ClassicServer gut für Multiprozessor-Umgebungen. Er verbraucht allerdings mehr Arbeitsspeicher.\n\nMit der Embedded-Variante ist es möglich, einer einzelnen Applikation exklusiven Zugriff auf eine Datenbank zu ermöglichen. Diese Servervariante eignet sich sehr gut für Einzelanwendungen, die mit einer eigenen Datenbank laufen und keinen Mehrbenutzermodus benötigen. Die Embedded-Version benötigt keine Installation und hat ausschließlich nur eine Programmbibliothek für die verfügbaren Plattformen.\n\nAb der Version 2.1.1 (September 2008) sind für Solaris und ab der Version 2.1.3 vom September 2009 auch für Windows und Linux getrennte 32- und 64-bit-Versionen verfügbar. Die FreeBSD- und AIX-Releases stehen noch auf dem 32-bit-Versionsstand 1.5, Firebird für HP-UX ist ebenfalls nur in einer 32-bit-Version 2.0.3 erschienen.\n\nFür den produktiven 24-Stunden-Betrieb erlaubt das mitgelieferte Backup-Programm (gbak) Online-Backups, ohne eine Datenbank herunterfahren zu müssen. Ab Version 2.0 sind nun auch inkrementelle Sicherungen möglich (nbackup). Um Zugriff auf einen Firebird-Server zu erhalten, muss der Standard-Port 3050 für TCP- und UDP-Zugriffe erreichbar sein.\n\nNeben dem eigentlichen Datenbankserver gibt es mittlerweile auch Treiber und Bibliotheken für Delphi, Free Pascal, Perl, Python, ODBC, .NET, PHP, Qt, C++ und Java (als Typ2- und Typ4-JDBC-Treiber) für den Zugriff auf die Firebird-Datenbank.\n\nSupport kann entweder kostenlos über die Community oder über kommerzielle Angebote erfolgen.\n\nDie Firebird-Datenbank beherrscht Gespeicherte Prozeduren mit einer Oracles PL/SQL ähnlichen Programmiersprache. Abfragen können ebenfalls gespeichert werden sowie Änderungen durchreichen. Kaskadierende Trigger für alle DML-Befehle und referentielle Integrität von Fremdschlüsseln werden angeboten. Alle Transaktionen sind vollständig ACID-kompatibel (Atomicity, Consistency, Isolation und Durability) und unterstützen Sicherungspunkte (\"Savepoints\").\n\nDer Server beherrscht unterschiedliche SQL-Dialekte, die individuell pro Datenbank festgelegt werden können. Abhängig von den Dialekten können zum Beispiel identifizierende Namen beliebige Zeichenketten mit Leerzeichen sein (\"quoted identifier\") oder Namen, die Groß-/Kleinschreibung ignorieren (\"case insensitive identifier\"). Mit den unterschiedlichen Dialekten können Daten und Applikationen aus älteren Anwendungen besser migriert werden.\n\nJede individuelle Spalte einer Tabelle kann ein eigenes Character-Set und eine eigene Sortierung haben (zum Beispiel unabhängig von Groß-/Kleinschreibung). Unterstützt wird eine große Anzahl von vordefinierten 8-Bit-Zeichensätzen und einige Unicode-Varianten. Zusätzliche Zeichensätze und Sortierungen können implementiert werden, ohne etwas am Firebird-Programmquelltext oder dem compilierten Programmcode ändern zu müssen.\n\nDie Server-Umgebung kann Nachrichten aus Triggern und Stored Procedures an Applikationen senden, soweit es die Treiber erlauben. So kann zum Beispiel die aktuelle Version des Java JDBC-Treibers (Name: Jaybird, Version 2.2.12, Stand: 30. März 2017) solche Nachrichten empfangen. Einer Applikation ist es damit möglich auf getriggerte Datenbankänderungen direkt zu reagieren und einem Anwender entsprechende Hinweise zu geben.\n\nMittels UDFs (User Defined Functions) kann der Sprachumfang der wertverarbeitenden Funktionen erweitert werden. Allerdings müssen entsprechende plattformspezifische Bibliotheken erzeugt und eingebunden werden. In einem experimentellen Stadium befinden sich Embedded Functions, die sich wie Stored Procedures verhalten sollen, aber in beliebigen Programmiersprachen entwickelt werden können. So gibt es zurzeit eine experimentelle Firebird-Version die Java-Methoden über eine im Server ausgeführte Java Virtual Machine einbinden kann.\n\nFirebird verfügt über ein Eventsystem, mittels dessen Nachrichten an Clients verschickt werden können; ausgelöst werden diese über Trigger bzw. Prozeduren.\n\nFür Datenbanken kann ausschließlicher Lesezugriff vereinbart werden, um auch nach Ablage auf Archivierungsmedien (z. B. CD-ROM, DVD) direkten Zugriff auf die enthaltenen Daten zu erlauben. Eine einzelne Datenbankdatei kann bis zu 64 Terabyte groß sein. Eine Datenbank kann aus mehreren Datenbankdateien bestehen. Damit können auch sehr große Datenbanken auf Dateisystemen angelegt werden, die zum Beispiel keine Dateien größer als 2 GB erlauben, beispielsweise FAT 16.\n\nMittels Software von Drittanbietern kann Firebird um Replikationsdienste erweitert werden.\n\n\n\n"}
{"id": "111235", "url": "https://de.wikipedia.org/wiki?curid=111235", "title": "System p", "text": "System p\n\nSystem p (früher pSeries) ist eine RISC-basierende Unix-Rechnerfamilie der Firma IBM. Die pSeries war Nachfolger der im Jahr 1990 von IBM eingeführten RS/6000-Serie (RISC System/6000).\n\nDas \"p\" im Namen steht für \"performance\", nicht, wie verschiedentlich angenommen, für \"Power-PC\".\n\nDie pSeries basiert auf der Power-CPU, in kleineren Modellen wird der PowerPC eingesetzt. In einem Rechner der pSeries können bis zu 32 dieser Power-CPUs zusammenarbeiten. Als Betriebssystem liefert IBM für die pSeries das proprietäre AIX oder Linux aus.\n\npSeries-Rechner unterstützen (von der p630 aufwärts) das dynamische logische Partitionieren LPAR. Der (2003) größte pSeries-Rechner p690 kann in bis zu 32 unabhängige Partitionen unterteilt werden und verwaltet 32 CPUs und 512 GB Arbeitsspeicher. Die Ressourcen können dynamisch von einer LPAR zu einer anderen verschoben werden, seit AIX 5.2 ist kein Reboot notwendig. \n\n2004 wurden IBM eServer p5 (siehe unten) und OpenPower-Server als Nachfolger von pSeries eingeführt.\n\n2006 wurde ein Rebranding der pSeries auf die Bezeichnung \"System p\" vorgenommen und in diesem Zuge auch neue Modelle eingeführt. Diese p5-{505, 510, 520, 550, 560, 570, 575, 590 und 595} arbeiten mit 1-64 POWER5-CPUs. Ein \"A\" in der Modellbezeichnung kennzeichnet POWER5+-Prozessoren, ein \"Q\" Quad-Core-CPUs.\n\nDie Power5-basierten Plattformen System i und System p sind seit den Ankündigungen von i5 und p5 physikalisch praktisch baugleich. Den Unterschied machen die charakteristischen Eigenschaften des gewählten Betriebssystems OS/400 (jetzt i5 OS), AIX oder Linux.\n\nIm IBM eServer p5 werden POWER5-CPUs eingesetzt, die Mikropartitionierung und SMT unterstützen.\nDie IBM eServer p5 Serverreihe erstreckt sich von einem 1-2 Wege p5 505 Server bis zu 64-Wege p595.\np5 Server können mit AIX und Linux betrieben werden und beherrschen logische Partitionierung, d. h. die Installation mehrerer Betriebssysteminstanzen auf einem Server. Die Betriebssysteme in den sogenannten Partitionen (LPAR- Logical Partition) sind voneinander unabhängig, und obwohl sie auf demselben Server laufen, haben sie nur Zugriff auf die ihnen zugeteilte Hardware.\nEs gibt eine Sonderreihe der p5 Server: OpenPower 710 (1-2 Wege) und OpenPower 720 (1-4 Wege), Server, die ausschließlich für den Einsatz von Linux bestimmt sind und keinen Betrieb von AIX erlauben.\nAlle p5 und OpenPower Server basieren auf POWER5-Technologie, können parallel und nativ 32-bit- und 64-bit-Programme ausführen.\n\nAlle auf p5 basierenden Server beherrschen logische Partitionierung, dynamische logische Partitionierung und Mikropartitionierung. Partitionierung ist die Verteilung von Hardware-Ressourcen an einzelne Betriebssysteminstanzen, dynamisch bezeichnet die Möglichkeit, diese Ressourcen ohne Neustart verschieben zu können und Mikropartitionierung bezeichnet die Möglichkeit, den einzelnen Partitionen Bruchteile der Prozessoren zuzuteilen. Die Virtualisierungstechnologie stammt aus dem Mainframebereich und findet heute in verschiedenen Soft- und Hardwarelösungen ihren Einsatz.\n\nDie Virtualisierung wird bei p5-Servern durch die Prozessortechnologie gestützt in der Firmware, genannt Hypervisor, erledigt. Der Hypervisor teilt die Hardware des Systems ein und stellt Teile davon dem Betriebssystem in einer logischen Partition (LPAR) zur Verfügung. Jede LPAR erhält damit eine „lokale“ Firmware (eine Art BIOS), die ihr nur die Hardware sichtbar macht, welche ihr zugeteilt worden sind.\n\nEine CPU kann an bis zu 10 LPARs verteilt werden, d. h. auf einer 4-Wege-Maschine können bis zu 40 unabhängige Betriebssysteminstanzen in logischen Partitionen installiert werden. Auf dem derzeit (2005) größten p5-Server, dem 64-Wege-p595 sind 254 Betriebssysteminstanzen möglich. Auf diesen können gleichzeitig sowohl AIX (5.2 und 5.3) als auch Linux installiert werden.\n\nIm Gegensatz zu den meisten softwarebasierten Virtualisierungslösungen gibt es bei der POWER-Architektur kaum Einschränkungen bezüglich der Skalierbarkeit einer einzelnen Partition. Die Partition kann von 0.1 bis zu 64 Prozessoren beinhalten und nutzen. Dedizierte und virtualisierte Ressourcen können innerhalb einer LPAR und eines Servers gemischt werden. Der durch die Virtualisierung entstehende Overhead ist schwer nachzuweisen, weil die Hypervisor-Schicht auf den p5-Servern immer präsent ist, d. h. alle Benchmarks sind auch mit Hypervisor gemacht worden.\n\nDurch die Virtualisierung und die automatische Lastverteilung wird eine sehr viel höhere Gesamtauslastung der Systeme erreicht. Durch die Virtualisierung der I/O-Ressourcen werden zusätzlich Adapter eingespart.\n\nFür die Virtualisierungseinrichtung wird ein spezieller Rechner, genannt HMC (Hardware Management Console) benötigt. Diese kann bis zu 32 Server und/oder bis zu 128 logische Partitionen (LPARs) verwalten und übernimmt neben der Hauptaufgabe, die Maschinen in LPARs einzuteilen, auch andere Funktionen, zum Beispiel ersetzt die HMC die seriellen Verbindungen zu den einzelnen Partitionen und macht damit zusätzliche Konsolen und KVM-Switches überflüssig. Unter POWER5+ ist für kleinere bis mittlere Systeme auch der Betrieb OHNE HMC virtualisierbar. Die Funktionen der HMC (mit gewissen funktionalen Einschränkungen) übernimmt in diesem Fall der sogenannte IVM (Integrated Virtualization Manager).\n\nHMC ist die grafische Schnittstelle für die Einteilung der Maschinen und für die Veränderung der Ressourcenzuteilung. Für den Betrieb der partitionierten Systeme selbst ist die HMC nicht notwendig und kann jederzeit abgeschaltet oder ersetzt werden. Bei den Linux-only OpenPower Systemen ist für die Virtualisierung eine kostenpflichtige Freischaltung notwendig.\n\nDie Virtualisierung der CPU ist auf zwei Arten möglich: CPUs können dediziert einer Partition (Betriebssysteminstanz) zugeteilt werden (dedicated), oder es werden Anteile an Rechenkapazität an eine Partition zugeteilt (shared). Auf einem Server können beide Arten der Zuteilung gemischt werden. Partitionen mit dedizierten CPUs nehmen an der automatischen Lastverteilung nicht teil. Alle CPUs, die nicht dediziert zugeteilt werden, verbleiben in einem sogenannten „shared CPU pool“. Dieser Pool wird von allen LPARs anteilig genutzt, die im „shared“ Modus eingerichtet werden. Werden die zugeteilten Zyklen von einer LPAR nicht gebraucht, so werden diese an den Pool zurückgegeben und können von den Instanzen benutzt werden, die gerade mehr CPU-Leistung benötigen.\n\nDer Arbeitsspeicher wird in Schritten einer für das gesamte System einstellbaren Größe (Memory Region Size) an einzelne LPARs zugeteilt. Die Speicherzugriffe werden durch Hypervisor derart umgesetzt, dass keine Partition auf die Speicherbereiche der anderen Zugreifen kann. AIX (ab 5.2) kann mit dynamischen (=im laufenden Betrieb) Speicherzuweisungen umgehen. Linux muss nach Veränderung der Speicherzuweisung neu gestartet werden.\n\nDer Hypervisor erfüllt die Funktion eines virtuellen Netzwerk-Switches. Der virtuelle Switch ist VLAN-fähig. Allen Partitionen können virtuelle Netzwerkkarten zugeteilt werden, diese verhalten sich aus der Sicht des Betriebssystems weitgehend wie echte physikalische Karten.\nPhysikalische und virtuelle Netzwerkkarten können gleichzeitig zugewiesen werden. So kann zum Beispiel eine der Partitionen, die sowohl eine physikalische als auch eine virtuelle Netzwerkkarte besitzt, als Router oder Bridge für die anderen Partitionen fungieren, die nur mit virtuellen Netzwerkkarten ausgestattet sind.\nÜblicherweise wird diese Aufgabe von dem VIO-Server übernommen (siehe Festplattenvirtualisierung). Eine physikalische und eine virtuelle Netzwerkkarte des VIO-Servers werden als Bridge konfiguriert (AIX Terminologie: SEA – Shared Ethernet Adapter). Wenn die virtuellen Netzwerkkarten der anderen LPARs im selben Netz und VLAN sind, werden diese von außen transparent (ohne Angabe eines Gateways) erreichbar.\n\nDer Hypervisor ist lediglich in der Lage, den Zugriff auf einen PCI-X-Slot einer Partition zu gewähren; einzelne Festplatten können nicht verteilt werden. Hierfür wird eine spezielle Partition benötigt, genannt VIO-Server (Virtueller I/O Server). Dieser bekommt den PCI-X-Slot zugewiesen, in dem der Adapter steckt, an dem wiederum die Festplatten angebunden sind, und er ist in der Lage, Teile dieser Platten den anderen LPARs als ganze virtuelle Festplatten zur Verfügung zu stellen. Es gibt eine Version des VIO-Servers, die auf AIX basiert, die gleiche Funktionalität kann aber mit Linux auch erreicht werden.\nAuf der Seite der Client-LPARs (solche, die virtuelle Festplatten nutzen, welche vom VIO freigegeben wurden) wird lediglich ein Treiber für einen virtuellen SCSI-Adapter benötigt. Dieser verhält sich genauso wie ein gewöhnlicher SCSI-Treiber.\n\n\n\nZum IBM eServer p5:\n"}
{"id": "111883", "url": "https://de.wikipedia.org/wiki?curid=111883", "title": "Thin Client", "text": "Thin Client\n\nEin Thin Client, lean client oder slim client ( \"dünner, schlanker\" bzw. \"magerer Client\") ist ein Client, d. h. ein Computer oder Programm, das auf die Hilfe eines Servers angewiesen ist, um seine Aufgaben zu erfüllen. Andere, oft herstellerabhängige Bezeichnungen lauten \"Cloud Client\", \"Zero Client\", \"Universal Desktop\" oder \"Clever Client\". Dies steht im Gegensatz zum \"Fat Client\" ( \"fetter Mandant\"), auch \"Full Client\" (\"vollwertiger Mandant\") der von seiner Hard- und Software so gebaut ist, dass er seine eigenen Aufgaben selbst erledigen kann.\n\nThin Clients sind sinnvoll, wenn Fat Clients zu teuer oder aufwändig sind, da sie entweder zu viel Rechenleistung oder Energie für die ihnen gestellten Aufgaben verbrauchen. Beispiele für Thin Clients sind Computerterminals eines Rechenzentrums (ausgelagerte Rechenleistung), oder Computer, die mit abgespeckten Betriebssystemen laufen (minimale Programm-Installation), auch Workstations in einem Netzwerk mit Datenservern (zentrale Speicherung), sowie Anwendungen, bei denen die gesamte Software am Server läuft, und nur die Darstellung ausgeliefert wird (wie Suchmaschinen oder GIS-Anwendungen wie Google Maps, der Thin Client ist hierbei der Webbrowser). Der Trend im Web 2.0, bei Portables und im Cloud-Computing geht prinzipiell hin zu Thin Clients.\n\nFür den Betrieb einer Thin-Client-Anwendung wird weniger Hardware und weniger Rechnerleistung benötigt. Ein Thin-Client stellt lediglich die Benutzerschnittstelle dar, die Datenverarbeitung erfolgt durch einen Server. Beispiele für Thin-Client-Anwendungen sind Web-Browser oder Anwendungen für den Zugriff auf Terminalserver. Bei der Nutzung eines Terminalservers werden alle Eingaben über ein spezielles Protokoll (X Window System, Remote Desktop Protocol (RDP), Citrix XenApp ICA) an den Terminalserver gesendet. Seit 2008 haben Virtualisierungstechnologien wie Hyper-V, Citrix XenDesktop und VMWare View an Bedeutung gewonnen, diese stellen eine weitere Einsatzmöglichkeit für Thin Clients dar und erlauben nun eine sehr individuelle und zentrale Bereitstellung von Arbeitsumgebungen.\n\nAuf dem Server bzw. virtualisierten Desktop werden die Eingaben verarbeitet und die Ausgabe wird zurück zum Client geschickt, der diese nur noch anzeigen muss. Die aktuelle Generation der Terminalserver- bzw. Virtualisierungslösungen erlaubt ebenfalls die Nutzung von Hardware über einen Drucker hinaus und arbeitet mit optimierten Methoden zur Wiedergabe von Audio- bzw. Videodaten.\n\nTypische Thin-Client-Betriebssysteme basieren dabei auf Linux, Windows CE, Windows Embedded Standard (Win32) oder Windows Embedded 7 und sind durch Schreibfilter vor Manipulationen durch User bzw. auch Viren und ähnliches geschützt. Einige Hersteller erlauben darüber hinaus die zentrale Bereitstellung von Systemanpassungen, um zum Beispiel Gerätetreiber, Hintergrundbilder usw. zu installieren.\n\nSpezielle Geräte, die für den Betrieb von Thin-Client-Anwendungen vorgesehen sind, werden oftmals als Thin Client bezeichnet. Für diese Thin Clients gibt es keine eindeutige Definition. In der Regel werden darunter Desktop-Computer ohne eigene Massenspeicher (Festplatte, CD-Laufwerke) verstanden, die über eine zentrale Verwaltungssoftware konfiguriert und verwaltet werden können.\n\nDer Begriff \"Thin Client\" stammt von Sun Microsystems. Der Sun Ray wurde \"Thin Client\" genannt. Auf dem Sun-Ray-Server wird das RDP-Protokoll von Microsoft umgesetzt, und die fertigen Bildschirminhalte werden zu den Sun-Ray-Geräten geschickt.\n\nHersteller wie Hewlett-Packard und Wyse haben Mikrocomputer hergestellt, die sie ebenfalls als \"Thin Clients\" bezeichnet haben. In diesem Fall wird das RDP-Protokoll von Microsoft auf den Geräten von einer grafischen Karte in Bildschirminhalte umgesetzt. Auf diesen Geräten läuft ein Betriebssystem und somit sind diese Geräte normale Computer, allerdings ohne Festplatte. Daraufhin hat Sun Microsystems die Sun-Ray-Geräte in Ultra Thin Clients umbenannt.\n\nLaut einer IDC-Studie aus dem Jahr 2006 wird der weltweite Markt für Thin-Client-Arbeitsplatzgeräte von den Firmen Wyse Technology (2012 übernommen von Dell) und Hewlett-Packard dominiert. Diese beiden Firmen erreichen einen Marktanteil von gut 70 %. Weitere Hersteller von Thin Clients sind Sun Microsystems, Igel Technology (Marktführer in Deutschland), ViewSonic, Chip PC, VXL Instruments, Hako Computing, Rangee, Devon IT, Athena oder Statodesk (ehemals LISCON), Fujitsu Technology Solutions, LuniLogic und andere.\n\nDer größte Vorteil von Thin Clients gegenüber Fat Clients ist der einfachere Betrieb. Auf den Thin Clients läuft nur die Software, die für den Zugriff auf zentral betriebene Anwendungen benötigt wird. Diese Basissoftware kann unabhängig von den Anwendungen, die tatsächlich genutzt werden, einheitlich betrieben werden. Ein Ansatz hierzu ist die Nutzung von Konfigurations-Images, die, nachdem sie erstellt wurden, an die zu konfigurierenden Thin Clients verteilt werden. Dies ermöglicht außerdem ein sehr einfaches Management durch zentrale oder dezentrale Steuerungssysteme. Darüber hinaus sind Thin Clients durch den Verzicht auf bewegliche Teile oft kostengünstiger. Hinzu kommt der stark reduzierte Verwaltungsaufwand, da eine nahezu unbegrenzte Anzahl von Thin Clients mittels einfacher Zuweisung von Konfigurationen verwaltet werden kann. Das schnelle Zu- und Abschalten von Anwendungen für den Endnutzer führt insbesondere bei weit entfernten Clients und langwierigen Installationen zu einem deutlichen Servicevorteil. In der Regel benötigen Thin-Client-Modelle mit etwa 10 bis 20 Watt deutlich weniger elektrische Leistung als Standard-Desktop-PCs, denn allein die Mehrkern-CPUs der PCs liegen ohne Peripherie bei 40 bis 120 Watt. Da weniger Abwärme entsteht als bei Standard-Desktop-PCs werden Klimaanlagen in Bürogebäuden in der Regel weniger belastet, auch wenn der höhere Aufwand bzw. zusätzliche Verbrauch der entsprechenden Serveranlage zu berücksichtigen ist.\n\nDie Nutzungsdauer eines Thin Clients beträgt durchschnittlich sieben Jahre, ein konventioneller Desktop wird drei bis vier Jahre genutzt.\n\nThin Clients ermöglichen einem Benutzer stets den gleichen Zugriff auf seine persönliche Benutzeroberfläche, Konfiguration, Verzeichnisse und installierte Programme, unabhängig davon, an welchem physischen Thin-Client-Arbeitsplatz er sich anmeldet. Somit muss keine feste Zuordnung von Arbeitsplätzen zu Benutzern erfolgen. Ein Benutzer kann ohne Einschränkungen jeden Tag an einem anderen Arbeitsplatz arbeiten.\n\nEs können Rahmenverträge mit Herstellern von Thin Clients geschlossen werden, diese werden dann vorkonfiguriert, wodurch der Thin Client in einem funktionierenden Netzwerk beim Anwender nur noch angeschlossen werden muss.\n\nGrafiklastige Anwendungen lassen sich mit vielen Server/Client-Lösungen nicht sinnvoll ausführen, wenn das Netz die Datenmenge nicht schnell genug verarbeiten kann oder wenn Anwendungen beschleunigte Grafikkarten voraussetzen, die auf Thin Clients selten vorhanden sind. Die meisten Anwendungen werden für Fat Clients programmiert. Oftmals ist der Betrieb von Anwendungen auf Servern durch Lizenzbestimmungen verboten und erfordert zusätzliche und meist kostspielige Genehmigungen des Softwareherstellers, die zudem nicht in jedem Fall gewährt werden. Thin Clients können ohne Netzwerkverbindung nicht genutzt werden. Für mobile Nutzer sind sie daher nur eingeschränkt nutzbar.\n\nAufgrund neuer Technologien auf der Serverseite und der zunehmenden besseren Virtualisierungtechnologie ist es jedoch absehbar, dass der Marktanteil von Thin Clients sich stetig steigern wird. Dies wird letztendlich auch durch steigenden Kostendruck und zunehmend komplexere Arbeitsumfelder in Unternehmen begünstigt. Somit ist der Thin Client in standardisierten Arbeitsumfeldern mit einer hohen Arbeitsplatzanzahl eine echte Alternative zum normalen PC.\n\nAußerdem kann ein Thin Client in der Regel kaum Peripheriegeräte erkennen, lediglich Maus, Tastatur und Monitor werden reibungslos akzeptiert. In der Praxis wird neben dem Server auch ein Netzwerkdrucker benötigt.\n\nSeit einiger Zeit wird von einigen Herstellern der Begriff \"Zero Client\" verwendet, diese Geräte sind aber zum größten Teil der Gruppe der Thin Clients zuzuordnen. Die Idee dabei ist primär, den potentiellen Käufern eine limitierte Funktion (z. B. nur ein möglicher Dienst wie Citrix XenDesktop, VMWare View oder IBM Virtual Desktop für Smart Business) mit einem einfachen, primitiven und mehr oder weniger sicheren Management (z. B. über DHCP-Optionen) anzubieten. Die Definition ist dabei je nach Hersteller flexibel und es ist nicht auszuschließen, dass die Bezeichnung „Zero Client“ früher oder später den Begriff „Thin Client“ ersetzen wird. So kann man, auch trotz anders lautender Herstelleraussagen, in nahezu jedem Zero Client sowohl eine Firmware (Linux-basiert oder vollständig eigenes OS), RAM als auch eine CPU finden. Die Firmware wurde jedoch auf einen minimalen Funktionsumfang limitiert. Kurze Bootzeiten werden unter anderem über einen Wechsel in den \"Suspend Mode\" erzielt und erwecken so bei dem Nutzer den Eindruck, es handle sich um einen „Zero Client“ mit kurzen Startzeiten.\n\nDa Zero Clients sich in der Regel auf eine Technologie auf die Server-Seite beschränken, muss man sich als Kunde bewusst sein, dass man sich langfristig an eine Lösung bindet und im Vergleich zu einem „normalen“ Thin Client nicht bei Bedarf die Lösung auf der Serverseite einfach wechseln kann. Auch bleibt abzuwarten, wie mittelfristig die Updatepolitik der Hersteller sein wird, beispielsweise bei größeren Updates auf der Serverseite; dies liegt im sehr limitierten Speicherplatz für Firmwares (je nach Modell aktuell bis 128 MByte) begründet.\n\nInzwischen wird bei verschiedenen Herstellern der Begriff System on Chip (SoC) benutzt, diese Systeme fallen jedoch auch unter den Begriff Zero/Thin Clients. Die System on Chip sind beispielsweise auf mobilen Geräten verbreitet und vereinen die wichtigsten Chips wie CPU, GPU (inkl. Videodecoder), Netzwerkcontroller und Chipsatz auf einem integrierten Schaltkreis. Neu ist auch, dass hier teilweise andere Architekturen als die auf dem Desktop dominierende x86-Architektur wie zum Beispiel die ARM-Architektur eingesetzt werden und auch Betriebssysteme, die nicht vom Desktop stammen und zum Beispiel auf Android basieren. Hier können Vorteile der ARM-Systeme genutzt werden. Sie sind sehr stromsparend und kostengünstig konzipiert. Es existieren auch x86-SoC da aber das Betriebssystem des Clients vom Betriebssystem des Servers unabhängig ist, kann es einen offenen Wettbewerb der Architekturen geben, ohne dass die vom Nutzer verwendete Software originär auf der Architektur lauffähig sein muss.\n\nIn vielen Bereichen, insbesondere was die Nutzung von USB-Geräten, Multimedia und grafiklastigen Anwendungen angeht, werden die Nachteile der Thin Clients nach und nach kleiner bzw. verschwinden sogar. Aktuelle Erweiterungen/Protokolle wie Citrix HDX, VMWare PCOIP, Microsoft RemoteFX und andere geben bereits eine Idee, was zukünftig möglich sein wird. Auch die Änderungen auf der Serverseite, zum Beispiel durch Pläne von AMD, auf der Serverseite GPU und CPU zu verschmelzen, und die permanente Verfügbarkeit höherer Netzwerkbandbreiten sowie das Bedürfnis des Nutzers nach einem einfach zu benutzenden System, das zusätzlich eine hohe Sicherheit bietet, werden diesen Trend beschleunigen und neue Einsatzfelder für Thin Clients erschließen.\n\nMit weiterer Entwicklung werden über kurz oder lang immer mehr Thin-Client-Produkte auch Privatanwender erreichen und hier Desktop-PCs ersetzen. Beispiele, die dieser Vision folgen, sind indirekt in Form des Apple iPad, Android Tablet PCs und entsprechenden Smartphones bereits vorhanden und erfreuen sich zunehmender Beliebtheit. Mittelfristig kann man davon ausgehen, dass hier unter dem Schlagwort des Cloud Computing auch vollständige virtuelle Desktops die Privatenanwender erreichen werden, da auch mittlerweile für alle gängigen Serversysteme bereits entsprechende Clients verfügbar und somit die Voraussetzungen geschaffen sind.\n\nBegünstigt wird diese Entwicklung ferner durch steigende Energiepreise und das deutlich leichtere Handling dieser Endgeräte, zum Beispiel, wenn diese zentral verwaltet und gewartet über Internetprovider oder andere zu einem akzeptablen Preis zur Verfügung gestellt werden.\n\n\n"}
{"id": "112377", "url": "https://de.wikipedia.org/wiki?curid=112377", "title": "Cubase", "text": "Cubase\n\nCubase ist ein MIDI-Sequenzer und eine Digital Audio Workstation (DAW) des Hamburger Unternehmens Steinberg. Es erschien erstmals 1989 und erzielte 1996 einen Verbreitungsschub durch die Einführung der Virtual Studio Technology VST, die wenig später zum Industriestandard wurde.\n\nDie Software wird in professionellen Tonstudios oder privaten Heimstudios eingesetzt. Sie bildet dort meist die Zentrale einer Produktionsumgebung für Musik oder Audioaufnahmen. Cubase kann nutzungsabhängig einzelne oder viele der Funktionalitäten wahrnehmen, die in einem analogen Tonstudio in Form von Geräten wie Bandmaschinen, Mischpulten, Effektgeräten oder elektronischen Klangerzeugern vorgehalten werden. Somit kann das Programm bei der Produktion entweder unterstützend oder ausschließlich genutzt werden, wobei letzteres die vollautomatische Wiederherstellung aller Einstellungen bietet („Total Recall“) und oft als Produktion „in the box“ (ITB) bezeichnet wird.\n\nCubase ist kein Begleitautomat, d. h. sein primärer Zweck liegt nicht darin, aus geringem musikalischen Input möglichst perfekte und voluminöse Arrangements zu erstellen. Vielmehr wird die Software zu Aufnahme bzw. Erstellung (auch manuell mithilfe diverser Editoren), Aufbereitung und Abmischung diverser Einzelspuren zu einer Gesamtaufnahme verwendet. Hierbei liegt der Fokus darauf, einzelne Signale zu verbessern, jedoch nicht darauf, sie zu erfinden. Für Live-Einsatz oder musikalische Kreativität sind zwar einzelne Funktionen enthalten, dennoch ist Cubase im Wesentlichen ein Werkzeug, um existierende Ideen in möglichst optimaler Qualität festzuhalten.\n\n\nUrsprünglich lief Cubase unter TOS auf dem Atari ST mit einem auf 8 MHz getakteten Motorola 68000 Prozessor und 1 Megabyte RAM. Der Atari hatte serienmäßig die nötigen Midi-Schnittstellen. Der Rechner, auf dem Cubase betrieben wird, benötigt Schnittstellen nach außen. Für Aufnahme und Wiedergabe von Audiomaterial wird eine Audio-\"Karte\", möglichst mit schnellem ASIO-Treiber, benötigt. Zur Aufnahme von MIDI-Daten muss ein entsprechendes Interface am Rechner angeschlossen sein. Über MIDI können dann interne sowie externe Klangerzeuger (z. B. Synthesizer) und MIDI-Eingabegeräte (in der Regel Klaviatur/Keyboard, aber auch MIDI-Gitarre, MIDI-Controller o. ä.) verbunden sein.\n\nDer Umfang des Einsatzes von Software-Instrumenten und Audioeffekten bestimmt zusammen mit der Erfordernis sehr kurzer Audio-Latenzzeiten maßgeblich die Anforderungen, die der Betrieb von Cubase hinsichtlich der Rechnerausstattung stellt. Für eine zeitgemäße Musikproduktion \"in the box\" mit der aktuellen Cubase-Version ist ein Rechner aktueller Generation mit überdurchschnittlicher Ausstattung empfehlenswert.\n\nCubase wurde zunächst für den Atari ST geschrieben und später auf den Apple Macintosh und Windows portiert. Für die letztgenannten Betriebssysteme ist Cubase bis heute erhältlich.\n\nDie letzten Versionen für die Atari-Plattform waren Cubase 3.1 (1993) und Cubase Score 2.0r6 (1994) für den Atari ST, sowie Cubase Audio 2.06 (1994) für den Falcon.\n\nDer Hersteller hat die Namenszusätze im Laufe der Versionsgeschichte mehrfach variiert und dabei die Versionsnummern teilweise wiederverwendet, was das Einordnen und Abschätzen der Aktualität einer bestimmten Version schwierig macht. So ist beispielsweise Cubase 4 etwa acht Jahre jünger/neuer als Cubase VST 5.\n\nDie folgende Übersicht zeigt die Cubase-Versionen seit 1997 in der zeitlichen Reihenfolge ihres Erscheinens bis heute (nur kostenpflichtige bzw. Major-Updates sind berücksichtigt). Wenn nicht anders angegeben, sind die genannten Versionen parallel bzw. im Abstand weniger Monate für die Macintosh- und die Windows-Plattform erschienen.\n\n\nDer Wechsel von VST zu SX fand 2002 statt, wobei die ersten SX-Versionen der letzten VST-Version 5.1 funktional deutlich unterlegen waren; vielmehr wurde das Produkt wegen der immer schwieriger werdenden Wartbarkeit des Programmcodes mit Erscheinen von Cubase SX auf eine nach Firmenangaben neue Codebasis gestellt, d. h. neu geschrieben.\n\nDerzeit werden vier kleinere Versionen von Cubase mit reduziertem Funktionsumfang und weniger Beigaben (z. B. Plug-ins, Sample-Content etc.) verkauft:\n\nSteinberg hat die Namen und Namenszusätze dieser im Funktionsumfang reduzierten Versionen im Laufe der Versionsgeschichte immer wieder geändert. Weitere Namensbeispiele für kleinere Versionen von Cubase, die es irgendwann im Laufe der Geschichte des Produktes einmal gab:\n\nDer Name Cubasis wurde Ende 2012 als Bezeichnung für Steinbergs erste Sequencer-App für iOS wiederverwendet.\n\nDie Software Nuendo vom selben Hersteller umfasst alle Funktionen von Cubase, sofern das Zusatzprodukt NEK installiert und lizenziert ist. Nuendo und Cubase sind hinsichtlich ihrer Bedienung praktisch identisch. Cubase-Projektdateien können auch mit Nuendo geöffnet und dort weiterverarbeitet werden. Nuendo bietet darüber hinaus zusätzlich Funktionen zur Video-Nachvertonung (Postproduktion).\n\nBezüglich der erstellten Projekt-Dateien herrscht generell Aufwärtskompatibilität. Ein mit einer der kleinen Cubase-Versionen oder einer älteren Version erstelltes Projekt lässt sich auch in den großen bzw. aktuellen Versionen (Cubase 8, Nuendo 7) problemlos öffnen. Allerdings gilt dies in den meisten Fällen nicht umgekehrt.\n\nSeit dem Erscheinen von Cubase 4 lassen sich die alten Dateiformate der „VST“-Produktreihe und deren Vorläufer nicht mehr öffnen, weshalb Steinberg für Nutzer der aktuellen Versionen kostenfrei eine ältere Variante von Cubase SX/SL/SE 3 für den Projektimport zur Verfügung stellt. Somit sind mittelbar auch uralte Musikstücke und Projekte aus dem letzten Jahrtausend noch in den aktuellen Versionen nutzbar.\n\nAlle größeren Versionen von Cubase benutzen seit mehr als zehn Jahren USB-Dongles als Kopierschutz, um eine unberechtigte Weitergabe der Programme zu unterbinden. Cubase kann gleichzeitig auf mehreren Rechnern installiert sein, es funktioniert jedoch nur auf dem Computer, mit dem der Dongle verbunden ist. Lizenzen, die noch auf den bis 2002 verwendeten LPT-Dongles für den Druckerport gespeichert sind, können auf Wunsch durch den Kundendienst der Herstellerfirma auf einen USB-Dongle übertragen werden.\n\nEs gibt drei Versionen der USB-Dongles, wobei die älteste in der Praxis Performance-Nachteile bei der Arbeit mit aktuellen Cubase-Versionen verursachen kann. Daher sollten Dongles der ersten Generation (bis etwa 2003, erkennbar an der größeren Baulänge) nach Möglichkeit gegen eine aktuellere Version des eLicensers getauscht werden. Die zweite und dritte Generationen unterscheiden sich nur äußerlich; Dongles der dritten Generation sind nochmals erheblich kürzer geworden.\n\nDer Kopierschutz wurde ursprünglich von der deutschen Firma Syncrosoft entwickelt und 2008 unter dem Begriff eLicenser von Steinberg übernommen.\n\nDie stark abgespeckten Versionen von Cubase (z. B. Cubase Elements, LE, AI) benutzen sogenannte \"Soft-eLicenser\" als Kopierschutz. Hier wird die Lizenz nicht auf einem Hardware-Dongle gespeichert, sondern mit einem einzigen Rechner verknüpft. Derartige Lizenzen lassen sich auf Wunsch manuell auf einen USB-eLicenser übertragen und sind dann durch Umstecken des Dongles auf verschiedenen Rechnern nutzbar. Dieser Schritt ist nicht umkehrbar.\n\nSteinberg hat zwei Apps für Apple iOS und eine für Android herausgegeben, die eine Fernsteuerung von Cubase-Funktionen erlauben. Die Apps sind in den üblichen App Stores für die jeweiligen Plattformen erhältlich. Sie funktionieren nur mit den beiden größten Cubase-Versionen.\nCubase iC erlaubt die Steuerung der wichtigsten Transportfunktionen von Cubase. Außerdem können zur Navigation innerhalb eines Projekts die einzelnen \"Events\" der \"Arranger-Spur\" namentlich angesprungen werden.\nCubase iC Pro bietet neben der erweiterten Navigation innerhalb eines Projekts auch die Steuerung des Mixers, so dass z. B. \"Cue-Mixes\" ferngesteuert werden können. Das namentliche Anspringen von \"Arranger-Events\" ist hier – im Gegensatz zu Cubase iC – jedoch nicht möglich. Die Android-Version wurde Anfang 2016 eingestellt.\n\nAlle jemals erschienenen Versionen von Cubase unterstützen MIDI zur Kommunikation mit elektronischen Musikinstrumenten. Cubase ist im MIDI-Bereich im Vergleich zu Konkurrenzprodukten sehr gut ausgestattet. MIDI-Daten können aufgenommen, fast beliebig manipuliert, zeitlich quantisiert und in diversen „Editoren“ genannten Unterfenstern sehr komfortabel bearbeitet werden. Zur Echtzeitmanipulation stehen darüber hinaus zahlreiche MIDI-Plug-ins zur Verfügung. Ein spezieller Drum-Editor erlaubt das komfortable Bearbeiten von via MIDI durchgeführten Schlagzeugaufnahmen. Einzige Schwäche ist der Umgang mit systemexklusiven MIDI-Daten, die innerhalb der Software seit dem Erscheinen von Cubase SX weitgehend ignoriert werden.\n\nZeitweise vertrieb Steinberg MIDI-Interfaces mit einer Technik für erhöhte Timing-Genauigkeit ('LTB'), um die technischen Schwächen von MIDI auszugleichen. Diese Geräte werden jedoch seit vielen Jahren nicht mehr angeboten und sind nur noch auf dem Gebrauchtmarkt erhältlich, obwohl LTB auch in den aktuellen Versionen nach wie vor unterstützt wird.\n\nCubase arbeitet im Audiobereich seit den ersten VST-Versionen spur- bzw. kanalbasiert und bietet einen komplexen Mixer, um die Audio-Signale mittels Equalizer und Insert- bzw. Send-Effekten sowie in der aktuellen Version mit einem vollständigen \"Channel-Strip\" pro Spur zu bearbeiten und zu Untergruppen oder Ausgangskanälen zusammenzumischen. Die Kanalautomation bietet sehr weitreichende Möglichkeiten zur automatisierten zeitgesteuerten Werteänderung. Die Ausstattung von Cubase im Audio-Bereich ist generell gut, es fehlt allerdings eine Echtzeit-Objektorientierung für einzelne Audio-Clips im Vergleich zu Konkurrenzprodukten.\n\nEin Alleinstellungsmerkmal in den großen Cubase-Versionen ist der sogenannte \"Control Room\", der ein komplexes Bus-System zur Bereitstellung unterschiedlicher Abhörsituationen darstellt, damit beispielsweise mehrere Musiker gleichzeitig individuelle Kopfhörermischungen erhalten können, während zeitgleich auf dem zentralen Abhörweg der Summenmix wiedergegeben wird. Vergleichbares ist in Konkurrenzprodukten nicht oder nur mit hohem manuellem Aufwand realisierbar.\n\nCubase bietet ab Version 5 unter dem Namen \"Variaudio\" Funktionen zur Korrektur von Tonhöhe und Timing monophoner Audioaufnahmen.\n\n1996 wurde von Steinberg die Audio-Stream-Input/Output-Architektur (ASIO) für Cubase eingeführt. Die ASIO-Architektur ermöglicht zusammen mit einer ASIO-kompatiblen Soundkarte eine schnelle Übertragung von Audiosignalen. Durch Verwendung der ASIO-Architektur konnten Verzögerungen bei Aufnahme und Wiedergabe von Audio-Signalen (Latenzen) derart verringert werden, dass VST-Instrumente quasi in Echtzeit gespielt werden können. Statt einer störenden Latenz von mehreren hundert Millisekunden können moderne Soundkarten Audiosignale mit wenigen Millisekunden Latenz aufnehmen und wiedergeben. Für preiswerte Karten ohne eigenen ASIO-Treiber existiert ein beliebter Wrapper in Form des ASIO4ALL-Treibers eines Drittanbieters.\n\nDie erzielbare Rechenleistung bei der Nutzung von ASIO hängt sowohl vom verwendeten Rechner und dessen Konfiguration als auch von der Audiohardware und dem dort verwendeten ASIO-Treiber ab. Für Nutzer, die extrem kurze Verzögerungszeiten benötigen, existieren Übersichten, die als Kaufberatung dienen können. Im Netz sind – besonders für Windows-Betriebssysteme – Anleitungen zur richtigen Konfiguration eines Audio-Rechners erhältlich, um die bestmögliche Leistung zu erzielen.\n\n1996 wurde mit Cubase VST eine auch für andere Anbieter offene Plug-in-Schnittstelle für virtuelle Effekte mit dem Namen \"Virtual Studio Technology (VST)\" implementiert. 1999 folgte die verbesserte VST-Version 2, 2006 schließlich die Version 3 der Schnittstelle, die aktuell in der Version 3.6.0 vorliegt. Die dritte Version wurde von den Third-Party-Entwicklern in den ersten Jahren sehr zögerlich aufgenommen, setzt sich aber inzwischen mehr und mehr durch. Nach wie vor erscheinen jedoch Audio-Plug-ins und virtuelle Instrumente von Drittanbietern noch als VST2-Plug-ins.\n\nVST-Plug-ins können als Audioeffekte, virtuelle Instrumente oder MIDI-Effekte fungieren. Es gibt auch Anbieter von Hardware-basierten Effekten für die VST-Schnittstelle; angesichts der Rechenleistung aktueller Prozessoren dient die Hardware-Plattform allerdings inzwischen eher als Kopierschutz denn als notwendige Ergänzung der Rechenleistung. Cubase wird von Haus aus mit einem Plug-in-Bündel geliefert, im Fall von Cubase 7 sind dies z. B. 66 Audio-Effekte, 18 MIDI-Plug-ins sowie fünf virtuelle Instrumente.\n\nSteinberg bietet seit vielen Jahren auf der Firmen-Webseite nach einer Registrierung kostenfrei die nötigen Entwicklungspakete an, mit denen sowohl gewerbliche als auch Hobby-Programmierer mit vergleichsweise geringem Aufwand neue Plug-ins erstellen können. Programme wie N.I. Reaktor bzw. Synthedit erlauben auch Personen ohne tiefere Programmierkenntnisse Instrumente und Effekte für die VST-Schnittstelle zu erstellen.\n\nDie VST-Schnittstelle ist mittlerweile ein Standard für digitale Audiobearbeitungsprogramme auf Macintosh und Windows-Systemen, die jedoch bei neuen MacOS-Versionen nicht mehr von allen Programmen unterstützt wird. Stattdessen kommen andere Schnittstellen, z. B. die AU-Schnittstelle (Audio Unit) oder AAX/TDM zum Einsatz. Steinberg-Produkte allerdings unterstützen nach wie vor auch unter macOS die hauseigene VST-Schnittstelle. Ebenfalls erhältlich als VST-Instrumente und Effekte sind mittlerweile Emulationen von real existierender Hardware.\n\nHeutzutage ist es möglich, ganz auf Outboard-Equipment zu verzichten und ganze Produktionen mit VST-Instrumenten und -Effekten auf PC- oder Mac-Basis durchzuführen. Verfahren zur Auslagerung rechen- bzw. speicherintensiver Plug-ins auf weitere Rechner existieren, verlieren jedoch im Zuge der Steigerung der Rechner-Leistungsfähigkeit und der Einführung von 64-Bit-Betriebssystemen zunehmend an Bedeutung (z. B. das in Cubase eingebaute \"VST System Link\", oder auch – als Beispiel eines Drittanbieters – die Software FX Teleport).\n\n\n\n"}
{"id": "112932", "url": "https://de.wikipedia.org/wiki?curid=112932", "title": "Apple Lisa", "text": "Apple Lisa\n\nDer Apple Lisa (auch nur „Lisa“ oder „The Lisa“ genannt) von 1983 war einer der ersten Personal Computer, der über eine Maus und ein Betriebssystem mit grafischer Benutzeroberfläche in einer monochromen schwarz/weiß Darstellung verfügte. Wegen des hohen Preises von rund 10.000 US-Dollar verkaufte sich der Rechner schlecht, und Apple stellte die Produktion bereits 1984 wieder ein. In der Computergeschichte gilt Lisa als Vorbereitung des deutlich preisgünstigeren, aber technisch ähnlichen Macintosh im Jahr 1984. Mittelfristig wurde der „Mac“ dann zum großen Erfolg für den kalifornischen Computerkonzern.\n\nBis Anfang der 1980er Jahre wurden Computer üblicherweise per Tastatur über eine Kommandozeile bedient. Lisa war – neben dem kaum bekannten Xerox Star von 1981 – der erste kommerzielle Computer, der eine graphische Benutzeroberfläche mit Schreibtischcharakter und Mausbedienung bot. Steve Jobs, der Lisa als Nachfolger des beliebten Apple II entwickeln ließ, hatte die Inspiration dazu bei einem Besuch des Xerox PARC 1979 bekommen, als er den Forschungsrechner Xerox Alto von 1974 sah. Lisa stand in Konkurrenz zum kurz zuvor erschienenen PC von IBM. Zwar bot Microsoft ab 1985 mit der Einführung von Microsoft Windows eine prinzipiell vergleichbare graphische Benutzeroberfläche für IBM-PC-kompatible Computer an, diese galt jedoch in den ersten Jahren als wenig ausgereift und erst mit der Version 3.0 ab etwa 1990 allmählich als ernst zu nehmende Konkurrenz zu Apples Produkten.\n\nDie \"New York Times\" schrieb am 19. Januar 1983, unmittelbar vor Erscheinen des Computers:\nDie Londoner \"Sunday Times\" schrieb am 25. Januar 1983:\n\nDass Lisa einen Meilenstein in der Computertechnik darstellte, wird auch anhand eines sehr positiv geschriebenen Erfahrungsberichts eines Computerredakteurs von 1983 deutlich:\nWährend in den mitgelieferten Originaldokumenten des Lisa OS lediglich von „The Lisa“ die Rede war, wurde von Apple offiziell behauptet, dass es sich bei Lisa um ein Akronym für L\"ocal \"I\"ntegrated \"S\"oftware \"A\"rchitecture\" handle, also „lokale integrierte Softwarearchitektur“. Da Steve Jobs’ erste Tochter, die 1978 geboren wurde, den Namen Lisa Jobs erhielt, geht man im Allgemeinen auch davon aus, dass der Name eine persönliche Bedeutung hat und es sich ergo um ein Backronym handelt. Später tauchte in der IT-Welt auch das ironisch gemeinte Akronym L\"et's \"I\"nvent \"S\"ome \"A\"cronym\" für LISA auf.\n\nAls Prozessor (CPU) kam der Motorola 68000 inklusive MMU mit 5,09376 MHz, einem 16‑Bit-Datenbus sowie einem 24‑Bit-Adressbus zum Einsatz. Auf einer austauschbaren Speicherplatine von Apple waren 512 kB Arbeitsspeicher (RAM) aufgelötet, in der Konfiguration mit 1 MB RAM waren zwei Speicherplatinen mit je 512 kB Speicher eingebaut. Durch Austausch der beiden Speicherplatinen durch eine einzelne 2‑MB-Speicherplatine von Sun Remarketing konnte der Nutzer den Rechner auf maximal 2 MB RAM aufrüsten. Für den Betrieb von Lisa-Software war mindestens 1 MB Arbeitsspeicher erforderlich, für den Betrieb von MacWorks reichten 512 kB. Später gab es mit Hardwaremodifikationen der Lisa-CPU/​MMU-Platine auch eine Aufrüstung auf 4 MB RAM. Es folgten CPU-Platinen von Fremdherstellern mit 12-, 16- oder 18‑MHz-68000-Prozessor und mit bis zu 8 MB (schnellerem) RAM direkt auf der CPU-Platine, die allerdings nur mit MacWorks nutzbar waren, da Lisa OS nur die Original-CPU unterstützte und nicht mehr als 2 MB RAM verwenden konnte.\n\nDer Lisa verfügte anfangs über zwei 5¼-Zoll-Diskettenlaufwerke mit jeweils 871 kB Speicherkapazität. Eine externe Festplatte, genannt Apple ProFile (siehe Bild rechts auf der Lisa), konnte extern angeschlossen werden und bot je nach Modell eine Speicherkapazität von 5 oder 10 MB.\n\nSchon 1984 erschien mit dem Lisa 2 ein überarbeitetes Modell, bei dem unter anderem die beiden 5¼-Zoll-Diskettenlaufwerke durch ein einzelnes, moderneres 3,5-Zoll-Laufwerk mit 400 kB Speicherkapazität ersetzt worden waren. Die Lisa 2 hatte eine interne 10‑MB-Festplatte, dafür keinen externen Anschluss für eine Apple ProFile. Später gab es auch eine Aufrüstung auf ein 800‑kB-3,5-Zoll-Laufwerk, das allerdings nur mit MacWorks nutzbar war.\n\nAls Betriebssystem wurde Lisa OS verwendet, alternativ auch Xenix, eine Unix-Variante von Microsoft (später an SCO abgetreten), sowie UniPlus+ UNIX von UniSoft. Daneben bestand ab der Lisa 2 mit MacWorks die Möglichkeit, die Macintosh System Software und somit Macintosh-Programme zu nutzen.\n\nDie graphische Benutzeroberfläche wurde \"Lisa Shell\" genannt und hatte große Symbole, die einen Schreibtisch darstellten, was später – im PC-Bereich mit Microsoft Windows erst in den 1990er Jahren – zum noch heute üblichen Industriestandard wurde.\n\nSechs Büroanwendungen wurden mitgeliefert: \"LisaCalc\", \"LisaGraph\", \"LisaDraw\", \"LisaWrite\", \"LisaProject\" und \"LisaList\". Das Programm \"LisaTerminal\" kostete 850 DM extra. Als zusätzliche Programmiersprachen gab es BASIC, COBOL und Pascal.\n\nDer komplette Quellcode wird 2018 als Open Source-Software veröffentlicht – ohne das American-Heritage-Wörterbuch des Schreibprogramms Lisawrite.\n\nApple Lisa war mit einem Preis von 9.995 US-Dollar sehr teuer (in Deutschland etwa 30.000 DM in Österreich 200.000 Schilling, nach heutiger Kaufkraft ca. Euro) und verkaufte sich trotz der guten Ausstattung und des damals hochinnovativen Konzepts schlecht. Um sie zumindest von der Steuer abschreiben zu können, wurden 2700 unverkäufliche Apple Lisa 1989 auf einer Müllhalde im US-Bundesstaat Utah einfach vergraben. Der Nachfolger \"Lisa 2\" (1984) wurde Anfang 1985 (nach Einführung des auch „Schuhkarton“ genannten kleinen \"Macintosh\") umbenannt in \"Macintosh XL\".\n\nIn der Simpsons-Episode 497 (\"Freundschaftsanfrage von Lisa\", im Original: \"The D’Oh-cial Network\". Staffel 23, Episode 11) ist Lisa Simpson in einer Aufnahme zu sehen, wie sie auf einem alten Lisa der Marke „Mapple“ – Persiflage des Elektronikherstellers \"Apple\" – arbeitet.\n\n\n"}
{"id": "113177", "url": "https://de.wikipedia.org/wiki?curid=113177", "title": "Lotus 1-2-3", "text": "Lotus 1-2-3\n\nLotus 1-2-3 ist eine Tabellenkalkulationssoftware der Firma Lotus Development Corporation (heute ein Unternehmen von IBM). Das Programm wurde mit dem Werbespruch „rechnet schneller als Sie 1-2-3 sagen können“ beworben. Die offizielle Unterstützung des Programms durch den Hersteller IBM endete am 30. September 2014.\n\nDie erste Version für den IBM PC wurde am 6. Januar 1983 veröffentlicht. Sie lief unter DOS und trug damals wesentlich zum Erfolg des IBM-PC bei. Die Zahlen „1-2-3“ stehen dabei für die Funktionalität der Software: 1 = Berechnungen, 2 = Diagramme, 3 = Datenbank.\n\nAnfangs orientierte sich das Programm noch an Visicalc, hat dieses aber letztlich überholt und stieg zum Marktführer bei den Tabellenkalkulationen auf. Mit 1-2-3 wurden erstmals Zellnamen und Zellbereiche eingeführt.\n\nMitte der 1980er Jahre war Lotus 1-2-3 R2 die dominierende Anwendung für Tabellenkalkulation auf dem IBM-PC. Der Nachfolger R3 war auch für OS/2 verfügbar und konnte unter DOS erweiterten Speicher nutzen. Auch Versionen für die UNIX-Derivate IBM AIX, HP-UX, Sun Solaris oder SCO UNIX waren erhältlich.\n\nAls grafische Oberflächen auf dem PC populärer wurden, entstanden Versionen für OS/2 Presentation Manager und Windows, 1991 auch die Version 1.0 für den Macintosh. Zeitgleich mit der Entwicklung von Microsoft Windows 2.x wurde von Microsoft das Tabellenkalkulationsprogramm Microsoft Excel entwickelt und gemeinsam vorgestellt. Den damit verbundenen Entwicklungsvorsprung konnte Lotus nicht mehr einholen, verbunden mit der OEM-Politik von Microsoft wurde 1-2-3 dann durch Microsoft Excel als Standardprogramm abgelöst.\n\nDer Konkurrent Borland entwickelte ebenfalls eine Tabellenkalkulation, die er in Anspielung auf Lotus’ 1-2-3 Quattro Pro (Quattro: italienisch für \"4\") nannte.\n\nSeit 2002 beschränkte sich die Weiterentwicklung jedoch auf kleinere Änderungen und Bugfixes. Zum 11. Juni 2013 kündigte die IBM das Produkt ab und stellte das Marketing dazu ein. Zum 1. Oktober 2014 wurde zusätzlich der Support beendet und das Produkt offiziell eingestellt.\n\n\n\n\n\n\n\n\n\nDie mit 1-2-3 erstellten Dateien erhalten die Endung codice_1 (in älteren Versionen auch codice_2 (Version 1), codice_3, codice_4 (Version 2), codice_5, codice_6 (Japanische Version 2), codice_7 (Version 3), codice_8 (Japanische Version 3), codice_9 (Version 4), codice_10 (Japanische Version 4), codice_11 (Version 5)). Vorlagendateien (so genannte SmartMaster) haben die Dateiendung codice_12.\n\nDie Dateien heißen bei 1-2-3 Arbeitsmappen. Eine Arbeitsmappe kann mehrere Arbeitsblätter (Tabellen) enthalten. Ein Arbeitsblatt besteht aus 256 Spalten, die jeweils von A (=1) bis IV (=256) bezeichnet sind, und 65536 Zeilen (von 1 bis 65536 nummeriert). Somit stehen in einem Arbeitsblatt 16.777.216 Zellen zur Verfügung.\n\nDas Dateiformat selbst ist proprietär und kann nicht ohne weiteres von anderen Programmen gelesen werden. Mit 1-2-3 kann man jedoch viele Fremdformate (z. B. Microsoft Excel) lesen, bearbeiten und speichern.\n\n"}
{"id": "113771", "url": "https://de.wikipedia.org/wiki?curid=113771", "title": "Präprozessor", "text": "Präprozessor\n\nEin Präprozessor (seltener auch \"Präcompiler\") ist ein Computerprogramm, das Eingabedaten vorbereitet und zur weiteren Bearbeitung an ein anderes Programm weitergibt. Der Präprozessor wird häufig von Compilern oder Interpretern dazu verwendet, einen Eingabetext zu konvertieren und das Ergebnis im eigentlichen Programm weiter zu verarbeiten.\n\nViele Programmiersprachen, zum Beispiel die Programmiersprache C sowie das Textsatzprogramm TeX besitzen Makroprozessoren als Präprozessoren, die die Möglichkeiten der jeweiligen Sprache zur Steigerung der Lesbarkeit von Programmtexten sowie zur Strukturierung und Modularisierung von Projekten wesentlich erweitern. PHP – eine verbreitete Skriptsprache zur Generierung von Webseiten – kann als Präprozessor für HTML gesehen werden.\n\nIm Bereich des CAD-Entwurfs, des CAM sowie der Finite-Elemente-Simulation wird häufig ein Präprozessor verwendet, um das entworfene Bauteil für die Weiterverarbeitung vorzubereiten. Hierzu gehören beispielsweise Arbeitsschritte wie Kombination mit einem anderen Bauteil, Aufteilung in Segmente, Gittererzeugung, Umsetzen in ein anderes Koordinatensystem usw.\n\nDer Präprozessor der Sprache C führt unter anderem die folgenden Änderungen am Programmtext durch, bevor der eigentliche C-Compiler das Programm übersetzt:\n\nNeben der einfachen Makroersetzung ist die bedingte Übersetzung mittels codice_9, codice_10 und codice_11 ein wichtiges Merkmal des C-Präprozessors: Der Entwickler kann damit steuern, welche Abschnitte des Programmtextes dem Compiler zugeführt werden und so beispielsweise Debug-Programmcode ein- oder ausblenden oder Anpassungen für unterschiedliche Prozessoren oder Betriebssysteme vornehmen.\n\nDer Präprozessor von FreeBASIC funktioniert syntaktisch ähnlich wie der von C. Er interpretiert Anweisungen wie #if, #endif, #ifdef, #else, #define und #include ähnlich oder gleich wie in C. Zusätzlich werden noch Optionen wie #include once (Kein wiederholtes Einschleusen) und #macro … #endmacro (Mehrzeiliges Macro) zu Verfügung gestellt.\n\nDie Fähigkeiten des im TeX-Satzsystem eingebauten Präprozessors beschränken sich im Wesentlichen auf die Definition von Makros zur Zusammenfassung von Formatierungsbefehlen. Darauf aufbauend wurden im Laufe der Zeit umfangreichere Präprozessoren (genauer: Makrosammlungen) für mitunter sehr spezielle Anwendungsgebiete entwickelt, um den Umgang mit TeX für den Benutzer komfortabler zu gestalten. Prominentester Vertreter ist LaTeX, weitere Beispiele sind ConTeXt, BibTeX und MusiXTeX.\n\nPHP ist eine Skriptsprache, die hauptsächlich zur Erstellung dynamischer Webseiten oder Webanwendungen verwendet wird. Wenn der Webserver eine Anfrage empfängt, liest er eine PHP-Datei. Enthält die Datei PHP-Start- und Endmarken wie codice_19 und codice_20, so wird der Text zwischen diesen Markierungen als \"PHP-Programm\" interpretiert, ausgeführt und die Ausgabe des Programms wird an den Webbrowser geschickt.\n\nEine Seite, die die aktuelle Uhrzeit ausgibt, könnte so aussehen:\n\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"de\">\n</html>\nDer PHP-Präprozessor wandelt dies z. B. um in\n\nDer Inhalt einer solchen Seite kann also dynamisch geändert werden. PHP verhält sich also tatsächlich wie ein Vor- oder Präprozessor, da der Originaltext verändert und erst danach zum Browser gesendet wird.\n\nEin in einem CAD-System entworfenes Bauteil, das mittels Finite-Elemente-Methode simuliert werden soll, muss zuvor mit einem Gitter überdeckt werden. Häufig sind zusätzlich Randbedingungen für die Simulation anzugeben, zum Beispiel, dass eine Seite des Bauteils fest eingespannt/verbunden ist, oder Drehzahl und Achse für eine Rotationsbewegung.\nFür derartige Angaben wird meist ein Präprozessor-Programm verwendet, der das entworfene Bauteil für die Weiterverarbeitung vorbereitet. Der Präprozessor muss sowohl das CAD-Format des Entwurfsprogramms einlesen können, als auch das Inputformat des FE-Lösers ausgeben können. Der Präprozessor bietet dann Eingabemasken für entsprechende Angaben.\nAufgaben eines Präprozessors sind unter anderem auch:\n\n"}
{"id": "114144", "url": "https://de.wikipedia.org/wiki?curid=114144", "title": "Macintosh IIfx", "text": "Macintosh IIfx\n\nDer Apple Macintosh IIfx war für seine Zeit ein außergewöhnlich schneller Motorola 68030-basierter Rechner, der mit Preisen ab umgerechnet ca. 9000 EUR in der Basiskonfiguration jedoch für viele unerschwinglich blieb. Gebaut wurde er von Frühjahr 1990 bis Frühjahr 1992.\n\nEingesetzt wurde er für klassisches Numbercrunching auf dem Schreibtisch, wissenschaftliche Arbeiten, DTP- und Grafik-Arbeitsstation oder Abteilungsserver unter A/UX. Ausgeliefert wurde der IIfx mit System 6.0.5, er kann aber auch das modernere Mac OS 7.6.1 oder A/UX bis Version 3.1.1 ausführen.\n\nDie CPU Motorola 68030 sowie der mathematische Coprozessor Motorola 68882 sind mit 40 MHz getaktet. Auf der Hauptplatine finden sich 32 KB Level-2-Cache, acht RAM-Sockel für die proprietären IIfx-SIMMs mit 64 Pins für einen Maximalausbau von 128 MB RAM (für die damalige Zeit unerhört viel), eine SCSI-Schnittstelle mit DMA-Unterstützung, schnelle serielle Schnittstellen sowie ein PDS- und sechs NuBus-Erweiterungssteckplätze.\n\nDie SIMMs des IIfx mussten, wie bei den seinerzeit gängigen 30-Pin SIMMs, in Vierergruppen gleicher Größe und Organisation bestückt werden (eine Bank mit 4 SIMMs). Durch die zusätzlichen 34 Pins pro SIMM war ein gleichzeitig schreibender und lesender Zugriff auf die gleiche SIMM-Bank möglich, was eine weitere Beschleunigung von Speicherzugriffen ermöglichte.\n\nIm Gegensatz zu anderen Modellen mit PDS war beim IIfx vom PDS aus kein Zugriff auf die NuBus-Slots möglich. Dadurch wurde der Bau von Beschleunigerkarten erschwert, es gab nur wenige Ausnahmen: Dort wurde die Beschleunigerkarte im PDS mit einer Kabelbrücke zu einer zusätzlichen, zum Beschleuniger gehörenden NuBus-Karte verbunden und so der Link PDS-NuBus hergestellt.\n\nDas Gehäuseformat wurde vom Mac II übernommen, intern waren zwei HD-Floppies sowie zwei SCSI-Festplatten unterbringbar. Typische Festplattengrößen der Zeit waren 40 bis 200 MB, meist im Format 5,25\". \n\n"}
{"id": "115469", "url": "https://de.wikipedia.org/wiki?curid=115469", "title": "RS/6000", "text": "RS/6000\n\nDie IBM RS/6000 war eine 1990 eingeführte Reihe von Unix-Workstations und Servern, die durch das IBM System p abgelöst wurde. Die Rechner wurden mit dem IBM-Unixderivat AIX ausgeliefert. RS/6000 stand dabei für RISC System/6000. In der RS/6000-Reihe waren Rechner vom kleinen Desktop-Computer bis zum Mehrwege-Computercluster verfügbar.\n\nDie RS/6000-Rechner werden heute nicht mehr vertrieben, aber in vielen Unternehmen oder Universitäten sind auch heute noch viele Geräte im Einsatz. Die Nachfolgereihe der IBM ist das System p und seit 2004 der IBM eServer p5.\n\nDas RISC System/6000 war der Nachfolger des Unix-basierten IBM RT. Während bei den ersten RS/6000-Modellen noch der MCA-Bus zum Einsatz kam, benutzten die neueren Modelle den PCI-Bus. Einige spätere Modelle nutzten die standardisierten PReP- und CHRP-Plattformen, die gemeinsam mit Apple und Motorola entwickelt worden waren. Es war geplant, viele verschiedene Betriebssysteme, wie Windows NT, NetWare, OS/2, Solaris, Taligent, AIX und MacOS, zu nutzen. Allerdings wurde nur die Unix-Variante AIX von IBM genutzt und unterstützt.\n\nEs gab verschiedene RS/6000-Computer-Typen: POWERserver (Server), POWERstation (Workstation) und Scalable POWERparallel (Supercomputer). Während die meisten Rechner in Desktop-, Tower- oder Rack-Gehäusen untergebracht waren, so gab es auch zwei Laptop-Modelle (Modell N40 und Modell 860).\n\nEinige bekannte Computer basieren auf dem RISC System/6000. So zum Beispiel der Schachcomputer Deep Blue, welcher im Jahre 1997 den damaligen Schachweltmeister Garri Kasparow schlug, oder der Supercomputer ASCI White, der von 2000 bis 2002 der schnellste Computer der Welt war.\n\nDas Model N40 war ein PowerPC basiertes Notebook, welches von der Firma Tadpole Technology für IBM entwickelt und hergestellt wurde. Es wurde ab 25. März 1994 für US$12.000 verkauft. Die interne Batterie konnte das System nur für etwa 45 Minuten mit Energie versorgen, weswegen ein externes Batterie-Pack für dieses Notebook verfügbar war.\n\n\n"}
{"id": "116230", "url": "https://de.wikipedia.org/wiki?curid=116230", "title": "Bochs", "text": "Bochs\n\nBochs [] ist ein freier x86- und AMD64-Emulator und Debugger, der den Bedingungen der LGPL unterliegt.\n\nViele Betriebssysteme, wie etwa Windows oder Linux, können unter Bochs betrieben werden. Ebenso ist Bochs für viele verschiedene Betriebssysteme erhältlich.\n\nBochs dient auch als Plattform, um etwa ein Betriebssystem oder hardwarenahe Anwendungen für einen PC zu entwickeln und zu testen. Ziel der Entwickler ist es, eine vollständige PC-Kompatibilität zu erreichen. Da Bochs allerdings als reiner Emulator die Maschinenbefehle nicht direkt ausführt, sind die Ausführungsgeschwindigkeiten entsprechend niedriger als bei virtuellen Maschinen wie VirtualBox, VMware Workstation oder Virtual PC.\n\nBochs ist in der Lage, auch auf einer Nicht-x86-Architektur einen x86-Prozessor zu emulieren. So kann man zum Beispiel unter Mac OS X auf dem Apple Macintosh mit PowerPC-Prozessor oder unter Solaris auf der SPARC-Architektur ein x86-Gastsystem wie Windows nutzen.\n\nInzwischen sind auch Varianten entstanden, wie etwa die Portierung von Bochs auf die PlayStation Portable, das iPhone oder den GP2X.\n\nBochs emuliert die komplette Hardware eines IBM-kompatiblen PC. Dadurch ist die Ausführungsgeschwindigkeit im Verhältnis zur Geschwindigkeit des Wirtsystems sehr langsam, jedoch ist dadurch die virtuelle Hardware auf jedem Wirtsystem gleich. Diese besteht zu einem Minimum aus:\n\nDamit der virtuelle PC wie ein echter PC verwendet werden kann, emuliert Bochs zusätzlich Anschlüsse für virtuelle Festplatten, Audio über eine virtuelle Soundkarte und eine Netzwerkkarte. Dies ist u. a. folgende virtuelle Hardware:\n\n\n"}
{"id": "116516", "url": "https://de.wikipedia.org/wiki?curid=116516", "title": "Monte-Carlo-Algorithmus", "text": "Monte-Carlo-Algorithmus\n\nMonte-Carlo-Algorithmen sind randomisierte Algorithmen, die mit einer nichttrivial nach oben beschränkten Wahrscheinlichkeit ein falsches Ergebnis liefern dürfen. Dafür sind sie im Vergleich zu deterministischen Algorithmen häufig effizienter. Ihr Nachteil besteht darin, dass das berechnete Ergebnis falsch sein kann. Durch Wiederholen des Algorithmus mit unabhängigen Zufallsbits kann jedoch die Fehlerwahrscheinlichkeit gesenkt werden (\"Probability Amplification\", weitere Einzelheiten im Artikel Randomisierter Algorithmus). Im Gegensatz zu Monte-Carlo-Algorithmen dürfen Las-Vegas-Algorithmen nur korrekte Lösungen berechnen.\n\nMonte-Carlo-Algorithmen gibt es für Suchprobleme (Probleme, bei denen eine Lösung zu berechnen ist) und Entscheidungsprobleme (Probleme, bei denen eine Ja/Nein-Frage zu beantworten ist). Bei Monte-Carlo-Algorithmen für Entscheidungsprobleme unterscheidet man zwischen ein- und zweiseitigen Fehlern. Bei einem zweiseitigen Fehler darf ein Monte-Carlo-Algorithmus sowohl \"false Positives\" liefern (also die Ausgabe Ja, obwohl Nein richtig wäre), als auch \"false Negatives\" (also die Ausgabe Nein, obwohl Ja richtig wäre). Bei einseitigem Fehler ist nur eine der beiden Fehlermöglichkeiten erlaubt. Eine häufige Vereinbarung besteht darin, von einem einseitigen Fehler zu sprechen und damit false Negatives zu meinen. Diese Konzepte werden im folgenden Abschnitt verdeutlicht, in dem Komplexitätsklassen für Probleme mit Monte-Carlo-Algorithmen definiert werden.\n\n\nDie angegebenen Schranken für die Wahrscheinlichkeiten müssen jeweils für alle Eingaben gelten; die Wahrscheinlichkeiten beziehen sich jeweils nur auf die vom Algorithmus verwendeten Zufallsbits (und nicht auf die Eingabe, die Eingabe wird also nicht als zufällig aufgefasst). Mit Hilfe von Probability Amplification kann man zeigen, dass die Konstante 2/3 aus der Definition von BPP durch jede andere Konstante aus dem Intervall (1/2,1) ersetzt werden kann, ohne die Menge BPP zu ändern; ebenso kann in den Definitionen von RP und co-RP die Konstante 1/2 durch jede Konstante aus dem Intervall (0,1) ersetzt werden.\n\nObwohl BPP und RP Mengen von Problemen sind, werden im allgemeinen Sprachgebrauch häufig Begriffe wie BPP-Algorithmen oder RP-Algorithmen benutzt, um Algorithmen mit den oben definierten Eigenschaften zu bezeichnen. \n\nZur Verdeutlichung der Definition von RP: Wenn ein RP-Algorithmus die Ausgabe Ja liefert, wissen wir mit Sicherheit, dass die Ausgabe Ja korrekt ist (da die Definition sicherstellt, dass bei korrekter Ausgabe Nein dies auf jeden Fall auch ausgegeben wird). Wenn dagegen ein RP-Algorithmus die Ausgabe Nein liefert, wissen wir nichts über die korrekte Ausgabe (da nach der Definition die Ausgabe Nein möglich ist, wenn Ja oder Nein korrekt wäre).\n\nEin Beispiel für einen Monte-Carlo-Algorithmus ist der Miller-Rabin-Test, bei dem probabilistisch bestimmt wird, ob eine natürliche Zahl prim ist oder nicht. Die Ausgabe des Tests lautet entweder „sicher zusammengesetzt“ oder „wahrscheinlich prim“. Die Wahrscheinlichkeit, dass eine zusammengesetzte Zahl als „wahrscheinlich prim“ klassifiziert wird, liegt pro Durchgang unter 25 % und kann durch mehrfache Ausführung weiter gesenkt werden. Der Miller-Rabin-Test liefert keine Aussage über die Faktoren einer zusammengesetzten Zahl, ist also kein Faktorisierungsverfahren.\nMan wählt hierzu zufällige Punkte formula_1 aus und überprüft (durch Anwendung des Satz von Pythagoras), ob diese innerhalb des Einheitskreises liegen:\n\nÜber das Verhältnis der Anzahl der Punkte innerhalb und außerhalb des Kreises kann dann folgendermaßen formula_3 bestimmt werden:\n\nDas obige Beispiel zur Bestimmung von Pi bildet praktisch das Flächenintegral einer Viertelkreisfläche. Entsprechend kann man das Flächenintegral allgemeiner, auch höherdimensionaler Funktionen nach dieser Methode berechnen. Soll das Integral\n\neiner Funktion formula_6 berechnet werden,\ndann wählt man formula_7 unabhängige im Intervall formula_8 gleichverteilte Punkte formula_9 und approximiert formula_10 durch\n\nIm allgemeineren Fall von höherdimensionalen Funktionen ist das Vorgehen ähnlich. Sei formula_12 eine beliebige formula_13-dimensionale Menge und formula_14 eine integrierbare Funktion. Um den Wert\n\nnäherungsweise zu berechnen, wählt man zufällig in der Menge formula_16 gleichverteilte Punkte formula_17 für formula_18. Dann approximiert\n\nden Wert formula_10 in Abhängigkeit von formula_7 beliebig genau. Um wie oben vorgestellt Pi zu berechnen, muss man formula_22 und formula_23 als charakteristische Funktion des Einheitskreises wählen. Hier ist formula_24 gerade die Fläche des Einheitskreises.\n\nHeutige Supercomputer (HPC) basieren auf massivem Multiprocessing mit vielen tausend Einzelprozessoren, die parallel arbeiten. Diese Gegebenheiten lassen sich besonders gut mit solchen probabilistischen Lösungsverfahren ausnutzen.\n\n\n\n"}
{"id": "116771", "url": "https://de.wikipedia.org/wiki?curid=116771", "title": "PC-Welt", "text": "PC-Welt\n\nDie PC-Welt (eigene Schreibweise: PC-WELT) ist ein monatlich erscheinendes deutschsprachiges Computermagazin. Die Erstausgabe erschien im November 1983, das Magazin ist damit eines der ältesten deutschsprachigen Computermagazine. Zielgruppe sind fortgeschrittene und professionelle Anwender.\n\nDie PC-Welt erscheint im Verlag der International Data Group (IDG), der neben dem US-amerikanischen Schwestermagazin \"PCWorld\" zahlreiche Ableger in vielen Ländern der Welt herausgibt.\n\nEnde 1983 gegründet, entwickelte sich die PC-Welt zur auflagenstärksten PC-Zeitschrift Deutschlands. Auch die schon 1978 als Technikzeitschrift gegründete \"Chip,\" über Jahre der Marktführer bei der PC-Presse, wurde rasch überholt. Seit 2002 hat die PC-Welt jedoch mit Auflagenproblemen zu kämpfen. Die Zahl der am Kiosk verkauften Hefte halbierte sich gegenüber dem Höchststand im 4. Quartal 2001 fast, die Zahl der zu niedrigen Preisen abverkauften Hefte („Sonderverkäufe“ und „Bordexemplare“) nahm von nur 88 (4. Quartal 1998) über 19.944 (4. Quartal 2003) auf 98.487 Exemplaren im 4. Quartal 2009 erheblich zu. Die PC-Welt verlor weit über dem Branchendurchschnitt an Auflage. Die Chefredakteure und die Verlagsleiter wechselten in schneller Folge. Seit dem 4. Quartal 2007 hat Chip bei der verkauften Auflage wieder die Nase vorn. Auch online hat die PC-Welt das Nachsehen; die Website von Chip ist seit ihrem Bestehen eine der meistbesuchten PC-Websites in Deutschland.\n\nViel zur einstigen Auflagenrallye hat die kritische Berichterstattung der PC-Welt beigetragen. Vor allem auf die Schwächen der Microsoft-Software wurde regelmäßig hingewiesen. 1998 etwa veröffentlichte die Zeitschrift den Artikel „Windows 95 unautorisiert“ (Ausgabe 03/1998), der die Zusammensetzung des CD-Keys erläuterte. Im Artikel war eine gültige Schlüsselnummer für Windows 95 abgedruckt, um zu zeigen, wie simpel Microsoft versuchte, Programme zu schützen: Die letzte siebenstellige Zahl des CD-Keys musste lediglich durch sieben teilbar sein, um einen gültigen Schlüssel darzustellen. Microsoft war über diesen und andere Tipps in dieser Titelstory empört, zog (erfolglos) vor Gericht und warf der Redaktion „Aufforderung zum Rechtsbruch“ und „Verrat von Geschäftsgeheimnissen“ vor. Der Artikel führte zu einem jahrelangen Boykott von Microsoft gegenüber der Zeitschrift. Ansonsten berichtet die PC-Welt auch über die Welt jenseits von Microsoft: Als Sonderheft-Reihe gibt es mittlerweile auch ein Linux-Magazin, das 4-mal pro Jahr erscheint.\n\nZu Zeiten von MS-DOS wurden immer wieder Artikel publiziert, in denen beschrieben wird, wie man DOS beschleunigt; unter anderem wurde behauptet, dass sich auf Basis des Windows-3.11-Kernels schon vor Erscheinen von Windows 95 ein 32-Bit-DOS zaubern ließe. Diese Art von Artikeln brachte der PC-Welt in Fachkreisen scherzhaft den Titel „Bildzeitung der Computerzeitschriften“ ein, lange bevor es die „echte“ \"Computer Bild\" gab. In Briefen beschwerten sich Leser, wenn alles stimmen würde, was die PC-Welt publiziert, dann müsste der eigene Rechner „mittlerweile mit Lichtgeschwindigkeit“ laufen.\n\nIm März 2009 erteilte der Deutsche Presserat der PC-WELT eine öffentliche Rüge. Der Presserat kritisierte damit einen Artikel über Hacker-Tools, der im Heft 10/2008 erschienen war. Eine solche Berichterstattung über nicht legale Programme entspräche nicht den journalistischen Grundsätzen. Das Ansehen der Presse gerate in Gefahr, wenn eine Zeitschrift „Gebrauchsanweisungen“ für verbotene Software gäbe.\n\n\nDie PC-Welt erscheint derzeit in drei Ausgaben: als reine Magazinausgabe ohne Datenträger, als DVD-Ausgabe und als Plus-Ausgabe mit 32 zusätzlichen Seiten und zwei DVDs. Anfang Juni 2014 erreichte ein Schreiben der Abonnementsverwaltung die PC-Welt-Bezieher mit der Mitteilung, dass die normale DVD-Ausgabe mit einem Datenträger eingestellt ist und man in Zukunft mit der Plus-Version beliefert würde. Hinweis (da fehlend im IntanService-Schreiben): Diese kostet pro Ausgabe 2 Euro mehr, und man kann natürlich außerordentlich kündigen. Im Juni 2015 erschien vorübergehend letztmals die DVD-Ausgabe (mit einer DVD). Seit Juli 2015 ist nur noch die Plus-Ausgabe als Abo erhältlich. Seit September 2015 ist die normale DVD-Ausgabe wieder im Handel erhältlich.\n\nIm vierten Quartal 2014 lag die durchschnittliche monatlich verbreitete Auflage nach IVW bei 105.386 Exemplaren. Das sind 30,23 Prozent (45.680 Hefte) weniger als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 14,06 Prozent auf 53.432 Abonnenten ab. Derzeit beziehen 50,70 Prozent der Leser die Zeitschrift im Abonnement.\n\nPC-Welt Online bietet täglich aktualisierte News, Tipps und Tests rund um den PC. Zusätzlich gibt es ein umfangreiches Angebot an täglichen Newslettern, die per E-Mail direkt an den Besteller geschickt werden. Der Zuspruch der Website steigt. Im Januar 2008 hatte www.pcwelt.de folgende IVW-geprüfte Zahlen: 12.480.042 Visits (Besucher) und 45.850.023 Pageimpressions (Seitenabrufe). Das ist im 5-Jahres-Vergleich eine spürbare Verbesserung, vor allem bei den Besuchern (Januar 2003: 5.553.277 Visits, 36.905.898 Pageimpressions). Der direkte Wettbewerber Chip-Online legte im selben Zeitraum von 12.373.789 Visits und 88.119.527 Pageimpressions (Januar 2003) auf 32.468.517 Visits und 205.066.148 Pageimpressions (Januar 2008) zu. Im Januar 2015 erreichte PC-Welt Online 9.770.264 Visits und 24.024.744 PIs, wobei allerdings nur 57 % des Traffics auf pcwelt.de entfallen. Sie hat damit eigentlich 5.569.051 Visits und 13.694.104 PIs. Der Rest wird laut IVW u. a. von der polnischen Website pcworld.pl erzielt.\n\nAb Anfang September 2009 kooperierte pcwelt.de mit RapidShare.com. Auf dem bekannten One-Click-Hoster wurden die Downloads von pcwelt.de gehostet. Die Kooperation endete mit der Schließung von RapidShare Ende März 2015. Seitdem bietet PC-Welt die Dateien über eigene Server zum Download an oder leitet auf die Seite des Entwicklers um.\n\n"}
{"id": "117684", "url": "https://de.wikipedia.org/wiki?curid=117684", "title": "Windows 3.x", "text": "Windows 3.x\n\nUnter der Bezeichnung Windows 3.x werden die Vorgänger der späteren Windows-Betriebssysteme des Softwareunternehmens Microsoft für x86-Prozessoren der Versionen 3.x zusammengefasst. Windows war bis Version 3.x ein grafischer Aufsatz für ein PC-kompatibles DOS-Betriebssystem wie MS-DOS.\n\nDie bekanntesten Windows-3.x-Versionen sind:\n\nEs gab noch weitere Versionen, die jedoch keine ähnlich große Verbreitung fanden. Auf eingebetteten Systemen wie Kassensystemen oder Ticketautomaten kam Windows 3.x noch fast 20 Jahre nach seiner Markteinführung zum Einsatz, Lizenzen dafür verkaufte Microsoft noch bis Ende Oktober 2008.\n\nMit der Windows-3.x-Reihe begann der Übergang von reinen 16-Bit-x86-Systemen (retronym auch als IA-16, ', bezeichnet) des Intel 8086 (nur Real Mode) und 80286 (zusätzlich \"16-Bit\") hin zu 32-Bit-x86-Systemen (auch bekannt als ', kurz IA-32) ab dem 80386 (\"32-Bit \" und \").\n\nSo hatte Windows 3.0 (1990) drei unterschiedliche Kernel: einen für 8086 (), einen für den 80286 (zusätzlich ) und einen für den 80386 (zusätzlich ). Je nachdem, welcher Prozessor im PC vorhanden war, startete Windows automatisch mit dem entsprechenden Kernel. Bei Windows 3.1 (1992) wurde der 8086-Kernel weggelassen und in Windows für Workgroups 3.11 (1993) gab es nur noch den 80386-Kernel.\n\nWindows 3.x setzte noch ein laufendes MS-DOS (oder kompatibel, also auch PC DOS oder DR DOS) voraus, auf dem es lief. In diesem Bezug war es nicht anders als die älteren Windows-Versionen bis Windows 2.0. Mit dem 80286- und 80386-Kernel bot Windows jedoch einen erweiterten Speichermanager, der DOS fehlte. Windows war damit bereits mehr als ein grafischer Aufsatz für das Betriebssystem MS-DOS, wenn es auf einem modernen Prozessor lief. Auf einem 80386 im \" fungierte der Kernel als DPMI-Client, womit auch der DOS-Unterbau Multitasking-fähig wurde, denn es konnten mehrere DOS-Programme parallel ablaufen. Auch gab es bereits erste 32-Bit-Windows-Gerätetreiber, obwohl Windows 3.x auch weiterhin die 16-Bit-Treiber von DOS verwenden konnte.\n\nMit Windows 3.0 und 3.1 gelang Microsoft der Durchbruch auf dem Markt für grafische PC-Betriebssysteme. Die eigentliche Bedeutung steckte jedoch in der stabilen Programmierschnittstelle (, kurz API), die in ihrer 16-Bit-Ausprägung auch \"Win16\" genannt wurde. 16-Bit-Windows-Programme aus Windows 2.0 funktionierten dabei weiterhin, jedoch nur im Real Mode.\n\nWindows 3.x bereitete den Weg hin zu Windows 9x, das als eigenständiges Betriebssystem den MS-DOS-Unterbau in das Betriebssystem integrierte und die 32-Bit-Funktionen sowohl im 32-Bit-API \"Win32\" als auch bei Kernkomponenten wie dem Speichermanager und Multitasking erweiterte.\n\nBeide Generationen, also Windows 3.x als auch Windows 9x (welches als Windows 4.x entwickelt wurde), waren in der Retrospektive Lückenfüller für das neu entwickelte Windows NT, das ein vollständiges 32-Bit-Betriebssystem war – auf der damaligen Hardware jedoch zu ressourcenhungrig und zu teuer. Erst mit Windows XP gelang Microsoft der vollständige Umstieg auf die mit Windows NT eingeführte neue Technik.\n\nWindows 3.x/9x und Windows NT haben ein ähnliches und in großen Teilen identisches API. Unter Windows 3.x konnte eine abgespeckte Variante der \"Win32\"-API nachinstalliert werden, \"Win32s\", während \"Win16\"-Applikationen auch unter Windows-NT-Versionen weiterhin lauffähig waren. Erst in 64-Bit-x86-Versionen von Windows, also ab Windows XP x64 Edition (2005) bzw. Windows Vista x64 (2007), steht das \"Win16\"-API nicht mehr zur Verfügung.\n"}
{"id": "117934", "url": "https://de.wikipedia.org/wiki?curid=117934", "title": "Liste von Mac-Clones", "text": "Liste von Mac-Clones\n\nDas klassische Mac OS des Computerherstellers Apple, damals System 7 (umbenannt in Mac OS ab Version 7.6), wurde von Sommer 1994 bis September 1997 an andere Computerhersteller lizenziert. Die aus dieser Lizenz resultierenden Macintosh-kompatiblen Geräte, die Macintosh-Klone oder genannt wurden, sind hier aufgelistet. Die Umbenennung von \"System\" in erfolgte um diese Klone vom Markt zu verdrängen.\n\nDie folgende Liste gibt eine Übersicht der Lizenznehmer und deren Modellreihen mit Prozessor und Taktfrequenz.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "118366", "url": "https://de.wikipedia.org/wiki?curid=118366", "title": "FrameMaker", "text": "FrameMaker\n\nFrameMaker ist ein professionelles Autorenwerkzeug zur Verwaltung und printorientierten Präsentation von technischen Dokumenten, das ursprünglich von der Firma Frame Technologies entwickelt und vertrieben wurde; Mitte der 1990er Jahre hat Adobe Systems die Firma übernommen.\n\nFrameMaker wurde ursprünglich für das Betriebssystem SunOS der Firma Sun entwickelt und später neben vielen Unix-Systemen auf die Plattformen Apple Macintosh und Windows portiert. Die Macintosh-Portierung wird jedoch seit 2004 nicht mehr weiterentwickelt. Heute ist FrameMaker nur noch für Microsoft Windows erhältlich.\n\nFrameMaker ist heute das führende Programm zur Erstellung technischer Dokumentation.\n\nDie Stärken des Programms liegen im Bereich technischer Dokumentationen, die viele tausend Seiten umfassen können. Im mathematisch-naturwissenschaftlichen Umfeld ist FrameMaker wegen seiner praxistauglichen Formelsatzimplementation eine brauchbare Alternative zu LaTeX. FrameMaker bietet auch eine gute Unterstützung für die Erstellung von SGML- und XML-Dokumenten.\n\nFrameMaker ist ein Programm zum Erstellen und Bearbeiten komplexer und hochwertig gelayouteter Dokumente mit semiautomatisch generierten Verzeichnissen (Inhaltsverzeichnis, Index, u.v.m.), Querverweisen, stabilem Handling importierter Abbildungen und Ausgabemöglichkeiten für alle Bedürfnisse im Bereich der anspruchsvollen technischen Dokumentation.\n\nDie Eigenschaften von FrameMaker können durch die Programmierung eigener Plug-ins mit dem \"FrameMaker Developer's Kit\" (in C++) erweitert werden. Ähnliche Möglichkeiten bietet \"FrameScript\" (in Deutschland: ElmScript), eine Skriptsprache für FrameMaker; viele Skripte sind in einschlägigen Webseiten oder Listen frei erhältlich. Ab Version 10 bringt FrameMaker mit ExtendScript eine eigene Skriptsprache (auf Basis JavaScript) mitsamt Editor-Umgebung mit, wofür ebenfalls eine Vielzahl freier oder kommerzieller Skripte erhältlich sind.\n\nDas Programm arbeitete bis Version 7.2 intern mit dem \"Macintosh-Roman\"-Zeichensatz und hatte daher Schwierigkeiten mit nichtlateinischen Fonts. Die Schrift-Engine von FrameMaker ist ab Version 8.0 Unicode-fähig und ermöglicht auch die Publikation z. B. indischer Texte, wenn entsprechende Fonts installiert sind. Ab Version 13.0 unterstützt FrameMaker die Publikation von Texten in Sprachen, die von rechts nach links gesetzt werden (arabisch, hebräisch).\n\nWährend seines Astrophysikstudiums an der Columbia University begann Charles \"Nick\" Corfield, einen WYSIWYG-Editor auf einer Sun-2-Workstation zu schreiben. Die Anregung stammte von seinem Zimmernachbarn Ben Meiry, der einen Markt für ein professionelles DTP-Programm erkannte.\n\nDas damals einzige DTP-Programm war Interleaf, das auch auf Sun-Workstations lief, jedoch mit einer Reihe von Einschränkungen behaftet war. Während Meiry sich um die Technik und die Verbindungen kümmerte, entwickelte Corfield die Algorithmen. Nach wenigen Monaten war ein stabiler Prototyp von FrameMaker entstanden. Einem Vertriebsmann von Sun Microsystems gefiel das Programm und Corfield erlaubte ihm, FrameMaker als Grafikdemo auf Sun-Rechnern einzusetzen.\n\nSteve Kirsch erkannte das Potenzial von FrameMaker und gründete mit Corfield (und anderen) die Firma Frame Technology Corp., um das Programm marktreif zu machen. FrameMaker wurde ein populäres Schreibprogramm für die Technische Dokumentation und brachte bald Geld ein. Ursprünglich für SunOS (eine Unix-Variante) geschrieben, wurde FrameMaker auf Apple Macintosh portiert, der sich zu der Zeit sehr gut verkaufte.\n\nIn den frühen 1990er Jahren finanzierten mehrere Hersteller von Unix-Workstations (Apollo, Data General, MIPS, Motorola und Sony) die Portierung für ihre Geräte. Auf dem Höhepunkt des Erfolges war FrameMaker auf über 13 UNIX-Plattformen einsetzbar, darunter auch AIX von IBM und NeXTStep. Zu diesem Zeitpunkt ermöglichte FrameMaker Autoren die Herstellung qualitativ hochwertiger Dokumente mit guter Typografie.\n\nFrame Technology portierte FrameMaker später auf Windows. Zielkunden waren bis zu diesem Zeitpunkt professionelle Autoren umfangreicher und stark technisch geprägter Publikationen (z. B. eines Wartungshandbuchs für die Boeing 777); dementsprechend kostete FrameMaker 2500 US-Dollar pro Lizenz. Die Windowsversion wurde jedoch für 500 US-Dollar angeboten, was das Programm für den Privatanwender erschwinglich machte, wodurch jedoch das bisher recht einheitliche Kundenprofil zerstört wurde. Das komplexe FrameMaker war für Privatanwender viel zu lernintensiv. Das Ausbleiben des erhofften Absatzes brachte die Firma an den Rand des Ruins.\n\nFrameMaker wurde schließlich von Adobe Systems aufgekauft und der Vertrieb wieder auf die professionelle Kundschaft ausgerichtet. Die Weiterentwicklung der Software verlief hingegen schleppend: Nach der Einführung von macOS vernachlässigte Adobe die Macintosh-Plattform und gab sie schließlich im Jahr 2004 ganz auf. Die aktuelle Version unterstützt ausschließlich Microsoft Windows.\n\nDie zu Beginn getrennt vertriebenen Programmversionen \"FrameMaker\" und \"FrameMaker+SGML\" (früher FrameBuilder; sehr teuer; hauptsächlich in der Luftfahrt eingesetzt) wurden 2002 mit FrameMaker 7 zu einer einzigen Version verschmolzen. Während der Installation (und auch später noch) kann seitdem entschieden werden, ob mit \"FrameMaker\" oder \"Strukturierter FrameMaker\" (XML-basiert) gearbeitet werden soll.\n\nIm Bereich der technischen Dokumentation hatte FrameMaker einige Konkurrenzprodukte, die aber mit der wachsenden Popularität von Microsoft Word sämtlich vom Markt verschwanden.\n\n\n\n\n\n"}
{"id": "118599", "url": "https://de.wikipedia.org/wiki?curid=118599", "title": "4th Dimension", "text": "4th Dimension\n\n4th Dimension (oder 4D) ist eine Entwicklungsumgebung für Datenbankanwendungen. 4D wurde ursprünglich für das klassische Mac OS entwickelt. Seit 1995 ist 4D auch für Microsoft Windows verfügbar. Der Hersteller von 4D ist die französische Firma \"4D SAS\".\n\nSeit der ersten Version von 4D im Jahr 1984, die von Laurent Ribardière als Relationale Datenbank (RDBMS) umgesetzt wurde, entwickelte sich das Produkt kontinuierlich von einer Einzelplatzanwendung zu einem Client-Server-System mit zahlreichen Plug-ins und Schnittstellen über ODBC zu anderen gängigen Datenbanksystemen wie (MySQL oder Oracle) weiter. 1992 wurde die 4D Client-Server-Version vorgestellt.\n\nMit der aktuellen Version von 4th Dimension (4D v17) besteht die Möglichkeit, Einzelplatzanwendungen, Client-Server-Lösungen, Webserverintegration und andere webserviceorientierte Anwendungen (wie SOAP-XML) mit nur einem Produkt zu erstellen. 4th Dimension steht unter anderem in Sprachen wie Deutsch, Englisch, Französisch, Spanisch, Japanisch, Hebräisch, Arabisch oder Koreanisch zur Verfügung.\n\n\n"}
{"id": "120881", "url": "https://de.wikipedia.org/wiki?curid=120881", "title": "Klammergriff", "text": "Klammergriff\n\nBenutzer von Personal Computern und anderen Rechnersystemen bezeichnen im EDV-Jargon mit Klammergriff, Affengriff oder auch (wegen der seltsam anmutenden Hände-Haltung) Geiergriff bzw. Geierkralle (kurz: Kralle) die Tastenkombination ++ (++), mit der nicht mehr reagierende (abgestürzte) Programme beendet oder der Computer \"warm\" neu gestartet werden kann. Diese Tastenkombination ist in der Regel nur beidhändig zu erreichen, um eine versehentliche Betätigung zu erschweren. In der Regel wird durch die Tastenkombination eine administrative Funktion des Betriebssystems oder der grafischen Oberfläche aufgerufen, die hilft, die Störung zu beheben.\n\nDer Begriff entwickelte sich von der Kombination ++, erstmals auf IBM-PCs unter MS-DOS. Es handelt sich jedoch nicht um einen Befehl von DOS, sondern um einen fest im Computer-BIOS integrierten Befehl. Ursprünglich hatte David Bradley, ein Informatikingenieur, dafür die Kombination ++ vorgesehen, diese konnte aber zu leicht versehentlich gedrückt werden, etwa wenn man mit der Hand links auf der Tastatur abrutscht. Die Kombination ++ wurde dann gewählt, da man sie kaum versehentlich drücken kann. Auf älteren Tastaturen, die nur links Strg- und Alt-Tasten hatten, konnte sie nur mit zwei Händen ausgelöst werden; auf neueren Tastaturen kann die gleiche Funktion mit ++ auch mit einer Hand ausgelöst werden.\n\nDie genaue Funktion von ++ ist abhängig vom Betriebssystem. Unter MS-DOS und OS/2 wird der Rechner ohne Nachfrage neu gestartet, wobei OS/2 vorher noch einige Aufräumarbeiten erledigt. Seit Windows NT wird die Kombination ++ als SAS (\"Secure Attention Sequence\") bezeichnet und ein Interrupt ausgelöst, heute wird der Begriff jedoch allgemeiner verwendet. Laut Microsoft kann diese Tastenkombination von keiner Applikation auf Hardwareebene emuliert werden, auch Fernwartungsprogramme wie VNC senden diesen Befehl nur als emulierten Tastaturbefehl an das Betriebssystem, ein sofortiger Reset ist auch hier nicht möglich.\n\nDiese Drei-Tasten-Kombination ist seit langem in das PC-BIOS integriert, es ist ein Software-Befehl, der fest in den PC eingebaut ist. So kann man auch einen PC, der in der Startsequenz hängengeblieben ist, zu einem sanften Reset bringen, also einen Warmstart auslösen. Ein Betriebssystem auf einem IBM-kompatiblen PC muss diesen BIOS-Befehl abfangen, damit nicht sofort ein Reset ausgeführt wird. Betriebssysteme, die die Standardbiosfunktion nicht überschreiben (wie zum Beispiel MS-DOS), können das nicht.\n\nIn neueren Windows-Versionen wird die Tastenkombination genutzt, um ein umfangreiches Menü mit Befehlen zum Abmelden oder Herunterfahren des Computers, Passwort ändern, Sperren der Station aufzurufen, in dem auch der Taskmanager geöffnet werden kann. Dieser ist aber auch durch ++ direkt aufrufbar. Mit ihm kann man Computerprogramme beenden, starten und auch überwachen. Des Weiteren bietet er die Möglichkeit, Auslastungen des Prozessors und des Arbeitsspeicher einzusehen und einzelne Tasks auf Prozesse und somit auf geöffnete Dateien zurückzuführen. Auch das Betriebssystem lässt sich über den Taskmanager herunterfahren, neustarten oder in den Standby-Modus schalten.\n\nÜbersicht:\n\nUnter IRIX lautet der Affengriff +++ (d. h. auf dem Ziffernblock).\n\nLinux besitzt gleich mehrere Affengriffe. Neben ++, das in Textmodus-Konsolen direkt einen Reboot auslösen kann (abhängig vom dazu nötigen \"ctrlaltdel\"-Eintrag in der /etc/inittab), gibt es Kombinationen mit der sog. \"Magischen S-Abf-Taste\". Diese werden mit + (auf englischsprachigen Tastaturen +) ausgewählt, wobei S-Abf, außer auf einigen Notebooks, eine Alternativbelegung der Taste (engl. ) ist, und zwar +. Ein möglichst sauberer Neustart eines abgestürzten Linux-Rechners wird durch Festhalten von + und währenddessen dem nacheinanderfolgenden Drücken der Tasten , , , , , ausgelöst. Statt kann auch verwendet werden, was mit einer Hand oft leichter machbar ist.\n\nUnter Linux wird per ++ ein Signal an den codice_1-Prozess gesendet, der dann ein konfiguriertes Kommando ausführt (z. B. Rechner neu starten). Ein lokaler X-Server fängt diese Tastenkombination jedoch ab und erlaubt es, eigene Funktionen aufzurufen. Das wird von Umgebungen wie KDE oder GNOME genutzt, um eine konfigurierbare Dialogbox darzustellen, die so dem Look and Feel von Windows recht ähnlich ist und teilweise darüber hinausgehende Funktionen bietet.\n\nLinux-Distributionen mit systemd ab Version 219 starten neu, wenn mehr als 7x innerhalb von 2s ++ gedrückt wird.\n\nUnter der plattformübergreifenden   grafischen Benutzeroberfläche X Window System steht ebenfalls eine Reihe von Affengriffen zur Verfügung.\nDer bekannteste davon ist möglicherweise ++ (Rücktaste), der die grafische Oberfläche und alle unter ihr laufenden Prozesse sofort beendet. Falls ein Display-Manager aktiv ist, wird die grafische Oberfläche nach Drücken der Tastenkombination vom Display-Manager neu gestartet und zeigt dann typischerweise einen neuen Anmeldebildschirm. Bei manchen Systemen ist diese Tastenkombination deaktiviert; sie wird durch die Optionen \"DontZap\" und \"HandleSpecialKeys\" in der ServerFlags-Sektion der /etc/X11/xorg.conf gesteuert.\nManchmal (z. B. bei OpenSUSE) ist der X-Server so konfiguriert, dass zuerst ein langer Piepton als Warnung ertönt und der X-Server erst bei einem erneuten ++ beendet wird.\n\nUm hängengebliebene Prozesse abzubrechen, wird entweder die Tastenkombination ++ gefolgt vom Anklicken des entsprechenden Fensters verwendet (das ruft das Programm xkill auf) oder die Tastenkombination +, die ein Fenster ähnlich der Registerkarte Prozesse des Windows Taskmanagers aufruft.\n\nDieses Verhalten ist jedoch vollständig abhängig vom verwendeten Window Manager und kann unter Umständen stark abweichen. Manche Window Manager stellen auch gar keine Notfall-Tastenkombinationen zur Verfügung, so dass oft auf den Konsolenwechsel mit ++ bzw. bis zurückgegriffen werden muss.\n\nBei Macintosh-Rechnern ist die Tastenkombination, um ein einzelnes Programm zu beenden: (Befehlstaste)+ (Wahltaste)+.\n\nFür einen Neustart: Gleichzeitig + und Einschaltknopf (abgesetzte Taste mit nach links zeigendem Dreieck) drücken, bei neueren Macintosh-Rechnern (ohne Einschaltknopf auf der Tastatur) + und \"Auswerfen\".\n\nParameter-RAM löschen: Nach dem Einschalten des Computers +++ gedrückt halten, bis der Startsound zum dritten Mal erklungen ist, dann Tasten freigeben.\n\nDie Rechner der älteren Apple-II-Serie (nicht zu verwechseln mit dem Macintosh II) haben eine -Taste auf der Tastatur, die direkt den Prozessor zurücksetzt. Bei allen Apple-II-Modellen außer dem seltenen Urmodell bewirkt jedoch nur dann überhaupt etwas, wenn zugleich gedrückt gehalten wird (beim Apple II+ ist dies über einen Schiebeschalter im Rechnerinneren, an der Tastaturplatine, wählbar, aber standardmäßig eingeschaltet). Dieser einfache Reset durch die Tastenkombination + kann ab dem Apple II+ von Programmen abgefangen werden, er wird auch schon von der Standard-Firmware des Rechners abgefangen und bewirkt in diesem Fall nur einen Stopp des eventuell laufenden Programms und ein Zurücksetzen des Bildschirms in den Textmodus; der Apple II+ kann daher nur durch Aus- und Wiedereinschalten komplett neu gestartet werden. Ab dem Modell Apple IIe gibt es zusätzlich die nicht abfangbare Tastenkombination ++ (bzw. ++ auf dem Apple II und dem Apple IIc+), die ohne Nachfragen einen Neustart auslöst. Die Tastenkombination ++ (bzw. ++) bewirkt dagegen einen Selbsttest des Systems. Auf dem Apple II dient diejenige Taste als -Taste, die auf dem Macintosh als Einschaltknopf dient – sie ist üblicherweise mit einem nach links zeigenden Dreieck markiert.\n\nAuf Amiga-Rechnern besteht der Affengriff aus der Tastenkombination ++. Diese wird schon vom in der Tastatur eingebauten Mikrocontroller erkannt; dieser wirkt direkt auf das Resetsignal des Computers. Je nach Ausführung verfügt der Mikrocontroller über eine eigene Verbindung zum Resetsignal (z. B. Amiga 500), oder er erzeugt eine ansonsten nicht verwendete Signalfolge auf der Datenverbindung zum Computer, die von einer Schaltung im Computer erkannt wird, welche daraus ein Resetsignal erzeugt (z. B. Amiga 1000). Dadurch braucht kein Teil des Betriebssystems zu laufen; ein Abfangen seitens der Software ist nur durch Einklinken in den Bootvorgang möglich.\n\nBeim VC 20, C64 und C128 löst die Taste , die als einzige Taste direkt mit der CPU verbunden ist, einen NMI (non maskable Interrupt) aus, der normalerweise vom Betriebssystem oder der Anwendung abgefangen wird und daher keine sichtbaren Auswirkungen zeigt. Wird aber während des Drückens von die Taste gedrückt gehalten, führt das Betriebssystem eine Art Soft-Reset aus, der ein laufendes Programm anhält (aber im Speicher belässt), den Sound- und den Videochip und die anderen Interfacebausteine auf die Starteinstellungen zurücksetzt, den Bildschirm löscht und den Benutzer zurück zur BASIC-Eingabeaufforderung bringt. Dieser Soft-Reset ist aber nur bei nicht abgestürztem Prozessor und noch halbwegs intaktem Speicherinhalt wirksam. Auf der alten C64-Variante im braunen Brotkasten-Gehäuse funktioniert wegen eines falsch dimensionierten Kondensators nur dann, wenn die Taste ganz kurz und kräftig angeschlagen wird; auf neueren C64 und auf den anderen Commodores kann sie aber auch ganz normal gedrückt werden.\n\nWeil in vielen Fällen nicht das gewünschte leistet, bauen viele Anwender zusätzlich einen Hardware-Resettaster ein oder schließen diesen extern am Userport bzw. dem seriellen Bus (CBM-Bus, 6-Pin-DIN-Buchse) an. Da über den seriellen Bus z. B. auch das Diskettenlaufwerk oder der Drucker angeschlossen wird, werden diese Geräte ebenfalls zurückgesetzt. Spätere Commodore-Modelle hatten bereits ab Werk einen seitlich angebrachten Hardware-Resettaster. Auch das Drücken eines solchen Hardware-Resettasters kann von entsprechend gestalteten Programmen abgefangen werden, viele kopiergeschützte Spiele löschen in diesem Fall z. B. den Speicher, um die Erstellung einer sog. Raubkopie zu erschweren.\n\nAuf einer 5250-Terminal-Sitzung im Betriebssystem OS/400 (heißt ab Version V5R3 i5/OS) kann man mit dem Affengriff ++ das Systemanfrage-Menü aufrufen. Hier gibt es dann die Möglichkeit, das vorherige Programm abzubrechen, einen alternativen Job (Sitzung) zu eröffnen oder den aktuell laufenden Dialogjob ganz aus dem System abzumelden. Die Terminal-Sitzung ist nur der Client, das Programm läuft auf dem Server. Die Taste ist, anders als bei der klassischen PC-Tastatur, eine Extrataste links oben, denn die Tastaturen für die AS/400 haben 122 Tasten.\n\n\n"}
{"id": "121009", "url": "https://de.wikipedia.org/wiki?curid=121009", "title": "XFS (Dateisystem)", "text": "XFS (Dateisystem)\n\nXFS ist ein vom Unternehmen Silicon Graphics (SGI) entwickeltes Journaling-Dateisystem für Unix-artige Betriebssysteme wie Linux. Das bis Ende 1994 ausschließlich für IRIX entwickelte 64-Bit-Dateisystem ist bekannt für seine hohe Geschwindigkeit. Seit dem 1. Mai 2001 ist das Dateisystem auch offiziell für Linux ab Version 2.4 quelloffen erhältlich. Seit Kernel-Version 2.6 ist es offizieller Bestandteil des Kernels. XFS bietet Zugriffskontrolllisten und ab der Version 1.0 unterstützt XFS auch Quotas sowohl für den einzelnen Benutzer als auch für Gruppen. Für den gleichzeitigen und konfliktfreien Zugriff auf XFS gibt es die proprietäre Lösung CXFS (Cluster XFS).\n\nXFS ist eines der ältesten für Unix verfügbaren Journaling-Dateisysteme überhaupt. Es zeichnet sich durch eine gereifte, weitgehend fehlerfreie Codebasis aus. Ursprünglich begann die Entwicklung von XFS bei SGI; es wurde erstmals im Jahr 1994 auf IRIX vorgestellt, wo es ab IRIX Version 5.3 bzw. 6 das bis dahin genutzte (EFS) ablöste. Im Jahr 2000 wurde XFS schließlich unter der GPL lizenziert und erschien 2001 erstmals auf Linux. Fast alle heutigen Linux-Distributionen beinhalten XFS-Unterstützung.\n\n\nDas XFS-Dateisystem ist beispielsweise in NAS-Geräten anzutreffen und auch in Fernsehgeräten mit USB-Recording-Funktion. XFS wird nativ weder von Windows noch von MacOS unterstützt.\n\nUm schnell auf spezielle Dateien zugreifen zu können, werden bei XFS Verzeichnisinhalte in einem B⁺-Baum abgespeichert. Dies erhöht in geringem Umfang zwar die Latenzzeit bei der Ausgabe eines kompletten Verzeichnisinhaltes, verringert aber die Zugriffszeit auf einzelne Dateien bei Verzeichnissen mit vielen Dateieinträgen.\n\nXFS zeichnet sich durch eine vollständige 64-Bit-Konzeption aus. Die Datenstrukturen sind darauf ausgelegt, Dateien mit einer Größe von bis zu 8 Exbibyte (2) auf einem XFS-formatierten Datenträger anzulegen. Heutige Betriebssysteme nutzen diese Grenzen noch nicht aus. So unterstützt Linux 2.4 eine maximale Dateigröße von 16 Tebibyte (2 = 2 · 2) bei einer Speicherseitengröße von 4 Kibibyte (2) und 64 Tebibyte (2 = 2 · 2) bei einer Speicherseitengröße von 16 Kibibyte (2).\n\nDas von XFS geführte Journal wird seriell abgelegt (es erfolgt keine Ablage in komplexen Datenstrukturen wie Bäumen oder Heaps). Dabei kann das Journal sowohl in dafür reservierten Bereichen auf dem entsprechenden Datenträger abgelegt als auch auf externen Speichermedien geführt werden. XFS fügt Transaktionen auf dem Dateisystem jedoch asynchron (der Dateisystem-Treiber arbeitet blockierungsfrei) zum Journal hinzu. Dadurch können Operationen schneller durchgeführt werden als auf vergleichbaren Systemen, im Falle einer Störung (Stromausfall) können aber einige Eintragungen im Journal fehlen.\n\nEine an einen Fehlerfall anschließende Überprüfung des Dateisystems wird jedoch zumindest eine Konsistenz wiederherstellen und Datenbereiche, die nicht geschrieben werden konnten, durch Nullen auffüllen. Dadurch sind mögliche Fehler durch „Datenreste“ ausgeschlossen.\n\nEine weitere Besonderheit von XFS sind sogenannte \"Belegungsgruppen\" (). Diese bilden eine eigene Einheit im XFS-System und verwalten eigenständig sowohl den freien Speicher als auch Inodes. Dadurch können mehrere Prozesse gleichzeitig auf ein Dateisystem zugreifen (sofern jeder Prozess auf eine andere Belegungsgruppe zugreift).\n\nInformationen über freie Speicherbereiche werden in B⁺-Bäumen abgelegt, wodurch es möglich ist, passende Speicherbereiche zu finden und so eine Fragmentierung größtenteils zu vermeiden.\n\nXFS unterstützt Blockgrößen von 512 Byte bis 64 Kibibyte. Dadurch lässt sich ein Dateisystem angepasst der erwarteten Nutzung anlegen. Sowohl kleine als auch große Dateien können gut verwaltet werden.\n\nNeben der größenbasierten Belegung bietet XFS auch noch eine weitere Verringerung möglicher Fragmentierung durch verzögerte Belegung. Dabei werden Dateien möglichst lange im Speicher gehalten, bevor sie auf den Datenträger geschrieben werden. Dadurch erhöht sich die Wahrscheinlichkeit, dass der XFS-Treiber einen passenden Speicherbereich finden und so auf Fragmentierung verzichten kann. Allerdings ist dadurch die Gefahr eines Datenverlustes, beispielsweise durch Stromausfälle, größer.\n\nDas Design von XFS hat im Vergleich zu einigen anderen Dateisystemen auch Nachteile:\nIn aktuellen Implementierungen ist es nicht möglich, ein XFS-Dateisystem zu verkleinern. Gelöschte Dateien sind nicht wiederherstellbar.\nWegen des verzögerten Schreibens von Daten sind Datenverluste bei aktuell geöffneten Dateien bei einem Systemabsturz (z. B. Stromausfall) möglicherweise größer als bei anderen Dateisystemen \"(siehe Abschnitt: Verzögerte Belegung)\".\n\nDas Journal ist auf hohe Leistung optimiert und daher architekturabhängig: Bei einem Wechsel der Prozessor-Architektur (z. B. von IA-32 auf x64) muss vor dem Einhängen des Dateisystems das Journal mit codice_1 geleert werden.\n\n\n"}
{"id": "121750", "url": "https://de.wikipedia.org/wiki?curid=121750", "title": "SHFS", "text": "SHFS\n\nSHFS steht für \"SH\"ell \"F\"ile\"S\"ystem und ist ein Dateisystem für Computernetzwerke. Es gehört zum Betriebssystem Linux und ist ein Softwaremodul für den Linux-Kernel ab Version 2.4. Mit SHFS kann ein Dateisystem mit einer einfachen Remote Shell von einem entfernten Rechner aus zur Verfügung gestellt (gemountet) werden. Das Mounten kann auch durch /etc/fstab-Einträge erfolgen. Die Entwicklung von SHFS wurde 2004 eingestellt; es gab bis 2006 jedoch noch Patches auf der Sourceforge-Projektseite.\n\nEinen anderen Ansatz, um Dateisysteme über das Dateiübertragungsprotokoll SSH zu mounten, verfolgt SSHFS, welches ein Dateisystemtreiber für das Kernelmodul FUSE bildet.\n\nDas SHFS stellt ein Kernelmodul bereit und lässt sich über die initrd laden. Da jedoch weitere Programme/Bibliotheken/Dateien zum Betrieb benötigt werden, ist das Mounten von Systemverzeichnissen zum Systemstart mit allen beschriebenen Dateisystemen wie etwa /usr oder /home kaum möglich. Dies leistet derzeit nur das wegen mangelnder Authentifizierung als unsicher geltende Netzwerkdateisystem NFS.\n\nAlle genannten Dateisysteme arbeiten mit OpenSSH und verschlüsseln die Übertragung der Daten aus dem Anmeldeprozess sowie der eigentlichen Datenübertragung. Das SHFS kann zum Beispiel durch folgenden Befehl:\nzur Verwendung des sehr schnellen Blowfish-Verschlüsselungsalgorithmus gebracht werden. Das SHFS nutzt den SFTP als serverseitige Schnittstelle.\n\n"}
{"id": "121805", "url": "https://de.wikipedia.org/wiki?curid=121805", "title": "Ad-Aware", "text": "Ad-Aware\n\nAd-Aware [] ist der Markenname eines proprietären Antivirenprogramms des kanadischen Unternehmens \"Lavasoft\".\n\nDas Geschäftsmodell ist Freemium. Neben der Freeware-Variante mit der Bezeichnung \"Ad-Aware Antivirus Free\" gibt es zwei kostenpflichtige Varianten mit erweitertem Funktionsumfang. Ad-Aware läuft unter allen Windows-Versionen ab Windows 7. Schon die Freeware-Variante enthält einen Echtzeitscanner. Vor Installation der Software sollte ein vorhandenes Antivirenprogramm vom Computer entfernt werden.\n\nAd-Aware untersucht in allen Varianten neben den aktuell laufenden Prozessen auch die Registry, die Favoriten und Cookies im Webbrowser und alle Dateien in einem angegebenen Verzeichnis.\n\n"}
{"id": "121997", "url": "https://de.wikipedia.org/wiki?curid=121997", "title": "Microsoft PowerPoint", "text": "Microsoft PowerPoint\n\nMicrosoft PowerPoint ist ein Präsentationsprogramm von Microsoft.\n\nPowerPoint gehört zum Microsoft Office 365-Abonnement und ist sowohl für Windows als auch für macOS verfügbar. Die aktuelle Version für beide Betriebssysteme ist \"Microsoft PowerPoint 2016\".\n\nDas Programm wurde seit 1984 bei der Firma Forethought in Sunnyvale entwickelt und trug in dieser Phase den Namen \"Presenter.\"\nDie erste Version erschien im April 1987 für Macintosh-Computer (mit 512 KB RAM). Durch die Übernahme von Forethought für 14 Millionen US-Dollar am 31. Juli 1987 erwarb Microsoft die Rechte an dem Programm. Die dann im Mai 1988 veröffentlichte Version 2.0 unterstützte erstmals Farben und wurde in mehrere Landessprachen lokalisiert. Die deutsche Sprachversion war ab dem 8. Dezember 1988 lieferbar.\n\nMit der Einführung von Windows 3.0 wurde die Version 2.0 von PowerPoint für Windows am 22. Mai 1990 ausgeliefert.\n\nPowerPoint ist das am weitesten verbreitete Präsentationsprogramm. 2001 schrieb Ian Parker, dass PowerPoint auf rund 250 Millionen Computern installiert ist. Die Zahl der täglich stattfindenden PowerPoint-Präsentationen wurde im selben Jahr von Microsoft mit 30 Millionen angegeben. Edward Tufte schätzt, dass jährlich bis zu 100 Milliarden Folien erstellt werden. Bei LaPorte et al. findet sich die Angabe, dass 95 Prozent aller Präsentationen mit PowerPoint erstellt sind.\n\nPowerPoint ist ein seitenorientiertes Programm. Für die einzelnen Seiten, auch Folien genannt, bestehen umfangreiche Gestaltungsmöglichkeiten. Sie reichen von der einfachen Textfolie über Folien mit Grafiken, Tabellen und Diagrammen bis hin zu Folien mit Multimedia-Inhalten wie Film und Sound. Grafiken können dabei sowohl in PowerPoint selbst mit verschiedenen Zeichenwerkzeugen erstellt als auch in Form von ClipArts oder Fotos (in den Formaten GIF, JPG, PNG, TIF, BMP) eingefügt werden. Ebenso ist es möglich, Audio- (zum Beispiel in den Formaten MP3 und WAV) und Videodateien (in den Formaten AVI, MOV, QT, MPG, MPEG und WMV) zu integrieren.\n\nNachdem das Vorführen von Präsentationen mit Hilfe von Videoprojektoren \"(Beamern)\" die Verwendung von klassischen Folien auf Tageslichtprojektoren mehr und mehr verdrängt hat, spielen auch die Animationsmöglichkeiten in PowerPoint eine große Rolle (sie wurden in der Version 2002 für Windows bzw. v.X für Mac OS deutlich erweitert). Texte und Bilder lassen sich mit vielen verschiedenen Animationen in die einzelnen Folien einbinden. Der Wechsel zwischen den einzelnen Folien kann mit Folienübergangseffekten erfolgen.\n\nEine spezielle \"Präsentationsansicht\" ermöglicht es, nur den Inhalt der Folie auf die Leinwand zu projizieren, während auf dem Monitor als Übersicht die aktuelle und die nächste Folie sowie Notizen angezeigt werden. Dies wurde mit der Version 2004 in der Mac-Version eingeführt, nachdem Office v.X noch ein anderes Konzept für die Präsentationsansicht verfolgte. Dort wurde die in der gewöhnlichen PowerPoint-Bedieneroberfläche aktuell am Monitor angezeigte Folie auf den Beamer als Vollbild übertragen, was den Vorteil hatte, dass Anpassungen an der Folie sofort an den Beamer übertragen werden konnten. Mit der 2004er-Version war dies nicht mehr möglich.\n\nPowerPoint wird sowohl einzeln als auch als Teil der verschiedenen Microsoft-Office-Pakete verkauft. Mit jedem neuen Release der Office-Pakete wird auch eine neue Version von PowerPoint herausgegeben, die den gleichen Namen trägt.\n\nPowerPoint lässt sich mittels VBA (Windows/Mac OS außer PowerPoint 2008) und AppleScript (nur Mac OS) automatisieren und erweitern. Das Konzept von VBA steht allerdings mit den XP-Nachfolgern zur Revision an. Mit PowerPoint 2008 wurde die VBA-Unterstützung für den Mac fallengelassen, mit PowerPoint 2011 aber wieder eingeführt.\nZudem ist die Windows-Version über .NET-Schnittstellen von externen Programmen automatisierbar, dazu existieren sogenannte „Interop“-Klassenbibliotheken.\n\nUm die Präsentationen anzeigen zu können, muss PowerPoint vorhanden sein. Unter Windows, nicht unter Macintosh, kann auch der kostenlos erhältliche \"PowerPoint Viewer\" verwendet werden. Dieser ist bis zur Version 2003 von CD lauffähig, der PowerPoint Viewer 2007 muss installiert werden.\n\nOft ist es jedoch auch möglich, PowerPoint-Präsentationen mit LibreOffice, Apache OpenOffice oder Keynote anzusehen und zu bearbeiten. Allerdings werden dabei nicht immer alle neuen Funktionen der jeweiligen PowerPoint-Versionen unterstützt.\n\nNachdem Microsoft bisher das proprietäre, nicht offen dokumentierte Format \".ppt\" verwendete, werden die Präsentationen seit PowerPoint 2007 bzw. 2008 standardmäßig im \"Office Open XML\"-Format (\".pptx\") gespeichert. Die erstellten Präsentationen lassen sich auch als Webseiten (*.html) oder als einzelne Folien in den gängigsten Grafikformaten ausgeben.\n\nNeben dem Standard-Präsentationsformat (\".ppt\" bzw. \".pptx\" und \".pptm\" für Dateien mit Makros, letzteres nur Windows) wird häufig das Bildschirmpräsentationsformat (\".pps\" bzw. \".ppsx\") verwendet, das der sofortigen Anzeige der Präsentation im Vollbildmodus dient. Zwischen den beiden Formaten besteht inhaltlich kein Unterschied; es werden die gleichen Informationen gespeichert. Zur Anzeige der Präsentationen muss entweder die Vollversion des Programms oder der kostenlose \"PowerPoint Viewer\" für Windows auf dem Computer vorhanden sein.\n\nAlternativ lässt sich mit der Windows-Version von PowerPoint ab der Version 2003, mit der Option \"Verpacken für CD\", eine Präsentations-CD erstellen. Dabei werden alle mit der Präsentation verlinkten Dateien zusammengefasst. Eine Option ermöglicht, eine Autostart-Datei sowie den \"PowerPoint Viewer\" mit auf die CD zu brennen. Dieser ist von der CD aus lauffähig und muss auf dem Zielrechner nicht installiert werden.\n\nFür alte Versionen gibt es die \"„Pack & Go“-Funktion\", mit der eine gepackte Datei kopiert und auf dem Zielrechner entpackt wird. Die gepackte Datei hat die Dateiendung \".ppz\" und ist im ZIP-Format komprimiert.\n\nWird PowerPoint mit Hilfe von VBA automatisiert, werden diese Makros entweder innerhalb der einzelnen Präsentationsdatei gespeichert oder können als Add-in (Dateiendung \".ppa\" bzw. \".ppax\") für alle Präsentationen die Funktionen von PowerPoint ergänzen.\n\nZur Vereinheitlichung des Erscheinungsbildes von Präsentationen, zum Beispiel zur Einhaltung eines Corporate Design, dienen Vorlagen mit den darin enthaltenen Mastern. Sie haben die Dateiendung \".pot\" bzw. \".potx\" oder \".potm\".\n\nFür PowerPoint wurde am 17. Dezember 2014 die erste iOS-App mit der Versionsnummer 1.4 lanciert. Es ist möglich, mit der App, welche für das iPhone wie das iPad optimiert ist, vollständige Präsentationen zu erstellen oder zu editieren. Durch den Microsoft-Cloud-Dienst OneDrive können Präsentationen online abgespeichert werden und sind so von allen Geräten erreichbar. Es ist auch möglich nur lokal eine neue Präsentation zu erstellen oder vorhandene zu speichern.\n\nMit der Version 1.6 wurde ebenfalls iCloud gekoppelt. Am 21. April 2015 erschien die erste Applikation für die Apple Watch. Es wurde möglich, die Präsentation vom Handgelenk aus zu steuern. Ist die App über Bluetooth oder WLAN mit einem Apple-TV Gerät verbunden, so können Präsentationen in der Referenten-Ansicht auf dem Abspielgerät angezeigt werden und auf dem verbundenen Bildschirm (TV-Gerät oder Beamer) präsentiert werden. Durch Wischen nach rechts lassen sich die einzelnen Folien wechseln und man kann direkt mit dem Finger auf die Folien zeichnen.\n\nEs kann zu Problemen mit Grafiken kommen, wenn Präsentationen auf Mac erzeugt und auf Windows abgespielt werden sollen. Dies liegt am nicht kompatiblen Format TIFF, welches zwar als Standardformat bei Mac verwendet wird, allerdings in der Windows-Version nicht unterstützt wird. Gegenwärtig scheint es keine Lösung für dieses Problem zu geben, außer dem absoluten Vermeiden von Drag and Drop, um Bilder einzufügen.\n\n\n\n"}
{"id": "122504", "url": "https://de.wikipedia.org/wiki?curid=122504", "title": "IBM Personal Computer", "text": "IBM Personal Computer\n\nIBM Personal Computer (dt. „persönlicher Rechner“, kurz IBM PC) war die Modellbezeichnung des ersten Personal Computers (PC) des US-amerikanischen Unternehmens IBM aus dem Jahr 1981. Dessen Nachfolgemodelle hatten die gleiche Bezeichnung, mit Namenszusätzen wie \"XT \"und \"AT\". Die Gerätelinie war ein großer kommerzieller Erfolg für IBM. Das Unternehmen setzte damit einen informellen, weltweiten Industriestandard und definierte die bis heute aktuelle Geräteklasse der \"IBM-kompatiblen Personal Computer\". Die zahlreichen Nachbauten und Fortführungen der IBM PCs durch andere Unternehmen wurden als IBM-PC-kompatible Computer bezeichnet. Die heute marktüblichen PCs mit Windows-Betriebssystem und x86-Prozessoren beruhen auf der stetigen Weiterentwicklung des damaligen Konzepts.\n\nDer erste IBM Personal Computer hatte noch keine Festplatte, sondern lediglich ein oder zwei Diskettenlaufwerke, er trug die interne Bezeichnung \"IBM model 5150\" und wurde ab 1981 fortan fast sechs Jahre lang unverändert gebaut. Nachfolgesysteme mit Festplatte nannten sich IBM Personal Computer XT und später, mit einem Intel-80286-Prozessor, IBM Personal Computer/AT. Nach der unglücklichen Einführung der hardwareseitig nicht PC-kompatiblen Personal System/2-Modelle durch IBM und mit dem Erscheinen von Microsoft Windows 3.0 wurde der Ausdruck „IBM PC“ bereits ab 1990 ein eher historischer Begriff. Seit der Einführung von Microsoft Windows 95 wurde im PC-Marktsegment praktisch nur noch von Windows-Kompatibilität gesprochen.\n\nDer IBM Personal Computer Model 5150 wurde am 12. August 1981 angekündigt und kam ab Oktober auf den US-amerikanischen Markt. Diese Maschine war schnell entwickelt worden, um den gerade rasant wachsenden Markt für Mikrocomputer nicht der Konkurrenz zu überlassen – vor allem dem Apple II. Bis zu diesem Zeitpunkt hatte IBM nur die ausschließlich für kommerzielle Kunden bestimmten Systeme IBM 5100 und System/32 hergestellt. Diese waren jedoch nicht mit den deutlich preiswerteren und flexibleren Systemen wie dem Apple II vergleichbar.\n\nObwohl die Entwicklung des IBM PC in kürzester Zeit und unter Verwendung der preisgünstigsten verfügbaren Komponenten erfolgte (siehe \"Commercial off-the-shelf\"), wurde er ein voller Erfolg. Einer der größten Vorteile bestand darin, dass er ebenso wie das Vorbild Apple II durch Steckkarten, die in den Computer nachträglich eingebaut werden konnten, erweiterbar war. Die Grundkonfiguration, die in den Vereinigten Staaten für 3.000 US-Dollar (nach heutiger Kaufkraft ca. US-Dollar), in der Bundesrepublik für 8.500 D-Mark (nach heutiger Kaufkraft ca. Euro) angeboten wurde, hatte keine Festplatte, sondern nur ein oder zwei Diskettenlaufwerke. Darüber hinaus entstanden durch den simplen Aufbau des PCs aus für jedermann leicht erhältlichen Standard-Chips schon ab 1983 in Fernost diverse Nachbauten (oft auch fälschlich, weil ungenau, \"IBM-Klon\" genannt), die für eine weite Verbreitung der Systemarchitektur sorgten. Der IBM-PC entwickelte sich schnell zu einem inoffiziellen Industriestandard, weil er ohne Lizenzierung durch IBM nachgebaut werden konnte. Selbst über das Betriebssystem des IBM-PC, DOS 1.0, hatte IBM keine vollständige Kontrolle, weil es ursprünglich von Microsoft entwickelt wurde.\n\nDer Begriff \"IBM-PC\" prägte die Auffassung, was ein PC ist. Ab Mitte der 1980er Jahre waren PCs, die nicht \"IBM-kompatibel\" waren, außer im Heimcomputersektor schlicht unverkäuflich. Für die zugesicherte Eigenschaft „IBM-kompatibel“ der Hersteller von Nachbauten eines IBM-PC gab es nie offizielle Tests oder Zertifizierungen. Als Kriterium diente häufig ein Kompatibilitätstest mit dem Flugsimulator-Programm von Microsoft. Dieser reizte die damals übliche Hardware bis an ihre Grenzen aus und griff so tief auf BIOS-Funktionen zu, dass er nur mit einem zu 100 % kompatiblen BIOS genutzt werden konnte.\n\nIn den späteren Jahren hatte IBM keine glückliche Hand bei der Weiterentwicklung des IBM-PCs. Während man bei IBM mit proprietären Konzepten (z. B. IBM PS/2-Computern und deren Micro Channel Architecture oder OS/2) versuchte, den Markt gegen Mitbewerber abzuschotten, entwickelten Hersteller wie Compaq, HP, Intel und Microsoft zukunftsfähigere Konzepte und herstellerübergreifende Standards (z. B. Extended Industry Standard Architecture), und konnten mit ihrer Marktmacht diese – im Gegensatz zu IBM – auch als Industriestandards durchsetzen. Einzig im geschäftlichen Bereich und in speziellen Branchen wie dem Banksektor konnten sich die IBM PS/2-Modelle – eigentlich als Nachfolger aller PCs bestimmt – einige Jahre relativ erfolgreich halten. Sie scheiterten dann aber – offenbar hatte die Abgrenzung gegenüber Mitbewerbern durch die Umstellung des verwendeten Bus-Systems auf die lizenzkostenpflichtige „IBM Mikrokanal“-Architektur zu gut funktioniert.\n\nAuch heute noch sind PCs auf der Basis der x86-Prozessoren von Intel oder AMD weitgehend kompatibel zu ihrem Urahn, dem IBM-PC, auch wenn mit dem Attribut „IBM-kompatibel“ schon sehr lange nicht mehr geworben wird.\n\nAls Prozessor verwendete man den 16-Bit-Prozessor 8088 von Intel, eine Version des 8086 mit einem externen 8-Bit-Datenbus. Die CPU war mit 4,77 MHz getaktet, das ist mal die Farbträgerfrequenz des NTSC-Farbfernsehsystems, da zunächst wie bei den Homecomputern auch Fernsehgeräte als Monitore vorgesehen waren. Mit der gleichen Taktrate lief auch der 8 Bit breite Systembus, der erst später in seiner mit dem IBM PC/AT eingeführten 16-Bit-Variante unter der Bezeichnung ISA-Bus standardisiert wurde.\n\nDas Gerät wurde nach seiner Vorstellung zunächst wahlweise mit 16 oder 64 kB Arbeitsspeicher ausgeliefert, mit einer weiterentwickelten Hauptplatine später dann auch mit bis zu 256 kB. Auch ein Koprozessor zur schnellen Gleitkommaberechnung war im Systemdesign vorgesehen. Dieser Baustein mit der Bezeichnung 8087 konnte nachträglich in einen leeren Stecksockel eingesetzt werden. Der Hauptspeicher selbst konnte auf der Hauptplatine – je nach Version der selbigen – auf bis zu 64 oder 256 kB erweitert werden, durch Einsteckkarten von Fremdherstellern später gar auf bis zu 640 kB. Der Prozessor selbst war zwar in der Lage, 1024 kB zu adressieren, im Systemdesign waren aber großzügige 384 kB des Adressbereichs für andere Zwecke vorgesehen, beispielsweise für das BIOS und den Grafikspeicher.\n\nDer PC konnte zunächst mit ein oder zwei 5,25″-Diskettenlaufwerken ausgestattet werden. Über ein optionales Erweiterungsgehäuse waren zwei weitere Diskettenlaufwerke anschließbar, wobei dies wegen der damals hohen Preise für Laufwerkskomponenten eher eine theoretische Möglichkeit darstellt. Die Diskettenlaufwerke konnten für einseitig beschreibbare Disketten mit einer Kapazität von 160 kB oder doppelseitigen Disketten von 320 kB, später sogar mit 360 kB genutzt werden.\n\nAnders als beim Nachfolgesystem \"PC XT\" war auch ein Anschluss für einen Datenrekorder vorhanden, wie er für Heimcomputer üblich war.\n\nAb 1983 konnte der 5150 (und die Erweiterungseinheit) auch mit Festplatten der Kapazität von 10 MB ausgestattet werden. Es handelte sich dabei um die bereits im IBM PC XT Model 87 verwendeten 5,25″-Laufwerke mit MFM-Aufzeichnung und voller Bauhöhe, also der doppelten Höhe eines heute üblichen DVD-Laufwerks.\n\nDer PC ließ sich mit der \"Expansion Unit 5161 Model 1\" (Erweiterungseinheit) um ein identisches Gehäuse mit Platz für zusätzliche Laufwerke und Erweiterungskarten vergrößern. Dazu wurde in den PC eine Karte mit Treiberbausteinen eingesteckt, die den Bus des PC mit dem der Erweiterungseinheit über ein Kabel verband.\n\n\n\n"}
{"id": "122719", "url": "https://de.wikipedia.org/wiki?curid=122719", "title": "ST-Computer", "text": "ST-Computer\n\nDie Computerzeitschrift ST-Computer aus dem Heim-Verlag (bis 1996) und dem falkemedia-Verlag ist die langlebigste Computerzeitschrift für den Atari ST, Atari STE, Atari TT und Atari Falcon. Die Erstausgabe erschien im Januar 1986, erst mit der Ausgabe 1/2004 wurde das Magazin vorerst eingestellt. Während dieser Zeit erschien es elfmal im Jahr. Neben den regulären Heften erschienen zwischen 1986 und 1992 auch vier Sonderhefte.\n\nIm Jahr 1993 wurden die Zeitschriften TOS Computer und ST Magazin übernommen, einige Rubriken des ST-Magazins wurden fortgesetzt (Atarium). Nach der Veröffentlichung der Jaguar-Konsole nahm die ST-Computer Jaguar-Spieletests in ihr Angebot auf. Ab 1995 kamen auch der Macintosh-Rechner zum Themenstoff hinzu, da viele ST-Anwender ihre nun langsam aussterbenden Rechner auf dem Macintosh emulierten. Im Jahre 1996 wurde der Macintosh-Teil aber wieder abgetrennt und der Verlag gewechselt. Bei falkemedia wurde die ST-Computer zusammengelegt mit der \"Atari Inside\" aus demselben Verlag. Vom Sommer 2003 bis zur Ausgabe 01/2004 erschien die ST Computer nur noch als 16-seitige Beilage der PC-Welt beim IDG-Verlag.\n\nSeit der Ausgabe 08/2014 erscheint die ST-Computer alle zwei Monate als kostenloses PDF sowie als gedrucktes Magazin in Kleinstauflage.\n\n"}
{"id": "122882", "url": "https://de.wikipedia.org/wiki?curid=122882", "title": "Projekt Blinkenlights", "text": "Projekt Blinkenlights\n\nProjekt Blinkenlights ist eine unabhängige Künstlergruppe, die aus dem Chaos Computer Club hervorgegangen ist und sich durch interaktive Lichtinstallationen an Gebäuden einen Namen gemacht hat. Das Projekt wurde 2003 für den Webby Award in der Kategorie \"Net-Art\" nominiert. Der Name \"Blinkenlights\" ist ein Ausdruck des internationalen Hackerjargons.\n\nProjekt Blinkenlights hat im Laufe der Jahre verschiedene Projekte durchgeführt.\n\nIm Jahre 2001 wurde der Name \"Blinkenlights\" als Titel für eine Lichtinstallation im Haus des Lehrers am Alexanderplatz in Berlin gewählt.\nSie ging zum 20. Jahrestag der Gründung des Chaos Computer Clubs am 11. September 2001 in Betrieb und wurde von Mitgliedern des Clubs erdacht und realisiert.\n\nHinter den Fenstern der oberen acht Etagen des Gebäudes wurden Baustrahler auf selbstgebauten Holzständern installiert. Die Baustrahler wurden über je ein Relais von einem zentralen Computer ein- und ausgeschaltet. Damit fungierten die insgesamt 144 Lampen als riesiger Bildschirm (acht Etagen mit je 18 Fenstern). Um die Fenster wie große Pixel wirken zu lassen, wurden sie von innen mit Wandfarbe angestrichen.\n\nMit der Software \"BlinkenPaint\" konnte jedermann selbst Animationen am eigenen Computer erstellen und sie per E-Mail einsenden. Die Einsendungen wurden der Playlist hinzugefügt und ergänzten sich zu einem abwechslungsreichen Programm, das die ganze Nacht über lief.\n\nMit Hilfe eines Mobiltelefons konnte man außerdem – alleine oder zu zweit – das Computerspiel Pong spielen.\nPersönliche Liebesbotschaften – die so genannten \"Blinkenlights Loveletters\" – konnten nach Einsendung per Telefon abgerufen werden.\n\nDie Betriebssoftware von Projekt Blinkenlights ist unter der Freie-Software-Lizenz GNU GPL veröffentlicht und wird für zahlreiche Nachbauten eingesetzt.\n\nDie Installation wurde am 23. Februar 2002 nach über fünf Monaten Laufzeit im Rahmen einer großen Abschlussparty abgeschaltet.\n\nIm Oktober 2002 wurde während der Nuit blanche in Paris eine neue Variante namens \"Arcade\" installiert. Diese neue Variante, welche im \"Tour de Lois\" der Bibliothèque nationale de France installiert war, konnte bis zu acht Graustufen darstellen. Neben dem von Blinkenlights bekannten Pong war es möglich, die Computerspiele-Klassiker Pac-Man, Breakout und Tetris mit dem Telefon zu spielen.\n\nMit 520 bespielten Fenstern (20 Etagen mit je 26 Fenstern) und einer 3.370 m² großen Gesamtleuchtfläche war Arcade eine der größten interaktiven Lichtinstallationen aller Zeiten.\n\nWährend des 20. Chaos Communication Congress im Dezember 2003 wurde Blinkenlights im Haus des Lehrers unter dem Namen \"Blinkenlights Reloaded\" noch einmal für zwei Wochen betrieben. Hier kam die gleiche Graustufentechnik wie in Paris zum Einsatz.\n\nWährend des Irakkrieges von 2003 wurde die Fassade des Verlagsgebäudes des Neues Deutschland mit der Parole \"\"No war (Kein Krieg)\"\" illuminiert. Die Installation wurde von der Typografin Verena Gerlach und Oliver Krieger, zwei Mitgliedern des Projekt Blinkenlights, umgesetzt.\n\nAm 2. und 3. Oktober 2004 wurde in einem neunstündigen Hack das Haus des Lehrers noch einmal mit einer einfachen Blinkenlights-Installation beleuchtet, nämlich mit einem pulsierenden Herz. Unter anderem bestand der Grund für dieses Unternehmen vermutlich darin, dass am Samstagabend auf dem Alexanderplatz ein Teil der 150. Folge der erfolgreichen Fernsehshow Wetten, dass..? ausgestrahlt wurde.\n\nAm 3. September 2008 kündigte Tim Pritlove eine neue Installation des Projekts Blinkenlights während der Nuit Blanche 2008 an. Beleuchtet wurde die City Hall in Toronto in Kanada.\nDer Unterschied zu allen vorherigen Projekten ist die besondere Form des Gebäudes:\nDas Rathaus besteht aus zwei Hochhäusern, welche nach innen gekrümmt sind. Beide Gebäude zusammen haben 960 Fenster, respektive 960 Pixel. Die Helligkeit der einzelnen Pixel ließ sich in 16 Stufen einstellen. Des Weiteren besteht die Möglichkeit, eigene Animationen mit Quartz Composer bzw. Stereoscope Paint zu erstellen, ein Simulator für Mac OS X und iPhone ist verfügbar.\n\nDer Videoclip zu der Single „Was zählt“ aus dem Album „Auswärtsspiel“ der deutschen Punkrock-Band Die Toten Hosen wurde 2001 während der Blinkenlights-Aktion im Haus des Lehrers gedreht. In dem Videoclip sind sowohl das bekannte Herz von außen als auch die Scheinwerfer von innen zu sehen.\n\nIn der Modelleisenbahnanlage Miniatur Wunderland gibt es eine Mini-Version von Blinkenlights, welche (da es keinen Berlin-Nachbau gibt) im Hamburg-Abschnitt zu sehen ist. Das dort gastgebende Gebäude ähnelt dem Haus des Lehrers.\n\n\n\n"}
{"id": "122886", "url": "https://de.wikipedia.org/wiki?curid=122886", "title": "Copland (Betriebssystem)", "text": "Copland (Betriebssystem)\n\nCopland war ein Projekt ab März 1994 zur Neuentwicklung eines Betriebssystem des Unternehmens Apple. Es sollte die Nachfolge von System 7 antreten. Als das Projekt jedoch nach über 2 Jahren Entwicklungszeit weder annähernd fertig war, noch stabil lief, wurde es 1997 eingestellt. Einige der Entwicklungen aus Copland wurden daraufhin in das bestehende Mac OS ab Version 7.6 integriert.\n\nAls Name für das fertige System wurde es auch als kommendes System 8, später als Mac OS 8, bezeichnet. Diese Bezeichnung findet sich auch in der letzten veröffentlichten Entwicklerversion D11E4. Das am 26. Juli 1997 veröffentlichte Mac OS 8 ist jedoch nicht aus Copland entstanden, sondern eine weitere Version von System 7, die ursprünglich sogar als Mac OS 7.7 hätte erscheinen sollen.\n\nBereits 1988 begann Apple, an einer Nachfolge für die als veraltet geltende Macintosh System Software (damals System 6) zu arbeiten. Bei einem Treffen einiger führender Apple-Manager und im März 1988 wurden auf Karteikarten mit den Farben Blau, Rosa und Rot Ideen für die kommenden Betriebssysteme geschrieben. Auf den blauen Karteikarten wurden die im bestehenden System 6 integrierbaren Weiterentwicklungen aufgeführt, die schließlich als „Project Blue“ („blaues Projekt“) in System 7 mündeten. Auf den pinken Karteikarten wurden Ideen vermerkt, die mit dem bestehen Betriebssystem, der Macintosh System Software, nicht möglich waren und daher eine Neuentwicklung notwendig machten. Dieses vollkommen neu entwickelte Betriebssystem wurde als „Projekt Pink“ („rosa Projekt“) entwickelt – die Arbeit an beiden Projekten begann zeitgleich bereits 1988. Es kam jedoch zu Streitigkeiten unter den Entwicklern beider Teams, woraufhin das Management die Entwicklung beider Betriebssysteme in ein Team zusammen legte. Die Arbeiten an „Project Blue“ – dem nächsten Apple-Macintosh-Betriebssystem System 7 – hatten jedoch Vorrang. Als 1991 System 7 erschien, war Pink als Betriebssystem immer noch nicht in Sicht. Apple war im gleichen Jahr eine Allianz mit IBM und Motorola eingegangen, um einen Ersatz für den von Motorola stammenden 68000-Prozessor (68k) zu entwickeln. Von IBMs POWER-Architektur wurde ein 32-Bit-Prozessor abgeleitet, der unter dem Namen PowerPC sowohl von IBM als auch von Motorola, nach den Bedürfnissen von Apple, entwickelt und gefertigt werden sollte. Apple machte IBM die gemeinsame Weiterentwicklung von „Pink“ als Betriebssystem für den PowerPC schmackhaft, woraufhin diese unter dem neuen Namen Taligent ab 1991 gemeinsam fortgeführt wurde.\n\nSystem 7 war indes ein durch und durch für den Motorola-68030 (32-Bit und mit MMU) entwickeltes Betriebssystem – für den PowerPC-Prozessor musste es über einen Microkernel, der eine transparente Emulation bot, lauffähig gemacht werden. Die Geschwindigkeitsvorteile des PowerPC gingen durch die notwendige Emulation verloren, sodass sich System 7 auf einem Power Macintosh mit dem schnelleren PowerPC-Prozessor genauso schnell bzw. langsam anfühlte wie auf einem Macintosh mit eigentlich langsamerem 68k-Prozessor.\n\nApple war daher ab 1991 sehr an einem neuen Betriebssystem gelegen. Mit dem ebenfalls bereits 1988 angedachten „Project Red“ („rotes Projekt“), das auf rote Karteikarten geschriebene Ideen enthielt, die man damals als „pinker than pink“ – „röter als rosa“ – einstufte, hatte Apple ebenfalls kein Glück. Nach dem Ende des Projekts „Star Trek“, einem zwischen 1992 und 1993 auf den Intel 486 portierten System 7.1, wurden die frei gewordenen Ressourcen zwar vorerst in das Projekt „Raptor“ (was als die Umsetzung des „roten Projekts“ gilt) verlagert, doch auch „Raptor“ musste wegen budgetären Kürzungen und Entwicklermangel eingestellt werden.\n\nProjekt „Pink“ wurde ab 1991 mit IBM in der gemeinsamen Tochterfirma Taligent als „TalOS“ auf Basis eines Mach-3-Microkernels entwickelt. Doch nur der als Laufzeitumgebung konzipierte Teil wurde unter dem Namen „TalAE“ (Taligent Application Environment) fertig und, da Apple 1995 aus der Entwicklung vollständig ausstieg, von IBM als CommonPoint alleine weiterentwickelt.\n\nMit MkLinux arbeitete Apple sogar an einem Linux (Kernel ab 1.3, später 2.0) gemeinsam mit dem Research Institude der Open Software Foundation (OSF), das im Mai 1996 als Developer Release 1 für den Power Macintosh veröffentlicht wurde. 1999 zog sich Apple aus der MkLinux-Entwicklung wieder zurück.\n\n1990 war Microsoft mit Windows 3.0, ein der Macintosh System Software ebenbürtiges grafisches Betriebssystem, der Durchbruch auf dem PC-Markt gelungen. 1991 folgte Windows 3.1. Für 1993 plante Microsoft die nächste, 32-bittige und Multitasking-fähige Windows-Generation, die unter dem Projektnamen „Chicago“ entwickelt wurde, auf den Markt zu bringen. „Chicago“ kam zwar erst 1995 als Windows 95, doch erschien 1993 mit Windows NT 3.1 ein Microsoft-Betriebssystem, das bereits Speicherschutz und präemptives Multitasking bot. Apple hingegen war es mit System 7 (1991) nicht möglich, das auf „System 1“ (1984) zurückgehende System um genau diese modernen Funktionen zu erweitern.\n\nDa alle bisherigen Versuche, ein vollständig neues, modernes Betriebssystem als Nachfolger von System 7 zu entwickeln, scheiterten, wurde ab 1994 am kommenden „System 8“ unter dem Entwicklungsnamen „Copland“ gearbeitet. Dieser neue Name war bewusst gewählt, um den Neuanfang zu verdeutlichen. Namensgeber für das Projekt war der zeitgenössische Komponist Aaron Copland, da auch vorherige Versionen der Apple-Betriebssysteme interne Projektnamen von klassischen Komponisten erhielten – wie Mozart oder Beethoven.\n\nDas Ziel war, endlich ein natives PowerPC-Betriebssystem mit einem modernen Kernel, der Speicherschutz und präemptives („echtes“) Multitasking beherrschen sollte, zu entwickeln. Der Desktop sollte die bekannte, als „leicht zu bedienend“ beworbene Macintosh-Oberfläche fortführen, jedoch erstmals Mehrbenutzerfähigkeit und eine optisch vollständig anpassbare Oberfläche (Designs) bieten. Auch bereits bestehende Technologien wie OpenDoc, QuickDraw GX, ColorSync, QuickDraw 3D, PowerTalk und PowerShare sollte das Betriebssystem ab Werk bieten. Doch auch die Interoperabilität, also der Austausch von Daten mit DOS und Windows, sollte verbessert werden, während Copland selbst der bestmögliche Netzwerk-Client werden sollte.\n\nMit System 7 hatte Apple begonnen, das eigene Betriebssystem an andere Hersteller zu lizenzieren. Auch Copland hätte offen lizenzierbar sein und somit nicht nur auf den eigenen Macintosh-Rechnern laufen sollen.\n\nDer Kernel des Betriebssystems war der NuKernel – ein Microkernel mit Unterstützung für symmetrisches Multiprocessing, präemptives Multitasking, Speicherschutz und verbesserter virtueller Speicherverwaltung, sowie einer Hardwareabstraktionsschicht, die es Apple und weiteren Herstellern erlauben sollte, ein Macintosh-kompatibles System auf unterschiedlicher Hardware zu erstellen.\n\nDer NuKernel wurde von Apple bereits unabhängig von Copland seit 1992 entwickelt, doch gelang es Apple bis zum Schluss nicht, den Kernel zu stabilisieren.\n\nAufgesetzt auf den NuKernel sollten alle kritischen Subsysteme wie I/O, Netzwerk, Dateisysteme u.d.gl. als Dienst laufen. Zusätzlich sollte Copland einen flexiblen Mechanismus für Systemerweiterungen und Low-level-Mechanismen wie X/Open Transport Interface (OTI), System-V-STREAMS und Data Link Provider Interface (DLPI) bieten.\n\nAls Resultat der Bemühungen, Copland kompatibel mit der bestehenden Macintosh-Programmierschnittstelle und damit mit bestehenden Macintosh-Applikationen zu machen, musste der Speicherschutz wieder gestrichen werden.\nZunächst war ein Veröffentlichungsdatum Ende 1995 geplant, das später auf Mitte 1996 und Ende 1997 verschoben wurde. Apple veröffentlichte zumindest zwei Entwicklerversionen: Version D9 im November 1995 und Version D11 im Juni 1996. Diese unfertigen Versionen benötigten jedoch einen Macintosh mit System 7 zur Installation und waren zudem nur mit bestimmten Macintosh-Modellen und bis zuletzt nicht mit bestehenden Macintosh-Applikationen kompatibel. Die Copland-Entwicklung, an der etwa 500 Entwickler arbeiteten und die insgesamt über 250 Mio. Dollar kostete, war jedoch auch 1997 noch nicht fertig und hoffnungslos im Verzug. Apple musste die Macintosh-Benutzer und die Entwickler mit dem gealterten System 7 vertrösten. Gleichzeitig feierte Microsoft Windows 95 große Erfolge, und die ersten ernst zu nehmenden Linux-Distributionen tauchten auf dem Markt auf.\n\nNachdem Apple Copland 1997 einstellte, wurde mit Gershwin ein Projekt als Nachfolge für Copland gestartet. Gershwin hätte den aus Copland gestrichenen Speicherschutz wieder integrieren sollen. Es blieb allerdings allein bei dem Projektnamen, da vermutlich kein Entwickler jemals an dem Projekt gearbeitet hat.\n\nInspiriert von Microsofts Verschmelzung von MS-DOS und Windows 3.11 zu Windows 95, die nicht einmal ein Jahr Entwicklungszeit in Anspruch genommen hatte, beschloss man bei Apple im Jahr 1997, Copland zu stoppen und so viele Funktionen davon wie möglich in ein überarbeitetes System 7.5 zu integrieren. Die ersten Neuerungen aus dem Copland-Projekt flossen schon in Version 7.6 von Mac OS ein. In Mac OS 8.0 wurden zahlreiche weitere integriert.\n\nCopland-Funktionen, die in das weiterentwickelte Mac OS einflossen, sind u. a.:\nAndere Funktionen, die für Copland geplant waren, wurden dank NeXTStep/OPENSTEP erst mit Rhapsody verwirklicht:\n\nCopland gab es nur als geleakte Alpha- bzw. Beta-Version oder als Entwicklervorschau in einem frühen Entwicklungsstadium. Es lief nie stabil und war bis zuletzt nicht mit existierenden Macintosh-Applikationen kompatibel. Nach der Einstellung des Projekts wurde der Name „Mac OS 8“ für das klassische Mac OS 7.7 wiederverwendet und zahlreiche Entwicklungen von Copland in das bestehende Mac OS überführt.\n\nApple hat in den 1990er-Jahren nach einer Nachfolge des veralteten System 7 gesucht. Doch war Apple bereits 1988 bewusst, dass es sein Betriebssystem wird erneuern müssen. Copland war ein vielversprechendes Projekt, doch Apple zog nach weniger als 3 Jahren den Stecker, mit dem Argument, dass die Entwicklung zu lange dauern würde. Ende 1997 wurde NeXT übernommen und ein Nachfolge-Betriebssystem entwickelt, das 2001 als Mac OS X auf den Markt kam. NeXT hatte mit OPENSTEP bereits ein stabiles Betriebssystem, dennoch dauerte es offenbar mehr als 4 Jahre, um dieses Betriebssystem Macintosh-kompatibel zu machen. Unklar bleibt, wie viel der Vorarbeit, die bei Apple in den unzähligen Projekten von Pink, über Star Trek, Raptor, MkLinux und Copland geleistet wurde, für Mac OS X verwendet werden konnte, und ob Copland, hätte es noch weitere Entwicklungszeit erhalten, nicht doch noch stabilisiert werden hätte können.\n\nApple wollte mit dem NuKernel einen modernen Microkernel für ein modernes Betriebssystem programmieren, schaffte es aber nicht, diesen zu stabilisieren. Nach der Übernahme von NeXT und dessen Betriebssystem wurde der Kernel von OPENSTEP in XNU umbenannt, was auch als Mac OS X NuKernel gedeutet wurde. Dies sollte verdeutlichen, dass Apple mit dem Zukauf endlich den erhofften stabilen Microkernel in der Hand hatte.\n\n"}
{"id": "123397", "url": "https://de.wikipedia.org/wiki?curid=123397", "title": "ITunes Store", "text": "ITunes Store\n\nDer iTunes Store ist eine weltweite Internet-Handelsplattform des US-amerikanischen Unternehmens Apple für Musikvideos, Filme, Fernsehserien und E-Books. Sie ist in den Sprachen Englisch, Französisch, Deutsch, Spanisch, Italienisch und Japanisch verfügbar.\n\nSeit 2008 nutzt Apple die Infrastruktur, um unter der Bezeichnung \"App Store\" Programme für iOS-Geräte wie etwa das iPhone anzubieten. Anfang 2011 erweiterte Apple das Angebot durch den Mac App Store für Mac-OS-X-Programme.\n\nBis zum 21. September 2006 wurde der iTunes-Store unter der Bezeichnung \"iTunes Music Store\" betrieben. Inhaber ist die Firma Apple Inc. bzw. die Apple-Tochterfirmen \"iTunes S.à.r.l.\" in Europa und \"iTunes K.K.\" in Japan. Der iTunes Store ist eine weltweit führende Online-Plattform für Musik.\n\nDer iTunes Store startete am 28. April 2003 zunächst in den USA mit mehr als 200.000 Titeln aus allen Bereichen, u. a. Classic, Rock, Pop, Rap, Jazz, New Age, Hörbüchern und vielen mehr. Der iTunes Store verwendet das MP4-Format, der Audiokodierung AAC mit einer Bitrate von zunächst 128 kbit/s und dem DRM-System FairPlay. Die gekauften Titel dürfen auf bis zu fünf Rechnern gleichzeitig unter Apples Jukebox-Software iTunes gespeichert und abgespielt werden, außerdem können sie innerhalb des von Apple angebotenen Lizenzierungsmodells beliebig oft und auf beliebig viele iPods geladen und in einer Playlist, d. h. in der gleichen Zusammenstellung der Musikstücke, bis zu fünfmal auf Audio-CDs gebrannt werden. Zusätzlich wird das Cover des Titels bzw. Albums heruntergeladen. Das DRM-System FairPlay stellt dabei sicher, dass sich die Musik, abgesehen von selbstgebrannten CDs, \"nur\" mit iTunes, Apple TV, Motorolas iTunes-Mobiltelefon, dem iPhone und allen iPods abspielen lässt. So wird der iTunes Music Store von Apple auch als Marketingplattform für den iPod und auch für die eigenen Computer verwendet.\n\nDie Nutzung des iTunes Stores erfordert das Programm iTunes, welches kostenlos von Apple für Mac OS X sowie Windows 2000, XP, Vista, Windows 7 und Windows 8 heruntergeladen werden kann. Die Datenbanksuche sowie das Probehören der Musikstücke (90 Sekunden in voller Qualität) ist allen Nutzern möglich. Der einheitliche Preis lag in Deutschland und Österreich bei 0,99 pro Musikstück und 9,99 Euro für ein Album, in der Schweiz dagegen bei 1,50 bzw. 15 Franken. DRM-freie Titel wurden im Angebot „iTunes Plus“ mit einer Bitrate von 256 kbit/s zum gleichen Preis von 0,99 Euro pro Stück angeboten.\n\nAm 6. Januar 2009 wurde das Angebot an DRM-freien Titeln auf 80 % des Katalogs erweitert. Die verbleibenden 20 % wurden im zweiten Quartal 2009 umgestellt. Zusätzlich wurde mit diesem Datum ein neues Preismodell eingeführt: 69 US-Cents für ältere Titel, 99 US-Cent für aktuelle Stücke und 1,29 Dollar für neue „Hits“. In der Schweiz wurde der Preis bei der Einführung auf 1,00, 1,50 bzw. 2 Franken festgelegt. 2015 lag er bei 0,90, 1,50 bzw. 1,90 Franken. Außerdem wurde der Zugriff auf den iTunes Store über eine mobile Internetverbindung auf dem iPhone möglich.\n\nDer Einkauf ist – je nach Land – über die Bezahlung per Kreditkarte, Geschenkgutschein (iTunes Karte), Lastschrifteinzug (in Deutschland bis April 2016 über ClickandBuy), mittels Prepaid-Karte oder über einen Drittanbieter, der die Transaktion abwickelt (z. B. PostFinance mobile), möglich.\n\nÜber den Verkauf von Musik hinaus dient der iTunes Store Apple mittlerweile als Vertriebsweg für zahlreiche digitale Inhalte wie Filme, Fernsehserien, Musikvideos und E-Books. Die amerikanische Konkurrenzfirma Google versucht durch den Play Store eine ähnliche Plattform, hauptsächlich für die Nutzer ihres mobilen Android-Betriebssystems, zu schaffen. Das Angebot ist hier jedoch derzeit noch deutlich geringer, und einige Angebote sind von Deutschland aus nicht verfügbar.\n\nDie Vertriebsplattform wurde 2003 zunächst für den Verkauf von Musik in den USA aufgesetzt. Seit Juni 2004 umfasst sie auch Käufer in Großbritannien, Frankreich und Deutschland. Im Oktober 2004 wurden neun weitere Staaten eingebunden, Österreich, Belgien, Finnland, Griechenland, Italien, Luxemburg, die Niederlande, Portugal und Spanien. Kanada wurde im Dezember 2004 eingebunden. Im Mai 2005 erweiterte Apple die Plattform auf die Schweiz, Norwegen, Dänemark und Schweden. Der australische Markt war für eine Eröffnung zusammen mit den vier europäischen gedacht, wurde aber aus Lizenzgründen verschoben. Am 4. August 2005 wurde das Angebot auf Japan ausgeweitet. Stand Januar 2010 erfolgt der Vertrieb über die Plattform in 77 Staaten.\n\nAnfangs wurden auf der Handelsplattform in Deutschland nur Interpreten vertrieben, die ihre Verträge bei großen Musiklabels hatten. Später wurden auch Interpreten aus dem Independent-Bereich ergänzt. Apple bot in Europa insgesamt über 5 Millionen Titel an. Das Angebot unterscheidet sich dabei allerdings in den einzelnen Ländern. Die Zahl der Titel in den USA belief sich im Juni 2008 auf rund 8 Millionen Titel. Die Angebote werden von Apple laufend ergänzt.\n\nAktuell verkauft Apple weltweit mit steigender Tendenz etwa 3 Millionen Titel pro Tag.\n\nAnfang Oktober 2008 forderte der \"Verband der Musikindustrie\" die Erhöhung der Lizenzgebühren um 66 Prozent von 9 auf 15 Cent. Apple wollte der Forderung nicht nachgeben: „Apple hat wiederholt deutlich gemacht, dass es in diesem Geschäft Geld machen will. Apple ist nicht gewillt den iTunes Store weiterhin zu betreiben, wenn das nicht mehr profitabel ist.“ Eine Abstimmung fand am 3. Oktober 2008 beim \"Copyright Royalty Board\" statt. Nach Angaben der Branchenvereinigung NMPA bleibt die \"Royalty rate\" bei 9,1 Cent pro Lied.\n\n"}
{"id": "123552", "url": "https://de.wikipedia.org/wiki?curid=123552", "title": "QuarkXPress", "text": "QuarkXPress\n\nQuarkXPress ist ein rahmenorientiertes Layoutprogramm des US-amerikanischen Herstellers Quark Inc. Die erste Version wurde 1987 für den Apple Macintosh veröffentlicht.\n\nDer ursprüngliche Entwickler von QuarkXPress ist Tim Gill, der Quark Inc. 1981 in Denver gründete und zunächst Textverarbeitungs-Software für Apple II und III schrieb.\n\nNeben dem 1985 erschienenen Aldus PageMaker gilt QuarkXPress als ein Vorreiter des Desktop-Publishing (DTP) und zählt heute mit Adobe InDesign zu den marktführenden DTP-Layoutprogrammen. Es verfügt über zahlreiche im Bereich der Printmedien benötigte Funktionen aus dem Bereich der Druckvorstufe, wie z. B. typografische Kontrolle, die Möglichkeit der Farbseparation, Überfüllungsalgorithmen, PostScript- und PDF-Ausgabe. Weiterhin bietet es für die fortschreitenden Crossmedia-Publishing-Anforderungen auch Funktionen für HTML5-, eBook- und App-Export.\n\nQuarkXPress wird in Werbeagenturen und von freien Grafikern, bei Prepress-Dienstleistern sowie in Druckereien und Verlagen eingesetzt. Zusammen mit Redaktionssystemen wie Quark Publishing System (QPS) ist es häufig in der professionellen Zeitungs- und Zeitschriftenproduktion anzutreffen. Weitere Einsatzbereiche sind Prospekte, Broschüren, Kataloge, Faltblätter, Plakate, Geschäftsdrucksachen und auch Database-Publishing-Anwendungen. Dank Unterstützung von Inhaltsverzeichnissen und Stichwortverzeichnissen eignet es sich auch für gestaltungsintensive Buchproduktionen.\n\nSeit QuarkXPress 5 können mit dem Programm auch direkt Webseiten (HTML) erstellt werden, ohne auf einen externen HTML-Editor zuzugreifen. Seit Oktober 2006 kann man mit dem Plug-in Quark Interactive Designer auch Flash-Inhalte direkt aus QuarkXPress erstellen. Dieses Plug-In wird seit QuarkXPress 8 mitgeliefert.\n\nAb Version 9 können Inhalte zudem in das ePUB-Format und in das von Ray Kurzweil entwickelte Blio-Format exportiert werden. Seit QuarkXPress 9.5 können auch HTML5-basierte Apps für Android, Apple iOS und Kindle Fire sowie Web-Apps exportiert werden.\n\nIm deutschsprachigen Raum wurde früher die Variante \"QuarkXPress Passport\" verkauft, die im Unterschied zu der in England und den USA erhältlichen Variante multilinguale Sprachunterstützung bot. Die Passport-Varianten wurden mit Version 8 abgeschafft. Der einzige Unterschied zwischen den heutigen „Editionen“ sind die Benutzeroberflächen-Sprachen und die Möglichkeit, japanische Typografie zu erstellen.\n\nJede Edition von QuarkXPress 8 und 9 unterstützt 38 Sprachen und Sprachvarianten – jeweils für Worttrennung und Rechtschreibprüfung. Die Benutzeroberfläche lässt sich dynamisch in andere Sprachen umstellen. Es werden folgende Editionen vertrieben: Amerikas (Nord und Süd), Amerikas Plus, Ostasien, Osteuropa, United Kingdom, United Kingdom Plus, Westeuropa und Westeuropa Plus. Die Plus-Varianten fügen jeweils die Möglichkeit hinzu, japanische Typographie zu erstellen.\n\nDurch die Unterstützung von Plug-ins (die bei Quark \"XTensions\" heißen) seit 1989 kann die Funktionalität des Programms erweitert werden.\n\nAb Version 3.1 ist QuarkXPress auch für Windows-Systeme verfügbar.\n\nAb Version 4.1 unterstützt QuarkXPress den Import und Export von Inhalten via XML und PDF-Import sowie PDF-Export (nur via Distiller).\n\nAb Version 5 stellt die Software neben Ebenen auch Features zum Erstellen von Webseiten (HTML) bereit. Aufgrund vielfältiger Proteste zu den zwingend notwendigen Hardwareschlüsseln (Dongles) ab Version 4 wurden diese mit der Version 5 wieder abgeschafft.\n\nQuarkXPress 6 ist die erste unter macOS nativ lauffähige Version, Version 7.01 war die erste Universal Binary Variante.\n\nVersion 6.5 fügte eine Unterstützung für das XML-Dokumentenbeschreibungsformat Document Object Model (DOM) hinzu, die allerdings nur gelesen werden kann. Ab Version 6.5 können auch native Photoshop-Dateien (PSD) und Excel-Dateien (XLS) eingelesen werden. Außerdem erlaubt sie, PDFs direkt ohne Zuhilfenahme von Distiller zu erstellen.\n\nVersion 7 fügt OpenType- und Unicode-Unterstützung sowie native Schatten und Transparenzen hinzu; letztere können auch granular auf Teilobjekte angewandt werden. Darüber hinaus bietet Version 7 Unterstützung für JDF und PDF/X sowie Flightcheck-Funktionen und solche zum Zusammenarbeiten (so können mehrere Layouter gleichzeitig an der gleichen Seite mit Hilfe sogenannter Composition Zones arbeiten).\n\nQuark liefert ab Version 7 Mehrbetriebssystem-Versionen aus und erlaubt die duale Verwendung einer Lizenz. Somit kann man mit einer Lizenz QuarkXPress z. B. sowohl auf dem heimischen Windows-PC wie auch dem Mac im Büro installieren und nutzen.\n\nVersion 8 verbesserte die Oberfläche und erlaubt, Drag & Drop von anderen Anwendungen (wie Finder oder Adobe Bridge) und wieder zurück. Dies soll laut einer von Quark beauftragten Studie die Arbeitsgeschwindigkeit signifikant erhöhen. Außerdem stellt QuarkXPress 8 ostasiatische Typografie, mehrere Grundlinienraster (auch vertikal), einstellbaren optischen Randausgleich, Objektstile, Illustrator (AI) Import und eingebauten Flashexport zur Verfügung.\nQuark hat mit dieser Version den mit 2015 eingeschlagenen Weg weiter beschritten, sich damit viel Lob der Anwender erarbeitet und den Vorsprung von Indesign weiter verkürzt.\n\nQuarkXPress 9 bietet Automatisierungsfunktionen und neue Designfunktionen, wie verschachtelte Stile, Legenden (Marginalelemente), Aufzählungen und Nummerierungen, einen Wizard für komplexe Bezierformen, Kontaktbögen (Multi-Bild-Import), Seiten klonen etc.\n\nQuarkXPress 10 ist die erste Cocoa-native Version von QuarkXPress 10 und bietet eine neue \"Grafikengine\", die nun EPS, PDF, AI und andere Vektorformate nativ und damit hochaufgelöst anzeigt. Für Nutzer einer 32-Bit Variante von Windows ist Version 10.5.2 die letzte Version, die installiert werden kann, ab Version 2015 braucht man ein 64-Bit Betriebssystem.\n\nQuarkXPress 2015 (Version 11) kann Fußnoten und Endnoten aus MS Word importieren oder selbst anlegen, Tabellenstile halten Einzug in Quark und auch Variablen (für lebende Kolumnentitel oder Querverweise). Außerdem kann man Tastenkürzel selbst definieren und als PDF/X-4 exportieren.\n\nVon November 2017 bis Oktober 2018 bietet die Zeitschrift c't (Heise Verlag) eine uneingeschränkte Version von QuarkXPress 2015 für 9,90 € an und berichtet, dass der Heise Verlag QuarkXPress zur Erstellung seiner Zeitschriften nutzt.\n\nQuarkXPress 2016 (Version 12): Umwandeln von PDF-, Illustrator- und EPS-Dateien in native QuarkXPress Objekte, Illustrator und Microsoft Office Objekte als native QuarkXPress Objekte einfügen, Umwandlung von Print Layout in ein HTML5 Layout, HTML5 Export mit direkter Vorschau, mehrstufige Verläufe mit beliebig vielen Farben, Unterstützung für OpenType Stylistic Sets, neue Windows Benutzeroberfläche, Farbwähler Werkzeug (Farbpipette), Unterstützung des Touchpads für Gestensteuerung (nur Mac), dynamische Hilfslinien für Textrahmen-Spalten, die Inhaltsvariablen brechen automatisch am Zeilenende um, Unterstützung von ICCv4-Profilen.\n\nQuarkXPress 2017 (Version 13): Nicht-destruktive \"Bildbearbeitung\" (Filter und Effekte sowie Farbkorrekturen), Text- und Absatzschattierungen, Text- und Absatzumrandungen, Textkontur (von live Text), Transparenzmodi (wie weiches Licht, Multiplizieren etc.), Zeilenspanner und -trenner, Export von responsive HTML5 Publikationen, Export von iOS Single Apps (ohne weitere Kosten).\n\nMit Version 2017 erlaubt Quark ein Upgrade auf QuarkXPress 2017 von Konkurrenzprodukten wie Adobe InDesign, CorelDraw oder Microsoft Publisher.\n\n\n"}
{"id": "123672", "url": "https://de.wikipedia.org/wiki?curid=123672", "title": "A1060 Sidecar", "text": "A1060 Sidecar\n\nDas A1060 Sidecar (von engl. \"sidecar\" = Seitenwagen oder Beiwagen) bzw. das \"XT Bridgeboard\" war ein eigenständiger PC (auf Basis des Intel 8088), der an der rechten Seite des Amiga 1000 angeschlossen wurde. Der IO-Bereich des 8088 wurde mittels shared Memory in den Adressraum des Amiga-Hostsystems eingeblendet. Zwischen Amiga und Sidecar war ein Datenaustausch möglich.\n\nDurch das Sidecar war es möglich, an nur einem Monitor, einer Tastatur, einer Maus und einem Drucker an zwei verschiedenen Rechnern gleichzeitig zu arbeiten. So wurden MS-DOS-Anwendungen in einem Fenster des Amiga-Betriebssystems ausgeführt und konnten die Peripherie des Amiga mitbenutzen. Mit dem Sidecar bestand auch erstmals die Möglichkeit, den Amiga 1000 durch Einbau einer Filecard mit einer Festplatte zu versehen, die für beide Teile des Gespanns nutzbar war.\n\nDas Sidecar war in der Braunschweiger Entwicklungsabteilung von Commodore entstanden, weil man dort dank der Commodore-PC-Reihe Erfahrung mit der PC-Architektur hatte.\n\nBedingt durch die hohen Systemkosten sowohl des Amiga 1000, der bereits nach kurzer Zeit durch kostengünstigere (Amiga 500) und besser erweiterbare (A2000) Varianten abgelöst wurde, als auch des Sidecars selbst, welches genau genommen ein Commodore PC 10 ohne Grafikkarte und Tastatur war (es verfügte sogar über ein eigenes Netzteil), war dem Sidecar ein relativ kurzes Leben beschieden.\n\nDie Nachfolger des Sidecar stellten die verschiedenen \"Bridgeboards\" dar, die ebenfalls einen kompletten PC darstellten, dies aber auf einer Steckkarte in einem Zorro-Steckplatz eines A2000, A3000 oder A4000. Es gab sie von Commodore mit 8088- (A2088), 80286- (A2286) und 80386SX- (A2386SX) Prozessor. Das XT-Bridgeboard wird im (deutschen) Handbuch und auf der Platine selbst als „PC-Emulator“ bezeichnet, in der Fachliteratur wurde auch von MS-DOS-Emulator gesprochen.\n\nSpäter gab es auch für andere Amiga-Modelle, wie den Amiga 500 oder Amiga 500 plus, von Fremdanbietern eigene PC-Emulator-Karten (z. B. das \"KCS Power PC Board\"). In der Folgezeit wurden Software-Emulatoren beliebter, wie z. B. \"PCTask\" oder \"PCx\", da die gesteigerte Leistung der 68060- bzw. powerUP-basierten Amigas hierfür ausreichte.\n\nSidecar\n\n\nA2088 (XT) Bridgeboard\n\n\nIm Lieferumfang vorhanden waren folgende Disketten:\n\nSoftware für den Amiga\n\nSoftware für den PC\nDiese Disketten enthalten im Gegensatz zu anderen MS-DOS-Versionen spezielle Software, die den Datenaustausch, die Verwendung der Amiga-Maus und die Abfrage der Amiga-Systemzeit ermöglichen.\n\nA2286 (AT) Bridgeboard\n\nA2386X (AT) Bridgeboard\n\nNeben MS-DOS war es mit dem A2386SX möglich, Windows 3.11 zu betreiben. Hierzu musste das Bridgeboard ausreichend viel RAM enthalten und eine eigene ISA-Grafikkarte verwendet werden, die die VGA-Darstellung beherrschte, die ein Amiga 2000 nicht darstellen konnte.\n\n"}
{"id": "124421", "url": "https://de.wikipedia.org/wiki?curid=124421", "title": "Cinema 4D", "text": "Cinema 4D\n\nCinema 4D ist eine 3D-Grafiksoftware des Unternehmens Maxon zum Erstellen von 3D-Modellen, Texturen, Computergrafiken und Animationen. Cinema 4D wird nicht nur für Fernsehwerbung und im privaten Bereich eingesetzt, sondern auch für Filme (vergleichbar mit Autodesk Maya). Entwickelt wird die Software von der \"Maxon Computer GmbH\" mit Sitz in Friedrichsdorf.\n\nDie Ursprünge von Cinema 4D reichen ins Jahr 1989 und auf das Programm \"FastRay\" zurück, eine Software für den Commodore Amiga. Dabei gab es zunächst noch keine grafische Benutzeroberfläche. Die zu berechnenden Szenen wurden aus Textdateien eingelesen.\n\nZwei Jahre später wurde FastRay in der Version 1.0 veröffentlicht. Damit gab es eine grafische Oberfläche, genannt FRED für \"FastRay Editor\", aber noch keine 3D-Editoransicht. Im Jahr 1993 ging aus diesem Projekt Cinema 4D 1.0 für den Amiga hervor.\n\nIm Jahr 1995 wurde Cinema 4D vom Amiga auf Windows und Mac OS Classic portiert, da Commodore 1994 Konkurs anmeldete und damit die Zukunft des Amiga ungewiss war. Der ursprüngliche Programmcode war in Modula-2 geschrieben. Für die Portierung entschied man sich mangels anderer Compiler für C++.\n\nIm Jahr 1997 erschien Cinema 4D 4.0 und damit auch die letzte Amiga-Version. Danach wurde nur noch für Windows und Macintosh entwickelt. Mit dieser Version wurde der Objekt-Manager, als Kernstück der Benutzerschnittstelle zur Manipulation der Objekte einer Szene, eingeführt.\n\nCinema 4D wird auch außerhalb Deutschlands vermarktet (wie beispielsweise in den Vereinigten Staaten, Kanada, Australien und Japan).\n\nCinema 4D wird auch in der Architekturvisualisierung eingesetzt.\n\nDie aktuelle Version ist Version \"20\".\n\nEinige wichtige Funktionen:\n\n\nCinema 4D ist für Windows und macOS verfügbar. Seit Cinema 4D R15 werden nur noch aktuelle 64 Bit Betriebssysteme unterstützt. Eine Linux-Version ist nicht frei erhältlich, sie steht lediglich wenigen Studios, die mit Maxon eng zusammenarbeiten, zur Verfügung. Cinema 4D läuft jedoch problemlos unter Wine.\n\nGenerell ist der Quellcode von Cinema 4D zu über 90 % plattformunabhängig. Dies hat dazu geführt, dass Anpassungen an neue Technologien (64 Bit, Intel-basierter Mac, 64 Bit COCOA Support) sehr schnell durchgeführt werden konnten. Gleichzeitig ist diese Tatsache für das sehr einheitliche Erscheinungsbild von Cinema 4D auf den unterschiedlichen Plattformen verantwortlich. Um die plattformunabhängige Programmierung zu vereinfachen, arbeitet der integrierte Modeller in einer OpenGL-Umgebung.\n\nEin kostenloses Software Development Kit (SDK) ermöglicht es Anwendern, in C++ Erweiterungen für Cinema 4D zu entwickeln. Außerdem wird die proprietäre Skriptsprache C.O.F.F.E.E. unterstützt, die ähnlich C++ und Java ist. Ab der Version R12 ist zudem die Scriptsprache Python enthalten, die das Entwickeln von Plug-Ins und Skripten in einer standardisierten Sprache ermöglicht und somit auch die Portierung von Skripten aus anderen Python-kompatiblen Anwendungen vereinfacht.\n\nSeit R12 gibt es statt der Module nur noch die Pakete \"Prime\", \"Studio\", \"Broadcast\", \"Visualize\" und \"BodyPaint 3D\".\n\nBis Cinema 4D R11.5 waren folgende Module erhältlich:\n\n\n\n"}
{"id": "125134", "url": "https://de.wikipedia.org/wiki?curid=125134", "title": "Adobe Director", "text": "Adobe Director\n\nAdobe Director (zuvor: Macromedia Director) ist ein Autorensystem zum Erstellen komplexer, interaktiver, multimedialer Internet-, DVD-, CD-ROM- und Kiosk-Anwendungen. Es verfügt über die individuelle objektorientierte Programmiersprache Lingo (und JavaScript) und integriert eine Vielzahl verfügbarer Medien.\n\nAm 27. Januar 2017 verkündete Adobe, dass Director ab dem 1. Februar 2017 nicht weiter verkauft wird.\n\nMit Director kann man Programme für Windows und macOS erzeugen (Cross-Plattform-Entwicklung). Diese Director-Anwendungen können auch als ».DCR« über das Internet gestreamt oder zur Offline-Nutzung heruntergeladen werden und plattformunabhängig in Webbrowsern genutzt werden, die das kostenlose Shockwave-Plug-in installiert haben. Dieses Browser-Plugin hat laut Adobe eine Verbreitung von 406 Millionen Installationen (Stand 12/07).\n\nDirector kann komfortabel eine Fülle audiovisueller Medientypen integrieren, darunter Flashfilme. Im Gegensatz zu Authoring-Programmen wie Adobe Flash bietet Director die Möglichkeit, echte 3D-Objekte zu importieren und in Echtzeit zu steuern. Die im Internet immer hochwertiger angebotenen 3D-Spiele und -Anwendungen werden in der Regel mit Director realisiert, und zwar als Shockwave 3D. Mit Director können auch interaktive »Enhanced DVDs« erstellt werden. Dabei greift Director vom DVD-ROM-Teil einer hybriden DVD auf den Video-DVD-Teil zu und kann damit MPEG-2-Videos in der für den Computer bestimmten interaktiven Anwendung darbieten. Ebenso können über den Export in das Shockwave-Format Video-DVD-Inhalte im Browser mit Online-Informationen verknüpft werden.\n\nViele Audio-CDs besitzen auch einen Datenteil, der – in einen PC eingelegt – die Audio-CD in ein Multimedia-Produkt verwandelt. Solche erweiterten Funktionen werden ebenfalls sehr oft mit Director entwickelt.\n\nDer immense Funktionsumfang kann durch Zusatzmodule erweitert werden. Diese Zusatzmodule werden Xtras genannt. Es gibt sowohl kostenlose als auch kostenpflichtige Xtras. Sie können den Funktionsumfang der Programmiersprache Lingo erweitern, weitere Medientypen (beispielsweise PDF, QuickTime, Datenbanken, Photoshop-Bilddateien) verfügbar machen oder z. B. Zugriff und Steuerung von speziellen Geräten ermöglichen. Darüber hinaus gibt es Xtras, die den Funktionsumfang der Entwicklungsumgebung erweitern. Xtras können unter anderem in der Programmiersprache C++ entwickelt werden; schon in der Grundversion umfasst Director zahlreiche Xtras. Durch diesen modularen Aufbau ist die Dateigröße der Directorprogramme nur so groß wie für den Anwendungsfall nötig.\n\nDirector ist nach wie vor das Standardtool zur Erstellung multimedialer Kiosk-, CD-ROM- und DVD-Anwendungen. Es wurde früher für die Postproduktion beim Film eingesetzt und ist heute in Kombination mit dem Programm Flash ein mächtiges Werkzeug, um anspruchsvolle on- und offline Multimediaprogramme zu erstellen. Ein großer, bislang nicht erreichbarer Vorteil von Director im Vergleich zu Flash ist wie erwähnt die Möglichkeit, Echtzeit-3D-Elemente zu integrieren.\n\nDas Unternehmen MacroMind wurde 1984 von Marc Canter, Jay Fenton und Mark Pierce gegründet. Diese entwickelte für den Apple Macintosh die Animationssoftware VideoWorks, welche zwei Versionen später in Director umbenannt wurde. Durch den Zusammenschluss mit der Firma Authorware entstand Macromedia.\n\nUnter dem Dach von Macromedia wurde Director bis zum Dezember 2005 kontinuierlich weiterentwickelt. Im Dezember 2005 wurde Macromedia von Adobe Systems komplett übernommen. Director erschien auch dort im Produktportfolio, bis der Verkauf am 1. Februar 2017 eingestellt wurde.\n\nDirector ermöglicht unter anderem die Einbindung, Wiedergabe und Steuerung von DVD-Video-Content. Bereits seit den 1990ern können Schriftarten (Fonts) eingebunden werden; das ermöglicht die Textdarstellung mit einem auf dem Zielrechner nicht installierten Zeichensatz.\n\n\n\n"}
{"id": "125149", "url": "https://de.wikipedia.org/wiki?curid=125149", "title": "Shareaza", "text": "Shareaza\n\nShareaza ist ein Filesharing-Client für Windows-Systeme, der G2, gnutella, eDonkey2000, BitTorrent und DC++ unterstützt. Des Weiteren kann er als Download-Manager in Webbrowsern integriert werden. Shareaza ist in rund 30 Sprachen erhältlich, darunter auch Deutsch. Die aktuelle Version 2.7.8.0 wurde am 14. Dezember 2014 veröffentlicht. Sie wird sowohl in der SSE2-optimierten als auch in der „normal“ kompilierten Version angeboten.\n\nSeit dem 1. Juni 2004, dem Erscheinungstermin der Version 2.0 von Shareaza, ist der Quelltext unter den Bedingungen der freien GNU General Public License für jedermann zugänglich.\n\nDer Urheber Shareazas ist Michael Stokes, welcher unter anderem auch das von ihm entwickelte Gnutella2-Netzwerk in die Software integrierte. Mitte 2002 veröffentlichte er die erste Version eines gnutella-Clients, den er „Shareaza“ taufte. Nach Stokes Angaben war es von Anfang an das erklärte Ziel der Entwicklung seines Clients, Features zu unterstützen und zu entwickeln, die andere Clients nicht besaßen. So geht beispielsweise die Entwicklung des „Swarmings“, d. h. des gleichzeitigen Downloads von Dateiteilen aus mehreren Quellen, ein Feature, das heute von allen großen Filesharing-Programmen unterstützt wird, maßgeblich auf das Werk Michael Stokes zurück.\n\nÜber die nächsten zwei Jahre fügte Stokes dem Programm Unterstützung für eDonkey2000, BitTorrent, sowie ein komplett überarbeitetes, gnutella-basiertes Protokoll, das er Gnutella2 nannte, hinzu. Dies führte zu einer Spaltung der Filesharing-Community in eine gnutella- und eine Gnutella2/Shareaza-Seite, da die Anhänger des originalen gnutella-Protokolls Stokes vorwarfen, der Name seines neuen Protokolls impliziere ein Update des „Originals“ oder eine Überlegenheit gegenüber ihm. Der Konflikt verschärfte sich, als Stokes begann, das „originale“ gnutella in Shareaza als „Gnutella1“ zu bezeichnen.\n\nAm 1. Juni 2004, veröffentlichte Michael Stokes den Quellcode des Programms unter der GNU General Public License auf SourceForge.net und begründete damit, zusammen mit Programmen wie LimeWire und Gnucleus, eine Bewegung hin zur Quellcodeöffnung bei Filesharing-Programmen, der sich im Laufe der Zeit viele Programme anschlossen. Heute sind alle wichtigen und namhaften Vertreter dieser Netzwerkprogramme quelloffen.\n\nMit dem Scheiden des Shareaza-Hauptentwicklers aus der Filesharing-Community legte sich allmählich dann auch der Protokollnamen-Konflikt und die Spaltung der Entwickler- und User-Communitys, auch wenn Vorbehalte eines Teils der übrigen gnutella-Entwickler gegenüber Shareaza bis heute erhalten geblieben sind. Aus diesem Grunde wird heute Gnutella2 auch häufig nur einfach nach seiner Abkürzung als G2 bezeichnet. Diese Veränderung spiegelt sich auch in Shareaza wider.\n\nSeit Dezember 2007 gehört die Domain \"shareaza.com\" nicht mehr den bisherigen Entwicklern. Die neuen Besitzer bieten dort nun einen anderen Client an, der fälschlicherweise als Shareaza bezeichnet wird, jedoch auf einer modifizierten Version von BearShare/iMesh beruht. Dieser wird teilweise als Malware/Spyware eingestuft, weswegen Sicherheitsdienste, wie zum Beispiel der McAfee SiteAdvisor, vor der Webseite warnen. Versionen vor v2.3.1.0 fragen teilweise noch bei shareaza.com Aktualisierungen ab und bitten den Nutzer, den gefälschten Client zu installieren. Dieses Verhalten tritt in Versionen ab v2.3.1.0 nicht mehr auf. Die Projekthomepage von Shareaza wurde daraufhin auf SourceForge.net übersiedelt.\n\n\nShareaza ist eine Anwendung für Windows. Es funktioniert auch unter Linux mit Hilfe von Wine.\n\nEs wird allerdings in mehreren voneinander unabhängigen Projekten an einer Portierung der Funktionalität Shareazas auf UNIX-basierte Systeme gearbeitet. Die wichtigsten davon sind Sharelin, das sich hauptsächlich auf die Funktionen des Gnutella2-Netzwerks konzentriert, und Quazaa, das mit Hilfe der Qt-Bibliothek annähernd alle Funktionen Shareazas implementieren will.\n\nDas Gnutella2-Netzwerk bietet unter anderem Unterstützung für eine Vielzahl von Metadaten, wie zum Beispiel Dateibewertungen oder Dateieigenschaften. Auch effizientes Suchen nach Metadaten ist möglich. Während des Suchens ermöglicht eine Echtzeit-Statistik, den Verlauf der Suche (durchsuchte Hubs usw.) zu überblicken. Des Weiteren ist es im Gnutella2-Netzwerk – im Gegensatz zu gnutella – möglich, das gesamte Netzwerk zu durchsuchen und daher (theoretisch) jede einzelne Datei, auf die die Suchanfrage zutrifft, im Netzwerk zu finden. Auch größere Dateien können sinnvoll getauscht werden (swarming, partial file sharing, effizientes Lokalisieren von Quellen). Jeder Benutzer kann ein Benutzerprofil mit Nicknamen und weiteren Informationen, wie zum Beispiel dem geografischen Standpunkt, erstellen. Auch Chat mit anderen Netzwerkbenutzern ist möglich.\n\nZusätzlich werden die Netzwerke gnutella, eDonkey2000 und BitTorrent unterstützt (nicht jedoch das auf Kademlia basierende KAD). Dateiteile können sogar aus allen Netzwerken gleichzeitig bezogen werden, um so eine Datei schneller herunterzuladen. Auch das gleichzeitige Hochladen in alle Netzwerke ist möglich. Hierbei werden Torrent-Uploads allen anderen Netzwerken vorgezogen, was nicht geändert, jedoch in der neusten Variante der Software begrenzt werden kann.\n\nAuch HTTP- und FTP-Downloads beherrscht Shareaza. Der Anwender kann auf diese Weise in Shareaza alle genannten Netzwerke und Protokolle verwenden und Dateien aus verschiedenen Netzwerken gleichzeitig herunter- beziehungsweise hochladen. Shareaza kann sogar als Downloadmanager verwendet werden, sodass es mit Hilfe eines Internet Explorer-Plugins (BHO) jegliche Internetdownloads übernimmt. Unterstützung für andere Browser, wie zum Beispiel Firefox, ist über Plugins von Drittanbietern möglich.\n\nZur sicheren Identifizierung der Dateien unterstützt Shareaza sowohl eD2K-, als auch Magnet-Links, welche – ähnlich wie Torrents – von Linkseiten oder Communities bereitgestellt werden können. Da diese Magnet-Links mehrere verschiedene Hash-Checksummen enthalten, welche eine Datei im Netzwerk eindeutig identifizieren, kann somit die Qualität der heruntergeladenen Dateien sichergestellt werden. Zusätzlich können diese Links, genau so wie BitTorrent-Dateien, aus dem Dateimanager-Kontextmenü Shareazas heraus erstellt werden.\n\nNeben den gewöhnlichen Filesharing-Funktionen bietet Shareaza eine Download-Dateivorschau, einen integrierten Media-Player und eine funktionsreiche Dateiverwaltungs-Bibliothek. Seit Version 2.4.0.0 ist außerdem ein IRC-Client im Programm integriert.\n\nFür Experten verfügt das Programm über einen sogenannten „Power-Modus“, der eine Vielzahl erweiterter Funktionen, wie das Bearbeiten von Downloads und Benutzen des Schedulers, sowie Zugang zu erweiterten Programmeinstellungen bietet. Auch erlaubt er, über verschiedene Fenster die Funktionalität des Programms zu überwachen und auf den Sicherheitsmanager zuzugreifen.\n\nDes Weiteren lässt sich die Darstellungsweise des Programms sehr weitgehend verändern: drei Ansichtsmodi können ausgewählt werden (eine vereinfachte, eine Tab- und eine Fenster-Ansicht), außerdem lässt sich mit sogenannten „Skins“ (Dateierweiterung .sks; basierend auf XML) das Erscheinungsbild der Benutzeroberfläche fast beliebig modifizieren.\n\nShareaza wird in einer modifizierten Version auch von Firmen zur Aufdeckung von Urheberrechtsverletzungen eingesetzt, um automatisch Tauschbörsen nach urheberrechtlich geschützten Werken zu durchsuchen und die IP-Adressen der Anbieter zur rechtlichen Verfolgung zu protokollieren. Mit dem Programm werden von diesen Firmen ebenfalls sogenannte Fakes oder Köder in P2P-Tauschbörsen gestellt. Fakes verbreiten sich aber kaum weiter, da die fraglichen Dateien schnell durch die Integration von Metadaten gekennzeichnet werden können.\nDiese Verwendung von Shareaza kann auf die Veröffentlichung der Quelltexte zurückgeführt werden.\n\n\n"}
{"id": "126875", "url": "https://de.wikipedia.org/wiki?curid=126875", "title": "BIND", "text": "BIND\n\nBIND ist ein Open-Source-Programmpaket für die Namensauflösung im Domain Name System. Sein Name geht zurück auf den \"Berkeley Internet Name Domain Server\", kurz \"BIND Server\". Neben dem Server umfasst das Programmpaket einen Client und Testprogramme. Der Server ist mit großem Abstand der am weitesten verbreitete seiner Art im Internet.\nAufgrund seiner weiten Verbreitung und der zeitnahen Umsetzung der aktuellen DNS-RFCs gilt BIND seit Jahren als DNS-Referenzsoftware.\n\nBevor es DNS gab, wurde die Auflösung von Namen in IP-Adressen über Listen (/etc/hosts.txt, vgl. /etc/hosts auf heutigen Unix-Systemen) vorgenommen, die auf jedem Rechner im Internet vorhanden sein mussten. Änderungen wurden zunächst manuell auf einem Masterserver durchgeführt und dann per Datei-Download an die einzelnen Rechner verteilt. Mit steigender Anzahl von IP-Teilnehmern wurde dieses Verfahren zunehmend unhandlicher.\n\n1983 wurde von Paul Mockapetris das Domain Name System (DNS) spezifiziert. Im gleichen Jahr wurde die erste DNS-Software – \"JEEVES\" – auf einem DEC-Rechner implementiert. Wenig später gingen die ersten drei Internet-Root-Server in Betrieb.\n\nAnfang der 1980er Jahre wurde an der Universität Berkeley an der Weiterentwicklung von UNIX gearbeitet. Einige Studenten begannen, für UNIX eine DNS-Software zu schreiben, die sie BIND (Berkeley Internet Name Domain) tauften. BIND wurde ständig weiterentwickelt, und die Version 4 wurde zum weltweiten Standard. Nachdem die Berkeley-Universität die Weiterentwicklung der Software eingestellt hatte, wurde die Verantwortung für kurze Zeit von der Firma DEC und anschließend von \"Vixie Enterprises\" übernommen. Paul Vixie war zu dieser Zeit treibende Kraft hinter dem Projekt.\n\nAb der Version 4.9.3 ging BIND in die Verantwortung der Non-Profit-Organisation ISC (\"Internet Software Consortium\" – ab 2004: \"Internet Systems Consortium\") über. Die Version 8 wurde 1997 fertiggestellt. 1999 beauftragte ISC die Firma \"Nominum Inc.\", die Version 9 zu entwickeln. Dessen erste Version BIND 9.0.0 erschien am 16. September 2000. BIND 9 gilt seit 2007 als Standard und bildet zusammen mit der noch verbreiteten Version 8 das Rückgrat des weltweiten \"Domain Name Systems\".\n\nAb August 2007 wurde die Version 8 \"nicht mehr gepflegt\" (Deprecated) und ISC legte allen Nutzern nahe, auf Version 9 zu wechseln.\n\nAm 1. April 2009 begann die Entwicklung von Bind 10, am 21. Februar 2013 wurde die Version \"BIND 10: 1.0.0\" veröffentlicht.\n\nAm 23. April 2014 erklärte das ISC überraschend, die weitere Entwicklung von BIND10 einzustellen. Man wolle sich künftig auf die Weiterentwicklung von BIND9 konzentrieren.\n\nVersion 4 gilt seit langem als veraltet und der Weiterbetrieb von BIND-4-Servern als Sicherheitsrisiko; auch die Weiterentwicklung von BIND 8 wurde im Jahr 2007 eingestellt. ISC empfiehlt allen DNS-Administratoren die schnellstmögliche Migration zu BIND 9, auf der ISC-Website werden hierzu umfangreiche Informationen bereitgestellt. Wegen der gegebenen weitestgehenden Interoperabilität zwischen den Versionen gibt es keinen technischen Zwang zur Migration, weshalb die Entwickler häufig Überzeugungsarbeit hierzu leisten müssen, z. B. auf der Mailing-Liste \"bind-users@isc.org\".\n\nIm Februar 2008 entdeckte Dan Kaminsky eine neuartige Angriffsmethode, die Cache Poisoning mit geringem Zeitaufwand ermöglicht, um den Nutzer durch gefälschte DNS-Antworten auf andere Server umzuleiten. Es handelt sich dabei um eine Lücke im Design des Domain Name Systems, von der neben BIND auch mehrere andere Nameserver betroffen waren. In Zusammenarbeit mit Nameserver-Entwicklern und -Betreibern, unter anderem Paul Vixie, wurden Patches entwickelt, um die Wahrscheinlichkeit eines erfolgreichen Angriffs zu senken. Im Juli 2008 veröffentlichte Kaminsky Details zu der Sicherheitslücke und schätzte, dass noch 41 % der DNS-Server angreifbar seien.\n\nDas Verhalten von BINDs zentraler Programmkomponente, dem Daemon „codice_1“, wird durch Konfigurations- und Zonendateien bestimmt, die manuell vom Administrator oder automatisch über Skripte, aber auch mit Hilfe von Frontends erstellt oder verändert werden können. Für den grundlegenden Betrieb sind mindestens zwei Dateien notwendig, einmal die Konfigurationsdatei, oft „codice_2“ genannt, und pro Zone eine Zonendatei, deren Name gewöhnlich aus dem Zonennamen und der Dateiendung „codice_3“ gebildet wird. Anstelle von Plain text-Dateien können auch Datenbanken, zum Beispiel BDB, als Quelle genutzt werden, dazu muss ein geeignetes Treibermodul mit einkompiliert werden.\n\nDie offizielle BIND-Dokumentation ist das \"BIND 9 Administrator Reference Manual\", kurz ARM genannt. Dort erhält man einen umfassenden, trotzdem gut verständlichen Überblick über alle Konfigurationsdirektiven sowie den Aufbau der Zonendateien.\n\nDer Begriff der \"Zone\" wurde im Kontrast zur Domain geprägt, weil sie zwar miteinander verwandt, aber nicht notwendigerweise \"kongruent\" sind: eine Zone kann durchaus eine Teilmenge einer Domain darstellen und zum anderen nicht auf Host-Deklarationen innerhalb einer Domain beschränkt sein, sondern Verweise auf Hosts in „Fremd“-Domains enthalten.\n\nDie Master-Zonendateien enthalten mindestens einen SOA Resource Record und einen oder mehrere für die Zone aussagefähige Nameserver (NS Resource Records), weiterhin eine beliebige Anzahl weiterer Resource Records (RRs) wie zum Beispiel A Resource Records oder PTR Resource Records. Allgemein notiert man RRs so:\n\n\"LinkeSeite\" [optional:Time-to-live-Wert] [optional:Class-Name] Typ \"RechteSeite\"\n\n\"LinkeSeite\" und \"RechteSeite\" sind grundsätzlich Zeichenketten, deren Format vom \"Typ\" bestimmt wird. Die RRs stellen also Wertepaare dar, getrennt durch die Typzuweisung – codice_4 usw. – und optionale zusätzliche Attribute, nämlich eines Time-to-live-Wertes sowie einer Klassenbezeichnung, („codice_5“ für \"Internet\", welches bei Fehlen der Klassenangabe als \"Default\" angenommen wird, sowie „codice_6“ für \"CHAOSnet\" und „codice_7“ für \"Hesiod\", zwei Bezeichnungen für historische Vorstufen der Internet-Entwicklung, mittlerweile irrelevant). \"LinkeSeite\" kann auch leer sein (als \"White Space\", d. h. Leer- oder Tabulator-Zeichen), dann gilt jeweils der \"LinkeSeite\"-Wert des \"vorangehenden\" RRs.\n\nMithin werden links immer die möglichen abfragbaren Informationen und rechts die zugehörigen Antwortwerte notiert. So liefert ein A-RR die einem Hostnamen zugeordnete IP-Adresse zurück („codice_8“); PTR-RRs dagegen dienen dem Umkehrfall, der Zuordnung von bestimmten Hostnamen zu abgefragten IP-Adressen (reverse DNS, „codice_9“).\n\nZonendateien für Vorwärts- und Rückwärtsauflösung müssen konsistent formuliert werden; wie auch grundsätzlich gilt, dass für jede einzelne abfragbare Information ein RR in der betreffenden Zonendatei vorhanden sein muss: es gibt keine automatische, deduktive Ableitung irgendwelcher DNS-Informationen, wie es etwa für die Bereitstellung der \"reverse-DNS-\"Auflösung denkbar wäre (indem man z. B. PTR-Anfragen durch „inverse“ Auflösung vorhandener A-RRs – \"RechteSeite\": Frage, \"LinkeSeite\": Antwort – beantworten würde).\n\nAllerdings sind sog. „Wildcard“-RRs möglich, bei denen ein Stern („codice_10“) auf der linken Seite für beliebige Hostnamen steht. Diese gelten jedoch grundsätzlich als problematisch, weil sie ein weiteres Szenario für Angriffe auf die Integrität eines Netzes oder Dienstes eröffnen: es wird beliebigen Rechnern erleichtert, eine falsche Identität anzunehmen.\n\nDie Zonendateien definieren damit den Inhalt einer Zone, im Wesentlichen – aber nicht ausschließlich – sind das die Hostnamen innerhalb einer Domain (also A-RRs, welche naturgemäß am häufigsten durch DNS-Clients abgefragt werden). Ebenso definiert man den für eine Domain zuständigen Mailserver (MX Resource Record, Mail Exchanger), Alias-Namen zu vorhandenen Hostnamen (CNAME-RRs, Canonical Names) oder Meta-Informationen (TXT-RRs).\n\nSubdomains definiert man durch sog. \"Zone Delegation\": in der Zonendatei der übergeordneten Domain wird dazu der gewünschte Subdomain-Name als Verweis auf den für die Subdomain authoritativen, also verbindlich aussagekräftigen Nameserver registriert (mithin ein NS-RR), diesen ergänzt man oft noch durch einen A-Record mit der IP-Adresse des betreffenden Subdomain-Nameservers, den sog. \"Glue-Record\" (der Begriff „Glue“ – engl. für Leim – symbolisiert, dass auf diese Art und Weise die hierarchische Anbindung zwischen Domain und Subdomain hergestellt wird).\n\nLetzterer kann (bzw. muss sogar) entfallen, wenn der Subdomain-Nameserver selbst weder in der Sub- noch der übergeordneten Domain verankert ist (mithin in einer „Dritt“-Domain liegt, für die der gerade abgefragte Nameserver nicht authoritativ ist; BIND 9 weist solche A-RRs als „\"out-of-zone\" data“ zurück und verweigert das Laden der betreffenden Zone und damit ggf. den Programmstart). Während eine solche Konstellation ansonsten problemlos realisierbar ist, wird man jedoch in den meisten Fällen – organisatorisch betrachtet – es vorziehen, das Hosting der Subdomain-Zone entweder an einen Nameserver \"in dieser Subdomain\" zu delegieren oder aber „selbst zu erledigen“, mit anderen Worten: auf dem Nameserver der übergeordneten Domain vorzuhalten.\n\nIst ein Glue-Record vorhanden, befähigt das den Nameserver zu sogenannten \"Smart-Answers\": wird im folgenden Beispiel „codice_11“ nach dem Hostnamen „codice_12“ gefragt (ein Client unterscheidet i. d. R. nicht weiter zwischen Host- und Domainnamen), lautet die Antwort sinngemäß: „Eine IP-Adresse für codice_12 kenne ich nicht. Aber codice_14 kann weiterhelfen, Du findest ihn unter der IP-Adresse 192.168.50.1.“ Ohne Glue-Record würde der letzte Teilsatz entfallen, bzw. müsste lauten: „Finde die IP-Adresse von codice_14 doch selbst heraus!“ Dem Client, der hier auf seine eigentliche Anfrage eine Negativantwort erhalten hat, ist es (bei entsprechend „smarter“ Programmierung seiner Resolver-Bibliothek) freigestellt, stattdessen die zusätzlich übermittelten Informationen auszuwerten und somit einen DNS-Request zur Auflösung von „codice_14“ einzusparen. An dieser Stelle liegt es, sowohl bei vorhandenem als auch fehlendem Glue-Record, ohnehin immer in der Verantwortung des \"Resolvers\", sich zum gewünschten Subdomain-Nameserver „durchzuhangeln“ (wobei er sich jedoch der – weiter unten betrachteten – Fähigkeit eines Nameservers zur Rekursion bedienen \"kann\", sofern nur dies dem betreffenden Client erlaubt ist).\n\nDas Beispiel gilt für eine Domain „codice_17“ mit\n\nDie „codice_17“-Domain wird als Zonendatei „codice_22“ auf „codice_11“ gehostet:\n\nAuf 192.168.50.1 muss dann ein weiterer Nameserver für die Zone „codice_12“ residieren. Jedoch könnte man diese genauso gut von „codice_11“ verwalten lassen – dazu ändert sich der vorletzte RR des Beispiels in „codice_26“, weiterhin kann der Glue-Record entfallen, da BIND hier selbständig erkennt, dass man für die Subdomain \"autoritativ\" ist (auf diesen Begriff wird gleich noch näher eingegangen).\n\nUnterhalb der Second-Level-Domain-Hierarchiestufe kann jeder Betreiber eines Nameservers nach Belieben Subdomains definieren, \"in\" derselben ist das den Domain-Registraren vorbehalten, die ihrerseits Zugriff auf die Nameserver der Top-Level-Domains haben.\n\nDa gemäß der DNS-Spezifikation Nameserver redundant vorgehalten werden sollen, aber das Pflegen identischer Zonefiles auf zwei oder mehreren unabhängigen Computern sehr umständlich und fehlerträchtig ist, unterscheidet man Master- und Slave-Server. Letztere holen eine Zonendatei per Zonentransfer von einem zugewiesenen Master-Server. Dabei wird die im SOA-Record der Zone definierte Seriennummer auf Veränderung geprüft, nur nach Inkrementierung derselben erfolgt ein Slave-seitiges Übernehmen der Zonendaten; seit BIND v8 existiert auch ein \"Notify\"-Verfahren, bei dem der Master-Server Slaves über die Veränderung von Zone-Files benachrichtigt (um die Latenz der Zonen-Updates zu minimieren). Dabei kann der Administrator durch „codice_27“- und „codice_28“-Direktiven genau festlegen, welcher Slave durch welchen Master zu benachrichtigen ist. Im „named.conf“-Beispiel weiter unten findet sich je ein Muster für eine Master- („codice_29“) und eine Slave-Zonendefinition („codice_30“).\n\nMan bezeichnet Nameserver bzw. ihre Antworten als \"autoritativ\", wenn die DNS-Anfragen unmittelbar aus einer vorliegenden Zonendatei beantwortet werden können – im Gegensatz zu durch Rekursion bzw. Forwarding gewonnenen DNS-Daten, die im Cache des Servers vorgehalten werden. Master- wie Slave-Nameserver können einander gleichwertig autoritative Antworten generieren (auch wenn ein Slave „nur“ Kopien der Master-Zonen vorhält).\n\nNeben dem Zugriff auf die in ihren Zonendateien verankerten Hostnamen beherrschen Nameserver auch das rekursive Auflösen „unbekannter“ Host- bzw. Domainnamen, dabei werden diese, von rechts beginnend, zerlegt und an die für die jeweiligen Top-Level- und Subdomains zuständigen Nameserver gerichtet. Die Abfrage beginnt dabei bei den Root-Nameservern, deren IP-Adressen jedem Nameserver vorab bekannt sein müssen und die ihrerseits Verweise auf die Nameserver der Top-Level-Domains zurückgeben.\n\nVerantwortungsbewusste DNS-Administratoren konfigurieren ihren Server nun allerdings so, dass zunächst ein oder mehrere (netz-)topologisch „benachbarte“ (bzw. „übergeordnete“) Nameserver befragt werden (das sog. Forwarding), ehe eine vollständige Rekursion über die Root-Server veranlasst wird (um letztere zu entlasten). Dabei spekuliert man darauf, dass bei den \"Forwardern\" die Wahrscheinlichkeit höher ist, dass die benötigte Information (oder Teile davon, etwa die Auflösung der abgefragten Top-Level-Domain) schon in ihrem Cache vorliegt.\n\nAus der traffic-minimierenden Vermaschung interagierender Nameserver sowie dem Zwischenspeichern (Caching) der gewonnenen Informationen mit wohldefinierten Minimal- und Maximal-„Haltbarkeitsfristen“ ergibt sich die optimierte, kooperative Arbeitsweise des internetweiten Domain Name Systems.\n\nWährend Forwarding bei einer „fabrikneuen“ BIND-Distribution standardmäßig aktiviert ist (Option „codice_31“), ist beim Aktivieren von Rekursion Vorsicht angesagt. Bei einem Nameserver, der sowohl aus dem Intra- wie auch aus dem Internet erreichbar ist, sollte man Rekursion nur für Benutzer aus dem Intranet erlauben (z. B. durch eine Option wie „codice_32“), da dies sonst als Einfallstor für Denial-of-Service- und Cache-Poisoning-Attacken aus dem Internet ausgenutzt werden kann.\n\nDie Informationen sind in verschiedenen Bereichen untergebracht. Die wichtigsten sind:\n\nIm \"Globalen Bereich\" werden Zugriffsberechtigungen, Krypto-Keys und Optionen definiert (siehe Abschnitt \"BIND-Options\" in der Online-Dokumentation). In der (optionalen) \"Serverliste\" sind Informationen über Partner-Server enthalten (z. B. ob ein Server inkrementellen Zonentransfer unterstützt).\nIn der \"Zonen-Liste\" ist für jede bereitzustellende Zone ein Eintrag enthalten, der den Namen der Zone, den Namen des zugeordneten Zonenfiles, den Zonen-Typ (Master oder Slave), Zugriffsberechtigungen sowie Options enthält. Mit letzteren können auch global schon definierte Options wieder überschrieben werden (und sind dann nur im Kontext der jeweiligen Zone gültig). In einer Minimal-Konfiguration eines Nameservers sind je eine Zonendatei für die Auflösung des Hostnamens „codice_38“ in die IP-Adresse 127.0.0.1 sowie die diesbezügliche Reverse-Zone enthalten. Im „named.conf“-Beispiel weiter unten sind das die ersten beiden „codice_39“-Direktiven. Die zugehörigen Zonendateien sind trivial und haben z. B. das folgende Aussehen (eine mögliche Zonendatei für die Domain „codice_17“ wurde bereits weiter oben dargestellt):\n\nDie „root“- bzw. „hint“-Zone (Direktive „codice_41“ im „named.conf“-Beispiel) kann ggf. weggelassen werden, da eine entsprechende Liste der Root-Server schon im Programmcode verankert ist. Durch Download einer aktuellen „codice_42“-Datei und Einbinden wie gezeigt kann jedoch leicht, d. h. ohne Quelltext-Modifikation und Neuübersetzung, auf Änderungen reagiert werden (die Liste der Root-Server wird zwar nur selten geändert, zuletzt aber am 12. Dezember 2008).\n\nDas Format der „codice_42“-Datei entspricht dem einer normalen Zonendatei mit NS- und A-RRs, jedoch ohne vorangestellten SOA-RR. Sie kann – neben dem Download bei der IANA – z. B. durch den Befehl\n\nbeschafft werden, sofern die aktuelle Adresse des A-Root-Nameservers bekannt ist.\n\nDer \"controls\"-Bereich definiert einen Control-Port als Schnittstelle für das rndc-Steuerprogramm und im \"logging\"-Bereich werden verschiedene Typen von Logdateien und deren Zuordnung von Programm-Ereignissen (Abfragen, Fehler etc.) eingestellt.\n\nBeispiel einer named.conf:\nNach dem Einlesen der Konfigurationsdateien nimmt BIND alle Pakete entgegen, die per UDP oder TCP am Port 53 der konfigurierten Schnittstellen oder IP-Adressen eintreffen. Bei diesen Paketen kann es sich um DNS-Abfragen, dynamische Updates oder Zonentransfers handeln. Normalerweise verwenden DNS-Anfragen UDP (einzelne IP-Pakete), nur wenn insbesondere bei Zonentransfers die Server-Antworten die maximale IP-Paketgröße überschreiten, wird auf TCP umgeschaltet.\n\nLiegt eine DNS-Abfrage vor, so versucht BIND, sie anhand der Einträge in den Zonendateien aufzulösen. Bei unbekannten Domainnamen (Anfragen für nicht-authoritative Hostnamen) wird in der Regel zunächst der eigene, dann der Cache der \"Forwarder\" überprüft und zuletzt eine rekursive Auflösung über die Root-Server versucht.\n\nBei dynamischen DNS-Updates wird die betreffende Zonendatei zur Laufzeit des named-Daemons aktualisiert (RRs werden hinzugefügt bzw. auch wieder entfernt), sofern der auslösende Client dazu berechtigt und verifiziert ist. Dynamische DNS-Updates werden insbesondere eingesetzt, um in einem Intranet, in welchem die TCP/IP-Protokollstacks neu hinzukommender Rechner automatisch konfiguriert werden, diese unter ihrem aktuellen, nicht von einer statisch konfigurierten Zone vorgegebenen Hostnamen erreichbar zu machen.\n\nBei UNIX- oder Linux-Systemen ist BIND manchmal im Lieferumfang enthalten. Neue Versionen können aus dem Internet entweder als Binärpaket (für Windows) oder als Sourcecode heruntergeladen werden. Mittlere UNIX-Kenntnisse sind ausreichend zur Installation und Betrieb eines BIND-Servers. Bei Windows NT/2000 wird eine komprimierte Binärdatei heruntergeladen, die ein Hilfsprogramm enthält, welches die Einrichtung von \"named\" als Systemdienst unterstützt.\n\nBei Änderungen in Zonenfiles darf nicht vergessen werden, deren Seriennummer zu inkrementieren und diese Änderung BIND bekannt zu machen, sei es durch einen kompletten Neustart des Servers, ein SIGHUP (UNIX) oder über die Management-Tools \"ndc\" (BIND 8) beziehungsweise \"rndc\" (BIND 9). Ohne diese Signalisierung muss erst die in der Zone eingetragene Time-to-live-Frist verstreichen, ehe \"named\" die Zone erneut lädt.\n\nDer Dienstprogramm-Name \"ndc\" bzw. \"rndc\" bedeutet \"(remote) name daemon controller\". Neben Befehlen zum Starten und Stoppen des Daemons sowie zum Neuladen der Konfiguration und von Zone-Files stehen eine Reihe von Logging- und Statistik-Funktionen zur Verfügung, mit denen die Arbeit der Software überprüft werden kann. Insbesondere, wenn BIND unter Betriebssystemen läuft, die Threads unterstützen, oder wenn dynamische Zone-Updates unterstützt werden, sollte unbedingt immer der Befehl <nowiki>rndc stop</nowiki> zum Beenden des Dienstes verwendet werden. Bevor der Nameserver mit \"rndc\" zusammenarbeitet, müssen die dazu autorisierten Hosts in „named.conf“ eingetragen sein; der Datenaustausch zwischen Daemon und \"rndc\" wird kryptografisch über einen Schlüssel abgesichert, der in „named.conf“ und „rndc.conf“ eingetragen sein muss. Standardmäßig arbeitet \"rndc\" über Port 953 (in Anlehnung an den für DNS reservierten Port 53); gegebenenfalls müssen Firewall-Regeln dafür eingerichtet werden.\n\n"}
{"id": "127010", "url": "https://de.wikipedia.org/wiki?curid=127010", "title": "Apple Safari", "text": "Apple Safari\n\nSafari ist ein Webbrowser des Unternehmens Apple. Er gehört zum Lieferumfang von macOS ab der Version Mac OS X Panther sowie von iOS und ersetzte den vorher mitgelieferten Microsoft Internet Explorer für Mac als Standard-Browser.\n\nVon Version 3 bis 5 war Safari auch für Windows verfügbar. Ab Version 6 erscheinen die neuen Versionen parallel zu den Veröffentlichungen der neuen Betriebssystemversionen.\n\nSafari basiert auf KHTML (HTML-Rendering) und KJS (JavaScript-Implementation) aus der K Desktop Environment. Beide werden von Apple im gemeinsamen Projekt WebKit als eigenständige Software \"WebCore\" (KHTML) und \"JavaScriptCore\" weiterentwickelt.\n\nWährend der Entwicklung des Browsers stand keine Namensgebung für die spätere finale Version fest. Erst im Monat vor der Produktfreigabe wurde der finale Name „Safari“ gewählt. Als mögliche Namensalternativen standen „Alexander“, „iBrowse“ oder „Freedom“ zur Auswahl.\n\nDie erste Version von Safari wurde auf der Macworld 2003 von Steve Jobs vorgestellt. Damit war es die erste Alternative zu Netscape, Opera und Internet Explorer, die damals die Vorherrschaft auf dem Betriebssystem OS X hatten.\n\nSafari 3.0 erschien am 11. Juni 2007 auf Apples Entwicklerkonferenz WWDC in einer Beta-Version und wurde erstmals auch auf Windows XP und Vista portiert. Die letzte unter Windows 2000 lauffähige Version ist 3.0.3, mit deaktivierter JavaScript-Funktion ist Version 3.2.3 die letzte. Die Windows-Version wurde laut dem Unternehmen innerhalb von 48 Stunden mehr als eine Million Mal heruntergeladen. Für Windows war zunächst eine Beta-Version verfügbar, die einige Instabilitäten zeigte. Die Benutzeroberfläche der Windows-Versionen lag ausschließlich in englischer Sprache vor.\n\nAm 18. März 2008 erschien die Version 3.1 für Mac OS X und Windows. Mit dieser Version verließ die Windows-Fassung das Beta-Entwicklungsstadium, sie ist erstmals auch in deutscher Sprache erhältlich. Außerdem wurde die Unterstützung aktueller Webstandards wie CSS3 verbessert.\n\nNeu in der am 13. November 2008 veröffentlichten Safari-Version 3.2 sind ein Phishing-Schutz und eine verbesserte Identifizierung von Online-Unternehmen.\n\nAm 24. Februar 2009 gab Apple eine „Public Beta“ von Safari 4 frei. Laut Apple sollte diese bis zu viermal schneller als der Vorgänger Safari 3 sein. Neuerungen waren unter anderem Cover Flow für besuchte Webseiten, eine sogenannte \"TopSite\"-Funktion, die die meistbesuchten Websites in einer grafischen Übersicht darstellt, und eine Suchfunktion, die diese Webseiten durchsucht. Dank der neuen Nitro-Engine führt Safari JavaScript deutlich schneller aus. Die „Public Beta“ erschien für Mac OS X und Windows XP/Windows Vista und bestand den Acid3-Test. Am 8. Juni 2009 erschien die fertige Version von Safari 4.\n\nAm 7. Juni 2010 erschien die Version 5.0 mit verbesserter HTML5-Unterstützung (u. a. mit Geolocation), erhöhter JavaScript-Geschwindigkeit und einem Safari-Reader zur verbesserten Darstellung von Artikeln, bei der der Inhalt hervorgehoben und der Rest der Webseite ausgeblendet wird. Weiterhin ist es nun möglich, Bing oder Yahoo als Standard-Suchmaschine zu wählen. Unter Windows verfügt Safari nun über Hardware-Beschleunigung.\n\nAb Version 5.0.1 bietet Safari – ähnlich wie Firefox, Opera oder Google Chrome – die Möglichkeit, Erweiterungen zu installieren.\n\nDie Version 5.1 benutzt das neue WebKit2-Framework, wodurch die Darstellung der Webseiten in einen eigenen Prozess ausgelagert werden kann. Dies erhöht die Stabilität des Programmes, da Programmierfehler in der Rendering-Engine nicht mehr zum Absturz des gesamten Browsers führen können. Unter Mac OS X Lion läuft der ausgelagerte Darstellungsprozess zur Verbesserung der Sicherheit in einer Sandbox.\n\nAm 25. Juli 2012 erschien Safari zusammen mit OS X Mountain Lion, für Mac OS X Lion ist die neue Version als Software-Aktualisierung verfügbar. Zu den neuen Funktionen zählen u. a. ein vereinheitlichtes Such- und Adressfeld, Synchronisierung geöffneter Tabs mit iOS-Geräten via iCloud, die Unterstützung des Do-Not-Track-Headers und der Web Audio API. Die Reader-Funktion ist erreichbar über einen größeren und auffälligeren Knopf am rechten Rand der Adressleiste, der sich blau verfärbt, sobald von einer Website ein Artikel aufgerufen wird. Ab Version 6.1 wird anstelle der Schrift Palatino nunmehr Georgia verwendet, die eine bessere Bildschirmdarstellung ermöglicht.\n\nVersion 6 wurde nicht mehr für Windows veröffentlicht.\n\nZusammen mit dem im Sommer 2013 präsentierten neuen Apple-Betriebssystem „OS X Mavericks“ wurde eine neue Version von Safari vorgestellt, deren Verbesserungen laut Apple insbesondere in der Geschwindigkeit beim Rendern von Websites und dem Verarbeiten von JavaScript-Anwendungen liegen. Die finale und erste der Öffentlichkeit zugängliche Version ist am 22. Oktober 2013 im Zuge der Veröffentlichung von Mac OS X 10.9 Mavericks erschienen.\n\nSafari 8 wurde ab Oktober 2014 zusammen mit OS X Yosemite verteilt. Neu aufgenommen wurde vor allem die Unterstützung für die Darstellung von 3D-Grafiken mittels WebGL sowie die Unterstützung für das Netzwerkprotokoll SPDY. Verschlüsselte Videos können mit HTML5 Premium Video wiedergegeben werden.\n\nDer am 30. September 2015 veröffentlichte Safari 9 wurde nicht nur für das gleichzeitig freigegebene Betriebssystem OS X El Capitan, sondern auch für die beiden älteren Systeme OS X Mavericks und OS X Yosemite bereitgestellt. Die neuen Features in dem Release können aber nur unter El Capitan genutzt werden. Dazu zählen eine tab-bezogene Kontrolle über die Tonwiedergabe, eine neue Darstellung im Reader und Neuerungen beim Ausfüllen von Webformularen. Nachgerüstet wurde das bereits von anderen Webbrowsern bekannte Fixieren von Tabs, die man öfter nutzt, damit diese nicht versehentlich geschlossen werden. Hinzu kommen eine Reihe von Sicherheitsupdates und -features.\n\nSafari 10 kam am 20. September 2016 zusammen mit macOS Sierra auf den Markt. Wie schon der Vorgänger ist Safari 10 wieder zwei Generationen abwärtskompatibel, hier also bis zu Version OS X Yosemite, wobei nicht alle neuen Funktionen bei den älteren Systemen anwendbar sind. Insgesamt wurden diesmal noch mehr Kopplungen an das Betriebssystem gelöst. Neuerungen sind unter anderem eine \"Bild-in-Bild-Funktion\", die Integration von Apple Pay, erstmals auch die sogenannten App-Erweiterungen, bei denen Softwareentwickler nunmehr auch die Möglichkeit haben, eigene Anwendungen mit Safari-Plug-ins auszustatten und als Erweiterungen anzubieten. Des Weiteren wurden Lesezeichenansicht und Verwaltung umfassend überarbeitet, eine standardmäßige Abschaltung von Flash-Inhalten eingeführt, der zu diesem Zeitpunkt neueste JavaScript-Standard (2015) unterstützt, sowie eine Kompatibilität mit der Force-Touch-Technologie integriert.\n\nApple veröffentlichte am 17. August 2016 eine erste lauffähige Developer-Preview zum Herunterladen, mit weiteren Releases in Folge. Standardmäßig blockiert Safari 11 alle Autoplay-Videos, die eine Tonspur haben. Im Optionen-Fenster kann die automatische Reader-Funktion aktiviert werden, um nur Text und Bilder darzustellen und Seitennavigation und Werbung auszublenden. Per Intelligent Tracking Prevention (ITP) kann die Erfassung des Surfverhaltens, das seitenübergreifende Tracking (Cross-Site-Tracking), durch Machine-Learning-Algorithmen auch bei eingeschalteten Cookies dezimiert werden. Werbefirmen warfen Apple daraufhin Sabotage vor, weil ihre Cookies zur Einblendung personalisierter, seitenübergreifender Werbeanzeigen binnen 24 Stunden getilgt werden. Apple verteidigte ITP, weil damit legitime Werbung nicht geblockt werde, sondern nur das Tracking des kompletten Browserverlaufs eines Nutzers.\n\nSafari 12 wurde am 17. September 2018 veröffentlicht und ist für die Systeme macOS Sierra 10.12.6, macOS High Sierra 10.13.6 sowie Mojave 10.14 verfügbar.\n\nSafari unterstützt die meisten aktuellen WWW-Standards, darunter CSS 3, HTML5, XSLT, XHTML 1.1 und JavaScript. Zudem lieferte Apple am 31. Oktober 2005 als erster Hersteller einen Browser (Safari 2.0.2), der den Acid2-Test besteht. Dieser Test überprüft unter anderem die Konformität des getesteten Webbrowsers zu den Standards des World Wide Web Consortium (W3C).\n\nBei einer Untersuchung der Unterstützung der W3C-Standards (sog. Empfehlungen) erreichte Safari in der Version 6.0 eine Quote von 94 % der getesteten Eigenschaften. Die gleiche Quote erreichte auch Opera 12.1. Mozilla Firefox 17.0 erreichte sogar 95 % der Eigenschaften, Internet Explorer 10.0 hingegen lediglich 86 %.\n\nZum Rendern von HTML-Seiten verwendet Safari Apples WebKit, ein C++-Framework mit Objective-C-API (in der Mac-OS-X-Variante), das auf der KHTML-Bibliothek des KDE-Projekts basiert. Dazu hat Apple eine Abspaltung (engl. „fork“) davon erstellt, bei der einige Änderungen vorgenommen wurden, einerseits, um eine verbesserte Anbindung an andere Mac-OS-X-Bibliotheken zu ermöglichen, andererseits, um die Darstellung zu verbessern. In zukünftigen Versionen des WebKits werden Standards der WHATWG implementiert werden.\n\nWie andere moderne Browser unterstützt Safari die Navigation mit Registerkarten („Tabbed Browsing“) und bietet einen Pop-up-Blocker.\n\nEine besondere Eigenschaft von Safari ist die Funktion „SnapBack“, die das Zurückkehren zur Ausgangsseite ermöglicht.\n\nÄhnlich anderen mitgelieferten Programmen im Betriebssystem OS X können zahlreiche Gesten, beispielsweise zur Navigation oder zum Vergrößern und Verkleinern genutzt werden. Das Programm unterstützt Farbmanagement. Safari 2.0, das als Bestandteil von Mac OS X 10.4 „Tiger“ seit dem 29. April 2005 zur Verfügung steht, enthält eine Funktion zum Surfen ohne Cache, Cookies und andere Datenspuren.\n\n\nDas Wall Street Journal machte Anfang 2012 bekannt, dass Google einen Weg gefunden hatte, Cookies in Safari zu speichern, obwohl der Nutzer dies generell nicht gestattet hatte. Google wurde deswegen durch die US-Handelsbehörde FTC im August 2012 zu einer Strafe von 22,5 Millionen US-Dollar verurteilt.\n\nIm Februar 2019 war Safari mit 15,56 Prozent laut \"StatCounter\" nach Google Chrome weltweit der am zweithäufigsten verwendete Browser, wenn man Desktopanwendung und mobile Nutzung zusammen rechnet. Der Anteil bei ausschließlich mobiler Anwendung (Handy) liegt bei 21,29 Prozent, bei Tablets bei 67,32 Prozent (deutlich vor allen anderen) und im Vergleich zu allen Browsern, die im Desktopbereich genutzt werden, bei lediglich 5,77 Prozent.\n\n\n"}
{"id": "127239", "url": "https://de.wikipedia.org/wiki?curid=127239", "title": "Megapixel", "text": "Megapixel\n\nMegapixel steht in Anlehnung an das SI-Präfix für eine Million Bildpunkte (Pixel) was einem quadratischem Bild mit einer Höhe und Breite von jeweils 1024 Pixel entspricht. Es ist die gebräuchliche Einheit zur Angabe der Sensor- und Bildauflösung in der Digitalfotografie. In der Werbung diente diese Zahl lange Zeit als weitgehend einziges Merkmal zur Bewertung einer Digitalkamera.\n\nEine einheitliche Abkürzung hat sich noch nicht eingebürgert, gebräuchlich sind sowohl „MP“ als auch „Mpx“, „Mpix“ und „MPixel“. Für die Angabe der physikalischen Auflösung zählen die Kamerahersteller jeden farbigen (Sub-) Pixel einzeln, also jeden roten, blauen oder grünen (Sub-)Sensor eines Sensors. Die Anzahl der Pixel einer Kamera ist daher nicht gleichzusetzen mit der Pixelangabe bei einem Bildschirm. Höhere Bildauflösungen ermöglichen größerformatige Fotoabzüge, da die Anzahl der Bildpunkte pro Fläche größer, also die Rasterung kleiner ist. Für die Bildqualität entscheidend sind die physikalischen Pixel des Bildsensors, nicht die durch Interpolation künstlich errechneten.\n\nDie nachfolgende Kritik an mehr Pixeln bei gleicher Sensorfläche bezieht sich fast ausschließlich auf einen Vergleich bei unterschiedlichen Ausschnitten bzw. Vergrößerungen des Bildes. Bei identischer Vergrößerung lassen sich die genannten Kritikpunkte nicht aufrechterhalten.\n\nDie meisten Digitalkameras haben sich in den letzten Jahren vornehmlich in zwei Merkmalen verändert. Zum einen hat die Zahl der Bildpunkte auf inzwischen zumeist 24 Megapixel (Stand: 2017; weit über zehn Megapixel, Stand Ende 2008; ca. 14 Megapixel, Stand Ende 2010, ca. 16 Megapixel, Stand 2015) zugenommen; zum anderen wurden die Bildsensoren immer kleiner. So hat sich die Größe der Bildsensoren in den Jahren von 2005 bis 2010 von zirka 60 mm² auf 30 mm² halbiert, während die Gesamtauflösung auf das Vierfache gestiegen ist – die einzelnen Bildpunkte waren damit nur noch ein Achtel so groß wie fünf Jahre zuvor. Da sich jedoch Fehlinformationen in Bezug zur eigentlichen Bildinformation bei einer kleineren Fläche stärker auswirken, kommt es zum sogenannten Bildrauschen. Dies wird insbesondere bei höheren Belichtungsindizies (in Analogie zu den früheren Filmempfindlichkeiten) oder bei dunklen Bildflächen zu einem Problem. Dieses Problem wirkt sich vor allem bei einer 1:1-Ansicht aus, wenn man im Vergleich zu einem Bild einer niedriger auflösenden Kamera deutlich näher in das Bild hineinzoomt. Bei einer gleich großen Ausgabe egalisiert sich das Problem wieder, da sich die Fehlinformation wieder auf mehrere Pixel verteilt. Um dieses Problem aber auch bei großen Ausgabeformaten zu umgehen, verwenden moderne Kameras zunehmend Rauschunterdrückungsverfahren, die versuchen, diese Bildfehler auf Kosten der Bildschärfe beziehungsweise durch Detailverlust zu korrigieren. Sehr kleine Pixelgrößen schränken allerdings die Freiheit bei der Wahl der Blendenzahl ein, wenn man bei Vergrößerung in der 1:1-Ansicht sehr gute Ergebnisse erwartet, außerdem fallen bei zu kleinen Pixeln Störungen durch Beugung des Lichts bei entsprechenden Vergrößerungen eher auf.\n\nAls Vorteil einer hohen Auflösung bleibt aber gerade bei Kameras mit dem üblichen Bayer-Sensor eine geringere Empfindlichkeit für den Moiré-Effekt und die Möglichkeit, auch kleinere Ausschnitte in immer noch ausreichender Auflösung zu erstellen.\n\nIm Gegensatz dazu verwenden digitale Systemkameras überwiegend größere Sensoren (oft APS-C mit zirka 350 mm²), die noch bei einer Auflösung von 37 Megapixeln eine Pixelgröße hätte, die beispielsweise einer heutigen Kompaktkamera mit nur drei Megapixeln entspräche. Rauschen tritt bei diesen Kameras oft nicht so stark auf wie bei Modellen mit kleineren Sensoren. Mit zunehmender Pixelzahl jenseits von 20 Megapixel und unterschiedlichen Ausstattungsvarianten sind pauschale Aussagen hinsichtlich des Rauschverhaltens nicht möglich. Bei gleicher Megapixelzahl sind größere Sensoren kleineren bauartbedingt hinsichtlich des Bildrauschens im Vorteil, allerdings zu Lasten der Schärfentiefe. Bei gleicher Schärfentiefe muss abgeblendet werden, wodurch sich der Lichteinfall verringert und meist die Sensorempfindlichkeit gesteigert werden muss. Damit verbunden ist aber wieder eine Erhöhung von Rauschen. Allerdings ist gerade bei sehr großen Pixelzahlen das Bildrauschen auf Pixelebene im Gesamtbild gar nicht mehr erkennbar, da die Wiedergabemedien, wie zum Beispiel Bildschirme oder Drucke, sowie die menschliche Netzhaut nicht in der Lage sind, so viele einzelne Bildpunkte aufzulösen, so dass entsprechend interpoliert wird, wodurch sich der Bildfehler verteilt.\n\nGroße Bildsensoren (meist 1 Zoll Sensoren) werden auch bei einigen wenigen Kompaktkameras der oberen Preisklassen eingesetzt.\n\nEin Mittelweg ist der sogenannte Micro-Four-Thirds-Standard, bei dem ein Bildsensor mit der Größenbezeichnung 4/3\" und einer Fläche von zirka 225 mm² eingesetzt wird. Dieser soll die Herstellung von vergleichsweise kleinen und leichten Kameras, mit dennoch rauscharmen Bildern ermöglichen.\n\nEin ähnlicher Trend ist bei Videokameras zu beobachten. Hier steigt zwar die Anzahl der Pixel nur unwesentlich, jedoch werden die Bildsensoren bei nichtprofessionellen Kameras immer weiter verkleinert, um immer größere Zoombereiche aus gleichbleibend kompakten und preisgünstigen Objektiven herauszuholen.\n\n"}
{"id": "127913", "url": "https://de.wikipedia.org/wiki?curid=127913", "title": "Numerische Strömungsmechanik", "text": "Numerische Strömungsmechanik\n\nDie numerische Strömungsmechanik () ist eine etablierte Methode der Strömungsmechanik. Sie hat das Ziel, strömungsmechanische Probleme approximativ mit numerischen Methoden zu lösen. Die benutzten Modellgleichungen sind meist Navier-Stokes-Gleichungen, Euler-Gleichungen, Stokes-Gleichungen oder Potentialgleichungen.\n\nDie Motivation hierzu ist, dass wichtige Probleme wie zum Beispiel die Berechnung des Widerstandsbeiwerts sehr schnell zu nichtlinearen Problemen führen, die nur in Spezialfällen exakt lösbar sind. Die numerische Strömungsmechanik bietet dann eine kostengünstige Alternative zu Versuchen im Windkanal oder Wasserkanal.\nDas umfassendste Modell sind die Navier-Stokes-Gleichungen. Es handelt sich hierbei um ein System von nichtlinearen partiellen Differentialgleichungen 2. Ordnung.\nInsbesondere sind auch Turbulenz und die hydrodynamische Grenzschicht enthalten, was allerdings zu höchsten Ansprüchen an Rechnerleistung, Speicher und die numerischen Verfahren führt.\n\nEin einfacheres Modell sind die Euler-Gleichungen, die aufgrund der vernachlässigten Reibung die Grenzschicht nicht abbilden und auch keine Turbulenz enthalten, womit beispielsweise Strömungsabriss nicht über dieses Modell simuliert werden kann. Dafür sind wesentlich gröbere Gitter geeignet, um die Gleichungen sinnvoll zu lösen. Für diejenigen Teile der Strömung, in denen die Grenzschicht keine wesentliche Rolle spielt, sind die Euler-Gleichungen sehr gut geeignet.\n\nDie Potentialgleichungen schließlich sind vor allem nützlich, wenn schnell grobe Vorhersagen gemacht werden sollen. Bei ihnen wird die Entropie als konstant vorausgesetzt, was bedeutet, dass keine starken Schockwellen auftreten können, da an diesen die Entropie sogar unstetig ist. Weitere Vereinfachung über konstante Dichte führt dann zur Laplace-Gleichung.\n\nBei Mehrphasenströmungen spielen Wechselwirkungskräfte zwischen den Phasen eine Rolle, wobei geeignete Vereinfachungen durchgeführt werden können.\n\nCFD-Verfahren bilden auch die Grundlage für die numerische Aeroakustik, die sich mit der Berechnung von Strömungsgeräuschen befasst.\n\nDie verbreitetsten Lösungsmethoden der numerischen Strömungsmechanik sind\n\nDie FEM ist für viele Probleme geeignet, insbesondere für elliptische und parabolische im inkompressiblen Bereich, weniger für hyperbolische. Sie zeichnet sich durch Robustheit und solide mathematische Untermauerung aus. FVM ist für Erhaltungsgleichungen geeignet, insbesondere für kompressible Strömungen. FDM ist sehr einfach und deswegen vor allem von theoretischem Interesse.\n\nWeitere gebräuchliche Methoden sind\n\nBei allen Methoden handelt es sich um numerische Näherungsverfahren, die zur Validierung mit quantitativen Experimenten verglichen werden müssen. Mit Ausnahme der partikelbasierten Methoden ist der Ausgangspunkt der oben genannten Methoden die Diskretisierung des Problems mit einem Rechengitter.\n\nBei zeitabhängigen Gleichungen führt die Reihenfolge von Orts- und Zeitdiskretisierung auf zwei verschiedene Lösungsansätze:\nDie erste Methode wird vor allem bei hyperbolischen Gleichungen und kompressiblen Strömungen, letztere bei inkompressiblen Strömungen eingesetzt. Außerdem ist die Rothe-Methode flexibler im Hinblick auf eine Implementierung einer adaptiven Gitterverfeinerung im Ort während der Zeitevolution der Strömungsgleichungen.\n\nBei turbulenten Strömungen gibt es für die numerische Strömungssimulation noch viele offene Fragen: Entweder man verwendet sehr feine Rechengitter wie bei der direkten numerischen Simulation oder man verwendet mehr oder weniger empirische Turbulenzmodelle, bei denen neben numerischen Fehlern zusätzliche Modellierungsfehler auftreten. Einfache Probleme können auf Highend-PCs in Minuten gelöst werden, während komplexe 3D-Probleme selbst auf Großrechnern teilweise kaum zu lösen sind.\n\nIm kommerziellen Bereich wird der Markt von den Produkten der Firma ANSYS (Fluent, CFX) und CD-adapco (Star-CCM+) dominiert, beide basieren auf der Methode der finiten Volumen (FVM). Im Open-Source-Bereich ist OpenFOAM das am meisten verbreitete Software-Paket, welches ebenfalls auf der FVM basiert.\n\nIn dem Bereich der gitterfreien Löser, welche direkt die Navier-Stokes-Gleichungen analog zu den FEM oder FVM lösen, gibt es die kommerzielle Software LS-DYNA, MPMSim und Nogrid points. Für Löser, die die Boltzmann-Gleichung lösen (sogenannte Partikelmethoden, Lattice-Boltzmann-Methode) gibt es andere kommerzielle und frei verfügbare Löser, wie z. B. Powerflow, OpenLB oder Advanced Simulation Library. Für die \"smoothed particle hydrodynamics\"-Methode (SPH) ist ebenfalls Software frei verfügbar, wie pysph oder sphysics.\n\nDaneben gibt es aber eine große Vielzahl von Lösern, die auf spezielle Strömungsprobleme ausgerichtet sind und dort Verwendung finden. An vielen Universitäten werden Löser entwickelt, die sich insbesondere in akademischen Kreisen großer Beliebtheit erfreuen.\n\n\nDetails zu den verwendeten Algorithmen stehen in den oben unter „Verfahren“ verlinkten Artikeln. Umfangreiche Übersichten zu verfügbaren Anwendungen und Programmcodes sind über die folgenden Links zu erreichen:\n"}
{"id": "127930", "url": "https://de.wikipedia.org/wiki?curid=127930", "title": "Nmap", "text": "Nmap\n\nNmap ist ein Werkzeug zum Scannen und Auswerten von Hosts in einem Computernetzwerk und fällt somit in die Kategorie der Portscanner. Der Name steht für \"Network Mapper\".\n\nNmap wurde von einem unter dem Nickname \"Fyodor\" bekannten Hacker ursprünglich für das Betriebssystem Linux entwickelt. Das textbasierte Programm unterliegt der GNU General Public License und ist somit freie Software, kann aber auch kostenpflichtig unter einer alternativen Lizenz erworben werden (um es zum Beispiel innerhalb von Nicht-GPL-Software zu benutzen). Neben der textbasierten Variante gibt es noch die grafische Benutzeroberfläche NmapFE zur komfortablen Einstellung von Nmap, welche mittlerweile durch die Zenmap genannte GUI abgelöst wurde. Obwohl nmap ursprünglich ein Unix-Werkzeug war, existiert mittlerweile auch eine Portierung auf Windows-Betriebssysteme. Sie bietet annähernd den vollen Funktionsumfang, hat jedoch einige Einschränkungen.\n\nNmap wird in erster Linie für Portscanning (also das Untersuchen der Ports eines Hosts) eingesetzt. Das Werkzeug wurde ständig erweitert und konnte sich vor allem durch die aktiven Techniken für OS-Fingerprinting (das Erkennen des eingesetzten Betriebssystems auf dem Zielhost) einen Namen machen. Auch das Mapping von Umgebungen (Erkennen aktiver Hosts) ist möglich. Darüber hinaus lassen sich mit Nmap vereinzelt die hinter einem Port stehenden Dienste und deren Version auslesen.\n\nNmap ist sowohl bei Angreifern als auch bei Administratoren sehr beliebt, da es sehr effizient und zuverlässig arbeitet. Es ist ein wichtiger Bestandteil bei der Netzwerkdiagnose und Auswertung von netzwerkfähigen Systemen. Unter anderem wird es auch vom Vulnerability Scanner Nessus zur Erfassung offener Ports eingesetzt.\n\nIn einigen Filmen kommt Nmap vor: In Matrix Reloaded hackt sich der Charakter Trinity mithilfe des 2001 entdeckten SSH1-CRC32-Exploit in ein Kraftwerk ein, nachdem sie den Rechner mit Nmap gescannt hat. In dem Film Battle Royale wird der Quelltext von Nmap gezeigt. Des Weiteren wird in Das Bourne Ultimatum die damalige Beta-Version 4.01 mit der grafischen Oberfläche \"Zenmap\" benutzt. Außerdem wurde Nmap als Portscanner in dem Film Who Am I – Kein System ist sicher benutzt, um dadurch die Elektrizität eines Hauses abzuschalten.\n\n"}
{"id": "127992", "url": "https://de.wikipedia.org/wiki?curid=127992", "title": "Deutsche Terminbörse", "text": "Deutsche Terminbörse\n\nDie DTB Deutsche Terminbörse war von 1990 bis 1998 eine Börse für den ausschließlichen Handel von standardisierten Finanzderivaten.\n\nSie war die erste neue Börse, die nach dem Zweiten Weltkrieg in Deutschland gegründet wurde. \nTräger der öffentlich rechtlichen Deutsche Terminbörse war die DTB Deutsche Terminbörse GmbH.\nGleich in mehrerer Hinsicht betrat man mit ihr Neuland in der Bundesrepublik:\n\n\nZwar wurden vor Gründung der DTB an anderen deutschen Börsen auch Derivate gehandelt. Diese – meist Optionsscheine – waren jedoch nicht standardisiert und die Marktliquidität war insbesondere in schwierigen Marktsituationen recht dürftig. Die DTB Terminbörse sollte daher als Spezialbörse für den ausschließlichen Handel von standardisierten Finanzderivaten gegründet werden.\n\nJörg Franke, damals Geschäftsführer der Berliner Börse, erhielt den Auftrag, ein Team zu führen, um die neue Terminbörse als ein modernes Dienstleistungsunternehmen zu konzipieren und umzusetzen.\n\nIm Gegensatz zu den damaligen herkömmlichen Parkettbörsen fand der Handel an der DTB fortlaufend statt, ohne die Einschaltung von Kursmaklern. Market Maker verpflichteten sich dazu, jederzeit verbindliche Kauf- und Verkaufsangebote zu tätigen. \n\nAls erste elektronische Börse der Bundesrepublik konnte die DTB die gesamte Marktliquidität bündeln; eine Zersplitterung auf die verschiedenen Börsenplätze fand nicht statt. Zudem ermöglichte das zentrale elektronische Orderbuch eine optimale und für alle Teilnehmer gleiche Transparenz.\n\nNeu war auch, dass die DTB Deutsche Terminbörse GmbH die Erfüllung aller Transaktionen garantierte und somit das Zahlungs- bzw. Lieferungsrisiko wegfiel. Die Abwicklung der Transaktionen wurde in das elektronische System integriert. \n\nErstmals konzipierte man mit der DTB eine deutsche Börse als Markenartikel mit eigenen Markenprodukten. Noch vor dem Start der Börse richtete man dazu unter Wilhelm Brandt eine eigene PR-Stelle ein.\n\nBis auf die Integration der Abwicklung wurden diese Innovationen bei der späteren Deutschen Börse und ihrem neuen Systemen Xetra integriert.\n\n"}
{"id": "128273", "url": "https://de.wikipedia.org/wiki?curid=128273", "title": "NDR-Klein-Computer", "text": "NDR-Klein-Computer\n\nDer NDR-Klein-Computer (kurz NKC) war ein Selbstbauprojekt für einen Computer aus den frühen 1980er Jahren, welches seit 1984 durch die Fernsehsendung ComputerTreff des Bayerischen Fernsehens begleitet wurde. Der Name bezieht sich sowohl auf die kleine Größe, als auch auf den Entwickler des Computers, Rolf-Dieter Klein.\n\nIm Jahre 1984 brachte das NDR-Schulfernsehen unter der Leitung von Joachim Arendt eine 26-teilige Fernsehserie mit dem Titel \"NDR-Klein-Computer\" heraus, die je 15 Minuten dauerte. In dieser Sendung, die von Thomas Naumann moderiert wurde, ging es nicht nur darum, dem Zuschauer die Funktionsweise eines Computers nahezubringen, sondern es wurde ein modulares System vorgestellt, mit dem der technisch interessierte Laie in der Lage war, von den einfachsten Beispielen aus der Sendung bis hin zum hochwertigen Heimcomputer seine praktischen Übungen zu absolvieren. Die Idee zu diesem modularen System stammt von Rolf-Dieter Klein, der damals Autor bei der Computerzeitschrift \"mc\" war.\n\nMit einer Artikelserie begleitete er die auch von ihm moderierte Fernsehserie. Außerdem brachte er mehrere Bücher zum NDR-Klein-Computer heraus. Der Franzis-Verlag brachte die Fernsehserie als Videokassetten heraus und von der Zeitschrift mc erschienen mehrere Sonderhefte zu speziellen Themen zu diesem Computersystem. Das System wurde von den Firmen Graf Elektroniksysteme in Kempten, und dem Elektronikladen Detmold vertrieben. Die Firma Fischertechnik lieferte einen Roboterbausatz, der eine der anschaulichsten Anwendungen des NDR-Klein-Computers darstellte. Das größte Mikrocomputer-Bildungsprojekt seiner Zeit war angelaufen.\n\nDie Modularität des NKC war seine größte Stärke. Vom einfachen 8-Bit-Einplatinencomputer auf Z80A-Basis, mit dem einfache Steuerungsaufgaben bis hin zur CP/M-Software erledigt werden konnten, bis zum 32-Bit-System mit einer 68020-CPU reichte sein Hardwarespektrum. Das Softwarespektrum dieses Systems reichte von Bits und Bytes in EPROMs bis zum Betriebssystem CP/M und den darauf lauffähigen Programmen und Programmiersprachen. Sogar MS-DOS fand mit Hilfe einer 8088-CPU-Karte seinen Weg zum NKC. Einige Systeme sind immer noch in Betrieb, allerdings mit PS/2-Tastaturen, 3½\"-Floppy, IDE-Festplatten und modernen Speichern. Inzwischen wurden von den Nutzern neue Karten entwickelt, welche die damaligen Spezialbausteine ersetzen. So ist eine Karte entwickelt worden, welche eine neue Grafikkarte (mit VGA-Anschluss), eine serielle Schnittstelle, eine Soundkarte sowie PC-Tastatur- und -Mausanschluss bereitstellt. Für das 68xxx-System gibt es ein neues Grundprogramm, welches die IDE-Schnittstelle auch zum Booten unterstützt.\n\nIm Laufe der Jahre 2017 und 2018 wurden die Originalplatinen des NDR Klein Computers von engagierten Nutzern neu layoutet und in Details verbessert. Die neuen Layouts wurden dann in China gefertigt, wodurch das NKC System nun wieder einer großen Gruppe neuer Anwender zur Verfügung steht.\n\n\n"}
{"id": "128378", "url": "https://de.wikipedia.org/wiki?curid=128378", "title": "Registrierungsdatenbank", "text": "Registrierungsdatenbank\n\nDie Windows-Registrierungsdatenbank (meist nur \"Registry\" genannt) ist seit der ersten Version von Windows NT die zentrale hierarchische Konfigurationsdatenbank des Betriebssystems \"Windows.\" Hier werden sowohl Informationen zu Windows selbst als auch Informationen zu Programmen gespeichert. Mit Microsoft Windows 3.1 wurde die Windows-Registry 1992 auch im Bereich der Consumerbetriebssysteme eingeführt. Während unter den frühen Windowssystemen hauptsächlich Dateierweiterungen in der Registry gespeichert wurden, ist sie seit Windows NT 3.1 und seit Windows 95 eine umfassende Datenbank zur Speicherung aller Einstellungen für die Verwaltung des Systems und aller integrierten Systemdienste und -prozesse. Auch viele Anwendungsprogramme (aber nicht alle!) speichern ihre Einstellungen hier. Das Symbol der Registrierungsdatenbank ist ein aus vielen kleineren Würfeln zusammengesetzter großer Würfel mit drei freischwebenden Teilwürfeln.\n\nBevor sich in Windows das Konzept der Registry durchgesetzt hatte, wurden Einstellungen in Konfigurationsdateien (z. B. INI-Dateien) separat für jedes einzelne Programm in dessen Verzeichnis gespeichert. Dies bringt jedoch einige Nachteile mit sich: So werden die Einträge in einem Textformat gespeichert und ausgewertet, wodurch diese zwar einfach mit einem Texteditor bearbeitet werden können, für die Weiterverwendung in Programmen aber erst geparst werden müssen. Dies brachte in den 1990er Jahren Performancenachteile mit sich. Ferner können Berechtigungen nur auf Dateiebene, nicht auf Eintragsebene gesetzt werden; verschiedene Berechtigungsstufen für einzelne Einträge ließen sich sonst – wo nötig – nur durch verschiedene Konfigurationsdateien abbilden.\n\nDie Registrierungsdatenbank hat diese Nachteile nicht: Sie wird in einem binären Format gespeichert, sodass ihre Inhalte direkt und ohne Konvertierung weiterverarbeitet werden können. Informationen, die in einer Konfigurationsdatei als langer Text vorliegen, werden in der Registrierungsdatenbank „zerstückelt“ und in einzelnen Einträgen gespeichert. Dadurch können nicht nur Berechtigungen auf Eintragsebene gesetzt werden, sondern eine Blockade durch gleichzeitigen Schreibzugriff zweier Programme wird vermieden, wenn diese unterschiedliche Einträge bearbeiten. In einer Konfigurationsdatei müssten beide dieselbe Datei öffnen und editieren, in der Registrierung sind beide Wertepaare nur logisch durch Listen-Zellen verbunden. Um durch die „Zerstückelung“ keinen physikalischen Nachteil bei Zugriff auf die Daten zu haben, sorgt das System seit Windows XP von selbst für eine Defragmentierung.\n\nVorteile bringt die Registry auch im Netzwerkverbund unter Verwendung des Verzeichnisdienstes Active Directory. Über Gruppenrichtlinien können mehrere Arbeitsplatzrechner zentral und auf einmal gesteuert werden, denn auf die Daten der Registry kann auch über ein Netzwerk zugegriffen werden, da die Pfade zu den Werten standardisiert sind: Ein Programm muss nicht wissen, wo eine bestimmte Datei liegt; es spricht nur die Standard-API an, welche den Registrywert in dem Schlüssel ausliest oder schreibt.\n\nSeit der Einführung der Registrierungsdatenbank wurden von Microsoft eine Reihe von Verbesserungen durchgeführt. Bis zur Version NT 5.2 (Microsoft Windows Server 2003) konnte der Startvorgang des Rechners scheitern, wenn Kernel und der Hive SYSTEM nicht in die ersten 16 MB Arbeitsspeicher passten. Mit der Einführung von Vista fiel die Beschränkung weg. Ebenfalls mit Windows Vista wurde der Kernel Transaction Manager eingeführt, mit dem sich atomare Operationen innerhalb der Registry realisieren lassen, siehe Abschnitt Ausfallsicherheit. Mit Windows 7 wurde die Registry in Bezug auf das Sperr-Verhalten verbessert: Zuvor wurden bei einem Zugriff auf einen Unterschlüssel möglicherweise einige Oberschlüssel des Pfades mitgesperrt; mit Windows 7 wird nur noch der Schlüssel gesperrt, auf den tatsächlich auch zugegriffen wird.\n\nDie Windows-Registrierung hat neben den genannten Vorteilen auch einige bedeutende Nachteile, die sich aufgrund ihrer Architektur ergeben:\n\nDie zentralisierte und hierarchische Struktur kann leicht zu einem Single Point of Failure führen, wenn hierarchisch übergeordnete Schlüssel fehlerhaft sind. Einstellungen, die keine oder eine flache hierarchische Struktur, wie Konfigurationsdateien, aufweisen, funktionieren meist auch dann noch, wenn einzelne Werte fehlerhaft oder einzelne Dateinamen falsch sind.\n\nAnwendungen, die ihre Einstellungen in der Registry speichern, sind oft an den lokalen Rechner gebunden, was bedeutet, dass die Migration von einem Computer zum anderen sehr oft eine komplette Neuinstallation des Programms erfordert. Das Migrieren oder zentrale Speichern von Einstellungen auf einem Server ist mit der Windows-Registrierung nur mit großem zusätzlichem Aufwand und spezieller Software durch Synchronisation möglich. Konfigurationsdateien können dagegen direkt und ohne Umweg auf einen eingebundenen Server geschrieben werden.\n\nDas Anzeigen und Bearbeiten der Einstellungen in der Registrierdatenbank benötigt spezielle Software. Konfigurationsdateien sind dagegen mit jedem einfachen Texteditor zugänglich. Dies kann je nach Aufgabe den Wartungsaufwand minimieren.\n\nDie Registrierungsdatenbank besteht aus Schlüsseln (engl. \"keys\") und Einträgen (engl. \"entries\"). Ein Schlüssel ist dabei ein Behälter für Einträge und weitere Unterschlüssel, ähnlich einem Ordner auf Dateiebene. Die nebenstehende Grafik zeigt eine Auswahl wichtiger Schlüssel der heutigen Registry, angeordnet in einer Baumstruktur. Ein Eintrag in der Registrierungsdatenbank ist ein Name-Wert-Paar, ähnlich einer Datei. Der Wert (engl. \"value\") eines Eintrags kann unterschiedliche Datentypen aufweisen, etwa Binärcode, Zahl oder Text. Manchmal wird mit dem Begriff „Wert“ auch das Name-Wert-Paar gemeint. Die eigentlichen Werte werden dann als „Daten“ bezeichnet. Einträge in der Registry können auch unbenannt sein, so kann jeder Schlüssel in der Registry einen unbenannten Eintrag beherbergen. Diese werden als \"Standard\"-Werte bezeichnet.\n\nDie Registrierungsdatenbank ist in mehrere Haupt- bzw. Wurzelschlüssel unterteilt. Folgende Hauptschlüssel sind in aktuellen Windows-Versionen vorhanden:\nDaneben gab es in früheren Windows-Versionen zusätzlich die folgenden Hauptschlüssel:\n\nDie Hauptschlüssel werden häufig abgekürzt geschrieben, z. B. „HKLM“ für HKEY_LOCAL_MACHINE oder „HKU“ für HKEY_USERS.\nDie Abkürzung HKEY steht dabei für „\"handle (to a) key\"“.\n\nJeder Wert kann eine theoretische Größe von 1024 kB haben, die meisten Werte sind aber deutlich kleiner, und bestehen nur aus wenigen Bits. Es sind folgende Datentypen für Windows Vista und höher möglich:\n\n\nDie Registrierungsdatenbank wird über mehrere Dateien verteilt gespeichert, die in verschiedenen Verzeichnissen des Rechners abgelegt sind. Somit wird die Registry in mehrere Teilabschnitte unterteilt, welche auch als \"Hives\" (englisch für \"Bienenstöcke\") bezeichnet werden. Ein Hive ist dabei nicht zwangsweise mit einem Wurzelschlüssel identisch. So gibt es Wurzelschlüssel, die aus mehreren einzelnen Hives bestehen (z. B. codice_6 bei Windows NT), des Weiteren können Wurzelschlüssel auch nur virtuell sein, also einen Link auf einen anderen Teil der Registrierungsdatenbank darstellen.\n\nJe nach Betriebssystem-Version unterscheiden sich Aufteilung und Speicherort der Dateien.\n\nIn Windows 9x gibt es die folgenden Hives:\n\nIn Betriebssystemen auf Basis des NT-Kernels, bis einschließlich Windows 10, gibt es folgende Hives:\n\nDie Registrierungsdatenbank ist wie oben gezeigt auf mehrere Dateien verteilt. „Die Registry“ als monolithisches Objekt und Single Point of Failure gibt es deshalb im strengen Sinne nicht (je nach beschädigter Datei kann dies aber dennoch so betrachtet werden, z. B. bei der SYSTEM.DAT). Das, was zum Beispiel mit dem Registry-Editor regedit.exe als monolithische Datenbank dargestellt wird, ist die Implementierung des Configuration Managers, der Teil des NT-Kernels ist. Die Registry besteht aus einzelnen Dateien, von denen einige oben aufgeführt sind, die als \"Hives\" bezeichnet werden. Jeder dieser Hives enthält einen Registry-Baum, wobei der erste Schlüssel in der Datei die Wurzel (root) des Baumes darstellt. Wie bereits oben erwähnt, sind nicht alle Registry-Bäume real, d. h., sie haben kein Wurzelverzeichnis. Manche sind nur Spiegelungen, oder volatil. Wenn in der Bootphase SYSTEM.DAT in den Arbeitsspeicher geladen wird, schaut der Configuration Manager unter codice_7 nach den Liegeplätzen der Hives. An einem Beispielrechner:\nWie ersichtlich gibt es für den NT-Kernel keine Laufwerksbuchstaben; das Stammverzeichnis beginnt mit „\\“. Der Configuration Manager legt nun Symbolische Verknüpfungen an, beispielsweise von codice_8 auf codice_9. Dies ist notwendig, weil der Object Manager des Kernels beim Parsen des Strings codice_10 den Handle an den Configuration Manager weitergibt. Die Registry wird also wie ein Gerät („Device“) angesprochen.\n\nDer Configuration Manager unterteilt jeden Hive in Datenblöcke zu je 4096 Bytes, wie es auch bei einer Festplatte der Fall ist. Der Hive kann nur blockweise vergrößert oder verkleinert werden, also in Schritten zu ±4kB. Der erste Block eines Hives ist der Basisblock, der die Signatur „regf“, Sequenznummern, Zeitstempel des letzten Schreibzugriffes im Hive, die Versionsnummer des Hives, eine Prüfsumme und dessen vollen Namen (z. B. %SystemRoot%\\CONFIG\\SAM) enthält. Die Daten der Registry werden in Zellen (cells) abgelegt, welche Schlüssel, Wert, Security Descriptor, eine Liste der Unterschlüssel oder Schlüsselwerte enthalten können. Ein Feld am Anfang der Zelle beschreibt den Typ und die Größe. Wird eine neue Zelle in den Hive gelegt, und ist dafür eine Expansion des Hives nötig (+4096 Bytes), wird ein Behälter (bin) geschaffen, der die Zelle und den Leerraum des Blockes beinhaltet. Der Raum zwischen dem Ende der Zelle und dem Ende des Behälters kann später mit weiteren Zellen gefüllt werden. Behälter (bins) haben ebenfalls einen Header, der die Signatur „hbin“ beinhaltet sowie den Offset vom Beginn des Behälters/Zelle zum Leerraum hinter der Zelle sowie die Größe des Behälters.\n\nDie Unterteilung der Registry in Behälter (bin) und Zellen (cell) ermöglicht eine effiziente Arbeitsweise: Da Behälter seltener neu zugewiesen werden als Zellen, kann der Configuration Manager entscheiden, statt der Zellen die Behälter in den Arbeitsspeicher zu laden, um die Zahl der (Ent)ladevorgänge zu senken. Beim Einlesen kann der Configuration Manager auch entscheiden, nur Behälter, die Schlüssel enthalten, in den Arbeitsspeicher zu laden, und die leeren Behälter zu ignorieren. Wenn eine Zelle angelegt oder entfernt wird, fragmentiert der Inhalt der Behälter mit der Zeit, ähnlich wie bei einem Laufwerk. Der Configuration Manager defragmentiert die Registry deshalb kontinuierlich selbst: Wenn ein Behälter leer wird, werden die leeren Behälter in möglichst zusammenhängende Anschnitte gelegt. Ferner führt er Zellen zusammen, die durch Löschungen fragmentiert wurden.\n\nDas Finden von Zellen und Werten findet durch Anspringen statt: Eine Schlüssel-Zelle enthält einen Zellen-Index, der Zeiger (Informatik) auf Unterschlüssel-Zellen enthält. Um die Unterschlüssel zu finden, ist in den Zellen auch eine Liste der Unterschlüssel enthalten, welche mit dem jeweiligen Zellen-Index verknüpft sind. Um die Suche zu beschleunigen, sortiert der Configuration Manager die Listen alphabetisch. Die Listen werden durch binäre Suche nach dem Zielwert abgesprungen: Der Configuration Manager springt zuerst in die Mitte der Liste, prüft dann, ob der Wert vor oder nach dem Zielwert im Alphabet kommt, und springt dann in die Mitte der oberen bzw. unteren Hälfte. Die Halbierung läuft so lange weiter, bis der Zielwert gefunden wird. Dann wird der Zellen-Index des Ziels ausgelesen und diese Zelle angesprungen. Der Vorgang wird so lange wiederholt, bis die Zielzelle gefunden ist, oder das Ziel nicht in der Liste der Unterschlüssel auftaucht. In diesem Fall wird eine Fehlermeldung zurückgegeben. Die folgende Grafik veranschaulicht die Sprünge von Zelle zu Zelle in einem Registry-Hive, um Werte (Val 1, Val 2) oder Unterschlüssel (root, Sub Key) auszulesen.\n\nEs gibt fünf verschiedene Arten von Zellen; der Einfachheit halber ist die Sicherheitsbeschreibungszelle in der Grafik nicht abgebildet. Der Configuration Manager springt erst den Basisblock an und springt dann auf den Wurzelschlüssel. Von diesem Schlüssel aus springt er zum einen zu einer Wert-Listen-Zelle (hellblau), welche ihn zu den Wert-Zellen Val 1 und Val 2 springen lässt. Andererseits wird vom Wurzelschlüssel aus eine Unterschlüssel-Listen-Zelle (dunkelblau) angesprungen, welche ihn zum nächsten Unterschlüssel (Sub Key) springen lässt. Die fünf verschiedenen Zellenarten sind:\n\n\nDer Configuration Manager greift nicht bei jeder Suche auf das Festplatten-Abbild des Hives zu. Stattdessen werden alle nötigen Hives in den Adressraum des Kernels einbezogen, indem diese in den Auslagerungsspeicher (pagefile.sys) geladen werden. Nur beim Booten wird der System-Hive komplett in den Arbeitsspeicher geladen. Der Configuration Manager betreibt aufgrund der Fragmentierung im Auslagerungsspeicher \"Cell Index Mapping\", was der virtuellen Speicherverwaltung entspricht, nur eben für die Zellen der Registry. Er unterteilt auch jeden Hive im Auslagerungsspeicher in Blöcke zu 512 Bytes und weist ihnen je ein Bit zu. Wird dieser Abschnitt modifiziert, wird das Bit von 0 auf 1 gedreht und der Abschnitt zur Synchronisierung freigegeben. Der Hive-Sync findet 5 Sekunden nach dem Ereignis statt und synchronisiert alle geänderten Hive-Abschnitte zwischen Auslagerungsspeicher und Image. Finden derweil oder danach weitere Modifikationen an Hives statt, so werden diese erst nach weiteren 5 Sekunden synchronisiert. Um sicherzugehen, dass eine Wiederherstellung auch nach einem Abschmieren des Rechners während der Synchronisierung möglich ist, werden die geänderten Abschnitte zuerst in die *.log-Dateien geschrieben, die neben allen Hive-Dateien vorhanden sind. Danach erhöht der Configuration Manager eine fortlaufende Nummer im Hive, schreibt den modifizierten Abschnitt vom *.log in die Hive-Datei *.DAT und erhöht eine zweite fortlaufende Nummer im Hive. Stürzt der Rechner während des Schreibvorganges ab, sieht der Configuration Manager nach dem Reboot, dass die fortlaufenden Nummern nicht passen und aktualisiert weiter von *.log in *.DAT.\n\nFrüher hatten jede Datei des Maschinenhives und die .DEFAULT.DAT eine *.sav und *.log als Redundanz, wobei SYSTEM.DAT zusätzlich noch SYSTEM.alt als Redundanz besaß. Nur NTUSER.DAT war auf eine *.log beschränkt. Dies wurde bis einschließlich Windows Vista beibehalten. Moderne NT-Systeme ab Windows 7 haben als Redundanz *.log, *.log1, *.log2 für jede Registry-Datei, nicht nur SYSTEM.DAT.\n\nFür jeden geöffneten Registry-Schlüssel legt der Configuration Manager einen \"Key Control Block\" (KCB) an. Dieser enthält den kompletten Pfad des Schlüssels, einen Zellen-Index des Knotenpunktes und eine Flag, ob der Schlüsselsteuerblock gelöscht werden soll, wenn der letzte Handle beendet wurde. Windows legt alle Schlüsselsteuerblöcke in einer alphabetisch geordneten Hashtabelle ab, um schnelleren Zugriff zu ermöglichen. Wenn der Object Manager des Kernels von einer Anwendung codice_11 zu parsen bekommt, übergibt er den Namenspfad an den Configuration Manager. Dieser springt wie oben beschrieben die Schlüssel und Unterschlüssel durch, bis der Zielschlüssel (Ziel-Zelle) gefunden wurde. Danach prüft der Configuration Manager, ob der Schlüssel bereits geöffnet wurde. Wenn ja, wird der Zähler im Schlüsselsteuerblock um 1 erhöht. Wenn nicht, erstellt der Configuration Manager einen weiteren Schlüsselsteuerblock und fügt diesen in die Hashtabelle ein. Dann erstellt er ein Schlüsselobjekt, das auf den Schlüsselsteuerblock zeigt, und übergibt dieses an den Object Manager, der es an die Anwendung weitergibt. Wenn noch eine andere Anwendung auf denselben Schlüssel zugreifen möchte, bekommt sie ebenfalls das Schlüsselobjekt zu sehen. Soll ein neuer Schlüssel erstellt werden, springt der Configuration Manager zuerst den letzten bestehenden Schlüssel der Sprungkette an. Dann prüft er, ob der Raum in der Liste der freien Zellen ausreichend ist, um den neuen Schlüssel aufzunehmen. Wenn nicht, wird ein neuer Behälter eröffnet. Ansonsten wird der neue Schlüssel mit allen Daten angelegt und in die Index-Liste des Vaterschlüssels eingetragen.\n\nBis Windows NT 6.1 gab es nur eine globale Hashtabelle für alle KCBs. Ab Windows 7 besitzt jeder Hive eine eigene Hashtabelle; ferner wurde der Zugriff auf Schlüssel verbessert: Bei Schreibzugriffen auf einen Unterschlüssel waren früher alle Oberschlüssel des Pfades abgeschlossen; nun ist davon nur der Schlüssel betroffen, der tatsächlich auch beschrieben wird. Ferner wurde der Synchronisierungszyklus erhöht.\n\nAnwenderprogramme können ihre eigenen Informationen in die Registry ablegen. \n\nFür Anwendungen ist es nicht erforderlich, die Windows-Registrierung zu verwenden. NET-Framework-Anwendungen verwenden beispielsweise XML-Dateien zur Konfiguration, während portable Anwendungen ihre Konfigurationsdateien in der Regel zusammen mit ihren ausführbaren Dateien ablegen. Anwendungen die plattformunabhängig sind, wie z. B. Firefox oder VLC speichern ihre Einstellungen in Konfigurationsdateien, die speziell für ihre Bedürfnisse gestaltet sind. Viele Anwendungen benutzen die Windows-Registrierung nur spärlich, um von nur in Windows verfügbaren Schnittstellen unabhängig zu sein.\n\nEs ist dem Programmhersteller überlassen, ob diese Informationen in die Registrierungsdatenbank oder in einem der unten aufgelisteten gemeinsamen Ordnern abgelegt werden sollen (Stand: Windows 10): \n\nNicht benutzerbezogen:\n\nBenutzerbezogen:\n\nUm manuell in der Registrierungsdatenbank zu editieren, stellt Windows den Registrierungs-Editor \"regedit.exe\" bereit. Dieser kann in der Suchleiste durch Eingabe von \"regedit\" aufgerufen werden. In der linken Spalte werden die Hives und Schlüssel-Zellen hierarchisch abgebildet, rechts werden die dazugehörigen Wert-Zellen eines Schlüssels samt ihrem Inhalt einzeln aufgelistet. Die Schlüssel-Listen-Zellen werden nicht dargestellt, sondern nur durch die Baumstruktur der Schlüssel abstrahiert. Die Wert-Listen-Zellen sind ebenfalls nicht sichtbar, bilden aber die Struktur der Liste rechts. Die Sicherheits-Beschreibungs-Zellen werden durch das Kontextmenü abstrahiert, wenn auf einen Schlüssel \"Rechtsklick > Berechtigungen...\" ausgeführt wird.\n\nDie gesamte Registry kann exportiert werden, wenn auf das Symbol „Computer“ \"Rechtsklick > Exportieren\" ausgeführt wird. Gerade nicht eingehängte Teile, z. B. Schema.DAT und Components.DAT bleiben dabei aber unberücksichtigt. Wird ein (Unter)Schlüssel angewählt und exportiert, wird dieser samt seinen Unterstrukturen in eine Registrierungsdatei mit der Dateiendung *.reg geschrieben, welche in Unicode kodiert und damit menschenlesbar ist. Wird ein Schlüssel gewählt und im Fenstermenü \"Datei > Drucken...\" gewählt, werden auch die Informationen der Schlüssel-Zellen ausgedruckt, welche im Registrierungs-Editor nicht angezeigt werden, aber Teil der Zelle sind (zum Beispiel den letzten Schreibzugriff und der Klassenname). Die *.reg-Dateien sind Unicode-Textdateien mit der Zeichenfolge „Windows Registry Editor Version 5.00“ in der ersten Zeile. Die Syntax ist wie folgt:\n\n[<Hivename>\\<Schlüsselname>\\<Unterschlüsselname>]\n\"Wertname\"=<Werttyp>:<Wertdaten>\n\nWenn der Standardwert eines Schlüssels bearbeitet werden soll, wird ein At-Zeichen vorangestellt:\n\n[<Hivename>\\<Schlüsselname>\\<Unterschlüsselname>]\n@=<Werttyp>:<Wertdaten>\n\nZeichenketten (\"string values\") benötigen keine Werttyp-Angabe. Pfade in Wert-Zellen müssen aber mit „\\\\“ geschrieben werden, einzelne „\"“ als „\\\"“. Für den Werttyp gibt es folgende hex()-Abkürzungen:\n\nWindows Registry Editor Version 5.00\n\n[HKEY_CURRENT_USER\\Software\\Wikipedia]\n\"PathToExe\"=\"C:\\\\Program Files (x86)\\\\ACME Corp\\\\ACE.exe\"\n\"haenschen\"=hex:<Binär-Wert>\n\"klein\"=dword:<DWORD-Wert>\n\"geht\"=hex(0):<REG_NONE-Wert>\n\"allein\"=hex(1):<REG_SZ-Wert>\n\"in\"=hex(2):<REG_EXPAND_SZ Wert>\n\"die\"=hex(3):<Binär-Wert> ; identisch mit \"hex:\"\n\"weite\"=hex(4):<DWORD-Wert> ; Little-Endian\n\"Welt\"=hex(5):<DWORD-Wert> ; Big-Endian\n\"hinein\"=hex(7):<REG_MULTI_SZ-Werte> ; getrennt durch Komma\n\"Stock\"=hex(8):<REG_RESOURCE_LIST-Werte> ; getrennt durch Komma\n\"und\"=hex(a):<REG_RESOURCE_REQUIREMENTS_LIST-Werte> ; getrennt durch Komma\n\"Hut\"=hex(b):<QWORD-Wert> ; acht Hex-Werte, getrennt durch Komma\n\nEin vorangestelltes Minus entfernt den Schlüssel:\n\n[-HKEY_LOCAL_MACHINE\\SOFTWARE\\Wikipedia]\n\nWerte in einem Schlüssel werden durch ein „-“ nach dem Wertname entfernt:\n\n[HKEY_LOCAL_MACHINE\\SOFTWARE\\Wikipedia]\n\"MeineMeinung\"=-\n\nwobei codice_17 den Standardwert entfernt und codice_18 die Zeichenkette codice_19 und seinen Wert. In die Reg-Daten können auch Kommentare einfließen:\n\n[HKEY_LOCAL_MACHINE\\SOFTWARE\\Wikipedia]\n\"MeineMeinung\"=\"WikipediaIstGut\"\n\nDie Reg-Dateien werden in die Registrierungsdatenbank gelesen, wenn sie doppelt geklickt werden. Ein Editieren ist mit \"Rechtsklick > Bearbeiten\" möglich.\n\nSeit dem Erscheinen der Windows PowerShell gibt es eine weitere sehr einfache Möglichkeit, die Registry zu verwalten. Dabei kann man auf die Registry direkt über die Konsole oder durch ein Shellskript wie auf ein herkömmliches Laufwerk zugreifen. Die PowerShell kann quasi zwischen den Verzeichnis-Welten des Object Managers und des Configuration Managers wechseln. Im „normalen“ Verzeichnis navigiert man mit den Aliassen codice_20 um sich Unterverzeichnisse anzeigen zu lassen, codice_21 um ein Unterverzeichnis anzunavigieren, codice_22 um ein Verzeichnis zurückzugehen usw. Gibt man beispielsweise codice_23 ein, wechseln man auf den Hauptschlüssel \"HKEY_LOCAL_MACHINE\". Zu den Unterschlüsseln gelangt man ebenfalls über den Befehl codice_24 oder in der Langform codice_25. Der Befehl codice_26 zeigt alle Eigenschaften (Registry-Einträge), die für den aktuellen Registryschlüssel gespeichert sind. Auf diese Weise lassen sich beispielsweise durch Eingabe der folgenden Befehlsfolge in der PowerShell alle Einträge des Run-Schlüssels anzeigen:\n\ncd HKLM:\ncd Software\\Microsoft\\Windows\\CurrentVersion\\Run\nGet-ItemProperty .\nNach dem Eingeben erfolgt unter anderem (PSPath, PSParentPath, PSChildName, PSProvider) als Ausgabe codice_27. Gleiches gilt, wenn man die Laufwerke des Rechners durch den Befehl codice_28 anzeigen lässt, wobei hier nicht alle Registry-Laufwerke angezeigt werden. Wie bereits oben im Anschnitt „Arbeitsweise der Registry“ gezeigt, wird die Registry von Windows selbst wie ein Laufwerk/Gerät verwaltet, was in der PowerShell auch sichtbar wird. Mit dem Befehl codice_29 oder einem anderen Laufwerksbuchstaben wechselt die PowerShell wieder in die Welt des Object Managers / Datei-Explorers zurück.\n\nAuch ein indirektes Auslesen der Registry wird damit möglich: Mit den Befehlen codice_30 wird der Pfad geholt und in die Variable $key geschrieben, codice_31 holt den Wert „Test“ und steckt ihn in die Variable $wert, und mit codice_32 kann die Eigenschaft „Test“ im Schlüssel „Run“ ausgegeben werden, hier also der Pfad des Autostarteintrages.\n\nDer Befehl codice_33 (Alias: md) legt einen neuen Schlüssel namens „Wikipedia“ an, codice_34 (Alias: del) entfernt ihn wieder. Mit codice_35 wird eine Zeichenfolge namens „MeineMeinung“ mit dem Wert „WikipediaIstGut“ im Schlüssel „Wikipedia“ abgelegt. Neben String (REG_SZ) sind ExpandString (REG_EXPAND_SZ), Binary (REG_BINARY), DWord (REG_DWORD), MultiString (REG_MULTI_SZ) und QWord (REG_QWORD) ebenfalls zulässig.\n\nDas Konsolenregistrierungsprogramm reg.exe läuft nur innerhalb einer Eingabeaufforderung cmd.exe, wobei die Befehle auch in der PowerShell eingesetzt werden können. Die Syntax ist dabei sehr einfach, der Nachteil aber die fehlende Befehlszeilenergänzung, was das Risiko von Schreibfehlern erhöht. Die Syntax zum Abfragen von Schlüsseln ist wie folgt:codice_36, mit den angehängten optionalen Parametern /v Wert (sucht nach einem bestimmten Registrierungswert), /ve (sucht nach dem Standard- oder leeren Wert), und /s (sucht nach allen Unterschlüsseln und Werten). Der Befehl\n\nreg query HKCU\\Software\\Microsoft\\Windows\\Currentversion\\run\n\nin einer cmd.exe eingegeben gibt eine Liste aller Autostarteinträge des Run-Schlüssels im Benutzer-Hive zurück. Die Syntax zum Anlegen von Schlüsseln ist wie folgt: codice_37 mit den angehängten optionalen Parametern /v Wert (hinzuzufügender Wert unter dem Schlüssel), /ve (fügt einen Standardwert hinzu), /t (Datentypen: REG_SZ | REG_MULTI_SZ | REG_DWORD_BIG_ENDIAN | REG_DWORD | REG_BINARY | REG_DWORD_LITTLE_ENDIAN | REG_NONE | REG_EXPAND_SZ), /s (bestimmt das Trennzeichen in der Datenzeichenfolge), /d (Daten), und /f (Überschreiben). Der Befehl\n\nreg add HKCU\\Software\\Microsoft\\Windows\\Currentversion\\run /v Test /t REG_SZ /d calc.exe\n\nlegt im Autostartschlüssel „Run“ die Zeichenfolge (REG_SZ) namens „Test“ an, welche als Wert „calc.exe“ hat. Würde dieser Schlüssel beibehalten, würde sofort nach dem Einloggen des Users der Taschenrechner aufpoppen. Die Syntax zum Entfernen von Schlüsseln ist wie folgt: codice_38 mit den angehängten optionalen Parametern /v Wert (zu löschender Wert unter dem Schlüssel), /ve (löscht den Wert des Standardwertes), /va (löscht alle Einträge des Schlüssels), und /f (für “mit Gewalt”). Der Befehl\n\nreg delete HKCU\\Software\\Microsoft\\Windows\\Currentversion\\run /v Test\n\nlöscht die Zeichenfolge „Test“ mit ihrem Wert „calc.exe“. Der böse Taschenrechner ist gebannt. Mit dem Befehl codice_39 wird der Schlüssel 1 an die Position von Schlüssel2 kopiert. Mit dem Parameter /s werden die kompletten Unterschlüssel mitgenommen, /f erzwingt das Kopieren. Weitere Befehle wie Save, Load, Unload, Restore, Compare, Export, Import, usw. usf. sind möglich.\n\nAufgrund der Tatsache, dass in der Registry große Teile der Systemkonfiguration gespeichert sind, wird diese oft als Single Point of Failure angesehen. Eine Beschädigung der Registrierungsdatenbank kann das Starten des Betriebssystems erschweren oder gar unmöglich machen. Aufgrund dessen werden eine Reihe von Maßnahmen ergriffen, die einer Beschädigung der Registrierungsdatenbank vorbeugen oder diese rückgängig machen können:\n\nVielfach wird damit geworben, dass eine „Reinigung“ der Registrierungsdatenbank notwendig oder wünschenswert sei, um einen Geschwindigkeits- und Stabilitätsvorteil zu erhalten.\n\nDer Nutzen von sogenannten „Registry-Cleanern“ wird jedoch überwiegend angezweifelt und als Mythos eingestuft. So würden ungenutzte und damit überflüssige Einträge in der Registry nur einen verschwindend geringen Teil ausmachen, deren Bereinigung nicht ins Gewicht falle. Der US-amerikanische Autor und \"Most Valuable Professional\" Ed Bott schätzt den Nutzen als verschwindend gering ein und warnt gleichzeitig davor, dass ein fälschlicherweise entfernter Eintrag dazu führen könne, dass auf dem System installierte Programme nicht mehr ordnungsgemäß funktionieren. Die Nutzung von Registry-Cleanern sei somit abzulehnen: „Don't run registry cleaner programs, period.“ (deutsch: „Benutze keine Programme zum Bereinigen der Registry. Punkt.“).\n\nAuch in Testberichten konnte der vermeintliche Nutzen durch das Bereinigen der Registry nicht nachgewiesen werden: Die Webseite \"Windows Secrets\" testete die Reinigungsprogramme CCleaner und jv16 PowerTools 2011 und verglich diese mit der Windows-internen Datenträgerbereinigung. Bei beiden Programmen konnte kein Geschwindigkeitsvorteil gegenüber der Windows-Datenträgerbereinigung gemessen werden. Die Windows-Datenträgerbereinigung lässt die Registry jedoch unberührt und beschränkt sich auf das Löschen überflüssiger Dateien auf der Festplatte.\n\nBis hin zu Windows XP (inkl. Windows Server 2003) konnte der Bootvorgang scheitern, wenn Kernel und SYSTEM.DAT mehr als die ersten 16 MB Arbeitsspeicher belegten. Microsoft bot deshalb das hauseigene Tool „RegClean“ zum Entfernen unnötiger Registry-Einträge an. Dies ist aber bei allen modernen Windows-Versionen überflüssig.\n\nWindows 95 sichert die Registrierung bei jedem erfolgreichen Start und speichert diese als codice_40 und codice_41 im Systemverzeichnis. Eine manuelle Sicherung ist mit dem Programm codice_42 möglich, das sich auf der Windows 95-CD befindet.\n\nUnter Windows 98 und Windows Me existiert stattdessen das Programm codice_43, das bei jedem erfolgreichen Start von Windows zahlreiche wichtige Systemdateien, darunter die Registrierung, sichert, aber auch manuell aufgerufen werden kann, um eine Sicherung anzulegen oder das System von einer Sicherung wiederherzustellen. Standardmäßig werden bis zu fünf Backups als CAB-Datei im Ordner %systemroot%\\Sysbckup angelegt. Über eine INI-Datei können diese und weitere Einstellungen modifiziert werden. Aufgrund eines Programmfehlers sichert codice_43 die codice_45 nicht, wenn diese nicht im Systemverzeichnis liegt, weil mehrere Benutzerprofile angelegt wurden.\n\nAlle Versionen von Windows 9x bieten zudem die Möglichkeit, mittels des Registrierungseditors codice_46 im MS-DOS-Modus die gesamte Registrierung in eine Registrierungsdatei zu exportieren und auch wieder zu importieren. Windows 9x sichert außerdem direkt nach Ende des Windows-Setups eine Kopie der codice_47 unter dem Namen codice_48 im Stammverzeichnis der Festplatte.\n\nWindows NT bis einschließlich Version 4.0 boten die Möglichkeit, eine Kopie der Registrierung unter dem Verzeichnis %systemroot%\\repair anzulegen und diese bei Bedarf auf einer sogenannten Notfalldiskette zu sichern. Standardmäßig legt Windows eine solche Notfalldiskette am Ende des Setups an, eine Sicherungskopie der Registrierung und (optional) eine Notfalldiskette kann aber auch manuell durch Aufrufen des Programms codice_49 erstellt werden. Standardmäßig werden die Dateien codice_50 und codice_51 nicht gesichert, es sei denn codice_49 wird mit dem Parameter /S aufgerufen.\n\nIn Windows 2000 und Windows XP wird die Registrierung stattdessen über das Programm \"Sicherung\" (codice_53) gesichert. Standardmäßig ist in der Windows XP Home Edition das Programm \"Sicherung\" nicht vorhanden, es kann aber von der Windows XP-CD nachinstalliert werden.\n\nBetriebssysteme ab Windows Vista aufwärts bieten keine Möglichkeit mehr, die Registrierung zu sichern.\n\nDas für Linux- und Unix-Systeme verfügbare Win32-API namens Wine enthält eine eigene Implementation der Windows-Registrierungsdatenbank. Wine selbst legt seine eigenen Einstellungen darin ab. Daneben können andere Windows-Programme, die auf Wine laufen, ihre Einstellungen dort eintragen. Für Win32-Anwendungen erscheint die Registrierungsdatenbank genau gleich wie auf einem Windows-NT-System. Im Hintergrund befindet sich aber – anders als bei Windows-NT-Systemen und wie in unixoiden System für Einstellungen üblich – keine Datenbank, sondern einfache ASCII-Textdateien. In den folgenden Dateien im Verzeichnis codice_54 ist die Registrierungsdatenbank von Wine in Form lesbarer Texte enthalten:\n\nDas ReactOS-Projekt, das versucht Windows-NT nachzubauen, übernimmt Teile von Wine, darunter auch die Umsetzung der Windows-Registrierungsdatenbank.\n\nIn den meisten unixoiden Betriebssystemen, wie FreeBSD, macOS oder in den Linux-basierten gibt es keine zentrale Konfigurationsdatenbank, sondern zahlreiche zentral abgelegte Konfigurationsdateien.\n\nJedoch gibt es Projekte, die Registry-artige Datenbanken auch für unixoide Systeme bereitstellen wollen, beispielsweise \"Elektra\" oder die Gnome-Konfigurationsdatenbank \"GConf\" bzw. der Nachfolger \"DConf\". GConf baute im Gegensatz zur Windows-Registry und DConf konsequent auf XML-Dateien auf, was die Möglichkeit bot, die Schlüssel mit jedem Texteditor oder XML-Parser zu lesen und bearbeiten. Ebenso legt Elektra die Schlüssel in Plain-text-Dateien ab, die z. B. mit Editoren wie vi bearbeitet werden können.\n\nApple setzt bei Mac OS X teilweise auf sogenannte Property Lists, die im XML-, JSON- oder in einem proprietären Binär-Format vorliegen können.\n\n"}
{"id": "128569", "url": "https://de.wikipedia.org/wiki?curid=128569", "title": "KSVG", "text": "KSVG\n\nBei KSVG handelt es sich um eine KDE-spezifische Implementierung des vom World Wide Web Consortium empfohlenen SVG-Standards. Obwohl KSVG grundlegend als Plug-in für den Webbrowser Konqueror entworfen wurde, können SVG-Dateien auch problemlos in jeder anderen beliebigen KDE-Applikation eingebunden und angezeigt werden, da KSVG die KParts-Komponenten-Technologie verwendet.\n\nDie Architektur von KSVG ähnelt sehr KDEs KHTML, der Rendering-Komponente für HTML, und wird außerdem von Apple für ihre eigene, von KHTML abgewandelte Rendering-Engine \"WebKit\" in dem Browser Safari verwendet.\n\n"}
{"id": "128650", "url": "https://de.wikipedia.org/wiki?curid=128650", "title": "Kleincomputer KC 85/2-4", "text": "Kleincomputer KC 85/2-4\n\nDie Kleincomputer der Reihe KC 85/2-4 wurden ab 1984 in der DDR vom volkseigenen Betrieb VEB Mikroelektronik „Wilhelm Pieck“ Mühlhausen aus dem \"Kombinat Mikroelektronik Erfurt\" in den Modellen HC 900, KC 85/2, KC 85/3 und KC 85/4 gebaut. Der Hersteller dachte zunächst an den Hobby- und Privatbereich, allerdings wurden die meisten Rechner für die Volksbildung reklamiert. Bis kurz vor dem Ende der DDR (ca. 1988) waren diese Computer dadurch für Privatpersonen schwer erhältlich. Auch der hohe Preis (3.900 M für den KC 85/3 – später reduziert auf 1.750 M, 4.600 M für den KC 85/4 – später reduziert auf 2.150 M) sorgte dafür, dass die „Kleincomputer“ kaum ihren Weg in Privathaushalte fanden.\n\nIm Jahr 1984 wurden in der DDR zwei Heimcomputer auf Basis des 8-Bit-Mikroprozessors U880 (Z80) vorgestellt:\nDie 1985 erfolgten Umbenennungen von Z 9001 in KC 85/1 sowie von HC 900 in KC 85/2 erfolgte aufgrund der Änderung des Anwendungszwecks, denn die KCs sollten nicht als Heimcomputer eingesetzt werden, sondern als Kleincomputer in Schulen und anderen Ausbildungseinrichtungen.\n\nDie Systemarchitektur des HC 900 war alles andere als perfekt: Die mit Zählerschaltkreisen realisierte Bildschirmansteuerung war mühsam zu programmieren. Wesentliche Systemfunktionen wurden mit PIOs (z. B. Bankswitching) und CTCs (Kassetteninterface, Blinken, Tonausgabe) realisiert. Die Tonerzeugung etwa erfolgte durch zwei CTC-Kanäle mit nachgeschaltetem Flipflop. Extras wie Blitter oder Sprites fehlten ganz. Das Betriebssystem CAOS (Cassette Aided Operating System) und HC-BASIC waren zwar recht komfortabel, aber langsam. Mit dem Mühlhäuser Originalbetriebssystem dauerte das Scrolling des Bildschirms 0,6 Sekunden und das Löschen des Bildschirms 1,75 Sekunden. Dies verbesserte sich mit dem KC 85/4 deutlich. Die KC 85/2-4 realisierten über Zusatzmodule einige Standard-Schnittstellen wie Centronics für Parallel-Drucker, und serielle Schnittstellen (wie RS232C bzw. V.24). Ab 1989 gab es für die Baureihe Floppy-Disc-Laufwerke als Zusatzgerät, das einen eigenen CP/M-Rechner darstellte und das Basisgerät als Terminal nutzte.\n\nEr basierte auf der 8-bit-CPU U880 (einem Zilog Z80-Clone) mit 1,75 MHz (HC 900 = KC 85/2, KC 85/3) bzw. 1,7734475 MHz (KC 85/4) Prozessortakt. Der typische Anwendungsfall der Mühlhausen-Rechner war ein KC 85/3 mit 32 KByte RAM (erweiterbar mit Zusatzmodulen), 16 KByte ROM-BASIC, angeschlossenem Kassettenrekorder zur Datenspeicherung und Anschluss an einen als Monitor benutzten Fernseher (über UHF-Modulator, FBAS-Ausgang oder RGB-Ausgang). Dem KC 85/2, er hatte nur 4 KByte ROM, fehlten das ROM-BASIC und die Kleinbuchstaben. Der KC 85/4 kam mit 128 KB RAM und verbesserten Grafikmöglichkeiten, die aber durch den Zusammenbruch der DDR kaum noch ausgenutzt wurden. Alle KC 85 aus Mühlhausen waren grafikfähig; die Bildschirmauflösung betrug 320×256 Bildpunkte. Allerdings war die „Farbauflösung“ wesentlich geringer: In einem Pixelrechteck von 4 × 8 Pixeln konnte es nur eine Vordergrundfarbe (aus 16 möglichen) und eine Hintergrundfarbe (aus 8 möglichen) geben. Diese Einschränkung verringerte sich beim KC 85/4 auf eine Linie aus 1 × 8 Pixeln, und zusätzlich konnte ein „echter“ Farbmodus mit 4 Farben und ohne Begrenzung eingeschaltet werden. Erst beim KC 85/4 entfiel die störende Eigenheit der Mühlhäuser KC-Reihe, dass Speicherzugriffe der CPU auf den Bildschirmspeicher Bildstörungen verursachten.\nDie Erweiterung des KC 85/3 waren:\n\nDie wesentlichen Erweiterungen des KC 85/4 waren:\n\nDie größte Umstellung war die Erweiterung (mit einer zum KC 85/2 und KC 85/3 inkompatiblen Organisation) des Bildschirmspeichers:\n\nEs gab eine Vielzahl von Erweiterungsmodulen für die KC 85/2-4.\n\nErkennen konnte man die Module durch Lesen vom I/O-Port xx80, wobei xx für die Nummer des Modulsteckplatzes steht. Im Basis-Gerät D001 standen die Modulsteckplätze 08 und 0C zur Verfügung. In Erweiterungsaufsätzen D002 standen vier weitere Steckplätze zur Verfügung (10, 14, 18 und 1C, umschaltbar auch auf andere Adressen).\n\nDurch \"Lesen\" vom Port xx80 erhielt man die „Strukturbyte“ genannte Modul-Kennung:\nDamit war ein gewisses Maß von Plug and Play realisierbar.\n\nDurch \"Schreiben\" auf diese Adresse konnte man Module aktivieren und konfigurieren:\n\n\n\nAls Monitor kam häufig das Schwarz-Weiß-Fernsehgerät Junost-402B zum Einsatz. Das Gerät verfügte nur über einen Antenneneingang.\nDie Bildqualität war sehr mäßig, Buchstaben waren mit störenden Geisterbildern versehen. Der HF-Ausgang lieferte keinen Ton und keinen Tonträger; der Lautstärkesteller am Fernsehgerät musste auf Linksanschlag gestellt werden, um das kräftige Rauschen zu unterdrücken.\n\nÜber einen Steckverbinder an der Rückseite waren weitere Signale verfügbar:\n\nSie erlaubten den Anschluss von Fernsehgeräten mit FBAS-Eingang (was eine mittlere Qualität ermöglichte) oder mit RGB-Eingang.\nDa die meisten Fernsehgeräte solche Eingänge aber nicht besaßen, musste man\n\nAls Massenspeicher kamen im Wesentlichen Kassettenrekorder, insbesondere im VEB Elektronik Gera hergestellte Kassettenrekorder der Marke Geracord GC 6000, GC 6010 oder GC 6020 zum Einsatz. Wichtig war vor allem die einfache Erreichbarkeit der Azimut-Justierschraube.\n\nObwohl die Aufzeichnungsfrequenzen mit 600 Hz bis 2.400 Hz recht niedrig waren (man kann das Signal per Telefon übertragen), so war die Azimutjustierung verschiedener Geräte häufig recht unterschiedlich. Ein weiteres Problem war die Drop-out-Rate von Kassetten aus dem VEB ORWO Wolfen.\n\nEs gab einige Ansätze, dies zu beschleunigen:\n\nDie KC 85/2-4 konnte in Maschinensprache und (vor dem KC 85/3 nur mit einem Zusatzmodul oder RAM-BASIC von Kassette) auch mit einem BASIC-Dialekt programmiert werden, der deutlich reichhaltiger war als etwa das BASIC im C64. Im Gegensatz zu den meisten Heimcomputern startete das System immer mit dem Betriebssystem CAOS (eher ein besserer Monitor); BASIC musste aus diesem Monitor explizit aufgerufen werden, sofern es überhaupt im ROM vorhanden war. Die Monitorkommandos konnten durch Assemblerprogrammierer sehr einfach erweitert werden.\n\nWeitere, aber wenig verbreitete Programmiersprachen für die Kleincomputer waren Pascal und Forth. In den letzten Jahren der DDR wurde ein Diskettenaufsatz für diese Rechner gebaut. Damit konnte dann auch CP/M (Mühlhausens Name dafür: „MicroDOS“) und Software dafür benutzt werden.\n\nAuch für die KC 85/2-4 gab es die Programmiersprache BASICODE. Sie ist ein für eine Reihe von Computern kompatibler BASIC-Dialekt, wobei Programme für BASICODE auch im Rundfunk übertragen wurden.\n\nEs standen im Wesentlichen zwei Textverarbeitungsprogramme zur Verfügung\n\nIm Wesentlichen gab es zwei Kategorien von Spielen:\n\nBeliebte Eigenbauprojekte waren/sind:\n\nDas CAOS-Betriebssystem erfährt regelmäßig Weiterentwicklung. Auch gibt es für das mit der Diskettenerweiterung gelieferte CP/M neu entwickelte Versionen.\nAn einer grafischen Bedienoberfläche wird gearbeitet.\nAußerdem gab es Modellversuche, industrielle Steueraufgaben (Speicherprogrammierbare Steuerung, SPS) für Lehrzwecke mit den Kleincomputern zu realisieren.\nAuf der Breakpoint 2009 wurde der KC 85/4 für eine Demo benutzt.\n\nDie Leiterplatte des KC 85/4 war zum Fertigungszeitpunkt bereits für die Nutzung leistungsfähigerer Speichertypen vorbereitet. Der Einsatz dieser Speicher wurde aber bis zur Einstellung der Serie vom Hersteller nicht mehr realisiert. Die verbauten 64Kbit-DRAMs können somit aber leicht gegen 256Kbit-Typen ausgetauscht werden. Auch die CAOS- und BASIC-ROMs können durch größere Typen mit weiterentwickelten Programmversionen ersetzt werden, ohne dass an der Originalhardware weitere Änderungen notwendig sind. Ein so aufgerüsteter KC 85/4 wird gemeinhin als KC 85/5 gezeichnet.\n\n\n\n"}
{"id": "129358", "url": "https://de.wikipedia.org/wiki?curid=129358", "title": "Skype", "text": "Skype\n\nSkype [] ist ein im Jahr 2003 eingeführter, kostenloser Instant-Messaging-Dienst, der seit 2011 im Besitz von Microsoft ist. Unterstützt werden Videokonferenzen, IP-Telefonie, Instant-Messaging, Dateiübertragung und Screen-Sharing. Der Dienst lässt sich sowohl mit dem zugehörigen Anwendungsprogramm nutzen, das für viele Betriebssysteme angeboten wird, als auch unter web.skype.com über einen Browser. Die Datenübertragung basiert auf einem proprietären Netzwerkprotokoll.\n\nSkype ermöglicht das kostenlose Telefonieren zwischen Skype-Kunden via Internet. Internettelefonate mit Kunden anderer Online-Dienste sind nicht möglich. Verbindungen ins Telefon-Festnetz und zu Mobiltelefonen sind gegen Gebühren möglich („SkypeOut“). Um Anrufe aus dem herkömmlichen Telefonnetz entgegenzunehmen, kann eine Festnetztelefonnummer erworben werden („SkypeIn“). Die Einrichtung einer solchen Rufnummer ist für rund 25 Länder möglich, ohne physisch in diesen Ländern anwesend zu sein. Für einige Länder ist aus rechtlichen Gründen der Nachweis des Wohnsitzes (aber nicht der tatsächlichen Anwesenheit dort) erforderlich; das betrifft gegenwärtig Frankreich, Deutschland, die Niederlande, die Schweiz und Südkorea. In der aktuellen Version für Microsoft Windows und Mac OS X sind Konferenzschaltungen mit bis zu 25 Gesprächsteilnehmern möglich.\n\nDie Software arbeitet hinter den meisten Firewalls und NAT-Routern problemlos, da für die Kommunikation unter anderem eine Variante des STUN-Protokolls zur Verbindung verwendet wird. Darüber hinaus kann Skype die TCP-Ports 80 und 443 zum Verbindungsaufbau verwenden, die normalerweise für das Surfen im World Wide Web Verwendung finden.\n\nZur Datenkompression verwendet Skype die Codecs SVOPC (16 kHz), AMR-WB (16 kHz), G.729 (8 kHz), G.711, früher auch ISAC und ILBC. Seit 2009 kommt der hauseigene Audio-Codec SILK zum Einsatz, dessen Quelltext im März 2010 bei der IETF eingereicht wurde.\n\nSkype arbeitet dabei mit jeder Standard-Headset- oder Mikrofon-Lautsprecher-Konfiguration – es wird das im Betriebssystem eingebundene Gerät erkannt.\n\nBei Verbindungen von Gerät zu Gerät wird laut Skype eine Verschlüsselung mit AES-256 verwendet, die Schlüssel würden mit 1.536 bis 2.048 bit RSA übertragen. Da es sich aber um ein Closed-Source-Programm handelt, können diese Angaben nicht von jedermann überprüft werden. Sie sind jedoch durch eine von Skype beauftragte \"Security Evaluation\" des renommierten Kryptologieexperten Tom Berson am 18. Oktober 2005 bestätigt worden. Kritik äußerte dagegen der Entwickler von \"Pretty Good Privacy\", Phil Zimmermann, im Zusammenhang mit Plänen für eine eigene VoIP-Software: Zfone. Seit Mitte 2010 sind die Verschlüsselungsmethoden von Skype auch öffentlich enttarnt. Durch Reverse Engineering wurde ermittelt, dass neben AES-256 drei Varianten von RC4 verwendet werden, deren Implementierungen angreifbar sind. Skype konnte schon zuvor durch Ermittlungsbehörden abgehört werden.\n\nDa das IP-Telefonie-Protokoll von Skype proprietär ist, kann es nur mit der originalen Skype-Software genutzt werden. Über die Skype-Programmierschnittstelle können jedoch auch externe Programme auf die Funktionalitäten des Skype-Clients und Teile des Netzwerkes zugreifen. Das wird unter anderem vom SAM-Anrufbeantworter genutzt. Neben dem Telefonieren ist das Haupt-Einsatzgebiet von Skype das Instant Messaging, wobei auch Chats mit mehreren Teilnehmern möglich sind, sowie das Übertragen von Dateien. Weiterhin existieren zum Beispiel Skype-Plug-ins für Adium, Miranda IM, Pidgin und Trillian, die die gemeinsame Nutzung von Skype mit anderen Protokollen in einem einzigen Instant Messenger ermöglichen. Allerdings muss dafür das Skype-Hauptprogramm im Hintergrund laufen.\n\nUrsprünglich war die Struktur von Skype als dezentrales Peer-to-Peer-Rechnernetz (FastTrack) angelegt. So wurden früher Verbindungen von Gerät zu Gerät teilweise über Rechner anderer Skype-Teilnehmer weitergeleitet, die dabei als \"supernode\" agierten. Aus Gründen der Fehleranfälligkeit und da mobile Geräte dafür schlecht geeignet sind, hat sich Microsoft entschieden, die Architektur zu ändern und eigene dedizierte Server einzusetzen. Gespräche ins Festnetz werden über speziell dafür vorgesehene Rechner abgewickelt. Bei Telefongesprächen in das Fest- oder Mobilfunknetz können die SIP-Dienste anderer Provider nur über die „SkypeOut“-Funktion genutzt werden. Mittlerweile ist Microsoft von dedizierten Servern komplett auf die hauseigene Cloud-Plattform Windows Azure migriert.\n\nSkype kann auch von einem Wechseldatenträger, wie beispielsweise einem USB-Massenspeicher, betrieben werden.\n\nDer Skype Translator ermöglicht es, simultan Videochats und Telefonate mittels maschineller Übersetzung in Echtzeit zu übersetzen. Verfügbar sind zurzeit acht Sprachen für Sprachanrufe und über 50 Sprachen für den Textchat. Im Mai 2014 gab Skype bekannt, an einer Übersetzungsfunktion für die Sprachen Englisch und Spanisch zu arbeiten. Später konnten sich Nutzer für eine Textversion \"registrieren\". Mitte Mai 2015 wurde die Funktion für jeden Nutzer freigeschaltet und es standen die vier Sprachen Englisch, Mandarin, Italienisch und Spanisch zur Verfügung. Anfangs war die Funktion nur auf den neueren Windowsversionen nach Windows 7 verfügbar, wurde aber später auch in die Windows-App integriert und später für Handy- und Festnetztelefonate freigeschaltet. Seit September 2018 können mit Call Recording Skype-Anrufe in den Apps und dem Desktop-Tool aufgenommen, abgespeichert und geteilt werden.\n\nSkype Technologies wurde im Juli 2003 von dem schwedischen Unternehmer Niklas Zennström und dem dänischen Unternehmer Janus Friis in Luxemburg gegründet. Sie erhielten Unterstützung durch die Draper Investment Company. Die Software selbst (in der Entwicklung noch als Skyper 1.0 bezeichnet) wurde von den Esten Ahti Heinla, Priit Kasesalu und Jaan Tallinn entwickelt. Dieselben Entwickler haben zusammen mit Niklas Zennström und Janus Friis bei der Filesharing-Software Kazaa mitgewirkt.\n\nEiner der ersten Namen für das Projekt war „Sky peer-to-peer“, was zu „Skyper“ verkürzt wurde. Da diese Domain bereits vergriffen war, wurde das \"r\" am Ende weggelassen und am 24. April 2003 die Domains skype.com und skype.net reserviert.\n\nBis zum 3. Dezember 2003 gab es von der Website bereits 3.355.593 Downloads der Software und die Anzahl stieg bis zum 31. Dezember 2004 auf 46.843.528 Downloads.\n\nIm September 2005 kaufte eBay Skype für 3,1 Milliarden US-Dollar. Davon wurden 2,6 Milliarden US-Dollar sofort gezahlt, der Rest als Aktien, verteilt auf die Jahre 2008 und 2009. Für die weiteren Zahlungen mussten allerdings bestimmte finanzielle Ziele erreicht werden. Im September 2009 bestätigte eBay den Verkauf von 65 % der Aktienanteile zu 1,9 Mrd. US-Dollar in bar an die Investmentgesellschaft Silver Lake.\n\nIm Mai 2011 bekundeten Facebook, Google und Microsoft Interesse an einer Übernahme von Skype. Den Zuschlag erhielt Microsoft mit einem Kaufpreis von 8,5 Milliarden US-Dollar, dem Zehnfachen des Jahresumsatzes von Skype. Das war die bisher teuerste Übernahme in der Geschichte von Microsoft. Im Jahr 2016 übernahm Microsoft LinkedIn für 26 Milliarden Dollar. Im August 2011 hat Skype wiederum den Dienstleister GroupMe übernommen, der sich auf Konferenzgespräche über Skype spezialisiert hatte.\n\nAm 14. Oktober 2011 gab Microsoft bekannt, dass die 8,5 Mrd. US-Dollar teure Übernahme von Skype abgeschlossen wurde. Somit ist Skype nun eine 100-prozentige Tochtergesellschaft von Microsoft.\n\nWie am 6. November 2012 angekündigt, hat Skype den Windows Live Messenger vollständig ersetzt.\n\nSeit August 2017 ist es möglich, mit der Funktion „Geld senden“ während eines Chats Geld mit PayPal zu überweisen. Sowohl der Absender als auch der Empfänger müssen aus einem der Länder stammen, in denen „Geld senden“ unterstützt wird. Dieser Dienst wurde in den USA, Kanada, Großbritannien, Österreich, Belgien, Zypern, Estland, Spanien, Finnland, Frankreich, Deutschland, Griechenland, Irland, Italien, Lettland, Luxemburg, Malta, Niederlande, Portugal, San Marino, Slowakei und Slowenien gestartet. Es kann Geld in den Währungen US-Dollar, kanadischer Dollar, britisches Pfund und Euro gesendet werden. Weitere Währungen und Länder sollen demnächst verfügbar sein.\n\nDer Skype-Chat dient in erster Linie dem Schreiben von Textnachrichten, die aber auch formatiert werden können. Zusätzlich gibt es die Möglichkeit, Emoticons, sogenannte Mojis oder Dateien zu versenden. Dabei ist es, wie auch beim Telefonieren über Skype möglich, einer Einzelperson oder einer Konversation eine Text-, Datei- oder Kontaktnachricht zukommen zu lassen.\n\nIn Skype kann man animierte Emoticons verschicken. Diese sind sehr beliebt. Man kann sie entweder per Klick im Emoticon-Menü einfügen, oder den dazugehörigen Code einfügen (bei dem Emoticon „Grinsen“ zum Beispiel „(smile)“). Einige Kurzschreibweisen wurden ebenfalls übernommen, die nicht in Klammern geschrieben werden. Eine Liste kann auf der Skype-Internetseite eingesehen werden.\n\nUm zu verhindern, dass fälschlicherweise aus Sonderzeichenkombinationen Emoticons interpretiert werden, lässt sich der Text in einer Monospace-Formatierung darstellen: Dazu muss entweder ein doppeltes Ausrufezeichen (codice_1) in Verbindung mit einem Zeilenumbruch oder einem Leerzeichen an den Beginn der Nachricht oder jeweils das Wort code in geschweiften Klammern (codice_2) an den Beginn und das Ende des zu entformatierenden Bereichs geschrieben werden. Dies ermöglicht zum Beispiel das Übermitteln von Programmcode, da unter anderem Laufwerksbezeichner wie codice_3 unter Windows in der ursprünglichen Darstellung erhalten bleiben.\n\nEs werden laufend neue Emoticons hinzugefügt. Dabei handelt es sich nicht mehr nur um Gesichter, sondern auch um Handgesten, Tiere und Gegenstände. Seit etwas längerer Zeit ist es auch möglich, kleine, durch Skype vorgeschlagene Videos zu versenden (sog. Mojis). Dabei handelt es sich in den meisten Fällen um sehr kurze Ausschnitte aus Kinofilmen, Fernsehsendungen oder kurze Musikvideos.\n\nSkype unterstützt einige besondere Formatierungszeichen, um beispielsweise kursiven oder fettgedruckten Text darzustellen. Emoticons werden dabei nicht verändert.\n\nDazu hier eine kleine Auswahl:\n\nDurch die Verwendung der Monospace-Darstellung werden sämtliche anderen Formatierungen ignoriert. Der Text innerhalb des in Monospace dargestellten Bereichs kann nicht durch umgebende Formatierungszeichen verändert werden. Die Formatierung kann unabhängig von den Emoticons durch das Schreiben von codice_4 und einem Leerzeichen oder Zeilenumbruch vor den Text verhindert werden. Dabei wird keine Monospace-Darstellung erzeugt und Emoticons werden weiterhin dargestellt.\n\nHyperlinks des Protokolle HTTP und HTTPS werden automatisch erkannt und von Skype im klassischen Blau angezeigt. Die Links werden dabei nicht auf ihre Gültigkeit geprüft. Ist im HTML-Code ein entsprechendes Bild zur Anzeige hinterlegt, wird dieses Bild mit der Adresse des Links zusammen angezeigt. Wird der Link dabei innerhalb eines Textes eingefügt, so wird er einmal in Textform im Text selbst und einmal als Bild unterhalb des Textes angezeigt.\n\nMit Skype können Dateien jeglichen Typs versendet werden, deren Größe kleiner als 300 Megabyte (MB) ist. Für größere Dateigrößen verweist Microsoft auf seinen Dienst OneDrive. Die Dateien werden mit einem bestimmten Symbol dargestellt. Das angezeigte Symbol wird dabei nicht vom installierten Standardprogramm des jeweiligen Dateityps, sondern durch Skype selbst bestimmt. Von Bildern bestimmter Bildformate wie zum Beispiel PNG und JPEG wird im Chatverlauf eine Vorschau angezeigt.\n\nEs besteht außerdem die Möglichkeit, eigene Skype-Kontakte zu verschicken. Das Senden von Kontakten kann durch einfaches Ziehen des entsprechenden Kontaktes in die Skypechat-Eingabezeile bewirkt werden. Dabei spielt es keine Rolle, ob mit dem Kontakt bereits die weiteren Kontaktdaten ausgetauscht wurden.\n\nBis Ende 2007 erschien alle zwei Wochen donnerstags eine neue Version. Seither sind die Versionsschritte deutlich länger.\n\nEs gibt mehrere verschiedene Skype-Versionen:\n\nUnter Windows 8/8.1 kann im Desktop-Modus auch die Skype-„Desktop“-Version genutzt werden. Für die Modern-UI-Oberfläche gab es einen anderen Entwicklungspfad, der mittlerweile eingestellt wurde. Versionsinformationen, siehe Tabelle.\n\nVersion 1.0 (18. März 2005) führte Unterstützung für den Dienst \"SkypeIn\" und den Anrufbeantworter ein. Ab Version 2.0.0.63 wird auch Videotelefonie unterstützt. Am 27. August 2009 wurde die Version 2.1.0.47 freigegeben, die auch direkte PulseAudio-Unterstützung bietet. Ein 64-Bit-Paket für die Linuxdistribution \"Ubuntu\" wird zwar angeboten, es enthält jedoch nur ein auf 64-Bit-Ubuntu installierbares 32-Bit-Skype.\n\nIm Juli 2016 veröffentlicht Skype eine neue Alpha-Version unter dem Namen \"Skype for Linux Alpha\", die im Wesentlichen auf \"Skype for Web\" (web.skype.com) basiert. Die Software ist als DEB- und RPM-Paket verfügbar.\n\nAnfang März 2017 wurde \"Skype for Linux\" 5.0 als Beta-Version freigegeben, das nun geteilte Desktops und Anrufen von Festnetznummern unterstützt. Die alte Version 4.3 soll nicht mehr auf alle Skype-Funktionen zugreifen können.\n\nWeitere Versionsinformationen, siehe Tabelle.\n\nSeit 2005 wird auch Mac OS X unterstützt. Am 14. Oktober 2010 wurde die Version 2.8.0.851 (Mac OS X) freigegeben. Skype hat unter OS X die Versionen 3.x und 4.x übersprungen. Versionsinformationen, siehe Tabelle.\n\nMit der Handheld-Konsole PlayStation Portable von Sony Interactive Entertainment war es möglich, auf einen eingeschränkten Skype-Dienst zuzugreifen. Verfügbar war die Applikation auf dem Modell der PSP-2000 (PSP Slim & Lite), PSP-3000 (PSP Slim & Lite) und der PSP-N1000 (PSP Go).\n\nMit der Veröffentlichung des Firmware Update 3.90 (2008), wurde erstmals die PSP-2000 (PSP Slim & Lite) mit dem integrierten Skype-Client ausgerüstet. Die ältere PSP-1000 Version blieb von dem Update außen vor, da der nur halb so große Hauptspeicher von 32 MB nicht mit Skype kompatibel ist.\n\nSkype für die PSP verfügt über die folgenden Funktionen:\n\nSkype ist für Microsoft Windows, OS X, Linux, das Apple iOS, Android, Symbian, Maemo, MeeGo, Windows Mobile, Windows Phone und Blackberry verfügbar.\nLaut Skype-Blog waren am 28. März 2011 erstmals mehr als 30 Millionen Benutzer zugleich „online“, d. h. über Skype erreichbar, im Oktober 2012 waren es rund 45 Millionen. Die Software wurde laut Skype bereits über 600 Millionen Mal heruntergeladen (Stand 1. Januar 2014).\n\nEinige Mobilfunkanbieter, beispielsweise die deutsche Telekom, blockieren Voice-over-IP-Telefonie und Videokonferenzübertragungen nicht nur von Skype in ihren Mobilfunknetzen, wenn kein Tarif oder keine Tarifoption gebucht ist, die diese Funktionen ausdrücklich zulässt. Kritiker führen dies als Verstoß gegen die Netzneutralität an, da beispielsweise große Datenströme von Videostreamingportalen nicht blockiert werden und der Kunde i. d. R. ein gewisses Hochgeschwindigkeitskontingent in seinem Datentarif bezahlt hat.\n\nVerschiedene Hersteller vertreiben Hardwareprodukte, die für VoIP-Einsatz mit Skype entwickelt wurden. Dabei handelt es sich meist um Schnurlostelefone, deren Basisstation zum einen an das analoge Telefonnetz (PSTN) angeschlossen werden können, zum anderen aber auch eine Schnittstelle (in der Regel USB) zur Verbindung mit einem PC bereitstellen. Vom Mobilteil aus können Gespräche in beiden Netzen geführt werden. Eine \"Stand-Alone\"-Nutzung, die eine Vermittlung von Gesprächen ins Skype-Netz ermöglicht, ohne dass ein zusätzlicher Rechner mit laufender Skype-Applikation angeschlossen sein muss, ist auch möglich.\n\nFür das kabellose und rechnerfreie Telefonieren werden seit Herbst 2006 von Skype Technologies lizenzierte Mobiltelefon-ähnliche Endgeräte unter der Bezeichnung „Wi-Fi Phone“ angeboten, in denen Skype-Software integriert ist und die sich unmittelbar an WLANs anmelden können. Genutzt werden können freie und passwortgeschützte offene Funknetze wie Hot Spots oder private Netze, nicht jedoch solche Netze, für deren Zugang eine Anmeldung \"in einem Browserformular\", einem sogenannten Captive Portal, nötig ist (wie z. B. viele Universitätsfunknetzwerke). Das vom Telefon abgefragte Passwort bezieht sich nur auf die eventuell vorhandene WEP/WPA-Verschlüsselung.\n\nAnfang 2006 schlossen sich 15 große Mobilfunkanbieter aus Europa und Asien zusammen, um gemeinsam gegen Instant-Messaging-Services wie Skype in Mobilfunknetzen vorzugehen und einen eigenen Instant-Messaging-Service auf Basis von IMPS zu erstellen. Umgekehrt versuchte Skype in den USA seit einiger Zeit die Mobilfunknetzbetreiber daran zu hindern, die Nutzung von Skype auf dem Handy zu blockieren.\n\nAm 24. April 2008 gab Skype die Unterstützung von ca. 50 Java-fähigen Mobiltelefonen bekannt. Außerdem wird Skype auf der PlayStation Portable unterstützt, allerdings ist dafür ein Mikrofon notwendig. Seit dem 25. April 2012 steht Skype auch für die PlayStation Vita bereit. Bereits seit 2006 ist die Skype-Software in abgewandelter Form (sogenannter Client) für Smartphones mit Windows Mobile verfügbar.\n\nFür Android-Smartphones ab der Betriebssystem-Version 2.1 „Eclair“ gibt es seit dem 5. Oktober 2010 eine offizielle Skype-Version.\n\nBei der Entwicklung von Skype-Software für Mobiltelefone arbeitet Skype mit dem Unternehmen iSkoot zusammen, die seit Mitte 2006 einen von Skype lizenzierten Client für Symbian und Java herstellt. Dabei wird die Verbindung im Mobilfunknetz als normaler Sprachanruf realisiert; erst durch einen Proxy in der Vermittlungsstelle des Mobilfunknetzes wird die Verbindung zwischen Mobilfunknetz und Skype-Netzwerk hergestellt. Damit werden die teilweise höheren Latenzzeiten bei Datenübertragungen in Mobilfunknetzen umgangen. Der Service ist derzeit in 41 Ländern in Nordamerika, Europa und Asien verfügbar. Das geschieht teilweise in Zusammenarbeit mit Mobilfunkanbietern, die die Proxys betreiben und die Verbindung im Mobilfunknetz kostenlos oder gegen eine Monatspauschale zur Verfügung stellen. Auch der Mobilfunkanbieter Drei bietet eine auf iSkoot basierende Software für die Skype-Nutzung am Handy an.\n\nMittlerweile werden auch SkypeIn und SkypeOut unterstützt. Zu diesem Zweck wird eine Sprachverbindung zu einem eigenen iSkoot-Festnetzanschluss im jeweiligen Land aufgebaut und das Gespräch von dort über den Skype-Benutzer weitervermittelt. Es entstehen Gebühren für den Aufbau der Sprachverbindung zum Festnetz und für SkypeOut; Daten werden dabei kaum übertragen, da es sich nicht um eine VoIP-Verbindung handelt.\n\nAndere Hersteller wie Nimbuzz und Fring boten ebenfalls Möglichkeiten, Skype auf dem Mobiltelefon zu nutzen, so unterstützte Nimbuzz Skype auf Blackberry und auf Android Smartphones (Unterstützung von Skype wurde im Oktober 2010 auf Anfrage von Skype Technologies eingestellt). Anders als bei der iSkoot-Lösung wurde hier die Sprachübertragung auch im Mobilfunknetz als Datenübertragung geführt, benötigte also eine höhere Bandbreite bzw. lieferte eine schlechtere Sprachqualität und erzeugte je nach Tarif entsprechend andere Kosten. Auch hier wurde ein Proxy verwendet, um die Verbindung zwischen Client und dem eigentlichen Skype-Netz herzustellen.\n\nDas Nokia N900, das werksseitig als Betriebssystem Maemo 5 verwendet, verfügt über eine integrierte Skype-Funktionalität, wodurch keine zusätzliche Client-Software mehr nötig ist. Das umfasst sowohl die reine Chat-Funktion als auch Skype-Anrufe mit Videotelefonie. Das 2011 erschienene Nokia N9 unterstützt Skype ebenso, jedoch nur ohne Videoanrufe, Sprachnachrichten und Dateiübertragungen Beide unterstützen Skype durch eine Telepathy Tube.\n\nSeit dem 30. Dezember 2010 bietet Skype für iOS-Geräte von Apple auch Videotelefonie an. So lassen sich mit dem iPhone 4, dem iPhone 3GS und dem iPod touch der vierten Generation sowie dem iPad der zweiten Generation Videotelefonate in beide Richtungen führen. Das iPhone 3G und das erste iPad können aufgrund der Leistung bzw. der fehlenden Kamera lediglich Video empfangen, jedoch auch weiterhin Audiosignale senden und empfangen.\n\nAuf Geräten mit HP webOS 3.0 sind die Skype-Funktionen wie Videotelefonie und Instant Messaging direkt in die Telefon- und SMS-App integriert und umfangreich mit dem Adressbuch vernetzt.\n\nSeit dem 27. Februar 2012 ist auch für Microsofts eigenes Handy-Betriebssystem „Windows Phone 7“ eine offizielle Skype-App verfügbar. Skype wurde somit bisher nicht in das Betriebssystem selbst integriert.\n\nAm 29. Oktober 2007 stellte Skype sein eigenes Handy vor, das \"3 Skypephone\". Seine „Skype-Taste“ erlaubt kostenlose Skype-Anrufe und Skype-Chat durch einen iSkoot-Clienten. Das Handy wurde gemeinsam mit dem Mobilfunkanbieter 3 entwickelt und anscheinend zunächst exklusiv über diesen vertrieben. Es erhielt die Auszeichnung „Global Mobile Award“ in der Kategorie „Bestes Handy“ im Rahmen des „Mobile World Congress“ in Barcelona. Es wurde nur in Österreich, Großbritannien, Italien, Dänemark, Irland, Schweden, Hongkong und Australien verkauft.\n\nSeit dem 19. Mai 2010 bietet Skype gegen eine monatliche Pauschalgebühr über den Skype Manager die Möglichkeit, einen Skype-Connect-Account anzulegen, der eine Anbindung von SIP-Telefonanlagen wie beispielsweise Asterisk an Skype ermöglicht. Um Anrufe auf die SIP-Telefonanlage entgegennehmen zu können, muss zumindest eine Leitung freigeschaltet sein. Laut Skype ist es allerdings nicht möglich, andere Skype-Benutzer über die SIP-Telefonanlage anzurufen.\n\nDie Linux-Version von Skype war seit Juni 2014 für über zwei Jahre nicht mehr aktualisiert worden. Dadurch fehlten Funktionen (z. B. Video-Konferenzen), die in der Windows-Version enthalten waren oder funktionierten nicht mehr zuverlässig. In jüngerer Zeit wurde eine neue Version auf Basis von WebRTC vorgestellt, der jedoch noch einige Funktionen fehlen.\n\nSkypecasts waren Gruppenchats, auf die jeder zugreifen konnte, die aber von Moderatoren kontrolliert wurden. Sie sollten dem weltweiten Meinungsaustausch dienen, wurden allerdings am 1. September 2008 eingestellt, da diese laut Skype die hohen Standards und Erwartungen an eine Kommunikationslösung nicht uneingeschränkt erfüllen können.\n\nSkype bietet zwar die Verschlüsselung bei direkten Gesprächen zwischen Benutzern an, was für Kriminelle und Strafverfolgungsbehörden erhebliche Probleme schafft, jedoch gab es in der Vergangenheit Spekulationen über mögliche Hintertüren, die von staatlichen Behörden zum Abhören von Gesprächen genutzt werden könnten: Im Jahre 2008 wurde bekannt, dass österreichische Behörden und Polizei Skype abhören können.\n\n2009 warf die europäische Behörde zur Koordinierung grenzüberschreitender Strafermittlung (Eurojust) Skype vor, das Abhören von VoIP-Telefonaten durch ein Verschlüsselungssystem zu verhindern. Dieser Vorwurf wurde zurückgenommen, als Skype seine Zusammenarbeit anbot.\n\nWie im Oktober 2010 bekannt wurde, benutzt der deutsche Zoll im Rahmen der sogenannten Quellen-Telekommunikationsüberwachung eine speziell entwickelte Software, um Inhalte von Gesprächen über Skype, noch bevor sie verschlüsselt wurden, auf einen bestimmten Server auszuleiten.\nDie Software namens \"Skype Capture Unit\" bringt sogar einen signierten Treiber (von der fiktiven \"Goose Cert\" signiert) für 64-Bit-Versionen von Windows mit.\nDamit muss die bisher angenommene erhöhte Sicherheit\n\nvon Windows in 64-Bit-Versionen als umgangen angesehen werden.\n\n2011 haben Wissenschaftler der Universität des US-Bundesstaats North Carolina ein Verfahren entwickelt, um Teile von Skype-Gesprächen zu entschlüsseln.\n\nIm Juni 2013 wurde mit PRISM ein Überwachungsprogramm des US-amerikanischen Geheimdienstes NSA bekannt, in dessen Rahmen großflächig ein beträchtlicher Teil des gesamten, durch die USA laufenden Internetverkehrs mitgeschnitten, unbefristet gespeichert und ausgewertet wird. Als Teil des an diesem Programm teilnehmenden Unternehmens Microsoft stand auch Skype bereits unmittelbar nach dem Bekanntwerden von PRISM unter dem Verdacht, die gesamte über das Netzwerk laufende Kommunikation amerikanischen Geheimdiensten zur Verfügung zu stellen, ohne dass dafür ein besonderer Grund vorliegen muss und ohne dass der Nutzer davon erfährt. Bereits Jahre bevor Skype an PRISM teilnahm, wurde laut New York Times bei Skype ein internes \"Project Chess\" eingerichtet, um zu erforschen, mit welchen technischen Mitteln Skype-Telefonate legal den Geheimdiensten und der Polizei zugänglich gemacht werden können.\n\nAm 12. Juli 2013 wurde durch von Edward Snowden veröffentlichte Informationen bekannt, dass den amerikanischen Geheimdiensten durch Microsoft tatsächlich direkter Zugriff auf den gesamten Skype-Verkehr gewährt wird und sowohl Textchats als auch Telefonate und Videotelefonate nach Belieben von der NSA mitgeschnitten und ausgewertet werden können, da es dem Geheimdienst mit Hilfe des direkten Zugriffs auf die Skype-Server möglich ist, die Skype-Verschlüsselung zu umgehen.\n\nIm Februar 2007 wurde durch einen Software-Bug bekannt, dass Skype nach dem Start im Verzeichnis für temporäre Dateien eine ausführbare Datei namens \"1.com\" anlegt, die sämtliche BIOS-Informationen des PCs auslesen kann; Skype versucht, ein Auslesen dieser Datei durch den Nutzer zu unterbinden. Nach Aussage von Skype diene diese Überprüfung dem „Skype Extras Manager“ zur eindeutigen Identifizierung von Rechnern, damit sichergestellt werde, dass lizenzpflichtige Extras nur von berechtigten Lizenznehmern installiert und betrieben würden.\n\nIm Mai 2013 deckte heise Security nach Hinweis eines Nutzers auf, dass Skype sich nicht nur die Erlaubnis, den kompletten Kommunikationsinhalt des Nutzers mitzulesen und auszuwerten, pro forma durch Akzeptieren der AGB und Nutzung des Dienstes erteilen lässt, sondern dass das lückenlose Mitlesen, Durchscannen und Überprüfen mindestens des Skype-Textchats in der Praxis auch tatsächlich durchgeführt wird. Aufgedeckt wurde dies dadurch, dass über den Skype-Chat privat versendete und speziell für diesen Zweck eingerichtete HTTPS-URLs samt Login-Daten kurze Zeit später automatisiert durch eine Microsoft zuzuordnende IP-Adresse abgerufen und überprüft wurden. Heise kommt zum Schluss:\n\nAnalysten im internationalen Umfeld warnen vor Sicherheitsrisiken für Unternehmen, die durch die Verwendung von Skype entstehen. Skype versucht beispielsweise, den Einsatz von Analyseprogrammen wie Debuggern durch Verschlüsselung zu verhindern. Auch verhindert das Programm seine Veränderung.\n\nDas Fraunhofer-Institut für Eingebettete Systeme und Kommunikationstechnik ESK warnt seit 2013 vor den Risiken einer (ungeregelten) Nutzung von Skype und rät wegen Sicherheitsbedenken von der Nutzung in Unternehmen ab.\n\nEinwohner der Volksrepublik China werden, wenn sie auf die Skype-Website zugreifen, auf die Seite eines chinesischen Partners weitergeleitet, von der eine eigene chinesische Version von Skype heruntergeladen werden kann: TOM-Skype. 2008 deckten Menschenrechtsaktivisten der Forschergruppe Citizen Lab der Universität Toronto auf, dass Skype in China Nachrichten auf politische Begriffe durchsucht und diese mitschneidet. Auch wurden in diesem Zusammenhang Mitteilungen mit persönlichen Daten, Benutzernamen, IP-Adressen oder Telefonnummern protokolliert und auf mehreren Servern von TOM, dem chinesischen Partner von Skype, gespeichert. Die Daten gerieten auch auf ungesicherte Server, die ganz einfach von außen zugänglich sind und sogar die Codes zur Entschlüsselung der Daten enthielten. Auch Nutzer der „normalen“ internationalen Versionen von Skype fallen der chinesischen Kontrolle zum Opfer, wenn sie mit einem der 70 Millionen Nutzer von TOM-Skype kommunizieren. Skype erklärte dazu, die Gesetze und Vorschriften vor Ort einzuhalten.\n\nUm Skype für Telefonate nutzen zu können, wird ein Internetzugang mit mindestens ISDN-Geschwindigkeit benötigt; ein Telefonmodem genügt nicht, da die Datenrate zu gering ist. Der Komprimierungscodec stellt sich zwar auf die geringe Sendegeschwindigkeit ein; die Verzögerungen bei der Übertragung machen ein vernünftiges Gespräch aber unmöglich.\n\nUrsprünglich verfiel das Skype-Guthaben eines Kontos ersatzlos, das mehr als sechs Monate nicht für einen kostenpflichtigen Dienst (SkypeOut, SMS, Personalisierung) genutzt wurde. Dieser Zeitraum verlängerte sich immer um sechs Monate nach der letzten Nutzung eines dieser Dienste. In einem Urteil vom 26. Januar 2006 (Az. 12 O 16098/05) hat das Oberlandesgericht München allerdings entschieden, dass Verfallsklauseln bei Handyverträgen unwirksam sind. Nach Einschätzung einiger Juristen ergibt sich hieraus, dass damit auch das Skype-Guthaben nicht verfallen darf. In der aktuellen Version (Stand: April 2013) sehen die Skype-Nutzungsbedingungen den Verfall von Guthaben (von einem Sonderfall abgesehen) auch nicht weiter vor. Stattdessen wird das Guthaben nach einer Frist von 180 Tagen Inaktivität „deaktiviert“, und kann über die Website „reaktiviert“ werden, wonach es wieder in voller Höhe zur Verfügung steht. Allerdings ist reaktiviertes Guthaben laut Nutzungsbedingungen nicht mehr rückerstattungsfähig, und Benutzer werden einige Zeit vor dem Ablauf der Frist gewarnt, dass bei weiterer Inaktivität ihr Guthaben bald verfällt, was einige ungenügend informierte Nutzer dazu veranlasst hat, Guthaben zu verbrauchen, nur um den Ablauf der Frist zu verhindern.\n\nFür Unmut bei vielen Anwendern sorgte im Frühjahr 2006 die Erweiterung der Funktion für Konferenzschaltungen in der Version 2.0 der Skype-Software, die nun zwar zehn statt fünf Teilnehmer zulässt – allerdings nur, wenn im Rechner des Konferenzleiters ein Mehrkernprozessor des Unternehmens Intel arbeitet. Die offizielle Begründung dafür war ein enormer Leistungsbedarf derartiger Funktionalität, den laut Skype und Intel nur diese Prozessoren decken könnten. Die zu jenem Zeitpunkt aktuellen Athlon-64-CPUs des Intel-Konkurrenten AMD boten jedoch keineswegs weniger Leistung, weshalb diese Kopplung der Funktionalität an bestimmte Prozessoren schnell als reiner Marketingtrick galt. Tatsächlich erschien nach wenigen Wochen ein Hack für den Skype-Client, der die 10er-Konferenzen auch auf anderen CPUs freischaltete und damit die Behauptungen von Skype als offenkundig falsch entlarvte. Diese künstliche Einschränkung existiert mit der Beta-Version 2.6.0.103 seit Oktober 2006 nicht mehr. Eine ähnliche Einschränkung gab es in Skype 3.6, die im November 2007 für Windows veröffentlicht wurde. Hier wurde eine höhere Videoqualität ausschließlich Nutzern von Webcams des Unternehmens Logitech ermöglicht.\n\n2007 wurde Skype von gpl-violations.org erfolgreich wegen Verletzung der GPL verklagt. Ein auf Linux basierendes Telefon war von Skype ohne die Quelltexte ausgeliefert worden.\n\n\n\n"}
{"id": "130868", "url": "https://de.wikipedia.org/wiki?curid=130868", "title": "Monte-Carlo-Simulation", "text": "Monte-Carlo-Simulation\n\nMonte-Carlo-Simulation oder Monte-Carlo-Studie, auch MC-Simulation, ist ein Verfahren aus der Stochastik, bei dem eine sehr große Zahl gleichartiger Zufallsexperimente die Basis darstellt. Es wird dabei versucht, analytisch nicht oder nur aufwendig lösbare Probleme mit Hilfe der Wahrscheinlichkeitstheorie numerisch zu lösen. Als Grundlage ist vor allem das Gesetz der großen Zahlen zu sehen. Die Zufallsexperimente können entweder – etwa durch Würfeln – real durchgeführt werden oder in Computerberechnungen, bei denen zur Simulation von zufälligen Ereignissen mit geeigneten Algorithmen scheinbar zufällige Zahlen berechnet werden, die auch als Pseudozufallszahlen bezeichnet werden.\n\nZu den Pionieren der Monte-Carlo-Methode in den 1940er Jahren gehören Stanislaw Ulam, Nicholas Metropolis und John von Neumann.\n\nAnwendungen der Monte-Carlo-Simulation sind beispielsweise:\n\n\nMit der Monte-Carlo-Methode kann man Probleme mit statistischem Verhalten simulieren. Diese Methode hat deshalb besonders in der Physik wichtige Anwendungen gefunden, und zwei Bücher des Autors Kurt Binder gehören zu den meistzitierten Veröffentlichungen in dieser Wissenschaftssparte.\n\n\nEnrico Fermi hatte in den 1930er Jahren die ersten Ideen zu Monte-Carlo-Simulationen. Ausgeführt wurden diese 1946 von Stanislaw Ulam und dem von ihm deshalb kontaktierten John von Neumann. Dies geschah zur Zeit des 2. Weltkriegs während der Arbeit an einem damals geheimen Projekt am Los Alamos Scientific Laboratory, für das ein Codename nötig war. Es ging im Rahmen der Entwicklung der ersten Atombombe um die Neutronendiffusion in nuklearen Materialien. Auch die mathematische Methode der Simulation musste geheim gehalten werden. Von Neumann wählte den Namen „Monte Carlo“ in Anspielung auf die Spielbank Monte-Carlo im gleichnamigen Stadtteil des Stadtstaates Monaco.\n\nMathematisch ist das System ein wahrscheinlichkeitsgewichteter Weg im Phasenraum (allgemein Zustandsraum). Monte-Carlo-Simulationen sind besonders geeignet, um statistische Mittelwerte einer Größe formula_1,\n\noder hochdimensionale Integrale (Monte-Carlo-Integration) wie\n\nzu berechnen. formula_4 soll in diesem Zusammenhang ein normiertes statistisches Gewicht (etwa ein Boltzmanngewicht) sein. formula_5 ist der Wert der Größe formula_1 im Zustand formula_7. Die Summation bzw. Integration verläuft hier über einen Raum formula_8, also der Phasenraum der Teilchen im System.\n\nHäufig ist der Raum formula_8 so groß, dass die Summation nicht vollständig durchgeführt werden kann. Stattdessen erzeugt man nun eine Markow-Kette formula_10 von Zuständen in formula_8, deren Häufigkeit wie das vorgegebene Gewicht formula_4 verteilt ist. Bereiche des Raumes formula_8 mit hohem Gewicht sollen also häufiger in der Markow-Kette vertreten sein als Bereiche mit niedrigem Gewicht. (Man spricht hier von Importance Sampling.) Gelingt dies, so lassen sich die Erwartungswerte einfach als arithmetisches Mittel der Größe formula_1 zu diesen Zuständen der Markow-Kette berechnen, also als\n\nDieser Zusammenhang basiert auf dem Gesetz der großen Zahlen. Je nach physikalischem System kann es schwierig sein, diese Markow-Kette zu erzeugen. Insbesondere muss man sicherstellen, dass die Markow-Kette tatsächlich den gesamten Raum formula_8 bedeckt und nicht nur einen Teil des Raumes abtastet. Man sagt, der Algorithmus muss \"ergodisch\" sein.\n\nDer von Nicholas Metropolis publizierte Metropolisalgorithmus zur Untersuchung statistisch-mechanischer Systeme mittels Computersimulation leitet sich von der Monte-Carlo-Integration ab.\n\nSequentielle Monte-Carlo-Methoden eignen sich zur Bayesschen Zustandsschätzung von dynamischen Systemen. Ziel ist es, den Systemzustand als Funktion der Zeit auf Basis einer Reihe von Beobachtungen des Systems und A-priori-Kenntnissen der Systemdynamik zu schätzen. Dazu wird die komplizierte Wahrscheinlichkeitsdichte des Zustandes diskret durch eine Menge von Partikeln approximiert. Sequentielle Monte-Carlo-Methoden werden auch Partikelfilter genannt.\n\nQuanten-Monte-Carlo-Methoden werden zur Berechnung physikalischer Observablen in quantenfeldtheoretischen Modellen benutzt. Beispiele sind Modelle aus der theoretischen Festkörperphysik wie das Hubbard-Modell oder das tJ-Modell.\n\nDie kinetische Monte-Carlo-Methode erlaubt es den zeitlichen Fortschritt eines Systems zu simulieren.\n\n\n\n\n\n\n\n"}
{"id": "131418", "url": "https://de.wikipedia.org/wiki?curid=131418", "title": "Microsoft Windows Vista", "text": "Microsoft Windows Vista\n\nDas Betriebssystem Windows Vista (in romanischen Sprachen für „Aussicht, Ausblick“, ursprünglich vom Lateinischen \"videre\", „sehen“) wurde am 30. Januar 2007 von Microsoft veröffentlicht und ist der Nachfolger von Windows XP und der Vorgänger von Windows 7. Der Support wurde durch Microsoft am 11. April 2017 eingestellt.\n\nWindows Vista wurde als Nachfolger von Windows XP mit der Versionsnummer NT 6.0 entwickelt. Es wurde als erstes Microsoft-Betriebssystem vollständig nach Microsofts Security Development Lifecycle produziert. Erstmals wurde \"Windows Longhorn\" (der interne Arbeitsname von Windows Vista), noch vor der Veröffentlichung von Windows XP, von Microsoft im Sommer 2001 angekündigt. Damals war es als Zwischenschritt zwischen XP und der übernächsten Version (damals \"Blackcomb\") gedacht und sollte bereits Ende 2003 erscheinen. Dies verzögerte sich jedoch immer weiter, sodass die Entwicklungsarbeit an der Version für Unternehmen am 8. November 2006 abgeschlossen wurde, während die Entwicklung der Version für Privatanwender noch einige Wochen weiter lief. Verfügbar wurde Windows Vista im November 2006 für Unternehmen und Entwickler bzw. am 30. Januar 2007 für Privatkunden.\n\nAufgrund der Verzögerungen hatte Microsoft kurzfristig auch \"Shorthorn\" in Planung, eine Zwischenversion zwischen Windows XP und Longhorn. Diese Zwischenversion wurde dann aber verworfen, um Longhorn schneller fertigzustellen. Einige Eigenschaften von Shorthorn wurden in das Service Pack 2 für Windows XP, andere in Vista integriert.\n\nEiner der Gründe für die Verzögerungen sind viele Funktionen, die ursprünglich erst für \"Blackcomb\" geplant waren, jedoch schon in Vista implementiert wurden. Andererseits fielen zunächst vorgesehene Funktionen und Neuerungen weg, so wurde die eigentlich für Vista geplante assoziative Dateiverwaltung WinFS erst auf einen Zeitpunkt nach Veröffentlichung von Vista verschoben, um, so Microsoft, die Entwicklung von Vista in einer „vernünftigen Zeitspanne“ abzuschließen, später dann aber ganz aufgegeben. Auch war ursprünglich geplant, unter dem Namen Next-Generation Secure Computing Base in Windows Vista eine Trusted-Computing-Umgebung einzuführen. Als Folge von Entwicklungsproblemen und massivem Protest wurde dieser Plan jedoch wieder verworfen.\n\nVom 30. August 2005 an veröffentlichte Microsoft monatliche „Community Technical Previews“ für MSDN-Abonnenten und ausgewählte Betatester, die am 8. Juni 2006 durch einen öffentlichen Betatest, genannt „Customer Preview Program“, abgelöst wurden, bei dem ein Download der Entwicklungsversion 2 von Windows Vista möglich war. Am 1. Juli 2006 wurde die Downloadmöglichkeit wieder eingestellt, da laut Microsoft die Anzahl an „Testern“ (= Registrierungen) groß genug war. Auch der erste Release Candidate (RC1) von Windows Vista war ab dem 6. September 2006 öffentlich verfügbar. Insbesondere waren Neuerungen im Bereich der Sicherheit zu finden, außerdem wurden Fehler bereinigt. Am 7. Oktober 2006 folgte der „Release Candidate 2“ (RC2, Build 5744) als letzte öffentliche Version.\n\nWeitere Versionen wurden hauptsächlich intern getestet. Bekannt ist insbesondere noch der Build 5808, der am 22. Oktober 2006 an einige Tester verteilt wurde. Der Versionssprung könnte durch das Hinzufügen der letzten unveröffentlichten Funktionen (Systemtöne, Startlogo etc.) zu erklären sein.\n\nAm 8. November 2006 wurde die endgültige Version für Unternehmen erstellt. Sie trägt, wie die Version für Privatkunden, die Buildnummer 6000.\n\nNach einer Schätzung des amerikanischen Wirtschaftsmagazins \"BusinessWeek\" hatte Microsoft fünf Jahre lang rund 10.000 Angestellte für das Projekt eingesetzt und etwa 10 Milliarden US-Dollar in die Entwicklung investiert. Microsoft selbst beziffert die Kosten für die Entwicklung auf 6 Milliarden Dollar.\n\nDie \"Washington Post\" berichtete 2007, dass Microsoft – ähnlich wie Apple bei der Entwicklung von Mac OS X – mit zahlreichen US-Behörden, darunter auch mit der NSA, und internationalen Einrichtungen zusammengearbeitet hat, die als Microsoft-Großkunden an Test und Entwicklung des neuen Betriebssystems mitwirken wollten.\n\nWindows Vista war in sechs verschiedenen Editionen erhältlich, die sich in Produktumfang und Preis unterschieden: als Starter Edition (lediglich für Schwellenländer), Home Basic (für Wachstumsmärkte und Netbooks), Home Premium (inklusive Media Center – für den breiten Markt gedacht), Business (für mittlere und kleinere Unternehmen konzipiert), Enterprise (Volumenlizenzen für Geschäftskunden) und als Ultimate (vereint alle Funktionen der anderen Versionen, ausgen. Starter).\n\nAlle Versionen befinden sich auf derselben DVD und die Installationsroutine erkennt anhand des Produktschlüssels, welche Version installiert werden soll.\n\nDer \"Mainstream-Support\" endete am 10. April 2012. Ursprünglich sollten nur die für den Unternehmenseinsatz gedachten Business- und Enterprise-Versionen den dann folgenden \"Extended Support\" erhalten, bei dem Microsoft weiterhin Sicherheitsupdates bietet. Am 20. Februar 2012 gab Microsoft jedoch bekannt, dass alle Windows Vista-Versionen diesen \"Extended Support\" bis zum 11. April 2017 erhalten.\n\nBis auf Windows Vista Starter (ehemals „Windows Starter 2007“) sind alle Versionen in einer 32-Bit- und einer 64-Bit-Version erhältlich. Windows Vista besitzt im Gegensatz zu Windows XP x64, das nur eine rudimentäre 64-Bit-Version als Zwischenlösung besaß, eine verbesserte AMD64-Unterstützung.\n\nBei Versionen für den europäischen Markt unterscheidet Microsoft (wie schon bei Windows XP) noch die „normale Edition“ und die „N-Edition“. Die EU-Wettbewerbskommission hatte Microsoft vorgeschrieben, auf eine Bündelung des Betriebssystems mit dem Windows Media Player zu verzichten, um Konkurrenten im Multimediabereich nicht zu benachteiligen.\n\nDie wohl populärste Neuerung in Vista ist \"Aero\", die neue vektorbasierte Benutzeroberfläche von Windows. Der Name leitet sich ab vom griechischen Begriff für „Luft“, und soll ein Backronym für „authentisch, energetisch, reflexiv, offen“ sein. Im sogenannten \"Aero-Glass\"-Modus bietet die Oberfläche dem Benutzer Anwendungsfenster mit Schattenwurf, halbtransparenten Rahmen sowie flüssige Animationen beim Minimieren, Wiederherstellen, Schließen und Öffnen. Diese Oberfläche ist ab der Home-Premium-Edition enthalten. Um die Funktionen optimal nutzen zu können, muss ein sogenannter „WDDM“ (Windows Display Driver Model)-Treiber für die entsprechende Hardware installiert werden. Der Mauszeiger wurde auch verändert: Bei Hintergrundaktivität oder Auslastung wird nun (statt der Sanduhr) ein animierter „Kringel“ angezeigt. Der Zeiger wurde allgemein etwas kürzer.\n\nMit Flip 3D kann man in Windows Vista zwischen mehreren Fenstern in einer 3D-Ansicht wechseln. Diese kann via Taskleiste oder mit + aufgerufen werden.\n\nNeu sind Microsofts \"Minianwendungen\". Dies sind kleine spezialisierte Dienstanwendungen, die in einer Sidebar auf dem Desktop eingebunden werden können und beispielsweise Informationen über Nachrichten, das Wetter und Ähnliches anzeigen. Die Sidebar ist in der Voreinstellung aktiviert. Im Vista Nachfolger Windows 7 wurde die Funktion leicht verändert beibehalten. Die Sidebar wurde deaktiviert, jedoch lassen sich die Minianwendungen (Gadgets) über die Systemsteuerung einzeln an beliebiger Stelle auf dem Desktop platzieren. Später (2012) erklärte Microsoft aus Sicherheitsgründen eine Abkehr von diesem Konzept und empfahl die völlige Deaktivierung von Sidebar und Gadgets.\n\nDas aus XP bekannte Programm Adressbuch wurde überarbeitet und in „Windows-Kontakte“ umbenannt.\n\nDas E-Mail-Programm Windows Mail kommt als Nachfolger von Outlook Express zum Einsatz und ersetzt dieses. Windows Mail enthält einen integrierten Spamfilter, welcher ohne Voreinstellung selbstständig Spamnachrichten ausfiltern kann und monatlich über Microsoft Update aktualisiert wird.\n\nIn Windows Vista wurde ein Programm namens Windows Kalender eingeführt. Mit diesem kann man mehrere Kalender erstellen und in diese Termine eintragen.\n\n, das erstmals mit Windows XP Tablet PC Edition ausgeliefert wurde, ermöglicht das Erstellen von Screenshots mit einer grafischen Benutzeroberfläche. Es ist nun auch möglich, einzelne Bildschirmbereiche, den gesamten Bildschirm oder ein Fenster aufzunehmen. ist für die Windows-Vista-Editionen \"Home Premium\", \"Business\", \"Enterprise\" und \"Ultimate\" verfügbar, mit Windows Vista \"Starter\" und \"Home Basic\" wird es nicht vertrieben.\n\nDas mit Windows XP eingeführte Programm \"Windows Bild- und Faxanzeige\" wurde mit Windows Vista umbenannt in \"Windows-Fotogalerie\". (Seit Windows 7 heißt es \"Windows-Fotoanzeige\".) Das Programm wurde überarbeitet und an die Aero-Oberfläche angepasst, zudem bietet es auch ein paar mehr Funktionen zur Bearbeitung oder verbesserten Darstellung von Bildern.\n\nDer in Windows XP veröffentlichte Windows Messenger wurde in Vista in den Windows Live Messenger umbenannt. Dieser wurde aus Windows auch entfernt. Er muss mit dem Softwarepaket Windows Essentials (früher Windows Live Dashboard und Windows Live Essentials) nachinstalliert werden. Das Paket gibt es kostenlos auf der Microsoft-Website. Der Link \"Windows Live Messenger Download\" leitet nun zur Skype-Website weiter, da der Messenger-Dienst eingestellt wurde.\n\nDie Suche wurde komplett überarbeitet, so werden Suchergebnisse bei laufender Suche angezeigt. Die Windows-Vista-Suche ermöglicht das Hinzufügen von mehreren Filtern, um die Suche stetig zu verfeinern (zum Beispiel „Datei enthält das Wort 'Beispiel'“), jedoch erst nachträglich. In Windows Vista ist in das Startmenü außerdem ein Suchfeld eingebaut, das zum schnelleren Start von Anwendungen dienen und das Auffinden von Dokumenten und E-Mails erleichtern soll. Suchanfragen können, wie die \"Intelligenten Ordner\" unter macOS, gespeichert werden und dann als \"virtuelle Ordner\" fungieren, indem beim Öffnen eines solchen Ordners die entsprechende Suchanfrage automatisch ausgeführt wird und deren Ergebnisse als normaler Ordner dargestellt werden. Die Vista-Suche basiert auf einer erweiterten und verbesserten Version des Indizierungsdienstes für die Windows-XP-Suche. Mit Windows Vista Service Pack 1 ist die Schaltfläche Suchen vom Startmenü entfernt worden, da diese durch das Suchfeld ersetzt wurde.\n\nEltern können festlegen, wann, wie lange und mit welchen Programmen ihre Kinder den Computer benutzen dürfen. Auch die Einschränkung des Internetzugriffs ist möglich. Schließlich beinhaltet die neue Funktion auch die Möglichkeit, die Nutzung des Computers zu überwachen. Die Software Windows Live Family Safety ermöglicht weiteres Einschränken und ist im Paket Windows Essentials (früher Windows Live Essentials und Windows Live Dashboard) enthalten. Dieses Softwarepaket gibt es auf der Microsoft Website kostenlos zum Download.\n\nMit Windows Vista wurde eine erweiterte Benutzerkontensteuerung eingeführt, die das Arbeiten mit unterschiedlichen Benutzerrechten erleichtern soll, um die Sicherheit zu erhöhen. Der Anwender arbeitet nun nach der Installation des Betriebssystems standardmäßig mit eingeschränkten Benutzerrechten. Sobald eine Anwendung administrative Berechtigungen für die Ausführung benötigt, wird ein Dialogfeld angezeigt, welches extra zu bestätigen ist, damit diese Anwendung mit Administratorrechten ausgeführt werden kann.\n\nVista enthält zudem neue Sicherheitsmaßnahmen, um das Ausnutzen von Schwachstellen des Betriebssystems oder der Anwendungsprogramme zu erschweren. Dazu gehört vor allem die Adressverwürfelung (ASLR). Mit dem 1. Service Pack wurde die \"Structured Exception Handler Overwrite Protection\" (SEHOP) eingeführt, die jedoch standardmäßig deaktiviert ist.\n\nDie Funktionen ReadyBoost und SuperFetch sollen den Start des Betriebssystems und der Programme beschleunigen. Zudem wurde das Dateisystem NTFS aktualisiert, so dass es unter anderem atomare Operationen auf Dateisystemebene unterstützt. Vista unterstützt das Lesen und Schreiben von DVD-RAM per Drag and Drop (→ Universal Disk Format) und kann WebDAV-Ordner im Explorer einbinden (Ersteres ist unter Windows XP nur mit Zusatzsoftware möglich).\n\nVista wird mit einer neuen Version der Grafikschnittstelle DirectX in der Version 10 ausgeliefert. Diese API ermöglicht eine schnellere Ausführung der Grafikfunktionen und zeichnet sich durch eine geringe Erweiterung der Effektpalette aus.\n\nDer Internet Explorer, jetzt „Windows Internet Explorer“ genannt, ist in Version 7.0 vorhanden. Weitere neue Anwendungen sind die Spiele \"Schach-Giganten\", \"Mahjongg-Giganten\" und \"Lila Land\", das Sicherungsprogramm Windows Backup (ersetzt das alte NTBackup), Windows Kalender (mit WebDAV-Unterstützung), Notizzettel, „Windows Teamarbeit“ (Kollaboration) und Windows Defender (Schutz vor Spyware).\n\nZudem gibt es mit \".NET Framework 3.0\" eine neue, auf .NET basierende Programmierschnittstelle für Windows, das Anwendungsprogrammierern Zugriff auf die neuen Funktionen von Windows Vista ermöglichen soll. Den Kern von .NET Framework 3.0 bilden die vier Bestandteile \"Windows Presentation Foundation\" (WPF), \"Windows Communication Foundation\" (WCF), \"Windows Workflow Foundation\" (WF) und \"Windows CardSpace\". Zusätzlich verwendet Vista das neu erstellte für kryptographische Aufgaben.\n\n\nDie Unterstützung von \"DirectSound-3D\"-Hardwarebeschleunigung wurde entfernt, als Ersatz wird auf eine Softwareemulation zurückgegriffen, bei der die Toneffekte von der CPU gemischt bzw. erzeugt werden. Computerprogramme, die DirectSound 3D verwenden, wie z. B. Spiele, können daher nicht mehr direkt auf die Audiohardware zuzugreifen und erweiterte 3D-Effekte der Hardware (wie zum Beispiel EAX) benutzen. DirectSound 3D als solches und \"DirectSound\"-3D-Toneffekte werden weiterhin unterstützt. Außerdem ist 3D-Beschleunigung noch durch die freie OpenAL-Schnittstelle möglich.\n\nUnter dem Namen \"XML Paper Specification\" (XPS, vormals „Metro“) ist Microsofts geräteunabhängiges Dokumentenformat integriert, das auf XML basiert. Es ist in vielerlei Hinsicht ähnlich Adobe Systems' PDF. XPS soll Benutzern ermöglichen, Dateien ohne das Originalprogramm, mit dem sie erstellt wurden, zu betrachten, zu drucken und zu archivieren. XPS dient unter Windows Vista als Standardformat für die Druckausgabe und soll in Zukunft auch von Druckern direkt unterstützt werden.\n\nWindows Vista unterstützt das Universal Disk Format (UDF). Diese Alternative zu FAT und NTFS ist vor allem für DVD-RAM von Bedeutung.\n\nDas Hilfesystem wurde unter dem Namen AP Help neu entwickelt. Dieses basiert auf XML und soll Inhalte völlig anders darstellen als von den bekannten HTMLHelp-Dateien gewohnt.\n\nDer TCP/IP-Stack wurde neu konstruiert und profitiert von mehreren physikalischen Prozessoren.\n\nIn Windows Vista wurde der Spieleexplorer eingeführt. Dort werden auf dem Computer installierte Spiele angezeigt. Es können manuell Spiele hinzugefügt werden. Es können auch Informationen zu Altersfreigabe für die Spiele abgerufen werden.\n\nIm Unterschied zur 32-Bit-Version kann die 64-Bit-Version wesentlich mehr Arbeitsspeicher verwalten und somit die bisher geltende 4-GB-Grenze überspringen (effektiv nur 3,12 GB). Mechanismen, mit denen auch die 32-Bit-Version mehr als 4 GB Arbeitsspeicher nutzen kann, wurden anders als in Microsoft Windows Server 2008 nicht ergänzt. Die von den jeweiligen Vista-Versionen maximal unterstützten Arbeitsspeichergrößen unterscheiden sich: Vista Home Basic unterstützt 8 GB RAM, Vista Home Premium 16 GB und Vista Business, Enterprise und Ultimate 128 GB RAM.\n\nFür die 64-Bit-Version wird ein Prozessor benötigt, der den x86-64-Befehlssatz unterstützt und somit 64-Bit-Code ausführt. Solche Prozessoren sind zum Beispiel alle AMD Athlon-64- und Phenom-Modelle, die 600er-Serie des Intel Pentium 4, einige Atom-Modelle und der Core 2 Duo.\n\nWindows Vista 64 kann 32-Bit- und 64-Bit-Anwendungen gleichzeitig ausführen. Dafür wird eine WOW64-Kompatibilitätsschicht verwendet. 16-Bit-Anwendungen werden hingegen nicht mehr unterstützt; das 16-Bit-Subsystem für alte MS-DOS- und Win16-Programme, welches auf dem Virtual 8086 Mode basierte, musste in der 64-Bit-Version entfallen, weil der Long-Mode-Betriebsmodus der x86-64-Architektur dies nicht mehr unterstützt. Des Weiteren müssen Treiber im 64-Bit-Format vorliegen, sowie eine digitale Signatur besitzen – dies gilt allerdings nicht für x86-64-Treiber die mittels User-Mode Driver Framework (UMDF) erstellt wurden.\n\nNeu an Vista ist das Windows Anytime Upgrade genannte Vertriebsmodell. Mit dem gleichen Datenträger können je nach Lizenznummer unterschiedliche Editionen installiert werden. Das Medium ist also nicht mehr auf eine Lizenz oder Lizenznummer zugeschnitten. Lediglich die 32-Bit- und 64-Bit-Versionen haben unterschiedliche Medien, erkennbar an der Farbe der Hüllen (grün bzw. blau).\n\nMicrosoft gibt als Systemanforderung einen Prozessor mit 800 MHz (empfohlen: 1 GHz), 512 MB RAM (empfohlen: 1 GB, bei der 64-Bit-Version jeweils das Doppelte) und eine Festplatte mit 20 GB (empfohlen: 40 GB) an. Die Voraussetzungen für Grafikkarten hängen von den drei Grafikmodi und der Auflösung ab. Dabei werden im Modus „Classic“ keine weiteren Anforderungen gestellt, in den Aero-Modi müssen jedoch noch weitere Voraussetzungen erfüllt werden. Dazu gehört DirectX 9, da ohne dieses keine Aero-Glass-Effekte ausgeführt werden können. Ist kein DirectX9 oder kompatibler Grafiktreiber vorhanden, wird standardmäßig der Aero-Basis-Modus verwendet, der ein ähnliches Design wie das Design \"Luna\" unter Windows XP hat.\n\nDie Umweltorganisation Greenpeace empfahl in diesem Zusammenhang, entweder den alten PC aufzurüsten oder auf Windows Vista zu verzichten.\n\nSideShow erlaubt das Darstellen von Informationen auf kleinen Zweitdisplays auf Geräten mit Windows Vista, zum Beispiel der Uhrzeit auf der Außenseite eines Notebooks.\n\nUpdates werden nicht mehr über den Internet Explorer wie noch bei Windows XP und älter aufgerufen, sondern über die Systemsteuerung. Im Rahmen des monatlichen Patchday veröffentlichte Microsoft regelmäßig Updates für Windows Vista. Bislang sind zwei Service Packs erschienen, die hauptsächlich diese Updates zusammengefasst enthielten. Am 11. April 2017 veröffentlichte Microsoft zum letzten Mal Updates und beendete damit die Unterstützung für Windows Vista.\n\nIm Februar 2008 stellte Microsoft das erste Service Pack für Vista fertig, das am 18. März 2008 der allgemeinen Öffentlichkeit in den Sprachen Englisch, Deutsch, Französisch, Japanisch und Spanisch zum Herunterladen bereitgestellt wurde. Das Service Pack 1 wird als Komplettpaket in einer jeweils rund 430 MB großen 32- und einer rund 730 MB großen 64-Bit-Variante sowie als etwa 50 MB großes Paket über Windows Update angeboten. Die Erstellung von eigenen Installationspaketen mit integriertem Service Pack soll laut Microsoft nicht möglich sein.\n\nFür die über die Aktualisierungsfunktion (\"Windows Update\") beziehbare Version wird die Installation aller zuvor veröffentlichten Patches vorausgesetzt. Für bestimmte Treiber, die nicht mit Service Pack 1 kompatibel sind, müssen Updates gesondert nachinstalliert werden. Sind auf dem System zusätzliche Windows-Sprachpakete installiert, wird es aktualisiert, wenn zu allen Sprachen eine Service-Pack-1-Version verfügbar ist.\n\nNeben bis zu diesem Zeitpunkt erschienenen Patches beinhaltet das Service Pack 1 eine Unterstützung des BIOS-Nachfolgers EFI. Außerdem wird die Treiberbibliothek mit dem Service Pack 1 auf etwa 80.000 Komponenten erweitert. Das für Speicherkarten konzipierte und mit Windows Mobile 6.0 eingeführte Datenträgerformat exFAT wird unterstützt und das Festplattenverschlüsselungssystem verbessert.\n\nAuf Druck der Google Inc. besteht nach der Installation des Service Packs die Möglichkeit, jedes beliebige Desktop-Suchprogramm als Standard zu definieren.\n\nMicrosoft verschärft zudem die Produktaktivierung von Vista und macht unter anderem zwei bekannte Methoden zu deren Umgehung unbrauchbar. Inwieweit dies wirksam ist, ist umstritten. Andererseits wird auf Druck der OEM-Partner und Unternehmenskunden die Sanktionierung von nichtaktivierten Windows-Lizenzen abgemildert, indem der \"Modus der reduzierten Funktionalität\" abgeschafft wurde.\n\nNachdem mit der Auslieferung des vorbereitenden Updates KB955430 Anfang Mai 2009 begonnen wurde, hat Microsoft am 26. Mai 2009 das Service Pack 2 zum Download freigegeben, es setzt ein installiertes Service Pack 1 voraus. Neben den seit der Veröffentlichung von Windows Vista SP1 erschienenen Patches enthält dieses folgende neue Hotfixes und Funktionen:\n\n\nAm 27. Oktober 2009 wurde ein Plattform-Update für Windows Vista veröffentlicht. Es benötigt Service Pack 2 und enthält sowohl neue Komponenten, die mit Windows 7 ausgeliefert wurden, als auch aktualisierte Laufzeitbibliotheken.\n\nWährend Microsoft behauptete, Windows Vista liefe auf fast allen ab 2006 verkauften PCs, hielten die höheren Anforderungen einiger Premiumfunktionen wie Aero einige Benutzer von einem Update ab. Dieses Problem betraf neben einigen Desktop-PCs vor allem tendenziell leistungsschwächere Laptops, da diese oftmals zu wenig Arbeitsspeicher aufwiesen und häufig der Grafikkarte oder integrierten Grafik DirectX 9 fehlte, was für die Ausführung von Aero erforderlich ist.\n\nDie Benutzerkontensteuerung verhindert, dass Software ohne Wissen des Benutzers Administratorrechte erhalten und so grundlegende Einstellungen des Systems stillschweigend ändern kann. So stellt die Benutzerkontensteuerung ein mächtiges Sicherheitsfeature dar. Sie wurde jedoch dafür kritisiert, zu viele Abfragen zu stellen, so dass sie für den Benutzer lästig wird, was dazu verleiten konnte, die Funktion zu deaktivieren. Dieses Problem hat Microsoft mit dem Service Pack 1 entschärft, indem die Anzahl der Abfragen deutlich reduziert wurde.\n\nDie Zeitschrift c’t berichtete, die Qualität der deutschen Übersetzung falle gegenüber anderen Microsoft-Produkten deutlich ab. Viele Texte und Dialoge seien verwirrend, uneinheitlich oder unverständlich, etwa durch Wortschöpfungen wie „blocken“ statt „blockieren“.\n\nDas 16-Bit-Subsystem der 32-Bit-Version von Windows Vista schränkt DOS-Programme innerhalb der NTVDM auf die Nutzung von 32 MB Arbeitsspeicher ein. Der Vorgänger Windows XP besaß diese Einschränkung nicht. Die 64-Bit-Version enthält wie schon Windows XP Professional x64 Edition und Windows Server 2003 64-Bit kein 16-Bit-Subsystem.\n\nFür die Versionen für Heimanwender \"Home Basic\" und \"Home Premium\" galt bis Januar 2008 die Beschränkung, dass diese nicht in einer virtuellen Maschine eingesetzt werden durften. Damit war es nicht möglich, Vista etwa unter Parallels auf einem Rechner von \"Apple\" neben MacOS X zu nutzen. Diese Möglichkeit war erst in den teureren Business- oder Ultimate-Lizenzen enthalten. Als Erklärung für das Verbot gab Microsoft an, dass die Virtualisierung nur für Geschäftskunden von Belang sei und ein Sicherheitsrisiko darstelle. Ende Januar 2008 hob Microsoft dieses Verbot jedoch auf.\n\nEnde März 2006 hat die Europäische Union Microsoft mit einem Verkaufsstopp gedroht, falls weiterhin die Auflagen der EU (etwa die Offenlegung der Kommunikationsschnittstellen des Betriebssystems) nicht beachtet würden.\n\nAllgemein wird Windows Vista im Vergleich zum Vorgänger Microsoft Windows XP und zum Nachfolger Microsoft Windows 7 in den Medien ein eher geringer Erfolg bescheinigt. Microsoft selbst kommentierte die Verkaufszahlen positiv, das System wurde laut Angaben des Herstellers im ersten Verkaufsjahr 2007 mehr als 100 Mio. Mal verkauft. In diesem Zeitraum wurden weltweit 271 Millionen PCs und Notebooks verkauft. Im Vergleich dazu verkaufte Microsoft Windows XP im ersten Verkaufsjahr 2002 67 Millionen Mal, in diesem Jahr gab es weltweit 132 Millionen verkaufte PCs.\n\nGianfranco Lanci, Präsident des PC-Herstellers Acer, kritisierte im Juli 2007, dass Windows Vista im Gegensatz zu früheren Windows-Versionen nur wenige Anreize zum Kauf von neuen PCs gegeben habe.\n\nDas US-Verkehrsministerium hat im Januar 2007 seinen Mitarbeitern eine Aufrüstung auf Windows Vista untersagt; ein ähnliches Verbot wurde im US-Luftfahrtministerium (FAA) erlassen. Zahlreiche Unternehmen hatten den Umstieg verschoben. Ende Juni 2008 wurde unter anderem von Intel, General Motors und Daimler erklärt, Windows Vista zu überspringen.\n\nEnde Februar 2008 hat Microsoft angekündigt, die Preisempfehlungen für Windows Vista teilweise deutlich zu senken. Begründet wurde dies mit dem Versuch, weitere Kunden zu einem Update auf Vista zu bewegen. Im April 2008 wurden Pläne für eine Marketingoffensive bekannt, die neben Vista auch das Online- und Handygeschäft von Microsoft fördern sollten. Die Resonanz auf das System vor dem SP1, die geringere Performance und die daher schlechte Benutzbarkeit auf Mini-Notebooks werden als weitere Gründe für diesen Schritt angesehen.\n\n\n"}
{"id": "131780", "url": "https://de.wikipedia.org/wiki?curid=131780", "title": "Touchscreen", "text": "Touchscreen\n\nEin Touchscreen (früher: „berührungsempfindlicher Bildschirm“, seltener: „Berührungsbildschirm“, „Tastschirm“, „Sensorbildschirm“) ist ein kombiniertes Ein- und Ausgabegerät, bei dem durch Berührung von Teilen eines Bildes der Programmablauf eines technischen Gerätes, meist eines Computers, direkt gesteuert werden kann. Die technische Umsetzung der Befehlseingabe ist für den Nutzer gleichsam unsichtbar und erzeugt so den Eindruck einer unmittelbaren Steuerung eines Computers per Fingerzeig. Das Bild, welches durch das (darauf oder darunter befindliche) Touchpad berührungsempfindlich gemacht wird, kann auf verschiedene Weise erzeugt werden: dynamisch mittels Monitoren, über Projektion oder physikalisch (etwa als Ausdruck).\n\nStatt einen Mauszeiger mit der Maus oder Ähnlichem zu steuern, kann der Finger oder ein Zeigestift verwendet werden. Die Anzeige eines Mauszeigers ist damit nur noch nötig, wenn eine genaue und/oder bleibende Positionierung gewünscht ist (zum Beispiel bei grafischem Design), oder der Bildinhalt beim Anwählen sichtbar bleiben muss (zum Beispiel wenn nicht genügend Anzeigefläche zur Verfügung steht).\n\nDie Analogie zum Mausklick ist ein kurzes Tippen. Durch Ziehen des Fingers oder Stiftes über den Touchscreen kann eine „Ziehen und Fallenlassen“-Operation ausgeführt werden. Manche Systeme können mehrere gleichzeitige Berührungen zu Befehlen verarbeiten \"(Multi-Touch)\", um zum Beispiel angezeigte Elemente zu drehen oder zu skalieren. Der Begriff „Multi-Touch“ wird meistens auch im Zusammenhang mit der Fähigkeit des Systems benutzt, Gesten zu erkennen, wenn zum Beispiel durch Wischen weitergeblättert werden kann.\n\nAndere Systeme erlauben, zum Beispiel durch die berührungslose Erkennung eines darüber schwebenden Fingers, die volle Emulation eines Mauszeigers mit einem vom Tippen separaten Zeigemodus.\n\nDer erste (kapazitive) Touchscreen wurde Anfang der 70er Jahre am CERN für die Steuerung des Super-Proton-Synchrotron-Teilchenbeschleunigers entwickelt. Der erste Touchscreen in einem Handy wurde laut PC-Welt in den IBM Simon 1992 eingebaut.\n\nTouchscreens finden als Info-Monitore, zum Beispiel auf Messen, zur Orientierung in großen Kaufhäusern, zur Bedienung von Smartphones oder für die Fahrplanauskunft auf Bahnhöfen Verwendung. Hin und wieder sind auch in den Schaufenstern von Apotheken oder Reiseveranstaltern Touchscreens zu finden, über die detaillierte Informationen abgerufen werden können. Darüber hinaus werden Touchscreens bei Spielautomaten und Arcade-Spielen eingesetzt. Oft werden sie auch für die Steuerung von Maschinen in der Industrie eingesetzt (Industrie-PCs), hier insbesondere weil sie weniger schmutzanfällig sind als andere Eingabegeräte wie Tastaturen. Bei manchen Banken gibt es Geldautomaten mit Touchscreen-Bildschirm. In Banken werden sie immer öfter für Überweisungsterminals eingesetzt, wobei die SAW-Technik (Surface Acoustic Wave) zum Einsatz kommt, weil diese relativ vandalensicher ist. Durch ihre Glasoberfläche verkratzt und beschädigt sie nicht so schnell wie beispielsweise resistive Systeme mit ITO-Folie als Oberfläche.\n\nTouchscreen-Terminals, die zur öffentlichen Informationsweitergabe eingesetzt werden, werden in der IT-Branche als Point-of-Interest-System (abgekürzt, \"POI\") oder Kiosksystem bezeichnet. Terminals, die zum Verkauf dienen, werden Point of sale oder abgekürzt \"POS\" genannt. Letztere haben sich entgegen der hohen Erwartung der Wirtschaft und der IT-Branche nur eingeschränkt durchgesetzt. Gründe dafür sind neben dem Wartungsaufwand für die Geräte oft die mangelnde Anpassung der Software an die besonderen Bedienungsbedingungen der Touchscreen-Geräte oder oft schlicht auch die unergonomische und unattraktive Software und fehlender Nutzen für die Bediener.\n\nIn neueren, modernen Autos werden immer öfter Multifunktionsdisplays als Touchscreen ausgelegt. Neue Techniken bieten hier sogar eine elektronisch erzeugte, taktile Wahrnehmbarkeit.\n\nIn Heimsystemen haben sich Touchscreens inzwischen stark verbreitet, vor allem im Bereich der PDAs, Tablet PCs, Smartphones, Digitalkameras und bei den Spielkonsolen Nintendo DS, PlayStation Vita und Wii U sind sie in größerem Einsatz. Die früher aufgrund der kleinen Bildschirme und der nicht daran angepassten Benutzeroberflächen eingesetzten Eingabestifte (auch: \"Stylus\") sind recht unergonomisch und haben den Durchbruch der Touchscreens in diesem Bereich lange verhindert. Erst mit den projiziert-kapazitiven Systemen (zuerst im \"LG Prada\") hat sich das nachhaltig verändert.\n\nEin Touchscreen braucht nicht vor ein Display montiert zu werden, auch die Verwendung als Ersatz einer Folientastatur ist möglich. Hierzu wird hinter dem Touchscreen (an der Stelle, an der normalerweise der Computerbildschirm sitzt) eine bedruckte (Polyester-)Folie aufgebracht. Es gibt verschiedene Ansätze, Touchscreens ganz von physikalischen Monitoren zu lösen, um auch Projektionen von Benutzeroberflächen interaktiv nutzbar zu machen. Beispiel hierzu ist der inzwischen wieder eingestellte „Virtual Touchscreen“ von Siemens oder verschiedene Systeme des Fraunhoferinstituts.\n\nEs gibt mehrere Funktionsprinzipien zur Umsetzung der Berührungsempfindlichkeit:\n\n\nDie ersten Touchscreens waren noch gewölbte Röhrenbildschirme, vor denen eine plane Fläche eines Lichtschrankengitters gespannt wurde. Die Strahlen – jeder einzelne zwischen einem Paar aus LED und Sensor – liefen zeilen- und spaltenweise zwischen Spalten oder Lochreihen in der Brüstung des Bildschirm-Gehäuserahmens und wurden durch eine Fingerspitze optisch unterbrochen. Damit wurde eine Auflösung in der Größenordnung von 5 mm erreicht, was zur Auswahl grober Schaltflächen eines am Bildschirm angezeigten Menüs ausreicht (siehe Bild). Heute werden sie der Robustheit wegen an Bildschirmen von Geldausgabe- oder Fahrscheinautomaten verwendet.\n\nDer 2017 auf den Markt gebrachte digitale Projektor \"Sony Xperia Touch\" verfügt über eine Infrarotkamera, mit der die Lage eines Fingers auf der Projektionsfläche ermittelt werden kann. Das Gerät kann damit in den im Projektor unter dem Betriebssystem Android installierten mobilen Apps Reaktionen beziehungsweise Interaktionen auslösen.\n\nResistive Touchscreens reagieren auf Druck, der zwei elektrisch leitfähige Schichten stellenweise verbindet. Die Schichten bilden so einen Spannungsteiler, an dem der elektrische Widerstand gemessen wird, um die Position der Druckstelle zu ermitteln. Die Bezeichnung dieser Touchscreens ist auf das englische Wort \"resistivity\" für \"(elektrischer) Widerstand\" zurückzuführen.\n\nSie bestehen aus einer äußeren Polyesterschicht und einer inneren Glas- oder Kunststoffscheibe, die durch Abstandhalter getrennt sind. Die einander zugewandten Flächen sind mit Indiumzinnoxid beschichtet, einem lichtdurchlässigen Halbleiter. Die Abstandshalter sind so klein, dass sie nur bei sehr genauem Hinsehen zu erkennen sind. Sie werden \"spacer dots\" genannt, wörtlich übersetzt \"Abstandspunkte\".\n\nUm die Position der Druckstelle zu ermitteln, wird an einer der leitfähigen Schichten Gleichspannung angelegt. Die Spannung fällt von einem Rand der Schicht zum gegenüberliegenden Rand hin gleichmäßig ab. An der Druckstelle ist die Spannung beider Schichten gleich, weil sie dort verbunden sind. Die zweite leitfähige Schicht ist die Verbindung dieser Stelle nach außen. Zwischen dem Rand dieser zweiten Schicht und den beiden gegenüberliegenden Rändern der ersten Schicht sind zwei Spannungen messbar. Wenn die beiden Spannungen gleich sind, ist der Druckpunkt genau in der Mitte zwischen den beiden Rändern der ersten Schicht. Je höher eine Spannung im Verhältnis zur anderen ist, desto weiter ist der Druckpunkt vom jeweiligen Rand entfernt.\n\nEin Beispiel:\n\n\n\n\nEs muss immer eine zweite Messung dieser Art durchgeführt werden, mit vertauschten Rollen der beiden Schichten, so dass die Abstände zu den anderen Rändern ermittelt werden. Erst dann ist die Position in der Fläche festgestellt. Um die zwei Dimensionen zu erfassen, wird die Gleichspannung also abwechselnd über Kreuz angelegt.\n\nFour-Wire (\"Vier-Draht\") ist die einfachste und älteste Konstruktion zur Bewerkstelligung dieser Kreuzung. Dabei wird die Spannung abwechselnd an beide leitfähigen Schichten angelegt, in jeweils unterschiedlicher Ausrichtung. Es sind deshalb vier Drähte zum Anschluss erforderlich, was dem ganzen seinen Namen gibt.\n\nFour-Wire hat den Nachteil schnell nachlassender Präzision bei der Erfassung der Druckstelle. Die äußere Polyesterschicht des Touchscreens wird durch seine Benutzung mechanisch belastet. Dadurch verliert die leitfähige Beschichtung ihrer Innenseite an Gleichmäßigkeit. Diese Beschichtung ist bei Four-Wire aber ein Maß für die Position der Druckstelle.\n\nFive-Wire vermeidet das Nachlassen der Präzision, indem die äußere leitfähige Schicht nicht als Maß für die Position der Druckstelle herangezogen wird. Sie dient nur zum Weiterleiten der Spannung von der unteren Schicht und ist mit einem zusätzlichen, fünften Draht angeschlossen. Die anderen vier Anschlüsse befinden sich an den Ecken der unteren Schicht. Vor jeder der beiden Messungen werden jeweils zwei benachbarte Ecken direkt verbunden und dann wird an die beiden Eckenpaare die Spannung angelegt. Zwischen erster und zweiter Messung wird zur zweiten möglichen Zusammenstellung von Eckenpaaren umgeschaltet.\n\nSix-Wire und Seven-Wire sind Variationen von Five-Wire, während Eight-Wire eine Variation von Four-Wire ist. Bei diesen Bauformen werden die zusätzlichen Leitungen dazu genutzt, die gemessenen Spannungen nicht an der Zuleitung, sondern über separate Messleitungen abzugreifen (Prinzip der Vierleitermessung).\n\nVorteile:\n\nNachteile:\n\n\nEin Oberflächen-kapazitiver Touchscreen ist eine mit einem durchsichtigen Metalloxid beschichtete Folie (meistens auf Glas auflaminiert). Eine an den Ecken der Beschichtung angelegte Wechselspannung erzeugt ein konstantes, gleichmäßiges elektrisches Feld. Bei Berührung entsteht ein geringer Ladungstransport, der im Entladezyklus in Form eines Stromes an den Ecken gemessen wird. Die resultierenden Ströme aus den Ecken stehen im direkten Verhältnis zu der Berührungsposition. Der Controller verarbeitet die Informationen.\n\nEine andere Bauart (meistens „PCT“ = „Projected Capacitive Touch“ oder „PCAP“ genannt) nutzt zwei Ebenen mit einem leitfähigen Muster (meistens Streifen oder Rauten). Die Ebenen sind voneinander isoliert angebracht. Eine Ebene dient als Sensor, die andere übernimmt die Aufgabe des Treibers. Befindet sich ein Finger am Kreuzungspunkt zweier Streifen, so ändert sich die Kapazität des Kondensators, und es kommt ein größeres Signal am Empfängerstreifen an. Der wesentliche Vorteil dieses Systems ist, dass der Sensor auf der Rückseite des Deckglases angebracht werden kann (die Erkennung wird „hindurchprojiziert“, daher der Name). So erfolgt die Bedienung auf der praktisch verschleißfreien Glasoberfläche. Ferner ist die Erkennung von Gesten und mehreren Berührungen (also Multi-Touch) möglich. Diese Touch-Variante wird inzwischen von praktisch allen Smartphones und Tablet-Computern verwendet.\n\nKapazitive Touchscreens können nur mit dem bloßen Finger (ob der Touchscreen nun mit kalten oder warmen Fingern berührt wird, spielt hierbei keine Rolle), leitfähigen Eingabestiften oder speziell angefertigten Hilfsmitteln, nicht aber mit einem herkömmlichen Eingabestift oder dicken Handschuhen bedient werden. Von dieser Einschränkung sind insbesondere auch Menschen mit Handprothesen betroffen, da sie nur mit speziellen Handschuhen oder Eingabestiften die Möglichkeit haben, die Bedienfelder zu aktivieren. In dieser Hinsicht bilden kapazitive Systeme unter Umständen eine Hürde im Sinne der Barrierefreiheit.\n\nAnwendungsbeispiele für kapazitive Touchscreens finden sich bei Tabletcomputern, Smartphones bzw. Handys mit Touchscreen, Electronic Organizern, PDAs, tragbaren Media Playern, Spielkonsolen und Gastronomiekassen.\n\nInduktive Touchscreens haben gegenüber den anderen beiden Verfahren den Nachteil, dass sie sich nur über spezielle Eingabestifte (mit einer integrierten Spule) nutzen lassen, eine Technik, die von Grafiktabletts übernommen wurde. Diese Spule dient dazu, ein elektromagnetisches Feld zu erzeugen, welches dann von Sensoren im Bildschirm erfasst wird. Diese Daten werden dann dazu genutzt, die genaue Position des Stiftes und, bei einigen Systemen auch die Distanz zum Stift, sowie den Neigungswinkel von ihm zu bestimmen.\n\nDennoch bieten sie gegenüber anderen Techniken einige Vorteile und werden z. B. bei teureren Tablet-PCs und Bildschirmen mit integriertem Grafiktablett genutzt:\n\n\nGrafikprogramme können durch diese zusätzlichen Informationen ein realistischeres Verhalten der simulierten Stifte und Pinsel ermöglichen. Induktive Touchscreens sind wegen des deutlich höheren Energiebedarfs für portable Geräte weniger geeignet.\n\nAnwendungsbeispiele für induktive Touchscreens finden sich bei Tablet PCs, Grafiktabletts und Bildschirmen mit integriertem Grafiktablett.\n\nDiese Systeme nutzen mehrere Techniken, um gegenseitige Nachteile auszugleichen.\n\n\n\n\n"}
{"id": "132381", "url": "https://de.wikipedia.org/wiki?curid=132381", "title": "Computerbetrug", "text": "Computerbetrug\n\nDer Computerbetrug stellt im deutschen Strafrecht einen Straftatbestand dar, der im 22. Abschnitt des Besonderen Teils des Strafgesetzbuchs (StGB) in StGB normiert ist. Er zählt zu den Vermögensdelikten.\n\nDie Strafnorm des StGB bezweckt den Schutz des Vermögens. Hierzu verbietet sie Handlungen, bei denen eine Person, ein Unternehmen oder eine Organisation durch das Manipulieren von Computern in betrügerischer Art finanziell geschädigt wird. Vorbild für den Tatbestand des Computerbetrugs ist der des Betrugs ( StGB), der lediglich Täuschungen gegenüber Menschen erfasst. Daher beschloss der Gesetzgeber mit Wirkung zum 1. August 1986 die Einführung eines neuen Tatbestands, um täuschungsähnliche Handlungen gegenüber Computersystemen in hinreichend bestimmter Weise zu erfassen.\n\nFür den Computerbetrug können eine Freiheitsstrafe bis zu fünf Jahren oder eine Geldstrafe verhängt werden. Der Computerbetrug stellt das häufigste Delikt im Bereich der Computerkriminalität dar. Laut Polizeilicher Kriminalstatistik wurden 2017 in Deutschland 86.372 Fälle des StGB angezeigt. Die Aufklärungsquote dieser Taten liegt mit knapp 40 Prozent im Vergleich zu anderen Deliktgruppen auf durchschnittlichem Niveau.\n\nDer Tatbestand des Computerbetrugs ist in StGB normiert und lautet seit seiner letzten Änderung am 22. Dezember 2003 wie folgt:\nWegen des Regelstrafrahmens von Freiheitsstrafe bis zu fünf Jahren oder Geldstrafe handelt es sich beim Computerbetrug nach Absatz 2 StGB um ein Vergehen. Geschütztes Rechtsgut der Norm ist wie beim Grundtatbestand der Betrugsdelikte, StGB, das Vermögen.\n\nDer Tatbestand des Computerbetrugs wurde das zweite Gesetz zur Bekämpfung der Wirtschaftskriminalität vom 15. Mai 1986 mit Wirkung zum 1. August 1986 ins Strafgesetzbuch aufgenommen. Der Gesetzgeber reagierte damit auf den Umstand, dass elektronische Datenverarbeitungsanlagen aufgrund ihrer zunehmenden Verbreitung im Geschäftsverkehr regelmäßig zum Ziel krimineller Handlungen wurden, etwa durch das unbefugte Verwenden fremder Zugangsdaten. Solche Handlungen wurden von StGB nicht erfasst, da dieser Tatbestand voraussetzte, dass der Täter bei seinem Opfer durch eine Täuschungshandlung einen Irrtum hervorruft. Das Erregen eines Irrtums war bei Computern ausgeschlossen, da sich diese keine Vorstellungen über die Wirklichkeit machten. Daher war eine Anwendung des Betrugsparagrafen schon aufgrund seines Wortlautes in Fällen ausgeschlossen, in denen der Täter nicht einen Menschen, sondern ein Computersystem überlistete. Eine analoge Anwendung des Betrugstatbestands auf Sachverhalte, bei denen Computersysteme manipuliert wurden, schied als Verstoß gegen das strafrechtliche Analogieverbot ( Absatz 2 des Grundgesetzes) von vornherein aus. Selbst wenn man konstruierte, dass durch die Manipulation eines Geräts der Betreiber darüber getäuscht werde, dass die Anlage ordnungsgemäß bedient wurde, wären diejenigen Fälle nicht erfasst, in denen der Manipulierende das Gerät mit richtigen Daten, aber in unbefugter Weise verwendet. Dies ist beispielsweise der Fall, wenn sich jemand vorübergehend eine fremde Bankkarte aneignet, um Geld abzuheben. Für einen Diebstahl ( StGB) fehlte es in derartigen Fällen an einem Gewahrsamsbruch. Gleiches galt, wenn der Täter mit einer gefälschten Codekarte vorgeht. Weitere Straftatbestände, etwa die Untreue ( StGB), kamen regelmäßig ebenfalls nicht in Betracht. Daher bestand eine Lücke im Strafgesetzbuch, die nur durch ein neues Gesetz geschlossen werden konnte, das die Besonderheiten der Computerkriminalität hinreichend berücksichtigte. Absatz 1 der neugeschaffenen Norm bezeichnet die Tathandlung. In Absatz 2 fügte der Gesetzgeber einen Verweis auf Bestimmungen des Betrugstatbestands bezüglich der Versuchsstrafbarkeit und der Strafzumessung ein.\n\nIm Rahmen des sechsten Strafrechtsreformgesetzes wurde der Verweis des Absatz 2 StGB mit Wirkung zum 1. April 1998 um die im Rahmen dieses Reformgesetzes neu geschaffenen Absatz 6 und 7 StGB, die weitere Sanktionsbestimmungen enthalten, erweitert.\n\nAm 22. Dezember 2003 wurde StGB durch das 35. Strafrechtsänderungsgesetz um zwei weitere Absätze erweitert. Hierdurch wurde der EU-Rahmenbeschluss zur Bekämpfung von Betrug und Fälschung im Rahmen mit bargeldlosen Zahlungsmitteln umgesetzt. Durch diese Gesetzesänderung wurden bis dahin straflose Vorbereitungshandlungen unter Strafe gestellt. Hierzu zählt beispielsweise das Entwickeln eines Programms, mit dem ein Computerbetrug begangen werden kann.\n\nDer Tatbestand des Computerbetruges fußt auf dem des Betrugs. Er weicht von diesem insoweit ab, wie es computerspezifische Besonderheiten gebieten, die dazu führen, dass die Tatbestandsmerkmale eines Betrugs nicht erfüllt wären. Aufgrund dieser engen Verwandtschaft beider Delikte wird der Computerbetrug in enger Anlehnung an den Betrugstatbestand StGB ausgelegt.\n\nDer Computerbetrug wird durch das Einwirken auf einen Datenverarbeitungsvorgang begangen. Als Datenverarbeitung werden alle technischen Vorgänge bezeichnet, bei denen Daten aufgenommen und durch Programme derart verknüpft werden, dass Arbeitsergebnisse, beispielsweise Berechnungen, erzielt werden. Gemäß Absatz 2 StGB handelt es sich bei Daten um Informationen, die elektronisch, magnetisch oder sonst nicht unmittelbar wahrnehmbar gespeichert sind oder übermittelt werden. Demnach handelt es sich um codierte Informationen. Die Codierung meint hierbei keine besondere Verschlüsselung. Es kommt lediglich auf die Möglichkeit an, eine Information derart darzustellen, dass sie von einem Computer verarbeitet werden kann. Dies ist der Fall, wenn sie sich im Binärcode darstellen lässt, wie es beispielsweise bei Ziffern und Buchstaben der Fall ist. Der Datenbegriff umfasst somit alle Formen der Eingaben in einen Computer sowie dessen Ausgaben. Der Tatbestand erfasst somit insbesondere Arbeitsprozesse innerhalb von EDV-Systemen, etwa elektronisch durchgeführte Echtheitskontrollen durch Geldprüfungsprogramme in Automaten. Handlungen im Zusammenhang mit rein mechanischen Automaten unterfallen dagegen mangels elektronischer Datenverarbeitung nicht dem Tatbestand.\n\nEin Computerbetrug kann durch vier Handlungsweisen begangen werden: Durch unrichtige Gestaltung eines Programms, durch Verwendung unrichtiger oder unvollständiger Daten, durch deren unbefugte Verwendung und durch unbefugte Einwirkung auf den Datenverarbeitungsvorgang in sonstiger Weise. Diese Handlungen entsprechen der Täuschunghandlung beim Betrug.\n\nBei einem Programm handelt es sich um eine Arbeitsanweisung an einen Computer. Gestalten umfasst das erstmalige Erstellen und das nachträgliche Ändern. Unter welchen Voraussetzungen ein Programm unrichtig ist, ist in der Rechtswissenschaft strittig. Im Wesentlichen haben sich zwei Ansätze herausgebildet. Der eine beurteilt die Unrichtigkeit des Programms nach einem zivilrechtlichen, der andere nach einem betrugsähnlichen Maßstab. Vertreter der erstgenannten Ansicht argumentieren mit der Intention des Gesetzgebers. Dieser stellte auf den Willen des Betreibers als entscheidendes Vergleichsmoment ab. Nach dieser Auffassung liegt daher eine unrichtige Gestaltung vor, falls das Programm vom Willen des Betreibers der Datenverarbeitung abweicht. Dies entspricht dem zivilrechtlichen Fehlerbegriff, der als Mangel das Abweichen der tatsächlichen Beschaffenheit von der vertraglich vorgesehenen beschreibt. Die Gegenansicht zieht eine Parallele zum Betrugstatbestand. Sie stellt bei der Unrichtigkeit nicht auf den Willen einer Person, sondern auf ein objektives Kriterium ab, nämlich die ordnungsgemäße Bewältigung der vom Betreiber gestellten Aufgabe. Das Programm ist demnach unrichtig, falls es die Aufgabe, für die es entwickelt wurde, nicht ordnungsgemäß erfüllt. Dies entspreche eher der Unwahrheit einer Tatsache, einem Tatbestandsmerkmal des Betrugs. Diese beiden Ansichten kommen beispielsweise dann zu unterschiedlichen Ergebnissen, falls der Auftraggeber den Programmierer anweist, Bilanzen zwecks Steuerhinterziehung falsch zu protokollieren. Nach der zivilrechtlich orientierten Ansicht wäre der Computerbetrug zu verneinen, da das Programm so funktioniert, wie es der Betreiber will. Folgt man der betrugsnahen Ansicht, kann die Betrugshandlung dagegen angenommen werden, da das Programm objektiv falsch bilanziert.\n\nEinschlägige Fälle der unrichtigen Programmgestaltung sind in erster Linie Manipulationen eines Programms selbst. Da sich ein Programm aber aus Daten zusammensetzt, stellt diese Handlungsvariante lediglich einen Unterfall der Verwendung unrichtiger oder unvollständiger Daten dar. Daher besitzt sie in der Rechtspraxis kaum eigenständige Bedeutung.\n\nDaten werden verwendet, indem sie auf beliebige Weise in einen elektronischen Verarbeitungsprozess eingeführt werden. Die Begriffe der Unrichtigkeit und der Unvollständigkeit orientieren sich am Begriff der beim Betrugstatbestand verwendeten Täuschung, wobei die Verwendung unrichtiger Daten dem aktiven Täuschen und die Verwendung unvollständiger Daten dem Täuschen durch schlüssiges Handeln nachempfunden sind.\n\nUnrichtig sind Daten, falls ihr Inhalt von der Wirklichkeit abweicht. Anders als bei der unrichtigen Gestaltung des Programms wird für die Beurteilung der Unrichtigkeit der Daten unstreitig ein objektiver Maßstab angelegt, da Eingabedaten nur objektiv richtig oder falsch sein können. Exemplarisch für diese Tathandlung sind Eingabemanipulationen, etwa das Nutzen gefälschter EC- oder Kreditkarten zum Abheben von Geld. Nicht erfasst ist das Nutzen gestohlener Bankkarten, da hierbei ja richtige Daten verwendet werden, nur nicht durch die berechtigte Person. Ein weiteres Beispiel ist Angabe fingierter Forderungen als Abbuchungen im Lastschriftverfahren.\n\nUmstritten ist in der Rechtswissenschaft, ob auch das Angeben falscher Daten im automatisierten Mahnverfahren eine unrichtige Verwendung von Daten darstellt. Befürworter argumentieren, dass der Täter durch die Angabe unwahrer Daten gegen die in Absatz 1 der Zivilprozessordnung normierte prozessuale Wahrheitspflicht verstoße und damit eine Täuschung begehe. Kritiker wenden ein, dass bereits die Behauptung eines Anspruchs den Erlass des Mahnbescheids bewirke, weshalb dieser Bescheid nicht auf falschen Tatsachen beruhe. Außerdem verstoße die Annahme eines Computerbetrugs gegen das Gebot der betrugsnahen Auslegung: Ein Rechtspfleger, der einen Antrag im Mahnverfahren bearbeitet, mache sich bei der Bearbeitung eines solchen Antrags keine Gedanken über das Bestehen des geltend gemachten Anspruchs, weshalb er hierüber auch nicht getäuscht werden könne.\n\nUnvollständig sind Daten, wenn sie wesentliche Informationen über den ihnen zugrundeliegenden Sachverhalt nur in unzureichender Weise erkennen lassen, sie also in Bezug auf eine relevante Tatsache lückenhaft sind. Da unvollständige Daten allerdings auch die Wirklichkeit zumindest teilweise nicht korrekt abbilden, sind sie zugleich falsche Daten. Daher ist die Unvollständigkeit nur ein besonderer Fall der Unrichtigkeit, weswegen dieses Merkmal keine eigenständige Relevanz besitzt.\n\nDer Gesetzgeber schuf die Handlungsform der unbefugten Datenverwendung, da Wissenschaftler und Sachverständige im Gesetzgebungsverfahren daran zweifelten, dass der Tatbestand der unrichtigen Verwendung von Daten den missbräuchlichen Einsatz von Codekarten an Geldautomaten erfasst. Derartige Fälle waren ein maßgeblicher Anlass zur Schaffung des Computerbetrugs, weshalb der Gesetzgeber sicherstellen wollte, dass diese vom neuen Tatbestand erfasst werden.\n\nDer Wortlaut dieser Tathandlung ist allerdings äußerst weit gefasst und nur wenig bestimmt. Um den Tatbestand nicht ausufern zu lassen und um nicht gegen das strafrechtliche Bestimmtheitsgebot zu verstoßen, besteht deswegen Einigkeit in Rechtsprechung und Lehre darin, dass das Tatbestandsmerkmal einschränkend ausgelegt werden soll. Einigkeit besteht ebenfalls darin, dass diese Auslegung möglichst betrugsnah erfolgen soll. Umstritten ist allerdings die Reichweite dieser Auslegung.\n\nDer Begriff der Verwendung entspricht dem der zweiten Handlungform. Da die Verwendung falscher oder lückenhafter Daten bereits dieser Tathandlung unterfällt, erfasst die dritte Begehungsmöglichkeit die Verwendung richtiger Daten durch eine unberechtigte Person. Ein sich in der Praxis häufig ereignendes Beispiel dafür stellt die Verwendung fremder PIN- oder TAN-Nummern dar.\n\nUmstritten ist aufgrund des weit gefassten Wortlauts der Tathandlung, wie weit der Anwendungsbereich des Tatbestands reichen soll. Diese Streitfrage spiegelt sich in der Frage wider, unter welchen Voraussetzungen eine Datenverwendung als unbefugt gilt.\n\nNach einer Ansicht, die den Tatbestand weit auslegt, liegt ein unbefugtes Verwenden bereits dann vor, falls die Daten gegen den tatsächlichen oder mutmaßlichen Willen des Betreibers der Datenverarbeitungsanlage in den Verarbeitungsprozess eingeführt werden.\n\nEine andere Auffassung, die als computerspezifische Auslegung bezeichnet wird, bemüht sich um eine restriktivere Handhabung der Tathandlung. Sie nimmt ein unbefugtes Handeln an, falls die Datenverwendung dem Willen des Betreibers der Datenverarbeitungsanlage widerspricht und sich dieser Wille in der Programmgestaltung widerspiegelt. Dies trifft beispielsweise zu, falls der Täter zur Verwendung der Daten eine PIN-Abfrage überwinden muss.\n\nEine dritte Auffassung, die auch von der Rechtsprechung vertreten wird, stellt die Unbefugtheit mithilfe einer Parallele zum Betrug fest. Nach dieser Ansicht handelt jemand unbefugt, falls das Verhalten des Täters bei der Datenverwendung einen Täuschungswert hat. Dies ist der Fall, wenn, angenommen, der Täter hätte die Daten nicht in einem Computer eingegeben, sondern gegenüber einem Menschen erklärt, in diesem hypothetischen Fall eine Täuschung vorliegt. Trifft dies zu, ist die Täuschungsäquivalenz gegeben, sodass der Täter unbefugt handelt. Bei dieser hypothetischen Datenverarbeitung durch einen Menschen darf jedoch allein auf solche Tatsachen abgestellt werden, die auch der Computer prüfen könnte. Die Täuschung muss also in den Daten angelegt sein. Ansonsten würden nach dieser Ansicht auch konkludente Täuschungen zur Begründung eines Computerbetrugs genügen, obwohl ein Computer nicht konkludent getäuscht werden kann.\n\nEine praktisch bedeutende Fallgruppe der unbefugten Datenverwendung stellt das missbräuchliche Abheben von Geld von einem Geldautomaten mithilfe einer Kredit- oder EC-Karte dar. Dabei sind mehrere Fallkonstellationen zu unterscheiden. Hebt der Kartennutzer unberechtigterweise Geld mit einer gestohlenen Karte ab, liegt eine unbefugte Verwendung vor. Wird dem Täter die Karte vom Inhaber überlassen und hebt er in Absprache mit diesem mehr ab, als es der Kreditrahmen des Berechtigten zulässt, nutzt er die Zugangsdaten zum Konto in unberechtigter Weise. Jedoch ist der speziellere Tatbestand des StGB, der Missbrauch von Scheck- und Kreditkarten, einschlägig, falls der Täter das Geld an einem institutsfremden Geldautomaten abhebt. Der Computerbetrug wird zwar mitverwirklicht, wird aber auf Konkurrenzebene verdrängt. Hebt der Täter dagegen an einem Geldautomaten des Instituts ab, das die Karte ausgestellt hat, ist StGB nicht anwendbar, da das Institut nicht wirksam zu einer Zahlung verpflichtet wird. Daher macht sich der Täter in diesem Fall nach StGB strafbar.\n\nUmstritten ist in der Rechtswissenschaft, welcher Tatbestand erfüllt ist, wenn dem Täter die Karte vom Inhaber überlassen wurde, der Täter aber mehr abhebt, als ihm der Inhaber gestattet hat. Nach der täuschungsäquivalenten Auslegung stellt dieses Verhalten keinen Computerbetrug dar, da der Karteninhaber dem Täter eine Bankvollmacht erteilt. Diese Vollmacht ist zwar hinsichtlich der Höhe des abzuhebenden Geldes beschränkt, den Inhalt dieser Vollmacht teilt der Täter einem hypothetischen Bankangestellten jedoch typischerweise nicht mit. Daher fehlt es an der Täuschungsäquivalenz. Die übrigen Auffassungen bejahen demgegenüber einen Computerbetrug, da das Abheben eines zu hohen Betrags durch einen Nichtberechtigten sowohl dem Willen des Berechtigten widerspricht als auch sich der entgegenstehende Wille, das niemand außer dem Berechtigten Geld abhebt, in Form der Abfrage der Zugangsdaten im Programm widerspiegelt.\nEbenfalls tatbestandsmäßig sind das Durchführen von Online-Transaktionen mithilfe fremder Daten, die beispielsweise durch Phishing erlangt wurden und die Warenbestellung bei Online-Geschäften unter fremdem Namen. Der Bundesgerichtshof bejahte darüber hinaus einen Computerbetrug, als die Täter die vom Computer errechneten Gewinnquoten zu ihren Gunsten beeinflussten, indem sie ein Fußballspiel manipulierten, auf das über das Internet gewettet wurde.\n\nDie letzte Handlungsvariante des StGB stellt einen Auffangtatbestand dar, der sonstige Eingriffe erfasst, welche die Informationsverarbeitung inhaltlich beeinflussen. Hierzu zählen beispielsweise das Manipulieren eines Glücksspielautomaten, die Einwirkung auf die Hardware des Geräts und das Nutzen einer unberechtigterweise wiederaufgeladenen Telefonkarte oder einer Telefonkartenattrappe. Die Rechtsprechung subsumierte unter diesen Tatbestand ebenfalls das Leerspielen eines Geldspielautomaten mithilfe eines Programms, das den Ablauf des Glücksspiels derart manipuliert, dass der Täter das Gewinnbild herbeiführen kann. Nicht tatbestandsmäßig ist allerdings das bloße Ausnutzen eines Programmfehlers, da es hierbei an einer mit einer Täuschung vergleichbaren Tathandlung fehlt.\n\nEine Strafbarkeit wegen Computerbetrugs setzt voraus, dass die Tathandlung das Ergebnis eines Datenverarbeitungsvorgangs beeinflusst und hierdurch einen Vermögensschaden bewirkt. Dieser Taterfolg stellt das computerspezifische Äquivalent zum Taterfolg des Betrugs dar, dem Erregen eines Irrtums, der zu einer Vermögensschädigung führt.\n\nEine Vermögensschädigung liegt vor, falls einem anderen ein finanzieller Schaden entsteht. Nach vorherrschender Auffassung in der Rechtswissenschaft genügt allerdings auch ein Gefährdungsschaden. Diese Rechtsfigur kommt zur Anwendung, wenn zwar noch kein Vermögensschaden eingetreten ist, die Gefahr eines solchen jedoch derart groß ist, dass sie den Vermögensinhaber ähnlich wie der tatsächliche Eintritt eines Schadens belastet.\n\nKeinen Vermögensschaden stellt der finanzielle Aufwand dar, eine manipulierte Anlage wieder für ihren bestimmungsgemäßen Gebrauch einsatzbereit zu machen. Eine Behinderung der Datenverarbeitung selbst, etwa durch Computersabotage, ist ebenfalls kein tauglicher Erfolg, da der Computer als Tatmittel nicht zugleich Tatopfer sein kann. Programmierfehler, die Arbeitsabläufe stören und dadurch finanzielle Schäden verursachen, sind ebenfalls nicht tatbestandsmäßig.\n\nDer Vermögensschaden muss unmittelbare Folge der Tathandlung sein. Hieran fehlt es beispielsweise, wenn eine elektronische Wegfahrsperre überwunden wird, da der Vermögensschaden erst durch eine weitere Handlung herbeigeführt wird, die Wegnahme des Fahrzeugs. Ebenfalls entsteht kein unmittelbarer Vermögensschaden durch Phishing. Hierbei handelt es sich um eine illegale Methode, um persönliche Daten von Internetnutzern zu erlangen. Verwendet der Täter allerdings die durch Phishing erlangten Daten und schädigt hierdurch Dritte, stellt dies einen Vermögensschaden dar.\n\nWie im Verhältnis von Betrug und Diebstahl besteht auch zwischen dem Computerbetrug und dem Diebstahl ein Exklusivitätsverhältnis; beide Tatbestände schließen sich also aus. Liegen zwischen der Manipulationshandlung und der Vermögensverschiebung andere Deliktshandlungen, fehlt es für einen Computerbetrug an der Unmittelbarkeit des Vermögensschadens. Daher stellt beispielsweise das Manipulieren einer elektronischen Zugangssperre eines verschlossenen Raums, um Gegenstände aus diesem zu entwenden, einen Diebstahl und keinen Computerbetrug dar. In den Anwendungsbereich des Diebstahls fällt ebenfalls das Überlisten eines Geldwechselautomaten mithilfe eines an einer Schnur befestigten Geldscheins, da durch das bloße manuelle Einwirken auf den Automaten kein Arbeitsergebnis der Maschine beeinflusst wird.\n\nSind der Betreiber des manipulierten Computersystems und der in seinem Vermögen Geschädigte personenverschieden, findet die Figur des Dreiecksbetrugs, die beim Betrug anerkannt ist, entsprechende Anwendung.\n\nDer subjektive Tatbestand des StGB ist wie der des Betrugs aufgebaut. Eine Strafbarkeit wegen Computerbetrugs erfordert gemäß StGB zunächst, dass der Täter hinsichtlich des objektiven Tatbestands zumindest mit bedingtem Vorsatz handelt. Hierfür muss er die Tatumstände erkennen und die Verwirklichung des Tatbestands billigend in Kauf nehmen. Zusätzlich muss der Täter mit der Absicht handeln, sich oder einem Dritten einen rechtswidrigen Vermögensvorteil zu verschaffen, also einen Vorteil, auf den er keinen Anspruch hat. Dieser Vorteil muss die Kehrseite des Vermögensschadens darstellen. Dieses Erfordernis der Stoffgleichheit, das auch beim Betrug besteht, ist beispielsweise nicht erfüllt, falls sich der Täter sich nicht durch die Vermögensverschiebung zulasten des Opfers, sondern erst durch die Belohnung eines Dritten für die Tat bereichern will.\n\nAuf Grund des Vergehenscharakters des Computerbetruges bedarf die Strafbarkeit des Versuchs gemäß Absatz Variante 2 StGB der ausdrücklichen Bestimmung im Gesetz. Diese Bestimmung erfolgt über den Verweis des StGB auf Absatz StGB, der den Versuch des Betrugs unter Strafe stellt. Der Versuch des Computerbetrugs ist daher strafbar.\n\n Absatz 3 StGB stellt mehrere Vorbereitungshandlungen unter Strafe. Die Auswahl der Tathandlungen orientiert sich an Absatz 1 StGB, der Vorbereitung der Fälschung von Geld und Wertzeichen. Wegen Vorbereitung eines Computerbetrugs macht sich strafbar, wer ein Computerprogramm, dessen Zweck die Begehung eines Computerbetrugs ist, erstellt, sich oder einem anderen verschafft, feilhält, verwahrt oder einem anderen überlasst. Gemeinsames Tatobjekt dieser Handlungen ist ein Programm, dessen objektiver Zweck die Begehung einer Tat nach Absatz 1 darstellt. Der Zweck, einen Computerbetrug zu ermöglichen, muss die Hauptfunktion des Programms sein. Unklar ist beim Wortlaut der Norm, ob das Programm dasjenige sein muss, mit dem später betrogen wird, oder ob es genügt, dass das Programm die Begehung der Tat ermöglicht. Relevant wird diese Frage beispielsweise bei Trojanern oder Phishing-Programmen, die nur dazu dienen, Daten zu erlangen, um sie später unbefugt zu verwenden. Da bereits die Strafbarkeit vorbereitender Handlungen im StGB die Ausnahme ist, wird eine derartige Ausdehnung der Strafbarkeit von vielen Juristen abgelehnt, sodass die Vorbereitungshandlungen sich nur auf solche Programme beziehen, mit denen der Betrug später durchgeführt werden soll.\n\nDer Täter muss bereits bei der Herstellung des Programms mit bedingtem Vorsatz bezüglich des Begehens eines Computerbetrugs handeln. Er muss also Kenntnis davon haben, dass mit dem Programm ein Computerbetrug begangen werden soll und die Verwirklichung dieser Tat in Kauf nehmen.\n\nDurch den Verweis des StGB auf die Vorschriften des Betrugs, werden dessen prozessuale Bestimmungen auf den Computerbetrug angewendet. Dies umfasst die Regelbeispiele sowie die Regelungen zum Strafantrag. Die Tat wird grundsätzlich als Offizialdelikt von Amts wegen verfolgt. Ausnahmsweise erfordert die strafrechtliche Verfolgung der Tat einen Strafantrag, wenn Opfer des Computerbetrugs ein Angehöriger, Vormund oder Betreuer ist oder der durch die Tat entstandene Schaden gering ist.\n\nDie Tat erreicht das Deliktsstadium der Vollendung, wenn zumindest teilweise eine Vermögensschädigung eintritt. Zur Beendigung eines Computerbetrugs kommt es, wenn der Täter den von ihm anvisierten Vermögensvorteil vollständig erlangt. Ab diesem Zeitpunkt beginnt gemäß StGB die Verjährung. Die Verjährungsfrist beträgt aufgrund des Strafrahmens der Tat nach Absatz 3 StGB fünf Jahre.\n\nWerden im Zusammenhang mit einer Tat nach StGB weitere Delikte verwirklicht, stehen diese zum Computerbetrug in Gesetzeskonkurrenz. Häufig tritt diese im Zusammenhang mit anderen Vermögensdelikten auf.\n\nWie der Betrug steht der Computerbetrug im Exklusivitätsverhältnis zum Diebstahl, beide Delikte schließen sich also gegenseitig aus. Gegenüber dem Betrug ist der Computerbetrug subsidiär. Werden also beide Delikte verwirklicht, so wird der Täter lediglich wegen vollendeten Betruges bestraft. Ist in einem Fall unklar, ob der Täter einen Betrug oder einen Computerbetrug verwirklicht hat, ist bei diesen beiden Delikten Wahlfeststellung möglich. Auch Postpendenz kommt in Betracht, wenn ein Täter mit Sicherheit einen Computerbetrug und möglicherweise einen Betrug begangen hat.\n\nTateinheit kommt insbesondere mit den in der Thematik verwandten Tatbeständen der Fälschung technischer Aufzeichnungen ( StGB), der Fälschung beweiserheblicher Daten ( StGB), der Datenveränderung ( StGB), der Fälschung von Zahlungskarten, Schecks und Wechseln ( StGB) oder der Fälschung von Zahlungskarten mit Garantiefunktion ( StGB) in Betracht.\n\nBegeht der Inhaber einer Scheck- oder Kreditkarte den Computerbetrug mit seiner Karte und schädigt hierdurch deren Aussteller, ist der Tatbestand des Missbrauchs von Scheck- und Kreditkarten ( StGB) vorrangig gegenüber StGB und verdrängt diesen. Ebenfalls Vorrang besitzt die nach der Abgabenordnung strafbare Steuerhinterziehung, die auch durch Tathandlungen des Computerbetrugs begangen werden kann.\n\nDas Bundeskriminalamt gibt jährlich eine Statistik über alle in Deutschland gemeldeten Straftaten heraus, die Polizeiliche Kriminalstatistik. Seit 1993 wird das gesamte Bundesgebiet erfasst. In den Statistiken von 1991 und 1992 wurden die alten Bundesländer und das gesamte Berlin erfasst. Ältere Statistiken erfassen nur die alten Bundesländer. Nicht erfasst sind Betrugsfälle mit Debitkarten und mit Zugangsberechtigungen zu Kommunikationsdiensten.\n\n2015 wurden 23.562 Fälle des Computerbetrugs erfasst. Gegenüber dem Vorjahr, in dem 22.308 Fälle registriert wurden, ist dies ein leichter Anstieg. Rund 33,1 % der Fälle wurden aufgeklärt, eine geringfügige Steigerung im Vergleich zu 2014. Der Computerbetrug machte etwa 7,2 % der Computer- und Internetkriminalität aus. Fälle des Computerbetrugs sind überwiegend in der kleinen und mittleren Vermögenskriminalität angesiedelt.\n\nFür die Statistik des Jahres 2016 wurde die Erfassung der Betrugsdelikte neu geordnet. Für den Computerbetrug wurde ein neuer Schlüssel angelegt, der Fälle mit einbezieht, die in den Vorjahren unter den allgemeineren Schlüssel der Betrugskriminalität fielen. Daher sind die Zahlen ab 2016 mit denen der Vorjahre lediglich eingeschränkt vergleichbar.\nEine dem StGB ähnliche Regelung findet sich im Schweizer Strafrecht mit Art. 147 StGB. Dieser stellt den betrügerischen Missbrauch einer Datenverarbeitungsanlage unter Strafe. Tathandlung und Taterfolg dieses Tatbestands decken sich im Wesentlichen mit der deutschen Norm.\n\nIm liechtensteinischen und österreichischen Strafgesetzbuch normiert § 148a den Tatbestand des betrügerischen Datenverarbeitungsmissbrauchs, der ebenfalls große Parallelen zum deutschen StGB aufweist.\n\n\n"}
{"id": "133479", "url": "https://de.wikipedia.org/wiki?curid=133479", "title": "Chemoinformatik", "text": "Chemoinformatik\n\nChemoinformatik, Cheminformatik oder Chemieinformatik (englisch: \"Chemoinformatics, Cheminformatics, Chemical Informatics\" oder \"Chemiinformatics\") bezeichnet einen Wissenschaftszweig, der das Gebiet der Chemie mit Methoden der Informatik verbindet mit dem Ziel, Methoden zur Berechnung von Moleküleigenschaften zu entwickeln und anzuwenden. Zu den Urvätern gehören unter anderem Paul deMain (1924–1999), Johann Gasteiger, Jure Zupan (* 1943) und Ivar Ugi.\n\nDer Begriff „Chemoinformatik“ ist relativ jung, während die älteren Termini Computerchemie (abgeleitet von englisch: \"Computational Chemistry\") und chemische Graphentheorie das gleiche Gebiet bezeichnen (Lit.: Bonchev/Rouvray, 1990). \"Computerchemie\" wird heutzutage eher als ein Teilgebiet der Theoretischen Chemie und der Quantenchemie begriffen.\n\nChemoinformatik beschäftigt sich mit Berechnungen an digitalen Repräsentationen von Molekülstrukturen. Molekülstrukturen können als Graphen aufgefasst werden. Als ihre Repräsentation ist für viele Anwendungen bereits die sog. \"Bindungstabelle\" (englisch: \"connection table\") ausreichend, in der die Art der Verknüpfungen (Bindungen) zwischen den einzelnen Atomen eines Moleküls abgelegt ist. Erst für weitergehende Betrachtungen kann die Einbeziehung von zweidimensionalen (2-D-) bzw. dreidimensionalen (3-D-)Koordinaten notwendig werden. Letztere werden insbesondere benötigt, wenn, etwa im Bereich der Medizinischen Chemie, Wechselwirkungen mit Biomolekülen wie Proteinen untersucht werden sollen.\n\nDie Größe des theoretischen chemischen Raumes aller pharmakologisch aktiven organischen Moleküle wird auf etwa 10 Moleküle geschätzt. Für diese Abschätzung wurden nur Moleküle mit dem Elementen Kohlenstoff, Sauerstoff, Stickstoff und Schwefel und einer molaren Masse von unter 500 g/mol angenommen (Lit.: Bohacek, 1999). Der Raum aller denkbaren organischen Verbindung ist deutlich größer, nämlich unendlich groß. Beide theoretischen chemische Räume übersteigen damit weit größer als die Menge der bisher real synthetisierten Moleküle (Lit.: Lahana, 1999). Mithilfe von computerbasierten Methoden lassen sich aber unter Umständen viele Millionen Moleküle bereits theoretisch (in silico) analysieren, ohne diese zunächst für Messungen im Labor synthetisieren zu müssen.\n\nDie Repräsentation chemischer Strukturen ist eine der grundlegenden Fragestellungen. Für einen Großteil der Anwendungen hat sich die Darstellung als Bindungstabelle (Connection-Table) basierend auf der Valenzstrukturtheorie durchgesetzt. Als Beispiel einer Bindungstabelle sei hier Acesulfam im Standardformat \"Molfile\" der Firma MDL angegeben. Die Zeilen 5–14 enthalten die \"x\"-, \"y\"- und \"z\"-Koordinaten und Elementbezeichner der Atome, die Zeilen 15–24 die Bindungstabelle mit den Ausgangs- und Endatomen jeder Bindung sowie dem Bindungstyp. Die Null-Spalten enthalten mögliche weitere Bezeichner.\n\nZusätzlich zur Bindungstabelle können 3-D-Koordinaten für real existierende Moleküle über Röntgenstrukturanalyse ermittelt werden. Wo dies nicht möglich ist oder ein Molekül physisch nicht existent ist, können 3-D-Koordinaten zumindest näherungsweise auch unmittelbar aus der Bindungstabelle durch iterative Energie-Minimierungsrechnungen für verschiedene Konformationen eines Moleküls erzeugt werden. 2-D-Koordinaten dienen in der Regel allein der Veranschaulichung eines Moleküls und müssen daher hauptsächlich ästhetischen Ansprüchen genügen. Sie werden ebenfalls unmittelbar aus der Bindungstabelle nach allgemein anerkannten chemischen Zeichenregeln errechnet, geben jedoch nur in den seltensten Fällen die tatsächlichen räumlichen Gegebenheiten in einem Molekül wieder.\n\nVerfahren, die keine empirischen Parameter benötigen, werden als Ab-initio-Methoden bezeichnet. Semiempirische Verfahren enthalten empirische Größen und weitere semiempirische Parameter, die durch theoretische Vorgehensweisen bestimmt wurden, jedoch keinen Bezug zu messbaren Größen mehr haben. Prinzipiell sind Ab-initio-Verfahren für kleinere Moleküle geeignet. Semiempirische Verfahren spielen ihre Stärke bei mittelgroßen (100 Atome) Molekülen aus. Beispiele für semiempirische Methoden sind MNDO und AM1.\n\nDie Güte, mit denen Ab-initio-Verfahren die Eigenschaften von Molekülen berechnen können, hängt vom Basissatz der Atome ab, das heißt, wie gut und mit wie vielen einzelnen Funktionen die Atomorbitale dargestellt werden und in welchem Ausmaß die Elektronenkorrelation berücksichtigt wird. Ab-initio-Verfahren, die auch die Elektronenkorrelation berücksichtigen, sind deutlich aufwändiger, liefern jedoch die besten Resultate. Man behilft sich mit einem Kompromiss und bezieht die Elektronenkorrelation näherungsweise ein. Beispiele für solche Verfahren sind: Møller-Plesset-Störungstheorie, CI (Configuration-Interaction), CC (Coupled Cluster), MCSCF (Multi-Configuration-self-consistent-Field). Ein Vorteil der ab-initio-Verfahren ist ihre systematische Verbesserbarkeit, da man durch Vergrößerung des Basissatzes und Erhöhung des Grades der Berücksichtigung der Elektronenkorrelation (z. B. CISD, CISDT...) die Genauigkeit der Ergebnisse systematisch verbessern kann.\n\nBei semiempirischen Verfahren wird ein Großteil der Integrale des Hartree-Fock-Formalismus vernachlässigt, andere werden durch spektroskopische Werte, Parameter oder parametrisierte Funktionen angenähert. Grund für diese Approximation war die geringe Rechenkapazität früherer Zeiten. Um die theoretischen Erkenntnisse dennoch auf chemische Fragestellungen anwenden zu können, musste der vorhandene Formalismus vereinfacht werden.\n\nDie Hückel-Näherung ist der einfachste semiempirische Ansatz, da sie gar keine Integrale berechnet. Allerdings ist sie auch nur auf formula_1-Elektronensysteme anwendbar. Die Theorie wurde später auch auf formula_2-Systeme erweitert (Extended-Hückel-Theory, EHT).\n\nEtablierte Methoden, die auch heutzutage noch häufig angewendet werden, gehören zur Klasse der NDDO-Näherung (Neglect of Diatomic Differential Overlap): MNDO (Modified Neglect of Differential Overlap), AM1 (Austin-Model 1), PM3 (Parametrised Method 3).\nFür kritische Berechnungen sind semiempirische Methoden mit CI und MCSCF kombiniert worden. Mit solchen Verfahren sind dann beispielsweise Reaktionsbarrieren und ganze Energieprofile komplexer Reaktionen berechenbar oder sogar angeregte Zustände (MNDO/CI, MNDO/MCSCF).\n\nDie Grenzen semiempirischer Methoden liegen in ihrer Parametrisierung: Eigentlich können mit der fertigen Methoden nur Systeme gerechnet werden, die in ähnlicher Weise im Parametrisierungsdatensatz vorhanden waren.\n\nKraftfeldprogramme verwenden einen klassisch-mechanischen Ansatz: Bindungen zwischen zwei Atomen \"A\" und \"B\" werden dabei einfach als Sprungfeder angenähert und mit einem harmonischen Potenzial beschrieben (Hookesches Gesetz):\n\nDa eine Doppelbindung zwischen zwei Kohlenstoffatomen eine andere Stärke und Gleichgewichtslänge als eine Einfachbindung besitzt, werden unterschiedliche Parametersätze benötigt (Kraftkonstante formula_4 und Ruhelage formula_5). Man verwendet daher zur Kennzeichnung der Atome keine einfachen Elemente mehr, sondern Atomtypen.\nÄhnliche Ansätze gibt es für Bindungs- und Torsionswinkel. Elektrostatische (Coulomb) und Van-der-Waals-Wechselwirkungen bezeichnet man als nicht-bindende Wechselwirkungen.\nKraftfeldmethoden müssen an empirische oder quantenmechanisch berechnete Daten parametrisiert werden, so dass ein Kraftfeld durch zweierlei charakterisiert ist, seine Energiefunktion und den Parametersatz.\n\nKraftfelder ermöglichen die Geometrieoptimierung sehr großer (Bio-)Moleküle (zum Beispiel: Proteine) und werden hauptsächlich für Moleküldynamik- oder Monte-Carlo-Simulationen verwendet.\n\nEs gibt verschiedene wichtige Themen innerhalb des Gebiets – eine Auswahl:\n\nIm Folgenden werden ausgewählte Anwendungsbeispiele genauer dargestellt.\n\nMithilfe geeigneter Algorithmen werden Kodierungen für Moleküle entwickelt. Durch Induktion können neue Hypothesen über molekulare Eigenschaften erstellt werden, wie zum Beispiel die Bioverfügbarkeit oder die Fähigkeit einer Substanz, die Funktion eines bestimmten Proteins im Organismus zu hemmen oder zu verstärken (siehe auch: QSAR).\n\nDurch geeignete chemische und biologische Hypothesen lässt sich dieser chemische Raum auf wenige Kandidaten reduzieren, die dann im Labor synthetisiert und klinisch getestet werden. Aus diesem Grund spielt die Cheminformatik im Bereich der pharmazeutischen Chemie und der Medizinalchemie eine große Rolle zur Optimierung von Leitstrukturen.\n\nIn der technischen Chemie werden Gruppenbeitragsmethoden verwendet, um Stoffeigenschaften wie Normalsiedepunkte, kritische Daten, Oberflächenspannungen und anderes mehr abzuschätzen.\n\nDie Molekulare Modellierung beschäftigt sich beispielsweise mit der Schaffung von Modellen unbekannter Makromoleküle anhand der Vorlage (Template) ähnlicher, bekannter Moleküle (Homologiemodeling), der Wechselwirkung zwischen kleinen und großen Molekülen (Rezeptordocking), wodurch QSAR möglich wird, der Moleküldynamik sowie die Entwicklung energetisch minimierter 3-D-Strukturen von Molekülen (Bergsteigeralgorithmus, Simulierte Abkühlung, Molekülmechanik etc.). Es geht also darum, aufgrund bekannter Strukturen Modelle von unbekannten Strukturen zu entwickeln, um so eine QSAR zu ermöglichen.\n\nEs gibt einen starken Bezug zur Analytischen Chemie und zur Chemometrie. Die Struktur-Eigenschafts-Beziehungen (beispielsweise: Spektrenkorrelation) spielen eine zentrale Rolle.\nAufgrund vergleichbarer Arbeitsweise existiert eine enge Beziehung zur Computerphysik, wodurch eine klare Trennung häufig nicht eindeutig gegeben ist.\n\nDie Programme der Computerchemie basieren auf verschiedenen quantenchemischen Methoden zur Lösung der molekularen Schrödingergleichung. Grundsätzlich lassen sich zwei Ansätze unterscheiden: Semiempirische Verfahren und Ab-initio-Verfahren.\n\nAlle beschriebenen Verfahren und Methoden sind in gängigen Softwarepaketen verfügbar. Beispiele hierfür:\nACES, GAUSSIAN, GAMESS, MOLPRO, Spartan, TURBOMOLE, Cerius2 und Jaguar. ArgusLab eignet sich als frei verfügbares Programm zum Einstieg in der Computerchemie.\n\nDie Herausforderung für den Anwender dieser Software ist es, das am besten geeignete Modell für seine Problemstellung zu finden und die Ergebnisse im Gültigkeitsbereich der Modelle zu interpretieren.\n\n\n\n"}
{"id": "133850", "url": "https://de.wikipedia.org/wiki?curid=133850", "title": "Atari TT", "text": "Atari TT\n\nAtari TT030 (Thirtytwo/Thirtytwo\" für den Datenbus und 030\" für die CPU) ist die Typenbezeichnung für eine Computerbaureihe der Herstellerfirma Atari Corporation, die zwischen 1990 und 1994 hergestellt wurde.\n\nAtaris für den professionellen Einsatz konzipierte Rechner sind ausgestattet mit Prozessoren des Typs Motorola 68030 und Gleitkomma-Koprozessoren Motorola 68882. Der CPU-Takt sollte ursprünglich bei 16 MHz (der doppelten Frequenz der Atari-ST-Computer) liegen, in Konkurrenz zum Commodore Amiga 3000 (25 MHz) wurde er dann doch – allerdings nur am Prozessor selbst – auf 32 MHz erhöht. Der Rest des Systems wurde weiterhin mit 16 MHz getaktet.\n\nEine Besonderheit beim TT ist die Aufteilung des Arbeitsspeichers in zwei nicht zusammenhängende Bereiche, ähnlich wie beim Commodore Amiga. Diese Aufteilung resultiert aus der Vorgabe, den TT so kompatibel wie möglich zu seinem Vorgänger, dem ST, zu machen; gewissermaßen enthält der TT ein \"ST-Subsystem\":\n\nDie speziell für den TT programmierte Software und die von Atari oder Dritt­anbietern angebotene Hardware (Grafik- und Soundkarten, hoch­auflösende Groß­bildschirme) machten den TT haupt­sächlich im Desktop-Publishing, MIDI-/Musik- sowie Büro­anwendungen interessant. Ein Beispiel dafür ist die DTP-Software \"Calamus\", die heute zwar haupt­sächlich auf Windows-basierten PCs und Apple-Macintosh-Computern mittels Emulatoren benutzt wird, aber immer noch nativ auf TTs benutzt werden kann (sofern die aktuellen Calamus-Anforderungen an Bildschirm­auflösung und Farbtiefe vom TT erfüllt werden).\n\nIm Wesentlichen wurden zwei Revisionen des TT gefertigt:\n\nEingebaut sind bei alten TTs (den sogenannten \"Daughterboard-TTs\") ein DD-Diskettenlaufwerk, bei neueren ein HD-Diskettenlaufwerk (1,44 MB). In jedem Fall aber eine SCSI-Festplatte – ab Werk meist mit 48 oder 80 MB Speicherplatz. Extern können ein zusätzliches Diskettenlaufwerk (SF314, SF354, PCF554 oder fremde Laufwerke), ACSI-Festplatten (Ataris \"Megafile\"-Serie), SCSI-Fest- und Wechselplatten und auch SCSI-CD-ROM-Laufwerke angeschlossen werden.\n\nAm TT kommt Ataris bekanntes Betriebssystem \"TOS\" (\"The Operating System\") in den Versionen 3.01, 3.05 oder 3.06 zum Einsatz. Geplant war auch ein TT mit einer UNIX-Variante unter dem Namen TT/X. Prototypen des TT/X wurden u. a. auf der ATARI-Messe in Düsseldorf vorgestellt. Es handelte sich dabei um ein Unix-System-V-Version-4-kompatibles System, seinerzeit eines der ersten SVR4-Systeme überhaupt. Die endgültige Version, erhältlich auf Tape oder Festplatte, wurde Ende 1990 lediglich an einige Entwickler ausgeliefert, aber nie an Endkunden.\n\n\nDie nachfolgende Peripherie wurde speziell für den Atari TT und teilweise auch für Ataris PC-Serie entwickelt.\n\n\nAnsonsten kann bis auf die Monitore der größte Teil der ST-Peripherie genutzt werden.\n\n"}
{"id": "133856", "url": "https://de.wikipedia.org/wiki?curid=133856", "title": "Chip (Zeitschrift)", "text": "Chip (Zeitschrift)\n\nDas Computermagazin Chip (eigene Schreibweise: \"CHIP\") wird in 14 Ländern Europas und Asiens herausgegeben und gehört in Deutschland zu den ältesten PC-Zeitschriften. Im September 1978 erschien die erste Ausgabe im Vogel-Verlag. Neben allgemeinen Nachrichten rund um Computer- und Technikthemen teilt sich das Magazin in verschiedene Ressorts auf: Trend, Test und Technik.\n\n\"Chip\" bedient mit seinen Artikeln ein breites Spektrum an Zielgruppen: Neben Einsteigern und Fortgeschrittenen können nach Verlagsmeinung auch Profis und IT-Entscheider auf ihre Kosten kommen.\n\n\"Chip\" betreibt eines der größten deutschen Technik-Testzentren mit über 1500 Produkttests pro Jahr.\n\nDie Zeitschrift erscheint seit Anfang 2008 im Verlag \"Chip Communications GmbH\", einer Tochtergesellschaft der Chip Holding, die wiederum zu einhundert Prozent der Hubert Burda Media gehört. Chefredakteur ist seit Februar 2012 Josef Reitberger.\n\nDer lange Bestand der Zeitschrift erklärt sich daraus, dass das Magazin sich immer wieder rigoros veränderte, um sich den wechselnden Bedingungen des Computermarktes anzupassen. So standen in den ersten Heften noch Taschenrechner und Programmierung im Vordergrund. Anfang der 1980er verlagerte sich der Fokus auf Homecomputer. Heute wird primär der nichtfachmännische, private Computernutzer angesprochen.\n\nInzwischen gibt es mehrere Produktableger:\n\nHinzu kommen verschiedene Special-Ausgaben, die in unregelmäßigen Abständen erscheinen und jeweils ein aktuelles Schwerpunktthema aus der digitalen Welt behandeln.\n\nDie Chip-Hefte gibt es in verschiedenen Versionen:\n\n\nIm vierten Quartal 2014 lag die durchschnittliche monatlich verbreitete Auflage nach IVW bei 196.808 Exemplaren. Das sind 9,0 Prozent weniger als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl lag bei durchschnittlich 84.766; derzeit beziehen rund 43,1 Prozent der Leser die Zeitschrift im Abo.\n\n\"Chip Online\" ist das Internetportal der Marke \"Chip\".\n\nAuf Chip Online findet man vor allem Testberichte zu IT- und Telekommunikationsprodukten sowie Verbraucherberatung rund um diese Produkte. Weiterer Schwerpunkt ist der Download-Bereich mit einer großen Anzahl redaktionell geprüfter Free- und Shareware-Programme zum Herunterladen. Chip Online betreibt zudem einen Preisvergleich.\n\nChip Online gehört zu den führenden deutschen Medien-Webseiten. Der größte Konkurrent, das von Gruner + Jahr betriebene Internetportal Computer Channel, wurde 2002 eingestellt.\n\nBetrieben wird die Seite von der \"Chip Digital GmbH\", die neben \"Chip Online\" seit Februar 2007 Download.Chip.eu im Internet publiziert (mittlerweile im Stillstand). Bis Ende September 2012 firmierte das Unternehmen als \"Chip Xonio Online GmbH\". Das Handyportal Xonio wurde 2009 in chip.de integriert. Die \"Chip Digital GmbH\" hat rund 200 feste Mitarbeiter und ist eine hundertprozentige Tochter von Hubert Burda Media.\n\nUnabhängig von \"Chip Online\" betreibt die Redaktion der \"Chip Communications\" zusätzliche Online-Portale, wovon einige bereits eingestellt wurden. Zwei davon, officewissen.net und pcwissen.net, standen in enger Beziehung zum Printobjekt \"Chip\" und hielten Tipps und Anleitungen zu den Anwendungen des Microsoft Officepaketes beziehungsweise zu PC-Themen generell bereit. Die Portale sind mittlerweile nicht mehr verfügbar. Daneben bot das eher Lifestyle-orientierte Zehn.de Bestenlisten aus den Bereichen Motor, Digital, Lebensart u. a., aber die Seite wurde zum 1. März 2016 eingestellt.\n\nÜber den hauseigenen Onlineshop \"Chip Kiosk\" können Chip-Specials, -Magazine, -DVDs und Downloads erworben werden, in der \"Chip Academy\" bietet der Verlag E-Learning-Kurse als Online-Stream an.\n\n"}
{"id": "134488", "url": "https://de.wikipedia.org/wiki?curid=134488", "title": "Informatik Spektrum", "text": "Informatik Spektrum\n\nDie Informatik Spektrum ist eine wissenschaftliche Computerzeitschrift und wird vom Springer-Verlag alle zwei Monate herausgegeben. Bekannt ist die \"Informatik Spektrum\" vor allem als offizielles Organ der Gesellschaft für Informatik e. V. (GI) und ihrer assoziierten Organisationen.\n\nRegelmäßige Rubriken der Informatik Spektrum sind:\n\nDie Mitglieder der GI erhalten die Papierausgabe () und einen kostenlosen Webzugriff auf die elektronische Ausgabe ().\n\n"}
{"id": "135066", "url": "https://de.wikipedia.org/wiki?curid=135066", "title": "Bonjour (Apple)", "text": "Bonjour (Apple)\n\nBonjour (franz. für „Guten Tag!“), ehemals Rendezvous (franz. für „das Treffen“), ist eine Technik, die die automatische Erkennung von Netzwerkdiensten in IP-Netzen bereitstellt. Es ist eine Implementierung des Zeroconf-Systems durch Apple. Bonjour implementiert Multicast DNS (mDNS), DNS-SD sowie IPv4LL. mDNS und DNS-SD sind Apple-Entwicklungen, die zur Anerkennung als offene Standards freigegeben wurden.\n\nUnter anderem verwendet iTunes die Bonjour-Technologie, um automatisch andere Computer zu finden, die ihre Musik freigeben. Unter Windows wird der Bonjour-Dienst von Programmen, die diesen benötigen, bei ihrer Installation mit installiert.\n\nIn Version 10.4 („Tiger,“ 2005) des Betriebssystems Mac OS X wurde \"Rendezvous\" in \"Bonjour\" umbenannt, da es rechtliche Probleme mit dem Namen gab.\n\nVerwendung findet Bonjour außerdem unter anderem in Apple Safari, iTunes, AirPrint und in einigen Adobe-Programmen sowie in netzwerkfähiger Hardware.\n\nMit der Einführung von Apples UNIX-basiertem Mac OS X wurde das Vorgänger-Netzwerkprotokoll AppleTalk nicht migriert. Die Probleme, die vorher auf Mac OS 9 mit AppleTalk gelöst worden waren, wie etwa, einen neuen Drucker per Ethernet-Verbindung zu erreichen, bestanden wieder. Apple hatte damals nicht vor, eine AppleTalk-Variante für Mac OS X zu entwickeln, wodurch auch auf einem Mac wieder eine Netzwerkkonfiguration mit IP-Adressen wie bei einem klassischen UNIX-System durchgeführt werden musste.\n\nEin Benutzer namens Stuart Cheshire wollte das nicht akzeptieren und schrieb in einer E-Mail-Diskussionsgruppe sein Leid. Diese E-Mail und die ganze Diskussion haben Geschichte geschrieben, weil daraufhin Apple beschloss, einen zeitgemäßen AppleTalk-Nachfolger zu entwickeln. Seine zwei Kernaussagen in dieser Diskussion waren, dass es erstens keine Dauerlösung sein kann, dass UNIX-Benutzer zu ihm kommen, um über seinen Mac ihre Dokumente per Netzwerk aus der Ferne auszudrucken, weil auf ihren Rechnern die Konfiguration der Drucker zu umständlich und fehlerträchtig ist. Einige Teilnehmer argumentierten, dass Netzwerkprobleme die Aufgabe des Administrators seien und man deshalb keine technisch bessere Lösung konstruieren muss. Er entgegnete als zweites, dass so einfache Aufgaben auch ohne Administrator möglich sein müssten.\n\nSpäter wurde Stuart Cheshire von Apple eingestellt, um einen zeitgemäßen AppleTalk-Nachfolger zu entwickeln, der auf dem TCP/IP-Protokollstapel aufbaut. Heraus kam \"Rendezvous\", das später wegen Markenrechtproblemen in \"Bonjour\" umbenannt wurde. Die gesamte Technik wurde als OpenSource freigegeben und wird als Standard Zeroconf auch für andere Betriebssysteme weitergepflegt.\n\nBonjour ist auch für Microsoft Windows verfügbar und steht auf der Apple-Website zum Herunterladen bereit. Die Software ermöglicht damit auch unter Windows und in heterogenen Netzen eine einfache Vernetzung von bonjour-fähiger Soft- und Hardware. Zumeist handelt es sich dabei um netzwerkfähige Drucker, die sich dank Bonjour ohne weitere Netzwerkkonfiguration durch den Benutzer verwenden lassen.\n\nBonjour erfüllt wie ZeroConf drei Hauptaufgaben:\n\n\nDie kompletten technischen Konzepte von ZeroConf spezifizieren eine Reihe von RFC-Dokumenten, hauptsächlich:\n\n\nIANA führt ein Verzeichnis von Dienste-Namen und Ports, die \"service names and ports registry\". In diesem Verzeichnis können sich Entwickler, die neue Dienste-Typen für ihre Systeme definieren und entwickeln möchten, eindeutige Namen reservieren und registrieren lassen.\n\nDie Hauptkomponente der ZeroConf-Implementierung in Gestalt von Bonjour, der Darwin-mDNSResponder, steht als Open-Source unter der Apache-Lizenz zur freien Verfügung.\n\nBonjour startet einen Daemon, mit dem sich die \"dns_sd\" aus der Anwendung heraus verbinden kann. Für Java gibt es eine JNI-Brücke.\n\n"}
{"id": "135078", "url": "https://de.wikipedia.org/wiki?curid=135078", "title": "Microsoft Excel", "text": "Microsoft Excel\n\nMicrosoft Excel (abgekürzt MS Excel) ( [], meist [] oder []) ist das am weitesten verbreitete Tabellenkalkulationsprogramm.\n\nExcel gehört zum Microsoft-Office-365-Abonnement und ist sowohl für Windows als auch für macOS verfügbar. Die aktuelle Version für beide Betriebssysteme ist \"Microsoft Excel 2019\".\n\nMicrosoft Excel war der Nachfolger von Microsoft Multiplan und wurde 1985 erstmals für den Apple Macintosh als rein grafisch orientierte Tabellenkalkulation vorgestellt. Am 31. Oktober 1987 wurde, gleichzeitig mit dem Erscheinen von Windows 2.0, mit \"Excel 2.0\" die erste Version für IBM-kompatible PCs ausgeliefert. Ab 1989 wurde eine Windows-2.11-Runtime-Version bei Excel mitgeliefert, da Windows noch kaum verbreitet war. Es gab auch eine Version für den OS/2 Presentation Manager, die mittels Windows Libraries for OS/2 (WLO) portiert wurde.\n\nDas 1990 für Microsoft Windows 3.0 vorgestellte Excel 3.0 verfügte erstmals über eine Symbolleiste und brachte auch sonst zahlreiche Neuerungen. Ab 1994 war mit \"Excel 5.0 für Windows NT\" erstmals eine 32-Bit-Version verfügbar. In Microsoft Office 95 erhielt Excel 95 die Versionsnummer 7.0, da Version 6 übersprungen wurde, um eine Vereinheitlichung der Bezeichnung mit den anderen Microsoft-Office-Programmen zu erzielen. Excel:mac 2001 war die letzte Version für Mac OS 8 und 9.\n\nWie die meisten Tabellenkalkulationen ermöglicht Excel umfangreiche Berechnungen mit Formeln und Funktionen, unter anderem mit kaufmännischen, statistischen und Datumsfunktionen.\nExcel besitzt auch zahlreiche mathematische Funktionen, so dass viele Probleme der Wirtschaftsmathematik berechnet werden können.\nEs können Texte verkettet oder logische Berechnungen (wenn…dann) durchgeführt werden. Abhängig von Inhalten und Werten in der Tabelle kann auf Inhalte an anderer Stelle der Tabellen zugegriffen werden. Die Ergebnisse können mit Hilfe von Sortier-, Gruppier- und Filterfunktionen sowie Pivot-Tabellen ausgewertet und in Diagrammen grafisch dargestellt werden. Tabellen oder Teile davon können gegen Layout- oder Inhaltsänderungen geschützt werden. Die Mindestvoraussetzungen, um in Teamarbeit an Tabellen arbeiten zu können, sind gegeben.\n\nDer Funktionsumfang von Excel kann durch Programmierung in Visual Basic for Applications (VBA) erweitert werden. VBA wurde mit \"Microsoft Excel 5.0\" eingeführt und später in Word, Access und weitere Office-Programme integriert. Unter Macintosh enthielt Excel 2008 keinen VBA-Support, während die Vorgänger- und Nachfolgerversionen diesen anboten. Excel ist zudem mit Visual Studio Tools for Office System (Windows) oder AppleScript (Mac) erweiterbar. \"Excel 4.0\" führte eine eigene Makrosprache ein.\n\nDer Arbeitsbereich von Excel besteht aus \"Arbeitsmappen\", die Dateien entsprechen, aus \"Blättern\", die in Registern angezeigt werden, und aus \"Zellen\", die die Daten enthalten. Pro Arbeitsmappe sind bis zu 65.536 Tabellenblätter möglich. Die Zellen eines Tabellenblatts sind in Zeilen und Spalten eingeteilt und können über ein Zellbezugssystem angesprochen werden.\n\nSeit Excel 2007 kann ein Tabellenblatt 1.048.576 Zeilen und 16.384 Spalten (A bis XFD), also 17.179.869.184 Zellen umfassen. Davor war die Größe auf 65.536 Zeilen und 256 Spalten (A bis IV), also 16.777.216 Zellen, begrenzt. Ist in jedem dieser Felder eine Ziffer eingetragen, so hat eine Datei in Office 2003 eine Größe von 227 MB, in Office 2013 eine Größe von 1382,4 MB also 1,35 GB.\n\nJede Zelle kann durch eine Kombination aus Buchstabe und Zahl eindeutig identifiziert werden, dem sogenannten Zellbezug, der aus Zeilen- und Spaltenangabe besteht. Die erste Zelle in der oberen linken Ecke heißt \"A1\", wobei \"A\" die erste Spalte und \"1\" die erste Zeile bezeichnet und der Bezug genau genommen auch noch den Tabellenblattnamen einschließt, da Formeln in verschiedenen Blättern und Mappen den gleichen Bezug aufweisen können, etwa \"Tabelle1!C4 + Tabelle3!C4\".\n\nAlternativ kann in den Programmoptionen, die von Microsoft Multiplan oder Microsoft Works benutzte \"Z1S1-Bezugsart\" eingestellt werden. Ein Bezug wie „B3“ (der relativ ist) ist in der Z1S1-Schreibweise als absoluter Bezug „Z3S2“. Ein Bezug ausgehend von „A1“ zu „B3“ würde in Z1S1-Schreibweise „Z(2)S(1)“ – in Worten „gehe von der momentanen Zelle – hier im Beispiel Z1S1 – um zwei Zeilen nach unten und um eine Spalte nach rechts“. Wie in der A1-Schreibweise können relativer und absoluter Bezug gemischt werden, als Beispiel etwa „Z(2)S3“ oder „Z3S(-2)“. Die Z1S1-Schreibweise bietet sich für den Einstieg an, da hier der Unterschied zwischen absoluter und relativem Bezug einfacher und schneller zu erklären ist.\n\nDie Z1S1-Schreibweise ist in der VBA-/VBS-/COM–Programmierung erforderlich. Hier werden, um Sprachunabhängigkeit zu erreichen, die englischen Begriffe „R“ für Row (Zeile) und „C“ für Column (Spalte) verwendet. Außerdem können damit relative Bezüge (siehe nächster Absatz) gleich dem gesamten betroffenen Bereich statt nur einer Ausgangszelle zugewiesen werden, was den Programmcode verkürzt.\n\nEs können sowohl relative als auch absolute Zellbezüge als Koordinaten in Tabellen verwendet werden. Formeln sind ebenso wie Werte zeilen- oder spaltenweise kopierbar. Dieser Ansatz wurde vom Vorgänger Multiplan übernommen. Um die Kopierbarkeit von Formeln und deren Bezügen zu gewährleisten, gibt es relative, wie \"A22\", absolute, wie \"$A$22\", und gemischte, wie \"$A22\" oder \"A$22\", Zellbezüge.\n\nNeben dem Einfügen eines gesamten in die Zwischenablage kopierten Inhalts einer oder mehrerer Zellen, gibt es die Möglichkeit, bestimmte Inhalte einzufügen, beispielsweise nur die Werte, anstelle der Formel, die diese Werte erzeugt hat. Dabei ist es auch möglich, den kopierten Wert mit dem Inhalt der markierten Zellen zu addieren oder andere Berechnungen auszuführen.\n\nFür die Anzeige von Werten steht eine Reihe von Formaten zur Verfügung. Neben vorgefertigten Formaten, wie Datum und Uhrzeit und Sonderformaten für Postleitzahlen, können benutzerdefinierte Formate angegeben werden. Zellen können in Excel auch Text enthalten.\n\nAb der Version Excel 2007 können Tabellen mit vordefinierten oder individuell definierbaren Tabellenformaten formatiert werden. Für vom Zellinhalt abhängige Hervorhebungen stehen ab dieser Version beliebig viele bedingte Formatierungen, statt bisher drei, zur Verfügung. Farbgebung und Schriftformatierung wurden an \"Word \"und\" PowerPoint\" angenähert und umfassen dasselbe Farbspektrum, aus dem 12 bevorzugte Farben und weitere Formatierungen als Office-Design festgelegt werden können.\n\nExcel verfügt über einfache Datenbankfunktionen. Es verfügt ab der Version 2013 eingeschränkt über die Funktionalitäten des weitverbreiteten Konzepts der relationalen Datenbank. So ist es möglich, Tabellen anhand von IDs zu verknüpfen. Es kann auch über Schnittstellen auf Daten aus Datenbanken zugegriffen werden. Diese Funktionalität stellte Excel bis Version 4 mit dem Programm des gleichnamigen Herstellers \"Q+A\" zur Verfügung, ab Version 5 mit Microsoft Query, einem Abfrageprogramm für auf SQL basierende Datenbanken über ODBC.\n\nEs gibt auch von Microsoft Datenbanktreiber für die Arbeitsmappen von Excel.\nDamit können die Daten in den Zellen von den Arbeitsblättern als Tabellen verwendet werden. Jedoch sind häufig nicht alle SQL-Befehle für diese Datenbanktreiber in Excel verfügbar.\n\nExcel kann ab der Version 4.0 mit der Excel-eigenen XLM-Sprache programmiert werden (heute versteckt und vergessen, wird aber immer noch für existierende Anwendungen unterstützt) und seit Excel 5.0 mit Visual Basic for Applications (VBA), oder unter macOS mit AppleScript, programmiert werden. Unter Windows gibt es die Möglichkeit, in Visual Studio.NET mit den Visual Studio Tools for Office System (VSTO) Add-ins zur Erweiterung der Funktionalität, sowie mit Visual Basic Script (VBS) unter Windows ScriptHost oder Classic ASP zu programmieren. Mit VBA und der XLM Sprache ist es möglich, eigene Funktionen zu programmieren, um analog den eingebauten Funktionen Berechnungen durchzuführen und das Ergebnis auszugeben.\n\nDie Macintosh-Variante Excel 2008 unterstützte kein VBA. Unter Windows ist es anderen Programmen wie Word, Access, Visual Basic Anwendungen oder Visual Basic Scripts möglich, über die COM- bzw. ActiveX-Schnittstelle Funktionen von Excel zu nutzen oder direkt mit Excel zusammenzuarbeiten.\n\nEin Add-on ist ein zusätzliches Programm, das mit Excel zusammen geladen wird und danach zur Ausführung bereitsteht. Einige Add-ons, wie der Solver, sind in Excel bereits enthalten. Anwender können eigene Add-ons erstellen und einbinden. Durch Add-ons lassen sich Arbeiten mit Excel automatisieren. Im Internet stehen viele Add-ons, häufig auch kostenlos, zur Verfügung.\n\nAm 6. November 2014 wurde eine Version von Microsoft Excel als Mobile App mit der Versionsnummer 1.2 eingeführt. Sie wurde mit einem angepassten Layout versehen und entspricht in der Bedienung den übrigen Apps Word und PowerPoint. Viele Funktionen waren bei der Erstveröffentlichung nicht enthalten, werden aber im Rahmen von Updates sukzessive nachgerüstet. Die Daten können per OneDrive oder Dropbox vom PC, Tablet oder Smartphone bearbeitet werden. Eine Lokale Speicherung oder die Offlinebearbeitung sind ebenfalls möglich. Die Größe des Programmpakets beträgt 490 MB.\n\nMicrosoft Excel ab 2007 und 2008 erstellen standardmäßig Dateien mit den Dateiendungen \".xlsx\" (Excel Spreadsheet) oder \".xlsm\" (Excel Spreadsheet mit Makros). Zusätzlich wird eine Reihe von Dateiendungen wie \".xlsb\" (platzsparendes Binärformat), \".xlam\" (Excel Add-ins), \".xltx\" (Excel-Vorlagen), \".xlk\" (Excel-Sicherungskopien) und \".xll\" (Excel-Makrobibliothek) verwendet.\n\nEinige der neuen Dateiformate basieren auf dem offenen OpenXML, das auf XML basiert und eine ZIP-Kompression nutzt. Excel kann verschiedene Dateiformate öffnen oder importieren bzw. speichern oder exportieren, darunter verschiedene Textformate und solche anderer Tabellenkalkulations- und Datenbankprogramme.\n\nDie von älteren Versionen erstellten Dateien haben die Dateiendung \".xls\" (Excel Spreadsheet), \".xla\" (Excel Add-ins) und \".xlt\" (Excel-Vorlagen). Diese Dateiformate basieren auf einem offenen Binärformat von Microsoft, dem Binary Interchange File Format (BIFF). Sie sind jedoch zu einem großen Teil proprietär, da Microsoft die Dokumentation nicht in allen Details veröffentlicht hat. Eine Dokumentation der Dateistruktur ab Excel 97 ist von Microsoft im Februar 2008 veröffentlicht worden, frühere ausführliche Analysen stammen aus der Open-Source-Gemeinde.\n\n\n\n\n"}
{"id": "135160", "url": "https://de.wikipedia.org/wiki?curid=135160", "title": "IPhoto", "text": "IPhoto\n\niPhoto ist eine Bilderverwaltungs-Software für macOS und gehört zum iLife-Paket, in dem auch GarageBand, iMovie, iDVD und iWeb enthalten sind. Mit der Betriebssystemversion OS X Yosemite 10.10.3 wurde iPhoto eingestellt zugunsten seines Nachfolgers Apple Fotos.\n\nNeue Bilder lassen sich von einer Digitalkamera importieren und in Kategorien einordnen. Nach dem Importieren kann iPhoto einfache Bildbearbeitungen wie Änderungen an der Helligkeit oder am Kontrast, Weißabgleich, Schärfen und Drehen durchführen. Weitere Funktionen sind: rote Augen entfernen, störende Gegenstände retuschieren, diverse Filter anwenden, drucken, über Mail versenden, als QuickTime-Movie oder Website exportieren. iPhoto bewahrt von jedem Bild ein Original auf, auf das später zurückgegriffen werden kann.\n\nAb Version 6 beherrscht iPhoto auch die Bearbeitung von RAW-Bildern.\n\nMobileMe-Mitglieder konnten mit Version 6 Bilder auch als sogenannte \"Photocasts\" im Internet veröffentlichen. Mit Version 7 wurde diese Funktion durch die Einbindung der MobileMe-Web-Gallery-Funktion ersetzt. Damit lassen sich veröffentlichte Bilder in Fotoalben sortiert, in einer Online-Galerie mit verschiedenen Darstellungsmodi betrachten, einzeln oder zusammen als ZIP-Archiv herunterladen oder als Album in iPhoto abonnieren. Ein abonniertes Album wird automatisch aktualisiert, sobald der Besitzer neue Bilder hinzufügt, sodass bei den Abonnenten neue Bilder nachgeladen werden und das abonnierte Album synchron mit dem des Besitzers ist. Außerdem können Besucher der MobileMe-Web-Gallery, wenn der Besitzer dies erlaubt, selbst Bilder per Webbrowser oder per E-Mail hinzufügen. Diese Bilder erscheinen dann sofort in der \"Web-Gallery\" und werden auch zum ursprünglich veröffentlichten Album sowie allen abonnierten Alben zurück-synchronisiert.\n\nAb der Version 8 beinhaltet iPhoto einige neue Funktionen: So bietet es die Möglichkeit, in Fotos Gesichter automatisch zu erkennen und gleiche Gesichter einer bestimmten Person zuzuordnen. Eine weitere Neuerung ist die Implementierung von Geotagging-Funktionen wie Anzeigen der Koordinaten auf einer Karte und nachträglicher Verortung des Bildes. Außerdem ist es nun möglich, Bilder auf dem Fotodienst Flickr und der Social-Networking-Plattform Facebook zu veröffentlichen. Bei letzterer werden auch die über Gesichtserkennung gewonnenen Informationen und Namen veröffentlicht. Bei manueller Markierung weiterer Personen mit Namen über die Facebook-Plattform werden diese Informationen wieder an iPhoto zurück-synchronisiert. Weiterhin können für Diashows jetzt themenbezogene Vorlagen verwendet werden. Dabei werden Fotos in vorgefertigte sogenannte „Artworks“ eingesetzt und virtuelle Kamerafahrten darüber abgespielt. Sowohl in den Diashows als auch in den über iPhoto käuflich erwerbbaren Drucksorten (Fotoalben etc.) können Karten verwendet werden, die auf den Informationen der neuen „Orte“-Funktion basieren.\n\nSeit Oktober 2010 ist die Version 9 von iPhoto verfügbar. Es können in einer Bibliothek bis zu 250.000 Fotos verwaltet werden. Außerdem lassen sich über Kodak die erstellten Fotos, Grußkarten, Wandkalender oder Fotoalben bestellen. Seit Anfang 2012 ist iPhoto auch als App für Apples iOS Geräte ab iOS 5.1 erhältlich, sodass es auch auf dem iPhone, und dem iPad genutzt werden kann. Der iPod touch wird hierbei nicht unterstützt. Erstmals wird in dieser Version nicht mehr auf Kartenmaterial von Google Maps zurückgegriffen, sondern auf Daten von OpenStreetMap.\n\nDer Vertrieb der iOS-Version wurde im Herbst 2014 mit Erscheinen des Betriebssystems iOS 8 eingestellt. Sie wird durch das im Betriebssystem integrierte App \"Fotos\" ersetzt, das nun durch Plug-ins anderer Entwickler erweiterbar ist. Im April 2015 wurde auch die OS-X-Version durch das Nachfolgeprodukt Apple Fotos ersetzt.\n\n\n\n\nDie offizielle Website von iPhoto wurde abgeschaltet und leitet direkt auf \"Photos\" (in deutscher Sprachanzeige \"Fotos\") weiter.\n"}
{"id": "135698", "url": "https://de.wikipedia.org/wiki?curid=135698", "title": "Pegasus Mail", "text": "Pegasus Mail\n\nPegasus Mail ist ein kostenloses E-Mail-Programm, das seit 1989 von dem Neuseeländer David Harris entwickelt wird.\n\nDas Programm bietet eine wahlweise englisch- oder deutschsprachige grafische Benutzeroberfläche. Für die aktuelle Version 4.x gibt es Sprachpakete für Französisch und Italienisch. Sprachpakete für Niederländisch, Tschechisch und Katalanisch stehen für Version 3.12 zur Verfügung.\n\nSeit Version 4 bietet Pegasus Mail als Alternative zum klassischen Multiple Document Interface (MDI) die von Outlook Express und dem Netscape Communicator bekannte Dreifensteransicht mit Mailvorschau.\n\nE-Mails können per Drag & Drop in Ordner einsortiert werden, auch sonst kann das Programm mit der Maus bedient werden. Für fast alle Funktionen existieren zusätzlich Tastenkombinationen zur Bedienung des Programms per Tastatur. Auch verfügt Pegasus Mail über viele Kommandozeilenfunktionen.\n\nDie Nachrichten eines Ordners können, wie auch bei anderen Mailprogrammen üblich, nach Datum, Betreff, Thema, Größe oder Absender sortiert werden. Darüber hinaus lassen sich die Nachrichten nach Tagen, Wochen, Monaten oder Threads gruppieren. Eine Anzeige in Baumstruktur (Threading) wie bei Mutt gibt es bei Pegasus Mail jedoch nicht.\n\nUnterstützt wird der E-Mail-Versand per Novells MHS oder SMTP. Authentifizierung ist per SMTP-After-POP und SMTP-Auth möglich.\n\nEs können mehrere POP3- und IMAP-Konten verwaltet werden. Der \"Selektive Maildownload\" erlaubt auch bei POP3 eine Vorschau auf die Mailbox und bietet die Wahlmöglichkeit, welche Mails man herunterladen möchte. Automatisches Filtern ist möglich, ohne die E-Mails vollständig herunterzuladen. Es besteht die Möglichkeit, per POP3 abgeholte E-Mails nach der Abholung nicht zu löschen, sondern auf dem Server zu lassen. Nachrichten eines IMAP-Ordners kann Pegasus Mail lokal in einem Cache speichern. Man hat so einen schnelleren Zugriff auf die Nachrichten und kann offline arbeiten.\n\nDie TLS-Varianten der genannten Protokolle werden unterstützt. Ebenfalls unterstützt werden die Protokolle LDAP und PH. Für MAPI und RSS existieren Plug-ins.\n\nIn Pegasus Mail wurde bereits 1991 die Filtermöglichkeit von E-Mails eingeführt. Gefiltert werden kann automatisch beim Öffnen oder Schließen eines Ordners oder auf Auslösung durch den Nutzer hin. Filterung direkt auf dem Server, also ohne die gesamte E-Mail herunterzuladen, ist möglich. Trifft eine Filterbedingung zu, können Aktionen wie Verschieben, Einfärben, Starten eines externen Programms, Umleiten oder Weiterleiten der E-Mails ausgeführt werden. Die einzelnen Filterkriterien können logisch verknüpft werden. Pegasus Mail unterstützt im Regelsatz für Filter reguläre Ausdrücke, deren Syntax sich jedoch von den im POSIX-Standard festgelegten \"Basic\" und \"Extended Regular Expressions\" erheblich unterscheidet. Über Sprungmarken können Filterregelsätze minimalistische Programmlogik enthalten.\n\nDarüber hinaus verfügt Pegasus Mail über eine wertungsbasierte Filtermöglichkeit, die „Inhaltskontrolle“. Zum Kampf gegen Spam wird bereits ein vordefinierter Regelsatz für die Inhaltskontrolle mitgegeben. Einen selbstlernenden bzw. trainierbaren Bayes-Spamfilter gibt es seit Version 4.41.\n\nPegasus Mail unterstützt die Multipurpose Internet Mail Extensions (MIME). Dank der GNU-libiconv-Bibliothek kommt das Programm mit zahlreichen internationalen Zeichensätzen und Kodierungen zurecht, darunter alle ISO 8859-Zeichensätze, KOI8-R, KOI8-U, UTF-8 und UTF-7.\n\nNachrichten können verschlüsselt und digital signiert werden. Für die Unterstützung von Pretty Good Privacy (PGP) und GNU Privacy Guard (GPG) existieren kostenlose Plugins.\n\nHTML-E-Mails können sowohl erstellt als auch dargestellt werden. Die Darstellung erfolgt entweder mit Hilfe der Rendering Engine des Internet Explorers oder alternativ unabhängig vom Internet Explorer mithilfe einer eigenen Rendering-Engine namens \"BearHTML\". Es werden keine aktiven Inhalte ausgeführt. Um die Privatsphäre des Benutzers vor Zählpixeln zu schützen, werden Bilder, Stylesheets oder Skriptdateien nur auf Befehl des Benutzers aus dem Internet nachgeladen. Die Adressen vertrauenswürdiger Organisationen lassen sich jedoch in eine Whitelist eintragen, damit Bilder in den Newslettern dieser Organisationen automatisch nachgeladen werden.\n\nAus Sicherheitsgründen verhindert Pegasus Mail das direkte Ausführen von Anhängen \"(Attachments)\", die unter Windows ausführbar und somit potentiell gefährlich sind. Angehängte Bilder in den Formaten JPEG, GIF, BMP, WMF und EMF können direkt in Pegasus Mail betrachtet werden. Externe Viewer für Dateien lassen sich unabhängig von den Einstellungen des Betriebssystems festlegen.\n\nIn den Grundeinstellungen enthält Pegasus Mail folgende Ordner: Im \"New Mail Folder\" landen neue Mails nach der Abholung, egal von welchem POP-Account das Mail stammt. Der Ordner \"Junk and Suspicious Mail\" sortiert Mails aus, die von der Inhaltskontrolle erfasst werden. Bei Verwendung des Papierkorbs wird ein entsprechender Ordner erstellt. Es ist zudem sinnvoll, einen Standardordner für gelesene Mails anzulegen. Pegasus Mail erlaubt das Anlegen mehrere Mailordner sowie von Schubladen, die Unterordner enthalten. Auch virtuelle Ordner werden von Pegasus Mail unterstützt.\n\nPegasus Mail hat sein eigenes Backend zu Speicherung von E-Mails: Neue E-Mails werden als Datei mit der Endung CNM im Mailverzeichnis gespeichert und erscheinen im Posteingangsordner (New Mail Folder). Alle anderen Ordner bestehen aus zwei Dateien: Eine Datei mit der Endung PMM enthält alle Mails eines Ordners im ASCII-Format. Einzelne E-Mails werden durch das Steuerzeichen Ctrl-Z getrennt. Eine gleichnamige Datei mit der Endung PMI ist die Indexdatei zum entsprechenden Ordner.\n\nEs gibt eine rudimentäre Unterstützung des mbox-Ordnerformats. Da dieses Ordnerformat von sehr vielen E-Mail-Programmen – wie beispielsweise Thunderbird, Opera M2 – verwendet wird, eignet es sich zum Import und Export von E-Mails.\n\n\nDas kostenlos verfügbare Programm wurde 1989 für den Nachrichtenaustausch innerhalb des Novell-Netware-Netzes der neuseeländischen Universität von Dunedin entwickelt. 1990 machte es David Harris erstmals über einen FTP-Server weltweit verfügbar. In Kombination mit dem Mailserverprogramm Mercury war Pegasus Mail in Universitäts- und Firmennetzwerken lange Zeit sehr verbreitet. Das Programm lässt sich jedoch auch im Einzelbenutzermodus betreiben.\n\nVon 1993 bis 1995 wurde eine abgespeckte Version von Pegasus Mail unter dem Namen \"FirstMail\" im Bundle mit Novell Netware vertrieben.\n\nPegasus Mail gab es ursprünglich nur in englischer Sprache. 1993 wurde die erste Version für das Microsoft-Windows-Betriebssystem veröffentlicht. Mit Erscheinen der Windows-Version wurde Pegasus Mail dann von Anwendern in andere Sprachen übersetzt, so auch auf Deutsch.\n\nDie Entwicklung der Version für den Apple Macintosh wurde 1997 eingestellt. Auch die Programmversion für Microsoft Windows 3.x (16-bit) wird mittlerweile nicht mehr weiterentwickelt.\n\nDemgegenüber wird die wegen ihres geringen Ressourcenbedarfs auch auf sehr leistungsschwachen Rechnern einsetzbare DOS-Version bislang in größeren Abständen weiterhin gepflegt, auch wenn sich diese Pflege im Wesentlichen auf die Beseitigung festgestellter Fehler beschränkt hat. Pegasus Mail für MS-DOS liegt derzeit in der Version 3.50 vor und ist zu jeder Version von MS-DOS oder PC-DOS ab Version 5.0 kompatibel.\n\nIm am 14. Dezember 2005 freigegebenen Release 4.31 für die 32-bit-Betriebssysteme Microsoft Windows 9x/ME/NT 4.0/2000/XP wurde eine neue HTML-Rendering-Engine („BearHTML“) sowie Unterstützung für UTF-8-Zeichensätze, Schutzmechanismen vor Phishing-Angriffen u. v. m. implementiert. (Die vollständige Liste der Änderungen am Code gegenüber der Vorgängerversion umfasst mehr als 1500 Einträge.)\n\nIm Juni 2006 erschien Pegasus Mail 4.41 in vier Sprachen. Wichtigste Neuerungen waren der Baysche Filter mit dem Namen \"Spamhalter\" und eine globale Whitelist für alle in Pegasus Mail enthaltenen Filter. Windows 95 wird von dieser Version nicht mehr unterstützt.\n\nSeit Juni 2009 gibt es Version 4.51. Statt mit dem Borland-Compiler wurde der Programmcode nun mit Visual C++ übersetzt. Als zusätzliche Features gab es z. B. eine automatische Filterfunktion für Ordner oder optionalen Blocksatz für das Verfassen von Nachrichten.\n\nIn der im Februar 2011 erschienenen Version 4.61 wurde das Erscheinungsbild der Symbole gründlich überarbeitet. Es kann nun zwischen einer auf dem Internet Explorer basierenden HTML-Anzeige und dem bisherigen BearHTML gewählt werden. Zudem lässt sich einstellen, ab welcher Größe eigene Anhänge verweigert werden.\n\nDie im Juli 2011 erschienene Version 4.62 bietet vor allem eine verbesserte HTML-Darstellung, in der im Januar 2012 veröffentlichten Version 4.63 wurden zahlreiche Fehler beseitigt.\n\nSeit der im März 2014 freigegeben Version 4.70 wird OpenSSL für die Verschlüsselung eingesetzt, zudem wird Hunspell für die Rechtschreibprüfung in verschiedenen Sprachen genutzt.\n\nIm Juni 2018 erschien Version 4.73, deren wichtigste Neuerung das völlig neu überarbeitete Hilfesystem ist. Es ist das erste große Modul der kommenden Pegasus Mail Version 5, welches in das bestehende Programm integriert wurde. \n\nFür Version 5 sind eine vollständige Überarbeitung des Adressbuchs und ein neues Mailordner-Format geplant.\n\nDavid Harris kündigte 1993 seinen sicheren Job an der University of Otago in Dunedin und bestreitet seinen Lebensunterhalt seitdem ausschließlich mit der Entwicklung seiner Programme. Finanziert wurde die Entwicklung in erster Linie durch den Verkauf von Handbüchern und Supportverträgen sowie Fanartikeln und Spenden.\n\nSeit 2007 hat er sein Geschäftsmodell auf Spendeneinkommen bei Pegasus Mail und kommerzielle Lizenzen (nur bei kommerzieller Nutzung, bei nicht-kommerzieller Nutzung bittet der Autor um Spenden) für Mercury geändert.\n\n"}
{"id": "135900", "url": "https://de.wikipedia.org/wiki?curid=135900", "title": "NASDAQ", "text": "NASDAQ\n\nDie NASDAQ [] ist die größte elektronische Börse in den USA, gemessen an der Zahl der gelisteten Unternehmen. Der Name ist ein Akronym für \"National Association of Securities Dealers Automated Quotations\". Die Börse wurde 1971 von der National Association of Securities Dealers (NASD) als vollelektronische Handelsplattform gegründet und ist in der nordwestlichen Ecke des Condé Nast Buildings am Times Square von Manhattan in New York untergebracht.\n\nSeit Februar 2008 wird die Börse von der Nasdaq, Inc. betrieben. Den Wertpapierhandel an der NASDAQ überwacht die United States Securities and Exchange Commission (SEC).\n\nDie NASDAQ setzt sich aus dem NASDAQ National Market und dem NASDAQ SmallCap Market zusammen. Der Sitz der Hauptbörse ist in den USA, mit Börsen in Kanada und Japan. Es bestehen Verbindungen zu den Börsen in Hongkong und Europa.\n\nDie NASDAQ erlaubt es mehreren Marktteilnehmern, am Handel mittels des Electronic Communication Network (ECN) teilzunehmen. Das \"Small Order Execution System\" (SOES) ist ein weiteres NASDAQ-Merkmal, das 1984 eingeführt wurde, um auch bei höherem Handelsaufkommen die Ausführung aller Aufträge sicherzustellen.\n\nAm OTC Bulletin Board der NASDAQ werden die Aktien unter dem Symbol NDAQ gehandelt.\n\nDie Handelszeiten der NASDAQ sind von 9:30 bis 16:00 New Yorker Ortszeit (EST), was 15:30 bis 22:00 Uhr deutscher Zeit (MEZ) entspricht. Zusätzlich gibt es, wie an vielen elektronischen Handelsplätzen, erweiterte Handelszeiten, während derer das Handelsvolumen allerdings deutlich geringer ist. Diese sind von 7:00 bis 9:30 Uhr Ortszeit („\"pre-market session\"“) sowie von 16:00 bis 20:00 Uhr Ortszeit („\"after-market session\"“).\n\nAls der Handel am 8. Februar 1971 begann, war dies die erste elektronische Börse.\n\nIm Jahr 1998 erfolgte die Fusion mit der American Stock Exchange.\n\nSeit 1999 ist es die größte amerikanische Börse, in der die Hälfte der US-amerikanischen Aktiengesellschaften gelistet ist.\n\nDie NASDAQ wurde am 11. September 2001 aufgrund der Terroranschläge vorübergehend geschlossen.\n\nIm Jahr 2002 führte die NASDAQ Supermontage, kurz SUMO, als elektronisches System ein.\n\nIn dem größten Zivilprozess in der Geschichte der USA verurteilte ein Bundesgericht Dutzende von Brokerhäusern (darunter Merrill Lynch, Goldman Sachs, und Salomon Smith Barney) zu Zahlungen in Höhe von 1,03 Milliarden US-Dollar an geschädigte Investoren, die mit einem groß angelegten Plan mittels Preisfixierung betrogen wurden.\n\nAm 25. Mai 2007 wurde bekannt, dass sich die NASDAQ mit der schwedischen Börse OMX zusammenschließen wird. Die NASDAQ bot 2,73 Milliarden Euro für den Börsenbetreiber OMX. Nach der Fusion, die am 27. Februar 2008 abgeschlossen war, heißt das Unternehmen NASDAQ OMX Group und besitzt eine Marktkapitalisierung von rund 7,1 Milliarden US-Dollar. Die Übernahme der London Stock Exchange (LSE) scheiterte dagegen im Jahr 2006 am Widerstand des Managements der Londoner Börse.\n\nAm 17. Juli 1995 schloss der Nasdaq-Composite-Aktienindex mit 1.005,89 Punkten erstmals über der Grenze von 1.000 Punkten. Am 10. März 2000 markierte der Index mit 5.048,62 Punkten ein Allzeithoch und signalisierte den Anfang vom Ende des Dot-Com-Booms. Am 23. April 2015 erreichte der Index ein neues Allzeithoch nach mehr als 15 Jahren.\n\nAn der NASDAQ werden täglich rund zwei Milliarden Aktien (Einfachzählung) gehandelt. Das höchste Handelsvolumen ihrer Geschichte erzielte die Börse am 26. Juni 2009 mit 5.214.013.855 gehandelter Aktien.\n\nDer beste Handelstag war der 3. Januar 2001 mit einem Gewinn von 14,17 Prozent. Den schlechtesten Handelstag erlebte die NASDAQ am 19. Oktober 1987 mit einem Verlust von 11,35 Prozent.\n\nDie Tabelle zeigt die Tage mit den höchsten Volumina aller gehandelten Aktien (Einfachzählung) an der NASDAQ.\n\n\n\n"}
{"id": "136420", "url": "https://de.wikipedia.org/wiki?curid=136420", "title": "Toolbox", "text": "Toolbox\n\nDie Toolbox war eine Zeitschrift für Software-Entwickler, die alle zwei Monate in einer Auflage von etwa 15.000 Exemplaren erschien. Wichtige Themen waren Borland Delphi (seit 2008 auch regelmäßig Lazarus und Free Pascal), C++ und C#/.NET, Datenbanken/SQL, Java und die Programmierung für mobile Geräte. In unregelmäßigen Abständen erschienen auch Beiträge, die sich an Elektronik-Bastler richten oder theoretische Grundlagen aus der Informatik behandeln. Zu einigen Themen hatte die Redaktion der Toolbox auch Bücher im Verlag C&L herausgegeben.\n\nJeder Ausgabe lag eine CD-ROM mit allen Quellcodes und zusätzlicher Software, APIs und OpenSource-Paketen bei. Insbesondere enthielt die Toolbox-CD viele ausgewählte Komponenten für Delphi-Entwickler.\n\nVorläufer der Toolbox war die seit 1986 im DMV-Verlag erschienene Zeitschrift \"Pascal International\", die 1989 in \"Toolbox\" umbenannt wurde und sich vor allem an Programmierer auf dem PC richtete.\n\nDer Name Pascal wurde von Blaise Pascal hergeleitet. Die Umbenennung in Toolbox erfolgte, weil es, so die Begründung der Redaktion, zu der Annahme verleitet hätte, die Zeitschrift behandele nur die Programmiersprache Pascal. Von 1991 bis 1993 wurde sie unter dem Namen \"DOS Toolbox\" als Ergänzung der Zeitschrift \"DOS International\" (heute \"PC Magazin\") vertrieben. In den Jahren 1995 und 1996 behandelte sie für kurze Zeit verstärkt Themen aus dem damals aufkommenden Gebiet Multimedia und trug den Namen \"Toolbox & Multimedia\". Seit 1996 erschien sie wieder unter dem Namen \"Toolbox\" im eigenen Verlag.\n\nSeit dem 1. August 2010 erschien die Toolbox im Verlag \"Neue Mediengesellschaft Ulm mbH\". Neuer Chefredakteur war Max Bold. Nach einigen Ausgaben wurde die Zeitschrift im Juni 2011 komplett eingestellt.\n"}
{"id": "137634", "url": "https://de.wikipedia.org/wiki?curid=137634", "title": "Journaled File System", "text": "Journaled File System\n\nDas Journaled File System (JFS) wurde im Jahr 1990 von IBM für ihr eigenes Betriebssystem AIX veröffentlicht. Hintergrund war eine weitgehende Virtualisierung der Hardwareschicht in dieser damals neu vorgestellten Version 3 von AIX: Ein ebenso neu vorgestellter Logical Volume Manager (LVM) löste die starren Zugriffsschemata auf Datenträgern ab, ein neuer Speichermanager brachte die Virtualisierung des Speicherraumes, also die Auslagerung von Hauptspeicher auf eine (virtuelle) Festplatte, und die PowerPC-CPU Familie, die noch heute das Herzstück unter anderem der pSeries ist, wurde eingeführt. JFS für AIX sollte nicht mit dem Veritas File System verwechselt werden, das unter HP-UX ebenfalls als \"JFS\" bezeichnet wird.\n\nDas primäre Designziel von JFS war die stetige Konsistenz des Dateisystems: Änderungen am Dateisystem werden transaktionsorientiert geschrieben sowie in einem Journal protokolliert. Bei einem Absturz kann somit – ausgehend von einem Konsistenzpunkt der Transaktionen – über das Journal sehr effizient ein konsistenter Status des Dateisystems hergestellt werden. Ein voller Zugriff auf das Dateisystem ist also sehr schnell wieder erreicht. Im Fokus steht damit die Verfügbarkeit der Ressource \"Dateisystem\", nicht die Performance oder die Integrität der Dateiinhalte (das Journaling bezieht sich nur auf Änderungen im Dateisystem, also beispielsweise Dateieinträge in Verzeichnissen, und nicht auf den eigentlichen Dateiinhalt).\n\nDer LVM ist für die Skalierbarkeit des Dateisystems nützlich: im laufenden Betrieb und unter Last können einfach Festplatten in der Konfiguration ergänzt und in die Volume Group mit aufgenommen werden um das Dateisystem zu erweitern.\n\nFür das ebenfalls von IBM veröffentlichte Betriebssystem OS/2 wurde eine neue Generation des JFS entwickelt und im Jahr 2000 vorgestellt. Dieses JFS stellt eine Neu-Implementation des JFS dar, da der „historische“ JFS-Code stark an die pSeries-Architektur angelehnt ist (OS/2 läuft auf x86-Computern). Dieser neue JFS-Code wurde in AIX 5.1 als JFS2 importiert und 2002 von IBM unter der GNU General Public License freigegeben.\n\nDie wichtigsten Größenunterschiede:\n\nDarüber hinaus wurden Optimierungen für aktuelle Server-Hardware vorgenommen; so ist die Leistung von JFS2 etwas besser als die von JFS.\n\nEs wird zwar Linux unterstützt, aber die Defragmentierung wurde bislang noch nicht auf Linux portiert. Dies kann dazu führen, dass durch das Anlegen und Löschen vieler kleiner Dateien (einige KiB) das Dateisystem fragmentiert und vor allem die Schreibzugriffe sich etwas verlangsamen und eine höhere CPU-Last erzeugen. Aufgrund der Extent (engl. Ausdehnung, bestehend aus einem Adresse-Länge-Paar)-basierten Allokation von Dateiblöcken und einer intelligenten Allokationsstrategie, d. h. benachbarte Extents derselben Datei werden während des Änderns von Dateien verschmolzen (dies wird wahrscheinlicher, je fragmentierter das Dateisystem wird), bleibt der Fragmentierungsgrad aber unter einem bestimmten Verhältnis. Viele andere Datei- und Datenbank-Systeme verwenden eine ähnliche Extent-basierte Dateiblock-Allokation.\n\n\n"}
{"id": "138035", "url": "https://de.wikipedia.org/wiki?curid=138035", "title": "Bilddatei", "text": "Bilddatei\n\nEine Bilddatei ist eine Datei, in der ein digitales Bild gespeichert wird. Der Inhalt der Datei wurde entweder digital errechnet (vergleiche errechnetes Bild), oder durch Analog-Digital-Wandlung digitalisiert und kann daher vom menschlichen Betrachter unmittelbar weder als \"Bild\" erkannt, noch als Text gelesen werden (vgl. Textdatei).\n\nZur Visualisierung wird ein Gerät benötigt, das den Inhalt wieder in analoge Daten zurückwandelt (sog. Digital-Analog-Wandlung); ein solches Gerät ist der Computer, der mit Hilfe einer Bildbetrachtungssoftware das Bild am Monitor visualisieren kann.\n\nMan unterscheidet grundsätzlich zwischen einer Rastergrafik, die aus Bildpunkten (Pixeln) zusammengesetzt ist, und einer Vektorgrafik, die mit grafischen Primitiven beschrieben wird. Daneben werden verschiedene Grafikformate zur Speicherung der Bilddateien verwendet, beispielsweise JPEG, GIF, PNG, TIF oder das Rohdatenformat (RAW-Format) für Rastergrafiken bzw. WMF, EPS, CDR oder SVG für Vektorgrafiken. Bei Bilddateien einer Digitalkamera handelt es sich immer um Rastergrafiken.\n\n"}
{"id": "138222", "url": "https://de.wikipedia.org/wiki?curid=138222", "title": "Mailüfterl", "text": "Mailüfterl\n\nDas Mailüfterl war der erste Computer auf dem europäischen Festland, der vollständig mit Transistoren arbeitete. Die offizielle Bezeichnung lautete Binär dezimaler Volltransistor-Rechenautomat. Vorgestellt wurde er im Mai 1958. Die ersten Rechner dieser Art weltweit waren der TRADIC und der TX-0.\n\nDas Mailüfterl wurde ab 1955 an der TU Wien von Heinz Zemanek gebaut. Zu seinem Team gehörten unter anderem Peter Lucas, Georg J. Leser, Viktor Kudielka, Kurt Walk, Ernst Rothauser, Kurt Bandat und Norbert Teufelhart.\n\nDer Erbauer spielte hinsichtlich der Benennung in einem Zitat auf die in den USA in Betrieb genommenen Röhrenrechner an: „Wenn es auch nicht die rasante Rechengeschwindigkeit amerikanischer Modelle erreichen kann, die ‚Wirbelwind‘ oder ‚Taifun‘ heißen, so wird es doch für ein Wiener ‚Mailüfterl‘ reichen.“ \n\nDer Rechner besteht aus 3.000 Transistoren, 5.000 Dioden, 1.000 Montageplättchen, 100.000 Lötstellen, 15.000 Widerständen, 5.000 Kondensatoren und 20.000 Metern Schaltdraht. Mit einem Gewicht von rund 500 Kilogramm sowie einer Breite von 4 Metern, einer Höhe von 2,5 Metern und einer Tiefe von 50 Zentimetern war die Anlage gegenüber den zeitgenössischen Röhrenrechnern vergleichsweise klein. Das Mailüfterl hatte eine damals beachtliche Taktfrequenz von 132 kHz.\n\nZemanek sagte über sein Projekt später, es sei ein halb illegales Unterfangen eines kleinen Hochschulassistenten gewesen, das er ohne offizielle Genehmigung und somit auch ohne finanzielle Unterstützung der Universität mit einer Gruppe von Studenten realisierte. Er reiste 1954 zu Philips nach Holland, um dort wegen einer Sachspende vorzusprechen. Die Menge von 1.000 Transistoren und deren Einsatzzweck waren nur sieben Jahre nach Erfindung des Transistors schwer zu vermitteln. \n\nZemanek erhielt aber dennoch eine Zusage über 1.000 eher langsame Hörgerät-Transistoren und bekam schließlich von Philips insgesamt 4.000 hochwertige Transistoren (lediglich vier waren defekt, sie waren vermutlich beim Löten beschädigt worden).\n\nNach der Konstruktion der Hardware widmete sich die Gruppe von 1958 bis 1961 der Programmierung. Am 27. Mai 1958 berechnete das Mailüfterl in 66 Minuten die Primzahl 5 073 548 261.\n\nDer Rhythmus des Programmablaufes konnte über ein Radio abgehört werden. War nur noch ein Dauerton zu hören, wussten die Techniker, dass etwas nicht mehr stimmt. Das wurde genutzt, um über Telefon von zu Hause aus festzustellen, ob das Gerät aufwendige Rechenarbeiten über Nacht noch bearbeitete.\n\n1961 bot IBM dem Computerpionier an, ein Laboratorium in Wien aufzubauen, woraufhin Zemanek die gesamte Gruppe zu dem Konzern übersiedelte. IBM kaufte dem Staat das Mailüfterl ab, um es dem Labor, dem Zemanek bis 1976 vorstand, zur Verfügung zu stellen. IBM musterte den Rechner 1966 aus und übergab ihn 1973 dem Technischen Museum Wien.\n\nAm 1. Oktober 2013 veröffentlichte Google in einem Blog ein Video, um dieses Projekt zu honorieren.\n\n"}
{"id": "139362", "url": "https://de.wikipedia.org/wiki?curid=139362", "title": "Nascom", "text": "Nascom\n\nNascom 1 und 2 waren Bausätze für Einplatinencomputer, die in den Jahren 1977 bzw. 1979 vom englischen Unternehmen \"Nascom Microcomputers\" angeboten wurden. \n\nDie Computer bauten auf dem Prozessor Zilog Z80 auf und boten eine Tastatur- und eine Anzeigeschnittstelle (Video-Out mit 16 Zeilen mit je 48 Spalten). Neben einer seriellen Schnittstelle, die Daten nach dem Kansas City Standard (KCS) auf Band abspeichern konnte, gab es auch eine Z80 PIO mit zwei parallelen Schnittstellen mit je 8 bit.\n\nDie \"Nascom Microcomputers\" wurde nach finanziellen Schwierigkeiten 1981 an \"Lucas Logic\" verkauft. Parallel zu deren Arbeiten wurde von einer Gruppe um den Nascom-Gründer John Marshall die \"Gemini Microcomputers\" (auch \"Gemini Company/Microvalue\") gegründet. \n\nDer Vorgänger des erfolgreichen Turbo-Pascal-Compilers und der integrierten Entwicklungsumgebung (IDE) für CP/M und MS-DOS wurden für den Nascom unter der Bezeichnung \"Blue Label Software Pascal\" entwickelt und vertrieben. \n\nDas Emulator-System MESS simuliert Nascom-Computer auf moderner Hardware.\n\n\n\n"}
{"id": "139439", "url": "https://de.wikipedia.org/wiki?curid=139439", "title": "Apple I", "text": "Apple I\n\nDer Apple I war ein von Steve Wozniak entwickelter Personal Computer (PC). Als erstes Gerät der Welt war er mit 666 US-Dollar für Privathaushalte erschwinglich und zugleich von Haus aus mit allen benötigten Anschlüssen ausgestattet, um ihn auf moderne Weise per Tastatur und Monitor zu bedienen (statt der bislang üblichen Kippschalter und Lämpchen der damaligen Rechner des unteren Preissegments). Aus diesem Grund wird er auch als erster PC der Welt bezeichnet.\n\n1975 arbeitete Wozniak für den Computerhersteller Hewlett-Packard und erkannte, dass die Kosten für die Komponenten so weit gefallen waren, dass auch preisgünstige Heimanwendungen möglich sein müssten. Der Journalist Steven Levy beschreibt in seinem Buch \"Hackers – Heroes of the Computer Revolution\" Wozniak als genialen Computer-Hacker, der es schaffte, dank eines pfiffigen Designs und der sparsamen und ungewöhnlich effizienten Ausnutzung der Chips einen besonders preiswerten Computer zu entwickeln. Dank Schreibmaschinentastatur und Bildschirm (zunächst in Form eines umfunktionierten TV-Geräts) war er für die damaligen Verhältnisse leicht zu bedienen und zugleich für Privathaushalte erschwinglich und so für einen vollkommen neuen Markt verfügbar; es entstand mit dem Apple I der weltweit erste in Serie hergestellte Personal Computer (kurz PC). Erst Geräte wie dieses lösten das aus, was Levy in seinem Buch als \"Computer Revolution\" bezeichnet.\n\nDer Entwickler Steve Wozniak war ein prominentes Mitglied des Homebrew Computer Clubs, der als „Schmelztiegel für eine ganze Branche“ bezeichnet wurde und aus dem zahlreiche Computerunternehmen entsprungen sind. Eines der Unternehmen ist Apple, wobei Wozniak 1976 zusammen mit seinem Freund Steve Jobs und Ronald Wayne einer der Gründer ist. Sein Computer wurde zwar vor der Unternehmensgründung entwickelt, aber dann dort in Serie produziert und unter dem Namen Apple I verkauft. Zusammen mit dem Nachfolgemodell Apple II sind dies die letzten industriell hergestellten Computer, die von einem einzelnen Entwickler entworfen wurden.\n\nDer Apple I wurde am 1. April 1976 auf einem Treffen des Homebrew Computer Clubs vorgestellt und war ein Einplatinencomputer. Er war ausgestattet mit einer Videoschnittstelle (die auf einen schwarz-weißen Textmodus beschränkt war), 4 KiByte dynamischem RAM, einer Tastatur und dem Mikroprozessor 6502 von Rockwell International (eine Entwicklung von MOS Technologies). Das Video-System war auf eine sehr eigenwillige Weise aufgebaut, es nutzte Schieberegister als Bildschirmspeicher, da diese damals noch billiger waren als die gleiche Menge dynamisches RAM.\n\nDer eigentliche Rechner wurde von Apple als fertig bestückte Platine geliefert und musste vom Händler oder Besitzer zusammengesetzt werden, der außerdem noch ein Netzteil, eine Tastatur, einen Bildschirm und optional ein Gehäuse zukaufen musste. Als einziges Peripheriegerät gab es ein Kassetten-Interface, mit dem sich in Kombination mit einem herkömmlichen Kassettenrecorder Programme auf Audiokassetten speichern und von diesen wieder laden ließen. Nur mit diesem Interface ließ sich die Integer-BASIC-Programmiersprache nutzen, die damals noch Apple BASIC genannt wurde; denn der Interpreter musste von Kassette hinzugeladen werden, da er sich noch nicht im ROM befand. Ohne das Interface konnte der Computer lediglich über einen Maschinensprachemonitor programmiert werden.\n\nDie Idee des Apple I wurde nach der Vorstellung des Rechners von dem lokalen Computerhändler The Byte Shop aufgegriffen, der bei Wozniak und Jobs 50 Geräte bestellte. Diese 50 Geräte mussten aber voll aufgebaut geliefert werden – das war die Bedingung von Byte Shop.\n\nInsgesamt wurden über einen Zeitraum von zehn Monaten circa 200 Apple I zu einem Einzelpreis von 666,66 US-Dollar verkauft; danach wurde der Apple I durch den Apple II abgelöst, der zum Welterfolg wurde.\n\nBedingt durch die geringe Stückzahl und die Bekanntheit der Marke Apple, gehört der Apple I zu den gefragtesten Sammlerstücken im Computerbereich. Es sind 70 namentlich erfasste Exemplare bekannt, wobei die tatsächlich existente Stückzahl deutlich darüber liegen dürfte. Wie viele tatsächlich noch funktionsbereit sind, lässt sich nicht sagen, zumal viele Besitzer das möglicherweise fragile Sammlerstück aus Angst vor Schäden nicht mehr in Betrieb nehmen wollen. In renommierten Auktionshäusern werden Apple-I-Computer regelmäßig für mehrere 100.000 Euro versteigert (2010 bei Christie’s für ca. 157.000 Euro, Juni 2012 bei Sotheby’s für 300.000 Euro, November 2012 bei Auction Team Breker für fast 500.000 Euro und im Mai 2013 in Köln für 516.461 Euro). Im Oktober 2014 zahlte das Henry Ford Museum bei einer Versteigerung in New York City für einen Apple I 905.000 US-Dollar (708.000 Euro). Am 20. Mai 2017 versteigerte das Auktionshaus Breker ein weiteres, noch betriebsbereites Exemplar des Apple I inklusive Handbuch, Originalrechnung und Herkunftsnachweis für 110.000 Euro. Hier war im Vorfeld der Versteigerung der Schätzpreis mit einer Höhe zwischen 180.000 und 300.000 Euro beziffert worden. Laut einem IT-Experten gibt es weltweit derzeit nur noch acht funktionsfähige Geräte.\n\nChristie's versteigerte im Juni 2016 einen Apple-1 für 355.500 USD. Charitybuzz im September 2017 für 401.000 USD und Bonhams im Dezember 2017 für 372.000 USD.\n\nIm Mai 2015 wurde bekannt, dass eine Frau aus der San Francisco Bay Area bereits Wochen zuvor einen Apple I zusammen mit anderem Elektronikschrott bei einem Recyclingunternehmen abgegeben hatte. Nach dem Tod ihres Mannes hatte sie das Haus aufgeräumt und war sich offensichtlich nicht über den Wert des Gerätes im Klaren. Das Recyclingunternehmen glaubte zunächst, es sei ein Nachbau bzw. eine Fälschung, verkaufte das echte Gerät dann aber für 200.000 USD an einen privaten Sammler. Der unbekannten Frau wurde öffentlich angeboten, dass sie vorbeikommen und sich einen Scheck über wenigstens die Hälfte des Erlöses abholen könne.\n\nBob Luther, Autor von \"The First Apple\", bezeichnet das Gerät als „Heiligen Gral“ für Techniksammler, wenn man bedenkt, dass von den 200 gebauten Exemplaren die erste Charge von 50 Stück in der Garage von Jobs Eltern entstand.\n\nEin kompatibler Nachbau des Apple I, genannt Replica I, wurde 2003 von Vince Briel gebaut. Als Ersatz für die TTL-Bausteine, von denen einige nicht mehr erhältlich sind, wurden zwei Mikrocontroller verwendet. Der Verkaufspreis des Bausatzes beträgt etwa 160 US-Dollar. Die Verwendung der Originalsoftware wurde von Steve Wozniak genehmigt.\n\n2006 wurde ein weiterer Apple-I-Nachbau vorgestellt, der \"A-ONE\". Er ist eine Entwicklung von Franz Achatz und San Bergmans, die beim \"Circuit Cellar Atmel AVR Contest\" 2006 in den Vereinigten Staaten als exzellente Entwicklung ausgezeichnet wurde.\n\n\n\n"}
{"id": "139473", "url": "https://de.wikipedia.org/wiki?curid=139473", "title": "Apple DOS", "text": "Apple DOS\n\nApple DOS (Apple Disk Operating System) ist ein 1978 veröffentlichtes Disketten-Betriebssystem von Apple, das auf den Apple-II-Computern eingesetzt wurde. Im Apple-Kontext wurde es meist einfach DOS genannt (ähnlich wie MS-DOS bei IBM-PC-kompatiblen Computern). Nachfolger wurde Ende 1983 das vielseitigere Apple ProDOS, das sich in weiten Teilen nicht an Apple DOS, sondern an Apple SOS orientierte, dem Betriebssystem des gescheiterten Apple III.\n\nDer Apple II wurde im Frühjahr 1977 auf den Markt gebracht. Zunächst diente die herkömmliche Compact Cassette als Speichermedium, indem ein handelsüblicher Kassettenrekorder über die Mikrofon- und Kopfhörerbuchsen an den Apple angeschlossen wurde; diese Speichermethode war wegen der nötigen Modulation in Tonsignale langsam, in der Bedienung unbequem, systembedingt unzuverlässig und für mehrteilige Programme kaum praktikabel. So erkannten Steve Wozniak und Steve Jobs, dass ein Laufwerk für Disketten wichtig für die Entwicklung ihrer Computer werden würde. Zu jener Zeit waren die erst wenige Jahre vorher erfundenen Diskettenlaufwerke im Microcomputer-Bereich noch teurer Luxus und für viele Computermodelle überhaupt nicht verfügbar.\n\nUm ein Diskettenlaufwerk, das spätere Disk II, ansteuern zu können, entwarf Wozniak dessen Hardware und als entsprechende Software Apple DOS. Die hardwarenahen Routinen des DOS für die Steuerung der Laufwerksmotoren sowie für die Umwandlung des GCR-Datenstroms in einzelne Datensektoren (und umgekehrt) schrieb er selbst, die Routinen für das Dateisystem wurden als Auftragsarbeit vergeben. Apple DOS wurde beim Systemstart mit einem einfachen Bootloader, der sich im Festwertspeicher (ROM) des Laufwerkscontrollers befand, von einer Diskette geladen. Es integrierte sich in das Apple Integer Basic, das im ROM des Computers gespeichert war.\n\nDie DOS-Versionen 0.x, 1.x, 2.x und 3.0 waren unveröffentlichte Testversionen; das erste öffentliche Apple-DOS war Version 3.1 vom Juli 1978. Etwa ein halbes Jahr später erschien DOS 3.2, das den neuen Apple II+ unterstützte und eine hohe Zahl von Programmfehlern beseitigte; kurz darauf DOS 3.2.1, eine weitere fehlerbereinigte Ausgabe. Die am weitesten verbreitete Apple-Version ist DOS 3.3 vom August 1980, die den Speicherplatz je Diskette von 113 auf 140 KB erhöhte, indem es durch eine verbesserte GCR-Codierung 16 statt 13 Sektoren auf jeder Diskettenspur unterbrachte. Danach wandte sich Apple dem Apple III zu, sodass über zweieinhalb Jahre kein neues DOS für den Apple II mehr erschien, obwohl weiter eine Anzahl bekannter Programmfehler im DOS-Code existierte; insbesondere der Befehl \"APPEND\" funktionierte oft nicht richtig.\n\nIn dieser Zeit erschienen einige stark beschleunigte Apple-DOS-Versionen von Drittanbietern. Diese ersetzten die ineffektive GCR-Codierung der Originalversion, bei der die Daten mehrfach im Speicher umkopiert und dann erst auf die Diskette geschrieben wurden, durch eine optimierte Fassung, die schon während des Schreibens bzw. Lesens einen Großteil der nötigen Arbeit erledigte. Bekannte DOS-Versionen dieser Periode sind u. a. \"ProntoDOS\", \"DaviDOS\" und \"ES-DOS\".\n\nNach dem Misserfolg des Apple III wandte sich Apple wieder der weiterhin gut laufenden Apple-II-Serie zu. Im Januar und August 1983 erschienen zwei korrigierte und besser an den neuen Apple IIe angepasste, aber weiterhin langsame, Versionen von Apple DOS. Beide trugen weiterhin die Versionsnummer 3.3, können aber an der Jahresangabe 1983 in der Startmeldung erkannt werden. Die Version von August 1983, die allerdings in der Startmeldung weiterhin „Januar 1983“ ausgab, war zugleich das letzte unter dem Namen \"Apple DOS\" vertriebene Betriebssystem, und das einzige, in dem der \"APPEND\"-Befehl fehlerfrei funktionierte.\n\n2013, 35 Jahre nach dem Apple II Veröffentlichung, wurde der Apple DOS Quelltext vom Computer History Museum und seiner Webseite veröffentlicht. Paul Laughton, der Programmierer, hatte ihn zur Verfügung gestellt.\n\nApple DOS unterstützte in unveränderter Form keine Speichermedien außer 5,25-Zoll-Disketten, war ohne weitgehende Veränderungen ungeeignet für Datenträger mit mehr als 400 KB Kapazität und bot keine Unterverzeichnisse. Sein System weniger festgelegter Dateitypen war für viele Zwecke zu unflexibel. Zudem mangelte es an einer dokumentierten Programmierschnittstelle zum DOS für Maschinensprachen-Programme, da Apple DOS ganz auf BASIC-Programme hin ausgelegt war. Daher wurde die Weiterentwicklung eingestellt, als größere Datenträger wie Festplatten und 3,5-Zoll-Disketten erschwinglich wurden.\n\n\n\n"}
{"id": "139486", "url": "https://de.wikipedia.org/wiki?curid=139486", "title": "Apple Pascal", "text": "Apple Pascal\n\nApple Pascal war eine Implementierung von UCSD Pascal, dem P-Code der University of California, San Diego (UCSD) für den Apple II. Pascal war in den 1970er und frühen 1980er Jahren sehr populär; daher wurde aus UCSD Pascal II.1 eine Variante für den Apple II abgeleitet. Zwei UCSD-Studenten, Mark Allen und Richard Gleaves, entwickelten im Sommer 1978 einen Interpreter für den im Apple II verwendeten Mikroprozessor 6502, welcher später die Grundlage für \"Apple Pascal\" wurde, das 1979 herausgegeben wurde und fünf Jahre lang ein Produkt von Apple blieb.\n\nUCSD Pascal ist von Roger T. Sumner am Institute for Information Systems der UCSD entwickelt worden, dort sind fünf Versionen veröffentlicht worden:\n\nVon Apple Pascal sind vier Versionen veröffentlicht worden:\n\n\nAb Version 1.2 wurden nur noch 128K-Systeme unterstützt.\n\nDie direkte wirtschaftliche Bedeutung des Apple-Pascal-Betriebssystems ist eher gering einzustufen:\n\n\nDer von Niklaus Wirth entwickelte Standard wurde im UCSD Pascal um einige Sprachelemente erweitert, z. B. um unit-Befehle (unitread, unitwrite, unitstatus), um direkt mit der angeschlossenen Hardware kommunizieren zu können. Um mit knappem Hauptspeicherplatz haushalten zu können, bestand die Möglichkeit, das Programm zu segmentieren. Apple Pascal unterscheidet sich im Sprachumfang nur unwesentlich von UCSD Pascal.\n\n"}
{"id": "139507", "url": "https://de.wikipedia.org/wiki?curid=139507", "title": "Apple SOS", "text": "Apple SOS\n\nApple SOS war das Betriebssystem des 1980 von Apple Computer auf den Markt gebrachten Apple III und somit Nachfolger des dem Apple II laufenden Apple DOS. Es war für professionelle Anwender konzipiert.\n\nDie Abkürzung SOS bezieht sich auf den internen Arbeitstitel des Projekts Apple III, „Sara“, nach der Tochter des Chef-Entwicklers Wendell Sander. Zur Markteinführung wurde SOS dann offiziell als Backronym von \"Sophisticated Operating System\" \"(„Hochentwickeltes Betriebssystem“)\" umgedeutet. „SOS“ wurde wie ein Wort \"(sauce)\" ausgesprochen, nicht wie die Bezeichnung des Notsignals SOS.\n\nSOS wurde mit den \"Apple III System Utilities\" bedient, die aus der \"Device Handling Commands Section\", der \"File Handling Commands Section\" und dem \"System Configuration Program (SCP)\" bestanden. Eine Neuerung von SOS waren Gerätetreiber, mit denen neben 5¼-Zoll-Disketten auch Festplatten und RAM-Disks verwendet werden konnten.\n\nEine Programmdiskette bestand aus einem Kernel \"SOS.kernel\", einem Interpreter \"SOS.Interp\", was ein Interpreter (BASIC, Pascal oder COBOL) oder die Anwendung selbst sein konnte, sowie einem Satz Treiber \"SOS.Driver\".\n\nSOS war für seine Zeit äußerst fortschrittlich: Es bot zum Beispiel eine sehr flexible Konfiguration sowie ein hierarchisches Dateisystem und beeinflusste somit auch spätere Produkte von Apple. So entstand das HFS des späteren Macintosh aus dem SOS-Dateisystem. Das Design des Betriebssystems selbst hatte auch Auswirkungen auf dessen Nachfolger Apple ProDOS.\n\nTrotz aller Vorteile war die Kombination aus Apple III und SOS kein Erfolg: So gut wie alle damaligen Programme liefen unter Apple DOS 3.2 oder 3.3 und wurden von SOS nicht nativ unterstützt; Apple-DOS-Programme konnten nur ausgeführt werden, wenn der Apple III auch mit Apple DOS gebootet wurde. Viele Anwender kamen außerdem mit der hohen Flexibilität und den vielen Fähigkeiten des Systems nicht zurecht. Da der Apple III insgesamt einen schlechten Ruf besaß und teuer war, scheiterte mit ihm auch Apple SOS.\n\n\n\n"}
{"id": "139532", "url": "https://de.wikipedia.org/wiki?curid=139532", "title": "Apple ProDOS", "text": "Apple ProDOS\n\nApple ProDOS ist ein Betriebssystem für Computer der Apple-II-Baureihe. Es wurde in Version 1.0 im Oktober 1983 als Ersatz für Apple DOS 3.3 von Apple Computer herausgegeben, basierte aber stärker auf Apple SOS als auf Apple DOS.\n\nFür BASIC-Programmierer änderte sich im Vergleich zu Apple DOS relativ wenig (einige neue Befehle), auch wenn der innere Aufbau des Systems ein ganz anderer war. Das neue Betriebssystem gab aber nun auch Assemblersprache-Programmierern bessere Entwicklungsmöglichkeiten zur Hand, da es eine einheitliche Einsprungadresse mit allen nötigen Parametern bot – eine Technik, wie sie heute bei allen Betriebssystemen gängig ist. Programmierer, die Assemblersprache verwendeten, konnten sich damit auf einen Standard stützen, statt wie bisher das DOS 3.3 auf nicht dokumentierte Weise zu manipulieren und dabei immer Inkompatibilität mit zukünftigen Versionen zu riskieren. Weiterhin waren eine bessere Interrupt-Behandlung und ein schnellerer Diskettenzugriff gegeben. Die einzige wesentliche Eigenschaft von Apple DOS, die in ProDOS nicht beibehalten wurde, war dessen Unterstützung für das älteste Apple-II-Modell und für die veraltete Integer-BASIC-Programmiersprache dieses Modells; es war also nun mindestens ein Apple II+ bzw. Apple II europlus erforderlich, außer man wollte überhaupt keine BASIC-Programme benutzen. Auch der Arbeitsspeicher-Bedarf von ProDOS war höher; lief DOS schon ab 20 KB RAM, so benötigte ProDOS für Maschinenspracheprogramme 48 KB, für die Nutzung zusammen mit BASIC 64 KB RAM. Ein Speicher von 64 KB war aber 1983 praktisch bei allen noch laufenden Apple-II-Rechnern durch Nachrüstung bereits gegeben.\n\nAbgesehen davon hatte \"Apple ProDOS\" ein relativ hochentwickeltes hierarchisches Dateisystem mit Eigenschaften wie multiplen logischen Laufwerken auf einem physischen Laufwerk, Unterstützung von bis zu 20 verschiedenen Dateitypen und 8 gleichzeitig geöffneten Dateien. 140 KB 5¼-Zoll-Disketten wurden weiterhin unterstützt, der Zugriff auf diese war weitaus schneller als mit Apple DOS. Zusätzlich wurden nun auch Festplatten mit bis zu 32 MB je Partition unterstützt. Alle Laufwerke außer den überkommenen 5¼-Zoll-Diskettenstationen enthielten jetzt jeweils einen eigenen, standardisierten Treiber in der Firmware ihrer jeweiligen Adapterkarten, wodurch später ohne weitere Modifikation des ProDOS-Kerns auch verschiedene 3½-Zoll-Diskettenlaufwerke und CD-ROM-Laufwerke unter ProDOS nutzbar wurden.\n\nAls im September 1986 der 16-bittige Apple IIgs herauskam, teilte sich \"Apple ProDOS\" in die Zweige Apple ProDOS 8 (für 8-Bit-Mikroprozessoren) und Apple ProDOS 16 (für 16-Bit-Mikroprozessoren). Letzteres ging schon bald in das grafische Betriebssystem GS/OS über.\n\nDie letzte offizielle Aktualisierung für das Betriebssystem ist anno 1993 erschienen, das gleiche Jahr in dem die Produktion der Apple-II-Reihe endgültig eingestellt wurde. Die Version 2.0.3 verlangt mindestens einen Apple IIc oder einen „enhanced“ Apple IIe, die mitgelieferten Utility-Programme verlangen zudem mindestens 128 KB Speicher. Auf dem Apple II+ und dem nicht-„enhanced“ IIe läuft maximal die Version 1.9.\n\nAm 15. September 2016 veröffentlichte der Entwickler John Brooks ProDOS 8 2.4. Zu den wichtigsten Neuerungen zählt \"Bitsy Bye\", eine Art Startmenü, mit dem es sich in den Verzeichnissen von Disketten stöbern lässt und auch der Start von Programmen möglich ist. Dazu kommen auch eine Reihe kleinerer Werkzeuge, wie etwa ein Basic-Compiler namens \"MiniBas\" oder ein Tool zum Verschieben von Dateien zwischen Speichermedien.\n\n\n\n"}
{"id": "139551", "url": "https://de.wikipedia.org/wiki?curid=139551", "title": "Lisa OS", "text": "Lisa OS\n\nLisa OS (\"Lisa Office System\") war nach dem Xerox Star (1981) das zweite kommerzielle Betriebssystem mit einer komplett grafischen Benutzeroberfläche.\n\nEs wurde 1983 ausschließlich auf der Apple Lisa von Apple eingesetzt. Es brachte einfaches Multitasking und virtuellen Arbeitsspeicher zum Einsatz, damals extrem fortschrittliche Funktionen für einen Personal Computer. Jedoch ließen die Benutzung von virtuellem Speicher zusammen mit dem relativ langsamen Diskettensystem die Leistung des Systems manchmal sehr langsam erscheinen. Vom Konzept her war Lisa OS an das des Xerox Alto (1973) angelehnt, aus dem auch der Xerox Star hervorging. Lisa hatte vor allem zwei Benutzungsmodi: das grafische Officesystem und den textbasierten Workshop.\n\nDer Workshop war eine fast vollständig textbasierte Programmierumgebung, die jedoch einen Editor mit grafischer Benutzeroberfläche einsetzte.\n\nDas Officesystem kam mit einem Tabellenkalkulationsprogramm (LisaCalc und LisaList), einem Programm zur Grafikerstellung (LisaGraph), einem Projektverwaltungsprogramm mit integriertem Program evaluation and review technique/Gantt-Diagramm (LisaProject), einem Zeichenprogramm (LisaDraw) und optional einem DEC VT/ANSI Terminalemulator (LisaTerminal) aus.\n\nNachdem der Computer jedoch zu teuer war und der Absatz der Lisa sich nicht einstellen wollte, führte letztlich der Misserfolg von Lisa und Lisa OS zur Entwicklung des Macintosh und der Macintosh System Software – ohne Multitasking und virtuellen Speicher.\n\nAls Macintosh XL wurden große Teile von Lisa weiter verwendet, weshalb er auch als Lisa 2 angesehen werden kann. Der Macintosh XL ist zwar voll mit Lisa OS kompatibel, kann jedoch das Nachfolgebetriebssystem Macintosh System Software nicht nativ ausführen – das mitgelieferte Emulationsprogramm MacWorks XL (bzw. dessen neuere Version MacWorks Plus und Plus II) erlaubt es jedoch auf dem Macintosh XL System 1.1 bis 7.5.5 auszuführen.\n\n"}
{"id": "139574", "url": "https://de.wikipedia.org/wiki?curid=139574", "title": "GS/OS", "text": "GS/OS\n\nGS/OS war ein Betriebssystem, welches für den Apple IIgs von Apple Computer eingeführt wurde und einen weiteren Schritt von Apple ProDOS in Richtung des moderneren Mac OS Classic darstellt. Die grafischen Fähigkeiten des Apple IIgs waren so weit entwickelt, dass erstmals eine grafische Benutzeroberfläche für einen Apple II entwickelt werden konnte. GS/OS war das erste farbige GUI von Apple; die Macintosh-Modelle dieser Zeit hatten alle noch einen Schwarz-Weiß-Monitor.\n\n\"GS/OS\" erlaubt den Zugriff auf verschiedene Dateisysteme wie Apple Pascal, Apple DOS 3.3, ISO/High Sierra, HFS, MS-DOS, Apple ProDOS und AppleShare. AppleShare erlaubt \"GS/OS\" einen Netzwerkzugriff über AppleTalk auf AppleShare Fileserver. \"GS/OS\" konnte sogar über ein Netzwerk geladen und gestartet werden. Allerdings kann ein IIgs mit GS/OS nicht selbst als Server im Netzwerk genutzt werden, diese Rolle muss ein Macintosh-Computer übernehmen.\n\nDie letzte Version von \"GS/OS\" war 4.02, sie wurde mit den Apple IIgs System Disks Version 6.0.1 ausgeliefert.\n"}
{"id": "140237", "url": "https://de.wikipedia.org/wiki?curid=140237", "title": "WebObjects", "text": "WebObjects\n\nWebObjects [] ist eine Entwicklungs- und Serverumgebung für Webanwendungen. Sie wurde ursprünglich von NeXT entwickelt und bei deren Aufkauf 1996 von Apple mit übernommen.\n\nDie Software führt Daten aus verschiedenen Quellen – oft relationalen Datenbanken – zusammen, präsentiert sie Nutzern über das Web und ändert sie gegebenenfalls aufgrund von Benutzeraktionen. Sie zeichnet sich unter anderem durch eine strikte Trennung von Datenhaltung, Verarbeitungsprozessen und Benutzeroberfläche aus – entsprechend dem Model-View-Controller-Entwurfsmuster. Die ursprüngliche Implementierung in Objective-C wurde in der Version 5.0 durch eine in Java ersetzt.\n\nBei der Vorstellung der 1.0 Version war WebObjects im Jahre 1995 einer der ersten Applikationsserver überhaupt. Es zeichnet sich durch eine hohe Integration der Entwicklungswerkzeuge aus. Damit stellt es ein klassisches Werkzeug des Computer-aided software engineering (CASE) dar, mit dem leistungsfähige und komplexe netzbasierte Applikationen erstellt und betrieben werden können.\n\nBekannte Beispiele für die Verwendung von WebObjects sind unter anderem der Apple Webshop und iTunes Music Store, das Formularmanagement-System openforms sowie nuLiga, die Ligaverwaltung deutscher und österreichischer Tennis-, Tischtennis- und Badminton-Verbände.\n\nWebObjects ist zwar nicht open source, aber im Prinzip kostenlos zu verwenden. Denn die Nutzungsrechte sind an die von Xcode geknüpft. Xcode ist die IDE von Apple und ist ebenfalls an und für sich kostenlos. Um wiederum Xcode nutzen zu dürfen wird lediglich eine Mac OS X Lizenz benötigt.\nDas von Apple empfohlene Werkzeug zur Entwicklung von WebObjects-Software ist WOLips. WOLips wird im Rahmen eines Open-Source-Projektes entwickelt. In einem weiteren Open-Source-Projekt namens Project Wonder (siehe Weblinks) werden wiederverwendbare Komponenten für die WebObjects-Entwicklung erstellt. Die Entwicklung von WebObjects wurde mit dem letzten Update 2008 eingestellt, seit 2009 ist WebObjects nicht mehr in Xcode enthalten.\n\nDie Ideen hinter WebObjects wurden von verschiedenen Open-Source-Projekten aufgegriffen. So existiert GNUstepWeb als freie Implementierung von WebObjects 4.5 in Objective-C (wie auch WebObjects bis zu dieser Version), des Weiteren SOPE, ein Framework, welches das Konzept von WebObjects außerdem um verschiedene Ideen von Zope erweitert. Als Java-Implementierungen wären sowohl Wotonomy als auch Tapestry und Cayenne zu nennen. Ersteres hält sich genauer an sein Vorbild, WebObjects 5.x, während die letzteren beiden eher von den grundlegenden Ideen hinter WebObjects inspiriert sind (ähnliche Software Design Pattern) und jeweils nur einen Teil von WebObjects umfassen: Tapestry ähnelt sehr stark dem Darstellungslayer von WebObjects (JavaWO* und JavaWeb* Frameworks) während sich Cayenne um den objektrelationalen Teil (JavaEO* Frameworks) kümmert. Beide Frameworks lassen sich gut in Kombination (und auch mit den jeweiligen WebObjects Gegenstücken kombiniert) einsetzen.\n\n"}
{"id": "141007", "url": "https://de.wikipedia.org/wiki?curid=141007", "title": "CADdy (Software)", "text": "CADdy (Software)\n\nCADdy ist ein CAD-Programm der ehemaligen ZIEGLER Informatics GmbH und wird heute von DataSolid, IGE+XAO-Group und Wenninger Geoinformatik weitergeführt.\n\nDie Software wird für die mechanische Konstruktion in 2D/3D, sowie Lösungen für Computer Aided Manufacturing (CAM), Elektrotechnik, Finite-Elemente-Methode (FEM), GIS (graf. Informationstechnologie) und Produktdaten-Management (PDM) eingesetzt.\n\nEs existieren verschiedene Module, u. a. für:\n\nAuch 3D-Module sind erhältlich, allerdings basiert die alte CADdy-Version (\"CADdy classic\") auf DOS-Technologie, daher ist die 3D-Funktionalität und die Objektorientierung recht altbacken. Auch die Bedienung entspricht nicht den gewohnten Windows-Programmen.\n\nAb etwa 1992 wurden für verschiedene Anwendungsbereiche unter der Bezeichnung \"CADdy++\" echte Windows-Programme entwickelt.\n\n\"CADdy Classic Vermessung\" wird vom ursprünglichen Lizenzgeber, dem Ing.-Büro für Geoinformatik aktiv weiterentwickelt. Module sind vorhanden für u. a. vermessungstechnische Berechnungen, Plangestaltung und CAD, DGM, Straßenbau, Kanalbau und Verwaltung.\n\n\"CADdy GIS/Kartografie\" ist ein fortschrittliches grafisches Informationssystem auf Windowsbasis, welches sich für die Verwaltung von grafischen Informationen, Dokumenten zur Routenplanung und Zielführung und vor allem zur Erstellung von individuellen Karten eignet.\n\nUnter \"CADdy++ electrical\" wird die CAD-Software für Elektrotechnik durch die IGE+XAO-Group weiterentwickelt, betreut und vertrieben. Diese Version ist eine echte Windows-Applikation und moderner als die Version \"CADdy Elektrotechnik\". Es gibt Einsteigerversionen mit eingeschränktem Funktionsumfang. Diese reichen für einfache Schaltpläne aus. CADdy++ electrical ist in drei Ausbaustufen erhältlich, \"CADdy++ basic\", \"CADdy++ Standard\" und \"CADdy++ Advanced\". Des Weiteren gibt es für \"CADdy++ electrical\" mehrere Module wie z. B. Schaltschrankaufbau, Elektroinstallation und einen CADdy++ electrical Viewer.\n\nDas Produkt \"CADdy++ Maschinenbau\" und \"CADdy++ basic\" wird seit 2001 von der Firma DataSolid weiterentwickelt und vertrieben. CADdy++ Maschinenbau ist ein 2D/3D-Paket, welches auf dem ACIS-Kern von Spatial aufbaut und so Kompatibilität der 3D-Modelle mit vielen anderen CAD-Systemen ermöglicht. Die Einsteigerversion CADdy++ basic ist ein zu CADdy++ Maschinenbau kompatibles universelles CAD-System.\n\nFür CADdy++ Maschinenbau gibt es die Ausbaustufen \"CADdy++ basic\", \"CADdy++ economy\" und \"CADdy++ professional\".\n\nEin weiteres Produkt der \"CADdy++ Maschinenbau\"-Serie ist \"CADdy++ PDM\". Das Produktdaten-Management-System ist speziell für Anwender von \"CADdy++ Maschinenbau\" und ist für die übersichtliche Verwaltung aller Produkt- und Engineering-Daten sowie Office-Dokumente entwickelt.\n\n"}
{"id": "141124", "url": "https://de.wikipedia.org/wiki?curid=141124", "title": "Netzkunst", "text": "Netzkunst\n\nNetzkunst ist ein Sammelbegriff für künstlerische Arbeit in Netzen oder Netzwerken. Darunter fallen als Kunstwerk definierte soziale Netze, die künstlerische Nutzung analoger Netze wie ursprünglich bei Mail Art, sowie künstlerische Arbeiten, die digitale Netzdienste wie das World Wide Web oder andere Kommunikationsnetze wie Mobilfunk benutzen.\n\n\n\n\n\nNetz und Netzwerk werden im Deutschen unterschieden. Der Begriff Netz tendiert wie bei Stromnetz oder Telefonnetz dazu, einen technischen Aspekt zu beschreiben. Netzwerk ist der deutsche Begriff für „netzartiges Gefüge“ (s. Wahrig 1968). Allerdings wird oft fälschlich der englische Begriff \"network\" übernommen wo \"Netz\" die bessere Übersetzung ist.\n\nIm übertragenen Sinne „netzartige Gefüge“ oder Netzwerke sind beispielsweise Soziale Gefüge, Beziehungsgefüge und Psychologische Zusammenhänge mit vielen Variablen, oder das Denken selbst. Solche Gefüge oder Netzwerke verändern und reproduzieren sich unter günstigen Bedingungen nach eigenen Regeln, die kaum linear, eher chaostheoretisch fassbar sind. Netzwerk kann daher in sozialwissenschaftlichen Texten, ebenso wie bei Kunst mit Netzwerken, der treffende Begriff sein.\n\nObwohl es in allen Fällen um Vernetzung geht, kann es für das medientheoretische Verständnis unverzichtbar sein, zwischen Kunst im Netz und Kunst mit Netzwerken zu unterscheiden. Wo sich komplexe Beziehungen zwischen Gegenständen und Menschen gleichzeitig als bewegliches, drei- und mehrdimensionales Netz, oder als soziales Netzwerk beschreiben lassen, das technische Hilfsmittel einsetzt, ergänzen sich diese verschiedenen Betrachtungsweisen.\n\nDa beide Blickwinkel zudem in Beziehung zu theoretischen Netzwerk- und Netzbegriffen gesetzt werden, öffnet sich ein weites Feld, in dem kreative Varianten künstlerischer Arbeit durch sprachliche und theoretische Missverständnisse begünstigt werden. Die Inflation der Begriffe „Netz“ und „Netzwerk“ legt nahe, zu prüfen, ob abgeleitete Ausdrücke sinnvoll sind:\n\nNetzparadigma meint bei „Kunst im Netz“ ein Vorstellungsmuster, wie das Netz technisch oder organisatorisch beschaffen ist oder sein könnte, um damit Netzkunst herstellen zu können. Der vermeintlich ähnliche Begriff Netzwerkparadigma bei „Kunst mit Netzwerken“ ist sinnvoll anwendbar nur mit durchdachtem medientheoretischen Hintergrund (wie z. B. in Manuel Castells \"Das Informationszeitalter\").\n\n\n\n\nNetzkunst ist manchmal gleichzeitig an Netze und Netzwerke gebunden: Mail Art entstand durch künstlerische Initiativen in einem kreativen Prozess auf Grundlage technische Netze von Post- und Telekommunikationsdienste und bestehender gesellschaftlicher Netzwerke als soziales, kommunikatives und künstlerisches Netzwerk und entwickelte sich nach eigenen, teils sogar selbst ausformulierten Gesetzen weiter. Wenn Teilnehmer telematischer Netze durch ständige kommunikative Prozesse Netzwerke kreieren und weiter verändern, beispielsweise eine „Online-Community“ können sie damit zu Netzwerkern werden.\n\n\"Kunst auf dem Netz\" dagegen ist keine Netzkunst. Sie nutzt das Netz (Internet) wie beliebige andere Medien. Dazu zählen Projekte und Werke analoger oder digitaler Kunst, die auf Webseiten vorgestellt werden, im Prinzip jedoch ohne das WWW möglich wären. Eine künstlerische Auseinandersetzung mit Netz oder Netzwerk findet dabei nicht oder nur in oberflächlicher Weise statt. Netzkunst liegt nicht vor bei: Angewandter Kunst mit Webseiten; Abbildungen von Kunstwerken auf Webseiten; Verwendung des Begriffs ‚Netzkunst‘ oder ‚Netart‘ aus Statusgründen. Ebenso sind Netzwerkbeziehungen zwischen Künstlern nicht automatisch Netzkunst: Sie müssen als Kunstwerk angelegt werden, um mehr zu sein als Vermarktungsvehikel, Vorteilstauschbörsen oder Adressvernetzung.\n\nGeschieht die künstlerische Arbeit oder der künstlerische Prozess in Auseinandersetzung mit digitalen Netzen und einem entsprechenden Netzparadigma, so handelt es sich um ‚Digitale Netzkunst‘ im engeren Sinne.\n\nDagegen ist Netzkunst mit Netzwerken nicht immer digital. Sie kann ebenso auf analoge soziale oder abstrakte Grundlagen bezogen geschaffen werden, selbst wenn digitale Medien dabei als Werkzeuge verwendet werden. Derartige Netzkunst ist bereits durch teilnehmende Interaktion in analogen telematischen Netzen erfahrbar.\n\nFür die Digitale Netzkunst benötigt der Teilnehmer oder Netzwerker jedoch Geräte, Displays, Webseiten und andere technische Mittel. Viele Erscheinungen, die erst mit dem Webseiten-Internet (WWW – World Wide Web) bekannt wurden, sind jedoch in analogen telematischen Netzen bereits zu beobachten. In einem einfachen Netz von Teilnehmern, die sich Postkarten senden, kann durchaus Virtualität entstehen, beispielsweise indem künstliche Personen imaginiert werden, die Persönlichkeit entwickeln und vergleichbar einem Avatar auf die Kommunikation zurückwirken.\n\nMarshall McLuhans Satz \"„The Medium is the Message“\" ist für Netzkunst und ihre Interpretation bedeutend. Sogar wenn ein Netzwerk scheinbar unabhängig von der Art der eingesetzten technischen Netze und Medientechnologien funktioniert, sind Form und Inhalt jeder Mitteilung und Darstellung von den technischen Grundlagen des Mediums beeinflusst und verändern dadurch die Wirklichkeit. Wie der Übergang vom Buchdruck zu elektronischen Netzen wirkt der Übergang von analoger zu digitaler Informationsverarbeitung gesellschaftsverändernd, denn digitale Technik beruht auf einem technologieabhängigen Verschlüsselungs- und Entschlüsselungsvorgang, dessen Beherrschung eine Wissensgesellschaft voraussetzt und weitreichende gesellschaftliche Folgen hat.\n\n\n\n\nIn der Anfangszeit digitaler Netze war ihre Veränderbarkeit für künstlerische Netzwerker leicht erfahrbar, da sie Macher und Nutzer in Personalunion waren. Analoge oder digitale Netzkunst war oft durch Vorstellungen von Gesellschaftsveränderung, sozialwissenschaftliche Theorien, soziale Utopien und literarische Vorbilder inspiriert. Mit dem Entstehen einer Internetkultur hat die Kritik am Bestehenden und die Begeisterung für soziale und technische Möglichkeiten neue Formen angenommen (siehe Kommunikationsguerilla, Medienguerilla, Telematische Gesellschaft bei Vilém Flusser). Kritische Versuchsanordnungen in Bereichen wie Wahrnehmung, Medien und Gesellschaft sind für Netzkunst nicht ungewöhnlich. So kann es Netzkunst sein, soziale oder kulturelle Traditionen des Internets bei Projekten außerhalb der technischen Struktur des Internets für Veränderungen einzusetzen.\n\nNetzkünstler interessiert oft die Dekonstruktion ästhetischer, digitaler und gesellschaftlicher Codes, aber Mediale Netzkunst kann sich ebenso auf alle anderen Phänomene von Kommunikationsnetzwerken beziehen.\n\nNetzwerke sind ohne positive mentale Teilhabe der Teilnehmer nicht von Dauer, unter Umständen können störende und unbequeme Netzwerkstrategien künstlerisch jedoch konsequent sein. So gilt in der internationalen Szene der Netzkünstler der „kreative Netz-Hack“ als Akt des politischen und ästhetischen Widerstands. Für die Künstler ist es nicht ungewöhnlich, Netzaktivist und 'Hacktivist' zu sein. Die Präsentation eines Computervirus auf der 49. Biennale Venedig war in dieser Hinsicht typisch, keine kriminelle Tat, sondern das kalkulierte Werk von Netzwerkkünstlern. Künstlerische Aktivitäten dieser Art geraten immer wieder in Gefahr, missinterpretiert und kriminalisiert zu werden.\n\nNetzkunst ist Teil einer Bewegung für den freien Austausch von Information, von Software und Ideen angesichts der Kommerzialisierung des Netzes. Kunst und elektronischer ziviler Ungehorsam (electronic civil disobedience und hacktivism) überschnitten sich. Zum verantwortlichen Einsatz destruktiver ästhetischer, digitaler oder sozialer Codes im Rahmen des zivilen Ungehorsams und der Freiheit der Kunst gehört die egozentrische Kunst-Propaganda des Neoismus ebenso wie die Verunsicherung der Internetbenutzer durch künstlerische Eingriffe beim Zugang zum World Wide Web, etwa durch Webseiten (Web Art), die ein kritisches Bewusstsein im Umgang mit dem Medium fördern.\n\nDie jeweils aktuellen Formen von Netzkunst stehen in Zusammenhang mit Veränderungen in den Bereichen Telekommunikation, gesellschaftliche Interaktion und Wahrnehmung in der Mediengesellschaft. Netzkunst kann diese Veränderungen reflektieren, daran beteiligt sein, und manchmal Entwicklungen vorwegnehmen.\n\nSchon im Mail Art Netzwerk wurden virtuelle Persönlichkeiten durch Netzkommunikation erzeugt. In Propaganda-Aktionen des Neoismus sind Persönlichkeiten, an denen jeder teilnehmen kann, propagiert worden. Die Persönlichkeit füllt sich durch das Netzwerk der an ihr Beteiligten Netzwerker mit virtuellem Leben. Als künstliche Persönlichkeit in Erscheinung getreten, kann sie ein durch die ursprünglichen Macher nicht mehr steuerbares kommunikatives Eigenleben entwickeln. Das im Internet und in virtuellen Welten gebräuchliche Konzept des Avatars wurde durch solche Figuren in analogen künstlerischen Netzen bereits vorweggenommen. So führt in einer großen Suchmaschine etwa die Eingabe „Karen Eliot“ in ein Dickicht von Webseiten, in dem sich Möglichkeiten ergeben, mit Karen Eliot zu kommunizieren oder selbst als Karen Eliot aufzutreten. Die Multi-Persönlichkeit reproduziert sich unter anderem als nom de plume oder Pseudonym für viele Nutzer. Karen Eliot verbreitete sich wie Monty Cantsin oder Luther Blissett (kollektives Pseudonym) als kollektives Pseudonym und multiple virtuelle Persönlichkeit zunächst in analogen künstlerischen Netzwerken, fand Eingang in die ersten künstlerisch genutzten Mailboxsysteme und eroberte schließlich alle geeigneten Internetdienste.\n\nAuf Grund ihres explizit unkommerziellen Charakters bewegen sich Netzkunst-Projekte im rechtsfreien Raum. Niemand hat z. B. ein \"persönliches Anrecht am Produkt\", wenn es zu verwertbaren Ergebnissen in Projekten kommt, welche literarische, musikalische oder grafische Werke zum Ziel haben. Sowohl der Initiator als auch der Teilnehmer an einem solchen Projekt muss mit dieser Tatsache leben: alle Ergebnisse sind gemeinfrei. Selbst beim Vorhandensein von \"Logfiles\" ließe sich nicht mehr rekonstruieren, wem welche IP-Adressen nachträglich zuzuordnen wären.\n\nVöllig ungelöst ist das Problem der \"Vergänglichkeit virtueller Netzkunstobjekte\", deren Nachweisbarkeit unmittelbar abhängig von der Verfügbarkeit im Internet ist, letztlich also von der Wartung der Projekte durch die Netzkünstler, die direkten (und im Idealfall auch administrativen) Zugriff auf die anbietenden Server haben. Eine nachträgliche Rekonstruktion ist in der Regel unmöglich.\n\nIn den 1960er Jahren entstanden, beeinflusst durch Konzeptkunst und Nouveau Realisme, ursprünglich konzeptuell und prozessual orientierte Netzwerke, wie das Mailart und Correspondence Art Netzwerk. Diese analoge „Kunst mit Netzwerken“ und „Kunst im Netz“ war kunsthistorisch nicht leicht zu erfassen: Nach Verwirklichung eines prozessualen Kunstwerks in kommunikativen Prozessen gab es zwar Nebenprodukte, wie versendete Objekte, Briefe, Karten und Mailart Kataloge, und auch Dokumente der gesellschaftlichen Rezeption in Künstlerarchiven, aber wenig im Kunstbetrieb Vermarktbares. Deshalb erfolgte die kunsthistorische Aufarbeitung nach heutigen Maßstäben verspätet und zunächst oberflächlich. Ab wann und wo der vielschichtige Begriff „Netzkunst“ in Kunsttheorie und Kunstgeschichte sinnvoll eingesetzt wird, bleibt daher diskussionswürdig.\n\nMailart, Happening und Fluxus setzten konzeptuell oder real, lokal oder global, vernetzt kommunizierende und agierende Teilnehmer und Netzwerker voraus. Zu den ersten Initiatoren solcher Netzwerke gehören Künstler wie Ray Johnson, der seine Kommunikationszusammenhänge für teils reale, teils virtuelle Ausstellungen nutzte; Yves Klein und Ben Vautier, die Post-Skandale inszenierten; Ken Friedman, dessen Ausstellungsprojekt „Omaha Flow Systems“ (1972) den Charakter eines Kommunikations- und Ereignisnetzwerkes hatte. Robert Filliou prägte 1968 mit George Brecht den Begriff „Fete Permanente/Eternal Network“ (Die Ständige Feier/Das Ewige Netzwerk), der für die damalige kulturelle Situation bezeichnend war und kulturell in Beziehung zur Idee und Entwicklung eines nichtmilitärischen Internet steht. Die Aktionen und Veröffentlichungen von Filliou markieren für künstlerische Netzwerker einen Wendepunkt. Mindestens ab diesem Zeitpunkt ist Kunst mit Netzwerken kunsthistorisch wahrnehmbar.\n\nBereits diese Formen von Netzkunst bezogen neben Netzen wie Briefpost elektronische Netze selbstverständlich ein, z. B. Telefon- und Fax. Am 12. Januar 1985 beteiligten sich Joseph Beuys, gemeinsam mit Andy Warhol und dem japanischen Künstler Kaii Higashiyama am „Global-Art-Fusion“ Projekt. Dies war ein vom Konzeptkünstler Ueli Fuchser initiiertes, interkontinental ausgelegtes, FAX-ART Projekt, bei dem ein Fax mit Zeichnungen aller drei beteiligten Künstler innerhalb von 32 Minuten um die Welt gesandt wurde. Dieses Fax sollte ein Zeichen des Friedens während des Kalten Krieges darstellen und ist eines der ersten Arbeiten im globalen Kontext – vor dem Zeitalter des gängigen Internets. Netzkunst wurde weit vor Entstehung des World Wide Web der Webseiten, in Zusammenhang mit der besonders für die digitale Bild- und Tonerzeugung bedeutenden Digitalen Kunst zu Digitaler Netzkunst; zunächst über vernetzte Rechner an einzelnen Forschungseinrichtungen, dann über das wachsende Internet. Bei den ersten telematischen Kunstprojekten (s. Telematik), die auf digitalen Netzen basierten, sind anfangs nur kurzzeitig Netzwerke als Kunstwerke entstanden. In den 1980er Jahren folgte die künstlerische Nutzung von Mailbox-Systemen. Es entstanden komplexere, auf digitaler Netztechnik basierende Netzwerke, die unter anderem politisch bedeutend wurden, wie das Zamir Netzwerk (siehe digitalcourage, vormals FoeBuD). Webseiten wurden etwas später, oft durch neue Akteure, als visuell und akustisch, aber auch als sozial und politisch einsetzbares Medium entdeckt. Dabei kann als einer der wichtigsten Bezugspunkte bis etwa 2000 The Thing genannt werden (Initiator und Betreiber: Wolfgang Staehle), und als frühe Webart- und Netart-Künstler Olia Lialina und Heath Bunting (irational.org).\n\nVorläufer für den Beginn von Netzkunst sind u. a.: Der Postkartenaustausch der Künstler der Brücke bis 1913; in Beziehung auf Kommunikationstheorie und Ästhetik auch Max Bense und die Stuttgarter Gruppe/Schule ab Beginn der 60er Jahre. Die Organisationen von Joseph Beuys (als Kunst mit Netzwerken) oder Robert Adrian X mit ARTEX (als digitale Netzkunst) machten soziale und technische Netze für künstlerische Netzwerke bewusst dienstbar.\n\nNetzkunst, oft als Mail Art, war im geteilten Deutschland, sofern grenzüberschreitend, eine Auseinandersetzung mit Postzensur, außerdem ein Besuchsnetz, das Künstler und Netzwerker aus vielen Ländern, gerade wegen der Ausreisebeschränkungen der DDR, dort zusammenbrachte. Es gab künstlerische Netzwerker, die als Kuriere zwischen Ost und West die Grenzen der Machtblöcke überschritten um Mailart zu transportieren. So konnten trotz Behinderung durch „staatliche Organe“ sogar zwischen Mail Art Netzwerkern und Akteuren des Samizdat einzelne Verbindungen hergestellt werden.\n\nEins der ersten bekannten Beispiele für deutschsprachige digitale Netzkunst war die Website Handshake von 1993 bis 1994.\n\nBis März 2003 existierte auf \"netzwissenschaft.de\", der Homepage des Konstanzer Medienwissenschaftlers Reinhold Grether, eine strukturierte \"Netzkunst-Liste\". Die Namen der dort aufgelisteten Netzkünstler bzw. Netzkunst-Projekte boten einen Gesamtüberblick über die bis zu diesem Zeitpunkt vorhandenen unterschiedlichen Formen der Netzkunst. 2008 wurde diese (auf \"webarchive.org\" blockierte) Liste von einigen Netzkünstlern rekonstruiert und wieder ins Internet gestellt.\n\n\n\n"}
{"id": "143200", "url": "https://de.wikipedia.org/wiki?curid=143200", "title": "MUPID", "text": "MUPID\n\nDer MUPID war ein in Österreich durch den Grazer Universitätsprofessor Hermann Maurer und sein Team entwickeltes und hergestelltes Gerät, mit dem es erstmals möglich war, normale Fernsehgeräte an das österreichische, das deutsche oder das schweizerische BTX-Netz anzuschließen. Das Gerät wies einige Merkmale eines Heimcomputers auf, in der Standardausführung fehlten aber nicht flüchtige Speichermöglichkeiten (z. B. Festplatte, Diskettenlaufwerk).\n\nIn gewisser Weise nahmen MUPID und BTX in den 1980ern einige Funktionen des Internets vorweg.\n\nMehrzweck Universell Programmierbarer Intelligenter Decoder ist die offizielle Deutung des Akronymes MUPID. Der Miterfinder Hermann Maurer hat zu Fan-Diskussionen Ende der 1990er Jahre angemerkt, dass die von Fans vermutete Deutung Maurers Und Poschs Intelligenter Decoder tatsächlich originär und daher gleichermaßen zutreffend ist. Der zweite Miterfinder Reinhard Posch, heute Chief Information Officer der österreichischen Bundesregierung, hat zu dieser Deutungsfrage noch nicht öffentlich Stellung genommen.\n\nDer MUPID wurde als Bildschirmtext-Gerät im Auftrag der Österreichischen Post- und Telegraphenverwaltung vom IIG, Institut für Informationsverarbeitung Graz unter der Projektleitung von Hermann Maurer für den PRESTEL-Standard entwickelt.\n\nAb 1982 wurde der \"MUPID 1\" in Österreich von der PTV ihren Btx-Kunden, normalerweise gegen monatliches Entgelt, zur Verfügung gestellt. Im März 1984 kostete der einmalige Anschaffungspreis für ein MUPID rund 1000 öS (72,67 Euro) und ein monatliches Mietentgelt in Höhe von 130 öS (9,45 Euro).\n\nIm Gegensatz zu anderen Btx-Terminals konnte er auch als eigenständiger Heimcomputer genutzt werden. Dem Fernsehgerät wurde über Cenelec-Schnittstelle für die Verbindung zur Btx-Zentrale ein externes Modem angeschlossen.\n\nMit der Umstellung auf die CEPT-Norm wurde 1984 der \"MUPID 2\" auf den Markt gebracht. Durch die Verwendung des Betriebssystems CP/M und externer Diskettenlaufwerke versuchte er der Entwicklung auf dem Heimcomputermarkt gerecht zu werden. Zu dieser Zeit erlebten Geräte wie C64 oder Tandy einen regelrechten Boom. Diese waren aber, im Gegensatz zum Mupid, nicht Btx-tauglich bzw. bei den Postverwaltungen in Österreich und in Deutschland nicht zugelassen.\n\nEine Weiterentwicklung war die \"MUPID-Karte\", mit deren Hilfe es möglich war, einen PC, zum Beispiel IBM PC XT oder IBM PC AT Btx-tauglich zu machen.\n\n1983 wurde die Firma MCG, Mupid Computer Gesellschaft, unter Beteiligung der Firmen ELIN, Siemens Österreich, VÖEST und Motronic zu je 25 %, gegründet. Firmenziel war die Weiterentwicklung, Produktion und Vermarktung des MUPID sowohl in Österreich als auch im Ausland.\n\nNach Deutschland und in die Schweiz wurde der MUPID mit einigem Erfolg exportiert, die vom Erzeuger erwarteten Stückzahlen wurden aber in keinem Land erfüllt. Dies vor allem auch, weil die von den Postverwaltungen prognostizierte Anzahl von Kundenanschlüsse für BTX niemals erreicht wurden.\n\nNeben der Verwendung als Heimcomputer kam der MUPID auch bei Firmen zum Einsatz. Einerseits konnten Daten regelmäßig von einem \"Externen Rechner\" kostengünstig geladen und dann offline angezeigt werden, andererseits war es möglich die Software des MUPID aufgrund der Verwendung der Telesoftware zentral zu warten (für die damalige Zeit nicht selbstverständlich).\n\nEinige typische Anwendungen waren:\n\nMit der abnehmenden Bedeutung von BTX kam auch das Ende für den MUPID. Die Produktion wurde 1989 eingestellt. In der Zwischenzeit ist er, wie viele andere Computer auch, ein interessantes Sammlerobjekt geworden. In den 1990er-Jahren verwertete die österreichische Post den Lagerbestand an MUPIDs und gab sie für Unterrichtszwecke an Schulen ab. Einige österreichische Höhere Technische Lehranstalten sicherten sich Stückzahlen und verwendeten die Geräte für diverse Projekte.\n\n"}
{"id": "143879", "url": "https://de.wikipedia.org/wiki?curid=143879", "title": "Quad Ultra Extended Graphics Array", "text": "Quad Ultra Extended Graphics Array\n\nQuad Ultra Extended Graphics Array (QUXGA) bezeichnet in der Computergrafik üblicherweise eine Bildauflösung von 3200 × 2400 Bildpunkten (Seitenverhältnis 4:3). \"Quad\" deutet dabei die gegenüber UXGA vervierfachte Pixelanzahl an.\n\nFür Breitbildschirme wurde außerdem QWUXGA (\"Quad WUXGA\") bzw. WQUXGA (\"Wide QUXGA\") geschaffen, was üblicherweise 3840 × 2400 Pixel (8:5, 16:10) bedeutet.\n\nGeräte mit diesen Auflösungen sind bisher sehr selten, aber zum Beispiel im medizinischen Bereich anzutreffen (zum Beispiel zur Beurteilung von Röntgenbildern).\n"}
{"id": "144709", "url": "https://de.wikipedia.org/wiki?curid=144709", "title": "Web (Browser)", "text": "Web (Browser)\n\nWeb (früher Epiphany) ist ein freier Webbrowser.\nEr ist aus Galeon hervorgegangen und heute der offizielle Browser des Gnome-Projekts.\n\nWeb wird unter der GNU General Public License (GPL) veröffentlicht.\nSeit 2009 verwendet er für die Darstellung von Webseiten WebKit, die Rendering-Engine von Safari und anderen Browsern.\n\nZiel der Entwickler ist es, einen Browser mit einer möglichst einfachen Benutzeroberfläche zu schaffen, der sich an die Human Interface Guidelines (HIG) des GNOME-Projektes hält, in die Desktop-Umgebung Gnome integriert ist und die relevanten Standards befolgt.\n\nEbenso wie Galeon und die meisten anderen Browser unterstützt Web Tabbed Browsing, Cookie-Verwaltung und hat einen Pop-up-Blocker. Ein Werbeblocker ist auch inklusive.\n\nVon Galeon wurde außerdem das Konzept der \"Smart Bookmarks\" übernommen. Smart Bookmarks sind Lesezeichen, die ein Argument aus einem Textfeld beziehungsweise aus der Adressleiste bekommen und diese in die aufgerufene URL integrieren. Dadurch kann der Benutzer zum Beispiel die Suchfunktion einer Website nutzen, ohne sie vorher laden zu müssen.\n\nDie wohl auffälligste Besonderheit von Web ist die Organisation der Lesezeichen (Bookmarks). Im Gegensatz zu den meisten anderen Browsern, die Lesezeichen auf Hierarchien ordnen, werden Lesezeichen in Web Themen (Topics) zugeordnet. Dabei können dem Lesezeichen beliebig viele Themen zugeordnet werden. Bei der Eingabe in der Adresszeile werden dann sowohl die Lesezeichen angezeigt, deren Titel oder URL mit den eingegebenen Buchstaben beginnen, als auch die Lesezeichen, deren zugeordnete Themen mit der Eingabe beginnen.\n\nÄhnlich wie Firefox unterstützt Web Erweiterungen (Extensions), womit zum Beispiel eine Steuerung mit Mausgesten ermöglicht wird.\n\nFür häufig genutzte Webseiten lassen sich Anwendungsstarter erstellen. So können die Webseiten wie normale Desktopanwendungen aufgerufen werden. Eine solche Seite wird in einem eigenen Fenster, ohne die Bedienungselemente des Browsers, gestartet.\n\nWeb entstand ursprünglich im Jahr 2002 unter dem Namen \"Epiphany\" als Abspaltung des Galeon-Projektes.\nAm 9. September 2003 erschien Version 1.0.\nSeit Gnome 2.4 (11. September 2003) war Epiphany der offizielle Webbrowser des Gnome-Projektes.\nAm 22. Oktober 2005 wurde bekannt gegeben, dass Galeon nicht mehr weiterentwickelt wird, sondern seine Besonderheiten als Erweiterungen in Epiphany übernommen werden.\n\nAb Version 2.19.6 (30. Juli 2007) bot Epiphany als Rendering Engine außer Mozillas Gecko experimentelle Unterstützung für WebKit.\nIm April 2008 entschieden die Entwickler von Epiphany, ganz auf WebKit zu setzen. Sie begründen diesen Schritt mit der klarer strukturierten, stabileren Programmierschnittstelle und kürzeren Veröffentlichungszyklen von WebKit (als Mozillas XULRunner/Gecko).\nIn der Übergangsphase von Version 2.19.6 bis 2.28 waren beide \"Rendering-Engines\" benutzbar.\nSeit dem 21. September 2009 (Version 2.28) ist Webkit die einzige verwendete \"Rendering-Engine\".\n\nZusammen mit GNOME 3 veröffentlichten die Entwickler am 6. April 2011 die Version 3.0. Die Benutzeroberfläche ist überarbeitet worden und nutzt seitdem GTK+ 3. In den folgenden Versionen wurde der Browser weiter in die Gnome-Shell integriert. Seit 28. März 2012 (Version 3.4) wird der Browser innerhalb der Desktop-Umgebung nur noch \"Webbrowser\" genannt.\n\nIm März 2017 wurde Epiphany 3.24 veröffentlicht. \n\n\n"}
{"id": "145569", "url": "https://de.wikipedia.org/wiki?curid=145569", "title": "Transputer", "text": "Transputer\n\nEin Transputer ist ein Parallelrechner, der mit einer zusätzlichen Kommunikationshardware ausgestattet ist, um den Datenaustausch zu benachbarten Rechnern mittels Message Passing effizient zu ermöglichen. Das Wort ist ein Kofferwort aus „Transistor“ und „Computer“. Entwickelt wurden entsprechende Geräte seit 1978 von der in England gegründeten Firma Inmos. 1983 wurde der Transputer offiziell vorgestellt. Dabei handelt es sich um vollständige Mikrorechner, die nach der Von-Neumann-Architektur aufgebaut sind.\n\nWesentlich an dem Konzept des Transputers waren seine Cluster-Fähigkeiten und die darauf basierenden Versuche der Parallelisierung der Rechenprozesse. Dafür wurde die neue Programmiersprache Occam entwickelt.\n\nIn diesem Zusammenhang wurde eine (damals) äußerst schnelle Verbindungstechnik zwischen den Rechenknoten entwickelt. Da die Transputerlinks aber nach den Rendezvous-Verfahren arbeiten und keinen FIFO aufweisen, ist die Parallelisierung schwierig.\n\nUm höhere Geschwindigkeiten zu erreichen, muss man einen unidirektionalen Stream verwenden. Der Kommunikationskanal muss die Daten des Senders in einem FIFO puffern, damit der Empfänger das Vorprodukt asynchron verarbeiten kann.\n\nAnwendung fanden die Transputer beispielsweise in der Atari Transputer Workstation (Tim King), der Meiko Computing Surface, den Parallelrechnern der Firmen SANG und Parsytec oder auch in der Amiga-Transputer-Workstation. An der TUHH wurde ein Entwickler-Board hergestellt. In der Industrie fanden Transputer eine relativ große Anwendung im Bereich der Regelungs- und Steuerungstechnik. Aber nicht nur dort:\n\nDie Technische Universität München baute eine Workstation mit 137 Transputern zur Lösung von Problemen in der mathematischen Optimierung.\n\nAn der Universität Siegen wurde eine Workstation zur medizinischen Bildverarbeitung entwickelt.\nDie S2M-T1-Karte von AVM konnte optional zur Echtzeitdatenkompression auf bis zu 30 ISDN-Kanälen auf eine in einem externen 1 HE hohen 19″-Gehäuse untergebrachte Zusatzkarte mit T800-Transputer zurückgreifen.\nAuch in der Weltraumfahrt werden Transputer verwendet: So sendet die Weltraumsonde SOHO Bilder mit Hilfe eines Transputernetzwerks. Im EUREKA-Prometheus-Projekt wurden Transputer im autonomen Roboterfahrzeug VaMoRs von Ernst Dickmanns eingesetzt, um ein Fahrzeug auf der Basis rein visueller Daten automatisch durch den Verkehr zu steuern.\n\nIm Jahre 1989 kaufte SGS-Thomson die Firma Inmos auf, in der Folge wurde Mitte der 1990er Jahre die Weiterentwicklung und Ende der 1990er Jahre auch die Fertigung des Transputers eingestellt. Die Architektur des Transputers überlebte teilweise in der Mikroprozessorfamilie ST20.\n\n"}
{"id": "145875", "url": "https://de.wikipedia.org/wiki?curid=145875", "title": "OPREMA", "text": "OPREMA\n\nOPREMA war der erste arbeitsfähige, in der DDR gebaute Computer. Der Relaisrechner wurde 1955 fertiggestellt. Sein Name leitet sich von \"OPtik-REchen-MAschine\" her, ein Verweis auf den Einsatzzweck: Berechnungen für Linsensysteme.\n\nDer Rechner wurde von einem Team unter der Leitung von Wilhelm Kämmerer und Herbert Kortum für die Firma Carl Zeiss in Jena ab etwa 1954 entwickelt. Die Dateneingabe erfolgte über Stecktafeln und die Ausgabe über einen Fernschreiber.\n\nBemerkenswert ist die Tatsache, dass die OPREMA zunächst aus zwei baugleichen Anlagen bestand, die jeden Arbeitsschritt redundant ausführten und die Ergebnisse miteinander verglichen. Zusammen verfügten die beiden Anlagen über 17.000 Relais. Nachdem sich im Testbetrieb gezeigt hatte, dass beide „Teilrechner“ zuverlässig arbeiteten, wurden die beiden Rechner getrennt, und man hatte zwei unabhängige Anlagen zur Verfügung.\n\nDie Taktfrequenz betrug durchschnittlich 100 Hertz, was zu Rechenzeiten von 120 ms für eine Addition, 800 ms für eine Multiplikation oder Division und 1.200 ms für das Wurzelziehen führte.\n\n"}
{"id": "145904", "url": "https://de.wikipedia.org/wiki?curid=145904", "title": "D4a", "text": "D4a\n\nDer Kleinrechner D4a (a für „abgerüstet“) war ein Computertyp aus der DDR. \n\nEr wurde, wie seine Vorgänger D1, D2 und D3, an der TU Dresden unter der Leitung von Nikolaus Joachim Lehmann entwickelt und hergestellt.\n\nDer D4 basierte vollständig auf Transistoren. Er verfügte über ca. 200 Stück, mit denen er 2000 Operationen/Sekunde ausführen konnte. Sein Trommelspeicher hatte 4096 Speicherzellen mit je 33 Bit. Als Eingabe-Geräte verfügte der D4a über integrierte Tastatur und Lochstreifenleser, zur Ausgabe diente ein Streifendrucker. In der Bauausführung als D4a war der Rechner einer der ersten Auftischcomputer Europas.\n\nAb 1966 wurden etwa 3000 Stück des D4/D4a vom VEB Büromaschinenwerke Zella-Mehlis in Serie gefertigt.\n\n"}
{"id": "146481", "url": "https://de.wikipedia.org/wiki?curid=146481", "title": "Final Cut Pro", "text": "Final Cut Pro\n\nFinal Cut Pro (FCP) ist eine kommerzielle Videoschnittsoftware des US-amerikanischen Unternehmens Apple. Es wird ausschließlich für das eigene Betriebssystem macOS vertrieben. Die Software bietet den Import unterschiedlicher Videoquellen, den digitalen nichtlinearen Videoschnitt, native Unterstützung für zahlreiche Videoformate sowie Erweiterungsmöglichkeiten und Interoperabilität auf professionellem Niveau.\n\nDas Programm wurde ursprünglich 1998 von Randy Ubillos für das US-amerikanische Unternehmen Macromedia unter dem Namen \"Keygrip\" als Schnittsystem für das Betriebssystem Windows entwickelt. Er ist auch der Kopf der ersten Versionen von Adobe Premiere. Er wechselte jedoch von Adobe zu Apple, wo er Final Cut und später Final Cut Studio entwickelte. FCPX war ein Wunsch von Steve Jobs mit dem Ziel ein Schnittsystem zu schreiben, das sowohl dem Einsteiger als auch dem Profi ein Werkzeug gibt, anspruchsvolle Filme zu schneiden.\n\nDie Programmversion \"Final Cut Pro\" bot Schnittmöglichkeiten für viele analoge und digitale Videoformate, von DV bis HD. Auch die Verarbeitung weiterer SD-Videoformate war möglich. So konnte man mit zusätzlicher Videohardware externe Videospieler wie Betacam SP oder Digital Betacam anschließen.\n\nAb der Version 4.5 \"(Final Cut Pro HD)\" kann DVCPRO-HD-Videomaterial mit Hilfe des integrierten FireWire-Anschlusses an einem entsprechend ausgestatteten \"Power Mac\" ohne zusätzliche Spezialausrüstung aufgenommen, bearbeitet und ausgegeben werden. Zu den bisherigen Standard-Videoformaten wie z. B. PAL, NTSC (wahlweise 10 oder 8 Bit unkomprimiert), DV, DVCPRO 25, DVCPRO 50, Serial Digital Interface kommen die neuen High-Definition-Formate im progressiven Vollbildformat (25p) oder in verschiedenen Halbbildformaten \"(interlaced)\" dazu. Final Cut Pro 4.5 „HD“ unterstützt nativ DVCPRO-HD-Medien sowie sämtliche HDTV-Standards (720p, 1080i, 1080p).\n\nIm Juni 2005 erschien die Version 5 unter dem Namen „Final Cut Pro 5“, ohne den Zusatz „HD“. Dieser war 2005 schon zum Standard für \"Final Cut Pro\" geworden. Neu aufgekommene Videoformate wurden integriert, z. B. Panasonic P2, HDV oder XDCAM des japanischen Unternehmens Sony. Diese Formate können direkt über \"Final Cut Pro\" importiert bzw. mit Erweiterungen wie „XDCAM Transfer“ der Anbieter importiert werden. Dabei kann eine XDCAM wahlweise über SDI oder Firewire 400 angesteuert werden. Sie verhält sich dann entweder wie eine Festplatte oder ein Rekorder.\n\n\"Final Cut Pro\" ist seit der Messe National Association of Broadcasters im Jahr 2006 nicht mehr einzeln erhältlich, sondern ist Bestandteil des Final-Cut-Studio-Paketes. Am 15. April 2007 veröffentlichte Apple die Version \"Final Cut Studio 2\". Neben \"Final Cut Pro 6\" selbst sind auch noch das DVD-Authoring-Programm \"DVD Studio Pro 4.2\", das Audiobearbeitungsprogramm \"Apple Soundtrack\", der Titelgenerator \"LiveType\", die Kodiersoftware \"Apple Compressor\", das Motion-Graphics-Programm \"Apple Motion 3\", die Negativschnitt-Software \"Cinema Tools\" (für echten Filmschnitt von 16 mm, 35 mm oder 70 mm), das High-End-Farbkorrektursystem \"Color\" sowie Apples High-End-\"QuickTime Pro 7\" enthalten.\n\n\"Final Cut Pro 6\" unterstützt den Red Digital Cinema Camera Company oder kurz \"RED\"-Codec selbst nicht, kann diesen aber als Proxy direkt oder als gewandelte YUV-Sequenz importieren. Das Bearbeiten von (RED)-RAW-Dateien, 10- oder 12-bit-RGB444 oder von für Kameras wie die Red verwendeten Auflösungen wie 4520 × 2160 beherrscht FCP nicht. Daher wird FCP oft als Offline-Schnittsystem eingesetzt, von dem aus dann Schnitt per EDL an onlinefähige >4K-, >12-bit-RGB-Systeme übergeben werden.\n\nAm 23. Juli 2009 veröffentlichte Apple \"Final Cut Pro 7\", das in \"Final Cut Studio 3\" enthalten ist. \"Final Cut Studio\" bietet in Version 3 nun auch rudimentäre BD-AV-Unterstützung, wobei aber der Blu-ray-Standard, der unter anderem auch BD-Java, BD-Menü, 3D, DRM und anderes voraussetzt, vom Programm zugunsten von iTunes nicht vollständig unterstützt wird.\n\nAuch wenn das Programm offiziell eingestellt war, so wurde es bis 2017 und bis zum Betriebssystem Mac OS 10.12 unterstützt. Mit Mac OS 10.13.1 (High Sierra) wird \"Final Cut Pro 7\" nicht mehr unterstützt.\n\n2003 veröffentlichte Apple mit \"Final Cut Express\" eine kostengünstigere Version von \"Final Cut Pro\", mit einer sehr ähnlichen Oberfläche, allerdings ohne einige \"Pro\"-Funktionen. Im Januar 2005 kamen die \"Pro\"-Tools \"Soundtrack\" und \"LiveType\" zu \"FCE\" hinzu, ebenso neue HDV-Funktionen. Zwecks Funktionsvereinheitlichung wurde \"FCE\" im Juni 2011 eingestellt, und zwar zugunsten von \"Final Cut Pro X\".\n\nAm 21. Juni 2011 veröffentlichte der Hersteller \"Final Cut Pro X\" auf dem „FCP User Supermeet“ im Rahmen der NAB Messe in LasVegas. Das Programm wurde neu geschrieben, um vollständig 64-bit-kompatibel zu sein sowie OpenCL und Grand Central Dispatch zu unterstützen. Die Version iMovie ist eine Variante für Kunden, die im Inneren eine Vollversion von FCPX ist, jedoch eine vereinfachte Benutzeroberfläche für Laien bietet. Mit der Version \"Final Cut Pro X\" wurde die Suite Final Cut Studio eingestellt und die Weiterentwicklung mehrerer Produkte beendet.\n\nDas Farbkorrektursystem \"Color\" wurde ersatzlos eingestellt, ebenso das Authoringsystem \"DVD Studio Pro\". Die vormals beinhalteten Programme \"Final Apple Motion\" und \"Apple Compressor\" wurden jetzt einzeln vertrieben oder waren nur stark eingeschränkt im Schnittprogramm nutzbar. Der Vertrieb erfolgte ausschließlich über den Mac App Store und auch nicht auf DVD. Apple fokussierte nun anders als zuvor mit Final Cut Studio mit \"Final Cut Pro X\" den Massenmarkt und senkte den Preis gegenüber Final Cut Studio um 70 Prozent.\n\nWegen des Fehlens einiger wichtiger Funktionen wie Multicam-Schnitt, dem Importieren von Schnittlisten und der Unterstützung externer Bildschirme gab es Kritik aus professionellen Kreisen. In späteren Versionen reagierte Apple darauf und reichte fehlende Funktionen nach, beispielsweise Multicam-Schnitt und Unterstützung des REDCODE-RAW-Formates und native MXF-Unterstützung. Inzwischen gibt es ein großes Angebot von Drittherstellern, die Effekte und Hardware anbieten.\n\nNach mehreren großen Kinofilmen in asiatischen Märkten wurde im Jahr 2014 mit Glenn Ficarras „Focus“ mit Will Smith der erste große Hollywood-Film (mit einem Budget von 50,1 Millionen US-Dollar) ausschließlich mit Final Cut Pro X produziert.\n\nProduktionshäuser wie BBC News und RTS sind bereits auf FCPX umgestiegen.\n\n\n"}
{"id": "146857", "url": "https://de.wikipedia.org/wiki?curid=146857", "title": "Microsoft Internet Information Services", "text": "Microsoft Internet Information Services\n\nInternet Information Services (IIS) (vormals \"Internet Information Server\") ist eine Diensteplattform des Unternehmens Microsoft für PCs und Server. Über sie können Dokumente und Dateien im Netzwerk zugänglich gemacht werden. Als Kommunikationsprotokolle kommen hierbei HTTP, HTTPS, FTP, SMTP, POP3, WebDAV und andere zum Einsatz. Über IIS können ASP- oder .NET-Applikationen (ASP.NET) ausgeführt werden, sowie – mit den passenden installierbaren ISAPI-Filtern – auch PHP und JSP.\n\nIIS-Dienste können auf folgenden Microsoft-eigenen Betriebssystemen eingesetzt werden: Windows NT Server, Windows 2000 Server, Microsoft Windows Server 2003, Windows Server 2008, Windows Server 2012 und Windows Server 2016, außerdem auch auf den nicht-Server-Systemen (insbesondere Windows 7, Windows 8 und Windows 10).\n\nBei Windows 2000 Professional und Windows XP Professional werden eingeschränkte IIS-Dienste in den jeweiligen Versionen als optionale Komponente mitgeliefert. Hier ist die Anzahl gleichzeitiger Verbindungen auf höchstens 10 beschränkt und es kann nur eine Website (die „Standardwebsite“) eingerichtet werden. Vorgesehen ist der Einsatz als reine Test- und Entwicklungsumgebung.\nDiese \"IIS Express\"-Version ist auch in allen Varianten von Visual Studio 2012 enthalten.\n\n\"IIS 7.0\" ist nicht nur in \"Windows Server 2008\" enthalten, sondern auch in den Business-, Enterprise- und Ultimate-Versionen von \"Windows Vista\". Eine eingeschränkte Version (maximal 3 Verbindungen) ist in \"Vista Home Premium\" enthalten.\n\n\n\n"}
{"id": "147303", "url": "https://de.wikipedia.org/wiki?curid=147303", "title": "User Mode Linux", "text": "User Mode Linux\n\nDas sogenannte (im Folgenden kurz UML genannt, nicht zu verwechseln mit der Unified Modeling Language, welche ebenfalls als UML abgekürzt wird) ist eine Variante des Linux-Kernels, die es erlaubt, komplette Linux-Kernel als Anwendungsprozesse innerhalb operierender Linux-Systeme auszuführen, ohne deren Konfiguration und damit Stabilität zu beeinflussen.\n\nDie Einsatzmöglichkeiten sind vielfältig. Netzwerkdienste können in einer UML-Umgebung komplett isoliert vom Hauptsystem ablaufen. Oft wird UML auch benutzt, um einen sogenannten „Honeypot“ zu installieren, mit dem die Sicherheit eines Computers oder Netzwerks getestet werden kann. Ein anderes Einsatzgebiet ist das Testen und die Fehlerbereinigung („Debuggen“) von Software, zum Beispiel eine Version des Linux-Kernels. Der Vorteil ist, dass das Hauptsystem nicht beeinflusst wird.\n\nEin weiterer denkbarer Bereich für den Einsatz von User-Mode-Linux ist die Treiberentwicklung für Linux, da die Fehlerbereinigung eines User-Mode-Prozesses leichter als bei einem vollwertigen Kernel ist. Außerdem ist eine \" (Halt des Systems auf Grund eines Kernel-Fehlers) bei einem virtuellen Kernel für das tatsächliche System folgenlos, denn es kann ein neuer virtueller Kernel gestartet werden. Schließlich ist auch das gleichzeitige Starten mehrerer virtueller Kernel interessant (z. B. für Anbieter von dedizierten Webservern, die dadurch dem Kunden auf ein und demselben System die Linux-Distribution seiner Wahl anbieten können).\n\nUrsprünglich sollte UML \" ( für \"Linux auf Linux\") genannt werden. Die Idee wurde jedoch verworfen, um eine Verwechslung mit dem Akronym LOL auszuschließen.\n\nSeit Kernelversion 2.6.0 ist UML in den offiziellen Linux-Quellen enthalten, so dass man diese Kernel als UML-Prozesse unterhalb eines Wirtslinux operieren lassen kann. Wenn auf dem Wirtslinux zusätzlich ein \"skas\"-Patch eingespielt wird, hilft der Wirtskernel dem UML-Kernel bei bestimmten Verwaltungsaufgaben, was Sicherheit und Leistung erhöht.\n\nUML wird nach wie vor aktiv entwickelt, dies betrifft vor allem die \"skas\"-Erweiterungen.\n\n\n"}
{"id": "147432", "url": "https://de.wikipedia.org/wiki?curid=147432", "title": "Bitwertigkeit", "text": "Bitwertigkeit\n\nDie Bitwertigkeit legt den Stellenwert eines einzelnen Bits fest, den es durch seine Position innerhalb einer Binärzahl (auch Dualzahl genannt) haben soll. Wichtig ist diese Festlegung auch bei der seriellen Datenübertragung sowie für parallele Busse in der Datenverarbeitung.\n\nWerden acht Bits zu Bytes gruppiert und diese wiederum zu größeren Zahlenformaten, so ist zusätzlich die Byte-Reihenfolge festzulegen.\n\nDie Bitnummerierung ist unabhängig von der Byte-Reihenfolge (Byte-Endianness) und von der „Bit-Endianness“.\n\nSind die Bits innerhalb einer Binärzahl gemäß LSB 0 (aus dem Englischen für ) nummeriert,\ndann hat das Bit der Nummer 0 (= das Bit mit dem Index 0) den \"niedrigsten\" Stellenwert. Ist bei einer Binärzahl mit den Stellen formula_1 die Bitposition 0 die \"niedrigstwertige\", dann ist deren Wert mit formula_2 zu multiplizieren. Der Gesamtwert der Binärzahl ist:\n\nSind die Bits innerhalb einer Binärzahl gemäß MSB 0 (aus dem Englischen für ) nummeriert,\ndann hat das Bit der Nummer 0 (= das Bit mit dem Index 0) den \"höchsten\" Stellenwert.\nIst bei einer Binärzahl mit den Positionen formula_1 das Bit 0 das \"höchstwertige\", dann ist sein Wert mit formula_5 zu multiplizieren. Der Gesamtwert der Binärzahl ist:\n\nWerden die Stellen gemäß ihrer absteigenden Wertigkeit horizontal von links nach rechts aufgetragen, so hängt es von der Bit-Reihenfolge ab, ob mit der Bit-Nummerierung rechts (bei \"LSB 0\") oder links (bei \"MSB 0\") begonnen wird.\n\nBeginnt die Zählung mit 0 für das niedrigstwertige Bit (LSB) auf der rechten Seite und geht von rechts nach links, so spricht man von „LSB“. Beginnt dagegen die Zählung mit 0 für das höchstwertige Bit (MSB) auf der linken Seite und geht von links nach rechts, so spricht man von „MSB“.\n\nDas höchstwertige Bit (MSB) zeigt im Einerkomplement, Zweierkomplement und anderen Zahlenformaten mit Vorzeichenbit auch an, ob die entsprechende Dezimalzahl positiv oder negativ ist. Beim Zweierkomplement zählt die Null dabei zu den nicht-negativen Zahlen, sodass man mehr negative als positive Zahlen darstellen kann. Bei anderen Zahlenformaten wie dem Einerkomplement oder Gleitkommazahlen nach IEEE-754-Standard gibt es zwei Darstellungen der Null (+0 und −0).\n\nDieselben Annahmen wie im Artikel „Byte-Reihenfolge“ seien vorausgesetzt.\n\nFür die allermeisten in der Praxis verwendeten Rechner besteht die kleinste adressierbare Einheit aus mehr als einem Bit, beispielsweise aus einem Byte enthaltend 8 Bits. Mithin lässt sich ein einzelnes Bit nicht unmittelbar adressieren. Fügt man einer Byte-Adresse ein Bit-Offset ∈ 0,1, …,7 bei, so wird ein einzelnes Bit eindeutig spezifizierbar. Diese Art der Spezifikation wird Bit-Adressierung genannt. Sie lässt sich über den Umweg von Registerverschiebungen verwirklichen. Dabei ist zu beachten, dass die Begriffe links/rechts bei Shift-Befehlen nichts mit links/rechts weiter oben zu tun haben, sondern sie orientieren sich ausschließlich an der üblichen Sicht, bei der der Links-Shift eine Binärzahl mit einer Zweierpotenz multipliziert, die Bits also in Richtung „Big-End“ (= Richtung most significant bit) verschiebt, und umgekehrt der Rechts-Shift. In der Form \"ByteAdresse\"*8+\"BitOffset\" hat die Bit-Adressierung folgende Eigenschaften:\n\n\n\nPlatziert man in der horizontalen Darstellung die niedrigen Adressen links und die hohen rechts, so lassen sich für 32 Bit lange Bit-Arrays oder vorzeichenlose Ganzzahlen die Konventionen der Bit-Nummerierung folgendermaßen gegenüberstellen:\n\nDie Nummerierung der Bits bei \"Big-Endian mit MSB 0\" und \"Little-Endian mit LSB 0\" entspricht somit der Bit-Adresse. Dagegen nummerieren die Konventionen \"Big-Endian mit LSB 0\" und \"Little-Endian mit MSB 0\" die Bits im Register gegenläufig zu ihrer Adresse, was mit der Indizierung in einem Array antikorreliert.\n"}
{"id": "147957", "url": "https://de.wikipedia.org/wiki?curid=147957", "title": "Microsoft Windows PE", "text": "Microsoft Windows PE\n\nMicrosoft Windows PE (\"PE\" steht für \"Preinstallation Environment\", englisch für \"Vor-Installations-Umgebung\"; auch kurz \"WinPE\" genannt) ist ein minimiertes Windows-Betriebssystem. Es basiert auf Windows XP und seinen Nachfolgern (bis einschließlich Windows 10) und lässt sich von verschiedenen startfähigen Speichermedien (wie etwa CD-ROM, USB-Stick oder Festplatte) hochfahren.\n\nFolgende Versionen sind mittlerweile auf dem Markt verfügbar:\n\nMicrosoft machte mit den darauf folgenden Versionen einen Schritt in eine ganz neue Richtung und änderte neben den Versionsnummern auch das Lizenzierungsmodell, um auch Resellern zu erlauben, Windows PE direkt auf mitgelieferten CDs an den Kunden weiterzugeben:\n\n\"Windows PE\" ist vor allem für Computerhersteller und Administratoren vorgesehen. Es soll die vollautomatische Installation eines Windows-Systems auf einem neuen Computer ermöglichen, auf dem bisher noch kein Betriebssystem installiert wurde.\n\nWindows PE bietet einen einfachen Kommandozeileninterpreter und benötigt nur etwa 512 Megabyte Arbeitsspeicher. Nach dem Start lassen sich z. B. Festplatten partitionieren und formatieren oder Netzwerklaufwerke verbinden, von denen dann eine Windows-Installation gestartet werden kann.\n\n\nDurch Modifikation von wenigen Registry-Einträgen soll eine für Auditing- und IT-forensische Untersuchungen nutzbare Abwandlung von Windows PE, das sogenannte WinFE (für Windows Forensic Environment) erzeugt werden können. Die Modifikation unterbindet das sonst übliche und automatische Einbinden von verbauten Speichermedien (z. B. ein Festplattenlaufwerk) und verhindert somit das Schreiben jeglicher Daten (z. B. der Zeitpunkt des letzten, lesenden Zugriffs auf Daten).\n\nBislang fehlen jedoch auf einer fundierten, empirischen Datenbasis durchgeführte Tests, die die (IT-forensisch) einwandfreie Funktion des WinFE belegen. Nutzern dieser Abwandlung wird empfohlen, vor Einsatz von WinFE selbst entsprechende Tests durchzuführen.\n\nIm Folgenden sind Funktionen aufgelistet, welche in normalen Windows-Versionen unterstützt, jedoch in \"Windows PE\" nicht oder nicht mehr enthalten sind.\n\n\n\n\n\nBis unter Windows XP benötigte man eine Lizenz für das volle Betriebssystem (OEM, ISV, VL usw.), um WinPE nutzen zu dürfen. Seit Vista ist das Preinstallation Environment (WinPE 2.0) jedoch allgemein zugänglich, das gilt auch für die Version 5.0 unter Windows 8.1.\n\nDamit Windows PE nicht als kostenloser Ersatz eines kommerziellen Betriebssystems verwendet werden kann, wird nach 72 Stunden kontinuierlicher Verwendung die Ausführung der Shell beendet und ein Neustart ausgeführt. Dieser Zeitraum ist nicht konfigurierbar.\n\n\n"}
{"id": "147988", "url": "https://de.wikipedia.org/wiki?curid=147988", "title": "Chaos Communication Camp", "text": "Chaos Communication Camp\n\nDas Chaos Communication Camp (kurz \"CCCamp\" oder in informierten Kreisen einfach nur \"Camp\" genannt) ist ein internationales Treffen von Hackern, das seit 1999 alle vier Jahre in der Nähe von Berlin stattfindet. Es wird organisiert vom Chaos Computer Club (CCC), der auch den jährlichen Chaos Communication Congress veranstaltet.\n\nDie ersten beiden Treffen in den Jahren 1999 und 2003 fanden auf dem \"Paulshof\" bei Altlandsberg statt. Das dritte Camp fand vom 8. bis 12. August 2007 und das vierte Camp vom 10. bis 14. August 2011 auf dem Gelände des Luftfahrtmuseums Finowfurt statt. Das bisher letzte und 5. Treffen fand vom 13. bis 17. August 2015 im Ziegeleipark Mildenberg statt. Das sechste Camp findet vom 21. bis 25. August 2019, ebenfalls im Ziegeleipark Mildenberg statt.\n\nDas \"Chaos Communication Camp\" bietet deutsch- und englischsprachige Informationsveranstaltungen zu technischen und gesellschaftlichen Themen wie z. B. Datenschutz, Informationsfreiheit und Datensicherheit. Ergänzt wird es durch Workshops und etwa 30 „Villages“, die Zeltstädten mit einer thematischen Ausrichtung entsprechen sollen. 2007 gab es einen Charterflug von der Def-Con-Konferenz in Las Vegas über Frankfurt am Main auf das Camp-Gelände im Rahmen des Projektes \"Hackers on a Plane\".\n\nIn einem „Art & Beauty“-Bereich werden vor allem künstlerische Bereiche der Technik wie etwa Lichtinstallationen oder Musik präsentiert. Jeder Teilnehmer hat die Möglichkeit, ein eigenes Zelt samt Computer aufzubauen und sich an das vom CCC aufgebaute Camp-eigene Stromnetz und eine schnelle Internetverbindung anzuschließen. Neben Funkverteilern werden für die Vernetzung des Geländes mit Netzwerktechnik bestückte Toilettenhäuschen (sogenannte Datenklos) zur Verfügung gestellt.\n\nNeben Zeitschriften aus der Informationsbranche berichten auch die herkömmlichen Medien und Fernsehsender über die Veranstaltung.\n\nDa der Ziegeleipark Mildenberg über keine ausreichende Infrastruktur verfügte, wurde in Vorbereitung auf das Chaos Communication Camp 2015 eine 2,5 Kilometer lange Glasfaserleitung von einem nahegelegenen Hochspannungsmast dorthin verlegt. Diese versorgte das Camp mit einer symmetrischen 10 Gigabit Standleitung. Aber Strom stand nicht in ausreichenden Maße zur Verfügung. Daher wurden Dieselaggregate mit einer Gesamtleistung von rund 1,8 Megawatt aufgestellt.\n\nEs wurden 4.500 Teilnehmer aus 20 Ländern erwartet. Mit dem Netzwerk des Camps waren etwa 18.000 Computer, Smartphones etc. verbunden.\n\nZu den Themen gehörten unter anderem die Landesverrat-Affäre um netzpolitik.org, Frauen in der Hackerszene, Bio-Hacking und die Emanzipation von Flüchtlingen .\n\nIn England findet in geraden Jahren das Electromagnetic Field statt. In den ungeraden Jahren, in denen kein CCCamp stattfindet, finden unter wechselnden Namen ähnliche Veranstaltungen in den Niederlanden statt, welche aus dem inzwischen eingestellten niederländischem Hackermagazin Hack-Tic hervorgegangen sind.\n\n"}
{"id": "148153", "url": "https://de.wikipedia.org/wiki?curid=148153", "title": "Flussband", "text": "Flussband\n\nFlussbänder bezeichnen in der Grafischen Datenverarbeitung ein Verfahren zur Visualisierung von Strömungen. Dabei werden zwei benachbarte Stromlinien oder Bahnlinien durch Triangulation miteinander verbunden und schattiert.\n\nEine Variante der Flussbänder sieht es vor, Flussbänder konstanter Breite zu erzeugen, welche um jeweils eine einzige Stromlinie oder Bahnlinie gelegt werden. Für die Orientierung im Raum wird in diesem Fall die lokale Rotation des Strömungsfeldes herangezogen.\n"}
{"id": "148475", "url": "https://de.wikipedia.org/wiki?curid=148475", "title": "ScanDisk", "text": "ScanDisk\n\nScanDisk ist ein Computerprogramm, das von Microsoft erstellt wurde, um (logische und physikalische) Fehler der Festplatte oder Partition zu ermitteln und zu berichtigen. Dadurch soll die Integrität aller Dateien gesichert werden. \n\nScanDisk löste in MS-DOS 6.2 und in den Produktserien Windows 9x und ME das Werkzeug CHKDSK zur Überprüfung von FAT-formatierten Partitionen. \n\nIn Windows-Versionen der NT-Serie wurde ScanDisk wiederum durch eine Kommandozeilen-Version von Chkdsk ersetzt, die um die Diagnosefähigkeiten des Dateisystems NTFS erweitert wurde. Alternativ kann ab Windows XP die Partition mit der rechten Maustaste im Windows-Explorer anklicken und im Pop-up-Menü unter Eigenschaften→Extras→Fehlerüberprüfung nutzen.\n"}
{"id": "148577", "url": "https://de.wikipedia.org/wiki?curid=148577", "title": "HotJava", "text": "HotJava\n\nHotJava war ein Webbrowser von Sun Microsystems, welcher komplett in der Programmiersprache Java geschrieben wurde und als erster Browser sogenannte \"Java-Applets\" ausführen konnte. Sun entwickelte den Mosaic-Clone unter dem Namen \"WebRunner\" (nach dem Film \"Blade Runner\") ab 1994 für Java-Entwickler zum Test ihrer Programme und als Referenzplattform. Später wurde er in \"HotJava\" umbenannt.\n\nDer Browser fand unter Entwicklern entsprechende Beachtung, erlangte im Vergleich zu damals auf dem Markt verbreiteten Browsern wie Windows Internet Explorer oder Netscape Navigator allerdings nie große Bedeutung. Dies hatte zwei Gründe. Der Browser hatte im Vergleich zu oben genannten Konkurrenten weniger Funktionalität und Features. Außerdem war \"HotJava\" durch die technischen Grenzen der damaligen Java Virtual Machine (sowohl in Bezug auf Geschwindigkeit, als auch auf Speicherbedarf) sehr langsam.\n\nIm Jahr 2000 wurde das HotJava-Projekt offiziell eingestellt.\nDie Idee eines Browsers auf Java-Basis wurde kurz nach der Veröffentlichung von \"HotJava\" Ende 1997 von Netscape aufgegriffen. Netscape kündigte im Dezember 1997 die Entwicklung des \"Javagator\" (eine Zusammensetzung aus \"Java\" und Netscapes damals verbreitetem \"Netscape Navigator\") an. Das Projekt wurde allerdings vor einer Veröffentlichung bereits im darauffolgenden Jahr wieder eingestellt. Einen weiteren Versuch eines in Java geschriebenen Webbrowser stellt das Jazilla-Projekt dar, welches zwar nie offiziell eingestellt wurde, aber seit 2006 keine Neuigkeiten oder Updates mehr hervorgebracht hat.\n"}
{"id": "152069", "url": "https://de.wikipedia.org/wiki?curid=152069", "title": "Amiga 3000", "text": "Amiga 3000\n\nDer Amiga 3000 war ein Computer aus der Amiga-Serie, hergestellt zwischen 1990 und 1992 von der Firma Commodore.\n\nEr ist eine technische Weiterentwicklung des Amiga 2000 und hatte bereits SCSI- und Zorro-3-Steckplätze. Bei den ersten ausgelieferten Modellen wurde das Kickstart–ROM ähnlich dem Amiga-Urvater A1000 als Datei von Festplatte geladen – im Gegensatz zu den übrigen Amiga-Modellen, bei denen dieser Teil des Betriebssystems im ROM saß.\n\nSpäter wurde auch der Amiga 3000 nur noch mit ROMs für den \"Kickstart\" ausgeliefert. Hierzu wurde aber im Gegensatz zum Amiga 500 oder Amiga 2000 der \"Kickstart\" in zwei ICs untergebracht, da dessen CPU einen 32 Bit breiten Datenbus verwendete.\n\n\nDen Amiga 3000 gab es sowohl als Desktop- als auch als Tower-Ausführung (Amiga 3000T, zunächst auch A3500 genannt). Weiterhin gab es die Version Amiga 3000UX, die zusammen mit UNIX System V Release 4 auf den Markt kam.\n\nUnter der Modellbezeichnung Amiga 3000+ existieren lediglich wenige Prototypen als Weiterentwicklung des Amiga 3000. Diese verfügen über einen digitalen Signalprozessor sowie den erst in den späteren Modellen A1200 und A4000 in Serie eingesetzten AGA-Chipsatz. Die Entwicklung wurde zugunsten des Nachfolgemodells Amiga 4000 eingestellt, bevor das Projekt Marktreife erlangte.\n"}
{"id": "152073", "url": "https://de.wikipedia.org/wiki?curid=152073", "title": "Amiga 3000UX", "text": "Amiga 3000UX\n\nDer A3000UX ist ein Modell der Amiga-Computer-Serie von Commodore und wurde mit Commodore Amiga UNIX statt AmigaOS ausgeliefert.\n\nAmiga UNIX - auch \"AMIX\" genannt - war kein Unix-Klon, sondern eine vollständige Portierung eines UNIX System V Release 4 von AT&T.\n\nSun Microsystems bot Commodore-Amiga Inc. an, den A3000UX unter Lizenz als Low-End-Ergänzung zu den eigenen Workstations zu produzieren. Das Angebot wurde aber vom Commodore-Management abgelehnt, weil man hier an den großen Durchbruch des A3000UX glaubte.\n\nDie technischen Daten entsprechen denen des Amiga 3000.\n\n"}
{"id": "152591", "url": "https://de.wikipedia.org/wiki?curid=152591", "title": "PowerBook G4 12 Zoll", "text": "PowerBook G4 12 Zoll\n\nDas PowerBook G4 12″ ist ein Notebook des Unternehmens Apple aus der PowerBook-Baureihe. Es hatte einen 12-Zoll-LCD-Bildschirm (sichtbar 25,7 cm × 18,6 cm, Diagonale 30,8 cm), den PowerPC-G4-Prozessor und ein charakteristisches Titan-/Aluminiumgehäuse. Es wurde wahlweise mit einem Combo-Laufwerk oder einem Brenner ausgestattet.\n\nDas Gehäuse besteht aus einer Aluminiumlegierung aus dem Flugzeugbau. Die Unterseite des Gehäuses besteht aus einem Aluminiumformteil und ruht mit vier kleinen Plastikfüßchen auf der Tischplatte. Das Bildschirmscharnier ist so gestaltet, dass es bei einem Sturz nicht gleich abbricht, die Anschlüsse sind seitlich ohne weitere Abdeckung angeordnet, so dass man sie bequem erreichen kann, ohne sich über das Gerät beugen oder es umdrehen zu müssen. Es wiegt 2,1 kg. und die Außenmaße betragen (B×T×H) 27,7 cm × 22 cm × 3 cm.\n\nDie AirPort-Antennen befinden sich hinter grauen Ausbuchtungen seitlich links und rechts vom Bildschirm auf dem Gehäuseoberteil. Der Steckplatz für die AirPort-Karte ist vom Akku-Fach aus zugänglich.\n\nIm Gegensatz zu seinen großen Brüdern besitzt es jedoch keine Tastaturbeleuchtung und keinen PCMCIA-Steckplatz.\n\nEs gibt zwei Varianten: die frühere besitzt nur einen VGA-Bildschirmausgang, die spätere besitzt einen Mini-DVI-Anschluss.\n\nDas Gerät, das es von Januar 2003 bis Mai 2006 gab, erfuhr zunächst reißenden Absatz, allerdings brachen die Verkäufe nach der Einführung des iBook G4 spürbar ein.\n\nDas PowerBook G4 12″, das unter dem Codenamen \"Thresher\" entwickelt wurde, wurde im März 2004 vom iF Industrie Forum Design für die gelungene Gestaltung mit einem Preis ausgezeichnet. Auf diesem PowerBook-Modell kann macOS bis einschließlich Mac OS X Leopard 10.5 betrieben werden. Auch Mac OS 9 läuft z. B. für ältere Programme.\n\nSeit der \"Auffrischung\" der PowerBook-Modellreihe zum 31. Januar 2005 verfügt das Gerät, ebenso wie das 15- und 17-Zoll-Modell, über zwei neuartige Funktionen, zum einen den von Apple patentierten Sudden Motion Sensor zum Schutz der Festplatte beim Herunterfallen des Gerätes, zum anderen ein neues Touchpad, mit dem es möglich ist, durch Auflage zweier Finger horizontal und vertikal in Fenstern zu scrollen.\n\nBestimmte Lithium-Ionen-Akkus, von der Sony Corporation in Japan hergestellt, wurden von Apple als Sicherheitsrisiko (durch seltene Fälle einer Überhitzung) eingestuft und in einem weltweiten kostenlosen Austauschprogramm auf freiwilliger Basis Ende 2006 zurückgerufen.\n\n2006 wurde die PowerBook G4-Reihe durch die MacBook Pros abgelöst, sowie die iBooks von den MacBooks. Dieses arbeitet mit einem Intel-Core-Duo-Prozessor und hat einen 13″-Bildschirm (33,78 cm diagonal).\n\n"}
{"id": "153094", "url": "https://de.wikipedia.org/wiki?curid=153094", "title": "Tastenkombination", "text": "Tastenkombination\n\nAls Tastenkombination (auch \"Tastaturkombination\", \"Tastaturbefehl\", \"Tastaturkürzel\", \"Tastenkürzel\", \"Tastensequenz\", \"Hotkey\", \"Shortcut\") wird das gleichzeitige oder aufeinanderfolgende Drücken mehrerer Tasten auf Computertastaturen in einer bestimmten Reihenfolge bezeichnet. Im Allgemeinen zählt man auch die Sondertasten (Funktionstasten und Ähnliches) alleine gedrückt zu den Tastaturbefehlen und zählt sie in Listen von Tastenkombinationen auf.\n\nMit einer Tastenkombination können bestimmte Steuerbefehle an ein Programm gesendet werden, beispielsweise „Programm starten“, „Datei öffnen“ und „Fenster schließen“. Außerdem können damit erweiterte Zeichen, wie Großbuchstaben und Sonderzeichen, eingegeben werden. Es können damit über numerische Zeichencodes auch Zeichen eingegeben werden, die auf dem Tastaturlayout nicht zu finden sind.\n\nBei nahezu allen modernen Programmen sind die Tastenkombinationen mehr oder weniger flexibel einstellbar. Viele Programme desselben Herstellers oder die, die für ein bestimmtes Betriebssystem geschrieben sind, unterstützen dieselben Tastenkombinationen. Das erleichtert deren Bedienung erheblich.\n\nEs gibt keinen betriebssystemunabhängigen, allgemeingültigen Standard für Tastenkombinationen, jedoch nennt Apple in den „Apple Human Interface Guidelines“ eine Liste von für das Betriebssystem reservierten Tastenkombinationen sowie verbindliche Vorgaben für die Funktionsweise bestimmter Tastenkombinationen in Anwendungsprogrammen. Viele dieser Tastenkombinationen wurden in anderen Betriebssystemen übernommen. Zu den bekanntesten gehören +, + und + für die Befehle Ausschneiden, Kopieren und Einfügen. Diese wurden in Windows und andere Betriebssysteme, in Ermangelung der Befehlstaste, als +, + und + übernommen. Auch Apples + für Öffnen, + für Drucken und + für Sichern wurden, wie viele andere Mac-typische Tastenkombinationen, in viele andere Betriebssystemen übernommen, wobei die Command-Taste meist durch die Steuerungstaste ersetzt wurde.\n\nEine vergleichbar große Bedeutung erlangte in der PC-Welt IBMs Richtlinie Common User Access.\n\nDas gleichzeitige Drücken mehrerer Tasten wird meist durch ein dazwischen liegendes Plus- oder Minuszeichen ausgedrückt. Gibt es kein solches Zeichen, ist damit oft das aufeinanderfolgende Betätigen der Tasten gemeint. Spitze oder eckige Klammern und Anführungszeichen verdeutlichen oft den Unterschied zwischen einer Tastenbezeichnung und einer einzugebenden Zeichenkette. Die Schreibung mit Großbuchstaben entspricht dabei dem Aufdruck auf der Taste: <nowiki><T>, [T]</nowiki>. Die Eingabe von Großbuchstaben wird explizit angegeben: <nowiki><Shift>+<T>, [Shift]+[T]</nowiki>. Beispiele:\n\nDie Emacs-Notation hat ihren Ursprung in der Frühzeit der Computertechnik. In den 1970er Jahren wurde diese Notation in Zusammenhang mit speziellen Tastaturen für die Lisp-Programmierung und dem schon damals vorhandenen Texteditor Emacs eingeführt. Gebräuchlich ist diese Schreibweise hauptsächlich in der Unix/Linux-Welt.\n\nDie Tastenkombination codice_3 bedeutete also: ++. Die Reihenfolge des Betätigens der Tasten „Ctrl“ und „Alt“ ist unwesentlich, wird jedoch als codice_3 angegeben, da das Gedrückthalten der Alt-Taste auch durch Drücken und Wiederloslassen von „Esc“ simuliert werden kann. Bei der Emacs-Notation wird die Umschalt-Taste nicht extra angegeben, sondern durch Großschreibung des letzten Buchstabens ausgedrückt:\ncodice_5 bedeutet also: +++.\n\nBei der Caret-Notation (engl. \"caret\" bedeutet u. a. \"Zirkumflex\") wird dem Buchstaben das Zirkumflex „codice_6“ vorangestellt: „codice_7“ bedeutet +.\n\nEine der bekanntesten Tastenkombinationen für PC-Betriebssysteme ist ++ bzw. ++, der sogenannte Klammergriff, da er meist mit beiden Händen ausgeführt werden muss. Diese Kombination war ursprünglich von IBM für den Reset, den Warmstart, des Rechners vorgesehen. In modernen PC-Betriebssystemen führt diese Tastenkombination eine Betriebssystemfunktion aus. Unter Windows wird je nach Version nur der Taskmanager aufgerufen oder ein Auswahlfenster mit sicherheitsrelevanten Funktionen angezeigt, unter KDE wird mit dieser Tastenkombination standardmäßig die Dialogbox zum Abmelden des Benutzers und zum Ausschalten oder Neustarten des Computers eingeblendet. Unter Linux kann sie beliebig konfiguriert werden. Unter Mac OS X besitzt diese Tastenkombination keine besondere Bedeutung, da dort ein Reset durch ++ oder durch eine spezielle Reset-Taste ausgelöst wird. Die \"ohne\" Zusatztasten ruft hingegen ein Dialogfenster auf zum Ausschalten, Neustarten oder Ruhezustand auslösen. Etwas dem Aufruf des Taskmanagers unter Windows grob Vergleichbares bietet unter Mac OS X die Tastenkombination ++, die das Fenster „Programme sofort beenden“ aufruft, in dem der Finder neugestartet sowie (insbesondere nicht mehr reagierende) Programme zwangsbeendet werden können.\n\nDie Liste ist für deutschsprachige MS Office-Versionen unter Microsoft Windows sowie deutschsprachige OpenOffice.org / LibreOffice-Versionen für Windows und Linux überprüft. Einige der im Folgenden genannten Tastenkombinationen (wie + ) können als allgemeingültig angesehen werden, andere sind spezifisch für diese Programmpakete. So folgen die meisten anderen Textverarbeitungsprogramme und -editoren auch in der deutschen Lokalisierung der englischsprachigen Nomenklatur, insbesondere bei den gängigen Schriftauszeichnungen:\nHinzu kommen weitere Inkonsistenzen zwischen den einzelnen Betriebssystemen, so wird beispielsweise auf einem Mac auch bei der deutschsprachigen MS-Office-Ausgabe der Fettdruck mit + erzeugt. Zudem können bei OpenOffice.org / LibreOffice Tastenkombination durch den Benutzer nach Belieben geändert werden.\n\n\n\nDie Betätigung von Hilfstasten hat keine eigene Wirkung, sondern verändert die Interpretation von anderen Tasten. (Allerdings reicht ihre alleinige Betätigung meistens auch schon zum „Aufwecken“ eines Computers aus einem Bereitschaftsbetrieb bzw. Ruhezustand.) Üblicherweise wird eine Hilfstaste oder Hilftastenkombinationen mit einer regulären Taste kombiniert gedrückt, um die Wirkung des Drucks der regulären Taste für die Dauer der Kombinationseingabe zu modifizieren.\n"}
{"id": "153173", "url": "https://de.wikipedia.org/wiki?curid=153173", "title": "Ars Electronica Center", "text": "Ars Electronica Center\n\nDas Ars Electronica Center (AEC), auch als „Museum der Zukunft“ bezeichnet, ist ein Museum in Linz. Es wurde 1996 in Linz-Urfahr eröffnet und hat sich zum Ziel gesetzt, die Technologien der kommenden Generationen bereits in der Gegenwart für jede Altersstufe erfahrbar zu machen.\n\nDabei werden verschiedene Kunst-, Wissenschafts- und Technologierichtungen angesprochen und miteinander verwoben (z. B. Bio- und Gentechnik, Neurowissenschaften, Robotik, Prothetik und Medienkunst). Alle Ausstellungen sind auf die Frage ausgerichtet, wie der Mensch mit seiner Umgebung umgehen kann und bieten dafür unterschiedliche Perspektiven. Die Besucher haben die Möglichkeit, mit den Installationen und Exponaten zu interagieren; Partizipation an den ständig wechselnden Ausstellungen ist ausdrücklich erwünscht.\n\nIm \"Deep Space 8K\" bietet das AEC eine einzigartige virtuelle Welt in Form von Wand- und Bodenprojektionen (jeweils 16 mal 9 Meter), Lasertracking und 3-D-Animationen. Die Bildwelten (z. B. aus den Bereichen Medizin, Geologie und Astronomie) werden in 8K-Auflösung projiziert.\n\nEs werden auch verschiedene Führungen und Workshops sowie Programme für Schulen angeboten.\n\nDas AEC beinhaltet neben dem Museum etwa auch das \"Futurelab,\" ein Atelier und Labor für Kunst und Forschung. Seit 1996 betreibt es Forschung und Entwicklung an der Schnittstelle von Kunst, Technologie und Gesellschaft. Das Futurelab setzt sich unter anderem mit künstlicher Intelligenz, Robotik, Medienarchitektur, interaktiven Technologien, neuen ästhetischen Ausdrucksformen oder Schwarmintelligenz auseinander.\n\nEinmal jährlich wird im AEC der Prix Ars Electronica für Computerkunst verliehen. 1999 stiftete das AEC gemeinsam mit der Stadt Linz und dem ORF den Marianne-von-Willemer-Preis. Jedes Jahr findet auch das Ars Electronica Festival statt, das seit 1979 durchgeführt wird, das international bedeutendste Festival der digitalen Kunst darstellt und Trends bzw. langfristige Entwicklungen zukunftsorientiert in Form künstlerischer Werke, Diskussionsforen und wissenschaftlicher Begleitung aufgreift.\n\nIm Hinblick auf Linz 2009 – Kulturhauptstadt Europas wurde das AEC für etwa 30 Millionen Euro umgebaut und zur Jahreswende 2008/09 neu eröffnet; es hat seitdem eine Gesamtfläche von 6.500 m². Der Umbau mit Erweiterung wurde von der Stadt Linz durchgeführt, wobei das Architektenbüro Treusch architecture ZT GmbH anhand eines Wettbewerbs ausgewählt wurde.\n\nNeben dem bisherigen Gebäude wurde ein zweites mehrstöckiges Gebäude errichtet. Beide wurden mit einer Glasfassade umhüllt, die sich in der Nacht in eine Lichtskulptur verwandelt. 40.000 Leuchtdioden (rot, grün, blau und weiß) sorgen dabei für abwechslungsreiche Farbenspiele. Ein Vorplatz, der am östlichen Ende wieder höher wird, und an dieser Stelle mit Sitzstufen versehen ist, verläuft parallel zur Donau.\n\nAm 1. Mai 2011 gab die Österreichische Post AG im Rahmen der Dauermarkenserie Kunsthäuser eine Briefmarke zu dem Objekt aus.\n\n2015 hatte das AEC mit einer Besucherzahl von 171.800 um 8,4 % mehr im Vergleich zum Vorjahr. Die Steigerung führten die Verantwortlichen auf die Präsentationen im \"Deep Space 8K\" zurück. 2015 wurde auch ein Kinder-Forschungslabor für 4- bis 8-Jährige eingerichtet.\n\nDas im Gebäude befindliche \"Cubus\" ist ein Restaurant und Kaffeehaus, in den Abendstunden eine Bar, und dient auch als Aussichtspunkt auf die Donau und das Panorama der Stadt Linz. Ein Außenlift ermöglicht den Zugang auch ohne Museumsbesuch.\n\n\n\n"}
{"id": "153575", "url": "https://de.wikipedia.org/wiki?curid=153575", "title": "PowerBook G4 Titanium", "text": "PowerBook G4 Titanium\n\nDas PowerBook G4 Titanium war eine Notebookserie der Firma Apple, Apples erste Notebookserie, dessen Gehäuse überwiegend aus Metall bestanden, während die vorherigen Serien alle Kunststoffgehäuse hatten.\n\nEs gab insgesamt vier Generationen von PowerBooks in der Titanium-Baureihe mit im Wesentlichen gleichen Gehäusen. Die Titanium-Baureihe hatte einen 15-Zoll-Panoramabildschirm und basierte erstmals auf dem PowerPC-G4-Prozessor, der je nach Modell zwischen 400 MHz und 1 GHz getaktet wurde.\nDas Titanium war Apples letztes Notebook, das die Anschlüsse (USB, FireWire, Display, Ethernet etc.) im Wesentlichen auf der Rückseite des Gerätes angebracht hatte, alle nachfolgenden Modellreihen haben die Anschlüsse seitlich.\nDas sehr flache Gehäuse aus Titan-Blechen (englisch \"Titanium\") und Kunststoffrahmen machte den Computer relativ leicht (Gewicht leicht variierend je nach Ausstattung unter 2,5 kg), war allerdings auch empfindlich. Es bildeten sich häufig Risse in den Blechen, vor allem im Bereich der Scharniere des Displays. Ebenso anfällig für Schäden war der Verriegelungsmechanismus des Displaydeckels sowie die Lackierung, die zur Ablösung neigte.\n\nDer Speicher war durch den Kunden selbst erweiterbar, indem die durch Magnete und eine Verriegelung gehaltene Tastatur abgehoben wurde, um Zugang zu den Steckplätzen zu erhalten. Ein Festplattentausch erforderte jedoch das Aufschrauben des Gehäuses, ebenso der Einbau der (in den meisten Ausstattungen nicht werksseitig verbauten) Airport-WLAN-Karte.\n\nDas Titanium-PowerBook war noch im Programm, als bereits 12″- und 17″-PowerBooks mit Aluminiumgehäuse eingeführt waren. Zusammen mit Einführung der zweiten Generation der 12″- und 17″-PowerBooks wurde das Titanium-PowerBook schließlich vom PowerBook G4 15 Zoll abgelöst.\n\nDie Angaben zu Speichergröße und Festplattenplatz betreffen die zum Zeitpunkt der Auslieferung bestellbaren Optionen, mit aktuellen Komponenten lässt sich der Hauptspeicher auf 1 GB aufrüsten, statt einer Festplatte kann auch eine SSD mit IDE/ATA-Anschluss eingebaut werden.\n"}
{"id": "153644", "url": "https://de.wikipedia.org/wiki?curid=153644", "title": "Power Macintosh", "text": "Power Macintosh\n\nPower Macintosh oder Power Mac ist der Produktname für die Macintosh-Modelle des Unternehmens Apple mit PowerPC-Prozessor. Die vorher nur umgangssprachlich benutzte Bezeichnung „Power Mac“ wird von Apple seit den G4-Modellen offiziell verwendet, zugleich wurde der Begriff „Macintosh“ fallen gelassen.\n\nDie erste Power Macintosh-Reihe wurde im März 1994 eingeführt und stellte für die Anwender einen nahtlosen Übergang auf eine völlig neue Art von Prozessor dar: Die konventionellen Prozessoren der Motorola-68k-Baureihe wurden abgelöst durch den RISC-Prozessor PowerPC 601.\n\nDer Übergang vom 68k- zum PowerPC-Prozessor gilt als technische Meisterleistung. Außer einer höheren Geschwindigkeit änderte sich für den Anwender so gut wie gar nichts: Das Betriebssystem war System 7.1.2 statt 7.1 und sah im Wesentlichen genauso aus wie vorher, nur im Systemordner lag ein spezieller System Enabler. Auch die Gehäuse der neuen Rechner waren mit nur kleinen Änderungen vom Vorgänger, der Quadra-Baureihe, übernommen worden. Sie unterschieden sich äußerlich fast nur durch einen anderen Diskettenlaufwerkseingriff und den zusätzlichen Schriftzug \"PowerPC.\" Der Startgong der Power Macintosh der ersten Generation wurde von dem Jazz-Gitarristen Stanley Jordan gestaltet und klingt ungefähr wie ein angeschlagener Gitarrenakkord. Bei späteren Macintosh-Modellen mit PowerPC-Prozessor wurde hingegen wieder der mit den Macintosh Quadra AV eingeführte Startgong verwendet.\n\nDas damalige klassische Mac OS war monolithisch (erst seit Mac OS 8.6 basierte es auf einem Nanokernel) und unterstützte bis zum Erscheinen der Power Macs ausschließlich 68k-Prozessoren. Mit System 7.1.2 wurden sowohl PowerPC- als auch 68k-CPUs unterstützt, wobei auf Power Macs fast alle für den alten Prozessor geschriebenen Programme weiterhin lauffähig waren, und zwar in einer Emulation, von der der Anwender nichts merkte. Die Emulation, die vom Prozessor-Hersteller Motorola stammte, wurde im Laufe der Zeit beschleunigt, außerdem wurden nacheinander immer mehr Teile des Systems von 68k- in PowerPC-Code übersetzt, wodurch die Ausführungsgeschwindigkeit auf Power Macs im Laufe der Betriebssystemversionen immer weniger gebremst wurde.\n\nProgramme konnten sowohl den 68k- als auch den PowerPC-Code enthalten; der Code Fragment Manager (CFM) sorgte für die Modusumschaltung zur Laufzeit. Derartige Programme heißen \"Fat Binaries\", dabei liegen die 68k-Codefragmente in bestimmten Sektionen der \"resource fork\" und der PowerPC-Code in der \"data fork\".\n\nDas Ende der Power Macs läutete der 2005 verkündete Wechsel von der PowerPC- zur x86-Architektur ein. Dieser Mitte 2006 abgeschlossene Übergang ist dem schon 1994 gezeigten Wechsel des Hauptprozessors vom Motorola 68k sehr ähnlich. Die Programme wurden, üblicherweise mit der Entwicklungsumgebung Xcode, als sogenannte auf Basis von Cocoa erzeugt, die sowohl PowerPC- als auch x86-Binärcode enthalten und so auf beiden Prozessortypen nativ ablaufen können. Das Betriebssystem, genauer gesagt der Mach-Kernel XNU, lädt daraus den jeweils nötigen Code für den Prozessor. Ältere Mac-OS‑X-Programme, die nur in PowerPC-Maschinencode vorliegen und die keinen x86-Binärcode aufweisen, also nicht als „“ vorliegen, liefen ab Mac OS X Tiger/Intel (10.4, 2005) bis Mac OS X Snow Leopard (10.6, 2009) transparent mit dem integrierten Rosetta-Emulator. Dazu zählen auch PowerPC-Programme, die sowohl unter Mac OS bis 9 als auch unter Mac OS X nativ laufen ().\n\nAb Mac OS X Lion (10.7, 2011) sind reine PowerPC-Programme für Mac OS X (Cocoa und Carbon) nicht mehr ausführbar, weil Rosetta ab dieser Version von Mac OS X nicht mehr unterstützt wird. (Mac OS X heißt ab 2012 nur mehr „OS X“ und ab 2016 „macOS“.)\n\nAlte 68k-Programme, die unter Mac OS X auf dem PowerPC nur in der Classic-Umgebung liefen, werden von x86-Macs nicht mehr unterstützt, können jedoch mit einem Emulator, wie z. B. SheepShaver, auch auf aktuellen Apple- und Windows-Systemen verwendet werden, was allerdings ein im Emulator installiertes originales klassisches Mac OS voraussetzt.\n\nDie frühen Power Macintosh basierten auf der Reihe PowerPC 60x, ihnen folgen die G3-, G4- und G5-Prozessoren.\n\nVon Unternehmen wie Metabox (joeCard), Phase5 (Power Booster, Maccelerate) und anderen (Sonnet, …) gab es Mitte/Ende der 1990er-Jahre zahlreiche Upgrade-Karten für unterschiedliche Macs, die diese mit schnelleren G3- und G4-CPUs versahen. Einige der entsprechenden Unternehmen waren eine Zeit lang auch im Markt für Mac-Clones aktiv, bis sich Apple wieder vom offenen CHRP-Plattformansatz distanziert hat.\n\nAm 7. August 2006 wurde der Power Mac aus dem Sortiment genommen und durch den mit Intel-Prozessor laufenden Mac Pro ersetzt. Um softwareseitig zwischen PowerPC- und Intel-Mac unterscheiden zu können, wurden die Bezeichnungen „Mac“ für die nunmehr „klassischen“ Macs mit PowerPC-Prozessor und „Mac/Intel“ für Intel-basierte Macs verwendet. Das war zur Zeit der Umstellung wichtig, da neuere Programme für Mac OS X nur noch auf Mac/Intel liefen. Ab Mac OS X Snow Leopard (10.6, 2009) hat Apple die Unterstützung für PowerPC-Prozessoren seitens des Betriebssystems fallen gelassen und ab Mac OS X Lion (10.7, 2011) die Unterstützung für PowerPC-Applikationen (Rosetta) komplett eingestellt, sodass seither der Zusatz „/Intel“ nicht mehr verwendet wird. Umgekehrt findet sich in Anlehnung an „Mac OS X/Intel“ nun für ganz alte Software gelegentlich „Mac OS X/PPC“ oder „Mac OS X/PowerPC.“\n\nEs gab drei Produktlinien: 6100, 7100 und 8100. Der Power Macintosh 6100 besaß das flache Gehäuse des Macintosh Quadra 610; er war zunächst mit 60, dann mit 66 MHz getaktet und besaß einen PDS. Der Power Macintosh 7100 besaß das massive Metallgehäuse des Macintosh Quadra 650 und eine Weiterentwicklung der Wombat-Platine mit drei NuBus-Steckplätzen und einem Processor Direct Slot (PDS), der standardmäßig mit einer Grafikkarte oder einer Videodigitalisierkarte (Modell 7100/80 AV) bestückt war, die als Grafikkarte benutzt werden konnte. Der Power Macintosh 7100 war zunächst mit 66, dann mit 80 MHz getaktet. Ähnlich ausgestattet war der Power Macintosh 8100, der das Gehäuse des Macintosh Quadra 800 nutzte und zunächst mit 80, dann mit 100 und 110 MHz getaktet war. Alle Rechner waren mit einem 16-bit-Stereotonsystem ausgestattet und besaßen 32/64-bit.\n\n\"Apple Power Macintosh 6100/60\" (flaches Gehäuse)\n\n\"Apple Power Macintosh 6100/66\" (Nachfolger des Apple Power Macintosh 6100/60, flaches Gehäuse)\n\n\"Apple Power Macintosh 6100/60 AV\" (flaches Gehäuse)\n\n\"Apple Power Macintosh 6100/66 AV\" (Nachfolger des Apple Power Macintosh 6100/60 AV, flaches Gehäuse)\n\n\"Apple Power Macintosh 6100/66 DOS-Compatible\" (erster Power Macintosh mit einer Karte für DOS-Kompatibilität)\n\n\"Apple Power Macintosh 6200/75\" (Nachfolger für alle 6100er)\n\nDie 5xxx-er-Serie (auch unter der Bezeichnung \"Performa 5200\" verkauft) kommt als All-in-one-Gehäuse daher. Monitor und Rechner sind gemeinsam in einem Gehäuse untergebracht. Seine Nutzer schätzen ihn als den wohl langsamsten Power Mac ein, der je Apples Werkhallen verließ. Seine Hauptplatine ist eine Weiterentwicklung derer des LC630, das für 68000er-Prozessoren ausgelegt war, verbindet dadurch die Möglichkeit, PPC-Software zu nutzen, aber gleichzeitig die proprietären Erweiterungskarten der 630er-Reihe zu nutzen (Video-In, CommSlotI, TV-In). Viele LC-PDS-Karten funktionieren ebenfalls darin. Es gab außerdem die 62xx und 63xx Serie, die die identischen Hauptplatinen im klassischen Desktopgehäuse des LC630 verwendeten.\n\nZunächst wurde der PPC603 mit 75 MHz Prozessortakt bei 37,5 MHz Bustakt fest verlötet verbaut, später PPC603e mit bis zu 120 MHz Prozessortakt bei 40 MHz Bustakt. Für Grafik standen nur 1 MB RAM zur Verfügung, der Hauptspeicher konnte bis auf 64 MB ausgebaut werden.\n\nDie 5xxx-Serie wurde ab April 1995 bis Anfang 1997 gebaut. Alle Rechner waren mit einem 16-bit-Stereotonsystem ausgestattet und besaßen ein 32/64-bit-System.\n\n\"Apple Power Macintosh 5200/75 LC\"\n\n\"Apple Power Macintosh 5200/100 LC\" („Würfelmac“ mit eingebautem 15″-Bildschirm)\n\nFür rund 2400–2700 DM gab es ab 1995 den kleinsten PCI-Rechner von Apple. Er war als Nachfolger des Power Macintosh 7100 konzipiert, lief jedoch mit seiner technischen Ausstattung und dem günstigen Preis auch den damals aktuellen PowerPC-Performas 5200 und 6200 in den eigenen Reihen den Rang ab. Der Power Macintosh 7200 war als günstige Alternative zu der zeitgleich eingeführten Modellreihe Power Macintosh 7500 (im gleichen Gehäuse) gedacht. Über diesen beiden Rechnern lag damals der ebenfalls gleichzeitig erschienene Power Macintosh 8500 und das etwas ältere Spitzenmodell Power Macintosh 9500. Dieser teurere 9500 hatte als erster Mac den Umstieg des Unternehmens Apple von NuBus auf PCI eingeläutet, mit dem 7200, 7500 und 8500 wollte man jetzt die für Macs neue Technik günstiger einem breiteren Markt zugänglich machen. Apple musste reagieren, hatte man den PCI-Zug, der damals bei PCs längst zur Regelausstattung gehörte, eigentlich schon verpasst. Der 8200er wurde 1996 nachgeschoben und war eine hochgezüchtete Variante des 7200.\n\nErsetzte den Power Macintosh 7200 und 7600 und war ab Februar 1997 rund 10 Monate lang auf dem Markt.\n\nDie PPCs mit AV-Einheit, 3 × PCI, 8 × RAM.\nDurch Cinch-Buchsen war die Ein-/Ausgabe von Stereoton möglich, und die Power Macs ließen sich an eine Stereoanlage anschließen. Alle Apple-Computer ohne AV-Karte wurden über die Mini-Klinkenstecker mit Aufnahme- und Wiedergabegeräten verbunden. Die Aufnahme eigener Systemklänge oder Musik war bei Apple bereits in den 1980er-Jahren möglich, nun kam die Wiedergabe und Aufnahme von Videosignalen (ab Modell 7100/80AV) hinzu.\nMit QuickTime oder Zusatzprogrammen, wie z. B. VideoShop, konnte man während der Arbeit am Rechner fernsehen oder Videos abspielen.\n\nDie Highend-Macs mit 6 PCI-Steckplätzen, davon einer belegt mit Grafikkarte, und 12 Ramslots. Der verbaute Prozessor war der 604, später der 604e. Die schnellsten Geräte der Baureihe erreichten 350 MHz (9600).\n\nEinstiegsmodelle mit 603er-CPU, nur drei (allerdings sehr schnellen) RAM-Steckplätzen für maximal 160 MB (2 × 64, 1 × 32), mit PCI-Riser-Karte, je nach Modell drei PCI-Steckplätzen oder zwei PCI-Steckplätzen und einem ComSlot II für Modem- oder Netzwerkkarten. Der Rechner war um das Motorola Tanzania Board aufgebaut, das Motorola eigentlich für Clone Hersteller vorgesehen hatte. Der Rechner kam ab Werk ohne einen L2 Cache, der ihn wesentlich beschleunigte. Man konnte diesen jedoch nachrüsten und viele Händler boten den Rechner auch mit diesem Upgrade an.\n\nAls Nachfolger der LC-PDS PowerMacs (siehe oben) aktualisierte Apple das Mainboard auf einen halbwegs aktuellen Stand und setzte nun auch im unteren Preisbereich auf PCI. Dadurch wurde die Einsteigerserie deutlich aufgewertet. Sie wurde durchwegs mit deutlich schnelleren Prozessoren und schnelleren Speichermodulen verkauft. Währen die 5er- Reihe das All-In-One Gehäuse mit dem bewährten 15 Zoll Monitor des Vorgängers übernahm, wurde die 6er Reihe auf einen Mini-Tower umgestellt. Dieser beinhaltete einen Subwoofer und konnte zwei PCI Karten aufnehmen. Die Serie umfasste Prozessoren von 180 bis 300 MHz und es gab später sogar noch G3 Upgrade-Karten von Drittherstellern. Mit dem Erscheinen des iMac wurden auch diese Einsteigermodelle abgelöst.\n\nDer G3 kam in zwei Generationen auf den Markt. Dieser Absatz beschreibt die ersten Modelle, diese hatten ein graues (Apple-Jargon: „Platinum“) Gehäuse.\n\nDer Ende 1997 eingeführte Power Macintosh G3 läutete einen Paradigmenwechsel bei Apple ein. Der G3 war in der Grundüberlegung des Rechneraufbaus eigentlich näher an Apples damaliger Privatanwenderlinie („Performa“), erkennbar erstens an der weitaus geringeren internen Erweiterbarkeit (verglichen mit den Gehäusen der Profi-Macs) und zweitens an der internen Festplattenanbindung (erstmals über IDE statt SCSI). Allerdings stellte sich heraus, dass die schiere Rechenleistung des G3, bedingt durch den schnelleren Prozessor und den um 16 MHz schnelleren Systembus, dem bis dahin schnellsten Rechner, dem teuren Platzhirsch Power Mac 9600, deutlich überlegen war. Der eher als Heimanwendergerät ausgelegte Rechner war damit leistungsfähiger, als von Apple ursprünglich vorgesehen.\n\nEs gibt drei Varianten: Desktop, Mini-Tower (beide mit dem Codenamen Gossamer) sowie den \"All-In-One\" (Codename \"Artemis\"). Der Desktop (liegend) ist am häufigsten anzutreffen, der Mini-Tower (fällt durch seine zweigeteilte Form auf) ist bedingt durch den damals beträchtlichen Preisaufschlag, deutlich seltener.\n\nDie Rechner haben einen Grafikchip von ATI auf der Hauptplatine sowie SCSI und Netzwerk (10 MBit Ethernet), zwei serielle Schnittstellen und ADB-Anschlüsse auf der Hauptplatine. Den „beigen Desktop“ gab es mit G3-Prozessoren von 233 MHz aufwärts; im Tower wurden sogar 366 MHz-Prozessoren verbaut. Es sind drei PCI-Steckplätze vorhanden; die Nachrüstung einer Grafikkarte, USB, Firewire, 10/100 Ethernet oder schnelleren IDE/SATA-Anschlüssen ist unproblematisch. Der Arbeitsspeicher kann bis auf 768 MB mit handelsüblichem SD-RAM (von 66 bis 133 MHz) aufgerüstet werden.\n\nDie im Chipsatz enthaltenen IDE-Schnittstellen unterstützen lediglich den Modus „Multiword-DMA“ und begrenzen daher die Datentransferrate auf maximal 16 Megabytes/s, allerdings werden auch „große“ Festplatten (bis 128 GiB, entsprechend 137 GByte) unterstützt. Die sogenannten „Old-World-Macs“ erfüllen alle Vorgaben für den Betrieb von Mac OS X 10.2 („Jaguar,“ 2002), jedoch unter der Voraussetzung, dass mindestens 128 MB Arbeitsspeicher zur Verfügung stehen. Die neueste noch offiziell unterstützte Betriebssystemversion ist 10.2.8, jedoch können Mac OS X Panther (10.3, 2003) und Mac OS X Tiger (10.4, 2005) u. a. mit Hilfe des Shareware-Programms \"XPostFacto\" installiert werden.\n\nDieses damals schon wieder außergewöhnliche Baumuster erinnerte in der Ferne an den ersten Würfel-Mac. Sein Hauptmerkmal ist der in das Gehäuse integrierte 15″-Monitor. Als seine unmittelbaren Vorgänger sind die Rechner der Power Mac 5000er-Serie anzusehen. Sein ausschließliches Arbeitsfeld war der Bildungsmarkt. Unmittelbarer Nachfolger dieses außergewöhnlichen G3 wurde Apples Welterfolg, der iMac.\n\nDie spätere Version des Power Mac G3 hatte ein (Mini-)Tower-Gehäuse, das mit farblosem und grünblau-transluzentem Kunststoff verkleidet war (im Apple-Jargon heißt das Gerät „Blau und Weiß“, \"engl.\" ) und die charakteristischen Tragegriffe. Das Gerät wurde mitunter spöttisch als „Schlumpf“ oder „Tupperdose“ bzw. „Tuppermac“ bezeichnet.\n\nDas Gerät hatte je zwei USB- und FireWire-Anschlüsse (USB 1.1 und FW 400). ADB war noch vorhanden, um die alte Tastatur oder andere vorhandene ADB-Geräte anschließen zu können. Die Grafik wurde von der Hauptplatine in einen 66‑MHz-PCI-Steckplatz verlegt, das Konzept der „Personality Card“ aufgegeben. Ein neu entwickelter IDE-Chip sorgte mit UDMA 33 für einen schnelleren Datentransfer zur Festplatte, während die anderen Laufwerke noch über den alten, langsameren IDE-Kanal angebunden waren. Wie auch beim beigefarbenen G3 war das Innere des Gehäuses besonders leicht zugänglich; durch einen Griff konnte die Seitenwand des Gehäuses heruntergeklappt werden. Darauf befand sich die Hauptplatine mit den Speicherbausteinen, dem Prozessor usw. Im Gehäuse verblieben die Laufwerke (darunter bis zu drei Festplatten, die am Gehäuseboden befestigt werden), die somit sehr leicht zugänglich waren. Das Design ist dabei so durchdacht, dass im laufenden Betrieb die Seitenklappe geöffnet werden kann (Apple rät allerdings davon ab).\n\nVom blauweißen G3 gibt es die Revisionen 1 und 2. Die Revision 1 hat einen defekten IDE-Chip, der bei fast allen größeren (mehr als 8 GByte) Festplatten Datenübertragungsfehler verursacht. Um dieses Problem zu umgehen, benötigt man eine PCI-IDE-Karte (ATA oder SATA), die auch notwendig ist, um große Festplatten (mit mehr als 128 bzw. 137 GByte) ausnutzen zu können. Als alternative reine Software-(Not)-Lösung kann ein Festplattentreiber eines anderen Herstellers genutzt werden, bei dem manuell der DMA-Modus auf „Multi-Word“ umgestellt werden kann (z. B. HardDiskToolkit des Unternehmens FWB). Die zweite Revision des blauweißen Power Mac G3 hat eine Vorrichtung („U bracket“), um jeweils eine weitere („IDE Slave-“)Festplatte oberhalb der beiden vorhandenen Einbauplätze einzubauen (dann passen bis zu vier Festplatten ins Gehäuse), einen verbesserten IDE-Chip (ohne die Datenfehlerprobleme der Revision 1) sowie eine etwas schnellere Grafikkarte (Rage 128 mit höherer Taktfrequenz).\n\nDer „Yosemite“ ist Apples erster PC mit Firewire-Schnittstelle; diese ist auch nicht in den Chipsatz integriert, sondern auf einer kleinen Zusatzplatine untergebracht. Auffallend viele Besitzer des Rechners klagen über teilweisen oder vollständigen Ausfall der Firewire-Schnittstelle oder über unzuverlässigen Datentransfer. Abhilfe schafft – wie bei der defekten IDE-Schnittstelle – eine FireWire-PCI-Einsteckkarte.\n\nDer blauweiße Power Mac G3 ist der älteste Macintosh, der das Betriebssystem Mac OS X Tiger (10.4, 2005) unterstützt. Da die G3-Prozessoren von Mac OS X Leopard (10.5, 2007) nicht mehr unterstützt werden, ist 10.4.11 die letzte noch lauffähige Betriebssystemversion.\n\nIm Sommer 1999 wurden als Nachfolger des blauweißen G3 gleich zwei Modelle des Power Mac G4 angekündigt: Eine Version mit PCI-Grafik und eine mit AGP-Grafik (so Apples offizielle Unterscheidung).\n\nDer Power Mac G4 hatte die gleiche Gehäuseform wie der blauweiße G3, war aber in transluzentem Kunststoff und silberfarbenen Seitenflächen gehalten. Diese Farbzusammenstellung wurde „Graphite“ genannt. Die Modelle besaßen den PowerPC-7400-Prozessor (auch „G4“ bezeichnet, daher die Apple-Bezeichnung „Power Mac G4“) von Motorola mit AltiVec (Apple-Jargon: \"Velocity Engine\"). Einige Programme von Apple, wie zum Beispiel iDVD oder GarageBand, setzen den G4-Prozessor (oder einen neueren) grundsätzlich voraus.\n\nBeim Modell mit PCI-Grafik („Yikes“) lag der Unterschied zum Vorgängermodell (G3 „Yosemite“) einzig in der Verwendung des G4-Prozessors. Die Grafikkarte war nach wie vor eine 16 MB ATI Rage 128 im PCI-Slot und die Hauptplatine („Logic Board“) baugleich mit der des Vorgängers. Der Rechner wurde zunächst mit 400 MHz angeboten, aber wegen Problemen mit der schnellsten Serie (siehe nächster Absatz) wurden nach kurzer Zeit alle G4 mit 50 MHz weniger verkauft. Die PCI-G4 liefen deswegen mit 350 MHz. Genau wie der Yosemite G3 verfügten die Rechner über USB- und Firewire-Schnittstellen. Es wurde gemutmaßt, dass Apple noch eine größere Menge von Hauptplatinen des Vorgängermodells weiterverwenden wollte, und so wurde das „Yikes“-Modell auch bis Ende 1999 ausverkauft. Somit mussten Yikes-Käufer mit einem veralteten IDE-Controller mit UDMA-33 und dem gegenüber dem Yosemite nicht veränderten FSB-Takt von 100 MHz vorliebnehmen.\n\nDas Modell mit AGP-Grafik („Sawtooth“) war etwas später erhältlich. Eigentlich wollte Apple zunächst Varianten anbieten, eine einfache mit 450 MHz und eine besser ausgestattete mit 500 MHz. Weil Motorola die 500 MHz-CPU nicht in ausreichenden Mengen liefern konnte, stufte Apple alle G4-Rechner im September 1999 (kurz nach der Markteinführung) um 50 MHz herunter. Die AGP-Modelle waren demnach lediglich mit 400 und 450 MHz zu haben, das PCI-Modell (siehe vorheriger Absatz) mit 350 MHz. Erst im Februar 2000 lieferte Apple die G4-Macs mit den ursprünglich angekündigten Taktfrequenzen von 450 und 500 MHz aus.\n\nAb Juli 2000 erhielt der G4 mit der Bezeichnung „Gigabit Ethernet“ erstmals einen Gigabit-Ethernet-Anschluss und die Möglichkeit, die Apple-Flachbildschirme (15″, 17″ und 22″, später auch 20″ und 23″) im Acryl-Design mit dem ADC-Anschluss der Grafikkarte zu verbinden. Der Vorteil des ADC-Anschlusses besteht in der Verbindung des Video-Signals (DVI), der Stromversorgung und des USB-Signals in einem einzigen Kabel. Bis zur vorletzten Power-Mac-G5-Baureihe (Early 2005) gab es die Möglichkeit, den ADC-Anschluss ohne Adapter zu nutzen.\n\nDer FSB des „AGP Graphics“ und des „Gigabit Ethernet“ lief mit 100 MHz (erst ab „Digital Audio“ mit 133 MHz). Die neue Hauptplatine war speziell für den G4-Prozessor und dessen Busprotokoll entwickelt worden, der Rechner daher deutlich schneller als das Modell mit PCI-Grafik. Der IDE-Controller konnte Festplatten mit UDMA-66 ansprechen. Es gab auch für die beiden USB-Anschlüsse jeweils einen eigenen Controller, während beim Yikes die zwei USB-Anschlüsse von nur einem Controller versorgt wurden. Dadurch konnten ab AGP-Grafik beide USB-Anschlüsse gleichzeitig mit voller Geschwindigkeit genutzt werden.\n\nDer letzte der graphitfarbenen G4-Rechner war der „Digital Audio“. Dieser besaß keinen Mikrofonanschluss mehr, sondern neben dem Lautsprecher-Anschluss einen aktiven Apple-Pro-Speaker-Anschluss (2,5 mm Klinke, vierpolig). Weiterhin hatte der G4 erstmals einen 4 × AGP-Steckplatz.\n\nSpätere Modelle waren häufig mit zwei Prozessoren ausgestattet, da der Prozessorhersteller Motorola die Taktraten des PowerPC 74xx (alias G4) über mehrere Jahre hindurch nur geringfügig steigern konnte und sich dadurch der Leistungsabstand zu Intel-kompatiblen PCs zunehmend vergrößerte. Das Dualprozessor-Konzept sollte diese Lücke verkleinern. Später wurden PowerPC-Prozessoren auch von IBM hergestellt.\n\nEine Sonderform des Power Mac G4 war der Power Mac G4 Cube, der mit seinem lüfterlosen Konzept sowie seinem würfelförmigen Äußeren in einem Kunststoffgehäuse eine völlige Neuentwicklung darstellte. Allerdings war er sehr teuer und schlecht erweiterbar, weshalb der Verkauf schon nach nur einem Jahr wieder eingestellt wurde. Das Design gilt zudem als fehlerhaft, da es am Gehäuse häufig zu Spannungsrissen kam und der Cube allgemein durch das Fehlen eines Lüfters mit Hitzeproblemen zu kämpfen hatte. Dennoch galt der Cube mehr als zehn Jahre nach seiner Einführung immer noch als gefragtes Sammlerstück. Er wird im Museum of Modern Art in New York ausgestellt.\n\nDie 2. Auflage des Power Mac G4 „Quicksilver“ von 2002 (daher auch „Quicksilver 2002“) war erstmals in der Lage, mit IDE-Festplatte von mehr als 128 GiB (137 GB) Speicherkapazität umzugehen, da dessen Open Firmware um die Unterstützung für LBA48 erweitert worden war. Größere Festplatten funktionieren in allen G4-Power-Macs mit AGP (da diese einen KeyLargo-IDE-Chip nutzen) dennoch, jedoch nur mit einem komplizierten Umgehen der LBA28-Grenze von 128 GB und viel Vorsicht bei der Partitionierung.\n\nDer erste Power Mac, der nicht mehr Mac OS 9 booten konnte, war das 2003 erschienene Modell mit FireWire-800-Unterstützung. Wegen der großen Nachfrage nach Mac-OS‑9-Bootfähigkeit legte Apple danach den dual-bootfähigen „Mirrored Drive Doors“ mit geringen Modifikationen wieder auf, der bis etwa Mitte 2004 verkauft wurde. Damit konnte sowohl klassisches Mac OS in Form von Mac OS 9.2.2 als auch das moderne Mac OS X ab Version 10.2 („Jaguar,“ 2002) gestartet werden.\n\nDer Doppelprozessor-Mac „FW 800“ (2003) mit 1,42 GHz war der leistungsfähigste Power Mac G4, der je von Apple gebaut worden war. Ausgestattet mit einem mit 167 MHz getakteten Front Side Bus und dem 2 MB großen L3-Cache pro Prozessor soll er von den Leistungsdaten her mit einem niedrig getakteten Power Mac G5 (1,6 GHz) vergleichbar sein.\n\nSpeziell die Sawtooth-Hauptplatine erwies sich als zukunftsträchtig, u. a. weil die CPU-Platine gesockelt war. So konnte ein ursprünglich mit einem G4 von 400 MHz ausgerüsteter Rechner noch acht Jahre nach dessen Erscheinen (Ende 1999, Ende 2007) mit z. B. zwei G4-Prozessoren (7448) mit je 1,8 GHz bestückt werden. Auch für den AGP×2-Steckplatz gab es leistungsfähigere Grafikkarten, bis hin zur ATI 9800 Pro. Eine Aufrüstung auf 2 GB RAM, Serial-ATA-Festplatten und USB-2.0-PCI-Erweiterungskarten ist ebenso möglich.\n\nDer letzte Mac mit G4-Prozessor war der Mac mini, wie der Cube ein Kompaktcomputer. Im Gegensatz zum teuren Cube, der die gleiche Leistung wie die Tower-Modelle brachte, handelt es sich beim Mac mini um ein vergleichsweise preiswertes Einstiegsmodell. So war der PowerPC mini ausschließlich mit dem G4-Prozessor zu haben; die G5-CPUs blieben den teureren und größeren Apple-Computern vorbehalten. Gespart wurde auch an der Ausstattung, im Gegensatz zu allen anderen Macs lieferte Apple den mini ohne Tastatur und Maus aus. Den Mac mini G4 gab es zunächst mit 1,25 und 1,42 GHz Taktfrequenz. Die kleinere Variante hatten eine 40 GByte große Festplatte und ein \"CD-Combodrive\" (DVDs lesen, CDs lesen und schreiben). Die 1,42‑GHz-Version hatte einen CD/DVD-Brenner und eine 80 GByte große Festplatte, außerdem waren Bluetooth und Airport bereits eingebaut. Ein Kritikpunkt war, dass das Gerät nur zwei USB-Anschlüsse hatte. Der Mac mini mit Intel-CPU (seit 2006 produziert) verfügt daher über vier USB-Anschlüsse. Als positiv wird empfunden, dass der Mac mini sehr leise ist; der eingebaute Lüfter springt nur bei intensiver Nutzung des Rechners an.\n\nMac OS X Leopard (10.5, 2005) setzt einen CPU-Takt von mindestens 867 MHz voraus. Somit sind Installationen von Leopard auf langsameren Modellen nicht mehr vorgesehen, obwohl dies mit den passenden Mitteln relativ einfach möglich ist. Jedoch ist die Leistung dieser Version von Mac OS X, die zur Darstellung durchwegs Beschleunigungsfunktionen der Grafikkarte verwendet, auf älteren Power Macs, deren Grafikkarten diese Funktionen nicht bieten, spürbar eingeschränkt.\n\nAußer für die Hauptplatinen gibt es auch für die Rechner, um die verschiedenen Modelle auseinanderhalten zu können, Bezeichnungen, die zwar inoffiziell sind, aber von Apple stammen:\n\nTower-Gehäuse:\n\n\n\n\nKompaktgehäuse:\n\n\nBis auf den Mac mini wurden die Rechner immer als „Power Mac G4“ zusammen mit dem jeweiligen Modellnamen bezeichnet, also z. B. „Power Mac G4 Gigabit Ethernet“.\n\nDer Power Mac G5 führte das zuvor bei den PowerBooks eingeführte Aluminium als Gehäusematerial in der Power Macintosh-Reihe ein (wegen des auffälligen Lochgitter-Designs der Frontseite in Fachkreisen häufig auch scherzhaft als „Käsereibe“ bezeichnet). Er benutzte neue PowerPC-970-Prozessoren von IBM, von Apple als \"G5\" vermarktet. Der Power Mac G5 hatte ein ausgeklügeltes Belüftungssystem mit neun Ventilatoren in vier unabhängigen Kühlzonen. Die Prozessoren selbst wurden durch passive Kühler gekühlt. Ab Mitte 2004 wurde von Apple erstmals serienmäßig ein Flüssigkeitskühlsystem im Topmodell mit Dual-2,5 GHz-Prozessoren verbaut (seit April 2005 war nur das Dual-CPU-2,7 GHz-Topmodell wassergekühlt, seit Oktober 2005 nur der \"Quad\" mit zwei Doppelprozessoren). Wurde das Gehäuseseitenteil abgenommen, konnte durch eine Plexiglasabdeckung ein Blick ins Innere bei laufendem Betrieb geworfen werden.\n\nDas letzte Einprozessormodell (mit 1,8 GHz, von Apple „Late 2004“ genannt, mit PCI-Bus und 600 MHz-FSB) wurde im Oktober 2004 eingeführt. Die Architektur unterschied sich grundlegend von allen anderen G5-Power-Macs, da der Systemaufbau dem iMac G5 (Rev A) gleicht. Statt PMU besitzt dieses Modell eine SMU sowie U3lite- und Shasta-Controller. Bereits im Juni 2005 (es war allerdings bis Juli 2005 noch in Europa erhältlich) wurde das Gerät, das bis dahin nie zuverlässig funktionierte, vermutlich wegen Serienfehlern eingestellt.\n\nErst Anfang 2005 – also lange nach Bekanntwerden des Fehlers – hat Apple eine Firmware-Aktualisierung veröffentlicht, mit der die Rechner zuverlässig funktionieren.\n\nVon Oktober 2005 bis August 2006 waren drei Modelle (mit einer Dual-Core-2,0-GHz-CPU, mit einer Dual-Core-2,3-GHz-CPU sowie das teuerste Modell mit zwei Dual-Core-CPUs à 2,5 GHz, alle mit PCI-Express) erhältlich. Alle diese Modelle verwendeten die Chipkombination U4/K2 mit PCI-Express-Bridge. Viele Exemplare dieser Modellreihe – insbesondere diejenigen mit 2 × 2,3 GHz – haben einen Serienfehler im Netzteil; für Geräte mit bestimmten Seriennummern wurden bis Januar 2010 defekte Netzteile von Apple auch außerhalb der regulären Garantiezeit kostenlos getauscht.\n\nAlle \"Quad-Core\"- und \"Dual-Core\"-Power-Mac-G5-Modelle unterstützten zusätzlich bis zu vier Grafikkarten, die in die PCI-Express-Steckplätze eingebaut werden konnten und an denen jeweils bis zu zwei 23″-Apple-Cinema-Displays angeschlossen werden konnten.\n\nDer Power Mac G5 war der erste 64-Bit-Computer von Apple. Betriebssystemseitig unterstützte anfangs Mac OS X Tiger (10.4, 2005) nur 64-Bit-Kommandozeilenprogramme. Seit Mac OS X Leopard (10.5, 2007) werden 64-Bit-Programme vollständig unterstützt.\n\nAm 7. August 2006 wurden die letzten Macs mit PowerPC- durch Rechner mit Intel-Prozessoren ersetzt; Nachfolger des Power Mac G5 ist der Mac Pro.\n\nIm Wesentlichen wurden folgende Varianten des Power Mac G5 von Apple gebaut:\n\n"}
{"id": "153658", "url": "https://de.wikipedia.org/wiki?curid=153658", "title": "Back to Gaya", "text": "Back to Gaya\n\nBack to Gaya ist der erste aus Deutschland stammende komplett computeranimierte Kinofilm.\n\nRegisseure sind Holger Tappe und Lenard Krawinkel. Produziert wurde der Film vom niedersächsischen Studio Ambient Entertainment, das auch die Neuverfilmung von \"Urmel aus dem Eis\" (2006), dessen Fortsetzung \"Urmel voll in Fahrt\" (2008) sowie die Kästner-Verfilmung \"Konferenz der Tiere\" (2010) realisierte. Der Film wurde in wesentlichen Teilen mit der Software Maya erstellt und kam am 18. März 2004 in die deutschen Kinos.\n\nDas Lied zum Film heißt „Hooray“ und ist von Frameless, einer deutschen Rock-Pop-Band.\n\nDas Land \"Gaya\" ist der Handlungsort einer Trickfilmserie im Fernsehen. Die Haupthelden sind Zino, der Nationalheld von Gaya, der zwar viel Mut besitzt, sich jedoch öfter mal ungeschickt anstellt und auch nicht gerade der Hellste ist, und sein Freund Buu, ein genialer (jedoch etwas ängstlicher) Erfinder, der Zino den Weg zum Erfolg freimacht und dabei ungewollt fast immer ganz im Hintergrund bleibt. Diese beiden, Prinzessin Alanta, die Tochter des Bürgermeisters, und drei \"Schnurks\", eine Familie fieser Gayaner, namens Brampf (der gerne viel isst und ebenfalls nicht oft nachdenkt, sondern einfach tut, was man ihm sagt) und Zeck, der nicht ganz mit der Rolle des Bösen zurechtkommt, sich am Anfang des Films in Alanta verliebt und ihr im weiteren Verlauf der Geschichte immer wieder zur Seite steht, unter dem Anführer Galger (der hauptsächlich für die Pläne der Gruppe verantwortlich ist), werden durch einen unglücklichen Zufall von einem bösen Forscher namens \"Prof. N. Icely\" mit seiner selbstgebauten Maschine, die er \"Proton-Plasma-Transporter\" nennt, in die reale Welt der Menschen verfrachtet. Er ist nur an dem Edelstein, Dalamit genannt, interessiert, der Gaya mit unendlicher Energie versorgt und den sich die Schnurks kurz vor der Reise in unsere Welt ergattern, um ihn zu retten, dadurch jedoch nur mittransportiert werden (genau wie die anderen drei Hauptfiguren Zino, Buu und Alanta). Die winzigen Gayaner müssen sich nun also, auf drei Gruppen aufgeteilt, in der Menschenwelt zurechtfinden, Gaya retten und wenn es klappt zusammen wieder zurück in ihre Heimat kommen. Dazu suchen sie ihren Schöpfer \"Albert Drollinger\" auf, der ihnen eventuell helfen kann, wieder nach Gaya zu kommen.\n\n"}
{"id": "154721", "url": "https://de.wikipedia.org/wiki?curid=154721", "title": "Shellcode", "text": "Shellcode\n\nShellcode ist ein Begriff aus der Software-Programmierung und bezeichnet die in Opcodes umgewandelte Form von Assemblersprachenbefehlen, die einen oder mehrere bestimmte Befehle ausführen soll. In der Regel wird eine Shell gestartet, daher auch der Name. Shellcodes werden in Pufferüberlauf- und anderen Dateninjektions-Attacken benutzt.\n\nDie Umwandlung findet hierbei nur zu dem Zweck statt, die Anweisung so im Speicher zu platzieren (dies wird meist über einen sogenannten Pufferüberlauf erreicht), dass der Prozessor sie ausführt.\n\nZur Erzeugung von Shellcode kann der auszuführende Befehl in C geschrieben und mit einem Compiler übersetzt werden. Das erzeugte Programm wird nun disassembliert (rückübersetzt) und die Funktionsweise des Programms in Assemblersprache nachprogrammiert. Viele Instruktionen können aber weggelassen oder verkürzt werden. Bei vielen Sicherheitslücken darf im Shellcode kein 0-Byte enthalten sein, weil dieses in C das String-Ende markiert. Manchmal müssen noch weitere Filter umgangen werden, beispielsweise werden nur Buchstaben und Zahlen zugelassen oder die Groß- und Kleinschreibung verändert.\n\nAnstatt eigenen Code auszuführen, was nicht immer möglich ist (zum Beispiel bei OpenBSD oder unter Verwendung von Speicherschutz), kann man auch direkt zu gewünschten Funktionen springen, die beispielsweise im Programm selber oder einer geladenen Bibliothek, beispielsweise der libc vorhanden sind. Dieses Verfahren wird return into libc genannt.\n\nDer Assembler Code (x86-Architektur):\nvoid main() {\n__asm__(\"\njmp 0x2a # 3 bytes - springt direkt vor den String\npopl %esi # 1 byte - Adresse des Strings wird in esi geladen\nmovl %esi,0x8(%esi) # 3 bytes - die Adresse des Strings wird in den Speicher geschrieben\nmovb $0x0,0x7(%esi) # 4 bytes - der String wird nullterminiert\nmovl $0x0,0xc(%esi) # 7 bytes - ein nullpointer für das environment\nmovl $0xb,%eax # 5 bytes - syscall-nummer in eax\nmovl %esi,%ebx # 2 bytes - ebx enthält die adresse von \"/bin/sh\"\nleal 0x8(%esi),%ecx # 3 bytes - argumente, ein pointer auf den string und ein nullpointer\nleal 0xc(%esi),%edx # 3 bytes - environment\nint $0x80 # 2 bytes - interrupt wird ausgelöst\nmovl $0x1, %eax # 5 bytes - exit-interrupt\nmovl $0x0, %ebx # 5 bytes - wird vorbereitet\nint $0x80 # 2 bytes - interrupt wird ausgelöst\ncall -0x2f # 5 bytes - ein call zurück, dabei wird der eip auf den Stack gepusht\n.string \\\"/bin/sh\\\" # 8 bytes\nDer Opcode String:\n\nchar shellcode[] =\n\"\\xeb\\x2a\\x5e\\x89\\x76\\x08\\xc6\\x46\\x07\\x00\\xc7\\x46\\x0c\\x00\\x00\\x00\"\n\"\\x00\\xb8\\x0b\\x00\\x00\\x00\\x89\\xf3\\x8d\\x4e\\x08\\x8d\\x56\\x0c\\xcd\\x80\"\n\"\\xb8\\x01\\x00\\x00\\x00\\xbb\\x00\\x00\\x00\\x00\\xcd\\x80\\xe8\\xd1\\xff\\xff\"\n\"\\xff\\x2f\\x62\\x69\\x6e\\x2f\\x73\\x68\\x00\\x89\\xec\\x5d\\xc3\";\nDieser Code ist jedoch nicht sonderlich geschickt, da er Nullbytes enthält und recht lang ist. Zur Vermeidung von „unerwünschten Zeichen“ werden häufig auch Encoder verwendet, welche eine Maskierung und spätere Demaskierung dieser Zeichen ermöglichen und den Shellcode eventuell noch zusätzlich komprimieren. Es gibt auch noch andere Techniken, die Adresse des Strings herauszufinden, als einen „jmp“ oder „call“. Es ist beispielsweise möglich, lediglich codice_1 auf den Stack zu pushen. Danach enthält der \"esp\" die Adresse.\n\nHeap Overflow, return into libc, double free(), Exploit, Assembler\n\n\n"}
{"id": "155867", "url": "https://de.wikipedia.org/wiki?curid=155867", "title": "Alias-Effekt", "text": "Alias-Effekt\n\nAls Alias-Effekte [] (auch \"Aliasing-Effekte\" oder kurz \"Aliasing\") werden im Bereich der Signalanalyse Fehler bezeichnet, die auftreten, wenn im abzutastenden Signal Frequenzanteile vorkommen, die höher sind als die halbe Abtastfrequenz (Nyquist-Frequenz).\n\nAliasing kann einerseits durch die Nichtbeachtung des Abtasttheorems (zu geringe Abtastfrequenz) beim digitalen Abtasten von Signalen auftreten und andererseits, wenn das abzutastende Signal von einem Störsignal überlagert ist, das seinerseits Frequenzen enthält, die höher als die Nyquist-Frequenz sind.\nIn der Bildverarbeitung und Computergrafik treten Alias-Effekte bei der Abtastung von Bildern auf und führen zu Mustern, die im Originalbild nicht enthalten sind. In der Audiotechnik äußern sich Alias-Effekte als Störgeräusche. \nUm Aliasing zu verhindern, können Tiefpassfilter eingesetzt werden, die unerwünschte Frequenzanteile herausfiltern.\n\nIn der Signalverarbeitung treten Alias-Effekte beim Digitalisieren analoger Signale auf. \n\nDamit das Ursprungssignal korrekt wiederhergestellt werden kann, dürfen im abzutastenden Signal nur Frequenzanteile vorkommen, die kleiner als die Nyquist-Frequenz sind. Kommen allerdings Frequenzanteile vor, die höher als die Nyquist-Frequenz sind, so werden diese als niedrigere Frequenzen interpretiert. Die höheren Frequenzen geben sich sozusagen als eine andere (niedrigere) aus (siehe Grafik), daher die Bezeichnung \"Alias\".\n\nStörende Frequenzanteile, die zu Aliasing führen können, treten bei einer Unterabtastung auf (d. h. das Abtasttheorem wurde nicht eingehalten).\nAber selbst wenn das Abtasttheorem eingehalten wird, kann es auch zu Aliasing kommen, wenn das abzutastende Signal von einem Rauschsignal überlagert ist, das Frequenzanteile enthält, die höher als die Nyquist-Frequenz sind. \n\nZur Vermeidung solcher Aliasing-Effekte wird das Eingangssignal durch einen Tiefpass gefiltert (Anti-Aliasing-Filter). Die Filterwirkung dieses Abschneidens der hohen Frequenzen kann auch durch die Begriffe Höhensperre, Höhenfilter, High Cut und Treble Cut beschrieben werden. Diese Filterung muss vor der Digitalisierung geschehen – eine nachträgliche Korrektur von Alias-Effekten ist nicht mehr möglich.\n\nIn der Bildverarbeitung und Computergrafik treten Alias-Effekte bei der Abtastung von Bildern auf, ein Beispiel ist das Auftreten von Moiré-Mustern.\n\nDer Treppeneffekt, der bei der Rasterung geometrischer Figuren auftritt, wird oft als Aliasing bezeichnet, obwohl es sich bei ihm nicht um „echtes“ Aliasing im Sinne der Signalanalyse handelt.\n\nBei Kameras ab 3 Megapixeln werden Alias-Effekte meistens zuverlässig durch geschicktes Auslegen der Optik unterdrückt. Die optische Auflösung bleibt hier absichtlich unter der Pixelauflösung. Die Optik bildet also ein wenig unscharf ab und dient somit als Tiefpassfilter.\n\nAls Beispiel eines Originalbildes, das in seiner sogenannten Ortsfrequenz Signalanteile oberhalb der Nyquist-Frequenz hat, soll die Fresnel-Zonenplatte in der Abbildung dienen. Wird sie mit 30 × 30 Punkten abgetastet, so kann nur die Struktur in der Mitte wiedergegeben werden. In den Randbereichen übersteigt die Ortsfrequenz des Objekts die Nyquist-Frequenz, so dass hier das Objekt nicht wiedergegeben werden kann. Stattdessen entstehen Alias-Objekte in Form der Kreise in den Randbereichen.\n\nZu einer ähnlichen Demonstration (allerdings in einer Dimension) siehe Frequenzbesen.\n\nAlias-Signale treten auch beim Scannen von Bildvorlagen mit wechselnden Ortsfrequenzen auf, man spricht dann von einem Moiré-Effekt, zum Beispiel bei Kleidungsstücken wie Wollpullovern oder Jackets mit dünnen Streifen, oder bei Abbildungen von Ziegeldächern. Oft sind Moiré-Effekte auch im Fernsehbild zu sehen, wenn entsprechende Texturen abgebildet sind. Die Ursache liegt in einer Überlagerung der Spektren der Abtast-Funktion, deren Ausgangssignale mit f periodisch sind.\n\nIn Filmen können Alias-Effekte auftreten, welche auf die Zusammensetzung des Films aus Einzelbildern zurückzuführen sind. Als weithin bekanntes Beispiel sei das scheinbare Rückwärtslaufen der Wagenräder in Western genannt. Es tritt auf, sobald das Rad sich von Bild zu Bild mehr als um den halben Winkel zwischen zwei Speichen weiterdreht.\n\nBeobachtet man die Beschleunigung eines Wagens im Film, dreht sich das Rad zunächst in die richtige Richtung. Von einer bestimmten Geschwindigkeit an aber scheint sich das Rad rückwärts zu drehen, um mit weiter zunehmender Geschwindigkeit der Kutsche scheinbar wieder langsamer zu werden. Dann scheint es stehenzubleiben, um sich gleich danach mit unnatürlich niedriger Geschwindigkeit wieder in die richtige Richtung zu bewegen. Das scheinbare Vor- und Rückwärtslaufen wiederholt sich bei weiterer Beschleunigung.\n\nSignaltheoretisch betrachtet stellt das Aufnehmen der Einzelbilder einen \"Abtastvorgang\" dar. Die Abtastfrequenz entspricht der Bildwiederholfrequenz. Die Signalfrequenz entspricht der Frequenz, mit der die Speichen einen Winkel durchlaufen, der dem Abstand der Speichen entspricht. Bei einer Bildwiederholfrequenz von 24 Bildern pro Sekunde ist ab einer Drehgeschwindigkeit des Rades von 12 Speichenabständen pro Sekunde das Nyquistkriterium verletzt, so dass dann Aliasing auftritt.\n\n\n"}
{"id": "157023", "url": "https://de.wikipedia.org/wiki?curid=157023", "title": "Remote Access Service", "text": "Remote Access Service\n\nMit dem (kurz RAS; von engl. \"remote\", „entfernt, fern“, \"access\", „Zugriff“ und \"service\", „Dienst“) bietet Microsoft Windows NT Clients, die sich außerhalb eines geschützten lokalen Netzwerks befinden, die Möglichkeit, sich über eine Modem-, ISDN- oder X.25-Verbindung mit diesem zu verbinden. Dabei werden nicht nur unterschiedliche Clients unterstützt, sondern es besteht auch eine große Flexibilität in der Auswahl und Kombinationsmöglichkeit der verwendeten Netzwerkprotokolle.\n\nDie über RAS mit dem NT-Netzwerk verbundenen Clients können auf die gesamte Funktionalität des Netzwerks zurückgreifen, als wären sie direkt lokal mit dem Netzwerk verbunden und ermöglicht so z. B. den Einsatz von Telearbeit.\n\n"}
{"id": "158624", "url": "https://de.wikipedia.org/wiki?curid=158624", "title": "FileMaker", "text": "FileMaker\n\nFileMaker ist ein proprietäres Datenbanksystem zur Verwaltung von Daten in Datenbanken und zur Entwicklung von Datenbankanwendungen, das unter den Betriebssystemen macOS, Apple iOS und Windows läuft. FileMaker ist ebenfalls der Name des Unternehmens, welches das Datenbanksystem vertreibt.\n\nEine FileMaker-Datei enthält sowohl die Daten als auch die Informationen über die Struktur der Daten. In der Struktur enthalten sind die Tabellen (Relationen) mit den zugehörigen Feldern und Felddefinitionen, die Formeln und Funktionen der Felder, die Beziehungen der Tabellen zueinander, die Layouts, die Scripte sowie die Zugriffskonten mit Passwörtern und Zugriffsrechten.\n\nIm Umfeld des Database Publishings zum Beispiel zum Erstellen von datenbankgestützten Katalogen wird FileMaker zum Sammeln und Konfektionieren von Daten verwendet. Die Datenübergabe an Layoutprogramme erfolgt über CSV- oder XML-Export. Auch über ODBC, SQL, den Zugriff von Adobe-InDesign-Plug-ins und QuarkXPress-XTensions kann zugegriffen werden.\n\nMit Instant WEB Publishing lässt sich ohne zusätzliche Investition per Webbrowser auf freigegebene FileMaker-Datenbanken zugreifen. Dies ermöglicht die Nutzung von FileMaker-Dateien im lokalen Netz oder im Internet, ohne dass zusätzliche Client-Lizenzen installiert werden müssten.\n\nDie Funktionalität einer FileMaker-Datenbank kann durch externe Plugins von Drittherstellern erweitert werden. Um ein Plugin im FileMaker-Client zu installieren, genügt es in der Regel, das Plugin in das Verzeichnis „Erweiterungen“ von FileMaker zu verschieben. Plugins sind jeweils für die Windows- oder Macintosh-Plattform separat erhältlich. Ab FileMaker 7 wurde die Plugin-Schnittstelle von FileMaker überarbeitet.\n\nEin Datenfeld in FileMaker kann maximal bis zu 2 GB (Textfeld) bzw. 4 GB (Medienfeld) Daten aufnehmen. Eine FileMaker-Datei kann bis zu 8 Terabyte an Daten speichern.\n\nIm Gegensatz zu anderen Datenbanksystemen benötigt FileMaker keine Datenbindung an separate Programmiersprachen und Benutzeroberflächen, sondern alle Elemente einer Anwendung werden direkt in derselben Laufzeitumgebung erstellt.\n\nEine Datenbank-Anwendung in FileMaker besteht aus einer oder mehreren Datenbank-Dateien. Diese enthalten neben dem Schema und den Daten auch die Benutzeroberfläche (GUI) und Script-Objekte, welche in der Programmiersprache FileMaker Script formuliert werden. Da es sich um eine Scriptsprache handelt, die zur Laufzeit des Systems interpretiert wird, lassen sich Anwendungen im laufenden Betrieb anpassen und weiter entwickeln. Dies ist eine der besonderen Stärken von FileMaker, weil sich so schnell und direkt umfangreiche Anwendungen anhand der Benutzer-Bedürfnisse weiter entwickeln lassen. FileMaker wird daher oft auch als RAP (Rapid Development Tool) bezeichnet.\nDer enthaltene Layout-Editor ermöglicht die Ausgestaltung von Benutzeroberflächen und Formularen für die Druckausgabe direkt innerhalb der Anwendung. Die Benutzeroberfläche einer FileMaker Anwendung wird plattformübergreifend identisch dargestellt. Somit lassen sich Anwendungen in gemischten Betriebssystem-Umgebungen einfach realisieren.\n\nObwohl FileMaker ursprünglich als Workgroup-Datenbank für den Office Bereich konzipiert wurde, wird die FileMaker-Plattform ausgehend von den flexiblen Möglichkeiten oft auch zur Entwicklung von professionellen Branchenlösungen verwendet.\n\nDie Wurzeln von FileMaker gehen auf ein unter MS-DOS laufendes Produkt mit Namen \"Nutshell\" zurück, das das Unternehmen Nashoba Systems aus Concord, Massachusetts in den Jahren 1982/1983 entwickelt hatte. Vertrieben wurde das Produkt \"Nutshell\" vom Unternehmen \"Leading Edge\", das in dieser Zeit begonnen hatte, PC-kompatible Hardware und Software zu vertreiben.\n\nMit der Einführung des Apple Macintosh 1984 und dessen graphischer Benutzeroberfläche führte Nashoba 1985 das Datenbank-Konzept von Nutshell mit der menü- und fenstergesteuerten Oberfläche auf dem Apple Macintosh zusammen. Es entstand eine wesentlich leichter und intuitiver zu bedienende Datenbank.\n\nDa \"Leading Edge\" nicht in den damals entstehenden Apple-Macintosh-Softwaremarkt einsteigen wollte, suchte Nashoba einen neuen Distributor und einen neuen Namen. Als Distributor fand Nashoba das Unternehmen \"Forethought\", das bereits mit dem eigenen Präsentationsprogramm „Presenter“ (später umbenannt in PowerPoint) im Macintosh-Markt aktiv war.\n\nAls neuen Namen wählte Nashoba \"FileMaker\". Der Name ist eine Kombination aus dem Namen des Microsoft-Datenbankprogramms \"MS-File\" für Apple Macintosh und des gerade auf den Markt gekommenen DTP-Programms \"PageMaker\" des Unternehmens Aldus. In Anlehnung an das neue Apple-Macintosh-Modell \"Macintosh Plus\" folgte 1986 die Version \"FileMaker Plus\". Im August 1987 übernahm Microsoft das Unternehmen Forethought und dessen Produkt PowerPoint. Die mitgekauften Distributionsrechte (inklusive Dokumentation und Verpackung) an FileMaker wollte Microsoft nutzen, um FileMaker unter eigenem Namen zu vertreiben. Nashoba als Inhaber am Softwarecode entschied sich aber, FileMaker nun unter eigenem Namen zur Version \"FileMaker 4\" (Juni 1988) weiterzuentwickeln und auch selbst zu vertreiben.\n\nKurz darauf, im August 1988, kaufte Claris (eine Apple-Tochter) Nashoba und brachte das Programm als \"Claris FileMaker II\" auf den Markt.\n\nMit der Vorstellung von Claris FileMaker Pro im Oktober 1990 sprang die Versionsnummer auf 1.0 zurück. FileMaker war zu dem Zeitpunkt das erste Datenbankprogramm, mit dem es möglich war, im laufenden Betrieb die Datenbankstruktur zu ändern, neue Datenfelder zu definieren und bereits existierende zu löschen.\n\nAb Version FileMaker Pro 2 (1992) war FileMaker neben dem Betriebssystem Mac OS auch für das Betriebssystem Windows in einer identischen Version verfügbar.\n\nAb der Version 3.0 (1996) ist FileMaker ein relationales Datenbanksystem, in dem mehrere FileMaker-Dateien (Tabellen) miteinander relational verknüpft sein können.\n\nMit der Version 4.0 (1997) verfügte FileMaker über ODBC-Import und rudimentäre SQL-Fähigkeiten sowie über eine eigene Plug-in-Architektur. Seit 1998 wird das Programm von dem eigens gegründeten, aus Claris hervorgegangenem Unternehmen FileMaker, Inc. hergestellt und vertrieben.\n\n1999 erschien die Version 5, im Mai 2001 die Version 5.5, die erstmals (außer unter Mac OS 9 und Windows) auch nativ unter dem Apple-Betriebssystem Mac OS X lief.\n\nIm September 2002 erschien die Version FileMaker 6. Darin unterstützte FileMaker erstmals Im- und Export von XML-Daten. Bestehende Suchergebnisse waren erstmals erweiterbar beziehungsweise einschränkbar.\n\nDie 2004 erschienene Version 7 wurde neu programmiert, das gesamte Datenmodell revidiert und die Funktionalität erweitert. Version 7 unterstützte mehrere Tabellen pro Datei und Unicode. Ergänzend bzw. alternativ zu \"Beziehungen\" zwischen \"Dateien\" (ab Version 3.0-6) traten nun zusätzlich vielschichtige \"Relationen\" zwischen \"Tabellen\" innerhalb einer Datei. Stark erweitert wurde die maximale Dateigröße von 2 Gigabyte auf 8 Terabyte sowie der maximale Dateninhalt einzelner Tabellenfelder von 64 Kilobyte auf 2 Gigabyte. Bis Version 7 wurden Beziehungen (Relationen) nur tabellarisch dargestellt, seither nur graphisch in einem Beziehungsdiagramm (nach Art eines Entity-Relationship-Modells).\n\nMit Version 8 (2005) kam die Möglichkeit hinzu, eigene Funktionen definieren zu können, die rekursive Aufrufe unterstützen. 2006 erschien die Version FileMaker Pro 8.5, die als neues Layoutobjekt über sogenannte \"Webviewer\"-Elemente HTML-Inhalte von Webseiten in FileMaker-Datenbanken direkt einbinden kann.\n\nDie Version 9 brachte die von der Eingabe abhängige Formatierung für Felder und Feldinhalte. In Bezug auf Anbindung an externe Datenbanken ist mit der Version 9 eine Echtzeitverbindung zu externen SQL-Datenquellen (zum Beispiel Microsoft SQL Server, Oracle, MySQL) möglich. Mehrere Scriptfenster können gleichzeitig geöffnet werden; Scripte in Gruppen und Untergruppen gruppiert werden. Eine bessere Anpassung an Bildschirmfenster bieten skalierbare Elemente.\n\nDie im Januar 2009 erschienene Version 10 von FileMaker brachte Neuerungen in den Bereichen Benutzeroberfläche, beim Suchen, bei den Berichten, bei Funktionen und neue Import- und Export-Formate. Der eingebaute E-Mail-Versand über SMTP erlaubt das direkte Versenden von E-Mails aus der Datenbank. Als neue Import- und Exportformate werden die Excel-12-Formate nativ unterstützt. Für Mac-Nutzer ist auch der direkte Import von Datenbankdateien aus Bento 2 möglich.\n\nFileMaker 11 (März 2010) ermöglicht die visuelle Präsentation gespeicherter Daten in Diagrammform sowie eine feldübergreifende Schnellsuche.\n\nSeit April 2012 mit Erscheinen der Version 12 arbeitet FileMaker mit einer neuen Layout-Design-Architektur (intern auf Cascading Style Sheets basierend), die FileMaker-Layouts mit einem Befehl vollständig in ein anderes Design umwandeln kann. FileMaker liefert dazu 40 Design-Themen, die auch spezielle \"Touch-Designs\" für iOS-Geräte enthalten. Die Funktion „Schnelldiagramme“ ermöglicht dem Anwender auf Knopfdruck flexible Diagramme zu erstellen. Containerfelder (Medienfelder) unterstützen nun Drag & Drop zum Speichern von Dokumenten, Fotos, Videos, Musik oder anderen Dateien direkt in einer FileMaker-Datei. Derart verwaltete Mediendateien (z. B. PDF-Dateien) können innerhalb des Containerfelds betrachtet bzw. abgespielt werden. Die Neuerungen erfordern ein neues Dateiformat (.fmp12). Bestehende FileMaker-fp7-Dateien können mit FileMaker 12 in das neue Dateiformat konvertiert und uneingeschränkt weiter benutzt werden.\nErstmals können mit der Version 12 nicht nur externe SQL-Datenquellen von FileMaker mit SQL-Befehlen angesprochen werden, sondern auch die eigenen Datenbanken.\n\nFileMaker 13 (Dezember 2013) erweitert die in FileMaker 12 hinzugekommene Layout-Design-Architektur um zusätzliche Elemente. Designvorlagen lassen sich nun individuell anpassen und können bei Bedarf weitergegeben werden. Das Erscheinungsbild einzelner Steuerelemente wie Tasten und Eingabefelder kann der Anwender über editierbare Stile besser einheitlich gestalten. Eine Änderung der Stilvorlage passt alle damit verbundenen Elemente automatisch an. Eine neue Feldauswahlpalette erleichtert das Erstellen von Datenmaskenlayouts. Für iOS-Geräte gibt es nun Wischgesten, Popover-Funktionen und einen eingebauten Barcodeinterpreter für alle gängigen Arten von Strichcodes und QR-Codes. Komplett neu entwickelt wurde die von FileMaker WebDirect genannte Kerntechnologie, die HTML5 basiert ist. Damit ist es möglich, FileMaker-Datenbanken ohne installierte Software direkt im Browser des Clients auszuführen, wenn auf einen FileMaker-Server zugegriffen wird. Die Version FileMaker 13 läuft ab Windows 7, OS X Lion (bzw. Mountain Lion für den Server) sowie iOS 6 und 7.\n\nFileMaker 14 (Mai 2015) bietet als Neuerungen einen neuen Script-Arbeitsbereich, neue Designwerkzeuge, Verbesserungen bei der HTML5-basierten Kerntechnologie WebDirect sowie neue Funktionen für iOS und eine neue Startzentrale. WebDirect unterstützt nun mehr Internet-Browser (Safari 8, Internet Explorer 11, Mobile Safari 8 und Chrome 38 auf Android). FileMaker Server bietet ab sofort die Möglichkeit, ohne zusätzliche Lizenzkosten einen Standby-Server einzurichten, der den Produktionsserver automatisch spiegelt und bei etwaigen Ausfällen automatisch zur Verfügung steht. Die Version FileMaker 14 läuft ab Windows 7, OS X Mavericks 10.9/ Yosemite 10.10 sowie iOS 8.1. FileMaker Server 14 erfordert Windows Server 2012 R2 Standard Edition mit Update, OS X Mavericks 10.9 oder OS X Yosemite 10.10.\n\nFileMaker 15 (Mai 2016) enthält einige Funktionserweiterungen für mobile Geräte wie die Touch ID- und 3D-Touch-Unterstützung auf iOS-Geräten sowie die Unterstützung von iBeacon. WebDirect-Anwendungen können nun auch auf iOS- oder Android-Mobiltelefonen ausgeführt werden. Im Script-Workspace werden fehlerhafte Scriptzeilen rot markiert, und Änderungen an Scriptbefehlen können mit der neuen Undo-Funktion bis zum Speichern beliebig oft widerrufen werden. Mit dem neuen ESS-Adapter können Verbindungen zu weiteren SQL-Datenquellen wie PostgreSQL oder IBM DB2 hergestellt werden. FileMaker 15 bietet ein neues maskiertes Bearbeitungsfeld für die Eingabe von Passwörtern oder anderen vertraulichen Daten. Die aktuelle Version unterstützt SSL-Zertifikate von neun häufig genutzten Anbietern sowie Intermediate-, subjectAltName- (SAN-) und Wildcard-Zertifikate. Mit proaktiven Sicherheitswarnungen können Nutzer gewarnt werden, wenn eine Verbindung mit einem ungültigen Sicherheitszertifikat aufgebaut werden soll. Mit der neuen Statistikprotokollierung können Verlangsamungen innerhalb der FileMaker-Plattform ermittelt werden. In Ausschnitten zeigt eine neue Statusleiste an, wenn eine Datenfilterung und -sortierung in einem Portal abgeschlossen wurde. Dieser Vorgang wird in FileMaker 15 außerdem parallel durchgeführt und ermöglicht die Weiternutzung der Lösung währenddessen. Mit FileMaker 15 führt FileMaker eine neue Lizenzierung für Teams ab fünf Benutzern ein. FileMaker 15 lässt keine Verbindungen mehr zu Datenbanken auf einem FileMaker 12 Server zu. Die Version FileMaker 15 läuft ab Windows 7, OS X Yosemite 10.10 / El Capitan 10.11 sowie iOS 9.3. FileMaker Server 15 erfordert Windows Server 2012 R2 Standard Edition mit Update, OS X Yosemite 10.10 oder OS X El Capitan 10.11.\n\nIn der FileMaker Pro 16 Desktop Anwendung ist vor allem das an die gängige Windows Oberfläche angelehnte FileMaker-Interface zu nennen. FileMaker 16 Datenbanken werden wie unter macOS in einem Fenster dargestellt; das bisher umgebende Windows-Hauptfenster von FileMaker Pro entfällt. Dies erlaubt auch unter Windows sehr gut mit zwei Bildschirmen zu arbeiten.\n\nMit Karteikarten steht in FileMaker 16 ein neues Layoutelement zur Verfügung, mit dem sogenannte „überlagernde Fenster“ dargestellt werden können. Damit kann ein Benutzer durch beliebige Abläufe geleitet werden, da die Karteikarten so programmiert werden können, dass der Benutzer den programmierten Abläufen folgen muss. Dies ermöglicht eine App-ähnliche Handhabung von Datenbanken. Unterstützt wird dies durch den Wegfall der bisherigen Funktionsleiste im Fuß aller FileMaker-Fenster.\n\nDie FileMaker Pro Advanced 16 Version stellt im Layoutmodus ein neues Layoutobjekt-Fenster zur Verfügung, in dem alle Objekte des aufgerufenen Layouts auf übersichtliche Weise dargestellt werden. Die Objekte können ausgewählt, versteckt und umbenannt, die Stapelreihenfolge kann verändert werden. Dies dient einer wesentlich besseren Übersichtlichkeit und Kontrollierbarkeit vor allem bei komplexen Layout mit Überlagerungen und nur bei bestimmten Zuständen sichtbaren Layoutobjekten.\n\nFileMaker 16 Go unterstützt neue scriptgesteuerte Animationen und Übergänge, um beispielsweise von einer Detailansicht in eine Listenansicht zu wechseln. Damit kann man FileMakeranwendungen für iOS genau nach den empfohlenen Richtlinien für iOS-Benutzeroberflächen entwickeln.\n\nDie Filemaker Server 16 Version verfügt nun über eine PDF-Unterstützung – erlaubt damit Druck von PDF-Rechnungen, Etiketten oder Aufklebern aus WebDirect. Scripts auf FileMaker Server sichern Informationen als PDF-Dateien.\n\nNicht mehr verfügbar ist „FileMaker Pro“, sondern nur noch die bisher als Entwicklerversion vermarktete Version FileMaker Pro Advanced. Die Entwickler-Funktionen können innerhalb der App ausgestellt werden. Die FileMaker 17 Plattform besteht dem nach aus folgenden Produkten: FileMaker Pro 17 Advanced (Einzelplatz und Kleine Arbeitsgruppen), FileMaker Server 17 (Serverlösung für kleine bis mittlerer Arbeitsgruppen), FileMaker 17 Cloud (cloudbasierte Serverlösung für einzelne sowie kleine bis mittlere Arbeitsgruppen), FileMaker Go 17 für iPad und iPhone.\n\nBezüglich der Neuerungen erwähnenwert sind folgende Verbesserungen:\n\n- Der neu gestaltete Layout-Modus hilft bei der Entwicklung eigener FileMaker-Anwendungen: Die Layout-Werkzeugregister „Inspektor“, „Objekte“ und „Felder“ sind nun in das Dokumentfenster einbindbar oder bei Bedarf in Paletten auszugliedern.\n- Eine neue Layout-Funktion erlaubt die gleichzeitige Darstellung einer Datensatzliste und die Detaildarstellung eines Datensatzes in einem Layout ohne Erstellung von Selbst-Beziehungen und zusätzlichen Scripts.\n- Einzelne Objekte innerhalb einer Gruppe lassen sich auswählen und ändern, ohne vorher die Gruppierung der Objekte aufzuheben.\n- Beim Erstellen einer Tabelle werden automatisch eine Reihe von Feldern erzeugt, die für jede Tabelle wichtig sind (eindeutige Datensatznummer, Erstellungs- und Änderungdatum eines Datensatzes etc.).\n- Mehrere E-Mail-Anhänge lassen sich an eine E-Mail-Nachricht anfügen (bisher nur eine).\n- Selbsterstellte angepasste Menüs, Menüsets und Menüeinträge können per „Kopieren und Einsetzen“ in eine andere Datei eingefügt werden.\n\nIm mobilen Bereich garantiert eine neue Formelfunktion die Integration von Sensoren, um so unter iOS Sensoren-Daten von im iPhone oder iPad erfassen zu können.\n\nFileMaker wird in verschiedenen Versionen mit mehr oder weniger identischem Funktionsumfang für die Betriebssysteme Mac OS X, iOS und Windows angeboten:\n\nFileMaker Go\n\n\n\n\n\n\n\n\n\nFileMaker Mobile\n\n\nFileMaker Pro\n\nMit FileMaker 17 wird die Pro-Version nicht mehr angeboten, nur noch die Version Pro Advanced\n\nFileMaker Server\n\n\nFileMaker ist als Trial-Version zum Ausprobieren erhältlich.\n\nBis zum Jahr 2007 veranstaltete FileMaker Deutschland eine jährliche Entwicklerkonferenz, zuletzt von 6. bis 7. Dezember 2007 auf Gut Ising am Chiemsee in Oberbayern. Seit 2010 wird die Entwicklerkonferenz vom eigens dafür gegründeten \"Verein FM Konferenz\" (FMK) organisiert. Die erste von der FMK organisierte Konferenz fand 2010 in Zürich statt, seitdem gab es jeden Oktober eine Konferenz in Deutschland, Österreich oder in der Schweiz. Die Konferenz 2016 fand vom 13. bis 15. Oktober in Salzburg statt.\n\nSeit 2013 findet in Berlin eine international ausgerichtete FileMaker Unconference statt; im ersten Jahr unter dem Namen „pause[x]Berlin“, von 2014 bis 2016 unter der Bezeichnung \"dotfmp[x]Berlin\" und ab 2017 unter der Bezeichnung \"dotfmp.berlin\". Vom 2. bis 4. Juni 2014 fand die dotfmp[x]Berlin 2014 im Hotel „nhow Berlin“ im Berliner Bezirk Friedrichshain mit insgesamt knapp 80 deutschsprachigen und internationalen FileMaker-Entwicklern statt. Für das Jahr 2017 wurde der Termin für die dotfmp.berlin auf den 1. bis 3. Juni festgelegt.\n\nDie dotfmp.berlin gilt mittlerweile als die führende Veranstaltung für professionelle FileMaker-Entwickler in Europa und findet jeweils am ersten vollen Wochenende im Juni statt.\n\n\n\n"}
{"id": "158767", "url": "https://de.wikipedia.org/wiki?curid=158767", "title": "Chaosradio", "text": "Chaosradio\n\nChaosradio ist eine monatliche Live-Talk-Radiosendung des Chaos Computer Club (CCC) aus Berlin über wechselnde Themen rund um Technologie und Gesellschaft.\n\nChaosradio sendet in der Regel am jeweils letzten Montag des Monats, wird aber jeden Dezember aufgrund des \"Chaos Communication Congress\" ausgesetzt. Seit November 1995 ist die Sendung Bestandteil der Talkradioshow Blue Moon – es ist damit eines der ältesten Tech-Radios überhaupt. Die Sendung wird am 28. Januar 2019 aufgrund Programmumstellungen das letzte Mal auf Radio Fritz ausgestrahlt und bis auf Weiteres pausiert.\n\nDie Themen der Sendung sind technischer (zum Beispiel Das Semantische Web) oder gesellschaftspolitischer Natur (zum Beispiel Videoüberwachung) oder beziehen sich auf aktuelle Entwicklungen, die gerade Grundlage einer öffentlichen Diskussion sind. Sowohl die Moderatoren als auch der Inhalt wechseln. Zusätzlich zu dem vom CCC gestellten, variierenden Team moderierten regelmäßig zunächst Johnny Haeusler, später dann Max von Malotki, Holger Klein, Jakob Kranz und Marcus Richter die Sendung.\n\nDie Sendungsinhalte sind oft technisch anspruchsvoll, doch bemühen sich der Moderator und seine Studiogäste um eine möglichst verständliche Vermittlung komplexer Inhalte, um die Hörer an die Technik heranzuführen, die sie heute zunehmend beeinflusst und prägt. Chaosradio versucht, Medienkompetenz zu vermitteln. Tim Pritlove vom CCC formuliert es so: \"„Chaosradio ist immer so ein bisschen die Sicht von hinten; der Versuch, Themen aufzugreifen, die nicht üblicherweise im Radio oder auch sonst in den Medien sind.“\"\n\nChaosradio ist wie jeder andere Blue Moon auch eine Talksendung. Die Hörer können mitdiskutieren und Fragen zum Thema stellen.\n\nBis zur Ausgabe 196 wurde als Titelmelodie der Song \"Nummern\" von Kraftwerk gespielt.\n\nDas Chaosradio ist auf allen Fritz-Frequenzen empfangbar. Darüber hinaus wird die Sendung im Internet per Livestream, auch vom Chaos Computer Club, angeboten. Fast alle gesendeten Folgen sind ferner zum Herunterladen und als Podcast verfügbar.\n\nSeit Mitte 2014 wird das Format nur noch in ungeraden Monaten im Rahmen des „Blue Moon“ bei Radio Fritz des RBB und sonst vom CCC selbst per Livestream gesendet.\n\nInzwischen gibt es auch Ableger in anderen Frequenzbereichen, so bieten Radio Darmstadt und der Ulmer Radiosender freeFM Sendeplatz für solche Sendungen.\n\nÜber das \"Chaosradio Podcast Network\" werden mehrere Podcasts angeboten. Das sind neben dem \"Chaosradio\" noch \"Chaosradio Express International\", \"Chaos TV\", \"Dossier Chaotique\", \"Die Datenschleuder\" und \"25C3 Video Recordings\". Von November 2005 bis Dezember 2011 wurde auch der Podcast \"CRE\" unter dem Namen \"„Chaosradio Express“\" über das Chaosradio Podcast Network angeboten, dann aber auf eine eigene Seite überführt, um den Unterschieden zum Chaosradio Rechnung zu tragen.\n\n\n"}
{"id": "161078", "url": "https://de.wikipedia.org/wiki?curid=161078", "title": "Risc PC", "text": "Risc PC\n\nDer Risc PC ist ein auf RISC-Technik basierender Computer der Firma Acorn. Wie sein Vorgänger, der Acorn Archimedes, besitzt er eine ARM-CPU, als Betriebssystem wird RISC OS in Version 3.5 oder neuer verwendet. Der erste Risc PC wurde 1994 mit einem ARM610-Prozessor vorgestellt, der mit 30 MHz getaktet war; ein Jahr später erschien ein Modell mit ARM710 (40 MHz). 1996 war der deutlich schnellere StrongARM-Prozessor (bis zu 287 MHz) für den Risc PC verfügbar.\n\nEine Basisversion des Betriebssystems ist in austauschbaren ROM-Bausteinen gespeichert, so dass der Rechner auch ohne Festplatte gestartet werden kann. Als Arbeitsspeicher wurden die weitverbreiteten EDO-SIMMs verwendet, die sich der Hauptprozessor anders als beim Archimedes nicht mehr mit der Grafikkarte teilen musste, jedoch nach wie vor konnte wenn kein Grafik-RAM-Modul eingesteckt ist. Der Hauptspeicher konnte maximal auf 256 MiB aufgerüstet werden.\nDas System besitzt zwei Slots für Prozessor-Steckkarten, in einer davon befindet sich der Hauptprozessor des Systems. In den zweiten Steckplatz kann ein weiterer Prozessor eingebaut werden, was es ermöglicht, problemlos auf neuere Prozessoren aufzurüsten und z. B. als Zweit-CPU einen 486 einzusetzen, um diesen als „eingebetteten PC“ zu betreiben. Dadurch ist es möglich, DOS oder Windows unter RISC OS auszuführen.\nDas Gehäuse ist nahezu schraubenfrei, modular aufgebaut und sehr leicht zu öffnen.\n\nDiese Daten sind gleich für alle Modelle.\n\nDer Risc PC erschien in seiner ersten Ausführung zunächst unter diesem Namen. Die Namenserweiterungen durch nachgestellte Zahlen (600,700) wurden erst nach Erscheinen der Prozessorkarte mit dem ARM710 eingeführt.\n\nDer Risc PC wurde original mit einer ARM610 Prozessorkarte und 4 MB RAM sowie einer 210 MB Festplatte ausgeliefert. Das Mainboard hatte zu der Zeit nur 8Bit Sound und das VideoRAM war zunächst nur bis 1 MB installierbar, da die 2 MB Riegel anfangs nicht lieferbar waren. Diese Machine wurde in verschiedenen Kombinationen mit Acorns eigenen Monitoren (AKF60,AKF85), größerer Festplatte (420 MB), einem CD-ROM, dem 1 MB VideoRAM Modul, Erweiterung auf 8 MB Arbeitsspeicher angeboten.\nDie direkte Vorgängermachine, der A5000, konnte durch einfache Erweiterung per 2MB-Steckmodul 4 MB RAM enthalten oder hatte dieses als Maximalausbau direkt auf dem Board aufgelötet; durch aufwendigere Umbauten (Nachrüstung jeweils eines MEMC Memory Controllers je weitere 4 MB und Einbau eines Sockels für den originalen MEMC) waren 8 MB oder 12 MB RAM möglich. Daher sind die 4 MB RAM Grundausstattung des RiscPC zu der Zeit recht großzügig gewesen.\nDer Einstandspreis des Risc PC in Deutschland lag für das kleine Modell (ARM610,4MB,210HDD) bei exakt 2999,- DM.\n\nWichtige einzeln von Acorn erhältliche Hardwareerweiterungen waren ein steckbares SoundModul mit 16Bit Soundausgabe (d. h. CD Qualität), das VideoRAM Upgrade, die PC-Kompatibilitätskarte.\nLetztere gehörte wohl zum Gesamtverständnis der Maschine aus Acorns Sicht unabdingbar dazu – das gesamte Gerät ist ja im Hinblick auf diese Möglichkeit konstruiert (OpenBus). Sie erschien zunächst in Form einer mit einem 486SX von TexasInstruments versehenen Version. Diese erlaubte durchaus das Betreiben von DR-DOS aus dem Lieferumfang, aber auch von MS-DOS, Windows 3.1, später Windows 95. Allerdings, natürlich, bei eingeschränkter Spieletauglichkeit.\n\nAufgrund seines sehr modularen Aufbaus, kann der Risc PC schrittweise ausgebaut werden. Genau dies erfolgte auch durch die Herstellerfirma selbst, indem sie im Laufe der Zeit weitere Prozessorkarten, RAM-Module, Netzwerkkarten, PC-Karten anbot und bestimmte Zusammenstellungen als Komplettgeräte unter offizieller eigener Gerätenummer (ACBxx) anbot.\nDaher ergeben sich auch bestimmte „Meilensteine“ der Entwicklung mit besonderer Bezeichnung, wie in obiger Tabelle.\n\nWährend der Herstellungszeit wurde das Mainboard zweimal leicht überarbeitet, blieb aber von den Haupteigenschaften her im Wesentlichen unverändert. Die dritte Version enthält dann bereits 16Bit Sound on-board und eine an europäische Normen angepasste Videoeinheit (was in diesem Fall keine Verbesserung des Bildes darstellte).\nPositiv ergibt sich daraus auch, dass man für den Betrieb etwa einer StrongARM Karte nicht auf ein passendes neueres Board angewiesen ist. Stattdessen lassen sich auch Geräte der Erst- oder Zweitgeneration auf den Stand eines offiziellen Vollausbaues bringen. Oft wird zusätzlich noch ein ROM Austausch nötig, da das OS den neuen Prozessor auch unterstützen muss.\n\nVon Acorn selbst war ein Upgrade Pfad vom ARM600 über den ARM700 (1995) hin zu einem angekündigten ARM800 vorgesehen. Dazu gab es schon zur Risc PC Einführung fixe garantierte Upgradepreise, um so eine geplante Aufrüstung zu ermöglichen. Alle diese Chips waren mit Taktraten in normaler Chipevolution geplant, beginnend bei den 30 MHz des ARM610 im Risc PC (Vergleich mit A5000 25Mhz bzw. 33 MHz und A540 mit 26 MHz, alle mit ARM3). Die ARM710 Prozessorkarte des Risc PC 700 wurde mit 40 MHz betrieben und war ca. 25 % schneller als der ARM610.\n1996 erschien eine CPU, die von der Digital Equipment Corporation (DEC) mit und für ARM entwickelt worden war. Der StrongARM. In diesem war viel von digital's Wissen um die Herstellung hochgetakteter schnellster RISC Prozessoren wie dem Alpha-Prozessor eingeflossen. Er veränderte so u. a. auch den bisherigen einfachen Cache-Aufbau der ARM Prozessoren hin zu einer Trennung des Prozessorcache in gesonderte Bereiche für Daten und Instruktionen nach der Harvard-Architektur und verlängerte die Befehlspipeline von drei auf fünf Stufen. Dies zog bei der Nutzung dieses Chips im Risc PC einige Änderungen bei Betriebssystem (RISC OS 3.7 benötigt) und vielen Programmen nach sich. Im Gegenzug war so Ende 1996 mit der StrongARM-Karte eine Prozessorkarte verfügbar, die bei 202 MHz Taktrate wieder in der oberen Liga der im Mikrocomputerbereich erreichbaren Rechenleistung angesiedelt war.\n\nDer zweite Prozessorsteckplatz ist eine in dieser Form sonst kaum anzutreffende Hardwarelösung, da es hier einem Zweitprozessor komplett anderer Bauart (ARM vs.x86) im echten Parallelbetrieb erlaubt wird, den kompletten Systembus inklusive aller Geräte (RAM, Grafik, I/O) anzusteuern, wenn auch unter der Voraussetzung, dass der eigentliche Busmaster immer ein ARM Prozessor sein muss, der daher auch im ersten Slot einzustecken ist. Die Anbindung des Zweitprozessors ans System erfolgt über ein besonderes IC, das auf allen PC-Karten zu finden ist und eine Umsetzung an das Busprotokoll des Risc PC vornimmt.\nEs ist in dem Sinn auch kein Dual- bzw. Mehr-Prozessor System der Art, wie sie von anderen Herstellern bekannt sind, etwa SUN SPARCstation 10, SUN Ultra 2, SGI Octane oder das Abit BP6 Mainboard für Celerons.\n\nEs gab aber auch seitens Acorn Bemühungen den Risc PC im Dual-Prozessor Modus zu betreiben. Ausdruck dafür ist die Existenz einer Prozessorkarte mit einem weiteren ARM610, die Acorn „Duet“-Karte.\n\nVon der englischen Firma SIMTEC Electronics wurde dieser Ansatz ebenfalls verfolgt und nahm in Form eines ganzen Steckkartensystemes namens Hydra Gestalt an mit dem bis zu 5 (!) ARM-Prozessorkarten auf den vorhandenen 2 Prozessorslots installiert werden können. Das Hydra Multiprocessor Board war dabei, im Gegensatz zur Duet Karte, tatsächlich käuflich zu erwerben und mittels eines Mandelbrotgenerators demonstrierbar.\n\nDie Dortmunder Firma ACE (Acorn Computer Enterprises) produzierte eine Umschaltplatine, den ARM Switcher, die es ermöglichte zwei unterschiedliche ARM Prozessoren im Risc PC eingebaut zu lassen und vor dem Anschalten entscheiden zu können, welche CPU beim Start benutzt wird. Dies war wohl insbesondere in der Zeit des Übergangs und der Anpassung auf die StrongARM Architektur eine sinnvolle Lösung, da so bereits vorhandene Software weitergenutzt werden konnte.\n\nDie von Acorn 1994 ebenfalls angekündigten Media-Prozessoren (etwa DSPs, MPEG-Decoder oder Grafikbeschleuniger) für den zweiten Prozessorslot sind nie in dieser Form allgemeinverfügbar geworden. Möglicherweise wäre dies die bessere Option gewesen, den Slot zu benutzen; dem widersprach allerdings wohl der Zwang zur PC-Kompatibiltät, dem in dieser Zeit alle Hersteller mit eigener Systemarchitektur unterlagen oder zu unterliegen meinten.\nDas Gehäuse ist modular über mehrere übereinander stapelbare Gehäuseteile erweiterbar. Dabei ist bereits die erste unterste Ebene baugleich zu allen potentiell folgenden. Eine Basisplatte fasst das Mainboard sowie das Netzteil und eine frontseitig installierte Festplatte. Jedes zusätzliche Ebenenmodul bietet Platz für ein weiteres 5.25\" Laufwerk und einen 3.5\" Schacht, etwa für Floppies oder weitere Festplatten. Alle Module werden durch insgesamt 4 steck- und drehbare Stifte an den Gehäuseecken mit der Basisplatte zusammengehalten und verriegelt. Erweiterungssteckplätze für Steckkarten, sogenannte Podules, werden über eine Riser-Karte, die bei Acorn Backplane genannt wird, vertikal angeordnet. Je Ebenenmodul sind dabei 2 Erweiterungsslots möglich. Ein Gehäuse mit 2 Ebenen kann also 4 Steckkarten aufnehmen, wenn die entsprechende Backplane vorhanden ist; höhere Gehäuse entsprechend mehr. Im Standardgehäuse mit einer Ebene sind es entsprechend nur die unteren 2 Slots, die dafür aber DMA fähig sind. Karten die diesen Modus unterstützen (z. B. SCSI Controller) sollten daher am besten hier installiert werden. Ab spätestens 4 Slots muss die Stromversorgung beachtet werden, die bei den Risc PCs nicht auf große Vollausbauten ausgelegt sind. Es existieren zwei unterschiedlich starke Netzteile mit 70 Watt bzw. 103 Watt Leistung.\n\nDer wohl aufwendigste und bemerkenswerteste Risc PC Aufbau ist die auf der Acorn World Show im November 1996 vorgestellte Maschine mit ganzen 10 Gehäuseebenen, die unter dem Namen „The Rocket Ship Computer“ bekannt geworden ist.\n\nDer offizielle Risc-PC-Nachfolger Phoebe oder auch \"Risc PC-2\" wurde von Acorn nie fertiggestellt. Im Jahre 2000 erschien der Kinetic Risc PC mit StrongARM, RISC OS 4 und Speicher auf der CPU-Karte.\nDieser wurde nach dem Ende von Acorn von Castle Technology Ltd., einem Anbieter von Zusatzhardware für die Acorn-Geräte, weiter gebaut und vertrieben. Castle Technology Ltd. hat später, im Dezember 2002, einen eigenen Nachfolger des Risc-PC auf Basis des X-Scale-Prozessors herausgebracht, den IYONIX pc.\nVon der britischen Firma MicroDigital gab es ein besonders interessantes Gerät dessen wichtigste Hardwarekomponenten durch FPGAs, Field Programmable Gate Array, dargestellt wurden, den Omega. Damit wären Hardwareanpassungen durch Software-Updates möglich geworden. Allerdings fiel der Zeitpunkt des ersten demonstrierbaren stabilen Betriebs unglücklich ungefähr mit dem Erscheinen des IYONIX PC zusammen, wodurch ein großer kommerzieller Erfolg für den Omega ausblieb.\nIhm gebührt aber die Ehre bei vielen ehemaligen RISC OS Nutzern, die Hoffnung auf eine neue Hardwaregeneration nach der Zerschlagung der Firma Acorn aufrechterhalten zu haben.\n\nDer A9home war ein kleiner Mini-Computer der Firma Advantage Six von 2005. Kleiner als der MacMini, der etwa zeitgleich erschien, ist er, wie der IYONIX PC schon ein Gerät der neueren Generation mit vollem 32Bit Adressbus. Der A9home deutet bereits den Übergang auf kleine Geräte mit Boards in Miniformat oder gar vollintegrierte Systeme in einem Chip, wie den Raspberry Pi, an.\n\nDie Miniaturisierung hat auch vor dem Gehäuse nicht haltgemacht, was sich an kommerziellen Geräten wie BIK (BeagleBoard-In-Kiste) (in England als ARMini) von 2010 oder PIK (Pandaboard-In-Kiste) (in England als ARMiniX) von 2011 gut erkennen läßt.\nMittels 3D-Druck ist heutzutage sogar ein, an das große Vorbild angelehnte, selbstgefertigtes Gehäuse möglich.\n\nDerzeit aktuelle Nachfolgesysteme sind bis zu 140mal schneller als ein Risc PC und basieren u. a. auf dem BeagleBoard, dem Wandboard, dem Titanium von Elesar oder dem RaspberryPi in allen seinen Versionen.\n\n"}
{"id": "161719", "url": "https://de.wikipedia.org/wiki?curid=161719", "title": "Microsoft Dynamics NAV", "text": "Microsoft Dynamics NAV\n\nMicrosoft Dynamics NAV ist eine Standardsoftware für ERP-Systeme. 2002 übernahm Microsoft den dänischen Hersteller Navision Software A/S und integrierte ihn in seinen Geschäftsbereich \"Microsoft Business Solutions\". Seitdem wird Microsoft Dynamics NAV schrittweise mit den anderen ERP-Lösungen von Microsoft zusammengeführt. Die aktuelle Version aus dem Jahr 2017 trägt daher nicht mehr den damaligen Namen \"Navision\", sondern heißt jetzt \"Microsoft Dynamics NAV\".\n\nMicrosoft Dynamics NAV wird besonders von kleinen und mittleren Unternehmen eingesetzt und durch entsprechende Microsoft-Partner im Bereich Business Solutions vertrieben. Diese passen die Standardsoftware an die jeweiligen Unternehmensprozesse an. Die Anpassungen werden in der produktspezifischen Programmiersprache C/AL realisiert. Für verschiedene Branchen existiert eine Vielzahl von Branchenlösungen, die oft von Microsoft-Partnern entwickelt werden.\n\nNavision wurde ursprünglich von Navision Software A/S, einem dänischen Unternehmen, entwickelt. Die Softwarelösung verbreitete sich vor allem in Europa, wo es von Navision Solution Centern (NSCs) vertrieben wurde. Die Schwerpunkte lagen in Dänemark und Deutschland. Die Software verfügte über eine eigene satzorientierte proprietäre Datenbank, die speziell auf die Bedürfnisse der ERP-Software zugeschnitten war.\n\nNavision hatte mit Microsoft lange Zeit kooperiert, bevor es im Juli 2002 von Microsoft übernommen wurde. Microsoft wollte sich damit auf dem Markt für ERP-Systeme positionieren. Die derzeit aktuelle Version, die im Dezember 2017 erschien, ist Microsoft Dynamics NAV 2018. Frühere Produktversionen hießen Microsoft Dynamics NAV 2017, NAV 2016, NAV 2015, NAV 2013 R2, NAV 2013, NAV 2009 R2, NAV 2009 SP1, NAV 2009, NAV 5.0 SP1, NAV 5.0, Microsoft Navision 4.0 SP3, 4.0 SP2, Microsoft Business Solutions Navision 4.0 SP1, 4.0, 3.70, Navision Attain 3.60, 3.10, 3.01, 3.0, Navision Solution 3.0, Navision Financials 2.65, 2.60, 2.50, 2.01, 2.0, 1.3, 1.2, 1.0 und unter einer zeichenorientieren Benutzungsoberfläche gab es NAVISION 1.00 bis NAVISION 3.56a, welche auch als „blaue Version“ bezeichnet wurde.\n\n\nC/AL (\"C/SIDE Application Language\") ist eine interne Programmiersprache für Microsoft Dynamics NAV auf Basis der C/SIDE-Entwicklungsumgebung. C/AL ist ereignisgesteuert, aber nicht objektorientiert. C/AL zählt zu den Programmiersprachen der 4. Generation. C/AL ist syntaktisch mit Turbo Pascal verwandt, hat allerdings keine so strenge Typisierung wie Pascal.\n\nPraktisch der gesamte Funktionsumfang von Microsoft Dynamics NAV ist in C/AL programmiert und für die Solution-Center zugänglich. Somit kann NAV an die Bedürfnisse und Wünsche der Kunden angepasst werden.\n\nDer Zugang zu C/AL kann optional in einer Entwickler-Lizenz erworben werden. Programmierer erhalten dadurch die Möglichkeit, die Anwendung an ihre spezifischen Anforderungen anzupassen. Die Anwendungsobjekte können durch eine binäre oder textbasierte Import-/Exportfunktion einfach portiert werden.\n\nUnter C/SIDE (\"Client/Server Integrated Development Environment\") versteht man zum einen die Microsoft Navision eigene Entwicklungsumgebung, welche sich in wesentlichen Teilen auf das interne Datenbankmodell stützt als auch die von Navision selbst verwendete proprietäre Datenbank.\n\nBis etwa Navision 3.60 wurde diese Datenbank als Basis für Installationen empfohlen.\nDiese ist auch bis Dynamics NAV 2009 R2 bei Verwendung des bisherigen \"Classic Clients\" weiter verwendbar, jedoch wurde für neuere Installationen vermehrt der Microsoft SQL-Server empfohlen, nur dieser unterstützt wahlweise auch den ab NAV 2009 eingeführten rollenbasierten Client. Ab NAV 2013 ist dann nur noch der Microsoft SQL-Server nutzbar.\nNachteil der oftmals auch als „Native Datenbank“ oder neuerdings als \"Microsoft Dynamics NAV Classic Server\" bezeichneten Navision-eigenen Datenbank ist, dass diese nicht oder nur eingeschränkt mit SQL-Befehlen angesprochen werden kann, sowie die eingeschränkten Sicherungs- bzw. Wiederherstellungsmöglichkeiten im Vergleich zum Microsoft SQL Server. Mit anderen Datenbank-Servern, etwa Oracle Database, ist Navision nicht kompatibel.\n\nDie Navision Datenbank zeichnet sich durch hohe Stabilität, Transaktionssicherheit, Trigger und durch hohe Geschwindigkeit bei der Bildung von Summen aus. Genau diese Funktionalität wird gerade in Buchhaltungssystemen sehr oft benötigt (Summenbilden auf Konten, bzw. innerhalb von Zeiträumen).\n\nDurch einen kleinen Trick wird erreicht, dass durch lediglich drei Datenbankzugriffe (richtigerweise durch Zugriffe auf einen speziellen Index) Summen aus beliebig großen Datenmengen gebildet werden können. Ein Zugriff auf die Einzelwerte zur Summenbildung ist somit nicht erforderlich.\n\nDiese Technik ist unter dem geschützten Namen SIFT (SumIndex Field Technology oder SumIndexed Flow Technology) direkt innerhalb der Datenbank implementiert.\nIn sogenannten FlowFields werden die berechneten Summen angezeigt. FlowFilter dienen zur Einschränkung der in den FlowFields angegebenen Werte auf bestimmte vorgegebene Kriterien.\n\nBei SIFT werden mit dem Feldindex auch die aufsummierten Werte mitgespeichert. Durch Subtraktion des höchsten Summenindexwertes von dem Summenindexwert des dem niedrigsten Datensatz vorhergehenden Wertes wird die Summe der Werte zwischen beiden Werten ermittelt.\n\nEine Tabelle enthält chronologisch sortiert folgende Werte:\n\nBeim Einfügen eines Datensatzes wird der zugehörige SumIndex-Wert (wie in der letzten Spalte der Tabelle ersichtlich) durch einfache Addition des Betrages mit dem vorhergehenden SumIndex-Werte ermittelt.\n\nZur Berechnung der Summe aller Werte von 2006-06-01 bis 2006-12-30 geht man wie folgt vor:\n\n\nDieses Verfahren funktioniert auch mit einer beliebig großen Anzahl von Werten zwischen den beiden gesuchten Größen.\n\nDie Funktionalität von Microsoft Dynamics NAV basiert auf Objekten, die in der Datenbank selbst abgelegt sind. Microsoft Dynamics NAV besteht aus den Objekttypen Table, Page, Report, Codeunit, XMLport, MenuSuite und Query. Die Objekttypen \"XMLport\" und \"MenuSuite\" sind seit der Version 4 von Microsoft Dynamics NAV enthalten, das Objekt \"Page\" seit Version 2009 und das Objekt \"Query\" seit Version 2013, in der die in älteren Versionen vorhandenen Objekte \"Form\" und \"Dataport\" entfernt wurden. Die Objekttypen \"TableData\" und \"System\" sind anwendungsinterne Objekttypen, die von Entwicklern, die die übrigen Objekttypen anpassen oder auch neu erstellen können, nicht verändert werden können.\n\n\nTable-, Form-, Page-, Report-, Codeunit-, Dataport- XMLport- und Query-Objekte enthalten Trigger, in denen Programmcode hinterlegt werden kann.\n\nIn der C/SIDE-Umgebung kann ein Programmcode in vorhandenen Triggern hinterlegt werden. Es existieren drei Arten von Triggern:\n\nDokumentations-Trigger befinden sich in allen Objekten (mit Ausnahme des \"MenuSuite\") und dienen zu Dokumentationszwecken. Jeglicher Inhalt dieser Trigger wird nicht ausgeführt.\n\nEvent-Trigger werden bei bestimmten Ereignissen von Datenbanken ausgeführt. Diese Trigger werden automatisch bei der Erzeugung neuer Objekte in Datenbanken vom System erstellt. Es ist nicht möglich diese Trigger manuell zu erstellen.\n\nWird in einem Objekt eine Funktion erstellt, entsteht ein neuer „Funktions-Trigger“, der den Namen der Funktion trägt. In neuen Funktions-Triggern wird dann der Programmcode hinterlegt, der bei Aufruf der Funktion ausgeführt wird.\n\nDie im Dezember 2017 aktuelle Version von Navision ist Dynamics NAV 2018.\n\nDer Support von NAV (Navision) wurde Anfang 2008 nach offiziellen Angaben von Microsoft auf 10 Jahre erweitert: 5 Jahre Mainstream Support und 5 Jahre Extended Support. Der Support beginnt mit der Version NAV 5.0.\n\nIm ersten Quartal 2008 erschien eine Lösung für kleine Unternehmen und Kleinstunternehmen. Diese Lösung erhielt den Namen Microsoft Dynamics Entrepreneur Solution 2008 und basierte auf Dynamics NAV 5.0. Microsoft hat die Weiterentwicklung von Entrepreneur jedoch eingestellt, um sein Marketing strategisch auf NAV 2009 konzentrieren zu können. (Verkündet per Brief und Mail an MS Partner sowie auf dem 2008er Microsoft Dynamics ERP Launch in Fürstenfeldbruck, 8./9. September 2008)\n\nDynamics NAV 2009 erhielt als erstes NAV-Release statt der bisherigen Versionsnummer (in fortlaufender interner Versionierung: 6) die bei Microsoft-Software übliche Jahreszahl. Diese Version bot erstmals:\n\n\nAb der Version NAV 2013 (in interner Versionierung: 7) werden ältere Technologien wie der Classic Client, der proprietäre „native“ Server, NAS (Navision Application Server), C/FRONT, Forms, Dataports und herkömmliche Reportobjekte nicht mehr angeboten bzw. unterstützt. NAV 2013 war die erste Version, die für Unicode und für drei Clienttypen konzipiert war, einen Windowsclient (bisher bekannt als RTC (Role Tailored Client)), sowie einen neuen ebenfalls rollenbasierten Webclient (kompatibel mit Internet Explorer, Firefox, Chrome und Apple Safari) und einen neuen SharePoint-Client. In NAV 2015 (in interner Versionierung: 8) wurden zusätzlich auf Tablets abgestimmte Tablet-Clients eingeführt, die über die App Stores für Windows, Android und iOS verfügbar sind. NAV 2015 kann innerbetrieblich, bei externen Providern oder per Cloud Computing auf Microsoft Azure gehostet werden. Als neuer Objekttyp steht ab NAV 2013 \"Query\" zur Verfügung, mit dem Leseoperationen in der Datenbank performanter umgesetzt werden können. Für den Datenaustausch wird erstmals Open Data Protocol (OData) unterstützt (in NAV 2013 beschränkt auf Lesezugriffe, ab NAV 2013 R2 sind auch Schreiboperationen in die Datenbank möglich). Als Entwicklungsumgebungen werden Funktionalitäten des ehemaligen Classic Client und von Microsoft Visual Studio genutzt, als Programmiersprachen kommen dabei sowohl Visual Basic für Berichte als auch C# für optionale Add-Ins zusammen mit der proprietären Sprache C/AL zum Einsatz. Um Add-ins auch beim Einsatz des Webclients nutzbar zu machen, werden hierbei ab NAV 2013 R2 sowohl JavaScript als auch HTML5 unterstützt. Die in älteren Versionen bereits vorhandene Mandantenfähigkeit zum Verwalten von Konzernstrukturen und Tochterfirmen wurde in NAV 2013 R2 um eine Multi-Tenant-Fähigkeit erweitert, um auch rechtlich eigenständige Unternehmen als getrennte „Tenants“ (Mieter) zusammen in einer Datenbank betreiben zu können. Die Verwaltung des Quellcodes wurde dafür ab NAV 2013 R2 von der C/SIDE-Entwicklungsumgebung auf den Server verlagert, damit auch zeitweilige Offline-Tenants bei ihrem nächsten Online-Status die Programmänderungen erhalten.\n\n\n"}
{"id": "161867", "url": "https://de.wikipedia.org/wiki?curid=161867", "title": "Leiterplattenentflechtung", "text": "Leiterplattenentflechtung\n\nDie Leiterplattenentflechtung ist ein Arbeitsschritt beim Layoutentwurf (kurz: layouten) von elektronischen Leiterplatten. Dabei wird der entworfene elektrische Schaltplan nach dem manuellen oder automatischen Platzieren (vgl. Autoplacer) der benötigten Bauelemente auf der Leiterplatte in ein Leiterbahnnetzwerk umgesetzt. Sie wird heute fast ausnahmslos am Computer per Hand oder automatisiert mithilfe eines sogenannten \"Autorouters\" durchgeführt.\n\nCAD-Software zum Leiterplattenentwurf umfasst neben dem Schaltplanentwurf und dessen Simulation oft auch Auto-Platzierer und Autorouter. Damit ein Autorouter sinnvolle Ergebnisse liefern kann, müssen ihm zuvor Designregeln vorgegeben werden. Macht man das nicht, stoßen diese automatischen Funktionen an Grenzen, so dass Leiterbahnen zumindest teilweise manuell verlegt werden müssen.\n\nDie Entflechtung erfolgt nach der Erstellung eines elektrischen Schaltplans, der auch in Form einer Netzliste vorliegen kann, und der Platzierung der Bauteile am Bildschirm. Nach diesen Schritten liegt ein sogenanntes „Rattennest“ () vor, bei dem alle elektrischen Verbindungen durcheinander auf den kürzesten Wegen dargestellt sind.\n\nBei der Entflechtung müssen u. a. Leiterplattentechnologie, Bauteilgeometrien, Lage der Bauteile, Signallaufzeiten, Stromstärken sowie Lage der äußeren Anschlüsse (Steckverbinder) berücksichtigt werden. Diese Arbeit wird durch kommerzielle Layout-Programme (CAD-Systeme) wie z. B. Pulsonix, EDWinXP, OrCAD, Eagle, TARGET 3001!, Cadence Allegro, Siemens AG VCAD, Altium Designer (vormals \"Protel\") oder Expedition PCB unterstützt. An Open-Source-Layout-Programmen sind KiCad und gEDA bekannt. Insbesondere durch Bauteile-Datenbanken und die Überprüfungen von Design-Regeln erleichtern Layout-Programme die Arbeit bei der Leiterbahnentflechtung.\nAusgegeben werden die CAD-Daten für Platinenherstellungen, Bestückungen und Einkauf typischerweise im Gerber-Format sowie als Stückliste (englisch: \"Bill of Materials\", BOM).\n\n\nDie weitere Platzierung wird so durchgeführt, dass:\n\nDie Design-Regeln können unter Umständen sehr umfangreich sein. Sie ermöglichen es aber dem Layouter, die Entflechtung mit weniger aufwendigen Iterationen und Fehlerkontrollen durchzuführen.\n\nDie Platzierung erfolgt in der Regel interaktiv. Dabei werden die zur erzeugenden Verbindungen als „Gummibänder“ zwischen den Anschlüssen dargestellt, um eine vorteilhafte Platzierung zu ermöglichen. Zur Platzierung unkritischer Bauteile am Ende des Platzierens eignen sich heute in weniger dichten Designs durchaus auch automatische Platzierhelfer (Autoplacer).\n\nWenn ein Entflechtungsprogramm (Autorouter) eingesetzt wird, müssen vorher Randbedingungen in das Layout-Tool eingegeben werden, damit der Router diese Regeln auch beachtet. Damit ein Autorouter brauchbare Ergebnisse liefert, können die Routing-Strategien vorgegeben werden. Ob es sinnvoller ist, einen Autorouter zu verwenden, oder von Hand zu routen, ist stark von der Art des Designs abhängig. Sollen mehrere ähnliche, digitale Designs bearbeitet werden, so kann es sinnvoll sein, Regeln und Routingstrategien zu optimieren und wiederzuverwenden. Teilweise nimmt die Regel-Eingabe mehr Zeit in Anspruch als die manuelle Leiterplattenentflechtung durch einen erfahrenen Layouter.\n\nOft müssen auch Regeln beachtet werden, die in einigen Routern nicht vorgegeben werden können, z. B. Sternpunkterdung, Bezugspunkte für Analogspannungen, Eigenschaften beim Löten, Wärmemanagement bzw. thermal vias. Hier können eventuell nur Teile des Designs mit dem Autorouter entflochten werden.\n\nBei manueller Entflechtung werden zuerst kritische Leitungen wie Hochfrequenzleitungen oder Busse verlegt.\nOft wählt man die Vorzugsrichtung der Leiterbahnen auf der Oberseite senkrecht zu derjenigen auf der Unterseite. Bei jeder Richtungsänderung kann nun mittels einer Durchkontaktierung die Ebene gewechselt werden.\n\nBei der Verlegung bietet die Layout-Software Unterstützung durch die Anzeige eines Saumes um den Leiterzug, der die Einhaltung der Designregeln erleichtert.\nWährend der Entflechtung ist es oft nötig, einzelne Bauteile noch einmal zu verschieben. Bei hochwertigen EDA-Programmen gibt es daher meist die Möglichkeit, einzelne Signale halbautomatisch zu entflechten und per „push aside“ oder „push and shove“ bereits verlegte Leiterbahnen und Bauteile aus dem Korridor für die gerade per Hand bearbeitete Leiterbahn schieben zu lassen.\n\nNach dem Layout kann die Software mittels automatischem Designregeltest (, DRC) prüfen, ob alle festgelegten Design-Regeln eingehalten werden und keine logischen Fehler vorliegen, z. B. nicht angeschlossene Bauteile oder Leiterbahnkreuzungen.\n\nBei sehr dicht belegten Leiterplatten können unter Umständen auch per Hand nicht alle Leiterbahnen verlegt werden, weil für die Verbindung einiger Bauteilanschlusspunkte keine freien „Gassen“ mehr existieren. Diese fehlenden Verbindungen müssen dann bei der Bestückung durch Drahtbrücken gebildet werden oder von Hand als Drähte verlegt werden. Das ist aus Kostengründen, aber auch aufgrund der technischen Anforderungen (Hochfrequenz, Signallaufzeiten, Zuverlässigkeit) nur bei wenigen Produkten vertretbar. Hinzu kommt, dass bei Leiterbahnbreiten und -abständen von 75 bis 150 µm Lötaugen zum Anschluss von Drähten kaum noch Platz finden. Als Alternative zu Drahtbrücken gibt es Lötbrücken (Jumper) und „Null-Ohm-Widerstände“. Gegebenenfalls kann man auch die Anzahl der Lagen erhöhen oder bei Mehrlagenleiterplatten mit Durchkontaktierungen arbeiten, die sich nicht auf allen Lagen befinden (sogenannte \"blind vias\" und \"buried vias\", siehe Durchkontaktierung).\n\nBei der Leiterplattenentflechtung sind meist weitere Vorgaben zu beachten, z. B.:\n\nDas Resultat ist ein (hoffentlich vollständig) entflochtenes Design, welches keine Luftlinien mehr enthält und das alle Informationen in sich trägt, um eine Leiterplatte herstellen zu können. Es besteht aus mehreren Schichten virtueller Kupferlagen und Verbindungen zwischen ihnen, sowie den Bauteildimensionen und Platzierungskoordinaten. Aus diesen Daten werden i. d. R. Produktionsdaten (Gerberdaten) für die Belichtung und Ätzung beim Hersteller gewonnen. Ferner kann das sogenannte \"drill file\", eine Datei mit Informationen über die Anordnung und Grössen der benötigten Bohrungen, erzeugt werden mit dessen Hilfe ein CNC-Bohrwerkzeug die Platine bearbeiten kann. Auch Informationen für den boundary-scan-Test und flying-probe-Test können aus dem Platinenfile gezogen werden.\n\n"}
{"id": "163248", "url": "https://de.wikipedia.org/wiki?curid=163248", "title": "Wired", "text": "Wired\n\nWired ist ein durch eine Gruppe um Louis Rossetto und Jane Metcalfe im März 1993 gegründetes US-amerikanisches Technik-Magazin. Es greift auch aktuelle Entwicklungen rund um Netzkultur, Architektur, Design und Politik auf. Es versteht sich als Medium der Geeks und Technik-Freaks. Wired gehört seit 1998 zum Condé-Nast-Verlag.\n\nBesonders bekannt wurde Wired durch die New-Economy-Bewegung, die es begleitete. Den spöttischen Beinamen „Zentralorgan der kalifornischen Ideologie“ erhielt das Magazin von Kritikern, die sich besonders gegen die von Wired vertretenen libertären Auffassungen richteten, welche großen Einfluss auf die Weltanschauung der frühen Internetnutzer hatten.\n\nNach dem Platzen der Dotcom-Blase 2000 wurde die Redaktion drastisch reduziert und die Ausrichtung verändert. Wired hat sich seitdem stark kommerziellen Themen in der Silicon-Valley-Kultur zugewandt und politische Themen ausgeblendet. Erst Jahre später hat Wired seine Ausrichtung geändert. Aktuell sind auch Themen wie Gadgets, Produktdesign, Politik und auch Architektur stark vertreten.\n\nSeit Februar 2009 veröffentlicht der Verlag Condé Nast Italia eine italienische Ausgabe von Wired und betreibt mit Wired.it eine dazugehörige Website. Seit Anfang April 2009 wird zudem mit Wired UK eine auf Großbritannien abgestimmte Version des US-Magazins erprobt, die sowohl Artikel aus der US-Ausgabe als auch eigene Artikel enthält.\n\nDie Online-Ausgabe des Magazins bietet Artikel zum kostenlosen Abruf im Netz und löst regelmäßig rege Diskussionen aus. Seit Juni 2010 ist das Magazin auch als kostenpflichtige App auf dem iPad verfügbar. Wired veröffentlicht seit 1996 jährlich eine Liste mit den bei seinen Lesern bekanntesten zehn Vaporware-Produkten.\n\nZu den regelmäßigen Kolumnisten des Blattes gehören der Juraprofessor Lawrence Lessig und der Cyberpunk-Autor Bruce Sterling. Bis November 2012 war Chris Anderson, der durch die Long-Tail-Theorie bekannt wurde, Chefredakteur. Aktueller Chefredakteur ist Scott Dadich, der sich von 2006 bis 2010 u. a. als künstlerischer Leiter bei dem Magazin betätigt hat.\n\nSeit dem 7. November 2011 werden die von den Photographen des Magazins erstellten Bilder unter einer Creative Commons-Lizenz veröffentlicht. Im Rahmen der Ankündigung wurden fünfzig Fotos, darunter Porträts von Steve Jobs, Steve Wozniak, Mark Zuckerberg, Steve Ballmer, Trent Reznor und J. J. Abrams, hochauflösend auf der Website und im neu eingerichteten Flickr-Stream eingestellt.\n\nDie 2013 erstellte Satireserie Codefellas wurde ebenfalls von Wired vertrieben.\n\nAm 8. September 2011 erschien eine erste Testausgabe zunächst als Beilage zum Magazin GQ und als App für das iPad. Ein eigenes Blog widmete sich thematisch der Entwicklung der Testausgabe, anhand deren Erfolges über das weitere Vorgehen entschieden werden sollte. Chefredakteur war Thomas Knüwer, der bereits ab der zweiten Ausgabe von Alexander von Streit abgelöst wurde.\n\nAm 7. Dezember 2011 gab Wired Deutschland bekannt, zunächst zwei weitere Ausgaben sowohl gebündelt mit GQ als auch als Einzelausgabe herauszubringen. Die zweite Ausgabe der Zeitschrift erschien im April, die dritte im September 2012.\n2013 erscheinen ebenfalls zwei Ausgaben und es wurde eine Magazin-App für Android-Tablets eingeführt.\n\nIm Oktober 2014 wurde ein erweiterter Versuch gestartet, sich auch auf dem deutschen Markt zu etablieren, mit Nikolaus Röttger als Chefredakteur. Die Startauflage betrug 120.000 Exemplare.\n\n2016 wurde die Erscheinungsweise der Printausgabe von Wired von zehn Mal im Jahr auf vierteljährlich umgestellt, Digital- und Eventaktivitäten wurden ausgebaut. Für diese Aufgabe verpflichtete das Magazin die Journalistin Domenika Ahlrichs, die zuvor stellvertretende Chefredakteurin bei Zeit Online war. Diese Funktion übernahm sie ab Januar 2016 auch bei Wired.\n\nIm Januar 2018 wurde die Printausgabe eingestellt. Wired erschien anschließend ausschließlich digital. Zum Ende des Jahres 2018 wurde auch die digitale Ausgabe eingestellt.\n\nDas Hauptquartier der Redaktion ist einer der ersten Innenräume, die in Google Street View aufgenommen wurden. Google hat die Büros in San Francisco im Sommer 2012 aufgenommen und Ende August öffentlich bereitgestellt. Gesichter und andere personenbezogene Informationen wurden dabei unkenntlich gemacht.\n\n\n"}
{"id": "164203", "url": "https://de.wikipedia.org/wiki?curid=164203", "title": "ENIAC", "text": "ENIAC\n\nDer Electronic Numerical Integrator and Computer (ENIAC) war der erste elektronische turingmächtige Universalrechner. Er diente der US-Armee zur Berechnung ballistischer Tabellen.\n\nIm Auftrag der US-Armee wurde ENIAC ab 1942 von John Presper Eckert und John William Mauchly an der University of Pennsylvania entwickelt und am 14. Februar 1946 der Öffentlichkeit vorgestellt.\n\nMauchly und Eckert gründeten 1946 eine Computerfirma, die Eckert-Mauchly Computer Corporation, die später von Remington Rand übernommen wurde. 1947 wurde ein Patent angemeldet, über dessen Gültigkeit 1967 langjährige Gerichtsverfahren begannen. Sie führten dazu, dass das Patent 1973 wegen der schon vom Atanasoff-Berry-Computer (ABC) bekannten Eigenschaften für ungültig erklärt wurde; da Mauchly während eines Besuches bei Atanasoff im Jahr 1941 Gelegenheit hatte, den ABC zu studieren, und wahrscheinlich einige Inspiration daraus zog, wurde der ENIAC vom Gericht als abgeleitetes Werk angesehen. Der Ruhm für die Erfindung des ersten elektronischen Rechners, den Mauchly und Eckert bis dahin geteilt hatten, geht seither auf Atanasoff über.\n\nVon Philadelphia aus zog der ENIAC 1947 ins nahegelegene \"Ballistic Research Lab\" in Aberdeen um.\nENIAC wurde am 2. Oktober 1955 abgeschaltet.\n\nÄhnlich dem Atanasoff-Berry-Computer (1938–1942) und dem britischen Colossus (1943), einem kryptographischen Spezialrechner, benutzte der ENIAC Elektronenröhren zur Repräsentation von Zahlen und elektrische Pulse für deren Übertragung. Dies bewirkte eine deutlich höhere Rechenleistung als die von Konrad Zuses Z3 (1941), der zwar eine modernere Architektur aufwies, aber noch auf elektromechanischen Relais basierte. Wie der ASCC (erbaut zwischen 1939 und 1944, später als „Mark I“ bekannt) verwendete der ENIAC ein Dezimalsystem zur Darstellung von Zahlen.\n\nDer ENIAC bestand aus 40 parallel arbeitenden Komponenten, von denen jede 60 cm breit, 270 cm hoch und 70 cm tief war. Die komplette Anlage war in U-Form aufgebaut, beanspruchte eine Fläche von 10 m × 17 m und wog 27 Tonnen. Er bestand aus 17.468 Elektronenröhren, 7.200 Dioden, 1.500 Relais, 70.000 Widerständen und 10.000 Kondensatoren. Die Leistungsaufnahme lag bei 174 kW. Der Bau des ENIAC kostete 468.000 US-$ – ein Betrag, der nur aufgrund des hohen Bedarfs an Rechenleistung seitens der US-Armee zur Verfügung stand (entspricht einem heutigen Wert von ungefähr  US-$). Im Vergleich zu seinen Vorgängern beeindruckt der ENIAC schon durch seine schiere Größe.\n\nEin großes Problem bei der Entwicklung des ENIAC war die Fehleranfälligkeit der Elektronenröhren. Wenn nur eine der 17.468 Röhren ausfiel, rechnete die gesamte Maschine fehlerhaft. Um die Kosten dieser unvermeidlichen Ausfälle gering zu halten, wurden in den ENIAC eigens Diagnoseprogramme eingebaut, die das Auffinden einer auszutauschenden Röhre erleichterten. Eine Gegenmaßnahme bestand darin, stärkere Röhren einzubauen, als man eigentlich gebraucht hätte, und diese nur mit etwa 10 % ihrer Nennleistung zu betreiben. Außerdem wurde bemerkt, dass mehr Röhren beim Ein- und Ausschalten kaputt gingen als während des laufenden Betriebs. Als Konsequenz ging man dazu über, den ENIAC einfach nicht mehr auszuschalten. Die Ausfallzeit konnte so auf wenige Stunden je Woche reduziert werden.\n\nDer ENIAC konnte addieren, subtrahieren, multiplizieren, dividieren und Quadratwurzeln ziehen.\n\nEine Addition/Subtraktion brauchte 0,2 Millisekunden, eine Multiplikation bis zu 2,8 ms, eine Division bis zu 24 ms und eine Quadratwurzel mehr als 300 ms.\n\nGrundlegende Komponente für die Funktion des ENIAC war der Akkumulator, der eine 10-stellige vorzeichenbehaftete Dezimalzahl speichern sowie addieren und subtrahieren konnte. Jeder der 20 Akkumulatoren konnte eine solche Rechenoperation in 0,2 Millisekunden ausführen. Dieses Zeitintervall wird auch als Additionszyklus bezeichnet. Für Rechnungen mit doppelter Genauigkeit ließen sich zwei Akkumulatoren zusammenschalten.\n\nWeitere arithmetische Komponenten waren der Multiplikator (drei Exemplare) und der Divider/Square-Rooter. Ein Multiplikator implementierte eine Multiplikationstabelle, nach der ein Unterprogramm gesteuert wurde, das auf vier Akkumulatoren lief. Eine Multiplikation dauerte (je nach Länge der Zahlen) bis zu 2,8 Millisekunden. Ähnlich war auch der Divider/Square-Rooter konstruiert, der für eine Division bzw. Quadratwurzel bis zu 65 Millisekunden (13 Additionszyklen je Ziffer) benötigte. Die Programmierung komplexer Berechnungen war mit dem Master Programmer (zwei Exemplare) möglich, der rekursive Programmierung erlaubte.\n\nFür den Start der Anlage war die Initiating Unit zuständig. Beim Einschalten des ENIAC nahmen die Flipflops zufällige Werte an, sodass die Komponenten in einem undefinierten Zustand waren. Durch ein spezielles Programm der Initiating-Unit konnten die Flipflops in einen definierten Zustand gebracht, und z. B. die Akkumulatoren mit 0 initialisiert werden. Des Weiteren hatte die Initiating-Unit einen Startknopf, mit dem ein ENIAC-Programm manuell gestartet wurde. Als Taktgeber diente die Cycling Unit, die die anderen Komponenten über statische Kabel mit Steuerpulsen versorgte. Sie konnte auch in einen Schritt-für-Schritt-Modus geschaltet werden, der die Fehlersuche vereinfachte.\n\nDer ENIAC wurde programmiert, indem man die einzelnen Komponenten mit Kabeln verband und die gewünschten Operationen auf Drehschaltern einstellte. Der ENIAC wurde von Frauen programmiert, den „ENIAC-Frauen“: Kay McNulty, Jean Bartik, Betty Holberton, Marlyn Wescoff, Frances Bilas und Ruth Teitelbaum. Sie hatten zuvor für das Militär ballistische Berechnungen an mechanischen Tischrechnern angestellt.\n\nDie Komponenten des ENIAC waren statisch miteinander verbunden, um die Taktimpulse der Cycling Unit zu empfangen. Weitere statische Verbindungen gab es zwischen den zusammenarbeitenden Komponenten (z. B. zwischen einem Multiplikator und den 4 zugeordneten Akkumulatoren). Alle weiteren Verbindungen für den Ablauf eines Programmes mussten manuell gesteckt werden. Für die Übermittlung von Programmimpulsen gab es auf Fußhöhe waagerecht verlaufende Kabel in Program Trays, für Zahlenpulse wurden die Digit Trays in Kopfhöhe genutzt. An Trays und Komponenten gab es Buchsen, in die Kabel gesteckt werden konnten.\n\nEin deutlicher architektonischer Nachteil des ENIAC war das Fehlen eines Befehlsspeichers. Schon die Z1, Z3 und der Mark I lasen ihre Befehle von einem Lochstreifen, während der ENIAC für jedes Programm neu verkabelt werden musste. Nach Ideen John von Neumanns wurde der ENIAC 1948 zu einem Computer mit Befehlsspeicher umgebaut. Dies verlangsamte seine Rechenleistung auf 1/6, aber die Dauer des Umprogrammierens verringerte sich ebenfalls, sodass insgesamt ein Zeitgewinn erzielt wurde.\n\nAls Festwertspeicher dienten der Constant Transmitter (bestehend aus drei Komponenten) und die Function Tables (drei Komponenten, je drei Exemplare). Ersterer diente hauptsächlich zur Ansteuerung eines Lochkartenlesers. Auf letzteren wurden je 104 zehnstellige Dezimalzahlen (allerdings nur sechs Stellen individuell einstellbar) bei einer Zugriffszeit von fünf Additionszyklen gespeichert. Rechenergebnisse konnten auch gedruckt werden: Über das Printer Panel (bestehend aus drei Komponenten) konnte ein Lochkartendrucker angesteuert werden.\n\nEine unmittelbare visuelle Ausgabe war in die Akkumulatoren integriert: Im oberen Bereich der Komponente gab es 102 Glimmlampen zur Anzeige der aktuell gespeicherten Zahl (je zehn für jede der zehn Ziffern, zwei für das Vorzeichen).\n\nAnlässlich der ersten öffentlichen Präsentation des ENIAC im Februar 1946 stülpte man einen halbierten Tischtennisball über jede Leuchte – ein Design, das Vorbild für viele folgende Computer war und stilbildend für die damalige Science-Fiction.\n\n\n\nNachbau und Simulation\n\nDie ENIAC-Programmiererinnen\n"}
{"id": "165590", "url": "https://de.wikipedia.org/wiki?curid=165590", "title": "GNU Make", "text": "GNU Make\n\nGNU Make (kurz \"gmake\") ist die GNU-Implementierung des in der Unix-Welt häufig gebrauchten Programms make. Gegenüber dem traditionellen Funktionsumfang enthält es eine Reihe von Erweiterungen.\nGNU make ist nicht voll kompatibel zu make, denn das Zeichen Backslash wird nicht POSIX-konform behandelt und da die Shell-Aufrufe ohne die Option '-e' erfolgen, bricht GNU make die weitere Bearbeitung nicht immer erwartungsgemäß nach scheiternden Kommandoaufrufen ab.\n\nAuf Linux-Rechnern wird als make-Utility in der Regel die GNU-Version installiert. Ab Version 4.0 wird GNU Guile als Erweiterungssprache unterstützt.\n\nGNU Make wurde von Richard Stallman und Roland McGrath implementiert. Ab Version 3.76 wird die Weiterentwicklung von Paul D. Smith geleitet.\n\n"}
{"id": "165645", "url": "https://de.wikipedia.org/wiki?curid=165645", "title": "Verweissensitive Grafik", "text": "Verweissensitive Grafik\n\nVerweissensitive Grafik () ist ein Begriff aus dem Webdesign und bezieht sich auch auf Multimediaprogramme. Sie bietet eine Möglichkeit, Hyperlinks innerhalb einer Grafik einzubetten. Diese werden mit der Variable \"shape=\" als rechteckige (\"rect\"), runde (\"circle\") oder freie (\"poly\") Schaltflächen realisiert, die sich wie Verweise (Anchor-Links) im Hypertext eines HTML-Dokumentes verhalten. In der Regel deutet ein sich verändernder Mauszeiger auf die nicht sichtbaren Schaltflächen hin. Verweissensitive Grafiken werden sowohl in Bild- als auch in Videodateien eingesetzt.\n\nAuf Webseiten häufig anzutreffende Beispiele sind Landkarten, in denen bestimmte Regionen zu den entsprechenden Seiten führen. Ein weiteres Beispiel sind im Winter oft „interaktive“ Adventskalender.\n\nEine in HTML definierte Imagemap besteht aus einem eigentlichen Bild, das mit einem codice_1-Tag definiert wird. Gleichzeitig wird bei diesem ein Attribut codice_2 angegeben, das auf die Imagemap verweist.\n\nDie Imagemap besteht aus dem codice_3-Tag sowie aus den einzelnen codice_4-Tags, die die eigentlichen Definitionen der Felder darstellen, auf die geklickt werden kann. Diese definieren, vergleichbar mit der Definition bei einem codice_5-Tag, welche URL aufzurufen ist. Folgender Code gibt an, dass ein Bereich (\"9,372,66,397\"), der als rechteckiger Bereich definiert wird, zur Wikipedia-Seite zeigt:\n\n<img src=\"bild.jpg\" alt=\"alternativtext\" usemap=\"#mapname\" />\n<map name=\"mapname\">\n</map>\nGrafische Hervorhebungen können mit CSS oder JavaScript umgesetzt werden.\n\nIn der verbreiteten Wiki-Software Mediawiki gibt es eine Erweiterung „Extension:ImageMap“, mit der Imagemaps dargestellt, und ein Werkzeug, mit dem Imagemaps erstellt werden können. \n<imagemap>\nDatei:Continents vide couleurs.png|300px|Beispiel „Kontinente“\npoly 156 126 274 288 362 340 432 290 676 34 326 26 210 66 Nordamerika\npoly 400 318 366 334 366 388 460 632 490 630 556 400 418 306 Südamerika\n</imagemap>\nAuch in digitalen Videodateien können verweissensitive Grafiken angewandt werden. Dabei muss der Produzent (oder Hersteller) des Videos eine Map-Datei (als reine Textdatei) schreiben, die alle notwendigen Informationen enthält, wie\n\n\nAnwendungen sind beispielsweise aktiv vom Zuschauer einzublendende Zusatzinformationen oder Werbemitteilungen.\n\nUm einen Effekt von Verweissensitivität hervorzurufen greifen viele Videomacher auf Videoportalen zu Tricks zurück, die ein ähnliches Resultat wie eine echte verweissensitive Grafik kreiert. Beispielsweise wird auf Youtube eine Anmerkung in bereits zuvor gefertigte Bereiche des Videos eingebettet, was diesen Bereich innerhalb des Videos \"klickbar\" macht.\n\n\n\n\nTutorials:\n\nSkripte:\n"}
{"id": "167747", "url": "https://de.wikipedia.org/wiki?curid=167747", "title": "Findet Nemo", "text": "Findet Nemo\n\nFindet Nemo (Originaltitel \"Finding Nemo\") ist ein US-amerikanischer Animationsfilm der Pixar Animation Studios aus dem Jahr 2003, der durch Walt Disney und Buena Vista vertrieben wurde. Es ist der fünfte abendfüllende Pixar-Spielfilm.\n\nDer Film lief am 30. Mai 2003 in den Vereinigten Staaten und Kanada in den Kinos an und spielte am Eröffnungswochenende mit etwa 70 Millionen US-Dollar das zum damaligen Zeitpunkt beste Einspielergebnis eines Animationsfilms ein. In Deutschland startete der Film am 20. November 2003. \"Findet Nemo\" erschien am 4. November 2003 als Doppel-DVD in den Vereinigten Staaten und in Kanada und avancierte mit 28 Millionen Kopien zur bestverkauften DVD dieses Jahrgangs.\n\nAls 3D-Fassung kam der Film am 14. September 2012 in die Kinos.\n\nDie Fortsetzung \"Findet Dorie\" kam im Juni 2016 in die US-amerikanischen Kinos.\n\nDer Film erzählt die Geschichte des kleinen Clownfischs Nemo, der im Pazifischen Ozean nahe Australien aufwächst. Sein liebevoller Vater Marlin ist durch den frühen Tod von Nemos Mutter und seinen Geschwistern ängstlich geworden. Er versucht daher, seinen Sohn vor den Gefahren des Meeres zu schützen. Wie der Zufall so will, verliert er ihn schließlich. Nemo wird von Menschen gefangen, und Marlin bricht in die Weiten des Ozeans auf, um seinen Sohn wiederzufinden.\n\nUnterwegs erlebt der vorsichtige Marlin allerlei Abenteuer, die ihn schließlich zu einem mutigen Helden werden lassen. Im Kampf um Nemo wächst er über sich selbst hinaus. Dabei hilft ihm eine unter Amnesie leidende Zufalls-Fischbekanntschaft, die Palettendoktorfisch-Dame Dorie. Auf der gemeinsamen Suche nach Nemo trifft das Duo auf vegetarische Haie, alte Schildkröten, Quallen, vorlaute Möwen, düstere Tiefsee-Anglerfische, hilfreiche Pelikane und Krebse.\n\nDas Clownfischpaar Marlin und Cora ist überglücklich. Sie haben ein Quartier in einer Symbioseanemone gefunden, wo sie ihre Jungen großziehen können. Doch ihr Glück wird von einem Barracuda jäh unterbrochen. Er verschlingt Cora und fast das gesamte Gelege. Nur ein Ei, darin Nemo, bleibt nahezu unversehrt. Nemo wächst wohlbehütet heran, nicht nur weil er Marlins einziges Kind ist: Er hat auch eine schwächere rechte Flosse, von seinem Vater „Glücksflosse“ genannt.\n\nAn seinem ersten Schultag macht sich Nemo zusammen mit seinem Vater Marlin auf den Weg zur Schule. Dem Vater fällt es schwer, den Sohn ziehen zu lassen. Kaum hat er die Fischkinderschar mit ihrem Lehrer allein gelassen, erfährt er, dass sie zum Abhang des Korallenriffs schwimmen wollen, was er für viel zu gefährlich hält. Marlin schwimmt hinterher und findet Nemo schließlich abseits der Schulklasse, als er mit seinen neuen Freunden neugierig ein auf dem Meer schwimmendes Boot bewundert. Als eine Art Mutprobe überbieten sich die jungen Fische gegenseitig darin, dem Boot immer näher zu kommen. Marlin und Nemo geraten in einen Streit, und Nemo schwimmt wütend auf das Boot zu. Er berührt es trotzig und wird plötzlich von einem Taucher gefangen, der mit ihm im Boot verschwindet und davonfährt. Panisch versucht Marlin, dem Boot nachzuschwimmen, verliert es jedoch schon bald aus den Augen.\n\nDer verzweifelte Marlin trifft auf die unter einem Verlust des Kurzzeitgedächtnisses leidende Dorie. Als er sie loswerden will, werden die beiden von einem Hai namens Bruce abgefangen, der sie auf eine Party einlädt. Dort erfährt Marlin, dass Bruce und die Haie Hammer und Hart eine Art Vegetarier-Selbsthilfegruppe gegründet haben, in der sie ihre Neigung, andere Fische zu fressen, überwinden wollen. Mitten in diesem Treffen entdeckt Bruce seinen Appetit auf Fleisch wieder, weil er Blut gewittert hat. Nun hat er es auf Marlin und Dorie abgesehen, sodass die beiden panisch das Weite suchen. Während der Flucht entdeckt Marlin die Tauchmaske von Nemos Entführer, auf der die Adresse des Besitzers „P. Sherman, 42 Wallaby Way, Sydney“ vermerkt ist.\n\nVon nun an ist das Ziel klar, Marlin muss nach Sydney, wo sich Nemo aller Wahrscheinlichkeit nach aufhält. Beim Abtauchen nach der Maske sind Marlin und Dorie jedoch immer tiefer geschwommen, und plötzlich taucht ein großer Tiefsee-Anglerfisch auf, dem sie in einer erneuten Flucht entkommen. Die beiden erfahren von einem Fischschwarm, dass sie nach Sydney dem „OAS“, dem Ostaustralstrom, folgen müssen.\n\nNemo hat währenddessen sein neues Zuhause erreicht. In der Praxis eines Zahnarztes ist er in einem Aquarium gelandet, wo er Freundschaft mit den anderen Fischen schließt. Bald erfahren sie, dass Nemo als Geschenk für Darla gedacht ist, eine Nichte des Zahnarztes, die als Fischmörderin gilt. Verzweifelt suchen die Aquariumsbewohner nach einer Möglichkeit, Nemo vor seinem Schicksal zu retten. Der Halfterfisch Khan nimmt die Fluchtpläne zum Anlass, auch die anderen Bewohner retten zu wollen und erklärt Nemo, dass ihre einzige Chance darin bestehe, den Filter des Aquariums mit einem kleinen Steinchen zu blockieren, damit das Aquarium verschmutze und der Zahnarzt sie alle in kleine Wasserbeutelchen packen müsse. Dann sollten sie sich aus dem Fenster und über die Straße bis ins nahegelegene Meer rollen.\n\nDer erste Fluchtversuch scheitert, Nemo entgeht in der Röhre des Aquariumfilters nur knapp dem Tod. Beim zweiten Mal gelingt der Plan zunächst und das Aquarium verschmutzt zusehends. Als die Fische aber am nächsten Morgen aufwachen, ist das Aquarium wieder sauber. Der Zahnarzt hatte in der Nacht ein Selbstreinigungssystem installiert, und der Fluchtplan ist erneut gescheitert.\n\nMarlin und Dorie haben dagegen im Ozean Glück im Unglück. Sie kämpfen sich durch ein Feld gefährlicher Quallen, an dessen Ende sie direkt auf den OAS treffen, wo sie in einer Kolonie von Meerschildkröten aufwachen, wo sie von Crush und seinem Sohn Racker aufgenommen werden. Hier beginnt Marlin zum ersten Mal zu verstehen, dass man statt immer nur vorsichtig zu sein auch gelegentlich Risiken auf sich nehmen muss. Durch die Unterstützung der Schildkröten kommen sie mit der Meeresströmung gut voran und geraten kurz vor Sydney in das Innere eines großen Wals, der sie bis an die Küste bringt. Mit Hilfe des Pelikans Niels gelangen sie rechtzeitig in die Praxis des Zahnarztes, in der Nemo bereits aus dem Aquarium genommen wurde, um bald an Darla übergeben zu werden.\n\nAls Darla schließlich eintrifft, stellt sich Nemo tot, um ins Klo gespült zu werden, das ihn über die Abwasserleitung in die Freiheit gebracht hätte, aber der Zahnarzt will ihn im Müll entsorgen. In diesem Moment stürmt Pelikan Niels herein und versucht, Nemo zu retten. Marlin erblickt seinen sich tot stellenden Sohn und glaubt, alles sei umsonst gewesen. Der Arzt schafft es schließlich, Niels mit Marlin und Dorie im Schnabel aus dem Fenster zu scheuchen. In diesem Augenblick katapultiert sich Khan mit Hilfe des Aquarium-Vulkans aus dem Aquarium. Er landet auf dem Zahnarztbesteck und schafft es, Nemo mit einem Mundspiegel in das Spülbecken zu bugsieren. Nemo schwimmt den Abfluss hinab und ist letztlich gerettet, da alle Abflüsse im Meer enden. Khan wird vom Zahnarzt wieder in das Aquarium gesetzt.\n\nZur selben Zeit setzt Pelikan Niels Marlin und Dorie wieder im Meer ab. Die Stimmung ist getrübt. Marlin trennt sich von Dorie. Er schwimmt mit einem Schwarm anderer Fische zurück in Richtung Heimat. Kurz darauf kommt Nemo aus einem Abflussrohr ins offene Meer, wo er auf Dorie trifft, die sich an Marlin nicht mehr zu erinnern scheint. Ahnungslos suchen die beiden nach Nemos Vater. Durch Zufall liest Dorie das Wort „Sydney“ auf dem Abwasserrohr, und da fällt ihr alles wieder ein. Sie erzählt Nemo aufgeregt, dass Marlin noch in der Nähe sein müsse, und tatsächlich gibt es kurz darauf ein Wiedersehen zwischen Marlin und Nemo.\n\nAber die drei geraten in einen Fischschwarm, der geradewegs in ein Fischernetz schwimmt. Mit einer List können sich die Fische aus dem Netz befreien. Doch Nemo hat dabei einen Schlag abbekommen und liegt nun reglos am Meeresboden. Für einen Moment denkt Marlin, er habe seinen Sohn endgültig verloren, erkennt dann aber, dass Nemo lebt. Marlin, Nemo und Dorie schwimmen nach Hause zu ihrem Korallenriff. Nachdem sie nach Hause zurückgekehrt sind, geht Nemo mit Crushs Sohn Racker und seinen Freunden begleitet vom Rochen zur Schule, und Marlin und Dorie sehen zu. Marlin ist fortan nicht mehr so besorgt um Nemo, und dieser ist geduldiger mit seinem Vater.\n\nAm Ende des Films gelingt auch den anderen Fischen die Flucht aus dem Aquarium des Zahnarztes. Zwar gelangen sie ins Hafenbecken, stellen dort aber fest, dass sie nicht bedacht haben, wie sie aus den Plastiktüten wieder herauskommen.\n\nNach den erfolgreichen Pixar-Filmen \"Toy Story\" und \"Monster AG\" sollte \"Findet Nemo\" das Ganze noch überbieten. Bereits dreieinhalb Jahre vor der Veröffentlichung des Films im Mai 2003 begann das Team mit ersten Storyentwürfen. Fast die Hälfte der gesamten Produktionszeit wurde dafür aufgewendet, die Geschichte bis ins kleinste Detail auszutüfteln und in einem animierten Story Reel immer weiter zu verfeinern.\n\nDass die Handlung bei diesem Animationsfilm im Meer angelegt ist, stellte sich als eine bis dahin unbekannte CGI-Herausforderung heraus. John Lasseter, der Produzent des Films, wusste um diese Schwierigkeit und ordnete intensive Recherchen an. Er schickte seine gesamte Crew in den Tauchurlaub, um als Vorbereitung auf die Animationsarbeiten Video- und Fotomaterial im Riff zu sammeln. Des Weiteren reisten die Macher nach Sydney, denn die im Film vorkommende Stadt sollte sehr realitätsnah dargestellt werden, so dass der Zuschauer gleich mit ihr vertraut sein würde. Auf der Tagesordnung standen außerdem Museumsbesuche, um die dort ausgestellten Fischexemplare genauestens unter die Lupe zu nehmen. Zu weiteren Studien wurde außerdem im Studio in Kalifornien ein Aquarium mit allen im Film relevanten Zierfischen eingerichtet. So sollte vor allem die Bewegung, aber auch das Verhalten der Fische untersucht werden.\n\nDie ersten Gehversuche am Computer erwiesen sich jedoch als schwierig. Das Problem war die Unterwasserwelt mit ihrer eigenen Optik wirklichkeitsgetreu darzustellen, erste Sequenzen erinnerten eher an milchigen Nebel. Zufriedenstellende Ergebnisse wurden erst erreicht, als das Meer durch viele Details wie die im Wasser schwimmenden Partikel, zahlreiche Lichtüberlagerungen und die einzelnen Strömungen in seiner ganzen Komplexität nachempfunden wurde. Nun sahen die Sequenzen derart realistisch aus, dass der Look ein wenig zurückgefahren werden musste, um den Vorstellungen einer Fantasiewelt, die es bei aller Genauigkeit immer noch sein sollte, zu entsprechen. Um die verschiedenen Wasserkonsistenzen darzustellen, wurden im Film verschiedene Unterwasserfarben verwendet. Als Nemo zu seinem ersten Schultag aufbricht, ist das Wasser in einem kristallklaren Türkisgrün gehalten, das im weiteren Verlauf des Films immer dunkler wird. Die Farbe ändert sich von Schwarz über Blau immer mehr in das Hafengrün Sydneys, je weiter die Handlung voranschreitet.\n\nEine weitere Herausforderung war die Darstellung der Fische. Wie wirken Fische mit Sprache, Emotionen und menschenähnlichen Gesten glaubhaft? Als optimale Lösung erwies sich ein Mittelweg. Die Figuren erhielten einerseits sehr viel Augenbrauenmasse, mit der sie jede erdenkliche Emotion ausdrücken können, andererseits wurden erste Gehversuche, bei denen die Fische ihre Flossen so einsetzten wie Menschen ihre Hände, verworfen und auf das Notwendigste zurückgefahren. Die Crew stellte außerdem fest, dass sie mit seitlichen Augen nicht viel anfangen konnte und stattete stattdessen alle Figuren mit einer Augenpartie vorne aus. Neue Überlegungen mussten zudem in der Animation der Bewegungen angestrengt werden. Die Pixar-Schmiede hatte bereits langjährige Erfahrung mit zweibeinigen \"(Toy Story)\", insektenfüßigen \"(Das große Krabbeln)\" und monsterartigen \"(Monster AG)\" Figuren, doch die seichte Bewegung von Fischen im schwerelosen Wasser erwies sich zunächst als schwierig. Erste Erfolge brachten Animationen, die vorhandenem Videomaterial exakt nachempfunden wurden.\n\nDas Konzept ging auf, doch durch die Perfektionierung der Handlung, die intensiven Recherchen und die unvorstellbare Detaildichte bei Fischen, Schauplätzen und Meerespflanzen wurde das 90 Millionen US-Dollar umfassende Budget um vier Millionen US-Dollar überzogen. Nicht eingehalten wurde auch der Starttermin, der ursprünglich für 2002 vorgesehen war.\n\nBei der Produktion ihrer Filme bedienen sich die Pixar Studios zumeist verschiedener Anspielungen, so auch bei der Namensgebung ihrer Figuren. Es ist z. B. denkbar, dass der Name \"Nemo\" der Figur \"Kapitän Nemo\" aus Jules Vernes Roman \"20.000 Meilen unter dem Meer\" nachempfunden ist. Möglich ist ferner, dass der auch als Anemonenfisch bekannte Clownfisch durch das in dieser Bezeichnung enthaltene „nemo“ sein eigenes Namensvorbild war. Sicher ist dagegen, dass Darla, die Nichte des Zahnarztes, ihren Namen in Anspielung auf die Pixarproduzentin Darla K. Anderson bekam. Bruce wurde hingegen eventuell in Anlehnung an den gleichnamigen mechanischen Hai benannt, den Steven Spielberg für seinen Kassenschlager \"Der weiße Hai\" (1975) verwendete.\n\nIm englischen Original heißt der Aquariumsfisch mit dissoziativer Identitätsstörung Deb und Flo. Diese Namen sind zum einen übliche englische Kurzformen von Deborah und Florence, typischen Vertretern britischer älterer Damen. Gleichzeitig spielen die Kurzformen lautmalerisch auf die englischen Wörter für Ebbe und Flut, \"ebb\" und \"flow\", an. Für Nemos Vater Marlin stand möglicherweise die Familie der Marline Pate. Die beiden Schildkröten heißen Crush und Squirt. Sie wurden offensichtlich nach zwei gleichnamigen Zitronenbrausen-Marken in den USA benannt.\n\nIn der deutschen Synchronfassung wurden viele der Rollennamen ersetzt. Die Schreibweise der Hauptrolle „Dory“ wurde in „Dorie“ angepasst. Im Aquarium des Zahnarztes gibt es in der deutschen Synchronversion die Figur Lee & Luv. Diese Namensgebung geschah wohl in Anspielung auf die beiden Begriffe aus der Seemannssprache, Luv und Lee. Die Baby-Schildkröte Squirt heißt im Deutschen Racker. Eine weitergehende Übersicht der Unterschiede ist in der Liste der Synchronsprecher zu finden.\n\nFür die Filmmusik von \"Findet Nemo\" war zunächst Danny Elfman im Gespräch, der jedoch ebenso wie Hans Zimmer absagte. Produzent John Lasseter verpflichtete schließlich Thomas Newman, den Cousin von Randy Newman, der bis dahin die Musik für alle Pixar-Filme komponiert hatte.\n\nThomas Newman, der sonst eher für Erwachsenen-Sujets wie \"In the Bedroom\" (2001) oder \"The Salton Sea\" (2002) engagiert wurde, steuerte hier erstmals die Musik zu einem Animationsfilm bei. Sein 39 Stücke umfassender Soundtrack ergänzte Robbie Williams mit dem ursprünglich von Bobby Darin gesungenen Song \"Beyond the Sea\", den Williams aus seinem bereits 2001 veröffentlichten Album \"Swing When You’re Winning\" auskoppelte.\n\nFür die Synchronisation war die FFS Film- & Fernseh-Synchron GmbH in München verantwortlich. Peter Stein schrieb das Dialogbuch, Dialogregie führte Frank Lenart.\nIm Abspann hat \"Mike\" aus dem Pixar-Film \"Die Monster AG\" einen Cameo-Auftritt, als er durch das Bild schwimmt. Aber auch Nemo hat einen Kurzauftritt in \"Die Monster AG\". Als Sulley das Kind am Schluss des Filmes wieder in sein Zimmer bringt und Abschied nimmt, liegt auf dem Fußboden ein Nemo-Kuscheltier.\n\nIn der Sequenz, in der Khan seinen Fluchtplan erläutert, fährt auf der Straße ein \"Pizza-Planet-Truck\" vorbei, der erstmals in \"Toy Story\" vorkam und in fast jedem Pixar-Film zu sehen ist. Der Wagen erscheint ein weiteres Mal gegen Ende des Films, gefolgt von einem gelben Fiat Nuova 500, wie er in Form des Charakters \"Luigi\" in \"Cars\" auftritt.\n\n\"Findet Nemo\" ist der erste Film der Pixar Animation Studios, in dem es – anders als bei Vorgängern wie \"Das große Krabbeln\" oder \"Monster AG\" – nach dem Abspann keine animierten Outtakes mehr gibt.\n\n\"Findet Nemo\" ist dem am 29. Oktober 2002 an schwarzem Hautkrebs gestorbenen Glenn John McQueen gewidmet. Er war ein Animator bei Pixar und leitender Mitarbeiter für \"Toy Story\", \"Das große Krabbeln\", \"Toy Story 2\" und \"Die Monster AG\". Als Nemo aus dem Aquarium sieht und in das Wartezimmer des Zahnarztes schaut, sieht man in der Ecke einige Spielzeuge aus „Toy Story“, besonders gut zu erkennen ist Buzz Lightyear.\n\nUrsprünglich wurde der Film am 30. Mai 2003 veröffentlicht. Die VHS und DVD kamen am 4. November 2003 heraus. Nach dem Erfolg der 3D-Wiederveröffentlichung von \"Der König der Löwen\" kündigten Disney und Pixar für den 14. September 2012 auch eine 3D-Wiederveröffentlichung von \"Findet Nemo\" an.\nIn Deutschland und Österreich hingegen feierte der Film am 14. Februar 2013 seine Rückkehr auf die Kino-Leinwand.\n\nDie deutsche Blu-ray wurde erstmals am 7. März 2013 veröffentlicht. Ausgewertet wird der Film sowohl in 2D als auch 3D. Ebenso erschien der Film erneut auf DVD.\n\nMit seinem Budget von 94 Millionen statt der ursprünglich geplanten 90 Millionen US-Dollar (etwa 79,8 Millionen Euro gegenüber 76,4 Millionen Euro, nicht inflationsbereinigt) weckte der Film von vornherein immense Erwartungen, die er größtenteils auch erfüllte. Er spielte allein an seinem Startwochenende in den Vereinigten Staaten 70 Millionen US-Dollar (59,4 Millionen Euro) ein. Das war zuvor noch keinem anderen Animationsfilm gelungen. Insgesamt brachte \"Findet Nemo\" rund 867,9 Millionen US-Dollar ein und wurde zum bis dahin finanziell erfolgreichsten Walt-Disney-Film, noch vor \"Der König der Löwen\" (siehe auch Liste erfolgreicher Filme). Durch die Wiederveröffentlichung des Films als 3D-Version im Jahr 2012 liegt das weltweite Einspielergebnis bei 936,7 Millionen US-Dollar, was ihn nach Toy Story 3 zum zweiterfolgreichsten Pixar-Film macht.\n\nIn der Liste der weltweit erfolgreichsten Filme aller Zeiten belegt \"Findet Nemo\" derzeit Platz .\n\nDer Erfolg des Films setzte sich beim DVD-Verkauf fort. Aufgrund der enormen Beliebtheit der Figuren und durch geschicktes Marketing erzielte der Film mit 28 Millionen Kopien noch einmal rund 950 Millionen US-Dollar Gewinn, was vor ihm noch keine andere DVD-Veröffentlichung geschafft hatte.\n\nAuch die Hörspielfassung des Films verkaufte sich erfolgreich. So belegte die Hörspielkassette mit rund 1 Mio. verkauften Exemplaren Rang 25 der meistverkauften Kassetten einer Hörspielreihe/eines Interpreten.\n\nIm deutschen Free-TV wurde \"Findet Nemo\" erstmals am Sonntag, dem 4. März 2007 im Programm des Fernsehsenders ProSieben gezeigt. Die Erstausstrahlung wurde von durchschnittlich 4,97 Millionen Zuschauern verfolgt, was einem Marktanteil von 13,5 % entsprach.\n\n2016 belegte \"Findet Nemo\" bei einer Umfrage der BBC zu den 100 bedeutendsten Filmen des 21. Jahrhunderts den 96. Platz.\n\nDer film-dienst empfand den Film als „stimmig bis in kleinste Einzelheiten, bewegend durch die nuancierte individuelle Charakterisierung und höchst amüsant“. „Man kann sich gar nicht sattsehen“, urteilte Die Zeit. Focus sah „ein Fest für die Augen“.\n\n„Die Pixar-Animationsstudios beweisen mit »Finding Nemo« erneut ihr Feingespür für gleichzeitig berührende wie humorvolle Trickgeschichten. Die auch sinnbildliche Suche des Vaters nach seinem Sohn ist liebevoll inszeniert, mit subtilen (Film-)Anspielungen gewürzt und dank feiner Figurenzeichnung auch ein geistiger Genuss. »Finding Nemo« ist vielleicht weniger komödienlastig als seine Vorgänger, dafür umso liebenswerter in seiner Geschichte“, urteilte der Filmspiegel.\n\nDer prominente filmische Einsatz von Clownfischen regte vor allem Kinder in den Vereinigten Staaten dazu an, sich einen Clownfisch als Haustier zu wünschen, obwohl die Haltung derartiger Fische im Film als kompliziert und teuer dargestellt wird. Um die stark gestiegene Nachfrage zu befriedigen, wurde in Vanuatu 2004 die Fangquote für Clownfische erhöht.\n\nGleichzeitig wird im Film behauptet, alle Abflüsse führten ins Meer. So entgeht Nemo seiner Gefangenschaft im Aquarium, indem er durch einen Abfluss das Meer erreicht. Da Abwasser üblicherweise einer Behandlung unterzogen wird, bevor es in den Wasserkreislauf zurückgegeben wird, stichelte die Firma JWC Environmental, ein passenderer Titel für den Film sei \"Grinding Nemo\", zu deutsch etwa \"Nemo mahlen\". Besonders Kinder initiierten die vermeintliche Befreiung ihrer Zierfische durch das Aussetzen im Abfluss, was für die meisten Tiere im sicheren Tod endete. In Sydney selbst hingegen endet die Kanalisation tatsächlich im offenen Meer, ohne dabei eine nennenswerte Behandlung erfahren zu haben, wenn man von einigen Filter- und Pumpprozeduren absieht.\n\nDer Film verursachte außerdem einen erheblichen Anstieg des Tourismus im Sommer und Herbst 2003 in Australien. Die Urlauber besuchten vor allem die im Film vorkommende Ostküste. Daraufhin startete die Australian Tourism Commission verschiedene Marketingkampagnen in China und den Vereinigten Staaten, um den Tourismus in Australien weiter anzukurbeln. Auch Queensland in Australien benutzte \"Findet Nemo\" als Promotion für seine Attraktivität als Urlaubsgegend.\n\nDer französische Kinderbuchautor Franck Le Calvez warf Disney vor, Handlung und Figuren seinem Buch \"Pierrot Le Poisson-Clown\" entnommen zu haben. Le Calvez hatte seine Idee 1995 schützen lassen und das Buch im November 2002 veröffentlicht. Mit anwaltlicher Hilfe versuchte Le Calvez einen Anteil der in Frankreich erreichten Merchandising-Einnahmen zu erklagen und ging dabei bis vor den französischen Gerichtshof. Er verlor die Klage am 12. März 2004, ging jedoch am 5. Oktober desselben Jahres in Berufung. Laut Urteilsbegründung soll Pixar die Idee zum Film bereits 2000 fixiert haben. Zudem ließen sich die Ähnlichkeiten der Figuren nicht als Argument anbringen, da z. B. bei der Figur eines Clownfisches nicht die nötige Schöpfungshöhe erreicht werde. Franck Le Calvez und sein Verlag Flaven Scene mussten einen fünfstelligen Betrag als Schadensersatz zahlen und die Gerichtskosten übernehmen.\n\nIm Jahr 2005 kam es zwischen Michael Eisner von Disney und Steve Jobs von Pixar zu Meinungsverschiedenheiten bezüglich des Vertriebs der Pixar-Filme. Im Zuge dessen gab Disney bekannt, dass mit Circle 7 Animation ein eigens geschaffenes Animationsstudio die Fortsetzungen der Disney-eigenen Pixar-Filme erstellen sollte (betroffen gewesen wären die Filme zwischen 1995 und 2006).\nDas Studio begann, \"Toy Story 3\" und \"Die Monster AG 2\" zu entwickeln; weiterhin wurde die Laurie Craig beauftragt, ein Drehbuch für \"Findet Nemo 2\" zu entwerfen. Nachdem Eisner von Robert Iger als CEO bei Disney abgelöst wurde und der Kauf von Pixar durch Disney arrangiert worden war, wurde Circle 7 geschlossen.\n\nIm Juli 2012 wurde berichtet, dass Andrew Stanton an einer Fortsetzung unter dem Titel \"Finding Dory\" arbeite, wobei Victoria Strouse das Drehbuch schreibe und der Kinostarttermin für 2016 geplant sei. Im April 2013 bestätigte Disney die Fortsetzung sowie dass Ellen DeGeneres und Albert Brooks ihre Rollen als Dorie bzw. Marlin noch einmal sprechen würden. Der Film sollte zunächst am 25. November 2015 ins Kino kommen. Am 18. September 2013 gab Pixar jedoch bekannt, dass der Kinostart der Fortsetzung auf 17. Juni 2016 in den amerikanischen Kinos verschoben sei, um so der Produktion mehr Zeit zu geben. Der ursprüngliche Starttermin am 25. November 2015 wurde durch den Pixar-Film \"Arlo & Spot\" besetzt. Der Film wurde schließlich am 17. Juni in den US-amerikanischen Kinos veröffentlicht.\n\n"}
{"id": "172095", "url": "https://de.wikipedia.org/wiki?curid=172095", "title": "Revolution OS", "text": "Revolution OS\n\nRevolution OS ist ein Kino-Dokumentarfilm aus dem Jahr 2001, der die Geschichte von GNU/Linux, freier Software und der Open-Source-Bewegung erzählt. Der Film wurde vor Ort im Silicon Valley auf 35-mm-Film in Cinemascope gedreht und enthält zur Evolution von GNU/Linux geschnittene und um veranschaulichende Clips ergänzte Interviews – unter anderem mit Richard Stallman, Michael Tiemann, Linus Torvalds, Eric S. Raymond, Larry Augustin, Frank Hecker, Brian Behlendorf und Bruce Perens. Regie führte J.T.S. Moore.\n\nDer Film führt einen nahtlosen geschichtlichen Bogen über die Evolution von GNU/Linux, von den Anfängen – als Software auf Papierbändern zum Preis eines Biers kopiert wurde und Bill Gates in den 70ern anfing, proprietäre Programme in BASIC für von Computerhobbyisten verwendete Kleinstcomputer zu schreiben und diese in einem bitterlichen Brief aufforderte, Software zu kaufen statt zu tauschen – bis zu Richard Stallman und einer Beschreibung dessen, was ihn motivierte, seine Stelle am MIT aufzugeben und sein Leben fortan der Entwicklung freier Software zu widmen.\n\nMichael Tiemann erklärt in der Wüste, wie er von Stallman eine sehr frühe Version von dessen GNU C-Compiler bekam, und ihn weiterentwickelte.\n\nLarry Augustin beschreibt am Originalschauplatz, einem amerikanischen Universitätscampus, wie er sich mit dem von Stallman gegründeten GNU-Projekt und einem normalen Personal Computer eine leistungsfähige Unix-Workstation bauen konnte, die ihn ein Drittel des Preises einer Workstation von Sun Microsystems kostete, aber das Doppelte leistete, und wie daraus die Firma VA Linux wurde, deren Börsengang im Film ebenfalls lebhaft mitverfolgt wird.\n\nBrian Behlendorf, einer der ursprünglichen Entwickler des Apache Webservers – des am häufigsten im Internet eingesetzten Webservers – erzählt durch Clips sehr anschaulich unterstützt, wie er mit anderen Web-Entwicklern, die den damals meistverwendeten Web-Server NCSA httpd verwendeten, Patches (deutsch: Flicken) austauschte und es schließlich zu Apache, dem a-„patchy“ Webserver (auf deutsch: „Ein geflickter Webserver“) kam.\n\nDa die Filmaufnahmen und Interviews ausschließlich im Silicon Valley gemacht wurden, könnte bei Zuschauern das falsche Bild entstehen, dass Linux praktisch dort entwickelt wurde. Interviews mit Entwicklern in Europa fehlen, dennoch dokumentiert das Werk die wichtigsten Ereignisse in der Evolution von GNU/Linux.\n\nRevolution OS wurde auf verschiedenen Filmfestivals gezeigt und gewann zwei Mal den Preis als beste Dokumentation auf dem \"Savannah Film Festival\" (30. Oktober bis 3. November 2001) und dem \"Kudzu Film Festival\" (11. Oktober 2001).\n\nDer Film ist auf zwei DVDs erhältlich, wobei die zweite DVD ungeschnittene Versionen der Interviews enthält.\n\nDaten der DVD:\n\n"}
{"id": "172535", "url": "https://de.wikipedia.org/wiki?curid=172535", "title": "ArchiCAD", "text": "ArchiCAD\n\nArchicad ist eine CAD/BIM-Software für Architekten, die vom Unternehmen Graphisoft entwickelt wird. Graphisoft ist seit 2007 Teil der Nemetschek Group. Weltweit arbeiten über 200.000 Planer mit Archicad.\n\nEs handelt sich um eine Entwicklung für das Bauwesen. Grundlage ist das sogenannte BIM (Building Information Modeling), das früher bei Graphisoft \"Virtuelles Gebäude\" genannt wurde: es werden 3D-Modelldaten, Massen, Materialeigenschaften, Klassifizierungen und Eigenschaften gespeichert. Daraus können Planungen in allen Leistungsphasen der Architektur wie Werkplanung, gerenderte Modelle, Massenlisten, Wohnflächenberechnungen, Detailpläne, Stücklisten erzeugt werden oder über die IFC-Schnittstelle das 3D Gebäudemodell an ein Ausschreibungsprogramm übergeben werden.\n\nDie aktuelle Version (2018) lautet Archicad 22, unterstützte Betriebssysteme sind Microsoft Windows 10, Microsoft Windows 8, Apple Mac OS X auf Intel CPU. Das Dateiformat für Standardprojekte lautet \"PLN\". Weitere Archicad-Dateiformate sind \"PLA\" (Planarchive), \"TPL\" (Projektvorlagen), \"MOD\" (Planmodule) und \"PMK\" (PlotMaker-Dateien). Schnittstellen sind vorhanden für DXF-, DWG-, IFC- und DGN-Dateien.\n\nSeit Archicad 13 gehört der \"BIM Server\" zum Programm, seit 2018 heißt der BIM Server BIMcloud Basic und ist im Lieferumfang von Archicad enthalten. Die kostenpflichtige Variante heißt BIMcloud und bietet einen erweiterten Funktionsumfang. Mit der BIMcloud und Archicad kann der Nutzer mit Teamwork 2.0 arbeiten; dabei arbeiten mehrere Nutzer zeitgleich von verschiedenen Rechnern an einem Projekt, das auf der BIMcloud gehostet wird.\n\nZu Archicad sind verschiedene Zusatzprodukte erhältlich, die den Leistungsumfang erweitern.\n\nArchicad ist als kommerziell nutzbare Version, als Studentenversion oder als 30 Tage gültige Testversion verfügbar. Seriennummern für die kostenlose Studentenversion oder die Testversionen werden auf myArchicad.com nach der Registrierung bereitgestellt.\n\nDaten der Archicad-Studentenversion sind kompatibel mit der Vollversion, werden aber durch ein Wasserzeichen kenntlich gemacht. Ein mit der Studentenversion erstelltes Projekt behält das Wasserzeichen.\n\nIn der 30-Tage-Testversion können Daten gespeichert werden, die mit der Vollversion kompatibel sind. \n\nBisher erschienene Versionen:\n\n\n\n\n"}
{"id": "173587", "url": "https://de.wikipedia.org/wiki?curid=173587", "title": "Slax", "text": "Slax\n\nSlax (früher \"Slackware-Live-CD\") ist eine GNU/Linux-Distribution, die sich als Live-CD direkt von einer CD starten lässt. Das rund 200  MB große ISO-Abbild passt somit auf eine Mini-CD. Daneben existiert noch eine gleich große Slax-Version im TAR-Format, die man in wenigen Schritten auf einen USB-Stick installieren kann. Diese Slax-Version für USB-Sticks speichert – im Gegensatz zu den meisten anderen Live-USB-Systemen – alle Änderungen; sie verhält sich folglich wie ein normal installiertes Betriebssystem.\n\nSlax basierte ursprünglich auf Slackware, seit November 2017 auf Debian.\n\nSlax verwendet – anders als die Knoppix-basierten Live-Distributionen – ein modulares Konzept. Die Module beinhalten eine oder mehrere Programmpakete. Zum Slax-System können optionale Module hinzugeladen und die Funktionalität des Systems so erweitert werden. Dieses Konzept ermöglicht ein einfaches Anpassen an eigene Bedürfnisse. Erstellt werden die Module mit Hilfe der Skripte von Linux-Live.org, wobei Slax die erste Distribution war, die dieses Konzept verwendete; inzwischen verwenden es auch weitere Live-Distributionen.\n\n\n\n"}
{"id": "175514", "url": "https://de.wikipedia.org/wiki?curid=175514", "title": "Dynamic Data Exchange", "text": "Dynamic Data Exchange\n\nDynamic Data Exchange ( für dynamischer Datenaustausch, kurz DDE) ist ein Protokoll für den Datenaustausch zwischen verschiedenen Anwendungsprogrammen, also eine Interprozesskommunikation nach dem Client-Server-Modell.\n\nDieses Protokoll ist lokal und innerhalb von Rechnernetzen in den Betriebssystemen Windows (ab Version 2.0) und OS/2 verfügbar.\n\nDamit Daten ausgetauscht oder Befehle abgesetzt werden können, müssen beide Anwendungsprogramme gleichzeitig laufen. Falls nötig muss der DDE-Client den benötigten Server-Prozess starten. Der Datenaustausch erfolgt grundsätzlich in kompletten Dateneinheiten in Windows-Zwischenablage-Formaten (also auch Binärdaten); kontinuierliche Datenströme werden üblicherweise nicht unterstützt. Da zwei sich kennende Programme neue Zwischenablageformate definieren können, ist auch der Austausch von großen arbiträren Datenmengen, etwa Matrizen in Matlab, kein Problem. Als Besonderheit bietet DDE ein „Advise“ genanntes Hot-Tracking (Datenänderungsbenachrichtigung) an, mit dem der Client automatisch über Veränderungen des Server-Datenbestands informiert wird.\n\nTypisch für DDE und auch festgelegt ist die dreistufige Adressierung von Datenelementen, eingeteilt \"Server\" (Dienst), \"Topic\" (Thema) und \"Item\" (Element). Eine tiefergehende Adressierung, etwa bestimmte Zeilen und Spalten einer Tabellenkalkulationsseite, muss durch ein wahlfreies, nicht-aufzählbares \"Item\" erfolgen.\n\nImplementiert ist DDE durch Nachrichten über die Windows-typischen Thread-Warteschlangen sowie mittels gemeinsam genutztem Speicher für die eigentlichen Daten.\n\nDie Leistungsfähigkeit von DDE liegt grob bei 1000 Übertragungen pro Sekunde, wenn die Kommunikation über einen GUI-Thread erfolgt, und kann bei Verwendung gesonderter Threads deutlich höher liegen. Die mit Windows 3.1 eingeführten DDE-Funktionen, zunächst in der DDEML.DLL verpackt, sind praktisch nur Wrapper, die die Verwendung vereinfachen aber keinen Geschwindigkeitsvorteil bringen.\n\nDDE überwindet „Bitgrenzen“ relativ problemlos. So ist es möglich, dass ein 16-Bit- und ein 32-Bit-Windows-Programm kommunizieren, so auch ein 32-Bit- und ein 64-Bit-Programm. Eine Kommunikation zwischen 16-Bit und 64-Bit ist regulär nur mittels Netzwerk-DDE machbar, da die 64-Bit-Windows-Versionen keine 16-Bit-Unterstützung haben.\n\nDDE findet heute immer noch Verwendung, etwa zum Weiterleiten von Kommandozeilen an bereits laufende Prozesse, typischerweise an MDI-Anwendungen. So kann der Windows-Explorer bei geeignet eingestellten Verknüpfungen ein Dokument an einen bereits laufenden Bearbeitungsprozess „senden“, ohne eine unnötige Prozess-Kopie zu erstellen.\n\nAuf Grund seiner Einfachheit ist Netzwerk-DDE bei Maschinen- und Fabriksteuerungen verbreitet.\n\nDDE wurde durch das OLE-Protokoll ergänzt und erweitert.\n\nAls Alternative, insbesondere für kontinuierliche Datenströme, bieten sich Sockets an.\n"}
{"id": "177478", "url": "https://de.wikipedia.org/wiki?curid=177478", "title": "Deep Blue", "text": "Deep Blue\n\nDeep Blue war ein von IBM entwickelter Schachcomputer. Deep Blue gelang es 1996 als erstem Computer, den damals amtierenden Schachweltmeister Garri Kasparow in einer Partie mit regulären Zeitkontrollen zu schlagen. 1997 gewann Deep Blue gegen Kasparow einen ganzen Wettkampf aus sechs Partien unter Turnierbedingungen.\n\nErfinder des Projekts war Feng-hsiung Hsu. Er startete es 1985 mit der Entwicklung eines auf einem Chip integrierten Zuggenerators als \"ChipTest\" an der Carnegie Mellon University und gab dem fertigen System den Namen \"Deep Thought,\" nach dem gleichnamigen Computer im Roman \"Per Anhalter durch die Galaxis\" von Douglas Adams. 1989 trat Hsu dem Team von IBM bei und forschte mit Murray Campbell über Problemstellungen der Parallelrechnung. Aus dieser Arbeit entstand Deep Blue. Dieser Name leitete sich vom amerikanischen Spitznamen für den US-Konzern IBM ab, welcher aufgrund seiner großen Marktkapitalisierung und seines blauen Logos \"„Big Blue“\" genannt wurde.\n\nDas System bezog seine Spielstärke hauptsächlich aus seiner enormen Rechenleistung. Deep Blue war ein massiv paralleler, SP-basierter RS/6000-Rechner. Die Version von 1996 bestand aus 36 Knoten und 216 speziellen VLSI-Schachprozessoren, die Version von 1997 aus 30 Knoten mit 480 Chips. Jeder Knoten verfügte über 1 GB RAM und 4 GB Festplattenspeicher. Die Schachsoftware war in C geschrieben und lief unter dem Betriebssystem AIX 4.2. Sie berechnete je nach Stellungstyp zwischen 100 und 200 Millionen, im Durchschnitt 126 Millionen Stellungen pro Sekunde.\n\nSeine Bewertungsfunktion bestand aus der in Hardware ausgeführten umfangreichen Parameterauswertung und der in Software ausgeführten Gewichtung dieser Parameter (z. B.: wie wichtig ist die Königssicherheit im Vergleich mit einem Raumvorteil im Zentrum). Die optimalen Werte der Parameter wurden vom System selbst bestimmt, indem es Tausende von Meisterpartien analysierte. Vor dem zweiten Match wurde das Schachwissen des Programms von Großmeister Joel Benjamin optimiert. Das Eröffnungsbuch kam von den Großmeistern Miguel Illescas Córdoba, John Fedorowicz und Nick de Firmian.\n\nKasparow konnte das erste Match, das mit sechs Partien von 10. bis 17. Februar 1996 in Philadelphia stattfand, für sich entscheiden. Er gewann drei Partien, machte zwei Remis und verlor eine Partie, womit er Deep Blue 4:2 schlug. Berühmt wurde die erste Partie des Matches, die Deep Blue gewann. Der Wettkampf ging um einen Preisfonds von 500.000 US-Dollar und wurde live im Internet übertragen.\n\nAnschließend rüstete IBM seine Maschine mit stärkerer Hardware aus und trat im Mai 1997 erneut gegen Kasparow an. Deep Blue, der mittlerweile 200 Millionen Stellungen pro Sekunde berechnen konnte, gewann die Revanche 3,5:2,5. Damit war das System auch der erste Computer, der einen Wettkampf unter „Turnierbedingungen“ gegen einen amtierenden Schachweltmeister für sich entscheiden konnte.\n\nNachdem Kasparow die erste Matchpartie gewonnen hatte, kam es in der zweiten Partie zu einem bemerkenswerten Partieschluss: Kasparow gab in Remisstellung auf. In der Diagrammstellung ging er davon aus, dass er die Damen tauschen müsse und seine Stellung nach 45. … Db6xc6 46. d5xc6 hoffnungslos wäre. Es wurde nach dem Spiel analysiert, dass er nach 45. … Db6–e3 46. Dc6xd6 Tb8–e8 47. h4 ein Remis durch Dauerschach hätte erreichen können, allerdings wurde diese Analyse zugunsten von 47. Dd7+ Te7 48. Dc6 verworfen, welches mit einfacher zu spielender Stellung für Weiß in ein unklares Endspiel führt. Kasparow hatte allerdings diese Varianten nicht in Betracht gezogen und war nach dieser Niederlage psychisch angeschlagen.\n\nIn der sechsten und letzten Partie am 11. Mai 1997 brach Kasparow mit Schwarz völlig ein und musste eine der kürzesten Niederlagen seiner Karriere einstecken:\n\n1. e2–e4 c7–c6 2. d2–d4 d7–d5 3. Sb1–c3 d5xe4 4. Sc3xe4 Sb8–d7 5. Se4–g5 Sg8–f6 6. Lf1–d3 e7–e6 7. Sg1–f3 h7–h6?! 8. Sg5xe6! Dd8–e7 9. 0-0 f7xe6 10. Ld3–g6+ Ke8–d8 11. Lc1–f4 b7–b5 12. a2–a4 Lc8–b7 13. Tf1–e1 Sf6–d5 14. Lf4–g3 Kd8–c8 15. a4xb5 c6xb5 16. Dd1–d3 Lb7–c6 17. Lg6–f5 e6xf5 18. Te1xe7 Lf8xe7 19. c2–c4 1:0\n\nNach der Partie wurde diskutiert, ob es sich bei dem 7. Zug von Schwarz um einen Fingerfehler gehandelt habe, denn durch den Zug 7. … Lf8–d6 hätte Kasparow taktische Verwicklungen vermeiden und eine solide Stellung erreichen können. Nach dem Figurenopfer von Weiß, das der Computer in seinem Eröffnungsbuch gespeichert hatte und à tempo spielte, schien Kasparow überrascht zu sein. Joel Benjamin vermutet jedoch, dass es sich bei seinen Reaktionen um Schauspielerei handelte, denn kurz zuvor kam die Variante in einer Partie von Gennadij Timoščenko gegen das Schachprogramm Fritz vor, die Kasparow wahrscheinlich bekannt war. Im 11. Zug beging Kasparow den partieentscheidenden Fehler, besser wäre das auch von Timoščenko gespielte 11. … Sf6–d5 mit unklarer Stellung gewesen. Möglicherweise vertraute Kasparow darauf, dass Deep Blue ähnlich wie Fritz spielen würde, was jedoch nicht zutraf. 2018 erklärte Kasparow, dass er bewusst eine von ihm noch nie angewandte Variante wählte in der Erwartung, dass sie sich nicht im Eröffnungsbuch von Deep Blue befinden würde. Er wusste, dass 7. … h7-h6 ein schlechter Zug ist, ging jedoch davon aus, dass Deep Blue das Potential des Figurenopfers nicht korrekt berechnen könnte, da Computer damals nur Material opferten, wenn ein konkreter Gewinnweg berechnet werden konnte. Das Opfer ist jedoch spekulativer Natur: Weiß erhält für die Figur \"nur\" einen sehr starken Angriff. Kasparow erwartete daher, dass Deep Blue den angegriffenen Springer g5 nach e4 zurückziehen würde. Da das Springeropfer sofort erfolgte, wusste Kasparow, dass es sich entgegen seiner Erwartung im Eröffnungsbuch des Computers befand. Der schnelle Verlust der Partie basierte laut Kasparow weniger auf seiner schlechten Stellung, sondern darauf, dass er schockiert war, einen derartigen Fehler gemacht zu haben, was ihn aus dem Konzept brachte. Jahre später erfuhr er, dass er mit seinen Annahmen Recht hatte: Deep Blue konnte das Figurenopfer nicht korrekt berechnen und hätte selbst einen anderen Zug gemacht, wie die Konstrukteure zugaben. Am Morgen vor dem Spiel entschieden sie sich jedoch dazu, das Opfer ins Eröffnungsbuch aufzunehmen.\n\n2003 erschien über das Match ein Dokumentarfilm von Vikram Jayanti unter dem Titel \"Game Over: Kasparov and the Machine.\"\n\nDas Team von Deep Blue verfügte über eine vollständige Historie aller öffentlichen Partien Kasparows, deren Analysen in die Programmierung einflossen. Überdies waren Hardware und Programmierung von Deep Blue gegenüber dem ersten Wettkampf im Vorjahr erheblich verbessert worden; Kasparow stand dadurch de facto einem unbekannten Gegner gegenüber.\n\nDie Regeln boten den Programmierern zudem die Möglichkeit, das Programm zwischen den Partien zu modifizieren, was sie ausgiebig taten. Noch während des Wettkampfs wurden im Quellcode Fehler beseitigt und Verbesserungen vorgenommen, wodurch Kasparow letztlich nicht nur gegen die Maschine, sondern auch gegen das Deep-Blue-Team spielte, da dieses seinem System half.\n\nHsu begegnete diesem Einwand, indem er darauf hinwies, dass auch ein menschlicher Gegner aus den bereits gespielten Partien lerne und einmal gemachte Fehler in weiteren Partien, so weit es geht, vermeide; jedoch entspricht eine händische Änderung des Codes weder dem maschinellen Lernen im Sinne der Künstlichen Intelligenz noch dem natürlichen Lernen des Menschen. Sogar IBM widerspricht Hsus Argumentation und stimmt mit Kasparow überein, dass Deep Blue kein lernendes System sei, wie das Unternehmen auf seiner Website des Deep-Blue-Projekts angibt:\n\nUnd:\n\nNach dem verlorenen Match meinte Kasparow, in manchen Zügen der Maschine hohe (menschliche) Intelligenz und Kreativität beobachtet zu haben und vermutete, der Maschine sei während des Spiels von Menschen geholfen worden. Kasparow verlangte Revanche, aber IBM verweigerte dem Weltmeister, unter anderem aufgrund der Anschuldigungen, ein Rematch und zerlegte Deep Blue in seine Einzelteile. Das Projekt kostete IBM insgesamt etwa 5 Millionen US-Dollar. Teile von Deep Blue sind heute in der Smithsonian Institution in Washington, D.C. sowie im \"Computer History Museum\" im Silicon Valley zu sehen. 20 Jahre nach dem Wettkampf zog Kasparow seine Anschuldigungen zurück.\n\nIn seinem Buch \"Behind Deep Blue\" behauptet Feng-hsiung Hsu, er habe von IBM die Rechte an den von ihm geschaffenen Schachchips erworben, um bei Bedarf eine noch stärkere Maschine zu bauen und mit dieser Kasparows Rematch-Angebot anzunehmen, aber Kasparow verweigere sich nun einem Rematch.\n\n\n"}
{"id": "177591", "url": "https://de.wikipedia.org/wiki?curid=177591", "title": "Kudzu (Software)", "text": "Kudzu (Software)\n\nKudzu ist eine ursprünglich von Red Hat entwickelte Software für die automatische Hardware-Erkennung unter den Linux-Distributionen Red Hat Linux und Fedora, die u. a. auch von der Debian-basierten Distribution Knoppix genutzt wird.\n\nEine Alternative zu Kudzu ist das von Progeny entwickelte Discover, das u. a. von der Linux-Distribution Gnoppix verwendet wird.\n\nAb der Version 9 von Fedora und Version 6 von Red Hat Enterprise Linux übernimmt die HAL die Aufgaben von Kudzu. Die Entwicklung von Kudzu wurde eingestellt.\n\n"}
{"id": "177704", "url": "https://de.wikipedia.org/wiki?curid=177704", "title": "Robotron Z 1013", "text": "Robotron Z 1013\n\nDer Robotron Z 1013 ist ein auf dem U880-Mikroprozessor basierender Heimcomputer des VEB Robotron aus der Deutschen Demokratischen Republik, der ausschließlich als Bausatz mit vormontierten Baugruppen erhältlich war.\n\nDie Produktion des Computers wurde ab Mitte 1985 unter anderem mit dem Ziel geplant, bei der aufwendigen Fertigung integrierter Schaltkreise anfallende Ausschussware mit eingeschränkten Bauteiledaten dennoch sinnvoll einsetzen zu können. Wegen der zu erwartenden Beeinträchtigungen beim Betrieb sollte der damit bestückte Z 1013 ausschließlich als Hobbyanwendung für Privathaushalte dienen, womit zudem die staatlichen Forderungen nach neuartigen Konsumgütern erfüllt wurden. Für industrielle Anwendungen konzipierte man dagegen eine spezielle Variante mit regulären Bauteilen. Aufgrund der zusätzlich geforderten möglichst geringen Herstellungskosten wurde der Rechner als gehäuseloser Einplatinencomputer mit Folientastatur entwickelt, dessen einzelne Baugruppen vom Benutzer endzumontieren waren. Die Programmiersprache BASIC konnte zunächst nur nach dem Laden von einer Kompaktkassette genutzt werden.\n\nAb Mitte 1987 fertigte Robotron überarbeitete Ausführungen des Computers nur noch mit regulären Schaltkreisen und zusätzlichem Arbeitsspeicher. Weitere vorgenommene Anpassungen gewährleisteten zusammen mit den ab diesem Zeitpunkt ebenfalls ausgelieferten Erweiterungsbaugruppen eine verbesserte Kompatibilität mit den ebenfalls von Robotron produzierten Kleincomputern Z 9001, KC 85/1 und KC 87.\n\nZwischen Ende 1985 und Mitte 1990 wurden insgesamt etwa 25.000 Bausätze ausgeliefert.\n\nTrotz des Kalten Krieges und des damit verbundenen Hochtechnologie-Embargos CoCom gelang es 1984, mit Z 9001 und HC 900 in der DDR entwickelte Heimcomputer herzustellen. Die Geräte und ihr Zubehör waren wegen der kleinen Produktionsserien für den großen landesweiten Interessentenkreis jedoch nur schwer zu beschaffen und zudem sehr teuer. Demgegenüber fiel in der Mikroelektronikindustrie aufgrund unausgereifter Produktionsprozesse eine große Anzahl qualitativ minderwertiger Bauelemente an, die in aktuellen Computermodellen nicht einsetzbar waren, aber auch nicht völlig unbrauchbar schienen („Anfalltypen“). Die Verantwortlichen der Herstellerbetriebe forcierten daher die Entwicklung einfacher ausfalltoleranter Lern- und Hobbycomputersysteme, deren Konfiguration die Verwendung vieler dieser Produktionsabfälle erlaubte. Dadurch konnten zum einen die vor den Planungskommissionen zu rechtfertigenden Ausschussquoten bei den Schaltkreisproduzenten gesenkt und zum anderen die Versorgungslücke in der Heimcomputerproduktion zumindest teilweise geschlossen werden. Wie bei den günstigen Schaltkreisen sollte auch bei den restlichen Baugruppen nur das am leichtesten zu Beschaffende und Preiswerteste zum Einsatz kommen. Im Ergebnis wurden insgesamt drei Computerprojekte geplant: der LC 80 mit Taschenrechnertastatur und sechsstelliger Siebensegmentanzeige, der Polycomputer 880 mit achtstelliger Siebensegmentanzeige und der etwas komfortablere Z 1013 mit Bildschirmausgabe.\n\nDas von den Initiatoren des Z 1013 angedachte Konzept sah einen gehäuselosen Einplatinencomputer mit Folientastatur vor, dessen puristische Aufmachung bei einem veranschlagten Verkaufspreis von weniger als 1000 M für die Zielgruppe der bastelbegeisterten Elektronik-Amateure als adäquat erschien. Die Entwicklung und Produktion wurde Anfang 1984 dem in der DDR-Computerindustrie etablierten Leiterplattenhersteller VEB Robotron in Riesa übertragen.\n\nDie staatlichen Planungsvorgaben für die zumeist jungen Ingenieure und Mitarbeiter der entsprechenden Entwicklergruppe („Jugendforscherkollektiv“) des VEB Robotron in Riesa sahen einen erweiterungsfähigen Einplatinencomputer mit möglichst geringen Material- und Herstellungskosten vor. Zur Senkung der Produktionskosten wurde das Gerät als gehäuseloser Bausatz konzipiert, dessen vorgefertigte Baugruppen durch den Benutzer endzumontieren waren. Die üblicherweise in den DDR-Privathaushalten vorhandene Heimelektronik wie Fernseher und Kassettenrekorder musste durch den Rechner verwendet werden können.\n\nBei der Fertigung sollte ausschließlich auf Anfalltypen bewährter integrierter Schaltkreise aus DDR- bzw. RGW-Produktion zurückgegriffen werden. Die zu entwickelnden Geräte hatten zudem kompatibel zu den ebenfalls von Robotron produzierten Kleincomputern Z 9001 und KC 85/1 zu sein. Diese engen Vorgaben waren nur durch eine Systemarchitektur realisierbar, die auf dem preisgünstigen und einsatzerprobten 8-Bit-Mikroprozessor U880 basierte. Vollwertige Grafik und Anschlussmöglichkeiten für spezielle Peripheriegeräte fielen dem Kostendruck zum Opfer. Die Konzeption des Computers als modulares System sah jedoch die Möglichkeit des Ansteuerns zusätzlicher Peripheriegeräte und beispielsweise den Ausbau des Arbeitsspeichers durch ebenfalls bereitzustellende Erweiterungsmodule vor.\n\nDie Entwicklungsarbeiten begannen Mitte 1985. Der erste Prototyp mit einem Arbeitsspeicher von 16 Kilobyte (KB) und Folienflachtastatur wurde im Herbst 1985 den Verantwortlichen vorgestellt und nach dessen Abnahme mit den Planungen und Vorbereitungen für die Serienproduktion begonnen, die bis November 1985 andauerten. Aufgrund der zu verwendenden Anfallbauteile wurde der Systemtakt im Sinne einer höheren Zuverlässigkeit von den im Heimcomputerbereich üblichen 2,5 MHz auf nur 1 MHz gesenkt.\n\nDie erste Serie von 150 Bausätzen ging im November 1985 in Produktion. Für Privatpersonen waren diese Ende des Jahres ausschließlich nach Vorbestellungen im Fachgeschäft für Heimelektronik des VEB Robotron-Vertriebs Erfurt und in einem Ladengeschäft der volkseigenen Handelsorganisation (HO) in Riesa für 650 M erhältlich. Neben dem Bildausgabegerät und einem Kassettenrekorder war vom Benutzer ebenfalls ein entsprechend dimensioniertes Netzteil bereitzustellen und vor der Inbetriebnahme das Verbindungskabel für die Tastatur auf der Computerplatine anzulöten. Einem breiteren Publikum offiziell vorgestellt wurde der fortan \"Z 1013.01\" genannte Computer erstmals auf der Leipziger Frühjahrsmesse 1986.\n\nTrotz mehrstufiger umfangreicher Prüfungen und mehrtägiger Dauertests in der Produktion führten die verbauten Anfalltypen häufig zu Störungen und damit zu Reklamationen seitens der Benutzer. Die wirtschaftlichen Nachteile durch die aufwendigen Prüfverfahren und nachträgliche Reparaturen konnten durch den günstigen Preis der verwendeten mangelhaften Bauteile nicht länger aufgewogen werden und führten zu einem Umdenken bei den Verantwortlichen. Daraufhin wurde die Produktion ab Juli 1987 auf Verwendung von ohnehin im Preis gefallenen regulären Bauteilen („getypte Bauteile“) umgestellt und durch die damit erreichte höhere Ausfallsicherheit einige technische Veränderungen zur Steigerung der Leistungsfähigkeit vorgenommen.\n\nNeben der Versorgung des Heimcomputermarktes mit dem Z 1013.01 wurden bis 1987 auch einige wenige Bausatzexemplare mit der Bezeichnung \"Z 1013.12\" für den industriellen Bereich („gesellschaftliche Bedarfsträger“) gefertigt. Dabei fanden getypte Bauelemente, ein Systemtakt von 2 MHz, 1 KB Bildwiederholspeicher und als Arbeitsspeicher ein SRAM mit einer Kapazität von 1 KB Verwendung.\n\nDa sich die Unterschiede zwischen Anfalltypen und regulären Schaltkreisen lediglich auf deren Belastbarkeit beschränkten, konnte die 1987 beschlossene Umstellung der Bausätze auf ausschließlich reguläre Bauteile ohne größere Änderungen an der Platine und daher kostensparend vorgenommen werden. Neben der erhöhten Verlässlichkeit verfügt die ab 1987 produzierte Variante \"Z 1013.16\" zudem über einen höheren Systemtakt von 2 MHz, was einer Verdoppelung der Rechenleistung gleichkommt. Daneben wurde die Systemsoftware um entsprechende Programmbestandteile zum Gebrauch mit einer wesentlich komfortableren Blocktastatur mit 58 Tasten ergänzt. Benutzer der älteren Bausätze konnten nach Beschaffung der Bauteile und des modifizierten Betriebssystems ihre Systeme unter Zuhilfenahme eines Lötkolbens ebenfalls aufrüsten.\n\nEin wichtiger Aspekt der vorgenommenen Aufwertungen ist – abgesehen von der verbesserten Verlässlichkeit – bei entsprechender Aufrüstung von Arbeitsspeicher die Herstellung der weitestgehenden Kompatibilität des Z 1013.16 mit den Kleincomputern Z 9001, KC 85/1 und KC 87. Neben der damit verbundenen Nutzbarmachung weiterer Software standen zudem deren Erweiterungsmodule, beispielsweise zur Aufrüstung des Arbeitsspeichers, nun auch den Z-1013-Anwendern zur Verfügung.\n\nDurch die zwischenzeitliche Lockerung des CoCom-Embargos und damit fallenden Preisen kam Ende 1988 eine weitere modernisierte Variante der Z-1013-Baureihe hinzu. Dieser \"Z 1013.64\" mit 64 KB Arbeitsspeicher wurde bis zum Produktionsende Mitte 1990 hergestellt.\n\nIm Gegensatz zu den Rechnern der Robotron-KC-Baureihe war der Z 1013 auch für Privatanwender – allerdings nur nach Vorbestellung, langer Wartezeit und persönlicher Abholung nebst Einweisung in Erfurt oder Riesa – erhältlich. Ursächlich für den eingeschränkten Vertrieb war die Weigerung des staatlichen Handels, den Z-1013-Bausatz zu vertreiben, mit der Begründung, dass ein Bastelgerät bei der Bevölkerung auf wenig Interesse stoßen werde. Zwischen Ende 1985 und Mitte 1990 wurden zusammen insgesamt etwa 25.000 Bausätze aller Ausführungen ausgeliefert.\n\nDie einfache und überschaubare Architektur des Systems, umfangreiche Dokumentationen des Herstellers und nicht zuletzt die freie Verwendbarkeit der Systemsoftware ermöglichen den miniaturisierten Nachbau des Z 1013 mit heutigen technischen Mitteln bei gleichzeitig überschaubarem Aufwand. Eine solche moderne Realisierung erfolgte erstmals 2013 – wie bei anderen Heimcomputersystemen auch – als Implementierung auf einem programmierbaren Logikschaltkreis (FPGA) nebst Einbettungssystem. Die Nachbildung mittels FPGA-Technologie war zunächst lediglich als technische Machbarkeitsstudie gedacht, stellte jedoch im Nachhinein auch ihren praktischen Nutzen unter Beweis: Durch die Miniaturisierung und die Möglichkeit des Batteriebetriebs ist sie eine leicht verstaubare, zuverlässig arbeitende und transportable Alternative zur originalen schonenswerten Technik.\n\nDie von Robotron vollständig bestückte Platine enthält den Hauptprozessor (englisch \"Central Processing Unit\", kurz \"CPU\"), den Arbeits- und Festwertspeicher, die Bildschirmansteuerung und mehrere Peripherieanschlüsse. Zum Lieferumfang des Bausatzes gehörten neben der Hauptplatine im Format 215 mm × 230 mm die 80 mm × 160 mm messende Folienflachtastatur nebst Anschlusskabel, diverse Kleinteile und die Dokumentationen. Die zum Gerät erhältliche – separat zu erwerbende – technische Dokumentation des Herstellers umfasste beim Modell Z 1013.01 eine Bedienungsanleitung, drei Handbücher und vier Schaltpläne. Erläutert wurden sowohl grundlegende Schritte zur Inbetriebnahme und Benutzung des Z 1013 als auch Details der Hard- und Software.\nDie Systemarchitektur basiert auf dem U880-Mikroprozessor, der in fast allen zeitgenössischen DDR-Computern eingesetzt wurde. Dieser nicht autorisierte Nachbau des Z80-Mikroprozessors von Zilog kann auf einen Adressraum von 65.536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobyte (KB) festlegt. Beim erstproduzierten Z 1013.01 kam eine mit nur 1 MHz getaktete Variante des U880 mit eingeschränkten Bauelementedaten („Anfalltyp“) zum Einsatz. Bei den übrigen Modellen war ein vollwertiger und mit 2 MHz getakteter U880-Mikroprozessor verbaut. Aus praktischen Gründen ist es üblich, für Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Dieser wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65.535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer von der CPU ansprechbare Adressraum ist segmentiert in Bereiche für die Systemsoftware, frei verwendbaren Arbeitsspeicher, steckbare Erweiterungen und den Grafikspeicher. Die häufig einfach als \"Maschinensprachemonitor\" bezeichnete Systemsoftware ist je nach Computerversion auf entweder 2 KB oder 4 KB großen ROM-Bausteinen untergebracht, deren Speicheradressen von $F000 bis $F7FF bzw. $FFFF reichen. Zum Zwischenspeichern nutzt die Systemsoftware nach Einschalten des Computers zusätzlich den Anfangsbereich des Arbeitsspeichers von $0000 bis $0100, so dass dieser für den Benutzer nicht ohne weiteres zur freien Verfügung steht.\n\nDie Adressen des frei verwendbaren Arbeitsspeichers von nahezu 16 KB RAM reichen beim Z 1013.01 und Z 1013.16 von $0100 bis $3FFF. Ist das System um zwei RAM-Module mit je 16 KB RAM erweitert, wird dieser Speicherbereich bis zur Adresse $E000 ausgedehnt. Beim mit 64 KB RAM ausgeliefertem Z 1013.64 reicht er bis zum für die Bilddarstellung benötigten Videospeicher, der sich ab $EC00 anschließt und bis $EFFF erstreckt. Zur Programmierung des Computers sind die gewünschten Sprachen wie zum Beispiel das 3 KB umfassende \"Tiny BASIC\" oder das mit 10 KB wesentlich umfangreichere \"Kleincomputer BASIC\" des KC 87 zuvor von Kassette in den Arbeitsspeicher zu laden. In der Grundausstattung mit 16 KB RAM stehen damit zum Beispiel nach dem Laden von Tiny BASIC etwa 12 KB RAM zum Erstellen von BASIC-Programmen zur Verfügung, so dass sich für umfangreichere Programmierprojekte eine Speicheraufrüstung empfiehlt. Befindet sich die Programmiersprache dagegen auf einem Steckmodul, so bleibt der verwendbare Arbeitsspeicher beim Z 1013.01 und Z 1013.16 davon unberührt.\n\nZur Ausgabe von Grafik beinhalten die Computer ab Werk lediglich einen Zeichengenerator, der Text beziehungsweise Grafiksymbole („Quasigrafik“) mit 32 × 32 Zeichen à 8 × 8 Bildpunkten darstellen kann. Der dazu benötigte, im Festwertspeicher befindliche feste Zeichensatz enthält 96 alphanumerische und Steuerzeichen sowie 146 grafische Symbole. Ein Pixelgrafikmodus („Vollgrafik“) steht standardmäßig nicht zur Verfügung, kann aber im Selbstbau ergänzt werden. Entsprechende Anleitungen wurden in verschiedensten Zeitschriften und Büchern bis in die frühen 1990er Jahre hinein publiziert. Die Schwarzweiß-Bildausgabe erfolgt über den koaxialen HF-Antennenanschluss an einem handelsüblichen Fernsehgerät; Umrüstungen auf Farbdarstellung sind möglich.\n\nZum Anschluss von Peripherie verfügen die Rechner über einen „User-Port“, der vom verbauten Ein- und Ausgabebaustein \"U855\" (englisch \"Parallel Input Output\" kurz \"PIO\") angesteuert wird. Der Betrieb der Tastatur und des anzuschließenden Kassettenrekorders erfolgt ebenfalls durch diesen elektronischen Schaltkreis. Im Falle des Z 1013.01 handelt es sich dabei – wie bei dessen CPU auch – um einen Anfalltypen. Für Erweiterungen beispielsweise des Arbeitsspeichers steht der herausgeführte Systembus mit seinem genormten Steckanschluss („Buserweiterung“) bereit.\n\nNeben den von Robotron vertriebenen Erweiterungen existieren weitere, die großteils in gedruckten Publikationen vorgestellt wurden. Im Folgenden sollten lediglich die bekanntesten vorgestellt werden.\n\nBei westlichen Heimcomputern der 1980er-Jahre kamen zur Datensicherung hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern immer häufiger auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die preisgünstigste Variante der Datenaufzeichnung durch Kompaktkassetten hat den Nachteil geringer Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren oder im Falle der DDR kaum erhältlich waren. Bei Erscheinen des Z 1013 standen zur Datenaufzeichnung lediglich Kassettenrekorder und Tonbandsysteme zur Verfügung. Anschlussmöglichkeiten für Diskettenlaufwerke kamen erst in der Nachwendezeit auf.\n\nDie Z-1013-Computer verfügen zur Speicherung von Daten über einen Anschluss für einen handelsüblichen Kassettenrekorder. Häufig zum Einsatz kamen dabei Geräte kleineren Ausmaßes, wie etwa die Typen \"Geracord\", \"Datacord\" und später \"LCR-C DATA\" des Herstellers VEB Elektronik Gera. Die maximale Datenübertragungsrate beträgt 1.200 Bit/s.\n\nEin Diskettensystem war von den Robotron-Entwicklern des Z 1013 aufgrund seiner niedrigen wirtschaftlichen Priorität nicht geplant, zumal entsprechende Ansteuerungselektronik bis 1987 teuer importiert werden musste. Mit dem Erscheinen des DDR-eigenen Schaltkreises \"U8272\", einem Nachbau des von Intel entwickelten Floppy-Disk-Controllers \"P8272A\", wurden zugleich auch Anregungen und grundlegende Vorgehensweisen zum Eigenbau von Diskettensystemen für die DDR-Heimcomputer publiziert. Der in der DDR vorherrschende Mangel insbesondere im Bereich der Laufwerksmechaniken machte deren Beschaffung und damit den Aufbau eines Diskettensystems für den volkswirtschaftlich unbedeutenden Z 1013 nahezu unmöglich, so dass Bauanleitungen erst in der Nachwendezeit veröffentlicht wurden.\n\nIn der Zeitschrift \"Funkamateur\" wurde Mitte 1992 eine einfache Möglichkeit zum Betrieb der Commodore-1541-II-Floppy mit dem Z 1013 vorgestellt. Pro Diskettenseite können damit 170 KB Daten gespeichert werden. Das Aufzeichnungsformat ist mit dem der Commodore-Rechner kompatibel, so dass die Daten beider Systeme ohne weiteres untereinander austauschbar sind.\n\nDie Computer sind aufgrund ihrer minimalistischen Hardwareausstattung lediglich für die Bearbeitung einfachster Aufgaben einsetzbar. Weitergehende Projekte und Anwendungen erfordern Aufrüstungen und Erweiterungen. Eine besondere Rolle spielt dabei der Baugruppenträger \"Z 1013.50\", der vier Erweiterungsschächte nebst entsprechender Ansteuerungselektronik zur Verfügung stellt. Einer dieser Schächte wird jedoch permanent vom Stromversorgungsmodul \"Z 1013.40\" belegt, denn das vom Hersteller für den Z 1013 empfohlene Netzteil ist nicht für den Betrieb zusätzlicher Baugruppen ausgelegt. Alle Schächte des Baugruppenträgers sind steckerkompatibel zu den Erweiterungen der Computer Z 9001, KC 85/1 und KC 87, wobei deren Module zur Verwendung häufig geringfügig modifiziert werden müssen. Soll der Baugruppenträger mit dem Computer Z 1013.64 betrieben werden, sind an beiden jeweils Änderungen vorzunehmen.\n\nZur Aufrüstung des Arbeitsspeichers empfiehlt der Hersteller den Einsatz von RAM-Modulen der Rechner Z 9001, KC 85/1 und KC 87. Daneben existiert ein frei bestückbares ROM-Modul von Robotron, auf dem bis zu vier EPROMs jeweils mit einer Speicherkapazität von 1, 2 oder 4 KB Platz finden. Bei beiden Erweiterungsmodulen muss der Adressbereich, in dem sie jeweils eingeblendet werden sollen, zuvor per DIP-Schalter eingestellt werden. Die Nachrüstung von zusätzlichen Ansteuerungsmöglichkeiten erfolgt mit dem Modul \"Z 1013.30\", das drei frei verwendbare Eingabe-/Ausgabeports und eine V.24-Schnittstelle zum Betrieb beispielsweise von Druckern bereitstellt.\n\nNeben den von Robotron vertriebenen Modulen existieren Lösungen von Dritten, die auch in größerer Stückzahl hergestellt wurden und häufig als Massenspeicherersatz in Form einer RAM-Disk dienten. Die am weitesten verbreitete Version stammt vom VEB Präcitronic und enthält 256 KB Arbeitsspeicher, wovon 64 KB als Hauptspeicher und 192 KB als umschaltbare Speicherbank dienen. Mit entsprechend modifizierter Systemsoftware kamen solche RAM-Disks häufig als Massenspeicherersatz zum Einsatz.\n\nDas Flachbandkabel der im Bausatz enthaltenen Folienflachtastatur musste vom Anwender vor dem Ersteinsatz an die Computerplatine gelötet werden. Die Tasten sind in 8-mal-4-Matrix alphabetisch angeordnet, wobei die Folienschalter wenig sensitiv sind und zum Prellen neigen. Ein effizientes Arbeiten ist nahezu unmöglich. Schon bald wurden durch viele Benutzer Alternativen gewünscht und auch entwickelt. Als Standardlösung zum Anbinden komfortablerer Tastaturen mit QWERTZ-Anordnung setzte sich der 1988 in der Zeitschrift \"Mikroprozessortechnik\" einem breiten Publikum vorgestellte \"Brosig-Monitor\" mit Bauanleitung für eine entsprechende Hardwareanbindung durch. Neben zusätzlichen Dienstprogrammen und der Abwärtskompatibilität zur Systemsoftware von Robotron bietet der 4 KB umfassende \"Brosig-Monitor\" zudem die Möglichkeit, Joysticks benutzen zu können.\n\nMit Hilfe einer separat erhältlichen Zusatzbaugruppe können beim Z 1013.64 auch Tastaturen mit 58 Tasten, wie sie mit den meisten DDR-Bürocomputern ausgeliefert wurden, betrieben werden.\nDer Verbesserung der grafischen Fähigkeiten wurden in Zeitschriften und Broschüren diverse Beiträge gewidmet. Diese enthalten Anleitungen zum Bau verschiedener Erweiterungen unterschiedlichen Umfangs und auch Hinweise zum käuflichen Erwerb bereits vorgefertigter Lösungen. Das Spektrum reicht dabei von verbesserten Zeichensatzlösungen (vom Computerclub Jena entwickelte Grafikkarte \"GDC\" mit 80 mal 25 Zeichen) über monochrome hochaufgelöste Pixelgrafik (256-mal-256-Lösung \"KRT-Grafik\" in \"Kleinstrechner Tips 11\" und 256-mal-192-Karte \"Spectrum-Grafik\" in der Zeitschrift \"Practic\") bis zu Vorschlägen von mehrfarbigen hochaufgelösten Varianten (384-mal-288-Karte \"VIS3\" mit 16 Farben von der Akademie der Wissenschaften). Robotron selbst bot keine derartigen Aufrüstungen an.\n\nBei der existierenden Software handelt es sich überwiegend um Eigenentwicklungen aus der DDR. Maschinennahe Portierungen von Programmen westlicher Z80-basierter Heimcomputersysteme waren aufgrund technischer Unterschiede in der Regel sehr aufwändig. Am einfachsten gestaltete sich der Programmaustausch und die entsprechende Anpassung von Software mit den Rechnern der Reihe Z 9001, KC 85/1 und KC 87.\n\nDie Verbreitung von Software sowie der Austausch von Erfahrungen erfolgten vor allem durch private Kontakte sowie über Zeitungsanzeigen, auf Messen, durch Abdruck von Programmen in Zeitschriften und durch Ausstrahlung im Rundfunk, wie beispielsweise in der Hörfunksendung \"Rem\". Von staatlicher Seite wurde die Erstellung von Software beispielsweise über die Gesellschaft für Sport und Technik (GST) mit ihrer Sektion \"Computersport\" gefördert. Häufig zählte die GST auch zu den Ausrichtern von Treffen und Tagungen.\n\nBeschränkungen der Weitergabe durch Urheberrechtsschutz oder Kopierschutzmechanismen existierten praktisch nicht. Vielmehr wurde die kostenlose Weitergabe von Software („Amateur-Software“) gefördert und auf entsprechenden Tagungen bestätigt. Für den Z 1013 wurden mehr als 500 Programme und Hardware-Erweiterungen erarbeitet und publiziert.\n\nZur Konfiguration der Computer-Hardware, zum Ansteuern der Kassettenschnittstelle sowie zum Eingeben und Auslesen von Speicheradressen dient das im Festwertspeicher der Geräte enthaltene Betriebssystem \"Monitorprogramm 2.02\" bei Z 1013.01, Z 1013.12 und Z 1013.16 beziehungsweise \"Monitorprogramm A.2\" bei Z 1013.64. Die Systemsoftware des Z 1013.64 ermöglicht den nachträglichen Anschluss einer komfortableren Tastatur mit 58 Tasten, im Gegensatz zu denen früherer Z-1013-Modelle, die maximal 32 Tasten umfassen.\n\nNeben Robotrons Monitorprogrammen existiert weitere Systemsoftware von Drittanbietern, die der Benutzer mit Hilfe von EPROMs aufrüsten kann. Hierbei hervorzuheben ist insbesondere der 1987 vorgestellte und nachfolgend weitverbreitete \"Brosig-Monitor\", der zur Systemsoftware von Robotron abwärtskompatibel ist. Neben nützlichen Dienstprogrammen bietet der 4 KB umfassende Brosig-Monitor unter anderem die Möglichkeit, die Computermodelle Z 1013.01 und Z 1013.16 mit komfortableren Tastaturen und mit Joysticks zu betreiben.\n\nBei Computermodellen, die über eine RAM-Disk verfügen, ist der Betrieb mit der CP/M-basierten Systemsoftware \"SCP\" möglich. Damit werden einige Programme aus der umfangreichen SCP-Bibliothek auch für die Z-1013-Rechner zugänglich.\n\nAufgrund der eingeschränkten Grafik- und Tonerzeugungsmöglichkeiten wurden die Z-1013-Rechner hauptsächlich zum Programmieren und für Anwendungen wie Textverarbeitung eingesetzt. Daneben existieren einige Spiele, die mit einfarbiger Grafik und ohne Tonuntermalung auskommen, wie beispielsweise die Schachprogramme \"Chess-Master\" und \"Cyrus-Chess\".\n\nZur Programmierung des Z 1013 stehen verschiedene Programmiersprachen und Hilfsmittel zur Verfügung. Neben Assemblern (\"Assembler 5.3 Scf\", \"Editor / Assembler EDAS\") sind höhere Programmiersprachen wie die auf der Programmkassette \"M 0111\" enthaltenen \"Tiny-BASIC\" und \"Kleincomputer BASIC\" aber auch \"BASICODE\", Forth und Pascal verfügbar.\n\nSpezielle Zeitschriften für den Z 1013 oder DDR-Kleincomputer im Allgemeinen gab es nicht. Viele Zeitschriften wie beispielsweise \"Funkamateur\", \"Jugend + Technik\", \"Mikroprozessortechnik\" und \"Practic\" veröffentlichten regelmäßig Neuigkeiten, Berichte, Bastelanleitungen zum Selbstbau von Zusatzhardware oder die Auf- und Umrüstung der Rechner sowie Programme zum Abtippen. Hannes Gutzer und Gerd Hutterer verfassten eine Broschüre \"BASIC mit dem Z 1013\", die der VEB Robotron-Elektronik Riesa herausgab.\n\nAuch nach der deutschen Wiedervereinigung wurde innerhalb der Anhängerschaft von DDR-Rechentechnik der Interessenaustausch in Publikationen geringer Auflage und ab den späten 1990er-Jahren zudem in Internetforen weiter gepflegt, bis hin zur Erstellung von entsprechenden Emulatoren.\n\nNach dem Ende der Heimcomputerära Anfang der 1990er-Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Mitte der 1990er-Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripherie entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reicht mit Hilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit unter anderem ein verstärktes Transferieren von sonst möglicherweise verlorengegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird.\n\nZur Emulation der DDR-Kleincomputer, insbesondere auch des Z 1013, wurde das unter Windows und Linux lauffähige Emulatorpaket \"JKCEMU\" entwickelt.\n\nVon den staatlich kontrollierten Zeitschriften wie beispielsweise \"Radio Fernsehen Elektronik\" und \"Funkamateur\" wurde das Erscheinen des Rechners begrüßt: „Als hardwarenah, preisgünstig und gut dokumentiertes System“ sei er bestens für „experimentelles Aneignen von Fähigkeiten auf dem Gebiet der angewandten Mikrorechnertechnik“ geeignet. Zugleich kritisierte man jedoch das anfänglich zum Z 9001 und KC 85/1 inkompatible BASIC, Inkompatibilitäten der Kassettenrekorderansteuerung bei den unterschiedlich getakteten Varianten und vor allem die für umfangreichere Texteingaben unbrauchbare Folienflachtastatur als „die Schwachstelle des Z 1013“. Insgesamt wurde der Z 1013 als brauchbares Gerät „für Elektronikamateure, Anfänger und Fortgeschrittene, Funkamateure sowie gesellschaftliche Bedarfsträger aus Lehre und Ausbildung“ eingestuft.\n\nDie Beliebtheit der Rechner in der Bevölkerung manifestierte sich in einer Vielzahl organisierter Computerclubs mit häufig stattfindenden lokalen Treffen bis hin zu gut frequentierten nationalen Tagungen, die alljährlich abgehalten wurden und beispielsweise zum Tauschen von Software, Erfahrungen und dem Festlegen von Programmierstandards dienten.\n\nIn jüngerer Zeit werden die in der DDR entwickelten und produzierten Rechner, darunter insbesondere Kleincomputer und Videospielautomaten, wieder verstärkt in den Medien – allen voran im Internet – wahrgenommen und auch in speziellen Museen ausgestellt. Dabei wird der Z 1013 als an westliche Einplatinencomputer angelehnte Eigenentwicklung charakterisiert, obwohl es sich bei vielen elektronischen Einzelkomponenten wie etwa dem U880-Mikroprozessor um die Kopie des westlichen Z80-Mikroprozessors von Zilog handelt. Im Gegensatz zu den DDR-Kleincomputern aus Dresden und Mühlhausen war der Z 1013 „in offener Bauart in verschiedenen Varianten als Konsumgut über den gesamten Produktionszeitraum erhältlich, ohne jedoch den Bedarf decken zu können.“ Die mit der geschichtlichen Aufarbeitung der Robotron-Rechentechnik befasste Arbeitsgemeinschaft in den Technischen Sammlungen Dresden charakterisiert das Verbreitungsumfeld des Z 1013 wie folgt:\n\nAuch wenn der Bausatz in der DDR sehr beliebt war, betrug der technologische Rückstand der Computer gegenüber den Produkten westlicher Industrieländer zum Zeitpunkt ihres Erscheinens stets etwa drei bis fünf Jahre: als die Produktion des Z 1013 in der DDR aufgenommen wurde, waren im westlichen Ausland bereits wesentlich leistungsfähigere Systeme für Privathaushalte erhältlich. Nach der Wende „entwickelte sich wegen Nachfragerückgangs ein Überangebot, trotz erheblicher Verkaufspreisreduzierung 1989 und 1990. Eine Fortsetzung der Produktion des Z 1013 war 1990 in Anbetracht des erwarteten Angebotes anderer westlicher Konkurrenzprodukte nicht mehr rentabel“, woraufhin die Produktion des Z 1013 Mitte 1990 eingestellt und die sich im Lager befindlichen Restgeräte der Verschrottung zugeführt wurden.\n\n\n"}
{"id": "177719", "url": "https://de.wikipedia.org/wiki?curid=177719", "title": "LC80", "text": "LC80\n\nDer Lerncomputer LC80 war ein in der DDR hergestellter Einplatinencomputer, der für Lehrzwecke vorgesehen war. \n\nEntwickelt wurde der LC 80 ab 1983 in der \"Beratungs- und Informationsstelle Mikroelektronik Erfurt\" von einem Entwicklerkollektiv. Zur Frühjahresmesse 1984 konnte er der Öffentlichkeit vorgestellt werden. Wenig später war er im Handel, er war somit der erste Computer den es in der DDR für die Bevölkerung zu kaufen gab.\n\nDie Produktion endete wohl um 1986/87.\n\nWomöglich durch eine Anfrage aus dem NSW (Großbritannien) wurde die Entwicklung einer Export-Variante angestoßen. \nDiese unterschied sich vom herkömmlichen LC80 in folgenden Punkten:\n\nWahrscheinlich wurde diese Variante nie in Serie produziert.\n\nProgrammiert wurde er durch die Eingabe hexadezimalen Maschinencodes über eine fest eingebaute Taschenrechner-Tastatur. Speichern und laden war via Kassette oder EPROM möglich.\n\n\nEs gab auch Zubehör zu kaufen wie:\n\nBis auf das Betriebssystem war keinerlei Software im Lieferumfang enthalten. Lediglich im Handbuch und in einschlägigen Zeitschriften wie rfe, Funkamateur usw. wurden gelegentlich Listings abgedruckt. Es gab Schaltkreistester, Spiele, Synthesizer, Morseprogramme und mehr. Für den geplanten Export war sogar ein (integriertes) Schachprogramm vorgesehen.\n\n"}
{"id": "178007", "url": "https://de.wikipedia.org/wiki?curid=178007", "title": "KC compact", "text": "KC compact\n\nDer KC compact war der letzte in der DDR gebaute 8-bit-Kleincomputer (Heimcomputer) mit 4 MHz und 64 KB RAM. Der Rechner ist ein Nachbau des Amstrad/Schneider CPC und stammt aus dem \"VEB Mikroelektronik „Wilhelm Pieck“ Mühlhausen\". Der KC compact wurde erst mit dem Ende der DDR (ironischerweise als Geschenk zum 40. Republikgeburtstag) serienreif, daher sind nur wenige Geräte produziert und verkauft worden.\n\nIn Zeiten des Kalten Krieges und der damit einhergehenden Handelsbeschränkungen war es im Ostblock durchaus üblich, Mikroelektronik und auch ganze Rechner auf irgendwelchen Wegen im Westen zu besorgen und mittels des Verfahrens des Reverse Engineering zu analysieren. Anschließend wurden die Geräte mit den im Osten vorhandenen Mitteln nachgebaut. Bedarfslücken konnten so relativ einfach geschlossen werden, außerdem bildete man Spezialisten heran. Ein gutes und bekanntes Beispiel sind die zahlreichen Spektrum-Klone des Ostblocks, aber auch IBM-kompatible Großrechner, vor allem das Einheitliche System Elektronischer Rechentechnik, sind jenseits des „eisernen Vorhangs“ erschienen.\n\nIm beiliegenden Servicehandbuch wurden Bauteile aus dem Ausland stets als solche gekennzeichnet, z. B. \"AY-3-8912 (Import NSW)\", \"SM 607 (Import Bulgarien)\" usw.\n\nDer Computer besteht aus einem Grundgerät mit externem Netzteil. Es wurde das gleiche Gehäuse wie beim Robotron BIC A 5105 benutzt, die Elektronik ist aber nicht austauschbar. Folgende Anschlüsse sind am KC vorhanden:\n\nWegen der zusätzlich eingebauten Spannungsversorgung für externe Geräte ist beim Anschluss von CPC-Originalteilen Vorsicht angebracht.\n\nIm Rechner arbeitet eine UA880-CPU, die auf dem U880 basiert. Von den 64 KB RAM dienen standardmäßig 16 KB als Bildwiederholspeicher. Damit lassen sich bei 640×200 Punkten 2, bei 320×200 Punkten 4 und bei 160×200 Punkten 16 aus 27 Farben darstellen. Die Bildschirmansteuerung wird wie beim CPC über einen unkonventionell angeschlossenen Motorola 6845 realisiert; daher sind Größe und Lage des Bildes und Bildwiederholspeichers sehr weitgehend programmierbar.\nDer Sound (AY-3-8912 Soundchip) konnte Anfang der 90er Jahre noch als gut eingestuft werden und ging deutlich über die Piepsgeräusche der KC-85-Serie hinaus. Es sind sowohl Hüllkurven als auch Rauschen erzeugbar. Der CIO-Schaltkreis U82536/U8036 (bzw. Zilog Z8536) erfüllt die Funktionen von PIO und CTC.\nEinige Spezialschaltkreise des westlichen Vorbildes wurden durch Logikgatter ersetzt, eine in der DDR gängige Methode.\n\nAls Zusatzgerät wurde auch ein Diskettenlaufwerk entwickelt. Damit ist der KC compact prinzipiell CP/M-tauglich.\n\nDer Einführungspreis war 2300 Mark der schnell auf 990Mark reduziert wurde.\n\nDie Stückzahl kann momentan mit mindestens 2440 beziffert werden.\n\nNach dem Einschalten wird der BASIC-Interpreter gestartet, und man kann sofort anfangen, Programme zu schreiben. Das \"BASIC 1.1\" ist sehr komfortabel (da einfach jenes Locomotive BASIC 1.1 des CPC kopiert wurde). Wegen der Softwarekompatibilität zu den westlichen CPCs kann man auf ein großes Angebot an Spielen und Anwendungen zurückgreifen. Vom Hersteller kamen einige Spiele sowie Anwendungen wie Textverarbeitung, Grafikprogramme und Pascal als Programmiersprache.\n\nMit 64 KB RAM und BASIC 1.1 könnte man diesen \"CPC-Rechner\" zwischen CPC 6128 und CPC 664 einordnen. Statt CP/M als Betriebssystem wurde ein eingedeutschter CP/M-2.2-Clone namens MicroDOS verwendet, der zuvor bereits auf verschiedenen DDR-Rechnern im Einsatz war. Software war also vermutlich erhältlich.\n\nÄußerlich im eckig-flachen, hellen Gehäuse ohne Laufwerk dem C64 ähnlicher als den CPCs, steckte unter der Tastatur ein Nachbau der bekannten Rechnertechnik mit anderen Mitteln. Verwendet wurde u. a.:\n\nExtern ist festzustellen:\n\nDer Nachbau ist derart gelungen, dass die Kompatibilität des doch etwas anderen Rechners sogar innerhalb (!) der Baureihe der CPC-Rechner einzuschätzen ist. Die Hardwareschnittstellen sind allerdings nur teilweise identisch, auch einzelne Aufrufe unterscheiden sich, es war ja eine andere Peripherie angeschlossen. Ob die mit der veränderten Chipset-Hardware eventuell möglichen I/O-Leistungssteigerungen je ausgenutzt wurden, ist fraglich.\n\nBei zwei mittels Kabel verbundenen Rechnern bestand, einzigartig innerhalb der CPC-Serie, die Möglichkeit, den Speicherinhalt auf den jeweils anderen Rechner zu klonen. Dies dürfte schwerpunktartig für die Programmentwicklung und die dabei notwendigen Testläufe gedacht gewesen sein.\n\nUnbestätigt ist die Produktionszeit zwischen 7. Oktober 1989 (DDR-Jubiläum) und 1990 (Wiedervereinigung). Über die Verbreitung, Stückzahlen bzw. Verwendung des Rechners innerhalb der DDR ist (noch) nicht viel bekannt. Das Diskettenlaufwerk war noch mind. bis zum 5. Juli 1990 für 300,00 DM erhältlich. Aufgrund der kurzen Produktionszeit und damit verbundenen geringen Stückzahlen haben funktionsfähige KC compact einen hohen Sammlerwert, der den seiner westlichen Brüder erheblich übersteigt.\n\n\n\n"}
{"id": "178375", "url": "https://de.wikipedia.org/wiki?curid=178375", "title": "Lerncomputer", "text": "Lerncomputer\n\nEin Lerncomputer ist ein Computer, bei dem die pädagogischen Gesichtspunkte im Vordergrund stehen. \nUrsprünglich handelte es sich dabei um Bausätze, die Aufbau und Funktion eines Computers vermittelten, heutzutage bezeichnet der Begriff meist Kleincomputer für Kinder auf denen spezielle Lernprogramme laufen.\n\nLerncomputer wurden seit Ende der 1940er Jahre, oftmals als Bausätze, zu relativ erschwinglichen Preisen angeboten. Diese waren änfänglich oft nur in den USA erhältlich. Bei einem Bausatz war der Lernerfolg besonders groß, da man jedes Bauteil einmal in die Hand nehmen musste. Das Verständnis für die Computerhardware wurde so gefördert. \n\nIn Deutschland verfügbar war 1968 Logikus, ein vom Kosmos-Lehrmittelverlag herausgebrachter Lerncomputer; Geräte wie der EZ80-DIT und PROFI-5 Mikrocomputerfamilie folgten. Ab 1969 war der Piko dat in der DDR erhältlich. 1981 stellte die DDR den LC80 her und in den Folgejahren wurden u. a. der Kosmos CP1, Polycomputer 880, Know-how-Computer, NDR-Klein-Computer und Robotron BIC A 5105 angeboten.\nMit der Herstellung erschwinglicher Großseriencomputer für den Heimgebrauch Anfang der 1980er Jahre, wurde der klassische Lerncomputer mehr und mehr zurückgedrängt.\n\nLerncomputer für Kinder werden heute z. B. von dem Unternehmen Vtech gefertigt. Bei dieser Art von Lerncomputern steht die Technik allerdings eher im Hintergrund. Auf diesen, speziell für Kinder angepassten Kleincomputern, laufen verschiedene Lernprogramme, um z. B. das Allgemeinwissen oder das Kopfrechnen zu fördern.\n\nEine noch junge Entwicklung sind Computerspiele mit virtuellen Lerncomputern. So lässt sich zum Beispiel das Open-World-Spiel Minecraft durch das Mod \"ComputerCraft\" aufrüsten, mit dessen Hilfe der Spieler virtuelle Computer und Roboter konstruieren kann, die in der Skriptsprache Lua programmiert werden können. Das Minecraft-Mod \"RedPower 2\" bietet Bauelemente für 6502-ähnliche virtuelle Computer, die in den Sprachen \"6502 Assembly\" oder Forth programmiert werden können.\n\nHäufig sind die Grenzen fließend, zwischen Lerncomputern, Einplatinencomputern, Entwicklungssystemen, Homecomputern usw.\n\nExperimentierkästen, Bausätze u. ä.:\nSchulungscomputer / Industrie:\n\nEntwicklungssysteme / Trainings-Kits:\n\nFür weitere, allgemeine Entwicklungssysteme, siehe System Design Kit.\n\n\n"}
{"id": "178657", "url": "https://de.wikipedia.org/wiki?curid=178657", "title": "Das große Krabbeln", "text": "Das große Krabbeln\n\nDas große Krabbeln (Originaltitel: \"A Bug’s Life\") ist der zweite abendfüllende, vollständig computergenerierte Animationsfilm der Pixar-Animations-Studios, der im Februar 1999 in den deutschen Kinos anlief. Er basiert lose auf der Handlungsidee von Akira Kurosawas Historienfilm \"Die sieben Samurai\", in dem eine Gruppe ein Dorf vor Unterdrückern retten soll.\n\nDer Film handelt von einer Kolonie Ameisen, die auf einer Insel lebt. Eine der Ameisen ist der impulsive und tollpatschige Erfinder Flik, der verzweifelt versucht, seinen Platz in der Gruppe zu finden. Er kann sich aber nicht in das konservative System im Ameisenhaufen einfügen und eckt mit seinen seltsamen, selten funktionierenden Ideen andauernd an; besonders Atta, die Kronprinzessin der Kolonie, ist nicht sehr begeistert von ihm. Die einzige Ameise der Kolonie, die Flik eine treue Freundin ist, ist Dot, Prinzessin Attas kleine Schwester.\n\nDie Kolonie sammelt Nahrung für den Winter – mehr als sie eigentlich müssten, denn jedes Jahr im Sommer spielt sich das gleiche Szenario ab: Der Grashüpfer Hopper und seine Bande terrorisieren die Kolonie und zwingen die Ameisen, regelmäßig für sie Futter bereitzustellen. Die Ameisen haben sich aus Angst längst damit abgefunden. Aber dieses Jahr passiert Flik mit einer seiner Erfindungen ein Missgeschick, und die gesamte Futteransammlung fällt in den Fluss, der die Insel umgibt. Die Drohung der Grashüpfer kommt umgehend: Im Herbst, wenn das letzte Blatt gefallen ist, werden sie wiederkommen. Dann soll die doppelte Menge für sie bereitliegen.\n\nDa Flik die ganze Sache unglaublich leidtut, hat er einen Plan: Er will Verstärkung besorgen, die gegen die Grashüpfer kämpft und sie endlich verjagt. Der Hohe Rat der Ameisen stimmt diesem Plan nur zu, um ihn bei ihren Reparaturbemühungen endlich aus dem Weg zu haben. Flik aber verkalkuliert sich: Zwar findet er Insekten, aber durch ein Missverständnis erfährt er erst später, dass es sich um Zirkusartisten handelt, die gerade von ihrem Direktor nach einer katastrophalen Vorstellung gefeuert wurden. Doch er will seine Leute nicht noch einmal enttäuschen und heckt einen neuen Plan aus. Zur Abschreckung der Grashüpfer hätten sich die Krieger eine raffinierte Taktik ausgedacht: Da Hopper sich vor Vögel zu Tode fürchtet, soll ein Vogel aus Ästen und Blättern gebaut werden, um die Grashüpfer von der Insel zu verjagen.\n\nDie Kolonie ist zunächst skeptisch, stimmt dann aber zu und setzt alles daran, die Idee zu verwirklichen. Doch der Schwindel, die Insekten seien Krieger, fliegt auf, als der Zirkusdirektor auf der Suche nach seinen Artisten auf der Insel auftaucht, und das Projekt erleidet einen herben Rückschlag. Flik wird aus der Gemeinschaft verstoßen und schließt sich den Zirkusartisten an, die nun wieder von dannen ziehen.\n\nDie Ameisen machen sich jetzt schleunigst wieder daran, das Futter zu beschaffen, doch es ist nicht genug da, um die Forderung zu erfüllen. Als die Grashüpfer eintreffen, beginnt für die Kolonie eine Tortur. Wie Sklaven werden sie von den Grashüpfern getrieben, um das Futter zu besorgen, und um die Ameisen weiter zu demoralisieren, fasst Hopper den Plan, bei Abschluss der Arbeit die Königin zu töten. Dot erfährt von dem Plan, eilt Flik und den Artisten nach und berichtet ihnen von der Lage. Flik wird bewusst, dass er nun alles wiedergutmachen kann, und eilt mit den Insekten zurück zur Insel. Sie geben den Grashüpfern eine Zirkusvorstellung – ein Ablenkungsmanöver, um die Königin unauffällig aus der Bahn zu bringen und den Vogel startklar zu machen.\n\nAls der Vogel startet, läuft zunächst alles nach Plan, doch der Zirkusdirektor nimmt fälschlicherweise an, der Angriff gelte seinen Artisten, und setzt den Vogel in Brand. Nun fliegt die ganze Sache auf, und alles scheint vorbei – doch dann macht Flik seinen Mitameisen bewusst, dass sie in ihrer Gemeinschaft viel stärker sind als die Grashüpferbande. Mit diesem neu gewonnen Selbstbewusstsein vertreiben sie die Grashüpfer. Hopper versucht aus Rache, Flik zu töten, wird aber von diesem zu einem Vogelnest gelockt und von dessen Bewohner aufgefressen.\n\nIm Frühling verabschiedet die gesamte Kolonie die Artisten als Freunde. Flik hat durch seine Heldentat endlich seinen Platz in der Kolonie gefunden, nicht nur weil seine Erfindungen jetzt endlich als nützlich angesehen werden, sondern auch als Gefährte von Atta, die die Königskrone übernimmt.\n\nFür \"Das große Krabbeln\" wurden aufwändige Recherchen betrieben, beispielsweise wurden mit winzig kleinen Kameras Aufnahmen in der Natur gemacht. Durch jene Studien entwickelte man beispielsweise die farbenfrohen Umgebungen sowie die Licht-Schatten-Verhältnisse. Außerdem versuchte man durch diverse Insektenaufnahmen, die Charaktere und das Verhalten der Insekten möglichst originalgetreu darzustellen. Im Gegensatz zum Erstfilm \"Toy Story\" wollte man hierbei eine organische, komplexe Umwelt schaffen, was in vielerlei Hinsicht eine enorme Herausforderung war. Aus diesem Grund wird der Film noch heute von zahlreichen Mitwirkenden als aufwändigster Pixar-Film bezeichnet.\n\nWas dieser gewollten Realitätsnähe jedoch entgegenwirkt, ist die Tatsache, dass die Ameisen nur über vier Gliedmaßen (Arme und Beine) anstatt sechs verfügen.\n\nDer Film sollte ursprünglich den Titel „Bugs“ tragen. Aufgrund einer möglichen Verwechslungsgefahr mit Bugs Bunny, der Figur des Konkurrenz-Studios Warner Bros., wurde der Titel dann jedoch überarbeitet.\n\nDer Film wurde im Breitbildformat 1:2,35 hergestellt. Für die Veröffentlichung auf VHS-Videocasetten im 4:3-Format (zur Vermeidung des Letterbox-Effektes mit großen schwarzen Balken auf dem Bildschirm) wurde der gesamte Film überarbeitet und entweder mit zusätzlichen Bildteilen über und unter dem eigentlichen Bild versehen, ins Bild hineingezoomt (und somit nur ein Ausschnitt verwendet, bei Schwenks zum Teil Pan & Scan) oder das ursprüngliche Bild wurde völlig neu berechnet und neu formatiert (Abstände/Verhältnis der einzelnen Bereiche im Bild neu angelegt). Hierüber findet sich auf der Deluxe-Edition der Special Collection DVD (die den Film in der 1:2,35-Version bietet) ein eigenes Feature auf der Bonusdisc.\n\nIm Abspann ist auf Englisch \"Filmed entirely on Location\" (dt. etwa: Vollständig am Schauplatz gefilmt) zu lesen, was auf die realen Recherchen für den Film zurückzuführen ist.\n\n\n"}
{"id": "178938", "url": "https://de.wikipedia.org/wiki?curid=178938", "title": "Arch Linux", "text": "Arch Linux\n\nArch Linux [] ist eine AMD64-optimierte Linux-Distribution mit Rolling Releases, dessen Entwicklerteam dem KISS-Prinzip („keep it simple, stupid“) folgt. Zugunsten der Einfachheit wird auf grafische Installations- und Konfigurationshilfen verzichtet. Aufgrund dieses minimalistischen Ansatzes ist Arch Linux als Distribution für fortgeschrittene Benutzer zu sehen. Arch Linux wurde Anfang 2001 von Judd Vinet eingeführt, inspiriert von Crux und BSD. Am 1. Oktober 2007 gab Vinet seinen Rücktritt als Projektleiter bekannt, sein Nachfolger wurde Aaron Griffin.\n\nArch Linux wird von einem ungefähr 25-köpfigen Kernteam und Helfern aus der wachsenden Community, sogenannten „Trusted Users“, weiterentwickelt. Sämtliche distributionsspezifischen Entwicklungen werden unter der GPL veröffentlicht. In Deutschland erlangte die Distribution besondere Bekanntheit als Beilage der Zeitschrift LinuxUser.\n\nArch Linux wurde mit Linux From Scratch komplett neu entwickelt, orientiert sich aber an Crux und anderen Distributionen. Für Arch wurde der Aufbau eines Slackware-Linux respektive eines BSD-Systems mit einer Debian-ähnlichen Paketverwaltung und dem Build-System von Gentoo kombiniert. Ähnlich wie in Gentoo sind die Releases lediglich Snapshots vom momentanen Entwicklungsstand (Rolling Release).\n\nAls Init-System wird seit Oktober 2012 auch bei einer Neuinstallation systemd verwendet. Konfigurationsprogramme für die Installation und Einrichtung des Grundsystems sowie für Anwendungs- und Serverprogramme gibt es nicht, stattdessen wird auf die Originaldokumentation und -konfiguration verwiesen, so dass allgemeine Howtos und Anleitungen herangezogen werden können.\n\nArch Linux wurde als „Basis-Betriebssystem für fortgeschrittene Anwender“ entwickelt. Die Philosophie von Arch Linux basiert auf den folgenden beiden Punkten:\n\nArch Linux ist auf den Einsatz von Binärpaketen ausgelegt. Grundsätzlich werden Pakete mit der eigens entwickelten Paketverwaltung \"Pacman\" organisiert, zusätzlich können mit dem \"Arch Build System (ABS)\" neue Pakete für Software, die nur im Quellcode vorliegt, erstellt werden.\n\nPacman ist ein speziell für Arch Linux entwickelter Paketmanager, der aber auch bei anderen Linux-Distributionen zum Einsatz kommt. Pacman kann Abhängigkeiten auflösen und automatisch alle notwendigen Pakete von den Arch-Repositorien herunterladen, installieren, aktualisieren und auch wieder entfernen, vergleichbar mit Debians APT. Das besondere an Pacman ist dessen konsequente Anwendung auch bei lokalen Quellen, die meistens als vom Arch Build System (ABS) erstellte Pakete vorliegen.\n\nBis zur Version 4 des Paketmanagers \"Pacman\" fehlte die Unterstützung für signierte Pakete. Pakete und Metadaten wurden von Pacman während des Download-Prozesses nicht auf Authentizität überprüft. Im März 2011 wurde durch einen Beitrag des renommierten Online-Magazins LWN.net die Sicherheit der Paketverteilungs-Infrastruktur mangels Integritätsprüfung der Paket-Metadaten kritisiert. Es existierten zwar Prüfsummen der einzelnen Pakete, diese Metadaten waren aber nicht mit einer digitalen Signatur versehen, weshalb bösartige Modifikationen an Paketen nicht festgestellt werden konnten. Im November 2011 wurde die Paket-Unterzeichnung Pflicht für neue Pakete und seit dem 21. März 2012 wird jedes offizielle Paket unterzeichnet.\n\nDie offiziellen Arch-Linux-Pakete werden in vier Software-Repositorien verwaltet:\n\nDer Hauptteil der Entwicklung findet in den \"testing\"-Repositorien statt, bevor die Pakete in die stabilen Repositorien verschoben werden:\nDabei müssen Transfers von Paketen, die von \"testing\" nach \"core\" wechseln, vorher von mehreren Entwicklern abgesegnet werden. Für Pakete in den anderen Repositorien sind deren jeweilige Entwickler verantwortlich.\n\nAußerdem gibt es noch einige Repositorien, die die neusten Versionen der Desktop-Umgebungen enthalten:\nZusätzlich können Repositorien Dritter eingebunden werden, welche angepasste oder neuere Versionen der Softwarepakete anbieten.\n\nDas Arch Build System ist eine Ports-ähnliche Paketverwaltung. Arch nutzt dabei jeweils eine Textdatei mit dem Namen PKGBUILD, die unter anderem die Anweisungen zum Herunterladen und Konfigurieren der jeweiligen Programme enthält. Der Nutzer kann mit dieser Datei die in der Paketverwaltung von Arch enthaltenen Programme seinen eigenen Bedürfnissen anpassen, indem er beispielsweise einen Patch einfügt. Das Programm makepkg führt diese Anweisungen aus und kompiliert und/oder bereitet die Pakete zur Installation durch pacman vor. Ein Arch-Paket ist im Grunde nicht mehr als ein komprimiertes tar-Archiv, das neben den zu installierenden Dateien noch einige weitere (.PKGINFO, .BUILDINFO und .MTREE) mit allen Metadaten enthält, die Pacman für den Umgang mit Paketen benötigt.\n\nZusätzlich bietet ABS die Möglichkeit, das komplette System mit eigenen Compiler-Flags neu zu bauen.\n\nZusätzlich zu den Repositories bieten Benutzer im Arch User Repository (AUR) selbstgemachte PKGBUILD-Skripte für Pakete an, die nicht in den Repositories enthalten sind. Die PKGBUILD-Skripte vereinfachen das Erstellen von Paketen aus den Quellen durch explizite Auflistung und Überprüfung von Abhängigkeiten und Konfiguration der Installation entsprechend der Arch-Architektur. Wegen der möglichen Sicherheitsrisiken werden diese PKGBUILD-Skripte jedoch nie automatisch in den offiziellen Repositories vorhanden sein. Das Arch User Repository bietet der Gemeinde über 40.000 PKGBUILDs, die nicht in den offiziellen Repositorien enthalten sind.\n\nIm Gegensatz zu anderen großen Distributionen wie Ubuntu und Fedora plant Arch Linux seine Versionen nicht zu bestimmten Terminen, sondern arbeitet mit einem Rolling-Release-System. Die Paketverwaltung ermöglicht es Benutzern, ihre Systeme immer aktuell zu halten. Anstatt den Benutzer zwischen diskreten Versionen zu bewegen, sind Arch-Linux-Versionen einfach Schnappschüsse des aktuellen Satzes von Paketen, manchmal mit überarbeiteter Installations-Software. Daher macht es keinen Unterschied, aus welchem Release Arch installiert wurde, wenn man Updates installiert hat. In der Tat zeigen einige Mitglieder der Arch-Foren stolz das Alter ihrer Installation.\n\nAm 22. Juli 2012 wurde angekündigt, dass das Installationsprogramm durch einen Satz einfacher Skripte ersetzt werde, um Verzögerungen im Release-Zyklus zu vermeiden. Es wird nun jeweils zum Monatsanfang ein neues Abbild angeboten, welches das Veröffentlichungsdatum als Version im Dateinamen trägt; beispielsweise 2013.01.04 für das Abbild, das am 4. Januar 2013 erschien. Frühe Installationsabbilder hatten Namen, so erschien Version 0.1 am 11. März 2002 unter dem Namen \"Homer\"; es folgten \"Vega\", \"Firefly\", \"Dragon\", \"Nova\" (2003), \"Widget\" (2004), \"Wombat\" (2005), \"Noodle\" (2006), \"Gimmick\", \"Voodoo\" (als Version 0.8, 2007), \"Duke\" (als Version 2007.05), \"Don’t Panic\", \"Core Dump\" (2008) und \"Overlord\" als letztes Release mit Namen.\n\nEs gibt mehrere Distributionen, die entweder direkt auf Arch Linux basieren oder dessen Programme nutzen.\n\n\n"}
{"id": "179066", "url": "https://de.wikipedia.org/wiki?curid=179066", "title": "Interaktive Medien", "text": "Interaktive Medien\n\nAls interaktive Medien bezeichnet man synchrone und asynchrone technische Kommunikationsmittel, die nichtlinear genutzt werden müssen. Ob ein Medium interaktiv genutzt werden kann, ist keine binäre Ja-Nein-Entscheidung, sondern eine graduelle; sinnvoll ist es, nach dem Grad der Interaktivität verschiedene Interaktivitätslevel zu unterscheiden.\n\nDas Internet ist ein Beispiel für ein hochgradig interaktives Medium, da es über Rückkanäle verfügt und so bidirektionale Kommunikation ermöglichen kann.\n\nMassenmedien wie Fernsehen und Hörfunk können zwar interaktive Elemente aufweisen, verfügen jedoch über eine geringe Interaktivität. Die Kommunikation erfolgt weitgehend unidirektional, da es keinen vollwertigen Rückkanal gibt, weshalb sie auch als Push-Medien bezeichnet werden.\n\nStärkere interaktive Eigenschaften hat das interaktive Fernsehen und die DVD. Noch ausgeprägter wird der Grad der Interaktivität bei Software-Anwendungen und bestimmten Online-Diensten.\n\n"}
