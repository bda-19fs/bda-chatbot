{"id": "340", "url": "https://de.wikipedia.org/wiki?curid=340", "title": "Awk", "text": "Awk\n\nawk ist eine Programmiersprache (Skriptsprache) zur Bearbeitung und Auswertung strukturierter Textdaten, beispielsweise CSV-Dateien. Der zugehörige Interpreter war eines der ersten Werkzeuge, das in der Version 3 von Unix erschien; es wird auch heute noch vielfach zusammen mit sed in Shell-Skripten eingesetzt, um Daten zu bearbeiten, umzuformen oder auszuwerten. Der Name \"awk\" ist aus den Anfangsbuchstaben der Nachnamen ihrer drei Autoren Alfred V. Aho, Peter J. Weinberger und Brian W. Kernighan zusammengesetzt.\n\nEine Version von awk ist heute in fast jedem Unix-System, das historisch auf UNIX zurückzuführen ist, sowie in jeder Linux-Distribution zu finden. Ein vergleichbares Programm ist aber auch für fast alle anderen Betriebssysteme verfügbar.\n\nDie Sprache arbeitet fast ausschließlich mit dem Datentyp Zeichenkette (). Daneben sind assoziative Arrays (d. h. mit Zeichenketten indizierte Arrays, auch Hashs genannt) und reguläre Ausdrücke grundlegende Bestandteile der Sprache.\n\nDie Leistungsfähigkeit, Kompaktheit, aber auch die Beschränkungen der awk- und sed-Skripte regte Larry Wall zur Entwicklung der Sprache Perl an.\n\nDie typische Ausführung eines awk-Programms besteht darin, Operationen – etwa Ersetzungen – auf einem Eingabetext durchzuführen. Dafür wird der Text zeilenweise eingelesen und anhand eines gewählten Trenners – üblicherweise eine Serie von Leerzeichen und/oder Tabulatorzeichen – in Felder aufgespalten. Anschließend werden die awk-Anweisungen auf die jeweilige Zeile angewandt. \n\nawk-Anweisungen haben folgende Struktur:\n\nFür die eingelesene Zeile wird ermittelt, ob sie die Bedingung (oft ein Regulärer Ausdruck) erfüllt. Ist die Bedingung erfüllt, wird der Code innerhalb des von geschweiften Klammern umschlossenen Anweisungsblocks ausgeführt.\nAbweichend davon\nkann ein Statement auch nur aus einer Aktion \n\noder nur aus einer Bedingung \n\nbestehen. Fehlt die Bedingung, so wird die Aktion für jede Zeile ausgeführt. Fehlt die Aktion, so wird als Standardaktion das Schreiben der ganzen Zeile ausgeführt, sofern die Bedingung erfüllt ist.\n\nDer Benutzer kann Variablen innerhalb von Anweisungsblöcken durch Referenzierung definieren, eine explizite Deklaration ist nicht notwendig. Der Gültigkeitsbereich der Variablen ist global. Eine Ausnahme bilden hier Funktionsargumente, deren Gültigkeit auf die sie definierende Funktion beschränkt ist. \n\nFunktionen können an beliebiger Stelle definiert werden, die Deklaration muss dabei nicht vor der ersten Nutzung erfolgen. Falls es sich um Skalare handelt, werden Funktionsargumente als Wertparameter übergeben, ansonsten als Referenzparameter. Die Argumente bei Aufruf einer Funktion müssen nicht der Funktionsdefinition entsprechen, überzählige Argumente werden als lokale Variablen behandelt, ausgelassene Argumente mit dem speziellen Wert \"uninitialized\" – numerisch Null und als Zeichenkette den Wert des leeren Strings – versehen.\n\nFunktionen und Variablen aller Art bedienen sich des gleichen Namensraums, so dass gleiche Benennung zu undefiniertem Verhalten führt. \n\nNeben benutzerdefinierten Variablen und Funktionen stehen auch Standardvariablen und Standardfunktionen zur Verfügung, beispielsweise die Variablen codice_1 für die gesamte Zeile, codice_2, codice_3, … für das jeweils i-te Feld der Zeile und codice_4 (von engl. ) für den Feldtrenner, sowie die Funktionen gsub(), split() und match().\n\nDie Syntax von awk ähnelt derjenigen der Programmiersprache C. Elementare Befehle sind Zuweisungen an Variablen, Vergleiche zwischen Variablen sowie Schleifen oder bedingte Befehlsausführungen (if-else). Daneben gibt es Aufrufe sowohl zu fest implementierten als auch zu selbst programmierten Funktionen.\n\nAusgeben von Daten auf der Standardausgabe ist durch den „codice_5“-Befehl möglich. Um etwa das zweite Feld einer Eingabezeile auszudrucken, wird der Befehl\n\nBedingungen sind in awk-Programmen entweder von der Form \n\noder von der Form\n\nReguläre Suchmuster werden wie beim grep-Befehl gebildet, und Matchoperatoren sind ~ für \"Muster gefunden\" und !~ für \"Muster nicht gefunden\". Als Abkürzung für die Bedingung „$0 ~ /\"reguläres Suchmuster\"/“ (also die ganze Zeile erfüllt das Suchmuster) kann „/\"reguläres Suchmuster\"/“ verwendet werden.\n\nAls spezielle Bedingungen gelten die Worte \"BEGIN\" und \"END\", bei denen die zugehörigen Anweisungsblöcke vor dem Einlesen der ersten Zeile bzw. nach Einlesen der letzten Zeile ausgeführt werden. \n\nDarüber hinaus können Bedingungen mit logischen Verknüpfungen zu neuen Bedingungen zusammengesetzt werden, z. B.\n\nDieser AWK-Befehl bewirkt, dass von jeder Zeile, die mit \"E\" beginnt und deren zweites Feld eine Zahl größer 20 ist, das dritte Feld ausgegeben wird.\n\nEinige einfache Programmbeispiele, die man z. B. unter Linux einfach in einer Shell eingeben kann: \n\necho Hallo Welt | awk '{print $1}'\necho Hallo Welt | awk '{print $2}'\n\nerzeugt die Ausgaben „codice_6“ bzw. „codice_7“\necho Hallo Welt | awk '{printf \"%s, %s!\\n\",$1,$2}'\n\nerzeugt die Ausgabe „codice_8“ sowie einen Zeilenumbruch.\nawk '$9 ~ /[45]../' /var/log/apache2/access?log\n\ngibt alle Zeilen einer Webserver-Logdatei aus, deren neunte Spalte drei Zeichen enthält, deren erstes eine 4 oder 5 ist. In der neunten Spalte steht üblicherweise der HTTP-Statuscode. Die ausgegebenen Zeilen gehören zu Anfragen, die fehlgeschlagen sind.\n\nDie erste awk-Version aus dem Jahr 1977 erfuhr 1985 eine Überarbeitung durch die ursprünglichen Autoren, die als \"nawk\" (new awk) bezeichnet wurde. Sie bietet die Möglichkeit, eigene Funktionen zu definieren, sowie eine größere Menge von Operatoren und vordefinierten Funktionen. Der Aufruf erfolgt zumeist dennoch über „awk“, seit eine Unterscheidung zwischen beiden Versionen obsolet geworden ist.\n\nDas GNU-Projekt der Free Software Foundation stellt unter dem Namen \"gawk\" eine nochmals erweiterte freie Variante zur Verfügung.\n\nEine weitere freie Implementierung ist \"mawk\" von Mike Brennan. mawk ist kleiner und schneller als gawk, was allerdings durch einige Einschränkungen erkauft wird.\n\nAuch \"BusyBox\" enthält eine kleine awk-Implementation, womit diese Sprache auch für Embedded Linux und Android zur Verfügung steht.\n\n\n"}
{"id": "348", "url": "https://de.wikipedia.org/wiki?curid=348", "title": "AutoCAD", "text": "AutoCAD\n\nAutoCAD [] ist Teil der CAD-Produktpalette von Autodesk.\n\nAutoCAD wurde als grafischer Zeichnungseditor von der Firma AutoDesk entwickelt. In den Anfangsjahren bis ca. 1990 wurde AutoCAD hauptsächlich als einfaches CAD-Programm mit Programmierschnittstellen zum Erstellen von technischen Zeichnungen verwendet. Heute umfasst die Produktpalette eine umfangreiche 3D-Funktion zum Modellieren von Objekten sowie spezieller Erweiterungen insbesondere für Ingenieure, Maschinenbauingenieure, Architekten, Innenarchitekten und Designfachleute sowie Geoinformatiker, Gebäudetechniker und allgemeine Bauingenieure.\n\nAutoCAD ist grundsätzlich ein vektororientiertes Zeichenprogramm, das auf einfachen Objekten wie Linien, Polylinien, Kreisen, Bögen und Texten aufgebaut ist, die wiederum die Grundlage für kompliziertere 3D-Objekte darstellen.\n\nDie zu AutoCAD entwickelten Dateiformate .dwg sowie .dxf bilden einen Industriestandard zum Austausch von CAD-Daten. Laut Autodesk wurden seit der Erfindung des DWG-Formates rund drei Milliarden Dateien erstellt, davon wurden im Jahr 2006 eine Milliarde aktiv bearbeitet.\n\nAutoCAD lief unter MS-DOS und wurde auch auf Unix und Apple Macintosh portiert. Seit Release 14 in den 1990er Jahren wurde nur noch Microsoft Windows als Betriebssystem unterstützt. Seit dem 15. Oktober 2010 ist AutoCAD für macOS erhältlich (ab Version 10.5.8).\n\nMit \"AutoCAD 360\" (vormals \"AutoCAD WS\"), stehen auch vereinfachte, kostenfreie Versionen als Webapp und native Mobile App für Smartphones und Tablet-PCs zur Verfügung (Android und iOS).\n\nBetriebssystemkompatibilität für Microsoft Windows:\n\nDie aktuelle Version ist AutoCAD 2019, erschienen im März 2018.\n\nAutoCAD Ersterscheinung\nDie ARX (AutoCad Runtime Extension)-Version wird durch die interne Versions-Variable ACADVER angezeigt. ARX-Anwendungen sind nur innerhalb des ganzzahligen Versions-Anteils (z. B. 17) kompatibel ausführbar.\n\nAutoCAD wird in verschiedenen Varianten mit unterschiedlichem Funktionsumfang angeboten.\n\n\"AutoCAD\" ist eine Software zur Bearbeitung von technischen Zeichnungen als Vektorgrafiken in 2D- und 3D. Die Software ist unter anderem in C++ programmiert und besitzt mehrere Programmierschnittstellen wie zum Beispiel AutoLISP. AutoCAD wird häufig mit zusätzlicher Software eingesetzt, die mit vorgegebenen Symbolen, Makros und Berechnungsfunktionen zur schnellen Erstellung von technischen Zeichnungen dient. Im Zuge der Weiterentwicklung wurden diese Funktionen direkt in die auf AutoCAD basierenden Produkte integriert.\n\n\"AutoCAD LT\" ist eine vereinfachte AutoCAD-Variante, mit der meist 2D-Zeichnungen erstellt werden und die weniger Programmierschnittstellen besitzt. Auch hier gibt es zusätzliche Software, die durch die vorgegebenen Symbole, Makros und Software mit Berechnungsfunktionen zur schnellen Erstellung von technischen Zeichnungen dient. Aufgrund der geringeren Funktionalität ist \"AutoCAD LT\" kostengünstiger als die 3D-Variante AutoCAD.\n\n\"AutoCAD Mechanical\" ist eine Erweiterung von AutoCAD für den Maschinenbau-Bereich (CAD/CAM), die aus dem ehemaligen deutschen Softwarehaus \"GENIUS CAD software GmbH\" im bayerischen Amberg durch Übernahme seitens \"Autodesk\" entstanden ist. Es ist eine sehr leistungsfähige 2D-Applikation mit deutlich erweitertem Befehlsumfang, Normteilen, Berechnungs- und Stücklistenfunktionen.\n\nDie früher vertriebene Erweiterung \"Mechanical Desktop\" für die mechanische 3D-Konstruktion wird nicht mehr weiterentwickelt. Stattdessen gibt es das wesentlich leistungsfähigere und modernere parametrische 3D-Programm für die Konstruktion in Mechanik und Maschinenbau \"Autodesk Inventor\". \"AutoCAD\", \"AutoCAD Mechanical\" und \"Autodesk Inventor\" werden mit weiteren Produkten als Paket mit dem Namen „Product Design Suite“ vermarktet. Eine Erweiterung stellt „Product Design Suite Ultimate“ mit dem „Inventor Professional“ dar, das die Funktionalität um FEM-Berechnung, dynamische Simulation, Rohrleitungs- und Kabelbaumkonstruktion erweitert. Die Verwaltung der Konstruktionsdaten kann mit \"Autodesk Vault\" erfolgen.\n\n\"AutoCAD Architecture\" ist eine erweiterte AutoCAD-Variante für den Bau- und Architekturmarkt (CAAD), die über eine vordefinierte 3D-Bibliothek für Bauteile, die zum Konstruieren von Gebäuden benötigt werden (Wände, Fenster, Treppen, Dächer, etc.) verfügt. AutoCAD Architecture ersetzt den bis zur Einführung von Autodesk entwickelten Architectural Desktop (ADT). Wie bei anderen Software-Lösungen auf Basis von AutoCAD (Civil3d, Inventor, …) handelt es sich um ein sogenanntes \"vertikales Produkt\". Die Zeichnung wird wahlweise in 2D oder 3D angefertigt und Grundrisse, Ansichten und Schnitte, die für den Bau notwendig sind, werden automatisch erstellt. Da AutoCAD Architecture objektorientiert arbeitet und das IFC-Format beherrscht, kann es zu den BIM-CAD Systemen gezählt werden.\n\n\"AutoCAD MEP\" () ist eine erweiterte AutoCAD-Architecture-Variante für die Gebäudetechnik (HVAC/MEP), die über eine vordefinierte 3D-Bibliothek für Bauteile, die zum Konstruieren von gebäudetechnischen Anlagen benötigt werden (Heizkessel, Heizkörper, Rohrleitungen, Rohrleitungsarmaturen, Klimakomponenten, Elektrotrassen, Schalter und Dosen, etc.) verfügt. Die Zeichnung wird vollständig 3D angefertigt und Grundrisse, Ansichten und Schnitte, die für die Gebäudetechnik notwendig sind, werden wie bei \"AutoCAD Architecture\" automatisch erstellt. Die Kompatibilität zu \"AutoCAD Architecture\" ist damit gewährleistet.\n\nAutoCAD ReCap ist eine AutoCAD-Erweiterung, die zusätzlich zu den 3D-Modellen das Verarbeiten von Punktwolken, wie sie zum Beispiel Laserscanner liefern, in AutoCAD ermöglicht.\n\nAutoCAD Map 3D basiert auf AutoCAD und ergänzt dieses um umfangreiche Funktionen für den Bereich Kartografie. Mit dem Programm erstellt und bearbeitet man technische Karten. Es lassen sich durch diverse Schnittstellen Daten aus zahlreichen Quellen integrieren und in gewissem Umfang auch Geodaten-Analysen durchführen. In der aktuellen Version sind die 3D-Funktionen erweitert worden, so lassen sich unter anderem auch Höhenlinienpläne generieren.\n\n1998 wurde die Software \"Topobase\" von der Schweizer Firma \"C-Plan AG\" in Gümligen als AutoCAD-Erweiterung veröffentlicht. 2006 wurde die Firma von Autodesk übernommen. Die Erweiterung machte \"AutoCAD Map 3D\" zu einem Geoinformationssystem und basiert auf einer nach Standards des Open Geospatial Consortium schematisierten Datenbank von Oracle mit Spatial-Erweiterung. Ab der Version 2012 ist sie in Map 3D integriert und für alle Fachschalen einsetzbar.\n\nAuch \"TB-Web GIS\" wurde von der Firma \"C-Plan\" neben Topobase entwickelt. Nach Übernahme durch Autodesk wurde die Software als \"Autodesk Topobase Web\" angeboten.\n\nAutodesk entwickelte das PHP-basierte Web-GIS-Framework \"Autodesk MapGuide Enterprise\", das von der Open Source Geospatial Foundation OSGeo quelloffen als MapGuide Open Source erhältlich ist.\n\nDie Produkte \"Autodesk MapGuide Enterprise\" und \"Autodesk Topobase Web\" wurden zusammengelegt zur Mapserver-Software mit Web-GIS-Framework namens Autodesk Infrastructure Map Server.\n\nAutoCAD Civil 3D basiert auf AutoCAD und ist für die Bearbeitung von Tiefbauprojekten, insbesondere Verkehrswege-, Landschaftsplanung, Geländemodellierung und Wasserbau , geeignet. Um die Bearbeitung von Projekten zu ermöglichen, die sich über weite und komplexe Geländeformen ziehen, ist die volle Funktionalität von AutoCAD Map 3D in AutoCAD Civil 3D integriert.\n\nAutoCAD ecscad basiert auf AutoCAD und ist für die Planung elektrotechnischer Steuerungssysteme, sogenannter Stromlaufpläne geeignet.\n\n\"Autosketch\" ist ein einfaches Vektor-Zeichenprogramm. Es wird von Autodesk nicht mehr unterstützt oder weiterentwickelt. Es wurde abgelöst von dem Programm \"Autodesk SketchBook\", welches auch gratis als Expressversion mit eingeschränktem Funktionsumfang für Windows, MacOS, IOS und Android erhältlich ist.\n\nAutoCAD bietet eine Vielzahl an Programmierschnittstellen (APIs) für Customizing und Automatisierung. Als interne Programmierschnittstellen stehen heute zur Verfügung\n\nsowie weitere Schnittstellen zu:\n\nDurch den Einsatz von Vorgabezeichnungen, Blöcken, Symbolen, Linientypen und externen Spezialprogrammen, zum Beispiel für die Ausgabe von Berechnungsergebnissen können relativ einfach fast alle geometrischen und technischen Darstellungen erzeugt oder modifiziert werden.\n\nAutoCAD verwendet überwiegend eigene Dateiformate.\n\nNach außen ist dieses Dateiformat durch den Dateinamenanhang \".dwg\", für ‚normale‘ Zeichnungsdateien gekennzeichnet. Das Kürzel steht für (engl. für „Zeichnung“). Die Dokumentation der Dateistruktur ist nicht frei erhältlich, jedoch findet man im Internet eine Dokumentation der Open Design Alliance.\n\nDas DWG-Dateiformat wurde kontinuierlich an die Anforderungen der jeweiligen AutoCAD-Versionen angepasst und erweitert. So wurde das Format mit Einführung der Versionen AutoCAD 2000, 2004, 2007, 2010, 2013 und 2018 geändert. Die als \"DWG 2000\", \"DWG 2004\", \"DWG 2007\", \"DWG 2010\", \"DWG 2013\" und \"DWG 2018\" bezeichneten Formate können nicht in ältere AutoCAD-Versionen eingelesen werden. Die eingeschränkte Kompatibilität des DWG-Dateiformats zu älteren AutoCAD-Versionen kann durch Abspeichern in älteren Formatversionen (kann im Programm generell festgelegt werden) sowie durch die Verwendung des DXF-Dateiformats und den Einsatz von externen Konverterprogrammen teilweise umgangen werden. Bei Nutzung des DXF-Formats ist dabei mit dem Zerfall von nicht unterstützen Objekten in einfachere Basisobjekte zu rechnen. Die ersten 6 Bytes einer dwg-Datei sind mit einem gewöhnlichen Texteditor lesbar. Sie geben die Version der DWG-Datei an. Aktuelle Dateien der Version AutoCAD 2013 bis AutoCAD 2015 beginnen mit dem Header AC1027.\n\nDie DXF-Schnittstelle ist eine quelloffene Schnittstelle des Herstellers Autodesk und unterliegt keinem neutralen Normungsausschuss, die Dokumentation für DXF ist aber frei verfügbar. Sie ist ein in ASCII-Zeichen lesbares Abbild der binär abgespeicherten DWG. AutoCAD unterstützt DXF (engl. , „Zeichnungsaustauschformat“) für den Datenaustausch mit anderen CAD-Programmen in der aktuellen Version und jeweils noch meist 3–4 älteren Stände.\n\nDas DXF-Dateiformat unterstützt direkt 2D- und 3D-Koordinaten sowie zum Beispiel Linien, Bögen und einfache Flächen und weitere komplexe Geometrieelemente wie zum Beispiel Blöcke, ARX-Objekte und Bemaßungen. Es ist mit einfachen Mitteln zum Beispiel mit Texteditoren und fast allen Programmiersprachen, einschließlich mit dem VBA von Excel möglich, DXF-Dateien zu erzeugen, auszuwerten oder zu manipulieren. Diese Möglichkeiten bieten sich besonders für geometrische und auf geometriebasierende Berechnungen von CAD-Modellen zum Beispiel zur Optimierung von Flächen an. Der Aufbau ist sehr klar, einfach und strukturiert.\n\nDiese Schnittstelle hat sich im CAD-Markt als ein Quasi-Datenaustauschstandard etabliert, obwohl sie nicht von Autodesk mit diesem Ziel entwickelt wurde. Das DXF-Format wurde von Autodesk dazu geschaffen, um geometrische Informationen von AutoCAD an eine interne oder externe Applikation zur weiteren Verwendung zu übergeben. Genauso sollte das Ergebnis zum Beispiel einer Berechnung wieder aus der Applikation zurück an AutoCAD übergeben werden. Dazu wurde eine Liste von geometrischen Objekten von den Entwicklern erstellt und sauber dokumentiert. Diese offene Dokumentation wurde dann von anderen CAD-, CNC- und CAM-Herstellern wegen ihrer einfachen Struktur und Übersicht als CAD-Schnittstelle übernommen. Sie ist der oft kleinste gemeinsame Nenner vieler Vektorgrafikprogramme und wird von fast allen unterstützt. Allerdings werden meist nicht alle Funktionen von den anderen Herstellern voll unterstützt und es gehen daher manchmal entscheidende Details beim Austausch via DXF verloren.\n\nAuch das DXF-Dateiformat wurde, wie das DWG-Dateiformat, kontinuierlich an die Anforderungen der jeweiligen AutoCAD-Versionen angepasst und erweitert.\n\nDas DXB-Dateiformat (engl. ) ist eine binäre Form des DXF-Dateiformates. Es ist extrem kompakt, kann im Verhältnis zu DXF schnell gelesen und geschrieben werden, ist aber für den Programmierer wesentlich aufwendiger als die ASCII-Variante. DXB wird nur in wenigen, hauptsächlich zeitkritischen Anwendungsfällen verwendet.\n\nEin weiteres Format ist das Dateiformat \"DWF\" (engl. ) als hochkomprimiertes Vektorformat zur Präsentation im Internet und zur Ansicht. Das Format ist dokumentiert. Ein DWF-Toolkit mit C++-API zum Lesen und Schreiben ist mit Quelltext kostenlos bei Autodesk erhältlich. DWFx ist eine Weiterentwicklung von DWF, die auf dem XPS-Format von Microsoft basiert.\n\nEin weiteres Format ist das Dateiformat \"DGN\", das von MicroStation definiert wird und auch in den aktuellen AutoCAD Versionen unterstützt wird. Das Kürzel DGN steht für (engl. für „Entwurf“).\n\nEin weiteres Format ist das Dateiformat \"SHP\" (engl. Shapefile; nicht zu verwechseln mit dem ESRI-Shapefile), eine Symboldefinition. Dieses Dateiformat wird zur Codierung von Zeichnungselementen auf unterster Ebene eingesetzt und wird vor der Verwendung zu \"SHX\" kompiliert. Anwendungsgebiete sind benutzerdefinierte Schraffuren, Linien, Bemaßungen oder Schriftarten. Es können nur die elementarsten Objekte definiert werden wie Linien und Bögen.\n\nDie Dateiendung für AutoCad-Schriftarten (Fonts) und Linientypen. Eine Schrift-shx-Datei ist jedoch in einem Binärformat codiert, eine Linientyp-shx-Datei in Reintext.\nSchriften im shx-Format werden z. T. graphisch anders behandelt, als z. B. Schriften, die vom Betriebssystem zur Verfügung gestellt werden (TrueType, Postscript-Fonts), da sie keine Füllungsflächen oder Rundungen unterstützen.\n\nFür AutoCAD gibt es zu vielen Bereichen Spezial-Anwendungen. Beispielsweise für das Bauwesen, den Maschinenbau (siehe oben), den Landschaftsbau, die Versorgungs- und Elektrotechnik. Diese sind in der Regel in C++ geschrieben. Autodesk bietet hier mit ObjectARX (C++-API) die entsprechenden Grundlagen. Die Entwicklung geht auch hier zu .NET. Einfache Programmwerkzeuge (Tools) sind bisweilen in Visual Basic oder VBA geschrieben worden. Hinzu kommen eine Vielzahl von AutoLISP-Routinen, die oft in freien Foren ausgetauscht werden. Eine Auflistung kommerzieller Anwendungen findet sich im Autodesk-Katalog.\n\nDie Stiftung Warentest hat im Februar 2015 AutoCAD-Kurse für Einsteiger getestet. Sieben Kurse wurden getestet, vier davon bekamen eine gute Qualität bescheinigt. Unter den Anbietern waren Handwerkskammern, Industrie- und Handelskammern und kommerzielle Bildungsanbieter. Die Kosten für die drei- bis fünftägigen Kurse variierten zwischen 143 und 2090 Euro, wobei sowohl der günstigste als auch der teuerste Kurs nur mittelmäßig abschnitten.\n\n\n"}
{"id": "782", "url": "https://de.wikipedia.org/wiki?curid=782", "title": "Computer", "text": "Computer\n\nEin Computer [] oder Rechner ist ein Gerät, das mittels programmierbarer Rechenvorschriften Daten verarbeitet. Dementsprechend sind vereinzelt auch die abstrahierenden bzw. veralteten, synonym gebrauchten Begriffe Rechenanlage, Datenverarbeitungsanlage oder elektronische Datenverarbeitungsanlage anzutreffen.\n\nAls Computersystem bezeichnet man (a) einerseits ein Netzwerk oder einen Verbund aus mehreren Computern, die individuell gesteuert werden und auf gemeinsam genutzte Daten und Geräte zugreifen können, (b) zum anderen aber auch die einen einzelnen voll funktionstüchtigen Rechner in ihrem Zusammenspiel bedingende Gesamtheit von externen und internen Komponenten, d. h. Hardware, Software wie auch angeschlossenen Peripheriegeräten sowie ferner (c) ein System von Programmen zur Steuerung und Überwachung von Computern.\n\nCharles Babbage und Ada Lovelace gelten durch die von Babbage 1837 entworfene Rechenmaschine \"Analytical Engine\" als Vordenker des modernen universell programmierbaren Computers. Konrad Zuse (Z3, 1941 und Z4, 1945), John Presper Eckert und John William Mauchly (ENIAC, 1946) bauten die ersten funktionstüchtigen Geräte dieser Art. Bei der Klassifizierung eines Geräts als \"universell programmierbarer Computer\" spielt die Turing-Vollständigkeit eine wesentliche Rolle. Sie ist benannt nach dem englischen Mathematiker Alan Turing, der 1936 das logische Modell der Turingmaschine eingeführt hatte.\n\nDie frühen Computer wurden auch (Groß-)Rechner genannt; ihre Ein- und Ausgabe der Daten war zunächst auf Zahlen beschränkt. Zwar verstehen sich moderne Computer auf den Umgang mit weiteren Daten, beispielsweise mit Buchstaben und Tönen. Diese Daten werden jedoch innerhalb des Computers in Zahlen umgewandelt und als solche verarbeitet, weshalb ein Computer auch heute eine Rechenmaschine ist.\n\nMit zunehmender Leistungsfähigkeit eröffneten sich neue Einsatzbereiche. Computer sind heute in allen Bereichen des täglichen Lebens vorzufinden, meistens in spezialisierten Varianten, die auf einen vorliegenden Anwendungszweck zugeschnitten sind. So dienen integrierte Kleinstcomputer (eingebettetes System) zur Steuerung von Alltagsgeräten wie Waschmaschinen und Videorekordern oder zur Münzprüfung in Warenautomaten; in modernen Automobilen dienen sie beispielsweise zur Anzeige von Fahrdaten und steuern in „Fahrassistenten“ diverse Manöver selbst.\n\nUniverselle Computer finden sich in Smartphones und Spielkonsolen. Personal Computer (engl. für Persönliche Computer, als Gegensatz zu von vielen genutzten Großrechnern) dienen der Informationsverarbeitung in Wirtschaft und Behörden sowie bei Privatpersonen; Supercomputer werden eingesetzt, um komplexe Vorgänge zu simulieren, z. B. in der Klimaforschung oder für medizinische Berechnungen.\n\nDer deutsche Begriff \"Rechner\" ist abgeleitet vom Verb \"rechnen\". Zur Etymologie siehe Rechnen#Etymologie.\n\nDas englische Substantiv „computer“ ist abgeleitet von dem englischen Verb „to compute“. Jenes ist abgeleitet von dem lateinischen Verb „computare“, was zusammenrechnen bedeutet.\n\nDer englische Begriff „computer“ war ursprünglich eine Berufsbezeichnung für Hilfskräfte, die immer wiederkehrende Berechnungen (z. B. für die Astronomie, für die Geodäsie oder für die Ballistik) im Auftrag von Mathematikern ausführten und damit Tabellen wie z. B. eine Logarithmentafel füllten.\n\nIn der Kirchengeschichte war mit der Hinrichtung des Jesus eine Ablösung von der jüdischen und eine Hinwendung zur römischen Zeitrechnung verbunden. Die hieraus resultierenden Berechnungsschwierigkeiten des Osterdatums dauerten bis zum Mittelalter an und waren Gegenstand zahlreicher Publikationen, häufig betitelt mit „Computus Ecclesiasticus“. Doch finden sich noch weitere Titel, z. B. von Sigismund Suevus 1574, die sich mit arithmetischen Fragestellungen auseinandersetzen. Der früheste Text, in dem das Wort Computer isoliert verwendet wird, stammt von 1613.\n\nIn der Zeitung The New York Times tauchte das Wort erstmals am 2. Mai 1892 in einer Kleinanzeige der United States Navy mit dem Titel „A Computer Wanted“ (Ein Rechner gesucht) auf, in der Kenntnisse in Algebra, Geometrie, Trigonometrie und Astronomie vorausgesetzt worden sind.\n\nAn der University of Pennsylvania in Philadelphia wurden im Auftrag der United States Army ballistische Tabellen berechnet. Das Ergebnis waren Bücher für die Artillerie, die für unterschiedliche Geschütze Flugbahnen unterschiedlicher Geschosse vorhersagten. Diese Berechnungen erfolgten größtenteils von Hand. Die einzige Hilfe war eine Tabelliermaschine, die zu multiplizieren und zu dividieren vermochte. Die Angestellten, die dort rechneten, wurden als „computer“ bezeichnet.\n\nKatherine Johnson ist ein Beispiel für eine Computerfrau Sie berechnete Raumflüge für die NASA.\n\nHenrietta Swan Leavitt war ein Computer.\n\nGrundsätzlich unterscheiden sich zwei Bauweisen: Ein Rechner ist ein Digitalrechner, wenn er mit digitalen Geräteeinheiten digitale Daten verarbeitet (also Zahlen und Textzeichen); er ist ein Analogrechner, wenn er mit analogen Geräteeinheiten analoge Daten verarbeitet (also kontinuierlich verlaufende elektrische Messgrößen wie Spannung oder Strom).\n\nHeute werden fast ausschließlich Digitalrechner eingesetzt. Diese folgen gemeinsamen Grundprinzipien, mit denen ihre freie Programmierung ermöglicht wird. Bei einem Digitalrechner werden dabei zwei grundsätzliche Bestandteile unterschieden: Die Hardware, die aus den elektronischen, physisch anfassbaren Teilen des Computers gebildet wird, sowie die Software, die die \"Programmierung\" des Computers beschreibt.\n\nEin Digitalrechner besteht zunächst nur aus Hardware. Die Hardware stellt erstens einen \"Speicher\" bereit, in dem Daten portionsweise wie auf den nummerierten Seiten eines Buches gespeichert und jederzeit zur Verarbeitung oder Ausgabe abgerufen werden können. Zweitens verfügt das Rechenwerk der Hardware über grundlegende Bausteine für eine freie Programmierung, mit denen jede beliebige Verarbeitungslogik für Daten dargestellt werden kann: Diese Bausteine sind im Prinzip die \"Berechnung\", der \"Vergleich\" und der \"bedingte Sprung\". Ein Digitalrechner kann beispielsweise zwei Zahlen addieren, das Ergebnis mit einer dritten Zahl vergleichen und dann abhängig vom Ergebnis entweder an der einen oder der anderen Stelle des Programms fortfahren. In der Informatik wird dieses Modell theoretisch durch die eingangs erwähnte Turing-Maschine abgebildet; die Turing-Maschine stellt die grundsätzlichen Überlegungen zur Berechenbarkeit dar.\n\nErst durch eine Software wird der Digitalcomputer jedoch nützlich. Jede Software ist im Prinzip eine definierte, funktionale Anordnung der oben geschilderten Bausteine Berechnung, Vergleich und bedingter Sprung, wobei die Bausteine beliebig oft verwendet werden können. Diese Anordnung der Bausteine, die als \"Programm\" bezeichnet wird, wird in Form von Daten im Speicher des Computers abgelegt. Von dort kann sie von der Hardware ausgelesen und abgearbeitet werden. Dieses Funktionsprinzip der Digitalcomputer hat sich seit seinen Ursprüngen in der Mitte des 20. Jahrhunderts nicht wesentlich verändert, wenngleich die Details der Technologie erheblich verbessert wurden.\n\nAnalogrechner funktionieren nach einem anderen Prinzip. Bei ihnen ersetzen analoge Bauelemente (Verstärker, Kondensatoren) die Logikprogrammierung. Analogrechner wurden früher häufiger zur Simulation von Regelvorgängen eingesetzt (siehe: Regelungstechnik), sind heute aber fast vollständig von Digitalcomputern abgelöst worden. In einer Übergangszeit gab es auch Hybridrechner, die einen Analog- mit einem digitalen Computer kombinierten.\n\nMögliche Einsatzmöglichkeiten für Computer sind:\n\n\nDas heute allgemein angewandte Prinzip, das nach seiner Beschreibung durch John von Neumann von 1946 als \"Von-Neumann-Architektur\" bezeichnet wird, definiert für einen Computer fünf Hauptkomponenten:\n\n\nIn den heutigen Computern sind die ALU und die Steuereinheit meistens zu einem Baustein verschmolzen, der so genannten CPU (Central Processing Unit, zentraler Prozessor).\n\nDer Speicher ist eine Anzahl von durchnummerierten, adressierbaren „Zellen“; jede von ihnen kann ein einzelnes Stück Information aufnehmen. Diese Information wird als Binärzahl, also eine Abfolge von ja/nein-Informationen im Sinne von Einsen und Nullen, in der Speicherzelle abgelegt.\n\nBezüglich des Speicherwerks ist eine wesentliche Designentscheidung der Von-Neumann-Architektur, dass sich Programm und Daten einen Speicherbereich teilen (dabei belegen die Daten in aller Regel den unteren und die Programme den oberen Speicherbereich). Demgegenüber stehen in der Harvard-Architektur Daten und Programmen eigene (physikalisch getrennte) Speicherbereiche zur Verfügung. Der Zugriff auf die Speicherbereiche kann parallel realisiert werden, was zu Geschwindigkeitsvorteilen führt. Aus diesem Grund werden digitale Signalprozessoren häufig in Harvard-Architektur ausgeführt. Weiterhin können Daten-Schreiboperationen in der Harvard-Architektur keine Programme überschreiben (Informationssicherheit).\n\nIn der Von-Neumann-Architektur ist das Steuerwerk für die Speicherverwaltung in Form von Lese- und Schreibzugriffen zuständig.\n\nDie ALU hat die Aufgabe, Werte aus Speicherzellen zu kombinieren. Sie bekommt die Werte von der Steuereinheit geliefert, verrechnet sie (addiert beispielsweise zwei Zahlen) und gibt den Wert an die Steuereinheit zurück, die den Wert dann für einen Vergleich verwenden oder in eine andere Speicherzelle schreiben kann.\n\nDie Ein-/Ausgabeeinheiten schließlich sind dafür zuständig, die initialen Programme in die Speicherzellen einzugeben und dem Benutzer die Ergebnisse der Berechnung anzuzeigen.\n\nDie Von-Neumann-Architektur ist gewissermaßen die unterste Ebene des Funktionsprinzips eines Computers oberhalb der elektrophysikalischen Vorgänge in den Leiterbahnen. Die ersten Computer wurden auch tatsächlich so programmiert, dass man die Nummern von Befehlen und von bestimmten Speicherzellen so, wie es das Programm erforderte, nacheinander in die einzelnen Speicherzellen schrieb. Um diesen Aufwand zu reduzieren, wurden Programmiersprachen entwickelt. Diese generieren die Zahlen innerhalb der Speicherzellen, die der Computer letztlich als Programm abarbeitet, aus Textbefehlen heraus automatisch, die auch für den Programmierer einen semantisch verständlichen Inhalt darstellen (z. B. GOTO für den „unbedingten Sprung“).\n\nSpäter wurden bestimmte sich wiederholende Prozeduren in so genannten Bibliotheken zusammengefasst, um nicht jedes Mal das Rad neu erfinden zu müssen, z. B.: das Interpretieren einer gedrückten Tastaturtaste als Buchstabe „A“ und damit als Zahl „65“ (im ASCII-Code). Die Bibliotheken wurden in übergeordneten Bibliotheken gebündelt, welche Unterfunktionen zu komplexen Operationen verknüpfen (Beispiel: die Anzeige eines Buchstabens „A“, bestehend aus 20 einzelnen schwarzen und 50 einzelnen weißen Punkten auf dem Bildschirm, nachdem der Benutzer die Taste „A“ gedrückt hat).\n\nIn einem modernen Computer arbeiten sehr viele dieser Programmebenen über- bzw. untereinander. Komplexere Aufgaben werden in Unteraufgaben zerlegt, die von anderen Programmierern bereits bearbeitet wurden, die wiederum auf die Vorarbeit weiterer Programmierer aufbauen, deren Bibliotheken sie verwenden. Auf der untersten Ebene findet sich aber immer der so genannte Maschinencode – jene Abfolge von Zahlen, mit der der Computer auch tatsächlich gesteuert wird.\n\n\n\nZukünftige Entwicklungen bestehen voraussichtlich aus der möglichen Nutzung biologischer Systeme (Biocomputer), weiteren Verknüpfungen zwischen biologischer und technischer Informationsverarbeitung, optischer Signalverarbeitung und neuen physikalischen Modellen (Quantencomputer).\n\nEin großer Megatrend sind derzeit (2017) die Entwicklung künstlicher Intelligenzen. Bei diesen simuliert man die Vorgänge im menschlichen Gehirn und erschafft so selbstlernende Computer, die nicht mehr wie bislang programmiert werden, sondern mit Daten trainiert werden ähnlich einem Gehirn. Der Zeitpunkt an dem künstliche Intelligenz die menschliche Intelligenz übertrifft nennt man technologische Singularität. Künstliche Intelligenz wird heute (2017) bereits in vielen Anwendungen, auch alltäglichen, eingesetzt (s. Anwendungen der künstlichen Intelligenz). Hans Moravec bezifferte die Rechenleistung des Gehirns auf 100 Teraflops, Raymond Kurzweil auf 10.000 Teraflops. Diese Rechenleistung haben Supercomputer bereits deutlich überschritten. Zum Vergleich liegt eine Grafikkarte für 800 Euro (5/2016) bei einer Leistung von 10 Teraflops. (s. technologische Singularität)\n\nFür weitere Entwicklungen und Trends, von denen viele noch den Charakter von Schlagwörtern bzw. Hypes haben, siehe Autonomic Computing (= Rechnerautonomie), Grid Computing, Cloud Computing, Pervasive Computing, ubiquitäres Computing (= Rechnerallgegenwart) und Wearable Computing.\n\nDie weltweite Websuche nach dem Begriff „Computer“ nimmt seit Beginn der Statistik 2004 stetig ab. In den 10 Jahren bis 2014 war diese Zugriffszahl auf ein Drittel gefallen.\n\nVerkaufszahlen und Marktanteile der Computerhersteller nach Angaben des Marktforschungsunternehmens Gartner Inc., basierend auf Verkaufszahlen von Desktop-Computer, Notebooks, Netbooks, aber ohne Tablet-Computer, an Endkonsumenten:\n\n\n\n\n\nComputermuseen\n"}
{"id": "819", "url": "https://de.wikipedia.org/wiki?curid=819", "title": "Computerlinguistik", "text": "Computerlinguistik\n\nIn der Computerlinguistik (CL) oder linguistischen Datenverarbeitung (LDV) wird untersucht, wie natürliche Sprache in Form von Text- oder Sprachdaten mit Hilfe des Computers algorithmisch verarbeitet werden kann. Sie ist Schnittstelle zwischen Sprachwissenschaft und Informatik. In der englischsprachigen Literatur und Informatik ist der Begriff \"Natural language processing (NLP) gebräuchlich.\"\n\nComputerlinguistik lässt sich als Begriff (oder dessen Umschreibung) in die 1960er Jahre zurückverfolgen. Mit den Anfängen der künstlichen Intelligenz war die Aufgabenstellung schon nahegelegt. Chomskys \"Syntactic Structures\" von 1957 präsentierte die Sprache in einem entsprechend passenden neuen formalen Gerüst. Hinzu kamen die Sprachlogiken von Saul Kripke und Richard Montague. Die teilweise aus dem US-Verteidigungsbudget sehr hoch geförderten Forschungen brachten jedoch nicht die erhofften Durchbrüche. Besonders die Koryphäen Chomsky und Weizenbaum dämpften die Erwartungen an Automatisierungen von Sprachübersetzung. Der Wende von behavioristischen Wissenschaftskonzeptionen zu mentalistischen (Chomsky) folgten umfassende Konzipierungen in den Kognitionswissenschaften.\n\nIn den siebziger Jahren erschienen zunehmend häufiger Publikationen mit dem Begriff \"Computerlinguistik\" im Titel. Es gab bereits finanziell aufwändige Versuche exegetischer Anwendungen (Konkordanzen, Wort- und Formstatistik), aber auch schon größere Projekte zur maschinellen Sprachanalyse und zu Übersetzungen. Die ersten Computerlinguistik-Studiengänge in Deutschland wurden an der Universität des Saarlandes und in Stuttgart eingerichtet. Die Computerlinguistik bekam mit der Verbreitung von Arbeitsplatzrechnern (PC) und mit dem Aufkommen des Internets neue Anwendungsgebiete. Im Gegensatz zu einer Internetlinguistik, die insbesondere menschliches Sprachverhalten und die darüber induzierten Sprachbildungen im und mittels Internet untersucht, entstand in der Computerlinguistik eine stärker informatisch-praktische Ausrichtung. Doch gab das Fach die klassischen philosophisch-linguistischen Fragen nicht ganz auf und wird heute in theoretische und praktische Computerlinguistik unterschieden.\n\nComputer sehen Sprache entweder in der Form von Schallinformation (wenn die Sprache akustisch vorliegt) oder in der Form von Buchstabenketten (wenn die Sprache in Schriftform vorliegt). Um die Sprache zu analysieren, arbeitet man sich schrittweise von dieser Eingangsrepräsentation in Richtung Bedeutung vor und durchläuft dabei verschiedene sprachliche Repräsentationsebenen. In praktischen Systemen werden diese Schritte typischerweise sequentiell durchgeführt, daher spricht man vom Pipelinemodell, mit folgenden Schritten:\n\nEs ist allerdings nicht so, dass sämtliche Verfahren der Computerlinguistik diese komplette Kette durchlaufen. Die zunehmende Verwendung von maschinellen Lernverfahren hat zu der Einsicht geführt, dass auf jeder der Analyseebenen statistische Regelmäßigkeiten existieren, die zur Modellierung sprachlicher Phänomene genutzt werden können. Beispielsweise verwenden viele aktuelle Modelle der maschinellen Übersetzung Syntax nur in eingeschränktem Umfang und Semantik so gut wie gar nicht; stattdessen beschränken sie sich darauf, Korrespondenzmuster auf Wortebene auszunutzen.\n\n\n\"Praktische Computerlinguistik\" ist ein Begriff, der sich im Lehrangebot einiger Universitäten etabliert hat. Solche Ausbildungsgänge sind nahe an konkreten Berufsbildern um die informatisch-technische Wartung und Entwicklung von sprachverarbeitenden Maschinen und ihrer Programme. Dazu gehören zum Beispiel:\n\nComputerlinguistik wird an mehreren Hochschulen im deutschsprachigen Raum als eigenständiger Studiengang angeboten. In der deutschen Hochschulpolitik ist die Computerlinguistik als Kleines Fach eingestuft. Es sind Bachelor- wie auch Master-Studienabschlüsse möglich. Zu den bekanntesten Angeboten zählen die Studiengänge der Technischen Universität München, Universität Bielefeld, der Ruprecht-Karls-Universität Heidelberg, der Ludwig-Maximilians-Universität München, der Universität Potsdam, der Universität des Saarlandes und der Universität Zürich.\n\n\n\n\n\n"}
{"id": "932", "url": "https://de.wikipedia.org/wiki?curid=932", "title": "Commodore 64", "text": "Commodore 64\n\nDer Commodore 64 (kurz C64, umgangssprachlich 64er oder „Brotkasten“) ist ein 8-Bit-Heimcomputer mit 64 KB Arbeitsspeicher.\n\nSeit seiner Vorstellung im Januar 1982 auf der \"Winter Consumer Electronics Show\" war der von Commodore gebaute C64 Mitte bis Ende der 1980er Jahre sowohl als Spielcomputer als auch zur Softwareentwicklung äußerst populär. Er gilt als der meistverkaufte Heimcomputer weltweit – Schätzungen der Verkaufszahlen bewegen sich zwischen 12,5 Mio. und 30 Mio. Exemplaren. Der C64 ermöglichte mit seiner umfangreichen Hardwareausstattung zu einem – nach einer teureren Einführungsphase – erschwinglichen Preis in den 1980er-Jahren erstmals Zugang zu einem für die damalige Zeit leistungsstarken Computer.\n\nIm Gegensatz zu modernen PCs verfügte der C64, wie es zu dieser Zeit bei Heimcomputern üblich war, über keinerlei interne Massenspeichergeräte. Alle Programme mussten von externen Laufwerken, wie dem Kassettenlaufwerk Datasette oder dem Diskettenlaufwerk VC1541, oder von einem Steckmodul \"(Cartridge)\" geladen werden. Lediglich Grundfunktionen wie der Kernal, der BASIC-Interpreter und zwei Bildschirmzeichensätze waren in drei ROM-Chips mit Speicherkapazitäten von zweimal acht und einmal vier KB gespeichert.\n\nIm Januar 1981 begann die frühere MOS Technology, jetzt als Commodore Semiconductor Group eine Tochter von Commodore International, mit der Entwicklung eines neuen Chipsatzes für Grafik und Audio für eine Spielkonsole der nächsten Generation. Die Arbeit an den beiden Chips VIC II (Grafik) und SID (Audio) war im November 1981 erfolgreich abgeschlossen. Im Anschluss entwickelte der japanische Ingenieur Yashi Terakura von Commodore Japan auf Basis der beiden neuen Chips den Rechner Commodore Max (in Deutschland als VC 10 angekündigt). Die Produktion wurde jedoch bereits kurz nachdem die ersten Commodore MAX in Japan ausgeliefert worden waren wieder eingestellt.\n\nMitte 1981 machten Robert Russell (System-Programmierer und Entwickler des VC 20) und Robert „Bob“ Yannes (Entwickler des SID) mit der Unterstützung von Al Charpentier (Entwickler des VIC-II) und Charles Winterble (Manager von MOS Technology) dem CEO von Commodore International, Jack Tramiel, den Vorschlag, aus den entwickelten Chips einen wirklichen Low-Cost-Rechner zu bauen, der der Nachfolger des VC 20 werden sollte. Tramiel war einverstanden und erklärte, dass der Rechner einen vergrößerten Speicher von 64 KB RAM, den vollen Adressraum von 16 Bit nutzend, haben solle. Auch wenn zu diesem Zeitpunkt 64 KB RAM noch über 100 US-Dollar kosteten, nahm er an, dass die RAM-Preise bis zur vollen Markteinführung des C64 auf einen akzeptablen Preis fallen würden. Tramiel setzte gleichzeitig das Fristende für die Präsentation des Rechners auf den Beginn der Consumer Electronics Show (CES) im Januar 1982 in Las Vegas. Die Besprechung fand im November 1981 statt, so dass den Entwicklern lediglich zwei Monate blieben, um entsprechende Prototypen des Rechners zu bauen.\n\nDas Projekt hatte zunächst den Codenamen VC-40, der in Anlehnung an das Vorgängermodell VC-20 gewählt worden war. Das Team, welches das Gerät entwickelte, bestand aus Robert Russell, Robert „Bob“ Yannes und David A. Ziembicki. Das Design des C64, Prototypen und einige Beispielsoftware wurden gerade rechtzeitig vor der CES in Las Vegas fertig, nachdem das Team die gesamte Weihnachtszeit (auch an den Wochenenden) durchgearbeitet hatte. Die 40 im Namen sollte die Textauflösung von 40 Zeichen pro Zeile kennzeichnen. Commodore legte diese Auflösung unter anderem deswegen so fest, um unter der Leistungsfähigkeit der für den professionellen Gebrauch vorgesehenen eigenen Rechner der CBM-8000-Serie zu bleiben, die zu der Zeit mit gleicher Prozessorgeschwindigkeit, kleinerer oder gleicher Speicherausstattung, nur monochromen oder deutlich eingeschränkten Farbmöglichkeiten und einem nur wenig leistungsfähigeren BASIC 4.0 angeboten wurden. Ein kennzeichnender Faktor für die professionelle Anwendbarkeit war damals die Möglichkeit, Textzeilen für die Druckausgabe in voller Breite darstellen zu können, wofür 80 Zeichen notwendig waren.\n\nIn der Produktionsperiode des C64 änderte man immer wieder optische und technische Details, um moderne Fertigungsmöglichkeiten auszunutzen und Produktionskosten zu senken. Obwohl sich das Innenleben der ersten C64 deutlich von dem der letzten Version unterscheidet, war es den Entwicklern gelungen, alle Versionen von Seiten der Software beinahe hundertprozentig kompatibel zueinander zu halten – was bedeutete, dass die Leistungsdaten des Rechners während des Produktionszyklus nicht gesteigert wurden. Beispielsweise wurde das Hauptplatinenlayout mehrfach geändert sowie CPU, Grafikchip, Soundchip und andere Bauteile überarbeitet. Auch die zur Verschaltung innerhalb des Rechners notwendigen Logikchips fasste man zusammen und integrierte sie in einem Custom-Chip.\n\nVom C64 gab es im Gegensatz zu anderen damaligen Heimcomputern keine Nachbauten aus Ostblock-Ländern, Lateinamerika oder Fernost. Das ist vor allem in der hochintegrierten Bauweise mit Custom-Chips und in der vertikalen Integration der Firma Commodore begründet – von der Chipfertigung über Chipdesign und Systemdesign bis zum Gehäusedesign war alles in einer Hand, wodurch diese Chips für Nachbauer nicht erhältlich waren.\n\n\"Commodore Business Machines\" (CBM) hatte vor dem C64 bereits erfolgreich den Bürorechner PET 2001 und seine Nachfolger, aber auch schon den Heimcomputer VC 20 eingeführt. Firmengründer Jack Tramiel prägte die Formel „We need to build computer for the masses, not the classes!“, was ihm mit dem C64 letztlich auch gelang.\n\nUm die Neuentwicklung in das vorhandene Produktangebot einbinden zu können, entschied sich die Marketingabteilung für den Namen „C64“, was für „Consumer“ und die Größe des verwendeten Speichers in KB stehen sollte. Für den amerikanischen Markt waren bereits nach gleichem Schema benannte Modelle, der B(usiness)256 bzw. der P(ersonal Computer)128, geplant. Letzterer gehörte in die in Europa als Commodore CBM 500 veröffentlichte Reihe und ist nicht identisch mit dem später erschienenen C128.\n\nIm September 1982 kam der C64 für 595 US$ auf den amerikanischen und Anfang 1983 zum Startpreis von 1.495 DM (in heutiger Kaufkraft €) auf den deutschen Markt und war in Deutschland, wie in allen wichtigen Märkten der Welt (mit Ausnahme von Japan), sehr erfolgreich. Schon 1983 sank der Preis auf 698 DM.\n\nHauptkonkurrent war der in den USA stark vertretene \"Atari 800 XL\". Viele Spiele waren gleichzeitig auf einer 5¼-Zoll-Diskette für beide Systeme erhältlich, wie etwa das Computer-Rollenspiel \"Alternate Reality\" (Vorderseite C64, Rückseite Atari), was als Hinweis auf die Dominanz der beiden Marken angesehen werden kann. Trotz der Konkurrenz durch Atari und vieler anderer Heimcomputer in dieser Zeit (TI-99/4A, Apple II, Sinclair ZX81, ZX Spectrum, Dragon 32) beurteilten viele Konsumenten das Preis-Leistungs-Verhältnis des C64 zum Beginn seiner Auslieferung günstig. In Kombination mit der rasch ansteigenden Zahl an Softwaretiteln für den C64 entwickelte sich der Rechner zum Erfolg. Auch trug die Tatsache, dass der Computer nicht nur in Fachgeschäften, sondern auch in Kaufhausketten, Versandhäusern (z. B. Quelle), Supermarktketten (z. B. allkauf) und Computer-Versandhäusern (z. B. Vobis) zum Verkauf stand, dazu bei, dass das Gerät in kurzer Zeit ein voller Erfolg wurde. Mit dem Aufstieg des C64 als Heimcomputer kam auch zugleich der endgültige Fall der bis dato am weitesten verbreiteten Konsole, dem Atari VCS 2600.\n\nCommodore produzierte den C64 etwa elf Jahre lang; über 22 Millionen Stück wurden verkauft (andere Quellen geben 17 Millionen an). Damit ist der C64 der meistverkaufte Computer der Welt.\n\nDer Prozessor ist ein 6510 (8500 beim C64C/II), eine Variante des 6502 von MOS Technology. Commodore hatte diese Firma Mitte der 1970er-Jahre aufgekauft, um über ein eigenes Halbleiterwerk zu verfügen. Der 6510 besitzt im Gegensatz zum 6502 einen 6 Bit breiten bidirektionalen I/O-Port, der sich über die Speicheradressen 0 und 1 ansprechen lässt und beim C64 unter anderem dazu genutzt wird, um in einzelnen Speicherbereichen zwischen RAM, ROM und dem I/O-Bereich durch Bank Switching umzuschalten.\n\nDer Prozessor arbeitet mit einer Taktfrequenz von 0,985249 MHz in der PAL-Version und 1,022727 MHz in der NTSC-Version. Der Unterschied ergibt sich daraus, dass im C64 aus der Schwingungsfrequenz nur eines Quarz-Oszillators alle benötigten Frequenzen einfach abgeleitet werden und dass die Farbträgerfrequenzen der beiden Farbübertragungssysteme unterschiedliche Werte haben, die eingehalten werden müssen. In der NTSC-Version stehen so mehr Taktzyklen pro Rasterzeile in der Grafikausgabe zur Verfügung, und auch insgesamt ist die CPU etwas schneller. Dafür hat NTSC weniger Zeilen pro (Halb-)Bild, nur 262 im Vergleich zu 312 bei PAL. Daher müssen Programme, die den Rasterzeileninterrupt (s. u.) zur bildsynchronen Ablaufsteuerung verwenden, austauschbare Codeteile für beide C64-Versionen besitzen oder gleich in zwei verschiedenen Versionen vorliegen.\n\nDer C64 verfügt über 64 KB RAM. Davon sind 38911 Bytes für BASIC-Programme nutzbar. Die Größe des Speichers war für die damalige Zeit üppig (der zwei Jahre ältere Vorgänger VC 20 hat nur 5 KB Arbeitsspeicher, wovon für die Programmiersprache BASIC 3584 Byte nutzbar sind). Zwei Bytes (0 und 1) sind nicht für das RAM nutzbar, hier befindet sich der Prozessorport des 6510.\n\nDer C64 verfügt über 20 KB ROM. Etwa 9 KB davon enthalten in nahezu unveränderter Form den BASIC-V2-Interpreter des älteren Commodore VC 20 (erschienen 1980), der ursprünglich von der Firma Microsoft stammt. In weiteren knapp 7 KB ist ein Betriebssystem, der sogenannte Kernal, untergebracht, welcher die Tastatur, den Bildschirm, die Kassettenschnittstelle, die RS-232-Schnittstelle sowie eine serielle IEC-Schnittstelle (den CBM-Bus) zur Ansteuerung von Druckern, Diskettenlaufwerken usw. verwaltet. Auch dieses stammt ursprünglich von älteren Commodore-Maschinen und wurde an die veränderte Hardware des C64 angepasst. Die restlichen 4 KB enthalten zwei Zeichensätze à 256 Zeichen in 8×8-Matrixdarstellung für den Bildschirm. Die Zeichensätze entsprechen dem Commodore-eigenen PETSCII-Standard und enthalten deshalb keine deutschen Umlaute.\n\nUm über verschiedene Versionen hinweg auf Maschinensprachenebene kompatibel zu bleiben, war ganz am Ende des ROM-Bereichs (also kurz vor codice_1) eine Sprungtabelle angelegt, über die man die wichtigsten Betriebssystemroutinen aufrufen konnte. Commodore behielt diese Sprungtabelle vom PET 2001 bis über den C64 hinaus bei. Die Kompatibilität von Anwendungssoftware hat sich dadurch nicht besonders gesteigert, weil viele Programmierer diese kompatible Methode des Aufrufs schlichtweg ignoriert haben und sie ohnehin nur für rein textbasierte Programme brauchbar war. – Beispiel: Der Aufruf codice_2 gibt auf jedem Commodore-8-Bit-Rechner den Inhalt des Akkumulators als Zeichen auf den Bildschirm aus.\nDer Grafikchip des C64 ist ein MOS 6569/8565 (PAL) bzw. MOS 6567 (NTSC) und wird VIC (Video Interface Controller) genannt. Er bietet:\n\nDa der VIC nur 14 Adressleitungen besitzt, kann er nur 16 KB des zur Verfügung stehenden Speichers auf einmal ansprechen. Die zwei fehlenden Adressbits steuert der zweite im C64 verbaute CIA6526-Chip bei. Diese vier Speicherseiten zu 16 KB verhalten sich nicht gleich – im Speicherbereich codice_3 bis codice_4 (bzw. codice_5 bis codice_6) wird vom VIC stets das Zeichengenerator-ROM ausgelesen. In diesen Bereichen können daher auch kein Bildschirmspeicher (Text oder Bitmap) und keine Spritedaten abgelegt werden. Umgekehrt muss in den beiden anderen Speicherseiten im Textmodus ein Zeichengenerator im RAM abgelegt werden.\n\nDas Farb-RAM, das aus Sicht des Hauptprozessors an den Adressen codice_7 bis codice_8 eingeblendet werden kann, ist aus Timinggründen ein einzelner 1024×4-Bit-SRAM-Chip (µPD2114), der vier eigene Dateneingänge in den VIC besitzt. Das Farb-RAM muss daher nicht in den „normalen“ VIC-Adressraum eingeblendet werden. Genau genommen besitzt der C64 damit 66048 Byte RAM. Da die letzten 24 Adressen nicht für die Farbdarstellung gebraucht werden, kann man die dahinterliegenden Speicherzellen für Sonderzwecke nutzen.\n\nDer VIC sorgt ebenfalls, wie damals für die Grafikhardware üblich, durch das regelmäßige Auslesen aller Speicherseiten für den nötigen Refresh der DRAM-Chips des C64.\n\nDer C64 ist dank der Rasterzeileninterrupts und des Grafikchipdesigns recht flexibel im Bildaufbau.\nViele der hardwaretechnischen Einschränkungen können durch kreative Programmierung und Ausnutzung von vom Hersteller nicht explizit implementierten Nebeneffekten umgangen werden. So lassen sich beispielsweise verschiedene Darstellungsmodi mischen (z. B. obere Bildschirmhälfte Textdarstellung mit Scrolling, untere Bildschirmhälfte Grafik) und auch die acht Sprites mehrfach in verschiedenen Bildbereichen verwenden, so dass viele Spiele weitaus mehr als acht Sprites darstellen können. Durch Ausnutzung von undokumentierten Videochip-Eigenschaften ist auch die Verwendung von zusätzlichen Videomodi möglich, die die Beschränkungen in der Farbwahl und Auflösung teilweise aufheben. Auch der Bildschirmrahmen kann mit einigen Tricks zur Darstellung von Grafik benutzt werden.\n\nDer Basic-Interpreter stellt keine Befehle zur Programmierung der hochauflösenden Grafik bereit, so dass deren Nutzung dem normalen Anwender verschlossen bleibt. Abhilfe schaffen kommerzielle Basic-Erweiterungen wie Simons’ Basic, s. u.\n\nKlänge werden über den dreistimmig polyphonen Soundchip MOS Technology SID 6581 (buskompatibel mit der Prozessorfamilie 65xx) erzeugt, welcher dem C64 damals revolutionäre, weit über andere Heimcomputer hinausgehende Möglichkeiten zur Klangerzeugung verlieh. Spätere C64-Varianten enthielten den 8580.\n\nDer SID besitzt drei universell einsetzbare monophone Stimmen mit einer in 65536 Stufen einstellbaren Grundfrequenz von 0 bis 4000 Hz und 48 dB Aussteuerung, die \"gleichzeitig\" in subtraktiver Synthese vier Schwingungsformen (Dreieck, Sägezahn, Rechteck in 4096 Stufen einstellbarer Pulsbreite, sowie Rauschen) erzeugen können. Die Lautstärke jeder Stimme kann einzeln mittels dreier programmierbarer ADSR-Hüllkurvengeneratoren mit exponentiellem Kurvenverlauf eingestellt werden. Weiterhin ist eine Synchronisierung von zwei oder allen drei Oszillatoren möglich. Ein Ringmodulator ergibt weitere Effekte. Eine der Stimmen kann außerdem wahlweise ausschließlich zur Modulation der anderen Stimmen verwendet werden.\n\nWeiterhin besitzt der SID ein subtraktives Multimode-Filter (Tiefpass, Hochpass, Bandpass oder Notch Filter), durch das die internen Stimmen sowie eine über die Monitorbuchse des C64 zumischbare externe Quelle geleitet werden können.\n\nDa die Lautstärke der Tonwiedergabe in 16 Stufen eingestellt werden konnte, benutzten schon bald einige Programme den Lautstärkesteller als D/A-Wandler, um Samples, zum Beispiel Sprache oder Schlagzeug, wiederzugeben. Bekannte Beispiele dafür sind das Spiel zum Film „Ghostbusters“ und das Musikspiel „To Be on Top“. Die Tonqualität war dabei allerdings nicht besonders gut, außerdem gab es eine Inkompatibilität zwischen den ursprünglichen und den späteren C64-Versionen: Der später verbaute SID II (MOS 8580) schaltete seinen Ausgang nur durch, wenn auf mindestens einer Stimme ein Ton abgespielt wurde. Dadurch verringerte sich zwar das Grundrauschen bei fehlender Tonwiedergabe, reine Samples ohne Hintergrundmusik wurden nur noch sehr leise abgespielt. Neuere Programme berücksichtigten diese Tatsache, Anpassungen für ältere Software gab es in der Regel nicht.\n\nDurch geschicktes Mischen unterschiedlicher Samples war auf Softwareebene außerdem die Wiedergabe mehrerer Samples möglich; dies bedingte jedoch zwangsläufig eine Einschränkung der Wiedergabegenauigkeit (resolution) bzw. der Abspielrate (sample/playback rate), das heißt, die so erzeugten Töne waren weniger gut aufgelöst und „ungenauer“. Eine Reihe von bekannten Spielemusikprogrammierern bediente sich dieser Technik.\n\nNeben der Audiowiedergabe besaß der SID noch zwei Analogeingänge mit niedriger Abtastrate, die im C64 zum Anschluss von Paddles oder einer speziellen Maus mit Analogausgang genutzt wurden.\n\nZum Ende der C64-Ära wurden in Bastlerkreisen Methoden entwickelt, um den C64 stereofähig zu machen. Dazu wurde ein zweiter SID eingebaut und zur Ansteuerung die Tatsache ausgenutzt, dass der Adressbereich des SID mehrfach gespiegelt ist. Durch geeignete Adress-Selektion konnten so beide SIDs unabhängig voneinander angesteuert werden. Diese Lösung wurde als Bauanleitung in der \"64'er\" beschrieben, kam jedoch nie kommerziell auf den Markt.\n\nDer C64 bietet mehrere Schnittstellen und war daher bei Hardware-Bastlern beliebt (von links nach rechts, von der Rückseite aus gesehen):\n\nFür den C64 stand eine große Auswahl an Peripheriegeräten zur Verfügung.\n\nDas Datasette (auch Datassette) genannte Bandlaufwerk war die günstigste Lösung für Datenspeicherung am C64. Es benutzt normale Kompaktkassetten. Meist war Software auf Kassetten günstiger als entsprechende Diskettenversionen. Anders als in Deutschland, wo das Diskettenlaufwerk (trotz höherer Anschaffungskosten) sehr verbreitet war, war die Datassette in Großbritannien das dominierende Datengerät. Von Commodore gab es das Datassetten-Laufwerk VC-1530, welches mit dem C64 kompatibel war. Auch andere Hersteller boten Datasetten-Laufwerke für den C64 und den C128 an. Lade- und Speichervorgänge geschehen sehr langsam und sind durch notwendige Spulvorgänge umständlich. Schnelllader wie Turbo Tape verringern die Ladezeiten etwa um den Faktor 10.\n\nDieses Laufwerk vom Typ VC1541 war das Standardlaufwerk für den C64 und wurde vom Großteil der Benutzer verwendet. Es benutzt die damals sehr weit verbreiteten 5¼-Zoll-Disketten mit doppelter Aufzeichnungsdichte \"(Double Density).\" Das Laufwerk arbeitet einseitig und bietet etwa 165 kB Speicherkapazität pro Diskettenseite, angegeben werden jedoch die zur Verfügung stehenden „Blöcke“, von denen es standardmäßig 664 gibt. Um die Rückseite beschreiben zu können, muss die Diskette dem Laufwerk entnommen und gewendet werden. Dafür gab es beidseitig beschreibbare Disketten mit Aussparungen für die Schreibfreigabe auf beiden Seiten. Jedoch konnte man auch die preisgünstigeren, offiziell nur einseitig beschreibbaren Disketten auf der Rückseite beschreiben. Dazu musste jedoch vorher seitlich eine zweite Kerbe ausgestanzt werden, beispielsweise mittels eines Diskettenlochers oder mit einem Teppichmesser. Die Daten werden von den Laufwerken als schreibgeschützt erkannt, wenn diese Kerbe überklebt wird. Entsprechende Aufkleber lagen den Disketten bei.\n\nÄltere Versionen der VC1541 hatten keine Möglichkeit zu erkennen, wann der Schreib–Lese-Kopf am unteren Ende („Spur 0“) angekommen ist, und hatten deshalb eine mechanische Sperre. Das führte zu dem bekannten mechanischen „Rattern“ des Laufwerks bei der Formatierung einer Diskette, da der Schreib-Lese-Kopf so bis zu fünfmal an den Anschlag fuhr – dadurch konnte er verstellt werden. Neuere Versionen hatten eine Lichtschranke, um das Problem zu lösen; da jedoch das ROM des Laufwerks geändert wurde, führte das teilweise zu Inkompatibilitäten mit Schnellladeprogrammen und Kopierschutzmechanismen.\n\nDas Laufwerk war ein eigenständiger Computer mit eigenem Prozessor und Speicher. Anders als praktisch alle anderen Firmen hatte Commodore das DOS als ROM im Laufwerk selbst realisiert, anstatt es in den Speicher des Computers zu laden. Es gab Programme, die Teile der Rechenarbeit auf das Laufwerk auslagerten und somit eine Art Parallelprogrammierung ermöglichten; wegen des kleinen Speichers des Laufwerks war das nur sehr eingeschränkt nützlich. Ebenfalls gab es Jux-Programme, die durch kreative Programmierung des für die Schreib-Lese-Kopfbewegung zuständigen Schrittmotors sogar Musik mit dem Laufwerk erzeugten.\n\nVon dem Laufwerk wurden drei Haupt- und viele Untervarianten hergestellt. Fremdhersteller boten Klone an, die zwar preisgünstiger, aber wegen des aus Urheberrechtsgründen abweichenden ROMs meist nicht vollständig kompatibel waren.\n\nDie Geschwindigkeit der Diskettenoperationen war aufgrund des geringen Speicherausbaus der Laufwerke, der seriellen Schnittstelle sowie umständlicher Programmierung der DOS-Funktionen – das 1541-DOS wurde aus dem der Doppelprozessor-Doppelfloppies CBM 8050 abgeleitet – sehr langsam, so dass viele verschiedene Turbolader als Software- oder als Hardwarebeschleuniger entwickelt wurden.\n\nDiese Beschleuniger schrieben als erstes eigene in Assembler entwickelte Routinen in den Speicher des Laufwerks, die anschließend zusammen mit im Computer ablaufenden Routinen den Datentransfer realisierten.\n\nDas Laufwerk vom Typ VC1581 fristete im Zusammenhang mit dem C64 aufgrund seiner Inkompatibilität zur VC1541 nur ein Schattendasein – trotz seines gegenüber der VC1541 erheblich gesteigerten Speichervermögens von 800 kB auf 3½-Zoll-DD-Disketten. Wegen Kopierschutzmaßnahmen erforderten sehr viele Programme das VC1541-Laufwerk, so dass dem Modell 1581 kein Erfolg beschert war. Wie die VC1541 war auch dieses Laufwerk technisch gesehen ein eigenständiger Computer.\n\nMäuse spielten als Eingabegeräte beim C64 eine eher untergeordnete Rolle, da sie sich erst Jahre nach ihrer Einführung etablierten. Es gab nur wenige Programme, die sie unterstützten bzw. für Mausbenutzung (anstatt Joystick) ausgelegt waren, so z. B. das grafikorientierte Betriebssystem GEOS, Hi-Eddi und Printfox.\n\nNeben der Tastatur sind Joysticks die wichtigsten Eingabegeräte am C64, denn fast alle Spiele und viele Anwendungen lassen sich nur mit ihnen steuern. Beim C64 wird der damals recht verbreitete Atari-Standard für Joysticks unterstützt, so dass die gleichen Joysticks wie an sehr vielen anderen Rechnern verwendet werden konnten. Zwar stellte Commodore eigene Joysticks her, beliebter und verbreiteter waren jedoch Spectravideos QuickShot-Joysticks, Joysticks von QuickJoy sowie – aufgrund seiner Robustheit – der Competition Pro.\n\nGrafiktablett für den C64, das für das Grafikprogramm KoalaPainter entwickelt worden war, aber auch von einigen anderen Programmen genutzt wurde.\n\nLichtgriffel sind „Stifte“, die zum Zeichnen direkt auf dem Monitor verwendet werden. Wie auch Paddles hatten sie auf dem C64 kaum eine Bedeutung.\n\nEine Lightgun ist von der Funktionsweise ähnlich wie die Lichtgriffel, jedoch meist in der Form einer Pistole und für Spiele gedacht. Auch dieses Eingabegerät war beim C64 kaum von Bedeutung.\n\nPaddles sind Eingabegeräte, die vor allem in den 1970er-Jahren bei vielen Videospielen verbreitet waren und so auch ihren Weg zum C64 fanden. Bis auf wenige der frühen C64-Spiele und einige spätere Ausnahmen wie Arkanoid hatten Paddles aber kaum eine Bedeutung auf dem C64.\n\nVon der Firma Scanntronik waren ein Schwarz-Weiß-Scanner erhältlich, der auf den Druckkopf geeigneter Nadeldrucker aufgesteckt wurde und das zu scannende Bild zeilenweise abtastete, während es von der Druckerwalze transportiert wurde, sowie ein Handscanner.\n\nCommodore verkaufte seine eigenen Druckermodelle MPS 801, -802, -803 und -1230 (hauptsächlich Seikosha-OEM, z. B. der baugleiche GP 500 VC). Diese Matrixdrucker können im Textmodus aufgrund technischer Limitierungen (Unihammer-Technik beim MPS 801/803) bzw. der Tatsache, dass nur 8 der 9 verfügbaren Nadeln angesteuert wurden (MPS 802), keine echten Unterlängen drucken. Für dieses Problem gab es aber einige Softwarelösungen im Angebot. Fremdhersteller produzierten einige spezielle Drucker für Commodore-Rechner, die wie ein Diskettenlaufwerk am seriellen Bus des C64 angeschlossen werden, z. B. der sehr beliebte Star LC10. Weit verbreitet waren zwei weitere Lösungen: Man konnte gängige Drucker mit Centronics-Schnittstelle über einen speziellen Konverter an den seriellen IEC-Bus des C64 anschließen und dann wie Commodore-Drucker ansteuern, oder man konnte sie mittels eines einfachen Kabels mit dem Userport verbinden, brauchte dann aber eine Software, die eine spezielle Unterstützung für diesen Anschlussmodus bot. In einigen Floppy-Schnellladern (z. B. SpeedDOS) waren solche Routinen bereits integriert. Es gab elektrische Schreibmaschinen, die von diesen Schnittstellen angesteuert und als Drucker verwendet werden konnten. Auf Treiberebene existieren zwei Standards, der MPS-801/803-Modus sowie der Epson FX-80-Modus (ESC-P) für Neun-Nadel-Drucker. Der Standard NEC-P6 wurde nur selten unterstützt, da die meisten NEC-P6-kompatiblen Drucker auch FX-80-kompatibel sind, wenngleich dann die Ausgabe lediglich mit neun Nadeln erfolgte. Die überwiegende Mehrheit der damaligen Drucker waren Nadeldrucker mit 7, 8, 9 oder 24 Nadeln, wobei 24-Nadel-Drucker aufgrund ihres hohen Preises eher selten waren und nur mit Spezialsoftware eingesetzt werden konnten. Im untersten Preissegment fanden sich einige Thermodrucker, die aber wegen ihres schlechten Druckbildes, dem notwendigen teuren Thermopapier und der schlechten Haltbarkeit des Drucks keine sehr weite Verbreitung fanden. Tintenstrahldrucker, Thermotransferdrucker und Laserdrucker waren zu dieser Zeit noch sehr teuer und somit selten bei Heimcomputeranwendern zu finden.\nMit Hilfe des HF-Ausgangs konnte der C64 über die Antennenbuchse an jeden Fernseher angeschlossen werden, so dass kein zusätzlicher Monitor für den Betrieb des Rechners nötig war. Die Bildqualität war aufgrund der Umsetzung naturgemäß bescheiden.Für den C64 und andere damalige 8-Bit-Heimcomputer gab es eine recht große Auswahl an Video-Monitoren. Hier sind vor allem der Commodore 1701 und der Philips CM8833, mit Auflösungen von 300×300 Pixeln, sowie die kompatiblen Monitore der Amiga-Baureihe zu nennen, die aufgrund ihrer feineren Lochmaske ein schärferes Bild lieferten.\n\nWeniger verbreitet war der Plotter Commodore VC-1520, ein einfacher Stiftplotter für Endlos-Rollenpapier. Die Papierrolle war ca. 10 cm breit. Das Gerät bot die Möglichkeit der einfachen Textausgabe in Rot, Grün, Blau und Schwarz. Außerdem konnten Zeichnungen in den gleichen Farben ausgegeben werden.\n\nDamals war der Betrieb von nicht durch die Deutsche Bundespost zertifizierten – und das waren die meisten – Modems am deutschen Telefonnetz illegal, so dass man anstelle dieser Modems sogenannte Akustikkoppler verwenden musste. Die Übertragung war damit allerdings sehr langsam, typischerweise 300 bis 1200 Bit/s und zudem sehr fehleranfällig, da Nebengeräusche oft zu Übertragungsfehlern führten.\n\nEs gab spezielle C64-Modems, die an den Userport des C64 angeschlossen wurden, sowie andere, die mit Hilfe einer (gegebenenfalls am Expansionsport anzuschließenden) RS-232-Schnittstelle am C64 betrieben werden konnten.\n\n2003 kam von \"individual Computers\" ein Netzwerkadapter für den C64 unter der Bezeichnung \"RR-Net\" auf den Markt. Für den Betrieb benötigt man allerdings das \"Retro Replay Cartridge\" oder das \"MMC64\", welches ebenfalls von individual Computers herausgebracht wurde.\n\nEin auf Flashspeicher basierendes Modul, das viele ROM-basierende Module ersetzen kann. Die Grundidee war dabei, die „großen“ Ocean-Spielmodule ersetzbar zu machen, indem der Inhalt eines solchen Moduls in den Flashspeicher geschrieben wird und sich das EasyFlash dann wie ein Original-„Ocean-Modul“ verhält. In der Entwicklungsphase wurden dann noch weitere Modulformate implementiert, so dass ein EasyFlash fast alle Spielmodultypen korrekt emulieren kann. Das EasyFlash verfügt über einen 1 MB großen Flashspeicher, der mittels des C64 und Diskettenlaufwerk oder größere Massenspeicher beschrieben wird. Daraufhin wurde Software (EasyLoader) entwickelt, die es ermöglicht, beliebige Programme oder auch Modulkopien für den C64 auf den Flashspeicher zu schreiben und über ein Startmenü auszuwählen. In Betrachtung dieser Möglichkeiten wurden mittlerweile viele Spieletitel auf das EasyFlash umgesetzt, so dass die Diskettenladezeiten entfallen und sogar die Möglichkeit gegeben ist, die Spielstände auf dem EasyFlash zu speichern. Die Umsetzung von „Prince of Persia“ für den C64 basiert auf dem Easyflash.\nDiese Karten erlaubten den direkten Zugriff auf ein oder mehrere EPROMs zum Aufruf fest gespeicherter Programme und waren meist elektronisch umschaltbar.\nIn den 1990er-Jahren entwickelte die Firma CMD neue Diskettenlaufwerke mit einer Speicherkapazität von bis zu 2880 kB. In den späten 1990er-Jahren entwickelten technisch versierte Bastler eine IDE-Schnittstelle. Sowohl an der IDE-Schnittstelle als auch an der SCSI-Festplatte lassen sich weitere Geräte wie CD-ROM-Laufwerke oder Compact-Flash-Karten betreiben. Die beiden Laufwerke CBM D9060 und CBM D9090 waren die einzigen IEEE-488 Festplatten, welche von Commodore für die PET und CBM 8-Bit Computer hergestellt wurden.\n\nDas MMC64 ist ein Steckmodul für den C64, das es ermöglicht, mit dem C64 MMC- und SD-Speicherkarten zu lesen und zu beschreiben. Programme können direkt von der Speicherkarte geladen und ausgeführt werden. Jedoch können Programme nicht vom Speicher des C64 auf die SD-Karte (oder MM-Karte) geschrieben werden. Damit kann für Selbstprogrammierer die MMC64 die Commodore Floppy 1541 als Speichermedium nicht ersetzen. Laden und danach Speichern funktioniert nur mit einer alten 1541. Das MMC64 ist daher eher für die Ausführung (Wiedergabe) von fertigen Spielen (oder auch eigenen Programmen) gedacht. Das Laden eines solchen Spiels wird in wenigen (Milli-)Sekunden bewerkstelligt. Darüber hinaus existieren zahlreiche Plugins, die es beispielsweise ermöglichen, sogenannte Diskettenimages von Disketten zu erzeugen oder diese auf Diskette zu schreiben (immer nur als ganze Image-Dateien).\n\nDas MP3@64 ist ein MP3-Modul für das MMC64.\n\nSie waren sehr verbreitet. Das hat vor allem mit der geringen Ladegeschwindigkeit der 1541 zu tun, die sich per Software auf die 10- bis 20-fache Geschwindigkeit steigern ließ. Den Anfang machten einfache Schnelllader-Cartridges, schnell kamen weitere Funktionen dazu, so dass am Ende Cartridges wie \"The Final Cartridge 3,\" \"Hypra Load II\" oder \"Action Replay\" mit einer großen Anzahl von Funktionen aufwarteten. Neben dem Schnelllader sind meist noch diverse BASIC-Erweiterungen, Funktionstastenbelegungen, Freezerfunktionen, Druckfunktionen, Maschinensprachemonitor und einiges mehr vorhanden. Auch heute wird noch ein solches Cartridge hergestellt und verkauft: das \"MMC Replay.\" Es ist wie sein (mittlerweile eingestellter) Vorgänger Retro Replay Cartridge weitgehend \"Action-Replay\"-kompatibel und um Fehler bereinigt. Das Modul verwendet höher integrierte und modernere Bauteile und bietet mehr Speicher, mehr Funktionen und hat die Möglichkeit des ROM-Updates, zusätzlich wurde die Funktionalität des (ebenfalls eingestellten) MMC64 integriert.\n\nUm den C64 zum Steuern von elektronischer Hardware zu benutzen, existierten diverse Relais-Karten. Diese wurden meist an den Userport angeschlossen und erlaubten so die Ansteuerung von acht Relais.\n\nEin Steckmodul, das es ermöglichte, durch Drücken eines Tastenschalters den C64 zu resetten. Die überwiegende Anzahl der in Maschinencode geschriebenen Programme konnte nur verlassen werden, indem der Computer aus- und wieder eingeschaltet wurde. Ebenso musste bei Abstürzen vorgegangen werden, nur selten funktionierte das eigentlich hierfür vorgesehene gleichzeitige Drücken von und . Die übermäßige Benutzung des Ein- und Ausschalters war nicht nur lästig, sondern konnte auch zu Defekten führen.\n\nEs gab eine Steckkarte für den C64, \"The Final Chesscard,\" die einen vollständigen Computer mit einer Schachspielsoftware (Schachcomputer) enthielt, der C64 übernahm dabei die Darstellung des Spiels und die Eingabe der Züge.\n\nEs kam vor, dass für bestimmte Anwendungen die 64 KB Hauptspeicher des C64 nicht ausreichend waren, so dass zahlreiche Speichererweiterungen hergestellt wurden, die meistens an den Expansionsport angeschlossen wurden. Von Commodore selbst vertrieben wurde die REU (RAM Expansion Unit CBM1700, CBM1764 und CBM1750). Alle Speichererweiterungen für den C64 konnten nur von Software ausgenutzt werden, die speziell darauf ausgelegt war; das schloss vor allem die meisten Spiele aus. Weitere, meist nur von GEOS oder wenigen Spezialanwendungen (z. B. \"Pagefox\") unterstützte Speichererweiterungen spielten nur eine untergeordnete Rolle.\n\nAls Bausatz wurden sogenannte Teleclubdecoder vertrieben. Damit konnte die recht einfache Verschlüsselung des Pay-TV-Senders Teleclub aufgehoben werden.\n\nEs gab einige wenige Versuche, dem C64 mit Hilfe eines schnelleren Prozessors zu mehr Leistung zu verhelfen. Als erstes kam die Erweiterung \"Turbo Process\" von \"Roßmöller\" auf den Markt, die einen 65C02-Prozessor mit 4 MHz hatte. Der direkte Nachfolger dieser Karte war die \"Flash 8,\" mit einer 8 MHz schnellen 65816-CPU. Beide Karten sind teilweise inkompatibel zu existierender Software und überdies im Betrieb sehr instabil, so dass sie lediglich ein Nischendasein fristeten. Erst der SuperCPU, einer Beschleunigerkarte basierend auf einem mit 20 MHz getakteten 65816-Prozessor, war ein gewisser Erfolg beschieden. Eine Prozessorkarte mit einem Z80, die den C64 zu einem CP/M-Computer werden ließ, wurde bei der Markteinführung des C64 stark propagiert, erreichte aber wegen der sehr geringen CPU-Geschwindigkeit und der schlechten Kompatibilität zu anderen CP/M-Rechnern keine große Verbreitung. Insbesondere verlangten fast alle kommerziellen CP/M-Programme eine Zeilenlänge von 80 Zeichen, was der C64 von Haus aus nicht bieten konnte.\n\nEs gab Erweiterungen, mit deren Hilfe der C64 die Videotexttafeln der Fernsehsender auslesen konnte.\n\nIntern gab es 16 verschiedene Versionen des C64-Mainboards.\n\nDer C64 wurde anfangs in einer beigefarbenen „Brotkasten“-Gehäuseform, zunächst mit orangefarbenen, dann mit dunkelbraunen Funktionstasten produziert. Urversionen mit den orangen Funktionstasten und dem silbernen Commodore-Typenschild gehören zu den Raritäten. Ein Großteil der deutschen Produktion wurde im Commodore-Werk in Braunschweig montiert.\n\nDer Educator 64 ist eine spezielle Version des C64 im PET-Gehäuse, er ist vor allem für Schulen gedacht. Das Modell ist auch als „4064“ oder „PET 64“ bekannt. Diese Version konnte sehr preisgünstig angeboten werden, da instand gesetzte Hardware von reklamierten C64 verwendet wurde.\n\nDer SX-64/DX-64 ist eine tragbare Version des C64 mit einem (SX-64) oder zwei (DX-64) eingebauten 1541-kompatiblen Diskettenlaufwerken und eingebautem 5-Zoll-Farbmonitor. Der Rechner war nicht hundertprozentig kompatibel, man konnte aber C64-ROMs anstelle der leicht geänderten SX-64-ROMs einsetzen. Aufgrund niedriger Absatzzahlen wurden jedoch nur wenige Geräte hergestellt: Vom SX-64 etwa 9000 Stück, vom DX-64 noch weniger.\n\nDie „Gold Edition“ des C64 besaß ein goldfarbenes Brotkasten-Gehäuse und war auf einer Acryl-Platte mit einem Emblem montiert. Anlass war der einmillionste verkaufte C64 in Deutschland. Produziert wurde die Kleinserie 1986 in sehr geringer Stückzahl von etwa 400 Stück, andere Quellen geben 1000 Stück an. Bei einer Feier am 5. Dezember 1986 im Münchner BMW-Museum wurde dieser C64 an wichtige Personen innerhalb des Unternehmens sowie Journalisten und Händler vergeben, die maßgeblich zum Erfolg des C64 beigetragen hatten. Die speziell in Braunschweig gefertigte „Gold Edition“ wurde damals von Hand ab Nummer 1000000 beschriftet. Dieses Gerät ist sehr selten und ein begehrtes Sammlerstück.\n\nDas Modell C64C hat ein neues, flacheres Gehäuse, das dem Gehäuse des C64-Nachfolger C128 nachempfunden ist und trägt die Aufschrift „Personal Computer“. Zudem ist es mit leicht überarbeiteter, kostenreduzierter Hardware ausgestattet – die Hauptplatine ist kleiner. In Deutschland wird der C64C oft als „C64-II“ bezeichnet.\n\nDer C64G besitzt wieder die alte Gehäuseform („Brotkasten“), diesmal grau/beige mit heller Tastatur und kostenreduzierter Hauptplatine. Die Grafikzeichen der Tastatur sind auf der Oberseite statt auf der Stirnseite der Tasten abgebildet. Das G in der Bezeichnung steht für Germany, da in Deutschland die Brotkastenform sehr beliebt war und man dem Wunsch der Kunden mit diesem Modell nachkommen wollte.\n\nDer Aldi-C64 ist dem C64G ähnlich. Er war nur in Deutschland erhältlich und der Vertrieb erfolgte über Discounter (zum Beispiel die Aldi-Gruppe). Durch das Wegfallen des 12-V-Spannungsreglers bei den 250469-Boards wurde in der Zeitschrift \"64er\" fälschlicherweise geschrieben, der neue SID 8580 würde ausschließlich 5 Volt Gleichspannung benötigen. Die 9 Volt Wechselspannung würde daher nicht mehr benötigt und am Userport fehlen. Diese Angaben waren falsch. Der neue SID 8580 benötigte zusätzlich zu den 5 Volt Gleichspannung auch 9 Volt Gleichspannung, die aus den 9 Volt Wechselspannung erzeugt wurde. Auch für das Taktsignal (50 Hz) der beiden CIA-Echtzeituhren und für die Motoransteuerung der Datasette wurde die 9 Volt Wechselspannung benötigt. Die neuen flacheren Gehäuse und hochintegrierten Platinen waren bei Bastlern unbeliebt, da sie mit internen Erweiterungen von Fremdherstellern nicht mehr kompatibel waren.\n\nDer Commodore 64 GS (GS = Games System) ist ein 1987 als Spielekonsole herausgebrachter C64. Es war der Versuch, die Marke Commodore auch auf den Konsolen-Markt zu etablieren. Es besaß keine Tastatur und keinen Anschluss für Datasette und Diskettenlaufwerke. Spiele konnten nur über Module geladen werden. Der Modulschacht befand sich auf der Oberseite des Gerätes. Der C64 GS war genauso teuer wie ein vollwertiger C64, weswegen der C64 GS floppte. Offiziell wurde das Modell C64GS nur in England vertrieben.\n\nDer Vorgänger des C64 war der 1981 zur Marktreife gebrachte, farbfähige VC 20, von dem erstmals in der Geschichte der Mikrocomputer über eine Million Exemplare verkauft wurden. Als offizieller Nachfolger des C64 wurde 1985 der Commodore 128 auf den Markt gebracht, welcher neben dem eigenen C128-Modus über einen C64- sowie einen CP/M-Modus verfügte. Die Produktion des Nachfolgemodells wurde jedoch wegen nicht zufriedenstellender Verkaufszahlen und hoher Produktionskosten schon 1989 und damit fünf Jahre vor dem Produktionsende des C64 eingestellt.\n\nDie ab 1984 gefertigten Modelle der Commodore-264-Serie, der C16, C116 und Plus/4, konnten sich aufgrund ihrer Inkompatibilität zum beliebten C64 sowie bestimmter technischer Defizite auf dem Markt ebenfalls nicht durchsetzen. Noch im gleichen Jahr erfolgte die Produktionseinstellung und verbliebene Geräte wurden für Schleuderpreise verkauft. Als späten Nachfolger des C64 entwickelte Commodore den Commodore 65, der jedoch nie in Serie produziert wurde, da man dem sehr erfolgreichen Amiga 500 mit dem C65 keine Konkurrenz machen wollte.\n\nDie als Tastaturcomputer ausgeführten Einsteigermodelle der von Commodore hergestellten Amiga-Reihe, insbesondere der Amiga 500, erfreuten sich Ende der 1980er-Jahre einer ähnlichen Beliebtheit als leistungsfähige Spielecomputer wie der C64, die jedoch den C64 nie vom Markt verdrängen konnten. Technisch war der Amiga dem C64 überlegen, er besaß allerdings auch eine vollkommen abweichende und modernere Hardware.\n\nCommodore International musste am 29. April 1994 Insolvenz anmelden. Mit dem Hersteller Commodore verschwand gleichzeitig auch der letzte Heimcomputer C64 vom Markt, dessen Produktionseinstellung eigentlich erst für 1995 vorgesehen war.\n\nWährend der 8-Bit-Ära gab es vom C64, anders als bei vielen Konkurrenzmodellen, keine legalen oder illegalen Nachbauten durch andere Firmen. Die vielen speziellen Chips im C64, die nur von Commodore selbst beziehungsweise von deren Tochter MOS Technology hergestellt und die nicht an potenzielle Nachbauer verkauft wurden, verhinderten dies.\n\nIm Jahre 1998 erschien von der belgischen Firma \"Web Computers International\" der \"Web.it\", ein PC-kompatibler Rechner mit Microsoft Windows 3.1 und vorinstalliertem C64-Emulator. Herz war ein AMD-Mikroprozessor auf 486-Basis (66 MHz), dazu kamen 32 MiByte RAM, 32 MiByte ROM. Der Web.it war zudem mit einem Webbrowser (Netscape Navigator), einem Textverarbeitungsprogramm (Lotus AmiPro) und einer Tabellenkalkulation (Lotus 1-2-3) ausgestattet. Wie beim Original-C64 befand sich der gesamte Rechner im selben Gehäuse wie die Tastatur. Die Produktion des erfolglosen Modells wurde relativ schnell wieder eingestellt. Das mag unter anderem damit zusammengehangen haben, dass das Gerät nicht annähernd die notwendige Prozessorgeschwindigkeit aufwies, um einen C64 in Echtzeit zu emulieren.\n\nJeri Ellsworth und Individual Computers entwickelten den C-One oder auch \"Commodore One\" als Nachbau des C64 und bildeten die Hardware mittels FPGAs nach. Erste Platinen wurden 2003 ausgeliefert.\n\nEnde 2004 brachte die englische Firma \"The Toy:Lobster Company\" den C64 Stick – auch als C64 DTV (\"Direct To TV\") bekannt – heraus, der auch in Deutschland erschien. Der Entwurf stammt ebenfalls von Jeri Ellsworth, es handelt sich im Wesentlichen um einen auf das Notwendigste reduzierten C-One. Es ist ein C64-Nachbau in Form des Joysticks Competition Pro mit 30 eingebauten Spielen (darunter unter anderem Summer Games, California Games sowie Pitstop, Super Cycle und Uridium). Der Anschluss erfolgt direkt an das Fernsehgerät. Begabte im Löten und technisch Bewanderte können den Joystick um weitere Joystickports sowie um PS/2-Port für Tastatur, IEC-Port für Drucker und Diskettenlaufwerke sowie Buchse für Stromanschluss erweitern. Es existieren NTSC- (seit 12/2004) und PAL-Versionen (seit 8/2005).\n\nIm August 2010 veröffentlichte \"Commodore USA\" die Nachricht, die weltweiten Lizenzrechte für bisherige Commodore-Marken erworben zu haben, insbesondere für den C64 und den Amiga-Computer. Im Dezember 2010 wurde ein \"Commodore 64\" genanntes PC-System im originalgetreuen Retro-Gehäuse angekündigt. Basis ist ein Mainboard mit einem Intel Atom D525 DualCore-Chip, nVidia ION2 Grafik, USB-Ports, Kartenleser sowie optional DVD- oder BluRay-Laufwerk. Der Rechner wurde mit dem Betriebssystem Ubuntu Version 10.10 ausgeliefert. Später erhielt es mit Commodore OS ein eigenes Betriebssystem und einen integrierten C64-Emulator.\n\nDer Chameleon 64 ist ein von \"Individual Computers\" entwickeltes Modul, das 2013 erschien. Es enthält unter anderem einen VGA-Port, PS/2-Anschlüsse für Maus und Tastatur und einen Slot für SD-Karten. Grundsätzlich stehen zwei Betriebsmodi zur Verfügung:\n\nWird das Modul am C64 betrieben, so bietet es eine VGA-Ausgabe, VC-1541-kompatible Floppy-Emulation von zwei Diskettenlaufwerken, die Emulation von REU, GeoRAM und diverser Anwendungs- und Spielmodule. Im autarken Betrieb stehen zusätzlich die Funktionen eines mittels FPGA umgesetzten C64 zur Verfügung. Das Modul bietet zusätzlich einen Uhrenport zum Anschluss einer Netzwerkkarte vom Typ RR-Net Mk2 oder Mk3, die im Modulgehäuse untergebracht werden kann.\n\nErstes Modell\n\nAm 1. April 2014 kündigte individual Computers an, neue C64-Mainboards unter dem Namen \"C64 reloaded\" zu produzieren. Beim Platinenentwurf wurde sich zum größten Teil an den Originalschaltplan mit der Commodore-Nummer 250466 gehalten. Jedoch gibt es auch Abweichungen vom Schaltplan. So wurden Nullkraftsockel verbaut und eine 12V DC-DC Wandlertechnik hielt Einzug. Statt eines TV-Modulators wurde ein S-Video-Ausgang und eine 3,5mm Audio-Klinkenbuchse verbaut. Der C64 reloaded kann ohne Lötarbeiten von PAL- auf NTSC-Videonorm umgeschaltet werden. Der C64 Reloaded benötigt zum Betrieb zusätzlich originale Chips, die aus defekten C64-Computern entnommen werden können. Dieses Modell wurde noch ohne den Markennamen \"Commodore\" vertrieben. Der Verkaufsstart des C64 reloaded war am 20. Mai 2015. Diese Boards waren schnell ausverkauft.\n\nZweites Modell\n\nDas \"C64 reloaded MK2\" ist das erste Board der Reihe welches unter dem Markennamen Commodore vertrieben wird. Im Gegensatz zum ersten Modell erkennt das MK2 die installierten Chipversionen automatisch und konfiguriert sich von selbst entsprechend. Der Verkaufsstart ist für den 21. November 2017 anvisiert.\n\n2017 wurde bekannt dass die britische \"Retro Games Ltd.\" und die österreichische Koch Media eine voll lizenzierte Mini-Version des C64 unter den Namen \"TheC64 Mini\" Anfang 2018 auf den Markt bringen wird. Das Gerät selbst lehnt sich an das Design des C64 an, ist aber nur halb so groß. Die Tastatur des \"TheC64 Mini\" ist eine Attrappe. Es verfügt über einen HDMI-Port für moderne Fernsehgeräte und Monitore. Der mitgelieferte Joystick wird über einen USB-Port mit dem Gerät verbunden. Auch eine USB-Tastatur lässt sich an dem Gerät anschließen, somit ist es möglich eigene Basic-Programme auf dem \"TheC64 mini\" zu schreiben. Das Gerät wird mit 64 vorinstallierten Spielen ausgeliefert.\n\nIm August 2016 wurde bekannt, dass \"individual Computers\" (icomp) neben der Lizenz für den Markennamen Commodore auch die originalen Gussformen des C64C-Gehäuses erworben hat und mit diesen neue Gehäuse produzieren will. Am 22. August 2017 wurde das Gehäuse auf der Gamescom der Öffentlichkeit präsentiert und bereits verkauft; der reguläre Verkauf startete am 1. September 2017. Die neuen Gehäuse sind dabei in den vier Farbgebungen Original Beige, Classic Bread Bin, SX-64 Style und Black Edition verfügbar. Da fast alle C64-Platinen vom Aufbau identisch sind und es nur kleinere Abweichungen von der C64C Platine gibt, lassen sich auch andere C64-Versionen in dem Gehäuse einbauen. Dies trifft auch auf die Modelle des \"C64 reloaded\" zu.\n\nObwohl der C64 oft als „Spielcomputer“ und „Daddelkiste“ bezeichnet wurde, da der überwiegende Teil der Software Spiele waren, wurden für das Gerät – auch wegen seiner für die damalige Zeit gehobenen Hardware-Eigenschaften – auch viele „ernsthafte“ Programme produziert. Neben Office-Programmen wie der Textverarbeitung Vizawrite oder Textomat und den Tabellenkalkulationen Microsoft Multiplan und SuperCalc gab es für alle erdenklichen Anwendungen eine Vielzahl von Programmen, von denen hier stellvertretend nur einige aus dem deutschen Raum genannt seien: Für grafische Anwendungen waren Programme wie \"Hi-Eddi\" (für HiRes-Grafik) von Hans Haberl, \"Amica Paint\" von Oliver Stiller für Multicolor-Grafiken und \"GIGA-CAD\" von Stefan Vilsmeier für 3D-Modelle konzipiert. Ebenfalls von Hans Haberl und veröffentlicht von Scanntronik waren die Desktop-Publishing-Programme Printfox und Pagefox. Letzteres wurde als Steckmodul entwickelt und enthielt eine zusätzliche Speichererweiterung, um Zeichensätze, Grafiken und Text für eine ganze DIN-A4-Seite im Speicher halten zu können. Dabei standen alle üblichen Layoutfunktionen zur Verfügung, inklusive Spezialfunktionen wie Kerning.\n\nAuch etliche Lernprogramme wurden für den C64 produziert, wenngleich er kein typischer Rechner war, der im Schulunterricht zum Einsatz kam. Hier waren besonders der Apple II und seine Klone stark verbreitet.\n\nNeben Lernprogrammen wie Vokabeltrainern, Mathekursen und Programmen zum Erlernen des Chemielernstoffes wurden auch Hardware-Erweiterungen angeboten, mit denen Schüler zum Beispiel mit der Fischertechnik-Schnittstelle 30562 für den C64/VC20 die Grundzüge der Robotik erlernen konnten. Der C64 konnte auch für Lern- und Forschungszwecke genutzt werden. So trat das Gerät in den 1980er-Jahren bei vielen Beiträgen der Jugend-forscht-Wettbewerbe als Bestandteil der Versuchsanordnungen in Erscheinung.\n\nAuch in der Fliegerei wurden Programme für den C64 eingesetzt. US-Piloten konnten beispielsweise Flüge nach Instrumentenflugregeln (IFR) mit dem \"Flight-Simulator II\" von Bruce Artwick machen, die für die Verlängerung der Pilotenlizenz angerechnet wurden. Das deutsche Pendant dazu war der \"Flight-Teacher\" von Uwe Schwesig, der eine Einführung in die Fliegerei bot.\n\nIm Jahr 1986 wurde das Betriebssystem GEOS (Graphic Environment Operating System) mit grafischer Oberfläche (GUI) für den C64 angeboten. Es wurde in mehreren Versionen veröffentlicht und enthielt sehr viele Anwendungsprogramme. Diese grafische Oberfläche erweiterte den C64 in seiner Anwendungsbreite stark. Das war notwendig geworden, weil ab Mitte der 1980er-Jahre grafische Oberflächen immer häufiger als Serienausstattung bei Heimcomputern zum Einsatz kamen, so zum Beispiel beim Commodore Amiga, dem Apple Macintosh oder beim Atari ST. GEOS wird auf unterschiedlichen Plattformen bis heute (Stand 2005) gepflegt und erweitert. Allerdings ist es sehr ressourcenaufwendig, so dass sich im Besitz des Anwenderkreises von GEOS auch am häufigsten moderne Hardware wie große Speichererweiterungen, Super-CPUs oder Festplatten befinden.\n\nDarüber hinaus wurde für den C64 ein Unix-Derivat namens \"LUnix\" entwickelt. Aktuell weiterentwickelt wird das ebenfalls Unix-orientierte \"Wings\"-Betriebssystem für den C64.\n\nNeue C64-Software und C64-Hardware wird auch heute noch von verschiedenen Firmen (zum Beispiel \"Protovision\") vertrieben und entwickelt.\n\nDie wichtigsten Programmiersprachen für den C64 waren das eingebaute BASIC und Assembler. Daneben gab es eine Vielfalt an Programmiersprachen und -Dialekten für den C64:\n\nDas eingebaute Commodore BASIC V2 bietet keinerlei Befehle, um die Grafik- und Soundmöglichkeiten des C64 anzusprechen, da diese beim VC20, von dem der Code übernommen worden war, noch nicht vorhanden waren. Das bereits vorhandene und bessere BASIC 4.0 der neueren PETs wurde beim C64 nicht verwendet, da man den PETs keine interne Konkurrenz machen wollte. Über die BASIC-Befehle codice_9 und codice_10 kann direkt auf die Hardware zugegriffen werden, weiterhin ist über den codice_11-Befehl das direkte Anspringen von Systemroutinen möglich: Beispielsweise bewirkt codice_12 einen Reset des C64. Sound und Grafik lassen sich nur in Assembler oder erweiterten BASIC-Varianten wie etwa Simons’ BASIC effektiv programmieren, die jedoch nicht Bestandteil des Lieferumfangs waren. Spiele für den C64 sind daher nahezu ausschließlich in Assembler programmiert. Bei späteren BASIC-Versionen, beispielsweise dem BASIC 3.5 des C16 und Plus4, ist der Befehlsvorrat wesentlich umfangreicher.\n\nNeben dem eingebauten Commodore BASIC V2 gab es noch diverse Dialekte und Compiler. Eine Auswahl:\n\nAustrospeed ist ein 2-Pass-Compiler (3-Pass-Compiler im Overlay-Modus), der BASIC V2.0-Code in einen kompakten, schnell interpretierbaren Zwischencode (ähnlich P-Code) übersetzt. Derart kompilierte Programme laufen drei- bis fünfmal schneller ab als unkompilierte. Zum Austrospeed gab es auch einen dazugehörigen Decompiler.\n\nBasic-Boss ist ein BASIC-Compiler aus dem Hause Markt & Technik Verlag, der im Jahre 1988 erschien und sehr stabile Programmergebnisse aus reinen BASIC-Programmen erzeugt. Reine BASIC-Programmierer können mit Hilfe des Compilers schnelle Programme erhalten, ohne auf Assembler ausweichen zu müssen. Der Benutzer muss dafür bestimmte „Definitionen“ in sein BASIC-Programm einbauen, die dann nach dem Kompilieren diese hohen Geschwindigkeiten ermöglichen. In sehr günstigen Fällen laufen die Programme 50- bis 100-fach schneller ab.\n\nFür den C64 gab es Bascoder für den BASIC-Dialekt BASICODE. Dabei handelte es sich um einen rechnerübergreifenden BASIC-Standard.\n\nExbasic Level II ist ein erweitertes und verbessertes BASIC für den C64, das von Diskette geladen oder per Cartridge installiert wurde. Im Gegensatz zu Simons’ Basic war Exbasic Level II ursprünglich nicht für den C64 geschrieben worden, so dass nicht alle Möglichkeiten der Hardware dieses Rechners durch diese BASIC-Erweiterung ausgenutzt wurden.\n\nG-Basic stellte umfangreiche Programmierfunktionen zur Verfügung, über die das Standard-BASIC des C64 nicht verfügte. Es wurde als Hardwareerweiterung geliefert, die in Form und Größe an eine Zigarettenschachtel erinnerte. Diese besaß einen eigenen Resettaster, da der C64 ab Werk nicht über einen solchen verfügte.\n\nGeo-Basic ist ein BASIC unter der grafischen Oberfläche GEOS. Es enthielt allerdings viele Fehler und lief langsam, weshalb es sich nicht durchsetzen konnte. Auch war der für die Anwendungsprogramme zur Verfügung stehende Arbeitsspeicher nur sehr klein.\n\nPetspeed ist ein Compiler für das eingebaute BASIC V2.0 von Commodore; für längere Programme benötigte der Compiler ein – selten vorhandenes – Doppelfloppylaufwerk.\n\nSimons’ Basic ist ein stark erweitertes BASIC mit grafischen Funktionen (Kreis, Ellipse) sowie teilweise strukturierter Programmierung. Vertrieben auf Diskette oder Cartridge.\n\nAssembler ist die wichtigste und – zusammen mit dem eingebauten BASIC – die am häufigsten genutzte Programmiersprache für den C64. Nur mit Assembler konnten die Fähigkeiten des Gerätes optimal genutzt werden. Es gab verschiedene Assembler-Entwicklungsumgebungen, die bekanntesten hießen TurboAss, Hypra-Ass und Giga-Ass. Für große Projekte wurden Cross-Assembler-Systeme eingesetzt. Diese bestanden aus zwei Computern, die mit einem Datenkabel verbunden waren: Einem C64, auf welchem das neu entwickelte Programm getestet wurde, und einem zweiten Computer, zum Beispiel ein weiterer C64, ein Amiga oder PC, auf welchem der Quelltext geschrieben und von einem Cross-Assembler übersetzt wurde. Das machte die Programmierung weitaus komfortabler, da auf dem Test-C64 der komplette Speicher bis auf die wenigen Bytes für die Übertragungsroutine zur Verfügung stand und im Fall eines Absturzes Quelltext und Assembler nicht gelöscht wurden. Jedoch reichte schon ein einfacher Maschinensprachemonitor aus, um Software für den C64 zu entwickeln: Das wohl bekannteste Exemplar eines solchen Programmes war der Smon. Auch brachten viele Erweiterungsmodule, wie das Action Replay oder die Final Cartridge, eigene Maschinensprachemonitore mit.\n\nMit Oxford Pascal gab es eine Pascal-Implementierung, die in der Lage war, eigenständige Programme auf Diskette zu schreiben oder im Speicher zu halten. Sie war durchaus standard-konform. Auch von UCSD Pascal gab es eine Portierung auf den C64; sie war jedoch so umständlich und langsam, dass sie in der Praxis keine Rolle spielte.\n\nZusätzlich zu den genannten Sprachen gibt es weitere Programmiersprachen, die aber eher exotisch sind. So gibt es einen C-Compiler (der allerdings nur eine Teilmenge von C implementiert), Forth und COMAL sind ebenfalls vertreten; es wurde sogar eine COBOL-Implementierung produziert. Auch Logo gibt es für den C64.\n\nWeiterhin existiert das Betriebssystem Contiki, das eine Internet- und Ethernetanbindung über den C64 erlaubt.\n\nHeute existiert mit cc65 ein leistungsfähiger Cross-Compiler für die Sprache C, der bis auf Gleitkommazahlen fast den ganzen ANSI-Standard abdeckt. Der Compiler selbst läuft auf den meisten modernen Plattformen.\n\nDie Spiele für den C64 waren eines der besten Verkaufsargumente für den Rechner: Fast jedes bekannte Computerspiel in den 1980er- und teilweise in den 1990er-Jahren wurde für den C64 umgesetzt, darunter viele Arcade-Spiele, so auch Donkey Kong und Pac-Man. Schätzungen gehen von etwa 17.000 kommerziellen Spieletiteln für dieses Gerät aus, nicht mitgezählt die zahllosen Spiele, die C64-Besitzer selbst programmierten. Über 95 Prozent aller Spiele haben eine Auflösung von 160 × 200 doppelbreiten Pixeln.\n\nIm Laufe der Jahre wurden insbesondere die Spiele immer komplexer und grafisch anspruchsvoller. Einige grafische Höhepunkte für den C64 sind unter anderem das Strategiespiel Defender of the Crown oder Manfred Trenz’ Actionspiel \"Turrican II: The Final Fight\", deren Grafiken teilweise an Amiga-Qualität heranreichen. Andere herausragende Beispiele sind Wizball (Rahmensprites), Stunt Car Racer (3D-Grafik mit ausgefüllten Polygonen) oder die \"Last-Ninja\"-Trilogie. Auch die Präsentation und Animation der beliebten Sportspiele der Firma \"Epyx/U.S.Gold\" (\"Summer Games 1+2,\" \"Winter Games\", \"California Games\" und so weiter) konnten überzeugen. Das von Nintendos Mario-Serie inspirierte Great Giana Sisters erfreute sich ebenfalls großer Popularität.\nSchon in den 1980er-Jahren erprobten politische Gruppierungen die Möglichkeit, Computerspiele für ihre Zwecke zu nutzen. Diese technisch primitiven Spiele, die als Kopien auf Schulhöfen getauscht wurden, basieren meist auf der Technik des C64, etwa das von einem 17-Jährigen programmierte rechtsextremistische Spiel „Anti-Türken-Test“, in dem rassistische Fragen über die Tastatur zu beantworten sind oder das Spiel „KZ-Manager“, in dem ein Konzentrationslager möglichst effektiv geführt werden muss. Viele dieser Programme wurden in den 1980er- und frühen 1990er-Jahren durch die Bundesprüfstelle für jugendgefährdende Medien (damals noch Bundesprüfstelle für jugendgefährdende Schriften, kurz BPjS) indiziert und später durch Gerichtsbeschlüsse bundesweit beschlagnahmt.\n\nDer C64 trug besonders zur Entwicklung einer vielfältigen Subkultur bei, in der talentierte Programmierer Tricks entwickelten (zum Beispiel die Ausnutzung undokumentierter Hardwarefunktionen, darunter sehr viele Tricks für den Grafikchip), um die augenscheinlichen Limitierungen des Computers zu umgehen. Teile dieser Szene leben heute noch fort (siehe auch Demoszene) oder entwickelten sich weiter zu anderen Computersystemen wie Amiga oder PC. Die Demoszene entstand in den 1980er-Jahren aus der damaligen Crackerszene. Die Intros, die ursprünglich als Vorspann zwecks Präsentation der Fähigkeiten und Wiedererkennung vor gecrackte Spiele gesetzt wurden, nahmen stetig an Komplexität zu und wurden schließlich als Einzelwerke (\"Demos\") ohne dazugehörige gecrackte Software veröffentlicht.\n\nEinem Außenstehenden erschließen sich die Schwierigkeiten dieser Programmierung häufig nicht, da er die Komplexität oder die laut Spezifikation eigentliche Unmöglichkeit des Effekts nicht einschätzen kann. Einige der grundlegenden Mechanismen betrafen die Nutzung des im Grafikchip integrierten sogenannten Rasterzeileninterrupts (Interrupt-Auslösung bei einer bestimmten Bildzeile) zur Synchronisierung von Code-Sequenzen, das ruckfreie Scrollen der Bildschirmfläche in beiden Achsen oder die Wiederverwendung von Sprites innerhalb eines Bildes. Typische Kennzeichen waren vor allem rasante, tanzende Rollschriften, mit 16 Farben vorgetäuschte, waagerechte Zylinderformen, sowie fast immer ein üppiges akustisches Beiwerk.\n\nDie Demoszene lotete die Möglichkeiten des C64 am weitesten aus. Höhepunkte setzten Demos wie \"Deus Ex Machina\" der Gruppen Crest und Oxyron, \"Tower Power\" der Gruppe Camelot, \"+H2K\" der Gruppe Plush oder \"Dutch Breeze\" der Gruppe Blackmail sowie \"Double Density\" von Mr.Cursor aka Ivo Herzeg, der in der Entwicklung mitverantwortlich für bekannte PC-Spiele wie Far Cry ist.\n\nDie Website der Demogruppe Alpha Flight 1970 enthält einige Flashversionen von szenetypischen Produktionen. Ein riesiges Repertoire an Informationen zu alten wie neuen Produktionen sind in der \"Commodore 64 Scene Database\" (CSDb) verzeichnet.\n\nMit dem rasanten Aufstieg des Heimcomputers in den 1980er-Jahren im Allgemeinen und des C64 im Speziellen entstand auch ein Tauschmarkt für Raubkopien von Software für diesen Rechner. Auch Anwendersoftware, aber im überwiegenden Maße Spiele wurden zwischen den C64-Besitzern getauscht. Das war mit den ersten kommerziellen Programmen noch sehr einfach machbar. Doch schon bald versuchte die Softwareindustrie, durch verschiedene Kopierschutzmaßnahmen (mittels Datenträger, durch Papier-basierte Abfragen oder auch durch Laufzeitmaßnahmen) der Situation Herr zu werden. Das gelang kaum, da quasi gleichzeitig die Szene dafür sorgte, dass die Software einerseits mit ihren eigenen Programmen wieder kopierbar wurde, andererseits erzeugte man durch Decodieren und gezieltes Modifizieren der Originale ungeschützte Versionen, die sich mit jedem beliebigen Kopierprogramm duplizieren ließen. Es entstand eine Art „Hase-und-Igel“-Wettlauf zwischen der Softwareindustrie und den C64-Besitzern, in dem immer neue Kopierschutzmaßnahmen die illegale Verbreitung von Software verhindern sollten. Letztlich war jedoch fast jedes Programm für den C64 früher oder später auch als „freie“ Raubkopie in Umlauf.\n\nEine erste Abmahnwelle veranlasste Ende 1992 der Rechtsanwalt Freiherr von Gravenreuth, als er über Testbesteller auf verdächtig erscheinende Kleinanzeigen in Computerzeitschriften, in denen überwiegend Privatleute inserierten, die sogenannten „Tanja-Briefe“ (unter dem Pseudonym „Tanja Nolte-Berndel“ und einigen weiteren weiblichen Pseudonymen) versandte. Falls ein so Angeschriebener auf die Bitte um Softwaretausch des angeblichen Teenagers einging, wurde dieser bei entsprechender Beantwortung wegen Verstoßes gegen das Urheberrecht abgemahnt, gegebenenfalls auch angezeigt. Auch führten einige Fälle zu Hausdurchsuchungen.\n\nMit der Zeit wurde es Brauch bei den Crackern, vor die von ihnen „geknackten“ Programme einen eigenen, mehr oder weniger aufwändigen Vorspann (ein sogenanntes „Cracktro“) zu setzen. Typischerweise wurde dort in Laufschriften die eigene Coolness gepriesen, es wurden befreundete Crackergruppen gegrüßt, und zunehmend stellte man auch optisch und akustisch die eigene Programmierkunst zur Schau. Die oben beschriebene Demoszene ging zuerst aus der Verselbstständigung dieser Cracker-Vorspänne zu eigenständigen Programmen hervor, auch wenn später eine klare Abgrenzung der Demo- von der Crackerszene stattfand.\n\nDer Soundchip des C64 war zum Verkaufsstart des C64 eine Sensation, da es bis dahin keinen vergleichbaren Heimcomputer gab, der eine solche Vielfalt an Klangvariationen bot. Durch diese technischen Möglichkeiten machten sich unzählige Programmierer daran, den C64 als Musikcomputer zu nutzen und entsprechende Musik auf ihm zu programmieren.\n\nFür den deutschen Sprachraum besonders zu erwähnen ist das komplett auf dem C64 programmierte Stück „Shades“ von Chris Hülsbeck, der mit diesem Song im Jahre 1986 den Musikwettbewerb der Fachzeitschrift 64’er gewann und damit den Grundstock für seine Karriere im Bereich der Spielevertonung legte. Weitere bekannte C64-Komponisten waren Rob Hubbard, Martin Galway, Ben Daglish, David Dunn, Markus Schneider, Stefan Hartwig, Reyn Ouwehand, Jonathan Dunn, Matt Gray, Jeroen Tel, Jens-Christian Huus (JCH) und Charles Deenen (Maniacs of Noise).\n\nAuch die professionelle Musikszene nutzte den C64 als Musikinstrument. So experimentierte der Musiker und Musikproduzent Michael Cretu in den 1980er-Jahren mit den Klängen des C64 und auch die Band von Inga Rumpf setzte den C64 ein. Viele Musiker geben noch heute an, durch den C64 den ersten Zugang zu einem Synthesizer bekommen zu haben, der eine Grundlage ihrer späteren Entwicklung war, so z. B. Rick J. Jordan von der Band Scooter. In der E-Musik wurde der C64 etwa von Yehoshua Lakner eingesetzt und bewusst als „historisches Musikinstrument“ mit eingeschränkten, aber produktiven Möglichkeiten gesehen.\n\nMitte der 1980er-Jahre kamen MIDI-Sequenzer-Softwares u. a. von der Hamburger Firma Steinberg (heute mit dem Produkt \"Cubase\" marktbeherrschend) auf den Markt, die den C64 als Steuerzentrum für MIDI-Synthesizer und MIDI-Sampler nutzten. Mit der Software \"Pro 16\" von Steinberg konnte man professionelle Popmusik-Produktionen erstellen. Der C64 konnte über eine grafische Darstellung und mit manipulierbaren Zahlenwerten 16 verschiedene Instrumente (Piano, Drums, Bass usw.) gleichzeitig ansteuern. Die Taktrate und der Speicher des C64 reichten voll und ganz aus, MIDI-Instrumente nach Belieben zu steuern. Der SID des C64 wurde dabei nicht benutzt, da die Sounds nur von den Peripheriegeräten kamen. Auch in der Filmmusikszene fasste der C64 (wenn auch nur kurzzeitig) Fuß. So wurde beispielsweise der 80-minütige Dokumentarfilm über die berüchtigten Meuterer von der Bounty „Pitcairn – Endstation der Bounty“ (Regie: Reinhard Stegen) vollständig mit Musik untermalt, die auf einem C64 komponiert worden war. Damit stellte der C64 seine Praxistauglichkeit auch im Profibereich vollends unter Beweis. Der kurz darauf, Ende der 1980er-Jahre aufkommende Atari ST übernahm in fast allen deutschen Musikstudios das Kommando in Sachen MIDI-Sequencing und löste den C64 im Profibereich ab.\n\nIn Deutschland kamen ab Anfang der 1980er-Jahre verschiedene Computermagazine speziell für den C64 auf den Markt. Am bekanntesten war die „64’er“ vom Verlag „Markt & Technik“, der Heise-Verlag gab mit der „Input 64“ ein Magazin auf einem Datenträger (Kassette und Diskette) heraus. Auch bekannt und verbreitet waren die Diskettenmagazine „Magic Disk 64“ und sein Ableger „Game On“ sowie die „RUN“. Als inoffizieller Nachfolger der 64'er erschien von 1997 bis 2006 die „Go64!“ (CSW-Verlag, Winnenden), die in der „Retro“ aufging, welche seit 2006 vierteljährlich erscheint. Des Weiteren existieren gegenwärtig noch zwei deutschsprachige Amateur-Printmagazine, die „Lotek64“ (auch als kostenlose PDF-Version im World Wide Web erhältlich) und die „Return“. In England waren „Commodore Force“ und „Commodore Format“ beliebt. Heute gibt es noch das englischsprachige Fanmagazin „Commodore Free“, das ebenfalls kostenlos als PDF erhältlich ist. Zudem erscheinen in mehr oder weniger regelmäßiger Reihenfolge Magazine auf Diskette, wie etwa die \"Digital Talk\", die \"Mail Madness\" oder das australische Diskmag \"Vandalism News\". Diese enthalten neben am Bildschirm zu lesenden Artikeln auch aktuelle Software, Musik und Bilder.\n\nAuch einige der damaligen Magazine, die viele verschiedene Rechnerplattformen abdeckten (wie „Happy Computer“, „Power Play“ und „ASM“) waren aufgrund des Markterfolges des C64 zunächst sehr auf diesen fixiert, was Besitzer anderer Rechner oftmals bemängelten.\n\nInhalte aller dieser Magazine war nicht nur die Berichterstattung über neue Hard- und Software für die jeweiligen Geräte, sondern auch der seitenweise Abdruck von Listings, also von Programmtexten, die der Leser dann per Hand in den Computer abtippen konnte. Diese Art des Vertriebs von Software für den C64 war für den Besitzer, neben dem Erwerb von Kaufsoftware oder Schwarzkopien, oft der einzige Weg, an Programme zu gelangen, da es den Download über das Internet noch nicht gab.\n\nAb Ende 1985 wurde der C64 im Intershop gegen „Westgeld“ oder Forumschecks verkauft, weitere Geräte fanden als Geschenk ihren Weg in die DDR. Gelegentlich konnte der C64 dank asiatischer Gastarbeiter im staatlich organisierten Gebrauchtwarenhandel („A & V“) für 8000 Mark als Neuware erstanden werden. Daneben gab es einen Privathandel über Kleinanzeigen. Die Gebrauchtpreise lagen bei 3000 bis 6000 Mark für den C64 und bis zu 5000 Mark für ein Diskettenlaufwerk. Bespielte Disketten und Kassetten unterlagen als Datenträger jedoch strengsten Importkontrollen, durften auch nicht als Geschenk aus dem Westen geschickt werden und waren daher ohne Beziehungen so gut wie nicht erhältlich.\n\nHeute gibt es etliche Commodore-64-Emulatoren, wie den VICE, den M.E.S.S., Power 64 (für Mac OS X und Mac OS 9), Frodo (u. a. für Symbian-Handys, sowie Apple iOS und Android) und den ccs64. Diese erlauben es, C64-Software auf moderneren Rechnern wie etwa einem Windows-PC auszuführen. Mit den Emulatoren kann neben Disk-Images auch Original-C64-Zubehör wie z. B. Disketten- und Datasettenlaufwerke angesteuert werden. Für die Verwendung der Datasette oder der Original-Diskettenlaufwerke sind jedoch Bastelarbeiten für Kabel notwendig, um die Geräte mit den heutigen Ports anzusteuern. Für Nutzer, die die langen Ladezeiten des C64 nicht mögen, bieten die Emulatoren einen virtuellen Lademodus.\n\nDie meiste C64-Software, die in den 1980er-Jahren veröffentlicht wurde, kann auf heutigen Systemen (PC, Mac) mit Hilfe dieser Emulatoren genutzt werden. Seit dem 28. März 2008 stehen ausgewählte C64-Spiele im Download-Katalog der Wii-Konsole zur Verfügung.\n\nDas Internet Archive bietet eine im Webbrowser benutzbare Emulatoroberfläche mit einer großen Menge an Programmen und Spielen.\n\n\nOriginalliteratur der Firma Commodore\n\nGeschichte\n\nAllgemeines\n\nHardware\n\nProgrammierung\n\nSpieleprogrammierung\n\nDemoszene\n\nGrafik\n\nMusik\n\nSpiele\n\nSonstiges\n\nZeitschriften\n\n"}
{"id": "941", "url": "https://de.wikipedia.org/wiki?curid=941", "title": "Rechnergestützte Entwicklung", "text": "Rechnergestützte Entwicklung\n\nDie rechnergestützte Entwicklung (englisch \"\" und kurz \"CAE\" genannt) umfasst alle Varianten der Rechner-Unterstützung von Arbeitsprozessen in der Technik.\n\nDie folgenden Teilgebiete sind bekannt:\n"}
{"id": "1077", "url": "https://de.wikipedia.org/wiki?curid=1077", "title": "Debian", "text": "Debian\n\nDebian ( []) ist ein gemeinschaftlich entwickeltes freies Betriebssystem. Debian GNU/Linux basiert auf den grundlegenden Systemwerkzeugen des GNU-Projektes sowie dem Linux-Kernel. Die aktuelle Version ist Debian 9 „Stretch“. Debian enthält eine große Auswahl an Anwendungsprogrammen und Werkzeugen; derzeit sind es über 51.000 Programmpakete.\n\nDebian wurde im August 1993 von Ian Murdock ins Leben gerufen und wird seitdem aktiv weiterentwickelt. Heute hat das Projekt über 1.000 offizielle Entwickler. Es ist eine der ältesten, einflussreichsten und am weitesten verbreiteten GNU/Linux-Distributionen. Viele weitere Distributionen benutzen Debian als Grundlage. Das heute bekannteste Debian-GNU/Linux-Derivat ist Ubuntu.\n\nDebian-Entwickler kann jeder werden, der den sogenannten New-Member-Prozess erfolgreich durchläuft: Bewerber werden hinsichtlich ihrer Kenntnisse und Fähigkeiten geprüft, außerdem wird sichergestellt, dass sie mit der Ideologie des Projektes vertraut sind.\n\nDer Name des Betriebssystems leitet sich von den Vornamen des Debian-Gründers Ian Murdock und seiner damaligen Freundin und späteren Ehefrau Debra Lynn ab. Bereits wenige Monate nach der Gründung, im Mai 1994, entschied sich das Projekt zu einer Änderung des offiziellen Namens von \"Debian Linux\" zu \"Debian GNU/Linux\", womit es der Auffassung der Free Software Foundation folgte, dass das häufig als Linux bezeichnete Betriebssystem eine Variante des GNU-Systems sei (zu den Hintergründen der diesbezüglichen Meinungsverschiedenheiten siehe GNU/Linux-Namensstreit). Da Debian ab Version 6.0 \"(Squeeze)\" offiziell in zwei Varianten – GNU/Linux und GNU/kFreeBSD – verfügbar ist, wird seitdem nur noch in Bezug auf diese der jeweilige Namenszusatz genannt; allgemein wird heute also nur noch von \"Debian\" gesprochen.\n\nDas System ist bekannt für seine Paketverwaltung dpkg und deren Frontend APT. Mit diesen ist es möglich, alte Versionen von Debian GNU/Linux durch aktuelle zu ersetzen oder neue Softwarepakete zu installieren. Sie sind ebenfalls dafür zuständig, alle von einem Programm benötigten Abhängigkeiten aufzulösen, also alle Programmpakete zu laden und zu installieren, welche die gewünschte Software benötigt.\n\nAm 16. August 1993 wurde von Ian Murdock das „Debian Linux Release“ angekündigt. Er hatte versucht, SLS, das eine der ersten umfassenden Linux-Distributionen war, zu nutzen. Da er jedoch mit deren Qualität unzufrieden war, konzipierte er sein eigenes System, ließ sich aber von SLS inspirieren. Im selben Jahr veröffentlichte er auch das \"Debian-Manifest,\" eine Zusammenstellung seiner Sichtweise zu Debian. Im Vordergrund stand hier eine offene Entwicklung „im Geiste von Linux und GNU“.\n\nBis 1995 veröffentlichte das Projekt die ersten Entwicklungsversionen mit den Versionsnummern 0.9x. In dieser Zeit wurde es auch von der Free Software Foundation gesponsert und zählte etwa 60 Entwickler. 1996 wurde letztlich die erste stabile Version 1.1 veröffentlicht. Weil ein CD-ROM-Verkäufer versehentlich eine Vorversion unter der Nummer 1.0 veröffentlicht hatte, kam es – um Verwirrung zu vermeiden – nie zu einer tatsächlichen Version 1.0. Im April 1996 wurde Murdock von Bruce Perens als Leiter des Projekts abgelöst. In den darauffolgenden Jahren wechselte diese Position einige Male. Am 17. Juni 1996 folgte mit \"Buzz\" (Version 1.1) das erste Release, welches einen Aliasnamen trug. Alle weiteren Veröffentlichungen wurden ebenfalls mit einem solchen versehen, wobei sich dieser immer nach einer Figur aus dem Film Toy Story bzw. seinen Fortsetzungen richtet. 1997 wurde nach vorheriger Diskussion der Debian-Gesellschaftsvertrag ratifiziert.\n\nAm 24. Juli 1998 wurde die Version 2.0 \"Hamm\" veröffentlicht, welche erstmals für mehrere Architekturen zur Verfügung stand. Das Projekt umfasste zu diesem Zeitpunkt 1500 Pakete und 400 Entwickler.\n\nEs folgten weitere 2.x-Veröffentlichungen mit neuen Portierungen zu anderen Architekturen sowie einer steigenden Zahl von Paketen. Besonders hervorzuheben ist die Entwicklung von APT. Auch entstand mit Debian GNU/Hurd die erste Portierung zu einem Nicht-Linux-Kernel.\n\nIm Jahr 2000 wurde der \"Testing-\"Zweig gegründet. In der nachfolgenden Zeit wurde die Debian-Website in 20 Sprachen übersetzt. Es kam zur Gründung der Unterprojekte \"Debian-Junior\" und \"Debian-Med,\" die sich an Kinder bzw. medizinische Forschung und Praxis richteten. Im gleichen Jahr fand auch erstmals die Entwicklerkonferenz DebConf statt, welche seitdem jährlich an unterschiedlichen Orten abgehalten wird.\n\nDie Version 3.0 \"Woody\" vom 19. Juli 2002 enthielt erstmals das \"K Desktop Environment,\" nachdem die Lizenzproblematik von Qt geklärt war. Das Projekt war auf 900 Entwickler und 8500 Binärpakete angewachsen. Die offizielle Distribution bestand aus 7 CDs.\n\nErst knappe drei Jahre später, am 6. Juni 2005, kam es zur Veröffentlichung von Version 3.1 \"Sarge.\" Der lange Zeitraum brachte dem Projekt einige Kritik ein. Mit Ubuntu entstand zwischenzeitlich auch das heute bedeutendste Debian-Derivat. \"Sarge\" enthielt etwa 15.400 Pakete und benötigte damit 14 CDs. Es beteiligten sich etwa 1500 Entwickler an dieser Veröffentlichung. Neben der Masse an aktualisierten und neu hinzugekommenen Paketen ist vor allem das neu geschriebene Installationsprogramm hervorzuheben, das in 40 Sprachen übersetzt wurde. Erstmals wurde auch OpenOffice.org aufgenommen.\n\n2006 wurde in Oaxtepec, Mexiko, die siebte DebConf abgehalten. Zudem wurde nach dem Namensstreit zwischen Debian und Mozilla seitens Debian das entsprechende Paket des Mozilla Firefox in \"Iceweasel,\" sowie das von Mozilla Thunderbird in \"Icedove\" umbenannt.\n\nAm 8. April 2007 wurde von etwa 1000 Entwicklern Version 4.0 \"Etch\" veröffentlicht. Diese enthielt rund 18.200 Binärpakete. Im Februar 2009 folgte 5.0 \"Lenny\", und im Februar 2011 wurde \"Lenny\" \"oldstable\" und 6.0 \"Squeeze\" mit über 29.000 Softwarepaketen als \"stable\" veröffentlicht.\n\nAb Version 6.0 „Squeeze“ ist mit Debian GNU/kFreeBSD die erste offizielle Portierung auf einen anderen Betriebssystemkern – jenen des FreeBSD-Projektes – als Technologievorschau verfügbar.\n\nAm 4. Mai 2013 wurde mit \"Wheezy\" die Version 7 als „stable“ gesetzt. Diese enthielt erstmals LibreOffice. Für Version 7 „Wheezy“ wurde die Veröffentlichung von Debian GNU/Hurd, einer offiziellen Portierung auf den GNU Hurd, diskutiert. Dies wurde jedoch verworfen.\n\nAm 25. April 2015 folgte ihr Version 8, Codename \"Jessie\", bei der das init-System vom bisher benutzten SysVinit auf das kontrovers diskutierte systemd umgestellt wurde. Auf die FreeBSD-Portierung wurde in Version 8 verzichtet.\n\nDie neunte Ausgabe von Debian, Codename \"Stretch\", erschien am 17. Juni 2017. Augenfälligste Neuerung ist die Rückkehr von Firefox und Thunderbird.\n\nDas Debian-Projekt konstituiert sich durch die Debian-Verfassung. Sie regelt die demokratische Organisationsstruktur mit regelmäßigen Wahlen. Darüber hinaus verpflichtet sich das Projekt mit dem Gesellschaftsvertrag \"Debian Social Contract\" zu \"freier Software.\"\n\nSeit dem 26. April 2004 ist die Version 1.1 des Gesellschaftsvertrages gültig. Die eigentliche inhaltliche Änderung besagt, dass alle Komponenten des Debian-Systems (im Hauptzweig \"main\") frei sein müssen, nicht mehr nur die Software. Die \"Debian-Richtlinien für freie Software\" beziehen sich also nicht mehr nur auf freie Software, sondern allgemein auf freie Werke. Da diese Auswirkungen einer als „editoriell“ bezeichneten Änderung für viele Entwickler überraschend war, wurde in einer zusätzlichen Abstimmung im Juli 2004 beschlossen, dass diese Änderung erst nach dem Release von Sarge im Juni 2005 wirksam wird.\n\nAktueller Leiter des Debian-Projekts ist Chris Lamb. Der Posten wird einmal im Jahr per Wahl neu vergeben. Alle Wahlen und Abstimmungen erfolgen elektronisch (mit Hilfe einer digitalen Signatur) nach der Schulze-Methode.\n\nAls eine Dachorganisation für Debian und weitere Freie-Software-Projekte wurde 1997 Software in the Public Interest gegründet.\n\nDer Debian-Gesellschaftsvertrag () ist eine vom Debian-Projekt beschlossene öffentliche Richtlinie, die Grundlagen regelt, wie die freie Software Debian hergestellt, verteilt und betreut wird. Der Gesellschaftsvertrag geht auf einen Vorschlag von Ean Schuessler zurück. Bruce Perens entwarf eine erste Version des Dokumentes, das dann mit anderen Debian-Entwicklern im Juni 1997 verfeinert wurde, bevor es als öffentliche Richtlinie akzeptiert wurde. Version 1.0 wurde am 5. Juli 1997 ratifiziert. Am 26. April 2004 wurde die überarbeitete Version 1.1 ratifiziert. Sie ersetzt seitdem ihren Vorgänger.\n\nEin besonders bedeutender, auch über das Debian-Projekt hinaus genutzter Teil des Vertrages sind die \"Debian-Richtlinien für freie Software\" (DFSG). Die Gemeinschaft um die Etablierung des Begriffes \"Open Source\" in der Öffentlichkeit verwendete diese als Grundlage, um ihre Definition von \"Open Source\" zu verfassen. Bruce Perens verallgemeinerte die Richtlinien, indem er Debian aus dem Text strich, um \"The Open Source Definition\" (dt. \"Die Open Source Definition\") zu schaffen. Sie wird seitdem von der Open Source Initiative (OSI) verwendet. Mit der Zeit haben sich hier allerdings einige Unterschiede ergeben.\n\nDie im Vertrag festgehaltene Verpflichtung zur Bereitstellung von freier Software wird vom Debian-Projekt sehr ernst genommen. Zentrale Diskussionen im Linux-Umfeld werden maßgeblich vom Projekt bestimmt, wie die konsequent freie Dokumentation der Programme (Diskussion über die GFDL) oder die Vermeidung von Markennamen, weil ein Hersteller darüber das Projekt beeinflussen kann. Eine Auswirkung dieser Politik war der Namensstreit zwischen Debian und Mozilla, der zu einer Umbenennung der Anwendung Firefox in Iceweasel innerhalb von Debian führte.\n\nSoftwareprobleme werden öffentlich behandelt, so auch sämtliche Sicherheitsprobleme. Aspekte der Sicherheit werden öffentlich auf der \"debian-security-announce-\"Mailingliste diskutiert. Debians Sicherheitsgutachten werden über eine öffentliche Mailingliste gesendet (sowohl innerhalb als auch außerhalb) und auf einem öffentlichen Server bekanntgegeben. Von dieser Verfahrensweise verspricht man sich ein schnelleres Auffinden von Sicherheitslücken und damit die Möglichkeit, diese eher beheben zu können. Die entgegengesetzte Herangehensweise des Security through obscurity wird dagegen als unpraktikabel angesehen. Die Tatsache, dass die Weiterentwicklung der Distribution öffentlich sichtbar unter Beteiligung einer Vielzahl von Personen geschieht, erfordert besondere Sicherheitsmaßnahmen. Beispielsweise werden Änderungen an Paketen grundsätzlich mit einem verifizierbaren Schlüssel digital signiert. Beim Anwender wird dann vor der Installation die Gültigkeit der Signatur überprüft. Diese Maßnahme soll es Dritten erschweren, schädliche Software in Debian-Pakete einzuschleusen.\n\nDie Paketbetreuer passen die Sicherheitsaspekte ihrer jeweiligen Software an die allgemeinen Grundsätze von Debian an. Daher sind Dienste nach der Installation oft „sicher“ voreingestellt, was von einem Benutzer als „Einschränkung“ empfunden werden kann. Dennoch versucht Debian, Sicherheitsaspekte und einfache Administration abzuwägen. Zum Beispiel werden Dienste wie ssh und ntp nicht inaktiv installiert, wie es bei den Distributionen der BSD-Familie üblich ist.\n\nWenn ein Sicherheitsproblem in einem Debian-Paket entdeckt wurde, wird es zusammen mit einer Einschätzung der dadurch entstehenden Gefahr direkt veröffentlicht. Parallel wird so schnell wie möglich ein Sicherheitsupdate dieses Pakets vorbereitet und auf speziellen Servern veröffentlicht. Kritische Sicherheitslücken werden auf diese Weise häufig innerhalb von Stunden geschlossen.\n\nDie von Debian angepasste Implementierung des für die Schlüsselerstellung zuständigen Zufallsgenerators der OpenSSL-Bibliothek arbeitete von September 2006 bis 13. Mai 2008 mit einer erheblichen Sicherheitslücke. Die generierten geheimen Schlüssel konnten abgeschätzt und damit in kurzer Zeit (vor-)berechnet werden (1024- und 2048-Bit-Schlüssel in ungefähr zwei Stunden). Insbesondere OpenSSH und die sichere Kommunikation in Webbrowsern waren davon betroffen – GnuPG hingegen nicht.\n\nDas Sicherheitsrisiko besteht weiterhin für alle RSA-Schlüssel, die in diesem Zeitraum auf betroffenen Systemen erstellt wurden und seit der Aktualisierung der Bibliothek nicht neu erstellt wurden. Auch alle DSA-Schlüssel, die jemals von einem Rechner (Client) mit fehlerhaftem Zufallszahlengenerator verwendet wurden, sind seitdem unsicher, selbst wenn diese ursprünglich auf einem Rechner mit korrekt arbeitendem Zufallszahlengenerator erstellt wurden.\n\nIm wurde in dem Paketmanagertool von Debian („apt“ bzw. „apt-get“) eine Sicherheitslücke entdeckt, die es einem Man-in-the-Middle-Angreifer ermöglichte Code bei einem Update auszuführen. (Remote Code Execution, CVE-2019-3462) Der Entdecker dieser Sicherheitslücke plädierte, auch vor dem Hintergrund, dass dies nicht die einzige Sicherheitslücke mit diesen Auswirkungen in apt war und solche Lücken immer passieren können, dafür, dass Debian in Zukunft standardmäßig HTTPS für Updates mit apt statt HTTP nutzt, da HTTPS die Integrität der gesamten Kommunikation mit dem Update-Server absichert. Dies wurde in der Vergangenheit mit dem Hinweis abgelehnt, dass apt selbst eine Verifizierung der Pakete vornimmt, was soweit auch korrekt ist, allerdings würde es bei Sicherheitslücken wie dieser dennoch zu einem Sicherheitsgewinn kommen, da die Sicherheitslücke dann nicht mehr von allen Man-in-the-Middle-Angreifern, die in die Verbindung eingreifen können, ausnutzbar ist; sondern nur noch von den jeweils gewählten Debian-Update-Mirrors des Gerätes.\n\nVon Debian werden zu jedem Zeitpunkt drei Varianten (Releases) parallel angeboten: (‚stabil‘), (‚Erprobung‘) und (‚instabil‘). Nach der Veröffentlichung jeder stable-Version wird die vorige stable-Version als (‚alt-stabil‘) für mindestens ein Jahr weitergeführt.\n\n\nJede Version hat einen Codenamen, der von Charakteren des Films \"Toy Story\" oder seinen Fortsetzungen stammt. Derzeit ist „Stretch“ (Debian 9) der Name des aktuellen \"stable-\"Zweigs. \"unstable\" wird seit Dezember 2000 immer „Sid“ genannt. Sid war im Film Toy Story der Junge von nebenan, der Spielzeuge kaputtgemacht hat. Erstmals erhielt Debian mit Veröffentlichung der Version 1.1 (17. Juni 1996) einen Aliasnamen. Zu diesem Zeitpunkt hatte Bruce Perens die Leitung des Projekts von Ian Murdock übernommen. Perens arbeitete beim Filmstudio Pixar, das die Toy-Story-Filme produziert.\n\nZeitweise lagen große Zeiträume zwischen den Debian-Veröffentlichungen. Darauf gab es verschiedene Reaktionen, etwa wurden Pakete verschiedener Veröffentlichungen gemischt. Dies wird jedoch unmöglich, wenn sich zentrale Teile des Systems zu stark unterscheiden. So gab es zwischen Sarge und Etch eine Änderung der glibc-ABI, die für die meisten Pakete eine Aktualisierung nötig machte. Für einige Aufgaben wie Spam- und Virenerkennung bot Debian zeitweise eine Paketquelle namens „volatile“ \"(unbeständig)\" an, die mit \"Squeeze\" durch eine neue Paket-Quelle „updates“ ersetzt wurde. Für einige Programme kann man sich auch mit sogenannten Backports behelfen. Das sind Pakete von neueren Programmversionen, die für eine alte Debian-Veröffentlichung kompiliert wurden. Dadurch werden nur die Programme aktualisiert auf die die jeweiligen Backports ausgelegt sind.\n\nInnerhalb eines Releases enthält die Abteilung \"main\" das eigentliche Debian-System. \"main\" besteht komplett aus freier Software und sonstigen Werken gemäß DFSG. Es ist möglich, allein mit Paketen aus \"main\" ein funktionstüchtiges System zu installieren. \"non-free\" enthält Software, die proprietär ist, und \"contrib\" beherbergt Software, die selbst frei ist, jedoch ohne Software aus \"non-free\" nicht lauffähig ist, wie früher Java-Programme, die die Java-Laufzeitumgebung von Sun Microsystems benötigten. \"contrib\" und \"non-free\" sind kein offizieller Teil von Debian, werden jedoch unter anderem durch Bereitstellung der für \"main\" üblichen Infrastruktur unterstützt.\n\nDebian unterstützt eine Anzahl verschiedener Hardware-Architekturen. Dabei wird zwischen offiziellen Release-Architekturen und \"Ports\" unterschieden. Um als Release-Architektur offiziell unterstützt zu werden, muss eine Anzahl Bedingungen erfüllt sein. So ist ein ausreichend großes Team nötig, eine ausreichende Anzahl entsprechender Rechner muss dem Debian-Projekt zum Erstellen von Paketen zur Verfügung stehen, und fast alle Pakete müssen auf der Architektur gebaut werden können und die Software benutzbar sein. Jede Architektur wird zunächst als Port unterstützt und kann zu einer offiziell unterstützten Architektur aufgewertet werden. Umgekehrt kann eine offizielle Release-Architektur zum Port abgewertet werden, wenn die Anforderungen an Release-Architekturen nicht mehr erfüllt sind. Für Ports gibt es keine stable-Veröffentlichungen, sondern es existiert nur die unstable-Variante.\n\nLaut einer Online-Umfrage von Heise online im Februar 2009 ist Debian Linux mit 47 % (Mehrfachnennung möglich) das am meisten verwendete freie Server-Betriebssystem in deutschen Unternehmen. Bei den freien Desktop-Betriebssystemen belegt Debian Linux mit einer Verbreitung von 29,9 % den zweiten Platz hinter Ubuntu (60,8 %), das auch von Debian abstammt – dicht gefolgt von openSUSE (28,8 %, Stand Februar 2009). Debian Linux ist die meistverwendete Linux-Distribution für Web-Server.\n\nDebian wird neben Scientific Linux, Red Hat Enterprise Linux und Windows auf der Internationalen Raumstation (ISS) eingesetzt, wie die NASA bekannt gab.\n\nDie Regierung der spanischen Region Extremadura hat von 2002 bis 2011 die Debian-basierte Distribution GNU/LinEx entwickelt und in den Schulen und im öffentlichen Gesundheitssystem eingeführt. Anfang 2012 gab die Regionalverwaltung bekannt, dass LinEx eingestellt werde, kurz darauf kündigte sie an, dass nun 40.000 Arbeitsplätze der Verwaltung auf Debian umgestellt würden.\n\nDie Stadt München ist mit ihren Debian-basierten Betriebssystemen LiMux zwischen 2006 und 2013 auf freie Software umgestiegen. Das deutsche Bundesamt für Sicherheit in der Informationstechnik setzte ab 2008 unter anderem Debian auf Desktopsystemen ein. Auch Wien bot von 2004 bis 2009 mit Wienux der Stadtverwaltung eine Debian-basierte freie Alternative an. 2009 wurde Skolelinux, eine angepasste Debian-Version, in einer Pilotphase an elf Schulen im Land Rheinland-Pfalz getestet, nachdem bereits in Hamburg das System vom „Projekt 3s“ in etlichen Schulen eingeführt worden war.\n\nDas Debian-Projekt unterstützt neben der Linux-Distribution \"Debian GNU/Linux\" mit Linux-Kernel noch weitere Varianten des GNU-Systems mit anderen Kernen.\n\nDurch die Veröffentlichung von \"Squeeze\" im Jahr 2011 fand mit Debian GNU/kFreeBSD die erste Veröffentlichung mit dem Kernel des FreeBSD-Betriebssystems statt. Diese steht vorerst nur für x86-Architekturen (32 und 64 Bit) zur Verfügung. Die Namensgebung Debian GNU/kFreeBSD soll betonen, dass es sich lediglich um den Kernel von FreeBSD handelt, während die Systemwerkzeuge wie make dem GNU-System entsprechen, nicht der BSD-Familie. Das System ist also für Anwender meist ähnlicher zu Debian GNU/Linux als zu FreeBSD. \"Jessie\" wird den GNU/kFreeBSD-Port aufgrund anhaltender Probleme und enttäuschender Entwicklungsfortschritte jedoch nicht mehr enthalten.\n\nIn Zukunft soll auch die Variante Debian GNU/Hurd mit dem Kernel GNU Hurd veröffentlicht werden. Konkrete Veröffentlichungspläne gibt es allerdings noch nicht. Eine Variante Debian GNU/NetBSD mit dem Kernel von NetBSD wurde 2002 aufgegeben.\n\nUnter einem \"Debian Pure Blend\" (Debian-intern auch kurz \"Blend\") versteht man eine interne Anpassung von Debian GNU/Linux, die einem speziellen Anwendungszweck dient. Blends bilden thematische Substrukturen innerhalb des unstrukturierten Paketpools von etwa 30.000 Binärpaketen von Debian und erlauben daher einen einfachen Zugriff auf die relevanten Pakete für spezifische Fachgebiete. Darüber hinaus steht hinter einem Blend auch ein für das Fachgebiet kompetentes Entwicklerteam, das als Ansprechpartner für bestimmte Fachgebiete dient und sich mit der Paketierung der zu diesem Fachgebiet gehörenden Software beschäftigt.\n\nDie bekanntesten Blends sind:\n\nDie große Auswahl an Paketen und das zuverlässige System der Paketverwaltung machen Debian attraktiv, um davon weitere eigenständige Distributionen abzuleiten. Rechtlich wird dies durch die für alle Komponenten geltende, weitgehende Freiheit gewährende Lizenz möglich. Daher gibt es eine große Anzahl von Distributionen, die hauptsächlich oder ausschließlich Pakete aus Debian verwenden. Einige weitverbreitete Distributionen nutzen Debian als Grundlage. Beispiele dafür sind Ubuntu, Knoppix und Linux Mint. Laut \"GNU/Linux Distribution Timeline\" gehen aus Debian und den daraus abgeleiteten Derivaten über 480 auf Debian basierende Distributionen hervor. \"(Stand Oktober 2012)\"\nViele dieser Distributionen sind für einen speziellen Zweck, wie zum Beispiel den Einsatz als Server oder in der Schule, ausgerichtet.\n\nUnter dem Namen \"Devuan\" erschien im Mai 2017 ein seit 2014 entwickeltes Debian-Derivat, das auf systemd explizit verzichtet.\n\nNach einer vagen Schätzung des Debian-Entwicklers James Bromberger ist der Quellcode aller in Debian 7.0 enthaltenen Programme etwa 14 Milliarden Euro wert. Der Schätzung liegen Annahmen über das Jahresgehalt und die Programmierleistung eines durchschnittlichen Programmierers zugrunde.\n\nDebian wird mit dem M68K-Port benutzt, um alte Systeme wie Atari ST, Amiga oder Macintosh mit aktueller Linux-Software zu betreiben. Etwa 20 aktive Entwickler arbeiten an diesem Port, der von etwa 150–200 Nutzern weltweit verwendet wird.\n\n\n"}
{"id": "1564", "url": "https://de.wikipedia.org/wiki?curid=1564", "title": "Fraktal", "text": "Fraktal\n\nFraktal ist ein vom Mathematiker Benoît Mandelbrot 1975 geprägter Begriff ( ‚gebrochen‘, von ‚ (in Stücke zer-)‚brechen‘), der bestimmte natürliche oder künstliche Gebilde oder geometrische Muster bezeichnet. \n\nDiese Gebilde oder Muster besitzen im Allgemeinen keine ganzzahlige Hausdorff-Dimension, sondern eine gebrochene – daher der Name – und weisen zudem einen hohen Grad von Skaleninvarianz bzw. Selbstähnlichkeit auf. Das ist beispielsweise der Fall, wenn ein Objekt aus mehreren verkleinerten Kopien seiner selbst besteht. Geometrische Objekte dieser Art unterscheiden sich in wesentlichen Aspekten von gewöhnlichen glatten Figuren.\n\nDer Begriff Fraktal kann sowohl substantivisch als auch adjektivisch verwendet werden. Das Gebiet der Mathematik, in dem Fraktale und ihre Gesetzmäßigkeiten untersucht werden, heißt \"fraktale Geometrie\" und ragt in mehrere andere Bereiche hinein, wie Funktionentheorie, Berechenbarkeitstheorie und dynamische Systeme. Wie der Name schon andeutet, wird der klassische Begriff der euklidischen Geometrie erweitert, was sich auch in den gebrochenen und nicht natürlichen Dimensionen vieler Fraktale widerspiegelt. Neben Mandelbrot gehören Wacław Sierpiński und Gaston Maurice Julia zu den namensgebenden Mathematikern.\n\nIn der traditionellen Geometrie ist eine Linie eindimensional, eine Fläche zweidimensional und ein räumliches Gebilde dreidimensional. Für die \"fraktalen Mengen\" lässt sich die Dimensionalität nicht unmittelbar angeben: Führt man beispielsweise eine Rechenoperation für ein fraktales Linienmuster tausende von Malen fort, so füllt sich mit der Zeit die gesamte Zeichenfläche (etwa der Bildschirm des Computers) mit Linien, und das eindimensionale Gebilde nähert sich einem zweidimensionalen.\n\nMandelbrot benutzte den Begriff der verallgemeinerten Dimension nach Hausdorff und stellte fest, dass fraktale Gebilde meist eine nicht-ganzzahlige Dimension aufweisen. Sie wird auch als fraktale Dimension bezeichnet. Daher führte er folgende Definition ein:\n\nJede Menge mit nicht-ganzzahliger Dimension ist also ein Fraktal. Die Umkehrung gilt nicht, Fraktale können auch ganzzahlige Dimension besitzen, beispielsweise die Peano-Kurve oder die Sierpinski-Pyramide.\n\nBesteht ein Fraktal aus einer bestimmten Anzahl von verkleinerten Kopien seiner selbst und ist dieser Verkleinerungsfaktor für alle Kopien derselbe, so verwendet man die Ähnlichkeitsdimension formula_1, die in solchen einfachen Fällen der anschaulichen Berechnung der Hausdorff-Dimension entspricht.\n\nDie Selbstähnlichkeit kann aber auch nur im statistischen Sinn bestehen. Man spricht dann von Zufallsfraktalen.\n\nEtwas abstrakter betrachtet wird diese Dimension, wenn man folgende Größen einführt:\n\nSelbstähnlichkeit, eventuell im statistischen Sinn, und zugehörige fraktale Dimensionen charakterisieren also ein fraktales System bzw. bei Wachstumsprozessen sog. „fraktales Wachstum“ (z. B. Diffusionsbegrenztes Wachstum).\n\nDie einfachsten Beispiele für selbstähnliche Objekte sind Strecken, Parallelogramme (u. a. Quadrate) und Würfel, denn sie können durch zu ihren Seiten parallele Schnitte in verkleinerte Kopien ihrer selbst zerlegt werden. Diese sind jedoch keine Fraktale, weil ihre Ähnlichkeitsdimension und ihre Lebesgue’sche Überdeckungsdimension übereinstimmen. Ein Beispiel für ein selbstähnliches Fraktal ist das Sierpinski-Dreieck, welches aus drei auf die Hälfte verkleinerten Kopien seiner selbst aufgebaut ist. Es hat somit die Ähnlichkeitsdimension formula_5, während die Lebesgue’sche Überdeckungsdimension gleich 1 ist.\n\nDie Selbstähnlichkeit muss nicht perfekt sein, wie die erfolgreiche Anwendung der Methoden der fraktalen Geometrie auf natürliche Gebilde wie Bäume, Wolken, Küstenlinien usw. zeigt. Die genannten Objekte sind in mehr oder weniger starkem Maß selbstähnlich strukturiert (ein Baumzweig sieht ungefähr so aus wie ein verkleinerter Baum), die Ähnlichkeit ist jedoch nicht streng, sondern stochastisch. Im Gegensatz zu Formen der euklidischen Geometrie, die bei einer Vergrößerung oft flacher und damit einfacher werden (etwa ein Kreis), können bei Fraktalen immer komplexere und neue Details auftauchen.\n\nFraktale Muster werden oft durch rekursive Operationen erzeugt. Auch einfache Erzeugungsregeln ergeben nach wenigen Rekursionsschritten schon komplexe Muster.\n\nDies ist zum Beispiel am Pythagoras-Baum zu sehen. Ein solcher Baum ist ein Fraktal, welches aus Quadraten aufgebaut ist, die so angeordnet sind wie im Satz des Pythagoras definiert.\n\nEin weiteres Fraktal ist das Newton-Fraktal, erzeugt über das zur Nullstellenberechnung verwendete Newton-Verfahren.\n\nBeispiele für Fraktale im dreidimensionalen Raum sind der Menger-Schwamm und die Sierpinski-Pyramide auf Basis des Tetraeders (so wie das Sierpinski-Dreieck auf dem gleichseitigen Dreieck basiert). Entsprechend lassen sich auch in höheren Dimensionen Fraktale nach Sierpinski bilden – bspw. basierend auf dem Pentachoron im vierdimensionalen Raum.\n\nDurch ihren Formenreichtum und den damit verbundenen ästhetischen Reiz spielen sie in der digitalen Kunst eine Rolle und haben dort das Genre der Fraktalkunst hervorgebracht. Ferner werden sie bei der computergestützten Simulation formenreicher Strukturen, beispielsweise realitätsnaher Landschaften, eingesetzt. Um in der Funktechnik verschiedene Frequenzbereiche zu empfangen, werden Fraktalantennen genutzt.\n\nFraktale Erscheinungsformen findet man auch in der Natur. Dabei ist jedoch die Anzahl der Stufen von selbstähnlichen Strukturen begrenzt und beträgt oft nur drei bis fünf. Typische Beispiele aus der Biologie sind die fraktalen Strukturen bei der grünen Blumenkohlzüchtung Romanesco und bei den Farnen. Auch der Blumenkohl hat einen fraktalen Aufbau, wobei man es diesem Kohl auf den ersten Blick häufig gar nicht ansieht. Es gibt aber immer wieder einige Blumenkohlköpfe, die dem Romanesco im fraktalen Aufbau sehr ähnlich sehen.\n\nWeit verbreitet sind fraktale Strukturen ohne strenge, aber mit statistischer Selbstähnlichkeit. Dazu zählen beispielsweise Bäume, Blutgefäße, Flusssysteme und Küstenlinien. Im Fall der Küstenlinie ergibt sich als Konsequenz die Unmöglichkeit einer exakten Bestimmung der Küstenlänge: Je genauer man die Feinheiten des Küstenverlaufes misst, umso größer ist die Länge, die man erhält. Im Falle eines mathematischen Fraktals, wie beispielsweise der Kochkurve, wäre sie unbegrenzt.\n\nFraktale finden sich auch als Erklärungsmodelle für chemische Reaktionen. Systeme wie die Oszillatoren (Standardbeispiel Belousov-Zhabotinsky-Reaktion) lassen sich einerseits als Prinzipbild verwenden, andererseits aber auch als Fraktale erklären. Ebenso findet man fraktale Strukturen auch im Kristallwachstum und bei der Entstehung von Mischungen, z. B. wenn man einen Tropfen Farblösung in ein Glas Wasser gibt. Die Lichtenberg-Figur zeigt ebenfalls fraktale Struktur.\n\nDas Auffasern von Bast lässt sich über die fraktale Geometrie von Naturfaserfibrillen erklären. Insbesondere ist die Flachsfaser eine fraktale Faser.\n\nFraktale können auf viele verschiedene Arten erzeugt werden, doch alle Verfahren enthalten ein rekursives Vorgehen:\n\n\nEs gibt fertige Programme, sogenannte Fraktalgeneratoren, mit denen Computeranwender auch ohne Kenntnis der mathematischen Grundlagen und Verfahren Fraktale darstellen lassen können.\n\nDas \"optionale\", also nicht notwendige \"F\" wird im Allgemeinen als Strecke benutzt, die durch eine Anweisungsfolge ersetzt wird. Wie das \"F\" stehen auch andere groß geschriebene Buchstaben wie \"R\" und \"L\" für einen Streckenabschnitt, der ersetzt wird. \"+\" und \"−\" stehen für einen bestimmten Winkel, der im Uhrzeigersinn oder gegen den Uhrzeigersinn läuft. Das Symbol \"|\" bezeichnet eine Kehrtwendung des Zeichenstiftes, also eine Drehung um 180°. Gegebenenfalls setzt man dafür ein entsprechendes Vielfaches des Drehwinkels ein.\n\n F → R\n R → +R--L+\n\nF ist eine einfache Strecke zwischen zwei Punkten. F → R heißt, dass die Strecke F durch R ersetzt wird. Dieser Schritt ist notwendig, da es zwei rekursive Ersetzungen R und L besitzt, die sich gegenseitig enthalten. Im Weiteren wird wie folgt ersetzt:\n\nAb einem bestimmten Abschnitt muss dieser Ersetzungsprozess abgebrochen werden, um eine Grafik zu bekommen:\n\nDabei stellen r und l jeweils eine fest vorgegebene Strecke dar.\n\nDaneben spielen in der Natur auch „Zufallsfraktale“ eine große Rolle. Diese werden nach probabilistischen Regeln erzeugt. Dies kann etwa durch Wachstumsprozesse geschehen, wobei man beispielsweise diffusionsbegrenztes Wachstum (Witten und Sander) und „Tumorwachstum“ unterscheidet. Im ersten Fall entstehen baumartige Strukturen, im letzten Fall Strukturen mit runder Form, je nachdem, in welcher Weise man die neu hinzukommenden Teilchen an die schon vorhandenen Aggregate anlagert. Wenn die fraktalen Exponenten nicht konstant sind, sondern z. B. von der Entfernung von einem zentralen Punkt des Aggregats abhängen, spricht man von sog. \"Multifraktalen\".\n\n\n"}
{"id": "1735", "url": "https://de.wikipedia.org/wiki?curid=1735", "title": "File Allocation Table", "text": "File Allocation Table\n\nFile Allocation Table (kurz FAT [], für \"Dateizuordnungstabelle\") bezeichnet eine ursprünglich 1977 von Microsoft entwickelte, weit verbreitete Familie von Dateisystemen, die zum Industriestandard erhoben wurde und bis heute auch über Betriebssystemgrenzen hinweg als fast universelles Austauschformat dient. Wesentliche Erweiterungen wurden auch von Seattle Computer Products, Compaq, Digital Research und Novell eingebracht. Als proprietäre Nachfolger entwickelte Microsoft NTFS und exFAT.\n\nDas FAT-Dateisystem wurde ursprünglich 1977 in einer 8-Bit-Variante von Marc McDonald für Microsofts Standalone Disk BASIC-80 für 8080-Prozessoren entwickelt, 1978 auf einer DEC PDP-10 unter Zuhilfenahme eines 8086-Simulators für Standalone Disk BASIC-86 portiert, und 1979 für Microsofts Betriebssystem MDOS/MIDAS adaptiert.\n\nEbenfalls 1979 wurde Standalone Disk BASIC-86 von Bob O'Rear auf eine von Seattle Computer Products (SCP) entwickelte S-100-Bus-Hardware-Plattform angepasst. Bei dieser Gelegenheit wurde Tim Paterson auf das Dateisystem aufmerksam, das er 1980 als konzeptionelle Grundlage seines 12-Bit-Dateisystems für SCPs Betriebssystem QDOS wählte, welches, umbenannt in 86-DOS, von Microsoft zunächst lizenziert, aufgekauft und daraufhin 1981 die Ausgangsbasis für MS-DOS und PC DOS 1.0 wurde.\n\nZu der Familie der FAT-Dateisysteme gehören:\n\n\nMit der 1980 erschienenen ersten Version von QDOS bzw. 86-DOS wurde FAT12 als Dateisystem für 8,0″- und 5,25″-Disketten eingeführt. Erst ab 86-DOS 0.42 von Februar 1981 wiesen die internen Ordnungsstrukturen jedoch ein Format auf, das dem späteren FAT12-Format in MS-DOS und PC DOS in allen wesentlichen Punkten glich. Aufgrund abweichender logischer Geometrien und der Tatsache, dass der BIOS Parameter Block (BPB) erst mit DOS 2 eingeführt wurde, können jedoch (mit Ausnahme von SCP MS-DOS 1.25) weder MS-DOS noch PC DOS auf unter 86-DOS formatierte Medien zugreifen.\n\nAnfangs wurden keine Unterverzeichnisse verwaltet. Das änderte sich mit MS-DOS Version 2.0.\n\nFAT12 wird nur auf Datenträgern bzw. Partitionen bis zu einer Größe von 16 MiB eingesetzt; es ist bis heute auf allen FAT-formatierten 3,5″-Disketten im Einsatz.\n\nMerkmale:\n\n\nFAT16 ist ein Dateisystem, das 1983 zu \"FAT12\" dazukam. Durch die zunehmende Größe der eingesetzten Festplatten wurde eine Erweiterung des Adressraumes notwendig. Nun waren selbst mit 512-Byte-Clustern zumindest theoretisch insgesamt 32 MiB große Platten verwaltbar.\n\nDie ursprüngliche FAT16-Implementierung verwendete auf partitionierten Medien in der Regel (abhängig vom jeweiligen DOS-OEM) den Partitiontyp 04h und einen noch vergleichsweise kurzen BIOS Parameter Block (BPB) im Bootsektor. Dessen genauer Aufbau und Inhalt hing insbesondere bei DOS 2.x noch stark von der verwendeten DOS-Version ab, er enthielt jedoch in allen Fällen nur einen 16-Bit breiten Eintrag für die Sektorenanzahl, was die Größe von FAT16-Laufwerken auf 32 MiB bis 512 MiB beschränkte (je nach Betriebssystemversion). Mit OS/2 Release 1 wurde ein Enhanced BIOS Parameter Block (EBPB) eingeführt, erkennbar am Signaturbyte 28h (für DOS-BPB-Version 4.0) an Offset +26h. Mit Einführung von DOS 3.31 wurde dieser durch den nochmals erweiterten, heute allgemein für FAT12 und FAT16 verwendeten \"Extended BIOS Parameter Block\" (XBPB) mit Signatur 29h (für DOS-BPB-Version 4.1) an Offset +26h ersetzt. EBPB und XBPB zeichnen sich u. a. dadurch aus, dass der Eintrag für die Zahl der Sektoren auf 32 Bit Breite wuchs, womit erstmals FAT16-Laufwerke mit bis zu 2 GiB, später 4 GiB, möglich wurden, auch wenn die damaligen Betriebssysteme davon noch keinen Gebrauch machen konnten. Diese größere Variante von FAT16 wurde in Entwicklerkreisen „BigDOS“ genannt, daher stammt auch ihr offizieller Name \"FAT16B.\" Da ältere Betriebssysteme mit diesem neuen Typ nicht arbeiten konnten, wurde für die Verwendung auf partitionierten Medien auch ein neuer Partitionstyp (06h) dafür definiert. Die alte FAT16-Variante wird zwar nach wie vor unterstützt, findet aber (bis auf die forcierte Erzeugung von sehr kleinen FAT16-Partitionen mit dem Partitionstyp 04h) in der Praxis keine Verwendung mehr, da spätestens seit DOS 5 bei der Erzeugung von FAT12- und FAT16-Partitionen gleichermaßen nur noch Bootsektoren mit XBPB geschrieben werden, um einige neue Betriebssystemfunktionen optimal zu unterstützen. Die Tatsache, dass es eigentlich zwei FAT16-Typen gibt, ist in der Allgemeinheit nicht mehr präsent, mehr noch, da \"FAT12\" fast nur noch für Disketten verwendet wird, wird heute \"FAT\" oft fälschlicherweise nur mit \"FAT16\" (und das auch nur in der beschriebenen \"FAT16B\"-Variante) gleichgesetzt, obwohl darunter eigentlich mehrere FAT12- und FAT16-Typen zu verstehen wären. Allerdings benötigt das Server-Betriebssystem Novell-Netware bis zur Version 4.0 noch eine bis zu 16 MiB große „DOS“-Bootpartition, die (automatisch) mit FAT12 erzeugt wurde.\n\nErfolgt der Zugriff über Logical Block Addressing (LBA), wird eine FAT16-Partition auch als \"FAT16X\" bezeichnet.\n\nFAT16 hat folgende Merkmale:\n\nEine Weiterentwicklung erfolgte mit \"FAT32\".\n\nFAT32 ist ein von Microsoft entwickeltes Dateisystem, das im Sommer 1996 mit Windows 95B eingeführt wurde und die Vorgängerversion \"FAT16\" ergänzt.\n\nPartitionen kleiner als 512 MiB werden nach wie vor mit FAT16 erzeugt, von 512 MiB bis 2 GiB hat man die Wahl, ab 2 GiB wird FAT32 benutzt. Die Adressierung arbeitet mit 32 Bit, wovon 4 Bit reserviert sind, so dass 2 = 268.435.456 Cluster adressiert werden können.\n\nFAT32 kann außerdem mit allen Windows-Versionen seit Windows 95B sowie – anders als NTFS – problemlos auch mit FreeDOS und Enhanced DR-DOS verwendet werden. Da Windows je nach Version von Haus aus nur wenige Dateisysteme unterstützt, wird FAT32 trotz seiner Beschränkungen zum Datenaustausch sowohl mit anderen Windows-Systemen als auch mit Nicht-Windows-Systemen (z. B. macOS, Linux) eingesetzt, z. B. auf USB-Speichersticks und mobilen Festplatten.\n\nSpielekonsolen wie beispielsweise die PlayStation 3 oder digitale Satellitenreceiver setzen bei extern angeschlossenen Festplatten häufig FAT32 als Dateisystem voraus.\n\nEin Nachteil eines standardkonformen FAT32-Dateisystems ist, dass nur Dateien erstellt werden können, die kleiner als 4 GiB sind. Mit der rückwärtskompatiblen Erweiterung FAT32+ sind zwar auch Dateien bis zu 256 GiB möglich, diese Erweiterung wird aber nur von wenigen Systemen unterstützt. Ein weiterer Nachteil ist, dass Windows ab Version 2000 mit dem eigenen Formatierungswerkzeug nur 32 GB formatieren kann.\n\nDa bis zu einer Partitionsgröße von 8 GiB ein Cluster nur 4 KiB groß ist (bei der Standardformatierung), werden diese „kleinen und alten“ Platten verhältnismäßig besser ausgenutzt als mit FAT16, wo ein Cluster bis zu 32 KiB belegt (unter Windows NT oder Windows 2000 FAT16-Clustergröße maximal 64 KiB).\n\nErfolgt der Zugriff über Logical Block Addressing (LBA), wird eine FAT32-Partition auch als FAT32X bezeichnet.\n\nFAT32 hat folgende Merkmale:\n\nDa auch in aktuellen Windows-Installationen FAT32 und NTFS koexistieren können, ist zu beachten, dass bei der Übertragung von Dateien von NTFS auf FAT32 sowohl NTFS-Streams als auch die Berechtigungen verloren gehen, was je nach Anwendungszweck sinnvoll oder störend sein kann.\n\nVFAT (\"Virtual File Allocation Table\") ist eine Erweiterung des FAT-Formats zur Verwendung langer Dateinamen, die auf FAT12, FAT16 und seit dessen Einführung im Jahr 1997 auch auf FAT32 angewendet werden kann. Gelegentlich wird im Sprachgebrauch auch fälschlich \"VFAT\" mit \"FAT32\" gleichgesetzt.\n\nDie Designer von Windows 95 hatten das Ziel, die Nutzung von langen Dateinamen zu ermöglichen, obwohl die auf MS-DOS aufbauenden Versionen das unter der NT-Serie dafür vorgesehene Nachfolge-Dateisystem NTFS nicht unterstützen. Das wird durch einen Trick im Layout des Dateisystems erreicht. Die Datei wird wie bisher als 8.3-Dateiname gespeichert, bei längeren Namen wird jedoch ein Alias in der Form xxxxxx~1.xxx verwendet, wobei die Nummer hochgezählt wird. Der lange Name wird dann über mehrere Verzeichniseinträge verteilt, die ältere Systeme als ungültig ansehen. Während bisher ein Eintrag auf eine Datei verwies, kann jetzt eine Datei mehrere Einträge mit je 32 Byte belegen. Das endgültige Format erlaubt bis zu 255 Zeichen lange Dateinamen (wobei der Name inklusive Speicherpfad bis zu 260 Zeichen enthalten kann) und nutzt Unicode als Zeichensatz mit der Kodierung UCS-2.\n\nIn bisher von Microsoft-Systemen nicht genutzten Bereichen des Eintrages mit dem 8.3-Dateinamen werden nun auch das Erstelldatum und das Datum des letzten Zugriffes gespeichert.\n\nAuch um lange Dateinamen auf FAT12-Disketten einsetzen zu können, nutzt Windows die VFAT-Erweiterung. Mehrere zusätzliche Verzeichniseinträge liegen vor dem eigentlichen Verzeichniseintrag im FAT12-Format zur Speicherung des langen Dateinamens. Ältere Systeme (z. B. DOS) ignorieren diese Verzeichniseinträge in der Regel, da sie durch eine spezielle Kombination von Attributen markiert sind, u. a. als „Volume“ und „Hidden“. Allerdings kann es durch die Verwendung des „Volume“-Attributes zur Kennzeichnung derartiger Einträge dazu kommen, dass ältere MS-DOS (vor 7.1) im dir-Befehl solche Einträge als Volumenamen interpretieren, speziell wenn der tatsächliche Volumename im Verzeichnis nicht an erster Stelle steht oder ganz fehlt.\n\n\"Windows for Workgroups 3.11\" unterstützt \"VFAT\" optional, jedoch nur für Festplatten und ohne die Möglichkeit langer Dateinamen.\n\n\"VFAT\" wird in \"Windows 95\" und höher und in \"Windows NT 3.5\" und höher unterstützt.\n\nUnter Linux wird die \"VFAT\"-Erweiterung vollständig unterstützt.\n\nIn den frühen 1990er Jahren wurde von vielen \"Linux\"-Distributionen die UMSDOS-Erweiterung für \"FAT16\" eingesetzt, um \"Linux\" zu installieren, ohne das Festplattenlaufwerk neu partitionieren und formatieren zu müssen. \"UMSDOS\" fügt zu einem \"FAT\"-Dateisystem eine darüberliegende \"Unix\"-kompatible Schicht hinzu. Diese verwaltet Dateien, die den Namen --linux-.--- tragen. Darin werden Benutzerrechte und auch lange Dateinamen gespeichert.\n\nIn \"Linux 2.6.11\" wurde \"UMSDOS\" aus dem Kernel entfernt, da es nicht mehr weiterentwickelt wird. Es gibt als Ersatz ein \"POSIX-Overlay\"-Dateisystem, das \"FUSE\" verwendet und über einem normalen \"FAT\"-Dateisystem „eingeblendet“ werden kann.\n\nUVFAT existierte nur für eine kurze Zeit und hat die \"VFAT\"-Erweiterung zur Speicherung langer Dateinamen genutzt, während der \"UMSDOS\"-Mechanismus für die unter allen \"FAT\"-Versionen fehlenden Benutzerrechte verwendet wurde. So waren unter \"Linux\" angelegte lange Dateinamen auch unter modernen \"Windows\"-Versionen lesbar und umgekehrt. Die Entwicklung wurde bereits vor derjenigen der \"UMSDOS\"-Erweiterung wieder eingestellt.\n\nEs gibt folgende Derivate:\nexFAT (\"Extended File Allocation Table\") ist ein speziell für Flash-Speicher entwickeltes Dateisystem. Eingeführt wurde es 2006 mit Windows CE 6.0. exFAT wird dort eingesetzt, wo NTFS nur schwer oder gar nicht implementierbar ist. Windows 7 unterstützt exFAT nativ, Windows Vista erst ab Service Pack 1. Für Windows XP ab SP2 hat Microsoft ein Aktualisierungspaket bereitgestellt und beschrieben. Ab Mac-OS-X-Version 10.6.5 wird exFAT auf Apple-Computern vollständig unterstützt.\n\nDie Vorteile gegenüber vorherigen Versionen sind:\n\nNachteile:\n\n\nTFAT (\"Transaction-safe File Allocation Table\") bietet insbesondere für mobile Geräte mit fest eingebautem Flash-Speicher Schutz vor Beschädigungen des Dateisystems, zum Beispiel wenn während einer Schreiboperation die Stromversorgung des Gerätes ausfällt.\n\nDafür wird die FAT doppelt geführt: einmal als FAT1 mit den aktuellen Dateizuordnungen und einmal als FAT0 mit dem letzten als konsistent bekannten Stand des Dateisystems. FAT0 wird erst nach erfolgreichem Abschluss einer Transaktion aktualisiert, indem FAT1 nach FAT0 kopiert wird. Eine Transaktion ist beispielsweise das Anlegen einer neuen Datei.\n\nWährend des Ablaufs einer Transaktion werden Änderungen am Dateisystem in neu angelegten Clustern gespeichert und FAT1 wird entsprechend angepasst. So kann im Fehlerfall eine unvollständig ausgeführte Transaktion durch Kopieren von FAT0 nach FAT1 rückgängig gemacht und das Dateisystem auf den Stand von vor Beginn der Transaktion zurückversetzt werden.\n\nDas rechnerische Limit für TFAT-Partitionen liegt bei einer Sektorgröße von 512 Byte bei bis zu 2 TiB.\n\nTFAT ist zwar explizit für nicht entfernbaren Speicher gedacht, kann jedoch auch mit Wechselspeichermedien verwendet werden. Allerdings kann es zu Problemen kommen, wenn ein TFAT-Medium in einem anderen Gerät ohne Unterstützung für TFAT verwendet wird. Prinzipiell ist es möglich, von dem Medium zu lesen, doch wird es dann wie ein normales FAT-Medium angesehen werden. Schreibvorgänge werden also nicht transaktionssicher geschrieben. Auch können mit TFAT erstellte Verzeichnisse nicht von FAT-Systemen gelöscht werden.\n\nTFAT wird üblicherweise nicht von Desktopsystemen unterstützt. Unterstützt wird es von Microsoft für Mobilgeräte seit Windows Mobile 6.5 und Windows CE ab Version 6.0.\n\nEin FAT-Dateisystem gliedert sich in fünf Bereiche:\n\nAlle Mehrbyte-Werte (16/32 Bit) sind im Little Endian gespeichert, d. h. niederwertigste Bytes zuerst.\n\nDer Bootsektor enthält teilweise ausführbaren x86-Maschinencode, der das Betriebssystem laden soll. An anderen Stellen enthält er Informationen über das FAT-Dateisystem.\n\nAnschließend unterscheiden sich die Daten je nach FAT-Variante. Bei FAT12 und FAT16 folgt diese Datenstruktur:\n\nFAT32 benutzt eine davon abweichende Struktur ab Offset 24:\n\nZwischen Bootsektor und der ersten FAT können Sektoren reserviert werden, die vom Dateisystem nicht benutzt werden. Dieser Bereich kann von einem Bootmanager oder für betriebssystemspezifische Erweiterungen genutzt werden. Auf den meisten FAT12- oder FAT16-Dateisystemen existieren – außer dem Bootsektor – keine weiteren reservierten Sektoren. Die FAT folgt somit direkt im Anschluss an den Bootsektor. FAT32-Dateisysteme enthalten in der Regel noch einige Erweiterungen zum Bootsektor sowie eine komplette Sicherungskopie des Bootsektors und der Erweiterungen.\n\nDie FAT ist eine Art Tabelle fester Größe, in der über die belegten und freien \"Cluster\" eines FAT-Dateisystems Buch geführt wird. Ein Cluster ist die aus einem oder mehreren Sektoren bestehende Zuordnungseinheit, die von einer Datei belegt werden kann. Der Datenbereich ist in eine feste Anzahl von Clustern eingeteilt. Zu jedem dieser Cluster existiert ein Eintrag in der FAT, der Folgendes angeben kann:\n\n\nDie Größe (in Bit) und der Wertebereich der Tabelleneinträge unterscheiden sich zw. FAT12, FAT16 und FAT32 wie folgt:\n\nDie Lage der belegten Cluster einer Datei können aus den Adressen der zugehörigen FAT-Einträge berechnet werden. Die FAT-Einträge bilden eine einfach verkettete Liste.\n\nWegen ihrer grundlegenden Bedeutung für das Dateisystem existieren in der Regel zwei Kopien der FAT, um bei Datenverlust noch immer eine funktionsfähige zweite FAT zu haben. Mit diversen Programmen ist so eine Datenwiederherstellung in vielen Fällen möglich.\n\nAuf Installationsdisketten oder mit Spezialprogrammen formatierten Medien findet man manchmal keine zweite FAT, wodurch der verfügbare Speicherplatz etwas größer wird. Theoretisch ist es auch möglich, ein Dateisystem mit mehr als zwei FAT-Kopien zu formatieren. Diese Dateisysteme können zwar in der Regel von jedem Betriebssystem gelesen werden, jedoch wird die dritte (und jede weitere FAT-Kopie) bei Schreibzugriffen meist nicht aktualisiert, so dass bei Beschädigung der ersten beiden FATs oft keine Reparatur unter Zuhilfenahme der weiteren Kopien möglich ist.\n\nDas Stammverzeichnis (englisch \"root directory\"), auch Wurzelverzeichnis oder Hauptverzeichnis genannt, ist eine Tabelle von Verzeichniseinträgen. Jede Datei oder Unterverzeichnis wird in der Regel durch je einen Verzeichniseintrag repräsentiert. Die bei Windows 95 eingeführte Erweiterung um „lange Dateinamen“ benutzt jedoch ggf. mehrere Verzeichniseinträge pro Datei bzw. Verzeichnis, um die langen Dateinamen unterzubringen.\n\nDas Stammverzeichnis folgt bei FAT12 und FAT16 direkt der FAT und hat eine feste Größe und damit eine Maximalanzahl an Verzeichniseinträgen. Diese wird beim Formatieren des Dateisystems festgelegt und kann später – außer mit Spezialsoftware – nicht mehr geändert werden.\n\nBei FAT32 hat das Stammverzeichnis eine variable Größe und kann an einer beliebigen Position des Datenbereichs beginnen.\n\nJe nach Medientyp gibt es unterschiedliche Vorgabegrößen für das Stammverzeichnis. Mit speziellen Formatierungsprogrammen lässt sich jedoch die Größe des Stammverzeichnisses frei wählen. So besitzen beispielsweise Installationsdisketten, die nur sehr wenige Archivdateien enthalten, oft ein minimales Stammverzeichnis, das nur einen Sektor groß ist und somit nur Platz für 16 Verzeichniseinträge bietet.\n\nEin Verzeichniseintrag besteht aus 32 Bytes.\n\nZusammenspiel:\nSoll nun eine Datei gelesen werden, wird der zugehörige Verzeichniseintrag herausgesucht. Neben den Attributen kann hier nun der Startcluster selektiert werden. Die weiteren Cluster werden dann über die FAT herausgesucht. Am Ende terminiert die Weitersuche jener FAT-Tabelleneintrag, welcher den Wert FFFFFFh enthält.\n\nEin Unterverzeichnis wird als normale Datei angelegt, außer dass der Eintrag im übergeordneten Verzeichnis mit dem entsprechenden Bit markiert ist. Der Aufbau der Einträge ist mit jenen des Hauptverzeichnisses identisch. Da die Cluster der Unterverzeichnisse über die FAT verknüpft werden, können sie beliebig wachsen und haben keine Begrenzung in der Zahl der verwaltbaren Dateien.\n\nDer Atari ST benutzt für Disketten eine Variante des FAT12-Dateisystems und kann daher unter MS-DOS formatierte und beschriebene Disketten lesen und schreiben. Ursprünglich konnten auf dem Atari formatierte Disketten nicht unter MS-DOS benutzt werden, wohl aber unter MS-DOS formatierte und auf dem Atari beschriebene Disketten. Diese Inkompatibilitäten wurden in späteren GEMDOS-Versionen behoben. Die Unterschiede der Atari-Implementierung im einzelnen:\n\n\n\n"}
{"id": "2801", "url": "https://de.wikipedia.org/wiki?curid=2801", "title": "Kylix (Entwicklungsumgebung)", "text": "Kylix (Entwicklungsumgebung)\n\nKylix war eine integrierte Entwicklungsumgebung vom Unternehmen Borland für das Betriebssystem Linux.\n\nDer Name stammt aus dem Griechischen und bezeichnet ein Trinkgefäß.\n\nKylix wurde basierend auf Delphi und C++Builder entwickelt. Als grafische Bibliothek setzt sie auf dem GUI-Framework Qt auf, das mittels einer Bibliothek namens CLX angesprochen wird. Die Kylix-IDE basiert auf libwine und ist damit keine native Linux-Anwendung. Leicht zu erkennen ist dies daran, dass sich die Farbe des Mauszeigers im Kylix-Fenster zu weiß ändert und Font-Metrics erstellt werden müssen. Borland hat diesen Weg gewählt, da die Kylix-IDE ein Nebenprodukt von Delphi 5 ist und die Windows-Systemaufrufe auf Linux umgesetzt werden mussten. Mit Kylix erstellte Anwendungen sind hingegen native Linuxanwendungen, die Wine nicht benötigen.\n\nUrsprünglich wurde als Programmiersprache nur Object Pascal von Delphi unterstützt; ab der Version \"Kylix 3\" (erschienen im Jahr 2002) ist auch die Programmierung in C++ möglich. Ähnlich wie bei Visual Basic wird die Programmierung durch eine visuelle Programmierumgebung erleichtert. Borland stellte neben den kommerziellen Versionen auch eine sogenannte \"Kylix Open Edition\" zum kostenlosen Herunterladen zur Verfügung, mit der allerdings nur Programme unter der GPL entwickelt werden durften.\n\nMittlerweile ist das Projekt Kylix eingestellt, es erfolgt keine Pflege mehr durch den Hersteller. Mit Kylix erstellte Programme sind auch weiterhin, auch auf aktuellen Linux-Distributionen, lauffähig. Lazarus ist eine stark an Delphi und damit auch Kylix angelehnte Entwicklungsumgebung, die unter der GNU General Public License (GPL) steht.\n\n\n"}
{"id": "2951", "url": "https://de.wikipedia.org/wiki?curid=2951", "title": "Linux", "text": "Linux\n\nAls Linux (deutsch []) oder GNU/Linux (\"siehe\" GNU/Linux-Namensstreit) bezeichnet man in der Regel freie, unixähnliche Mehrbenutzer-Betriebssysteme, die auf dem Linux-Kernel und wesentlich auf GNU-Software basieren. Die weite, auch kommerzielle Verbreitung wurde ab 1992 durch die Lizenzierung des Linux-Kernels unter der freien Lizenz GPL ermöglicht. Einer der Initiatoren von Linux war der finnische Programmierer Linus Torvalds. Er nimmt bis heute eine koordinierende Rolle bei der Weiterentwicklung des Linux-Kernels ein und wird auch als Benevolent Dictator for Life (deutsch \"wohlwollender Diktator auf Lebenszeit\") bezeichnet.\n\nDas modular aufgebaute Betriebssystem wird von Softwareentwicklern auf der ganzen Welt weiterentwickelt, die an den verschiedenen Projekten mitarbeiten. An der Entwicklung sind Unternehmen, Non-Profit-Organisationen und viele Freiwillige beteiligt. Beim Gebrauch auf Computern kommen meist sogenannte Linux-Distributionen zum Einsatz. Eine Distribution fasst den Linux-Kernel mit verschiedener Software zu einem Betriebssystem zusammen, das für die Endnutzung geeignet ist. Dabei passen viele Distributoren und versierte Benutzer den Kernel an ihre eigenen Zwecke an.\n\nLinux wird vielfältig und umfassend eingesetzt, beispielsweise auf Arbeitsplatzrechnern, Servern, Mobiltelefonen, Routern, Netbooks, Embedded Systems, Multimedia-Endgeräten und Supercomputern. Dabei wird Linux unterschiedlich häufig genutzt: So ist Linux im Server-Markt wie auch im mobilen Bereich eine feste Größe, während es auf dem Desktop und Laptops eine noch geringe, aber wachsende Rolle spielt.\n\nLinux wird von zahlreichen Nutzern verwendet, darunter private Nutzer, Regierungen, Organisationen und Unternehmen.\n\n1983 rief Richard Stallman das GNU-Projekt ins Leben. Es war das Ziel, ein frei verfügbares Unix-ähnliches, POSIX-kompatibles Betriebssystem zu schaffen. Zwar war bereits Anfang der 90er Jahre eine ansehnliche Menge von Software geschrieben worden, doch steckte der eigentliche Betriebssystem-Kernel noch in einer frühen Phase und entwickelte sich nur langsam. Die ebenso freie Berkeley Software Distribution, die sich in den 80er Jahren entwickelt hatte, war in einen Rechtsstreit mit ungewissem Ausgang verwickelt und war aus diesem Grund ebenso keine Alternative als freies Betriebssystem. Damit stand Anfang der 1990er kein vollständiges, freies System zur Verfügung, welches für Entwickler interessant gewesen wäre.\n\n1991 begann Linus Torvalds in Helsinki (Finnland) mit der Entwicklung einer Terminal-Emulation, um unter anderem seinen eigenen Computer besser zu verstehen. Mit der Zeit merkte er, dass sich das System immer mehr zu einem Betriebssystem entwickelte; daraufhin kündigte er es in der Usenet-Themengruppe für das Betriebssystem Minix, \"comp.os.minix\", an. Im September desselben Jahres sollte das System dann auf einem Server den Interessierten zur Verfügung gestellt werden. Dem damaligen FTP-Server-Administrator Ari Lemmke gefiel keiner der von Torvalds vorgeschlagenen Namen \"Freax\" oder \"Buggix\", deshalb veröffentlichte er es stattdessen in einem Verzeichnis mit dem Namen Linux. Torvalds war mit diesem Namen zunächst nicht einverstanden, gab seinen Widerstand aber schnell auf, weil er nach eigener Aussage eingestehen musste, dass Linux einfach ein besserer Name war.\n\nLinux wurde zu dieser Zeit noch unter einer proprietären Lizenz von Torvalds veröffentlicht, welche die kommerzielle Nutzung verbot. Er merkte jedoch bald, dass das den Fortschritt der Entwicklung behinderte. Er wollte allen Entwicklern deutlich mehr Freiraum geben und stellte Linux deshalb im Januar 1992 unter die GNU GPL. Es war nun möglich, Linux in GNU zu integrieren und dies als das erste freie Betriebssystem zu vertreiben. Dieser Schritt machte das System für eine noch größere Zahl von Entwicklern interessanter, da er die Modifizierung und Verbreitung vereinfachte.\n\nDie Bezeichnung Linux wurde von Torvalds anfänglich nur für den von ihm geschriebenen Kernel genutzt. Dieser wurde anfänglich auf Minix verwendet. Torvalds und die anderen Linux-Autoren lizenzierten 1992 Linux unter der GNU GPL, so dass der Kernel in GNU integriert werden konnte. Diese GNU-Variante wurde schnell zur meist genutzten Variante, da es zu dieser Zeit keinen anderen funktionsfähigen freien Kernel gab. Als Torvalds und seine Anhänger später auch das gesamte Betriebssystem als \"Linux\" bezeichneten, versuchte der Gründer des GNU-Projekts, Richard Stallman, bald, den Namen GNU/Linux durchzusetzen, um der Rolle von GNU eine in seinen Augen angemessene Geltung zu verschaffen. Diese Forderung stieß auf unterschiedliche Reaktionen. Während das GNU-Projekt und das Debian-Projekt den Namen annahmen, lehnten die meisten Entwickler und anderen Linux-Distributoren dies ab oder widersetzten sich deutlich. Begründet wurde dies einerseits mit Bequemlichkeit, weil der Name \"Linux\" als einfacher angesehen wurde, und andererseits mit dem Hinweis, dass mittlerweile eine beachtliche Menge der mit Linux ausgelieferten Software nicht aus dem GNU-Projekt stamme.\n\nDie Entwicklung des Linux-Kernels wird noch immer von Torvalds organisiert. Dieser ist dafür bei der gemeinnützigen Linux Foundation angestellt. Andere wichtige Entwickler werden oft von verschiedenen Unternehmen bezahlt. So arbeitet z. B. Andrew Morton im Auftrag von Google am Linux-Kernel und ist dabei im sogenannten \"Merge Window\" für das Sammeln aller Änderungen und das Weiterleiten an Torvalds zuständig.\n\nNeben der Kernel-Entwicklung haben sich auch andere Projekte um das Betriebssystem gesammelt, die es für eine größere Nutzerzahl interessant machten. So ermöglichen grafische Benutzeroberflächen wie KDE oder Gnome einen hohen Benutzerkomfort beim Einsatz als Desktop-System. Verschiedene auf den Desktop ausgelegte Linux-Distributionen vereinfachten die Installation und Konfiguration von Linux so weit, dass sie auch von Anfängern problemlos gemeistert werden können.\n\nEine weltweite Entwickler- und Nutzergemeinde erstellt eine Vielzahl an weiterer Software und Dokumentation rund um Linux, welche die Einsatzmöglichkeiten enorm ausgedehnt haben. Hinzu kommt, dass Hersteller proprietärer Software zunehmend einen Markt bei Linux-Anwendern erkennen und mit der Zeit vermehrt Programme für Linux anbieten. Dabei läuft die Entwicklung schwerpunktmäßig freier Software sowohl in selbstorganisierten Projekten, bestehend aus ehrenamtlichen und bezahlten Entwicklern, als auch in teilweise von Unternehmen unterstützten Stiftungen. Gemein ist allen Modellen, dass sie sich stark über das Internet vernetzt haben und dort ein Großteil der Organisation und Absprache stattfindet.\n\nSchon früh kam es rund um Linux zum Streit. 1992 griff Andrew S. Tanenbaum Linux wegen eines aus seiner Sicht veralteten Designs und eines zu liberalen Entwicklungsmodells an. Später kam Tanenbaum erneut ins Spiel, als Ken Brown an seinem Buch \"Samizdat\" schrieb und nach Anhaltspunkten suchte, dass Linux nur eine Kopie von Tanenbaums Minix sei. Tanenbaum nahm Linux diesmal in Schutz. Linux habe ein zu schlechtes Design, als dass es abgeschrieben sein könne.\n\nAnderen Streit gab es mit erklärten Konkurrenten. Schon früh wurden interne Microsoft-Dokumente (Halloween-Dokumente) bekannt, die aufzeigten, dass Microsoft annahm, Linux sei die größte Gefahr für Windows. Später begann Microsoft mit einer Kampagne, um Windows bei einer Gegenüberstellung mit Linux technisch wie wirtschaftlich gut aussehen zu lassen. Während die Community diese Kampagne recht gelassen sah, starteten vor allem Unternehmen im Linux-Umfeld Gegenkampagnen. Im Herbst 2006 aber kündigten Microsoft und Novell an, bei Interoperabilität und Patentschutz zusammenzuarbeiten, um so die Zusammenarbeit der einzelnen Produkte zu verbessern.\n\nEin anderer Konkurrent, der Unix-Hersteller SCO, erhob wiederum 2003 den Vorwurf, dass bei IBM angestellte Linux-Entwickler Quellcode von SCOs Unix in Linux kopiert hätten. Das Verfahren wurde im Sommer 2007 eingestellt, die SCO Group hat mittlerweile Insolvenz angemeldet und wurde vom Börsenhandel ausgeschlossen. 2013 wurde eine Wiederaufnahme des Verfahrens beantragt. Im Artikel SCO gegen Linux ist der Streit chronologisch dokumentiert.\n\nEbenfalls machte das Markenrecht Linux schon früh zu schaffen. So ließen einige Privatpersonen Mitte der 1990er Jahre den Namen Linux auf sich eintragen, was Torvalds nur mit viel Hilfe wieder rückgängig machen konnte. Er übertrug die Verwaltung der Markenrechte an das Linux Mark Institute, welches wiederum im Jahr 2005 auffiel, als es die Lizenzen für den Markenschutz auf bis zu 5.000 Dollar pro Jahr festlegte. Diese Summe brachte hauptsächlich viele an Community-Projekten beteiligte Gemüter in Wallung, woraufhin sich Torvalds genötigt fühlte, in einem offenen Brief Stellung zu nehmen und klarzustellen, dass das Geld schlichtweg benötigt werde, damit das gemeinnützig arbeitende Linux Mark Institute seine eigenen Kosten decken könne.\n\nDie Bezeichnung \"Linux\" wurde von Linus Torvalds anfänglich nur für den Kernel genutzt, dieser stellt der Software eine Schnittstelle zur Verfügung, mit der sie auf die Hardware zugreifen kann, ohne sie genauer zu kennen. Der Linux-Kernel ist ein in der Programmiersprache C geschriebener monolithischer Kernel. Wichtige Teilroutinen sowie zeitkritische Module sind jedoch in prozessorspezifischer Assemblersprache programmiert. Der Kernel ermöglicht es, nur die für die jeweilige Hardware nötigen Treiber zu laden. Weiterhin übernimmt der Kernel auch die Zuweisung von Prozessorzeit und Ressourcen zu den einzelnen Programmen, die auf ihm gestartet werden. Bei den einzelnen technischen Vorgängen orientiert sich das Design von Linux stark an seinem Vorbild Unix.\n\nDer Linux-Kernel wurde zwischenzeitlich auf eine sehr große Anzahl von Hardware-Architekturen portiert. Das Repertoire reicht von eher exotischen Betriebsumgebungen wie dem iPAQ-Handheld-Computer, Navigationsgeräten von TomTom oder gar Digitalkameras bis hin zu Großrechnern wie IBMs System z und neuerdings auch Mobiltelefonen wie dem Motorola A780 sowie Smartphones mit Betriebssystemen wie Android oder Sailfish OS auf dem Jolla. Trotz Modulkonzept blieb die monolithische Grundarchitektur erhalten. Die Orientierung der Urversion auf die verbreiteten x86-PCs führte früh dazu, verschiedenste Hardware effizient zu unterstützen und die Bereitstellung von Treibern auch unerfahrenen Programmierern zu ermöglichen. Die hervorgebrachten Grundstrukturen beflügelten die Verbreitung.\n\nAuf kernel.org werden alle Kernel-Versionen archiviert. Die dort zu findende Version ist der jeweilige Referenzkernel. Auf diesem bauen die sogenannten Distributionskernel auf, die von den einzelnen Linux-Distributionen um weitere Funktionen ergänzt werden. Eine Besonderheit stellt dabei das aus vier Zahlen bestehende und durch Punkte getrennte Versionsnummernschema dar, z. B. \"2.6.14.1\". Es gibt Auskunft über die exakte Version und damit auch über die Fähigkeiten des entsprechenden Kernels. Von den vier Zahlen wird die letzte für Fehlerbehebungen und Bereinigungen geändert, nicht aber für neue Funktionen oder tiefgreifende Änderungen. Aus diesem Grund wird sie auch nur selten mit angegeben, wenn man beispielsweise Kernel-Versionen vergleicht. Die vorletzte, dritte Zahl wird geändert, wenn neue Fähigkeiten oder Funktionen hinzugefügt werden. Gleiches gilt für die ersten beiden Zahlen, bei diesen müssen die Änderungen und neuen Funktionen jedoch drastischer ausfallen. Ab Version 3.0 (August 2011) wird auf die zweite Stelle verzichtet.\n\nDie Entwicklung von Linux liegt durch die GPL und durch ein sehr offenes Entwicklungsmodell nicht in der Hand von Einzelpersonen, Konzernen oder Ländern, sondern in der Hand einer weltweiten Gemeinschaft vieler Programmierer, die sich in erster Linie über das Internet austauschen. In vielen E-Mail-Listen, aber auch in Foren und im Usenet besteht für jedermann die Möglichkeit, die Diskussionen über den Kernel zu verfolgen, sich daran zu beteiligen und auch aktiv Beiträge zur Entwicklung zu leisten. Durch diese unkomplizierte Vorgehensweise ist eine schnelle und stetige Entwicklung gewährleistet, die auch die Möglichkeit mit sich bringt, dass jeder dem Kernel Fähigkeiten zukommen lassen kann, die er benötigt. Eingegrenzt wird dies nur durch die Kontrolle von Linus Torvalds und einigen speziell ausgesuchten Programmierern, die das letzte Wort bei der Aufnahme von Verbesserungen und Patches haben. Auf diese Weise entstehen täglich grob 4.300 Zeilen neuer Code, wobei auch täglich ungefähr 1.800 Zeilen gelöscht und 1.500 geändert werden (Angaben nach Greg Kroah-Hartman als Durchschnitt für das Jahr 2007). An der Entwicklung sind derzeit ungefähr 100 Verantwortliche („maintainer“) für 300 Subsysteme beteiligt.\n\nDer stabile Kernel 2.6 wurde ab Dezember 2001 auf Basis des damaligen 2.4er-Kernels entwickelt und weist eine Reihe von Neuerungen auf. Die auffälligste Auswirkung dieser Änderungen ist, dass graphische und interaktive Anwendungen deutlich schneller ausgeführt werden.\n\nEine der wichtigsten Änderungen war dabei die Verbesserung des sogenannten \"O(1)-Schedulers\", den Ingo Molnár für den 2.6er-Kernel komplett neu konzipierte. Er hat die Fähigkeit, das Zuweisen von Prozessorzeit zu unterschiedlichen Prozessen unabhängig von der Anzahl der Prozesse in konstanter Zeit zu erledigen. Seit Kernel 2.6.23 kommt allerdings stattdessen der sogenannte \"Completely Fair Scheduler\" zum Einsatz.\n\nEine andere Neuerung stellt die Einführung von Access Control Lists dar, mit deren Hilfe ein sehr fein abgestimmtes Rechtemanagement möglich ist, was vor allen Dingen in Umgebungen mit vielen Benutzern sehr wichtig ist. Ebenso verfügt der neue Kernel über ein deutlich verbessertes System der Dateiüberwachung. In der neuen Version, \"Inotify\" genannt, gibt die Überwachung bei jeder Operation an einer Datei eine Nachricht ab, was z. B. für Desktop-Suchmaschinen wichtig ist, die daraufhin ihren Index in Bezug auf diese Datei aktualisieren können.\n\nDa der Linux-Kernel alleine nicht lauffähig bzw. bedienbar wäre, muss man ihn mit Hilfssoftware zusammen verteilen, beispielsweise den GNU Core Utilities und vielen anderen Anwendungsprogrammen. Solch eine Zusammenstellung nennt man „Linux-Distribution“, sie ist eine Zusammenstellung verschiedener Software, die je nach Bedingung unterschiedlich sein kann. Die so entstehenden Distributionen unterscheiden sich teilweise sehr deutlich. Der Herausgeber einer Linux-Distribution ist der \"Distributor\".\n\nDie Notwendigkeit von Linux-Distributionen ergab sich durch das Entwicklungsmodell von Linux nahezu sofort. Die Werkzeuge des GNU-Projekts wurden zügig für Linux angepasst, um ein arbeitsfähiges System bereitstellen zu können. Die ersten Zusammenstellungen dieser Art waren 1992 \"MCC Interim Linux\", \"Softlanding Linux System\" (SLS) und \"Yggdrasil Linux\". Die älteste bis heute existierende Distribution, Slackware von Patrick Volkerding, folgte 1993 und stammt von Softlanding Linux System ab.\n\nMit der Ausbreitung der Linux-Distributionen bekamen mehr Menschen die Möglichkeit, das System zu testen, des Weiteren wurden die Distributionen immer umfangreicher, so dass ein immer größerer Einsatzbereich erschlossen werden konnte, was Linux zunehmend zu einer attraktiven Alternative zu Betriebssystemen etablierter Hersteller werden ließ. Im Laufe der Zeit änderte sich auch der Hintergrund der Distributionen: Wurden die ersten Distributionen noch der Bequemlichkeit halber und von Einzelpersonen oder kleinen Gruppen geschrieben, gibt es heutzutage teilweise sehr große Gemeinschaftsprojekte Freiwilliger, Unternehmens-Distributionen oder eine Kombination aus beidem.\n\nHinter den meisten, vorrangig kleinen Distributionen stehen heutzutage über das Internet koordinierte Projekte Freiwilliger. Die großen Distributionen werden eher von Stiftungen und Unternehmen verwaltet. Auch die Einsatzmöglichkeiten der einzelnen Distributionen differenzierten sich mit der Zeit stark. Vom Desktop-PC über Server-Installationen und Live-CDs bis hin zu Distributionen zu technischen Forschungszwecken ist alles vertreten. Die Zusammensetzung einer üblichen Linux-Distribution für den Desktop-PC umfasst eine große Zahl von Softwarekomponenten, die das tägliche Arbeiten ermöglichen. Die meisten Distributionen werden in Form fertiger CD- oder DVD-Images im Internet bereitgestellt oder mit Support-Verträgen oder Handbüchern verkauft.\n\nFür besondere Anwendungsgebiete existieren oft keine direkt installierbaren Distributionen. Hier werden Frameworks wie OpenEmbedded z. B. für Router oder Handys verwendet, um eine Distribution für den Einsatz auf dem Gerät vorzubereiten.\n\nEs wird eine große Anzahl an Distributionen angeboten, die dem Benutzer eine sehr feine Abstimmung der Auswahlkriterien auf die eigenen Bedürfnisse ermöglicht. Die Auswahl der geeignetsten Distribution ist für viele unerfahrene Benutzer daher nicht einfach. Die verwendete Software kann mehr Gewicht für Privatanwender haben als für Unternehmen, die wiederum mehr Wert auf die Verfügbarkeit eines offiziellen Kundendiensts („Support“) legen. Auch kann die Politik des Projekts oder die des Unternehmens hinter der Distribution, z. B. in Bezug auf proprietäre Software, ebenso eine Rolle spielen wie die Eigenschaften der Community in diesem Projekt.\n\nDie Liste von Linux-Distributionen enthält eine Aufzählung der wichtigsten oder populärsten Distributionen.\n\nDie Vielfalt der Distributionen, die teilweise verschiedene binäre Formate, eigene Verzeichnisstrukturen und ähnliche Unterschiede aufweisen, führt zu einem gewissen Grad an Inkompatibilität zwischen den Distributionen, der bisher auch durch Richtlinien wie den Filesystem Hierarchy Standard und der Linux Standard Base nicht behoben werden konnte. So kann Software, die für die Distribution A bereitgestellt wird, nicht notwendigerweise auch auf der Distribution B installiert werden. Verschiedene Sichtweisen und Lösungsansätze zu dieser Problematik werden im Hauptartikel Linux-Distribution näher beleuchtet.\n\nDie Einsatzgebiete von Linux sind seit der ersten Version stetig erweitert worden und decken heutzutage einen weiten Bereich ab.\n\nLinux, beziehungsweise eine Linux-Distribution, lässt sich als allein installiertes Betriebssystem betreiben, aber auch innerhalb eines Multi-Boot-Systems einsetzen. Parallel installieren kann man Linux beispielsweise neben Windows oder einem BSD wie FreeBSD oder macOS. Moderne Distributionen wie OpenSUSE, Debian oder Ubuntu führen den Nutzer mit Hilfe von grafischen Benutzeroberflächen durch die Installation auf dem PC und erkennen andere Betriebssysteme nahezu immer selbstständig. Aus weit über tausend kostenlosen Programmen kann eine individuelle Kombination ausgewählt werden. Textverarbeitung, Tabellenkalkulation, Multimedia-Anwendungen, Netzwerktools, Spiele oder wissenschaftliche Anwendungen decken die meisten Anwendungsbereiche ab, die im Büroalltag und im Privatbereich wichtig sind.\n\nTrotz Sicherheitsvorsprungs gegenüber dem am weitesten verbreiteten Betriebssystem Windows und der Möglichkeit der Parallelinstallation und umfangreichen, kostenlosen Softwareangebots wird Linux auf Desktoprechnern zögerlich eingesetzt. Auch wenn sich die verbreitetsten Linux-Desktop-Umgebungen ähnlich bedienen lassen wie Windows oder macOS, unterscheiden sie sich durch diverse Systemfunktionen von ihnen. Daher kann wie bei fast jedem Wechsel des Betriebssystems eine gewisse Einarbeitungszeit nötig sein. Im Gegensatz zur geringen Verbreitung auf dem Desktop ist Linux auf Server-Systemen, bei Embedded-Systemen und auf Smartphones bereits ein etabliertes Betriebssystem.\n\nDie Installation der meisten Distributionen ist einfach und gibt geläufige Einstellungen vor, auch die Installation der Anwendungen läuft meist vollautomatisch ab, da sie üblicherweise von einem Paketmanager übernommen wird. Da das genaue Vorgehen aber nicht bei allen Linux-Distributionen einheitlich geregelt ist, kann ein Wechsel der Linux-Distribution Einarbeitungszeit erfordern. Die Installation von Programmen, die nicht zum Umfang der Distribution gehören, kann unterschiedlich sein: Im Idealfall existiert eine Paketquelle der Programmentwickler, die im Paketmanager eingebunden werden und über diesen dann installiert werden kann. Daneben gibt es für eine Reihe von Programmen Pakete, die auf die Distribution abgestimmt zum Download verfügbar sind. Im ungünstigsten Fall muss die Software als Quellcode bezogen werden und für das jeweilige System kompiliert werden. Anwendungen, die vom Anbieter nur für macOS oder Windows auf den Markt gebracht wurden, kann man i. d. R. unter Linux mittels API-Implementierungen wie Wine, Cedega oder Darling bzw. GNUstep verwenden. In anderen Fällen muss man zu alternativen Anwendungen greifen, die für Linux verfügbar sind.\nDie beiden weit verbreiteten Desktop-Umgebungen Gnome und KDE haben unterschiedliche Bedienungskonzepte, weshalb viele Distributoren Standards und Richtlinien veröffentlichen, um sowohl Entwicklern als auch Nutzern den Umgang mit verschiedenen Desktop-Umgebungen nahezubringen und ihn zu vereinheitlichen.\n\nBekannt geworden sind größere Migrationen von Unternehmen oder Institutionen, die mehrere hundert oder tausend Rechner auf Linux-Desktops umgestellt haben, wie die Stadt München im Rahmen des LiMux-Projekts oder die Umstellung von 20.000 Desktops bei Peugeot Citroën. Durch die Auslieferung vorinstallierter Systeme durch einige Fachhändler sowie die wachsende Beliebtheit einiger Distributionen wie Ubuntu wuchs die Linux-Verwendung auf Desktoprechnern von Anfang 2007 bis Mitte 2008 um fast 30 Prozent. In Großbritannien lag der Marktanteil 2008 bei etwa 2,8 Prozent. Weltweit wurde im April 2009 im \"Market-Share-Report\" von \"Net Applications\" erstmals ein Marktanteil von einem Prozent ermittelt. Nachdem er 2010 gemäß NetMarketShare wieder auf 0,9 % gefallen war, stieg der Marktanteil bis Dezember 2011 auf 1,41 %. Ende 2016 lag der Marktanteilanteil bei\n2,2 %.\n\nAufgrund der Kompatibilität von Linux mit anderen unixoiden Systemen hat sich Linux auf dem Servermarkt besonders schnell etabliert. Da für Linux schon früh zahlreiche häufig verwendete und benötigte Serversoftware wie Webserver, Datenbankserver und Groupware kostenlos und weitgehend uneingeschränkt zur Verfügung stand, wuchs dort der Marktanteil stetig.\n\nDa Linux als stabil und einfach zu warten gilt, erfüllt es auch die besonderen Bedingungen, die an ein Server-Betriebssystem gestellt werden. Der modulare Aufbau des Linux-Systems ermöglicht zusätzlich das Betreiben kompakter, dedizierter Server. Außerdem hat die Portierung von Linux auf verschiedenste Hardwarekomponenten dazu geführt, dass Linux alle bekannten Serverarchitekturen unterstützt.\n\nEingesetzt wird es dabei für praktisch alle Aufgaben. Eines der bekanntesten Beispiele ist die Linux-Server-Konfiguration LAMP, bei der Linux mit Apache, MySQL und PHP/Perl (manchmal auch Python) kombiniert wird. Auch proprietäre Geschäftssoftware wie SAP R/3 ist mittlerweile auf verschiedenen Distributionen verfügbar und hat eine Installationszahl von über 1.000 Systemen erreicht. Das Linux Terminal Server Project ermöglicht es, sämtliche Software außer dem BIOS der Clients zentral zu verwalten.\n\nDa Linux auf einer Vielzahl von verschiedenen Hardwaretypen betrieben werden kann, ist auch die für Linux-Server genutzte Hardware ähnlich umfangreich. Auch moderne Hardware wie die von IBMs eServer p5 wird unterstützt und ermöglicht dort das parallele Ausführen von bis zu 254 Linux-Systemen (Modell p595). Auf IBM-Großrechnern der aktuellen System-z-Linie läuft Linux wahlweise nativ, mittels PR/SM in bis zu 30 LPARs oder in jeder davon unter z/VM in potenziell unbegrenzt vielen, real einigen zehntausend virtuellen Maschinen.\n\nIm Januar 2017 wurden mindestens 34 % aller Websites mittels eines Linux-Servers verfügbar gemacht. Da nicht alle Linux-Server sich auch als solche zu erkennen geben, könnte der tatsächliche Anteil um bis zu 31 Prozentpunkte höher liegen. Damit ist ein tatsächlicher Marktanteil von bis zu etwa 65 % nicht auszuschließen. Der Marktanteil von verkauften Linux-Server-Systemen lag im zweiten Quartal 2013 bei 23,2 %. Da bei Servern nicht selten von einem Kunden selbst ein anderes Betriebssystem installiert wird, gibt diese Zahl nur bedingt Auskunft über die effektive Verwendung von Linux auf Server-Systemen.\n\nFür Smartphones und Tablets gibt es speziell optimierte Linux-Distributionen. Sie bieten neben den Telefonie- und SMS-Funktionen diverse PIM-, Navigations- und Multimedia-Funktionen. Die Bedienung erfolgt typischerweise über Multi-Touch oder mit einem Stift. Linux-basierte Smartphonesysteme werden meist von einem Firmenkonsortium oder einer einzelnen Firma entwickelt und unterscheiden sich teilweise sehr stark von den sonst klassischen Desktop-, Embedded- und Server-Distributionen. Anders als im Embedded-Bereich sind Linux-basierte Smartphonesysteme aber nicht auf ein bestimmtes Gerät beschränkt, vielmehr dienen sie als Betriebssystem für Geräte ganz unterschiedlicher Modellreihen und werden oft herstellerübergreifend eingesetzt.\n\nDie Architektur dieser Smartphone- und Tablet-Distributionen hat neben dem Linux-Kernel teilweise wenig mit den klassischen Distributionen zu tun. So wird von Android nur ein Teil der sonst üblichen GNU-Software-Umgebung genutzt. Die meist auf Linux genutzten UNIX-artigen Dienste und Tools werden teilweise durch eine Java-Laufzeitumgebung ersetzt. Dadurch entstehen neue Programmierschnittstellen, die sich auf beliebigen anderen Plattformen emulieren bzw. umsetzen lassen. Trotzdem wird Android als Linux-Distribution angesehen, die viele Eigenschaften mitbringt, die es mit zahlreichen Embedded-Linux-Distributionen teilt. Andere Smartphone-Distributionen, wie etwa Firefox OS, Ubuntu for phones, Maemo, Tizen, Mer, Sailfish OS und MeeGo nutzen größere Teile der klassischen GNU-Software-Umgebung, so dass diese Distributionen teilweise einfacher mit klassischen Linux-Anwendungen ergänzt werden können und somit eher Linux-Distributionen im klassischen Sinne entsprechen.\n\nDas von HP Palm entwickelte WebOS setzt ebenfalls auf dem Linux-Kernel auf, das Userland jedoch besteht aus einer proprietären Entwicklung unter anderer Lizenz. Auch das ehemals von Samsung entwickelte Bada war neben einem RTOS-Kernel auch auf einem Linux-Kernel nutzbar, was aber von Samsung nie in dieser Kombination verkauft wurde.\n\nLinux-Systeme haben seit Ende 2010 die Marktführerschaft auf dem schnell wachsenden Smartphone-Markt übernommen. Sie weisen in Deutschland seit Februar 2013 durchgehend einen Marktanteil von über 70 % auf mit einem bisherigen Maximum von über 82 % im Juli 2014 (Anteile Linux-basierter Alternativen zu Android wurden in der Statistik nicht explizit angegeben). Vorwiegend Android-Geräte haben Apple iOS, Windows Phone und Symbian OS erfolgreich zurückgedrängt.\n\nDa Linux beliebig angepasst und optimiert werden kann, hat es sich auch in Rechenzentren stark verbreitet, in denen speziell angepasste Versionen auf Großrechnern, Computerclustern (siehe Beowulf) oder Supercomputern laufen.\n\nIn der TOP500-Liste der schnellsten Supercomputer (Stand Juni 2018) werden alle gelisteten Systeme mit Linux betrieben. Der im Desktop-Bereich größte Konkurrent Windows spielt bei Höchstleistungsrechnern keine Rolle. Im Juni 2011 waren es noch 4 Systeme (darunter Platz 40), die mit dem Betriebssystem Windows liefen.\n\nLinux setzt sich aus vielfältigen Gründen auch immer mehr in der Industrie, speziell in der Automobilindustrie, durch. Das weltweit erste von Linux betriebene Infotainment-System wurde von General Motors in Kooperation mit Bosch entwickelt. Die GENIVI Alliance definiert Anforderungen an eine Linux-Distribution speziell für Infotainment-Systeme in Fahrzeugen. Die größte Marktdurchdringung hat Linux in Japan. Zu den bekannten Unternehmen, die Linux verwenden, gehören: Ashisuto, Aisin AW, JVC KENWOOD Corporation, NTT DATA MSE und Turbo Systems.\n\nFerner können auch NAS-Speichersysteme oder WLAN-Router Linux als Betriebssystem nutzen. Vorteil ist, dass eine sehr aktive Entwickler-Community besteht, auf deren Ressourcen (der Kernel mit den Schnittstellen-, Speicherverwaltungs- und Netzwerkfunktionen, aber z. B. auch umfangreiche Entwicklerprogramme, bereits bestehender Code wie die Benutzeroberflächen OPIE oder GPE Palmtop Environment, Erfahrung etc.) die Hersteller dabei zurückgreifen können.\n\nDie Gründe für die Bewertung von Linux als sicheres System sind verschieden und hängen von dessen Aufgaben und der verwendeten Softwarekonfiguration ab. So verfügt Linux als Desktop-System über eine strenge Unterteilung der Zugriffsrechte, die bei anderen verbreiteten Desktop-Systemen im Normalfall nicht eingehalten wird. Dies führt unter anderem dazu, dass viele Funktionsprinzipien verbreiteter Würmer und Viren bei Linux nicht greifen können beziehungsweise nur den ausführenden Benutzer, jedoch nicht das ganze System, kompromittieren können. Eine Kompromittierung des Nutzers kann gleichwohl zu sensiblen Datenverlusten führen. Bisher traten nur sehr wenige Viren unter Linux auf, beispielsweise Staog und Bliss. Im Vergleich zu anderen Desktop-Systemen hat Linux die erste größere Verbreitung bei Nutzern mit einem sehr technischen und sicherheitsbewussten Umfeld erfahren. Die Entwicklung geschah somit, verglichen mit anderen verbreiteten Desktop-Systemen, unter den Augen eines sehr sicherheitskritischen Publikums. Im Gegensatz zu Desktop-Systemen hängt die Sicherheit bei Serversystemen primär vom Grad der Erfahrung der Administratoren mit dem System selbst ab. Linux punktet dabei durch die freie Verfügbarkeit, die es Administratoren ermöglicht, das System ohne Mehrkosten in verschiedensten Testszenarien zu installieren und dort ausgiebig zu untersuchen. Zudem gibt es eine Reihe von speziell gehärteten Linux-Distributionen, welche besonderen Wert auf Sicherheitsaspekte legen. Initiativen wie SELinux bemühen sich dort um das Erfüllen hoher Sicherheitsstandards.\n\nVorteilhaft ist, dass Linux nicht auf eine Hardware-Architektur festgelegt ist. Würmer und Viren können sich immer nur auf dem Teil der Linux-Systeme verbreiten, auf deren Hardware sie zugeschnitten sind. Hinzu kommt, dass Linux quelloffene Software ist. Jeder kann also den Quellcode studieren, untersuchen und anpassen. Dies führt unter anderem auch dazu, dass der Quellcode (sei es zum Zwecke der Anpassung, zum Zwecke der Schulung, aus dem Sicherheitsinteresse einer Institution oder eines Unternehmens heraus oder aus privatem Interesse) von mehr Menschen studiert wird, als dies bei proprietären Programmen der Fall sein kann, wodurch Sicherheitslücken schneller auffallen (und dann behoben werden können).\n\nEin wesentliches Merkmal vieler Linux-Distributionen ist es, dass sie kostenlos und automatisiert Sicherheitsaktualisierungen für alle bereitgestellte Software anbieten. Diese Funktion existiert zwar auch bei anderen gängigen Betriebssystemen, erfasst dort aber nicht alle bereitgestellte Software, funktioniert nicht durchgehend automatisch oder ist nicht kostenlos, weshalb die Hürde, solche Aktualisierungen einzuspielen, bei anderen Betriebssystemen höher ist als bei Linux.\n\nUnter anderem wegen der allgemein verfügbaren Sicherheitsaktualisierungen sind Antivirenprogramme für Linux wenig verbreitet. Anstatt mit einem Antivirenprogramm nach Schadsoftware suchen zu lassen, die bekannte Sicherheitslücken in der installierten Anwendungssoftware ausnutzt, können die bekannten Lücken bereits über Sicherheitsaktualisierungen geschlossen werden. Die existierenden Antivirenprogramme für Linux werden daher hauptsächlich dafür eingesetzt, um Datei- und E-Mail-Server auf Viren für andere Betriebssysteme zu untersuchen.\n\nLinux verfügt über viele der Fähigkeiten, welche für eine sicherheitstechnisch anspruchsvolle Umgebung erforderlich sind. Dazu gehört sowohl eine einfache Nutzer- und Gruppenrechteverwaltung mittels Role Based Access Control, wie auch eine komplexere Rechteverwaltung mit Hilfe von Access Control Lists. Zusätzlich implementieren viele aktuelle Distributionen auch Mandatory-Access-Control-Konzepte mit Hilfe der SELinux/AppArmor-Technik.\n\nEbenso bietet fast jede Linux-Distribution auch eine Secure-Shell-Implementierung (zumeist OpenSSH) an, mit der authentifizierte verschlüsselte und deswegen sichere Verbindungen zwischen Computern gewährleistet werden können. Andere Verschlüsselungstechniken wie Transport Layer Security werden ebenfalls voll unterstützt.\n\nIm Rahmen der Verschlüsselung für auf Medien gespeicherte Daten steht das Kryptographie-Werkzeug dm-crypt zur Verfügung, das eine Festplattenverschlüsselung ermöglicht. Es bietet dabei die Möglichkeit der Verschlüsselung nach aktuellen Standards wie dem Advanced Encryption Standard. Transparente Verschlüsselung, bei der nur einzelne Dateien statt ganzer Festplatten verschlüsselt werden, stellen die Verschlüsselungserweiterung EncFS und das Dateisystem ReiserFS zur Verfügung. Zu den Sicherheitszertifikaten, die im Zusammenhang mit Linux erworben wurden, siehe den Abschnitt Software-Zertifikate.\n\nUm den Grad der Kenntnisse von Technikern und Administratoren messbar zu machen, wurden eine Reihe von Linux-Zertifikaten ins Leben gerufen. Das Linux Professional Institute (LPI) bietet dafür eine weltweit anerkannte Linux-Zertifizierung in drei Levels, die ersten beiden Level (LPIC-1 und LPIC-2) mit jeweils zwei Prüfungen und den dritten Level (LPIC-3) mit einer Core-Prüfung (301) und mehreren optionalen Erweiterungsprüfungen. Auch die großen Linux-Distributoren wie Red Hat, openSUSE und Ubuntu bieten eigene Schulungszertifikate an, die aber zum Teil auf die Distributionen und deren Eigenheiten ausgelegt sind.\n\nUm den Grad der Sicherheit von Technikprodukten zu bewerten, gibt es ebenfalls eine Reihe von Zertifikaten, von denen wiederum viele für bestimmte Linux-Distributionen vergeben wurden. So hat z. B. das Suse Linux Enterprise Server 9 des Linux-Distributors Novell die Sicherheitszertifikation EAL4+ nach den Common Criteria for Information Technology Security Evaluation erhalten, Red Hat hat für seine Redhat Enterprise Linux 4 Distribution ebenso die EAL4+-Zertifizierung erhalten. Ein Problem bei der Zertifizierung stellen für viele Distributoren allerdings die hohen Kosten dar. So kostet eine Zertifizierung nach EAL2 etwa 400.000 US-Dollar.\n\nEine häufige Schwierigkeit beim Einsatz von Linux besteht darin, dass oft keine ausreichende Hardware-Unterstützung gegeben ist. Tatsächlich verfügt Linux zahlenmäßig über mehr mitgelieferte Treiber als vergleichbare Systeme (Windows, macOS). Das führt dazu, dass in der Regel nicht einmal eine Treiber-Installation notwendig ist und dass sogar ein Wechsel von Hardware reibungslos möglich ist. Das bietet dem Anwender deutlich mehr Komfort als bei vergleichbaren Betriebssystemen, da so z. B. ein problemloser Umzug des Betriebssystems auf einen anderen Rechner oder sogar die Installation des Betriebssystems auf Wechseldatenträgern möglich ist, ohne dass hierfür spezielle Anpassungen am System nötig wären.\n\nOft ist diese reibungslose Hardware-Unterstützung jedoch nicht gegeben. Das gilt insbesondere für aktuellere Hardware. Die Ursache liegt darin begründet, dass nur wenige Hardwarehersteller selbst Linux-Treiber für ihre Hardware zur Verfügung stellen oder diese nur in schlechter Qualität vorliegen. Während für Hardware mit offen dokumentierter, standardisierter Schnittstelle (z. B. Mäuse, Tastaturen, Festplatten und USB-Host-Controller) Treiber zur Verfügung stehen, ist dies für andere Hardwareklassen (z. B. Netzwerkschnittstellen, Soundkarten und Grafikkarten) nicht immer der Fall. Viele Hardwarehersteller setzen auf proprietäre hardwarespezifische Schnittstellen, deren Spezifikation zudem nicht öffentlich zugänglich ist, sodass sie mittels Black-Box-Analyse bzw. Reverse Engineering erschlossen werden muss. Beispiele hierfür sind Intels HD Audio-Schnittstelle und deren Linux-Implementierung \"snd-hda-intel\" oder der freie 3D-Grafiktreiber nouveau für bestimmte 3D-Grafikchips von Nvidia. Ein anderes Beispiel ist der Energieverwaltungsstandard ACPI, der sehr komplex und auf die jeweilige Hauptplatine zugeschnitten ist, sodass eine Implementierung durch die Linux-Gemeinschaft aus Mangel an Ressourcen oder Hintergrundwissen oft unzureichend ist. Oft kann in diesem Zusammenhang auch das Mitwirken der Anwender hilfreich sein, indem sie auf Probleme hinweisen und idealerweise sogar technische Informationen zu ihrer Hardware ermitteln und der Linux-Gemeinschaft zur Verfügung stellen oder Entwicklerversionen vor der Veröffentlichung testen.\n\nEin oft genannter Grund für die Nichtbereitstellung von Linuxtreibern ist das Entwicklungsmodell des Linux-Kernels: Da er keine feste Treiber-API besitzt, müssen Treiber immer wieder an Veränderungen in den einzelnen Kernel-Versionen angepasst werden. Direkt in den Kernel integrierte Treiber werden zwar von den Kernel-Entwicklern meist mit gepflegt, müssen aber unter der GNU General Public License (GPL) veröffentlicht sein, was einige Hardware-Hersteller ablehnen. Extern zur Verfügung gestellte Treiber müssen aber ebenfalls ständig angepasst und in neuen Versionen veröffentlicht werden, was einen enormen Entwicklungsaufwand mit sich bringt. Außerdem ist die rechtliche Lage solcher externen Module, die nicht unter der GPL stehen, umstritten, weil sie in kompilierter Form technisch bedingt GPL-lizenzierte Bestandteile des Kernels enthalten müssen.\n\nDas Problem der Hardwareunterstützung durch sogenannte Binärtreiber (Gewähren von Binärdateien ohne Offenlegung des Quellcodes) wird im Linux-Umfeld kontrovers diskutiert: Während manche für einen Ausschluss proprietärer Kernel-Module plädieren, befürworten andere, dass einige Hersteller überhaupt – zur Not auch proprietäre – Treiber bereitstellen, mit dem Argument, dass die Linux-Nutzer ohne sie benachteiligt wären, weil sie sonst von bestimmter Hardware schlicht abgeschnitten wären.\n\nAllerdings können Treiber für viele Geräteklassen (z. B. alle per USB oder Netzwerk angeschlossenen Geräte) auch ganz ohne Kernelcode programmiert werden, was sogar die bevorzugte Vorgehensweise ist.\n\nLinus Torvalds betont, dass sich Linux und digitale Rechteverwaltung (DRM) nicht ausschließen. Auch sind freie DRM-Verfahren zur Nutzung unter Linux verfügbar.\n\nIn der Praxis ist die Nutzung von DRM-geschützten Medien unter Linux jedoch seltener möglich als unter anderen Systemen, denn aufgrund des Prinzips digitaler Rechteverwaltung können Rechteinhaber alleine entscheiden, auf welchen DRM-Systemen ihre Medien verwendet werden dürfen. Die dabei eingesetzten Verfahren sind nicht standardisiert, sondern werden von den jeweiligen Herstellern kontrolliert, und die beiden größten Hersteller digitaler Rechteverwaltungssysteme im Endverbraucherumfeld, Microsoft und Apple, haben mit Stand Oktober 2009 keine entsprechenden Programme für Linux veröffentlicht oder auch nur entsprechende Absichten bekannt gegeben.\n\nAllerdings gibt es Windows-DRM-zertifizierte Software, die unter Linux eingesetzt werden kann, wie sie beispielsweise bei der AVM FRITZ!Media 8020 verwendet wird.\n\nGrundsätzlich besteht bei DRM-Verfahren die Notwendigkeit, dass die Daten, an denen der Nutzer nur eingeschränkte Rechte erhalten soll, dem Nutzer zu keiner Zeit in unverschlüsselter Form zur Verfügung gestellt werden dürfen, da er ja sonst in diesem Moment eine unverschlüsselte Kopie anfertigen könnte. Da Linux quelloffen ist, ist es dem Nutzer leicht möglich, den entsprechenden Programmteil eines lokalen, rein softwarebasierten DRM-Systems durch eigenen Code zu ersetzen, der genau dies tut.\n\nBis 2014 war der LinuxTag die größte jährlich stattfindende Messe zu den Themen Linux und freie Software in Europa. Neben den Ausstellungen aller namhaften Unternehmen und Projekte aus dem Linux-Umfeld wurde den Besuchern auch ein Vortragsprogramm zu verschiedenen Themen geboten. Der LinuxTag selbst existierte von 1996 bis 2014 und zog zuletzt jährlich mehr als 10.000 Besucher an. Neben dem großen LinuxTag gibt es noch eine Vielzahl kleinerer und regionaler Linuxtage, die oft mit Unterstützung von Universitäten organisiert werden. Seit 2015 sind die Chemnitzer Linux-Tage die größte Veranstaltung dieser Art in Deutschland.\n\nZu den weiteren internationalen Messen gehört der \"Linux Kongress – Linux System Technology Conference\" in Hamburg. Ein Kuriosum ist die jährlich stattfindende \"LinuxBierWanderung\", die Linux-Enthusiasten der ganzen Welt eine Möglichkeit zum gemeinsamen „Feiern, Wandern und Biertrinken“ geben will.\n\nNeben den allgemeinen Messen und Kongressen findet jedes Jahr das LUG-Camp statt. Dieses wird seit dem Jahr 2000 von Linux-Benutzern aus dem Raum Flensburg bis hin zur Schweiz organisiert und besucht.\n\nMit der zunehmenden Verbreitung von Linux hat sich auch ein Angebot an Printmedien entwickelt, die sich mit der Thematik beschäftigen. Neben einer Vielzahl an Büchern zu nahezu allen Aspekten von Linux haben sich auch regelmäßig erscheinende Zeitschriften auf dem Markt etabliert. Bekannteste Vertreter sind hier die einzelnen Hefte der Computec Media, die monatlich \"(Linux-Magazin, LinuxUser)\" oder vierteljährlich \"(EasyLinux)\" erscheinen. Schon seit einer ganzen Weile produzieren auch andere große Verlage wie IDG mit der zweimonatlich erscheinenden \"LinuxWelt\" sowie Heise mit der in unregelmäßiger Abfolge erscheinenden \"c't Linux\" Heftreihen beziehungsweise Sonderhefte zu langjährig bestehenden Computerzeitschriften, nämlich \"PCWelt\" und \"c’t\". Darüber hinaus gibt es auch noch für die Distribution \"Ubuntu Linux\" und ihre Derivate das jährlich viermal erscheinende Magazin \"UbuntuUser\", das durch den Medienanbieter Computec Media veröffentlicht wird.\n\nDer am 12. Oktober 1994 entdeckte Asteroid (9885) Linux wurde nach dem Linux-Kernel benannt.\n\nDie Thematik rund um Linux wurde auch in einer Reihe von Dokumentationen behandelt. So behandelt der Kino-Dokumentationsfilm \"Revolution OS\" die Geschichte von Linux, freier Software und Open Source und stützt sich dabei größtenteils auf diverse Interviews mit bekannten Vertretern der Szene. Die TV-Dokumentation \"\", in Deutschland von Arte ausgestrahlt, geht ähnliche Wege, stellt aber auch einen chronologischen Verlauf der Entwicklung von Linux und Unix dar.\n\n\n\n"}
{"id": "3249", "url": "https://de.wikipedia.org/wiki?curid=3249", "title": "Mac OS", "text": "Mac OS\n\n„Mac OS“ [] ist ein Markenname, den Apple seit Januar 1997 für Betriebssysteme der hauseigenen Macintosh-Rechner nutzt. Ab 1998 wurde der „Macintosh“ auch offiziell mit „Mac“ abgekürzt. Der Begriff ist abgeleitet von \"Macintosh Operating System\" – eine Bezeichnung, die so niemals verwendet wurde.\n\nDer Begriff bezeichnet einerseits das klassische Mac OS, das kontinuierlich seit der System Software von 1984 weiterentwickelt wurde, jedoch ab Anfang der 1990er als technisch überholt galt.\n\nAndererseits findet sich die Bezeichnung auch bei Mac OS X, dem ab 1998 aus Rhapsody weiterentwickelten Unix-Betriebssystem, das ab 2001 das klassische Mac OS ersetzte. 2012 wurde es in OS X umbenannt – bereits 2011 wurde Mac OS X Lion (Version 10.7) als „OS X“ ohne „Mac“ beworben. Seit 2016 ist macOS der Name dieser Desktop-Betriebssystemlinie (macOS Sierra, Version 10.12), ohne Leerzeichen und mit klein geschriebenem „m“ – entsprechend dem Namensschema der übrigen, abgeleiteten Systeme iOS, tvOS und watchOS. Die Server-Version basiert jeweils auf der Desktop-Version und folgt deren Bezeichnung.\n\nAb 1996 wurde mit Mac OS 7.6 das ursprünglich 1984 erschienene Macintosh-Betriebssystem erstmals unter diesem Namen veröffentlicht. Das 1984 mit dem Macintosh 128k erstmals veröffentlichte „System 1.0“ hatte das um einige moderne Funktionen reichere Betriebssystem Lisa OS als Vorbild, das für den vorangegangenen aber wirtschaftlich erfolglosen Rechner Apple Lisa entwickelt worden war. Die Mac-OS-Linie wurde in den Jahren 1984–2002 auf Macintosh- und Mac-Computern und -Laptops ausgeliefert, als gratis Upgrade vertrieben sowie teilweise einzeln verkauft. 1984 hieß es einfach System und wurde ab 1987 in seiner Gesamtheit auch als Macintosh System Software bezeichnet.\n\nZu Beginn der 1990er galt es nach System 6 mit dem nur teilweise modernisierbaren System 7 mehr und mehr als technisch veraltet und Apple versuchte ein von Grund auf neues aber kompatibles Betriebssystem zu schreiben. Als dieses jedoch auch 1996 noch nicht fertig war, wurde das klassische „System“ in „Mac OS“ umbenannt und mit Technologien des unvollendeten Copland-Projekts verbessert, während gleichzeitig am zugekauften Betriebssystem OPENSTEP gearbeitet wurde. Seit 1999 wird es auch als Mac OS Classic – „klassisches Mac OS“ – bezeichnet, um es vom eigenständig entwickelten Nachfolger Mac OS X klar zu unterscheiden.\n\nEnde 1996 beschloss Apple den Kauf der von Steve Jobs gegründeten Firma NeXT mitsamt modernem Unix-Betriebssystem OPENSTEP, der im Januar 1997 über die Bühne ging. Bei Apple wurde das neue Betriebssystem bis 1998 unter dem Namen Rhapsody weiterentwickelt und sollte einerseits das klassische Mac OS komplett ersetzen als auch auf Computern laufen, die nicht von Apple selbst stammten, etwa IBM-kompatiblen PCs. Rhapsody nutzte die von OPENSTEP bekannte neue Programmierschnittstelle „Yellow Box“ (umbenannt von OpenStep) und bot moderne Betriebssystemfunktionen wie Speicherschutz und präemptives Multitasking. Klassische Macintosh-Programme wurden in der neu entwickelten Blue Box ausgeführt, konnten dadurch jedoch nicht von den modernen Funktionen Gebrauch machen.\n\nAuf der WWDC 1998 wurde die Verschmelzung vom klassischen Mac OS (damals Mac OS 8.1) und Rhapsody zu Mac OS X (gesprochen „Mac OS ten“, also Mac OS Zehn) bekannt gegeben. Die Strategie sah eine exklusive Verfügbarkeit für Apple-Computer vor und beinhaltete die Weiterentwicklung von Mac OS 8 (ab 1999 auch Mac OS 9) sowie die Integration der Macintosh-Programmierschnittstelle, soweit möglich, in das moderne Betriebssystem Mac OS X. Diese wurde Carbon genannt und bildete mit 6.000 der rund 8.000 Funktionen einen Großteil der bestehenden Schnittstelle ab. Dies erlaubte es für die Macintosh-Plattform unverzichtbaren Drittherstellern von Anwendungsprogrammen ihre Produkte sowohl für das klassische Mac OS als auch für das kommende Mac OS X anzubieten, da der Portierungsaufwand bestehender Programmteile auf Carbon weit geringer war als jener auf die neue Programmierschnittstelle „Yellow Box“, die unter Mac OS X in Cocoa umbenannt wurde. Carbon hingegen wurde in Form der CarbonLib auch auf Mac OS ab Version 8.1 „rückportiert“ – das erlaubte mit „carbonisierten Anwendungen“ () die native Ausführung von ein und demselben Programm sowohl unter Mac OS Classic als auch unter Mac OS X.\n\nMac OS X hätte nach den ursprünglichen Plänen bereits 1999 erscheinen sollen, erschien jedoch erst 2001. Mac OS 9, das letzte klassische Mac OS, wurde danach nicht mehr weiterentwickelt. Um den Übergang zum neuen Betriebssystem so sanft wie möglich zu gestalten wurde auch die Blue Box, nun umbenannt in Classic-Umgebung, verbessert, sodass klassische Programme direkt auf der mit Mac OS X eingeführten Aqua-Oberfläche als Fenster dargestellt wurden.\n\nMac OS X vereinte somit zu Beginn „das Beste aus beiden Welten“ – mit Carbon war es möglich, dass Anwendungsprogramme wie Adobe Photoshop und Microsoft Office relativ schnell auf Mac OS X verfügbar wurden. Auch Apple selbst übernahm Teile des Finder, iTunes und QuickTime anfangs aus dem klassischen Mac OS. Um externe Entwickler zu überzeugen wurde der Finder sogar mit Ideen aus dem NeXTStep/OPENSTEP Workspace Manager in Carbon erweitert. Dennoch war Cocoa die bevorzugte Programmierschnittstelle unter dem neuen Betriebssystem, worauf Apple auch immer wieder deutlich hinwies. Nach und nach wurden die meisten Programme von Carbon zu Cocoa portiert, schließlich auch der Finder in Mac OS X Snow Leopard (2009) und die Adobe Creative Suite in Version 5 (2010).\n\nAus Mac OS X wurde ab 2005 auch das auf iPod, iPhone und iPad laufende iOS entwickelt. Auch das auf dem Apple TV laufende Betriebssystem tvOS entstammt Mac OS X sowie das seit 2014 auf der Apple Watch laufende watchOS.\n\nNach 2007 und nach der Umstellung von der PowerPC- auf die Intel-x86-Prozessorarchitektur verschwand die offizielle Unterstützung für klassisches Mac OS, da bis auf das (derzeit; Stand: 2017) noch bestehende Carbon in Mac OS X keine Komponente mehr mit klassischem Mac OS direkt in Verbindung steht. Auch wird Carbon ebenfalls seit 2007 nicht mehr weiterentwickelt, wurde nicht auf 64-Bit portiert und Carbon-Anwendungen bleiben folglich auf 32-Bit beschränkt. Zudem fehlt die Programmierschnittstelle Carbon auf den abgeleiteten Varianten iOS, tvOS und watchOS. In diesem Licht betrachtet markiert die Umbenennung von „Mac OS X“ in „OS X“ 2011 und schließlich in „macOS“ 2016 das sichtbare Ende der Übergangsphase. Mit Cocoa wurzelt macOS, iOS, tvOS und watchOS auf der mit NeXTStep entwickelten Programmierschnittstelle OpenStep, und gemeinsam mit neuen Entwicklungen wie Swift ermöglichen diese modernen Schnittstellen und Frameworks das Programmieren auf allen aktuellen Apple-Plattformen.\n\nMac OS ist ein eingetragenes Markenzeichen, das GUI-basierende Betriebssysteme der Apple Inc. (zuvor Apple Computer Inc.) für deren Macintosh-Computersysteme bezeichnet.\n\nSystem 7.5.1 (1995) war die erste Version mit dem bekannten Mac-OS-Logo, das von da an den „Picasso“-Macintosh ersetzte und wie dieser beim Hochfahren angezeigt wird. Das Symbol ersetzt gleichzeitig unter Mac OS X auch jenes des Finder. Der Name „Mac OS“ wurde erstmals mit Version 7.6 (1997) auch für das Betriebssystem selbst verwendet – dieser wurde von da an bei allen klassischen Mac-OS-Versionen unterhalb des Symbols beim Startbildschirm hingeschrieben. Unter Mac OS X 10.0 (2001) wurde beim Hochfahren hingegen wieder das altbekannte Happy-Mac-Symbol verwendet. Ab Mac OS X 10.2 (2002) wird ein graues Apple-Logo gezeigt.\n\nBetriebssysteme, die den Namen „Mac OS“ tragen:\n\nMit Mac OS X Lion (Version 10.7 von Mac OS X) wurde das Betriebssystem bereits als „OS X“ beworben und die Server-Version heißt bereits OS X Server 1.0 (ohne „Mac“ im Namen). Ab OS X Mountain Lion (Version 10.8) heißt das Betriebssystem offiziell „OS X“ und gibt dies bei einem Klick auf „Über diesen Mac“ an. Als Grund für das Entfernen von „Mac“ im Namen gelten die Varianten des Betriebssystems für Apple TV, iPod, iPhone und iPad neben der ursprünglichen Variante für Macintosh-Computer. Da das Betriebssystem nicht mehr nur auf Macs läuft, sollte es auch nicht mehr „Mac OS“ genannt werden. Beim Apple TV der 1. Generation wurde eine leicht modifizierte Version von Mac OS X (Tiger, Version 10.4) verwendet, bei iPod, iPhone und iPad sowie ab Apple TV der 2. Generation heißt das Betriebssystem iOS (ursprünglich iPhoneOS und iPadOS) bzw. tvOS (basiert auf iOS) und hat mit Darwin dieselbe Basis und denselben Kernel wie Mac OS X. 2016 schließlich wurde die Bezeichnung analog zu den anderen Betriebssystemen in macOS umbenannt, sodass das ursprüngliche Mac OS X nun macOS heißt, also wieder mit „mac“ im Namen, jedoch anders geschrieben, und erstmals ohne das X, das offiziell für die Version 10 als römische Zahl stand und damit das moderne aus Rhapsody entwickelte Betriebssystem vom klassischen Mac OS eindeutig unterschied.\n\nApple hatte im Laufe der Zeit einige Betriebssysteme für die hauseigenen Apple-Computer angeboten. Der erste Apple-Computer, der Apple I, hatte sein Betriebssystem noch fest in der Firmware verankert. Mit dem Apple II kamen ab 1977 vorwiegend textbasierte Betriebssysteme wie Apple DOS, sowie bei dem kommerziell nicht erfolgreichen Apple III von 1980 dessen Weiterentwicklung Apple SOS zum Einsatz. Davon inspiriert erschien für den Apple II ProDOS, aber auch Apple CP/M lief auf dem Apple II. Mit der Apple Lisa wagte Apple den ersten kommerziellen Versuch eines Computerherstellers, einen Personal Computer (PC) mit rein grafischem Betriebssystem zu vermarkten – die von Xerox bereits in den 1970er-Jahren angebotenen Systeme mit grafischer Bedienung waren keine PCs, sondern teure Business-Systeme. Die Apple Lisa war jedoch viel zu teuer für Privatpersonen und ein kommerzieller Flop, dennoch war das Betriebssystem Lisa OS die Vorlage für das des 1984 erscheinenden Apple Macintosh, der den Apple II ablösen sollte. Es dauerte jedoch einige Jahre, bis der erfolgreiche Apple IIgs mit dem ab 1987 verfügbaren ebenfalls grafischen GS/OS vom Macintosh als erfolgreicheres Apple-Computermodell abgelöst werden konnte.\n\nAuch Unix-Betriebssysteme gab es für Apple-Computer. Den Anfang machte Microsoft mit Xenix für Apple Lisa. 1988 erschien von Apple selbst A/UX, ein Unix System V, das auf der Oberfläche der System Software des Macintosh lief (System 6 und System 7) und somit kein eigenständiges Betriebssystem darstellt.\n\nJahrelang hat Apple die Existenz des Betriebssystems absichtlich heruntergespielt, um die Macintosh-Systeme benutzerfreundlicher erscheinen zu lassen und um sich von anderen Personal Computern und deren Betriebssystemen abzugrenzen. Ein wesentlicher Teil dieser frühen Systemsoftware, namentlich der Macintosh-Baukasten (), wurde direkt im ROM gespeichert. Diese Vorgehensweise hatte in Zeiten von teurem RAM den Vorteil, dass nicht alle Komponenten in den aktiven Arbeitsspeicher geladen werden mussten und dennoch sofort zur Verfügung standen, da die Programme im ROM jederzeit und schneller als von den damals üblichen Disketten und Festplatten abrufbar waren. Auch war so die begrenzte Speicherkapazität von Disketten () nicht durch Teile des Betriebssystems belegt und konnte rein für Programme und Daten genutzt werden. Die ersten Macintosh-Computer wurden ohne Festplatte ausgeliefert.\n\nZum Starten war dennoch eine Systemdiskette erforderlich. (Nur der Macintosh Classic von 1991 war jemals in der Lage, gänzlich aus dem ROM zu starten.) Die Architektur der „System Software“ erlaubte ein komplett grafisches Betriebssystem ohne einen Kommandozeilen-Modus. Abstürzende Programme und sogar Hardwarefehler wie das Fehlen von Laufwerken wurden dem Benutzer grafisch über Kombinationen von Symbolen (), Hinweisfenstern, Knöpfen, dem Mauszeiger und der markanten Bitmap-Schrift Chicago kommuniziert.\n\nEine wohl nicht unbeabsichtigte Nebenerscheinung war, dass das Betriebssystem fest mit der Hardware verschmolzen war. Die „System Software“ hing von diesem Kern-System im Mainboard-ROM ab. Dieser Umstand half später sicherzustellen, dass es nur von Apple-Computern oder lizenzierten Klonen (mit den kopierrechtlich geschützten ROMs von Apple) ausgeführt werden konnte. Aktualisierungen für das Betriebssystem wurden von Apple-Händlern bis System 6 auf Disketten zum Selbstkostenpreis verteilt.\n\nDas frühe Macintosh-Betriebssystem bestand aus zwei Teilen: dem „System“ und dem „Finder“, jedes mit eigener Versionsnummer und in Assembler sowie in Pascal programmiert.\n\nDas klassische „System“ fällt vor allem durch das Nichtvorhandensein einer Kommandozeile auf – die Anwenderschnittstelle ist also vollständig grafisch. Berühmt für seine gute Benutzbarkeit und sein kooperatives Multitasking wurde es jedoch für das Fehlen der Unterstützung von Speicherschutz (Arbeitsspeicher) und seine Anfälligkeit für Konflikte zwischen Betriebssystemerweiterungen (zum Beispiel Gerätetreiber) kritisiert. So funktionierten manche Erweiterungen nicht zusammen oder nur, wenn sie in bestimmter Reihenfolge geladen wurden.\n\nBeim Macintosh wurde ursprünglich das Macintosh File System (MFS) verwendet, welches keine Unterordner erlaubte und daher auch als flaches Dateisystem bezeichnet wird. Es wurde 1984 eingeführt und bereits 1985 durch das Hierarchical File System (HFS), einem (wie der Name sagt) hierarchischen Dateisystem mit echtem Verzeichnisbaum, ersetzt. Beide Dateisysteme sind zwar kompatibel, allerdings brachten die neuen HFS-Funktionen Probleme beim Datenaustausch mit anderen Nicht-Macintosh-Dateisystemen.\n\nMit steigender Speicherkapazität und Leistungsfähigkeit war es irgendwann nicht mehr möglich, ein modernes grafisches Betriebssystem in großen Teilen im ROM vorzuhalten bzw. auch zu aktualisieren. Ab den ersten PowerPC-G3-Systemen war daher fast das gesamte Betriebssystem auf der Festplatte abgespeichert. Auf dem physischen ROM des Mainboard verblieb nur noch ein aus lizenzrechtlichen Gründen notwendiger Teil, ohne den Mac OS weiterhin nicht funktionierte. Der Inhalt des ROM wird von System 7 und Mac OS u. a. dazu genutzt, herauszufinden, auf welchem Macintosh-Modell es läuft. Während Aktualisierungen bis System 6 kostenlos waren, wurden Versionen ab System 7 teilweise als separate, kostenpflichtige Produkte vertrieben. Diese neuen Macintosh-Computer wurden von Apple als (übersetzt: „neue Welt“) bezeichnet, während die ältere Generation, bei der noch wesentliche Teile des Systems im ROM abgespeichert waren, als (übersetzt: „alte Welt“) bezeichnet wurden. Zudem wurde kurz darauf auf Open Firmware umgestellt und mit dem iMac (1998) erstmals die zuvor inoffiziell verwendete Bezeichnung „Mac“ auch als offizieller Produktname eingeführt.\n\nDas bis dahin einfach als „System“ bezeichnete Betriebssystem der Macintosh-Computer, wie es seit 1984 ständig weiterentwickelt wurde, wies einige konzeptionelle Mängel auf, die man nur durch eine Neuentwicklung beheben konnte. Das System unterstützte weder präemptives Multitasking, Mehrbenutzerbetrieb, Speicherschutz noch dynamische Speicherverwaltung und galt deshalb als instabil und technisch veraltet. Apple hatte daher im Laufe der Zeit verschiedene Projekte gestartet, um ein neues Betriebssystem, das diese Schwächen und Mängel beheben sollte, zu entwickeln.\n\n1988 wurde das Projekt „Pink“ in Angriff genommen. Pink hatte ein vollständig objektorientiertes Betriebssystem zum Ziel. Mitte 1991 gelang es Apple, die Firma IBM von seiner damaligen Entwicklung zu überzeugen, weshalb die gemeinsame Tochterfirma Taligent gegründet wurde, um Pink als TalOS zu vollenden. Bald merkte man aber, dass der Markt abseits von Apple kein neues Betriebssystem brauchte, weshalb Pink bzw. TalOS in die Laufzeitumgebung TalAE, Taligent Application Environment, später auch \"CommonPoint\" genannt, umgewandelt wurde. 1995 stieg Apple komplett aus der Entwicklung aus und übergab Taligent (TalAE) vollständig an IBM. Somit blieb das Problem eines veralteten Betriebssystems bei Macintosh-Computern weiterhin bestehen.\n\nBereits 1985, nachdem Steve Jobs Apple nach einem Streit mit John Sculley verlassen hatte, gab es die Idee für ein Projekt, welches das Apple-Betriebssystem auf x86-Hardware portieren sollte. Doch erst 1992 wurde das Projekt, das den Codenamen „Star Trek“ trug, wieder aufgenommen, nachdem Novell an Apple herangetreten war, um ein konkurrenzfähiges Betriebssystem zum damals erstmals kommerziell erfolgreichen grafischen PC-Betriebssystem Windows 3.0 von Microsoft zu entwickeln.\n\nNovell war damals mit Netware der führende Entwickler von Netzwerksystemen und -software und sah sich durch den Erfolg von Windows gefährdet. Bereits 1991 hatte Novell das von Digital Research entwickelte Betriebssystem DR DOS samt der grafische Benutzeroberfläche GEM übernommen und wollte es zu einem modernen graphischen Betriebssystem, mit GEM als Alternative zu Windows und einem mit DR DOS (später Novell DOS) mitgelieferten Netware Client, weiterentwickeln. Doch unter Digital Research war GEM schon einmal der Grund für eine Klage durch Apple gewesen, da es dem Macintosh-Betriebssystem zu ähnlich sah. Um eine Wiederholung einer solchen Klage zu vermeiden, kontaktierte man Apple, um stattdessen deren Betriebssystem auf die Intel-x86-Architektur zu portierten. John Sculley, 1991 CEO von Apple, stimmte zu. Auch Intel war daran interessiert und steuerte 486-PCs für das Entwickler-Team bei.\n\nAb Sommer 1992 wurde von nur 18 Entwicklern System 7 auf x86 portiert und bereits Ende Oktober stand ein voll funktionsfähiger Prototyp zur Verfügung. Bei Apple war man erstaunt das eigene Betriebssystem auf einem IBM-kompatiblen PC laufen zu sehen. Da große Teile davon in Assembler (für den in Macintosh-Rechnern verwendeten Motorola 68000) geschrieben waren, musste fast das gesamte Betriebssystem neu programmiert werden. Nur der Finder und viele Teile, die in Pascal geschrieben waren, konnten mit nur wenigen Änderungen weiterverwendet werden. Das Resultat war auf x86 Quelltext-kompatibel: Programme, die für den Macintosh (und dessen Betriebssystem „System“ auf der m68k-Architektur) geschrieben worden waren, hätten auf x86 neu kompiliert – hardwarenahe Programmierung jedoch komplett neu geschrieben werden müssen.\n\nBei Apple ergab sich nun einerseits die Situation, dass man auch Hardware verkaufen wollte und gerade mit IBM und Motorola eine Allianz zur PowerPC-Entwicklung gebildet hatte. Da wäre ein x86-Betriebssystem Konkurrenz aus dem eigenen Hause gewesen. Anderseits waren die PC-Hersteller nicht gewillt, einen guten Preis für das Betriebssystem zu zahlen, wenn es auf einem neuen PC vorinstalliert verkauft worden wäre. Microsoft hatte mit den Herstellern von PCs einen Vertrag abgeschlossen, bei dem mit jedem verkauften PC ein gewisser Betrag an Microsoft abzuführen war – egal, welches Betriebssystem nun vorinstalliert war. Aber auch die Software-Entwickler waren vorsichtig, da sie nicht wussten, wie viel Portierungsaufwand es bedeutet hätte, ihre Programme fit für den x86-Befehlssatz zu machen. In vielen Programmen waren Funktionen zur Beschleunigung hardwarenah in 68k-Assembler geschrieben.\n\nZudem gab es 1993 an der Spitze von Apple einen Wechsel und Michael Spindler wurde neuer CEO. Unter ihm wurde ein Sparprogramm realisiert und, obwohl nach dem lauffähigen Prototyp von „Star Trek“ das Team auf 50 Programmierer aufgestockt wurde, das gesamte Projekt schließlich im Juni 1993 eingestellt.\n\nIm März 1988 trafen sich Apple-Manager und -Entwickler, um die Nachfolge von System 6 zu besprechen. Dabei wurden alle Ideen für die nächsten Betriebssysteme auf Karteikarten geschrieben: auf blauen Karteikarten waren Ideen, die mit dem bestehenden Betriebssystem als Weiterentwicklung realisiert werden konnten. Daraus wurde im „Project Blue“ 1991 System 7. Auf rosa Karteikarten wurden Ideen für ein neues Betriebssystem der nächsten Generation geschrieben, die neu geschrieben werden mussten. Daraus wurde im „Project Pink“ später TalAE, das aber keinen stabilen Kernel beinhaltete. Auf roten Karteikarten wurden alle Ideen geschrieben, die frühestens für die übernächste Generation als realistisch galten. Diese Ideen wurden nach dem Ende von „Star Trek“ im Projekt „Raptor“ – dem eigentlichen „Project Red“ – wieder aufgegriffen.\n\n„Raptor“ wurde jedoch bereits nach einem Jahr gestoppt, da Apple sowohl die finanziellen als auch die personellen Ressourcen fehlten.\n\nCopland war der Projektname für ein von Grund auf neugeschriebenes Betriebssystem der Firma Apple, das die Nachfolge von System 7 antreten sollte; stattdessen wurde Letzteres zu Mac OS 8 weiterentwickelt. Namensgeber für das Projekt war der zeitgenössische Komponist Aaron Copland. Apple plante, im Jahr 1994 die ersten PowerPC basierten Macintosh-Computer mit dem Betriebssystem Copland auszurüsten. Das Ziel war ein System, das auf einem Microkernel (von Apple gerne als Nanokernel bezeichnet) aufbaut und endlich präemptives Multitasking und Speicherschutz beherrscht. Das ganze System sollte mehrbenutzerfähig sein und anders als System 7 nativ auf dem PowerPC-Prozessor arbeiten.\n\nCopland wurde im März 1994 gestartet. Zunächst war ein Veröffentlichungsdatum Ende 1995 geplant, das später auf Mitte 1996 und Ende 1997 verschoben wurde. Im November 1995 wurde eine Betaversion für Softwareentwickler veröffentlicht. Die Copland-Entwicklung, an der etwa 500 Entwickler arbeiteten und die insgesamt über 250 Mio. Dollar kostete, war zu diesem Zeitpunkt aber noch nicht fertig und hoffnungslos im Verzug. Apple musste die Macintosh-Benutzer und die Entwickler mit dem gealterten System 7 vertrösten. Gleichzeitig feierte Microsoft mit Windows 95 große Erfolge, und die ersten ernst zu nehmenden Linux-Distributionen tauchten auf dem Markt auf. Inspiriert von Microsofts Verschmelzung von MS-DOS und Windows 3.11 zu Windows 95, die nicht einmal ein Jahr Entwicklungszeit in Anspruch genommen hatte, beschloss man bei Apple im Jahr 1997, Copland zu stoppen und so viele Funktionen davon wie möglich in ein überarbeitetes System 7 zu integrieren. Die ersten Neuerungen aus dem Copland-Projekt flossen schon in Version 7.6 von Mac OS ein. In Mac OS 8.0 wurden zahlreiche weitere integriert.\n\nDas Research Institute der Open Software Foundation (OSF) erstellte einen Mach-3.0-Microkernel auf Basis von Linux für die Intel-486-Architektur. Weil Apple immer noch ein modernes Betriebssystem fehlte, sponserte der Konzern die Entwicklung und portierte MkLinux auch auf die Power-Macintosh-Plattform. Im Mai 1996 veröffentlichte Apple „MkLinux Developer Release 1“ für den Power Macintosh. Erst 1999 zog sich Apple aus der MkLinux-Entwicklung wieder zurück.\n\nBei der Entwicklung von Rhapsody und daraus wiederum schließlich Mac OS X (siehe Abschnitt „Entwicklung von Mac OS X“) wurde der vorhandene Mach-2.5-Kernel von OPENSTEP ebenfalls auf Mach 3 aktualisiert – und dabei mit Teilen des Mach 3 aus MkLinux verbessert. So war MkLinux eine nicht unwesentliche Vorarbeit für die Modernisierung von OPENSTEP und damit der Entstehung von Mac OS X.\n\nNachdem Copland eingestellt worden war, wurde bei Apple das Betriebssystemprojekt „Gershwin“ gestartet. Es sollte das, was bei Copland schief gelaufen war, im Rahmen einer Neuentwicklung korrigieren. Allerdings war Gershwin nichts weiter als ein Projektname und eine Idee, denn es kam zu keiner realen Entwicklung.\n\nIn der Retrospektive jedoch kann Copland als die Idee zu Mac OS 8 und Gershwin als die Idee zu Mac OS 9 betrachtet werden, da einige der angedachten Funktionen in die realen Betriebssysteme mit diesen Bezeichnungen Eingang gefunden haben.\n\nIm Dezember 1994 wurde durch den damaligen CEO Michael Spindler ein Markt für autorisierte Macintosh-Klone geöffnet. Der Vertrag sah vor, dass andere Hersteller Macintosh-kompatible Computer verkaufen durften, auf denen System 7 lauffähig war. Die Hoffnung war, dass durch den Verkauf von preiswerteren „“ die Verkaufszahlen insgesamt weiter ansteigen würden. Per Vertrag hatte Apple mit jedem verkauften Klon pauschal 50 US-Dollar mitverdient. Die Rechnung ging jedoch nicht auf, da einerseits 1995 das nunmehr konkurrenzfähige PC-Betriebssystem Windows 95 auf den Markt kam – und Apple mit System 7 nicht mehr die alleinige einfach zu bedienende grafische Benutzeroberfläche anbot – und andererseits die Klone im eigenen Markt für geringere Absatzzahlen sorgten. Wurden 1995 noch 4,5 Millionen Macs verkauft, so waren es 1996 nur noch 4 Millionen und 1997 nur noch 2,8 Millionen.\n\nIm Januar 1997 wurde NeXT von Apple übernommen und Steve Jobs als Interims-CEO eingesetzt. Eine seiner ersten Tätigkeiten war es das Klon-Programm zu beenden. Da Apple die Kontrolle über das Betriebssystem hatte, wurde es ab Version 7.6 in Mac OS 7.6 umbenannt und es wurde versucht, die Klone auf das Niedrigpreissegment unter 1.000 US-Dollar zu beschränken und die Lizenzkosten pro verkauftem PC zu erhöhen. Als die Macintosh-Klon-Hersteller die neuen Bedingungen nicht akzeptierten, wurde das geplante Mac OS 7.7 kurzerhand als Mac OS 8.0 herausgebracht. Das beendete schlagartig den Markt für Macintosh-Klone, da der Vertrag spezifisch „System 7“ lizenzierte (inklusive Mac OS in Version 7, also auch Mac OS 7.6).\n\nDer Grund, warum der Nachfolger von System 7.5.5 in „Mac OS 7.6“ umbenannt wurde, sind die Macintosh-Klone und die sich ab Ende 1996 abzeichnende geänderte Strategie bei Apple: die Benutzer sollten das System mehr als bisher mit Apple in Zusammenhang bringen – auch wenn sie einen Klon eines anderen Herstellers nutzten. Der Vertrag mit den Macintosh-Klon-Herstellern ist auch der Grund warum Mac OS 8.0 so früh auf den Markt kam. Tatsächlich ist Mac OS eine weitere Version von „System“, „System Software“ oder „Macintosh System Software“ – ab Version 7.6 jedoch mit Entwicklungen aus dem nie fertig gestellten Betriebssystem Copland, die nach dessen Ende in das bestehende „System“ integriert wurden.\n\nDa Apple bereits an dem Nachfolgebetriebssystem arbeitete, wurde die Überführung von Copland-Technologie in den Versionen 8.0 bis 9.2.2 konsequent weiter fortgeführt. Diese und die Portierung von Neuentwicklungen (wie CarbonLib und die bessere Integration in die Classic-Umgebung) bereiteten den Weg für Apples nächste Betriebssystem-Inkarnation, die ebenfalls den Namen „Mac OS“ tragen sollte: Mac OS X.\n\nBei Apple waren die Verkaufszahlen bis Ende 1996 weiterhin rückläufig. Branchenkennern war klar, dass Apple wenig Zeit hatte, da System 7 hoffnungslos veraltet war und der PC mit Windows 95 technisch überlegen und kommerziell erfolgreicher war.\n\nNachdem es Apple nicht geschafft hatte, sein veraltetes Betriebssystem durch eine eigene Neuentwicklung zu ersetzen, suchte das Management fieberhaft nach einer modernen Alternative, die man entweder lizenzieren oder zukaufen wollte. So wurde neben BeOS, das bald zum Favoriten werden sollte, auch über Windows NT 4.0 nachgedacht. Sogar über die Wiederbelebung von TalOS (Projekt Pink) wurde nachgedacht. Parallel dazu wurde den Programmierern der Auftrag erteilt, so viele Funktionen des gescheiterten Betriebssystems Copland wie möglich in Mac OS zu überführen.\n\nDie Be Incorporated war eigentlich als Hersteller von Hardware aufgetreten und hatte das Betriebssystem zusammen mit den als BeBox bezeichneten Personalcomputern mit PowerPC-Architektur verkauft. Doch der eigentliche Wert der Firma waren nicht die BeBoxen, sondern das Betriebssystem: BeOS lief bereits auf den Power Macs von Apple und bot all das, was Apple mit Copland hatte erreichen wollen; es war technisch auf dem Stand der Zeit und bereits voll funktionsfähig.\n\nAls Apple mit der Be Inc. über den Kauf von BeOS in Verhandlung trat, war sich Jean-Louis Gassée, damals CEO von Be, sehr sicher, dass Apple – wirtschaftlich mit dem Rücken zur Wand stehend – keine andere Möglichkeit haben werde als BeOS zu kaufen. Er trieb den Preis daher in die Höhe, weil er etwas hatte, was Apple dringend brauchte.\n\nFür Apple war BeOS jedoch nicht vollständig; schon alleine deshalb, weil es keine Kompatibilität zu bestehenden Macintosh-Anwendungen gab. Ein Resultat der Bemühungen, diese Kompatibilität herzustellen, ist SheepShaver: eine Virtualisierungsumgebung bzw. ein PowerPC-Emulator für BeOS, der es ermöglichte, das klassische System ab 7.5.2 auf BeOS zu virtualisieren bzw. auf Nicht-PowerPC-Architekturen zu emulieren und bestehende Anwendungen somit weiter verwenden zu können.\n\nLetztlich konnte man sich nicht über den Preis für BeOS einigen. Apple suchte nach neuen Optionen und fand diese im Apple-Mitgründer Steve Jobs und dessen neuer Firma NeXT. Die Be Inc. ging leer aus und wurde schließlich aufgelöst, ihr geistiges Eigentum ging an die Palm Inc. über, die mittlerweile selbst aufgekauft und aufgelöst wurde.\n\nNeXT, die von Steve Jobs gegründete Computer- und Software-Firma, hatte ein funktionierendes modernes Betriebssystem und verkaufte es anfangs zusammen mit Hardware, später als reines Software-Produkt einzeln, war jedoch auf dem Markt nicht erfolgreich. Apple auf der anderen Seite hatte einen treuen Kundenstock, aber kein modernes Betriebssystem. Nachdem Steve Jobs mit Gil Amelio telefoniert und ihm von BeOS abgeraten haben soll, einigten sich beide Firmen auf eine Übernahme von NeXT durch Apple. Im Dezember 1996 wurde diese angekündigt und Anfang 1997 vollzogen. Steve Jobs selbst wurde zuerst Berater von Gil Amelio, kurz darauf schließlich interimsmäßiger CEO von Apple.\n\nNeben vom neuen CEO eingeläuteten überarbeiteten Computer-Designs wie den iMacs (1998) und den iBooks (1999) wurde sofort an einer Überführung des Betriebssystems von NeXT, dem ab Version 4 1996 in OPENSTEP umbenannten ursprünglichen NeXTStep, in den Mac-OS-Nachfolger gearbeitet.\n\nUnter dem Projektnamen „Rhapsody“ wurde das Betriebssystem OPENSTEP, das bis Version 3.3 von 1995 noch NeXTStep hieß, weiterentwickelt. Mit OPENSTEP erhielt Apple den lange ersehnten stabilen Mach-Kernel und konnte so einige der im Copland-Projekt angestrebten Ziele auf Basis von Rhapsody verwirklichen.\n\nBeim Erscheinen von Mac OS 8 wurde bereits an Rhapsody gearbeitet. Das neue auf BSD basierende Betriebssystem musste zunächst auf die PowerPC-Architektur des Macintosh portiert werden. Ursprünglich sollte es die NeXTStep-typische Optik beibehalten, erhielt dann aber das Aussehen von Mac OS 8 im Platinum-Design.\n\nApple hatte großes vor mit Rhapsody: Bereits im Januar 1997 kündigte Apple das kommende Betriebssystem als den Nachfolger von Mac OS 7.6 an und ging davon aus, dass Anwender von Mac OS direkt auf Rhapsody migrieren würden. Im Mai 1997 wandte sich Apple an Entwickler: Software, die für Rhapsody programmiert werde, lasse sich problemlos für weitere Betriebssysteme kompilieren, wenn das zugrundeliegende OPENSTEP-kompatible Framework „Yellow Box“ installiert sei.\n\nYellow Box war jene Programmierschnittstelle (API), die von OPENSTEP übernommen wurde und aus NeXTstep gemeinsam mit Sun Microsystems als OpenStep weiterentwickelt worden war. Mit GNUstep gab es bereits eine Implementierung der OpenStep-Schnittstelle für Linux, während Sun OpenStep auf Solaris portiert hatte. Da Sun jedoch mit Java eine eigene plattformübergreifende Schnittstelle entwickelt hatte, war das Ende von OpenStep auf Solaris absehbar. Da NeXT selbst OpenStep zudem bereits auf Windows portiert hatte, wurde während der Rhapsody-Entwicklung auch Yellow Box in einer Windows-Version an Entwickler abgegeben, die ihre Programme damit für Rhapsody und für Windows gleichermaßen entwickeln konnten.\n\nRhapsody als Betriebssystem sollte sowohl für die Apple-eigenen PowerPC-basierten Rechner als auch für Intel-PCs erscheinen. Weil native Applikationen erst programmiert werden mussten, beinhaltete Rhapsody einerseits bereits aus OPENSTEP bekannte Programme, andererseits wurde in der PowerPC-Variante eine als Blue Box bezeichnete Virtuelle Maschine mitgeliefert, die für ein virtualisiert laufendes Mac OS 8 für bereits existierende Macintosh-Applikationen gedacht war. Auf der Intel-Version fehlte die Blue Box.\n\n1998 schließlich stand diese offene Multiplattform-Strategie vor dem Ende. Apple musste die realen Tatsachen darüber akzeptieren, dass das angekündigte Betriebssystem in dieser Form am Markt nicht angenommen wurde. Zum einen beinhaltete das Yellow-Box-API das von OpenStep übernommene Display Postscript, das von Adobe lizenziert werden musste. Die Kosten von mehreren Tausend Dollar pro Lizenz standen einer kostengünstigen Multiplattform-API, die Yellow Box sein sollte, entgegen. Zum anderen waren die Hersteller von für den Macintosh geschriebener Software nicht bereit ihre Programme für Rhapsody komplett neu zu programmieren. Von der Macintosh-API auf die Yellow-Box-API zu wechseln kam einem radikalen Systemwechsel gleich, für den große Teile des vorhandenen Quelltextes hätten neu geschrieben werden müssen. Aus dieser Sicht war Rhapsody eigentlich ein weiteres NeXTStep bzw. OPENSTEP, das lediglich das Aussehen und den Desktop mit Mac OS gemein hatte. Und zum Dritten gab es keinen realen Markt für das Rhapsody-Betriebssystem auf PCs, da aus gutem Grund fast alle neu verkauften PCs mit einem Windows-Betriebssystem ausgeliefert wurden. Durch die Verträge von Microsoft waren PC-Verkäufer daran gebunden mit jedem verkauften PC Lizenzkosten an Microsoft zu zahlen – also auch dann, wenn ein anderes Betriebssystem, z. B. Rhapsody, vorinstalliert verkauft worden wäre. Rhapsody wurde zwar als „Rhapsody 1.0“ für die Prozessorarchitekturen PowerPC und Intel fertiggestellt, die Veröffentlichung aber buchstäblich im letzten Moment abgesagt.\n\nApple gab damit dem Druck der Dritthersteller nach, eine für Mac-OS-Anwendungen kompatible Programmierschnittstelle auch für das kommende Betriebssystem zu schaffen. Als Konsequenz wurde der Produktname „Rhapsody“ wie auch die Yellow Box für weitere Plattformen verworfen. Steve Jobs verkündete auf der MacWorld am 8. Juli 1998, dass Rhapsody als Mac OS X Server 1.0 exklusiv für den Power Macintosh herausgegeben werde – und als reines Serverbetriebssystem.\n\nWeil Apple somit 1998 immer noch kein fertiges modernes Desktop-Betriebssystem anbieten konnte, wurden alle bestehenden Entwicklungen und Technologien so gut wie möglich in das bereits vorhandene klassische Mac-OS-Betriebssystem integriert. Zur Überbrückung der Zeit bis zur Fertigstellung des Nachfolgebetriebssystems Mac OS X – „Mac OS Version 10“ – wurde somit weiter am klassischen Mac OS gefeilt, aber auch vorbereitend neue Entwicklungen eingearbeitet. So wurde u. a. für Mac OS ab Version 8.1 die CarbonLib verfügbar gemacht, ab Mac OS 8.5 das „alte“ Betriebssystem mit Teilen des Copland-Kernels verbessert und in Mac OS 9 war der aus OPENSTEP bekannte Schlüsselbund integriert. Sherlock führt die für Copland entwickelte Suchfunktion in Mac OS 8.5 ein, später auch in Mac OS X 10.2. (Unter Mac OS X Tiger ersetzte Spotlight die Suchfunktion mit Sherlock.)\n\nWährenddessen wurde auf Hochtouren an Mac OS X gearbeitet.\n\nDie Entwicklung von Mac OS X ergab sich als Folge des Scheiterns von Rhapsody und begann 1998. Der Name für das neue Betriebssystem verdeutlicht, dass es sich um ein Mac OS handelt.\n\nRhapsody bot mit der Blue Box zwar eine Möglichkeit, existierende Mac-OS-Anwendungen auch auf dem neuen Betriebssystem zu verwenden, doch profitierten diese nicht von den Vorteilen des modernen Kernels (Speicherschutz, präemptives Multitasking), da sie ja in Wirklichkeit auf einem virtualisierten klassischen Mac OS liefen. Da eine Portierung der bestehenden Programme auf die Yellow Box zu viel Aufwand bedeutet hätte, musste Apple notgedrungen die Macintosh-Programmierschnittstelle auf Mac OS X portieren: diese wurde Carbon genannt und beinhaltete all jene Macintosh-API-Aufrufe, die Speicherschutz und preämptivem Multitasking nicht im Weg standen. Durch Carbon mussten existierende Anwendungen nur geringfügig modifiziert werden, um vom modernen Kernel von Rhapsody, der in XNU umbenannt wurde, profitieren zu können. Der Portierungsaufwand existierender Macintosh-Applikationen hielt sich dadurch in Grenzen und ermöglichte zudem auch Apple selbst, bestehenden Programme in Mac OS X zu integrieren. So waren QuickTime und der Finder auch auf Mac OS X Carbon-Programme, während Neuentwicklungen wie iTunes und Safari in dem von Yellow Box in Cocoa umbenannten neuen API geschrieben wurden.\n\nDamit an das Carbon-API angepasste Programme nicht nur auf Mac OS X liefen, wurde Carbon auch auf Mac OS 8.1 in Form der CarbonLib portiert, sodass die neue Schnittstelle (die sich in Details von der Macintosh-API unterschied) auch für klassische Mac-OS-Anwendungen verwendet werden konnte. Anwendungen, die sowohl auf Mac OS X als auch auf klassischem Mac OS mit CarbonLib liefen, wurden „carbonized applications“, was übersetzt in etwa „carbonisierte Anwendungen“ heißt, genannt. CarbonLib setzte jedoch einen PowerPC-Prozessor voraus, wie auch Mac OS selbst ab Version 8.5. Anwendungen für den 68k-Prozessor sind auf das klassische Macintosh-API angewiesen und laufen weiterhin auf Mac OS bis zur letzten Version 9.2.2 in einer transparenten Emulation.\n\nDas X im Namen von Mac OS X zeigte sowohl – als römische Zahl für 10 – die Nachfolge auf Mac OS 9 und die Kompatibilität dazu auf, als auch den neuartigen unixoiden Charakter der neuen Betriebssystem-Generation („Uni\"x\"“). Mit der Developer Preview 3 (Januar 2000) wurde Aqua, einer auf Ideen von OPENSTEP basierenden neu geschriebenen grafischen Benutzeroberfläche, eingeführt. Damit entstand ein unverkennbares, eigenständiges Look and Feel, das Mac OS X bis heute prägt. In diesem Zuge wurde das teure Display Postscript durch das freie Display PDF ersetzt.\n\nDer Kernel von Mac OS X ist XNU, was einerseits auf Unix anspielt („“), andererseits als „Mac OS \"X\" \"Nu\"Kernel“ interpretiert werden kann – denn Apple hatte ja mit dem NuKernel im Projekt Copland versucht, selbst einen Mach-3-basierten modernen Microkernel zu entwickeln. XNU war nun ein stabiler Microkernel, basierte unter OPENSTEP jedoch noch auf Mach 2.5. Während der Rhapsody-Entwicklung wurde für den Kernel das Mach-3-Konzept, jedoch nicht vollständig, übernommen, was XNU zu einem Hybrid-Kernel auf Basis des FreeBSD-Kernels macht. Zudem wurde das Userland von 4.3BSD-Reno unter OPENSTEP auf 4.4BSD-Lite unter Rhapsody aktualisiert. Für Mac OS X ist zu Entwicklungsbeginn, da es direkt auf Rhapsody basiert, der einzige Unterschied, dass der Kernel den Namen XNU erhalten hat und mit Carbon ein Macintosh-API enthalten ist.\n\nErstmals veröffentlichte Apple den Teil, der quelloffen ist, als eigenes Projekt unter dem Namen Darwin. Die Hoffnung war, dass sich unabhängige und freie Entwickler finden würden, die an der Basis des Betriebssystems mitarbeiten würden, wovon letztlich auch Mac OS X profitieren würde. Als Distribution wurden jedoch nur einige der frühen Darwin-Versionen veröffentlicht, die man nochdazu meist unter Mac OS installieren musste und die es von Apple nur für den PowerPC gab. Darwin ist jedoch der Kern für Mac OS X und das darauf basierende iOS geblieben.\n\nAuch die Blue Box wurde weiterentwickelt und als Classic-Umgebung in Mac OS X integriert. Allerdings wurde kein dazu notwendiges Mac-OS-Betriebssystem beigelegt, sodass ein bereits vorhandenes (vorinstalliertes) verwendet oder ein separat zu erwerbendes Mac OS 9 installiert werden muss, um die Classic-Umgebung nutzen zu können.\n\nAm 13. September 2000 veröffentlichte Apple die Public Beta von Mac OS X, das nun die Versionsnummer 10.0 erhielt. Dies sollte die Nachfolge für Mac OS 9 verdeutlichen. Am 24. Mai 2001 erschien schließlich die finale Version als Mac OS X 10.0, Codename „Cheetah.“\n\nEnde 2000 wurden erste Power Macs mit vorinstalliertem Mac OS 9 und Mac OS X 10.0 ausgeliefert, Mac OS 9 allerdings blieb das Standard-Betriebssystem. Ab 2001, mit Mac OS X 10.0.3, war Mac OS X das Standardsystem. Ein mitgeliefertes Mac OS 9.1 oder 9.2 konnte durch den Benutzer nachinstalliert werden. Die letzte Version des klassischen Mac OS war Version 9.2.2.\n\nDie Classic-Umgebung ist bis Mac OS X Tiger (10.4, 2005; 10.4.11 ist Ende 2007 die letzte Aktualisierung) enthalten. Im Unterschied zur Blue Box sind die Fenster der unter diesem System laufenden „klassischen“ Mac-OS-Programme frei verschiebbar, der Schreibtischhintergrund des Mac OS ist dabei ausgeblendet. Diese Kompatibilitätsumgebung ermöglicht auch die Ausführung von 68k-Programmen, weil diese Möglichkeit als transparente Emulation in Mac OS für PowerPC enthalten ist und auch in der Virtualisierung funktioniert. Die Classic-Umgebung war ausschließlich auf PowerPC-basierten Macs nutzbar und wurde mit Mac OS X Leopard (10.5, 2007) eingestellt.\n\nEbenfalls seit Leopard ist Mac OS X UNIX zertifiziert.\n\nMit Lion (10.7; ab 2011) wurde im Oktober 2012 die letzte Version (10.7.5) der neuen Betriebssystemgeneration mit „Mac OS“ in der ursprünglichen Schreibweise im vollen Namen veröffentlicht. Beworben wurde dieses bereits als „OS X Lion“. Sein Nachfolger Mountain Lion (10.8; ab Juli 2012) hieß dann offiziell „OS X“ – das „Mac“ im Namen wurde weggelassen.\nMac OS X bildet ab Version 10.4 (Tiger, 2005) die Grundlage für Apples ab 2006 entstandenes Apple TV und die Mobilsysteme iPhone sowie iPod, iPad und Apple Watch: tvOS (ab 2006), iOS (ab 2007) und watchOS (ab 2014). Konform zu deren Bezeichnungen heißt das Betriebssystem für Computer seit 2016 ab Version 10.12 „macOS.“\n"}
{"id": "3256", "url": "https://de.wikipedia.org/wiki?curid=3256", "title": "Macintosh", "text": "Macintosh\n\nDer Macintosh [] des kalifornischen Unternehmens Apple war der erste Mikrocomputer mit grafischer Benutzeroberfläche, der in größeren Stückzahlen produziert wurde. Der Name ist von der Apfelsorte \"McIntosh\" abgeleitet; unter Nutzern etablierte sich früh die Abkürzung Mac []. Heute tragen Apples Personal Computer auch offiziell diese Produktbezeichnung – in Kombinationen wie \"Mac mini, MacBook Air, MacBook, MacBook Pro, iMac\" und \"Mac Pro\". Auch intern wird „Macintosh“ heute nicht mehr verwendet. „Mac“ war ebenso Teil des Namens der auf den Geräten laufenden Betriebssystemreihen \"Mac OS\" (bis 2001) und \"Mac OS X\" (bis 2012, danach nur \"OS X\", ab Mitte 2016 macOS).\n\nDer erste Mac war der Nachfolger des technisch ähnlichen, aber wirtschaftlich erfolglosen und 10.000 USD teuren Apple Lisa. Der Macintosh 128k wurde am 24. Januar 1984 von Apple-Mitbegründer Steve Jobs vorgestellt. Der Werbespot „1984“ für den Mac wurde beim Super Bowl XVIII aufgeführt. Zum Preis von 2.495 USD (entsprachen damals etwa 7.200 DM/ungefährer Verkaufspreis in Deutschland: 10.000 DM, letzteres entspricht rund  EUR heute) erhielt man einen Rechner auf der Basis von Motorolas 68000-CPU, die mit 8 MHz getaktet war und auf 128 KB Arbeitsspeicher (RAM) zugreifen konnte – was sich schnell als zu wenig erwies. Ein 3,5-Zoll-Diskettenlaufwerk mit 400 KByte Speicherplatz und ein integrierter 9-Zoll-Monitor vervollständigten den ersten Macintosh.\n\nEbenso wie der Vorgänger Lisa war auch der Macintosh mit einer grafischen Benutzeroberfläche und einer Maus ausgestattet. Lizenziert wurde die Mausbedienung und Grundzüge der grafischen Oberfläche von der Firma Xerox, die 1973 mit dem Xerox Alto dieses Konzept entwickelte, welches Steve Jobs dann per Lizenz für seine Modelle ab 1979 übernahm und modifizierte. Das Betriebssystem des Macintosh hatte ursprünglich keinen Namen und wurde nur „System“ (mit angehängter Versionsnummer) genannt. Ab Version 7.5.1 hieß es dann \"Mac OS\" (abgeleitet von \"Macintosh Operating System\"). Es war von Beginn an auf die Bedienung mit der Maus zugeschnitten und enthielt zu diesem Zeitpunkt revolutionäre Konzepte, wie den „Papierkorb“, mit dem das Löschen von Dateien wieder rückgängig gemacht werden konnte, den „Schreibtisch“, Drag and Drop, das Auswählen von Text oder Objekten zwecks Änderung der Attribute und das Navigieren im Dateisystem mit Hilfe von Icons. Weitere grundlegende Konzepte, die den Anwendern die damals noch weitverbreitete Scheu vor der Benutzung von Computern nehmen sollten, waren die Undo-Funktion und die durchgängig einheitliche Bedienung verschiedener Anwendungsprogramme.\n\nTrotz dieser Neuerungen verkaufte sich der neue Computer anfangs nur in kleinen Stückzahlen. Gründe dafür wurden in seinem hohen Preis und darin gesehen, dass er in seiner Form und Art der Benutzung weit von dem entfernt war, was man zu jener Zeit gemeinhin unter einem professionellen Computer verstand (Monitore mit grüner Schrift auf schwarzem Hintergrund und die Eingabe langer Kommandozeilen). Erst die Nachfolgemodelle des originalen Macintoshs konnten eine größere Nutzerschaft an sich binden, die dann einen hohen Gesamtmarktanteil erreichte, jedoch zahlenmäßig von Windows-Systemen überholt wurde. Der Marktanteil des Macintosh war um 2000 auf einen Tiefstand von je nach Zählmethode zwischen drei und fünf Prozent gesunken. Mit der Einführung von Mac OS X stieg der Marktanteil kontinuierlich und hat 2011 in den USA etwa 13 % und weltweit ca. 6 % erreicht.\n\nVon Sommer 1994 bis September 1997 wurde das klassische Mac OS an andere Computerhersteller (unter anderem Umax und Power Computing) lizenziert. Die aus dieser Lizenz resultierenden Macintosh-kompatiblen Computer wurden Mac-Clones genannt.\n\nDer Macintosh ist nach der Apfelsorte McIntosh benannt. Der McIntosh war der Lieblingsapfel von Jef Raskin, der Mitglied des Macintosh-Designteams war. Ein alternativer Name während der Entwicklungszeit des Projektes war „Bicycle“: Apple-Mitbegründer Steve Jobs sah den letztlich Macintosh genannten Rechner als „Bicycle for your mind“, doch aufgrund des Widerstandes des Entwicklerteams setzte sich die neue Bezeichnung nicht durch.\n\nFür den Namen Macintosh hat Apple 1983 ein 10-jähriges Lizenzabkommen mit dem US-amerikanischen HiFi-Hersteller McIntosh Laboratory, Inc. abgeschlossen.\n\nTypisch für die Hardware der kompakten Macintosh-Modelle war, möglichst alle Grundfunktionen auf der Hauptplatine zu vereinen. Von Anfang an waren in den Kompaktmodellen (Macintosh Plus, Macintosh SE, Macintosh Classic usw.) Grafik, Ton und netzwerkfähige serielle Schnittstellen integriert, kurz darauf kamen ADB und SCSI und später Ethernet und Modem hinzu. Schließlich wurden FireWire und USB Standard. Einsteigergeräte wie der iMac sind nur durch externe Anschlüsse erweiterbar. Die Modelle der Pro-Reihe bieten durch interne PCI- und AGP- bzw. PCIe-Steckplätze eine größere Flexibilität.\n\nDie Prozessoren wurden zunächst von Motorola, später auch von IBM hergestellt. Motorola hat sich jedoch vollständig auf die Produktion von Mobiltelefonen konzentriert, während IBM PowerPC-Prozessoren nicht mehr in für Apple optimierten Ausführungen weiterentwickelte. Deshalb wurde nach Ankündigung im Jahr 2005 die gesamte Modellreihe auf Intel-Prozessoren umgestellt. Am 10. Januar 2006 wurden der erste Intel-iMac sowie die ebenfalls Intel-basierte Laptopreihe MacBook/MacBook Pro (Nachfolger der iBooks und PowerBooks) vorgestellt. Noch im selben Jahr wurden auch die Workstations (Mac Pro) und Server (Xserve) auf Intel-Prozessoren umgestellt.\n\nMit den ersten iMacs wurden zunehmend statt Eigenentwicklungen (wie ADB) verbreitete Komponenten (wie USB) verwendet. Bestehende Industriestandards wie ATA wurden auch in Mac-Computern Standard. Apples frühes Engagement für WLAN, Bluetooth oder FireWire als junge Industriestandards half diesen Techniken bei der Verbreitung.\n\nWährend Apple weiterhin seine Motherboards selbst entwickelt, sind seit 2006 die verwendeten Bausteine im Computer (Prozessoren, Controller, Grafik-, Sound-, Netzwerk-Chips) die gleichen wie in anderen Computern. Statt des BIOS wird allerdings das Extensible Firmware Interface verwendet. Seit der Umstellung auf Intel-Prozessoren kann Windows mit der Software „Bootcamp“ auf Apple-Computern installiert werden. Die technische Ausstattung von Mac-Rechnern weist keine relevanten Unterschiede zu anderen PCs auf. Durch die Zusammenstellung, das Motherboard-Design und die Konzeption des Gesamtrechners und das eigene, größtenteils proprietäre Betriebssystem behält Apple jedoch weiterhin die Kontrolle über das Gesamtgerät.\n\nFür diese kontrollierte Computerplattform entwickelt Apple das Betriebssystem selbst. Laut Apple sollen Hard- und Software gut aufeinander abgestimmt sein, was Treiberprobleme verhindere und die Energieeffizienz verbessere. Hard- und Software sollen als Produkt eine Einheit bilden.\n\nWeitere technische Ausstattungsmerkmale sind oder waren:\n\nBereits die im März 2001 eingeführte Erstfassung des bis heute von Apple für den Macintosh verwendeten Betriebssystems macOS weist gegenüber ihren (namentlichen) Vorgängern technisch gesehen praktisch keine Gemeinsamkeiten mehr auf. Mac OS X wurde auf Basis des NeXTstep-Betriebssystems entwickelt, eines Unix-Derivats. Dessen Herstellungsunternehmen NeXT, das Steve Jobs 1986 nach seinem Weggang von Apple gegründet hatte, war 1996 von Apple für 400 Millionen US-Dollar übernommen worden. Der Kernel von Mac OS X wurde Darwin getauft, er ist ein Open-Source-Unix-Derivat, das wie der Kernel von NeXTstep von FreeBSD und dem Mach Microkernel abgeleitet ist. Dies ermöglicht es, dass ein Großteil der Open-Source-Software, die unter anderen, offenen Unix-Derivaten entwickelt wurde, auch unter Mac OS X verwendet werden kann oder mit verhältnismäßig wenig Aufwand auf den Macintosh portiert werden kann.\n\nAuf den Kernel setzt die sogenannte Aqua-Oberfläche auf, eine gegenüber dem „klassischen“ Mac OS völlig neu gestaltete Benutzeroberfläche, deren Designelemente (Transparenz- und Schatteneffekte, detailliertere Icons) zum Teil auch in Oberflächen von Unix/Linux- und Windows-Systemen Eingang fanden. Daneben steht jedoch auch – ein Novum für den Macintosh – die Bedienung per unixtypischer Kommandozeile zur Verfügung.\n\nMit der Vorstellung der Software Boot Camp im April 2006 wurde es erstmals möglich, einen Macintosh mit einem Betriebssystem für x86-Prozessoren zu starten, was etwa die Parallelinstallation eines Microsoft-Windows-Systems ermöglicht. Diese Möglichkeit besteht allerdings nur bei einem Apple-Rechner mit x86-Prozessor.\n\n\n\n"}
{"id": "3257", "url": "https://de.wikipedia.org/wiki?curid=3257", "title": "Mac OS Classic", "text": "Mac OS Classic\n\nMac OS, retronym auch als Mac OS Classic oder Classic Mac OS – zu Deutsch klassisches Mac OS – bezeichnet, ist das ursprüngliche Betriebssystem des Macintosh von Apple, das seit 1982 entwickelt und seit 1984 vertrieben wurde. Es hatte anfangs nur die sehr unspezifische Bezeichnung System, bis es in System-Version 4.0 vom Januar 1987 erstmals in seiner Gesamtheit als Macintosh System Software bezeichnet wurde. Diese Bezeichnung wurde dann bei den Versionen 5 und 6 offiziell verwendet, bis es zu Beginn der 1990er-Jahre wieder einfach nur System hieß: System 6 und System 7. Ab Version 7.6 wurde es in Mac OS umbenannt. Das Betriebssystem war nach Xerox Star und Lisa OS das dritte kommerziell vertriebene Betriebssystem, das eine grafische Benutzeroberfläche besaß und eine Maus unterstützte. Zentrales Element ist der einem Schreibtisch nachempfundene Finder, der bis System 6 eine eigenständige Versionsnummer führte.\n\nMac OS ist die einzig vollständige Implementierung der Macintosh-Programmierschnittstelle. Es wurde nach der Übernahme von NeXT Ende 1996 durch das aus OPENSTEP heraus entwickelte Mac OS X ersetzt, für das zur einfacheren Portierbarkeit existierender Macintosh-Anwendungen die Programmierschnittstelle als Carbon neu implementiert werden musste.\n\nAb Mai 2001 wurden alle neuen Macs mit Mac OS 9 und Mac OS X ausgeliefert, bis Januar 2002 blieb Mac OS 9 das vorinstallierte Standard-Betriebssystem.\n\nUm teuren Arbeitsspeicher und knappen Disketten-Speicherplatz zu sparen, waren anfangs wesentliche Teile des Betriebssystems in einen Nur-Lese-Speicher (Read-only-Memory; ROM) ausgelagert. Dadurch wurden platzsparende und trotzdem leistungsfähige Programme möglich: Das erste Textverarbeitungsprogramm MacWrite war weniger als 30 KB groß und hatte dennoch eine graphische Oberfläche, mehrere Fenster und bot WYSIWYG. Auch MacPaint zum Zeichnen war derart klein.\n\nZunächst konnte immer nur ein Programm zur selben Zeit laufen; ab 1987 ermöglichte es der MultiFinder, ab 1991 System 7, mehrere Programme gleichzeitig laufen zu lassen. System 7 war bereits ein 32-Bit-Betriebssystem.\n\nGegen Ende der 1990er Jahre zeigten sich die Nachteile des klassischen Mac OS gegenüber echten Mehrbenutzersystemen wie Solaris, Linux, BSD oder Windows NT. Die Multitasking-Fähigkeiten waren nicht vollständig vom System kontrolliert (sog. kooperatives Multitasking), sodass es gemeinsam mit dem fehlenden Speicherschutz zum Totalabsturz des Systems führen konnte, wenn ein einzelnes Programm einen Fehler auslöste und nicht mehr reagierte.\n\nUm diese Defizite auszugleichen, versuchte Apple ein moderneres Betriebssystem unter dem Projektnamen Copland neu zu entwickeln. Das Projekt scheiterte zwar, einige der Neuerungen aus Copland wurden jedoch in das bestehende „System“ integriert, das damit auch gleich in Mac OS umbenannt wurde. Bereits Mac OS 7.6 erhielt Teile aus Copland, Mac OS 8 wartete zudem mit der überarbeiteten Copland-Oberfläche „Platinum“ auf. Bei Mac OS 8.5 wurden Kernfunktionen aus Copland übernommen, sodass Mac OS 8.5 einen vollkommen überarbeiteten Kernel aufweist, der sogar Multiprozessor-fähig ist. All diese Modifikationen konnten jedoch die Kerndefizite nicht beheben, unter anderem auch deshalb, weil das Betriebssystem auf Ebene der Programmschnittstelle (API) zum Macintosh-API kompatibel bleiben sollte.\n\nApple machte sich daher auf die Suche nach Alternativen. Unter anderem wurde erwogen, das Unternehmen Be Incorporated zu kaufen, um deren in der Entwicklung befindliches, jedoch bereits stabiles und auf der Basis ausgereiftes Betriebssystem BeOS als Grundlage für das neue Macintosh-Betriebssystem zu nutzen. Das Vorhaben scheiterte jedoch – über die Gründe kursieren viele Gerüchte.\n\nLetztendlich kaufte Apple die von ihrem ehemaligen Vorstandsvorsitzenden Steve Jobs gegründete Firma NeXT für knapp 400 Millionen US-Dollar auf, um so mit der Hilfe von Steve Jobs auf Basis von OPENSTEP (ursprünglich NeXTStep) ein vollkommen neues Betriebssystem mit dem Namen „Mac OS X“ zu entwickeln.\n\nMac OS 9 wurde in der Classic-Umgebung bis Mac OS X Tiger (bis 10.4.11, 2007) weiter unterstützt, um den Umstieg der Anwender zu erleichtern, die so ihre alten Programme weiter nutzen konnten. Auf Macs mit Intel-Prozessor (ab 2006) sowie unter Mac OS X Leopard (10.5, 2007, letzte auf dem PowerPC lauffähige Version) lief die Classic-Umgebung allerdings nicht mehr; hier ist man auf Fremdprogramme wie SheepShaver angewiesen.\n\nDer Name „Mac OS“ findet seine erste Erwähnung im Start-Logo von System 7.5.1; mit Version 7.6 heißt das Betriebssystem dann offiziell „Mac OS 7.6“. Macintosh-Betriebssysteme hatten bis System 6 „Macintosh System Software“ geheißen, doch anschließend wurde wieder zunehmend die ursprüngliche Bezeichnung „System“ verwendet – so auch bei System 7, System 7.1 und System 7.5. Die letzte Ausgabe von Mac OS in seiner Ursprungsform ist Mac OS 9.2.2 vom Dezember 2001.\n\nUrsprünglich war jede Betriebssystemkomponente sowie jede Softwarekomponente mit einer eigenen unabhängigen Versionsnummer versehen. Das führte auch deshalb zu Verwirrung, weil das eigentliche Betriebssystem, schlicht „System“ genannt, eine andere Version führte als das Hauptprogramm des Macintosh: der Finder. Zusätzlich bedruckte Apple die an Händler abgegebenen Disketten für die Aktualisierung bestehender Systeme mit eigenen Versionsnummern, die mit der Betriebssystemversion und der Version des Finder nichts zu tun hatten.\n\nDie Bezeichnung „“ (oder auch nur „“) wird erst ab System Software 5 offiziell von Apple genutzt. Da es jedoch kein System 5 gibt, sondern die System Software 5.0 das System 4.2 und den Finder 6.0 enthält, ist die Identifizierung oft nicht ganz eindeutig, zumal das laufende System selbst keinerlei Informationen über die vermeintliche Version der „“ bietet. Die Bezeichnung „“ wurde dennoch von Händlern und weiteren Quellen (Anwender, Seiten im Internet) verwendet, obwohl dies lediglich der Aufdruck auf den Speichermedien (Disketten) oder der beiliegenden Dokumentation war und überdies zumeist anders lautete, z. B. „Macintosh Utilities“ oder „Macintosh Software“.\n\nVersionen vor der „“ sind daher grundsätzlich nicht mit der Version des „System“ (oder des Finder) gleichzusetzen. Um die Situation zu bereinigen wurde System 5 übersprungen und die Versionsnummern ab System 6, der „Macintosh System Software 6“ mit dem Finder 6.1 und dem MultiFinder 6.0, weitestgehend vereinheitlicht.\n\nDie bis Version 5.0 als \"\" aufgeführte Version ist jene vom Aufdruck der Medien. Sie ist auf einem laufenden Macintosh nicht auslesbar. Ab „Macintosh System Software 6“ (System 6) sind die Versionen weitgehend (außer der Subversion des Finder) und in System 7 zur Gänze vereinheitlicht. System 7.5 war das letzte Betriebssystem für klassische Macintosh-Computer aus den 1980er-Jahren. Ab Version 7.6 wurde System 7 zudem in Mac OS 7.6 umbenannt.\n\nDer Macintosh XL kann, da er baugleich mit der Apple Lisa ist, nur Lisa OS nativ ausführen. Er wurde jedoch mit MacWorks XL ausgeliefert, einem Port des Macintosh-Plus-ROMs, womit System 1.1 bis 3.2 auf dem Macintosh XL lauffähig wird. Mit MacWorks Plus ist er kompatibel bis System 6.0.3 und mit MacWorks Plus II Basic bis 6.0.8. MacWorks Plus II Pro ermöglicht es dem Macintosh XL sogar System 7.5.5 auszuführen, das letzte System für 68k-Macs mit dem ursprünglichen Motorola-68000-Prozessor.\n\nSystem 6 wurde im Juni 1988 für alle bestehenden Macintosh-Computer mit Ausnahme des Macintosh 128k und 512k als gratis Upgrade ausgeliefert. Version 6.0.1 war beim Macintosh IIx enthalten, Version 6.0.3 beim Macintosh SE/30, IIcx und zudem im ROM des Macintosh Classic, Version 6.0.4 wurde mit dem Macintosh IIci und dem Macintosh Portable ausgeliefert, Version 6.0.5 mit dem Macintosh IIfx und die fehlerhafte Version 6.0.6 mit dem Macintosh Classic, IIsi und LC. Die letzte Version war 6.0.8L vom Februar 1992, fast ein Jahr nach Erscheinen von System 7.\n\nDas 1991 erschienene System 7 stellte für Apple einerseits eine komplette Überarbeitung und Modernisierung des bestehenden Betriebssystems dar, andererseits markierte es auch einen Totpunkt in der Entwicklung, da es Apple nicht möglich war, moderne Betriebssystemfunktionen wie Multitasking und Speicherschutz stabil zu integrieren. Projekte zur Entwicklung eines modernen neuen Betriebssystems – „Pink“ bzw. TalOS, „Red“ bzw. Raptor, Copland und Gershwin – blieben unvollendet oder wurden bereits in der Planungsphase gestoppt.\n\nApple suchte daher nach einem Nachfolgebetriebssystem eines anderen Herstellers. Schließlich wurde Ende 1996 NeXT übernommen und deren modernes Unix-Betriebssystem OPENSTEP bis 2001 in Mac OS X weiterentwickelt, wobei die Programmierschnittstelle des Macintosh als Carbon neu implementiert wurde. Carbon war auch auf Mac OS ab Version 8.1 verfügbar, sodass „carbonisierte Programme“ () sowohl auf Mac OS X als auch auf Mac OS laufen konnten.\n\nVersion 7.6 wurde in Mac OS 7.6 umbenannt, da System 7 auch auf Macintosh-Klonen laufen konnte und man mit dem Namen einen Hinweis auf die von Apple selbst produzierten Macintosh-PCs geben wollte, auch wenn das System auf einem der Klone lief. Mac OS 7.6 war außerdem das erste Mac OS, das Teile des unvollendeten Copland erhielt.\nAb Mac OS 8 wurde das Betriebssystem mit weiteren Teilen von Copland verbessert, wie etwa dem Platinum-Design oder ab Mac OS 8.5 einem verbesserten Kernel. Mac OS 9 wurde nur noch vorbereitend auf das kommende Mac OS X 10.0 eingeschoben, da etwa die internen Unterschiede zwischen Mac OS 8.6 und Mac OS 9.0 weit geringer ausfallen als jene zwischen Mac OS 8.1 und Mac OS 8.5.\n\nMac OS war in einer virtualisierten Umgebung namens Blue Box unter dem Betriebssystem Rhapsody, das Apple ab 1997 entwickelte, weiterhin lauffähig. In der zweiten Vorabversion von 1998, interne Versionsnummer Rhapsody 5.1, lief darin Mac OS 8.1, unter Mac OS X Server 1.0 (1999), das Rhapsody 5.3 entspricht, lief Mac OS 8.5 und in der letzten Aktualisierung Mac OS X Server 1.2v3 (2000, intern Rhapsody 5.6) lief Mac OS 8.6. Rhapsody wurde zu Mac OS X weiterentwickelt und die Blue Box zur Classic-Umgebung.\n\nDer letzte Macintosh, der Mac OS 9 direkt starten kann, ist der Power Mac G4 „Mirrored Drive Doors 2003“, der nach Anfragen von Anwendern von 2003 bis 2004 noch einmal aufgelegt wurde. Auf neueren PowerPC-basierten Mac-Modellen ist Mac OS ab Version 9.1 nur noch in der Classic-Umgebung des Nachfolger-Betriebssystems Mac OS X bis Tiger (10.4, 2005; letzte Aktualisierung 10.4.11, 2007) nutzbar, nicht jedoch auf den Server-Modellen Xserve. Auf Intel-basierten Macs ist die Classic-Umgebung nicht mehr verfügbar und somit auch kein offizielles (virtualisiertes) klassisches Mac OS.\n\n"}
{"id": "3555", "url": "https://de.wikipedia.org/wiki?curid=3555", "title": "OPENSTEP", "text": "OPENSTEP\n\nOPENSTEP [] war ein Betriebssystem des Unternehmens NeXT, das Steve Jobs nach seinem Weggang von Apple 1985 gegründet hatte. OPENSTEP, in Großbuchstaben, war der Name des Betriebssystems ab Version 4.0 von 1996. Von Version 3.1 bis 3.3 (1993–1995) hieß es NeXTSTEP bzw. NEXTSTEP [], ursprünglich bis Version 3.0 (bis 1992) in der Schreibweise NeXTStep.\n\nEs basiert auf dem Unix-ähnlichen Betriebssystem 4.3BSD und einem Mach-2.5-Kernel. Verbreitung hatte es vor allem im wissenschaftlichen, aber auch im Bankbereich, wo dank der damals ungewöhnlichen, objektorientierten Entwicklungsumgebung schnell komplexe Applikationen gebaut werden konnten.\n\nNeXT wurde Ende 1996 von Apple aufgekauft, und Steve Jobs kehrte im Sommer 1997 zu Apple als CEO zurück. OPENSTEP 4.2 wurde die Basis des unter dem Codenamen Rhapsody weiterentwickelt und ab 2000 als Mac OS X auf den Markt gebrachten Nachfolge-Betriebssystems für Apple-Macintosh-Computer, um das ältere „klassische“ Mac OS (1984–2001) zu ersetzen. Die NeXTstep- bzw. OpenStep-Programmierschnittstelle (API) wurde bei Apple als Cocoa weiterentwickelt und ist nicht nur für Mac OS X zum wichtigsten API geworden – Mac OS X wurde 2012 in OS X und 2016 in macOS umbenannt –, sondern auch das der mobilen Betriebssysteme iOS, tvOS und watchOS.\n\nDer auf NeXTStep/OPENSTEP zurückgehende Unix-Unterbau von Rhapsody bzw. Mac OS X erhielt 1999 den Namen Darwin und wurde im Quelltext veröffentlicht.\n\nBei NeXTStep handelt es sich um ein Microkernel-Betriebssystem, das den Mach-Mikrokernel verwendet. Auf Basis dieses Kernels ist ein gewöhnliches BSD-Unix aufgebaut. Dadurch bietet NeXTStep Funktionen wie präemptives Multitasking, Multithreading und Speicherschutz, jedoch fehlt Multiprozessorunterstützung; diese war im Mach-Kernel zwar vorgesehen, wurde aber nicht aktiviert. Zur Grafikausgabe wird Display PostScript von Adobe verwendet, dies ist die PostScript-Variante für Monitore (statt für Drucker) und ermöglicht echtes WYSIWYG. Zusammen mit Display PostScript kommt ein objektorientiertes Anwendungs-Framework zum Einsatz, das die Programmierung von grafischen Benutzeroberflächen stark vereinfacht. Objective-C wird als Standard-Programmiersprache unter NeXTStep eingesetzt und war mit ein Grund für die OO-Entwickler-Tools, die mit dem Betriebssystem mitgeliefert wurden. Als Dateisystem wird das auch bei den verschiedenen BSD-Unix-Varianten eingesetzte UFS verwendet.\n\nDie Benutzerumgebung ist standardmäßig reichhaltig ausgestattet. Es gibt einen Installer/Deinstaller, das Webster-Wörterbuch, einen leistungsfähigen Texteditor, Softwareschnittstellen für eine Fax-Einbindung, etc.\n\nDie Bedienung der GUI weist verschiedene Besonderheiten auf, welche die Arbeit erleichtern sollen:\n\nDie Schreibweisen „NeXTstep“ („step“ in Kleinbuchstaben) und „OpenStep“ bezeichnen die Programmierschnittstellen (APIs, s). Hingegen wurde das Unix-basierte vollständige Betriebssystem, das diese Schnittstelle implementiert, „NeXTStep“ (großer Anfangsbuchstabe bei „Step“) und ab Version 3.1 „NeXTSTEP“ („STEP“ zur Gänze in Großbuchstaben) geschrieben. In Version 4.0 wurde das gesamte Betriebssystem, analog zum Namen der Spezifikation zu diesem Zeitpunkt, in „OPENSTEP“ (alles Großbuchstaben) umbenannt. Diese unterschiedlichen Schreibweisen sorgten immer wieder für Ungenauigkeiten, da sich viele Artikel, auch in der Fachpresse, nicht an diese Schreibkonvention hielten. Oft war von „NextStep“ oder „Nextstep“ die Rede, ohne genauer zu differenzieren, ob das Betriebssystem oder die Spezifikation der Programmierschnittstelle gemeint war.\n\nNeXTStep bis Version 3.0 lief ausschließlich auf NeXT-Hardware wie dem NeXTcube und der NeXTstation. NeXT sah sich als Hersteller von Hardware mit speziell darauf optimierte Software, wie es Apple zuvor mit der Lisa und dem Macintosh vorgemacht hatte. Der Gründer der Firma NeXT, Steve Jobs, war vor seinem Abgang bei Apple 1985 mit der Leitung der Macintosh-Entwicklung beauftragt gewesen. Bei NeXT wurde dieses Konzept der Einheit zwischen Computer-Hardware und -Software übernommen: das Benutzerhandbuch zu NeXTStep 1.0 bezeichnete das Betriebssystem in seiner Gesamtheit als – „NeXTStep“ kann daher auch, gerade in den ersten Versionen, nur in diesem Zusammenhang gesehen werden. Zu Anfang existierte mit dem NeXTStep-Betriebssystem nur eine einzige Implementierung der NeXTstep-API, sodass die Schnittstelle und das Betriebssystem in gewisser Weise gleichbedeutend und nicht trennbar verbunden waren.\n\nAls IBM 1988 an NeXT herantrat, um die Programmierschnittstelle zu lizenzieren, war es erstmals notwendig, das Betriebssystem NeXTStep von der Programmierschnittstelle, die ab diesem Zeitpunkt NeXTstep (also „step“ in Kleinbuchstaben) genannt wurde, zu trennen. IBM wollte die NeXTstep-Schnittstelle auf das AIX-Betriebssystem portieren, um das eigene UNIX-Betriebssystem für Programmierer von Anwendersoftware attraktiver zu machen. Programme für NeXTStep hätten mit nur minimalem Portierungsaufwand auch auf AIX laufen können. Bald gab es auch Gerüchte, IBM würde das NeXTstep-API auch in OS/2 einfließen lassen. Später stellte sich jedoch heraus, dass es einzig bei der Lizenzierung blieb. Die Programmierschnittstelle wurde bei IBM nie auf ein weiteres Betriebssystem, auch nicht AIX, portiert.\n\nSo blieb bis 1993 NeXT-Hardware die einzige Plattform für das Betriebssystem inklusive API. Da NeXT jedoch keine Gewinne machte, musste im Februar 1993 die Produktion eigener Hardware notgedrungen eingestellt werden. Für das Betriebssystem und die NeXTstep-API interessierten sich jedoch einige Firmen, weshalb NeXTstep (die Programmierschnittstelle) nun auch auf Windows NT von Microsoft und Solaris von Sun portiert werden sollte. Das Betriebssystem selbst wurde ab NeXTStep 3.1 neben den NeXT-Computern auch auf Intel-i486-Hardware, einigen PA-RISC-Workstations von HP (konkret die Workstation HP 9000 Model 712 \"Gecko\"), auf SPARC und zumindest im Labor auch auf PowerPC portiert. Die Namensnennung des neuen Betriebssystems war NeXTSTEP/Intel, NeXTSTEP/SPARC und später NEXTSTEP/PA-RISC (wobei ab der PA-RISC-Portierung das klein geschriebene „e“ für alle Plattformen fallen gelassen wurde).\n\nDie Programmierschnittstelle wurde ab 1994 gemeinsam mit Sun weiterentwickelt und als offene Spezifikation herausgegeben. Um diese Neuerung noch sichtbarer zu machen, wurde die API in OpenStep umbenannt (und entgegen der Schreibweise von NeXT als „OPENSTEP Enterprise“ vermarktet). Ein System darf sich als „OpenStep compliant“ bezeichnen, wenn es die Spezifikation erfüllt. Sun kaufte Teile des Quelltextes von NeXT um mit „OpenStep für Solaris“ genau dies zu tun – OpenStep griff dabei auf das auf Solaris laufende X11 für einige Grundfunktionen zurück. NeXT selbst entwickelte auf gleiche Weise „OPENSTEP für Windows“ (was entgegen der Konvention in Großbuchstaben geschrieben wurde und Teil des Produkts „OPENSTEP Enterprise“ ist), welches Funktionen von Windows NT für die Umsetzung der API nutzte. NeXTs eigenes Betriebssystem setzte mit Version 4.0 die OpenStep-Spezifikation ebenfalls um und wurde als „OPENSTEP für Mach“ in Versionen für NeXT-Hardware (Motoroloa-68k-Architektur, „OPENSTEP für CISC“), Intel (i486-Architektur, „OPENSTEP für CISC“) und SPARC (Sun SPARC, „OPENSTEP für RISC“) herausgebracht. Die Version für PA-RISC wurde eingestellt.\n\nDa die OpenStep-Programmierschnittstelle offengelegt worden war, konnte mit GNUstep eine quelloffene Implementierung der API geschaffen werden, die auf vielen weiteren Betriebssystemen lauffähig ist. Vor allem unter Linux und diversen freien BSD-Varianten ist oft ein GNUstep-Desktop zu finden, der neben dem API auch das Look-and-Feel von NeXTStep nachbildet.\n\nSun entwickelte indes mit Java eine eigene plattformübergreifende Programmierschnittstelle. Um nicht mit Java in Konkurrenz zu treten stellte Sun die Solaris-Version von OpenStep schließlich ein.\n\nMit OPENSTEP 4.1 erschien noch eine letzte Version des ursprünglichen Unix- und BSD-basierten NeXTStep-Betriebssystems, bevor sich Steve Jobs Ende 1996 mit Apple auf eine vollständige Übernahme von NeXT durch Apple einigen konnte. OPENSTEP diente Apple als Basis für das zu dieser Zeit gesuchte Nachfolgebetriebssystem für Mac OS Classic, denn mit dem Projekt Rhapsody wurde neben OpenStep, BSD und POSIX nun auch die Macintosh-API, anfangs nur durch die „Blue Box“ (der späteren Classic-Umgebung) und später in Form des Carbon-API, sowie Java unterstützt. Das letzte reine NeXT-UNIX, OPENSTEP 4.2, wurde bereits von Apple veröffentlicht und fünf Jahre lang unterstützt.\n\nDa die Programmierschnittstelle bereits für Windows NT vorhanden war, wurde das Cross-Plattform-Konzept ursprünglich auch von Apple übernommen. Auf der WWDC 1997 gab man bekannt, dass die nun in „Yellow Box“ (englisch für „Gelbe Box“) umbenannte Programmierschnittstelle auch auf weiteren Betriebssystemen zur Verfügung stehen werde. Yellow Box ist der neue Name und somit die Weiterentwicklung der OpenStep-Programmierschnittstelle. Ähnlich Java von Sun hätte Yellow Box auf Windows 95 und Windows NT laufen sollen, über weitere Betriebssysteme wurde nachgedacht. Apple selbst arbeitete an Rhapsody, einem auf OPENSTEP aufbauenden Betriebssystem, das aus einem Mach-Kernsystem (genannt \"Core OS\"), der \"Yellow Box\" (eine weiterentwickelte OpenStep-API), der \"Blue Box\" (der späteren Classic-Umgebung) und dem mit Copland entwickelten Desktop-Design „Platinum“, das auch in Mac OS 8 integriert wurde, bestand. Dazu musste Rhapsody als direkte Weiterentwicklung von OPENSTEP 4.2 auf die von Apple-Hardware genutzte PowerPC-Architektur portiert werden. Entwickelt wurde auch eine Intel-Version von Rhapsody (jedoch ohne Blue Box), diese wurde jedoch nie vermarktet. Angedacht, aber nie verwirklicht, wurde die als \"Red Box\" bezeichnete Funktion auf der Intel-Version, die ähnlich wie die Blue Box zur Virtualisierung eines anderen Betriebssystems auf dem Rhapsody-Desktop, allerdings nicht für ein Mac OS, sondern für ein Windows-Betriebssystem, für eine zusätzliche Kompatibilitätsschicht gesorgt hätte. Ähnlich wie auf OS/2 hätte damit auf Rhapsody/Intel die Möglichkeit bestanden, eine Vielzahl von existierenden Windows-Programmen (auf einem ebenso existierenden oder zusätzlich zu erwerbenden virtualisierten Windows-Betriebssystem) zu nutzen.\n\nEin Jahr später, auf der WWDC 1998, ließ Apple plötzlich verlauten, dass das Rhapsody-Experiment gescheitert sei. Es werde keine Intel-Version geben und auch keine plattformübergreifende Programmierschnittstelle. Rhapsody wurde nur noch in einer für PowerPC-basierte Apple-Computer laufenden Version als Mac OS X Server 1.0 (bis 1.2v3) herausgebracht und die Yellow Box wurde unter dem neuen Namen „Cocoa“ in Mac OS X integriert. Die Blue Box hingegen kam bei den Entwicklern nicht gut an, da die Programme, die darauf liefen, nicht von den modernen Vorzügen von Cocoa profitieren konnten. Ein Programm, das innerhalb der Blue Box lief, war auf die Funktionen von Mac OS 8 bzw. 9 limitiert. Gleichzeitig hätte aber eine Portierung auf Mac OS X und dessen modernes Cocoa-API (OPENSTEP, Yellow Box) einen immensen Personalaufwand für die Portierung des Quelltextes bedeutet, da wesentliche Programmteile vom Macintosh-API auf das nicht dazu kompatible Cocoa-API umgeschrieben hätten werden müssen. Um den Portierungsaufwand für bestehende Macintosh-Programme zu verringern wurde auf Druck der Entwickler mit Carbon eine zusätzliche Programmierschnittstelle in Mac OS X integriert, die Teile der ursprünglichen Macintosh-API unter Mac OS X verfügbar machte. Somit konnten vorhandene Mac-OS-Programme mit überschaubaren Anpassungen am Quelltext für Mac OS X herausgebracht werden, die auch von den Vorzügen des modernen Betriebssystems profitieren konnten, wie Speicherschutz und präemptives Multitasking.\n\nIn GNUstep, das die OpenStep-Spezifikation vollständig umsetzt, wurden die Neuerungen der API aus Yellow Box und Cocoa nur teilweise umgesetzt. Momentan (Stand: Januar 2016) unterstützt GNUstep Cocoa aus Mac OS X 10.4 vollständig, allerdings muss ein Programm aus dem Quelltext neu mit GNUstep übersetzt werden, damit es auf einem anderen Betriebssystem als Mac OS X läuft. 2013 wurde eine Crowdfunding-Kampagne gestartet um Cocoa aus Mac OS X Lion und 10.6 vollständig unterstützen zu können. Mit der Laufzeitumgebung Darling, die GNUstep mit einer Umgebung ähnlich Wine umsetzt, wäre es sogar möglich, OS-X-Applikationen unmodifiziert (ohne Neukompilierung) auf einem anderen unterstützten Betriebssystem auszuführen. Das Finanzierungsziel wurde jedoch nicht erreicht.\n\nAuf den NeXT-Computern, die Motorolas 68030- und 68040-Prozessoren nutzten, liefen alle Versionen von NeXTStep und OPENSTEP, bis hin zur letzten veröffentlichten Version 4.2 von 1996. Ab NeXTSTEP 3.1 von 1993 war das Betriebssystem für weitere Plattformen verfügbar. Rhapsody (1998) lief auf Intel-PCs und PowerPC-Macs. Seit Mac OS X, ab 1999, laufen alle Versionen exklusiv auf Apple-Hardware.\n\nAb Mitte der 1990er-Jahre war Apple auf der Suche nach einem Nachfolger für das damals als technisch veraltet geltende System 7, dem Betriebssystem für Macintosh-Computer, das ab Version 7.6 den Namen Mac OS trug. Die Wahl fiel schließlich auf NeXTStep der Firma NeXT – 1996 wurde das Unternehmen von Apple übernommen und ein neues Betriebssystem auf Basis von OPENSTEP entwickelt, das zunächst den Codenamen „Rhapsody“ trug. Dabei wurde das Betriebssystem nicht nur auf die PowerPC-Plattform des Macintosh portiert, es wurde auch der Unix-Unterbau von 4.3BSD auf 4.4BSD-Lite aktualisiert und der Kernel von Mach 2.5 auf einen Hybrid zwischen Mach 3.0 und dem monolithischen FreeBSD-Kernel neu implementiert. Erstmals wurde der Unix-Unterbau inklusive XNU-Kernel vollständig als Open Source ausgegliedert und als eigenständiges Betriebssystem verfügbar gemacht: Darwin. Rhapsody besaß das Look and Feel des klassischen Mac OS (das Platinum-Design von Mac OS 8) und war nur als Vorschau für Entwickler veröffentlicht worden. Mit Aqua wurde jedoch ein neues Aussehen für das Nachfolgebetriebssystem von Mac OS entwickelt, das im März 2001 unter dem Namen Mac OS X erschien. Um die zeitliche Lücke zu füllen wurde 1999 ein direkt auf Rhapsody basierendes Server-Betriebssystem, das ebenfalls den Namen Mac OS X trug, nur mehr für die hauseigenen Power-Macintosh-Computer veröffentlicht – im Gegensatz zu Rhapsody, das noch auf Intel-i486- und PowerPC-Hardware lief, erschien Mac OS X Server 1.0 (Rhapsody 5.3), ebenfalls noch im Platinum-Design ohne Aqua, nur für die Power-Macintosh-Serie. Intern wurde indes weiterhin sichergestellt, dass Mac OS X portierbar blieb – der Darwin-Teil etwa lief von Anfang an neben PowerPC auch auf Intel. Bei der Umstellung von der PowerPC- auf die Intel-Architektur 2006 profitierte Apple von diesem Erbe – obwohl seit der ersten Veröffentlichung von Mac OS X nur mehr eigene Hardware unterstützt wurde, war ein Architekturwechsel auf Basis des Darwin-Betriebssystems, das wiederum direkt von Rhapsody, OPENSTEP und NeXTStep abstammt, relativ einfach möglich. Auch die Portierung auf die ARM-Architektur mit iOS ist dieser Abstammung zu verdanken. Ab 2012 hieß das Betriebssystem nur noch OS X – ohne „Mac“ im Namen, weil Teile davon, wie der Darwin-Kern, mit dem davon abstammenden iOS nunmehr auch auf anderen Geräten laufen. 2016 wurde der Name jedoch analog zu iOS (ab 2010, davor iPhone OS und iPad OS) in macOS geändert, ebenso bei tvOS (ab 2015, davor -Software) und watchOS (2015 ab Version 2, davor Watch OS).\n\nDas letzte Rhapsody-Betriebssystem, Mac OS X Server 1.2v3, trägt die interne Versionsnummer 5.6 und ist somit noch direkt von NeXTStep, bis Version 3.3, und OPENSTEP, bis Version 4.2, abgeleitet. Auch die Codenamen folgen denen von NeXTStep und OPENSTEP, z. B. „Lightning9I“ für NeXTStep 3.3 und „Titan1U“ für Rhapsody 5.1. Im Gegensatz dazu wird bei der Weiterentwicklung von Rhapsody, Mac OS X ab 10.0 und den vorangegangenen Preview- und Beta-Versionen, die Versionsnummer zu Mac OS (bis 9.2.2) in Bezug gebracht, um dessen Rolle als Mac-OS-Nachfolger zu unterstreichen. Auch die Versionsnummern von Darwin sind mit Mac OS X / OS X / macOS synchronisiert. Nur der XNU-Kernel trägt eine unabhängige Versionierung, die mit der Entwicklung von Mac OS X begonnen wurde; der Kernel selbst basiert auf dem Rhapsody-Kernel, welcher wiederum aus dem OPENSTEP-Kernel entwickelt wurde.\n\nMit NeXTSTEP 3.1 wurden erstmals andere Rechnerarchitekturen unterstützt. Um die Versionen unterscheiden zu können, wurden diese mit einem Schrägstrich, gefolgt von der jeweiligen Architektur, gekennzeichnet sowie mit einer Farbe bezeichnet, die auch oft von den Benutzern von NeXTSTEP, z. B. bei Diskussionen in Foren, verwendet wurde:\n\nBei OPENSTEP (sozusagen NeXTStep ab Version 4) wird die Sache noch ein wenig komplizierter: Hier heißt das vollständige Unix-Betriebssystem mit eigenem Mach-Kernel nun „OPENSTEP für Mach“ und somit z. B. „OPENSTEP für Mach/Intel“ auf „weißer“ Hardware.\n\n\n"}
{"id": "3807", "url": "https://de.wikipedia.org/wiki?curid=3807", "title": "Apache OpenOffice", "text": "Apache OpenOffice\n\nApache OpenOffice (vormals OpenOffice.org) ist ein freies Office-Paket, das aus einer Kombination verschiedener Programme zur Textverarbeitung, Tabellenkalkulation, Präsentation und zum Zeichnen besteht. Ein Datenbankprogramm und ein Formeleditor sind ebenfalls enthalten. Es ist für alle wichtigen Betriebssysteme verfügbar. Das quelloffene Projekt war bis zur Abspaltung von LibreOffice eines der international führenden Office-Pakete, inzwischen hat LibreOffice diese Rolle aufgrund der größeren Entwicklerzahl und häufigerer Sicherheitsupdates übernommen.\n\nDer Zugang zu Funktionen und Daten wird durch offengelegte Schnittstellen und ein XML-basiertes Dateiformat ermöglicht. OpenOffice.org wurde unter der LGPL verbreitet. Apache OpenOffice wird unter der Apache-Lizenz Version 2 herausgegeben. Da diese keine Verbreitung zu gleichen Lizenzbedingungen vorschreibt, kann der Code weiterhin in das Projekt LibreOffice fließen.\n\nOpenOffice.org entstand im Jahr 2000 aus den offengelegten Quelltexten des damaligen StarOffice und wurde seither maßgeblich von Sun Microsystems, das später von Oracle aufgekauft wurde, entwickelt. Heute wird es von der Apache Software Foundation weiterentwickelt. Nachdem im September 2010 aus Unzufriedenheit mit Oracles Lizenz- und Entwicklerpolitik durch die von vielen ehemaligen OpenOffice.org-Entwicklern gegründete The Document Foundation die Abspaltung LibreOffice entstand, teilte Oracle im Juni 2011 mit, seine Rechte an \"OpenOffice.org\" auf die Apache Software Foundation (ASF) übertragen zu wollen.\n\nDas Apache OpenOffice Projekt verließ innerhalb von rund einem Jahr den „Incubator“-Projektstatus und wurde 2012 zu einem „Top-Level-Projekt“ hochgestuft, war jedoch ab Sommer 2014 weitestgehend inaktiv.\n\nDas Office-Paket enthält die folgenden Module, die in den anschließenden Abschnitten genauer beschrieben werden:\nApache OpenOffice ist für die Betriebssysteme Windows, Apple macOS (bis zur Version 2.x als X11-Version und als Nebenprojekt NeoOffice verfügbar; ab Version 3.0 ist Apache OpenOffice eine native Aqua-Anwendung), IBM OS/2, eComStation, Linux, Solaris (SPARC- und x86-Prozessorarchitektur), FreeBSD und andere Unix-Varianten erhältlich. ReactOS wird, je nach Version, durch die MS-Windows-Version unterstützt.\n\nMit „OpenOffice.org Portable“, auch „Portable OpenOffice.org“ genannt (siehe auch PortableApps), stehen seit 2.0.4 ausgewählte Versionen für Windows zur Verfügung, die zum Beispiel von einem USB-Stick lauffähig sind, ohne notwendigerweise Datenrückstände auf dem genutzten Rechner zurückzulassen (siehe auch Portable Software). Weiterhin gibt es eine U3-Version, die von einem USB-Stick ausführbar ist und abgespeicherte Daten verschlüsselt sowie mit einem Passwort schützt. Die aktuelle portable Version ist 4.1.5.\n\nApache OpenOffice kann die Daten vieler anderer Programme sowie die verbreiteten Dateiformate von Microsoft Word (*.doc/*.docx), Microsoft Excel (*.xls/*.xlsx) und Microsoft PowerPoint (*.ppt/*.pptx) zumeist ohne Probleme importieren und exportieren (Export nur *.doc/*.xls/*.ppt), doch es können (wie auch zwischen unterschiedlichen MS Office-Versionen) Formatierungsprobleme auftreten, zum Beispiel verrutschte Absätze. Auch lassen sich diverse „Legacy-Formate“ (ältere Dateiformate) anderer Anbieter importieren. Alle Formate lassen sich ohne Umwege ins Portable Document Format (PDF) exportieren.\n\nApache OpenOffice ist modular aufgebaut, aber als Gesamtpaket konzipiert. Identische Utensilien werden in der gesamten Suite genutzt. Die Werkzeuge, die es etwa im \"Writer\" zum Arbeiten mit Grafiken gibt, finden sich auch in \"Impress\" und \"Draw\" wieder. Alle Module teilen sich zudem dieselbe Rechtschreibprüfung etc. Das komplette Office-Paket kann in einem einzigen Vorgang installiert werden.\n\nMit \"Writer\" können sowohl kurze Texte wie Briefe, Serienbriefe, Memos, Etiketten, Visitenkarten als auch umfangreiche Schriften wie Bücher oder mehrteilige Dokumente mit Tabellen sowie Inhalts- und Literaturverzeichnissen geschrieben und gestaltet werden. Die Textverarbeitung bietet gängige Funktionen wie Textbausteine, Teamfunktionen, Rechtschreibprüfung, Silbentrennung, Thesaurus, Autokorrektur, mehrstufiges Undo sowie verschiedene Dokumentvorlagen. Mit Hilfe eines Assistenten werden eigene Dokumentvorlagen, Briefe, Faxe und Tagesordnungen erstellt. Neben dem Zugriff auf die Systemschriftarten enthält das Paket einen Satz freier Schriften. Versionsverwaltung von Dokumenten ist möglich. Das Paket ist voll Unicode-tauglich, es beherrscht CJK-Unterstützung und neben Rechtslauf auch Linkslauf.\n\nFormatvorlagen für einzelne Zeichen, Absätze, Rahmen und Seiten können mit dem \"Stylist\" (ab Version 2.0 Fenster \"Formatvorlagen\") erstellt und zugewiesen werden. Der \"Navigator\" erlaubt es, sich schnell im Dokument zu bewegen, es in einer Gliederungsansicht zu betrachten und den Überblick über darin eingefügte Objekte zu behalten. Innerhalb der Texte können verschiedene Verzeichnisse (Inhalt, Literatur, Stichworte, Abbildungen u. a.) erzeugt und angepasst werden. Querverweise können gesetzt werden, und mit Hyperlinks kann man über Textmarken direkt zu Textstellen springen.\n\nTexte können mehrspaltig formatiert und mit Textrahmen, Tabellen, Grafiken und anderen Elementen versehen werden. Mit Hilfe der Zeichenwerkzeuge werden innerhalb des Dokuments Zeichnungen, Legenden und andere Zeichenobjekte erstellt. Grafiken unterschiedlicher Formate können eingebunden werden, zum Beispiel Grafiken in den Formaten GIF oder PNG. Es lassen sich die gängigen Bildformate im Textverarbeitungsdokument mit dem mitgelieferten Bildbearbeitungswerkzeugen bearbeiten. Clipartsammlungen, Animationen und Klänge werden in der \"Gallery\" verwaltet und nach Themen geordnet.\n\nTextdokumente verfügen über eine integrierte Rechenfunktion, mit der Rechenoperationen oder logische Verknüpfungen ausgeführt werden. Die für die Berechnung benötigte Tabelle lässt sich in einem Textdokument erstellen.\n\nDer in \"Writer\" enthaltene HTML-Editor ist ein WYSIWYG-Editor zum Erstellen von HTML-Webseiten. Ein umfassendes Hilfesystem steht zur Verfügung, das Anweisungen für einfache und komplexe Vorgänge abdeckt.\n\nIn \"Calc\" werden Daten in Tabellen bearbeitet, analysiert, verwaltet und verdeutlicht. Daten können angeordnet, gespeichert und gefiltert werden. Die Tabellenkalkulation bietet über 450 Berechnungsfunktionen etwa aus den Bereichen elementare Mathematik, Finanzen, Statistik, Datum und Zeit. Es steht ein Funktionsassistent zum Erstellen von Formeln und komplexen Berechnungen einschließlich Vektoralgebra (Matrizen-Rechnen) zur Verfügung. Mit externen Add-ins lassen sich weitere Funktionen nachrüsten.\n\nTabellen können durch Ziehen und Ablegen aus Datenbanken übernommen und Tabellendokumente diverser Formate (OOo-intern, aber auch etwa CSV) als Datenquelle eingesetzt werden. Auch das Einbetten von Webinhalten (Tabellen aus HTML-Dokumenten) ist möglich. Bestimmte Datenbereiche können ein- oder ausgeblendet werden. Es gibt einen \"Datenpilot\" für die Analyse von Zahlenmaterial und zur Erstellung von Pivottabellen. Es besteht die Möglichkeit, in Berechnungen, die aus mehreren Faktoren bestehen, die Auswirkungen von Änderungen einzelner Faktoren zu beobachten. Zur Verwaltung umfangreicher Tabellen stehen verschiedene vordefinierte Szenarien zur Verfügung; Teil- oder Gesamtergebnisse können berechnet werden.\n\n\"Calc\" ermöglicht die Darstellung von Tabellendaten in dynamischen Diagrammen, die bei Änderung der Daten automatisch aktualisiert werden. Ein Assistent für Diagramme ist vorhanden.\n\nMit \"Impress\" können Vortragsfolien mit Animationen und verschiedenen Hintergründen erstellt werden. Präsentationen können mit Diagrammen, Zeichenobjekten, Multimedia- und vielen anderen Elementen versehen werden. Einzelnen Folien können unterschiedliche Übergangseffekte zugeordnet werden.\n\nEin Assistent für das Erstellen von Präsentationen ist ebenso enthalten wie verschiedene Vorlagen. Beim Erstellen einer Präsentation stehen mehrere Ansichten zur Verfügung. Die Folienansicht zeigt zum Beispiel die Folien im Überblick, während die Handzettelansicht zusätzlich zur Präsentation begleitenden Text enthält. Die Folien können auf dem Bildschirm automatisch vorgeführt oder manuell gesteuert werden. Der zeitliche Ablauf der Präsentation kann angepasst werden. Die Präsentationen können als Handzettel verteilt oder als HTML-Dokumente gespeichert werden.\n\nMit dem vektorbasierten \"Draw\" ist es möglich, verlustfrei skalierbare 2D-Vektorgrafiken inklusive dreidimensionaler Effekte zu erstellen. \"Draw\" verarbeitet die üblichen geometrischen Grundelemente, Splines und Bézierkurven und beherrscht grundlegendes Shading und Manipulation der Lichtquelle. Es sind Vorlagen und eine Auswahl an anpassbaren Ausgangsformen für Zeichnungselemente enthalten. Raster und Fanglinien sind optische Hilfen, die die Anordnung von Objekten in Zeichnungen erleichtern. Textmanipulation ist mit dem Modul \"FontWork\" möglich.\n\nIn \"Draw\" lässt sich die Beziehung zwischen verschiedenen Objekten mit speziellen Linien, den sogenannten \"Verbindern\", darstellen. Die Verbinder werden an die Klebepunkte der Zeichenobjekte angefügt und lösen sich auch nicht, wenn die miteinander verbundenen Objekte verschoben werden. Daher ist adaptive Bemaßung möglich, zum Beispiel für technische Zeichnungen. Mit Draw können lineare Größen anhand von Bemaßungslinien berechnet und angezeigt werden. Außerdem sind Flow-Charts, Concept-Maps und Ähnliches damit umsetzbar.\n\nTabellen (aus \"Calc\"), Diagramme, Formeln (aus dem Modul \"Math\") und andere in Apache OpenOffice erzeugte Elemente können in Zeichnungen eingefügt und umgekehrt die Grafiken in OOo-Dokumente anderen Typs eingebettet werden.\n\nZeichnungen können in unterschiedlichen Formaten gespeichert werden – darunter SVG, EPS, Windows WMF und MacPict, Adobe PDF und Shockwave SWF (nicht aber DWG/DXF). Auch Rastergrafikkonvertierung nach BMP, GIF, PNG, TIFF und JPG ist möglich.\n\nDas Importieren dieser Dateien (außer PDF und SVG, für die externe Erweiterungen vorhanden sind, sowie SWF) ist ebenfalls möglich, zusätzlich ist ein Werkzeug zur Vektorisierung (Umwandlung von Raster- in Vektorgrafik) implementiert. Der Importfilter für PDF-Dateien befindet sich zwar nicht mehr im Beta-Stadium, arbeitet jedoch noch nicht fehlerfrei.\n\nDas Datenbankmanagementsystem (DBMS) \"Base\" kann große Datenmengen speichern und für Abfragen und Berichte bereitstellen. Es verwaltet relationale Datenbanken, in denen die Daten in Tabellenform abgelegt sind. Externe Datenbanksysteme, wie beispielsweise MySQL, HSQL, SQLite oder PostgreSQL, können mittels ODBC oder JDBC angebunden werden und stehen somit ebenfalls als Datenquelle etwa für Serienbriefe zur Verfügung. Für OpenOffice.org 3.1 in Verbindung mit MySQL ab 5.1 gibt es einen nativen Datenbanktreiber, der Umweg über ODBC/JDBC entfällt somit. \"Base\" unterstützt einige Datenbankformate, wie zum Beispiel das dBASE-Format.\n\n\"Math\" dient zum Verfassen von mathematischen Formeln. Formeln werden in \"Math\" nicht ausgewertet, es ist also kein „Rechenprogramm“ (das ist das Modul \"Calc\") oder gar ein Computeralgebraprogramm, sondern ein Editor für Formelsatz, der auf einem TeX-Dialekt aufgebaut ist. Formeln werden als Objekte innerhalb eines anderen Dokuments erstellt, lassen sich also wie Bilder in den Textfluss einpassen.\n\nBeim Einfügen einer Formel in ein anderes Dokument wird \"Math\" automatisch gestartet. Die Befehle zum Aufbau der Formeln sind in einem Auswahlfenster zu finden und können dort mit der Maus angeklickt werden, um sie hinzuzufügen. Vordefinierte Symbole, Sonderzeichen und eine Basisauswahl an Funktionen stehen zur Verfügung. Es können eigene Symbole erstellt und Zeichen aus fremden Zeichensätzen übernommen werden. Eine Formel kann entweder direkt eingegeben oder in einem Befehlsfenster bearbeitet werden – die Eingaben im Befehlsfenster werden gleichzeitig im Textfenster angezeigt (WYSIWYG-Editor), einschließlich Fehlererkennung.\n\nDer \"Navigator\" und das \"Globaldokument\" sind die zwei grundlegenden Werkzeuge, die die Funktion als Gesamtpaket der Text- und weiteren Datenverarbeitung als „Office“ zusammenstellen.\n\n\nDie Benutzeroberfläche kann konfiguriert werden. Symbole und Menüs lassen sich anpassen. Auch Tastaturkürzel können festgelegt werden. Bestimmte Programmfenster und die meisten Symbolleisten sind als schwebende Fenster frei platzierbar oder können am Rand des Arbeitsbereichs angedockt werden.\n\nAssistenten zur Konvertierung von Dokumenten sind enthalten, beispielsweise können alle Microsoft-Word-Dokumente, die sich in einem Verzeichnis befinden, mit einem einzigen Vorgang umgewandelt werden. Es lassen sich auch Dateien einlesen, die von anderen Officepaketen erstellt wurden, wie etwa Lotus Notes, Corel WordPerfect. Weiterhin können andere Einzelformate, wie auch alle Vorgängerformate seit StarOffice 3.0 gelesen und meist auch geschrieben werden. Auch Konvertierer für Wiki-Syntax sowie DocBook können installiert werden. Mit der Reparaturfunktion können beschädigte Dateien oft wiederhergestellt werden, weiterhin gibt es einen automatischen regelmäßigen Abspeichermechanismus.\n\nIn der StarOffice-Basic-IDE können Makros erstellt werden. Zur Erweiterung der Programmfunktionen steht eine Vielzahl von Vorlagen, \"Add-ons\", \"Add-ins\" und Makros in den Sprachen StarOffice Basic, Python, Java und JavaScript zur Verfügung, eine Entwicklungsumgebung dafür kann installiert werden. Java-Applets können in die Dokumente eingebunden werden. Auch Plug-ins für die Websuche können ergänzt werden.\n\nFür einige Assistenten, die eingebaute HSQL-Datenbank und einige Exportfilter wird ein Java Runtime Environment (JRE) benötigt. Von diesen Funktionen abgesehen ist Apache OpenOffice auch ohne JRE lauffähig. Das kostenlose Java Runtime Environment wird bei einigen OOo-Installationspaketen mitgeliefert, lässt sich aber auch nachträglich installieren.\n\nErweiterungen in Libreoffice und Openoffice teilen sich die Dateinamenserweiterung \".oxt\".\n\nMarco Börries gründete 1984 im Alter von 16 Jahren in Lüneburg die Firma Star Division, deren Hauptprodukt das Office-Paket StarOffice wurde. Nachdem StarOffice mehr als 25 Millionen Mal verkauft worden war, erwarb Sun Microsystems 1999 die inzwischen in Hamburg ansässige Firma Star Division für einen zweistelligen Millionenbetrag. Sun bot StarOffice zunächst als kostengünstiges Konkurrenzprodukt zu Microsoft Office an. Ab Version 5.1a und später 5.2 wurde das Programm inklusive der fremdlizenzierten Bestandteile (wie z. B. Rechtschreibprüfung) als kostenlose Version zum Herunterladen und auf CDs angeboten, die Computerzeitschriften beigelegt waren. Parallel dazu wurde weiterhin eine kommerzielle Version inklusive der Datenbankanwendung Adabas vertrieben. Am 19. Juli 2000 wurde das OpenOffice.org-Projekt von Sun Microsystems öffentlich bekanntgegeben und am 13. Oktober 2000 ging die Website OpenOffice.org online, über die der Quelltext einer Vorversion von StarOffice 6.0 bezogen und von der Community bearbeitet und verbessert werden konnte. Der Quelltext war zu diesem Zeitpunkt etwa 400 MB groß und enthielt über 35.000 Dateien mit insgesamt rund 7,5 Millionen Zeilen C++-Code. Von Drittanbietern lizenzierte Komponenten waren zuvor daraus entfernt worden.\n\nBuild 638c – die erste funktionierende, frei verfügbare OpenOffice-Version – wurde im Oktober 2001 veröffentlicht. OpenOffice.org 1.0 wurde am 1. Mai 2002 und OpenOffice.org 1.1 im September 2003 herausgegeben. Im Oktober 2005 erfolgte der Schritt auf Version 2.0, im Oktober 2008 die Version 3.0 veröffentlicht.\n\nDie letzten Versionen von StarOffice, seit 2010 als Oracle Open Office bezeichnet, basieren auf OpenOffice.org, werden aber von Sun Microsystems/Oracle um die aus dem OpenOffice.org-Quellcode entfernten Komponenten (darunter Rechtschreibkorrektur, Thesaurus, Datenbankmodul Adabas D und Cliparts) erweitert. Aufgrund der Lizenzierung kann der OpenOffice.org-Code für das nicht quelloffene Oracle Open Office verwendet werden. Sun Microsystems hatte beim Projektstart OOo unter die GNU Lesser General Public License (LGPL) und unter die Sun Industry Standards Source License (SISSL) gestellt. Seit September 2005 steht OpenOffice.org nur noch unter der LGPL, nachdem Sun bekanntgegeben hatte, die SISSL in Zukunft nicht mehr zu nutzen.\n\nSeit Herbst 2007 gibt es mehr als 80 Sprachversionen. Allein die Version 3 von OpenOffice.org wurde bereits über 100 Millionen Mal heruntergeladen.\n\nNach der am 27. Januar 2010 erfolgten Sun-Übernahme durch die Oracle Corporation wird OpenOffice.org in einer eigenen Abteilung weitergeführt. Die freie Community-Version existiert weiterhin.\n\nAm 28. September 2010 gab die neu gegründete „The Document Foundation“ bekannt, dass sie das Projekt unter dem Namen LibreOffice weiterführen wolle und sich damit von Sun/Oracle komplett löst. Auf der Website der Stiftung wurde erklärt, dass man darauf hoffte, dass Oracle auch die Rechte am Namen OpenOffice.org an die Stiftung übergeben werde. Dazu ist es jedoch nicht gekommen.\n\nIn der Folgezeit verließen viele Entwickler OpenOffice und arbeiteten stattdessen an LibreOffice weiter.\n\n2015 kristallisierte sich heraus, dass OpenOffice den Wettlauf gegen LibreOffice verloren hatte. Zu viele Entwickler waren zum Konkurrenten LibreOffice abgewandert, es gab kaum neue OpenOffice-Versionen und im Vergleich zu LibreOffice nur wenig neue Features. Zudem war die Entwicklung von OpenOffice geprägt von Streitigkeiten zwischen den Entwicklergruppen der beiden Programme, vor allem wegen der unterschiedlichen Lizenzierung der beiden Softwarepakete.\n\n2016 hat Dennis E. Hamilton, der Vorsitzende des OpenOffice Project Management Committees, öffentlich das Aus des Projekts diskutiert. An der Entwicklung beteiligten sich zu diesem Zeitpunkt nur noch sechs Entwickler, die kaum die Zeit aufbringen konnten, auch nur die Sicherheitslücken zu beheben, geschweige denn neue Features zu entwickeln oder regelmäßig neue Versionen zu veröffentlichen.\n\nOpenOffice.org 1.0 wurde am 1. Mai 2002 veröffentlicht. Augenfällige Änderung gegenüber StarOffice 5.2 war das Weglassen des integrierten Desktops. Auch die im StarOffice 5.2 enthaltenen Anwendungen Mail-Client, Organizer und die Datenbank Adabas D fielen weg. Es kamen drei Aktualisierungen heraus, wobei die letzte im April 2003 unter der Versionsnummer 1.0.3.1 erschien. Version 1.0.3.1 war offiziell die letzte Version für Windows 95, Version 1.1.5 die letzte Version für Windows NT 4.0.\n\nIm Oktober 2003 wurde die Version 1.1 freigegeben. Auch bei dieser Version kamen in unregelmäßigen Abständen fehlerkorrigierte Versionen heraus. Wichtige Änderungen in der Version 1.1 waren:\n\nAm 14. September 2005 erschien OpenOffice.org 1.1.5. Diese letzte Aktualisierung unter der Versionsnummer 1 enthält neben zahlreichen Fehlerkorrekturen als hauptsächliche Neuerung Importfilter für die OASIS OpenDocument-Formate, die ab OpenOffice.org 2.0 als Standardformat genutzt werden. Am 4. Juli 2006 erschien das Sicherheitspatch \"1.1.5secpatch\", welches das unaufgeforderte Ausführen von Makro-Befehlen (BASIC) in manipulierten OpenOffice.org-Dateien unterbindet.\n\nDie Entwicklung an Version 2 von OpenOffice.org begann bereits im Juli 2003. Es wurden zwei Beta-Versionen und mehrere Snapshots unter der Versionsnummer 1.9 veröffentlicht. Die endgültige Version wurde am 20. Oktober 2005 freigegeben. Wichtigste Neuerungen waren die eigene Datenbankanwendung (Base), das neue Dateiformat OpenDocument und eine sich den Desktop-Einstellungen anpassende Oberfläche. Außerdem wurde die Benutzerführung optimiert, um Benutzern von Microsoft Office einen möglichst einfachen Umstieg auf OpenOffice.org zu ermöglichen. Wichtige Änderungen in der Version 2.0 sind:\n\nImpress wurde von Grund auf neu programmiert und bietet jetzt unter anderem mehr Diashow-Übergänge und Animationseffekte. Der PDF-Export wurde erweitert: Hyperlinks sind jetzt möglich, das Format für Formularübermittlung ist auswählbar, Notizen können exportiert werden, Vorschaubilder und mehr Stufen für die Komprimierung von Bildern. Mit der neuen Wortzählfunktion können jetzt markierte Textabschnitte gezählt werden. Am 15. Dezember 2005 wurde die Version 2.0.1 veröffentlicht. Diese erste Aktualisierung für OpenOffice.org 2.0 behob eine Reihe von Fehlern. Außerdem wurde eine Serien-Mail-Funktion integriert. OpenOffice.org 2.0.2 erschien am 8. März 2006. Diese Version ersetzt das bisher für die Rechtschreibprüfung verwendete MySpell durch Hunspell. Weitere Neuerungen sind Icons für KDE und Gnome sowie ein Importfilter für das MS-Word 2/5-Textformat. Die englische Version von OpenOffice.org 2.0.3 wurde am 29. Juni veröffentlicht. Die deutsche Version erschien am 3. Juli 2006. Neben der Beseitigung von Fehlern wurden auch neue Funktionen implementiert, unter anderem Unterstützung von x86-64-Plattformen, eine Aktualisierungsfunktion und die optionale Unterstützung der Grafikbibliothek Cairo. Letztere verspricht unter anderem Antialiasing in Präsentationen. Auffälligster Fehler war der Export in PDF-Dateien, der mitunter (oft) ungültige („Unbekannter Token“) und zum Teil nicht lesbare PDF-Dateien erzeugte, was aber erst beim Öffnen mit einem PDF-Reader sichtbar wurde. Am 13. Oktober 2006 wurde die Version 2.0.4 veröffentlicht. Diese Version erhielt einen Exportfilter für LaTeX und PDF-Verschlüsselung. Sie bildet den Abschluss der 2.0-Produktreihe.\n\nVersion 2.1 von OpenOffice.org erschien am 12. Dezember 2006. In ihr wurden neu implementiert: ein Aktualisierungssystem, der überarbeitete Schnellstarter sowie die Verbesserung der Exportfunktion von HTML-Dateien aus Calc. Die Version enthielt einen auffälligen Fehler, durch den in Textdokumenten nach Seitenumbrüchen das Inhaltsverzeichnis versetzt und umformatiert wurde. Kapitel 7.2 des offiziellen Installationshandbuches beschreibt zudem die Installation (geht bis Version 2.1) unter Windows NT 4.0. Mit einer aktualisierten Systemdatei kann OOo 2.0.2 unter Windows 95 gestartet werden. Unter diesen Windowsversionen gilt die OOo-Unterstützung als experimentell und wird nicht vom Support abgedeckt.\n\nAm 28. März 2007 erschien die Version 2.2 mit erweiterten Vista-Funktionen, erweitertem Umgang mit Extensions und ebenfalls erweitertem PDF-Export. Mit dieser Version ändert OpenOffice.org die Zeitabstände zwischen Aktualisierungen mit neuen Funktionen von drei auf sechs Monate. Auffälligster Fehler war in CALC unsichtbarer Text beim Bearbeiten von Notizen von Zellen. Dieser Fehler wurde mit der Version 2.2.1 behoben. OpenOffice.org 2.2.1 erschien am 12. Juni 2007 und brachte einige Fehlerkorrekturen – neue Funktionen wurden nicht integriert.\n\nVersion 2.3 wurde am 17. September 2007 veröffentlicht. Die Installationsdatei ist mit minimal 100 MB wesentlich größer als die der Vorgängerversionen. Neben Fehlerkorrekturen sind auch neue Funktionen integriert. In dieser Version ist das Diagrammmodul Chart neu programmiert worden. Verbesserungen gibt es bei der Geschwindigkeit der Darstellung, bei Regressionsdarstellungen in Diagrammen und der 3D-Funktionalität; auch der Diagrammassistent ist überarbeitet. Für das Datenbankmodul ist ein neuer Reportgenerator verfügbar. Writer beherrscht nun teilweise das MediaWiki-Format (Tabellen, Zeichenformatierung, Weblinks) als Exportoption. Weiterhin wurden Calc, die Rechtschreibprüfung und der HTML-Export bei Präsentationen verbessert. OpenOffice.org 2.3.1 erschien am 4. Dezember 2007 und brachte einige Fehlerkorrekturen, neue Funktionen gab es nicht.\nDie Version 2.4.0 erschien am 27. März 2008. Verbessert wurden in Calc die Formulareingabe und das Sortieren von Spalten im Datenpilot mittels Drag and Drop. Der Datenpilot erlaubt jetzt einen Drilldown aus Ergebniszellen. Mit der Komponente Chart sind Diagramm-Beschriftungen besser positionierbar. In Impress können Hintergrundgrafiken per Kontextmenü eingebunden werden. Folientitel werden beim PDF-Export als Lesezeichen abgespeichert. Die Statusleiste in Writer zeigt die Sprachversion des Absatzes an. Blockmarkierungen in Textdokumenten sind ab sofort möglich. Base unterstützt unter Windows zusätzlich Datenbanken im Access-2007-Format. Die Sicherheitsfunktionen wurden um ein Master-Passwort für Internet-Verbindungen erweitert. Der Zugriff auf WebDAV-Server über HTTPS ist ebenfalls möglich geworden. Der PDF-Export bietet PDF/A-1 (ISO 19005-1) zur Langzeitarchivierung. Beim automatischen Suchen von Programmaktualisierungen wird auch geprüft, ob neue Versionen installierter Erweiterungen vorliegen. Die Hilfefunktion wurde ebenfalls erweitert.\n\nDie Version 2.4.1 erschien am 10. Juni 2008. Es handelt sich dabei um eine reine Fehlerkorrektur-Version ohne neue Funktionen; insbesondere wurde damit ein Sicherheitsproblem behoben. Ende Oktober 2008 erschien mit Version 2.4.2, und im September 2009 mit Version 2.4.3 die letzte Aktualisierung der Version 2.4.x, welche ebenfalls reine Fehlerkorrektur-Versionen waren. Es war die letzte Version für Windows 98, Windows 98 SE und Windows Me.\n\nSeit Dezember 2009 wird der 2.x-Versionszweig von der Entwickler-Community nicht mehr kostenfrei weiterentwickelt.\n\nVersion 3.0 wurde am 13. Oktober 2008 veröffentlicht. Diese Version enthält gegenüber den Versionen 2.x zahlreiche neue Funktionen und ist erstmals auch nativ, also ohne X11, für macOS mit Aqua-Unterstützung erhältlich. OpenOffice 3 enthält eine Überarbeitung von Calc sowie erweiterte Kommentarfunktionen. Ab Version 3.0.1 wurde der \"Extension Manager\" für freie Programmerweiterungen mit StarOffice 9 PU 1 harmonisiert, um dieselben Erweiterungen verwenden zu können. Zusätzlich sind die Importfunktionen um die Dateiformate aus Microsoft Office 2007 für Text- und Tabellendokumente, das „Office Open XML“, ergänzt worden.\n\nAb der Version 3.1.1 werden Grafikobjekte mit Hilfe von Antialiasing in höherer Qualität dargestellt. Die Version 3.2 brachte eine erhöhte Stabilität und Geschwindigkeit. Außerdem unterstützt OpenOffice.org ab Version 3.2 Graphite- und OpenType-Schriften, Kommentarfunktionen für Impress und Draw und neue Blasendiagramme für Calc. Zusätzlich wurde die Startgeschwindigkeit (Kalt- und Warmstart) erheblich erhöht.\n\nDie ebenfalls vorgesehene Erweiterung um einen Personal Information Manager (PIM) wurde auf eine spätere Version verschoben. Sun Microsystems betätigt sich zu diesem Zweck im Lightning-Projekt der Mozilla-Foundation. Ziel ist die Integration des Mail-Clients Mozilla Thunderbird mit Adressbuch und Kalender als Groupware-Client in OpenOffice.org.\n\nDie Version 3.4.0 ist am 8. Mai 2012 veröffentlicht worden. Der Name wurde von \"OpenOffice.org\" auf \"Apache OpenOffice\" (ohne „.org“) geändert. Sie ist die erste Version nach der Übernahme durch die Apache Software Foundation. Die Version 3.4.1 ist am 23. August 2012 veröffentlicht worden.\n\nVersion 4.0.0 wurde am 23. Juli 2013 veröffentlicht. Die auffälligste Neuerung dieser Version stellt eine Seitenleiste dar. Diese soll die Arbeit an Breitbildmonitoren erleichtern, weil dadurch die sonst oben liegenden Bedienelemente an die Seite verlagert werden. Darüber hinaus wurde die Unterstützung für drei weitere Sprachen geschaffen, 500 Programmfehler bereinigt, die Kompatibilität mit Microsoft Office erhöht, die Arbeit mit Grafiken verbessert und vieles mehr. Die Unterstützung für Windows 2000 entfiel.\n\nVersion 4.0.1 wurde am 1. Oktober 2013 veröffentlicht. Einige Programmfehler wurden beseitigt, die Geschwindigkeit der Speicherung von .XLS-Dateien erhöht und zusätzliche Sprachen werden unterstützt.\n\nVersion 4.1.0 wurde am 29. April 2014 veröffentlicht. Es wurden neue Funktionen hinzugefügt und Verschiedenes geändert: Unterstützt werden nun Kommentare/Anmerkungen zu Textbereichen, iAccessible2, In-place-Bearbeitung von Eingabefeldern sowie eine interaktive Funktion zum Zuschneiden. Sie setzt macOS 10.7 oder höher voraus, die Version OS 10.6 (Snow Leopard) wird damit nicht mehr unterstützt.\n\nVersion 4.1.1 wurde am 21. August 2014 veröffentlicht. Es handelt sich um eine Aktualisierung, die einige Fehler bereinigt und verbesserte Übersetzungen enthält. Neu hinzugekommen ist Katalanisch.\n\nVersion 4.1.2 wurde am 28. Oktober 2015 veröffentlicht. Das WebDAV- Management zur Bereitstellung von Dateien im Internet wurde überarbeitet und die Integration mit SharePoint bereitgestellt. Der PDF-Export wurde überarbeitet und Fehlerbereinigungen in Writer, Calc, Impress/Draw und Base durchgeführt.\n\nVersion 4.1.3 wurde am 12. Oktober 2016 veröffentlicht. Es ist ein Release zur Fehlerbeseitigung, welches Sicherheitsprobleme beseitigt, Wörterbücher aktualisiert und einige sonstige bekannte Fehler korrigiert.\n\nVersion 4.1.4 wurde am 19. Oktober 2017 veröffentlicht. Es ist ein Wartungs-Release mit einigen wenigen wichtigen Bugfixes, Sicherheitsfixes, aktualisierten Wörterbüchern und Buildfixes.\n\nVersion 4.1.5 wurde am 30. Dezember 2017 veröffentlicht. Es ist ein Wartungs-Release, das darauf abzielt, eine Reihe sogenannter Regressionsbugs (3) zu korrigieren und das aktuellste Sprachwörterbuch für Englisch zu liefern.\n\nVersion 4.1.6 wurde am 18. November 2018 veröffentlicht. Es ist ein Wartungs-Release, das einige Fehler korrigiert (13) und das Wörterbuch für (UK-)Englisch aktualisiert.\n\nApache OpenOffice lässt sich unter aktuellen Windows-, macOS-, Linux-, Unix- und eComStation-Systemen nutzen und ist damit weitgehend plattformunabhängig.\n\nDie aktuelle Version 4.1.6 von OpenOffice lässt sich unter Windows XP, Windows Vista, Windows 7, Windows 8 und Windows 10 installieren.\n\nDie Mac-OS‑X-Portierung von OpenOffice.org erfordert mindestens Mac OS X Tiger (10.4, 2005) und bis OpenOffice.org Version 2.4.3 auch die X11-Bibliotheken. Ab Version 3.0 wurde sowohl die PowerPC- als auch die Intel-x86-Architektur nativ unterstützt (separate Versionen, nicht als Universal Binary); Version 3.2.1, gleichzeitig die letzte PowerPC-Version des Office-Paketes, ist als „OpenOffice.org X11“ wie zuvor als X11-Version und als „OpenOffice.org Aqua“ nativ für die grafische Benutzeroberfläche von Mac OS X verfügbar; letztere setzt Mac OS X Tiger 10.4.11 (November 2007) voraus.\n\nDie letzte unter Mac OS X Snow Leopard (10.6, 2009) lauffähige OpenOffice-Version ist 4.0.1.; ab Version 4.1 wird Mac OS X Lion (10.7, 2011) oder neuer vorausgesetzt.\n\nBei den verbreiteten Linux-Distributionen (z. B. openSUSE, Fedora, Ubuntu) wurde OpenOffice durch LibreOffice ersetzt. OpenOffice kann aber von der Webseite des Apache-Projektes heruntergeladen werden, wird dann aber nicht automatisch mit Updates versorgt.\n\nFür Solaris wird mindestens Solaris 8 auf der SPARC- oder x86-Prozessorplattform vorausgesetzt.\n\nFür OS/2 und eComStation gab es bis zur Version GA 3.2 spezielle Versionen von OpenOffice.org von Serenity Systems International und Mensys BV im Rahmen kostenpflichtiger Supportverträge. Apache OpenOffice 4.1.6 wird für diese Betriebssysteme über die Firma \"bww bitwise works\" distribuiert.\n\nApache weist darauf hin, dass es Portierungen von Drittanbietern gibt. Insbesondere ist eine Portierung auf Android ab Version 2.3 erhältlich, sowie verschiedene Anpassungen an bestimmte Linux-Varianten, die nicht offiziell unterstützt werden. Bedingt durch die Quelloffenheit des Programmcodes und der freien Lizenzen hängt es von den Bedürfnissen und Kompetenzen externer Entwickler ab, ob und welche weiteren Betriebssysteme mit entsprechenden Anpassungen unterstützt werden.\n\nInzwischen gibt es einige Projekte, die OpenOffice.org an besondere Bedürfnisse oder Verwendungszwecke angepasst haben:\n\n\nIm Gegensatz zu den vorgenannten Projekten ist \"LibreOffice\" eine Abspaltung von OpenOffice.org. Sie wird koordiniert durch die \"Document Foundation\" und seit September 2010 entwickelt. Ziel ist es, im Rahmen einer unabhängigen Stiftung die Arbeit aus der vorangegangenen zehnjährigen Entwicklung von OpenOffice.org weiterzuführen und neue Beiträge zentral durch die Stiftung zu verwalten. Die Stiftung wurde von OpenOffice.org-Projektmitgliedern aus Unzufriedenheit darüber gegründet, dass die Unterstützung durch die Entwicklungsabteilung von Oracle (vormals Sun Microsystems) für OpenOffice.org immer spärlicher ausgefallen war und die Markenrechte unklar waren. Nachdem Oracle eine Beteiligung an der Document Foundation abgelehnt und OpenOffice.org wie bisher weiterführen wollte, war die Übertragung der Namensrechte an die Document Foundation und eine Zusammenführung von OpenOffice.org und LibreOffice zunächst unwahrscheinlich geworden. 2011 übergab Oracle das OpenOffice.org-Projekt samt Namens- und Logorechten an die Apache Software Foundation, was neue Chancen für einen Codeaustausch zwischen beiden Projekten brachte.\nDas Dateiformat von OpenOffice.org wurde von der Organization for the Advancement of Structured Information Standards (OASIS) als Basis für das neue offene Austauschformat OpenDocument verwendet, welches das Standardformat von OpenOffice.org ab Version 2.0 ist. Die XML-Dateien sind gepackt und belegen deshalb sehr wenig Speicherplatz. Die Dokumentinhalte werden im Java-Archive-Format gespeichert, einer ZIP-Datei mit speziellen Einträgen. Die Dateiendung eines Java-Archivs ist normalerweise „.jar“, jedoch werden für OpenDocument-Dateien Dateiendungen des Musters „.od?“ verwendet, wobei an der Stelle des ‚?‘ je nach Dokumententyp ein spezifischer Buchstabe steht, z. B. ‚t‘ für Writer-Dokumente: „.odt“.\n\nEs kann mit jedem üblichen Packprogramm entpackt werden. Die eigentliche Textinformation (Datei \"content.xml\") kann danach mit jedem Texteditor angesehen und verändert werden. Zum Beispiel kann man Programme schreiben, die Formulare automatisch mit Inhalten einer Datenbank ausfüllen. Außerdem ist sichergestellt, dass auch in vielen Jahren noch uneingeschränkt auf diese Dateien zugegriffen werden kann; das ist gerade im kommerziellen und behördlichen Einsatz wegen der langen Aufbewahrungsfristen für Unterlagen ein nicht zu unterschätzender Vorteil. Die Europäische Union plant, das OASIS-Dateiformat als einheitliches Standarddatenformat für ihre Dokumente einzusetzen.\n\nIm Mai 2006 wurde „OASIS-OpenDocument 1.0“ zum ISO-Standard (ISO 26300) erklärt.\n\nIn OpenOffice.org 1.0 und 1.1 wurden Dokumente standardmäßig im eigenen XML-basierten Dateiformat mit der Dateiendung „.sx?“ gespeichert. Dieses Dateiformat ist nicht identisch mit dem OpenDocument-Format, das in diesen OOo-Versionen noch nicht unterstützt wurde. Erst mit OpenOffice.org 1.1.5 konnten OpenDocument-Dateien zumindest geöffnet und bearbeitet werden – das Speichern musste im alten Dateiformat geschehen. OpenOffice.org 2.0 kann alle Dateiformate früherer Versionen verlustfrei lesen und schreiben, dazu zählen auch die alten StarOffice-Dateiformate mit der Dateiendung „.sd?“.\n\nAb OpenOffice.org 2.0.3 und dem darin neu eingeführten Rechtschreibprogramm Hunspell wird mit der deutschsprachigen Version von OpenOffice.org auch die Hunspell-Variante des deutschen Wörterbuchs igerman98 mitgeliefert. In früheren Versionen musste dieses teilweise noch aufgrund nicht kompatibler Lizenzen nachträglich hinzugefügt werden. Das Rechtschreibprogramm Hunspell, das ein direkter Nachfolger des vorher verwendeten Myspell ist, ermöglicht eine deutlich bessere Unterstützung von Sprachen, die Kompositabbildung (Wortzusammensetzungen) erlauben. Die Wörterbücher für eine Kontrolle in weiteren Sprachen können ab Version 3.0 als „Extensions“ aus dem Internet bezogen werden und wie alle Programmerweiterungen automatisch auf Aktualisierungen überprüft werden.\n\nEine Grammatikprüfung wird bislang nicht angeboten. Mit der freien Erweiterung \"LanguageTool\" lässt sie sich für einige Sprachen, darunter Deutsch, nachrüsten.\n\nDie Brockhaus-Tochter Brockhaus-Duden Neue Medien (BDNM) bot bis Ende 2013 die proprietäre Erweiterung „Duden-Korrektor“ für OpenOffice.org an, die eine Rechtschreib-, Stil-, und Grammatikprüfung sowie eine automatische Silbentrennung und einen Thesaurus umfasste. Das Produkt wurde jedoch im Zuge der Aufgabe des Geschäftsbereichs Sprachtechnologie eingestellt und ist mit aktuellen Versionen von Apache OpenOffice und LibreOffice nicht mehr kompatibel.\n\nMit dem \"Apache OpenOffice Software Development Kit\" (SDK) können Entwickler das Office-Paket um weitere Funktionen erweitern oder externe Programme einbetten. Im SDK sind alle notwendigen Tools und Anleitungen enthalten. Die englischsprachige Dokumentation beschreibt die Konzepte der API und Komponententechnik UNO \"(Universal Network Objects)\", seit 2.0 auch Common Language Infrastructure (CLI). Es kann in den Sprachen StarOffice Basic, C, C++, Python und Java programmiert werden, in der Standardinstallation von Apache OpenOffice sind jedoch nur StarBasic und Python als portable Laufzeitumgebungen vorhanden. Das SDK steht unter der Apache-Lizenz Version 2 und kann für Windows, macOS und Linux von den Projektseiten kostenlos geladen werden.\n\nAuch ein API-Plugin für NetBeans ist verfügbar.\n\nOpenOffice.org wurde nur selten auf neuen Rechnern vorinstalliert. Im Sommer 2007 hat das amerikanische Unternehmen Everex Computer mit OOo 2.2 ausgestattet und in Nordamerika über den Einzelhandel vertrieben. Im Frühjahr 2008 wurden einige Modelle des Eee PC (mit Linux) in Deutschland und Österreich mit OOo ausgeliefert.\n\nStudien aus den Jahren 2003 bis 2010 kamen auf Zahlen zwischen 3 und 15 Prozent für den internationalen Marktanteil und 5 Prozent für den deutschen Marktanteil im professionellen Umfeld. Im Oktober 2005 wurde eine strategische Partnerschaft von Google Inc. und Sun Microsystems geschlossen. Sie sollte unter anderem die Verbreitung von OpenOffice.org fördern. In einer Studie eines Web-Analytics-Dienstes im Januar 2010 wurde der OpenOffice-Marktanteil in Deutschland auf 21,5 Prozent bestimmt. Dazu wurden die installierten Office-Programme von über einer Million Internetnutzern in Deutschland bestimmt. Damit liegt Deutschland bei der OpenOffice-Marktdurchdringung im internationalen Vergleich im vorderen Bereich.\n\nIn einigen Firmen und öffentlichen Verwaltungen, wie etwa in München (LiMux-Projekt) und Wien (Wienux-Projekt), wurde zeitweilig OpenOffice.org eingesetzt. Seit 2009 wurde das Projekt in Wien nicht weiter verfolgt; 2017 beschloss auch der Stadtrat der Stadt München, zukünftig anstatt der Open-Source-Lösung einen Windows-Basis-Client mit Produkten aus dem Hause Microsoft zu nutzen. Die Open-Source-Lösungen hinkten den kommerziellen Produkten bisweilen im Funktionsumfang hinterher und wiesen nicht diejenige Kompatibilität mit anderen Produkten auf, die gewünscht sei. Ein großer Anwender war beispielsweise auch die französische Gendarmerie, die im Jahre 2005 etwa 70.000 Desktoprechner von Microsoft Office zu OpenOffice.org migriert hat.\n\nIn den wichtigsten Linux-Distributionen wurde OpenOffice mittlerweile durch LibreOffice ersetzt.\n\nDie Arbeit am Quelltext wird vorrangig von den Entwicklern von Oracle (ehemals Sun Microsystems) übernommen. Weitere Unternehmen, die Entwickler stellen, sind beispielsweise IBM, Novell, Intel, Red Hat und Red Flag.\n\nDie vielen Unterprojekte sind in drei Kategorien aufgeteilt:\nIm Gesamtprojekt bildet der \"Community Council\" das oberste Organ. Er legt unter anderem die Ziele des Projektes fest.\n\nDas Programm wird oft auch kurz \"OpenOffice\" genannt; dieser Begriff ist bzw. war jedoch in einigen Ländern markenrechtlich durch Dritte für andere Produkte geschützt. Das Projekt und das Programm nannten sich deshalb \"OpenOffice.org\" (Abkürzung \"OOo\") und seit 2012 (ab Version 3.4.0) \"Apache OpenOffice\" (Abkürzung \"AOO\"), um dieses Problem zu umgehen. Zwischenzeitlich war die Notwendigkeit hinzugekommen, \"Apache OpenOffice\" von dem damit nicht identischen, inzwischen eingestellten \"StarOffice\" (zwischenzeitlich \"Oracle Open Office\") zu unterscheiden.\n\nSun Microsystems hält die Urheberverwertungsrechte an Apache OpenOffice. Entwickler unterschreiben eine \"Sun Microsystems Inc. Contributor Agreement\" (SCA) genannte Vereinbarung (Nachfolger des früher verwendeten \"Joint Copyright Assignment\"), womit Sun ein gemeinsames Verwertungsrecht an Beiträgen erhält, die die Entwickler an Apache OpenOffice leisten. Die Übertragung der Nutzungsrechte wird von einigen Entwicklern, zum Beispiel vom Novell-Mitarbeiter Michael Meeks, als problematisch angesehen. Durch das SCA wird Sun Microsystems unter anderem in die Lage versetzt, mögliche Urheberrechts- bzw. Lizenzverletzungen rechtlich verfolgen zu lassen und die Lizenz festzulegen. So wurde für die Version 1 von OpenOffice.org eine duale Lizenzierung aus GNU Lesser General Public License (LGPL) und der Sun Industry Standards Source License (SISSL) verwendet, für die Version 2 wurde nur noch die LGPL (Version 2) genutzt und die Version 3 des Programms ist unter den Bedingungen der LGPL Version 3 veröffentlicht worden.\n\nDie Rechte an der Wort- und Wort-Bild-Marke „OpenOffice.org“ hält in den USA und in der EU Oracle America, Inc. In manchen Ländern werden aber von Dritten Rechte am Markennamen „OpenOffice.org“ oder ähnlich klingenden geltend gemacht, so dass OpenOffice.org dort nicht unter seinem eigentlichen Namen in den Markt eintreten kann. Deshalb heißt zum Beispiel in Brasilien die Software „BrOffice.org“.\n\n\n"}
{"id": "3819", "url": "https://de.wikipedia.org/wiki?curid=3819", "title": "Personal Computer", "text": "Personal Computer\n\nEin Personal Computer (engl., zu dt. „persönlicher Rechner“, kurz PC) ist ein Mehrzweckcomputer, dessen Größe und Fähigkeiten ihn für den individuellen persönlichen Gebrauch im Alltag nutzbar machen; im Unterschied zu vorherigen Computermodellen beschränkt sich die Nutzung nicht mehr auf Computerexperten, Techniker oder Wissenschaftler. Das Konzept geht zurück auf eine Idee aus den 1970er Jahren, begründet von Hackern. Die leichte Bedienbarkeit und ein für Privathaushalte erschwinglicher Preis waren wichtige Voraussetzungen für das Konzept, das seit 1976 technisch umgesetzt wird. Erst Geräte dieser Art lösten das aus, was der Journalist Steven Levy als Computerrevolution bezeichnet. Demgegenüber werden Geräte aus einer früheren Zeit vereinzelt bereits Personal Computer genannt, obgleich sie nicht in das Konzept passen.\n\nEin PC ist ein Mikrocomputer, in Abgrenzung zu einem Minirechner oder Großrechner. Er tritt beispielsweise als Desktop-, Notebook- oder Tablet-Computer in Erscheinung und kann unter einem beliebigen Betriebssystem laufen, wie beispielsweise Windows, iOS oder Unix. Das Spektrum reicht vom Bereich des Heimcomputers bis hin zum typischen Arbeitsplatzrechner. Überdurchschnittlich leistungsfähige Arbeitsplatzrechner für rechen- und speicherintensive Anwendungen werden als Workstation bezeichnet; ihr Preis kann ein Vielfaches eines PCs betragen.\n\nObwohl bereits in den 1970er-Jahren üblich, wurde der Begriff „Personal Computer“, vor allem dessen Kurzform „PC“, ab 1981 im Sprachgebrauch zunehmend und exklusiv mit dem IBM Personal Computer und dessen IBM-kompatiblen PC-Nachbauten verknüpft. Das war dem Marketing von IBM mit seiner erfolgreichen Werbung geschuldet. Die Verknüpfung bezog sich auf die darin verbaute x86-Prozessor-Familie und der darauf laufenden Betriebssysteme DOS und Windows. Darüber hinaus wird der Begriff vereinzelt mit der Bauart eines x86er-Desktop-PCs assoziiert, was jedoch im Widerspruch steht zur Bauart und den Bezeichnungen alternativer x86er-PC-Geräte, wie beispielsweise dem Microsoft Tablet-PC.\n\nEin (aktueller) PC verfügt normalerweise über die folgenden Komponenten:\nAußerdem (nicht im Bild):\n\nStreng genommen zählen Peripheriegeräte wie Monitor, Tastatur, Maus und Drucker nicht zwangsläufig zu den Komponenten des Personal Computers.\n\nDer Journalist Steven Levy veröffentlichte 1984 das weltweit erste Buch, das sich unter anderem mit der frühen Geschichte des Personal Computers auseinandersetzt und dabei die Entwickler und deren Motivation in den Mittelpunkt stellt. Es trägt den Titel \"„Hackers – Heroes of the Computer Revolution“\". Darin beschreibt er eine Gruppierung von Hackern – eine Art stark ausgeprägter Technikenthusiasten – die sich in den 1970er Jahren für die Idee eines persönlichen Computers begeistern konnten. Ihnen ging es darum, Computer im Alltagsleben zu integrieren, sie für jedermann öffentlich zugänglich zu machen, bis hin zu dem damals visionären Ziel, einer breiten Masse die Nutzung universell einsetzbarer persönlicher Computer zu ermöglichen.\n\nUm dieses Ziel zu erreichen, musste der persönliche Computer einige Voraussetzungen erfüllen: So war eine praktikable Größe wichtig, die es einer durchschnittlichen Person erlaubt, ihn transportieren und beispielsweise auf einem Schreibtisch installieren zu können. Er musste für Privathaushalte verfügbar, erschwinglich und universell einsetzbar sein. Entscheidend war eine Handhabung, die für die breite Masse geeignet ist. Das machte eine intuitive und universelle Datenein- und -ausgabe erforderlich, die weit hinaus ging über die Kippschalter und Leuchtdioden der bislang üblichen Computer des unteren Preissegments. Damit die Nutzung nicht nur Elektronik-Fachleuten vorbehalten blieb, mussten Bausätze auch über fertig verlötete Komponenten verfügbar sein, die der Computerhändler oder sogar der Benutzer leicht zusammenfügen kann. Über die jeweilige Bedienungsanleitung hinaus sollte es keine spezielle Schulung erfordern, sowohl den Computer als auch eine darauf installierte Anwendungssoftware zu betreiben. Darüber hinaus sollte der Endbenutzer die Möglichkeit erhalten, seinen persönlichen Computer frei programmieren zu können. Erst Geräte dieser Art lösten das aus, was Levy in seinem oben genannten Buch als Computerrevolution bezeichnet.\n\nDiese Idee wurde von der damals vorherrschenden Industrie als absurd abgetan. So soll Thomas J. Watson, der frühe Chef von IBM, 1943 erklärt haben: „Ich glaube, es gibt einen Weltmarkt für vielleicht 5 Computer“. Wenn auch nicht in diesem Ausmaß, folgten in den 1970er Jahren Unternehmen wie Texas Instruments, Fairchild, IBM und DEC im Grunde noch immer diesem Dekret. Von einem Mitarbeiter auf die Entwicklung eines persönlichen Computers angesprochen, wies DEC-Chef Ken Olsen 1977 diesen Vorschlag mit der Begründung von sich, dass er sich keine Privatperson vorstellen könne, die einen solchen Computer haben wolle.\n\nDer Idee eines öffentlich zugänglichen Computers, der Vorstufe auf dem Weg zum persönlichen Computer, widmete Ted Nelson 1974 ein Buch mit dem Titel „Computer Lib“, welches zum Standardwerk unter den damaligen Verfechtern dieser Idee wurde. Lee Felsenstein gründete bereits im selben Jahr das „Community Memory“-Projekt, welches über öffentliche Terminals in Plattenläden und Bibliotheken den Zugriff auf einen Computer ermöglichte. Das Projekt war für die damalige Zeit wegweisend und hatte den praktischen Nutzen eines schwarzen Bretts, auf dem man per ADD einen beliebigen Beitrag einfügen und mit FIND finden konnte.\n\nAufgrund der Größe und Kosten der Computer der 1950er und 1960er Jahre, die meist ganze Räume füllten oder als \"Minicomputer\" etwa schrankgroß waren, konnten diese kaum einem einzelnen Menschen persönlich zugewiesen werden. Das änderte sich erst allmählich, als seit 1961 TTL-Chips und seit 1971 die Mikroprozessoren auf den Markt kamen und die bis dahin vorherrschenden Kernspeicher durch Halbleiterspeicher ersetzt wurden. Für Privathaushalte und damit für den Bau eines PCs erschwinglich wurden solche Komponenten seit Mitte der 1970er Jahre.\n\nRund um den von Fred Moore und Gordon French im März 1975 gegründeten Homebrew Computer Club in der Region von San Francisco, der Westküste der Vereinigten Staaten, trafen sich technikbegeisterte Menschen, Hacker, wie Lavy schreibt. Angefangen von praktischen Projekten und Entwicklungen, bis hin zur Geburt einer vollkommen neuen Industrie im Silicon Valley, haben sie die Entwicklung des persönlichen Computers entscheidend vorangetrieben. Sie machten bezüglich des PCs immer wieder mit Konzepten und praktischen Entwicklungen auf sich aufmerksam. Viele Computer-Pioniere gingen aus ihren Reihen hervor; Mitglieder dieses Vereins gründeten zahlreiche Computerunternehmen. Der Homebrew Computer Club wird daher als „Schmelztiegel für eine ganze Branche“ bezeichnet.\n\nLevy setzt in seinem oben genannten Buch die Grenze zum Personal Computer dort, wo er das damals visionäre Ziel erreicht sieht, den die von ihm interviewten Entwickler im Personal Computer sahen. Aus dieser Sichtweise heraus gilt der im April 1976 veröffentlichte Apple I als erster Personal Computer der Welt, 1977 gefolgt vom Commodore PET, dem Tandy TRS-80 Model 1 und dem Apple II.\n\nDie oben genannten Voraussetzungen für einen PC sind jedoch nicht festgeschrieben; der Begriff \"Personal Computer\" hat keine feste Definition. Die Benennung des Apple I als ersten Personal Computer ist daher nicht unumstritten.\n\nEs gibt je nach Gewichtung tatsächlich mehrere Computer, die jeweils als erster Personal Computer der Welt bezeichnet werden. Vernachlässigt man beispielsweise den Punkt der einfachen für die breite Masse geeigneten Handhabung und setzt auch die universelle Einsatzmöglichkeit des Computers nicht zwingend voraus, so ist der für Privathaushalte erstmals erschwingliche Simon aus dem Jahr 1949 der erste Personal Computer; ein auf Relais basierender Lerncomputer, der ausschließlich als Selbstbausatz zu erwerben war. Soll es ein rein elektronischer Computer mit integrierten Schaltkreisen sein, der als komplett montiertes Gerät ausgeliefert wurde, so gilt der Kenbak-1 von 1971 als der erste PC. Wird ein in Serie produzierter Mikrocomputer vorausgesetzt, der also als zentrale Recheneinheit (CPU) einen Mikroprozessor nutzt, dann ist es der Micral N im Jahr 1973. Einigen gilt auch der Altair 8800 von 1975 als erster Personal Computer der Welt, auch wenn er sich nicht viel unterschied vom Micral N. Allen bis hier hin genannten Computern gemein ist die Voraussetzung, das ein Personal Computer für Privathaushalte erschwinglich sein muss. Doch selbst dieser Punkt erhebt keinen Anspruch auf Allgemeingültigkeit; Menschen die der einfachen Handhabung eine besonders hohe Gewichtung geben, aber weder dem Preis noch der Verfügbarkeit eine Bedeutung beimessen, bezeichnen den Xerox Alto von 1973 als den ersten PC der Welt.\n\nDie Bedeutung des Wortes liegt in seinem Gebrauch. Für Levys Abgrenzung spricht, dass der PC umgangssprachlich nicht mit einem Gerät assoziiert wird, das man per Kippschalter und Lämpchen bedient, sondern per Tastatur und Monitor. Der für 666 US-Dollar erhältliche Apple I war unbestritten der erste für Privathaushalte erschwingliche Personal Computer, der ab Werk mit einem Betriebssystem und allen benötigten Anschlüssen ausgestattet war, um ihn auf moderne Weise per Tastatur und Monitor zu betreiben. Nicht im Lieferumfang enthalten waren einige für den Betrieb wichtige Komponenten; die Tastatur, das Gehäuse und Netzteil mussten separat erworben werden und ohne ein heimisches Fernsehgerät als Monitorersatz und einen Kassettenrecorder als Datenspeicher war er nicht arbeitsfähig. Im Januar 1977 wurde der weltweit erste PC dieser Art mit einer kompletten betriebsbereiten Ausstattung vorgestellt: der für 795 US-Dollar erhältliche Commodore PET 2001. Der vier Jahre ältere Xerox Alto wurde zwar auch per Tastatur und Monitor betrieben und verfügte sogar über eine Maussteuerung, jedoch wird er im Unterschied zu diesen beiden Geräten als Workstation klassifiziert aufgrund seines hohen Preises. Eine Workstation kann ein vielfaches eines Personal Computers kosten. Seine Herstellungskosten lagen bei 12.000 US-Dollar (was auf die heutige Kaufkraft bezogen einem Wert von US-Dollar entspricht); der führende Entwickler Charles P. Thacker schätzt, dass der Verkaufspreis im Jahr 1973 sogar bei 40.000 US-Dollar gelegen hätte (damals wurde der Xerox Alto nicht offiziell zum Verkauf angeboten). Bereits vor dem Xerox Alto existierten andere grafische Workstations mit Tastatur und Monitor, wie beispielsweise der IMLAC PDS-1 von 1970 und der IBM 2250 aus dem Jahr 1964.\n\nIhre einfache Handhabung und der geringe Preis machten Personal Computer seit 1976 für durchschnittliche Privatanwender weitgehend tauglich und attraktiv. Erst der überragende Verkaufserfolg solcher Geräte durch andere (zum Teil branchenfremde, zum Teil neu gegründete) Unternehmen sollte die vorherrschende Computerindustrie dazu veranlassen, sich der Idee des persönlichen Computers anzunehmen, eigene Produkte zu entwickeln und seit 1981 auf den Markt zu bringen. Dabei war das Marketing von IBM mit der Werbung für ihren IBM Personal Computer, kurz IBM-PC, derart erfolgreich, dass der Begriff „Personal Computer“ häufig mit dieser Marke in Verbindung gebracht wurde, obwohl gerade dieser PC preislich grenzwertig zum ursprünglichen PC-Konzept war. Dafür setzten sich die wesentlich preiswerteren Nachbauten, die \"„IBM-PC-kompatiblen Computer“\", als eine der erfolgreichsten Plattformen für den persönlichen Computer durch; die heute marktüblichen PCs mit Windows-Betriebssystem und x86-Prozessoren beruhen auf der stetigen Weiterentwicklung des damaligen Entwurfs von IBM. Der erfolgreichen Werbung von IBM ist es zu verdanken, dass häufig der IBM-PC 5150 von 1981 als erster Personal Computer der Welt bezeichnet wird. Dabei war IBM nicht einmal das erste Unternehmen, das ihr Produkt als \"Personal Computer\" bewarb; in der Werbung erstmals als Personal Computer bezeichnet wurde der Tischrechner HP-9100A von 1968, gefolgt vom HP-9830 von 1972 und dem Altair 8800 von 1975.\n\n1949 stellte Edmund C. Berkeley mit \"Simon\" den ersten Computer für den Heimgebrauch vor. Er bestand aus 50 Relais und wurde für 300 US-Dollar in Gestalt von Bauplänen vertrieben, von denen in den ersten zehn Jahren über 400 Exemplare verkauft wurden. Er ist ein für damalige Verhältnisse kompakter digitaler programmierbarer und weitgehend automatisierter Computer, der für Privathaushalte erschwinglich ist. Rechnet man die Bauteile hinzu, konnte er damals für rund 500 US-Dollar gebaut werden (was auf die Kaufkraft des Jahres bezogen einem Wert von US-Dollar entspricht). Für seinen Betrieb benötigt man über die Bedienungsanleitung hinaus keine spezielle Schulung. Damit erfüllt er bereits viele Voraussetzungen für einen persönlichen Computer und gilt daher manchen Menschen als erster PC der Welt. Mit seinen fünf Bedientasten, dem Lochstreifen als Programmablaufspeicher und den fünf Lämpchen als Ausgabeeinheit, die Zahlen von 0 bis 4 darstellen konnten, entspricht dieses Gerät jedoch technisch nicht dem, was man heute unter einem Personal Computer versteht. Der \"Simon\" ist speziell als Lerncomputer entwickelt worden, der dem Anwender die grundlegende Funktionsweise eines Computers näher bringen sollte. Ähnlich verhält es sich mit dem auf Drehscheiben basierenden GENIAC von 1955, dem analogen, auf Röhren basierenden Heathkit EC-1 von 1959 und dem Relais-Computer Minivac 601 aus dem Jahr 1961.\n\nDer erste frei programmierbare Tischrechner der Welt, der \"Programma 101\" von der Firma Olivetti, erschien 1965 für einen Preis von 3.200 US-Dollar. Drei Jahre später brachte die Hewlett-Packard Company mit dem \"HP-9100A\" ein programmierbares Rechengerät auf den Markt, das im Vergleich zum Programma 101 bereits mehr Möglichkeiten der Anzeige und Programmierung bot, aber mit 4.900 UD-Dollar rund das Doppelte eines damaligen durchschnittlichen Bruttojahresgehaltes kostete. Dieser Rechner wurde in einer Werbeanzeige erstmals in der Literatur als Personal Computer bezeichnet, obgleich er weder preislich noch technisch dem heutigen Verständnis eines PCs entspricht. Bemerkenswert ist, dass die Leistung beider Tischrechner ohne die Verwendung von integrierten Schaltkreisen erbracht wurde.\n\n1967 erschien ein Buch mit dem Titel „How To Build a Working Digital Computer“ von den Autoren Edward Alcosser, James P. Phillips und Allen M. Wolk. Das Buch beschreibt, wie man einen einfachen Computer aus Alltagsgegenständen bauen kann, wie beispielsweise aus Büroklammern für Schalter und einer Konservendose für den Trommelspeicher. Für 1.000 US-Dollar vertrieb das Unternehmen COMSPACE 1969 eine professionell zusammengebaute Version dieses Lerncomputers unter dem Namen \"Arkay CT-650\".\n\nMit dem \"IMLAC PDS-1\" erschien 1970 eine vernetzte Grafik-Workstation des Herstellers Imlac Corporation of Needham, einem kleinen Unternehmen aus Massachusetts, USA. Bemerkenswert war, dass dieses Unternehmen eine sehr effiziente Konstruktion entwickelte, die es erlaubte, dass sie ihren Computer bereits für 8.300 US-Dollar zum Kauf anbieten konnten (zum Vergleich kostete die technisch in etwa vergleichbare \"IBM 2250\" aus dem Jahr 1964 noch 280.000 US-Dollar). Die PDS-1 war ein Vorreiter auf dem Weg zu einem grafischen Personal Computer, in Teilen vergleichbar mit dem weitaus teureren Xerox Alto aus dem Jahr 1973.\n\nIm September 1971 erschien der von John Blankenbaker entwickelte \"Kenbak-1\" für 750 US-Dollar. Obwohl die ersten Mikroprozessoren seit 1971 verfügbar waren, verwendete sein Computer keinen Mikroprozessor; Blankenbaker konstruierte die Maschine auf einer einzigen Platine mit TTL-Chips. Der Kenbak-1 wird mitunter als erster Personal Computer der Welt bezeichnet. Da er kein Betriebssystem erhielt, mussten sämtliche Aktionen in einem reinen Maschinencode programmiert werden über eine Reihe von Tasten und Schaltern, die auf der Frontseite untergebracht waren. Die Ausgabe bestand aus einer Reihe von Lichtern auf der Rückseite. Geräte wie dieses waren für durchschnittliche Privatanwender weitgehend untauglich und auch kaum attraktiv; der Kenbak-1 passt daher weder in das Konzept des persönlichen Computers noch entspricht er technisch dem, was man umgangssprachlich unter einem Personal Computer versteht.\n\nDer 1972 veröffentlichte \"HP-9830\" war der erste Tischrechner mit einem im ROM integriertem BASIC-Interpreter. Im Unterschied zum HP-9100A und dem Programma 101 verfügt er über eine vollständige alphanumerische Tastatur und ein alphanumerisches Display, wodurch er eine Brücke schlug zwischen einem üblichen Tischrechner und einem All-in-One-Desktop-Computer. Zwar besitzt dieses Gerät nur eine einzige Bildschirmzeile mit lediglich 32 Zeichen. Zudem ist er mit 5.975 US-Dollar (was auf das Jahr bezogen einem Wert von US-Dollar entspricht) für Privathaushalte kaum erschwinglich. Dennoch kommt er dem heutigen Verständnis zum Begriff Personal Computer schon recht nahe. Daher gilt er manchen Menschen als erster PC der Welt.\n\nDer \"Micral N\" war ein weiterer Vorläufer des Personal Computers; der erste in Serie hergestellte Computer seiner Art mit einem Mikroprozessor, in diesem Fall ein Intel 8008. Er wurde in Frankreich von André Truong Trong Thi und François Gernelle entwickelt und dort seit 1973 für 8.500 FF verkauft (umgerechnet 1.750 US-Dollar, was nach heutiger Kaufkraft einem Wert von US-Dollar entspricht). Erfolgte die Datenein- und -ausgabe zunächst per Kippschalter und Lämpchen, wurde er seit 1974 gegen Aufpreis mit einer Tastatur und einem Bildschirm ausgeliefert; Festplatten waren ab 1975 erhältlich. Bedientechnisch entsprach dieser Computer seither dem modernen Verständnis eines PCs, jedoch preislich nicht. Ebenfalls im Jahr 1973 erschien der \"Scelbi-8H\", ein weiterer Mikrocomputer mit einem Intel 8008.\n\nMit dem \"HP-65\" kam 1973 für 795 US-Dollar der erste vollständig programmierbare Taschenrechner der Welt auf den Markt. Da er jedoch über kein alphanumerisches Display verfügt, kann er als anspruchsvoller programmierbarer Rechner betrachtet werden, der technisch weiter von einem PC entfernt ist, als beispielsweise der ein Jahr ältere HP-9830.\n\nDas Unternehmen Xerox PARC stellte 1973 ihren \"Xerox Alto\" der Weltöffentlichkeit vor, ein etwa kühlschrankgroßes Gerät. Mit einer schreibmaschinenähnlichen Tastatur, einer 3-Tasten-Maus, einer zusätzlichen kleinen 5-Tasten-Akkordtastatur für besondere Befehle, einem objektorientierten Betriebssystem, einem Bildschirm mit grafischer Benutzeroberfläche (engl. \"graphical user interface\", kurz \"GUI\") und einer Ethernet-Schnittstelle war er wegweisend für den künftigen Personal Computer. Diese Workstation war jedoch als wissenschaftliches Gerät gedacht; sie war weder für den privaten Gebrauch erschwinglich, noch in dieser Zeit für den Handel verfügbar und wurde erst ab 1978 zu einem Preis von 32.000 US-Dollar zum Kauf angeboten (nach heutiger Kaufkraft wären das US-Dollar).\n\nDer \"Mark-8\" erschien 1974 und war ein weiterer Mikrocomputer, betrieben von einem Intel 8008. Vom Mark-8 wurde ausschließlich der Bauplan und die Platine verkauft; er war somit lediglich als Selbstbausatz erhältlich.\n\nMit dem \"Altair 8800\" des Anbieters MITS kam 1975 ein in Serie produziertes Gerät auf den Markt, das ebenfalls als \"Personal Computer\" bezeichnet wird und als Bausatz für 397 US-Dollar, als Komplettgerät für 695 US-Dollar zu erwerben war. Innerhalb der frühen Szene rund um den \"Homebrew Computer Club\" erfreute sich der Altair 8800 großer Beliebtheit und diente den Mitgliedern des Clubs als Kernstück für eigene Erweiterungen. Zukunftsweisend war die Ausstattung mit einem Bus-Stecksystem für Erweiterungskarten nach dem S-100-Bus-Standard. Mit seinen Kippschaltern als Eingabeeinheit und Leuchtdioden als Ausgabeeinheit entspricht jedoch auch dieses Gerät technisch nicht dem, was man heute unter einem Personal Computer versteht; zur Benutzung von Altair BASIC oder CP/M als Kommandozeilen-Betriebssystem musste ein Text-Terminal über die serielle Schnittstelle (das RS-232-Interface) angeschlossen werden. Ähnlich war es mit dem im selben Jahr erschienenen \"KIM-1\" des Unternehmens MOS Technology, der immerhin schon eine 24-Tasten-Eingabeeinheit im Taschenrechnerformat zur direkten Eingabe von HEX-Code besaß sowie über eine 6-stellige 7-Segment-LED-Anzeige als Ausgabeeinheit verfügte.\n\nDer 1975 veröffentlichte \"IBM 5100\" wartete mit seiner schreibmaschinenähnlichen Tastatur, einem integrierten Monitor und einem Kassettenlaufwerk für wechselbare Datenspeicher auf. Rein technisch konnte er alles vorweisen, was man heute unter einem Personal Computer versteht. Jedoch war der Preis von damals 9.000 bis 20.000 US-Dollar ( bis US-Dollar auf das Jahr bezogen) für Privathaushalte deutlich zu hoch, weshalb er ebenfalls nicht in das Konzept des Personal Computers passte.\n\nSteve Wozniak (in der Szene bekannt als \"The Woz\") war ein prominentes Mitglied des Homebrew Computer Clubs. Im April 1976 stellte er der Öffentlichkeit seinen Computer vor, der dem Altair 8800 technisch weit überlegen war. Als erstes Gerät der Welt war er mit 666 US-Dollar für Privathaushalte erschwinglich und entsprach zugleich den modernen bedientechnischen Vorstellungen eines persönlichen Computers: Sein Computer verwendete eine schreibmaschinenähnliche Tastatur als Eingabeeinheit und einen Bildschirm (zunächst in Form eines umfunktionierten Fernsehgerätes) als Ausgabeeinheit. Als einziges Peripheriegerät gab es ein Kassetten-Interface, mit dem sich in Kombination mit einem herkömmlichen Kassettenrecorder Programme auf Audiokassetten speichern und von diesen wieder laden ließen. Apple war eines der Unternehmen, die aus dem Homebrew Computer Club hervorgingen, wobei Steve Wozniak neben Steve Jobs und Ronald Wayne einer der Gründer ist. Sein Computer wurde zwar vor der Unternehmensgründung entwickelt, aber dann dort in Serie produziert und unter dem Namen \"Apple I\" verkauft. Als Einplatinencomputer wurde er in Form einer komplett bestückten Platine ausgeliefert und vom Händler oder Endbenutzer um ein Netzteil, Gehäuse und eine Tastatur ergänzt, ehe er am heimischen Fernseher betrieben werden konnte.\n\nDas Nachfolgemodell, der Apple II, wurde nun auch in kompletter Ausführung ausgeliefert mit einem Gehäuse, Netzteil, Tastatur und Monitor, später sogar mit einer Maus. Gleichzeitig war er der letzte industriell hergestellte PC, der vollständig von einer einzelnen Person, Steve Wozniak, entworfen wurde. Er wurde im April 1977 in den USA vorgestellt und für einen Preis von 1.298 US-Dollar angeboten (das entspricht einem Wert von US-Dollar auf das Jahr bezogen). Bei seiner Markteinführung hatte er acht freie Steckplätze des 8-Bit-Apple-Bus-Systems, mit denen er durch Einsetzen der entsprechenden Erweiterungskarte für unterschiedliche Anwendungen (z. B. Textverarbeitung, Spiele, Steuerungstechnik) genutzt werden konnte. Diese Eigenschaft eines Computers, der also durch Steckplätze individuell an die Wünsche des Konsumenten angepasst werden kann, gilt heute als Grundeigenschaft eines PCs. Außerdem konnten mit diesem Computer bereits Farben dargestellt und Töne wiedergegeben werden. Die Apple-II-Baureihe war ein offenes System, das heißt, alle wesentlichen Konstruktionsdetails wurden veröffentlicht.\n\nDer weltweit erste industriell hergestellte PC in kompletter Ausführung (inklusive Gehäuse, Netzteil, Tastatur, Monitor und Massenspeicher in Form einer Datasette) wurde im Januar 1977 vorgestellt: der Commodore PET 2001, der für 795 US-Dollar über den Ladentisch ging. Im August desselben Jahres folgte der Tandy TRS-80 Model 1 für 599 US-Dollar. Von den Leistungsdaten her waren beide Geräte dem Apple II ähnlich, hatten aber keine Steckplätze für Erweiterungskarten, keine Farbdarstellung und keine Tonausgabe. Der PET verfügte über den in der professionellen Messtechnik verbreiteten (parallelen) IEC-Bus, was zur Folge hatte, dass er in Forschung und Industrie Verbreitung fand.\n\nAm 12. August 1981 wurde der erste IBM-PC 5150 vorgestellt. Er bewegte sich preislich an der Obergrenze der handelsüblichen PCs. In der Grundausstattung konnte er für 1.565 US-Dollar erworben werden (ohne Diskettenlaufwerke und Monitor, dafür mit TV-Anschluss) oder für 3.005 US-Dollar in kompletter Ausführung (dies wären heute US-Dollar). In der maximalen Ausbaustufe mit mehr Speicher und Farbgrafik wurde er für 6.000 US-Dollar angeboten. IBM nutzte ihre damalige Marktführung für (Großrechner-)Datenverarbeitungsanlagen und schaffte es, dass ihr IBM-PC als Arbeitsplatzcomputer in zahlreichen Unternehmen eingesetzt wurde.\n\nDas Gerät war mit dem Intel-8088-Prozessor ausgestattet und verfügte über ein 8-Bit-ISA-Bussystem. Auch die folgenden Modelle wurden mit Prozessoren von Intel ausgerüstet. Der bereits ein Jahr vor dem 8088-Prozessor (4,77–9,5 MHz Takt; interne CPU-Wortbreite 16 Bit; System-Datenbus 8 Bit) von Intel vorgestellte 8086-Prozessor (6–12 MHz Takt; CPU-Wortbreite 16 Bit; System-Bus 16 Bit) sorgte dafür, dass sich für die Serie die Abkürzung „x86-Architektur“ etablierte.\n\nDer IBM-PC wurde von 1981 bis 1995 ausschließlich mit dem Betriebssystem von IBM, PC DOS, vertrieben, das von Microsoft an IBM lizenziert worden war. Die 1981 begonnene Zusammenarbeit endete 1985. Beide Unternehmen entwickelten danach das Betriebssystem getrennt weiter, achteten jedoch auf gegenseitige Kompatibilität. Das Betriebssystem MS-DOS von Microsoft gab es seitdem nur auf Computern, die in der Bauweise jenen von IBM entsprechen.\n\nDas Unternehmen IBM legte die Grundkonstruktion seines PC offen und schuf einen informellen Industriestandard; es definierte damit die bis heute aktuelle Geräteklasse der \"„IBM-PC-kompatiblen Computer“\". Zahlreiche preiswerte Nachbauten und Fortführungen der IBM PCs durch andere Unternehmen machten die Plattform sowohl am Arbeitsplatz als auch im Heimbereich sehr erfolgreich.\n\nIm Februar 1984 wurde der IBM Portable Personal Computer vorgestellt, eine frühe Vorstufe der Laptops, später Notebooks genannt (als Klasse der tragbaren Personal Computer).\n\nMit TV-Ausgang und Tonausgabe kamen ab den 1980er-Jahren weitere Geräte als Heimcomputer auf den Markt. Die meistverkauften Modelle waren der Commodore C64 und die Geräte der Amiga-Reihe, wie auch verschiedene Ausführungen des Atari ST.\n\nIm deutschen Sprachraum wurde in den 1980er-Jahren das englische Wort \"personal\" (persönlich) mitunter inkorrekt mit dem deutschen \"Personal\" (Arbeiter, Angestellte) assoziiert. Eine Ableitung von \"Personal Computer\" hin zu einer professionellen Nutzung entsprechender Geräte wurde hierzulande daher gebräuchlich. So wurden in den Medien Geräte mittlerer Leistung manchmal als \"„reicht an die Leistung eines Personal Computers [nicht] heran“\" klassifiziert, obgleich es sich bei solchen Geräten tatsächlich auch um Personal Computer handelte. Da die Amiga-Reihe und der Atari ST zu Heimcomputerpreisen die Leistung der IBM PC XT und AT übertrafen und teilweise die Gehäuseform der professionellen Geräte verwendeten, verschwand die irrtümliche Unterscheidung zum Ende der 1980er-Jahre.\nDa IBM kein Monopol auf die verwendeten Komponenten hatte (mit Ausnahme des BIOS), konnte Compaq 1983 den ersten zum IBM-PC kompatiblen Computer auf den Markt bringen. Vor allem in Ostasien schufen Unternehmen eine Reihe von Nachbauten, in Deutschland waren es Unternehmen wie Commodore und später Schneider. Der sich so entwickelnde Markt führte durch den Konkurrenzkampf zu sinkenden Preisen und verstärkter Innovation.\n\nDie Stückzahlen waren zu Anfang noch bei weitem nicht mit den heutigen vergleichbar. Die Marktsituation Ende 1983 laut für professionelle Mikrorechner (ohne Heimcomputer):\n\nMarktpositionen der sechs wichtigsten Anbieter von professionellen Mikros per Ende 1983:\n\nIm amerikanischen Weihnachtsgeschäft 1984 spielten Personal Computer erstmals eine signifikante Rolle. Jedoch hatten sowohl IBM als auch Apple zu viele Geräte produziert und klagten im Frühjahr 1985 über ein enttäuschendes Ergebnis. Viele Händler blieben auf den PCs sitzen, und Kunden klagten, „sie könnten nicht viel mit den Maschinen anfangen.“ Ein Apple-Händler schenkte sogar jedem, der einen Rechner kaufte, ein italienisches Fahrrad dazu.\n\nAuch Apple-Computer wurden teils nachgebaut, aber das Unternehmen konnte sich (mit deutlich geschrumpftem Marktanteil) behaupten. Die Apple-II-Linie wurde Anfang der 1990er-Jahre eingestellt. Heute wird nur noch die Macintosh-Reihe hergestellt. Apple und Sun (Unix) sind die beiden einzigen Hersteller, die Hardware und Software (Betriebssystem und Anwenderprogramme) selbst entwickeln und auch zusammen vermarkten.\n\nDie meisten anderen Hersteller, wie etwa Commodore und Schneider, verschwanden Anfang der 1990er-Jahre weitgehend vom Markt oder wandten sich wieder anderen Geschäftsfeldern zu (Atari). Die aktuelleren PC-Modelle von IBM, wie der PC 300GL, blieben weitgehend unbekannt und gingen auf dem Markt neben den Produkten anderer Hersteller unter. Ähnlich erging es dem Versuch von IBM, den Markt mit der Personal-System/2-Reihe und dem Betriebssystem OS/2 zurückzuerobern.\n\nIm Privatbereich wurden Heimcomputer und PC zunächst zum Experimentieren, Lernen und Spielen benutzt. Zunehmend wurden sie auch in Bereichen wie Textverarbeitung, Datenbanken und Tabellenkalkulation eingesetzt und fanden so Eingang in den betrieblichen Alltag.\n\nDie Leistungsfähigkeit von Personal Computern nahm seit ihrer Entstehung stetig zu (Moore’sches Gesetz). Neben den Aufgaben der Textverarbeitung und Tabellenkalkulation wurde der Multimedia-Bereich zu einem der Hauptanwendungsgebiete. Um auch den Anforderungen neuester PC-Spiele gerecht zu werden, gibt es sogenannte Gaming-PCs, die mit hoher Rechenleistung und sehr leistungsfähigen Grafikkarten ausgestattet sind.\n\nBei modernen PCs kommt seit 2006, unabhängig vom eingesetzten Betriebssystem, praktisch durchweg Hardware auf Basis der x86-Architektur zum Einsatz, die historisch auf den IBM Personal Computer von 1981 bzw. dessen sogenannte IBM-kompatible Weiterentwicklungen zurückgeht. Von den anderen Computerarchitekturen für Einzelplatzrechner waren bis Anfang 2006 die PowerPC-Modelle von Apple erhältlich, bevor auch Apple diese durch x86-Modelle ersetzte. PowerPC-Rechner von Apple werden vom Betriebssystem seit Mac OS X Snow Leopard nicht mehr unterstützt.\n\nAls Betriebssysteme werden neben dem marktführenden Windows hauptsächlich unixoide Betriebssysteme eingesetzt, vor allem Linux und BSD. Auch das Apple-Betriebssystem ist seit Einführung von Mac OS X ein Unix-Derivat, das im Gegensatz zu den verschiedenen Linux-Distributionen und freien BSD-Betriebssystemen ab der Version 10.5 als UNIX zertifiziert ist (\"siehe auch\" Liste von Betriebssystemen).\n\nEntsprechend der technischen Entwicklung wandelten sich auch die Bauformen mit der Zeit.\n\nDer erste IBM PC war wortwörtlich ein Desktop-Computer, er und seine Zeitgenossen von anderen Herstellern hatten Gehäuse im Querformat und standen auf dem Arbeitstisch. Auf ihnen stand wiederum der Monitor mit einer Bildschirmdiagonalen von damals nur 10 bis 13 Zoll. Als diese Desktop-PCs mit der Zeit noch etwas größer wurden und nicht nur auf dem Schreibtisch immer mehr im Weg waren, sondern die auch langsam größer werdenden Monitormodelle auf dem Computer ergonomisch immer ungünstiger standen, ging man zu \"neben\" dem Monitor stehenden PC-Gehäusen im Hochformat über, sogenannten \"Tower\"-Modellen. Letztere differenzierten sich im Anschluss in \"Big Towers\", \"Midi-Towers\" und weitere Abstufungen. Je nach Höhe des Towers und Vorlieben des Benutzers stehen viele heutige Personal Computer auch unter oder neben dem Tisch.\n\nSchon seit Anfang der 1980er-Jahre bemühte man sich parallel dazu, \"tragbare Computer\" zu entwickeln. Damit wird üblicherweise ein Gerät mit der Technik und den Ausmaßen eines Desktop-Computers bezeichnet, dessen zumeist kofferförmiges Gehäuse jedoch zum regelmäßigen Transport ausgelegt ist. Die ersten Geräte dieser Art waren 1981 der Osborne-1 und der Kaypro, sowie 1983 der SX64. Sie benötigten für den Betrieb zwingend einen Stromnetzanschluss; an Batteriebetrieb war noch nicht zu denken, vor allem weil diese Modelle noch mit integrierten Bildröhren ausgestattet waren, die viel Energie benötigten.\n\nDie Geräteklasse der \"mobilen Computer\" (mit Akkubetrieb) wurden seit der Verfügbarkeit preisgünstiger LCD-Anzeigen entwickelt. 1981 erschien der GRiD Compass 1100 und eröffnete die Klasse der noch recht schweren Schoßrechner (Laptop genannt); der erste kommerziell erfolgreiche Laptop erschien 1986 mit dem IBM PC Convertible. Die Bezeichnung Notebook wird tendenziell für die mittelkleinen und leichteren Ausführungen der mobilen Computer benutzt, während der Begriff Netbook für ein deutlich kleineres Gerät ohne optisches Laufwerk verwendet wird, dessen Tasten auch zu klein für die Verwendung im Zehnfingersystem sein können.\n\nEine spezielle Bauform eines Personal Computers, die zu den \"Handheld-Geräten\" zählt, ist der Tablet-PC. Auch wenn schon vorher Tablets existiert haben, erlangten sie 2002 mit Microsofts Windows XP Tablet PC Edition größere Aufmerksamkeit; der Durchbruch für diese Geräteklasse erfolgte jedoch erst 2010 mit der Veröffentlichung von Apples iPad.\n\nDie Zahl der weltweit verkauften PCs ist im Jahr 2013 weiter zurückgegangen, insgesamt wurden ca. 316 Millionen Stück verkauft, davon knapp 26 Millionen in Europa (genauer: EMEA – die Wirtschaftsregion, die Europa, den Mittleren Osten und Afrika umfasst). Insgesamt sank der Verkauf gegenüber 2012 um ca. 10 %. Dieser Rückgang ist u. a. auf die weitere Verbreitung von Tabletcomputern und Smartphones zurückzuführen.\n\nIn der Vergangenheit wurde die Mehrzahl der verkauften PCs als Arbeitsplatzrechner in Wirtschaft und Verwaltung eingesetzt, aber auch viele Privathaushalte verfügten über PCs.\n\nBesonders in aufstrebenden Ländern („Emerging Markets“) haben die Menschen heute anstelle eines PCs mit Internetzugang als erste Geräte eher ein Smartphone für die Kommunikation und einen Tabletcomputer als Computer.\n\nSpätestens seit ca. 2005 sind durch PCs verursachte Umweltauswirkungen anerkannt und werden erforscht.\nDie Umweltauswirkungen sind durch die hohen Absatzzahlen und vielfältige Schadstoffe in der Produktion erheblich, sie belasten die Umwelt insbesondere rund um Produktionsanlagen und durch den Material- und Energieverbrauch. Das Gebiet in der Informatik, das sich mit Umweltaspekten von PCs und Computerhardware im Allgemeinen beschäftigt, ist die Green IT.\n\nEiner Studie aus dem Jahr 2003 zufolge braucht man für die Herstellung eines Computers samt 17-Zoll-Röhrenmonitor 240 Liter fossile Brennstoffe. Geht man bei einem Gesamtgewicht des Systems – inklusive Röhrenmonitor – von rund 24 Kilogramm aus, entspricht das dem Zehnfachen seines Eigengewichts. Zusätzlich werden rund 22 kg Chemikalien und 1.500 kg Wasser benötigt.\n\nUm gegenwärtig (Stand 2013) seinen PC möglichst sparsam betreiben zu können, empfiehlt sich die Beachtung gewisser Normen der Industrie. Für Netzteile ist dies heute die „80-PLUS“-Zertifizierung in Bronze, Silber, Gold Platinum oder Titanium nach der ENERGY-STAR-Richtlinie der US-Umweltbehörde EPA.\n\nEin einzelner PC in \"Desktop-Ausführung\" brauchte über lange Zeit weitgehend konstant um die 50 W an elektrischer Leistung. Dieser Wert hielt sich etwa bis zur Einführung des Intel-Pentium-III-Prozessors Ende der 1990er-Jahre. In der Folgezeit stiegen diese Werte rapide auf weit über 100 W alleine für den Prozessor und teilweise über 200 W für den kompletten Rechner an. Eine Trendwende gab es 2004, als der Prozessorhersteller AMD für seinen AMD Athlon 64 erstmals bisher nur bei Notebooks eingesetzte Funktionen zur dynamischen Änderung des Prozessortaktes einsetzte. Durch diese heute in sämtlichen Prozessoren verfügbare Funktion ist der Stromverbrauch zumindest ohne eine dedizierte Grafikkarte und ohne aufwändige Berechnungen wieder gefallen.\n\nDeutliche Abweichungen davon ergeben sich, wenn der Prozessor tatsächlich ausgelastet wird, und noch wesentlich mehr bei der Verwendung einer dedizierten Grafikkarte, die – auch wenn nur ein normaler Desktop darzustellen ist – bereits zwischen 10 und 80 W benötigt.\n\nLaptops und Notebooks, die mobil sein sollen und auf Akkubetrieb ausgelegt sind, versuchen, möglichst sparsam mit der elektrischen Energie umzugehen, um möglichst lange Akkulaufzeiten zu erreichen. Hier werden je nach Geschwindigkeitsanforderung und Auslastung zwischen ca. 10 W und (z. B. für mobile 3D-Grafik) deutlich über 60 W erreicht. Die Werte sind über die Zeit weitgehend konstant; Verbesserungen bei der Akkutechnik werden hauptsächlich in eine Verkleinerung der Gehäuse und nur zu kleinen Teilen in eine Verlängerung der Laufzeit gesteckt. Auch Industrie-PCs verwenden oft Laptop-Technik, das jedoch weniger aufgrund des Stromverbrauchs, sondern um auf bewegliche Teile in Gestalt von Lüftern verzichten zu können und so die mechanische Robustheit zu erhöhen.\n\nDie noch kleineren Einplatinencomputer, UMPCs oder Netbooks benötigen mit teilweise unter 10 W noch weniger elektrische Leistung, wobei hier jedoch meist Zugeständnisse bei der Rechenleistung gemacht werden müssen.\n\nAufgrund des hohen Ressourcenaufwandes bei der Herstellung ist es nicht sinnvoll, allein mit Hinblick auf eine Energieeinsparung ein sparsames Neugerät zu kaufen, da im Vergleich zum Energieverbrauch bei Herstellung und Entsorgung der Energieverbrauch beim Gebrauch vergleichsweise gering ist. Der durch die Neuproduktion anfallende zusätzliche Energieverbrauch kann – wenn das überhaupt bei normalem privaten Gebrauch möglich ist – nur nach etlichen Jahren durch die geringere Leistungsaufnahme kompensiert werden.\n\nAufwändige Berechnungen wie 3D-Bilder in Computerspielen, Bildberechnungen von Grafikprogrammen oder Videobearbeitung erhöhen den Energiebedarf auf 300 W. Leistungsstarke PCs mit sehr schnellen Prozessoren kommen auf Werte bis zu 425 W. Hochleistungsgrafikkarten benötigen jeweils weitere bis zu 275 W, so dass bei zwei Grafikkarten unter Volllast des Systems Leistungsaufnahmen von knapp 1.000 W möglich sind.\n\nPersonal Computer bestehen aus den unterschiedlichsten Komponenten, hauptsächlich Elektronik und Metall. Sie werden in Deutschland nach der Elektronikschrottverordnung von den Herstellern über Erfassungsstrukturen zurückgenommen. Besitzer sind verpflichtet, die Geräte getrennt vom Restmüll den Erfassungsstellen zuzuführen. Die Rücknahme ist in Deutschland kostenfrei. Im Zuge der Verschrottung werden heute viele Elektronikkomponenten der Wiederverwendung zugeführt, um seltene Erden zu retten.\n\nVeraltete, noch funktionsfähige PCs oder Bauteile können auch verkauft oder an Bastler oder Bedürftige weitergegeben werden – z. B. im Rahmen des Projektes linux4afrika. Oft werden alte Geräte auch illegal in Drittweltländer verfrachtet, wo, oft unter Vernachlässigung von Arbeits- und Umweltschutzmaßnahmen, die wertvollen Metalle extrahiert werden und der Rest auf Deponien abgelagert wird (z. B. Guiyu in China oder Agbogbloshie in Ghana).\n\nAllgemein ist ein Personal Computer ein US-Produkt, da der größte Teil beziehungsweise der größte Kostenfaktor aus Importprodukten von Herstellern aus den Vereinigten Staaten stammt, gefolgt von Taiwan. So ist in den meisten PCs ein Intel- oder AMD-Prozessor verbaut. Auch die gebräuchlichsten Grafikkarten stammen von US-Unternehmen wie Nvidia, Intel oder AMD.\n\nBei den PC-Mainboards hingegen führt die Republik China (Taiwan) die Produktion an, mit Produkten von Asus, Gigabyte Technology und Micro-Star International inklusive der intern meistverbauten Soundchips des Anbieters Realtek. Bei der eigentlichen Herstellung der Boards ist jedoch die ebenfalls taiwanesische Firma Foxconn führend.\n\nMarktführer der externen Soundlösungen ist hingegen Creative Technology (Singapur) mit der Soundblaster-Serie.\n\nBei den Festplatten (HDDs) führen hingegen US-Anbieter wie Seagate Technology und Western Digital den Markt an.\n\nBei den Netzteilen führen die Hersteller Seasonic, Thermaltake und Enermax aus Taiwan die Produktion an. Wobei das allgemeine Qualitätskriterium für Netzteile heute, nämlich die 80-PLUS-Zertifizierung in Bronze, Silber, Gold, Platinum und Titanium von der US-amerikanischen Umweltbehörde EPA stammt und sich als Marktstandard durchgesetzt hat. Netzteile ohne 80+-EPA-Prüfsiegel sind heute praktisch unverkäuflich.\n\nBei den Speichermodulen führen US-Hersteller wie Corsair Memory, Mushkin, Micron Technology und Kingston Technology den Markt an gefolgt von G.Skill und TeamGroup aus Taiwan. Die Speicherchips werden jedoch hauptsächlich von den koreanischen Herstellern Samsung und Hynix sowie von der amerikanischen Firma Micron Technology hergestellt.\n\nFerner haben auch die drei größten FPGA-Hersteller Xilinx, Altera und Atmel ihren Sitz in den USA.\n\nAn der Fertigung aktueller Personal Computer haben europäische Hersteller damit nur einen verschwindend geringen Anteil und sind hier stark auf Importe angewiesen. Auch der US-Marktführer Microsoft der häufigst eingesetzten Betriebssystemsoftware Windows trägt hier einen großen Anteil zur Wertschöpfung in den USA bei.\n\nFür das Vereinigte Königreich ist hier jedoch noch die ARM Limited vorteilhaft, welche die ARM-Architektur weltweit lizenziert, sowie Raspberry Pi für die Marktführerschaft unter den Einplatinencomputern mit dem Raspberry Pi. Die Eurozone selber profitiert hiervon jedoch erst mal nicht.\n\nDie Entwicklung europäischer Alternativen kam über das Prozessordesign kaum hinaus, da bereits in den 1980er-Jahren alle bedeutenden Heimcomputerhersteller wie Commodore und Atari ihren Sitz in den USA hatten, sowie auch die Prozessorhersteller MOS Technology, Motorola und Zilog.\n\nAm europäischsten war in diesem Zusammenhang noch der Acorn Archimedes des britischen Unternehmens Acorn mit zudem auch der eigenen ARM-Architektur, sowie heute auch noch das niederländische Unternehmen ASML das die EUV-Lithografie-Belichtungsmaschinen, die Schlüsseltechnologie für die Intel- und AMD-Prozessorfertigung, herstellt.\n\nDie Entwicklung in Deutschland hatte bis in die 60er Jahre mit den Computern der Zuse KG des Computerpioniers Konrad Zuse eine gewisse Bedeutung. Der Z1 bis zum 5-Kanal-Lochstreifen gesteuerten Zeichentisch Zuse Z64 Graphomat waren bedeutende Eigenentwicklungen mit internationaler Anerkennung.\n\nDie Siemens AG übernahm 1969 die Zuse KG komplett und lagerte die Computersparte nach München in das neue Unternehmen Siemens Nixdorf aus, das später zu Fujitsu Siemens Computers fusionierte. Der Schwerpunkt lag in Kassensystemen und Computerhandel.\nDer 1975 erschienene Nixdorf Quattro 8870 Großrechner mit dem Betriebssystem Business BASIC und der Anwendungssoftware COMET hatte jedoch noch bis in die 80er Jahre einen gewissen Erfolg in Unternehmen. Die CPU (ein nicht näher definierter 1585.01, vermutlich ein Plagiat) stammte bereits von der Firma Digital Computer Controls, Inc. aus den USA.\n\nDie CPU Entwicklung in Russland stützt sich vor allem auf den russischen Elbrus-2000-Mikroprozessor, hierfür existiert sogar ein eigener 130-Nanometer-Fertigungsprozess nach der von AMD übernommenen Ausrüstung aus der Fab 30.\n\nIm Mai 2015 machte ein aktueller Elbrus Heim-PC basierend auf dem Elbrus-4C Chip Schlagzeilen durch die flüssige Darstellung des Spiels Doom BFG von 2004.\n\nIn der Vergangenheit begnügte sich Russland mit dem Kopieren des Zilog Z80, dessen Derivate wie der MME U880 auch die Grundlage für Computer in der DDR stellte.\n\n1982 stellte NEC den PC-98 vor, der mit einem Intel-8086-Prozessor und 128 kB RAM bestückt war. Er wurde aus dem PC-88 entwickelt, der noch einen Zilog Z80 nutzte. Obwohl der PC-98 dem IBM-PC sehr ähnlich war, nutzte er den 16-Bit breiten C-Bus, der schon im PC-88 vorhanden war, während beim IBM-PC der ISA-Bus verwendet wurde. Der PC-98 war in Japan über eine Dekade lang so erfolgreich, dass man vom „IBM-PC Japans“ sprechen kann. Die Leistung der verbauten Komponenten (CPU, RAM, Speicherkapazitäten, etc.) wurde während dieser Zeit ständig angepasst. Erst mit dem Aufstieg von Windows wurde der PC-98 zunehmend aus dem Markt verdrängt. Da es Windows 3.1 und Windows 95 auch in Versionen für den PC-98 gab, griffen Kunden zunehmend zu billigeren PCs, die ebenfalls Windows-fähig waren. Innerhalb von 5 Jahren schwand der Marktanteil in Japan von 60 auf 33 %. 1997 wurde die Produktion des PC-98 eingestellt.\n\nAls der japanische Heimcomputer galt allgemein in den 80er-Jahren der MSX-Computer, der sich dort auch als Alternative zum C64 etabliert hatte, wobei auch der MSX-1 im Wesentlichen noch auf US-Herstellern aufbaute, so auch hier wieder der Z80-Prozessor des Herstellers Zilog, der Grafikchip von Texas Instruments und der Soundchip von General Instrument. Japanische Eigenmarken stellten erst die Nachfolger der MSX-2 und MSX turbo R mit dem Yamaha v9958 Grafikprozessor und Yamaha YM2149/YM2413 Soundchip dar. Die 7,16 MHz schnelle R800 CPU für den MSX turbo R war dabei zwar Zilog kompatibel, tatsächlich aber auch eine Eigenentwicklung der japanischen ASCII Corporation und wurde hergestellt von der Mitsui Bussan. 1987 folgten der X68000-Heimcomputer von Sharp mit einer von Hitachi produzierten HD68HC000-CPU (später wurden 68000er von Motorola verbaut) und 1989 der FM Towns mit erstmals serienmäßig eingebauten CD-ROM-Laufwerk, der jedoch auch schon auf der Intel-8086-Architektur basierte.\n\nAfrika spielt für den weltweiten PC-Handel eine Rolle, da hier der Großteil des Computer- und Elektroschrotts landet. So recyceln z. B. Kinder in Agbogbloshie auf der giftigsten Müllhalde der Welt in einem Slum am Rande der Hauptstadt Accra in Ghana, viele Altgeräte durch das Herauslösen von Aluminium aus Monitorrahmen und Kupfer aus den Kabeln.\n\nIn den arabischen Ländern fand wie in Afrika praktisch keine Entwicklung statt. Am ehesten entwickelt noch Israel mit dem Rüstungshersteller Rafael Advanced Defense Systems einen Teil der Computer-Technologie. Bedeutende Eigenentwicklungen waren hier bereits das Raketenabfangsystem Iron Dome wie auch das Trophy (APS) System zur Verteidigung von Panzern.\n\n\n\n"}
{"id": "4358", "url": "https://de.wikipedia.org/wiki?curid=4358", "title": "Raytracing", "text": "Raytracing\n\nRaytracing (dt. Strahlverfolgung oder Strahlenverfolgung, in englischer Schreibweise meist \"ray tracing\") ist ein auf der Aussendung von Strahlen basierender Algorithmus zur Verdeckungsberechnung, also zur Ermittlung der Sichtbarkeit von dreidimensionalen Objekten von einem bestimmten Punkt im Raum aus. Ebenfalls mit Raytracing bezeichnet man mehrere Erweiterungen dieses grundlegenden Verfahrens, die den weiteren Weg von Strahlen nach dem Auftreffen auf Oberflächen berechnen.\n\nProminenteste Verwendung findet Raytracing in der 3D-Computergrafik. Hier ist der grundlegende Raytracing-Algorithmus eine Möglichkeit zur Darstellung einer 3D-Szene. Erweiterungen, die den Weg von Lichtstrahlen durch die Szene simulieren, dienen, ebenso wie das Radiosity-Verfahren, der Berechnung der Lichtverteilung.\n\nWeitere Anwendungsgebiete von Raytracing sind die Auralisation und Hochfrequenztechnik.\n\nVor der Entwicklung von Raytracing bestand das junge Gebiet der 3D-Computergrafik im Wesentlichen aus einer Reihe von „Programmiertricks“, die versuchten, die Schattierung von beleuchteten Objekten nachzuahmen. Raytracing war der erste Algorithmus auf diesem Gebiet, der einen gewissen physikalischen Sinn ergab.\n\nDas erste mit Raytracing berechnete Bild wurde 1963 an der University of Maryland auf einem oszilloskopartigen Bildschirm ausgegeben. Als Entwickler des Raytracing-Algorithmus gelten oft Arthur Appel, Robert Goldstein und Roger Nagel, die den Algorithmus Ende der 1960er Jahre veröffentlichten. Weitere Forscher, die sich zu dieser Zeit mit Raytracing-Techniken beschäftigten, waren Herb Steinberg, Marty Cohen und Eugene Troubetskoy. Raytracing basiert auf der geometrischen Optik, bei der das Licht als eine Gruppe von Strahlen verstanden wird. Die beim Raytracing verwendeten Techniken wurden bereits wesentlich früher, unter anderem von Linsenherstellern, verwendet. Heute verwenden viele Renderer (Computerprogramme zur Erzeugung von Bildern aus einer 3D-Szene) Raytracing, eventuell in Kombination mit weiteren Verfahren.\n\nEinfache Formen des Raytracings berechnen nur die direkte Beleuchtung, also das direkt von den Lichtquellen eintreffende Licht. Raytracing wurde seit seiner ersten Verwendung in der Computergrafik jedoch mehrmals wesentlich erweitert. Höher entwickelte Formen berücksichtigen auch das indirekte Licht, das von anderen Objekten reflektiert wird; man spricht dann von einem globalen Beleuchtungsverfahren.\n\nDer Begriff \"Raycasting\" bezeichnet meist eine vereinfachte Form des Raytracings, wird teilweise aber auch synonym dazu gebraucht.\n\nDie Erzeugung eines Rasterbildes aus einer 3D-Szene wird \"Rendern\" oder \"Bildsynthese\" genannt. Voraus geht die Erstellung einer solchen Szene vom Benutzer mit Hilfe eines 3D-Modellierungswerkzeugs.\n\nIn der Szenenbeschreibung werden zumindest folgende Daten angegeben:\n\n\nDaneben wird beim Raytracing auch die Position eines \"Augpunktes\" sowie einer \"Bildebene\" angegeben, die zusammen die Perspektive angeben, aus der die Szene betrachtet wird. Der Augpunkt ist ein Punkt im Raum, der der Position einer virtuellen Kamera oder eines allgemeinen Beobachters entspricht. Die Bildebene ist ein virtuelles Rechteck, das sich in einiger Entfernung zum Augpunkt befindet. Sie ist die dreidimensionale Entsprechung des zu rendernden Rasterbildes im Raum. Rasterförmig verteilte Punkte auf der Bildebene entsprechen den Pixeln des zu erzeugenden Rasterbildes.\n\nRaytracing ist in erster Linie ein Verfahren zur Verdeckungsberechnung, also zur Ermittlung der Sichtbarkeit von Objekten ab dem Augpunkt. Das Grundprinzip ist recht einfach.\n\nRaytracing arbeitet mit einer Datenstruktur, \"Strahl\" genannt, die den Anfangspunkt und die Richtung einer Halbgeraden im Raum angibt. Es wird für jedes Pixel die Richtung des Strahls berechnet, der vom Augpunkt aus zum entsprechenden Pixel der Bildebene weist. Für jedes Primitiv der Szene wird nun mittels geometrischer Verfahren der eventuelle Schnittpunkt, bei dem der Strahl auf das Primitiv trifft, ermittelt. Dabei wird gegebenenfalls die Entfernung vom Augpunkt zum Schnittpunkt berechnet. Der „Gewinner“, also das vom Augpunkt aus sichtbare Primitiv, ist dasjenige mit der geringsten Distanz.\n\nDas Prinzip der Aussendung der Strahlen vom Augpunkt aus ähnelt dem Aufbau einer Lochkamera, bei der ein Objekt auf einem Film abgebildet wird. Beim Raytracing sind allerdings „Film“ (Bildebene) und „Loch“ (Augpunkt) vertauscht. Ähnlich wie bei der Lochkamera bestimmt der Abstand zwischen Bildebene und Augpunkt die „Brennweite“ und damit das Sichtfeld.\n\nDa die Strahlen nicht wie in der Natur von den Lichtquellen, sondern vom Augpunkt ausgehen, spricht man auch von \"Backward Ray Tracing\". Raytracing beschäftigt sich mit der Frage, \"woher\" das Licht kommt. Einige Veröffentlichungen nennen das Verfahren allerdings \"Forward Ray Tracing\" oder \"Eye Ray Tracing.\"\n\nDer oben erwähnte Test auf einen eventuellen Schnittpunkt von Strahl und Primitive ist das Herzstück des Raytracings. Solche Tests lassen sich für eine Vielzahl von Primitiventypen formulieren. Neben Dreiecken und Kugeln sind unter anderem Zylinder, Quadriken, Punktwolken oder gar Fraktale möglich.\n\nBei Kugeln ist der Schnittpunkttest eine relativ kurze und einfache Prozedur, was die Popularität dieser Objekte auf Raytracing-Testbildern erklärt. Viele Renderprogramme lassen jedoch aus Gründen der Einfachheit nur Dreiecke als Primitiven zu, aus denen sich jedes beliebige Objekt näherungsweise zusammensetzen lässt.\n\nSeit Kurzem werden auch komplexere Geometrien für den Schnittpunkttest wie etwa NURBS verwendet. Vorteilhaft dabei ist ein Maximum an Präzision, da die Fläche nicht wie sonst üblich in Dreiecke unterteilt wird. Der Nachteil ist eine erhöhte Renderzeit, da der Schnittpunkttest mit komplexen Freiformflächen sehr viel aufwändiger als mit einfachen Dreiecken ist. Eine hinreichende Annäherung an die Genauigkeit von NURBS ist zwar auch mit Dreiecken möglich, in diesem Fall muss aber eine sehr große Anzahl gewählt werden.\n\nBei der Ermittlung des nächsten Primitivs wird nicht nur der Schnittpunkt und seine Distanz zum Augpunkt, sondern auch die Normale des Primitivs am Schnittpunkt berechnet. Damit sind alle Informationen vorhanden, um die zum Augpunkt reflektierte „Lichtstärke“ und somit die Farbe zu ermitteln. Dabei werden auch die Beschreibungen der Lichtquellen der Szene genutzt. Den Berechnungen liegen lokale Beleuchtungsmodelle zugrunde, die die Materialbeschaffenheit eines Objekts simulieren. Diesen Teil des Renderers, der für die Ermittlung der Farbe zuständig ist, nennt man \"Shader\".\n\nDie Programmierung eines einfachen \"Raytracers\" erfordert wenig Aufwand. In Pseudocode lässt sich das Prinzip folgendermaßen darstellen:\n\nJeder Raytracer, unabhängig von der verwendeten Raytracing-Variante, folgt einer ähnlichen Struktur, die noch einen Schnittpunkttest (\"Teste_Primitiv\") und einen Shader (\"Farbe_am_Schnittpunkt\") enthält.\n\nBei der Bestimmung des ersten Primitivs, auf das ein Strahl trifft, kann, wie im weiter oben aufgeführten Beispielcode, jedes Primitiv der Szene gegen den Strahl getestet werden. Dies ist jedoch nicht grundsätzlich erforderlich, wenn bekannt ist, dass gewisse Primitive sowieso nicht in der Nähe des Strahls liegen und daher nicht getroffen werden können. Da Schnittpunkttests die größte Laufzeit beim Raytracing beanspruchen, ist es wichtig, so wenig Primitive wie möglich gegen den Strahl zu testen, um die Gesamtlaufzeit gering zu halten.\n\nBei den Beschleunigungsverfahren wird die Szene meist in irgendeiner Form automatisch aufgeteilt und die Primitiven diesen Unterteilungen zugewiesen. Wenn ein Strahl durch die Szene wandert, so wird er nicht gegen die Primitiven, sondern zunächst gegen die Unterteilungen getestet. Dadurch muss der Strahl nur noch gegen die Primitive derjenigen Unterteilung getestet werden, die der Strahl kreuzt.\n\nEs wurde eine Vielzahl derartiger Beschleunigungstechniken für Raytracing entwickelt. Beispiele für Unterteilungsschemas sind Voxelgitter, BSP-Bäume sowie Bounding Volumes, die die Primitiven umschließen und eine Hierarchie bilden. Mischformen dieser Techniken sind ebenfalls populär. Auch für Animationen gibt es spezielle Beschleunigungstechniken. Die Komplexität dieser Techniken lassen einen Raytracer schnell zu einem größeren Projekt anwachsen.\n\nKeine Technik ist generell optimal; die Effizienz ist szenenabhängig. Dennoch reduziert jedes Beschleunigungsverfahren die Laufzeit enorm und macht Raytracing erst zu einem praktikablen Algorithmus. Auf Kd-Bäumen basierende Unterteilungen sind für die meisten nicht-animierten Szenen die effizienteste oder nahezu effizienteste Technik, da sie sich mittels Heuristiken optimieren lassen. Mehrfach festgestellt wurde, dass die asymptotische Laufzeit von Raytracing in Abhängigkeit von der Anzahl der Primitiven logarithmisch ist.\n\nEs wurde gezeigt, dass auf modernen Rechnern nicht die Prozessorleistung, sondern Speicherzugriffe die Geschwindigkeit des Raytracings begrenzen. Durch sorgfältige Nutzung von Caching durch den Algorithmus ist es möglich, die Laufzeit wesentlich zu verringern. Ebenfalls möglich ist die Nutzung der SIMD-Fähigkeit moderner Prozessoren, die parallele Berechnungen ermöglicht, sowie speziell darauf optimierter Unterteilungsschemata. Damit ist das gleichzeitige Verfolgen mehrerer, in „Paketen“ zusammengefasster, Strahlen möglich. Grund dafür ist, dass die vom Augpunkt ausgesendeten Strahlen meist sehr ähnlich sind, also meist die gleichen Objekte schneiden. Mit dem Befehlssatz SSE etwa können vier Strahlen gleichzeitig auf einen Schnittpunkt mit einem Primitiv getestet werden, was diese Berechnung um ein Vielfaches beschleunigt. Auf entsprechenden Hardwareimplementationen – zum Beispiel auf FPGAs – können auch größere Pakete mit über 1000 Strahlen verfolgt werden. Allerdings büßen Caching- und SIMD-Optimierungen bei erweiterten Formen des Raytracings viel von ihrem Geschwindigkeitsvorteil ein.\n\nWeiterhin ist es möglich, den gesamten Raytracing-Vorgang zu parallelisieren. Dies lässt sich etwa dadurch trivial bewerkstelligen, dass verschiedene Prozessoren bzw. Maschinen unterschiedliche Ausschnitte des Bildes rendern. Lediglich gewisse Beschleunigungstechniken oder Erweiterungen müssen angepasst werden, um parallelisierungstauglich zu sein.\n\nDas grundlegende Raytracing-Verfahren benötigt kaum Speicher. Jedoch belegt die Szene selbst, die sich heutzutage bei komplexen Szenen oft aus mehreren Millionen Primitiven zusammensetzt, sehr viel Speicher und kann mehrere Gigabyte umfassen. Hinzu kommt der mehr oder weniger hohe zusätzliche Speicherbedarf der Beschleunigungstechniken. Da solch große Szenen nicht vollständig in den Arbeitsspeicher des Rechners passen, wird häufig Swapping nötig.\n\nBei größeren Objekten, die mehrmals in der Szene vorhanden sind und sich nur durch ihre Position und Größe unterscheiden (etwa bei einem Wald voller Bäume), muss nicht die gesamte Geometrie neu gespeichert werden. Durch diese \"Instancing\" genannte Technik lässt sich bei bestimmten Szenen erheblich Platz einsparen.\n\nEiner der Gründe für den Erfolg des Raytracing-Verfahrens liegt in seiner natürlichen Erweiterbarkeit. Das oben beschriebene primitive Verfahren ist für die heutigen Anforderungen der Bildsynthese unzureichend. Mit steigender Rechenleistung und zunehmender Inspiration aus der Physik – vor allem der Optik und der Radiometrie – kamen mehrere Erweiterungen und Varianten auf, von denen einige hier kurz vorgestellt werden sollen.\n\nGrundsätzlich gilt, dass mit jeder Erweiterung die erreichbare Qualität der gerenderten Bilder sowie der relative Zeitbedarf stark anstieg und mit Path Tracing das Maximum erreichte. Erst nachfolgende Entwicklungen zielten darauf ab, den Zeitaufwand von Path Tracing zu verringern, ohne an Qualität einzubüßen.\n\nAufgrund der Flexibilität des Raytracing-Algorithmus ist es möglich, Lichtstrahlen nicht nur vom Augpunkt, sondern auch von beliebigen anderen Punkten des Raums auszusenden. Wie Arthur Appel bereits 1968 demonstrierte, kann dies dazu benutzt werden, Schatten zu simulieren.\n\nEin beliebiger Punkt einer Oberfläche befindet sich genau dann im Schatten, wenn sich zwischen ihm und der Lichtquelle ein Objekt befindet. Indem vom Schnittpunkt an der Oberfläche ein \"Schattenstrahl\" in Richtung der Lichtquelle ausgesendet wird, lässt sich bestimmen, ob ein Objekt dessen Weg kreuzt. Ist dies der Fall, so befindet sich der Schnittpunkt im Schatten, und es wird als Helligkeit des Strahls 0 zurückgegeben. Im anderen Fall findet normales Shading statt.\n\nRaytracing lässt sich nicht nur auf einfache lichtundurchlässige, sondern auch auf durchsichtige und spiegelnde, reflektierende Objekte anwenden. Dabei werden weitere Lichtstrahlen von den Schnittpunkten ausgesendet. Bei spiegelnden Flächen etwa muss dabei lediglich die Richtung des von der Fläche ausgehenden Strahls gemäß dem Reflexionsgesetz (Einfallswinkel ist gleich Reflexionswinkel) berücksichtigt und ein entsprechender \"Reflexionsstrahl\" errechnet werden.\n\nBei lichtdurchlässigen Objekten wird ein Strahl gemäß dem Snelliusschen Brechungsgesetz ausgesendet, diesmal ins Innere des betreffenden Objektes. Generell reflektieren transparente Objekte auch einen Teil des Lichts. Die relativen Farbanteile des reflektierten und des gebrochenen Strahls lassen sich mit den Fresnelschen Formeln berechnen. Diese Strahlen werden auch \"Sekundärstrahlen\" genannt.\n\nDa die Sekundärstrahlen auf weitere Objekte fallen können, wird der Algorithmus rekursiv aufgerufen, um mehrfache Spiegelungen und Lichtbrechungen zu ermöglichen. Die hierarchische Gesamtheit der Aufrufe wird auch \"Renderbaum\" genannt.\n\n\"Rekursives Raytracing\" wurde um 1980 von Kay und Whitted entwickelt.\n\nIn Pseudocode sieht der Shader beim rekursiven Raytracing in etwa wie folgt aus:\n\nDer Rest des Programms kann wie beim einfachen Raytracing bleiben. Die hier aufgerufene Funktion \"Farbe_aus_Richtung\" kann wiederum \"Farbe_am_Schnittpunkt\" aufrufen, woraus der rekursive Charakter des Verfahrens deutlich wird.\n\nRekursives Raytracing ermöglicht neben Lichtbrechung und -reflexion die Simulation von harten Schatten. In Wirklichkeit haben Lichtquellen jedoch eine bestimmte Größe, was dazu führt, dass Schatten weich und verschwommen wirken.\n\nDieser Effekt, sowie Antialiasing, glänzende Reflexion und mehr, lassen sich mit \"diffusem Raytracing\" (auch \"stochastisches Raytracing\" oder \"distributed ray tracing\" genannt) simulieren, das 1984 von Cook u. a. veröffentlicht wurde. Die Idee ist, in verschiedenen Situationen statt eines Strahls mehrere Strahlen auszusenden und aus den errechneten Farben den Mittelwert zu bilden. Beispielsweise lassen sich weiche Schatten mit Kern- und Halbschatten erzeugen, indem die Richtungen der Schattenstrahlen zufällig verteilt die Oberfläche der Lichtquelle abtasten. Der Nachteil ist, dass dabei Bildrauschen entsteht, wenn zu wenig Strahlen verwendet werden. Es gibt jedoch Möglichkeiten wie Importance Sampling, die das Rauschen reduzieren.\n\nObwohl diffuses Raytracing zahlreiche Effekte ermöglicht, ist es immer noch nicht in der Lage, die globale Beleuchtung mit Effekten wie diffuser Interreflexion und Kaustiken (durch Bündelung von Licht erzeugte helle Lichtflecken) zu simulieren. Dies liegt daran, dass zwar bei spiegelnden Reflexionen, nicht jedoch bei diffusen Oberflächen Sekundärstrahlen ausgesendet werden.\n\nIn seiner 1986 veröffentlichten Publikation beschrieb James Kajiya die Rendergleichung, die die mathematische Basis für alle Methoden der globalen Beleuchtung bildet. Die von einem Strahl beigetragene „Helligkeit“ wird dabei radiometrisch korrekt als Strahldichte interpretiert. Kajiya zeigte, dass zur globalen Beleuchtung Sekundärstrahlen von allen Oberflächen ausgesendet werden müssen. Daneben wies er auch darauf hin, dass ein Renderbaum den Nachteil hat, dass zu viel Arbeit für die Berechnungen in großer Hierarchietiefe verschwendet wird und es besser ist, jeweils einen einzigen Strahl auszusenden. Diese Methode ist heute als \"Path Tracing\" bekannt, da ein Strahl sich vom Augpunkt aus seinen „Weg“ durch die Szene sucht. Path Tracing hat eine rigorose mathematische und physikalische Basis.\n\nFalls beim Path Tracing der von einer diffusen Oberfläche ausgesandte Sekundärstrahl eine Lichtquelle direkt trifft, so wird dieser Helligkeitsanteil üblicherweise ignoriert. Der Anteil der direkten Beleuchtung wird stattdessen weiterhin per Schattenstrahl berechnet. Alternativ kann die direkte Beleuchtung berechnet werden, indem nur ein Sekundärstrahl gemäß dem lokalen Beleuchtungsmodell ausgesendet wird und, falls dieser eine Lichtquelle direkt trifft, deren Strahldichte zurückgegeben wird. Welche dieser beiden Methoden effizienter ist, hängt vom lokalen Beleuchtungsmodell der Oberfläche sowie vom von der Oberfläche betrachteten Raumwinkel der Lichtquelle ab. Die konzeptuell einfachere Variante des Path Tracing, bei der keine Schattenstrahlen ausgesandt werden, ist als \"Adjoint Photon Tracing\" bekannt.\n\nObwohl Path Tracing die globale Beleuchtung simulieren kann, nimmt die Effizienz des Verfahrens bei kleinen Lichtquellen ab. Insbesondere Kaustiken und deren Reflexionen sind mit Path Tracing sehr verrauscht, sofern nicht sehr viele Strahlen ausgesendet werden. Deshalb werden meist andere, auf Path Tracing basierende Verfahren oder Erweiterungen verwendet.\n\n\"Light Ray Tracing\" ist eine seltene Variante, bei der die Lichtstrahlen nicht vom Augpunkt, sondern von den Lichtquellen ausgesendet werden. Die Pixel, die vom Strahl auf der Bildebene getroffen werden, werden eingefärbt. Dadurch lassen sich bestimmte Effekte wie Kaustiken gut, andere Effekte jedoch nur sehr ineffizient simulieren, da viele Strahlen die Bildebene verfehlen.\n\nDa sich einige Effekte nur vom Augpunkt, andere nur von den Lichtquellen aus gut simulieren lassen, wurden Algorithmen entwickelt, die beide Methoden kombinieren. Das Ziel ist es, Szenen mit beliebig komplexer Lichtverteilung und -reflexion effizient rendern zu können.\n\n\n\nDie angeführten gängigen Varianten des Raytracings lassen sich erweitern, um zusätzliche Effekte zu ermöglichen. Einige Beispiele:\n\n\nRaytracing-Berechnungen gelten als sehr zeitintensiv. Raytracing wird daher vornehmlich bei der Erzeugung von Darstellungen eingesetzt, bei denen eher die Qualität als die Berechnungszeit im Vordergrund steht. Ein Bild mit Raytracing zu berechnen, kann abhängig von der verwendeten Technik, der Szenenkomplexität, der verwendeten Hardware und der gewünschten Qualität beliebig lange – in der Praxis oft mehrere Stunden, in Einzelfällen sogar mehrere Tage – dauern. In Bereichen wie der Virtuellen Realität, in der räumliche Darstellungen in Echtzeit berechnet werden müssen, konnte sich Raytracing daher bisher nicht durchsetzen. Computeranimationsfilme werden überwiegend mit dem REYES-System erzeugt, bei dem Raytracing-Berechnungen so weit wie möglich vermieden werden. Gelegentlich wurde Raytracing von der Demoszene genutzt.\n\nGegenüber üblichen Echtzeitrenderern auf Z-Buffer-Basis hat Raytracing jedoch mehrere Vorteile: eine einfache Implementierung mit überschaubarer Komplexität, eine im Gegensatz zur Grafikpipeline hohe Flexibilität sowie die leichtere Austauschbarkeit der Shader und dadurch eine erleichterte Implementierung neuer Shader. Die Geschwindigkeit von Raytracing muss daher in Relation zur erreichten Bildqualität gesetzt werden. Für die anspruchsvollen Qualitätsanforderungen der realistischen Bildsynthese gibt es, insbesondere bei komplizierten Szenen mit beliebigen Materialien, keine Alternative zu Raytracing.\n\nEs existieren Bestrebungen, echtzeitfähige Raytracer für komplexe Szenen zu implementieren, was bereits unter bestimmten Voraussetzungen mit prozessor- und speicheroptimierten Softwarelösungen gelungen ist. Auf Hardware optimierte Implementierungen von Raytracing zeigen, dass die künftige breite Nutzung von Raytracing im Echtzeitbereich denkbar ist. Mit diesen Anwendungen beschäftigen sich Projekte wie die OpenRT-Programmierschnittstelle und diverse Implementierungen für programmierbare Grafikprozessoren (GPGPU). Außerdem wurden spezielle Architekturen für hardwarebeschleunigtes Raytracing entwickelt.\n\nDas Raytracing-Prinzip kann auf beliebige Anwendungsbereiche ausgeweitet werden, bei denen die Ausbreitung von Wellen in einer Szene simuliert werden soll. Strahlen repräsentieren dabei stets die Normalenvektoren zu einer Wellenfront. In der Auralisation und Hochfrequenztechnik versucht man, die Auswirkungen einer Szene auf die Akustik beziehungsweise auf ein elektromagnetisches Feld zu simulieren. Das Ziel ist es, für bestimmte Frequenzen den Energieanteil zu berechnen, der von einem Sender zu einem Empfänger über die verschiedenen möglichen Wege durch die Szene übertragen wird.\n\nIn der Akustik ist Raytracing neben der Spiegelschallquellenmethode und der Diffusschallberechnung eine Möglichkeit zur Lösung dieses Problems. Zur Simulation müssen die Materialeigenschaften der verschiedenen Körper sowie die Dämpfung des Schalls durch die Luft berücksichtigt werden.\n\nEine Möglichkeit zum Auffinden der Übertragungswege besteht darin, Strahlen von einer Quelle isotrop (in alle Richtungen) auszusenden, eventuell mit Energieverlust an den Gegenständen zu reflektieren und die Gesamtenergie der auf den Empfänger auftreffenden Strahlen zu ermitteln. Diese Methode wird Ray launching genannt. Strahlen können auch eine bestimmte „Form“ – etwa die einer Röhre – haben, um punktförmige Empfänger simulieren zu können. Der Nachteil dieser Methode ist ihre Langsamkeit, da viele Strahlen nie den Empfänger erreichen und für präzise Statistiken eine hohe Anzahl vonnöten ist.\nEin weiteres Problem ergibt sich dadurch, dass die Wellenlänge oft nicht gegenüber den Abmessungen der Körper innerhalb einer Szene vernachlässigbar ist. Sofern die Beugung von Strahlen nicht berücksichtigt wird, kann es daher zu merklichen Fehlern in der Simulation kommen.\n\n\n"}
{"id": "4501", "url": "https://de.wikipedia.org/wiki?curid=4501", "title": "Split Screen", "text": "Split Screen\n\nSplit Screen oder Bildschirmaufteilung (wörtlich „geteilter Bildschirm“) ist eine in visuellen Medien verwendete Technik, die das Bild in zwei (oder mehr) Bereiche aufteilt, um zwei (oder mehr) Handlungen oder Bilder gleichzeitig zu zeigen.\n\nIm Film war der \"Split Screen\" vor allem in den 1960er und 1970er Jahren eine sehr beliebte Technik, wobei das Verfahren selbst bereits in den 1920er Jahren von Abel Gance in seinem monumentalen Spielfilm Napoleon (1927) angewandt wurde. Vorreiter in den 60er Jahren war der US-amerikanische Künstler Andy Warhol in seinem Spielfilm The Chelsea Girls (1966), ein weiteres berühmtes Beispiel ist der Woodstock-Film von Michael Wadleigh (1970). Der bekannteste Mainstream-Film dieser Epoche ist Thomas Crown ist nicht zu fassen (1968). Einer der prominentesten Beispiele der 70er-Jahre ist Brian De Palma, bei dem der \"Split Screen\" zu einem Markenzeichen und häufigem Stilmittel seiner Filme wurde.\n\nZumeist wird die Darstellung vertikal geteilt, um zum Beispiel zwei Telefonierende gleichzeitig zu zeigen. Der Effekt ist ähnlich dem der Parallelmontage, wirkt aber um einiges künstlicher. In Dr. Jekyll und Mr. Hyde (1931) wird das Bild hingegen diagonal geteilt und steht sinnbildlich für Jekylls Persönlichkeitsspaltung.\n\n\"Split Screens\", die aus mehreren, verschieden großen Elementen bestehen, sind u. a. zu sehen in:\n\n\nDuovison bezeichnet ein Verfahren das in den 1970er-Jahren in einigen US-amerikanischen Filmen im Rahmen des New Hollywood Anwendung fand, zum Beispiel bei \"Wicked, Wicked\" von 1973.\n\nDabei wird nicht nur ein Bild gleichzeitig, sondern zwei Bilder, entweder nebeneinander oder untereinander gezeigt. Damit kann eine Handlung aus zwei Blickwinkeln gezeigt werden, oder zwei parallel ablaufende Handlungen werden gleichzeitig gezeigt.\n\nEinige wenige Filme wurden komplett mit aufgeteiltem Bild in Duovison gedreht. Die meisten Duovison-Filme haben nur einige Szenen in Duovision. Teilweise wurde die Duovisonsszenen bei Neuauflagen auf Video bzw. DVD herausgeschnitten, bzw. nur ein Bild wurde verwendet.\n\nBei Interviews oder Diskussionen, bei denen sich die Teilnehmer an unterschiedlichen Orten aufhalten, werden sie teilweise in einem Bild zusammengeschnitten.\n\nEine neuere Anwendung findet der Split Screen bei Live-Sportübertragungen; die hier verwendete so genannte \"Split-Screen-Werbung\" zeigt zum überwiegenden Teil die Werbefilme, während in einer Ecke weiterhin die Sportsendung gezeigt wird.\n\nBei Computerspielen wird der Split Screen als eine Lösung eingesetzt, um mehrere Spieler an einem Gerät und einem Bildschirm spielen lassen zu können und doch jedem sein eigenes Spielfeld zur Verfügung zu stellen, in dem er agieren kann und dessen Blickwinkel er unabhängig von anderen Spielern beeinflussen kann.\n\nDabei wird der Bildschirm je nach Format des Bildschirms getrennt. Bei 5:4- und 4:3-Geräten gibt es die horizontale Teilung, bei 16:10 und 16:9 meist eine vertikale. Jede Bildschirmhälfte wird einem Spieler zugeteilt, dabei spielen aber beide Spieler dasselbe Spiel.\n\nDiese Technik ist im Konsolenbereich, besonders bei Ego-Shootern und Rennspielen, sehr weit verbreitet.\n\nEine andere Variante wird dazu eingesetzt, den Bildschirm für einen einzelnen Spieler in zwei grundverschiedene Bereiche zu teilen, beispielsweise einen farbigen Bereich in Einzelpunktgrafik mit der eigentlichen Spielwelt und einen im Textmodus, der nur der Anzeige von Spielständen und anderen Statusinformationen dient. Bei geeigneter Implementierung (beispielsweise über einen Rasterzeileninterrupt oder den Copper des Amiga) kann der zweite Bereich einen ganz anderen, einfacheren Grafikmodus aufweisen, so dass er weniger Ressourcen an Speicherplatz und Zeit verbraucht, beim aktualisierenden Beschreiben weniger Zeit erfordert und dadurch einfach schneller arbeitet.\n\nDas Paradebeispiel einer narrativen Nutzung des Split-Screen-Verfahrens in PC- und Videospielen ist das von Kritikern hoch gelobte Spiel \"Fahrenheit\". Der Spieler steuert seine Figur in einem der Fenster, während er zeitgleich die Aktionen anderer Figuren beobachtet. Das Verfahren wird, wie in der TV-Serie \"24\", hauptsächlich zur Erzeugung von Spannung eingesetzt. Zum Beispiel muss der Spieler aus einem Gebäudekomplex fliehen und sieht dabei gleichzeitig, wie ein Polizeibeamter sich der Wohnung des Protagonisten nähert.\n\nEinige Monitore, zum Beispiel Fernsehgeräte, aber insbesondere Überwachungsmonitore, bieten die Möglichkeit, gleichzeitig verschiedene, ständig aktualisierte Quellen oder sequentiell die Kanäle eines Empfängers anzuzeigen. Hier erfolgt die Aufteilung meist quadratisch mit gleichem Seitenverhältnis, also zum Beispiel 2×2 4:3-Bilder auf einer 4:3-Anzeige oder 3×3 16:9-Bilder auf einem 16:9-Gerät. Mehr als 25 (5×5) oder maximal 36 (6×6) Bilder gleichzeitig sind dabei nicht sinnvoll. Unterscheiden sich Seitenverhältnis der Signale und des Anzeigegerätes, kommen auch nicht quadratische Aufteilungen in Betracht, zum Beispiel 4×3 4:3-Bilder auf einem 16:9-Gerät oder 3×4 16:9-Bilder auf einem 4:3-Gerät. Auch asymmetrische Aufteilung ist möglich, zum Beispiel ein großes 4:3-Bild (2/3-Breite) neben drei kleinen übereinander (1/3-Breite) in einem 16:9-Rahmen.\n\n\n"}
{"id": "4657", "url": "https://de.wikipedia.org/wiki?curid=4657", "title": "Slackware", "text": "Slackware\n\nSlackware [] ist die älteste aktive Linux-Distribution und die erste, die große weltweite Verbreitung fand. Wegen dieses frühen Erfolges und des konsequenten Verzichts auf unnötigen Ballast nach dem KISS-Prinzip bildete Slackware die Grundlage für andere bekannte Distributionen wie z. B. SuSE Linux. Slackware ist für die Verwendung mit i486-kompatiblen (ursprünglich i386-kompatiblen) und x86-64-Architekturen sowie Alpha- und SPARC-Architekturen bestimmt. Mit Slackware ARM existiert eine offizielle Portierung auf die ARM-Architektur, Slack/390 bietet eine offizielle Portierung für die Großrechnerarchitektur S/390 und deren Nachfolger System z. Slackware findet seine Hauptanwendung im professionellen Umfeld.\n\nSlackware ist auf größtmögliche Unix-Ähnlichkeit ausgelegt. Alle Einstellungen am System können vom Nutzer durch Editieren der Konfigurationsdateien vorgenommen werden. Es existieren keine offiziellen \"distributionsspezifischen\" Werkzeuge mit Benutzeroberfläche zur Systemkonfiguration (wie beispielsweise YaST bei openSUSE). Dies verstieße gegen das KISS-Prinzip.\n\nDie Veröffentlichung einer neuen Slackware-Version erfolgt immer erst dann, \"wenn sie fertig ist\" – das heißt, es gibt keine festgelegten Deadlines und keine Vorankündigung. Slackware setzt Linux Loader (LILO) als Bootmanager ein und hat einen BSD-ähnlichen init-Prozess.\n\nEs gibt so gut wie keine distributionsspezifischen Änderungen an den zur Verfügung stehenden Paketen, was die Übersetzung und Installation eigener Software im Gegensatz zu anderen Distributionen erleichtert. Slackware verwendet komprimierte Tar-Archive als Paketformat; ab Version 13.0 werden diese mit dem Lempel-Ziv-Markow-Algorithmus anstelle von gzip komprimiert (*.txz statt *.tgz). Aufgrund des einfachen Paketaufbaus werden Paketabhängigkeiten weder vom System überprüft noch wird das Überschreiben von Dateien bereits installierter Pakete verhindert. Das ist ein wesentlicher Unterschied zu den in vielen anderen Linux-Distributionen enthaltenen Paketmanagern. Diese Eigenschaften ermöglichen es dem fortgeschrittenen Benutzer, auch Teile von umfangreicherer Software zu nutzen und auf unnötig erscheinende Software zu verzichten oder Anpassungen am System vorzunehmen, ohne mit Nebeneffekten durch Inkonsistenzen im Paketmanagement rechnen zu müssen. Im Verzeichnis codice_1 findet sich eine Liste der installierten Pakete mit Beschreibungen, welche Dateien mit welchem Paket installiert wurden. Mit dem Kommando codice_2 (List Dynamic Dependencies) können Abhängigkeiten dynamischer Bibliotheken angezeigt werden, woraus sich deren Notwendigkeit ableiten lässt.\n\nFür die einfache Installation und Konfiguration des Systems stellt Slackware Skripte zur Verfügung. Das Einspielen aktueller, sicherheitsrelevanter Softwareupdates kann ab Version 12.2 mit dem Programm \"slackpkg\" vorgenommen werden.\n\nDa Slackware mit einer recht minimalistischen Paketauswahl daher kommt, existieren diverse inoffizielle Programme, um auf einfache Art und Weise zusätzliche Software installieren zu können. \"Sbopkg\" ist unter diesen zusätzlichen Programmen das derzeit populärste. Es benutzt \"Buildskripte\", um Software aus ihren Quelltexten zu kompilieren und ein Paket daraus zu erstellen. Außerdem existieren Programme wie \"swaret\" und \"slapt-get\". die für Softwareinstallationen mit Abhängigkeitsauflösung benutzt werden können, sofern entsprechende Pakete und Repositorien verfügbar sind.\n\nDas Management der Software-Repositorien ist eher konservativ ausgelegt. Es ist möglich, dass über mehrere Versionen hinweg kein Upgrade einer bestimmten Software erfolgt.\n\nAb Ende 1992 versuchte sich Patrick Volkerding an Fehlerkorrekturen für Softlanding Linux System (SLS), der ersten umfassenden Linux-Distribution überhaupt. Nachdem die Ergebnisse dieser Arbeit in seinem Umfeld populärer wurden, veröffentlichte er sie am 17. Juli 1993 als eigene Distribution unter dem Namen \"Slackware Linux 1.00\". Einen Monat später tat Ian Murdock es ihm mit seiner aus den gleichen Beweggründen entwickelten ersten Version von Debian gleich. Der erste Teil des Namens, Slack, leitet sich von einem Glaubensprinzip der Religionsparodie Church of the SubGenius ab und steht für Freiheit, Unabhängigkeit und originelle Ideen, die zum Erreichen der persönlichen Ziele führen.\n\nDie erste offizielle Version von Slackware bestand aus 24 3,5″-Disketten. Die ersten 13 Disketten enthielten einen DOS-Emulator, den Linux-Kernel 0.99pl11 in einer Alphaversion, die GNU Compiler Collection (GCC) in Version 2.4.5 und die zwei Unix-Shells (pd)ksh und tcsh. Die anderen elf Disketten umfassten Treiber für Grafikkarten, ein XFree86-System in Version 1.3 und einige grafische Anwendungen.\n\nSlackware 1.1.1 bestand aus 51 Disketten und beinhaltete u. a. XFree86 2.0 und eine TeX-Installation.\nIn Slackware 1.2 war die Linux-Kernelversion 1.0 enthalten. Für die vollständige Installation wurden mindestens 200 MB Festplattenspeicher benötigt.\n\nVersion 2.1 der Distribution benötigte bereits 65 Disketten sowie eine Root- und eine Bootdisk. Slackware 2.2.0.1 enthielt XFree86 in Version 3.1.1.\n\nSlackware 3.0 erschien am 30. September 1995 zum ersten Mal im Executable and Linking Format und konnte bei Walnut Creek (heute FreeBSD Mall) als CD-ROM bezogen werden. Die im Juli 1996 veröffentlichte Version 3.1 wird auch Slackware 96 genannt (als Anspielung auf Windows 95) und enthielt den Linux-Kernel in Version 2.0. Für Slackware 3.6 waren 500 MB Festplattenspeicher für die Vollinstallation notwendig.\n\nZusammen mit Slackware 3.9 erschien im Mai 1999 Version 4.0, welche 1 GB auf der Festplatte belegt und u. a. XFree86 3.3.3.1 und den ersten freien KDE in Version 1.1.1 enthält.\n\nNach Version 4.0 erfolgte im Jahr 1999 ein Versionssprung auf Version 7.0. Grund dafür war nach Angabe von Volkerding der aus Marketinggründen inflationäre Gebrauch neuer Versionsnummern bei anderen Distributionen und die sich ständig daraus ergebenden Anfragen „von Leuten, die nichts über Linux wissen,“ wann man denn upgraden würde oder ob die in Slackware enthaltenen Komponenten 3 Versionen älter seien als anderswo. Für Slackware 7.0 wird eine 2 GB große Installationspartition empfohlen. Mit Slackware 7.1 wurde auch Gnome Teil der Distribution.\n\nIn der Releasereihe von Slackware 8.x werden die KDE-Versionen 2.1.1 bzw. 3.0.1 verwendet, XFree86 4.1.0 bzw. 4.2.0 und die Mozilla Application Suite kamen dazu.\n\nDie Slackware 9.x-Reihe ist für den Linux-Kernel 2.6 vorbereitet, benutzt aber die Kernel 2.4.20 und 2.4.22 und dazu KDE 3.2.3 sowie Gnome 2.6.2. Slackware 9.0 ist die letzte Version, die sich auf einer i386-kompatiblen Architektur installieren lässt, ab Version 9.1 wird mindestens eine i486-kompatible Architektur vorausgesetzt.\n\nMit Slackware 10.0 wurde 2004 XFree86 durch den X.Org-Server ersetzt. Version 10.1 entfernte Gnome wieder aus der Distribution, und es begann die Migration zur Linux-Kernelserie 2.6, die ab Slackware 10.2 optional zur Verfügung steht.\n\nAb Slackware 12.0 kam ein 2.6er-Kernel zum Einsatz, ab Version 12.1 sogar ausschließlich.\n\nAb Slackware 13.0 steht die Distribution unter der Bezeichnung „Slackware64“ auch für x86-64-Architekturen zur Verfügung. Die 32-Bit-Version wird weiterhin „Slackware“ genannt. Ferner wird der bisher verwendete Kompressionsalgorithmus der Slackware-Pakete von gzip auf xz umgestellt.\n\nSlackware 13.1 entfernt mit Nutzung der Kernelserie 2.6.33 die Unterstützung für das alte IDE-Subsystem; ab jetzt werden alle Festplatten unter den Gerätenamen /dev/sd* angesprochen. Ferner wird mit KDE 4.4.3 auch dessen Berechtigungsdienst PolicyKit sowie das Sitzungsverwaltungssystem ConsoleKit in speziellen und auf Slackware angepassten Versionen eingeführt. Das ist insbesondere bemerkenswert, weil Slackware traditionell viel Wert darauf legt, möglichst geringe Anpassungen an der mitgelieferten Software durchzuführen. Die Anpassungen wurden nötig, um die von Slackware genutzte Shadow-Passwort-Methode zum Schutz von Passwörtern zu unterstützen. Außerdem wird insbesondere die Unterstützung für Laptops verbessert: Es wird ein \"tickless\" Kernel ohne regelmäßige Timer Interrupts verwendet; wie oft dieser geweckt wird, lässt sich mit \"powertop\" festlegen. Ferner wurde \"usb_modeswitch\" integriert und so die temporäre Verwendung von USB-Geräten vereinfacht.\n\nSlackware 13.37 enthält neben einer umfassenden Aktualisierung der mitgelieferten Pakete wesentliche Neuerungen in der Installationsroutine.\nDeren Hardware-Initialisierung wird nun von udev erledigt. Ein Vorteil dessen ist, dass bei der Installation über ein Netzwerkprotokoll die Konfiguration der Netzwerkkarte über ältere Hilfsskripte entfällt. Als weitere Neuerung unterstützt Slackware nun GPT-Partitionstabellen während der Installation.\nWerkzeuge zur Verwaltung des Btrfs-Dateisystems gehören nun ebenfalls zur Distribution. Aufgrund der Aktualisierung von X.org wird HAL nicht länger für die Erkennung und Initialisierung der Eingabegeräte verwendet. Stattdessen findet auch hier udev Verwendung. Weitere Neuerungen sind die Aufnahme von ddrescue, rfkill und lxc, ein System ähnlich chroot zur Isolierung von Systemprozessen und -ressourcen. Letztlich ist es jedoch wesentlich leistungsfähiger wodurch sich damit virtuelle Systeme betreiben lassen.\n\nSlackware64 ist die offizielle Portierung von Slackware auf die 64-Bit-Prozessorarchitektur. Die Portierung wurde offiziell mit der Veröffentlichung von Slackware-13 eingeführt. Änderungen an Slackware und Slackware64 finden gleichzeitig statt, weil beide Projekte vom selben Team betreut werden. Die Freigabe einer neuen Version von Slackware64 erfolgt gleichzeitig mit jener von Slackware. Die Entwicklung von Slackware64 wurde durch das mittlerweile nicht mehr weiterentwickelte Slamd64 sowie BlueWhite64 maßgeblich begünstigt. Slackware64 ist eine reine 64-Bit-Distribution, die jedoch für die Einrichtung der gleichzeitigen Unterstützung für 32- und 64-Bit Prozessorarchitekturen (Multilib-Umgebung) vorbereitet ist. Die Tools zur Einrichtung der Multilib-Umgebung sind jedoch nicht Bestandteil von Slackware64.\n\nSlackware ARM wurde im Jahr 2002 unter dem Namen „ARMedslack“ begründet und ist eine offiziell anerkannte Portierung von Slackware auf die ARM-Prozessorarchitektur. Mit der offiziellen Anerkennung im Jahr 2009 erfolgte die Umbenennung von „ARMedslack“ zu „Slackware ARM“. Die aktuelle Version ist 14.1. Derzeit werden die Plattformen ARM Versatile und Marvells SheevaPlug unterstützt. Durch die Unterstützung der ARM Versatile Plattform kann Slackware ARM auf emulierter Hardware, z. B. mittels QEMU laufen, was die Entwicklung für diese Plattform vereinfacht.\n\nSlack/390 wurde im Jahr 2004 begründet und ist eine offiziell anerkannte Portierung von Slackware auf die S/390-Architektur. Die aktuelle Version ist 10.0. Nach der Einführung von Slackware64 wurde auch eine 64-Bit-Version für die S/390-Nachfolgearchitektur, das System z erstellt.\n\n\n"}
{"id": "4872", "url": "https://de.wikipedia.org/wiki?curid=4872", "title": "Feststelltaste", "text": "Feststelltaste\n\nEine Feststelltaste () oder eine Umschaltsperre () befindet sich auf einer Computer- oder Schreibmaschinen-Tastatur direkt über der linken Umschalttaste (). Sie dienen zum permanenten Umschalten in eine Alternativbelegung der Tastatur. So hilft sie etwa bei der Eingabe von Text in Großbuchstaben (s. Majuskelschrift).\n\nSie kann unterschiedlich beschriftet sein: Je nach Funktion finden sich beispielsweise die standardisierten Symbole ⇩ oder ⇪, ein Vorhängeschloss-Symbol (nicht standardisiert) aber auch der Aufdruck „Caps Lock“. Der Aufdruck „Shift Lock“ wird praktisch nicht mehr verwendet.\n\nAuf einigen (älteren) Tastaturen ist die Taste mit einer besonderen Mechanik ausgestattet, welche ein von den übrigen Tasten deutlich abweichendes taktiles Feedback liefert. Dabei wird die Funktion der Umschaltsperre einer mechanischen Schreibmaschine nachgeahmt: die Taste verbleibt nach Aktivierung in der heruntergedrückten Position und springt erst nach nochmaliger Betätigung wieder in die Ausgangsposition zurück. Ein Beispiel für eine solche Taststatur ist das Apple Extended Keyboard II, eine ADB-kompatible Tastatur mit Tastenmechanik des Herstellers ALPS. Oft ist die Taste auch zusätzlich oder anstatt der besonderen Mechanik mit einem Leuchtmittel zur Statusanzeige versehen.\n\nDie Taste kann zwei verschiedene Funktionen haben:\n\n\nAuf deutschen Computertastaturen fungiert die Taste in der Regel als Umschaltsperre, auf schweizerischen oder amerikanischen Tastaturen als Feststelltaste. Windows folgt dieser Konvention und ändert die Funktion der Taste entsprechend der aktiven Tastaturbelegung. Allerdings kann in den Spracheinstellungen eine Tastaturbelegung „Deutsch (IBM)-Tastatur“ (früher auch „Deutsch (EDV)“ genannt) gewählt werden. Bei dieser Einstellung bewirkt die Taste, dass die Buchstaben großgeschrieben werden, aber z. B. die Zahlen weiterhin normal erreichbar sind. Unter macOS fungiert die Taste standardmäßig als Feststelltaste.\n\nUrsprünglich wurde die Taste auch auf unterschiedlichen Wegen deaktiviert – auf deutschen Tastaturen durch das Drücken der Umschalt-, auf schweizerischen oder amerikanischen Tastaturen durch ein erneutes Betätigen der Caps-Lock-Taste.\n\nDer Ursprung der Umschaltsperre liegt in der mechanischen Schreibmaschinentastatur: Der Druck der Taste senkte den Typenwagen ab oder hob die Schreibwalze an, wobei im Gegensatz zur normalen Umschalttaste die Mechanik in dieser Stellung einrastete. In dieser Stellung wurde jeweils die Zweitbelegung der anderen Tasten aufs Papier gebracht. Ein Druck auf die linke Umschalttaste löste die Einrastung wieder.\n\nUnter den meisten Betriebssystemen kann die Taste deaktiviert werden, um ein versehentliches Betätigen zu vermeiden. Auch besteht im Grundsatz die Möglichkeit, die Taste gänzlich anders zu belegen – \n\n\nWindows:\n\nGNU/Linux:\n"}
{"id": "5134", "url": "https://de.wikipedia.org/wiki?curid=5134", "title": "Tux (Maskottchen)", "text": "Tux (Maskottchen)\n\nTux ( [], []) ist das offizielle Maskottchen des freien Linux-Kernels. Es stellt einen wohlgenährten, glücklichen, rundlichen Pinguin dar.\n\nLinus Torvalds kündigte im Juni 1996 ein Logo für Linux an: den Pinguin. Entscheidend bei der Auswahl war 1993 ein Aquariumsbesuch in Canberra während einer Auslandsreise nach Australien. Torvalds empfand dort eine starke Sympathie für die sehr kleinen Zwergpinguine (in Australien auch als \"fairy penguin\", also etwa \"Feenpinguin\" bezeichnet). Als er dann 1996 ein „hübsches“ Logo suchte, kam er gleich auf die Pinguine zurück. Doch nach der Bekanntgabe glaubten manche Leute nicht, dass ein kleiner, dicker Pinguin die Eleganz von Linux widerspiegeln könne; es gab Personen, die stattdessen einen Fuchs als Logo vorschlugen. Darauf verteidigte Torvalds sich mit dem Argument: \n\nLinus Torvalds schreibt in seiner Biografie \"Just for Fun\":\n\nEine weitere Quelle der Inspiration für Torvalds war wohl ein Bild, das er auf einem FTP-Server entdeckt hatte. Es zeigt eine Pinguinfigur, die stark an einen \"Creature-Comforts\"-Charakter von Nick Park erinnert. Der Name wurde von James Hughes als Ableitung von Torvalds Unix vorgeschlagen. Auch ist der Name \"Tux\" wahrscheinlich als Abkürzung für den amerikanischen Begriff für Smoking, \"Tuxedo\", mitinspiriert worden. Eine weitere Erklärung ist die angebliche Verwendung als rekursives Akronym \"\"Tux uses X\"\", welche auf das unter Linux gebräuchliche X Window System anspielt.\n\nDer ursprüngliche Entwurf für Tux erfolgte 1996 durch Larry Ewing mit GIMP, einem freien Bildbearbeitungsprogramm, und er hat ihn unter den folgenden Bedingungen freigegeben:\n\nLaut Jeff Ayers hatte Linus Torvalds eine „Fixierung auf flugunfähige, fette Wasservögel“, und Torvalds behauptete, mit \"Penguinitis\" infiziert worden zu sein, als er von einem Pinguin leicht gebissen wurde. „Penguinitis bewirkt, dass du nachts wach bleibst, um nur noch an Pinguine zu denken, und eine starke Zuneigung für sie empfindest.“ Torvalds angebliche Krankheit ist selbstverständlich ein Witz, aber er wurde wirklich bei einem Besuch in Canberra von einem Zwergpinguin gebissen.\n\nTux ist ein Symbol für Linux und die Open-Source-Community geworden, und eine britische Linux User Group hat sogar mehrere Pinguine im Zoo von Bristol adoptiert.\n\nHeute assoziieren Menschen, die sich mit IT beschäftigen, den Pinguin mit dem freien Betriebssystem GNU/Linux. Ein oft gehörter Spruch lautet: , Tux steht also auch für Stabilität.\n\nAm 17. März 2009 wurde Tux von Linus Torvalds in ein dreimonatiges Sabbatical entlassen. Die Rolle als Linux-Maskottchen übernahm in der Kernelversion 2.6.29 \"Tuz\", ein Tasmanischer Teufel, der einen aufgesetzten gelben Schnabel trägt. Durch diesen vorübergehenden Austausch des Maskottchens sollte Aufmerksamkeit auf den ausschließlich auf Tasmanien lebenden Beutelteufel gelenkt werden, dessen natürlicher Fortbestand durch die Krankheit Devil Facial Tumour Disease (DFTD) bedroht ist. Tuz hatte bereits zuvor als Maskottchen der im Januar 2009 in Hobart auf Tasmanien abgehaltenen Linux-Konferenz linux.conf.au gedient. Tuz wurde von Andrew McGown entworfen, von Josh Bush mit Inkscape als SVG neuerstellt und unter der CC-BY-SA-Creative-Commons-Lizenz veröffentlicht.\n\nAufgrund der freien Verwendbarkeit (mit Autorennennung), die Ewing für Tux erlaubt, existieren auch einige Verwendungen abseits der Softwarebranche, beispielsweise wurde 2003 eine Sonderedition von Pfannen mit dem Tux-Abbild verkauft, ein Teil der Erlöse floss an die FSF.\n\n2007 wurde Tux vom Besteckfabrikanten WMF im Kinderbesteckset \"Seelöwe\" verwendet.\n\n2010 wurde ein Monument eines Tux mit Flügeln im russischen Tyumen durch die lokale Linux User Community errichtet.\n\n\n\n"}
{"id": "5205", "url": "https://de.wikipedia.org/wiki?curid=5205", "title": "TWAIN", "text": "TWAIN\n\nTWAIN ist ein 1992 von den Unternehmen Aldus Corporation, Eastman-Kodak, Hewlett-Packard und Logitech festgelegter Standard zum Austausch von Daten zwischen Bildeingabegeräten (Scanner, Digitalkameras etc.) und Programmen für Microsoft Windows und Apple Macintosh. \n\nEin mit einer TWAIN-Schnittstelle ausgestattetes Bildverarbeitungsprogramm kann Daten von jedem Bildeingabegerät entgegennehmen, das seinerseits entsprechende Unterstützung bietet.\nDie derzeit aktuelle Version des TWAIN-Standards ist TWAIN 2.4 und wurde am 15. Dezember 2015 veröffentlicht.\n\nTWAIN besteht aus drei Elementen: Der Datenquelle (\"data source\"), dem Anwendungsprogramm (\"application program\") und dem Quellenmanager (\"source manager\"). Die Datenquelle wird von dem Scanner- oder Digitalkamera-Treiber gebildet, der normalerweise bei dem Gerät mitgeliefert wird. Das Anwendungsprogramm ist das Programm, in dem die aufgenommenen Bilddaten weiterverwendet werden, z. B. ein Bildverarbeitungsprogramm. Der Quellenmanager ist in der Regel Bestandteil des Betriebssystems. Von den Datenquellen und Anwendungsprogrammen können durchaus mehrere auf einem Rechner vorhanden sein, die dann vom Quellenmanager verwaltet werden.\n\nNeben dem TWAIN-Standard gibt es auch noch andere Verfahren, Bildeingabegeräte und Anwendungsprogramme miteinander zu verbinden:\n\nDie Erklärung, die von der TWAIN Working Group als offizielle Antwort auf die Frage nach der Herkunft des Namens veröffentlicht wurde, ist die Ableitung aus dem Ausspruch \"„Never the twain shall meet!“\" aus Rudyard Kiplings \"The Ballad of East and West\", was so viel bedeutet wie „Niemals werden die zwei sich treffen!“. Das ist eine Anspielung auf TWAIN als Vermittler zwischen Geräten, die nicht direkt interagieren können. Das Wort „twain“ in diesem Ausspruch geht auf Altenglisch „twegen“ zurück (vgl. frühneuhochdeutsch „zween“). Es wurde willkürlich in Großbuchstaben geschrieben, um es markanter aussehen zu lassen.\n\nDa die Schreibung TWAIN wie ein Akronym aussieht, wird sie häufig auf passende Wortgruppen zurückgeführt, vor allem Technology Without An Interesting Name (Varianten für T: Toolkit, Thing; für A: Any, für I: Important), zu deutsch etwa \"Technik (oder: Werkzeugsatz; Ding) ohne (irgend)einen interessanten (oder: wichtigen) Namen\".\n\nAndere Quellen benennen TWAIN als Abkürzung für Transmit Windows Advanced Interface.\n\n\n"}
{"id": "5574", "url": "https://de.wikipedia.org/wiki?curid=5574", "title": "Microsoft Windows 95", "text": "Microsoft Windows 95\n\nMicrosoft Windows 95 ist ein Betriebssystem mit grafischer Benutzeroberfläche für Personal Computer (PC). Es war das erste Betriebssystem der Windows-Reihe von Microsoft, das den 32-Bit-Betrieb des Prozessors weitreichend unterstützte, ohne auf die Abwärtskompatibilität zu den damals noch weit verbreiteten DOS-Programmen zu verzichten. Diese wurden (und werden bis heute) unter NT-Systemen lediglich in einer Virtual DOS Machine ausgeführt, was z. B. direkte Hardwarezugriffe, die viele dieser Programme voraussetzen, konsequent verhindert. Wie auch Windows NT ist Windows 95 abwärtskompatibel für 16-Bit-Windowsprogramme.\n\nNach seiner Einführung am 24. August 1995 entwickelte sich Windows 95 zum bis dahin erfolgreichsten Betriebssystem auf dem Markt und begründete die Windows-9x-Reihe.\n\nIm Februar 1995 wurde an eine Handvoll Personen eine Testversion des bis dahin geheimen Windows 95 verteilt. Davor kannte man es nur als Windows 4.0 oder unter seinem Arbeitstitel Windows „Chicago“. Jeder, der an den Testphasen teilnehmen durfte, musste einen Geheimhaltungsvertrag unterzeichnen.\nAm 24. August 1995 gab Microsoft nach weiteren zahlreichen Tests die Endversion zum Verkauf frei. Deren Versionsnummer war 4.00.950. Microsoft begann die größte Produkteinführung der Konzerngeschichte.\n\nDamit läutete Microsoft auf breiter Front das Ende der 16-Bit-Architektur ein. Im 16-Bit-Modus der x86-Linie laufen unter anderem DOS und die ersten Versionen von Windows bis einschließlich Microsoft Windows 3.1 (Windows 3.1 ist teilweise 32-Bit-fähig). Windows 95 setzt, ebenso wie seine direkten Nachfolger Windows 98 und Windows Me, auf MS-DOS auf, das zum Starten und für einige wichtige Systemprozesse und Treiber benötigt wird.\n\nUmstritten ist, ob Windows 95 als „grafische Oberfläche für DOS“ zu betrachten sei oder als weitgehend eigenständiges Betriebssystem:\n\nDer offizielle Support von Windows 95 von Microsoft mit Aktualisierungen und Korrekturen endete am 31. Dezember 2001.\n\nIm August 1995 führte Microsoft das Produkt mit seiner bis dahin größten Werbekampagne ein. Die Einführung des „Start“-Knopfes wurde bei Fernsehspots mit dem Lied „Start me Up“ von den Rolling Stones untermalt.\n\nDie eigens für Windows 95 geschaffene Startmelodie wurde 1994 von Brian Eno komponiert, nachdem er von Mark Malamud und Erik Gavriluk (Senior-Entwickler des Microsoft Chicago-Projekts) angesprochen worden war.\nMicrosoft wollte ein Musikstück, das inspirierend, universell, optimistisch, futuristisch, gefühlvoll und emotional sei, und noch weiteren Attributen gerecht werden sollte. Auch sollte es maximal 3¼ Sekunden lang sein. Schließlich wurden daraus jedoch sechs Sekunden.\n\nUm sowohl computerunerfahrenen als auch Nutzern älterer Windowsversionen eine schnelle Eingewöhnung in die neue Oberfläche des Betriebssystems, seiner Bedienung und der neuen Multimediamöglichkeiten zu ermöglichen, lag vielen vorinstallierten Computern eine CD-Rom mit dem Titel \"Windows 95 Start!\" bei, ein interaktiver Computerkurs.\n\nDie \"Pen Services\" sind eine Betriebssystemerweiterung, die auf die Eingabe mittels Lichtgriffel ausgelegt war und auf Tablet-PCs ausgeführt wurde. Die Bedienung zeichnete sich hauptsächlich dadurch aus, dass sie Funktionen wie trainierbare Handschrifterkennung und Gesten einführte. Entwickelt wurde \"Windows für Pen Computing\" in zwei Unterversionen zusammen mit den Kooperationspartnern IBM und Compaq. Nachdem Compaq 1994, also vor dem Erscheinen von Windows 95, seine Pläne, einen PDA auf Grundlage dieses Betriebssystems herauszubringen eingestellt hatte, entschied Microsoft, die Version für Windows 95 ohne Beteiligung von Kooperationspartnern zu entwickeln, aber ebenfalls ausschließlich an OEM-Kunden zu lizenzieren.\n\nNach Angaben von Microsoft existieren unter anderem folgende Verbesserungen gegenüber Windows 3.1:\n\nWindows 95 ist nach Windows NT 3.1 und 3.5 (beide mit der Benutzeroberfläche von Windows 3.x) das erste Microsoft-Betriebssystem, das zum größeren Teil auf der 32-Bit-Architektur (im x86-kompatiblen Schutzmodus) basiert. IBM mit OS/2, das diese Technik schon längere Zeit beherrschte, konnte sich auf dem Markt gegen Windows nicht durchsetzen. Dabei wurden von Microsoft die 16-Bit-DOS-, 16-Bit-Windows- und 32-Bit-Windowsarchitekturen (mit ihren spezifischen Speicherschutzmodi) in einer Art Symbiose vereint. Die meiste Software lief damals noch unter DOS, was eine konsequente Windows-NT-Entwicklung unattraktiv machte.\n\nMit Windows 95 konnte nun auch ein Win-3.x-Nachfolger mehrere Programme gleichzeitig ausführen. Bisher mussten Programme (unter Windows 3.x) warten, bis das Vorgängerprogramm den Prozessor freigab. Multitasking ist zwar schon in vorigen Windows-Versionen vorhanden, jedoch handelt es sich dort noch um kooperatives Multitasking, es läuft also immer nur ein Programm gleichzeitig, die anderen werden lediglich im Speicher gehalten und solange angehalten. Das präemptive Multitasking im 32-Bit-Modus ermöglicht nun einen systemkontrollierten Quasi-Parallelbetrieb im Zeitscheibenverfahren (vgl. auch Scheduling), allerdings aus Gründen der Abwärtskompatibilität nur mit eingeschränktem Speicherschutz.\n\nMit der Registrierungsdatenbank wurde ein zentraler, systemweit eindeutiger und auch konkurrierend erreichbarer Platz für Konfigurationsinformationen eingeführt; sie löste das System der Initialisierungsdateien von Windows 3.1 fast vollständig ab. Jedoch verwenden auch heute gelegentlich noch Anwendungsprogramme Konfigurationsdateien anstatt Registry-Einträge, besonders Portable Software.\n\nDurch die Dateisystem-Erweiterung VFAT erlaubt das System erstmals die Nutzung längerer Dateinamen unter Windows, wodurch die von DOS bekannte Begrenzung auf 8+3 Zeichen für den Namen entfällt. Jetzt sind 255 Zeichen erlaubt, jedoch \"inklusive\" des Pfadnamens, was beim Kopieren in Unterordner zusätzliche Probleme verursachen kann. Dabei unterscheidet Windows zwar keine Groß- und Kleinbuchstaben, behält aber die vom Benutzer vergebene Schreibweise bei. Mit VFAT wollte Microsoft das neue Dateisystem kompatibel zum alten machen, sodass jeder lange Name noch einen automatisch generierten DOS-kompatiblen Namen erhält, z. B. „DOKUME~1.DOC“ neben „Dokumentation des neuen Projekts.doc“ (hinter der Tilde werden mehrfach vorhandene Namen einfach durchnummeriert). Dadurch können alle auf VFAT erstellten Dateien auch von DOS-Nutzern und Nutzern von Windows bis Version 3.11 verwendet werden (wenn das zugrundeliegende Dateisystem das von DOS unterstützte FAT12 oder FAT16 ist, nicht jedoch FAT32).\n\nWindows 95B unterstützt erstmals FAT32, wodurch ein erweiterter Adressraum zur Verfügung steht. Die Verbesserung gegenüber FAT16 besteht hauptsächlich in der Unterstützung größerer Festplattenpartitionen (mehr als 2 GB) und in kleineren Speichersektoren, wodurch der ungenutzte Speicher vor allem bei kleinen Dateien reduziert wird.\n\nDie Online-Datenkomprimierung DriveSpace aus DOS 6.22/DOS 7 ist erstmals auch mit einer grafischen Bedienoberfläche konfigurierbar. Zusammen mit Microsoft Plus! wurde die Effektivität dieser Komprimierung durch bessere Algorithmen (HiPack und UltraPack) in DriveSpace 3 (die dritte Version) nochmals gesteigert und in Windows 95B auch ohne Zusatzsoftware ins Betriebssystem integriert. Die problematische Datensicherheit und andere Nachteile führten jedoch dazu, dass das Programm seit der Verfügbarkeit großer Festplatten zu günstigen Preisen schnell an Bedeutung verlor.\n\nUmfangreich sind auch die Neuerungen im grafischen Bereich, allen voran das \"Windows-Startmenü\". Microsoft hat mit Windows 95 die Grafische Benutzeroberfläche so weiterentwickelt, dass sie ähnlich zu bedienen ist wie das anfänglich gemeinsam mit IBM entwickelte Betriebssystem OS/2. Auch die Taskleiste am unteren Rand des Bildschirms war unter Windows neu. Klickt der Nutzer auf „Start“, so erhält er ein Menü, in dem er die verfügbaren Programme abrufen, die zuletzt benutzten Dokumente aufrufen, Einstellungen ändern, Hilfe aufrufen sowie den Computer ausschalten kann (oft zitiert wurde die kurios anmutende Aufforderung: „Klicken Sie auf \"Start\", um zu beenden“). Die Taskleiste (das „Band“ neben diesem „Start“-Knopf) zeigt die aktuell laufenden Programme an, mit einem Klick kann man zwischen diesen wechseln. Der alte Programm-Manager aus Windows 3.1 wurde ersetzt durch den so genannten Desktop, eine Oberfläche, auf der sich mit entsprechenden Anwendungen verknüpfte Symbole („Verknüpfungen“) befinden. Der alte Programm-Manager war dennoch ebenso wie der alte Datei-Manager im Lieferumfang enthalten, die entsprechenden Programmdateien befinden sich als „Progman.exe“ und „Winfile.exe“ im Windows-Installationsverzeichnis. Bei der Installation des ersten Windows-95-Betriebssystems (Windows 95A) konnte man die alte Benutzeroberfläche sogar noch alternativ als Standardoberfläche auswählen.\n\nDer von Windows 3.x bekannte Datei-Manager wurde durch den neuen Windows-Explorer ersetzt. Neben der eigentlichen Dateiverwaltung ist er auch für die Symbole (auf dem Desktop), die Fenster, die Taskleiste und einiges mehr zuständig. Neu für Windows sind auch die Kontextmenüs. So kann man praktisch alles mit der rechten Maustaste anklicken, um zu sehen, welche Aktionen man auf dem jeweiligen Objekt durchführen kann; so zeigen sich beispielsweise Unterschiede der möglichen Aktionen im Kontextmenü zwischen Textdateien und etwa Worddokumenten. Unter Windows 3.x ist die rechte Maustaste – im Gegensatz zu vielen Anwendungsprogrammen, beispielsweise WordPerfect – meist ohne Funktion.\n\nAbgesehen vom \"Datei-Manager\" (ein Überbleibsel von Windows 3.x) ist Windows 95 vollständig Jahr-2000-kompatibel. Für diesen ist jedoch von Microsoft ein Update erschienen. Im Service Pack 1 (etwa Februar/März 1996) befindet sich bereits der nachinstallierbare Internet Explorer, Version 2.0.\n\nMit Windows 95 gab es erstmals WordPad, in allen Vorgängerversionen war nur der WordPad-Vorgänger Microsoft Write enthalten.\n\nIn der Betaversion können mit dem virtuellen Gerätetreiber „cdfs.vxd“ (Größe: 77,2 KB) Musik-CDs noch wie ein gewöhnlicher Windows-Ordner geöffnet werden. Dort werden die einzelnen Musikstücke als kopierbare WAV-Dateien in Mono und Stereo in jeweils drei Qualitätsstufen angezeigt. Ein Rippen von Musikdateien war damit nicht nötig. Die „cdfs.vxd“ wurde in der Verkaufsversion durch eine nur noch 57,7 KB große Datei ersetzt, die nur noch Verknüpfungen anzeigt (*.cda-Dateinamen). Die „cdfs.vxd“ der Beta-Version war bis einschließlich Windows ME funktionsfähig. Sie wurde von verschiedenen Computerzeitschriften in beigelegten CD-ROMs oder online zum Download angeboten.\n\nNeben der ebenfalls neuen (und wegen Fehlern fast unbrauchbaren) USB-Unterstützung unterstützt das Betriebssystem ab der B-Version erstmals auch AGP-Grafikkarten.\n\nWindows 95 hatte seit jeher Probleme mit der stetig wachsenden Leistungsfähigkeit der Hardware. Bei zu schnellen Prozessoren kam es aufgrund eines Timingfehlers zu einem Absturz; dieser Fehler wurde, da er zuerst beim AMD K6 auftrat, auch „AMD-K6-Bug“ genannt. Ein weiterer Fehler in einer anderen Systemkomponente, der von Microsoft nicht behoben wurde, sorgt für einen Absturz, wenn der Prozessor schneller als 2,1 GHz ist. Auch werden Festplatten, die größer als 32 GB sind, von Windows 95 nicht unterstützt.\n\nVon „Windows 95“ wurden vier Versionen entwickelt, von denen sich die letzte nochmals in verschiedene Versionen unterteilt. Allerdings war nur die erste Version im Handel erhältlich, wahlweise auch als Diskettensatz, alle anderen waren nur als OEM-Versionen vorinstalliert und mit neu gekauften Rechnern und auf CD-ROM (nicht bootfähig, mit zusätzlicher Startdiskette) erhältlich.\n\nUnter DOS melden sich alle OSR 2.x-Versionen mit 4.00.1111. Unter Windows ohne USB-Unterstützung ebenfalls, sie sind nur am „B“- bzw. „C“-Eintrag erkennbar.\n\nNach der Entwicklung von Windows 95 (Ur- bzw. A-Version) erschien eine Systemaktualisierung unter dem Codename „Nashville“. Sie ist eine unter Windows 95 installierbare Betaversion, die sich in der Software-Systemsteuerung als „Windows 96“ ausweist. Sie kam später jedoch nicht unter diesem Namen in den Handel, sondern wurde als aktualisiertes Windows 95B verkauft. Die Neuerungen betrafen hauptsächlich die Unterstützung neuer Hardware, wie Infrarot- und USB-Schnittstellen.\n\nFür Windows 95 (Ur-Version) und Windows 95a (OEM-Service Release 1) gelten folgende Mindest-Systemvoraussetzungen:\n\nFür Windows 95b und Windows 95c (OEM-Service Release 2, 2.1 und 2.5) gelten folgende Mindest-Systemvoraussetzungen:\n\nWindows 95 brachte nicht nur Neuerungen, sondern auch Probleme mit sich. Ziel der Architektur war vollständige 16-Bit-Kompatibilität zu Windows 3.11 und DOS unter gleichzeitiger Verwendung der neuen 32-Bit-Architektur, was jedoch nur teilweise erreicht wurde. Auch aufgrund dieses Kompatibilitätsansatzes reichte Windows 95 bei Weitem nicht an die Stabilität der Windows-Versionen der NT-Linie heran.\n\nIm Vergleich zu Apples Mac OS Classic wurde resümiert, dass diese Betriebssysteme bei Windows 95 neue Features wie das Kopieren von Dateien per Kopieren und Einfügen schon „seit Jahren“ können. So breche „keine neue Ära“ an, auch wenn das Betriebssystem schon vor Verkaufsstart eine hohe Medienpräsenz, auch dank großer Werbeaufgaben seitens Microsofts, zeigte (siehe den Abschnitt „#Mediale Einführung und Startmelodie“). \n\nDurch die Unterstützung sowohl von alten 16-Bit- als auch von neuen 32-Bit-Programmen wurde der Kernel signifikant komplexer als bei der Vorgängerversion 3.1x, was in deutlich geringerer Ausführungsgeschwindigkeit von 16-Bit-Programmcode – insbesondere beim Bildschirmaufbau – resultiert. Die Windows-Kerneldateien greifen bei 16-Bit-Programmcode weiterhin wie bei DOS oder Windows 3.1 auf grundlegende Ein-/Ausgabefunktionen des DOS-Systemkernels IO.SYS zu.\n\nBei der B- und C-Version gibt es zudem einige Probleme mit der vorher nicht vorhandenen USB-Unterstützung, die sich als fehlerhaft erwies. Auch einige Grafikkartentreiber verweigern unter Version C ihren Dienst, laufen jedoch mit der älteren Version B anstandslos.\n\n"}
{"id": "5575", "url": "https://de.wikipedia.org/wiki?curid=5575", "title": "Microsoft Windows 98", "text": "Microsoft Windows 98\n\nWindows 98 (Codename: \"Memphis\") ist ein ab 25. Juni 1998 von Microsoft vertriebenes Betriebssystem. Microsoft beendete den Support für Windows 98, Windows 98 SE und Windows ME ab 11. Juli 2006. Diese Betriebssysteme sind im Wesentlichen eine stetige Weiterentwicklung von Windows 95 (1995).\n\nDie Beta-Version „Memphis“ durchlief insgesamt drei Beta-Phasen und eine Pre-Beta-Phase. Die Pre-Beta erschien im Dezember 1996 und ist ab einem bestimmten Datum nicht mehr lauffähig. Als Boot-Logo der Pre-Beta wird „Microsoft Memphis Developer Release“ angezeigt. Sie bietet keine sichtbaren Neuerungen, selbst in den Systemeigenschaften heißt sie noch Windows 95. Allerdings bietet sie z. B. USB-Unterstützung.\n\nSpäter erschien die Beta 1. Sie beinhaltet folgende Neuerungen:\n\nMit der Beta 2 wurde „Memphis“ in Windows 98 umbenannt. Da ursprünglich bereits ein Release im Jahr 1997 geplant war, war vorher der Name Windows 97 geplant. Das Setup hat (bis auf ein paar Bilder während des Kopiervorgangs) das Aussehen des Setups der finalen Version. Das Boot-Logo trägt unter dem Schriftzug „Windows 98“ den Untertitel „Microsoft Internet Explorer“, der Untertitel wurde in der finalen Version entfernt. Der Internet Explorer 4.0 ist enthalten und bringt alles mit, was in der finalen Version dabei ist:\n\nZudem befindet sich in der Systemsteuerung der Punkt „Benutzerverwaltung“. Außerdem können Ordner mit einem Klick geöffnet werden.\n\nUrsprünglich sollte Windows 98 im November 1997 – und nicht ein 95 C – veröffentlicht werden, allerdings verschob sich dieser Termin bis zum Frühling 1998. Microsoft verkündete offiziell, dass diese Verzögerung dazu diene, ein Upgrade von Windows 3.1 auf Windows 98 zu ermöglichen, in Wirklichkeit integrierte Microsoft währenddessen ihren Internet Explorer 4.01 unlöschbar tief ins Betriebssystem, um so Netscape im laufenden Browserkrieg aggressiv vom Markt zu verdrängen. Erst unter Schadensersatz-Klagen gegen Microsoft im Jahr 1999 wurden unfaire Details öffentlich.\n\nWindows 98 ist, wie bereits sein Vorgänger, ein 16-Bit/32-Bit-Hybrid-System. Es verwendet, wie alle Betriebssysteme der Win9x-Reihe, ein 16-Bit MS-DOS-Betriebssystem zum Start und basiert teilweise darauf. Windows 98 unterstützt, wie schon der Vorläufer Windows 95, echtes präemptives Multitasking. Das bedeutet, dass das Betriebssystem bei der Zuteilung von CPU-Zeit an aktive Programme eine strikte Kontrolle durch feste Zeitscheiben ausübt. Eingeschränkt ist diese Konsequenz nur durch Hardwarezugriffe, die länger als die zugeteilte Zeitscheibe dauern, z. B. Timeouts. Konsequenter Speicherschutz ist bei Windows 98 nicht gewährleistet, sondern erst ab NT/2000/XP/Vista (dort mit dem Nachteil des Verlustes der Abwärtskompatibilität zu älterer Software, die direkten Zugriff auf physikalische Adressen benötigt). Windows 98 ermöglicht mit dem mitgelieferten MS-DOS 7.1 den Betrieb von DOS- und damals noch nicht häufigen 32-Bit Windows-Programmen. Windows 98 ist wesentlich größer als sein Vorgänger. Windows 98 erstellt beim ersten erfolgreichen Systemstart eines jeden Tages automatisch oder manuell eine Sicherungs-Kopie der Registrierung. Es verkaufte sich als Upgrade zu Windows 95 von Anfang an gut.\n\nWindows 98 ist das erste DOS-basierte (größtenteils) 32-Bit Betriebssystem, das im Gegensatz zu Windows 95 nicht mehr einzelne Hintergrundprogramme im 16-Bit-Modus ausführt. Aber selbst hier kommen immer noch einzelne Dienstprogramme im 16-Bit-Modus zum Einsatz.\n\nMit Windows 98 führte Microsoft das „Win32 Driver Model“ ein („WDM“), welches auf dem Gerätetreibermodell von NT basierte. Das 2001 folgende Model hieß Windows Driver Foundation.\nWeitere Neuerungen waren z. B. bessere AGP- und USB-Unterstützung (beides bereits ab Windows 95 B, aber die USB-Unterstützung ist dort so fehlerhaft, dass die meisten Hardwaretreiber sich erst ab Windows 98 installieren lassen), Unterstützung von ACPI, Festplattenpartitionen größer als 2 GB mit FAT32-Dateisystem (bereits ab Windows 95 B). Einige Funktionen, die sich bei Windows 95 mit dem Internet Explorer 4.0 nachrüsten lassen, sind bei Windows 98 bereits integriert. Dazu gehören die Integration des Internet Explorers in die Benutzeroberfläche, der Active Desktop, der verbesserte Windows Explorer (bessere Bedienung, UNC-Pfade und Netzwerkrechner lassen sich über die Adresszeile ansteuern).\n\nFerner war Windows 98 das erste grafische Betriebssystem von Microsoft, das mehrere Monitore unterstützte.\n\nWindows 98 SE \"(Second Edition)\" (Build 2222) wurde am 5. Mai 1999 veröffentlicht. Die deutsche Version war am 10. Juni 1999 verfügbar.\n\nEntscheidende Verbesserungen gegenüber der Erstausgabe sind unter anderem – neben einer weiter verbesserten USB-Unterstützung – wesentliche Erweiterungen in der Netzwerkunterstützung, wie z. B. die sogenannte Internetverbindungsfreigabe (ICS), welche die gemeinsame Nutzung einer einzigen Verbindung ins Internet durch mehrere Rechner ermöglichte. Allerdings war diese Funktion problembehaftet, schwer zu aktivieren und nicht immer stabil. Weiter neu war die Möglichkeit einer unbeaufsichtigten Installation.\n\nAndere SE-Funktionen wie DirectX 6.1, Internet Explorer 5.0, Windows Media Player 6.1, MDAC (Datenbankanbindung), MSI (Microsoft-Installer) etc. sind im Vorgänger installierbar.\n\nAls Voraussetzungen gibt Microsoft an:\n\nDiese Systemvoraussetzungen nennt Microsoft als Mindestvoraussetzungen für Installation und (sinnvollen) Betrieb. Tatsächlich kann das System mit noch geringerer Ausstattung betrieben werden oder (insbesondere unter Zuhilfenahme eines anderen PCs) auf anderem Weg installiert werden. So ist, mithilfe von ein paar Modifikationen, auch eine Installation über oder sogar auf einem beliebigen USB-Stick möglich; Voraussetzung ist eine nicht vorhandene RAM-Begrenzung im MS-DOS, die wiederum vom Prozessor abhängig ist. Bei USB-Sticks, die sich als USB-Festplatte ausgeben, ist eine Installation immer möglich, da diese für DOS normale Festplatten sind.\n\n2003 entschied Microsoft in Anbetracht der häufigen Nutzung des Systems weltweit, den Support mit Patches statt wie geplant bis Januar 2004 bis zum 11. Juli 2006 zu liefern. Danach wurde zudem die Windows-Update-Funktionalität beendet, wodurch auch die bereits erschienenen Updates nicht mehr über Windows Update bezogen werden konnten.\n\nAlle als PC-kompatibel geltende Chipsätze und gängige Hardware, die zum Zeitpunkt der Auslieferung auf dem Markt waren, funktionieren mit Windows-Standardtreibern. Neuere Hardware läuft ebenfalls unter Windows 98, sofern ein solcher Support durch den Hersteller vorgesehen ist. Ab Mitte 2006 unterstützen Hauptplatinen-Chipsätze Windows 9x nicht mit angepassten Treibern.\n\nEine Besonderheit der \"Second Edition\" ist, dass diese sowohl alte Windows Gerätetreiber als auch neue WDM-Treiber unterstützt. In Windows Me werden nur WDM-Treiber unterstützt. Damit bietet SE die Möglichkeit, sowohl alte Hardware, für die es keine WDM-Treiber gibt, als auch neue Hardware mit WDM-Treibern zu kombinieren oder aber je nach Stabilität den alten Windows-Treiber oder den neueren WDM-Treiber einzusetzen.\n\nWindows 9x kann MBR-formatierte Festplatten über 128 GB mit den Standardtreibern ohne 48-Bit-LBA-Unterstützung ansteuern, aber ein Schreibzugriff auf eine Datei oberhalb dieser Grenze führt durch Überschreiben zum Datenverlust. Die 128 GB gelten dabei pro tatsächlich vorhandener Festplatte (physikalisches Laufwerk) und nicht pro partitioniertem logischem Laufwerk. Das gilt auch für extern angeschlossene USB- und FireWire-Festplatten. Einige Hersteller bieten Treiber an, mit denen der Betrieb großer Medien problemlos möglich ist.\n\nUm mehr als 512 MB Arbeitsspeicher betreiben zu können, sind kleine Veränderungen, z. B. an der codice_1, notwendig. Dazu wird der VCache begrenzt. Ab 1 bis 2 GB muss der adressierbare physische Speicher MaxPhysPage begrenzt werden, um die Stabilität des Systems aufrechtzuerhalten.\n\nEtliche Softwarehersteller haben Windows 9x Jahre weiter unterstützt, nachdem Microsoft den Support beendet hatte. Die letzte installierbare Version von Microsoft Office ist Office XP, OpenOffice.org unterstützt Windows 98 bis einschließlich Version 2.4.3. Internet Explorer 6 ist die letzte Version für Windows 98. Firefox 2 wurde bis zur Version 2.0.0.20, Opera bis zur Version 10.5 gepflegt. Die letzte Flash-Version ist 9.262.\n\n2005 gab es inoffiziellen Support in Form von Service-Packs, die von der Windows-98-User-Community erstellt wurden. Die Supportseite von Creopard wird weiterhin aktuell gehalten.\n\nDurch Verwendung des Open-Source-Patchs \"KernelEx\" ist es möglich, manche nur für Windows XP geschriebene Programme unter Windows 98 zu verwenden (etwa Firefox 3). Das wird durch umfangreiche Anpassungen von Windows-DLL-Systemdateien erreicht, mit denen fehlende API-Funktionen nachgerüstet werden. Da KernelEx ein inoffizieller Patch ist, gibt es keine offizielle Unterstützung und der Benutzer arbeitet auf eigenes Risiko.\n\nEine direkte Installation und Ausführung von Windows 98 auf aktueller Hardware ist mangels Treiber und inkompatibler Geräte, vor allem aber zu moderner PC-Hardware (Arbeitsspeicher von über 1,5 GB, EFI statt BIOS, NVMe) nur mehr stark eingeschränkt oder gar nicht mehr möglich.\n\nWenn das EFI mit einem CSM ( – eine BIOS-Emulation) ausgestattet ist, kann Windows 98 unter Umständen in diesem Modus betrieben werden. Möglicherweise fehlen jedoch Treiber für Chipsätze, Grafikkarten oder verbaute Controller-Chips wie USB-Hostcontroller, womit ein Teil der Hardware nicht benutzbar ist.\n\nWindows 98 kann auch in einer virtuellen Maschine oder mit einem Emulator betrieben werden, falls diese Windows 98 unterstützen.\n\nBei der Vorführung des Betriebssystems durch Bill Gates auf der CES 1998 stürzte das Betriebssystem beim Anschluss eines Scanners über USB mit einem Blue Screen of Death ab, was sichtlich zum Amüsement des Publikums beitrug.\n\n"}
{"id": "5578", "url": "https://de.wikipedia.org/wiki?curid=5578", "title": "Microsoft Windows Millennium Edition", "text": "Microsoft Windows Millennium Edition\n\nMicrosoft Windows Me ist das letzte von Microsoft veröffentlichte Betriebssystem aus der Windows-9x-Linie, die auf MS-DOS aufsetzt. Die Abkürzung \"Me\" steht für die offizielle Schreibweise \"Millennium Edition\" (zu Deutsch \"Jahrtausend-Ausgabe\"). \n\nUrsprünglich sollte Windows 98 das letzte Betriebssystem der Windows-9x-Linie werden; ein Nachfolger von \"Windows NT 5.0\", dem späteren Windows 2000, sollte die NT- und 9x-Linien zusammenführen. Die Entwicklung von NT 5.0 war jedoch von massiven Verzögerungen betroffen, die auch die Veröffentlichung des Nachfolgers in weite Ferne rücken ließ. Im März 1999 organisierte Microsoft seine Unternehmensstruktur neu und spaltete vom bisherigen Windows-Team, das mit der Entwicklung von Windows 2000 beschäftigt war, ein Entwicklerteam ab, das sich auf Windows für Heimanwender konzentrieren sollte. Die Öffentlichkeit interpretierte dies zunächst als Plan, eine Version von Windows 2000 für Heimanwender zu entwickeln, aber am 7. April 1999 kündigte Microsoft völlig überraschend an, nun doch einen Nachfolger von Windows 98 zu veröffentlichen, der unter der Bezeichnung \"Millennium\" bekannt wurde. Dieser plötzliche Umschwung hatte mehrere Gründe. Die Systemanforderungen von Windows 2000 galten weiterhin als zu hoch für Heimanwender, und die Hardware- und Softwareunterstützung war nicht so gut wie bei Windows 9x. Außerdem benötigte die Zielgruppe der Heimanwender nach Ansicht der Entwickler keine der erweiterten Funktionen von Windows 2000 wie etwa einen Verzeichnisdienst, die das Betriebssystem nur aufblähten.\n\nKurz darauf versuchten die Entwickler, Ideen zu sammeln, die in das neue Produkt einfließen könnten. Diese resultierten in der ersten Vorversion des Betriebssystems, das am 23. Juli 1999 präsentiert wurde. Die Ziele, die sich das Entwicklerteam setzte, waren eine bessere Multimediaunterstützung, eine tiefere Einbindung in das Internet und eine einfachere Einrichtung von Heimnetzwerken. Das Produkt sollte funktionieren, ohne dass sich der Benutzer tiefer mit dem Betriebssystem beschäftigen muss. Um dies zu gewährleisten, sollte das Betriebssystem sogenannte Aktivitätszentren (englisch \"\") beinhalten. Diese sollten durch eine intuitive, HTML-basierte Bedienung auch seltener benutzte Funktionen, wie das Bearbeiten von Bildern, vereinfachen. Aufgrund von Problemen mit der Einbindung dieser Aktivitätszentren in das Betriebssystem musste das Konzept jedoch verworfen werden; lediglich die Windows-Hilfe sowie die Systemwiederherstellung stellen letzte Überreste der Aktivitätszentren dar. \n\nIm September 1999 folgte der erste Betatest des Betriebssystems. Ausführliche Berichte über das Betriebssystem lehnte Microsoft jedoch mit der Begründung ab, diese Beta-Version stelle noch nicht den Funktionsstand der Endversion dar. Dies änderte sich erst am 24. November 1999, als die zweite Beta-Version des Betriebssystems veröffentlicht wurde. Die Entwicklung stockte kurz danach, da sich die Entwickler entschieden, den TCP/IP-Protokollstapel von Windows 2000 zu portieren, was viel Zeit beanspruchte. Am 1. Februar 2000 schließlich kündigte Microsoft den endgültigen Namen des Betriebssystems, \"Microsoft Windows Millennium Edition\", an. Die enge Konzentrierung auf Heimanwender brachte dem Betriebssystem während der Entwicklung Kritik von zahlreichen Fachzeitschriften ein. Ursprünglich sollte das Betriebssystem nicht an MSDN-Abonnenten verteilt werden, was Microsoft aber nach Protesten änderte. Auch der NetWare-Client würde nun doch Bestandteil des Betriebssystems werden. \n\nAm 11. April 2000 folgte ein dritter Betatest. Probleme mit dem Windows Media Player und dem Internet Explorer sorgten für Verzögerungen im Entwicklungsprozess, sodass das Entwicklungsstadium nicht wie ursprünglich vorgesehen im Mai, sondern erst am 19. Juni 2000 erreicht wurde. Die Veröffentlichung des Betriebssystems folgte am 14. September 2000. Das Betriebssystem kostete, genau wie Windows 98 zuvor, 209 US-Dollar als Vollversion und 109 USD als Upgrade-Variante. Für Benutzer von Windows 98 gab es ein Sonderangebot, sodass sie die Upgrade-Version bereits für 60 USD erwerben konnten.\n\nDer Mainstream-Support für Windows Me lief am 31. Dezember 2003 aus. Ursprünglich sollte der Extended Support ein Jahr später enden, Microsoft verlängerte jedoch den Support von Windows Me zusammen mit Windows 98 bis zum 30. Juni 2006.\n\nMicrosoft behauptete anfangs, dass Windows Me, ähnlich wie die Betriebssysteme der Windows-NT-Reihe, nicht mehr auf DOS basiere. Dies stellte sich jedoch schnell als falsch heraus, wenngleich Microsoft zahlreiche Möglichkeiten, den MS-DOS-Modus aufzurufen, aus dem Betriebssystem entfernte. Das in Windows Me vorhandene MS-DOS wurde mit dem Ziel einer kürzeren Startzeit modifiziert. Die Dateien AUTOEXEC.BAT und CONFIG.SYS werden vom Betriebssystem ignoriert und nur Definitionen von Umgebungsvariablen werden ausgewertet, indem diese Einstellungen in die Windows-Registrierungsdatenbank übertragen werden. Die vormals eigenständigen Dateien HIMEM.SYS und SmartDrive wurden in die IO.SYS integriert und diese komprimiert, sodass sie schneller in den Arbeitsspeicher geladen wird. Die Windows-Registrierungsdatenbank selbst wurde ebenfalls optimiert; von den bisherigen Dateien \"SYSTEM.DAT\" und \"USER.DAT\" wurde die \"CLASSES.DAT\" abgespalten, die den Inhalt des Hive codice_1 enthält. So werden während des Startvorgangs nur die nötigen Teile der Registrierung geladen.\n\nWindows Me enthält einige neue Programmierschnittstellen, die vor allem auf die Bedürfnisse von Heimanwendern abzielen. Windows Image Acquisition (WIA) dient dem automatischen Erkennen von Scannern und Digitalkameras, etwa um den Assistenten zum Einscannen eines Bildes zu starten. Auch das Versenden von Bildern per E-Mail direkt vom Scanner, ohne das Bild auf der Festplatte speichern zu müssen, ist so möglich. Über DirectPlay Voice können Spieler über ein Mikrofon direkt miteinander reden, sofern das Spiel für diese Schnittstelle programmiert wurde. Mithilfe von Universal Plug and Play (UPnP) können kompatible Geräte vom Betriebssystem aus konfiguriert werden.\n\nBereits beim ersten Start des Betriebssystems wird ein Assistent geladen, der den Benutzer durch die ersten Schritte der Einrichtung des Betriebssystems führt. Die Figur \"Merlin\", ein Zauberer aus Microsoft Agent, dient während dieses Prozesses als Hilfefunktion. Neben einem Tutorial zur Benutzung der Maus sind dies insbesondere die Regionseinstellungen und die Eingabe des Lizenzschlüssels. Danach geht der Startprozess nahtlos in einen Willkommensbildschirm über, der die neuen Funktionen von Windows Me vorstellt.\n\nDie Benutzeroberfläche von Windows Me entspricht größtenteils dem Windows-2000-Pendant. Windows Me enthält den Internet Explorer 5.5 sowie den Windows Media Player 7.0, der sich stark von der vorherigen Version unterscheidet. Neu ist der Windows Movie Maker, ein einfaches Videoschnittprogramm, mit dem Videos aufgenommen und bearbeitet werden können. Aus \"Plus! für Windows 98\" übernommen wurde die Funktion Komprimierte Ordner, die ZIP-Dateien auch ohne Programme von Drittherstellern unterstützt. Zu den Spielen wurden, neben dem ebenfalls aus Plus! stammenden Spiel \"Spider Solitär\" sowie dem Spiel \"3D Pinball: Space Cadet\" aus Windows NT 4.0, einige simple Online-Spiele hinzugefügt, die nur im Internet gespielt werden können. Ein neuer Assistent erleichtert das Einrichten eines Heimnetzwerkes. Die Hilfefunktion wurde in Windows Me komplett überarbeitet, sie vereint nun die Hilfedateien aller Windows-Bestandteile und erlaubt auch das Stellen von Supportanfragen über das Internet.\n\nÄhnlich wie Windows 2000 enthält Windows Me die Systemdateiüberprüfung, die wichtige Systemdateien des Betriebssystems überwacht und diese durch Sicherungskopien ersetzt, wenn sie verändert oder gelöscht werden. Die neu eingeführte Systemwiederherstellung sichert in regelmäßigen Abständen, wenn Anwendungen installiert werden oder auf Wunsch des Benutzers die wichtigsten Dateien des Betriebssystems und erlaubt im Bedarfsfall die Wiederherstellung auf einen früheren Stand. Sie ersetzt das ältere Programm Microsoft Backup, welches dennoch auf der Windows Me-CD vorhanden ist. Über die neue Funktion \"Automatische Updates\" kann das Betriebssystem automatisch auf den neuesten Stand gehalten werden.\n\nDie Mindestvoraussetzungen zur Installation von Windows Me sind ein Intel-Pentium-Prozessor mit 150 MHz, 32 MB Arbeitsspeicher, 320 MB freier Festplattenspeicher, ein CD-ROM-Laufwerk sowie ein Diskettenlaufwerk, eine VGA-kompatible Grafikkarte und eine Soundkarte mitsamt Lautsprecher. Neben der Vollversion war auch eine Upgrade-Version erhältlich, mit der ein bestehendes Windows 95 oder Windows 98 auf Windows Me aktualisiert werden konnte.\n\nDie Resonanz auf Windows Me war zunächst gemischt. Es wurde bemängelt, dass zahlreiche Neuerungen von Windows Me, wie etwa der neue Internet Explorer, auch für ältere Betriebssysteme verfügbar seien, wodurch es weniger Anreize für eine Aktualisierung des Betriebssystems gäbe. Kritisiert wurde zudem die Tatsache, dass sich der Windows Media Player und der Movie Maker nicht deinstallieren ließen, und das obwohl genau zu dieser Zeit Microsoft rechtliche Konsequenzen wegen Ausnutzung seiner Monopolstellung im Browserkrieg drohten.\n\nBald nach der Veröffentlichung jedoch kippte die Meinung stark ins Negative, denn zahlreiche Fehler im Betriebssystem brachten Windows Me einen schlechten Ruf ein. Bereits am Tag der Veröffentlichung wurde eine Sicherheitslücke bekannt, durch die Windows Me zum Absturz gebracht werden konnte. Vor allem Instabilität und Kompatibilitätsprobleme mit Anwendungen und Treibern sorgten für Unmut bei den Anwendern. Aber auch neue Funktionen von Windows Me waren von den Fehlern betroffen; die Systemwiederherstellung etwa stellte aufgrund eines Fehlers ihren Dienst nach dem 8. September 2001 ein, sodass neuere Wiederherstellungspunkte nicht mehr funktionierten.\n\n"}
{"id": "5579", "url": "https://de.wikipedia.org/wiki?curid=5579", "title": "Microsoft Windows NT", "text": "Microsoft Windows NT\n\nWindows NT (ursprünglich von \"N-Ten\", einem Simulator, auf dem das System in der Anfangsphase betrieben wurde und später für \"New Technology\") ist ein Kernel, der bei Betriebssystemen der Windows-Reihe des US-amerikanischen Unternehmens Microsoft zum Einsatz kommt. Seit seiner Version 5.0 wird Windows NT nicht mehr als Teil des Produktnamens, sondern nur noch als internes Versionskürzel verwendet.\n\nDie Entwicklung an Windows NT begann, als die Allianz zwischen dem US-amerikanischen Unternehmen IBM und Microsoft zur Entwicklung des Betriebssystems OS/2 zerbrach.\n\nLeiter des NT-Projekts wurde David N. Cutler. Er galt als einer der renommiertesten Entwickler von Betriebssystemen überhaupt und war maßgeblich an der Entwicklung des Betriebssystems VMS beteiligt gewesen, weshalb der Windows NT-Kernel viele Ähnlichkeiten mit VMS aufweist. Microsoft warb ihn und Mitglieder seines Teams von DEC ab und setzte sie auf die Entwicklung eines neuen Betriebssystems an. Diese Abwerbung beantwortete DEC mit einer Klage, die Microsoft durch die Zahlung von 150 Millionen US-Dollar und die Zusage, mit Windows NT auch Alpha-Prozessoren zu unterstützen, beilegen konnte.\n\nCutler setzte sich zwei wesentliche Ziele für Windows NT. Ihm ging es darum, \"Zuverlässigkeit\" zu erreichen – eine abstürzende Anwendung sollte nicht mehr das gesamte System zum Absturz bringen können. Diese Stabilität war unter Betriebssystemen wie VMS oder unixoiden Systemen längst üblich. Auch wichtig war \"Portabilität\" – Windows NT sollte auf allen modernen Computerarchitekturen lauffähig sein. Außerdem sollte Windows NT, ähnlich wie es der Mach-Kernel konnte, als Basis für verschiedene Betriebssysteme gleichzeitig dienen und so z. B. Windows-, MS-DOS-, OS/2- und POSIX-Programme gleichzeitig ablaufen lassen können. Der Arbeitstitel während der Entwicklung hieß demnach auch \"Portasys.\"\n\nNach Aussage des früheren Microsoft-Mitarbeiters Mark Lucovsky stand NT ursprünglich für \"N-Ten.\" Dies war der Codename für den in Entwicklung befindlichen Prozessor Intel i860. Er war als Plattform für NT gedacht, lag jedoch nicht bei Microsoft vor. Deshalb wurde auf einem Emulator entwickelt. Zu Vermarktungszwecken wurde das Kürzel später in \"New Technology\" umgedeutet.\n\nDie erste ausgelieferte Version hatte die Versionsnummer 3.1. So sollte ein Bezug zu Windows 3.1 hergestellt werden, das die gleiche grafische Benutzeroberfläche besaß und beim Erscheinen von Windows NT die aktuell auf dem Markt erhältliche DOS-basierte Windows-Version darstellte.\n\nNach Windows NT 4.0 wurden das Kürzel NT und die Versionsnummer im Produktnamen fallen gelassen. Die Nachfolgeversionen werden Windows 2000, Windows XP, Windows Server 2003, Windows Vista, Windows Server 2008 (sowie R2), Windows 7, Windows 8, Windows Server 2012 (sowie R2), Windows 8.1, Windows 10 und Windows Server 2016 genannt. Alle geben in der Umgebungsvariablen codice_1 als Betriebssystem codice_2 an. Windows 2000 weist noch im Startbildschirm mit dem Text „Auf NT-Technologie basierend“ auf die Verwandtschaft hin.\n\nCutler hatte seine zwei Primärziele erreicht: Das neue Betriebssystem war stabil, lief aufgrund seiner modularen Entwicklung auf mehreren Plattformen (MIPS und x86, später auch PowerPC und Alpha) und bot verschiedenen Programmarten Unterstützung. Es liefen sowohl 16-Bit-Windows-3.x-Programme als auch Programme für das neue 32-Bit-Windows-NT-API sowie textbasierte OS/2-Software und POSIX-1.0-kompatible Programme. Über die Jahre fand hier allerdings wieder eine Rück- bzw. Weiterentwicklung statt. Die OS/2- und POSIX-Versionen wurden zunächst nicht weiter gepflegt und später entfernt. Die Versionen für PowerPC, MIPS und Alpha wurden eingestellt, dafür kamen später IA-64- und x64-Versionen und mit Windows RT auch eine ARM-Version hinzu, wobei letztere die Ausführung von Win32-Anwendungen, die nicht durch Microsoft signiert worden sind, nicht mehr unterstützt.\n\nIn den ersten NT-Versionen läuft das GDI zusammen mit den anderen Subsystemen auf Ring 3 der Intel-Privilegstufe außerhalb des Kernel-Bereichs. Dadurch ist der Kernel selbst vor Abstürzen in den Programmen geschützt. Ab NT 4.0 läuft das Grafiksubsystem aus Geschwindigkeitsgründen teilweise direkt im Kernel, womit Fehler in Grafiktreibern moderne Windows-NT-Versionen zum Absturz bringen können. Windows Vista verwendet mit dem neuen Grafiktreiber-Modell allerdings wieder Userspace-Treiber.\n\nWindows NT besitzt einen modularen Aufbau. Die unterste Ebene bildet die Hardwareabstraktionsschicht (engl. \"Hardware Abstraction Layer,\" abgekürzt HAL). Darauf bauen der eigentliche Kernel (ein Hybridkernel) und die Subsysteme auf. Der Kernel kümmert sich um die Vergabe von Arbeitsspeicher und Rechenzeit. Auf den Kernel setzen die Subsysteme auf. Dem Win32-Subsystem kommt dabei die größte Bedeutung zu, da es sich auch um den Aufbau der grafischen Benutzeroberfläche kümmert und die Signale der Eingabegeräte verarbeitet. In den Enterprise- und Ultimate-Editionen von Windows Vista sind die Microsoft Windows Services for UNIX in Form eines POSIX-kompatiblen Subsystems für UNIX-basierte Applikationen enthalten.\n\nAus Kompatibilitäts- und Geschwindigkeitsgründen, vor allem für Spiele, entwickelte Microsoft die DOS-basierte Betriebssystemlinie Windows 3.x/9x neben NT zunächst weiter. Erst mit dem Erscheinen von Windows XP wurde die DOS-basierte Linie aufgegeben, wobei Windows XP (wie der Vorläufer Windows 2000) einen reinen NT-Kernel hat.\n\nBereits die erste Windows-NT-Version war vollständig von MS-DOS losgelöst. Aus Gründen der Abwärtskompatibilität konnten allerdings ältere 16-Bit-DOS-Programme wie beispielsweise der MS-DOS-Kommandozeileninterpreter COMMAND.COM in einer Virtual DOS Machine ausgeführt werden. Programme, die direkt (also ohne das Subsystem von Windows) auf die Hardware zugreifen, werden aus Sicherheitsgründen nicht mehr ausgeführt. Zusätzlich stand dem Anwender ein weiterentwickelter, vollständig 32-Bit-fähiger Kommandozeileninterpreter namens cmd.exe zur Verfügung. Außerdem unterstützte Windows NT bereits in der Version 3.1 das Dateisystem NTFS \"(New Technology File System)\" und verfügt seit jeher über einen 32-Bit-Kernel.\n\n RTM Build Final\n\n\n"}
{"id": "5580", "url": "https://de.wikipedia.org/wiki?curid=5580", "title": "Microsoft Windows 2000", "text": "Microsoft Windows 2000\n\nWindows 2000, kurz \"W2K\" oder \"Win 2k\" (von Kilo: „2k“ = 2000), ist ein Betriebssystem von Microsoft. Es ist eine Weiterentwicklung von Windows NT 4.0 und der Vorgänger von Windows XP. Die interne Bezeichnung bei Microsoft lautet Windows NT 5.0.\n\nDie Planungen für Windows NT 5.0, der ursprüngliche Name von Windows 2000, begannen kurz nach der Veröffentlichung von Windows NT 4.0. Mit dem neuen Betriebssystem wollte Microsoft die Administrationskosten für Unternehmen senken, das hauptsächlich durch die Einführung eines Verzeichnisdienstes namens Active Directory geschehen sollte. Das Betriebssystem sollte Ende 1997 veröffentlicht werden. Anfang 1997 verteilte Microsoft eine Vorversion von Active Directory an Entwickler, gleichzeitig kündigte das Unternehmen an, dass sich die Fertigstellung des Betriebssystems in das Jahr 1998 verschiebe. Diese erste Verzögerung wurde zunächst begrüßt, da die Presse sich davon ein stabileres Betriebssystem erhoffte und viele Unternehmen ohnehin mit der Migration auf den Vorgänger Windows NT 4.0 beschäftigt seien.\n\nIn darauffolgenden Presseständen von Microsoft, unter anderem auf der CeBIT im März 1997 und auf der WinHEC im Mai 1997, erläuterte das Unternehmen die Ziele des neuen Betriebssystems. Windows NT 5.0 sollte die Windows-9x- und Windows-NT-Linien vereinigen und in diesem Zuge Funktionen wie Plug and Play und USB-Unterstützung beinhalten. Neben der bisherigen 32-Bit-Version sollte es erstmals auch eine 64-Bit-Version von Windows für den Alpha-Prozessor von DEC und einem Prozessor von Intel mit dem Codenamen \"Merced\" (dem späteren Intel Itanium) geben. Wie NT 4.0 sollte NT 5.0 in einer Workstation-, Server- und Enterprise-Edition erscheinen. Microsoft lizenzierte am 12. Mai 1997 eine Mehrbenutzertechnologie von Citrix, die neben NT 4.0 (in Form der Terminal Server Edition) auch Bestandteil von NT 5.0 sein sollte.\n\nAuf der COMDEX im Frühjahr 1997 kündigte Microsoft einen Betatest im Zeitraum August/September und eine Veröffentlichung Anfang 1998 an, was in etwa dem Entwicklungszeitraum von Windows NT 4.0 entsprach. Das Betatest-Datum wurde später auf den September 1997 festgesetzt, was sich später als großer Fehler herausstellte, da die Entwickler weit hinter dem Zeitplan lagen und nicht in der Lage waren, in so kurzer Zeit eine Beta-Version des Betriebssystems mit den zuvor versprochenen Funktionen fertigzustellen. Als am 20. September 1997 die erste Beta-Version schließlich veröffentlicht wurde, galt sie als instabil und unausgereift; zahlreiche Neuheiten des Betriebssystems waren in dieser Vorversion nicht vorhanden oder funktionsuntüchtig. Der Termin für den zweiten Betatest, der für den 15. Dezember 1997 vorgesehen war, musste in das Jahr 1998 verschoben werden. In diesem Zuge war auch der geplante Veröffentlichungstermin Anfang 1998 nicht zu halten und musste zum Ende des Jahres verschoben werden. Einige Zeitschriften schrieben gar, dass mit einer Fertigstellung erst 1999 zu rechnen sei. Der zweite Betatest wurde zunächst für den April 1998 versprochen, aber auch dieser Termin fiel schließlich und so veröffentlichte das Unternehmen zunächst nur eine Vorversion im März.\n\nIm Februar 1998 bestätigte Microsoft, dass zu ambitionierte Ziele Schuld an den massiven Verzögerungen im Entwicklungsprozess seien. Zu den Plänen, die für Windows NT 5.0 vorgesehen waren, zählten etwa eine TV-Funktion (die später unter der Bezeichnung \"WebTV\" Bestandteil von Windows 98 wurde) sowie ein Projekt mit dem Codenamen \"Chrome\", das DirectX und HTML kombinieren sollte, um Multimediainhalte im Web bereitzustellen, aber letztendlich nie realisiert wurde. Spekulationen, wonach gar das von Anfang an versprochene Active Directory dem Entwicklungsprozess zum Opfer fallen könnte, dementierte Microsoft klar. Der zweite Betatest wurde auf den Juni 1998 festgesetzt, das Endprodukt sollte nunmehr tatsächlich Anfang 1999 erscheinen. Der endgültige Termin für den zweiten Betatest war, nach weiteren Verzögerungen, der 18. August 1998. Zwar enthielt diese Version laut Microsoft alle für das Endprodukt vorgesehenen Funktionen, sie galt jedoch auch als instabil und unausgereift. Aufgrund dessen plante Microsoft einen dritten Betatest zu einem noch unbestimmten Zeitpunkt.\n\nAm 27. Oktober 1998 wurde dann der Name \"Windows 2000\" durch Microsoft offiziell festgelegt. Dieser Schritt war in der Öffentlichkeit äußerst kontrovers, da Windows NT bislang der Name für Business-Betriebssysteme war, während hingegen die Bezeichnung Windows ohne Zusatz mit Consumer-Betriebssystemen assoziiert wurde. Diese Entscheidung sollte sich erst im Nachhinein als richtig herausstellen, denn viele Nutzer sahen Windows 2000 nach seiner Veröffentlichung als ein besseres Betriebssystem als Windows NT an, obwohl Windows 2000 letztlich auch nur eine Version von Windows NT ist. Gleichzeitig wechselten die drei Versionen des Betriebssystems ihren Namen; sie hießen nunmehr \"Professional\", \"Server\" und \"Advanced Server\". Der Windows 2000 Server würde anders als die bisherigen Server-Versionen von Windows NT nur noch zwei statt vier Prozessoren unterstützen, der Advanced Server nur noch vier statt acht Prozessoren. Neu angekündigt wurde der \"Datacenter Server\", eine Version für große Rechenzentren, die bis zu 16 Prozessoren und 64 Gigabyte Arbeitsspeicher unterstützen werde.\n\nIm Januar 1999 erklärte Microsoft, dass das Endprodukt erst am 25. Februar 2000 erscheinen werde; der dritte Betatest sollte im April 1999 stattfinden. Zu dieser erneuten Verzögerung trugen zahlreiche Faktoren bei: die parallel verlaufende Hardwareentwicklung und die dadurch entstehende Notwendigkeit, Treiber für diese neue Hardware zu schreiben (etwa den Pentium-III-Prozessor), Vorsorgemaßnahmen aufgrund des Jahr-2000-Problems, die zuvor erfolgte Namensänderung sowie die parallele Arbeit an der 64-Bit-Version. Aufgrund dessen entstanden kurzzeitig Gerüchte um eine Version des Betriebssystems, der zwar bestimmte Funktionen, wie das Upgrade einer bestehenden Windows-NT-Domäne auf Active Directory, fehlen würden, die aber die zeitliche Lücke bis zur endgültigen Veröffentlichung schließen sollte. Der dritte Betatest, an dem 650.000 Betatester teilnahmen, startete schließlich am 30. April 1999. Diese Version löste die Probleme, die in vergangenen Betatests auftraten, und brachte große Hoffnungen auf die Endversion.\n\nAm 1. Juli 1999 folgte der Release Candidate von Windows 2000. Auch wenn das Betriebssystem nochmals stabiler war als beim letzten Betatest, zeigten sich immer noch Probleme im Zusammenhang mit Active Directory. Am 18. August 1999 entschied Microsoft, dass die Server-Varianten von Windows 2000 die doppelte Anzahl an Prozessoren unterstützen würden – 4 beim Server, 8 beim Advanced Server und 32 beim Datacenter Server. Damit revidierte Microsoft seine frühere Entscheidung, die Anzahl der unterstützten Prozessoren im Vergleich zu NT 4.0 zu reduzieren und reagierte auf das zu erwartende Erscheinen von Systemen mit acht Prozessoren.\n\nDer zweite Release Candidate, der ursprünglich am 6. September folgen sollte, erschien schließlich wenige Tage später am 15. September. Doch auch dies sollte nicht der letzte Release Candidate sein; Microsoft brachte am 17. November eine dritte Version heraus, und am 15. Dezember erreichte Windows 2000 schließlich den Status Release to Manufacturing. Am 17. Februar 2000 erschienen schließlich Windows 2000 Professional, Server und Advanced Server. Der Start von Windows 2000 drohte zunächst, überschattet zu werden: laut einer internen Nachricht von Microsoft solle Windows 2000 63.000 Fehler haben. Es stellte sich jedoch heraus, dass dies nur das Resultat eines Programms sei, das den Quelltext von Windows 2000 automatisiert überprüfe und daher nichts mit der Anzahl der Fehler im Betriebssystem zu tun habe. Der Windows 2000 Datacenter Server kam am 26. September 2000 auf den Markt.\n\nDer \"Mainstream Support\" von Windows 2000 lief am 30. Juni 2005 aus. Der \"Extended Support\", in dessen Rahmen Sicherheitsaktualisierungen veröffentlicht wurden, endete am 13. Juli 2010.\n\nParallel zur 32-Bit-Version arbeitete ein separates Entwicklerteam, geführt von David N. Cutler, an der 64-Bit-Version des Betriebssystems, die Anfang 2000 für den Alpha-Prozessor und später gemeinsam mit der Veröffentlichung des Itanium-Prozessors auch für diese Architektur erscheinen sollte. Diese Version sollte nicht nur mehr als die bei 32-Bit-Prozessoren adressierbaren 4 Gigabyte an Arbeitsspeicher unterstützen, sondern noch einige zusätzliche Funktionen beinhalten, um sie für Großunternehmen attraktiver zu machen. Compaqs Ankündigung, die Entwicklung von Alpha-Prozessoren zu beenden, brachte jedoch nicht nur das Ende für die 32-Bit-Version, die sich bereits in der Release Candidate-Phase befand, sondern auch für die 64-Bit-Version. Da jedoch funktionierende Prototypen des Itanium-Prozessors fehlten und es auch keine anderen 64-Bit-Systeme gab, die für Windows 2000 in Frage gekommen wären, arbeiteten die Entwickler vorerst weiter mit Alpha-Rechnern.\n\nIm August 1999 demonstrierten Microsoft und Intel erstmals Windows 2000 auf einem Prototyp eines Itanium-Systems. Im Juni 2000 erschien eine Vorversion der 64-Bit-Version von Windows 2000; diese erhielten die Besitzer der 5.000 bis dahin ausgelieferten Itanium-Prototypen. Danach endeten die Arbeiten an der 64-Bit-Version von Windows 2000; diese wurde fortan auf Basis des Nachfolgers, Windows Whistler, entwickelt.\n\nFür Windows 2000 erschienen insgesamt vier Service Packs. Diese erschienen erstmals in zwei Version. Zum einen ist dies die \"Webinstallation\", die automatisch den Versionsstand des Betriebssystems überprüft und nur die Dateien herunterlädt, die aktualisiert werden müssen. Zum anderen ist dies die \"Netzwerkinstallation\", die wie bisher sämtliche Dateien enthält. Zudem konnte wie bisher das Service Pack als CD bestellt werden.\n\nEine Neuheit der Service Packs für Windows 2000 ist das sogenannte Slipstreaming. Dabei können die Dateien des Service Packs in das Installationsverzeichnis von Windows 2000 integriert werden, sodass bei einer Neuinstallation des Betriebssystems die Installation des Service Packs nicht mehr notwendig ist.\n\nDas erste Service Pack für Windows 2000 kam am 31. Juli 2000 heraus. Das Service Pack selbst beschränkte sich dabei größtenteils auf die Behebung der Programmfehler, die seit dem Erscheinen von Windows 2000 entdeckt wurden. Eine Neuheit, die nur auf der Service Pack-CD enthalten war, aber auch separat aus dem Internet heruntergeladen werden konnte, war der \"Terminal Services Advanced Client\", eine Erweiterung der Terminaldienste des Windows 2000 Servers. Enthalten waren ein ActiveX-Client, mit dem eine Verbindung auch über das Internet mittels des Internet Explorers hergestellt werden konnte, ein Snap-In der Terminaldiensteverwaltung für die Microsoft Management Console, sowie ein Windows-Installer-Paket, mit dem das Clientprogramm auf Windows-2000-Clients installiert werden kann.\n\nDas Service Pack 2 folgte am 16. Mai 2001. Da mit diesem Service Pack die Exportbeschränkungen der USA bezüglich Kryptografieverfahren entfielen, aktualisierte das Service Pack die Verschlüsselungsverfahren auf 128 Bit, einschließlich der Systeme außerhalb der USA, die bisher auf eine maximale Schlüssellänge von 56 Bit beschränkt waren.\n\nMit dem Service Pack 2 unterstützte das Betriebssystem erstmals den Kompatibilitätsmodus, der Probleme mit Anwendungen lösen soll, die für Windows NT 4.0 oder Windows 95 geschrieben wurden und unter Windows 2000 standardmäßig nicht korrekt ausgeführt werden. Der Kompatibilitätsmodus ist standardmäßig deaktiviert, kann aber bei Bedarf aktiviert werden. Zudem wird er nur auf Windows 2000 Professional installiert, für die Serverversionen konnte der Kompatibilitätsmodus allerdings aus dem Internet heruntergeladen werden.\n\nAm 1. August 2002 veröffentlichte Microsoft das Service Pack 3. Mit diesem Service Pack erhielt Windows 2000 die Funktion \"Automatische Updates\", die im Hintergrund automatisch nach verfügbaren Aktualisierungen sucht und den Anwender informiert, falls neue Aktualisierungen verfügbar sind. Zudem können mit dem Service Pack 3 die Standardprogramme wie Webbrowser und E-Mail-Programm konfiguriert werden. Der in Windows 2000 installierte Windows Installer wird mit diesem Service Pack auf die Version 2.0 aktualisiert.\n\nMit dem Service Pack 3 unterstützt Windows 2000 48-Bit-LBA und kann damit korrekt mit Festplatten umgehen, die größer sind als 137 GB. Die Unterstützung muss jedoch manuell in der Windows-Registrierung aktiviert werden. Zudem können mit dem Service Pack 3 Computercluster erstmals in Active Directory integriert werden.\n\nDas letzte Service Pack für Windows 2000 erschien am 26. Juni 2003. Mit diesem Service Pack unterstützte Windows 2000 erstmals USB-2.0-Controller. Zudem führte das Service Pack 4 eine Unterstützung für drahtlose Netzwerke nach dem IEEE-802.11-Standard ein, die dem Nachfolgebetriebssystem Windows XP entnommen wurde und ähnlich funktioniert, aber im Vergleich zu diesem einige Einschränkungen besitzt. So muss zur Herstellung einer Verbindung ein Programm des Adapterherstellers verwendet werden, außerdem kann immer nur ein drahtloser Netzwerkadapter verwendet werden und nicht mehrere gleichzeitig.\n\nDas Service Pack 4 enthält im Gegensatz zu früheren Versionen keine Aktualisierungen der Microsoft Virtual Machine mehr, diese können jedoch manuell heruntergeladen und installiert werden. Die Service Pack-CD enthält zudem Updates für das Windows 2000 Resource Kit; diese betreffen die Netzwerkdiagnoseprogramme sowie das Programm Sysprep.\n\nNach dem Service Pack 4 plante Microsoft zunächst ein Service Pack 5. Im November 2004 kündigte Microsoft jedoch an, dass es kein Service Pack 5 mehr geben würde, stattdessen sollten die neuesten Aktualisierungen in Form eines Update-Rollup-Pakets erscheinen. Dieses Update-Rollup-Paket kam am 28. Juni 2005 heraus, setzte ein installiertes Service Pack 4 voraus und enthielt alle seitdem erschienen Hotfixes. Da das Update-Rollup-Paket einige Fehler enthielt, erschien am 13. September 2005 eine aktualisierte Version.\n\nWindows 2000 wurde in vier Versionen veröffentlicht: \"Professional\", \"Server\", \"Advanced Server\" und \"Datacenter Server\". Eine Embedded-Version wie zuvor bei Windows NT 4.0 war zwar geplant, Microsoft gab aber am 24. April 2000 das Ende der Entwicklungsarbeiten an dieser Version bekannt.\n\n\nDie Benutzeroberfläche von Windows 2000 entspricht der des zuvor erschienenen Windows 98; sie profitiert zudem von einigen Verbesserungen durch den im Betriebssystem enthaltenen Internet Explorer 5.0. Darüber hinaus enthält Windows 2000 nur kleinere Neuheiten; so unterstützt Windows 2000 personalisierte Menüs, das heißt, selten benutzte Verknüpfungen im Startmenü werden automatisch ausgeblendet. Windows 2000 ermöglicht zudem wie Windows 98 SE die Internetverbindungsfreigabe.\n\nWindows 2000 enthält eine neue Version des Dateisystems NTFS. Zu den neuen Funktionen dieser Version zählen etwa Datenträgerkontingente, mit denen der von einem Benutzer beanspruchbare Festplattenspeicher festgelegt werden kann, sowie das Encrypting File System, mit dem Dateien auf der Festplatte verschlüsselt werden können. Zudem unterstützt NTFS mit dieser Version erstmals Sparse-Dateien. Ältere Versionen von Windows sind mit der neuen Version von NTFS nicht kompatibel, das Service Pack 4 für Windows NT 4.0 enthält jedoch einen Patch, der das Lesen und Schreiben von mit Windows 2000 erstellten NTFS-Partitionen ermöglicht. Windows 2000 unterstützt zudem das FAT32-Dateisystem, welches bereits in den Consumer-Versionen von Windows Verwendung fand.\n\nDie Systemdateiüberprüfung überwacht wichtige Systemdateien des Rechners und ersetzt sie automatisch, falls sie beschädigt oder gelöscht werden sollten. Windows 2000 beinhaltet zudem erstmals ein Defragmentierungsprogramm, eine beschränkte Version des Programms Diskeeper von \"Executive Software\". Im Gegensatz zu Windows NT 4.0, das nur bestimmte Komponenten von DirectX implementierte, bietet Windows 2000 eine vollständige Unterstützung von DirectX.\n\nEine der größten Neuheiten der Serverversionen von Windows 2000 ist Active Directory. Dabei handelt es sich um einen auf LDAP basierenden Verzeichnisdienst, in dem alle Ressourcen des Netzwerks, wie Benutzer, Gruppen und Computer zentral hierarchisch verwaltet werden. Active Directory verwendet Dynamisches DNS, um die Netzwerkressourcen zu adressieren. Im Gegensatz zu Windows NT 4.0 kann jeder Server zu einem Domänencontroller werden, ohne dass das Betriebssystem neu installiert werden muss. Mit Active Directory kommt auch Kerberos, ein ticketbasiertes System zur Authentifizierung von Personen. Ähnlich wie Windows 98 können Benutzer sich in Windows 2000 mittels einer Smartcard authentifizieren.\n\nWindows 2000 unterstützt Gruppenrichtlinien. Damit können Berechtigungen für einen Computer gesetzt werden, etwa das Recht, die Systemsteuerung aufzurufen. Zudem führt Windows 2000 das verteilte Dateisystem DFS ein, das es ermöglicht, Ressourcen, die sich auf mehreren Servern befinden, unter einem Namen zusammenzufassen.\n\nMit \"Routing und RAS\" enthält Windows 2000 eine Erweiterung des bis Windows NT 4.0 enthaltenen RAS-Dienstes. Dieses enthält eine verbesserte und vereinfachte Benutzeroberfläche und ermöglicht erstmals Network Address Translation (NAT), ähnlich wie es bereits bei der Internetverbindungsfreigabe verwendet wird. Für VPNs bietet Windows 2000 das L2TP-Protokoll, welches auf IPsec basiert und sicherer ist als das ältere PPTP-Protokoll. Zudem entfällt die Beschränkung von 256 gleichzeitigen Verbindungen, prinzipiell können sich beliebig viele RAS-Clients mit einem Windows-2000-Server verbinden. In einem kleinen Netzwerk kann Windows 2000 mithilfe von APIPA die IP-Adressen automatisch ohne administrative Konfiguration zuweisen.\n\nDie Fernsteuerungsfunktionen, die erstmals mit der Windows NT 4.0 Terminal Server Edition eingeführt wurden, sind Bestandteil aller Serverversionen von Windows 2000. Diese führt zudem eine neue Version des Remote Desktop Protocols ein, mit der Daten vom Server auf dem Drucker des Clients gedruckt werden können und eine auf Text und Dateien beschränkte gemeinsame Zwischenablage ermöglicht wird. Die Terminaldienste unter Windows 2000 unterstützen zwei Modi: den Remoteverwaltungsmodus, der lediglich zur Administration des Servers gedacht ist und nur bis zu zwei eingehende Verbindungen ermöglicht, und der Anwendungsservermodus, der zur Einrichtung einer Thin-Client-Umgebung dient und die Ressourcenverteilung des Servers entsprechend anpasst. Clients, die sich mit einem Terminalserver im Anwendungsservermodus verbinden wollen, benötigen eine Lizenz von einem Lizenzserver; dieser muss innerhalb von 90 Tagen bei Microsoft aktiviert werden, danach warnt das System bei jeder Remoteanmeldung, dass die Lizenz abgelaufen ist.\n\nWindows 2000 besitzt einen modularen Aufbau. Die unterste Ebene bildet der HAL. Darauf bauen der eigentliche Betriebssystem-Kern und die Subsysteme auf. Der HAL selbst wurde für frühere Windows-NT-Versionen hardwareunabhängig entwickelt. Der Betriebssystemkern kümmert sich um die Vergabe des Arbeitsspeichers und der Rechenzeit. Auf den Kern setzen die verschiedenen Subsysteme (Win32, OS2 und POSIX) auf. Dem Win32-Subsystem kommt dabei die größte Bedeutung zu, da es sich auch um den Fensteraufbau kümmert und die Signale der Eingabegeräte verarbeitet. Mit Windows NT 4.0 hat Microsoft Teile des GDI-Systems mit in den Kernel-Bereich genommen.\n\nWindows 2000 führt zwei neue Module des Betriebssystemkerns ein. Dies ist zum einen der \"PnP-Manager\", der Plug and Play implementiert und es so Windows 2000 ermöglicht, ähnlich wie Windows 95 und Windows 98 angeschlossene Hardware automatisch zu erkennen und zu installieren. Zum anderen ist dies der \"Power-Manager\", der die Stromsparfunktionen des ACPI-Standards implementiert, dadurch kann Windows 2000 erstmals in den Standby-Modus oder den Ruhezustand geschaltet werden. Dies erfordert allerdings neue Gerätetreiber, die mit dem Power-Manager kompatibel sind – werden ältere Gerätetreiber, etwa für Windows NT 4.0, verwendet, stehen die Stromsparfunktionen nicht zur Verfügung.\n\nMit Windows 2000 führt Microsoft Unterstützung für Physical-Address Extension (PAE) ein, um Arbeitsspeicher über 4 GB adressieren zu können. Die Address Windowing Extension bietet Programmen durch neue Programmierschnittstellen die Möglichkeit, auf diesen zusätzlichen Arbeitsspeicher zugreifen zu können, indem die entsprechenden Speicherbereiche in den virtuellen Speicher des Programms eingeblendet werden. Zwar ist diese Funktionalität in allen Versionen von Windows 2000 vorhanden, jedoch können nur der Advanced Server und der Datacenter Server mehr als 4 GB Arbeitsspeicher nutzen.\n\nDie Subsysteme arbeiten in der Regel nur auf Ring 3 (Privilegierungsstufe). Dadurch ist der Betriebssystemkern selbst vor Abstürzen in den Programmen geschützt.\n\nWindows 2000 unterstützt das Windows Driver Model, mit dem es unter anderem möglich ist, Gerätetreiber zu schreiben, die sowohl mit Windows 2000 als auch mit Windows 98 kompatibel sind. Das Betriebssystem enthält zahlreiche neue Gerätetreiber, unter anderem unterstützt es erstmals in der NT-Reihe USB-Geräte.\n\nDie Systemvoraussetzungen für Windows 2000 Professional sind ein Pentium-Prozessor mit 133 MHz, 64 MB Arbeitsspeicher, eine 2 GB große Festplatte mit mindestens 650 MB freiem Speicherplatz und ein CD-ROM-Laufwerk. Eine Aktualisierung ist sowohl von Windows NT Workstation 4.0 und 3.51, als auch von Windows 95 und 98 möglich. Die Systemvoraussetzungen für Windows 2000 Server und Advanced Server sind ähnlich, sie benötigen allerdings 128 MB Arbeitsspeicher und 1 GB freier Festplattenspeicher. Mit Windows 2000 Server kann eine bestehende Installation von Windows NT Server 3.51 und 4.0 sowie der Terminal Server Edition aktualisiert werden, Windows 2000 Advanced Server erlaubt zusätzlich eine Aktualisierung der Windows NT Server 4.0 Enterprise Edition.\n\nSysteme, die mit Windows 2000 Datacenter Server ausgeliefert werden sollen, müssen mindestens acht Prozessoren unterstützen; soll das System in einer Clusterumgebung verwendet werden, müssen auch tatsächlich acht Prozessoren vorhanden sein. Ansonsten sind mindestens ein Pentium III Xeon-Prozessor, 256 MB Arbeitsspeicher, eine 2 GB große Festplatte mit mindestens 1 GB freiem Speicherplatz sowie ein CD-ROM-Laufwerk erforderlich. Da der Datacenter Server ausschließlich auf dafür spezialisierter Hardware verwendet werden soll, ist ein Upgrade eines bestehenden Betriebssystems nicht vorgesehen.\n\nSollte der Rechner nicht in der Lage sein, von einer CD zu starten, enthält Windows 2000 Professional einen Diskettensatz bestehend aus vier Startdisketten.\n\nMicrosoft unterstützte Windows 2000 bis 13. Juli 2010 mit sicherheitskritischen Korrekturen („\"Extended Support\"“). Viele Firmen gingen davon aus, dass das System bis dahin noch ausreichend war, so waren in Deutschland Ende 2009 rund 61.000 Installationen mit Windows 2000 Server im Betrieb. Nach dem Ende des \"Extended Support\" hat Microsoft automatische Aktualisierungen über Windows Update für Windows 2000 eingestellt, ein frisch installiertes Windows 2000 lässt sich daher nicht mehr automatisch auf den letzten Stand bringen.\n\nBis zum Ende des Supports konnte das Betriebssystem – gegebenenfalls durch Softwarekomponenten anderer Hersteller – in allen wichtigen Anwendungsbereichen Office, Internet und Multimedia mit der aktuellen Entwicklung Schritt halten. Das letzte unter Windows 2000 nutzbare Microsoft Office ist die Version 2003. OpenOffice.org unterstützt das System bis Version 3.3, LibreOffice bis Version 3.6.7 (vom 10. Juli 2013). Die letzten Firefox-Versionen sind 12.0 und 10.0.12esr. Das jüngste unterstützte Internet Explorer war 6, .NET wird bis zur Version 2.0 unterstützt.\n\nIn vielen Fällen lässt sich Hardware, die noch bis 2010 verkauft wurde, problemlos nutzen. Gerätetreiber für Windows 2000 sind oft identisch mit denen für Windows XP. Bei vielen mit Windows-XP-Treiber verkauften Multimedia-Komponenten wie TV-Karten, Kameras und Scanner gibt es hingegen meist keinen kompatiblen Treiber. DirectX wird bis Version 9.0c unterstützt. Einige Komponenten (z. B. ASPI-Treiber) wurden nicht durch Service Packs nachgerüstet und müssen von Fremdanbietern den Treibern beigefügt werden.\n\n\n\n"}
{"id": "5581", "url": "https://de.wikipedia.org/wiki?curid=5581", "title": "Microsoft Windows XP", "text": "Microsoft Windows XP\n\nWindows XP (\"„eXPerience“,\" für \"Erlebnis,\" \"Erfahrung\") ist ein Betriebssystem von Microsoft. Es basiert auf dem\nWindows-NT-Kernel und ist der technische Nachfolger von Windows 2000 und der Vorgänger von Windows Vista. Das interne Versionskürzel lautet Windows NT 5.1 und der interne Codename in der Entwicklungsphase war \"Whistler.\" Windows XP kam am 25. Oktober 2001 auf den Markt. Es löste Windows ME der MS-DOS-Linie in der Version „Home Edition“ als Produkt für Heimanwender und Privatnutzer ab.\n\nUrsprünglich plante Microsoft noch, Windows 2000 in zwei Richtungen weiterzuentwickeln: zum einen \"Neptune,\" welches als Nachfolger von \"Millennium\" hauptsächlich Endverbraucher ansprechen sollte und für welches die Funktionen vorgesehen waren, die architekturbedingt nicht in Millennium implementiert werden konnten, und zum anderen \"Odyssey,\" welches für Firmenkunden bestimmt war. Eine Vorversion von Neptune erreichte am 27. Dezember 1999 die Betatester, doch schlussendlich gab Microsoft die Pläne auf.\n\nAm 21. Januar 2000 erreichte die Presse die Meldung, dass die Projekte Neptune und Odyssey zusammengelegt würden. Das so entstandene Projekt erhielt den neuen Codenamen \"Whistler.\" Auf der WinHEC im April 2000 stellte Microsoft das neue Betriebssystem erstmals vor und kündigte bereits einige neue Funktionen für das Betriebssystem an, etwa die Möglichkeit, ohne Abmeldung zwischen verschiedenen Benutzerkonten zu wechseln. Whistler sollte außerdem modular sein, sodass es auch auf kleinen Mobilgeräten lauffähig sein sollte.\n\nAuf der Professional Developers Conference im Juli 2000 kündigte Microsoft die Veröffentlichung von Whistler für die zweite Jahreshälfte von 2001 an. Am 13. Juli erschien die erste Vorversion von Whistler, die bereits die neue Design-Funktion der Benutzeroberfläche demonstrierte. Die hier verwendete und zunächst für das Endprodukt vorgesehene Benutzeroberfläche, zunächst \"Professional\" und später \"Watercolor\" genannt, gab Microsoft erst im Februar 2001 zugunsten von \"Luna\" auf. Am 24. August folgte eine zweite Vorversion, die das neue Startmenü demonstrierte und erstmals die Windows-Firewall enthielt. Während sich der erste Betatest, der ursprünglich für September vorgesehen war, immer weiter nach hinten verschob, kam am 3. Oktober die nächste Vorversion heraus, die jedoch lediglich kleinere Verbesserungen enthielt. Erst die zwei Wochen später folgende Vorversion zeigte eine neu gestaltete Installationsroutine sowie ein neues Hilfesystem.\n\nAm 31. Oktober 2000 startete schließlich der Betatest von Whistler. Auf der darauffolgenden COMDEX stellte Microsoft den Tablet PC vor, der mit einer speziell angepassten Version von Whistler erscheinen sollte. Am 18. Dezember kündigte Microsoft mit \"Whistler Embedded\" das neue Betriebssystem der Embedded-Reihe an, das Windows NT 4.0 Embedded ablösen sollte.\n\nDie am 4. Januar 2001 ausgelieferte Vorversion beinhaltete erstmals die Produktaktivierung. Mit einem Beispiel-Design demonstrierte Microsoft das Wechseln des Designs im laufenden Betrieb, das Unternehmen hielt aber an der Meinung fest, dass es für Endverbraucher keine Möglichkeit geben werde, ein eigenes Design zu kreieren. Am 16. Januar folgte bereits die nächste Vorversion und am 23. Januar schließlich eine weitere Vorversion.\n\nAm 13. Februar stellte Microsoft erstmals die neue Benutzeroberfläche \"Luna\" vor, gleichzeitig gab es den endgültigen Namen für das Produkt bekannt, \"Windows XP.\" Die zeitgleich veröffentlichte Vorversion enthielt bereits diese Neuerungen. Nach heftiger Kritik an der neuen Benutzeroberfläche besserte Microsoft vereinzelt nach und reduzierte unter anderem die Größe der Symbole in der Schaltflächenleiste. Nach zahlreichen Verzögerungen und mehreren Vorversionen startete der zweite Betatest schließlich am 26. März mit einer offiziellen Ankündigung auf der WinHEC. Große Kritik erhielt Microsoft für die Entscheidung, Unterstützung für USB 2.0 nicht mit Windows XP auszuliefern, an der das Unternehmen dennoch festhielt. Die Entwicklung der Server-Version nahm ihren eigenen Verlauf und am 30. April nannte Microsoft die Server-Version provisorisch \"Windows 2002.\"\n\nAm 5. Mai 2001 erschien eine weitere Vorversion. Neu war eine Sicherheitsfunktion, die bei Benutzerkonten mit einem leeren Passwort lediglich die lokale Anmeldung ermöglicht und unter anderem eine Anmeldung über das Netzwerk in solchen Fällen sperrt. Am 24. Mai erschien die letzte Version, die neue Funktionen implementierte, die nachfolgenden Versionen konzentrierten sich auf die Behebung von Programmfehlern. Pläne, Windows XP mit AOL zu bündeln, zerschlugen sich im Juni, stattdessen kündigte Microsoft den Windows Messenger offiziell an. Ebenso plante Microsoft, dass die Home Edition nur einen Monitor unterstützten sollte, auch das änderte sich.\n\nMit der am 21. Juni veröffentlichte Vorversion band Microsoft das Betriebssystem stärker an das Microsoft-Passport-System an. Am 2. Juli 2001 eröffnete Microsoft die Release-Candidate-Phase. Der endgültige Veröffentlichungstermin wurde auf den 25. Oktober festgesetzt. Aufgrund eines Urteils eines US-Gerichts musste Microsoft entgegen ursprünglichen Plänen Erstausrüstern () erlauben, eigene Icons auf dem Desktop zu platzieren; ebenso können diese den Internet Explorer und Outlook Express aus dem Startmenü entfernen und der Internet Explorer konnte in der Systemsteuerung deinstalliert werden, wenngleich dies lediglich verschiedene Icons entfernte, den IE-Kern allerdings im Betriebssystem beließ. Ebenso fuhr Microsoft die Beschränkungen bei der Produktaktivierung drastisch zurück.\n\nAm 27. Juli erschien der zweite Release Candidate. Durch die neueste Gerichtsentscheidung versuchte AOL nun, Erstausrüster dazu zu bringen, Windows XP trotz der zuvor gescheiterten Verhandlungen ausschließlich mit AOL auszuliefern. Microsoft konterte mit der Pflicht, einen Link zum Microsoft-Network-Dienst auf dem Desktop zu platzieren, falls Erstausrüster eigene Icons dort platzieren wollen. Am 24. August 2001 erreichte Windows XP schließlich den RTM-Status, und am 25. Oktober 2001 erschien das Betriebssystem wie geplant im Handel.\n\nMit Windows XP wollte Microsoft die Benutzerfreundlichkeit des Betriebssystems erhöhen. Im Gegensatz zu älteren Heimanwender-Betriebssystemen von Microsoft basiert Windows XP auf einem Windows-NT-Kernel. Dieser Wechsel sollte für eine verbesserte Stabilität sorgen. Zudem wurde Augenmerk auf die Verbesserung der Sicherheit gelegt.\nDie mit einem gekennzeichneten Elemente waren bereits unter Windows ME verfügbar, nicht jedoch unter Windows 2000.\n\nDie für Benutzer auffälligste Neuheit in Windows XP ist die Benutzeroberfläche „Luna“, die im Auslieferungszustand eine buntere und verspieltere Desktop-Oberfläche bietet als bei älteren Windowsversionen; wahlweise steht auch eine leicht modifizierte Version der Oberfläche aus Windows 2000 zur Verfügung („klassische Darstellung“). Die „Luna“-Oberfläche enthält auch den voreingestellten Bildschirmhintergrund „Grüne Idylle“, ein Bild einer grünen Wiese unter blauem Himmel. Sie weckte während und auch noch nach der Vertriebsperiode des Betriebssystems Assoziationen mit dem Szenenbild aus der Kinderfernsehserie Teletubbies.\n\nDas Startmenü wurde erweitert: So ist es in Windows XP in zwei statt bisher einer Spalte angeordnet. Während in der linken Spalte die zuletzt benutzten Programme angezeigt werden, bietet es rechts zusätzliche Einträge, etwa zum „Arbeitsplatz“ oder zu Benutzerordnern wie dem Ordner „Eigene Dateien“ oder „Eigene Musik“. Neu sind dort auch ein Link zu den eingerichteten „Netzwerkverbindungen“ sowie eine Schaltfläche zum Einstellen von „Programmzugriffen und -standards“.\n\nIm Windows-Explorer wurden Funktionen zur Unterstützung von digitaler Fotografie eingebaut. So wird nun z. B. die \"Windows Bild- und Faxanzeige\" mitgeliefert, mit der gängige Bildformate geöffnet und rudimentär bearbeitet werden können. Auch ist die Bildanzeige als Bildschirmpräsentation ohne Zusatzsoftware möglich. Auch Musikdateien werden besser unterstützt: Die sogenannten ID3-Tags, die Informationen wie z. B. Interpret, Titel usw. in der Musikdatei speichern, werden im Explorer angezeigt und können über das Eigenschaftenmenü direkt bearbeitet werden. Der Explorer beinhaltet nun auch eine einfache Funktion zum Brennen von CDs. Software von Drittanbietern ist insoweit nicht mehr notwendig. Auch kann der Explorer ZIP-komprimierte Dateien erstellen und verwalten.\n\nDie Systemwiederherstellung ist eine Funktion, welche es dem Benutzer mit Hilfe sogenannter Wiederherstellungspunkte ermöglicht, das System in Hinsicht auf System- und Konfigurationsdateien in einen früheren Zustand zurückzuführen. Dies soll vor allem bei fehlgeschlagenen Treiber- oder Software-Installationen weiterhelfen. Diese erstmals mit Windows ME eingeführte Technik wurde verbessert und mit Windows XP in die Windows-NT-Linie übernommen.\n\nWindows XP enthält auch Kompatibilitätsoptionen für Anwendungen, die für ältere Windowsversionen geschrieben wurden. Diese Funktion wurde zwar bereits mit dem \"Service Pack 2\" unter Windows 2000 eingeführt, muss dort nach der Service-Pack-Installation aber erst im System registriert werden und steht nur Administratoren zur Verfügung. Unter Windows XP steht sie standardmäßig zur Verfügung und kann für jede Anwendung einzeln festgelegt werden.\n\nWährend die meisten Windowsversionen bisher lediglich das Laufwerksdateisystem FAT verwenden konnten, kommen nun für alle Anwender die bisher nur unter Windows NT/2000 bekannten Funktionen des NTFS-Dateisystems hinzu. Das sind beispielsweise Dateigrößen über 4 GB, Metadaten-Journaling, Datenträgerkontingente oder eine zuverlässigere und einfachere Datenträgerkomprimierung als DriveSpace (Windows 9x) bzw. \"Doublespace\" (DOS). Einige NTFS-Funktionen sind in der Homevariante allerdings nicht nutzbar, so etwa die Verschlüsselung und standardmäßig (d. h. ohne Fremdsoftware im laufenden Betrieb) die Vergabe von Dateizugriffsberechtigungen.\n\nUm Software-Piraterie einzudämmen, verwendet Microsoft bei Windows XP erstmals das System der Produktaktivierung. Bei diesem Verfahren tauscht das Betriebssystem im Zuge der Installation bestimmte Daten mit Microsoft aus, bevor eine dauerhafte Verwendung gestattet wird. Die ausgetauschten Daten enthalten vor allem Informationen über die verwendete Hardware. Falls sich diese Daten ändern, zum Beispiel durch Austausch oder Erweiterung von Hardware-Komponenten, kann das Betriebssystem in einigen Fällen eine erneute Aktivierung verlangen.\n\nDie übertragenen Daten enthalten nach Angaben Microsofts einen Hash-Wert der folgenden Merkmale in verschlüsselter Form:\n\nSpäter wurde bekannt, dass bei einer Aktivierung bestimmter Notebooks, deren Hardware nicht ohne weiteres geändert werden kann, nicht alle obengenannten Daten ausgetauscht werden. Für Kunden, Partner und Entwickler mit großem Installationsaufwand, welche oft automatisierte Installationsroutinen verwenden, gibt es für die erworbene \"Corporate Edition\" oder MSDN-Version einen firmenweiten Lizenzschlüssel, der keine weitere Produktaktivierung erforderlich macht. Wurde eine weite unerlaubte Verbreitung entdeckt, wurden diese Schlüssel im Zuge der Produktupdates gesperrt oder Online-Produktupdates verweigert.\n\nFür die Umgehung der Aktivierung gibt und gab es Cracks, die geläufigsten darunter waren:\n\nMicrosoft versucht außerdem seit geraumer Zeit, durch Studien zu belegen, wie transparent sich die Produktaktivierung verhält und wie sie funktioniert. Der deutsche TÜViT hat die Anonymität des Aktivierungsverfahrens „bestätigt“, wobei TÜViT gerade an entscheidender Stelle nicht selbst nachprüfte, sondern den Angaben seines Auftraggebers Glauben geschenkt hat.\n\nUngültige Seriennummern werden beim Windows Update durch ein ActiveX-Programm namens Windows Genuine Advantage (WGA) zurückgewiesen. Da in alternativen Browsern kein ActiveX unterstützt wird, musste dazu in der Vergangenheit die ausführbare Datei \"GenuineCheck.exe\" heruntergeladen werden. Sie generierte eine Nummer, die im Download Center und bei Windows Updates eingegeben werden musste. Diese Nummer wurde aus der Seriennummer und einem Code, der in den Systemeigenschaften einzusehen ist, errechnet. Diese Methode wurde von Softwarepiraten schnell geknackt, indem der windowseigene Kompatibilitätsmodus genutzt wurde. Dieses Verfahren wurde durch die ausführbare Datei \"legitcheck.hta\" ersetzt, die manuell heruntergeladen und ausgeführt werden muss. Mit ihr entfiel die manuelle Eingabe einer Nummer.\n\nDie Windows-Firewall wurde neu eingeführt. Sie dient dem Schutz gegen Internetangriffe und wurde mit dem Service Pack 2 stark erweitert. Die Funktion „Schneller Benutzerwechsel“ erlaubt es nun, dass mehrere Benutzer gleichzeitig angemeldet sind. Zwischen diesen kann dann besonders schnell gewechselt werden. Windows XP ermöglicht mit der Remoteunterstützung die Fernwartung über Terminal Services (Remote Desktop Protocol). Die Möglichkeiten, das System per Kommandozeile zu verwalten, wurden vereinheitlicht und erweitert. Zudem wurde die Kantenglättung für Schriften (ClearType) eingeführt.\n\nDie CD-ROM von Windows XP ist bootfähig, im Gegensatz zu Windows 2000 oder Windows 98 liegen keine Startdisketten bei. Sollte das System keine Möglichkeit bieten, von einer CD zu starten, können Abbilder eines Diskettensatzes, bestehend aus sechs Disketten, aus dem Internet heruntergeladen werden, um die Installation des Betriebssystems zu ermöglichen.\n\nFür Windows XP wurde das Windows-2000-System für Intel-Prozessoren als Grundlage übernommen. Es sollte aber darüber hinaus auch die alten MS-DOS-basierten Windows-Versionen ersetzen. Daher mussten Möglichkeiten geschaffen werden, weitere ältere, nicht unter Windows NT lauffähige Programme auszuführen.\nEine weitere Ergänzung ist ein Kompatibilitätsmodus genanntes \"Personality,\" der bei Bedarf Routinen aus älteren Systemen emuliert. Damit soll das Ausführen von Programmen ermöglicht werden, die an Vorgängerversionen angepasst wurden.\n\nAndere Programme nehmen den vollen Speicherschutz von Windows in Anspruch. Das System ist daher vergleichsweise zuverlässig und, insbesondere im Vergleich mit Windows 98, stabil.\n\nWindows XP formatiert Partitionen, wie schon Windows 2000, standardmäßig mit dem Dateisystem NTFS. Für große Festplatten ist es möglich, die Verwaltung im Modus mit 48-bit-LBA zu aktivieren. Es ist auch in der Lage, mit FAT-Partitionen umzugehen. Außerdem ist es möglich, im Setup die Schnellformatierung anzuwenden.\n\nDie \"Home Edition\" ist primär für den privaten Einsatz zu Hause gedacht. Da mit Windows XP die Weiterentwicklung der DOS-basierenden „9x“-Systeme eingestellt wurde, tritt die Home Edition an ihre Stelle und wurde trotz der abweichenden technischen Basis als Nachfolger dieser Systeme beworben. Als solche fehlen ihr zahlreiche Funktionen, die nur in einer Firmenumgebung relevant sind. Im Gegenzug wurde z. B. die Verwendung des Kompatibilitätsmodus für Endanwender vereinfacht, der unter Windows 2000 (ab SP2) eingeführt wurde, aber Administratorrechte erforderte und standardmäßig deaktiviert war.\n\nDie Home Edition kann keiner Domäne beitreten, ebenso fehlt der Remote-Desktop, die Gruppenrichtlinienverwaltung sowie die Möglichkeit, Zugriffsrechte über den Windows-Explorer zu setzen. Ebenso fehlt der Home Edition das verschlüsselte Dateisystem EFS. Das Programm NTBackup fehlt standardmäßig in der Home Edition, kann aber von der CD nachinstalliert werden. Außerdem sind der Internet Information Server und zahlreiche Administrationsprogramme in der Home Edition nicht verfügbar. Windows XP Home Edition unterstützt zudem nur einen einzigen Prozessor.\n\nDie \"Professional Edition\" tritt die direkte Nachfolge von Windows 2000 Professional an. Diese Version kann im Gegensatz zur Home Edition nicht nur von Windows 98 und Me, sondern auch von Windows NT 4.0 und Windows 2000 aktualisiert werden. Windows XP Professional unterstützt bis zu zwei Prozessoren. Die meisten anderen Versionen von Windows XP basieren auf der Professional Edition.\n\nIm Zuge eines Verfahrens mit der Europäischen Kommission im März 2004, infolgedessen Microsoft zur Zahlung von 497 Millionen Euro verurteilt wurde, musste das Unternehmen außerdem eine Version von Windows XP ohne den Windows Media Player in den Handel bringen. Nach zähen Verhandlungen einigten sich Microsoft und die EU-Kommission auf den Namen \"Windows XP N.\" Da Microsoft den Preis für Windows XP N genauso hoch setzte wie für das normale Produkt, verzichteten die meisten Erstausrüster darauf, das Produkt in den Handel zu bringen, sodass es kaum verbreitet war.\n\nNach einem Untersuchungsverfahren der südkoreanischen Kartellbehörde musste Microsoft die normalen Versionen von Windows XP vom Markt nehmen und zwei neue Versionen von Windows XP für den südkoreanischen Markt veröffentlichen: zum einen \"Windows XP K,\" welches zusätzlich Links zu Medienspielern und Instant Messengern von Drittanbietern enthält, und \"Windows XP KN,\" welches sowohl den Windows Media Player als auch den Windows Messenger nicht enthält.\n\nDie \"Media Center Edition\" basiert ebenfalls auf der „Professional Edition“ und enthält spezifische Erweiterungen für auf multimediale Inhalte sowie deren Wiedergabe spezialisierte Computer, die in der Regel mit einer TV-Karte ausgestattet sind. Ein Merkmal ist die Möglichkeit der vereinfachten Bedienung durch die Darstellung auf einem normalen Fernsehapparat und die Steuerung mit einer Fernbedienung. Microsoft versuchte damit erstmals, die Lücke zwischen einem reinen Computer und einer Medienzentrale für das Wohnzimmer zu schließen. Windows XP Media Center Edition erfuhr 2003 die erste Aktualisierung, die letzte XP-Version ist die Media Center Edition 2005. Während die erste Version der Windows XP Media Center Edition nur im Paket mit entsprechenden Computern vertrieben und nicht als Einzelprodukt verfügbar war, sind die aktualisierten Fassungen auch einzeln über den Vertriebskanal \"System Builder\" zu erwerben. Seit der letzten Version können Endbenutzergeräte wie z. B. DVD-Recorder, die Xbox 360 von Microsoft und weitere über eine Netzwerkverbindung mit dem Betriebssystem kommunizieren. Dafür ist in diesen Endgeräten ein Windows XP Media Center Edition als „embedded Version“ oder ein zur Media Center Edition kompatible Benutzerschnittstelle implementiert.\n\nAm 9. November 2002 erschien die Windows XP Tablet PC Edition. Damit erhoffte sich das Unternehmen, den seit Jahren produzierten, aber kaum erfolgreichen Tablet-PCs zum Durchbruch zu verhelfen. Dazu veröffentlichte Microsoft die Microsoft-Tablet-PC-Spezifikation, die bestimmte Kriterien an Tablet-PCs stellte, die mit dem neuen Betriebssystem ausgeliefert werden sollten. Das Betriebssystem selbst basiert auf Windows XP Professional mit integriertem Service Pack 1, enthält aber zusätzlich Funktionen zur Handschrifterkennung. Zusätzlich veröffentlichte Microsoft ein Add-On zur Integration von Office XP sowie zahlreiche Programmierschnittstellen, mit denen Entwickler die Stiftfunktionen in ihren eigenen Programmen nutzen konnten. Die Windows XP Tablet PC Edition war nicht im Handel erhältlich, sondern wurde nur mit passender Hardware verkauft; lediglich MSDN-Abonnenten und Volumenlizenzkunden konnten die Tablet PC Edition auch ohne Tablet erhalten.\n\nZusammen mit dem Service Pack 2 veröffentlichte Microsoft eine aktualisierte Version unter der Bezeichnung \"Windows XP Tablet PC Edition 2005.\" Besitzer der älteren Tablet PC Edition konnten kostenlos auf die neue Version aktualisieren, außerdem lag sie neuen Tablet-PCs bei. Die Tablet PC Edition 2005 bot hauptsächlich eine verbesserte Handschrifterkennung sowie eine Integration mit Office 2003. Die neue Version unterstützte das .NET Framework, sodass auch Managed Code für die Tablet PC Edition geschrieben werden konnte. Die Tablet PC Edition 2005 wurde schnell durch ein Speicherleck in den Stiftfunktionen bekannt, das dazu führen konnte, dass das Betriebssystem wegen fehlendem freien Arbeitsspeicher unbenutzbar wurde.\n\nDas Betriebssystem und die zugehörigen Tablet-PCs konnten sich jedoch kaum auf dem Markt durchsetzen. Vor allem aufgrund des hohen Preises und der schlechten Vermarktung war die Verbreitung auf wenige Nischen beschränkt. Bis 2005 konnten weltweit lediglich 650.000 Tablet-PCs verkauft werden.\n\nWindows XP Embedded wird primär im industriellen Umfeld, aber auch in medizinischen Geräten, Geldautomaten oder für Kassenterminals eingesetzt, seltener in Haushalts- und Unterhaltungselektronik oder in Voice-over-IP-Komponenten. Diese Version basiert ebenfalls auf der Professional Edition. Zudem wird die Edition noch bis Mitte 2019 unterstützt.\n\nDie \"Windows XP 64-Bit Edition\" war eine spezielle Version von Windows XP für den Intel-Itanium-Prozessor. Sie erschien zeitgleich mit den 32-Bit-Versionen von Windows XP; bereits die ersten ausgelieferten Itanium-Prozessoren waren mit einer Vorversion von Windows XP ausgestattet, die von Microsoft offiziell unterstützt wurde. Der 64-Bit-Edition fehlten zahlreiche Funktionen des 32-Bit-Pendants, darunter der Windows Media Player, NetMeeting, sowie Unterstützung für alte DOS- und 16-Bit-Anwendungen, ansonsten war sie jedoch ein vollwertiges Betriebssystem, das bis zu 16 GB Arbeitsspeicher verwalten konnte.\n\nIm März 2003 folgte zusammen mit der Veröffentlichung von Windows Server 2003 die neue \"Windows XP 64-Bit Edition Version 2003\" für die neuen Itanium-2-Prozessoren. Nachdem als letzter Hersteller Hewlett-Packard im September 2004 die Auslieferung von Workstations mit Itanium-Prozessoren einstellte, beendete Microsoft im Januar 2005 die Unterstützung der Windows XP 64-Bit Edition. Insgesamt waren die Verkaufszahlen enttäuschend und die Windows XP 64-Bit Edition hatte praktisch keinerlei Verbreitung.\n\nDie Windows XP „x64 Edition“ (Codename „Anvil“) ist eine Version, die ausschließlich für Prozessoren mit AMD64- oder Intel-64-Erweiterung entwickelt wurde. Sie läuft nicht auf 64-Bit-Prozessoren anderer Hersteller und ähnelt Windows XP Professional zwar, basiert aber auf dem Kernel von Windows Server 2003 und besitzt somit eine modernere Basis (NT 5.2). Die x64-Edition erschien am 25. April 2005. Im Zusammenspiel zwischen Prozessor und Betriebssystem kann auch eine konventionelle 32-Bit-Software – über den \"Windows on Windows 64-x86-Emulator\" (WOW64) – ausgeführt werden. Somit müssen auszuführende Programme nicht als 64-Bit-Versionen vorliegen. Dieses Verfahren der x64-Prozessoren wird \"Mixed-Mode\" genannt – dem gleichzeitigen Ausführen von 64- und 32-Bit-Software auf einem Prozessor. Für die einwandfreie Funktion der Hardware werden 64-Bit-Gerätetreiber vorausgesetzt. Die Treiber werden in der Regel vom Hardware-Hersteller für das Betriebssystem her- und bereitgestellt. Besonders zu beachten ist, dass sämtliche Programme auf 16-Bit-Basis unter Windows XP x64 nicht funktionieren, da der Betriebsmodus „Long Mode“ der x64-Architektur dies nicht mehr unterstützt. Dies betrifft alle Programme, die – teilweise oder ausschließlich – für MS-DOS oder Windows 3.x entwickelt worden sind.\n\nDie Vorteile dieser 64-Bit Version gegenüber XP mit 32-Bit-Architektur sind:\n\nDas letzte \"Service Pack\" für Windows XP Professional x64 ist das Service Pack 2 vom 12. März 2007, die \"Service Packs\" der Windows-XP-32-Bit-Editionen sind nicht mit der x64-Variante kompatibel.\n\nAm 11. August 2004 kündigte Microsoft die \"Starter Edition\" von Windows XP an. Diese Version sollte in Zusammenarbeit mit Erstausrüstern die Verbreitung von PCs mit Windows in Schwellenländern fördern. Zunächst startete das Projekt in Thailand, Malaysia, und Indonesien, später kamen noch weitere Länder wie Indien und Mexiko dazu.\n\nMit Preisen um die 30 US-Dollar war die Starter Edition weit günstiger als andere Versionen von Windows XP, dafür hatte sie einige einschneidende Einschränkungen. So lief das System ausschließlich auf den Prozessoren Intel Celeron, AMD Duron und AMD Sempron und verweigerte den Dienst auf anderen Prozessoren. Außerdem unterstützte diese Edition lediglich eine Bildschirmauflösung von 800×600 (außer man installierte zusätzliche Treiber, wie z. B. in VirtualBox die Guest Additions) und enthielt weder Netzwerkfunktionen noch Unterstützung für mehrere Benutzerkonten. Es konnten zudem lediglich drei Programme gleichzeitig ausgeführt werden.\n\nDer Verkauf der Starter Edition kam in den Zielmärkten nur schleppend voran. Vor allem durch die in diesen Ländern weit verbreitete Produktpiraterie waren vollwertige Versionen von Windows XP für weniger Geld erhältlich, zumal die Starter Edition nicht im Handel erhältlich war und nur zusammen mit einem entsprechenden PC erworben werden konnte. Außerdem sah Microsoft keine Möglichkeit vor, die Starter Edition auf die Home oder Professional Edition zu aktualisieren.\n\nWindows \"Fundamentals for Legacy PCs\" (Windows FLP) ist ein Betriebssystem/Thin Client, das basierend auf Windows XP Embedded für ältere und weniger leistungsstarke PC optimiert wurde. Die Codenamen waren „Eiger“ und „Mönch“. Microsoft wollte mit dieser Version grundlegende Dienste auf älteren Computern zur Verfügung stellen. Es wurden viele Kerneldienste des Service Packs 2 für Windows XP übernommen, beispielsweise die Windows-Firewall, Gruppenrichtlinienverwaltung, automatische Aktualisierungen und andere Verwaltungssysteme. Es wurde speziell für Büroanwendungen und für die Fernverbindung (\"Remote Desktop\") optimiert. Windows FLP kann leicht zu einer Diskless-Arbeitsstelle umgebaut werden. Diese Version wird ausschließlich an Kunden mit „Microsoft Software Assurance“ (Volumenlizenz) abgegeben.\n\nAnders als in den vorherigen Windows-NT-Versionen gibt es keine Server-Variante von Windows XP. Die Serverprodukte zu Windows XP sind in der separaten \"Windows-Server-2003\"-Produktfamilie zusammengefasst. Die einzelnen Server-Versionen gliedern sich dabei in \"Standard Edition,\" \"Enterprise Edition,\" \"Datacenter Edition,\" \"Web Edition\" und \"Small Business Server,\" wobei die Datacenter Edition ausschließlich als OEM-Lizenz (Lizenz für Kunden von Erstausrüstern) in Verbindung mit entsprechender Hardware erhältlich ist.\n\nMicrosoft bietet mit der Herausgabe seiner Produkte wie Windows XP befristeten Support an. Bei der Befristung wird nach dem Anwender (z. B. Privatanwender) und nach Phasen unterschieden. Microsoft unterscheidet zwei Phasen:\n\nDen bis zu zwei- oder fünfjährigen \"Mainstream Support\" und den darauffolgenden, bis zu fünfjährigen \"Extended Support.\" Aktualisierungen wurden nach bestimmten Zeitabständen zusammengefasst und als Service Pack bereitgestellt. Diese Service Packs enthielten teilweise auch neue Funktionen, wie beispielsweise die verbesserte Firewall im Service Pack 2. Microsoft stellte für Windows XP bisher drei Service Packs zur Verfügung. Mit dem 14. April 2009 ging die Produktunterstützung von Windows XP vom Mainstream- in den Extended Support über. Dieser Supportzyklus beinhaltete Aktualisierungen, die bis zum 8. April 2014 erfolgten. In dieser Phase wurden keine neuen Funktionen mehr geliefert, sondern ausschließlich Sicherheitslücken behoben. Bei der Aktualisierung konnte zwischen automatischer und manueller Installation unterschieden werden. Für Geschäftskunden wurden auch nach April 2009 kostenpflichtige Serviceverträge (Support für Fehlerbehebungen) angeboten. Die Aktualisierung der Signaturen für Schadprogramme erfolgte noch bis Juli 2015, wodurch die Rechner noch gegen bestimmte Angriffe geschützt werden konnten, allerdings keine vollständige Sicherheit hergestellt werden konnte.\n\nWie bei Computersystemen üblich werden häufig Sicherheitslücken und Fehler entdeckt, die oft schon kurz nach dem Bekanntwerden von Angreifern direkt (z. B. Cracker) oder indirekt (z. B. Virenprogrammierer) ausgenutzt werden, um anfällige Systeme für eigene Zwecke zu missbrauchen, anderweitig zu manipulieren oder außer Funktion zu setzen.\n\nUm Benutzern das Installieren entsprechender Sicherheitsaktualisierungen zu erleichtern, stellt Microsoft seit Windows 98 eine Funktion zur automatischen Aktualisierung per Internet zur Verfügung. Das erweitert die bisherige Strategie der Verbreitung von Service Packs und Hotfixes durch manuelles Herunterladen. Der automatische Prozess erleichtert die Verteilung entsprechender Aktualisierungen und erhöht so Verbreitungsgeschwindigkeit und -grad von Updates. Er kann in vier Stufen angepasst werden (Bestätigung des Downloads, Bestätigung der Installation, vollautomatisch oder deaktiviert).\n\nDie Updates können aber auch zwangsweise eingespielt und aktiviert werden, denn die konfigurierbaren Update-Stufen gelten nicht für den Update-Dienst selbst, was viele Benutzer überrascht. Das Gleiche gilt nach den Lizenzbedingungen für die integrierte Digitale Rechteverwaltung.\n\nDas \"Service Pack 1\" für Windows XP, das vor allem alle bis dahin veröffentlichten Sicherheitspatches in einem einzelnen Paket vereinte, wurde am 9. September 2002 veröffentlicht. Hardwareseitig kamen der standardmäßige Support von Festplattengrößen jenseits von 137 GB sowie die uneingeschränkte Nutzung der USB-2.0-Schnittstelle hinzu.\n\nMicrosofts Unterstützung für Windows XP mit SP1 oder SP1a lief zum 10. Oktober 2006 aus. Seit diesem Datum liefert Microsoft für Windows XP mit Service Pack 1 keinerlei Sicherheitsaktualisierungen mehr aus.\n\nDas \"Service Pack 2\" wurde am 9. August 2004 öffentlich verfügbar gemacht und zielte vor allem auf eine verbesserte Systemsicherheit ab. Ursprünglich sollte das Service Pack 2 schon im Juni 2004 von Microsoft herausgegeben werden, es stellte sich aber heraus, dass noch einige Fehler zu beheben waren, was die Veröffentlichung um zwei Monate verzögerte. Zum ersten Mal fügte Microsoft mit einem Service Pack Windows XP neue Funktionen hinzu, wie etwa eine überarbeitete Windows-Firewall, die Unterstützung für die Datenausführungsverhinderung, mehr Software zur Unterstützung drahtloser Netze und einen Pop-up-Blocker für den Internet Explorer 6.0, der nach dem Aufspielen des Service Packs ebenfalls aktualisiert wird und in der Version 6.0 SP2 vorliegt. Durch das neu hinzugekommene Sicherheitscenter können eine Vielzahl von Personal Firewalls und Antivirenprogrammen überwacht, indem Hersteller dieser Programme die mit diesem Service Pack neu eingeführten APIs benützten, und die Funktion „automatische Updates“ eingerichtet werden. Microsoft unterstützte das Service Pack 2 bis zum 13. Juli 2010.\n\nDas \"Service Pack 3,\" das zugleich das letzte für Windows XP ist, sollte am 29. April 2008 veröffentlicht werden. Es gab jedoch ein Kompatibilitätsproblem mit Microsofts Dynamics Retail Management System (RMS), sodass es erst am 6. Mai über das Microsoft Download Center und Windows Update verfügbar wurde. Die Nutzer von Microsofts kostenpflichtigem MSDN sowie Nutzer mit Volumenlizenzverträgen hatten schon vorab die Möglichkeit, sich das Service Pack 3 herunterzuladen.\n\nBei der 313 MB umfassenden Aktualisierungsdatei handelt es sich um eine Sammlung aller Software-Aktualisierungen und Fehlerbereinigungen, die seit dem Erscheinen von Windows XP veröffentlicht wurden. (Bei Download über Windows Update hat das Service Pack eine geringere Größe, da ausschließlich für die laufende Windows-Version benötigte Dateien heruntergeladen werden müssen.) Aus Support-Gründen lässt sich das SP3 nur installieren, wenn mindestens das Service Pack 1 bereits zuvor installiert wurde; die Slipstream-Integration in eine Installationsquelle ist dagegen in jedem Fall möglich. Zusätzlich zu den Aktualisierungen beinhaltet das Service Pack 3 auch einige weitere aktualisierte Programme, wie den Background Intelligent Transfer Service (BITS) 2.5, Windows Installer 3.1, Management-Console (MMC) 3.0 und die Core XML Services 6.0. Programmaktualisierungen des Internet Explorers 7 und Media Players sind nicht enthalten. Das Update erlaubt die Verwendung von Windows XP als Gastsystem in Microsofts Virtualisierungssystem Hyper-V. Ebenfalls enthalten ist eine Clientkomponente für das von Windows Server 2008 bereitgestellte NAP-System.\nWeiterhin wird nun die Erkennung von „Black-Hole“-Routern unterstützt. Das Sicherheits-Center wartet zusätzlich mit besseren Beschreibungen auf und es wurde ein Windows-Kryptographie-Modul (FIPS) implementiert, das im Kernel-Modus läuft.\nNach der Installation des Service Packs 3 verschwindet die Möglichkeit, die Adress-Symbolleiste in die Taskbar einzubinden. Microsoft sah sich nach eigenen Angaben zu diesem Schritt gezwungen, da regulierende Behörden das gefordert hätten. Microsoft empfiehlt, auf die Windows Desktop Search umzusteigen.\n\nNachdem Microsoft den Extended-Support-Zeitraum für Windows XP im Jahr 2007 bis zum April 2014 verlängert hatte, endete er nach 13 Jahren am 8. April 2014 endgültig mit Ausnahme der Embedded-Versionen, bei denen der Extended-Support am 12. Januar 2016 endete. Microsoft weist darauf hin, dass es nach diesem Termin keinerlei Sicherheitsaktualisierungen und technischen Support mehr gibt. Für Großkunden mit einem gesonderten, kostenpflichtigen Supportvertrag wird Microsoft jedoch auch über dieses Datum hinaus für eine begrenzte Zeit Aktualisierungen zur Verfügung stellen. Da ein Jahr vor dem Supportende laut Netapplication der Marktanteil von Windows XP noch immer über 38 % lag, hat Microsoft die Get2Modern-Kampagne ins Leben gerufen, die kleine und mittlere Unternehmen dabei unterstützen soll, auf Windows 7 oder Windows 8 umzusteigen.\n\nMitte Januar 2014 gab der Konzern bekannt, dass die Microsoft Security Essentials, die System Center Endpoint Protection, sowie Forefront Client Security, Forefront Endpoint Protection und Windows Intune auch nach dem XP-Supportende am 8. April 2014 mit Updates versorgt werden. Dieser Teil-Support wurde bis zum 14. Juli 2015 aufrechterhalten.\n\nAm 2. Mai 2014 veröffentlichte Microsoft trotz ausgelaufenem Support-Lifecyle ein weiteres Sicherheitsupdate für Windows XP. Microsoft begründete dies mit der zeitlichen Nähe zum Supportende.\n\nNach dem Supportende kursierte im Internet eine Beschreibung für eine Modifikation an der Registrierungsdatenbank, durch die man über das Supportende hinaus Updates für Windows XP via Windows Update erhalten könne. Durch diese Änderung identifiziert sich das System als Windows Embedded POSReady 2009, ein auf Windows XP basierendes Kassenbetriebssystem, dessen Support-Lifecycle erst am 9. April 2019 endet. Von Seiten der Fachpresse wird jedoch von dieser Modifikation abgeraten, da diese Updates nicht für Windows XP entwickelt und getestet wurden.\n\nAufgrund eines schwerwiegenden Cyber-Angriffs auf ungepatchte Windows-Systeme im Mai 2017 mit dem Schadprogramm WannaCry, welches eine Sicherheitslücke in der Implementierung des SMB-Prokolls zur wurmartigen Verbreitung ausnutzte, veröffentlichte Microsoft am 12. Mai 2017 ein weiteres, außerplanmäßiges Sicherheitsupdate für Windows XP unter der Bezeichnung KB4012598. Ein weiteres außerplanmäßiges Update wurde von Microsoft im Juni 2017 unter der Bezeichnung KB4012583 veröffentlicht.\n\nEinige Nutzergruppen stellen die öffentlich verfügbaren Systemaktualisierungen (z. B. Sicherheitsaktualisierungen) gebündelt als sogenannte inoffizielle Service Packs zur Verfügung. Die Verwendung dieser inoffiziellen Service Packs wird von Microsoft nicht unterstützt und birgt die Gefahr einer Infektion, etwa mit Schadprogrammen.\n\nVerwendet ein Benutzer standardmäßig ein uneingeschränktes Benutzerkonto, so werden alle Programme im Sicherheitskontext eines Administratorkontos ausgeführt. Damit hat auch Schadsoftware (Viren, Würmer, Trojaner, Spyware, Adware usw.) alle Möglichkeiten, Veränderungen am System vorzunehmen. Oftmals werden diese Veränderungen so umgesetzt, dass der Anwender des befallenen Computers diese zunächst nicht bemerkt (z. B. wird eine Schadsoftware als Systemdienst eingerichtet und dann automatisch ständig ausgeführt).\n\nZur Lösung dieses Problems bietet Windows XP die Möglichkeit, den Computer standardmäßig mit einem eingeschränkten Benutzerkonto zu verwenden.\n\nZur Markteinführung von Windows XP waren viele Programme nicht an Windows-NT-Systeme angepasst, sie waren von Konzept und Realisierung her nur auf die nun beendete Windows-9x-Linie abgestimmt. Daher funktionierten sie oft nicht richtig, wenn der angemeldete Anwender nicht alle Administrator-Berechtigungen hatte. Später entwickelte Programme ließen sich dagegen auch vollständig mit einem „eingeschränkten Benutzerkonto“ benutzen. Für die systemweite Installation von Programmen ist ein Administratorkonto notwendig, da besondere Berechtigungen nötig sind, wenn Teile des Betriebssystems, dessen Konfiguration oder Einstellungen anderer Benutzer modifiziert werden. Auf Administratorrechte kann bei der Installation eines Programms nur verzichtet werden, wenn das Programm ausschließlich für das Benutzerprofil des angemeldeten Benutzers installiert wird. Unter Windows XP (auch Windows 2000 und Windows NT) können sehr detaillierte Berechtigungen auf Dateien und weitere Systemobjekte (z. B. Registry-Schlüsseln, Pipes etc.) vergeben werden.\n\nNeben älteren Spielen betraf diese Problematik weitere Programme, die nicht nur für den privaten Gebrauch vorgesehen waren.\n\nWie andere Microsoft-Produkte steht auch Windows XP unter der Kritik, durch den Kauf werde ein „Quasi-Monopolist“ unterstützt. Tatsächlich ist die Dominanz von Windows auf dem Heimcomputer-Betriebssystem-Markt unübersehbar, so erfordern viele Anwendungsgebiete Microsoft-Produkte und der Einsatz von Windows XP oder anderer Windows-Betriebssysteme ist dort – zumindest sekundär – zwingend.\n\nGerade Windows XP integrierte viele Anwendungen, die bisher durch andere Anbieter bereitgestellt worden waren, und wurde dafür stark kritisiert und teilweise streng beobachtet. Solche Anwendungen sind zum Beispiel Mediaplayer (Windows Media Player), Instant Messenger (Windows Messenger) oder die enge Bindung an das Microsoft-Passport-Netzwerk, das in der Fachwelt teilweise als ein Sicherheitsrisiko und eine potentielle Bedrohung der Privatsphäre angesehen wird. Das wird ebenso als eine Fortführung von Microsofts traditionell wettbewerbsbeschränkendem Verhalten angesehen.\n\nObwohl die jüngste Kritik vor allem diese drei Programme im Blick hatte, waren auch in früheren Windows-Versionen – beispielsweise Windows 95 – schon Komponenten so in das System integriert, dass sie mit herkömmlichen Mitteln nicht mehr trennbar waren (Unmöglichkeit der Deinstallation) und laut Microsoft auch überhaupt nicht mehr getrennt werden konnten. Vor allem der Webbrowser (Internet Explorer, siehe auch Browserkrieg) fiel dabei oft in Kritik, aber auch der graphische Dateimanager (Windows Explorer) oder der TCP/IP-Stack.\n\nMicrosoft argumentiert zudem, dass solche Systemwerkzeuge nicht mehr Spezialanforderungen bedienen, sondern in den Bereich allgemeinen Interesses gerückt seien und damit ihre Existenzberechtigung als allgemeine Komponenten des Betriebssystems verdienen würden. Als Bestätigung dieser Auffassung findet sich zudem fast kein anderes Betriebssystem, das nicht ebenfalls Systemwerkzeuge integriert hat.\n\nEbenso werden Neuentwicklungen für Windows von Microsoft teilweise nur für neuere Windows-Betriebssysteme verfügbar gemacht, obwohl diese technisch auch für ältere Windows-Versionen möglich wären, zum Beispiel DirectX oder die .NET-Laufzeitumgebung. Andererseits gibt es keinen Hersteller von Betriebssystemen, der Ergänzungen und Erweiterungen stets für alle älteren Versionen herausgibt.\n\nMicrosoft erfüllte manchmal nur notdürftig Gerichtsanordnungen bezüglich gebündelter Software durch Veröffentlichung von speziellen Downgrades oder Versionen ohne den betreffenden Software-Teil. Es wird dabei kritisiert, dass Microsoft diese Komponenten häufig nicht vollständig entfernt habe, auch wenn das technisch möglich gewesen wäre. Microsoft rechtfertigte diesen Schritt mit der Tatsache, dass Schlüsselfunktionen von Windows von dieser Software abhängen würden, so das HTML-Hilfesystem oder die Windows-Schreibtischoberfläche (Desktop).\n\nEin weiterer Kritikpunkt an Windows XP und seinen Komponenten ist die Übermittlung von Daten an Microsoft. Windows XP sendet regelmäßig Daten an Microsoft. Laut Microsoft handelt es sich dabei um Daten, deren Art veröffentlicht ist, Kritiker bezweifeln das jedoch. Keine Studie überprüfte bisher, welchen Inhalt diese in verschlüsselter Form übertragenen Datenpakete tatsächlich haben. Kritiker befürchten, dass kaum nur die Daten übermittelt werden, die Microsoft offiziell angibt; dafür seien die Pakete nach der Meinung mancher zu groß. Gegen eine Darstellung des Spiegels und des Heise-Verlags im Jahre 2002, dass beispielsweise der Windows Media Player die genutzten Medieninhalte an Microsoft-Server übermittle, protestierte Microsoft nicht öffentlich.\n\nWindows XP wurde seit seinem Erscheinen häufig mit dem freien Betriebssystem Linux verglichen. Es wurde argumentiert, dass die Anforderungen an die Hardware zu hoch und die von Microsoft herausgegebenen Mindestanforderungen unrealistisch für ein produktives Arbeiten seien. Ein paar Jahre später hat die Hardware-Entwicklung diese Aussage eingeholt, da auch preisgünstige Rechner genügend Leistung bringen. Tatsächlich wurde neben Linux auch Windows XP auf vielen Netbooks eingesetzt, auf denen der Windows-XP-Nachfolger Vista wegen dessen höherer Hardware-Anforderung nicht brauchbar gewesen wäre. Obwohl Windows Vista das aktuelle Windows-Betriebssystem war, verkaufte Microsoft ein besonders günstiges Windows XP speziell für Netbooks bis mindestens 2009. Erst dann waren einerseits etwas bessere Netbooks und andererseits mit Windows 7 Starter eine günstige Windows-Version auch für Netbooks verfügbar.\n\nDa die SATA-Schnittstelle bei der Produkteinführung noch sehr neu war, beinhaltet die Installations-CD noch keine generischen Treiber für diese Controller. Durch das Einstellen des IDE-Modus für den SATA-Controller im BIOS lässt sich Windows XP auch ohne SATA-Treiber installieren und bietet praktisch die gleiche Performance wie über den AHCI-Modus. Wenn das BIOS des Rechners keinen Modus für IDE-Kompatibilität bietet, kann auf die zur Installation vorgesehene Festplatte nicht ohne Weiteres zugegriffen werden. Wie bei anderen speziellen (SCSI, RAID) oder neuen Kontrollern kann man den benötigten Treiber mit einer Diskette – und nur mit dieser – während der Installation zur Verfügung stellen. Viele neue Computer verzichten aber auf ein Diskettenlaufwerk und eine Routine für einen CD-Wechsel oder das Laden über USB ist nicht vorgesehen. Es muss daher entweder ein Diskettenlaufwerk nachgekauft oder eine eigens angepasste Installations-CD erstellt werden.\n\nDie Systemvoraussetzungen für Windows XP \"Home\" und \"Professional\" Edition werden wie folgt angegeben:\n\nDabei ist zu beachten, dass diese Voraussetzungen für eine grundlegende Installation ohne zusätzliche Programme und sonstige Patches und auf Festplatten von maximal etwa 2 TB gelten.\n\nSeit etwa 2010 werden allerdings auch für den nicht-professionellen Einsatz in zunehmendem Umfang Festplatten von mehr als 2 Tebibyte (TiB) Gesamtgröße angeboten. Deren Partitionen können nicht mehr durch den seit der Einführung von DOS üblichen Master Boot Record (MBR) verwaltet werden, sondern dies erfolgt beispielsweise durch eine GUID Partition Table (GPT). Microsoft verweist darauf, dass dann je nach Version von Windows XP Einschränkungen sowohl hinsichtlich der Installierbarkeit des Systems als auch hinsichtlich der Nutzbarkeit der Kapazität gelten. Einschränkungen gelten laut Microsoft je nach Version von Windows XP auch für Festplatten, bei denen – unabhängig von der Größe des gesamten Mediums – die physische Größe der Sektoren nicht 512 Bytes, sondern beispielsweise 4 Kibibyte (KiB) beträgt. Software-Anpassungen an 4 KiB-Sektoren-Platten gibt es durch Microsoft nur für Windows 7 und jüngere Betriebssysteme. Daher wird die Kompatibilität solcher Platten mit Sektoren von mehr als 512 Bytes unter Windows XP herstellerseitig entweder (unabhängig vom Nutzer) mit besonderer Firmware oder (vom Nutzer anzuwenden) mit Anpassungs-Programmen (beispielsweise für Platten von Western Digital) erreicht.\n\nMessungen des tatsächlichen Nutzungsanteils eines Betriebssystems sind schwierig, so dass verschiedene Erhebungen deutlich unterschiedliche Ergebnisse liefern können. Laut der StatCounter, welches Webzugriffe analysiert, sei XP bis 2011 das am meisten eingesetzte Betriebssystem gewesen, ehe es im Laufe des Jahres 2011 von Windows 7 überholt worden sei. Auswertungen von Net Applications, das ebenfalls Webzugriffe analysiert, ergaben, dass Windows XP noch bis September 2012 das führende Betriebssystem gewesen sei.\n\nGeplant wollte Microsoft die Auslieferung im Januar 2008 beenden, da aber ihr Nachfolger Vista viel zu hohe Hardwareanforderungen an preisgünstige und mobile Rechner stellte, verschob der Konzern sein Aus bis zum 30. Juni 2008. Für Subnotebooks und Netbooks wurde Windows XP sogar bis 2010 ausgeliefert, um dieses Marktsegment – trotz schlankerem Windows 7 ab 2009 – nicht an Konkurrenten zu verlieren. Erst später konnten viele Subnotebooks und Netbooks mit Windows 8 / 8.1, welches ähnliche Hardwareanforderungen wie das sechs bzw. fünf Jahre ältere Vista hat, ausgeliefert werden.\n\n\n"}
{"id": "5672", "url": "https://de.wikipedia.org/wiki?curid=5672", "title": "Wget", "text": "Wget\n\nWget ist ein freies Kommandozeilenprogramm des GNU-Projekts zum Herunterladen von Dateien aus dem Internet. Zu den unterstützten Protokollen gehören ftp, http und https. Das Programm gibt es unter anderem für Unix, GNU/Linux, OS/2, Windows und SkyOS. Es steht unter der GNU General Public License. Wget wird mit diversen Linux-Distributionen mitgeliefert. Ein Programm ähnlicher Funktion ist cURL, das zudem das Hochladen unterstützt.\n\nAußer einzelnen Dateien kann Wget auf einfache Art zugleich die mit Webseiten assoziierten Ressourcen wie etwa Bilder herunterladen. Es kann auch komplette Websites kopieren, etwa zum Offline-Lesen oder zur Archivierung. Wget kann durch mehr als einhundert Kommandozeilenparameter detailliert konfiguriert werden.\n\nWget wurde ursprünglich 1995 von Hrvoje Nikšić entwickelt und erstmals im Januar 1996 (zuerst unter dem Namen Geturl) veröffentlicht. Von Januar 2008 bis April 2010 wurde Wget von Micah Cowan betreut und seit April 2010 ist der Hauptentwickler Giuseppe Scrivano.\n\n"}
{"id": "5739", "url": "https://de.wikipedia.org/wiki?curid=5739", "title": "Extended Graphics Array", "text": "Extended Graphics Array\n\nExtended Graphics Array (Abkürzung XGA) bezeichnet sowohl einen Typ Grafikkarte, der von IBM im Oktober 1990 für die IBM-PS/2-Serie von Computern eingeführt wurde (\"XGA Display Adapter/A\"), als auch den dazugehörigen Grafikstandard.\n\nNeben dem ursprünglichen \"XGA Display Adapter/A\" stellte IBM außerdem den ähnlichen \"XGA-2 Display Adapter/A\" her, der eine höhere erreichbare Farbtiefe ermöglichte.\n\nDie XGA-Hardware war abwärtskompatibel zum Grafikstandard VGA, der 8514/A-Treiber-Schnittstelle und bot außerdem einen 132-Spalten-Textmodus (üblich waren 40 und 80). Die maximale Bildauflösung betrug 1024 × 768 Bildpunkte, die maximale Farbtiefe 16 bit (65.536 Farben). Diese Maximalwerte konnten zwar aus Speichergründen nicht gleichzeitig erreicht werden, mit einer Erweiterung des Grafikspeichers waren jedoch 256 Farben bei 1024 × 768 Bildpunkten möglich, bzw. 65.536 Farben bei 800 × 600 Bildpunkten. Der Framebuffer ist bei XGA-Grafikkarten direkt ansprechbar, so dass ihre Hardware theoretisch jede Kombination aus Auflösung und Farbtiefe liefern kann, sofern der Grafikspeicher dazu ausreicht. Sie besitzen – anders als bei normalem VGA und analog zu 8514/A – hardwarebeschleunigte Zeichenbefehle und Cursor-Darstellung.\n\nDer XGA-Standard konnte sich im Bereich IBM-PC-kompatibler Computer nicht durchsetzen. Die wenig später erscheinenden SVGA-kompatiblen Grafikkarten von Fremdherstellern waren durch VESA-Kompatibilität vergleichbar flexibel, erreichten ähnliche Leistungen und waren für alle Busarchitekturen verfügbar. Vorbild für diese Urväter der modernen PC-Grafikhardware waren jedoch die Merkmale des XGA-Standards.\n\nDer Name \"XGA\" wurde in Anlehnung an diesen Grafikstandard auch in einem weiteren Sinne für einen Bildmodus innerhalb des Standards VESA 2.0 benutzt, der der maximalen Bildauflösung (1024 × 768 Bildpunkte) des XGA-Standards entspricht. Zusätzlich enthält VESA 2.0 den Bildmodus \"SXGA\" (\"Super\" XGA), mit 1280 × 1024 Bildpunkten, der aber außer dem Namen nichts mit dem ursprünglichen Standard zu tun hat. Auch ähnliche Marketingbezeichnungen für bestimmte Bildmodi, die in den 2000er-Jahren aufkamen und ebenfalls das Kürzel XGA enthalten, sind nicht mit dem XGA-Standard verwandt.\n\n\n"}
{"id": "5795", "url": "https://de.wikipedia.org/wiki?curid=5795", "title": "Zwischenablage", "text": "Zwischenablage\n\nDie Zwischenablage (englisch \"Clipboard\") ist ein Puffer, also ein Zwischenspeicher, für das kurzzeitige Speichern und Übertragen von Daten. Dieses Verfahren wird meist nur auf einem Computer zwischen Dokumenten oder Anwendungen angewandt. Betriebssysteme benutzen für die Interaktion mit der Zwischenablage die Technik Kopieren und Einfügen. Die meisten Programme greifen auf deren Schnittstellen für die Zwischenablage zu, wenn sie auf eine festgelegte Benutzereingabe, wie eine Tastenkombination oder eine Menüauswahl, reagieren. Ein Element in der Zwischenablage kann in verschiedenen Formaten vorgehalten werden. Jedes Programm bestimmt dabei in welchen Variationen die Daten gespeichert werden. Dies ist nützlich, da zum Zeitpunkt des Speicherns keine Rückschlüsse auf die Zielanwendung gezogen werden können.\n\nWindows, Linux und MacOS nutzen für die Zwischenablage den RAM und können nicht mehrere Elemente zwischenspeichern, da sie den vorherigen Inhalt überschreiben. Zudem wird das Objekt in der Zwischenablage nach dem Einfügen nicht gelöscht, sondern weiter vorgehalten. Ein Neustart oder Ausschalten des Systems leert aber dennoch den RAM und somit das gespeicherte Objekt. Die Verfahrensweise der Zwischenablage kann dabei zwischen Betriebssystemen und ihren Versionen variieren. Diese Kernfunktionen der Zwischenablage können zudem durch Clipboard Manager und Einstellungen verändert oder erweitert werden.\n\nDie Interaktionstechnik Drag and Drop benutzt die Zwischenablage dagegen nicht, obwohl die Funktionalität Ähnlichkeiten mit der von Kopieren und Einfügen aufweist.\n\nEine erste Form der Zwischenablage wurde von Pentti Kanerva im Texteditor TVEDIT implementiert. Die Software nutzte einen Zwischenspeicher für das Ausschneiden von Texten. Der Nutzer musste erst Zeichen auswählen und danach löschen. Darauffolgend konnte er diese an einer bestimmten Stelle wiederherstellen.\n\nSeitdem ein Text an einer Stelle gelöscht und an einer anderen Stelle wieder eingefügt werden konnte, entspricht der Begriff „Löschen“ in diesem Zusammenhang nicht mehr der eigentlichen Bedeutung des Wortes. Larry Tesler benannte das Szenario deswegen 1973 in Ausschneiden, Kopieren, Einfügen (englisch \"cut, copy and paste\") um. Außerdem definierte er den Begriff „clipboard“, was wörtlich übersetzt „Klemmbrett“ heißt, als Zwischenspeicher für diese Interaktionstechnik. Tesler war gleichzeitig auch einer der Entwickler des ersten PCs namens Apple Lisa, der diese Technologie ab Werk unterstützte.\n\nDie Zwischenablage unterstützt das Speichern von mehreren Typen des Datenobjekts zur selben Zeit, sodass komplexe Datenstrukturen ebenso abgelegt werden können. Diese reichen von Textformaten mittels Auszeichnungssprache, wie zum Beispiel RTF oder HTML, über eine Vielfalt von Bitmaps und Vektorgrafiken bis hin zu komplexen Varianten, wie Dateien und Verzeichnisse, Tabellen und Einträgen in Datenbanken. Das Ausschneiden von mehreren Zellen einer Tabelle und das darauffolgende Einfügen in eine andere Tabelle könnte beispielsweise die zugrunde liegenden Formeln, Datensätze und sogar automatische Übersetzungen der Referenzen innerhalb einer Zelle mit einfließen lassen. Dies würde zum Beispiel dazu führen, dass nach einem Kopieren und Einfügen von Zellen, die von einer Summenfunktion benutzt werden, ein Programm den Inhalt dieser Funktion mit den neuen Referenzen aktualisiert.\n\nEine Anwendung kann Daten in vielen verschiedenen Formaten der Zwischenablage hinzufügen. Diese können native, einfachere oder häufigere Datenformate sein, die eine höhere Chance haben von unterschiedlichsten Anwendungen erkannt zu werden. Deshalb kann die Zielanwendung beim Einfügen der Zwischenablage das Format wählen, das am ehesten dem der Anwendung entspricht, sodass nach Möglichkeit alle Originaldaten vorhanden bleiben.\n\nDas Hijacking der Zwischenablage ist ein Angriff, der den Inhalt der Zwischenablage gegen einen bösartigen Inhalt wie eine mit Viren behaftete Webseite austauscht. Während einige Sicherheitslücken geschlossen wurden, kann über JavaScript und damit über jede Webseite das sogenannte „pastejacking“ weiterverwendet werden. Dylan Ayrey fand diese Lücke und stellt eine Webseite bereit, die zeigen soll, dass das System Aktionen ausführt, die der Nutzer gar nicht beabsichtigt.\n\n2013 entdeckten Forscher ein Sicherheitsrisiko von Applikationen zur Passwortverwaltung auf Android-Geräten. Sie fanden heraus, dass 21 der beliebtesten dieser Anwendungen Passwörter ausgelesen werden können. Dies geschieht durch die Verwendung einer anderen Applikation und den Zugriff auf die Zwischenablage. Joe Siegrist sagte, dass dieser Angriff alle Anwendungen des Betriebssystems Android betrifft. Das Fraunhofer Institut SIT fand 2016 nochmals eine ähnliche Sicherheitslücke, die es ebenfalls ermöglichte Passwörter aus der Zwischenablage zu stehlen.\n\nDie Zwischenablage in Windows speichert einen Eintrag in 3 verschiedenen Formaten. Jeder Eintrag liegt in mindestens einem dieser Formate vor.\n\n\nWindows unterstützt außerdem das verzögerte Erstellen (englisch delayed rendering) der Formate eines zwischengespeicherten Datenobjekts. Das bedeutet, dass die Anwendung nur bei Bedarf ein Format generiert und in die Zwischenablage verschiebt. Falls also eine Applikation zeitaufwändige Prozesse benötigt, um die Daten in ein spezielles Format aufzubereiten, können die lang andauernden Rechenoperationen mit dieser Funktionalität umgangen werden.\n\nBis einschließlich Windows XP konnte auf die Zwischenablage mit Hilfe von clipbrd.exe zugegriffen werden. Diese Anwendung erlaubte es dem Nutzer, die aktuelle Zwischenablage in einer CLP Datei abzuspeichern. Darauffolgend konnte die Datei entweder im eigenen System oder per Übertragung innerhalb eines anderen Windowscomputer verwendet werden. Indem die CLP mittels clipbrd.exe in die Zwischenablage importiert wird, kann der Anwender den Inhalt an den gewünschten Ort des Zielsystems einfügen. Dies ist nützlich, da die verschiedenen Formate gespeichert werden und somit auch für andere Nutzer eine funktionierende Variante bereithält.\n\nMit Windows 10 1809 erweitert Microsoft die Möglichkeiten der Zwischenablage deutlich. Die Zwischenablage erhält einen Verlauf, der es ermöglicht mehrere Objekte oder Textpassagen in die Zwischenablage zu übernehmen. Aus dem Verlauf kann dann beim Einfügen das gewünschte Objekte ausgewählt werden. Zusätzlich kann der Verlauf der Zwischenablage mit der Cloud (automatisch oder manuell) synchronisiert werden.\n\nAußerdem ist die Zwischenablage über PowerShell erreichbar:\n\nSet-Clipboard -Path \"C:\\directory\\\"\nGet-Clipboard\n\nDaten können auch über die Eingabeaufforderung gespeichert werden:\n\n$ # Um den Inhalt eines Ordners in die Zwischenablage zu speichern.\n$ dir | clip\nAuch macOS speichert einen Eintrag in verschiedenen Formaten in die Zwischenablage.\n\nDessen Inhalt kann über das Menü der Schaltfläche Bearbeiten des Finders und die Auswahl des Eintrags „Zeige Zwischenablage“ angezeigt werden. Außerdem kann die Zwischenablage geräteübergreifend mit derselben Apple-ID benutzt werden, solange sie WLAN, Bluetooth und Handoff aktiviert haben. So kann ein kopierter Text auf einem Gerät, auf einem Zweiten eingefügt werden.\n\nMit Hilfe folgender Kommandos kann die Zwischenablage benutzt werden:\n\n$ # Um Daten in die Zwischenablage zu kopieren.\n$ echo 'hello world' | pbcopy\n$ # Um aus dem Clipboard heraus einzufügen.\n$ pbpaste\nhello world\nDas X Window System, das vorwiegend in Unix oder Linux basierten Betriebssystemen eingesetzt wird, stellt drei Zwischenablagen namens „PRIMARY“, „SECONDARY“ und „CLIPBOARD“ bereit.\n\nDie Interaktion mit diesen ist nicht standardisiert. Dennoch benutzen die meisten modernen Bibliotheken und Desktopumgebungen, wie GNOME oder KDE, eine weit verbreitete Konvention, die durch freedesktop.org spezifiziert wurde.\n\n„CLIPBOARD“ wird identisch wie das Pendant aus Windows benutzt. Der Unterschied zu den bereits erwähnten Systemen ist, dass die Daten nicht extra in der Zwischenablage abgelegt werden, sondern nur deren Referenz. Die Anwendung übernimmt den Besitz der Selektion und kommuniziert dies dem X Server. Sobald ein Einfügen erfolgt, wird das Datenobjekt und seine verfügbaren Formate von der Anwendung angefordert und abhängig davon eingefügt.\n\n„PRIMARY“ hingegen, ist ein X11 spezifischer Mechanismus. Das bedeutet, dass die Selektion direkt ein Kopieren der Daten zur Folge hat und über die dritte beziehungsweise mittlere Maustaste eingefügt wird. Dieser Prozess ist komplett unabhängig von der Variante „CLIPBOARD“ und verändert deshalb nicht dessen Inhalt.\n\n„SECONDARY“ wurde als Alternative zu „PRIMARY“ geplant, wird aber meist nicht verwendet.\n\nZwei Kommandos ermöglichen den Zugriff auf das „CLIPBOARD“:\n\n$ # Speichern mit Hilfe von xclip.\n$ echo text | xclip -in -selection clipboard\n$ # Speichern mit Hilfe von xsel.\n$ echo text | xsel --clipboard\n\nAuf der Linux-Konsole bietet GPM (General Purpose Mouse Manager) eine ähnliche Funktion, allerdings nur für Text. Das Einfügen erfolgt auf der Linuxkonsole mit der Rolltaste der Maus beziehungsweise der mittleren Maustaste.\n\nDas Betriebssystem Amiga verwendet 256 Speicher für die Zwischenablage, sodass ein Nutzer mehrere Zwischenablagen zur gleichen Zeit nutzen kann. Diese werden in einem speziellen Speicherplatz aufbewahrt, der durch die AmigaDOS Zuweisung codice_1 definiert ist.\n\nAndroid stellt ein Framework bereit, das ein Objekt zwischenspeichert und systemweit verfügbar macht. Während einfacher Text direkt gespeichert wird, werden komplexe Datenstrukturen als Referenz abgelegt. Das Objekt gehört zu einer von drei verschiedenen Arten:\n\n\nDer Inhalt des Clipboards kann nur durch Anwendungen oder das System verwendet werden.\n\nEntwickler können die Klasse ClipboardManager benutzen, um das Kopieren und Einfügen von Daten zu steuern.\n\nDie Zwischenablage in iOS hat den Namen „Pasteboard“ und hat eine ähnliche Funktionsweise wie das Pendant in macOS. Applikationen können weitere Pasteboards mit Hilfe von Instanzen der UIPasteboard Klasse erstellen. Diese können privat oder öffentlich gesetzt werden. Eine Instanz kann einen oder mehrere Einträge mit verschiedenen Formatierungen, die durch Uniform Type Identifiers festgelegt wurden, enthalten.\n\nÄhnlich zu Android können die Daten der Zwischenablage nicht über die GUI des Betriebssystems erreicht werden. Nur Anwendungen und das System selbst können auf diese zugreifen.\n\nFür viele Programmiersprachen gibt es Bibliotheken und APIs, die eine einfache Zugriffsmöglichkeit auf die Zwischenablage bereitstellen.\n\nJavaScript stellt eine Schnittstelle über ein Event (ClipboardEvent) bereit, die ein Lesen und Setzen von Daten erlaubt. Dies wird aber nicht von jedem Browser unterstützt, da eine Manipulation dieses Speichers ein Sicherheitsrisiko bedeuten kann.\n\nDas Framework für plattformübergreifende Benutzerschnittstellen Qt wird unter anderem von Python und C++ benutzt. Qt enthält durch die Klasse QClipboard, eine Möglichkeit auf die verschiedenen Zwischenablagen Schnittstellen der Betriebssysteme zuzugreifen. Dadurch können gewöhnliche Datentypen in der Zwischenablage über Funktionen erreicht werden. Jedes gespeicherte Element wird mittels MIME typisiert und kann somit ausgelesen sowie in einen bestimmten Datentyp gespeichert werden.\n\nClipboard Manager fügen zu der bereits integrierten Zwischenablage weitere Funktionen hinzu, sodass der Nutzer den Speicher verändern kann. Viele Anwendungen dieser Art können mehrere Einträge zwischenspeichern und einfügen. Dabei kann der letzte Eintrag trotzdem wie üblich benutzt werden. Des Weiteren wird dem Nutzer oft die Möglichkeit gegeben, den Verlauf der Speichereinträge zu sehen, zu selektieren, zu editieren, in andere Formate zu konvertieren und zu durchsuchen.\n\nDa die meisten Betriebssysteme (Windows, macOS, Linux, X11, Android, iOS) den Inhalt der Zwischenablage nicht in den Festspeicher ablegen, wird dieser gelöscht sobald sich der Nutzer abmeldet oder das System neustartet. Deswegen implementieren Clipboard Manager sowie Anwendungen wie der Texteditor Emacs häufig eine persistente Speicherung.\n\n"}
{"id": "8474", "url": "https://de.wikipedia.org/wiki?curid=8474", "title": "GNU Compiler Collection", "text": "GNU Compiler Collection\n\nGCC ist der Name der Compiler-Suite des GNU-Projekts. GCC stand ursprünglich für GNU C Compiler. Da GCC heute aber außer C noch einige andere Programmiersprachen übersetzen kann, hat GCC inzwischen die Bedeutung GNU Compiler Collection erhalten ( für \"GNU-Compilersammlung\"). Das Kommando gcc (in Kleinbuchstaben) steht weiterhin für den C-Compiler.\n\nDie Sammlung enthält Compiler für die Programmiersprachen C, C++, Objective-C, Fortran, Ada und Go. Die Compilersammlung unterliegt den Bedingungen der GNU General Public License.\n\nGCC wird von einer Reihe von Systemen als Standardcompiler genutzt, darunter viele Linux-Distributionen, BSD-Varianten, NextStep, BeOS und ZETA. Zudem bietet er auch Unterstützung für die Laufzeitumgebung Cygwin und die Entwicklerwerkzeuge MinGW. Er wurde auf mehr Systeme und Rechnerarchitekturen portiert als jeder andere Compiler und bietet sich besonders für Betriebssysteme an, die auf verschiedenen Hardware-Plattformen laufen sollen. Der GCC lässt sich auch als Cross-Compiler installieren.\n\n2014 erhielt sie den Programming Languages Software Award von ACM SIGPLAN.\n\nDie erste öffentliche Version (0.9) des GCC wurde am 22. März 1987 von Richard Stallman für das GNU-Projekt freigegeben (Version 1.0 erschien am 23. Mai desselben Jahres) und wird heute von Programmierern auf der ganzen Welt weiterentwickelt. Die Erweiterung des C-Compilerpakets zur Compiler-Collection erfolgte im Rahmen des EGCS-Projektes, das eine Weile parallel zum GCC existierte und schließlich zum offiziellen GCC wurde.\n\n1997 spaltete sich das Projekt \"Experimental/Enhanced GNU Compiler System\" (\"EGCS\", engl. für \"experimentelles/verbessertes GNU-Compilersystem\") von GCC ab, und wurde 1999 mit diesem wieder vereinigt.\n\nGCC 1.x hatte 1991 eine gewisse Stabilität erreicht, jedoch verhinderten architekturbedingte Einschränkungen viele Verbesserungen, sodass die Free Software Foundation (FSF) damit begann, GCC 2.x zu entwickeln. Mitte der 1990er kontrollierte die FSF jedoch sehr genau, was zu GCC 2.x hinzugefügt werden durfte und was nicht, sodass GCC als Beispiel für das „Cathedral“-Entwicklungsmodell Verwendung fand, das Eric S. Raymond in seinem Buch Die Kathedrale und der Basar beschreibt.\n\nDie Tatsache, dass GCC freie Software ist, erlaubte es Programmierern, die in eine andere Richtung arbeiten wollten, eigene Abspaltungen zu entwickeln. Viele Abspaltungen erwiesen sich jedoch als ineffizient und unübersichtlich. Dass ihre Arbeiten vom offiziellen GCC-Projekt oft nicht, oder nur unter Schwierigkeiten akzeptiert wurden, frustrierte viele Entwickler.\n\nDaher gründete eine Gruppe von Entwicklern 1997 EGCS, um mehrere experimentelle Abspaltungen in einem einzigen Projekt zu vereinen. Dazu gehörten g77 (Fortran), PGCC (Pentium-optimierter GCC), das Einpflegen vieler Verbesserungen an C++, sowie Compiler-Versionen für weitere Prozessor-Architekturen und Betriebssysteme.\n\nDie Entwicklung von EGCS erwies sich als schneller, lebhafter und insgesamt besser als die des GCC-Projektes, sodass die FSF 1999 offiziell die Weiterentwicklung von GCC 2.x einstellte und stattdessen EGCS als offizielle GCC-Version übernahm. Die EGCS-Entwickler wurden zu Projektverantwortlichen (engl. ) des GCC. Von da an wurde das Projekt explizit nach dem „Basar“-Modell entwickelt, nicht mehr nach dem „Cathedral“-Modell. Mit der Veröffentlichung von GCC 2.95 im Juli 1999 waren beide Projekte wiedervereinigt.\n\nDas GCC-Projekt bezeichnet einige Plattformen offiziell als primäre und andere als sekundäre Evaluationsplattformen. Vor jeder Veröffentlichung einer neuen Version werden insbesondere diese beiden Gruppen getestet. GCC kann Programme für folgende Prozessoren erzeugen (primäre und sekundäre Evaluationsplattformen sind markiert):\n\nDazu kommt noch eine Reihe von Prozessoren von eingebetteten Systemen, wie\n\nNicht Bestandteil des offiziellen GCC, aber davon abgeleitet und kommerziell vertrieben gibt es Derivate für\n\nInsgesamt unterstützt der GCC mehr als 60 Plattformen.\n\nDas externe Interface des gcc entspricht dem eines Standard-Unix-Compilers.\n\n\nJeder Sprachcompiler ist ein separates Programm, das Quellcode entgegennimmt und Assemblersprache produziert. Im Schema auf der rechten Seite sind Beispiele für C und Assembler gegeben, welche sich beide dem Preprocessing unterziehen müssen, bei dem Compilermakros, eingebundene Header-Dateien und Ähnliches umgewandelt werden, um reinen C-Code bzw. Assembler zu erhalten. Jenes sprachabhängige Frontend parst die entsprechende Sprache und erzeugt einen abstrakten Syntaxbaum, der an ein Backend übergeben wird, das den Baum in GCCs \" (RTL)\" überführt (im Diagramm nicht gezeigt), verschiedene Codeoptimierungen durchführt und zum Schluss Assemblersprache erzeugt.\n\nUrsprünglich wurden die meisten Bestandteile der GCC in C geschrieben. Im Rahmen des Vorhabens „GCC in Cxx“ wurde 2010 die Umstellungen der gcc-Quellen auf C++ geplant und begonnen. Ziel dieser Umstellung ist, die GCC verständlich und wartbar zu halten. Im Nachfolgeprojekt wurde auch die noch fehlende Stufe 1 des GCC-Bauprozesses auf C++-Code umgestellt. Ausnahmen sind Backends, die in wesentlichen Teilen in RTL formuliert sind, sowie das Ada-Frontend, welches zum größten Teil in Ada geschrieben ist.\n\nFrontends müssen Bäume produzieren, die vom Backend verarbeitet werden können. Wie sie dies erreichen, bleibt ihnen überlassen. Einige Parser benutzen Yacc-ähnliche Grammatiken, andere verwenden handgeschriebene, rekursive Parser.\n\nBis vor kurzem war die Baumrepräsentation des Programms nicht völlig vom Zielprozessor unabhängig. Die Bedeutung eines Baums konnte für unterschiedliche Sprachfrontends unterschiedlich sein, und Frontends konnten ihren eigenen Baumcode zur Verfügung stellen.\n\nMit dem Tree-SSA-Projekt, das in die Version GCC 4.0 integriert wurde, wurden zwei neue Formen von sprachunabhängigen Bäumen eingeführt. Diese neuen Baumformate wurden \"GENERIC\" und \"GIMPLE\" getauft. Parsing wird nun durchgeführt, indem ein temporärer sprachabhängiger Baum nach GENERIC konvertiert wird. Der sogenannte „Gimplifier“ überführt diese komplexe Form in die SSA-basierte GIMPLE-Form, von der ausgehend eine Reihe neuer sprach- und architekturunabhängiger Optimierungen durchgeführt werden kann.\n\nOptimierung an Bäumen passt eigentlich nicht in das Schema von „Frontend“ und „Backend“, da sie nicht sprachabhängig sind und kein Parsen beinhalten. Die GCC-Entwickler haben diesem Teil des Compilers daher den Namen „Middleend“ gegeben. Zu den gegenwärtig am SSA-Baum durchgeführten Optimierungen gehören \"Dead code elimination\", \"Partial Redundancy Elimination\", \"Global Value Numbering\", \"Sparse Conditional Constant Propagation\", und \"Scalar replacement of Aggregates\". Array-basierende Optimierungen wie zum Beispiel automatische Vektorisierung, wie sie der Intel-Compiler anbietet, werden gegenwärtig entwickelt.\n\nDas Verhalten des GCC-Backends wird teilweise durch Präprozessor-Makros und architekturspezifische Funktionen bestimmt, mit denen zum Beispiel die Endianness, Wortgröße, und Aufrufkonventionen definiert und die Registerstruktur der Zielmaschine beschrieben werden. Unter Verwendung der Maschinenbeschreibung, einer Lisp-ähnlichen Beschreibungssprache, wandelt GCC die interne Baumstruktur in die RTL-Darstellung um. Obwohl diese dem Namen nach prozessorunabhängig ist, ist die Sequenz an abstrakten Instruktionen daher bereits an das Ziel angepasst.\n\nDie Art und Anzahl der vom GCC an der RTL durchgeführten Optimierungen werden mit jeder Compiler-Version weiterentwickelt. Zu ihnen gehören etwa \"(global) common subexpression elimination\", verschiedene Schleifen- und Sprungoptimierungen () sowie der \"combine-pass\", in dem mehrere Instruktionen zu einer einzigen kombiniert werden können.\n\nSeit der kürzlichen Einführung von globalen SSA-basierten Optimierungen an GIMPLE-Bäumen haben die RTL-Optimierungen leicht an Bedeutung verloren, da in der RTL-Repräsentation des Programms weit weniger der für viele Optimierungen wichtigen High-Level-Informationen enthalten sind. Allerdings sind auch maschinenabhängige Optimierungen sehr wichtig, da für viele Optimierungen Informationen über die Maschine vorliegen müssen, etwa darüber, welche Instruktionen eine Maschine kennt, wie teuer diese sind und wie die Pipeline der Zielarchitektur beschaffen ist.\n\nIn der „Reload“-Phase wird die prinzipiell unbeschränkte Anzahl an abstrakten Pseudo-Registern durch die begrenzte Anzahl an echten Maschinenregistern ersetzt, wobei hier unter Umständen neue Instruktionen in den Code eingefügt werden müssen, um zum Beispiel Pseudo-Register auf dem Stack der Funktion zwischenzuspeichern. Diese Registerzuteilung ist recht kompliziert, da die verschiedenen Eigenheiten der jeweiligen Zielarchitektur besonders berücksichtigt werden müssen.\n\nIn der letzten Phase werden Optimierungen durchgeführt, wie \"peephole optimization\" (engl. für \"Guckloch-Optimierung\") und \"delay slot scheduling\" (engl. wörtlich für \"Verzögerung-Schlitz-Ablaufplanung\"), bevor die recht maschinennahe Ausprägung der RTL auf Assemblercode abgebildet wird, indem die Namen von Registern und Adressen in Zeichenketten umgesetzt werden, welche die Instruktionen spezifizieren.\n\n\n\n"}
{"id": "8745", "url": "https://de.wikipedia.org/wiki?curid=8745", "title": "Finite-Elemente-Methode", "text": "Finite-Elemente-Methode\n\nDie Finite-Elemente-Methode (FEM), auch „Methode der finiten Elemente“ genannt, ist ein allgemeines, bei unterschiedlichen physikalischen Aufgabenstellungen angewendetes numerisches Verfahren. Am bekanntesten ist die Anwendung der FEM bei der Festigkeits- und Verformungsuntersuchung von Festkörpern mit geometrisch komplexer Form, weil sich hier der Gebrauch der klassischen Methoden (z. B. die Balkentheorie) als zu aufwändig oder nicht möglich erweist. Logisch basiert die FEM auf dem numerischen Lösen eines komplexen Systems aus Differentialgleichungen.\n\nDas Berechnungsgebiet (z. B. der Festkörper) wird in endlich viele Teilgebiete (z. B. Teilkörper) einfacher Form aufgeteilt, z. B. in viele kleine Quaderchen oder Tetraeder. Sie sind die „finiten Elemente“. Ihr physikalisches Verhalten kann aufgrund ihrer einfachen Geometrie mit bekannten Ansatzfunktionen gut berechnet werden. Das physikalische Verhalten des Gesamtkörpers wird dadurch nachgebildet, wie diese Elemente auf die Kräfte, Lasten und Randbedingungen reagieren und wie sich Lasten und Reaktionen beim Übergang von einem Element ins benachbarte fortpflanzen durch ganz bestimmte problemabhängige Stetigkeitsbedingungen, die die Ansatzfunktionen erfüllen müssen.\n\nDie Ansatzfunktionen enthalten Parameter, die in der Regel eine physikalische Bedeutung besitzen, wie z. B. die Verschiebung eines bestimmten Punkts im Bauteil zu einem bestimmten Zeitpunkt. Die Suche nach der Bewegungsfunktion ist auf diese Weise auf die Suche nach den Werten der Parameter der Funktionen zurückgeführt. Indem immer mehr Parameter (z. B. immer mehr, kleinere Elemente) oder immer höherwertige Ansatzfunktionen benutzt werden, kann die Genauigkeit der Näherungslösung verbessert werden.\n\nDie Entwicklung der FEM war in wesentlichen Etappen nur mittels der Entwicklung leistungsfähiger Computer möglich, da sie erhebliche Rechenleistung benötigt. Daher wurde diese Methode von vornherein computergerecht formuliert. Sie brachte einen wesentlichen Fortschritt bei der Behandlung von Berechnungsgebieten beliebiger Form.\n\nMit der FE-Methode können Probleme aus verschiedenen physikalischen Disziplinen berechnet werden, da es sich grundsätzlich um ein numerisches Verfahren zur Lösung von Differentialgleichungen handelt. Zunächst wird das Berechnungsgebiet („Bauteil“) in eine große Anzahl von Elementen unterteilt – ausreichend fein. Diese Elemente sind endlich klein (finit), d. h. ihre tatsächliche Größe bleibt relevant und ist damit nicht unendlich klein (infinit). Das Aufteilen des Gebiets/Bauteils in eine bestimmte Anzahl Elemente finiter Größe, die sich mit einer endlichen Zahl von Parametern beschreiben lassen, gab der Methode den Namen „Finite-Elemente-Methode“.\n\nFür diese Elemente gibt es Ansatzfunktionen (z. B. lokale Ritz-Ansätze je Element), die beschreiben, wie es auf äußere Einflüsse und Randbedingungen reagiert. Setzt man diese Ansatzfunktionen in die zu lösende Differentialgleichung ein, die die physikalischen Gesetze beschreiben, erhält man zusammen mit den Anfangs-, Rand- und Übergangsbedingungen ein Gleichungssystem. Es (zumindest näherungsweise) zu lösen ist die Aufgabe des FE-Gleichungslösers. Die Größe des zu lösenden Gleichungssystems hängt maßgeblich von der Anzahl der finiten Elemente ab. Seine Näherungslösung stellt letztlich die numerische Lösung der betrachteten Differentialgleichung dar – ist für alle Elemente gelöst, wie sie sich unter den Lasten verhalten, so hat sich dadurch auch die Reaktion des Gesamtbauteils ergeben.\n\nDer Einsatz der FEM in der Praxis begann in den 1950er Jahren bei einer Strukturberechnung von Flugzeugflügeln in der Luft- und Raumfahrtindustrie (Turner, Clough 1956) und sehr bald auch im Fahrzeugbau. Die Methode basiert hier auf den Arbeiten bei der Daimler AG in Stuttgart, die das selbst entwickelte FEM-Programm ESEM (Elastostatik-Element-Methode) einsetzte, lange bevor die computerunterstützte Konstruktion (CAD) Anfang der 1980er Jahre ihren Einzug hielt. Der Ausdruck Finite-Elemente-Methode wurde erstmals 1960 von R. W. Clough vorgeschlagen und wird seit den 1970er Jahren überall verwendet. Die gängigste deutschsprachige Bezeichnung für industrielle Anwender ist Berechnungsingenieur.\n\nDie Geschichte der Finite-Elemente-Methode erschließt sich aus den Forschungen und Veröffentlichungen der folgenden Autoren (Auswahl):\n\nDie erste Anwendung der FEM war die lineare Behandlung von Festkörpern und Strukturen in Form der Verschiebungsmethode und davon ausgehend hat die FEM ihre Anstöße erhalten. Die Bezeichnung „Finite Elemente“ wurde erst etwas später benutzt. Im weiteren Verlauf der Forschung wurde die Finite-Elemente-Methode immer weiter verallgemeinert und kann nunmehr in vielen physikalischen Problemstellungen, u. a. in verschiedenen gekoppelten Feldberechnungen, Wettervorhersagen oder bei technischen Fragestellungen in den Branchen Fahrzeugbau, Medizintechnik, Luft- und Raumfahrttechnik, Maschinenbau oder Konsumgüter in den Ingenieurwissenschaften verwendet werden. Ein Haupteinsatzgebiet der Methode ist die Produktentwicklung, wobei unter anderem mechanische Festigkeitsberechnungen einzelner Komponenten oder beispielsweise komplette Fahrwerks- und Karosseriestrukturen berechnet werden, um aufwändige Crashtests zu sparen.\n\nProgramme, welche die Finite-Elemente-Methode verwenden, arbeiten nach dem EVA-Prinzip: Der Anwender erstellt in einem CAD-Programm eine (Bauteil-)Geometrie. Anschließend gibt er im sogenannten FE-Präprozessor weitere Eingaben vor. Ein FEM-Gleichungslöser führt die eigentliche Rechnung durch, und der Benutzer erhält die berechneten Ergebnisse, welche er dann im sogenannten FE-Postprozessor in Form grafischer Anzeigen betrachten kann. Oft sind Prä- und Postprozessor in einem Programm kombiniert oder sogar Bestandteil des CAD-Programms.\nIm CAD-Programm wird das Bauteil konstruiert und mittels einer Direktschnittstelle oder mit einem neutralen Austauschformat wie STEP in den FE-Präprozessor übertragen. Durch die Anwahl von Netzparametern wie Elementgröße und Elementart (z. B. Tetraeder, Hexaeder bei 3D) im Vernetzungsmodul werden mit Hilfe eines Vernetzungsalgorithmus die Finiten-Elemente erzeugt. Für die mechanische Festigkeitsanalyse ist das Materialverhalten einzugeben, das maßgeblich angibt, welche Reaktionen das Bauteil auf äußere Belastungen einnimmt (z. B. Verformung). Je nach Werkstoff ist der Zusammenhang zwischen Spannung und Dehnung unterschiedlich und es ergeben sich verschiedene Verformungen. Wenn dieser Zusammenhang linear ist, werden für die FE-Berechnung lediglich der Elastizitätsmodul und die Poissonzahl benötigt, sonst sind weitere Werkstoffkennwerte und Eingaben im Präprozessor nötig.\nWeitere Randbedingungen sind zum Beispiel einwirkenden Belastungen auf das Bauteil (Kräfte, Druck, Temperatur, etc.). Um eine möglichst realitätsnahe Abbildung zu erhalten, werden schließlich die homogenen (Einspannungen) und die inhomogenen Randbedingungen (Verschiebungen) sowie alle zu berücksichtigenden Lasten auf das Modell angegeben.\n\nJe nach Programm kommt nun ein separater (eigenständiges Programm) oder ein integrierter Gleichungslöser zum Einsatz. Es berechnet die Simulation, wie sich die Lasten, Kräfte und Randbedingungen auf die Einzelelemente des Bauteils auswirken, und wie sich die Kräfte sowie die Auswirkungen im Bauteil fortpflanzen und auf benachbarte Elemente auswirken. Die Berechnung liefert zunächst eine grobe Näherungslösung. Weitere Iterationen verbessern die Näherung. Meist werden so viele Iterationen berechnet, bis sich nur noch geringste Änderungen ergeben – dann hat die Näherung „konvergiert“ und ist das Ergebnis der Simulation.\n\nIm Falle der mechanischen Festigkeitsberechnung erhält der Benutzer als Ergebnis des FEM-Gleichungslösers insbesondere Spannungs-, Deformations- und Dehnungswerte. Diese kann der Postprozessor zum Beispiel in einem Falschfarbenbild darstellen. Die Vergleichsspannungswerte werden beispielsweise zum Festigkeitsnachweis eines Bauteils verwendet.\n\nDie Finite-Elemente-Methode ist ein diskretes Verfahren, d. h. die Lösung wird auf einer diskreten Untermenge des Grundgebietes berechnet.\nHierzu wird dieses in einfache Teilgebiete, die so genannten finiten Elemente zerlegt (Vernetzung). Die Bezeichnung „finit“ hebt den Unterschied zur analytischen Betrachtung auf infinitesimalen Elementen hervor. Die Ecken der finiten Elemente heißen Knoten. Diese Knoten bilden die diskrete Untermenge für das numerische Verfahren. Auf den Elementen werden Approximationsfunktionen eingeführt, welche\ndie unbekannten Knotengrößen als Parameter enthalten. Die lokalen Approximationen werden in die schwache Formulierung des Randwertproblems eingeführt. Die dabei entstehenden Elementintegrale\nwerden mit numerischer Quadratur berechnet. Dabei werden die Approximationsansätze „herausintegriert“, so dass auf den Elementen nach der Integration nur noch die Knotenwerte als Unbekannte verbleiben. Durch Kontinuitätsforderungen an den Elementgrenzen werden die Elementgleichungen assembliert. Auf diese Weise werden Randwertprobleme für lineare partielle Differentialgleichungen in\nein lineares Gleichungssystem mit symmetrischen Systemmatrizen überführt. Für nichtlineare Differentialgleichungen verläuft der Algorithmus analog mit dem Unterschied, dass die nichtlinearen\nAbhängigkeiten mit geeigneten Methoden (z. B. Newton-Verfahren) iterativ linearisiert werden und das lineare Gleichungssystem in jedem Teilschritt für inkrementelle Größen aufgestellt wird.\n\nBei gewissen Aufgabenstellungen ist die Unterteilung in Elemente durch das Problem bereits weitgehend vorgegeben, zum Beispiel bei räumlichen Fachwerken, bei denen die einzelnen Stäbe die Elemente der Konstruktion bilden. Das gilt auch bei Rahmenkonstruktionen, wo die einzelnen Balken oder unterteilte Balkenstücke die Elemente der Aufgabe darstellen.\nBei zweidimensionalen Problemen wird das Grundgebiet in Dreiecke, Parallelogramme, krummlinige Dreiecke oder Vierecke eingeteilt. Selbst wenn nur geradlinige Elemente verwendet werden, erreicht man mit einer entsprechend feinen Diskretisierung eine recht gute Approximation (Annäherung) des Grundgebietes. Krummlinige Elemente erhöhen die Güte der Annäherung. Jedenfalls erlaubt diese Diskretisierung eine flexible und auch dem Problem angepasste Erfassung des Grundgebietes. Allerdings muss darauf geachtet werden, dass sehr spitze oder überstumpfe Winkel in den Elementknoten vermieden werden, um numerische Schwierigkeiten auszuschließen. Dann wird das gegebene Gebiet durch die Fläche der approximierenden Elemente ersetzt. Mit dem Patch-Test kann man später überprüfen, ob das gut gelungen ist.\n\nRäumliche Probleme werden mit einer Unterteilung des dreidimensionalen Gebietes in Tetraederelemente, Quaderelemente oder andere dem Problem angepasste, möglicherweise auch krummflächig berandete Elemente, dies sind i. d. R. Serendipity- oder Lagrange-Elemente, bearbeitet.\n\nDie Feinheit der Unterteilung, d. h. die Dichte des Netzes, hat maßgeblichen Einfluss auf die Genauigkeit der Resultate der Näherungsrechnung. Da gleichzeitig der Rechenaufwand bei der Verwendung feinerer und dichterer Netze steigt, gilt es, möglichst intelligente Vernetzungslösungen zu entwickeln.\n\nIn jedem der Elemente wird für die gesuchte Funktion, bzw. allgemeiner für die das Problem beschreibenden Funktionen, ein problemgerechter Ansatz gewählt. Im Besonderen eignen sich dazu ganze rationale Funktionen in den unabhängigen Raumkoordinaten. Für eindimensionale Elemente (Stäbe, Balken) kommen Polynome ersten, zweiten, dritten und gelegentlich sogar höheren Grades in Frage. Bei zweidimensionalen Problemen finden lineare, quadratische oder höhergradige Polynome Verwendung. Die Art des Ansatzes hängt dabei einerseits von der Form des Elementes ab, und andererseits kann auch das zu behandelnde Problem den zu wählenden Ansatz beeinflussen. Denn die Ansatzfunktionen müssen beim Übergang von einem Element ins benachbarte ganz bestimmte problemabhängige Stetigkeitsbedingungen erfüllen. Die Stetigkeitsanforderungen sind häufig aus physikalischen Gründen offensichtlich und aus mathematischen Gründen auch erforderlich. Zum Beispiel muss die Verschiebung eines zusammenhängenden Körpers in einer Richtung beim Übergang von einem Element zum anderen stetig sein, um die Kontinuität des Materials zu gewährleisten. Im Fall der Balken- oder Plattenbiegung sind die Stetigkeitsanforderungen höher, da dort aus analogen physikalischen Gründen sogar die Stetigkeit der ersten Ableitung bzw. der beiden ersten partiellen Ableitungen gefordert werden muss. Elemente mit Ansatzfunktionen, welche den Stetigkeitsbedingungen genügen, heißen konform.\n\nUm nun die Stetigkeitsanforderungen tatsächlich zu erfüllen, muss der Funktionsverlauf im Element durch Funktionswerte und auch durch Werte von (partiellen) Ableitungen (den Knotenpunktverschiebungen) in bestimmten Punkten des Elementes, den Knotenpunkten, ausgedrückt werden. Die in den Knotenpunkten benutzten Funktionswerte und Werte von Ableitungen nennt man die Knotenvariablen des Elements. Mit Hilfe dieser Knotenvariablen stellt sich die Ansatzfunktion als Linearkombination von sogenannten Formfunktionen mit den Knotenvariablen als Koeffizienten dar.\n\nEs ist zweckmäßig, für die Knotenpunktkoordinaten neben einem elementbezogenen lokalen ein globales Koordinatensystem zu verwenden. Beide werden durch Transformationsfunktionen miteinander verknüpft. Werden für diese Transformation dieselben Formfunktionen wie für den Verformungsansatz benutzt, so sind es \"isoparametrische Elemente\", bei Funktionen niedrigeren bzw. höheren Grades sub- bzw. superparametrische Elemente.\n\nNachdem ein gegebenes Problem diskretisiert ist und die Elementmatrizen aufgestellt sind, führt man vorgegebene Randbedingungen ein. Ein typisches FE-Problem kann zwei Arten von Randbedingungen haben: Dirichlet-Randbedingungen und Neumann-Randbedingungen. Sie gelten (wirken) immer an den Knotenpunkten.\n\nEine Dirichlet-Randbedingung gibt einen Funktionswert direkt vor und eine Neumann-Randbedingung gibt eine Ableitung eines Funktionswertes vor. Ist eine Dirichlet-Randbedingung vorgegeben, bedeutet dies, dass das Problem einen Freiheitsgrad weniger bekommt und die zugehörige Zeile und Spalte in der Gesamtmatrix gestrichen wird. Ist die Dirichlet-Randbedingung ungleich Null, so wird der Wert entsprechend seinem Vorfaktor der Linearform („rechten Seite“) hinzugefügt. Je nach Art des physikalischen Problems kann es sich um verschiedene physikalische Größen handeln, wie in der Tabelle beispielhaft dargestellt. Die Neumann-Randbedingungen haben des Weiteren einen Anteil an der Linearform („rechte Seite“).\n\nEine weitere Variante sind periodische Randbedingungen, bei denen die Werte an einem Rand als Daten für einen anderen Rand genommen werden und so ein periodisch unendlich fortgesetztes Gebiet simuliert wird. Für rotationssymmetrische Probleme werden sogenannte \"zyklische\" Randbedingungen definiert.\n\nDie Verschiebungsmethode ist die \"Standardformulierung\" der Finite-Elemente-Methode bei der die Verschiebungen die primären Unbekannten sind, die die Translation, Rotation und Verformung eines Festkörpers beschreiben. Die Verschiebungsmethode ist in allen gängigen Finite-Elemente-Programmen verfügbar, mit denen Probleme der Festkörpermechanik berechnet werden können. Für die Lösung von Festkörper-Problemen liegen mehrere Grundgleichungen vor.\n\nEine der Verschiebungsmethode zugrunde liegende Gleichung, mit der allgemeine Probleme der Festkörpermechanik behandelt werden können, ist das Prinzip von d’Alembert, wie es die Kontinuumsmechanik in der Lagrangeschen Beschreibung formuliert. Mit diesem Prinzip können sowohl lineare Probleme, wie die Frage nach Eigenschwingungen, als auch hoch nichtlineare Probleme, wie Crashtests, analysiert werden. Hier wird die Methode der gewichteten Residuen nach Galerkin, auch Galerkin-Methode oder Galerkin-Ansatz genannt, verwendet.\n\nIn \"konservativen\" Systemen können bei einem statischen Problem die Knotenpunktverschiebungen aus der Bedingung ermittelt werden, dass im gesuchten Gleichgewichtszustand die potenzielle Energie ein Minimum hat. Mit dem Prinzip vom Minimum der potenziellen Energie können die Steifigkeitsgleichungen finiter Elemente direkt bestimmt werden. Die potenzielle Energie einer Konstruktion ist die Summe aus der inneren Verzerrungsenergie (der elastischen Formänderungsenergie) und dem Potenzial der aufgebrachten Lasten (der von äußeren Kräften geleisteten Arbeit).\n\nDas Bogenlängenverfahren ist eine Methode, bei der man kraftgesteuert bis über das Maximum der Traglast hinaus rechnen kann. Die Notwendigkeit von kraftgesteuerten Methoden liegt darin, dass man im Gegensatz zu verschiebungsgesteuerten Methoden mehrere Lasten direkt proportional steigern kann. Beim Bogenlängenverfahren wird die Last wie vorgegeben gesteigert; würde diese Belastungssteigerung zu einer zu großen Deformation führen, so wird die Last mit einem Faktor kleiner als 1 multipliziert, nach Erreichen der Traglast sogar mit negativen Werten.\n\nBei der Variante der stochastischen Finiten-Elemente-Methode (SFEM) werden Eingangsgrößen des Modells, welche mit einer Unsicherheit behaftet sind, zum Beispiel Materialfestigkeiten oder Belastungen, durch stochastische Größen modelliert. Dies kann mithilfe gewöhnlicher Zufallsvariablen erreicht werden. Oft werden auch Zufallsfelder verwendet, wobei es sich um zufällig variierende, stetige mathematische Funktionen handelt. Eine geläufige Berechnungsmethode ist dabei die Monte-Carlo-Simulation. Dabei wird die FE-Berechnung für viele zufällige Realisierungen (\"samples\") der Eingangsgrößen wiederholt, bis man einen gewissen, im Vorfeld definierten, stochastischen Fehler unterschreitet. Anschließend werden aus allen Ergebnissen die Momente, also Mittelwert und Varianz, berechnet. Je nach Streuung der Eingangsgrößen sind oftmals sehr viele Wiederholungen der FE-Berechnung nötig, was viel Rechenzeit in Anspruch nehmen kann.\n\nStrukturmechanische FEM-Systeme werden durch lineare Gleichungssysteme\n2. Ordnung dargestellt:\n\nformula_2 und formula_3 sind Massen-, Dämpfungs- und Steifigkeitsmatrix des Systems; formula_4 ist der Vektor der externen Kräfte, die auf das Modell wirken. formula_5 ist der Vektor der Freiheitsgrade. \n\nOft bestehen komplexe Bauteilmodelle aus mehreren Millionen Knoten, \nund jeder Knoten kann bis zu 6 Freiheitsgrade besitzen. Somit müssen FEM-Solver (Gleichungssystemlöser) gewisse Anforderungen in Bezug auf effektives Speichermanagement und ggf. Nutzung mehrerer CPUs erfüllen. Es gibt zwei grundsätzlich verschiedene Arten von FEM-Solvern: implizite und explizite.\n\nImplizite FEM-Solver gehen von bestimmten Annahmen aus, unter denen der berechnete Lösungsvektor formula_5 gültig ist. Wirkt z. B. eine zeitlich unveränderliche Last formula_7 auf ein System mit Dämpfung, \ndann wird sich nach ausreichend langer Zeit auch ein konstanter Verschiebungsvektor formula_8 einstellen.\nFür formula_9 ist dann formula_10 , und das Gleichungssystem vereinfacht sich zu formula_11 mit der Lösung formula_12\n\nFür einen gegebenen Lastvektor formula_13 kann der Verschiebungsvektor formula_5 mit Hilfe des Gauß-Algorithmus oder durch QR-Zerlegung von formula_3 berechnet werden.\n\nIst ein mechanisches System einer harmonischen Anregung formula_16 ausgesetzt, \ndann kann es erforderlich sein, die Eigenfrequenzen des Systems zu ermitteln, um Resonanzen im Betrieb zu vermeiden.\n\nEigenfrequenzen sind alle Frequenzen formula_17, für die ein Verschiebungsvektor\nformula_18 eine Lösung des unbelasteten (formula_19) und ungedämpften\n(formula_20) Gleichungssystems darstellt. Für den Geschwindigkeits- und Beschleunigungsvektor gilt dann\n\nund das Gleichungssystem lautet damit\n\nUm die Eigenfrequenzen formula_17 und die dazugehörigen Eigenformen formula_24 zu berechnen, muss der implizite Solver also das Eigenwertproblem\n\nlösen.\n\nExplizite FEM-Solver\n\nExplizite Solver berechnen die Verschiebungsvektoren formula_26 zu bestimmten diskreten Zeitpunkten formula_27\ninnerhalb eines vorgegebenen Zeitintervalls. Knotengeschwindigkeiten und -beschleunigungen werden durch Differenzenquotienten \naus den Verschiebungen formula_28 zu aufeinanderfolgenden Zeitpunkten\nformula_29 angenähert. Mit konstanter Zeitschrittweite formula_30 gilt\n\nhat das diskretisierte Gleichungssystem die Form\n\nDurch Auflösen dieser Gleichung erhält man eine Beziehung, mit der der Verschiebungsvektor formula_34\naus den vorher berechneten Vektoren formula_26 und formula_36 ermittelt werden kann:\n\nDie Berechnung der Inversen formula_38 wird in der Praxis nicht durchgeführt, da explizite Solver in der Regel\nformula_39 als Diagonalmatrix annehmen und daher jede Zeile des Gleichungssystems nur durch den Diagonaleintrag in der entsprechenden\nZeile von formula_39 geteilt werden muss.\n\nExplizite Solver werden u. A. im Fahrzeugbau für die Berechnung von Crash-Lastfällen verwendet.\n\nDer Vorteil \"direkter\" Gleichungslöser nach dem Gauß-Verfahren liegt für die praktische Anwendung in der numerischen Stabilität und dem Erhalt eines exakten Ergebnisses. Nachteilig sind die schlechte Konditionierung der üblicherweise dünn besetzten Steifigkeitsmatrizen und der hohe Speicherbedarf, wie oben erwähnt. Iterative Gleichungslöser sind unempfindlicher bei schlechter Konditionierung und benötigen weniger Speicher, wenn die Nicht-Nullelemente-Speicherung verwendet wird. Allerdings verwenden iterative Solver ein Abbruchkriterium für die Berechnung der Ergebnisse. Wenn dieses erreicht wird, bevor eine annähernd exakte Lösung gefunden wurde, kann das Ergebnis, beispielsweise ein Spannungsverlauf, leicht fehlinterpretiert werden.\nIn manchen Implementierungen werden auch sogenannte \"sparse matrices\" (dünn besetzte Matrizen) verwendet. Es werden dann lediglich die Positionen und Werte der Einträge, die von Null abweichen gespeichert. Damit kann man das Gleichungssystem weiterhin direkt lösen, spart aber erheblich Speicherplatz bei der Verarbeitung der Steifigkeitsmatrix.\n\nFinite-Elemente-Software und ihre Anwendung ist mittlerweile eine Industrie mit mehreren Milliarden US-Dollar Jahresumsatz.\n\nDas untersuchte Lösungsgebiet formula_41 wird zunächst in Teilgebiete formula_42, die finiten Elemente, eingeteilt:\n\nInnerhalb formula_41 werden für die gesuchte Lösungsfunktion formula_45 nun verschiedene Ansatzfunktionen formula_46 definiert, die jeweils nur auf wenigen Elementen ungleich Null sind. Diese Eigenschaft ist der eigentliche Grund für die Bezeichnung „finite“ Elemente.\n\nDurch eine Linearkombination der Ansatzfunktionen werden mögliche Lösungen der numerischen Näherung festgelegt\n\nDa jede Testfunktion auf den meisten Elementen verschwindet, lässt sich umgekehrt die auf das Element formula_48 eingeschränkte Funktion formula_49, durch die Linearkombination weniger Testfunktionen formula_50 beschreiben.\n\nLassen sich die Differentialgleichungen und die Randbedingungen des Problems als lineare Operatoren bezüglich der Funktionen formula_51 darstellen, führt dies zu einem linearen Gleichungssystem bzgl. der freien Variablen der Linearkombination formula_52:\n\nmit\n\nUm ein endliches lineares Gleichungssystem zu erhalten, wird auch der Wertebereich von formula_54 über Ansatzfunktionen formula_60 modelliert. Dann lässt sich formula_58 über Linearkombinationen der formula_62 beschreiben:\n\nund man erhält insgesamt das Gleichungssystem\n\nmit\n\nDie Dimension der Matrix ergibt sich aus der Anzahl der Ansatzfunktionen multipliziert mit den dem physikalischen Modell zugrunde liegenden Freiheitsgraden formula_71. Die Dimension der Matrix ist die Anzahl der Gesamtfreiheitsgrade, wobei dem Modell entsprechende Festlegungen bezüglich der Eindeutigkeit des Problems (z. B. im Fall eines elastischen Körpers die Starrkörperverschiebungen) ausgeschlossen werden müssen.\n\nWeil jedes Element nur mit wenigen benachbarten Elementen verbunden ist, sind die meisten Werte der Gesamtmatrix Null, so dass sie „dünnbesetzt“ ist. In den meisten Anwendungsfällen werden die gleichen Funktionen als Ansatzfunktionen formula_50 und Testfunktionen formula_62 verwendet. In diesem Fall ist die Matrix außerdem symmetrisch zu ihrer Hauptdiagonale.\n\nIst die Anzahl der Freiheitsgrade nicht allzu groß (bis ca. 500.000), lässt sich dieses Gleichungssystem am effizientesten mittels eines direkten Verfahrens lösen, zum Beispiel mit dem gaußschen Eliminationsverfahren. Hierbei kann die dünnbesetzte Struktur des Gleichungssystems effektiv genutzt werden. Während beim Gauß-Algorithmus der Berechnungsaufwand für formula_74 Gleichungen formula_75 beträgt, lässt sich der Aufwand durch geschickte Pivotwahl (zum Beispiel Markowitz-Algorithmus oder graphentheoretische Ansätze) aber deutlich reduzieren.\n\nFür mehr als 500.000 Unbekannte bereitet die schlechte Kondition des Gleichungssystems den direkten Lösern zunehmend Schwierigkeiten, so dass man für große Probleme im Allgemeinen iterative Löser, die schrittweise eine Lösung verbessern, verwendet. Einfache Beispiele dafür sind das Jacobi- und Gauß-Seidel-Verfahren, praktisch werden aber eher Mehrgitterverfahren oder vorkonditionierte Krylow-Unterraum-Verfahren, wie das Verfahren der konjugierten Gradienten oder GMRES, verwendet. Aufgrund der Größe der Gleichungssysteme ist manchmal der Einsatz von Parallelrechnern nötig.\n\nIst die partielle Differentialgleichung nichtlinear, ist auch das resultierende Gleichungssystem nichtlinear. Ein solches lässt sich in der Regel nur über numerische Näherungsverfahren lösen. Ein Beispiel für ein solches Verfahren ist das Newton-Verfahren, in dem schrittweise ein lineares System gelöst wird.\n\nEs gibt heute eine Vielzahl von kommerziellen Computerprogrammen, die nach der Methode der Finiten Elemente arbeiten.\n\nEine Elliptische partielle Differentialgleichung lässt sich schwach formulieren, d. h. die Problemstellung kann auf eine Art ausgedrückt werden, die von der Lösung weniger Glattheit verlangt. Dies geschieht wie folgt.\n\nGegeben sei ein Hilbert-Raum formula_76, ein Funktional (Funktion aus dessen Dualraum) formula_77, sowie eine auf formula_76 stetige und elliptische Bilinearform formula_79, so heißt formula_80 Lösung des Variationsproblems, wenn\n\nExistenz und Eindeutigkeit der Lösung formula_5 liefert der Darstellungssatz von Fréchet-Riesz (für den Fall, dass die Bilinearform formula_83 symmetrisch ist) bzw. das Lemma von Lax-Milgram (allgemeiner Fall).\n\nWir wissen, dass der Raum formula_84 ein Hilbert-Raum ist. Ausgehend hiervon kann man die Sobolewräume formula_85 über die sogenannte schwache Ableitung definieren.\n\nDas Problem formula_86 kann man als eine Variante einer partiellen Differentialgleichung auf einem Gebiet formula_87 auffassen.\n\nDas Poissonproblem als Beispiel:\n\nwobei hier formula_90 den Laplace-Operator bezeichnet. Eine Multiplikation mit unendlich oft stetig differenzierbaren Funktionen formula_91 ergibt nach einer Integration\n\nEine partielle Integration (Erste Greensche Formel) sowie die Nullrandbedingungen für formula_93 liefern dann\n\nNun ist formula_95 eine elliptische und stetige Bilinearform auf formula_96, sowie die rechte Seite formula_97 eine stetige Linearform auf formula_98\n\nBesitzt der betrachtete Funktionenraum/Hilbert-Raum eine endliche Basis, so kann man ein lineares Gleichungssystem aus der Variationsformulierung gewinnen.\n\nFür Funktionenräume entscheidet die Wahl der Basis über die Effizienz des Verfahrens. Gängig sind hierbei die Verwendung von Splines mit Triangulierungen, sowie in bestimmten Fällen die diskrete Fourier-Transformation (Aufspaltung in Sinus und Cosinus).\n\nAufgrund von Flexibilitätsüberlegungen bezüglich der Geometrie des Gebietes formula_87 wird in der Regel folgender Ansatz gewählt.\n\nMan diskretisiert das Gebiet formula_87, indem man es in Dreiecke zerteilt und man benutzt Splines formula_101, assoziiert mit den Eckpunkten p, um den endlichdimensionalen Funktionenraum auf formula_87 aufzuspannen. Die Splines erfüllen an festgelegten Punkten auf den Dreiecken formula_103 (wobei δ das Kronecker-Delta ist). Damit kann man dann eine diskrete Funktion formula_104 darstellen durch\n\nmit formula_106 den Koeffizienten bezüglich der Basisdarstellung. Aufgrund der endlichen Basis muss man nicht mehr gegen alle formula_107 testen, sondern nur noch gegen alle Basisfunktionen, die Variationsformulierung reduziert sich aufgrund der Linearität auf\n\nAlso haben wir ein lineares Gleichungssystem zum Lösen gewonnen\n\nmit\nund\n\nDieses Resultat erhält man mit jeder endlichen Basis des Hilbert-Raumes.\n\nEin finites Element ist ein Tripel formula_112, wobei:\n\nEs gelte für die Funktionale, dass sie zu Funktionen der Basis assoziiert seien:\n\nSo gilt für jede Funktion\n\nFür Sinus als Basisfunktion im formula_121 ist dann\nund die Funktionale\n\nFür Splines genügt hingegen die Punktauswertung auf den festgelegten Punkten der Dreiecke: formula_124.\n\nDer \"FEM-Raum der stetigen, stückweise lineare Funktionen\" ist definiert als:\nwobei formula_126 ein Gebiet und formula_127 die Triangulierung des Gebietes mit Dreiecken formula_90 ist sowie formula_129 die Einschränkung der stetigen Funktion formula_5 auf das Dreieck formula_131 bezeichnet.\n\nDas Referenzelement formula_132 ist definiert als:\n\nDie formula_134 linearen Elemente sind Funktionen vom Typ: \nZur Definition der Funktion formula_136 reicht es dabei schon aus die Werte an den Eckpunkten formula_137 vorzugeben. Deshalb kann man alle Funktionen formula_136 mittels Basisfunktionen formula_139 darstellen:\nDie Basisfunktionen sind gegeben als lineare Funktionen, die jeweils nur an einem Eckpunkt ungleich Null sind:\nwobei formula_142 die Kronecker-Delta Funktion ist.\n\nZur Verknüpfung des Referenzelementes mit einem beliebigen Dreieck formula_113 (Eckpunkte: formula_144) nutzt man eine lineare Transformation formula_145:\n\nBei vielen Aufgaben bezüglich partieller Differentialgleichungen muss das formula_147-Skalarprodukt von Basisfunktionen formula_62 (definiert auf einem beliebigen Dreieck formula_113) berechnet werden:\n\nMithilfe des Transformationssatzes kann man die Integration auf das Referenzelement verschieben:\n\n\n"}
{"id": "8747", "url": "https://de.wikipedia.org/wiki?curid=8747", "title": "RPM Package Manager", "text": "RPM Package Manager\n\nRPM Package Manager ist ein freies (GPL) Paketverwaltungs-System, ursprünglich entwickelt von dem Unternehmen Red Hat. Es umfasst das Paketformat und alle nötigen Programme, um RPM-Pakete zu erstellen und zu verwalten.\n\nDas RPM-Format ist Teil der Linux Standard Base und bedeutet RPM Package Manager, früher Red Hat Package Manager.\n\nIn der Frühzeit von Linux waren die \"tgz\"-Pakete gang und gäbe; eine automatisierte Verwaltung war mit ihnen kaum möglich. Ein gezieltes Deinstallieren einzelner Pakete war nicht möglich. Auch wurden Abhängigkeiten weder aufgelöst, noch wurde zumindest eine Warnung ausgegeben. Anwender, die Software installieren wollten, mussten entweder genug wissen, um diese Abhängigkeiten selbst aufzulösen, oder installierten einfach alle Pakete, was aber wiederum die Gefahr von Paketkonflikten mit sich brachte.\n\nGewisse Ansätze einer Paketverwaltung gab es, als in Anlehnung an die großen Paketverwaltungen der etablierten Unix-Systeme dieser Zeit – wie SunOS (ein Vorläufer von Solaris), HP-UX, OSF/1, IRIX oder Apollo Domain/OS – erste Werkzeuge entwickelt wurden, die jedoch nur wenige Funktionen mitbrachten.\n\nDies verursachte den Linux-Distributoren dieser Zeit erhebliche Probleme beim Support und bei der Pflege ihrer Software, woraufhin zwei konkurrierende Systeme entwickelt wurden: das Debian-Paketmanagement dpkg für Debian-Pakete, initiiert vom Debian Projekt, und \"RPM\" von Red Hat.\n\nDas Ziel war es, Softwarepakete genauso für den Entwickler wie auch für den Anwender einfacher handhabbar zu machen, Abhängigkeiten sollten berücksichtigt und möglichst automatisch aufgelöst werden. Redundanzen wie doppelte Dateien oder Verzeichnisse sollte das System vermeiden, und es sollte möglich sein, Software sauber zu deinstallieren und dabei Abhängigkeiten zu beachten. Auch sollte es möglich werden, Software einfach zu aktualisieren und Konfigurationsdateien sicher zu verwalten.\n\nEine Funktion zur Rückabwicklung bei Fehlern (Transaktionen und Rollback) ist inzwischen ebenso Bestandteil von RPM wie bei den großen Paketverwaltungen von HP-UX und Solaris, wenn auch noch nicht in gleichem Umfang.\n\nDie erste 1995 für Red Hat Linux 2.0 entwickelte Version von RPM basierte auf RPP, PMS (in den 1990er Jahren von Rik Faith für Linux-Betriebssysteme entwickelt) und PM; sie wurde in Perl geschrieben. Später wurde RPM in C neu geschrieben.\n\nUrsprünglich bedeutete RPM \"R\"ed Hat \"P\"ackage \"M\"anagement. Ende der Neunziger wurde der Name jedoch in \"R\"PM \"P\"ackage \"M\"anagement geändert. Der Hintergrund waren rechtliche Aspekte. RPM wurde inzwischen von vielen Distributoren verwendet, und man strebte die Aufnahme von RPM in die Linux Standard Base an. Der Unternehmensname (Firma) im Namen der Software hätte dies zumindest erschwert.\n\nRPM wurde von zahlreichen Linux-Distributionen, wie z. B. SUSE, Mageia oder Mandriva Linux übernommen. Auch einige Unix-Systeme wie z. B. IBM AIX oder Solaris sowie nicht-unixoide Systeme wie Novell Netware nutzten RPM.\n\nZwischen 1999 und 2006 kam die Weiterentwicklung der Software praktisch zum Erliegen. Jeff Johnson, der Hauptentwickler von RPM bei Red Hat bis 1999, entwickelte jedoch die Software teilweise weiter und kümmerte sich um Fehlerkorrekturen. Aus diesem Zweig entstand RPM5 als Weiterentwicklung von RPM der Version 4.\n\nDie verschiedenen Distributoren entwickelten in der Zwischenzeit (1999–2006) eigene Patches für RPM, um Fehler zu beseitigen; Johnsons Patches wurden meist nicht verwendet.\n\nIm Dezember 2006 schlossen sich verschiedene Distributoren unter Red Hat und Novell zum RPM-Projekt zusammen und entwickelten RPM mit den jeweiligen distributionseigenen Patches – weiterhin ohne Johnsons Änderungen – gemeinsam weiter.\n\nDaraus resultieren nun zwei Versionen von RPM: RPM mit Patches der Distributoren und RPM5, welches sich als RPM aus der „originalen Codebasis“ bezeichnet.\n\nDa die meisten Linux-Distributionen freie Software sind, besteht der Vorteil einer bestimmten Distribution gegenüber anderen zu einem großen Teil aus den Annehmlichkeiten des jeweiligen Paket-Managers (und aus den zusammengestellten, getesteten und regelmäßig aktualisierten (RPM-)Paketsammlungen des jeweiligen Distributors im Internet).\nPakete zwischen Distributoren auszutauschen, war früher nahezu unmöglich. Dies wurde mit RPM einfacher, ist aber immer noch mit Problemen behaftet, da die verschiedenen Distributionen, die RPM benutzen, oft verschiedene Versionen und/oder Konfigurationen von Systemsoftware wie etwa Grafik-Bibliotheken verwenden.\n\nFür den Entwickler vereinfacht ein Paketverwaltungssystem wie RPM das Einbauen von Software in die durch die Distribution vorgegebene Struktur (Pfade, Abhängigkeiten, Dateinamen, Paketnamen). Das Paketformat selbst überprüft allerdings den jeweiligen Inhalt eines Pakets nicht automatisch. Der Entwickler bestimmt den Aufbau des Paketes selbständig und muss sich damit für jede Distribution in die jeweiligen Besonderheiten einarbeiten.\n\nMit Einführung der Linux Standard Base wurden viele Unstimmigkeiten zwischen den Distributoren ausgeräumt. LSB erleichtert die Arbeit der Paketbauer erheblich, weil zumindest Dateinamen und Pfade vereinheitlicht sind. Entsprechende Sorgfalt des Paketmaintainers vorausgesetzt, lässt sich relativ sicher ein RPM von einem anderen LSB-Distributor in ein beliebiges anderes LSB-System einspielen.\n\nEine Paketverwaltung ist systemabhängig. So besitzen außer verschiedenen Distributoren nicht mal alle Linuxanbieter das gleiche Paketformat. Andere Systeme bringen natürlich ihre eigene Software mit.\n\nDa aber auf vielen anderen Unix-Systemen mittlerweile die GNU-Software zur Grundausstattung gehört (zumindest installiert der Systemverwalter sie in der Regel ganz schnell nach) wurde auch RPM – vor allem im Rahmen des Projektes OpenPKG – auf andere Systeme portiert, wodurch man einfach zusätzliche Software einspielen kann.\n\nIn gewissem Sinn ist dies gleichermaßen ein Fortschritt und ein Rückschritt. Bisher haben Systemverwalter aus den Quellen die Software selber kompiliert und dann in das System eingepflegt. Dieser Schritt und die damit angefallene Arbeit entfallen natürlich.\n\nAllerdings gibt es zwischen der Paketdatenbank von RPM und der systemeigenen Paketdatenbank keine Kommunikation, und somit werden doppelte Installationen genauso wenig aufgelöst wie fehlende Systemabhängigkeiten über die Paketsysteme hinweg.\n\nFür OS/2 bzw. ecomstation wurde das System auch portiert, so dass es sich inzwischen nicht mehr nur auf unixartige Betriebssysteme beschränkt.\n\nDie RPM-Dateien sind in der Regel mit gzip oder LZMA (xz) komprimierte cpio-Archive, die einzelnen Teile können jedoch einfach nach bestimmten Informationen durchsucht werden, denn ein Kopfdatenbereich im Binärformat ist an jedes Paket angefügt. Diese Kopfdaten sind nicht komprimiert und enthalten alle wichtigen Informationen. Das erleichtert das schnelle Durchsuchen von RPM-Paketen.\n\nEntpacken ohne Installation ist möglich: codice_1\n\nIn den RPM-Spezifikationen finden sich ebenfalls die Spezifikationen für Verzeichnisse mehrerer RPM-Pakete. Die Pakete werden dabei in einem Verzeichnis gespeichert, das neben den Paketen noch Informationen über die Pakete selbst in Form von Metadaten-Dateien enthält. Wenn ein geeignetes Programm dann Informationen über die im Verzeichnis enthaltene Software benötigt, reicht es, die Metadaten herunterzuladen. In diesen Daten sind die vorhandenen Pakete, deren Versionen, Abhängigkeiten, Architektur etc. enthalten.\n\nProgramme, die XML-Package-Metadaten verwalten können, sind yum, Yast, Red Carpet, smartpm und apt-rpm.\n\n\nLinux-Distributionen:\n\nUNIX-Systeme:\n\nAndere:\n\n\n"}
{"id": "8940", "url": "https://de.wikipedia.org/wiki?curid=8940", "title": "Emacs", "text": "Emacs\n\nEmacs [] ist eine Familie von Texteditoren. Die erste Emacs-Implementierung wurde von Richard Stallman (zusammen mit Guy L. Steele, Jr. und anderen) entwickelt. Besonders populär ist heute der GNU Emacs, der durch seine Programmierschnittstelle in der Programmiersprache Emacs Lisp mit beliebigen Erweiterungen ausgestattet werden kann. Es gibt aber auch noch eine Vielzahl von anderen Editoren, die zur Emacs-Familie zählen.\n\nGNU Emacs ist als freie Software unter der GNU General Public License erhältlich und läuft auf den meisten heute üblichen Betriebssystemen (Unix, GNU/Linux, macOS und Windows).\n\nEmacs bietet eine ganze Reihe Betriebsarten (engl. \"modes\"), die bei der Erstellung von Quelltext für diverse Programmier- bzw. Auszeichnungssprachen hilfreich sind. So kann man Emacs z. B. als HTML-Editor verwenden, der auch Syntaxüberprüfungen vornimmt.\n\nSyntaxhervorhebung wird in den meisten dieser Betriebsarten unterstützt. Dabei wird der Text aufgrund der Syntax des bearbeiteten Textes (LaTeX, HTML, Perl, Java und andere) eingefärbt, was dem Benutzer die Orientierung erleichtert. Die Modi bieten in der Regel wesentlich mehr als eine Syntaxhervorhebung: Übersetzungsvorgänge, Syntaxprüfer, Debugger und dergleichen mehr lassen sich von Emacs aus aufrufen.\n\nIn der Grundkonfiguration verfügt Emacs bereits über einen Kalender, mehrere News- und Mailreader mit POP- und IMAP-Unterstützung, eine eingebaute Shell, Spiele, einen FTP-Client und einen Webbrowser.\nEs gibt zusätzlich zahlreiche Tools, die in Emacs eingebunden werden können, darunter IRC-Clients, IM-Clients, Adressbücher, Audioplayer und sogar Webserver.\n\nEmacs ermöglicht per Wiki Mode das Bearbeiten von Webseiten als Wikitext.\n\nZum Spaß und zur Demonstration, was mit Emacs Lisp alles möglich ist, enthält Emacs mit ELIZA ein Programm zur Unterhaltung mit einem vom Computer generierten „Psychologischen Psychotherapeuten“ (Aufruf mit „M-x doctor“; „M-x“ ist z. B. „ESC x“ oder „Alt-x“). Das Programm wandelt Aussagen des Benutzers in Fragen um, ermuntert ihn, mehr zu erzählen, und suggeriert Lebensprobleme allgemeinster Art. Ein weiterer nostalgischer Zusatz ist ein Textadventure („M-x dunnet“).\n\nMan kann Emacs auch als eine Umgebung zur Programmierung von Spezialeditoren betrachten; so gibt es einen po-mode, mit dem man Übersetzungen erstellen kann.\n\nEmacs entstand 1976 am MIT zunächst als Sammlung von Makros für den Editor TECO.\nDer Name ist die Abkürzung von „Editor MACroS“.\n\n1978/79 portierte Bernard Greenberg den Editor auf das Großrechner-Betriebssystem Multics. Er benutzte dazu die Programmiersprache Maclisp.\n\n1981 schrieb James Gosling den ersten Emacs für Unix-Systeme in C. Die Erweiterungssprache Mocklisp ähnelt Lisp, kennt aber keine strukturierten Datentypen. Gosling schränkte die Verbreitung zunächst nicht ein, aber verkaufte den Code später an UniPress, die diese Version als UniPress Emacs vertrieben. Gosling Emacs zeichnet sich durch einen hocheffizienten Code zur Textausgabe aus; Stallman verwendete Teile des Gosling-Codes in GNU Emacs, was später zu einer Kontroverse mit UniPress führte.\n\n1984 begann Richard Stallman, an einer neuen Implementierung von Emacs, GNU Emacs, zu arbeiten, die das erste Programm des damals entstehenden GNU-Projekts wurde. Die Lizenz des Programms war zu Beginn der Entwicklung die \"GNU Emacs General Public License\". Es war die erste Copyleft-Lizenz und die Grundlage für die später entwickelte \"GNU General Public License\" (GPL). GNU Emacs ist zum größten Teil in Emacs Lisp, einem eigenen Dialekt der Lisp-Programmiersprache, programmiert. Diese Lisp-Version von Emacs fußt nicht auf Greenbergs in Maclisp geschriebenem Multics-Emacs, der ersten Lisp-Version, und benutzt auch ganz andere Datenstrukturen. Den Kern bildet ein in C geschriebener Interpreter für Emacs Lisp. Gerd Möllmann hat Version 21 (21.1 und 21.2) als Hauptprogrammierer betreut und veröffentlicht. Version 23 wurde im Jahr 2009 fertiggestellt.\n\nWie Clifford Stoll aufdeckte, ermöglichte ein Programmfehler in Emacs 1986 dem für den KGB spionierenden Hacker Markus Hess den Einbruch in das Lawrence Berkeley National Laboratory.\n\nVon Emacs wurden einige Derivate entwickelt, das am weitesten verbreitete ist XEmacs, dessen Projektgruppe sich schon seit längerem mit der Einbindung von GUI-Elementen in Emacs beschäftigt. Bekannt ist auch MicroEmacs, der unter anderem mit AmigaOS ausgeliefert wurde. Ein weiterer Klon ist QEmacs.\n\nAquamacs von David Reitter ist eine an die Human Interface Guidelines angepasste Emacs-Variante für Mac OS X, die – wie der ehemalige Carbon Emacs von Seiji Zenitani – viele zusätzliche Pakete bereits vorinstalliert enthält. Aquamacs kann aber auch über die klassische Emacs-Bedienoberfläche verwendet werden.\n\nEine besonders kleine, aber dennoch recht leistungsstarke Version ist Zile. Der Name ist ein rekursives Akronym und bedeutet \"Zile is lossy Emacs\".\n\n\nRichard Stallman hat scherzhafterweise den Editor Emacs zu einer Spaßreligion erhoben, der „Church of Emacs“, und bezeichnet sich selbst als St. IGNUcius. Als Glaubensbekenntnis muss man dreimal „There is no system but GNU, and Linux is one of its kernels.“ sagen. Dazu gibt es noch die Newsgroup \"alt.religion.emacs\", die sich dieser Parodie widmet. Als Reaktion darauf gründeten die Anhänger des Konkurrenz-Editors vi den Cult of Vi.\n\nBenutzer haben weitere, scherzhafte Deutungen aus Eigenarten von Emacs abgeleitet:\nEight Megabytes And Constantly Swapping (\"Acht Megabyte groß und swappt dauernd\") nimmt den für damalige Zeiten großen Arbeitsspeicher-Bedarf aufs Korn, ebenso Emacs Makes Any Computer Slow (\"Emacs macht jeden Computer langsam\").\nEscape-Meta-Alt-Control-Shift ist eine Anspielung auf die Tastenkombinationen, mit denen die meisten Funktionen von Emacs auszulösen sind.\n\nIn Anspielung auf den großen Funktionsumfang schrieb Thomer M. Gil: (deutsch: Emacs ist ein großartiges Betriebssystem – allerdings fehlt ein guter Editor).\n\n\n"}
{"id": "9035", "url": "https://de.wikipedia.org/wiki?curid=9035", "title": "Quantencomputer", "text": "Quantencomputer\n\nEin Quantenprozessor bzw. Quantencomputer ist ein Prozessor, dessen Funktion auf den Gesetzen der Quantenmechanik beruht.\n\nIm Unterschied zum Digitalrechner arbeitet er nicht auf der Basis der Gesetze der klassischen Physik bzw. Informatik, sondern auf der Basis quantenmechanischer Zustände. Die Verarbeitung dieser Zustände erfolgt nach quantenmechanischen Prinzipien. Hierbei sind\nvon Bedeutung.\n\nTheoretische Studien zeigen, dass unter Ausnutzung dieser Effekte bestimmte Probleme der Informatik, z. B. die Suche in extrem großen Datenbanken (siehe Grover-Algorithmus) und die Faktorisierung großer Zahlen (siehe Shor-Algorithmus) effizienter gelöst werden können als mit klassischen Computern. Dies würde viele mathematische Probleme leichter lösbar machen.\n\nDer Quantencomputer war lange ein überwiegend theoretisches Konzept. Es gibt verschiedene Vorschläge, wie ein Quantencomputer realisiert werden könnte, und in kleinem Maßstab wurden einige dieser Konzepte im Labor erprobt und Quantencomputer mit wenigen Qubits realisiert. Neben der Anzahl der Qubits ist aber auch zum Beispiel eine geringe Fehlerquote beim Rechnen und Auslesen wichtig und wie lange die Zustände in den Qubits aufrechterhalten werden können. Gegenwärtig (2018) investieren viele große Computerfirmen in die Entwicklung von Quantencomputern und der Rekord liegt bei rund 50 bis 70 Qubits.\n\nIn einem klassischen Computer wird sämtliche Information in Bits dargestellt. Physikalisch wird ein Bit dadurch realisiert, dass ein Spannungspotential entweder oberhalb eines bestimmten Pegels liegt oder unterhalb.\n\nAuch in einem Quantencomputer wird Information in der Regel binär dargestellt. Dazu bedient man sich eines physikalischen Systems mit zwei nicht notwendigerweise orthogonal gewählten Basiszuständen eines zweidimensionalen komplexen Raums, wie er in der Quantenmechanik auftritt. Ein Basiszustand repräsentiert den quantenmechanischen Zustandsvektor formula_1, der andere den Zustandsvektor formula_2. Dabei wird die Dirac-Notation genutzt. Bei diesen quantenmechanischen Zwei-Niveau-Systemen kann es sich z. B. um den Spin eines Elektrons handeln, der entweder nach oben oder z. B. nach vorn zeigt. Andere Implementierungen nutzen das Energieniveau in Atomen oder Molekülen oder die Flussrichtung eines Stroms in einem ringförmigen Supraleiter. Ein solches quantenmechanisches Zweizustandssystem wird Qubit (\"Quanten-Bit\") genannt.\n\nEine Eigenschaft quantenmechanischer Zustandsvektoren ist, dass diese eine Überlagerung anderer Zustände sein können. Dies wird auch Superposition genannt. Im konkreten Fall bedeutet dies, dass ein Qubit nicht \"entweder\" formula_1 \"oder\" formula_4 sein muss, wie dies für die Bits des klassischen Computers der Fall ist. Vielmehr ergibt sich der Zustand eines Qubits in dem oben erwähnten zweidimensionalen komplexen Raum allgemein zu\n\nwobei wie in der kohärenten Optik beliebige Überlagerungszustände zugelassen sind. Der Unterschied zwischen klassischem und quantenmechanischem Computing ist also analog dem zwischen inkohärenter bzw. kohärenter Optik (im ersten Fall werden Intensitäten addiert, im zweiten direkt die Feldamplituden, wie etwa in der Holographie).\n\nHierbei sind formula_6 und formula_7 komplexe Zahlen.\nZur Normierung fordert man formula_8.\nOhne Beschränkung der Allgemeinheit kann formula_6 reell und nichtnegativ gewählt werden.\nDie Wahrscheinlichkeit dafür, als Resultat einer Messung am Zustand formula_10 den Wert 0 zu erhalten, beträgt\nformula_11.\nDie Wahrscheinlichkeit dafür, als Resultat einer im Allgemeinen anderen Messung am Zustand formula_10 den Wert 1 zu erhalten, beträgt\nformula_13. Die Summe formula_14 ist nur für orthogonale Basiszustände 1.\nDavon unabhängig darf dieses probabilistische Verhalten nicht so interpretiert werden, dass sich das Qubit mit einer bestimmten Wahrscheinlichkeit im Zustand formula_15 und mit einer anderen Wahrscheinlichkeit im Zustand formula_16 befindet, während andere Zustände nicht zugelassen sind. Ein solches ausschließendes Verhalten könnte man auch mit einem klassischen Computer erzielen, der einen Zufallsgenerator verwendet, um beim Auftreten von überlagerten Zuständen zu entscheiden, ob er mit 0 oder 1 weiterrechnet. Ein solches ausschließendes Verhalten kommt in der statistischen Physik vor, die im Gegensatz zur Quantenmechanik inkohärent ist.\n\nBei Berücksichtigung der kohärenten Überlagerung erhält man allgemein\n\nwobei formula_18 den Realteil der komplexen Zahl formula_19 bedeutet, formula_20 die konjugiert-komplexe Zahl zu formula_6 und formula_22 das quantenmechanische Skalarprodukt der betreffenden Zustände ist.\n\nWie beim klassischen Computer fasst man mehrere Qubits zu Quantenregistern zusammen. Der Zustand eines Qubit-Registers ist dann gemäß den Gesetzen der Vielteilchen-Quantenmechanik ein Zustand aus einem formula_23-dimensionalen Hilbert-Raum. Eine mögliche Basis dieses Vektorraums ist die Produktbasis über der Basis formula_24. Für ein Register aus zwei Qubits erhielte man demnach die Basis formula_25. Auch der Zustand des Registers kann eine beliebige Superposition dieser Basiszustände sein, also bei formula_26 Qubits von der Form\nmit beliebigen komplexen Zahlen formula_28 und der üblichen Dualbasis. Auch Summen bzw. Differenzen solcher Terme sind erlaubt, während bei klassischen Computern nur die Basiszustände selbst vorkommen, d. h. nur aus den Ziffern 0 bzw. 1 zusammengesetzte Vorfaktoren.\n\nDie Zustände eines Quantenregisters lassen sich nicht immer aus den Zuständen unabhängiger Qubits zusammensetzen: Beispielsweise kann der Zustand\n\"nicht\" in ein Produkt aus einem Zustand für das erste und einem Zustand für das zweite Qubit zerlegt werden.\n\nMan nennt einen derartigen Zustand daher auch verschränkt (in der englischsprachigen Literatur spricht man von \"entanglement\"). Das Gleiche gilt für den von formula_30 verschiedenen Zustand\n\nDiese Verschränkung ist ein Grund, warum Quantencomputer effizienter als klassische Computer sein können, d. h. dass sie prinzipiell bestimmte Probleme schneller als klassische Computer lösen können: Um den Zustand eines klassischen formula_26-Bit-Registers darzustellen, benötigt man formula_26 Bits an Information. Der Zustand des Quanten-Registers ist aber ein Vektor aus einem formula_23-dimensionalen Vektorraum, so dass zu dessen Darstellung formula_23 komplexwertige Koeffizienten benötigt werden. Bei großem formula_26 ist die Zahl formula_23 viel größer als formula_26 selbst.\n\nDas Superpositionsprinzip wird oft so dargestellt, dass ein Quantencomputer in einem Quantenregister aus formula_26 Qubits \"gleichzeitig\" alle formula_23 Zahlen von 0 bis formula_41 speichern könnte. Diese Vorstellung ist irreführend. Da eine am Register vorgenommene Messung stets genau einen der Basiszustände auswählt, lässt sich unter Anwendung des so genannten Holevo-Theorems zeigen, dass der maximale zugängliche Informationsgehalt eines einzelnen unverschränkten Qubits wie im klassischen Fall genau ein Bit beträgt.\nEs ist dennoch korrekt, dass das Superpositionsprinzip eine Parallelität in den Rechnungen erlaubt, die über das hinausgeht, was in einem klassischen Parallelrechner passiert. Bei der Vorstellung einiger Quantenalgorithmen wird darauf näher eingegangen.\n\nBeim klassischen Computer werden durch Logikgatter (engl. \"Gates\") elementare Operationen auf den Bits durchgeführt. Mehrere Gatter werden zu einem Schaltnetz verbunden, das dann komplexe Operationen wie das Addieren zweier Binärzahlen durchführen kann. Die Gatter werden durch physikalische Bauelemente wie Transistoren realisiert und die Information als elektrisches Signal durch diese Bauelemente geleitet.\n\nBerechnungen auf einem Quantencomputer laufen grundsätzlich anders ab: Ein Quantengatter (engl. \"Quantum Gate\") ist kein technischer Baustein, sondern stellt eine elementare physikalische Manipulation eines oder mehrerer Qubits dar. Wie genau so eine Manipulation aussieht, hängt von der tatsächlichen physikalischen Natur des Qubits ab. So lässt sich der Spin eines Elektrons durch eingestrahlte Magnetfelder beeinflussen, der Anregungszustand eines Atoms durch Laserpulse. Obwohl also ein Quantengatter kein elektronischer Baustein, sondern eine im Verlauf der Zeit auf das Quantenregister angewendete Aktion ist, beschreibt man Quantenalgorithmen mit Hilfe von Schaltplänen, vgl. hierzu den Artikel Liste der Quantengatter.\n\nFormal ist ein Quantengatter eine unitäre Operation formula_42, die auf den Zustand des Quanten-Registers wirkt:\n\nEin Quantengatter kann daher als unitäre Matrix geschrieben werden. Ein Gatter, welches den Zustand eines Qubits umdreht (negiert), würde im Falle eines zweidimensionalen Zustandsraums der folgenden Matrix entsprechen:\n\nKomplizierter zu schreiben sind Quantengatter (unitäre Matrizen), die Zwei- oder Mehr-Qubitzustände modifizieren, z. B. das in formula_45 definierte CNOT-Gatter, mit der Zwei-Qubit-Zustandstabelle\nformula_46,\nformula_47,\nformula_48 und\nformula_49.\nDas Ergebnis lässt sich zusätzlich bezüglich Stellenindizes formula_50 und formula_51 symmetrisieren bzw. antisymmetrisieren, etwa nach dem Schema\nwodurch verschränkte Zustände entstehen.\n\nEin Quantenschaltkreis besteht aus mehreren Quantengattern, die in fester zeitlicher Abfolge auf das Quantenregister angewendet werden. Beispiele hierfür sind die Quanten-Fouriertransformation oder der Shor-Algorithmus. Mathematisch ist ein Quantenschaltkreis auch eine unitäre Transformation, deren Matrix das Produkt der Matrizen der einzelnen Quantengatter ist.\n\nEin weiterer Ansatz zur Implementierung eines Quantencomputers ist der sogenannte Einweg-Quantencomputer (\"one-way quantum computer\", Hans J. Briegel, Robert Raußendorf 2001). Dieser unterscheidet sich vom Schaltkreismodell dadurch, dass zuerst ein universeller (also vom Problem unabhängiger) verschränkter Quantenzustand generiert wird (beispielsweise ein sogenannter Clusterzustand), und die eigentliche Rechnung durch gezielte Messungen an diesem Zustand durchgeführt wird. Dabei bestimmen die Ergebnisse früherer Messungen, welche weiteren Messungen durchgeführt werden.\n\nAnders als im Schaltkreismodell wird hier der verschränkte Quantenzustand nur als Ressource benutzt. Bei der eigentlichen Rechnung werden nur einzelne Qubits des verwendeten Zustands gemessen und klassische Rechnungen durchgeführt. Insbesondere werden dabei keine Mehr-Qubit-Operationen durchgeführt (die Herstellung des Zustands benötigt solche natürlich). Dennoch lässt sich zeigen, dass der Einweg-Quantencomputer genauso leistungsfähig ist, wie ein auf dem Schaltkreismodell beruhender Quantencomputer.\n\nEin weiterer Ansatz für Quantencomputer beruht auf einem anderen Konzept: Gemäß den Gesetzen der Quantenmechanik bleibt ein quantenmechanisches System, das sich im Grundzustand (Zustand minimaler Energie) eines zeitunabhängigen Systems befindet, auch bei Veränderungen des Systems im Grundzustand, wenn die Veränderung nur hinreichend langsam (also adiabatisch) passiert. Die Idee des adiabatischen Quantencomputers ist es, ein System zu konstruieren, das einen zu dieser Zeit noch unbekannten Grundzustand hat, der der Lösung eines bestimmten Problems entspricht, und ein anderes, dessen Grundzustand leicht experimentell zu präparieren ist. Anschließend wird das leicht zu präparierende System in das System überführt, an dessen Grundzustand man interessiert ist, und dessen Zustand dann gemessen. Wenn der Übergang langsam genug erfolgt ist, hat man so die Lösung des Problems.\n\nDie Firma D-Wave Systems hat 2007 erklärt, einen kommerziell verwendbaren Quantencomputer entwickelt zu haben, der auf diesem Prinzip beruht.\nAm 26. Mai 2011 verkaufte die Firma D-Wave Systems den ersten kommerziellen Quantencomputer \"D-Wave One\" an die Lockheed Martin Corporation. Ihre Ergebnisse sind allerdings noch umstritten. 2015 stellte D-Wave Systems ihre verbesserte und aufwärts skalierbare Version D-Wave-2X der Öffentlichkeit vor. Der adiabatische Quantencomputer, der speziell für die Lösung von Optimierungsproblemen entwickelt wurde, soll bei einigen Problemen bis zu 15mal schneller sein als herkömmliche klassische Spezialcomputer für die jeweiligen Probleme (beim D-Wave One war das noch nicht so). Nach Angaben von D-Wave benutzt er supraleitende Technologie und über 1.000 Qubits (genannt 1000+ Qubits, ausgelegt auf 1.152 Qubits), bei einer Arbeitstemperatur von 15 mK. Unter Qubit versteht die Firma eine supraleitende Schleife auf ihrem Chip, in der die Information über die Flussrichtung codiert ist. Ein Exemplar wurde an Google und die NASA verkauft, die schon 2013 einen D-Wave-Computer der ersten Generation mit 512 Qubits erwarben. Google benutzt ihn, um die Vorteile von Quantum Annealing Algorithmen auszuloten, das heißt Quantenversionen von Simulated Annealing.\n\nDas bisher beschriebene Konzept ist zunächst abstrakt und allgemein gültig. Will man einen konkret nutzbaren Quantencomputer bauen, muss man die natürlichen physikalischen Einschränkungen beachten, die im Folgenden beschrieben werden.\n\nÜberlässt man ein System sich selbst, neigt es dazu, sich ins thermische Gleichgewicht mit seiner Umgebung zu entwickeln. Im einfachsten Fall geschieht dies über Energieaustausch mit der Umgebung, der mit Zustandsänderung der Qubits einhergeht. Dies führt dazu, dass ein Qubit aus dem Zustand formula_53 nach einer gewissen Zeit mit einer bestimmten Wahrscheinlichkeit in den Zustand formula_15 gesprungen ist und umgekehrt. Diesen Prozess nennt man Relaxation. Als Relaxationszeit formula_55 bezeichnet man die charakteristische Zeit, in welcher sich das System (meist exponentiell) seinem stationären Zustand nähert.\n\nMit Dekohärenz ist der Verlust der Superpositionseigenschaften eines Quantenzustands gemeint. Durch den Einfluss der Umgebung entwickelt sich aus einem beliebigen Superpositionszustand formula_56 (wobei formula_57) entweder der Zustand formula_15 oder der Zustand formula_53 (mit entsprechenden Wahrscheinlichkeiten, die zum Beispiel durch formula_60 gegeben sein können, während gemischte Terme (z. B. formula_61) \"nicht\" auftreten (Zustandsreduktion; inkohärente vs. kohärente Superposition; Thermalisierung, wie in der statistischen Physik)). Dann verhält sich das Qubit nur noch wie ein klassisches Bit. Die Dekohärenzzeit formula_62 ist in der Regel ebenfalls exponential verteilt und typischerweise kleiner als die Relaxationszeit. Während die Relaxation auch für klassische Computer ein Problem darstellt (so könnten sich Magnete auf der Festplatte spontan umpolen), ist die Dekohärenz ein rein quantenmechanisches Phänomen.\n\nDie Verlässlichkeit von Quantencomputern kann durch die sogenannte Quantenfehlerkorrektur erhöht werden.\n\nDa formal festgelegt ist, wie ein Quantencomputer arbeitet, können die aus der theoretischen Informatik bekannten Begriffe wie Berechenbarkeit oder Komplexitätsklasse auch auf einen Quantencomputer übertragen werden. Man stellt dabei fest, dass ein Quantencomputer keine prinzipiell neuen Probleme lösen kann, einige Probleme allerdings schneller gelöst werden können.\n\nEin klassischer Computer kann einen Quantencomputer simulieren, da die Wirkung der Gates auf dem Quantenregister einer Matrix-Vektor-Multiplikation entspricht. Der klassische Computer muss nun einfach all diese Multiplikationen ausführen, um den Anfangs- in den Endzustand des Registers zu überführen. Die Konsequenz dieser Simulierbarkeit ist, dass alle Probleme, die auf einem Quantencomputer gelöst werden können, auch auf einem klassischen Computer gelöst werden können. Umgekehrt bedeutet dies, dass Probleme wie das Halteproblem auch auf Quantencomputern nicht gelöst werden können.\n\nEs lässt sich zeigen, dass die Simulation eines Quantencomputers in der Komplexitätsklasse PSPACE liegt. Man geht daher davon aus, dass es keinen Simulationsalgorithmus gibt, der einen Quantencomputer mit polynomiellem Zeitverlust simuliert.\n\nUmgekehrt kann ein Quantencomputer auch einen klassischen Computer simulieren. Dazu muss man zunächst wissen, dass jeder logische Schaltkreis allein aus NAND-Gattern gebildet werden kann. Mit dem Toffoli-Gatter kann man bei geeigneter Beschaltung der drei Eingänge nun ein Quantengatter erhalten, das sich auf Qubits in der Basis der klassischen Bits formula_24 wie ein NAND-Gatter verhält. Außerdem lässt sich das Toffoli-Gate dazu verwenden, ein Eingangsbit zu verdoppeln. Aufgrund des No-Cloning-Theorems ist dies allerdings nur für die Zustände formula_24 möglich. Diese Verdopplung (auch \"Fan-out\" genannt) ist deshalb nötig, weil es bei einem klassischen Schaltkreis möglich ist, ein Bit auf zwei Leitungen zu verteilen.\n\nIm Rahmen der Komplexitätstheorie ordnet man algorithmische Probleme sogenannten Komplexitätsklassen zu. Die bekanntesten und wichtigsten Vertreter sind die Klassen P und NP. Dabei bezeichnet P diejenigen Probleme, deren Lösung deterministisch in zur Eingabelänge polynomieller Laufzeit berechnet werden kann. In NP liegen die Probleme, zu denen es Lösungsalgorithmen gibt, die nicht-deterministisch polynomiell sind. Der Nicht-Determinismus erlaubt, gleichzeitig verschiedene Möglichkeiten abzutesten. Da unsere aktuellen Rechner deterministisch laufen, muss der Nicht-Determinismus durch Hintereinanderausführung der verschiedenen Möglichkeiten simuliert werden, wodurch die Polynomialität der Lösungsstrategie verloren gehen kann.\n\nFür Quantencomputer definiert man die Komplexitätsklasse BQP (eingeführt 1993 durch Umesh Vazirani und Ethan Bernstein). Diese enthält diejenigen Probleme, deren Laufzeit polynomiell von der Eingabelänge abhängt und deren Fehlerwahrscheinlichkeit unter formula_65 liegt. Aus dem vorhergehenden Abschnitt folgt, dass BQP formula_66 PSPACE. Ferner gilt P formula_66 BQP, da ein Quantencomputer auch klassische Computer mit nur polynomiellem Zeitverlust simulieren kann.\n\nWie BQP zur wichtigen Klasse NP in Beziehung steht, ist noch unklar. Man weiß nicht, ob ein Quantencomputer ein NP-vollständiges Problem effizient lösen kann oder nicht. Könnte man nachweisen, dass BQP eine echte Teilmenge von NP ist, wäre damit auch das P-NP-Problem gelöst: Dann gälte nämlich P formula_68 NP. Andererseits würde aus dem Nachweis, dass NP echte Teilmenge von BQP ist, folgen, dass P echte Teilmenge von PSPACE ist. Sowohl das P-NP-Problem als auch die Frage P formula_68 PSPACE sind wichtige ungelöste Fragen der theoretischen Informatik.\n\nLange offen war, ob es Probleme gibt, die Quantencomputer beweisbar schneller und effizienter als jeder klassische Computer lösen können, anders ausgedrückt, die Teil von BQP sind, aber nicht von PH, einer Verallgemeinerung von NP. 2018 wurde ein Beispiel von Ran Raz und Avishai Tal gefunden, das in BQP (Scott Aaronson 2009), aber nicht in PH ist (genauer bewiesen sie, dass das Problem für beide Fälle Orakel-separiert ist), das \"Forrelation\"-Problem. Gegeben sind zwei Zufallszahlengeneratoren. Das Forrelation-Problem besteht darin, aus den erzeugten Zufallszahlenfolgen herauszufinden, ob die beiden Zufallszahlgeneratoren unabhängig sind oder die Folgen doch in verborgener Weise verbunden sind, genauer ob die eine die Fouriertransformation der anderen ist. Raz und Tal bewiesen, dass Quantencomputer sehr viel weniger Hinweise (Orakel) für die Lösung benötigen als klassische Computer. Ein Quantencomputer benötigt sogar nur ein Orakel, in PH gibt es auch mit unendlich vielen Orakeln keinen Algorithmus, der das Problem löst. Das Beispiel zeigt, dass selbst für P=NP es Probleme gibt, die Quantencomputer lösen können, klassische Rechner aber nicht.\n\nBei anderen Problemen wie dem Faktorisierungsproblem ganzer Zahlen wird zwar vermutet, dass Quantencomputer prinzipiell schneller sind (Quantencomputer lösen es polynomialzeitlich mit dem Shor-Algorithmus), es lässt sich aber bisher nicht beweisen, da unbekannt ist, ob das Problem in der Komplexitätsklasse P liegt.\n\nEin anderes Problem, von dem erwartet worden war, dass es effizient von Quantencomputern gelöst werden kann, nicht aber von klassischen Computern, ist das Empfehlungsproblem (Recommendation Problem), das sogar breite praktische Anwendung hat. Betrachtet wird zum Beispiel das für Online-Dienste wichtige Problem, aus dem Abruf von Diensten oder Waren durch Nutzer Voraussagen über deren Vorlieben zu machen, was sich formalisieren lässt als Auffüllen einer Matrix, die zum Beispiel Waren den Nutzern zuordnet. 2016 gaben Iordanis Kerenidis und Anupam Prakash einen Quantenalgorithmus, der exponentiell schneller war als jeder damals bekannte klassische Algorithmus. 2018 gab die Studentin Ewin Tang allerdings einen klassischen Algorithmus an, der genauso schnell war. Tang fand den klassischen Algorithmus in Anlehnung an den Quantenalgorithmus von Kerenidis und Prakash.\n\nDie bisher gefundenen Algorithmen für Quantencomputer lassen sich grob in drei Kategorien einteilen:\n\nViele Algorithmen für Quantencomputer liefern nur mit einer gewissen Wahrscheinlichkeit ein korrektes Ergebnis; man spricht von probabilistischen Algorithmen. Durch wiederholtes Anwenden des Algorithmus kann die Fehlerwahrscheinlichkeit beliebig klein werden. Ist die anfängliche Erfolgswahrscheinlichkeit groß genug, reichen wenige Wiederholungen aus.\n\nAlle bisher experimentell demonstrierten Quantencomputer bestanden aus wenigen Qubits und waren hinsichtlich Dekohärenz- und Fehlerraten sowie der verwendeten Architektur nicht skalierbar. Unter Architektur versteht man in diesem Kontext das Konzept zur \"skalierbaren\" Anordnung einer sehr großen Zahl von Qubits: wie kann sichergestellt werden, dass die Fehlerrate pro Gatter klein ist (unterhalb der Schwelle für fehlertolerantes Rechnen) und zwar unabhängig von der Zahl der Qubits des Quantencomputers und von der räumlichen Entfernung der beteiligten Qubits im Quantenregister.\n\nDas Problem wurde von David DiVincenzo in einem Katalog von fünf Kriterien, die ein skalierbarer, fehlertoleranter Quantencomputer erfüllen muss, zusammengefasst. Die \"DiVincenzo-Kriterien\" sind\n\n\nDie größten Anforderungen ergeben sich aus dem ersten und dem letzten Punkt. Skalierbarkeit heißt in diesem Fall, dass es möglich sein muss, die Zahl der Qubits beliebig groß zu wählen und dass die anderen Eigenschaften unabhängig von der Zahl der Qubits erfüllt sein müssen. Die Schwelle für fehlertolerantes Rechnen liegt je nach verwendetem Code und verwendeter Geometrie des Quantenregisters bei einer Fehlerwahrscheinlichkeit von formula_74 bis formula_75 (oder noch kleineren Werten) pro Gatter. Bisher ist kein universelles Set von Gattern mit dieser Genauigkeit realisiert worden.\n\nOft werden die oben genannten Kriterien um zwei weitere ergänzt, die sich auf die Vernetzung innerhalb von Quantencomputern beziehen:\n\nDie Suche nach einer skalierbaren Architektur für einen fehlertoleranten Quantencomputer ist Gegenstand aktueller Forschung. Die Fragestellung ist, wie man erreichen kann, dass Quantengatter auf verschiedenen Qubits parallel (gleichzeitig) ausgeführt werden können, auch wenn die Wechselwirkung zwischen den physikalischen Qubits lokal ist, d. h. nicht jedes Qubit mit jedem anderen in direkter Wechselwirkung steht. Je nach verwendetem Konzept (Gatter-Netzwerk, Einweg-Quantencomputer, adiabatischer Quantencomputer, …) und der gewählten Implementierung (gefangene Ionen, supraleitende Schaltkreise, …) gibt es hierzu verschiedene Vorschläge, die bislang allenfalls für kleine Prototypen demonstriert wurden. Zu den konkretesten und weitest fortgeschrittenen Vorschlägen gehören die folgenden:\n\n\nQuantencomputer mit wenigen Qubits konnten bereits in den 1990er Jahren realisiert werden. So wurde Shors Algorithmus im Jahre 2001 mit einem auf Kernspinresonanz beruhenden System am IBM Almaden Research Center mit 7 Qubits realisiert und konnte die Zahl 15 in ihre Primfaktoren 3 und 5 zerlegen. Ebenso konnte im Jahre 2003 ein auf in Ionenfallen gespeicherten Teilchen basierender Quantencomputer den Deutsch-Jozsa-Algorithmus realisieren.\n\nIm November 2005 gelang es Rainer Blatt am Institut für Experimentalphysik der Universität Innsbruck erstmals, ein Quantenregister mit 8 verschränkten Qubits zu erzeugen. Die Verschränkung aller acht Qubits musste durch 650.000 Messungen nachgewiesen werden und dauerte 10 Stunden.\n\nIm März 2011 haben die Innsbrucker Wissenschaftler die Zahl der Qubits noch einmal beinahe verdoppelt. In einer Ionenfalle hielten sie 14 Calciumatome gefangen, welche sie nach dem Prinzip eines Quantenprozessors mit Laserlicht manipulierten.\n\nAn der Yale University kühlte ein Forscherteam um Leo DiCarlo ein Zwei-Qubit-Register auf einem 7 mm langen und 2 mm breiten, von einem mehrfach gekrümmten Kanal durchzogenen Quantenprozessor auf eine Temperatur von 13 mK ab und erzeugte damit einen 2-Qubit-Register-Quantencomputer. Der supraleitende Chip spielte nach einer Veröffentlichung von Nature 2009 zum ersten Mal Quantenalgorithmen durch.\n\nEiner Forschergruppe am National Institute of Standards and Technology (NIST) in Boulder, USA, ist es 2011 gelungen, Ionen mittels Mikrowellen zu verschränken. Die NIST-Forschergruppe hat gezeigt, dass man solche Operationen nicht nur mit einem komplexen, raumfüllenden Lasersystem realisieren kann, sondern auch mit miniaturisierter Mikrowellenelektronik. Um die Verschränkung zu erzeugen, integrierten die Physiker die Mikrowellenquelle in die Elektroden einer so genannten Chipfalle, einer mikroskopischen chipartigen Struktur zur Speicherung und Manipulation der Ionen in einer Vakuumzelle. Mit ihrem Experiment haben die Forscher gezeigt, dass die Verschränkung der Ionen mit Mikrowellen in 76 % aller Fälle funktioniert. Die bereits seit mehreren Jahren in der Forschung verwendeten laserbasierten Quantenlogikgatter sind mit einer Quote von 99,3 % derzeit noch besser als die Gatter auf Basis von Mikrowellen. Das neue Verfahren hat den Vorteil, dass es nur ungefähr ein Zehntel des Platzes eines Laser-Experiments beansprucht.\n\nAm 2. Januar 2014 meldete die \"Washington Post\" unter Berufung auf Dokumente des Whistleblowers Edward Snowden, dass die National Security Agency (NSA) der USA an der Entwicklung eines „kryptologisch nützlichen“ Quantencomputers arbeitet.\n\nIBM ermöglicht seit 2015 den Online-Zugriff auf einen supraleiterbasierten Quantenprozessor. Zunächst standen 5 Qubits zur Verfügung, seit November 2017 sind es 20. Die Website umfasst einen Editor, mit dem Programme für den Quantencomputer geschrieben werden können, sowie ein SDK und interaktive Anleitungen. Bis November 2017 wurden über 35 wissenschaftliche Publikationen veröffentlicht, die den IBM-Computer \"Q Experience\" verwendet haben. Über die Cloud bietet IBM auch Zugriff auf die 50-Qubit-Maschine in ihrem Labor an. Der Quantenzustand dieses Systems wird für 90 Mikrosekunden gehalten, was Ende 2017 ein Rekord war. Bei der Technik für effiziente Simulation von Quantencomputern auf klassischen Hochleistungsrechnern kündigte IBM 2017 an, die 49-Qubit-Grenze erreicht zu haben.\n\nAußer bei IBM entwickeln (Stand 2018) viele große Computerfirmen sogenannte Quantencomputer bzw. deren Technologie, so Google, Microsoft, Intel und Startups wie Rigetti in San Francisco. Google stellte 2018 seinen neuen Quantenprozessor \"Bristlecone\" mit 72 Qubits (skaliert von vorher 9 Qubits) und niedriger Fehlerrate für logische Operationen und Auslesen vor. Er basiert auch auf Supraleitern und dient hauptsächlich der Erforschung der Technologie und eventuell in naher Zukunft dem Nachweis von \"Quantum Supremacy\" (John Preskill 2012), also eines Problems, bei dem der Quantencomputer einem klassischen Supercomputer überlegen ist und dessen Lösung als nächster großer Schritt auf dem Weg zum Quantencomputer gilt (Stand August 2018). Google schätzt wie auch andere Computerfirmen, dass zur Demonstration von \"Quantum Supremacy\" mindestens 49 Qubits, eine Schaltkreistiefe von über 40 und eine Fehlerrate unter einem halben Prozent erforderlich sind. Die Anzahl der Qubits alleine ist nicht entscheidend, sondern zum Beispiel auch die Fehlerrate und die Tiefe des Schaltkreises, das heißt die Anzahl der Gatter (logischen Operationen), die in den Qubits implementiert werden können, bevor die Kohärenz aufgrund zu hoher Fehlerrate zerstört wird. Vor Bristlecone erreichte Google eine Fehlerrate von rund 1 Prozent für Auslesen und für die logischen Operationen 0,1 Prozent für Gatter eines einzelnen Qubits und 0,6 Prozent für Zwei-Qubit-Gatter. Ein kommerziell nutzbarer Quantencomputer liegt nach Google bei rund 1 Million Qubits.\n\nMicrosoft konzentriert sich (Stand 2018) auf theoretische Arbeiten über die Fehlerkorrektur mit Hilfe topologischer Quantencomputer (ein Konzept, das Alexei Jurjewitsch Kitajew 1997 einführte) unter Leitung des Mathematikers Michael Freedman und entwickelte einen Simulator, mit dem Quantencomputer auf klassischen Computern simuliert werden können, und Software für Quantencomputer. Sie haben ein eigenes Quantencomputerlabor (Station Q) in Santa Barbara.\n\n\n"}
{"id": "9139", "url": "https://de.wikipedia.org/wiki?curid=9139", "title": "ASCII-Art", "text": "ASCII-Art\n\nASCII-Art ( für \"ASCII-Kunst\") ist eine Kunstrichtung, die mit Buchstaben, Ziffern und Sonderzeichen einer nichtproportionalen Schrift kleine Piktogramme oder ganze Bilder darzustellen versucht. Auf Computern eignet sich der ASCII-Zeichensatz hierfür besonders, da er weltweit auf nahezu allen Systemen verfügbar ist.\nVor dem Aufkommen der Computer gab es bereits die Typewriter Art, bei der mittels Schreibmaschine oder Fernschreiber Bilder erzeugt wurden. Der Amerikaner Paul Smith gilt hier als künstlerischer Vorreiter.\n\nMittlerweile existieren aber auch spezialisierte Programme, die das Zeichnen erleichtern und mit denen sich auch Animationen gestalten lassen.\n\nVor der Einführung von grafikfähigen PCs war die ASCII-Art die einzige Möglichkeit zur Schaffung von Bildelementen und Illustrationen und vor allem in Mailboxen weit verbreitet. Es stehen dabei nur die 128 ASCII-Zeichen zur Verfügung, was bedeutet, dass z. B. Umlaute und viele Sonderzeichen nicht verwendet werden können. Erweiterungen des ASCII-Zeichensatzes auf 256 Zeichen sind betriebssystem- oder länderspezifisch und damit nicht mehr global anwendbar.\n\nFrühe Beispiele nach diesem Schema wurden bereits in den 1940er Jahren und zuvor mit Setzkästen oder mechanischen Schreibmaschinen in mühevoller Kleinarbeit erstellt. Zur Vervielfältigung diente zunächst Kohlepapier und später auch Fotokopiergeräte. Einen ersten Höhepunkt erlebte das Genre mit der Verbreitung des Fernschreibers, da die Grafiken dort auf Lochstreifen gespeichert und an andere verschickt werden konnten, die ihrerseits die Grafik beim Empfang auf Lochstreifen „mitlochen“ konnten. In den 1970er Jahren war das Erstellen großformatiger ASCII-Bilder ein beliebter Zeitvertreib an Großrechnern.\n\n\n Kuh Yin/Yang Person Radfahrer Eule Hausschwein Gesichter\nDie Kuh wird angezeigt, wenn man unter Systemen mit dem Debian-Paketverwalter apt den Befehl „apt-get moo“ eingibt (siehe auch Easter Egg).\n\nBei vielen nicht-trivialen ASCII-Werken ist es üblich, eine Signatur in die Grafik einzubauen, hier \"SSt\", Kürzel eines Autors. Es gilt als zum guten Ton gehörend, solche Signaturen bei einer Weiterverbreitung der Grafik niemals zu entfernen, zumal ASCII-Art von vielen Autoren als urheberrechtlich geschütztes Werk angesehen wird. Ob und ab welcher Komplexität oder Originalität ein solches Werk schützbar ist, ist nicht geklärt.\n\nUm einer Inflation immer größerer Grafiken in den Signaturen von Usenet-Postings vorzubeugen, gilt es als netiquettekonform, nicht mehr als vier Zeilen automatisch anzuhängen. Hieraus ergibt sich die Schwierigkeit, mit nur relativ wenigen Zeichen aussagekräftige Bilder zu erzeugen, wie bei dieser Vier-Zeilen-Grafik, die eine Katze darstellt:\nAnstatt nur die Umrisse darzustellen, gibt es auch die Möglichkeit, flächig zu arbeiten. Dabei kann man mit verschiedenen Zeichen Hell- und Dunkel-Effekte erzielen.\n\nEinzeilige ASCII-Art kann wie Emoticons in Fließtext eingebaut werden. Ein durch ASCII-Art ausgedrückter zugeworfener roter Hering ist eine typische Antwort auf den Beitrag eines Trolls. Er soll die Wertlosigkeit seines Beitrags unterstreichen:\nEin weiteres Beispiel ist die Rose, die als Text gewordener Blumengruß verwendet wird:\nDa es im Usenet oft nicht möglich ist, Binärdateien anzuhängen, werden z. B. in Elektronikgruppen ASCII-Zeichnungen für die Darstellung von Schaltplänen verwendet. Somit ist auch kein externes Anzeigeprogramm oder ein Browser-Plugin erforderlich. Folgendes Beispiel zeigt einen Multivibrator (astabile Kippstufe):\nEs gibt keine festen Regeln, wie Schaltzeichen optimal in ASCII-Grafiken darzustellen sind. Je nach Platzbedarf verwendet man möglichst geeignete Anlehnungen, teils aber auch eigene Erfindungen oder ältere Formen, die nicht den heutigen Standards genügen, die aber leichter in ASCII darstellbar sind.\n\nEine andere, raumsparende Darstellung zu obigem Beispiel wäre:\nAls weiteres Beispiel aus der Beschaltung von Scartkabeln (komplett ausgeführte Versionen siehe dort):\nBeispiel Logikgatter nach DIN 40700 (vor 1976) statt der aktuellen IEC 60617-12\n\nIm Vergleich:\nMit Logisim erstellter Schaltplan eines Volladdierers und der dazugehörige ASCII-Art-Schaltplan.\n→ \"Siehe auch: Tabulatur#Moderne Gitarrentabulatur\"\n\nASCII-Tabulaturen für Gitarre und Bass existieren ebenfalls. Das Beispiel zeigt den Gitarrenriff zu Smoke on the Water, gespielt mittels Powerchords.\nZur ASCII-Art gehören auch die sogenannten FIGlet-Fonts, die es in vielfältigen Ausführungen gibt.\nBei der Suche in Google nach „ascii art“ (ausschließlich in Kleinbuchstaben) wurde das Google-Logo als ASCII-Art angezeigt:\nFalls man sich auf der Internetseite des Guardian den Quelltext anzeigen lässt, steht dort in einer der ersten Zeilen \"We are hiring\" als ASCII-Art.\n\nDas Penrose-Dreieck\nSeit 1996 gibt es Stereogramme nach der \"SIS\"- oder \"SIRDS\"-Methode, die normalerweise mit horizontal periodisch wiederholten Punktgrafikmustern arbeitet. Das geht aber auch in ASCII, wie ein Student (hier anonymisiert und etwas ausgebaut) in seiner Usenet-Signatur demonstrierte:\nEs gibt Programme, die ASCII-Grafik in Bilder echter Grafikformate konvertieren. Das kann man zunächst einfach von Hand tun, indem man ein Bildschirmfoto vom Fenster mit der ASCII-Grafik macht. Für eine skalierbare Grafik ist der Aufwand allerdings höher.\n\nWeiterhin gibt es auch Programme und Programmbibliotheken, die Bilder und Filme in ASCII-Bilder (\"aalib\") oder ASCII-Filme (\"aaplayer\") umwandeln. Eine Interpretation der ursprünglichen Grafik als Vorlage für Linienführung ist hierbei eher selten. Oftmals wird die Umsetzung der Bildhelligkeit durch die Flächendeckung der verwendeten Zeichen realisiert. Das Programm JavE erlaubt beispielsweise durch die Wahl verschiedener Zeichengruppen sowie der Änderung von Helligkeit und Kontrast der Vorlage eine Beeinflussung des Ergebnisses einer solchen Umwandlung. Solche automatischen Konvertierungen erzeugen meist sehr große Bilder, bei kleinen Formaten ist die Qualität ohne massive manuelle Nachbearbeitung je nach Bildvorlage meist wenig zufriedenstellend.\n\nBei ATI-Grafikkarten unter Windows gibt es bei der Benutzung von OpenGL einen sogenannten „Smartshader“ (ein Echtzeit-Berechnungseffekt auf Shader-Basis), der die gerenderten Bilddaten als ASCII-Art auf dem Bildschirm wiedergibt.\n\nVideo-Player wie MPlayer und der VLC Media Player in neueren Versionen können Videos in farbigen ASCII-Zeichen wiedergeben.\n\nAuch auf Smartphones gibt es Apps, die Kamera-Bilder live in eine ASCII-Grafik umwandeln. Ein Beispiel dafür ist AsciiCam für Android.\n\n\nASCII-Art im Internet\n\nSoftware zum Erstellen von ASCII-Art\n"}
{"id": "10193", "url": "https://de.wikipedia.org/wiki?curid=10193", "title": "High Performance File System", "text": "High Performance File System\n\nHPFS ist das Dateisystem von OS/2. \n\nEs wurde 1989 mit der OS/2-Version 1.2 als erste Implementierung eines installierbaren Dateisystems (IFS) eingeführt. In Gegensatz zum direkt im Systemkern integrierten FAT16-Dateisystem unterstützt es Partitionen bis zu einer theoretischen Größe von 2 Tebibyte (in der Praxis bis zu 64 Gibibyte) und lange Dateinamen mit bis zu 255 Zeichen, sowie sogenannte erweiterte Attribute, die es erlauben, beliebige Metainformationen mit einer Größe bis zu 64 KiB je Datei, an eine Datei zu binden, ohne deren Inhalt zu verändern. So lässt sich z. B. eindeutig hinterlegen, mit welcher Anwendung eine Datei bearbeitet werden kann. Hierdurch entfällt gleichzeitig der Zwang, einer Datei eine bestimmte Dateiendung geben zu müssen (wie z. B. unter Windows).\n\nDas fehlende Journaling führt dazu, dass das Dateisystem durch Abstürze verwundbar ist. Aufgrund dessen und der Größenbeschränkungen für Dateien und Partitionen wurde von IBM später das Journaled File System entwickelt, welches seit 2000 von OS/2 unterstützt wird.\n\nHPFS wurde in OS/2 in zwei Varianten implementiert. Die Standard-Version lag dem Betriebssystem bei, daneben gab es eine 32-Bit-Variante namens \"HPFS386\", die unter anderem Bestandteil des LAN Server war.\n\nEs sind verschiedene Programme zum Lesen von HPFS-Dateisystemen verfügbar. Einige bieten auch Schreibmöglichkeiten an.\n\nDie ersten Versionen von Windows NT unterstützten neben dem damals neuen Dateisystem NTFS auch HPFS und konnten bei Bedarf auf HPFS-formatierte IFS-Partitionen installiert werden. Windows NT 4.0 unterstützt standardmäßig kein HPFS mehr, es sei denn, es wird eine Update-Installation von Windows NT 4.0 von einer bestehenden Windows NT 3.51-Installation vorgenommen. Der Treiber kann aber auch manuell von der Windows NT 3.51-CD kopiert und installiert werden. Bei Windows 2000 ist der nötige Treiber auf der Windows 2000-CD vorhanden und kann bei Bedarf nachinstalliert werden. In allen Fällen unterstützt der mitgelieferte Treiber lediglich Partitionen bis zu einer Größe von 4 GiB, größere Partitionen werden nicht unterstützt und beim Zugriff zerstört. In Windows XP wurde die HPFS-Unterstützung entfernt – auch ein auf einer HPFS-Partition installiertes Windows-NT-Betriebssystem lässt sich fortan nicht mehr starten.\n\nEin Kernelmodul sorgte ursprünglich nur für den lesenden Zugriff. In späteren Versionen wurde die Schreibfunktionalität hinzugefügt und sonstige Erweiterungen vorgenommen.\n\n\n"}
{"id": "10223", "url": "https://de.wikipedia.org/wiki?curid=10223", "title": "Galeon", "text": "Galeon\n\nGaleon ist ein freier Webbrowser der primär für die Integration in die Desktop-Umgebung Gnome gedacht war. Die Entwicklung wurde 2008 eingestellt.\n\nZum Anzeigen der Webseiten verwendet er Gecko, die HTML-Rendering-Engine von Mozilla. Ein Unterschied im Vergleich zu offiziellen Mozilla-Browsern besteht darin, dass anstatt der XUL- eine reine GTK+-2-Oberfläche eingesetzt wird. Die Weiterentwicklung ist seit September 2008 endgültig eingestellt. Zielgruppe von Galeon waren vor allem Benutzer, die einen in Gnome integrierten und im Vergleich zum offiziellen Gnome-Browser Epiphany funktional vielfältigeren Browser bevorzugten. Im Unterschied zu Epiphany, der seinerseits eine Abspaltung von Galeon ist, richtet sich Galeon daher weniger an den Gnome Human Interface Guidelines (HIG) aus und bietet zahlreiche Einstellmöglichkeiten und Funktionen.\n\nDie erste stabile Version von Galeon, Galeon 0.6, wurde im Juni 2000 veröffentlicht. Im November 2001 folgt die Version Galeon 1.0. Mit Galeon 1.3 wird im Oktober 2002 die erste Gnome-2-Version veröffentlicht.\n\nNach Meinungsverschiedenheiten über die Designziele des Projekts und die Zukunft des Browsers verlässt der führende Entwickler Marco Pesenti Gritti im November 2002 das Galeon-Team und fängt mit Epiphany ein neues Projekt an. Drei Jahre später, im Oktober 2005, wird beschlossen, dass Galeon nicht mehr eigenständig weiterentwickelt wird, sondern die besonderen Galeon-Funktionen sollen bei Epiphany als Erweiterungen eingebracht werden.\n\nIm November 2005 erscheint Galeon in Version 2.0.0., im September 2006 folgt die Version 2.0.2 und im September 2008 erscheint die finale Version 2.0.7.\n\n\n"}
{"id": "10425", "url": "https://de.wikipedia.org/wiki?curid=10425", "title": "Gnome", "text": "Gnome\n\nGnome (Eigenschreibweise \"GNOME\") [] ist eine Desktop-Umgebung für Unix- und Unix-ähnliche Systeme mit einer grafischen Benutzeroberfläche und einer Sammlung von Programmen für den täglichen Gebrauch. Gnome wird unter den freien Lizenzen GPL und LGPL veröffentlicht und ist Teil des GNU-Projekts.\n\nGnome ist unter anderem der Standard-Desktop von Fedora und Ubuntu. Einige Komponenten von Gnome wurden nach Windows und MacOS portiert, etwa Evolution oder GStreamer, werden jedoch teilweise, wie im Falle von Evolution, nicht mehr länger gepflegt.\n\nDer Gnome-Desktop soll Einfachheit und Benutzerfreundlichkeit betonen; die Software soll „einfach funktionieren“. Deshalb wird vor allem auf folgende Dinge Wert gelegt:\n\nDas Gnome-Projekt wurde 1997 von Miguel de Icaza und Federico Mena initiiert und war konzipiert als Antwort auf K Desktop Environment. Dieses baute von Anfang an auf dem damals nur ohne freie Software-Lizenz verfügbaren Qt auf. Weil zu dieser Zeit das Unternehmen Trolltech (der Hersteller von Qt) keinen Handlungsbedarf sah, die Lizenzierung zu ändern, riefen Mitglieder von GNU zwei neue Projekte ins Leben: „Harmony“ sollte einen Ersatz für die Qt-Bibliotheken produzieren, während das Gnome-Projekt einen Desktop erstellen sollte, der nicht auf Qt basierte und vollständig aus freier Software bestand.\n\nIm November 1998 gab Trolltech die Lizenzierung der Qt-Bibliotheken unter der QPL bekannt, doch die Diskussion um die Kompatibilität zur GPL dauerte noch bis zum September 2000 an. Schließlich veröffentlichte Trolltech die Linux-Version der Qt-Bibliotheken auch unter der GNU GPL, womit die meisten Einwände gegen eine weitere Benutzung beseitigt waren. Dies galt jedoch nicht für die Windows-Varianten, die erst seit Juni 2005 ebenfalls unter einem dualen Lizenzsystem verfügbar sind.\n\nMan entschied sich, für Gnome auf das im GIMP verwendete GTK (GIMP-Toolkit) zurückzugreifen, das unter der LGPL lizenziert ist und damit den ausgeführten Programmen eine wesentlich größere Auswahl an möglichen Lizenzen zulässt.\nGTK verwendet die Programmiersprache C anstelle von C++, welche in Qt zum Einsatz kommt. Durch die Verwendung von C versprach man sich auch eine leichtere und höhere Portabilität, sowie eine verbesserte Anbindung an andere Programmiersprachen. Mittlerweile gibt es sogenannte Sprachbindungen für alle gängigen Programmiersprachen. Das Gnome-Projekt selbst bewirbt auf seinem Entwicklerportal für Anwendungsentwickler die Sprachen C, C++, Python, JavaScript und Vala.\n\nDer Name „GNOME“ war ursprünglich ein Akronym für „GNU Network Object Model Environment“. Dieses Akronym wurde später fallengelassen und gilt heute als veraltet, trotzdem schreibt sich das Projekt noch mit Großbuchstaben. \n\nIm März 1999 erschien die Version 1.0 von Gnome, die noch einige Fehler enthielt. Die Version 1.0.55, die im Oktober desselben Jahres veröffentlicht wurde, beinhaltete die notwendigen Korrekturen.\n\nIm selben Jahr wurden auch die Firmen Eazel und HelixCode gegründet, die sich mit Programmen und Services rund um Gnome beschäftigten. Bei Eazel wurde der Dateimanager Nautilus entwickelt, bei HelixCode arbeitete man hauptsächlich an einer eigenen Gnome-Distribution und dem Personal Information Manager Ximian Evolution. Die \"Eazel Services\", ein kommerzieller Add-On-Service zu Nautilus, erwiesen sich jedoch nicht als gewinnbringend genug, um Eazel finanziell abzusichern. Eazel konnte insgesamt elf Millionen US-Dollar einwerben und ging am 15. Mai 2001 in Insolvenz. HelixCode benannte sich später in Ximian (heutiger Name: Xamarin) um und wurde im August 2003 an Novell verkauft.\n\nZur Veröffentlichung der Version 1.2 (Bongo Gnome) kam es im Mai 2000 – die Version enthielt hauptsächlich Verbesserungen der Bedienbarkeit und war vollständig binärkompatibel zur Version 1.0.\n\nIm August 2000 wurde die Gnome Foundation gegründet, der sich Firmen wie IBM, Sun Microsystems und Hewlett-Packard anschlossen, um ihre Unterstützung für Gnome zu verstärken.\n\nDie Gnome-Version 1.4, die im April 2001 herauskam, enthielt als offizielle Versionen erstmals den Dateimanager Nautilus sowie das neue Komponenten-Framework Bonobo. Im Kinofilm „Startup“ (Originaltitel „Antitrust“) von Peter Howitt aus dem Jahr 2001 ist der Gnome-Desktop in der Version 1.4 mehrmals in Aktion zu sehen.\n\nDas zugrundeliegende GTK+ wurde stark erweitert, zudem wurden einige Bibliotheken aus Gnome herausgelöst und in eine neue GTK+ Version integriert. Darauf basierte die im Juni 2002 veröffentlichte Gnome-Version 2.0, die zugleich auch eine Zäsur mit sich brachte: Es wurden viele Vereinfachungen durchgeführt, und eine ganze Reihe bis dahin enthaltener Konfigurationsoptionen verschwanden, da sie von den Entwicklern als unnötig empfunden wurden. Es sollten bereits beim ersten Start der Benutzeroberfläche sinnvolle Einstellungen gewählt und nicht Designfehler durch das Hinzufügen von vermeintlichen Konfigurationsoptionen kaschiert werden. Diese Entwicklung zog sich einiges an Kritik zu, doch mittlerweile ist gerade die übersichtliche Bedienbarkeit eines der Hauptcharakteristika von Gnome.\n\nDie Gnome-Version 2.2 erschien im Februar 2003 und beinhaltete wieder eine ganze Reihe an Verbesserungen, besonders bei der Ergonomie und Benutzerführung. Mit dieser Version wurde der bis dahin verwendete Fenstermanager Sawfish durch Metacity ersetzt. Zudem wurde der Veröffentlichungs-Zyklus nun halbjährlich: Alle bis zu dem jeweiligen Zeitpunkt vollständig implementierten Erweiterungen und Verbesserungen sind dann enthalten.\n\nIm September 2003 wurde die Gnome-Version 2.4 freigegeben. Vor allem die Nutzungsmöglichkeiten für Behinderte sowie der Dateimanager Nautilus wurden bearbeitet, ebenso das Gnome-Panel und das Gnome-Kontrollzentrum. Ferner wurde Epiphany (eine Abspaltung von Galeon) als Web-Browser integriert.\n\nIm März 2004 stand die Gnome-Version 2.6 zum Herunterladen bereit. Verbessert wurde vor allem der Dateimanager Nautilus, der neu mit einem Spatialinterface aufwartet, das heißt, jeder Ordner wird durch ein eigenes Fenster repräsentiert. Auch erfolgte mit dieser Veröffentlichung der Umstieg auf GTK+ 2.4, was auch einen neuen Dateidialog nach sich zog. Das Hilfesystem Yelp erfuhr auch einige Verbesserungen und wurde vor allem wesentlich beschleunigt.\n\nIm September 2004 wurde Gnome-Version 2.8 freigegeben. Neu waren unter anderem die vollständige Integration von Evolution, die automatische Einbindung von Datenträgern sowie eine verbesserte Verwaltung von Dateitypen.\n\nIm März 2005 erschien die Gnome-Version 2.10. Es wurden die Programme Sound Juicer (ein CD-Ripper) und Totem (ein Mediaplayer, der wahlweise auf GStreamer oder xine aufsetzen kann) integriert. Weiterhin wurden Benutzerfreundlichkeit sowie Stabilität verbessert und die Panel-Applets stark erweitert bzw. teilweise neu geschrieben.\n\nIm September 2005 wurde die Gnome-Version 2.12 fertiggestellt. Als augenscheinlichste Neuerung brachte sie standardmäßig eine neue Theme-Engine namens Clearlooks mit, unter der Haube erfuhr das GTK+-Toolkit eine Aktualisierung auf Version 2.8, was nun den großflächigen Einsatz von Vektorgrafiken durch die Cairo-Bibliothek ermöglichte. Der bisher weitgehend vernachlässigte Browsermodus des Dateimanagers Nautilus und der Mediaplayer Totem wurden ebenso verbessert. Zudem wurde Evince als Standardbetrachter für PostScript- und PDF-Dokumente eingeführt und ersetzte damit gpdf und ggv.\n\nDie Version 2.14.1 ist im April 2006 freigegeben worden. Neben einigen Geschwindigkeitsverbesserungen für das Terminal wurde gesteigerter Wert auf die bessere Integration der einzelnen Komponenten gelegt. Die wichtigste Neuerung stellt jedoch Ekiga dar, eine neue Version der VoIP-Software, die jetzt auch SIP-Telefonate beherrscht.\n\nDie Version 2.16 wurde am 6. September 2006 fertiggestellt. Die wohl bedeutendste – allerdings auch stark umstrittene – Änderung ist die Aufnahme des Mono-Projekts als Bestandteil des Desktops. Weiterhin gibt es Verbesserungen für Menschen mit Behinderung (Barrierefreiheit) und die direkte Unterstützung von AIGLX durch den Fenstermanager Metacity.\n\nDie Version 2.18 wurde am 15. März 2007 veröffentlicht. Sie enthält sowohl die neuen Spiele glChess und Gnome-Sudoku als auch Netzwerkunterstützung für Nibbles, Iagno und Vier gewinnt. Bestandteil des Desktops ist nun auch Seahorse, eine Anwendung zur Verwaltung von GPG- und SSH-Schlüsseln sowie der Passwörter im Gnome-Schlüsselbund. Die Benutzeroberfläche des Interface-Designers Glade wurde umstrukturiert und das Programm von Grund auf neu geschrieben, so dass nun alle benötigten Werkzeuge in einem Fenster zusammengefasst sind.\n\nMit der am 19. September 2007 veröffentlichten Version 2.20 wurde vor allem der Dateimanager Nautilus überarbeitet. Das Programm kann jetzt auf eine Desktopsuche (Beagle oder Tracker) zurückgreifen, die Exif-Daten von Bildern auslesen und per SSH auf andere PCs zugreifen. Dazu wurden diverse Konfigurationsprogramme überarbeitet oder zusammengelegt.\n\nDie Version 2.22 wurde am 12. März 2008 veröffentlicht. Neuerungen sind erweiterte Multimediafähigkeiten, verbesserte Netzwerkdateisysteme (GVFS ersetzt GnomeVFS), eingebaute Desktopeffekte (Fenster-Compositing) und zahlreiche andere kleine Verbesserungen. Als neue Programme sind Cheese (zum Aufnehmen und Bearbeiten von Fotos und Videos) und ein Betrachter für entfernte Desktops mit dabei.\n\nDie Version 2.24 wurde am 25. September 2008 veröffentlicht. Sie enthält den Chat-Client Empathy und Verbesserungen des Dateimanagers, der jetzt unter anderem auch Reiter und eine kompakte Listenansicht unterstützt. Neu ist außerdem die Unterstützung von Klangthemen.\n\nDie Version 2.26 wurde am 18. März 2009 veröffentlicht.\n\nDie Version 2.28 wurde am 23. September 2009 veröffentlicht.\n\nDie Version 2.30 wurde am 31. März 2010 fertiggestellt.\n\nMit Gnome 2.32 wurde am 29. September 2010 die letzte Version der 2.x Reihe veröffentlicht.\n\nGnome 2 wird nun als MATE Desktop Environment weiter entwickelt.\n\nNach Kritik am Gnome-Desktop, der Stagnation, der fehlenden Vision des Projekts und der daraus entstandenen Diskussion kündigte das Gnome-Release-Team die Entwicklung von Gnome 3.0 an. Zur Sicherstellung der Stabilität von Gnome sollte die Veröffentlichung der endgültigen Version 3.0 auf September 2010 verschoben werden. Bei der GUADEC in Den Haag wurde jedoch der Releasetermin um weitere 6 Monate auf April 2011 verschoben. Wie ursprünglich geplant, wurde im März 2010 die Version 2.30 veröffentlicht. Im Gegensatz zu KDE Plasma Desktop soll Gnome 3.0 eine inkrementelle Weiterentwicklung und keine große Umwälzung sein. Dennoch wurden wesentliche Einschnitte vollzogen, so dass Teile von Gnome 3.0 nicht mehr abwärtskompatibel zu Gnome 2.x sind. Die primären Neuerungen in Gnome 3.0:\n\nEinige der Neuerungen (wie die Gnome Shell) sind optional schon seit Gnome 2.28 erhältlich, allerdings noch nicht mit vollem Funktionsumfang.\n\nAm 6. April 2011 wurde Gnome 3.0 offiziell veröffentlicht, die Version 3.2 wurde am 28. September 2011 veröffentlicht. Letztere ermöglicht eine einfachere Anbindung von GNOME an Cloud-Dienste, da Konten bei diversen Anbietern mit Hilfe des Programms Gnome Online Accounts zusammengefasst werden.\nAm 28. März 2012 wurde die Version 3.4 veröffentlicht, im September die Version 3.6.\nAm 27. März 2013 wurde die Version 3.8 veröffentlicht.\n\nAm 25. September 2013 wurde die Version 3.10 veröffentlicht, welche erstmals experimentelle Unterstützung für Wayland bietet. Daneben wurde die Gnome-Shell dahingehend geändert, dass eine Reihe von Menüs zu einem Einzigen zusammengefasst worden sind. Des Weiteren haben sog. „Kopfleisten“ Einzug gefunden, welche die typischen Titelleisten am oberen Rand eines Fensters mit den Werkzeugleisten des jeweiligen Programms verbindet, um so der eigentlichen Anwendung mehr Platz bieten zu können. Ferner wurden einige neue Anwendungen vorgestellt, z. B. Karten, Fotos und Notizen. Viele bereits vorhandene Anwendungen wurden aktualisiert und verbessert. Komplett überarbeitet wurde das sog. „Optimierungs-Werkzeug“, um es benutzerfreundlicher zu machen. Das erste Mal lässt sich auch, seit GNOME 3.0, wieder ein separates Hintergrundbild für den Sperr-Bildschirm einrichten.\n\nAm wurde die Version 3.12 veröffentlicht.\nDer Assistent zur Ersteinrichtung wurde benutzerfreundlicher und bietet nun eine größere Karte zum Auswählen der Zeitzone. Weiterhin werden die Sprache und Region nun getrennt eingestellt und es kann ein Avatar für das Benutzerkonto ergänzt werden.\nDie Software-Anwendung wurde überarbeitet und läuft nach den GNOME-Entwicklern nun flüssiger. Hinzugefügt wurde die Möglichkeit Softwarequellen zu überarbeiten und die Programmeinträge um Bildschirmfotos und Bewertungen ergänzt.\nDie Video-Anwendung wurde überarbeitet und neben lokalen Dateien können auch Online-Dienste wie The Guardian und Blip.tv durchsucht werden.\nDer Texteditor gedit bekam ein neues kompakteres Aussehen und die Einstellungsmenüs wurden überarbeitet.\nDie Oberfläche wurde in weiten Teilen um Funktionen ergänzt, so werden nun drahtgebundene Netzwerke in der Systemstatusanzeige angezeigt, Dialoge sind in der Aktivitätenübersicht verfügbar. Mit Anwendungsordnern können Anwendungen gruppiert werden, um jene leichter zu finden.\nDie mit Version 3.10 eingeführte Unterstützung von hochauflösenden Monitoren wurde in dieser Version weiter ausgebaut.\nDie Reiter bekamen ein neues Aussehen, welches aus Sicht der GNOME-Entwickler an vielen Stellen harmonischer als das alte Design ist. Als Beispiele nennen die Entwickler verschiedene Hintergründe der Reiter und eine bessere Ausnutzung des verfügbaren Platzes.\nNeu hinzugekommen ist mit GNOME 3.12 das Element \"Einblenddialoge\". Dies sind überblendende Menüs, welche verschiedene Konfigurationsmöglichkeiten einer Anwendung bieten.\nDie Integration von Cloud-Diensten in GNOME ist weiter fortgeschritten, sodass Unterstützung für Google Cloud Print und Google Drive ergänzt wurden. Das Programm Photos erhielt Facebook-Integration und in den Einstellungen können für das Hintergrundbild auch eigene Flickr-Konten durchsucht werden.\nDie Bluetooth-Einstellungen wurden überarbeitet, sodass verfügbare Geräte automatisch erkannt werden.\nWeiterhin bauten die GNOME-Entwickler die in Version 3.10 experimentelle Wayland-Unterstützung weiter aus.\n\nAm 24. September 2014 erschien die GNOME-Version 3.14. \n\nAm wurde Gnome 3.16 freigegeben, welche ein neues Benachrichtigungssystem enthält. Die Benachrichtigungs-Popups sind geblieben, jedoch werden alte Benachrichtigungen nun zusammen mit dem Kalender angezeigt. Der Kalender zeigt nun die Weltzeit an und Event-Erinnerungen. In zukünftigen Versionen sollen Wetterinformationen und Geburtstagserinnerungen erscheinen.\nDer Dateimanager Nautilus zeigt Vorschaubilder nun größer an und wurde um einen Einblenddialog ergänzt. Nach Angaben der Entwickler ist die Oberfläche durch besser lesbare Zeilen deutlich ansprechender als vorher. Weiterhin können Dateien nun mit codice_1 statt codice_2 gelöscht werden.\nMit der neuen GNOME-Version wurde die Oberfläche erneuert. Der Anmeldebildschirm, die Aktivitätenübersicht, die Systemmenus, etc. erhielten ein neues zu GNOME 3.16 aus Sicht der Entwickler passendes Design.\nDer Bildbetrachter erhielt neben einer überarbeiteten GUI ein Steuerelement um Bilder zu vergrößern bzw. zu verkleinern.\nMit der neuen Version sind die Programme \"Kalender\", ein Kalender der mit den Online-Konten verknüpft werden kann, die Zeichentabelle \"Zeichen\" und der E-Book-Reader \"Bücher\" hinzugekommen. Letzterer unterstützt bisher nur Comics.\nDas Frontend der Paketverwaltung ermöglicht neben der Installation von Programmen nun auch die von Codecs.\nZur Erleichterung Software mit grafischen Oberflächen für Linux zu erstellen, wurde die integrierte Entwicklungsumgebung Builder implementiert.\nDie Wayland-Unterstützung wurde ausgebaut, sodass der Login-Bildschirm nun Wayland unterstützt.\n\nAm erschien die GNOME-Version 3.18. Mit dieser wurde die Möglichkeit geschaffen, Google Drive in den Dateimanager zu integrieren. Generell erhielt der Dateimanager viele Überarbeitungen. So zeigt Nautilus nun den Fortschritt und weitere Informationen beim Kopieren an. Wenn ein Lichtsensor vorhanden ist, ermöglicht es Gnome 3.18 die Bildschirmhelligkeit automatisch der Umgebung anzupassen. Hinzu kam eine Anwendung zur Aktualisierung der Firmware der verfügbaren Hardware. Mit der Versionsaktualisierung können Gesten auch mit Tastfeldern genutzt werden. Dieses mit Gnome 3.14 eingeführte Feature war vorher nur mit berührungsempfindlichen Displays möglich. Der Dokumentenbetrachter Evince ermöglicht fortan das Abspielen integrierter Multimediainhalte in PDF-Dokumenten, sowie eine bessere Kommentarfunktion. Weitere Verbesserungen erhielten unter anderem der IRC-Client Polari und die Virtualisierungssoftware Boxes.\n\nAm wurde Gnome in der Version 3.20 veröffentlicht. Mit dieser Version wird die Unterstützung des Displayservers Wayland als für die tägliche Nutzung ausreichend entwickelt betrachtet. Jedoch werden noch nicht alle Funktionen wie Wacom-Grafiktablets unterstützt. Die Softwareverwaltung ermöglicht dem Nutzer, von nun an nicht nur die installierte Software, sondern auch das gesamte Betriebssystem zu aktualisieren. Zusätzlich ermöglicht die Anwendung dem Benutzer, die Software zu bewerten. Das Programm Fotos ermöglicht nun die teilweise Bearbeitung von Bildern, wie z. B. der Farbe oder die Anwendung einiger künstlerischen Filter. Die Originaldatei bleibt jedoch dabei unverändert. Der Benachrichtigungsbereich ermöglicht nun einen Zugriff auf die Mediensteuerung, welche Informationen wie den Titel und Künstler abgespielter Musik anzeigt. Der Dateimanager wurde weiter verbessert, besonderes Augenmerk wurde auf die Suchfunktion gelegt, welche überarbeitete Suchfilter erhielt. Die neu eingeführten Kürzelfenster sollen ein schnelleres Finden von Tastaturkürzeln ermöglichen. In der neuen Version der Applikation Karten kann der Anwender die OpenStreetMap bearbeiten und sie zeigt detaillierte Informationen zu den Orten an. Außerdem können die Karten nun in andere Formate exportiert werden. Eine weitere Überarbeitung erhielt der Ortungsdienst. Dieser kann nun für jede Anwendung einzeln an- und abgeschaltet werden. Wird mit GNOME 3.20 nach W-LAN-Access-Points gesucht, wird die MAC-Adresse des Benutzers verschleiert, um ihn weniger verfolgbar zu machen.\n\nGNOME 3.24 wurde am vorgestellt. Mit dieser Version wurde ein Nachtmodus neu eingeführt und in den Einstellungen wird bei Druckern zusätzlich der Tintenstand mitangezeigt. Die Desktopumgebung wurde um die Anwendung „Rezepte“ erweitert. Verbessert wurde u.  der Benachrichtigungbereich, der Webbrowser, der IRC-Client und die Anwendung „Fotos“. Die Auflösung der Symbole wurde vervierfacht und weitere Anwendungen laut Entwicklern verbessert.\n\nDas unter dem Codenamen „Chongqing“ veröffentlichte GNOME 3.28 wurde am 14. März 2018 veröffentlicht. Neben neuen Applikationen wie Auslastung (einem Diagnosetool zur Behebung von Leistungs- und Kapazitätsproblemen), wurde die Geräteunterstützung insbesondere Thunderbold 3 verbessert. Daneben wurden vor allem ästhetischen Verbesserungen wie die Verbesserung der Standardschriftart Cantarell durchgeführt.\n\nMit der am 5. September 2018 veröffentlichten Version 3.30 mit dem Codenamen „Almería“ können Flatpak-Pakete über die Softwareverwaltung aktualisiert werden und die GNOME-Anwendung „Boxen“ unterstützt nun das Verbinden mit einem Server per Remote Desktop Protocol. Die Anwendung „Laufwerke“ kann nun automatisch mit Veracrypt verschlüsselte Laufwerke einhängen und die Geschwindigkeit der Desktop-Umgebung im Allgemeinen soll gesteigert worden sein. Weiterhin baut GNOME 3.30 auf GTK+ 3.24 und die Designrichtlinien für Anwendungen wurden überarbeitet.\n\nDer Gnome-Desktop basiert auf verschiedenen Programmbibliotheken und Modulen, davon einige freedesktop.org-Projekte. Eine Auswahl wichtiger Komponenten, die als Grundlage für die Anwendungen des Gnome-Projekts dienen, und damit die sogenannte Gnome-\"Plattform\" bilden, ist hier aufgeführt:\n\n\nWeitere Projekte, die in der Gnome-Plattform zum Einsatz kommen, sind beispielsweise libxml2 für XML-Verarbeitung, \"Soup\" für HTTP-Kommunikation, Poppler zur PDF-Darstellung, \"Rygel\" als DLNA/UPnP-Medienserver und udev für die Kommunikation mit der Hardware.\n\nVeraltete Komponenten, die bereits ausgemustert wurden oder in Vorbereitung auf Gnome 3.0 ausgemustert werden, sind Bonobo, ORBit, GConf, HAL, libgnome, libglade, GtkHTML, GnomeVFS, ESD.\n\nVerschiedene Sprachbindungen erlauben es, Applikationen für Gnome in einer Vielzahl von Programmiersprachen zu schreiben. Die Benutzung von GTK+ als Toolkit ermöglicht es, das Aussehen der Icons, Fenster und Komponenten mit Hilfe von Themen individuell anzupassen.\n\nDie Gnome Foundation arbeitet für das Ziel, von Gnome einen kompletten Desktop bestehend aus Freier Software bereitzustellen. Dazu legt sie fest, welche Projekte offizieller Teil von Gnome sind. Die Foundation ist das offizielle Sprachrohr von Gnome. Sie produziert auch Dokumentationen oder Lehrmaterial für die Öffentlichkeit. Außerdem veranstaltet sie Gnome-bezogene Konferenzen wie die GUADEC (\"Gnome Users and Developers European Conference\") oder den Boston Summit, hilft beim Erstellen technischer Standards und fördert die Benutzung und Weiterentwicklung von Gnome.\n\nSpätestens ab der Version 2.0 sind mehrere große Unternehmen und Organisationen in der Weiterentwicklung des Gnome-Desktops involviert. Die Unterstützung reicht von Hardwarespenden über das Beschäftigen der Hauptentwickler und das Herstellen freier Applikationen bis zur Vorgabe einer Entwicklungsstrategie. Nachfolgend sind einige dieser Organisationen aufgeführt:\n\nDie Bemühungen, alles einfach und übersichtlich zu halten, werden zum Teil als zu weitgehend kritisiert und beklagt, dass teils sogar sinnvolle Funktionalität im Sauberkeitswahn wieder entfernt wird. So warf Linus Torvalds Gnome-Entwicklern vor, den Benutzer zum Idioten zu stempeln und bezeichnete Gnome-Entwickler als „Schnittstellen-Nazis“ (orig. engl.: „interface nazis“) und rief wiederholt zur Benutzung der alternativen Desktop-Umgebung K Desktop Environment auf.\nDiesen Kritikpunkten wird entgegengesetzt, dass es versierten Benutzern durchaus möglich ist, mit externen Erweiterungen wie Devil’s Pie oder dem GConf-Editor auch Einstellungen über die Standardmöglichkeiten von Gnome hinaus zu setzen. Zudem nutzte Torvalds vorübergehend selbst Gnome, da er KDE Plasma Desktop 4.0, welches bei ihm unter Fedora per Aktualisierung eingespielt wurde, als Desaster empfand. KDE 4.0 und 4.1 waren jedoch ausdrücklich nicht für Endnutzer gedacht, die entsprechende Freigabe erfolgte erst mit KDE 4.2.\n\nSeit der Ubuntu-Version 11.04 ist die Gnome Shell dort nicht mehr die Standardoberfläche, sie wurde diesbezüglich von Unity abgelöst. Zurückzuführen ist dies auf Unstimmigkeiten zwischen Gnome und Canonical-Entwicklern. Gnome hatte mehrere Entwicklungen von Canonical für das Gnome-Projekt abgelehnt, worauf Canonical beschloss, Gnome 3 nicht zu unterstützen und Gnome 2 nur als Ausweichoption anzubieten.\n\nIm Frühjahr 2017 wurde bei Canonical eine Rückkehr zu Gnome als Standard-Desktop angekündigt. Zunächst war diese für die Version 18.04 vorgesehen, wurde dann aber auf 17.10 vorgezogen.\n\nBesonders die neue Version Gnome 3 wurde vielfach kritisiert, da sie unter anderem das bereits erwähnte „Idiotenkonzept“ weiterführe und völlig vom Unix-Konzept abweiche, Dinge einfach und geradlinig zu halten. Konkret kritisiert zum Beispiel Linus Torvalds:\n\nDiese Kritik beruht darauf, dass Gnome 3 ein völlig anderes, bisher ungewohntes Bedienkonzept einführt. Es wird argumentiert, dass Neulinge damit vielleicht besser zurechtkämen, erfahrene Nutzer jedoch nicht effizient damit arbeiten könnten. Weiter sind einige Gnome-Entwickler der Meinung, dass man von den Gewohnheiten der „Geeks“ abweichen müsse, wenn man den Linux-Desktop zum Erfolg führen wolle. In einem späteren Kommentar schrieb Torvalds jedoch, dass er andere Desktop-Umgebungen, die er ausprobiert habe, wie etwa Unity, KDE Plasma Workspaces und Xfce, noch schlechter als Gnome 3 finde.\n\nIn der Version 3.2 von Gnome sind einige der am häufigsten kritisierten Dinge verbessert oder entfernt worden.\n\nMögliche Alternativen mit einem zu Gnome 2 vergleichbaren Bedienkonzept sind die Desktop-Oberflächen Xfce und MATE. Beide basieren wie Gnome 2 auf GTK+ 2; bei MATE handelt es sich zudem um einen Fork von Gnome 2 und somit um eine direkte Weiterentwicklung dieser Gnome-Version unter einem anderen Namen. Des Weiteren versucht das Projekt Cinnamon, die von Gnome 2 gewohnte Bedienung mit Neuerungen aus der Nachfolgeversion Gnome 3 zu verknüpfen. Debian, Ubuntu, openSUSE, Arch Linux sowie Fedora haben Cinnamon in ihre Paketquellen aufgenommen.\n\nAls Reaktion auf die Kritik wurde in der Version 3.8 von Gnome der \"Classic Mode\", dessen Bedienkonzept sich an das von Gnome 2 anlehnt, um Neuerungen aus Gnome 3 erweitert.\n\n\n"}
{"id": "10524", "url": "https://de.wikipedia.org/wiki?curid=10524", "title": "Chaos Computer Club", "text": "Chaos Computer Club\n\nDer Chaos Computer Club (CCC) ist ein deutscher Verein, in dem sich Hacker zusammengeschlossen haben. Der Verein hat sich zu einer maßgebenden Nichtregierungsorganisation (NGO) in allen Fragen der Computersicherheit entwickelt.\n\nDie Informationsgesellschaft – so der CCC – erfordere „ein neues Menschenrecht auf weltweite, ungehinderte Kommunikation“, weshalb der Club sich „grenzüberschreitend für Informationsfreiheit einsetzt und mit den Auswirkungen von Technologien auf die Gesellschaft sowie das einzelne Lebewesen beschäftigt“.\n\nDie Mitgliedschaft steht jedem offen, der sich mit diesen Zielen identifizieren kann. Der CCC ist ein eingetragener Verein nach deutschem Recht mit Sitz in Hamburg und hat nach eigenen Angaben über 5500 zahlende Mitglieder. Er wurde gegründet, um Hackern eine Plattform zu geben und über Aktivitäten berichten zu können. Die Mitarbeit im CCC ist nicht an eine Mitgliedschaft gebunden.\n\nDer CCC e. V. sieht sich als \"„galaktische Gemeinschaft von Lebewesen“\" und ist dezentral in regionalen Gruppen organisiert. Kleinere Gruppen heißen \"Chaostreffs\", während sich aktivere und größere \"Erfa-Kreise\" (Erfahrungsaustauschkreise) nennen.\n\nMitglieder und Interessierte treffen sich seit 1984 einmal jährlich zum Chaos Communication Congress. Außerdem fand im Sommer 1999 und 2003 das Chaos Communication Camp auf dem Paulshof nahe der Kleinstadt Altlandsberg auf dem Land statt. Dem 4-Jahres-Takt folgend, fand es seit 2007 auf dem Gelände des Luftfahrtmuseums Finowfurt statt, 2015 ist es in den Ziegeleipark Mildenberg umgezogen. Der internationale Charakter des Camps hat sich inzwischen auf den Kongress übertragen, so dass dieser seinem Untertitel „Die europäische Hacker-Party“ nachkommt und Englisch als Konferenzsprache dominiert. Neben den vielen Vorträgen über technische und gesellschaftspolitische Themen gibt es auch Workshops, zum Beispiel über das Lockpicking. Zu Ostern findet regelmäßig in kleinerem Rahmen der Workshop-orientierte Easterhegg statt. Darüber hinaus gibt es über das Jahr verteilt seit Anfang des Jahrzehnts viele kleine Veranstaltungen mit bis zu 200 Personen, die von regionalen Gruppen organisiert werden und teils ein offenes Zusammenkommen der Gemeinschaft sind, teils Vorträge zu einem bestimmten Thema bieten.\n\nDer traditionelle Preis „CCCeBIT-Award“ wurde bis 2007 jedes Jahr zur Computermesse CEBIT in Hannover verliehen.\n\nDie überwachungskritischen Demonstrationen Freiheit statt Angst werden vom Chaos Computer Club unterstützt und teilweise mit eigenen Mobilen begleitet.\n\nDer CCC gibt die Zeitschrift \"Die Datenschleuder, das wissenschaftliche Fachblatt für Datenreisende\", heraus. Zusätzlich ist in den 1980er Jahren in zwei Ausgaben die \"Hackerbibel\" erschienen, ein umfangreiches Kompendium und Sammelsurium mit zahlreichen Dokumenten der Hackerszene. Von 1989 bis 1992 gab der Verein mit der \"Chalisti\" eines der ersten deutschsprachigen elektronischen Magazine heraus. Die Hackerbibeln und alle Ausgaben der Datenschleuder bis zum Jahr 2000 sind digitalisiert und auf der Chaos-CD erhältlich. Außerdem wird seit dem 21. Chaos Communication Congress (2004) ein Tagungsband verfasst und veröffentlicht.\n\nAuf dem Radiosender Fritz aus Potsdam wird seit 1995 das \"Chaosradio\" ausgestrahlt, und zwar in der Regel am letzten Donnerstag im Monat im Rahmen der Sendung „Blue Moon“. Weitere Radiosendungen des CCC sind \"C-RaDaR\" aus Darmstadt, \"/dev/radio\" aus Ulm, \"Radio Chaotica\" aus Karlsruhe, \"Freibyte\" aus Freiburg im Breisgau, \"Fnordfunk\" aus Mainz, \"Pentaradio\" aus Dresden, \"Nerds on Air\" aus Wien und \"Hackerfunk\" aus Zürich. Im Chaosradio Podcast Network werden zahlreiche Podcasts des CCC angeboten.\n\nSchließlich nutzt der CCC als Medium zur Informationsverbreitung auch Twitter und ein Blog.\n\nDer Chaos Computer Club hat 1999 zur Durchführung des Chaos Communication Camps die \"Chaos Computer Club Veranstaltungsgesellschaft mbH\" gegründet, deren Geschäftsführer bis 2006 Tim Pritlove war. Diese richtet seitdem die Großveranstaltungen des CCC aus. Eine ehrenamtliche Sprecherin des Vereins ist Constanze Kurz.\n\n2003 kam die Wau Holland Stiftung als gemeinnützige Organisation hinzu, die seitdem Veranstaltungen und Projekte des CCC trägt.\n\nAls eine Art „regionaler Niederlassungen“ gibt es sogenannte \"Erfa-Kreise\" (Erfahrungsaustausch-Kreis) und \"Chaostreffs\". Die Erfa-Kreise sind fest in der Satzung verankert und bilden in der Regel lokale Vereine mit Clubräumen.\nErfa-Kreise gibt es derzeit (Stand 2017) in den deutschen Städten Aachen, Bamberg, Berlin, Bremen, Darmstadt, Dresden, Düsseldorf, Erlangen/Nürnberg/Fürth, Essen, Frankfurt am Main, Freiburg, Göttingen, Hamburg, Hannover, Kaiserslautern, Karlsruhe, Kassel, Köln, Mainz/Wiesbaden, Mannheim, München, Paderborn, Stuttgart, Ulm, Würzburg sowie in Salzburg, Wien (C3W) und Zürich (CCCZH).\n\"Chaostreffs\" sind losere Zusammenkünfte von Mitgliedern und Interessierten ohne eingetragenen Verein.\n\nHäufig arbeitet der CCC auch mit anderen Organisationen zusammen, die sich gegen Zensur, für Informationsfreiheit oder den Datenschutz einsetzen, wie dem FITUG und digitalcourage (vormals FoeBuD). Außerdem ist er Mitunterzeichner der gemeinsamen Erklärung des AK Vorrat zum Gesetzesentwurf über die Vorratsdatenspeicherung.\n\nDer Ortsverband D23 des DARC, auch Chaoswelle genannt, ist eine dem CCC nahestehende Gemeinschaft von Funkamateuren.\n\nDer Chaos Computer Club France (CCCF) bestand von 1989 bis 1993. Er wurde von Jean-Bernard Condat unter Regie von Jean-Luc Delacour, einem Geheimagenten des französischen Nachrichtendienstes Direction de la surveillance du territoire, gegründet und geführt. Das primäre Ziel war, die französische Hackercommunity zu überwachen und Informationen über sie zu sammeln.\n\nDer CCCF gab vom 4. Januar 1993 bis 5. August 1993 ein digitales Magazin namens \"Chaos Digest (ChaosD)\" mit insgesamt 73 Ausgaben heraus ().\n\nDer CCC wurde am 12. September 1981 in Berlin (West) am Tisch der Kommune I in den Redaktionsräumen der \"taz\" gegründet. Jedoch entwickelte sich der Club in den folgenden Jahren hauptsächlich in Hamburg, da sich dort die Gründungsmitglieder Wau Holland und Klaus Schleisiek alias Tom Twiddlebit aufhielten.\n\nAnfang 1984 wurde die erste Ausgabe der Vereinszeitschrift \"Die Datenschleuder\" veröffentlicht. Mitte 1984 wurde ein selbst gebautes, von der Bundespost nicht zugelassenes Modem entwickelt, das \"Datenklo\", dessen Bauanleitung 1985 in der Hackerbibel abgedruckt wurde.\n\nÖffentliche Bekanntheit erlangte der CCC am 19. November 1984 mit einer Aktion, die „Btx-Hack“ oder „Haspa-Hack“ genannt wurde. Durch einen Datenüberlauf im Btx-System, das von der Bundespost als sicher bezeichnet worden war, bekam ein Teilnehmer Teile des Hauptspeicherinhalts des betreffenden Serie-1-Zugangsrechners auf sein Endgerät ausgegeben. Bei einer Analyse des Dumps stellte sich heraus, dass sich darin die Zugangskennung eines Benutzerkontos der Hamburger Sparkasse (Haspa) einschließlich des Passworts im Klartext befand. Daraufhin loggte sich ein Mitglied des CCC als dieser Benutzer der Haspa ein und rief wiederholt eine kostenpflichtige Seite des CCC ab. Dadurch wurden in einer Nacht knapp 135.000 DM der Hamburger Sparkasse (Haspa) zugunsten des Kontos des Vereins fällig, die nach der Aufdeckung zurückgezahlt wurden.\n\nVoraus ging eine Demonstration einer vergleichbaren Sicherheitslücke durch Wau Holland bei der 8. DAFTA, doch wurde das Problem bei der Post nicht behoben. Nach dem Hack erklärte Haspa-Vorstand Benno Schölermann, die Versicherung der Post, dass Btx sicher sei, sei falsch gewesen, und dass man vor der Tüchtigkeit der Leute vom CCC hohe Achtung habe.\nDer CCC wurde in den kommenden Jahren bei der Schaffung des Datenschutzgesetzes in der Bundesrepublik Deutschland immer wieder konsultiert. Auch wurden Gutachten auf höchster politischer Ebene ausgestellt.\n\nNach dem Btx-Hack wurde der Ruf nach einer Veranstaltung immer lauter, auf der man sich den bekannten und noch kommenden Hacks widmen könne. So wurde kurzerhand Ende Dezember 1984 im Eidelstedter Bürgerhaus in Hamburg-Eidelstedt der erste \"Chaos Communication Congress\" veranstaltet.\n\nSchon 1985 wurde der Club in eine Angelegenheit verwickelt, in der es um Informationsfreiheit ging – eines der späteren Schwerpunktthemen des CCC. Unter Berufung auf die Informationsfreiheit sammelten sich auf den BTX-Seiten des Clubs diverse Texte zu kontroversen Themen an.\n\nSo ließ sich auch ein Auszug aus der Dissertation „Penisverletzungen bei Masturbation mit Staubsaugern“ von Michael Alschibaja Theimuras aus dem Jahr 1978 aufrufen. Da insbesondere Staubsauger des Typs \"Kobold\" der Firma Vorwerk zu Verletzungen führten, fürchtete der Traditionsbetrieb negative Schlagzeilen und sah sich daher durch den CCC geschädigt. Er verklagte den Club auf 500.000 DM Schadensersatz wegen Rufschädigung und verlangte von der Bundespost als Betreiber des BTX-Systems eine Sperrung der Seite. Erst nachdem der Doktorvater der Dissertation und ein Betroffener nachgewiesen worden waren, zog die Firma die Klage zurück.\n\nAm 14. April 1986 wurde der Chaos Computer Club e. V. gegründet und in das Vereinsregister beim Amtsgericht Hamburg unter der Nummer 10940 eingetragen. Eine Gemeinnützigkeit wurde jedoch vom Finanzamt Hamburg nicht anerkannt. Ein Artikel in der \"Datenschleuder\" 60 bringt die Motivation zur Vereinsgründung auf den Punkt: \"„Die damals in Aussicht stehenden Ermittlungsverfahren (wegen NASA/Span-Hack etc.) sollten klar kanalisiert werden, um eine weitergehende Kriminalisierung der Hackerszene (§ 129a) zu verhindern und vor allem die Ermittlungsverfahren an (anwaltlich) gerüstete Stellen (Vorstand) zu lenken. Das hat auch soweit ganz gut funktioniert.“\"\n\nDer Verein ist das finanzielle Rückgrat der \"Datenschleuder\" und für Projekte zur Erforschung von neuen Technologien. Außerdem sind seine Sprecher als Sprachrohr der Hacker-Szene aktiv.\n\nAn das von der NASA und ESA betriebene SPANet (Space Physics Analysis Network) waren weltweit etliche Großrechner insbesondere der Firma Digital angeschlossen. Aufgrund einer Sicherheitslücke im Betriebssystem VMS, die 1986 in den USA behoben wurde, aber erst Mitte 1987 in Europa, gelang es norddeutschen Hackern, Zugriff auf die Systeme und etliche Rechner in diesem Netzwerk zu erhalten. Hierzu zählten Maschinen der NASA, der ESA, Rechner der französischen Atomenergiekommission (Commissariat à l’énergie atomique), Universitäten und Forschungseinrichtungen. Nachweislich konnte jedoch nur Schaden auf Rechnern des „Hacker-Fahrschule“ getauften CERN entdeckt werden, von wo aus weitere Netze erreicht werden konnten.\n\nDie norddeutschen Hacker wandten sich, als ihnen die Situation zu „heiß“ wurde, an den CCC. Dieser wiederum kontaktierte im August 1987 das Bundesamt für Verfassungsschutz, das eine vom Club zusammengestellte Liste aller „besuchten“ Vaxen an die US-amerikanischen Kollegen beim CIA weitergab. Als Folge gab es im September 1987 aufgrund von Strafanzeigen vom CERN in der Schweiz und von Philips Frankreich etliche Hausdurchsuchungen durch das BKA in Zusammenarbeit mit der französischen Staatsanwaltschaft. Es wurde der Vorwurf erhoben, dass die Rechner des Rüstungsunternehmens Thomson, heute zu Thales gehörend, in Grenoble geknackt, die Datenbestände der Zementfabrik Lafarge gelöscht und bei Philips möglicherweise Konstruktionspläne für einen Chip ausspioniert wurden.\n\nAls glücklich mag sich erwiesen haben, dass CCC-Pressesprecher Steffen Wernéry während der Hausdurchsuchung ein in der Nähe befindliches TV-Team des Senders Sat.1 traf. Damit wurde die Hausdurchsuchung Teil der Live-Berichterstattung in den Abendnachrichten des Senders.\n\nAm 14. März 1988 reiste Wernéry zur SECURICOM 88, dem 6. Internationalen Kongress über Datenschutz und Datensicherheit, nach Paris. Bei der Ankunft am Flughafen wurde er auf Grund einer Strafanzeige von \"Philips Frankreich\" verhaftet und zum Verhör festgehalten. Am 20. Mai 1988 wurde er aus der Haft entlassen und konnte nach Deutschland zurückkehren.\n\nAus dem NASA-Hack entstand der KGB-Hack, oder vielmehr haben beide parallel stattgefunden und es waren auch zu einem kleinen Teil dieselben Personen beteiligt. Zusammengefasst wurden erspähte Daten aus westlichen Computern in den Osten verkauft. Der Hauptbeteiligte Karl Koch wurde nach mehreren Therapien zur Erholung von seiner Drogensucht und nach Aussagen gegenüber dem Verfassungsschutz im Juni 1989 verbrannt aufgefunden.\n\nInfolge des KGB-Hacks und der Ermittlungsarbeiten durch den Verfassungsschutz wurde besonders im Hamburger Club das Misstrauen unter den eigenen Mitgliedern immer größer. Die nächsten Jahre waren davon geprägt, dass kaum noch große Aktionen angegangen wurden. Dessen ungeachtet wurde weiterhin regelmäßig der jährliche \"Chaos Communication Congress\" ausgerichtet, auch \"Die Datenschleuder\" erschien meist viermal im Jahr, und auf der CEBIT traf man sich jährlich am Chaosdienstag zur „Belagerung“ der Post, später dann der Telekom.\n\nDie politische Wende in Deutschland nach dem Mauerfall nutzte der CCC, um Verbindungen in die damalige DDR zu knüpfen. Zwar hatte der Osten in den späten 80er Jahren stark in der Computertechnik aufgeholt (was hauptsächlich auf Nachbauten von Westcomputern zurückzuführen war), jedoch waren der Zugang und die Beschaffung von West-Technik durch die CoCom-Liste untersagt geblieben oder unerschwinglich teuer.\n\nSchon im Februar 1990 wurde eine „Hacker-Wiedervereinigung“ unter dem Namen \"KoKon\" („Kommunikationskongress“; die Anlehnung an CoCom war durchaus beabsichtigt) im Haus der jungen Talente im Berliner Osten ausgerichtet. Die zweitägige Veranstaltung wurde vom \"Computer Club im HdjT\" zusammen mit dem \"Chaos Computer Club\" organisiert. Infolgedessen wurde ein neuer CCC Berlin gegründet, der sich in den Wirren der Wiedervereinigung einen Clubraum in Berlin-Mitte, zwischen Friedrichstraße und Reichstag gelegen, ergattern konnte. Außerdem wurde beim Aufbau eines ersten Datenkommunikationsnetzes in der ehemaligen DDR mitgewirkt.\n\nAufgrund diverser Meinungsverschiedenheiten, insbesondere mit dem Stammclub in Hamburg, entwickelten sich Anfang der 90er Jahre immer mehr regionale Gruppen des CCC, die jedoch oft nicht zur Zusammenarbeit mit Hamburg zu bringen waren. Neben der schon angesprochenen Neugründung in Berlin gab es einen CCC in Oldenburg, in Lübeck (der zeitweilig die Herausgabe der \"Datenschleuder\" koordinierte), und eine Gruppe in Ulm. In Bielefeld entstand bereits 1987 auf Initiative der Künstler Rena Tangens und padeluun der dem CCC nahestehende, aber auf andere Schwerpunkte ausgerichtete Verein Digitalcourage (ehemals FoeBuD), der heute die Big Brother Awards ausrichtet und in Bereichen des Datenschutzes und der Überwachung mit dem CCC zusammenarbeitet.\n\nDazu kommt eine ganze Reihe an kleinen Clubs, die an Orten entstanden, in die es ehemalige Mitglieder der großen Clubs verschlug. Dazu zählen die Gruppen in Köln oder Heidelberg.\n\nAls problematisch wurde die Dezentralisierung nie empfunden, da in der Hackerethik die Förderung der Dezentralität als wichtiges Ziel betont wird. Selbst die 1986 verabschiedete Satzung des CCC e. V. sah die Gründung von eigenständigen Erfahrungsaustausch-Kreisen (Erfa-Kreisen) vor. Lediglich heute noch vorkommende „Rivalitätskämpfe“, wie zwischen Hamburg und Berlin, beeinflussten die Produktivität und führten dazu, dass manche Mitstreiter dem CCC vollends den Rücken kehrten.\n\nZur Zeit des beginnenden Internetbooms war das Geschäft mit technisch schlecht beratenen Personen besonders gut. Auf diesen Zug sprangen Personen wie Sönke Ungerbühler auf. Als vorgebliches Mitglied des CCC behauptete er gegenüber Vorstandsmitgliedern von Banken und Wirtschaftsunternehmen, dass er durch Hacking auf brisante Informationen gestoßen sei, die für die Presse ein gefundenes Fressen seien. Gegen Zahlung von mehreren Tausend DM würde er jedoch schweigen und das aufgedeckte Material übergeben. Die Treffen wurden meistens in London, Cambridge oder Brüssel vereinbart, wo Ungerbühler dann einen Satz leere Disketten überreichte. Aus Angst vor Rufschädigung wurden diese Betrugsfälle selten angezeigt. So trieb Ungerbühler lange Zeit im Namen des CCC sein Unwesen, ohne dass der Verein davon wusste. Aus der ersten Haft, die durch einen vorsichtigen Journalisten eingeleitet wurde, konnte Ungerbühler nach London fliehen. Dort kam es zu weiteren Treffen, jedoch konnte ihn ein Sportartikelhersteller zur Übergabe in Deutschland überreden, wo er von der Polizei überwältigt werden konnte. Nach seiner Festnahme berichtete Ungerbühler, dass ihm das Schweigegeld von den verunsicherten Führungskräften teilweise geradezu aufgedrängt worden sei. Außerdem habe Ungerbühler laut eigener Aussage keine Ahnung von Computern.\n\nHäufig im CCC-Umfeld anzutreffen war in dieser Zeit Kim Schmitz alias Kimble, der sich als Sicherheitsberater ausgab. Er beteiligte sich an der Newsgroup de.org.ccc im Usenet und wurde somit oft mit dem CCC in Verbindung gebracht. Nach seiner Verurteilung wegen Betrugsdelikten erhielt er ein bis heute andauerndes Hausverbot zu CCC-Veranstaltungen.\n\nEnde 1997 wurde der Algorithmus COMP128 bekannt, der für die Verschlüsselung des sogenannten Identifikations-Code auf GSM-Karten – in Deutschland nur von Mannesmann Mobilfunk – verwendet wurde. Dadurch wurde es technisch möglich, eine GSM-Karte zu klonen, was der CCC im Frühjahr 1998 bewies.\n\nMit geklonten Karten lassen sich nicht nur Gespräche auf Kosten des ursprünglichen Teilnehmers absetzen, es wird auch mit seiner Identität telefoniert. Eine einmal eingegebene PIN muss kein weiteres Mal eingegeben werden. Insbesondere Händler von GSM-Karten standen somit in Verdacht, die entdeckte Lücke ausnutzen zu können; denn sie hatten ungestörten Zugang zu Karten und den dazugehörigen PINs, da die damals verwendeten Briefe leicht zerstörungsfrei geöffnet und später wieder geschlossen werden konnten.\n\nDas Problem konnte nur durch Umstellung des Verschlüsselungsverfahrens und Austausch der Karte behoben werden. Jedoch sollen laut des Weltmarktführers bei SIM-Karten, \"Schlumberger\", selbst 2002 noch etwa 30 % der im Umlauf befindlichen Karten mit dem anfälligen COMP128-Algorithmus ausgestattet gewesen sein.\n\n1999 fand in Altlandsberg das erste Chaos Communication Camp mit etwa 1.500 Personen und dem bislang weltweit größten zivilen Freiluft-LAN statt.\n\nOstern 2001 kehrte der CCC mit dem ersten Easterhegg in das Eidelstedter Bürgerhaus zurück. 2002 kamen die jährlich stattfindende Gulaschprogrammiernacht (GPN) des Karlsruher Erfa-Kreises und die \"Intergalaktische Club-Mate Party\" (ICMP) des Erlanger Erfa-Kreises hinzu, ein kleines Hacker-Camp, das alle zwei Jahre in Münchsteinach in der Nähe von Erlangen stattfindet. Seit 2004 veranstaltet der Erfa-Kreis Dresden jährlich die zweitägige Informationsveranstaltung \"Datenspuren\". Seit 2016 veranstaltet der Chaos Computer Club Wien die PrivacyWeek.\n\nIm Jahr 2001 feierte der Club sein 20-jähriges Bestehen mit der interaktiven Lichtinstallation Blinkenlights am Haus des Lehrers auf dem Alexanderplatz in Berlin. Im Oktober 2002 entstand mit dem Fortsetzungsprojekt \"Blinkenlights Arcade\" an der Fassade der neuen Bibliothèque nationale de France in Paris mit 3.370 m² das größte Display aller Zeiten. 2008 wurde eine weitere Fortsetzung in Toronto umgesetzt: \"Blinkenlights Stereoscope\".\n\nIm Herbst 2001 zeigte die Bezirksregierung Düsseldorf mit ihrem Regierungspräsidenten Jürgen Büssow an der Spitze Bestrebungen, unter Berufung auf den Mediendienstestaatsvertrag zumindest in Nordrhein-Westfalen „ungewünschte“ Inhalte im Internet zu sperren. Eine der Aktionen des CCC war im April 2002 die erste von ihm organisierte Straßendemonstration in seiner Geschichte. Etwa 400 Teilnehmer zogen durch die Düsseldorfer Altstadt mit Kundgebung vor dem Schlossturm und Abschlusskundgebung bei einem direkten Gespräch mit Jürgen Büssow vor dem Gebäude der Bezirksregierung. Hierbei wurden ihm eine rote Netzwerkkarte und ein Ausdruck der von der Initiative ODEM erstellten Unterschriftenliste gegen die Netzzensur überreicht.\n\nAm 26. Juli 2004 veröffentlichte der freie IT-Unternehmer Dirk Heringhaus in der \"Datenschleuder\" einen Bericht über Sicherheitslöcher im Auftragsabwicklungssystem OBSOC der Deutschen Telekom. Heringhaus bezeichnete diese Aktion als \"T-Hack\". Es handelte sich um den Zugriff auf abgeänderte URLs, was den Zugang zu geschützten Daten in der OBSOC-Datenbank ermöglichte. Das Problem konnte nur nach anfänglichem Widerwillen der Deutschen Telekom behoben werden. Unter den betroffenen Nutzergruppen waren unter anderem der Bundesnachrichtendienst und die GSG 9.\n\nMitglieder des CCC und die niederländische Stiftung „Wij vertrouwen stemcomputers niet“ („Wir vertrauen Wahlcomputern nicht“) demonstrierten im Dezember 2006, wie leicht sich ein Wahlcomputer der Firma Nedap manipulieren lässt. Auswirkungen des Hacks waren erhöhte Sicherheitsmaßnahmen bei mehreren Wahlen in Deutschland, ein Verzicht der Stadt Cottbus und weiterer kleiner Gemeinden, Wahlcomputer zu kaufen, und der Entzug der Zulassung für Wahlcomputer der Firma SDU sowie der Firma Nedap in den Niederlanden. Aufgrund des Hacks wurde Anfang 2007 in Deutschland eine Wahlprüfungsbeschwerde beim Bundesverfassungsgericht eingereicht. Dazu führte der CCC in Zusammenarbeit mit der Stiftung „Wij vertrouwen stemcomputers niet“ nach einer Anfrage des Gerichts eine Untersuchung durch, die im Mai 2007 veröffentlicht wurde. Die Untersuchung fasst die bereits aufgedeckten Mängel zusammen, beschreibt neue Angriffsszenarien und rät von der Verwendung von Nedap-Wahlcomputern ab. Mit dem Urteil vom 3. März 2009 erklärte das Bundesverfassungsgericht den Einsatz der Wahlcomputer bei der Bundestagswahl 2005 für verfassungswidrig.\n\nAm 23. Januar 2008 wies der Staatsgerichtshof Hessen den Antrag des CCC auf einstweilige Anordnung zurück und ließ die Wahlcomputer für die hessischen Landtagswahlen zu. Nach der Landtagswahl in Hessen am 27. Januar 2008 legte der CCC beim Staatsgerichtshof Hessen einen Einspruch gegen die Wahl ein. Dazu wurde aufgrund von vorgezogenen Neuwahlen in der Sache nicht mehr entschieden. Ab der Landtagswahl vom 18. Januar 2009 wurde vom hessischen Innenministerium keine Genehmigung zum Einsatz der Wahlcomputer mehr erteilt.\n\nIm Zuge der Kampagne gegen Wahlcomputer wurden auch zwei Sicherheitslücken beim sogenannten Hamburger Wahlstift veröffentlicht.\n\nEnde März 2008 veröffentlichte der CCC in seiner Mitgliederzeitschrift einen angeblichen Fingerabdruck von Innenminister Wolfgang Schäuble. Dies geschah aus Protest gegen die geplante Ausweitung der Verwendung von biometrischen Daten, z. B. im sogenannten E-Pass. Der CCC wollte damit deutlich machen, wie leicht der eigene Fingerabdruck „gestohlen“ und von anderen Personen benutzt werden könne. Im vorliegenden Fall seien die Fingerabdrücke von einem Wasserglas abgenommen worden, aus dem Wolfgang Schäuble bei einer Podiumsdiskussion getrunken haben soll.\nSchäuble kommentierte den Bericht: „Mein Fingerabdruck ist kein Geheimnis, den kann jeder haben“.\n\nChaos macht Schule ist eine seit 2007 bestehende Initiative mehrerer Erfakreise des CCC, die mit verschiedenen Bildungsinstitutionen zusammenarbeiten. Ziel dieser Initiative ist es, Schüler, Eltern und Lehrer in den Bereichen Medienkompetenz und Technikverständnis zu stärken.\n\nDer Datenbrief ist eine Forderung des Chaos Computer Clubs zur Verbesserung des Datenschutzes. Das Konzept sorgte vor allem Anfang 2010 für Diskussionen in der Politik.\n\nAnfang Oktober 2011 veröffentlichte der Chaos Computer Club eine technische Analyse einer angenommenen Version einer staatlichen Spionagesoftware, eine der untersuchten Varianten wurde in einem Ermittlungsverfahren in Bayern verwendet. In der Analyse kam der CCC zu dem Schluss, dass die verfassungsrechtlich vorgeschriebenen Befugnisse in vielerlei Hinsicht weit überschritten worden seien. So sei es z. B. entgegen früheren Beteuerungen möglich, Daten auf dem infizierten System zu verändern oder angeschlossene Geräte (Mikrofon, Kamera) für einen „großen Lauschangriff“ zu nutzen. Am 10. Oktober bestätigte der zuständige bayerische Innenminister Joachim Herrmann, dass die Software aus Bayern stammt und dort vom Landeskriminalamt eingesetzt wurde. Diese Aktion könnte jedoch ein strafrechtliches Nachspiel haben. „Insgesamt erscheint es nicht ausgeschlossen, dass die Veröffentlichung des Quellcodes eines sogenannten staatlichen Trojaners als Tathandlung einer Strafvereitelung gemäß Paragraf 258 Strafgesetzbuch angesehen wird.“\n\nAm 7. November 2017 veröffentlichte der Chaos Computer Club eine technische Analyse der bei verschiedenen Wahlen eingesetzten Software PC-Wahl 10. Bei der Analyse wurden mehrere gravierende Sicherheitsmängel festgestellt. Bemängelt wurden der Updatemechanismus, die Sicherheit des Updateservers und die Anfälligkeit des Exports der Wahlergebnisse.\n\nBekanntere Ehrenmitglieder des Chaos Computer Clubs sind:\n\nIm CCC und Umfeld sind drei Logos/Symbole anzutreffen:\n\n\n"}
{"id": "10767", "url": "https://de.wikipedia.org/wiki?curid=10767", "title": "Chaos Communication Congress", "text": "Chaos Communication Congress\n\nDer Chaos Communication Congress ist ein mehrtägiges, in Deutschland stattfindendes Treffen der internationalen Hackerszene, das vom Chaos Computer Club (CCC) ausgerichtet und organisiert wird. Der Kongress widmet sich in zahlreichen Vorträgen und Workshops technischen und gesellschaftspolitischen Themen. 2018 haben 17.000 Menschen am Congress teilgenommen.\n\nDer Kongress findet einmal im Jahr statt, von der Gründung 1984 an bis 2004 vom 27. bis zum 29. Dezember, seit 2005 vom 27. bis zum 30. Dezember. Als Abkürzung dient die laufende Nummer der Veranstaltung, ergänzt um den Zusatz C3 (für „Chaos Communication Congress“, Details s. u.).\n\nNachdem der Kongress, der seit 1984 im Eidelstedter Bürgerhaus in Hamburg stattfand, für diesen Ort zu groß geworden war, zog er 1998 nach Berlin ins Haus am Köllnischen Park, wo er von mehr als 4.000 Teilnehmern besucht wurde. Von 2003 bis 2011 fand der Kongress im Berliner Congress Center am Alexanderplatz statt. 2012 wurde er ins Congress Centrum Hamburg (CCH) verlegt. Im Dezember 2013 besuchten ihn über 9.000 Teilnehmer. Das gesteigerte Interesse wurde der Globalen Überwachungs- und Spionageaffäre und den Veröffentlichungen Edward Snowdens zugerechnet. 2017 zog der Kongress in die Leipziger Messe, da das CCH wegen Sanierungsarbeiten nicht mehr zur Verfügung stand.\n\nDer Kongress bemüht sich um möglichst niedrige Eintrittspreise – vor allem, um Jugendlichen die Teilnahme zu ermöglichen. 2011 kostete ein Ticket für die gesamte Kongressdauer 80 Euro, für Jugendliche unter 18 Jahren 25 Euro. Ein großer Teil der Referenten rekrutiert sich aus der Szene selbst, und die organisatorische Arbeit vor Ort wird von freiwilligen Helfern geleistet, die im CCC-Jargon als Engel bezeichnet werden.\n\nTeil des Kongresses ist das \"Hackcenter\", ein großes Areal, in dem die verschiedenen regionalen Gruppierungen des Clubs mit ihrer Technik auf dem Kongress präsent sind. Das Hackcenter und die anderen Bereiche der Veranstaltung sind über eine breitbandige Anbindung mit dem Internet verbunden. Der CCC legt aber Wert auf die Feststellung, dass es sich beim Hackcenter nicht um eine LAN-Party handelt, sondern um ein „Hands-On“-Labor zum gemeinsamen Erforschen und Testen von Netzwerktechnologie. Mit dem Umzug ins Congress Centrum Hamburg im Jahr 2012 wurde das Hackcenter um so genannte \"„Assemblies“\" erweitert, abgegrenzte Bereiche außerhalb der Vortragssäle, die verschiedenen sozialen Gruppierungen und Projekten als Präsentationsfläche dienen konnten.\n\nVon 1997 bis 2004 wurde während des Kongresses gleichzeitig auch die \"Deutsche Meisterschaft im Lockpicking\" ausgetragen.\n\nDas Streaming der Vorträge wurde die letzten Jahre von der Forschungsgemeinschaft elektronische Medien e. V. (FeM) umgesetzt. Durch die hohe Nachfrage war 2007 und 2008 die 1-Gbit/s-Anbindung der TU Ilmenau zu 99 Prozent ausgelastet. Beim Wechsel des Chaos Communication Congress nach Hamburg (29C3), konnten die Rechnerressourcen und Bandbreite anderer Studentennetzwerke in Deutschland genutzt werden. Außerdem erhielt man Unterstützung aus dem CCC-Umfeld. Im Jahr darauf wurde innerhalb des CCC das Video Operation Center (VOC) ins Leben gerufen, welches unterstützt wurde. Seit dem 31C3 hat das VOC die Leitung übernommen und wird von der FeM sowie der ags (TU Braunschweig) unterstützt.\n\nStatt einer Creative-Commons-by-nc-nd-Lizenz für 32C3-Videos wurden die meisten 33C3-Videos mit einer CC-by-Lizenz publiziert.\n\nIm Rahmen des Chaos Communication Congress 2009 verschafften sich Unbekannte vermutlich illegal Zugang zu internen Daten der Singlebörse Ma-flirt.de, zudem wurden öffentlich zugängliche Profilfotos von der Singlebörse Harzflirt.de heruntergeladen. Zu diesen Daten wurden Download-Links im Kongress-Wiki des CCC erstellt. Passwörter wurden dort direkt veröffentlicht. Ein älterer Hack der Kundendatenbanken des Bekleidungshändlers Thor Steinar wurde ebenfalls bekanntgemacht. Zu den veröffentlichten Daten gehörten Profilnamen, Passwörter, E-Mail-Adressen, Ortsangaben, Wohnadressen, Fotos und Umsatzzahlen. Zwei der Seiten stehen im Ruf, von Mitgliedern der rechten Szene frequentiert zu werden, die anderen nicht. Der Verein wollte zu den Datenschutzverletzungen keine Stellungnahme geben. Kongressteilnehmer distanzierten sich jedoch von dem Vorfall. Er stehe im Widerspruch zur Hackerethik.\n\nDer Chaos Communication Congress steht fast jedes Jahr unter einem Motto, meist in Bezug auf wichtige Themen des zurückliegenden oder kommenden Jahres.\n\nSeit dem Chaos Communication Congress 1999 hat sich eine abkürzende Schreibweise eingebürgert. Anstatt \"16. Chaos Communication Congress\" wurde der Begriff \"16C3\" verwendet. Die offizielle Schreibweise des Chaos Communication Congress 2004 lautet somit \"„21C3: The Usual Suspects“ 21. Chaos Communication Congress\". Der 20. Kongress 2003 hingegen bildete mit dem Kürzel NaN eine zum Motto passende Bezeichnung.\n\n\n"}
{"id": "11690", "url": "https://de.wikipedia.org/wiki?curid=11690", "title": "Knoppix", "text": "Knoppix\n\nKnoppix ist eine freie GNU/Linux-Distribution, deren Hauptanwendungsbereich im Live-Betrieb liegt. Sie wird von Klaus Knopper entwickelt, von dessen Namen sich die Benennung ableitet. Knoppix basiert auf der Distribution Debian, die Auswahl an Software wird aus Debians Entwicklungszyklen \"stable\" und \"testing\" zusammengestellt.\n\nKnoppix liegt gelegentlich Computerzeitschriften bei und wurde vom deutschen Bundesamt für Sicherheit in der Informationstechnik unterstützt und verteilt.\n\nKnoppix war die erste Live-CD-Distribution, die eine große Popularität erlangte.\n\nIn der Version 5.2, die auf der CeBIT 2007 vorgestellt wurde, sind erstmals der 3D-Fenstermanager Beryl sowie verschiedene Virtualisierungslösungen integriert.\n\nDie auf der CeBIT 2008 vorgestellte Version 5.3 enthält erstmals \"Adriane Audio Desktop\", ein Sprachausgabe-unterstütztes Desktop-System, das sich vor allem an blinde Anwender richtet.\n\nVersion 6.0 wurde komplett neu geschrieben, sie bekam ein neues Bootsystem und ist um einiges ressourcensparender.\nAuch die Softwareauswahl hat sich geändert, so kommt anstelle von KDE jetzt LXDE als voreingestellte Desktop-Umgebung zum Einsatz. Im August 2011 ist Knoppix 6.7 erschienen, die auf Debian 6.0.0 alias Squeeze basiert\n\nDen Sprung zur Version 7.x im Jahr 2012 begründet Klaus Knopper u. a. mit einer kompletten Umstellung des Boot-Systems und der Umstellung des Systems vom reinen 8-Bit-ISO-Encoding auf UTF-8.\nAb Version 7.01 kam als weitere Neuheit die „zram-RAM-Kompression“ hinzu.\n\nAuf der \"DELUG-DVD\" des Linux-Magazins 04/2016 wurde Version 7.7 veröffentlicht, welche eine Sicherheitslücke in der \"GNU C Library\" („Glibc“) behebt und den verwendeten Linux-Kernel auf Version 4.4 aktualisiert.\n\nDas Grundprinzip eines Live-Systems ist zunächst der gänzliche Verzicht auf eine Installation; die benötigten Komponenten des Systems werden stattdessen in den Arbeitsspeicher geladen. So kann neben der gängigen Nutzung beispielsweise schon vorab getestet werden, ob die vorhandene Hardware zu der jeweiligen Linux-Kernel-Version kompatibel ist. Andere spezielle Anwendungsgebiete sind Notfall- sowie Diagnosearbeiten am Computer, etwa im Falle einer Virusinfektion oder eines Hardwaredefekts. Auch generelle Sicherheitsaspekte können eine Rolle spielen, da ein Betriebssystem, welches von einem nicht-beschreibbaren Medium aus gestartet wird, höchstens für die Dauer einer Sitzung kompromittiert werden kann (beim nächsten Start also wieder in den Ursprung zurückversetzt ist).\n\nKnoppix führt eine ausführliche Hardware-Erkennung durch, die auch Braille-Geräte mit einschließt. Ein eventuell auf der Festplatte installiertes Betriebssystem bleibt standardmäßig unangetastet, Knoppix kann jedoch auch konventionell installiert werden. Auch ist es möglich, im Live-Betrieb eine Swap-Partition auf einem beschreibbaren Datenträger festzulegen. Wenn genügend Arbeitsspeicher vorhanden ist, kann das System auch komplett darin zwischengelagert werden, was besonders schnelle Reaktionszeiten ermöglicht. Ferner lässt sich Knoppix auch komplett auf einen USB-Stick kopieren, sodass dieser anstelle der CD zum Einsatz kommt. Das ab Version 3.8 unterstützte Overlay-Dateisystem UnionFS wurde mit Version 5.1.0 wegen Problemen durch das Derivat „aufs“ (Another UnionFS) ersetzt. Damit können die schreibgeschützten Inhalte der CD mit dem laufenden System gemischt und somit verändert oder ergänzt werden. So ist beispielsweise die Installation zusätzlicher Software auf dem Live-System möglich.\n\nBestandteile sind unter anderem:\nIm Installationspaket sind außerdem über 200 weitere Programme verfügbar.\n\nKnoppix bietet sich durch seine gute Hardware-Erkennung und den in neueren Versionen modularen Aufbau zum Erzeugen weiterer Derivate und Distributionen an. Knoppix-Derivate sind in der Regel Live-CDs:\n\n\"* Nicht als Download verfügbar\"\n\"** auch als LPD-Edition verfügbar\"\n\"*** Erschienen auf der DELUG-DVD 04/2016 bzw. 04/2017 bzw. 07/2018, als 32-Bit und 64-Bit Version\"\n\n\n\n"}
{"id": "11691", "url": "https://de.wikipedia.org/wiki?curid=11691", "title": "Fli4l", "text": "Fli4l\n\nfli4l (flexible internet router for linux, früher floppy isdn for linux) ist eine seit 2000 aktiv weiterentwickelte Linux-Distribution, deren Hauptaufgabe das Bereitstellen eines Routers ist. Die Distribution ist von Diskette lauffähig und wurde mit dem Ziel einer einfachen Konfiguration und der Unterstützung von älterer Hardware geschaffen. fli4l kann zwischen Ethernet und ISDN, UMTS oder DSL bzw. zwischen zwei Ethernet-Netzwerken routen.\n\nfli4l basiert auf dem Linux-Kernel. Die Dokumentation ist sehr umfangreich. Es werden für die Installation keine Linuxkenntnisse benötigt. Grundkenntnisse der Netzwerktechnik sollten jedoch vorhanden sein.\n\nDie Hardwareanforderungen für fli4l sind gering, ein Pentium mit MMX-Unterstützung, 64 MiB RAM und (je nach Konfiguration) ein bis zwei Netzwerkkarten reichen völlig aus. Eine Festplatte wird nicht benötigt, kann aber von fli4l auch verwendet werden. Ab Version 3.0.0 bietet fli4l zudem die Möglichkeit, eine lauffähige Installation mittels Card-Reader direkt auf einer CF-Karte zu erzeugen, die dann ihrerseits mit einem CF-Adapter im IDE-Slot betrieben werden kann. Ebenfalls unterstützt wird die direkte Erzeugung eines ISO-Abbildes zum Betrieb von CD sowie DOC/DOM für eingebettete Systeme.\n\nAb der Version 3.2 werden durch den Standardkernel 2.6.16.56 usb2serial-Adapter, Intel- und Ralink-WLAN-Karten unterstützt.\n\nAb der Version 3.6 werden UMTS und IPv6 unterstützt, wobei der Linux-Kernel 2.6.32 eingesetzt wird.\n\nDie Version 3.10 baut auf der Linux-Kernelreihe 3.16 auf und bringt neben der verbesserten Hardware-Unterstützung vor allem Verbesserungen in der Firewall-Konfiguration und IPv6-Unterstützung mit.\n\nfli4l kann unter Linux, Unix und Windows über Textdateien konfiguriert werden. Darüber hinaus bietet fli4l die Möglichkeit, Verbindungen über das Internet oder im WLAN mittels OpenVPN oder PPTP (Poptop) zu verschlüsseln.\n\nEs gibt zwei Entwicklungszweige von fli4l, eine stabile und eine Entwicklerversion.\n\nDie stabile Version 3.10.x basiert auf einem Kernel der 3.16er-Serie und beinhaltet auch Unterstützung für virtualisierte Systeme auf Basis von Xen und KVM mit. USB und WLAN werden weitgehend unterstützt, ebenso aktuellere Embedded-Hardware wie APU, ALiX, Soekris und die Epia-Reihe.\n\nDie 4.0-Entwicklerversion (Tarball Version) stellt Linux-Kernel der 4.1er-Serie zur Verfügung. Des Weiteren implementiert sie eine von Grund auf neu entwickelte Verwaltung und Konfiguration von Verbindungen (sogenannten Circuits), mit denen u. a. mehrere WAN-Anbindungen (z. B. via DSL, ISDN oder UMTS) parallel betrieben werden können.\n\nfli4l baut auf ein modulares System über sogenannte OPT-Pakete. Nur die Softwarepakete für gewünschte Optionen werden heruntergeladen und in den Verzeichnisbaum zur Erstellung der individuellen Konfiguration entpackt. Zur Erweiterung der Grundfunktionen steht eine breite Palette von Anwendungen in der sogenannten OPT-Datenbank zur Verfügung, so können beispielsweise optionale Pakete zur Volumenbeobachtung eingebunden werden oder ein fli4l kann als Druckserver fungieren. Auch eigene Entwicklungen sind durch dieses Verfahren leicht möglich und können über die Datenbank anderen Nutzern zugänglich gemacht werden.\n\nEin fli4l-Router kann über ein Webinterface gesteuert und überwacht werden. Darüber hinaus steht als Alternative das Programm ImonC (ISDN Monitor Client) für Windows und für Linux (GTK) zur Verfügung, das ebenfalls eine umfangreiche Steuerung zulässt und auch ein ferngesteuertes Update der Routersoftware ermöglicht.\n\nDie PC-Welt bietet seit 2010 ein Konfigurationsprogramm für fli4l zum kostenlosen Download an, mit dem sich eine Bootdiskette auf PC mit Windows-Betriebssystem erstellen lässt.\n\n\n"}
{"id": "11698", "url": "https://de.wikipedia.org/wiki?curid=11698", "title": "CAD", "text": "CAD\n\nCAD (von engl. \"computer-aided design\" [], zu Deutsch rechnerunterstütztes Konstruieren) bezeichnet die Unterstützung von konstruktiven Aufgaben mittels EDV zur Herstellung eines Produkts (Beispielsweise Auto, Flugzeug, Bauwerk, Kleidung).\n\nWelche Tätigkeiten unter den Begriff CAD fallen, wird in der Literatur verschieden behandelt. In einem engeren Sinn versteht man unter CAD das rechnerunterstützte Erzeugen und Ändern des geometrischen Modells. In einem weiteren Sinn versteht man darunter sämtliche rechnerunterstützten Tätigkeiten in einem Konstruktionsprozess, einschließlich der geometrischen Modellierung, des Berechnens, des Simulierens und sonstiger Informationsgewinnung und Informationsbereitstellung, von der Konzeptentwicklung bis zur Übergabe an die Herstellung bzw. Fertigung (Arbeitsvorbereitung).\n\nVerwendete man anfangs CAD-Anwendungen primär für die Herstellung von Fertigungs- bzw. Herstellungsunterlagen (Marketingbezeichnung: CAD als Computer aided Drafting/Draughting), wurden mit zunehmender Rechnerleistung CAD-Systeme mit komplexen Expertensystemen und integrierten FEM-Lösungen für den Entwurf und die Konstruktion technischer Lösungen verfügbar, wobei die Objekte von vornherein als dreidimensionale Körper behandelt werden (dreidimensionales CAD). Allenfalls erforderliche Technische Zeichnungen lassen sich aus den virtuellen Modellen dreidimensionaler Objekte automatisch herstellen. Ein besonderer Vorteil des 3D-CAD ist die Möglichkeit, von den Objekten eine Abbildung aus beliebiger Richtung zu erzeugen. Der 3D-Drucker ermöglicht den auch im Hobbybereich angewendeten Übergang vom virtuellen Modell zum realen Objekt. Zusammen mit den erfassbaren Materialeigenschaften werden erweiterte CAD-Modelle zur Beschreibung der physikalischen Eigenschaften (zum Beispiel Festigkeit, Elastizität) der Objekte erstellt.\n\nCAD ist ein Teil der sogenannten CAx-Technologien, zu denen auch die Computerunterstützte Fertigung zählt (Computer-aided manufacturing (CAM)) oder die computerunterstützte Qualitätssicherung (Computer-aided quality (CAQ)). Genutzt wird CAD in fast allen Zweigen der Technik: Architektur, Bauingenieurwesen, Maschinenbau, Elektrotechnik und all deren Fachrichtungen und gegenseitige Kombinationen bis hin zur Zahntechnik.\n\nCAD dient dem Erzeugen von digitalen Konstruktionsmodellen unterschiedlicher Ausprägung, welche die Informationen bereitstellen, aus denen das gewünschte Produkt hergestellt werden kann (via Techn. Zeichnung, NC-Weitergabe etc.). Der Vorteil der rechnerinternen Darstellung des Modells besteht hierbei im Rationalisieren des Konstruktionsprozesses. Die Funktionalitäten des CAD zielen etwa darauf ab, dem Konstrukteur Routinetätigkeiten (zum Beispiel durch vielfältige Nutzung des 3D- oder Feature-Modells oder durch das automatische Schraffieren oder Bemaßen von techn. Zeichnungen) oder wiederholende Arbeitsvorgänge abzunehmen, Modelle auch in fortgeschrittenen Phasen des Konstruktionsprozesses leicht und schnell ändern zu können (bspw. mittels Parametrik) oder Informationsverlusten und Fehlern vorzubeugen. Die erzeugten CAD-Modelle können dabei vielfältig in anderen Anwendungen weiterverwendet werden (bspw. in Simulations- oder Berechnungsverfahren oder als \"Digital Mock-Up\"), und die Ergebnisse aus diesen Anwendungen wiederum das CAD-Modell verändern, wodurch die Konstruktion laufend optimiert (und damit die Produktqualität gesteigert) wird, bis das Produkt herstellungsreif ist.\n\nIm Bereich der virtuellen Produktentwicklung unterstützt CAD, als Teil der CAx-Technologien., aus eher technischer Sicht insbesondere den Aufbau von Prozessketten (CAD-CAM), die integrierte Modellierung (zum Beispiel mittels \"Features\"), den durchgängigen Informationsfluss auf Grundlage einer einheitlichen Datenbasis \"(Digital master)\", das Modellieren vollständiger Produktmodelle \"(Virtuelles Produkt)\" und aus eher organisatorischer Sicht das schnellere Bereitstellen von Informationen (Beispielsweise via Parametrik und 3D-Modellierung), insbesondere in frühen Phasen des Entwicklungsprozesses \"(Frontloading)\" durch verteilte und parallele Arbeitsweisen \"(Simultaneous-/Concurrent engineering)\"\n\nEin weiterer Anwendungsbereich ist die Modellierung von Fertigungsverfahren mit CAD-Software. Dies ermöglicht einen kostengünstigen und vergleichsweise schnellen Einblick ohne Halbzeugverbrauch und Maschinenbelegung zum verwendeten Fertigungsverfahren.\n\nDas CAD bedient sich verschiedener Werkzeuge, welche in Ursprung in verschiedenen Bereichen haben, wie insbesondere dem geometrischen Modellieren (welches wiederum auf bspw. die Differentialgeometrie, Mengenlehre, Matrixalgebra, Graphentheorie zurückgreift) oder der theoretischen und angewandten Informatik (zum Beispiel Software Engineering, Datenstrukturen). Diese Werkzeuge lassen sich kombinieren und sind je nach CAD-System und Branche unterschiedlich stark ausgeprägt. Teilweise sind sie auch nicht integraler Bestandteil eines CAD-Systems, sondern können als (branchenspezifische-) Erweiterung \"(Add-on, Plug-in)\" installiert werden, wobei die Datenbasis für das Modell die gleiche bleibt.\n\nDie CAD-Systeme sind dabei vektororientiert (Gegenteil: Rasterorientierung), da sich so alle geometrischen Objekte auf Linien und Punkte zurückführen und vollständig charakterisieren lassen.\n\nBei der 2D Modellierung werden geometrische Elemente in einer Ebene, überwiegend in Form von Schnitten und Ansichten von Bauteilen gezeichnet.\nDie Arbeitsweise ähnelt hierbei der wie bei einer händischen Zeichnung. Man wählt den gewünschten Befehl aus (Beispielsweise „Linie zeichnen“), wählt die erforderlichen Punkte zur Modellierung des Objekts in der Modellumgebung aus und das Programm erstellt das gewünschte Objekt. Dabei kommen insbesondere Linien (Geraden, Strecken), Freiformkurven \"(Splines)\", Kreise/Kreisbögen und Punkte zum Einsatz, welchen weitere Attribute zugewiesen werden können wie bspw. Strichdicke, Strichart (Beispielsweise gestrichelt, strich-punktiert, punktiert) oder Farbe.\n\nDes Weiteren können auf diese geometrischen Objekte unterschiedliche Transformationen (Translation, Skalierung, Rotation, Spiegelung, Scherung etc.) angewendet oder davon Äquidistanten \"(offset curve)\" abgeleitet werden. Sie können ebenso getrimmt oder als geschlossene Linienzüge („Polylinie“) gefast, abgerundet oder mit einer Farbe oder mit einem Muster gefüllt werden. In die Modellumgebung lassen sich auch Texte, Symbole oder externe Bilddateien (bspw. eingescannte Handskizzen, welche als Vorlage dienen) in die Modellumgebung einfügen.\n\nDie 2D-Modellierung wird vor allem zur Erzeugung von Volumina eingesetzt, welche durch bestimmte Operationen aus dem 3D-Bereich (Extrudieren, \"Sweeping\", Rotation usw.) aus zweidimensionalen Geometrieelementen erstellt wurden. Weiters findet die 2D-Modellierung in jenen Bereichen Anwendung, wo zur Darstellung und Erklärung eines Bauteils ein 2D-Modell ausreicht bzw. wo die 3D-Modellierung in Relation zum Nutzen zu aufwendig wäre oder auch als Ergänzung zu Techn. Zeichnungen, welche aus einem 3D-Modell erzeugt wurden. Weiters findet sie Anwendung beim Zeichnen von Grafiken von technischen Details eines Bauteils oder bei bloß schematischen Darstellungen von Konstruktionen (bspw. zur Erklärung von Konzepten, Überlegungen, Anweisungen an die Produktion).\n\nBei der 3D-Modellierung werden geometrische Objekte in einer dreidimensionalen Form aufgebaut und gespeichert. Dadurch erlauben sie einerseits eine realitätsnahe Darstellung und bessere räumliche Vorstellung des Körpers bei und nach der Modellierung, andererseits lassen sich durch die Dreidimensionalität bestimmte darstellungsbezogene (Beispielsweise Darstellung von Schnitt- und Ansichtsdarstellungen aus unterschiedlichen Blickwinkeln), unterlagenbezogene (Techn. Zeichnungen, Stücklisten, Arbeitspläne, Ersatzteilkataloge, Montage- und Bedienungsanleitungen) und technisch-visuelle Darstellungen (Kollisionsbetrachtung, Explosionsdarstellungen, Zusammenbau-, Einbau-, Montageuntersuchungen) vom System (teil-) automatisieren. Weiters ist die 3D-Beschreibung eines Objekts Voraussetzung für viele andere Anwendungen in und außerhalb des CAD-Systems (DMU, MKS, CFD, FEM, NC-Bearbeitung, Virtuelle Realität, Rendering, 3D-Druck etc.) und unterstützt damit den Aufbau von Prozessketten (insb. CAD-CAM-Prozesskette) in der virtuellen Produktentwicklung. Diesem Vorteil der größeren Anwendungsbreite von 3D-Modellen steht allerdings ein höherer Konstruktionsaufwand, ein entsprechend umfangreiches Wissen und Übung mit den Modellierwerkzeugen gegenüber.\n\nDie 3D-Modelle lassen sich, ähnlich wie im 2D-Bereich, mittels verschiedener räumlicher Operationen transformieren (Translation, Skalierung, Rotation etc.) oder deformieren (Verjüngung, Verdrehung, Scherung, Beulen).\n\nDabei kommen im CAD insbesondere folgende rechnerinternen Repräsentationsarten vor:\n\n\nBei der direkten Modellierung \"(explicit modeling\") werden die geometrischen Elemente direkt über bestimmte Funktionen (Skalieren, Verschieben, Dehnen etc.) verändert. Die geometrischen Elemente enthalten nur feste Werte (und keine Variablen), welche erst durch das Anwenden von Funktionen geändert werden können. Hierbei wählt man das geometrische Element und die entsprechende Funktion, welche die gewünschte Änderung hervorrufen soll, aus und verändert das Objekt entweder interaktiv mit der Maus oder über Koordinateneingabe mittels Tastatur. Verändert werden dabei nur die gewählten geometrischen Elemente. Es bestehen (im Unterschied zum parametrischen Modellieren) keinerlei dauerhaften Abhängigkeiten zwischen den geometrischen Elementen, wodurch ein sehr intuitives und freies Ändern der Geometrie möglich ist.\n\nAufgrund dieser sehr freien Modellierungsmöglichkeit wird die direkte Modellierung vor allem in der frühen Konzeptphase eingesetzt, wo ein schnelles und unkompliziertes Ändern der Geometrie (ohne Verstehen der „Entstehungschronologie“ des parametrischen Modells bzw. Suchen des „richtigen“ Parameters, der die gewünschte Änderung hervorruft) wünschenswert ist. Weiters kann das direkte Modellieren bei Änderung von importierten („fremden“), parametrisierten CAD-Modellen aufgrund bspw. unklarer bzw. nicht nachvollziehbarer Chronologie oder durch Dateiformatübertragungsfehler hilfreich sein. Dabei wird durch Markieren und anschließendes Ziehen, schieben oder ähnliches der gewünschten Flächen oder Kanten das Modell angepasst und die Änderung in den Chronologie-Baum eingepflanzt (zum Beispiel \"Synchronous Technology\" bei NX oder \"Live Shape\" bei CATIA).\n\nUnter parametrischer Modellierung versteht man das Steuern des Modells mittels Parametern. Das heißt, dass – anders als bei der direkten Modellierung – das Modell nicht direkt über seine Geometrie, sondern über seine Parameter angesprochen wird, welche das Modell jederzeit ändern können. Die Arten der Parameter unterscheiden sich je nach Anwendung grob in Geometrieparameter (zum Beispiel geometrische Maße, Positionen), physikalische Parameter (Beispielsweise Werkstoffe, Lasten), Topologieparameter, Prozessparameter (zum Beispiel Toleranzen, Daten für Wärmebehandlungen oder NC-Verfahrwege). Dadurch dass diese Parameter systemintern gespeichert werden, lassen sich Beziehungen und Abhängigkeiten zwischen ihnen herstellen. Dies wird über Restriktion bzw. Zwangsbedingungen \"(Constraints)\" umgesetzt. Hierbei werden die eingegebenen oder auch aus einem externen Programm (Beispielsweise Tabellenkalkulationsprogramm) automatisierend verknüpften Werte „gehalten“, wodurch ihre Abhängigkeiten und Beziehungen (mittels mathematischer Berechnungsverfahren) berechnet werden können \"(constraint-based design)\". Diese Restriktionen können bspw. Maße, algebraische Zusammenhänge (Länge = 2 × Breite), logische Operationen (Wenn Länge > 5 cm, dann Breite = 8 cm, sonst 12 cm) sein („explizite Restriktionen“) oder auch die Horizontalität, Parallelität oder Kongruenz („implizite Restriktionen“) von geometrischen Elementen festlegen. Dadurch ist es möglich ein intelligentes Modell aufzubauen, welches mit „Wissen“ in Form von Konstruktionsregeln und nur wenigen maßgeblichen Werten gesteuert werden kann („wissensbasierte Parametrik“).\n\nIm zweidimensionalen Bereich erfolgt die parametrische Modellierung über Bemaßungen und Restriktionsanzeigen, welche die Parameter repräsentieren und mit der Geometrie assoziativ verbunden sind \"(dimension-driven geometry)\". Der Benutzer fängt zunächst an die Geometrie grob zu zeichnen. Das System versucht dabei (mittels regel-basierten Verfahren) die Konstruktionsabsicht \"(Design Intent)\" durch Anbringen von impliziten Restriktionen zu erkennen, welche durch eine Restriktionsanzeige in Piktogrammform angezeigt werden. Anschließend kann der Benutzer individuell Bemaßungen (mit den Werten) an die Geometrie anbringen bis die Geometrie vollständig durch Parameter definiert ist. Die Geometrie lässt sich nun nurmehr über die Parameter ändern.\n\nMit der parametrischen Modellierung können auch Normteile oder auch ganze Baugruppen aus einer Bibliothek in die Modellumgebung eingefügt werden, wobei die Veränderbarkeit der zugrundeliegenden Parameter erhalten bleibt.\n\nAufgrund der vielfältigen Beziehungen und Abhängigkeiten bei der parametrischen Modellierung hat sich eine Konstrutionsmethodik entwickelt, bei der auf eine saubere, fehlerfreie Parametrierung und ein logischer Aufbau der CAD-Modelle in seiner geometrischen Konstruktion durch den Benutzer bzw. Konstrukteur zu achten ist.\n\nHier kommen Datenstrukturen zur Anwendung, die den Erzeugungsverlauf des Modells aufzeichnen. Für den Benutzer wird dies in einem Chronologiebaum \"(History tree)\" dargestellt, der während der Modellierung laufend aktualisiert wird und in dem die einzelnen Modellierschritte und der Aufbau des Modells eingesehen und bei Bedarf in jeder Phase des Konstruktionsprozesses verändert werden können.\n\nSo werden in der Chronologie bestimmte Abhängigkeiten („Eltern-Kind-Beziehungen“) dargestellt, die auf den Erzeugungsweg des Modells schließen lassen, wie beispielsweise die für eine Extrusion zugrundeliegende 2D-Zeichnung („Skizze“) oder der für eine Durchdringung zugrundeliegende Volumenkörper. Die jeweiligen Elemente sind hierbei assoziativ miteinander verbunden, das heißt, ändert sich das zugrundeliegende Element (zum Beispiel die 2D-Zeichnung), ändert sich das darauf aufbauende Element (zum Beispiel der aus der Extrusion erstellte Volumenkörper) automatisch mit.\n\nBei der Baugruppenmodellierung werden getrennt gespeicherte CAD-Modelle („Einzelteile“) durch Referenzieren zu einem ganzheitlichen Modell („Baugruppe“) zusammengebaut, wobei die so erzeugte Datei nur Verweise zu den Modellen und keine Geometrie enthält. Hierbei lassen sich die Einzelteile zueinander in Beziehung bringen (zum Beispiel mittels Abstandsangaben zu Flächen oder Punkten). Ein Baugruppenstrukturbaum erleichtert hierbei die Übersicht.\n\nDer Vorteil der Baugruppenmodellierung ist insbesondere die Gesamtdarstellung des Produkt mit seinen Einzelteilen, und dient der Überprüfung von Kollisionen und des Zusammenbaus \"(Packaging)\" oder auch visueller Inspektionen (Beispielsweise „Fly-Through-Analyse“) oder Kinematikanalysen.\n\nUnter Feature-basierter Modellierung versteht man das Verwenden von Features zu Konstruktionszwecken in der Modellumgebung. Im CAD stellen Features hierbei Werkzeuge dar, mit denen konstruktive Aspekte als Einheit in das Modell implementiert und (mittels Parametrik) manipuliert werden können. Sie besitzen einen über die reine Geometrie hinausgehenden höheren Informationsgehalt (Beispielsweise zu technologischen, fertigungstechnischen oder qualitätsbezogenen Aspekten) auf den andere Anwendungen (zum Beispiel CAM, FEM, CAPP) zugreifen können, was zu einem höheren Automatisationsgrad der Prozesse führt und Features zu „Informations- und Integrationsobjekten“ im gesamten Produktentwicklungsprozess macht.\n\nDie Features können hierbei sowohl geometrischer \"(Form Features)\" als auch/oder semantischer Natur sein, können eine Vielzahl an Produktinformationen in sich vereinen und weisen daher in Art und Umfang ihrer Repräsentation eine erhebliche Flexibilität und Varietät auf, weswegen sie auch in allen Branchen eingesetzt werden können. Beispiele hierfür sind etwa Bohrungen mit zusätzlichen Toleranzinformationen im Maschinenbau oder mehrschichtige Wandbauten mit sämtlichen Materialkennwerten in der Architektur. Diese hohe Flexibilität bringt allerdings auch den Nachteil der schwierigen Übertragung bzw. Konvertierung von Features von einem Programm in ein anderes mit sich. Hierbei können das Kategorisieren von Features \"(Feature taxonomy)\", das Übertragen von einzelnen Features mittels bestimmter „Mapping-Techniken“ \"(Feature mapping)\" oder das Speichern des Features aus verschiedenen konstruktiven oder fertigungstechnischen Blickwinkeln als integriertes Modell \"(Multiple-View-Feature modeling)\" helfen, sind aber derzeit noch Gegenstand der Forschung.\n\nFeatures können auf 3 verschiedene Arten erzeugt werden. Aufgrund der Verbindung zwischen Features und Parametrik lassen sich alle Features im Nachhinein über ihre Parameter ändern.\n\n\nDie Makrotechnik dient im CAD dazu, oft verwendete Geometrie oder Features mit nur wenigen Eingaben zu erzeugen, wobei das Makro nach Erstellung der Objekte aufgelöst wird. Es lässt sich also im Nachhinein nicht mehr feststellen, ob ein Objekt via Makro erzeugt wurde. Man unterscheidet dabei zwischen zwei Arten:\n\n\nBei der Variantenprogrammierung wird das Modell mittels systemeigener oder höherer Programmier- wie C++, Python, Fortran oder Skriptsprachen wie Visual Basic for Applications, oder AutoLISP erzeugt. Der Benutzer schreibt hierbei (optional mit Unterstützung von Dienstprogrammen) in einem Texteditor die gewünschten Modellierschritte zur Erzeugung des Modells. Das CAD-System liest und führt die Prozeduren aus und erstellt das Ergebnis in der Modellumgebung. Möchte man das Modell ändern, werden die entsprechenden Änderungen prinzipiell im Text (und nicht in der Modellumgebung) vorgenommen und das Modell anschließend vom System neu berechnet und geändert. Das Modell kann aber auch als diskretes (das heißt mit festen Werten ausgestattetes) Modell in die Modellumgebung eingepflanzt werden und mittels direkter Modellierung oder parametrischer Modellierung (durch „Nachparametrisieren“) verändert werden. Weiter bieten einige Systeme „visuelle Programmiersprachen“ (zum Beispiel Rhino 3D in Kombination mit Grasshopper) an, mit denen geometrische Modelle, algorithmisch und parametrisiert, ohne Programmierkenntnisse erstellt werden können.\n\nCAD-Systeme verfügen noch über weitere Fähigkeiten, die den Benutzer beim Modellieren unterstützen. Ein wesentliches Element der CAD-Systeme ist das Ansichtsfenster \"(viewport)\", in dem das Modell bildlich dargestellt wird. So ist es in ihm möglich, das Modell unter verschiedenen Projektionsarten (beispielsweise axonometrisch, perspektivisch) und aus verschiedenen Entfernungen \"(Zoom)\" zu betrachten, zu verschwenken \"(Pan)\" oder auch zu drehen. Dabei kann der Benutzer in nur einem Ansichtsfenster oder in mehreren Ansichtsfenstern gleichzeitig arbeiten. Das Objekt kann hierbei in jedem der einzelnen Ansichstsfenster modelliert werden, wobei diese aber hinsichtlich Projektionsart oder graphischer Darstellung getrennt voneinander gesteuert werden können (zum Beispiel eine schattierte Darstellung in einem und eine Drahtgitterdarstellung in einem anderen Ansichtsfenster). Ebenso ist es möglich, 3D-Schnittdarstellungen der Geometrie oder Modellauschnitte in einem Sichtrahmen bei gleichzeitiger Unterdrückung der Geometrie außerhalb des Sichtrahmens anzeigen zu lassen \"(clipping).\"\n\nUm geometrische Objekte im Raum leichter bewegen und positionieren zu können, werden verschiedene Hilfsmittel eingesetzt wie bspw. Koordinatensysteme (zum Beispiel einerseits karteschisches oder auch Polarkoordinatensystem und andererseits Welt- und Arbeitskoordinatensystem), Objektfänge (mit denen bspw. Endpunkte, Kreismittelpunkte oder Tangentenpunkte von bereits bestehenden Geometrien erkannt werden können, auf die eingerastet werden kann), das Ausrichten an temporären „Spurlinien“ in bestimmten Winkelabständen oder an einem vordefinierten Raster.\n\nWeiters gibt es einige Techniken zum Organisieren des Modells. Eine davon ist die Ebenentechnik („Layertechnik“). Dabei können unterschiedliche Objekte, wie bspw. Bemaßungen, konstruktive Objekte, Texte usw. kategorisiert werden, um das Modell einerseits übersichtlich zu halten und andererseits die Objekte bspw. später bei Bedarf ein-/ausblenden zu können (zum Beispiel weil sie nur Hilfsgeometrien waren) oder ihnen die gleichen Attribute (wie beispielsweise die gleiche Strichart oder Farbe) zu geben oder auch zu sperren/entsperren, damit sie bspw. während des Modellierungsprozesses nicht verändert oder ausgewählt werden können. Eine weitere Art des Organisierens ist das Zusammenfassen von Objekten zu einer Gruppe, um für alle in der Gruppe enthaltenen Objekte die gleichen Operationen wie zum Beispiel Transformationen durchführen zu können.\n\nEine wesentliche Funktion des CAD ist das Herstellen von Unterlagen zur Herstellung/Fertigung des Produkts (zum Beispiel Technische Zeichnungen, Stücklisten, Montagepläne) und zur Dokumentation und Archivierung. Hierzu werden in einer eigenen Zeichenblattumgebung über Ansichtsrahmen (welche die Verbindung zum Modellbereich herstellen), die gewünschten Modellansichten oder Schnitte (inkl. Projektions- und Darstellungsart, Maßstab etc.) auf dem Zeichenblatt positioniert.\n\nBei parametrischen Modellen ist die in der Zeichenblattumgebung erzeugte, abgeleitete 2D-Darstellung mit dem zugrundeliegenden Modell uni- oder bidirektional assoziativ verbunden, das heißt, dass bspw. Änderungen im Modellbereich automatisch im Zeichnungsbereich wirksam werden. Darüber hinaus lassen sich bei parametrischen Modellen unter anderem Strichstärken, Schraffuren und Bemaßungen vom System automatisch generieren. In der Regel sind aber auch bei diesen aus 3D-Modellen abgeleiteten Darstellungen gewisse zweidimensionale Nacharbeiten erforderlich, um eine normgerechte Techn. Zeichnung zu erstellen, wie bspw. eine nur symbolhafte bzw. abstrahierte Darstellung bestimmter Teile, die unter bestimmten Maßstäben die Zeichnung überladen würden oder die nicht notwendigerweise dreidimensional modelliert werden mussten.\n\nIm Falle der direkten Modellierung wird die Zeichnung schon im Modellbereich „gezeichnet“ (inkl. der Attribute). Bemaßungen, Schriftfelder, Texte und ähnliches können entweder im Modellbereich oder in der Zeichenblattumgebung eingefügt werden.\n\nDie im Rechner hergestellte Zeichnung kann dann anschließend gedruckt (bis zum Papierformat A3) bzw. geplottet (ab dem Papierformat A2) oder auch (bspw. als PDF) gespeichert werden.\n\nMittels bestimmter Verfahren lassen sich im Rechner konstruierte Modelle direkt aus den 3D-CAD-Daten (ohne Arbeitsvorbereitung) als Ganzes und in einem Verfahrensgang als reale (physische) Objekte schichtweise herstellen, um sie bspw. als Funktionsmuster, Anschauensmodell, Prototyp oder sogar als Urform zu verwenden. Dabei wird die Oberfläche des 3D-CAD-Modells in Dreiecksflächen umgewandelt („Triangulation“) und als STL-Datei gespeichert. Nach der Definition der Schichtdicke \"(Slicing)\", nach der das physische Modell aufgebaut wird, wird es hergestellt und anschließend, wenn erforderlich, einer Nachbearbeitung oder Reinigung unterzogen.\n\nMittels bestimmter Visualisierungen kann eine bessere Darstellung und Vorstellung über das Produkt wiedergegeben werden, sie können zu Präsentations- oder Werbezwecken verwendet werden oder zur Vermeidung von Verständnisproblemen beitragen. Hierzu ist es neben der Visualisierung mittels Konzeptgraphiken möglich, eine bestimmte Ansicht des 3D-Modells photorealistisch darzustellen \"(Rendering)\". Dazu werden etwa bestimmte Licht- (Beispielsweise diffuses Flächen-, Punkt- oder Richtungslicht) und Projektionseinstellungen (zum Beispiel Projektionsart, Entfernung vom Objekt) vorgenommen oder, wenn nicht schon in der Modellumgebung gemacht, Material dem Objekt (inkl. Textur, Lichtdurchlässigkeit, Mappingart etc.) zugewiesen. Über globale Beleuchtungsmodelle, welche das Licht mit all diesen Einstellungen berechnen (zum Beispiel Raytracing), wird je nach Hardwarestärke und Auflösung unterschiedlich schnell schrittweise eine Szene „gerendert“, welche anschließend als Rastergraphik in einem Graphikformat (beispielsweise BMP oder JPG) gespeichert werden kann.\n\nSystembedingt können beim Datenaustausch nicht alle Informationen übertragen werden. Während reine Zeichnungselemente heute kein Problem mehr darstellen, ist der Austausch von Schriften, Bemaßungen, Schraffuren und komplexen Gebilden problematisch, da es keine Normen dafür gibt. Selbst auf nationaler Ebene existieren in verschiedenen Industriezweigen stark unterschiedliche Vorgaben, was eine Normierung zusätzlich erschwert.\n\nDie meisten Programme setzen auf ein eigenes Dateiformat. Das erschwert den Datenaustausch zwischen verschiedenen CAD-Programmen, weshalb es Ansätze zur Standardisierung gibt. Als Datenaustauschformat für Zeichnungen und zur Archivierung von Unterlagen wird heute üblicherweise das Format DXF des Weltmarktführers Autodesk verwendet.\n\nEs ist zwischen CAD-systemneutralen und CAD-systemspezifischen Datenformaten zu unterscheiden. Wesentliche CAD-systemneutrale Datenformate sind VDA-FS, IGES, SAT, IFC und STEP sowie für spezielle Anwendungen die STL-Schnittstelle.\nDie Datenformate im Einzelnen:\n\n\nMit den CAD-systemneutralen Formaten gelingt in der Regel nur die Übertragung von Kanten-, Flächen- und Volumenmodellen. Die Konstruktionshistorie geht in der Regel verloren, damit sind die übertragenen Daten in der Regel für eine Weiterverarbeitung nur bedingt geeignet.\nCAD-systemspezifische Datenformate ermöglichen die Übertragung der vollständigen CAD-Modelle, sie sind jedoch nur für wenige Systeme verfügbar.\n\nFür die Weitergabe von PCB-Daten zur Erstellung von Belichtungsfilmen für Leiterplatten hat das sogenannte Gerber-Format und das neuere Extended Gerber-Format große Bedeutung (siehe Fotografischer Film).\n\nCAD-Programme gibt es für zahlreiche verschiedene Anwendungsfälle und Branchen. Anders als bei Officelösungen gibt es im Bereich des CAD starke Spezialisierungen. So existieren oftmals nationale Marktführer in Bereichen wie Elektrotechnik, Straßenbau, Vermessung usw. Siehe dazu die Liste von CAD-Programmen und die Liste von EDA-Anwendungen.\n\nEin weiteres Anwendungsgebiet ist der Entwurf von elektronischen Schaltungen. Entsprechende Programme werden oft auch unter den Begriffen eCAD und EDA zusammengefasst, insbesondere bei Anwendungen im Chipentwurf, dem Leiterplattenentwurf, der Installationstechnik und der Mikrosystemtechnik.\n\nWegen der besonderen Anforderungen haben sich Spezialbereiche mit teilweise stark unterschiedlichen Entwicklungsmethoden gebildet. Dies gilt insbesondere für den computerbasierten Chipentwurf, das heißt die Entwurfsautomatisierung (EDA) für analoge oder digitale Integrierte Schaltkreise, zum Beispiel ASICs. Damit verwandt ist das Design von programmierbaren Bausteinen wie Gate-Arrays, GALs, FPGA und anderen Typen programmierbarer Logik (PLDs) unter Benutzung von zum Beispiel VHDL und Abel. Die automatisierte Layouterstellung bei integrierten Schaltkreisen wird oft als Layoutsynthese bezeichnet.\n\nBei der Entwicklung von Leiterplatten findet zuerst der Entwurf der Schaltung in Form eines Schaltplans statt, gefolgt vom rechnergestützten Layoutentwurf.\n\nAuch in der klassischen Installationstechnik existieren zahlreiche Anwendungsbereiche für Software, insbesondere bei Hausinstallationen für Industrie oder öffentliche Gebäude oder der Entwurf und die Umsetzung von SPS-basierten Steuerungsanlagen.\n\nIm Bereich der Mikrosystemtechnik besteht eine besondere Herausforderung darin, Schaltungsdaten mit den mechanischen Produkt-Konstruktionsdaten (CAD) zusammenzuführen und mit solchen Daten direkt Mikrosysteme herzustellen.\n\nDer Begriff „Computer-Aided Design“ entstand Ende der 50er Jahre im Zuge der Entwicklung des Programmiersystems APT, welches der rechnerunterstützten Programmierung von NC-Maschinen diente.\n\nAm MIT in Boston zeigte Ivan Sutherland 1963 mit seiner \"Sketchpad\"-Entwicklung, dass es möglich ist, an einem computergesteuerten Radarschirm interaktiv (Lichtstift, Tastatur) einfache Skizzen (englisch \"Sketch\") zu erstellen und zu verändern.\n\n1965 wurden bei Lockheed (Flugzeugbau, USA) die ersten Anläufe für ein kommerzielles CAD-System zur Erstellung technischer Zeichnungen (2D) gestartet. Dieses System, CADAM (Computer-augmented Design and Manufacturing), basierend auf IBM-Großrechnern und speziellen Bildschirmen, und mit hohen Kosten verbunden, wurde später von IBM vermarktet und war, zumindest im Flugzeugbau, Marktführer bis in die 1980er Jahre. Es ist teilweise in CATIA aufgegangen. Daneben wurde eine PC-basierende Version von CADAM mit dem Namen HELIX entwickelt und vertrieben, das aber praktisch vom Markt verschwunden ist.\n\nAn der Universität Cambridge, England, wurden Ende der 1960er Jahre die ersten Forschungsarbeiten aufgenommen, die untersuchen sollten, ob es möglich ist, 3D-Grundkörper zu verwenden und diese zur Abbildung komplexerer Zusammenstellungen (zum Beispiel Rohrleitungen im Chemieanlagenbau) zu nutzen. Aus diesen Arbeiten entstand das System PDMS (Plant Design Management System), das heute von der Fa. Aveva, Cambridge, UK, vermarktet wird.\n\nEbenfalls Ende der 1960er Jahre begann der französische Flugzeughersteller Avions Marcel Dassault (heute Dassault Aviation) ein Grafikprogramm zur Erstellung von Zeichnungen zu programmieren. Daraus entstand das Programm CATIA. Die Mirage war das erste Flugzeug, das damit entwickelt wurde. Damals benötigte ein solches Programm noch die Leistung eines Großrechners.\n\nUm 1974 wurden B-Spline-Kurven und -Flächen für das CAD eingeführt.\n\nDie 1980er Jahre waren bestimmt von der mittleren Datentechnik der 32-bit-Superminirechner und der Workstations (Digital Equipment Corporation, Prime Computer, Data General, Hewlett-Packard, Sun Microsystems, Apollo Computer, Norsk Data, etc.), auf denen CAD-Pakete wie MEDUSA und CADDS beispielsweise von ComputerVision liefen. Auch ME10 und I-DEAS sind zu nennen.\n\nNachdem in der zweiten Hälfte der 1980er Jahre die ersten Personal Computer in den Unternehmen standen, kamen auch CAD-Programme dafür auf den Markt. In dieser Zeit gab es eine Vielzahl von Computerherstellern und Betriebssysteme. AutoCAD war eines der ersten und erfolgreichsten CAD-Systeme, das auf unterschiedlichen Betriebssystemen arbeitete. Um den Datenaustausch zwischen diesen Systemen zu ermöglichen, definierte AutoDesk für sein CAD-System AutoCAD das DXF-Dateiformat als „neutrale“ Export- und Importschnittstelle. 1982 erschien AutoCAD für das Betriebssystem DOS. Das Vorgehen bei der Konstruktion blieb jedoch beinahe gleich wie zuvor mit dem Zeichenbrett. Der Vorteil von 2D-CAD waren sehr saubere Zeichnungen, die einfach wieder geändert werden konnten. Auch war es schneller möglich, verschiedene Versionen eines Bauteils zu zeichnen.\n\nIn den 1980er Jahren begann wegen der sinkenden Arbeitsplatzkosten und der besser werdenden Software ein CAD-Boom. In der Industrie wurde die Hoffnung gehegt, mit einem System alle anstehenden Zeichnungs- und Konstruktionsaufgaben lösen zu können. Dieser Ansatz ist aber gescheitert. Heute wird für jede spezielle Planungsaufgabe ein spezielles System mit sehr leistungsfähigen Spezialfunktionen benutzt. Der Schritt zur dritten Dimension wurde durch die immer höhere Leistungsfähigkeit der Hardware dann gegen Ende der 1980er Jahre auch für kleinere Unternehmen erschwinglich. So konnten virtuelle Körper von allen Seiten begutachtet werden. Ebenso wurde es möglich, Belastungen zu simulieren und Fertigungsprogramme für computergesteuerte Werkzeugmaschinen (CNC) abzuleiten.\n\nSeit Anfang der 2000er Jahre gibt es erste Ansätze, die bis dahin immer noch zwingend notwendige Zeichnung verschwinden zu lassen. In die immer öfter vorhandenen 3D-Modelle werden von der Bemaßung über Farbe und Werkstoff alle notwendigen Angaben für die Fertigung eingebracht. Wird das 3D-Modell um diese zusätzlichen, geometriefremden Eigenschaften erweitert, wird es zum Produktmodell, unterstützt beispielsweise durch das STEP-Datenformat. Die einzelnen einheitlichen Volumenobjekte werden zu Instanzen unterschiedlicher Klassen. Dadurch können Konstruktionsregeln und Verweise zwischen einzelnen Objekten (zum Beispiel: Fenster wird in Wand verankert) realisiert werden.\n\n\n"}
{"id": "11711", "url": "https://de.wikipedia.org/wiki?curid=11711", "title": "Red Hat Linux", "text": "Red Hat Linux\n\nRed Hat Linux [] (RHL) war für mehrere Jahre eine der bekanntesten Linux-Distributionen. Sie wurde von der Firma Red Hat zusammengestellt. Im September 2003 wurde sie eingestellt und ging im Fedora-Projekt und in Red Hat Enterprise Linux auf. Als direkter Nachfolger ist Fedora anzusehen, welches seine erste Version auf der Basis von Red Hat \"Linux 9\" entwickelte.\n\nDie Version 1.0 wurde am 3. November 1994 veröffentlicht. RHL ist damit nicht ganz so alt wie Slackware, jedoch älter als die meisten anderen Distributionen. Für RHL wurde das \"RPM\"-Paketformat entwickelt, das heute neben dem DEB-Paketformat von den meisten kommerziellen Distributionen zur Verteilung von Softwarepaketen verwendet wird. Verschiedene andere Distributionen begannen ihre Geschichte ursprünglich als Zweig von Red Hat Linux, so z. B. das desktoporientierte Mandriva Linux (ursprünglich nicht mehr als ein „Red Hat Linux mit KDE“), Yellow Dog Linux (eigentlich ein „Red Hat Linux mit PowerPC-Unterstützung“), Red Flag Linux, Aurox Linux oder ASPLinux.\n\nRed Hat Linux wurde traditionell ausschließlich innerhalb von Red Hat entwickelt, das einzige Feedback von Benutzern waren deren Bug-Reports oder Beiträge zu den enthaltenen Open-Source-Softwarepaketen. Dies änderte sich am 22. September 2003, als Red Hat seine Privatkunden-Aktivitäten und damit Red Hat Linux in das communitybasierte Fedora-Projekt einbrachte. Dieser Schritt legte die Grundlage für ein offenes, eng mit der Community verzahntes Entwicklungsmodell einer neuen Distribution, die zukünftig die Basis der weiteren Produkte von Red Hat darstellen wird. Dabei wurde gleichzeitig beschlossen, nur noch eine Enterprise-Version der kommerziellen Distribution anzubieten und Red Hat Linux zugunsten von Red Hat Enterprise Linux auf der einen und Fedora auf der anderen Seite einzustellen.\n\nRed Hat Linux wurde mit einem grafischen Installer mit dem Namen \"Anaconda\" installiert, der auch für Einsteiger leicht bedienbar ist. Für die Konfiguration der integrierten Firewall gab es das eingebaute Tool \"Lokkit\". Das Desktop-Design war beherrscht vom viel gelobten als auch kritisierten Desktop Theme \"Bluecurve\", das ein einheitliches Look and Feel unter Gnome und KDE ermöglichen sollte.\n\nDer 1995 für Red Hat Linux 2.0 entwickelte \"RPM Package Manager\", der ursprünglich \"Red Hat Package Manager\" hieß, war für alle nachfolgenden RHL-Versionen für die Paketverwaltung zuständig. RPM wurde von zahlreichen anderen Linux-Distributionen, wie z. B. SUSE, Mageia oder Mandriva Linux übernommen und ist heute Teil des LSB-Standards. Auch einige Unix-Systeme wie z. B. IBM AIX oder Solaris sowie nicht-unixoide Systeme wie Novell NetWare nutzten RPM.\n\nSeit Red Hat Linux 8.0 war Red Hat dazu übergegangen, nur noch Software einzubinden, die vollständig frei ist. Aus diesem Grund fehlten zum Beispiel die Fähigkeiten MP3-Dateien abzuspielen oder auf NTFS-Partitionen zuzugreifen, da diese patentierte Verfahren beinhalteten.\nEine weitere Besonderheit stellte die mit Red Hat Linux 8 erfolgte Umstellung auf UTF-8 dar. Red Hat Linux war damit eine der ersten Distributionen, die konsequent auf diese Zeichenkodierung und Unicode setzte.\n\nRed Hat Linux prägte indirekt, durch seine Innovationen, die Weiterentwicklung von vielen anderen Linux-Distributionen jener Zeit maßgeblich.\n\nDie Release-Daten beziehen sich auf die öffentlichen Ankündigungen in der Newsgroup comp.os.linux.announce, und nicht auf die tatsächliche Verfügbarkeit im Handel. Für mehr Informationen über die Versionsnamen siehe Fedora- und Red-Hat-Versionsnamen.\n\nEnde April 2004 beendete Red Hat den Support für alle Red Hat Linux-Versionen, um sich vollständig auf den Unternehmensbereich und RHEL zu konzentrieren. Aus der Code-Basis von RHL 9 war 2003 Fedora Core 1 entstanden. Die weitere kostenfreie Versorgung der bestehenden RHL-Versionen mit Patches stellte das Fedora-Projekt durch \"Fedora Legacy\" bis 2006 sicher. Daneben bot Red Hat nach dem Erwerb einer \"Progeny\"-Lizenz Support für RHL 8 und 9 bis ins Jahr 2011.\n\n"}
{"id": "11713", "url": "https://de.wikipedia.org/wiki?curid=11713", "title": "OpenSUSE", "text": "OpenSUSE\n\nopenSUSE [], ehemals \"SUSE Linux\" und \"SuSE Linux Professional\", ist eine Linux-Distribution des Unternehmens SUSE LLC. Sie wird insbesondere in Deutschland (knapp 26 Prozent Anteil an der weltweiten Nutzerbasis) und den USA (knapp 14 Prozent der weltweiten Nutzerbasis) verbreitet eingesetzt. Der Fokus der Entwickler liegt darauf, ein stabiles und benutzerfreundliches Betriebssystem mit großer Zielgruppe für Arbeitsplatzrechner und Server zu erschaffen.\n\nDie SUSE Linux GmbH (heute eine Tochter der SUSE LLC) vertrieb in den frühen 1990er Jahren unter der Firmierung \"S.u.S.E. GmbH\" (Gesellschaft für Software- und Systementwicklung GmbH) die Linux-Distribution Slackware, in der bereits die Eigenentwicklung YaST als Konfigurationsprogramm angeboten wurde. Im Mai 1996 wurde erstmals die eigenständige auf Basis von jurix entwickelte Distribution \"S.u.S.E. Linux\" mit der Versionsnummer 4.2 veröffentlicht. Die Nummerierung bezieht sich auf die Zahl 42 aus dem Roman Per Anhalter durch die Galaxis, die darin als Antwort auf die Frage „nach dem Leben, dem Universum und dem ganzen Rest“ gewertet wird.\n\nMit der Version 5 erschien im Juni 1997 eine für Großkunden optimierte Version von S.u.S.E. Linux als sogenanntes \"Business-Linux-Produkt\", das längere Releasezyklen und einen erweiterten Support anbot, und dessen Konzept durch SUSE Linux Enterprise Server (SLES) weitergeführt wird. S.u.S.E. Linux wurde neben der Intel-80386-Plattform ab der Version 6.1 auch auf die DEC-Alpha-AXP-Plattform, ab Version 6.3 auf die PowerPC-Plattform portiert. Im weiteren Verlauf kamen Versionen für AMD-Athlon-64-, Intel-Itanium- und IBM-S390-(Z-Series)-Systeme hinzu.\n\nFür Endbenutzer wurden in den Versionen 7.0 bis einschließlich 9.1 drei Produktlinien angeboten:\nIm November 2003 übernahm das amerikanische Softwareunternehmen Novell die SUSE Linux GmbH, womit die Bezeichnung der Distribution ab der Version 9.1 von \"S.u.S.E.\" zu \"SUSE\" abgewandelt wurde. Mit der Übernahme gingen weitere Innovationen wie die Installationsmöglichkeit des Betriebssystems über das Internet via FTP, Softwareangebote für 64-Bit-Systeme (AMD64- und Intel-64-CPUs) und die Freigabe von YaST für die GNU General Public License, einher. Zu denen von Novell eingeführten Neuerungen gehörte außerdem die stärkere Gewichtung des Gnome-Desktops gegenüber dem K Desktop Environment, die gleichberechtigt als Vorauswahl für die Desktop-Umgebung angeboten wurden.\n\nFür SUSE 9.2 wurden erstmals umfassende ISO-Abbilder der Distribution zum Download angeboten und die Personal-Version wurde eingestellt. Die ermäßigte Campusversion und das preiswertere Update-Paket wurden noch bis einschließlich Version 9.3 vertrieben. Ab Version 10.0 wurden die verschiedenen Produktlinien gänzlich eingestellt. Ab Version 11.2 ist KDE Plasma Desktop wieder die Vorauswahl für die Desktop-Umgebung. Gnome wird aber weiterhin angeboten.\n\nMit der Schaffung des openSUSE-Projekts wurde die Entwicklung von SUSE Linux öffentlich gemacht, wodurch Nutzer auch die Alpha- und Beta-Versionen von SUSE Linux testen und gefundene Fehler in einem öffentlichen Bugtracker melden können. Weiterhin kann man sich im Rahmen des Projekts auch aktiv an der Entwicklung beteiligen, indem man Patches zu bestehenden Paketen beisteuert oder am Schreiben der openSUSE-Dokumentation mitwirkt.\n\nDie erste unter Mitwirkung dieses Projekts entstandene finale Version konnte entweder ohne jegliche proprietäre Software als vollständige Open-Source-Distribution heruntergeladen werden (\"SUSE Linux OSS 10.0\") oder aber im Bündel mit proprietärer Software wie dem Adobe Reader und Flash Player sowie MP3-Spielern heruntergeladen oder gekauft werden (\"SUSE Linux 10.0 Eval\").\n\nSeit dem 11. Mai 2006 wurde das „OSS“ im Namen entfernt. Ferner werden inzwischen nur noch Versionen veröffentlicht, die ausschließlich freie Software und Treiber enthalten. Proprietäre Software wurde jedoch bis einschließlich Version 11.0 weiterhin in einem separaten Verzeichnis mitgeführt.\n\nMit der Veröffentlichung der Version 10.2 am 7. Dezember 2006 wurde der Name der Distribution von \"SUSE Linux\" in \"openSUSE\" geändert, um den Einfluss des Projekts widerzuspiegeln und Verwechslungen mit den kommerziellen Ablegern zu vermeiden.\nAm 27. April 2011 wurde Novell von dem amerikanischen Softwareunternehmen Attachmate übernommen, wobei einige Patente von Novell an ein von Microsoft geführtes Konsortium mit dem Namen CPTN Holdings veräußert wurden.\n\nEnde Januar 2014 gab SUSE-Mitarbeiter Michal Hrušecký bekannt, dass sich die von SUSE zur Entwicklung der openSUSE-Distribution angestellten Mitarbeiter zeitweilig anderen Aufgaben innerhalb des openSUSE-Projektes zuwenden werden, wodurch die Veröffentlichung der Version 13.2 von Juli auf November 2014 verschoben wurde. Signifikante Innovationen sind das in SUSE Linux Enterprise etablierte Dateisystem Btrfs/XFS sowie die aktuelle Version von KDE Plasma 5, welches jedoch aus den Repositories nachinstalliert werden muss.\n\nZeitgleich gab das Projekt seine bislang \"Tumbleweed\" genannte Distributions-Variante auf. Der Name blieb aber für das \"Rolling Release\" erhalten, welches aus dem Entwicklungszweig (\"Factory\") durch Snapshots erzeugt wird, die mit openQA getestet worden sind. Anders als bei anderen Distributionen ist \"Tumbleweed\" somit ein getestetes Rolling Release, was der Stabilität sehr entgegen kommt.\nFür die im Herbst 2015 erschienene Version hat sich das Entwicklerteam auf den Namen \"openSUSE Leap\" mit der abweichenden Versionsnummer 42.1 geeinigt. Wie bereits in der openSUSE Version 4.2 aus Mai 1996, die damals als S.u.S.E. Linux bezeichnet wurde, bezieht sich die Zahl 42 auf die Frage nach „nach dem Leben, dem Universum und dem ganzen Rest“ der Buchreihe Per Anhalter durch die Galaxis. Zukünftig soll der Unterbau aus Software-Komponenten bestehen, die das jeweils neueste SUSE Linux Enterprise verwendet; Desktop-Oberflächen und Anwendungen will das Projekt indes aus \"Tumbleweed\", der Rolling Release Ausgabe von openSUSE, übernehmen.\n\nAuf der im Jahr 2016 in Nürnberg abgehaltenen openSUSE-Konferenz wurden Statistiken bekannt gegeben, dass seit der konzeptionellen Neuorientierung mit openSUSE Leap 42.1 steigende Benutzerzahlen zu verzeichnen seien. Demnach liegt die Anzahl der Downloads bei 400.000 DVD-Images pro Monat mit steigender Tendenz. Jeden Monat kämen 1.600 Installationen hinzu, und 500.000 Pakete werden installiert. Die Anzahl der \"Tumbleweed\"-Benutzer liegt bei 60.000, von denen die Hälfte häufig Updates vornehmen. Damit habe sich die Anzahl der \"Tumbleweed\"-Installationen im letzten Jahr verdoppelt.\n\nAndere Erkenntnisse aus den Statistiken sind, dass die meisten Installationen über DVD-Images vorgenommen werden. Die dominierende Architektur ist x64. Die geografische Verteilung der Nutzer hat sich nach diesen Zahlen kaum geändert. Ein Drittel der Benutzer kommt aus Deutschland, 12 % sind in den USA zu finden, 5 % in Russland und 3 % in Brasilien.\n\nEine signifikante Eigenschaft der Distribution ist das hauseigene Installations- und Konfigurationswerkzeug YaST („Yet another Setup Tool“, zu deutsch: „Noch ein weiteres Einrichtungswerkzeug“), das ein zentrales Werkzeug zur Installation, Konfiguration und Administration des Systems darstellt. Das Programm stellt auch eine komfortable Suche nach Paketen zur Verfügung.\n\nDas seit openSUSE 10.2 als Backend für die Paketverwaltung eingesetzte Werkzeug \"zypper\" ermöglicht das Auflösen von Abhängigkeiten, das Installieren und Entfernen von Paketen und die Aktualisierungsverwaltung. Des Weiteren bietet zypper Lösungen von Erfüllbarkeitsproblemen und SAT-Problemen an. Die installierten Repositories in zypper sind synchron mit denen in YaST, da YaST selbst auch auf die libzypp als Paketverwaltungsengine setzt.\n\nIm Vergleich mit anderen Heimnutzer-Distributionen nutzt openSUSE die LSB-Zertifizierung.\n\nDer Build-Service ermöglicht Entwicklern und Benutzern Software für openSUSE zu packen und so fest und automatisiert in openSUSE zu integrieren. Dadurch kann openSUSE eine große Menge an Software und verschiedenen Versionen bereitstellen.\n\nTraditionell ist openSUSE eine KDE-zentrierte Distribution. Dessen auf der Qt-Bibliothek basierende Desktop-Umgebung wurde seit den späten 1990er Jahren standardmäßig als Desktop-Umgebung installiert. Die SUSE Linux GmbH fördert das KDE-Projekt finanziell. Nach der Übernahme SUSEs durch den Softwarekonzern Novell, der zuvor das Unternehmen Ximian (heutiger Name: Xamarin) des Gnome-Gründers Miguel de Icaza aufgekauft hatte, verschob sich die Gewichtung der Entwicklungsarbeit stärker in Richtung Gnome. Ab openSUSE 10.3 standen nun auf jenen Installationsmedien, die beide Plattformen enthalten, Gnome und KDE Plasma Workspaces als gleichwertige Optionen zur Verfügung. Ab Version 11.2 ist KDE wieder die Standardoberfläche. Zudem werden Live-DVDs als Installationsmedium bereitgestellt, die jeweils nur eine dieser beiden Desktop-Umgebungen enthalten. Auf Installationsmedien werden zusätzlich weitere Desktop-Umgebungen wie Blackbox, IceWM, LXDE, LXQt, Openbox, WMaker und Xfce mitgeliefert.\n\nEine weitere Besonderheit stellten die SUSE-Support-Datenbank und die SUSE-Component-Database dar. Beide stellten umfangreiche Informationen und Hilfestellungen für die Installation und Konfiguration des Systems und der entsprechenden Hardware zur Verfügung. Dabei ist vor allen Dingen die Hardware-Datenbank auch von Nutzern anderer Distributionen rege genutzt worden. Beide Mechanismen waren ebenfalls auf den Medien der Distribution enthalten und ergänzten so die Produkt-Dokumentation. Die Supportdatenbank (SDB) wurde in das openSUSE-Wiki integriert.\n\nSeit openSUSE 10.3 bietet die Distribution durch die Migrationshilfe \"instlux\" die Möglichkeit, openSUSE aus einer bestehenden Microsoft-Windows-Installation heraus zu installieren.\n\nZur Qualitätssicherung des kontinuierlich bearbeiteten Entwicklungszweiges wurde seit openSUSE 12.1 ein Test-Framework namens openQA etabliert. Grundlegende Funktionalität, z. B. ob Bootloader, Kernel und Fensterverwaltung starten sowie ob die Standardprogramme wie Browser und Textverarbeitung ordnungsgemäß funktionieren, kann kontinuierlich geprüft werden. Sowohl die automatische Installations- und Testkomponente als auch das Webinterface wurden als freie Software unter der GNU GPL veröffentlicht.\n\n\n\n\n\n\nBekannte Abkömmlinge (\"Derivate\"):\n\n"}
{"id": "11715", "url": "https://de.wikipedia.org/wiki?curid=11715", "title": "Linux From Scratch", "text": "Linux From Scratch\n\nDie erste Version von \"\" wurde am 16. Dezember 1999 veröffentlicht.\n\nDie aktuelle Version 8.3 wurde am 1. September 2018 freigegeben. Zudem gibt es auch eine deutschsprachige Version, die zurzeit bei der Versionsnummer 6.4 steht und am 23. November 2008 veröffentlicht wurde.\n\nDie Anleitung \"\" (\"LFS\") erklärt die Installation eines einfachen Grundsystems, mit dem gerade so gearbeitet werden kann – sie kann jedoch auch als kleiner Distributionsbaukasten betrachtet werden. Die Installation eines vor-kompilierten Linux einer Linux-Distribution ist schneller, da keine oder nur wenige Installationspakete übersetzt werden müssen. Weiterführende Komponenten wie die Installation verschiedener Server oder Desktop-Umgebungen (wie z. B. KDE oder Gnome) werden in der Dokumentation \" LFS\" (kurz \"BLFS\") beschrieben.\n\nDes Weiteren existiert das Projekt \" LFS\" (kurz \"ALFS\"), bei dem versucht wird, die Installation eines LFS, die langwierig und kompliziert ist, zu automatisieren und zu vereinfachen. Es richtet sich an Benutzer, die bereits einige Male durch den LFS und BLFS Installationsprozess durchgegangen sind und eine Automatisierung wünschen. \" LFS\" (kurz \"HLFS\") dagegen setzt sich mit der Absicherung des LFS-Grundsystemes auseinander und befindet sich in der Entwicklung.\n\nDas ursprüngliche LFS-Projekt beschäftigt sich mit der Installation auf einem IA-32-System. Für andere Architekturen wurde das Projekt \"\" (kurz \"CLFS\") ins Leben gerufen, das sich speziell mit den Problemen des inklusive der Kompilierung von headless und eingebetteten Systemen beschäftigt. CLFS unterstützt eine breite Palette von Prozessoren und verwendet weiterführende Techniken, die nicht im ursprünglichen LFS Buch enthalten sind. Dazu gehören cross-build Toolchains und Unterstützung. In der Version 7.0 soll auch in der Grunddokumentation eine 64-Bit-Unterstützung eingebaut werden.\n\nEin weiteres Unterprojekt, \"\" (englisch für „Tipps“), sammelt Kurzrezepte rund um LFS.\n\nEine Live-CD diente als Basissystem, von dem aus ein LFS-System gebaut werden konnte.\n\nDa Linux zum Bauen eines LFS-Systems notwendig ist und die CD außerdem ausgiebig auf die Kompatibilität zu LFS getestet wurde, war es empfehlenswert, mit der CD zu beginnen.\n\nSie enthielt alle Werkzeuge, die zum Übersetzen der Quellen notwendig sind. Ebenso waren das -Buch und der Quelltext auf der CD enthalten. Somit war keine Internetverbindung beim Bau des Systems erforderlich.\n\nAm 30. November 2007 wurde die letzte Live-CD-Version der Reihe 6.3 (mit der Revisionsnummer 2160) veröffentlicht. Am 30. März 2008 wurde das \"LiveCD\"-Projekt mangels Beteiligung eingestellt. Sie kann mittlerweile nicht mehr verwendet werden, da die Programme zu alt zum Kompilieren einiger Quelltexte sind.\n\nNeben \"\" sind auch die folgenden Distributionsbaukästen bekannt:\n\nFolgende Programmquellen sind in der LFS Version 8.2 enthalten:\n"}
{"id": "11717", "url": "https://de.wikipedia.org/wiki?curid=11717", "title": "United Linux", "text": "United Linux\n\nUnited Linux war eine gemeinsame Linux-Distribution, die von Anfang 2002 bis Januar 2004 von den Unternehmen SCO (vormals Caldera International), Conectiva, SuSE Linux AG und Turbolinux entwickelt wurde. Mandrakesoft sollte auch noch beitreten, verweigerte aber die Zusammenarbeit. Red Hat reagierte verärgert, da das Unternehmen erst einen Tag vor der Gründung eine Anfrage erhielt, und bildete die Red Hat Alliance.\n\nUnited Linux sollte Entwicklungsressourcen sparen und eine gemeinsame Basis für Zertifizierungen schaffen. Im Bemühen um einheitliche Standards für die Distribution hielt man sich eng an die „Linux Standard Base“ (LSB), die selbst allerdings andere Ziele verfolgt. Das Produkt richtete sich ausschließlich an Unternehmenskunden und zielte auf den Servereinsatz. United Linux umfasste KDE, Gnome, Compiler, Antivirenprogramm und sehr viele Analysewerkzeuge, aber kein Office-Paket. Als Installationsprogramm diente das YaST2 von SuSE Linux.\n\nDie Distribution darf nicht frei an Dritte verteilt werden. Das Vervielfältigen und Einsetzen innerhalb des Unternehmens kann jedoch beliebig oft erfolgen. Dafür hatten große Softwarehersteller wie zum Beispiel BEA, IBM, Hewlett-Packard und Oracle ihre Produkte für UnitedLinux zertifiziert.\n\nVon SuSE und Novell wurde mitgeteilt „\"dass die Ziele von United Linux erreicht worden seien. Da SCO die Arbeit an dem Projekt unterbrochen hat,\" (siehe dazu auch SCO gegen Linux, Anmerkung der Autoren) \"gebe es keinen Nutzen, an dem Projekt weiter zu arbeiten\"“. Ferner wurde im Januar 2004 SuSE von Novell übernommen, wodurch der wichtigste Partner von UnitedLinux entfiel. Die Distribution wird seit Mitte 2005 nur noch von Turbolinux vertrieben, da Conectiva von Mandrake Linux übernommen wurde. Die Distribution und die Webseite werden seit dem 22. April 2003 nicht mehr aktualisiert. An der Technologiepartnerschaft mit Turbolinux will man jedoch festhalten und für die Produkte, die auf United Linux aufsetzen, mindestens fünf Jahre Support bieten.\n\nMandrake Linux, Conectiva, Turbolinux und Progeny schlossen sich am 16. November 2004 zum kurzlebigen Linux Core Consortium (LCC) zusammen.\n"}
{"id": "11719", "url": "https://de.wikipedia.org/wiki?curid=11719", "title": "CATIA", "text": "CATIA\n\nCATIA (\"Computer Aided Three-Dimensional Interactive Application\") ist ein CAD-System der französischen Firma Dassault Systèmes, das ursprünglich für den Flugzeugbau entwickelt wurde und sich heute in verschiedenen Branchen etabliert hat.\n\nDer Konstruktionsprozess umfasst in CATIA die Erstellung dreidimensionaler Modelle sowie die Ableitung dazugehöriger zweidimensionaler Zeichnungen. Darüber hinaus gibt es zusätzliche Module, die Funktionalitäten wie z. B. DMU-Untersuchungen, Kinematik, Kabelbaumkonstruktion, Composite Design, FEM-Berechnungen und NC-Programmierung bieten.\nDie Funktionalität gliedert sich in Arbeitsumgebungen bzw. Module (\"workbenches\") nach denen die Befehle geordnet sind:\nZwischen den Arbeitsumgebungen kann beliebig gewechselt werden.\n\nDie aktuell etablierte Version 5, wie auch die neue Version 3DEXPERIENCE (ex V6) besitzen eine Oberfläche, die von der Menüführung Windows-basierend ist. CATIA liegt in einer echten 64-Bit-Version vor. Ältere Versionen von CATIA V5 sind auch als 32-Bit-Version und (von der Windows-Version portiert) für diverse UNIX-Plattformen verfügbar.\n\nCATIA wird in der Luftfahrt- (u. a. Boeing, Airbus) und Automobilindustrie (u. a. BMW, Mercedes-Benz wechselt auf NX, Volkswagen-Konzern, Renault, PSA, Jaguar-Landrover, Ford, Toyota, Honda) sowie deren Zulieferern (z. B. Continental AG, Brose, Magna, SMP, SMIA) oder auch bei diversen Formel-1-Rennställen (z. B. Mercedes AMG F1 Team, McLaren Racing, Force India, Sauber F1 Team, Williams F1 in Kombination mit NX, Caterham F1 Team) als Standard-CAD-System eingesetzt. Mit CATIA arbeiten aber auch zahlreiche Unternehmen in anderen Branchen: im Energie- und Transportbereich (z. B. Alstom), in der Medizintechnik (z. B. Lawton), im Maschinen-, Anlagen- und Werkzeugbau (z. B. Schuler, Arburg), im Bauwesen (z. B. Gehry Partners), im Schiffbau (z. B. Meyer Werft) und im Konsumgüterbereich (z. B. L’Oréal) sowie in der High-Tech-Branche (z. B. AMD, Clarion Malaysia).\n\nBeim französischen Flugzeughersteller Avions Marcel Dassault (heute Dassault Aviation) wurde 1969 damit begonnen, mit Hilfe der Computertechnologie interaktiv technische Zeichnungen zu erstellen. Bei der Entwicklung des Alpha Jet kamen diese Techniken erstmals zum Einsatz. Im Jahr 1974 kaufte Dassault eine der ersten Lizenzen des CAD-Programms CADAM (Computer Augmented Design And Manufacturing) von der Firma Lockheed, das bis dorthin nur ein zweidimensionales Programm war. Zu diesem CAD-Programm wurde 1978 von Dassault mit der Entwicklung eines dreidimensionalen Werkzeugs begonnen, das zunächst CATI (Conception Assistée Tridimensionelle Interactive) genannt und 1981 in CATIA (Computer Aided Three-dimensional Interactive Application) umbenannt wurde.\n\nEs gab Bestrebungen, diese Eigenentwicklung auch an andere Firmen zu verkaufen. Daher wurde am 5. Juni 1981 die Konzerngesellschaft Dassault Systèmes gegründet. Mit IBM schloss \"Dassault Systemes\" ein Vertriebsabkommen für CATIA ab. Die erste CATIA-Version (CATIA V1) war 1982 ein Add-on für CADAM. Es erweiterte CADAM mit 3D-Funktionen und ermöglichte die Flächenmodellierung, die dann über NC-Programmierung weiterverwendet werden konnten.\n\nIm Jahr 1984 wurde CATIA um Zeichnungsfunktionen erweitert und wurde damit unabhängig von CADAM. Die zweite Version (CATIA V2) wurde 1985 eingeführt. Drei Jahre später, im Jahre 1988, wurde die dritte CATIA-Version (CATIA V3) freigegeben. Dies war die erste Version, die auch auf Unix-Workstations (IBM 6150, später RS/6000) lief. \n\nDie vierte Version (CATIA V4) wurde 1993 fertiggestellt. Das Programm lief 1996 auf IBM AIX und auch auf Unix-Workstations von HP, SGI und SUN.\n\nCATIA V5 wurde im Jahr 1999 vorgestellt. Die Version wurde auf der Windows-Plattform neu entwickelt und ermöglicht ein parametrisch assoziatives Konstruieren auf PC-Basis. Nicht nur die Bedienoberfläche, auch das Dateiformat wurde umfassend geändert. Es ist zwar möglich, Konstruktionen, die mit älteren Versionen erstellt wurden, einzulesen, die Weiterbearbeitung ist jedoch nur in begrenztem Umfang möglich. Mit Erscheinen des Release V5R16 stand 2005 erstmals eine reine 64-Bit-Version von CATIA V5 für Windows XP Professional x64 Edition zur Verfügung. Das Standard-Betriebssystem für die Releases bis V5-6R2015 (alias R25) ist Windows 7, seit 2016 wird auch Windows 10 für das Release V5-6R2016 unterstützt. Bei der Umstellung von V4 auf V5 wurden auf Druck des Marktes viele neue und moderne Bedien- und Datenkonzepte wie auch bei anderen vergleichbaren CAD-Systemen eingeführt. Dies zusammen mit dem Wechsel auf eine neue Benutzeroberfläche machte den Umstieg von CATIA V4 zu V5 schwierig.\n\nIm Januar 2008 kündigte Dassault Systèmes seine Version 6-(V6-)Plattform unter dem Schlagwort „PLM 2.0 – PLM online for all“ an. In diesem Zuge wurde auch die CATIA Version 6 (3DEXPERIENCE) vorgestellt. CATIA V6 unterstützt die spartenübergreifende, virtuelle Konstruktion mit Systemengineering, Formenbau, mechanischer Konstruktion und Anlagenbau sowie der Wiederverwendung von unternehmenseigenem Wissen.\n\nDie zurzeit in Deutschland am weitesten verbreitete Version ist weiterhin CATIA V5, sie findet vor allem Anwendung in der Automobil- und Zulieferindustrie, der Luft- und Raumfahrt-Industrie sowie bei deren Zulieferern.\n\n"}
{"id": "11875", "url": "https://de.wikipedia.org/wiki?curid=11875", "title": "Commodore 128", "text": "Commodore 128\n\nDer Commodore 128 (kurz C128; umgangssprachlich „Hundertachtundzwanziger“) ist der letzte zur Marktreife gebrachte 8-Bit-Mikrocomputer des US-amerikanischen Technologiekonzerns Commodore International. Die in der Modellbezeichnung enthaltene Zahl bezeichnet dabei die Größe des ab Werk verbauten Arbeitsspeichers (RAM) in Höhe von 128 Kilobytes (KB). Aufgrund des breiten Leistungsspektrums, das nach zeitgenössischer Wahrnehmung Eigenschaften von Heimcomputern mit denen von Arbeitsplatzrechnern verbindet, lässt sich der Rechner nicht eindeutig einer Geräteklasse zuordnen. Bedient und programmiert werden kann der Rechner mit Hilfe eines herstellereigenen Dialekts der Interpretersprache BASIC.\n\nDer als Nachfolger des weltweit meistverkauften Heimcomputers Commodore 64 geltende C128 wurde erstmals im Januar 1985 auf der Winter Consumer Electronics Show in Las Vegas nach fünfmonatiger Entwicklungszeit der Weltöffentlichkeit vorgestellt. Der Markteinführungspreis lag nur wenig später in den Vereinigten Staaten bei 300 US$, in Großbritannien bei 269 £ und in Westdeutschland bei 1.198 DM. Mit weltweit rund vier Millionen verkauften Einheiten gehört der bis 1989 in drei unterschiedlichen Varianten produzierte C128 zu den kommerziell erfolgreichsten Rechnern der zweiten Hälfte der 1980er Jahre. Trotzdem konnte das vielseitige Gerät in keinem Marktsegment dominieren: Im hochpreisigen Bereich blieben die Verkaufszahlen hinter denen der IBM-PC-Kompatiblen deutlich zurück, im mittleren Preissegment beherrschten Atari 520 ST und Amiga 500 den Markt und im niedrigpreisigen 8-Bit-Heimcomputerbereich hielt nach wie vor der Vorgänger Commodore 64 die Spitzenposition bei den Verkäufen.\n\nDie technikgeschichtliche Relevanz des C128 leitet sich vor allem aus der ungewöhnlichen Ausstattung des Rechners mit gleich zwei 8-Bit-Hauptprozessoren unterschiedlicher Hersteller und gleich drei verschiedenen Betriebssystemen ab.\n\nIn der ersten Hälfte der 1980er Jahre hatten sich die Heimcomputer als Massenprodukt bereits fest am Markt für Unterhaltungselektronik etabliert. Allerdings wurde in Nordamerika und Westeuropa heftig zwischen vornehmlich US-amerikanischen Herstellern wie Commodore, Atari, Apple und Texas Instruments um Marktanteile gerungen. Daher wird diese von zahlreichen zueinander inkompatiblen Modellen geprägte Ära bisweilen als „Heimcomputer-Krieg“ bezeichnet.\n\nAuch firmenintern entstanden bei Commodore zunehmend Spannungen zwischen Konzernleitung und Entwicklungsabteilung. Fast alle der an der Entwicklung des C64 beteiligten Ingenieure, darunter die Chipentwickler Bob Yannes und Al Charpentier, klagten über trotz des großen Verkaufserfolgs ausbleibende Gehaltserhöhungen. Hinsichtlich neuer Hardwareprojekte herrschte keine Einigkeit. Charpentier schlug die Entwicklung eines neuen Rechners namens C80 mit 80-Zeichen-Bildschirm, 256 kB RAM, hochauflösendem Monitor und schnellerem Diskettenlaufwerk für das mittlere Preissegment vor.\n\nDiese Idee wurde jedoch vom für seine Niedrigpreispolitik bekannten Hauptgeschäftsführer Jack Tramiel abgelehnt. Tramiel versprach sich von einem billigeren, wie der C64 an herkömmliche Fernsehgeräte anschließbaren neuen Rechner mehr Profit. Im Sommer 1983 begann daher auf Geheiß Tramiels die Arbeit an der Commodore-264-Serie mit dem Ziel der Entwicklung eines Konkurrenzmodells zum erfolgreichen britischen Billigrechner ZX Spectrum. Daraufhin verließen Yannes, Charpentier sowie weitere führende Ingenieure das Unternehmen.\n\nTramiel selbst musste aufgrund unüberbrückbarer Meinungsverschiedenheiten mit dem Hauptaktionär und Aufsichtsratsvorsitzenden Irving Gould nach gut dreißigjähriger Firmenzugehörigkeit am 13. Januar 1984 seinen Posten als Hauptgeschäftsführer räumen. An seine Stelle rückte am 21. Februar 1984 der erfahrene, zuvor in der Stahlindustrie tätige Marshall F. Smith. Zwar verkaufte sich der marktführende C64 immer noch ausgezeichnet, die Anfang 1984 zur Marktreife gebrachten, nicht zum C64 kompatiblen Rechner der Commodore-264-Serie stellten sich jedoch als Ladenhüter heraus.\n\nUm sich Klarheit über die Kundenwünsche hinsichtlich eines C64-Nachfolgers zu verschaffen, führten Commodore-Mitarbeiter anlässlich der in Chicago stattfindenden Summer Consumer Electronics Show im Juni 1984 eine Umfrage unter den einen C64 besitzenden Messebesuchern durch. Diese ergab große Zufriedenheit hinsichtlich der Grafikfähigkeiten, der Klangerzeugungsmöglichkeiten sowie des vergleichsweise niedrigen Preises des C64. Neben der am häufigsten genannten C64-Softwarekompatibilität zählten ein verbessertes BASIC, mehr Arbeitsspeicher, die Fähigkeit zur Darstellung von 80 Zeichen pro Zeile, ein numerischer Ziffernblock, ein schnelleres Diskettenlaufwerk sowie die native Fähigkeit zur Verwendung professioneller CP/M-Software zu den dringendsten Verbesserungswünschen.\n\nHauptgeschäftsführer Smith gab schließlich im September 1984 unter Berücksichtigung der genannten Verbesserungswünsche die Entwicklung des C128 in Auftrag. Der neue Rechner sollte rechtzeitig zur für Anfang Januar 1985 in Las Vegas angesetzten Winter Consumer Electronics Show fertig sein. Damit standen lediglich vier Monate an Entwicklungszeit zur Verfügung. Die Projektleitung übernahm der 1983 im Alter von 24 Jahren zum Leiter der Hardware-Entwicklungsabteilung ernannte Bil Herd. Das Wissen darum, dass der C128 Commodores letzter Vertreter der 8-Bit-Heimcomputer-Pioniergeneration sein würde, stellte für das Entwicklerteam eine besondere Motivation dar.\n\nBereits seit 1983 arbeitete ein Entwicklungsteam unter der Leitung Robert Russells an einem neuen Heimcomputermodell namens D128. Der D128 ging seinerseits auf im Zusammenhang mit der Planung der CBM-500-Serie angestellte Überlegungen zurück, die mit dem B128 ein Bürocomputermodell und dem P128 ein Heimcomputermodell vorsahen. Ähnlich wie der P128 sollte auch der D128 mit dem zur Verwaltung von mehr als 64 kB Arbeitsspeicher fähigen 8-Bit-Hauptprozessor MOS Technology 6509 (kurz MOS 6509) sowie dem schon im C64 verbauten Soundchip MOS Technology 6581 (kurz MOS 6581) ausgestattet werden. Da sich Russells Ingenieure nach dem Weggang Charpentiers nicht an eine Weiterentwicklung des spritefähigen und sehr komplexen 40-Zeichen-Grafikchips MOS Technology VIC II (kurz VIC II) heranwagten, gestaltete sich die Bildschirmausgabe beim D128 jedoch als problematisch. In Erwägung gezogen wurde nicht zuletzt die Verwendung zweier parallel arbeitender Grafikchips in Gestalt des bewährten VIC II sowie eines noch zu entwickelnden farbfähigen 80-Zeichen-Grafikchips.\n\nDa die Planungen so oder so aber weder eine C64-Kompatibilität noch eine CP/M-Fähigkeit vorsahen und damit den von der Firmenleitung gemachten Vorgaben widersprachen, wurde das D128-Projekt kurzerhand von Herd beendet. Herds eigenes Entwicklerteam griff aber einige der im Zusammenhang mit dem D128 angestellten Überlegungen bei der Planung des C128 wieder auf, etwa hinsichtlich der Verwendung von zwei Grafikchips. Um die angestrebten Verbesserungen der Leistungsfähigkeit ohne Verlust der vollständigen C64-Kompatibilität umsetzen zu können, sollte außerdem der im C64 seit 1982 verbaute Chipsatz mit dem Hauptprozessor MOS Technology 6510 (kurz MOS 6510), dem Grafikchip VIC II, dem Soundchip MOS 6581 sowie weiteren Bausteinen für den C128 einfach weiterentwickelt werden. Zwecks Implementierung der geforderten CP/M-Fähigkeit bot sich die Verwendung der weitverbreiteten CPU Zilog Z80A (kurz Z80A) als Zweitprozessor an.\n\nUm die gesteckten Ziele zu erreichen, sollte der C128 außerdem mit drei voneinander völlig unabhängigen Betriebsarten versehen werden. Zur Erschließung der für den C64 bereits existierenden umfangreichen Softwarebibliothek sollte die Hardware des neuen Rechners im C64-Modus das Vorgängermodell lückenlos emulieren. Eine höhere Arbeitsgeschwindigkeit, ein komfortableres BASIC und ein vergrößerter Arbeitsspeicher sollten im C128-Modus zur Verfügung stehen, der als Hauptbetriebsart vorgesehen war. Der CP/M-Modus schließlich war vornehmlich für ernsthafte berufliche Anwendungen und den Einsatz der bewährten sowie in Fülle vorhandenen CP/M-Software gedacht. Während für den C64-Modus der ursprüngliche Betriebssystemkern des Vorgängermodells unverändert übernommen werden konnte, musste für den C128-Modus ein neuer Betriebssystemkern sowie ein leistungsstärkerer Dialekt des Commodore BASIC programmiert werden.\n\nUm im C128-Modus die angestrebte höhere Arbeitsgeschwindigkeit realisieren zu können, wurde der altbekannte, mit einer Taktfrequenz von rund 1 MHz arbeitende 8-Bit-Hauptprozessor MOS 6510 aus dem C64 überarbeitet und weiterentwickelt. Diese Aufgabe übernahm die konzerneigene Abteilung für Halbleiterentwicklung. Sie trug die Bezeichnung Commodore Semiconductor Group (kurz CSG) und war aus dem 1976 von Commodore übernommenen Halbleiterhersteller MOS Technology hervorgegangen. Die Überarbeitung führte schließlich zur Fertigstellung des mit einer Taktfrequenz von rund 2 MHz doppelt so schnellen und mit zusätzlichen Funktionen versehenen MOS Technology 8502 (kurz MOS 8502).\n\nDer Grafikchip VIC II aus dem C64 wurde von Dave DiOrio weiterentwickelt und konnte nun bei abgeschaltetem Videosignal mit dem gleichen Basistakt wie der MOS 8502 Grafikdaten verarbeiten. Allerdings gab es beim daraus entstandenen MOS Technology VIC IIe (kurz VIC IIe) keine signifikanten Verbesserungen etwa im Hinblick auf die Bildauflösung, die Farbtiefe oder die für die Spieleindustrie wichtige Spritefähigkeit.\n\nFrank Palaia übernahm die Aufgabe der im Dezember 1984 erfolgreich zum Abschluss gebrachten Integration des Z80A in die bewährte 8-Bit-Rechnerarchitektur von Commodore. Zu diesem Zweck wurde die Taktfrequenz des eigentlich doppelt so schnellen Z80A auf 2,04 MHz gedrosselt. Für den Betrieb unter CP/M musste außerdem eine auf die Hardware des C128 zugeschnittene Portierung der aktuellen Betriebssystemversion CP/M-Plus Version 3.0 (kurz CP/M 3.0 bzw. CP/M-Plus) entwickelt werden. Diese Aufgabe wurde dem Programmierer Von Ertwine übertragen. Terry Ryan schrieb den für zur Programmierung und Bedienung gedachten neuen BASIC-Dialekt des C128, fortan als Commodore BASIC V7.0 bezeichnet. Fred Bowen wurde mit der Programmierung der Betriebssystemroutinen betraut.\n\nDer Arbeitsspeicher des neuen Rechners wurde auf namengebende 128 kB RAM aufgestockt. Da die 16-Bit-Adressbusstrukturen des MOS 8502 nicht zur Verwaltung eines so großen Arbeitsspeichers ausreichten, mussten außerdem ein Speicherverwaltungsbaustein sowie ein Adressmanager neu entwickelt werden. Dave Haynies Erfahrungen bei der Emulation des Adressmanagers sowie der Konzeption der Zeitsteuerung flossen später in die Entwicklung des Commodore Amiga ein. Außerdem sollte in Übereinstimmung mit den Kundenwünschen das für seine extreme Langsamkeit bei der Datenübertragung berüchtigte 5¼-Zoll-Diskettenlaufwerk VC1541 des Vorgängermodells C64 durch ein neu entwickeltes Gerät mit deutlich höherer Datenübertragungsrate ersetzt werden. Greg Berlin war für die Planung der Hardware des neuen 5¼-Zoll-Diskettenlaufwerks VC1571 verantwortlich, während Dave Siracusa das zugehörige Diskettenbetriebssystem Commodore DOS 3.0 programmierte.\n\nDer C128 erhielt außerdem ein völlig neues, im Gegensatz zur klobigen Brotkastenform des C64 auf Professionalität, Bürotauglichkeit sowie verbesserte Ergonomie abzielendes Design. So wurde das Gehäuse gegenüber dem Vorgängermodell deutlich abgeflacht, um den Anwendern das ermüdende Anheben der Handballen bei der Bedienung der Tastatur zu ersparen. Außerdem erhielt die Tastatur einen numerischen Ziffernblock und zusätzliche Funktionstasten. Wer genau das Gehäuse des C128 entworfen hat, ist nicht bekannt. Vermutet wird eine Beteiligung des preisgekrönten Industriedesigners Ira Velinsky, der bereits die Gehäuse der Modelle Commodore Max, SX-64 und Plus/4 entworfen hatte, bevor er im Jahr 1984 gemeinsam mit Tramiel Commodore International verließ.\n\nZum Zeitpunkt der Entwicklung des C128 verfügte die Hardware-Entwicklungsabteilung von Commodore bereits über Erfahrungen mit zur Darstellung von 80 Zeichen pro Zeile fähigen Grafikchips. So hatte die CSG für die Bürorechner der CBM-8000-Serie bereits den Motorola 6845 zum als Ansteuerschaltung für die Kathodenstrahlröhre des fest eingebauten Bildschirms dienenden MOS Technology 6545 (kurz MOS 6545) weiterentwickelt. Der im englischsprachigen Raum auch als \"Cathode Ray Tube Controller\" (kurz CRTC) bezeichnete MOS 6545 vermochte Texte jedoch lediglich in zwei Farben auf den Bildschirm zu bringen. Daher wurde der Grafikchip für den D128 sowie den als 16-Bit-Workstation konzipierten, aber ebenfalls nie zur Serienreife gebrachten CBM 900 unter der Leitung von Kim und Anne Eckert ab Anfang 1983 in rund anderthalb Jahren zum mit einer Palette von 16 Farben und dediziertem Grafikspeicher arbeitenden MOS Technology 8563 (kurz MOS 8563) weiterentwickelt. Da der MOS 8563 vornehmlich für die Textverarbeitung gedacht war, wurde auf die Fähigkeit zur Darstellung von Sprites verzichtet.\n\nZwecks Umsetzung der für den C128 vorgesehenen Fähigkeit zur Darstellung von 80 Zeichen pro Zeile entschied sich die Entwicklungsabteilung für einen Einbau des MOS 8563 in den neuen Rechner. Beim Versuch der Integration des MOS 8563 in die Systemarchitektur des C128 kam es allerdings zu Kommunikationsstörungen zwischen Herd und der unabhängig arbeitenden CSG. Zwar wusste Herd, dass der MOS 8563 eine Weiterentwicklung des schon für eine Verwendung im D128 in Erwägung gezogenen Motorola 6845 sowie des MOS 6545 darstellte. Allerdings war der C128-Projektleiter nicht von den Kollegen der Abteilung für Halbleiterentwicklung über Änderungen der Adressbusstrukturen, der Taktung sowie der Handhabung der Lese-/Schreibleitung in Kenntnis gesetzt worden. Der ab September 1984 prinzipiell einsatzfähige 80-Zeichen-Grafikchip des C128 bereitete den Hardwareentwicklern daher immer wieder Probleme, vor allem mit seiner aus Herds Unkenntnis resultierenden Neigung zum Überhitzen und seiner vom 40-Zeichen-Grafikchip VIC IIe abweichenden Taktung.\n\nLaut Herd war die Zeitknappheit bei der Planung des C128 so groß, dass die Waschbecken des Entwicklungslabors als provisorische Duschen herhalten mussten. Die heißgelaufenen Diskettenlaufwerke wurden zum Warmhalten der bei der Arbeit nebenbei eingenommenen Fertigmahlzeiten verwendet. Noch in der Nacht vor der Eröffnung der Winter Consumer Electronics Show (kurz CES) vom 5. bis 6. Januar 1985 musste bis zwei Uhr morgens an den Prototypen des C128 gearbeitet werden, um den Rechner überhaupt rechtzeitig der Öffentlichkeit präsentieren zu können. Obendrein waren die Hotelzimmerreservierungen des Präsentationsteams in Las Vegas im Vorfeld der Messe von einer unbekannten Person annulliert worden. Dabei handelte es sich möglicherweise um einen Sabotageakt des ehemaligen Commodore-Geschäftsführers Tramiel.\n\nWirklich zuverlässig war der mit einem Listenpreis von unter 300 US$ angekündigte C128 zum Zeitpunkt der offiziellen Präsentation indessen noch nicht. Pro Tag brannten im Durchschnitt zwei Exemplare des 80-Zeichen-Grafikchips MOS 8563 durch. Das Präsentationsteam ersetzte die defekten Grafikchips klammheimlich hinter den Kulissen durch funktionsfähige Ersatzbausteine. Auf diese Weise entstand beim Messepublikum der Eindruck eines bereits perfekt funktionierenden, sofort einsetzbaren Rechners. Erst im Verlauf der nächsten Monate gelang den Commodore-Entwicklern durch Veränderungen am Layout der Hauptplatine eine auch im Dauerbetrieb technisch zuverlässige Implementierung des MOS 8563 ins Gesamtsystem.\n\nNeben dem C128 stellte Commodore auch das neue, CP/M-kompatible 5¼-Zoll-Diskettenlaufwerk VC1571, den Farbmonitor 1902, einen Monochrommonitor sowie die Computermaus 1350 dem Fachpublikum vor und kündigte die Veröffentlichung einer teureren Desktop-Version des Rechners namens C128D mit integrierter VC1571 an, ohne einen konkreten Termin für die Markteinführung zu nennen. Außerdem wurden neben diversen Speichererweiterungen mit dem Modell 1660 ein 300-Baud-Modem sowie dem Modell 1670 ein 1.200-Baud-Modem für den C64 bzw. C128 angekündigt.\n\nDem kontinentaleuropäischen Publikum wurde der C128 auf der Hannover-Messe vom 17. bis 24. April 1985 vorgestellt. Dabei handelte es sich um einen Prototyp mit deutscher Tastatur. Zwar funktionierte der als „Superding“ beworbene neue Rechner nun technisch anstandslos und zog viel Aufmerksamkeit auf sich. Zu Demonstrationszwecken entwickelte neue Software blieb jedoch zu diesem Zeitpunkt zur Enttäuschung der Messebesucher – von wenigen Ausnahmen wie der Textverarbeitung \"Superscript\" abgesehen – eine Seltenheit. Auch die Portierung von CP/M-Plus war noch nicht abgeschlossen und die vorgestellte Testversion zudem sehr langsam. Der westdeutsche Einführungspreis lag bei 1.198 DM. Neben dem C128 wurde auch ein Prototyp der Desktop-Version C128D ausgestellt.\n\nIn Großbritannien wurde der C128 anlässlich der International Commodore Computer Show vom 7. bis 9. Juni 1985 offiziell eingeführt. Der vom Hersteller zu diesem Zeitpunkt für Großbritannien noch nicht bekanntgegebene Einführungspreis für den C128 wurde auf 300–350 £ und für den noch nicht marktreifen C128D auf 500–600 £ geschätzt.\n\nAuf der vom 15. bis 18. Januar 1986 im National Exhibition Centre der englischen Industriestadt Birmingham abgehaltenen Which Computer Show stellte Commodore dem europäischen Fachpublikum den bereits im Vorjahr angekündigten C128D mit platzsparendem Kunststoffgehäuse, ausklappbarem Tragegriff, abgesetzter Tastatur und integriertem 5¼-Zoll-Diskettenlaufwerk VC1571 offiziell vor. Die unverbindliche Preisempfehlung für das neue, auf Geschäftsleute ausgerichtete Modell lag zunächst bei 499 £ bzw. 538,85 £ inklusive Mehrwertsteuer. Ein Monochrommonitor sollte ebenfalls in diesem Preis inbegriffen sein. Das Gesamtpaket kostete aber schließlich dann doch 599 £.\n\nTrotz erster Verkaufserfolge in Westeuropa im Laufe des Jahres 1986 gelangte der C128D jedoch nicht in den US-amerikanischen Handel, da der Rechner nach Auffassung der dort für die Zulassung elektronischer Geräte zuständigen Federal Communications Commission (FCC) nicht hinreichend funkentstört war. Um die strengen FCC-Standards doch noch zu erfüllen und keine Marktanteile zu verlieren, entwickelte das Unternehmen mit dem C128D-CR ein weiteres Desktop-Modell mit Metallgehäuse und überarbeiteter Elektronik, das den C128D ablösen sollte. Das neue Gerät wurde auf der vom 8. bis 11. Januar 1987 in Las Vegas abgehaltenen Winter Consumer Electronics Show dem nordamerikanischen Publikum vorgestellt. Die unverbindliche Preisempfehlung lag bei 550 US$.\n\nVermutlich bereits ab Ende 1985 arbeitete die Commodore-Entwicklungsabteilung außerdem an einer weiteren Variante des C128D mit integriertem 3½-Zoll-Diskettenlaufwerk. Es blieb jedoch bei der Entwicklung eines funktionsfähigen, aber nie über das Planungsstadium hinaus gelangenden NTSC-Prototypen. Dieser enthielt die Platine sowie Laufwerksmechanik des ebenfalls nie zur Serienreife gebrachten 3½-Zoll-Diskettenlaufwerks VC1563 mit eigenem, bereits für die spätere VC1581 programmierten Diskettenbetriebssystem auf einem provisorischen EPROM-Chip, eine stark modifizierte Hauptplatine, mehrere improvisierte Zusatzplatinen, ein Kunststoffgehäuse mit Lüftungsschlitzen sowie einen ausklappbaren Tragegriff. Zwecks Unterscheidung von anderen Modellvarianten wird dieser Prototyp inoffiziell auch als C128D/81 bezeichnet, da keine Klarheit über den von Commodore intern verwendeten Projektnamen besteht (möglicherweise wurde hierfür die Bezeichnung „Kentron“ verwendet).\n\nBis zum Frühsommer 1986 wurden alle für Westeuropa gedachten C128-Modellvarianten in Commodores Zweigwerk im englischen Corby hergestellt. Nach der Schließung der einzigen britischen Produktionsstätte des weltumspannenden Konzerns wurde die Produktion des Rechners aus Kostengründen ins westdeutsche Zweigwerk in Braunschweig verlegt. Ende 1986 entschied die Firmenleitung, zukünftig den neuen Amiga 2000 in Braunschweig fertigen zu lassen, während die C128-Produktion ins Mutterwerk nach West Chester im US-Bundesstaat Pennsylvania sowie nach Fernost verlagert wurde.\n\nChefentwickler Bil Herd verließ Commodore kurz nach der Markteinführung des C128. Dave Haynie und Frank Palaia aus dem einstmaligen Entwicklerteam arbeiteten trotz des offensichtlichen Bedeutungsverlustes der Rechner mit 8-Bit-Architektur ab 1986 an möglichen Nachfolgemodellen auf der Basis des C128.\n\nAus dieser Zusammenarbeit gingen mehrere Designstudien hervor. Eine davon bestand im Desktop-Modell Commodore 256 (kurz C256), das es immerhin bis zum Stadium eines vorführbaren Prototypen schaffte und auch in einer Wartungsanleitung für den C128 aus dem Jahr 1987 als bereits geplantes Nachfolgemodell des C128 erwähnt wird. Der C256-Prototyp besaß neben einem integrierten 3½-Zoll-Diskettenlaufwerk und einer internen Festplatte mit einer Speicherkapazität von 25 MB einen großzügigeren Arbeitsspeicher von 256 kB RAM sowie einen auf volle 4 MHz getakteten Zweitprozessor Z80A. Die höhere Taktung sollte gegenüber dem C128 zu einer wesentlichen Erhöhung der Arbeitsgeschwindigkeit im CP/M-Modus führen.\n\nEine weitere Designstudie hatte eine abgespeckte Version des im Grunde überkomplexen C128 zum Ziel. Sie sollte unter Verzicht auf jegliche C64-Kompatibilität lediglich über den 80-Zeichen-Grafikchip MOS 8563 verfügen und daher in der Herstellung deutlich kostengünstiger sein.\n\nBeide Konzepte wurden jedoch von der Firmenleitung rundweg abgelehnt. Da es für den C128 bereits Speichererweiterungen aus dem eigenen Hause gab, mit deren Hilfe der Arbeitsspeicher auf bis zu 640 kB RAM ausgebaut werden konnte, bestand kein Bedarf nach einem weiteren Modell auf C128-Basis mit einer Speicherkapazität von lediglich 256 kB. Auch der Verzicht auf jegliche C64-Kompatibilität und die Spritefähigkeit des 40-Zeichen-Grafikchips VIC IIe überzeugte die um die Wichtigkeit der Spielesoftware wissende Konzernspitze nicht. Daraufhin konzentrierten sich Haynie und Palaia ganz auf die Entwicklung des noch unvollendeten 16-Bit-High-End-Rechners Amiga 2000.\n\nZwar brachte Commodore kein C128-Nachfolgemodell zur Marktreife, das runderneuerte Design des Rechners inklusive abgeflachter Gehäuseform, beiger Gehäusefarbe und ergonomischer Tastatur wurde aber bei der Planung des C64C – einer 1986 auf den Markt gebrachten Revision des ursprünglich in grauer Brotkastenform gefertigten C64 – von der Entwicklungsabteilung übernommen.\n\nDen ursprünglichen Planungen zufolge sollte der C128 spätestens ab April 1985 in den Vereinigten Staaten und ab dem folgenden Sommer in Europa erhältlich sein. Die Serienproduktion des C128 lief aber erst im Sommer 1985 an, sodass sich diese Termine nur teilweise einhalten ließen. Ab Ende Juli 1985 waren erste Exemplare des Rechners in westdeutschen Kaufhäusern erhältlich. Ende August 1985 folgten die großen US-amerikanischen Kaufhausketten wie Kmart oder Sears Roebuck, während der laut Planung erst etwas später für die Markteinführung vorgesehene, jedoch keine FCC-Zulassung erhaltende C128D dem Fachhandel vorbehalten bleiben sollte. In Kanada war der Rechner ab September 1985 zunächst nur in kleinen Stückzahlen lieferbar, da es Probleme bei der Abnahme des Netzteils durch die zuständige Behörde gab und jedes Exemplar vor dem Verkauf einzeln überprüft werden musste.\n\nAb dem 1. September 1985 sollte der C128 ursprünglich auch in Großbritannien verfügbar sein. Die dortige Auslieferung wurde jedoch hinausgezögert, um Zeit für die Entwicklung einer billigeren, lediglich 199 £ kostenden Alternative zur relativ teuren VC1571 zu gewinnen, die schließlich mit dem im Gehäuse des Vorgängermodells VC1541 untergebrachten und nur über einen Schreib-/Lesekopf verfügenden 5¼-Zoll-Diskettenlaufwerk VC1570 realisiert wurde. Da britische Verbraucher weniger für einen neuen Rechner auszugeben bereit waren als die Kundschaft im wohlhabenderen Nordamerika, versprach sich die Marketingabteilung von der kostengünstigeren VC1570 größere Absatzchancen für den C128 selbst. Das Modell VC1570 ist deshalb etwa in den Vereinigten Staaten praktisch unbekannt. Ab Anfang Oktober 1985 war der Rechner schließlich auch in Großbritannien erhältlich – zunächst nur vereinzelt in unabhängigen Fachgeschäften, dann auch in den großen Kaufhäusern.\n\nCommodore lieferte zunächst ausschließlich den C128 aus. Die Peripheriegeräte sollten einige Zeit später folgen. Bei der Produktion der Diskettenlaufwerke VC1570 und VC1571 sowie der Herstellung des neuentwickelten RGBI-fähigen Farbmonitors 1902 gab es jedoch mehrwöchige Verzögerungen. In den Vereinigten Staaten waren die VC1571 und der NTSC-Farbmonitor 1902 ab November 1985 in kleineren Mengen erhältlich. Etwa zeitgleich waren das günstigere Diskettenlaufwerk VC1570 sowie der PAL-Farbmonitor 1901 auch in Großbritannien verfügbar. Die VC1571 hingegen gab es dort erst ab März 1986 für 269 £ zu kaufen und war damit genauso teuer wie der Rechner selbst. In Westdeutschland wiederum waren beide Diskettenlaufwerke erst um den Jahreswechsel lieferbar. Commodore-Pressesprecher Gerold Hahn dementierte in diesem Zusammenhang aufgekommene Gerüchte um technische Probleme und machte Lieferschwierigkeiten bei den Zulieferern des Gehäuses sowie der Laufwerksmechanik der VC1571 für die Verzögerungen verantwortlich. Der Einführungspreis der VC1570 lag in Westdeutschland bei 750 DM, während die VC1571 mit 950 DM etwas weniger als der C128 kostete. Der Farbmonitor 1901 war in Westdeutschland ab dem gleichen Zeitpunkt für 998 DM erhältlich. Das 1.200-Baud-Modem 1670 war bereits ab Ende 1985 lieferbar. Frühe Bauserien des Gerätes enthielten jedoch einen Hardwarefehler. Dieser wurde zwar in späteren Bauserien korrigiert, insgesamt erreichte das nur in geringen Stückzahlen hergestellte, für 89,95 US$ erhältliche Modem 1670 jedoch keine hohe Marktdurchdringung und war bis Mitte des Jahres 1988 kaum verfügbar.\n\nDie schon im Zuge der Markteinführung aufgetretenen Produktionsverzögerungen und Auslieferungsschwierigkeiten setzten sich bei den nach 1985 von Commodore zur Marktreife gebrachten Peripheriegeräten sowie dem Desktop-Modell C128D-CR fort, während der C128D in Westeuropa pünktlich Anfang 1986 erschien und im zweiten Quartal bereits in hohen Stückzahlen lieferbar war. Bereits im Frühjahr 1986 verkündete Commodore die bevorstehende Serienreife der digitalen Joystickmaus 1350 sowie der Speichererweiterungsmodule 1700, 1750 und 1764 mit Kapazitäten von 128 kB, 256 kB bzw. 512 kB. Im Sommer 1986 wurden überdies Pläne zur Entwicklung des 3½-Zoll-Diskettenlaufwerks VC1581 bekanntgegeben. Spätestens ab Herbst 1986 sollten das Diskettenlaufwerk VC1581 und die Maus 1350 laut Planung lieferbar sein. Die unverbindliche Preisempfehlung für die VC1581 lag zunächst bei 399 US$, wurde später aber auf 249,95 US$ herabgesenkt.<ref name=\"1764/1581 Prices\">Mark R. Brown, Benn Dunnington: \"1764/1581 Prices.\" In: \"Info. The Useful Guide to Commodore & Amiga Computing.\" Nr. 16, 1987, S. 68.</ref> Der Straßenpreis für die VC1581 lag im Herbst 1987 in Westdeutschland bei rund 600 DM.\n\nEs kam jedoch erneut zu Produktionsverzögerungen. Erst Ende 1986 waren die 128-kB-Speichererweiterung 1700 für 198 DM sowie die 512-kB-Version 1750 für 298 DM im Handel erhältlich. Anfang 1987 folgte die 256-kB-Speichererweiterung 1764 für zunächst 129 US$, später dann 149,95 US$. Aufgrund von Lieferungsschwierigkeiten seitens der Zuliefererfirmen bei den RAM-Chips konnte die 512-kB-Version ohnehin nur in kleinen Stückzahlen produziert werden. Sie blieb daher stets schwer erhältlich, auch in Nordamerika. In Westdeutschland war das Modell 1750 schon nach wenigen Monaten ausverkauft und musste fortan – sofern verfügbar – aus den Vereinigten Staaten importiert werden. Die übrigen genannten Peripheriegeräte kamen erst im Laufe der ersten drei Quartale des Jahres 1987 nach und nach in die Läden. Zusätzlich wurde im Sommer 1987 die schon auf der vorhergehenden Winter Consumer Electronics Show vorgestellte analoge Proportionalmaus 1351 für 49 US$ auf den Markt gebracht. Ebenfalls im Sommer 1987 veröffentlichte Commodore auf ROM-Chips gebrannte, fehlerbereinigte Versionen des Commodore DOS 3.0 für 9,95 US$ sowie des C128-Betriebssystems für 24,95 US$.\n\nIm Herbst 1987 erfolgte in Zusammenarbeit mit der damaligen Deutschen Bundespost die Markteinführung des für 399 DM erhältlichen BTX-Decoder-Moduls II in Westdeutschland, mit dessen Hilfe das zu diesem Zeitpunkt deutschlandweit über gerade einmal 70.000 Anschlüsse zugängliche interaktive Endbenutzer-Informationssystem Bildschirmtext (kurz BTX) am C128 betrieben werden konnte, das als gescheiterter Vorläufer des heutigen Internets sowie des World Wide Webs gilt. Mit dieser Kooperation sollte den ursprünglich von der Bundespost anvisierten drei Millionen BTX-Anschlüssen zumindest näher gekommen werden. Bis Anfang 1989 gelang aber lediglich eine Verdopplung der BTX-Anschlüsse auf knapp 150.000.\n\nLieferbar war selbst das neuentwickelte, bereits im Januar 1987 offiziell eingeführte Spitzenmodell C128D-CR trotz gegenüber der Tastaturcomputerversion und dem C128D vereinfachter Fertigungsprozesse und niedrigerer Herstellungskosten erst ab dem dritten Quartal des Jahres 1987. Aufgrund der unerwartet hohen Nachfrage kam es im Frühjahr 1988 vorübergehend sogar zu Auslieferungsschwierigkeiten bei dieser letzten zur Marktreife gebrachten Modellvariante des C128.\n\nIm Zuge der Markteinführung schaltete Commodore im US-amerikanischen Fernsehen und in Fachzeitschriften eine gegen die Konkurrenzmodelle IBM-PC, IBM-PCjr und Apple IIc gerichtete Werbekampagne mit dem Slogan ‚Schlechte Neuigkeiten für IBM und Apple‘ (englisch \"„Bad News for IBM and Apple“\"). Weitere, in diversen Computerzeitschriften veröffentlichte Werbeanzeigen hoben die Überlegenheit des C128 gegenüber dem Apple IIc etwa hinsichtlich der Speicherkapazität mit Slogans wie ‚Danke für den großen Arbeitsspeicher‘ (englisch \"„Thanks for the memory“\") hervor und betonten überdies neben der um einen numerischen Ziffernblock erweiterten Tastatur die herausragenden Grafik- und Soundfähigkeiten des neuen Rechners. Verschwiegen wurde allerdings die im Vergleich zum C128 kompaktere Bauform des Apple IIc, dessen angebliche technische Unterlegenheit im beigefügten Werbefoto durch vom Baum gefallene Äpfel symbolisiert wird. In einer weiteren Werbeanzeige war der C128 in Anspielung auf gängige Darstellungen der durch Evolution bedingten Stammesgeschichte des Menschen in einer horizontalen Bilderfolge als sich ständig fortentwickelndes und erweiterndes Computersystem mit Rechner, Diskettenlaufwerk, Speichererweiterung, Maus, Modem, Drucker und Farbmonitor zu sehen, begleitet von dem Slogan ‚Wie man sich zu einer höheren Form von Intelligenz entwickelt‘ (englisch \"„How to evolve to a higher intelligence“\").\n\nDie Softwarehersteller verhielten sich in Bezug auf den C128 zunächst abwartend. Nur wenige etablierte Publisher wie Timeworks, Audiogenic, Thorn EMI, Spinnaker Software oder Precision Software kündigten Programme für betriebswirtschaftliche Zwecke, aber keine Spiele für die nähere Zukunft an. Als sich gegen Ende des Jahres 1985 in diversen Computerzeitschriften immer mehr C128-Besitzer über den Mangel an Software für ihre neuen Rechner zu beklagen begannen, veröffentlichte Commodore Werbeanzeigen, die unter Verwendung des Slogans ‚Harte Fakten über die Software‘ (englisch \"„Hard Facts About the Software“\") die Entwicklung hunderter neuer Anwendungsprogramme für den C128-Modus ankündigten. Insgesamt verblassten die Werbemaßnahmen für den C128 in den Vereinigten Staaten jedoch im Vergleich zum intensiver beworbenen Amiga 1000.\n\nIn Großbritannien sah Hauptgeschäftsführer Smith den in Westdeutschland von Schneider vertriebenen, ebenfalls CP/M-fähigen Amstrad CPC6128 als Hauptkonkurrenten des C128 an – eine auch von Teilen der britischen Fachpresse geteilte Sichtweise. Mit Werbetexten wie ‚Die Fakten sprechen eine eindeutige Sprache‘ (englisch \"„When you look at the facts, they do seem to weigh heavily in our favour“\") und Begleitfotos, die den C128 als Gewinner eines Gewichtsvergleichs mit einem nicht genau identifizierbaren Konkurrenzmodell auf einer Balkenwaage zeigten, wurde in britischen Computerzeitschriften für den Rechner geworben. Dabei sollte dem C128 das Image eines auch für Geschäftsleute und Kleinunternehmer interessanten Bürocomputers gegeben werden, mit dem man nicht nur spielen konnte.\n\nIm deutschsprachigen Raum war der Apple IIc genau wie seine Vorgänger Apple IIe und Apple II Europlus wegen seines hohen Preises kaum verbreitet. Deshalb wurde dort anfänglich eine andere Werbestrategie verfolgt. Den deutschsprachigen Kunden wurde der C128 in der Tradition der erfolgreichen Bürorechner der CBM-8000-Serie als professioneller, dem weitaus teureren IBM-PC technisch überlegener Personal Computer vorgestellt. Besonders hervorgehoben wurde dabei, dass der neue Rechner bei voller C64-Kompatibilität mit seinem 80-Zeichen-Bildschirm, seiner CP/M-Fähigkeit sowie seinem großen, obendrein auf 640 kB RAM erweiterbaren Arbeitsspeicher „weit über die Grenzen der Heimcomputerklasse“ hinausrage. Tatsächlich war der C128 dem IBM-PC bei Benchmarktests hinsichtlich der Berechnung von Primzahlen und Fließkommazahlen in BASIC leicht überlegen und konnte auch hinsichtlich der Geschwindigkeit beim Einlesen von auf Diskette gespeicherten Daten sowie der Speicherkapazität pro Diskette mit dem IBM-Rechner mithalten. Lediglich bei Disketten-Schreiboperationen besaß der IBM-PC gegenüber dem C128 Geschwindigkeitsvorteile. Später wurde dann mit Slogans wie „Mächtiges Gedächtnis. Starke Programme. Eine höhere Form der Intelligenz“ oder „Hohe Intelligenz. Mächtiger Wortschatz. Drei Mikrocomputer in einen gepackt“ an die in der englischsprachigen Welt geführte Werbekampagne angeknüpft.\n\nFinanziell geriet der Konzern nach der Markteinführung des C128 zunehmend in eine Schieflage, die sich auch auf die Produktwerbung auswirkte. Im dritten Quartal des Jahres 1985 mussten 39,2 Millionen US$ an Verlusten verbucht werden, die zum Teil den hohen Entwicklungskosten für den C128 und den Amiga 1000 zugeschrieben wurden. Im vierten Quartal des Jahres 1985 wuchs der Fehlbetrag sogar auf 50,2 Millionen US$ an. Insgesamt betrugen die Verluste im Kalenderjahr 1985 satte 144 Millionen US$. Auch das erste Quartal des Jahres 1986 brachte mit Verlusten von 36,7 Millionen US$ keine Verbesserung. Im April 1986 löste Thomas J. Rattigan aufgrund dieser Talfahrt seinen Vorgänger Smith als Commodore-Hauptgeschäftsführer ab. Rattigan schloss unprofitable Zweigwerke wie das im englischen Corby und nahm einen bis zum 15. März 1987 laufenden Kredit in Höhe von zunächst 135 Millionen US$ auf, der im Herbst sogar auf 140 Millionen US$ aufgestockt wurde und nicht zuletzt eine angemessene Vermarktung des C128 sowie des Amiga 1000 gestatten sollte.\n\nDiese Maßnahmen brachten das Unternehmen tatsächlich wieder zurück in die Gewinnzone. Dennoch wurden die Ausgaben für die Werbung unter Rattigan zunächst zurückgefahren. Das Ausbleiben von Werbeanzeigen und die bevorstehende Veröffentlichung des nur etwas teureren, aber deutlich leistungsstärkeren Amiga 500 nährten von der Firmenspitze umgehend dementierte Gerüchte um eine Produktionseinstellung des C128 nach dem Weihnachtsgeschäft 1986. Erst unter Rattigans am 16. April 1987 ernannten Nachfolger, dem Mehrheitsaktionär Irving Gould, wurden die Werbemaßnahmen kurzzeitig wieder etwas verstärkt. Danach verzichtete Commodore vollständig auf Werbung und aggressives Marketing. Werbung machte für den C128 fortan – wenn auch eher indirekt – nur noch Berkeley Softworks, der Publisher der 1987 für den Rechner herausgebrachten grafischen Benutzeroberfläche GEOS 128. Slogans wie ‚Wird ihr C128 erwachsen oder alt?‘ (englisch \"„Is your 128 growing up or growing old?“\") bzw. ‚Wissenschaftler der Universität Berkeley stoppen den Alterungsprozess‘ (englisch \"„Scientists at Berkeley stop the aging process“\") zielten darauf ab, dem Rechner trotz seiner in die Jahre gekommenen 8-Bit-Architektur das Image eines immer noch modernen Personal Computers zu verleihen.\n\nIn Großbritannien wurde die Heimcomputerindustrie Mitte der 1980er Jahre von einheimischen Herstellern wie Sinclair, Acorn oder Amstrad beherrscht, während Commodore in den Vereinigten Staaten und Westdeutschland als Branchenführer galt. Das Unternehmen machte daher große Anstrengungen, um die Verkäufe speziell in Großbritannien anzukurbeln. Dazu zählten zahlreiche Sonderangebote.\n\nIn der Vorweihnachtszeit des Jahres 1985 war der C128 im Paket mit dem Diskettenlaufwerk VC1570 für preisgünstige 449,99 £ erhältlich. Flankiert wurde dieses Paket von weiteren Sonderangeboten. So wurde Besitzern des C64 ein Rabatt in Höhe von 50 £ angeboten, sofern sie beim Kauf eines C128 ihren alten Rechner abzugeben bereit waren. Beim Kauf eines C128 wurde außerdem den Besitzern anderer Computermodelle im Austausch für ihre bisherigen Rechner eine kostenlose Datasette des Typs 1530 im Wert von 45 £ als Kaufanreiz in Aussicht gestellt. Die Marketingabteilung verband mit diesem Angebot die Hoffnung auf Umsteiger, die bislang Heimcomputersysteme anderer Hersteller verwendet hatten. Schließlich erhielten die Kunden mit der kostenfreien Datasette Zugriff auf die gesamte, auf Kompaktkassetten preisgünstig zu erwerbende Spielesoftware des C64. Im Vorfeld der Sommerferien des Jahres 1986 führte die Marketingabteilung außerdem spezielle Bündelangebote mit zusätzlichen Kaufanreizen ein. Jedem aus einem C128, einer VC1570 sowie einem Commodore-Monitor bestehenden Paket wurden fünf Gutscheine im Wert von jeweils 50 £ beigelegt. Die Gutscheine konnten in ausgewählten Reisebüros beim Buchen von Pauschalreisen eingelöst werden.\n\nDa zum Betrieb des Rechners im 80-Zeichen-Modus ein relativ kostspieliger und für die meisten britischen Heimanwender daher unerschwinglicher RGBI-Farbmonitor vonnöten war, begann Commodore ab Anfang 1986 neue, auf die vergleichsweise finanzkräftigen Kleinunternehmer ausgerichtete Sonderangebote zu entwickeln. Zu diesem Zweck wurde die noch immer nicht abgeschlossene Entwicklung des bürotauglichen und bereits im Sommer 1985 angekündigten C128D, der Maus des Typs 1530 sowie der Speichererweiterungen forciert. Günstige, aus einem C128D, einem Monochrommonitor und einem Softwarepaket bestehende Bündelangebote zum Preis von 499 £ sollten den Rechner außerdem auch auf dem bis dahin von Acorns BBC Micro und dessen Nachfolger BBC Master beherrschten britischen Bildungsmarkt zu größeren Marktanteilen verhelfen.\n\nUS-amerikanischen Verbrauchern wurde der Rechner in den Monaten nach der Markteinführung zusammen mit einem Freiabonnement für den Online-Dienstanbieter QuantumLink angeboten, über den ab November 1985 auf das zuvor in Zusammenarbeit mit Compuserve betriebene und neben dem Informationsaustausch auch für den Kundendienst in Anspruch genommene Commodore Information Network zugegriffen werden konnte.\n\nWährend der Entwicklungsabteilung von Commodore bei der Herstellung des C64 immer wieder signifikante Kosteneinsparungen gelangen, litt der wesentlich komplexere C128 stets an hohen Produktionskosten und vergleichsweise niedrigen Gewinnmargen. Im Niedrigpreissegment war der Rechner deshalb nur eingeschränkt konkurrenzfähig. Im mittleren Preissegment erreichte der C128 dagegen eine höhere Marktdurchdringung. Die erheblich leistungsfähigeren und allmählich günstiger werdenden 16-Bit-Rechner wie der Atari ST, der 1987 erschienene Amiga 500 und die zahlreichen IBM-PC-Kompatiblen eroberten jedoch nach und nach Marktanteile in diesem für den Absatz des C128 entscheidenden Bereich.\n\nAuf der vom 1. bis 6. November 1987 abgehaltenen Computermesse COMDEX gab die Firmenleitung trotz dieser wachsenden Konkurrenz sogar aus dem eigenen Hause offiziell bekannt, den C128 bei nicht nachlassender Nachfrage auch über Weihnachten 1987 hinaus weiter produzieren zu wollen. Danach näherte sich der Rechner jedoch allmählich dem Ende seiner Marktpräsenz. Bei einer der Ermittlung des Computers des Jahres 1988 dienenden Umfrage unter den Lesern der Computerzeitschrift \"64’er\" landete der C128 nur noch im Mittelfeld, hinter leistungsstärkeren 16-Bit-Rechnern wie dem Apple Macintosh II, dem Amiga, dem Compaq Deskpro, dem IBM Personal System/2 sowie den Modellen der Atari-ST-Serie, aber immerhin noch vor dem C64, den Heimcomputern der Atari-XL-Serie oder dem standardsetzenden IBM-PC/XT/AT. Im Januar 1989 wurde zunächst die Produktion der ursprünglichen Tastaturcomputerversion zugunsten des C128D-CR eingestellt. Außerdem bot Commodore potenziellen Käufern eines Amiga 500 bzw. eines Amiga 2000 in den Vereinigten Staaten einen Preisnachlass in Höhe von 100 US$ im Tausch gegen ihre alten C128-Modelle an. Im März 1989 wurde das 5¼-Zoll-Diskettenlaufwerk VC1571 ebenfalls vom Markt genommen, was eilig dementierte Gerüchte um einen bevorstehende Produktionsstopp auch des C128-DCR auslöste. In Kanada und Westeuropa war die VC1571 aber noch einige Zeit lieferbar. Gleichzeitig häuften sich die in den Computerzeitschriften abgedruckten Beschwerden über den mangelhaften Support des C128 seitens Commodore. Im Juli 1989 entschied die Firmenleitung schließlich, die Produktion des nicht mehr profitablen C128D-CR nunmehr ebenfalls einzustellen.\n\nSinkende Preise machten die noch nicht abverkauften Restexemplare des C128D-CR im Jahr 1990 noch einmal für viele westdeutsche Kleinunternehmer attraktiv, da sich der Rechner gut zur Verwaltung der Firmenfinanzen eignete. Nach dem Abschluss des mit dem zweiten Quartal endenden Commodore-Geschäftsjahres 1989/90 spielte der Rechner in den Firmenbilanzen des Herstellers keine Rolle mehr. Die Mehrheit der C128-Besitzer stieg bis 1991 auf die mittlerweile marktbeherrschenden IBM-PC-kompatiblen Rechner mit XT- oder AT-Architektur bzw. andere Plattformen mit leistungsstärkeren 16-Bit-Hauptprozessoren wie den Amiga um. Von wenigen Ausnahmen wie der Textverarbeitung \"Wordstar 128,\" der Datenbankanwendung \"dBase II\" oder der Tabellenkalkulation \"Microsoft Multiplan\" abgesehen, waren viele kommerzielle Anwendungsprogramme für den CP/M-Modus zu diesem Zeitpunkt bereits nicht mehr im Handel erhältlich, da MS-DOS in der Zwischenzeit CP/M bereits als faktisches Standardbetriebssystem abgelöst hatte.\n\nGelegentlich wurden in Deutschland im Jahr 1991 noch unverkaufte, aus den übrigen EG-Staaten reimportierte Restexemplare des C128D-CR aus alter westdeutscher Produktion für 499 DM in diversen Warenhäusern angeboten. Peripheriegeräte wie etwa das 3½-Zoll-Diskettenlaufwerk VC1581, Speichererweiterungen und kommerzielle Software für den C128-Modus bzw. den Betrieb unter CP/M-Plus waren zu diesem Zeitpunkt allerdings praktisch nur noch in den Vereinigten Staaten erhältlich. Um diese Versorgungsengpässe zu beseitigen, wurden 1992 in Deutschland Nachbauten des Diskettenlaufwerks VC1571 sowie der 512-kB-Speichererweiterung 1750 vom Hardwarehersteller CEUS-Computersysteme auf den Markt gebracht.\n\nIn den Vereinigten Staaten wurde der C128D-CR noch bis Mitte 1991 vom Versandgroßhändler Montgomery Grant inklusive eines Gratis-Computerspiels für 399 US$ angeboten. Bis 1997 wurden ferner instandgesetzte Gebrauchtexemplare des Rechners vom Hardwarehersteller Creative Micro Designs zum Verkauf inseriert. Dabei erzielten beide Modellvarianten allmählich ansteigende Preise, die beim C128 zwischen 129 US$ und 159 US$, beim C128D-CR zwischen 239 US$ und 299 US$ lagen.\n\nZum Zeitpunkt der Markteinführung ging die Firmenleitung von einer Million verkaufter Exemplare des C128 bis Ende 1986 aus. Tatsächlich verkaufte sich der Rechner anfangs ausgesprochen gut. Im Juni 1985 gab es bereits 100.000 Vorbestellungen. Bis Ende 1985 konnten weltweit 425.000 Einheiten abgesetzt werden, davon 60.000 in Westdeutschland. Commodore stellte Anfang September 1985 sogar 350 neue Arbeitskräfte ein, um den C128 und seinen Vorgänger überhaupt in ausreichender Menge produzieren zu können. Bis zur CEBIT im März 1986 wurden weltweit fast 500.000 Exemplare verkauft, was Harald Speyer, Chef des deutschen Zweiges von Commodore International, in einem Interview als bis dahin „erfolgreichste Markteinführung aller Zeiten“ bezeichnete. Bis Mitte 1986 wurden allein in den Vereinigten Staaten 600.000 Einheiten abgesetzt. Dort galt der C128 zu diesem Zeitpunkt als einer der sich am schnellsten verkaufenden Computer der jüngeren US-amerikanischen Technikgeschichte. Außerhalb Nordamerikas verlief der Absatz jedoch schleppender. Von den weltweit bis August 1986 verkauften ca. 800.000 Einheiten entfielen beispielsweise lediglich 10 Prozent auf den von Commodore beherrschten westdeutschen Markt, also gerade einmal 80.000 Muster. Trotzdem war der Rechner durchaus ein Verkaufserfolg: Commodore-Geschäftsführer Rattigan bestätigte in einem Interview vom Frühjahr 1987, dass bis Ende 1986 tatsächlich weltweit rund eine Million Einheiten des C128 abgesetzt worden seien. Damit hatten sich die ursprünglichen Erwartungen der Firmenspitze erfüllt.\n\nBis Juli 1987 stieg die Zahl der in Westdeutschland verkauften Exemplare sämtlicher C128-Modellversionen auf 210.000. Das entspricht einem Anteil von 10,67 Prozent aller dort bis zu diesem Zeitpunkt verkauften Commodore-Rechner. Im April 1988 lag die geschätzte Zahl der nordamerikanischen C128-User bereits bei 1,5 Millionen. Besonders populär war der C128 dort unter bereits einen Commodore-Rechner besitzenden Anwendern. 78 Prozent dieses Personenkreises gaben laut einer im Mai 1986 von der US-amerikanischen Computerzeitschrift \"Run\" veröffentlichten Umfrage an, sich in naher Zukunft einen C128 anschaffen zu wollen. Die Firmenleitung selbst hatte dagegen nur mit 28 Prozent gerechnet. Im August 1988 überschritt die Zahl der weltweit verkauften Einheiten die Zwei-Millionen-Grenze. Mit insgesamt vier Millionen weltweit bis 1990 abgesetzten Einheiten erreichte der C128 schließlich im Großen und Ganzen durchaus „akzeptable Verkaufszahlen“.\n\nMit 284.300 bis 1990 verkauften Einheiten blieb der C128 in Westdeutschland allerdings weit hinter den 3,05 Millionen abgesetzten Exemplaren des Vorgängers C64 zurück. Die Verkaufszahlen lagen damit auf dem gleichen Niveau wie die der 1984 zur Marktreife gelangten, gemeinhin als Flops geltenden Modelle der Commodore-264-Serie. Allerdings erklären sich die relativ hohen Verkaufszahlen dieser Modellreihe vor allem durch die Schleuderpreise, zu denen die Geräte ab 1985 nach einer Marktpräsenz von lediglich einem Jahr in den Filialen der Supermarktkette Aldi abverkauft wurden. Im Übrigen fanden sich auch unter den westdeutschen C128-Besitzern viele treue Commodore-Kunden. Die meisten hatten zuvor bereits einen C64 oder einen Plus/4 erworben.\n\nBis zur deutschen Wiedervereinigung kamen laut einer im \"SPIEGEL\" veröffentlichten Schätzung rund 200.000 Heimcomputer aus westlicher Produktion in die DDR, die meisten davon als Privatimport im Reisegepäck. Darunter war auch eine unbekannte Anzahl an Exemplaren des C128. Bei einer von der Computerzeitschrift \"64’er\" im Frühjahr 1990 durchgeführten Umfrage gaben 26 Prozent der westdeutschen Befragten an, im Besitz eines C128 zu sein. In der DDR lag der Anteil des C128 dagegen bei lediglich 11 Prozent.\n\nWährend der Marktpräsenz der C128 blieben die Preise für den Rechner in den Vereinigten Staaten relativ konstant. Die zunächst vom Hersteller anvisierte unverbindliche Preisempfehlung von unter 300 US$ für die Tastaturcomputerversion – mehrere Quellen sprechen sogar von gerade einmal 250 US$ als ursprünglich geplantem Einführungspreis – ließ sich aufgrund der hohen Produktionskosten jedoch nicht dauerhaft aufrechterhalten. Sie lag ab dem vierten Quartal 1985 stets zwischen 349 US$ und 399 US$. Die unverbindliche Preisempfehlung für den C128D-CR pendelte zwischen 549 US$ und 599 US$.\n\nDie Straßenpreise des oft im Bündel mit einem Diskettenlaufwerk sowie einem Monitor in Kaufhäusern, Fachgeschäften und von zahlreichen Versandgroßhändlern wie etwa Lyco Computer, Protecto, Computer Direct oder Montgomery Grant angebotenen Rechners lagen meist recht deutlich unter den Listenpreisen Commodores, vor allem in der Vorweihnachtszeit. Im Dezember 1987 beispielsweise verlangte Montgomery Grant 219,95 US$ für einen C128 und lag damit rund 130 US$ unterhalb der aktuellen Preisempfehlung. Zur gleichen Zeit bot Lyco Computer den C128D-CR für 439,95 US$ an, rund 160 US$ unterhalb des gültigen Listenpreises.\n\nIn Großbritannien lagen die Versandgroßhandelspreise anfänglich kaum unter der stabil bleibenden unverbindlichen Preisempfehlung von 269 £ für den C128 bzw. 499 £ für den C128D. Bei Dimension Computers kostete die Tastaturcomputerversion im Januar 1986 beispielsweise volle 269,95 £, für das Desktopmodell wurden 499,95 £ verlangt. Evesham Micros sowie HiVoltage dagegen boten die Tastaturcomputerversion zur gleichen Zeit etwas günstiger für 259 £ an. 489,95 £ verlangte HiVoltage für einen C128D.\n\nErst nach einiger Zeit wurde der Rechner schließlich auch im Vereinigten Königreich deutlich unter dem offiziellen Listenpreis angeboten. Beispielsweise inserierte Dimension Computers die Tastaturcomputerversion im Dezember 1987 für 199,95 £ und damit rund 70 £ günstiger als von Commodore empfohlen. Für den mittlerweile an die Stelle des C128D gerückten C128D-CR wurden dagegen 399,95 £ verlangt, also rund 100 £ weniger als die unverbindliche Preisempfehlung des Herstellers.\n\nIn Westdeutschland bewegten sich die Preise für den Rechner zunächst im Vergleich zu Großbritannien auf einem erheblich höheren Niveau. Nur wenige Versandgroßhändler verlangten in den ersten Monaten der Marktpräsenz des C128 im Herbst 1985 allerdings tatsächlich einen Preis in Höhe der unverbindlichen Preisempfehlung von 1.198 DM für die Tastaturcomputerversion. Zu diesen Anbietern gehörte im September 1985 der Hard- und Software-Vertrieb H. Steber (kurz HSV Steber). Schon im November 1985 reduzierte HSV Steber den Preis für den C128 allerdings auf 1.098 DM.\n\nDie große Mehrheit der Versandgroßhändler veranschlagte dagegen am Ende des dritten bzw. Anfang des vierten Quartals 1985 bereits Preise unterhalb der verkaufspsychologisch wichtigen 1.000-DM-Grenze. Neckermann, die IES Computerhandelsgesellschaft, Computer Reschke sowie Valasik-Computer etwa verlangten 998 DM für einen C128. Bei Abacomp kostete der Rechner 960 DM. Der Computer- und Softwarevertrieb Riegert (kurz CSV Riegert) offerierte den C128 im September 1985 für 949 DM und im Oktober 1985 für 929 DM. Mit einem Preis von 898 DM blieb der CC-Computerversand sogar unterhalb der 900-DM-Grenze und damit 300 DM unter der unverbindlichen Preisempfehlung des Herstellers. Zwecks Ankurbelung der Weihnachtsverkäufe ging schließlich auch HSV Steber im Dezember 1985 mit dem Preis auf 998 DM herunter.\n\nDie Handelskette Vobis bot den C128 im Januar 1986 für 975 DM und den C128D für 1.785 DM an. Im weiteren Verlauf des Jahres 1986 sanken die Versandgroßhandelspreise sowohl für die Tastaturcomputerversion als auch das Desktopmodell jedoch spürbar. Die ProSoft GmbH offerierte Ende des ersten Quartals 1986 den C128 für 798 DM und den C128D für 1.698 DM. Zum gleichen Zeitpunkt verlangte Dela Elektronik 899 DM für den C128. CSV Riegert reduzierte bis zum Ende des zweiten Quartals 1986 den Preis für den C128 auf 749 DM. Ende des dritten Quartals verlangte CSV Riegert noch 679 DM für den C128 und 1.475 DM für den C128D.\n\nZum Weihnachtsgeschäft 1986 bot ProSoft den C128 für 679 DM und den C128D für 1.288 DM an. Zum gleichen Zeitpunkt verlangte CSV Riegert nach wie vor 679 DM für den C128, aber nur noch 1.299 DM für den C128D. Abacomp offerierte den C128 unterdessen für 665 DM, den C128D für 1.368 DM. Andere Versandgroßhändler wie Computertechnik Luda, Computer Discount München oder die Syndrom Computer GmbH boten das Tastaturcomputermodell in der Vorweihnachtszeit ebenfalls durchgehend unterhalb der 700-DM-Grenze an.\n\nAm Ende des ersten Quartals des Jahres 1987 verlangte ProSoft nach wie vor 679 DM für den C128, aber nur noch 1.279 DM für den mittlerweile auf den Markt gekommenen C128D-CR. CSV Riegert führte dagegen den C128 zu diesem Zeitpunkt bereits nicht mehr im Sortiment und bot den C128D-CR für nunmehr 1.169 DM an. Bei Vobis kostete der C128D-CR zum gleichen Zeitpunkt noch 1.248 DM. Im zweiten Quartal reduzierte ProSoft die Preise für den C128 auf 630 DM und für den C128D-CR auf 1.099 DM. CSV Riegert ging währenddessen nur um 20 DM mit dem Preis des C128D-CR auf 1.149 DM herunter. Bei Abacomp war der Rechner mittlerweile für 580 DM in der Tastaturcomputerversion und für 1.140 DM als Desktopmodell erhältlich. Im dritten Quartal 1987 verlangte Vobis 569 DM für einen C128. CSV Riegert reduzierte den Preis für den C128D-CR im gleichen Zeitraum um weitere 100 DM auf 1.049 DM. Abacomp blieb dagegen bei 1.140 DM für den C128D-CR. ProSoft inserierte zu diesem Zeitpunkt den Rechner bereits nicht mehr.\n\nMit Beginn des vierten Quartals des Jahres 1987 wurde die unverbindliche Preisempfehlung seitens Commodore auf 499 DM für den C128 und 999 DM für den C128D-CR nach unten korrigiert. Fortan unterschritten zahlreiche Anbieter erstmals die 1.000-DM-Marke beim Desktopmodell. So konnte der C128D-CR bei Quelle sowie Abacomp für 998 DM bestellt werden. CSV Riegert bot den C128D-CR für 969 DM an. Bei Zweifach Computer mussten die Kunden im Vorweihnachtsgeschäft des Jahres 1987 für einen C128 nur noch 444 DM und für einen C128D-CR 958 DM bezahlen. Beim Tornado Computervertrieb waren es dagegen 549 DM für den C128 und 979 DM für den C128D-CR.\n\nIn der ersten Hälfte des Jahres 1988 verschwand das Tastaturcomputermodell allmählich aus den Sortimenten der meisten Großhandelsketten und Versandgroßhändler. Gleichzeitig änderte sich kaum etwas an den für den C128D-CR verlangten Preisen. Erst Mitte des Jahres 1988 kam wieder Bewegung in das Preisgefüge. Vobis bot das Desktopmodell im Sommer 1988 für 899 DM an. CSV Riegert reduzierte den Preis für den C128D-CR im dritten Quartal des Jahres 1988 auf 929 DM. Zweifach Computer folgte diesem Beispiel im vierten Quartal mit 888 DM.\n\n1989 erfolgten unter dem Eindruck der Produktionseinstellung sämtlicher Modellvarianten sowie des Aufstiegs der leistungsstärkeren 16-Bit-Computer wie dem konzerneigenen Amiga, der Atari-ST-Serie oder den IBM-PC-Kompatiblen weitere, teils deutliche Preisrücknahmen, die den C128D-CR bzw. noch unverkaufte Restexemplare der Tastaturcomputerversion vor allem für Einsteiger interessant machen sollten. Dabei lagen die Preise für das Desktopmodell nunmehr durchgehend unter der 700-DM-Grenze bzw. für das Tastaturcomputermodell unter der 350-DM-Marke. Bis zum Weihnachtsgeschäft 1989 reduzierte etwa Zweifach Computer die Preise für den C128 auf 333 DM und den C128D-CR auf 666 DM. Bei CSV Riegert kostete ein C128D-CR zum gleichen Zeitpunkt noch 699 DM.\n\nIm Verlauf des Jahres 1990 nahmen viele Versandgroßhändler den mittlerweile technisch überholten Rechner schließlich aus ihrem Sortiment. Erhältlich waren sowohl der C128 als auch der C128D-CR aber immer noch. Im Juni 1990 verlangte Zweifach Computer 333 DM für ein aus einem C128 sowie einem Joystick und zwei Spielen bestehendes Bündelangebot, während der C128D-CR zum gleichen Zeitpunkt 577 DM kostete. Bei Vobis war der C128D-CR zu diesem Zeitpunkt für 599 DM erhältlich. Deutlich teurer war der Rechner Mitte 1990 in westdeutschen Kaufhausketten wie Karstadt oder Horten. Letzte Restexemplare des C128 schlugen dort mit rund 450 DM zu Buche, ein C128D-CR kostete durchschnittlich 850 DM. Weihnachten 1990 verlangte Zweifach Computer nur noch 299 DM für das C128-Bündelangebot und 555 DM für einen C128D-CR. Ende 1990 lagen die Gebrauchtmarktpreise für einen C128 bei ca. 200–300 DM, für einen C128D-CR bei 300–530 DM.\n\nWährend hinsichtlich des Einsatzes von Peripheriegeräten viele Gemeinsamkeiten zwischen nordamerikanischen und westeuropäischen Anwendern des C128 bestanden, lassen sich bemerkenswerte Unterschiede insbesondere im Hinblick auf die Verwendung des Rechners im Alltag bzw. im Bildungssystem feststellen.\n\nIn Nordamerika nutzten die C128-Besitzer ihre Rechner wesentlich häufiger zur Datenfernübertragung als die C64-User. Auch im Bereich der Anwendungsprogramme wurde der C128 häufiger eingesetzt als sein marktführender Vorgänger. Gegen Ende der 1980er Jahre kam das Desktop-Publishing als neues Einsatzgebiet hinzu.\n\nZu den beliebtesten Einsatzgebieten des C128 gehörten im deutschsprachigen Raum Anwendungen wie etwa das Erstellen und Ausdrucken von Texten, der CP/M-Modus sowie das Programmieren in BASIC oder Assemblersprache, während der Rechner nur selten zur Datenfernübertragung oder zum Spielen eingesetzt wurde. Neben meist jugendlichen Männern zählten auch Frauen in Westdeutschland zur Anwenderbasis des C128.\n\nAn Schulen, Universitäten und anderen Bildungseinrichtungen in den Vereinigten Staaten konnte sich der C128 nicht gegen das bereits seit den 1970er Jahren den Markt dominierende Erfolgsmodell Apple II durchsetzen. Der Rechner wurde aber privat von Schülern und Studenten etwa für das Abfassen von Hausaufgaben oder Seminararbeiten verwendet. In Westeuropa dagegen gelang es dem C128D bis zum Frühjahr 1986, in mehreren westdeutschen Bundesländern sowie in Belgien zum offiziellen Schulcomputer zu avancieren.\n\nDiese Entscheidungen machten insbesondere die Desktop-Modellvarianten in Westeuropa für neue Produkte im Bereich der Lernsoftware interessant. So zeigten auf der vom 16. bis 20. Februar 1987 abgehaltenen Didacta, der jährlichen Fachmesse für den Bereich Schule und Ausbildung, mehrere Softwarefirmen den C128D-CR beispielsweise als Rechner zur Steuerung physikalischer Experimente im Schulunterricht. Der bekannte dänische Spielzeughersteller Lego präsentierte die Steuersoftware seiner neuen Produktreihe Lego Technic Control bei gleicher Gelegenheit ebenfalls auf einem C128D-CR. Auf der vom 16. bis 23. März 1988 abgehaltenen CeBIT präsentierte neben Lego auch Fischertechnik den zum Erlernen des Einsatzes von Computern in den Bereichen Messen, Steuern und Regeln gedachten Baukasten \"Computing Experimental\" auf einem C128D. Darüber hinaus wurde der Rechner unter Verwendung von Desktop-Publishing-Software zum Erstellen von Schülerzeitungen verwendet und ermöglichte so Jugendlichen erste Einblicke in die Welt des Journalismus.\n\nDer C128 baut technisch auf seinem Vorgänger C64 auf. Der Rechner verfügt aber über eine verbesserte Tastatur, mehr Schnittstellen mit gegenüber dem C64 erweiterter Funktionalität sowie einen wesentlich umfangreicheren und technisch leistungsfähigeren Chipsatz mit Bausteinen, die größtenteils vollständig abwärtskompatible Weiterentwicklungen der im Vorgängermodell verwendeten Bausteine darstellen. Die sehr komplexe 8-Bit-Architektur des C128 besteht ferner aus zwei Hauptprozessoren, zwei Grafikchips, zwei I/O-Bausteinen, zwei Speicherverwaltungseinheiten, einem Soundchip sowie einer Reihe von Speicherchips, die über einen für damalige Verhältnisse außergewöhnlich aufwändig gestalteten Systembus miteinander Daten austauschen können.\n\nWeder die Hardwareeigenschaften noch die Systemsoftware des C128 lassen eine eindeutige Zuordnung zu einer bestimmtem Geräteklasse zu. Für eine Zuordnung zu den Heimcomputern sprechen die 8-Bit-Architektur, die Verwendung eines herstellereigenen Betriebssystems in Gestalt eines nativen BASIC-Dialekts, das Vorhandensein von Anschlüssen für zwei Joysticks und eine Datasette, die Kunststoffgehäuse der Modellvarianten C128 bzw. C128D sowie der vergleichsweise niedrige Preis. Dagegen legen die Fähigkeit zur Darstellung von 80 Zeichen pro Zeile, das integrierte 5¼-Zoll-Diskettenlaufwerk bei den Modellvarianten C128D sowie C128D-CR, das Stahlblechgehäuse des C128D-CR und schließlich die Verwendung des Standard-Betriebssystems CP/M eine Zurechnung zu den Personal Computern bzw. Arbeitsplatzrechnern nahe. Entsprechend erschien der C128 in der zeitgenössischen Wahrnehmung als „Mischung zwischen Spiele-Computer und Profimaschine“ bzw. als \"„general-purpose computer“\" (deut. Allzweckrechner).\nDer erste im C128 verwendete Hauptprozessor MOS 8502 besitzt 40 Anschlusspins und stellt eine Weiterentwicklung des im C64 verwendeten MOS 6510 dar. Er wurde eigens für den C128 in HMOS-II-Technologie entwickelt und steuert sowohl den C64- als auch den C128-Modus. Der eine typische 8-Bit-Prozessorarchitektur aufweisende MOS 8502 verfügt über acht Daten- sowie 16 Adressleitungen. Außerdem weist er einen Programmzähler (PC), einen Akkumulator (AC), ein Statusregister (SR), zwei Indexregister (XR, YR), einen Stackpointer (SP), eine Interruptlogik, einen Timer sowie eine als elektronisches Rechenwerk fungierende, für sämtliche logischen sowie arithmetischen Operationen zuständige arithmetisch-logische Einheit (englisch \"Arithmetic Logic Unit\", kurz ALU) auf. Zur Steuerung der RAM-Chips, ROM-Chips, I/O-Bausteine, Datasette sowie der Feststelltaste beim US-amerikanischen Tastaturlayout bzw. der Zeichensatz-Umschalttaste bei den nicht für die englischsprachigen Länder produzierten Versionen des C128 besitzt der MOS 8502 außerdem ein spezielles 7-Bit-Datenrichtungsregister zur Festlegung der Datenflussrichtung sowie ein zugehöriges Datenregister zur Auswahl der genannten Systemkomponenten.\n\nPer Softwaresteuerung lässt sich der MOS 8502 wahlweise mit einer langsameren Taktfrequenz von 0,985 MHz (PAL-Version) bzw. 1,02 MHz (NTSC-Version) sowie einer schnelleren Taktfrequenz von 1,97 MHz (PAL) bzw. 2,04 MHz (NTSC) betreiben. Damit ist er im 2-MHz-Modus theoretisch etwa doppelt so schnell wie der MOS 6510. Da beide CPUs über den gleichen Befehlssatz verfügen, sind sie zueinander vollständig softwarekompatibel. Auch hinsichtlich der Adressierungsarten gleichen sich MOS 6510 und MOS 8502. Unterschiede bestehen dagegen bei den Pinbelegungen.\n\nGeneriert wird die Taktfrequenz des MOS 8502 vom Taktbaustein MOS Technology 8701, der seinerseits mit einem externen Schwingquarz verbunden und sowohl zur in Westeuropa verbreiteten PAL-Fernsehnorm als auch zum nordamerikanischen NTSC-Standard kompatibel ist. Allerdings muss das Videosignal des für die Darstellung von 40 Zeichen pro Zeile verantwortlichen Grafikchips VIC IIe im 2-MHz-Modus des MOS 8502 abgeschaltet werden. Nach dem Vorbild des MOS 6510 verwendet auch der MOS 8502 zwecks Speicherverwaltung die ersten 256 Bytes des Arbeitsspeichers als Zeropage. Überdies weist er wie sein Vorgänger insgesamt 4.000 Transistoren auf.\n\nMit dem Z80A des US-amerikanischen Chipherstellers Zilog besitzt der C128 einen weiteren Hauptprozessor mit typischer 8-Bit-Prozessorarchitektur, der mit einer Taktfrequenz von bis zu 4 MHz betrieben werden kann, aus Gründen der Synchronisation mit dem MOS 8502 jedoch effektiv nur auf maximal 2,04 MHz getaktet ist. Als Taktbaustein fungiert der 40-Zeichen-Grafikchip VIC IIe. Der als Zweitprozessor agierende und in NMOS-Logik ausgeführte Z80A dient der Steuerung des C128 im CP/M-Modus. Er besteht aus 8.500 Transistoren und verfügt über 40 Anschlusspins mit acht Daten- und 16 Adressleitungen. Mit einer maximalen Speicherzugriffszeit von 380 Nanosekunden zählt der Z80A auf diesem Gebiet zu den überdurchschnittlich schnellen 8-Bit-Hauptprozessoren.\n\nIm Gegensatz zum speicherorientierten MOS 8502 handelt es sich beim aus dem Intel 8080 hervorgegangenen Z80A wie bei sämtlichen CPUs der im IBM-PC, IBM-PC XT und IBM-PC AT verbauten Intel-80xxx-Familie um einen registerbezogenen Hauptprozessor. Trotz seiner doppelt so hohen Taktfrequenz ist der Z80A zwar schneller, aber nicht doppelt so schnell wie der MOS 8502. Für die Abarbeitung von Maschinenbefehlen braucht der Z80A nämlich häufig mehr Taktzyklen als der MOS 8502 – ein Nachteil, der nur zum Teil durch das beim Z80A in die Prozessorarchitektur integrierte Pipelining ausgeglichen wird, das der Zilog-CPU erlaubt, während der Bearbeitung des aktuellen Maschinenbefehls bereits einen neuen Befehl zu laden.\n\nDer C128 ist neben dem in den Vereinigten Staaten als SuperPET bekannten MMF 9000, dem CBM 630 sowie dem CBM 730 der einzige 8-Bit-Rechner von Commodore, in dem eine nicht vom konzerneigenen Halbleiterhersteller MOS Technology stammende CPU verbaut wurde. Der Z80A ermöglicht dem Rechner das Ausführen von Software, die für das Betriebssystem CP/M-Plus geschrieben wurde. Da die beiden Hauptprozessoren MOS 8502 und Z80A nicht gleichzeitig, sondern ausschließlich seriell operieren können, stellt der C128 kein Multiprozessorsystem dar.\n\nEine Besonderheit des C128 stellt die Ausstattung des Geräts mit gleich zwei 8-Bit-Grafikchips dar, von denen einer für die Bildschirmausgabe im 40-Zeichen-Modus, der andere für die Bildschirmausgabe im 80-Zeichen-Modus verantwortlich ist. Da beide Grafikchips ihr eigenes Videosignal erzeugen und über eigene Schnittstellen zur Bildausgabe verfügen, können im C128-Modus bei aktiviertem 80-Zeichen-Modus gleichzeitig zwei Monitore am C128 betrieben werden. Dabei dient der 80-Zeichen-Bildschirm zur Eingabe von Befehlen über den BASIC-Interpreter und zur Textausgabe, während der 40-Zeichen-Bildschirm zur Grafikausgabe verwendet wird. Von beiden Grafikchips wurden mehrere Versionen entwickelt und in den verschiedenen Modellvarianten des C128 verbaut.\n\nFür den ersten im C128 verbauten 8-Bit-Grafikchip des Typs MOS 8563 bürgerte sich die Abkürzung VDC ein, die für die in der englischsprachigen Welt übliche, aber auch im deutschsprachigen Raum gebräuchliche Bezeichnung Video Display Controller steht. Der mit 42 Anschlusspins versehene MOS 8563 kommt in den Modellvarianten C128 sowie C128D zum Einsatz und ist für den Bildschirmaufbau im hochauflösenden 80-Zeichen-Modus verantwortlich. Der in der HMOS-II-Technologie hergestellte Grafikchip übernimmt nicht nur die Erzeugung des CGA-kompatiblen RGBI-Videosignals, sondern verwaltet mithilfe seiner 16 Adressleitungen, die einen Adressraum von bis zu 64 kB ermöglichen, auch den in der Grundkonfiguration ab Werk eingebauten dynamischen Grafikspeicher von 16 kB VRAM direkt. Dieser besteht aus einem 2-kB-Bildwiederholspeicher, einem 2-kB-Farbspeicher bzw. Attribut-RAM und einem 8-kB-Zeichensatzspeicher, während die restlichen 4 kB Grafikspeicher ungenutzt bleiben.\n\nDer MOS 8563 besitzt überdies 37 interne Register. Mithilfe der Register lassen sich zahlreiche Parameter einstellen, beispielsweise die Anzahl der Zeichen pro Zeile, die Pixelbreite, der Darstellungsmodus, die Bildauflösung, die Farben für Vorder- und Hintergrund, die Cursoreinstellungen usw. Für eine Programmierung des MOS 8563 ist das native Commodore BASIC V7.0 des C128 allerdings zu langsam. Daher musste der 80-Zeichen-Grafikchip in maschinennahen Programmiersprachen wie etwa der Assemblersprache programmiert werden. Der MOS 8563 beherrscht ferner etliche Bildformate, darunter auch die Fernsehnormen PAL und NTSC.\n\nDer MOS 8563 verfügt über eine Farbtiefe von 4 Bit und damit über eine Palette von 16 Farben, wobei die Farbwerte über den Farbspeicher bzw. das Attribut-RAM programmiert werden können. Zwar gestattet der MOS 8563 keine Darstellung von Sprites und ist daher nur eingeschränkt für die Spieleprogrammierung tauglich, erlaubt aber dafür einen sanften Bildlauf (englisch \"smooth scrolling\") in horizontaler sowie vertikaler Richtung. Außerdem ist der MOS 8563 in der Lage, Rastergrafiken und Bobs (Abkürzung für englisch \"blitter objects\") über den Bildschirm zu bewegen. Zu diesem Zweck stehen spezielle Verschiebebefehle (englisch \"block movement commands\") zur Verfügung, die das schnelle Kopieren und Transferieren zusammenhängender Speicherinhalte gestatten (englisch \"Bit Block Image Transfer\").\n\nMittels der Systemroutinen zur Bildschirmausgabe werden die Register des MOS 8563 so gesetzt, dass zwischen einem Textmodus mit einer für die Textverarbeitung geeigneten Standardeinstellung von 80 × 25 Zeichen sowie einem Grafikmodus mit einer Standardauflösung von 640 × 200 Bildpunkten hin und her geschaltet werden kann.\n\nIm Textmodus verfügt der MOS 8563 sowohl über einen Buchstabenzeichensatz mit Groß- und Kleinbuchstaben als auch einen Grafikzeichensatz, die im Gegensatz zum C64 allesamt gleichzeitig auf dem Bildschirm dargestellt werden können. Über Veränderungen am Attribut-RAM lassen sich blinkende, unterstrichene oder inverse Buchstaben anzeigen. Genau wie die übrigen 8-Bit-Rechner von Commodore verwendet auch der MOS 8563 – sofern kein landestypischer Zeichensatz aktiviert ist – in der Standardeinstellung den mit einer Punktmatrix von 8 × 8 Pixeln pro Zeichen arbeitenden CBM-ASCII-Zeichensatz. Dieser wird im 80-Zeichen-Modus zunächst vom Zeichensatz-ROM in den zum Grafikspeicher gehörenden Zeichensatzspeicher kopiert, weshalb die gewünschten Zeichen erst mit kurzer Verzögerung auf dem Bildschirm erscheinen. Die Größe der Buchstabenmatrix kann ebenfalls verändert werden. Möglich sind bis zu 32 × 8 Pixel pro Zeichen.\n\nIm Grafikmodus erreicht der C128 in der Grundkonfiguration mit seinen voreingestellten 640 × 200 Bildpunkten eine den wesentlich teureren 16-Bit-Rechnern IBM-PC und der NTSC-Version des Amiga 1000 ebenbürtige Standardauflösung. Monochrome Bitmap-Grafiken verbrauchen allerdings in dieser Auflösung bereits die gesamten 16 kB VRAM der frühen Modellvarianten C128 sowie C128D. Mehrfarbige Bitmap-Grafiken oder höhere Auflösungen setzten also einen Ausbau des dedizierten Grafikspeichers voraus.\n\nZusätzlich gibt es noch – wie bei den Rechnern der Amiga-Reihe – einen allerdings weder vom Betriebssystem unterstützten noch von professioneller Software mit nennenswerter Regelmäßigkeit eingesetzten Interlace-Modus, der durch Verwendung zweier gegeneinander versetzter Halbbilder bei allerdings verminderter Bildqualität die Darstellung von bis zu 80 × 50 Zeichen und eine Auflösung von 640 × 400 Bildpunkten gestattet. Zu diesem Zweck werden sowohl der Bildwiederholspeicher als auch der Farbspeicher zulasten des vom Betriebssystem ungenutzten Grafikspeicherbereichs auf jeweils 4 kB verdoppelt. Prinzipiell sind im Interlace-Modus auch noch etwas höhere Auflösungen als die erwähnten 640 × 400 Bildpunkte möglich, beispielsweise 640 × 536 Bildpunkte.\n\nIm ab Werk über einen vollausgebauten Grafikspeicher von 64 kB VRAM verfügenden C128D-CR kam ab 1987 eine uneingeschränkt softwarekompatible Weiterentwicklung des MOS 8563 namens MOS Technology 8568 (kurz MOS 8568) mit identischer Grafikleistung zum Einsatz. In den neuen Grafikchip sind allerdings Logikfunktionen integriert, die in den Vorgängermodellen C128 sowie C128D von externen Bauteilen erfüllt wurden und über Glue Logic mit dem ursprünglichen MOS 8563 verbunden waren. Durch den höheren Grad der Integration sparte Commodore mit der Einführung des MOS 8568 an den Herstellungskosten, ohne dabei Einbußen bei Leistung, Zuverlässigkeit oder Softwarekompatibilität zu riskieren. Außerdem besitzt der MOS 8568 ein zusätzliches, also insgesamt 38 Register. Ermöglicht wird dadurch der Einsatz eines IBM-PC-kompatiblen EGA-Monitors. Da die Pinbelegungen voneinander abweichen, können die beiden Versionen des VDC nicht untereinander ausgetauscht werden.\n\nDie Fähigkeiten des MOS 8568 zur Textausgabe gleichen denen des Vorgängers. Durch den auf 64 kB VRAM vergrößerten Grafikspeicher können im Grafikmodus mit dem MOS 8568 jedoch noch höhere Auflösungen generiert werden als mit dem MOS 8563 in der Grundausstattung mit 16 kB VRAM. Unter der Voraussetzung eines Grafikspeichervollausbaus sind diese nochmals höheren Auflösungen auch mit dem MOS 8563 auf den älteren Modellvarianten C128 sowie C128D realisierbar. Dazu ist allerdings eine sorgfältige Abstimmung der VDC-Register notwendig. So lassen sich beispielsweise Auflösungen mit 720 × 350, 720 × 400, 750 × 300 oder 750 × 400 Bildpunkten erreichen.\n\nDarüber hinaus können aber noch zahlreiche weitere Bildformate umgesetzt werden. Das im Jahr 1988 zum Abtippen in einer Computerzeitschrift erschienene, vom mehrfach Oscar-nominierten Science-Fiction-Filmklassiker \"Tron\" aus dem Jahr 1982 inspirierte Geschicklichkeitsspiel \"Super-Vectors\" etwa arbeitet mit einer Auflösung von 736 × 354 Bildpunkten. Im Interlace-Modus können mithilfe der 64 kB VRAM sogar Auflösungen mit 750 × 600, 752 × 600, 640 × 720 oder 720 × 700 Bildpunkten realisiert werden.\n\nDer zweite im C128 verwendete, für den 40-Zeichen-Bildschirm zuständige 8-Bit-Grafikchip wurde in drei Ausführungen mit jeweils 48 Anschlusspins gefertigt. Die dem NTSC-Standard entsprechende Version erhielt die Bezeichnung MOS Technology 8564, die zur PAL-B-Fernsehnorm kompatible Variante wurde als MOS Technology 8566, die zur in Argentinien, Uruguay und Paraguay üblichen PAL-N-Fernsehnorm kompatible Version schließlich als MOS Technology 8569 bezeichnet. Alle drei Varianten des 40-Zeichen-Grafikchips sind besser unter der Kollektivbezeichnung VIC IIe bekannt.\n\nDer VIC IIe verfügt über eine Farbtiefe von 4 Bit, kann also je nach gewähltem Text- bzw. Grafikmodus bis zu 16 Farben sowie acht Sprites in drei verschiedenen Größen gleichzeitig auf dem Bildschirm darstellen. Von wenigen Erweiterungen und acht zusätzlichen Anschlusspins am DIP-Gehäuse abgesehen ist der VIC IIe nahezu identisch mit dem im C64 verwendeten Grafikchip VIC II. So unterscheiden sich beide Grafikchips etwa hinsichtlich der vom Betriebssystem unterstützten Text- und Grafikmodi sowie des für die Bildschirmausgabe in Anspruch genommenen Speicherplatzes nicht signifikant voneinander.\n\nDer VIC IIe verfügt wie sein Vorgänger über zwei Textmodi mit 40 × 25 Zeichen. Im einfachen Textmodus (englisch \"Text Mode\") kommt pro Zeichen eine Punktmatrix von 8 × 8 Bildpunkten in zwei frei wählbaren Farben für Vorder- und Hintergrund zum Einsatz, während im Mehrfarben-Textmodus (englisch \"Multicolor Text Mode\") pro Zeichen lediglich 8 × 4 Bildpunkte mit doppelter Breite verwendet werden, für den Vordergrund aber gleich drei Möglichkeiten der Farbauswahl gleichzeitig zur freien Verfügung stehen. Außerdem lassen sich mehrfarbige Bitmap-Grafiken mit einer Auflösung von 320 × 200 Bildpunkten in zwei Farben (englisch \"High-resolution Mode\") bzw. 160 × 200 Bildpunkten mit doppelter Breite in vier Farben (englisch \"Multicolor Mode\") generieren. Für den Bildschirmaufbau werden in beiden Bitmap-Grafikmodi jeweils 8 kB RAM benötigt, die vom Arbeitsspeicher abgezogen werden. Zu den neu hinzugefügten Funktionen des VIC IIe gehören eine erweiterte Tastaturabfrage, die Steuerung der Systemuhren sowie die Fähigkeit, die CPU bei abgeschaltetem Videosignal mit einer verdoppelten Taktfrequenz von rund 2 MHz arbeiten zu lassen.\n\nDer 40-Zeichen-Grafikchip des C128 beherrscht sowohl Rasterzeilen- als auch Spritekollisionsinterrupts und eignet sich daher für die Programmierung von Spielen. Außerdem verfügt der VIC IIe über einen 14-Bit-Adressbus mit einem Adressraum von 16 kB, der sich durch Verwendung von zwei weiteren Registern des vom C128-Entwicklerteam firmenintern auch als CIA 2 bezeichneten I/O-Bausteins MOS Technology 6526 (kurz MOS 6526) auf 64 kB erweitern lässt. Der für den Bildschirmaufbau in Anspruch genommene Grafikspeicher und das Zeichensatz-RAM lassen sich im Gegensatz zum festliegenden Farb-RAM im Arbeitsspeicher des Rechners verschieben.\n\nDer VIC IIe arbeitet ferner mit dem für Commodore-Rechner typischen CBM-ASCII-Zeichensatz, der noch auf den ersten Tischrechner des Herstellers – den All-in-one-Computer Commodore PET 2001 aus dem Jahr 1977 – zurückgeht und auch in allen nachfolgenden Commodore-8-Bit-Heimcomputern der 1980er Jahre verwendet wurde. Aufgrund seiner Beschränkung auf maximal 40 Zeichen pro Bildschirmzeile ist der VIC IIe für Büroarbeiten weitgehend untauglich. Im Gegensatz zum 80-Zeichen-Modus wird der aus alphanumerischen Zeichen und Blockgrafiksymbolen bestehende Zeichensatz vom VIC IIe direkt im Zeichensatz-ROM ausgelesen.\n\nMit dem 1981 unter der Leitung von Bob Yannes entwickelten MOS 6581 verfügen der C128 sowie der C128D über den gleichen 8-Bit-Soundchip wie der Vorgänger C64. Unter dem Kürzel SID (englisch \"Sound Interface Device\") hat der innovative und flexibel einsetzbare MOS 6581 Berühmtheit erlangt und gilt sogar als „kleine Revolution im Bereich der Heimcomputer“. So verfügt der Soundchip über drei einzeln programmierbare Tongeneratoren, die jeweils aus einem Tonoszillator mit integriertem Generator für Wellenformen, einem Hüllkurvengenerator und einem Amplitudenmodulator bestehen. Der Tonoszillator kann vier Wellenformen digital erzeugen (Sägezahnschwingungen, Rechteckschwingungen, Dreiecksschwingungen sowie weißes Rauschen) und ist überdies für die Tonhöhe und die Klangfarbe verantwortlich. Der Amplitudenmodulator reguliert in Zusammenarbeit mit dem Hüllkurvengenerator die Lautstärke sowie die ADSR-Parameter für Anstieg, Abfall, Halten und Freigeben (englisch \"Attack, Decay, Sustain, Release\"). Ausgangsseitig besitzt der SID außerdem einen programmierbaren, mit dem Verfahren der subtraktiven Synthese arbeitenden analogen Klangfilter für die Erzeugung komplexerer dynamischer Klangfarben durch den Einsatz von Tiefpass, Hochpass und Bandpass. Im Gegensatz zum C64 lässt sich der SID beim C128 über die sechs Befehle codice_1, codice_2, codice_3, codice_4, codice_5 sowie codice_6 des Commodore BASIC V7.0 komfortabel programmieren.\n\nNeben der Klangerzeugung wurde der SID auch zur Steuerung von Eingabegeräten wie Paddles oder Mäusen sowie zur Erzeugung von Zufallszahlen eingesetzt.\n\nWährend im C128 sowie C128D die ältere, in NMOS-Logik ausgeführte Version MOS 6581 verbaut wurde, kam im C128D-CR mit dem in HMOS-II-Technologie gefertigten MOS Technology 8580 (kurz MOS 8580) eine weiterentwickelte, aber vollständig abwärtskompatible Variante des SID mit geringerer Betriebstemperatur, weniger Störgeräuschen und klarerem Klang durch Korrektur der Filterstärke zum Einsatz. In der US-amerikanischen Fachpresse wurde der MOS 8580 aufgrund dieser Eigenschaften auch als „Hi-Fi version of the C64 SID chip“ bezeichnet.\n\nDer C128 besitzt zwei unterschiedliche Speicherverwaltungsbausteine, mit deren Hilfe Zugriffe auf den Arbeitsspeicher des Rechners gesteuert werden.\n\nBeim auch als \"Programmable Logic Array\" (kurz PLA) bekannten und mit 48 Anschlusspins ausgestatteten MOS Technology 8721 handelt es sich um eine programmierbare logische Anordnung. Die PLA fungiert primär als Adressmanager und erzeugt u. a. sämtliche Chip-Select-Signale für die RAM- bzw. ROM-Chips sowie den 40-Zeichen-Grafikchip VIC IIe, regelt Schreibzugriffe auf das Farb-RAM bzw. DRAM mit Hilfe eines Puffers und reguliert die Datenflussrichtung auf dem Datenbus.\n\nDaneben kommt im C128 die auch als \"Memory Management Unit\" (kurz MMU) bekannte Speicherverwaltungseinheit MOS Technology 8722 zum Einsatz. Die Aufgabe der ebenfalls mit 48 Anschlusspins ausgestatteten MMU besteht in der Unterstützung der beiden Hauptprozessoren bei der Verwaltung des 128 kB umfassenden Arbeitsspeichers mittels Adressspeicherumschaltung (englisch \"bank switching\"). Diese Unterstützung ist aufgrund der 16-Bit-Adressbusstrukturen beider CPUs notwendig, da diese deren Adressraum auf jeweils 64 kB begrenzen. Zur Erfüllung dieser Aufgabe erzeugt die MMU neben den Steuersignalen für die verschiedenen Betriebsarten auch die Selektierungssignale für die RAM- bzw. ROM-Speicherbänke des Rechners, sodass zwischen diesen hin und her gewechselt werden kann. Das Volumen der einzelnen Speicherbänke entspricht dabei der maximalen Größe des von den beiden Hauptprozessoren individuell ansteuerbaren Adressraums von 64 kB. Insgesamt vermag die MMU 1 MB RAM, 96 kB internes ROM und 32 kB externes ROM zu verwalten. Die Adressübersetzung wird in den 17 Registern der MMU vollzogen.\n\nDer C128 verfügt über zwei baugleiche, auch als Schnittstellen-Adapter bezeichnete I/O-Bausteine. Sie sind unter dem Kürzel CIA (englisch \"Complex Interface Adapter\") bekannt und regulieren die im Rahmen von Ein- und Ausgabeoperationen über die Joystickanschlüsse, die Tastatur, den Kassettenanschluss, den Userport sowie die serielle Schnittstelle anfallenden Datenströme. Die beiden I/O-Bausteine des Typs MOS Technology 6526 sind mit 40 Anschlusspins ausgestattet, besitzen 16 einzeln programmierbare Ein- und Ausgabeleitungen und können mit einer Taktfrequenz von bis zu 2,04 MHz getaktet werden. Außerdem verfügen die beiden Schnittstellen-Adapter über ein 8-Bit-Schieberegister für die serielle Ein- und Ausgabe von Daten, eine 24-Stunden-Zeituhr sowie die Fähigkeit zum 8-Bit- bzw. 16-Bit-Datentransport mit Quittungsbetrieb (englisch \"handshaking\") bei Lese- oder Schreiboperationen.\n\nDer erste der beiden Schnittstellen-Adapter, der in der technischen Dokumentation des C128 zur Vermeidung von Verwechslungen auch als CIA 1 bezeichnet wird, ist für die über die Joystickbuchsen, die Tastatur sowie die über die serielle Schnittstelle im schnelleren 2-MHz-Modus abzuwickelnden Ein- und Ausgabeoperationen zuständig.\n\nDer zweite der beiden Schnittstellen-Adapter, kurz CIA 2 genannt, ist für die über die serielle Schnittstelle im langsameren, die Kompatibilität des C128 zu älterer C64-Hardware garantierenden 1-MHz-Modus sowie die über den Userport laufenden Ein- und Ausgabeoperationen verantwortlich.\n\nDer C128 ist ab Werk mit einem Arbeitsspeicher von 128 kB RAM ausgestattet, der in zwei 64-kB-Bänke aufgeteilt ist. Daneben besitzt der Rechner, je nach Modellvariante, zusätzliche 16 respektive 64 kB Video-RAM sowie 2 kB Farb-RAM. Insgesamt verfügt ein C128 bzw. C128D also in der Basiskonfiguration über 148 kB RAM, ein C128D-CR sogar über üppige 196 kB RAM. Außerdem umfassen die beiden nativen BASIC-Betriebssysteme des C128 insgesamt 72 kB ROM, von denen 16 kB für den C64-Modus und 48 kB für den C128-Modus reserviert sind. Hinzu kommen noch 8 kB Zeichensatz-ROM. Sämtliche im Rechner verbauten RAM-Chips stammen aus meist japanischer Fremdproduktion, die ROM-Chips dagegen ausschließlich von Commodores eigener US-amerikanischer Tochterfirma MOS Technology.\n\nDer 128 kB umfassende Arbeitsspeicher der älteren Modellvarianten C128 sowie C128D setzt sich aus 16 dynamischen 1-Bit-RAM-Chips des Typs 4164 mit 16 Anschlusspins und einer Speicherkapazität von jeweils 8 kB zusammen. Der höher integrierte C128D-CR besitzt dagegen nur noch vier 32-KByte-RAM-Chips des Typs 41256 mit ebenfalls 16 Anschlusspins sowie einem Speichervolumen von jeweils 32 kB.\n\nDer 16 kB große Grafikspeicher der Modellvarianten C128 und C128D besteht aus einem dynamischen 4-Bit-VRAM-Chip des Typs 4416 mit 18 Anschlusspins. Im mit 64 kB ausgestatteten Grafikspeicher des C128D-CR sind zwei 4-Bit-VRAM-Chips der Typen 41464 mit 18 Anschlusspins, einer mittleren Zugriffszeit von 120–150 Nanosekunden und einem Speichervolumen von jeweils 32 kB verbaut. Die als 80-Zeichen-Bildwiederholspeicher dienenden VRAM-Chips können nicht direkt von den Hauptprozessoren angesteuert werden, sondern nur vom Grafikchip MOS 8563. Sie wurden gelegentlich im 40-Zeichen-Modus alternativ als RAM-Disk verwendet.\n\nSchließlich besitzen sämtliche C128-Varianten noch einen statischen, vom 40-Zeichen-Grafikchip VIC IIe als Hochgeschwindigkeits-Farbspeicher verwendeten 8-Bit-Farb-RAM-Chip des Typs 2016 mit 24 Anschlusspins und einer Speicherkapazität von 2 kB. Im C64-Modus wird allerdings nur 1 kB des Farb-RAMs für die Textdarstellung verwendet, während im C128-Modus die vollen 2 kB bei der Textdarstellung und im hochauflösenden Grafikmodus zum Einsatz kommen. Beim Farb-RAM handelt es sich um einen Nibble-Speicherbaustein, da von beiden Betriebssystemmodi nur die ersten vier Bits zur Bestimmung der Farbwahl verwendet werden.\n\nDas C64-Betriebssystem mit dem Commodore BASIC V2.0, 40-Zeichen-Editor und dem Betriebssystemkern ist in einem 8-Bit-ROM-Chip des Typs 23128 untergebracht, der 28 Anschlusspins und eine Speicherkapazität von 16 kB besitzt. Das umfangreichere C128-Betriebssystem ist dagegen in drei 8-Bit-ROM-Chips des Typs 23256 enthalten, die ebenfalls 28 Anschlusspins und eine Speicherkapazität von jeweils 16 kB aufweisen. Zwei dieser ROM-Chips enthalten das Commodore BASIC V7.0, während der dritte die Bildschirmeditoren für den 40- bzw. 80-Zeichen-Modus sowie den Betriebssystemkern birgt. Der Zeichensatz der für den US-amerikanischen Markt hergestellten Version des C128 befindet sich schließlich in einem weiteren 8-Bit-ROM-Chip des Typs 2364, der 24 Anschlusspins und eine Speicherkapazität von 8 kB besitzt, von denen jeweils 4 kB für den C64- bzw. den C128-Modus verwendet werden.\n\nDie Hauptplatine des C128 verfügt auf der linken Seite neben den das Betriebssystem enthaltenden ROM-Chips ferner über einen freien Sockel mit ebenfalls 28 Anschlusspins, der zur wahlweisen Aufnahme eines nichtflüchtigen programmierbaren 8-Bit-EPROM-Chips des Typs 27128 mit einer Speicherkapazität von 16 kB oder eines vom Speichervolumen abgesehen die gleichen Eigenschaften aufweisenden 32-kB-EPROM-Chips des Typs 27256 dient.\n\nDie vom MOS 8502 mit Hilfe der MMU verwalteten 96 kB ROM des C128 verteilen sich auf die externen Festspeicher (24 kB), das C64-Betriebssystem (24 kB) und das C128-Betriebssystem (48 kB) inklusive der beiden Betriebssystemkerne nebst Sprungtabellen und Maschinensprachemonitor (C128-Modus), der beiden nativen Dialekte des Commodore BASIC sowie der beiden als Benutzerschnittstelle dienenden Texteditoren mit 40 bzw. 80 Zeichen pro Zeile. Der 128 kB RAM umfassende Arbeitsspeicher setzt sich aus zwei Speicherbänken mit jeweils 64 kB RAM zusammen, die über ein spezielles Konfigurationsregister der MMU aktiviert werden. Die Speicherbank null dient der Aufnahme des BASIC-Programmtextes, während die Speicherbank eins die vom aktuellen BASIC-Programm verwendeten Variablen enthält. Bestimmte, auf eine Größe von jeweils bis zu 16 kB variabel einstellbare Speicherbereiche (englisch \"Common Areas\") teilen sich die ROM-Speicherbank sowie die RAM-Speicherbänke, um beim Umschalten zwischen den RAM-Speicherbänken auf Teile des Betriebssystems wie etwa den Stapelspeicher der Zeropage zugreifen zu können.\nWie bei 8-Bit-Mikrocomputerarchitekturen der 1980er Jahre allgemein üblich besteht der Systembus des C128 aus einem Adressbus, einem Datenbus sowie diversen Steuerleitungen. Er hat die Hauptaufgabe, gleich zwei 8-Bit-Hauptprozessoren unterschiedlicher Hersteller mit eigentlich inkompatiblen Hardwareeigenschaften die Kommunikation mit ihrer technischen Umgebung über ein komplexes System von Leiterbahnen auf der Hauptplatine zu ermöglichen. Zu diesem Zweck besitzt der C128 zusätzlich einen besonders gestalteten Prozessorbus mit eigenen Daten- und Adressbusleitungen. Überdies verwendet der C128 mehrere lokale Daten- und Adressbusse mit jeweils eigenen Spezialfunktionen zur Verschaltung aller weiteren elektronischen Baugruppen.\n\nUnter dem Prozessorbus des C128 werden die Datenleitungen von Daten- und Adressbus verstanden, die direkt an den Hauptprozessor MOS 8502 angeschlossen sind. Der Prozessorbus verbindet dabei den MOS 8502 mit denjenigen ROM-Chips, die das Betriebssystem enthalten, den drei Speicherverwaltungsbausteinen, dem 80-Zeichen-Grafikchip MOS 8563, dem Soundchip SID sowie den beiden I/O-Bausteinen. Außerdem ist auch der Zweitprozessor Z80A unmittelbar an die 16 Adressleitungen des Prozessorbusses angeschlossen, sodass sie im CP/M-Modus von beiden CPUs im Wechsel verwendet werden können (englisch \"bus sharing\"). Zur Vermeidung von Zugriffskonflikten ist der Zweitprozessor mit speziellen, der Arbitration, sprich der Buszuweisung dienenden Steuerleitungen ausgestattet und wird zugunsten des Hauptprozessors mithilfe von hochohmig gesetzten Tri-State-Gattern vorübergehend von den Adressleitungen des Prozessorbusses getrennt.\n\nAn die acht Datenleitungen des Prozessorbusses dagegen ist der Zweitprozessor Z80A nicht direkt angebunden. Vielmehr besitzt er einen eigenen lokalen 8-Bit-Datenbus. Dieser ist an die Datenleitungen des Prozessorbusses lediglich indirekt ausgangsseitig über einen Puffer des Typs 74LS244 sowie eingangsseitig über ein transparentes, der Konservierung von Informationen dienendes Latch (im Deutschen auch als „Schlüsselspeicher“ bezeichnet) des Typs 74LS373 angeschlossen. Sowohl Puffer als auch Latch agieren dabei als Bustreiber. Sofern im CP/M-Modus das Buszugriffe regelnde Steuerungssignal Address Enable Control (kurz AEC) des taktgebenden Grafikchips VIC IIe bei logisch null liegt, bleibt der Z80A von den Datenleitungen des Prozessorbusses getrennt. Springt das AEC-Steuerungssignal dagegen auf logisch eins, wird eine Verbindung mit dem Prozessorbus hergestellt, sodass seitens des Zweitprozessors Schreib- und Leseoperationen durchgeführt werden können. Über das Steuerungssignal Read Enable (RE) veranlasst der Z80A das Latch dann dazu, seine zwischengespeicherten Daten auf den lokalen Datenbus des Zweitprozessors zu laden. Mithilfe des Steuerungssignals Write Enable (WE) dagegen wird der Puffer vom Z80A dazu gebracht, die zwischengespeicherten Daten des Zweitprozessors auf die Datenleitungen des Prozessorbusses zu übertragen.\n\nDen 16-Bit-Adressbus des C128 teilen sich die beiden Hauptprozessoren mit dem Grafikchip VIC IIe. Auf diese Weise können MOS 8502, Z80A und VIC IIe gleichzeitig überschneidungsfrei auf das Zeichensatz-ROM, den Farbspeicher sowie den Arbeitsspeicher zugreifen, der dem VIC IIe teilweise als Grafikspeicher dient. Dabei ist der Adressbus in Bereiche mit gemeinsamem Zugriff von CPU und VIC IIe sowie in Bereiche mit Alleinzugriff des Hauptprozessors unterteilt. Die Bereiche mit gemeinsamem Zugriff nennt man den „Sharing-Adressbus“.\n\nDie MMU des C128 verfügt über einen eigenen 8-Bit-Adressbus, der als „TA-Adressbus“ (englisch \"Translated Address Bus\") bezeichnet wird. Die Hauptaufgabe des TA-Adressbusses besteht darin, dem Rechner durch Umwandlung der normalen in höherwertige Speicheradressen die Verwaltung der vollen 128 kB RAM trotz der Beschränkung der Adressräume der beteiligten Hauptprozessoren auf 64 kB zu ermöglichen. Darüber hinaus steuert der TA-Adressbus auch den 8-Bit-MUX-Adressbus. Dessen Aufgabe besteht wiederum in der Koordination von TA-Adressbus und den Bereichen des Adressbusses, die nicht dem Sharing-Adressbus zugehören. Auch der VIC IIe besitzt einen eigenen 16-Bit-Adressbus, erzeugt die Adressen aber in Zusammenarbeit mit einem der CIAs.\n\nDer C128 verfügt überdies über einen bidirektionalen 8-Bit-Datenbus. Der Datenbus verbindet die Hauptprozessoren mit sämtlichen ROM- und RAM-Speicherchips, den I/O-Bausteinen, der MMU, der PLA, den Grafikchips VIC IIe bzw. MOS 8563 sowie dem Soundchip SID. Daneben bestehen aber noch weitere, mehr oder minder autonome Datenbusstrukturen. Der Zweitprozessor Z80A besitzt beispielsweise den schon erwähnten eigenen lokalen Datenbus für Schreib- und Leseoperationen. Auch gibt es einen eigenen Farbdatenbus (englisch \"Color Data Bus\") für die Übertragung von Farbinformationen zwischen dem Hochgeschwindigkeits-Farbspeicher und dem VIC IIe.\n\nSchließlich existiert mit dem Videodatenbus (englisch \"Video Data Bus\", auch \"Display Data Bus\") noch eine weitere lokale Datenbusstruktur für den Datenaustausch zwischen dem MOS 8563 und den VRAM-Chips des 80-Zeichen-Bildwiederholspeichers. Beim Videodatenbus handelt es sich um einen hochspezialisierten, vom Rest des Systembusses vollkommen abgetrennten Datenbus. Der Grafikchip MOS 8563 generiert nicht nur das Videosignal des 80-Zeichen-Bildschirms, sondern besorgt über den Videodatenbus auch die Wiederauffrischung des Speicherinhalts der VRAM-Chips.\n\nDie laut technischer Dokumentation des Herstellers nicht zu einem eigenen Steuerbus zusammengefassten Steuerleitungen des C128 dienen der CPU zur Übermittlung von Steuerinformationen an die einzelnen Baugruppen des Rechners. Dazu zählen etwa die Steuersignale zur Regelung der Datenflussrichtung auf dem Systembus, zur Chipauswahl (englisch \"chip select\") und Chipfreigabe (englisch \"chip enable\"). Hinzu kommen Taktsignale, Lese- und Schreibanweisungen, Interrupts, Halte- und Quittungssignale.\n\nDas Gehäuse des C128 ist rechteckig und aus beigem Kunststoff gefertigt. Im hinteren Bereich sind auf der Ober- und Unterseite Lüftungsschlitze zur Kühlung der Elektronik ins Gehäuse eingelassen. Im vorderen Teil befindet sich das Tastaturfeld, das zur Vorderseite hin abgeflacht ist. Das Gehäuse misst  ×  ×  (Breite × Tiefe × Höhe).\n\nDie Tastatur-Layout des C128 lehnt sich an das Vorgängermodell an und weist 92 Tasten auf. Im Vergleich zum C64 ist die Tastatur aber wesentlich ergonomischer und um einen numerischen Tastenblock inklusive einer Enter-Taste sowie zwölf in Vierergruppen angelegte Funktionstasten erweitert, die sich oberhalb der eigentlichen Schreibmaschinentastatur befinden. Die Tastaturmechanik wurde gegenüber dem Vorgängermodell ebenfalls verbessert. Außerdem verfügen die Tasten , und des Ziffernblocks über kleine Erhebungen zur Blindorientierung bei der Verwendung des Zehnfingersystems. Nach dem Vorbild des C64 besitzt auch die Tastatur des C128 eine mit dem Firmenlogo des Herstellers bedruckte Commodore-Taste, die u. a. zum Einstellen der Bildschirmfarben sowie zum Aufrufen bestimmter Grafiksymbole des CBM-ASCII-Zeichensatzes dient.\n\nZu den zusätzlichen Funktionstasten zählen zwei Tasten mit Umschaltsperre. Im Falle der Versionen des C128, die für die Märkte der nicht-englischsprachigen Länder hergestellt wurden, erlauben diese dem Anwender einerseits die Wahl zwischen dem US-amerikanischen ASCII- und dem jeweiligen landesüblichen Zeichensatz (wie etwa dem deutschen DIN-Zeichensatz), andererseits den Betrieb des Rechners wahlweise im 40- bzw. 80-Zeichen-Modus. Die C128-Tastatur wurde in acht Versionen für folgende Länder bzw. Regionen hergestellt: Vereinigte Staaten/Großbritannien/Niederlande, Dänemark/Norwegen, Schweden/Finnland, Deutschland, Frankreich/Belgien, Italien, Schweiz und Spanien. Die Tasten enthalten die landesüblichen Zeichen in Form von Overlays, die rechts neben den im englischsprachigen Raum gebräuchlichen Zeichen zu finden sind. Daneben gibt es vier separate, einen eigenen Block bildende Cursortasten, eine Escape-Taste, eine Tabulatortaste, eine Alt-Taste, eine Help-Taste, eine Line-Feed-Taste sowie eine No-Scroll-Taste, die das Bildschirmrollen etwa bei der Ausgabe von Programmlistings unterdrückt. Die vier oberhalb des numerischen Tastenblocks liegenden, doppelt belegbaren Funktionstasten sind mit den häufig verwendeten BASIC-Befehlen codice_7, codice_8, codice_9, codice_10, codice_11, codice_12, codice_13 sowie codice_14 vorbelegt und lassen sich frei programmieren. Außerdem lässt sich durch gleichzeitiges Drücken der Control-Taste sowie der Buchstabentaste ein bei jedem Tastenanschlag hörbarer Klingelton an- und abschalten.\n\nAuf der rechten Seite verfügt der C128 über zwei neunpolige Sub-D-Buchsen, die als Anschlüsse für Atari-kompatible Joysticks oder andere Regler dienen. Daneben besitzt der Rechner auf der rechten Gehäuseseite einen Resetschalter, einen Netzschalter sowie eine Netzanschlussbuchse für das externe Schaltnetzteil.\n\nAuf der Rückseite verfügt der C128 über eine Erweiterungsschnittstelle (englisch \"Expansion Port\") mit 44 Kontakten u. a. für die Aufnahme von Steckmodulen, einen Kassettenanschluss für eine Datasette in Gestalt eines Platinensteckers mit zwölf Kontakten sowie eine proprietäre, als serielle Schnittstelle (englisch \"Serial Port\") dienende DIN-Buchse mit sechs Pins, die für den Anschluss von CBM-Diskettenlaufwerken sowie Druckern gedacht ist und auch als CBM-Bus bezeichnet wurde. Daneben weist der Rechner auf der Rückseite noch eine als Composite-Video-Anschluss dienende achtpolige DIN-Buchse, einen Schalter für die Wahl des Fernsehkanals, einen Hochfrequenz-Ausgang für den Betrieb mit einem Fernseher, einen neunpoligen RGBI-Anschluss für den Betrieb mit hochauflösenden Farbmonitoren sowie schließlich einen 24-poligen Platinenstecker auf, der als Userport bzw. universelle, in dieser Form nur von Commodore implementierte 8-Bit-Schnittstelle fungiert. Wegen der anders als beim Hochfrequenz-Anschluss nicht notwendigen Demodulation des vom 40-Zeichen-Grafikchip VIC IIe erzeugten Ausgangssignals durch den Fernseher ist die über den Composite-Video-Anschluss erreichbare Bildqualität deutlich höher.\n\nDie für den nordamerikanischen Markt produzierten Exemplare des C128 beziehen ihren Strom über unterschiedlich ausgeführte externe Schaltnetzteile mit der Typennummer PN-252449-xx, die von verschiedenen asiatischen Herstellern wie Dee-Van (Taiwan) oder Newtronics / Mitsumi Electric (Japan) stammen. Zum Betrieb wird eine Netzspannung von mindestens 117 Volt bei einer Netzfrequenz von 60 Hertz benötigt. Die für den westdeutschen Markt gedachten und ebenfalls nicht identisch ausgeführten C128-Schaltnetzteile wurden dagegen in Westdeutschland von verschiedenen Herstellern gefertigt. Sie tragen die Typennummer PN-310416-xx und benötigen für einen ordnungsgemäßen Betrieb eine Netzspannung von 220 Volt bei einer Netzfrequenz von 50 Hertz. Das C128-Schaltnetzteil liefert 5 Volt Gleichstrom sowie 9 Volt Wechselstrom bei einer Stromstärke von 4,3 Ampere. Bei den Desktop-Varianten des C128 kommen herkömmliche, ins Gehäuse integrierte Trafonetzteile zum Einsatz.\n\nDer C128 wurde in insgesamt drei Modellvarianten ausgeliefert. Alle außerhalb der englischsprachigen Welt angebotenen Modellvarianten enthielten neben dem US-amerikanischen ASCII-Standardzeichensatz zusätzlich eine an die jeweiligen nationalen Gepflogenheiten angepasste Tastatur mit landestypischem Zeichensatz inklusive Sonderzeichen wie Umlaute, diakritische Zeichen usw. Auch die Netzteile waren ab Werk an die in den jeweiligen Ländern üblichen Netzspannungen angepasst.\n\nBeim C128 handelt es sich um einen klassischen Tastaturcomputer mit flachem Plastikgehäuse, zahlreichen Lüftungsschlitzen auf Ober- und Unterseite, 16 kB VRAM und externem Schaltnetzteil. Diese häufigste Modellvariante war eher auf Heimanwender ausgerichtet, die vor großen Einzelinvestitionen zurückschreckten und lieber nach und nach ihr Computersystem erweitern wollten, auch wenn dies zu Kabelsalat auf dem heimischen Schreibtisch führte. Der C128 war in Westdeutschland und Nordamerika ab dem dritten Quartal 1985 erhältlich. Firmenintern wurde der C128 im Gegensatz zu den Desktop-Modellvarianten zu den „Consumer-Produkten“ gerechnet.\n\nBeim C128D handelt es sich um einen Desktop-Computer mit integriertem 5¼-Zoll-Diskettenlaufwerk des Typs VC1571, abgesetzter Tastatur, 16 kB VRAM, Plastikgehäuse mit Aussparung zum Unterbringen der Tastatur, Lüfter, Tragegriff, separater Laufwerkselektronik, Floppy-Resetschalter und integriertem Trafonetzteil. Das Gehäuse des C128D misst  ×  ×  (Breite × Tiefe × Höhe). Im Gegensatz zum Stand-Alone-Diskettenlaufwerk VC1571 besitzt der C128D keinen extern zugänglichen DIP-Schalter zum Einstellen der Geräteadresse und auch keinen Schacht zum Einbau eines zweiten Diskettenlaufwerkes. Der Kassettenanschluss befindet sich auf der Rückseite, der Netzschalter vorne an der linken Seitenwand. Die Hauptplatine des C128D ist mit der des C128 identisch, die Laufwerkselektronik ist die gleiche wie die der VC1571.\n\nDiese Modellvariante besitzt Lüftungsschlitze auf der Oberseite und richtete sich eher an professionelle Anwender. Das angehängte „D“ in der Modellbezeichnung steht für den englischen Begriff „Desktop“, also einen für den Schreibtisch konzipierten Computer mit abgesetzter Tastatur und flachem Gehäuse zur Aufnahme der Elektronik. Wegen des nicht erbrachten Nachweises der elektromagnetischen Verträglichkeit gegenüber der US-amerikanischen Zulassungsbehörde FCC war diese Modellvariante ab Februar 1986 ausschließlich in Europa erhältlich.\n\nBeim C128D-CR handelt es sich um einen Desktop-Computer mit integriertem 5¼-Zoll-Diskettenlaufwerk des Typs VC1571, abgesetzter Tastatur, 64 kB VRAM, Blechgehäuse, in die Hauptplatine integrierter Laufwerkselektronik, Floppy-Resetschalter und integriertem Trafonetzteil ohne Lüfter. Das Gehäuse des C128D-CR ist flacher als das des C128D, die Grundflächen beider Modelle sind aber identisch. Der Kassettenanschluss befindet sich auf der rechten Seite, der Netzschalter hinten links. Im Laufwerk wurde ein preiswerterer Schrittmotor verbaut. Aufgrund des massiven Blechgehäuses mussten beim C128D-CR intern keine weiteren Abschirmbleche verbaut werden. Die stärkere Abschirmung schützt das interne Diskettenlaufwerk überdies besser vor der Strahlung von auf dem Gehäuse abgestellten Monitoren als das Kunststoffgehäuse des Vorgängermodells C128D.\n\nDie Hauptplatine des C128D-CR wurde komplett neu gestaltet. Ferner besitzt das Gerät höher integrierte Schaltkreise als der C128D inklusive des weiterentwickelten 80-Zeichen-Grafikchips MOS 8568 sowie des verbesserten Soundchips MOS 8580 und weist einen geringeren Stromverbrauch als das Vorgängermodell auf, weshalb er auch keinen Lüfter benötigt und kostengünstiger zu produzieren war. Aus dieser Tatsache leitet sich auch das angehängte Kürzel „CR“ in der Modellbezeichnung ab, das für den englischen Ausdruck \"„cost-reduced“\" (deut. ‚kostenreduziert‘) steht. Außerdem sorgte die Überarbeitung für eine Verringerung der Geräuschemissionen sowie der Störanfälligkeit. Allerdings machten die Veränderungen am Design viele der für die Vorgängermodelle entwickelten Hardwarezusätze unbrauchbar.\n\nDiese letzte Modellvariante besitzt weder Lüftungsschlitze auf der Oberseite noch einen Tragegriff. Sie richtete sich ebenfalls eher an professionelle Anwender. Die Systemprogramme sind im Gegensatz zu den früher erschienenen Modellen C128 und C128D nicht auf vier 16-kB-ROM-Chips verteilt, sondern nur noch auf zwei 32-kB-ROM-Chips. Der Arbeitsspeicher besteht nur noch aus vier 32-kB-RAM-Chips. Die im Vergleich zum C128D höher integrierte Laufwerkselektronik nebst neu entwickeltem Floppy-Disk-Controller MOS Technology 5710 (kurz MOS 5710) und überarbeitetem Diskettenbetriebssystem Commodore DOS 3.1 führt zu (allerdings geringen) Einschränkungen bei der Softwarekompatibilität. Aufgrund des viermal so großen Videospeichers kann der C128D-CR ohne Hardwarezusätze deutlich höhere Auflösungen generieren als die älteren Modellvarianten mit lediglich 16 kB VRAM. Der bereits im Januar 1987 offiziell vorgestellte C128D-CR war erst ab dem dritten Quartal 1987 weltweit erhältlich. Gelegentlich wurde diese Modellvariante in den zeitgenössischen Computerzeitschriften in Anspielung auf ihr Gehäuse aus gewalztem Metall scherzhaft als „Blech-Diesel“, „Diesel im Blechkleid“ oder auch als „C128D-Blech“ bezeichnet.\n\nCommodore entwickelte eine Reihe von Peripheriegeräten, mit deren Hilfe Leistungsfähigkeit und Einsatzspektrum des C128 vergrößert werden können. Dazu zählen Eingabegeräte, Speichergeräte, Speichererweiterungen, Ausgabegeräte, mehrere Modems zur Datenfernübertragung, ein Bildschirmtextmodul, verschiedene Hardwarezusätze und diverses Zubehör. Neben Commodore produzierten zahlreiche Drittanbieter Peripheriegeräte für den Rechner.\n\nMehrere, von zeitgenössischen Computerzeitschriften durchgeführte Umfragen liefern ein ziemlich genaues Bild von der Zusatzausstattung, mit deren Hilfe der C128 in der alltäglichen Praxis betrieben wurde. Beinahe alle nordamerikanischen C128-User verwendeten ihr Gerät zusammen mit einem 5¼-Zoll-Diskettenlaufwerk, einem Drucker sowie einem Joystick. Ungefähr die Hälfte besaß ein Modem und eine Computermaus, aber nur 28 Prozent eine Speichererweiterung. 72 Prozent der deutschsprachigen Besitzer des Rechners besaßen im Jahr 1992 – also drei Jahre nach Produktionseinstellung – einen C128D oder C128D-CR. Die übrigen 28 Prozent waren im Besitz der ursprünglichen Tastaturcomputerversion, die meist im Gespann mit dem 5¼-Zoll-Diskettenlaufwerk VC1571 betrieben wurde.\n\nDie wichtigsten, gezielt von Commodore für den C128 und seine Modellvarianten produzierten Peripheriegeräte, die allesamt auch mit dem Verkaufsschlager C64 verwendet werden können, werden im Folgenden dargestellt. Berücksichtigt werden überdies auch Festplattenlaufwerke, Hardwarezusätze sowie Zubehör von Drittanbietern mit einem gewissen Bekanntheits- und Verbreitungsgrad.\n\nBei der Commodore-Maus des Typs 1350 handelt es sich um eine einfache mechanisch-elektrische Joystickmaus mit vier bei entsprechenden Bewegungen von einer schweren, gummiummantelten Kugel geschlossenen Joystickkontakten, die im Gehäuseinneren untergebracht sind. Auf der Oberseite besitzt das Gerät zwei Tasten und am unteren Ende ein festes Verbindungskabel, über das die Maus Bewegungssignale an den Rechner übermittelt. Die linke Maustaste diente in der Regel der Auslösung von Aktionen seitens des Anwenders, während die Verwendung der rechten Maustaste von der gerade verwendeten Software abhing und daher stark variierte. Die Maus 1350 kann an beiden, für Joysticks gedachten neunpoligen Sub-D-Buchsen des C128 angeschlossen werden und musste zwecks Aufrechterhaltung ihrer Funktionsfähigkeit regelmäßig gereinigt werden. Dafür musste das Gehäuse geöffnet und die Kugel entnommen werden.\n\nBei der Commodore-Maus des Typs 1351 handelt es sich um eine mechanisch-elektrische Proportionalmaus. Äußerlich lässt sie sich nicht ohne Weiteres vom Vorgängermodell 1350 unterscheiden, weist aber eine erweiterte Funktionalität auf. So verfügt sie neben dem schon von der Maus 1350 bekannten und für ältere Software gedachten Joystickmodus auch über einen präziseres Arbeiten ermöglichenden Proportionalmodus. Nach dem Einschalten ist der auf besondere Maustreiber angewiesene Proportionalmodus automatisch aktiviert, der etwa von der grafischen Benutzeroberfläche GEOS unterstützt wurde. Durch Gedrückthalten der rechten Maustaste während des Einschaltens des Rechners kann man in den Joystickmodus wechseln. Um den Proportionalmodus zu realisieren, verfügt das Eingabegerät über einen eigenen Mikrochip mit Quarz-Oszillator-Schaltung, die Impulse von 2,5 Kilohertz bei einer Impulsbreite von 0,15 Mikrosekunden erzeugt. Zwei im rechten Winkel angeordnete, jeweils mit einer Schlitzscheibe sowie zwei Lichtschranken verbundene Walzen im Gehäuseinneren übersetzen die Bewegungen der Mauskugel in elektrische Impulse.\n\nIm Joystickmodus wird – genau wie bei handelsüblichen Joysticks – alle 20 Millisekunden abgefragt, welche Joystickkontakte im Mausinneren gerade geöffnet bzw. geschlossen sind. Die linke Maustaste wird gleichzeitig wie die Feuertaste eines konventionellen Joysticks behandelt. Die Ergebnisse werden über das Joystickkabel an den Rechner weitergegeben. Im Proportionalmodus wird jede Bewegung von der Maus registriert und alle 512 Mikrosekunden, d. h. mit einer 39 mal höheren Frequenz und damit erheblich präziser als im Joysttickmodus an den Rechner übermittelt. Dafür werden die beiden Register codice_15 und codice_16 des Soundchips SID verwendet.\n\nCommodore produzierte mit den Modellen VC1570, VC1571 sowie VC1581 insgesamt drei Diskettenlaufwerke gezielt für den Betrieb mit dem C128. Sie werden über die serielle Schnittstelle mit dem Rechner verbunden und verfügen hardwareseitig allesamt über einen eigenen, auf 2 MHz getakteten Hauptprozessor des Typs MOS 6502A sowie 32 kB ROM des Typs 23256 zur Aufnahme der mit Copyright-Schutz versehenen Diskettenbetriebssysteme Commodore DOS 3.0 (VC1570/1571) bzw. Commodore DOS 10.0 (VC1581). Aufgrund dieser Ausstattung gehören sie zu den selbständig operierenden „intelligenten Diskettenstationen“, die weder eigens hochgefahren werden müssen noch kostbaren Speicherplatz belegen.\n\nDarüber hinaus besitzen die Laufwerke auf der Vorderseite zwei Statusanzeigen für den Stromfluss (rot) bzw. Diskettenzugriffe (grün) und auf der Rückseite einen Netzanschluss, zwei serielle Anschlussbuchsen sowie einen DIP-Schalter zum Einstellen der Geräteadresse. Alle drei Laufwerke sind sowohl C64-kompatibel als auch prinzipiell CP/M-fähig. Ein viertes, in der Tradition der älteren CBM-Diskettenlaufwerke der Typen 8050 bzw. 8250 stehendes Modell – das 5¼-Zoll-Doppeldiskettenlaufwerk VC1572 – wurde zwar als Prototyp auf der Summer Consumer Electronics Show des Jahres 1985 der Öffentlichkeit vorgestellt, aber nie zur Marktreife gebracht.\n\n1985 erschienen zunächst die Modelle VC1570 sowie VC1571 für die seinerzeit weitverbreiteten 5¼-Zoll-Disketten. Die Elektronik beider Geräte ist weitgehend identisch. Beide verfügen über einen Floppy-Disk-Controller des Typs WD1770 (bzw. WD1772 bei späten Bauserien) zum Lesen und Beschreiben von Disketten im MFM-Format, zwei sich primär die Aufgaben eines Floppy-Disk-Controllers teilende Gate-Arrays zum Lesen und Beschreiben von Disketten im GCR-Format, zwei I/O-Bausteine des älteren Typs MOS 6522 zur Steuerung der Laufwerksmechanik sowie zur Regelung des über die serielle Schnittstelle abgewickelten Datenflusses, einen als Buscontroller agierenden I/O-Baustein des Typs MOS 6526 sowie einen Pufferspeicher mit 2 kB statischem RAM des Typs 2016. Sowohl die VC1570 als auch die VC1571 besitzen eine als Lichtschranke fungierende Fotozelle, die eine schonende, weitgehend verschleißfreie Ausrichtung des Schreib-/Lesekopfs auf die erste Spur der Diskette ohne Kontakt mit der übrigen Laufwerksmechanik ermöglicht. Ein weiterer optischer Sensor löst bei jedem Diskettenwechsel eine Drehung der Laufwerksspindel aus. Durch diese Vorrichtung wird das für einen fehlerfreien Betrieb notwendige passgenaue Sitzen der eingelegten Disketten gewährleistet. Die in beiden Modellen verwendeten Hauptplatinen mit den Leiterbahnen zur Verbindung der einzelnen Bauelemente sind identisch. Beide Laufwerke sind außerdem beim Betrieb des C128 im C64-Modus zu einer nahezu lückenlosen Hardware-Emulation des Vorgängermodells VC1541 in der Lage.\n\nDie beiden von Commodore entwickelten Gate-Arrays ergänzen einander funktional. Teilweise liegen ihre Aufgabenbereiche aber auch jenseits der üblicherweise von Floppy-Disk-Controllern ausgeführten Funktionen. Das größere Gate-Array des Typs 64H156 mit der firmeninternen Bezeichnung MOS Technology 251828 (kurz MOS 251828) sowie 42 Anschlusspins führt die anfallenden Lese- und Schreiboperationen durch, dient dem Hauptprozessor MOS 8520A als Taktgeber und steuert den Schrittmotor des Laufwerks. Das kleinere Gate-Array des Typs 64H1567 mit der firmeninternen Bezeichnung MOS Technology 251829 (kurz MOS 251829) sowie 20 Anschlusspins ist für die Chipauswahl, die Fehlerkorrektur beim Einlesen von Daten im GCR-Format sowie den Ausgleich der Motordrehzahl zuständig.\n\nVor allem hinsichtlich der Laufwerksmechanik, der Gehäuseform sowie des Netzteils unterscheiden sich beide 5¼-Zoll-Laufwerke voneinander. Die einen Knebelverschluss aufweisende Laufwerksmechanik der VC1571 stammt vom japanischen Zulieferer Newtronics Mitsumi und verfügt über zwei Schreib-/Leseköpfe, mit deren Hilfe Disketten doppelseitig gelesen und beschrieben werden können. Die mit Schnappverschluss ausgestattete Laufwerksmechanik der VC1570 gleicht dagegen dem Vorgängermodell VC1541, wurde vom japanischen Hersteller Alps geliefert und besitzt lediglich einen Schreib-/Lesekopf. Während die Elektronik der VC1571 in einem dem Design des C128 angepassten flachen Gehäuse untergebracht ist, befindet sie sich im Fall der VC1570 noch im altbekannten und voluminöseren Gehäuse des Vorgängermodells VC1541, das lediglich im Farbton dem C128 angeglichen wurde. Aufgrund dieser Verbindung der neuen Elektronik mit der älteren Laufwerksmechanik und dem bewährtem Gehäusedesign der VC1541 gilt die VC1570 auch als aus der Not geborenes „Zwitter-Laufwerk“ „Zwischending“ oder auch als „Interims-Floppylaufwerk“. Das Gehäuse der VC1571 misst 21,6 cm × 34,6 cm × 7,6 cm (Breite × Tiefe × Höhe) und wiegt 3,5 kg. Die Leistungsaufnahme beider mit internen Netzteilen ausgestatteter Laufwerke liegt bei maximal 25 Watt. Im Gegensatz zur VC1570 besitzen die frühen Bauserien der VC1571 jedoch ein modernes Schaltnetzteil, das vollkommen vom Metallchassis des Laufwerks getrennt ist und über ein eigenes Abschirmblech verfügt. Dadurch ist die VC1571 besser vor Überhitzung und Datenverlust durch Magnetabstrahlung geschützt. Bei späteren Bauserien der in Westdeutschland produzierten VC1571 wurde schließlich zwecks Verringerung der Produktionskosten wieder ein zwar älteres, jedoch geringere Magnetabstrahlungen aufweisendes Netzteil verbaut, sodass auf das Netzteilabschirmblech verzichtet werden konnte.\n\nDie VC1571 kann im GCR-Format maximal 350 kB und im MFM-Format bis zu 410 kB an Daten pro Diskette abspeichern, die VC1570 im GCR-Format maximal 170 kB und im MFM-Format bis zu 200 kB. Laut Wartungsanleitung von Commodore können beide Laufwerke bei Stoßbetrieb sowohl im C128-Modus als auch unter CP/M-Plus Daten mit einer Geschwindigkeit von bis zu 5.200 Baud einlesen. Von diversen Computerzeitschriften durchgeführte Tests ergaben jedoch niedrigere Werte. Dort erreichte die VC1571 unter Idealbedingungen bei Stoßbetrieb im C128-Modus Spitzenwerte von 3.800–4.000 Baud. Unter realistischen, von Disketteninitialisierungen, zahlreichen Spurwechseln und der Sektorensuche geprägten Alltagsbedingungen wurden im C128-Modus Durchschnittswerte von 1.100 Baud bei Normalbetrieb und 2.200 Baud bei Stoßbetrieb gemessen. Bei Schreiboperationen ergaben sich im C128-Modus Messwerte von 400 Baud bei Normalbetrieb und 600 Baud bei Stoßbetrieb.\n\nDer US-amerikanische Hardwareproduzent Emerald Components brachte 1987 mit dem Excel 2001 einen 20 US$ billigeren, softwarekompatiblen Klon der VC1571 mit identischem Diskettenbetriebssystem, etwas kleinerem Gehäuse, externem Schaltnetzteil und verbessertem Wärmemanagement auf den Markt. Auf dem Excel 2001 liefen sämtliche Programme ohne Probleme, inklusive kopiergeschützter C128-Originalsoftware. Die Arbeitsgeschwindigkeit des Geräts entsprach weitgehend dem des Commodore-Originallaufwerks. Die westdeutsche VTS Data GmbH bot ab Anfang 1988 mit dem Blue Chip 128 für 498 DM ebenfalls einen VC1571-Klon mit ähnlichem Leistungsumfang, aber geringeren Geräuschemissionen an.\n\n1987 folgte dann das schnellere und erheblich kleinere Modell VC1581 für die moderneren und kompakteren 3½-Zoll-Disketten. Neben dem bereits erwähnten Hauptprozessor des Typs MOS 6502A und einem Festspeicher von 32 kB ROM verfügt die im Vergleich zu den Vorgängermodellen weniger umständliche und effizientere Elektronik der VC1581 über einen Floppy-Disk-Controller des Typs WD1772 (bzw. WD1770 bei frühen Bauserien) zum Lesen und Beschreiben von Disketten im MFM-Format, einen I/0-Baustein des Typs MOS Technology 8520A (kurz MOS 8520A) und einen als Arbeitsspeicher dienenden 8-kB-ROM-Chip des Typs 4346. Als Taktgeber fungiert ein Standard-Quarzoszillator des Typs 325566-01. Die gut abgeschirmte Laufwerksmechanik mit der Modellbezeichnung Chinon 80 besitzt zwei Schreib-/Leseköpfe und stammt vom gleichnamigen japanischen Zulieferer Chinon. Das Gehäuse der VC1581 misst 14 cm × 23 cm × 6,3 cm (Breite × Tiefe × Höhe) und wiegt lediglich 1,4 kg. Das externe Netzteil mit einer Leistung von 10 Watt sorgte zwar für Kabelsalat, aber auch ein gegenüber den 5¼-Zoll-Vorgängermodellen verbessertes Wärmemanagement. Die VC1581 ist außerdem wesentlich leiser im Betrieb als ihre Vorgängerinnen. Die Geräteadresse kann per DIP-Schalter auf der Rückseite eingestellt werden.\n\nDie Datenübertragungsgeschwindigkeit kann bei Stoßbetrieb mittels eines in den MOS 8520A integrierten Baudratengenerators eingestellt werden. Theoretisch sind im 2-MHz-Modus Übertragungsraten von bis zu 166.000 Baud möglich, die aber in der Praxis aufgrund von Hardwarebeschränkungen beim Rechner nicht erreicht werden können. Mit einer Speicherkapazität von 800 kB pro Diskette, die in bis zu 296 Dateien abgelegt werden können, der Fähigkeit zur Partitionierung von Disketten in verschiedene Unterverzeichnisse und einer unter Testbedingungen gemessenen maximalen Datenübertragungsrate von 7.000–8.500 Baud bei Stoßbetrieb übertrifft die VC1581 das Vorgängermodell VC1571 recht deutlich. Zum Laden einer 36,5 kB umfassenden Datei benötigt die VC1581 bei Standardbetrieb im C128-Modus beispielsweise 6,4 Sekunden, während die VC1571 für die gleiche Aufgabe 11,5 Sekunden braucht. Für das Speichern der gleichen Datenmenge benötigt die VC1581 31,5 Sekunden, die VC1571 dagegen 71 Sekunden. Nicht kopiergeschützte Software konnte auf der VC1581 problemlos verwendet werden. Im Zusammenhang mit kopiergeschützter Software wurde das Gerät meist zusätzlich zu einer VC1571 als Zweitlaufwerk zur reinen Datenspeicherung eingesetzt, etwa bei Datenbankanwendungen, Textverarbeitungen oder im CP/M-Modus.\n\nDie VC1581 wurde Anfang 1988 von den Lesern der auf Commodore-Rechner spezialisierten Computerzeitschrift \"64’er\" in der Kategorie der Peripheriegeräte zum Produkt des Jahres 1987 gewählt.\n\nDie für den Vorgänger C64 konzipierte Datasette des Typs 1530 waren auch zum C128 kompatibel. Nur sehr wenige C128-Besitzer machten allerdings kurz nach der Markteinführung des Rechners von diesem kostengünstigen, jedoch langsamen und überdies recht fehlerträchtigen Speichergerät überhaupt Gebrauch. Meist geschah dies, da in den ersten Monaten nach dem Verkaufsstart des C128 die Diskettenlaufwerke VC1570 sowie VC1571 gar nicht oder noch nicht in größeren Stückzahlen lieferbar waren und daher die Alternativen fehlten.\n\nDanach verloren die Datasetten im Zusammenhang mit dem C128 schnell stark an Bedeutung. Anfang der 1990er Jahre verwendeten kaum noch Besitzer eines Commodore-Heimcomputers ein solches Bandlaufwerk. So gut wie keine kommerzielle Software für den C128-Modus oder den Betrieb unter CP/M-Plus erschien auf den im Vergleich zu Disketten im alltäglichen Gebrauch langsamen, umständlichen und von der Speicherkapazität her unzulänglichen Kompaktkassetten.\n\nMittels eines speziellen Interfaces lassen sich die älteren, von Commodore für den PET 2001 und die Bürocomputer der CBM-Serien 3000, 4000 sowie 8000 entwickelten IEEE-488-Festplatten wie das 5-MB-Modell CBM D9060 bzw. das 7,5-MB-Modell CBM D9090 über den CBM-Bus auch mit dem C128 betreiben. Mehrere US-amerikanische sowie ein westdeutscher Fremdhersteller entwickelten im Gegensatz zu Commodore selbst aber auch gezielt für den C128 ausgelegte, modernere, zuverlässigere und leichter zu bedienende SCSI-Festplattenlaufwerke mit eigenem Betriebssystem, darunter Xetec mit dem auf der Winter Consumer Electronics Show sowie der CEBIT Anfang 1987 vorgestellten, mit eigenem Mikroprozessor, Schnittstellenmodul, Boot-ROM sowie 16 kB RAM ausgestatteten Lt. Kernal. Weitere Festplattenlaufwerke wurden von JCT mit den Modellen JCT-1000, JCT-1005 und JCT-1010 sowie ICT mit den Modellen Data Chief HFD-5, HFD-10 und HFD-20 auf den Markt gebracht. Die Preise bewegten sich zwischen 595 US$ für das günstigste Modell JCT-1000 mit einer Speicherkapazität von 3,7 MB, 949,95 US$ für den Lt. Kernal mit 20 MB sowie 995 US$ für den im Gegensatz zu allen anderen Festplatten CP/M-fähigen Data Chief HFD-20 mit ebenfalls 20 MB freiem Speicherplatz. Als einziges Festplattenlaufwerk unterstützte das 900 US$ teure Device 9 – The Vault von Progressive Peripherals den Burst-Modus des C128. ICT brachte mit dem Mini Chief im Jahr 1988 ein weiteres Festplattenlaufwerk zum Preis von 795 US$ heraus. Das Gerät bestand aus einer umgebauten VC1571, aus der ICT-Techniker das interne Netzteil entfernt und durch eine 20-MB-Festplatte ersetzt hatten. 1991 folgte Creative Micro Designs mit der 20-MB-Festplatte HD-20, die in ihrem nativen Modus betrieben werden kann, aber auch in der Lage ist, die Commodore-Diskettenlaufwerke VC1541, VC1571 sowie VC1581 zu emulieren.\n\nDie Firma Roßmöller Computertechnik GmbH brachte mit der HD 128 ebenfalls eine 20-MB-Festplatte mit integriertem Netzteil für 2.498 DM auf den Markt. Darüber hinaus entwickelte Roßmöller mit dem Modell SASI 128 auch einen an den Userport anzuschließenden Festplatten-Controller für den C128. Die genannten Festplattenlaufwerke erreichten jedoch keine weite Verbreitung.\n\nCommodore produzierte drei gezielt für den C128 entwickelte Speichererweiterungsmodule (englisch \"RAM Expansion Units\"), die neben zusätzlichen dynamische RAM-Chips zur Steuerung indirekter Speicherzugriffe einen eigenen Mikrocontroller (englisch \"RAM Expansion Controller\") besitzen. Untergebracht sind diese Bausteine in ein an das Design des C128 angepasstes Gehäuse, das mittels eines Platinensteckers an die Erweiterungsschnittstelle des Rechners angeschlossen werden kann. Die RAM Expansion Units lassen sich sowohl im C128- als auch im CP/M-Modus verwenden. Dazu ist jedoch eine aktualisierte Version des Betriebssystems CP/M-Plus notwendig, die in Form einer zusätzlichen Diskette zum Lieferumfang gehörte. Die von Frank Palaia konzipierten Speichererweiterungen können aufgrund ihres Stromverbrauchs ohne stärkeres Netzteil jedoch nur mit Einschränkungen auch am Vorgängermodell C64 betrieben werden. Das Low-End-Modell 1700 verfügt über 128 kB, das Modell 1764 über 256 kB und das High-End-Modell 1750 über 512 kB zusätzlichen Arbeitsspeicher. Mit insgesamt 640 kB RAM erreicht der C128 den Vollausbau seiner Speicherkapazität. Der zusätzliche Arbeitsspeicher ist in zwei (Modell 1700), vier (Modell 1764) respektive acht (Modell 1750) Speicherbänke mit jeweils 64 kB RAM aufgeteilt.\n\nDas kleinste Modul 1700 war aufgrund eines Mangels an speziell auf ihre Eigenschaften ausgerichteter Software nur eingeschränkt praxistauglich. Die beiden größeren Module 1764 und 1750 wurden dagegen vor allem im Zusammenhang mit der grafischen Benutzeroberfläche GEOS 128 sowie als schnelle RAM-Disks eingesetzt. Als solche verwendet übertreffen die RAM Expansion Units mit einer Datenübertragungsrate von ca. 1 MB pro Sekunde herkömmliche Diskettenlaufwerke um ein Vielfaches. Unter GEOS 128 lassen sich die Speichererweiterungen auch als Schattenlaufwerke einsetzen, die den Inhalt einer gesamten Diskette zwecks Erhöhung der Arbeitsgeschwindigkeit fortlaufend ohne die Notwendigkeit des Nachladens von Daten im Arbeitsspeicher zur freien Verfügung halten. Eingesetzt wurden sie ferner oft im CP/M-Modus sowie als Alternative zu wesentlich teureren Zweitdiskettenlaufwerken oder schwierig zu montierenden und obendrein langsameren Diskettenlaufwerksbeschleunigern.\n\nNeben Commodore entwickelten auch Fremdhersteller Speichererweiterungen für den C128. Dazu zählt beispielsweise das an die Erweiterungsschnittstelle anzuschließende, in Westdeutschland von Rex Datentechnik in Lizenz hergestellte und vertriebene GeoRAM von Berkeley Softworks mit einer zusätzlichen Speicherkapazität von 512 kB. Das 1989 auf den Markt gebrachte GeoRAM besitzt keinen eigenen Mikrocontroller für direkte Speicherzugriffe und wurde für den Betrieb mit GEOS 128 konzipiert. Im Lieferumfang waren zwei Treiberdisketten enthalten, von denen eine vor der Inbetriebnahme zunächst als Bootdiskette installiert werden musste. Die Bootdiskette beinhaltet zwar die zum Systemstart notwendigen Grundkomponenten beider Versionen von GEOS 128 in einer revidierten Fassung namens \"GEOS 2.0r\", nicht aber die Anwendungen \"GeoWrite\" und \"GeoPaint\".\n\nCommodore brachte mehrere, vornehmlich für eine Verwendung mit dem C128 entwickelte und von verschiedenen Erstausrüstern in Lizenz hergestellte Ausgabegeräte heraus, darunter drei Farbmonitore als Sichtgeräte und einen Nadeldrucker. Eine Gemeinsamkeit aller dieser Ausgabegeräte besteht in ihrer an den C128 und seine komplexe Architektur angepassten Vielseitigkeit.\n\nDie Monitore sind durchgehend sowohl auf die Darstellung von 40 als auch 80 Zeichen pro Zeile mit einer Standardauflösung von 640 × 200 Bildpunkten ausgelegt, RGBI-fähig und besitzen allesamt ins Gehäuse integrierte Mono-Lautsprecher. Sie unterscheiden sich jedoch u. a. hinsichtlich ihrer Farbfähigkeit, Signalverarbeitung, Konnektivität, Gehäuseform und der Größe ihrer Bildschirmdiagonalen. Neben den gezielt für den C128 herausgebrachten Farbmonitoren ließ sich der Rechner aber auch noch mit vielen anderen Commodore-Farbmonitoren betreiben, etwa dem 1988 erschienenen, für sämtliche Commodore-Heimcomputer geeigneten Commodore 1804 oder den primär für die Amiga-Reihe konzipierten Modellen Commodore 2002, Commodore 1084 oder Commodore 1084S. Ähnliches gilt für die zur MPS-Serie gehörenden Druckermodelle der 12xx-Reihe wie etwa den Commodore MPS 1230 oder den Commodore MPS 1250. Hinzu kamen zahlreiche Monitore sowie Drucker von Fremdanbietern.\n\nMit dem Modell 1901 brachte Commodore einen zur PAL-Norm kompatiblen 14-Zoll-Farbmonitor auf den westeuropäischen Markt. Das in Singapur in Lizenz vom französischen Elektronikkonzern Thomson gefertigte Gerät verfügt über eine neunpolige Sub-D-Eingangsbuchse für das digitale RGBI-Signal sowie drei umgangssprachlich oft als Cinch-Buchsen bezeichnete RCA-Stiftbuchsen. Zwei dieser Stiftbuchsen sind für das analoge, sich aus dem Chrominanzsignal für die Farbigkeit und dem Luminanzsignal für die Helligkeit zusammensetztende FBAS-Signal zuständig. Die Übertragung von Chrominanzsignal und Luminanzsignal über voneinander getrennte Leitungen entspricht dabei dem in der englischsprachigen Welt als Composite Video bekannten Übertragungsverfahren, das dem hierfür nur eine Leitung verwendenden FBAS-Signal qualitativ leicht überlegen ist. Die dritte Stiftbuchse dient schließlich der Übertragung des ebenfalls analogen Mono-Audiosignals. Das Gerät war für seine hohe Bildqualität und scharfe Darstellung von Buchstaben im 80-Zeichen-Modus bekannt. Spätere Bauserien des Commodore 1901 besitzen überdies meist zusätzlich eine SCART-Buchse.\n\nÜber zahlreiche Drehregler lassen sich u. a. Farbintensität, Helligkeit, Kontrast, Lautstärke, Fokus und Bildhöhe einstellen. Der Commodore 1901 besitzt außerdem auf der Rückseite einen Schiebeschalter zum Wechseln zwischen analogem PAL-Betrieb (40-Zeichen-Modus) und digitalem RGBI-Betrieb (80-Zeichen-Modus). Der 10 cm × 10 cm große Mono-Lautsprecher hat eine Leistungsaufnahme von einem Watt. Das recht voluminöse Gerät misst 36 cm × 37 cm × 34,5 cm (Breite × Tiefe × Höhe) und wiegt 9,5 kg. Die Leistungsaufnahme beträgt 60 Watt.\n\nMit dem Modell 1902 veröffentlichte Commodore einen im Vergleich zum Commodore 1901 etwas kompakteren 13-Zoll-Farbmonitor. Das vielseitige, für den nordamerikanischen Markt produzierte und daher zum NTSC-Standard kompatible Gerät kann im 40-Zeichen-Modus allerdings neben Composite Video auch das höherwertige, ebenfalls analoge S-Video-Signal verarbeiten. Der in Japan von Fujitsu in Lizenz gefertigte Commodore 1902 gleicht hinsichtlich seiner Konnektivität weitgehend dem Commodore 1901, verfügt aber zusätzlich über einen separaten Kopfhöreranschluss. Außerdem befindet sich die Feststelltaste zum Wechsel zwischen analogem NTSC-Betrieb (40-Zeichen-Modus) und digitalem RGBI-Betrieb (80-Zeichen-Modus) auf der Vorderseite.\n\nMit dem Modell 1902A brachte Commodore einen weiteren NTSC-fähigen, vom niederländischen Elektronikkonzern Philips als Lizenznehmer in Taiwan gefertigten 13-Zoll-Farbmonitor auf den nordamerikanischen Markt. Diese Modellvariante ist allerdings im Gegensatz zum ansonsten technisch ähnlich konzipierten Commodore 1902 nicht zur Verarbeitung des höherwertigen S-Video-Signals fähig. Auch hinsichtlich der Konnektivität unterscheidet sich der Commodore 1902A von den Modellen 1901 und 1902. So besitzt das Gerät eine 8-polige DIN-Buchse für das digitale RGBI-Signal und eine 6-polige DIN-Buchse für die Eingangssignale des analogen Composite Video sowie das Audiosignal. Wie das Modell 1902 weist auch der Commodore 1902A einen eigenen Kopfhöreranschluss auf. Das Gerät misst 35 cm × 38,7 cm × 32 cm (Breite × Tiefe × Höhe), die Leistungsaufnahme beträgt 75 Watt.\n\nMit dem Modell MPS 1200 erschien auch ein primär auf den Betrieb mit dem C128 ausgelegter Nadeldrucker von Commodore. Das auf dem Citizen 120D des japanischen Herstellers Citizen Holdings basierende Gerät verfügt über einen Druckkopf mit neun Nadeln und verwendet im Standardmodus eine aus 9 × 9 Punkten sowie im Schönschriftmodus (englisch \"Near Letter Quality\") eine aus 17 × 17 Punkten bestehende Buchstabenmatrix. Je nach gewählter Druckqualität druckt der Commodore MPS 1200 mit einer Geschwindigkeit von 24 Zeichen pro Sekunde im Schönschriftmodus oder bis zu 120 Zeichen pro Sekunde im Standardmodus. Erreicht werden diese Geschwindigkeiten durch bidirektionalen Druck. Zwecks Verbindung zum proprietären CBM-Bus des Rechners verfügt die Standardausführung Commodore MPS 1200 über zwei als 6-polige DIN-Buchsen ausgeführte serielle Schnittstellen, während die Modellvariante MPS 1200P eine parallele Centronics-Schnittstelle (englisch \"Centronics Parallel Basic Interface Pack\") aufweist.\n\nDer Commodore MPS 1200 verfügt ferner über einen Festspeicher mit 64 kB ROM sowie einen Arbeitsspeicher mit 8 kB RAM. Der Arbeitsspeicher dient einerseits als Pufferspeicher, andererseits zum Abspeichern benutzerdefinierter Zeichensätze. Die Modellvariante MPS 1200P besitzt zwar lediglich 32 kB ROM, dafür aber eine eigene CPU. Der Festspeicher beider Modellversionen enthält sämtliche internationalen Zeichensätze aller C128-Varianten, die per DIP-Schaler gewählt werden können. Das Gerät misst 40,2 cm × 25,5 cm × 9 cm (Breite × Tiefe × Höhe) und wiegt 3,7 kg. Die Leistungsaufnahme beträgt 50 Watt.\n\nFür die Datenfernübertragung veröffentlichte Commodore zunächst das als Steckmodul ausgeführte Modem 1660. Das Gerät stammt aus dem Jahr 1985. Es ließ sich mit dem C64 und dem C128 verwenden, sowohl im C64- als auch im C128-Modus mit 80-Zeichen-Bildschirm. Das Modem 1660 überträgt Daten mit 300 Baud. Es beherrscht hinsichtlich der Datenübermittlungsweise sowohl Wechselbetrieb (englisch \"half-duplex\") als auch Gegenbetrieb (englisch \"full-duplex\"). Der Anschluss an den Rechner erfolgt über den Userport. Auf der Rückseite besitzt das Modem 1660 zwei Modularbuchsen zur Verbindung mit dem Telefon und dem Telefonanschluss. Außerdem weist das Gerät einen Audioeingang mit RCA-Stiftbuchse für Cinch-Stecker auf. Zum Lieferumfang des Geräts gehörte neben den Verbindungskabeln auch ein aus dem Telekommunikationsprogramm \"QuantumLink\" sowie dem Terminalprogramm \"Common Sense\" bestehendes Softwarepaket.\n\nFür die Datenfernübertragung entwickelte Commodore ferner das ebenfalls als Steckmodul ausgeführte Modem 1670. Das Gerät erschien 1987 und war primär auf eine Verwendung mit dem C128 zugeschnitten, konnte aber auch am C64 betrieben werden. Das Modem 1670 überträgt Daten wahlweise mit 300 oder 1.200 Baud. Wie das Vorgängermodell 1660 beherrscht es sowohl Wechselbetrieb als auch Gegenbetrieb, ist zusätzlich aber Hayes-kompatibel. Der Anschluss an den Rechner erfolgt wie beim Modem 1660 über den Userport. Auf der Rückseite des Modems 1200 befinden sich lediglich zwei Modularbuchsen zur Verbindung mit dem Telefon und dem Telefonanschluss, aber kein separater Audioeingang. Die Geräteadresse lässt sich per DIP-Schalter einstellen. Zum Lieferumfang gehörten wie beim Modem 1660 die Verbindungskabel sowie das für die Bedienung notwendige Softwarepaket.\n\nMit dem BTX-Decoder-Modul II brachte Commodore im vierten Quartal des Jahres 1987 auch einen als Steckmodul ausgeführten Dekodierer für das in den deutschsprachigen Ländern angebotene Endbenutzer-Informationssystem Bildschirmtext (kurz BTX) für den C128 auf den Markt. Das BTX-Decoder-Modul II löste das noch für den C64 entwickelte, Ende 1985 auf den Markt gebrachte Vorgängermodell BTX-Decoder-Modul I ab und war mit 399 DM rund 300 DM billiger als dieses. Es wird über die Erweiterungsschnittstelle an den Rechner angeschlossen und kann wahlweise über drei DIN-Buchsen an der Rückseite mit einem Commdore-Monitor, RGB-Monitor oder Fernseher mit SCART-Anschluss verbunden werden. Das BTX-Decoder-Modul II konnte sowohl mit einem Akustikkoppler als auch der BTX-Modem-Box DBT03 der Deutschen Bundespost betrieben werden.\n\nDas Herzstück der Elektronik des BTX-Decoder-Moduls II bildet der im Auftrag des damaligen Bundespostministeriums hergestellte Spezialbaustein EUROM (kurz für englisch \"European Read Only Memory\"). Der auch als SAA 5350 bekannte, sehr hitzeempfindliche EUROM-Chip wurde von der Philips-Tochtergesellschaft Valvo entwickelt. Er besteht aus einem Videoprozessor und einem Zeichengenerator zur Darstellung der insgesamt 520 europaweit genormten Zeichen sowie grafischen Symbole des BTX-Zeichensatzes inklusive Umlauten, die zur Erzeugung der vom Anwender aufgerufenen Bildschirmtextseiten benötigt wurden. Verwendet wird für den BTX-Zeichensatz eine Punktmatrix mit 12 × 10 Punkten. Der EUROM-Chip kann auch im 80-Zeichen-Modus betrieben werden und ist fähig, Farbgrafiken mit einer Auflösung von 480 × 240 Bildpunkten zu generieren. Er ist ferner in der Lage, 4.096 Farben auf dem Bildschirm darzustellen. Davon können 32 gleichzeitig verwendet werden. Zur besseren Wärmeabfuhr besitzt der EUROM-Chip ein Keramik-Gehäuse mit 40 Anschlusspins. Überdies verfügt das von Siemens gefertigte, auch für den Betrieb mit dem C64 geeignete BTX-Decoder-Modul II über 32-kB-EPROM-Festspeicher für die BTX-Betriebssoftware und unterstützte neben dem in Westdeutschland damals üblichen BTX-Standard CEPT-1 auch den Standard PRESTEL für den in Großbritannien verwendeten Abrufdienst Viewdata sowie den Standard ANTIOP für den in Frankreich weit verbreiteten Abrufdienst Minitel.\n\nDie Betriebssoftware des BTX-Decoder-Moduls II steht dem Anwender direkt nach der Inbetriebnahme des Rechners zur Verfügung. Es gibt mehrere Versionen der Betriebssoftware. Da bei der von Commodore zuletzt entwickelten Version 3.3 sowie der vom Markt+Technik Verlag veröffentlichten Version 3.4 gelegentlich Probleme beim Betrieb mit der BTX-Modem-Box DBT03 auftraten, wurde Mitte 1989 mit der Version 3.5 eine fehlerbereinigte Fassung erstellt. Sie konnte von der BTX-Seite des Markt+Technik Verlages kostenlos heruntergeladen werden. Der ab Werk eingebaute ROM-Chip mit der Betriebssoftware musste zwecks Installation durch einen wahlweise selbstgebrannten oder beim Markt+Technik Verlag zu beziehenden EPROM-Chip mit der Version 3.5 ersetzt werden.\n\nÜber die mit den wichtigsten BTX-Steuerzeichen belegten Funktionstasten lässt sich das Modul bedienen. Eine beigefügte Tastaturschablone sollte Anfängern das Erlernen der BTX-Steuerzeichen erleichtern. Außerdem weist die Betriebssoftware eine Protokollfunktion auf und ermöglicht mit Einschränkungen das Erstellen von Screenshots. Sogar das damals noch ungewohnte Herunterladen und Abspeichern von Programmen auf Diskette war über das BTX-Decoder-Modul II möglich. Mit dem Einsatz dieser „Telesoftware“ entfiel die umständliche und zeitraubende Notwendigkeit, Programmlistungs aus Zeitschriften abzutippen. Die Computerzeitschrift \"64’er\" bot zu diesem Zweck ab Ende 1988 eine eigene, auch für den C128-Modus geeignete Software zum Herunterladen enthaltende BTX-Seite an. Auch etwa der seit 1981 ausgestrahlte WDR Computerclub verfügte über ein vergleichbares BTX-Angebot. Darüber hinaus wurden ab Ende der 1980er Jahre sowohl kostenlose als auch kostenpflichtige Computerspiele über BTX-Seiten angeboten.\n\nDie beiden Versionen des in den drei Modellvarianten des C128 verbauten 80-Zeichen-Grafikchips boten Drittherstellern die Möglichkeit zur Entwicklung von Hardwarezusätzen mit dem Ziel der Verbesserung der Grafikfähigkeiten, da sowohl der MOS 8563 als auch der MOS 8568 vom Betriebssystem des Rechners nicht ausgenutzte höhere Auflösungen beherrschen und mit 64 kB VRAM viermal so viel Grafikspeicher adressieren können als die in den älteren Modellvarianten C128 bzw. C128D ab Werk eingebauten 16 kB VRAM.\n\nDie schweizerische Combo AG brachte mit dem \"Graphic Booster 128\" im Jahr 1986 eine für 174 DM erhältliche Grafikerweiterung für den C128 und den C128D heraus. Für den Einbau musste der 80-Zeichen-Grafikchip MOS 8563 zunächst entfernt und an seine Stelle eine mitgelieferte Zusatzplatine eingesetzt werden. Auf dieser befinden sich ein freier Steckplatz für den MOS 8563 sowie bereits eingebaute 48 kB zusätzliches VRAM. Die Zusatzplatine wird über einen Draht, an dessen Ende sich ein Sockel mit 28 Anschlusspins befindet, an den freien Steckplatz des C128 angeschlossen. Die auf Diskette beigefügte Software erweitert das native Commodore BASIC V7.0 um zusätzliche Grafikbefehle und erleichtert die Verwendung der jenseits der Standardeinstellung von 640 × 200 Bildpunkten liegenden, vom Betriebssystem jedoch nicht unterstützten Auflösungen von bis zu 720 × 700 Bildpunkten des MOS 8563 im Interlace-Modus und maximal 720 × 400 Bildpunkten im flimmerfreien Normalmodus. Für jeden der insgesamt 7.200 jeweils 6 × 8 Bildpunkte umfassenden Farbblöcke können bei der verbesserten Version \"Graphic Booster 128 N2\" aus einer Palette von rund 65.000 Farben 256 für den Vordergrund und 256 für den Hintergrund gewählt werden. Im Textmodus können – wie bei den für den IBM-PC AT entwickelten EGA-Grafikkarten − bis zu 80 × 43 Zeichen angezeigt werden. Auch die Verwendung der 512-kB-Speichererweiterung 1750 wird von der Software des \"Graphic Booster 128\" unterstützt. Spätere Versionen enthielten die BASIC-Erweiterung wahlweise auf einem EPROM. Beim C128D-CR musste dieses lediglich in den freien Steckplatz eingesetzt oder die BASIC-Erweiterung von Diskette geladen werden, da der Grafikspeicher des Rechners bereits ab Werk voll ausgebaut war. Die Hardware des \"Graphic Booster 128\" ließ sich nicht ohne Weiteres von anderen Programmen nutzen. Für bestimmte kommerzielle Software wie etwa das Grafikprogramm \"StarPainter 128\", die Textverarbeitung \"Protext 128\" oder die grafische Benutzeroberfläche GEOS 128 erschienen zu diesem Zweck Patches.\n\nIn der Fachpresse wurde 1988 die Vermutung geäußert, Commodore habe Teile der Software des \"Graphic Booster 128\" ohne Wissen der Combo AG in das Betriebssystem des C128D-CR integriert. Diese Behauptung ist jedoch falsch und basiert auf Unkenntnis der beiden schlecht dokumentierten 80-Zeichen-Grafikchips. So war selbst in Fachkreisen weitgehend unbekannt, dass sowohl der MOS 8563 als auch der MOS 8568 schon ab Werk bei nur geringem Programmieraufwand in der Lage sind, jenseits der vom C128-Betriebssystem verwendeten Standardeinstellung von 640 × 200 Bildpunkten liegende Auflösungen zu generieren, deren Punktdichte vornehmlich von der Größe des zur Verfügung stehenden dedizierten Grafikspeichers abhängt.\n\nSoftware Support International vertrieb ebenfalls eine als Steckkarte für den Sockel des MOS 8563 ausgeführte Grafikspeichererweiterung für den C128 bzw. C128D. Zum Lieferumfang gehörten eine Zusatzplatine sowie die zum Grafikspeichervollausbau auf 64 kB nötigen VRAM-Chips. Im Gegensatz zum \"Graphic Booster 128\" enthielt das mit 34,95 US$ wesentlich günstigere Angebot jedoch keine die Verwendung des erweiterten Grafikspeichers unterstützende Software.\n\nMehrere US-amerikanische sowie westdeutsche Elektronikfirmen entwickelten zwecks zusätzlicher Steigerung der Datenübertragungsrate auch als Floppy-Speeder bekannte, in der Regel alternative Diskettenbetriebssystemsoftware mit Hardwarezusätzen kombinierende Diskettenlaufwerksbeschleuniger zum Einbauen für die 5¼-Zoll-Diskettenlaufwerke VC1570/71.\n\nDer westdeutsche Anbieter Dolphin Software brachte 1987 das auch für die beiden Desktop-Modelle geeignete, mit zwei EPROM-Chips ausgestattete \"DolphinDOS 128\" für 198 DM auf den Markt. Ein EPROM-Chip enthält das Diskettenbetriebssystem für den C128-Modus, der andere das für den C64-Modus. Im C64-Modus kann zwischen dem originalen Commodore DOS, dem vollständigen \"DolphinDOS\" sowie einer abgespeckten Version des alternativen Diskettenbetriebssystems gewählt werden, die zwecks Wahrung größtmöglicher Softwarekompatibilität zu handelsüblichen C64-Programmen nur die schnellen Laderoutinen enthält. Im C128-Modus bietet \"Dolphin DOS 128\" neben schnelleren Lade- und Speicheroperationen auch eine beschleunigte Formaterkennung sowie eine schnellere Autoboot-Funktion. Die mitgelieferte Zusatzplatine ersetzt den VC1571-Hauptprozessor MOS 6502A, der zu diesem Zweck herausgenommen bzw. bei nicht gesockelten CPUs ausgelötet und wieder in die Zusatzplatine eingesteckt werden muss. Die Zusatzplatine wird per Flachbandkabel an den Userport angeschlossen. 1989 erschien eine verbesserte Version von \"DolphinDOS 128\" mit nur einem EPROM-Chip für 220 DM.\n\nDer US-amerikanische Hersteller Creative Micro Designs brachte 1989 den erfolgreichen, zu den originalen Commodore-Diskettenbetriebssystemen kompatiblen Diskettenlaufwerksbeschleuniger \"JiffyDOS 128\" heraus. \"JiffyDOS 128\" ist sowohl im C64- als auch im C128-Modus lauffähig und erschien in mehreren Versionen, darunter eine 69,95 US$ kostende für den C128D, eine für die VC1571 sowie eine für die VC1581 zum Preis von jeweils 29,95 US$. Zur Inbetriebnahme musste das originale Diskettenbetriebssystem Commodore DOS 3.0 der VC1570/71 bzw. des C128D sowie das Commodore DOS 3.1 des C128D-CR durch einen mitgelieferten, das alternative Diskettenbetriebssystem enthaltenden ROM-Chip ersetzt werden. Mittels eines Kippschalters kann zwischen \"JiffyDOS 128\" und Commodore DOS 3.0 bzw. 3.1 hin und her gewechselt werden. Unter \"JiffyDOS 128\" können mit der VC1570/71 im C128-Modus Geschwindigkeitszuwächse von bis zu 400 Prozent beim Laden und fast 100 Prozent beim Speichern erreicht werden. Mit der VC1581 lassen sich im C128-Modus Daten bis zu sechsmal so schnell laden und bis zu dreimal so schnell speichern.\n\nDer westdeutsche Anbieter Roßmöller veröffentlichte 1986 mit \"Mach 70\" und \"Mach 71\" Floppy-Speeder jeweils für die VC1570 bzw. VC1571. Die Urversion von \"Mach 70/71\" besteht aus einer Zusatzplatine, die per Flachbandkabel an die Erweiterungsschnittstelle angeschlossen wird. Spätere Varianten werden dagegen an den Userport angebunden. Dadurch kann der Diskettenlaufwerksbeschleuniger gleichzeitig mit Speichererweiterungen, Druckern, Modems und Akustikkopplern betrieben werden. Bis zu vier Diskettenlaufwerke können unter \"Mach 70/71\" gleichzeitig beschleunigt werden. Im Lieferumfang war auch ein Patch mit Anpassungen an das Betriebssystem CP/M-Plus enthalten, sodass der Rechner im CP/M-Modus unter Verwendung des 80-Zeichen Grafikchips im 2-MHz-Modus fast doppelt so schnell laden und speichern kann. Neben einer Zusatzplatine mit der notwendigen Elektronik war im Lieferumfang auch ein ROM-Chip für den freien Steckplatz des C128 mit Kopier- und Hilfsprogrammen enthalten. Zur Inbetriebnahme braucht der VC1571-Hauptprozessor MOS 6502A nicht entfernt zu werden. Beide Versionen für die Stand-Alone-Diskettenlaufwerke VC1570/71 kosteten 259 DM, die Version für die Desktop-Modelle dagegen 298 DM.\n\nDie westdeutsche VTS Data GmbH brachte 1987 mit \"Professional DOS\" einen besonders schnellen Diskettenlaufwerksbeschleuniger heraus, der in allen drei Betriebsmodi verwendet werden kann. Allerdings ist der Geschwindigkeitszuwachs im CP/M-Modus vergleichsweise gering. \"Professional DOS\" wurde in zwei Varianten angeboten. Die teurere, zunächst für 298 DM erhältliche, später dann auf 258 DM reduzierte Ausführung lässt sich an die Erweiterungsschnittstelle anschließen und erlaubt eine gleichzeitige Verwendung eines Modems bzw. Akustikkopplers. Die zunächst mit 239 DM, später dann 189 DM günstigere und aufgrund der Verwendung eines eigenen Dateiformats um einiges schnellere Ausführung wird über den Userport mit dem Rechner verbunden, gestattet aber im Gegensatz zur teureren Variante keine gleichzeitige Datenfernübertragung. Beide Varianten erlauben auch im C64-Modus die Verwendung des numerischen Ziffernblocks. Zur Installation muss der VC1571-Hauptprozessor MOS 6502A aus seinem Sockel entfernt und in eine mitgelieferte Zusatzplatine eingesteckt werden, die über zwei EPROM-Chips mit der Betriebssoftware verfügt und ihrerseits per Flachbandkabel an den Prozessorsockel angeschlossen wird. Nicht gesockelte Hauptprozessoren müssen vor der Inbetriebnahme ausgelötet werden.\n\nVon der westdeutschen Firma Lamm Computersysteme stammt das 1987 auf den Markt gebrachte und an den Userport anzuschließende, jedoch nicht unter CP/M lauffähige \"Prospeed 71\" mit um den Faktor 27 gesteigerter Ladegeschwindigkeit, zusätzlichen Diskettenbefehlen und integriertem Kopierprogramm für 198 DM. Beim Einbau der mitgelieferten Zusatzplatine braucht der VC1571-Hauptprozessor MOS 6502A nicht ausgelötet zu werden. Überdies erhöht die Betriebssoftware die Softwarekompatibilität der VC1571 zum Vorgängermodell VC1541 durch Spiegelung der erweiterten Zeropage ab Speicheradresse codice_17, da einige C64-Programme entgegen den Empfehlungen des Herstellers auf diesen Bereich zugreifen. Unter \"ProSpeed 71\" wird außerdem der numerische Ziffernblock des C128 auch im C64-Modus aktiviert.\n\nAus dem gleichen Haus stammt auch der noch leistungsfähigere, sowohl im C64- als auch im C128-Modus arbeitende Floppy-Speeder \"ProSpeed GTI\" für 248 DM. Dessen 1989 veröffentlichter Nachfolger \"ProSpeed GTI 2.0\" besteht aus einer Zusatzplatine u. a. mit einem I/O-Baustein des Typs MOS 6526 sowie einem freien Steckplatz für das zwecks Wahrung größtmöglicher Softwarekompatibilität von der Hauptplatine zu entfernende Betriebssystem-ROM, das seinerseits durch einen EPROM-Chip mit den schnellen Datenübertragungsroutinen des \"ProSpeed\"-Diskettenbetriebssystems sowie gleich drei Kopierprogrammen ersetzt werden muss. Da das \"ProSpeed\"-Diskettenbetriebssystem einen höheren Kompatibilitätsgrad zum originalen Commodore DOS 3.0 des C128 bzw. C128D aufweist als das im C128D-CR implementierte Commodore DOS 3.1, lassen sich fast alle auf dem C128D-CR im Auslieferungszustand ohne Weiteres nicht lauffähigen Programme auf einem C128D-CR mit \"ProSpeed GTI 2.0\" problemlos betreiben.\n\nDer US-amerikanische Anbieter Access Software veröffentlichte 1986 mit \"Mach 128\" für 49,95 US$ einen an die Erweiterungsschnittstelle anzuschließenden Floppy-Speeder mit Hilfsprogrammpaket sowie eigenen festgespeicherten Diskettenbetriebssystemroutinen, der sich sowohl im C64- als auch im C128-Modus verwenden lässt und die Ladezeiten in beiden Betriebsarten ungefähr um das Fünf- bis Zehnfache beschleunigt.\n\nMit \"Magic Formel 128\" von der westdeutschen Firma Grewe Computertechnik erschien 1988 ein weiterer Floppy-Speeder für 238 DM.\n\nCreative Micro Designs brachte mit der SuperCPU 128 (auch SuperCPU v2) für 259 US$ als einziger Hardwarehersteller im März 1998 und damit neun Jahre nach Produktionseinstellung noch eine auf Erhöhung der Arbeitsgeschwindigkeit des Rechners ausgerichtete Beschleunigerkarte für den C128 auf den Markt.\n\nDas an die Erweiterungsschnittstelle anzuschließende Modul mit durchgeschleiftem Platinenstecker verfügt über 256 kB statisches RAM, bis zu 512 kB ROM sowie einen wahlweise zuschaltbaren, auf 20 MHz getakteten 16-Bit-Hauptprozessor des Typs Western Design Center 65816 (kurz WDC 65816), der sowohl zum MOS 6510 als auch zum MOS 8502 abwärtskompatibel ist. Der Festspeicher enthält die für den C64- bzw. C-128-Modus notwendigen Betriebssystemroutinen sowie das alternative Diskettenbetriebssystem \"JiffyDOS\" zur Beschleunigung der angeschlossenen Diskettenlaufwerke. Die mit eigener Statusanzeige ausgestattete Beschleunigerkarte weist überdies drei Kippschalter zum Ein- und Ausschalten des Moduls, Aktivieren des integrierten Floppy-Speeders sowie zur Inbetriebnahme des 20-MHz-Modus auf.\n\nMit dem Quickbyte II und dem Pulsar 128 gaben die westdeutschen Firmen Jann Datentechnik und Roßmöller jeweils eigene 8-Bit-EPROM-Programmiergeräte für den C128-Modus heraus. Beide umgangssprachlich auch als EPROM-Brenner bezeichneten Geräte werden über die Erweiterungsschnittstelle angeschlossen. Der Pulsar 128 belegt zusätzlich den Userport. Die Spannungsversorgung beider Brenner erfolgt über den Rechner. Die Betriebssoftware des Pulsar 128 muss von Diskette geladen werden, während die des Quickbyte II in Form eines ROM-Chips ins Gerät integriert ist. Zum Lieferumfang beider EPROM-Programmiergeräte gehörten außerdem ein zusätzliches Softwarepaket mit Maschinensprachemonitor und Hilfsprogrammen für die Dateiverwaltung. Die Treibersoftware des mit durchgeschleifter Erweiterungsschnittstelle ausgestatteten Quickbyte II wird nach dem Einschalten des Rechners automatisch initialisiert. Die Betriebssoftware des sich mit insgesamt 24 verschiedenen EPROM- bzw. EEPROM-Typen verwenden lassenden Quickbyte II verfügt ferner über ein optional zuschaltbares Schnellladeprogramm. Der Brenner besitzt aber kein die Elektronik schützendes Gehäuse.\n\nAuch das ebenfalls westdeutsche Unternehmen Alcomp bot ein 8-Bit-EPROM-Programmiergerät für den C128-Modus an. Das als C64/C128-Eprommer bezeichnete, auch im C64-Modus verwendbare Gerät besitzt ein viereckiges Gehäuse mit aufgesetztem Sockel zur Aufnahme des EPROM-Chips und wird an den Userport des C128 angeschlossen, über den es auch seine Betriebsspannung bezieht. Der C64/C128-Eprommer kann mit sämtlichen damals gängigen EPROM-Typen mit bis zu 64 kB Speicherkapazität verwendet werden und erkennt selbständig die notwendige Programmierspannung. Die Betriebssoftware für den C64-Modus sowie den C128-Modus wurde auf Diskette mitgeliefert und arbeitet mit einem Fenstersystem.\n\nZahlreiche Firmen wie Alcomp, Dela Elektronik, Jann Datentechnik, Klemmer & Schulter, Markt+Technik, Mükra, Message Computer, Rex Datentechnik oder Roßmöller produzierten weitere, an die Erweiterungsschnittstelle bzw. den Userport des C128 anzuschließende Hardwarezusätze wie für den Anschluss von Druckern, Plottern und Messgeräten benötigte IEEE-488-Schnittstellen, Schaltinterfaces, Betriebssystem-Umschaltplatinen, EPROM-Bänke, Schnittstellenweichen oder Mehrfach-Steckplätze.\n\nRoßmöller entwickelte ferner mit den Modellen Stereo 128 eine Stereo-Soundkarte, CP/M 128 eine schneller als die C128-Portierung von CP/M-Plus arbeitende CP/M-Steckkarte, PAL 128 ein Programmiergerät für Logikgatter sowie Shugart 128 einen Controller für bis zu vier Diskettenlaufwerke für den C128. Das Ingenieurbüro Hollmann brachte einen EPROM-Chip mit deutschem Zeichensatz für den CP/M-Modus heraus. Ein weiteres EPROM für gestochen scharfe Textausgabe im 80-Zeichen-Modus namens Graphic Editor 128 erschien von der Combo AG.\n\nDie US-amerikanische Firma Ketek bot mit dem Command Center für 149,95 US$ ein alternatives Gehäuse mit integriertem Netzteil und Lüfter an, in dem ein C128 sowie zwei VC1571 untergebracht werden können. Das Command Center verfügt außerdem über einen Telefonanschluss sowie einen Hauptnetzschalter, über den sich Rechner und Diskettenlaufwerke gleichzeitig an- und ausschalten lassen.\n\nDie westdeutsche Firma Idee-Soft brachte eine Tastaturschablone mit Übersichten der Befehlssätze des Commodore BASIC V7.0 sowie des Maschinensprachemonitors auf den Markt. Überdies enthielt die Tastaturschablone Informationen über den ASCII-Zeichencode, die CP/M-Funktionscodes, nützliche codice_18-Befehle, Hilfsgrafiken für die Erstellung von Sprites und Erläuterungen zu den Fehlermeldungen des C128. Ferner wurden Abdeckhauben aus Kunststoff sowie Staubschutzhüllen aus Vinyl von diversen Anbietern wie etwa dem US-amerikanischen Unternehmen Omicron Industries im Rahmen der \"Classic-Covers\"-Serie auf den Markt gebracht.\n\nDie für die drei Betriebsmodi des C128 erhältliche Software lässt sich in Systemprogramme, native und optionale höhere Programmiersprachen, maschinennahe Programmiersprachen, Lernprogramme, Anwendungsprogramme und Spiele klassifizieren. Insgesamt wurden schätzungsweise 10.000 auf dem C128 lauffähige kommerzielle Programme entwickelt und veröffentlicht.\n\nZum Zeitpunkt der Markteinführung existierte bereits ein sehr umfangreiches Softwareangebot für den C64-Modus, das zwar unterschiedlichste Einsatzgebiete abdeckte, aber keinen Gebrauch von der weiterentwickelten Hardware des C128 machte. Der C128-Modus wurde dagegen von den Softwarehäusern zum Ärger der Anwender vernachlässigt. Insbesondere auf dem Gebiet der Spiele blieb das Angebot überschaubar. Den Löwenanteil der für den C128-Modus geschriebenen kommerziellen Software machen Anwendungsprogramme wie Textverarbeitungen, Grafikprogramme, CAD-Anwendungen, Datenbankanwendungen, Steuer- und Finanzsoftware, Tabellenkalkulationen sowie Büroanwendungen aus.\n\nFür den Betrieb unter CP/M-Plus gab es sowohl kostenlose Public-Domain-Software als auch zahlreiche professionelle Anwendungsprogramme, die hauptsächlich von US-amerikanischen Anbietern stammten. In Europa war diese Anwendungssoftware oft nur schwer erhältlich und zudem aufgrund der hohen Importkosten für viele Endverbraucher unerschwinglich. So kosteten die meisten professionellen CP/M-Programme 1985 in Westdeutschland noch um die 1.000 DM. Ab dem vierten Quartal des Jahres 1985 fielen die Preise für kommerzielle CP/M-Anwendungen jedoch auf durchschnittlich rund 200 DM. Grund hierfür war die zunehmende Verbreitung von kostengünstigen CP/M-Rechnern wie eben dem C128 oder den Heimcomputermodellen der Schneider-CPC-Serie, die vorübergehend noch einmal für eine deutliche Vergrößerung der Anwenderbasis des bereits seit zehn Jahren erhältlichen, aber zunehmend von MS-DOS bzw. PC-DOS verdrängten CP/M-Betriebssystems sorgte.\n\nDa der C128 mit dem C64-Modus, dem C128-Modus und dem CP/M-Modus über gleich drei unabhängig voneinander operierende Betriebsmodi verfügt, besitzt er eine entsprechend umfangreiche Systemsoftware. Ihre Hauptaufgabe besteht in der Vermittlung zwischen den im Arbeitsspeicher abgelegten Anwenderprogrammen und der Hardware des Rechners. Dafür stehen den Betriebssystemen des C128 jeweils eigene Systemprogramme zur Verfügung, die verschiedene Verwaltungs- und Dienstleistungsfunktionen übernehmen. Zur Verwaltung zählt neben der Fehlerbearbeitung insbesondere die Steuerung der Datenflüsse zwischen Mikroprozessoren, Arbeitsspeicher, Tastatur und Peripheriegeräten. An Dienstleistungen stellen die jeweiligen Betriebssysteme dem Anwender beispielsweise Programmiersprachen mit Interpreter, Texteditoren sowie Dateiverwaltungsprogramme zur Verfügung. Die für den C64- sowie den C128-Modus benötigten Systemprogramme befinden sich im Gegensatz zum CP/M-Modus im Festspeicher und sind daher direkt nach dem Einschalten einsatzbereit. Aufgrund der Autonomie der einzelnen Betriebssysteme ist der Wechsel von einer Betriebsart zur anderen nur bedingt möglich und setzt das Löschen der aktuellen Programmspeicherinhalte voraus.\n\nAb 1986 veröffentlichte die US-amerikanische Softwarefirma Berkeley Softworks die grafische Benutzeroberfläche GEOS in jeweils eigenen Versionen für den C128- bzw. C64-Modus als bedienungsfreundliche und zeitgemäße Alternative zu den drei ursprünglichen, ab Werk implementierten Betriebssystemen, die umständliche Kommandozeileninterpreter verwenden und tiefergehende Computerkenntnisse seitens des Anwenders voraussetzen. Die C128-Version GEOS 128 stellte im deutschsprachigen Raum die beliebteste Software für den C128 dar.\n\nIm C64-Modus besteht eine nahezu vollständige Softwarekompatibilität zwischen C64 und C128. Darüber hinaus kann der C128 im C64-Modus auch mit dem Commodore BASIC V2.0 programmiert werden.\n\nAuch die eigens für den C128 entwickelten 5¼-Zoll-Diskettenlaufwerke der Typen VC1570 und VC1571 besitzen eine auf den C64-Modus abgestimmte Betriebsart, in der sie sich wie eine VC1541 verhalten. Allerdings laufen manche Programme mit vielen Diskettenzugriffen oder aufwändigem Kopierschutz nicht einwandfrei auf diesen Modellen.\n\nEine Besonderheit des C128 besteht darin, dass verschiedene, beim ursprünglichen C64 nicht zur Verfügung stehende Hardwareeigenschaften im C64-Modus zum Einsatz gebracht werden können. Beispielsweise kann die 2-MHz-Taktfrequenz des Hauptprozessors auch in dieser Betriebsart verwendet werden. Wie im C128-Modus schaltet der Grafikchip VIC IIe allerdings dann das Videosignal ab.\n\nEs gibt vier Möglichkeiten, in den C64-Modus zu gelangen: Erstens, man drückt beim Hochfahren des Rechners gleichzeitig die Commodore-Taste; zweitens, man drückt den Reset-Knopf und hält die Commodore-Taste gedrückt; drittens, man gibt im C128-Modus einfach den BASIC-Befehl codice_19 ein, drückt die Return-Taste und bestätigt die automatische Sicherheitsabfrage; oder viertens, man schiebt vor dem Hochfahren einfach ein C64-Steckmodul in die Erweiterungsschnittstelle und schaltet dann den Rechner ein. Es gibt keine Möglichkeit, vom CP/M-Modus aus direkt in den C64-Modus überzuwechseln. Vom C64- zurück in den C128-Modus wiederum gelangt man nur durch einen Reset oder Neustart.\n\nDer C128-Modus stellt die grundlegende Betriebsart des C128 dar. Das C128-Betriebssystem ist für die Konfiguration der Hardware des C128 und des Commodore BASIC V7.0 verantwortlich. Es besteht aus einem für die Daten-, Geräte- und Prozessverwaltung verantwortlichen Betriebssystemkern (englisch \"CBM Kernal\") mit 58 Unterprogrammen für verschiedenste grundlegende Aufgaben wie etwa der Einstellung der Systemparameter nach dem Einschalten oder der Bearbeitung von Interrupts, einem Texteditor zur Eingabe von BASIC-Befehlen und -Programmen sowie einem Maschinensprachemonitor. Dieser unterstützt den Anwender bei der Erstellung von Programmen in Assemblersprache und verfügt über 14 Anweisungen. Außerdem ist der Betriebssystemkern für die Ausführung sämtlicher im Arbeitsspeicher abgelegter Programme zuständig. Am Ende des vom Betriebssystemkern belegten Speicherbereichs von codice_20 bis codice_21 befindet sich eine Sprungtabelle mit den Einsprungadressen zum Aufruf der Unterprogramme des Betriebssystems. Alle auch vom C64 verwendeten Betriebssystemroutinen besitzen zur Wahrung der Softwarekompatibilität im C128-Betriebssystemkern die gleiche Einsprungadresse wie beim Vorgängermodell. Auch Zeropage und Systemvariablen befinden sich an den vom C64 her gewohnten Stellen des Arbeitsspeichers.\n\nNach dem Einschalten bzw. einem Hardware-Reset werden zunächst einige BASIC-Routinen sowie sämtliche für die Verwendung durch Anwendungsprogramme gedachte Betriebssystemroutinen vom Festspeicher in einen besonderen, 1 kB großen Bereich des Arbeitsspeichers kopiert (englisch \"Common Area\"). Bei angeschlossenem Diskettenlaufwerk wird überdies ein Autoboot ausgeführt. Danach wird der Startbildschirm angezeigt und der BASIC-Interpreter wartet auf Eingaben des Anwenders. Per Tastendruck kann vor der Inbetriebnahme des Rechners zwischen einer Bildschirmdarstellung von 40 und 80 Zeichen pro Zeile gewählt werden.\n\nInsgesamt umfasst das komplett in Maschinensprache programmierte C128-Betriebssystem rund 16 kB ROM. Davon entfallen 12 kB auf den Betriebssystemkern nebst Sprungtabelle und 4 kB auf den Maschinensprachemonitor. Zwar wurde die Betriebssystemsoftware des C128 mehrfach überarbeitet, alle Revisionen enthalten jedoch die unveränderte Sprungtabelle des \"CBM Kernal\". Deshalb sind sie unter der Voraussetzung des Verzichts auf wilde, unter Umgehung der Sprungtabelle vorgenommene Einsprünge ins Betriebssystem untereinander hundertprozentig softwarekompatibel.\n\nÜber den eigentlich dem Aufruf von Maschinenspracheprogrammen dienenden BASIC-Befehl codice_22 lässt sich ein Easter Egg mit den Namen der Entwickler der C128-Systemsoftware und der pazifistischen Botschaft \"„Link arms, don’t make them“\" aufrufen.\n\nAls dritte Betriebsart ist die Verwendung des diskettenbasierten, für 8-Bit-Rechner mit Z80-Hauptprozessor und 128 kB Arbeitsspeicher konzipierten Betriebssystems CP/M-Plus (englisch \"Control Program for Microcomputers\") sowohl im 40- als auch im 80-Zeichen-Modus auf dem C128 möglich. Allerdings ist ein CP/M-Betrieb auf dem C128 ausschließlich unter Verwendung des US-amerikanischen ASCII-Zeichensatzes möglich, nicht aber der landestypischen Zeichensätze. Das ab 1973 unter der Führung von Gary Kildall entwickelte CP/M wurde 1975 erstmals in der Version 1.4 für Rechner mit Hauptprozessoren des Typs Intel 8080 sowie 8-Zoll-Diskettenlaufwerke von IBM kommerziell angeboten und schließlich 1977 als eigene Marke von Digital Research eingetragen. CP/M gilt als das erfolgreichste plattformunabhängige 8-Bit-Betriebssystem überhaupt mit dem weltweit größten Softwareangebot. Ende 1985 boten über 300 Computerhersteller weltweit CP/M-fähige Rechner an, darunter auch der Branchenführer IBM.\n\nGegenüber dem standardsetzenden, auf Rechner mit maximal 64 kB Arbeitsspeicher ausgelegten Vorgänger CP/M 2.2 von 1979/80 (daher auch als CP/M-80 bezeichnet) bietet das vollständig abwärtskompatible CP/M-Plus (auch CP/M 3.0) aus dem Jahr 1983 eine erweiterte Funktionalität mit größerem Arbeitsspeicher sowie zusätzlichen Befehlen und ist auf kleinere Diskettengrößen wie 5¼ Zoll oder 3 Zoll zugeschnitten.\n\nAm 1. August 1985 wurde die erste C128-Portierung von CP/M-Plus veröffentlicht, die zum Lieferumfang des Rechners gehörte. Im Gegensatz zur Mehrheit der auf anderen Rechnern umgesetzten CP/M-Plus-Versionen umfasst sie weder ein benutzerfreundliches Assemblerprogramm noch einen Debugger. Die C128-Portierung stellt 59 kB als freien Programmspeicher (englisch \"Transient Program Area\", kurz TPA) zur Verfügung. Das erste Update mit zusätzlicher Druckerunterstützung, aber irrtümlich weggelassenem Treiber für die serielle Schnittstelle erschien am 8. Dezember 1985. Ohne Zusatzsoftware konnte daher unter CP/M-Plus keine etwa zur Datenfernübertragung über ein Modem notwendige RS232C-Schnittstelle verwendet werden.\n\nAm 28. Mai 1987 veröffentlichte Commodore das zweite und letzte, das 3½-Zoll-Diskettenlaufwerk VC1581 sowie die Speichererweiterungen der Typen 1700, 1750 und 1764 unterstützende CP/M-Plus-Update für den C128, dessen TPA zur Unterbringung der dazu nötigen Systemroutinen auf 58 kB reduziert werden musste. Das Update enthält jedoch irrtümlicherweise nicht das zur Auswahl der Druckerschnittstelle und des deutschen Zeichensatzes notwendige Konfigurationsprogramm.\n\nWie alle CP/M-Versionen besteht auch CP/M-Plus aus drei in Maschinensprache programmierten Komponenten: dem monolithischen Betriebssystemkern BDOS (englisch \"Basic Disk Operating System\"), dem modularen, für die Regelung der Ein- und Ausgabeoperationen zuständigen BIOS (englisch \"Basic Input/Output System\") sowie dem als Benutzeroberfläche dienenden Kommandozeileninterpreter CCP (englisch \"Console Command Processor\").\n\nDas BDOS steuert mittels 69 geräteunabhängigen, bei allen CP/M-Plus-Rechnern identischen Systemroutinen mit genormten Einsprungadressen – den sogenannten BDOS-Funktionen – sämtliche Diskettenoperationen, die Tastaturabfrage, die Zeichenausgabe über Monitor und Drucker sowie den Arbeitsspeicher. Im Vergleich zur Vorgängerversion CP/M 2.2 besitzt CP/M-Plus damit 28 neue BDOS-Funktionen. Das ebenfalls feste Einsprungadressen verwendende, eine Ebene unter dem BDOS operierende BIOS dagegen fungiert als Bindeglied zwischen den standardisierten BDOS-Funktionen und der herstellerspezifischen Hardware des jeweiligen CP/M-Rechners, indem es auf Veranlassung des Betriebssystemkerns seine zur Steuerung der entsprechenden Hardwarekomponenten implementierten 30 Unterprogramme aufruft. Deshalb benötigt auch jeder CP/M-Rechner ein herstellereigenes BIOS. Das BIOS des C128 enthält Systemroutinen sowohl in der Maschinensprache des Z80A als auch in der des MOS 8502. Die in der Maschinensprache des MOS 8502 geschriebenen Unterprogramme sind für die Ansteuerung von Tastatur, Bildschirm, Drucker und Diskettenlaufwerken verantwortlich und übernehmen im Bedarfsfall die Kontrolle vom Z80A. Außerdem erlauben sie Zugriffe auf beide Grafikchips und den Soundchip des C128.\n\nDa es sich bei CP/M-Plus nicht um ein im Festspeicher residierendes Betriebssystem handelt, muss es bei Inbetriebnahme des Rechners erst von Diskette gebootet werden. Die CP/M-Plus-Betriebssystemsoftware umfasst zwei Dateien: Während BDOS und BIOS gemeinsam unter dem Dateinamen codice_23 abgelegt sind, enthält die Datei codice_24 den Kommandozeileninterpreter CCP. In den CP/M-Modus gelangt man entweder durch das Einlegen der CP/M-Plus-Systemdiskette bei einem System-Reset bzw. System-Neustart oder durch Eingabe des BASIC-Befehls codice_25 bei eingelegter CP/M-Plus-Systemdiskette vom Commodore-BASIC-V7.0-Interpreter aus. Während des Hochladens erscheint auf dem Bildschirm die Meldung codice_26. Nach dem Hochfahren wartet der Kommandozeileninterpreter auf Eingaben des Anwenders. Durch gleichzeitiges Drücken der Control-Taste sowie der Enter-Taste lässt sich im CP/M-Modus ein Reboot des CP/M-Plus-Betriebssystems durchführen. Vom CP/M-Modus aus kann man nur durch das Betätigen der Reset-Taste oder einen System-Neustart zurück in den nativen C128-Modus gelangen.\n\nCP/M-Plus verfügt ferner über dauerhaft in den Arbeitsspeicher geladene, jederzeit ausführbare Befehle (englisch \"resident commands\") und über nur bei Bedarf in den Arbeitsspeicher geladene Befehle (englisch \"transient programs\"). Insgesamt umfasst der Befehlsvorrat 31 Kommandos. Die insgesamt sechs speicherresidenten Befehle codice_27, codice_28, codice_29, codice_30, codice_31 und codice_32 beziehen sich vor allem auf Dateien und Diskettenoperationen. Grundlegende transiente Befehle wie codice_33, codice_34, codice_35, codice_36, codice_37, codice_38 und codice_39 erlauben das Anzeigen von Systemdateien, die Veränderung von Datensuchpfaden, das Einstellen von Zugriffsrechten über Passwörter und die Verwendung von Zeitstempeln zur Erleichterung der Archivierung von Dateien. Der Hauptvorteil der Auslagerung der transienten Befehle auf Diskette bestand im Einsparen von Speicherplatz.\n\nDie Speicherorganisation des C128 unterscheidet sich im CP/M-Modus von den übrigen Betriebsarten. Beide Speicherbänke teilen sich einen 8 kB umfassenden, von codice_40 bis codice_41 reichenden Speicherbereich, der als jederzeit vom Hauptprozessor ohne Bank Switching zugängliche Schnittstelle zwischen den Speicherbänken fungiert. Der gemeinsame Speicherbereich enthält diverse Hardware-Puffer, die ungebankten Komponenten von BDOS und BIOS sowie einen kleinen, 3,5 kB großen Teil der TPA. Die Speicherbank eins besteht größtenteils aus dem Rest der von codice_42 bis codice_43 liegenden, insgesamt 59 kB umfassenden TPA und enthält zusätzlich von codice_44 bis codice_43 die primär der Kommunikation zwischen Betriebssystem und Anwenderprogrammen dienende, 256 Bytes umfassende \"Base Page\" (auch \"Zero Page\"). Die übrigen, gebankten Bestandteile von BDOS und BIOS sowie der CCP werden in die Speicherbank null geladen. Die 4 kB umfassenden, nicht dem gemeinsamen Speicherbereich zugehörigen BIOS-Komponenten werden beim Hochladen von der MMU in den Speicherbereich von codice_46 bis codice_47 der Speicherbank null kopiert. Den Rest der Speicherbank null nehmen neben den nicht im gemeinsamen Speicherbereich liegenden BDOS-Bestandteilen sowie einem 4 kB großen Puffer für den CCP bestimmte CP/M-Plus-Diskettenbefehlsroutinen ein, die daher im Gegensatz zur Vorgängerversion nicht alle fortlaufend von Diskette nachgeladen werden müssen und CP/M-Plus einen Geschwindigkeitsvorteil gegenüber CP/M 2.2 verschaffen.\n\nIm CP/M-Modus arbeitet der C128 aber dennoch „relativ langsam“. Die Gründe hierfür sind in bestimmten Eigenheiten der Systemarchitektur des C128 zu suchen. Das im Vergleich zum üblichen CP/M 2.2 umfangreichere und komplexere CP/M-Plus führt nicht selbst die Ein- und Ausgabeoperationen aus, sondern überlässt diese dem Hauptprozessor . Dieser ist aber mit 2,04 MHz deutlich niedriger getaktet als gewöhnliche CP/M-Rechner, die es meist auf 4 MHz bringen. Daher muss die an sich schnelle Z80A-CPU ständig zahlreiche Waitstates durchlaufen, bis der MOS 8502 diese Aufgaben abgearbeitet hat. Obendrein verwendet der C128 für Diskettenzugriffe seine serielle Schnittstelle, die aber länger für das Übertragen von Daten braucht als herkömmliche CP/M-Systeme. So liegt die durchschnittliche Schreib-/Lesegeschwindigkeit selbst bei Verwendung des neuentwickelten schnellen 5¼-Zoll-Diskettenlaufwerks VC1571 bei lediglich gut 3 kB pro Sekunde, während konventionelle CP/M-Systeme um die 20 kB erreichen. Das Arbeiten mit umfangreichen Datensätzen wurde deshalb für die CP/M-Anwender häufig „zur Geduldsprobe“. Durch den Einsatz von Speichererweiterungen wie etwa den RAM Expansion Units der Typen 1700 bzw. 1750 als RAM-Disks lässt sich die Arbeitsgeschwindigkeit unter CP/M-Plus allerdings um das Zehn- bis Fünfzehnfache steigern.\n\nBei den beiden Diskettenbetriebssystemen Commodore DOS 3.0 (auch CBM DOS 3.0) sowie Commodore DOS 3.1 (auch CBM DOS 3.1) handelt es sich um Firmware, die nicht im Festspeicher des C128 selbst, sondern in den ROM-Chips des eigens für den Rechner entwickelten 5¼-Zoll-Diskettenlaufwerks VC1571 bzw. dessen im Falle der Desktop-Modelle C128D sowie C128D-CR ins Rechnergehäuse integrierten Varianten residiert. Beide Versionen bilden den direkten Nachfolger des für die VC1541 des C64 geschriebenen Commodore DOS 2.6. Das native Commodore BASIC V7.0 des C128 verfügt über insgesamt 17 Diskettenbefehle zum komfortablen Aufruf der unterschiedlichen Funktionen von Commodore DOS 3.0/3.1. Die beiden für die Bürorechner der CBM-Reihe entwickelten Festplattenlaufwerke D9060 und D9090 besitzen ebenfalls ein jedoch nicht mit der Systemsoftware der VC1571 identisches Diskettenbetriebssystem mit der Bezeichnung Commodore DOS 3.0.\n\nDie Diskettensystemsoftware des ab 1985 in drei Versionen auf den Markt gebrachten Commodore DOS 3.0 umfasst insgesamt 32 kB ROM. Sie belegt den Adressbereich von codice_48 bis codice_49 des laufwerkseigenen Speichers. Die Sprungtabelle des Commodore DOS 3.0 liegt zwischen codice_50 und codice_51. Zwecks Gewährleistung größtmöglicher Kompatibilität zur VC1541 im C64-Modus enthält das Diskettenbetriebssystem der VC1571 in nahezu unveränderter Form im Adressbereich von codice_52 bis codice_49 das gesamte Commodore DOS 2.6 und im Adressbereich von codice_48 bis codice_51 die hinzugefügten Unterprogramme des neuen Commodore DOS 3.0. Dazwischen liegt von codice_56 bis codice_57 ein „unbenutzter Leerbereich“.\n\nLediglich drei wirklich neue Funktionen wurden dem Commodore DOS 3.0 gegenüber dem weitverbreiteten Vorgänger hinzugefügt.\n\nErstens wurde es mit zusätzlichen Steuerroutinen zur Verwendung der in der VC1571 verbauten Lichtschranke sowie zur Regulierung der über zwei Schreib-/Leseköpfe verfügenden Laufwerksmechanik ausgestattet.\n\nZweitens wurde das Commodore DOS 3.0 um Systemroutinen erweitert, die neben dem bis dahin ausschließlich von Commodore unter Einsatz des gruppencodierten Datenaufzeichnungsverfahrens implementierten GCR-Format (englisch \"Group Coded Recording\") eine Verwendung von Disketten im MFM-Format (englisch \"Modified Frequency Modulation\") erlauben. Das mithilfe eines Floppy-Disk-Controllers des Typs WD1770 von Western Digital erzeugte MFM-Format gestattet der VC1571 den Datenaustausch mit den CP/M-Rechnern von Osborne, Kaypro (Kaypro II und IV), Epson und IBM (CP/M-86). Das Format CP/M-86 erlaubt ferner den Datenaustausch mit den Heimcomputern der Modellreihe Schneider CPC, sofern anstelle der standardmäßigen 3-Zoll-Diskettenlaufwerke von Schneider zusätzliche Zweitlaufwerke für 5¼-Zoll- bzw. 3½-Zoll-Disketten von Fremdherstellern angeschlossen werden. Das zur Formatierung in diesen Diskettenformaten nötige Dienstprogramm wurde allerdings erst mit dem Update von 1987 in die CP/M-Plus-Betriebssystemsoftware integriert. Mittels des nicht zur Betriebssystemsoftware gehörenden Hilfsprogramms \"Jugg’ler\" lassen sich insgesamt 170 CP/M-Diskettenformate verarbeiten, darunter auch zahlreiche ECMA-Formate. Da der Floppy-Disk-Controller WD1770 zu seinem im IBM-PC bzw. IBM-PC XT verwendeten Pendant µPD765 von NEC und dessen Derivaten kompatibel ist, können Rohdaten mittels eines weiteren Hilfsprogramms sogar mit IBM-PC-kompatiblen Rechnern geteilt werden. Mit einer Speicherkapazität von bis zu 200 kB pro Diskettenseite bei einer Sektorengröße von 1024 Bytes übertrifft das MFM-Format das GCR-Format um gut 29 kB.\n\nDrittens erhielt das Commodore DOS 3.0 neue Busroutinen zur optimalen Ausnutzung des im Vergleich zum Vorgängermodell deutlich schnelleren seriellen Busses des C128. Diese Busroutinen ermöglichen der VC1571 die Durchführung von Lese- und Schreiboperationen mit hohem Datendurchsatz im C128-Modus sowie im für damalige Verhältnisse extrem schnellen Stoßbetrieb, dem sogenannten Burst-Modus. Bei nur einer Diskettenumdrehung können im Burst-Modus ganze Spuren auf einmal in den Arbeitsspeicher eingelesen werden. Ihre Schnelligkeit verdanken die Busroutinen ihrem vergleichsweise einfachen Aufbau.\n\nDas Commodore DOS 3.0 verwendet teils etwas andere Befehlsparameter, verfügt aber von einer wichtigen Ausnahme abgesehen über den gleichen Befehlssatz wie der Vorgänger Commodore DOS 2.6. Hinzugekommen ist lediglich der vielseitige, etwa zum Einstellen der Geräteadresse, Umschalten zwischen dem VC1541- und dem VC1571-Modus, Aktivieren des Burst-Modus sowie Festlegen des Abstandes zwischen den Sektoren auf der Diskette dienende Befehl codice_58. 31 Fehlermeldungen erleichtern dem Anwender die Fehlersuche.\n\nDas Commodore DOS 3.0 besteht ferner aus einem Hauptprogramm und einer Unterbrechungsroutine. Das Hauptprogramm verwendet die im Adressbereich von codice_46 bis codice_60 liegende Zeropage der in der VC1571 verbauten CPU des Typs MOS Technology 6502 (kurz MOS 6502) als sogenannten „Jobspeicher“, in den alle anstehenden Diskettenbefehle und -operationen (englisch \"jobs\") inklusive der notwendigen Parameter für Spurnummer, Sektornummer, Diskettenseite usw. eingetragen werden. Die Unterbrechungsroutine emuliert ihrerseits die Logikfunktionen eines vollwertigen Floppy-Disk-Controllers, da viele der Fähigkeiten des von einem Fremdhersteller stammenden WD1770 etwa zur Steuerung der Laufwerksmechanik überhaupt nicht vom Commodore DOS 3.0 verwendet werden und es sich bei den beiden in der VC1571 verbauten Schnittstellenbausteinen des Typs MOS Technology VIA lediglich um einfache „Pseudo-Controller“ mit begrenzten Logikfunktionen handelt. Die Unterbrechungsroutine wird regelmäßig über einen Timer aufgerufen und überprüft den Jobspeicher auf auszuführende Befehle. Dieser Vorgang wird auch als „Jobschleife“ bezeichnet. Nach erfolgreicher Befehlsausführung hinterlässt sie im Jobspeicher eine Rückmeldung, die das Hauptprogramm über die ordnungsgemäß durchgeführten Diskettenoperationen informiert. Der Jobspeicher bzw. die Zeropage liegen im 2 kB umfassenden Arbeitsspeicher der VC1571, der außerdem neben den CPU-Registern des MOS 6502 zahlreiche Puffer etwa für das Zwischenspeichern von Fehlermeldungen, Rechnerbefehlen, gerade bearbeiteten Dateiblöcken usw. enthält.\n\nDa das Diskettenlaufwerk VC1570 Disketten nur einseitig verwenden kann, besitzt es eine eigene Fassung des Commodore DOS 3.0, das in einem 32-kB-ROM-Baustein mit der Nummer 315090-01 untergebracht ist. Von der für die VC1571 programmierten Fassung existieren dagegen zwei Versionen: Die ursprüngliche auf einem ROM-Chip mit der Nummer 310654-03 sowie eine spätere fehlerbereinigte Revision, die nach Markteinführung bekanntgewordene Probleme etwa bei der Positionierung des Schreib-/Lesekopfes auf der zweiten Diskettenseite oder der zur Angabe der freien Diskettenblöcke dienenden BAM-Anzeige (englisch \"Block Availability Map\") beseitigt und auf einem ROM-Baustein mit der Nummer 310654-05 zu finden ist. Die Revision erhöht deutlich sowohl die Geschwindigkeit der Datenübertragung als auch die Stabilität des Diskettenbetriebssystems. Der entsprechende ROM-Chip konnte über den Commodore-Kundenservice bezogen werden. Wer sich den Selbsteinbau nicht zutraute, konnte den Eingriff aber auch kostenlos in einer Commodore-Kundenwerkstatt durchführen lassen. Zwecks Gewährleistung vollständiger Softwarekompatibilität enthalten sämtliche Versionen des Commodore DOS 3.0 identische Fassungen der Sprungtabelle.\n\nBeim 1987 fertiggestellten Commodore DOS 3.1 handelt es sich um das auf den Betrieb mit dem C128D-CR ausgelegte Diskettenbetriebssystem der VC1571. Zur Steuerung des internen 5¼-Zoll-Diskettenlaufwerks wurde beim C128D neben zwei Schnittstellenbausteinen des Typs MOS Technology VIA ein als Buscontroller agierender Schnittstellen-Adapter des Typs MOS Technology CIA mit Echtzeituhr verbaut, dessen Fähigkeiten im Betrieb allerdings – ähnlich wie beim Floppy-Disk-Controller WD1770 – nur teilweise in Gestalt seines seriellen Schieberegisters eingesetzt werden, während gleichzeitig die Schieberegister der beiden Schnittstellenbausteine ungenutzt bleiben. Vor der Markteinführung des C128D-CR überarbeitete Commodore die Hardware des C128D. Daher wurden der MOS Technology CIA sowie der WD1770 zum neu entwickelten MOS 5710 zusammengefasst. Dieser Floppy-Disk-Controller wurde ausschließlich in diesem Modell eingesetzt. Zwar verringerte der MOS 5710 die Produktionskosten und konnte ebenfalls die die Diskettenformate der CP/M-Rechner von Osborne, Kaypro, Epson sowie IBM lesen und schreiben. Auf weitere Formate ließ sich der neue Floppy-Disk-Controller im Gegensatz zum WD1770 jedoch nicht programmieren. Die beim C128D noch auf einer eigenen Nebenplatine untergebrachten Bausteine des ins Rechnergehäuse eingelassenen 5¼-Zoll-Diskettenlaufwerks wurden überdies beim C128D-CR in die Hauptplatine integriert.\n\nNeben der Verschaltung musste natürlich auch die ursprüngliche Diskettensystemsoftware an die neue, nunmehr höher integrierte Hardwareumgebung angepasst werden. Aus dieser Anpassung ging schließlich das etwas umfangreichere Commodore DOS 3.1 hervor, das einige veränderte bzw. verschobene Systemroutinen sowie einzelne neue Sprunganweisungen enthält, für die zuvor ungenutzte Bereiche des Laufwerkfestspeichers verwendet werden. Darüber hinaus wurde eine Fehlerbereinigung durchgeführt, in deren Rahmen etwa die Sprungtabelle, der Jobspeicher und die Prüfsummenroutine überarbeitet wurden. Diese Veränderungen führen jedoch im C128-Modus bei einigen, auf den älteren Modellvarianten C128 mit VC1571 und C128D problemlos laufenden Anwendungen mit intensiver Nutzung der Routinen des Diskettenbetriebssystems zu Abstürzen. Insbesondere hardwarenahe, unter Umgehung der Sprungtabelle Einsprünge in das Diskettenbetriebssystem vornehmende Programme – wie etwa Kopierprogramme – bereiten unter Commodore DOS 3.1 vielfach Probleme. Wer die inkompatible Software nicht eigenhändig für den C128D-CR umprogrammieren konnte oder wollte, war letztlich zur Anschaffung eines zusätzlichen, noch das Commodore DOS 3.0 enthaltenden externen Diskettenlaufwerks des Typs VC1571 gezwungen.\n\nDas Commodore DOS 3.1 existiert lediglich in einer Fassung und ist auf einem 32-kB-ROM-Chip mit der Nummer 318047-01 untergebracht. Aufgrund der Ausrichtung auf unterschiedliche Hardwareumgebungen ist eine Ersetzung defekter ROM-Bausteine des Commodore DOS 3.1 durch die unterschiedlichen Varianten des Commodore DOS 3.0 nicht möglich.\n\nBeim Commodore DOS 10.0 handelt es sich um das 32 kB ROM umfassende, von Grund auf neu entwickelte Diskettenbetriebssystem der VC1581. Der Befehlssatz des Commodore DOS 10.0 enthält neben sämtlichen von den Versionen Commodore DOS 2.6–3.1 her bekannten Instruktionen einige zusätzliche Funktionen, etwa in Bezug auf den Burst-Modus, die Formatierung, die Fehlermeldungen oder die Suchfunktionen. Auch das Commodore DOS 10.0 verwendet eine für die Adressierung der Speicherzellen des Laufwerks vorgesehene Zeropage. Sie liegt im Bereich von codice_46 bis codice_60 des Arbeitsspeichers der VC1581. Ansonsten ist die Speicherorganisation des Commodore DOS 10.0 aber eine völlig andere als die der Vorgängerversionen und führt deshalb zu Einschränkungen bei der Kompatibilität. Über der Zeropage liegt von codice_63 bis codice_64 der Stapelspeicher und von codice_65 bis codice_66 ein für Variablen reservierter Bereich. Als einziges 8-Bit-Diskettenbetriebssystem von Commodore verfügt das Commodore DOS 10.0 über Systemroutinen zur Verwaltung von Unterverzeichnissen sowie zur Verwendung des zu diesem Zeitpunkt bereits zum Industriestandard gewordenen, mithilfe des Floppy-Disk-Controllers WD1772 erzeugten MFM-Formats des IBM System/34 mit 80 Spuren pro Diskettenseite und zehn Sektoren pro Spur. Allerdings liegt das Disketteninhaltsverzeichnis nicht wie beim Commodore DOS 3.0 bzw. 3.1 auf Spur 18, sondern auf Spur 40, was ebenfalls zu Kompatibilitätsproblemen führt. Überdies ist das Commodore DOS 10.0 zum Anlegen von bis zu 296 Dateien pro Diskette in der Lage, während alle bis dahin von Commodore entwickelten 8-Bit-Diskettenbetriebssysteme lediglich bis zu 144 Dateien verwalten können.\n\nDas Commodore DOS 10.0 reserviert 5 kB der insgesamt 8 kB RAM der VC1581 zur Zwischenspeicherung der von einer gesamten Diskettenspur eingelesenen Daten. Dieser Diskettenspur-Zwischenspeicher (englisch \"track cache buffer\") liegt im Bereich von codice_67 bis codice_68 und erlaubt schnelle Datentransfers zum Arbeitsspeicher des Rechners. Er kann aber auch für andere Zwecke eingesetzt werden, etwa zur Programmierung weiterer, nativ nicht lesbarer, aber ebenfalls auf dem MFM-Verfahren aufbauender Aufzeichnungsformate. Mit nur geringem Programmieraufwand lassen sich beispielsweise andere 3½-Zoll-Disketten-Formate wie die des IBM Personal System/2, des Atari ST oder MS-DOS einlesen. Außerdem steht dem Commodore DOS 10.0 ein vergrößerter Jobspeicher zur Verfügung. Daher kann es mehr Dateien gleichzeitig öffnen als die Vorgängerversionen. Der aus neun jeweils 256 Bytes großen Puffern bestehende, u. a. für die Zwischenspeicherung der Disketteninhaltsverzeichnisse zuständige Jobspeicher umfasst insgesamt 2.303 Bytes und liegt im Bereich von codice_69 bis codice_70. Maschinennah programmiert werden kann der Jobspeicher mit über 30 vom Commodore DOS 10.0 zur Verfügung gestellten Assembler-Befehlen (englisch \"job codes\").\n\nDas Commodore DOS 10.0 besitzt ferner eine eigene Autoboot-Funktion. Bei einem System-Reset oder einem System-Neustart sucht das Diskettenbetriebssystem automatisch nach einer User-Datei mit dem Dateinamen codice_71 auf der gerade eingelegten Diskette, lädt das entsprechende Dienstprogramm – sofern vorhanden – in den Arbeitsspeicher der VC1581 und führt dieses dann aus. Außerdem ist eine Selbsttestfunktion in das Commodore DOS 10.0 integriert.\n\nNur komplett in den Arbeitsspeicher geladene ältere Programme für den C128 laufen auch unter Commodore DOS 10.0 anstandslos. Software mit Einsprüngen ins Diskettenbetriebssystem bringt den Rechner dagegen wegen des völlig anders gearteten Aufbaus des Commodore DOS 10.0 zum Abstürzen. Zu den wenigen umstandslos lauffähigen Programmen gehört die in Westdeutschland populäre Textverarbeitung \"StarTexter 128\", während weit verbreitete Software wie etwa die diskettenbasierten Betriebssysteme GEOS 128 und CP/M-Plus, die Datenbanken \"Superbase 128\" und \"Datamat 128\", die Textverarbeitungen \"Vizawrite 128\" und \"Textomat 128\" oder der \"Basic 128 Compiler\" nicht ohne Weiteres unter Commodore DOS 10.0 betrieben werden können.\n\nMitte der 1980er Jahre erlangte die mausgesteuerte grafische Benutzeroberfläche (englisch \"Graphical User Interface\", kurz GUI) des Apple Macintosh aufgrund ihrer über den Einsatz von Icons und Computermäusen im Vergleich zu den herkömmlichen Texteditoren gesteigerten Benutzerfreundlichkeit hohe Popularität. Nach diesem Vorbild wurden weitere GUIs für andere Computersysteme entwickelt, darunter auch das von Berkeley Softworks stammende \"Graphic Environment Operating System\" (kurz GEOS). Die 1986 für den C64 entwickelte Version GEOS 64 erfreute sich bald ebenfalls großer Beliebtheit. Mit GEOS 128 wurde im Jahr darauf aber auch eine GEOS-Portierung für den C128-Modus veröffentlicht. Bis Anfang 1990 wuchs die Zahl der weltweiten GEOS-Anwender unter den Besitzern eines Commodore-Rechners auf 1,8 Millionen. Davon entfielen 70.000 auf eine der deutschsprachigen Fassungen von GEOS 64 bzw. GEOS 128. Insgesamt wurden bis zu diesem Zeitpunkt inklusive der die grafische Benutzeroberfläche ergänzenden GEOS-Applikationen über 100.000 ins Deutsche übersetzte GEOS-Softwareprodukte verkauft.\n\n1987 wurde mit GEOS 128 Version 1.3 (kurz GEOS 128 1.3) auf der vom 4. bis 6. März in Hannover abgehaltenen CeBit erstmals auch eine für den C128-Modus geschriebene GEOS-Version vorgestellt. Ab Ende Mai 1987 sollte GEOS 128 im Handel erhältlich sein. Die Markteinführung der englischsprachigen Version verzögerte sich jedoch bis Oktober 1987. Im Wesentlichen handelt es sich bei GEOS 128 um eine auf die Hardwareverbesserungen des C128 ausgerichtete Neuauflage von GEOS 64.\n\nZum Lieferumfang der für die Standardauflösung von 640 × 200 Bildpunkten des C128 im 80-Zeichen-Modus entwickelten, prinzipiell aber auch im 40-Zeichen-Modus lauffähigen GEOS-Version gehörten neben einer integrierten Taschenrechnerfunktion, einem Notizbuch und einer Alarmuhr das Malprogramm \"GeoPaint 128\" sowie die nach dem WYSIWYG-Prinzip funktionierende Textverarbeitung \"GeoWrite 128\". Für die für den deutschsprachigen Raum produzierte Version des C128 erschien eigens eine portierte Fassung mit deutschem Zeichensatz. Für den Betrieb unter GEOS 128 1.3 wurden außerdem verschiedene Anwendungsprogramme entwickelt. Neben der Rechtschreibprüfung \"GeoSpell 128\" erschienen im Jahr 1988 etwa das Dateiverwaltungsprogramm \"GeoFile 128\" sowie die Tabellenkalkulation \"GeoCalc 128\" – alle ebenfalls aus dem Hause Berkeley Softworks. \"GeoCalc 128\" arbeitet mit hoher Rechengenauigkeit und berücksichtigt Veränderungen sofort, gestattet jedoch nicht das Einbinden von Bildern oder Grafiken.\n\nZum Betrieb muss das nicht zum Lieferumfang des C128 gehörende GUI zunächst von einer Systemdiskette gebootet werden. Dabei können die Commodore-Diskettenlaufwerke VC1541, VC1571 oder VC1581 zusammen mit einer beliebigen Version des C128 im 80-Zeichen-Modus verwendet werden. Alternativ kann die Hauptplatine des C128 mit einem die GEOS-Systemsoftware enthaltenden ROM bestückt werden. Aufgrund des im 80-Zeichen-Modus doppelt so hoch getakteten Hauptprozessors MOS 8502, des größeren Arbeitsspeichers sowie der mindestens 16 kB dedizierten Grafikspeichers laufen sämtliche Versionen von GEOS 128 auf dem C128 wesentlich schneller als das ursprüngliche GEOS 64 auf dem C64. Durch die zusätzliche Verwendung von Speichererweiterungen als RAM-Disk lässt sich die Arbeitsgeschwindigkeit nochmals deutlich erhöhen.\n\nZu den Mindestsystemanforderungen zählen neben Rechner und VC1541-Diskettenlaufwerk ferner ein 80-Zeichen-Monitor mit RGBI-Anschluss und wahlweise ein Joystick oder eine Maus als Eingabegerät. Optimiert wurde GEOS 128 1.3 jedoch auf den Betrieb mit den Diskettenlaufwerken VC1571 bzw. VC1581 im Burst-Modus sowie den Speichererweiterungen des Typs 1700 bzw. 1750. Das GUI kann mit Geschwindigkeitseinbußen aber auch ohne diese verwendet werden. Programmiert wurde die Urversion von GEOS 128 von Jim Defrisco, Brian Dougherty, Dave Durran, Michael Flarr, Doug Fults, Chris Hawley, Clayton Jung und Tony Requist.\n\n1989 wurde schließlich auf der vom 7. bis 10. Januar in Las Vegas abgehaltenen Winter Consumer Electronics Show das verbesserte, vollständig softwarekompatible GEOS 128 Version 2.0 (kurz GEOS 128 2.0) offiziell vorgestellt. Diese Revision von GEOS 128 erschien auch in einer deutschsprachigen Fassung. An den Mindestsystemanforderungen änderte sich gegenüber der Vorgängerversion nichts. Für einen optimalen Betrieb wird neben einer Maus und einer 512-kB-Speichererweiterung allerdings die Verwendung eines grafikfähigen Druckers empfohlen. Die unverbindliche Preisempfehlung des Herstellers für GEOS 128 2.0 lag bei 69,95 US$ bzw. 139 DM. Für 79 DM konnten Besitzer der Vorgängerversion ein Upgrade auf GEOS 128 2.0 erwerben. Ungefähr die Hälfte der nordamerikanischen C128-Besitzer verwendete im Jahr 1989 eine Version von GEOS 128 auf ihren Rechnern.\n\nZur erweiterten Funktionalität von GEOS 128 2.0 gehören farbig markierte Dateitypen, die Möglichkeit der Mehrfachauswahl von Dateien sowie ein Dienstprogramm zur Druckeranpassung. \"GeoWrite 128\" wurde um eine verbesserte Version der Rechtschreibprüfung \"GeoSpell 128\", Editierfunktionen wie etwa das Hoch- und Tiefstellen von Text sowie zahlreiche Druckertreiber erweitert. Mit \"GeoPaint 128\" lassen sich nunmehr Bilder auch verzerren. Beide Anwendungen sind ausschließlich im 80-Zeichen-Modus lauffähig. Ohne Speichererweiterung laufen sie aber relativ langsam und das Malprogramm neigt zu Deformationen bei der Erstellung von Grafiken. \"GeoPaint 128\" arbeitet mit einer Maximalauflösung von 640 × 720 Bildpunkten, von denen aber aufgrund von Hardwarebeschränkungen immer nur ein 640 × 145 Pixel großer Ausschnitt auf dem Bildschirm angezeigt werden kann.\n\nÜberdies wurden weitere GEOS-Applikationen wie die auf das Verfassen von Serienbriefen spezialisierte Textverarbeitung \"GeoMerge\" oder der auf den Betrieb mit Laserdruckern ausgerichtete Druckertreiber \"GeoLaser\" in die Systemsoftware integriert. Mit \"GeoChart 128\" erschien außerdem ein Programm zur grafischen Darstellung von Statistiken, \"GeoDex 128\" eine Adressverwaltung, \"GeoTerm 128\" eine Terminalemulation für die Datenfernübertragung mit 1.200 Baud sowie \"GeoPublish 128\" ein Desktop-Publishing-Programm. \"GeoTerm\" wurde im April 1988 zunächst für den Betrieb unter GEOS 64 als Listing zum Abtippen, ein Jahr später dann in einer auch unter GEOS 128 im 80-Zeichen-Modus des C128 lauffähigen Bookware-Version mit erweiterter Funktionalität, höherer Bedienerfreundlichkeit sowie einem um 4 kB vergrößerten Pufferspeicher veröffentlicht. \"GeoPublish\" erschien ebenfalls 1988 – noch ohne den Zusatz „128“ im Titel − zunächst in einer nur mit 40 Zeichen pro Zeile arbeitenden, immerhin aber deutschsprachigen Version.\n\nSchließlich wurde mit dem \"GeoProgrammer\" auch eine Entwicklungsumgebung für GEOS-Software mit Assembler (\"GeoAssembler\"), Linker (\"GeoLinker\") und Debugger (\"GeoDebugger\") veröffentlicht. Im C128-Modus lässt sich \"GeoProgrammer\" aber nur mit 40 Zeichen pro Zeile betreiben. Als Editor dient unter \"GeoProgrammer \" die bereits erwähnte GEOS-Applikation \"GeoWrite 128\". Der \"GeoProgrammer\" gestattet die Verwendung sogenannter VLIR-Dateien (Abkürzung für englisch \"Variable Length Index Record\"), die das Programmieren von nur teilweise im Arbeitsspeicher residierender GEOS-Software gestatten und bei Bedarf zusätzlich benötigte Daten einfach von Diskette nachzuladen erlauben. Außerdem ist das Einbinden von Grafiken in den Quellcode möglich. Mit dem \"MegaAssembler\" brachte der Markt+Technik Verlag eine weitere Entwicklungsumgebung für GEOS-Software mit ähnlichem Leistungsumfang heraus.\n\nIm C64-Modus können alle offiziellen Versionen von GEOS 64 – also die von 1986 bis 1988 sukzessive veröffentlichten Versionen 1.2, 1.3 und 2.0 – inklusive sämtlicher Anwendungen problemlos auf dem C128 betrieben werden. Im Gegensatz zum C64C, einer vom Design des C128 inspirierten Revision des C64 aus dem Jahr 1986, zu deren Lieferumfang GEOS 64 gehörte, mussten GEOS 64/128 jedoch von den Anwendern des C128 hinzugekauft werden. GEOS 64 und GEOS 128 sind untereinander weitgehend softwarekompatibel. Wer bereits GEOS 64 besaß, konnte für 22 US$ ein Upgrade auf GEOS 128 erwerben.\n\nAls ab Werk eingebaute Programmierumgebung dient im C128-Modus das Commodore BASIC V7.0, eine stark erweiterte Version des in den Vorgängern Commodore VC 20 (kurz VC20) sowie C64 verwendeten Commodore BASIC V2.0. Der Interpreter des im Festspeicher residierenden Commodore BASIC V7.0 ist direkt nach dem Einschalten verfügbar und belegt 28 kB ROM. Mit 122.365 Bytes stellt er dem Anwender in etwa doppelt so viel Programmspeicher wie der Commodore Plus/4 und gut dreimal so viel Programmspeicher wie der C64 zur Verfügung.\n\nCommodore BASIC V7.0 verfügt über einen umfangreichen, 162 Instruktionen umfassenden Befehlssatz, der neben allen Befehlen, Anweisungen, Funktionen und Variablen der Vorgängerversionen Commodore BASIC V2.0, V3.5 sowie V4.0 weitere Befehle zur strukturierten Programmierung, Fehlerbehandlung, Klang- und Grafikerzeugung, Steuerung von Diskettenlaufwerken sowie zur Verwaltung von Speichererweiterungen enthält. Auch ein leicht zu bedienender Sprite-Editor gehört zur Grundausstattung. Insgesamt 14 Instruktionen wie etwa codice_72, codice_73 oder codice_74 dienen ausschließlich der Generierung von Grafiken. Auch Shapes lassen sich programmieren. Sogar Fenster können mit Hilfe des Befehls codice_75 sowohl im 40- als auch im 80-Zeichen-Modus eingerichtet werden. Allerdings enthält das Commodore BASIC V7.0 keine Grafikbefehle zur Programmierung des hochauflösenden 80-Zeichen-Grafikchips MOS 8563. Programmzeilen dürfen bis zu 160 Zeichen lang sein. Mit dem Befehl codice_14 lässt sich der in die Systemsoftware integrierte Maschinensprachemonitor aufrufen. Zur Fehlerbeseitigung stehen 41 codierte Fehlermeldungen zur Verfügung.\n\nMit Hilfe der Befehle codice_77 und codice_78 kann die Taktfrequenz des Hauptprozessors MOS 8502 wahlweise auf 1 MHz oder 2 MHz eingestellt werden. Beim Betrieb mit 2 MHz ist das Commodore BASIC V7.0 des C128 gut doppelt so schnell wie das Commodore BASIC V3.5 des Commodore Plus/4. Auch die Arbeitsgeschwindigkeit des wesentlich einfacheren Commodore BASIC V2.0 wird vom BASIC-Dialekt des C128 beim Benchmarktest übertroffen. Allerdings beträgt der Geschwindigkeitsvorteil gegenüber dieser in den Erfolgsmodellen VC20 und C64 eingesetzten Variante des Commodore BASIC nur ein gutes Drittel, die ausschließlich im 2-MHz-Modus erreicht werden. Im für die Darstellung von Grafik und Sprites benötigten 1-MHz-Modus ist das Commodore BASIC V7.0 dagegen signifikant langsamer, da es für die Dekodierung des wesentlich umfangreicheren Befehlssatzes mehr Zeit benötigt und der BASIC-Interpreter beim Bankswitching keinen direkten Speicherzugriff hat, sondern hierfür zunächst spezielle Umschaltroutinen aufrufen muss. Damit ist das Commodore BASIC V7.0 paradoxerweise sowohl der schnellste als auch langsamste auf einem Commodore-Computer umgesetzte native BASIC-Dialekt.\n\nAllerdings unterscheiden sich die Benchmarkergebnisse je nach Einsatzgebiet voneinander. Bei arithmetischen Funktionen oder komplexen Berechnungen in den Grundrechenarten beispielsweise ist das Commodore BASIV V7.0 im 2-MHz-Modus etwa doppelt so schnell wie das Commodore BASIC V2.0, während beide BASIC-Dialekte bei der Bildschirmausgabe ungefähr gleich schnell operieren. Deutlich schneller arbeiten indes die BASIC-Dialekte konkurrierender 16-Bit-Rechner wie das Omikron BASIC der Atari-ST-Reihe, das AmigaBASIC V2.0 des Amiga 500 oder das GW-BASIC V3.22 des IBM-PC AT.\n\nIm C64-Modus kann der C128 ohne Einschränkungen im für den VC20 und den C64 entwickelten Commodore BASIC V2.0 programmiert werden. Durch bestimmte Programmiertricks lässt sich auch die verbesserte C128-Hardware in Programme einbinden. Diese Programme laufen allerdings auf dem C64 wegen der unterschiedlichen Hardware nicht fehlerfrei und können den Rechner zum Abstürzen bringen.\n\nNeben den nativen Dialekten des Commodore BASIC erschien eine Reihe weiterer höherer Programmiersprachen für den C128, darunter Dialekte der Programmiersprachen BASIC, C, COBOL, COMAL, Forth, Fortran, Lisp, Pascal, PILOT und Prolog. Diese optionalen höheren Programmiersprachen erschienen entweder auf komfortablen Steckmodulen und sind sofort nach dem Einschalten einsatzbereit oder müssen erst von Diskette oder Kompaktkassette in den Arbeitsspeicher geladen werden. Das Laden von Diskette stellt dabei den Regelfall dar.\n\nZum Zeitpunkt der Markteinführung besaßen fast alle Heimcomputer einen eigenen, im Festspeicher residierenden BASIC-Dialekt, dessen Kommandozeileninterpreter im Alltag als Benutzeroberfläche diente. Aufgrund der großen Durchdringung des Marktes mit BASIC-Varianten aller Art wurden zahlreiche BASIC-Erweiterungen sowie BASIC-Compiler für das native Commodore BASIC V7.0 entwickelt. Für den CP/M-Modus erschienen ferner mehrere eigenständige BASIC-Dialekte und -Compiler.\n\nDa der Befehlsvorrat des nativen Commodore BASIC V7.0 keine Grafikbefehle für den 80-Zeichen-Modus aufweist, veröffentlichte 1986 zunächst Patech Software, später dann Free Spirit Software eine auf Diskette erhältliche, bewusst nicht kopiergeschützte BASIC-Erweiterung namens \"BASIC 8.0,\" die 53 zusätzliche BASIC-Befehle sowie 32 neue Grafikmodi bereitstellt und obendrein sogar für die damalige Zeit ungewöhnliche 3D-Grafikbefehle enthält. Außerdem unterstützt \"BASIC 8.0\" die Verwendung von Speichererweiterungen, Druckern, Joysticks und Computermäusen. Darüber hinaus ist \"BASIC 8.0\" eines der wenigen, den gesamten Grafikspeicher des C128D-CR als RAM-Disk ausnutzenden kommerziellen Programme. Sämtliche Instruktionen dieser in den Vereinigten Staaten wohl bekanntesten kommerziellen BASIC-Erweiterung für den C128 lassen sich in BASIC-Programmen gemeinsam mit dem Befehlssatz des Commodore BASIC V7.0 verwenden. Zur Kennzeichnung beginnen alle \"BASIC 8\"-Befehle mit einem vorangestellten At-Zeichen, also etwa codice_79 zum Ausfüllen geschlossener Flächen mit bestimmten Farben bzw. Mustern oder codice_80 zum Aufruf der erwähnten Grafikmodi. \"BASIC 8.0\" kostete 39,95 US$, für einen Aufpreis von 19,95 US$ konnte die Software auch in Form eines die Ladezeiten ersparenden ROM-Chips erworben werden.\n\nMit \"Hyper-BASIC\" erschien auch in Westdeutschland eine BASIC-Erweiterung als Steckmodul. Ferner ist \"Macro Basic Highway\" (kurz \"MB Highway\") von der Firma System- & Anwender-Software Hermann-Josef Bernd zu den in Westdeutschland entwickelten BASIC-Erweiterungen des C128 zu zählen. Das ebenfalls in Form eines Steckmoduls erhältliche \"MB Highway\" bietet über 200 neue Befehle zur strukturierten Programmierung, Bildschirmverwaltung, Stringbehandlung, Speicherverwaltung, Tabellenverarbeitung, Datei- und Fensterverwaltung sowie die Möglichkeit der Verwendung interruptgesteuerter Unterprogramme.\n\nNeben diesen BASIC-Erweiterungen erschienen zahlreiche BASIC-Compiler-Pakete für den C128. Data Becker und Abacus Software veröffentlichten bereits 1985 den \"Basic 128 Compiler\" von Thomas Helbig, der in Commodore BASIC V7.0 geschriebene Programme optimiert, mit einer Geschwindigkeit von 1–2 kB pro Minute wahlweise in P-Code oder Maschinensprache überträgt und überdies eine integrierte Entwicklungsumgebung enthält. Unter \"Basic 128\" stehen 64 kB an Speichervolumen für den Quellcode zur Verfügung. In Österreich erschien ein Jahr später mit dem \"Austro-Comp 128\" von Digimat ein weiterer, auch die Befehlssätze von BASIC-Erweiterungen akzeptierender Compiler. Die für den Quellcode zur Verfügung stehende Speicherkapazität beträgt unter \"Austro-Comp 128\" ebenfalls 64 kB. Auch Skyles Electric Works brachte mit \"Blitz! 128\" einen BASIC-Compiler mit vergleichbarem Leistungsumfang heraus. Weitere BASIC-Compiler erschienen mit dem \"Gnome Speed Compiler 128\" von SM Software, dem \"SM Compiler 128\" ebenfalls von SM Software, \"PetSpeed 128\" von Oxford Computer Systems und \"Zoom! 128\" von Abacus.\n\nFür den CP/M-Modus wurden mehrere BASIC-Dialekte bzw. -Compiler entwickelt. Digital Research brachte den in Westdeutschland vom Markt+Technik Verlag vertriebenen \"CBASIC Compiler\" heraus. Dieser gestattet optional das Programmieren ohne Zeilennummern und arbeitet mit lokalen Variablen. Für den Quellcode stehen 56 kB an Arbeitsspeicher zur Verfügung. Die C128-Portierung von CBASIC kann ferner Strings mit einer Länge von bis zu 32 kB verarbeiten und unterstützt strukturierte Programmierung durch die Verwendung mehrzeiliger Funktionen und Prozeduren mit Übergabe von Parametern. Ferner können Unterprogramme einzeln übersetzt und in Bibliotheken für die Verwendung in verschiedenen Programmen bereitgehalten werden. Die \"CBASIC\"-Kompilate sind sofort lauffähig. Außerdem veröffentlichte der Markt+Technik Verlag in Westdeutschland eine mit Interpreter, Compiler, Linker, Cross-Reference-Liste, Programmbibliothek und komfortablem Z80A-Makroassembler ausgestattete Version von \"Microsoft BASIC\" (auch \"MBASIC\"). Während ihrer Erstellung können Programme vom Interpreter getestet werden, bevor sie vom Compiler in Maschinensprache übersetzt werden. Der Quellcode kann dabei bis zu 56 kB lang sein. Die Ausführung von Programmen nimmt unter \"Microsoft BASIC\" aufgrund der niedrigen Taktung des Z80A im CP/M-Modus allerdings drei- bis viermal so viel Zeit in Anspruch wie im C128-Modus unter Commodore BASIC V7.0.\n\nVon Comfood stammt die BASIC-Entwicklungsumgebung \"Nevada BASIC\". Die Kompilate des Interpreters sind ebenfalls sofort lauffähig, es lassen sich jedoch im Gegensatz zu anderen BASIC-Dialekten nur sequentielle, aber keine relativen oder Index-Dateien verwenden. Unter \"Nevada BASIC\" stehen ca. 40 kB an Arbeitsspeicher für den Quelltext zur Verfügung. Außerdem stand mit \"E-BASIC\" ein leistungsfähiger, von Gordon Eubanks programmierter BASIC-Compiler für den CP/M-Modus als Public-Domain-Software zur Verfügung.\n\nAbacus brachte Anfang 1986 mit \"Super C\" einen Dialekt der oft zur Systemprogrammierung eingesetzten, prozeduralen und assemblernahen Compilersprache C heraus. Das mitgelieferte Softwarepaket enthält eine Programmierumgebung mit Editor, Compiler und Linker. Für Quelltexte stehen unter \"Super C\" bis zu 41 kB an freiem Programmspeicher zur Verfügung, für den Objektcode maximal 53 kB. Darüber hinaus unterstützt die Software die Verwendung von RAM-Disks, etwa beim Betrieb mit Speichererweiterungen. Von Spinnaker Software stammt ein weiterer C-Dialekt namens \"Power C\".\n\nIn Westdeutschland veröffentlichten der Markt+Technik Verlag und Data Becker die C-Programmierumgebungen \"Small C\" bzw. \"Profi C 128\". \"Small C\" umfasst eine aus Editor, Compiler, Assembler, Linker, Lader, Archivverwaltungsprogramm mit C-Funktionsbibliothek und zahlreichen Hilfsprogrammen bestehende Entwicklungsumgebung für den Zweitprozessor Z80A im CP/M-Modus. Der für den Quellcode verfügbare Speicherplatz beträgt 56 kB. \"Profi C 128\" weist einen ähnlichen Leistungsumfang wie \"Small C\" auf und verfügt über eine am CP/M-Betriebssystem angelehnte Benutzeroberfläche mit Kommandozeileninterpreter, obwohl dieser C-Dialekt für den Hauptprozessor MOS 8502 im C128-Modus entwickelt wurde.\n\nAbacus veröffentlichte 1986 mit \"COBOL 128\" einen Dialekt der für kaufmännische Anwendungen gedachten, an die englische Standardsprache angelehnten prozeduralen Compiler-Hochsprache COBOL (englisch \"Common Business Oriented Language\"). Zum Lieferumfang gehören ein Editor, ein Compiler, ein Interpreter, ein Debugger sowie mehrere Dienstprogramme, etwa zur Optimierung des Programmcodes. Auf dem C64 geschriebene COBOL-Programme lassen sich ohne großen Aufwand mithilfe der beigefügten Dienstprogramme auf den C128 übertragen. Mit \"VS128COBOL\" wurde auch von Visionary Software ein COBOL-Ableger mit vergleichbarem Leistungsumfang entwickelt.\n\nComfood brachte den COBOL-Dialekt \"Nevada COBOL\" für den Betrieb unter CP/M-Plus heraus. Dessen Kompilate sind jedoch nur mit Ladeprogramm lauffähig und gestatten lediglich die Verwendung sequentieller und relativer Dateien.\n\nMit \"COMAL 80\" erschien 1987 beim Markt+Technik Verlag ein Editor nebst Interpreter der für Programmieranfänger entwickelten und strukturierte Programmierung unterstützenden, heute aber nur noch selten verwendeten höheren Programmiersprache COMAL als Public-Domain-Version. Dieser COMAL-Dialekt zeichnet sich durch die Möglichkeit der Verwendung von RAM-Disks sowie spezielle Grafik- und Soundbefehle aus. Für den Quellcode stehen relativ üppige 80 kB an Programmspeicher zur Verfügung. Außerdem brachte die westdeutsche Firma Belz ein Steckmodul namens \"Comal-80\" mit Interpreter, Editor, Grafik- und Soundbefehlen, RAM-Disk-Unterstützung und rund 40 kB an freiem Programmspeicher heraus.\n\nDer westdeutsche Verlag Holtkötter brachte im Jahr 1986 mit \"C-128-Forth\" eine Version der imperativen, stackbasierten und maschinennahen Programmiersprache Forth mit Compiler, Interpreter, Debugger und RAM-Disk-Funktion heraus. Zu den Vorzügen des für den C128 geschriebenen Forth-Dialektes zählt die im Vergleich zum nativen Commodore BASIC V7.0 um das Zehnfache erhöhte Arbeitsgeschwindigkeit sowie die leichte Erweiterbarkeit des Basisbefehlssatzes. Ein weiterer Forth-Dialekt mit zusätzlichen Grafik- und Soundbefehlen erschien beim Markt+Technik Verlag unter dem einfachen Titel \"Forth\". Überdies war mit \"FORTH-83\" eine zum Multitasking fähige, von Henry Laxen und Michael Perry entwickelte Forth-Implementierung mit Assembler, Decompiler und Editor als Public-Domain-Software für den CP/M-Modus verfügbar.\n\nComfood veröffentlichte mit \"Nevada Fortran\" eine Version der prozeduralen, vor allem für numerische Berechnungen in Wissenschaft und Forschung eingesetzten Programmiersprache Fortran für den C128 im CP/M-Modus. Das \"Nevada Fortran\"-Softwarepaket umfasst Compiler, Linker sowie Hilfsprogramme und erlaubt das Einbinden von Unterprogrammen in Maschinensprache. Die Kompilate von \"Nevada Fortran\" sind nur mit einem Ladeprogramm lauffähig und es werden ausschließlich sequentielle Dateien unterstützt.\n\nTesco brachte einen Dialekt der häufig zur Programmierung künstlicher Intelligenz (kurz KI) experimentell eingesetzten funktionalen, prozeduralen Interpreter-Hochsprache Lisp unter dem Titel \"Lisp/80\" auf den Markt. Das \"Lisp/80\"-Softwarepaket enthält Editor, Linker sowie einige Hilfsprogramme. Im Gegensatz zu RAM-Disks werden jedoch weder Grafikbefehle noch das Einbinden von Unterprogrammen in Maschinensprache unterstützt. Vom US-Amerikaner David Betz stammt der als Public-Domain-Software für den CP/M-Modus veröffentlichte, neben KI-Funktionen auch objektorientiertes Programmieren unterstützende Lisp-Dialekt \"XLISP\".\n\nSystems Software brachte mit \"Oxford Pascal 128\" eine später von Free Spirit neu aufgelegte Version der weitverbreiteten prozeduralen Compiler-Hochsprache Pascal heraus. Von Abacus stammt das in Zusammenarbeit mit Data Becker entstandene, eine komfortable Programmierumgebung inklusive Editor und Assembler umfassende \"Super Pascal 128\" aus dem Jahr 1986. Bei diesem Pascal-Dialekt handelt es sich eine weiterentwickelte, mit vergrößertem Funktionsumfang ausgestattete C128-Portierung von \"Super Pascal 64\". Zum Lieferumfang gehörten auch zahlreiche Dienstprogramme, etwa zur Fehlerbereinigung, Erzeugung von Grafiken, Verwendung von RAM-Disks oder Unterstützung des Burst-Modus des Diskettenlaufwerks VC1571. Kyan Software veröffentlichte mit \"Kyan Pascal 128\" einen weiteren Pascal-Dialekt mit vergleichbarem Leistungsumfang. Das mitgelieferte Softwarepaket umfasste neben einem zusätzlichen Makroassembler auch einen Schnellkurs zum Erlernen des Programmierens in Pascal.\n\nAuch der Markt+Technik Verlag brachte mit \"Pascal C128\" eine eigene Pascal-Version heraus. Diese verfügt über Compiler, Editor, Linker, Grafik- und Soundbefehle, aber nur 22 kB an Programmspeicher für den Quellcode und unterstützt ausschließlich relative sowie sequentielle Dateien. Dafür lassen sich Maschinenspracheroutinen in die Pascal-Programme einbinden. Data Becker veröffentlichte mit \"Profi-Pascal Plus\" einen weiteren Pascal-Dialekt mit Grafikbefehlen, Soundbefehlen, Linker, Editor und schnellem Compiler. Unter \"Profi-Pascal Plus\" ist ferner ebenfalls die Einbindung von Maschinenspracheroutinen sowie die Verwendung von RAM-Disks mit einer Speicherkapazität von bis zu 58 kB möglich.\n\nDigital Research entwickelte mit \"Pascal/MT+\" einen auf den Einsatz im Geschäftsbereich ausgerichteten Pascal-Dialekt mit Programmierumgebung für die Rechner der Scheider-CPC-Reihe, der in Westdeutschland vom Markt+Technik Verlag vertrieben wurde und als Portierung auch auf dem C128 im CP/M-Modus lauffähig ist. Insgesamt 59 kB stehen für den Quellcode an Speicherplatz zur Verfügung. Borland brachte schon früh eine ebenfalls unter CP/M laufende Version des für seine Schnelligkeit bekannten \"Turbo Pascal\" auf den Markt. Spätere Versionen von \"Turbo Pascal\" mit lediglich 27 kB an Speicherkapazität für den Quellcode wurden von Tesco, Heimsoeth und dem Markt+Technik Verlag vertrieben. Eine weitere C128-Portierung eines CP/M-Pascal-Dialektes mit allerdings recht begrenztem Befehlssatz trägt den Titel \"Nevada Pascal\" und erschien bei Comfood sowie bei Tesco. Mit dem vom US-Amerikaner James Robert Tyson entwickelten \"JRT Pascal\" erschien auch eine leistungsfähige Public-Domain-Pascal-Programmierumgebung mit Editor, Compiler, Assembler und Linker für den CP/M-Modus. Zu den Besonderheiten von \"JRT Pascal\" zählen von Diskette nachladbare externe Prozeduren.\n\nFür den CP/M-Modus brachte Tesco ferner einen Ableger der zur Entwicklung von Übungen, Tests und interaktiven Lernprogrammen für computergestütztes Lernen dienenden Interpreter-Hochsprache PILOT heraus. Die C128-Portierung dieser heute kaum noch verwendeten Programmiersprache trägt den Titel \"Nevada PILOT\". Sie umfasst weder Grafikbefehle für den 80-Zeichen-Modus noch RAM-Disk-Unterstützung und erschien ausschließlich auf Diskette.\n\nMit \"E-Prolog\" wurde auch eine unter CP/M-Plus lauffähige, vom US-Amerikaner G.A. Edgar geschriebene Version der logischen, deklarativen und häufig in der KI-Forschung eingesetzten Interpreter-Hochsprache Prolog als Public-Domain-Software veröffentlicht. Überdies erschien mit dem \"VALGOL Compiler\" ein in \"E-Prolog\" programmierter Compiler für VALGOL, einem vom ebenfalls US-amerikanischen Programmierer Dewey Val Schorre entwickelten und zur Familie der ALGOL-Programmiersprachen gehörenden Dialekt.\n\nEine bestmögliche Ausnutzung der Computerhardware ist nur durch die Verwendung von maschinennaher Assemblersprache möglich, deren Programme schneller laufen und obendrein weniger Speicher verbrauchen als in höheren Programmiersprachen geschriebene. Benötigt wird hierfür ein Assembler, also ein Übersetzungsprogramm, das die Programmanweisungen des in Assemblersprache geschriebenen Quelltextes in den direkten Binärcode der Maschinensprache überträgt. Das Ergebnis dieses Übersetzungsvorgangs wird als Objektcode bezeichnet. Programme in Assemblersprache sind nicht nur kompakter, sondern überdies in der Ausführung erheblich schneller als solche in höheren Programmiersprachen. Sie besitzen gegenüber der noch schnelleren Maschinensprache obendrein den Vorzug einer leichteren Handhabung durch die Verwendung von dem Wortschatz des Englischen entnommenen und leicht erinnerbaren Abkürzungen – den sogenannten Mnemonics. Angehende Programmierer und Hobbyisten bevorzugten in den 1980er Jahren allerdings meist die zwar leistungsschwächeren, aber komfortableren höheren Programmiersprachen. Besonders populär waren vor allem die häufig in die Systemsoftware der gängigen Rechnermodelle integrierten BASIC-Dialekte sowie die zahlreichen Ableger der Programmiersprachen ALGOL, COBOL und Pascal.\n\nClick Here Software brachte die integrierte Entwicklungsumgebung \"Buddy 64/128 Assembly Development System\" mit Editor, Assembler, Linker und zahlreichen Dienstprogrammen heraus. Diese ursprünglich für den C64 konzipierte und später dann für den Betrieb mit dem C128 erweiterte Programmierumgebung erlaubt die Übersetzung von Assemblerprogrammen sowohl in die Maschinensprache des Hauptprozessors MOS 8502 als auch in die des Zweitprozessors Z80A. Auch Spinnaker veröffentlichte diese von Chris Miller geschriebene Programmierumgebung unter dem Titel \"Better Working: Power Assembler\". Weitere Programmierumgebungen mit vergleichbarem Leistungsumfang, aber ausschließlicher Konzentration auf die Maschinensprache des Hauptprozessors MOS 8502 erschienen mit dem \"JCL Assembler and Program Development System\" von JCL Software, dem \"Karma Assembler 64/128\" von PHD Software, dem \"C-128 Midnight Assembly System\" von Mountain Wizardry Software, dem \"Rebel Assembler/Editor\" von Nu Age Software und dem zusätzliche Dienstprogramme für die Entwicklung von Sprites und Audiodateien enthaltenden \"Total Software Development System\" von NoSync Software. Robert Wagner Publishing veröffentlichte mit \"Merlin 128\" ein nicht kopiergeschütztes Softwarepaket mit ausschließlich im 80-Zeichen-Modus lauffähigem Makroassembler und Disassembler. Für den Quelltext stehen unter \"Merlin 128\" bis zu 35 kB an freiem Programmspeicher zur Verfügung.\n\nDie niederländische Softwarefirma Radarsoft veröffentlichte mit \"Fast 128\" ein vergleichbares Programmpaket. In Westdeutschland kam bereits Ende 1985 mit \"Top-Ass\" beim Markt+Technik Verlag ebenfalls eine Programmierumgebung für Assemblersprache zum Preis von 89 DM heraus, für die 1987 zusätzlich noch ein Programmierkurs zum Selbstlernen namens \"Top-Ass Plus\" erschien. Neben Editor, Makroassembler, Maschinensprachemonitor und Disassembler enthält \"Top-Ass Plus\" auch Informationen über illegale Opcodes und Fehler des Hauptprozessors MOS 8502. Commodore selbst veröffentlichte erst im Herbst 1988 die Entwicklungsumgebung \"C128 Developers Package\" mit Editor, Assembler sowie Makroassembler für 50 US$.\n\nDie Firma Holtkötter veröffentlichte mit \"C128-Learn\" eine Einführung in die Maschinensprache des Zweitprozessors Z80A nebst einem aus Maschinensprachemonitor, Assembler und Disassembler bestehenden Softwarepaket. Mit \"C128-Macro\" sowie \"C128-Profi\" brachte die Holtkötter zusätzlich einen Makroassembler und eine ergänzende Programmbibliothek für den Z80A heraus. Das für den CP/M-Modus geschriebene \"C128-Macro\"-Softwarepaket enthält einen Compiler sowie einen Editor. Assemblerprogramme werden dabei nicht wie üblich als Ganzes gespeichert, sondern in Form von Screens, die dem Inhalt einer Bildschirmseite entsprechen. Die Programmbibliothek \"C128-Profi\" beinhaltet überdies ein leistungsstarkes Hilfsprogramm, das die ansonsten nicht vom CP/M-Plus-Betriebssystem vorgesehene Darstellung von Farbgrafiken im 80-Zeichen-Modus und Zugriffe auf den 80-Zeichen-Grafikchip auch im CP/M-Modus ermöglicht. Außerdem erschien mit dem Assembler \"ZMAC\", dem Linker \"ZLINK\", dem interaktiven Disassembler \"DASM\" sowie dem Debugger \"ZMON\" eine Z80A-Assembler-Entwicklungsumgebung inklusive eines Editors als Public-Domain-Softwarepaket.\n\nFür Studierende der Psychologie erschien das 1989 von C. R. Leith, S. L. Bums und H. Hamm an der Northern Michigan University entwickelte \"Psychology Laboratory on a C-128\" mit anspruchsvoller Grafikausgabe und zahlreichen Arbeitsblättern zum Selbstlernen.\n\nFür den C128-Modus sowie den Betrieb unter CP/M-Plus erschienen zahlreiche kommerzielle Anwendungsprogramme, darunter Textverarbeitungen, Grafikprogramme, CAD-Anwendungen, Datenbankanwendungen, Steuer- und Finanzsoftware, Tabellenkalkulationen sowie Büroanwendungen für den professionellen Einsatz des Rechners in unterschiedlichen Geschäftsbereichen.\n\nMit \"Vizawrite 128\" brachte Solid State Software eine nach dem Vorbild des Z80-basierten Wang Word Processor Systems von Kevin Lacy in Maschinensprache programmierte und entsprechend schnelle Portierung der unter dem Namen \"Vizawrite\" schon auf dem C64 erfolgreichen Textverarbeitung heraus. Das mit Pull-down-Menüs, Fenstersystem, Taschenrechnerfunktion, komfortabler Druckeransteuerung, einem Wörterbuch mit über 30.000 Einträgen sowie einem Kopierschutzmodul ausgestattete \"Vizawrite 128\" war mit 348 DM sehr teuer, sodass der westdeutsche Vertreiber Besitzern der C64-Version einen Preisnachlass für die deutschsprachige Ausgabe anbot. \"Vizawrite 128\" unterstützt ferner eine RS232C-Schnittstelle, besitzt eine Serienbrieffunktion und verfügt über einen Textspeicher von 56 kB.\n\nWeitere Textverarbeitungen erschienen in der englischsprachigen Welt mit \"Wordpro 128\" von Spinnaker, \"The Write Stuff\" von Busy Bee Software, \"Fleet System 4\" von Professional Software, dem ursprünglich für den Apple II entwickelten \"Trio 128\" von Softsync, dem über einen Zeichensatzeditor, 57 Schriftarten inklusive eines deutschen Zeichensatzes sowie zahlreiche Druckertreiber verfügenden \"Fontmaster 128\" von Xetec sowie \"Word Writer 128\" von Timeworks. Von Free Spirit stammt ein für den 80-Zeichen-Bildschirm des C128 geeignetes Desktop-Publishing-Programm namens \"News Maker 128\". In der US-amerikanischen Computerzeitschrift \"Compute!’s Gazette\" erschien überdies das von Robert Kodadek programmierte, rein diskettenbasierte \"SpeedScript 128\" mit 51 kB Textspeicher, 12 kB Textpuffer für die Zwischenablage von Dokumentteilen und Suchfunktion in der Oktober-Ausgabe 1987 zum Abtippen. Im September 1989 wurde an gleicher Stelle die von Michael Gruber entwickelte Nachfolgeversion \"SpeedScript 128 Plus\" mit erweitertem Befehlssatz und verbesserten Editiermöglichkeiten publiziert.\n\nCommodore selbst brachte in Zusammenarbeit mit Precision Software das menügesteuerte, leicht zu bedienende \"Superscript 128\" mit 80 kB Textspeicher, bis zu 240 Zeichen pro Zeile und Serienbrieffunktion heraus. Mit \"Superscript 128\" erstellte Dateien konnten auch von der Dateiverwaltung \"Superbase 128\" verwendet werden – und umgekehrt. Commodore veröffentlichte überdies ein Softwarepaket names \"Jane,\" das neben der Textverarbeitung \"Janewrite\" auch die Tabellenkalkulation \"Janecalc\" und die Datenverwaltung \"Janelist\" umfasst. \"HomePak 128\" von Batteries Included verfügt über eine ähnliche Ausstattung, weist aber anstelle der Tabellenkalkulation ein Telekommunikationsprogramm auf.\n\nDer Markt+Technik Verlag veröffentlichte das zuvor schon für den C64 und die CBM-Bürorechner entwickelte, für seinen günstigen Preis von 89 DM ausgesprochen leistungsstarke und mit 60 kB Textspeicher, Silbentrennung sowie einem 25.000 Einträge umfassenden Wörterbuch für die Rechtschreibkorrektur ausgestattete \"Protext 128\". \"Protext 128\" weist überdies eine Split-Screen-Funktion, eine integrierte Tabellenkalkulation und eine eigene Programmiersprache auf, die von der Programmiersprache Pascal her bekannte und Verschachtelungen zulässt. Auch das noch kostengünstigere, vor allem für Einsteiger gedachte \"Master-Text 128\" mit 64 kB Textspeicher, Fenstersystem, Serienbrieffunktion, Taschenrechnerfunktion und programmierbaren Floskeln stammt von Markt+Technik. Eine Stärke von \"Master-Text 128\" besteht in der Abstimmung auf die als Zwischenspeicher für Texte verwendete Commodore-Speicherweiterungen der Typen 1700, 1750 oder 1764.\n\nSybex veröffentlichte das mit 60 kB Textspeicher, fünf Zeichensätzen und Serienbrieffunktion ausgestattete \"StarTexter 128\" für den westdeutschen Markt. Das \"StarTexter 128\"-Paket umfasste ferner einen Selbstlernkurs für Neueinsteiger auf dem Gebiet der Textverarbeitung. Data Becker brachte die mit 80 kB Textspeicher, Grafikdruck, Trennvorschlägen und Datenfernübertragung arbeitende Textverarbeitung \"Textomat Plus 128\" heraus. Der Stark Verlag veröffentlichte mit \"SV-Text\" eine auf den Einsatz in Schulen und Universitäten ausgerichtete, das Erstellen von Arbeitsblättern, Diplomarbeiten mit Inhalts- und Stichwortverzeichnis sowie das Katalogisieren von Quellen erleichternde Textverarbeitung.\n\nDer Markt+Technik Verlag veröffentlichte außerdem eine deutschsprachige Version der von MicroPro stammenden, in der ersten Hälfte der 1980er Jahre standardsetzenden Textverarbeitung \"WordStar 3.0\" für den CP/M-Modus. \"WordStar 3.0\" erlaubt die Verwaltung von Fußnoten und besitzt umfangreiche Editierfunktionen. Der Texteditor arbeitet mit 256 Zeichen pro Zeile, besitzt eine Serienbrieffunktion und verfügt über 60 kB an Textspeicher. Die meist über Tastenkombinationen mit der Control-Taste erfolgende Bedienung erfordert allerdings eine längere Zeit der Einarbeitung. Außerdem arbeitet der Texteditor von \"WordStar 3.0\" vergleichsweise langsam.\n\nBei entsprechender Ausstattung mit einem leistungsfähigen Drucker und guter Software galt der C128 auf dem Gebiet der Textverarbeitung als den wesentlich kostspieligeren IBM-PC-Kompatiblen bzw. den Personal Computern durchaus ebenbürtig. Allerdings musste man – in Abhängigkeit vom verwendeten Textverarbeitungsprogramm – beim Bedienkomfort gelegentlich Abstriche machen, etwa hinsichtlich bestimmter Editierfunktionen wie der Berücksichtigung der Groß- und Kleinschreibung bei der Stichwortsuche, der Darstellung unterschiedlicher Schriftarten auf dem Bildschirm oder der Einbindung von Grafiken.\n\nFree Spirit brachte die mausgesteuerten, auf die Standardauflösung von 640 × 200 Bildpunkten des 80-Zeichen-Grafikchips ausgerichteten Malprogramme \"Sketchpad 128\" und das leistungsstärkere \"Spectrum 128\" auf den Markt. Letzteres läuft allerdings ebenso wie \"News Maker 128\" ohne Erweiterung des VRAM nur auf dem C128D-CR, da die genannten Anwendungen auf den lediglich bei dieser Modellvariante ab Werk auf volle 64 kB ausgebauten Grafikspeicher zurückgreifen. Weitere Grafikprogramme mit ähnlichem Leistungsumfang erschienen mit \"3D Graphics Drawing Board\" von Glentop Publishers, \"Colorez-128\" von B-Ware Computer Systems, \"Ipaint\" von Living Proof Software, \"Page Illustrator 128\" von Patech Software, \"Poster Maker 128\" von Free Spirit und \"Spray Paint 128\" von PHD Software Systems. In Westdeutschland wurde 1987 das befehlsgesteuerte, per Joystick oder Maus zu bedienende \"StarPainter 128\" mit Editoren für Sprites, Zeichensätze und Füllmuster von Sybex veröffentlicht. Überdies erlaubt das Malprogramm das Kombinieren von Grafik und Text und enthält Treiber zum Betrieb mit fast allen damals gängigen Druckermodellen. Der große Nachteil von \"StarPainter 128\" besteht in der Begrenzung auf 40-Zeichen-Grafikchip VIC IIe. Der Markt+Technik Verlag brachte das wahlweise joystick- oder mausgesteuerte, mit einer Maximalauflösung von 640 × 192 Pixeln arbeitende Malprogramm \"Paint R.O.I.A.L.\" heraus. \"Paint R.O.I.A.L.\" zeichnet sich durch einfache Bedienbarkeit aus und kann sowohl in einem Schwarzweiß- als auch Farbmodus betrieben werden.\n\nAbacus veröffentlichte mit \"CadPak 128\" eine ebenfalls den vollausgebauten Grafikspeicher voraussetzende, mausgesteuerte CAD-Anwendung mit einer Maximalauflösung von damals beeindruckenden 640 × 360 Bildpunkten im Interlacemodus und der Möglichkeit der gleichzeitigen Verwendung zweier Monitore im 40- und 80-Zeichen-Modus. Micro Aided Designs brachte das gleichermaßen mausgesteuerte CAD-Programm \"Technological Highbred Integrated System\" (kurz \"T.H.I.S.\") heraus, das auch die Verwendung eines zusätzlichen Lichtgriffels unterstützt. Bei \"T.H.I.S.\" handelt es sich um eines der wenigen von Haus aus auf die Verwendung mit einer der recht kostspieligen Speichererweiterungen der Typen 1700, 1750 sowie 1764 ausgerichteten kommerziellen Programme für den C128-Modus. Von K&K Software stammt die ebenfalls mit Maussteuerung versehene, 1987 für Architekten und Innenarchitekten geschriebene CAD-Anwendung \"Home Designer\". In Westdeutschland veröffentlichte der Markt+Technik Verlag die mit einer optionalen Auflösung von 640 × 200 Bildpunkten im Schwarzweißmodus und 640 × 176 Bildpunkten im Farbmodus arbeitende, mit integriertem Zeichen- sowie Sprite-Editor ausgestattete CAD-Anwendung \"High-Screen-CAD C128\". Die Steuerung erfolgt vorwiegend über die Tastatur, weshalb sich \"High-Screen-CAD C128\" besonders zum Anfertigen technischer Zeichnungen eignet.\n\nCardinal Software veröffentlichte die eine Verwaltung von bis zu 7.000 Datensätzen und die Verwendung von bis zu 20 Suchkriterien gestattende Datenbankanwendung \"Flex File 128\". Solid State Software brachte ein in Maschinensprache geschriebenes und entsprechend schnelles Softwarepaket namens \"Vizastar 128\" mit Datenverwaltung, Tabellenkalkulation und Malprogramm auf den Markt. Mit \"Data Manager 128\" erschien auch eine Datenverwaltung von Timeworks. Weitere Datenbankprogramme waren \"Datafiler 128\" von Free Spirit mit maximal 5.000 Datensätzen, das preisgünstige, aber leistungsstarke \"DFile 128\" von Michaelsoft, \"Paperback Filer 128\" von Digital Solutions, \"Record Master 128\" von Woodsoftware und \"Ultrabase 128\" von Gold Disk. Commodore selbst brachte in Zusammenarbeit mit Precision Software das in Europa sehr erfolgreiche relationale Datenbankprogramm \"Superbase 128\" heraus. Die menügesteuerte \"Superbase 128\" verfügt über eine eigene, leicht erlernbare Programmiersprache, 62 kB Arbeitsspeicher und lässt sich neben der Datenverwaltung sowohl zur Tabellankalkulation als auch zur Fakturierung einsetzen.\n\nVon Sybex stammt das mit deutschsprachiger Menüsteuerung ausgestattete Dateiverwaltungsprogramm \"StarDatei\". Der Markt+Technik Verlag veröffentlichte mit \"Prodat 128\" eine für Einsteiger gedachte, kostengünstige Datenbankanwendung mit ähnlichem Leistungsumfang. Aus dem gleichen Hause kommt auch die als Nachfolgerin von \"Prodat 128\" konzipierte, menügesteuerte Dateiverwaltung \"Prodatei 128\" mit leistungsfähigem Suchsystem, umfangreicher Druckerunterstützung über den CBM-Bus sowie eigener Programmiersprache mit 30 Instruktionen. Data Becker entwickelte die mit 99 DM vergleichsweise preiswerte, über Pull-down-Menüs gesteuerte Datenbank \"Datamat 128\" sowie deren Nachfolgerin \"Datamat Plus 128.\" Beide Versionen besitzen jedoch weder eine eigene Programmiersprache noch umfassende Möglichkeiten der Auswertung von Datensätzen. Für den CP/M-Modus gab der Markt+Technik Verlag außerdem das von Ashton-Tate entwickelte, standardsetzende relationale Datenbanksystem \"dBase II\" in einer deutschsprachigen Version heraus. Zum Leistungsumfang der per Kommandozeileninterpreter zu bedienenden \"dBase II\" gehört ebenfalls eine eigene Programmiersprache zur Bearbeitung und Verknüpfung von Datensätzen.\n\nCMS Software Systems veröffentlichte ein vier Disketten umfassendes, auf mittelständische Unternehmen und Großbetriebe ausgerichtetes Finanzbuchhaltungsprogramm namens \"CMS Accounting System\" für die Bereiche Hauptbuchhaltung, Gehaltsabrechnung, Kostenrechnung, Rechnungserstellung, Kreditoren- und Debitorenbuchhaltung. Zusätzlich konnte mit \"CMS Inventory 128\" ein Anwendungsprogramm zur Verwaltung von Lagerbeständen erworben werden. Auch Softsync entwickelte zwei Finanzbuchhaltungsprogramme: \"Personal Accountant\" für Kleinunternehmen und \"Accountant, Inc.\" für mittelständische Betriebe. Ein weiteres Finanzbuchhaltungsprogramm für Kleinunternehmen namens \"The Accountant\" erschien bei KFS Software. Mit \"Faktustar 128\" brachte die Firma Willi Fornoff Soft ein für mittelständische Unternehmen gedachtes Programmpaket zum Ausstellen von Lieferscheinen und Rechnungen, Führen von Kundendateien, Kassenbüchern und Lagerlisten sowie zum Verfassen von Geschäftsbriefen heraus. Das von Jean-Daniel Lehmann Software & Service angebotene \"System Support Programm 128\" (kurz \"SSP-128\") bietet einen vergleichbaren Leistungsumfang, erlaubt aber zusätzlich das Einbinden von Software anderer Hersteller, etwa auf dem Gebiet der Textverarbeitungsprogramme.\n\nDaneben erschienen zahlreiche Programme für die private Finanzbuchhaltung, das Erstellen von Steuererklärungen und das persönliche Portfoliomanagement, darunter \"Checkbook 128\" von Nu Age Software, \"Finance and Statistics\" von Cardinal Software, \"Money Master\" von PRG Software, \"Personal Portfolio Manager 128\" und das \"Technical Analysis System 128\" von Abacus sowie \"Swiftax 128\" und \"Sylvia Porter’s Personal Financial Planner\" von Timeworks. In Westdeutschland brachte der Markt+Technik Verlag das etwa zur Erstellung von Kontenplänen, Umsatzsteuerauswertung oder Kostenstellenrechnung geeignete Programm \"Finanzbuchhaltung\" für den CP/M-Modus auf den Markt. Von der Firma Dialog-Partner stammt das ebenfalls unter CP/M laufende, dem Erstellen des Lohnsteuerjahresausgleichs dienende Programm \"Privat-87\", von dem jedes Jahr eine aktualisierte Version erschien (\"Privat-88\", \"Privat-89\" usw.).\n\nTabellenkalkulationen wurden für den C128 teils als Einzelprogramme, teils als Bestandteil umfangreicherer Softwarepakete angeboten. Zu den reinen Tabellenkalkulationen zählen \"Swiftcalc 128\" von Timeworks, \"SwiftSheet 128\" von Cosmi Corporation sowie \"Paperback Planner 128\" (zunächst auch unter dem Titel \"Pocket Planner 128\" vertrieben) von Digital Solutions. Softwarepakete mit Tabellenkalkulation und weiteren Dienstprogrammen wie Textverarbeitungen oder Datenbankanwendungen erschienen mit \"Rhapsody 128\" von King Microware, \"Trio 128\" von Softsync und \"Personal Choice Collection\" von Activision. Kommerziell wenig erfolgreich war \"Multiplan C128\", eine vom auf Computerspiele spezialisierten Publisher Epyx auf den C128 im CP/M-Modus portierte Fassung des Klassikers \"Microsoft Multiplan.\" \"Multiplan C128\" litt – bedingt durch den relativ langsamen, viel Speicherplatz beanspruchenden CP/M-Modus – an niedriger Datenverarbeitungsgeschwindigkeit und an lediglich 14 kB freiem Arbeitsspeicher.\n\nSoftsync veröffentlichte mit dem \"Desk Manager\" ein sowohl im 40- als auch im 80-Zeichen-Modus lauffähiges Softwarepaket mit verschiedenen Büroanwendungen inklusive Taschenrechnerfunktion, Terminplaner, Notizbuchfunktion, Telefondatei und einfachem Schreibprogramm für das Verfassen von Briefen. Ein vergleichbares Angebot stellt der von Commodore selbst herausgegebene \"Partner 128\" dar.\n\nBeinahe alle für den marktführenden Vorgänger C64 produzierten kommerziellen Spiele laufen auch auf dem C128 im C64-Modus problemlos. Ausnahmen bilden vor allem mit Diskettenschnellladern und Kopierschutzvorrichtungen ausgestattete Titel ohne Abstimmung auf die zum Vorgänger Commodore DOS 2.6 nicht vollständig kompatiblen Diskettenbetriebssysteme Commodore DOS 3.0 bzw. 3.1. Ein Beispiel hierfür ist der von Ocean Software veröffentlichte Action-Adventure-Titel \"Frankie Goes to Hollywood\". Gelegentlich liefen auch auf Kompaktkassetten veröffentlichte C64-Titel mit Schnellladern nicht reibungslos. Ein Beispiel hierfür ist das ebenfalls von Ocean Software stammende Arcadespiel \"Roland’s Rat Race\".\n\nDie fast vollständige C64-Kompatibilität des C128 sowie der im Vergleich zum Vorgängermodell niedrigere Verbreitungsgrad des Rechners lieferten professionellen Publishern kaum Anreize, Spielesoftware eigens für den C128-Modus und dessen leistungsfähigere Hardware zu entwickeln. Das Angebot an Spielen blieb daher überschaubar – ein in der Fachpresse häufig beklagter Zustand. Die meisten Titel erschienen auf Diskette und wurden in den Jahren 1986 bis 1988 auf den Markt gebracht. Ein Großteil der wenigen Actionspiele für den C128-Modus besteht jedoch lediglich aus grafisch kaum verbesserten Portierungen von bereits veröffentlichten C64-Spielen, etwa \"Kikstart 2\" oder \"The Last V8\" vom Billiganbieter Mastertronic. Nach der Produktionseinstellung im Jahr 1989 wurde keine neue Spielesoftware mehr für den C128-Modus geschrieben. Für den auf Anwendungsprogramme spezialisierten CP/M-Modus wurden überhaupt keine kommerziellen Spiele produziert. Vereinzelt erschienen aber auf aufwändige Grafik verzichtende Spieletitel wie das von Mike Goetz vom DEC-Minicomputer PDP-10 auf CP/M-Rechner portierte Textadventure \"Colossal Cave\" als Public-Domain-Software.\n\nInsgesamt sind derzeit (Stand 1. September 2016) im Spielearchiv der Online-Datenbank MobyGames lediglich 23 im C128-Modus lauffähige kommerzielle Computerspiele dokumentiert. Zu den seinerzeit beliebtesten Genres zählten interaktive, an Motive aus der Science-Fiction- bzw. Fantasy-Literatur anknüpfende Textadventures und Rollenspiele, die von den verbesserten Textdarstellungsfähigkeiten des 80-Zeichen-Grafikchips Gebrauch machen und vor allem von den US-amerikanischen Publishern Infocom sowie Sir-Tech herausgegeben wurden. Dazu zählen Umsetzungen einflussreicher Spiele-Franchises wie etwa \"Ultima\" oder \"Wizardry\". Daneben wurden vereinzelt Actionspiele, Rennspiele und eine Marinesimulation von verschiedenen US-amerikanischen, britischen und japanischen Publishern wie Origin Systems, Mastertronic oder Taito veröffentlicht. Nach anfänglichem Enthusiasmus zogen sich die britischen Spieleproduzenten jedoch schon 1987 vollständig vom wenig lukrativen Markt für C128-Computerspiele zurück. 1989 erschien mit \"Fun Pak 128\" eine acht Titel umfassende, von MobyGames nicht dokumentierte Spielesammlung. Ebenfalls dort nicht dokumentiert ist das 1986 von Free Spirit Software herausgegebene, den Ersten Weltkrieg thematisierende Strategiespiel \"The Great War\".\n\nVon einer Ausnahme abgesehen gab es keine kommerzielle Computerzeitschrift mit ausschließlichem Bezug zum C128. Allerdings berichteten neben diversen von Commodore selbst herausgegebenen Magazinen verschiedene, nicht an eine bestimmte Plattform oder einen bestimmten Hersteller gebundene Periodika in der englischsprachigen Welt sowie im deutschsprachigen Raum mehr oder minder regelmäßig über den C128 und versorgten ihre Leserschaft mit Informationen über verschiedenste, mit der Verwendung und Programmierung des Rechners verbundene Themengebiete. Wem diese Quellen nicht ausreichten, konnte sich auf dem Büchermarkt mit Fachliteratur versorgen. Zahlreiche westdeutsche Fachverlage wie Markt+Technik, Sybex oder Data Becker publizierten umfangreiche Monografien etwa über die Grundlagen des Rechners, seine BASIC-Programmierung oder das Programmieren in Assemblersprache.\n\nIn den Vereinigten Staaten erschien mit dem \"Commodore Magazine\" von 1987 bis 1989 eine ausschließlich Commodore-Rechner thematisierende Computerzeitschrift, die auch über den C128 berichtete. Außerdem erschienen in der zweiten Hälfte der 1980er Jahre mehrere unabhängige kommerzielle Computerzeitschriften mit inhaltlichem Bezug auf sämtliche 8-Bit-Rechner von Commodore, die sich auch regelmäßig mit dem C128 beschäftigten. Zu diesen vor allem mit Testberichten, Programmausdrucken zum Abtippen und Kaufberatungshinweisen aufwartenden Publikationen zählen \"Ahoy!, Compute!’s Gazette,\" das bis Anfang 1988 auf einem C128 mit der Textverarbeitung \"Vizastar 128\" produzierte Magazin \"Info: The Useful Guide to Commodore Computing\" (ursprünglich \"Info–64\") und \"Run\". Letztere wurde vom US-amerikanischen Mutterkonzern – der International Data Group (kurz IDG) – auch in einer deutschsprachigen Fassung auf den Markt gebracht. Das als Autorität geltende kanadische Computermagazin \"The Transactor\" war in ganz Nordamerika erhältlich und setzte den Schwerpunkt seiner Berichterstattung auf Hardwareprojekte, Bauanleitungen und Reparaturtipps, die gelegentlich auch den C128 betrafen. Ab Januar 1986 erschien außerdem das exklusiv über den C128 berichtende semiprofessionelle Informationsblatt \"Twin Cities 128\". Gegen Ende der Marktpräsenz des C128 wurde noch das rein diskettenbasierte US-amerikanische Computermagazin \"Loadstar 128 Quarterly\" auf den Markt gebracht, das sich in 42 vierteljährlich ab 1989 erschienenen Ausgaben ebenfalls ausschließlich dem C128 widmete.\n\nIn Großbritannien wurden mit \"Commodore Horizons, Commodore Computing International, Commodore Disk User,\" dem \"Commodore User Magazine\" sowie \"Your Commodore\" mehrere ebenfalls rein auf Commodores 8-Bit-Computer Bezug nehmende, unabhängige Computerzeitschriften herausgebracht.\n\nCommodore International selbst gab ab September 1986 die Zeitschrift \"Commodore Magazine\" heraus, die ebenfalls neben den übrigen 8-Bit-Heimcomputern aus dem eigenen Hause den C128 in ihre Berichterstattung einbezog. Entstanden ist dieses Periodikum aus der Zusammenlegung zweier älterer Commodore-Zeitschriften namens \"Commodore Power Play\" mit dem Schwerpunkt Computerspiele und \"Commodore Microcomputers\" mit dem Schwerpunkt Bürorechner der CBM-Reihe.\n\nIm deutschsprachigen Raum erschienen regelmäßig Programmausdrucke und Artikel über die Hardware des C128 in der populären Computerzeitschrift \"64’er,\" die sich allerdings hauptsächlich mit dem marktführenden Vorgängermodell C64 beschäftigte und ab Mitte des Jahres 1990 die Berichterstattung über den C128 – von gelegentlichen Programmiertipps abgesehen – weitgehend einstellte.\n\nUnter dem Titel \"128’er\" wurden jedoch von Zeit zu Zeit Sonderhefte mit ausschließlichem Bezug zum C128 herausgebracht. Frühere, in den Jahren 1986 bis 1988 veröffentlichte Ausgaben dieser insgesamt 14 Sonderhefte enthalten Programmlistings zum Abtippen und kosteten 14 DM, während späteren Ausgaben ab 1989 zum Preis von 24 DM eine Diskette mit den im Heft behandelten Programmen beilag. Inhaltliche Schwerpunkte der \"128’er\"-Sonderhefte bilden die volle Ausreizung der technischen Fähigkeiten der 80-Zeichen-Grafikchips MOS 8563 bzw. MOS 8568, Anwendungsprogramme, Hilfsprogramme, das Programmieren in Maschinensprache, Peripheriegeräte wie etwa das Diskettenlaufwerk VC1571, diverse CP/M-Anwendungen, Hardwareprojekte, Denkspiele und Programmierwettbewerbe. Das letzte \"128’er\"-Sonderheft erschien 1995.\n\nWeitere, auf Commodore-Rechner spezialisierte Zeitschriften mit regelmäßiger Berichterstattung über den C128 waren die \"CBM-Revue\" (1984–1986) sowie deren Nachfolgerin \"Commodore Welt\" (1986–1988). Außerdem berichteten auch unabhängige, nicht an eine bestimmte Plattform oder einen bestimmten Hersteller gebundene Computermagazine wie \"Chip, c’t, Computer Kontakt, Computer Persönlich\" oder \"Happy Computer\" gelegentlich über den Rechner.\n\nAm 1. Juni 1987 wurde mit dem \"Club 128’er Aktuell\" im nordrhein-westfälischen Oer-Erkenschwick ein ausschließlich dem C128 gewidmeter Computerclub gegründet. Der über 300 Mitglieder zählende Club veröffentlichte zweimonatlich ein eigenes, mit Testberichten, Programmiertipps sowie beigefügter Diskette aufwartendes Clubmagazin mit dem Titel \"128’er Aktuell\".\n\nZum gegenwärtigen Zeitpunkt (Stand 1. Juli 2016) gibt es im Internet nur einen regelmäßig gepflegten C128-Emulator. Der auf zahlreichen gängigen modernen Betriebssystemen wie etwa Windows, macOS, Linux oder Unix lauffähige und von einem vielköpfigen internationalen Entwicklerteam als Freeware zum kostenlosen Download zur Verfügung gestellte \"Versatile Commodore Emulator\" (kurz VICE) emuliert neben den Commodore-Rechnern PET 2001, den CBM-Bürorechnern, der CBM-600-Serie, dem VC20, C64 und Plus/4 auch den C128. Die VICE-Webseite enthält neben einer ausführlichen technischen Dokumentation mit Informationen zu den Hardware-Eigenschaften des C128 und einer frei zugänglichen Wissensdatenbank auch eine umfangreiche Bedienungsanleitung des preisgekrönten Emulationsprogramms. Die aktuelle Version 2.4 wurde am 16. November 2012 veröffentlicht.\n\nZum Leistungsspektrum des C128-Emulators gehören Emulationen des Speicherverwaltungsbausteins MMU, des 80-Zeichen-Grafikchips MOS 8563, des 2-MHz-Modus des Hauptprozessors MOS 8502, des Zweitprozessors Z80A, des seriellen Busses des C128 sowie des C64-Modus. Die Arbeit am C128-Emulator von VICE ist aber noch nicht abgeschlossen; so steht beispielsweise eine Umsetzung der Diskettenlaufwerke VC1571 bzw. VC1581 derzeit noch aus.\n\nAls Nachfolgemodell des bis dahin erfolgreichsten Heimcomputers C64 mit einer weltweiten Anwenderbasis von mehreren Millionen Usern erregte die Markteinführung des C128 im Spätsommer des Jahres 1985 große Aufmerksamkeit in den führenden zeitgenössischen Computerzeitschriften, vor allem in den Vereinigten Staaten, Großbritannien und Westdeutschland. Schon Monate vor dem eigentlichen Verkaufsbeginn erschienen die ersten Testberichte. Dabei fielen die Urteile der Rezensenten sowohl in der englischsprachigen Welt als auch im deutschsprachigen Raum fast durchgehend positiv aus. Lediglich vereinzelt wurde Kritik an bestimmten Eigenschaften des C128 geübt.\n\nZu den wichtigsten in der englischsprachigen Fachpresse behandelten Eigenschaften des C128 zählten das native Commodore BASIC V7.0, die C64-Kompatibilität des Rechners, sein Design sowie die Leistungsfähigkeit der im Gerät verbauten Hardware angesichts der aufkommenden Konkurrenz durch erschwingliche Rechner mit fortschrittlicher 16-Bit-Architektur. Daneben spielten aber auch der Preis, das Softwareangebot, die mitgelieferte Dokumentation sowie der CP/M-Modus eine Rolle.\n\nIn der englischsprachigen Fachpresse wurde das leistungsfähige und mit umfangreichem Befehlssatz ausgestattete Commodore BASIC V7.0 des C128 fast durchgehend gelobt. Das gilt sowohl für ausschließlich Commodore-Computer behandelnde Computerzeitschriften als auch für nicht an eine Plattform bzw. einen Hersteller gebundene Computermagazine. Gelobt wurden beispielsweise die neuen Grafik- und Soundbefehle sowie die bedienungsfreundlichen Diskettenbefehle. Auch die neuen Befehle zur einfachen Programmierung von Sprites wurden positiv hervorgehoben. Die vom Commodore BASIC V7.0 gewährleistete Unterstützung strukturierten Programmierens überzeugte die Rezensenten ebenfalls. In diesem Kontext wurden nicht zuletzt die neuen, der Fehlersuche dienenden Befehle des Commodore BASIC V7.0 genannt. Bemängelt wurde hingegen das Fehlen spezieller Grafikbefehle für den hochauflösenden Grafikmodus des 80-Zeichen-Grafikchips MOS 8563.\n\nAußerdem fiel positiv auf, dass beim BASIC-Dialekt des C128 die vom C64 her gewohnte Notwendigkeit zahlreicher umständlicher codice_81 und codice_82Befehle zum direkten Auslesen und Beschreiben von Speicheradressen entfällt. Auch wenn das Commodore BASIC V7.0 im 1-MHz-Modus generell ungefähr um ein Viertel langsamer ist als das Commodore BASIC V2.0, laufen daher bestimmte Anwendungen wie etwa das Darstellen geometrischer Figuren auf dem C128 schneller als auf dem Vorgängermodell. Ferner wurde die Überlegenheit des Commodore BASIC V7.0 gegenüber den nativen BASIC-Dialekten anderer, mit dem C128 konkurrierender Rechner wie dem Applesoft BASIC des Apple IIc oder der Portierung von Microsoft BASIC des IBM-PCjr konstatiert.\n\nDie nahezu vollständige Softwarekompatibilität des neuen Rechners zum C64 wurde ebenfalls von zahlreichen Rezensenten positiv herausgestellt. Der C128 war damit der erste Commodore-Heimcomputer, auf dem auch die für das Vorgängermodell entwickelte Software weitgehend problemlos lief. Allerdings gab es hinsichtlich der Lauffähigkeit von C64-Programmen mit Kopierschutz von Anfang an durchaus berechtigte Zweifel.\n\nAls weiterer Vorteil wurde die kostensparende Hardwarekompatibilität des Rechners zu sämtlichen für den C64 entwickelten Peripheriegeräten gewertet. Auch auf diesem Gebiet wurde jedoch schon früh eine gewisse Skepsis deutlich. Sowohl die von Commodore behauptete hundertprozentige C64-Hardwarekompatibilität des C128 als auch die VC1541-Hardwarekompatibilität des neuen 5¼-Zoll-Diskettenlaufwerks VC1571 wurden von einem Rezensenten offen angezweifelt. Bemängelt wurde überdies die Tatsache, dass im C64-Modus der zusätzliche Arbeitsspeicher des C128 nicht etwa als die Arbeitsgeschwindigkeit erhöhende RAM-Disk verwendet werden kann, um die vollständige C64-Hardwarekompatibilität des Rechners nicht zu gefährden.\n\nDass der C128 bei eingelegtem C64-Steckmodul beim Einschalten automatisch in den C64-Modus springt, wurde ebenfalls lobend erwähnt. Allerdings berichtete eine Rezension von Schwierigkeiten mit der Darstellung von Sprites beim Betrieb der Steckmodulversion des C64-Sportspielklassikers \"International Soccer\".\n\nAußerdem wurde die gelungene Ästhetik des Gehäusedesigns zu den hervorstechendsten Vorzügen des C128 gezählt, das etwa als ‚elegant‘ (englisch \"elegant\"), ‚eindeutig attraktiv‘ (englisch \"obviously attractive\"), ‚stilvoll‘ (englisch \"stylish\"), ‚schnittig‘ (englisch \"sleek\"), ‚umwerfend‘ (englisch \"stunning\") oder gar ‚aufreizend‘ (englisch \"sexy\") bezeichnet wurde. Die flache, an der Vorderseite spitz zulaufende Gehäuseform erinnerte einen Rezensenten sogar an das damals den technischen Fortschritt symbolisierende, von Großbritannien und Frankreich gemeinsam entwickelte Überschall-Passagierflugzeug Concorde mit hydraulisch absenkbarer Cockpitnase. Die ‚superflache beige Konsole‘ (englisch \"slimline beige console\") entspreche im Übrigen dem typischen Geschmack eines ‚stilbewussten Geschäftsmanns‘ (englisch \"style-conscious businessman\"). Das gelte auch für den an Arbeitsplatzrechner bzw. Personal Computer erinnernden numerischen Ziffernblock.\n\nDie ‚schlanke, sanft angeschrägte Tastatur‘ (englisch \"slim, gently sloping keyboard\") wusste ebenfalls durch ihr gutes Tippgefühl, ihren ‚langen Tastenhub‘ (englisch \"deep key travel\"), ihre programmierbaren Funktionstasten sowie ihre das Editieren von Programmlistings erleichternden zusätzlichen Funktionstasten wie die Help-Taste, die Tabulatortaste, die Alt-Taste oder die Escape-Taste zu überzeugen. Allerdings wurde die Tastatur gelegentlich als ‚leicht schwammig‘ (englisch \"slightly squishy\") bemängelt. Der helle, cremefarbene Kunststoff des Gehäuses wurde zwar als ‚ästhetisch gefällig‘ (englisch \"aesthetically pleasing\") gelobt, aber auch als ‚Schmutz anziehend‘ (englisch \"it does attract the dirt\") kritisiert. Das Tastaturlayout nebst Lage der Cursor-Tasten, die ungünstige Position des Kassettenanschlusses sowie die große Stellfläche des C128D stießen ebenfalls vereinzelt auf Kritik.\n\nHardwareseiig wurde das externe Schaltnetzteil für seine Wartungsfreundlichkeit gelobt, da es im Gegensatz zum C64-Netzteil weder versiegelt noch vergossen sei und man die Sicherung leicht von außen erreichen könne. Zwecks Reparatur müsse einfach nur das massive Gehäuse aufgeschraubt werden. Gelobt wurden ebenfalls die hohe Verarbeitungsqualität sowie das gute Wärmemanagement des Netzgerätes. Obendrein bot das Schaltnetzteil des C128 mehr Leistung als das des Vorgängermodells.\n\nAuch die hohe Qualität der Bildschirmausgabe, der hinzugefügte Reset-Schalter, die Schnelligkeit des Diskettenlaufwerks VC1571, die guten Soundfähigkeiten, das Wärmemanagement, die komplexe und hochwertige Hauptplatine sowie der dedizierte Grafikspeicher des MOS 8563 wurden gelobt. Die Komplexität der Hardwarearchitektur wurde sogar mit der zu Verspieltheit und zahlreichen Verzierungen neigenden Kunstepoche des Rokoko verglichen. Aufgrund seiner Hardwareeigenschaften könne der C128 tatsächlich, wie schon in Commodores Werbekampagne suggeriert, mit wesentlich teureren Rechnern wie dem Apple IIc oder dem IBM-PC konkurrieren. Die recht niedrige Arbeitsgeschwindigkeit von angeblich effektiv lediglich 1,5 MHz sowie die Begrenztheit des Rechners auf eine Datenfernübertragungsrate von lediglich 1.200 Baud lösten hingegen gelegentlich Skepsis aus. Auch das als umständlich empfundene Umschalten zwischen 40- und 80-Zeichen-Monitor stieß vereinzelt auf Kritik.\n\nBereits vor der Markteinführung des C128 wurde der Rechner ferner vereinzelt als bloßer ‚Lückenfüller‘ (englisch \"stop-gap\") angesehen, dessen Hauptfunktion in der Überbrückung der bis zur Serienreife des neuen Amiga 1000 nötigen Zeitspanne bestehe. Der C128 stelle lediglich den letzten Versuch Commodores dar, noch einmal Geld auf dem schrumpfenden 8-Bit-Heimcomputermarkt zu verdienen, während die Zukunft den neuen 16-Bit-Mikrocomputern auf Basis des Hauptprozessors Motorola 68000 gehöre. Diese Sichtweise wird gelegentlich auch heute noch vertreten. Neben dem Amiga 1000 wurde auch der ebenfalls den Motorola 68000 als CPU verwendende Atari 520 ST als möglicherweise übermächtiger Konkurrent für den C128 gesehen.\n\nDas bereits mit Erscheinen des C128 extrem umfangreiche Softwareangebot für den C64-Modus sowie den CP/M-Modus wurde ebenfalls zu den großen Vorzügen des neuen Rechners gezählt. Schließlich handele es sich bei der in den C128 integrierten Systemsoftware von C64 und CP/M-Plus um die beiden populärsten Betriebssysteme überhaupt. Eine Rezensentin schätzte im Juni 1985 die Zahl der für den C64-Modus verfügbaren kommerziellen Programme auf 6.000, von denen die meisten 1983 oder später erschienen seien, während die für das Konkurrenzmodell Apple IIc verfügbare Software meist deutlich älter sei. Obendrein gebe es für den CP/M-Modus weitere, in die Tausende gehende Anwendungsprogramme. Der C128 stelle daher für Einsteiger, Hobbyisten und Geschäftsleute gleichermaßen ein ‚Schnäppchen‘ (englisch \"bargain\") dar. Schon Anfang 1986 zeichnete sich jedoch ein Mangel an Software für den C128-Modus ab. Außerdem stieß die Softwareinkompatibilität des C128 zum mittlerweile zum Standardbetriebssystem aufgestiegenen PC-DOS des IBM-PC bzw. MS-DOS der IBM-PC-Kompatiblen auf Kritik.\n\nDie Systemsoftware selbst stieß ebenfalls auf ein positives Echo. Das C128-Betriebssystem etwa wurde für seinen integrierten Maschinensprachemonitor gelobt. Dieser sei jederzeit bequem und zeitsparend einsetzbar, da er im Festspeicher residiere. Ferner sei er aufgrund seiner Bedienerfreundlichkeit auch für Anfänger gut nutzbar. Beispielsweise erlaube er das Übersetzen von Dezimalzahlen in Hexadezimalzahlen – und umgekehrt. Auch die C128-Portierung von CP/M-Plus wusste durch ihre im Vergleich zum für den C64 produzierten CP/M-Steckmodul relativ hohen Arbeitsgeschwindigkeit, ihre Flexibilität hinsichtlich der Verwendung unterschiedlicher Datenaufzeichnungsformate sowie ihre gelungene Emulation eines ADM-31-Terminals des US-amerikanischen Technologiekonzerns Lear Siegler zu überzeugen.\n\nDie große Vielseitigkeit sowie das breite Einsatzspektrum des C128 gehörten ebenfalls in der zeitgenössischen Rezeption zu den Vorzügen des neuen Rechners. In dieser Hinsicht könne es der C128 mit dem erheblich teureren Apple II aufnehmen und übertreffe diesen womöglich sogar. Was die Dokumentation betrifft, gab es Lob für das ausführliche und leicht verständliche, im Lieferumfang enthaltenen Handbuch. Allerdings wurde das Handbuch auch für seine Unvollständigkeit kritisiert.\n\nHinsichtlich des Einführungspreises überwog das Lob. Die unverbindliche Preisempfehlung in Höhe von gerade einmal 300 US$ bzw. der erwartete Straßenpreis von ca. 250 £ galten im Vergleich zu Konkurrenzmodellen wie dem Apple IIc oder dem IBM-PCjr als günstig. Auch im Vergleich zu den meist hochpreisigen CP/M-Computersystemen schnitt der C128 gut ab. Zum Zeitpunkt der Markteinführung im Sommer 1985 gab es weltweit keinen günstigeren CP/M-Rechner als den C128, was ebenfalls lobende Erwähnung fand. Ein Rezensent wertete ein aus einem C128 mit Commodore-Farbmonitor 1902 und Diskettenlaufwerk VC1571 bestehendes Komplettsystem im Vergleich zum Ende 1985 in Westeuropa erschienenen, technisch nur geringfügig leistungsschwächeren und ebenfalls CP/M-fähigen, aber deutlich preiswerteren britischen All-in-one-Computer Amstrad CPC6128 allerdings als überteuert.\n\nVolker Everts sieht im \"64’er\"-Magazin den C128 „in einer völlig neuen Leistungsklasse“ und verortet den neuen Rechner „im Bereich zwischen Homecomputer und Personal Computer“. Begründet wird dies mit dem leistungsstarken BASIC-Dialekt, dem Sprite-Editor, der Fähigkeit zur Darstellung von 80 Zeichen pro Zeile, den guten Peripheriegeräten, dem gelungenen Design von Tastatur und Gehäuse sowie dem ausbaufähigen Arbeitsspeicher. Außerdem wird die umfangreiche Programmbibliothek hervorgehoben, die sich aus der C64-Kompatibilität sowie der CP/M-Fähigkeit des C128 ergebe. In einem weiteren, sehr ausführlichen Testbericht, der ebenfalls im \"64’er\"-Magazin erschienen ist, loben Everts und Coautor Harald Meyer überdies die höhere Rechengeschwindigkeit des Hauptprozessors MOS 8502 im Vergleich zum MOS 6510 des Vorgängers C64. Auch die Komfortabilität des BASIC V7.0 hinsichtlich der Programmierung des Soundchips SID sowie der fest zum Betriebssystem gehörende Maschinensprachemonitor finden lobende Erwähnung. Allerdings bemängeln die Autoren die Tatsache, dass der neuentwickelte Grafikchip MOS 8563 im 80-Zeichen-Modus weder über einen eigenen Grafikmodus noch über BASIC-Befehle zur Erstellung von Grafiken in der Maximalauflösung verfügt.\n\nAuch Peter Zumbach von der Zeitschrift \"Happy Computer\" sieht den C128 als Grenzgänger zwischen Bürorechner und Heimcomputer. Lobende Erwähnung finden dabei die beiden vergleichsweise schnellen Hauptprozessoren, die C64-Kompatibilität, der große Arbeitsspeicher, die Ausbaufähigkeit des Arbeitsspeichers zu einer RAM-Disk, die laut Zumbach oft übersehene Grafikfähigkeit im hochauflösenden 80-Zeichen-Modus und die Möglichkeit, gleichzeitig zwei Monitore am C128 zu betreiben. Außerdem äußert sich der Rezensent positiv über die Grafikbefehle des komfortablen, strukturiertes Programmieren ermöglichenden BASIC V7.0, den Sprite-Editor, den Maschinensprachemonitor, die CP/M-Fähigkeit des Rechners sowie das verbesserte Diskettenlaufwerk VC1571. Schließlich lobt Zumbach noch die Programmierbarkeit der Funktionstasten sowie die bereits zum Zeitpunkt der Markteinführung zur Verfügung stehende „gigantische Palette an Software“. Weniger positiv äußert sich Stefan Grainer in der Fachzeitschrift \"c’t\". Der C128 sei zwar CP/M-fähig und komme in einem professionellen Design daher, bringe aber gegenüber dem Vorgänger keinen ernsthaften technologischen Fortschritt. So arbeite der Rechner im CP/M-Modus viel zu langsam. Gelobt wird dagegen das umfangreiche BASIC V7.0 mit stark vergrößertem Befehlsumfang und Befehlen zur strukturierten Programmierung. Neben dem vergleichsweise günstigen Preis werden auch das schnellere Diskettenlaufwerk VC1571 und die Fähigkeiten des Speicherverwaltungschips MMU gewürdigt.\n\nIm \"Computer Jahrbuch ’86\" wird der C128 einmal mehr als „Mittelding zwischen Heimcomputer und Bürocomputer“ beschrieben. In der gleichen Publikation wird der Rechner überdies neben dem 16-Bit-Computer Atari 520 ST zu den „spektakulären Neuvorstellungen des Jahres 1985“ gerechnet. Peter Niemann stellt den C128 in die Tradition der erfolgreichen Commodore-Heimcomputermodelle VC20 und C64 und macht die Stärken des Rechners in seinem günstigen Preis, seiner technischen Leistungsfähigkeit, geringen Größe, guten Grafikfähigkeit sowie seinem breiten Einsatzspektrum aus, das von Computerspielen bis zu ernsthaften Anwendungen wie Adressverwaltung oder Textverarbeitung reiche.\n\nAuf Kritik stießen ferner die als unangenehm empfundenen Betriebsgeräusche des im C128D verbauten Lüfters.\n\nZwar besitzt der C128 einen festen Platz im kollektiven Gedächtnis und wird in fast allen Überblicksdarstellungen zur Geschichte der Mikrocomputer erwähnt und in vielen Technikmuseen als Exponat ausgestellt. Trotzdem wird der Rechner aus der Retrospektive meist eher als Misserfolg gewertet, was insbesondere an den im Vergleich zum C64 deutlich geringeren Verkaufszahlen und diversen Designfehlern festgemacht wird. Einer der Gründe für das relative Scheitern des C128 wird darin gesehen, dass der Rechner gegenüber dem C64 keine wirkliche technische Verbesserung darstellte und dem Vorgängermodell einfach zu ähnlich gewesen sei. So weise der C128 lediglich eine reine 8-Bit-Architektur auf, obwohl zum Zeitpunkt der Markteinführung bereits klar war, dass die 8-Bit-Ära dem Ende entgegenging. Mit dem Intel 8088 habe jedoch zum Zeitpunkt der Entwicklung bereits ein kostengünstiger 16-Bit-Hauptprozessor zur Verfügung gestanden, der den Zweitprozessor Z80A hätte ersetzen und den Rechner IBM-PC-kompatibel hätte machen können. Schließlich hatte MS-DOS zu diesem Zeitpunkt CP/M als Standard-Betriebssystem im professionellen Bereich bereits abgelöst. So blieb der C128 trotz seines eleganten Designs, seiner vielen Schnittstellen und seines hochwertigen RGBI-Videosignals als eher langsamer Bürorechner weitgehend erfolglos. Auch die kurz nach dem C128 erfolgte Markteinführung des Amiga sowie des Atari ST wird für den relativ bescheidenen Markterfolg des Rechners ins Feld geführt.\n\nDie Tatsache, dass der Rechner nur im C64-Modus zum Vorgängermodell kompatibel ist, nicht aber im eigentlich innovativen und leistungsstärkeren C128-Modus, wird ebenfalls zu den Nachteilen des C128 gerechnet. Aufgrund der C64-Kompatibilität liefen alle für den Vorgänger programmierten Spiele auch auf dem Nachfolgemodell, weshalb es nur wenig Anreize für Drittanbieter gab, Spiele-Software eigens für den C128-Modus zu entwickeln. Für die Mehrheit der potenziellen Käufer, die sich vor allem für ein Spielgerät interessierte, war der C128 daher nicht wirklich attraktiver als der ohnehin kostengünstigere C64. So blieb die Programmbibliothek für den nativen C128-Modus sehr überschaubar. Neben einigen Anwendungsprogrammen sowie Programmiersprachen seien lediglich rund 20 Computerspiele gezielt für das Hauptbetriebssystem des Rechners entwickelt worden. Daher sei der C128 ganz überwiegend nur im C64-Modus verwendet worden, während der C128- sowie der CP/M-Modus eher selten betrieben worden seien.\n\nFerner sei CP/M zum Zeitpunkt der Markteinführung bereits „längst überholt“ oder gar „vollkommen veraltet“ gewesen, weshalb der C128 keine ernsthafte Konkurrenz für den IBM-PC mit seinem moderneren Standardbetriebssystem PC DOS dargestellt habe. Diese Wahrnehmung spiegelt sich auch in der Metaphorik zeitgenössischer Computerzeitschriften wider. Dort wurde CP/M schon zum Zeitpunkt der Markteinführung des C128 beispielsweise als „Großpapa der Betriebssysteme“ (englisch \"granddaddy of operating systems\") bezeichnet, der mit über zehn Jahren bereits ein „biblisches Alter“ erreicht habe. Die mangelhafte Dokumentation der C128-Portierung von CP/M-Plus etwa im Hinblick auf die für einen CP/M-Rechner ungewöhnlichen Grafik- und Soundfähigkeiten bereitete den Publishern obendrein Probleme bei der Entwicklung neuer CP/M-Software für den C128.\n\nIm Übrigen sei der Arbeitsspeicher von 128 kB gegenüber den seinerzeit üppigen 64 kB des drei Jahre älteren Vorgängermodells in der Preisklasse des C128 Mitte der 1980er Jahre nichts Besonderes mehr gewesen, sondern branchenüblicher Standard. Letztlich wurde der in die Jahre gekommenen 8-Bit-Technologie durch die komplexe Systemarchitektur des C128 zwar eine überdurchschnittliche, aber gegenüber dem C64 nicht wirklich herausragende Leistung abgerungen, für die man allerdings einen erheblich höheren Preis bezahlen musste.\n\nDer Technikjournalist Tony Smith wies dem C128 im Jahr 2013 auf der Neuigkeiten aus dem Technologiebereich präsentierenden Webseite \"The Register\" den dritten Platz unter den zehn attraktivsten Rechnern aller Zeiten zu, direkt hinter dem Apple MacBook Air und dem Apple Power Mac G4 Cube des britischen Stardesigners Jonathan Ive, aber noch vor Designklassikern wie dem unter der Mitwirkung von Steve Jobs entwickelten NeXTcube oder dem Supercomputer Cray-2. Als Sammlerobjekt ist der C128 auf dem Gebiet des Retrocomputing heutzutage tatsächlich immer noch populär, nicht zuletzt auch unter den zahlreichen C64-Enthusiasten, die den C128 aufgrund seiner technischen Überlegenheit sowie seines zuverlässigeren und leistungsstärkeren Netzteils schätzen. Beispielsweise können die an sich hardwarekompatiblen Speichererweiterungen 1700, 1750 sowie 1764 auf dem C64 mit dem schwächeren Originalnetzteil nicht betrieben werden, da sie 200 Milliampere mehr Leistung benötigen.\n\nDas ursprüngliche Tastaturcomputermodell von 1985 wird heutzutage immer noch regelmäßig auf Sammlerbörsen oder Internetauktionen wie eBay oder Craigslist angeboten. Während die ab 1986 erschienenen Desktop-Modelle in Europa ebenfalls relativ häufig zum Verkauf eingestellt werden, sind sie in Nordamerika aufgrund ihrer dort kürzeren Marktpräsenz etwas seltener. Den größten Seltenheitswert besitzt der nur relativ kurz hergestellte C128D mit Plastikgehäuse. Der heutige Wert eines C128-Modells schwankt jedoch stark und hängt neben der Zugehörigkeit zu einer Modellvariante vom Zustand, der Funktionsfähigkeit sowie dem Vorhandensein von Originalverpackung und Zubehör ab.\n\nIn der Retrogaming-Szene dagegen ist der C128 unter Spielern, die der Originalhardware gegenüber Emulatoren den Vorzug geben, etwas weniger begehrt. Einerseits liegt das am eher mageren Angebot an Spielesoftware für den C128-Modus, andererseits wird lieber auf den technisch weniger komplexen und preisgünstigeren C64 zurückgegriffen, um gelegentlich auftretende Kompatibilitätsprobleme beim Betrieb klassischer C64-Computerspiele zu vermeiden.\n\n\n\n\n\n\nOnline-Computermuseen\n"}
{"id": "11917", "url": "https://de.wikipedia.org/wiki?curid=11917", "title": "NTFS", "text": "NTFS\n\nNTFS ist ein proprietäres Dateisystem von Microsoft für alle Betriebssysteme der Windows-NT-Reihe. Die Abkürzung steht für New Technology File System.\n\nIm Vergleich zum Dateisystem FAT bietet NTFS unter anderem einen gezielten Zugriffsschutz auf Dateiebene sowie größere Datensicherheit durch Journaling. Allerdings ist keine so breite Kompatibilität gegeben wie bei FAT. Ein weiterer Vorteil von NTFS ist, dass die Dateigröße nicht wie bei FAT auf 4 GiB beschränkt ist. Größere Dateien werden beispielsweise beim Erstellen von DVD-Abbildern benötigt.\n\nAls die Entwicklung von Windows NT, dem späteren Microsoft Windows NT 3.1 begann, war noch nicht klar, welches Dateisystem das zukünftige Betriebssystem benutzen würde. Zu diesem Zeitpunkt existierten das Dateisystem FAT16, welches von MS-DOS verwendet wurde, und HPFS, das Dateisystem von OS/2. Das Dateisystem FAT war zu diesem Zeitpunkt bereits weit verbreitet, aber nach Ansicht von David Cutler erfüllten sowohl FAT als auch das fortschrittlichere HPFS nicht die Voraussetzungen an Zuverlässigkeit, die er an ein Dateisystem stellte. Das neue Dateisystem musste nach seiner Ansicht in der Lage sein, beschädigte Dateien automatisch wiederherzustellen. Zudem hatten beide Dateisysteme Beschränkungen in der maximalen Dateigröße und -anzahl, und es war zu erwarten, dass das neue Betriebssystem in Zukunft größere Datenmengen verwalten würde. Die Entwicklung eines dritten Dateisystems drohte allerdings den Zeitplan des Betriebssystems zu gefährden.\n\nSo begann zunächst die Spezifikationsphase des neuen Dateisystems. Unter FAT16 waren Dateinamen auf das 8.3-Format beschränkt. Diese Namen waren durch die erzwungene Kürze oft kryptisch und darüber hinaus schwer zu merken. HPFS unterstützte zwar Dateinamen, die bis zu 255 Zeichen lang sind, aber ältere DOS- oder Windows-Programme konnten solche Dateien nicht sehen. NTFS sollte dieses Problem lösen, indem jeder lange Dateiname automatisch eine Kurzform erhielt, durch die die Datei auch von älteren Anwendungen bearbeitet werden konnte.\n\nDie Entwicklung des neuen Dateisystems stand jedoch auf wackeligen Beinen. Der April 1991 hätte beinahe das Aus für NTFS bedeutet, als sich mehrere Entwickler dafür aussprachen, die Entwicklung dieses Dateisystems aus Zeitgründen zu beenden. Erst als Cutler, der sich zu dieser Zeit im Urlaub befand, zurückkehrte und die Wiederaufnahme der Entwicklung anordnete, gingen die Arbeiten weiter. Im Februar 1992 begann die Testphase des neuen Dateisystems, erst im Oktober 1992 war das Dateisystem stabil genug für eine Implementierung.\n\nNTFS erbte viele Konzepte des Dateisystems HPFS von IBM, das in dem anfangs zusammen mit Microsoft entwickelten Betriebssystem OS/2 verwendet wurde, geht aber in einigen Aspekten weit darüber hinaus.\n\nIm Gegensatz zu Inode-basierten Dateisystemen, welche bei Unix zum Einsatz kommen (Konzept: alles ist eine Datei), werden bei NTFS alle Informationen zu Dateien in einer Datei (Konzept: alles ist in einer Datei), der Master File Table, kurz MFT gespeichert.\nIn dieser Datei befinden sich die Einträge, welche Blöcke zu welcher Datei gehören, die Zugriffsberechtigungen und die Attribute. Zu den Eigenschaften (Attributen) einer Datei gehören unter NTFS Dateigröße, Datum der Dateierstellung, Datum der letzten Änderung, Freigabe, Dateityp und auch der eigentliche Dateiinhalt.\n\nSehr kleine Dateien und Verzeichnisse werden in der MFT direkt abgespeichert. Größere Dateien werden dann als Attribut in einem Datenlauf gespeichert. Es existieren 4 Stadien des Dateiwachstums.\n\nBeim Formatieren der Festplatte wird für die MFT ein fester Platz reserviert, der nicht von anderen Dateien belegt werden kann. Wenn dieser Bereich mit Informationen komplett gefüllt ist, beginnt das Dateisystem freien Speicher vom Datenträger zu benutzen, wodurch es zu einer Fragmentierung der MFT kommen kann. Standardmäßig wird ein Bereich von 12,5 % der Partitionsgröße für die MFT reserviert.\n\nBeim Speichern von Metadaten wird ein Journal geführt, was bedeutet, dass eine geplante Aktion zuerst in das Journal geschrieben wird. Erst dann wird der eigentliche Schreibzugriff auf die Daten ausgeführt, und abschließend wird das Journal aktualisiert. Wenn ein Schreibzugriff nicht vollständig beendet wird, zum Beispiel wegen eines Absturzes, braucht das Dateisystem nur die Änderungen im Journal zurückzunehmen und befindet sich anschließend wieder in einem konsistenten Zustand.\n\nDie folgende Liste spiegelt die Zuordnung zwischen NTFS- und Windows-Version wider:\n\n\n\nDie Unterschiede gegenüber FAT sind:\n\n\n\nAnalysepunkte (englisch auch \"\" genannt) stellen eine flexible Erweiterung für das Dateisystem dar, indem es Dateisystemeinträge mit Funktionen verknüpft. Diese können auf vielfältige Art verwendet – so etwa über den Befehl codice_1 verwaltet – und auch in zukünftigen Versionen erweitert werden. Ein Dateisystemtreiber, der eine bestimmte Art Analysepunkt nicht kennt, führt diesen nicht aus. Beim Zugriff auf einen Analysepunkt werden die funktionsspezifischen Analysedaten dynamisch durch die entsprechende Funktion ausgewertet (daher „Analyse“). Dies impliziert, dass eine solche Analyse auch fehlschlagen kann und ein Zugriff auf die durch den Analysepunkt bereitgestellten Daten (möglicherweise durch aktuelle, vorübergehende Umstände) nicht möglich ist.\n\nFolgende Funktionen werden derzeit von NTFS unterstützt:\n\nMit der Einführung von Windows Vista wurde das NTFS-Dateisystem um das Konzept atomarer Operationen (Transaktionen) erweitert. Dieses transaktionsbasierte NTFS (englisch \"Transactional NTFS\"; kurz: TxF) ermöglicht es Anwendungen, Dateioperationen atomar auszuführen. Veränderungen am Dateisystem werden also nur dann ausgeführt, wenn die gesamte Transaktion erfolgreich durchgeführt werden konnte. Zu einer Transaktion kann dabei eine Einzeloperation oder eine Abfolge von Dateioperationen gehören (beispielsweise das Erzeugen, Löschen oder Umbenennen einer oder mehrerer Dateien bzw. Verzeichnisse).\n\nTransactional NTFS wurde auf Basis des ebenfalls mit Windows Vista eingeführten \"Kernel Transaction Manager\" (KTM) implementiert, der Transaktionen auf der Ebene des Kernels ermöglicht. Es erweitert die bereits in vorigen NTFS-Versionen enthaltene Journal-Funktionalität, die sich auf die Integrität der Strukturen des Dateisystems beschränkt, um folgende Möglichkeiten:\n\n\nWindows unterstützt Transaktionen ab Windows Vista bzw. Windows Server 2008. Mittlerweile empfiehlt Microsoft allerdings den Einsatz von Alternativen, die API muss damit als \"deprecated\" betrachtet und von einem Einsatz abgeraten werden.\n\nJe nach Größe des Laufwerks werden folgende Standard-Clustergrößen vergeben:\n\n„nicht unterstützt (MBR)“ = Der Master Boot Record unterstützt nur Laufwerke bis 2 Tebibyte, darüber hinaus wird die GUID Partition Table verwendet, welche erst ab Windows 2000 und von Computern mit Extensible Firmware Interface unterstützt wird.\n\n\n\n\n\n\nDa es sich bei NTFS um ein proprietäres Dateisystem handelt, ist ein Zugriff durch andere Betriebssysteme als die der Windows-NT-Reihe unter Umständen nur in begrenztem Umfang möglich.\n\nFür DOS-basierte Betriebssysteme, zu denen auch die Betriebssysteme Windows-9x-Reihe zählen, existieren Treiber wie \"NTFS4DOS\", die einen vollständigen Zugriff auf NTFS-Laufwerke ermöglichen.\n\nLinux unterstützt über NTFS-3G vollständigen Lese- und Schreibzugriff, Lesezugriff auf verschlüsselte Dateien und kann Datenträger in NTFS formatieren. Weiterhin gibt es einen Kerneltreiber. macOS kann ab Version 10.3 NTFS-Dateisysteme lesen, aber nicht schreiben. In Version 10.6 (Snow Leopard) wurde eine versteckte Schreibfunktionalität gefunden, die aber nicht offiziell freigegeben ist.\n\nUnter Linux, macOS und FreeBSD kann außerdem über den User-Mode-Treiber NTFS-3G auch schreibend auf NTFS-Dateisysteme zugegriffen werden. Darüber hinaus stellt Paragon einen kommerziellen Treiber zur Verfügung.\n\n\n\n"}
