{"id": "179578", "url": "https://de.wikipedia.org/wiki?curid=179578", "title": "Texas Instruments TI-99/4A", "text": "Texas Instruments TI-99/4A\n\nBeim Texas Instruments TI-99/4A (kurz TI-99/4A, umgangssprachlich „Neunundneunziger“) handelt es sich um einen Heimcomputer des US-amerikanischen Technologiekonzerns und zu Beginn der 1980er-Jahre weltweit führenden Halbleiterherstellers Texas Instruments (TI). Der Rechner verfügt über einen für damalige Verhältnisse sehr leistungsstarken 16-Bit-Hauptprozessor, 16 Kilobyte Arbeitsspeicher (RAM), 26 Kilobyte Festspeicher (ROM) sowie Spezialbausteine für die Bild- und Tonausgabe. Er wurde erstmals im Rahmen der vom 31. Mai bis 3. Juni 1981 in Chicago abgehaltenen \"Summer Consumer Electronics Show\" der Weltöffentlichkeit vorgestellt.\n\nWie die meisten zeitgenössischen Heim- und Kleincomputer verfügt auch der TI-99/4A über einen Interpreter, mit dessen Hilfe der Rechner in einem herstellereigenen Dialekt der Programmiersprache BASIC bedient und programmiert werden kann. Gegenüber seinem Vorgängermodell TI-99/4 – dem ersten in Serie hergestellten 16-Bit-Heimcomputer – zeichnet sich der farb- und spritefähige TI-99/4A vor allem durch verbesserte Grafikfähigkeiten, eine komfortablere Tastatur und einen günstigeren Preis aus. Der mit diesen Leistungsmerkmalen intensiv beworbene Rechner war bei seiner Markteinführung 1981 in Nordamerika für 525 USD erhältlich, in Westdeutschland kurz darauf für 1490 DM.\n\nEnde 1982 errang der TI-99/4A vorübergehend die Marktführerschaft in seiner Geräteklasse. Die zunehmende Konkurrenz durch Hersteller wie Commodore, Atari und Sinclair, eine verfehlte Vermarktungsstrategie und überdurchschnittlich hohe Produktionskosten führten jedoch bereits ab Sommer 1982 zu stetig wachsenden finanziellen Verlusten beim Hersteller. Daraufhin verkündete TI am 28. Oktober 1983 seinen Rückzug vom Heimcomputermarkt und stellte die Produktion des TI-99/4A ein. Insgesamt wurden weltweit etwa 2,8 bis 3 Millionen Geräte verkauft. Damit gilt der TI-99/4A als erster 16-Bit-Rechner mit einer nennenswerten Verbreitung unter Privatanwendern. Aus dieser Tatsache leitet sich auch die besondere technikgeschichtliche Relevanz des TI-99/4A ab.\n\nDas Vorgängermodell TI-99/4 wurde bereits Ende 1979 zur Serienreife gebracht, konnte aber erst Anfang 1980 in größeren Stückzahlen geliefert werden. Zudem war der TI-99/4 zunächst nicht zum Betrieb mit handelsüblichen Fernsehern zugelassen, da TI nicht in der Lage war, die strengen Auflagen der US-amerikanischen Federal Communications Commission (FCC) zur Funkentstörung von HF-Modulatoren zu erfüllen. Der daraufhin nur im Paket mit einem teuren Zenith-Farbmonitor angebotene Rechner blieb mit einem Paketpreis von anfänglich 1150 USD in Nordamerika, der bis Herbst 1980 auf 1400 USD angehoben wurde, und mit 2700 DM in Westdeutschland für die meisten Privatanwender jedoch unerschwinglich.\n\nDarüber hinaus litt der TI-99/4 an technischen Mängeln. Hierzu zählte insbesondere die schwergängige, nicht alle Standardzeichen umfassende und daher für die Eingabe größerer Datenmengen ungeeignete Kaugummitastatur. Auch die Grafikfähigkeiten waren beschränkt. Beispielsweise kann der Rechner weder Bitmapgrafiken noch Kleinbuchstaben darstellen, was ihn für Textverarbeitungszwecke nahezu unbrauchbar machte. Weitere Produktionsverzögerungen, ein Ende 1980 lediglich 30 Titel umfassendes Softwareangebot, eingeschränkte BASIC-Programmiermöglichkeiten sowie teils abschätzige Kritiken in der Fachpresse verschafften dem TI-99/4 ein eher mäßiges Image.\n\nSchleppende Verkäufe brachten TI schließlich dazu, den Rechner einzeln zum deutlich geringeren Preis von 600 USD bzw. 1500 DM anzubieten – ohne Erfolg, der TI-99/4 blieb ein Ladenhüter. Die Verantwortlichen bei TI veranlassten daraufhin im Herbst 1980 eine gründliche Überarbeitung und Weiterentwicklung des Vorgängermodells zum TI-99/4A, um endlich einen konkurrenzfähigen Heimcomputer anbieten zu können.\n\nDer Leiter der Abteilung für Unterhaltungselektronik namens Peter Bonfield empfahl im Zuge der Überarbeitung des Vorgängermodells die Ersetzung der aus konzerneigener Produktion stammenden 16-Bit-CPU TMS9900 durch die im Ankauf 11 USD billigere 8-Bit-CPU Z80 von Zilog. Geschäftsführer Mark Shepperd wollte jedoch nicht auf den technisch rückständigen Mikroprozessor eines Fremdherstellers zurückgreifen. Außerdem erhoffte sich die Halbleiterabteilung von TI beim Verkauf des in der Produktion lediglich 2,25 USD teuren TMS9900 große, innerhalb des Konzerns verbleibende Gewinne. Im November 1980 wurde Bonfield durch Don Bynum ersetzt, der die technische Leitung der Überarbeitung des TI-99/4 (Codename \"„Ranger“\") übernahm und am TMS9900 festhielt.\n\nWeitgehend unverändert blieben auch Soundchip, I/O-Baustein und Systembus. Zwecks Reduktion der Produktionskosten auf 340 USD wurden allerdings höher integrierte Versionen der Schaltkreise verwendet. Der Expansionsport wurde zur Erweiterung seiner Funktionalität modifiziert.\nDurch das Weglassen der Taschenrechnerfunktion \"Equation Calculator\" wurde der umfangreiche Festspeicher um 5 KB abgespeckt. Auch der Arbeitsspeicher wurde um 256 Bytes verkleinert, wobei damit verbundene mögliche Probleme bei bereits existierenden Programmen in Kauf genommen wurden.\n\nDer entscheidende Unterschied zum Vorgängermodell bestand indessen in Verbesserungen am Grafikchip TMS9918, die das Darstellen von Kleinbuchstaben, Bitmapgrafik und den Betrieb mit SECAM- und PAL-Fernsehern ermöglichen. Im Gegensatz zum Vorgänger konnte somit das neue Modell auch auf Märkten außerhalb Nordamerikas angeboten werden. Der in der US-Version des neuen Rechners verbaute Grafikchip erhielt die Bezeichnung „TMS9918A“. Der angehängte, für engl. \"„advanced“\" (deut. „weiterentwickelt“) stehende Großbuchstabe „A“ wurde im Zuge der Suche nach einem geeigneten Namen für das Nachfolgemodell kurzerhand auch der Modellbezeichnung des TI-99/4 hinzugefügt.\n\nAuch äußerlich gab es Veränderungen: Auf Lautsprecher und Mikrofonanschluss wurde beim TI-99/4A verzichtet. Dafür besaß die Konsole nun eine Schreibmaschinentastatur mit zusätzlicher Funktionstaste und Autowiederholungsfunktion. Außerdem gab TI die Entwicklung neuer Peripheriegeräte in Auftrag. Durch den Nachweis der elektromagnetischen Verträglichkeit von Rechner und HF-Modulator gegenüber der FCC gelangte der TI-99/4A schließlich im Sommer 1981 zur Marktreife.\n\nAnders als im Fall der meist in den Billiglohnländern Südostasiens hergestellten, weltweit erfolgreichen Heimcomputer von Commodore und Atari blieb eine nennenswerte Verbreitung des in den texanischen Städten Lubbock, Abilene und Austin, im niederländischen Almelo sowie italienischen Rieti gefertigten TI-99/4A auf Nordamerika, Großbritannien, Westdeutschland, Frankreich, Italien und die Niederlande beschränkt. TI verfügte bei Markteinführung über bereits etablierte Vertriebsstrukturen in den Vereinigten Staaten und der TI-99/4A war sowohl in eigenen Filialen als auch unabhängigen Fachgeschäften, Kaufhäusern und Supermärkten erhältlich. Alternativ konnte der Rechner über den Versandhandel direkt ab Werk bezogen werden.\n\nFür einen TI-99/4A mussten die Zwischenhändler im Sommer 1981 ca. 340 USD bezahlen, während der tatsächliche Verkaufspreis mit 550 USD anfänglich etwas höher ausfiel als die unverbindliche Preisempfehlung von 525 USD. Die Wochenproduktion belief sich zu diesem Zeitpunkt auf knapp 8.000 Einheiten. Auf der Grundlage optimistischer Verkaufsprognosen veranlasste Vertriebsleiter William J. Turner bis Ende 1981 eine schrittweise Absenkung des Listenpreises auf zunächst 450 USD, dann 375 USD. Die Gewinnmarge konnte dabei durch Verringerung der Produktionskosten auf einem stabilen Niveau von 40 Prozent pro Rechner gehalten werden.\n\nIm Februar 1982 musste TI den TI-99/4A wegen schadhafter Netzteile vorübergehend vom Markt nehmen. Dem Konzern entstanden daraus finanzielle Schäden in Höhe von 50 Millionen USD. Zur Überwindung dieser Krise, d. h. zur Ankurbelung der Verkäufe, setzte Turner auf eine aggressivere Vermarktung und senkte daraufhin den Preis auf 300 USD, begleitet von einer Werbekampagne mit dem Slogan \"„TI's Home Computer. This is the one“\", für die der bekannte Komiker Bill Cosby verpflichtet und mit 1 Million USD pro Jahr entlohnt wurde. Auf Geheiß des im August 1982 zum Direktor der Abteilung für Unterhaltungselektronik beförderten Turner trat TI in einen offenen Preiskrieg mit dem Hauptkonkurrenten Commodore und dessen Heimcomputer VC20 ein. Turner hoffte dabei zu Recht auf Großbestellungen der wichtigsten Handelsketten wie J.C. Penney, Sears Roebuck, K-Mart oder Toys “R” Us. Rabatte von 100 USD verringerten den effektiven Verkaufspreis am 1. September 1982 auf unter 200 USD und nährten das Gerücht, TI sitze auf Lagerbeständen von bis zu 50.000 unverkauften Einheiten. TI-Werbepartner Cosby scherzte mit Blick auf diese Rabattaktionen für den TI-99/4A in der Öffentlichkeit darüber, wie einfach das Verkaufen eines Heimcomputers sei, wenn man den Kunden nur eine Belohnung von 100 USD dafür zahle.\n\nTurners aggressive Marketingstrategie führte im Herbst 1982 zu einer deutlichen Vergrößerung der Nachfrage und Ausweitung der Produktion. Auf jeden verkauften VC20 kamen zu diesem Zeitpunkt drei Exemplare des technisch überlegenen TI-99/4A. Die Monatsproduktion belief sich zwischen Juli und Dezember 1982 auf ca. 150.000 Einheiten, während das Vertriebsnetz nunmehr 12.000 Verkaufsstellen umfasste. Zu Spitzenzeiten wurden bis zu 5.000 Einheiten pro Tag hergestellt. Eine weitere Verminderung der Produktionskosten gelang allerdings nicht und die Gewinne pro verkauftem Rechner gingen um 50 Prozent zurück. Der Umsatz der Abteilung für Unterhaltungselektronik wuchs auf 200 Millionen USD und hatte sich damit innerhalb kurzer Zeit verzehnfacht. Insgesamt wurden 1982 rund 500.000 Exemplare abgesetzt. Mit 575.000 Benutzern und damit einem Marktanteil von rund 35 Prozent galt der TI-99/4A zum Jahreswechsel 1982/83 als der am weitesten verbreitete Heimcomputer in den Vereinigten Staaten.\n\nAuch im folgenden Jahr blieb die Nachfrage zunächst hoch. Wöchentlich rund 30.000 Einheiten wurden allein im Januar 1983 abgesetzt. Die Bestellungen der Händler blieben ebenfalls auf hohem Niveau. Im April 1983 erreichte die Zahl der verkauften Einheiten die Millionengrenze. In Europa jedoch konnte an diesen Erfolg nicht angeknüpft werden; in Westdeutschland gelang es TI, bis Ende 1983 lediglich einen Marktanteil von 8 Prozent zu erobern. Wolfgang Glöckle von der deutschen TI-Niederlassung gab daraufhin in einem Interview an, der Konzern habe nunmehr „den Durchbruch des Home-Computers auch in Deutschland geschafft.“\n\nInsbesondere mit dem Commodore 64 (C64) und Sinclair ZX Spectrum erwuchs dem TI-99/4A ab Sommer 1982 neue Konkurrenz. Daraufhin erwog TI die Entwicklung eines verbesserten TI-99/4A mit 64 KB Arbeitsspeicher und CP/M-Fähigkeit, um insbesondere mit dem leistungsstärkeren C64 gleichzuziehen. Nach dem Rücktritt von Chefentwickler Bynum im April 1983 wurden diese Pläne jedoch fallengelassen. Mit Blick auf den mittlerweile nur noch 99 USD teuren VC20 senkte TI ebenfalls den Preis für den TI-99/4A im Juni 1983 auf 150 USD und 550 DM. Damit lag der Verkaufspreis nur noch 25 USD über den Herstellungskosten in Höhe von 125 USD.\n\nDie Produktion lief unterdessen weiterhin auf Hochtouren. Die Verkaufsprognosen Turners sollten sich jedoch als illusorisch erweisen. Ab April 1983 wurden immer größere Stückzahlen von den US-amerikanischen Vertriebsstellen an die Konzernzentrale zurückgeschickt, da sie sich als unverkäuflich herausgestellt hatten. Turner wurde daraufhin seines Postens enthoben und im Mai durch J. Fred Bucy ersetzt, der die Zusammenarbeit mit Cosby beendete und eine neue Werbekampagne mit stärkerem Zuschnitt auf den Bildungsmarkt initiierte. Für den westdeutschen Markt wurde beispielsweise der Slogan „Mit dem lernen Sie spielend“ verwendet. Die Vertriebsleitung wurde Jerry Junkins übertragen, der daraufhin sofort mehrere Produktionsstätten schloss, eine erneute Überarbeitung der Elektronik anordnete und den Verkaufspreis auf 99 USD reduzierte. Angesichts des dramatischen Preisverfalls des TI-99/4A sprach Everett Purdy, stellvertretender Geschäftsführer der Handelskette Service Merchandise, in der angesehenen \"New York Times\" von einem in der Computer-Branche noch nie dagewesenen „Selbstzerstörungsmuster“ (engl. \"„self-destruct pattern“\"). Aus der Überarbeitung ging im Juni 1983 die beige Version des TI-99/4A ohne Aluminiumverkleidung und Statusanzeige hervor. Mit den zeitgleich auf den Markt gebrachten Rechnern der Atari-XL-Serie erhielt der TI-99/4A weitere ernstzunehmende Konkurrenz.\n\nBis August 1983 gelang eine Verringerung der Anzahl der verbauten integrierten Schaltkreise von 42 auf 35. Zudem erhielt der Rechner ein Netzteil mit verbessertem Wärmemanagement. Daneben wurde diese letzte, den inoffiziellen Namenszusatz „QI“ (engl. \"„quality improved“\") tragende Modellversion des TI-99/4A mit einem leicht modifizierten Betriebssystem versehen, das zum Versagen der ausschließlich mit EPROM-Chips bestückten Steckmodule von Atarisoft sowie einiger anderer ungeliebter Drittanbieter führte und damit die Verkäufe der TI-eigenen Programmveröffentlichungen fördern sollte. Im Rahmen dieser Vermarktungsbemühungen wurden auch die Preise für Peripheriegeräte um 50 Prozent gesenkt. An der nachlassenden Attraktivität des Rechners änderten diese Maßnahmen jedoch nichts – die Kunden wendeten sich verstärkt dem C64 zu.\n\nBereits im Juli hatte TI die Markteinführung des für das untere Marktsegment konzipierten TI-99/2 abgesagt. Auf Geheiß Bucys wurde auch die Entwicklung des als Nachfolger des TI-99/4A gedachten TI-99/8 (Codename \"„Armadillo“\") eingestellt, als im Herbst Gerüchte über einen Einstieg von IBM ins Heimcomputergeschäft und einen Low-End-Rechner von Apple aufkamen.\n\nSchon im Sommer 1983 belief sich der Schuldenstand der Abteilung für Unterhaltungselektronik auf 119 Millionen USD. Bis zum Jahresende stieg dieser Betrag auf fast 223 Millionen USD. Da der Preiskrieg mit Commodore den ganzen Konzern in die Insolvenz zu treiben drohte, gab die Konzernleitung mit Berufung auf die durch ausbleibende Nachbestellungen und volle Lagerhallen in die Höhe getriebenen Verluste am 28. Oktober den Rückzug aus dem Heimcomputermarkt bekannt. Zur Wahrung des Unternehmensimages wurde der Kundendienst noch über längere Zeit hinweg aufrechterhalten und auch die Produktion neuer Software angekündigt. Neugeräten wurden Briefe der Firmenleitung beigelegt, die einen Hinweis auf die Aufrechterhaltung der einjährigen Gewährleistung seitens TI sowie eine Hotline-Telefonnummer (codice_1) enthielten, über die mit der Produktionseinstellung des TI-99/4A verbundene Kundenfragen beantwortet wurden.\n\nZum Weihnachtsgeschäft 1983 erfolgte noch einmal eine vorübergehend den Verkaufserfolg des C64 schmälernde Preissenkung auf 50 USD.\nIn Großbritannien fiel der Preis auf 100 £ und damit auf das Niveau des einheimischen, technisch weniger leistungsfähigen und lediglich mit einer Folienflachtastatur ausgestatteten Billigrechners Sinclair ZX Spectrum 16K.\n\nIn Westdeutschland sank der Preis bis September 1983 auf 475 DM.\nUm zusätzliche Kaufanreize zu schaffen, verlängerte TI ab Oktober die Garantie auf Neugeräte von sechs Monaten auf ein ganzes Jahr.\nÜberdies wurde der Rechner weiterhin beworben. Anfang November kostete der TI-99/4A nur noch 398 DM bei weiter stark fallender Tendenz, was Vobis-Geschäftsführer Theo Lieven zu dem Kommentar veranlasste, „billiger und besser“ könne „man nicht in die Computertechnik einsteigen.“\n\nIn den letzten beiden Monaten des Jahres 1983 wurden weltweit ca. 150.000 Einheiten verkauft.\nPro verkauftem Rechner machten die Texaner dabei einen Verlust von nicht weniger als 50 USD. Im Januar 1984 erreichte die Zahl der insgesamt verkauften Exemplare die Marke von 2,5 Millionen.\n\nMit Beginn des Jahres 1984 setzte sich der Ausverkauf des TI-99/4A mit unverminderter Geschwindigkeit fort. In Westdeutschland stürzte der Preis bis auf 150 DM.\nTI-Deutschland verscherbelte im Zuge der Lagerabverkäufe für 298 DM sogar seine nicht mehr benötigten Heimcomputer-Messeverkaufsstände inklusive eines TI-99/4A sowie eines 17 Titel umfassenden Game-Bundles.\nWährend in den Vereinigten Staaten die Warenlager von TI sowie diversen Drittanbietern noch reichlich Peripheriegeräte, Zubehör und Software aufwiesen, kam es auf dem vergleichsweise kleinen westdeutschen Markt schon bald zu Versorgungsengpässen.\nDadurch entstand im deutschsprachigen Raum der kuriose Fall einer Angebots- und Preisanomalie: Die hohe Zahl der im Zuge der Abverkäufe zu Schleuderpreisen noch zu Besitzern des TI-99/4A gewordenen Kunden ließ die Nachfrage nach Software und Zubehör sprunghaft ansteigen. Da beides aufgrund der Produktionseinstellung im Herbst 1983 jedoch kaum noch erhältlich war, wurden für Gebrauchtware bald über denen für Neuware liegende Preise gezahlt.\n\nAm 28. März 1984 stellte TI den Vertrieb jeglicher mit dem TI-99/4A verbundener Produkte offiziell ein. Das Versandhaus Triton aus San Francisco übernahm die noch übrigen Lagerbestände.\nDie zu Schleuderpreisen erfolgten Lagerabverkäufe, die teilweise über das Jahr 1984 hinausgingen, brachten die Gesamtzahl der verkauften Einheiten in die Nähe der Drei-Millionen-Grenze.\nRund 150.000 Geräte entfielen dabei auf Westdeutschland, Österreich und die Schweiz.\nMit diesen Verkaufszahlen gilt der TI-99/4A als erster 16-Bit-Mikrocomputer mit einer nennenswerte Verbreitung unter Privatanwendern.\n\nAufgrund seines fortschrittlichen 16-Bit Hauptprozessors erfreute sich der TI-99/4A noch einige Jahre großer Beliebtheit, auch in Westdeutschland. Ende 1985 kostete ein gebrauchter, funktionsfähiger TI-99/4A dort im Durchschnitt ca. 130 DM.\n\nMit dem Desktop-Computer Geneve 9640 erschien Anfang 1987 für 998 DM ein inoffizieller, vom US-amerikanischen Hersteller Myarc produzierter Nachfolger des TI-99/4A.\n\nBeim Nachfolgemodell handelt es sich im engeren Sinne um einen technisch verbesserten Klon des TI-99/8-Prototypen. Der Geneve 9640 war mit einem zum TMS9900 zu 95 Prozent softwarekompatiblen 16-Bit-Hauptprozessor des Typs TMS9995, dem zum TMS9918A softwarekompatiblen, RGB-fähigen und die Darstellung von 512 Farben gestattenden Grafikchip Yamaha V9938, 512 KB RAM, 128 KB dediziertem VRAM und einem im Festspeicher residierenden, 64 KB ROM umfassenden Software-Emulator des Vorgängermodells ausgestattet, mit dessen Hilfe die Steckmodule des TI-99/4A auf dem Geneve 9640 verwendet werden konnten. Als Betriebssystem diente das eigens für den Rechner programmierte, grafische Benutzeroberflächen unterstützende MDOS (kurz für engl. \"Myarc Disk Operating System\"). Der zur Darstellung von 80 Zeichen pro Zeile fähige und mit einer Taktfrequenz von 12 MHz arbeitende Geneve 9640 verfügte außerdem über eine Echtzeituhr, Sprachausgabe, einen Mausanschluss, TI-99/4A-kompatible Joystickanschlüsse und eine abgesetzte, IBM-PC-kompatible Tastatur.\n\nDie Elektronik des TI-99/4A besteht im Wesentlichen aus einem Hauptprozessor, mehreren Spezialbausteinen, einem Arbeitsspeicher sowie einem Festspeicher. Diese Systemkomponenten sind auf einer Hauptplatine befestigt und über die Leiterbahnen des Systembusses miteinander verbunden. Von einigen Speicherchips abgesehen stammen sämtliche elektronischen Baugruppen aus hauseigener Produktion. Das entspricht der damaligen Unternehmensphilosophie von TI, die sich auf den bei der Entwicklung und dem Vertrieb von Taschenrechnern gemachten Erfahrungen gründete. Außerdem zählen Gehäuse, Tastatur, Schnittstellen und Netzteil zur Hardware des Rechners.\n\nMit dem TMS9900 verfügt der TI-99/4A über einen komplexen 16-Bit-Hauptprozessor mit DIP-Gehäuse und 64 Anschlusspins, der als „Quantensprung“ in der Geschichte der Mikroelektronik gilt. So war der TMS9900 der weltweit erste auf nur einem Chip realisierte 16-Bit-Mikroprozessor. Der 1976 zur Serienreife gebrachte TMS9900 gehört zur zweiten Generation der von TI entwickelten Mikroprozessoren und löste die erfolgreichen, meist zu Steuerungszwecken in elektronischen Geräten eingesetzten 4-Bit-Mikroprozessoren wie etwa den TMS1000 ab. Der TMS9900 kam nicht nur im Heimcomputerbereich, sondern auch in den hochpreisigen Minicomputern der TI-990-Serie zum Einsatz – etwa in den frühen Modellen TI-990/4 (1976) sowie TI-990/5 (1979). Neben dem zivilen Bereich wurde der TMS9900 auch im militärischen Bereich eingesetzt.\n\nDer TMS9900 ist mit einer als fest verdrahtetes elektronisches Rechenwerk fungierenden arithmetisch-logischen Einheit ausgestattet, die auf eine Verarbeitung von 16-Bit-Datenwörtern und die Berechnung von Adressen ausgerichtet ist. Die CPU ist ferner mit NMOS-Logik ausgestattet und kann mit Frequenzen von bis zu 3,3 MHz getaktet werden. Im TI-99/4A läuft der TMS9900 aber aus Gründen der Synchronisation mit dem Grafikchip nur auf 3 MHz. Generiert wird diese Taktfrequenz vom Taktbaustein TIM9904 bzw. dem baugleichen TIM9904A, der mit einem externen Schwingquarz verbunden ist und mit vier phasenverschobenen Taktsignalen in Form von Rechtecksignalen arbeitet. Diese werden über Transistor-Transistor-Logik aus einer Grundfrequenz von 40 MHz erzeugt. Der TMS9900 benötigt für die Ausführung eines Befehls 2–31 Mikrosekunden (µs). Daraus resultiert eine durchschnittliche Arbeitsgeschwindigkeit von etwa 0,3 Millionen Instruktionen pro Sekunde (MIPS). Die Anzahl der im TMS9900 realisierten Transistoren liegt bei rund 8.000.\nDer TMS9900 verfügt über einen Befehlssatz von 69 Instruktionen inklusive Multiplikation und Division. Dazu zählt auch der damals ungewöhnliche, bereits eine schrittweise Fehlersuche (engl. \"„Single-Step-Debugging“\") auf reiner Softwarebasis erlaubende codice_2-Sprungbefehl. Der Befehlssatz weist fünf funktionelle Gruppen auf: Befehle für den Datentransfer, arithmetische Befehle, logische Befehle, Prozessorsteuerbefehle und Programmsteuerbefehle. Die Befehlswörter des TMS9900 können 2–6 Bytes umfassen. Für Datentransfers und Speicherzugriffe besitzt die in Speicher-Speicher-Architektur ausgeführte CPU außerdem separate, über Speicherdirektzugriff sowie Memory Mapping das Verwalten eines Adressraums von 64 KB erlaubende 16-Bit-Busstrukturen. Darüber hinaus verwendet der TMS9900 drei interne 16-Bit-Hardwareregister für die schnelle Zwischenspeicherung von Daten. Dazu zählen der Programmzähler (PC), das Statusregister (ST) sowie der sogenannte \"„Workspace Pointer“\" (WP).\n\nDer im Deutschen auch als „Arbeitsbereichzeiger“ oder „Zeigerregister“ bezeichnete WP stellt insofern eine Besonderheit dar, als er seine Registerinhalte nicht auf der CPU selbst, sondern extern in einem besonderen Bereich des Arbeitsspeichers ablegt (engl. \"„Workspace“\"). Dieses CPU-RAM gestattet die Verwendung einer hohen Zahl von bis zu 16 Softwareregistern, zwischen denen ohne Datenverlust hin- und hergesprungen werden kann. Dazu zählen neben den Inhalten von PC, ST und WP die Basisadresse des CRU-Steuerbusses, die XOP-Adresse sowie elf frei verwendbare Register für Daten, Adressen oder einen Shift-Befehlszähler. Die Möglichkeit des Hin- und Herspringens zwischen den Softwareregistern erleichtert die Verarbeitung von Interrupts sowie den schnellen Kontextwechsel zwischen verschiedenen Registersätzen, also z. B. zwischen diversen Unterprogrammen. Der WP befähigt den Rechner damit prinzipiell sogar zum Multitasking. Erkauft wird dies allerdings mit einer geringfügigen Geschwindigkeitseinbuße, da beim Zugriff auf das CPU-RAM zunächst die entsprechende Speicheradresse vom WP übermittelt sowie ein Schreib-/Lese-Befehl vollzogen werden muss. Der TMS9900 bietet 17 Hardware- und 16 Software-Interrupts, also insgesamt 33 Interruptebenen.\n\nDer 8-Bit-Grafikchip des TI-99/4A wurde in drei verschiedenen Varianten gefertigt: TMS9918A sowie TMS9928A für das 525-Zeilen-Format des NTSC- bzw. SECAM-Standards und der TMS9929A für das 625-Zeilen-Format der PAL-Norm. Zum Betrieb des Rechners mit PAL-Fernsehern ist jedoch die Verwendung eines zusätzlichen, separaten HF-Modulators vonnöten.\n\nDer TMS9918A erreicht eine Maximalauflösung von 256 × 192 Pixeln, verfügt über eine Palette von 15 Farben (plus Transparenz) und ist in der Lage, bis zu 32 Sprites gleichzeitig darzustellen. Aufgrund dieser hohen Anzahl von Sprites und der damit verbundenen Fähigkeit zur Kollisionserkennung gehörte der TMS9918A seinerzeit zu den leistungsfähigsten Grafikchips. Die Größe und Auflösung der zusätzlichen Speicherplatz benötigenden Sprites kann variiert werden. Nativ möglich sind 8 × 8, 16 × 16 und 32 × 32 Bildpunkte jeweils in monochromer Darstellung. Durch Übereinanderlegen von Sprites in unterschiedlichen Farben können mehrfarbige Objekte mit Sprite-Eigenschaften generiert werden.\n\nDer mit 40 Anschlusspins ausgestattete Grafikchip erzeugt nicht nur das Videosignal, sondern verwaltet auch den für Speicherung, Abruf und Aktualisierung der Bildschirmdaten benötigten Grafikspeicher von bis zu 16 KB. Dazu zählt auch der für die bis zu 256 alphanumerischen Schriftzeichen, Satzzeichen und Grafiksymbole des frei programmierbaren Zeichensatzes benötigte Speicherplatz. Die voreingestellten alphanumerischen Schriftzeichen entsprechen den 95 druckbaren Zeichen des ASCII-Codes (Zeichencodes 32–127 der ASCII-Zeichentabelle). Die Grafiksymbole können zu einfachen Blockgrafiken kombiniert werden. Da für den Grafikspeicher ein Teil des Arbeitsspeichers verwendet wird, hängt die Größe des zur Verfügung stehenden Programmspeichers vom verwendeten Grafikmodus ab. Insgesamt vier Grafikmodi stehen je nach Bedarf zur Verfügung:\n\n\n\n\n\nGemäß dem Datenblatt liefern die drei analogen Farbausgänge des Chips die folgenden Pegel:\n\nBei der Farbe 9 \"light red\" ist das bemerkenswert, denn werden diese Farbdifferenzsignale auf RGB umgewandelt, dann ergäbe sich hier für den Rot-Kanal ein Wert von 113 %, was nicht sein kann. Der Wert R-Y dürfte maximal 80 % sein. Es handelt sich dabei jedoch nicht um einen Druckfehler im Datenblatt. Werden die Signale des Chips nämlich mit einem Oszilloskop nachgemessen, so ergibt sich, dass die Tabelle im Datenblatt vollständig richtig ist. Der Fehler steckt also tatsächlich bereits im Chip und treibt den Rot-Kanal bei einer nachfolgenden RGB-Wandlung somit in die Sättigung.\n\nWeiterhin ist zu beachten, dass der Chip zu einer Zeit produziert wurde, als zur Bildwiedergabe ausschließlich die Kathodenstrahlröhre zur Verfügung stand. Deren Wiedergabefunktion ist jedoch nicht linear. Deswegen sind die Werte mit einem Gamma-Korrekturfaktor versehen, der bei für Fernseher verwendeten Röhren bei 1,6 lag (zum Vergleich: bei Macintosh-Monitoren bei 1,8 und bei PC-Monitoren bei 2,2). Auf heutigen Displays würden die Farben somit blasser aussehen als wie sie im Original waren, wenn dieser Umstand nicht berücksichtigt wird. Die korrekten Werte für heutige Displays sind somit wie folgt (Werte bereits in das Hexadezimalsystem umgerechnet):\n\nDer TMS9919 stellt den für die Tonausgabe zuständigen 8-Bit-Soundchip des TI-99/4A dar. Der auch als \"„Complex Sound Generator“\" bezeichnete TMS9919 verfügt über drei individuell programmierbare Tongeneratoren und einen Rauschgenerator, die gleichzeitig vier Töne bzw. Geräusche auf 16 unterschiedlichen Lautstärkeniveaus erzeugen können. Die Abstände zwischen den jeweils wählbaren Lautstärkepegeln liegen bei 2 Dezibel, die maximale Lautstärke beträgt 28 Dezibel.\n\nDie drei Tongeneratoren arbeiten mit Rechteckschwingungen, der Rauschgenerator mit Pseudozufallsrauschen (engl. \"„periodic noise“\") und weißem Rauschen (engl. \"„white noise“\"). Die Tongeneratoren erzeugen hörbare Töne innerhalb eines Spektrums von fünf Oktaven, das von 110 Hertz bis 44 Kilohertz reicht. Die Tonlänge kann zwischen 1 Millisekunde und 4,25 Sekunden betragen. Der TMS9919 besitzt 16 Anschlussstifte und verwendet die DIN-Buchse auf der Rückseite für die Übermittlung des Audiosignals an den Lautsprecher des angeschlossenen Ausgabegerätes.\n\nBeim TMS9901 handelt es sich um einen multifunktionalen I/O-Baustein mit 22 Anschlusspins. Er unterstützt die CPU bei Ein- und Ausgabeoperationen, etwa bei Eingaben über die Tastatur, der Verwendung von externen Speichergeräten oder Joysticks. Ausgelesene Daten können über Speicherdirektzugriff an jede Stelle des Arbeitsspeichers weitergeleitet werden. Intern verfügt der TMS9901 über einen Prioritätsscheduler, einen Codierer, eine Echtzeituhr, ein Steuerwerk für die Kommunikation mit dem Steuerbus und drei Puffer für die Zwischenspeicherung von Daten.\n\nDer Arbeitsspeicher des TI-99/4A besteht aus acht 1-Bit-DRAM-Chips des Typs TMS4116 mit 16 Anschlusspins und einer Speicherkapazität von jeweils 2 KB. Hinzu kommen zwei nichtflüchtige 8-Bit-SRAM-Chips von Motorola mit jeweils 128 Byte Speichervolumen und 24 Anschlusspins. Sie werden auch als „Notizblockspeicher“ (engl. \"„Scratchpads“\") bezeichnet und dienen als CPU-RAM. Aufgrund ihrer hohen Zugriffsgeschwindigkeit gelten die SRAM-Chips als Schnellspeicher.\n\nDer Festspeicher des TI-99/4A besteht ausschließlich aus maskenprogrammierten ROM-Chips. Er weist zwei u. a. den Betriebssystemkern (engl. \"„System Monitor“\") sowie den Interpreter der sogenannten \"„Graphics Programming Language“\" (GPL) enthaltende 16-Bit-ROM-Chips mit 24 Anschlusspins und einem Speichervermögen von jeweils 4 KB auf. Außerdem besitzt der Rechner drei ladungsgekoppelte 8-Bit-GROM-Chips (engl. \"„Graphics Read-Only Memories“\") mit 16 Anschlusspins und einem Speichervolumen von jeweils 6 KB. Diese ausschließlich von TI produzierten Festspeicherchips dienen primär zur Aufnahme von in der GPL geschriebenen Unterprogrammen und sind in Memory-Map-Technik ausgeführt. Die GROM-Chips verfügen neben einem 8-Bit-Datenbus über einen Nur-Lese-Speicher, der mit Hilfe von maskenprogrammierter Firmware einen bordeigenen Befehlszähler (engl. \"„program counter“\") emuliert. Dieser übernimmt intern anstelle des entsprechenden CPU-Registers die Aufgabe des Setzens und Zählens von Speicheradressen.\n\nErmöglicht wird dadurch der Verzicht auf das ansonsten bei ROM-Chips übliche zentrale Auslesen der Speicherzellen durch die CPU über Speicherdirektzugriff. Stattdessen setzt der Adresszeiger des bordeigenen Befehlszählers zunächst einmalig eine bestimmte Adresse, ab der dann fortlaufend Speicherinhalte ausgelesen werden. Abgelegt werden diese Speicherinhalte in einem lokalen Pufferspeicher zur weiteren Verwendung durch die CPU. Nach jeder Leseoperation wird der Adresszähler des GROM-Chips automatisch ohne die Notwendigkeit eines neuen Setzens der Adresse erhöht. Auf diese Weise kann relativ schnell auf ein großes Datenvolumen über nur einen Eingang zugegriffen werden. TI ermöglichte das den Verzicht auf eine Verwendung der damals üblichen, aber teuren Hochgeschwindigkeits-ROMs. Daher ließ sich TI die Erfindung des automatisch hochzählenden, in die Firmware des GROM-Chips eingebauten Adresszählers (engl. \"„auto-incrementing memory“\") eigens patentieren.\n\nDie 64 KB Adressraum sind in acht vom Betriebssystem für unterschiedliche, vorabdefinierte Aufgabenbereiche reservierte Blöcke mit jeweils 8 KB unterteilt. Das Betriebssystem-ROM ist u. a. für die Steuereinheit des Diskettenlaufwerks, die RS232-Schnittstellen des PES sowie die Druckersteuerung reserviert. Es wird gelegentlich auch als „Konsolen-ROM“ bezeichnet. Das Gerätetreiber-ROM (engl. \"„DSR-ROM“\" für \"„Device Service Routines“\") ist insofern für damalige Verhältnisse ungewöhnlich, als es die Verwendung von Peripheriegeräten ohne Inanspruchnahme des Arbeitsspeichers oder Veränderungen am Rechner gestattet. Es wird bisweilen auch als „Peripherie-ROM“ bezeichnet.\n\nDie Rechnerarchitektur des TI-99/4A unterscheidet sich wesentlich von der anderer zeitgenössischer Heimcomputer, denn sie stellt eine Mischform aus klassischer 8-Bit-Architektur (8-Bit-Datenbusbreite für Spezialbausteine und RAM) und im Heimcomputerbereich damals noch nicht üblicher 16-Bit-Architektur (16-Bit-Hauptprozessor, 16-Bit-Datenbusbreite für SRAM und ROM) dar. Der Hauptprozessor kommuniziert mit den verschiedenen elektronischen Bausteinen über die Datenleitungen des Systembusses, der aus den drei Komponenten Datenbus, Adressbus und Steuerbus besteht.\n\nDer Datenbus dient der Übertragung von Daten zwischen den einzelnen Systemkomponenten. Mit den beiden SRAM-Chips sowie den beiden ROM-Chips ist nur ein kleiner Teil der Systemkomponenten über einen bidirektionalen 16-Bit-breiten Datenbus direkt mit dem Hauptprozessor verbunden. Jenseits dieses Kernbereichs sorgt ein als Busconverter fungierender Multiplexer für eine Reduktion der Datenbusbreite auf 8 Bit. Auf diese Weise können alle 8-Bit-Systemkomponenten wie Grafikchip, Soundchip oder GROM-Chips von der CPU mit der entsprechenden Wortbreite angesteuert werden. Durch diese Serialisierung wird jedoch die Ausführungsgeschwindigkeit des Gesamtsystems im Vergleich zu Rechnern mit reiner 16-Bit-Architektur deutlich verringert.\n\nExpansionsport und Modulschacht sind für den Hauptprozessor ebenfalls nur über den langsameren 8-Bit-Bereich des Datenbusses erreichbar. Hinzu kommt eine weitere Einschränkung bei den DRAM-Chips: Auf die gerade nicht für Videosignal und Bildwiederholung verwendeten Bereiche des Arbeitsspeichers kann die CPU bei der Ausführung von Programmen in TI BASIC oder Maschinensprache nur auf dem zeitraubenden Umweg über den 8-Bit-Grafikchip zurückgreifen.\n\nDer Adressbus überträgt unidirektional Speicheradressen zwischen Hauptprozessor und Speicherchips zwecks Weitergabe der Information, welche Speicherzelle als nächste ausgelesen oder beschrieben werden soll. Die CPU legt dabei die gewünschte Adresse vor dem Versenden als Binärmuster auf dem Adressbus ab, der mit der für 8-Bit-Architekturen typischen Busbreite von 16 Bit arbeitet.\n\nDie verschiedenen Systemkomponenten werden mit variabler Wortbreite vom Adressbus angesteuert. Die 16 Adressleitungen des Expansionsports ermöglichen der CPU das Verwalten eines auf max. 48 KB RAM erweiterten Arbeitsspeichers sowie von Peripheriegeräten mit bis zu 16 KB Gerätetreiber-ROM. Die am Modulschacht anliegenden 13 Adressleitungen gestatten den Betrieb von Steckmodulen mit einem Adressraum von 8 KB. Dieser zusätzliche Speicher kann wahlweise von ROM- oder RAM-Chips geliefert werden. Weit häufiger wurden jedoch GROM-Chips verwendet, deren Speicher über einen Adressdecoder (engl. \"„Memory Address Decoder“\") gemanagt wird. Mit zwölf Adressleitungen sind die 4 KB der beiden 16-Bit-ROM-Chips abgedeckt. Für die 256 Bytes der beiden SRAM-Chips reichen gar acht Adressleitungen.\n\nEine Besonderheit besteht bei den GROM-Chips. Obwohl es sich um Speicherchips handelt, sind sie nicht über eigene Leiterbahnen mit dem Adressbus verbunden. Stattdessen ist der Adressdecoder eingangsseitig mit den sechs höherwertigen Bits des Adressbusses verknüpft und teilt den beiden Koprozessoren für Grafik und Sound sowie den GROM-Chips über Chipselect-Signale mit, wer an den gerade anlaufenden Speicheroperationen teilnimmt.\n\nDas Entwicklerteam von TI bezeichnete den unidirektionalen Steuerbus des TI-99/4A als \"„Communications Register Unit“\" (CRU). Dieses synchrone 1-Bit-Schieberegister dient der CPU zur Steuerung sowohl interner als auch externer Systemkomponenten über serielle Datenübertragung. Zu diesem Zweck werden Steuerinformationen Bit für Bit an die entsprechenden Systemkomponenten gesendet, etwa um die Datenflussrichtung auf dem Systembus zu regeln. Neben der Lese-Schreib-Steuerung werden auch Interrupts und Buszugriffe vom Steuerbus aus geregelt. Mit Hilfe der Statusleitung können zwecks Prüfung der Betriebsbereitschaft einzelne Statusbits an jede einzelne Systemkomponente gesendet werden. Systemkomponenten können auf diese Weise auch aktiviert oder deaktiviert werden. Darüber hinaus ist die CRU mit der Aufgabe der Synchronisierung von Rechner und Peripheriegeräten betraut, was über Halte-, Unterbrechungs- und Quittungssignale bewerkstelligt wird.\n\nDer TMS9900 besitzt drei eigens für die Verwendung der CRU konstruierte Leiterbahnen mit eigenen Anschlusspins: CRUIN zum Auslesen von Speicherzellen, CRUOUT zum Versenden von Daten sowie CRUCLK zum Einschreiben von Daten. Zusätzlich werden noch zwölf Leiterbahnen des Adressbusses für den Steuerbus in Anspruch genommen.\n\nDie Urversion des TI-99/4A besitzt ein rechteckiges Kunststoffgehäuse „im Metallic-Look mit schwarzer Tastatur“, das auf der Oberseite mit gebürstetem Aluminium verkleidet und im futuristischen Space-Age-Design gehalten ist. Die ins Gehäuse eingelassenen Schlitze wurden vom Design des TI-99/4 übernommen. Bei diesem befand sich dahinter ein Lautsprecher, der beim TI-99/4A weggelassen wurde. Deswegen dienen sie bei ihm als Lüftungsschlitze zur Kühlung der Elektronik. Außerdem besitzt der Rechner einen Hauptschalter, eine Statusanzeige sowie einen Schacht zur Aufnahme von Steckmodulen, jedoch keine Resettaste. Der Rechner wiegt 2,3 kg ohne Netzteil und misst 25,9 cm × 38,1 cm × 7,1 cm (Länge × Breite × Höhe). Der unter der freien Fläche vor dem Modulschacht befindliche Leistungsregler neigt bei Dauerbetrieb zu recht hohen Temperaturen. Dieser Teil des Gehäuses wurde deshalb scherzhaft als „Kaffeetassenwärmer“ bezeichnet.\nDie mechanische QWERTY-Schreibmaschinentastatur des TI-99/4A weist 48 alphanumerische Tasten sowie ein weit von heutigen Standards entferntes Layout auf. Eine Version mit deutscher Tastaturbelegung gibt es nicht. Die Tasten des Hauptblocks sind in fünf Reihen angeordnet. Lediglich eine rechts neben der Leertaste zu findende Funktionstaste gehört zur Ausstattung. Abgesehen von der sehr breiten Leertaste und der rechten Shifttaste besitzen alle weiteren Sondertasten dieselbe Größe wie die einfachen alphanumerischen Tasten. Ein Ziffernblock zur Eingabe größerer Zahlenmengen fehlt ebenso wie eine Tabulatortaste im Hauptblock. Dafür wartet der Rechner mit einer feststellbaren Umschaltsperre auf.\n\nDie Funktionstaste dient nicht den heute üblichen Funktionen wie Hilfe, Suchen oder Löschen, sondern ebenso wie die Steuerungstaste der Mehrfachbelegung einzelner Tasten. Während die Buchstabentasten meist doppelt belegt sind, weisen die numerischen Tasten fast alle sogar Dreifachbelegungen auf. Zwecks Erleichterung der Bedienung sind die Mehrfachbelegungen auf einer Tastaturschablone oberhalb des Tastenfeldes verzeichnet. Die wichtigsten Editierfunktionen sowie einige häufig verwendete Befehle des TI BASIC lassen sich durch gleichzeitiges Betätigen der Funktionstaste und bestimmter Zifferntasten aktivieren. Die Pfeiltasten sind insofern ungewöhnlich, als sie nicht in einem abgesetzten Cursorblock liegen, sondern ebenfalls nur über doppelbelegte Buchstabentasten im Hauptblock aktivierbar sind. Über einen 15-poligen Pfostenstecker und ein entsprechendes Kabelbündel ist die Tastatur mit der Hauptplatine verbunden.\nDer TI-99/4A verfügt über sechs Schnittstellen. Auf der linken Seite befindet sich eine neunpolige Sub-D-Buchse, mit deren Hilfe Joysticks, Paddles oder vergleichbare digitale Steuergeräte angeschlossen werden können. Trotz ihrer äußerlichen Ähnlichkeit ist die Belegung der neun Pole jedoch nicht mit dem damals von den Atari-Joysticks gesetzten Standard kompatibel. Im Gegensatz zu den meisten Heimcomputern gibt es nur einen Joystickanschluss. Auf der rechten Seite befindet sich der Expansionsport. Dabei handelt es sich um einen ins Gehäuse eingelassenen Platinenstecker mit 44 Kontakten, der im unbenutzten Zustand mit einem Schutzdeckel versiegelt wird. Der Expansionsport ermöglicht eine direkte Verbindung mit dem Systembus. So lassen sich Diskettenlaufwerke, Drucker und Modems, aber auch Speichererweiterungen usw. an den Rechner anschließen.\n\nAuf der Rückseite findet sich links eine weitere neunpolige Sub-D-Buchse, die auf den Anschluss handelsüblicher Kassettenrekorder ausgerichtet ist. Rechts neben der Kassettenschnittstelle befindet sich ein vierpoliger Anschluss für das Netzteil. Auf der rechten Seite weist der Rechner eine fünfpolige (NTSC) bzw. sechspolige DIN-Buchse (PAL/SECAM) auf. Mittels dieser Buchse kann der Rechner mit einem Monitor, über einen zusätzlichen HF-Modulator aber auch mit einem Fernsehgerät betrieben werden. Das Audiosignal wird ebenfalls über die DIN-Buchse ausgegeben. Der Modulschacht weist 18 zur Aufnahme der in den Steckmodulen verwendeten Platinenstecker gedachte Kontakte auf.\n\n<gallery class=\"centered\" caption=\"Schnittstellen an den Seiten und der Rückseite des TI-99/4A\" widths=\"150\" heights=\"120\" perrow=\"4\">\n</gallery>\n\nNeben den von TI speziell für den TI-99/4A entwickelten Peripheriegeräten lassen sich auch die sogenannten \"„Sidecars“\" (deut. „Seitenwagen“ oder auch „Beiwagen“) des Vorgängermodells TI-99/4 verwenden. Darüber hinaus existieren weitere Zusätze von Fremdherstellern wie A/D Electronics, Axiom, Boxcar Peripherals, CorComp, Doryt Systems, Horizon, ISC, Millers Graphics, Myarc, Navarone, Newport Controls, Percom Data sowie Triton, die teilweise auch erst nach der Produktionseinstellung des TI-99/4A im Jahr 1983 ausgeliefert wurden.\n\nDer Herstellungszeitpunkt und -ort sämtlicher in den frühen 1980er-Jahren hergestellter TI-Produkte lässt sich anhand ihrer Seriennummern feststellen: Diese bestehen jeweils aus einer Zahl mit bis zu sechs Stellen gefolgt von einer Kombination aus drei Buchstaben mit einer vierstelligen Zahl. Die Buchstaben codice_3 bezeichnen dabei das TI-Zweigwerk in Abilene, codice_4 steht für Austin, codice_5 für Lubbock, codice_6 für Almelo (Niederlande) und codice_7 für Rieti (Italien). Die ersten beiden Ziffern der sich anschließenden vierstelligen Zahl beziehen sich auf die Kalenderwoche des durch die letzten beiden Ziffern gekennzeichneten zugehörigen Produktionsjahres.\n\nDie \"Sidecars\" stellten sich bereits bei der Nutzung mit dem TI-99/4 aufgrund ihres hohen Platzbedarfs und der Fülle an Kabeln auf dem Schreibtisch als unpraktisch heraus. Als Alternative entwickelte TI daraufhin das auf der \"Winter Consumer Electronics Show 1982\" vorgestellte \"„Peripheral Expansion System“\" (PES) mit der Typennummer PHP1200. Das Gerät mit eigener Stromversorgung verfügt über acht Steckplätze für die Erweiterungskarten der anzuschließenden Peripheriegeräte, einen Schacht zur Unterbringung von bis zu zwei 5¼-Zoll-Diskettenlaufwerken einfacher Bauhöhe, einen Ventilator zur Kühlung sowie einer mitgelieferten Schnittstellenkarte zum Anschluss an den Computer. Das PES kam in zwei fast identischen, jeweils kompatiblen Versionen in einem stabilen Metallgehäuse auf den amerikanischen Markt. Für die europäischen Absatzgebiete und die dort üblichen Netzspannungen wurden entsprechend angepasste Varianten produziert. Die deutsche Version war im September 1983 im Paket mit dem TI-99/4A für 1.500 DM erhältlich. Insgesamt wurden 250.000 Exemplare des rund 250 USD teuren PES abgesetzt.\n\nDie damals in dieser Form neuartigen Erweiterungskarten besitzen solide Metall- bzw. Kunststoffgehäuse nebst Statusanzeige und verfügen auf der Unterseite über einen 30-poligen Platinenstecker, über den die Verbindung mit dem PES hergestellt wird. Sie funktionieren ähnlich unkompliziert wie heutige Plug-and-Play-Karten und können ohne vorherige Treiberinstallation sofort nach dem Einstecken verwendet werden. Die folgende Auflistung liefert eine Übersicht der von TI produzierten Erweiterungen:\n\nDie seinerzeit für 174,95 USD angebotene RS232-Schnittstellenkarte besitzt zwei Anschlussbuchsen: Eine nach dem namengebenden RS232-Standard ausgeführte Buchse mit 25 Anschlusspins und zwei seriellen Schnittstellen zur Verbindung mit RS232C-kompatiblen Peripheriegeräten und eine als parallele Schnittstelle ausgeführte Buchse mit 16 Anschlusspins. Die Treibersoftware zur Umsetzung der Übertragungsprotokolle ist in einem 4-KB-ROM-Chip auf der Platine der RS232-Schnittstellenkarte untergebracht und gestattet über entsprechende TI-Extended-BASIC-Befehle eine Steuerung der Datenübertragung sowohl zwischen lokalen als auch fernvernetzten Rechnern.\n\nAn die RS232-Buchse können mit Hilfe eines sogenannten Y-Kabels maximal zwei Peripheriegeräte gleichzeitig angeschlossen werden, wobei die softwareseitig einstellbaren Datenübertragungsraten von 110, 300, 600, 1.200, 2.400, 4.800 und 9.600 Baud relativ gering sind. Höhere Datenübertragungsraten lassen sich über die parallele, jedoch nicht zum damals weit verbreiteten Centronics-Standard pinkompatible 8-Bit-Schnittstelle realisieren. Bei Benutzung eines entsprechenden Adapterkabels können beispielsweise mit Centronics-Schnittstelle ausgestattete Drucker, Plotter und Terminals von Fremdherstellern angeschlossen werden. Mit dem PES lassen sich maximal zwei RS232-Schnittstellenkarten gleichzeitig betreiben.\n\nBei der Laufwerksteuerungskarte handelt es sich um eine mit einem Floppy-Disk-Controller des Typs FD1771 von Western Digital sowie einem 8-KB-ROM-Chip ausgestattete Steuereinheit zum Verwalten von bis zu drei 5¼-Zoll-Diskettenlaufwerken. Der Floppy-Disk-Controller führt alle Diskettenoperationen aus und übernimmt die Steuerung der Motoren und magnetischen Schreib-Lese-Köpfe der angeschlossenen Laufwerksmechaniken. Der Festspeicher enthält die hierfür benötigten vier Gerätetreiberroutinen. Darüber hinaus verwaltet die Steuereinheit auch die Disketten-Inhaltsverzeichnisse mit ihren indizierten Dateien. Abgelegt werden die Inhaltsverzeichnisse in den Sektoren 0 und 1 der ersten Spur.\n\nDer Betrieb der Laufwerksteuerungskarte ist nur mit dem achten Steckplatz, der sich direkt neben dem Laufwerksschacht befindet, möglich. Die Verbindung mit der Laufwerksmechanik erfolgt über ein entsprechendes Kabel, das mit dem 34-poligen Anschluss auf der Rückseite verbunden wird. Weitere zwei Laufwerke können im Daisy-Chain-Verfahren an das erste Diskettenlaufwerk angeschlossen werden. Für eine Laufwerksteuerungskarte inklusive des für die Inbetriebnahme unverzichtbaren Steckmoduls mit dem Diskettenbetriebssystem \"Disk Manager\" mussten seinerzeit rund 300 USD investiert werden.\n\nDas seinerzeit rund 400 USD teure 5¼-Zoll-Diskettenlaufwerk besitzt auf der Vorderseite ein Disketteneinschubfach nebst Klappverschluss sowie eine Statusanzeige. Auf der Rückseite befindet sich ein Kabel zwecks Anschluss an die interne Stromversorgung des PES sowie ein weiteres 34-poliges Kabel für die Verbindung mit der Laufwerksteuerungskarte.\n\nDie Laufwerksmechanik verwendet einen magnetischen Schreib-Lese-Kopf mit einer mittleren Zugriffszeit von 463 Millisekunden. Das Laufwerk gestattet das Abspeichern eines Datenvolumens von bis zu 89 KB auf einer Diskettenseite (engl. \"„Single Sided“\") in einfacher Dichte (engl. \"„Single Density“\"). Pro Diskettenseite werden dabei 40 Spuren mit jeweils neun Sektoren verwendet.\n\nDie 32-KB-RAM-Speicherkarte weist 16 vom Unternehmen Mostek stammende 1-Bit-DRAM-Chips des Typs MK4116 mit 16 Anschlusspins und einer Kapazität von jeweils 2 KB auf. Die zusätzlichen DRAM-Chips vergrößern den frei programmierbaren Arbeitsspeicher des TI-99/4A auf 48 KB RAM. Damit erreicht der Rechner in Bezug auf die Speicherkapazität seine höchste Ausbaustufe. Mittels des \"Mini Memory\"-Steckmoduls können dem System allerdings noch weitere 4 KB SRAM hinzugefügt werden, die den Arbeitsspeicher insgesamt sogar auf 52 KB bringen.\n\nDer zusätzliche Arbeitsspeicher ist mit dem Datenbus des PES über acht Datenleitungen verbunden. Wie beim ab Werk eingebauten Arbeitsspeicher können Daten also nur mit 8-Bit-Wortbreite in die Speicherzellen der Erweiterungskarte eingeschrieben oder dort ausgelesen werden. Die Speicherkarte besitzt zwecks Hardwarefehlerlokalisierung eine automatische Selbsttestfunktion und wird zur Inbetriebnahme einfach in einen der Steckplätze des PES eingesteckt, bevor der Rechner eingeschaltet wird. Der Neupreis lag seinerzeit bei 300 USD.\n\nMit Hilfe der P-Code-Interpreterkarte kann die Compiler-Hochsprache UCSD-Pascal mit dem TI-99/4A betrieben werden. Statt einer tatsächlichen besitzt die P-Code-Interpreterkarte lediglich eine virtuelle CPU mit eigenem hardwareunabhängigen Befehlssatz, die sogenannte „Pseudo-Maschine“, und einen aus einem 2-KB-ROM-Chip, einem 4-KB-ROM-Chip sowie acht 6-KB-GROM-Chips bestehenden Festspeicher von insgesamt . Dieser enthält neben der Software-Emulation der auf keinem tatsächlich verwendeten Mikroprozessor basierenden hypothetischen CPU einen komfortablen P-Code-Interpreter, der für andere Systeme entwickelte UCSD-Pascal-Software verarbeiten kann.\n\nDie P-Code-Interpreterkarte kann nur verwendet werden, sofern neben einer 32-KB-RAM-Speichererweiterung auch ein Diskettenlaufwerk oder Kassettenrekorder an den Rechner angeschlossen ist. Mit Hilfe eines Schalters auf der Rückseite kann sie vor Inbetriebnahme des Rechners aktiviert werden. Bei aktivierter Karte wird nach dem Einschalten innerhalb von 30–60 Sekunden zunächst der P-Code-Interpreter initialisiert. Danach wird der Befehlsmodus des P-Code-Interpreters ausgeführt.\n\nIn Ergänzung zur P-Code-Interpreterkarte wurde 1982 ein UCSD-Pascal-Softwarepaket für 499,95 USD auf den Markt gebracht. Es besteht aus folgenden Komponenten:\n\nDer TI-99/4A konnte mit handelsüblichen Kassettenrekordern betrieben werden; TI brachte aber trotzdem einen speziell auf den Rechner zugeschnittenen Programmrekorder (engl. \"„TI Program Recorder“\") mit einer Datenübertragungsrate von 450 Baud heraus. Das Gerät mit der Typennummer PHP2700 verfügt über alle üblichen Eigenschaften eines Kassettenrekorders, ist aber für zusätzlich für den Gebrauch als Speichergerät optimiert. Der Preis lag bei 70 USD.\n\nDer Programmrekorder ist auf Kompaktkassetten des Formats C60 mit 30 Minuten Abspielzeit pro Seite zugeschnitten und wurde in zwei an das Design der beiden Versionen des TI-99/4A angepassten Varianten angeboten. Er verfügt neben Tasten für Aufnahme, Abspielen, Rückwärts- und Vorwärtsspulen, Anhalten und Auswerfen über zwei Drehregler für Lautstärke (engl. \"„Volume Control“\") und Klang (engl. \"„Tone Control“\"), einen eingebauten Lautsprecher, ein serienmäßiges Mikrofon sowie eine Pausetaste. Darüber hinaus ist der Programmrekorder mit einem Zählwerk ausgestattet und besitzt drei Anschlüsse für eine Verbindung mit der Konsole (engl. \"„Ear Phone Jack“\", \"„Mic Jack“\" und \"„Remote Jack“\"). Die Stromversorgung erfolgt intern über vier Babyzellen mit insgesamt 6 Volt oder extern wahlweise über einen Gleichstromadapter (engl. \"„DC Adapter“\") oder das Stromnetz (engl. \"„AC Input“\").\n\nDas Gerät war für seine Zuverlässigkeit, aber auch seine Langsamkeit bekannt. Beide Eigenschaften ergeben sich aus dem vom Kansas-City-Standard abweichenden redundanten Aufzeichnungsverfahren. Sämtliche Datensätze werden dabei gleich zweimal aufgenommen und überdies Prüfsummenbytes zur Erkennung von Datenübertragungsfehlern verwendet. Zum Einlesen der Daten werden vom Timer des TMS9901 die genauen Längen der Halbwellen des Audiosignals vermessen und in für den Computer verständlichen Binärcode übertragen. Eine lange Halbwelle (689,37 Hz) bedeutet dabei eine Null, zwei kurze Halbwellen (1379 Hz) stehen dagegen für eine Eins. Beim Aufnehmen einer neuen Datei wird das Magnetband zunächst einige Sekunden vorgespult und dann ein Dauerton mit konstanter Frequenz aufgezeichnet.\n\nDer aus alledem resultierende typische „TI-Sound“ war jedem Benutzer wohlvertraut und schaffte es sogar in eine Fernsehserie. Der Autor des Buches \"Programme für den TI-99/4A\" Rainer Heigenmoser arbeitete als technischer Berater in der deutschsprachigen Fernsehserie Der Bastard (1989) mit. Darin wird an einer Stelle ein Faxgerät gezeigt. Der während der Faxübertragung gespielte Sound stammt jedoch nicht von einer Faxübertragung, sondern von einem auf Kassette speichernden TI-99/4A.\n\nIm Gegensatz zu den damals vorherrschenden Speichermedien wie Kompaktkassette oder Diskette entfallen bei den Steckmodulen (engl. \"„Solid State Software Cartridges“\" bzw. \"„Command Modules“\") durch die Verwendung von Nur-Lese-Speichern die lästigen Ladezeiten. Allerdings können die Steckmodule im Gegensatz zu diesen Datenträgern nicht kopiert und nur bei Verwendung von EPROM- oder batteriegepufferten RAM-Chips beschrieben werden. Aufgrund der relativ hohen Produktionskosten von ca. 6 USD pro Einheit waren die Steckmodule überdies relativ teuer.\n\nDie von rechteckigen Kunststoffgehäusen geschützten Steckmodulplatinen besitzen einen Platinenstecker mit 18 Kontakten und enthalten meist einen 6-KB-GROM-Chip. Dieser wird im Gegensatz zur herkömmlichen Praxis nicht in den vergleichsweise kleinen Arbeitsspeicher des TI-99/4A kopiert, sondern als zusätzliche Speicherbank verwendet. Die sogenannten „Multimodule“ besitzen mehrere GROM-Chips mit weiteren Programmen, zwischen denen mit Hilfe des Adressdecoders gewählt werden kann. Insgesamt können bis zu 30 KB GROM hinzugefügt werden. Daher befinden sich auf den Steckmodulplatinen fünf Steckplätze. Darüber hinaus besteht die Möglichkeit des Hinzufügens von bis zu 8 KB EPROM bzw. 4 KB RAM. TI schloss mit einer Reihe von Drittanbietern wie Imagic, Milton Bradley, Walt Disney oder Addison Wesley Verträge ab, die diesen Unternehmen die Entwicklung eigener Steckmodul-Software gestattete, wobei die Herstellung der patentierten GROM-Chips und der Vertrieb der fertigen Steckmodule in den Händen von TI verblieben.\n\nTI produzierte eigens für den TI-99/4A einen an das Design des PES angepassten 10-Zoll-Farbmonitor (engl. \"„TI Color Monitor“\") mit einer Maximalauflösung von 720 × 300 Pixeln. Das Gerät mit der Typennummer PHA4100A verfügt über eine eigene Stromversorgung sowie zahlreiche Regler, etwa zur Einstellung von Farbintensität, Kontrast oder Helligkeit. Es wurde in jeweils eigenen Versionen für die Standards NTSC, PAL und SECAM hergestellt. Es kostete rund 400 USD.\n\nTI entwickelte bereits 1979 ein Sprachmodul (engl. \"„Solid State Speech Synthesizer“\") für den Vorgänger TI-99/4, das den Rechner bei einem Preis von rund 100 USD mit der Fähigkeit zur künstlichen Sprachausgabe ausstattete und auch mit dem TI-99/4A verwendet werden kann. Die Sprachsynthese war eine Spezialität von TI und wurde in einer eigens hierfür gegründeten Abteilung erforscht. Allerdings steckte sie damals noch in den Kinderschuhen.\n\nIm Sprachmodul verbaut ist ein 8-Bit-Sprachchip des Typs TMS5200, der zur zweiten Generation der von TI entwickelten Spezialchips für Sprachsynthese gehört. Der TMS5200 besitzt einen Puffer für Sprachdaten sowie 4-Bit-Steuerbusstrukturen. Daneben verfügt das Sprachmodul über zwei speziell entwickelte 16-KB-Sprach-ROM-Chips des Typs TMS6100 mit hochkomprimierten Sprachdateien. Diese setzen sich aus immer wieder abrufbaren und somit speichersparenden digitalen Repräsentationen stimmhafter wie stimmloser Phoneme zusammen, was von den TI-Entwicklern als „\"Linear Predictive Coding\"“ bezeichnet wurde. Vom Sprachchip können diese Sprachdateien über direkten seriellen Zugriff eingelesen werden. Der Sprachchip simuliert dabei ein Filtermodell des Vokaltraktes und speist dieses mit den eingelesenen Daten, um eine synthetische Wellenform zu generieren. Der Output dieses Filtermodells durchläuft einen Digital-Analog-Umsetzer, der abschließend als Audiosignal verwendet und an die Tonausgabe des Rechners weitergeleitet wird. Die Sprach-ROM-Chips verfügen zudem über 373 vorprogrammierte, vom TI BASIC aus direkt abrufbare Wörter, die zu einfachen Sätzen miteinander kombiniert werden können. Einige Arcadespiele wie \"Parsec\" machen zwecks Schaffung einer realistischen Spielatmosphäre von den Fähigkeiten des Sprachmoduls Gebrauch.\n\nTI brachte im Herbst 1982 für ca. 750 USD einen Schwarzweiß-Matrixdrucker (engl. \"„TI 99/4 Impact Printer“\") mit der Typennummer PHP2500 auf den Markt. Dabei handelt es sich um einen das TI-Logo tragenden Standarddrucker des Typs Epson MX80. Das Gerät beherrscht vier Schrifttypen und druckt wahlweise 40, 66, 80 oder 132 Zeichen pro Zeile bei einer Druckgeschwindigkeit von 80 Zeichen pro Sekunde. Grafiken können wahlweise in zwei unterschiedlichen Auflösungen zu Papier gebracht werden: 480 Pixel pro Zeile (engl. \"„normal density“\") oder 960 Pixel pro Zeile (engl. \"„dual density“\"). Auf der Oberseite finden sich Bedientasten für Blattvorschub, Zeilenvorschub und Direktdruck. Auf der Rückseite weist das Gerät eine serielle RS-232-Buchse sowie eine parallele Schnittstelle auf.\n\nTI produzierte duale Joysticks (engl. \"„Wired Remote Controllers“\") mit der Typennummer PHP1100 für den TI-99/4A, die ohne Adapter an keinen anderen Rechner angeschlossen werden konnten. Zwei im Paket angebotene Joysticks wurden dabei über ein gemeinsames Kabel mit dem Joystickanschluss des Rechners verbunden und ihre Signale fortlaufend unter Inkaufnahme verminderter Reaktionszeiten im stetigen Wechsel vom I/O-Baustein abgefragt. Diese Steuergeräte besitzen jeweils einen Steuerknüppel mit acht möglichen Einstellungen und einen breiten Feuerknopf. Sie kosteten rund 35 USD.\n\nFür die Datenfernübertragung entwickelte TI bereits 1979 eigens ein als Akustikkoppler ausgeführtes Modem (engl. \"„TI Telephone Coupler“\"), das Daten mit einer Geschwindigkeit von 300 Baud übertragen kann. Das Gerät mit der Typennummer PHP1600 besitzt einen Stromanschluss und zwei Schiebeschalter zum Ein-/Ausschalten, Initialisieren des Testlaufs sowie Einstellen der Datenübermittlungsweise. Möglich sind Wechselbetrieb (engl. \"„Half-duplex“\") sowie Gegenbetrieb (engl. \"„Full-duplex“\"). Zum Betrieb muss über eine der RS-232-Schnittstellen eine Verbindung zum Rechner hergestellt werden. Der Akustikkoppler kostete seinerzeit ca. 200 USD.\n\n<gallery class=\"centered\" caption=\"Peripheriegeräte des TI-99/4 und TI-99/4A (Auswahl)\" widths=\"155\" heights=\"120\" perrow=\"4\">\n</gallery>\n\nFür den TI-99/4A waren Ende 1983 etwa 800 verschiedene Programmtitel auf verschiedenen Datenträgern erhältlich, darunter Programmiersprachen, Anwendungssoftware, Lernsoftware und Computerspiele. Der Großteil dieser Programme (ca. 700) wurde von Lizenznehmern bereitgestellt, der Rest stammt von Texas Instruments selbst. Da nur ungefähr jeder zehnte Besitzer des TI-99/4A das teure PES mit dem dazu passenden Diskettenlaufwerk erwarb, wurde die Software hauptsächlich auf Steckmodulen veröffentlicht. Auch nach der offiziellen Produktionseinstellung wurden noch einige Jahre neue Spiele für den Rechner veröffentlicht, beispielsweise von Atarisoft.\n\nDie gesamte zum Betrieb des TI-99/4A benötigte Systemsoftware nebst BASIC-Interpreter befindet sich auf im Gerät verbautem Festwertspeicher und ist deshalb ohne Booten direkt nach dem Einschalten einsatzbereit.\n\nDie Konfiguration der Hardware des TI-99/4A sowie des eingebauten TI BASIC übernimmt das aus dem für die Daten-, Geräte- und Prozessverwaltung verantwortlichen Betriebssystemkern sowie zahlreichen Systemroutinen bestehende Betriebssystem. Dazu zählen die Initialisierungsroutine nach dem Einschalten (engl. \"„power up“\") und verschiedene auf den GROM-Chips untergebrachte mathematische Funktionen. Die ROM-Chips enthalten die Systemprogramme zur Ausführung von Interrupts für Bildschirmaufbau, Tastaturabfrage und Betrieb von Peripheriegeräten, zur Steuerung der Kassettenschnittstelle sowie verschiedene Hilfsroutinen, beispielsweise zur Berechnung von Fließkommazahlen. Nach dem Einschalten des Rechners werden sämtliche Einsprungpunkte (Zeiger) und alle vorhandenen GROM-Bausteine initialisiert, der Gerätetreiber für den Kassettenrekorder konfiguriert und danach der Startbildschirm nebst Startmenü erzeugt.\n\nZum Betrieb des TI-99/4A mit 5¼-Zoll-Diskettenlaufwerken wurde das nicht zum Lieferumfang der Diskettenlaufwerke gehörende und auf Steckmodul ausgelieferte Diskettenbetriebssystem \"Disk Manager\" entwickelt. Mit Hilfe dieses Diskettenbetriebssystems lassen sich Disketten formatieren und Dateien verwalten (speichern, löschen, kopieren und umbenennen). Pro Diskettenseite lassen sich dabei bis zu 127 Dateien unterbringen. Weiterhin ist es möglich, Dateien mit einem Schreibschutz zu versehen und Funktionstests für die Diskettenlaufwerke durchzuführen.\n\nEine spätere Version der Diskettensystemsoftware, die im März 1983 unter dem Titel \"Disk Manager 2\" herausgebracht wurde, gestattet die Benutzung beider Diskettenseiten ohne manuelles Wenden des Datenträgers. Zum Ausschöpfen dieser Möglichkeit musste der Anwender über entsprechende Geräte von Drittherstellern verfügen, da TI selbst keine Laufwerke mit der benötigten Anzahl von zwei Schreib-Lese-Köpfen anbot.\n\nTI BASIC fungiert sowohl als Benutzerschnittstelle als auch als Programmierumgebung und verfügt über 82 Befehle, Anweisungen, Funktionen und Variablen. Wird es im Startmenü angewählt, erscheint auf dem Bildschirm die Einschaltmeldung codice_8 sowie der auf Eingaben wartende Prompt des Befehlsmodus (engl. \"„Command Mode“\"). Daneben kennt TI BASIC noch den Programmiermodus (engl. \"„Edit Mode“\") sowie den Programmausführungsmodus (engl. \"„Run Mode“\").\n\nDurch Betätigen der Entertaste wird der Interpreter zur Ausführung von Befehlen veranlasst. Der Programmiermodus lässt sich durch Verwendung von Zeilennummern am Anfang der Kommandozeile aktivieren. Mit Hilfe der Pfeiltasten kann der Cursor an jede beliebige Stelle des Bildschirms manövriert werden. Die Programmausführung wird durch Eingabe des codice_9-Befehls eingeleitet. Laufende Programme können durch Drücken der Breaktaste angehalten werden. Der Rechner befindet sich dann wieder im Befehlsmodus. Verlassen werden kann TI BASIC entweder durch den codice_10-Befehl, der den Programmspeicher unwiederbringlich löscht, oder den codice_11-Befehl, der das spätere Wiederaufrufen von Programmspeicherinhalten gestattet. Beide Befehle führen den Anwender wieder zum Startbildschirm.\n\nTI-BASIC-Programme können nur in den nicht als Grafikspeicher genutzten Bereichen des Arbeitsspeichers abgelegt werden. Der Kern des TI-BASIC-Interpreters liegt im ROM-Speicherbereich von codice_12 bis codice_13. Außerdem enthalten die ROM-Chips eine Sprungtabelle für die in den GROM-Chips befindlichen TI-BASIC-Routinen. Zusammen besitzen TI-BASIC-Interpreter und -Routinen ein Festspeichervolumen von 14 KB.\n\nBei der \"Graphics Programming Language\" (GPL) handelt es sich um eine von TI entwickelte höhere Anweisungssprache mit einem Befehlsvorrat von insgesamt 59 Instruktionen. Die Hauptaufgabe der GPL besteht im Bereitstellen einer professionellen Programmierumgebung zur Ausnutzung sämtlicher, in TI BASIC teils nicht zugänglicher Hardwareeigenschaften des bordeigenen Chipsatzes. So können mit Hilfe der GPL etwa hochauflösende Bitmapgrafiken programmiert und die Klangerzeugungsmöglichkeiten des Soundchips TMS9919 vollumfänglich ausgeschöpft werden.\n\nWeniger komfortabel als TI BASIC, aber benutzerfreundlicher als Assemblersprache, verwendet die über einen speziellen Puffer Direktzugriffe auf den Grafikspeicher zulassende GPL viele mit dem Befehlssatz des TMS9900 identische Befehle. Sie ist daher als „sehr prozessornahe Zwischensprache“ erheblich schneller bei der Ausführung von Programmen als der TI-BASIC-Interpreter. Allerdings kommt diese Eigenschaft bei alltäglichen Anwendungen kaum zum Tragen, denn die im Benutzerhandbuch des TI-99/4A unerwähnt bleibende GPL, für die es auf dem freien Markt kein von TI autorisiertes Programmierhandbuch gab, war nicht als Benutzerschnittstelle vorgesehen. In der Basiskonfiguration kann der Rechner ausschließlich mit über Kommandozeilen eingegebenen TI-BASIC-Befehlen bedient werden.\n\nTrotz seiner Prozessornähe kann der insgesamt 12 KB ROM umfassende GPL-Code nicht unmittelbar vom TMS9900 ausgeführt werden, sondern nur mittels des eingebauten GPL-Interpreters. Dieser belegt den ROM-Speicherbereich von codice_14 bis codice_15. Der GPL-Interpreter ist in Assemblersprache programmiert. Um das Anfertigen von Raubkopien und die Produktion unautorisierter Software durch Drittanbieter zu unterbinden, kopiert er GPL-Unterprogramme vor der Ausführung nicht in den frei zugänglichen Arbeitsspeicher, sondern führt sie speichersparend und vor unbefugtem Zugriff geschützt direkt im GROM aus.\n\nDie Grenzen zwischen GPL- und TI-BASIC-Interpreter sind fließend, da einzelne GPL-Befehle wie etwa codice_16, codice_17 oder codice_18 nur für den BASIC-Interpreter, nicht aber den Hauptprozessor verständlich sind. Da TI BASIC ausschließlich im GPL-Code programmiert ist und BASIC-Programme vor der Ausführung mit hohem Zeitaufwand sowohl vom TI-BASIC- als auch vom GPL-Interpreter verarbeitet werden müssen, ist der BASIC-Dialekt des TI-99/4A im Vergleich zu denen anderer Heimcomputer eher langsam.\n\nSchon kurz nach Markteinführung des TI-99/4A erkannte TI die Langsamkeit des doppelt interpretierten TI BASIC als Problem. Noch im Sommer 1981 wurde daher die BASIC-Erweiterung TI Extended BASIC herausgebracht. In Westdeutschland war sie erst ab 1984 erhältlich und wurde in Lizenz von Mechatronic in Sindelfingen vertrieben.\n\nDas weitgehend abwärtskompatible TI Extended BASIC wartet mit einer Reihe zusätzlicher Fähigkeiten und einem gegenüber der Basisversion um 35 Befehle, Anweisungen, Funktionen, Subroutinen und logische Operatoren erweiterten Befehlssatz auf. So besitzt es eine Autoboot-Funktion, gestattet die Verwendung von Unterprogrammen in Maschinensprache und die Darstellung von bis zu 28 Sprites. Strings können bis zu 154 Zeichen enthalten, Variablen bis zu 15 Zeichen lang sein. Darüber hinaus erlaubt TI Extended BASIC eine recht komfortable Fehlerbehandlung, erhöht die Zahl der für Felder (engl. \"„Arrays“\") zur Verfügung stehenden Dimensionen von drei auf sieben und stellt sogar Befehle für Kopierschutzmaßnahmen zur Verfügung. Außerdem können mehrere Befehle speichersparend in einer einzigen Programmzeile eingegeben werden. Da der Großteil des TI Extended BASIC statt im GPL-Code in Maschinensprache geschrieben ist, beschleunigt sich die Ausführung von Programmen merklich. Die Geschwindigkeitszunahme liegt ungefähr beim Doppelten des TI BASIC. Aufgrund dieser Eigenschaften wurde das TI Extended BASIC vom Fachbuchautor Rainer Heigenmoser auch mit einem Luxusmodell des britischen Automobilherstellers „Rolls Royce“ verglichen, während das ursprüngliche TI BASIC den Autoren eher an einen Kleinwagen wie den „VW-Käfer“ erinnerte.\n\nMit 32 KB ROM ist TI Extended BASIC ausgesprochen umfangreich und belegt weitere 2 KB des Arbeitsspeichers. Damit stehen für Grafik- und Programmspeicher nur noch 14 KB zur Verfügung, was zu einer spürbaren Einschränkung der Programmiermöglichkeiten führt. Für den Betrieb ist eine Speichererweiterung aber dennoch nicht zwingend erforderlich, sofern auf speichersparende Programmiertechniken geachtet wird.\n\n\"TI FORTH\" ist ein von TI entwickelter Dialekt der stackbasierten, assemblernahen und daher schnellen Compiler-Hochsprache Forth. Neben einem Betriebssystem stellt \"TI FORTH\" eine diskettenbasierte Entwicklungsumgebung mit 64 Zeichen pro Zeile, hochauflösender Bitmapgrafik und Interruptroutinen zur Verfügung. \"TI FORTH\" benötigt neben einer 32-KB-RAM-Speichererweiterung zusätzlich das \"Editor/Assembler\"-Steckmodul. Eine weitere Forth-Version wurde von Wycove Systems entwickelt.\n\n\"TI LOGO\" sowie die mit einem erweiterten Befehlssatz, Druckerfunktionen und zusätzlichen Grafikfähigkeiten aufwartende Fortsetzung \"TI LOGO II\" sind ebenfalls von TI fabrizierte Dialekte der gleichnamigen funktionalen Interpreter-Hochsprache. Sie dienen der Vermittlung von mathematischen, logischen und kommunikativen Fähigkeiten an Kinder sowie deren Übung im Umgang mit Computern. Zum Betrieb mit Kompaktkassette, Diskette oder Steckmodul ist eine 32-KB-RAM-Speichererweiterung notwendig. Eine stark vereinfachte Schnupperversion mit begrenzten Programmiermöglichkeiten namens \"Early Learning LOGO Fun\" konnte auch ohne Speicherausbau betrieben werden.\n\nAuch \"TI PILOT\" ist ein von TI realisierter Ableger der gleichnamigen Interpreter-Hochsprache und ermöglicht die Entwicklung von Übungen, Tests und interaktiven Lernprogrammen für computergestütztes Lernen. Diese Programmiersprache kann nur mit 32-KB-RAM-Speichererweiterung, Diskettenlaufwerk und P-Code-Interpreterkarte betrieben werden.\n\nEine optimale Ausnutzung der Hardware des TI-99/4A ist nur durch die Verwendung von Assemblersprache nebst Übersetzungsprogramm (engl. \"„Assembler“\") möglich, das die Programmanweisungen des Quelltextes (engl. \"„Sourcecode“\") in Maschinensprache überführt. TI bot ein entsprechendes Softwarepaket namens \"Editor/Assembler\" an, das ein Steckmodul, zwei Disketten und ein umfangreiches Bedienungshandbuch umfasste. Die Software enthielt neben dem Editor und dem Assembler noch einen Debugger zur Beseitigung von Programmierfehlern.\n\nProgramme in Assemblersprache sind wesentlich schneller als solche in höheren Programmiersprachen und bieten gegenüber der noch schnelleren Maschinensprache den Vorteil, dass sich ihr Befehlsvorrat durch die Verwendung von verständlichen und leicht erinnerbaren Abkürzungen (engl. \"„Mnemonics“\") leichter handhaben lässt. Allerdings bevorzugten die meisten Programmiereinsteiger die zwar leistungsschwächeren, aber einfacher zu bedienenden höheren Programmiersprachen wie etwa Pascal oder BASIC.\n\nFür den TI-99/4A wurden einige Anwendungsprogramme aufgelegt, von denen viele jedoch nur mit einer 32-KB-RAM-Speichererweiterung und einem Diskettenlaufwerk betrieben werden können. Das gilt für Dateiverwaltungsprogramme wie \"Personal Report Generator\" und \"Personal Tax Plan\" ebenso wie für das Textverarbeitungsprogramm \"TI Writer\" oder das Tabellenkalkulationsprogramm \" Microsoft Multiplan\".\n\nZu den beliebtesten Steckmodulen zählte die Speichererweiterung \"Mini Memory\", die zusätzlich Hilfsprogramme wie etwa einen Maschinensprachemonitor enthält. \"Mini Memory\" stattet den Rechner mit 14 KB Zusatzspeicher aus, von denen 6 KB auf das GROM und 4 KB auf das einfache ROM entfallen. Darüber hinaus ist es mit 4 KB batteriegepuffertem SRAM bestückt. Kürzere BASIC- und Maschinenspracheprogramme können so ohne weitere Speichergeräte direkt auf dem Modul gesichert werden. Alternativ können aber auch TI-BASIC-Unterprogramme sowie ein Fehlersuchprogramm gestartet werden. Bei Verwendung einer 32-KB-RAM-Speicherkarte erlaubt \"Mini Memory\" direkten Zugriff auf deren zusätzliches RAM.\n\nEbenfalls große Popularität genoss das ebenfalls auf Steckmodul veröffentlichte Telekommunikationsprogramm \"Terminal Emulator II\", das eine Vernetzung des Rechners über einen Akustikkoppler ermöglicht. Darüber hinaus erweitert die Telekommunikationssoftware die Einsatzmöglichkeiten des Sprachmoduls um zusätzliche Funktionen.\n\nZu den beliebtesten Lernprogrammen für den TI-99/4A gehörte die \"Miliken Home Math Series\" mit Titeln, die sich etwa der Vermittlung der Grundrechenarten, der Prozentrechnung oder den Dezimalbrüchen widmeten. Auch Addison Wesley setzte auf Lernsoftware zum Thema Mathematik und brachte die Steckmodule der \"Computer Math Games\"-Serie heraus. Das Minnesota Educational Computing Consortium entwickelte Lernprogramme für geistes-, sozial- und naturwissenschaftliche Disziplinen. Die Control Data Corporation brachte für Highschool-Absolventen aller Altersstufen und Fachrichtungen die \"Plato Courseware\"-Serie heraus. Der auf Grundschüler spezialisierte Verlag Scott Foresman veröffentlichte Lernprogramme mit künstlicher Sprachausgabe, die vor allem auf eine Verbesserung der Lesekompetenz abzielten.\n\nTI selbst konzentrierte sich auf die Rechtschreibung und brachte eine sechs Teile umfassende Serie mit dem Titel \"Scholastic Spelling\", ein damals futuristisch wirkendes Programm für künstliche Sprachausgabe namens \"Text-to-Speech\" und ein Übungsprogramm namens \"Touch Typing Tutor\" für das Erlernen des Zehnfingersystems heraus. Eine Mischung aus Arcadespiel und Lernprogramm stellt der grafisch aufwändige Titel \"Microsurgeon\" mit seiner ungewöhnlichen Spielmechanik dar.\n\nDie rund 40 auf Steckmodulen erschienenen Arcadespiele bildeten das populärste Spielegenre für den TI-99/4A. Zu den beliebtesten Arcadespielen, die in der Regel zwischen 11 und 45 USD kosteten, gehörten von TI selbst produzierte Titel wie \"Alpiner\", \"The Attack\", \"Blasto\", \"Car Wars\", \"Chisholm Trail\", \"Choplifter\", \"M*A*S*H\", \"Munchman\", \"TI Invaders\", \"TI Trek\" sowie \"Tombstone City\". Das Shoot ’em up \"Parsec\" aus dem Jahr 1982 gilt als bestes Spiel oder gar „Killerapplikation“ für den Rechner. Einige besonders gefragte Titel wurden für knapp 20 USD zusätzlich auch auf Diskette veröffentlicht, setzten aber neben einem Diskettenlaufwerk die 32-KB-RAM-Speichererweiterung voraus.\n\nZu den erfolgreichsten Spielen von Drittanbietern gehörten portierte Titel wie \"Dig-Dug\", \"Donkey Kong\", \"Jungle Hunt\", \"Moon Patrol\", \"Pac-Man\" und \"Pole Position\" von Atarisoft (mit alternativem Gehäusedesign), \"Q-Bert\" von Parker Brothers, \"Buck Rogers\" und \"Star Trek\" von SEGA, \"Space Bandits\" von Milton Bradley, der \"Frogger\"-Clone \"Princess & Frog\" von Romox sowie \"Super Demon Attack\" von Imagic.\n\nDas erfolgreichste Strategiespiel war \"Hunt the Wumpus\". An Brettspielen standen \"Backgammon\", \"Blackjack and Poker\" sowie \"Video Chess\" zur Verfügung. Sportfans konnten sich mit Titeln wie \"Football\" oder \"Indoor Soccer\" vergnügen. Als erste Flugsimulation erschien \"Dow-4 Gazelle\" von John T. Dow. Mit \"Bankroll\" wurde von Not Polyoptics außerdem eine Wirtschaftssimulation veröffentlicht. Aus demselben Haus stammt auch die erst 1987 veröffentlichte Doppeldecker-Luftkampfsimulation \"Spad XIII\", die die erste echte 3D-Flugsimulation für den TI-99/4A darstellt.\n\nAdventures stellten ebenfalls ein beliebtes Spielegenre dar. Vor allem die auf einer Kompaktkassette bzw. Diskette Platz findenden Titel von Scott Adams wie \"Ghost Town\", \"Mystery Fun House\" oder \"Voodoo Castle\" wären in diesem Zusammenhang zu nennen. Das grafisch üppige Rollenspiel-Adventure \"Tunnels of Doom\" war dagegen so umfangreich, dass es auf gleich zwei Datenträgern geliefert werden musste.\n\nMehrere Zeitschriften im In- und Ausland beschäftigten sich mit allen Fragen rund um den TI-99/4A und versorgten ihre Leser mit Testberichten, Kaufberatungshinweisen, Bauanleitungen, Reparaturtipps, Kleinanzeigen und Programmausdrucken für Spiele, Anwendungen und Hilfsprogramme.\n\nDas \"99’er\"-Magazin war die bedeutendste Zeitschrift für den TI-99/4A und erschien ab Mai 1981 zunächst alle zwei Wochen, ab November 1982 dann einmal pro Monat unter dem vollen Titel \"99’er Home Computer Magazine\". Inhaltliche Schwerpunkte bildeten die Programmiersprachen \"TI LOGO\" und \"TI PILOT\". Für Auflockerung sorgten eingestreute Kreuzworträtsel und Cartoons. Bereits im November 1983 wurde das \"99’er\"-Magazin vom Markt genommen.\n\nDas zunächst unter dem Titel \"Home Computer Compendium\" veröffentlichte Magazin \"MICROpendium\" erschien monatlich von Februar 1984 bis Juni 1999 in Round Rock (Texas). Damit füllte es die vom \"99’er\"-Magazin hinterlassene Lücke aus. Mit seiner betont neutralen Berichterstattung, einem schlichten Schwarzweiß-Layout und einem günstigen Preis von 1,50 USD konnte sich das \"MICROpendium\" 15 Jahre lang behaupten, musste aber schließlich aufgrund zu geringer Verkaufszahlen eingestellt werden.\n\nVon 1983 bis 1987 erschien im Wiener Fiedler-Verlag monatsweise das \"TI-99 Journal\". Für 11 DM wartete es mit einer mehrfarbigen Titelseite auf und enthielt auch Artikel über andere Produkte von TI. Von Anfang 1984 bis 1987 erschien außerdem zunächst im TI-Aktuell-Verlag in Lohhof, später dann bei der München-Aktuell-Verlags-GmbH die in unregelmäßigen Abständen ungefähr alle zwei Monate veröffentlichte Zeitschrift \"TI-Revue: Das Magazin für TI PC & TI-99/4A\" für anfänglich 4,80 DM. Der Fachverlag Reinhold Hasse aus Bendorf gab überdies das neben dem TI-99/4A auch den programmierbaren Taschenrechner TI-59 behandelnde \"TI-Fachmagazin\" heraus.\n\nDaneben erschien ab 1981 im Selbstverlag das Periodikum \"TI-99 Software\" mit Berichten über aktuelle Entwicklungen auf dem Gebiet der Software für sämtliche TI-Heimcomputer. Im Jahr 1982 wurde das Magazin zunächst in \"TI Software Home-Computer Magazin - Fachzeitschrift für Anwender des TI-99/4A\" umbenannt. Ab 1983 wurde der umständliche Zusatz aus dem Titel der Zeitschrift entfernt.\n\nIm Laufe der Zeit sind auf unterschiedlichen Hardwareplattformen zahlreiche Emulatoren des TI-99/4A erschienen. In den 1990er-Jahren waren sie auf dem IBM PC oder dem Commodore Amiga, aber auch auf anderen Rechnern populär. Nachdem diese Trägersysteme technisch veraltet waren, wurden neue Emulatoren für modernere Rechner entwickelt. Es gibt im Internet vier regelmäßig gepflegte Emulatoren des TI-99/4A, die teils auf aktuellen, teils etwas älteren Versionen der weit verbreiteten Betriebssysteme MS-Windows, OS X und Linux laufen.\n\nDer Emulator \"PC99\" bzw. \"PC99A\" wurde von Greg Hill, Mark van Coppenolle und Mike Wright von der US-amerikanischen Firma CaDD Electronics für IBM-PC-Kompatible geschrieben. Sowohl die Standardversion \"PC99\" als auch die beschleunigte Version \"PC99A\" laufen unter den Betriebssystemen PC DOS 5.0 (oder höher), Windows 95 und Windows 98. Empfohlen wird die Verwendung mindestens eines Intel 80486 mit 66 MHz Taktfrequenz. Überdies können Rechner mit CPUs der Typen Pentium II, III, und IV oder AMD K6-III verwendet werden. Zu den weiteren Systemvoraussetzungen gehören eine VGA-Videokarte, ein freier Festplattenspeicher von mindestens 10 MB und ein 3½-Zoll-Diskettenlaufwerk.\n\nDer Emulator \"V9t9\" wurde von Edward Swartz im Java-Code programmiert und ist als Freeware im Internet herunterladbar. Zum Leistungsumfang gehören u. a. UCSD-Pascal, ein P-Code-Interpreter und eine Emulation des TI-Matrixdruckers. Der \"V9t9\" läuft unter den Betriebssystemen MS-Windows, OS X oder Linux. Der \"Win994a-TI-99/4A-Simulator\" stammt von Cory Burr und ist ebenfalls als Freeware erhältlich. Er arbeitet auf modernen PCs unter MS-Windows. Das Emulatorsystem \"M.E.S.S.\" schließlich unterstützt sowohl den TI-99/4 als auch den TI-99/4A inklusive Sprachmodul und Erweiterungskarten, für deren Emulation allerdings die entsprechenden ROM-Inhalte benötigt werden.\n\nIn der Fachpresse wurde der TI-99/4A überwiegend positiv bewertet und sogar als „einer der besten Heimcomputer“ beschrieben, die „es bislang auf dem Markt gab.“ Lob erfuhr der Rechner dabei für seine Benutzerfreundlichkeit, seine Hardware-Erweiterbarkeit, seine gegenüber dem Vorgängermodell verbesserte Tastatur, seine überzeugenden Farb- und Klangfähigkeiten, seine Fähigkeit zur Sprachsynthese, sein „kompakte[s], schlanke[s]“ Design, seine Robustheit, seine Vielseitigkeit sowie das umfangreiche Softwareangebot. Überdies wurde die Existenz gleich mehrerer exklusiv auf den TI-99/4A bezogener Computerzeitschriften wie dem \"99’er\"-Magazin oder dem \"TI-Fachmagazin\" und die daraus abgeleitete leichte Verfügbarkeit von Informationen über den Rechner positiv hervorgehoben.\n\nBemängelt wurde indessen die immer noch zu geringe Größe der neuen Tastatur sowie deren zahlreiche Mehrfachbelegungen, das Wärmemanagement des internen Leistungsreglers, die unterdurchschnittlichen Klangfähigkeiten, der begrenzte Befehlsvorrat des TI BASIC sowie die an den britischen Billigrechner Sinclair ZX81 erinnernde niedrige Arbeitsgeschwindigkeit des TI-BASIC-Interpreters.\n\nIn fast allen technikgeschichtlichen Überblicksdarstellungen wird der TI-99/4A als bedeutsamer Heimcomputer erwähnt. Viele Technikmuseen stellen den Rechner aus und er ist auch auf vielen Webseiten mit Bezug zur Geschichte der Heimcomputer zu finden. Außerdem besteht eine aktive, sich für die Bewahrung gut erhaltener Exemplare sowie weiterer mit dem Rechner verbundener Produkte einsetzende Retrocomputing-Szene. Damit hat der TI-99/4A einen festen Platz im kollektiven Gedächtnis, obwohl er nicht die gleiche hohe Wertschätzung erfährt wie etwa der C64, Apple II, Sinclair ZX Spectrum oder die Atari-Heimcomputer.\n\nTypisch für die technikgeschichtliche Einordnung des TI-99/4A sind drei Aspekte. Erstens gilt er als technologisch fortschrittlich, was vornehmlich auf seine 16-Bit-CPU sowie die „für damalige Verhältnisse ausgezeichnete[n] Grafikeigenschaften“ zurückgeführt wird. Zweitens erfährt der Rechner aufgrund der Eigentümlichkeiten seiner Rechnerarchitektur häufig eine Einschätzung als exotischer „Außenseiter“, dessen Entwicklung in einer evolutionären Sackgasse geendet habe. Drittens gilt der TI-99/4A trotz insgesamt respektabler Verkaufszahlen als letztlich am Markt gescheitert und ist sogar als das „vielleicht glückloseste System auf dem Heimcomputermarkt“ bezeichnet worden. Mit dieser Feststellung einher geht eine intensive Forschung nach den Ursachen für dieses Scheitern, das Erinnerungen an den das Ende des Wirtschaftswunders einläutenden Konkurs eines Bremer Automobilherstellers aus dem Jahr 1961 weckte:\n\nDie vergleichsweise kurze Marktpräsenz des TI-99/4A lässt sich nicht auf eine einzige Ursache zurückführen. Vielmehr führte ein ganzes Bündel unterschiedlicher Verfehlungen zur vorzeitigen Produktionseinstellung des Rechners. Dazu gehören Marketingfehler, Designfehler, eine mangelhafte Systemdokumentation, bestimmte Eigenheiten der Unternehmenskultur von TI sowie die Favorisierung von Steckmodulen als Hauptspeichermedium.\n\nTI beging einige schwere Marketingfehler. Vertriebsleiter Turner setzte fast ausschließlich auf Preisreduktionen, anstatt etwa durch geeignete Werbemaßnahmen potenziellen Käufern die zweifellos vorhandenen technischen Vorzüge des TI-99/4A wie etwa den leistungsstarken 16-Bit-Hauptprozessor zu erklären. Eingedenk der Verwendung des im Vergleich zu gängigen 8-Bit-CPUs mit 20 USD rund fünfmal teureren TMS9900 und den damit einhergehenden hohen Produktionskosten ist dies umso erstaunlicher. Trotzdem ließ TI sich auf einen riskanten Preiskrieg mit dem von Jack Tramiel geführten Billiganbieter Commodore ein und musste dabei eine bittere Niederlage einstecken. Berühmt geworden ist in diesem Zusammenhang ein eher umgangssprachlicher Kommentar des damaligen Spectravideo-Geschäftsführers Harry Fox: \"„TI got suckered by Jack“\" (deut. Übersetzung: „TI ist von Jack ausgetrickst worden“).\n\nDer TI-99/4A litt an einigen Designfehlern, zu denen an erster Stelle das doppelt interpretierte und daher langsame TI BASIC zählt. Bei den damals üblicherweise in BASIC durchgeführten Benchmarktests schnitt der Rechner trotz 16-Bit-CPU entsprechend schlecht ab und landete hinter Konkurrenzmodellen wie dem VC20, C64 oder Apple II.\n\nAuch die Speicherorganisation hatte ihre Nachteile. So dienten die 16 KB Arbeitsspeicher gleichzeitig als Grafik- und Programmspeicher. Im hochauflösenden und damit grafikspeicherintensiven \"Graphics-II\"-Modus etwa standen nur 4 KB für den Programmspeicher zur Verfügung. Umfangreichere TI-BASIC-Programme ließen sich daher nur im leistungsschwächeren \"Graphics-I\"-Modus realisieren. Ein Programmieren in Maschinensprache setzte darüber hinaus eine recht kostspielige Speichererweiterung voraus. Außerdem konnten solche externen Speicher von dem 16-Bit-Prozessor nur mit 8 Bits angesprochen werden. Der als Busconverter agierende Multiplexer wandelte dazu jeden entsprechenden 16-Bit-Zugriff des TMS9900 in zwei 8-Bit-Zugriffe auf den externen Speicher um. Die dabei generierten Waitstates reduzierten die Arbeitsgeschwindigkeit des Rechners spürbar.\n\nWegen des für das Eintippen mit dem Zehnfingersystem ungeeigneten Tastatur-Layouts konnte sich der TI-99/4A nicht als Bürocomputer etablieren. Überdies wurde versäumt, den in die Konsole integrierten Leistungsregler mit einer elektrischen Sicherung auszustatten, wodurch das Risiko von Stromschlägen stieg. Obendrein war der Joystickanschluss nicht Atari-kompatibel und entsprach damit nicht dem damaligen De-facto-Standard. Umsteiger von anderen Systemen mussten neue Joysticks erwerben, was die Attraktivität des Rechners verringerte.\n\nTI wollte zwecks Gewinnmaximierung die alleinige Kontrolle über die Softwareentwicklung behalten. Die Konzernspitze betrieb daher gegen den ausdrücklichen Rat von Chefentwickler Bynum eine geschäftsschädigende Heimlichtuerei und verzichtete auf eine offene Dokumentation von Betriebssystem, GPL und Rechnerarchitektur. Fremdanbietern erschwerte das die Softwareproduktion, sofern sie mit TI keine kostspieligen und profitminimierenden Kooperationsverträge aushandelten. Wem trotzdem die Entwicklung kommerzieller Programme für den TI-99/4A gelang, wurde mit rechtlichen Schritten gedroht. Diese Vorgehensweise schreckte professionelle Softwarehäuser ebenso wie die kreative Hackerszene von einer Beschäftigung mit dem Rechner und seiner ohnehin wenig verbreiteten CPU ab. Zwar wurde das Konzept einer geschlossenen Architektur im Sommer 1981 zunächst gelockert, nach Einsetzen des Preiskriegs mit Commodore im September 1982 aber wieder aufgenommen. Erst 1985 erschien schließlich in einem westdeutschen Verlag ein vollständiges, jedoch nicht von TI unterstütztes Listing des Betriebssystems inklusive der GPL-Routinen.\n\nDie Unternehmensleitung glaubte überdies, alle Programmwünsche der Kunden im Alleingang erfüllen zu können. Rund 20 Millionen USD wurden jedes Jahr in die Softwareentwicklung investiert. Auf die damals übliche, von den Kunden erwartete Lizenzierung und Portierung bereits bewährter Anwendersoftware wie \"Microsoft BASIC\",\"Visicalc\", \"WordStar\" sowie vieler Spiele wurde dagegen verzichtet. Daher blieb die für den TI-99/4A entwickelte Software mit Ausnahme der Lernprogramme insgesamt eher mittelmäßig.\n\n1983 wurden Umsetzungen von Steven Spielbergs weltweit erfolgreichem Kinofilm \"E.T. – Der Außerirdische\" für verschiedene Hardwareplattformen entwickelt, darunter der TI-99/4A sowie die marktführende Spielekonsole Atari 2600. Als Spielberg zufällig von der deutlich niedrigeren Qualität der Atari-2600-Version erfuhr, entzog er aus Furcht vor finanziellen Einbußen TI kurzerhand wieder die Lizenz. Die Atari-2600-Version von \"E.T. the Extra-Terrestrial\" wurde zu einem der größten Flops der Videospielgeschichte und gilt heute als Inbegriff des noch im gleichen Jahr einsetzenden Video Game Crashs.\n\nDie zu diesem Zeitpunkt von Konservatismus und Selbstbezüglichkeit geprägte Unternehmenskultur von TI trug ebenfalls zum Misserfolg des TI-99/4A bei. Aus Überheblichkeit verzichtete der Technologiegigant auf die Entwicklung eines herkömmlichen 8-Bit-Mikroprozessors nach dem Vorbild kleinerer, aber hochinnovativer Hersteller wie Zilog, MOS Technology oder Intel, obwohl der Trend klar in die Richtung der 8-Bit-Architekturen ging. Da die Unternehmensphilosophie eine Verwendung von Mikroprozessoren aus Fremdherstellung ausschloss und sich die eigene 16-Bit-CPU am Markt nicht durchsetzen konnte, wurde bei der Planung des TI-99/4A nicht von zu erreichenden Leistungsmerkmalen oder Kundenwünschen ausgegangen, sondern ein zum TMS9900 passender Rechner entworfen, obwohl TI zu diesem Zeitpunkt noch nicht einmal geeignete 16-Bit-Koprozessoren entwickelt hatte. Auch gelang es TI nicht, in den konzerneigenen Halbleiterfabriken kostengünstigere Chips für den TI-99/4A herzustellen und damit eine Verringerung der Produktionskosten zu erreichen.\n\nDie Unternehmensspitze war außerdem davon überzeugt, auf das Abwerben erfahrener Computertechniker verzichten zu können. Dieser Aspekt der damaligen Unternehmenskultur zeigt sich in der 1977 erfolgten Verlegung des Hauptquartiers der Abteilung für Unterhaltungselektronik von der Millionenstadt Dallas in die verschlafene Baumwollmetropole Lubbock. Für etablierte Computerexperten aus dem liberalen Milieu des kalifornischen Silicon Valley war der Ausblick auf ein Leben in der tiefsten texanischen Provinz meist kein Anreiz für einen Wechsel zu TI. Bei der Entwicklung des TI-99/4A fehlte daher ein von außen kommender, die eingefahrene Unternehmensphilosophie kritisch hinterfragender Geist.\n\nNach Markteinführung bot TI monatelang keine externen Speichergeräte für den TI-99/4A an, nicht einmal einen Programmrekorder. Stattdessen setzte der Konzern zunächst fast ausschließlich auf die relativ teuren Steckmodule. Die eine wichtige Käuferschicht bildenden Jugendlichen konnten sich diese oft nicht leisten und bevorzugten daher Heimcomputersysteme, deren Software auf den günstigeren Kompaktkassetten oder Disketten erschien. Diese Speichermedien boten auch den Vorteil, dass sich die unter Jugendlichen damals üblichen Raubkopien leichter anfertigen und untereinander tauschen ließen. Steckmodule dagegen unterbanden diese Praxis.\n\n\n\nAllgemeine Informationen\nSpiele-Enzyklopädie\nAktuelle Emulatoren\n"}
{"id": "180584", "url": "https://de.wikipedia.org/wiki?curid=180584", "title": "Norton Utilities", "text": "Norton Utilities\n\nDie Norton Utilities (kurz: \"NU\") sind eine Zusammenstellung von unterschiedlichen Programmen, um Microsoft-Betriebssysteme und Mac-Betriebssysteme in ihrer Funktionalität zu erweitern. Der Hersteller Symantec gehört in dieser Kategorie von Programmen zu den Marktführern. Hauptaugenmerk wird vor allem auf das Aufspüren und Beheben von Fehlern in Dateien, Dateisystemen und Festplatten sowie auf die Systemdiagnose, Systembeschleunigung und Datenwiederherstellung gelegt. Die ersten Versionen wurden von Peter Norton 1981 für das Betriebssystem MS-DOS entwickelt. Seit 1990 gehören die Norton Utilities zu Symantec.\n\nBis zur Version 7 waren alle Versionen ausschließlich für MS-DOS und ergänzten dieses. Enthalten waren unter anderem: \n\nAb Version 8 gab es die Norton Utilities auch für Windows 3.x, das heißt einige Programme hatten neben der DOS-Version auch eine Version für Windows. \nMit der Einführung von Windows 95 wurde die Toolsammlung gänzlich auf Windows portiert und man begann erneut mit der Versionsnummer 1. Mit der Version 4.5 (erschienen im Jahr 2000) änderte Symantec die Versionsbezeichnungen auf die Jahreszahlen, welches 2009 revidiert wurde. Zwischen 2003 und 2007 waren die Utilities integraler Bestandteil der Norton SystemWorks, zu denen auch Norton AntiVirus gehörte. Die im Jahr 2009 erschienene Version 14 läuft unter Windows XP, Vista und 7.\n\nIn den 1980er und 90er Jahren erfreuten sich die Norton Utilities großer Beliebtheit. Auch Microsoft nahm einige Programmteile als Bestandteile des Betriebssystems MS-DOS in Lizenz auf (z. B. den Festplattendefragmentierer). Dennoch blieben die originalen Norton-Utilities-Programme in ihrer Handhabung flexibler. Mit dem Aufkauf von Central Point Software, des Herstellers der PC Tools, 1994 übernahm Symantec seinen in diesem Bereich schärfsten Konkurrenten. \n\nAuf dem Mac gibt es die Norton Utilities seit 1988. Auf dieser Plattform sind die Hauptkonkurrenten \"Micromat TechTool Pro\" und \"Alsoft DiskWarrior\".\n\nDie Zusammensetzung und der Mehrwert der Toolsammlung schwankte mit den Jahren und Versionen deutlich und nimmt seit Jahren ab. Vor allem die Einführung neuer Betriebs- und Dateisysteme, wie der Umstieg von DOS auf Windows und dessen stete Weiterentwicklung, machte einige althergebrachte Erweiterungen überflüssig. Dem passte sich die Toolsammlung an, was unterschiedlich gut gelang. Mit der Einführung des Papierkorbes in Windows 95 verlor z. B. das eigenständige Wiederherstellungsprogramm seinen Nutzen, stattdessen erweiterten die Norton Utilities die Funktionen des Papierkorbes, was aber mit Windows XP wieder überflüssig wurde. Nachdem sich große NTFS-formatierte Festplatten immer mehr durchsetzten, galt dies auch für den Defragmentierer SpeedDisk. Ab der Version 14 ist er nicht mehr enthalten (nur noch als Verweis auf den integrierten Defragmentierer von Windows).\n\nImmer wieder kritisiert wurde, dass die Toolsammlung unter Windows XP nicht mit eingeschränkten Rechten umgehen konnte.\n\n"}
{"id": "182407", "url": "https://de.wikipedia.org/wiki?curid=182407", "title": "Finite Difference Time Domain", "text": "Finite Difference Time Domain\n\nDas Verfahren wurde erstmals 1966 vom chinesisch-US-amerikanischen angewandten Mathematiker Kane S. Yee (* 1934) vorgeschlagen.\n\nDie Maxwell-Gleichungen beschreiben die Zeitentwicklung von elektrischen- und magnetischen Feldern. Die zeitliche Änderung des elektrischen Feldes ist durch die räumliche Änderung des magnetischen Feldes bestimmt, und die zeitliche Änderung des magnetischen Feldes durch die räumlichen Änderung des elektrischen Feldes. \n\nIm Yee-Verfahren wird der Raum mit Hilfe eines speziellen Gitters (Yee-Gitter) diskretisiert. An den Gitterpunkten wird zu einem Zeitpunkt der Wert der elektrischen Feldstärke \"E\" bzw. der magnetischen Feldstärke \"H\" gespeichert. An jedem Gitterpunkt wird abwechselnd das neue \"E\"-Feld und das neue \"H\"-Feld für den nächsten Zeitpunkt bestimmt. Die Änderung des \"E\"-Feldes berechnet sich aus der numerischen Rotation des angrenzenden \"H\"-Feldes. Die Änderung des \"H\"-Feldes wiederum, berechnet sich aus der Rotation des angrenzenden \"E\"-Feldes.\n\n\nSoftware:\n"}
{"id": "183181", "url": "https://de.wikipedia.org/wiki?curid=183181", "title": "Die Datenschleuder", "text": "Die Datenschleuder\n\nDie Datenschleuder. Das wissenschaftliche Fachblatt für Datenreisende ist die unregelmäßig erscheinende Zeitschrift des Chaos Computer Clubs (CCC). Sie erscheint seit 1984 und kann auch unabhängig von einer Mitgliedschaft im CCC abonniert und seit dem Jahr 2002 im Internet gelesen werden.\n\nThemen sind vor allem die politischen und technischen Aspekte der digitalen Welt (Informationsfreiheit, Datenschutz, Videoüberwachung und Privatsphäre, Kryptographie etc.). Derzeit wird sie im DIN-Format A5 herausgegeben. Die Redaktion wird virtuell über das Internet betrieben, jedoch wird die Datenschleuder in Berlin gedruckt und von dort aus vertrieben.\n\nAls Gimmick lag der Ausgabe #92 vom März 2008 eine Folie mit dem Fingerabdruck des damaligen Innenministers Wolfgang Schäuble bei.\n\nAls Vorbild der Datenschleuder wird die 1971 gegründete US-Zeitschrift TAP – The Youth International Party Line (YIPL) genannt, die als Protest gegen den Vietnamkrieg Tipps zum kostenlosen Telefonieren verbreitete.\n\nDie letzte aktuelle Ausgabe, Nr. 98, erschien im Juli 2018 (Stand Ende Juli 2018).\n\n\n"}
{"id": "183613", "url": "https://de.wikipedia.org/wiki?curid=183613", "title": "Apple III", "text": "Apple III\n\nDer Apple III (Eigenschreibweise Apple ///) war der erste Computer von Apple, der ohne Steve Wozniak (er entwarf 1976 den Apple II) entwickelt wurde. Die Arbeit am Apple III begann im Jahr 1978, vorgestellt wurde er im Mai 1980.\n\nDer Apple III wurde aus mehreren Gründen zum teuren Flop. Dazu gehörten der hohe Preis und die in der Praxis unvollständige Kompatibilität zum beliebten Apple II und dessen bereits recht großer Softwarebibliothek – für Apple-II-Programme wurden nur maximal 48 KB RAM unterstützt, während die meisten Programme damals bereits 64 KB erforderten. Vielleicht noch bedeutender waren technische Probleme, wie etwa häufige Abstürze durch Überhitzung und die Tendenz der Chips, aus ihren (billigen) Sockeln zu rutschen.\n\nDie technischen Probleme wurden zwar mit dem 1983 eingeführten Apple III Plus behoben, aber der entstandene schlechte Ruf des Gerätes ließ sich dadurch nicht mehr ändern. Die so entstandene Marktlücke oberhalb des Apple II füllte 1981 der IBM-PC.\n\nDa der Apple II sich, anders als der III, weiterhin gut verkaufte, nannte Apple seinen nächsten 8-Bit-Rechner 1982 dann nicht Apple IV, sondern Apple IIe und achtete diesmal auf volle Kompatibilität zum Apple II.\n\n"}
{"id": "183899", "url": "https://de.wikipedia.org/wiki?curid=183899", "title": "Input 64", "text": "Input 64\n\nInput 64 (eigene Schreibweise \"INPUT 64\") war ein von 1985 bis 1988 im Heise-Verlag erschienenes Computermagazin für den Commodore 64.\n\nDas Magazin befand sich vollständig – abgesehen von einem kleinen Begleitheft – auf digitalen Datenträgern: zuerst auf Compact Cassette für die Datasette des C64, später auf Diskette für das Diskettenlaufwerk VC1541 des Heimcomputers.\n\nHauptvorteil des Konzeptes war es, Programme für den Computer direkt digital laden zu können. Da Programme seinerzeit in der Regel als ausgedruckter Programmcode auf Papier verbreitet wurden, stellte der Ansatz von \"Input 64\" eine Innovation dar.\n\nDas eigentliche Magazin wurde als Programm vom Datenträger in den Computer geladen und erschien dann in Textform auf dem Bildschirm. Über ein Auswahlmenü konnte man die verschiedenen Artikel, Programme – darunter Tools, Befehlssatzerweiterungen, Lernserien (zum Thema C64 selbst sowie Sprachen) und Spiele – auswählen.\n\nIm Dezember 1988 wurde das Magazin eingestellt. Die alte Redaktion gründete dann das Magazin \"iX\" im selben Verlag.\n\n"}
{"id": "184593", "url": "https://de.wikipedia.org/wiki?curid=184593", "title": "Hackerbibel", "text": "Hackerbibel\n\nDie Hackerbibel ist eine Publikation des Chaos Computer Clubs. Sie ist bisher in zwei Ausgaben in den Jahren 1985 und 1988 erschienen. Beide Ausgaben wurden von Wau Holland herausgegeben und vom Verlag Grüne Kraft veröffentlicht.\n\nDie Hackerbibel ist ein Sammelsurium aus Dokumenten und Geschichten der Hacker-Szene, wie beispielsweise die Bauanleitung für den als „Datenklo“ betitelten Akustikkoppler. Sie bietet darüber hinaus Bauanleitungen und andere technische Hintergründe. Die 1. Ausgabe erschien 1985 mit dem Untertitel \"Kabelsalat ist gesund\", und erzielte bis Mitte 1988 eine verkaufte Auflage von 25.000 Exemplaren. Die Ausgabe 2 aus dem Jahr 1988 wird auch \"Das neue Testament\" genannt. Die Comic-Zeichnungen der Umschlagbilder sind eine Schöpfung der deutschen Comic-Zeichner Mali Beinhorn und Werner Büsch von der Comicwerkstatt Büsch-Beinhorn. Die Produktion und der Vertrieb der Hackerbibel wurde schon vor 1990 eingestellt. Seit 1999 bietet der CCC eine gescannte und im Volltext verfügbare Version mit weiterem Material, wie Texte von Peter Glaser, eine Dokumentation zu Karl Koch und die Arbeiten von Tron, auf der Chaos-CD an.\n\n\n"}
{"id": "184622", "url": "https://de.wikipedia.org/wiki?curid=184622", "title": "Datenklo", "text": "Datenklo\n\nDas Datenklo (oder CCC-Modem) ist ein Selbstbau-Akustikkoppler des Chaos Computer Clubs. Die Bauanleitung wurde 1985 in der Hackerbibel veröffentlicht, und fand damit bis Mitte 1988 eine Verbreitung von mehr als 25.000 Exemplaren. Das per Bauanleitung selbst zu bauende Telefonmodem war eine kostengünstige, aber illegale Alternative zu den wenigen in dieser Zeit von der Deutschen Bundespost zugelassenen Modems. \n\nDas Datenklo (ursprünglicher Name \"CCC-Modem\") wurde Mitte 1984 in Reaktion auf die recht restriktive Gesetzgebung im Bereich der Telekommunikation in Deutschland entwickelt und bot eine mit 300 DM günstige Möglichkeit, Datenfernübertragung zu betreiben. Modems waren damals in Deutschland noch verboten, mit Ausnahme der teuren Mietgeräte der Bundespost. Anfang 1987 betrug die monatliche Gebühr für ein BTX-MultiTel 1 48 DM (nach heutiger Kaufkraft  €) und für ein \"MultiTel 2\" 78 DM (nach heutiger Kaufkraft  €). Das erste im Mai 1988 mit FTZ-Zulassung versehene Modem kostete als Tischmodell 1950 DM (nach heutiger Kaufkraft etwa  €). Die Bundespost-Modems entsprachen den internationalen Normen der CCITT (heute ITU-T), viele Hobby-Benutzer verlangten aber nach einfacheren Standards, zum Beispiel nach Modems der Firma Hayes.\n\nZentraler Baustein des CCC-Modem ist der AM7910 (FSK-Modem-Chip, unterstützt Bell 103/113/108/202 und CCITT V.21/V.23) von AMD, der auf einem einzigen Chip ein asynchrones Frequenzumtastungs-Modem mit Übertragungsraten von 300 bis 1200 Baud realisiert und voll kompatibel mit den Bell- und CCITT-Standards V.21 und V.23 ist.\nDen Namen erhielt das Datenklo durch die für den Akustikkoppler vorgesehenen Gummimuffen, bei denen es sich um Gummidichtungen zwischen Spülwasserrohr und WC-Becken aus dem Sanitärfachbedarf handelte, sowie in Anlehnung an das Kunstwort „Datenfön“, einer umgangssprachlichen Bezeichnung für Akustikkoppler, basierend auf der einst populären Dataphon-Baureihe S21 der Firma Woerltronic aus Cadolzburg. Die Größe ist dabei so gewählt, dass ein Lautsprecher mit 66 mm Durchmesser in die Gummidichtung passt und sich diese passgenau über eine Telefonhörermuschel stülpen lässt. Letzteres ist laut Bauanleitung wichtig für die Abschirmung von Umgebungsgeräuschen.\n\nSpäter wurde der Name für die aus mobilen Toilettenhäuschen gebauten Netzwerkverteilzentralen auf dem Chaos Communication Camp wiederverwendet.\n\nIm Spiel findet man die moderne Version des Datenklos in Form mehrerer Toilettenhäuschen am nordwestlichen Rand der Karte.\n\n"}
{"id": "185658", "url": "https://de.wikipedia.org/wiki?curid=185658", "title": "IX – Magazin für professionelle Informationstechnik", "text": "IX – Magazin für professionelle Informationstechnik\n\nIX – Magazin für professionelle Informationstechnik ist eine 1988 gegründete, in deutscher Sprache erscheinende Computerzeitschrift. Nach eigener Darstellung leitet sich der Name vom Betriebssystem Unix ab. Die Ausgabe wird vom Heise-Zeitschriften-Verlag publiziert. Sie hat etwa 170 Seiten und erscheint monatlich.\n\nDie Zielgruppe sind professionelle Administratoren und Programmierer. Im Gegensatz zu ihrer Schwesterzeitschrift \"c’t\" zielt die \"iX\" hauptsächlich auf Fachleser. Der Werbeslogan „Versteht nicht jeder – ist auch besser so“ versucht, elitebildend diese Abgrenzung zu schärfen.\n\n\nIm vierten Quartal 2015 lag die durchschnittliche monatlich verbreitete Auflage nach IVW bei 35.065 Exemplaren. Das sind 1,18 Prozent (420 Hefte) weniger als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 1,27 Prozent auf jetzt 31.148 Abonnenten ab. Damit bezogen 88,8 Prozent der Leser die Zeitschrift im Abo.\n\nZahlen des jeweils vierten Quartals.\nZahlen des jeweils vierten Quartals.\n\n"}
{"id": "186154", "url": "https://de.wikipedia.org/wiki?curid=186154", "title": "Kabelsalat", "text": "Kabelsalat\n\nDer Begriff Kabelsalat beschreibt ungewollt auftretende und schwer zu entwirrende Knotenbildungen oder Verwicklungen von Kabeln. Die Wahrscheinlichkeit der Knotenbildung ist ab einer kritischen Länge praktisch unabhängig von der Länge des Kabels. Allerdings nimmt die benötigte Zeit, um diese Knoten (mittels bloßen Ausschüttelns) zu entfernen, stark mit der Länge des Kabels zu. Die Knoten entstehen in der Regel an den Enden der Kabel, im weiteren Verlauf der Entstehung des Kabelsalats rutschen sie dann in Richtung Kabelmitte. Um Kabelsalat zu verhindern, werden Kabel geordnet nebeneinander verlegt und ggf. so aneinander fixiert, dass sie sich nicht miteinander verschlingen und problemlos wieder voneinander trennen lassen. Beispiele für Kabelfixierungen sind Kabelbinder und Kabelbäume.\n\nIm übertragenen Sinn wurde der Begriff auch in den 1980er Jahren von der Hackerszene verwendet, um gegen die Verlegung von Fernsehkabeln durch die damalige deutsche Bundespost und die dabei befürchteten Verschlechterungen beim Datenschutz zu protestieren. Das Kabel-Logo der Post wurde vom Chaos Computer Club (CCC) zu einem Knoten verfremdet.\n\n"}
{"id": "186566", "url": "https://de.wikipedia.org/wiki?curid=186566", "title": "Capella (Software)", "text": "Capella (Software)\n\ncapella ist ein Notensatzprogramm, das vom Unternehmen \"capella-Software AG\" aus Söhrewald entwickelt und vertrieben wird (man beachte die Schreibweise mit \"einem\" p; vgl. \"a cappella\", hingegen z. B. \"capella coloniensis\"). \"capella\" ist für Windows und seit Version 8 (November 2017) auch für macOS verfügbar. Das Programm liegt in verschiedenen Sprachen vor, darunter Deutsch, Englisch und Französisch. Es ist vor allem im deutschsprachigen Raum verbreitet. Meist wird es im praxisbezogenen und semiprofessionellen Bereich verwendet (Schulmusiker, Kirchenmusiker, Chorleiter und Hobbymusiker). Im Netz finden sich Webseiten mit umfassenden Notensammlungen im \"capella\"-Format, z. B. bei Musicalion.\n\n\"capella\" findet Einsatz bei ein- und mehrstimmigem Notensatz (z. B. Chorsatz und Orchesterpartituren) und für das Notieren von Liedern mit Akkorden, Strophen-Texten oder Gitarrengriffen. Die Eingabe erfolgt meist vollständig über die Computertastatur, kann aber auch per Maus oder mit einem MIDI-Keyboard durchgeführt werden. \"capella\" verfügt über eine Im- und Export-Funktion in das gebräuchliche MusicXML-Format und ermöglicht somit den Datenaustausch mit anderen Notensatzprogrammen. Ein nativ integrierter PDF-Export existiert seit Version 8. Der Betrieb von Software-Instrumenten (VST-Plug-ins) als Klangerzeuger ist möglich, die capella-eigene Klangerzeuger-Engine heißt \"capella-tune\".\n\n\nDer Begründer und Autor von \"capella \" ist Hartmut Ring, Professor für Mathematik und Informatik an der Universität Siegen. Die erste Veröffentlichung erfolgte 1992 unter dem Namen \"Allegro\". Jedoch erfolgte für die Version 1.01 – datiert auf den 15. Mai 1992 – wegen namensrechtlicher Probleme die Umbenennung in \"capella\", nach dem Hauptstern Capella im Sternbild Fuhrmann. \"capella\" wurde zunächst für das Betriebssystem MS-DOS entwickelt und hier bis zur Version 1.5 gepflegt, die im Jahr 1993 erschien. In der DOS-Version war bereits eine eigene grafische Benutzeroberfläche integriert. Die Version 2.0 war die erste für Microsoft Windows 3.1, aber erst die Version 2.1 lief stabil.\n\nMit der Version 2004 wurde die Möglichkeit geschaffen, Partituren über Skripte zu bearbeiten. Auf dieser Basis gibt es vermehrt kostenlose Plugins, die den Funktionsumfang von \"capella\" erweitern. Neu in dieser Version ist auch der Datenexport in ein von \"capella-Software\" entwickeltes offenes XML-Format, welches aber bisher von anderer Notensatzsoftware nicht unterstützt wird.\n\nIn den 2.x-Versionen (Windows 16-Bit) war zumindest die Basisversion des Programms trotz Zwang zur vollständigen Installation über einen Kopierschutz an die eingelegte CD gebunden. Mit \"Capella 800\" (Version 3.0, 32-Bit) wurde dieser Kopierschutz wieder entfernt. 2004 wurde eine hardwarebasierende Produktaktivierung eingeführt.\n\nIm Jahr 2010 übernahm Bernd Jungmann die Pflege und Weiterentwicklung von \"capella\".\nZeitgleich beendete Hartmut Ring seine aktive Beteiligung an der Programmierung von \"capella\".\n\nAn der Version capella 8 arbeitete neben Bernd Jungmann auch Christian Schauß.\n\n\nDie Produktlinie der \"capella-Software AG\" besteht zurzeit aus insgesamt acht Programmen im unteren Preissegment. Diese sind \"capella\", \"capella-scan\", \"capella playAlong\", \"capriccio\", \"tonica fugata\", \"rondo\", \"audite!\" und \"capella wave kit\". \"capella-scan\" dient dem Einlesen gedruckter Noten in \"capella\" mittels Optical Music Recognition.\n\nAußer \"capella\" (ca. 300.000 registrierte Lizenzen) und \"capella-scan\" (ca. 120.000 registrierte Lizenzen) hat bislang keines der genannten Programme nennenswerte Verbreitung gefunden.\n\n\n"}
{"id": "187391", "url": "https://de.wikipedia.org/wiki?curid=187391", "title": "Tonwertumfang", "text": "Tonwertumfang\n\nDer Tonwertumfang gibt an, wie viele Farbinformationen ein Bild oder eine Bilddatei enthält. Er bezeichnet den Unterschied zwischen der hellsten und der dunkelsten Stelle eines Bildes. Der Tonwertumfang wird bezogen auf den maximal möglichen Umfang angegeben, beispielsweise von 5 % Weiß bis 93 % Schwarz.\n\nDer maximale Tonwertumfang (Farbraum) wird durch das Medium begrenzt, bei Fotografien etwa die Eigenschaften des gewählten Fotopapiers oder Druckverfahrens, bei digitalen Grafiken durch die Farbtiefe des verwendeten Grafikformates und die technischen Möglichkeiten des Wiedergabegerätes wie eines Bildschirmes.\n\nUm eine differenzierte und kontrastreiche Wiedergabe eines Bildes zu ermöglichen, wird in der Regel versucht, einen Farbraum optimal auszunutzen, es gibt jedoch auch die bewusste Reduzierung des Tonwertumfanges, um spezielle Effekte wie in der High- oder Low-Key-Fotografie zu erzielen.\n\nMit einer Bildbearbeitungssoftware lässt sich der Tonwertumfang eines digitalen Bildes regulieren, nicht nur die Maximal- und Minimalwerte lassen sich ändern, sondern auch die statistische Verteilung der Tonwerte im Bild. Die Tonwertkorrektur ist damit das wichtigste Werkzeug, um Helligkeit und Kontrast zu optimieren. Das Histogramm dient dabei als Hilfsmittel, um die Verteilung der Tonwerte zu analysieren, die sich dann gezielt korrigieren lassen. Bei einer starken Erhöhung des Tonwertumfanges kann es jedoch auch zu Qualitätseinbußen kommen, wenn es zu einer sichtbaren Spreizung der Tonwerte kommt, durch die feine Farbverläufe nicht mehr homogen wiedergegeben werden können.\n\n"}
{"id": "187448", "url": "https://de.wikipedia.org/wiki?curid=187448", "title": "Kanotix", "text": "Kanotix\n\nKanotix ist eine Debian-basierte Linux-Distribution mit integriertem Live-System. Zunächst unter Verwendung der Knoppix-Technologie, basiert es seit 2010 auf Debian-Live.\nKanotix wurde explizit für moderne Hardware angepasst und – neben der \"Live-Funktion\" auch für Festplatten-Installationen ausgelegt. Als grafische Oberfläche wird standardmäßig KDE eingesetzt. Seit Februar 2013 gibt es auch eine Version mit dem LXDE-Desktop.\n\nDer Name „Kanotix“ leitet sich vom Nicknamen (Kano) des Entwicklers Jörg Schirottke ab.\n\nDie Distribution \"Kanotix\" ging aus \"Kano’s Scriptpage for Knoppix\" im Dezember 2003 mit der \"Xmas-Preview\" als eigenständiges, auf dem \"unstable\"-Zweig Debian-Sid basierendes Knoppix-Derivat hervor.\n\nIm Jahr 2004 wurden 10 Kanotix-Versionen mit dem Namen \"Bughunter 1-X\" veröffentlicht, 2005 erschienen 4 stabile Versionen, die nur mit Erscheinungsjahr und laufender Nummer bezeichnet wurden. Aktuelle Ereignisse gaben im Jahr 2006 einigen Versionen den Namen „CEBIT“- und „Easter“-Edition. Die letzte auf Debian-Unstable basierende Version erschien am 2. Oktober 2006: \"Kanotix-2006-01-RC4\".\n\nNach Unzufriedenheit mit der Stabilität wurde 2007 die Distribution auf Debian GNU/Linux 4.0 („Etch“) als Basis umgestellt, das vom 8.  April 2007 bis 14. Februar 2009 der neue \"stable\"-Zweig von Debian war. Dabei blieb ein Teil der Community bei Debian Sid, woraus siduction entstand.\nWährend der Umstellungsphase im 1. Halbjahr 2007 konnten bestehende Kanotix-Installationen ebenfalls per \"apt-get dist-upgrade\" aktualisiert werden, vorausgesetzt, die Quellen waren vor dem Debian-Etch-Release am 8. April 2007 von \"Sid\" auf \"Etch\" geändert worden.\nDie neue Basis brachte die beabsichtigten Vorteile mit sich: So können nunmehr weitgehend risikofrei Aktualisierungen (dist-upgrades) durchgeführt und zusätzliche Pakete installiert werden.\n\nUm weiterhin auf moderner Hardware, insbesondere auf tragbaren Computern wie Laptops oder Netbooks lauffähig zu sein, wurde ein aktueller Kernel (aus Ubuntu-Sourcen) verwendet, der für Debian-„stable“ angepasst und modifiziert war. Daneben wurden die eigenen Pakete bzw. Scripte laufend aktuell gehalten.\n\nBei Kanotix handelt es sich um ein Linux-Live-System mit Installationsfunktion, das von einem bootfähigen Medium wie CD, DVD oder mit den neueren Hybrid-Isos auch von bootfähigen USB-Medien wie USB-Stick gestartet werden kann; eine Festplatte bzw. eine Installation ist zum Testen und ggf. zum Arbeiten nicht erforderlich. Die enthaltenen WLAN-Treiber ermöglichen z. B. den Zugang ins Internet ohne Kabelverbindung. Auf Datenträger wie Festplatten, USB-Stick, Disketten kann im Live-Modus zugegriffen werden, Dateien auf NTFS-Partitionen können gelesen und ab Thorhammer-RC7 standardmäßig geschrieben werden (NTFS-3g).\nDie neueren Kanotix-Versionen haben – wie vergleichbare Distributionen auch – ext4 als Standardfilesystem beim Installieren. Reiserfs, XFS, JFS sind optional. Mit \"Kanotix-Excalibur\" konnte ab Kernel-Version 2.6.28 ext4 gewählt werden, es ist seit \"Kanotix-Hellfire\" die Standardeinstellung.\n\nKanotix hat eine funktionierende Hardware-Erkennung, auch für neuere Hardware. Es enthält freie und quelloffene Anwendungen, die Bildbearbeitung GIMP, den Browser Firefox bzw. Iceweasel und zahlreiche KDE-Anwendungen. Als Büroanwendung ist die aktuelle Version von OpenOffice.org bzw. LibreOffice statt der älteren Versionen aus Debian-stable enthalten. Ebenso werden wine und der Multimessenger Pidgin, der IRC-Client Konversation und die VoIP Software Skype in der jeweils aktuellen Version im Kanotix-Repository vorgehalten. Darin sind auch Anwendungen enthalten, die auf den Kanotix.iso aus Platzgründen fehlen. In der neuen, mit Kanotix-Dragonfire eingeführten LXDE-Version, wurde auf LibreOffice, wine und icedove verzichtet.\n\nBis Kanotix-Thorhammer war – ebenso wie in Knoppix – der klik-Client enthalten. Dieser ist vor allem dazu gedacht, Programme im „live“-Betrieb zu installieren und zu testen.\n\nKanotix kann im Livemodus ohne Festplatteninstallation zum Testen, aber auch zum Arbeiten benutzt werden. Durch die unionFS- bzw. aufs-Unterstützung können persönliche Einstellungen, zusätzlich „installierte“ Programme getestet werden. Auch die Speicherung von Daten und der Kanotix-Konfiguration auf der Festplatte oder einem mobilen Datenträger z.  B. einem USB-Stick ist möglich. (Erstellung eines bootbaren USB-Sticks mit \"persistent\" Option).\nDas Kanotix-Image kann von Festplatte oder einem USB-Stick, wenn vom BIOS unterstützt, gebootet werden. Dadurch ergibt sich ein erheblicher Geschwindigkeitsvorteil und macht ein (fast) normales Arbeiten wie bei einer Festplatteninstallation möglich.\n\nKanotix kann mittels des mitgelieferten grafischen Installers auf die Festplatte installiert werden. Dabei bietet die neue Version des acritoxinstallers einen erheblich erweiterten Funktionsumfang. Dieser wurde laufend an die neuen Versionen bzw. neue Hardware angepasst. Für die Vorbereitung einer Installation kann neben der manuellen Partitionierung mit den Programmen gparted eine ganze Reihe „automatischer“ Einstellungen gewählt werden.\n\nDer Installationsvorgang ist innerhalb von circa 10 bis 20 Minuten abgeschlossen, wenn man von einem mobilen Datenträger (anfangs CD heute DVD und USB-Stick) installiert, bei Verwendung einer Kanotix.iso auf der Festplatte läuft die Installation schneller ab.\nDas so installierte Kanotix verhält sich wie eine reguläre Debian-Installation und kann somit auch durch Aktualisierungen aus dem Debian-Pool und dem Kanotix-Repository gepflegt werden.\n\n\"Kanotix-Thorhammer-2007\" war die erste Version die auf Debian \"stable\" basierte und wurde nach einer Reihe interner Testversionen am 15. September 2007 als \"Release Candidate 6\" veröffentlicht. Eine aktualisierte Version wurde am 1.  Januar 2008 als \"Release Candidate 7\" veröffentlicht. (Eine weitere bzw. \"Final\"-Version erschien nicht mehr.)\n\nAlle neueren Kanotix-Versionen haben einen aktuellen Kernel aus Ubuntu-Sourcen. Einige Kernel-Korrekturen (sog. \"Kernel patches\") flossen umgekehrt auch in die Ubuntu-Version 8.04 (\"Hardy Heron\") ein.\n\nDie \"Entwicklung im Jahr 2008\" konzentrierte sich neben der Kernel-Entwicklung auf die Aktualisierung und Pflege des \"Thorhammer\"-Repositorys sowie der Erprobung von zusätzlichen Paketen und Funktionen. Dazu gehören auch Anpassungen für Netbooks bzw. Subnotebooks. Zu den wichtigsten Backports und Aktualisierungen gehörte OpenOffice.org 3.x. in der jeweils aktuellen Version.\nNach Angaben des Entwicklers ist ein Upgrade incl. Kernel-update grundsätzlich zu empfehlen, da auch wichtige Sicherheitsupdates enthalten sind. Nach der Freigabe war die Kernel-Version \"2.6.28\" als \"generic\" und \"server\" erhältlich, allerdings standen auch Updates der Kernel-Versionen ab 2.6.22 weiterhin zur Verfügung. Die Kernel-Versionen 2.6.24 bis 2.6.32 sind im \"Thorhammer\"-Repository enthalten.\nSeit der Veröffentlichung von Kanotix-Excalibur-2010 (siehe unten) gab es keine updates mehr.\n\nDie Version \"Kanotix-Excalibur\" basiert auf Debian 5.0 („Lenny“) und erfuhr gegenüber der Vorgängerversion einige grundsätzliche Neuerungen. Statt Knoppix wurde Debian-Live als Basis verwendet und der neue Bootloader GRUB2 eingesetzt. Die Kanotix-Excalibur.iso erschien in 7 Varianten: Im Gegensatz zu Kanotix-Thorhammer gab es neben der 32-Bit auch wieder eine 64-Bit-Version, sowie eine 2in1-ISO mit beiden Versionen. Des Weiteren gab es auch die drei Versionen mit einem KDE Plasma Desktop 4.3 und eine \"all-in-one\"-ISO mit allen Varianten.\n\nKanotix setzte auch bei \"Excalibur\" auf Funktionalität, so dass es bereits in der Grundausstattung produktiv eingesetzt werden konnte. Daher erschien beim offiziellen Release die Kanotix-Excalibur.iso nicht mehr als CD-Version, sondern ist mit deutlich mehr als 700 MB für den Start von USB-Stick (empfohlen) oder DVD konzipiert.\n\nDie unter dem Codenamen \"Hellfire\" entwickelte Kanotix-Version basiert auf Debian 6 (\"Squeeze\"). Kanotix-Hellfire brachte die KDE Software Compilation 4 Version 4.4.5 in einer 32-Bit und 64-Bit-Version aus Debian-Squeeze sowie ein neues Artwork. Wie bei Kanotix üblich wurden die enthaltenen Pakete des aktuellen Debian \"Stable\" Zweigs durch eine Reihe zum Releasezeitpunkt topaktueller Pakete ergänzt u.  a. den Kernel (Ubuntu – recompiled), LibreOffice und Kanos aktuelle Scripte.\nDie erste Version Kanotix-Hellfire 2011-03 erschien am 2. März 2011 die zweite Kanotix-Hellfire 2011-05 am 11. Mai 2011 zum LinuxTag 2011.\n\nWie bei den Vorgängerversionen waren auch bei Kanotix-Hellfire nach dem Release laufend Aktualisierungen aus dem eigenen Repository (Kernel, LibreOffice, Iceweasel, Pidgin, wine u.  a.) erhältlich. Mit der \"Hellfire\"-Version wurden erstmals die \"GFX-Overlays\"eingeführt, die eine Verwendung der binären 3D Grafiktreiber von Nvidia oder ATI bereits im Live Modus erlauben.\n\nNeben einer aktualisierten Version von \"Kanotix-Hellfire\" wurde auf dem Linuxtag 2012 die auf Debian 7 \"Wheezy\" basierende Version \"Kanotix-2012-Dragonfire\" vorgestellt.\nAlle Versionen wurden sowohl für 32-Bit als auch für 64-Bit angeboten, die 32-Bit-Versionen zusätzlich als sogenannte \"Mini\"-Versionen < 1 GB, wo umfangreiche Anwendungen wie LibreOffice, Wine u.  a. entfernt wurden.\nEine aktuelle und spezielle Version wurde während der CeBIT 2013 angeboten.\nDie neue stabile Version von Kanotix-Dragonfire wurde zum LinuxTag 2013 vorgestellt. Neben dem bei Kanotix traditionellen KDE-Desktop ist auch eine schlankere Version mit LXDE unter den Kanotix-Images. Daneben wird auch – ähnlich der speziellen \"CeBIT-2013-Version\" – ein \"KDE Spezial\" angeboten, das u. a. einen Steam-client sowie vorinstallierte 3D-Treiber für Nvidia und AMD enthält.\n\nAuf dem LinuxTag 2014 wurde neben einer aktuellen Version von Kanotix-Dragonfire als stabile Version auch ein \"Preview\" von Kanotix-Spitfire, welches auf Debian 8 \"Jessie\" basiert, vorgestellt.\nNach der Veröffentlichung von Debian 8 \"Jessie\" am 25. April 2015 wurden die \"Nightly Builds\", das sind tagesaktuelle snapshots für \"Kanotix-Spitfire\" (weiterhin auch \"Kanotix-Dragonfire\"), neu aufgesetzt und mit den stabilen Original-Kernelversion (3.16.7) von Debian versehen. Die \"Nightly Builds\" sind auch auf dem regulären Download-Server enthalten, sodass Kanotix de facto bzw. ausschließlich ein Rolling Release ist. Die neuesten Entwicklungen wurden auf der OpenRheinRuhr 2015 vorgestellt.\n\nSeit dem 27. August 2017, rund 2 Monate nach der Veröffentlichung von Debian 9 \"Stretch\" am 17. Juni 2017, gibt es in den \"nightlys\" und auf dem Mirror \"LXDE-Steelfire\" in der i386- und amd64-Variante. Die \"Nightly Builds\" sind auch auf dem regulären Download-Server enthalten. Die KDE-Variante wurde zur \"OpenRheinRuhr\" am 4.–5. November 2017 vorgestellt, eine aktualisierte Version zur \"OpenRheinRuhr 2018\" am 3./4. November 2018.\n\n\"Kanotix-Silverfire\" basiert auf Debian 10 \"Buster\". Ebenso wie die Debian-Basis ist Kanotix-Silverfire noch im \"Testing\"-Modus. Einige \"Previews\" mit den Desktop-Umgebungen KDE und LXDE, jeweils in der 32- und 64-Bit Variante sind auf der Kanotix-Homepage verlinkt.\n\nDie Distribution \"Parsix\" war ein Derivat von Kanotix, das im Gegensatz zu Kanotix auf die Desktop-Umgebung Gnome ausgerichtet war.\n\n\n"}
{"id": "187930", "url": "https://de.wikipedia.org/wiki?curid=187930", "title": "Dithering (Bildbearbeitung)", "text": "Dithering (Bildbearbeitung)\n\nDas Dithering (von [] ‚schwanken‘, ‚zittern‘) ist eine Technik in der Computergrafik, um bei Bildern, die aufgrund technischer Restriktionen mit verringerter Farbtiefe wiedergegeben werden müssen, die Illusion einer größeren Farbtiefe zu erzeugen. Dithering ist eine Art des Rasterns, es wird auch als Fehlerdiffusion bezeichnet.\n\nBei einem \"geditherten\" Bild werden die fehlenden Farben durch eine bestimmte Pixel-Anordnung aus verfügbaren Farben nachgebildet und dadurch harte Übergänge zwischen den Farben vermieden. Das menschliche Auge nimmt das Dithering als Mischung der einzelnen Farben wahr.\n\nDie häufigste Verwendung findet Dithering bei der Farbreduktion. In den Beispielen rechts sind Verläufe mit unterschiedlicher Anzahl an Farben zu sehen. Aus der Ferne betrachtet sind die Unterschiede kaum wahrnehmbar.\n\nFür das Dithering gibt es eine Vielzahl an unterschiedlichen Algorithmen, die oft auch mit einer Fehlerstreuung arbeiten. Auftretende Rundungsfehler oder Überläufe werden dabei auf benachbarte Bildpunkte übertragen, um eine feinere Darstellung zu erreichen. Beispiele sind:\n"}
{"id": "189165", "url": "https://de.wikipedia.org/wiki?curid=189165", "title": "UNIVAC I", "text": "UNIVAC I\n\nDer UNIVAC I (\"UNIV\"ersal \"A\"utomatic \"C\"omputer I) war der erste in den USA hergestellte kommerzielle Computer. Er wurde von John Presper Eckert und John William Mauchly von der Eckert-Mauchly Computer Corporation entwickelt und von der Computerfirma Remington Rand gebaut. \n\nDer erste UNIVAC wurde am 30. Mai 1951 an das United States Census Bureau ausgeliefert und am 14. Juni in Betrieb genommen. Am 1. Februar 1952 erhielt die US Air Force einen UNIVAC I. Den fünften erhielt CBS im Jahre 1952 zur Vorhersage der Präsidentschaftswahlergebnisse. Mit einer Stichprobengröße von nur sieben Prozent berechnete er das korrekte Ergebnis: Eisenhower wird mit großer Mehrheit gewinnen.\n\nDer UNIVAC I bestand aus 5.200 Röhren, 18.000 Kristall-Dioden und wog bis zu 13 Tonnen, benötigte eine elektrische Leistung von bis zu 125 Kilowatt und konnte 1905 Rechenoperationen pro Sekunde durchführen. Der Laufzeitspeicher aus Quecksilber umfasste 4,3 m × 2,4 m × 2,6 m. Die Zugriffszeit betrug 40 bis 404 µs.\nDas gesamte System benötigte eine Stellfläche von 35,5 Quadratmetern.\n\nMan verwendete Verzögerungsleitungen zur dynamischen Datenspeicherung. Sie bestanden aus einer Quecksilberröhre, an deren beiden Enden ein Schwingquarz angebracht war. Wurde der Quarz durch einen kurzen Stromstoß zum Schwingen angeregt, so pflanzten sich die erzeugten Ultraschallwellen im Quecksilber mit der Ausbreitungsgeschwindigkeit von zwei Kilometern pro Sekunde fort. Nach einer entsprechenden Verzögerungszeit erzeugte der Druckanstieg der Wellen am anderen Schwingquarz einen Spannungsanstieg. Wurde nun der sendende Quarz mit entsprechenden Stromstößen synchron zum Takt des Rechenwerks aufgefrischt (analog zum DRAM), so konnte ein serielles Bitmuster einer bestimmten Länge beliebig lange im Quecksilber umlaufen. Auf diese Weise erfolgt die Speicherung im Quecksilber durch Schallwellen.\n\nDer Hauptspeicher fasste 1000 Worte mit zwölf Dezimalstellen (inklusive einer Stelle für das Vorzeichen). Die Befehlsliste umfasste 45 verschiedene Befehle, wobei zwei Befehle jeweils in ein Wort codiert waren.\n\nDas amerikanische Bundesbüro zur Durchführung von Volkszählungen erhielt im Oktober 1954 eine zweite Maschine. Der anfängliche Preis von 159.000 US-Dollar kletterte bis auf 1,5 Millionen US-Dollar und war damit für die meisten Universitäten zu hoch. Es wurden lediglich drei Maschinen an US-amerikanische Universitäten gespendet.\n\nDer erste UNIVAC I in Deutschland wurde am 19. Oktober 1956 von Carl Hammer, Direktor des Frankfurter Battelle-Instituts, offiziell in Betrieb genommen. Die Einweihung des ersten europäischen Rechenzentrums von Remington Rand gilt als Start der kommerziellen Datenverarbeitung und besonders der DV-Ausbildung in Deutschland. Sie wurde im Schichtbetrieb von jeweils zwölf bis 20 Personen bedient. Freie Rechenkapazitäten an der UNIVAC I wurden damals für 1470 Mark pro Stunde (2014 ca. 3.500 Euro) vermietet.\n\nInsgesamt wurden 46 UNIVAC-I-Maschinen gebaut und ausgeliefert.\n\n\n"}
{"id": "189171", "url": "https://de.wikipedia.org/wiki?curid=189171", "title": "Universal Automatic Calculator", "text": "Universal Automatic Calculator\n\nDer Universal Automatic Calculator oder UNIVAC (nach anderen Quellen Universal Automatic Computer) ist ein Computer aus dem Jahr 1951, der von J. Presper Eckert und John W. Mauchly (beide aus der ENIAC-Gruppe) konstruiert wurde. Der UNIVAC nutzte erstmals als externen Speicher ein Magnetband. Der Ausdruck \"UNIVAC\" wurde 1951 stellvertretend für Computer verwendet. Bis heute wird die \"UNIVAC\"-Serie weiterentwickelt und betreut.\n\nNach dem Zweiten Weltkrieg waren nur wenige Menschen der Ansicht, dass Computer eine Zukunft haben würden. Die 1946 von J. Presper Eckert und John W. Mauchly gegründete Eckert-Mauchly Computer Corporation (\"EMCC\") war eine der wenigen Firmen, welche überzeugt war, dass elektronische Rechenanlagen \"universell\" eingesetzt werden können, also nicht nur für spezifische Berechnungen von komplexen mathematischen Problemen, sondern dass diese ‚universellen‘ Rechenmaschinen auch in der Wirtschaft einen Verwendungszweck finden werden.\n\nDie Eckert-Mauchly Computer Corporation arbeitete am BINAC, einer kleineren Version des ENIAC, welche für die Northrop Corporation gebaut wurde. 1948 bekam \"EMCC\" den Auftrag, einen Rechner für das US Census Bureau zu bauen, der für die Volkszählung 1950 bereit sein sollte. Wegen finanzieller Schwierigkeiten konnte der Termin nicht gehalten werden, und die Eckert-Mauchly Computer Corporation wurde am 15. Februar 1950 von Remington Rand übernommen. Die \"EMCC\" wurde als Geschäftseinheit \"UNIVAC\" in die Remington Rand Organisation eingebunden.\n\nAls 1951 die UNIVAC I dem US Census Bureau übergeben wurde, läutete sie die Ära kommerzieller elektronischer Rechenanlagen ein, und für einige Jahre wurde die Bezeichnung \"UNIVAC\" stellvertretend für alle Computer benutzt. Die UNIVAC war nach der Zuse Z4 der zweite kommerzielle Computer weltweit. Sie wurde einige Monate nach der Auslieferung der Z4 an die ETH Zürich installiert.\nBerühmt wurde der UNIVAC nach der Präsidentschaftswahlnacht im Jahr 1952. Mit ihm wurde eine Hochrechnung erstellt, basierend auf 7 % der ausgezählten Stimmen. Als Ergebnis sagte er um 9 Uhr abends einen Erdrutschsieg für Eisenhower voraus, im Widerspruch zu konventionell ermittelten Prognosen eines Kopf-an-Kopf-Rennens. Die Auftraggeber trauten der UNIVAC-Prognose nicht und beschlossen, sie nicht zu veröffentlichen. Später stellte sich heraus, dass sie recht genau war: Die vorausgesagte Wahlmännerverteilung von 438 für Eisenhower und 93 für Stevenson kam nah an die tatsächliche Verteilung von 442:89 heran. Dieses Ergebnis machte den UNIVAC weltweit bekannt.\n\n1955 fusionierte Remington Rand mit der Sperry Corporation zu Sperry Rand. Die Geschäftseinheit \"Univac\" blieb unverändert bei Sperry Rand bestehen. Alle ausgelieferten Rechner wurden weiterhin als \"UNIVAC\" bezeichnet. Erst im Jahre 1982 wurde der Name \"UNIVAC\" durch die Bezeichnung \"Sperry\" abgelöst. 1986 fusionierte Sperry mit der Burroughs Corporation zu Unisys. Ab diesem Zeitpunkt wurde die Rechnerserie in \"Unisys\" umbenannt.\n\nÜber all die Jahre wurde die Rechnerarchitektur weiterentwickelt. Angefangen bei Vakuumröhren über Transistoren bis zu CMOS wurde die für \"UNIVAC\"s spezifische Prozessorarchitektur beibehalten, auch die ab der UNIVAC-1100/2200- und OS2200-Serie gebräuchliche Eigenschaft, dass 1 Zeichen = 9 Bits = 1 Byte ist.\n\n\n\nUNIVAC Systeme von Sperry Rand / Sperry Univac basierend auf Transistoren. Noch heute werden diese Prozessoren durch die Unisys Clearpath Modelle unterstützt.\n\n\n1975 wurde das Speichersystem ausgetauscht und eine neue Namenskonvention eingeführt, wobei die letzten Zahlen der maximalen Anzahl Prozessoren oder \"CAU (Command Arithmetic Unit)\", respektive CPU, entsprach.\n\n\n1982 wurde die Bezeichnung \"UNIVAC\" fallen gelassen und die Rechner wurden mit \"Sperry\" bezeichnet.\n\n\nNach der Fusion 1986 von Sperry und Burroughs Corporation zu Unisys wurden die Rechner mit der Bezeichnung \"Unisys 2200\" später mit der Marke \"Unisys Clearpath\" betitelt.\n\n\nFolgende Grafik gibt Auskunft über die zeitliche Entstehungsgeschichte der Univac Rechner.\n\nUNIVAC als Unternehmen entstand durch die Zusammenlegung verschiedener Bereiche von Remington Rand, nämlich dem Bereich Tabelliermaschinen, dem Bereich für wissenschaftlich genutzte Computer und dem Bereich der kommerziell genutzten Computer (dieser stellte die UNIVAC her). Remington Rand wurde 1955 mit der Sperry vereint und wurde \"Sperry Rand\". Der Geschäftsbereich Computer wurde in \"Sperry UNIVAC\" umbenannt. 1978 verschwand der Begriff UNIVAC aus dem Firmennamen, das Unternehmen hieß nur noch Sperry. \n\nSperry wurde 1986 mit Burroughs verschmolzen und erhielt nach einem firmeninternen Wettbewerb zur Namenssuche den Namen Unisys.\n\nUNIVAC gehörte in den 1960er Jahren neben IBM, Burroughs, Scientific Data, Control Data Corporation, General Electric, RCA und Honeywell zu den acht großen Computerfirmen.\n\n\n"}
{"id": "189423", "url": "https://de.wikipedia.org/wiki?curid=189423", "title": "Lena (Testbild)", "text": "Lena (Testbild)\n\nLena (auch Lenna) ist eines der meistverbreiteten Testbilder in der Bildverarbeitung. Es kommt oft zum Einsatz, wenn zum Beispiel in Lehrbüchern oder Folienvorträgen die Funktionsweise bestimmter Algorithmen demonstriert wird, oder die Bildauflösung dargestellt werden soll. Infolgedessen hat Lena in Informatikerkreisen einen gewissen Kultstatus erreicht.\n\nDas Bild eignet sich unter anderem deshalb so gut als Testobjekt, weil es gleichermaßen aus großen, einfachen Flächen und Flächen mit vielen Details besteht. Es ist jedoch geringfügig durch Rauschen kontaminiert, so dass Tests und Bewertungen von Bildalgorithmen möglicherweise dadurch leicht beeinträchtigt werden, in denen Rauschen bzw. dessen statistische Eigenschaften eine Rolle spielt.\n\nDas ursprüngliche Lena-Bild stammt aus der US-amerikanischen November-Ausgabe des Männermagazins Playboy des Jahres 1972. Es zeigt das schwedische Playmate Lena Söderberg (vom Playboy „Lenna Sjööblom“ genannt). Playboy hat sich für dieses Bild ausnahmsweise dazu entschieden, Urheberrechtsverletzungen nicht zu verfolgen.\n\nDas Original des als Testbild verwendeten Ausschnitts liegt in der Bilder-Datenbank des Signal & Image Processing Institute der University of Southern California (USA). Es misst 512 × 512 Pixel und hat eine Größe von 768 KB.\n\nÜber die dargestellte Lena Söderberg ist bekannt, dass sie am 21. März 1951 in Schweden geboren wurde, 1988 durch einen Journalisten einer Computerzeitschrift von der Zweckentfremdung ihres Fotos erfuhr und 1997 zum 50. Geburtstag der Konferenz der \"Society for Imaging Science and Technology\" eingeladen wurde, wo sie Autogramme gab und mit den Besuchern für Erinnerungsfotos posierte. Einen ähnlichen Auftritt hatte Lena Söderberg auf der IEEE Conference on Image Processing 2015, auf der sie Publikationspreise des IEEE an die Preisträger überreichte und für Fotos posierte. Sie ist verheiratet und hat drei Kinder.\n\n\n"}
{"id": "191297", "url": "https://de.wikipedia.org/wiki?curid=191297", "title": "GIMP", "text": "GIMP\n\nGIMP (\"GNU Image Manipulation Program\") ist ein pixelbasiertes Grafikprogramm, das Funktionen zur Bildbearbeitung und zum digitalen Malen von Rastergrafiken beinhaltet. Das Programm ist eine freie Software und kann kostenlos genutzt werden.\n\nDie erste öffentliche Testversion von GIMP wurde von Peter Mattis am 21. November 1995 auf der Mailingliste \"comp.os.linux.development.apps\" angekündigt. Zusammen mit seinem Kommilitonen Spencer Kimball hatte er das Programm als Studienarbeit entwickelt. Die erste Betaversion wurde für Linux (1.2.13), Solaris (2.4), HP-UX (9.05) und IRIX bereitgestellt. Die erste offizielle Version 0.54 erschien im Januar 1996.\n\nBereits zu diesem Zeitpunkt war GIMP ein umfangreiches Programm, das unter anderem mit einem Plug-in-System, beliebigem Rückgängigmachen und Wiederholen, intelligenter Schere, Dithering, Unterstützung von 8, 15, 16 und 24 Bit Farbtiefe pro Bild, Zoom und Verschieben in Echtzeit, simultaner Bearbeitung mehrerer Bilder, Unterstützung der Formate GIF, JPEG, PNG, TIFF und XPM sowie vielen Auswahl- und Bearbeitungswerkzeugen aufwarten konnte. Das Programm galt jedoch anfänglich als fehlerbehaftet und absturzfreudig. Zudem verwendete es damals noch das proprietäre Motif-Toolkit für die Bedieneroberfläche, für welches die nötigen Header-Dateien nicht frei und kostenlos zur Verfügung standen. Deshalb konnte es von vielen Benutzern nur als statisch gelinktes und fertig kompiliertes Programm verwendet werden; selbst an GIMP zu arbeiten und das Ergebnis sofort zu nutzen, war ihnen nur schwer oder gar nicht möglich.\n\nAls Peter Mattis von Motif auf eine freie Lösung umsteigen wollte, entwickelte er sein eigenes Toolkit GTK+, das inzwischen als offenes Projekt ein Eigenleben führt und beispielsweise in der Desktop-Umgebung Gnome verwendet wird. Damit konnte GIMP völlig ohne Motif-Aufrufe umgesetzt werden, und es war relativ einfach, das Programm auf andere Systeme zu portieren.\n\nGIMP erlangte vor allem unter Linux eine große Verbreitung und etablierte sich dort im Laufe der Zeit als Marktführer für digitale Bildbearbeitung. Das Programm ist Bestandteil vieler Linux-Distributionen, die nicht konsequent auf GTK basierende Programme verzichten. Allerdings wurde auf dem \"Ubuntu Developer Summit\" nach der Veröffentlichung von Ubuntu 9.10 beschlossen, in Ubuntu 10.04 GIMP nicht mehr vorzuinstallieren, da es „zu kompliziert“ sei, es zu viel Platz auf der Installations-CD benötige und bereits F-Spot mitgeliefert werde. Außerdem könne man es weiterhin über die Software-Repository nachladen.\n\nGIMP unterstützt mehr als dreißig Dateiformate.\n\nDie Version 1.0 von GIMP wurde am 5. Juni 1998 veröffentlicht. Die bedeutendsten Neuerungen waren eine neue Programmierschnittstelle und eine Prozedurendatenbank, welche es ermöglichten, GIMP mittels einfacher Skripte zu erweitern. Mit dieser \"Skript-Fu\" genannten Funktion konnten nun Abläufe automatisiert werden. Zusätzlich gab es eine neue Speicherverwaltung, mit der das Laden großer Bilddateien kein Problem mehr darstellte. Außerdem wurde mit dieser Version das GIMP-eigene Dateiformat XCF eingeführt.\n\nAm 25. Dezember 2000 gaben die Programmierer die Version 1.2 von GIMP frei. Die Neuerungen gegenüber der stabilen Variante 1.0 hielten sich in Grenzen; neben Fehlerbereinigungen wurde vor allem die Benutzeroberfläche überarbeitet.\n\nNach einer langen Pause zwischen den Veröffentlichungen wurde am 24. März 2004 schließlich GIMP 2.0 mit vielen Verbesserungen freigegeben. Die wichtigsten Änderungen waren die strikte Trennung von Programmlogik und Benutzungsoberfläche sowie eine einfache CMYK-Umsetzung. Damit verfügte GIMP erstmals, wenn auch nur eingeschränkt und in einfachem Umfang, über eine Druckvorstufe. Weiterhin wurden die Menüs überarbeitet und die Übersichtlichkeit verbessert. In jedem Bildfenster befand sich jetzt eine Menüleiste. An neuen Funktionen bot das Programm bessere Pfad- und Text-Werkzeuge. Es beinhaltete jetzt auch Import- und Exportfunktionen für SVG.\n\nGIMP 2.2 wurde am 20. Dezember 2004 veröffentlicht. Die wichtigste Änderung der neuen Version stellte eine verbesserte Benutzeroberfläche dar. So haben zahlreiche Werkzeuge von GIMP nun eine Vorschaufunktion. Während GIMP 2.0 noch für Drehungen und Verzerrungen nur ein Gitter verwendete, um Aktionen des Benutzers zu visualisieren, drehte und verzerrte GIMP 2.2 den gewählten Bereich simultan. Ferner wurde eine Vielzahl von Dialogen an die \"GNOME Human Interface Guidelines\", die Richtlinien zur Gestaltung der Benutzeroberfläche von GNOME-Programmen, angeglichen. Zu den Verbesserungen der grafischen Oberfläche gehörte auch eine bessere Zusammenarbeit mit anderen Anwendungen.\n\nGIMP 2.4 wurde am 24. Oktober 2007 veröffentlicht. Neuerungen der 2.4er-Reihe sind insbesondere die Nutzung von ICC-Profilen, ein Werkzeug zum automatischen Freistellen von Objekten, verbesserte Skalierung durch Einsatz des Lanczos-Filters sowie die Möglichkeit, Text an Pfaden entlanglaufen zu lassen. Weiterhin können jetzt auch länger zurückliegende Änderungen zurückgenommen werden, ohne dass alle dazwischen vorgenommenen Änderungen verlorengehen – eine von Grafikern oft genutzte Funktion. Obendrein wurde die Benutzeroberfläche umfassend überarbeitet.\n\nMit der am 1. Oktober 2008 erschienenen Version 2.6 wurde der Umstieg vom bisherigen Grafik-Kernel auf die unabhängige Bibliothek GEGL (\"Generic Graphics Library\") begonnen. Zum Freigabetermin waren noch nicht alle Funktionen portiert, so dass die erhoffte Unterstützung höherer Farbtiefen fehlte. Die Benutzeroberfläche wurde erneut verändert und ein permanentes Bildfenster implementiert, was vor allem den Anwendern mit weniger ausgefeilten Fenstermanagern zugutekommen sollte.\n\nGIMP 2.8 erschien am 3. Mai 2012. Seit dieser Version gibt es einen optionalen Ein-Fenster-Modus. Die klassische, besonders bei Mehrmonitorbetrieb bewährte Mehrfenstervariante bleibt als Standard erhalten. GIMP speichert seit dieser Version nur noch in das eigene XCF-Format, für andere Formate existiert seitdem eine Export-Funktion. Ebenen können in Gruppen eingeteilt werden. Texte werden nicht mehr in einem Extra-Fenster editiert, sondern direkt auf der Zeichenfläche („Leinwand“). Die meisten Werkzeuge auf der Leinwand werden mit Cairo dargestellt. Die Version 2.8.2 läuft erstmals ohne X11-Server nativ unter macOS. Seit 2016 wird die Version 2.8 nur noch mit Fehlerkorrekturen und kleinen Änderungen versorgt.\n\nGimp 2.9 war die Entwicklungsversion für 2.10. Zuletzt brachte die Version 2.9.8 einige Verbesserungen bei Gradienten, Clips und Importen.\n\nDie aktuelle stabile Version GIMP 2.10 baut vollständig auf GEGL auf. Die maximale Dateigröße von XCF-Dateien wurde über 4 GB erweitert (seit 2.9.6). Die Zahl der maximalen Threads ist von 16 auf 64 gestiegen. Im Jahr 2019 werden weitere Versionen von 2.10 erscheinen und die Verbesserungen des Jahres 2018 fortsetzen.\n\nGimp 3.0 wird als GTK3-Port entwickelt. Zurzeit ist GTK 3.24 minimale Voraussetzung für die Linux- bzw. Unix-Varianten von Gimp 3.0. Die Windows- und auch Mac-Versionen werden auch von den neuen Möglichkeiten wie repariertem Wacom-Tablet-Support in GTK3 profitieren. Aktuell zum Jahr 2019 ist die Entwicklerversion 2.99 parallel zu 2.10 erschienen. Und spätestens 2020 soll dann 3.0 erscheinen.\n\nGimp 3.2 soll dann nicht destruktives Editieren voll unterstützen und damit anspruchsvolles UNDO und REDO beherrschen. Die Entwicklung kann starten, wenn GEGL in 2.10 finalisiert wurde.\n\nDie Bearbeitungsfunktionen sind über Werkzeugleisten, Menüs und dauerhaft eingeblendete Paletten (in GIMP \"Dialog\" genannt) zu erreichen. Diese enthalten sogenannte \"Filter\" für grafische Effekte, zudem Pinsel sowie Umwandlungs-, Auswahl-, Ebenen- und Maskierungsfunktionen. Zum Standardumfang gehören eine Reihe von Pinsel, weitere lassen sich erzeugen, zudem sind alle bezüglich Kantenschärfe und Deckung einstellbar.\n\nGIMP hat Farbpaletten für RGB, HSV, CMYK, ein Farbrad sowie Funktionen, um Farben aus einem Bild zu entnehmen (Pipette). Auch eine direkte Eingabe der hexadezimalen Farbwerte aus HTML ist möglich. Auch wenn das Programm eine CMYK-Palette anbietet, arbeitet es immer in RGB mit einer Farbtiefe bis 8 Bit (ab 2.10 auch 16bit). Es unterstützt außerdem Muster, die direkt auf eine Fläche aufgetragen werden können. Auch diese lassen sich weitgehend anpassen, so dass auch Zwischenfarben möglich sind.\n\nGIMP besitzt Auswahlfunktionen für rechteckige, runde und freiförmige Bereiche sowie nach Farbe und nach starken Farbkanten. Daneben kennt das Programm Ebenen, die sich ausblenden oder in der Deckung verändern lassen. Auch eine direkte Beeinflussung der einzelnen Farbkanäle ist möglich.\n\nNahezu alle Vorgänge in GIMP können durch sogenannte \"GIMP-Skripte\" automatisiert werden. Diese Art von Programmen kann durch den eingebauten Scheme-Interpreter (Skript-Fu) sowie über eine externe Anbindung von Perl, Python oder Tcl verarbeitet werden. Die Unterstützung von in Ruby geschriebenen GIMP-Skripten befindet sich noch in einem experimentellen Stadium. Makros können daher mit den oben genannten Programmiersprachen erstellt werden, entsprechende Schnittstellen bzw. Bindings sind vorhanden. Auf diese Weise erstellte Skripte und Plug-ins für GIMP können interaktiv sowie im Batch-Modus ausgeführt werden, das heißt ohne Interaktion eines Benutzers. Wiederkehrende (auch komplexe) Bildbearbeitungsvorgänge können so automatisiert werden. Grafiken für eine Webseite können beispielsweise direkt über CGI-Skripte erzeugt werden, man kann bei einer großen Anzahl an Bilddateien eine Farbkorrektur vornehmen oder das Grafikformat ändern. Hinweise zur Verwendung solcher Skripte finden sich im GIMP-Wikibook.\n\nFür zukünftige Ausgaben von \"GIMP\" ist vor allem die alleinige Nutzung einer ursprünglich eigens für GIMP entwickelten \"übergeordneten Grafikbibliothek\" (englisch \"\", kurz \"GEGL\") geplant. Diese Bibliothek bietet neben der bisherigen Farbtiefe von 8 Bit pro Farbkanal auch 16-Bit-Farbwerte sowie einen Modus mit 32-Bit-Gleitkomma-Genauigkeit, welcher beispielsweise bei der Bearbeitung von HDR-Aufnahmen von Bedeutung ist. Zusätzlich ist mit GEGL neben der bisherigen RGB-Darstellung mit CMYK und Lab die Unterstützung weiterer Farbräume geplant. Um dies umzusetzen, werden die Bearbeitungsfunktionen in der Bibliothek implementiert und rechnen intern mit einer Genauigkeit von 32-Bit Gleitkomma pro Farbkanal. GEGL arbeitet nicht-destruktiv und soll weitere Dateiformate, zum Beispiel Rohdatenformate von Digitalkameras, unterstützen. Die Auslagerung der Bildbearbeitung in eine eigene Bibliothek ermöglicht die Nutzung der Funktionen in anderen Programmen – entsprechend gibt es bereits Anbindungen für C#, Ruby und Python. (Gegl-Sharp, GEGL-Vala, Pygegl obsolet seit April 2012)\n\nWeiterhin geplant sind eine druckempfindliche Sprühdose (für Grafiktabletts), das Laden von Photoshop-Pinseln und höhere HIG-Konformität.\n\nAb Version 2.10 werden mehrere Prozessorkerne (zurzeit max. 64 Threads) unterstützt werden. Die Wirkung von Filtern soll nicht mehr in einem kleinen Vorschaubild im Filterfenster gezeigt werden, sondern live im Hauptfenster zu sehen sein. Außerdem wird auch die GPU für die Berechnungen herangezogen, dafür haben die Entwickler OpenCL integriert, auf die GEGL zugreifen kann.\n\nDas vergleichbare Photoshop Elements hat diese geplanten Funktionen zwischen 2007 und 2012 eingeführt.\n\nUnter dem Namen CinePaint (früher \"Film Gimp\") ist ein professioneller Ableger des Programmes entstanden, der sich durch größere Farbtiefen und ein Farbmanagement auszeichnet. Diese Version wird in der Filmindustrie verwendet und hat dort die eingestellte IRIX-Version von Adobe Photoshop ersetzt. Letzte Version ist 1.0-4 aus dem Jahr 2013. Seitdem findet keine signifikante Entwicklung mehr statt.\n\nGIMPshop war eine Modifikation des GIMP, die v. a. in Erscheinungsbild und Bedienung Adobe Photoshop angeglichen war. Der ursprüngliche Entwickler Scott Moschella wollte damit langjährigen Nutzern des kommerziellen Photoshops den Umstieg erleichtern. Die Entwicklung ist seit 2006 und der Version 2.2.11 (auf GIMP 2.2.11 bezogen) ausgesetzt. Zuletzt wurde die Version 2.8 freigegeben (vorerst nur für Windows und OS X). Unter der ursprünglichen Downloadadresse wird mittlerweile Malware verbreitet.\n\nIn der Tradition von GIMPshop steht GimPhoto, das ebenfalls mit einer Photoshop-ähnlicheren Bedienung aufwartet. Es kann über die kleine Anwendung \"GimPad\" noch mehr an individuelle Bedürfnisse im Erscheinungsbild angepasst werden. Die aktuelle Version von GimPhoto 24.1 kann mit dem aktualisierten Installer auf Windows 8.1 und damit auch auf Windows 7 und 10 installiert werden und basiert wie die Vorgängerversion 1.4.3 auf dem veralteten GIMP 2.4.3 aus dem Jahr 2008. Selbst ein relativ einfaches Update auf die letzte Version 2.4.7 der Gimp-2.4-Reihe mit über 100 Fehlerbereinigungen und Verbesserungen gegenüber 2.4.3 fehlt. Für Linux steht ebenfalls die auf Gimp 2.4.3 basierende Version für Ubuntu 14, Fedora und als universelle Quelle zum Kompilieren auf anderen Unix-Versionen bereit. Für Mac OS X 10.6+ steht die Version 26.1 basierend auf der Gimp-Version 2.6.8 zur Verfügung. Da zurzeit nur ein Entwickler die Software pflegt, ist nicht mit schnellen Updates und mit keiner schnellen Weiterentwicklung basierend auf Gimp 2.8.2x oder 2.9.x zu rechnen.\n\n\n"}
{"id": "192736", "url": "https://de.wikipedia.org/wiki?curid=192736", "title": "Histogramm", "text": "Histogramm\n\nEin Histogramm ist eine grafische Darstellung der Häufigkeitsverteilung kardinal skalierter Merkmale. Es erfordert die Einteilung der Daten in Klassen (), die eine konstante oder variable Breite haben können. Es werden direkt nebeneinanderliegende Rechtecke von der Breite der jeweiligen Klasse gezeichnet, deren Flächeninhalte die (relativen oder absoluten) Klassenhäufigkeiten darstellen. Die Höhe jedes Rechtecks stellt dann die (relative oder absolute) Häufigkeitsdichte dar, also die (relative oder absolute) Häufigkeit dividiert durch die Breite der entsprechenden Klasse.\n\nAnwendung finden Histogramme in der beschreibenden Statistik und in der Bildverarbeitung. Man verwendet Histogramme beispielsweise dann,\nIn der physikalischen Forschung oder angewandten Gebieten (z. B. Analytik) werden gemessene Spektren als Histogramme dargestellt, siehe z. B. Vielkanalanalysator.\n\nFolgende Schritte sind bei der Konstruktion eines Histogramms nötig:\n\nZur Konstruktion eines Histogramms wird der Wertebereich der Stichprobe in \"k\" aneinandergrenzende Intervalle geteilt, die sogenannte Klassen. Dabei ist darauf zu achten, dass die Randklassen nicht offen sind. Das heißt, die erste und die letzte Klasse müssen eine untere bzw. obere Grenze besitzen. Die Klassen müssen nicht gleich breit sein. Allerdings vereinfachen zumindest im Mittelbereich gleich große Klassen die Interpretation. Über jede Klasse wird dann ein Rechteck errichtet, dessen Fläche proportional zur jeweiligen Klassenhäufigkeit ist. Im Histogramm entsprechen diese Klassen der Breite der einzelnen Rechtecke.\n\nBei der Erstellung eines Histogramms gibt es zwei Vorgehensweisen: Die Klassenhäufigkeit spiegelt entweder einen absoluten oder einen relativen Wert wider. Der absolute Wert entspricht der Anzahl an Werten, die zu einer Klasse gehören. Der relative Wert hingegen drückt aus, wie viel Prozent der Werte einer Klasse angehören. Je nach Anwendungsfall kann sowohl das Arbeiten mit absoluten als auch mit relativen Werten Vorteile mit sich bringen. Im Histogramm entspricht die Klassenhäufigkeit dem Flächeninhalt der Rechtecke.\n\nDa die Fläche des j-ten Rechtecks gleich der Klassenhäufigkeit n ist, errechnet sich die Höhe des Rechtecks, die sogenannte Häufigkeitsdichte h, als Quotient n/d der Klassenhäufigkeit n durch die Klassenbreite d. Dies wird unmittelbar klar, wenn man sich überlegt, dass die Fläche eines Rechtecks das Produkt aus Breite (Klassenbreite) und Höhe (Häufigkeitsdichte) ist. Die Klasse mit der größten Häufigkeitsdichte wird Modalklasse genannt. Sind die Klassen gleich breit, so sind Häufigkeitsdichte und absolute bzw. relative Häufigkeiten proportional zueinander. Die \"Höhen\" der Rechtecke sind in diesem Fall vergleichbar und (unter Beachtung der Klassenbreite als Proportionalitätsfaktor) als Häufigkeit interpretierbar.\n\nOft werden die ermittelten Klassenhäufigkeiten beim Wiederholen der Datenerfassung streuen. So stellt sich, beispielsweise bei einer Wahlprognose, die Frage nach der Präzision der erhobenen Zahlen. Die zu erwartende Schwankungsbreite der Klassenhäufigkeit strebt bei unbegrenzt wachsender Anzahl der Klassen gegen formula_1\n\nUm ein Histogramm zeichnen zu können, muss eine genügend große Anzahl an Messwerten einen sinnvollen Verlauf ergeben. Eine falsche Einteilung der Klassen kann zu einer Fehlinterpretation des Histogramms führen. Für die Festlegung der Anzahl der Klassen bzw. Rechtecke existieren verschiedene Faustregeln:\n\nGegebenenfalls kann man die Anzahl der Balken formula_2 auch nach der Sturges-Regel berechnen:\nDie Sturges-Regel sollte aber nicht mehr verwendet werden, weil sie zum einen die Streuung nicht berücksichtigt. Zum anderen wählt sie die Klassenzahl zu klein für formula_4 selbst im Fall einer (idealen) normalverteilten wahren Dichte.\n\nAlternativ kann die Klassenbreite formula_5 mit der Regel nach Scott\n\noder der Regel nach Freedman und Diaconis\n\nberechnet werden. Dabei sind formula_8 die Standardabweichung, formula_9 die Anzahl der Messungen und formula_10 der Interquartilsabstand.\n\nDie Regel nach Scott ist so nur für normalverteilte Daten definiert. Für andere Fälle führte Scott Korrekturfaktoren in Abhängigkeit von Schiefe und Exzess ein.\n\nEin Histogramm ist eine \"flächenproportionale\" Darstellung der vorliegenden Häufigkeiten. Die Fläche eines Rechtecks entspricht formula_11, wobei formula_12 die relative Klassenhäufigkeit der Klasse formula_13 und formula_14 ein Proportionalitätsfaktor ist.\n\nIst formula_14 gleich dem Stichprobenumfang, das heißt formula_16, so ist die Fläche eines jeden Rechtecks gleich der absoluten Klassenhäufigkeit. Das Histogramm wird in diesem Fall, in dem die Summe der Flächeninhalte der Rechtecke dem Stichprobenumfang n entspricht, \"absolut\" genannt. Werden zur Konstruktion des Histogramms exakt die relativen Klassenhäufigkeiten verwendet (formula_17), wird das Histogramm als \"relativ\" oder normiert bezeichnet. Da die Flächeninhalte der Rechtecke nun den relativen Klassenhäufigkeiten entsprechen, summieren sich die Flächeninhalte in diesem Fall zu 1.\n\nBei einem Histogramm grenzen die Rechtecke im Gegensatz zum Säulendiagramm direkt aneinander, das heißt, es existieren keine Abstände zwischen ihnen. Denn die Breite der Rechtecke entspricht den gebildeten Intervallen (Klassen), die ebenfalls direkt aneinandergrenzen.\n\nIm Unterschied zum Säulendiagramm muss bei einem Histogramm die x-Achse immer eine Skala sein, deren Werte geordnet und gleichabständig sind.\n\nDrei Kennzeichen eines Histogramms können zu dessen Beurteilung herangezogen werden:\n\nEs liegen für 32 europäische Länder als Indikator für den Wohlstand die Zahlen der PKWs pro 1000 Einwohner vor. Die Werte werden wie folgt in Klassen eingeteilt:\nMit Hilfe der Tabelle erhält man das folgende Histogramm:\n\nAuf der Abszisse werden die Klassengrenzen und Klassenmittel abgetragen. In der Regel gibt man bei einem Histogramm die Ordinate nicht an, weil sonst die Gefahr besteht, die Höhe eines Rechtecks anstatt seiner Fläche als Häufigkeit zu interpretieren. Sind dagegen alle Klassen gleich breit, kann man für die Höhe der Rechtecke die Klassenhäufigkeit n verwenden und diese auf der Ordinate abtragen.\n\nDas linke Bild zeigt vier Histogramme für den gleichen Datensatz. Zwar sind die Klassenbreiten in jedem Histogramm gleich 2,0, jedoch verschiebt sich der Beginn der ersten Klasse (-6.0, -5.5, -5.0 und -4.5). Obwohl jeweils der gleiche Datensatz benutzt wurde, kommen doch unterschiedliche Histogramme heraus.\n\nNeben dem Problem der Klassenanzahl bzw. Klassenbreite spielt also auch die Wahl der (linken) Klassengrenzen eine Rolle. David Scott hat daher das Average-Shifted-Histogramm vorgeschlagen.\n\nIm rechten Bild wurden die vier Histogramme überlagert und dann für jeden Wert formula_18 die Histogrammhöhen gemittelt. Dies ergibt das Average-Shifted-Histogramm. Üblicherweise werden deutlich mehr als vier Histogramme überlagert und gemittelt.\n\nDas Average-Shifted-Histogramm löst das Problem der Wahl der (linken) Klassengrenzen, jedoch nicht das Problem der Wahl der optimalen Klassenbreiten.\n\nEinzuordnen ist das Average-Shifted-Histogramm zwischen dem Histogramm und der Kerndichteschätzung.\n\nIn der digitalen Bildverarbeitung versteht man unter einem Histogramm die statistische Häufigkeit der Grauwerte bzw. der Farbwerte in einem Bild. Das Histogramm eines Bildes erlaubt eine Aussage über die vorkommenden Grau- bzw. Farbwerte und über Kontrastumfang und Helligkeit des Bildes. In einem farbigen Bild kann entweder ein Histogramm über alle möglichen Farben oder Histogramme über die einzelnen Farbkanäle erstellt werden. Letzteres ist meist sinnvoller, da die meisten Verfahren auf Grauwertbildern basieren und so die sofortige Weiterverarbeitung möglich ist. Die Anzahl der Farbkanäle in einem Bild ist abhängig vom Modus, das heißt pro Farbauszug gibt es einen Kanal. Daher haben CMYK-Bilder vier Farbkanäle, RGB-Farbbilder nur drei.\n\nEin Histogramm visualisiert die Verteilung der Helligkeitswerte eines Bildes. Über einer Achse, die den Wertebereich der Farbwerte darstellt, sind als Balken die einzelnen Häufigkeiten des Vorkommens der Farbwerte aufgetragen. Je höher der Balken über einem Farbwert ist, desto häufiger kommt dieser Farbwert im Bild vor.\n\nHistogramme findet man häufig im Bereich der digitalen Fotografie. Gut ausgestattete digitale Fotoapparate zeigen auf dem Display während der Motivsuche als Hilfe für ein ausgewogeneres Bild in Echtzeit oder für bereits gespeicherte Aufnahmen ein Histogramm an. Das Betrachten eines Histogramms erlaubt es dem Fotografen, das Ergebnis oder das geplante Foto genauer zu kontrollieren, als es das Kameradisplay erlaubt. Zum Beispiel kann man typische Fehler wie Unter- und Überbelichtung erkennen und diese durch entsprechende Belichtungskorrektur beheben. Da die Helligkeit und vor allem der Kontrastumfang des Bildes bei der späteren Bearbeitung und Verwertung eine große Rolle spielen, lohnt es sich beim Fotografieren, auf die Histogrammanzeige zu achten.\n\nEine klassische Anwendung von Histogrammen in der Bildverarbeitung liegt in der Egalisierung (Äqualisierung, ), bei der das Histogramm mit einer Egalisierungsfunktion transformiert wird. Dadurch kann eine bessere Verteilung der Farbgebung erreicht werden, die über eine bloße Kontrastverstärkung hinausgeht.\n\nBei Low-key-Aufnahmen konzentrieren sich die Details in den niedrigen Tonwerten. Der Ausschlag ist demnach im unteren Bereich am stärksten. (Es liegen viele Pixel mit niedrigen Tonwerten vor.)\n\nFür High-key-Aufnahmen gilt das Gegenteil, also viele Pixel mit hohen Tonwerten und kaum ein Ausschlag in den niedrigen Tonwerten.\n\nBei überbelichteten Aufnahmen „schmiegt“ sich die Wahrscheinlichkeitskurve an der rechten (hellen) Seite an und das Maximum wird möglicherweise gar nicht erreicht. Es werden also nicht alle hellen Details wiedergegeben, da ein bestimmter Helligkeitsbereich abgeschnitten ist und der darunterliegende als weiß definiert wird.\n\nWohl erstmals tauchte ein Histogramm 1786 in der Arbeit \"The Commercial and Political Atlas\" des um 1800 lebenden schottischen Ingenieurs und Volkswirts William Playfair auf, der zuvor auch das Balken- und Tortendiagramm einführte. Im Jahr 1833 verwendete auch der Franzose André-Michel Guerry Histogramme zur Visualisierung von Daten. Weiterentwickelt wurde das Histogramm durch den belgischen Statistiker und Sozialwissenschaftler Adolphe Quetelet um 1846. Der Begriff \"„histogram“\" (\"historical diagram\") wurde jedoch erstmals vom englischen Mathematiker Karl Pearson im Jahr 1891 in einer Vorlesungsreihe genutzt und schließlich 1895 in seiner heutigen Bedeutung eingeführt.\n\n\n"}
{"id": "192745", "url": "https://de.wikipedia.org/wiki?curid=192745", "title": "Matlab", "text": "Matlab\n\nMatlab (Eigenschreibweise: MATLAB) ist eine kommerzielle Software des US-amerikanischen Unternehmens MathWorks zur Lösung mathematischer Probleme und zur grafischen Darstellung der Ergebnisse. Matlab ist vor allem für numerische Berechnungen mithilfe von Matrizen ausgelegt, woher sich auch der Name ableitet: \"MATrix LABoratory\".\n\nMatlab wurde Ende der 1970er Jahre von Cleve Moler an der Universität New Mexico entwickelt, um den Studenten die Fortran-Bibliotheken LINPACK und EISPACK für lineare Algebra von einer Kommandozeile aus ohne Programmierkenntnisse in Fortran zugänglich zu machen. Zusammen mit Jack Little und Steve Bangert gründete Moler 1984 \"The MathWorks\" und machte Matlab zu einem kommerziellen Produkt, das zusammen mit einer ersten Funktionssammlung, der \"Control System Toolbox\", vor allem in der Regelungstechnik viele Anwender fand. Die akademische Bindung ist in der Entwicklung und im Vertrieb von relativ preisgünstigen Studentenversionen bis heute erhalten geblieben und war möglicherweise auch die Grundlage für den Erfolg der Software neben anderen numerischen Plattformen wie MatrixX. Die von Matlab verwendeten Bibliotheken LINPACK und EISPACK wurden im Jahr 2000 durch die ebenfalls in Fortran geschriebenen freien Bibliotheken LAPACK und BLAS ersetzt.\n\nMatlab dient im Gegensatz zu Computeralgebrasystemen nicht der symbolischen, sondern vorrangig der numerischen (zahlenmäßigen) Lösung von Problemen. Die Software wird in der Industrie und an Hochschulen vor allem für numerische Simulation sowie Datenerfassung, Datenanalyse und -auswertung eingesetzt. Ein weiterer Anwendungsschwerpunkt sind die Wirtschaftswissenschaften, für die Mathworks Erweiterungspakete (z. B. Ökonometrie und Finanzmarkttheorie) bereitstellt.\n\nMatlab ist auch die Basis für Simulink, ein anderes Produkt des Unternehmens The MathWorks, das zur zeitgesteuerten Simulation dient, und Stateflow, das für die ereignisorientierte Simulation benutzt wird, sowie für zahlreiche anwendungs- und domänenspezifische Erweiterungen.\n\nProgrammiert wird unter Matlab in einer proprietären Programmiersprache, die auf der jeweiligen Maschine (Computer) interpretiert wird. Kleinere Programme können als sogenannte Skripte oder Funktionen zu geschlossenen Einheiten verpackt werden, was das Erstellen von anwendungsorientierten „Werkzeugkisten“ (\"Toolboxes\") erlaubt.\n\nViele solcher Pakete sind auch kommerziell erhältlich. Durch die vereinfachte, mathematisch orientierte Syntax der Matlab-Skriptsprache und die umfangreichen Funktionsbibliotheken für zum Beispiel Statistik, Signal- und Bildverarbeitung ist die Erstellung entsprechender Programme wesentlich einfacher möglich als z. B. unter C. Ein Beispiel ist die Symbolic Toolbox zur Nutzung symbolischer Ausdrücke im Gegensatz zu mit Zahlen belegten Variablen. Ferner gibt es Schnittstellen, um C-Code einzubinden, sowie einen Compiler, mit dem aus einem Skript unabhängig von Matlab lauffähiger C-Code erstellt werden kann. Damit können mathematisch aufwendige Module für C-Projekte in der Matlab-Umgebung entwickelt und getestet werden.\n\nMatlab bietet aus der objektorientierten Programmierung die Konzepte von Klassen, Vererbung, Pakete und Call-by-value-Aufrufen.\n\nMatlab besteht neben der Sprache Matlab aus einer grafischen Desktop-Umgebung, um verschiedene Ansichten wie Variablen, Plots und Code auf einen Blick sehen und viele Aufgaben durch Mausinteraktion und Tastaturkürzel bewältigen zu können.\n\nMatlab kann Funktionen etwa in C oder Fortran aufrufen. Dazu muss eine Adapter-Funktion (die sogenannte \"mexFunction\") enthalten sein, die die Übergabe von Parametern und Rückgabewerten steuert.\n\nBibliotheken in Java, ActiveX oder .NET können direkt aus Matlab aufgerufen werden. Viele Bibliotheken in Matlab, wie beispielsweise jene für die Anbindung von XML oder SQL, sind als Adapter um Java oder ActiveX aufgebaut. Über den Matlab-Compiler und sogenannte Builder-Addons kann auch die umgekehrte Richtung genutzt werden und man kann aus JAVA oder .NET heraus Funktionen und Code in Matlab aufrufen.\n\nAls Alternative zur MuPAD-basierten \"Symbolic Math Toolbox\" (ebenfalls von MathWorks) kann Matlab auch an Maple oder Mathematica angeschlossen werden.\n\nDa Matlab als Sprache ab Version 6 auf den quelloffenen Bibliotheken LAPACK und BLAS basiert, existieren mittlerweile mehrere kostenlose Alternativen zu Matlab mit gleicher numerischer Qualität. Diese Alternativen fokussieren sich oft auf die Möglichkeiten als Programmiersprache. Vom französischen INRIA (Institut National de Recherche en Informatique et en Automatique) stammt die Alternative Scilab/Xcos. Im Bereich der \"freien Software\" gibt es mehrere quelloffene Projekte, welche die Funktionalität von Matlab nachbilden und einzelne Aspekte hervorheben. Diese Projekte streben oft keine direkte Kompatibilität an; die Unterschiede zu Matlab variieren. Im Rahmen des GNU-Projektes ist GNU Octave entstanden, das in weiten Teilen codekompatibel zu Matlab ist. Ein anderes ist das Softwarepaket FreeMat.\n\nMatplotlib und NumPy sind Python-Bibliotheken, die Python zur Matlab-Alternative ausbauen. Numpy basiert ebenfalls auf LAPACK und BLAS. Die Syntax von Python/numpy unterscheidet sich von Matlab, möchte jedoch vergleichbar einfach sein.\n\nZwei weitere Alternativen sind Scala, eine JVM-basierte Programmiersprache, und ILNumerics, das auf .NET aufsetzt.\nEine neue Alternative zu Matlab ist die in der Syntax ähnliche Programmiersprache Julia.\nDie größeren Computeralgebrasysteme, die in erster Linie für symbolische Berechnungen gedacht sind, enthalten auch numerische Algorithmen; als Beispiele seien Maple, Maxima und Mathematica genannt.\n\n\n"}
{"id": "193530", "url": "https://de.wikipedia.org/wiki?curid=193530", "title": "Mail (Apple)", "text": "Mail (Apple)\n\nMail ist ein von Apple mit dem Betriebssystem macOS ausgeliefertes E-Mail-Programm. Wegen der Verwechselungsgefahr mit der gebräuchlichen Bezeichnung E-Mail für elektronische Post wird es auch „Apple Mail“ oder „Mail.app“ (.app ist bei OS X bzw. macOS die Dateiendung für Programme) genannt. Das Programm wird mit allen Apple-Computern und -Mobilgeräten (Mac, iPod touch, iPhone und iPad) mitgeliefert. Es konnte vor iOS 10 auf mobilen Geräten nicht deinstalliert werden.\n\nEs unterstützt die E-Mail-Protokolle POP3 und IMAP, erlaubt mehrere Mailkonten und zeichnet sich vor allem durch die nahtlose Integration ins Betriebssystem aus. So zeigt es die Anzahl ungelesener Mails im Dock-Icon, benutzt die Adressen aus dem systemeigenen Adressbuch und kann zum Versenden von Bildern direkt auf die Bilder-Bibliotheken von iPhoto, Aperture und Photo Booth zurückgreifen. Zum Gestalten von E-Mails bietet Mail eine Vielzahl von Vorlagen an, die ohne HTML-Kenntnisse oder Zusatzprogramme angepasst werden können.\n\nWeiterhin bietet Mail\n\n\nApple Mail ist seit jeher Bestandteil von macOS und wurde mit jeder neuen Version des Betriebssystems aktualisiert und erweitert.\n\nBeinhaltet alle Versionen bis zur Einführung des Betriebssystems „Tiger“. In dieser Version wurden E-Mail im weit verbreiteten Format \"Mbox\" gespeichert. Andere Mail-Clients, wie etwa Mozilla Thunderbird verwenden dieses Format ebenfalls, was einen Import aus diesen Programmen vereinfachte.\n\nWurde zusammen mit Mac OS X 10.4 „Tiger“ ausgeliefert. Ab dieser Version werden E-Mails im \"Emlx\"-Format gespeichert. Dabei wird für jede E-Mail eine eigene Datei angelegt, was für die neue Suchfunktion (Spotlight) nötig wurde. Eine Konvertierung vom neuen Emlx-Format ins Mbox-Format ist mittels des Programms emlxconvert möglich.\n\nWeitere Neuerungen umfassten\n\nWurde zusammen mit Mac OS X 10.5 „Leopard“ ausgeliefert. Laut Apple wurden 15 neue Funktionen hinzugefügt. Diese sind u. a.:\n\n\nWurde zusammen mit Mac OS X 10.6 „Snow Leopard“ ausgeliefert. Eine Neuerung war die Unterstützung von Microsoft Exchange Server 2007.\n\nWurde zusammen mit Mac OS X 10.7 „Lion“ ausgeliefert. Für diese Version wurde die Bedienoberfläche stark überarbeitet und an die auf dem iPad verwendete Version angenähert. Unterstützung für Exchange 2010, chronologische Darstellung von Mail-Konversationen, erweiterte Suchfunktion.\n\nWurde zusammen mit OS X 10.8 „Mountain Lion“ ausgeliefert und am 25. Juli 2012 veröffentlicht. Die RSS-Unterstützung entfiel.\n\nWurde zusammen mit OS X 10.9 „Mavericks“ ausgeliefert und am 22. Oktober 2013 veröffentlicht. Die Applikation „Karten“ wurde implementiert, die Archivierung von Google-Mails verbessert.\n\nWurde zusammen mit OS X 10.10 „Yosemite“ ausgeliefert und am 16. Oktober 2014 veröffentlicht.\n\nWurde zusammen mit OS X 10.11 „El Capitan“ ausgeliefert und am 30. September 2015 veröffentlicht.\n\n\n"}
{"id": "194732", "url": "https://de.wikipedia.org/wiki?curid=194732", "title": "Simula", "text": "Simula\n\nSimula ist eine Programmiersprache, die von Ole-Johan Dahl und Kristen Nygaard in den 1960er Jahren am \"Norsk Regnesentral\" \"(Norwegisches Rechenzentrum)\" an der Universität Oslo entwickelt wurde, um Simulationen von z. B. physikalischen Prozessen am Rechner durchführen zu können.\n\nDie Sprache gilt als erste objektorientierte Programmiersprache. Sie baute auf der Definition von Algol 60 auf, ist blockstrukturiert mit den üblichen Grunddatentypen und Kontrollstrukturen und führt Klassen ein, die die Konzepte von Datenstrukturen und Koroutinen vereinigen.\n\nSimula gilt als Vorgänger von Smalltalk. Viele der mit Simula eingeführten Konzepte finden sich in modernen objektorientierten Programmiersprachen wieder. Das Klassenkonzept von Simula-67 diente beispielsweise als Vorbild für das von C++; die Sprache benutzte schon damals einige der noch heute in modernen objektorientierten Programmiersprachen verwendeten Schlüsselwörter wie class, new, this.\n\nDas Wort \"Simula\" setzt sich aus den Bestandteilen \"simu\" für \"simulation\" und \"la\" für \"language\" zusammen.\n\nDie Sprache basiert auf Algol 60, ergänzt diese aber um Konzepte von Objekten und Koroutinen. Simula führt auch das Klassenkonzept ein. Was in späteren Jahren unter den Begriffen Datenabstraktion – das Verbergen von Implementierungsdetails – oder Modularisierung – das Trennen eines Programms in Funktionseinheiten – bekannt wurde, ist schon in \"Simula\" als Begriff vorhanden. Ein Objekt fasst Untereinheiten ggf. unterschiedlicher Datentypen zu einer neuen Einheit zusammen. Zur Manipulation eines Objektes werden dazu passende Prozeduren und Funktionen vereinbart.\n\nIn \"Simula\" ist diese Implementierung noch nicht für andere Blöcke unsichtbar, aber der erste Schritt in diese Richtung ist getan. Gibt es mehrere Objekte mit einer ähnlichen Struktur, die sich vielleicht nur in einigen Komponenten unterscheiden, so besteht die Möglichkeit, Ober- und Unterklassen zu vereinbaren, was heutzutage als Vererbung bezeichnet wird.\n\nZusätzlich gibt es eine Möglichkeit, sogenannte Koroutinen zu vereinbaren, damit Objekte miteinander kommunizieren und selbständig agieren können; und es gibt große Bibliotheken mit Funktionen zur Verwaltung von Warteschlangen und zur Ausführung von Prozeduren zu festgelegten Zeitpunkten, so dass es auch möglich ist, umfangreiche Simulationen zu programmieren. Die Sprache fand außerhalb Europas kaum Verbreitung, obwohl sie in Skandinavien sehr verbreitet war. 1987 wurde der letzte gültige Simula-Standard verabschiedet. Hier ist ein Code-Beispiel:\n\nVereinbarung einer Klassendefinition:\n\nErzeugung eines „Datum-Objektes“:\n\nVerwendung von Datum als Oberklasse – der Wochentag wird hinzugefügt:\n\n1962 trafen sich Ole-Johan Dahl und Kristen Nygaard am \"Norsk Regnesentral (NR)\" in Oslo, im selben Jahr wurde eine erste formale Beschreibung der Sprache auf dem IFIP 62 Weltkongress in München vorgestellt. Als UNIVAC das Simula-Projekt unterstützte, wurde eine UNIVAC 1107 am NR installiert. Ein erster Prototyp eines Simula-Compilers lief 1964 auf der \"UNIVAC 1107\" des NR und das Simula-I-Handbuch wurde 1965 veröffentlicht. 1967 erschien die überarbeitete Sprachversion Simula-67, für die auf mehreren damals existierenden Großrechnersystemen Compiler entwickelt wurden. In den 70er Jahren wurde Simula in der Praxis vielfach eingesetzt, und die theoretischen Konzepte der Sprache hatten großen Einfluss auf damals moderne Programmiersprachen. Die Konzepte der Objektorientierung wurden weiterentwickelt und schließlich in Smalltalk-80 erstmals konsequent umgesetzt. Die erste Smalltalk-Version wurde in Simula geschrieben.\n\nErzählt wird, dass Dahl und Nygaard an Schiffssimulationen gearbeitet hatten. Dabei ergab sich durch die kombinatorische Explosion von Parameterbeziehungen eine verwirrende Vielfalt an Möglichkeiten, wie sich die verschiedensten Attribute der unterschiedlichen Schiffe gegenseitig beeinflussen konnten. So kam die Idee auf, die unterschiedlichen Schiffstypen jeweils als eigenständige Objekte zu klassifizieren, wobei jede Klasse von Objekten für die \"eigenen\" Daten und das \"eigene\" Verhalten selbst zuständig war.\n\nSimula wird heute immer noch gelegentlich eingesetzt, aber der historische Einfluss der Sprache ist wichtiger als ihre Bedeutung in der modernen Programmierung. In den skandinavischen Ländern wurde Simula lange Zeit als Ausbildungssprache verwendet; das Buch \"SIMULA begin\" gilt auch in Deutschland als Klassiker. Bjarne Stroustrup, der als Student das Programmieren mit Simula erlernt hatte und bei seinem späteren Arbeitgeber AT&T in C programmieren musste, reicherte C mit Hilfe eines Präprozessors (cfront) um die wesentlichen Konstrukte von Simula an, um weiter in der erlernten Art programmieren zu können. Seine Erweiterung hieß zunächst \"C with classes\" und ist heute als C++ bekannt.\n\nEin Nachfolger von Simula mit dem Namen Beta wurde zwar entwickelt, aber kaum eingesetzt.\n\n\n"}
{"id": "194882", "url": "https://de.wikipedia.org/wiki?curid=194882", "title": "MUTE", "text": "MUTE\n\nMUTE ist ein freies Peer-to-Peer-Filesharingprogramm, das versucht, die Identität der Benutzer zu verschleiern. MUTE wurde von Jason Rohrer als native Anwendung für Windows, Linux und Mac OS X entwickelt und unter Public domain veröffentlicht. Seit 2009 wird MUTE nicht mehr weiterentwickelt.\n\n\nSomit soll MUTE das Tauschen von Dateien mit einer möglichst großen Privatsphäre ermöglichen. Ähnliche Techniken werden auch bei Freenet, RetroShare, GNUnet, I2P und weiteren verwendet.\n\nFolgende Clients sind mit MUTE kompatibel, werden aber genau wie MUTE nicht mehr weiterentwickelt.\n\n\n\nAlternative Clients:\n\n"}
{"id": "195133", "url": "https://de.wikipedia.org/wiki?curid=195133", "title": "Integrated Facility for Linux", "text": "Integrated Facility for Linux\n\nIntegrated Facility for Linux (kurz \"IFL\") ist ein Spezialprozessor für IBM-Großrechnersysteme (System z) und IBM Power Systems.\nAuf System z können nur die Betriebssysteme z/VM für Linux und seit November 2008 auch Open Solaris ausgeführt werden.\n\nDer Vorteil dieser Lösung besteht darin, dass die Lizenzgebühren der Software, die unter z/OS lizenziert wurde, nicht durch den zusätzlichen Prozessor beeinflusst wird.\n\nAuf \"IBM Power Systems\" kann auf dem IFL-Prozessor nur Linux ausgeführt werden. Für IFLs gelten andere Preise für Hard- und IBM-Software als für Standardprozessoren, die AIX ausführen können.\n\n"}
{"id": "195256", "url": "https://de.wikipedia.org/wiki?curid=195256", "title": "PowerPC G5", "text": "PowerPC G5\n\nDer PowerPC 970 oder PPC 970 ist ein 2002 von IBM vorgestellter 64-bit-RISC-Mikroprozessor der PowerPC-Familie, der von Apple auch als PowerPC G5 bezeichnet wird, da er für Generation 5 der PowerPC-Entwicklung steht. Er wurde aus dem leistungsfähigeren IBM POWER4+ entwickelt, einem Spross der IBM-Power-Familie für Hochgeschwindigkeitsrechner, und ist der zweite PowerPC-Prozessor mit 64 Bit Wortbreite. Der zuvor von IBM und Motorola bereits 1996 vorgestellte PowerPC 620 mit 64 Bit aus der 2. Generation (G2) war jedoch kommerziell nicht erfolgreich.\n\nAufgrund spekulativer Ausführung ist der PowerPC 970 von der 2017 entdeckten Sicherheitslücke Spectre betroffen. Da Systeme, die diesen Prozessor nutzen, schon lange nicht mehr vom Hersteller unterstützt werden, bleiben diese Sicherheitslücken offen. Ein Patch müsste dabei aus einer Kombination aus Firmware- und Betriebssystem-Fix bestehen (Mikrocode-Update und aktualisierter Kernel).\n\nDank der AltiVec-kompatiblen Erweiterung VMX ist der PowerPC 970 vollständig Code-kompatibel zu 32-bit-G4-Prozessoren. Die einzige Ausnahme stellt die laut PowerPC-Spezifikation optionale Little-Endian-Unterstützung dar, die dem PowerPC 970 gegenüber seinen Vorgängern fehlt. Die Kommunikation mit dem System erfolgt über zwei 32 Bit breite Busse. Er ist mit 64 KB L1-Cache für Instruktionen bzw. 32 KB für Daten und 512 bis 1024 KB fehlertolerantem (Fehlerkorrekturverfahren) L2-Cache für Instruktionen und Daten ausgestattet. Es wurden Modelle mit interner Taktfrequenz von 1,0 bis 2,5 GHz gebaut, der PowerPC 970FX wurde von Apple auf 2,7 GHz übertaktet.\n\nDer G5-Prozessor ist multiprozessorfähig und verfügt über zwei Gleitkommaeinheiten mit doppelter Genauigkeit, die die besonders für \"Number-Crunching\" (Hochleistungsrechnen mit vielen gleichartigen Zahlen und Rechenoperationen) wichtigen FMAC-Instruktionen (Fused Multiply Add) beherrschen, bei welchen der Prozessor mit nur einem Befehl eine Multiplikation und Addition durchführt. Seine AltiVec-Implementierung entspricht der der ersten G4 (PowerPC 7400/7410), während G4e-Versionen (ab der PowerPC-7450-Generation) über eine verbesserte AltiVec-Implementierung verfügen.\n\nIm Gegensatz zum 1999 von Motorola im Alleingang entworfenen G4 (PowerPC-7400- und -7450-Serien), der aus dem von IBM und Motorola gemeinsam erarbeiteten G3 (PowerPC-750-Serie) entwickelt wurde, hat der G5 einen wesentlich größeren Lookahead-Puffer, mit dem er eintreffende Instruktionen antizipieren und entsprechend reagieren kann (beispielsweise um schon vorab später benötigte Daten aus dem Speicher anzufordern). Dies bedeutet allerdings auch, dass zusammen mit seinem höheren Takt die Pipeline (die Anzahl der Stationen, die ein Befehl im Prozessor durchläuft) deutlich länger ist als die des G4. Der G5 arbeitet Befehle in Fünfergruppen ab, die gemeinsam durch den Chip geschleust werden, da sonst der Verwaltungsaufwand bei insgesamt maximal 215 gleichzeitig in der Verarbeitung befindlichen Instruktionen zu groß wäre. Diese Fünfergruppen, die immer aus vier Rechenbefehlen und einem Sprungbefehl bestehen, versucht der G5 am Anfang der Verarbeitung möglichst optimal zu füllen, ungenutzte \"Slots\" bleiben frei. Angepasste Compiler können durch entsprechende Codegenerierung helfen, die Fünfergruppen besser zu füllen.\n\nIn Apples G5-Modellen ist der Prozessor über den \"Elastic Bus\" genannten, bidirektionalen, in jede Richtung 32 Bit breiten Front Side Bus (FSB) mit dem \"System Controller\" verbunden. Der FSB läuft in den Power Macs und Xserve G5 normalerweise mit der Hälfte des jeweiligen Prozessortaktes. Nur beim iMac G5 und dem danach veröffentlichten Single-1,8 GHz Power Mac G5 \"Late 2004\" (welcher intern weitgehend dem iMac G5 Rev. A entspricht und daher von allen anderen Power-Mac-G5-Modellen abweicht) taktet der \"Elastic Bus\" mit einem Drittel der CPU-Geschwindigkeit, aber auch andere Multiplikatoren sind laut IBM möglich.\n\nAls Chipsatz verwendete Apple den gemeinsam mit IBM entwickelten Chip CPC925 (von Apple U3 und U3H genannt), der die Verbindung mit PCI- und AGP-Erweiterungskarten bewerkstelligt und mit dessen Speichercontroller sich DDR-400-Speicher, optional auch mit ECC, anbinden lässt.\nIn den letzten Power-Mac-G5-Modellen mit Dual-Core-Prozessor PowerPC 970MP wird wie in den letzten iMac G5-Modellen der neuere von IBM alleine entwickelte Chipsatz CPC945 (von Apple U4 genannt) verwendet, wodurch diese Modelle nicht nur PCI-Express bei Macs einführten, sondern auch den verwendeten Speicher auf DDR2-533 (wieder optional auch mit ECC) beschleunigte. Auch wurde damit die Speicherlatenz verbessert, was neben der Integerleistung der Hauptkritikpunkt der ersten G5-Generation war.\n\nDer letzte 2006 vorgestellte Chipsatz CPC965 bietet vor allem Stromverbrauch-Optimierungen.\n\nIBM setzte den PowerPC 970 in den eigenen JS20- (970/970FX) und JS21- (970MP/970FX) Blades ein, die einige TOP500-Cluster wie der MareNostrum in Spanien nutzten. Auch das bis 2007 erhältliche \"IBM System p5 185 Express\" verwendete G5-Prozessoren. Andere Hersteller boten ebenfalls G5-Blades an, auch der PPC-Hersteller Genesi bot ab 2006 eine \"Open Server Workstation\" auf Basis des Dual-Core-G5 PowerPC 970MP an.\nDa der PowerPC 970 noch der AIM-Allianz (Apple, IBM, Motorola) entstammt, war Apple auch der Hauptabnehmer und setzte den „G5“, was gleichzeitig der umgangssprachliche Name für den damit betriebenen Power Mac selbst ist, im Power Mac G5 und in den zur selben Zeit verkauften iMacs ein. Um die Geräuschentwicklung des Power Mac G5 auf niedrigem Niveau zu halten, hat Apple bei dem damaligen Topmodell mit 2 × 2,5-GHz-CPUs (sowie bei den beiden nachfolgenden Revisionen mit 2 × 2,7 GHz-CPU bzw. 2 × 2,5 GHz-Dual-Core-CPU jeweils beim Topmodell) eine Flüssigkeitskühlung eingesetzt; mit insgesamt neun individuell gesteuerten Lüftern ist das Gerät angenehm leise.\nEin Power Mac G5 mit einem 3 GHz sollte laut Steve Jobs im Jahr 2004 erscheinen, wozu es jedoch nie kam. Inwieweit dieses Versprechen in Rücksprache mit IBM nicht mehr umgesetzt wurde blieb unklar, jedoch wechselte Apple Anfang 2006 von PowerPC- auf Intel-Prozessoren.\n\n\n\n\n\n\n"}
{"id": "197296", "url": "https://de.wikipedia.org/wiki?curid=197296", "title": "Phone Operation Center", "text": "Phone Operation Center\n\nDas Phone Operation Center (PoC) ist ein integriertes Hard- und Softwareprojekt, welches es ermöglicht, auf Großveranstaltungen ein flächendeckendes und funktionsfähiges DECT-Netz zu realisieren. Eine Besonderheit des PoC ist seine Mobilität und die Fähigkeit es an beliebigen Orten innerhalb weniger Stunden in Betrieb nehmen zu können. \n\nDie Teilnehmer können ihr eigenes DECT-Telefon anmelden und sich eine eigene Nebenstelle aussuchen. Dabei arbeitet das PoC relativ problemlos mit dem \"Wildwuchs\" an Firmwarebeständen und DECT-GAP-Implementierungen der unterschiedlichen DECT-Chip-Hersteller. Neben den diversen internen Diensten, die man mit dem PoC nutzen kann, bietet es zeitlich unbegrenzte und kostenlose Telefonate ins gesamte deutsche Festnetz. Bei HackingAtRandom2009 wurden in Zusammenarbeit mit einem holländischen Telefonprovider sogar kostenlose Anrufe in ganz Europa und die USA ermöglicht, auf der OHM2013 konnte man sogar kostenlos in nahezu die ganze Welt telefonieren. Auf dem Chaos Communication Congress entsteht durch das PoC regelmäßig ein Telefonnetz. Während des 35. Chaos Communication Congresses im Jahr 2018 wurden über 6400 Nebenstellen genutzt (3861 DECT, 1346 SIP, 1281 GSM) und etwa 269.800 Gespräche geführt. Die Bezeichnung entstand während der Vorbereitungen zum zweiten Chaos Communication Camp in Berlin und ist angelehnt an \"Network Operation Center\". Allerdings wird der Buchstabe „O“ in der Abkürzung PoC entgegen der häufigen Annahme klein geschrieben. Die Abkürzung PoC wurde erstmals in einem TWiki verwendet und Seitennamen mussten Systembedingt in CamelCase geschrieben sein. Diese Schreibweise wurde dann beibehalten.\n\nDie Initiatoren des PoC sind Sascha Ludwig und Martin Assenmacher. Realisiert wird das Netz auf der Basis einer Telefonanlage der Firma Alcatel.\n\nZentraler Baustein des PoC ist das Eventphone Rack, welches eine Telefonanlage mit 28 Einschüben für ISDN-Karten, analoge Telefone, Voicemail, etc. beinhaltet, einen Linux Call Router, einen SIP Express Router, sowie einen Server, auf dem die Asterisk- und Yate-Telefonsoftware läuft, sowie mehrere DECT-Basisstationen.\n\n"}
{"id": "197380", "url": "https://de.wikipedia.org/wiki?curid=197380", "title": "Computerunterstützte Übersetzung", "text": "Computerunterstützte Übersetzung\n\nEine computerunterstützte Übersetzung (englisch \"computer-aided translation\", auch \"computer-assisted translation\", abgekürzt \"CAT\") ist die von Menschen durchgeführte Übersetzung von Sprache mit Hilfe von Computerprogrammen.\n\nCAT-Systeme bestehen meist aus folgenden Teilsystemen:\n\n\nProgramme zur computerunterstützten Übersetzung übersetzen nicht selbst, sondern unterstützen den menschlichen Übersetzer (den so genannten „Humanübersetzer“) bei seiner Arbeit. Im Gegensatz dazu erfolgt eine maschinelle Übersetzung automatisch ohne Notwendigkeit eines Humanübersetzers.\n\nEs sind jedoch Mischformen möglich, bei denen eine maschinelle Übersetzung von einem menschlichen Übersetzer geprüft und vervollständigt wird.\n\nNeben einer verbesserten Textkonsistenz werden die Verwendung kundenspezifischer oder branchenspezifischer Terminologie sowie verkürzte Lieferzeiten, Kostenreduzierung und eine bessere Kostenkontrolle genannt.\n\nDie nachstehende, keinesfalls erschöpfende Liste enthält einige der wichtigsten Anwendungen, die derzeit auf dem Markt sind.\n\n"}
{"id": "197431", "url": "https://de.wikipedia.org/wiki?curid=197431", "title": "Elektronika BK-0010", "text": "Elektronika BK-0010\n\nBeim Elektronika BK-0010 (russisch Электроника БК-0010) handelt es sich um eine Serie von Heimcomputern, die ab 1983 in der Sowjetunion hergestellt wurde.\n\nDie Ersterwähnung des BK-0010 datiert aus dem Jahre 1983, dem Datum der Erstellung der Bedienungsanleitung. Entwickelt wurde der Computer in der Nationalen Forschungsuniversität für Elektronische Technologie – Moskauer Institut für Elektronische Technologie (MIET) in Selenograd bei Moskau, und gefertigt im Werk „Exiton“ in Pawlowski Possad. Im Januar 1985 tauchte der Computer erstmals im Handel auf.\n\nDer BK-0010 basiert auf dem 16-bit-Mikroprozessor K1801WM1 () mit einer PDP-11-Architektur. Die CPU K1801WM1 gab es in zwei Ausführungen: In einem Keramik- und einem Plastegehäuse. Sie arbeitete mit der Frequenz von 3 MHz, aber es gab keine Kommandos für Multiplikation und Division. Die Rechnerarchitektur der von der Digital Equipment Corporation gebauten PDP-11, eines der erfolgreichsten Minirechners der 1970er Jahre, fand hier den Weg in ein Tischgerät (ähnlich dem TI990 und TI99).\n\nDer BK-0010 verfügt über 32 KB RAM und 32 KB ROM. Das ROM enthält neben einem 16 KB großen Bereich für das Betriebssystem einen gleich großen Bereich für einen Interpreter für die Programmiersprache FOCAL (, von ), die von der Digital Equipment Corporation zuerst für die PDP-6 entwickelt wurde. Ein BASIC-Interpreter war als Zusatzmodul erhältlich.\n\nEs war möglich, über einen externen Anschluss (Industriestandard QBus) zusätzlichen RAM (anstelle des ROMs mit BASIC) oder eine Netzwerkkarte anzukoppeln.\n\nDer Grafik-Controller K1801WP1-037 () erlaubt mit 16 KB RAM Bildspeicher die Darstellung in zwei Modi: Bei einer Auflösung von 256 × 256 Bildpunkten mit vier Farben oder 512 × 256 Bildpunkten mit zwei Farben (schwarz/weiß). Das Bild konnte über einen Monitor oder ein Fernsehgerät ausgegeben werden.\n\nAn Schnittstellen sind neben dem Videoausgang ein Anschluss für einen handelsüblichen Kassettenrecorder zur Datenspeicherung (mit 1200 Bd), eine serielle RS-232-Schnittstelle (mit 50 Bd bis 9600 Bd) sowie 16 Eingabe- und 16 Ausgabeleitungen vorhanden.\n\nDer Rechner und die Tastatur mit 92 Tasten und kyrillischem Layout befinden sich zusammen in einem Gehäuse mit den Abmessungen 37 cm × 18 cm × 7 cm (ähnlich dem C-64).\n\nDie Programmierung in BK-Assembler war bequem und angenehm: linearer Speicher, 16-bit-Befehle und -Adressen, alle acht Register des Prozessors waren gleichberechtigt (es gab keine Einteilung in Indexregister, etc.)\n\nDer Keramikprozessor konnte mit bis zu 8 MHz betrieben werden und anstatt des BASIC auch schnellen (statischen) Speicher verwalten. Ein Mangel war der langsame dynamische Grafikspeicher. Er begrenzte die praktische Verarbeitungsgeschwindigkeit aufgrund Wartezyklen auf 5 MHz.\n\nSchnell erschienen zusätzliche Geräte: Spezialisten wie Vadim Novak entwickelten Erweiterungskarten, die in Kleinserien hergestellt wurden. Eine besonders erfolgreiche Entwicklung war eine kleine Erweiterung, die einen Diskettencontroller, ein Winchester-Laufwerk und 16-Kb zusätzlichen RAMs kombinierte. Dazu wurden einige leistungsstarke Betriebssysteme, Texteditoren und anderes programmiert, die den BK-0010 aufwerteten.\n\nDas Nachfolgemodell wurde der BK-0011, der bereits 128 Kb RAM hatte. Auch der Videospeicher des „БК-0011“ wurde erhöht: er bestand nun aus zwei wechselnden Seiten. Die Frequenz des Prozessors wurde auf 4 MHz angehoben. Das neue Modell erreichte schnell große Popularität, da dafür ein erweitertes DOS geschrieben wurde und das alte System ersetzte. Für den Anschluss eines Festplattenlaufwerkes (\"„Winchester“\") und eines Diskettenlaufwerkes war der zusätzliche Controller noch immer erforderlich. Neben Festplatten- und Diskettenlaufwerken konnten ohne großen Aufwand andere externe Geräte angeschlossen werden: Drucker, Modems, Mouse und Joystick. Besonders Talentierte schafften es, in die Tastatur ein 2\"-Festplattenlaufwerk samt Controller einzubauen. In den BK wurde der Klangprozessor AY-3-8912, bekannt aus dem ZX Spectrum und dem Yamaha MSX, eingebaut. Der günstigste Weg den Klang zu verbessern war der Einbau eines Covox-Adapters.\n\nBesonderes Merkmal des Rechners war, dass er ohne jegliche Software auf den Markt kam. Einfache Spiele, der Interpreter Fokal, der BASIC ersetzte, und ein paar Testanwendungen waren praktisch alles. Aber es erschienen zu dem Rechner Dokumentationen des BASIC und des Machinencodes. Darauf aufbauend schrieben Programmierer wie Wladimir Sawin neue Spiele, die sie von anderen Plattformen transferierten. Viele Ein-Mann-Gesellschaften wie BIL Corp., SW Corp., RDA Corp. hatten nicht weniger Ansehen unter den Nutzern als Firmen wie Ocean Software, Electronic Arts, Virgin Interactive und Activision Bedeutung für Systeme wie den ZX Spectrum hatten.\n\nZu Beginn der 1990er Jahre bauten Programmierer aus Samara in die Weiterentwicklung BK-0011M einen Musikprozessor AY-3-8912 ein und schrieben ein neues Betriebssystem namens CSI-DOS. Daraus entwickelte sich eine Musikerszene, die sich des Computers als Soundmaschine bedienten.\n\n"}
{"id": "199280", "url": "https://de.wikipedia.org/wiki?curid=199280", "title": "Z-Buffer", "text": "Z-Buffer\n\nDas Z-Buffering (auch \"Depth Buffering,\" \"Tiefenpuffer-\" oder \"Tiefenspeicher-Verfahren\") ist ein Verfahren der Computergrafik zur Verdeckungsberechnung, also um die vom Betrachter aus sichtbaren dreidimensionalen Flächen in einer Computergrafik zu ermitteln. Durch Tiefeninformationen in einem sogenannten Z-Buffer („Z-Puffer“) stellt das Verfahren pixelweise fest, welche Elemente einer Szene gezeichnet werden müssen und welche verdeckt sind. Praktisch alle heutigen Grafikkarten implementieren Z-Buffering direkt in Hardware. Als Entwickler des Z-Buffer-Verfahrens gilt Edwin Catmull; allerdings beschrieb Wolfgang Straßer das Prinzip etwa zur gleichen Zeit in einem anderen Kontext. Die wichtigste Alternative zum Z-Buffering ist der Raytracing-Algorithmus.\n\nDas Prinzip des Z-Buffering ist sehr einfach. Neben dem sichtbaren Teil des Bildspeichers, der die aktuellen Farbwerte enthält, gibt es einen weiteren Speicher, den Z-Buffer, der die Tiefe des sichtbaren Objekts an jedem Pixel enthält. Alternativ können die Pixelwerte im Framebuffer um einen formula_1-Wert erweitert werden. Zu Beginn werden die Einträge im Z-Buffer auf einen Wert gesetzt, der für eine unendliche Entfernung steht (Backplane Distance). Der Framebuffer wird mit der Hintergrundfarbe initialisiert. Jedes Polygon wird nun gerastert. Nur wenn der aktuell gerasterte Punkt des Polygons näher am Betrachter liegt als der Punkt, dessen Entfernung im Z-Buffer eingetragen ist, werden die Werte im Z-Buffer und im Framebuffer durch die Entfernung beziehungsweise die Farbe des aktuellen Polygons ersetzt.\n\nDie Reihenfolge, in der die Polygone gerastert werden, ist im Prinzip beliebig. Nicht nur Polygone, sondern beliebige weitere grafische Primitive können mit Hilfe des Z-Buffers gerendert werden.\n\nDie Speichergröße der Werte im Z-Buffer hat einen großen Einfluss auf die Qualität des gerenderten Bildes. Wenn zwei Objekte sehr eng beieinander liegen, können bei einem Z-Buffer mit 8 Bit pro Pixel leicht Artefakte entstehen. 16, 24 oder 32 Bit tiefe Z-Buffer erzeugen weniger Artefakte.\n\nAuf aktuellen Grafikkarten beansprucht der Z-Buffer einen bedeutenden Teil des verfügbaren Speichers und der Datenübertragungsrate. Mit verschiedenen Methoden wird versucht, den Einfluss des Z-Buffers auf die Leistung der Grafikkarte zu reduzieren. Dies ist zum Beispiel durch die verlustfreie Kompression der Daten möglich, da das Komprimieren und Dekomprimieren der Daten kostengünstiger ist als die Erhöhung der Datenübertragungsrate einer Karte. Ein anderes Verfahren spart Löschvorgänge im Z-Buffer: die Tiefeninformation wird mit alternierendem Vorzeichen in den Z-Buffer geschrieben. Ein Bild wird mit positiven Vorzeichen gespeichert, das nächste Bild mit negativem, erst dann muss gelöscht werden. Eine weitere Möglichkeit zur Optimierung ist die Vorsortierung der Primitiven: Werden zunächst die näherliegenden Primitiven gerendert, kann bei den weiter entfernten später direkt entschieden werden, ob bzw. welche Pixel gerendert werden müssen und welche von Vordergrundobjekten verdeckt werden, wodurch Texturierungs- und Pixel-Shader-Vorgänge eingespart werden können.\n\nPseudocode\nInitialisierung:\nBeginn:\n\nDer Bereich der Tiefeninformation im Kameraraum, der zu rendern ist, wird häufig durch den \"nah\"-Wert und \"fern\"-Wert von formula_1 definiert. Nach einer Perspektivtransformation wird der neue Wert von formula_1, hier als formula_5 bezeichnet, wie folgt berechnet:\n\nDabei ist formula_5 der neue Wert von formula_1 im Kameraraum. Manchmal werden auch die Abkürzungen formula_9 und formula_10 verwendet.\n\nDie resultierenden Werte von formula_5 werden auf Werte zwischen −1 und 1 normiert, wobei die Fläche bei \"nah\" den Wert −1 und die Fläche bei \"fern\" den Wert 1 erhält. Werte außerhalb dieses Bereichs stammen von Punkten, die sich nicht im Sichtbereich befinden, und sollten nicht gerendert werden.\n\nBei der Implementierung eines Z-Buffers werden die Werte der Scheitelpunkte eines Polygons linear interpoliert und die formula_5-Werte einschließlich der Zwischenwerte im Z-Buffer gespeichert. Die Werte von formula_5 sind wesentlich enger an der Nah-Fläche verteilt und wesentlich mehr zur Fern-Fläche hin verstreut, was zu einer höheren Genauigkeit der Darstellung nahe dem Kamerastandpunkt führt. Je enger die Nah-Fläche an die Kamera gesetzt wird, desto geringer ist die Präzision im Fernbereich. Eine häufige Ursache für unerwünschte Artefakte bei entfernten Objekten ist, dass die Nah-Fläche zu eng an die Kamera gesetzt wurde. Diese als \"Z-Fighting\" (Z-Konflikt, Tiefenkonflikt) bezeichneten Artefakte treten insbesondere dann auf, wenn zwei koplanare Flächen sehr nahe beieinander sind, beispielsweise eine Wand und ein darauf angebrachtes Plakat. Welches von beiden Polygonen dann im Vordergrund liegt, ist im Wesentlichen zufällig und kann sich auch durch geringfügige Änderungen des Kamerastandortes ändern. Zur Abhilfe müssen vom Programmierer explizit Maßnahmen ergriffen werden, etwa indem die Z-Werte des Plakates künstlich verändert werden oder durch den Einsatz eines sogenannten Stencilbuffers.\n\nDa die Abstandswerte nicht gleichmäßig im Z-Buffer abgelegt werden, werden nahe Objekte besser dargestellt als ferne, da ihre Werte genauer abgespeichert sind. Allgemein ist dieser Effekt erwünscht, er kann aber auch zu offensichtlichen Artefakten führen, wenn sich Objekte voneinander entfernen. Eine Variation des Z-Bufferings mit ausgeglicheneren Entfernungswerten ist das sogenannte W-Buffering. Um einen W-Buffer zu implementieren, werden die unveränderten Werte von formula_1 bzw. formula_9 in den Buffer gespeichert, im Allgemeinen als Gleitkommazahlen. Diese Werte können nicht linear interpoliert werden, sondern müssen invertiert, interpoliert und wieder invertiert werden. Die resultierenden formula_9-Werte sind, im Gegensatz zu formula_1, gleichmäßig zwischen \"nah\" und \"fern\" verteilt. Ob ein Z-Buffer oder ein W-Buffer zu besseren Bildern führt, hängt vom jeweiligen Anwendungszweck ab.\n\n\n\n"}
{"id": "199283", "url": "https://de.wikipedia.org/wiki?curid=199283", "title": "Maleralgorithmus", "text": "Maleralgorithmus\n\nDer Maleralgorithmus (engl. \"painter's algorithm\") ist eine einfache Lösung des Sichtbarkeitsproblems in der 3D-Computergrafik. Bei der Darstellung einer dreidimensionalen Szene auf einer zweidimensionalen muss häufig entschieden werden, welche Polygone sichtbar und welche verdeckt sind.\n\nDer Name Maleralgorithmus ist eine Anspielung auf einen Maler, der die entfernten Objekte einer Szene zuerst zeichnet und sie dann mit den näher gelegenen übermalt. Entsprechend kann der Algorithmus in der Implementierung einer computergrafischen Anwendung eingesetzt werden: Zuerst werden alle Polygone ihrer Tiefe nach sortiert (Tiefensortierung, engl. \"depth sort\") und dann werden sie der Reihenfolge nach gezeichnet. Durch das Überzeichnen der Bildanteile, die normalerweise nicht sichtbar sind, wird das Sichtbarkeitsproblem gelöst.\n\nDiese Verfahrensweise führt zu etlichen Problemen. Was passiert, wenn Polygon A teilweise Polygon B, B teilweise C und C wiederum teilweise A überschneidet? Es kann nicht mehr entschieden werden, welches Polygon vor welchem liegt. Ein ähnlicher Fall liegt vor, wenn sich zwei Polygone gegenseitig im dreidimensionalen Raum überschneiden. In solchen Fällen muss mindestens eines der betroffenen Polygone unterteilt werden, damit die Sortierung möglich ist und der Maleralgorithmus ein korrektes Ergebnis liefert.\n\nEin anderes Problem ist, dass der Maleralgorithmus ineffizient ist, weil der Computer die Intensitäten aller Punkte eines Polygons berechnen muss, auch wenn das Polygon in der endgültigen Szene gar nicht sichtbar ist.\n\nDiese und andere Probleme mit dem Maleralgorithmus führten zur Entwicklung des Z-Buffers, der als logische Weiterentwicklung des Maleralgorithmus betrachtet werden kann. Durch die Verwendung eines Z-Buffers müssen die Objekte nicht mehr in der Reihenfolge ihrer Tiefe gerendert werden.\n"}
{"id": "202788", "url": "https://de.wikipedia.org/wiki?curid=202788", "title": "Prix Ars Electronica", "text": "Prix Ars Electronica\n\nDer Prix Ars Electronica (\"Prix\": französisch für „Preis“, \"Ars\": lateinisch für „Kunst“) ist ein Kulturpreis, der seit 1987 im Rahmen des Ars Electronica Festivals für Kunst, Technik und Gesellschaft vom Veranstalter ORF Oberösterreich und dem Land Oberösterreich ausgeschrieben wird. Die Verleihung der „Goldenen Nica“ findet im Brucknerhaus in Linz an der Donau statt. Die Trophäe ist der Nike von Samothrake nachempfunden, einer Statue der antiken griechischen Siegesgöttin Nike.\n\nDas Ziel des Prix Ars Electronica ist zu zeigen, dass der Computer und die digitale Technik längst zu wichtigen Werkzeugen für Künstler und Kreative geworden sind. Computerkunst soll nicht nur in Zusammenhang mit Computerkultur und Netzkultur verstanden werden, sondern in Wechselwirkung mit aktuellen gesellschaftlichen und technologischen Entwicklungen.\n\nDer Prix Ars Electronica ist weltweit einer der bedeutendsten Preise im Bereich der elektronischen Kunst und Kultur. Jede Goldene Nica ist mit einem Preisgeld von 10.000 Euro (bis 2001: 100.000 Schilling) dotiert und damit auch eine der höchstdotierten Auszeichnungen in diesem Bereich.\n\nDas Ars Electronica Festival mit dem Prix Ars Electronica hat zusammen mit dem Ars Electronica Center (AEC) und der Linzer Klangwolke wesentlich zum Wandel des Images der Stadt Linz von einem Industriestandort zu einem Zentrum zeitgenössischer und zukunftsorientierter Kunst beigetragen.\n\nDie Preise für diese Kategorie werden seit 1990 vergeben. Diese Kategorie umfasst verschiedenste Arbeiten von Installationen und Performances, typischerweise mit Publikumsbeteiligung, virtueller Realität, Multimedia und Telekommunikation.\n\nDiese Kategorie ist für alle, die elektronische Musik und Klangkunst durch digitale Werkzeuge machen. Von 1987 bis 1998 wurde diese Kategorie \"Computer Music\" genannt. 1987 wurden zwei Ehren-Nicas vergeben. 1990 wurde keine Goldene Nica vergeben und 1991 fehlte die Kategorie gänzlich. 2012 wurde der Titel um den Zusatz \"Sound Art\" erweitert, um das inhaltliche Spektrum besser wiederzugeben.\n\nIn den Kategorien \"World Wide Web\" (1995–1996) und \".net\" (1997–2000) wurden interessante webbasierte Projekte ausgezeichnet. Die Kriterien sind dabei webspezifische, gemeinschaftsorientierte Identität und Interaktivität. Im Jahr 2001 wurde die Kategorie weiter gefasst und unter dem Namen \"Net Vision/Net Excellence\" als Auszeichnung für Innovationen im Bereich Onlinemedien festgelegt.\n\n\n\n\nDiese Kategorie wurde von 1987 bis 1994 vergeben und war offen für Computergrafiken aus verschiedenen Bereichen – Kunst, Kultur, Wissenschaft und Forschung. Eingereicht werden konnten computergenerierte Grafiken, die durch die individuelle Programmierung von Computern oder den kreativen Gebrauch erhältlicher Computerprogramme geschaffen wurden.\n\n\nDiese Kategorie wurde 2004, zum 25-jährigen Jubiläum des Festivals, eingeführt. Die erste Verleihung fand in New York statt, um die internationale Ausrichtung des Prix Ars Electronica zu unterstreichen. Eine der beiden Statuen wurde damals an Wikipedia vergeben. Als Hauptsponsor der Kategorie ist die SAP AG beteiligt.\n\nDie Kategorie \"Computer Animation\" wurde von 1987 bis 1997 vergeben und 1998 auf \"Computer Animation/Visual Effects\" umbenannt. Sie ist offen für Computeranimationen aus verschiedenen Bereichen – Kunst, Kultur, Wissenschaft und Unterhaltung. Eingereicht werden können computergenerierte Filme, die durch die individuelle Programmierung von Computern oder den kreativen Gebrauch erhältlicher Computerprogramme geschaffen wurden. Eine digitale Integration von oder Kombination mit auf traditionelle Weise hergestelltem Filmmaterial ist erlaubt.\n\nPreise in dieser Kategorie wurden zum ersten Mal 2007 vergeben. Eingereicht werden können Arbeiten, welche sich speziell durch die Verbindung von verschiedenen Medien und Genres auszeichnen.\n\nIn dieser 1998 geschaffenen Kategorie werden Arbeiten von Kindern und Jugendlichen mit österreichischem Wohnsitz ausgezeichnet.\n\n\nVergeben vom Ludwig Boltzmann Institut \"Medien.Kunst.Forschung.\" werden hier herausragende theoretische Arbeiten ausgezeichnet.\n\nIn dieser Spezialkategorie vergibt die Ars Electronica in Kooperation mit der voestalpine Stipendien für zukunftsweisende, aber noch nicht realisierte Konzepte in den Bereichen Kunst, Design oder Technologie.\n\nAuf vorerst drei Jahre angelegt wurde 2011 das Artist in Residence-Programm \"Collide@CERN\" geschaffen. In Zusammenarbeit mit dem CERN erhalten Künstler die Möglichkeit, direkt in der Forschungseinrichtung über einen Zeitraum von bis zu drei Monaten Projekte zu realisieren.\n\nSeit 2013 werden in dieser Kategorie in Kooperation mit der Stiftung OHMI (\"One-Handed Musical Instruments)\" Innovationen im Bereich des Musikinstrumentenbaus ausgezeichnet, die Körperbehinderten das einhändige Spielen von Instrumenten auf professionellem Niveau ermöglichen sollen.\n\n"}
{"id": "202938", "url": "https://de.wikipedia.org/wiki?curid=202938", "title": "GAP (Software)", "text": "GAP (Software)\n\nGAP (Akronym aus: \"Groups, Algorithms and Programming\") ist ein freies Computeralgebrasystem. Es soll, ähnlich wie Mathematica, Berechnungen im Bereich der diskreten Algebra ausführen, insbesondere im Gebiet der algorithmischen Gruppentheorie.\n\nGAP wurde zwischen 1986 und 1997 am Lehrstuhl D für Mathematik an der RWTH Aachen entwickelt. Nach der Emeritierung des verantwortlichen Prof. Joachim Neubüser ging die Zuständigkeit für Entwicklung und Pflege an die Universität Saint Andrews in Schottland über. Im März 2005 wurde die Koordination unter mehreren GAP-Zentren aufgeteilt: die Universität Saint Andrews, die RWTH Aachen, die Technische Universität Braunschweig und die Colorado State University in Fort Collins.\n\nIm Jahr 2008 wurden die Entwickler von GAP, die \"GAP Group\", mit dem „ACM/SIGSAM Richard Dimick Jenks Memorial Prize for Excellence in Software Engineering applied to Computer Algebra“ ausgezeichnet.\nDie Preisverleihung fand im Juli 2008 während des \"International Symposium on Symbolic and Algebraic Computation 2008\" (ISSAC) in Linz statt.\n\nDie Software läuft auf jedem Unix-System, außerdem unter den Betriebssystemen Microsoft Windows und macOS. Es werden mindestens 32 MB Plattenplatz benötigt; die volle Distribution verwendet etwa 300 MB. GAP und sein Quelltext sind unter Copyleft-Bedingungen (GPL) frei verfügbar.\n\n gap> G:=SmallGroup(8,1); # G sei die erste im Katalog enthaltene Gruppe mit 8 Elementen.\n\n"}
{"id": "202994", "url": "https://de.wikipedia.org/wiki?curid=202994", "title": "Easterhegg", "text": "Easterhegg\n\nDas Easterhegg (auch \"Easter(H)egg\" oder \"./easter -h -egg\" oder \"EAST erh, egg\" – kurz einfach EH – genannt) ist eine jährliche internationale Veranstaltung des Chaos Computer Club. Sie findet seit 2001 während der Osterfeiertage statt. Das Easterhegg wendet sich an Hacker und Interessierte.\n\nDie Teilnehmerzahl des Easterheggs beschränkt sich auf einige hundert Leute, im Gegensatz zu den größeren Veranstaltungen des CCC (Chaos Communication Congress, Chaos Communication Camp), zu denen mehrere tausend Besucher kommen. Auf dem Easterhegg werden Workshops und Vorträge angeboten.\nEin Markenzeichen des Easterheggs ist das im Eintrittspreis enthaltene Frühstück bis zum Abend einschließlich Kaffee-Flatrate, zu der jeder Teilnehmer eine persönliche Erinnerungs-Tasse erhält. Dies soll den gemütlich-kommunikativen Charakter der Veranstaltung betonen.\nAuf dem Easterhegg 2007 demonstrierten Forscher der TU Darmstadt erstmals, wie der WLAN-Verschlüsselungsalgorithmus WEP in weniger als einer Minute geknackt werden kann.\n\nDas Easterhegg fand bis 2011 in ungeraden Jahren immer in Hamburg und dazwischen jeweils in anderen Städten statt. Bisherige Austragungsorte waren Düsseldorf (2002), München (2004, 2010), Wien (2006) und Köln (2008). 2012 fand das Easterhegg in Basel statt.\nAm 27. Juni 2012 wurde bekanntgegeben, dass das Easterhegg 2013 nicht wie erwartet in Hamburg, sondern in Paderborn stattfinden wird. Als Grund wurde die zu erwartende Entscheidung für Hamburg als Austragungsort des Chaos Communication Congress 2012 (29C3) angegeben. 2014 fand das Easterhegg in Stuttgart statt.\n\nAuf dem Easterhegg 2018 präsentierte das Phone Operation Center eine komplett neu entwickelte DECT Lösung mit Selbstregistrierungsfunktion. Dies sollte vor allem als Test für den 35C3 dienen, um zu prüfen wie sich das System bei gleichzeitigen Anmeldungen von vielen Teilnehmern verhält. Während der Veranstaltung wurden signifikante Änderungen an der Software gemacht und kritische Fehler behoben. Um das System einem erneuten Stresstest auszusetzen wurden, nachdem über 200 Telefone angemeldet waren, am 1. April um 16:00 Uhr alle Telefone der Besucherinnen und Besucher abgemeldet und diese per Hausdurchsage gebeten sich erneut anzumelden. Der Test verlief erfolgreich.\n\n"}
{"id": "204060", "url": "https://de.wikipedia.org/wiki?curid=204060", "title": "Liste der Heimcomputer", "text": "Liste der Heimcomputer\n\nHeimcomputer, alphabetisch nach Hersteller sortiert\n\n\n\n\n\n\n\n\n\n\n\n\nkompatibel mit Tandy TRS-80 Color Computer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "205367", "url": "https://de.wikipedia.org/wiki?curid=205367", "title": "Mathematica", "text": "Mathematica\n\nMathematica ist ein proprietäres Softwarepaket des Unternehmens Wolfram Research und stellt eines der meistbenutzten mathematisch-naturwissenschaftlichen Programmpakete dar. Mathematica 1.0 wurde 1988 auf den Markt gebracht.\n\nDas Softwarepaket „Mathematica“ enthält unter anderem\n\nDer Autor und Unternehmensbegründer Stephen Wolfram begann die Entwicklungsarbeit im Jahr 1986, die erste Version von Mathematica wurde 1988 herausgebracht.\n\nMathematica unterscheidet zwischen dem \"Kernel\", welcher die eigentlichen Berechnungen vornimmt, sowie dem \"Notebook\", welches eine reine grafische Benutzeroberfläche darstellt. Die Benutzung des Programms erfolgt in der Regel durch das Notebook, welches die Ein/Ausgabe formatiert darstellt. Das Notebook verfügt außerdem über Funktionen einer Textverarbeitung und erlaubt das Darstellen und Manipulieren von Grafiken. Ein besonderes Merkmal ist auch die umfangreiche Unterstützung mathematischer Sonderzeichen, die in Mathematica (im Gegensatz zu klassischen Programmiersprachen) an jeder Stelle (auch als Variablennamen) genutzt werden können.\n\nDie Auswertung oder Abarbeitung des Programmcodes erfolgt in der Regel gleich nach der Eingabe durch den Kernel (als einen Interpreter), Ergebnisse oder Programmierfehler sind damit sofort ersichtlich, es kann ein interaktives Programmieren erfolgen. Wird Programmcode mehrfach durchlaufen, etwa beim nicht-interaktiven Programmaufruf, so wird der Programmcode automatisch kompiliert. Der Programmcode ist betriebssystemunabhängig. Mathematica wird angeboten für Windows, Linux und Mac OS, bis einschließlich Version 6.0.3 auch zusätzlich für MS-DOS, NeXT, OS/2, Unix und VMS.\n\nAb Version 8 ist der sogenannte \"free form input\" in Mathematica verfügbar, der es ermöglicht, anstatt der Eingabe der korrekten Syntax für Berechnungen und andere Befehle, „natürliches“ Englisch zu verwenden. Dafür ist allerdings eine Internetverbindung nötig. So wird beispielsweise das gleiche Ergebnis – der Graph der Sinusfunktion mit hellroter Füllung und gestrichelten Linien – erzielt, wenn man\n\neingibt, wie aber auch den \"free form input\" verwendend\n\nals Befehl nimmt. Nachdem man einen mit \"free form input\" vorgenommenen Befehl getätigt hat, kann man denselben in die genaue Syntax übersetzen lassen und genauere Änderungen vornehmen, sodass Kenntnis über die Syntax dennoch stark von Vorteil bleibt.\n\nMathematica wird in der Wissenschaft beziehungsweise im Studium natur- oder wirtschaftswissenschaftlicher Fächer eingesetzt. Ebenso wendet es sich an den professionellen Anwender in der Industrie und Wirtschaft. So verwenden Banken Mathematica zur Simulation von Aktienkursentwicklung, Bewertung von Derivaten, Risikoabschätzung bzw. -wandlung und so weiter. Die Anforderung an die Korrektheit der Ergebnisse (analytisch wie numerisch) ist daher hoch.\n\nNeben den Grundrechenarten, Ableitungs- und Integralberechnung, Lösen von Gleichungssystemen, Matrizenmanipulation und numerischen Berechnungen in beliebiger Genauigkeit (keine Beschränkung auf die Maschinenpräzision) sind eine Vielzahl spezieller Funktionen, etwa aus den Bereichen der Kombinatorik, implementiert. Die Programmiersprache von Mathematica umfasst implizite Typenzuweisung und -wandlung, automatisches Speichermanagement und Musterauswertungstechniken (englisch \"pattern matching\"). Seit Ende 2013 wird die Programmiersprache \"Wolfram Language\" unabhängig von Mathematica vertrieben.\n\nDie Programmiersprache von Mathematica (\"Wolfram Language\") ist stark an die funktionale Programmiersprache Lisp angelehnt. Zusätzlich basiert Mathematica wesentlich auf Pattern Matching (wie zum Beispiel auch Haskell). Dies sorgt besonders bei Einsteigern für Verwirrung, weil \"Patterns\", also Muster-Platzhalter, vorkommen, sobald man mit Funktionen arbeitet. Zusammen mit den fast ständig genutzten eckigen Klammern entsteht so ein Code-Aussehen, das sich stark von den verbreiteten C-artigen Programmiersprachen unterscheidet. Bei folgender Definition handelt es sich für Mathematica nicht um eine Funktion, sondern um eine Ersetzungsregel, bei der jedes Vorkommen von codice_1 durch codice_2 ersetzt wird, wobei codice_3 sprichwörtlich alles sein kann:\n\nEine wirkliche Funktion im Sinne der funktionalen Programmierung/des Lambda-Kalküls wird hingegen durch codice_4 erzeugt:\ncodice_5\n\nDer Vorteil des Pattern-Matching in der Computeralgebra liegt darin, dass man komplizierte Ersetzungsregeln kompakt schreiben kann. Eine abschnittsweise definierte \"Funktion\" könnte man über Matching-Regeln wie folgt definieren:\n\nAufrufe codice_6 werden damit als codice_7 ausgewertet, wohingegen codice_8 die Auswertung codice_9 ergibt.\n\n]</nowiki></code> als TreeForm]]\nWie Lisp weist Mathematica die Eigenschaft der Homoikonizität auf. Das bedeutet, dass Mathematica-Code und das Ergebnis einer Berechnung aus demselben Typ Daten besteht. Mathematica-Anweisungen wie -Ausgaben sind eigentlich Bäume, und eine Auswertung einer Eingabe besteht in der Transformation eines solchen Baumes. Aus diesem Grund nennt man das erste \"Element\" eines Mathematica-Ausdruckes auch \"Head\", im folgenden Beispiel ist der \"Head\" die Anweisung \"Plus\":\n\nAls Baum dargestellt sieht dieser Ausdruck wie rechts gezeigt aus. Der \"Head\" (also Wurzelknoten) des Baumes ist die Funktion codice_10. Mathematica kennt verschiedene Weisen, Ein/Ausgaben darzustellen. In \"natürlicher\" Schreibweise entspricht dieser Ausdruck dem besser lesbaren\ncodice_11\n\nMathematica unterstützt als Computeralgebrasystem die Verarbeitung beliebiger Symbole in derartigen Ausdrücken. Anhand von einer Liste von Ersetzungsregeln werden diese Ausdrücke zu anderen Bäumen umgeformt. So zusammengebaut sind komplexe Rechnungen möglich. Mathematica ist damit dynamisch typisiert. Im Gegensatz zu anderen Sprachen ist bei Mathematica damit eine nicht auswertbare Zeile im Allgemeinen kein Fehler, sondern verbleibt unverändert als Rückgabe.\n\nDer Programmierer ist nicht auf ein einziges Programmierparadigma festgelegt, sondern kann ebenso imperative Anweisungen programmieren. Durch zehntausende von eingebauten Funktionen kann man sehr schnell umfangreiche Programme schreiben.\n\nMehrere Primzahlen werden von der Funktion berechnet\nUntenstehend drei Arten, mit „Mathematica“ den Mittelwert einer Werteliste zu berechnen. Im interaktiven Modus nummeriert Mathematica die Ein- und Ausgaben und liefert die Ergebnisse direkt.\n\n\"Werteliste definieren:\"\n\n\"Mathematica-eigene Funktion benutzen:\"\n\n\" Listenmanipulation benutzen: \"\n\n\" Prozedurales Vorgehen: \"\n2014 haben drei Mathematiker veröffentlicht, dass Mathematica bei der Berechnung von Determinanten bestimmter Matrizen mit recht großen Ganzzahlen (10.000 Stellen) falsche Ergebnisse liefert. Dieser Fehler sei 2013 gemeldet und auch nach über einem Jahr nicht behoben worden; die Darstellung von Wolfram Research widerspricht diesen Angaben jedoch in Teilen und geht von einem neuen Fehler aus. Der Berechnungsfehler (\"CASE:303438\") ist ab Version 11.1 behoben und nicht mehr vorhanden.\n\n\n\n"}
{"id": "208231", "url": "https://de.wikipedia.org/wiki?curid=208231", "title": "Computermusik", "text": "Computermusik\n\nComputermusik ist Musik, zu deren Entstehung die Verwendung von Computersystemen notwendig oder wesentlich ist. Der Begriff der elektronischen Musik bezieht sich dagegen allgemeiner auf die elektronische Erzeugung der Klangsignale.\n\nBereits im 17. Jahrhundert wuchs die Überzeugung, dass Musik die Kunst geschickter Zahlenordnungen ist. Als einen der frühesten Überlieferungen dieser Art gilt die \"Arca Musarithmica\", die in der 1650 gedruckten \"Musurgia Universalis\" des Jesuitenpater und Musikgelehrten Athanasius Kircher erwähnt wurde. Ebenso sind von Caspar Schott Schriften zu mechanischen Musikinstrumenten überliefert.\n\nNach 1770 verlagerte sich das Zentrum von Musikautomaten von Deutschland nach Frankreich, England und in die Schweiz. Erwähnenswert sind die Erfindungen des Franzosen Vaucanson. Bei einigen Erzählungen E. T. A. Hoffmanns war der mechanische Automat deren Gegenstand.\n\nUnter anderen soll Wolfgang Amadeus Mozart sich ein \"Musikalisches Würfelspiel\" (KV 294 d) ausgedacht haben, eine „Anleitung, Walzer oder Schleifer mit zwei Würfeln zu componieren …“. Dort sind in einer Tabelle 3/8-Takte im Klaviersatz aufgelistet, deren Auswahl durch die Augenzahl der geworfenen Würfel geschieht und hintereinander notiert, eine fertige Komposition ergeben.\n\nWenn nun der Computer „würfelt“, d. h. Zufallszahlen erzeugt, entsprechen den Zahlen Noten. Gleichzeitig werden dem Computer Regeln beigebracht, die darüber befinden, welche der erwürfelten Noten zugelassen werden und welche ggf. verworfen werden, da sie in dem Fall den Regeln widersprechen. Die Regeln können einem Lehrbuch entnommen sein, oder man programmiert den Computer so, dass er z. B. nach Eingabe eines Bachchorals die dortigen Regeln und Bedingungen selber findet, etwa welche Harmonien vorkommen, welche Tonfolgen öfter auftreten, usw. Der russische Mathematiker Andrei Andrejewitsch Markow führte im Zusammenhang mit Textuntersuchungen die nach ihm benannten Markow-Ketten ein, bei denen Übergangswahrscheinlichkeiten für einzelne Elemente betrachtet werden. Claude Elwood Shannon, der Begründer der Informationstheorie, hat dann darauf hingewiesen, dass die Markowmethode auch bei musikalischen Experimenten Anwendung finden könnte.\n\nIm August 1951 wurde mit dem australischen CSIRAC (Council for Scientific and Industrial Research Automatic Computer) öffentlich Musik wiedergegeben.\n\nLejaren A. Hiller und Leonard M. Isaacson schließlich nannten den vierten (Experimenten-)Satz ihrer ILLIAC-Suite für Streichquartett (die erste, 1955/1956 entstandene Computerkomposition) dementsprechend \"Markov chain music\". Im ersten und zweiten Satz dominierten hier die \"Palestrina-Kontrapunktregeln\", welche Johann Joseph Fux in seinem berühmten Werk \"Gradus ad Parnassum\" formuliert hatte. Im dritten und vierten Satz dominierten neuzeitlichere Kompositionsregeln wie Zwölftontechnik bis hin zu Stochastik. Bei Hillers zweitem Projekt, der \"Computer Cantata\", kam ein spezielles Kompositionsprogramm namens MUSICOMP zur Anwendung.\n\nIn den Bell Laboratories in den USA beschäftigte sich Max Mathews ursprünglich mit künstlicher Sprachsynthese. Um den \"IBM 7090\" als Klangerzeuger zu verwenden, unterteilte er den Klangsyntheseprozess in zwei Phasen: in der ersten wurden die Momentanwerte der Wellenform in einem Datenspeicher (Magnetband) abgenommen; in der zweiten wurden die gespeicherten Werte ausgelesen und in digitale Audiosignale umgewandelt. Um die großen Zahlenmengen durch wenige musikalische Parameter zu ersetzen, entstand die \"MUSIC\" Computerprogramm-Familie. Bei dem Programm \"MUSIC III\" (1960) konnten erzeugte Klänge zur Modulation weiterer Oszillatoren eingesetzt werden. Unter dem Titel \"The Technology of Computer Music\" lieferte Max Matthews gemeinsam mit seinen Mitarbeitern eine gründliche Beschreibung der Programmiersprache \"MUSIC V\", die in den 1970er Jahren auf dem Gebiet der Computerklangsynthese einen bedeutenden Meilenstein markierte. Das später durch Miller Puckette bekannt gewordene Programm Max/MSP (1997) ist nach Matthews benannt.\n\nEines der ersten Hybridsysteme (analog und digital) war der 1970 von Max Matthews und John R. Pierce in den \"Bell Telephone Laboratories\" konstruierte \"GROOVE-Synthesizer\". Der Komponist hatte hier die Möglichkeit, sein zuvor programmiertes Stück in verschiedenen Interpretationen wiederzugeben. Zur selben Zeit wurde das Hybridsystem MUSYS von David Cockerell, Peter Grogono und Peter Zinovieff in England entwickelt.\n\nEnde der 1970er Jahre entstanden sogenannte \"Gemischte Digitale Systeme\", bei denen ein Computer einen anderen klangerzeugenden Computer steuern konnte. Pionierarbeit leisteten hier die Electronic Music Studios. Ab 1976 entwickelte Giuseppe Di Giugno für das Pariser Klangforschungsinstitut IRCAM mehrere digitale Synthesizer unter Beratung von Pierre Boulez und dem Komponisten Luciano Berio. Ebenfalls wurde an der University of Toronto der \"SSSP-Digital-Synthesizer\" von einer Forschungsgruppe konstruiert. In Australien wurde der Fairlight CMI entworfen. Gleichzeitig wurde von den Amerikanern Jon Appleton, Sydney Alonso und Cameron Jones das Synclavier entwickelt.\n\nZur zentralen Kategorie musikalischer Abläufe wurde für Iannis Xenakis die Dichte von Klangerzeugnissen und ihre Anordnung nach den Gesetzen mathematischer Wahrscheinlichkeit. Er benutzte für seine ersten Werke einen IBM-7090-Rechner. Musikalische Experimente ergaben sich hierbei mit der Spieltheorie, der Gruppentheorie und der Reihentheorie. Mit dem System \"UPIC\" konnten dabei größere Anforderungen an den Rechner vorgenommen werden.\n\nAnfang 1983 einigten sich führende Synthesizer-Hersteller auf einen einheitlichen Standard, damit eine größere Anzahl an Synthesizertypen durch möglichst viele Computertypen steuerbar wird: \"Musical Instrument Digital Interface\", kurz MIDI.\n\nAuch die Komposition von Musik kann mithilfe von Computern erfolgen. \"Partitursynthese\" ist ein Anwendungsbereich der rechnerunterstützten Komposition in Form von rechnergenerierten Partituren. In einer Reihe von Ansätzen wurde versucht, derartige Strukturen durch Programmierung abzubilden, anfangs etwa mit der Programmiersprache Fortran.\n\nMit der fortschreitenden Entwicklung der Elektronik und der Digitaltechnik wurden die Verfahren der Klangsynthese zunehmend flexibler und leistungsfähiger.\n\nDie additive Synthese ist zur Klangsynthese von Interesse, grundsätzlich ist ihre Anwendung jedoch nicht an digitale Signalverarbeitung gebunden.\n\nEin Klang erweist sich auf dem Bildschirm eines Oszillografen als periodische, nicht sinusförmige Kurve (so z. B. rechtes Bild der Abb. A). Wie Jean Baptiste Fourier festgestellt hat, kann jede periodisch verlaufende Schwingung als Überlagerung von sinusförmigen Kurven (verschiedenster Frequenz und Amplitude) aufgefasst werden. Dies ergibt die Möglichkeit, Klänge (periodische Kurven) durch Addition einzelner Töne (schlichte Sinuskurven) zusammenzusetzen (Klangsynthese). Durch Auswahl und Variation z. B. der Amplitude der einzelnen Komponenten ergibt sich eine Vielzahl von verschiedenen Klängen (Additive Synthese) (vgl. Abb. A). Da jedoch ein Klang, z. B. der der auf dem Klavier angeschlagenen Taste a' (mit der Grundschwingung 440 Hz), sich während seiner Dauer ändert (Jean-Claude Risset), reicht diese einfache Klangsynthese zur Erzeugung eines echten Klavierklanges nicht aus. Ganz wesentlich hierfür erweist sich u. a. der Einschwingvorgang, d. h. wichtig ist der Zeitraum, in dem sich der Klang aufbaut. Darüber hinaus trägt \"farbiges Rauschen\", das aus fast unendlich vielen Teilschwingungen mit einem Frequenzmaximum besteht, in besonderem Maße zum Klang bei.\n\nEine wichtige Charakteristik sind die zeitlichen Differenzen beim Auf- und Abbau von Obertönen; jeder einzelne Oberton besitzt gewissermaßen seine eigene komplexe Hüllkurve. Bei elektronischen Orgeln bleibt die Realisierung des additiven Syntheseprinzips auf das kontrollierte Hinzufügen einiger in festen Intervallen zum Grundton gestimmten sinusförmigen Oberwellen beschränkt (meist weniger als zehn). Daher ist hier keine Hüllkurvenbeeinflussung möglich.\n\nDie Schnelle Fourier-Transformation erreichte hier durch mehrere kleinere Computersysteme eine Reduzierung des Speicherplatzbedarfs. Für eine vollwertige additive Synthese ist die separate Beeinflussung der einzelnen Hüllkurven notwendig. Ob eine gemeinsame Hüllkurve für alle Obertöne, oder die einzelne Programmierung für mehrere Obertöne eines Klanges – die Klangergebnisse unterscheiden sich deutlich in der Qualität. Hüllkurven-Kopierfunktionen können hier Hilfe leisten.\n\nÄndert sich die Frequenz f eines Tones periodisch, so bezeichnet man diese Tonhöhenschwankungen als Vibrato. Man sagt, der Ton sei frequenzmoduliert (vgl. Abb. B). Nähern sich \"Trägerfrequenz\" und \"Modulationsfrequenz\" (siehe Abb. C), so ergeben schon wenige Wellenformen, z. B. die beiden in der Abb. C, reiche Resultate. Diese FM-Synthese ist einerseits weniger aufwändig als die additive Klangsynthese und außerdem eine flexiblere Technik, weil Träger- und Modulationsschwingungen jedwede Form (nicht bloß Sinusschwingungen) annehmen dürfen und sich so völlig neue Klänge synthetisieren lassen.\n\nBeim analogen Synthesizer ist es möglich, die Stärke der Modulationsspannung sowie deren Frequenz zu verändern. Dazu wird i. d. R. der Modulationsoszillator LFO verwendet. Die untere Grenze seines Frequenzbereiches beginnt außerhalb der menschlichen Hörfläche (etwa bei 0,01 Hz). Moduliert man die Lautstärke eines Klanges im VCA mit einer LFO-Frequenz, die innerhalb des Bereiches von 0,01 bis 16 Hz ansteigt, so erfährt dieser Klang in Abhängigkeit von Modulationsfrequenz und -stärke zunehmende rhythmische Veränderungen. Der bekannte Tremolo-Effekt steigert sich bis zu einer eigentümlichen Rauheit des Klanges; man spricht hier von \"subauditiver Steuerung\". Erst durch die Erhöhung der LFO-Frequenz in den menschlichen Hörbereich entsteht ein stationärer Klang.\n\nDen Ausgangspunkt der Wellenform-Synthese bildet eine Sinuswelle, deren Amplitude von einem Hüllkurvengenerator kontrolliert wird. Diese Welle passiert eine Baugruppe, die als nichtlinearer Prozessor (engl. \"waveshaper\") bezeichnet wird. Der Prozessor formt dabei die obertonlose Sinuswelle in ein Klangsignal mit wechselndem Obertonanteil um. Die Firma Casio entwickelte Anfang der 1980er Jahre die Phase-Distortion-Synthese, eine Variante der Wellenform-Klangsynthese, die 1988 zur \"Interactive Phase Distortion\" erweitert wurde.\n\nSound-Sampling ist ein digitales Verfahren zur Speicherung von Klängen. In der Praxis ist Sound-Sampling als Kombination mit digital und analog vorgenommenen Klangmanipulationen bereits eine eigenständige Klangsynthesetechnik geworden, obwohl es sich vom Prinzip her um die Wiedergabe bereits vorhandener Klänge handelt. Mit zunehmender Speicherkapazität von Rechnern konnten Wellenformen mechanischer Instrumente mit enormem Speicher und deren Ein- und Ausgangsvorgänge in einer Wavetable im Computer gespeichert werden. Durch Manipulation (Filter, Modulatoren, u. a.) dieser Werte lassen sich Klänge verändern sowie Effekte (z. B. Hall) hinzufügen. Da einerseits das Hören als analoger Vorgang angesehen werden kann, der Computer auf der anderen Seite eine digitale Maschine ist, müssen bei der Ausgabe die diskreten Zahlen des Computers in glatte (elektrische Spannungs-)Kurven umgewandelt werden, um letztlich per Lautsprecher hörbar gemacht zu werden. Dies geschieht über Digital-Analog-Wandler. Umgekehrt gelangen analoge Schallereignisse in den Computer, wenn Analog-Digital-Wandler eingesetzt werden. In diesem Fall können momentane Amplituden (Hochwerte) einer Schallkurve – ein reiner Ton, über ein Mikrofon aufgenommen und auf dem Bildschirm eines Oszillographen (= Schwingungsschreiber) sichtbar gemacht, ergibt eine Sinuskurve – in kleinen Zeitschritten abgetastet werden, wobei aus der (glatten) Schallkurve eine Treppenkurve entsteht, die umso besser sich dem Original nähert, je mehr Abtastungen in der Zeiteinheit stattfinden, je höher also die Abtastrate ist. Setzt sich die Schallkurve aus mehreren Sinuskurven zusammen, wird also ein Klang aufgenommen, so muss nach dem Abtasttheorem von Nyquist und Shannon (1948) die Abtastrate doppelt so hoch angesetzt werden wie die höchste der in der Summe vorkommenden Frequenz (Schwingungszahl f, gemessen in Hertz (Hz)), soll der gespeicherte Klang in all seinen Feinheiten vollkommen aufgezeichnet oder wiedergegeben werden. Wenn z. B. auf einer Compact Disc die Abtastrate (Samplingfrequenz) 44,1 Kilo-Hertz beträgt, dann liegt die höchste (theoretisch) vorkommende Frequenz f bei 22,05 Kilo-Hertz.\n\nWenn ein Instrumentalklang mathematisch analysiert wird, können durch mathematische Vorgaben Klänge entstehen. Wegen sehr hohen Rechenanforderungen hat man nach Ersatzmodellen (Julius O. Smith) gesucht, indem z. B. elektrische Wellenleiter anstelle von Longitudinalwellen in einem Rohr, wie sie in einer schwingenden Pfeife vorkommen, untersucht werden.\n\nGranularsynthese bezeichnet eine Sample-Methode, durch die Klänge aus extrem kurzen Fragmenten hergestellt werden können. Formen der Granularsynthese sind \"Glisson-Synthese\", \"Pulsar-Synthese\" u. a. (Curtis Roads).\n\n\"Sequenzer\"- bzw. \"Composer-Programme\" dienen der externen Tonhöhensteuerung. Die Tastatur eines Synthesizers wird hier durch einen Computer ersetzt, der den Synthesizer mit zuvor programmierten Tonhöhenabläufen steuert. Entweder können musikalische Parameter einzeln eingegeben werden oder Töne werden in Echtzeit eingespielt, gespeichert und verändert.\n\nUm den Prozess der Klangsynthese anschaulich zu steuern, werden Sound-Editor- bzw. Voicing-Programme verwendet. Vorteilhaft sind dabei die graphische Darstellung der Parameter sowie dynamische Klangverläufe wie Hüllkurvendarstellungen auf dem Bildschirm. Der Trend geht hier zu universellen, für mehrere Synthesizertypen gleichzeitig verwendbaren Editorprogrammen.\n\n\"Tracker\" bezeichnen Software-Sequenzer-Programme; die Audioschnittstellen sind meist alphanumerisch, Parameter oder Effekte werden hexadezimal eingegeben.\n\n\n\n"}
{"id": "208768", "url": "https://de.wikipedia.org/wiki?curid=208768", "title": "Winsock", "text": "Winsock\n\nWinsock oder Windows Sockets ist eine Windows-Programmierschnittstelle (API) zum Zugriff auf Rechnernetze über Sockets.\n\nWinsock ergänzt Windows um die Internetprotokollfamilie und ist für die Verbindung des PC mit dem Internet zuständig. Es wird von Programmen verwendet, um über das Internetprotokoll (IP) auf das Netzwerk zuzugreifen. Das Programm ruft dazu bestimmte Funktionen in der Bibliothek winsock.dll (bzw. ab Windows 95/NT wsock32.dll) auf.\n\nWenn ein Programm (zum Beispiel ein Web-Browser) über das Netzwerk kommunizieren will (zum Beispiel einen Webserver kontaktiert), gibt es die Anforderung an Winsock weiter, welches dann versucht, die Verbindung aufzubauen. Winsock meldet an das aufrufende Programm zurück, ob der Verbindungsaufbau gelungen oder fehlgeschlagen ist. Konnte eine Verbindung hergestellt werden, können mit weiteren Winsock-Funktionsaufrufen über diese Verbindung Daten über das Netzwerk ausgetauscht werden (z. B. Laden von Internetseiten).\n\nDurch die Verwendung der Winsock-API ist es prinzipiell möglich, IP-benutzende Programme zu schreiben, die quellcodekompatibel zwischen Windows und POSIX-Betriebssystemen sind, wobei seit Winsock 2.0 auch andere Protokolle als die der Internetprotokollfamilie möglich sind, wie z. B. IPX.\n\nWinsock wurde 1992 von Microsoft entwickelt. Dazu wurden praktisch alle Funktionen von den Betriebssystemen Unix und BSD übernommen. Zuvor war man auf Implementationen von Drittanbietern angewiesen, wie z. B. der verbreiteten Shareware von Peter Tattam.\n\n"}
{"id": "209180", "url": "https://de.wikipedia.org/wiki?curid=209180", "title": "Reason (Software)", "text": "Reason (Software)\n\nReason ist eine Musiksoftware der schwedischen Firma Propellerhead, die seit dem Jahr 2000 auf dem Markt ist und inzwischen in der Hauptversion 10 vorliegt.\n\nAngefangen hat es bei der schwedischen Firma Propellerhead im Jahre 1997 mit dem Programm ReBirth RB-338, einer virtuellen Wiedergeburt der TB-303, TR-909 und des TR-808 von Roland. Drei Jahre zuvor hatte die Firma bereits den Loop-Editor ReCycle auf den Markt gebracht, damals noch unter der Obhut von Steinberg.\n\nEigentlich sollte schon ReBirth ein komplettes virtuelles Studio werden. Laut Ernst Nathorst-Böös, einem der drei Gründungsväter von Propellerhead, war damals die Computerleistung einfach noch nicht ausreichend. Außerdem bestand die Firma zu dem Zeitpunkt tatsächlich nur aus drei Leuten. Es dauerte weitere drei Jahre, bis die erste Version von Reason auf den Markt kam.\n\nIm Jahre 2002 wurde das Programm auf der Frankfurter Musikmesse mit dem MIPA-Preis für das beste Software-Instrument ausgezeichnet.\n\nDer Vorgänger ReBirth ist mittlerweile als Freeware erhältlich (siehe Weblinks).\n\nDie Software ist als virtuelles Studiorack mit integriertem Sequenzer und Mixer konzipiert und läuft in der aktuellen Version 9 ab Windows 7 und Mac OS X 10.7 bis zu einer Samplingfrequenz von 96 kHz und 24 bit. Seit der Version 6 ist Reason mit dem vorher eigenständigen Programm Record verschmolzen. Seitdem kann Reason auch für Audioaufnahmen genutzt werden.\n\nReason beinhaltete mit Subtractor, Malström und Thor drei Synthesizer, zwei Sampler (NN19 und NN-XT), einen Loop-Player (Dr. Rex) sowie mit Redrum einen Drumcomputer als Klangerzeuger. Klangerzeuger und Effekte können im Combinator zu neuen Geräten verknüpft und gespeichert werden. In Reason 5 wurden die Klangerzeuger um Kong erweitert. Dieser Drumsoundspezialist besteht aus 16 Pads und verschiedenen Klangerzeugungsmodulen sowie Effekten für diese. Der Loop-Player Dr. Rex ist zudem durch Dr. Octo Rex ersetzt worden. Die neue Live-Sampling-Funktion ermöglicht es, mit allen vier Sampling-fähigen Klangerzeugern (NN19, NN-XT, Redrum und Kong) direkt Klänge aufzunehmen und einzusetzen.\n\nSampler und Drummachines können mit beliebigen Samples verwendet werden. Außerdem lassen sich simultane Audiospuren des Vorgängerprodukts ReBirth (nur unter Windows möglich) und vom Vorgängerprodukt ReCycle präparierte und in einem speziellen Datenformat vorliegende Loops innerhalb von Reason verwenden. Die Auswahl der Effektgeräte reicht von einfachen Delays über ein hochwertiges Hallgerät bis zu einem Loudness Maximizer, so dass nahezu die komplette Produktionskette abgedeckt wird.\n\nIm Rack lassen sich untereinander beliebig viele Geräteinstanzen erzeugen. Verbunden werden diese Geräte auf der Rückseite des Racks durch virtuelle Kabel. Dabei gibt es zwei Kategorien von Anschlüssen: Audio und CV (Control Voltage). Die Control Voltage ist eine Steuerspannung und dient der Übertragung von Steuersignalen an die Synthesizer (Note, Reglerwerte) bzw. ausgewählte Effektgeräte, so wie es bei den ersten Synthesizern des Robert Moog auch schon der Fall war.\n\nAnschluss zur Außenwelt bietet das Reason Hardware Interface, welches bis zu 64 Audio- und MIDI-Kanäle unterstützt. Die Software kann über eine MIDI Clock zu externem Equipment synchronisiert werden. Innerhalb eines Rechners können via ReWire MIDI- und Audiodaten synchron in eine andere Software übertragen werden.\n\nReason benutzt das hauseigene Refill-Format zur Erweiterung. In diesem Format sind alle für Reason benutzbaren Dateien in einer Zusammenstellung komprimierbar (Patches, Samples, Rex Dateien). Dabei werden die WAVE/AIFF Dateien besonders klein komprimiert, wobei aber gleichzeitig keine Klangqualität verloren gehen soll. Des Weiteren können alle benötigten Daten in eine Datei gepackt werden, welche an andere Reason-Benutzer weitergegeben werden kann – so können Songprojekte leichter ausgetauscht und bearbeitet werden.\n\nDer in Reason integrierte Synthesizer Malström des Entwicklers Magnus Lidström bedient sich einer recht ausgefallenen Syntheseform: der Graintable-Synthese. Sie beruht auf einer Kombination aus der populären Wavetable-Synthese, die zum Beispiel schon im Waldorf PPG verwendet wurde und der Granularsynthese.\n\nIn einer Wavetable sind verschiedene Wellenformen als Samples tabellarisch angelegt, zum Beispiel Sinus, Sägezahn, Rechteck oder auch ganz ausgefallene Schwingungen. Diese werden geloopt und dann vorwärts oder rückwärts in unterschiedlichen Geschwindigkeiten abgespielt. Die Wavetable bildet sozusagen den Oszillator des Synthesizer, wobei bei einem echten Oszillator die Schwingungen tatsächlich erst erzeugt werden und nicht als Samples vorliegen.\n\nAuch im Rahmen der Graintable-Synthese bilden gesampelte Wellenformen die Grundlage für den Klang. Der Hauptunterschied zur Wavetable-Synthese liegt darin, dass selbst eine einzelne gesampelte Schwingung in viele kleine Abschnitte, so genannte „Grains“, zerlegt wird. Zusätzlich werden auch mehrere Loops in einer Schwingung gesetzt und beliebig oft wiederholt. Dadurch erhält man eine sehr flexible Klangerzeugung. Tonhöhe und Wiedergabegeschwindigkeit sind unabhängig voneinander. Außerdem werden die einzelnen Grains in einer Tabelle wie bei der Wavetable-Synthese angelegt, die dann in verschiedenen Geschwindigkeiten, Richtungen und Tonhöhen „durchfahren“ werden können.\n\nMit Version 6.5 wurde die proprietäre Plugin-Schnittstelle Rack Extensions (RE) eingeführt. Diese ermöglichte es Drittanbietern, eigene Erweiterungen für Reason zu entwickeln und zu verkaufen. Der Vertrieb von REs erfolgt ausschließlich über die Website von Propellerhead Software. Diese werden von Propellerhead jedoch vor der Veröffentlichung geprüft. Außerdem verspricht Propellerheads Software die Kompatibilität der Rack Extensions auf alle zukünftigen Versionen von Reason und den unterstützten Betriebssystemen.\n\nWurde anfangs noch mit bekannten Marken wie KORG oder GForce Software geworben, so wurde das neue Format sehr schnell von ambitionierten Nutzern erobert, welche innerhalb weniger Monate zahlreiche Utilities entwickelten, die von den Reason-Nutzern meist begeistert aufgenommen wurden. Inzwischen haben sich weitere bekannte Anbieter wie U-he, Rob Papen, Softube oder Synapse Audio dem neuen Format gewidmet und teilweise exklusive Plugins entwickelt – und umgekehrt wurden von einigen der ursprünglich „ambitionierten Nutzern“ inzwischen durchaus professionelle Produkte vorgestellt.\n\nTrotz allem hat das RE-Format seine Grenzen: Es ist an die Reason-eigene Rack-Analogie gebunden, d. h. eine RE \"muss\" in das virtuelle 19″-Rack passen. Außerdem sind die Designmöglichkeiten noch eng begrenzt und im Wesentlichen auf Knöpfe, Schalter und einfache Displays beschränkt. Designfreiheit wie beispielsweise bei der VST-Schnittstelle existiert bis dato nicht. Andererseits bieten REs – Reason ist nun mal ein Rack – auch eine Rückseite (mit allen Verkabelungsmöglichkeiten).\n\nReason war lange Zeit eine All-in-one-Lösung, d. h. die gesamte Produktion geschah in Reason. Nachdem der Hersteller Propellerhead Software im Laufe der Jahre zuerst Audio-Aufnahme und -Bearbeitung und später Plugins (wenn auch in einem eigenen Format) ermöglichte, so wurde mit Reason 7 die Option eingeführt, welche seit Jahren von den Nutzern verlangt wurde: MIDI-Out, d. h. das Steuern von externen MIDI-Geräten über Reason.\n\nSeit dem Softwareupdate auf Version 9.5 (Mai 2017) ist es nun auch möglich VST- Instrument und -Effekte in der VST -Version 2.4 (VST3 ist nicht kompatibel) einzubinden. Diese müssen dann allerdings in der jeweiligen Bitbreite passend zu Reason vorliegen. Somit erfordert z. B. die 64-Bit-Version von Reason, dass die VSTs dann ebenfalls in 64 Bit vorliegen. Über eine eigene 32/64bit-Bridge verfügt Reason (noch) nicht.\n\nDer Hauptvorteil von Reason liegt im All-in-one-Konzept der Software. Professionelle Effektgeräte und virtuelle Instrumente sind bereits enthalten, funktionieren reibungslos miteinander und müssen nicht erst hinzugekauft werden. Durch den modularen Aufbau lädt das Programm zum Experimentieren ein.\n\nDie Software ist ebenfalls bekannt für ihre Stabilität, unabhängig vom verwendeten Betriebssystem. Viele professionelle Musiker nutzen sie deshalb auch live auf der Bühne.\n\nDurch die virtuelle Nachbildung eines Studioracks finden sich besonders die Anwender schnell zurecht, die bisher nur mit entsprechender Hardware gearbeitet haben. Aber auch Neulingen dürfte sich das Konzept rasch erschließen. Nahezu alle Funktionen sind über einen Knopf oder Regler direkt auf der Benutzeroberfläche zugänglich, was intuitives Arbeiten ermöglicht.\n\nDie mitgelieferten Samples, Loops und Geräte-Voreinstellungen in den beiden Soundbänken sind zahlreich und auf technisch und musikalisch hohem Niveau, so dass man sogleich mit dem Songschreiben beginnen kann.\n\nReason kann über die ReWire-Funktion an Audio-Sequenzer wie Steinbergs Cubase, Apple Logic, FL Studio oder Renoise gekoppelt werden; bis dato allerdings nur als Slave, Ableton Live kann sowohl als ReWire Slave und als Master betrieben werden.\n\nDie interne Auflösung wird zwar mit 32 bit angegeben, die meisten Parameter bieten jedoch nur das MIDI-Raster mit maximal 128 Abstufungen. Gerade zur Automation von Lautstärke- oder Filterverläufen wäre eine feinere Abstufung sinnvoll.\n\nGewöhnungsbedürftig ist auch die Tatsache, dass die Werte der Parameter oft keine konkrete Bezeichnung besitzen, sondern generell einen Wertebereich von 0 bis 127 besitzen.\nAls Exportformate für Audiodaten stehen lediglich WAVE und AIFF zur Verfügung.\n\nVerschiedene Hersteller wie Image Line (FL Studio) oder Synapse Audio (Orion) bieten ebenfalls virtuelle Studios an, die zum Teil auch Audioaufnahme und Plug-in-Unterstützung bieten.\n\nSeit Version 10 ist die direkte Aufnahme von Audio sowie die Manipulation innerhalb von Reason möglich. Des Weiteren können Audio Dateien direkt im Sequenzer geladen oder per Drag'n'Drop platziert werden. Seit Version 10 besitzt Reason eine VST-Plugin Schnittstelle. Die Plugins können als Instrumente, Effekte usw. eingebunden werden und das Ausgangssignal weiterverarbeitet werden. Parameter Fernsteuerung der Plugins ist ebenfalls möglich, sowohl mit dem Sequenzer als auch anderen Reason Komponenten.\n\nMöglich ist natürlich auch die Kombination eines beliebigen Sequenzers wie Steinbergs Cubase, Apple Logic oder Digidesign Pro Tools mit dem Zukauf von virtuellen Instrumenten und Effektgeräten; dies wird allerdings erheblich teurer als Reason.\n\nEine weitere Alternative zu Reason ist der Sequenzer Ableton Live, der ebenfalls modular miteinander verknüpfbare Effekte und Klangerzeuger mitbringt, zusätzlich aber Plug-ins anderer Anbieter im VST- und AU-Format unterstützt sowie die Aufnahme und tempounabhängige Wiedergabe von Audiospuren.\n\n\n\n"}
{"id": "209561", "url": "https://de.wikipedia.org/wiki?curid=209561", "title": "Web Mining", "text": "Web Mining\n\nUnter Web Mining (') auch Webmining versteht man die Übertragung von Techniken des Data-Mining zur (teil)automatischen Extraktion von Informationen aus dem Internet, speziell dem World Wide Web. Web Mining übernimmt Verfahren und Methoden aus den Bereichen Information Retrieval, maschinelles Lernen, Statistik, Mustererkennung und Data-Mining.\nDabei können drei Untersuchungsgegenstände unterschieden werden:\n\n\"Web-Usage-Mining\" versucht Regularitäten in der Benutzung von Webseiten beziehungsweise Webressourcen zu erkennen. Dabei werden alle sekundären Daten, die durch Interaktion des Benutzers mit einer Webressource entstehen, verarbeitet und analysiert. Zum Web-Usage-Mining gehört beispielsweise auch die Analyse der Customer Journey.\n\n\"Web-Structure-Mining\" versucht, die einer Webseite beziehungsweise Domäne zugrunde liegende Verweisstruktur zu erkennen. Basierend auf der Topologie der Verweise (Hyperlinks) der Webseite, mit optionaler Beschreibung derselben, wird ein Modell erstellt. Dieses kann für die Kategorisierung und das Ranking einer Webseite nützlich sein und lässt Rückschlüsse auf Ähnlichkeiten zwischen Webseiten und deren Beziehungen zueinander zu. Zum Beispiel könnten inhaltsreiche Webseiten (sog. Authorities) und überblicksartige Webseiten (sogenannte Hubs) für ein bestimmtes Thema ausfindig gemacht werden (vgl. HITS Algorithmus). \n\n\"Web-Content-Mining\" befasst sich mit der Erkennung von Regularitäten in den Inhalten einer Webressource. Web-Content-Mining ist ein Anwendungsgebiet für das Text Mining. Die Daten im Web bestehen aus unstrukturierten Daten wie Textdokumenten, semi-strukturierten Daten wie HTML-Dokumenten und stärker strukturierten Daten wie Tabellen oder dynamisch generierten HTML-Seiten. Grundsätzlich bestehen die Inhalte einer Webseite aus verschiedenen Datentypen, wie Texten, Bildern, Audio-, Video-, Metadaten und Hyperlinks. Web-Content-Mining von multiplen Datentypen wird als „Multimedia-Data-Mining“ bezeichnet und kann als Teil von Web-Content-Mining verstanden werden. Hauptsächlich bestehen die Inhalte des Webs jedoch aus unstrukturiertem Text. Text Mining kann als Ausprägung und übergeordnetes Forschungsgebiet von Web-Content-Mining verstanden werden. Die verwendeten Methoden sind allgemeine Data-Mining-Methoden, wobei statistische und computerlinguistische Verfahren die Transformation der Texte in eine (für das Data-Mining) adäquate Form realisieren.\n\n\n\n"}
{"id": "209973", "url": "https://de.wikipedia.org/wiki?curid=209973", "title": "Webby Awards", "text": "Webby Awards\n\nDie Webby Awards werden einmal im Jahr von der \"International Academy of the Digital Arts and Sciences\" vergeben. Ausgezeichnet werden die besten Arbeiten in vier Haupt- und über 100 Unterkategorien – von interaktiver Werbung über Websites bis hin zu Online-Filmen und Videos. Darüber hinaus haben Teilnehmer die Gelegenheit, den \"People’s Voice Award\" zu gewinnen, eine Auszeichnung, die nicht von der Award-Jury, sondern vom Publikum vergeben wird.\n\nDie Webby Awards wurden 1996 ins Leben gerufen und waren ursprünglich ein Bestandteil von \"The Web\", einem Magazin des IDG-Verlags \"()\". Anfänglich beschränkten sie sich auf 15 Kategorien. Die Gewinner wurden damals von den Internetexperten auserkoren, die später auch die \"\" gründeten. 1998 stellte der IDG-Verlag zwar \"The Web\" ein, erlaubte aber dem Webby-Team die Awards weiterhin auszurichten. Folgende Kategorien werden ausgezeichnet: \n\n2018 wurden die zum 22. Mal vergeben und erhielten fast 13.000 Einreichungen aus über 70 Ländern, davon 464 Auszeichnungen.\n\nMittlerweile gibt es bei den \"Webby Awards\" – neben den Auszeichnungen für herausragende Leistungen (') – sieben Kategorien mit insgesamt mehreren Hundert Unterkategorien. Websites können in mehreren Unterkategorien gleichzeitig eingereicht werden – und auch in mehreren Kategorien gewinnen. In jeder Unterkategorie werden zwei Awards verliehen: der von einer renommierten Jury vergebene \"Webby Award\" und der ', dessen Gewinner von den Besuchern der Webby-Awards-Webseite ausgewählt wird.\n\nDie Webby Awards sind dafür bekannt, die Rede der Gewinner auf lediglich fünf Wörter zu beschränken, was häufig zu lustigen Aussagen führt: David Bowie bemerkte zum Beispiel 2007 bei seiner Rede: „I only get five words? Shit, that was five. Four more there. That's three. Two.“\n\nDie Lovie Awards sind ein Publikumspreis und Ableger der Webby Awards.\n\n"}
{"id": "210116", "url": "https://de.wikipedia.org/wiki?curid=210116", "title": "Links2", "text": "Links2\n\nLinks2 ist ein freier (unter der GPL stehender) textbasierter Webbrowser, der Tabellen und Frames darstellen kann.\n\nDie erste Version des Nachfolgers von Links wurde 2005 von der tschechischen Entwicklergruppe Twibright Labs veröffentlicht. Der Hauptprogrammierer von \"Links\", Mikuláš Patočka, ist am Projekt beteiligt.\n\nIm Gegensatz zum originalen \"Links\" und auch zu dessen weiterer Abspaltung ELinks, besitzt \"Links2\" einen optionalen grafischen Modus, in dem der Webbrowser auch Bilder rendern kann, wenn man die dafür benötigten Grafikbibliotheken installiert hat. Bis zur früheren Version 2.1pre28 von 2007 enthielt \"Links2\" auch eine JavaScript-Unterstützung.\n\nNeben den offiziell unterstützten Betriebssystemen existieren Portierungen externer Programmierer unter anderem für macOS, Haiku, Plan 9 und MorphOS.\n"}
{"id": "210282", "url": "https://de.wikipedia.org/wiki?curid=210282", "title": "Dropper", "text": "Dropper\n\nEin Dropper oder Viren-Dropper ist eine eigenständig ausführbare Programm-Datei, die der meist erstmaligen Freisetzung eines Computervirus dient. Computerviren sind keine eigenständigen lauffähigen Programme, sondern befallen nur parasitär anderen Programmcode. Aus diesem Grund benötigt ein Computervirus für seine erstmalige Ausführung ein spezielles Trägerprogramm, den Virusdropper. Der Dropper als solches ist ein trojanisches Pferd (\"Trojaner\"). Die Ausnahme bilden hier einige wenige Dropper, die selbst zu den Dateiviren zählen, aber zusätzlich auch einen Bootsektor-Virus droppen können. Diesen Malware-Typ nennt man auch Hybrid-Virus, da er zwei Typen von Viren vereint. Ein Hybrid-Virus kann wiederum durch ein Trojanisches Pferd als Dropper freigesetzt werden.\n\nEin On-Access-Scanner eines Antivirenprogramms kann heuristisch erkennen, dass neuer Maschinencode in eine ausführbare Datei eingeschleust werden soll, und so verhindern, dass der Virus auf dem betreffenden System abgesetzt wird.\n\nEine weitere abgeänderte Art eines Droppers, die lediglich eine Malware im temporären Speicher ablegt, nennt sich \"Injector\". Diese Version ist etwas gefährlicher, weil ein Anwender den Schadcode nicht direkt bemerken kann.\n\nDropper sind oft in Dateien aus Tauschbörsen enthalten und tarnen sich zum Beispiel als No-CD-Cracks. Da diese Programme dafür bekannt sind, bei Virenscannern Fehlalarme zu verursachen, ist die Chance einer erfolgreichen Infektion trotz Virenschutz deutlich höher. Ein Anwender der diesbezüglich bereits mehrere Fehlmeldungen des Anti-Viren-Programmes erlebt hat, würde eventuell auch echte Malware als falschen Alarm einstufen.\n"}
{"id": "210384", "url": "https://de.wikipedia.org/wiki?curid=210384", "title": "BogoMips", "text": "BogoMips\n\nBogoMips ist ein im Linux-Kernel verwendetes Maß für die CPU-Geschwindigkeit. Der Wert wird beim Booten ermittelt, um eine interne Warteschleife zu justieren.\n\nDer Name leitet sich vom englischen \"bogus\" – gefälscht, scheinbar – und dem Maß der (Millionen) Instruktionen pro Sekunde ([M]IPS) – ab. Eine oft zitierte Definition ist „Die Anzahl der Millionen Wiederholungen pro Sekunde, die ein Prozessor in der Lage ist, absolut nichts zu tun“.\n\nBogoMips ist ein Wert, der verwendet wird, um zu überprüfen, ob der Prozessor im Vergleich zu anderen Prozessoren der gleichen Bauart im Rahmen der üblichen Leistungsspezifikation liegt. BogoMips stellt also die Taktfrequenz eines Prozessors sowie den potentiell vorhandenen CPU-Cache fest. Er ist nicht geeignet für einen Leistungsvergleich zwischen verschiedenen CPUs.\n\n1993 veröffentlichte Lars Wirzenius eine Mail, in der er die Notwendigkeit der Einführung der BogoMips im Linux-Kernel auf comp.os.linux erklärte:\n\nDie BogoMips können mit der folgenden Tabelle vorausberechnet werden.\n\nDie angegebene Bewertung ist für die CPU mit der dann aktuellen und verwendeten Linux-Version. Der Index ist das Verhältnis der „BogoMips pro Taktgeschwindigkeit“ für jede CPU im Vergleich mit einer Intel 386DX-CPU und dient dem Vergleich damit.\n\n"}
{"id": "210918", "url": "https://de.wikipedia.org/wiki?curid=210918", "title": "NCR Corporation", "text": "NCR Corporation\n\nDie NCR Corporation (kurz für \"National Cash Register\", NYSE: NCR) ist ein international tätiges US-amerikanisches Unternehmen mit Hauptsitz in Duluth (Georgia), das derzeit (Stand 2010) weltweit ca. 30.200 Mitarbeiter beschäftigt. Das Unternehmen ist im Aktienindex S&P 500 gelistet. NCR zählt zu den 10 renommiertesten Technologieunternehmen weltweit.\n\nDas Portfolio der Firma umfasst Geldautomaten, Einzelhandelssysteme, Data-Warehousing-Systeme, Kassensysteme für die Gastronomie, IT-Dienstleistungen, Travel-Lösungen und wird von NCR mit dem Begriff \"Consumer Transaction Technology\" zusammengefasst. Das Unternehmen erzielte 2014 einen Umsatz von 6,59 Mrd. US-Dollar nach GAAP. Im Qualitätsmanagement setzt NCR auf die Six-Sigma-Philosophie.\n\nDie NCR Corporation ist Weltmarktführer im Finanz-SB-Bereich (Quelle: Nilson Report), Scannerkassen (Quelle: Venture Development Corp.), Data Warehousing (Quelle: Gartner), mit 100.000 Kunden einer der führenden Anbieter von Gastronomie-Kassenlösungen und umfasst folgende Unternehmenszweige:\n\nNCR hält einen globalen Marktanteil von ca. 25 % im Bereich Geldautomaten und ist die Nr. 2 nach dem Hauptwettbewerber Diebold Nixdorf. Im Bereich Retail sind IBM und Wincor Nixdorf die Hauptkonkurrenten.\n\nTeradata, ehemals zuständig für die Produkte Data Warehousing und Customer-Relationship-Management Systems, ist seit Oktober 2007 ein eigenes an der Börse notiertes Unternehmen und von NCR abgespalten.\n\nDie NCR besteht bereits seit mehr als 130 Jahren und kann als ältestes IT-Unternehmen der Welt angesehen werden.\n\nNachdem James Ritty 1879 die Registrierkasse erfunden und am 30. Januar 1883 zusammen mit John Birch ein Patent darauf erhalten hatte, gelangte die von ersterem gegründete und geleitete Firma in die Hände von Jacob H. Eckert aus Cincinnati, der sie in \"National Manufacturing Company\" umbenannte, schon 1884 aber an John Henry Patterson verkaufte. Patterson führte sie nun als National Cash Register Company (NCR) weiter, die die ersten mechanischen Registrierkassen nun in großem Umfang fabrizierte. Das Geschäft florierte und die NCR expandierte bald auch nach Europa. 1896 wurde in Deutschland die Nationale Registrierkassen GmbH (NRK) gegründet.\n\nPatterson etablierte in seinem Unternehmen moderne Vertriebsmethoden, die für die damalige Zeit ungewöhnlich waren. Alle Verkäufer mussten stets auf ein gepflegtes Äußeres achten, an regelmäßigen Schulungen teilnehmen und Pattersons Verkaufsmaximen – in Form eines kleinen gedruckten Büchleins, intern „die Bibel“ genannt – verinnerlichen. Auch nutzte Patterson bereits massiv das Prinzip der Werbung in Form von Prospekten, die er an Händler überall in den Staaten sandte, und erfand nebenbei das Flipchart.\n\nJohn H. Patterson war auch bekannt für seine rigorose „hire and fire“-Praktik. Zu den von ihm wegen Streitigkeiten entlassenen Mitarbeitern zählte Thomas J. Watson, der daraufhin zu CTR wechselte, die er jedoch bald in IBM umbenannte. Sowohl Watson als auch Patterson wurden wegen unfairer Geschäftspraktiken zu je einem Jahr Gefängnis verurteilt. Sie fochten jedoch das Urteil an und konnten sich mit Kaution freikaufen.\n\nDie Produktpalette der NCR wurde fortwährend weiterentwickelt und vergrößert. 1906 entwickelte Charles F. Kettering die erste elektrische Registrierkasse, die von einem Motor angetrieben wurde; später folgten elektronische Registrierkassen. Im Jahr 1911 hatte die NCR bereits über eine Million Registrierkassen verkauft und beschäftigte 6.000 Angestellte, der Marktanteil in den USA lag bei 95 Prozent.\n\n1953 stieg das Unternehmen in die elektronische Datenverarbeitung ein und war auch dort einer der Pioniere. 1974 brachte die NCR die ersten kommerziellen Barcodeleser auf den Markt. 1982 lief der erste NCR UNIX Super-Microcomputer vom Band. 1983 kam der NCR DM V (der in Deutschland entwickelt wurde) auf den Markt. 1991 wurde NCR von AT&T übernommen und 1994 in AT&T Global Information Solutions (GIS) umbenannt. Ende 1996 trennte sich die AT&T GIS wieder von AT&T und firmierte erneut unter NCR. 1997 zog sich die Firma aus dem PC-Geschäft zurück.\n\nDie NCR Corporation hält alleine in den USA 1.450 Patente. Zu den jüngsten Innovationen zählen \"NCR Kalpana\", eine neuartige Geldautomatenlösung, und \"NCR Orderman7\", ein speziell für die Gastronomie entwickeltes Handheld zur effizienten Bestellannahme.\n\nSeit August 2005 ist Bill Nuti CO (\"Chief Officer\", Vorstand) des Unternehmens.\n\nDie NRK, die deutsche Tochter der NCR, siedelte 1945 von Berlin in die US-amerikanische Besatzungszone um, zunächst ins fränkische Gunzenhausen, 1947/1948 schließlich nach Augsburg. So machte sie Augsburg zur „Stadt der Registrierkassen“, das Augsburger Werk war das größte seiner Art in Europa. Der Architekt des Verwaltungsgebäudes, des Casinos mit Bühne, der Gebäude „Fabrik 1“, „Fabrik 2“ und des Hochhauses war Carl Weber (1904–1987).\n\nMitte der 1950er Jahre entstanden durch NCR enge Kontakte zwischen der Stadt Augsburg und Dayton, die 1964 in einer Städtepartnerschaft zwischen den beiden Städten gipfelten. Der durch das US-amerikanische (Kasernen-) Viertel des Augsburger Stadtteils Kriegshaber verlaufende Abschnitt der heute ausgebauten B17-Westtangente trägt den Namen \"Dayton-Ring\".\n\nIn den 1960er und 1970er Jahren beschäftigte NCR allein am Standort Augsburg 5000 Mitarbeiter (nach anderen Quellen sogar 7000 Mitarbeiter). In den 1980er und 1990er Jahren wurde der Standort jedoch wieder stark reduziert. Die Gebäude Fabrik 1 und Fabrik 2 wurden 2000 abgebrochen, Halle 1 wurde 2001 abgebrochen; das Versammlungsgebäude, in dem früher Konzerte und zur Faschingszeit große Bälle stattfanden, und das Schulungsgebäude wurden 2002 abgebrochen. Für kurze Zeit verschwand der NCR-Schriftzug und überall war der Schriftzug AT&T (am Hochhaus sogar beleuchtet) zu finden. 2007 beschäftigte die NCR GmbH noch ca. 350 Mitarbeiter in Augsburg. Die verbliebenen Grundstücke und die beiden letzten noch nicht abgerissenen Gebäude wurden im Herbst 2004 verkauft. Das zehngeschossige Hochhaus wurde bis 2015 zurückgemietet und trug bis dahin wieder den beleuchteten NCR Schriftzug.\n\nIm Herbst 2015 zog NCR aus, verließ Kriegshaber und zog in frei gewordene Räume des insolventen Weltbild-Verlags in Augsburg-Lechhausen.\n\nIm Januar 2016 wurde mit dem Abtragen des Hochhauses begonnen (Beginn der eigentlichen stockwerkweisen Abrissarbeiten, nach der zuvor erfolgten Entkernung). Ende September 2016 war der Abbruch beendet.\n\n"}
{"id": "212783", "url": "https://de.wikipedia.org/wiki?curid=212783", "title": "Hisax", "text": "Hisax\n\nHisax ist eine Treibersoftware für passive ISDN-Karten für das Betriebssystem Linux.\n\nDer Hisax-Treiber wurde ab 1995 von Karsten Keil entwickelt und ist mittlerweile Teil des Linux-Kernels.\n\nMit der Einführung des 2.6er Kernels wird Hisax praktisch nicht mehr weiterentwickelt. Das Nachfolgeprojekt nennt sich mISDN.\n\n"}
{"id": "212902", "url": "https://de.wikipedia.org/wiki?curid=212902", "title": "Projekt Xanadu", "text": "Projekt Xanadu\n\nXanadu ist ein 1960 gegründetes Hypertext-Projekt von Ted Nelson; durch das nach dem legendären Ort Xanadu benannte Projekt sollte das Docuverse, eine universale Bibliothek mit zahllosen miteinander vernetzten Dokumenten, entstehen.\n\nDas Hypertext-Konzept von \"Xanadu\" ist vergleichsweise komplex; beispielsweise ist ein Transklusions-Mechanismus vorgesehen, mit dem Teile aus anderen Objekten nahtlos in ein Dokument eingebunden werden können. Darüber hinaus war in \"Xanadu\" auch immer ein Abrechnungsmodell vorgesehen, ähnlich den neueren Ansätzen des Micropayments.\n\nWie das World Wide Web war Xanadu als dezentrales Speichersystem für Dokumente gedacht. Jedes Dokument in Nelsons Hypertext-Raum sollte eine absolut eindeutige Adresse (unabhängig vom Speicherort) besitzen. Innerhalb des Dokuments sollten selbst einzelne Zeichen direkt von anderswo adressierbar sein. Dokumente stellte sich Nelson als unlöschbare Einträge in einer globalen Datenbank vor. Man konnte zwar, so die Idee, eine neue Version veröffentlichen, doch die alte Version des gleichen Dokuments blieb verfügbar, und Unterschiede zwischen zwei Versionen ließen sich auf einfache Weise sichtbar machen. Zusammengehörende Dokumente sollten in parallelen Fenstern, so genannten \"transpointing Windows\", samt den Verbindungen dazwischen angezeigt werden.\n\nVerweise sollten bidirektional sein; wenn man eine Seite in Xanadu betrachtete, sollte man also auch sehen, welche anderen Seiten auf diese Seite verwiesen. Anstelle des im Web üblichen „Copy & Paste“, des einfachen Kopierens von Inhalten, sollten die Adressen von Inhalten an der Stelle, an der man sie benutzt, eingefügt werden. Wenn man also z. B. ein Buch zitiert, würde man einfach die Adresse (also die global eindeutige Nummer des Buches sowie die Zahl der zu zitierenden Zeichen) an der entsprechenden Stelle einfügen, nicht den Zitattext selbst (sog. Transklusion). Der Client (das Xanadu-Äquivalent zum Webbrowser) würde die entsprechenden Daten dann an der richtigen Stelle einfügen.\n\nZitate bleiben automatisch aktuell, wenn dies gewünscht ist, ihre Echtheit kann gewährleistet werden, man kann sofort den Kontext eines Zitats anfordern, und Urheber können ggf. ohne großen Aufwand im Hintergrund vergütet werden. Nelson suchte bereits nach Lösungen für das Problem der Vergütung im digitalen Zeitalter, als kaum jemand sich überhaupt über dessen Existenz im Klaren war.\n\nAnstatt mühsam jede Rechte-Verletzung zu verfolgen, sollten Dokumente in Xanadu so günstig sein, dass man ihre Bezahlung gar nicht beachtete. Bruchteile von Cents sollten für die Verwertung eines Dokuments innerhalb eines anderen fällig werden, und aufgrund des Systems der direkten Adressierung von Inhalten anstelle ihres Kopierens würden solche Verwertungsvorgänge auch erfassbar bleiben, sofern man das System nicht mit Absicht umging. „Ich würde gerne in einer Welt leben, in der es kein Copyright gibt, aber so liegen die Dinge nun einmal nicht“, meint Nelson – und nennt sein alternatives Modell \"Transcopyright\". Essenziell dafür ist es, Kleinstbeträge zwischen Nutzern wirtschaftlich übertragen zu können.\n\nXanadu scheiterte an seiner Komplexität. Das System wurde nie fertiggestellt; bis heute existieren nur Prototypen. Nelson hatte an der Harvard-Universität Philosophie studiert und war technisch nicht versiert genug, das System im Alleingang umzusetzen oder andere bei der Implementierung zu unterstützen.\n\n1988 übernahm die Firma Autodesk 80 % von XOC, wo Ted Nelson bis 1992 an Xanadu arbeitete. Danach wurde das Projekt bis 1998 an der Keio University in Japan fortgeführt. Als Grundlage entwickelte Ted Nelson dort unter anderem die ZigZag-Datenstruktur. 1999 wurde beschlossen, den Quellcode unter dem Namen Udanax freizugeben. Die in einem Smalltalk-Dialekt programmierte Software wurde von David Jones im Abora-Projekt teilweise nach Java portiert. Die aktuelle Entwicklungsversion von Xanadu (2009) wird von Andrew David Pam verwaltet, der als Student an der Keio University zum Projekt stieß.\n\nNelsons konzeptuelle Ideen für \"Xanadu\" beeinflussten gleichermaßen Tim Berners-Lee bei der Entwicklung des World Wide Web wie auch Ward Cunningham bei seinem Wiki-Konzept. Alle heute verbreiteten Umsetzungen des Hypertext-Konzepts sind funktionale Teilmengen von Nelsons \"Xanadu\".\n\n\n\n"}
{"id": "213718", "url": "https://de.wikipedia.org/wiki?curid=213718", "title": "Pages (Software)", "text": "Pages (Software)\n\nPages ist eine Textverarbeitungs-Software des Unternehmens Apple für die Betriebssysteme macOS und iOS. Es ist Teil des Büropakets iWork und wurde erstmals am 11. Januar 2005 im Rahmen der Macworld als \"Pages ’05\" vorgestellt.\n\nEs hebt sich von übrigen Textverarbeitungsanwendungen ab, da es ein Bedienungskonzept ähnlich dem von Layout-Anwendungen sowie deren Funktionen aufweist. So können auf einfache Weise Bilder und Text rahmenbasiert platziert werden. Dies ist zwar auch in einigen anderen Textverarbeitungsanwendungen möglich, etwa MS Word, allerdings war es dort stets eine nebensächliche Funktion und vergleichsweise mühsam umsetzbar.\n\nPages wird mit zahlreichen grafisch gestalteten Vorlagen geliefert, die auch erweitert werden können.\n\nPages wurde von derselben Abteilung entwickelt, die zuvor Keynote entwickelt hatte, weshalb sich diese beiden Programme ähneln. Zentrale Funktionen, an die das Programm angepasst wurde, sind eine kontextsensitive Formatierungsleiste, Änderungen verschiedener Autoren zu protokollieren, sowie ein besseres Handling bei der Verkettung von Fließtext in Textfeldern.\n\nAm 6. Januar 2009 erschien mit \"Pages ’09\" die vierte Version des Programms, in der abermals mehrere Verbesserungen und Ergänzungen vorgenommen wurden.\n\nSeit dem Erscheinen des iPad 2010 gibt es auch eine Version für iOS, die ab iOS 5 auch auf dem iPhone und iPod touch lauffähig ist.\n\nWährend der WWDC 2013 stellte Apple die Beta-Versionen von „Pages, Numbers und Keynote (iWork) for iCloud“ vor. Nun kann man Pages, Numbers und Keynote kompatible Dokumente direkt im Web über eine etwas veränderte Oberfläche bearbeiten.\n\nAuf einer Produktpräsentation am 22. Oktober 2013 stellte Apple neue Versionen von iWork für OS X und iOS vor, die primär mit neuem, modernerem Design, aber auch einigen neuen Funktionen versehen wurden, wiederum wurden viele alte Funktionen entfernt. Wer ab sofort einen neuen Mac oder ein neues iOS-Gerät kauft, kann diese Programme kostenlos herunterladen.\n\nStandardmäßig verwendet das Programm ein Dateiformat mit der Endung \".pages\", zu dem keine andere Textverarbeitung kompatibel ist.\n\nEin Pages-Dokument ist in der Regel in eine ZIP-Datei eingekapselt. Entpackt handelt es sich um ein Paket, also ein Ordner, der in OS X vom Finder wie eine einzelne Datei behandelt wird. Darin enthalten sind Vorschaubilder (JPEG) wie auch die platzierten Originalbilddateien, XML-plist-Dateien mit Metadaten und Binärdateien mit den Dokumentinhalten. Bis einschließlich Version ’09 waren die Dokumentinhalte in einer zentralen strukturgebenden XML-Datei gespeichert. Auch eine Vorschau im PDF-Format war enthalten.\n\nDas Pages-Format ändert sich von Programmversion zu -version. Pages kann daher ins Format der Vorgängerversion exportieren. Der Import älterer Pages-Formate ist immer möglich.\n\nPages kann PDF-Dokumente exportieren (PDF 1.3, mit Transparenz), bietet dafür jedoch lediglich drei unterschiedliche Qualitätsstufen (Gut, Besser, Optimal). In Pages-Dokumente können PDF-Dokumente platziert, jedoch nicht vollständig importiert werden.\n\nPages kann Word-Dateien (<samp>.doc</samp> und <samp>.docx</samp>) importieren und exportieren. Allerdings funktionierte dies bis zu \"Pages ’07\" nur bei einfach gehaltenen Texten; an komplexeren Word-Dokumenten mit genau positionierten Grafiken oder Tabellen scheiterte das Programm. Mit \"Pages ’08\" wurde der Im- und Export von Word-Dokumenten verbessert. So werden Textrahmen richtig positioniert und bei Problemen durch genaue Meldungen beschrieben, welche Probleme es bei Im- und Export von Word-Dateien gibt.\n\nPages exportiert ins ePub-Format, etwa für iBooks, sowie in unformatierten „reinen“ Text (, UTF-8, LF).\n\nDas OpenDocument-Format wird nicht unterstützt.\n\n\n"}
{"id": "213893", "url": "https://de.wikipedia.org/wiki?curid=213893", "title": "YaBasic", "text": "YaBasic\n\nYaBasic ist ein quelloffener BASIC-Interpreter für Linux/Unix und für Windows. Er zeichnet sich durch seine Kompaktheit (gepackte Größe etwa 140 kB) und eine flexible Syntax aus: Für die Farbwahl sind zum Beispiel sowohl der Befehl \"color\" als auch \"colour\" möglich, und auch für \"if ... then ... else ... endif\"-Konstrukte gibt es eine Reihe verschiedener Syntax-Varianten usw.\n\nYaBasic wurde ab 1995 von \"Marc-Oliver Ihm\" entwickelt. Die Version 2.763 wurde von ihm im Jahr 2005 als „final“ bezeichnet, da er sich nunmehr anderen Softwareprojekten widmen wollte. Im Jahr 2008 wurde die Weiterentwicklung von YaBasic durch \"Pedro Sá\" und \"Thomas Larsen\" aufgenommen. Sie entwickeln \"Patches\" für die Version 2.763 und arbeiten derzeit an YaBasic 3. Im Dezember 2010 wurde die erste Beta-Version von YaBasic 3 freigegeben.\nSeitdem ist es um dieses Projekt still geworden. Eine offizielle Version 3 ist bisher (Ende 2012) nicht erschienen. YaBasic v2.78.1 wurde am 13. August 2017 veröffentlicht. Die Version 2.78.0-1 ist in verschiedenen Linux-Distributionen integriert, wie z. B. Ubuntu 17.04 oder Debian 9.\n\nYaBasic zeichnet sich gegenüber anderen BASIC-Interpretern durch folgende Eigenschaften aus:\n\n\nDas allgegenwärtige Hallo-Welt-Programm benötigt in YaBasic nur eine Zeile:\n\nDer folgende Code gibt zwei rote Kreise aus:\n\nZeichenketten können vom Interpreter während der Laufzeit zu ausführbarem Code übersetzt werden, was einem Programm erlaubt, sich selbst zu modifizieren.\n\nDer Codeabschnitt\n\nerzeugt zum Beispiel eine neue Prozedur,\n\nwährend das Programm läuft. Von nun an kann die Prozedur einfach durch\n\noder über das geringfügig komplexere Kommando\n\naufgerufen werden.\n\nDie Version 3 befindet sich seit Dezember 2009 im Beta-Stadium.\n\nYaBasic wird unter dem Namen \"yab\" auf BeOS/ZETA portiert.\n\nSony liefert YaBasic für die PlayStation 2 kostenlos mit der Demo-DVD der PAL-Version aus.\n\nYaBasic steht unter der GPL und der Artistic License.\n\n"}
{"id": "214037", "url": "https://de.wikipedia.org/wiki?curid=214037", "title": "TRS-80", "text": "TRS-80\n\nTRS-80 ist eine Marke der Tandy Corporation. Sie diente ursprünglich als Name des ersten Microcomputers des Unternehmens, dem Tandy TRS-80 Model 1. Dabei steht TRS für Tandy RadioShack, die Elektronikladenkette der Firma sowie 80 für den verwendeten Z80-Prozessor. Die Marke wurde für alle Computerprodukte der Firma von 1977 bis 1984 verwendet und der individuellen Bezeichnung vorangestellt. Um ein höherwertiges Image zu erreichen, und dem Spottnamen Trash-80 zu entkommen, wurden ab dem Erscheinen des ersten PC-kompatiblen Rechners, dem Tandy PC2000, neue Produkte nur noch als Tandy beworben.\n\nFolgende Rechner wurden mit der Marke TRS-80 beworben:\n\n\nNeben den Rechnern wurde auch alle Peripherie und Software zwischen 1977 und 1984 mit der TRS-80-Marke verbunden.\n"}
{"id": "214497", "url": "https://de.wikipedia.org/wiki?curid=214497", "title": "Textmodus", "text": "Textmodus\n\nAls Textmodus bezeichnet man die bis zum Erscheinen der ersten Videospiele und GUI-Betriebssysteme (Mac OS, TOS, AmigaOS) übliche Betriebsart von Grafik-Hardware. Hierbei werden – im Gegensatz zum Grafikmodus – von der Software nicht einzelne Bildpunkte mit ihrer jeweiligen Farbe verwaltet, sondern einzelne Schriftzeichen, wie Buchstaben und Ziffern. Diese Schriftzeichen werden dann von der Grafik-Hardware des Rechners in Bildpunkte umgewandelt. Dies verhindert zwar, dass Grafiken hochauflösend dargestellt werden können, aber es verringert auch den Bedarf an Grafikspeicher enorm, reduziert den Programmieraufwand deutlich, und beschleunigt Programme, da pro Zeichen meist nur ein oder zwei Byte im Video-RAM benötigt werden.\n\nÜbliche Auflösungen reichen von 40×25 Zeichen, was für TV-Bildschirme die größte praktisch nutzbare Auflösung darstellt, über 80×25, welche heute noch der Modus ist, in dem sich PC-Grafikkarten nach dem Booten befinden, bis hin zu 132×50, welche von textbasierten Tabellenkalkulationen und ähnlichen Programmen unter DOS benutzt wurden, die viele Informationen gleichzeitig darstellen müssen. Weitere Varianten waren 80×24 (viele Großrechner-Terminals), 64×16 (Tandy TRS-80 Model 1) oder 22×23 (Commodore VC20, also mehr Zeilen als Spalten!).\n\nDie einzelnen Zeichen besitzen in der Regel eine feste Größe von 8×8 bis 9×16 Pixeln (in den Anfangszeiten mit viel höheren Speicherpreisen auch 5×7), wobei auf PC-Grafikkarten nur Zeichen mit 8 Pixeln Breite unterstützt werden. Diese werden in einem eigenen Zeichenspeicher abgelegt, siehe auch bei Zeichengenerator. Je nach Computer- und Grafikkartenmodell kann dieser Zeichengenerator ein unveränderliches ROM oder ein vom Benutzer veränderliches RAM sein; nur im letzteren Fall kann der Benutzer eigene Zeichenformen definieren und verwenden.\n\nDer Zeichenspeicher bei PC-Grafikkarten speichert die Zeichen als Bitmap, wobei jedes Zeichen so viele Bytes benötigt, wie es Pixel hoch ist. Im ROM der EGA-Karten befinden sich zwei Zeichensätze mit der Zeichengröße 8×8 und 8×14 Pixel, bei VGA-Karten kommt noch ein 8×16-Zeichensatz hinzu. Benutzerdefinierte Zeichensätze, welche ab EGA möglich sind, können 1 bis 32 Pixelzeilen hoch sein.\n\nDie farblich hervorgehobenen Modi sind die direkt vom BIOS unterstützten Textmodi; die übrigen lassen sich nur durch direktes Programmieren der Grafikkarte erzielen. Die horizontale Grafikauflösung im Textmodus der VGA-Karte beträgt standardmäßig 720 Pixel. Damit ist jedes Zeichen 9 Pixel breit, was die Lesbarkeit der Textdarstellung verbessert, da sich die Abstände zwischen den Buchstaben etwas vergrößern. Da im Zeichenspeicher aber nur 8 Pixel pro Zeichen gespeichert sind, wird die Farbe des 9. Pixels gesondert ermittelt: Für die meisten Zeichen ist dieses Pixel in der Hintergrundfarbe, außer für die Zeichen mit den Codewerten C0 bis DF. Bei diesen Zeichen wird das 8. Pixel jedes Zeichens wiederholt. Damit wird ein nahtloser Übergang bei Grafik- und Rahmenzeichen erreicht. Die ROM-Zeichensätze sind in der Regel in der Codepage 437 kodiert, die die Grafik- und Rahmenzeichen, welche solche horizontalen Verbindungen mit ihren rechts benachbarten Zeichen eingehen können, in diesem Bereich unterbringt. Sowohl die 9 Pixel breiten Zeichen, als auch die Sonderbehandlung der Zeichen C0 bis DF lassen sich allerdings über direkte Programmierung der Grafikkartenregister deaktivieren.\n\nIm Video-RAM wird für jedes Zeichen nur die Zeichennummer gespeichert. Auf PC-Systemen ist diese stets 8 Bit groß, es lassen sich so 256 verschiedene Zeichen ansprechen; außerdem wird für jedes Zeichen ein so genanntes \"Attribut-Byte\" gespeichert, welches die Farbe des Zeichens oder besondere Zeichenattribute (Fettdruck, Unterstreichung, Blinkend usw.) kodiert.\n\nFettdruck wird meist durch eine hellere Farbe simuliert. Somit lassen sich auf Farbgrafikkarten 16 verschiedene Vordergrundfarben darstellen. Die Bedeutung des Bit 7 lässt sich umprogrammieren, so dass es entweder blinkende Zeichendarstellung oder 8 weitere (helle) Hintergrundfarben erlaubt. Ab EGA lassen sich jedoch die 16 Farben umprogrammieren, so dass eine freiere Farbwahl möglich ist.\n\nEine besondere Eigenschaft der EGA/VGA-Karten ist es, zwei Zeichensätze (und somit bis zu 512 verschiedene Zeichen) gleichzeitig anzeigen zu können. Das Bit 3 des Attributbytes bestimmt, aus welchem Zeichensatz das Zeichen entnommen werden soll. Standardmäßig sind beide Zeichensätze identisch. Die Linux-Console beherrscht dieses Merkmal, sobald eine Bildschirmschrift mit mehr als 256 Zeichen geladen wird. Hierbei werden die Farben 8 bis 15 so umprogrammiert, dass sie identisch mit den Farben 0 bis 7 sind, so dass die Zeichen aus dem zweiten Zeichensatz nicht heller dargestellt werden. Es steht somit kein „Fettdruck“ mehr zur Verfügung.\n\nDurch die Verwendung spezieller „Blockzeichen“ lassen sich niedrig aufgelöste „Klötzchen“-Grafiken im Textmodus darstellen. Werden zudem geschickt pro Zeichen die Vorder- und Hintergrundfarbe gewählt, so können einfache mehrfarbige Bilder erzeugt werden. Im Bildschirmtext wurde diese Funktion sehr häufig verwendet, im Videotext ist dies bis heute üblich. Einige Systeme erlauben auch die Neudefinition der Zeichenformen durch den Anwender; in Verbindung mit hardwareunterstütztem Soft-Scrolling und sogenannten Sprites für Vordergrundelemente erlaubt diese Technik beispielsweise die Darstellung von schnell bewegten Hintergründen für Computerspiele im Textmodus, der dann kaum noch als solcher zu erkennen ist. Dieser Ansatz wurde beispielsweise in vielen Spielen für die 8-Bit-Rechner der Firma Commodore genutzt.\n\nComputersysteme für den geschäftlichen Einsatz verfügten bis in die frühen 1980er Jahre hinein meist nur über einen Textmodus. Erst mit dem Aufkommen der Videospiele und Heimcomputer wurde der Grafikmodus üblich; einige Heimcomputer, wie zum Beispiel der Schneider/Amstrad CPC und der Commodore Amiga, hatten gar keinen Textmodus mehr. Die Apple-Macintosh-Modelle der 68000er und der PowerPC-Reihe hatten keinen Textmodus, die auf BIOS basierenden oder es emulierende Rechner haben den Textmodus bis heute, er wird aber von den meisten Anwendern kaum noch genutzt, seit Windows 95 sich ab 1995 durchsetzte. Nur beim Start des Rechners ist er weiterhin für kurze Zeit aktiv. Wenn man ein Eingabeaufforderungs-Fenster maximiert (Alt+Eingabetaste, ab Windows Vista nicht mehr möglich), wird der Rechner ebenfalls in den Textmodus gesetzt. Linux bzw. Unix-ähnliche Betriebssysteme kennen in ihrer \"Console\" immer den Textmodus, der Grafikmodus ist optional.\n\nDa die Unterstützung für den Textmodus auf modernen Grafikkarten selten über die Auflösungen und Bildwiederholraten der VGA-Karten hinausgehen (einige Super-VGA-Karten beherrschen Text-Modi mit 132 Text-Spalten, was einer Grafikauflösung von 1056 Pixeln entspricht), benutzen einige Betriebssysteme für ihre Textkonsole inzwischen auch einen Grafikmodus und emulieren das Verhalten des Textmodus komplett in Software. Da die dabei zu verarbeitende Datenmenge deutlich größer ist, ist eine solche emulierte Textkonsole deutlich langsamer (z. B. beim Scrollen) als ein Hardware-Textmodus (siehe auch: Framebuffer-Console von Linux).\n\nÄltere Fernsehgeräte und Videorekorder verfügen für On-Screen-Menüs und Videotext häufig über Graphikchips, die nur einen Textmodus besitzen. Diese enthalten dann spezielle Zeichensätze (oft maskenprogrammiert), die geschickt dazu benutzt werden, Symbole, oder sogar Licht- und Schatteneffekte zu erzeugen.\n\n\n"}
{"id": "214715", "url": "https://de.wikipedia.org/wiki?curid=214715", "title": "YafaRay", "text": "YafaRay\n\nYafaRay (Yet Another Free Raytracer, früher \"YafRay\") ist ein freier Raytracer, mit dem sich fotorealistische Bilder und Animationen erstellen lassen. YafaRay wird unter der GNU LGPL veröffentlicht.\n\nDas Projekt Yafaray wurde 2001 unter dem damaligen Namen Yafray von einem Entwicklerteam um Alejandro Conty Estévez  gegründet. Im Juli 2002 erschien ein erstes Release. Im August 2004 erschien Yafray erstmals als mitgeliefertes Plugin für Blender 2.34.\n\nEtwa um diese Zeit wurde klar, dass umfangreiche Umstrukturierungen der Codebasis notwendig waren, um neue Features hinzufügen zu können. Die letzte Version mit dem Namen Yafray erschien im Sommer 2006 als Yafray 0.0.9.\n\nAb Dezember 2005 begann eine vollständige Neufassung von Mathias Wein. Um dies deutlich zu machen wurde der Name geändert zu Yafaray – ohne das jemals begründet wurde, wofür der hinzugefügte Buchstabe a stehen würde. Die erste stabile Version des neuen Raytracers, YafaRay 0.1.0, erschien im Oktober 2008.\n\nDie Weiterentwicklung wird kontinuierlich unter einem mittlerweile vollständig veränderten Team von Kernentwicklern betrieben.\n\n\nDurch die Unterstützung von Multi-Prozessor-Systemen, sowie Mosix/openMosix kann YafaRay ohne Probleme in einem entsprechend konfigurierten Rechnerverbund betrieben werden, mit dem zu erwartenden Geschwindigkeitsvorteil. Allerdings haben die Entwickler bewusst auf prozessorspezifische Optimierungen verzichtet, um die Portierung von YafaRay auf andere Architekturen zu erleichtern.\n\nYafaRay verwendet als Dateiformat XML und ist in das 3D-Programm Blender (bis zu Version 2.48a) integriert, kann aber auch als Kommandozeilen-Programm verwendet werden. Mit Blender erstellte Szenen können wahlweise mit dem internen Raytracer oder mit YafaRay gerendert werden. Dafür legt Blender eine temporäre XML-Datei im YafaRay-Format an und lässt diese dann von YafaRay rendern.\n\nEs ist aber auch möglich Blender-Szenen in das von YafaRay unterstützte XML-Format zu konvertieren – entweder mit dem integrierten Exporter oder mit Python-Skripten, wie z. B. YaBlE (Yet Another Blender Exporter). Meistens ist es jedoch einfacher, die temporäre Datei zu verwenden.\n\n"}
{"id": "215272", "url": "https://de.wikipedia.org/wiki?curid=215272", "title": "MiKTeX", "text": "MiKTeX\n\nMiKTeX ist eine TeX-Distribution für Windows. Die ergänzenden \"MiKTeX Tools\" wurden ebenso auf GNU/Linux portiert.\n\nMit einem Installationsprogramm werden die benötigten TeX-Pakete (beispielsweise für LaTeX oder ConTeXt) aus dem Internet (CTAN-Server) oder von einem \"local package repository\" (z. B. der MiKTeX-CD) geladen und danach auf dem Rechner installiert. Ein Aktualisieren der Pakete ist möglich, außerdem werden benötigte, noch nicht vorhandene Pakete bei Bedarf nachgeladen und installiert.\n\nSeit einiger Zeit wird ebenfalls eine 64-Bit-Version angeboten.\n\nBei einer kompletten Installation der Version 2.9 müssen ungefähr 1,3 GB aus dem Internet geladen und 2,4 GB auf der Festplatte installiert werden. Neben der normalen Installationsroutine gibt es auch ein „Basic MiKTeX“-Installationsprogramm, mit welchem es möglich ist, ein TeX-Minimalsystem zu installieren. In der Datei zur Installation sind die notwendigen Dateien schon enthalten, die sonst aus dem Internet geladen werden. Die Installationsdatei ist etwa 150 MB groß.\n\nDie meisten Programme der Distribution liegen als Kommandozeilenversionen vor, daher ist zum Schreiben des LaTeX-Quelltextes ein Texteditor notwendig. Es existieren mehrere Editoren, die für die Verwendung mit MiKTeX angepasst sind und ein IDE-artiges Frontend bieten. Dazu zählen das für MiKTeX entworfene und häufig parallel genutzte TeXnicCenter (GPL), der plattformunabhängige Editor Texmaker (GPL), der Public-Domain-Editor WinShell, das Shareware-Programm WinEdt, die kostenlose Entwicklungsumgebung LEd (LaTeX Editor), der Editor Kile (GPL) des KDE-Projektes sowie das LaTeX-Frontend LyX. In neueren Distributionen ist der Editor TeXworks, welcher dem für Mac entwickelten TeXShop nachempfunden ist, integriert. Es gibt jedoch auch für den Editor emacs bzw. XEmacs eine spezielle LaTeX-Umgebung, sie heißt AUCTeX und bietet zusammen mit dem Paket RefTeX eine komfortable Arbeitsumgebung, insbesondere für größere Arbeiten (Diplomarbeiten, Bücher), die über mehrere Filialdokumente verteilt sind; unter Unix bietet XEmacs zusammen mit X-symbol auch eine WYSIWYG-ähnliche Ansicht. Des Weiteren existiert ein Plug-in namens TeXlipse für die Entwicklungsumgebung Eclipse, so dass auch mit diesem Programm das Erstellen von LaTeX-Dokumenten möglich ist.\n\n\n"}
{"id": "216501", "url": "https://de.wikipedia.org/wiki?curid=216501", "title": "Utah teapot", "text": "Utah teapot\n\nDie so genannte Utah-Teekanne (englisch: \"Utah teapot\") gehört zu den ältesten und bekanntesten 3D-Modellen der Computergrafik. Es handelt sich um ein einfaches Oberflächenmodell einer Teekanne, bei der der Innenhohlraum nicht einsehbar ist. Martin Newell entwickelte sie 1975 im Rahmen seiner computergrafischen Forschungsarbeit an der Universität von Utah.\n\nNewell benötigte ein einfaches mathematisches Modell eines Gebrauchsgegenstandes für seine Arbeit, und die Melitta-Teekanne seiner Frau schien geeignet: Die Form hat einige für die damaligen Zwecke notwendigen Eigenschaften; sie ist rund, hat Sattelpunkte und konkave Elemente (besonders das Loch im Griff) und sieht auch ohne aufwändige Oberflächentextur recht ansprechend aus.\n\nNachdem Newell eine Beschreibung der Teekanne als Bézier-Fläche entwickelt hatte, übernahmen andere Forschungsgruppen das Modell für ihre Arbeit, so dass das Objekt zu einem Referenzmodell für Computergrafik wurde.\n\nDas Original schenkte Newell dem Computer History Museum in Kalifornien, wo es in der Dauerausstellung platziert ist. Hergestellt wurde die Teekanne in der Friesland Porzellanfabrik in Rahling, Niedersachsen, die von 1954 bis 1991 unter der Marke Melitta fertigte. Die Teekanne wird unverändert auch heute noch produziert.\n\nIn Computerzeitschriften wurden über die Jahre regelmäßig Varianten der Teekanne präsentiert und bekannte 3D-Softwareprodukte enthalten heute meist Beispielbilder oder -szenen, die die Teekanne enthalten. Die OpenGL-Grafikbibliothek GLUT enthält sogar eine Funktion namens \"glutSolidTeapot()\" und das 3D-Animations-Programm \"3d Studio Max\" enthält bis heute die Teekanne als Grundkonstruktionsobjekt in einer Ebene mit dem Zylinder und dem Quader. \n\nInzwischen hat sich die Teekanne zu einer Art Running Gag in der Computergrafikszene entwickelt und das Modell wurde in den ersten computeranimierten Kurzfilmen und später auch in großen Kinofilmen „versteckt“. Die Teekanne ist in Die Monster AG, Toy Story und in der Disney-Produktion \"Die Schöne und das Biest\" zu entdecken. Auch im Computerspiel \"Serious Sam: The First Encounter\" ist sie im Rahmen einer Benchmark-Spielstufe zu finden. Außerdem findet sie sich im Windows-Bildschirmschoner (nur Versionen Windows 95/98/NT) \"3D-Rohre,\" wo sie zufallsgesteuert (sehr selten) anstatt einer normalen Eckverbindung der Rohre dargestellt wird. Auch der Open-GL-Bildschirmschoner \"Pipes\" für X-Window-Systeme baut die Teekanne gelegentlich in unterschiedlichen Lagen in die Rohrwege ein.\n\nDie Firma Pixar brachte 2014 zu Promotionszwecken für die Software \"RenderMan\" den \"Walking Teapot\" als Kurzfilm und Spielzeug heraus. Die Fakultät für Computer Graphics an der Universität von Utah hat nicht nur die Kanne als Logo gewählt, sondern richtet einen jährlichen \"Utah Teapot Rendering-\"Wettbewerb aus.\n\nWeitere bekannte Testmodelle der 3D-Computergrafik sind die \"Standard Procedural Databases\" von Eric Haines, die oft zur Performancemessung von Raytracing-Beschleunigungstechniken verwendet werden, die \"Cornell Box\", sowie die Modelle des \"Stanford 3D Scanning Repository\".\nDie kostenlose 3D-Software Blender verwendet ein Low-Poly-Modell eines Affenkopfs, Suzanne genannt.\n\n\n\nUtah-Teekanne:\n\n\nWeitere Standardmodelle:\n"}
{"id": "216865", "url": "https://de.wikipedia.org/wiki?curid=216865", "title": "Computerphysik", "text": "Computerphysik\n\nComputerphysik, auch Computational Physics (CP) oder Computergestützte Physik, ist ein Teilgebiet der Physik, das sich mit der Computersimulation physikalischer Prozesse befasst. Es wird bisweilen auch Physikinformatik genannt.\n\nAls Grundlage dienen die Verfahren der numerischen Mathematik. Die Computerphysik befasst sich mit Methoden, welche die Ausgangsgleichungen, die ein physikalisches System beschreiben, numerisch oder algebraisch mit dem Computer lösen oder auch mit der Simulation von Regelsystemen, was die Aufstellung von Gleichungen erübrigt. Aufgrund vergleichbarer Verfahren existiert eine enge Beziehung zur Computerchemie, wodurch sie sich sehr stark gegenseitig beeinflussen.\n\nDie computergestützte Physik untersucht physikalische Probleme, die sich in der Regel zwar mit Gleichungen beschreiben lassen, deren Lösung sich aber nicht direkt in einer geschlossenen Formel berechnen lassen. Solche geschlossenen Lösungen existieren nur für sehr wenige idealisierte Systeme (z. B. Keplerproblem, Wasserstoffatom oder zweidimensionales Ising-Modell). \n\nGrundlage jeder Simulation ist ein Modell, das die Wirklichkeit im Rahmen gewisser Näherungen beschreibt. Der Computer dient zur Realisierung des modellierten Systems und zur Messung physikalischer Größen sowie zur Bestimmung der Auswirkungen der Modellparameter. Computergestützte Physik umfasst ggf. auch die Anpassung der Soft- und Hardware an das zu lösende Problem.\n\nDas Spektrum der benötigten Rechenressourcen reicht von einigen Millisekunden auf einfachen PCs bis zu monatelangen Rechnungen auf Großrechnern und Supercomputern.\n\nComputergestützte Physik wird inzwischen zur Forschung in nahezu allen Teilgebieten der Physik eingesetzt:\n\nViele Computersimulationen physikalischer Systeme lassen sich auf die Lösung der folgenden mathematischen Probleme zurückführen:\n\nZu den gängigsten Methoden der computergestützten Physik zählen:\n\n\n\n"}
{"id": "217682", "url": "https://de.wikipedia.org/wiki?curid=217682", "title": "Apple CP/M", "text": "Apple CP/M\n\nApple CP/M war die CP/M-Version für den Apple II. Um sie nutzen zu können, muss der Rechner mit einer Z80-Steckkarte ausgerüstet sein.\nDer Apple II war dank Visicalc nach kurzer Zeit zum Verkaufsschlager geworden; nachdem sich viele das Gerät nur wegen dieser weltweit ersten und zunächst einzigen Tabellenkalkulations-Software gekauft hatten, die anfänglich nur für den Apple erhältlich war, folgte aber schnell die Ernüchterung: Man konnte zwar dank des eingebauten BASIC-Interpreters recht schnell kleinere eigene Programme erstellen und mit Visicalc auch Kalkulationen ausführen; eine (wirklich) gute Textverarbeitung oder eine Datenbank suchte man jedoch vergeblich. Die gab es zwar, in Form von WordStar und dBASE, aber nur für das damalige Standardbetriebssystem CP/M, und das (wie auch die Programme dafür) lief nicht auf dem Apple II, da dieser einen 6502-Prozessor enthielt, CP/M und seine Programme damals aber nur auf 8080-Prozessoren oder dem aufwärtskompatiblen Z80 liefen.\n\nDa der Apple II mit seinen damaligen gut 50 % Marktanteil ein enormes Potential bot, arbeiteten viele an Erweiterungskarten mit einem solchen Prozessor darauf, mit denen man CP/M und damit eine – für damalige Verhältnisse – gewaltige Anzahl von Programmen auf dem Apple laufen lassen konnte. Die Firma Microsoft (damals noch „Micro-Soft“ geschrieben) gewann das Rennen mit ihrer „Softcard“ genannten Zusatzkarte, die einen Z80-Prozessor und die nötige Glue Logic enthielt, um diesen in das andersartige 6502-Bussystem des Apple einzubinden. Die Karte wurde mit einer von Digital Research lizenzierten Version von CP/M geliefert. Die Hardware der Karte hatte Microsoft dabei von einer Drittfirma entwickeln lassen, ihre eigene Leistung bestand somit vor allem in der Organisation des Projektes und im Marketing.\n\nDas Ziel von Microsoft mit der Softcard war eigentlich eine Erschließung eines zusätzlichen Kundenkreises für ihre eigenen CP/M-basierten Interpreter und Compiler für verschiedene Programmiersprachen, aber die meisten Käufer erwarben die Karte nicht zum Programmieren, sondern vor allem, um diverse fertige Bürosoftware anderer Hersteller nutzen zu können (z. B. dBASE der Firma Ashton-Tate). Schon bald erschienen auch diverse Nachbauten der Softcard von anderen Firmen.\n\nEtwas später kamen leistungsfähigere Z80-Karten auf den Markt: Digital Research selbst vertrieb etwas später die sogenannte ALS-Card, mit eigenem 64-KB-Hauptspeicher und einem wesentlich schneller getakteten Z80 (6 MHz). Diese konnte sich aber am Markt gegen die etablierte MS Softcard nicht mehr durchsetzen. Auch andere brachten Z80 Karten auf den Markt: z. B. Applied Engineering. Die Grundlage für den Geschwindigkeitsrekord bei den Z80 Karten legte die Applicard von PCPI: ein Retro-Projekt von Alex Freed ergab ca. 2009 einen Klon der ursprünglich mit 6 MHz getakteten Applicard unter dem Namen Freed-card. Diese konnte theoretisch mit bis zu 20 MHz betrieben werden. Um dies einschätzen zu können, muss man wissen, dass die ursprüngliche Softcard von Microsoft zwar mit 4 MHz getaktet war, aber aufgrund des Timesharings mit dem 6502 nur 50 % der Takte selbst nutzen konnte. Damit brachte es die Softcard nur auf ein Äquivalent von 2 MHz. Für jede nicht direkt zur Softcard kompatible Karte wurden Varianten des Betriebssystems mit entsprechend angepasstem BIOS erstellt.\n\nDen meisten Nutzern und Kritikern von Microsoft-Software heute unbekannt, aber nicht weniger interessant, ist die Tatsache, dass diese Karte gut zwei Jahre lang (1979–1981) die Haupteinnahmequelle für Microsoft war, als die junge Firma noch keine Betriebssysteme und Büroanwendungen, sondern praktisch nur Compiler und Interpreter für verschiedene Programmiersprachen anbot, die sich nur in mäßigen Stückzahlen an den Käufer bringen ließen. Aufgrund der Erfahrungen, die diese Firma mit Heimcomputern und der passenden Software hatte, nahm IBM 1980 Kontakt mit Microsoft auf und bat um ein Betriebssystem nebst BASIC für den gerade in Entwicklung befindlichen IBM-PC.\n"}
{"id": "220190", "url": "https://de.wikipedia.org/wiki?curid=220190", "title": "Umschalttaste", "text": "Umschalttaste\n\nDie Umschalttaste oder Shift-Taste (engl. \"shift\" ‚umschalten‘, ‚verstellen‘, ‚wechseln‘) ist eine Taste, die auf Tastaturen von Computern und Schreibmaschinen (Hochstelltaste) vorkommt –  sogar doppelt: linke und rechte. Sie sind auf PC-Tastaturen meist linksbündig mit einem nach oben zeigenden invertierten Pfeil beschriftet: \n\nDie Tasten werden zum Umschalten zwischen Betriebsmodi benutzt, z. B. um zwischen der Erst- und Zweitbelegung aller Tastaturtasten zu wechseln. So kann man statt eines Kleinbuchstabens durch gleichzeitiges Drücken der Shift-Taste den entsprechenden Großbuchstaben eingeben. Außerdem kann man verschiedene Sonderzeichen eingeben, die den Nummerntasten des alphanumerischen Tastaturbereichs als Zweitbelegung zugeordnet sind. Zur besseren Unterstützung des Zehnfingersystems gibt es zwei Umschalttasten, sowohl rechts als auch links, mit meist identischer Funktion. Einige Programme unterscheiden zwischen linker und rechter Umschalttaste, insbesondere Spiele, Textverarbeitungsprogramme i. d. R. aber nicht.\n\nBei mechanischen Schreibmaschinen hebt die sogenannte Hochstelltaste die Schreibwalze oder senkt den Typenhebelkorb ab und sorgt so dafür, dass die Typen an einer anderen Position gegen das Farbband geschlagen werden, wodurch Großbuchstaben (bzw. Sonderzeichen) auf das Papier kommen. Bei einer jüngeren (elektro)mechanischen Schreibmaschine bleibt dagegen die Papierwalze immer unverändert, und die nun Umschalttaste genannte Taste senkt entweder den Typenhebelkorb ab oder dreht den Kugelkopf oder das Typenrad um 180 Grad. \n\nAuch für verschiedene Tastenkombinationen zur Steuerung von Betriebssystem-Funktionen wird die Taste benötigt. So wird beispielsweise unter Microsoft Windows kein Autostart beim Starten und kein Autorun beim Einlegen bzw. Einstecken von Medien (CDs, Speicherkarten, USB-Sticks) ausgeführt, während die Taste gedrückt ist. + Pfeiltaste markiert Text in die entsprechende Richtung, zusätzliches Halten von markiert immer ganze Wörter (nur nach links und rechts).\n\nUm zufällige Ergebnisse zu vermeiden, ist das „gleichzeitige Drücken der Umschalttaste“ nicht wörtlich zu nehmen. Genau genommen muss sie mehr oder weniger kurz vor der beabsichtigten Zeichentaste gedrückt und je nach verwendeter Technik entsprechend lang gedrückt und gehalten werden. Bei einer Computertastatur genügt ein Tastendruck wenige Millisekunden vorher und das Lösen der Umschalttaste kann gefahrlos vor dem Lösen der Zeichentaste erfolgen. Anders ausgedrückt: Während der Eingabe des gewünschten Großbuchstabens (oder Sonderzeichens) muss diese Taste gedrückt sein. Bei einer (elektro)mechanischen Schreibmaschine muss die Umschalttaste wesentlich früher gedrückt und meistens auch bis nach dem Anschlag der Type gehalten werden.\n\n"}
{"id": "221233", "url": "https://de.wikipedia.org/wiki?curid=221233", "title": "Andrew File System", "text": "Andrew File System\n\nDas Andrew Filesystem (AFS) ist ein gut skalierendes Netzwerkprotokoll für verteilte Netzwerkdateisysteme. Zehntausende Workstations, zehntausende Benutzer und hunderte Dateiserver sind keine Seltenheit. Durch das im Client integrierte Caching eignete sich das Protokoll auch für den Betrieb über das Internet. Klassischen Netzwerkdateisystemen wie NFS hat es voraus, dass Sekundärspeicher-Erweiterungen und Servertausch aus Nutzersicht vollkommen transparent möglich sind. Das wird durch eine zusätzliche Abstraktionsebene zwischen den Pfaden, mit denen Nutzer arbeiten, und den eigentlichen Datenobjekten des AFS realisiert.\n\nDas Konzept von AFS ist ganzheitlich. Es umfasst neben dem Austausch von Dateien auch Benutzerverwaltung, (Kerberos-basierte) Authentifikation, Datensicherung, bei Bedarf auch die für die kryptografischen Komponenten nötige Synchronisation der Uhrzeit zwischen Clients und Servern. In der Praxis werden heute ein separater Kerberos-Server und ein separates Backup-System benutzt.\n\nDie verschiedenen für AFS nötigen Funktionen (Client, Dateiserver, Datenbankserver) sind strikt voneinander getrennt und laufen i. d. R. auf verschiedenen physischen Maschinen. AFS-Serverprozesse laufen niemals auf Rechnern unter Nutzerkontrolle (z. B. Workstations) wie es z. B. bei Windows-SMB-Freigaben üblich ist.\n\nEin lokaler Cache auf AFS-Clients entlastet die Dateiserver und verbessert die Performance – vor allem beim Betrieb über WANs. Eine Cache-Konsistenz-Garantie ist in das Protokoll integriert. Authentifikation von Benutzern geschieht auf den Dateiservern – nicht wie z. B. bei NFS (unsicher) auf den Clients. Trotzdem erfordert nicht jede Sitzung eines Benutzers eine eigene explizite Verbindung zu einem Dateiserver, wie das z. B. bei SMB der Fall ist.\nZugriffsrechte werden per ACLs definiert, allerdings nur pro Verzeichnis. AFS ermöglicht, auf allen Clients einer Zelle einen zentral administrierten, einheitlichen Namensraum aufzubauen. AFS-Server arbeiten üblicherweise unter Linux, Solaris oder AIX, jedoch werden weitere Unix-Varianten als Serverplattform unterstützt.\n\nEs existieren verschiedene Programme, die AFS als Protokoll umsetzen. AFS-Clients sind für eine Vielzahl von Betriebssystemen verfügbar – i. d. R. lizenzkostenfrei. Leistungsfähige AFS-Server sind lizenzkostenfrei für Linux und andere Unix-Betriebssysteme verfügbar. AFS-Server mit speziellen Funktionen sind kommerziell verfügbar.\n\nDas AFS beherrscht Datenreplikation, jedoch nicht in Echtzeit. Die Replikation muss (was natürlich automatisierbar ist) vom Administrator ausgelöst werden. Es ist im AFS nicht wirtschaftlich, Daten oft (z. B. einmal pro Minute) zu replizieren.\n\nUnabhängige Verwaltungseinheiten im AFS heißen \"Zellen.\" Eine Zelle umfasst einen oder mehrere \"Datenbankserver\" (die einzigen Objekte, die ein AFS-Client über eine Zelle am Anfang kennen muss) und einen oder mehrere \"Dateiserver.\" AFS-Clients sind üblicherweise (unter Windows kann das anders sein) einer „Heimatzelle“ zugeordnet, allerdings existiert in den bestehenden AFS-Implementierungen keine zentrale Instanz, die über alle Clients Buch führt. Auf Dateiservern befinden sich Datenpartitionen, die wiederum Instanzen von Volumes enthalten. Volumeinstanzen können symbolische Verknüpfungen auf andere Volumeinstanzen (auch in anderen AFS-Zellen) enthalten, was benutzt wird, um einen Baum aufzuspannen, der den Dateinamensraum des AFS bildet. Ein definiertes Volume (üblicherweise \"root.afs\") wird von jedem AFS-Client an eine definierte Stelle (\"/afs\" unter Unix) im Dateisystem eingehängt und bildet die Wurzel dieses Baumes, wobei jedoch durch die symbolischen Verknüpfungen auch Zyklen in der Verzeichnisstruktur möglich sind.\n\nEs gibt weltweit zahlreiche AFS-Zellen – vor allem größere Einrichtungen wie Universitäten zählen dazu. Zellen werden unabhängig voneinander verwaltet und können auch öffentlich sein. Öffentliche Zellen zeichnen sich durch folgende Eigenschaften aus:\n\nZellen können sich auch gegenseitig das Vertrauen aussprechen, wodurch Benutzer einer Zelle in ACLs von AFS-Verzeichnissen Rechte erhalten können. Dieses Vertrauen wird durch entsprechende Kerberos-Mechanismen realisiert.\n\nDer Begriff Volume steht im Rahmen von AFS für zwei Dinge:\n\nVolumes und Volumeinstanzen werden ausschließlich vom Administrator verwaltet. Sie besitzen eine änderbare Maximalgröße. Diese wird ähnlich einer Quota benutzt, gilt jedoch für das Volume und nicht einzelne Benutzer. Es existieren vier Arten von Volumeinstanzen:\n\nFür alle Volumeinstanzen wird von Dateiserver jeweils eine Statistik geführt, in der Zugriffe unterteilt nach lesend/schreibend, lokales Netz/anderes Netz und einigen anderen Kriterien erfasst werden. OpenAFS-Dateiserver besitzen zusätzlich einen Modus, umfangreiche Logginginformationen über Zugriffe auf Instanzen auszugeben – wahlweise direkt an andere Programme (per Pipe).\n\nAFS-Dateiserver enthalten eine oder mehrere Datenpartitionen, die wiederum Volume-Instanzen enthalten. Das AFS-Netzwerkprotokoll kümmert sich prinzipiell nicht darum, in welchem Format die Volumes auf den Datenpartitionen lagern. Allen AFS-Implementierungen ist jedoch gemein, dass man die Dateistruktur des AFS-Namensraumes nicht wiedererkennt, wenn man sich eine Partition auf dem Dateiserver ansieht.\n\nEs ist deshalb auch nicht möglich, die Datenpartitionen mittels eines anderen Filesharing-Protokolls zusätzlich freizugeben.\n\nRW-Instanzen lassen sich im laufenden Produktivbetrieb zwischen Servern verschieben – Lese- und Schreibzugriff auf die Daten der Instanz ist weiterhin möglich. Dadurch ist die Wartung von Dateiservern möglich, ohne Zugriff auf dort gelagerte Daten zu verlieren.\n\nBei der heute meist-benutzten AFS-Implementierung (OpenAFS) besteht der Dateiserver aus mehreren Prozessen (die teilweise aus mehreren Threads bestehen):\n\nDa AFS nur ein Protokoll ist, kann sich hinter einem Dateiserver jedoch auch z. B. ein Bandroboter verbergen, der AFS-Dateien auf tertiären Speichermedien ablegt (z. B. MR-AFS).\n\nDateiserver können mehrere IP-Adressen haben. AFS-Clients wechseln beim Ausfall eines Dateiserver-Netzwerkinterfaces einfach auf das nächste. Clients testen aus diesem Grund regelmäßig die Erreichbarkeit aller Dateiserver-Netzwerkinterfaces, mit denen sie zu tun haben.\n\nDie Datenbankserver sind untereinander vernetzt und verwalten zwei oder mehr Datenbanken. Obligatorisch sind dabei:\nFolgende Datenbanken sind außerdem noch gängig:\n\nAlle Datenbanken werden pro Datenbankserver von jeweils einem Prozess verwaltet. Dabei kommt das Protokoll UBIK zum Einsatz. Dieses ermöglicht, dass immer dann noch Lese- und Schreibzugriff auf die AFS-Datenbanken möglich ist, wenn sich mehr als die Hälfte der Datenbankserver noch über das Netzwerk erreichen. Für Lesezugriff ist nur ein erreichbarer Datenbankserver nötig. Existieren also 5 Datenbankserver, kann z. B. einer auf eine neue Maschine migriert werden und der Ausfall eines weiteren würde immer noch nicht den Schreibzugriff kosten. Wenn die ausgefallenen Datenbankserver wieder online sind, gleichen sie automatisch den Datenbestand untereinander ab.\n\nDer aufwendige Synchronisationsmechanismus erfordert exakten Gleichlauf der internen Uhren der Datenbankserver. Wenn sich die Uhrzeiten zweier beliebiger Datenbankserver um mehr als 10 s unterscheiden, sperrt die Datenbank den Schreibzugriff.\n\nDatenbankserver sind die einzigen Objekte, die ein AFS-Client kennen muss, wenn er auf eine gegebene Zelle zugreifen will. Das kann zum einen über eine lokale Datei \"(CellServDB)\" oder auch per Domain Name System (über \"AFSDB\" Resource Record) geschehen.\n\nDer \"bosserver\" kommt auf allen AFS-Servern zum Einsatz. Ähnlich dem Init-Prozess auf Unix-Systemen verwaltet er eine Liste mit Prozessen, die auf einem Server zu laufen haben. Die laufenden Prozesse weisen einen AFS-Server dann als Datenbankserver, Dateiserver oder auch beides (nicht zu empfehlen) aus. Diese Liste und noch einige andere Sachen lassen sich über das Netzwerk verwalten.\n\nIn manchen AFS-Zellen kommen sog. Update-Server und Update-Clients zum Einsatz, die andere Serversoftware (z. B. Dateiserver-Prozesse) bei Bedarf aktualisiert.\n\nEin sog. \"butc\" kommt auf AFS-Tapecontrollern (sprich: AFS-Backup-Servern) zum Einsatz, um Daten von Dateiservern entgegenzunehmen und auf Band oder auch auf Festplatten zu speichern.\n\nAFS arbeitet heutzutage ausschließlich per UDP, allerdings existiert mit RX eine Abstraktionsschicht, die prinzipiell auch andere Protokolle wie TCP zulässt – es gibt Pläne, genau das für OpenAFS zu realisieren.\n\nDas Rx-Protokoll arbeitet im authentifizierten Modus (sprich: wenn ein Benutzer nicht ohne sich vorher einzuloggen arbeitet) immer signiert – üblicherweise auch verschlüsselt. Das bezieht sich z. B. auch auf Übertragungen zwischen AFS-Client und AFS-Dateiserver.\n\nAFS ist sehr empfindlich im Bezug auf Firewalls. Folgende (UDP-)Ports müssen zwischen Servern und Clients sowie zwischen den Servern untereinander freigeschaltet sein:\n\nAbgesehen von derzeitig nicht bekannten Sicherheitsschwachstellen werden alle diese Ports als sicher angesehen, können also auch per Internet erreichbar sein.\n\nAFS arbeitet mit festen Portnummern und hat deshalb keine Probleme mit üblichen NAT-Routern.\n\nDie Sicherheit von AFS wird dadurch gewährleistet, dass jeder AFS-Server (Datenbank- wie Fileserver) einen zellenweit einheitlichen symmetrischen Schlüssel (Shared-Secret) erhält. Dieser Schlüssel ist auch dem Kerberos-Server bekannt und kann deshalb dafür eingesetzt werden, um Benutzer zuverlässig zu authentifizieren. Der Schlüssel ist 56 bit breit und damit nicht mehr State-of-the-Art.\n\nDatenübertragungen werden mit einem ebenfalls 56 bit breiten Sitzungsschlüssel signiert und bei Bedarf mit einem AFS-eigenen Algorithmus namens fcrypt verschlüsselt.\n\nBei anonymen Zugriffen auf das AFS (sprich: wann immer ein Nutzer kein AFS-Token hat) existiert für den Client keine Möglichkeit, den Fileserver sicher zu authentifizieren, womit weder die Integrität noch die Vertraulichkeit von Datenübertragungen sichergestellt werden kann.\n\nWird ein Fileserver kompromittiert und der Zellenschlüssel fällt in die Hände eines Angreifers, so ist es diesem möglich, mit Superuserrechten auf allen Fileservern zu agieren, Daten sämtlicher Nutzer auszulesen und auch diese zu verändern. DFS, der „ehemalige Nachfolger“ von AFS, räumt mit diesem Problem auf, für AFS ist noch keine Lösung in Sicht.\n\nDie geringe Schlüsselbreite ist ebenfalls ein Problem und rückt Brute-Force-Angriffe in den Bereich des Möglichen. Durch die Verwendung von Sitzungsschlüsseln ist die Gefahr noch relativ gering und nicht zu vergleichen mit der Schwäche z. B. von WEP.\n\nDie fehlende Integritätsprüfung bei anonymen Zugriffen ist eine kritische Schwachstelle, da bei der verbreitetsten AFS-Client-Variante „OpenAFS“ ein Shared-Cache zum Einsatz kommt. Anonym vom Fileserver geholte Dateien werden deshalb auch eingeloggten AFS-Nutzern zurückgeliefert, wenn diese auf solche zugreifen. Ein Angreifer kann – wenn man keine Gegenmaßnahmen ergreift – mit ein wenig Aufwand die Integritätsprüfung für angemeldete Benutzer aushebeln. Diese Schwachstelle ist unkritisch für Single-User-Maschinen, an denen Benutzer lediglich authentifiziert arbeiten. Mehrbenutzersysteme sind jedoch besonders gefährdet. Es ist derzeitig kein praktisch durchgeführter Angriff bekannt.\n\nGegen das Problem der zellenweit einheitlichen Schlüssel sind folgende organisatorische Maßnahmen zu treffen:\n\nGegen die geringe Schlüsselbreite hilft nur eine Neuimplementierung der Sicherheitsschicht des verwendeten RPC-Protokolls (Rx). Es existieren Unternehmen, die AFS-Programmierdienste anbieten und gegen Bezahlung auch solche Probleme angehen. Ein regelmäßiger Schlüsselwechsel vermindert die Gefahr erfolgreicher Brute-Force-Angriffe.\n\nUm die beschriebenen Angriffe gegen die Integrität übertragener Daten auszuschließen, muss man auf dem jeweiligen Client anonyme AFS-Zugriffe unterbinden. Das ist nur auf Maschinen praktikabel, auf die keine normalen Benutzer authentifizierten Zugriff (Shell-Accounts, FTP, Webdav, …) haben. Alle Dienste müssen auf einem solchen Rechner grundsätzlich mit einem Token arbeiten. Auch Cronjobs dürfen dabei nicht vergessen gehen.\n\nZur Vereinfachung wird der Dateinamensraum im AFS vom Administrator i. d. R. zyklenfrei aufgebaut. Eine Garantie dafür kann es jedoch nicht geben, sobald Nutzer das Recht bekommen, Volume-Mountpoints anzulegen oder Rechte zu verändern. Das kann z. B. für Backup-Software ein Problem darstellen.\n\nDas Dateisystem kennt drei Objekttypen:\n\nDer Administrator einer Zelle definiert den Namensraum, indem er Volumes gut strukturiert ineinanderhängt. Beginnend mit dem Standardvolume \"root.cell\" greift man dann z. B. auf Volumes wie \"homedirectories,\" \"software,\" \"projekte\" und \"temp\" zu und hängt z. B. in das \"homedirectories\"-Volume weitere mit dem Namen \"home.ernie,\" \"home.bert\", … ein. Der Pfad zu Bert sieht dann z. B. so aus:\n\n\"Hinweise:\"\n\nUnter Betriebssystemen, denen das Konzept der symbolische Verknüpfungen fremd ist (z. B. Windows), erscheinen diese als Verzeichnisse im Dateinamensraum des AFS. Neuere Windows-Clients enthalten entsprechende Erweiterungen, um solche Verknüpfungen als Junction Points auszudrücken sowie Shell-Erweiterungen um damit umzugehen.\n\nDas AFS-Protokoll unterstützt netzwerkweite Dateisperren, jedoch nur sog. Advisory Locks \"(flock()),\" keine Byte Range locks \"(lockf()).\" Der OpenAFS-Windows-Client ist seit Version 1.5.10 in der Lage, lokal Byte Range Locks zu emulieren. Dabei können lokale Anwendungen auf der Client-Maschine derartige Locks benutzen, der AFS-Client sperrt entsprechende Dateien auf dem Dateiserver jedoch über einfache Advisory Locks.\n\nDie Anzeige des freien bzw. belegten Speichers des gemounteten AFS (Unix) ist eine Fantasiezahl. Prinzipiell kann in einem verteilten Netzwerkdateisystem der freie bzw. belegte Speicher nur pro Verzeichnis ermittelt werden. Der Windows-Client ist in der Lage, den freien Speicher pro Verzeichnis an Anwendungen zurückzumelden.\n\nAFS-Clients sind Computer (z. B. Workstations), die auf den AFS-Dateinamensraum zugreifen können. Unter Unix-Betriebssystemen ist dazu eine Kernelerweiterung nötig. Das geschieht entweder über einen generischen Dateisystemtreiber wie FUSE (Arla-AFS-Client) oder über ein umfassenderes AFS-spezifisches Kernel-Modul (OpenAFS). In beiden Fällen sind zusätzliche Userspace-Prozesse nötig, die den Kernel-Treibern zuarbeiten. Der Openafs-Windows-Clients basiert auf einem für AFS entwickelten Redirektor, der mit einem Windows-Dienst kooperiert.\n\nAFS-Clients (auch \"Cache-Manager\") sind in der Lage, große Datenmengen von Dateiservern zwischenzuspeichern, wobei nicht ganze Dateien, sondern Stückchen anpassbarer Größe abgelegt werden. Die optimale Größe eines solchen Caches ist abhängig von Nutzungsmuster und kann auch viele Gigabytes betragen.\n\nDie Cache-Integrität wird vom AFS garantiert. Ein Dateifragment, das vom Cachemanager eingelagert wird, ist gültig, bis der entsprechende AFS-Server es aktiv invalidiert. Das geschieht für RW-Instanzen z. B. wenn die entsprechende Datei von einem anderen AFS-Client modifiziert wurde, bei RO-Instanzen z. B. dann, wenn der Administrator die Replikation auslöst.\n\nAktiv gecacht werden ausschließlich Lesevorgänge. Schreibzugriffe werden zwar auch gepuffert, wenn jedoch eine zum schreibenden Zugriff geöffnete Datei geschlossen wird, blockiert das close()-Kommando solange, bis alle Daten zum Dateiserver geschrieben wurden.\n\nDer Cache ist bei Openafs für Unix persistent. Die Cache-Integrität wird nach Neustart durch den Abgleich der Änderungszeitstempel von Dateien mit dem Fileserver realisiert. Die Persistenz des Caches macht u. U. auch in lokalen Netzen die Nutzung von riesigen Caches zur Geschwindigkeitssteigerung sinnvoll.\n\nUnter Windows besteht der Cache aus einer einzigen Datei, die per Memory Mapping benutzt wird. Die Maximalgröße des virtuellen Speichers (4 GB bei einem 32-Bit-System) ist deshalb eine unüberwindbare Hürde für die Cache-Größe.\n\nUnter Openafs für Unix-Systemen besteht der Cache aus vielen Dateien in einem Verzeichnis. An das Dateisystem, in dem dieses Verzeichnis liegt, werden erhöhte Ansprüche gestellt:\n\nOpenafs erlaubt auch die Verwendung des Hauptspeichers (RAM) anstatt eines Verzeichnisses auf der Festplatte für den Cache (Option afsd -memcache).\n\nAFS wird auf vielen Plattformen unterstützt. Das ist für AFS-Serverprozesse leichter zu realisieren, als für AFS-Clients da keine Kernelerweiterungen nötig sind. Es existieren verschiedene Projekte, die das AFS-Protokoll ganz oder teilweise implementieren – hier eine nicht auf Vollständigkeit pochende Liste:\n\nLegende:\n\nFür AFS-Server sollte man tunlichst auf die jeweils empfohlenen Plattformen zurückgreifen. Es existieren zwar z. B. experimentelle AFS-Server-Versionen für Windows oder alte AFS-Server-Versionen für IRIX, jedoch werden diese nicht offiziell unterstützt bzw. von Fehlern befreit.\n\nTransarc-AFS und seine Nachfolger verfügen über einen NFS-Server im Client, der andere Plattformen, für die kein Client existiert per NFS Zugriff auf den AFS-Namensraum gewähren kann. Allerdings ist nur von Solaris bekannt, dass ein darauf laufender aktueller OpenAFS-Client das noch unterstützt. Prinzipiell sollte jedoch jeder im Userspace laufende Serverprozess (z. B. Samba, Userspace-NFS-Server, Webdav, …) problemlos Dateien aus dem AFS freigeben können. Ohne spezielle Anpassungen an der Serversoftware werden so aber nur anonyme Zugriffe möglich sein.\n\nAFS war ursprünglich ein universitäres Projekt der Carnegie Mellon University und umfasste eine Client- sowie eine Server-Implementierung. Es wurde von der Firma Transarc später unter dem Namen \"Transarc AFS\" vermarktet. Transarc wurde von IBM aufgekauft, AFS unter dem Namen IBM-AFS vermarktet. IBM gab AFS im Jahre 2000 unter einer Open-Source-Lizenz (IBM Public License) frei – es heißt seitdem OpenAFS und wird aktiv weiterentwickelt. Weiterhin sind jedoch weltweit zahlreiche Transarc- und IBM-AFS-Server im Einsatz.\n\nOpenAFS ist die am aktivsten gepflegte AFS-Implementierung.\n\nDas Hauptaugenmerk der Entwicklung bei OpenAFS liegt derzeitig für Server auf\nGrundsätzlich gilt, dass der AFS-Server nur in geringem Maße vom Wirtsbetriebssystem abhängig ist. Es sollte also z. B. auch auf einer älteren Version von Linux möglich sein, den Server (der i. d. R. ausschließlich im Userspace arbeitet), zu übersetzen und zu betreiben. Ausnahmen sind Serverversionen, die spezielle Modifikationen am Wirtsdateisystem vornehmen (sog. inode-Server). Diese benötigen zusätzliche Kernelmodule und werden auch für neue AFS-Installationen praktisch nicht mehr eingesetzt.\n\nUnterstützte Client-Plattformen sind\n\nClients für ältere Plattformen – z. B. für ältere Windowsversionen findet man auf OpenAFS durch etwas suchen in den alten OpenAFS-Releases.\n\nIm Rahmen des DCE-Standards wurde das verteilte Dateisystem DFS als Nachfolger von AFS entwickelt. Dieses bietet u. a. folgende Vorteile:\n\nDem DFS war trotz Abwärtskompatibilität zu AFS jedoch kein Erfolg beschieden, da dessen Einsatz an hohe Lizenzgebühren gebunden war.\n\nDas Arla-Projekt entstand zu einer Zeit, als es noch keine freie AFS-Implementierung gab und Transarc-AFS mit Lizenzzahlungen verbunden war. Es wurde unabhängig vom „AFS-Mainstream“ (AFS … OpenAFS) an der KTH als Open-Source-Software entwickelt. Bis jetzt existiert nur eine Client-Implementierung, diese deckt jedoch einige von OpenAFS nicht unterstützte Plattformen ab.\n\nMR-AFS (\"Multi-Resident\"-AFS) entstand als kommerzielle Weiterentwicklung von Transarc-AFS. MR-AFS’ Stärke ist, dass die Dateiserver Dateien aus dem AFS-Namensraum auf Tertiärspeicher (Bänder, optische Datenträger, …) auszulagern in der Lage sind. Die Dateiserver schreiben dazu in ein HSM-Dateisystem und überlassen die eigentlichen Migrationsentscheidungen der HSM-Software. Dabei können normale AFS-Clients mit MR-AFS-Servern Daten austauschen. MR-AFS befasst sich ausschließlich mit Serversoftware. MR-AFS-spezifische Funktionen sind z. B. in die Kommandozeilentools von OpenAFS eingebaut. Die Zukunft von MR-AFS ist ungewiss, da der einzige Entwickler bereits in Rente ist.\n\nHostafs ist eine kleine AFS-Server-Implementierung, die zum Ziel hat, normale Verzeichnisse als Volumes zu tarnen und per AFS freizugeben. Auf diese Weise kann man z. B. CDROMs im AFS verfügbar machen. Hostafs stellt jedoch keine Zugriffsschutzmechanismen wie ACLs zur Verfügung – alle Freigaben sind für alle lesbar.\n\nDiese AFS-Implementierung besteht aus einem Client, der als Linux-Kernelmodul realisiert ist. Er ist Teil des Standard-Linux-Kernels. Der Client ist jedoch nicht für produktiven AFS-Betrieb gedacht, sondern z. B. für das Booten über Netzwerk, wenn der Administrator wirklich alles im AFS vorhalten will. Es besitzt keine Möglichkeit authentifiziert auf das AFS zuzugreifen, unterstützt nur lesenden Zugriff und spricht nur mit Dateiservern. Letzteres bedeutet, dass man den zu benutzenden Dateiserver explizit angeben muss – das Modul kann dazu nicht den vlserver befragen.\n\nWegen Unzufriedenheit mit den organisatorischen Mechanismen der Weiterentwicklung des AFS-Protokolls arbeiten einige Openafs-Entwickler an einem kommerziellen Fork von Openafs mit dem Namen \"YFS\". Diese Abspaltung kann sowohl mit dem AFS- als auch mit dem massiv verbesserten YFS-Protokoll umgehen. Es gibt derzeit keine offizielle Veröffentlichung (Stand Januar 2013).\n\nAm Rechenzentrum Garching ist ein AFS-Server (mit entsprechenden in den OpenAFS-Client einfließenden Client-Modifikationen) mit OSD (Object Storage) in Entwicklung. Dabei werden die Metadaten (Zugriffsrechte, Timestamps, Verzeichnisstrukturen) weiterhin vom AFS-Server verwaltet, die Daten liegen jedoch auf sog. Object-Storage-Servern, mit denen der Client dann direkt kommuniziert. Auf diese Weise können z. B. Dateien auf mehreren Servern liegen (Striping) und deutlich schneller gelesen und geschrieben werden.\n\n\nDiverse Programme stören sich daran, wenn sie im AFS laufen.\n\n\nDas Aufsetzen einer AFS-Zelle ist deutlich schwerer, als z. B. das Anlegen einer SMB-Share oder eines NFS-Exports.\nDie kryptografische Absicherung der Authentifikation mittels Kerberos erfordert einen gewissen Aufwand, der unabhängig von der Größe der Zelle ist. Zusätzlich kostet das Design der Zelle Zeit.\n\nSeine Vorteile spielt AFS in folgenden Situationen aus:\n\nWenn die AFS-Zelle erstmal läuft, beschränkt sich die Arbeit des AFS-Administrators auf das Aufrüsten und ggf. Austauschen von Servern. Der Administrationsaufwand ist dann im Verhältnis zur Nutzeranzahl und Speichergröße extrem niedrig. Zellen mit vielen Terabytes an Daten und mehreren tausend Nutzern kommen dabei u. U. mit einer Administratorstelle aus.\n\nDer Aufwand für Nutzer sollte nicht unterschätzt werden – die pro-Verzeichnis-ACLs sind ungewohnt und ACLs allgemein sind ein Konzept, das vor allem in der Unix-Welt erst langsam Bedeutung gewinnt.\n\nAls sinnvolle Strategie hat sich erwiesen, die AFS-Homedirectories mit gewissen Standardpfaden zu versehen, die das Sicherheitsniveau ausdrücken (z. B. codice_1, codice_2) und den Benutzer so, abgesehen von Ausnahmefällen, von ACLs fernzuhalten.\n\nDa eine Benutzer-Anleitung jedoch nicht abstrakt sein sollte, wird ein AFS-Administrator eine solche für die eigene Zelle meist selbst schreiben und lokale Besonderheiten darin berücksichtigen.\n\nAFS wird von vielen Herstellern von Backup-Lösungen nicht unterstützt. Die Gründe dafür sind vielschichtig. Eine eigene Backup-Lösung ist jedoch vergleichsweise schnell in Form einiger Shell-Skripts programmiert.\n\n\n\n"}
{"id": "222172", "url": "https://de.wikipedia.org/wiki?curid=222172", "title": "Bresenham-Algorithmus", "text": "Bresenham-Algorithmus\n\nDer Bresenham-Algorithmus ist ein Algorithmus in der Computergrafik zum Zeichnen (Rastern) von Geraden oder Kreisen auf Rasteranzeigen. Für Linienalgorithmen gibt es einen eigenen Übersichtsartikel, hier wird mehr die konkrete Implementierung erläutert.\n\nDer Algorithmus wurde 1962 von Jack Bresenham, damals Programmierer bei IBM, entwickelt. Das Besondere an seinem Algorithmus ist, dass er Rundungsfehler, die durch die Diskretisierung von kontinuierlichen Koordinaten entstehen, minimiert, und gleichzeitig einfach implementierbar ist, mit der Addition von ganzen Zahlen als komplexeste Operation, und somit ohne Multiplikation, Division und Gleitkommazahlen auskommt.\n\nDurch eine geringfügige Erweiterung lässt sich der ursprüngliche Algorithmus, der für Geraden entworfen wurde, auch für die Rasterung von Kreisen verwenden. Sogar die Quadratterme, die beim Kreis vorkommen, lassen sich rekursiv ohne jede Multiplikation aus dem jeweils vorhergehenden Term ableiten nach formula_1, wobei der Term formula_2 nicht als Multiplikation zählt, da er in Hardware bzw. auf Assemblerebene als einfache Shift-Operation implementiert wird und der Term formula_3 im Endeffekt ganz vermieden werden kann.\n\nAufgrund obiger Eigenschaften ist die Bedeutung des Bresenham-Algorithmus bis heute ungebrochen, und er kommt unter anderem in Plottern, in den Grafikchips moderner Grafikkarten und in vielen Grafikbibliotheken zur Verwendung. Dabei ist er so einfach, dass er nicht nur in der Firmware solcher Geräte verwendet wird, sondern in Grafikchips direkt in Hardware implementiert werden kann.\n\nDer Name „Bresenham“ wird heute zudem für eine ganze Familie von Algorithmen verwendet, die eigentlich von Anderen später entwickelt wurden, jedoch in der Nachfolge von Bresenham und mit einem verwandten Ansatz. Siehe weiterführende Literaturhinweise unten.\n\nDie hier vorgestellte Variante ist ein sehr praxisnaher Ansatz und wurde zuerst von \"Pitteway\" veröffentlicht und von \"van Aken\" verifiziert. Weiter unten wird die Variante mit der originalen Formulierung von Bresenham verglichen und gezeigt, dass die Lösungen äquivalent sind.\n\nZum Verständnis des Algorithmus beschränkt man sich auf den ersten Oktanten, also eine Linie mit einer Steigung zwischen 0 und 1 von formula_4 nach formula_5. Seien formula_6 und formula_7 mit formula_8. Für andere Oktanten muss man später Fallunterscheidungen über Vorzeichen von formula_9 und formula_10 und die Rollenvertauschung von formula_11 und formula_12 treffen.\n\nDer Algorithmus läuft dann so, dass man in der „schnellen“ Richtung (hier die positive formula_11-Richtung) immer einen Schritt macht und je nach Steigung hin und wieder zusätzlich einen Schritt in der „langsameren“ Richtung (hier formula_12). Man benutzt dabei eine Fehlervariable, die bei einem Schritt in formula_11-Richtung den (kleineren) Wert formula_10 subtrahiert bekommt. Bei Unterschreitung des Nullwerts wird ein formula_12-Schritt fällig und der (größere) Wert formula_9 zur Fehlervariablen addiert. Diese wiederholten „Überkreuz“-Subtraktionen und -Additionen lösen die Division des Steigungsdreiecks formula_19 in elementarere Rechenschritte auf.\n\nZusätzlich muss dieses Fehlerglied vorher sinnvoll initialisiert werden. Dazu betrachtet man den Fall von formula_20, bei dem in der Mitte (nach der Hälfte von formula_9) ein formula_12-Schritt kommen soll. Also initialisiert man das Fehlerglied mit formula_23. (Ob dabei zu einer ganzen Zahl auf- oder abgerundet wird, spielt kaum eine Rolle.)\n\nMathematisch gesehen wird die Geradengleichung\naufgelöst in\nund die Null links durch das Fehlerglied ersetzt. Ein Schritt um 1 in formula_11-Richtung (Variable formula_11) bewirkt eine Verminderung des Fehlerglieds um formula_28. Wenn das Fehlerglied dabei unter Null gerät, wird es durch einen Schritt um 1 in formula_12-Richtung (Variable formula_12) um formula_31 erhöht, was nach der Voraussetzung formula_32 das Fehlerglied auf jeden Fall wieder positiv macht, bzw. mindestens auf Null bringt.\n\nDer originale Ansatz nach Bresenham (s. u.) geht mehr geometrisch vor, wodurch in seinen Iterationsformeln auf beiden Seiten (bis auf das Fehlerglied) ein zusätzlicher Faktor 2 mitgeführt wird und auch die Fehlergliedinitialisierung anders hergeleitet wird.\n\nEine erste Implementierung ist nicht sehr elegant, demonstriert das Prinzip des Algorithmus aber sehr gut.\n\n REM Quasi-Bresenham-Algorithmus REM Original-Bresenham\n\nAbgesehen von der Anpassung an den verwendeten BASIC-Dialekt sind folgende Unterschiede bei der originalen Formulierung zu beachten:\n\nWenn man diese Unterschiede in der Formulierung berücksichtigt, stellt sich heraus, dass beide Formulierungen identisch arbeiten und somit äquivalent sind.\n\nAls eleganter formulierte Beispiele folgen als erstes der Quellcode eines BASIC-Programmes und anschließend eines Unterprogramms in C, welche die Fallunterscheidung in acht Oktanten nicht ausdrücklich vornehmen müssen.\n\nDer Algorithmus soll für alle Oktanten gültig werden. Dabei müssen die Vorzeichen der Koordinatendistanzen und die eventuelle Vertauschung der Rollen von x und y berücksichtigt werden. Wenn man diese Fallunterscheidungen innerhalb der innersten Schleife treffen würde, also in hoher Anzahl, würde das die Geschwindigkeit der Berechnungen deutlich verringern. Eine effiziente Lösung versucht, all diese Fallunterscheidungen in der Initialisierungsphase des Verfahrens vor der inneren Hauptschleife abzuarbeiten, so dass innerhalb der inneren Schleife weiterhin nur die eine Abfrage für das Bresenham-Fehlerglied erfolgen muss.\n\nDiese Formulierung führt (wie schon Stockton indirekt vorschlug) diverse Abstraktionen ein: Zunächst wird der Schritt in die schnelle Richtung jetzt als \"Parallelschritt\" (parallel zu einer Koordinatenachse) angesehen, und wenn zusätzlich ein Schritt in die langsame Richtung erfolgt, wird das zu einem \"Diagonalschritt\". Dann kann man in der Initialisierung Variablenwerte ermitteln, die für diese Fälle die Schrittweiten in den beiden Koordinatenrichtungen vorab festlegen und somit die Verallgemeinerung für die acht Oktanten erreichen. Beispielsweise ist bei einem Parallelschritt die Schrittweite in die dazu senkrechte Richtung eben Null. Zweitens rechnet man den Fehlerterm weiterhin wie im ersten Oktanten, mit Absolutbeträgen der Distanzen. In der innersten Schleife wird dann nicht mehr zuerst der Schritt in der schnellen Richtung ausgeführt, sondern als Erstes der Fehlerterm aktualisiert, und danach erst werden die Schrittweiten zu den bisherigen Koordinaten addiert, je nachdem, ob ein Parallel- oder ein Diagonalschritt erfolgen muss:\n\n REM Bresenham-Algorithmus für eine Linie in einem beliebigen Oktanten in Pseudo-Basic\n\nIn dieser Implementierung wird Gebrauch von der Signumfunktion gemacht.\n\n/* signum function */\nint sgn(int x){\n\nvoid gbham(int xstart,int ystart,int xend,int yend)\n\n/* Entfernung in beiden Dimensionen berechnen */\n\n/* Vorzeichen des Inkrements bestimmen */\n\n/* feststellen, welche Entfernung größer ist */\n\n/* Initialisierungen vor Schleifenbeginn */\n\n/* Pixel berechnen */\n} /* gbham() */\nDer Bresenham-Algorithmus kann auch in einer einfachen Variante in C implementiert werden:\n\nvoid line(int x0, int y0, int x1, int y1)\n\nDas Fehlerglied wird dabei sowohl für die langsame als auch die schnelle Richtung als Schritterkennung verwendet. Die vier Quadranten werden über das Vorzeicheninkrement (sx, sy) abgedeckt. Die Akkumulation des Fehlerglieds löst bei Überschreitung des Schwellwertes den bedingten Schritt aus, im Unterschied zur ursprünglichen Variante simultan in beide Richtungen: positive Fehlerwerte für x und negative für die y-Achse. Das Beispiel zeigt damit auch elegant die xy-Symmetrie des Bresenham-Algorithmus.\n\nDer Ansatz für die \"Kreisvariante\" des Bresenham-Algorithmus geht auch nicht auf Bresenham selbst zurück, er ähnelt der \"Methode von Horn\" aus dem Übersichtsartikel, siehe auch \"Pitteway\" und \"van Aken\" unten. Man geht entsprechend von der Kreisgleichung\nformula_33\naus. Man betrachtet zunächst wieder nur den ersten Oktanten. Hier möchte man eine Kurve zeichnen, die beim Punkt (r,0) anfängt und dann nach oben links bis zum Winkel von 45° fortgesetzt wird.\n\nDie „schnelle“ Richtung ist hier die formula_12-Richtung. Man macht immer einen Schritt in die positive formula_12-Richtung, und ab und zu muss man auch einen Schritt in die „langsame“, negative formula_11-Richtung machen.\n\nDie ständigen Quadratberechnungen (siehe Kreisgleichung) oder womöglich sogar trigonometrische oder Wurzelberechnungen vermeidet man wieder durch Auflösen in Einzelschritte und rekursive Berechnung der Terme aus den jeweils vorangehenden.\n\nMathematisch: Von der Kreisgleichung kommt man zur umgeformten Gleichung\nwobei formula_38 gar nicht explizit berechnet werden muss,\nwobei auch hier formula_41 nicht als eigene Variable mitgeführt werden muss, sondern nur die Differenz von einem Schritt zum nächsten formula_42 dem Fehlerglied aufgeschlagen wird. Wieder wird die Null in der Gleichung durch das Fehlerglied ersetzt.\n\nZusätzlich muss man dann beim Setzen der Pixel noch die Mittelpunktskoordinaten hinzuaddieren. Diese ständigen Festkommaadditionen schränken die Performance aber nicht merkbar ein, da man sich ja Quadrierungen oder gar Wurzelziehungen in der innersten Schleife erspart.\n\nDurch den Ansatz von der Kreisgleichung aus ist sichergestellt, dass die Koordinaten maximal um 1 Pixel, den Digitalisierungsfehler, von der Idealkurve abweichen. Die Initialisierung des Fehlerglieds soll nun bewirken, dass der erste Schritt in die langsame Richtung dann erfolgt, wenn die echte Kreiskurve um ein halbes Pixel in der langsamen Koordinate nach innen gekommen ist. Details zur Rechnung weiter unten, es läuft auf eine Initialisierung des Fehlerglieds mit dem Radius r hinaus.\n\nDie Formulierung wird hier wieder nur für den ersten Oktanten gezeigt, und wieder ergeben sich die anderen Oktanten durch Vorzeichenwechsel in formula_9 und formula_10 und Rollenvertauschung von formula_11 und formula_12. Die Erweiterung auf den Vollkreis, wie sie für Grafikdisplays, aber nicht für Plotter geeignet ist, ist in Kommentaren beigefügt.\n\nEine mögliche Implementierung des Bresenham-Algorithmus für einen Vollkreis in C. Hierbei wird für die quadratischen Terme eine weitere Variable mitgeführt, die dem Term formula_47 von oben entspricht, sie muss von einem Schritt zum nächsten lediglich um 2 erhöht werden:\n\nDen Schnittpunkt, an dem die Kreislinie um 1/2 Pixel nach innen gekommen ist, berechnet man nach der Kreisgleichung:\n\nAm gefragten Punkt (x,y) soll gelten: formula_50, also ergibt sich:\n\nDa bis hierhin noch kein x-Schritt stattgefunden haben soll, ist das Fehlerglied mit\nzu initialisieren, wobei y² durch Runden zu r wird.\n\n\"Beispiel:\" im Bild oben: r=11, y=3 (abgerundet), Fehlerglied bei y=y=3 ist 1+3+5=9, Fehlerglied bei y=4 ist 1+3+5+7=16. Wurde das Fehlerglied mit r=11 initialisiert, so findet der erste Vorzeichenwechsel und damit der erste x-Schritt tatsächlich beim Übergang von y zu y+1 statt.\n\nDie obigen Implementierungen zeichnen immer nur komplette Oktanten bzw. Kreise. Wenn man nur einen bestimmten Kreisbogen von einem Winkel formula_53 bis zu einem Winkel formula_54 zeichnen will, muss man das so implementieren, dass man sich die formula_11- und formula_12-Koordinaten dieser Endpunkte im Vorhinein berechnet, wobei man unvermeidlich auf Trigonometrie oder Wurzelrechnung zurückgreifen muss (s. a. Heron-Verfahren). Dann lässt man den Bresenham-Algorithmus über den kompletten Oktanten bzw. Kreis laufen und setzt die Pixel aber nur dann, wenn sie in den gewünschten Bereich fallen. Nach Beendigung dieses Kurvenstücks kann man den Algorithmus vorzeitig abbrechen.\n\nAuch für Ellipsen gibt es wieder mehrere mögliche Ansätze. Man kann bei achsenparallelen Ellipsen von der entsprechenden Gleichung\nwobei formula_58 und formula_59 die beiden Halbachsenlängen angeben, ausgehen und dann ähnlich wie oben beim Kreis vorgehen.\n\nMan kann aber auch durch Skalierung der gezeichneten formula_11- und formula_12-Werte (und ggf. horizontaler bzw. vertikaler Linienerweiterungen) auf Basis des Kreisalgorithmus solche achsenparallele Ellipsen erzeugen. Dabei benutzt man den Kreisalgorithmus mit der kleineren Ellipsenachse als Radius und addiert in der anderen Richtung einen Wert hinzu, den man wiederum per Bresenham-Linienalgorithmus ansteigend vom Pol zum Äquator ermitteln kann. Da die Ellipse in die längere Achsenrichtung gestreckt werden muss, setzt man dann nicht nur einzelne Pixel, sondern muss ggf. eine Linie (allerdings eine einfache, horizontale oder vertikale) vom vorherigen Punkt zum nächsten ziehen.\n\nEine allgemeine Ellipse kann man aus so einer achsenparallelen gewinnen, indem man die komplette Grafik zusätzlich einer Scherung unterwirft. Wieder benutzt man einen zusätzlichen Bresenham-Linienalgorithmus, um den Versatz in eine der Achsenrichtungen ansteigend zu ermitteln und ihn bei jeder zu zeichnenden Koordinate einzubeziehen.\n\nWie bei dem Algorithmus für die Linie kann auch die Kreisvariante xy-symmetrisch formuliert werden. Damit kann also ein kontinuierlicher Viertelkreis gezeichnet werden, was bei Ellipsen hilfreich ist.\n\nvoid ellipse(int xm, int ym, int a, int b)\n\nDer Algorithmus testet dabei immer einen Diagonalschritt und korrigiert diesen bei zu großer Abweichung. Die Schritte enden aber immer im nächsten Quadranten und dann wird bei flachen Ellipsen (b=1) zu früh abgebrochen. In diesen Fällen ist also eine Ergänzung notwendig. Die Fehlervariable muss den 3-fachen Wertebereich (Stellenanzahl, Bits) vom Radius (Halbachsen) aufweisen (etwa 64-bit oder Gleitkommazahl).\n\nDie Methode kann auch für Kreise (a=b=r) verwendet werden. Die Vereinfachung (indem etwa die Fehlervariable durch 2r gekürzt wird) führt dann zu den oben gezeigten Kreisbeispielen. Aus vier nahtlosen Viertelkreisen wird so ein kontinuierlicher Vollkreis, wie es etwa bei Plottern erforderlich ist.\n\nBereits im Jahr 1968 wurde die Idee publiziert, den Bresenham-Algorithmus für die Digitalisierung von durch kubische Gleichungen beschriebenen Kurven zu verallgemeinern. Wirklich ausgeführt wurden die Details erst 1993 u. a. von Pitteway und unabhängig davon in US-Patent 5717847. Eine Anwendung zur Strukturierung im Sub-Mikrometer-Bereich von durch rationale kubische Bézierkurven berandeten geometrischen Figuren fand das Verfahren in dem Lithografie-Tool LION-LV1.\n\n"}
{"id": "222179", "url": "https://de.wikipedia.org/wiki?curid=222179", "title": "Ipfw", "text": "Ipfw\n\nipfw oder \"ipfirewall\" ist eine Internet-Protocol-Firewall des Betriebssystems FreeBSD.\n\nipfw besteht aus 7 Komponenten:\n\nHinzu kommt eine Benutzerschnittstelle gleichen Namens, die Befehle entgegennimmt.\n\nipfw wurde ursprünglich von Daniel Boulet für das Unternehmen \"Berkeley Software Design Incorporated\" programmiert. Für FreeBSD programmierte Ugen J. S. Antsilevich die Software neu. FreeBSD 2.0 war die erste Version mit ipfw. Mit FreeBSD 2.2.8 wurde ipfw um die Komponente \"dummynet\" für Traffic-Shaping ergänzt, die Luigi Rizzo programmiert hatte.\n\nSeit FreeBSD 4.0 unterstützt ipfw auch Stateful Packet Inspection.\n\n2002 löste ipfw2 mit neu gestaltetem Kern die erste Generation ab. In FreeBSD 4.7 war die zweite Generation erstmals enthalten und fasste auch die separaten Benutzerschnittstellen für IPv4 und IPv6 zusammen.\n\nDie Implementierung von Network Address Translation erfolgte 2005.\n\nSeit FreeBSD 6.2 kann ipfw je nach Bedarf beim Booten als Modul geladen werden und muss nur noch für Network Address Translation dauerhaft bei der Kompilierung des Kernels eingebunden werden.\n\nIpfw war Bestandteil von Mac OS X bis Version 10.9, ersetzt wurde es durch pf.\n\nIn Grundeinstellung unterbindet ipfw jeglichen Netzwerkkontakt. Die zentrale Konfiguration erfolgt in den Dateien /etc/rc.conf und /etc/rc.firewall. Dort kann ipfw aktiviert und eine grundlegende Konfiguration gewählt oder auf eine Datei mit selbst erstellten Regeln verwiesen werden. Alternativ kann auf ein Skript verwiesen werden, das alle Befehle der Benutzerschnittstelle enthalten kann und dadurch vielfältige Möglichkeiten eröffnet. So können auch Präprozessoren aufgerufen werden, um mittels Programmiersprachen wie C Regeln zu erzeugen.\n\nTrotz der vielfältigen Möglichkeiten kann eine Befehlsfolge für eine komplette Konfiguration übersichtlich sein:\n\nDieses Beispiel erlaubt eingehende Verbindungen auf dem Port 80 für das Hypertext Transfer Protocol, alle eingehenden Verbindungen aus einem bestimmten Bereich von IP-Adressen, sowie alle abgehenden Verbindungen, aber keine anderen Verbindungen.\n\nDie Regeln werden bis zur ersten ohne unzutreffende Bedingung durchlaufen, worauf die betreffende Freigabe oder Ablehnung wirksam wird. Eine Konfiguration kann bis zu 65535 Regeln umfassen. Jede Regel erhält eine Nummer, die manuell oder automatisch gesetzt wird und als Sprungmarke dienen kann.\n\nDer erste Paketfilter von Linux war eine Portierung von ipfw. Die ursprüngliche Benutzerschnittstelle wurde später durch ipfwadm ersetzt. ipfw mit ipfwadm wurde anschließend durch ipchains abgelöst und letztlich durch netfilter mit iptables.\n\nwipfw ist eine Portierung von ipfw auf Microsoft Windows NT. Seit Ende 2006 unterstützt sie auch unmittelbar 64-Bit-Architekturen. Einstweilen ist allerdings nur die Unterstützung von Windows NT 5 gegeben und die Unterstützung von Windows NT 6.1 experimentell. Traffic-Shaping und das Ändern von Datenpaketen ist in wipfw nicht implementiert.\n\n"}
{"id": "223785", "url": "https://de.wikipedia.org/wiki?curid=223785", "title": "Farbfilter", "text": "Farbfilter\n\nAls Farbfilter werden etwa Vorsatzfilter für Kameras bezeichnet, die nur eine bestimmte Farbe (Strahlung bestimmter Wellenlänge) passieren lassen oder die eine bestimmte Farbe herausfiltern (viel seltener). Sie werden meistens aus eingefärbtem Glas, Kunststoff oder Gelatinefolien hergestellt.\n\nBei der Beleuchtung finden auch Filterfolien aus Kunststoff oder entsprechende Reflektoren Verwendung.\n\nDer Begriff wird auch für Software zur Bildbearbeitung verwendet, die das Farbspektrum verändern und so den Kontrast erhöhen oder Farbfehler beseitigen können oder das Bild künstlerisch verfremden.\n\nIn der modernen Astrofotografie werden für Farbaufnahmen drei Filter in Rot, Grün und Blau verwendet und die Einzelbilder zum farbigen Bild zusammengesetzt.\n\nIn der Farbfotografie hat insbesondere die Korrektur unerwünschter Farbwiedergabe eine große Bedeutung.\n\nKorrekturfilter sind optische Filter (Gläser, Kunststoffscheiben, Folien), die in den Strahlengang des Linsensystems eines optischen Gerätes eingebaut werden, um unerwünschte Abbildungen zu vermeiden (korrigieren). Soweit sie zur Anpassung der Farbtemperatur der Beleuchtung an das Filmmaterial dienen, spricht man von Konversionsfilter.\n\nDerartige Filter sind insbesondere bei der Fotografie auf Farbfilmen von Bedeutung. Bei der Dia-Fotografie werden die endgültigen Farben bereits zum Zeitpunkt der Aufnahmen festgelegt. Bei der Ausarbeitung zum Papierbild ist zwar eine Farbkorrektur in weiten Grenzen möglich, die Filterung direkt bei der Aufnahme führt jedoch meist zu einer ausgewogeneren Wiedergabe.\n\nIn der Fotografie werden Glasfilter vor das Objektiv geschraubt oder aufgesteckt, bzw. Kunststoffscheiben oder Folien (Gelatine oder auch Kunststoff) in einen Halter gelegt, in folgenden Farben:\n\nsubtraktive Filter:\n\nadditive Filter:\n\nBei digitalen Fotoapparaten (und Videokameras) kann ein elektronischer Farbabgleich durch eine entsprechende Berechnung der Bilddaten vorgenommen werden, den sog. Weißabgleich. Ebenso besteht die Möglichkeit der Farbveränderung durch Nachberechnung am bereits gespeicherten Bild. Dabei sind in einem Rohdaten-Format gespeicherten Aufnahmen flexibler als komprimierte jpeg-Dateien. Grundsätzlich ist jedoch zu erwarten, dass insbesondere extreme Umrechnungen, etwa um außergewöhnliche Farbstiche zu vermeiden, auch zu gewissen Qualitätsverlusten (z. B. durch das Rauschen des Sensors oder Komprimierungseffekte bedingt) führen.\n\nKonversionsfilter dienen der Anpassung der Farbwiedergabe des Films an das umgebende Licht. So lässt sich Tageslichtfarbfilm an verschiedene Kunstlichtarten anpassen, Kunstlichtfilmmaterial an Tageslicht. Es lässt sich auch die Umsetzung der Farbskala auf Schwarz-Weiß-Filme in die jedem Filmtyp eigene Grautonpalette gezielt beeinflussen.\n\nZu den Konversionsfiltern zählt auch der zart rötlich eingefärbte Skylightfilter, der häufig zur Vermeidung eines Blaustichs bei Aufnahmen mit hochstehender Sonne, insbesondere bei hohem Anteil blauen Himmels oder im Schnee eingesetzt wird.\n\nKonversionsfilter gibt es grundsätzlich in zwei Ausprägungen: bläulich gefärbte Typen erhöhen die Farbtemperatur, um beispielsweise mit Tageslichtfilm bei künstlicher Beleuchtung (Glühlampen, Halogenstrahler) zu fotografieren. Umgekehrt dienen rötlich gefärbte Filter dazu, Licht mit zu hoher Farbtemperatur auszugleichen, beispielsweise um Kunstlichtfilm mit Blitzgeräten zu verwenden, aber auch um Motive, die im Schatten liegen, bei Tageslicht korrekt wiederzugeben. Hinzu kommen spezielle Konversionsfilter, die auf die Verwendung von Leuchten mit diskontinuierlichem Spektrum (Leuchtstofflampen) abgestimmt sind.\n\nFür die Bezeichnung der Konversionsfilter haben sich zwei völlig unterschiedliche und inkompatible Systeme etabliert. Weit verbreitet ist die Kennzeichnung nach dem \"Kodak-Wratten-System\", bei dem den einzelnen Typen willkürliche Nummernkombinationen zugeordnet sind. Das zweite System verwendet Zahlenwerte, die die Verschiebung der Farbtemperatur in Dekamired-Stufen angibt, bei blauen Filtern mit einem B, bei roten mit einem R ergänzt.\n\nMit einem einfachen Farbtemperaturmessgerät lässt sich die nötige Stärke und der Typ des Konversionsfilters bestimmen: Zum Beispiel wird man bei Kunstlicht von 3200 Kelvin mit einem Tageslichtfilm ein Konversionsfilter KB15 (bzw. B15, s. o.) benötigen.\n\nSchwieriger ist es bei Licht, das kein kontinuierliches Spektrum wie Tages- oder Glühlampenlicht aufweist, z. B. Leuchtstoffröhren, Dampfdruck- bzw. Gasentladungslampen, o. ä.: Hier benötigt man ein Dreifarbenfarbtemperaturmessgerät, das Rot, Grün und Blau misst. Es zeigt dann, in Abhängigkeit vom Filmtyp Tageslichtfilm oder Kunstlichtfilm, die benötigte Korrekturfilterung an. Diese besteht aus einem KB (Blau) oder KR (Rotorange) Konversionsfilter und einem M (Magenta) oder G (Grün) Korrekturfilter.\n\nAlternativ kann man auf Tabellen des Filmherstellers zurückgreifen, die sich in den Datenblättern zu den professionellen Filmen finden, wo abzulesen ist, welche Filterkombination (meistens aus zwei (oder drei) der additiven Farben R (Rot), G (Grün), B (Blau) und/oder den subtraktiven Y (Yellow/Gelb), M (Magenta), C (Cyan)) zu bestimmten Leuchtentypen anzuwenden ist.\n\nAllerdings kann man sich auf die Messung selbst solch eines teuren Gerätes nicht völlig verlassen, ebenso wenig, wie auf die genannten Tabellen. Um zu einer wirklich guten Farbwiedergabe zu kommen, muss man Versuchsreihen mit verschiedenen Filterungen bei gegebener Beleuchtung durchführen.\n\nWährend die verschiedenen Farbfilter bei Farbfilm die farbrichtige Wiedergabe herstellen sollen, wirkt sich das bei Schwarz-Weiß-Material ganz anders aus. Hier führt der Einsatz der Konversionsfilter zu einer Verschiebung bei der Grautonumsetzung. Die Farbe des Filters wird verstärkt, das heißt, sie wird heller dargestellt, während ihre Komplementärfarbe unterdrückt, das heißt, dunkler dargestellt wird. Mit diesem Effekt lässt sich die Bildgestaltung mit Schwarz-Weiß-Filmmaterial sehr interessant verändern.\n\nDiese Beschreibung bezieht sich auf die „klassische Fotografie“ mit chemisch sensibilisierten Filmmaterialien, die entsprechend auf den Filtereinsatz reagieren.\n\nDie Effekte der Filter für die Schwarz-Weiß-Fotografie können daher wie folgt zusammengefasst werden:\n\n\nDie Verwendung blauer Filtergläser hat in der Schwarzweißfotografie kaum eine Bedeutung, allerdings kann man durch die Verwendung eines blauen Konversionsfilters bei Beleuchtung mit Glühlampen oder Kerzenlicht unter Umständen eine natürlichere Wiedergabe von Hauttönen erreichen sowie Dunst und Nebel etwas verstärken.\n\nEin Sonderfall sind Infrarotfilter, die nahezu alles sichtbare Licht sperren und für das menschliche Auge schwarz oder tief dunkelrot wirken. Mit geeignetem Film oder infrarotempfindlichen Sensoren mancher Digitalkameras lassen sich mit der Infrarotfotografie (nicht zu verwechseln mit Wärmebildkameras) beeindruckende Effekte erzielen. Eine Belichtungsmessung durch das Objektiv ist mit diesen Filtern nicht möglich, aber auch bei Orange- und Rotfiltern kommt es bei TTL-Belichtungsmessungen oft zu erheblichen Abweichungen. Orange- und Rotfilter können mit orthochromatisch sensibilisiertem Schwarzweißfilm nicht eingesetzt werden.\n\n\"Bei der fotografischen Arbeit im Labor\" werden Filter zur Erzielung des gewünschten Farbeindruckes am Papierbild eingesetzt. Ebenso werden Farbfilter zur Kontraststeuerung bei Schwarzweißpapieren mit variabler Gradation verwendet. Auch beim Dunkelkammerlicht bzw. beim Einstelllicht für Schwarz-Weiß-Material werden Fotofilter bzw. deren Wirkung verwendet.\n\nSowohl bei der Fotografie mit konventionellem Film als auch in der Digitalfotografie können Filter eingesetzt werden. In vielen Fällen erlaubt jedoch der digitale Weißabgleich oder die Einstellung der gewünschten Farbtemperatur den Verzicht auf die Filterung bei Digitalkameras.\n\nDer kameraseitige Weißabgleich bei extrem vom Standard-Tageslicht abweichenden Lichtverhältnisse kann jedoch zu erhöhtem Rauschen oder falscher Farbfilterung führen. Insbesondere die Beleuchtung mit (weitgehend) monochromatischem Kunstlicht führt zu einer störenden Irritation der Kameraelektronik bzw. der eingebauten Logik, sodass der Einsatz von Filtern zur Erzielung rauscharmer Bilddaten sinnvoll wird.\n\nZudem lässt die Digitalfotografie eine Filterung in der digitalen Nachbearbeitung zu. Dies gilt insbesondere dann, wenn die rohen Digitaldaten des Sensors in hoher Qualität gespeichert wurden. Durch die Erstellung von Farb-Histogrammen und Erzeugung beliebiger Farbkanäle ist dabei eine gezieltere und teils einfachere Bearbeitung möglich, als es die analoge Bildverarbeitung zulässt.\n\nWird bei einer digitalen Kamera ein Farbfilter vor das Objektiv gesetzt, könnte der automatische Weißabgleich die Farbverschiebung erkennen und (teilweise) kompensieren. In diesem Falle sollte ohne Filter ein manueller Weißabgleich durchgeführt und mit Filter die Automatik deaktiviert werden.\n\nFilter haben durch ihren Komplementärfarbensperreffekt auch eine Lichtreduktion zur Folge, die in der Regel auf dem Filter angegeben ist. Dieser Lichtverlust muss beim fotografischen Einsatz mit berücksichtigt werden. Besonders schwierig wird dies bei Konversionsfiltern im Rottonbereich, da hier die Lichtreduktion nicht statisch ist, sondern je nach Motiv recht stark schwanken kann.\n\nDies gilt insbesondere für die Belichtungsmessung mit externen Geräten (Handbelichtungsmessung) oder auf Grund von Belichtungstabellen. Die Belichtungsmessung durch das Objektiv berücksichtigt grundsätzlich die verminderte Lichtmenge, die Messzelle in der Kamera kann aber durch die geänderte spektrale Zusammensetzung des Lichtes getäuscht werden. Typisch sind beispielsweise Unterbelichtungen bei TTL-Messung mit älteren Kameras in Verbindung mit Orange- oder strengen Rotfiltern.\n\nDies gilt auch für die Digitalfotografie. Werden die Histogramme der Belichtung nur als Summe der Kanäle Rot, Grün und Blau angezeigt, kann bereits bei einem Kanal deutliche Überbelichtung auftreten, ohne dies am Histogramm zu erkennen. Dies gilt auch für die Überbelichtungswarnung mancher Kameras bei der Kontrollwiedergabe des Bildes.\n\nBei der Beleuchtung werden Farbfilter zur Erzielung besonderer Effekte eingesetzt: In der Theaterbeleuchtung und bei Fotografischen Aufnahmen. Hierbei kommen häufig Folienfilter zu Einsatz.\n\nInsbesondere im englischsprachigen Raum haben sich für die Bezeichnung der einzelnen Farben Abkürzungen etabliert, beispielsweise „\"CTO\"“ für „\"Correct To Orange\"“ (wörtlich in Etwa „Farbtemperatur in Richtung Orange korrigieren“), „\"CTB\"“ für „\"Correct To Blue\"“ („Farbtemperatur in Richtung Blau korrigieren“) und so weiter. Gemeint sind hiermit orange (blau, …) Filter, die das Licht entsprechend einfärben und so beispielsweise für ein als wärmer (orange – nicht zu verwechseln mit der tatsächlichen Farbtemperatur) oder kälter (blau) empfundenes Licht sorgen.\nBei der Reproduktion (Vergrößerungsgerät) werden sie zur nachträglichen Korrektur eingesetzt. Durch die Verwendung eines Farbmischkopfes erreicht man dabei eine stufenlose Veränderung der Lichtfarbe.\n\nZur Beurteilung des Bildausschnitts kann bei vielen Vergrößerungsgeräten ein Rotfilter vor das Objektiv geschwenkt werden. Damit wird das eingelegte Fotopapier nicht belichtet, das Bild kann aber trotzdem beurteilt werden.\n\nAuch Dunkelkammerleuchten verwenden in der Regel Farbfilter, um eine Belichtung des Fotomaterials während der Bearbeitung zu vermeiden.\n\nStreifenfilter dienen zur Aufteilung des Farbspektrums vor einem Bildwandler. Sie bestehen aus vertikalen Streifen der Grundfarben Rot, Grün und Blau. Der Bildwandler wird zeilenweise, also quer zu den Streifen, abgetastet. Somit werden die einzelnen Farbintensitäten an jedem Ort des Bildes zeitlich nacheinander übertragen. Streifenfilter werden auch bei der Bildwiedergabe in entsprechender Weise eingesetzt. Dort wird beim Durchgang eines weißen Lichtstrahls durch den Filter mit entsprechend gesteuerter Intensität ein Farbbild erzeugt. Infolge der Streifenanordnung sind Moiré-Störungen unvermeidbar.\n\nMosaikfilter sind eine Weiterentwicklung der Streifenfilter. Durch die versetzte Anordnung der Farbflächen können die Moireeffekte stark gemildert werden. Sie werden in der Digitalfotografie verwendet, um auf einen Strahlteiler verzichten zu können. Dies ist erforderlich, wenn die Herstellungskosten gering gehalten werden sollen (Videokamera für den Konsumbereich) oder die Schnittweite des Objektivs keinen Platz für den Strahlteiler lässt (kompakte Bauweise, Weitwinkelobjektiv höherer Lichtstärke).\n\n\n\n"}
{"id": "225032", "url": "https://de.wikipedia.org/wiki?curid=225032", "title": "Hotfix", "text": "Hotfix\n\nEin Hotfix ist eine Softwareaktualisierung (), die der Hersteller einer Software-Applikation (Programm) bereitstellt, um einen Fehler zu korrigieren. Hotfix leitet sich aus den beiden englischen Wörtern \"hot\" – heiß und \"to fix\" – reparieren ab. Es handelt sich also wörtlich um eine „heiße“ (hier im Sinne von schnelle, eilige) Reparatur.\n\nDabei ist der Fehler so gravierend, dass er schnell und gezielt behoben werden muss. Ein Hotfix enthält daher meist nur die Korrektur für einen oder wenige Fehler. Er sollte auf keinen Fall eine Erweiterung der Funktionalität enthalten, sondern nur den konkreten Fehler korrigieren.\n\nAbgrenzung zu anderen Aktualisierungen ( Service Pack):\n\n\nDurch den letzten Punkt bringt die Installation eines Hotfixes auch stets ein Risiko mit sich. Die Hersteller verweisen meist darauf, dass aufgrund der Dringlichkeit keine vollständigen Tests durchgeführt werden können. Als Computeranwender oder Administrator sollte man daher einen Hotfix nur dann einspielen, wenn man unmittelbar vom Softwarefehler betroffen ist oder wenn das Problem bei allen Systemen auftritt und der Hersteller explizit darauf hinweist, den Hotfix einzuspielen, um einen gravierenden (Sicherheits-)mangel zu beheben.\n\nViele moderne Computerprogramme verbinden sich eigenständig über das Internet mit einem Server ihres Herstellers und prüfen, ob Hotfixes zur Installation zur Verfügung bereitstehen. Falls dies der Fall ist, wird der Hotfix automatisch heruntergeladen und installiert. \n\nBei Windows ab der Version Windows XP ist diese Funktion in das Betriebssystem integriert und sollte laut Microsoft zum Schutz gegen Viren, Würmer und sonstige Internetattacken aktiviert werden. Oft ist es von entscheidender Bedeutung, dass ein Hotfix installiert wird, \"bevor\" man mit Windows ins Internet geht, denn bestimmte Malware, etwa der Wurm Sasser, kann bereits kurz nach dem Verbindungsaufbau das System befallen. Daher kann man wichtige Hotfixes in die Installations-CD per Unattended-Modus integrieren, sodass diese von vornherein im System verankert sind.\n\n"}
{"id": "225611", "url": "https://de.wikipedia.org/wiki?curid=225611", "title": "AMule", "text": "AMule\n\naMule ist eine Abspaltung des Filesharing-Clients eMule für das eDonkey2000-Netzwerkprotokoll. Im Gegensatz zu seinem Vorbild ist aMule leicht auf andere Betriebssysteme zu portieren. Das freie Projekt ging 2003 aus \"xMule\" hervor, dessen Entwickler sich zerstritten hatten.\n\nProjektziel ist eine ebenbürtige Alternative zu eMule, so dass auch Nicht-Windows-Benutzer die Möglichkeiten des eDonkey2000-Netzwerkes voll ausschöpfen können. aMule greift zum Auffinden von anderen Netzwerkteilnehmern ebenfalls auf den Kademlia-Algorithmus zurück. Vorbild und Nachbau sind zueinander gänzlich kompatibel und die grafischen Benutzeroberflächen sind bewusst ähnlich gehalten. In aMule finden sich stellenweise auch Funktionen, die aus eMule-Mods übernommen wurden, welche ebenfalls nur für Windows erhältlich sind.\n\nVom Entwicklerteam werden zusätzlich die Programme \"amuled\" und \"amulecmd\" angeboten. Bei \"amuled\" handelt es sich um eine aMule-Version auf Daemon/Windows-Systemdienst Basis, die einer Benutzeroberfläche entbehrt und somit besonders wenig Ressourcen beansprucht. Sie kann beispielsweise auch auf Servern eingesetzt werden, die normalerweise keine Benutzeroberfläche bieten.\n\nMittels \"amulecmd\" kann \"amuled\" oder auch aMule selbst lokal oder über ein Netzwerk per Kommandozeile gesteuert werden. Die grafische Oberfläche \"amulegui\" kann sowohl für eine lokale als auch für eine entfernte aMule-Installation als Steuerung dienen, man kann so beispielsweise einen auf einem Server installierten Client vom lokalen System aus bedienen, als handelte es sich um eine lokale Installation. Ebenfalls ist die für eMule übliche Fernsteuerungs-Methode per HTTP/Webinterface möglich (\"amuleweb\"). Auf der Homepage des Projektes finden sich neben dem Quelltext auch binäre, kompilierte Versionen von aMule für verschiedene Betriebssysteme.\n\nMittlerweile wird aMule auf der offiziellen Homepage von eMule als Alternative für andere Betriebssysteme empfohlen.\n\nDer Name \"aMule\" steht für „All-Platform Mule“ (engl. für „Maultier für alle Plattformen“) und ist eine Hommage an sein Vorbild \"eMule\" („Electronic-Mule“; „Elektronisches Maultier“). Der Name eMule wiederum war eine Anlehnung an den damaligen \"eDonkey2000-Client\" („Electronic-Donkey-2000“; „Elektronischer Esel 2000“) welchen eMule als Vorbild hatte.\n\n"}
{"id": "228719", "url": "https://de.wikipedia.org/wiki?curid=228719", "title": "Pure Data", "text": "Pure Data\n\nPure Data (Abkürzung: \"Pd\") ist eine datenstromorientierte Programmiersprache und Entwicklungsumgebung, die visuelle Programmierung benutzt. Sie wird vor allem zur Erstellung von interaktiver Multimedia-Software eingesetzt, etwa für Software-Synthesizer in der elektronischen Musik.\n\nEin Programm wird in Pd als \"Patch\" bezeichnet und besteht aus Objekten und den Datenströmen zwischen ihnen. Die Patches werden in einer graphischen Benutzeroberfläche erstellt und sehen Datenflussdiagrammen sehr ähnlich.\n\nBeim Anlegen eines Patches greift man auf die in Pd vorhandenen und vordefinierten Objekttypen zurück. Das geschieht, indem man ein neues Objekt erstellt und diesem den entsprechenden Namen gibt. Grundsätzlich lassen sich die Objekttypen in drei Gruppen einteilen:\n\nDie Ein- und Ausgänge der Objekte werden mit der Maus durch gezeichnete Linien verbunden, die den Weg angeben, den der Datenstrom geht.\n\nMessages sind Befehle bzw. Botschaften, die an ein Pure-Data-Objekt gesendet werden und eine Änderung der Art der Signal- bzw. Datenverarbeitung bewirken. Die Message \"count down\", die an ein Zähler-Objekt gesendet wird, könnte dieses beispielsweise zu einer Umkehr der Zählrichtung veranlassen.\n\nEs gibt in Pd die Möglichkeit, einen ganzen Patch als Objekt innerhalb eines anderen Patches zu verwenden: Jede Patch-Datei, die Pd in seinem Suchpfad auffinden kann, ist automatisch auch als Objekt verfügbar. Im Pd-Jargon heißen solche Patches auch Abstraktionen („abstractions“). Sie entsprechen grob den Funktionen in textbasierten Sprachen wie C oder Python und können wie diese durch die Übergabe von Argumenten unterschiedlich initialisiert werden. Änderungen an der originalen Abstraktions-Datei werden automatisch an alle Vorkommen des Objekts weitergegeben.\n\nIm Unterschied zu Abstraktionen werden Subpatches als Teil ihres Elternpatches gespeichert. Subpatches dienen im Allgemeinen dazu, die Objekte eines Patches sinnvoll zu gruppieren und den begrenzten Bildschirmplatz effektiv zu nutzen. Subpatches können außerdem durch Nachrichten modifiziert werden und spielen eine wichtige Rolle bei der Arbeit mit „Data Structures“ in Pd.\n\nDie Daten in einem Array werden als Graph dargestellt, können durch das Verändern des Graphen manipuliert werden und durch andere Objekte ausgelesen werden.\n\nDie meisten Operationen sind nicht nur auf einem Strom von Zahlen, sondern auch auf Audiosignalen möglich. Deren Verarbeitung erfolgt innerhalb von Pure Data in (üblicherweise 32 bit breiter) Gleitkomma-Darstellung, die verwendete Abtastrate ist frei wählbar, der Defaultwert ist 44.100 Hz.\nWie bei Max/MSP tragen die Objekte für Audiosignale per Übereinkunft den gleichen Namen wie für Operationen auf Zahlen, nur mit einer angehängten Tilde ~.\n\nPure Data wurde in den 1990ern von Miller Puckette entwickelt, um damit interaktive Computermusik zu erzeugen. In seinem Umfang und seinen Zielen ist Pure Data dem ursprünglichen Max sehr ähnlich, das ebenfalls von Puckette entwickelt wurde und der Vorgänger des kommerziellen MSP ist. Im Gegensatz zu Max/MSP handelt es sich bei Pd um freie/Open-Source-Software. Pd besitzt eine aktive Entwickler-Community. Diese traf sich im Herbst 2004 zur ersten internationalen \"pd~convention\" in Graz, auf der Workshops, Performances und Vorträgen für Entwickler und Anwender angeboten wurden. Weitere \"Conventions\" fanden 2007 in Montreal, 2009 in São Paulo und 2011 in Weimar/Berlin statt.\n\nDas Standard-Paket von Pd enthält nur die notwendigen Objekte für das Bearbeiten von seriellen Zahlenströmen (z. B. MIDI-Daten) und Audiosignalen, es gibt jedoch zahlreiche Erweiterungen (Plugins, Bibliotheken), z. B. für Videodaten (PDP \"(Pure Data Packet)\", PiDiP \"(PiDiP Is Definitely In Pieces)\"), Grafikdaten (Gem \"(Graphics Environment for Multimedia)\"), etc.\n\nErweiterungen können sowohl in Pd selbst als auch in einer allgemeinen höheren Programmiersprache wie z. B. C, C++, Python, Ruby geschrieben sein. Die für Echtzeitverarbeitung von Audiosignalen entwickelte Programmiersprache Faust bietet neben anderen Plugin-Formaten auch das Erstellen von Pd-Erweiterungen an.\n\n\nPure Data findet zurzeit vielfache Anwendung in Kunst, Wissenschaft und Lehre, vor allem für interaktive Multimedia-Projekte. Die Möglichkeit, über ein Netzwerk verteilt zu arbeiten und zu interagieren, ist häufig für Künstler interessant.\n\n\n"}
{"id": "228901", "url": "https://de.wikipedia.org/wiki?curid=228901", "title": "AppleWorks", "text": "AppleWorks\n\nAppleWorks war ein integriertes Programmpaket des US-amerikanischen Unternehmens Apple. Es enthielt verschiedene Büroanwendungen, u. a. eine Textverarbeitung, eine Tabellenkalkulation, eine Datenbank, ein Zeichen-, Mal- und Präsentationsmodul sowie ein Kommunikationsmodul. AppleWorks wurde 2007 durch das Programmpaket iWork abgelöst.\n\nUrsprünglich wurde ein Programmpaket 1984 unter diesem Namen für den Apple II auf den Markt gebracht. Es bestand aus einer Textverarbeitung und einer Tabellenkalkulation, die Visicalc ablöste. Die Darstellung war zeichenorientiert. Verschiedene Anbieter, u. a. Beagle Bros Inc. (TimeOut-Module) und PinPoint Software, boten Erweiterungen an, mit denen Appleworks automatisiert werden konnte.\n\nMit der Einführung des Apple IIgs entwickelte die Firma StyleWare eine integrierte Lösung mit graphischer Benutzeroberfläche GS Works, welche Textverarbeitung, Tabellenkalkulation, ein einfaches Malprogramm und Kommunikationswerkzeug beinhaltete. GS Works wurde von Claris aufgekauft und in AppleWorks GS umbenannt. Die unzureichende Codebasis soll der Hauptgrund dafür gewesen sein, dass es mit Ausnahme einer einzigen Fehlerkorrektur nie zu einer Folgeversion kam. Von der Funktionalität her kam AppleWorks GS nicht an die zeichenorientierte Vorgängerversion heran, die fortan unter dem Namen AppleWorks Classic verkauft wurde. Mit dem Verschwinden der Apple-II-Familie wurde schließlich der Name für die Macintosh-Familie wieder frei. Obwohl Apple weder für AppleWorks Classic noch AppleWorks GS großangelegte Werbung betrieb, zählte die Produktlinie zur meistverkauften Software der 80er Jahre.\n\nAppleWorks für den Mac trug ursprünglich den Namen ClarisWorks (nach der damaligen Apple-Tochterfirma Claris). Die erste Version für den Macintosh erschien im Herbst 1991. In den folgenden Jahren kamen mehrere Versionen heraus, teilweise auch für Microsoft Windows, allerdings ohne das Kommunikationsmodul. Nachdem Claris Anfang 1998 aufgelöst wurde, übernahm Apple das Programm direkt und änderte den Namen in AppleWorks. Die Version 6.0 kam im März 2000 heraus, das letzte kleinere Update (Version 6.2.9 in der Version für Mac OS X) im Herbst 2003.\n\nDas Kommunikationsmodul diente zu einer Zeit, als das Internet noch wenig verbreitet war, dazu, mittels Modem oder direkt über serielle Schnittstellen auf Server zuzugreifen. Verschiedene Dateienübertragungsmodi (XModem, ZModem) und Terminalemulationen standen zur Verfügung. Besonders die VT100-Emulation zeigte Treue zum Original (zeichenorientierte Graphik). Wegen des Wegfalls der seriellen Schnittstellen bei den neueren Macs wurde das Kommunikationsmodul ab Version 6 weggelassen.\n\nAm 11. Januar 2005 stellte Apple das Büropaket iWork vor, welches das Präsentationsprogramm Keynote und ein neues Textverarbeitungsprogramm namens Pages enthält. Seit der im August 2007 erschienenen Version iWork '08 ist auch die Tabellenkalkulation Numbers enthalten. Mit der Einführung von iWork '08 teilte Apple seinen Wiederverkäufern mit, dass AppleWorks nun den „End of Life“-Status erreicht habe und nicht mehr weiter verkauft werde. Die Website von AppleWorks wurde mit Hinweis auf den Nachfolger iWork '09 geschlossen. AppleWorks wurde noch bis zur Umstellung von Mac OS X auf Intel-Prozessoren im Jahr 2006 standardmäßig mit den Consumer-Macs (Mac mini, iBook, iMac) ausgeliefert.\n\n"}
{"id": "229917", "url": "https://de.wikipedia.org/wiki?curid=229917", "title": "ISync", "text": "ISync\n\niSync ist ein Personal Information Manager, der zum Betriebssystem macOS (bis zu dessen Version 10.6) des Unternehmens Apple gehörte. Es hat die Aufgabe, Adressen, Termine und ähnliche Daten, die mit den Programmen Adressbuch und iCal erstellt wurden, zwischen (mehreren) verschiedenen digitalen Geräten synchron zu halten. Es arbeitet mit verschiedenen Mobiltelefonen ebenso zusammen wie mit Palm-OS-basierten PDAs.\n\nOffiziell wurden nur Palm-Inc.-Geräte aus der Tungsten- oder Zire-Reihe unterstützt. Praktisch ließen sich allerdings die meisten PDAs mit Palm OS ab Version 4.0 verwenden, teils mit separat zu erwerbenden Plugins.\n\nIn früheren Versionen war iSync auch für die Aktualisierung von Adressbuch- und Kalenderdaten auf dem iPod zuständig. Da jedoch diese Daten auf dem iPod nicht verändert werden können und es sich damit nicht um eine Synchronisierung, sondern lediglich um einen Kopiervorgang handelt, wurde diese Funktion ab iSync 2.3 und iTunes 6 auf das Programm iTunes verlagert. So konnten die Daten auf dem iPod separat aktualisiert werden, ohne gleichzeitig alle anderen Geräte zu aktualisieren.\n\nÜber den kostenpflichtigen MobileMe-Dienst konnten zudem verschiedene Macs (und Windowssysteme mit entsprechender Clientsoftware) über das Internet abgeglichen werden. Dabei konnten noch zusätzliche Daten, wie zum Beispiel Apple-Safari-Lesezeichen und Apple-Mail-Nachrichten, übertragen werden. MobileMe ersetzte am 10. Juli 2008 .Mac.\n\nMit der Veröffentlichung von Mac OS 10.7 wurde die Auslieferung des Programms eingestellt.\n\n"}
{"id": "229974", "url": "https://de.wikipedia.org/wiki?curid=229974", "title": "Linux-Einsatzbereiche", "text": "Linux-Einsatzbereiche\n\nLinux wurde ursprünglich als Kernel für Computer mit einem 386-Prozessor geschrieben. Mit dem wachsenden Erfolg des Programms (Systems) wurden die Einsatzmöglichkeiten erweitert, indem unzählige freie Programme hinzugefügt wurden. Dieser Artikel gibt einen Überblick über die technischen Einsatzmöglichkeiten von Linux. Sowohl Privatpersonen als auch Unternehmen und Öffentliche Einrichtungen können Linux nutzen.\n\nEine anspruchsvolle Computerinstallation ist der PC als Schreibtischgerät. Der Benutzer soll mit ihm arbeiten können, ohne sich des technischen Hintergrunds des Systems bewusst sein zu müssen. Eine typische Installation einer Linux-Distribution enthält einen X11-Grafikserver sowie eine Desktop-Umgebung und wichtige Anwenderprogramme. Dazu gehören sowohl Office-Programme wie OpenOffice.org, als auch Programme zur Bildbearbeitung (häufig GIMP), Browser und E-Mail-Programme. Bei Installationen für Firmen und Büros kommen noch andere Programme wie zum Beispiel zur Unternehmensplanung hinzu. Für Entwickler gibt es Entwicklerwerkzeuge wie Eclipse oder KDevelop.\n\nIn der Praxis wird Linux eher zögerlich im Desktop-Bereich eingesetzt. Die Verbreitung kann wegen der kostenlosen und dezentralen Verfügbarkeit nur schwer abgeschätzt werden. 2002 lief Linux auf 2,8 % aller in diesem Jahr verkauften Rechner (Schätzwert). Im Jahr 2011 lag der Wert ungefähr im 1 %-Bereich, die Quelle nutzt zur Bestimmung des Marktanteils die „user agent information“ des Webbrowsers. Bei den Netbook-Betriebssystemen, einer ursprünglich reinen Linux-Domäne, hat sich der Anteil bei Verkäufen mit vorinstallierten Linux-Betriebssystem mit dem verfügbar werden von Windows XP als Option bis 2009 auf unter 10 % verringert. Von Sommer 2011 bis Anfang 2012 konnte ein starker Anstieg (ca. 40 %) der Zugriffszahlen auf Webseiten durch Linux-Desktop-Systeme verzeichnet werden. Somit belief sich der Marktanteil im Dezember 2011 auf 1,4 %. Ob der Anstieg mit den Chromebooks oder dem 20-jährigen Jubiläum von Linux im Zusammenhang steht ist unklar. Bis zum Jahr 2015 war ein weiterer Anstieg auf rund 1,6 % festzustellen.\n\nDie tatsächlichen Gründe für die geringe Verbreitung sind nicht hinreichend geklärt und werden kontrovers diskutiert. So wird häufig das faktische Monopol des Betriebssystems Windows und die daher notwendige Umgewöhnung der Benutzer auf ein neues System genannt. Als Grund wird häufig genannt, dass das Installieren von Software aus Dritt-Quellen oft schwerer zu handhaben sei als beispielsweise unter Windows. Allerdings trifft diese Kritik auf eine Vielzahl von Nutzungsszenarien gar nicht zu, da die gängigen Distributionen für Desktop-Anwendung alle dafür benötigte Software bereits enthalten.\n\nNicht zwingend aber üblich werden die grafischen Oberflächen auf einem der verfügbaren Fenstersystemen eingesetzt. Als Alternative zum allgemein üblichen X Window System zeichnet sich Wayland ab. Auf dem bekanntesten Linux-Abkömmling Android kommt wiederum eine eigene, nicht X-Window-basierte GUI zum Einsatz.\n\nHeutzutage sind viele übliche Funktionen des Systems über intuitive grafische Benutzeroberflächen erreichbar. Weiterhin wird bei der Weiterentwicklung der direkten Schnittstelle mit dem Nutzer, der Desktop-Umgebung, immer mehr Wert auf eine benutzer- und einsteigerfreundliche Gestaltung gelegt.\n\nDie beiden größten Desktop-Umgebungen für Linux, Gnome und KDE, haben dafür Richtlinien erstellt, die von jedem Programm und jeder Funktion eingehalten werden sollten, um dem Benutzer ein einheitliches Erscheinungsbild und Bedienkonzept (Look and Feel) zu bieten.\n\nDa die Richtlinien beider Desktops voneinander abweichen, erscheinen Programme der einen Umgebung in der anderen Umgebung uneinheitlich. Diesem Problem soll durch Standardisierung und Zusammenarbeit der Projekte begegnet werden. Am bekanntesten ist hier die Initiative freedesktop.org. Auch die Linux Standard Base hat eine eigene Projektgruppe in den späten 1990ern, die LSB Workgroup, ins Leben gerufen. Ziel ist Schaffung verlässlicher Standards für Entwickler von Anwendungsprogrammen und Linux-Distributionen, um eine weitreichendere Kompatibilität zwischen diesen zu erreichen, ein Ziel das noch nicht erreicht ist. Andere Projekte kümmern sich auch um Einzelbereiche, dazu gehört z. B. das Tango!-Projekt, das ein einheitliches Aussehen durch Gestaltungsrichtlinien und die Verwendung einheitlicher Icons (Schaltflächen) zu erreichen versucht.\n\nProjekte wie Xgl oder AIGLX waren experimentelle Versuche X11 zu erweitern um mit Hilfe von Composition-Managern hardwarebeschleunigte 3D-Effekte auf den Desktop zu bringen.\n\nUm die Entwicklung und auch die Verbreitung von Linux auf dem Desktop voranzubringen, hat sich in der Linux Foundation die \"The Desktop Linux Working Group\" gebildet, die alle Kräfte bündeln und koordinieren soll, die sich mit der Thematik beschäftigen.\n\nDie Multimediaunterstützung wird je nach Nutzerbedarf und -verhalten unterschiedlich bewertet. Der Umgang mit gängigen Musik-Formaten ist kein Problem. Allerdings bieten einige Distributionen aus lizenzrechtlichen Gründen von Haus aus keine Möglichkeit, Multimedia-Formate wie MP3 abzuspielen. Die entsprechenden Dekodierverfahren müssen vom Endbenutzer erst nachinstalliert werden. Dies wird sich möglicherweise Ende 2015 ändern, wenn ein wesentliches MP3-Patent in den USA seine Gültigkeit verliert. Zur Wiedergabe stehen eine Reihe leistungsstarker Abspielprogramme wie Amarok oder Rhythmbox bereit.\n\nDas Abspielen und Umkodieren von Videodateien und Videostreams für eine Vielzahl von verbreiteten aber auch ungewöhnlichen Formaten ist unter Linux beispielsweise mit den Programmen MPlayer, Xine und VLC möglich. Diese Programme können auch DVDs abspielen, für das Abspielen mit CSS verschlüsselter Medien ist allerdings die Programmbibliothek libdvdcss nötig, die wegen unklarer Rechtslage in vielen Ländern vom Benutzer selbst nachinstalliert werden muss. Kommerzielle DVD-Abspielsoftware wie PowerDVD existiert, hat aber nie große Bedeutung erlangt. Einzig der Fluendo DVD Player ist direkt durch Endkunden käuflich erwerbbar.\n\nEbenso gibt es auch keine Linux-Version der beiden weit verbreiteten Multimediaprogramme QuickTime Player und Windows Media Player, deren eigene Videoformate aber inzwischen durch Reverse Engineering verstanden und die Unterstützung dafür in die freien Abspielprogramme und teilweise auch in die freie Umkodiersoftware eingeflossen ist. Ähnlich sieht es mit proprietären Audioformaten wie AC3 aus. Auf x86-basierten Systemen können zudem die für MS-Windows geschaffenen originalen Codec-Bibliotheken der Hersteller verwendet werden, sofern kein nativer Codec existiert.\n\nDas Abspielen von DRM-geschützten Audio- und Videodateien ist unter Linux nur in Ausnahmefällen möglich, da diese zumeist an nicht portierte anbieterspezifische Playeranwendungen gekoppelt sind.\n\nEine deutlich andere Situation zeigt sich im Bereich professioneller Multimedia-Bearbeitung. Mit dem JACK Audio Connection Kit steht unter Linux eine spezielle Sound-Architektur zur Verfügung, die besonders niedrige Latenzzeiten bietet. Sie wird von Programmen wie Ardour genutzt. In der Filmbranche erfreut sich Linux besonderer Beliebtheit: die Spezialeffekte vieler Filme wurden mit Hilfe von Linux-Rechnerverbünden gerendert. So hat beispielsweise das häufig unter Linux eingesetzte Programm CinePaint bei der Erstellung von Filmen wie den Harry-Potter-Verfilmungen geholfen.\n\nZwischen diesen verschiedenen Situationen ist der Übergang aber fließend. Mit der zunehmenden Entwicklung proprietärer Lösungen auch für Linux ist aber davon auszugehen, dass die vorhandenen Lücken in naher Zukunft geschlossen werden. Ein Beispiel ist der Bereich des Videoschnitts, bei dem es sowohl proprietäre Lösungen wie das Programm MainActor der Firma MainConcept gibt, als auch Lösungen der Freie-Software-Bewegung wie z. B. die Software Kino oder Cinelerra, das für professionelle Hardware ausgelegt ist.\n\nAls Programmierschnittstelle für hardwarebeschleunigtes Rendering ist OpenGL verfügbar und auch geeignet. Obwohl der Hauptaugenmerk bei der Entwicklung von OpenGL eher CAD-Anwendungen waren, als Computerspiele. Aufgrund dieser Prioritätensetzung galt und gilt OpenGL in einigen Kreisen als den neusten Versionen von Direct3D nicht ganz ebenbürtig. Die proprietären Linux-Treiber der Hersteller implementieren jeweils die neueste Version und sind bezüglich ihrer Leistung mit ihren Windows Pendants vergleichbar. Die verfügbaren freien Implementationen sowohl von Gerätetreibern als auch der OpenGL sowie weiterer APIs werden im Mesa-3D-Projekt entwickelt und hinken, mit einigen Ausnahmen, sowohl bezüglich Leistung als auch bezüglich der Unterstützten API Versionen teils stark hinterher. Es fehlen OpenGL-APIs zur Tonerzeugung sowie eine API für Eingabegeräte, ähnlich XAudio2, sowie verfügbare Softwareimplementationen davon. Der Linux-Kernel ist sehr leistungsfähig, wie seine ubiquitärer Einsatz auf Supercomputern, Servern oder mobilen Geräten eindrucksvoll untermauert, und der Umstand, dass er frei verfügbar ist, ist grundsätzlich als Vorteil anzusehen. Die geringe Verbreitung von Linux auf Heimcomputern sowie die geschilderten Probleme, die sich allesamt lösen ließen, insbesondere vom zahlungskräftigen Spielmarkt, sind als Hauptgrund für die stiefmütterliche Behandlung seitens der Spieleindustrie zu nennen. Trotz allem sind durchaus kommerzielle Spiele für Linux verfügbar, und es gibt außerdem eine Reihe von freien Spielen, siehe Liste quelloffener Computerspiele.\n\nManche Befürworter von Linux als Spieleplattform sind der Ansicht, klassische Spiele wie Quake III Arena oder Unreal Tournament wären nur der Beginn des Umdenkens der Spielehersteller. Spiele wie Doom 3, und andere gibt es von Anfang an auch als Linux-Version, ebenso das Werbespiel America’s Army der US-Armee. Ankündigungen diverser Hersteller lassen auf viele weitere Portierungen hoffen. Einige Entwickler der Linux-Distribution Gentoo haben sich besonders auf den Spielesektor spezialisiert und ein Unternehmen gegründet, das die Portierung von Computer-Spielen anbietet. OpenGL-basierende Spiele, wie z. B. Half-Life, sind mithilfe der Windows-Laufzeitumgebung Wine oft auch unter Linux lauffähig. Spiele, die auf den aktuellen DirectX-Versionen basieren, laufen häufig nicht, weil DirectX unter Linux nicht verfügbar ist. Das Unternehmen Transgaming hat sich des Problems angenommen und mit seinem Wine-Fork Cedega (ehemals WineX), das Ziel gesetzt, weitestgehende Kompatibilität auch zu der jeweils neuesten DirectX-Version herzustellen, womit sich schon heute sehr viele neue Spiele unter Linux spielen lassen. Ein großer Nachteil ist, dass das Programm nicht vollständig quelloffen ist, die CVS-Version ist immer deutlich älter als die nicht-quelloffene Variante und enthält keine Unterstützung für den Kopierschutz der Spieleinstaller und nicht den Installationsmanager „Point2Play“.\n\nNeuerdings setzt auch Valve mit seiner Vertriebsplattform Steam auf Linux. Seit Februar 2013 ist Steam offiziell für die Linux-Distribution Ubuntu erhältlich, und auch auf zahlreichen weiteren Distributionen lauffähig. In diesem Rahmen sind nicht nur etliche independent Games für Linux verfügbar geworden, sondern es werden Blockbuster-Spiele (wie z. B. Left4Dead 2) von Valve auf Linux portiert. Valve hat auch die Weiterentwicklung von SDL auf Version 2 und gleichzeitig den Lizenzwechsel von der LGPL auf die zlib-Lizenz bewirkt. Außerdem umfasst das Engagement von Valve auch eine Zusammenarbeit mit Gerätetreiber-Entwicklern von nvidia und Intel um deren Linux Grafikkartentreiber für aufwendige 3D-Spiele zu optimieren. Valve plant zudem eigene Spielehardware auf Basis handelsüblicher i686-Komponenten, in Zusammenarbeit mit diversen Hardwareherstellern, auf den Markt zu bringen. Das Betriebssystem für diese „Steamboxen“ soll bevorzugt Ubuntu Linux werden.\n\nValve hat im September 2013 SteamOS, ein Linux-basiertes Betriebssystem für Spiele, angekündigt.\n\nMit Linux Game Publishing gibt es nach wie vor zumindest ein aktives Unternehmen, welches Spiele auf Linux portiert.\n\n\nWeltweit erfolgte innerhalb zahlreicher Verwaltungen und Unternehmen die Migration von Microsoft Windows auf das Linux Betriebssystem. Das bedeutet, es kommt eine der zahlreichen Linux-Distributionen samt Fenstersystem und Desktop-Umgebung oder ein eigener Fork zum Einsatz. Weitere Verwaltungen und Unternehmen erwägen die Umstellung ihrer Arbeitsplatzrechner auf das Linux Betriebssystem.\n\nZwei der bekannteren erfolgreichen Beispiele für einen Fork, sind die Stadtverwaltung von München, die viele ihrer Arbeitsplatz-Computer auf LiMux-Projekts, sowie die Gendarmerie nationale, die 72.000 Arbeitsplätze auf GendBuntu umgestellt haben.\n\nEin Beispiel aus dem Industriebereich ist der Auto-Hersteller Citroën, der Anfang des Jahres 2007 20.000 Desktops auf Linux umgestellt hat.\n\nBekannte Schwierigkeiten sind einer mangelhaften Koordination sowie Kommunikation geschuldet, und münden häufig in einer verzögerten Akzeptanz der neuen und fremden Arbeitsoberfläche durch entweder technisch weniger versierte oder unwillige Mitarbeiter. Die fast grenzenlos mögliche Anpassung des gesamten Betriebssystems und insbesondere der Arbeitsoberfläche an die Wünsche der Mitarbeiter kann sich so unnötig in die Länge ziehen, unnötige Kosten verursachen und sogar zum Scheitern der Migration führen. Dies ist insbesondere der Fall, wenn nicht nur das Betriebssystem, sondern auch die zum Einsatz kommende Software gewechselt wird. Die Software wird in der Regel aus dem gleichen Grund gewechselt, aus welchem das Betriebssystem gewechselt wird, also hauptsächlich die Einsparung der Lizenzkosten sowie die Einstellung des Supports für die alte Version. Es kann jedoch auch sein, dass die alte Software vom Hersteller nicht auf Linux portiert wird, und man somit gezwungen ist, mit dem Betriebssystem auch die Software zu wechseln.\n\nSo kann die technische Umstellung der Zusatzsoftware teuer werden, andererseits müssen sich viele Benutzer auch erst an die neue Desktop-Umgebung gewöhnen, was eventuell zeit- und kostenintensiv werden könnte. Ein lohnender Zeitpunkt für eine Umstellung der Firmendesktops ist daher, wenn ohnehin auf ein neues Betriebssystem mit all seinen Neuerungen in der Ablauflogik umgestellt werden muss, da der Hersteller seine alte Version oder den Support dafür aufgekündigt hat. Die Umgewöhnung eines Sachbearbeiters von Windows XP auf Windows 8 ist etwa vergleichbar mit der Umgewöhnung von Windows XP auf den KDE- oder Gnome-Desktop unter Linux.\n\nDa eine Umrüstung auf eine aktuelle Windows-Version sehr oft auch den Kauf neuer Hardware erfordert, setzen viele Institutionen verstärkt auf eine Thin-Client-Lösung mit Linux, bei der die rechenintensiven Aufgaben nicht mehr von den Arbeitsplätzen, sondern von zentralen Servern erledigt werden. Auf diese Weise erspart man sich große Teile eines sonst fälligen Hardware-Aufrüstungen. Wenn die vorhandene Netzwerk-Infrastruktur es außerdem erlaubt die Installation übers Netzwerk vorzunehmen, so verringert dieser Umstand den personellen Aufwand für die Installation von Linux auf sämtlichen Arbeitsrechnern erheblich.\n\nFalls große Teile der Software sowieso bereits über ein Web Interface bereitgestellt werden, die Interaktion also über einen Webbrowser stattfindet, wie zum Beispiel, bei den Arbeitsagenturen, sind die Hürden für eine Migration besonders gering.\n\nEs gibt zahlreiche Distributionen, die gezielt für den Einsatz in Schulen bzw. zur Lernunterstützung entwickelt worden sind. Dabei reicht das Spektrum der enthaltenen Anwendungen von schultypischen Verwaltungsaufgaben über Lernsoftware bis hin zu altersgerechten Internetfiltern. Die Filter sollen Kindern den Zugang zum Internet ermöglichen, ohne sie dabei jugendgefährdenden Inhalten auszusetzen. Ein Augenmerk vieler Entwickler gilt der einfachen Bedienbarkeit. Distributionen mit dem Schwerpunkt Schule und Lernen sind z. B Open School Server, Arktur-Schulserver, KmLinux, Skolelinux, paedML und Seminarix.\n\nAllerdings gibt es auch Beispiele für den Einsatz von Linux in Schulen, bei dem eine standardmäßige Desktopinstallation einer Distribution wie Ubuntu mit ein paar Erweiterungen auch für den Einsatz von heterogenen Netzwerken geeignet ist. Dabei wird die Altersbeschränkung direkt per Konfigurationen umgesetzt.\n\nDa sich Linux in der Betriebssystemarchitektur stark von Windows unterscheidet, ist es nicht direkt möglich, Windows-Programme unter Linux zu betreiben. In diesen Fällen bieten sich verschiedene Alternativen an:\n\n\nApple macOS ist GNU/Linux in vielen Belangen sehr ähnlich. Beide haben große Teile des Systems von BSD übernommen oder basieren direkt auf BSD-Quellcode. Beide Systeme sind weitgehend POSIX-kompatibel, weshalb sie zu einem großen Teil die gleichen Systemwerkzeuge und Systemdienste verwenden. Dazu gehören: rsync, NFS, Samba, Bash, vi, grep, CUPS, syslog oder cron. Da native OS-X-Anwendungen eine von der Firma Apple speziell für OS X entwickelte Programmierschnittstelle verwenden, sind viele OS X-Anwendungen nicht direkt für Linux kompilierbar. Das Projekt GNUstep entwickelt eine zu Apples Cocoa-Schnittstelle kompatible Programmierschnittstelle. Damit ist es möglich, OS X-Anwendungen unter Linux auszuführen.\n\nDas Projekt \"OS2Linux\" stellt Bibliotheken zur Verfügung um OS/2- oder eComStation-Anwendungen einfach auf Linux zu migrieren.\n\nUm unter Linux Barrierefreiheit zu gewährleisten, arbeiten mehrere Projekte an der Thematik. Während die beiden großen Desktops, Gnome und KDE, jeweils eigene Projektgruppen haben, die sich mit der Thematik beschäftigen, gibt es auch Arbeitsgruppen innerhalb der Distributoren oder Gruppen, die Projekt- und Firmenübergreifend arbeiten. Am bekanntesten ist hierbei die \"FSG Accessibility Workgroup\".\n\nDie Arbeit dieser Projekte ermöglicht es unter anderem, unter Linux Braillezeilen zu nutzen, sich aus vielen Programmen Dokumente und Geschriebenes vorlesen zu lassen oder auf dem Bildschirm nur mit Maus oder nur mit speziellen Tasten zu navigieren.\n\nLinux-Kernel basierte Betriebssysteme eignen sich als Plattform für Computerspiele. Entweder kann ein typisches Linux auf dem Desktop durch den Einsatz von entsprechender Software dermaßen erweitert werden, dass es sich zusätzlich für die Entwicklung und das Spielen von Videospielen eignet, oder aber, es kann auch eigen eigene Plattform konzipiert werden, die ausdrücklich diesem Zweck dient. Beispiele sind das Betriebssystem SteamOS oder die Betriebssysteme der Handheld-Konsolen Pandora (Konsole), GP2X, Nintendo 3DS und Neo Geo X. Auf dem Nvidia Shield läuft Android.\n\nAufgrund der Verwandtschaft von Linux mit UNIX hat sich Linux auf dem Servermarkt besonders schnell etabliert. Da für Linux schon früh viel häufig verwendete und benötigte Serversoftware wie Webserver, Datenbankserver und Groupware kostenlos und weitgehend uneingeschränkt zur Verfügung stand, wuchs dort der Marktanteil stetig.\n\nDa Linux als stabil, sicher und einfach zu warten gilt, erfüllt es auch die besonderen Bedingungen, die an ein Server-Betriebssystem gestellt werden. Der modulare Aufbau des Linux-Systems ermöglicht zusätzlich das Betreiben kompakter, dedizierter Server. Außerdem hat die Portierung von Linux auf verschiedenste Hardware-Komponenten dazu geführt, dass Linux alle bekannten Serverarchitekturen unterstützt.\n\nGemessen am Umsatz wurde der Marktanteil von Linux 2005 bei mit Betriebssystem verkauften Servern je nach Studie und Zählweise auf etwa 12 % geschätzt. Das jährliche Wachstum betrug dabei etwa 35 %. Nach Stückzahlen gemessen lag das Wachstum bei 20,5 %.\n\nDieses Wachstum geht teilweise auch auf Kosten traditioneller UNIX-Systeme, die durch Linux abgelöst werden. Die Firmen, die früher ein eigenes UNIX entwickelt und verkauft haben, verkaufen zunehmend Rechner mit Linux und beteiligen sich immer stärker an der Entwicklung von Linux. Der größte Konkurrent für Linux auf dem Servermarkt ist Microsoft Windows, das Studien zufolge 2005 einen Anteil von etwa einem Drittel am Gesamtmarkt hatte.\n\nDie Zählungen der Studien sind aber nur bedingt repräsentativ, da viele Linux-Distributionen auf beliebig vielen Geräten installiert werden können, ohne dass dafür Lizenzgebühren entrichtet werden müssen. So entsteht eine unbekannte Dunkelziffer an Linux-Servern, die von den Studien nicht erfasst werden.\n\nIm Oktober 2012 wurden mindestens 32 % aller Webseiten auf einem Linux-Server gehostet. Da nicht alle Linux-Server sich auch als solche zu erkennen geben, könnte der tatsächliche Anteil um bis zu 24 Prozentpunkte höher liegen. Damit wäre ein tatsächlicher Marktanteil von bis zu 55 % nicht auszuschließen.\n\nEines der wohl bekanntesten Beispiele für eine Linux-Server-Konfiguration ist LAMP. LAMP steht dabei als Abkürzung für den kombinierten Einsatz der Softwareprodukte Linux, Apache, MySQL und PHP (manchmal auch Perl oder Python). Diese Kombination ermöglicht es, auf einem Computer einen Webserver zu betreiben, der beim Aufruf der Seiten mit dem Webbrowser dynamische Inhalte aus Datenbanken zu generieren, und auch Inhalte wieder in diese Datenbank zu schreiben. Ein bekanntes Beispiel für einen solchen Einsatz ist die Software MediaWiki, die auf einem LAMP-System läuft.\n\nEin anderer häufiger Einsatzbereich von Linux ist die Nutzung von Samba, oft auch in Verbindung mit einem LDAP-Verzeichnisdienst. Während der Verzeichnisdienst eine zentrale Anmeldung von Windows- und Linux-Clients ermöglicht, ermöglichen die Fähigkeiten von Samba den Dateiaustausch zwischen Computern mit Linux-Betriebssystemen und Computern mit Windows-Betriebssystemen. So ermöglicht Samba, in gemischten Netzwerken einen Linux-Rechner als zentralen Datei- und Drucker-Server einzusetzen. Dabei werden alle wichtigen Dateien an einem zentralen Punkt gespeichert, und so mehreren Nutzern gleichzeitig zur Verfügung gestellt. Da Samba ebenso wie Linux von seinen Nutzern für seine Stabilität, Performance und Skalierbarkeit gelobt wird, eignet sich die Kombination sehr gut für zentrale und wichtige Knotenpunkte von großen Netzwerken, bei denen eine heterogene Umgebung vorliegt.\n\nAls Beispiel kann das Projekt MigOS des Deutschen Bundestags gelten. Hierbei wurden insgesamt über 100 Server von Windows NT auf Linux umgestellt. Die etwa 5000 Arbeitsplatzrechner (mit Windows) der Abgeordneten und Verwaltungsangestellten wurden über Samba und OpenLDAP eingebunden.\n\nNeben diesen weit verbreiteten Einsatzbereichen gibt es noch eine Reihe weiterer Server-Software, die unter Linux betrieben wird. So wird die Software-Telefonanlage Asterisk häufig als zentrale Schnittstelle in Firmennetzen genutzt. Ebenso werden viele für Netzwerke elementare Dienste auf Linux-Rechnern betrieben. Dazu gehören sowohl DNS-Server als auch Mailserver und Datenbankserver.\n\nViele Server von Online-Spielen, sogenannte Gameserver, werden unter Linux betrieben, selbst dann, wenn das eigentliche Spiel nicht unter Linux zur Verfügung steht.\n\nDie Hardware, auf der Linux als Server betrieben werden kann, ist vielfältig. Da Linux auf eine Vielzahl von Plattformen portiert wurde, kann ein Linux Server ebenso auf handelsüblicher i686-Hardware wie auch auf klassischen Serverarchitekturen, wie Alpha oder SPARC betrieben werden.\n\nEin Beispiel für die Linux-Unterstützung auch modernster Server-Hardware ist der IBM eServer p5. Diese Familie von 64-Bit-Servern basiert auf IBM-Power-CPUs, und gehört zu den Schwergewichten der verfügbaren Server-Hardware. Auf dieser Hardware können bis zu 256 Linux-Installationen parallel betrieben werden.\n\nDer Begriff \"Embedded Linux\" bezieht sich auf den Einsatz von Linux in kleineren Endgeräten für den Massenmarkt wie in Mobiltelefonen, Tabletcomputern oder PDAs, aber auch in kleinen Einplatinencomputern wie dem Raspberry Pi, dem BeagleBone Black, dem Orange Pi, dem phyBoard-WEGA-AM335x, dem Arduino oder Verwandten. Des Weiteren bezieht sich der Einsatz auf \"Embedded Hardware\", wie sie in der Industrie für diverse Zwecke verwendet wird. Vorteil ist dabei, dass jeder Hersteller Linux auf der einen Seite nach eigenen Bedürfnissen verändern kann, auf der anderen Seite aber auch eine sehr aktive Entwickler-Community vorherrscht, auf deren Ressourcen (z. B. umfangreiche Entwickler-Programmen, bereits bestehender Code wie die Benutzeroberflächen, Erfahrung, etc.) die Hersteller dabei zurückgreifen können.\n\nDie meisten Hersteller haben sich für die Entwicklung in verschiedenen Gruppen oder Projekten zusammengeschlossen, die sich meist über die verwendete Hardware oder den Einsatzzweck der Systeme definiert. So existiert auf dem Markt für Mobilfunksysteme die von Google maßgeblich entwickelte Linux-Distribution \"Android\", die seit 2010 Marktführer auf dem Smartphonemarkt ist. So besaß Android von Google im 3. Quartal 2014 einen Marktanteil von 83,1 %, gefolgt von iOS von Apple mit 12,7 %, gefolgt von Windows Phone von Microsoft mit 3 %, gefolgt von Blackberry OS von Blackberry mit 0,8 %, gefolgt von Andere mit 0,4 %. Im gleichen Markt positioniert, aber noch brandneu, ist auch das von Samsung vorangetriebene Betriebssystem \"Tizen\", das hauptsächlich auf ARM-Microprozessorsysteme zugeschnitten ist. Entsprechend der gerade erfolgten Neueinführung erscheint Tizen noch nicht in den Verkaufscharts (Stand: Januar 2015).\n\nTechnisch gesehen werden Smartphones, Tablets und PDAs meist mit spezialisierten stromsparenden Prozessoren und einem Flash-Speicher ausgestattet. Dort wird dann ein angepasstes und kompaktes Linux betrieben. Beispiele für Hardware, auf der heutzutage Linux eingesetzt wird, sind die Motorola Mobiltelefone A728, A760, A768, A780, A910, E680, E895, das Nokia 770 Internet Tablet und der Sharp Zaurus PDA.\n\nWeitere \"Embedded Hardware mit integriertem Linux-Betriebssystem\" findet sich im Bereich SOHO, wo einige Router von Linksys und WLAN-Geräte wie das \"4G Access Cube\" derart ausgestattet sind. Auch in vielen Festplattenrekordern, Satellitenreceivern und DVD-Abspiel- und Aufnahmegeräten findet sich häufig Hardware mit einer angepassten Linux-Variante darin.\n\nDer Begriff „eingebettetes System“ ist ausgesprochen weitläufig und umfasst sehr viele Arten von äußerst unterschiedlichen Geräten. Neben der Nutzung von Linux in verbreiteten Kommunikationsgeräten wird es auch in elektronischen Steuerungen und Geräten der Mess- und Regelungstechnik und im Bereich der µC (Mikrocontroller) eingesetzt.\n\nIm Unterschied zum Embedded Linux wird das System in diesem Fall für technische Spezialanwendungen eingesetzt. Damit entfällt auf der einen Seite der Massenmarkt, auf der anderen Seite besteht zum Beispiel aber auch weniger Bedarf an einer benutzerfreundlichen und einfach gehaltenen Oberfläche.\n\nFür Smartphones und Tablets gibt es speziell optimierte Linux-Distributionen. Sie bieten neben den Telefonie- und SMS-Funktionen, diverse PIM-, Navigations- und Multimedia-Funktionen. Die Bedienung erfolgt typischerweise meist über Multi-Touch oder mit einem Stift. Linux-basierte Smartphonesysteme werden meist von einem Firmenkonsortium oder einer einzelnen Firma entwickelt und unterscheiden sich teilweise sehr stark von den sonst klassischen Desktop-, Embedded- und Server-Distributionen. Anders als im Embedded-Bereich sind Linux-basierte Smartphonesysteme aber nicht auf ein bestimmtes Gerät beschränkt, vielmehr dienen sie als Betriebssystem für Geräte ganz unterschiedlicher Modellreihen und werden oft herstellerübergreifend eingesetzt.\n\nBekannte Smartphone- oder Tablet-Linux-Distributionen sind neben dem sehr weit verbreiten Android auch Firefox OS, Ubuntu for phones, Maemo, Tizen, Mer, Sailfish OS, MeeGo und WebOS.\n\nMobile Linux-Distributionen haben seit Ende 2010 die Marktführerschaft auf dem schnell wachsenden Smartphone-Markt übernommen. Sie wiesen im Juli 2011 einen Marktanteil von mindestens 45 % auf. Vorwiegend Android-Geräte haben Apple iOS, Windows Phone und Symbian OS erfolgreich zurückgedrängt.\n\nLinux gilt innerhalb von Netzwerken als ausgesprochen sicher und an die jeweiligen Gegebenheiten anpassbar. Daher wird es häufig in sicherheitsrelevanten Bereichen verwendet. Beispiele sind die Nutzung von Linux als Gateway, Router oder als Firewall. Vor allen Dingen die Nutzung als Firewall hat sich schon früh verbreitet und führte dazu, dass eine Vielzahl von Linux-Distributionen speziell für die Firewall-Nutzung entwickelt wurden, die zum Beispiel zum Schutz von Bastion-Host-Systemen eingesetzt werden.\n\nMit der freien Verfügbarkeit des Quellcodes und der daraus resultierenden Möglichkeit, das System bestimmten Zwecken anzupassen, hat sich Linux auch in den Anwendungsbereichen von Rechenzentren ausgebreitet. So macht Linux auf Großrechnern, die auf Zuverlässigkeit und hohen Datendurchsatz optimiert sind und häufig in Banken, Versicherungen und großen Unternehmen gefunden werden können, den dort früher häufig installierten speziellen UNIX-Versionen zunehmend Konkurrenz.\n\nEine weitere Anwendung ist im Bereich der Computercluster zu finden, bei dem Linux, häufig im Zusammenhang mit Grid-Computing, auf den einzelnen Computern arbeitet, die dann zu großen Netzwerken zusammengeschlossen werden. Dafür stehen neben speziell angepassten Linux-Distributionen auch besondere Dateisysteme wie z. B. das Global File System zur Verfügung. Häufig wird auch ein Linux-Cluster genutzt, um damit die Hochverfügbarkeit unternehmenskritischer Netzwerk-Infrastrukturen sicherzustellen.\n\nDer wohl prestigeträchtigste Einsatz von Linux ist der in Supercomputern. Diese Computer stellen die Spitze aktueller Hochleistungsrechner dar, und erfahren aus diesem Grund meist besondere Aufmerksamkeit der Presse. Derzeit (Juni 2012) laufen 92 % der 500 weltschnellsten Supercomputer unter Linux. Diese Dominanz hat sich beständig von 70 % (Juni 2006) über 85 % (November 2007) entwickelt.\n\nFindigen Tüftlern gelingt es immer wieder, Linux auch für elektronische Geräte anzupassen, die von Hause aus über eine proprietäre Firmware verfügen. Beispiele hierfür sind Linux auf der d-box 2 (digitaler Fernsehreceiver), iPod Linux (MP3-Player) und Xbox-Linux (Spielekonsole). Motivation hierfür sind häufig ihrer Meinung nach vorhandene Unzulänglichkeiten oder nicht notwendige Einschränkungen der Originalsoftware.\n\n\n"}
{"id": "230362", "url": "https://de.wikipedia.org/wiki?curid=230362", "title": "Advanced Packaging Tool", "text": "Advanced Packaging Tool\n\nDas Advanced Packaging Tool (APT) ist ein Paketverwaltungssystem, das im Bereich des Betriebssystems Debian entstanden ist und dpkg zur eigentlichen Paketverwaltung benutzt.\nZiel ist es, eine einfache Möglichkeit zur Suche, Installation und Aktualisierung von Programmpaketen zur Verfügung zu stellen. APT besteht aus einer Programmbibliothek und mehreren diese Bibliothek nutzenden Kommandozeilen-Programmen, von denen codice_1 und codice_2 zentral sind. Seit Debian 3.1 wird die Benutzung von Aptitude als konsolenbasiertes APT-Frontend empfohlen.\n\nAPT steht auch in OpenSolaris zur Verfügung (es wurde in die Distribution \"Nexenta OS\" aufgenommen) und wurde auch auf macOS portiert, wo es im Rahmen des Fink-Projektes zur Installation von Debian-Softwarepaketen genutzt wird.\n\nIn der Datei codice_3 stehen die sogenannten \"Repositories\", also Quellen für Pakete. Dies können entweder CDs oder DVDs, Verzeichnisse auf der Festplatte oder, öfter, Verzeichnisse auf HTTP- oder FTP-Servern sein.\nBefindet sich das gesuchte Paket auf einem Server (oder einem lokalen Datenträger), so wird dieses automatisch heruntergeladen und installiert.\n\nDie Pakete liegen im Debian-Paketformat (.deb) vor, in dem auch die jeweiligen Abhängigkeiten der Programmpakete untereinander abgelegt sind. So werden automatisch für ein Programm auch eventuell erforderliche Programmbibliotheken mit heruntergeladen und installiert.\n\nAPT setzt auf dpkg auf. APT beschäftigt sich in erster Linie mit der Beschaffung von Paketen, dem Vergleich von verfügbaren Versionen der Pakete und der Verwaltung von Paket-Archiven.\n\n\nWill der Benutzer beispielsweise Gnome installieren:\n\nAußer der oben erwähnten Kommandozeilenschnittstelle gibt es weitere Frontends für die Paketverwaltung, die APT verwenden. Aptitude bietet eine zeichenorientierte Benutzerschnittstelle. Synaptic ist ein GUI-Frontend für die Desktop-Umgebung. Unter KDE stehen außerdem die Programme Adept oder KPackageKit zur Verfügung. Letzteres unterstützt neben APT zusätzlich weitere Systeme wie RPM. Mit der auf der Abstraktionsschicht QApt aufbauenden Muon Package Management Suite sind daneben auch weitere GUI-Frontends in der Entwicklung.\n\nSmart Package Manager kann ebenfalls auf Repositories zugreifen, die für APT erstellt wurden. Er nutzt dabei aber nicht APT, sondern eigene Routinen.\n\nMit Fink und Cydia stehen Frontends für macOS und dessen Derivate zur Verfügung.\n\nObwohl ursprünglich für Debian-Programmpakete geschrieben, wurde APT später auch auf RPM-Systeme portiert. Gustavo Niemeyer führte die Portierung für die Linux-Distribution Conectiva durch und pflegte diese geraume Zeit als Maintainer.\nInzwischen arbeitet er aber hauptsächlich an seinem neuen Projekt Smart Package Manager, und APT-RPM wird von anderen Entwicklern gepflegt und weiterentwickelt.\n\nWer mehrere Rechner mit einer APT-basierten Paketverwaltung im Netz hat, kann sein Netz und die Server, die die Pakete zur Verfügung stellen, erheblich entlasten, indem er eines dieser Programme einsetzt. Im Gegensatz zu weniger spezifischen Proxyservern wie etwa Squid, die nur die in den Übertragungsprotokollen enthaltenen Informationen verwenden können, kennen und verwerten diese Proxys die Paket-Struktur und sichern die Aktualität ihrer gespeicherten Daten entsprechend. Apt-cacher kann so eingestellt werden, dass er sich in regelmäßigen Abständen automatisch aktualisiert.\n\n"}
{"id": "230499", "url": "https://de.wikipedia.org/wiki?curid=230499", "title": "Mac Life", "text": "Mac Life\n\nMac Life ist eine monatlich erscheinende Computerfachzeitschrift des falkemedia-Verlages in Kiel. \n\nThemenschwerpunkt der im Jahre 2000 gegründeten Zeitschrift sind der Mac, das iPhone, sowie das iPad von Apple und deren Betriebssysteme OS X und iOS sowie Software- und Hardwaretests. Zielgruppen sind Einsteiger und Fortgeschrittene, vornehmlich private Nutzer von Mac-, iPhone- und iPad-Nutzer, die mit ihren Geräten effektiv arbeiten, aber auch Spaß haben wollen. Daher beinhaltet das Magazin im hinteren Heftdrittel eine Rubrik mit Lifestyle-Themen und Spielen.\n\nChefredakteur ist seit 2014 Sebastian Schack. Beiträge zum Thema Digitalfotografie werden oft durch Autoren der verlagseigenen Zeitschrift \"DigitalPHOTO\", Artikel zu Sound- und Musikthemen durch Autoren der Zeitschrift \"Beat\" abgedeckt. Am 14. August 2015 wurde bekannt, dass die 12.000 Abonnenten der eingestellten Konkurrenzzeitschrift Macwelt von Mac Life übernommen werden.\n\nIm selben Verlag erschien im Zeitraum von August 2003 bis September 2004 14-täglich eine Zeitung namens \"macnews/paper\", die in Zusammenarbeit mit dem Internetportal \"macnews.de\" gestaltet wurde. Trotz der aktuelleren Berichterstattung sowie des antizyklischen Modus (alle Mac-Zeitschriften erscheinen gegen Monatsende, die macnews/paper jeweils zwei Wochen davor und danach) trug sich das Projekt nicht und wurde eingestellt bzw. die Zeitung ging redaktionell in der Zeitschrift Mac Life auf.\n\nDer falkemedia-Verlag verlegte ebenfalls die Fachzeitschrift \"iPodLOVE\", die sich ausschließlich mit Apples kleinen MP3-Spielern beschäftigt. Da die Nutzung des Namens von Apple angefochten wurde, ist die Zeitschrift ab Sommer 2006 mit neuem Titel erschienen. Seit Anfang September 2006 erscheint viermal jährlich das unabhängige Sondermagazin der Mac Life namens \"iPod & more\", das sich dem iPod widmet. Die iPod & more wurde dann zur \"iPhone & more\", die seit zwischen 2011 und 2014 in \"iPhone Life\" umbenannt wurde und von der \"iPad Life\" in den Jahren 2012 bis 2014 Gesellschaft bekommen hat.\n\nZwischen 2007 und 2015 erschien zusätzlich die Zeitschrift \"MAC easy\", zuerst viermal jährlich, später alle 2 Monate. Sie richtete sich an Einsteiger und Umsteiger von anderen Betriebssystemen.\n\nVon 2008 bis 2010 erschien eine weitere Zusatzzeitschrift, die \"MacGAMES\", die sich an Spieler richtet und die redaktionell von den Spiele-Autoren der Mac Life betreut wird.\n\nSeit 2015 wird die Mac Life von der zweimonatlich erscheinenden \"iPhone & iPad Life\" sowie der Sonderheftreihe \"Mac Life Wissen\" flankiert. Bereits seit 2009 veröffentlicht Falkemedia mit der \"MacBibel\" ein halbjährliches Kompendium, das seit 2010 die \"iPhoneBibel\" zur Seite gestellt bekommt. Die 2012 gestartete \"iPadBibel\" ging 2014 in der iPhoneBibel auf.\n\nIm ersten Quartal 2016 lag die durchschnittliche verbreitete Auflage nach IVW bei 30.692 Exemplaren. Das sind 11.607 Exemplare mehr (+60,82 %) als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 8.337 Abonnenten auf durchschnittlich 15.957 pro Ausgabe zu (+109,41 %). Bedingt durch eine andere Zählweise der von der Macwelt übernommenen Abonnenten, weicht die von der IVW ausgewiesene Abonnenten-Zahl von der des Verlags ab.\n\nAuf der Webseite der Zeitschrift findet primär tagesaktuelle Berichterstattung rund um die Themen der Apple-Welt statt. Analog zum Heft veröffentlicht die Redaktion hier auch Tipps und Tricks für den Umgang mit Apple-Produkten, sowie Tests von Soft- und Hardware, und Workshops.\n\n"}
{"id": "231232", "url": "https://de.wikipedia.org/wiki?curid=231232", "title": "IDVD", "text": "IDVD\n\niDVD ist ein Computerprogramm zum einfachen Erstellen von DVDs und gehörte zum iLife-Paket von Apple. Mit iDVD können Filme oder Foto-Diashows erzeugt werden. Zahlreiche Themes (vorgefertigte Designs) werden mitgeliefert, die der Anwender nur leicht verändern kann. \n\nDas Programm war für Mac OS X erhältlich und im iLife-Paket enthalten (bis iLife 2011). Zum iLife-Paket gehörten weiterhin iMovie, iPhoto, iWeb und GarageBand.\n\nErst ab der Version 6.xx werden Brenner anderer Hersteller unterstützt. Alle vorherigen Versionen setzen ein originales Apple SuperDrive (oder ein kompatibles Gerät) voraus.\n\nDas Programm ist nicht mehr im Mac App Store zu finden und wurde eingestellt.\n\n\n"}
{"id": "231236", "url": "https://de.wikipedia.org/wiki?curid=231236", "title": "IMovie", "text": "IMovie\n\niMovie ist ein nicht-lineares Videoschnittprogramm für die Betriebssysteme macOS und iOS der Firma Apple.\n\nEingeführt wurde iMovie im Jahr 1997 als eine Anwendung für klassische Mac OS 8. Ab 1999 lag es einem neuerworbenen iMac DV bei, der eine FireWire-Schnittstelle hatte. Version 3.x lief ausschließlich unter Mac OS X (heute macOS). Seit iOS 4 sind funktional reduzierte Versionen von iMovie auch für iPhone und iPad erhältlich. Mit dem iPhone 4 kam auch eine iPhone-Version des Programms in den App Store. Mit Einführung des iPad 2 wurde auch dafür eine Version veröffentlicht.\n\nDas Schnittprogramm iMovie ist Teil des sogenannten iLife-Pakets, welches außerdem die Programme iPhoto, iDVD, iWeb und GarageBand enthält und bei jedem Mac mitgeliefert wird. Als Apple den Mac App Store einführte, wurde iMovie ab der Version 9 online auch einzeln verkauft.\n\nIm Oktober 2013 wurde angekündigt, dass iMovie fortan kostenlos aktualisierbar sein werde. Auf iOS-Geräten, die nach dem 1. September 2013 erworben wurden, ist eine kostenlose Installation möglich.\n\nSeit der Programmversion 5 (Januar 2005) können mit dem Programm Videofilme im High-Definition-Format geschnitten werden, weswegen es zeitweilig als „iMovie HD“ bezeichnet wurde. Mit Einführung der neu entwickelten Programmversion 7 im August 2007 (iLife ’08) wurde dieser Namenszusatz wieder weggelassen.\n\niMovie stellt Videoeffekte, wie beispielsweise zur Verfremdung des Bildes, zur Verfügung. Genauere Bildkorrekturen sind durch Drittanbieter-Plugins möglich. Außerdem beinhaltet das Programm viele vorgefertigte Übergangs- und Titeleffekte. Ein für Hobbyfilmer wesentliches Merkmal ist die Möglichkeit, verwackelte Videoaufnahmen nachträglich zu stabilisieren, was mit einem gewissen Beschnitt des Bildes einhergeht.Ab Version 9 (iMovie ’11) ist zusätzlich zu den thematisch gestalteten Übergangseffekten auch eine Funktion zur Anwendung vorgestalteter Kinofilm-Trailer nach Hollywood-Klischee, samt Fantasie-Studiotrailern, Bewegungseffekten und orchestralen Filmmusikstücken eingebaut worden.\n\nDas Programm bietet eine vereinfachte Form des Arbeitens mit der Greenscreen-Technik (iMovie ’09 und ’11), wobei jedoch keine manuelle Anpassung des Grün-Schwellenwertes wie etwa bei fortgeschrittenen Videoschnitt- und Effektprogrammen möglich ist. In iMovie ’11 wurde ersatzweise die Option eingebaut, das letzte Bild eines Greenscreen-Clips als Referenz zur automatischen Anpassung zu verwenden.\n\n\n\n"}
{"id": "232293", "url": "https://de.wikipedia.org/wiki?curid=232293", "title": "Geschichte von Linux", "text": "Geschichte von Linux\n\nDieser Artikel gibt einen detaillierten Überblick über die Geschichte von Linux, einem freien Betriebssystem. Ein allgemeiner und technischer Überblick über die Thematik findet sich im entsprechenden Hauptartikel.\n\nDie Geschichte von Linux ist mit der Geschichte von Unix eng verflochten, aber dennoch nicht gleich. Bis zur Unix-Version 7, die 1979 von AT&T (ursprünglich Bell Laboratories) veröffentlicht wurde, war Unix ein nahezu frei verfügbares System. Bis dahin wurde der Quellcode von Unix gegen Erstattung der Kopier- und Datenträgerkosten an Universitäten und andere Einrichtungen verteilt – Unix hatte damit als eines der ersten Betriebssysteme den Charakter eines freien und portablen Betriebssystems. Der Code wurde auch in Vorlesungen und Veröffentlichungen verwendet und konnte den eigenen Vorstellungen entsprechend geändert, ergänzt oder portiert werden.\n\nDie in den folgenden frühen 1980er Jahren immer mehr aufkommende kommerzielle Denkweise drängte auch AT&T dazu, das gesamte System, bestehend aus eigenem geistigem Eigentum sowie aus frei beigesteuerten Erweiterungen, als proprietäres AT&T-Unix zu vermarkten. Infolgedessen durfte nun auch der AT&T-Quellcode nicht mehr öffentlich zugänglich gemacht werden. Dies sorgte insbesondere im universitären Umfeld für Unverständnis, woraufhin Richard Stallman im September 1983 das GNU-Projekt ankündigte und im Januar 1984 startete, nachdem er seine Anstellung beim MIT-AI gekündigt hatte. Ziel des Projekts war es, ein Unix-ähnliches, POSIX-kompatibles Betriebssystem zu schaffen. 1985 gründete er die \"Free Software Foundation (FSF)\" und schrieb die GPL (GNU General Public License), um freie Software innerhalb des amerikanischen Copyright-Systems zu ermöglichen. Mittlerweile gibt es weitere Lizenzen mit ähnlichen Ansätzen (z. B. OAL) sowie mehrere Abwandlungen und Erweiterungen der GPL (z. B. LGPL).\n\nAuf diesem Wege verbreitete sich die GNU-Software sehr schnell und wurde von vielen Menschen weiterentwickelt. Es entstand in kurzer Zeit eine Vielzahl von Programmen, so dass bereits Anfang 1990 genug GNU-Software bereitstand, um ein eigenes Betriebssystem daraus zu erstellen. Allerdings fehlte noch immer ein Kernel. Dieser sollte eigentlich im Projekt GNU Hurd entwickelt werden. Doch der als Mikrokernel ausgelegte Kernel entwickelte sich nur sehr schleppend, weil das Finden und Beheben von Fehlern (Debuggen) aufgrund technischer Besonderheiten sehr schwierig und aufwendig war.\n\nEin anderes Projekt rund um ein Betriebssystem aus freier Software war in den 1980er Jahren auch die Berkeley Software Distribution, kurz BSD. Diese hatte sich aus Eigenentwicklungen der Universität Berkeley aus den Unix-Versionen der 4er-Edition und folgender von AT&T entwickelt. Da aber in den BSD-Versionen noch immer Code von AT&Ts Unix enthalten war, kam es Anfang der 1990er Jahre zu einem Rechtsstreit zwischen AT&T und der Universität Berkeley, der die Entwicklung von BSD stark einschränkte und einige Jahre stark verlangsamte.\n\nAnfang der 1990er gab es also kein vollständiges, freies Betriebssystem. Die Zukunft von BSD war wegen des Rechtsstreits ungewiss, die Weiterentwicklung gelähmt, das GNU-Projekt wurde zwar konstant weiterentwickelt und ausgebaut, verfügte aber über keinen Unix-artigen Kernel, vielmehr war es eine Anzahl freier Softwareprojekte, die auf den verschiedensten (proprietären) Unix-Varianten mittels des GNU-Compilers übersetzt werden konnten und lauffähig waren.\n\n1991 begann Linus Torvalds in Helsinki mit der Entwicklung des Kernels, der später Linux genannt wurde. Anfänglich war es eine Terminalemulation, die Torvalds zum Zugriff auf die großen Unix-Server der Universität benutzte. Er schrieb das Programm hardwarenah und unabhängig von einem Betriebssystem, weil er die Funktionen seines neuen PCs mit einem Prozessor des Typs 80386, dessen x86-Befehlssatz auch heute noch zum Standard zählt, optimal nutzen wollte. Als Grundlage dienten dabei das Minix-System und der GNU-C-Compiler.\n\nIrgendwann, so Torvalds in seinem Buch \"Just for Fun\", merkte er, dass er eigentlich ein Betriebssystem geschrieben hatte. Am 25. August 1991 kündigte er in einem Usenet-Posting an die Gruppe \"comp.os.minix\" dieses System an. Dieses Usenet-Posting wird an vielen Stellen immer wieder zitiert und dürfte zu den bekanntesten Postings im Usenet zählen:\nAm 17. September 1991 wurde Linux in der Version 0.01 das erste Mal öffentlich auf einem FTP-Server zur Verfügung gestellt.\n\nEigentlich sollte Linux nach dem Willen von Linus Torvalds \"Freax\" heißen, eine Wortschöpfung aus \"Freak\" (Verrückter, aber auch jemand, der sich für etwas begeistert), \"Free\" für Freie Software und dem oftmals üblichen \"x\" in Anspielung auf die Ähnlichkeit zu Unix. Aus diesem Grund hatte Torvalds zu Beginn seiner Arbeit an dem System etwa ein halbes Jahr lang die Dateien unter Freax abgelegt. Auch den Namen Linux hatte sich Torvalds bereits überlegt, er erschien ihm aber zu egozentrisch.\nUm anderen Leuten die Möglichkeit zu geben, am System mitzuarbeiten oder Verbesserungsvorschläge zu machen, sollten die Dateien im September 1991 auf dem FTP-Server (ftp.funet.fi) der Helsinki University of Technology (HUT) abgelegt werden. Der damalige Verantwortliche für den Server, Ari Lemmke (Mitarbeiter am HUT), war mit dem Namen \"Freax\" nicht einverstanden, er bevorzugte den Arbeitsnamen Linux. Ohne mit Torvalds darüber zu diskutieren, nannte er den Bereich am Server einfach Linux, was Torvalds schließlich akzeptierte, um große Diskussionen zu vermeiden und auch, wie Torvalds zugibt, weil Linux einfach der bessere Name war. Im Quellcode der Version 0.01 von Linux kam noch der Name \"Freax\" vor („Makefile for the FREAX-kernel“), später wurde nur noch der Name Linux verwendet. So setzte sich der eigentlich gar nicht geplante Name Linux weltweit durch.\n\nTorvalds gab Linux zuerst unter einer eigenen, proprietären Lizenz heraus, entschied sich aber später dafür, die GNU GPL den übrigen Urhebern vorzuschlagen. Im Änderungsprotokoll zur Version 0.12 im Januar 1992 kündigte er die Lizenzänderung an. Die Mitte Dezember 1992 veröffentlichte Version 0.99 ist die erste Version unter der GNU GPL.\n\nDieser Schritt erst machte es möglich, Linux als freies Betriebssystem zu vertreiben. Dieses Ereignis zog weltweit viele Programmierer an, die sich an der Entwicklung von Linux und GNU beteiligten. Später sagte Linus Torvalds in einem Interview, dass die Entscheidung, Linux unter die GNU GPL zu stellen, die beste gewesen sei, die er je getroffen habe: \"„Making Linux GPL'd was definitely the best thing I ever did.“\"\n\nAuch begannen einige Leute aus Neugier oder aus praktischen Gründen, sich mit Linux zu beschäftigen. Im Vergleich zu den kostspieligen, exklusiv-verfügbaren Unix-Workstations wie beispielsweise einer DECstation in Universitäts-Laboren oder Firmen hatte man nun die Möglichkeit, auch auf wesentlich günstigerer PC-Hardware ein *nix-artiges Test- und Programmiersystem einzurichten. Dabei holte man sich zumeist über ans Internet angeschlossene Universitätsrechner via binärem FTP die Installationsdateien und beispielsweise in LaTeX gesetzte, frei verfügbare Handbücher (aus anderen Quellen, aber auch unter GNU GPL-Lizenz). Die Dateien wurden dann auf ganze Diskettenstapel (Installationssatz mit bis zu 30 gepackten 5,25-Zoll-HD-Disketten mit je 1,2 MB) übertragen. Die PostScript-Umwandlung der Dokumentation konnte auf einem Postscript-fähigen Drucker ausgedruckt werden. Man sieht hieran die damalige Vorgehensweise, das System zu beschaffen, vorzubereiten und zu installieren. Da es aufwendiger als heute war, traf man noch eine recht unbedeutende Anzahl von Linux-Anwendern und diese eher in entsprechenden Hochschul-Fachbereichen bzw. im beruflichen Umfeld an. Doch der GPL-Schritt war eine bedeutende Initialzündung zur weiteren Verbreitung.\n\nDie Bezeichnung \"Linux\" wurde von Torvalds anfänglich nur für den von ihm initiierten Kernel genutzt. Der Kernel wurde aber häufig zusammen mit anderer Software, vor allem der des GNU-Projekts, ausgeliefert. Diese GNU-Variante wurde schnell zur meist genutzten Variante von GNU. Als im Laufe der Zeit der Name \"Linux\" auch häufig für diese Softwaresammlungen genutzt wurde, versuchte der Gründer des GNU-Projekts, Richard Stallman, bald, den Namen \"GNU/Linux\" durchzusetzen, um der Rolle von GNU eine in seinen Augen angemessene Geltung zu verschaffen.\nIm Juni 1994 wurde im \"GNU’s Bulletin\" mit den Worten „freier UNIX-Klon“ auf Linux verwiesen und im selben Jahr gab das Debian-Projekt seiner GNU/Linux-Distribution den Namen \"GNU/Linux\". In der Januar-Ausgabe 1995 des \"GNU’s Bulletin\" änderten sich die Verweise auf Linux zu \"GNU/Linux\".\nIm Mai 1996 gab Richard Stallman den Editor Emacs 19.31 heraus, in dem der Systemtyp von \"Linux\" nach \"Lignux\" umbenannt wurde. Er meinte, es wäre angemessen, die Begriffe \"Linux-based GNU system\", \"GNU/Linux system\" oder \"Lignux\" zu benutzen, um auf die Kombination von Linux-Kernel und GNU-Software hinzuweisen. Er gab jedoch bald den Ausdruck \"Lignux\" auf und benutzte nur noch \"GNU/Linux\".\n\nInsgesamt stieß die Forderung auf unterschiedliche Reaktionen. Während das GNU-Projekt und das Debian-Projekt den Namen annahmen, lehnten die meisten Entwickler und andere Linux-Distributoren dies ab oder widersetzten sich deutlich. Begründet wurde dies einerseits mit Bequemlichkeit, weil der Name Linux als einfacher angesehen wurde, und andererseits mit dem Hinweis, dass mittlerweile eine beachtliche Menge der mit Linux ausgelieferten Software nicht aus dem GNU-Projekt stamme.\n\nEin Grund für das Ausbleiben des Begriffs „GNU/Linux“ ist sicherlich, dass „Linux“ der deutlich einfachere, griffigere Begriff ist. Ein weiterer Grund für die weit verbreitete Nutzung des Begriffs „Linux“ für das System ist wohl, dass Linus Torvalds es seit der Veröffentlichung 1992 schon immer \"Linux\" genannt hatte. Stallman hingegen meldete seine Forderung nach Namensänderung erst an, nachdem das System bereits populär geworden war.\n\nIm Jahre 1996 kündigte Torvalds ein Maskottchen für Linux an, es sollte ein Pinguin werden. Die Bedingungen, die an das Maskottchen gestellt wurden, finden sich unter anderem in Torvalds Biografie \"Just For Fun\":\n\nLarry Ewing erstellte daraufhin den ursprünglichen Entwurf des heute bekannten Maskottchens. Den Namen \"Tux\" schlug James Hughes als Ableitung von Torvalds UniX vor. Ein weiterer Grund für diese Konstruktion ist vermutlich auch, dass die Farben der Pinguine den Eindruck vermitteln, als würden sie einen Smoking tragen, der im Englischen \"tuxedo\" heißt.\n\nAls Linux-Kernel-Betreuer sind neben Torvalds auch Alan Cox und Marcelo Tosatti sehr bekannt. Cox betreute bis Ende 2003 die Kernel-Reihe 2.2, Tosatti kümmerte sich bis Mitte 2006 um die Version 2.4 und Andrew Morton steuerte die Entwicklung und Verwaltung des neuen 2.6-Kernels, welcher am 18. Dezember 2003 in einer als stabil (\"stable\") vorliegenden Version veröffentlicht wurde. Auch die älteren Zweige werden nach wie vor ständig verbessert.\n\nDer Erfolg von Linux in vielen Einsatzbereichen ist insbesondere auf die Eigenschaften freier Software bezüglich Stabilität, Sicherheit, Erweiterbarkeit und Wartbarkeit, aber auch auf die entfallenden Lizenzkosten zurückzuführen.\n\nMit den grafischen Benutzeroberflächen wie KDE oder Gnome bietet Linux im Bereich der Desktops mittlerweile einen vergleichbaren Komfort zu Microsoft Windows oder Mac OS. Umfangreiche Tests der Umgebungen auf Benutzerfreundlichkeit und Effizienz ermöglichen eine Bedienung des Computers ohne besondere Kenntnisse. Techniken wie Xgl oder AIGLX ermöglichen darüber hinaus hardwarebeschleunigte, grafische Effekte auf dem Desktop.\n\nNeben dem wachsenden Angebot proprietärer Software für Linux hat vor allen Dingen die Community das Softwareangebot für Linux stetig vergrößert und in unterschiedlichste Bereiche ausgedehnt: Mit der Zeit sind immer mehr freie Softwareprojekte entstanden, die von Entwicklungsumgebungen über Businessanwendungen bis hin zu komplexen Multimediaanwendungen reichen. Die Windows-API-Nachbildung \"Wine\" erlaubt es außerdem, mit einer stetig steigenden Anzahl von für Windows geschriebenen Programmen auch unter Linux zu arbeiten.\n\nDie auf den Desktop ausgelegten Distributionen lassen sich einfach installieren, es werden aber auch zunehmend Komplettrechner mit vorinstalliertem Linux ausgeliefert, was der Verbreitung als Einzelplatzsystem Vorschub leistet. Im Bereich mit Masseninstallationen wie in Unternehmen oder Behörden hat Linux durch groß angelegte Migrationen z. B. in München oder Wien von sich reden gemacht. Der Erfolg eines Desktopsystems wird aber auch durch die Verbreitung von Spielen entschieden. Einige neue Spiele der großen Spielehersteller kommen auch in Linuxversionen heraus, so stehen beispielsweise auch \"id Softwares\" grafiklastige Spiele Doom 3 sowie die Teile 1 bis 4 der Quake-Reihe für Linux zur Verfügung.\n\nDer größte Teil der Arbeit an und um Linux wird durch die Community, also durch freiwillige Mitarbeiter auf der ganzen Welt, erledigt. Diese teilweise auch von Unternehmen unterstützten oder direkt angestellten Programmierer und Entwickler helfen nicht nur direkt bei der Entwicklung des Kernels, sondern auch beim Schreiben der gesamten Zusatzsoftware, die für und rund um Linux zur Verfügung steht.\n\nDabei gibt es sowohl die vollständig frei und selbstorganisierten Projekte wie Debian, aber auch die mit Unternehmen direkt verbundenen Projekte wie Fedora und openSUSE. Die Mitglieder der jeweiligen Projekte treffen bei verschiedenen Konferenzen und Messen zusammen, um sich auszutauschen. Eine der größten Messen ist dabei der LinuxTag, bei dem jährlich etwa 10.000 Menschen zusammenkommen, um sich über Linux und die darum angesiedelten Projekte zu informieren und auszutauschen.\n\nDie Linux Foundation ist ein Zusammenschluss der Open Source Development Labs (OSDL) und der \"Free Standards Group\". Sie ist eine unabhängige und gemeinnützige Organisation, die das Ziel verfolgt, die Entwicklung von Linux zu fördern und zu unterstützen. Sie dient als gesponserte Arbeitsstelle für Linus Torvalds und lange Zeit auch für Andrew Morton, der aber Mitte 2006 zu Google wechselte und in dessen Auftrag seitdem am Linux-Kernel arbeitet. Torvalds kümmert sich im Auftrag des OSDL in Vollzeit um die Entwicklung des Linux-Kernels. Finanziert wird die nichtkommerzielle Einrichtung von namhaften Unternehmen wie Red Hat, Novell, AMD, Intel, IBM, Dell und HP.\n\nMittlerweile verdienen eine Reihe von Unternehmen mit Linux Geld. Diese Unternehmen, von denen die meisten auch Mitglieder der \"Linux Foundation\" sind, investieren teilweise erhebliche Ressourcen in die Weiterentwicklung und den Ausbau von Linux, um es für verschiedene Einsatzbereiche tauglich zu machen. Dies reicht von Hardwarespenden an Entwickler über Treiber und Geldspenden für Stiftungen, die sich mit Linux-Software beschäftigen, bis hin zur Anstellung von Programmierern beim Unternehmen selbst. Bekannte Beispiele dafür sind IBM und HP, die Linux vor allen Dingen auf den eigenen Servern einsetzen, aber auch Red Hat, das eine eigene Distribution unterhält. Ebenso unterstützt Qt Development Frameworks Linux durch die Entwicklung und die GPL-Lizenzierung von Qt, was die Entwicklung von KDE erst möglich macht, und durch die Förderung einiger X- und KDE-Entwickler.\n\nSeit Beginn der Entwicklung gab es immer wieder Streit um das System.\n\nIm Jahre 1992 kam es durch einen Usenet-Artikel Andrew S. Tanenbaums in der Newsgroup \"comp.os.minix\" mit dem Titel \"Linux is obsolete\" zu einer berühmt gewordenen Debatte um die Struktur des Linux-Kernels, in dem der anerkannte Informatiker und Autor des Mikrokernel-Systems Minix \"Tanenbaum\" eine ganze Reihe von Kritikpunkten an dem damals noch recht jungen Linux-Projekt anbrachte. Vor allem kritisierte er\n\nRückblickend kann man heute sagen, dass Tanenbaum mit seiner Prognose, Linux sei innerhalb weniger Jahre veraltet und durch ein (aus seiner Sicht) modernes GNU Hurd ersetzt, falsch lag. Linux ist auch auf alle wichtigen Plattformen portiert worden. Das liberale Entwicklungsmodell hat zu einer beispiellosen Geschwindigkeit bei der Weiterentwicklung geführt; GNU Hurd hingegen ist 2018 noch immer nicht für den stabilen Produktionseinsatz geeignet.\n\nJahre später wurde Andrew Tanenbaum erneut mit Linux in Verbindung gebracht. Als Kenneth Brown sein 2004 veröffentlichtes Buch \"Samizdat\" schrieb und deshalb mit Tanenbaum sprach, erklärte dieser, Torvalds habe nicht von ihm abgeschrieben. In seiner Stellungnahme zu Brown schrieb er einen Abschnitt, der sein Verhältnis zu Linux gut dokumentiert: Natürlich habe Torvalds sein Buch und Minix gekannt.\n\nObwohl es Torvalds nach eigener Aussage nicht interessierte, ob Microsoft (unter anderem Hersteller des Betriebssystems Windows) durch Linux in der Vergangenheit in Bedrängnis geriet (1997–2001), wurde von beiden Seiten ein harter Konkurrenzkampf ausgetragen. Das erste Mal äußerte sich dies deutlich, als Ende Oktober 1998 das erste Halloween-Dokument von Eric S. Raymond an die Öffentlichkeit gebracht wurde. Dieses von einem Microsoft-Entwickler verfasste Dokument beschäftigt sich ausführlich mit den Gefahren freier Software für Microsoft und zeigt Strategien auf, diesen zu begegnen. Die Free Software Foundation distanzierte sich von der dadurch ausgelösten Verachtung, die sich speziell auf Microsoft bezog, und erinnerte die Community daran, dass \"jeder\" Produzent proprietärer Software den Software-Anwendern schade.\n\nAnfang 2004 erreichte der Konkurrenzkampf eine neue Phase, als Microsoft eine Reihe von in Auftrag gegebenen Studien zum Thema „Windows vs. Linux“ unter dem Namen \"Get the Facts\" auf einer eigenen Website veröffentlichte. Die Studien sollten anhand von Umfragen, Erhebungen und Untersuchungen nachweisen, dass sich der Betrieb von Linux auf Servern verglichen mit Windows nachteilig auswirkt.\n\nDie kommerziellen Anbieter von Linux-Software bemühten sich daraufhin, ebenfalls durch Studien, Umfragen und Erfahrungsberichte, Microsofts Kampagne etwas entgegenzustellen. So hat Novell Ende 2004 eine eigene Website unter dem Titel \"Die reine Wahrheit\" geschaltet, auf der die Vorteile als auch die rechtliche Sicherheit von Linux hervorgehoben werden. Bemerkenswert dabei ist, dass Novell sich bei vielen Behauptungen explizit auf die von Microsoft veröffentlichten Studien bezieht. Auch IBM veröffentlichte eine Reihe von Studien unter dem Kampagnentitel \"The Linux at IBM competitive advantage\", um auf die von Microsoft initiierte Kampagne zu antworten. Red Hat hingegen startete die Kampagne „Truth Happens“, die darauf abzielt, im Gegensatz zu Microsoft die Produkte nicht mit Studien zu bewerben, sondern die Leistungsfähigkeit der Produkte selbst entscheiden zu lassen.\n\nDie meisten Mitglieder der Linux-Community nahmen die Thematik aber gelassen und stichelten mit Witzen wie „Linux – und dein PC macht nie wieder blau“ (vom Bluescreen) oder „Früher oder später migrieren wir euch“. Unter anderem veröffentlichte auch das Magazin LinuxUser ein nicht ganz ernst gemeintes Review von Windows XP unter den Kritikpunkten einer typischen Linux-Distribution.\n\nIm Herbst 2006 kündigten Novell und Microsoft aber an, künftig bei den Themen Interoperabilität und Patentschutz zusammenarbeiten zu wollen. Im Rahmen der Virtualisierung wurde vereinbart, den Austausch von Office-Dokumenten zu verbessern, die Virtualisierung der Enterprise-Lösungen jeweils unter dem Konkurrenz-Produkt zu vereinfachen sowie die Eingliederung von Linux- und Windows-Maschinen in eine gemeinsame Verzeichnisstruktur zu vereinfachen. Der Patentschutz sah gleichzeitig vor, dass Kunden eines Anbieters für die Nutzung dessen Software vom jeweils anderen Anbieter nicht wegen Patentverletzung verklagt werden dürfen. Dieser Patentschutz wurde auch auf nicht-kommerzielle Freie-Software-Entwickler ausgedehnt. Gerade der letzte Schritt erntete auch Kritik, da er nur nicht-kommerzielle Entwickler mit einschloss.\n\nMicrosofts Hypervisor Hyper-V unterstützt offiziell die Distributionen Red Hat und SuSE als Gastsysteme. Außerdem sind die \"Integrationskomponenten\" im Linux-Kernel enthalten.\n\nIm Jahre 2003 erhob SCO schwere Vorwürfe gegen den Weltkonzern IBM: Laut der Darstellung von SCO haben IBMs Linuxentwickler Code unverändert aus Unix übernommen und in Linux eingepflegt. Da SCO für sich die Urheberrechte an UNIX beanspruchte und in dem Verhalten von IBM eine Verletzung der eigenen Rechte sah, wurde eine Klage gegen IBM angestrengt. Gleichzeitig verkaufte SCO seit dem Beginn des Verfahrens Linux-Lizenzen an Nutzer, die keine mögliche Klage von Seiten SCOs riskieren wollten. Allerdings hat ein Geschworenengericht inzwischen entschieden, dass Novell rechtmäßige Eigentümerin des Unix-Copyrights ist.\n\nMehrere Personen in verschiedenen Ländern hatten 1994 und 1995 versucht, den Namen \"Linux\" als Markennamen eintragen zu lassen. Daraufhin ergingen an mehrere Linux-Unternehmen Aufforderungen zu Lizenzzahlungen, womit viele Entwickler und Anhänger des Linux-Systems nicht einverstanden waren. Linus Torvalds ging mit Hilfe von Linux International gegen diese Eintragungen vor und bekam die Markenrechte der Marke \"Linux\" zugeteilt. Diese übergab Torvalds an Linux International. Später übernahm die dafür gegründete, nicht gewinnorientierte Organisation \"Linux Mark Institute\" die Verwaltung der Marke. Im Jahre 2000 legte Linus Torvalds die Grundregeln für die Vergabe der Lizenzen fest. Diese besagen, dass jeder, der ein Produkt oder eine Dienstleistung mit dem Namen \"Linux\" anbietet, eine Lizenz dafür besitzen muss, welche durch einen einmaligen Kauf erlangt werden kann. Ausnahmen bilden dabei nicht-kommerzielle Verwendungen, die eine kostenlose Lizenz erhalten können oder keine benötigen.\n\nIm Juni 2005 kam ein neuer Streit um die Lizenzgebühren für die Benutzung des geschützten Markennamens \"Linux\" auf, weil das Linux Mark Institute, welches Linus Torvalds Rechte vertritt, Preise von 5000 Dollar statt bislang 500 Dollar für die Verwendung des Namens angekündigt hatte. Begründet wurde der Schritt mit den gestiegenen Kosten für die Durchsetzung der Rechte am Markennamen.\n\nIn der Community sorgte diese Erhöhung für Unmut und Missverständnisse, weshalb sich Linus Torvalds am 21. August 2005 selbst zu der Thematik zu Wort meldete, um die Wogen zu glätten und die Missverständnisse aufzulösen. In einer E-Mail erläuterte er ausführlich die aktuelle Situation sowie die Hintergründe und ging auch auf die Frage ein, wer Lizenzkosten zahlen müsse:\n\n\n\n\n\n\n"}
{"id": "232737", "url": "https://de.wikipedia.org/wiki?curid=232737", "title": "SOFFEX", "text": "SOFFEX\n\nDie SOFFEX (Swiss Options and Financial Futures Exchange) war die Derivate-Handelsplattform der Schweiz, die ihre Geschäftstätigkeit im Frühjahr 1988 als erste vollelektronische Terminbörse der Welt mit integriertem Clearinghaus aufgenommen hatte. Im Jahre 1998 fusionierte sie mit der DTB (Deutschen Terminbörse) zur Eurex, der größten Derivathandelsplattform der Welt.\n"}
{"id": "232919", "url": "https://de.wikipedia.org/wiki?curid=232919", "title": "Bauerneinheit", "text": "Bauerneinheit\n\nDie Bauerneinheit ist ein System zur Quantifizierung des Wertes von Schachfiguren. Als grundlegende Vergleichsgröße wird der Wert eines Bauern gleich 1 gesetzt und die Werte der anderen Schachfiguren relativ dazu angegeben. Die Bauerneinheit dient Schachspielern und Schachcomputern zur Stellungsbewertung und zum Abschätzen der Folgen eines Zuges.\n\nKlassisch werden die Figuren folgendermaßen bewertet:\n\nDer König besitzt keinen Wert in Bauerneinheiten, da er nicht geschlagen werden kann. In Endspielsituationen, wo keine unmittelbare Mattgefahr besteht und der König aktiv wird, liegt sein Kampfwert meist zwischen dem einer Leichtfigur (Läufer oder Springer) und dem eines Turmes.\n\nAus den angegebenen Zahlen folgt, dass bei ungleichen Materialverhältnissen üblicherweise zum Beispiel zwei Türme stärker sind als eine Dame oder zwei Leichtfiguren in der Regel stärker sind als ein Turm. Läufer und Springer werden als ungefähr gleich gewertet. Der Läufer kann auf größere Entfernungen wirken und beherrscht im Schnitt mehr Felder als der Springer, doch er ist an seine Feldfarbe gebunden. Da beide Läufer zusammen diesen Nachteil umgehen, ist das Läuferpaar in Mittel- und Endspiel jedoch meist stärker als zwei Springer.\n\nDa zwei Läufer stärker als zwei Springer sein können, zeigt sich, dass die Bauerneinheit nur als Faustregel zu verstehen ist, die bei bestimmten Stellungen ungenau wird. So kann es durchaus sinnvoll sein, eine höherwertige Figur für eine Figur mit geringerem Bauernwert zu opfern, wenn man dadurch einen Stellungsvorteil erreicht.\n\nFür den Mehrwert eines Turmes gegenüber einer Leichtfigur ist die Bezeichnung \"Qualität\" gebräuchlich. Der Tausch einer eigenen Leichtfigur gegen einen gegnerischen Turm wird als \"Qualitätsgewinn\" bezeichnet, für den Gegner dagegen als \"Qualitätsverlust\". Ein \"Qualitätsopfer\" ist der bewusste Tausch eines Turmes gegen eine Leichtfigur. Damit verbunden ist die Notwendigkeit, als Ausgleich andere Vorteile zu erringen. Dies kann zum Beispiel ein Tempogewinn sein oder ein Stellungsvorteil.\n\nUmgangssprachlich verwenden Schachspieler des Öfteren die Verballhornung \"Qualle\" für Qualität. Die traditionsreiche Vereinszeitung des Schach-Clubs Kreuzberg e. V. in Berlin nennt sich beispielsweise \"Kreuzqualle\".\n\nDa die Stellung eine Rolle für die Bewertung der Figuren spielt, folgt, dass sich die Werte auch leicht im Verlauf des Spiels verändern. Cecil Purdy veranschlagte für die Leichtfiguren 3,5 Bauerneinheiten in der Eröffnung und im Mittelspiel, aber nur noch 3,0 im Endspiel. Weitere Beispiele sind:\n\n\nDie Bauerneinheit wird auch im Computerschach genutzt. Moderne Schachprogramme rechnen allerdings mit genaueren Werten, die zur schnelleren Berechnung mit Hundert multipliziert werden. Außerdem berücksichtigen sie die Stellung und verändern so je nach Stellung die einzelnen Werte, sie ziehen also etwas vom Bauernwert einer Figur ab, wenn diese ungünstig steht. Auf diese Weise können die Programme sich für einen bestmöglichen Zug entscheiden und auch feststellen, wie sich der Wert der Stellung der beiden Spieler nach einem Zug verändern würde. Eine möglichst genaue Kalibrierung dieser Werte, meist anhand von Ergebnissen in Stellungstests, gehört zu den wichtigen Aufgaben eines Schachprogramms.\n\nInsbesondere für Schachprogramme hat es in der Schachliteratur mehrere Versuche gegeben, die klassischen Werte zu präzisieren. Bereits im Handbuch des Schachspiels von 1852 wurde betont, dass der aktuelle Wert einer bestimmten Figur stark von deren Postierung und Zugmöglichkeiten abhängt. Auch Emanuel Lasker schrieb in seinem \"Lehrbuch des Schachspiels\", dass Vergleichswerte zwischen den Figuren nur ceteris paribus gelten.\n\nSo wurden beispielsweise folgende Anpassungen an die klassischen Werte vorgeschlagen, wobei Larry Kaufman für ein Läuferpaar einen Bonus von 0,5 Bauerneinheiten hinzurechnet:\n"}
{"id": "233455", "url": "https://de.wikipedia.org/wiki?curid=233455", "title": "Debian-Paket", "text": "Debian-Paket\n\nDebian-Pakete (Dateiendung: codice_1) dienen der Softwareinstallation auf Debian-basierten Betriebssystemen. Sie enthalten die zu installierenden Programme in komprimierter Form. Das Paketformat wurde von Ian Murdock entwickelt, die Abkürzung \"Deb\" leitet sich hierbei aus dem ersten Teil des Namens der Distribution \"Debian\", der sich wiederum von den Namen „Debra“ (der Vorname Murdocks damaliger Frau) und „Ian“ ableitet.\n\nEine Debian-Paketdatei besteht einmal aus einem ar-Archiv, das wiederum mit gzip, bzip2 oder LZMA komprimierte tar-Archive enthält. Diese enthalten die eigentlichen Programmdateien sowie Metainformationen wie Versionsinformationen des enthaltenen Programms und Abhängigkeiten zu weiteren Paketen, welches dieses Programm zum Laufen benötigt oder die Funktion verbessern.\n\nUm diese Informationen verwenden und verwalten zu können, konzipierte Murdock ebenfalls den \"Debian Package Manager\" (kurz dpkg). Darauf wiederum baut APT (Kommandozeilenprogramm) auf, bzw. dessen Frontends (z. B. aptitude oder Synaptic), welches auch weitere für das Programmpaket nötige Pakete automatisch installiert.\n\nDie Debian-Repositorys enthalten neben diesen Binärpaketen auch Quelltextpakete.\n\nBinärpakete können von den Werkzeugen der Debian-Paketverwaltung direkt installiert werden.\n\nDas Auspacken des Debianpakets und die Ausführung diverser Skripte und Helferprogramme geschieht mit den Rechten des Systemadministrators, genauso wie bei den Paketverwaltungen anderer Betriebssysteme.\n\nEinige Pakete sind auch als udeb-Pakete erhältlich. Diese werden meist dazu benutzt, ein minimales Debiansystem für die Installation zu laden. Sie bieten nur einen Bruchteil der Funktionen eines normalen deb-Paketes und werden nur vom Debian-Installer verwendet, nicht in einem installierten Debiansystem.\n\nJedes Binärpaket besteht aus drei Dateien, die mittels des UNIX-Kommandos ar oder dem debianspezifischen Kommando \"dpkg-deb\" entpackt werden können (z. B. \"ar x datei.deb\"):\n\nQuelltextpakete enthalten unkompilierte Programmdaten, den Quelltext der Programme. Sie bestehen aus einem Archiv mit den originären Quelltexten (Endung codice_2), einer (komprimierten) diff-Datei mit Debian-spezifischen Modifikationen (codice_3) und einer Beschreibungsdatei (codice_4).\n\nQuellpakete können nicht direkt installiert werden. Es müssen zuerst die Quelltexte kompiliert und dann eventuell ein installierbares Binärpaket daraus erstellt werden.\n\nDer Dateiname eines binären Debian-Paketes folgt einem festgelegten Schema: Dem Namen der Software (eventuell mit Präfixen wie z. B. \"lib\" für Programmbibliotheken oder Postfixen wie z. B. \"-doc\" für Dokumentationen oder \"-dev\" für entwicklungsspezifische Pakete), einem Unterstrich, der Versionsnummer der Software, eventuell einem Bindestrich und einer Debian-internen Revisionsnummer, dann einem weiteren Unterstrich, einem Kürzel für die Prozessorarchitektur (codice_5 für plattformunabhängige) und der Dateinamenserweiterung codice_1 (z. B. codice_7).\n\nDebian-Pakete sind nicht mit RPM-Paketen kompatibel, können aber mit Hilfe des Programms alien innerhalb einer Architektur (z. B. PowerPC oder x86-basierte Systeme) konvertiert werden. Einige Dateimanager, wie z. B. Konqueror, Midnight Commander oder 7-Zip, erlauben das Anzeigen des Paketinhalts und der Kontrollinformation, ohne das Paket zu installieren.\n\nMittels debhelper können Quellpakete in Binärpakete übersetzt werden.\n\nAufgrund der einfachen Portierbarkeit von APT und dpkg hat das Debian-Paketformat auch in vielen anderen Betriebssystemen Einzug gehalten.\n\n\n"}
{"id": "233648", "url": "https://de.wikipedia.org/wiki?curid=233648", "title": "Betriebsdatenerfassung", "text": "Betriebsdatenerfassung\n\nBetriebsdatenerfassung (BDE) ist ein Sammelbegriff für die Erfassung von Istdaten über Zustände und Prozesse in Betrieben.\n\nDiese Daten können in Plant Information Management Systeme, Manufacturing Execution Systems bzw. SCADA-Systeme eingebunden sein.\n\nBei der Betriebsdatenerfassung können verschiedene Arten von Betriebsdaten unterschieden werden.\n\nDie Datenerfassung erfolgt zentral über Bildschirmarbeitsplätze an zentralen PPS-Systemen, dezentral z. B. über Bildschirmarbeitsplätze an dezentralen BDE-Systemen oder Leitständen, über Datenerfassungsterminals oder durch direkte Maschinendatenerfassung. Häufig erfolgt die Identifizierung des \"BDE-Objekts\", z. B. eines Arbeitsvorgangs, für den eine BDE-Meldung erfasst werden soll, mit Barcode-Unterstützung. Neben diesen modernen Methoden besteht auch ein Papieransatz, der mit Hilfe von Schreibgegenständen Schriftzeichen oder auch primitive Symbole (beispielsweise Zählstriche) auf das Papier überträgt.\n\nDie BDE ist hierarchisch aufgebaut. Sie besteht aus BDE-Maschinen-, BDE-Bereichsterminals, BDE-Gruppenrechnern, die durch BDE-Interfaces an einen Leitrechner angebunden sind.\n"}
{"id": "234726", "url": "https://de.wikipedia.org/wiki?curid=234726", "title": "Autostart", "text": "Autostart\n\nAutostart ist die unter Microsoft Windows übliche Bezeichnung für das automatische Starten von Programmen beim Booten des Systems oder beim Anmelden. Ein solches Programm muss also nicht von einem Benutzer gestartet werden, sondern startet automatisch ohne dessen Zutun. Autorun ist dagegen die Funktion von Betriebssystemen, beim Einlegen oder Einstecken von auswechselbaren Datenträgern eine bestimmte Aktion zu starten.\n\nDas Kofferwort setzt sich aus den Wörtern \"Automatisch\" und \"Starten\" zusammen.\n\nBeim Systemstart (oder genauer: nach der Benutzeranmeldung) werden alle Dateien, die sich im Startmenü-Ordner \"Autostart\" befinden, automatisch ausgeführt. Bei den echten Mehrbenutzersystemen Windows NT, Windows 2000, Windows XP und den neueren Windows-Versionen gibt es diese Autostart-Ordner mindestens zweimal: einmal für jeden Benutzer, den dieser selbst verwalten kann, und einmal für alle Benutzer (\"All Users\"), den in der Regel nur Administratoren des Systems oder das System selbst (z. B. innerhalb einer Programminstallation) verwalten können. In der Regel befinden sich in diesem Ordner nur Verknüpfungen, es können jedoch auch alle anderen Dateien ausgeführt bzw. geöffnet werden. Daneben gibt es Stellen, die tief im Betriebssystem verwurzelt sind und die auch die Funktion des Autostarts besitzen. Ein Beispiel hierfür sind bestimmte Positionen der Registry, wie unter anderem die Schlüssel codice_1 oder codice_2.\n\nEs werden alle Programme geladen, die sich in den folgenden Registry-Schlüsseln befinden:\n\nDie Befehle in den Pfaden dieser Schlüssel werden für alle Benutzer gestartet. Eine Ausnahme bildet der abgesicherte Modus, normalerweise werden hier keine dieser Schlüssel beachtet.\n\n\nDie Befehle in den Pfaden dieser Schlüssel werden nur für den aktuellen Benutzer gestartet. Eine Ausnahme bildet der abgesicherte Modus.\n\nDes Weiteren ist es möglich, dass sich Prozesse als Dienst (HKLM\\SYSTEM\\CurrentControlSet\\Services\\) installieren und ebenfalls bei jedem Systemstart im Hintergrund starten, dem Benutzer allerdings meist verborgen bleiben. Fast alle Viren und Schadprogramme schreiben sich selbsttätig in die Registry in einen dieser Schlüssel oder installieren sich als Dienst, um sicherzustellen, dass sie bei jedem Systemstart ausgeführt werden.\n\nDas automatische Starten von Programmen beim Booten kann unter Microsoft Windows durch Halten der Umschalttaste während des Bootvorgangs verhindert werden. Bei älteren Windows-Versionen ist der Autostart auch in der codice_3-Datei durch die Einträge codice_4 oder codice_5 möglich.\n\nSpezielle Software zum Bearbeiten von Autostart-Einträgen bietet meist eine automatische Sicherung an. Es gibt auf dem Markt eine ganze Reihe an kostenloser und kostenpflichtiger Software, die diese Arbeit beherrschen. Ab Windows 8 könnte für einige Nutzer eine solche zusätzliche Software unnötig werden. Das Betriebssystem bietet selbst eine neue und benutzerfreundlichere Möglichkeit zur Verwaltung der Autostart-Einträge.\n\nAutostart-Einträge lassen sich mit dem Programm msconfig bearbeiten, einem Systemtool von Microsoft Windows. Auch das kostenlosen Programm AutoRuns von Windows Sysinternals kann die Programme, die sich (meist automatisch) in den Autostart eingetragen haben, deaktivieren und das Booten dadurch beschleunigen. Autoruns bietet im Vergleich zu msconfig umfangreichere Konfigurationsmöglichkeiten.\n\nUnter MS-DOS erfüllt die Batchdatei codice_6 diesen Zweck. Alle Kommandos, die hier eingetragen sind, werden beim Booten automatisch ausgeführt.\n\nUnter unixartigen Betriebssystemen (z. B. Linux) verläuft die Ausführung von Startscripten in Abhängigkeit von der verwendeten Distribution und den Vorlieben des Administrators. Üblich ist der Start von Diensten über Runlevels, die wiederum andere aufrufen. Durch solche Rekursionen ist es möglich, ein System vollautomatisch in einen gewünschten Zustand zu versetzen, ohne zur Laufzeit Befehle eingeben zu müssen. Diese Startscripte werden meist in gesonderten Verzeichnissen innerhalb von codice_7 gelagert und je nach gefordertem Runlevel ausgeführt oder beendet.\nBenutzerspezifische Autostart-Scripte können beispielsweise von der Login-Shell oder vom Fenstermanager aus gestartet werden. Die klassischen Unix-Shells, die per Befehlszeile bedient werden, kennen jeweils einen bestimmten Dateinamen wie codice_8 oder codice_9, unter dem sie im Benutzerverzeichnis des jeweiligen Benutzers eine Shellskript-Datei suchen. Existiert die Datei beim Start der Shell, wird sie ausgeführt.\n\nUnter Mac OS X wurden bis Version 10.3 sogenannte \"StartUp Items\" verwendet, um den Zweck eines Autostarts zu erfüllen. Ab Version 10.4. sind die \"Launch Services\" hierfür zuständig.\n\nBei AmigaOS werden alle Programme im Ordner \"WBStartup\" ausgeführt. Der Tooltype \"STARTPRI\" legt dabei die Reihenfolge fest.\n\nBei CP/M 2.2 gibt es einen \"default command buffer\", eine einzelne Kommandozeile, die beim Systemstart ausgeführt wird. Sollen mehrere Befehle ausgeführt werden, so kann das mittels codice_10-Befehl bewerkstelligt werden, z. B. codice_11, wobei der Name der Autostart-Datei im Rahmen der CP/M-Dateinamenkonventionen frei wählbar ist. \n\nBei CP/M 3 bzw. CP/M Plus gibt es eine der codice_12 entsprechende Datei namens codice_13, die beim Systemstart ausgeführt wird.\n\n"}
{"id": "235005", "url": "https://de.wikipedia.org/wiki?curid=235005", "title": "Apple II-Modelle", "text": "Apple II-Modelle\n\n1976: Apple I (Bausatz, 8-Bit-Datenbus MOS 6502)\n\n\n"}
{"id": "235121", "url": "https://de.wikipedia.org/wiki?curid=235121", "title": "Einzelplatzcomputer", "text": "Einzelplatzcomputer\n\nEinzelplatzcomputer \"(veraltet)\" sind PCs, die keine Verbindung zu anderen Computern haben. Diese als Insellösung betriebenen PCs sind also nicht vernetzt, d. h. nicht an Kommunikationsnetzen angeschlossen.\n\nAls Einzelplatzcomputer oder -rechner kann man auch die deutsche Übersetzung des \"Personal Computer\" bezeichnen. Es haben sich jedoch hier eher Begriffe wie Heimcomputer und Arbeitsplatzrechner eingebürgert. Bei dieser Verwendung ist jedoch nicht eine Nutzung im Netzwerk verneint, sondern es soll der Gegensatz zu traditionellen, bis in die 70er Jahre weit verbreiteten, Bildschirmsystemen (englisch Terminal) betont werden, die gemeinsam an einen Großrechner angeschlossen sind und keine bis wenig eigene Rechenleistung bieten.\n"}
{"id": "236993", "url": "https://de.wikipedia.org/wiki?curid=236993", "title": "Paralleladdierer mit Übertragsvorausberechnung", "text": "Paralleladdierer mit Übertragsvorausberechnung\n\nDer Paralleladdierer mit Übertragsvorausberechnung bzw. Carry-Look-Ahead-Addierer (kurz: CLA-Addierer) ist eine logische Schaltung zur Addition mehrstelliger Binärzahlen (siehe auch Addierwerk).\n\nDer CLA-Addierer addiert zwei n-stellige Binärzahlen, verfügt also über 2·n Eingänge, sowie in der Regel über einen weiteren Übertragseingang. Da das Ergebnis einen etwaigen Übertrag enthalten kann, gibt es n+1 Ausgänge.\nDer Vorteil des CLA-Addierers ist, dass die Verzögerung der Schaltung nur logarithmisch zur Zahl seiner Eingänge ist, bei zugleich nur linearer Zahl an Logikgattern gemessen an der Zahl seiner Eingänge. Seine Komplexität beträgt in der Landau-Notation ausgedrückt also formula_1 für die Schaltungsverzögerung und formula_2 für die Schaltungsgröße. Der CLA-Addierer ist also ähnlich schnell wie ein Conditional-Sum-Addierer, dessen Verzögerung ebenfalls formula_1 beträgt, und braucht zugleich ähnlich einem Carry-Ripple-Addierer nur formula_2 wenige Bauteile. Conditional-Sum-Addierer brauchen im Vergleich mit dem CLA-Addierer jedoch formula_5 mehr Bauteile, Carry-Ripple-Addierer weisen eine exponentiell größere Verzögerung von formula_2 auf. Der CLA-Addierer ist dagegen asymptotisch schnell und günstig zugleich.\n\nEin Addierwerk kann einen Großteil seiner Berechnungen auch dann durchführen, wenn der eingehende Übertrag noch nicht vorliegt. Dazu werden die beiden Summanden zunächst ohne Berücksichtigung desselben addiert. Am Ergebnis kann dann direkt abgelesen werden, welche Wirkung der eingehende Übertrag auf den ausgehenden haben wird. Die untenstehende Tabelle stellt den Zusammenhang am Beispiel eines 4-Bit Addierers dar. Sobald der Übertrag dann tatsächlich eingeht, kann er durch ein einfaches Gatter absorbiert, propagiert oder generiert werden. Die Gatterlaufzeit zwischen ein- und ausgehenden Überträgen ist dabei unabhängig von der Bitbreite des Addierwerks. Auf dieser Eigenschaft fußt der Geschwindigkeitsvorteil des Carry-Look-Ahead-Addierers.\n\nDer CLA-Addierer ist eine spezielle Anwendung einer Parallelen Präfix Berechnung formula_7 welche sich durch eine Schaltung mit Kosten formula_2 und Verzögerung formula_9 implementieren lässt. Um die raffinierte Anwendung der Parallelen Präfix Berechnung leichter verständlich zu machen, wird zunächst ihre Anwendung am Beispiel eines schnellen Inkrementers dargelegt.\n\nEin Inkrementer formula_10 addiert zu einer formula_11-stelligen Binärzahl den Wert formula_12 und hat formula_11 Eingänge sowie formula_11 Ausgänge und einen weiteren Ausgang für einen etwaigen Übertrag beim höchsten Stellenwert.\n\nformula_15\n\nformula_16\n\nEin Übertrag von Stelle formula_17 zu formula_18 tritt dabei nur dann auf, wenn alle formula_19 sind, d. h. wenn die formula_20 den Übertrag \"propagieren\". Daher gilt beim Inkrementer für jedes Ergebnisbit formula_21 genau dann, wenn entweder formula_20 propagieren oder formula_23 für formula_24.\n\nMittels einer Parallelen Präfix Berechnung formula_25 kann man für alle formula_17 die Funktionen „formula_27 propagieren“ formula_28 zugleich berechnen, indem man ausnutzt, dass die logische UND Funktion formula_29 eine assoziative zweistellige Verknüpfung auf den binären Zahlen ist.\n\nZu jeder assoziativen zweistelligen Verknüpfung formula_30 auf einer Menge formula_31 ist ihre formula_11-stellige Parallele Präfix Funktion formula_7 wie folgt definiert:\n\nformula_34\n\nformula_35 mit formula_36 für formula_24\n\nAls Schaltung lässt sich formula_38 rekursiv aus formula_39 konstruieren:\n\nformula_40\n\nFür formula_41 sei formula_42 dann gilt:\n\nformula_43 für formula_44\n\nformula_45 für formula_46\n\nformula_47\n\nBeispiel: für formula_48 gilt folglich formula_49\nSeien formula_50 und formula_51 die Ziffern der beiden zu addierenden Zahlen und formula_52 der Eingangsübertrag. Mit formula_53 bezeichnet man das Übertragsbit von Stelle formula_17 zu Stelle formula_18. Dann gilt für das formula_17-te zu berechnende Summenbit formula_57 formula_58 formula_59. Sofern alle Übertragsbits formula_53 bekannt sind, lassen sich die formula_61 parallel berechnen, mit konstanter Schaltungsverzögerung und linearen Bauteilkosten.\n\nUm die formula_53 zu berechnen, reicht es nicht wie beim Inkrementer allein zu prüfen, ob der Eingangsübertrag propagiert wird. Denn ein Übertrag wird an der formula_17-ten Stelle \"propagiert\", wenn entweder formula_64 oder formula_65 sind, weiterhin wird ein Übertrag \"generiert\", wenn formula_66.\n\nMan schreibt formula_67 falls die formula_17-te Stelle einen Übertrag propagiert:\n\nformula_69 für formula_44\n\nWeiter schreibt man formula_71 falls die formula_17-te Stelle einen Übertrag generiert:\n\nformula_73 für formula_44\n\nSowohl Propagieren als auch Generieren lassen sich ohne Kenntnis der Überträge formula_75 berechnen!\n\nUm alle Überträge formula_53 für formula_24 zugleich effizient zu berechnen, definiert man eine assoziative Verknüpfung (Beweis Assoziativität durch Nachrechnen) formula_78 die man in einer parallelen Präfix-Berechnung einsetzen kann:\n\nformula_79\n\nDie beiden Komponenten erklären sich wie folgt. Es ist der Übertrag formula_80, wenn die formula_17-te Stelle generiert oder wenn die formula_17-te Stelle propagiert und die formula_83-te Stelle einen Übertrag hat, also wenn formula_84. Aufeinander folgende Stellen formula_85 propagieren gemeinsam einen Übertrag, wenn formula_86 ist. Die Verknüpfung formula_87 eignet sich daher, um alle formula_53 wie folgt zu berechnen; die formula_89 sind dabei reine Hilfsvariablen:\n\nformula_90. oder anders ausgedrückt:\n\nformula_91\n\nMit den nun vorliegenden Zwischenergebnissen lässt sich schließlich die Summe formula_92 von formula_93 und formula_94 einfach berechnen. Es gilt:\n\nformula_95 für formula_24\n\nformula_97\n\n\n"}
{"id": "237039", "url": "https://de.wikipedia.org/wiki?curid=237039", "title": "GarageBand", "text": "GarageBand\n\nGarageBand ist eine Anwendungssoftware von Apple zur Musikproduktion. Sie ist Bestandteil der iLife-Programmpalette und für die Anwendung zu Hause gedacht.\n\nDas Programm ermöglicht es dem Benutzer, schnell und einfach eigene Stücke zu produzieren. Dazu stehen im Wesentlichen drei verschiedene Komponenten zur Verfügung: Von Apple wird bereits eine große Sammlung von fertigen Audio- und MIDI-Loops mitgeliefert. Mit Hilfe eines Audio-Interfaces können echte Instrumente (z. B. E-Gitarre oder Bass) oder Gesang aufgenommen werden. Teilweise sind hier noch Vorverstärker erforderlich. Schließlich können mit Hilfe eines MIDI-Keyboards auch MIDI-Daten eingespielt werden, die von Software-Instrumenten in Klang umgesetzt werden. Dabei können sowohl die mitgelieferten Instrumente als auch auf dem Computer installierte Audio Units verwendet werden.\n\nAnders als vergleichbare professionelle Programme besitzt GarageBand keine Sequencer-Funktion, d. h. die Ausgabe von MIDI-Signalen zur Steuerung externer Klangerzeuger ist nicht – oder nur behelfsmäßig über ein Freeware-Plugin – möglich.\n\nVorgestellt wurde GarageBand erstmals am 6. Januar 2004 auf der Macworld Conference & Expo. Die Entwicklung von GarageBand wurde durch den Kauf der deutschen Firma Emagic durch Apple im Jahr 2002 ermöglicht, da Apple dadurch auf die Technologien in Emagics Audio-Software Logic zurückgreifen konnte.\n\nEin Jahr später wurde Version 2 von GarageBand veröffentlicht. Wichtigste Neuerung in GarageBand 2 ist, dass man nun mehrere Audiospuren gleichzeitig aufnehmen kann, vereinfacht wurde die Takt- und Tonhöhenanpassung von Audio-Dateien. Außerdem können eingespielte Keyboard-Noten als Noten am Bildschirm ausgegeben werden.\n\nAm 10. Januar 2006 wurde GarageBand 3 vorgestellt. In dieser Version steht das Thema Podcast im Vordergrund. Mit Hilfe des Podcast-Aufnahmestudios kann man unter Verwendung von typischen Radio-Jingles qualitativ hochwertige Podcasts erstellen. Ferner kann man das Rauschen in Audiospuren reduzieren und sogar Interviews mit iChat führen und aufnehmen.\n\nAm 7. August 2007 wurde GarageBand 4 vorgestellt. Neben neuen Funktionen wie Multi-take-Aufnahmen, Arrangements, Visual-EQ und Unterstützung von 24-bit Audio wird auch \"Magic GarageBand\" eingeführt. Der Anwender kann aus neun Musikgenres auswählen und auf einer virtuellen Bühne Vorgaben für die Besetzung machen. Dabei gibt \"Magic GarageBand\" die etwa tausend möglichen Songkombinationen direkt wieder. So wird ein neues Projekt erstellt, das bereits eine Begleitung enthält, ohne dass der Anwender diese erst einzuspielen braucht. In GarageBand 4 kann nun auch das Tempo innerhalb des Songs wechseln, und die Notationsansicht kann gedruckt werden.\n\nAm 6. Januar 2009 wurde GarageBand 5 vorgestellt. Wichtigste Neuerung ist die Möglichkeit, mit GarageBand ein Instrument zu lernen. Man kann außerdem verschiedene Lehrvideos berühmter Künstler, wie z. B. Sting, käuflich erwerben. Außerdem wurde ein neuer Gitarrensimulator eingebaut. Die Optik von GarageBand hat sich nur leicht verändert.\n\nAm 20. Oktober 2010 wurde GarageBand 6 vorgestellt. Zu den Neuerungen gehören die Funktionen Flex-Time, Groove-Anpassungen, neue Gitarren-Amps und Stompbox-Effekte, Lehrvideos für Gitarre und Klavier, sowie die „Wie gut habe ich gespielt“-Funktion: Hierbei funktioniert GarageBand wie ein virtueller Musiklehrer, der bewertet, wie gut ein Musikstück gespielt wurde und was verbesserungswürdig ist.\n\nFür GarageBand gab es von Apple sogenannte Jam Packs, die weitere Loops und Voreinstellungen für Audio- und Software-Instrumente beinhalten. Diese werden zurzeit nicht mehr verkauft, allerdings sind diese jetzt in Logic Pro und MainStage enthalten.\n\n"}
{"id": "239193", "url": "https://de.wikipedia.org/wiki?curid=239193", "title": "Trustix Secure Linux", "text": "Trustix Secure Linux\n\nTrustix Secure Linux (TSL) war eine frei verfügbare Linux-Distribution. TSL wurde von der gleichnamigen Firma Trustix (heute Teil der Comodo-Gruppe) entwickelt. Die letzte Version, die herausgegeben wurde, war 3.0.5. Im November 2007 wurde die Einstellung der Distribution bekannt. Seit Anfang 2008 gibt es keine Sicherheitsupdates mehr. \n\nTSL war speziell auf Sicherheit getrimmt und für den Server-Betrieb ausgelegt. Aus diesem Grund wurden einige Funktionen wie Audiowiedergabe oder eine grafische Oberfläche nicht angeboten.\nDie neuste Trustix-Version beinhaltete jedoch alle modernen Dienste, die ein Linux-(Web)Server bereitstellen kann. Einige sind zum Beispiel:\n\n\nund viele mehr.\n\n"}
{"id": "239230", "url": "https://de.wikipedia.org/wiki?curid=239230", "title": "Addierwerk", "text": "Addierwerk\n\nDas Addierwerk ist die Hauptkomponente des Rechenwerks einer CPU.\n\nDas Addierwerk ermöglicht die Addition zweier mehrstelliger Binärzahlen. Da sich alle vier Grundrechenarten auf die Addition zurückführen lassen, bildet es das zentrale Element des Rechenwerks (Arithmetisch-logische Einheit, ALU) eines Prozessors. Während das Addiernetz asynchron funktioniert, arbeitet das Addierwerk getaktet und puffert das Ergebnis mit Speicherbausteinen.\n\nEin Addierwerk kann auf verschiedene Arten realisiert sein. Die bekanntesten Schaltungen sind das Paralleladdierwerk, das Serienaddierwerk und das Von-Neumann-Addierwerk. Alle drei verwenden Halb- und/oder Volladdierer zur Durchführung der Addition.\n\nIn der Grundform wird der Carry-Ripple-Addierer als Addiernetz verwendet, wobei das Carry-Out mit dem Carry-In des nächsten Volladdierers verbunden wird. Zur Bildung der Summe muss im Worst Case das Signal des Carry-Bits vom niederstwertigen Addierer bis zum höchstwertigen Addierer wandern. Daraus folgt bei großen Rechenwerken eine erhebliche Laufzeit im Addiernetz, in der Folge wurden Schaltnetze mit reduzierter Laufzeit entwickelt.\n\nDer Vorteil des Paralleladdierers liegt in der gleichzeitigen Arbeit aller beteiligten Addierer. Der Nachteil der Grundform sind die langen Signal-Laufzeiten, da sich der Übertrag im Extremfall von rechts nach links durch die Schaltung propagieren muss.\nDie Weiterentwicklungen benötigen eine große Zahl zusätzlicher Gatter, deren Zahl linear mit der Breite \"n\" der zu addierenden Stellen des Dualsystems steigt.\n\nDas Serienaddierwerk benötigt einen Volladdierer sowie ein D-Flip-Flop zur Addition zweier \"n\"-stelliger Binärzahlen. In jedem Takt wird das äußerste rechte Bit der Binärzahlen aus den Schieberegistern sowie der Wert im D-Flip-Flop in den Volladdierer übertragen. Das Ergebnis wird in einem Register gespeichert und der Übertrag im D-Flip-Flop für den nächsten Takt gespeichert. Der Vorteil des Serienaddierwerks liegt in der geringen Anzahl der benötigten elektronischen Bauteile. Der Nachteil liegt in der seriellen Abarbeitung, so dass \"n + 1\" Takte zur Addition zweier \"n\"-stelliger Binärzahlen nötig sind.\n\nDie Kombination der Vorteile des Parallel- und Serienaddierers führt zum Von-Neumann-Addierwerk.\n\n Das Von-Neumann-Addierwerk benötigt \"n\" Halbaddierer zur Addition zweier \"n\"-stelliger Binärzahlen. Die Addition erfolgt durch folgenden Algorithmus\nWiederhole\n\nDas AND an P sorgt dafür, dass nach dem ersten Addierdurchlauf P von da an mit 0 gesetzt wird.\n\nDas Addierwerk benötigt im ungünstigsten Fall (worst-case) \"n+1\" Takte; die Erfahrung der Praxis zeigt, dass es im Mittel nur formula_1 sind.\n\nBeispiel 13 + 11:\n\nDie Subtraktion ist mit der Addition eng verwandt und lässt sich durch Vorzeichenwechsel des Subtrahenden in eine Addition umwandeln. Ein Vorzeichenwechsel bei Dualzahlen ermöglicht das Zweierkomplement, umgesetzt als Einerkomplement (invertieren aller Bits) und danach mit 1 addiert (setzen von carry in). Die Erweiterung des Addierwerks zum Addier-/Subtrahierwerk ermöglicht die Vorschaltung von XOR-Verknüpfungen am Eingang des zweiten Operanden. Steht der Wahlschalter auf 1 invertiert das XOR das Eingangssignal und setzt das Carry-In-Bit, hierdurch entsteht das Zweierkomplement und aus der Addition wird eine Subtraktion.\nViele Prozessoren verfügen über eine Gleitkommaeinheit für Berechnungen mit Kommazahlen. Bei Gleitkommazahlen werden Mantisse und Exponent getrennt verarbeitet. Dazu werden die Exponenten der Zahlen in einem Normierer angeglichen, die Zahlen mit Hilfe eines Addierer/Subtrahierer addiert und schließlich wird das Ergebnis und der neue Exponent in einem Normierer zu einer neuen Gleitkommazahl – dem Ergebnis – verarbeitet. \n\nZuerst muss der Exponent angeglichen werden. Dazu werden die beiden Exponenten subtrahiert und die Mantisse der Gleitkommazahl mit dem kleineren Exponenten mit einem Schieberegister um die durch die Subtraktion der Exponenten ermittelte Anzahl an Stellen (Bits) verschoben. Dadurch besitzen beide Zahlen denselben (größeren) Exponenten. Der größere Exponent wird über einen Multiplexer anhand des Vorzeichens des bei der Subtraktion entstandenen Wertes (Exponenten-Differenz) ausgewählt und direkt an den Normierer weitergeleitet. \n\nIn der Addierer/Subtrahierer-Stufe arbeitet im Wesentlichen ein Addierer mit vorgeschalteten Invertern an den Eingängen. Die Inverter invertieren (negieren) die Mantisse, wenn das zugehörige Vorzeichenbit der jeweiligen Gleitkommazahl gesetzt ist. Anschließend können die beiden Zahlen addiert werden. Hierbei entsteht die neue Mantisse. Zudem wird das Prioritäts-Bit (Überlauf-Bit) des Addierers gesetzt, wenn die Summe der beiden Mantissen zu groß ist um in der neuen Mantisse gespeichert werden zu können. \n\nIm Normierer wird die Exponenten-Differenz um eins reduziert und das Ergebnis aus der Addierer/Subtrahierer-Stufe um eins nach links verschoben, falls das Prioritäts-Bit gesetzt ist. Anschließend wird das Ergebnis aus der Addierer/Subtrahierer-Stufe zur Mantisse – und die Exponenten-Differenz zum Exponenten – der Ergebnis-Gleitkommazahl.\n\n"}
{"id": "239999", "url": "https://de.wikipedia.org/wiki?curid=239999", "title": "Auflösung (Digitaltechnik)", "text": "Auflösung (Digitaltechnik)\n\nDie Auflösung gibt in der Digitaltechnik an, wie fein abgestuft eine ursprünglich analoge Größe digital dargestellt werden kann, die vor der Weiterverarbeitung mit einem Analog-Digital-Umsetzer digitalisiert wird.\n\nDie Auflösung kann sich hier auf verschiedene Dimensionen beziehen:\n\nDie digitale Signalverarbeitung verwendet den Begriff \"Auflösung\" im Zusammenhang mit der Quantisierung im Sinne von \"Schrittweite\". Dieses steht in Übereinstimmung mit der für die Messtechnik grundlegenden Norm, die den Begriff Auflösung festlegt als quantitative Angabe, wie weit ein Messgerät zwischen nahe beieinander liegenden Messwerten eindeutig unterscheiden kann.\n\nDer Zusammenhang zwischen dem stufenlosen Eingangssignal und dem gestuften Ausgangssignal wird durch die Quantisierungskennlinie beschrieben. Bei einer linearen Quantisierungskennlinie ist die Schrittweite oder Breite einer Quantisierungsstufe konstant. Sie ergibt sich aus dem Eingangssignalbereich und aus der \"Anzahl der Stufen\" bzw. aus der \"Anzahl der Stellen\" des Ausgabewertes.\n\nBei bestimmten Anwendungen (z. B. Sprach- oder Bildübertragung) kann es von Vorteil sein, eine nichtlineare Kennlinie zu verwenden. Die Schrittweite hängt dabei vom Eingangswert ab und kann für jedes Intervall unterschiedlich groß sein.\n\nDie gängige Bezeichnung bei Soundkarten und Audiosoftware ist einfach „Auflösung“. Diese wird für die Lautstärke durch die Anzahl der Binärstellen und für den Zeitbereich durch die Abtastrate angegeben. Zum Beispiel: „16 Bit / 48 kHz“.\n\nBis ungefähr 1995 arbeiteten die meisten Soundkarten mit einer Auflösung von 8 Bit pro Sample, wodurch noch ein leises Hintergrundrauschen wahrnehmbar war. Bei Audio-CDs und moderneren Soundkarten sind inzwischen 16 Bit pro Kanal üblich; bei Audio- und Video-DVDs bis zu 24 Bit. Bei der ISDN-Telefonie wird das analoge Eingangssignal mit 8 Bit pro Sample abgetastet, wobei bei der Quantisierung die Besonderheiten der menschlichen Wahrnehmung berücksichtigt werden.\n\nViele Programme der Musikproduktion arbeiten mit 32 Bit breiten Samples, die aber nur zusammen mit einer Ausrüstung genutzt werden können, die eine entsprechende Dynamik erlauben (z. B. Mikrofone, Verstärker, Lautsprecher, Räumlichkeiten).\n\nComputergrafiken werden in drei Dimensionen aufgelöst:\n\nBei einem Grauwertbild genügt eine Auflösung von 8 Bit aus, um mit den daraus resultierenden 256 Schattierungen ein natürlich erscheinendes Bild zu erhalten. Nur wenn der Kontrast später stark verändert werden soll, ist eine feinere Auflösung nötig, um das Bildergebnis nicht zu verfälschen.\n\nBei Farbbildern führen 256 Farben zu mangelhaften, körnigen oder comichaften Aufnahmen, sodass heute meistens jeder der drei Farbkanäle (rot, grün, blau) mit 8 Bit aufgelöst wird.\n\nBei der Bildauflösung ist neben der Signalauflösung ferner das Auflösungsvermögen zu beachten, das Vermögen, dicht beieinander liegende Objekte als eigenständig erfassen zu können.\n\nBei der Aufnahme von digitalen Videos werden die Einzelbilder wie im Abschnitt Computergrafik beschrieben aufgelöst. Zusätzlich kommt, wie bei der Tontechnik, eine zeitliche Auflösung bei der Abtastung der Bilder hinzu, die Bildfrequenz.\n"}
{"id": "240894", "url": "https://de.wikipedia.org/wiki?curid=240894", "title": "Pkgsrc", "text": "Pkgsrc\n\npkgsrc (\"package source\") ist eine Paketverwaltung für UNIX-artige Betriebssysteme. Es ist die Standardpaketverwaltung für NetBSD, SmartOS, Minix und Draco Linux.\n\nIm Gegensatz zu vielen anderen Paketverwaltungen läuft es auf beinahe allen UNIX-artigen Betriebssystemen, ist nicht auf ein bestimmtes Verzeichnis – z. B. codice_1 – festgelegt und kann auch von Benutzern installiert werden, die nicht über Systemadministratorrechte verfügen.\n\nDie bevorzugte Installationsmethode bei pkgsrc ist das Kompilieren aus dem Quelltext der Pakete, es gibt jedoch auch vorgefertigte Binärpakete für einige Betriebssysteme, insbesondere NetBSD.\n\npkgsrc ist quelloffen unter BSD-Lizenz veröffentlicht.\n\npkgsrc stellt ein paar Befehle bereit, mit denen Binärpakete installiert (codice_2), aktualisiert und wieder aus dem System entfernt werden können (codice_3). Die Binärpakete für NetBSD sind auf dem FTP-Server des NetBSD-Projekts verfügbar.\n\nDie Benutzung der Binärpakete hat den Nachteil, dass nicht alle Programme aus dem pkgsrc-Verzeichnis (siehe Weblinks) zur Verfügung stehen. Insbesondere fehlen Pakete, deren Weiterverteilung nicht erlaubt ist. Ein weiterer Nachteil ist, dass mit der Zeit die Binärpakete an Aktualität verlieren.\n\nAm Ende eines jeden Quartals wird von pkgsrc ein „stabiler“ Zweig angelegt. Die Pakete in diesem Zweig werden nur aktualisiert, wenn Sicherheitslücken behoben werden. Außerdem werden von diesen Zweigen für einige der von NetBSD unterstützten Plattformen Binärpakete erzeugt. Im „Entwicklungszweig“ sind Aktualisierungen häufiger, dafür fließen hier regelmäßig experimentelle Neuerungen ein, sodass es sein kann, dass einige Pakete sich nicht installieren lassen.\n\nWenn es für eine Plattform keine Binärpakete gibt, bleibt nur die Möglichkeit, die Pakete selbst aus den Quellen zu bauen. Darüber hinaus gibt es für viele Pakete zusätzliche Optionen, die schon zum Zeitpunkt des Kompilierens feststehen müssen. Ein anderer Grund, pkgsrc aus den Quellen zu installieren, ist, dass sehr viele Pakete auch von „normalen“ Benutzern installiert werden können, also keine Systemadministratorrechte erfordern. Für solche Konfigurationen werden generell keine Binärpakete bereitgestellt, da das Verzeichnis, in dem die Pakete installiert werden, oft individuell gewählt wird. Im Wip-Zweig (Work-In-Progress) werden nichtoffizielle Packages von Entwicklern gepflegt und können gemeinsam mit den regulären genutzt werden, ähnlich dem Portstree.\n\nAlle drei Monate veröffentlichen die pkgsrc-Entwickler eine neue Version. Diese wird nach dem aktuellen Jahr und Quartal benannt – z. B. pkgsrc-2015Q2. In einer zweiwöchigen \"Freeze-Periode\" vor dem Release liegt der Entwicklungsfokus auf Stabilität.\n\nSicherheitsupdates werden in die jeweils letzte Version zurückportiert.\n\n"}
{"id": "241633", "url": "https://de.wikipedia.org/wiki?curid=241633", "title": "Windows Application Programming Interface", "text": "Windows Application Programming Interface\n\nDas Windows Application Programming Interface (kurz: WinAPI; englisch für: „Windows-Anwendungs-Programmierschnittstelle“) ist eine Programmierschnittstelle und Laufzeitumgebung, welche Programmierern bereitstehen, um Anwendungsprogramme für Windows-Betriebssysteme zu erstellen.\n\nDie Funktionen der WinAPI sind ausschließlich in den Programmiersprachen C und Assembler geschrieben und können von allen Windows-Programmierern im selbst erstellten Quelltext verwendet werden. Sie befinden sich in dynamischen Bibliotheken, den sogenannten DLL-Dateien, beispielsweise \"kernel32.dll\", \"user32.dll\" und \"gdi32.dll\". Diese sind schon im Betriebssystem integriert. Die WinAPI abstrahiert die nativen Funktionen des Betriebssystems, welche durch die ntdll.dll exportiert und im Windows-Kernel (ntoskrnl.exe) implementiert werden. Dies ermöglicht es Microsoft, die Implementierung der WinAPI zu modifizieren, ohne deren Funktionsweise zu ändern.\n\nDie WinAPI wird immer verwendet, wenn native Windows-Anwendungen geschrieben werden, obwohl der Programmierer sie eventuell direkt im Quelltext gar nicht aufgerufen hat. Die Aufrufe der API werden durch eine Laufzeitbibliothek gekapselt, die ihrerseits Funktionen der Windows-API aufrufen. Windows NT/2000 stellt eine native API zur Verfügung, die sowohl für im Kernel-Mode als auch für im User-Mode laufenden Programmen eine Programmierschnittstelle offenbart. Diese Schnittstellen sind Bestandteil des Betriebssystemkerns, welcher letzten Endes alle hardwarenahen Operationen mit und an der Hardware durchführt.\n\nDie objektorientierten Bibliotheken, wie die MFC-Bibliothek und das .NET Framework, sind eine Alternative zur direkten Nutzung der Windows API. Hierbei werden die nativen Methoden der Windows API gekapselt und ermöglichen auf diese Art und Weise eine bequemere objektorientierte Handhabung der zur Verfügung stehenden Funktionen. Viele von Programmen ausgeführte Aktionen, seien es I/O-Operationen, Windows-Dialoge oder Speicherverwaltung, wären ohne die Windows API nur sehr eingeschränkt durchführbar. Systemnaher Zugriff auf das Windows-Betriebssystem, welcher hauptsächlich von Gerätetreibern benötigt wird, wird mithilfe des \"Windows Driver Model (WDM)\" unter allen Windows-NT-Versionen realisiert.\n\nBeinahe mit jeder neuen Version von Windows wurde die Windows API erweitert und abgeändert. Der Name der API wurde dennoch zwischen den Versionen beibehalten und nur leicht verändert, um gravierende Unterschiede zwischen den Windows-Architekturen und den Plattformen zu verdeutlichen. So wurde der ursprüngliche Begriff WinAPI, der in den 16-Bit-Versionen von Windows vorherrschte, um die Zahl 32 zu Win32 API erweitert, um dem bedeutsamen Sprung zur 32-Bit-Architektur deutlich zu machen. Dennoch wird heute der allgemeine Begriff Windows API verwendet, der sowohl die alte API als auch die neue mit einschließt.\n\n\"Win16\" war die erste API für die 16-Bit-Versionen von Windows. Gängiger Begriff war schlicht Windows API, wurde aber später umbenannt in Win16, um sie von der neueren Windows API der 32-Bit-Architektur unterscheiden zu können. Die Funktionen der Win16 API liegen hauptsächlich im Kern des Betriebssystems: \"kernel.exe\" (oder \"krnl286.exe\" oder \"krnl386.exe\"), \"user.exe\" und \"gdi.exe\". Trotz der Dateiendung codice_1 sind diese Dateien tatsächlich sogenannte Programmbibliotheken.\n\n\"Win32\" ist die 32-Bit-API für moderne Versionen von Windows. Die API besteht aus Funktionen, die, wie bei Win16, in Programmbibliotheken implementiert sind. Die Kern-DLLs von Win32 sind kernel32.dll, user32.dll und gdi32.dll. Win32 wurde mit Windows NT eingeführt. Die Version von Win32, die mit Windows 95 ausgeliefert wurde, lief ursprünglich unter dem Namen Win32c, wobei das „c“ für Kompatibilität () stand, aber dieser Ausdruck wurde später von Microsoft zugunsten von Win32 wieder verworfen. In Windows NT und seinen Nachfolgern (eingeschlossen alle modernen Windows-Versionen) werden Win32-Aufrufe durch zwei Module ausgeführt, csrss.exe () im User-Modus und win32k.sys im Kernel-Modus. Dies dient dem Schutz des Betriebssystems und verhindert, dass laufende Anwendungen des Benutzers kritische Daten des Betriebssystems modifizieren oder darauf zugreifen können. Die Modi werden direkt vom Prozessor zur Verfügung gestellt.\n\nObwohl auch Windows CE eine Win32-ähnliche API enthält, gibt es tiefgreifende Unterschiede, die Portierungen von Quellcode in der Realität meist aufwändig gestalten.\n\n\"Win32s\" ist die 32-Bit-API für die Windows-3.1x-Familie und als solche die 32-Bit-Erweiterung für die ansonsten 16-bittigen Betriebssysteme. Das „s“ steht für Teilmenge (englisch \"\"). Dabei wurden die Funktionsbibliotheken aus Windows NT nicht komplett übernommen, sondern lediglich eine Auswahl daraus, so ist beispielsweise MS Office 97 unter Windows NT 3.51 lauffähig, nicht jedoch unter Windows 3.1x. In Kombination mit Grafikschnittstellen wie OpenGL oder Video for Windows sollte damit jedoch bis zum Erscheinen von Windows 95 ein ausreichender Standard für Heimanwender gesetzt werden. Sie wurde mehrmals angepasst und in höheren Versionsnummern ergänzt.\n\n\"Win32 für 64-Bit-Windows\", auch bekannt unter dem Namen \"Win64\", ist die Version der API, die für 64-Bit-Versionen von Windows – namentlich Windows XP „x64 Edition“, Windows Server 2003 „x64 Edition“ (für AMD64-Prozessoren) und Windows Server 2003 für Itanium-Serien – entworfen wurde. Bei den 64-Bit-Versionen handelt es sich nur um zwei weitere unterstützte Plattformen innerhalb der Windows-NT-Architektur, so dass sowohl die 32-Bit- als auch die 64-Bit-Version einer Anwendung aus demselben Quellcode kompiliert werden können. Alle Zeiger auf den Speicher sind standardmäßig 64 Bit groß, weshalb der Quellcode gegebenenfalls auf Kompatibilität überprüft werden muss. Obwohl keine neuen Funktionen zur API hinzugefügt wurden, gibt es Unterschiede zwischen Win32 und Win64. Deshalb wird auf 64-Bit-Systemen – für 32-Bit-Anwendungen – durch eine Abstraktionsebene namens WOW64 eine zu Win32 kompatible API bereitgestellt.\n\nDie \".NET-Framework\"-API (früher \"WinFX\" genannt) ist eine neue, objektorientierte API, die die native Windows-API erweitert. Die API wurde unter anderem entworfen, um zukünftigen Anwendungen einen einfachen, verwalteten Zugriff auf die zahlreichen neuen Features in Windows Vista zu geben. .NET-Anwendungen laufen als sogenannter Managed Code (verwalteter Code) unter einer Laufzeitumgebung namens Common Language Runtime (CLR), einer virtuellen Maschine, die im Sinne der Abstraktion keine direkten Maschinenbefehle ausführt, sondern stattdessen das aus Bytecode bestehende Programm in Maschinencode umwandelt, bevor dieses dann vom Prozessor ausgeführt werden kann. Der GUI-API-Satz für WinFX, der unter dem Codenamen \"Avalon\" läuft, wird Windows Presentation Foundation genannt und löst die alte GDI- und GDI+-APIs ab. Sie setzt direkt auf DirectX auf und benötigt Grafikkarten mit Hardwarebeschleunigung, um alle Effekte angemessen darstellen zu können.\n\nÜberdies haben frühere Versionen von Windows auch die APIs anderer Betriebssysteme mitgebracht. Windows NT 3.1 sowie Windows NT 4.0 waren noch mit einer leicht abgespeckten Version der OS/2- und einer POSIX-API ausgestattet. Dadurch war es in eingeschränkter Form möglich, auch Programme auszuführen, welche eigentlich für OS/2 oder Unix-Systeme kompiliert waren. Möglich wurde dies durch eine tiefer liegende zweite API-Schicht – die weitestgehend undokumentierte \"\" \"API\". Auf die Funktionen dieser API stößt man z. B., wenn man Betriebssystem-DLLs mit einem Debugger durchläuft. Die Windows-API-Funktionen von Win32 usw. rufen zum Teil wiederum Funktionen der Native API auf, ebenso wie das OS/2- und POSIX-Subsystem. Inzwischen hat Microsoft die anderen APIs entfernt, der prinzipielle Mechanismus ist aber auch in Windows Vista noch unverändert. Mit der .NET-API wurde sogar eine dritte, nun aber wesentlich modernere objektorientierte Schicht über die Win32-API und die Native API gesetzt.\n\nNeben der in aktuellen Windows-Versionen enthaltenen Win32/64-API von Microsoft gibt es auch Varianten für andere Betriebssysteme. Diese Nachbildungen dienen dazu, Windows-Anwendungsprogramme ohne Windows-Betriebssystem zu nutzen. Aus rechtlichen Gründen sind meist nicht alle Funktion der original API vorhanden, was die Kompatibilität mit Anwendungsprogrammen einschränken kann. Da die wenigsten Anwendungsprogramme alle Funktionen der Win32/64-API benötigen, sind viele aber trotzdem voll nutzbar.\n\nBeispiele von Win32-Implementierungen:\n\nDas Mono-Projekt bietet eine Entwicklungsumgebung und eine API für .NET-Anwendungen in Linux- und Unix-artigen Betriebssystemen. Mono orientiert sich dabei vor allem am Common-Language-Infrastructure-Standard.\n\nFolgender Quelltext implementiert ein Programm mit Hilfe der WinAPI, welches ein Fenster erzeugt und den Text „Hello World!“ ausgibt. Das Programm ist in der Programmiersprache C verfasst.\n\nLRESULT CALLBACK WndProc(HWND, UINT, WPARAM, LPARAM);\n\nint WINAPI WinMain(HINSTANCE hInstance, HINSTANCE hPrevInstance,\n\n// Die Hauptnachrichtenschleife\nLRESULT CALLBACK WndProc(HWND hWnd, UINT message, WPARAM wParam, LPARAM lParam)\n\n\n"}
{"id": "241756", "url": "https://de.wikipedia.org/wiki?curid=241756", "title": "PuTTY", "text": "PuTTY\n\nPuTTY ist eine freie Software zum Herstellen von Verbindungen über Secure Shell, Telnet, Remote login oder serielle Schnittstellen. Dabei dient PuTTY als Client und stellt die Verbindung zu einem Server her. Beim Verbindungsaufbau wird die Identität des Benutzers mittels einer der bereitgestellten Methoden zur Authentifizierung überprüft. PuTTY ist für Windows und Linux verfügbar.\n\nIn der so bereitgestellten textorientierten Terminalsitzung können direkt Befehle abgesetzt werden, die auf dem fernen System ausgeführt werden. Eine grafische Ausgabe ist nicht möglich, jedoch kann ein X-Server genutzt werden, der auf dem Client-Rechner läuft. Zudem wird IPv6 ab der Version 0.58 und die serielle Schnittstelle ab der Version 0.59 unterstützt.\n\n\nDie Programmteile PuTTYgen und Pageant sind in WinSCP enthalten, auch einige Android-Apps bauen auf der Software auf. Inoffizielle Versionen existieren für Windows Mobile und Symbian OS.\n\nPuTTY gilt als das Standard-Werkzeug für SSH-Verbindungen unter Windows und ist in Firmenumgebungen häufig bereits vorinstalliert, da Windows bis Ende 2017 keinen eigenen SSH-Client bereitstellte.\n\n"}
{"id": "241771", "url": "https://de.wikipedia.org/wiki?curid=241771", "title": "Cygwin", "text": "Cygwin\n\nMit Cygwin [] lassen sich Computerprogramme, die üblicherweise unter POSIX-Systemen wie GNU/Linux, BSD und Unix laufen, auf Microsoft Windows portieren. Es ist eine Kompatibilitätsschicht, die die Unix-API für verschiedene Versionen von Windows zur Verfügung stellt, auf deren Basis eine Vielzahl von Programmen aus der Unix-Welt unter Windows übersetzt werden kann.\n\nMittels Cygwin portierte Programme laufen Stand 2016 unter allen Windows-Versionen ab Windows Vista.\n\nCygwin wurde ursprünglich von der Firma Cygnus Solutions programmiert und seit deren Übernahme durch die Softwarefirma Red Hat erfolgt dort die Weiterentwicklung.\n\nKern von Cygwin ist die so genannte \"Cygwin-DLL\" (cygwin1.dll), eine Dynamic Link Library, die Unix-APIs zur Verfügung stellt. Die APIs der Cygwin-DLL bilden das Pendant zu den System Calls unter Unix. Die mit Cygwin portierten Programme sind normalerweise nicht alleine lauffähig, da sie gegen die Cygwin-DLL gelinkt werden und somit von dieser abhängig sind.\n\nEs gibt mit \"Cygwin/X\" auch eine Portierung des X.Org-Servers auf die Cygwin-Umgebung, so dass unter Microsoft Windows ein kompletter X-Server bereitsteht.\n\nDurch den portierten X-Server ist es möglich, entweder UNIX/Linux-Programme, die für Windows kompiliert wurden, \"lokal\" auf dem Windows-Rechner auszuführen (Ausführen von \"startxwin.bat\"), oder aber Programme, die auf einem Unix- oder Linuxrechner ausgeführt werden, auf Windows darzustellen; auch kann man sich, ausgehend von dem Windows-Rechner, auf dem Unix-Rechner einloggen (Ausführen von \"startxdmcp.bat\").\n\nDies kann für Privatanwender und Programmierer interessant sein, die Windows und Unix/Linux gleichzeitig einsetzen.\n\nLokale Partitionen werden mit codice_1, codice_2 etc. angesprochen. Auf Windows-Freigaben kann mit codice_3 zugegriffen werden.\n\nAuch ein SSH-Server ist durch Cygwin möglich, der unter Windows als Service installiert werden kann. Häufig wird Cygwin auch genutzt, um Bash-Prozeduren zu programmieren, die dann automatisierte Systemfunktionen erfüllen können. Dazu gehören unter anderem lokale oder entfernte (remote) Datensicherungen (backups) auf andere Unix-Systeme. Eine ähnliche Funktionalität wie Cygwin realisiert mittlerweile auch Microsofts Services-für-Unix-Paket (SFU), das auf dem Interix-System basiert. Microsoft Windows Services for UNIX (SFU) wurde im Zuge der Vista-Einführung in „Subsystem for UNIX-based Applications“ (SUA) umbenannt und ist in den Editionen „Enterprise“ und „Ultimate“ von Vista enthalten.\n\nCygwin begann im Jahr 1995 als ein Projekt von Steve Chamberlain, einem Cygnus-Entwickler. Ihm war aufgefallen, dass auf PCs mit Intel-x86-CPUs, die Windows NT und Windows 95 als Betriebssystem hatten, in der Regel COFF als Objekt-Dateiformat benutzt wurde. Außerdem war ihm aufgefallen, dass die GNU-Compiler (GCC) bereits Unterstützung für x86 und COFF in Zusammenhang mit der C-Bibliothek newlib boten. Also sollte es, so seine Folgerung, – zumindest in der Theorie – nicht allzu schwierig sein, den GCC neu auszurichten, um damit einen weiteren Cross-Compiler zu schaffen, der dann unmittelbar ausführbare Dateien für die Windows-Plattform lauffähig erzeugt. In der Praxis war die Aufgabe mit einem gewissen Aufwand verbunden, den Chamberlain schließlich erfolgreich meisterte, so dass erste Beispielprogramme erzeugt und getestet werden konnten.\n\nAls Nächstes sollte auch der Compiler selbst dazu gebracht werden, auf einem Windows-System zu laufen; dazu mussten zunächst einige Basiskomponenten der üblichen GNU-Konfiguration, diverse Shell-Skripte sowie die Bash-Shell selbst auf dem Windows-System in einer Emulation lauffähig gemacht werden. Das Win32-API von Windows enthielt bereits zahlreiche sehr ähnliche Funktionen, sodass das Gros der verwendeten Systemaufrufe lediglich angepasst werden musste. Dies mündete in diversen Cygwin-Bibliotheken, sogenannten DLLs, die direkt auf dem Windows-System aufsetzten, aber nach oben hin die für Unix typischen Dienste (APIs) anboten.\n\nBis 1996 fanden sich einige weitere Entwickler, die sich dem Projekt anschlossen, insbesondere weil langsam klar wurde, dass es sich lohnen könnte, Unix-Anwendungen unter Verwendung der Cygwin-Komponenten für Windows-Systeme anzubieten. (Frühere Unix-zu-Windows Portierungen basierten zumeist auf der Entwicklungsumgebung DJGPP). Etwa 1998 begann Cygnus damit, Cygwin gewinnbringend zu vermarkten. Mittlerweile wird Cygwin als freie Software im Rahmen der GPL angeboten und zugleich unter Federführung von Red Hat weiterentwickelt.\n\nIm Juni 2016 gab Red Hat bekannt, dass ab Version 2.5.2 die Cygwin-DLL nicht mehr unter GPLv3, sondern unter LGPLv3 veröffentlicht wird. Daher kann die Cygwin-DLL ab dieser Version auch für proprietäre Programme eingesetzt werden. Vorher musste dafür eine Lizenz bei Red Hat erworben werden. Weiterhin vereinfacht diese Änderung die Einreichung von Patches durch externe Entwickler, da keine Copyright-Vereinbarung mit Red Hat mehr erforderlich ist.\n\n\n"}
{"id": "243414", "url": "https://de.wikipedia.org/wiki?curid=243414", "title": "GFA-BASIC", "text": "GFA-BASIC\n\nGFA-BASIC ist ein Dialekt der Programmiersprache BASIC, entwickelt von Frank Ostrowski. Die erste Version wurde 1986 vollendet und für den Atari ST herausgebracht. Mitte und Ende der 1980er Jahre wurde die Programmiersprache für diesen Heimcomputer sehr populär (da das ST BASIC, das bei den Computern mitgeliefert wurde, recht fehlerhaft und beschränkt war). Später wurden auch Portierungen für den Commodore Amiga, DOS und Windows vermarktet.\n\nOffiziell sind der Interpreter und der Compiler heute (2011) nicht mehr verfügbar, es existiert jedoch noch eine kleine Entwicklergemeinde, welche alte Programme pflegt. Der Sprachdialekt existiert in modifizierter Version heute nur noch im Klon X11-Basic fort.\n\nDie Sprache übernimmt wie viele andere moderne Basic-Dialekte von Pascal und C die Kontrollstrukturen und erinnert daher fast ein wenig an Fortran. Damit erlaubt auch GFA-BASIC strukturiertes Programmieren (siehe zum Beispiel Programmieren ohne Goto).\n\nEs fehlen gänzlich die klassischen Zeilennummern, daher ist pro Zeile nur ein Befehl erlaubt (in späteren Versionen gab es dann auch einen Befehlsseparator, um mehrere Befehle in einer Zeile unterzubringen). Als Sprungmarken dienen analog zu den meisten anderen Programmiersprachen „Labels“, also Zeilen, die einen Sprungmarkennamen enthalten.\n\nUnterprogramme werden analog zu Pascal als Prozeduren und Funktionen definiert, wobei Funktionen im Gegensatz zu Prozeduren einen Rückgabewert haben. Beide akzeptieren Parameter, wahlweise per Wert- oder auch per Referenz-Übergabe. Lokale Variablen sind ebenfalls möglich.\n\nDie Datentypen umfassen bei GFA-Basic für Windows 32-Bit die einfachen Typen: Boolean, Byte, Short, Card, Integer, Long, Handle, Large, Float, Single, Pointer, Currency, Date, String, Fixed-Length-String, Variant; Strukturen (= Types) sind ebenfalls definierbar.\n\nEditor und Interpreter sind ein einziges Programm, welches bereits bei der Programmierung Fehler meldet und Befehle vervollständigt. Ein RunOnly-Interpreter (für ATARI, Amiga, MS-DOS, Windows 3.x) kann die (in einem gesonderten Format gespeicherten) Quelltexte auch unabhängig vom echten Interpreter ausführen und ein Compiler schnellere, ausführbare Programme erzeugen.\n\n\nSeit 1986 existieren auch Versionen für MS-DOS, Windows und Amiga-Betriebssysteme. Für Windows ab Windows 3.0 existieren ein 16-bit-Interpreter und Compiler sowie ein 32-bit-Interpreter und Compiler, womit kompakte ausführbare Dateien erzeugt werden können.\n\nGFA-Basic war in den erfolgreichsten Jahren des Atari ST sehr verbreitet, nicht zuletzt wegen des für die damalige Zeit komfortablen Editors, der Neuerungen wie etwa Code-Faltung einführte. Wegen der aus Betriebssystem-Sicht unsauberen Systembibliotheken sowie der nicht in die übliche GEM-Benutzeroberfläche integrierten Entwicklungsumgebung verlor GFA-Basic in den Mittneunzigern an Popularität zugunsten von TurboC, PureC und Modula-2.\n\nDie Entwicklung auf der ATARI-Seite wurde von offizieller Seite nach Version 3.6 TT eingestellt, jedoch von engagierten Programmierern ohne Kenntnis des Quelltexts weitergeführt, indem nach und nach die Module der Bibliothek ausgetauscht wurden und der Interpreter und die IDE binär gepatcht wurden. Die inoffizielle und letzte eigenständige Weiterentwicklung des GFA-Entwicklungspaketes stellen die RUN!Lib, der RUN!Only-Interpreter von \"RUN! Software\" sowie GBE von \"ENCOM\" dar.\n\nGFA-Basic wurde als 16-Bit- (letzte Version: 4.38) und als 32-Bit-Version (letzte Version: 2.30 vom 25. Juli 2001) für die Programmierung unter Windows bereitgestellt.\n\nSeit Ende 2002 reagiert \"GFA Software Technologies\" nicht mehr auf Bestellungen und Kundenanfragen, im Sommer 2005 wurde die offizielle Mailingliste ohne Vorankündigung abgeschaltet.\n\nSeit Ende 2006 führt eine kleine Gruppe die Weiterentwicklung der Windows-Version (32-bit) durch. Bei Google entstanden neue Seiten speziell für GFA-BASIC 32 und GFA-BASIC 16.\n\n\n\n\nUmfangreiche Sammlungen von Programmierbeispielen sind frei für die Atari- und Windows-Versionen verfügbar:\n\n\nDie Windows-Versionen: auf den Seiten gibt es entsprechende Downloads der Interpreter sowie neue Informationen, Patches, Software etc.\n\nKonverterprogramm für GFA-WIN 16-Bit Sources nach VB.Net\n"}
{"id": "243856", "url": "https://de.wikipedia.org/wiki?curid=243856", "title": "Damn Small Linux", "text": "Damn Small Linux\n\nDamn Small Linux [] (DSL [], ) ist eine englischsprachige Linux-Distribution, die sich von einer Visitenkarten-CD (50 MB) starten lässt. Das Damn-Small-Linux-Projekt wurde von John Andrews und Robert Shingledecker ins Leben gerufen und basierte ursprünglich auf Knoppix. Heute basiert Damn Small Linux auf Debian GNU/Linux. Kurzzeitig geriet die Entwicklung ins Stocken, da sich der Erfinder nicht mit dem Hauptentwickler einigen konnte.\n\nDamn Small Linux ist dafür konzipiert, auch auf alten Rechnern mit veralteter Hardware (zum Beispiel mit 486er-Architektur, 16 MB RAM und kleinen Festplatten unter 54,0 MB) zu laufen. Die Distribution lässt sich sowohl als Live-CD verwenden als auch auf der Festplatte installieren. Sie kann sogar in einem MS-Windows-Fenster gestartet werden (dsl-embedded), wie es beispielsweise auch bei \"TopologiLinux\", Puppy Linux, CoLinux und WinLinux möglich ist. Aufgrund ihres geringen Speicherbedarfs bietet diese Distribution die Möglichkeit, komplett aus dem RAM (ab 128 MB) oder einem USB-Stick zu arbeiten.\n\nAb der Version 4.0 wird der Linux-Kernel 2.4.31 verwendet. Ab der Version 3.0 wird bei DSL das Dateisystem UnionFS verwendet, bei dem die Möglichkeit besteht, im Live-Betrieb lokale Dateien zu ändern und somit zum Beispiel Programme nachzuinstallieren.\n\nTrotz ihres geringen Umfangs bietet diese Distribution einen kompletten Desktop.\n\n\nDank eines mitgelieferten Paket-Managers lässt sich die Basisversion schnell und unkompliziert um eine große Zahl zusätzlicher Pakete erweitern.\n\nAb Version 0.7 ist Damn Small Linux mittels der \"myDSL extensions\" erweiterbar, GZip-komprimierter Binärarchive mit der Dateiendung \".dsl\". Das Prinzip von Damn Small Linux ist mit dem von VectorLinux oder mit dem des schon oben erwähnten Knoppix vergleichbar.\n\n2006 gab es mit DSL-N (Damn Small Linux Not) den Versuch, eine an DSL orientierte Linux-Distribution zu entwickeln, die jedoch nicht an die 50-MB-Grenze von DSL gebunden war. Dadurch wurde die Verwendung des Linux-Kernels 2.6 und von GTK2 möglich. Für DSL-N sind nicht so viele Programme verfügbar wie für DSL (im Gegensatz zu DSL lassen sich Debian-Paketquellen nicht einbinden). Die letzte Version DSL-N 0.1RC4 (24. August 2006) hat eine Größe von 95 MB.\n\nDie erste Version 0.1 erschien im Februar 2003. Nach mehr als 20 weiteren Vorabversionen folgte am 13. April 2005 Version 1.0. Am 27. September 2012 erschien die bislang letzte Version 4.4.11 RC2.\n\n\n"}
{"id": "244094", "url": "https://de.wikipedia.org/wiki?curid=244094", "title": "PDP-1", "text": "PDP-1\n\nDer PDP-1 (Programmed Data Processor 1) war der erste Minicomputer und wurde 1959 von der Firma DEC (Digital Equipment Corporation) entwickelt.\n\nDie Bezeichnung \"Minicomputer\" erscheint aus heutiger Sicht unangemessen, da der PDP-1 so groß war wie zwei Kühlschränke. Dieser PDP konnte aber, im Gegensatz zu den viel größeren IBM-Maschinen, von einer einzigen Person hochgefahren und gesteuert werden. Darüber hinaus konnte er von mehreren Personen simultan genutzt werden. Anfragen, die man an den PDP stellte, wurden sofort verarbeitet und ausgegeben – anders als bei größeren Rechnern, die im Batch(Stapel)betrieb liefen, bei denen man die Programme als Lochkartenstapel beim Operator abgab und sich die Ergebnisse als Ausdruck am nächsten Tag abholen konnte. Angekündigt wurde der PDP-1 1960, das erste Exemplar wurde bereits im Dezember 1959 installiert.\n\nDer PDP-1 wurde mit diskreten Transistor-Schaltkreisen implementiert, statt mit den damals üblichen Elektronenröhren. Integrierte Schaltkreise wurden jedoch noch nicht verwendet.\n\nEntwickler war der DEC-Gründer Ken Olsen. Direkter Vorgänger war der von noch am MIT entwickelte TX-0.\n\n\nDer Computer wird von den Emulatoren M.E.S.S. und SIMH unterstützt. Ebenso existieren zahlreiche Spacewar!-Simulationen.\n\nProgrammiert wurde der Rechner in Assemblersprache oder in LISP. Der Speicher wurde, im Gegensatz zu heutigen Computern, nicht oktettweise, sondern in 18 Bit großen Worten adressiert. Der Hauptspeicher bestand in der Grundversion aus 4096 dieser Worte, was 9216 Oktetts entspricht.\n\nEs gibt einige wenige Grundbefehle. Die meisten anderen betreffen die Ausgabegeräte.\n\nGrundbefehle mit entsprechendem Code\n\nEinige Beispiele von anderen wichtigen Befehlen\n\nVorgänger war der TX-0 (1955/1956), der erste Computer mit Transistoren. Auf ihm liefen bereits Text-basierte Spiele, z. B. Tic-Tac-Toe.\n\nDas zweite Modell des PDP-1 wurde deshalb berühmt, weil es als Geschenk an das Massachusetts Institute of Technology (MIT) Cambridge ging und dort das berühmte Spiel Spacewar! von Steve Russell und anderen Studenten entwickelt wurde.\n\nNachfolger waren:\n\n\n"}
{"id": "244481", "url": "https://de.wikipedia.org/wiki?curid=244481", "title": "Apple iSight", "text": "Apple iSight\n\niSight ist eine Webcam des Computerherstellers Apple. Sie wurde erstmals am 23. Juni 2003 zusammen mit einer passenden Videokonferenz- und Chat-Software namens iChat AV auf der \"Worldwide Developers Conference\" vorgestellt. Im Juni 2006 wurde die erste Generation der Kamera in Deutschland aus dem Handel genommen.\n\nDie erste Generation der Kamera besitzt einen 1/4\"-Farb-CCD-Bildsensor mit VGA-Auflösung (640×480 Pixel) und 24-Bit-Farbspektrum. Sie hebt sich von den meisten Webcams durch die eingebaute Autofokusfunktion im Bereich von 50 Millimeter bis unendlich ab. Zudem besitzt die iSight ein drei-teiliges Objektiv aus zwei asphärischen Linsen mit einem Sichtwinkel von 54,3° und einer Blendenzahl von 2,8. Darüber hinaus hat sie ein eingebautes Voll-Duplex-Richtmikrofon mit Nebengeräuschunterdrückung. Das Gerät kann bei voller Auflösung bis zu 30 Einzelbilder pro Sekunde (30 fps) liefern. Der Anschluss an einen Mac erfolgt über FireWire. Die Kamera besitzt ein Gehäuse aus einer Aluminiumlegierung, wiegt ohne Halterung 63,8 Gramm und lässt sich sowohl drehen als auch neigen. Auf der Oberseite des Gehäuses befindet sich eine grüne Aufnahme-LED, an der sich erkennen lässt, ob die Kamera aktiv ist. Zum sicheren Schutz der Privatsphäre kann man mit einem Dreh am vorderen Objektivring die Kamera abschalten und komplett verdecken. Auffällig ist, dass die Kamera im Betrieb sehr warm wird. Daraus resultierende Probleme sind zwar nicht bekannt, Apple rät jedoch dazu, eine Umgebungstemperatur von weniger als 35 Grad Celsius sicherzustellen. Sie wurde von Apple für die Verwendung mit der Chatsoftware iChat AV entwickelt.\nDas Design der Kamera stammt von Apple in Kalifornien.\n\nBei den folgenden Generationen des Modells wurde die Kamera in das Bildschirmgehäuse des Apple-Gerätes eingebaut. Des Weiteren wurden diese Modelle technisch weiterentwickelt.\n\nDie Kamera wird zusammen mit einem Kunststoff-Schutzbehälter für selbige, einem weißen FireWire-Kabel von etwa 1,5 Meter Länge sowie vier Halterungen ausgeliefert. Mit diesen kann man das Gerät an verschiedenen Monitoren befestigen oder auf einer horizontalen Fläche abstellen. Durch den eingeschränkten Neigungswinkel nach oben kann man sie nur bedingt auf dem Tisch stehend einsetzen.\n\nDie iSight funktioniert außer mit Macs auch mit Windows- und Linux-PCs mit FireWire-Anschluss. Unter Windows reicht der in Windows XP eingebaute generische IEEE1394-WDM-Treiber von Microsoft, für Linux wird die \"libdc1394\"-Bibliothek benötigt. Beide Betriebssysteme können derzeit aber noch nicht mit dem eingebauten Mikrofon umgehen, für Linux existiert jedoch bereits ein Ansatz.\n\nSomit ist es auch Nicht-Mac-Besitzern möglich, die iSight mit Videotelefonie-Programmen zu benutzen. Verschiedene Kamera-Parameter wie beispielsweise Helligkeit, Kontrast, Farbe und Fokus lassen sich mit der Open-Source-Software Coriander dabei auch manuell einstellen. Ähnliches ist mit dem Plugin iGlasses auch bei der Verwendung von iChat AV möglich.\n\nDas Computermagazin \"c't\" testete in der Ausgabe 22/2003 mehrere Webcams unterschiedlicher Hersteller und bescheinigte der iSight \"„hervorragende technische Leistungen“\". Weiterhin wird die hohe Bildwiederholrate gelobt: \"„Dank FireWire übermittelt die Apple-Kamera souverän durchweg 30 fps, unabhängig von den Beleuchtungsverhältnissen. Selbst bei 640×480 Pixeln rauschen zwischen 29 und 30 Bilder pro Sekunde über den Draht.“\" Ebenso wie der vergleichsweise hohe Anschaffungspreis wird allerdings auch der \"„etwas übereifrige automatische Weißabgleich“\" kritisiert. Als einzige Kamera im Test hat die iSight einen Autofokus. Der Artikel macht keine Angabe zur Firmware des verwendeten Modells – man kann jedoch davon ausgehen, dass die Tester eine Kamera mit der Version 1.0 untersucht haben, da die entsprechende Ausgabe des Magazins am 22. Oktober 2003 erschienen ist, Aktualisierungen aber erst später verfügbar wurden.\n\nNachdem bereits in der letzten iMac G5 Revision eine iSight-Kamera integriert wurde, wurden im Zuge des Wechsels zur Intel-x86-Plattform in alle Geräte mit Bildschirm iSight-Kameras integriert.\nSomit verfügen nun folgende Modelle über eine iSight-Kamera:\n\nSeit längerem ist die iSight-Kamera vor allem in iPod touch, iPhone und iPad als Hauptkamera verbaut. Die Webcam heißt in den iOS- und Mac-Geräten nun FaceTime-HD-Kamera.\n\n"}
{"id": "245291", "url": "https://de.wikipedia.org/wiki?curid=245291", "title": "Windows Virtual PC", "text": "Windows Virtual PC\n\nWindows Virtual PC, ehemals \"Microsoft Virtual PC\" (bis 2009) und \"Connectix Virtual PC\" (bis 2003), ist eine Software für virtuelle Maschinen für diverse x86-basierte Gastbetriebssysteme.\n\nUrsprünglich wurde \"Virtual PC für Mac\" von Connectix für PowerPC-basierte Mac-OS-Systeme als Emulator für den Betrieb von Windows entwickelt. Nach der Übernahme durch Microsoft wurde der Emulator zusätzlich Bestandteil des Produktes \"Microsoft Office Professional für Mac\", wurde jedoch nach der Umstellung der Macintosh-Reihe auf Intel-x86-Prozessoren 2006 nicht weitergeführt.\n\nDie als virtuelle Maschine laufende Windows-Version wurde später kostenlos von Microsoft abgegeben, ebenso wie die bereits von Connectix begonnene Server-Variante \"Virtual Server\" für Windows von Microsoft frei verfügbar gemacht wurde. In Windows 7 ist die Technik unter dem Namen \"Windows XP Mode\" zudem ein optionaler Bestandteil des Betriebssystems. Unter Windows 8 ist \"Virtual PC\" nicht mehr lauffähig: Es wurde durch \"Hyper-V\" ersetzt.\n\n\"Virtual PC für OS/2\", das auf der Windows-Version basiert, wurde nach der Übernahme durch Microsoft nicht weitergeführt.\n\nUnter \"Windows Virtual PC\" können nur 16- und 32-Bit-Betriebssysteme ausgeführt werden.\n\n\"Virtual PC\" wurde ursprünglich von der Firma Connectix als Lösung entwickelt, x86-Betriebssysteme mit entsprechender Softwareanwendung auf Apple-Macintosh-Systemen mit PowerPC-Prozessoren zu benutzen. Dabei musste die Hardware eines Intel-PCs vollständig in Software emuliert werden, was einen erheblichen Teil der Rechenleistung beanspruchte. Nicht allzu anspruchsvolle PC-Programme konnten so aber auch problemlos auf einem Apple-Macintosh-System mit PowerPC-Prozessor ausgeführt werden.\n\nSpäter wurde eine als virtuelle Maschine ausgelegte Version von \"Virtual PC\" für x86-Systeme entwickelt. Dabei entfällt die Notwendigkeit einer aufwendigen Emulation, um x86-Maschinensprache auf der PowerPC-Plattform ausführen zu können, wodurch ein wesentlicher Geschwindigkeitsvorteil gegenüber der Mac-Version erzielt wird. Mit \"Virtual PC für Windows\" können unter Windows als Wirtsystem verschiedene x86-Betriebssysteme (darunter auch OS/2 bzw. eComStation) als Gastsysteme ausgeführt werden.\n\n2003 erwarb Microsoft die Virtual-PC-Technik von Connectix. Über die Details des Übernahmegeschäfts vereinbarten beide Seiten Stillschweigen. Microsoft entwickelte sowohl die PowerPC-basierte Mac-Version als auch die x86-basierte Windows-Version von \"Virtual PC\" unter eigenem Namen weiter. Mit dem Umstieg von Apple auf Intel-Prozessoren (Ende 2005/Anfang 2006) stellte Microsoft die Entwicklung der Mac-Version ein, nahm jedoch mit \"Virtual PC for Mac\" Version 7 noch eine Optimierung für den Apple Power Mac G5 vor; frühere Versionen sind auf G5-Prozessoren nicht ausführbar.\n\nDie Windows-Version wurde unter Connectix durch das deutsche Unternehmen Innotek auf OS/2 als Wirtsystem portiert. Nach der Übernahme wurde \"Virtual PC für OS/2\" von Microsoft kommentarlos aus dem Angebot genommen. Die kostenlos abgegebene Version \"2004 SP1\" von \"Virtual PC für Windows\" beinhaltet jedoch die für OS/2, womit man OS/2 weiterhin als Gast betreiben kann.\n\nDie Firma InnoTek arbeitete nach der Übernahme durch Microsoft weiter an der Linux-Unterstützung für \"Virtual PC\" und \"Virtual Server\" und entwickelte seit 2004 eine eigene Virtualisierungslösung namens VirtualBox. Diese ist seit 2007 auch in einer freien Version verfügbar. Entgegen dem Vorgehen von Microsoft wurde VirtualBox auch auf die Intel-Mac-Plattform portiert. InnoTek wurde Anfang 2008 von Sun Microsystems übernommen, das 2009 von Oracle aufgekauft wurde.\n\n2009 wurde die Windows-Version in \"Windows Virtual PC\" umbenannt und stärker in Windows integriert. So lassen sich virtuelle Computer nun im Windows-Explorer in einer speziellen Ansicht verwalten, ähnlich der Druckerverwaltung im Explorer. Ebenfalls neu war der für Windows 7 Professional, Enterprise und Ultimate verfügbare XP-Modus, welcher eine vorbereitete und registrierte (und aktivierte) Windows XP Professional-Installation bereitstellt und dort installierte Programme über die spezielle Gasterweiterung RemoteApp im Startmenü des Wirtsystems verfügbar macht.\n\nUnter Windows 8 lassen sich \"Windows Virtual PC\" als auch \"Virtual PC 2007\" und frühere Versionen nicht mehr installieren oder starten. Stattdessen ist dessen Nachfolger Hyper-V in den Server- und Business-Versionen von Windows enthalten. Die bessere Integration zeigt sich jedoch auch hier mit der Möglichkeit von Windows 8, direkt aus einem virtuellen Festplattenabbild heraus zu starten. Das neue Format für Festplattenabbilder codice_1 unterstützt bis zu 16 TB, während die ebenfalls noch verwendbaren codice_2-Abbilder nur maximal 2 TB groß werden können (→ Virtual-Hard-Disk-Format).\n\nMit \"Virtual PC\" wird ein kompletter PC virtualisiert bzw. emuliert. Das Programm stellt eine virtuelle Maschine zur Verfügung, innerhalb derer ein Standard-PC-Betriebssystem ablaufen kann. Dadurch wird es möglich, mehrere Betriebssysteme gleichzeitig auf nur einem PC zu betreiben. Als Gäste können 16- und 32-Bit-Betriebssysteme innerhalb eines 32- und 64-Bit-Wirtsystems laufen.\n\nUnter Mac OS auf der PowerPC-Plattform emuliert \"Virtual PC\" einen Standard-PC mit Pentium-II-Prozessor. Unter Windows und OS/2 auf der IA-32-Plattform wird der physisch existierende Prozessor des Wirtsystems virtualisiert. Als virtuelle Komponenten bietet \"Virtual PC\" bis zu drei Festplatten, die im VHD-Format eingebunden werden, ein CD- oder DVD-Laufwerk, Arbeitsspeicher einstellbarer Größe (abhängig von der Arbeitsspeicherkapazität des Wirtsystems), eine 100-MBit-Netzwerkkarte, eine Soundkarte und eine Grafikkarte. Unterstützung für PCI-Geräte fehlt. USB wird nur von \"Virtual PC für Mac\" ab Version 3.0 und von \"Windows Virtual PC\" ab Version 6.1 unterstützt.\n\nVirtual PC bietet grundsätzlich keine Möglichkeit, physische Partitionen oder Laufwerke an das Gastsystem durchzureichen. Nur bei dem virtuellen optischen Laufwerk kann entweder das real existierende Gerät oder ein ISO-Abbild für den Gast eingebunden werden.\n\nAls vollständiger Emulator ist nur die Version für Macintosh angelegt. Virtual PC für Mac gibt es nur für PowerPC-basierte Macs, nicht jedoch für Mac-Computer mit 68k-Prozessor (bis 1995) oder mit x86-Prozessor (seit 2006). Als Wirtsystem waren die Mac-OS-Versionen ab 7.5.5 (PowerPC wurde seit System 7.1.2 unterstützt) und Mac OS X bis Tiger/PowerPC (10.4, 2005; lauffähig auch auf Leopard/PowerPC, 10.5, 2007) vorgesehen; andere Betriebssysteme auf der PowerPC-Plattform werden nicht unterstützt.\n\nFolgende Hardware-Komponenten stehen dem Gastsystem in der Emulation zur Verfügung:\n\n\nDie Hauptvermarktung von \"Virtual PC\" auf den PowerPC-basierten Apple-Computern sieht das Ausführen eines Windows-, PC-kompatiblen DOS- oder Linux-Betriebssystems vor und bringt somit eine große Anzahl an Programmen auch auf PowerPC-Macs. Grundsätzlich läuft jedoch eine Vielzahl an x86-Betriebssystemen unter \"Virtual PC für Mac\", obwohl diese nicht offiziell unterstützt werden. Die enge Integration in das Wirtsystem ist dann wegen der fehlenden \"Virtual PC Additions\" jedoch nicht möglich.\n\n\"Virtual PC für Mac\" wurde vor allem auch mit vorinstallierten Betriebssystemen verkauft, darunter MS‑DOS mit Windows 3.11, PC‑DOS 2000, Windows 95, Windows 98 (auch \"Zweite Ausgabe\"), Windows Me, Windows 2000, Windows XP (Home und Professional) sowie RedHat Linux 6.1. Einerseits erwarb man dadurch eine Lizenz für das als Gast verwendete Betriebssystem gleich mit, andererseits entfällt durch das bereits installierte Gastsystem auf einem virtuellen Festplattenabbild eine eventuell zeitaufwändige Installation desselben.\n\nMit dem Emulator als Ausgangspunkt wurde auch eine als Virtualisierung ausgeführte Version von Virtual PC geschrieben, die nur für die Windows-Plattform gedacht war. Es gab jedoch auch eine auf OS/2 portierte Version, die auf der jeweiligen Windows-Version basierte.\n\nFolgende Hardware-Komponenten stehen dem Gastsystem in der virtuellen Maschine zur Verfügung:\n\n\nFür die Soundkarte und die USB-Schnittstelle stehen keine allgemeinen Treiber zur Verfügung, da die emulierten Komponenten nicht wie echte Hardware im Gastsystem angesprochen werden können. Die Soundkarte funktioniert daher nur mit den speziellen Treibern der , und damit nur unter unterstützten Windows-Gästen. Selbiges gilt für die virtuelle USB-Schnittstelle, die über einen \"Virtual PC\"-spezifischen internen Systembus realisiert wurde, der selbst ebenfalls nur von den verwendbar ist.\n\nFür die restliche emulierte Hardware können Standard-Treiber verwendet werden, sodass diese Komponenten unter einer Vielzahl weiterer Betriebssysteme verwendbar sind.\nDie , Treiber und Programme für die Integration des Gastsystems in das Wirtsystem, gibt es für MS-DOS, Windows 98 und Me, sowie für Windows NT 4.0, 2000 und XP. Sie sind identisch in jeder Variante von Virtual PC, ob nun der Emulator für Mac oder die virtuelle Maschine für Windows und OS/2 und funktionieren meist über die jeweilige Version hinaus: ein nicht mehr offiziell unterstütztes Gastbetriebssystem kann die älteren \"Virtual PC Additions\" auch in einer neueren Version von Virtual PC verwenden – neue Funktionen, die in dieser Version von Virtual PC hinzugekommen sind, werden damit jedoch nicht unterstützt.\n\nDa die für Linux verfügbaren von auch mit Virtual PC funktionieren ist eine Integration auch mit RedHat Linux und SuSE Linux möglich.\n\nMicrosoft kaufte 2003 die Firma Connectix und deren Produkt \"Virtual PC\" auf und integrierte \"Virtual PC\" in die eigene Produktpalette. Da nach Aussage eines Microsoft-Managers die Anpassung von \"Virtual PC für Mac\" an die Intel-Plattform einen zu großen Aufwand bedeutet hätte, wurde das Produkt mit Aufkommen der Intel-Macs fallengelassen. Die Windows-Version entwickelte Microsoft weiter und integrierte sie fortan immer tiefer in das hauseigene Betriebssystem Windows.\n\nIm Januar 2004 veröffentlichte Microsoft die Version 2004 bzw. 5.3. Für Kunden von \"Connectix Virtual PC 5\" war diese Aktualisierung gratis. Im Dezember folgte das Service Pack 1.\n\nSeit dem 12. Juli 2006 stellt Microsoft die Windows-Version kostenlos zur Verfügung.\n\nAm 19. Februar 2007 erschien die neue Version „Virtual PC 2007“, welche nun auch eine Unterstützung für Windows Vista bietet und seitdem ebenfalls kostenlos verfügbar ist. Allerdings ist hier zu beachten, dass offiziell nicht alle Versionen von Windows Vista unterstützt werden. Bei der Installation und Einrichtung des Programms auf bestimmten Vista-Versionen wird darauf hingewiesen, dass für diese nicht unterstützten Vista-Versionen kein Support-Anspruch besteht. Auf Windows Vista Home Premium z. B. läuft das Programm dennoch. Im Mai 2008 folgte das Service Pack 1, welches auch Windows XP Service Pack 3, Windows Vista Service Pack 1, sowie Windows Server 2008 offiziell unterstützt.\n\nZusammen mit der Markteinführung von Windows 7 im Herbst 2009 wurde auch „Virtual PC“ in einer neuen angepassten Version verfügbar gemacht. Es heißt nun „Windows Virtual PC“ und wurde sowohl optisch wie funktionell stärker in Windows integriert. Es ist ein optionaler Windows-Download und weiterhin frei verfügbar (gratis). Außerdem gibt es unter dem Namen „XP Mode“ eine Variante mit vorinstalliertem Windows XP unter den Windows-7-Versionen Professional, Ultimate und Enterprise. Die darüber installierten Programme werden ins Windows-7-Startmenü integriert, was dem ebenfalls frei herunterladbaren Zusatzprogramm \"RemoteApp\" für den Windows-XP-Gast geschuldet ist. Die ursprüngliche Systemvoraussetzung für „Windows Virtual PC“ in Form einer hardwareunterstützten Virtualisierung (Secure Virtual Machine: AMD-V und Intel VT; Microsofts Bezeichnung ist „HAV“) wurde im März 2010 mit einem Update aufgehoben.\n\nAls Host-Betriebssystem können für „Windows Virtual PC“ die folgenden Windows 7-Versionen verwendet werden: Windows 7 Home Basic, Windows 7 Home Premium, Windows 7 Professional, Windows 7 Ultimate, Windows 7 Enterprise.\n\nAls Gastbetriebssystem, also als Betriebssysteme, die virtualisiert werden, sind nur 32-Bit-Systeme vorgesehen. Der Support für 64-Bit-Systeme ist nicht vorhanden. Verwendet werden können unter „Windows Virtual PC“ offiziell folgende Windows-Betriebssysteme: Windows XP Service Pack 3 (SP3) Professional, Windows Vista Enterprise Service Pack 1 (SP1), Windows Vista Ultimate Service Pack 1 (SP1), Windows Vista Business Service Pack 1 (SP1), Windows 7 Professional, Windows 7 Ultimate, Windows 7 Enterprise. Meistens funktioniert aber auch die Virtualisierung sowohl anderer Windows-Editionen, als auch anderer Betriebssysteme (z. B. diverse unixoide Betriebssysteme wie Linux), bei der Installation bzw. Konfiguration sind dann aber möglicherweise kleinere Hürden zu überwinden.\n\n\"Virtual PC\" kann ab Windows 8 nicht mehr verwendet werden. Jedoch bietet Windows 8 die Möglichkeit, VHD-Festplattenabbilder direkt im Betriebssystem als virtuelle Laufwerke einzubinden. Unter Windows 8 Pro ist es zudem möglich, Windows selbst aus einem Festplattenabbild zu starten, jedoch mit dem Nachteil auf die Schnellstart-Funktion verzichten zu müssen.\n\nZur Entwicklung bietet \"Virtual PC\" eine flexible und wiederherstellbare Umgebung für Tests unter verschiedenen Betriebssystemen und Konfigurationen. So lässt sich ohne zusätzliche Hardware ein Programm unter diversen Windows-Betriebssystemen testen. Auch ist Virtual PC ideal, um zum Beispiel den Internet Explorer 6 und Internet Explorer 7 auf einem PC zu installieren, um zu testen, ob Websites mit beiden Versionen kompatibel sind. Beim Programmieren und Testen von Netzwerkprogrammen oder Client-Server-Anwendungen ist Virtual PC nützlich, indem es mit dem Host und einem virtuellen PC (oder zwei virtuellen) auf einer Hardware (unter anderem auch ein mobiler Laptop) die Möglichkeit gibt, einen Netzwerk-Datenverkehr zwischen mehreren Rechnern zu simulieren.\n\nEin virtueller PC kann dazu verwendet werden, unbekannte Programme ohne Risiko für das Hostsystem zu testen. Im schlimmsten Fall wird nur das Gastsystem beeinträchtigt. Bei Gefallen kann das Programm dann auf dem „echten“ System installiert werden.\n\nDer virtuelle PC kann auf Windows XP 64-Bit dazu benutzt werden, um 16-Bit-Setup-Programme aufzurufen, die unter der 64-Bit-Engine nicht mehr laufen. Somit ist es möglich, alte Spiele und Anwendungen zu installieren, die zwar als 32-Bit-Versionen vorliegen, aber dennoch einen alten Installer verwenden.\n\nAuf dem Apple Macintosh liegt der Schwerpunkt eher darin, dass Windows-Programme auch auf einem Apple Macintosh lauffähig gemacht werden können, sowie für Webentwickler, welche ihren Code auf dem Internet Explorer testen müssen. Mithilfe von Virtual PC lassen sich auch diverse Linux-Varianten in das bestehende Windows-Betriebssystem einbinden; Microsoft bietet dafür allerdings (noch) keine offizielle Unterstützung.\n\nObwohl Linux als Gastsystem nicht offiziell unterstützt wird, lassen sich viele Linux-Distributionen trotzdem ohne größere Probleme installieren. Neuere Linux-Kernel (2.6) erkennen in Virtual PC in vielen Fällen die AUX-Schnittstelle (/dev/psaux) nicht, die für die Maussteuerung benötigt wird. Es gibt verschiedene Möglichkeiten, dieses Problem zu beheben. Eine der einfachsten Lösungen ist es, die Kernelparameter i8042.noloop und psmouse.proto=imps in die Konfiguration der Bootmanager GRUB oder LILO einzutragen.\n\nDa der Mainstream-Support der letzten Version von \"Virtual PC für Mac\" am 13. April 2010 endete, gibt es keine Unterstützung seitens Microsoft für dieses Produkt mehr. Da auch Apple die Unterstützung für PowerPC-basierte Apple-Computer und die Betriebssysteme, die auf diesen PCs liefen, eingestellt hat, kann auf der ganzen Linie von Abandonware gesprochen werden – es ist jedoch zu beachten, dass das Urheberrecht dadurch nicht betroffen ist.\n\"Windows Virtual PC\" wurde in Windows 8 durch dessen Nachfolger \"Hyper-V\" ersetzt; es ist somit als Wirt auf Windows-Versionen neuer als Windows 7 nicht mehr lauffähig.\nEs gibt keinen Support mehr für \"Virtual PC für OS/2\", das neben OS/2 auch auf dessen Weiterentwicklung eComStation lauffähig ist. \"Virtual PC für OS/2\" ist inzwischen als Abandonware zu bezeichnen – das Urheberrecht ist davon jedoch nicht betroffen.\nProdukte, die in direkter Konkurrenz zu \"Virtual PC\" stehen:\n\nWeitere Virtualisierungsprodukte:\n\nVerwandte Themen:\n\n"}
{"id": "245683", "url": "https://de.wikipedia.org/wiki?curid=245683", "title": "Keylogger", "text": "Keylogger\n\nEin Keylogger (dt. „Tasten-Protokollierer“) ist eine Hard- oder Software, die dazu verwendet wird, die Eingaben des Benutzers an der Tastatur eines Computers zu protokollieren und damit zu überwachen oder zu rekonstruieren. Keylogger werden beispielsweise von Crackern, Nachrichtendiensten oder Ermittlungsbehörden verwendet, um an vertrauliche Daten – etwa Kennwörter oder PINs – zu gelangen. Ein Keylogger kann entweder sämtliche Eingaben aufzeichnen oder gezielt auf Schlüsselwörter wie z. B. Zugangscodes warten und dann erst aufzeichnen, um Speicherplatz zu sparen.\n\nSoftware-Keylogger schalten sich zwischen Betriebssystem und Tastatur, lesen die Tastendrucke und geben sie an das Betriebssystem weiter. Manche Keylogger speichern die Eingaben auf der Festplatte des überwachten Rechners, andere senden sie über das Internet an einen anderen Rechner.\n\nEin Beispiel ist die Software \"inputlog\" der Universität Antwerpen, die zur wissenschaftlichen Untersuchung des Schreibprozesses verwendet wird.\n\nHardware-Keylogger erfordern einen unmittelbaren physischen Zugang zu dem betroffenen Computer. Sie werden in Situationen verwendet, in denen eine Installation von Software-Keyloggern nicht möglich, nicht sinnvoll oder zu aufwändig ist. Hardware-Keylogger werden direkt zwischen Tastatur und Rechner gesteckt und können somit innerhalb von Sekunden angebracht werden. Geräte, welche die ausgespähten Daten in einem integrierten Speicher (RAM, EPROM etc.) ablegen, werden später dann wieder entfernt. Die von ihnen protokollierten Eingaben werden dann an einem anderen Computer ausgelesen. Andere Techniken versenden die mitprotokollierten Daten über Netzwerke oder per Funk. Die einfachste Möglichkeit Hardware-Keylogger zu erkennen, besteht darin, die eingesetzte Hardware (primär Tastatur sowie Verbindungskabel zwischen Tastatur und Computer) zu untersuchen. Jedoch besteht für viele der erhältlichen Modelle auch die Möglichkeit, diese mittels Software zu erkennen.\n\nIn Deutschland ist der unerlaubte (ohne Einverständnis erfolgende) Einsatz von Keyloggern an fremden Computern als Ausspähen von Daten gemäß des Strafgesetzbuches strafbar. Unternehmen, die Keylogger an den Firmencomputern einsetzen wollen, müssen zuvor die Zustimmung des Betriebsrats einholen. Zudem darf gemäß Ziffer 22 des Anhangs zur Bildschirmarbeitsverordnung \"„ohne Wissen der Benutzer […] keine Vorrichtung zur qualitativen oder quantitativen Kontrolle verwendet werden“\".\n\nSchützen kann man sich vor Hardware-Keyloggern mit einer virtuellen Tastatur. Die Eingaben dieser Bildschirmtastatur zeichnet der Keylogger nicht mit auf. Deshalb ist es ratsam, diese zu nutzen, um sich vor Hardware-Keyloggern zu schützen. Gegen Softwarekeylogger bietet dies allerdings keinen Schutz.\n\nZum Schutz vor Software-Keyloggern sollte man sein System mit Anti-Spyware-Programmen oder aktuellen Virenscannern auf dem neuesten Stand halten. Auf fremden Rechnern kann man sich gegen Tastaturlogger durch gezielte „Durchwürfelung“ der Tastatureingaben schützen: Wenn man sich z. B. auf einer Webseite anmeldet, entfernt man zwischen den einzelnen Zeicheneingaben des Passwortes den Fokus des Cursors, indem man auf eine freie Stelle der Webseite klickt, tippt die verunreinigenden Buchstaben ein und tippt am Passwort weiter. Grenzen und eine genauere Beschreibung der Methode sind in „How To Login From an Internet Cafe Without Worrying About Keyloggers“ (Weblinks) zu finden.\n\n\n"}
{"id": "248306", "url": "https://de.wikipedia.org/wiki?curid=248306", "title": "Pro-Linux", "text": "Pro-Linux\n\nPro-Linux ist mit 7,5 Mio. Seitenaufrufen im Monat eines der größten deutschsprachigen Internet-Portale zum Thema GNU/Linux und freie Software.\n\nDie Seiten des Portals umfassen einen Nachrichtenticker, Hintergrundartikel, Testberichte, Workshops und Anleitungen sowie Foren. Die Zielgruppe sind dabei sowohl Anfänger als auch Fortgeschrittene und Experten. Das Portal, dessen Informationen und Dienste allesamt frei und kostenlos zugänglich sind, wird von Freiwilligen geführt und mit Inhalten gefüllt.\n\nAnfang 1999 ging das von Mirko Lindner gegründete Portal unter dem Namen \"linuxworld\" ans Netz, wurde aber nach Problemen mit der Domain \"linuxworld.de\" in \"4linux\" umbenannt. Im April 1999 einigten sich die Betreiber dann auf den Namen \"Pro-Linux\", der bis heute geblieben ist.\nAb dem Jahr 2000 folgten Stände auf verschiedenen Messen, unter anderem dem LinuxTag, die Seite wurde ausgebaut, ergänzt und bekam 2002 und 2004 ein neues Layout. Im Januar 2010 wurde ein überarbeitetes Design und das in Eigenregie entwickelte CMS \"NewsBoard3\" (NB3) vorgestellt.\n\nPro-Linux veröffentlichte über 21.000 Nachrichten (Stand Februar 2015). Pro-Linux bietet eine Anwendungsdatenbank die redaktionell betreut wird und über 19.000 Applikationen mit über 130.000 Versionen auflistet.\n\n"}
{"id": "249124", "url": "https://de.wikipedia.org/wiki?curid=249124", "title": "Portable Game Notation", "text": "Portable Game Notation\n\nPortable Game Notation (PGN) ist ein als Text lesbares Datenformat zur Speicherung von Schachpartien. Es wurde 1994 von Stephen J. Edwards entwickelt, um den Austausch von Schachdaten zwischen verschiedenen Schachprogrammen (zum Beispiel über das Internet) zu ermöglichen und zu vereinfachen.\n\nDas PGN-Format verwendet Zeichen aus dem ISO-8859-1-Zeichensatz (umfasst ASCII-Zeichen sowie viele Sonderzeichen westeuropäischer Sprachen) und besteht aus zwei Teilen: den Metadaten und den Zügen. Im ersten Teil, den Metadaten, werden Angaben wie Turnier, Ort, Datum, Runde, Spielernamen, Ergebnis und andere Informationen in normierten Feldern erfasst. Die Notation der Züge erfolgt größtenteils in der \"Standard Algebraic Notation\" (SAN): Hierbei handelt es sich um die allgemein übliche verkürzte algebraische Notation, wobei die Buchstaben der englischen Figurenbezeichnungen (K für King (König), Q für Queen (Dame), R für Rook (Turm), B für Bishop (Läufer) und N für Knight (Springer)) verwendet werden. Kommentare werden in geschweifte Klammern codice_1 eingeschlossen. Als Ausnahme von der Standardnotation wird die Rochade nicht mit 0–0 bzw. 0–0–0 (Ziffer Null), sondern mit O-O bzw. O-O-O (Buchstabe O) notiert.\n\nDas Format ist nicht proprietär, es kann daher von fast allen Schachprogrammen gelesen werden. Oft ist auch ein Export einer Partie ins PGN-Format möglich. Es können auch mehrere Partien in einer einzigen PGN-Datei gespeichert werden.\n\nPGN-Daten können mit der Abfragesprache Chess Query Language (CQL) durchsucht werden. Viele gängige Schachprogramme beherrschen mindestens die Suche nach den zu Partien gespeicherten Metadaten wie Spielernamen sowie Stellungen oder Stellungsausschnitten in PGN-Dateien.\n\n [Event \"IBM Kasparov vs. Deep Blue Rematch\"]\n\n\n"}
{"id": "250463", "url": "https://de.wikipedia.org/wiki?curid=250463", "title": "JACK Audio Connection Kit", "text": "JACK Audio Connection Kit\n\nJACK Audio Connection Kit oder JACK ist ein Daemon bzw. eine Software-Schnittstelle für Audio-Computerprogramme unter Unix-ähnlichen Systemen. Der Name ist ein rekursives Akronym.\n\nDer JACK-Daemon verwaltet die Ein- und Ausgänge von Audioprogrammen (z. B. Sequenzer, Software-Synthesizer) und Audio-Hardware (z. B. Mikrofoneingang, Audioausgang zum Lautsprecher) und routet die Audiosignale zwischen ihnen. So können die einzelnen Elemente des computergestützten Tonstudios wie in einem herkömmlichen Studio intuitiv miteinander verbunden werden.\n\nDer JACK-Server synchronisiert die Clients, indem er zu festen Zeiten Callback-Funktionen aufruft, die einen Block von Audiodaten lesen oder schreiben.\n\nBei Programmen, die nicht speziell für den Einsatz mit JACK programmiert sind, ist es durch virtuelle Adapter auch möglich, Software für ALSA oder OSS unter Jack zu verwenden. So können inkompatible oder auch alte Programme mit JACK benutzt werden.\n\nEine besondere Eigenschaft von JACK ist die niedrige Latenzzeit. Um diese vollständig auszuschöpfen, ist ein moderner Rechner und evtl. ein modifizierter Linux-Kernel nötig, der über Ingo Molnárs und Thomas Gleixners Realtime-Patch verfügt.\n\nMit Qjackctl und Patchage gibt es benutzerfreundliche Qt/GTK-basierte Oberflächen zum Einrichten und Handhaben von JACK. Das Programm verwaltet zusätzlich noch das MIDI-Routing unter ALSA.\n\nJACK funktioniert mit ALSA, PortAudio, FFADO/FreeBob und OSS als Backend und ist für alle POSIX-konformen Betriebssysteme verfügbar, wie GNU/Linux, macOS und *BSD. Die Jack Library ist unter der LGPL frei erhältlich, der Rest steht unter der GPL.\n\nNeben dem klassischen Jack1 setzt sich seit 2010 immer mehr die Variante Jack2/jackdmp durch. Jack2 unterstützt zur Laufzeit auch jedes für Jack1 gebaute Programm, ist aber besonders für moderne Multiprozessor-Computer optimiert.\n\n\n\n"}
{"id": "250658", "url": "https://de.wikipedia.org/wiki?curid=250658", "title": "GermaNet", "text": "GermaNet\n\nGermaNet ist ein von der Universität Tübingen betriebenes maschinenlesbares lexikalisch-semantisches Netz der deutschen Sprache. Ein Teil davon ist in EuroWordNet integriert, das die Wortnetze mehrerer europäischer Sprachen über einen interlingualen Index miteinander verbindet.\n\n\nAngestrebt ist auch eine Kompatibilität zu WordNet.\n\nGermaNet umfasst zurzeit (Mai 2017) 154.814 lexikalische Einheiten, die in ca. 120.032 Synonymenmengen (\"synsets\") zusammengefasst sind. Die lexikalischen Einheiten und Synsets sind durch 4.210 lexikalische und 133.652 konzeptuelle Relationen miteinander verbunden, wobei die taxonomische Relation der Über- und Unterordnung (\"Hyperonymie\" und \"Hyponymie\") von Konzepten die wichtigste Relation ist.\n\n\"GermaNet\" fußt auf dem Ansatz von WordNet, das 1985 von der Princeton-Universität begonnen wurde.\nEs entstand als \"GermaNet I\" in den Jahren 1996 und 1997, also etwa zur gleichen Zeit wie \"EuroWordNet I\", das 1997 bis 1999 weiter entwickelt wurde zu \"EuroWordNet II\". GermaNet wird bis heute als monolinguale lexikalische Ressource gepflegt und ausgebaut. Seit Mai 2017 ist Version 12.0 erhältlich.\n\nDie Nutzung von GermaNet ist für akademische Benutzer kostenfrei, zusätzlich kann man die Daten kostenlos bei canoonet nachschlagen.\n\n\n"}
{"id": "251964", "url": "https://de.wikipedia.org/wiki?curid=251964", "title": "AppleScript", "text": "AppleScript\n\nAppleScript ist eine Skriptsprache von Apple. Sie ist Bestandteil des Betriebssystems Mac OS. AppleScript dient dazu, Mac-OS-Programme zu automatisieren, zu erweitern und innerhalb eines Rechnernetzes fernzusteuern.\n\nDie erste Version von AppleScript basierte auf dem HyperCard-Projekt. HyperCard beinhaltete mit der Skriptsprache HyperTalk eine auf der englischen Sprache basierende Skriptsprache. Dadurch sind AppleScript-Programme auch für Personen, die nicht programmieren können, relativ einfach zu verstehen.\n\nAppleScript ist an die natürliche englische Sprache angelehnt. Der Aufruf einer objektorientierten Methode geschieht wie folgt:\nAuf Deutsch würde es so heißen:\n\nIn Java dagegen wie folgt:\nString[] x = {\"one\",\"two\",\"three\"};\nchar y = x[1].charAt(0);\nAppleScript wird primär dazu benutzt, andere Programme von einem Programm aus „fernzubedienen“. Mac OS bietet AppleEvents an, ein applikationsübergreifendes Kommunikationsprotokoll, mit dem es möglich ist, Nachrichten von einer Applikation zu einer anderen Applikation zu senden, so dass diese sich wie gewünscht verhält.\n\nSo kann man zum Beispiel aus einem Skript heraus eine Applikation anweisen, ein bestimmtes Dokument zu öffnen. Mit dem Wort „tell“ wird eine solche Nachricht z. B. an Word geschickt:\ntell application \"Microsoft Word\"\nend tell\nAuf Deutsch:\n\nAppleScript ist nicht nur für applikationsübergreifende Aufgaben entworfen worden. Es kann u. a. auch eingesetzt werden, um häufig wiederkehrende Aufgaben zu automatisieren oder Berechnungen durchführen zu lassen. Ein Beispiel:\nset pix to 72\nset answer to text returned of (display dialog \"Enter the length in inches\" default answer \"1\")\ndisplay dialog answer & \"in = \" & (answer * pix) & \"px\"\nAuf Deutsch würde es so heißen:\n\nEs zeigt ein Dialogfeld und fragt nach der Länge in Zoll. Es wird dann berechnet, wie viele Pixel es auf dieser Länge geben wird. Ein zweites Dialogfeld zeigt dann das Resultat.\nApple ließ für eine kurze Zeit AppleScript sogar so erweitern, dass man die Skripte nicht zwingend auf Englisch schreiben musste. Man sollte die Skripts in der eigenen Muttersprache schreiben können. So wurde die Skriptterminologie auf Französisch, Japanisch und Italienisch übersetzt und Mac OS konnte diese von einem Dialekt in einen anderen Dialekt übersetzen.\nDies funktionierte zwar, doch die Entwickler von Macintosh-Software (außerhalb von Apple) unterstützten die Mehrsprachigkeit von AppleScript wegen des großen Aufwandes nur unzureichend. Da auch der Support umständlich war, wurde diese Idee in Mac OS 8.5 und höher nicht mehr weiter verfolgt.\n\n"}
{"id": "253040", "url": "https://de.wikipedia.org/wiki?curid=253040", "title": "Squid", "text": "Squid\n\nSquid (engl. für „Kalmar“) ist ein freier Proxyserver und Web-Cache, der unter der GNU General Public License steht. Er zeichnet sich vor allem durch seine gute Skalierbarkeit aus. Squid unterstützt die Netzwerkprotokolle HTTP/HTTPS, FTP über HTTP und Gopher.\n\nSquid-Server können sowohl für sehr kleine Netze (5–10 Benutzer) als auch für sehr große Proxyverbunde in Weitverkehrsnetzen mit mehreren hunderttausend Benutzern eingesetzt werden.\n\nSquid hat sich ebenfalls als \"transparenter Proxy\" bei ISPs (Internet Service Provider) bewährt. In dieser Funktion werden alle Anfragen von Kunden über den Proxy geleitet, was zur Beschleunigung der Datenübertragung sowie zur Reduktion der Datenrate des Providers führt. Häufig wird Squid auch als Reverse Proxy zum Schutz und zur Beschleunigung von Webservern eingesetzt. Ab Version 2.6 läuft Squid auch als HTTPS-Proxy. Damit wird die SSL-Verschlüsselung vom Webserver auf den Proxy verlagert.\n\nEr kann auch mittels zusätzlicher Redirector-Software eine Filterfunktion wahrnehmen. Damit werden bestimmte Seiten oder Seiteninhalte nicht dargestellt, sondern stattdessen eine Ersatzseite oder Ersatzgrafik angezeigt. Dies wird oft zur Vermeidung von Werbeinhalten, aber auch zur Zensur von Webinhalten eingesetzt. Der SquidGuard, der mit Squid ausgeliefert wird, ist ein Programm, das diese Weiterleitungs-Funktionalität (in Squid mit ‚url_rewriter‘ bezeichnet) anwendet. Ein alternatives Produkt ist UfdbGuard, das zusätzlich zu den Filterfunktionen auch Erkennung und Blocken von Tunneln und nicht autorisierten Zertifikaten beherrscht.\n\nIn beschränktem, aber für praktische Zwecke oft ausreichendem Maß kann Squid über sogenannte „Delay-Pools“ auch zur Bandbreitenkontrolle eingesetzt werden. Dabei können Benutzerklassen (genauer gesagt Adressbereiche) definiert werden, die unterschiedliche Anteile bei der Leitungsnutzung haben. Einer Benutzerklasse kann ein Vorrat an Kapazität (Buckets) nach dem Token-Bucket-Algorithmus zugeordnet werden, der mit voller Datenrate aufgebraucht werden darf. Dadurch tritt die Beschränkung erst bei intensiver Nutzung auf, während bei „normaler“ Nutzung aus der Sicht des Anwenders die volle Datenrate zur Verfügung steht. Es ist konfigurierbar, wie viele nichtbenutzte Buckets eine Klasse ansparen kann, um bei Spitzen mit voller Datenrate bedient werden zu können. Was nicht möglich ist, ist die Zuordnung der vollen Kapazität, falls diese gerade verfügbar ist.\n\nIn gewissem Umfang kann Squid auch anonymisieren, indem bestimmte Header-Zeilen einer Web-Anfrage entfernt werden.\n\nDa Squid sowohl das Internetprotokoll in der Version 4 als auch in der Version 6 gleichzeitig (Dual-Stack) beherrscht, kann er auch zwischen beiden Protokollen übersetzen.\n\nEin gutes Beispiel für das Einsatzgebiet von Squid-Caches ist Wikipedia. Derzeit hat das Wikimedia-Projekt zirka 115 Squids weltweit im Einsatz. Diese Reverse Proxys halten viele Seiten im Speicher, damit häufige oder wiederholte Anfragen nicht immer von den Web- und Datenbankservern bearbeitet werden müssen. Diese Systeme bedienen hauptsächlich Seitenaufrufe von unangemeldeten Benutzern und halten über 75 % der Datenanfragen von den Webservern fern; die Effizienz der Webserver wird durch die Squid-Caches also mehr als vervierfacht.\n\nSquid wurde ursprünglich von Duane Wessels im Rahmen des \"Harvest project\" der University of Colorado at Boulder als der „Harvest object cache“ entwickelt. Weitere Arbeit an der Software wurde an der University of California, San Diego vorgenommen und mit zwei Zuschüssen der National Science Foundation finanziert. Duane Wessels führte eine Projektabspaltung von der letzten nicht-kommerziellen Version von Harvest durch und gab diesem Zweig den heutigen Namen \"Squid\", um Verwechslung mit der anderen, kommerziell weitergeführten Version zu vermeiden, die sich \"Cached 2.0\" nannte, aus der wiederum \"NetCache\" entwickelt wurde. Version 1.0.0 von Squid wurde im Juli 1996 veröffentlicht.\n\nSquid wird mittlerweile fast ausschließlich in freiwilliger Arbeit weiterentwickelt.\n\nEs existieren grafische Benutzeroberflächen (GUI) zur Administration des Servers, zum Beispiel:\n\n"}
{"id": "253516", "url": "https://de.wikipedia.org/wiki?curid=253516", "title": "PostGIS", "text": "PostGIS\n\nPostGIS ist eine Erweiterung für die objektrelationale Datenbank PostgreSQL, die geografische Objekte und Funktionen umfasst. PostgreSQL mit PostGIS bildet eine Geodatenbank, die in Geoinformationssysteme eingebunden werden kann. Das Projekt implementiert die Simple-Feature-Access-Spezifikation des Open Geospatial Consortium und wird von der Open Source Geospatial Foundation betreut.\n\nPostGIS wird seit 2000 von Refractions Research entwickelt. Obschon PostgreSQL selber bereits Geometrietypen unterstützt, reichen diese laut den Entwicklern nicht aus, um räumliche Daten umfassend speichern und analysieren zu können. PostgreSQL bietet die Möglichkeit eigene Datentypen zu definieren. Diese guten Voraussetzungen waren die Hauptmotivation, die Entwicklung einer räumlichen Erweiterung für PostgreSQL zu starten.\n\nDie erste Version wurde im Mai 2001 unter der GNU General Public License veröffentlicht und trug die Nummer 0.1. Diese frühe Version verfügte über die Möglichkeit, räumliche Objekte zu laden und abzufragen, über einfache räumliche Funktionen, über einen räumlichen Index zum schnellen Zugriff auf die Daten und über eine JDBC-Erweiterung zur Verbindung mit Java.\n\nMit der Version 0.5 wurde PostGIS um das OpenGIS-\"Well-known-Binary\"-Format ergänzt und die Namen bestehender Funktionen wurden den OpenGIS-Spezifikationen angepasst.\n\nIn den nachfolgenden Versionen wurden vor allem zahlreiche räumliche Funktionen und Operatoren hinzugefügt. Refractions Research begann diese Funktionen in einer eigenständigen Bibliothek mit dem Namen GEOS zusammenzufassen, welche die OpenGIS-Spezifikationen berücksichtigt und eine C++ Portierung der \"JTS Topology Suite\" ist. Ab Version 0.8 benutzt PostGIS die GEOS-Bibliothek.\n\nDie nächsten Entwicklungen zielten vor allem in Richtung einer effizienteren Speicherverwaltung. Die neue Datenstruktur wurde \"light-weight geometry\" genannt und ab Version 1.0 wird ausschließlich diese verwendet.\n\nDie aktuelle Entwicklung von PostGIS berücksichtigt verstärkt die ISO-SQL/MM-Spezifikation, die mehr Geometrietypen wie z. B. Kurven implementiert als die OpenGIS-Spezifikation.\n\nMit der Version 2.0 wird das seit der PostgreSQL 9.1 bestehende PostgreSQL-Extensions-System genutzt.\n\nPostGIS unterstützt die folgenden Geometrietypen:\n\nVon PostGIS unterstützte Funktionen und Operatoren:\n\nDer Zugriff auf PostGIS erfolgt mit den gleichen Werkzeugen wie der auf PostgreSQL-Datenbanken. Beispiele für Open-Source-Programme zur Datenbankverwaltung sind \"psql\" oder \"pgAdmin\". Alle vorhandenen PostgreSQL-Schnittstellen zu verschiedenen Programmiersprachen können ebenfalls für den Zugriff auf PostGIS-Datenbanken verwendet werden, wie z. B. eine Java/JDBC-Verbindung oder die C-Bibliothek \"libpq\".\n\nEine Reihe von GIS-Programmen oder -Bibliotheken unterstützen PostGIS nativ, wie z. B. GeoTools, MapServer, QGIS oder GDAL/OGR.\n\nDaneben gibt es die Kommandozeile-Programme \"pgsql2shp\" und \"shp2pgsql\", welche zur Konvertierung von ESRI-Shapefiles zu PostGIS-Tabellen und umgekehrt eingesetzt werden. Ebenso können mithilfe von \"osm2pgsql\" OpenStreetMap-Daten importiert werden.\n\nWKT Raster ist eine Erweiterung zur Unterstützung von Raster-Daten in PostGIS. Wesentliches Merkmal dieser Erweiterung ist die Einführung eines \"Raster\"-Datentyps analog dem bestehenden \"Geometry\"-Datentyp.\n\nMit dem Modul pgRouting können PostGIS-Datenbanken Funktionalitäten zur Routenplanung hinzugefügt werden. PgRouting ermöglicht das Erstellen von Topologien und Lösen von folgenden Problemen:\n\n"}
{"id": "253913", "url": "https://de.wikipedia.org/wiki?curid=253913", "title": "Pixel-Art", "text": "Pixel-Art\n\nPixel-Art ist eine Stilrichtung in der Computerkunst, die Rastergrafiken verwendet und dabei das beschränkte Auflösungsvermögen von Bildschirmen als Stilmittel bewusst herausstellt. Sie nimmt dabei – teils ironisch – Bezug auf die Anfänge der Bildschirmgrafik von Video- und Computerspielen sowie der grafischen Benutzeroberflächen von Computern in den 1980er und frühen 1990er Jahren.\n\nDer Begriff \"Pixel-Art\" wurde von Adele Goldberg und Robert Flegal vom Xerox Palo Alto Research Center 1982 zum ersten Mal verwendet. Das Konzept an sich hingegen gab es schon etwa 10 Jahre vorher, zum Beispiel in Richard Shoups SuperPaint (1972).\nDie Ästhetik der frühen Bildschirmgrafik war technisch durch Rechenkapazität und Speicherplatz der damaligen Computer bestimmt. Anfangs bestanden die Grafiken aus reinen Schwarz-Weiß-Bitmaps, also einer definierten Anordnung von schwarzen und weißen Bildpunkten (Pixeln). Grautöne konnten nur grob durch überdeutlich sichtbare Schwarz-Weiß-Raster simuliert werden, schräge und gebogene Linien waren als treppenartig abgestuft zu erkennen. Im Laufe der Jahre wurde es möglich, die Bildschirmgrafik parallel zur steigenden Leistungsfähigkeit der Computer immer aufwändiger zu gestalten.\n\nDie Kapazität moderner Computersysteme erlaubt eine Bildschirmdarstellung mit Millionen von Farbnuancen, die auch für das sogenannte Antialiasing verwendet werden, das heißt ein Verschleiern der bis heute unverändert groben Bildschirmauflösung durch automatisch berechnete Farb- und Helligkeitsabstufungen an den Rändern kontrastierender Flächen, die die optische Illusion von „echten“ Rundungen, Diagonalen und Farbverläufen ermöglichen. Der Preis dafür ist ein leicht unscharfer Bildeindruck und ein um Größenordnungen höherer Rechenaufwand.\n\nPixel-Art verzichtet – zumindest in ästhetischer Hinsicht – auf diese neuen Möglichkeiten und verwendet bewusst nur die Mittel der frühen Bildschirmgrafik. Neben einem deutlichen nostalgischen Aspekt spielen dabei die Ökonomie der Mittel und das Ideal der „Ehrlichkeit“ dieser Mittel eine Rolle. Auch manche Formen der traditionellen Kunst, wie Kreuzstich oder Mosaik, zeigen durch das „Konstruieren“ von Bildern mit kleinen farbigen Einheiten eine gewisse Ähnlichkeit zur Pixel-Art auf.\n\nPixel-Art wird von anderen Formen der digitalen Kunst durch das manuelle Bearbeiten auf Pixelniveau (oft mit hoher Vergrößerung und fast immer ohne die Anwendung von Grafikfiltern, automatischem Antialiasing oder speziellen Rendermodi) unterschieden. In dieser Form wird allgemein gesagt, dass „jedes Pixel sorgfältig gesetzt“ wurde, um ein gewünschtes Resultat zu erzielen.\n\nPuristen in der Pixel-Art-Szene sind der Meinung, dass „richtige“ Pixel-Art nur mit Werkzeugen erstellt werden sollte, die einzelne Pixel setzen und dass Pixel-Künstler daher alle anderen Werkzeuge, beispielsweise das Linien-, Bézierkurven-, Kreis- oder Rechteck-Werkzeug, vermeiden sollten. Andere widersprechen, dass Werkzeuge wie \"Linie\" oder \"Füllen\" akzeptabel seien, da ihre Funktionen genau so einfach, wenn nicht sogar genauso schnell, auf der Basis einzelner wiederholt werden könnten.\n\nAufgrund dieser Regel werden Grafikfilter (Weichzeichnen, Alpha Blending) oder Werkzeuge mit automatischem Antialiasing generell nicht als gültige Werkzeuge für Pixel-Art anerkannt, weil solche Werkzeuge automatisch neue Pixelwerte berechnen, was im Gegensatz zum präzisen, manuellen Anordnen der Pixel, das mit „richtiger“ Pixel-Art verbunden wird, steht.\n\nDer Begriff „Pixel-Art“ wird daneben häufig als werblicher Ausdruck für die allgemeine Bearbeitung digitaler Bilddaten verwendet.\n\n\"Siehe auch:\" Sprite (Computergrafik)\n\nZeichnungen beginnen üblicherweise mit der sogenannten Lineart, die die Grundlinie für den Charakter, das Gebäude oder den Gegenstand, den der Künstler zu zeichnen beabsichtigt, darstellt. Linearts werden normalerweise über gescannte Zeichnungen gewonnen und oft unter Pixel-Künstlern weitergegeben. Auch andere Techniken existieren, manche ähneln dem Malen.\n\nDie oft in die Pixel-Art eingebundene, begrenzte Farbpalette fördert den Gebrauch des Ditherings um verschiedene Schattierungen und Farben zu erzeugen, allerdings wird dies, passend zur Natur dieser Kunstform, komplett per Hand gemacht. Außerdem wird auch „handgemachtes“ Antialiasing verwendet.\n\nHier sind drei Ausschnitte des oberen Bildes von „The Gunk“ vergrößert; sie stellen einige der Techniken dar:\nPixel-Art wird vorzugsweise in einem Dateiformat gespeichert, das verlustfreie Datenkompression verwendet, sodass jeder einzelne Pixel exakt abgespeichert und zurückgeholt werden kann.\n\nGIF und PNG sind zwei platzsparende Dateiformate, die häufig zum Speichern von Pixel-Art benutzt werden. Speichert man Pixel-Art hingegen im JPEG-Dateiformat, kann das Kunstwerk Schaden nehmen. Dies liegt am verlustbehafteten Kompressionsalgorithmus und Chroma Subsampling der JPEG-Norm. Das BMP-Format wird aufgrund seiner schlechten Kompression und unzulänglichen Unterstützung von Plattformunabhängigkeit auch vermieden. Soll die Grafik tatsächlich in Computerspielen eingesetzt werden, so ist der De-facto-Standard das – an den Grafikspeicher angelehnte – PCX-Format, das zwar schlecht komprimiert, aber sehr einfach einzulesen ist.\n\nPixel-Art wird normalerweise in zwei Unterkategorien aufgeteilt: \"Isometrisch\" und \"Nicht-isometrisch\".\n\nDie isometrische Art wird in einer isometrie-nahen dimetrischen Projektion gezeichnet. Allgemein wird dies in Computerspielen gemacht, um eine 3-Dimensionale Ansicht zu erstellen, ohne wirkliche 3D-Grafik zur Verfügung zu stellen. Technisch gesehen müsste ein isometrischer Winkel bei 30/45 Grad von der Horizontalen liegen, jedoch gibt dies kein gutes Ergebnis, da die Pixel in diesen Linien keinem „ordentlichen Muster“ folgen. Um dies zu beheben, werden Linien mit einem Seitenverhältnis von 1 zu 2 Pixeln gewählt, was zu einem Winkel von zirka 26,565° (dem Arkustangens des Seitenverhältnisses) führt.\n\nNicht-isometrische Pixel-Art ist jede andere Pixel-Art, die nicht der isometrischen Kategorie zuzuordnen ist; so etwa Ansichten von oben, von unten, von der Seite, von vorn oder perspektivische Ansichten.\n\nPixel-Art wurde sehr oft in älteren Computer- und Konsolenspielen benutzt. Mit dem zunehmenden Gebrauch von 3D-Grafik und hochauflösenden Bildern in Spielen verlor Pixel-Art etwas an Bedeutung. Trotzdem ist die Pixel-Art ein sehr aktiver Bereich, sowohl unter Profis als auch unter Amateuren, geblieben.\n\nIn der Kunst wird aber auch die bewusste Beschränkung der Auflösung in Kauf genommen. So hat der Chaos Computer Club e. V. im Projekt Blinkenlights Hochhäuser als Displays niedriger Auflösung verwendet, und darauf Grafiken, Kurzfilme und Videospiele gezeigt.\n\nManchmal wird Pixel-Art auch zu Werbezwecken eingesetzt. So ist etwa eine Firma, die mit Pixel-Art wirbt Bell Canada. Die Gruppe eboy hat sich auf Pixel-Grafiken für Werbung spezialisiert und wurde schon in Magazinen wie \"Wired\", \"Popular Science\" und \"Fortune 500\" gezeigt.\nNeben seiner digitalen Form, gibt es Pixel-Art auch aus Bügelperlen, erschaffen durch Sprühschablonen oder schlicht als Gemälde.\n\n"}
{"id": "254657", "url": "https://de.wikipedia.org/wiki?curid=254657", "title": "AmigaOne", "text": "AmigaOne\n\nDer AmigaOne stellte einen Versuch dar, einen offiziellen Nachfolger der Amiga-Computerserie zu schaffen.\nEr wurde von 2002 bis 2005 hergestellt. Von der Firma A-EON Technology Ende 2011 fertig entwickelt wurde der AmigaOne X1000.\n\nAnders als die originalen Amigas auf m68k-Basis verwenden die AmigaOne-Rechner eine PowerPC-CPU. Die am weitensten verbreiteten Desktop-Computer und Server, die ebenfalls auf PowerPC setzten, waren zwischen 1996 und 2005 die Power-Macintosh-Modelle von Apple.\n\nEtwa 10 Jahre nach den letzten 68k-Amiga-Modellen, die noch unter der Leitung von Commodore entwickelt wurden, hatte die britische Firma Eyetech – unter Verwendung des Teron-Mainboard-Designs – neue Boards mit eigenen Spezifikationen entwickeln lassen. Das originale Teron-Design von Mai Logic basierte auf dem von IBM im Jahr 2000 freigegebenen Design für ein vollständiges CHRP-Referenzboard.\n\nDas zugehörige Betriebssystem AmigaOS4, welches inzwischen nativ auf der PowerPC-Plattform läuft und auf Commodores AmigaOS 3.1 – ergänzt um einen neuen Kernel – basiert, wird von der belgischen Firma Hyperion entwickelt.\n\nUrsprünglich war der AmigaOne nur mit Linux lieferbar, bis zur Fertigstellung der ersten Beta-Version von AmigaOS verging fast ein Jahr.\n\nObwohl es von 2005 bis September 2008 auf dem Markt keine geeignete Hardware gab, wurde das Betriebssystem weiterentwickelt. Seit Oktober 2008 steht AmigaOS 4.1 für die PPC-Mainboards der Baureihe Sam440 (seit 2012: Sam460) der Firma Acube Systems zur Verfügung.\n\nDes Weiteren ist verfügbar der AmigaONE 500 der Firma Acube Systems, sowie in absehbarer Zeit (Stand: November 2016) der AmigaONE X5000. \n\nDer AmigaONE X5000 ist der neueste PowerPC-Computer, der von A-EON Technology entwickelt wurde. Das neue Board mit dem Codenamen Cyrus Plus wird der Ersatz für das Nemo (AmigaONE X1000) Board sein. A-EON-Technologie haben Ultra Varisys erneut beauftragt, eine neue High-End-Prestigeplattform zu entwickeln, die speziell für den Betrieb des AmigaOS entwickelt wurde. Es unterstützt aber auch eine Reihe anderer PowerPC-Betriebssysteme wie Ubuntu und Debian usw. Das Cyrus Plus-Board ist im Zuge der Freescale's QorlQ P5 64-bit PowerPC CPU SoC entworfen worden. Diese leistungsstarken Multikern-Prozessoren mit niedrigem Energieverbrauch enthalten 64-Bit-e5500-Kerne, die in 45-nm-Prozesstechnologie hergestellt werden. \n\nIn einigen Monaten (Stand: Dezember 2016) soll zudem das Tabor-Board (AmigaONE A1222) ausgeliefert werden. Mehrere Exemplare befinden sich bereits in den Händen von AmigaOS-Betatestern. \n\nZu den unterschiedlichen verfügbaren Mainboards von Eyetech zählen:\n\n\nDas AmigaOne-Starter-System besteht aus den folgenden Komponenten:\n\n\nDas AmigaOne-Power-System besteht aus den folgenden Komponenten:\n\n\nEyetech hatte hierzu jeweils unterschiedliche Grundkonfigurationen als Earlybird-Referenzsysteme im Angebot.\n\nDer fälschlich als \"Micro AmigaOne\" bezeichnete µA1 von Eyetech (G3/G4 CPU austauschbar, 800 MHz – 1,3 GHz, Mini-ITX) gehört nicht in diese Reihe, da ihm die offizielle Lizenz für die Marke AmigaOne fehlt.\n\nAmigaOS4 ist ebenfalls lauffähig auf PPC-Mainboards von ACube, die auch ein Komplettsystem unter dem Namen AmigaOne 500 anbieten.\n\n\nAmigaONE X1000:\n\n\nAmigaONE X5000:\n\n\nDas Tabor-Board (AmigaONE A1222) wird mit folgenden Merkmalen ausgestattet sein:\n\nPowerPC-CPU: Freescale QorIQ P1022, 1.2GHz, 32-bit, e500V2, Dual-Core\n\nDDR3 SODIMM\n\nRS232 Serial\n\nSATA\n\nUSB\n\nEthernet\n\nPCIe\n\nAudio\n\nHDMI LCD Interface\n\nGPIO\n\nMicro SD\n\nWCU Prog\n\nLED Ports für Strom, CPU, HDD\n\nAm 24. Dezember 2006 wurde von Hyperion nach fünfjähriger Entwicklung (und mehreren Betaversionen) die finale Version 4.0 von AmigaOS veröffentlicht, dem ersten AmigaOS, das keine 68k-Prozessoren mehr unterstützt, sondern ausschließlich auf IBMs PowerPC-Reihe läuft. Im August 2008 erschien die Version 4.1 des Betriebssystems. Am 14. Januar 2010 ist die Version 4.1 für die SAM440ep-Modelle nun keine Beta-Version mehr, sondern mit Update 1 eine Vollversion geworden.\n\nMittlerweile aktuell (Stand: November 2016) ist das AmigaOS 4.1 Final Edition, welche alle bis dato erschienenen Updates sowie neue Funktionen enthält. Weitere Aktualisierungen werden über die OnLine-Updatefunktion von AmigaOS 4.1 Final Edition verteilt. Zudem existiert mittlerweile noch ein zusätzliches Erweiterungspaket mit der Bezeichnung \"Enhancer Software\" von A-EON Technology, welches diverse Systembestandteile mit weiteren Funktionen versieht. \n\n"}
{"id": "257013", "url": "https://de.wikipedia.org/wiki?curid=257013", "title": "Computerkriminalität", "text": "Computerkriminalität\n\nDer Begriff Computerkriminalität oder Cyberkriminalität () umfasst „alle Straftaten, die unter Ausnutzung der Informations- und Kommunikationstechnik (IuK) oder gegen diese begangen werden“.\n\nEs gibt keine allgemein gültige Definition des Begriffs Computerkriminalität. Gewöhnlich sind darunter alle Straftaten zusammengefasst, die unter Ausnutzung der Informations- und Kommunikationstechnik oder gegen diese begangen werden. Im polizeilichen Bereich wird darüber hinaus zwischen Computerkriminalität \"im engeren Sinn\" und Computerkriminalität \"im weiteren Sinn\" unterschieden.\n\n\"Cyberkriminalität\" lässt sich dabei unterscheiden in:\n\nDie Unterscheidung ist, dass unter Internetkriminalität auch viele Straftaten und Vergehen fallen, die auch ohne Internet möglich wären (unter anderem Verbreitung verbotener Inhalte, Belästigung), während Computerkriminalität sich in diesem Sinne speziell auf den Kontext der elektronischen Daten bezieht.\n\n\"Cybercrime\" im engeren Sinne bezieht sich gemäß dem Deutschen Bundeskriminalamt (BKA) auf „spezielle Phänomene und Ausprägungen dieser Kriminalitätsform, bei denen Elemente der elektronischen Datenverarbeitung (EDV) wesentlich für die Tatausführung sind“.\n\nDer Begriff Computerkriminalität \"im weiteren Sinne\" wird in Deutschland umgangssprachlich auch für im Zusammenhang mit Computern stehende Handlungen verwandt, die zwar keine Straftaten, jedoch rechtswidrige Handlungen darstellen. Dabei hängt die Zuordnung zu den jeweiligen Bereichen insbesondere davon ab, ob am entsprechenden Tatort einschlägige Strafvorschriften existieren.\n\nZu Cybercrime \"im engeren Sinn\" zählt das Cybercrime Bundeslagebild 2011 des deutschen Bundeskriminalamts:\n\n\nDas österreichische Bundeskriminalamt fasst ebenfalls darunter Straftaten, bei denen Angriffe auf Daten oder Computersysteme unter Ausnutzung der Informations- und Kommunikationstechnik begangen werden (beispielsweise Datenbeschädigung, Hacking, DDoS-Attacken).\n\nZur Computerkriminalität im \"weiteren Sinne\" zählen in Deutschland alle Straftaten, bei denen die EDV zur Planung, Vorbereitung oder Ausführung eingesetzt wird. Diese erstrecken sich mittlerweile auf nahezu alle Deliktsbereiche, in denen das Tatmittel Internet eingesetzt wird. Beispielsweise:\n\n\nDiese Delikte werden in der Polizeilichen Kriminalstatistik (PKS) nicht unter dem Begriff Cybercrime registriert, sondern mit speziellen PKS-Schlüsselnummern.\n\nIn Österreich versteht man unter Straftaten der Computerkriminalität im \"weiteren Sinne\", Delikte bei denen die Informations- und Kommunikationstechnik zur Planung, Vorbereitung und Ausführung für herkömmliche Kriminalstraftaten eingesetzt wird, wie beispielsweise Betrugsdelikte, Kinderpornografie und Cyber-Mobbing.\n\nGemäß BKA-Lagebild wurden 2012 insgesamt 63.959 Fälle von Cybercrime im engeren Sinn erfasst.\n\nDie Fälle von Computerbetrug (24.817) sind gegenüber 2011 um 7,13 % gefallen, auch „Betrug mit Zugangsberechtigung zu Kommunikationsdiensten“ (2.952) haben 2012 um rund 38 % abgenommen. Bei den Delikten „Fälschung beweiserheblicher Daten, Täuschung im Rechtsverkehr bei Datenverarbeitung“ (8.539) ist eine Zunahme um rund 11 % zu verzeichnen. Während die „Straftaten durch Ausspähen, Abfangen von Daten einschließlich Vorbereitungshandlungen“ 16.794 Fälle ausmachten. Delikte „Datenveränderung/ Computersabotage“ (10.857) sind gegenüber 2011 um fast 140 % gestiegen.\n\nMit einem Anteil von rund 39 % ist Computerbetrug die größte Straftatengruppe aller im Lagebild ausgewiesenen Fälle.\n\nComputerkriminalität im weiteren Sinn bezieht über die unter Cyberkriminalität subsumierten Straffälle hinaus auch die Internetkriminalität ein. Welche Bedeutung das Internet als Tatmittel hat, zeigt sich daran, dass Internetkriminalität mit 229.408 Fällen fast 10-mal so viele Fälle aufweist wie Cybercrime im engeren Sinn. Allein Phishing in Zusammenhang mit Onlinebanking verursachte 2012 einen Gesamtschaden von 13,8 Mio. €.\n\nDurch Cybercrime verloren bereits 2/3 der deutschen Unternehmen Firmengeheimnisse. Damit steht Deutschland auf Platz 2, hinter den USA und vor Frankreich.\n\nMit der Ratifikation der Europaratskonvention über die Cyberkriminalität beteiligt sich die Schweiz an der verstärkten internationalen Bekämpfung der Computer- und Internetkriminalität. Die Konvention trat für die Schweiz am 1. Januar 2012 in Kraft. Zum gleichen Zeitpunkt hat der Bundesrat die erforderlichen Gesetzesanpassungen in Kraft gesetzt. Der Straftatbestand des unbefugten Eindringens in eine elektronische Datenverarbeitungsanlage (EDV) („Hacking“, Art. 143 bis 147 schweizerisches StGB) liegt nun im Bereich der Strafbarkeit. Auch werden neu bereits das Zugänglichmachen und das in Umlaufbringen bringen von Passwörtern, Programmen und anderen Daten unter Strafe gestellt, wenn der Betreffende weiß oder in guten Glauben davon ausgehen muss, dass diese für das illegale Eindringen in ein geschütztes Computersystem verwendet werden können (Siehe Hackerparagraph, bezogen auf Deutschland § 202c deutsches StGB).\n\nIn der Schweiz ist es die schweizerische Koordinationsstelle zur Bekämpfung der Internetkriminalität (KOBIK) welche Statistik über verfolgte Computerverbrechen führt und diese in einem jährlichen Rechenschaftsbericht veröffentlicht.\nFür das Geschäftsjahr 2012 wurden im Mai 2013 die aktuelle Statistik veröffentlicht. Über das Onlineformular auf der Webseite wurden 8242 Meldungen abgegeben, verglichen zum Vorjahr ist das ein Anstieg von 55 %. Zum ersten Mal seit Bestehen von KOBIK überholte die Anzahl der Meldungen über strafbare Handlungen gegen das Vermögen (3260 Meldungen) die der Meldungen über strafbaren Handlungen gegen die sexuelle Integrität (3083 Meldungen). In deutlich geringerem Umfang wurden strafbare Handlungen gegen Leib und Leben (99 Meldungen) und strafbare Handlungen gegen den öffentlichen Frieden (85 Meldungen) verübt. Dabei sei zu bemerken, dass die eingegangen Meldungen mit 80 % strafrechtlicher Relevanz von guter Qualität sind.\n\nIm Jahr 2013 verzeichnete das österreichische Bundeskriminalamt 11.199 angezeigte Fälle von Cybercrime. Aufgeklärt wurden 17,8 %. 421 Fälle davon betreffen Datenverarbeitungsmissbrauch mit Bereicherungsabsicht inklusive gewerbsmäßigem betrügerischem Datenmissbrauch, bei dem Daten eingegeben, verändert, gelöscht, unterdrückt oder auf andere Art der Datenverarbeitungsablauf gestört wird ( StGB). Unter diesen Paragraphen fallen auch Phishing und Malware-Angriffe. Die Funktionsfähigkeit eines Computersystems gemäß wurde 2013 in Österreich in 470 Fällen gestört und zur Anzeige gebracht. Unter diesen Paragraphen fallen auch DDoS-Attacken. Der widerrechtliche Zugriff auf Computersysteme ist in Österreich in geregelt. 2013 wurden 391 Fälle angezeigt.\n\n45,6 % der Tatverdächtigen waren zwischen 26 und 40 Jahre alt; 26,9 % der Tatverdächtigen waren über 40. Die Täter stammten zu 58,3 % aus dem österreichischen Inland. 468 Tatverdächtige wurden in Deutschland geortet, 373 in Serbien und 176 in Nigeria.\n\nMan versucht, die Arbeit zur Bekämpfung von Computerkriminalität in Österreich zu professionalisieren, um der zunehmenden Professionalisierung der Täter gegenübertreten zu können. Deshalb wurde im österreichischen Bundeskriminalamt ein Cybercrime-Competence-Center (C4) eingerichtet. Die Ermittler und IT-Forensiker werden auf internationale Schulungen geschickt, wie etwa der International Association for Computer Informations Systems (IACIS) und der European Cybercrime Training and Education Group (ECTEG). Im Sommer 2013 wurde aufgrund eines Hinweises vom FBI im Rahmen der Europol-Operation „BlackShades“ 19 tatverdächtige Hacker festgenommen.\n\nGemäß Lagebild werden in der deutschen Kriminalstatistik (PKS) nur die Schadenssummen bei den Straftaten „Computerbetrug“ und „Betrug mit Zugangsberechtigungen zu Kommunikationsdiensten“ erfasst.\n\nDiese Schäden sind 2011 um rund 16 % auf rund 71,2 Mio. Euro gestiegen (2010: 61,5 Mio. Euro). Davon entfallen 2011 rund 50 Mio. Euro auf den Bereich Computerbetrug und rund 21,2 Mio. Euro auf den Betrug mit Zugangsdaten zu Kommunikationsdiensten.\n\nFür Phishing im Bereich Onlinebanking hat das BKA eine durchschnittliche Schadenssummen errechnet: für 2011 rund 4.000 Euro pro Fall und insgesamt 25,7 Mio Euro. Vergleicht man die errechnete (ungefähre) Schadenssumme mit der von 15,7 Mio Euro für 2007, so ergibt sich ein Anstieg von 35 %. Angesichts des „vorhandenen Schadenspotenzials und der Lukrativität für die Täterseite“ bildet Phishing „weiterhin einen Schwerpunkt im Bereich Cybercrime“ (gemeint ist: Computerkriminalität im weiteren Sinn – der Verf.).\n\nDie Schaffung einer europaweiten „European cybercrime Plattform“ gehört zu den Handlungsfeldern der „Digitalen Agenda“.\n\nGemäß Symantec ist Deutschland (2012) „mit einem Schaden durch bösartige Software von 24 Milliarden Euro die Nr. 1 in Europa“; weltweit entstand ein „Schaden von 284 Milliarden Euro“, so die Sicherheits-Managerin Orla Cox von Symantec, Dublin. Eugene Kaspersky sagte im November 2018 kokettierend, in Russland seien die „übelsten Cyberkriminellen“ am Werk.\n\nIn verschiedenen Ländern wurden inzwischen spezielle Polizeieinheiten mit der Bekämpfung von Computerkriminalität beauftragt. Europol hat seit 2013 eine eigene Abteilung zur Koordination der Zusammenarbeit eingerichtet, das Europäische Zentrum zur Bekämpfung der Cyberkriminalität (EC3). In Deutschland wurden seit 2014 bei den Landeskriminalämtern und beim Bundeskriminalamt sogenannte Zentrale Ansprechstellen Cybercrime (ZAC) eingerichtet.\n\n\n"}
{"id": "257017", "url": "https://de.wikipedia.org/wiki?curid=257017", "title": "Derive", "text": "Derive\n\nDerive (engl. für „herleiten“) ist eine kommerzielle Mathematik-Software, die von Soft Warehouse (Honolulu) in den 1980er-Jahren als Nachfolger von muMATH entwickelt und später von Texas Instruments vertrieben wurde. Im Vergleich zu anderen PC-gestützten Computeralgebrasystemen ermöglichen die geringen Hardwareansprüche von Derive (die MS-DOS-Versionen fanden leicht auf einer Diskette Platz) den Einsatz von Computeralgebrasystemen auch in Umgebungen mit älterer oder beschränkter Hardwareausstattung.\n\nMitte der 1990er-Jahre diente Derive als Basis für einige Taschenrechnerentwicklungen wie dem TI-92 von Texas Instruments (mit seinen Nachfolgern und Parallelversionen TI-89, TI-92 Plus und Voyage 200). \n\nEines der ersten Länder, in denen in diversen Projekten der Einsatz von Derive in der Praxis erprobt wurde, war Österreich. Seit einigen Jahren wird in Deutschland das Programm im so genannten „projektorientierten Mathematikunterricht“ angewendet. Derive wird von Texas Instruments nicht in bestehender Form weiterentwickelt, sondern soll mittelfristig in TI-Nspire CAS aufgehen. Die letzte Version ist Derive 6.1.\n\n\nSOLVE([2a^2x+3y=7,x-5y=0],[x,y])\n\nführt zur Lösung:\n\n[x=35/(10a^2+3),y=7/10a^2+3]\n\n\nDIF(SIN(a x^2),x)\n\nwird differenziert bzw. abgeleitet zu:\n\n2a x COS(a x^2)\n\nDie Entwicklung und der Verkauf von Derive wurden mit dem 29. Juni 2007 eingestellt.\n\n"}
{"id": "257561", "url": "https://de.wikipedia.org/wiki?curid=257561", "title": "Mac IG", "text": "Mac IG\n\nDie Macintosh Interessen-Gemeinschaft kurz Mac IG ist die Dachorganisation einzelner Apple Macintosh-Benutzergruppen im deutschsprachigen Raum.\nMac IGs gibt es in diversen Städten in Deutschland, Österreich und der Schweiz und werden den \"MUG\" = Macintosh User Group zugeordnet.\n\nDie einzelnen Gruppen sind keine eingetragenen Vereine, sondern zwanglose Treffen von Macintosh-Anwender(inne)n und interessierten Gästen. Die Treffen finden in der Regel in Gaststätten, Universitäten oder Vereinsheimen statt.\n\nDie Mac IGs stehen allen Benutzern und Liebhabern von Apple-Computern offen, egal ob es sich um Heimanwender oder Profis handelt.\nDie lokalen Gruppen wenden sich gezielt gegen jede Form der Vereinsmeierei und bieten kostenlos Hilfe, Kaufberatung, veranstalten Vorträge, organisieren Treffen und private Kontakte und führen Mac-Flohmärkte durch.\n\nGäste und Interessierte sind bei den regelmäßigen Zusammenkünften (in der Regel monatlich) jederzeit gerne gesehen...\n\n"}
{"id": "257737", "url": "https://de.wikipedia.org/wiki?curid=257737", "title": "Linux Standard Base", "text": "Linux Standard Base\n\nDie Linux Standard Base (LSB) ist eine Arbeitsgruppe der Linux Foundation, die Ende der 1990er ins Leben gerufen wurde. Die LSB definiert Standards für Binärschnittstellen, Programmbibliotheken und andere Betriebssystembestandteile mit dem Ziel, die Kompatibilität zwischen den verschiedenen Linux-Distributionen, z. B. mit Hinblick auf die Lauffähigkeit von Programmen, zu verbessern. Bis heute erfüllt nur ein kleiner Teil der Linux-Distributionen die Anforderungen der LSB, auch sind die Anforderungen noch nicht umfassend genug, um eine vollständige Betriebssystemplattform zu definieren.\n\nIm Laufe der Geschichte von Linux haben sich eine Reihe von verschiedenen Linux-Distributionen entwickelt, die in vielen Details unterschiedliche Ansätze verfolgten, wie beispielsweise inkompatible Software-Paket-Formate, abweichende Verzeichnisstrukturen oder unterschiedliche Versionen der integrierten Softwarepakete. Dadurch sind unabhängige Softwareanbieter (ISVs) gezwungen, ihre Software für jede Distribution spezifisch anzupassen (oder anpassen zu lassen) und separiert anzubieten, ein enormer Mehraufwand.\n\nZiel der LSB ist, mit Standards und Richtlinien eine einheitliche binärkompatible Plattform für Softwareinstallationen unter Linux zu erzeugen. Sie macht u. a. Vorgaben, welche grundlegenden Programme und Programmbibliotheken auf einem LSB-konformen System vorhanden sein müssen und legt gemäß dem Filesystem Hierarchy Standard eine Verzeichnisstruktur fest. Die Basis der LSB Standards waren die Standards POSIX und Single UNIX Specification, welche erweitert wurden. Inzwischen weicht der LSB-Standard in einigen Aspekten jedoch Linux-spezifisch von den Open-Group-Unix-Standards ab.\n\nDas Ende der 1990er ins Leben gerufene LSB-Projekt wollte anfänglich die Standards POSIX und Single UNIX Specification vollständig einhalten und diese nur an einigen Stellen erweitern, daher hatte die Open Group auch angeboten, eine Zertifizierung für einen US-$ durchzuführen.\n\nIm Jahr 2005 begann die LSB jedoch darauf zu beharren, bestimmte in Linux-Distributionen übliche Abweichungen (Inkompatibilität) nicht zu beseitigen. Seitdem gab es bei dem Prozess zur Erreichung der UNIX-Standardkonformität keine Fortschritte, das Ziel der vollständigen POSIX- und SUS-Konformität scheint zugunsten der in bestehenden Linux-Systemen üblichen Konventionen aufgegeben worden zu sein. Im Gegenteil, Linus Torvalds hat wiederholt klargemacht, dass er bereit ist, von POSIX zu divergieren, wenn es dafür gute Gründe gibt.\n\n2005 wurde die LSB von glibc-Maintainer Ulrich Drepper als ineffektiv kritisiert, konkret monierte er fehlerhafte Testsuiten. Jeff Licquia von der LSB gab zu, dass Tests unvollständig und Code fehlerhaft seien, trotzdem hätten die Tests eine Aussagekraft. Außerdem seien definierte und prüfbare Standards alternativlos für einen freien, interoperablen Softwaremarkt.\n\nDie erste Version 1.0 der LSB umfasste ältere, schon weiter verbreitete Standards. Anfang Januar 2004 wurde die LSB das erste Mal der Internationalen Organisation für Normung (ISO) vorgelegt. Die darauffolgende Version 2.0 unterstützte mehr Architekturen. Die LSB 3.0 zeichnet sich durch Aktualisierungen der bereits bestehenden Standards aus. Anfang November des gleichen Jahres wurde dann bekannt, dass die ISO die LSB als internationalen Standard anerkannt hatte. Die anerkannte Version ist die Version 2.0.1. Neuere Versionen der LSB sollen folgen.\n\n"}
{"id": "257872", "url": "https://de.wikipedia.org/wiki?curid=257872", "title": "Amapi", "text": "Amapi\n\nAmapi ist eine 3D-Modellier-Software für Mac OS X und Windows, die anfangs vorrangig in der Spieleentwicklung Verwendung fand. Heute wird sie auch für anspruchsvolle 3D-Präsentationen von Objekten aller Art genutzt.\n\nDas Programm wird aktuell in der Version \"Amapi Pro 7.5\" angeboten. Eine Version \"Amapi 8 Pro\" wurde für das erste Quartal 2007 angekündigt, ist aber noch nicht veröffentlicht worden.\n\nIm Verlauf der Entwicklung konzentrierten sich die Entwickler von Amapi immer mehr auf umfassende Modellier-Möglichkeiten und ließen die (anfangs vorhandenen) Rendering-Fähigkeiten des Programms in den Hintergrund treten. Mittlerweile ist Amapi eine Modellier-Software und das von der gleichen Firma vertriebene Carrara 4 Pro übernimmt das Rendering und die Animation von den in Amapi erstellten 3D-Objekten.\n\nEntwickelt wurde Amapi von der französischen Softwarefirma \"Eovia\". Mitte 2006 erwarb die amerikanische Softwarefirma \"E frontier\" die Rechte für das Programm und ist jetzt verantwortlich für Entwicklung und Vertrieb des 3D-Programms. \n\nDas Programm unterstützt NURBS (Non-Uniform Rational B-Splines), wodurch eine komplexe 3D-Objektentwicklung ermöglicht wird. So kann der Anwender beispielsweise auch polygonale Objekte direkt in NURBS-Objekte umwandeln und weiterbearbeiten. Die eingebaute Funktion zur Erzeugung und Ausgabe von \"Watertight-STL-Dateien\" ermöglicht dem Anwender die rasche Nutzung des Rapid-Prototyping-Verfahrens. \n\nAmapi unterstützt folgende Dateiformate (in der Version Pro 7.5): \n\n"}
{"id": "258037", "url": "https://de.wikipedia.org/wiki?curid=258037", "title": "Shrek – Der tollkühne Held", "text": "Shrek – Der tollkühne Held\n\nShrek – Der tollkühne Held ist ein computeranimierter Kinofilm von DreamWorks aus dem Jahr 2001. Darin wird die Geschichte eines gleichnamigen Ogers erzählt, der in einer Märchenwelt lebt und versehentlich in ein Abenteuer um die Rettung einer Prinzessin gerät. Die Geschichte basiert auf dem Kinderbuch \"Shrek!\" von William Steig. Das Wort \"Shrek\" ist dem Jiddischen entliehen und entspricht dem deutschen Wort „Schrecken“.\n\nDer Film beginnt mit dem Eintauchen der Kamera in ein Märchenbuch, das die Geschichte einer Prinzessin erzählt, auf der ein schrecklicher, nicht genau genannter, Fluch liegt. Sie ist in einem Turm gefangen und wird von einem feuerspeienden Drachen bewacht – so lange bis ein tapferer Ritter sie befreit und sie durch den Kuss der wahren Liebe vom Fluch befreit.\n\nDer Held der Geschichte, Shrek, ist ein Oger. Daher wird er von seinen Zeitgenossen als groß, dumm, hässlich und vor allem gefährlich empfunden. Er lebt in einem Sumpf und liebt es, allein zu sein.\n\nAls Lord Farquaad, der Herrscher des Landes „DuLoc“, alle Fabelwesen vertreibt und sie in den Sumpf von Shrek umsiedelt, schließt dieser mit dem Lord einen Handel ab: Er darf seinen Sumpf wieder für sich alleine haben, wenn es ihm gelingt, die schöne Prinzessin Fiona aus ihrem von einem Drachen bewachten Turm zu befreien. Farquaad verspricht sich dadurch insgeheim eine Heirat mit der Prinzessin, die ihn zum König machen würde.\n\nBei dieser Mission wird Shrek von einem sprechenden Esel begleitet. Zusammen erleben die beiden so unterschiedlichen Lebewesen verschiedene Abenteuer. Es gelingt ihnen, Fiona aus dem Turm zu befreien und vor dem Drachen zu retten. Fiona selbst ist empört, dass Lord Farquaad Shrek als Handlanger gesendet hat, anstatt sie selber heldenhaft zu retten. Auf dem Rückweg nach DuLoc muss Shrek sie anfangs tragen, da sie sich weigert, mitzukommen. Dass sich auf dem Weg sowohl Shrek und Fiona als auch Esel und die Drachendame ineinander verlieben, macht die Reise nicht einfacher. Shrek weiß zu diesem Zeitpunkt nicht, dass auf Fiona ein Fluch liegt, der sie nach Einbruch der Nacht selbst zum Oger werden lässt.\n\nIn der Nacht, bevor Fiona Lord Farquaad treffen soll, entdeckt Esel, dass Fiona sich nach Sonnenuntergang in einen Oger verwandelt. Esel versucht, Fiona zu überzeugen, bei Shrek zu bleiben. Shrek hört durch die Tür von der Unterhaltung nur, dass Fiona sagt, niemand könnte ein Monster lieben. Shrek denkt, dass Fiona von ihm spricht, da er nicht wissen kann, dass Fiona sich selbst als Monster bezeichnet. Wütend bringt er Farquaad kurz nach Sonnenaufgang zu Fiona, die wieder Menschengestalt angenommen hat. Shrek verabschiedet sich von Fiona mit den Worten, dass sie recht habe und sich niemand in ein Monster verlieben könnte. Fiona wiederum denkt, er spräche von ihr, obwohl er von sich selbst spricht.\n\nUnglücklich beschließt Fiona, Farquaad zu heiraten. Nachdem Esel Shrek das Missverständnis erklärt hatte, verhindern die beiden mit Hilfe des Drachen die Hochzeit. Farquaad, der Shrek hinrichten und Fiona zurück in den Turm sperren will, wird von dem Drachen verschlungen. Durch den darauf folgenden Kuss der wahren Liebe zwischen Fiona und Shrek wird der Fluch, der auf Fiona liegt, aufgehoben. Sie nimmt endgültig die Gestalt einer Ogerfrau an und heiratet Shrek.\n\n\nAls Filmmusik wird englischsprachige Rockmusik eingesetzt.\n\nDa der Produzent Jeff Katzenberg Mitte der 1990er die Disney-Studios im Streit verlassen hatte, sind viele der Märchenfiguren als Parodien auf erfolgreiche Adaptionen von Disney zu verstehen. Auch Filme wurden parodiert:\n\n\n\nDer Film erhielt überwiegend gute Kritiken. Nach Angaben der Webseite Rotten Tomatoes erhielt der Film von 89 % der Filmkritiker ein positives Urteil, basierend auf 176 Filmkritiken.\n\nJames Berardinelli bezeichnete den Film auf \"ReelViews\" als „wundervoll“ und „schrullig“. Der Film könne genauso die Kinder, wie auch deren Eltern „packen“. Berardinelli lobte stark die Animationstechnik.\n\nRoger Ebert schrieb in der \"Chicago Sun-Times\" vom 18. Mai 2001, der Film sei eine „erstaunliche visuelle Freude“, die Animation sei zugleich realitätsnah und fantastisch. Der Film lebe jedoch von seiner unterhaltsamen Geschichte und dem liebenswürdigen Charakter des Ogers Shrek.\n\nMick LaSalle schrieb in der \"San Francisco Chronicle\" vom 18. Mai 2001, der Film sei „visuell elegant“. Er sei „ehrlicher“ und „warmherziger“ als die meisten Zeichentrickfilme. \"Shrek\" lebe zwar in einer Fantasiewelt, doch sein Verhalten und seine Geisteshaltung seien in der realen Welt verankert. Man könne den Film deswegen als den ersten wirklich modernen animierten Film bezeichnen.\n\n\nInfolge des großen Erfolgs von Shrek wurden ironisch-satirische Märchen-Parodien in Filmen beliebt. Weitere Animationsfilme mit ähnlichem Humor sind etwa Es war k'einmal im Märchenland und Die Rotkäppchen-Verschwörung. Auch Spielfilme und TV-Serien wie etwa 7 Zwerge und Die Märchenstunde nahmen sich der ironischen Persiflage klassischer Märchen an.\n\nDie deutsche Synchronisation wurde von der Berliner Synchron AG, unter der Dialogregie von Michael Nowka – der auch den Zauberspiegel spricht, produziert.\n\nDer Kinostart des Films in Deutschland war am 5. Juli 2001. Im deutschen Free-TV war er erstmals am 31. Mai 2004 ab 17.50 Uhr in der ARD zusehen.\n\n\n"}
{"id": "258356", "url": "https://de.wikipedia.org/wiki?curid=258356", "title": "PhotoLine", "text": "PhotoLine\n\nPhotoLine (bis Version 13 noch PhotoLine 32 oder PL32) ist ein Bildbearbeitungsprogramm der im bayerischen Bad Gögging ansässigen Firma \"Computerinsel\". Es wird als Shareware vertrieben und wurde ursprünglich für den Atari Falcon 030 entwickelt.\n\nDie aktuelle Version des Programms läuft unter verschiedenen Versionen von Windows (ab XP) und Mac OS X (ab 10.6). Eine Besonderheit ist die Bearbeitung und auch Kombination von Raster- und Vektorgrafiken. Außerdem besitzt es Funktionen wie Bildübersicht, Poster- und Etikettendruck, Erzeugen von Strichcodes, Erstellen von Diagrammen und bietet grundlegende Layoutfähigkeiten (im Dokumentmodus können z. B. mehrseitige, druckfertige Dokumente erstellt und als PDF-Dateien exportiert werden). Via Aspell kann unter Windows auch eine Rechtschreibprüfung eingebunden werden, unter Mac OS X wird die systemeigene Prüfung verwendet. PhotoLine bringt bereits eine große Auswahl an Filtern und Effekten mit, kann aber auch durch Photoshop-Plugins erweitert werden; darüber hinaus können PSP tubes importiert werden.\n\nPhotoLine arbeitet mit Ebenentechnik, Arbeitsebenen (Einstellungsebenen) und Ebenenstilen (Ebeneneffekten). Es wird durchgehend die Bearbeitung in 16 Bit Farbtiefe pro Kanal unterstützt. Rasterbilder können in Vektorgrafiken umgewandelt werden. Immer wiederkehrende Tätigkeiten lassen sich automatisieren, die Funktion \"Web-Export\" erleichtert die Wahl des Bildformates, erstellt GIF-Animationen oder generiert Buttons für Webseiten nebst HTML-Code. Das Lesen, Bearbeiten und Speichern von PDF-Dateien ist ebenfalls möglich, Unicode wird unterstützt. In den meisten Filterdialogen können die einzelnen (Farb-) Kanäle direkt getrennt gewählt werden: Neben CMYK, RGB und Graustufen werden auch HIS und Lab unterstützt. Eine rudimentäre Importfunktion für Kamera-Rohdatenformate ist vorhanden.\n\nPhotoLine unterstützt nahezu vollständiges Farbmanagement gemäß ICC-Standards; dazu gehören die freie Wahl des Arbeitsfarbraums, die Konvertierung und Zuweisung von Farbräumen, ein eigenes Druckerfarbmanagement sowie Softproof (Drucksimulation).\n\nDas Programm wird in engem Kontakt mit den Anwendern weiterentwickelt und kann zu Testzwecken kostenlos von der Herstellerseite heruntergeladen werden. Die komplette Installation benötigt ca. 20 MB auf einem PC, 30 MB auf einem Mac (wegen Universal Binary) und gilt als äußerst stabil.\n\nStand Mai 2015 war PhotoLine in folgenden Sprachen erhältlich: Deutsch, Englisch, Französisch, Italienisch und Chinesisch. Die Software kann als Installationspaket oder ZIP-Datei ohne Installer heruntergeladen werden, jeweils als 32- und 64-Bit-Variante. Auch Handbücher in verschiedenen Sprachen und Formaten sind erhältlich.\n\n\n"}
{"id": "258372", "url": "https://de.wikipedia.org/wiki?curid=258372", "title": "Adabas D", "text": "Adabas D\n\nAdabas D (früherer Name \"Entire SQL DB-Server\") ist ein relationales Datenbankmanagementsystem des Darmstädter Unternehmens Software AG. Es basiert auf dem durch die Software AG von Nixdorf erworbenen Programm DDB4. Dieses geht wiederum auf ein Forschungsprojekt der TU Berlin aus dem Jahr 1977 zurück.\n\n1997 erwarb das Softwareunternehmen SAP ebenfalls Rechte an dieser Technik und entwickelte sie in der Folge unter dem Namen SAP DB, dann MaxDB und schließlich SAP MaxDB getrennt weiter.\n\nBereits seit 1971 vertreibt die Software AG das Datenbanksystem Adabas, welches zwar auch eine SQL Schnittstelle anbietet, aber bis auf den Namen nichts mit Adabas D gemeinsam hat.\n\nAdabas D ist auf verschiedenen Computerplattformen bzw. Prozessortypen (x86, Alpha, Z-Series, Sparc) sowie unter verschiedenen Betriebssystemen (Windows, Solaris, AIX, HP-UX, SUSE-Linux, Red Hat Linux) einsetzbar. Es werden verschiedene SQL-Ausprägungen (ANSI, Oracle und Native) unterstützt. Schnittstellen bestehen zu C, C++ und Cobol.\n\nAb der Version 14 von ADABAS D wird innerhalb dieses DBMS-Systems erstmals Unicode-Unterstützung angeboten.\n\nVerschiedene Unternehmen haben Adabas D lizenziert und liefern oder lieferten es teilweise als Bestandteil ihrer Softwareprodukte mit aus.\n\n"}
{"id": "259992", "url": "https://de.wikipedia.org/wiki?curid=259992", "title": "Information Retrieval", "text": "Information Retrieval\n\nInformation Retrieval [] (IR) bedeutet Information abzurufen. Das Fachgebiet beschäftigt sich mit computergestütztem Suchen nach komplexen Inhalten (also z. B. keine Einzelwörter) und fällt in die Bereiche Informationswissenschaft, Informatik und Computerlinguistik.\n\nWie aus der Wortbedeutung von \"retrieval\" (deutsch \"Abruf\", \"Wiederherstellung\") hervorgeht, sind komplexe Texte oder Bilddaten, die in großen Datenbanken gespeichert werden, für Außenstehende zunächst nicht zugänglich oder abrufbar. Beim Information Retrieval geht es darum, \"bestehende\" Informationen aufzufinden, nicht \"neue\" Strukturen zu entdecken (wie beim Knowledge Discovery in Databases, zu dem das Data-Mining und Text Mining gehören).\n\nEng verwandt ist der Begriff Document Retrieval, das hauptsächlich auf (Text-)Dokumente als zu ermittelnde Information abzielt. oder Informationsrückgewinnung, gelegentlich ungenau Informationsbeschaffung,\n\nMethoden des Datenabrufs werden in Internet-Suchmaschinen (z. B. Google), aber auch in digitalen Bibliotheken (z. B. zur Literatursuche) sowie bei Bildsuchmaschinen verwendet. Auch Antwortsysteme oder Spamfilter verwenden IR-Techniken.\n\nDie Problematik des Zugangs zu gespeicherten komplexen Informationen liegt in zwei Phänomenen:\n\n\nGenerell sind am IR zwei (sich unter Umständen überschneidende) Personenkreise beteiligt (vgl. Abbildung rechts).\n\nDer erste Personenkreis sind die Autoren der in einem IR-System gespeicherten Informationen, die sie entweder selbst einspeichern, oder aus anderen Informationssystemen auslesen lassen (wie es z. B. die Internet-Suchmaschinen praktizieren). Die in das System eingestellten Dokumente werden vom IR-System gemäß dem System-internen Modell der Repräsentation von Dokumenten in eine für die Verarbeitung günstige Form (Dokumentenrepräsentation) umgewandelt.\n\nDie zweite Benutzergruppe, die Anwender, haben bestimmte, zum Zeitpunkt der Arbeit am IR-System akute Ziele oder Aufgaben, für deren Lösung ihnen Informationen fehlen. Diese Informationsbedürfnisse möchten Anwender mit Hilfe des Systems decken. Dafür müssen sie ihre Informationsbedürfnisse in einer adäquaten Form als Anfragen formulieren.\n\nDie Form, in der die Informationsbedürfnisse formuliert werden müssen, hängt dabei von dem verwendeten Modell der Repräsentation von Dokumenten ab. Wie der Vorgang der Modellierung der Informationsbedürfnisse als Interaktion mit dem System abläuft (z. B. als einfache Eingabe von Suchbegriffen), wird vom Modell der Interaktion festgelegt.\n\nSind die Anfragen formuliert, dann ist es die Aufgabe des IR-Systems, die Anfragen mit den im System eingestellten Dokumenten unter Verwendung der Dokumentenrepräsentationen zu vergleichen und eine Liste der zu den Anfragen passenden Dokumente an die Benutzer zurückzugeben. Der Benutzer steht nun vor der Aufgabe, die gefundenen Dokumente gemäß seiner Aufgabe auf die Lösungsrelevanz hin zu bewerten. Das Resultat sind die Bewertungen zu den Dokumenten.\n\nAnschließend haben die Benutzer drei Möglichkeiten:\n\n\n\n\nDer genaue Ablauf der drei Modifikationsformen wird vom Modell der Interaktion bestimmt. Zum Beispiel gibt es Systeme, die den Benutzer bei der Reformulierung der Anfrage unterstützen, indem sie die Anfrage unter Verwendung expliziter (d. h. dem System vom Benutzer in irgendeiner Form mitgeteilter) Dokumentenbewertungen automatisiert reformulieren.\n\nDer Begriff „Information Retrieval“ wurde erstmals 1950 von Calvin N. Mooers verwendet. Vannevar Bush beschrieb in einem Artikel 1945, wie man die Nutzung des vorhandenen Wissens durch den Einsatz von Wissensspeichern revolutionieren könne. Seine Vision hieß Memex. Dieses System sollte alle Arten von Wissensträgern speichern und mittels Links die gezielte Suche und das Stöbern nach Dokumenten ermöglichen. Bush dachte bereits an den Einsatz von Suchmaschinen und Retrievalwerkzeugen.\n\nEinen entscheidenden Schub erhielt die Informationswissenschaft durch die Sputnikschocks. Der russische Satellit hielt den Amerikanern zum einen ihre eigene Rückständigkeit in der Weltraumforschung vor Augen, welche durch das Apollo-Programm erfolgreich beseitigt wurde. Zum anderen – und das war der entscheidende Punkt für die Informationswissenschaft – dauerte es ein halbes Jahr, den Signalcode des Sputnik zu knacken. Und das, obwohl der Entschlüsselungscode in einer russischen Zeitschrift längst zu lesen war, welche bereits in den amerikanischen Bibliotheken stand.\n\nMehr Information führt also nicht zu mehr Informiertheit. Im Gegenteil. Der sogenannte Weinberg-Report ist ein vom Präsidenten in Auftrag gegebenes Gutachten zu diesem Problem. Der Weinberg-Report berichtet von einer „Informationsexplosion“ und erklärt, dass Experten benötigt werden, die diese Informationsexplosion bewältigen. Also Informations-Wissenschaftler.\nHans Peter Luhn arbeitete in den 1950er Jahren an textstatistischen Verfahren, die eine Basis für das automatische Zusammenfassen und Indexieren darstellen. Sein Ziel war es, individuelle Informationsprofile anzulegen und Suchterme hervorzuheben. Die Idee des Pushdienstes war geboren.\n\nEugene Garfield arbeitete in den 1950ern an Zitierindices, um so die verschiedenen Wege von Informationsübermittlung in Zeitschriften widerzuspiegeln. Dazu kopierte er Inhaltsverzeichnisse. 1960 gründete er das Institute for Scientific Information (ISI), eines der ersten kommerziellen Retrieval-Systeme.\n\nIn Deutschland entwickelte Siemens zwei Systeme, GOLEM (Großspeicherorientierte, listenorganisierte Ermittlungsmethode) und PASSAT (Programm zur automatischen Selektion von Stichwörtern aus Texten). PASSAT arbeitet unter Ausschluss von Stoppwörtern, bildet Wortstämme mithilfe eines Wörterbuches und gewichtet die Suchterme.\n\nSeit den 1960er Jahren gilt die Informationswissenschaft als etabliert.\n\nDIALOG ist ein von Roger K. Summit entwickeltes interaktives System zwischen Mensch und Maschine. Es ist wirtschaftlich orientiert und geht 1972 über die Regierungsdatenbanken ERIC und NTIS online.\nDas Projekt ORIBIT (heute Questel-Orbit) wurde durch Forschung und Entwicklung vorangetrieben unter der Leitung von Carlos A. Cuadra. 1962 geht das Retrievalsystem CIRC online und verschiedene Testläufe finden unter dem Codenamen COLEX statt. COLEX ist der direkte Vorläufer von Orbit, welches 1967 mit dem Schwerpunkt auf Forschungen der US Air Force online geht. Später verlagert sich der Schwerpunkt auf Medizininformationen. Das Suchsystem MEDLINE geht 1974 für die bibliographische Medizindatenbank MEDLARS online.\nOBAR ist ein von der Rechtsanwaltskammer in Ohio 1965 initiiertes Projekt. Es endet im System LexisNexis und erfasst schwerpunktmäßig Rechtsinformationen. Das System basiert auf der Volltextsuche, welche optimal für die Ohio-Urteile funktioniert.\n\nMit dem Internet wird Information Retrieval zum Massenphänomen. Ein Vorläufer war das ab 1991 verbreitete System WAIS, das verteiltes Retrieval im Internet ermöglichte. Die frühen Web-Browser NCSA Mosaic und Netscape Navigator unterstützen das WAIS-Protokoll, bevor die Internet-Suchmaschinen aufkamen und später dazu übergingen, auch Nicht-HTML-Dokumente zu indexieren. Zu den bekanntesten und populärsten Suchmaschinen gehören derzeit Google und Bing. Verbreitete Suchmaschinen für Intranets sind Autonomy, Convera, FAST, Verity sowie die Open-Source-Software Apache Lucene.\n\nDer Informationsbedarf ist der Bedarf an handlungsrelevantem Wissen und kann dabei konkret und problemorientiert sein. Beim konkreten Informationsbedarf wird eine Fakteninformation benötigt. Also beispielsweise \"Was ist die Hauptstadt von Frankreich?\". Die Antwort \"Paris\" deckt den Informationsbedarf vollständig. Anders ist es beim problemorientierten Informationsbedarf. Hier werden mehrere Dokumente benötigt, um den Bedarf zu stillen. Zudem wird der problemorientierte Informationsbedarf nie ganz gedeckt werden können. Gegebenenfalls ergibt sich aus der erhaltenen Information sogar ein neuer Bedarf oder die Modifikation des ursprünglichen Bedarfs.\nBeim Informationsbedarf wird vom Nutzer abstrahiert. Das heißt, es wird der objektive Sachverhalt betrachtet.\n\nDas Informationsbedürfnis spiegelt den konkreten Bedarf beim anfragenden Nutzer wider. Es geht um das subjektive Bedürfnis des Nutzers.\n\nUm eine Suchanfrage so präzise wie möglich formulieren zu können, müsste man eigentlich wissen, was man nicht weiß. Es muss also ein Basiswissen vorhanden sein, um eine adäquate Suchanfrage zu verfassen. Zudem muss die natürlichsprachige Suchanfrage in eine Variante umgewandelt werden, die vom Retrievalsystem gelesen werden kann.\nHier einige Beispiele für Suchanfrageformulierungen in verschiedenen Datenbanken. Gesucht werden Informationen über den Schauspieler \"Johnny Depp\" im Kinofilm \"Chocolat\".\n\nLexisNexis:\nHEADLINE:(„Johnny Depp“ w/5 „Chocolat“)\n\nDIALOG:\n(Johnny ADJ Depp AND Chocolat) ti\n\nGoogle:\n“Chocolat” “Johnny Depp”\n\nDer Nutzer gibt dabei vor, wie der Retrievalprozess abläuft, und zwar dies durch die Art und Weise seiner Suchanfrageformulierung im jeweils verwendeten System. Zu unterscheiden sind wort- und begrifforientierte Systeme. Begrifforientierte Systeme können die Mehrdeutigkeiten von Wörtern erkennen (z. B. Java = die Insel, Java = der Kaffee oder Java = die Programmiersprache).\nÜber die Suchanfrage wird die Dokumentationseinheit (DE) angesprochen. Die DE stellt den informationellen Mehrwert der Dokumente dar. Das bedeutet, in der DE wird Information zu Autor, Jahrgang etc. verdichtet wiedergegeben. Je nach Datenbank werden entweder das komplette Dokument oder nur Teile davon erfasst.\n\nWeder die Dokumentarische Bezugseinheit (DBE) noch die Dokumentationseinheit (DE) sind das Originaldokument. Beide sind nur Stellvertreter desselben in der Datenbank.\nZuerst wird die Dokumentationswürdigkeit eines Dokumentes geprüft. Das findet über formale und inhaltliche Kriterienkataloge statt. Ist ein Objekt für dokumentenwürdig befunden, wird eine DBE erstellt. Hier entscheidet sich, in welcher Form das Dokument abgespeichert wird. Werden einzelne Kapitel oder Seiten als DBE genommen oder das Dokument im Ganzen?\nEs schließt sich der informationspraktische Prozess an. Die DBE werden formal beschrieben und der Inhalt verdichtet. Dieser informationelle Mehrwert findet sich dann in der DE wieder, die als Stellvertreter für die DBE dient. Die DE repräsentiert die DBE und steht somit am Ende des Dokumentationsprozesses.\nDie DE dient dem Nutzer dazu, eine Entscheidung darüber zu treffen, ob er die DBE gebrauchen kann und anfordert oder eben nicht.\nInformation Retrieval und Information Indexing sind aufeinander abgestimmt.\n\nDiese sind Teil der empirischen Informationswissenschaft, da sie sich auf die Vorkenntnisse, den sozio-ökonomischen Hintergrund, die Sprachkenntnisse usw. der Nutzer beziehen und darüber Informationsbedarfs-, Nutzungs- und Nutzeranalysen anstellen.\n\nDas Suchen nach Informationen beschreibt Marcia J. Bates als Berrypicking (dt. \"Beeren pflücken\"). Es reicht nicht aus, nur an einem Strauch respektive einer Datenbank nach Beeren bzw. Informationen zu suchen, damit der Korb voll wird. Es müssen mehrere Datenbanken angefragt und die Suchanfrage aufgrund neuer Informationen ständig modifiziert werden.\nPulldienste werden überall da zur Verfügung gestellt, wo der Nutzer aktiv nach Informationen suchen kann.\nPushdienste versorgen den Nutzer aufgrund eines abgespeicherten Informationsprofils mit Informationen. Diese Profildienste, sogenannte Alerts, speichern erfolgreich formulierte Suchanfragen ab und informieren den Nutzer über das Eintreffen neuer relevanter Dokumente.\n\nDen Informationsfluss behindern verschiedene Faktoren. Solche Faktoren sind beispielsweise Zeit, Ort, Sprache, Gesetze und die Finanzierung.\n\nDer Recall bezeichnet die Vollständigkeit der angezeigten Treffermenge. Die Precision dagegen berechnet die Genauigkeit der Dokumente aus der Treffermenge zu einer Suchanfrage.\nPrecision bezeichnet den Anteil aller relevanten Dokumente an den selektierten Dokumenten einer Suchanfrage und ist damit das Maß der in der Trefferliste enthaltenen bezüglich der Aufgabenstellung bedeutungsvollen Dokumente. Recall hingegen beschreibt den Anteil aller relevanten Dokumente an der Gesamtzahl relevanter Dokumente der Dokumentensammlung. Dabei handelt es sich um das Maß für die Vollständigkeit einer Trefferliste. Beide Maße bilden entscheidende Kennzahlen für ein Information Retrieval-System. Ein ideales System würde in einer Suchanfrage alle relevanten Dokumente einer Dokumentensammlung unter Ausschluss nicht zutreffender Dokumente selektieren.\n\nRecall: formula_1\n\nPrecision: formula_2\n\na = gefundene, relevante Treffer\n\nb = gefundene, nichtrelevante DE / Ballast\n\nc = relevante DE, die nicht gefunden wurden / Verlust\n\n„c“ ist nicht direkt messbar, da man ja nicht wissen kann, wie viele DE nicht gefunden wurden, sofern man den Inhalt der Datenbank bzw. die DE nicht kennt, die aufgrund der Suchanfrage eigentlich hätten angezeigt werden müssen.\nDer Recall kann auf Kosten der Precision vergrößert werden und umgekehrt. Das gilt allerdings nicht bei einer Faktenfrage. Hier sind Recall und Precision gleich eins.\n\nWissen kann relevant, muss aber nicht pertinent sein. Relevanz bedeutet, dass ein Dokument unter der Suchanfrage, die formuliert wurde, passend ausgegeben wurde. Wenn der Nutzer den Text aber bereits kennt oder er ihn nicht lesen will, weil er den Autor nicht mag oder keine Lust hat, einen Artikel in einer anderen Sprache zu lesen, ist das Dokument nicht pertinent. Pertinenz bezieht die subjektive Sicht des Nutzers mit ein.\n\nVoraussetzungen für erfolgreiches Information Retrieval sind das richtige Wissen, zum richtigen Zeitpunkt, am richtigen Ort, im richtigen Umfang, in der richtigen Form, mit der richtigen Qualität. Wobei \"richtig\" heißt, dass dieses Wissen entweder Pertinenz oder Relevanz besitzt.\n\nWissen ist dann nützlich, wenn der Nutzer daraus neues handlungsrelevantes Wissen erzeugt und dieses in die Praxis umsetzt.\n\nRelevanz ist die Relation zwischen der Suchanfrage (query) in Bezug auf das Thema und die systemseitigen Aspekte.\n\nDer binäre Ansatz sagt aus, dass ein Dokument entweder relevant oder nicht-relevant ist. In der Realität ist das nicht unbedingt zutreffend. Hier spricht man eher von „Relevanzregionen“.\n\nDafür können beispielsweise Themenketten gebildet werden. Ein Thema kann in mehreren Ketten vorkommen. Je häufiger ein Thema vorkommt, desto größer ist sein Gewichtungswert. Kommt das Thema in allen Ketten vor, liegt sein Wert bei 100; kommt es in keiner Kette vor, bei 0.\nBei Untersuchungen haben sich drei verschiedene Verteilungen herauskristallisiert. Dabei ist anzumerken, dass diese Verteilungen nur bei größeren Dokumentenmengen zustande kommen. Bei kleineren Dokumentenmengen gibt es eventuell gar keine Regelmäßigkeiten.\n\nBei der binären Verteilung ist kein Relevanceranking möglich.\n\nformula_3\n\nformula_8\n\nDie informetrische Verteilung sagt aus: Wenn das erstplatzierte Dokument eine Relevanz von eins hat (bei formula_12), dann hat das zweitplatzierte Dokument eine Relevanz von 0,5 (bei formula_13) oder von 0,25 (bei formula_14).\n\nEs sei noch einmal darauf hingewiesen, dass in der Informationswissenschaft unterschieden wird zwischen dem Ausgangsdokument der DBE und der DE.\nAber wann ist „etwas“ eigentlich ein Dokument? Das entscheiden vier Kriterien: die Materialität (einschließlich des digitalen Vorhandenseins), die Intentionalität (Das Dokument trägt einen gewissen Sinn, eine Bedeutung), die Erarbeitung und die Wahrnehmung.\n\nObjekte können in Textform auftreten, müssen es aber nicht. Bilder und Filme sind Beispiele für nicht-textuelle Dokumente. Textuelle und nicht-textuelle Objekte können in digitaler und in nicht-digitaler Form auftreten. Sind sie digital und treffen mehr als zwei Medienformen aufeinander (Ein Dokument besteht beispielsweise aus einer Videosequenz, einer Audiosequenz und Bildern), nennt man sie Multimedia. Die nicht-digital vorliegenden Objekte brauchen in der Datenbank einen digitalen Stellvertreter, etwa ein Foto.\n\nAls formal publizierte Textdokumente werden alle Dokumente bezeichnet, die einen formalen Veröffentlichungsprozess durchlaufen haben. Das bedeutet, die Dokumente wurden vor der Veröffentlichung geprüft (z. B. durch einen Lektor). Ein Problem stellt die sogenannte „Graue Literatur“ dar. Diese ist zwar geprüft, aber nicht veröffentlicht worden.\n\nEs existieren mehrere Ebenen von formal publizierten Dokumenten. Am Anfang steht die Arbeit, die Schöpfung des Autors. Gefolgt vom Ausdruck dieser Arbeit, der konkreten Realisierung (z. B. verschiedene Übersetzungen). Diese Realisierung wird manifestiert (z. B. in einem Buch). An unterster Stelle dieser Kette steht das Item, das einzelne Exemplar. In der Regel richtet sich die DBE auf die Manifestation. Ausnahmen sind aber möglich.\n\nZu den informell publizierten Texten gehören vor allem Dokumente, die im Internet veröffentlicht wurden. Diese Dokumente sind zwar publiziert, aber nicht geprüft.\n\nEine Zwischenstufe von formell und informell publizierten Texten sind beispielsweise Wikis. Diese sind publiziert und kooperativ geprüft.\n\nHierzu zählen Briefe, Rechnung, interne Berichte, Dokumente im Intranet oder Extranet. Eben alle Dokumente, die nie öffentlich gemacht wurden.\n\nBei den nicht-textuellen Dokumenten unterscheidet man zwei Gruppen. Zum einen die digital vorliegenden oder digitalisierbaren Dokumente, wie Filme, Bilder und Musik und zum anderen die nicht digitalen und nicht digitalisierbaren Dokumente. Zu letzteren gehören Fakten, wie chemische Stoffe und deren Eigenschaften und Reaktionen, Patienten und deren Symptome und Museumsobjekte. Die meisten nicht digitalisierbaren Dokumente entstammen den Disziplinen Chemie, Medizin und Wirtschaft. Sie werden in der Datenbank von der DE vertreten und oftmals zusätzlich durch Bilder, Videos und Audiodateien dargestellt.\n\nMan differenziert zwischen strukturierten, schwach strukturierten und nicht-strukturierten Texten. Zu den schwach strukturierten Texten zählen alle Arten von Textdokumenten, die eine gewisse Struktur haben. Dazu zählen Kapitelnummern, Titel, Zwischenüberschriften, Abbildungen, Seitenzahlen etc. Über informationelle Mehrwerte können den Texten strukturierte Daten hinzugefügt werden.\nNicht-strukturierte Texte kommen in der Realität kaum vor. In der Informationswissenschaft beschäftigt man sich hauptsächlich mit schwach strukturierten Texten.\nDabei ist zu beachten, dass es nur um formale, nicht um syntaktische Strukturen geht. Es ergibt sich ein Problem mit dem Sinnzusammenhang der Inhalte.\n\n„The man saw the pyramid on the hill with the telescope.“ Dieser Satz kann vierfach interpretiert werden. Daher bevorzugen einige Anbieter menschliche Indexer, da diese den Sinnzusammenhang erkennen und korrekt weiterverarbeiten können.\n\nInformation Retrievalsysteme können entweder mit oder ohne terminologische Kontrolle arbeiten. Arbeiten sie mit terminologischer Kontrolle, ergeben sich die Möglichkeiten sowohl intellektuell, als auch automatisch zu indexieren. Retrieval Systeme die ohne terminologische Kontrolle arbeiten, bearbeiten entweder den reinen Text oder der Prozess läuft über eine automatische Bearbeitung.\n\nTerminologische Kontrolle bedeutet nichts anderes als die Verwendung von kontrolliertem Vokabular. Das erfolgt über Dokumentationssprachen (Klassifikationen, Schlagwortmethode, Thesauri, Ontologien). Die Vorteile liegen darin, dass der Rechercheur und der Indexer über dieselben Ausdrücke und Formulierungsmöglichkeiten verfügen. Daher ergeben sich keine Probleme mit Synonymen und Homonymen. Nachteile von kontrolliertem Vokabular sind etwa die mangelnde Berücksichtigung von Sprachentwicklungen, sowie das Problem, dass diese Kunstsprachen nicht von jedem Nutzer korrekt angewandt werden.\nEine weitere Rolle spielt natürlich der Preis. Intellektuelles Indexieren ist sehr viel teurer als automatisches.\n\nInsgesamt lassen sich vier Fälle unterscheiden:\n\nBei der Variante ohne terminologische Kontrolle wird am besten mit den Volltexten gearbeitet. Das funktioniert allerdings nur bei sehr kleinen Datenbanken. Die Terminologie der Dokumente muss von den Nutzern genau gekannt werden.\nDer Prozess mit terminologischer Kontrolle setzt eine informationslinguistische Bearbeitung (Natural Language Processing = NLP) der Dokumente voraus.\n\nDie informationslinguistische Textbearbeitung geht wie folgt vor. Zuerst wird das Schriftsystem erkannt. Ist es beispielsweise ein lateinisches oder arabisches Schriftsystem. Danach folgt die Spracherkennung. Nun werden Text, Layout und Navigation voneinander getrennt. An dieser Stelle gibt es zwei Möglichkeiten. Zum einen die Zerlegung der Wörter in n-Gramme oder die Worterkennung. Egal für welche Methode man sich entscheidet, schließen sich Stoppwortmarkierung, Eingabefehlererkennung und -korrektur sowie Eigennamenerkennung und die Bildung von Grund- bzw. Stammformen an. Es werden Komposita zerlegt, Homonyme und Synonyme erkannt und abgeglichen und das semantische Umfeld oder das Umfeld nach Ähnlichkeit untersucht. Die letzten beiden Schritte sind die Übersetzung des Dokumentes und die Anaphoraauflösung.\nEs kann nötig sein, dass während des Ablaufes das System mit dem Nutzer in Verbindung tritt.\n\nEs existieren mehrere konkurrierende Retrievalmodelle, die sich aber keineswegs ausschließen müssen. Zu diesen Modellen zählen das Boolesche und das erweiterte Boolesche Modell. Das Vektorraummodell und das probabilistische Modell sind Modelle, die auf der Textstatistik beruhen. Zu den Linktopologischen Modellen gehören der Kleinberg-Algorithmus und der PageRank. Schließlich gibt es noch das Netzwerkmodell und die Nutzer-/Nutzungsmodelle, welche die Textnutzung und den Nutzer an seinem spezifischen Standort untersuchen.\n\nGeorge Boole veröffentlichte 1854 seine „Boolesche Logik“ und ihre binäre Sicht der Dinge. Sein System hat drei Funktionen oder auch Operatoren: UND, ODER und NICHT. Bei diesem System ist keine Sortierung nach Relevanz möglich. Um ein Relevanzranking zu ermöglichen, wurde das Boolesche Modell um Gewichtungswerte erweitert und die Operatoren mussten uminterpretiert werden.\n\nIn der Textstatistik werden die im Dokument auftretenden Terme analysiert. Die Gewichtungsfaktoren heißen hier WDF und IDF.\n\nWithin-document Frequency (WDF): Anzahl des vorkommenden Terms/ Anzahl aller Wörter\n\nDer WDF beschreibt die Häufigkeit eines Wortes in einem Dokument. Je Häufiger ein Wort in einem Dokument vorkommt, desto größer sein WDF\n\nInverse Dokumenthäufigkeit Gesamte Anzahl an Dokumenten in der Datenbank/ Anzahl der Dokumente mit dem Term\n\nDer IDF beschreibt die Häufigkeit, mit der ein Dokument mit einem bestimmten Term in einer Datenbank vorkommt. Je häufiger ein Dokument mit einem bestimmten Term in der Datenbank vorkommt, desto kleiner sein IDF.\n\nDie zwei klassischen Modelle der Textstatistik sind das Vektorraummodell und das probabilistische Modell.\nIm Vektorraummodell spannen n-Wörter einen n-dimensionalen Raum auf. Die Ähnlichkeit der Wörter zueinander wird über die Winkel ihrer Vektoren zueinander berechnet.\nBeim probabilistischen Modell wird die Wahrscheinlichkeit berechnet, mit der ein Dokument auf eine Suchanfrage zutrifft. Ohne Zusatzinformationen ist das probabilistische Modell dem IDF ähnlich.\n\nDokumente sind im WWW untereinander und miteinander verlinkt. Sie bilden somit einen Raum von Links.\nDer Kleinberg-Algorithmus nennt diese Links „Hub“ (ausgehende Links) und „Authority“ (eingehende Links). Die Gewichtungswerte entstehen darüber, inwiefern Hubs auf „gute“ Authorities treffen und Authorities von „guten“ Hubs gelinkt werden.\nEin weiteres linktopologisches Modell ist der PageRank von Sergey Brin und Lawrence Page. Er beschreibt die Wahrscheinlichkeit, mit der ein nach dem Zufallsprinzip Surfender eine Seite findet.\n\nClusterverfahren versuchen, Dokumente zu klassifizieren, so dass ähnliche oder miteinander in Beziehung stehende Dokumente in einem gemeinsamen Dokumentenpool zusammengefasst werden. Dadurch tritt eine Beschleunigung des Suchverfahrens ein, da sämtliche relevanten Dokumente im günstigsten Fall mit einem einzigen Zugriff selektiert werden können. Neben Dokumentenähnlichkeiten spielen aber auch Synonyme als semantisch ähnliche Wörter eine bedeutende Rolle. So sollte eine Suche nach dem Begriff „Wort“ auch eine Trefferliste für Kommentar, Bemerkung, Behauptung oder Term präsentieren.\n\nProbleme entstehen aus der Art der Zusammenfassung von Dokumenten:\n\nBei dem Nutzer-Nutzungsmodell ist die Häufigkeit der Nutzung einer Website ein Rangkriterium. Zusätzlich fließen Hintergrundinformationen beispielsweise über den Standort des Nutzers bei geographischen Anfragen mit ein.\n\nBeim systematischen Suchen ergeben sich Rückkopplungsschleifen. Diese laufen entweder automatisch oder der Nutzer wird wiederholt aufgefordert, Ergebnisse als relevant oder nicht-relevant zu markieren, ehe die Suchanfrage modifiziert und wiederholt wird.\n\nDas Oberflächenweb liegt im Web und ist kostenlos für alle Nutzer erreichbar. Im Deep Web liegen etwa Datenbanken, deren Suchoberflächen über das Oberflächenweb zu erreichen sind. Ihre Informationen sind aber in der Regel kostenpflichtig.\nEs lassen sich drei Arten von Suchmaschinen unterscheiden. Suchmaschinen wie Google arbeiten algorithmisch, das Open Directory Project ist ein intellektuell erstellter Webkatalog und Metasuchmaschinen beziehen ihren Content aus mehreren anderen Suchmaschinen, die sich ansprechen. In der Regel verwenden intellektuell erstellte Webkataloge nur die Einstiegsseite einer Website als Bezugsquelle für die DBE. Bei algorithmisch arbeitenden Suchmaschinen wird jede Webseite verwendet.\n\nEs gibt digitale und nicht-digitale Speichermedien, wie etwa Steilkarten, Bibliothekskataloge und Sichtloskarten. Digitale Speichermedien werden von der Informatik erarbeitet und sind Beschäftigungsbereich der Informationswissenschaft.\nMan unterscheidet zwischen der Dateistruktur und ihrer Funktion. Darüber hinaus gibt es Schnittstellen des Retrievalsystems mit den Dokumenten und mit ihren Nutzern. Bei der Schnittstelle zwischen System und Dokument unterscheidet man wieder drei Bereiche. Das Finden von Dokumenten, das sogenannte Crawling, die Kontrolle dieser gefundenen Dokumente auf Updates und die Einordnung in ein Feldschema. Die Dokumente werden entweder intellektuell oder automatisch erfasst und weiter verarbeitet. Dabei werden die DE zweifach abgespeichert. Einmal als Dokumentendatei und zusätzlich noch als invertierte Datei, welche als Register oder Index den Zugriff auf die Dokumentendatei erleichtern soll.\nNutzer und System treten in folgender Weise in Kontakt. Der Nutzer verfasst\n\n1963 entstand der ASCII-Code (American Standard Code for Information Interchange). Sein 7 bit-Code konnte 128 Zeichen erfassen und abbilden. Er wurde später auf 8 bit (= 256 Zeichen) erweitert. Der bislang größte Zeichensatz Unicode umfasst 4 Byte, also 32 bit und soll alle Zeichen abbilden, die überhaupt auf der Welt genutzt werden.\nDie ISO 8859 (International Organisation for Standardization) regelt darüber hinaus sprachspezifische Varianten, wie etwa das „ß“ in der deutschen Sprache.\n\nNeue Dokumente können sowohl intellektuell, als auch automatisch der Datenbasis hinzugefügt werden. Bei der intellektuellen Aufnahme neuer Dokumente ist ein Indexer verantwortlich und entscheidet, welche Dokumente wie aufgenommen werden. Der automatische Prozess erfolgt durch einen „Robot“ oder einen „Crawler“. Grundlage ist eine bekannte Menge an Webdokumenten, eine sogenannte „seed list“. Die Links aller Webseiten, die diese Liste enthält, ist nun Aufgabe der Crawler. Die URL der jeweiligen Seiten wird geprüft, ob sie bereits in der Datenbasis vorhanden ist oder nicht. Darüber hinaus werden Spiegel und Dubletten erkannt und gelöscht.\n\nEiner der Best-First-Crawler ist der Page Rank-Crawler. Er sortiert die Links nach Anzahl und Popularität der eingehenden Seiten. Zwei weitere sind der Fish-Search- und der Shark-Search-Crawler. Ersterer beschränkt seine Arbeit auf Bereiche im Web, in denen sich relevante Seiten konzentrieren. Der Shark-Search-Crawler verfeinert diese Methode, indem er zusätzliche Informationen zum Beispiel aus den Ankertexten zieht, um ein Relevanzurteil zu treffen.\nJeder Seitenbetreiber hat die Möglichkeit, seine Seite gegen Crawler zu verschließen.\n\nDamit ein Crawler auch im Deep Web erfolgreich arbeiten kann, muss er verschiedene Anforderungen erfüllen. Zum einen muss er die Suchmaske der Datenbank „verstehen“, um eine adäquate Suchanfrage formulieren zu können. Darüber hinaus muss er Trefferlisten verstehen und Dokumente anzeigen können. Das funktioniert allerdings nur bei kostenlosen Datenbanken. Wichtig für Deep Web Crawler ist es, dass sie Suchargumente derart formulieren können, dass alle Dokumente der Datenbank angezeigt werden. Ist in der Suchmaske ein Jahrgangsfeld vorhanden, müsste der Crawler der Reihe nach alle Jahrgänge anfragen, um an alle Dokumente zu gelangen. Bei Stichwortfeldern ist eine adaptive Strategie am sinnvollsten.\nSind die Daten einmal erfasst, muss der Crawler nur noch die Updates der gefundenen Seiten erfassen. Um die DE möglichst aktuell zu halten, gibt es mehrere Möglichkeiten. Entweder die Seiten werden im selben Abstand regelmäßig besucht, was allerdings die Ressourcen weit übersteigen würde und daher unmöglich ist, oder der Besuch nach Zufall, was allerdings eher suboptimal funktioniert. Eine dritte Möglichkeit wäre der Besuch nach Prioritäten. Beispielsweise nach dem Takt ihrer Änderungen (seitenzentriert) oder der Häufigkeit ihrer Aufrufe oder Downloads (nutzerzentriert).\nWeitere Aufgaben der Crawler sind es, Spam, Dubletten sowie Spiegel zu erkennen. Die Erkennung von Dubletten erfolgt in der Regel über den Vergleich der Pfade. Die Vermeidung von Spam gestaltet sich etwas schwieriger, da Spam oft versteckt auftritt.\n\nZu den FIFO-Crawlern gehören der Breadth-First-Crawler, welcher allen Links einer Seite folgt, diese abarbeitet und den Links der gefundenen Seiten weiter folgt und der Depth-First-Crawler. Dieser arbeitet im ersten Schritt wie der Breadth-First-Crawler, trifft im zweiten Schritt allerdings eine Auswahl, welchen Links er weiter folgt und welchen nicht.\n\nThematische Crawler sind auf eine Disziplin spezialisiert und daher geeignet für Fachexperten. Thematisch nicht relevante Seiten werden identifiziert und „getunnelt“. Dennoch werden die Links dieser getunnelten Seiten weiter verfolgt, um weitere relevante Seiten zu finden.\n\"Distiller\" finden derweil einen günstigen Ausgangspunkt für die Crawler, indem sie Taxonomien und Musterdokumente nutzen. \"Classifier\" eruieren diese Seiten auf Relevanz. Der ganze Vorgang läuft semiautomatisch, da Taxonomien und Musterdokumente regelmäßig aktualisiert werden müssen. Darüber hinaus wird eine Begriffsordnung benötigt.\n\nDie gefundenen Dokumente werden in die Datenbasis kopiert. Dafür werden zwei Dateien angelegt, zum einen die Dokumentendatei, zum anderen eine invertierte Datei. In der invertierten Datei werden alle Wörter oder Phrasen geordnet und nach Alphabet oder einem anderen Sortierkriterium aufgelistet. Ob man einen Wortindex oder einen Phrasenindex verwendet, hängt vom Feld ab. Bei einem Autorenfeld eignet sich beispielsweise der Phrasenindex wesentlich besser als der Wortindex. In der invertierten Datei finden sich Angaben über die Position der Wörter oder Phrasen im Dokument und Strukturinformationen. Strukturinformationen können für das Relevanceranking nützlich sein. Wenn etwa angegeben ist, dass ein Wort größer geschrieben wurde, kann man dieses auch höher gewichten. Die Wörter und Phrasen werden sowohl in der richtigen Reihenfolge geschrieben, als auch rückwärts abgelegt. Das ermöglicht eine offene Linkstrukturierung. Die Speicherung der invertierten Datei erfolgt in einem Datenbankindex.\n\nEine zweidimensionale Klassifikation von IR-Modellen zeigt die nachstehende Abbildung. Folgende Eigenschaften lassen sich bei den verschiedenen Modellen in Abhängigkeit von ihrer Einordnung in der Matrix beobachten:\n\n\nInformation-Retrieval hat Querbezüge zu verschiedenen anderen Gebieten, z. B. Wahrscheinlichkeitstheorie der Computerlinguistik.\n\n\n"}
{"id": "261442", "url": "https://de.wikipedia.org/wiki?curid=261442", "title": "64’er", "text": "64’er\n\nDie Zeitschrift 64’er – Das Magazin für Computer-Fans war ein von 1984 bis 1996 vom Markt+Technik Verlag vertriebenes Computermagazin. In den 1980er Jahren war es neben den Data-Becker-Büchern eine der wichtigsten Informationsquellen für Commodore 64-Anwender im deutschsprachigen Raum. Die Erstausgabe erschien auf der Hannover Messe 1984. Der Chefredakteur der ersten Jahre war Michael M. Pauly, der zugleich auch die Schwestermagazine \"Computer Persönlich\" und \"Happy Computer\" (Erstausgabe 11/83) betreute.\n\nNeben zahlreichen Kursen für Einsteiger und Fortgeschrittene, oft seitenlangen Listings zum Abtippen und vielerlei Tipps und Tricks wagte das Magazin auch oft einen Blick über den Tellerrand und war daher nicht nur ein relativ genaues Abbild, sondern auch ein Motor der damals existierenden Heimcomputerszene. Daraus entstanden die Ablegermagazine \"Amiga-Magazin\" (Erstausgabe Nr. 6 und 7/1987), die \"PCgo\" (Erstausgabe Nr. 10/1993) oder das Sonderheft \"Archimedes Computer-Faszination\" (ab 1993). In jedem Heft gab es ein \"Listing des Monats\" und eine \"Anwendung des Monats\" die als besonders gelungen mit einem Geldpreis von 2000 DM für das \"Listing des Monats\" (ab Ausgabe 9/1987 3000 DM) und 500 DM für die \"Anwendung des Monats\" (ab Ausgabe 9/1987 1000 DM) an den Autor dotiert war. Erwähnenswert ist, dass bei diesen Wettbewerben einige später international bekannte Programmierer und Künstler wie Chris Hülsbeck und Manfred Trenz teilnahmen und hier ihre Karriere begannen.\n\nUm dem Leser das Abtippen der Programmlistings zu erleichtern, wurden Anfang 1985 die Hilfsprogramme MSE (Eingabeeditor für Assembler-Listings) und Checksummer (Prüfsummenprogramm zur Kontrolle des eingegebenen BASIC-Programmcodes) eingeführt. Diese wurden im Laufe der nächsten Jahre immer wieder verbessert. Da sich diese Methode bewährte, wurde sie bald von anderen Computerzeitschriften übernommen.\n\nAuch viele 64’er-Sonderhefte mit einem Themenschwerpunkt (wie Assembler, Einsteiger, Programmierung, Spiele, GEOS) rund um den Commodore 64, aber auch für seine nahen 8-Bit-Verwandten C16/116, VC20, Plus/4 und C128 wurden veröffentlicht. Diesen Sonderheften lag ab Heft 46 eine Programm-Diskette bei, so dass man sich das langwierige Abtippen der Listings ersparen konnte. Das letzte Monats-Sonderheft war das Heft 97 (Thema Spiele) im Jahr 1993.\n\nMit der Ausgabe Nr. 12/1996 wurde das Magazin als eigenständige Zeitschrift eingestellt. Abonnenten bekamen fortan die im gleichen Verlag erschienene Zeitschrift \"PCgo\" oder eine Ausgabe des PC-Spielemagazins \"Power Play\" zugeschickt(eingestellt zum Jahresanfang 2000), nebst einem Faltblatt und einer Programmdiskette. Nach der Ausgabe Nr. 1/1999 fiel auch das Faltblatt weg, so dass man nur noch eine unbeschriftete Diskette als Zugabe zur \"PCgo\" erhielt. Ab Ende 1999 erfüllte eine halbjährlich erscheinende CD-ROM als Beilage diese Aufgabe. Seit 2009 wird auch dieses Medium nicht mehr produziert.\n\nAls sich das Ende der \"64’er\" als Printmagazin abzeichnete, gründeten einige C64-Fans die Zeitschrift \"GO64!\". Die gedruckte Erstausgabe erschien im März 1997.\n\n"}
{"id": "265237", "url": "https://de.wikipedia.org/wiki?curid=265237", "title": "Amiga 4000", "text": "Amiga 4000\n\nDer Amiga 4000, ein Desktop-Computer, war eine technische Weiterentwicklung des Amiga 3000 und wurde Ende 1992 eingeführt. Es gab verschiedene Versionen des Rechners. Zunächst gehörte ein 25 MHz schneller Prozessor, der Motorola 68040, zur Grundausstattung. Wenig später folgten „Sparversionen“ mit einem 25-MHz-68030 (dieses Modell wurde dann Amiga 4000/EC30 getauft) und einem 68LC040. Sogar ein Amiga 4000 mit einem 68060-Prozessor war geplant; dieses Modell mit der Bezeichnung Amiga 4000/060 wurde jedoch in Deutschland nicht mehr ausgeliefert. In den USA lieferte Quikpak, der USA-Vertrieb des Unternehmens Amiga Inc., den Amiga 4000T auch mit einem 68060 aus, wegen des erneuten Konkurses jedoch nicht mehr in Europa.\n\nDie große Neuerung des Amiga 4000 war der neue Grafikchipsatz AGA (in Deutschland AA genannt), der einen deutlichen Fortschritt gegenüber dem ECS Chipsatz des Vorgängermodells bedeutete.\n\nDer Amiga 4000T (im Tower-Gehäuse) erschien 1993 und war das letzte Modell, das Commodore vor der Liquidierung auf den Markt bringen konnte. Lediglich 200 Exemplare sollen damals ausgeliefert worden sein. Amiga Technologies legte den Amiga 4000T später nahezu unverändert neu auf.\n\n\nDer onboard vorhandene RAM-Speicher von 2 MB (Chip-RAM für die AGA-Custom-Chips und die CPU, in der ersten Platinenversion auch als PS2-Steckmodul, später gelötet) kann durch 4 PS2-Module zu jeweils 4 MB um maximal 16 MB Fast-RAM (nur für die CPU) erweitert werden.\n\nDurch Zorro-3-Steckkarten ist das RAM theoretisch auch bis 2 GB erweiterbar, in der Praxis gab es jedoch nur Erweiterungskarten mit maximal 256 MB Speicher. In den vorhandenen vier Zorro-Slots war somit bei Bestückung mit vier solcher Karten ein für die damalige Zeit enormer RAM-Ausbau von 1 GB möglich.\n\n\n"}
{"id": "265404", "url": "https://de.wikipedia.org/wiki?curid=265404", "title": "Amiga 2000", "text": "Amiga 2000\n\nDer Amiga 2000 ist ein Personal Computer aus der Amiga-Reihe. Er stellt das Highend-Gegenstück zum ebenfalls 1987 erschienenen Amiga 500 dar.\n\nDer Amiga 2000 war als Desktop-Computer konstruiert, im Gegensatz zum Amiga 500. In dem größeren Gehäuse war so Platz für Erweiterungen über den Zorro-Bus. Dieser ermöglichte (vergleichbar mit dem fünf Jahre später spezifizierten PCI-Standard) Autoconfig (vergleichbar mit Plug and Play bei Windows), während IBM-kompatible PCs zu dieser Zeit meist mit ISA-Slots ausgestattet waren. \n\nDer Amiga 2000 sollte das Profisegment besetzen, im Gegensatz zum Parallelmodell Amiga 500, was aber überwiegend auf den Bereich Bildverarbeitung bzw. TV-Stationen beschränkt blieb.\n\nDie Amiga 2000 unterschieden sich durch verschiedenen Mainboardmodelle (A und B) und den darauf befindlichen Spezialchips.\n\nDas erste Modell, der Amiga 2000A, wurde in Deutschland in der Braunschweiger Entwicklungsabteilung entworfen. Er war mit einem OCS-Chipsatz und dem Agnus-Chip des Amiga 1000 ausgestattet, der maximal 512 KB Chip-RAM ermöglicht, die aber nicht erweitert werden konnten. \n\nIm 1988 erschienenen Amiga 2000B (entwickelt in den USA) wurde der \"Fat-Agnus\"-Chip eingesetzt, der 1 MB Chip-RAM verwalten konnte. Spätere Versionen des B-Modells enthielten jedoch den ECS-Chipsatz mit \"Super Big Agnus\", der dann auch im Amiga 3000 verwendet wurde.\n\nDer ECS-Chipsatz wurde dann ausschließlich in der 1990 erschienenen Revision 6.x (nur von manchen Benutzern A2000C genannt) verbaut, mit diesem waren dann 2 MB Chip-RAM möglich. \nDes Weiteren hat der Amiga 2000, wie auch der Amiga 500, gegenüber dem Amiga 1000 das Kickstart im ROM liegen, das heißt, es muss nicht erst per Diskette geladen werden, wenn man den Computer startet.\n\nNachfolgemodelle innerhalb der Amiga-Reihe sind der Amiga 3000 und 4000.\n\n\nDer Amiga 1500 ist eine Variante des Amiga 2000 mit einem zusätzlichen (insgesamt zwei) internen Diskettenlaufwerken.\n\nVerkauft wurde er hauptsächlich von Commodore England.\n\nDer Amiga 2500, der 1989 auf den Markt kam, entspricht dem Amiga 2000, Hardware Revision 6.2 und höher. Ausgestattet wurde er mit einer 68020- (A2620) oder 68030-CPU-Karte (A2630) und einem SCSI-Controller (A2091) mit einer SCSI-Festplatte.\n\nIm Lieferumfang des Amiga 2500/UX war das Unix-Derivat AMIX enthalten.\n\n"}
{"id": "266143", "url": "https://de.wikipedia.org/wiki?curid=266143", "title": "Advanced Linux Sound Architecture", "text": "Advanced Linux Sound Architecture\n\nDie Advanced Linux Sound Architecture (ALSA) ist eine freie Soundarchitektur für Linux-Systeme, die über (PCM-)Audio- und MIDI-Funktionalität verfügt. ALSA steht unter der GPL (Treiber & Hilfsprogramme) sowie der LGPL (Anwendungsbibliotheken).\n\nALSA besteht aus Linux-Kernelmodulen, die verschiedene Kerneltreiber für Soundkarten bereitstellen. Unterschiedliche Aufgaben (allg. Sound; Midi, Wave, Synthesizer; Hardware) werden durch einzelne Gerätetreiber im Soundstack abstrahiert. Wiedergabe von Dolby Digital ist möglich. Die Ziele des ALSA-Projektes waren insbesondere die Unterstützung einer automatischen Konfiguration der Soundkarten und eine elegante Handhabung mehrerer Soundgeräte in einem System. Diese Ziele wurden größtenteils erreicht. Verschiedene Frameworks wie JACK und PulseAudio nutzen ALSA für Audiobearbeitung und -abmischung auf professionellem Niveau mit niedriger Latenz.\n\nDie wenig gepflegten Treiber für die OSS3-Architektur werden in aktuellen Kernel-Versionen zugunsten von ALSA als \"deprecated\" (veraltet) markiert.\n\nDas Projekt entstand August 1998 aus einem Treiber für die Soundkarte UltraSound des Herstellers Gravis unter der Leitung von Jaroslav Kysela. Kysela ärgerte sich über den schlechten Treiber des Open Sound System für seine Karte und schrieb einen Ersatz dafür, den auch andere Besitzer der Karte begeistert nutzten.\n\nMit Version 1.0.17 wurde das Versionskontrollsystem von Mercurial auf Git umgestellt und viele neue Treiber für System-on-a-Chip hinzugefügt. Außerdem kam eine Unterstützung für High-End Audiokarten und I²C hinzu.\n\nALSA wurde separat vom Linux-Kernel entwickelt, bis es 2002 in den Entwicklungszweig der Kernelversion 2.5.5 aufgenommen wurde. Es ist seit der Kernelversion 2.6 das standardmäßige Soundsystem.\n\nVerwenden Programme die \"ALSA Userspace Library\", ist softwareseitiges Abmischen durch ALSAs PCM-Plugin-Schnittstelle möglich, z. B. durch das \"Dmix Plugin\". Das bedeutet, dass verschiedene Soundstreams, z. B. von verschiedenen Programmen, zur gleichen Zeit wiedergegeben werden können, ohne dass ein Soundserver wie PulseAudio, ESD oder aRts, verwendet werden muss.\n\nALSA und besonders die Treiber für Soundhardware sind voll modularisiert. ALSA benötigt eigene Treiber für die anzusprechenden Geräte, weshalb OSS-Treiber nicht weiter verwendet werden können. Von Endanwendergeräten bis zu professionellen Mehrkanalkarten wird eine Vielzahl von Geräten unterstützt.\n\nFür Entwickler relevant ist, dass ALSA vollständig Multithreading- und SMP-fähig ist. Als Programmierschnittstellen dienen die \"ALSA-API\", die eine Schnittstelle zum ALSA-Kernelmodul bereitstellt und die \"ALSA Userspace Library\", libasound, die über die volle Funktionalität der ALSA-API verfügt, aber die Verwendbarkeit wesentlich verbessert. Die Verwendung der \"Userspace Library\" ist auch aus Gründen des softwareseitigen Abmischens vorteilhafter.\n\nALSA enthält eine Emulation, die \"libaoss\"-Bibliothek, für OSS-Programme, um diese weiterhin nutzen zu können.\n\nEs unterstützt unbegrenzt viele Kanäle, den unbeschränkten Full-Duplex-Betrieb und enthält ein Loopback-Device.\n\nDie Treiber für ALSA befinden sich ab Version 2.6 des Linux-Kernel direkt in ihm. Sie selbst sind in mehrere Ebenen unterteilt. Die oberste ist der ALSA-Soundkernel, der den Zugriff auf die mittlere Ebene ermöglicht. In der mittleren befinden sich die Hardwareschnittstellen wie z. B. Mixer, Sequencer, MIDI und hardwareabhängige Komponenten. Die untere Ebene enthält den karten- und chipspezifischen Code. Sofern vorhanden, befinden sich zwischen mittlerer und unterer Ebene die Module zur OSS-Kompatibilität.\n\nDie Schnittstelle zwischen den Treibern und den Anwendungen bilden dann die Bibliotheken im \"alsa-lib\"-Paket. Damit mehrere Programme gleichzeitig die Soundkarte verwenden können, kommt ein sogenannter Soundserver zum Einsatz.\n\nDieser Absatz liefert eine Übersicht über die Grundkonzepte von ALSA\n\nÜblicherweise unterstützt ALSA bis zu acht \"cards\", die mit 0 bis 7 nummeriert werden; jedes \"card\" ist entweder ein physisches oder ein logisches Kernel-Gerät, welches Klang aufnehmen, ausgeben oder kontrollieren kann. Jedes \"card\" kann über seine String-ID adressiert werden, z. B. \"\"Headset\"\" oder \"\"ICH9\"\".\n\nEin \"card\" besitzt \"devices\", deren Nummerierung startet ebenfalls mit 0; Es gibt mehrere \"device\"-Typen/-Arten, z. B. \"playback\", \"capture\", \"control\", \"timer\" oder \"sequencer\". Als Default wird 0 angenommen.\n\nEin \"device\" kann \"subdevices\" besitzen, deren Nummerierung startet mit 0; ein \"subdevice\" repräsentiert irgendeinen relevanten Klang-Endpunkt für dieses \"device\", z. B. ein Lautsprecher-Paar. Wenn kein bestimmter \"subdevice\" spezifiziert wird, oder die Nummer -1, werden alle verfügbaren subddevices angesteuert.\n\nDas \"interface\" einer \"card\" ist die Beschreibung eines ALSA-Protokols für den Zugriff darauf; Verfügbare \"interfaces\" sind: \"hw\", \"plughw\", \"default\", und \"plug:dmix\". Die \"hw\"-Schnittstelle erlaubt den direkten Zugriff auf das Kernel-Gerät, ohne software mixing oder stream adaptation. Die Schnittstellen \"plughw\" und \"default\" erlauben die Ausgabe von Klang in Fällen, wo die \"hw\"-Schnittstelle eine Fehlermeldung produziert.\n\nEine Applikation spezifiziert die Ausgabe von Klang indem die oben beschrieben Parameter zusammen in ein einem \"device string\" übergeben werden. Es hat eine der Folgenden Syntax (sie sind case sensitive):\n\nEin ALSA \"stream\" ist ein Datenfluss, der Klang repräsentiert; das häufigste Format ist PCM. Der erzeugte \"stream\" muss genau zur Hardware passen, bezüglich:\n\nALSA benötigt auch einen Puffer-Parameter; dieser bestimmt wie häufig die CPU angesprochen werden muss, um neue Klang-Daten zu liefern.\n\nDas ALSA Projekt liefert viele Werkzeuge zur Konfiguration des Soundsystems mit. Dazu gehören viele Einstellungsprogramme zur Einstellung von ALSA und der verwendeten Soundkarte, wie \"alsaconf\" als auch für den Ton wie \"alsamixer\" und \"amixer\".\n\nAußerdem sind auch einfache Abspielprogramme für WAVE- und MIDI-Dateien vorhanden.\n\n"}
{"id": "266454", "url": "https://de.wikipedia.org/wiki?curid=266454", "title": "Pegasos", "text": "Pegasos\n\nPegasos ist ein CHRP-basierter PowerPC-Desktop-Rechner von Genesi, der Betriebssysteme wie MorphOS und Linux unterstützt. Über Mac-on-Linux können auch Power Macs emuliert werden.\n\nDer Pegasos ist auch Kernbestandteil der ODW (Open Desktop Workstation) bzw. OSW (Open Server Workstation) von IBM und Freescale. Als Entwicklungsziele sind hier jedoch weniger Desktop-Rechner als z. B. eingebettete Systeme im Fokus. Auch Genesi adressiert mit dem EFIKA-Board den Markt für Thin Clients, Heimkino-Systeme und Embedded Systems bzw. Unterhaltungselektronik.\n\nÜber Lizenzierungsprogramme für Drittunternehmen war Genesi bestrebt, der Pegasos-Architektur eine breitere Basis im Markt zu verschaffen. Die Produktion und Weiterentwicklung der Pegasos-Architektur ist inzwischen eingestellt. \n\nGenesi betrachtet den IEEE 1275-kompatiblen Pegasos HAL/OF (Hardware Abstraction Layer / Open Firmware) als logischen Nachfolger des CHRP-Standards und bietet ein entsprechendes Lizenzierungsprogramm an (nominale Lizenzgebühr pro Einheit).\n\nKonsequenterweise sind Informationen zum Design und den benötigten Komponenten für die Pegasos-Hardware selbst inzwischen unter der Überschrift \"Open Hardware\" zum freien Herunterladen verfügbar. Es ist unklar, inwieweit der aktuelle Pegasos-II in seiner derzeitigen Form RoHS-konform ist.\n\n\n\nEntwickelt wurden die PPC-Motherboards des Pegasos vom Unternehmen bplan (bplan Gesellschaft für Planung und Fertigung elektrotechnischer Baugruppen mbH, Oberursel). Das Unternehmen hat seinen Sitz in derselben Stadt wie seinerzeit der bekannte Amiga-Zusatzhardware-Hersteller phase5, und veröffentlichte die erste Spezifikation der Pegasos PowerPC-Mainboards Ende 2000 (am 8. Dezember 2000). Einer der Ex-Geschäftsführer von phase5, Gerald Carda, ist auch Chefentwickler (CTO) von bplan.\n\nEs existieren seitdem zwei Pegasos-Modelle. Die erste Generation der Motherboards wurde mit dem Erscheinen des Pegasos-II vom Markt genommen, da die Boards Probleme mit dem Northbridge-Chip haben. Ende 2006 wurde auch der Pegasos-II eingestellt, wobei für das Jahr 2007 ein Nachfolger mit dem (vorläufigen) Namen \"Pegasos 8641D\" angekündigt ist. Prototypen des neuen Motherboards werden bereits an Entwickler ausgeliefert. Auf Basis der Pegasos Boards werden individuelle Komplettsysteme bei diversen Händlern angeboten, wobei das Motherboard hauptsächlich einzeln verkauft wird. Trotz der Einstellung des Pegasos-II vor der Erscheinung eines Nachfolgers ist das Board (Stand Januar 2007) noch nicht vergriffen.\n\nDer Pegasos II basiert auf einem Prozessor aus der PowerPC-Architekturfamilie. Zum Einsatz kommen G4-Prozessoren von Freescale mit einer Taktung von 1 GHz (bzw. auch mit 1,4 und 1,7 GHz für die ODW).\n\nDer Pegasos-I unterscheidet sich vom Pegasos-II hauptsächlich durch die verwendete Northbridge: die zunächst eingesetzte ArticiaS-Northbridge von MAI Logic musste mittels eines Hardwarepatches (April 2-Fix) provisorisch korrigiert werden und wurde deshalb später durch einen Chip von Marvell vollständig ersetzt. Des Weiteren verfügte der Pegasos-I lediglich über eine G3-CPU, besaß nur 100 MBit-Ethernet und verwendete PC133-SDRAM. In der Vorankündigung von bplan vom 30. Oktober 2001 war ursprünglich daher noch die Rede von einem 133 MHz Prozessor-Slot, sogar in Kombination mit „350 MHz G3 / 512k Cache“ – allerdings schon damals „bis hin zu Dual MPC 7450 G4 PowerPC / 2 MB Cache“ (ersterer als Option und letzterer „in aktuellen Taktraten“).\n\nEine Besonderheit beider Systeme ist die Auswechselbarkeit der CPU, die auf einer eigenen Tochterplatine (CPU-Board) untergebracht ist. Ähnlich wie bei PowerMacs sind CPU-Upgrades also leicht möglich.\n\nDie sonstigen Daten des aktuellen Pegasos-II sind wie folgt:\n\n\nAls weiteren Pegasos-Abkömmling, der die gleiche Pegasos HAL/OF wie der Pegasos-I/II einsetzt, bietet Genesi seit Dezember 2005 den EFIKA 5K2 an, ein „Performance Evaluation Board“, das um das PowerPC SoC (System-on-Chip) MPC5200B herum konstruiert ist. Die Leistungsdaten sind wie folgt:\n\n\nAuf der Roadmap stehen ein Quad-CPU-Serverboard auf Basis des PPC970 sowie der \"Pegasos 8641D\" als Nachfolger des Pegasos-II, wobei eine neue Generation von Freescale-CPUs zum Einsatz kommt, die Apple bedingt durch den Wechsel zur X86-Architektur nie verbaut hat und dort erstmals auf einer Workstation zum Einsatz kommen.\n\n\nDas Komplettsystem wurde von bplan offiziell auf der Amiga 2001 in Köln zusammen mit MorphOS der Öffentlichkeit vorgestellt (30. Oktober 2001), das zwei Monate zuvor für die Plattform verfügbar wurde (30. August 2001).\n\nFolgende Betriebssysteme laufen inzwischen auf dem Pegasos entweder direkt oder per Emulation bzw. befinden sich auf dem Weg der Portierung (Stand Januar 2009):\n\n\nDie Unterstützung der Pegasos-Plattform wurde aus dem OpenBSD-Kernel wieder entfernt. Mehrere Anfragen bei Maintainern des OpenDarwin-Projektes ergaben, dass eine Portierung des Systems auf die Pegasos-Plattform nie begonnen und angefragt wurde, sodass es sich bei den Angaben auf der Homepage des Herstellers um eine falsche Information handelt.\n\nDer Pegasos gilt als Amiga-Clone, da er eine Zeit lang standardmäßig mit dem zu AmigaOS 3.1 (per 68k-Emulation) binärkompatiblen Betriebssystem MorphOS ausgeliefert und entsprechend vermarktet wurde.\n\nIm Gegensatz zum ursprünglich als offiziellen Amiga-Nachfolger vorgestellten System, dem ebenfalls PowerPC-basierten AmigaOne, wird der Pegasos von seinen Befürwortern als technisch deutlich besser ausgereift angesehen, und MorphOS bietet auch eine bessere Kompatibilität zu den klassischen Amiga-Systemen. Seit dem 31. Januar 2009 steht für den Pegasos II auch AmigaOS 4 zur Verfügung.\n\nMehrere Ansätze, auch einen größeren Markt zu erreichen, scheiterten bislang anscheinend – es sind keine größeren Deals öffentlich bekannt gegeben worden, obwohl die Pressearbeit von Genesi ansonsten auf vielfältige Art stattfindet und allgemein sehr kommunikativ ist. So wurde ein OpenBSD Port begonnen, der dann jedoch aufgrund von Differenzen nicht weiter fortgeführt wurde. Damit verbunden war ein Projekt einer Netzüberwachung, der Pegasos Guardian, welcher somit nicht realisiert werden konnte. Auch wurden sogenannte Set-Top-Boxen auf Basis des Pegasos angekündigt – doch wurden nie konkrete Produkte bekannt.\n\nMöglicherweise kommt durch das EFIKA-Board etwas mehr Bewegung in diesen Bereich.\n\nDer Fokus der softwareseitigen Entwicklung seitens Genesi liegt inzwischen eher auf Linux, so dass die Zukunft von MorphOS von dieser Seite her als eher ungewiss angesehen werden darf.\n\nDie weitere hardwareseitige Entwicklung dürfte davon geprägt sein, dass die Zukunft von PowerPC-Prozessoren in Hinblick auf Desktopsysteme in Fachkreisen generell als unsicher gilt, nachdem Apple mit seinen PPC-basierten Powermacs auf x86-Prozessoren wechselt und diesen Prozess bereits Ende 2006 beendet hat. Die Zukunft des PowerPC scheint im Embedded-Bereich, bei Servern (IBM Power) und bei Unterhaltungselektronik/Konsolen (Xbox-360-CPU, Cell-Prozessor) zu liegen. Der sogenannte G6-Prozessor (auch als PPC980 lanciert und vom Power 5 abgeleitet), ist aus verschiedenen Präsentationen folgerichtig inzwischen verschwunden. Die hardwareseitige Weiterentwicklung, z. B. in Hinblick auf 64-Bit-Prozessoren wie den PPC970 (von Apple G5 genannt), schnellere Bussysteme, SATA, schnelleren FSB usw. ist derzeit ungewiss.\n\nFür Genesi positiv zu vermerken sind die qualifizierten Entwicklungs-Partnerschaften mit Freescale und IBM (ODW/OSW), die ein zunehmend solides Standbein im Embedded-Bereich vermuten lassen.\n\nWie Genesi Mitte Dezember 2005 bekannt gab, existiert mit dem Unternehmen ODM Technologies inzwischen ein Lizenznehmer der Pegasos- bzw. Efika-Technologie. Genesi hat demzufolge an ODM eine Produktionslizenz für Efika-Boards unter Verwendung des Pegasos-HAL/OF vergeben. Der Mindestproduktionsumfang beträgt 50000 Einheiten.\n\nIm Januar 2009 gab Genesi bekannt, bei zukünftigen Mainboards Prozessoren der ARM-Architektur einzusetzen.\n\n"}
{"id": "266532", "url": "https://de.wikipedia.org/wiki?curid=266532", "title": "Mac-on-Linux", "text": "Mac-on-Linux\n\nNeben den PCs der Power-Macintosh-Reihe läuft auf allen weiteren Rechnern mit PowerPC-Architektur (), darunter Macintosh-Klone wie der Pegasos, aber auch der AmigaOne.\n\nDa die derzeit letzte Version vom Juni 2007 stammt, ist es im besten Fall problematisch, auf einem aktuellen Linux/PPC-System zu nutzen.\n\nBei handelt es sich nicht um einen klassischen Emulator, sondern um eine sogenannte virtuelle Maschine (siehe Virtualisierung). Es wird im Gegensatz zu vielen Mac-Emulatoren kein ROM-Image eines Mac-Rechners benötigt, um klassisches Mac OS bzw. Mac OS X in der virtualisierten Umgebung installieren und nutzen zu können. Für manche Funktionen nutzt Teile bereits vorhandenen offenen Quelltextes von Basilisk II (X11 und Ethernet) und QEMU (QCOW-Format für virtuelle Festplatten).\n\n\nAuf dem Wirtsystem sind folgende Frontends implementiert:\n\nIm Quellpaket von liegen die Quelltexte für Mac-OS-X-Treiber bei, die unter Mac OS X kompiliert werden müssen. Voraussetzungen sind jedoch Xcode, das Mac-OS-X-10.2.8-SDK und das X11-SDK. Daraus lässt sich auch ein Installationspaket für Mac OS X erstellen, das dann auf Mac OS X 10.2 („Jaguar,“ 2002) bis Tiger (10.4, 2005) funktionieren sollte. Die Installation der MOL-Treiber ist auch unter einem nativ laufenden Mac OS X möglich, da die Treiber nur genutzt werden, wenn Mac OS X unter ausgeführt wird. So lässt sich das Apple-Betriebssystem je nach Bedarf nativ oder unter von derselben Partition nutzen. Unter Debian-basierten Linux/PPC-Distributionen gab es das Paket codice_1, welches die Treiber bereits in binärer Form enthielt.\n\nEin verwandtes Projekt ist der Emulator PearPC, der eine vollständige PowerPC-Emulation auf der x86-Architektur zur Verfügung stellt.\n\nGeschaffen von Samuel Rydh wurde unter dem Dach der Firma Ibrium im offenen Entwicklungsmodell gepflegt und weiterentwickelt. Laut der Versionsgeschichte begann die Entwicklung am 1. Juni 1997. Sieben Jahre später, am 21. März 2004, beendete Samuel Rydh mit der Veröffentlichung der Version 0.9.70 sein Engagement für MOL. Danach wurde das Projekt auf SourceForge von Joseph Jezak gepflegt, der Quelltext wurde in ein SVN-Repository überführt.\n\nDennoch hat sich seit der Veröffentlichung von Version 0.9.72.1 im Juni 2007 nicht viel getan. Einer der naheliegenden Gründe könnte Apples Umstieg auf die x86-Architektur sein, der 2006 abgeschlossen wurde. Dadurch ist es auf x86-Hardware möglich, Mac OS X/Intel (ab Tiger, Version 10.4.4, 2006) direkt zu verwenden (Hackintosh), für Mac OS bis Version 9 muss auf Emulationsprogramme wie SheepShaver zurückgegriffen werden.\n\nAuf 64-Bit-PowerPC-Prozessoren wie dem G5, wie er im Power Mac G5, Xserve G5 und iMac G5 Verwendung findet, ist nicht lauffähig, obwohl erste Anpassungen dafür bereits 2007 begonnen wurden.\n\n unterstützt folgende Gastsysteme:\n\nDie aktuelle Version 0.9.72.1 von Mac-on-Linux stammt vom Juni 2007 und wurde damals auf den Kernel 2.6.22 angepasst. Für neuere Linux-Versionen existieren vereinzelt Patches, doch auch diese folgen nicht dem aktuellen Stand der Kernel-Entwicklung.\n\nWährend Mac OS X Tiger (10.4, 2005) keine Probleme macht, funktioniert Mac OS X Leopard (10.5, 2007) (mit Stand 2017) nicht unter .\n\nDa die Apple-EULA die Nutzung von Mac OS auf nicht-Apple-PCs untersagt, kann es legal nur auf Apple-Rechnern unter Linux verwendet werden, sofern die EULA im jeweiligen Land in diesem Punkt ihre Gültigkeit hat. In Deutschland ist diese Klausel als nichtig anzusehen, es darf also mit MOL auch auf jedem anderen PowerPC-Rechner ein legal erworbenes Mac OS virtualisiert werden.\n\n\n"}
{"id": "267127", "url": "https://de.wikipedia.org/wiki?curid=267127", "title": "Linux68k", "text": "Linux68k\n\nLinux68k ist ein Oberbegriff für Linux-Derivate für die 68k-Architektur von Motorola (ab Motorola 68020 mit Memory Management Unit). Es ist in Form verschiedener Linux-Kernel-Distributionen zum Beispiel auf 68k-Macs, auf Atari STs oder auf Amiga-PCs lauffähig.\n\nEine erste Portierung des Linux-Kernels auf die 68k-Prozessorarchitektur war 1993 für Amiga-Computer verfügbar, womit der Linux-Kernel erstmals abseits der x86-Architektur lief. Die erste kommerzielle Distribution ALD lieferte delta labs media für Atari TT und Atari Falcon ab Anfang 1995. Verschiedene weitere Projekte verfolgten die 68k-Portierung von Linux, so war etwa auch die weit verbreitete Linux-Distribution Debian ab Version 2.0 (1998) für 68k-Rechner verfügbar (mit Version 4.0 wurde der offizielle Support fallengelassen, inoffiziell wird die Entwicklung jedoch weitergeführt). Im Portage-Paketsystem der Distribution Gentoo Linux wird die m68k-Architektur berücksichtigt, allerdings wird erst seit Mai 2013 ein für die Installation notwendiges Stage-Archiv auf manchen Spiegelservern des Gentoo-Projektes im experimentellen Zustand zur Verfügung gestellt, zuvor musste ein solches Archiv vom Anwender per Cross-Compiling erstellt werden. Linux68k-Distributionen können auf moderner Hardware im Atari-TT-Emulator ARAnyM ausgeführt und getestet werden.\n\nFür neuere 68k-Systeme ohne MMU wie den Dragonball ist μClinux verfügbar, das mittlerweile in den Standard-Linux-Kernel eingeflossen ist, für klassische 68000-Systeme, speziell den Amiga 500, war ein µClinux-Fork in Entwicklung.\n\n"}
{"id": "268165", "url": "https://de.wikipedia.org/wiki?curid=268165", "title": "Shrek 2 – Der tollkühne Held kehrt zurück", "text": "Shrek 2 – Der tollkühne Held kehrt zurück\n\nShrek 2 – Der tollkühne Held kehrt zurück ist ein US-amerikanischer Computeranimationsfilm von DreamWorks SKG und bildet die Fortsetzung des Animationsfilms \"Shrek – Der tollkühne Held\" aus dem Jahr 2001. Am 1. Juli 2004 lief er in den deutschen Kinos an. Seine Fortsetzung \"Shrek der Dritte\" startete am 21. Juni 2007 in Deutschland. \"Shrek 2\" war mit einem Einspielergebnis von 880 Millionen US-Dollar der erfolgreichste Animationsfilm, bis er 2009 von Ice Age 3 und 2010 von Toy Story 3 überholt wurde.\n\nNach ihren Flitterwochen statten der grüne Oger Shrek und Prinzessin Fiona der königlichen Familie einen Besuch ab. Fionas Eltern, insbesondere ihr Vater, sind von ihrem Schwiegersohn jedoch wenig begeistert und würden lieber den Schönling Prinz Charming an der Seite ihrer Tochter sehen. Um Shrek loszuwerden, beauftragt der König einen Auftragsmörder – den gestiefelten Kater. Der Anschlag misslingt, und der gestiefelte Kater wird zu Shreks treuem Begleiter. Gemeinsam rauben sie der Mutter von Prinz Charming, der „guten Fee“, einen Zaubertrank. Shrek und der Esel trinken beide davon, worauf hin sie sich in einen attraktiven jungen Mann und ein edles Pferd verwandeln. Auch Fiona wird vom Zauber getroffen und nimmt ihre menschliche Gestalt an.\n\nDie gute Fee, deren Charakter im Verlauf des Filmes immer düsterer erscheint, zwingt den König, Fiona einen Zaubertrank zu verabreichen. Dieser soll dafür sorgen, dass Fiona sich in den Mann verliebt, der sie nach Einnahme des Tranks als erster küsst. Während die Fee Shrek verhaften und einkerkern lässt, gibt sich Prinz Charming Fiona gegenüber als Shrek in Menschengestalt aus und trachtet danach, sie auf einem Ball dazu zu bringen, ihn zu küssen.\n\nShrek kann aus dem Gefängnis flüchten und dringt in das Schloss ein, um den Kuss zwischen Fiona und Prinz Charming zu verhindern. Nachdem Shrek sich in seiner Menschengestalt Fiona zu erkennen gegeben hat, ergreift Prinz Charming Fiona und küsst sie gegen ihren Willen. Der König hat seiner Tochter jedoch den Zaubertrank nicht verabreicht. Charming empfängt daher anstatt ihrer Liebe einen Schlag ins Gesicht. Die Fee, deren Handeln und Auftreten nun endgültig dem einer bösen Hexe entspricht, will Shrek daraufhin vernichten. Der König springt dazwischen, seine Rüstung reflektiert den Fluch, die Fee zerplatzt und Fionas Vater wird zum Froschkönig – seiner ursprünglichen Gestalt. Fiona entschließt sich, gemeinsam mit Shrek wieder Ogergestalt anzunehmen, indem sie die Frist, in der der Zauberkuss geschehen muss, verstreichen lässt. Fionas Mutter schließt den Froschkönig trotz seiner neuen Gestalt ins Herz, da er charakterlich jetzt mehr Mensch sei als zuvor.\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh dem Film das Prädikat besonders wertvoll.\n\n\nWie im ersten Teil besteht die Musik im Film aus Pop- und Rocksongs. (Die Liste entstammt dem Soundtrack zum Film.)\n\nDie deutsche Synchronisation wurde wieder von der Berliner Synchron AG, unter der Dialogregie von Michael Nowka – der auch den Zauberspiegel spricht –, produziert.\n\n"}
{"id": "268726", "url": "https://de.wikipedia.org/wiki?curid=268726", "title": "Naval Ordnance Research Calculator", "text": "Naval Ordnance Research Calculator\n\nDer Naval Ordnance Research Calculator (NORC) ist ein 1954 von IBM fertiggestellter Supercomputer für militärische Zwecke, der im Auftrag der United States Navy entwickelt und auch beim Nachrichtendienst National Security Agency (NSA) eingesetzt wurde.\n\nTechnischer Leiter der Entwicklung (1950 bis 1954) war Byron Havens.\n"}
{"id": "268739", "url": "https://de.wikipedia.org/wiki?curid=268739", "title": "Mark I (Computer)", "text": "Mark I (Computer)\n\nDer Mark I, auch Automatic Sequence Controlled Calculator (ASCC) genannt, ist ein in den USA zwischen 1943 und 1944 vollständig aus elektromechanischen Bauteilen gebauter, früher Computer. Der Rechner wurde von Howard H. Aiken von der Harvard-Universität in Cambridge, Massachusetts, und IBM-Ingenieuren (Clair Lake, Frank E. Hamilton, Benjamin Durfee, James W. Bryce) entwickelt und von IBM gebaut. Er hat ein Gewicht von 5 Tonnen bei einer Frontlänge von 16 Metern.\n\nDer Rechner wurde von der US-amerikanischen Marine zwischen 1944 und 1959 unter anderem für ballistische Berechnungen genutzt. Das erste Programm ließ John von Neumann 1944 für das Manhattan Project (Rechnungen am Implosionskonzept der Plutonium-Bombe) laufen.\n\n1998 wurde bewiesen, dass der \"Mark I\" turingmächtig war. Damit war er nach der Zuse Z3 der zweite turingmächtige Computer.\n\nDer \"Mark I\" steht heute im Cabot Science Building der Harvard University. Für ihren Beitrag zum Mark I wurden Aiken, Durfee, Hamilton und Lake 2014 in die National Inventors Hall of Fame aufgenommen. \n\nDer Vorstandsvorsitzende von IBM, Thomas J. Watson war verärgert, dass sich Aiken als alleiniger Erfinder der \"Mark I\" ausgab, und nur James W. Bryce, seinen unmittelbaren Kontakt bei IBM erwähnte. Das Watson Scientific Computing Laboratory entwickelte daraufhin unter der Leitung von Wallace John Eckert 1946/47 den Selective Sequence Electronic Calculator.\nAiken hingegen konstruierte 1947/48 einen weiteren Relais-Rechner \"Mark II\"; 1949 folgte -ebenfalls im Auftrag der U.S. Navy- der \"Mark III\", der bereits teilweise mit Vakuumröhren und Dioden sowie Magnettrommelspeicher ausgestattet war, und 1952 der Mark IV für die U.S. Air Force als rein elektronisches Gerät mit Magnetkernspeicher.\n\n\n"}
{"id": "268831", "url": "https://de.wikipedia.org/wiki?curid=268831", "title": "Power Mac G4 Cube", "text": "Power Mac G4 Cube\n\nDer PowerMac G4 Cube (: englisch für „Würfel“) war ein Rechnermodell (Personal Computer) des Unternehmens Apple, das am 19. Juli 2000 auf der MacWorldExpo in New York von Steve Jobs angekündigt, als technischer Olymp gepriesen und nur kurze Zeit (19. Juli 2000 bis Juli 2001) gebaut wurde. Außer dem Netzteil war die gesamte Hardware des Rechners in einem Quader aus Acryl untergebracht, der nur ca. 20×20 cm Grundfläche hatte.\n\nDer Rechner wurde so konstruiert, dass auf einen aktiven Lüfter (wie beim allerersten Macintosh von 1984) verzichtet werden konnte: Im Gehäuse befinden sich oben und unten Lüftungsöffnungen, durch die erwärmte Luft (nach oben) entweichen kann und kühlere Luft von unten nachgeführt wird.\nDurch diese Bauweise ist das einzige, was vom Cube im Betrieb zu hören ist, mechanische Geräusche der Festplatte vor allem beim Lese-/Schreibvorgang.\n\nVerbaut wurden PowerPC G4-Prozessoren mit 450 MHz (normale Version) oder 500 MHz (optional). An optischen Laufwerken standen ein Slot-in-DVD-ROM-Laufwerk bzw. später ein CD-RW-Laufwerk zur Auswahl. Die maximale Speicherkapazität (RAM) lag bei 1,5 GB PC100-SDRAM in drei Sockeln. Für den drahtlosen Netzwerkbetrieb (WLAN) war ein Steckplatz für eine AirPort-Karte vorhanden. Modem, 10-/100-BaseT-Netzwerkanschluss sowie jeweils zwei USB- und FireWire-Anschlüsse waren ebenfalls auf der Hauptplatine untergebracht. Zur Standardausstattung gehörten 64 MB Arbeitsspeicher (RAM), eine 20-GB-Festplatte sowie als Grafikkarte eine ATI Rage 128 Pro mit 16 MB RAM. Beim 500-MHz-Modell wurde eine ATI-Radeon-Karte mit 32 MB Bildspeicher oder eine NVidia-Karte eingesetzt. Im Lieferumfang des Cubes waren außerdem enthalten: eine ApplePro USB-Tastatur mit transparenter Eintastenmaus sowie zwei kleine USB-Lautsprecherkugeln von Harman Kardon, die an den USB-Anschluss des Cubes (bzw. des Bildschirms) angeschlossen werden konnten. Das vorinstallierte Betriebssystem war Mac OS 9.0.4 (in der überarbeiteten Version des Cubes vom Frühjahr 2001 Mac OS 9.1).\n\nPeripherieanschlüsse waren:\n\nOptional wurde der Cube mit einem 15-Zoll-TFT-StudioDisplay verkauft, das eine Auflösung von 1024 × 768 Pixeln hatte.\n\nApple entwickelte seinerzeit auch an einer Gigabit-Ethernet-Option (es sind mindestens zwei Exemplare entsprechender interner Steckkarten bekannt, welche die serienmäßige 100-MBit/s-Steckkarte ersetzen); aufgrund des frühzeitigen Verkaufsstops kam es aber nicht mehr zu einer großen Verbreitung.\n\nApple-Designer Jonathan Ive gewann internationale Preise für das Design. Der G4 Cube findet sich in vielen Publikationen über Design und Produktdesign sowie ausgestellt in einigen Technikmuseen wieder.\nEr ist auch einer der wenigen Computer, die im New Yorker Museum of Modern Art (MoMA) ausgestellt sind.\n\nViel Kritik wurde am Cube geübt, weil er intern kaum erweiterbar war. Gegenüber seinem größeren Bruder, dem Power Mac G4, fehlen dem Cube die vier PCI-Erweiterungssteckplätze und die AGP-Grafikkarte musste eine bestimmte Größe aufweisen, um in den Cube überhaupt hineinzupassen. So bleibt lediglich mehr Arbeitsspeicher, eine passende Grafikkarte, eine größere interne Festplatte (mehr als 128 GB nur mit OS-X-Treiber) für die einzige interne 3,5″-Festplattenaufnahme und eventuell noch ein anderes optisches Laufwerk, um den Cube zu erweitern. Wer mehr erweitern wollte, war auf externe Geräte am USB- oder FireWire-400-Anschluss angewiesen. Weitere Kritikpunkte waren der um etwa 10 % höhere Preis gegenüber den anderen G4-Rechnern und das Fehlen eines CD-Brenners, der von Apple erst in einer späteren Revision verbaut wurde.\n\nFür Apple geriet der Cube finanziell zum Flop. Wegen seiner schlechten Verkaufszahlen (148.000 laut Apple) belastete er den Wert der Apple-Aktie nachhaltig – erst der iPod konnte die Krise beenden. Technologisch war der Cube dagegen ein Vorreiter, der den Weg freimachte für thermisch anspruchsvolles Computerdesign.\n\nAm 3. Juli 2001 gab Apple bekannt, dass der Cube mangels Verkaufserfolgs eingestellt wurde. Grund für die zögerlichen Verkäufe waren vor allem der hohe Preis des Cubes sowie möglicherweise seine schlechte Erweiterbarkeit.\n\nDen Support für den Cube stellte Apple am 6. August 2008 endgültig ein, wobei schon zuvor nur noch für in Kalifornien gekaufte Geräte Ersatzteile bestellt werden konnten.\n\nDie Käufer des Cube lieb(t)en ihn wegen seiner kompakten Bauweise und der Ruhe im Büro durch die lüfterlose Bauweise. Bereits kurze Zeit nach seiner Produktionseinstellung avancierte der Cube zum Kultobjekt, was in erster Linie an seinem außergewöhnlichen Design (der Würfelform in einem transparenten Plexiglas-Gehäuse) lag, zum anderen aber auch an seinem lüfterlosen Konzept.\nUm dies zu ermöglichen, wird die warme Luft durch Konvektion abtransportiert. Daher hat der Cube zwei Öffnungen: eine auf der Rückseite des Gehäuses, durch die Kaltluft eintreten kann, und eine Öffnung oben, durch die warme Luft austreten kann. Der Effekt ist ähnlich wie bei einem Schornstein (Kamineffekt).\n\nNoch heute gibt es eingeschworene Fans des Cubes, die ihren Lieblingscomputer durch Aufrüstungen schneller machen. Vor allem der Austausch des Prozessors (im August 2005 bis 1,8 GHz möglich) und ein Austausch der Grafikkarte (am beliebtesten ist die lüfterlose Geforce 2 MX, die es auch in einer Cube-Version gab) sind beliebt. Mit der weiten Verbreitung von Solid-State-Drives werden auch diese gerne in den Cube eingebaut, da dann gar kein Betriebsgeräusch mehr entsteht. Die schnelleren Prozessoren benötigen jedoch meist einen Lüfter, so dass das lüfterlose Konzept untergraben wird.\n\nWegen seines senkrecht eingebauten Slot-In-Laufwerks wurde er auch Apple-Toaster genannt (die ausgeworfene CD erinnert an Toastbrot).\n\nEine weitere Innovation war der sog. , was dem Sinn nach als „Annäherungsschalter“ übersetzt werden kann. Bloße Positionierung der Hand oder eines Fingers über einem Bereich des Gehäuses schaltet den Cube ein.\n"}
{"id": "269770", "url": "https://de.wikipedia.org/wiki?curid=269770", "title": "Szenengraph", "text": "Szenengraph\n\nEin Szenengraph ist eine Datenstruktur, die häufig bei der Entwicklung computergrafischer Anwendungen eingesetzt wird. Es handelt sich um eine objektorientierte Datenstruktur, mit der die logische, in vielen Fällen auch die räumliche Anordnung der darzustellenden zwei- oder dreidimensionalen Szene beschrieben wird.\n\nDer Begriff Szenengraph ist nur unscharf definiert. Dies liegt daran, dass konkrete Szenengraphen in der Regel anwendungsgetrieben entwickelt werden. Die Programmierer nutzen also die Grundidee, passen sie aber für die spezifischen Erfordernisse der Anwendung an. Feste Regeln, welche Funktionen ein Szenengraph erfüllen muss, gibt es daher nicht.\n\nAus graphentheoretischer Sicht ist ein Szenengraph ein zusammenhängender gerichteter Graph ohne gerichtete Kreise, dessen Wurzelknoten die Gesamtszene (das „Universum“) enthält. Dieser Wurzel untergeordnet sind Kindknoten, die die einzelnen Objekte der Szene, oder Eigenschaften wie Transformationen und Farben enthalten. Diese Knoten können wiederum Wurzel eines weiteren Baumes, also einer weiteren Hierarchie von Objekten sein. Da es sich um einen Graphen, nicht um einen Baum, handelt, kann ein Knoten auch mehrere Elternknoten haben.\n\nDieser Ansatz ermöglicht die hierarchische Modellierung der Objekte in einer Szene. Jeder Knoten des Szenengraphen hat üblicherweise eine Transformationsmatrix. Bei Manipulation dieser Matrix wird das zugehörige Objekt selbst, aber auch die Objekte aller untergeordneten Knoten transformiert. Man unterscheidet in diesem Fall zwischen Objektkoordinaten (Koordinaten eines Objektes bezüglich des übergeordneten Objektes) und Weltkoordinaten (Koordinaten eines Objektes bezüglich des Ursprungs des Universums – der Wurzel des Szenengraphen). Durch diese hierarchische Sicht wird der Aufbau und das Manipulieren einer Szene deutlich vereinfacht. Man muss nicht jedes Einzelteil eines Objektes einzeln transformieren, sondern transformiert einfach die Gesamtheit aller Einzelteile. Enthält eine Szene viele Kopien eines Objekts, so können all diese Kopien durch ein Objekt repräsentiert werden. Es gibt dann mehrere Wege von der Wurzel zu dem Knoten mit diesem Objekt. Jeder Weg mit seinen eigenen Transformationen und anderen Eigenschaften. Man spricht von Instancing.\n\nAls Beispiel mag die Modellierung eines Autos mit vier Rädern dienen. Ein Knoten im Szenengraph repräsentiert das Objekt Auto. Dieser Knoten hat vier Kindknoten, die jeweils die Transformationsmatrizen und Rotationsmatrizen der einzelnen Räder enthalten. Diese vier Kindknoten wiederum haben ein und den gleichen Kindknoten der ein Objekt vom Typ Rad enthält. Ein Objekt – vier Darstellungen. Wird die Position oder die Lage des Auto-Knotens verändert, so wirkt sich die Veränderung auch auf alle Kindknoten, also in diesem Fall die Räder, aus. Eine manuelle Neuberechnung der Position der Räder ist also nicht erforderlich.\n\nOft werden Szenengraphen eingesetzt, um die Szenen einer Anwendung effizienter zu rendern oder um Berechnungen wie Kollisionsabfragen zu beschleunigen. Dazu wird zusammen mit einem Szenengraphen eine Hierarchie aus Bounding Volumes mitgeführt. Jedem Knoten ist also zusätzlich ein Bounding Volume zugeordnet, das die räumliche Ausdehnung des Knotens samt Kindknoten anzeigt. Als Bounding Volumes werden einfache geometrische Körper wie achsenparallele Quader (AABBs), am Objekt ausgerichtete Quader (OBBs) oder Kugeln verwendet.\n\nMit Hilfe der Bounding Volumes werden dann vor dem Rendervorgang alle unsichtbaren (also nicht im View Frustum liegenden) Elemente bestimmt. Wenn ein Knoten bereits als nicht sichtbar klassifiziert wurde, ist eine Überprüfung seiner Kindknoten nicht mehr notwendig. So kann mit geringem Aufwand die Menge der Geometrie, die potentiell sichtbar ist und darum gerendert wird, verringert werden.\n\n"}
{"id": "271376", "url": "https://de.wikipedia.org/wiki?curid=271376", "title": "Ereignisorientierte Simulation", "text": "Ereignisorientierte Simulation\n\nEreignisorientierte Simulation oftmals abgekürzt als DES (\"discrete event simulation\") ist eine Art der diskreten Simulation. Bei der ereignisorientierten Simulation erfolgt der Simulationsfortschritt durch Abarbeitung einer Liste von Ereignissen (englisch: \"events\").\n\nEin ereignisorientiertes Simulationsmodell kann durch einen endlichen Automaten, einen Ereigniskalender (oder eine Ereignisliste), Ereignisroutinen und die Zeit eindeutig beschrieben werden. Der endliche Automat beschreibt die Übergänge zwischen den Ereignissen, der Ereigniskalender enthält eine Liste zukünftiger Ereignisse mit dem Namen und der Art des Ereignisses und dem Zeitpunkt seines Eintretens. Die Ereignisroutinen enthalten Anweisungen zur Ausführung je eines Ereignistyps und legen die Logik fest, nach der die Simulation abzulaufen hat. Dabei können bestimmte Ereignisse wiederum neue Ereignisse in der Zukunft (niemals in der Vergangenheit) auslösen. Somit können auch komplexe Verhalten simuliert werden. In der programmiertechnischen Umsetzung wird die Ereignisliste nach dem Eintrittszeitpunkt sortiert und immer das als nächstes eintretende Ereignis abgearbeitet.\nNur wenn es sich beim Zustandsmodell um ein formales Modell handelt, kann das zugehörige Simulationsmodell mit Hilfe eines Computerprogramms verarbeitet werden (Kausalität).\nEine Simulation endet in der Regel, wenn keine Ereignisse mehr im Ereigniskalender vorhanden sind oder eine definierte simulierte Zeit erreicht wurde.\n\nVorteil dieser Art der Simulation im Vergleich z. B. zur zeitdiskreten Simulation ist, dass nur die jeweiligen Ereignisse simuliert werden und nicht die Zeit zwischen selbigen. Dies ermöglicht es lange Zeiträume innerhalb kurzer Zeit zu simulieren. Ereignisorientierte Simulationen werden z. B. im Bereich der Rechnernetze verwendet: ns-3 und OMNeT++ sind zwei Beispiele für Frameworks bzw. Simulatoren, die ereignisorientierte Simulationen verwenden.\n\nAm Beispiel einer Fußgängerampel soll nun die Ereignisorientierte Simulation beschrieben werden. Wir nehmen an, dass die Vorzugsrichtung der Straßenverkehr ist und dauerhaft grün hat. Kommt ein Fußgänger an die Ampel und betätigt den Taster, wechselt die Ampel für den Fahrzeugverkehr auf rot und für den Fußgänger auf grün. Nach einer definierten Zeit wechselt die Ampel wieder auf rot für den Fußgänger und grün für die Fahrzeuge. Zur Vereinfachung nehmen wir an, dass die Ampel nur rot und grün (ohne gelb) zeigt.\n\nDas Zustandsmodell besteht aus folgenden Zuständen:\n\nDie Übergänge sind wie folgt definiert:\n\nWir gehen davon aus, dass unsere Simulation zum Zeitpunkt t=0 startet und zum Zeitpunkt t=10 ein Fußgänger die Ampel erreicht. Folgende Ereignisse werden nun simuliert:\n\n\nDieses Beispiel zeigt ein die grundsätzliche Idee hinter der ereignisorientierten Simulation: Statt die gesamte Zeitdauer von 24 Sekunden zu simulieren, werden nur die für die Ergebnisse wichtigen Schritte simuliert.\n\nDieses Beispiel lässt sich nun recht einfach zu einem praxisnahen Simulationsszenario erweitern. Man könnte z. B. annehmen, dass Fußgänger im Mittel alle 15 Sekunden normalverteilt an der Ampel ankommen und 5 Sekunden für die Überquerung der Straße benötigen. Mit Hilfe des obigen Beispieles könnte man nun simulieren, wie groß die Wahrscheinlichkeit ist, dass 1, 2, 3... Fußgänger in einer Grünphase die Straße überqueren oder wie man die Zeiten anpassen müsste, um möglichst viele Fußgänger mit möglichst wenigen Rotphasen für den Fahrzeugverkehr die Straße queren zu lassen.\n"}
{"id": "271838", "url": "https://de.wikipedia.org/wiki?curid=271838", "title": "Make Money Fast", "text": "Make Money Fast\n\nSchnelles-Geld-Briefe („schnell viel Geld verdienen“) oder englisch Make Money Fast (MMF) bezeichnet eine Art von Kettenbriefen nach dem Schneeballsystem, die heute vor allem im Internet kursieren. Es handelt sich dabei um einen Netzmissbrauch, der mit allen zum Veröffentlichen geeigneten Medien vorgenommen werden kann, im Internet insbesondere als Spam im Usenet und per E-Mail.\n\nDas Prinzip ist einfach: Mit einem verwirrenden, sehr langen, emotionalisierten Text erzählt der Initiator eine erschwindelte Lebensgeschichte, wonach er früher bettelarm war und heute steinreich ist und lädt die Leserschaft zur Nachahmung ein. Diese besteht darin, an 5 oder 6 Personen, die im Schreiben mit Name und Adresse aufgeführt sind, je einen sehr kleinen Bargeld-Betrag (1 Dollar oder Euro) per Post zu versenden, dann die Liste um den eigenen Namen mit Adresse zu ergänzen und das ganze an sehr viele Empfänger weiterzuschicken.\n\nDas Initiieren und Weiterverbreiten solcher Schreiben sind im Usenet sehr verbreitet. Es handelt sich aber um Spam mit hohem Breidbart-Index. Solche Beiträge werden deshalb häufig sofort gelöscht.\n\nNetzmissbrauchsbeauftragte von Providern berichten, dass vor allem junge (also lebensunerfahrene) Benutzer auf die Idee kommen, mitzuspielen. Mathematische Betrachtungen haben ergeben, dass bestenfalls nur der Initiator selbst und ganz wenige Ebenen seiner Nachfolger auf der Liste wirklich Geld erhalten können. Durch die Tatsache, dass auf der Erde nur eine begrenzte Zahl Menschen leben, kann sich der Schneeballeffekt nicht unendlich fortsetzen. Selbst unter der illusorischen Annahme, dass alle Empfänger teilnehmen würden, so dass die Kette niemals reißt, steigt die Zahl der Beteiligten exponentiell. Wenn jeder Teilnehmer den Brief nur an 2 Empfänger weiterschickt, würde die Gesamtzahl der Empfänger schon nach 22 Umläufen die Weltbevölkerung übersteigen, was offenkundig nicht möglich ist. Wenn in diesem Szenario nur die folgenden 6 Stationen einen Geldbetrag von z. B. 1 Dollar versenden, erhalten selbst frühe Teilnehmer nur formula_1 Dollar. Nur knapp 4,8 Millionen Teilnehmer können diese Summe empfangen. Wenn jeder der Empfänger mehr Briefe weiterschickt, steigen zwar die Gewinnchancen, aber die Zahl der Empfänger wächst noch schneller, so dass nur wenige am Anfang der Kette einen hohen Gewinn erhalten können. Bei 10 Empfängern ist die Grenze schon bei 11 Umläufen überschritten.\n\n\n"}
{"id": "272157", "url": "https://de.wikipedia.org/wiki?curid=272157", "title": "MegaCAD", "text": "MegaCAD\n\nMegaCAD ist ein CAD-Programm von der deutschen Softwarefirma Megatech Software GmbH. Es ist lauffähig unter den 32- und 64 Bit-Versionen von Microsoft Windows 7, 8.1 und 10.\n\nDas Programm ist in verschiedenen Leistungsstufen erhältlich, angefangen von einer Einsteiger-Lösung (\"MegaCAD Lt\") über \"MegaCAD 2D\" und \"MegaCAD 3D\" bis zur vollausgestatteten \"MegaCAD Profi plus\". Neben dem Grundprogramm werden verschiedene Speziallösungen für unterschiedliche Fachbereiche angeboten:\n\n"}
{"id": "272628", "url": "https://de.wikipedia.org/wiki?curid=272628", "title": "Linux Professional Institute", "text": "Linux Professional Institute\n\nDas Linux Professional Institute (LPI) ist eine 1999 in New Brunswick, Kanada gegründete Non-Profit-Organisation, die professionelle Zertifizierungen für das Betriebssystem GNU/Linux entwickelt, welche unabhängig von Software- oder Schulungsanbietern (also weitgehend distributionsunabhängig) sind. Bisher wird ein Zertifizierungsprogramm, Linux Professional Institute Certification (LPIC), angeboten, dessen erste Stufe allgemeines Linux-Wissen prüft, wie es für „ambitionierte Anwender“, Systemadministratoren, Entwickler oder Berater wichtig ist. Dabei tendieren die höheren Stufen deutlicher in Richtung Systemadministration. In Deutschland, Österreich und der Schweiz ist das Linux Professional Institute durch den 2003 in Karlsruhe gegründeten \"LPI e. V.\" vertreten.\n\nDas LPI bietet Prüfungen an, die in ihrer Abfolge zu den Zertifikaten LPIC-1 („Junior Level Linux Professional“), LPIC-2 („Advanced Level Linux Professional“) und LPIC-3 („Senior Level Linux Professional“) führen. Für LPIC-1 und LPIC-2 müssen jeweils zwei Prüfungen abgelegt werden; LPIC-3 besteht aus einer Basisprüfung, LPI-301 (die alleine zum Erwerb des Zertifikats führt), und optionalen Spezialisierungsprüfungen. Die LPI-Prüfungen können in beliebiger Reihenfolge abgelegt werden, allerdings bekommt man ein Zertifikat erst, wenn alle Voraussetzungen für dessen Erwerb gegeben sind, und dazu gehört neben dem Bestehen der Prüfungen der betreffenden Ebene auch der Besitz des Zertifikats der Ebene darunter (falls es eine gibt).\n\nDie Prüfungen selbst bestehen aus jeweils 60 Fragen (LPI-301: 50), die in maximal 90 Minuten beantwortet werden müssen. Bei den Fragen handelt es sich entweder um Multiple-Choice-Fragen mit entweder einer oder mehreren richtigen Antworten oder aber um Textfragen, bei denen man die Antwort (typischerweise den Namen einer Datei oder eines Kommandos) eintippen muss.\n\nEs gibt zwei Möglichkeiten, LPI-Prüfungen abzulegen:\nDie meisten Prüfungen stehen nicht nur in Englisch, sondern auch in anderen Sprachen (Deutsch, Japanisch, Chinesisch, Französisch, Portugiesisch, Spanisch) zur Verfügung.\n\nDas LPI macht keine Vorschriften darüber, wie man sich auf die Prüfungen vorbereitet (Pflichtkurse, offizielle Lernmaterialien, …), sondern beschränkt sich darauf, für jede Prüfung „Prüfungsziele“ (\"objectives\") zu nennen und deren Inhalte kurz zusammenzufassen. Das „LPI-ATM“-Programm („LPI Approved Training Materials“) war dafür gedacht, Lernmaterialien auf Vollständigkeit im Sinne der Prüfungsziele und didaktischen Sinn hin zu zertifizieren, wurde aber 2009 weltweit abgekündigt; es ist den regionalen Organisationen des LPI freigestellt, das Programm weiterzuführen. Zur Vorbereitung auf LPI-Prüfungen gibt es außer LPI-ATM-zertifizierten Lernmaterialien Bücher im Buchhandel, kostenlos oder frei verfügbares Studienmaterial aus dem Internet, und natürlich die klassischen Linux-How-tos und -Manuals, die Bestandteil der meisten Linux-Distributionen sind.\n\nDas LPI hat weltweit mehr als 245.000 Zertifizierungsprüfungen ausgeliefert und über 80.000 Zertifikate ausgestellt. Die Diskrepanz ergibt sich einerseits aus dem Umstand, dass für die meisten Zertifikate zwei Prüfungen notwendig sind, und andererseits daraus, dass nicht jeder Kandidat tatsächlich jede Prüfung besteht. Genauere Daten (etwa darüber, wie die 80.000 Zertifikate sich auf die Ebenen LPIC-1, LPIC-2 und LPIC-3 verteilen) veröffentlicht das LPI nicht.\n\nLPIC-Zertifikate sind prinzipiell unbegrenzt gültig; allerdings legt das LPI Zertifikatsinhabern „dringend nahe“, sich spätestens alle 5 Jahre zu rezertifizieren (und unterfüttert das damit, den Status von existierenden Zertifikaten 5 Jahre nach dem Erwerb auf \"inaktiv\" zu setzen). Ursprünglich sollte gar keine Rezertifizierung verlangt werden; der Sinneswandel des LPI beruhte darauf, dass es eine Akkreditierung bei der \"National Organization for Competency Assurance\" (NOCA), einer Qualitätssicherungs-Organisation für Zertifikatsanbieter, erreichen wollte. Zu den Voraussetzungen einer solchen Akkreditierung gehört ein Rezertifizierungs-Mechanismus. Für eine Weile galt eine Rezertifizierungsfrist von 10 Jahren. Eine Rezertifizierung erreicht man entweder durch das erneute Ablegen der für das Zertifikat ursprünglich erforderlichen Prüfungen oder durch das Ablegen der Prüfungen für ein höherwertiges LPIC-Zertifikat, in welchem Fall die 5 Jahre auch für die niedrigeren Zertifikate neu gestartet werden. Über eine spezielle geraffte Rezertifizierungsprüfung wurde beim LPI nachgedacht, aber aktuell ist das kein Thema mehr. Das heißt konkret, jemand, der am 2. September 2004 die Prüfung LPI101 abgelegt hat und am 1. März 2005 LPI102, bekommt ein LPIC-1-Zertifikat, das bis zum 1. März 2010 rezertifiziert werden sollte. Dazu kann er entweder die LPI101- und -102-Prüfungen neu ablegen oder aber LPIC-2 in Angriff nehmen; bekommt er etwa am 1. April 2008 ein LPIC-2-Zertifikat ausgestellt, dann bleiben sowohl sein LPIC-1- als auch sein LPIC-2-Zertifikat bis zum 31. März 2013 aktiv. Das LPI verspricht, die Inhalte der Prüfungsziele der einzelnen Prüfungen in einem 2,5-Jahres-Turnus zu überarbeiten, und empfiehlt eine Rezertifizierung im selben Abstand.\n\nEtwas komplizierter ist die Rezertifizierung auf der LPIC-3-Ebene. Hier gilt, dass das eigentliche LPIC-3-Zertifikat (das man durch Bestehen der Prüfung LPI-301 erwirbt) entweder durch erneutes Ablegen von LPI-301 oder durch das Ablegen einer beliebigen Spezialisierungsprüfung (LPI-302 bis LPI-306) rezertifiziert wird. Spezialisierungsprüfungen kann man derzeit aber nur rezertifizieren, indem man die betreffende Prüfung wiederholt.\n\nDie größten Sponsoren sind IBM, Bradford Learning, Linux-Magazin, Linuxcare, Maxspeed, SGI, Turbolinux, Novell, Hewlett-Packard, Linux Journal und Wave Technologies.\n\nDie Reihenfolge der beiden Prüfungen (x01 und x02, siehe unten) ist beliebig. Die Prüfungen werden an einem PC in einem – entweder von einer Person oder einer Kamera – überwachten Raum vorgenommen. Es existiert eine kleine Anzahl unterschiedlicher Fragenkataloge, die jeweils aus einem großen Vorrat von Fragen so ausgewählt wurden, dass gewisse psychometrische Bedingungen erfüllt sind (sie werden auch hin und wieder neu zusammengestellt, um den Nutzen veröffentlichter Gedächtnisprotokolle (\"brain dumps\") zu reduzieren). Der Prüfling bekommt einen dieser Fragenkataloge vorgelegt, wobei die Reihenfolge der Fragen beliebig sein kann. Es ist gestattet, Fragen zu überspringen oder bereits gegebene Antworten innerhalb der Prüfungszeitspanne zu korrigieren. Das Bestehen oder Nichtbestehen wird nebst der erreichten Punktzahl unmittelbar nach Beendigung der Prüfung auf dem Bildschirm angezeigt. Das Prüfungszentrum gibt nach Abschluss der Prüfung einen Ausdruck des Prüfungszeugnisses über Gesamtergebnis und Abschneiden in den jeweiligen Themenbereichen aus.\n\nAlternativ hierzu werden auf Messen und anderen Veranstaltungen Prüfungen angeboten, bei denen die zu prüfende Person einen ausgedruckten Fragenkatalog erhält, auf dem die eigenen Antworten kenntlich zu machen sind. Diese Antwortbögen werden vom LPI zentral ausgewertet. Bis zum Erhalt der Prüfungsergebnisse kann es hierbei mehrere Wochen dauern.\n\nDas Zertifikat (inklusive einer kreditkartengroßen Zertifikatskarte) wird nach Erreichen der Mindestpunktzahl bei beiden Prüfungen durch den LPI-Hauptsitz in Cobourg, Ontario, Kanada verschickt.\n\nIm Falle des Nichtbestehens der Prüfung kann diese nach frühestens einer Woche wiederholt werden. Ab dem zweiten Nichterreichen der Mindestpunktzahl ist 30 Tage auf einen Wiederantritt zu warten.\n\nSeit dem gibt es für die LPIC-1 Prüfungen neue Inhalte in der Version 4.0. Der bisherige Stoff wurde um weitere Inhalte ergänzt und Bestehendes aktualisiert. Am 29. Oktober 2018 wurde die Version 5.0 eingeführt. Dabei wurde neu das Thema Virtualisierung eingeführt sowie veraltete Inhalte aktualisiert oder gestrichen. Es gibt eine Übergangsfrist für Prüflinge die weiterhin an Prüfungen der vorherigen Version teilnehmen wollen.\n\nDie nächsten Änderungen werden LPIC-2 und LPIC-3 betreffen. Die neuen Fragen sind für die LPIC-2 Prüfungen ab dem sowie für LPIC-3 ab dem gültig.\n\nSeit Juni 2012 wird vom Linux Professional Institute ein zusätzliches Zertifizierungsprogramm „Linux Essentials“ angeboten, welches sich primär an Schulen und Jugendliche richtet und als Einstieg in die Open-Source-Welt dienen soll. Die Prüfung umfasst Fragen zur Linux- und Open-Source-Community, gängige Distributionen, wichtige Open-Source-Anwendungen und deren Nutzung, Lizenzen, Arbeiten mit Desktop und Kommandozeile, einfache Scripts in Scriptsprachen wie Bash, Perl und Python sowie Fragen zur Sicherheit und zu Dateiberechtigungen. Die detaillierten aktuellen Lernziele sind im Wiki der Organisation zu finden.\n\nVom Linux Professional Institute wird ebenfalls ein Zusatzzertifikat für DevOps angeboten. Inhalt sind Containerverwaltung, Kontinuierliche Integration und Versionsverwaltung. Es gibt nur eine Prüfung.\n\nAnzahl der Fragen: 60\nDauer: 90 Minuten\n\nSoftwareentwicklung\n\nContainerverwaltung\n\nBetreiben von Software\n\nUm dieses Zertifikat zu erhalten, müssen die Prüfungen 101 und 102 bestanden werden. Diese Prüfungen gibt es auf Englisch, Japanisch und Deutsch. Man hat für die Prüfungen jeweils 90 Minuten Zeit.\n\nFrüher konnte man zwischen zwei Varianten der Prüfung 101 wählen, die entweder Fragen zum Paketmanagement mit rpm (Red Hat, SUSE, Mandriva, …) oder aber mit dpkg (Debian, Ubuntu, Xandros, …) enthielten. Seit 2006 ist das aber nicht mehr möglich; es werden beide Bereiche geprüft.\n\nAnzahl der Fragen: 60\nDauer: 90 min\nMindestpunktzahl: 500 (von insgesamt 800)\n\nSystemarchitektur\nLinux Installation und Paket-Management\nGNU- und Unix-Befehle\n\nLaufwerke, Dateisysteme, Verzeichnisstruktur und Filesystem Hierarchy Standard (FHS)\n\nAnzahl der Fragen: 60\nDauer: 90 min\nMindestpunktzahl: 500 (von insgesamt 800)\n\nShell, Skripten und SQL\nBenutzerinterface und Desktop\nAdministrative Aufgaben\nWichtige Systemdienste\nNetzwerkgrundlagen\nSicherheit\n\nUm dieses Zertifikat zu erhalten, müssen die Prüfungen 201 und 202 bestanden werden. Diese Prüfungen gibt es auf Englisch, Japanisch und Deutsch.\n\nMan hat für die Prüfungen jeweils 90 Minuten Zeit. Wenn Pilotfragen gestellt werden, die nicht mit in die Bewertung der Prüfung einfließen, stehen 120 Minuten Zeit zur Verfügung.\n\nPrinzipiell ist es möglich, Prüfungen auf LPIC-2-Niveau abzulegen, \"bevor\" man das LPIC-1-Zertifikat erworben hat. Allerdings bekommt man das LPIC-2-Zertifikat selbst erst, wenn man über ein aktives LPIC-1-Zertifikat verfügt.\n\nAnzahl der Fragen: 60\nMindestpunktzahl: 500 (von insgesamt 800)\nThemen:\n\nAnzahl der Fragen: 60\nMindestpunktzahl: 500 (von insgesamt 800)\nThemen:\n\nSeit Oktober 2013 ist die Zertifizierung nicht mehr in eine Core- und eine Spezialisierungsprüfung aufgeteilt, sondern das Zertifikat wird durch Bestehen einer Prüfung erreicht. Dazu wurde eine neue Prüfung 300 (\"Mixed Environments\") kreiert, die sowohl 301 als auch 302 ersetzt. Die anderen Spezialisierungen (z. Zt. 303 und 304) führen alleine zum LPIC-3 Zertifikat.\n\nDie Prüfung LPI-300 befasst sich vor allem mit Samba und der Integration von Linux- und Windows-Systemen. Ersetzt seit Oktober 2013 die Prüfungen 301 und 302\n\nDie Prüfung LPI-303 umfasst die Themen Kryptografie, Zugriffskontrolle (ACLs, SELinux, andere MAC-Systeme), Anwendungssicherheit (DNS, Apache, FTP, OpenSSH, …), Betriebs- und Netzwerksicherheit. Das Bestehen dieser Prüfung führt zur Spezialisierung „LPI-303: Security“.\n\nDie Prüfung LPI-304 adressiert die Themen Virtualisierung (Xen, KVM und andere), Lastverteilung und redundanter Speicher, Clusterverwaltung und Nachrichteninfrastrukturen für Hochverfügbarkeit. Das Bestehen dieser Prüfung führt zur Spezialisierung „LPI-304: Hochverfügbarkeit und Virtualisierung“.\n\nDiese Prüfung befindet sich in Vorbereitung. Mit der Arbeit an LPI-305 wurde 2010 begonnen.\n\nDiese Prüfung befindet sich in Planung.\n\nDie Prüfung LPI-301 beschäftigt sich mit dem Verzeichnisdienst LDAP, Authentisierung, Fehlersuche, Netzwerkintegration und Kapazitätsplanung. Im Gegensatz zu den anderen LPI-Prüfungen umfasst die Prüfung LPI-301 nur 50 Fragen statt 60; die Dauer ist trotzdem 90 Minuten.\n\nDie Prüfung LPI-302 befasst sich vor allem mit Samba und der Integration von Linux- und Windows-Systemen.\n\n\n"}
{"id": "273659", "url": "https://de.wikipedia.org/wiki?curid=273659", "title": "Festkommazahl", "text": "Festkommazahl\n\nEine Festkommazahl ist eine Zahl, die aus einer festen Anzahl von Ziffern besteht. Die Position des Kommas ist dabei fest vorgegeben, daher der Name.\nDer Grundgedanke dahinter ist die informationstechnische Repräsentation eines Ausschnitts der rationalen Zahlen. Diese Abbildung auf einen begrenzt großen Datentypen, typischerweise Integer verschiedener Bitbreiten, erfordert eine feste Anzahl von Ziffern für den Vorkomma- wie den Nachkommaanteil. Üblicherweise sind gemäß Definition die ersten formula_1 Stellen Vorkommastellen und die restlichen formula_2 Nachkommastellen. In der Informatik haben die dezimalen wie die binären Festkommazahlen eine große praktische Bedeutung.\n\nDurch die feste Position des Dezimalkommas fällt Rechenaufwand im Vergleich zur Rechnung mit Gleitkommazahlen weg. Wird in einem Computerprogramm eine binäre Festkommadarstellung gewählt, können zudem die zur Umrechnung und Korrektur notwendigen Multiplikationen und Divisionen durch schnelle Schiebeoperationen ersetzt werden. Ein Beispiel für eine Applikation, die gezielt aus Rechenaufwandsgründen mit Festkomma-Arithmetik entworfen wurde, ist z. B. Fractint, ein Fraktalgenerator. Seit Mitte der 1990er Jahre haben Prozessoren dedizierte Gleitkommarechenwerke, die dazu führen, dass die Berechnung von Fraktalen mit Gleitkommazahlen schneller als die mit Ganzzahlen ist.\n\nAufgrund der exakten Darstellung ist der Wertebereich einer Festkommazahl kleiner als der jeweilige Wertebereich einer Gleitkommazahl der gleichen (Bit-)Länge. Dafür ist jedoch die Exaktheit der Darstellung einer Zahl im gesamten Wertebereich gesichert, bei Gleitkommazahlen nicht immer (z. B. durch Absorption). Ein Beispiel für eine Anwendung, die genaue Arithmetik benötigt und daher Festkommazahlen verwendet, ist GnuCash.\n\nAlle binären Festkommazahlen der Länge formula_3 mit formula_4 Vorkommastellen:\n\nMan beachte, dass jedes der vier aufgelisteten binären Muster für jeweils drei unterschiedliche Zahlen steht, je nachdem an welche Stelle das Komma gesetzt wird. Da die Anzahl der Vorkommastellen bereits per Definition fest liegt, ist es unnötig, das sonst übliche Komma zu schreiben beziehungsweise zu speichern, d. h. die Repräsentation ist immer die der Spalte „Binärmuster“.\n\nBei der Rechnung mit Festkommazahlen werden die binären Muster prinzipiell so verarbeitet wie bei der Rechnung mit ganzen Zahlen. Festkomma-Arithmetik kann daher von jedem digitalen Prozessor durchgeführt werden, der arithmetische Operationen mit ganzen Zahlen unterstützt. Dennoch sind einige Regeln zu beachten, die sich auf die Position des Kommas vor und nach der Rechenoperation beziehen:\n\n\nDie folgenden Beispiele gehen von einer dezimalen Festkommadarstellung aus, bei der zwei Nachkommastellen vorgesehen sind. Dies wird durch den Faktor 100 ausgedrückt, mit denen die ursprünglichen Werte multipliziert wurden, um die Festkommadarstellung zu erhalten.\n\nIn der Festkommadarstellung:\nDezimal:\nBinär:\n\nBerechnung in der 8-Bit-Festkommadarstellung mit 4 Nachkommastellen:\n\nMultiplikation:\n\nErgebnis in der Festkommadarstellung:\n\nmit der Bedeutung\n\nFehlerhafte Darstellung der Kommaposition, daher ist die Schiebeoperation notwendig:\nErgebnis wäre hier entsprechend der 8-Bit-Festkommadarstellung mit 4 Nachkommastellen:\nDas Ergebnis nach der Korrektur entspricht nun dem erwarteten Ergebnis.\n\nBei der Darstellung einer reellen Zahl formula_26 kann es einige Probleme geben. Im Folgenden hat die Festkommazahl (angelehnt an die Darstellung in einem Rechner) eine Länge von formula_27 und formula_28 Vor- und Nachkommastellen. Der Ziffernvorrat sei formula_29 – also eine binäre Festkommazahl der Länge eines Bytes mit gleich vielen Vor- und Nachkommastellen. Der tiefgestellte Index bezeichnet die Darstellung der Zahl: formula_30 für eine reelle Zahl in üblicher Dezimaldarstellung und formula_31 für eine derartige Festkommazahl.\n\n\nWie man sieht, können also mit acht Bits und vier Vor- und Nachkommastellen nur Festkommazahlen zwischen formula_42 und formula_43 (bei einer Auflösung von formula_44) dargestellt werden. Dieser geringe Darstellungsbereich ist auch der entscheidende Nachteil gegenüber Gleitkommazahlen.\n\nWeiterhin entstehen wie auch bei Gleitkommazahlen Rundungsfehler bei der Umwandlung der dezimalen, reellen Zahlen in eine binäre Festkommadarstellung. formula_45 kann im Gegensatz zu formula_46 exakt dargestellt werden. formula_47 kann allerdings bei noch so vielen Nachkommastellen nicht als Summe von Zweierpotenzen dargestellt werden. Um diese Probleme zu umgehen, kann aber bei Bedarf eine dezimale Festkommadarstellung eingesetzt werden.\n\nDas Zahlenformat für Festkommazahlen mit einer Nachkommastelle wird auch als „Q1“ bezeichnet, mit zwei Nachkommastellen „Q2“ usw. Bei einer Wortlänge von mehreren Bits kann das Format noch genauer mit der Angabe der Vorkommastellen angegeben werden. Hierbei werden die Vor- und Nachkommastellen durch einen Punkt getrennt. Eine Festkommazahl mit acht Bits kann beispielsweise als „Q7.1“ bzw. „Q6.2“ bezeichnet werden.\n\nInsbesondere bei Geldbeträgen ist die Nutzung von dezimalen Festkommazahlen sinnvoll. Wird ein bestimmter Rechnungsbetrag beispielsweise zunächst mit einer Anzahlung und später mit einer Restzahlung beglichen, kann es bei Verwendung von Gleitkommazahlen vorkommen, dass aufgrund von Rundungsfehlern ein Restbetrag oder Guthaben von weit unter der kleinsten Währungseinheit übrig bleibt. Entsprechende Aufträge in einer Datenbank würde man später bei einer Suche nach nicht vollständig bezahlten Aufträgen (also mit Restbetrag ungleich 0) finden, da der Restbetrag sehr klein, aber eben nicht 0 ist.\n\nIn den meisten Programmiersprachen und Datenbanken gibt es darum einen Datentyp, der englisch \"Currency\" (dt.: Währung), \"Money\" (Geld) oder \"Decimal\" (Dezimalzahl) genannt wird. Ihm liegt eine Ganzzahl mit Vorzeichen (\"Signed Integer\") zugrunde, meist von 64 Bit Länge. Zuweisung, Addition und Subtraktion zweier solcher Datentypen untereinander erfolgen wie bei einem Integer, ebenso Multiplikation von Festkommazahl und Integer sowie abrundende Division einer dezimalen Festkommazahl durch ein Integer. Bei anderen Operationen, die zudem unter Umständen verlustbehaftet sind, muss der Computer die Anzahl der Nachkommastellen beachten. Diese ist in manchen Sprachen frei definierbar. Falls dies nicht der Fall ist, hat der Datentyp üblicherweise vier Nachkommastellen und könnte informell als Zehntausendstel bezeichnet werden. Derzeit auf der Welt vorhandene Währungsuntereinheiten neben den weitverbreiteten Hundersteln sind Fünftel (Mauretanien und Madagaskar) und in der MENA-Region Tausendstel (vgl. Dezimalwährung), die sich somit darstellen lassen. Der Wertebereich einer 64-Bit-Zahl mit vier Nachkommastellen ist −922.337.203.685.477,5808 bis 922.337.203.685.477,5807. Das ist rund das Zehnfache der Weltwirtschaftsleistung 2017 in Dollar. Eine 32-Bit-Zahl mit vier Nachkommastellen, in T-SQL beispielsweise \"smallmoney\" (wörtlich: kleines Geld) genannt, erlaubt Zahlen von −214.748,3648 bis 214.748,3647.\n\n"}
{"id": "274149", "url": "https://de.wikipedia.org/wiki?curid=274149", "title": "QuickTime Streaming Server", "text": "QuickTime Streaming Server\n\nDer QuickTime Streaming Server ist eine von Apple entwickelte Serversoftware, die in Echtzeit MPEG-4-, 3gp- und QuickTime-kompatible Live-Streams über das Internet bereitstellen und streamen kann.\n\nProgramme oder Geräte, wie der QuickTime Player, der VLC media player, die PlayStation Portable oder moderne Mobiltelefone, können diese Audio- oder Video-Streams live wiedergegeben.\n\nWährend der QuickTime Streaming Server nur für Mac OS X (Server) zur Verfügung steht, veröffentlicht Apple den Source Code kostenlos unter dem Namen Darwin Streaming Server, der identisch mit dem QuickTime Streaming Server ist.\nWeitere Unterschiede sind die besseren Administrator- und Medienverwaltungsfunktionen im QuickTime Streaming Server.\n\n\n"}
{"id": "276162", "url": "https://de.wikipedia.org/wiki?curid=276162", "title": "Apple Display Connector", "text": "Apple Display Connector\n\nDer Apple Display Connector (ADC) ist eine Videoschnittstelle für Monitore des Herstellers Apple.\n\nBei dieser Eigenentwicklung von Apple werden drei Kanäle (USB, DVI und Strom) über dasselbe Kabel übertragen, wodurch Kabelsalat vermieden werden soll. Auch ist es damit möglich, den Mac per Knopfdruck am Monitor einzuschalten. Der ADC wurde von Apple von 2000 bis 2005 verwendet.\n\nDer ADC-Anschluss war der Standardbildschirmanschluss am PowerMac bis zur Vorstellung der neuen Apple-Cinema-Display-Reihe auf der WWDC am 28. Juni 2004 durch den damaligen CEO Apples, Steve Jobs. Hier wurde der ADC-Anschluss durch eine sogenannte Kabelpeitsche ersetzt, bei der weiterhin ein Kabel zum Monitor führte, aber am Computer die jeweiligen Anschlüsse für Strom, Monitor, USB mit drei einzelnen Steckern angeschlossen werden.\n\nDiese Kabelpeitschen-Lösung wurde erst 2011 mit dem neuen Thunderbolt-Anschluss wieder durch ein einziges Kabel abgelöst, das wie USB zusammen mit Intel entwickelt wurde und herstellerübergreifend verfügbar ist.\n\nFür ältere, noch im Einsatz befindliche Apple-Rechner gibt es Adapter auf DVI, um Standardmonitore an den Mac anzuschließen.\n"}
{"id": "277967", "url": "https://de.wikipedia.org/wiki?curid=277967", "title": "Grafische Datenverarbeitung", "text": "Grafische Datenverarbeitung\n\nDie Grafische Datenverarbeitung (GDV) ist ein Teilbereich der Informatik. Sie umfasst eine Technologie, mit der Bilder mit Hilfe von Rechnern erfasst bzw. erzeugt, verwaltet und in gewünschter Weise verändert werden können und mit weiteren nicht-grafischen Daten in Verbindung gebracht werden können. Dazu gehören auch die computergestützte Integration und Verknüpfung dieser Bilder mit anderen Kommunikationsmedien, wie Audio, Sprache und Video, zur Realisierung komplexer multisensorischer Dialogtechniken.\nTeilgebiete sind unter anderem die Computergrafik und Visualisierung, die Geometrische Modellierung, die Bildverarbeitung sowie Teile der Mustererkennung und der Szenenanalyse.\n\n\n"}
{"id": "277981", "url": "https://de.wikipedia.org/wiki?curid=277981", "title": "Colour Genie", "text": "Colour Genie\n\nDas Colour Genie (EG2000) war ein 8-Bit-Computer, der im August 1982 auf den deutschen Markt kam. Das Gerät erreichte trotz anfänglichen Erfolgen nicht die angestrebten Verkaufszahlen und spielte insbesondere nach dem Erfolg des später vorgestellten C64 keine große Rolle mehr. Außerdem gab es eine Negativ-Bewertung in der Zeitschrift \"test\" der Stiftung Warentest. Hergestellt wurden die Genies von EACA in Hongkong. Der Vertrieb in Deutschland erfolgte über die Firma \"Trommeschläger Computer Studio\" (TCS) und den Computerfachhandel z. B. über die Firma Schmidtke Electronic in Aachen. Nach dem Konkurs von EACA und somit dem Wegfall eines der Hauptlieferanten musste auch TCS im August 1985 Konkurs anmelden.\n\nDie Programmierung erfolgte entweder über einen Assembler-Compiler oder über das im ROM hinterlegte \"Colour Basic\". Nach dem Anschluss eines Diskettenlaufwerks erweiterte sich über EPROMs auf der Cartridge, die über ein Flachbandkabel mit dem Diskettenkontroller verbunden war, der BASIC-Befehlssatz, vor allem um Befehle zur Verarbeitung von Zeichenketten. Außerdem kamen die für den Diskettenbetrieb benötigten \"CMD\"-Befehle hinzu. Auf der Platine der Cartridge war auch Platz für ein drittes (EP)ROM, das dann z. B. einen Maschinensprachemonitor oder ein Hardcopy-Programm enthalten konnte.\n\nDie Benutzung des Kassettenrecorders als Speichermedium stellte eine große Herausforderung für die Geduld dar. Es dauerte zum Beispiel gut 20 Minuten, eines der größten Programme, das Spiel \"Deathtrap\", zu laden, ein 3D-Adventure mit Zeichensatz-Grafik. Die Kassetten, auf denen die Spiele verkauft wurden, waren oft mit einem Zusatzbit am Ende als Kopierschutz versehen, der Kopierversuche erschweren sollte. Es wurde sogar extra ein Programm vertrieben, welches Kassettenprogramme mit Kopierschutz versah (und mit dem die Kassetten dennoch – wie auch mit jedem guten HIFI-Kassettendeck mit zwei Laufwerken – kopiert werden konnten).\n\nDie Basicprogrammierung erfolgte nicht wie heute üblich mit einem Texteditor, sondern mit einem Zeileneditor.\n\nEs gab einen Grafik- und einen Textmodus für die Anzeige, zwischen denen im Programm bzw. über die <MOD SEL> Taste umgeschaltet werden konnte. Da die beiden Videospeicherbereiche an unterschiedlichen Adressen lagen war es so möglich während einer Textdarstellung im Hintergrund schon die Grafik aufzubauen. Eine Textdarstellung war auch im Grafikmodus möglich, aber die geringe Auflösung dieses Modus machten die Schriften extrem groß. Viele Spiele nutzten daher nicht den Grafikmodus, sondern bauten die Grafiken aus Sonderzeichen auf, welche sich frei definieren ließen. Ein einfacher Flugsimulator (eine Version eines TRS-80 Programms) benutzte so im Textmodus die programmierbaren Sonderzeichen, um die einfache, ruckelnde Vektorlandschaft und die Instrumente darzustellen. Durch entsprechende Programmierung der CRT-Register konnte die Grafik- und Textauflösung in begrenzten Bereichen verändert werden, Anwendung fand dies z. B. in den Spielen Chopper und Trashman.\n\nEs wurde auch eine „Grafikkarte“ als Erweiterung angeboten, die allerdings nur die Zeichenanzahl im Textmodus veränderten.\n\nDer Joystick EG2013 war eine Besonderheit, da er nicht wie bei Atari, Amiga und C64 üblich digital, sondern analog abgefragt wurde. So lieferte der Joystick je nach Stellung Werte zwischen 0 und 255 für die X- und Y-Achse (128 in Mittelstellung). So ließ sich eine Steuerung realisieren, welche die Weite der Joystickbewegung berücksichtigte.\nZweigte man die Steuerleitungen der Tastatur zu einer 9-poligen Buchse ab, konnte auch ein Atari-kompatibler Joystick verwendet werden. Alle Spiele, die die Pfeiltasten und die Leertaste verwendeten, konnten so auch mit einem Joystick gespielt werden.\n\nDer Soundchip ermöglichte auch Sprachausgaben, wie z. B. im Spiel Crazy Paint II. Hier hat der Programmierer zwei kurze Samples seiner eigenen Stimme verwendet.\n\nIn Deutschland wurde das Handbuch „COLOUR BASIC - leicht gelernt“ mitgeliefert und es gab in Hamburg einen User-Club mit eigenem Clubmagazin „BYTE“, das allerdings nach 10 Ausgaben eingestellt wurde.\n\nAb April 1983 wurde das Colour Genie mit den sog. „Neuen Roms“ ausgeliefert. Äußerlich sind diese Modelle an dem eingebauten Pegelmesser für den Kassettenrekorderanschluss erkennbar. Die Auflösung im Textmodus wurde auf 40×25 Zeichen, die Grafikauflösung auf 160×102 Punkte erhöht. Außerdem wurden neue Basic Befehle hinzugefügt und Fehler in Basic-Befehlen korrigiert. So war bei den ersten Modellen der Algorithmus des Basic-Befehls zum Füllen von Bildschirmbereichen bis zu einer Grenzfarbe fehlerhaft implementiert: die Farbe „lief aus“. Man konnte die – in einer Steckfassung untergebrachten – Halbleiterbausteine im DIL-Gehäuse mit dem Basic-Interpreter an den deutschen Distributor einschicken und bekam ca. 2 Wochen später eine korrigierte Version zurück. Offensichtlich war aber der Speicherplatz nicht ausreichend, sodass die Befehlsoption zum „schlagartigen“ Farbwechsel des Bildschirmhintergrunds im Grafik-Modus entfallen war und durch eine wesentlich langsamere Methode (Zeichnen einer „Box“ in der gewünschten Farbe über den ganzen Bildschirm) ersetzt werden musste. Eine ausführliche Beschreibung der Veränderungen die die ROMs mit sich bringen, befindet sich im Anhang des Handbuches „COLOUR BASIC - leicht gelernt“.\n\n\nVon verschiedenen Drittanbietern (meist einzelne Programmierer) und der Firma TCS selbst wurden viele verschiedene Programme für das Colour Genie angeboten. Vor allem bei Spielen und Lernprogrammen gab es eine reichliche Auswahl, aber auch Textverarbeitung, Malprogramme oder Datenbankprogramme waren vorhanden.\n\n\nAnmerkungen\n\n\n\n"}
{"id": "278406", "url": "https://de.wikipedia.org/wiki?curid=278406", "title": "Ettercap", "text": "Ettercap\n\nEttercap ist eine freie Software zum Durchführen von Man-in-the-Middle-Angriffen. Es unterstützt Sniffing auf IP- wie auch auf ARP-Basis, Echtzeitkontrolle über Verbindungen selbst in geswitchten Netzwerken, inhaltbezogenes Filtering und aktive wie auch passive Analysen von einzelnen Hosts und ganzen Netzwerken.\n\nVerfügbar ist Ettercap für Linux, die BSD-Derivate, Solaris, macOS wie auch Windows.\n\nAls Benutzerschnittstelle stehen neben der einfachen Konsolenbedienung das NCurses-Frontend und auch ein GTK2-GUI zur Verfügung. Für spezialisierte, zeitintensive und benutzerinteraktionsunabhängige Angriffe besteht ebenfalls die Möglichkeit, Ettercap im sogenannten Daemonmode zu starten, welcher das Programm im Hintergrund laufen lässt und die gewünschten Aktivitäten, angegeben durch Shellparameter, ausführt. Dies ist beispielsweise automatisches Protokollieren (\"logging\") von Benutzernamen und Passwörtern des gesamten Netzwerkes in eine zentrale Datei (unterstützt unter anderem HTTP, HTTPS, ICQ, POP3, IMAP, SMB, Q3A, Oracle, MySQL, SMTP).\n\nObwohl es sich offiziell um ein Sicherheits-Programm handelt, wird es im Gegensatz zu manch anderen, ähnlichen Programmen, von vielen Unternehmen als äußerst gefährlich eingestuft. Die Websense Inc. beispielsweise sperrt den Zugang zu der Homepage von Ettercap permanent.\n\nEttercap lässt sich auch sehr gut in Verbindung mit anderen Sniffern, wie z. B. Wireshark, nutzen: Ettercap leitet den Netzwerkverkehr (z. B. mit ARP-Spoofing) auf die Schnittstelle des Angreifers, der ihn mit Wireshark sniffen und weiterleiten kann.\n\n"}
{"id": "279542", "url": "https://de.wikipedia.org/wiki?curid=279542", "title": "Tar (Packprogramm)", "text": "Tar (Packprogramm)\n\ntar ist ein im Unix-Umfeld sehr geläufiges Packprogramm. Außerdem wird so auch das Dateiformat bezeichnet, das von diesem Programm verwendet wird.\n\nDer Name wurde aus tape archiver (Bandarchivierer) gebildet, da mit dem Programm ursprünglich Daten auf Bandlaufwerken gesichert wurden. Gleichzeitig ist \"tar\" auch das englische Wort für Teer (mit dem Programm werden Dateien unkomprimiert zu einer Datei „zusammengeklebt“).\n\nTar bietet die Möglichkeit, Dateien, Verzeichnisse und andere Objekte eines Dateisystems sequenziell in eine einzige Datei zu schreiben bzw. aus selbiger wiederherzustellen. Die entstehende Datei trägt die Endung codice_1 und wird im Englischen auch als \"Tarball\" (dt. \"Teerklumpen\" oder \"Teerkugel\") bezeichnet.\n\nDer MIME-Typ für tar-Dateien ist \"application/x-tar\".\n\nDer wahlfreie Zugriff auf einzelne Dateien ist bei tar nicht möglich, da die Archivdateien kein Verzeichnis haben, das die Datei-Offsets zum schnellen Zugriff vorhält, wie es etwa bei Zip der Fall ist (dies bedeutet nicht, dass nicht auch einzelne Dateien aus einem Archiv entpackt werden können). Der Verzicht auf diese zusätzliche Struktur ermöglicht aber auch das einfache Vergrößern von Archiven und vor allem auch das Extrahieren von Dateien aus unvollständigen oder defekten Archiven.\n\nHeute werden tar-Archive häufiger in tar-Dateien gefunden als auf Bändern. Diese Archiv-Dateien sind meist komprimiert, um ihre Größe zu reduzieren. Dazu kommen üblicherweise Unix-typische Packprogramme wie compress, gzip, bzip2, xz oder lzma zum Einsatz. Der Ansatz, erst alle Dateien unkomprimiert aneinanderzuhängen, um sie dann zu komprimieren, wird als solide Kompression bezeichnet und mittlerweile auch bei anderen Archivformaten wie etwa RAR oder 7-Zip genutzt. In Abhängigkeit vom verwendeten Kompressionsprogramm lauten die Dateiendungen eines Tarballs üblicherweise \".tar.Z\" (compress), \".tar.gz\" oder kurz \".tgz\" (gzip), \".tar.bz2\" oder \".tbz2\" oder \".tbz\" (bzip2) bzw. \".tar.xz\" oder \".txz\" (xz), oder \"tar.lzma\" (lzma).\n\nWenn keine solide Kompression gewünscht ist, können auch die einzelnen Dateien zunächst komprimiert und anschließend in den Tarball eingegliedert werden. Dadurch ist es weiterhin möglich, unvollständige Einzelteile eines Tar-Archivs zu entpacken, falls ein Algorithmus für solide Kompression gewählt wurde, der ein Wiederaufsetzen nach einem Defekten Block nicht beherrscht. Die Nachteile (begrenzte Dateigröße durch den notwendigen temporären Platz für die Kompression einzelner Dateien, bzw. völliges Versagen bei sich während der Archivierung ändernder Dateien) überwiegen jedoch, sodass dieser Ansatz in der Regel nicht gewählt wird. Zudem ist die Kompressionsrate normalerweise geringer als bei solider Kompression, welche auch die Attribute der Datei in die Kompression einbezieht. Hinzu kommt, dass sich nur ein geringer Geschwindigkeitsvorteil beim Entpacken einzelner Dateien ergibt, da das Archiv hierzu ohnehin sequentiell durchsucht werden muss.\n\nDas \"tar\"-Format erschien 1979 in einem Update für UNIX Version 7, \"ustar\" und \"pax\" sind im POSIX-Standard spezifiziert. Das unter Linux gebräuchliche \"GNU tar\" entspricht nicht ganz dem POSIX-Standard. Insbesondere die oft fehlende Fähigkeit, Zugriffskontrolllisten zu speichern, machen \"tar\" und \"GNU tar\" für manche Nutzer zu nur eingeschränkt brauchbaren Datensicherungsprogrammen. Die bei manchen Implementierungen unzureichende Unterstützung von Sparse-Dateien kann zudem beim Wiedereinspielen eines Archivs zu Problemen führen. \"star\" oder \"bsdtar\" versuchen diese Nachteile zu vermeiden.\n\nEin weiterer, systembedingter Nachteil liegt in der Art und Weise der Kompression. Solide Kompression bedingt nämlich, dass der Verlust eines einzigen Blocks den Verlust des gesamten restlichen Bandarchivs zur Folge haben kann, falls das Kompressionsprogramm nach diesem Punkt nicht mehr synchronisieren kann. Auf diesem Gebiet gibt es bisher Versuche wie \"afio\", das dateiweise komprimiert, aber auf einer privaten Variante des durch POSIX inzwischen als veraltet deklariertem cpio-Formats aufsetzt, und bestimmte blockweise komprimierende Algorithmen, zu denen bis zu einem gewissen Grad \"bzip2\" bereits zählt.\n\nEin Unix-Kommando, welches in seinen Funktionen \"tar\" sehr ähnelt, ist \"cpio\". Der POSIX-Standard \"pax\" gibt vor, die Kommandos \"tar\" und \"cpio\" zu vereinen und ist ein Ergebnis der sogenannten Tar-Wars, die um das Jahr 1992 geführt wurden. Das beliebteste Programm zum Archivieren von Dateien unter Unix ist unabhängig von dieser Standardisierung weiterhin tar.\n\nAnders als \"jar\"-Archive enthält ein \"tar\"-Archiv wie \"cpio\"- und \"zip\"-Archive keine Information über den Zeichensatz der Dateinamen. In der Regel wird in den Dateisystemen wie bei \"jar\" UTF-8 verwendet.\n\nArchive mit Inhalt von /etc und /home erstellen:\n\ntar cvf test.tar /etc/ /home/ # Erstellt ein neues Archiv, der Inhalt besteht aus den Verzeichnissen /etc und /home\ntar cvf - /etc /home | gzip > test.tar.gz # Dasselbe, aber mittels einer Pipe werden die Daten umgehend in eine komprimierte gzip-Datei umgeleitet\ntar czvf test.tar.gz /etc/ /home/ # *GNU tar* Kurzform, dasselbe, aber ohne Pipe\ntar -czvf test.tar.gz /etc/ /home/ # *GNU tar* Alternative: Das führende Minus kann weggelassen werden\ntar --create --gzip --verbose --file test.tar.gz /etc/ /home/ # auch dieser Stil ist möglich\nArchiv updaten, etwa für Backup-Zwecke:\n\ntar uvf test.tar /etc/ /home/ # u für \"Update\". Neue und geänderte Dateien werden dem Archiv hinzugefügt. Gelöschte Dateien verbleiben im Archiv.\ntar --update --verbose --file test.tar /etc/ /home/ # ausführliche Form\n\nDie Update-Option funktioniert nicht bei komprimierten Archiven.\n\nArchive entpacken:\n\ntar xvf test.tar\ngunzip < test.tar.gz | tar xvf -\ntar xzvf test.tar.gz # *GNU tar* Kurzform\ntar -xzvf test.tar.gz # *GNU tar* Alternative\ntar -xzvf test.tar.gz --no-anchored singlefile.txt # einzelnes File auspacken\nArchivinhalt ansehen:\n\ntar tvf test.tar\ngunzip < test.tar.gz | tar tf -\ntar tzvf test.tar.gz # *GNU tar* Kurzform\ntar -tzvf test.tar.gz # *GNU tar* Alternative\nDie Schreibweise der Kommandos ohne führendes Minus ist dabei die kompatible UNIX-Syntax und sollte bevorzugt verwendet werden.\n\nSeit Windows 10 1803 wird tar mitinstalliert. Unter älteren Windows-Versionen können die mit tar gepackten Dateien nicht direkt entpackt bzw. geöffnet werden. Dazu ist ein zusätzliches Programm notwendig. Archivprogramme wie 7-Zip, TUGZip oder IZArc können tar unter Windows entpacken, aber auch andere gängige Archivprogramme können tar-Archive öffnen.\n\n"}
{"id": "279667", "url": "https://de.wikipedia.org/wiki?curid=279667", "title": "Ncurses", "text": "Ncurses\n\nncurses (Abk. für \"new curses\") ist eine freie C-Programmbibliothek, um zeichenorientierte Benutzerschnittstellen (Text User Interface, TUI) unabhängig vom darstellenden Textterminal bzw. Terminalemulator darzustellen. Die Ansteuerung des Terminals wird hierbei soweit abstrahiert, dass die Programmierung unabhängig von der Art des verwendeten Terminals erfolgen kann.\n\nObwohl ncurses zum GNU-Projekt gehört, wird es nicht unter der GPL oder LGPL, sondern unter einer leicht veränderten MIT-Lizenz vertrieben.\n\nncurses begann als eine Neuimplementierung (Klon) von curses in Veröffentlichung 4.0 des System V, das seinerseits eine Weiterentwicklung der gleichnamigen BSD-Implementierung ist.\n\n\n\n"}
{"id": "279672", "url": "https://de.wikipedia.org/wiki?curid=279672", "title": "K-Meleon", "text": "K-Meleon\n\nK-Meleon ist ein auf Mozillas Gecko-Engine basierender Webbrowser für Windows.\n\nDas Ziel von K-Meleon ist es, ein möglichst schneller, maximal konfigurierbarer und benutzerfreundlicher Webbrowser zu sein. Er ist nur für Windows programmiert; eine Portierung auf andere Systeme ist nicht geplant; in Verbindung mit Wine läuft das Programm aber auch unter Linux.\n\nK-Meleon wurde unter der GNU General Public License veröffentlicht. Die erste Version (0.1) erschien im August 2000 als Versuch, winEmbed, die Mozilla Gecko-Test-Engine, in Windows zu integrieren. Am 15. Juli 2006 erschien Version 1.0 des Programms, basierend auf Mozilla 1.8.0.5 (Gecko/20060706); die lange Zeit aktuelle Version 1.5.4 basiert auf der Gecko-Engine 1.8.1.24. Danach gab es stabile K-Meleon-1.6-Betaversionen, die auf der Gecko-Engine 1.9.1.x basierten, und ebenfalls stabile K-Meleon-1.7-Alphaversionen, die auf der Gecko-Engine 1.9.2.x basierten.\n\nDie lange Zeit aktuelle Release-Version erschien somit im Jahr 2010. Die Weiterentwicklung ist seither verlangsamt, da Mozilla den MFC-Embedding-Code für die Rendering-Engine Gecko aus den Quellen entfernt hat und nicht mehr pflegt.\n\nSeit Dezember 2013 baut K-Meleon deshalb auf eine XULRunner-Laufzeitumgebung auf und am 27. September 2014 erschien Version 74.0.\n\nDer Vorteil von K-Meleon gegenüber XUL-basierenden Browsern wie z. B. Firefox ist die Verwendung der nativen Windows-Oberfläche, wodurch er wesentlich ressourcenschonender arbeitet, was sich vor allem auf langsamen Systemen bemerkbar macht.\n\nK-Meleon kann durch Skins im Aussehen verändert werden. Die Menüführung und Bedienung sind größtenteils durch Makros frei konfigurierbar. Sie werden in der K-Meleon eigenen Makrosprache in \"macros.cfg\" definiert und durch \"menus.cfg\" angezeigt.\n\nDie Bedienoberfläche ist auch in deutscher Sprache verfügbar. Bis Version 0.9 konnten die Dateien für die deutsche Sprache von der K-Meleon-Homepage heruntergeladen werden, ab Version 1.0 ist die offizielle Installationsdatei mit deutscher Sprachunterstützung auf SourceForge verfügbar. Da K-Meleon auf Mozilla Gecko aufbaut, unterstützt das Programm alle Mozilla-Plugins.\n\nWeitere Funktionen sind unter anderem ein Pop-up-Blocker, die Bedienung durch Mausgesten, ein eingebauter Feedreader, die parallele Benutzung von Internet-Explorer-Favoriten, Opera-Bookmarks und Mozilla-Lesezeichen sowie eine Möglichkeit, die Anzeige von Werbung zu unterdrücken. Das Programm kann mit Installer oder als Standalone-Version heruntergeladen werden.\n\nAls Hilfe-, Website- und Download-System nutzt K-Meleon das KMeleonWiki.\n\nK-Meleon hat eine kleine Entwicklergemeinde (zeitweise war es nur eine einzige Person in ihrer Freizeit); so wurde der Browser nicht immer so oft aktualisiert, wie es wünschenswert gewesen wäre, um allen Sicherheitsproblemen zu begegnen, die sich in der Gecko-Engine eingeschlichen hatten. Ab 1. April 2005 sind jedoch alle Mozilla-Gecko-Engine-Updates über das K-Meleon-Benutzer-Forum als K-Meleon-Update verfügbar. Um die von K-Meleon benutzte Gecko-Engine selbst zu aktualisieren, wurde eine einfache Anleitung geschrieben. Die meisten 0.9er-Versionen nutzen die Mozilla-Gecko-Engine 1.7.x – spätere (1.x) die SeaMonkey-Gecko-Engine 1.8.x. Alternativ findet man jeweils fertige Builds und Vorabversionen (RC) im K-Meleon-Forum. Deutsche Fragen werden gegenwärtig dort beantwortet (Stand 2017).\n\n"}
{"id": "280816", "url": "https://de.wikipedia.org/wiki?curid=280816", "title": "2600: The Hacker Quarterly", "text": "2600: The Hacker Quarterly\n\nDas 2600: The Hacker Quarterly ist eine vierteljährlich erscheinende US-amerikanische Zeitschrift und zählt zu den bedeutendsten Magazinen der Hacking- und Phreakingszene. Sie wurde 1984 von Eric Corley, auch bekannt als Emmanuel Goldstein, ins Leben gerufen und beschäftigt sich primär mit Sicherheitslücken in der Informations- und Kommunikationstechnik. Ihren Namen erhielt die Zeitschrift in Anlehnung an einen in den späten 1960er Jahren von Phreakern verwendeten Ton von 2600 Hz, welcher im Rahmen des Blue Boxing verwendet wurde. Damit konnten im damaligen amerikanischen Telefonsystem kostenlose Telefonate erschlichen werden.\n\nIm Jahr 1999 wurde das Magazin von acht US-Filmstudios wegen Veröffentlichung des DeCSS-Codes unter Berufung auf den \"Digital Millennium Copyright Act\" verklagt. Nach einem Aufsehen erregenden Prozess in New York musste \"2600\" nicht nur DeCSS von ihrer Website entfernen, sondern auch sämtliche Links auf andere Websites, bei denen DeCSS erhältlich war. Die Revision im Jahr 2001 verlor \"2600\" ebenfalls.\n\nAnlässlich des 10-jährigen Bestehens des Magazins fand 1994 die erste Konferenz der Reihe \"Hackers on Planet Earth\" statt.\n\n\n"}
{"id": "280855", "url": "https://de.wikipedia.org/wiki?curid=280855", "title": "Happy Computer", "text": "Happy Computer\n\nDie Happy Computer aus dem damaligen Markt+Technik Verlag (später teilweise vom WEKA-Verlag übernommen) war neben verschiedenen anderen deutschen Computerzeitschriften (64’er, Chip, CPC Amstrad International, Computer Persönlich) eine der Fachpublikationen für Heimcomputeranwender in den 1980er-Jahren.\n\nVon November 1983 bis zur letzten Ausgabe 1990 bot sie aktuelle Neuheiten, Vergleichstests, Tipps und Tricks sowie Listings rund um damals erfolgreiche Heimcomputersysteme wie Amstrad CPC, Atari 400, Atari 800, Commodore 64, Dragon 32, Dragon 64, MSX, TI-99/4A, VC 20, Sinclair ZX81, Sinclair ZX Spectrum. Die \"Happy Computer\" war eine der Computerzeitschriften, die sich mit verschiedenen Systemen beschäftigte, und bot so dem Leser, in jeder Ausgabe liebevoll mit \"„Hallo Freaks“\" begrüßt, einen Blick über den Tellerrand „seines“ Systems.\nDer Spieleteil der \"Happy Computer\" bestand anfangs aus nur einigen Seiten. Aus ihm wurde „Der große Spiele-Sonderteil“, welcher von Ausgabe 11/1986 bis 9/1988 erschien. Aus diesem wiederum entstand ab Anfang 1988 das Spielemagazin Power Play. Einige der Autoren der ersten Stunde sind auch heute noch bekannt, vornehmlich Heinrich Lenhardt und Boris Schneider.\n\nDie Erstausgabe hatte den Titel \"Hobby Computer\". Nach einer rechtlichen Auseinandersetzung wegen der Namensähnlichkeit zum Magazin \"Hobby\" wurde die Zeitschrift mit der zweiten Ausgabe (Dezember 1983) in \"Happy Computer\" umbenannt.\n\nEine weitere Namensänderung, verbunden mit einer kompletten Umstellung des Heftkonzepts, erfolgte im Januar 1990: Aus Happy Computer wurde \"Computer Live\", welche von der bisherigen Leserschaft aufgrund des Wandels vom Heimcomputer-orientierten zum „seriösen“ PC-Magazin jedoch nicht angenommen wurde.\n\n"}
{"id": "281001", "url": "https://de.wikipedia.org/wiki?curid=281001", "title": "Multi-Agenten-Simulation", "text": "Multi-Agenten-Simulation\n\nDie Multi-Agenten-Simulation in der Informatik wendet das Konzept der Multi-Agenten-Systeme in der Simulation an. Aktive Komponenten des zu untersuchenden Systems werden als Agenten betrachtet, deren Verhalten einzeln spezifiziert wird. Damit können insbesondere emergente Phänomene und dynamische Wechselwirkungen zwischen Agenten nachgewiesen werden. Multi-Agenten-Simulation ist sehr verbreitet in den Bereichen Biologie, Soziologie, Verkehrsphysik und bei Evakuierungssimulationen. Dort werden sie verwendet, um Zusammenhänge zu verstehen und Theorien in einer kontrollierten Laborumgebung zu belegen. Seit einigen Jahren wird die Multi-Agenten-Simulation auch verstärkt in der 3D-Computergrafik und Filmtechnik eingesetzt, um Massenszenen kostengünstig zu simulieren. Ein weiteres Anwendungsgebiet sind makroökonomische Wirtschaftssimulationen. Wenn die Agenten in einer (simulierten) räumlichen Umgebung handeln und diese diskret abgebildet wird, ist der Übergang zu Zellularautomaten fließend.\n\n\nEs gibt zahlreiche Werkzeuge und Frameworks zur Modellierung und Simulation von Agentensystemen. Wichtige Werkzeuge sind:\n"}
{"id": "281523", "url": "https://de.wikipedia.org/wiki?curid=281523", "title": "Hacking Challenge", "text": "Hacking Challenge\n\nDer englischsprachige Begriff Hacking Challenge (deutsch: „Herausforderung zum Hacken“), auch Hacking Contest oder Hackit genannt, bezeichnet eine Aufgabe oder eine Abfolge von Aufgaben aus dem Bereich des Hackens. Hackits bieten ein spielerisches erlernen der IT. Programmieren, Mathematik, Kryptographie und Logik Aufgaben widmen sich den Grundlagen eines Computersystems. Exploitation und Cracking wird mitbetrachtet. Alltägliche Protokolle wie HTTP (Cookies) und IP werden dem Teilnehmer im Detail nahegebracht.\n\nEs gibt viele statische Hackit Seiten die mehrere hundert unterschiedliche Aufgaben bieten. Zeitlich terminierte Hackit Events bezeichnet man eher als CTF. Auf einigen Hacker Conventions (z. B. Blackhat) wird jährlich ein CTF veranstaltet wo professionelle hacker gegeneinander antreten. \n\nSolche Aufgaben werden zumeist auf speziellen Webseiten für Menschen angeboten, die sich für Computersicherheit interessieren und fördern die spielerische und sportliche Auseinandersetzung mit diesem Themenbereich. Ein ähnliches Konzept sind Wettbewerbe, mit denen ein Unternehmen oder eine Organisation versucht zu testen, ob ein System hinreichend sicher ist und, um dies zu prüfen, zum Knacken des Systems aufruft. Diese Wettbewerbe sind meist mit vergleichsweise hohen Preisgeldern im Bereich von umgerechnet mehreren Tausend Euro dotiert. \n"}
{"id": "284706", "url": "https://de.wikipedia.org/wiki?curid=284706", "title": "DKBTrace", "text": "DKBTrace\n\nDKBTrace ist ein 1986 von Aaron A. Collins und David Buck geschriebener Raytracer, dessen Quellcode 1991 freigegeben wurde. Auf dieser Basis entstand der populäre Raytracer POV-Ray.\n\nDKBTrace lief zunächst auf dem Amiga, später auch unter MS-DOS und UNIX. Es besaß keine grafische Benutzeroberfläche; die Szene wurde mit einer speziellen Beschreibungssprache (Scene Description Language, SDL) beschrieben, die DKBTrace dann als Textdatei per Kommandozeile übergeben wurde.\n\n"}
{"id": "284737", "url": "https://de.wikipedia.org/wiki?curid=284737", "title": "Bluescreen (Windows)", "text": "Bluescreen (Windows)\n\nEin Bluescreen (auch Blue Screen, selten Blauschirm; wörtlich übersetzt \"Blauer Bildschirm\"; von Microsoft (bis Windows 8) offiziell \"Bugcheck\" genannt), auch scherzhaft \"Blue Screen of Death\" (Blauer Bildschirm des Todes, kurz auch \"BSoD\") oder \"Blauer Tod\" (in Anlehnung an den Schwarzen Tod genannt), ist eine Beschreibung einer bestimmten Kategorie von Fehlermeldungen (\"stop errors\"), die insbesondere von Microsoft-Windows-Betriebssystemen angezeigt werden. Nach einem kritischen Systemfehler wird das System gestoppt und die Bedienoberfläche des Betriebssystems vollständig durch einen blauen Bildschirm ersetzt, auf dem in weißer Schrift die Fehlerinformationen erscheinen.\n\nAusgelöst werden diese Meldungen in den häufigsten Fällen nicht durch Fehler in Anwendungsprogrammen, sondern durch Fehler in Gerätetreibern oder in der Hardware. Wird der \"Bluescreen\" nicht von einem Defekt in der Hardware ausgelöst, so handelt es sich entweder um einen Programmierfehler in einem Treiber, im Kernel, der oft mittels eines Patches vom entsprechenden Hersteller behoben werden kann, oder um eine fehlerhafte Systemkonfiguration. In bestimmten Fällen kann auch ein Fehler in der Softwarearchitektur zu Grunde liegen, der bis zum Ende der Lebensdauer des Produktes erhalten bleibt.\nIn anderen Betriebssystemen werden derartige Fehler als Kernel panic bezeichnet.\n\nGeräte- und andere Hardwaretreiber werden in einem privilegierten Modus (\"Kernel mode\") ausgeführt, wobei sie direkten Zugriff auf Systemspeicherbereiche und Hardwareschnittstellen haben. Schreibt ein fehlerhafter Gerätetreiber Daten in einen Speicherbereich, der von anderen Systemteilen (auch anderen Gerätetreibern) benutzt wird, so wird die Systemintegrität verletzt. Wenn das System in solchen Fällen weiterliefe, bestünde die Gefahr der irreversiblen Zerstörung von Daten (zum Beispiel auf der Festplatte). Daher wird das System sofort angehalten und ist nicht mehr bedienbar, was wiederum ebenfalls nachteilige Wirkung auf die Datenintegrität haben kann, insbesondere gehen nicht gespeicherte Daten des Benutzers verloren. Das System wird ebenfalls angehalten, falls ein Hardwareproblem auftritt, das nicht ignoriert werden kann. Dazu gehört zum Beispiel das Versagen der primären Festplatte des Systems beim Versuch, Daten von der Auslagerungsdatei zu lesen.\n\nBei anderen Betriebssystemen (etwa Unix) entspricht dem die sogenannte Kernel panic.\n\nBei modernen Hauptplatinen (siehe auch ATX) kann auch eine Überhitzung des Prozessors zu einem \"Bluescreen\" führen, da die Temperaturkontrolle den Prozessor in den \"HALT\"-Modus setzt.\n\nIn den Windows-9x-Versionen sind diese Meldungen zur Problemlösung schlecht geeignet, da dem Anwender kaum Informationen mitgeteilt werden:\n\nZusätzlich werden noch zwei Adressen und teilweise auch die betroffene Datei ausgegeben.\n\nIm Internet, insbesondere in der \"Microsoft Knowledgebase\", werden viele Kombinationen von Fehlercodes und Adressen sowie mögliche Ursachen beschrieben.\n\nMit der Einführung von Windows NT 4.0 wird eine Fehlernummer und Fehlerbezeichnung angezeigt und zudem vier Zahlenwerte, mit deren Hilfe erfahrene und fachkundige Anwender unter Zuhilfenahme der Microsoft-Website meist Fehlerursachen ausmachen können. Windows kann auch so konfiguriert werden, dass der Inhalt des Arbeitsspeichers zur nachträglichen Analyse als \"Kernel-\", \"kleines\" oder \"vollständiges Speicherabbild\" auf die Festplatte geschrieben wird (englisch \"core dump\"), sofern das noch möglich ist.\n\nAb Windows 8 wurde der Bluescreen vereinfacht, auf dem Bildschirm wird ein trauriges Gesicht angezeigt, darunter steht, dass ein Fehler aufgetreten ist und dass Informationen darüber gesammelt werden, wie fortgeschritten das ist, wird in Prozenten beschrieben. Unten steht ein Fehlercode, allerdings ohne Stop-Code. Standardmäßig wird, sobald die Informationen gesammelt wurden, ein Neustart ausgeführt. In neueren Versionen von Windows 10 bietet Microsoft an, einen QR-Code zu scannen, um dann auf die Webseite mit den Lösungen weitergeleitet zu werden.\nBei MS-DOS-basierten Windows-Versionen erfordert diese Meldung häufig, bei Windows-NT-basierenden immer einen Neustart des Systems. In den Home-Versionen von Windows XP (Service Pack 1 und 2) wurde der erforderliche manuelle Neustart in der Standard-Konfiguration durch einen automatischen Neustart ersetzt; in kritischen Situationen kann dies jedoch zur Verklemmung (\"deadlock\") des Systems führen: Der Computer stürzt mit blauem Bildschirm ab, startet automatisch neu und stürzt sofort wieder ab. Abhilfe bringt in solchen Fällen die Auswahl der Option „Automatischen Neustart bei Systemfehler deaktivieren“ im Menü des Bootmanagers sowie eventuell das Starten des Rechners im abgesicherten Modus, welches das zuvor genannte oft nahezu sofortige Neustarten verhindert.\n\nDas vielleicht berühmteste Auftreten eines Windows-9x-Bluescreens trat während einer Präsentation von Windows 98 Beta durch Bill Gates auf der COMDEX am 20. April 1998 auf: Der Vorführ-Computer stürzte mit einem Bluescreen ab, als Gates’ Assistent Chris Capossela einen Scanner mit dem PC verband, um die Unterstützung von Plug-and-Play-Geräten in Windows 98 zu demonstrieren. Dieses Ereignis brachte donnernden Applaus im Publikum, und Gates antwortete nach einer nervösen Pause: „That must be why we’re not shipping Windows 98 yet.“ (Das ist wohl der Grund, warum wir Windows 98 noch nicht ausliefern.)\n\nIn einigen Beta-Versionen von Windows 8 wurde der Blue Screen durch einen Black Screen (Schwarzer Bildschirm) ersetzt. Trotzdem erscheint in den offiziellen Versionen von Windows 8 ein Blue Screen, allerdings sieht er etwas moderner aus als die Vorgängerversionen.\n\nMS-DOS 6 ist das älteste Microsoft Betriebssystem, das den Blue Screen hat, jedoch wird er in der eingestellten Farbe des BIOS-Bildschirmtreibers (in der Regel Hellgrau auf Schwarz) angezeigt. Er wird von dem Treiber EMM386.EXE – der dem Kernel von Windows oder Linux entspricht – angezeigt, wenn im Protected Mode ein unbehandelter Prozessor-Interrupt (z. B. Segfault, Division by Zero) auftritt. Bei MS-DOS 5 wird in der gleichen Situation ein kritischer Fehler angezeigt, und dem Benutzer, wird die Option gelassen, den Protected Mode zu beenden und zum DOS Befehlszeileninterpreter zurückzukehren, während MS-DOS 6 den „echten“ Bluescreen anzeigt, bei dem nur noch der Neustart möglich ist.\n\nIn Microsoft Windows 3.x erscheint der \"Black Screen of Death\", wenn eine MS-DOS Anwendung nicht richtig ausgeführt werden konnte. Sehr oft tritt dies auf, wenn man bestimmte Funktionen ausführt, während der Netzwerktreiber im Speicher aktiv ist. So beispielsweise während des Ladens des Novell-Netware-Clients NETX für MS-DOS: Das System versucht beim Auftreten des Fehlers in den Textmodus zu wechseln, zeigt aber nichts an, sodass der Anwender vor einem leeren schwarzen Bildschirm sitzt, auf dem in der linken oberen Ecke ein Cursor blinkt. Der Anwender kann zu diesem Zeitpunkt keine Eingaben mehr machen und ist gezwungen, einen Kaltstart des Systems durchzuführen, um weiterzuarbeiten.\n\nLaut Wallace McClure von ASP.net wurde der Ausdruck \"Black Screen of Death\" das erste Mal im Sommer 1991 von Ed Brown, einem IT-Techniker der Coca-Cola Company in Atlanta verwendet, der berichtete, dass die Angestellten des Unternehmens beim Versuch, WordPerfect zu starten, ab und zu einen \"Black Screen of Death\" erhielten.\n\nAuf der Xbox 360 wird bei einem Generalfehler ein Schwarzer Bildschirm angezeigt und die Konsole kann nicht mehr verwendet werden. Auf dem Black Screen steht, man sollte den Xbox Support kontaktieren. Außerdem wird ein Fehlercode (z. B. E71, E64...) angezeigt. Der Kreis, auf dem normalerweise die Anzahl der angeschlossen Controller angezeigt wird, leuchtet ein Viertel rot, ähnlich wie bei dem Red Ring of Death.\n\nIn OS/2 ist ein \"Black Screen of Death\" entweder ein TRAP Screen oder ein full-screen hard-error VIO pop-up. Beide sorgen dafür, dass die Anzeige in den Textmodus geschaltet wird. Diese Anzeige hat zur Darstellung des Textes 25 Zeilen mit je 80 Spalten, in denen weiße Buchstaben auf einen schwarzen Hintergrund dargestellt werden, woher diese Anzeige auch ihren Namen hat.\n\nTRAP Screen\n\nEin TRAP Screen wird angezeigt, wenn im Kernel ein Fehler auftritt, der zur Laufzeit nicht korrigiert werden kann. Normalerweise passiert dies nach dem Übertakten von Hardware, aber auch bei Softwarefehlern im Kernel oder bei Gerätetreibern.\n\nDer TRAP Screen enthält dabei sowohl eine Auflistung der Prozessor-Register und des Stacks als auch Informationen über die Version des Betriebssystems und den aufgetretenen Fehler im Prozessor.\n\nDie einzige Möglichkeit die der Anwender in dieser Situation hat ist entweder ein Neustart des Systems durch den Klammergriff (gleichzeitiges Drücken von + + ) oder durch zweimaliges Drücken von + + das System anzuweisen alle Informationen aus dem Speicher auf Diskette zu speichern.\n\nHard Error Screen\n\nEin Full-screen hard-error VIO pop-up wird angezeigt, wenn ein Prozess einen schweren Fehler verursacht, entweder bei einem Absturz eines Programmes oder einem schweren Fehler, bei dem es möglich ist, das System wiederherzustellen, z. B. ausgelöst durch den Versuch, eine Diskette einzulesen, obwohl sich keine im Diskettenlaufwerk befindet.\n\nDiese Anzeige wird erstellt vom Hard-Error-Daemon-Prozess, der schwere Fehler von anderen Prozessen behandelt. Technisch betrachtet ist diese Anzeige eine VIO-pop-up-Anzeige. Alle Prozesse (ausgenommen derjenige, der den schweren Fehler ausgelöst hat) werden weiter ausgeführt. Dieser Daemon benutzt einen VIO-Pop-up, wenn entweder das System im Textmodus gestartet wurde oder wenn der Fehler in einem Prozess auftritt, der im Vollbildmodus läuft.\n\nDieser Pop-up enthält Informationen über den aufgetretenen Fehler und den auslösenden Prozess.\n\nDer Anwender wird daraufhin gefragt, wie weiter vorgegangen werden soll. Er kann zwischen folgenden Aktionen wählen:\n\nDer Red Screen of Death (RSoD) ist eine Variante, der in einigen Beta-Versionen des Betriebssystems Windows Vista vorhanden war.\n\nAls \"Red Screen of Death\" werden außerdem auch fatale Fehler in neueren Versionen von Lotus Notes bezeichnet. Dort erscheint eine Fehlermeldung, aber nicht wie bei Windows Vista im Vollbildmodus, sondern als rote Box mit schwarzem Rahmen.\n\nAndere Verwendungen\n\nAuch das iPhone 5s zeigt in Folge eines Systemabsturz einen leeren blauen Bildschirm und führt anschließend einen Neustart des Geräts durch. Eine mögliche Ursache wird softwareseitig in der Synchronisation der Appleeigenen iWork Apps Pages, Numbers oder Keynote gesehen. Auch eine Falschmontage eines Ersatzdisplays kann den Fehler auslösen.\n\n\n"}
{"id": "286186", "url": "https://de.wikipedia.org/wiki?curid=286186", "title": "WordStar", "text": "WordStar\n\nWordStar war eines der ersten Textverarbeitungsprogramme für Personal Computer. Die Version 1.0 war eine Weiterentwicklung des Programms \"WordMaster\" und wurde im September 1978 für das Betriebssystem CP/M veröffentlicht.\n\nDer ehemalige IMSAI-Marketingchef Seymour Rubinstein gründete 1976 die Firma \"MicroPro International Inc.\" Der Systemprogrammierer John Robbins Barnaby, welcher zuvor ebenfalls für IMSAI tätig war, entwickelte \"WordMaster\" für das Betriebssystem CP/M.\n\nWordStar wurde erstmals im April 1979 auf der \"West Coast Computer Faire\" in der \"Brooks Hall\" in San Francisco gezeigt.\n\nDie Programmierer Rob Barnaby und Jim Fox hatten für dieses relativ umfangreiche Programm neue Techniken entwickelt, so zum Beispiel das Benutzen von Overlay (Programmierung), das heißt das Verarbeiten von Dateien, die größer sind als der Arbeitsspeicher des Computers – denn CP/M lief damals auf Prozessoren, die nur bis zu 64 kB RAM adressieren konnten, wodurch etwa zehn Schreibmaschinenseiten Text auf einmal in den Speicher passten.\n\nNachdem es das Programm anfangs für CP/M gab, wurde es später auch auf andere Betriebssysteme portiert (Apple II, MS-DOS, Windows).\n\nWordStar brachte viele neue Funktionen mit, so etwa das \"WordStar-Kreuz\" oder \"Steuerungsdiamant\": die Tasten Strg-S, Strg-D, Strg-E und Strg-X bildeten ein Kreuz, mit dem der Zeiger nach links, rechts, oben oder unten bewegt wurde – die Pfeiltasten gab es damals auf vielen Tastaturen noch nicht. Eine ganze Zeile wurde mit Strg-Y gelöscht. Viele dieser Tastenbefehle wurden von WordStar-kompatiblen Editoren übernommen, so zum Beispiel von dem Unix/Linux-Editor Joe, dem MS-DOS-Editor „EDIT“ oder den Programmieroberflächen für Turbo Pascal und QBasic.\n\nMan konnte die Hälfte des Bildschirms mit einem Hilfe-Fenster füllen, auf dem alle wichtigen Tastaturbefehle erklärt waren. Wenn man die Befehle kannte, konnte man das Hilfe-Fenster verkleinern oder ganz wegschalten, um mehr Platz für den eigenen Text zu haben. Bei Befehlen, die aus zwei Tasten bestanden (wie etwa Strg-OL5 – linken Rand auf die 5. Spalte setzen) konnte, wenn man nach dem ersten Tastendruck ausreichend lange zögerte, automatisch das zu diesem ersten Tastendruck passende Hilfe-Fenster erscheinen.\n\nDie Fähigkeiten von WordStar 3.0 (1982 erschienen) waren immens. Es gab variable Tabulatorstopps, man konnte die Zeilen automatisch mit Leerzeichen füllen lassen, um Blocksatz zu erhalten, und es gab „weiche“ Trennungsstriche, die nur dann gedruckt wurden, wenn sie tatsächlich am Zeilenende standen. WordStar war auch den Möglichkeiten von CP/M und den damaligen Bildschirmen weit voraus. So konnte man auch Befehle geben, Zeichen kursiv oder fett zu drucken – diese wurden zwar am Bildschirm nicht kursiv bzw. fett dargestellt, sondern nur durch besondere Kennzeichen hervorgehoben, aber der Drucker druckte sie korrekt (eine Druckvorschau existierte spätestens seit der DOS-Version 5). Einige Bearbeitungsfunktionen, wie rechtsbündige Tabulatorstopps und manuelle Zeilenwechsel innerhalb eines Absatzes, fehlten allerdings in allen DOS-Versionen.\n\nNachdem das Betriebssystem CP/M keine einheitliche Bildschirm- oder Druckerausgabe unterstützte, war es jeweils erforderlich, das Anwenderprogramm entsprechend anzupassen. WordStar beinhaltete dazu an definierten Stellen des ausführbaren Programms speziell reservierte Code-Blöcke, um mit Hilfe eines Debuggers Zeichenketten (z. B. Escape-Sequenzen) oder kleine Maschinenprogramme einfügen und so die Hardware-Anpassung flexibel vornehmen zu können. Dies war einer der Gründe für die damalige sehr große Verbreitung von WordStar.\n\nFür die Druckersteuerung wurden eine Menge von \"Dot Commands\" eingeführt: eine Buchstabenkombination, die einem Punkt am Zeilenanfang folgte. Zum Beispiel bedeutete \".CP 10\", dass der Drucker ein neues Blatt beginnen sollte, falls weniger als 10 leere Zeilen bis zum regulären Blattende frei waren.\n\nWordStar wurde durch Zusatzpakete erweitert, so beispielsweise SpellStar für Rechtschreibkorrektur und MailMerge für Serienbriefe.\n\nNeben einer Zahl von an Robotron und andere Hardware angepasster Originalversionen wurde auch eine Textverarbeitung names \"TP\" auf dem CP/M-Derivat SCP benutzt, welche außer der Lokalisierung der Benutzeroberfläche von Englisch nach Deutsch, der Änderung der Dateiendung \"DOC\" auf \"TXT\" und einem Konfigurationsprogramm für Robotron-Drucker weitgehend identisch zu WordStar war.\n\nDas 1987 erschienene WordStar 4 war die letzte größere kommerzielle Software für das CP/M-Betriebssystem. Ende der 80er Jahre wurde dann WordStar 7 für DOS entwickelt: mit Maus-Unterstützung, mit einer Macrosprache und mit Zugriff auf die Zwischenablage von Windows 3. Jedoch konnte WordStar ab 1990 den Vorsprung von WordPerfect und später zu Microsoft Word nicht mehr aufholen. Außerdem wurden in dieser Zeit auch für weniger anspruchsvolle Textverarbeitungsaufgaben integrierte Programmpakete wie Microsoft Works und weitere Programme, die nach dem WYSIWYG-Prinzip arbeiten, immer populärer.\n\nMehrere Autoren verwenden das Programm WordStar bis heute, so der kanadische Science-Fiction-Autor Robert J. Sawyer und der Autor der Fantasy-Saga Das Lied von Eis und Feuer George R. R. Martin. Auch der Autor William F. Buckley, Jr. war ein bekannter Befürworter von WordStar.\n\nWordstar\n\nNewStar\n\nWordstar\n\nMicroPro Easy\n\nWordStar 2000 für DOS\n\nWordStar für Windows\n\n\n"}
{"id": "287279", "url": "https://de.wikipedia.org/wiki?curid=287279", "title": "Address Windowing Extension", "text": "Address Windowing Extension\n\nAddress Windowing Extension (AWE, engl. für \"Erweiterung (durch) Adressierungsfenster\") ist eine Windows-Programmierschnittstelle zur Unterstützung von mehr als 4 GiB Hauptspeicher auf i386-kompatiblen 32-Bit-Plattformen.\n\nAWE erlaubt es einem Programm, physische Adressbereiche zu reservieren und diese in den virtuellen Adressraum des Prozesses einzublenden. Falls der reservierte Speicherbereich größer ist als mit einem Mal eingeblendet werden kann, muss das Mapping umgeschaltet werden, um auf die übrigen Speicherbereiche zuzugreifen. Es kann dann stets nur auf einen Ausschnitt des reservierten Adressbereiches zugegriffen werden. Diese als „Overlay“ oder „Windowing“ bekannte Technik ist vom Prinzip her vergleichbar zu dem unter DOS angebotenen EMS-Speicher.\n\nMit der AWE können bis zu 2 Bytes (= 64 GiB) Hauptspeicher angesprochen werden.\n\nNutzbarer Speicher bei Windows 2000 Server 32-Bit-Versionen:\n\nNutzbarer Speicher bei Windows Server 2003 32-Bit-Versionen:\n\nWindows XP und Windows Vista (32-Bit-Versionen) sind aus Kompatibilitätsgründen (Treiber und PCI-Subsystem) auf 4 GiB beschränkt. Die AWE ist insoweit abhängig von der Physical-Address Extension (PAE) als ohne PAE-Technik die AWE-Funktionen keinen Arbeitsspeicher (RAM) über 4 GiB reservieren können.\n\n"}
{"id": "287383", "url": "https://de.wikipedia.org/wiki?curid=287383", "title": "Bildhelligkeit", "text": "Bildhelligkeit\n\nDer Begriff der Bildhelligkeit kann als Beleuchtungsstärke E (Einheit \"Lux\" oder \"Lumen/m²\") in der Bildebene eines abbildenden Systems\noder als Grauwert (Einheitenlos, bei 8 Bit Graustufen ein Wert zwischen 0 und 255) bei einem digitalen Bild interpretiert werden.\nFür die subjektive Qualität eines Bildes ist neben der Bildhelligkeit vor allem ein ausreichender Kontrast notwendig.\n\nMathematisch kann die Bildhelligkeit eines Grauwertbildes als Mittelwert aller Grauwerte, der Kontrast als Varianz aller Grauwerte definiert werden.\n"}
{"id": "287476", "url": "https://de.wikipedia.org/wiki?curid=287476", "title": "Amadeus IT Group", "text": "Amadeus IT Group\n\nAmadeus ist ein europäisches Softwareunternehmen, das das gleichnamige Computerreservierungssystem (CRS) vertreibt.\n\nDas Flugreservierungssystem \"Amadeus\" entstand Ende der 1980er-Jahre: Am 17. Juni 1987 unterzeichneten Air France, Iberia, SAS und die Deutsche Lufthansa in Paris Verträge zur Gründung des CRS Amadeus, nach der ein international vermarktungsfähiges CRS auf europäischer Basis entwickelt werden sollte, um die US-amerikanische Vormachtstellung zu brechen, die unter anderem durch das CRS \"Sabre\" bestimmt wurde. Die erste Phase der Systementwicklung war 1991 abgeschlossen, die erste Buchung mit Amadeus erfolgte am 7. Januar 1992. 1995 wurde der amerikanische Konkurrent \"System One\", eine Tochter der Fluggesellschaft Eastern Air Lines, übernommen. Im Jahr 2002 stieg Amadeus zum Weltmarktführer der Flugreservierungssysteme auf. 2005 wurde eine strategische Neuausrichtung eingeleitet. Amadeus strebt an, zum weltweit führenden Anbieter von IT-Lösungen für Airlines und die gesamte Touristikindustrie zu werden.\n\nHauptsitz des Unternehmens ist Madrid, von wo aus auch das Marketing geleitet wird. Der Standort Sophia Antipolis bei Nizza ist unter dem Namen \"Amadeus S.A.S\" für die Entwicklung und den Vertrieb der Software zuständig. Das Rechenzentrum namens \"Amadeus Data Processing GmbH\" befindet sich in Erding bei München. Weitere wichtige Standorte von Abteilungen und Tochterunternehmen sind Bad Homburg vor der Höhe, Miami, Bangkok, Bangalore, London, Buenos Aires und Sydney.\n\nEnde 2005 wurde Amadeus mit einer fremdfinanzierten Übernahme weitestgehend (99,0 %) an die neu gegründete Gesellschaft WAM Acquisition SA verkauft. Diese Gesellschaft ist im Mehrheitsbesitz der Amadelux Investment SA (54,24 %) (BC Partners/Cinven), bei einer Minderheitsbeteiligung von Air France (22,88 %), Iberia (11,44 %) und Lufthansa (11,44 %). Amadeus war seitdem nicht mehr an den Börsen von Madrid, Paris und Frankfurt notiert.\n\nIm April 2010 brachte die WAM Acquisition SA einen Teil von Amadeus zurück an die Madrider Börse. Die erste Tranche des Börsengangs, der vierfach überzeichnet war, brachte ca. 1,3 Milliarden Euro ein, wovon knapp eine Milliarde zur Schuldentilgung eingesetzt werden soll. Während Lufthansa und Air France ihre Anteile ebenfalls an die Börse bringen wollen, will Iberia den eigenen Anteil behalten.\n\nDie Lufthansa hat am 13. November 2012 bekanntgegeben, 3,61 % der dort verbliebenen 7,61 % verkaufen zu wollen.\n\nIn den einzelnen Ländern sind \"Amadeus Commercial Organisations\" (ACOs) für die Betreuung der Reisebüros und der Veranstalter zuständig. Die ACOs sind eigenständige Gesellschaften, die sich aber oft im Besitz von Amadeus befinden. Die für Deutschland zuständige ACO ist Amadeus Germany in Bad Homburg bei Frankfurt am Main. Die deutsche ACO nimmt eine Sonderstellung ein, da sie zusätzlich zu regionalen auch globale Aufgaben übernommen hat, wie etwa den weltweiten zentralen Einkauf, Entwicklung oder Security-Dienste.\n\nDer Entwicklungsstandort Frankfurt ist eng mit Sofia Antipolis und Antwerpen vernetzt. In Bad Homburg finden zum einen Entwicklungen für das frühere Start – jetzt German Backend System statt. Zum anderen sind hier die globalen Entwicklungsabteilungen für die Amadeus Selling Platform sowie das Mid- und Backoffice situiert.\n\nIm Dezember 2016 wurde bekannt, dass Forscher des Berliner Unternehmens „SRLabs“ Sicherheitslücken im Amadeus-Buchungssystem gefunden hatten, wobei auch gleichartige Sicherheitslücken in den Systemen von Sabre (CRS) oder hauseigenen Systemen von Airlines gefunden wurden, die deutsche Presse sich bei der Berichterstattung aber nur auf Amadeus fixierte.\nDabei war durch eine unzureichende Absicherung der „CheckMyTrip“ Website ein systematisches Ausprobieren der Kombination aus Buchungsnummer und Nachname des Reisenden möglich. Ist dann die Kombination bekannt, können Details der Buchung bösartig geändert werden. Nach der Veröffentlichung des Berichtes wurde die Website durch restriktiveres \"Rate Limiting\" gesichert.\n\n"}
{"id": "288566", "url": "https://de.wikipedia.org/wiki?curid=288566", "title": "Computerreservierungssystem", "text": "Computerreservierungssystem\n\nEin Computerreservierungssystem (CRS) ist ein meist vernetztes Informationssystem über Preise, Verfügbarkeiten und Buchungsmöglichkeiten von Reisen wie Flügen, Hotels, Mietwagen, Fähren, Kreuzfahrten, Bahnen, Bussen und Pauschalreisen.\n\nEs wird zumeist auch der Begriff Globales Distributionssystem (GDS) verwendet. Ein globales Distributionssystem (GDS) ist ein Medium, mit dem Reisebüros Informationen und Vakanzen abfragen sowie Kundendaten und Leistungen erfassen, verarbeiten bzw. buchen können. Typischerweise handelt es sich um Systeme, die eine (informations-)logistische Funktion wahrnehmen. Sie halten aktuelle Informationen über alle verfügbaren Leistungsanbieter bereit und verfügen über die notwendige Infrastruktur zur Datenübermittlung. Systembeteiligte sind:\n\n\nIn der Reisebranche ist es üblich, dass Reisebüros keine eigenen Produkte verkaufen, sondern lediglich im Namen der Reiseveranstalter vermitteln. Als Handelsvertreter oder Handelsmakler wollen die Reisebüros hierbei auf möglichst viele Leistungsträger (Beherbergungs-, Verpflegungs- und Transportbetriebe) zurückgreifen, um ihren Kunden attraktive Angebote zu machen. Das lässt sich nicht durch Reisebüro-eigene Computersysteme erreichen. Wirtschaftlich unsinnig wäre auch eine direkte Anbindung an die Leistungsträger, denn es müssten dann parallel mehrere Anbindungen in einem Reisebüro geschaltet werden, wenn mehr als ein Leistungsträger im Angebot sein soll. Abgesehen davon wäre ein direkter Zugriff auf die unterschiedlichen Systeme der Leistungsträger mit jeweils spezifischen Eingabemasken mit einem nicht zu bewältigenden Schulungsbedarf für die Reisebüromitarbeiter verbunden.\n\nAus diesem Grunde werden Computerreservierungssysteme eingesetzt. Technisch werden diese durch die Anbindung des Rechenzentrums des CRS-Anbieters an viele verschiedene Leistungsträger realisiert, über die die jeweiligen Anfragen der Reisebüros direkt an die verschiedenen Anbieter weitergeleitet werden. Die Eingaben erfolgen über einheitliche und produktspezifisch angepasste Masken, welche für alle angeschlossenen Leistungsträger gelten, wodurch der Schulungsaufwand für die Reisebüros auf ein Mindestmaß reduziert wird. Die CRS übernehmen zudem die Abrechnung der bei einer Buchung anfallenden Kosten sowie bei Bedarf auch die Auszahlung von Incentives, also Vergütungen der Leistungsträger für die Reisebüros. Die CRS liefern aber auch Auswertungen für die Backoffice-Systeme der Reisebüros und bieten Zusatzprodukte zur Unterstützung der Verkaufsprozesse an.\n\nMit zunehmender Ausweitung des Internets begann die zum Teil monopolartige Stellung der CRS zu bröckeln. Die CRS versuchen dem mit zusätzlichen Produkten (wie Ertragsmanagement und Geomarketing) und Dienstleistungen zu begegnen. Teilweise kaufen oder gründen die CRS auch Internet-Portale, um diesen Vertriebskanal für sich nutzen zu können.\n\nWeltweit haben sich vier Computerreservierungssysteme mit nennenswertem Marktanteil etabliert:\n\nNicht immer ganz trennscharf ist die Unterscheidung zwischen den sogenannten CR (CRS) und GD (GDS)-Systemen.\n\nWährend sich die CRS hauptsächlich mit der eigentlichen Bearbeitung der Reservierungen beschäftigt (z. B. direkt im Hotel), sind die Global Distribution Systeme (GDS) globale Datenbanken, die die Reservierungsdaten ausschließlich verwalten. Ohne ein passendes CRS kann jedoch meist nicht auf die GDS zugegriffen werden. Bei den oben genannten großen Systemen sind die Übergänge jedoch oft fließend, da sowohl GDS als auch CRS aus einer Hand angeboten werden. Grundsätzlich handelt es sich jedoch um verschiedene Systeme im Buchungsprozess.\n\n\n\n"}
{"id": "292470", "url": "https://de.wikipedia.org/wiki?curid=292470", "title": "Microsoft Groove", "text": "Microsoft Groove\n\nMicrosoft Groove ist eine Kollaborationsanwendung, die ursprünglich vom US-amerikanischen Unternehmen Groove Networks entwickelt wurde. 2002 wurde das Unternehmen für Groove mit dem \"World Technology Award\" in der Kategorie \"Software\" ausgezeichnet. Groove Networks wurde im März 2005 von Microsoft für 120 Mio. US-Dollar übernommen und in Microsoft Office 2007 integriert; Groove ist jedoch auch einzeln erhältlich. Ab Microsoft Office 2010 wurde Groove durch Microsoft SharePoint Workspace 2010 ersetzt.\n\nVorteilhaft ist die Möglichkeit, dass Kommunikation und Abgleich auf Peer-to-Peer-ähnliche, idealerweise serverlose Strukturen setzen, welche die gemeinsame Nutzung von Dateien jeglicher Art ermöglichen. Ebenfalls ist durch Groove ein gemeinsames Bearbeiten desselben Office-Dokuments sowie Chat, Voicemailing, und einfache Anwendungen möglich. Geeignet ist es zudem auch für verteilte Projekt-Teams und bietet dazu verschlüsselte Übertragung End2End. Die Synchronisierung der Dokumente und der Austausch von Informationen setzt nicht voraus, dass die Beteiligten gleichzeitig online sind, da die Daten auf einen Microsoft Server zwischengespeichert werden können.\n\nGroove benötigt einen Relay über Groove-Networks-Server, deshalb kein reines P2P. Zudem ist es ausschließlich für Microsoft Windows erhältlich.\n\nDer integrierte Chat (basierte auf XMPP) wurde durch den Windows Live Messenger ersetzt.\n\n"}
{"id": "294653", "url": "https://de.wikipedia.org/wiki?curid=294653", "title": "SafeGuard Easy", "text": "SafeGuard Easy\n\nSafeGuard Easy ist eine kommerzielle Software zur partitionsweisen Verschlüsselung von Festplatten, Disketten und Wechseldatenträgern (beispielsweise iomega Zip oder USB-Stick) vom deutschen Hersteller Utimaco Safeware AG. Seit der Übernahme durch Sophos wird das Produkt als \"Sophos SafeguardEasy\" vermarktet. Die momentane Version 5.60 ist für die Microsoft-Betriebssysteme Windows 2000, Windows XP, Windows Server 2003, Vista und Windows 7 geeignet. In der Vergangenheit wurden seit der ersten Version 1992 aber auch MS-DOS, Windows 3.x, Windows 9x, Windows NT 4 und OS/2 unterstützt.\n\nDie Verschlüsselung erfolgt in Echtzeit und transparent. Im Normalfall wird ein System mit SafeGuard Easy vollständig verschlüsselt, so dass beispielsweise bei einem Notebook-Diebstahl oder -Verlust kein Zugriff auf die gespeicherten Daten möglich ist – auch nicht beim Ausbau der Festplatte. Selbst die Systempartition von Windows ist ohne Schlüssel nicht auslesbar. SafeGuard Easy ist vor allem bei Unternehmen und auch in der öffentlichen Verwaltung als Lösung zur Festplattenverschlüsselung weit verbreitet, wo es zum Schutz von mobilen Geräten eingesetzt wird.\n\nSafeGuard Easy verwendet anerkannte, teilweise als Open Source verfügbare Verschlüsselungsmethoden. Dazu gehören AES (128 und 256 bit), DES, IDEA, Blowfish-8, -16 und Stealth-40. Eine Bewertung der Sicherheit durch jedermann ist mangels Offenlegung des Quellcodes nicht abschließend möglich. Das Produkt ist jedoch nach Common Criteria (CC) Stufe EAL3 zertifiziert, womit die Vertrauenswürdigkeit entsprechend den CC-Vorgaben nachgewiesen ist.\n\nWeitere Merkmale der Software sind die Authentisierung vor dem Booten (PBA = Pre-Boot Authentication), die Zusammenarbeit mit TPM-Chips, Unterstützung für den Ruhezustand, Unterstützung mehrerer Benutzerkonten und automatische Benutzeranmeldung beim Betriebssystem nach der PBA (Single Sign-on). Zusätzlich können andere Produkte aus der SafeGuard-Familie auch den Zugriff auf Schnittstellen (USB, LAN, FireWire etc.) kontrollieren.\n\nUnterstützt werden die Dateisysteme FAT-12, FAT-16, FAT-32, HPFS, NTFS und NTFS5. Partitionen mit anderen Dateisystemen bleiben unverändert und sind damit nicht durch SafeGuard Easy geschützt. Damit ist ein Parallelbetrieb mit nicht unterstützten Betriebssystemen möglich, die verschlüsselten Partitionen können diese jedoch nicht lesen. Parallele Windows-Installationen können dagegen gegenseitig auf die verschlüsselten Partitionen zugreifen, falls SafeGuard Easy jeweils installiert ist. Um das Starten von Windows bei verschlüsselter Systempartitionen zu ermöglichen, wird SafeGuard Easy über den Master Boot Record geladen. Dabei kann die PBA vorgenommen werden. Anschließend sorgen die geladenen Module dafür, dass sämtliche Schreib- und Lesezugriffe von Windows automatisch ver- bzw. entschlüsselt werden.\n\nÄhnliche Funktionen sind auch in Open-Source-Programmen wie TrueCrypt/VeraCrypt oder FreeOTFE enthalten, welche zudem auch alternative Datei- und Betriebssysteme unterstützen.\n\n"}
{"id": "295543", "url": "https://de.wikipedia.org/wiki?curid=295543", "title": "X68000", "text": "X68000\n\nDer X68000 war ein Heimcomputer der Firma Sharp, welcher nahezu ausschließlich auf dem japanischen Markt verkauft wurde. Daher sind auch alle originären Programme und das Betriebssystem in japanischer Sprache, was die Verbreitung außerhalb Japans verhinderte.\n\nMarkteinführung war im Jahre 1987 als Nachfolger des Sharp X1 und die letzten Modelle kamen 1993 auf den Markt. Der Rechner fuhr sogar auf Knopfdruck in den Standby-Modus und gab so Anwendungen Zeit, ordentlich herunterzufahren und Daten auf die Festplatte zu schreiben.\n\nDer Computer ist in Japan immer noch sehr beliebt und es gibt neuentwickelte Betriebssysteme oder Portierungen wie Minix oder Unix NetBSD. Es gibt auch Ko-Windows, eine neue grafische Benutzeroberfläche und eine Ethernetkarte namens XNeptune.\n\nDie Hardware wurde auch für Spielautomaten verwendet und es gibt viele Portierungen von Spielhallenspielen auf dieses System.\n\nGeordnet nach Erscheinungsjahr, in Klammern interne Modellbezeichnung bzw. Nummer\n\n\n\n"}
{"id": "296170", "url": "https://de.wikipedia.org/wiki?curid=296170", "title": "Tabulatortaste", "text": "Tabulatortaste\n\nDie Tabulatortaste , abgekürzt Tab, (gelegentlich „Tab-Taste“) ist eine Taste auf einer Tastatur (für Schreibmaschinen und Computer, einschließlich Bildschirmtastaturen).\n\nIm folgenden Artikel wird die Funktion der Tabulatortaste bei Computern beschrieben. Eine Beschreibung des Tabulators und des Dezimaltabulators befindet sich im Artikel Typenhebelschreibmaschine.\n\nDer Wortteil „Tabulator“ bezieht sich auf die so benannte Funktion, Texte oder Zahlen in Spalten aufgeteilt sinnvoll fluchtend untereinander anzuordnen (z. B. Tabellen, Rechnungen, Listen). Man unterscheidet dabei zwischen „links ausgerichtet“, „rechts ausgerichtet“, „zentriert“ und „Dezimaltabulator“.\n\nDie Taste befindet sich auf Standardtastaturen links oben, oberhalb der Feststelltaste ( / Caps Lock) und links neben der Q-Taste (). Sie ist auf einer PC-Tastatur mit deutscher / österreichischer Belegung mit zwei querstehenden Pfeilen mit je einem Endstrich gekennzeichnet (), bei schweizerischer Belegung zusätzlich mit „Tab“. Auf einer Apple-Tastatur wird die Taste durch ein Symbol mit einem nach rechts gerichteten Pfeil und einem Endstrich () dargestellt. Mit gedrückter Umschalttaste () bewirkt sie in der Regel einen Rückwärts-Tabulatorschritt.\n\nMit der Tabulatortaste kann man die \"Schreibmarke\" (engl. \"cursor\") um einen vorgegebenen Abstand weiterrücken. Diese Funktion der Tabulatortaste ist in den meisten Textverarbeitungen verfügbar. In vielen Programmen, die der Textbearbeitung dienen, wird dazu unmittelbar ein Tabulator-Zeichen (ASCII-Code 9) in den Text eingefügt. Andere Programme benutzen hierfür eigene Zeichencodes oder ersetzen den Abstand durch Leerzeichen, was nur bei nicht proportionalen Schriftarten sinnvoll ist.\n\nIn einer fortgeschritteneren Variante ist man nicht auf gleiche Abstände der anspringbaren Positionen beschränkt, sondern kann sie frei wählen, um z. B. bestimmte Formulargestaltungen zu erreichen. Diese Funktionalitäten waren so auch schon zumindest bei den besseren Schreibmaschinenmodellen verfügbar.\n\nIn vielen grafischen Benutzerschnittstellen, etwa unter Microsoft Windows und vielen Fenstermanagern unter Unix, kann mit der Tastenkombination + das fokussierte Fenster gewählt werden und auf diese Weise ein im Hintergrund verborgenes Fenster in den Vordergrund geholt werden.\n\nAuf der Befehlszeile von Unix, MacOS und neueren Windows-Versionen dient die Tabulatortaste der Befehlszeilenergänzung.\n\nIn vielen grafischen Anwendungen kann man mit der Tabulatortaste in Dialogfenstern von einem Eingabefeld zum nächsten springen. Als Programmierer bestimmt man die Reihenfolge des Springens durch die sogenannte „Taborder“.\n\nAuch in Webseiten können Elemente, wie z. B. Links oder Formular-Buttons, mit der Tabulatortaste ausgewählt werden, sofern sie ein codice_1-Attribut besitzen. Sie bekommen dann den Eingabefokus und können dann per Zeilenschalter oder Eingabetaste benutzt werden.\nDas zur Darstellung der Tabulatortaste verwendete Symbol \"↹\" ist in Unicode unter \"21B9\" kodiert. In HTML ist es mit &#8633; darstellbar.\n"}
{"id": "296694", "url": "https://de.wikipedia.org/wiki?curid=296694", "title": "WinRAR", "text": "WinRAR\n\nWinRAR ist ein Packprogramm zur Datenkompression. Es ist eines der wenigen Programme, mit dem sich nativ RAR-Archive erstellen lassen, da es sich um ein proprietäres Format handelt.\n\nWinRAR wird seit 1995 von der WinRAR GmbH aus Berlin entwickelt. Diese gehört nach wie vor mehrheitlich den beiden Gründern Burak Canboy und Öncül Kaya. Der Firma Givanto gehören ca. 12 % der Anteile an der WinRAR GmbH.\n\nWinRAR unterstützt die folgenden Funktionen:\n\nAlle mit Versionen vor 5.31 erstellten selbstentpackenden ausführbaren Archive für Microsoft Windows (inklusive des Installationsprogramms von WinRAR) weisen in Rechteausweitung resultierende Sicherheitslücken auf.\n\nEine Shareware-Version von WinRAR sollte nach einem Testzeitraum von 40 Tagen mittels Kauf eines Registrierungsschlüssels zur Vollversion gemacht werden. Ohne diesen lässt sie sich zwar weiterhin nutzen, regelmäßige Registrierungsaufforderungen weisen aber auf die an und für sich widerrechtliche Weiternutzung hin.\n\n"}
{"id": "298646", "url": "https://de.wikipedia.org/wiki?curid=298646", "title": "Sprite (Computergrafik)", "text": "Sprite (Computergrafik)\n\nEin Sprite (engl. unter anderem für ein Geistwesen, Kobold) ist ein Grafikobjekt, das von der Grafikhardware über das Hintergrundbild bzw. den restlichen Inhalt der Bildschirmanzeige eingeblendet wird. Die Positionierung wird dabei komplett von der Grafikhardware erledigt. Beispielsweise stellen die meisten Grafikkarten ein Hardware-Sprite für den Mauszeiger zur Verfügung.\n\nDer Name rührt daher, dass ein Sprite sich auf dem Bildschirm bewegt, aber im Grafikspeicher nicht zu finden ist, also scheinbar „umherspukt“. Mit der Zeit hat sich der Begriff aber auch auf alle Objekte ausgedehnt, die so aussehen, auch wenn sie softwaremäßig erzeugt werden und im Grafikspeicher vorliegen.\n\nHeute ist die echte Sprite-Technik überholt, vor allem, da Computer inzwischen schnell genug sind, ohne Probleme tausende spriteartige Objekte auf dem Bildschirm darzustellen und zugleich den Hintergrund in ursprünglicher Form wiederherzustellen. Auch der dafür nötige Speicherplatz ist weniger wichtig geworden.\n\nOft wird der Begriff \"Sprite\" auch verallgemeinernd für Objekte benutzt, die per Software (statt Grafikhardware) über den Hintergrund eingeblendet werden. Dies ist jedoch streng genommen ein Shape, der meistens auch als „Software-Sprite“ bezeichnet wird. Gleiches gilt für ein Objekt, das mit Hilfe von Hardware-Routinen in den Grafikspeicher einkopiert wird, ohne jedoch komplett autonom von der Hardware verwaltet zu werden; hier handelt es sich um ein BOB (\"B\"litter \"Ob\"ject) bzw. MOB (\"M\"ovable \"O\"bject \"B\"lock).\n\nIn 3D-Spielen wird der Begriff \"Sprite\" manchmal für flache Objekte mit einer animierten Textur verwendet. Dabei handelt es sich fast um ein Software-Sprite im klassischen Sinne, allerdings wird dieser mittels 3D-Grafikroutinen als flaches 3D-Objekt in die 3D-Szenerie eingefügt.\n\nDas Sprite wird von der Grafikhardware (Grafikprozessor) zum Anzeigezeitpunkt an der gewünschten Position im Bild eingefügt. Dabei wird der Inhalt des Grafikspeichers nicht verändert – im Gegensatz zum MOB bzw. BOB müssen in diesem nicht immer wieder neu Grafikdaten umkopiert werden. Durch diese Entlastung des Hauptprozessors beanspruchen Sprites kaum Systemressourcen und sind gleichzeitig einfach zu programmieren.\n\nDie Grafikdaten für die Sprites werden nach den gleichen Verfahren wie die Grafikdaten für den normalen Bildaufbau zur Verfügung gestellt – als Beispiele seien der C64 sowie der Amiga genannt, wo dies durch DMA-Speicherzugriffe des Grafikprozessors auf spezielle Bereiche des Hauptspeichers geschieht.\n\nDie Steuerdaten für die Sprite-Darstellung (vor allem die Bildschirmposition) werden entweder direkt in Registern der Grafikhardware vorgehalten (Beispiel: C64, Breite und Höhe der Sprites sind fest vorgegeben) oder in speziellen RAM-Bereichen, auf die diese Hardware genügend schnellen Zugriff hat. Ein Beispiel für die letztere Variante ist der Amiga, bei dem die Steuerdaten zusammen mit den Grafikdaten per DMA-Verfahren aus dem Speicher übermittelt werden. Im Gegensatz zum C64 ist die Höhe der Amiga-Sprites nicht begrenzt.\n\nZur Bewegung eines Sprites reicht es aus, lediglich dessen x- und y-Koordinaten in den Steuerregistern zu ändern; die komplizierte Berechnung der Adresse im Grafikspeicher entfällt, was ebenfalls den Hauptprozessor entlastet. Der Grafikprozessor fügt selbständig an den vorgegebenen Koordinaten das Sprite beim Aufbau des nächsten Bildes ein. Auch animierte Sprites sind möglich; dazu müssen nur die Daten der Sprite-Grafik im Speicher durch die des nächsten Einzelbildes ersetzt werden. Ggf. kann auch einfach ein anderer Speicherbereich für das nächste Einzelbild angegeben werden.\n\nEin Sprite ist prinzipiell rechteckig. Aber auch unregelmäßig geformte Sprites sind möglich, indem der Rand transparent dargestellt wird – entweder durch eine bestimmte Farbe, welche vom Grafikprozessor transparent dargestellt wird, oder durch eine Maske, die definiert, an welchen Stellen das Sprite deckend und an welchen es transparent dargestellt werden soll.\n\nDas Aufkommen von Sprites in den 1980er Jahren revolutionierte die Möglichkeiten von Computerspielen, da damals die Prozessorleistung nicht ausreichte, um komplexe Grafikdaten auf dem Bildschirm zu verschieben (siehe auch Heimcomputer, z. B. C64, Amiga). Die Sprites wurden durch Shapes (C16) abgelöst oder ergänzt (C128) bzw. um BOBs (Amiga) ergänzt.\n\nEine weitere Verbesserung für Computerspiele war die Sprite-Sprite-Kollisionserkennung. Sobald der Grafikprozessor beim Erstellen des Bildes erkennt, dass sich zwei Sprites überlappen, wird dies der Software per Interrupt, Flag oder Event mitgeteilt. Damit sind die meisten Computerspiele vergleichsweise einfach zu programmieren, weil sich das Programm nicht um die Kollisionserkennung kümmern muss und für diese keine Rechenleistung verbraucht. Eine vernünftige Kollisionserkennung ist in beinahe allen Grafikprozessoren vorhanden, welche auch BOBs oder Sprites beherrschen.\n\nSprites sind mit Tiles, welche auch Kachelgrafik genannt werden, verwandt. In sehr vielen 2D-Spielen wie Jump' n' Runs werden beide Techniken parallel eingesetzt. Tiles sind ebenfalls kleine Grafikelemente. Aus ihnen wird die Spielewelt zusammengesetzt und sie bilden Wände und Plattformen, verharren also immer auf dem gleichen Platz. Auf diese Weise wird Arbeitsspeicher gespart, da sich wiederholende Muster nur einmal in kleinen Tilegrafiken gespeichert werden müssen, welche entsprechend einer Tilemap angeordnet werden.\n\nDie „Software-Sprites“ oder „\"Shapes\"“, die keine echten Sprites sind, haben in der Regel nur zwei Dinge mit echten Sprites gemeinsam: Erstens sind es Rastergrafiken, die über den Bildschirm bewegt werden, zweitens besitzen sie nicht-rechteckige Umrisse, also eine Maske bzw. transparente Bereiche.\n\nDie Vorteile echter Sprites, wie etwa die geringe Prozessorbelastung oder die automatische Kollisionserkennung, fallen weg. Dafür erhält man als Vorteil, dass die hardwaremäßigen Einschränkungen für die Zahl und die Größe der Sprites wegfallen, wobei diese Grenzen durch die verfügbare Rechenzeit für die Umsetzung in Software neu gesetzt werden.\n\nIm Gegensatz zu echten Sprites muss der Hintergrund, auf dem die Shapes gezeichnet werden, immer wieder nachgezeichnet werden. Am schnellsten geht das, indem man den kompletten Hintergrund der Szene im Speicher zwischenspeichert und immer wieder über das letzte Bild kopiert, allerdings braucht das zusätzlichen Speicherplatz. Speicherplatzschonend wäre, wenn man den Hintergrund immer wieder für jedes Bild neu erstellt, dazu waren frühere Computer aber zu langsam. In Fällen, bei denen sich auch der Hintergrund für jedes Bild ändern kann, fallen diese Betrachtungen natürlich weg. Allenfalls gewisse Optimierungen bei rollenden Szenerien sind denkbar.\n\nEs gibt verschiedene Verfahren für die Speicherung und Anzeige von Softwaresprites. Sie unterscheiden sich in der benötigten Rechenzeit für die Darstellung der Sprites sowie dem zusätzlichen Speicherbedarf für die Kennzeichnung von transparenten Bildpunkten:\n\n\nAlle Verfahren für Softwaresprites haben gemein, dass der Untergrund wiederhergestellt werden muss, da er durch die Softwaresprites zerstört wird.\n\nDie einfachste Methode zur Lösung des Problems ist das Zwischenspeichern des kompletten Hintergrundes oder nur der Stelle, an die das Shape soll, sowie das nachfolgende Zurückkopieren, wenn eine neue Szene gezeichnet werden soll. Der Hintergrund darf sich während dieser Zeit nicht ändern. Für das Sichern und Wiederherstellen von mehreren Shapes über Ausschnitte muss beim Wiederherstellen die umgekehrte Reihenfolge des Sicherns benutzt werden. Würde man dies nicht tun, könnte es zu Artefakten kommen, wenn sich einzelne Shapes überschneiden. Derartige Artefakte sind unter anderem von Mauszeigern auf Fenster-basierten Plattformen bekannt, wenn diese nicht als echte Sprites realisiert wurden und eines der auf dem System laufenden Programme ein Problem hat.\n\nSeit Beginn der Benutzung von Sprites wurde aus Realismusgründen angestrebt, zumindest menschliche Spielfiguren und weitere Lebewesen wie Gegner, Monster etc. weiter zu animieren. Gerade bei der Darstellung eines menschlichen Helden ist es wichtig, dass sich bei der Fortbewegung nach rechts oder links auf einem zweidimensionalen Bildschirm die Beine bewegen. Deshalb besteht eine solche Figur in moderneren Spielen aus mehreren Sprites, die wie bei einem Zeichentrickfilm schnell hintereinander dargestellt werden, um einen fließenden Bewegungseindruck zu erzeugen. Zur Erzeugung dieser Animationssprites wurde in früheren Jahren gerne die Software Deluxe Paint (Amiga) verwendet. Heute gibt es für diesen Zweck Nachfolgeprogramme, wie z. B. Cosmigo Pro Motion.\nZu Beginn der Computerspielezeit gab es daneben auch andere Techniken. So verwendete Atari Basketball Ende der 1970er Jahre den sehr grob dargestellten seitlichen Umriss einer gerade stehenden Person als Spielfigur. Beim „Laufen“ blinkte in regelmäßigen Abständen ein abgestrecktes Bein (Linie) auf, welches an den Figursprite kopiert wurde.\n\nIm Webdesign wird der Begriff \"Sprites\" als Bezeichnung für Grafikdateien verwendet, welche aus mehreren kleinen Einzelgrafiken bestehen. Diese Sprites werden per CSS derart in Webseiten eingebunden, dass vom Webbrowser jeweils die benötigten entsprechend zugeschnittenen Teilgrafiken angezeigt werden. Zweck dieser Technik ist einerseits die Verringerung der Gesamtladezeit einer Webseite durch Reduzierung der Serveranfragen und andererseits das Verhindern von Verzögerungen, die durch Nachladen – beispielsweise bei Hover-Effekten – entstehen würden.\n"}
{"id": "298753", "url": "https://de.wikipedia.org/wiki?curid=298753", "title": "Microsoft Bob", "text": "Microsoft Bob\n\nMicrosoft Bob (kurz MS Bob) ist ein im März 1995 von Microsoft veröffentlichtes Softwarepaket für Windows-Systeme. Das Paket enthält einen Ersatz für die grafische Benutzeroberfläche des Betriebssystems – damals aktuell war Windows 3.1 – und diverse Einzelprogramme für private Anwender. Zielgruppe waren PC-unerfahrene Benutzer, denen der Zugang zum Computer erleichtert werden sollte. Die Benutzeroberfläche verbirgt dazu technische Details und arbeitet mit Analogien aus der häuslichen Umgebung.\n\nDie Einzelprogramme (Finanzberater, Haushaltsmanager, E-Mail-Clienten, GeoSafari Quiz, Schreibprogramm, Kalender, Adressbuch usw.) finden sich als Objekte in den Zimmern eines virtuellen Hauses wieder. Benutzereingaben werden über animierte Figuren (Assistenten) abgewickelt, die auch andere Benutzeraktionen begleiten.\n\nEinige der Microsoft Agent-Figuren, bekannt geworden als Assistenten in Microsofts Office und Windows XP, gehen auf MS Bob zurück, so z. B. \"Fredo\" (Such-Assistent in Windows XP), \"Hüpfer\" (Assistent in Office 97 und 2000) oder \"Peedy\" (Javascript Extra im IE – ab Version 5.1). Auch die Schriftart Comic Sans MS sollte ursprünglich auf Microsoft Bob eingesetzt werden.\n\nVon MS Bob wurden 30.000 Exemplare verkauft. Zum Vergleich: Die Verkaufszahlen des wenige Monate später veröffentlichten Windows-3.1-Nachfolgers Windows 95 beliefen sich während der ersten drei Monate auf etwa 45 Millionen.\n\nMS Bob wurde unter Fachleuten kritisiert, da es den Lernprozess von Computeranfängern erschwert; durch die einfache Oberfläche würden die Anwender nie die Benutzung der normalen Windows-Oberfläche lernen.\n\nKeinen Schutz vor unerwünschten Zugriffen bietet die Kennwortabfrage beim Loginvorgang von MS Bob: Hat ein Anwender sein Kennwort mehrfach falsch eingegeben, kann er ohne Sicherheitsabfragen ein neues Kennwort vergeben.\n\nDie Software ist auch unter Windows Vista und Windows 7 lauffähig.\n\nProjektmanagerin bei der Entwicklung war Melinda Gates, Ehefrau von Bill Gates.\n\n"}
{"id": "300186", "url": "https://de.wikipedia.org/wiki?curid=300186", "title": "Lotus SmartSuite", "text": "Lotus SmartSuite\n\nDie Lotus SmartSuite ist ein Office-Paket des Herstellers IBM (ursprünglicher Hersteller Lotus) für die Betriebssysteme Windows und OS/2. Das letzte Fixpack für SmartSuite 9.8.5 erschien im Jahr 2008. \"Smartsuite\" 9.8x und \"Organizer\" 5.x bis 6.1 sind jedoch unter Windows Vista und Windows 7 verwendbar. Dennoch wurden die Anwendungen noch bis November 2013 vermarktet. Der erweiterte Support für registrierte Kunden endete im September 2014.\n\nDie Anwendungen der Programmsammlung in den 9.8.x-Versionen können Microsoftformate lesen und schreiben, wobei hinsichtlich Layout und Zeilen- bzw. Seitenumbrüchen keine DTP-ähnliche Originaltreue erwartet werden darf. Bei früheren Versionen (bis ca. 1999/2000) beschränkt sich eine rudimentäre Interoperabilität auf die Verwendung von Austauschformaten wie beispielsweise das RTF-Format. Microsoft unterstützt die Lotusformate kaum, Microsoft Office kann lediglich WKS- und WK1/WK3/WK4-Dateien einiger älterer 1-2-3-Programmversionen öffnen.\n\nDie Sammlung der Büroprogramme von Lotus Development ist besonders gut für die Teamarbeit geeignet und kann zusammen mit Lotus Notes eingesetzt werden. Die objektorientierte Programmiersprache Lotusscript steht in den meisten Anwendungen bereit.\n\nDiese Zusammenstellung enthält meist folgende Programme:\n\nDie Programme entsprechen im Wesentlichen den Programmen des Marktführers Microsoft, wobei Organizer und Fastsite bereits zur Standardausstattung gehören. Hinzu kommen die Dienstprogramme \"ScreenCam\" und \"SmartCenter\". ScreenCam zeichnet alle Bildschirmaktivitäten in Echtzeit auf und ermöglicht so in Verbindung mit einem angeschlossenen Mikrofon die einfache Erstellung von Filmen zu Demonstrationszwecken oder für Tutorials u. a. im Wav-Format. Das SmartCenter, welches früher noch \"Lotus Application Manager (LAM)\" hieß, ist das Pendant zur Microsoft-Office-Startleiste, hat aber mit seiner Schreibtischmetapher und mit Mini-Anwendungen, die üblicherweise auf einem Schreibtisch zu finden sind, einen erweiterten Funktionsumfang. Die einzelnen Kategorien werden am oberen Bildrand so angeordnet, dass sich die einzelnen Kategorien ähnlich einer Schublade nach unten öffnen und diese geöffneten Leisten je nach Kategorien weitere karteikartenähnlich angeordnete Inhalte anbieten. So lassen sich beispielsweise Inhalte z. B. aus Organizer, Adressbuch, Notizzetteln im Vordergrund verwalten und von dort die übrigen Anwendungen starten, beispielsweise durch einen Klick im Adressbuch eine E-Mail verfassen.\n\nOb es sich bei Lotus Symphony, das seit Mai 2008 in einer stabilen Version vorliegt, um das Nachfolgeprodukt handelt, ist unklar. Symphony basiert auf dem Code von OpenOffice.org und ist entwicklungsgeschichtlich nicht mit SmartSuite vergleichbar.\n\n\n\nUnterstützte Sprachen sind Englisch, Dänisch, Deutsch, Spanisch, Französisch, Italienisch, Niederländisch und Portugiesisch.\n\n\nHinweis: alle Quellen der IBM-Seiten sind, wie alle Produktinformationen, die über eine Basisinformation hinausgehen, englischsprachig.\n"}
{"id": "301831", "url": "https://de.wikipedia.org/wiki?curid=301831", "title": "Gadu-Gadu", "text": "Gadu-Gadu\n\nGG (früher \"Gadu-Gadu\", poln. für „Laber-Laber“ oder „Plauschen“) ist ein kommerzieller Instant-Messaging-Dienst. Mit über 5,4 Millionen täglich aktiven Nutzern war er bis 2012 der in Polen am weitesten verbreitete Onlinedienst für Nachrichtensofortversand, hat seit 2016 aufgrund des zunehmenden Erfolgs von Skype oder Facebook in seinem Heimatmarkt an Popularität eingebüßt.\n\nDie Identifizierung der Nutzer erfolgt, wie beispielsweise bei \"ICQ\" auch, über laufende Nummern. Die offizielle Version des Programms ist recht schlank, kann aber mit zahlreich verfügbaren Add-ons ausgebaut werden. Ein Komplettpaket stellt \"PowerGG\" dar. Eine Alternative zu dem offiziellen Client sind \"Konnekt\", \"Miranda IM\", \"Miranda NG\", \"AQQ\", \"Pidgin\", \"Kopete\", \"Adium\" und andere, die mit einem GG-Plug-in ausgeliefert werden.\n\nDie offizielle Version unterstützt Instant Messaging (mit über 190 Smileys), Offlinenachrichten, Datenversand und IP-Telefonie. Zudem gibt es die Möglichkeit einer gesicherten Verbindung per SSL. Ab Version 7 verfügt das Programm auch über die Funktion der Videokommunikation. Das kostenlos nutzbare Programm blendet während seiner Benutzung Werbung ein.\n\nGG ist der erste \"Instant Messenger\" weltweit, der für eine öffentliche Verbindung ins Weltall, auf die Erdumlaufbahn, genutzt wurde. Es handelte sich dabei um eine Verbindung zwischen polnischen GG-Benutzern und den Astronauten der Internationalen Weltraumstation, die zum 30. Jahrestag des ersten Weltraumflugs eines Polen, Mirosław Hermaszewski, hergestellt wurde.\n\n"}
{"id": "302468", "url": "https://de.wikipedia.org/wiki?curid=302468", "title": "Alpha Blending", "text": "Alpha Blending\n\nAlpha Blending ist eine Technik in der Bild- oder Videobearbeitung, bei der verschiedene Bilder zu einem Gesamtbild überlagert werden, wobei neben der Farbinformation auch der Alphakanal berücksichtigt wird.\n\nDer Alphawert eines Bildpunktes (Pixel) ist dabei ein Maß für die Transparenz bzw. Opazität. Ein α-Wert von 0 steht dabei für völlige Transparenz, das Pixel wird unsichtbar. Ein α von 1 steht für völlige Lichtundurchlässigkeit, das Pixel überlagert alle Bildpunkte hinter ihm.\n\nHat man zwei Farben A und B gegeben und möchte A über B legen, benutzt man folgende Gleichung, um den neuen Wert für die nicht transparente Endfarbe C zu erhalten:\n\nformula_1.\n\nEinen Ausdruck der Form formula_2 nennt man auch Konvexkombination.\n\nSo ist es möglich einen RGBA- in einen RGB-Wert zu konvertieren, ohne den α-Kanal einfach abzuschneiden.\n\nBeispiel: Ein transparentes Lila (0.5,0,1,0.75) auf weißem Grund (1,1,1) ergibt dann (0.625,0.25,1).\n\nZum Überblenden zweier transparenter Farben ist dagegen der Porter-Duff-Algorithmus geeignet. Dabei berechnet sich die Endfarbe C zu\n\nformula_3,\n\nund die Transparenz der Endfarbe bestimmt sich durch\n\nformula_4\n\nDies entspricht der A over B Operation im nebenstehenden Bild. Dieser Algorithmus wird in den PDF und SVG Formaten benutzt.\n\nBeispiel: Das Überblenden eines transparenten Blau (0,0,1,0.5) über ein transparentes Rot (1,0,0,0.5) ergibt ein transparentes Lila (formula_5,0,formula_6,0.75).\n\nIn einigen Rastergrafikformaten kann α nur die Werte 0 oder 1 annehmen. In diesem Fall ist ein Bildpunkt also entweder vollständig unsichtbar oder vollständig deckend. Das GIF-Format ist ein Beispiel hierfür.\nNeben dieser Art der Berechnung kann man die Farben A und B noch auf andere Weise zusammenfügen. Damit werden die unterschiedlichsten Effekte erzielt. Man könnte z. B. die Differenz der Farbwerte bilden, A nur dort einblenden, wo der α-Wert von B nicht 0 ist usw.\n\nEine Hauptanwendung des Alpha Blending ist die Bildkomposition: Wie bei einer Fotomontage soll ein Objekt aus einem Bild ausgeschnitten und in ein neues Bild eingefügt werden. Dazu muss der Alphakanal des Bildes mit dem auszuschneidenden Bild entsprechend editiert werden. Schneidet man das Objekt mit herkömmlichen Methoden aus, kann die Transparenz nicht berücksichtigt werden. Stattdessen setzt man für jedes Pixel einen entsprechenden Alphawert. Stellt man den Alphakanal grafisch dar, indem man die Werte als Graustufen zwischen Schwarz und Weiß interpretiert, erhält man die sogenannte Alpha-Matte. Das Erstellen dieser Maske nennt man \"Pulling the Matte\" oder Matting. Bis vor kurzem musste man die Matte größtenteils von Hand erstellen, da keine leistungsfähigen Algorithmen mit befriedigenden Ergebnissen zur Verfügung standen. Die SIGGRAPH-Konferenzen der Jahre 2004 und 2005 präsentierten allerdings vielversprechende Ansätze wie das Bayesian Matting und das Poisson Matting.\n\nDie meisten Bildbearbeitungsprogramme wie GIMP oder Photoshop unterstützen die separate Bearbeitung des Alphakanals über Ebenenmasken. So kann man mit den bekannten Malwerkzeugen direkt die Transparenz eines Objektes bearbeiten.\n\nAlpha Blending wird in Computerspielen und anderen Grafikanwendungen benutzt, um die Transparenzeigenschaften von Objekten zu modellieren. So würde eine getönte Fensterscheibe einfach als farbige Fläche mit geringem α gespeichert und dargestellt. Dazu wird zunächst das erste vollkommen undurchsichtige Objekt im Sichtbereich gesucht. Ab diesem wird nun die oben vorgestellte Konvexkombination sukzessive mit den Farbwerten der weiteren Objekte gebildet. Bei High Color steht maximal 1 bit für den Alphakanal zur Verfügung, bei True Color sind es 8 bit.\n\n"}
{"id": "302855", "url": "https://de.wikipedia.org/wiki?curid=302855", "title": "Remote Desktop Protocol", "text": "Remote Desktop Protocol\n\nDas Remote Desktop Protocol (RDP) ist ein proprietäres Netzwerkprotokoll von Microsoft für den Fernzugriff auf Windows-Computer. Es ermöglicht das Darstellen und Steuern des Bildschirminhalts eines entfernten Computers. Das Remote Desktop Protocol regelt, wie die Terminaldienste (\"Remote Desktop Services\", vormals \"Terminal Services\") unter Microsoft Windows NT angesprochen und genutzt werden. Der Dienst wird normalerweise auf Port TCP 3389 bereitgestellt. Ab der Version 8.0 des Remote Desktop Protokolls wird zusätzlich UDP 3389 verwendet.\n\nMicrosoft lizenzierte von Citrix die Technik \"Multiwin\", um im Jahr 1998 das darauf aufbauende Produkt \"Windows Terminal Server\" zu veröffentlichen. Multiwin ermöglichte die gleichzeitige Ausführung mehrerer Benutzersitzungen.\n\nDas Protokoll ICA, das Citrix beim eigenen Produkt \"WinFrame\" verwendete, war nicht Bestandteil der Lizenzierungsvereinbarung. Stattdessen entwickelte Microsoft das Protokoll RDP als Erweiterung des Protokolls \"T.Share\" (\"T.Share\" war die Bezeichnung für das Protokoll T.128 in dessen Entwurfsphase).\n\nBei RDP fungiert eines der beiden Systeme als Terminalserver. Dieser erzeugt Bildschirmausgaben auf dem Terminalclient. Außerdem können Maus- und Tastatureingaben vom Terminalclient entgegengenommen werden. Es gibt zwei Möglichkeiten: Die Fernausgabe auf dem Terminalclient kann entweder die einzige Ausgabe sein, die der Terminalserver für diese Sitzung erzeugt, oder aber die eigentliche Bildschirmausgabe der Sitzung erfolgt auf einem lokalen Bildschirm des Terminalservers und der Terminalclient erhält lediglich eine Kopie der Ausgabe.\nNeben Bildschirmausgaben sowie Tastatur- und Maus-Eingaben kann mit RDP auch die Tonausgabe der Sitzung zum Terminalclient umgeleitet werden. Außerdem ist die Nutzung eines Druckers und der Zugriff auf Speichermedien des Terminalclients möglich.\n\nJe nach Einsatzzweck wird der Benutzer des Terminalclients dadurch in die Lage versetzt, den Arbeitsplatz seines Terminalservers zu „beobachten“ oder sogar aus der Ferne zu steuern. RDP regelt die Übertragung der Bildschirminhalte sowie Tastatur- und Mauseingaben über das Netzwerk.\n\nRDP basiert auf dem ITU-Protokoll T.128 und ist ein Protokoll der Ebenen 4–7 des OSI-Modell. Es ist nicht abhängig vom Transmission Control Protocol oder einem anderen Protokoll der unteren Ebenen.\n\nAuf Grund einer Designschwäche dieses Protokolls in Versionen vor 6.1 ist es möglich, dass Mitarbeiter in einem Netzwerk via ARP-Spoofing an sensible Daten gelangen. Allerdings besteht die Möglichkeit, RDP-Verbindungen mittels Transport Layer Security (TLS) zusätzlich abzusichern, womit eine sichere Authentifizierung gewährleistet ist.\n\nAls Server für RDP werden Windows NT 4.0, Windows Server 2000, 2003, 2008, 2012, 2016 sowie 2019, NetMeeting, Windows XP, Windows Vista, Windows 7, Windows 8 und Windows 10 verwendet. Terminalserver (RDP) für Windows von Drittanbietern sind z. B. Thinstuff XP/VS Server, Thinsoft Winconnect Server und AADS Terminal Server. Clients wie mstsc.exe existieren für fast alle Betriebssysteme. RDP wird seit Windows XP standardmäßig für die Fernwartung von Windows-Rechnern (Remoteunterstützung) verwendet.\n\nSeit Windows XP Service-Pack 1 ist die RDP-Version 5.1 verfügbar. Remote Desktop Protocol 5.2 ist eine Komponente von Windows XP Professional SP2.\n\nMit der Einführung von RDP Version 6.0, welches ein Bestandteil von Windows Vista ist, wurde der Funktionsumfang zum Teil erheblich verändert sowie die Verschlüsselung überarbeitet. Unter anderem wurden\neingeführt.\n\nSeit Oktober 2009 ist die RDP-Version 7.0 für Windows XP SP3, Windows Vista SP1 und Windows Vista SP2 verfügbar. Diese Version fügt über 10 neue Funktionen hinzu, welche bei Verbindungen zu Windows 7 bzw. Windows Server 2008 R2 verfügbar sind. Davon stehen 4 Funktionen ab Windows 7 auf dem Client-PC zur Verfügung. Aktuelle Client-Programme sind für Windows XP, Windows Server 2003, Windows Vista, Windows 7 und Mac OS X verfügbar. Die maximale Auflösung pro Monitor bleibt unverändert, darf sich aber jetzt zu 32766 × 32766 Pixel aufsummieren.\n\nDiese Version erschien zusammen mit Windows 8 und Windows Server 2012. Ende Oktober 2012 hat Microsoft die Remote Desktop Protocol 8.0 Updates für Windows 7 SP1 und Windows Server 2008 R2 veröffentlicht (auch via Windowsupdate und MS download center (KB2592687) und (KB2574819)), das dort die Nutzung der neuen Funktionen beim Zugriff auf einen Windows Server 2012 sowie auf ein Windows 8 (pro, enterprise) erlaubt. Die Remote Desktop Anwendung (mstsc.exe) wurde dabei auf Version 6.2.9200 aktualisiert und unterstützt damit die Remote Desktop Version 8.0. Mit dem Update KB2923545 wurde die Version auf 8.1 erhöht. Die maximale Auflösung pro Monitor erhöht sich auf 8192 × 8192 Pixel.\n\nDiese Version ist erstmals im Windows 10 1511 Update sowie in der Windows Server 2016 Technical Preview 4 enthalten beinhaltet neue Features wie AutoSize Zoom sowie Verbesserungen der Grafikkompression mit Hilfe von H.264/AVC.\n\nFür Linux, FreeBSD, macOS, AmigaOS, MorphOS, Android, Apple iOS und Chrome OS existieren jeweils Clients, die den Zugriff auf Windows-RDP-Server erlauben. Ebenso gibt es kommerzielle Java-Clients, die auf unterschiedlichen Betriebssystemen einsetzbar sind.\n\nAuf der CeBIT wurde im Jahr 2000 der erste RDP Java-Client (HOBLink JWT) präsentiert. Damit wurde mit Hilfe der Java-Plattform der Zugriff auf RDP-Server unter zahlreichen Betriebssystemen ermöglicht.\n\nAuf der Messe CeBIT wurde im März 2005 der erste kommerzielle RDP-Server für Linux präsentiert (Thinstuff LX Server). Diese Software ermöglicht den Einsatz von RDP-Clients zur Verbindung zu X11-Servern.\n\nEs ist auch ein freier RDP-Server mit dem Namen xrdp verfügbar.\n\nDie Virtualisierungssoftware VirtualBox von Oracle besitzt einen eigenen RDP-Server, der mit rdesktop und dem Microsoft Client kompatibel ist.\n\n\n"}
{"id": "303347", "url": "https://de.wikipedia.org/wiki?curid=303347", "title": "Alphakanal", "text": "Alphakanal\n\nDer Alphakanal oder α-Kanal ist ein zusätzlicher Kanal, der in Rastergrafiken zusätzlich zu den Farbinformationen die Transparenz (Durchsichtigkeit) der einzelnen Pixel (Bildpunkte) speichert. Die Darstellung eines Bildes mit Alphakanal auf einem Hintergrund wird als Alpha Blending bezeichnet.\n\nIn der Filmtechnik waren bereits Jahrzehnte vor der Erfindung des Computers sogenannte \"Mattes\" gebräuchlich – Filmstreifen, die einzelne Objekte darstellten und die über einen Hintergrund gelegt werden konnten (Compositing). Mit der Einführung digitaler Filmtechniken wurden auch digitale Mattes entwickelt.\n\nDer Begriff \"integral alpha\" wurde 1977 von Alvy Ray Smith und Edwin Catmull für die Idee eingeführt, dass Transparenzinformationen nicht in einer separaten digitalen Matte, sondern zusammen mit den Pixeln gespeichert werden sollten, um den Prozess zu vereinfachen. Dieser zusätzliche Bildkanal wurde „Alphakanal“ (\"alpha channel\"), die entsprechenden Pixel \"„RGBA“-Pixel\" genannt. Die Bezeichnung „Alpha“ bezieht sich auf die Variable formula_1 in der klassischen Formel zur linearen Interpolation \n\nwie sie beim Alpha Blending verwendet wird. Durch die Einführung des Alphakanals wurden digitale Mattes überflüssig. 1984 entwickelten Thomas Porter und Tom Duff verfeinerte Alpha-Blending-Techniken (siehe auch Porter-Duff Composition).\n\nFür ihre Verdienste bei der Entwicklung digitaler Compositing-Techniken erhielten Catmull, Porter, Duff und Smith 1996 einen Engineering Award der Academy of Motion Picture Arts and Sciences.\n\nIm Alphakanal werden bei verschiedenen Grafikformaten (z. B. PNG, PSD, TGA, DDS oder TIFF) Transparenzinformationen zusätzlich zu den eigentlichen Bilddaten gespeichert. Dabei besitzt ein Alphakanal meist dieselbe Farbtiefe wie ein Farbkanal eines Bildes. So umfasst ein Alphakanal bei einem 8-Bit-Bild 256 Stufen. Dem Wert Alpha=0 entspricht das Attribut „vollständig transparent“, d. h. unsichtbar. Alpha=255 entspricht „nicht transparent“. \n\nDie maximale Anzahl der möglichen Transparenzabstufungen richtet sich nach der Anzahl der für den Alphakanal verwendeten Bits. Ein \"binärer Alphakanal\" ist ein minimaler Alphakanal, der 1 Bit verwendet und daher nur angeben kann, ob ein Bildpunkt entweder vollständig transparent oder vollständig opak ist. Im Ergebnis der Benutzung eines binären Alphakanals vergleichbar, kann auch eine Farbe eines Bildes als transparent definiert werden. Diese Art Transparenz wird im Graphics Interchange Format (GIF) verwendet, wodurch ein Farbton weniger zur Darstellung verwendet werden kann. Sie ist jedoch kein Alphakanal im technischen Sinne, da die Transparenzinformationen nicht für jeden Bildpunkt einzeln gespeichert werden. Andere Formate erlauben oft ein zusätzliches Byte pro Pixel und somit 2 = 256 Abstufungen.\nIm Gegensatz zum GIF kann im PNG-Dateiformat ein 8- oder 16-Bit-Alphakanal benutzt werden. Bei diesem „echten“ Alphakanal spricht man dann auch von Grafiken mit vier Kanälen, die oft als RGBA abgekürzt werden (Rot, Grün, Blau, Alpha).\n\nAlphakanäle können auf drei verschiedene Arten gespeichert werden:\n\n\n\n\nAlphakanäle können in diversen Bildbearbeitungsprogrammen definiert werden. Dabei können etwa auch Auswahlen im Bild für die spätere Wiederverwendung gespeichert werden. In Photoshop werden temporäre Alphakanäle gebildet, wenn eine Ebene maskiert wird. Alphakanäle können auch zum Freistellen eines Bildes verwendet werden. Dies geschieht oft, indem ein bestehender Farbkanal zu einem Alphakanal kopiert wird und dann mit den Bildbearbeitungsfunktionen (Kurven, Kontraste, Pinsel usw.) rasch bearbeitet werden kann.\n\nBei GIMP lässt sich der Alphakanal über die im Kontextmenü vorhandene Funktion „Auswahl aus Alphakanal“ auch für das Freistellen (auf anderen Ebenen oder in anderen Bildern) verwenden.\n\nEin Alphakanal kann auch für Videos verwendet werden, um Objekte vom Hintergrund zu trennen. Dabei kann der Alphakanal direkt mit dem Video oder in einer separaten Videodatei gespeichert werden.\n\n"}
{"id": "303461", "url": "https://de.wikipedia.org/wiki?curid=303461", "title": "Direkte Numerische Simulation", "text": "Direkte Numerische Simulation\n\n<onlyinclude>Unter Direkter Numerischer Simulation, kurz DNS, versteht man eine Berechnungsmethode der Strömungsmechanik zur rechnerischen Lösung der vollständigen instationären Navier-Stokes-Gleichungen. Sie unterscheidet sich von anderen Berechnungsmethoden der Strömungsmechanik dadurch, dass kleinskalige turbulente Schwankungen numerisch in Raum und Zeit aufgelöst und nicht durch Turbulenzmodelle dargestellt werden.\n</onlyinclude>\nAufgrund der zeitlich variierenden, räumlich kleinen Schwankungen in einer turbulenten Strömung ist neben der hohen räumlichen Auflösung eine instationäre Betrachtungsweise für die detaillierte Beschreibung der physikalischen Vorgänge erforderlich. Die DNS ist damit die genaueste Methode, Strömungen zu berechnen; sie stellt allerdings auch die höchsten Anforderungen an das numerische Verfahren sowie an die zur Verfügung stehende Rechenleistung. Deshalb findet die DNS hauptsächlich in der Grundlagenforschung Verwendung. Ein großer Anwendungsbereich ist der laminar-turbulente Umschlag, unter dem man den Übergang von einer glatten stationären Strömung in den quasichaotischen turbulenten Zustand versteht. Des Weiteren wird die DNS in den Bereichen Ablöseblasen, vollturbulente Strömungen und Aeroakustik eingesetzt und dient auch der Weiterentwicklung von Turbulenzmodellen.\n\nStrömungsfelder verhalten sich gemäß der Navier-Stokes-Gleichungen. Für niedrige Geschwindigkeiten bei einer Mach-Zahl kleiner als 0,3 reicht es in der Regel die Navier-Stokes-Gleichungen für inkompressible Fluide anzuwenden. Sie bestehen im Wesentlichen aus drei Transportgleichungen, den Impulsgleichungen in den drei Raumrichtungen. Aus der Massenerhaltung ergibt sich für inkompressible Strömungen die Divergenzfreiheit der Geschwindigkeit. Mit zunehmender Geschwindigkeit spielen Kompressibilitätseffekte zunehmend eine Rolle, so dass die Navier-Stokes-Gleichungen für kompressible Fluide anzuwenden sind. Diese sind auch erforderlich, wenn zum Beispiel aeroakustische Phänomene untersucht werden sollen, da sich nur bei Berücksichtigung der Kompressibilität eine endliche Schallgeschwindigkeit ergibt. Da die Navier-Stokes-Gleichungen für kompressible Strömungen durchgehend Transportgleichungen sind, gibt es im Gegensatz zu den Gleichungen für inkompressible Fluide keine elliptischen Terme. Je nach Anwendungsfall können zusätzliche Gleichungen mit einbezogen werden. So spielen etwa im Hyperschall bei hoher Mach-Zahl chemische Reaktionen wie Dissoziation von Molekülen oder Ionisation von Atomen eine Rolle.\n\nDa in einer DNS das Strömungsfeld ohne Turbulenzmodell berechnet werden soll, lassen sich die vom Verfahren zu erfassenden strömungsmechanischen Vorgänge am anschaulichsten anhand des laminar-turbulenten Umschlags beschreiben. Unter dem Begriff Umschlag, auch Transition genannt, versteht man hierbei den Übergang von einer ursprünglich laminaren stationären Strömung zu einem turbulenten Zustand, der von kleinskaligen nahezu chaotischen Schwankungen bestimmt ist. Die Ursache hierfür ist eine Instabilität der Strömung gegenüber kleinen Störungen, die abhängig vom jeweiligen Strömungszustand ab einer gewissen Reynolds-Zahl existiert. Bereits mit Hilfe der Linearen Stabilitätstheorie lassen sich die Frequenzen der angefachten Störungen und deren Anfachungsraten sehr gut für ein gegebenes stationäres Strömungsfeld vorhersagen. Für eine inkompressible Plattengrenzschicht ohne Druckgradient ergeben sich z. B. angefachte Störungen ab einer Reynolds-Zahl von Re⋅x ≈ 91.000, wobei x die Koordinate in Strömungsrichtung mit ihrem Ursprung an der Plattenvorderkante darstellt. Anstelle der Stromabkoordinate wird häufig die lokale Verdrängungsdicke δ der Grenzschicht verwendet. Damit ergibt sich eine kritische Reynolds-Zahl von Reδ=520, ab der das erste Mal angefachte Störungen existieren. Diese angefachten Störungen, Tollmien-Schlichting-Wellen genannt, wachsen in Stromabrichtung exponentiell an. Erreichen die Instabilitätswellen eine merkliche Amplitude, sind sie nicht mehr unabhängig voneinander zu betrachten, sondern wechselwirken miteinander. In den Navier-Stokes-Gleichungen ergibt sich dies aus den nichtlinearen Termen. Unterteilt man eine Größe \"U\" in ihren stationären Anteil u sowie den instationären Anteil \"u\", so ergibt sich aus einem nichtlinearen Term\nNimmt man für den instationären Anteil eine Welle der Form\nmit der Wellenzahl α, so ergibt sich aus dem Produkt von \"u\" mit sich selbst\ndas heißt, es werden Wellen mit der doppelten Wellenzahl generiert. Die nichtlineare Generierung bewirkt ein Anwachsen auch von eigentlich linear gedämpften Störungen. Zunehmend kurzwelligere Störungen erreichen nichtlineare Amplituden und ziehen Energie aus den gröberen Skalen ab. Da die zweite Raumableitung einer Welle proportional zum Quadrat der Wellenzahl ist, wirkt für kurzwelligere Störungen jedoch zunehmend die Viskosität der Generierung entgegen. \n\nJe nach eingebrachten Störungen ergeben sich unterschiedliche Wirbelstrukturen im nichtlinearen Bereich. So wird bei der Grenzschicht zwischen K-Typ (nach Philip Klebanoff), H-Typ (nach Thorwald Herbert) und Oblique (englisch für schief) unterschieden. In den Spätstadien der Transition brechen die großen Wirbelstrukturen auf und es ergibt sich ein chaotisches Bild aus kleinen Wirbeln. Zweidimensionale DNS sind nur in Ausnahmefällen sinnvoll. Generell gilt, dass eine 2D-Turbulenz nicht existiert. Deshalb ist es für eine DNS im Allgemeinen notwendig, die vollen dreidimensionalen Navier-Stokes-Gleichungen zu simulieren, was einen entsprechend hohen Rechenaufwand bedeutet.\nVon einer DNS spricht man, wenn alle relevanten Skalen räumlich und zeitlich aufgelöst werden. Dies bedeutet, dass Wellenzahlen soweit sauber abgebildet werden müssen, bis nur mehr der Einfluss der Viskosität der nichtlinearen Generierung entgegenwirkt. Andere Methoden der numerischen Strömungsmechanik betrachten die instationären Vorgänge der Turbulenz entweder eingeschränkt oder überhaupt nicht. Bei der Large-Eddy-Simulation (LES, im deutschen auch Grobstruktursimulation) beschränkt man sich auf die groben turbulenten Skalen, indem man räumlich gefilterte Strömungsgrößen betrachtet. Der Einfluss der nicht mehr aufgelösten Skalen wird durch sogenannte Subgridscale-Modelle abgebildet. Da diese Modelle im Wesentlichen künstliche Viskosität einbringen, kann eine unteraufgelöste DNS auch als LES angesehen werden.\n\nIm Gegenzug dazu werden bei RANS-Rechnungen (englisch für Reynolds-gemittelte Navier-Stokes-Gleichungen) keinerlei turbulente Schwankungen aufgelöst, sondern vollständig durch Turbulenzmodelle modelliert. Mit den zeitlich gemittelten Navier-Stokes-Gleichungen werden auch andere Gleichungen als bei DNS und LES gelöst, weshalb ein RANS-Ergebnis auch bei beliebig feiner Auflösung nicht gegen die exakte Lösung der direkten numerischen Simulation konvergiert.\n\nAufgabe eines numerischen Verfahrens zur Direkten Numerischen Simulation ist es somit, instationäre Wellenprozesse ausreichend fein in Raum und Zeit aufzulösen. Außerdem ist auf passende Randbedingungen zu achten, um ein nicht unnötig großes Rechengebiet zu benötigen sowie um Reflexionen an den Rändern zu vermeiden. Typischerweise sind zusätzlich Störungen einzuleiten. Grundsätzlich sind zwar Störungen bereits immanent im Verfahren enthalten (z. B. Diskretisierungsfehler, Näherungen in den Anfangs- und Randbedingungen oder Rundungsfehler), jedoch ist das Ergebnis dann abhängig vom verwendeten Rechengitter und für Rundungsfehler sogar vom verwendeten Rechner. Außerdem möchte man bei einer DNS den Einfluss von unterschiedlichen Störungen untersuchen.\n\nAufgrund des hohen Rechenaufwandes beschränkt man sich bei der DNS üblicherweise auf periodische Randbedingungen in der dritten Raumrichtung. Für die Grenzschicht an einer ebenen Platte bedeutet dies, dass von einer unendlichen Ausdehnung in Quer- beziehungsweise Spannweitenrichtung ausgegangen wird. Bei rotationssymmetrischen Körpern entspricht die Spannweitenrichtung dann der Umfangsrichtung. Das nebenstehende Bild zeigt beispielhaft ein Integrationsgebiet für die Plattengrenzschicht.\n\nDa der Erfolg einer DNS wesentlich von der Fähigkeit abhängt, Wellen ausreichend exakt zu transportieren,\nsind entsprechende Anforderungen an die Numerik zu stellen. Bei der Raumdiskretisierung sind gute\nDispersions- und Dissipationseigenschaften zu erreichen, weshalb fast ausschließlich Verfahren hoher Ordnung verwendet werden.\nFür die spannweitige Richtung ist aufgrund der Periodizität ein Spektralansatz geeignet, da dieser ideal für den Wellentransport ist. In nicht-periodischen Raumrichtungen ist ein spektraler Ansatz zwar möglich, jedoch sind Diskretisierungsmethoden, die nicht auf periodischen Randbedingungen basieren, praktikabler. Beispiele dafür sind Finite Differenzen oder Finite Volumen. Häufig werden in DNS-Codes Finite Differenzen verwendet, da sie auf strukturierten Gittern die schnellste Methode zur Berechnung von Raumableitungen hoher Ordnung sind.\n\nAufgrund der stetigen Generierung von höherharmonischen Wellen ist es für die Stabilität einer Rechnung erforderlich, kurzwellige Störungen zu entfernen. Das Problem der kurzwelligen Anteile ist, dass auf einem gegebenen Gitter nur Wellen bis zu einer bestimmten Wellenzahl aufgelöst werden können. Werden jedoch Wellen mit Wellenzahlen oberhalb dieser Grenze generiert, so werden diese zu gewissen Anteilen auf niedrigere Wellenzahlen abgebildet (sogenanntes Aliasing), was im Allgemeinen zum Absturz des Verfahrens führt. Auch für die Wellentransporteigenschaften jeder räumlichen Diskretisierung existiert eine Aliasing-Grenze. Für Wellenzahlen oberhalb dieser Aliasing-Grenze ergibt sich eine negative Gruppengeschwindigkeit, das heißt diese instationären Anteile laufen entgegen der physikalisch richtigen Transportrichtung. Ab welcher Wellenzahl dies auftritt, hängt von der Art und der Ordnung des Verfahrens ab. Mit zunehmender Ordnung wandert die Aliasing-Grenze zu höheren Wellenzahlen. Prinzipiell ist ein Entfernen der kurzwelligen Anteile auch durch eine entsprechend hohe räumliche Auflösung möglich. In diesem Fall sorgt die Viskosität für eine Dämpfung dieser Anteile. Dies ist jedoch nicht praktikabel, da dies eine allzu große räumliche Auflösung erfordert. Gängige Methoden zum Dealiasing sind räumliche Filterung mit einem Tiefpassfilter, Upwinding oder eine abwechselnde Gewichtung der Diskretisierung. Bei einer spektralen Diskretisierung sind die Anteile in Abhängigkeit von der Wellenzahl gegeben. Dealiasing erfolgt dann einfach durch Nullsetzung hoher Wellenzahlen (üblicherweise ab 2/3 der maximalen Wellenzahl.)\n\nAus den Navier-Stokes-Gleichungen ergibt sich mittels der Raumdiskretisierung die Zeitableitung der einzelnen Größen.\nPrinzipiell kann die Zeitintegration mit expliziten oder impliziten Verfahren durchgeführt werden. Bei einer expliziten Zeitintegration ist das Zeitschrittlimit für eine stabile Simulation einzuhalten. Je nachdem, ob konvektive oder viskose Terme überwiegen, skaliert der Zeitschritt proportional oder quadratisch mit der räumlichen Auflösung. Häufig wird das Runge-Kutta-Verfahren 4. Ordnung für die Zeitintegration angewendet, da es hervorragende Eigenschaften bezüglich Genauigkeit und Stabilität aufweist. Implizite Verfahren haben zwar den Vorteil, (zumindest prinzipiell) kein Zeitschrittlimit zu besitzen, jedoch erfordern sie einen deutlich höheren Rechenaufwand pro Zeitschritt. Wählt man den Zeitschritt so grob, so dass sich ein implizites im Vergleich zu einem expliziten Verfahren lohnt, so dämpft die implizite Integration nahezu alle Instabilitätswellen weg. Aus diesem Grund werden implizite Integrationsverfahren bei DNS-Codes in der Regel nicht angewendet. Eine Ausnahme davon stellen kompressible Rechnungen bei niedriger Mach-Zahl dar, bei denen der Zeitschritt eines expliziten Verfahrens sehr fein sein muss.\n\nAufgrund des hohen Rechenaufwandes ist es meist unerlässlich, auf mehreren Prozessoren zu rechnen. Da DNS-Codes typischerweise auf strukturierten Gittern arbeiten und sich damit gut vektorisieren lassen, kann auf Vektorrechnern (z. B. Earth Simulator oder NEC-SX8) eine hohe Rechenleistung erreicht werden, die teilweise über 50 % der theoretischen Rechenleistung erreicht. Unstrukturierte Verfahren konnten sich bislang in DNS-Codes nicht durchsetzen, da die größere Flexibilität im Hinblick auf die Geometrie zu Lasten der Rechengeschwindigkeit geht.\n\nWie in der Linearen Stabilitätstheorie unterscheidet man zwischen zeitlichem und dem räumlichen Problem. Bei einer zeitlichen Simulation werden Störungen, die über den Ausströmrand das Integrationsgebiet verlassen, am Einströmrand wieder eingebracht. Dies bedeutet, dass Störungen in der Zeit bis zur Sättigung anwachsen. Hintergrund dieser Vorgehensweise ist die Tatsache, dass aus mathematischer Sicht wegen des parabolischen Charakters der viskosen Terme auch am Ausströmrand Randbedingungen vorzugeben sind. Durch einen periodischen Ansatz in Strömungsrichtung wird das Problem umgangen, die in der Regel unbekannten passenden Randbedingungen vorzugeben. Ein Problem hierbei ist natürlich die Tatsache, dass die Strömung am Ausströmrand nicht den gleichen Zustand wie am Einströmrand aufweist. So wächst etwa eine Grenzschicht in Stromabrichtung an oder in einem Kanal sorgt ein Druckgradient dafür, dass die Strömung nicht zum Erliegen kommt. Deshalb sind die Störgrößen entsprechend zu skalieren, bevor sie am Einströmrand aufgeprägt werden können. Trotz der Probleme und Einschränkungen des zeitlichen Modells wird es bis heute verwendet, da mit einem relativ kleinen Integrationsgebiet eine turbulente Strömung berechnet werden kann.\n\nBei einer räumlichen Simulation hingegen werden Ein- und Ausströmrand getrennt voneinander betrachtet. Damit wachsen Störungen räumlich an (mit Ausnahme vom Auftreten einer absoluten Instabilität) was deutlich besser die Realität wiedergibt. In einer Grenzschicht etwa werden Tollmien-Schlichting-Wellen stromab transportiert, während sie anwachsen, was einer Zunahme der Amplitude in Stromab-Richtung entspricht. Nach ausreichender Simulationszeit erreichen die Störungen bei zeitlich periodischer Störungsanregung einen in der Zeit periodischen Zustand. Besonders wichtig ist bei einer räumlichen Simulation die Modellierung eines geeigneten Ausströmrandes, da ein Auftreffen von nichtlinearen Störungen auf den Ausströmrand Reflexionen verursachen würde, was wiederum zu unphysikalischen Ergebnissen führt. Anfangs wurde das Integrationsgebiet mit der Zeit in Stromab-Richtung verlängert, so dass Störungen nie den nach hinten laufenden Ausströmrand erreichen konnten. Diese Methode wird jedoch mit zunehmender Simulationsdauer immer aufwändiger. Der Durchbruch der räumlichen Simulation gelang erst mit der Entwicklung von geeigneten Dämpfungszonen vor dem Ausströmrand. Heutzutage wird zum Großteil das räumliche Modell verwendet.\n\nAls Anfangsbedingung wird typischerweise eine Lösung der Grenzschichtgleichungen verwendet, da diese eine gute Approximation einer laminaren reibungsbehafteten Strömung darstellen. Bei einer Plattengrenzschicht können selbstähnliche Profile (Blasius oder Falkner-Skan) auf das Rechengitter interpoliert werden. Handelt es sich um komplexere Geometrien, so sind die Grenzschichtgleichungen in Stromabrichtung zu integrieren. Ist dies nicht möglich, z. B. wegen Rückströmung, so kann das Strömungsfeld auch mit einfachen Annahmen initialisiert werden, da bei sauberen Randbedingungen Anfangsstörungen aus dem Integrationsgebiet heraus laufen sollten.\n\nFür eine Wand lassen sich die Randbedingungen vergleichsweise einfach formulieren. Aufgrund der Reibung sind die drei Geschwindigkeitskomponenten an der Wand Null (Haftbedingung). Im kompressiblen Fall ist eine weitere Randbedingung für die Temperatur zu geben, die wahlweise isotherm oder adiabat sein kann. Isotherm bedeutet, dass die Temperatur fest vorgeschrieben ist, adiabat, dass der Wärmestrom und damit der wandnormale Temperaturgradient null ist. Weiterhin können Aktuatoren an der Wand platziert werden, wie z. B. ein Störstreifen zur Störungsanregung, um das Verhalten bestimmter Störmoden zu untersuchen.\n\nAm Einströmrand sind bei inkompressiblen Rechnungen und im Überschall sämtliche Strömungsgrößen vorzuschreiben. Bei kompressiblen Rechnungen im Unterschall ist z. B. durch eine charakteristische Randbedingung dafür zu sorgen, dass stromauflaufende Schallwellen das Integrationsgebiet verlassen können. Der Einströmrand eignet sich auch um definierte Störungen einzubringen. Bei kleinen Störungen kann hierzu z. B. auf Amplituden- und Phasenverläufe aus der Linearen Stabilitätstheorie zurückgegriffen werden.\n\nProblematisch sind grundsätzlich Randbedingungen, die sich nur aufgrund des endlichen Rechengitters ergeben. Während bei RANS-Rechnungen typischerweise nur die stationären Freistrombedingungen vorgeschrieben werden, ist aufgrund der instationären Lösung die Wahl geeigneter Randbedingungen für den Erfolg einer DNS entscheidend. Eine besondere Bedeutung kommt hierbei dem Ausströmrand zu, da Reflexionen aufgrund der nichtlinearen Fluktuationen das Ergebnis signifikant verfälschen können. Hierzu wurden in der Vergangenheit verschiedene Dämpfungszonen entwickelt, in denen z. B. die Lösung auf eine stationäre Grundströmung gezogen wird. Insbesondere für aeroakustische Rechnungen ergeben sich besonders strenge Anforderungen an den Ausströmrand, da der betrachtete Schall um Größenordnungen kleiner als die strömungsmechanischen Schwankungen ist. Eine Möglichkeit ist hierbei die Kombination von Gitterstreckung und räumlichem Tiefpassfilter, wodurch instationäre Anteile in der Dämpfungszone sukzessive aus der Lösung zu entfernen werden bevor diese am eigentlichen Rand Reflexionen verursachen und das empfindliche akustische Feld kontaminieren können.\n\nFreistromränder sind grundsätzlich weniger kritisch als Ausströmränder, da die auftretenden Amplituden deutlich geringer sind als beim Ausströmrand.\nAllerdings können ungeeignete Randbedingungen zu falschen Anfachungsraten der Störwellen führen oder die richtige Lösung erfordert ein zu großes Integrationsgebiet. Aufgrund der kleinen Schwankungen basieren die Freistromränder typischerweise auf linearisierte Formulierungen, wie z. B. Abkling- oder charakteristische Randbedingungen. Wird Periodizität in Querrichtung angenommen, stellt sich die Frage nach passenden Randbedingungen an dieser Stelle nicht.\n\nDie aufzulösenden Skalen werden durch die Physik bestimmt. Das Rechengitter definiert, welche Skalen bei einem bestimmten numerischen Verfahren aufgelöst werden können. Die sich daraus ergebende erforderliche Schrittweite muss somit in der Größenordnung von (proportional zu und nicht gleich) den kleinsten Skalen sein, die durch die Kolmogorow-Längenskala\n\nbestimmt werden, wobei formula_5 die kinematische Viskosität und ε die Dissipationsrate der kinetischen Energie ist. Bei einer räumlichen Schrittweite \"h\" ist die Anzahl der Punkte \"N\" in einer Raumrichtung\n\nDie Dissipationsrate ε der kinetischen Energie lässt sich mit dem RMS-Wert der Geschwindigkeitsschwankung u' abschätzen zu\n\nSomit ergibt sich für die Anzahl der benötigten Punkte in einer Raumrichtung\n\nmit der Reynolds-Zahl\n\nUnterliegen alle drei Raumrichtungen der Skalierung mit der Reynolds-Zahl, so wächst die Gesamtanzahl der benötigten Punkte mit Re. Geht man von periodischen Randbedingungen in spannweitiger Richtung aus, so ist eine Skalierung der Punkte in dieser Richtung mit Re nicht zwingend erforderlich, wodurch die Gesamtauflösung nur noch mit Re skaliert. Dies kann sich weiter reduzieren, da z. B. die Grenzschichtdicke nicht linear mit der Stromabrichtung anwächst, weshalb die Anzahl der Punkte in wandnormaler Richtung schwächer als mit Re skalieren kann. Die zu simulierende Zeit ist proportional der turbulenten Längenskala τ, die durch\n\ngegeben ist. Unter der Annahme, dass das Zeitschrittlimit durch konvektive Terme dominiert wird (Δt ∼ h ∼ η/u'), ergibt sich die Anzahl der Zeitschritte zu\n\ndas heißt, die Anzahl der zu rechnenden Zeitschritte skaliert mit der Potenz ¾ der Reynolds-Zahl.\nSomit ist der gesamte Rechenaufwand proportional zu Re für ein voll-dreidimensionales Integrationsgebiet ohne spannweitige Periodizität.\n\nBei einem Skalierungsfaktor des Gesamtproblems von Re beziehungsweise Re erscheint eine Anwendung der DNS auf praxisrelevante Probleme auf den ersten Blick als ausgeschlossen. Eine Auswertung so großer Datenmengen (man beachte, dass es sich hierbei um instationäre Probleme handelt) erscheint auch nicht praktikabel. Bei der DNS geht es jedoch nicht darum, etwa ein komplettes Flugzeug zu simulieren, vielmehr ist man an den physikalischen Grundlagen interessiert, durch deren Verständnis auch ein Gesamtsystem mit hohen Reynolds-Zahlen, wie zum Beispiel das Profil eines Tragflügels, verbessert werden kann. Entwickelt man etwa Methoden zur Laminarhaltung, so ist es ausreichend, den relevanten Bereich der Grenzschicht zu simulieren, um damit den Widerstand des gesamten Flugzeugs zu verringern.\n\nDa man aus der DNS instationäre Daten von großen Gebieten erhält (um die 100 Millionen Gitterpunkte für die Raumauflösung sind durchaus üblich), fallen riesige Datenmengen von etlichen Gigabyte an. Daraus sind natürlich nicht direkt Aussagen über die physikalischen Vorgänge ableitbar, es bedarf einer Auswertung der Daten (postprocessing). Da beim räumlichen Modell der zeitliche Verlauf nach genügend gerechneten Zeitschritten periodisch oder zumindest quasi-periodisch ist, bietet es sich an, die Daten mittels Fourieranalyse zu untersuchen. Dabei wird bei periodischen Randbedingungen in Spannweitenrichtung häufig eine doppelspektrale Analyse durchgeführt. Die resultierenden Moden werden mit (h,k) bezeichnet, wobei h ein Vielfaches der Grundfrequenz und k ein Vielfaches der spannweitigen Grundwellenzahl\n\nformula_12\n\nist, welche sich aus der spannweitigen Ausdehnung λ des Integrationsgebiets ergibt. So bezeichnen die Moden (0,1), (0,2), usw. stationäre Störungen, die über der dritten Raumrichtung modelliert sind. Entsprechend wird eine zweidimensionale Störung mit der Fundamentalfrequenz mit (1,0) und ihre Höherharmonischen mit (2,0), (3,0), usw. bezeichnet. Das Wachstum der Amplituden in Strömungsrichtung sowie der Amplituden- bzw. Phasenverlauf normal dazu kann auch mit Ergebnissen der linearen Stabilitätstheorie verglichen werden.\n\nIm turbulenten Bereich sind Amplitudenverläufe in der Regel weniger aussagekräftig, da eine Vielzahl der Moden die Sättigung erreicht hat. Deshalb werden Wirbel z. B. mit Hilfe des lambda2-Kriteriums visualisiert, um einen besseren Einblick in die strömungsmechanischen Mechanismen zu erhalten. Das Bild zeigt beispielhaft dreidimensionale Wirbelstrukturen in einer Scherschicht mittels Isoflächen des lambda2-Kriteriums.\nDurch das Einbringen stationärer spannweitiger Störungen werden Längswirbel erzeugt, die zum Zusammenbrechen der Kelvin-Helmholtz-Wirbel führen. Um bei aeroakustischen Rechnungen die Schallabstrahlung darzustellen, kann man den Druck selbst, aber auch die Dilatation, die Divergenz des Geschwindigkeitsfeldes, darstellen. Einen schönen Eindruck vermitteln Dichtegradienten, da man damit Schlierenbilder erzeugen kann. Ein Auswerteprogramm ist z. B. EAS3, das an verschiedenen Universitäten zur Auswertung von DNS-Daten eingesetzt wird.\n\nDen Beginn der numerischen Strömungsmechanik stellt die Berechnung eines Kreiszylinders mit Re=10 aus dem Jahre 1933 dar. Thom erzielte die Lösung durch Handrechnung mittels eines Differenzenverfahrens, welche bereits erstaunlich genau war.\nVon einer ersten DNS im eigentlichen Sinn kann man bei der Simulation von Orszag & Patterson aus dem Jahre 1972 sprechen, die isentrope Turbulenz bei Re=35 auf einem 32 Gitter mit spektralen Methoden berechneten.\nDie erste räumliche DNS stammt von Fasel aus dem Jahre 1976. In dieser wurde das Wachstum kleiner Störungen in einer Grenzschicht untersucht und mit der Linearen Stabilitätstheorie verglichen. Die turbulente Strömung in einem ebenen Kanal mit Re=3300 und periodischen Randbedingungen in Strömungsrichtung wurde von Kim et al. im Jahr 1987 auf einem Gitter mit bereits 4 Millionen Punkten berechnet. Im Jahr 1988 veröffentlichte Spalart DNS-Ergebnisse einer turbulenten Grenzschicht mit Reθ = 1410 (θ steht hierbei für die Impulsverlustdicke der Grenzschicht), denen ebenfalls das zeitliche Modell zugrunde liegt.\nEin Durchbruch bei der Anwendung des räumlichen Modells gelang Anfang der 1990er Jahre mit der Entwicklung passender Dämpfungszonen vor dem Ausströmrand, z. B. durch Kloker et al.\nMithilfe entsprechender Randbedingungen konnten Colonius et al. 1997 eine der ersten akustischen DNS durchführen, wobei der abgestrahlte Schall der simulierten freien Scherschicht direkt mittels der Navier-Stokes-Gleichungen und nicht nach einer akustischen Analogie berechnet wurde.\nDie bis heute größte DNS, bezogen auf die räumliche Auflösung, ist eine Rechnung von Kaneda und Ishihara aus dem Jahr 2002, die auf dem Earth Simulator in Japan durchgeführt wurde. Sie verwendeten 4096 ≈ 68,7 Milliarden Gitterpunkte zur Simulation von isentroper Turbulenz in einem periodischen Integrationsgebiet. Der große Fortschritt, der im Bereich der DNS erzielt wurde, basiert natürlich zum einen auf den stetig steigenden Rechnerkapazitäten, aber ebenso auf den entwickelten numerischen Verfahren, die eine effektive Nutzung dieser Ressourcen erst ermöglichen.\n\nEin Beispiel für die Anwendung ist die Ablösung einer Grenzschicht aufgrund eines der Strömung entgegenwirkenden Druckgradienten. Durch das Anregen der Grenzschicht mit bestimmten Störungen wird versucht, Ablöseblasen zu verkleinern, oder ganz zu vermeiden. Anwendungsbeispiele sind etwa Turbinenschaufeln oder die Tragflügel am Flugzeug. Wäre man in der Lage, den Strömungsabriss bei hohem Anstellwinkel zu vermeiden, könnten höhere Auftriebsbeiwerte erzielt werden und man könnte auf Landeklappen verzichten.\n\nEin weiteres Thema ist die Laminarhaltung, bei der versucht wird, die Grenzschicht über den natürlichen Bereich hinaus laminar zu halten und den Umschlag von laminar zu turbulent herauszuzögern. Dies geschieht z. B. mit Absaugung oder dem Einbringen von Längswirbeln in die Grenzschicht. Da eine laminare Grenzschicht einen geringeren Widerstand als eine turbulente aufweist, könnte dadurch der Kerosinverbrauch von Flugzeugen um bis zu 15 % reduziert werden.\n\nIn der Aeroakustik werden die Mechanismen der Schallentstehung, verursacht durch strömungsmechanische Prozesse, untersucht. Ziel ist es dabei, den abgestrahlten Schall durch entsprechende Aktuatoren zu reduzieren. Die Aeroakustik ist ein relativ neues Feld im Bereich der DNS, da es als Mehrskalenproblem relativ schwierig zu lösen ist. Die strömungsmechanischen Fluktuationen, z. B. in einer freien Scherschicht, haben große Amplituden mit einer kleinen räumlichen Ausdehnung. Der abgestrahlte Schall dagegen ist relativ langwellig mit einer äußerst geringen Amplitude. Daraus ergibt sich, dass besondere Anforderungen (Randbedingungen, Genauigkeit) an ein numerisches Verfahren zu stellen sind, um die Ergebnisse nicht z. B. durch Reflexionen zu verfälschen.\nEin wichtiger Teilaspekt ist der Strahllärm, da er eine der Hauptlärmquellen eines Flugzeugs – insbesondere während des Starts – ist. Ein Durchbruch auf diesem Gebiet würde die Lebensqualität vieler Anwohner von Flughäfen erhöhen. Aufgrund von Auflagen wie z. B. Nachtflugverbot oder lärmabhängigen Start-/Landegebühren sind aber auch Fluggesellschaften und Flughafenbetreiber an einer Verringerung des Fluglärms interessiert.\n\nIm Bereich des Über- und Hyperschalls ist es erforderlich, nicht nur zeitlich gemittelte Größen, sondern auch instationäre Vorgänge in der Grenzschicht abzubilden, da hohe thermische Belastungen die Struktur zerstören können. Die DNS wird dabei für Grundlagenuntersuchungen des laminar-turbulenten Umschlags oder zur Entwicklung von Kühlkonzepten eingesetzt. Um Effekte im Hyperschall zu berücksichtigen, werden mittlerweile auch komplexere Gleichungen verwendet, die chemische Reaktionen wie Dissoziation oder Ionisation, sowie thermisches Nichtgleichgewicht berücksichtigen.\n\nDie DNS ist auch ein wichtiges Werkzeug für die Modellbildung. Dabei werden die hochaufgelösten instationären Strömungsdaten für die Weiterentwicklung von Turbulenzmodellen verwendet, um so weniger rechenintensive Verfahren wie Large-Eddy-Simulation oder RANS-Rechnungen zu verbessern. Außerdem können damit die Ergebnisse neuer Methoden der Strömungsmechanik validiert werden.\n\n\n\n"}
{"id": "304275", "url": "https://de.wikipedia.org/wiki?curid=304275", "title": "Linphone", "text": "Linphone\n\nLinphone (Akronym aus \"Linux phone\") ist eine freie Software für die IP-Telefonie, die unter der Lizenz \"GNU GPLv2\" verfügbar ist. Die Qualität der Videoübertragung eignet sich auch für Unterhaltungen in Gebärdensprache.\n\nDas Programm ist in einer Linux-, Windows- und macOS-Version erhältlich, für unixähnliche Betriebssysteme wie FreeBSD kann der frei zugängliche Quelltext genutzt werden. Zudem existieren Clients für Android, iOS, Blackberry, Windows Phone und Windows 10 Mobile. Neben der GTK+-basierenden grafischen Oberfläche existieren auch zwei Konsolen-Programme. Mit Linphone Web ist auch eine Version für Webbrowser verfügbar.\n\nDas Programm hat folgende Funktionalitäten:\n\nFür die Sprachübertragung stehen folgende Codecs zur Verfügung:\n\nVideoübertragungen können mit folgenden Codecs durchgeführt werden:\n\nVerschlüsselung kann mit folgenden Protokollen durchgeführt werden:\n\nFür den Betrieb hinter einem NAT-Router steht das STUN-Protokoll zur Verfügung.\n\nUbuntuWiki zufolge können Gespräche u. a. auch mit folgenden Gegenstellen geführt werden:\n\n\n"}
{"id": "304284", "url": "https://de.wikipedia.org/wiki?curid=304284", "title": "Graphics Device Interface", "text": "Graphics Device Interface\n\nDas Graphics Device Interface (GDI) ist eine Komponente des Betriebssystems Windows. Es dient als Programmierschnittstelle zu den logischen Grafikgeräten (Grafikkarte, Drucker) und kapselt die Komplexität der Hardware ab (Hardwareabstraktion). GDI ist zuständig für Aufgaben wie das Zeichnen von Linien und Kurven, Darstellung von Schriftzeichen und Bitmaps und Verwaltung von Farbpaletten. Die Komponenten DIB-Engine (Device-Independent Bitmap) und das ICM-Farb-Subsystem erlauben eine geräteunabhängige Verarbeitung von Bitmaps bzw. Farben. Benötigt ein Programm Informationen über gerätespezifische Eigenschaften (Bildschirmauflösung, Bildschirmtyp), kann es sie vom Device Context beziehen.\n\nDa die meisten Routinen zum Zeichnen auf dem Prozessor arbeiten und nicht auf der Grafikkarte, ist die Grafikausgabe von GDI relativ langsam. Unter Windows Vista wird die Grafikausgabe von GDI komplett von der CPU übernommen, wodurch diese noch langsamer als in früheren Windows Betriebssystemen ist. Seit Windows 7 werden mit dem Windows \"Display Driver Model v1.1\" wieder wenige Zeichenoperationen von der Grafikkarte beschleunigt.\nFür Anwendungen, die eine schnellere 3D-Grafikschnittstelle benötigen, vor allem für Spiele und CAx-Applikationen, wurden DirectX und OpenGL geschaffen.\n\n\n"}
{"id": "305048", "url": "https://de.wikipedia.org/wiki?curid=305048", "title": "Errechnetes Bild", "text": "Errechnetes Bild\n\nAls errechnetes Bild bezeichnet man Bilder, die durch die Verbindung von formaler Mathematik (Algebra, Analysis) und Geometrie entstehen. In der Regel werden solche Bilder am Computer durch bestimmte Algorithmen erzeugt. Dabei kann es sich sowohl um ausschließlich auf der Basis zweidimensionaler Bildbeschreibungen gerasterter und bearbeiteter, als auch um aus einer dreidimensionalen Szene gerenderter Bilder handeln.\n\nDie Vorläufer der errechneten Bilder können auf den Beginn der Neuzeit zurückgeführt werden, als die Linearperspektive durch Leon Battista Alberti und Filippo Brunelleschi entdeckt wurde.\n\nDie ersten im engeren Sinne errechneten Bilder stammen jedoch von den Brüdern Ernst Heinrich Weber und Wilhelm Eduard Weber aus dem Jahr 1836 und wurden in dem Buch \"Die Mechanik der menschlichen Gehwerkzeuge. Eine anatomisch-physiologische Untersuchung\" (Göttingen 1836) veröffentlicht; es handelt sich dabei um die zeichnerische Darstellung der Positionen des menschlichen Ganges in den verschiedenen Fortbewegungsphasen. Zur Anfertigung dieser Phasenbilder wurden Differentialgleichungen eingesetzt.\n\nDurch die Erfindung des Phenakistiskops durch Joseph Plateau und Simon Ritter von Stampfer wurde es möglich, die gezeichneten Phasenbilder zu animieren. Mit Hilfe der Fotografie hielten Étienne-Jules Marey, Eadweard Muybridge und Ottomar Anschütz etwa ab den 1870er Jahren ähnliche Bewegungsstudien in ihren Momentfotografien fest, ohne dass es sich dabei um errechnete Bilder handelte.\nDurch die Entwicklung von grafikfähigen Computern seit den 1950er Jahren wurde eine neue Dimension der errechneten Bilder möglich. Die unter anderem durch Benoît Mandelbrot um 1980 begründete Fraktale Geometrie visualisierte mathematische Gleichungen. Weitere Meilensteine des errechneten Bildes war die Entwicklung der Rastergrafik in den 1970er Jahren sowie verschiedener Renderverfahren wie Raytracing und Radiosity; siehe dazu Geschichte der Computergrafik.\n\nErrechnete Bilder nehmen eine medientheoretische Sonderstellung ein, da sie sich weder dem klassischen Kanon der Kunstgeschichte zuordnen lassen, noch eine geeignete bildwissenschaftliche Theorie existiert.\n\nDie wohl besten Erklärungsmodelle liefert Vilém Flusser mit seiner Theorie der technischen Bilder im Rahmen der Kommunikations- und Medienphilosophie; hier werden alle Varianten des Bildes subsumiert, die durch Anwendung von mathematischen Formeln entstehen, also auch die Fotografie. Diese Betrachtung fasst den Rahmen weiter als Friedrich Kittler, der errechnete Bilder auf die konkrete und unmittelbare Anwendung von Formeln wie beispielsweise Differentialgleichungen beschränkt; bei Flusser entstehen technische Bilder dagegen durch Apparate oder apparatfreie Anwendungen, die durch Formeln und Gleichungen ermöglicht werden.\n\nEine neuere Bezeichnung ist \"prozedurale Animation\" für Bewegtbildsequenzen, die mittels Software erzeugt werden, also jede Art von 3D- und 2D-Animation, bei der alle oder ein Großteil der Bewegungsphasen errechnet werden.\n\n\n\n"}
{"id": "305958", "url": "https://de.wikipedia.org/wiki?curid=305958", "title": "Ereignisprotokoll", "text": "Ereignisprotokoll\n\nDas Ereignisprotokoll () bezeichnet in Betriebssystemen der Windows-NT-Familie eine zentrale Sammlung von Logdateien und kann über die Ereignisanzeige (englisch: \"\") eingesehen oder mit der EventLog-Klasse programmatisch angesteuert werden. Die Dateien werden von einem Systemdienst verwaltet, den der Prozess \"Services\" aus der Dynamic Link Library \"Eventlog\" lädt.\n\nZu den protokollierten Ereignissen können beispielsweise fehlgeschlagene Anmeldeversuche, Statusänderungen der Netzwerkverbindungen, Warnungsmeldungen über die Systemsicherheit und Fehler bei der Einrichtung von Treibern oder beim Starten von Diensten gehören. Zu jedem Ereignis werden die Art des Ereignisses, die Ereignisquelle sowie Datum und Uhrzeit festgehalten. Als Ereignisquellen werden „Anwendung“ (Ereignisse innerhalb von Anwendungsprogrammen), „System“ (Ereignisse innerhalb des Betriebssystems) und „Sicherheit“ (Ereignisse, die für die Systemsicherheit relevant sind) unterschieden.\n\n\n"}
{"id": "310592", "url": "https://de.wikipedia.org/wiki?curid=310592", "title": "Apache Tomcat", "text": "Apache Tomcat\n\nApache Tomcat ist ein Open-Source-Webserver und Webcontainer, der die Spezifikation für Java Servlets und JavaServer Pages (JSP) implementiert und es damit erlaubt, in Java geschriebene Web-Anwendungen auf Servlet- beziehungsweise JSP-Basis auszuführen.\n\nTomcat besteht aus dem eigentlichen Servlet-Container \"Catalina\", der JSP-Engine \"Jasper\" und dem Connector-Framework \"Coyote\". Mittels verschiedener Connectoren unterstützt Tomcat diverse Kommunikationsprotokolle und kann mit dem HTTP-Connector entweder als eigenständiger Webserver betrieben oder mittels des AJP-Connectors in andere Webserver wie Apache Web-Server oder Microsoft IIS integriert werden. Mit der gegenwärtig existierenden AJP-Implementierung (Version 1.3) ist es möglich, den Servlet-Container auf einem gesonderten Host-Rechner zu betreiben, um den Webserver zu entlasten; insbesondere erlaubt es die Lastverteilungsfunktionalität, bei entsprechendem Leistungsbedarf dem Webserver einen Cluster aus mehreren Servlet-Containern zur Seite zu stellen.\n\nDie Verzeichnishierarchie einer Tomcat 6.x/7.x/8.x Installation umfasst:\n\nDie Entwicklung von Tomcat startete ursprünglich als Projekt von James Duncan Davidson bei Sun Microsystems als Referenz-Implementierung für die Java-Servlet- und JavaServer-Pages-Spezifikationen. Sun übertrug 1999 die Codebasis von Tomcat auf die Apache Software Foundation, die das Projekt unter dem Dach ihres Top-Level-Projekts Jakarta als Open-Source-Projekt weiterführte. Im Jahr 2005 wurde Tomcat selbst zu einem eigenen Apache-Top-Level-Projekt und hat seitdem seine eigene Organisations- und Management-Struktur.\n\nVor Übernahme des Tomcat-Projekts unterhielt die Apache Software Foundation bereits einen Servlet-Container namens JServ. Die Entwicklung von JServ wurde zugunsten von Tomcat im Jahr 2000 eingestellt. Lediglich der Connector, der Tomcat an andere Webserver über das AJP-Protokoll anbinden kann, wurde aus der Codebasis von JServ heraus in Tomcat übernommen. Im Jahr 2001 erfolgte mit der Tomcat-Version 4 ein tiefgreifendes Redesign der Tomcat-Struktur und weite Teile der Codebasis wurden neu erstellt. \n\nTomcat findet auch in einer Reihe von JavaEE-Anwendungsservern Anwendung, so ist er beispielsweise Bestandteil von Apache Geronimo und \"Apache TomEE\".\n\n"}
{"id": "311791", "url": "https://de.wikipedia.org/wiki?curid=311791", "title": "Produktaktivierung", "text": "Produktaktivierung\n\nDie Produktaktivierung (\"Softwareaktivierung\") ist eine Form des Kopierschutzes, die die Voraussetzung für die Nutzung mancher Software bildet. Im Gegensatz zur Produktregistrierung erfolgt bei der Produktaktivierung in der Regel keine Übermittlung personenbezogener Daten. Stattdessen erfolgt eine Bindung der Software an die Hardware des Anwenders.\n\nBis zur erfolgreichen Produktaktivierung ist die Nutzung von entsprechend geschützter Software nicht oder nur (zeitlich) eingeschränkt möglich.\n\nDie Produktaktivierung wurde – soweit sich dies nachvollziehen lässt – für Privatanwender zum ersten Mal von Microsoft eingesetzt, um die illegale Nutzung von Windows XP und Office zu unterbinden.\nSomit sind der Begriff selbst sowie der Ablauf des Verfahrens stark von Microsoft geprägt.\n\nDie Unterschiede im technischen Ablauf der Produktaktivierung sind im Falle verschiedener Softwarehersteller nur sehr gering.\nWenn der Anwender die Software installiert, muss er zunächst den individuellen Lizenzschlüssel (auch CD-Key oder Produkt-Key genannt), eingeben, den er zusammen mit der erworbenen Software erhalten hat. Im Laufe der Installationsroutine wird dann aus dem Lizenzschlüssel sowie aus Hardwaremerkmalen eine sogenannte Installations-ID errechnet. Diese hat die Eigenschaft, dass sich aus ihr die einzelnen Hardwaremerkmale praktisch nicht mehr nachvollziehen lassen, der Produkt-Key aber problemlos ausfindig gemacht werden kann. Die Installations-ID wird schließlich per Internet oder Telefon an den Hersteller übermittelt, dem nun der Lizenzschlüssel in Verbindung mit einem Hash-Wert, also einer Zahl, die aus Hardwaremerkmalen errechnet wurde, vorliegt. Der Hersteller hat nun die Möglichkeit in seiner Datenbank zu überprüfen, auf wie vielen verschiedenen Computern die Software bereits installiert wurde. Nun kann der Hersteller dem Nutzer die Bestätigungs-ID übermitteln, die wiederum aus der Installations-ID berechnet wird und somit für jede Installation individuell ist.\n\nMicrosoft bietet für Unternehmen zwei Produktaktivierungsverfahren für Volumenlizenzschlüssel an, die ohne Benutzereingriff ablaufen: die Einrichtung eines Key Management Service (\"KMS\") oder die Verwendung eines Multiple Activation Key (\"MAK\"). KMS setzt eine Mindestzahl von 25 zu aktivierenden Clients voraus, die regelmäßig über das Netzwerk mit dem KMS verbunden sind. MAK hingegen empfiehlt sich, wenn weniger als 25 Computer aktiviert werden müssen oder diese eingeschränkte Netzanbindung haben. KMS und MAK wird sowohl für Betriebssysteme als auch andere Produkte von Microsoft angeboten.\n\n\"Siehe auch: Produktaktivierung von Windows XP\"\n\nDie Verwendung von Technologien zur Produktaktivierung im Massenmarkt wirft verschiedene juristische Probleme auf. Diese werden in Fachzeitschriften diskutiert, sind jedoch weitestgehend ungeklärt, da es in der Vergangenheit noch keine gerichtlichen Auseinandersetzungen hinsichtlich der folgenden Konflikte gab. Es stellen sich insbesondere die Fragen, ob der Einsatz von Produktaktivierungs-Routinen\n\n\n\nDes Weiteren wird vielfach von einer Verletzung des Rechtes auf informationelle Selbstbestimmung gesprochen, die allerdings bei einer reinen Softwareaktivierung zu verneinen ist, da hier wie bereits erwähnt keinerlei personenbezogene Daten übertragen werden. Jedoch erfordern einige Produktaktivierungstechniken wie Steam das Anlegen eines personengebundenen Kontos. Insbesondere letzteres stellt sich als Hindernis heraus, da so ein Weiterverkauf nach Nutzung unmöglich oder nur mit großem Aufwand im Vorfeld der Aktivierung möglich ist.\n\n\n\nMit einigen sogenannten \"PE-Detektor\"-Programmen können auch Laien neben allgemeinen technischen Angaben zu einer Programmdatei teilweise die jeweils verwendeten Aktivierungstechnologien des Programms herausfinden.\n\nKaspersky Lab beispielsweise verwendet für seine Antivirenprogramme regionalisierte Aktivierungscodes (so funktioniert ein US-amerikanischer Code nicht in Deutschland und umgekehrt), teils um den internationalen Handel mit Raubkopien zu erschweren, teils weil die Programme in den verschiedenen Regionen zu verschiedenen Preisen verkauft werden. Einmal aktiviert, kann die Software aber überall benutzt werden. Probleme können sich ergeben, wenn die Lizenz erneuert oder die Software reinstalliert werden muss in einer anderen Region als in der, für die man die Lizenz erworben hat. Die Region wird an der IP-Adresse erkannt, so dass für die Aktivierung eine Internetverbindung erforderlich ist.\n\nAuch etliche Computerspiele haben eine regionalisierte Aktivierung.\n\nDie in Amerika und Europa verkauften Galaxy-Smartphones von Samsung müssen durch eine lokale SIM-Karte aktiviert werden, können dann aber auch in anderen Regionen benutzt werden. Die regionalisierte Aktivierung soll Grauimporte verhindern.\n\nManche Kreditkarten lassen sich für bestimmte Regionen der Welt sperren, um Diebstahl und Missbrauch im Ausland zu verhindern. Möchte der Karteninhaber ins Ausland reisen und die Karte dort benutzen, kann die Sperre für die betreffende Region vorläufig aufgehoben werden.\n\nVon der regionalisierten Aktivierung abzugrenzen ist der Regionalcode, der die Benutzung eines Produkts in anderen Regionen verhindern soll.\nDas bekannteste Beispiel sind regionsgeschützte DVDs und Blu-ray-Discs, die nur in Playern mit passendem Regionalcode laufen. Bei Blu-ray-Discs wird der Regionalcode nur vom Abspielprogramm, nicht aber vom Laufwerk oder dem Betriebssystem abgefragt, so dass man durch Manipulation des Abspielprogramms auf dem Computer beliebig oft Discs aus verschiedenen Regionen abspielen kann. Lediglich bei Standalone-Playern ist der Regionalcode Teil der Firmware. Blu-ray-Discs können aber auch den Ländercode des Players/des Abspielprogramms abfragen, so dass z. B. eine so geschützte Disc aus den USA nicht in Japan läuft oder umgekehrt (die USA und Japan liegen beide in Blu-ray-Region A, haben aber verschiedene Ländercodes). Obwohl es nur drei Blu-ray-Regionen gibt, ist damit eine präzisere Verbreitungskontrolle möglich als bei DVDs.\n\nAber auch zum Beispiel Drucker und ihre Patronen können einen Regionalcode haben, so dass der Anwender sie in seiner Region kaufen muss und nicht auf Importe aus billigeren Regionen ausweichen kann. Meist haben diese Patronen auch ein \"eingebautes Verfallsdatum\" (nach einem bestimmten Zeitraum funktionieren sie nicht mehr, selbst wenn sie noch voll sind), so dass man sich keinen Vorrat anlegen kann, und können auch erkennen, ob man versucht hat, sie nachzufüllen.\n\nManche Videospiele und Spielkonsolen sind ebenfalls regionsgeschützt.\n\nManche Programme müssen bereits zur Probezeit im Internet aktiviert werden. Das Programm nimmt dann beim Start Verbindung mit dem Internet auf und prüft, ob die Probezeit abgelaufen ist. Da die Informationen zur Probezeit hier nicht auf dem Computer des Anwenders, sondern auf einem externen Internetserver gespeichert werden, ist eine Manipulation zum Zurücksetzen der Probezeit äußerst schwierig. Ein Nachteil für den Anwender ist, dass man ohne Internetverbindung das Programm nicht benutzen kann, selbst wenn es dann keine mehr braucht.\n\n\n"}
{"id": "315847", "url": "https://de.wikipedia.org/wiki?curid=315847", "title": "DebianEdu", "text": "DebianEdu\n\nDebianEdu (früher Skolelinux) ist eine spezialisierte Anpassung der Linux-Distribution Debian (eine sogenannte Debian Pure Blend) für schulische Bedürfnisse. Das mittlerweile internationale Projekt wurde in Norwegen gegründet (Skole = Schule) und liefert auf einer einzigen Installations-CD-ROM alle notwendigen Konfigurationsprofile, um ein typisches Schulnetzwerk zu betreiben. Auch ungeschulte Anwender können in wenigen Schritten sowohl einen Kommunikationsserver mit LDAP-Nutzerdatenbank, einen Terminalserver, eine Arbeitsstation oder ein Notebook als Einzelplatzrechner installieren. Dabei kommt ausschließlich freie Software zum Einsatz, darunter auch viel Lernsoftware. Das CipUX-Framework mit seiner zentralen Benutzerbetreuung, Server- und Clientverwaltung CAT wurde 2005 auf Skolelinux portiert.\n\nDas Projekt wurde am 2. Juli 2001 gegründet, ungefähr zeitgleich begann Raphael Herzog in Frankreich mit Debian-Edu. Seit 2003 sind beide Projekte eins, und die Namen sind Synonyme. Manche sagen, dass Debian-Edu der Name des Projektes ist, und Skolelinux der Name der Distribution.\nHeute arbeiten zahlreiche Ehrenamtliche an der Weiterentwicklung des Projekts: Über 130 Programmierer unterstützen Skolelinux, das als Debian Edu Entwickler aus Norwegen, Frankreich, Spanien, Deutschland, Litauen, Brasilien und den USA einschließt. 2009 wird Skolelinux in einer Pilotphase an elf Schulen im Land Rheinland-Pfalz getestet, nachdem bereits Hamburg das System in seinen Schulen einführte.\n\nNachdem die Sichtbarkeit des Projekts in Deutschland zuletzt stark abgenommen hatte, übernimmt seit November 2017 der Verein Teckids e.V. die offizielle Vertretung des Projekts in Deutschland.\n\n\n"}
{"id": "316007", "url": "https://de.wikipedia.org/wiki?curid=316007", "title": "CrossOver", "text": "CrossOver\n\nCrossOver ist eine kommerzielle Wine-Version des Unternehmens CodeWeavers. Es sind Versionen für Linux und macOS erhältlich. CrossOver war zu Anfang speziell auf die Unterstützung von Microsoft Office ausgelegt, inzwischen wird aber eine Reihe weiterer Windows-Software unterstützt.\n\nCrossOver wird von CodeWeavers entwickelt. Dabei fließen Verbesserungen, die an dem unter LGPL stehenden Wine-Sourcecode vorgenommen werden, in das freie Projekt zurück. CodeWeavers beschäftigt außerdem einige der Wine-Entwickler. Die Konfigurationssoftware, die eine komfortable Installation der Programme erlaubt, steht jedoch unter einer proprietären Lizenz.\n\n\"CrossOver Server\" war eine Client-Server-System-Lösung, die die aufwendige, mehrfache Installation von CrossOver und der damit zu installierenden Windows-Software, wie z. B. in Unternehmen, überflüssig machte. Dabei wurde man von einigen dafür zusammengestellten Management-Programmen unterstützt. Die Clients konnten im Gegensatz zum Server nicht nur unter Linux, sondern auch unter Solaris laufen. \"CrossOver Server\" wurde im Jahr 2007 eingestellt und in CrossOver Linux Professional integriert.\n\nBis einschließlich CrossOver 10.3.0 wurde zwischen \"CrossOver\" und \"CrossOver Games\" unterschieden, die jeweils auf den Betrieb von Office-Software bzw. Spielen optimiert waren und auch im Paket mit verlängertem Support-Zeitraum erworben werden konnten. Mit CrossOver 11 wurde diese Trennung jedoch aufgegeben. Käufer können jetzt zwischen drei verschiedenen Support-Leveln wählen und haben grundsätzlich Zugang zu beiden Versionen (Linux und macOS).\n\n"}
{"id": "316990", "url": "https://de.wikipedia.org/wiki?curid=316990", "title": "Quanta Plus", "text": "Quanta Plus\n\nQuanta Plus (auch \"Quanta+\") oder nur Quanta war ursprünglich sowohl ein Quelltext-Editor als auch ein WYSIWYG-Editor zur Erstellung von Internetseiten, der für die Arbeitsfläche K Desktop Environment geschrieben wurde.\n\nQuanta bot darüber hinaus umfangreiche Funktionen wie komplexes Projektmanagement und unterstützte unter anderem HTML, XHTML, XML, Java, PHP und JavaScript. Außerdem war die Unterstützung für Debugging implementiert. Daher kann Quanta als (Web-)Entwicklungsumgebung bezeichnet werden.\n\nIn den Jahren 2003 bis 2006 wurde Quanta Plus von den Mitgliedern des Forums LinuxQuestions.org zum besten Web-Editor gewählt.\n\nQuanta wird seit 2009 nicht mehr aktiv entwickelt, aber einige von den Funktionen wurden 2012 in das Schwesterprojekt KDevelop übertragen.\n\nDarüber hinaus wird das Projekt von der Trinity Desktop Umgebung gepflegt und ist als Teil dieser verfügbar.\n\n"}
{"id": "318231", "url": "https://de.wikipedia.org/wiki?curid=318231", "title": "Farbanpassung", "text": "Farbanpassung\n\nAls Farbanpassung (englisch \"Color matching\" oder \"Image color matching\") bezeichnet man in der digitalen Bildbearbeitung die Farbabstimmung für Monitor-Bildausgaben in Hinblick auf Eingabegeräte wie Kamera, Scanner oder Digitalkamera und Ausgabegeräte wie Drucker und Plotter, aber auch für den Mehrfarbendruck, um eine möglichst große Farbtreue von der Vorlage bis zum Endprodukt zu erhalten. Dies Verfahren ist erforderlich, um in der Druckvorstufe verschiedene Farbmodelle wie RGB (Monitor) und CMYK aneinander anzugleichen, da die Bildschirmausgabe eine subtraktive Farbmischung nur unzureichend simulieren kann.\n\nIn der Regel geschieht die Farbanpassung mit Hilfe von Farbtabellen bzw. Farbfächern nach festgelegten Systemen von Volltonfarben und Schmuckfarben wie z. B.\n\ndie mit der Darstellung am kalibrierten Bildschirm verglichen werden. Um auch geringste Differenzen auszugleichen, werden diese Farbsysteme an den jeweiligen Bedruckstoff (unterschiedliche Papiersorten, Kunststoffe, Textilien, aber auch Metalle, Glas oder Keramik im Siebdruckverfahren) angepasst.\n\nZur Farbanpassung benutzt man außerdem optische Messgeräte wie z. B. Spektralfotometer, eine Art Handscanner, der die spektralen Remissionswerte einer Referenzfarbe misst. Ebenso gibt es zahlreiche Softwarelösungen, die die \"individuellen\" Eigenschaften einzelner Eingabegeräte – meist sind dies Farbstiche – ausgleichen können. Zusätzlich werden für das Produktdesign Plättchen aus durchgefärbten oder transparenten Kunststoffen verwendet, die bei der CAD-Arbeit helfen, das Endprodukt schon vor der Fertigung des Prototyps farblich zu beurteilen.\n\nAktuelle Betriebssysteme und Bildverarbeitungsprogramme bieten Color Matching per ICC-Profil gestütztem Colormanagement (Farbmanagement), das unter anderem die geräteneutrale Aufbereitung von Sonderfarben über Lab-Farbraum-Definitionen ermöglicht.\n"}
{"id": "322124", "url": "https://de.wikipedia.org/wiki?curid=322124", "title": "Elektronische Medien", "text": "Elektronische Medien\n\nEin elektronisches Medium ist ein Medium, das auf elektronischem Weg empfangen und wiedergegeben wird. In zunehmendem Maße werden die übertragenen Mitteilungen digital kodiert. In diesem Fall wird auch der Begriff „digitale Medien“ verwendet. Rein begrifflich muss zwischen Medium als Träger von Inhalten (Informationsmittel/Kommunikationsmittel) und von Medieninhalten selbst unterschieden werden.\n\nDer Begriff „elektronische Medien“ entwickelte sich im 20. Jahrhundert als Synonym für Hörfunk und Fernsehen (Rundfunkmedien). Inzwischen findet eine Begriffserweiterung statt, da moderne elektronische Medien wie Internet, PC und sämtliche mobilen Anwendungen die gleichzeitige Produktion, Übertragung und Rezeption von Inhalten ermöglichen.\n\nZu den elektronischen Medien zählen:\n\n\n\n\n"}
{"id": "324681", "url": "https://de.wikipedia.org/wiki?curid=324681", "title": "Native POSIX Thread Library", "text": "Native POSIX Thread Library\n\nDie Native POSIX Thread Library (NPTL) ist eine moderne Implementierung einer Threading-Bibliothek für Linux. Sie wird in Verbindung mit der GNU C Library (glibc) verwendet und erlaubt Linux-Programmen die Verwendung von POSIX-Threads (pthreads).\n\nSeit der Kernel-Version 2.0 existierte für Linux die Threading-Bibliothek \"LinuxThreads\", deren grundlegende Design-Prinzipien unter Einfluss der 1996 vorhandenen Beschränkungen des Linux-Kernels und der libc5 zustande gekommen waren. Linux hatte keine echte Unterstützung für Threads im Kernel, kannte aber den \"clone()\"-Systemaufruf, der eine Kopie des aufrufenden Prozesses mit identischem Adressraum erzeugte. LinuxThreads benutzte diesen Systemaufruf, um Thread-Unterstützung im Userspace zu simulieren. Die Bibliothek wurde zwar kontinuierlich verbessert, war aber konzeptionell veraltet und eingeschränkt. \n\nFolgende Probleme mit der existierenden \"LinuxThreads\"-Implementation wurden identifiziert:\n\nUm die bestehenden Probleme zu lösen, wurden zusätzliche Infrastruktur im Kernel und eine neu geschriebene Threading-Bibliothek benötigt. Es wurden zwei konkurrierende Projekte gestartet: Next Generation POSIX Threads (NGPT) unter Leitung von IBM und NPTL unter Federführung der bei Red Hat angestellten Kernel- und glibc-Programmierer Ingo Molnár und Ulrich Drepper. Weil sich abzeichnete, dass sich in der Praxis die NPTL durchsetzen würde, wurde das NGPT-Projekt Mitte 2003 eingestellt.\n\nDas NPTL-Team setzte sich folgende Ziele für seine neue Bibliothek:\n\n\nUnter diesen Voraussetzungen begann Mitte 2002 die Arbeit an der neuen Native POSIX Thread Library. Im August/September 2002 wurde der Linux-Kernel 2.5 für die NPTL vorbereitet. Dazu war es notwendig, einige neue Systemaufrufe einzuführen und vorhandene zu optimieren. In ersten Benchmarks konnten nun auf einem IA-32-System innerhalb von 2 Sekunden 100.000 parallele Threads erzeugt werden; ohne NPTL dauerte allein die Erzeugung der Threads fast 15 Minuten. Trotz dieser Last blieb das Testsystem mit annähernd gleicher Geschwindigkeit benutzbar.\n\nRed Hat Linux 9 war die erste Linux-Distribution, in der die NPTL in einem gepatchten 2.4er-Kernel verwendet wurde (und deren Benutzer dadurch bisweilen zu unfreiwilligen Betatestern wurden). Inzwischen benutzen praktisch alle modernen Distributionen die NPTL, wenn sie einen Kernel der Version 2.6 oder höher verwenden.\n\nNPTL funktioniert ähnlich wie \"LinuxThreads\". Der Kernel verwaltet immer noch Prozesse und keine Threads, und neue Threads werden mit einem von der NPTL aufgerufenen codice_1 erzeugt. Die NPTL benötigt allerdings spezielle Kernelunterstützung und implementiert damit Synchronisationsmechanismen, bei denen Threads schlafen gelegt und wieder aufgeweckt werden. Dazu werden Futexes verwendet.\n\nDie NPTL ist eine sogenannte 1:1-Threading-Bibliothek. Die vom Benutzer mit der codice_2-Funktion erzeugten Threads stehen dabei in einer 1-zu-1-Beziehung mit Prozessen in den Scheduler-Queues des Kernels. Dies ist die einfachste denkbare Threading-Implementierung. Die Alternative wäre m:n. Dabei existieren typischerweise mehr Threads im Userspace, als es Prozesse im Kernel gibt. Die Threading-Bibliothek wäre dann dafür verantwortlich, die Prozessorzeit auf die einzelnen Threads im Prozess zu verteilen. Mit diesem Konzept wären sehr schnelle Kontextwechsel möglich, da die Anzahl der notwendigen Systemaufrufe minimiert wird, andererseits würde aber die Komplexität erhöht, und es könnte leichter zu Prioritätsinversion kommen.\n\n"}
{"id": "324768", "url": "https://de.wikipedia.org/wiki?curid=324768", "title": "Pidgin (Instant Messenger)", "text": "Pidgin (Instant Messenger)\n\nPidgin [] (früher Gaim, nicht zu verwechseln mit Gajim) ist ein freier Instant Messenger für verschiedene Protokolle. Das Programm wurde von Mark Spencer ursprünglich für unixähnliche Systeme (Linux, BSD) geschrieben, ist aber auch auf Windows lauffähig. Pidgin kann mit Plug-ins stark erweitert werden.\n\nAm 6. April 2007 wurde bekanntgegeben, dass \"Gaim\" infolge rechtlicher Probleme mit AOL bezüglich des Markenzeichens „AIM“ in Pidgin umbenannt wurde. Der Name Pidgin ist ein Wortspiel zwischen dem Begriff der Pidgin-Sprachen und dem ähnlich klingenden Begriff „pigeon“ (engl. Taube, Brieftaube) – deshalb auch die lilafarbene Pidgin-Taube („pidgin-pigeon“).\n\nPidgin unterstützt verschiedene Netzwerkprotokolle. Dies wird durch Plug-ins ermöglicht, die das jeweilige Protokoll für Pidgin zumindest teilweise implementieren. Neben den offiziellen Plug-ins für die offiziell unterstützten Protokolle gibt es dabei noch extra Plug-ins, die von anderen Entwicklern oder Projekten bereitgestellt werden, um Pidgin zu diesen Netzwerken kompatibel zu machen. Zurzeit gibt es Plug-ins für unter anderem folgende Protokolle:\nAb der Version 2.6.0 unterstützt Pidgin auch Sprach- und Videoanrufe über XMPP (Jabber), jedoch noch nicht unter Windows.\n\nNachrichten können mit den Plug-ins \"pidgin-encryption\" und Off-the-Record Messaging (OTR) verschlüsselt werden, die mit unterschiedlichen Verschlüsselungsstandards arbeiten.\n\nNeben der Unterstützung zusätzlicher Protokolle kann Pidgin auch mit Plug-ins ausgestattet werden, die den Funktionsumfang erweitern. Diese reichen bis hin zu Plug-ins, die kryptografische Funktionen bieten, um Nachrichten zu verschlüsseln. Zurzeit gibt es, neben den offiziellen Protokoll-Plug-ins, insgesamt mehr als 200 weitere Plug-ins für Pidgin.\n\nEinige Funktionen wie Audio- und Video-Telefonie oder Theming wurden im Rahmen geförderter Projekte des Google Summer of Code entwickelt.\n\nDie letzte als stabil deklarierte Version vor der Umbenennung zu \"Pidgin\" war Gaim 1.5.0, welche am 11. August 2005 erschien.\n\nNach sieben Beta-Versionen, wobei die letzte bereits den neuen Namen \"Pidgin\" trug, und fast anderthalb Jahre nach dem Erscheinen der ersten Beta-Version wurde am 3. Mai 2007 die endgültige Version 2.0.0 veröffentlicht. Seither gab es jedoch einige Neuerungen, so dass am 2. Juli 2008 die Version 2.4.3 erschien, in der das Problem mit dem geänderten ICQ-Protokoll behoben wurde. Die Version 2.6 brachte die Möglichkeit zu Audio- und Video-Telefonaten, die Mike Ruprecht 2008 in Googles Summer of Code umzusetzen begann.\n\nDer Kern von Pidgin ist für Programmierer als C-Bibliothek unter dem Namen libpurple verfügbar. Folgende Programme basieren auf Pidgin oder libpurple:\n\n\n\n"}
{"id": "326858", "url": "https://de.wikipedia.org/wiki?curid=326858", "title": "Backspace", "text": "Backspace\n\nDie Backspace-Taste (, Rückschritttaste oder Rücklöschtaste, keine gebräuchliche Abkürzung) ist eine Taste der PC-Tastatur, die bei Betätigung den Cursor um eine Position nach links verschiebt und das dort stehende Zeichen entfernt. Dies entspricht der Funktion der Korrekturtaste bei Schreibmaschinen mit Korrekturfunktion. Sie löscht damit in die entgegengesetzte Richtung der Entfernen-Taste. Auf Tastaturen ist die Taste häufig mit oder beschriftet.\n\nDie meisten Webbrowser interpretieren die Backspace-Taste als „Zurück“ – das heißt, die im Verlauf zuvor aufgerufene Seite wird aufgerufen.\n\nDas Steuerzeichen für den Backspace (BS) hat im ASCII den Wert 8 und entspricht in vielen Terminals ^H (+). Bei EBCDIC hat BS den Wert 16. Verfügt ein ASCII-Terminal nicht über die Funktion, den Cursor nach links zu verschieben, wird stattdessen ^H ausgegeben.\n\nEine Erweiterung des Backspace stellt (vor allem unter Unix/Linux) ^W (+) oder die Kombination + (analog der Kombination +) dar, womit das letzte Wort gelöscht werden kann, d. h., es werden alle Zeichen bis zum letzten vorigen Whitespace entfernt. Ferner können mit ^U (+) alle vorigen Zeichen der aktuellen Zeile gelöscht werden. Auch einige Texteditoren wie vim oder emacs bieten diese Möglichkeit.\n\nUnicode enthält als Symbol für das Steuerzeichen das Zeichen ␈ (U+2408).\n\nIn Anspielung auf die Darstellung des zugehörigen Steuerzeichens mittels ^H wird in Internetforen manchmal ein Wort in einem Satz begonnen und danach mit ausgeschriebenem „^H“ „gelöscht“, um eine Art absichtlichen Freudschen Versprecher darzustellen:\n\nEs zeigt, was eigentlich gemeint war, aber dann „gelöscht“ wurde.\n"}
{"id": "329225", "url": "https://de.wikipedia.org/wiki?curid=329225", "title": "PCtipp", "text": "PCtipp\n\nDer PCtipp ist die meistverkaufte und meistgelesene Schweizer PC-Zeitschrift. Als \"PCtip\" erschien sie erstmals 1994 als IT-Beilage der Zürcher Tageszeitung \"Tages-Anzeiger\" in Kooperation mit dem IT-Fachverlag IDG – zunächst einmal pro Quartal, später 10 Mal pro Jahr. Die Beilage wurde grösser, machte sich 1997 selbständig und wurde vom IT-Fachverlag IDG alleine herausgegeben. Seit dem Redesign anlässlich der Ausgabe 10/2003 wird die Zeitschrift \"PCtipp\" (mit zwei \"p\") geschrieben. Inzwischen erscheint der \"PCtipp\" 14 Mal pro Jahr – inklusive zweier Spezialhefte zu Themenschwerpunkten wie etwa Fotografie, Android oder Top 100 Produkte. Das Magazin hat eine WEMF-beglaubigte Auflage von 46'034 (Vj. 48'419) verkauften bzw. 52'157 (Vj. 56'535) verbreiteten Exemplaren und eine Reichweite von 205'000 (Vj. 217'000) Lesern (WEMF MACH Basic 2018-II) vor allem in der Deutschschweiz, aber auch im deutschsprachigen Ausland.\n\nIm Juli 2015 wurde die IDG Communications AG von der Verlagsgruppe Ebner Ulm übernommen und in die [Neue Mediengesellschaft Zürich AG] umfirmiert. Sie ist nicht nur Herausgeberin von PCtipp und dem B2B-Magazin Computerworld, sondern unterhält auch die gleichnamigen Computerportale http://www.pctipp.ch, http://www.computerworld.ch/ sowie http://www.onlinepc.ch/.\n\nNach wie vor verzichtet der \"PCtipp\" bewusst auf das sonst übliche Hochglanzpapier der Computermagazine; die Monatszeitschrift wird auf gewöhnlichem Zeitungspapier etwa im DIN-A4-Format gedruckt. Die Beiträge sind praxisorientiert, als Zielpublikum wird der \"etwas anspruchsvollere\" «Otto-Normal-Benutzer» angesprochen.\n\nIm Mittelpunkt der Beiträge stehen Tipps für Microsoft Windows, Microsoft Office und weitere Windows-Software; aber auch Apple Macintosh und Linux werden berücksichtigt. Daneben gibt es Tipps und Tricks rund um Internet, E-Mail und Digitalfotografie, Tablets und Smartphones. Schliesslich werden Produkte-Neuvorstellungen, Produkte-Tests und Kaufhilfen behandelt.\n\nSämtliche im Heft vorgestellte Gratis-Software ist auf der Website zum Herunterladen verfügbar. Auch werden sporadisch PDF-Seiten aus dem \"PCtipp\"-Archiv zum Download angeboten.\n\n"}
{"id": "330298", "url": "https://de.wikipedia.org/wiki?curid=330298", "title": "Atari 1040 STE", "text": "Atari 1040 STE\n\nDer Atari 1040 STE war ein Heimcomputer der Atari Corporation, der im Jahr 1989 als Nachfolger der Baureihe Atari ST eingeführt wurde, um verlorene Marktanteile vom konkurrierenden Amiga zurückzuerobern. \"STE\" stand für \"ST Enhanced\".\n\nIm Vergleich zum Atari 1040 ST erhielt der STE u. a. folgende neue Eigenschaften:\n\n\nAnsonsten ist das Gerät weitgehend mit dem 1040 STFM identisch. Gehäuse und Tastatur sind gleich, es finden sich weitgehend die gleichen Anschlüsse. Zusätzlich verfügt der STE über zwei Cinch-Anschlüsse zur Tonausgabe in Stereo.\n\nNeben der hier gezeigten und in Deutschland populärsten Variante 1040 STE war in Frankreich und England auch der 520 STE recht populär, der sich nur im Speicherausbau vom 1040 STE unterschied. Da auch bei diesem Rechner SIMMs verwendet wurden, konnte der Rechner leicht auf 1, 2 oder 4 MiB aufgerüstet werden.\n\nFerner boten Händler den Rechner mit verschiedenem Ausbau des Hauptspeichers an und bewarben diese als 2080 STE oder 4160 STE. Von Atari selbst gab es zwar einen 4160 STE-Prototyp, dieser gelangte aber nie in den Handel und wurde vermutlich nur an Entwickler ausgeliefert.\n\nDarüber hinaus plante Atari einen STE mit integriertem 80286-Prozessor, um einen IBM-kompatiblen Computer anbieten zu können. Der Rechner sollte zusätzlich mit einer 40 MB großen 2,5\"-Festplatte ausgerüstet werden, kam jedoch nie über den Status eines Prototyps hinaus.\n\nDas erste Mal wurde der STE auf der Atari-Messe im August 1989 der Öffentlichkeit vorgestellt.\n\nDie Änderungen am Betriebssystem und die veränderten Farbregister für die erweiterte Palette brachten jedoch viele Inkompatibilitäten an bestehender Software, vor allem bei Spielen, mit sich. Zunächst unterstützten vor allem Public-Domain-Spiele und -Software den STE, z. B. Musikprogramme wie der ProTracker oder Audio Sculpture. Später nutzten auch kommerzielle Spiele, vor allem die des deutschen Entwicklers Thalion Software, die erweiterten Fähigkeiten des STE teilweise aus. Auch einige Demos, allen voran \"Braindamage\" der Gruppe Aggression, demonstrierten die multimedialen Stärken des STE.\nErst gegen Ende der 16-Bit Ära erschienen kommerzielle Spiele, die nur auf dem STE lauffähig waren, z. B. \"Sleepwalker\" von Ocean, \"Obsession\" und \"Substation\" von Unique Development Sweden oder \"Stardust\" von Bloodhouse.\n\nZusammengefasst konnte der STE jedoch nur teilweise mit dem bereits lange existierenden Amiga technologisch gleichziehen, ihn aber nicht überholen. Der Amiga 500 blieb weiterhin die favorisierte Spieleplattform. Der 1040 STE blieb in den Verkaufszahlen deutlich hinter den Erwartungen zurück, unter anderem, da Atari in Deutschland scheute, den STFM durch den STE abzulösen und beide Systeme gleichzeitig – den STE zu einem höheren Preis – anbot.\n\nAtari veröffentlichte 1991 mit dem MegaSTE eine für den semi-professionellen Einsatz gedachte Variante des STE, 1992 stellte Atari mit dem Falcon030 den Nachfolger des 1040STE vor.\n\nFür den STE kam einige auf dieses Gerät zugeschnittene Hardware heraus, unter anderem folgende:\n\n\nHeutzutage gibt es für klassische Computer, wie die Amiga- oder ST-Reihe, immer noch eine Fanbasis, die regelmäßig Spiele und Demos für diese Maschinen produziert.\nDie Atariszene nutzt den STE in fast allen Produktionen zumindest bei der erweiterten Farbpalette oder dem Digisound aus. Jedoch gibt es bis heute nur eine Minderheit, die sich dem STE intensiv oder ausschließlich widmet.\n\n\nViele Programme für die STE-Serie sind mit den Emulatoren Steem oder Hatari auf heutigen PCs lauffähig. Erhältlich sind diese hier:\n\n"}
{"id": "330497", "url": "https://de.wikipedia.org/wiki?curid=330497", "title": "Auflösung (Fotografie)", "text": "Auflösung (Fotografie)\n\nAls Auflösung oder Auflösungsvermögen bezeichnet man in der Fotografie die Fähigkeit eines Objektivs, Films oder Sensors, bestimmte kleinste Strukturen noch wiedergeben zu können.\n\nZur Ermittlung des Auflösungsvermögens werden Testaufnahmen von Strichmustern angefertigt, die in der Regel schwarz-weiß oder zumindest einfarbig sind. Dazu wird ein Film mit einem Strichraster belichtet, das einen immer geringer werdenden Abstand (zunehmende Ortsfrequenz) hat. Mit einem Mikrodensitometer wird gemessen, wie stark die Unterscheidungen zwischen den immer enger werden Linien sind.\n\nDie Ermittlung des Bildauflösungsvermögens erfolgt durch Abzählen der noch erkennbaren Anzahl von Strichintervallen pro Millimeter. Der ermittelte Wert wird auch Modulationsübertragungsfunktion genannt. Es werden nur Werte in Bezug auf Hell-Dunkel-Kontraste geliefert. Daher kann daraus nicht abgeleitet werden, wie gut die Auflösung von vielfarbigen Strukturen ist. Hierzu ist es notwendig mehrfarbige Vorlagen, wie zum Beispiel Weißlichtinterferogramme oder farbige Rauschmuster zu verwenden.\n\nEs lässt sich immer noch eine Unterscheidung im Strichraster wahrnehmen, auch wenn diese Unterscheidung unscharf oder mit Störungen versehen ist.\n\nAus diesem Grund wird bei jedem Messverfahren eine Toleranzgrenze festgelegt. Damit wird die Menge tolerierbarer Abweichungen (Unschärfe, Störungen, Kontrast, …) definiert.\n\n\nAusgewählte Farbnegativ- und Umkehrfilme:\nAuch bei den Sensoren von Digitalkameras wird die Auflösung in Linienpaaren pro Millimeter angegeben. Da diese Auflösung letztlich, wie beim Film auch, aber nicht nur vom Kamerasensor, sondern auch vom verwendeten Objektiv abhängt, ergeben sich hierbei nur Anhaltswerte. Zu Beginn der digitalen Fotografie wurde auch die Anzahl der Pixel zur Orientierung über die Auflösung verwendet. Diese Anzahl wird meist in Megapixeln angegeben. Die tatsächlich von der Kamera erreichte Auflösung hängt jedoch nicht nur von der Anzahl der Pixel, sondern auch der Größe des Bildsensors sowie der von der Kamera zur Verarbeitung der Bildsignale verwendeten Hard- und Software ab. Eine Messung der tatsächlich physikalisch vorhandenen Auflösung kann nur über eine differenzierte Messmethode ermittelt werden. Für solche Messverfahren gibt es noch keinen einheitlichen Standard, so dass zum Teil starke Schwankungen der Messergebnisse entstehen.\n\nUnter anderem muss berücksichtigt werden, dass Struktureffekte (beispielsweise Moiré) das Messergebnis verändern können.\n"}
{"id": "335108", "url": "https://de.wikipedia.org/wiki?curid=335108", "title": "SharpDevelop", "text": "SharpDevelop\n\nSharpDevelop (abgekürzt „#develop“) ist eine freie integrierte Entwicklungsumgebung (IDE) für das .NET Framework von Microsoft. Neben den ersten Sprachen C# und Visual Basic unterstützt die Entwicklungsumgebung auch weitere Programmiersprachen. SharpDevelop wird seit 2000 durch IC#Code entwickelt, einen Zusammenschluss freier Softwareentwickler. Nach Aussagen eines führenden Entwicklers ist die Weiterentwicklung inzwischen eingestellt.\n\nAm 11. September 2004 wurde die Version 1.0 der quelloffenen .NET-Programmierschnittstelle „SharpDevelop“ veröffentlicht, welche die Programmiersprachen C# und Visual Basic .NET (VB.NET) unterstützte und das „Microsoft .NET Framework“ in der Version 1.0 voraussetzte.\n\nAm 14. Februar 2006 wurde die letzte Version der Reihe 1.1.x veröffentlicht, welche das .NET-Framework in der Version 1.1 voraussetzte.\n\nIm August 2006 wurde die Version 2.0 veröffentlicht, mit welcher .NET- und Mono-Programme nun zusätzlich in der Programmiersprache Boo entwickelt werden können. Zudem ist hier bereits das Öffnen und Bearbeiten von Visual-Studio-Projekten möglich. Zur Inbetriebnahme dieser Version ist das .NET-Framework in der Version 2.0 und Microsoft Windows (ab Windows 2000) erforderlich, zudem wird die Installation des dazugehörigen SDK von Entwicklern empfohlen.\n\nAm 7. März 2007 wurde die Version 2.1 veröffentlicht und mit einem FxCop-gestützten Quelltext-Analysator, einem Typen-Browser für Assemblies und COM-Komponenten (auf Basis des .NET-Component-Inspector von Oakland), einer inkrementellen Suche, einem Abfrage-Erstellungswerkzeug für SQL-Datenbankabfragen, einer Unterstützung für das Versionsverwaltungs-Werkzeug Subversion sowie das Installationswerkzeug WiX und einigem mehr ergänzt.\n\nAm 8. August 2007 wurde die letzte Version der Reihe 2.2.x veröffentlicht, welche das .NET-Framework in der Version 2.0 voraussetzte.\n\nSeit der im Februar 2009 veröffentlichten Version 3.0 werden die Programmiersprachen IronPython und F# unterstützt. Zudem wurde die Unterstützung von Mehrkernprozessoren und eine Designer-Vorschau für die Windows Presentation Foundation (WPF) hinzugefügt. Für die Inbetriebnahme wird nun das .NET-Framework in der Version 3.5 und Windows – ab XP, mit SP2 – vorausgesetzt.\n\nAm 21. September 2009 wurde die Version 3.1 veröffentlicht, mit welcher nun eine Debugging-Unterstützung für IronPython, die freie Python-Implementierung für .NET, der IronPython-Windows-Forms-Designer und ein Profiler für sogenannte „Managed Applications“ hinzugefügt wurden. Zudem wurde auch bekannt gegeben, dass die kommende Version 4.0 parallel mit der Version 3.1 entwickelt wird, welche bei der Fertigstellung das .NET-Framework in der Version 4.0 unterstützen soll.\n\nAm 12. Dezember 2009 wurde die Version 3.1.1 veröffentlicht. Neu hinzugekommen ist die Unterstützung für die Sprache IronPython Version 2.6, eine verbesserte Quelltext-Verwaltung Python, eine Aktualisierung von NUnit auf die Version 2.5.3.9345, einige Korrekturen für den Debugger und Fehlerbereinigungen in der Quelltext-Vervollständigung.\n\nAm 13. Januar 2010 wurde die Version 3.2.0, als „Community Technology Preview“ (CTP), veröffentlicht. Dabei wurde die Unterstützung für die Sprache IronRuby hinzugefügt, die .NET-Reporting-Technik „SharpDevelop Reports“ (SDR) überarbeitet, erweitert und wieder eingefügt sowie die Unterstützung der Sprache „Boo“ für die Version 0.9.3.3457 aktualisiert.\n\nAm 6. Januar 2011 wurde die Version 4.0 veröffentlicht und mit der Unterstützung für .NET 4.0 erweitert.\n\nAm 6. Mai 2012 wurde die Ausgabe 4.2 veröffentlicht, in dessen vorausgegangenen Vorschau-Versionen unter anderem die Unterstützung für \".NET 4.5\" sowie für \"Windows 8\" hinzugefügt wurden.\n\nAm 23. Dezember 2013 wurde die Ausgabe 4.4 veröffentlicht.\n\nDie letzte aktuelle Version aus Version 4 ist 4.4.2 vom 14. April 2015.\n\nAm 28. Oktober 2014 wurde nach fünf Betaversionen und einem Release Candidate Version 5.0 (Codename: \"Zimnitz\") veröffentlicht. Neu in dieser Ausgabe sind neben der Unterstützung für .NET Framework 4.5.1 die Integration der neu implementierten quelloffenen Codeanalysebibliothek NRefactory und ILSpy-Debugging sowie zahlreiche kleine Verbesserungen wie ein neuer Add-In-Manager und Ressourceneditor. Die zum Teil weitreichenden architektonischen Änderungen durch die Umstellung des alten DOM auf die neue NRefactory-Bibliothek brachten jedoch nicht nur Vorteile mit sich. So wird beispielsweise nach jetzigem Stand nur noch C# und nicht mehr Visual Basic .NET von der IDE unterstützt.\n\nDes Weiteren wurde das Lizenzierungsmodell mit Beginn der Version 5.0 von der LGPL auf die MIT-Lizenz umgestellt.\n\nDurch die Möglichkeit, die grafische Benutzeroberfläche (GUI) komfortabel mit einem so genannten Formdesigner zu entwerfen, wird das UI-Design stark erleichtert. Insgesamt ähneln die Funktionen Microsofts Visual Studio. Die enthaltenen C#- und VB.NET-Parser wurden mit Coco/R erzeugt. Die Umgebung verfügt über die Funktion, VB.NET-Projekte nach C# zu übersetzen.\n\nDie unterstützten Funktionen im Einzelnen sind:\n\nDie ebenfalls freie Entwicklungsumgebung MonoDevelop für die freie .NET-Implementierung Mono stammt ursprünglich von SharpDevelop ab.\n\nAb der Version 2 wird der Microsoft .Net-Debugger cordbg als Debugger-Backend unterstützt. Für den Mono-Debugger mdb ist derzeit keine Unterstützung geplant.\n\n"}
{"id": "335783", "url": "https://de.wikipedia.org/wiki?curid=335783", "title": "Cupertino", "text": "Cupertino\n\nCupertino [] ist eine Stadt im Santa Clara County in Kalifornien mit rund 60.000 Einwohnern. Sie liegt westlich von San José, dem County-Hauptsitz und südlich von Sunnyvale und der Bucht von San Francisco. Die Stadt wurde nach dem italienischen Mönch und heiligen Josef von Copertino benannt, dessen Name sich wiederum von seinem Geburtsort Copertino in Italien ableitet.\n\nCupertino liegt im so genannten Silicon Valley. Der Computerhersteller Apple ist der mit Abstand größte Arbeitgeber, was sich durch den im April 2017 eröffneten Apple Park noch verstärken wird. Weitere Computertechnologie-Unternehmen wie die amerikanische Niederlassung von Trend Micro oder Zend Technologies (früher auch Symantec) haben ihren Sitz in Cupertino; weiterhin hat der irische Festplattenhersteller Seagate Technology hier seinen Verwaltungssitz.\n\nIm Mai 2016 beklagte Cupertinos damaliger Bürgermeister, Barry Chang, die fehlenden Steuereinnahmen zur Finanzierung der gestiegenen Kosten für Erhalt und Ausbau der städtischen Infrastruktur. Das gestiegene Verkehrsaufkommen in Verbindung mit den häufig genutzten Modellen zur Steuervermeidung führen zu einer zunehmenden Krise.\n\n\nPartnerstädte Cupertinos sind\n\nAußerdem bestehen freundschaftliche Beziehungen zu den Städten\n\n"}
{"id": "338028", "url": "https://de.wikipedia.org/wiki?curid=338028", "title": "High Color", "text": "High Color\n\nHigh Color ist ein Begriff aus der Computertechnik und bezeichnet eine Farbtiefe (maximale Anzahl der gleichzeitig darstellbaren Farben). Bei High Color mit Windows (16 Bit Farbtiefe, 2 = 65.536 Farben) werden meist je fünf Bit für die Anteile Rot und Blau und sechs Bit für Grün (weil das menschliche Auge dafür am empfindlichsten ist) reserviert und durch additive Farbmischung die resultierende Farbe bestimmt. Macintosh-Rechner können dagegen in der High-Color-Betriebsart nur 15 Bit Farbtiefe darstellen, d. h. 2 = 32.768 Farben.\n\nFür viele Anwendungen ist diese Farbtiefe ausreichend. Vor allem bei Farbverläufen, Fotos und Filmen kann es jedoch zu sichtbaren Abstufungen kommen; um dies zu umgehen, benötigt man True Color (hier gibt es keine Unterschiede von Windows zu Macintosh). High Color wird heute kaum noch verwendet. Früher wurde es auch für Computerspiele verwendet, da es die zu behandelnden Datenmengen für eine schnelle Bildfolge gegenüber True Color reduziert und so flüssigere bewegte Bilder ermöglichen kann.\n\nHigh Color wurde vor allem als Anzeigeformat bei Grafikkarten verwendet; die meisten Grafikformate unterstützen High Color nicht. Eine Ausnahme ist Windows Bitmap mit sogenannten „Bitmasken“, dieses Unterformat wird allerdings nur von wenigen Anwendungen unterstützt.\n"}
{"id": "339725", "url": "https://de.wikipedia.org/wiki?curid=339725", "title": "Vorschau (Software)", "text": "Vorschau (Software)\n\nVorschau (in der englischen Originalversion Preview) ist ein von Apple entwickeltes Programm zur Darstellung und Bearbeitung von Bildern, zur Darstellung und Annotierung von PDF-Dateien und zur Steuerung angeschlossener Scanner. Es unterstützt unter anderem die Dateiformate PDF, JPEG, JPEG 2000, TIFF, GIF, PICT, BMP, SGI, TGA, PSD, ICO, ICNS, PNG und EPS.\n\nVorschau ist standardmäßig in macOS integriert, die aktuelle Version ist 10.1 und Bestandteil von macOS Mojave.\n\nVorschau kann Bilder und PDF-Dokumente anzeigen, bearbeiten und in verschiedene Formate exportieren. Des Weiteren ist es möglich, mittels der Erweiterung \"Markierungen\", Linien, Formen und Text einem Dokument hinzuzufügen. Auch das Einfügen von Notizen und Unterschriften ist möglich.\n"}
{"id": "340148", "url": "https://de.wikipedia.org/wiki?curid=340148", "title": "Internetbetrug", "text": "Internetbetrug\n\nDer Begriff Internetbetrug beschreibt Betrugsdelikte im Rahmen der Internetkriminalität. Umgangssprachlich werden mit diesem Begriff auch Sachverhalte beschrieben, die nicht der juristischen Definition von Betrug entsprechen, sondern Bauernfängerei sind. Während manche Formen des Internetbetrugs ausschließlich im Internet vorkommen, stellen andere Varianten von Verhalten außerhalb des Netzes dar.\n\nDer Internetbetrug lebt unter anderem von dem massiven Informationsgefälle zwischen Opfer und Täter. Da viele Mechanismen im Internet sicher erscheinen, es aber nicht sind, fühlen sich die Opfer zu Unrecht sicher.\n\nEine bekannte Art des Internetbetruges ist das Phishing. Bei dieser Betrugsmethode wird eine gefälschte E-Mail an die potentiellen Betrugsopfer geschickt mit dem Inhalt, die Hausbank dieser Person hätte ein Computerproblem, verbunden mit der Bitte, die vertraulichen Daten (PIN, TAN etc.) an die Bank zu übermitteln. Dabei werden die E-Mail und die Internetpräsenz der Bank völlig authentisch nachgebildet. Werden die Daten an die gefälschte Website übertragen, haben die Betrüger nun die Möglichkeit, die Daten des Opfers missbräuchlich zu nutzen.\n\nAls Identitätsdiebstahl (auch Identitätsbetrug, Identitätsklau; engl. Identity Theft), wird die missbräuchliche Nutzung personenbezogener Daten (der Identität) einer natürlichen Person durch Dritte bezeichnet.\n\nDas Ziel eines Identitätsdiebstahls ist es in der Regel, einen betrügerischen Vermögensvorteil zu erreichen, Daten der betroffenen Person an interessierte Kreise zu verkaufen (illegale Auskunfteien) oder den rechtmäßigen Inhaber der Identitätsdaten in Misskredit zu bringen (Rufschädigung).\n\nDer Eingehungsbetrug ist eine besondere Erscheinungsform des Betrugs. Der Betrüger täuscht hierbei über seine Absicht, die ihm aus einem Vertrag erwachsenden Verpflichtungen zum Zeitpunkt der Fälligkeit auch tatsächlich zu erfüllen. Auf das Internet bezogen bedeutet dies vor allem das Angebot von Waren, die man gar nicht hat, in Online-Shops oder bei Onlineversteigerungen. \n\nDer Eingehungsbetrug im Internet setzt eine Zahlung im Voraus (z. B. über Kreditkarte) voraus. Eine Variante tritt bei Bestellungen aus nicht-EU Ländern auf. Man bekommt die Aufforderung, mit Direktüberweisung, zum Beispiel über Western Union, den Betrag zu überweisen. Dieses Zahlverfahren ist aus Käufersicht so unsicher wie eine Barzahlung im Voraus.\n\nWeiterhin gibt es den Informationsdiebstahl bei webbasierten Onlineberatungen und Kontaktportalen (beispielsweise Gesundheitsdaten, Problemstellungen, Lebensgewohnheiten, sexuelle Präferenzen). Informationsdiebstahl verläuft häufig unbemerkt von der betroffenen Person, da zunächst keinerlei Spuren vorhanden sind (im Gegensatz zum Onlinebanking – dort wird der Betrug spätestens durch die Abbuchung von Geldbeträgen sichtbar). Die durch den Betrug gewonnenen Informationen werden unter anderem von illegal arbeitenden Auskunfteien vermarktet.\n\nDer Unternehmer bietet dem Verbraucher an, nach einer einmaligen Registrierung eine Dienstleistung zu beziehen. Einige Zeit später (meist nach zwei Wochen) behauptet der Dienstleister, ein Dauerschuldverhältnis mit einer Vertragslaufzeit von mindestens zwei Jahren sei zustande gekommen, und die gesetzliche Widerrufsfrist sei abgelaufen. Der Dienstleister verlangt − häufig über einschlägig bekannte Rechtsanwälte oder Inkassobüros – Zahlungen für die Möglichkeit, die Dienstleistung zwei Jahre lang zu beziehen.\n\nEs gibt noch zahlreiche weitere Betrugsmöglichkeiten, die vornehmlich auf Gutgläubigkeit basieren. Ein Problem beim Internetbetrug besteht in der Schwierigkeit, ihn nachzuweisen (wenn er überhaupt bemerkt wird).\n\nKlauseln in Allgemeinen Geschäftsbedingungen, in denen sich die Kostenpflichtigkeit eines Internetangebots versteckt, sind für Verbraucher gemäß Abs. 1 BGB überraschend und unwirksam, wenn sie aufgrund des Erscheinungsbilds der Internetseite nicht mit einer kostenpflichtigen Leistung rechnen mussten.\n\n"}
{"id": "340350", "url": "https://de.wikipedia.org/wiki?curid=340350", "title": "DVD Studio Pro", "text": "DVD Studio Pro\n\nDVD Studio Pro war eine DVD-Authoring-Software des US-amerikanischen Unternehmens Apple. Sie bot die nahtlose Integration mit der Schnittsoftware \"Final Cut Pro\" und der Grafikanimationssoftware \"Apple Motion\". Mit Erscheinen der Version 4 im April 2005 war erstmals auch das Erstellen von DVDs mit HD-Material (High Definition, bis 1920 × 1080 Pixel) möglich.\n\nApple stellte das Produkt zusammen mit \"Apple Color\" ersatzlos ein.\n"}
{"id": "340364", "url": "https://de.wikipedia.org/wiki?curid=340364", "title": "Apple Motion", "text": "Apple Motion\n\nMotion ist eine Video-Software des US-amerikanischen Unternehmens Apple. Sie dient der Erstellung von Animationen für Film, Video, Fernsehen und DVD-Menüs. Das Programm bietet eine Echtzeit-Designumgebung und 3D-Grafikanimationen sowie die Integration in Apples \"Final Cut Pro\", wo z. B. über die Plugin-Schnittstelle die Texte eines Motion-Projektes geändert werden können.\n\nDie erste Version des Programms erschien im Sommer 2004. Die Software war Teil von Final Cut Studio, das unter anderem auch die Programme \"DVD Studio Pro\", \"Final Cut Pro\", \"LiveType\" und \"Soundtrack\" enthielt. Seit Juni 2011 wird \"Motion\" als eigenständige Software über die Online-Plattform \"Mac App Store\" vermarktet.\n\nAm 23. Juli 2009 stellte Apple \"Final Cut Studio 3\" vor, in dem auch \"Motion 4\" enthalten war.\n\nAm 21. Juni 2011 wurde \"Final Cut Pro X\" (Version 10.0) zusammen mit \"Motion 5\" und \"Compressor 4\" veröffentlicht. Seitdem sind die Applikationen nicht mehr über die Softwaresammlung \"Final Cut Studio\" erhältlich, sondern können als separate Programme über die Plattform \"Mac App Store\" bezogen werden.\n\n"}
{"id": "340385", "url": "https://de.wikipedia.org/wiki?curid=340385", "title": "Apple Cinema Display", "text": "Apple Cinema Display\n\nBei den Cinema Displays handelt es sich um eine Computerbildschirm-Modellreihe der Firma Apple. Der Nachfolger trägt den Namen Thunderbolt Display.\n\nDas erste Cinema Display war das 22″-Display im Acryl-Design des Power Mac G4, entworfen von Jonathan Ive, das im September 1999 auf den Markt kam.\nDie Auflösung betrug 1600 × 1024 Pixel. Auf der Rückseite befanden sich zwei USB-Anschlüsse. Bis zum Juni 2000 wurde die DVI-Schnittstelle verbaut, von Juli 2000 bis Januar 2003 der ADC-Anschluss, der neben dem Bildsignal auch die Stromversorgung des Bildschirms sowie das USB-Signal in einem Kabel über einen Anschluss zur Verfügung stellte.\n\nAb März 2002 kam das \"Apple Cinema HD Display\" mit 23″ und einer Auflösung von 1920 × 1200 Pixel, und ab Januar 2003 das 20″-Display mit einer Auflösung von 1680 × 1050 auf den Markt. Beide im Acryl-Design, mit ADC-Anschluss und je zwei USB-Anschlüssen.\n\nDie neuen Flachbildschirme erschienen im Sommer 2004 im neuen Alu-Design des G5 als 24″-, 27″- und 30″-Modelle.\n\nDie neueren Bildschirme sind mit einer DVI-Schnittstelle ausgerüstet. Für das 30″-Modell wird eine Grafikkarte mit Dual-Link-Anschluss benötigt. An Grafikkarten mit zwei Dual-Link-Ausgängen können zwei 30-Zoll-Bildschirme gleichzeitig betrieben werden.\n\nDas 30″ Cinema Display besitzt eine Auflösung von 2560 × 1600 Pixeln (100 ppi) und war zu seiner Einführung der größte verfügbare Computerbildschirm.\n\nDie Bildschirme dienen auch als USB- und je nach Modell teilweise auch als FireWire-Hub. Die Anschlüsse befinden sich auf der Rückseite im unteren Bereich.\n\nDie Verbindung mit dem Computer sowie dem Netzteil wird über ein einziges Kabel hergestellt, das sich am Ende auf einzelne Anschlüsse für das Grafiksignal (Mini Display Port oder DVI), USB und ggf. FireWire und die Stromversorgung verzweigt.\n\nIm Juli 2011 erschien das Apple Thunderbolt Display, welches die vorherige Reihe der Cinema Displays ablöst.\n\nAn Apples Cinema Displays wurde immer wieder Kritik geübt. Der einfachen Bedienbarkeit und dem simplen Anschlusskonzept (ein Kabel, das USB und FireWire sowie die Stromversorgung mit einschließt) stehen folgende Kritikpunkte gegenüber:\n\nApple Cinema Display 24″ (Alu-Design-Modell Ende 2009):\n\nApple Cinema HD Display 30″:\n\nApple Cinema HD Display 27″:\n\n"}
{"id": "341611", "url": "https://de.wikipedia.org/wiki?curid=341611", "title": "High Dynamic Range Image", "text": "High Dynamic Range Image\n\nEin High Dynamic Range Image (HDRI, \"HDR-Bild\", „Bild mit hohem Dynamikumfang“) oder Hochkontrastbild ist eine Rastergrafik, die große Helligkeitsunterschiede detailreich wiedergibt. Digitale Bilder mit geringem Dynamikumfang werden als \"Low Dynamic Range Images\" oder LDR-Bilder bezeichnet.\n\nHDR-Bilder können von Spezialkameras aufgenommen, als 3D-Computergrafiken künstlich erzeugt oder aus einer Belichtungsreihe von Fotos mit niedrigem Dynamikumfang (, LDR) rekonstruiert werden. Auf herkömmlichen Bildschirmen und Medien können sie nicht direkt dargestellt werden, sondern müssen in LDR-Bilder umgewandelt werden, indem die Helligkeitskontraste des HDR-Bildes verringert werden. Dieser Vorgang wird Dynamikkompression () genannt. Ungeachtet dieser Einschränkung können ausgehend von HDR-Bildern Über- und Unterbelichtungen vermieden, Bilddetails besser erhalten und weiterreichende Bildbearbeitungen vorgenommen werden. Nicht nur die Fotografie und Computergrafik, sondern auch Anwendungen wie die Medizin oder virtuelle Realität profitieren von diesen Vorteilen.\n\nDie meisten digitalen Bilder verwenden nur 256 Helligkeitsstufen (8 Bit) für jeden der Rot-, Grün- und Blau-Farbkanäle. Diese Farbtiefe reicht oftmals nicht aus, um die in natürlichen Szenen vorkommenden Helligkeitsunterschiede wiederzugeben. Höhere Farbtiefen werden üblicherweise kaum verwendet, da Bildschirme und Druckmedien zu deren Darstellung ohnehin nicht fähig sind.\n\nDie von einer Kamera oder einem Betrachter aus sichtbare Umgebung weist typischerweise einen Dynamikumfang (Verhältnis von größter und kleinster Leuchtdichte) in der Größenordnung von 10.000:1 auf. Der Dynamikumfang ist noch wesentlich größer, wenn eine Lichtquelle direkt sichtbar ist oder sowohl ein Innenraum als auch ein vom Sonnenlicht erhellter Außenbereich zu sehen sind. Die menschliche visuelle Wahrnehmung ist in der Lage, sich Lichtverhältnissen anzupassen, die über nahezu zehn Größenordnungen (ein Faktor von 10) reichen; innerhalb einer Szene sind bis zu ungefähr fünf Größenordnungen gleichzeitig sichtbar. In der Fotografie ist auch die Angabe des Dynamikumfangs in Lichtwerten (LW) üblich. Eine weitere Einheit zur Angabe des Dynamikumfangs ist das Dezibel (dB).\n\nIm Gegensatz zur visuellen Wahrnehmung leiden Fotografien, die mit herkömmlichen Digitalkameras erzeugt wurden, häufig an Über- und Unterbelichtungen. Beim High Dynamic Range Imaging werden Bilddateien mit einem Dynamikumfang erzeugt, der die in der Natur vorkommenden Helligkeiten in ihrer Gesamtheit besser erfassen kann. Die Pixelwerte stehen dabei in proportionalem Verhältnis zur tatsächlichen Leuchtdichte. Erst bei der Darstellung eines HDR-Bildes wird dessen Helligkeitsumfang geeignet reduziert. Auch wenn nach wie vor fast alle Bildschirme einen geringen Helligkeitsumfang besitzen, bieten HDR-Bilder Vorteile; so etwa bleiben ausgehend von HDR-Bildern Details in sehr dunklen und hellen Bereichen erhalten.\n\nDie physikalisch basierte Bildsynthese („Rendering“) war die vielleicht erste Anwendung von HDR-Bildern. Die von Greg Ward Larson ab 1985 entwickelte Rendering-Software Radiance verwendete intern Gleitkommazahlen zur Speicherung von Helligkeitswerten. Um die gerenderten Bilder speichern zu können, ohne dass Helligkeitsinformationen verloren gingen, entwickelte Ward das Radiance-HDR-Format. Auch Paul Debevec befasste sich mit HDR-Techniken, als er zur Simulation von Bewegungsunschärfe bei einer Computeranimation bewegte Glanzlichter mit hohem Dynamikumfang abspeicherte. Bereits 1968 hatten Oppenheim und andere in einem anderen Zusammenhang den ersten Tone-Mapping-Operator veröffentlicht; die dort vorgestellten Prinzipien wurden von einigen neueren Operatoren wiederentdeckt.\n\nDie Anwendungen von High Dynamic Range Imaging umfassen folgende Bereiche:\n\n\n\n\n\n\n\n\nEs gibt zwei gebräuchliche Möglichkeiten, die Pixelwerte in HDR-Bildern geräteunabhängig zu kodieren. Idealerweise nähern sich HDR-Kodierungen dem nichtlinearen Sinneseindruck des Auges auf Helligkeiten an. Dadurch wird vermieden, dass unterschiedliche Helligkeiten im Bild mit scheinbar unterschiedlicher Präzision kodiert werden und es zu sichtbaren Farbabstufungen kommt.\n\nEine Möglichkeit ist die logarithmische Kodierung, die Helligkeiten gemäß folgender Formel quantisiert:\n\"v\" ist hierbei die normalisierte kodierte Helligkeit, die Werte zwischen 0 und 1 annehmen kann. Aufeinanderfolgende Werte in dieser logarithmischen Kodierung haben ein konstantes Verhältnis von formula_2, wobei \"N\" die Anzahl der Quantisierungsschritte ist.\n\nHDR-Daten können auch mittels einer Kombination aus Mantisse und Exponent (als Gleitkommazahlen) kodiert werden. Aufeinanderfolgende Gleitkommazahlen haben kein konstantes Verhältnis, sondern folgen einem Sägezahn-artigen Verlauf. Damit die Farbquantisierung unsichtbar bleibt, darf die \"relative Abstufung\" (Differenz zwischen zwei aufeinanderfolgenden Helligkeitswerten, geteilt durch den Wert) 1 % nicht überschreiten.\n\nWeitere Eigenschaften von HDR-Kodierungen neben der relativen Abstufung sind der Farbraum und die Farbtiefe. Auf die Speicherung der absoluten Leuchtdichte (in der Einheit cd/m²) wird oft verzichtet.\n\nVon der Kodierung der Pixelwerte ist das verwendete Grafikformat zu unterscheiden, das bestimmt, in welche zusätzlichen Datenstrukturen die eigentlichen Bilddaten eingebettet werden. Einige HDR-Formate unterstützen mehrere Kodierungen.\n\nDie von den meisten Programmen unterstützten HDR-Formate sind verlustfrei komprimiert. Es wurde jedoch auch eine Erweiterung des JPEG-Formates zur verlustbehafteten Speicherung mit geringer Dateigröße entwickelt. Dieses „JPEG-HDR“-Bild speichert eine dynamikkomprimierte Version eines HDR-Bildes als gewöhnliche JFIF-Datei, fügt aber in einem zusätzlichen Marker ein \"Verhältnisbild\" hinzu, das die HDR-Informationen kodiert. JPEG-HDR ist ebenso wie andere JPEG-Formate für HDR-Bilder, etwa Kodaks ERI-JPEG oder das von der Software Panorama Tools verwendete FJPEG, zurzeit (2009) wenig verbreitet.\n\nDie herstellerabhängigen, internen Dateiformate von Digitalkameras (Rohdatenformate oder Raw-Formate) bieten mit 10 bis 14 Bit (linear kodiert, meist mit Offset, 1:1.024 bis 1:15.360) eine ähnliche Dynamik wie gewöhnliche sRGB-8-Bit-Bilder (nicht linear kodiert, 1:3.300), erreichen aber bei weitem nicht den Dynamikumfang von HDR-Bildern. Sie können noch als LDR-Formate oder allenfalls als \"Medium Dynamic Range,\" Formate mit mittelgroßem Dynamikumfang, bezeichnet werden. Dessen ungeachtet ist es möglich, Raw-Dateien in HDR-Formate zu konvertieren. Die als IEC-Norm veröffentlichte scRGB-Kodierung weist ebenfalls nur einen mittelmäßigen Dynamikumfang auf.\n\nDie folgende Tabelle bietet einen Überblick über verschiedene bekannte HDR-Formate und die darin verwendeten Kodierungen.\n\nHDR-Bilder können auf drei verschiedene Arten erzeugt werden: durch direkte Aufnahme mit Spezialkameras, indirekt aus einer Reihe unterschiedlich belichteter LDR-Bilder oder als künstliche Computergrafik.\n\nDigitale Bildsensoren mit hohem Dynamikumfang befinden sich in der Entwicklung. Zwar sind bereits einige dieser Produkte auf dem Markt, aber nur wenige umfassende Lösungen verfügbar. Der Preis für professionelle HDR-Kameras bewegt sich im Bereich von 50.000 US-Dollar (2008). Selbst hochwertige Bildsensoren sind jedoch noch nicht in der Lage, den Dynamikumfang beliebiger natürlicher Szenen vollständig abzudecken, insbesondere von Außenaufnahmen an einem sonnigen Tag. Zu den vermarkteten oder entwickelten Produkten zählen:\n\n\n\n\n\n\n\n\n\nNeben diesen Produkten zur direkten Aufnahme von HDR-Bildern gibt es Kameras für den Amateur- und semiprofessionellen Markt, die automatisch HDR- oder dynamikkomprimierte LDR-Bilder aus mehreren Aufnahmen mit unterschiedlichen Belichtungseinstellungen erzeugen können (siehe nächster Abschnitt). Hierzu reicht ein herkömmlicher Bildsensor aus. Als erste Kompaktkamera bot die \"Ricoh CX1\" im März 2009 diese Funktion in Form eines \"Doppelbelichtungsmodus\" zur „Erhöhung des Dynamikumfangs“ an.\n\nMit etwas Aufwand ist es möglich, auch mittels herkömmlicher Digitalkameras HDR-Bilder zu erzeugen. Dabei wird von der Szene eine Belichtungsreihe aufgenommen, bei der jede Bildregion in mindestens einem der Einzelbilder korrekt belichtet wird. Die Einzelbilder werden anschließend per Software zu einem HDR-Bild kombiniert. Wichtig ist dabei, dass sich das Motiv zwischen den einzelnen Aufnahmen nicht bewegt. Obwohl es bis zu einem gewissen Grad möglich ist, Verwacklungen nachträglich zu korrigieren, wird die Verwendung eines Fotostativs empfohlen.\n\nDamit aus der Belichtungsreihe korrekte Helligkeitsdaten berechnet werden, sind die Lichtwerte der Einzelbilder (oft ohnehin in den Exif-Einträgen der Bilddateien gespeichert) sowie die opto-elektronische Übertragungsfunktion der Kamera erforderlich. Da die Übertragungsfunktion von den meisten Herstellern nicht veröffentlicht wird, sollte sie selbst ermittelt werden, idealerweise anhand einer Kalibrierungsszene mit möglichst vielen Grautönen. Nach der Erzeugung des HDR-Bildes sollte die Linsenstreuung (Lens Flare) der Kamera herausgefiltert werden, um übermäßige Lichtstreuungen im Bild zu vermeiden.\n\nEin besonderes Problem mit fotografischen Techniken stellt die korrekte Aufnahme der direkt sichtbaren oder reflektierten Sonne dar, da es hier selbst bei kleinster Blende und Belichtungszeit zu massiven Überbelichtungen kommt. Die korrekte Leuchtdichte der Sonne kann mit Hilfe eines Neutraldichtefilters oder indirekt durch unterschiedliche Beleuchtungen einer diffus reflektierenden Kugel ermittelt werden.\n\nAuch von Durchsichtvorlagen wie Dias, Negativen und Filmstreifen können durch mehrfaches Scannen mit unterschiedlichen Belichtungen HDR-Bilder rekonstruiert werden (siehe etwa Multi-Exposure).\n\nNeuere Grafikkarten unterstützen das hardwarebasierte Echtzeitrendern mit hohem Dynamikumfang, oft \"High Dynamic Range Rendering\" (HDRR) genannt. Dies ist besonders bei Computerspielen sinnvoll, bei denen der Spieler oft zwischen dunklen und hellen Szenen wechselt. Auch Grafikeffekte wie Linsenstreuung wirken mit HDRR realistischer. Die erreichbare Präzision und der Dynamikumfang sind durch die zur Verfügung stehende Rechenleistung beschränkt.\n\nEine wichtige Technik bei der Bildsynthese ist das \"Image-based Lighting\" (IBL). Hierbei wird eine Szene vollständig von einem \"HDR Environment Map\" (auch \"Light Probe\") umhüllt. Environment Maps können am einfachsten mit Hilfe von rotierenden Kameras mit Fischaugenobjektiv aufgenommen werden. Alternativ kann eine die Umgebung reflektierende Kugel fotografiert oder mehrere Einzelfotos per Stitching kombiniert werden. Um IBL mit bekannten Renderverfahren wie Monte-Carlo-Raytracing zu kombinieren, ist eine besondere Abtaststrategie der Environment Map notwendig, damit das Bildrauschen gering bleibt. IBL lässt sich nicht nur dazu verwenden, um künstliche Szenen durch komplexe Lichtquellen zu beleuchten, sondern auch, um reale (Film-)Szenen um künstliche Objekte zu erweitern. IBL wird mittlerweile von allen größeren 3D-Renderern unterstützt.\n\nDiffus reflektierende Drucke sind prinzipiell LDR, da die maximale Helligkeit von der Umgebungsbeleuchtung abhängt. Um HDR-Bilder in Druckmedien darzustellen, müsste lichtemittierendes Papier erfunden werden. Es wäre allenfalls denkbar, Blendeffekte hinzuzufügen, um wie in der Malerei die Illusion eines helleren Lichtes zu erzeugen, als durch das Medium dargestellt werden kann. Möglich ist auch die Aufnahme gedruckter Bilder durch eine Kamera und anschließende Rückprojektion auf das Bild (siehe Superimposing Dynamic Range). Folien und fotografische Filme besitzen zwar einen (möglicherweise bis zu zehnmal) höheren Dynamikumfang als Drucke, sind aber in der Anwendung problematisch.\n\nKathodenstrahlröhrenbildschirme besitzen technisch gesehen einen hohen Dynamikumfang, weil sie zur Darstellung sehr geringer, nicht mehr wahrnehmbarer Helligkeiten fähig sind. In der Praxis ist dies jedoch irrelevant, da ihre maximale Leuchtdichte zu gering ist, als dass HDR-Bilder mit der erwünschten Wirkung angezeigt werden könnten. Herkömmliche Flüssigkristallbildschirme hingegen sind zwar zur Darstellung hoher Helligkeiten in der Lage, allerdings ist die Lichtstreuung in benachbarte Pixel recht hoch, was den effektiven Dynamikumfang begrenzt.\n\nErste Prototypen von HDR-Anzeigegeräten existieren spätestens seit 2004. Dazu gehört der HDR-Bildschirm DR37-P von BrightSide Technologies (vormals Sunnybrook Technologies, mittlerweile von Dolby übernommen). Bei diesem Bildschirm handelt es sich um einen Flüssigkristallbildschirm (LCD), der nicht von einer gleichmäßigen Lichtquelle, sondern von einer Matrix aus Leuchtdioden mit individuell regelbarer Helligkeit beleuchtet wird. Bilddetails werden vom LC-Bildschirm angezeigt, während die großen Helligkeitsunterschiede durch die Leuchtdioden moduliert werden. Die Leuchtdioden-Matrix kann eine geringe Auflösung besitzen, da Helligkeitsunterschiede in der Nähe heller Pixel ohnehin durch die Punktspreizfunktion des Auges maskiert werden. Die Helligkeit des Bildschirms reicht von 0,015 bis 3000 cd/m²; damit beträgt der Kontrastumfang etwa 200.000:1.\n\nWeitere Entwicklungen von HDR-Ausgabegeräten sind vor allem im Digitalkinobereich zu finden. Die meisten digitalen Projektionssysteme für Kinos basieren auf dem \"Digital Micromirror Device\" von Texas Instruments, einem Mikrospiegelaktor. Dabei handelt es sich um eine hochauflösende Matrix aus elektronisch gesteuerten Spiegeln, die Licht entweder auf eine Leinwand oder auf einen Absorber spiegeln können. Helligkeitsabstufungen entstehen durch Pulsweitenmodulation. Der praktische Dynamikumfang kommerzieller Mikrospiegelaktoren liegt bei etwa 500:1.\n\nUnter \"Tone Mapping,\" auch \"Tone Reproduction\" genannt, versteht man die Umwandlung eines HDR-Bildes in ein LDR-Bild, indem der Kontrastumfang verringert wird. Dies ist notwendig, um ein HDR-Bild angenähert auf einem herkömmlichen Anzeigegerät oder Medium darstellen zu können. Der naturgetreue Helligkeitseindruck geht dabei verloren. Umso wichtiger ist es, die besonderen Eigenschaften des HDR-Bildes, etwa den Detailreichtum in dunklen und hellen Bildregionen, so gut wie möglich beizubehalten. Tone-Mapping-Operatoren sind üblicherweise darauf abgestimmt, möglichst natürlich wirkende oder detailreiche Resultate zu erzeugen. Manche HDR-Software enthält jedoch auch Operatoren, die dem Anwender einen künstlerischen Spielraum lassen.\n\nMan unterscheidet verschiedene Arten von Tone-Mapping-Operatoren. Die einfachsten Verfahren verarbeiten jedes Pixel unabhängig voneinander. Diese \"globalen\" Tone-Mapping-Operatoren sind vergleichsweise schnell und eignen sich daher für Anwendungen, bei denen das Tone Mapping in Echtzeit stattzufinden hat. Sogenannte \"lokale\" oder \"frequenzbasierte\" Operatoren sind in der Lage, Bilder mit einem besonders großen Kontrastumfang ohne übermäßigen Detailverlust zu komprimieren. Hierbei werden Bildregionen mit hohem Kontrast stark, Regionen mit geringem Kontrast weniger stark komprimiert. Derartige Verfahren erfordern besondere Techniken, um Bildartefakte wie Halos zu vermeiden. Schließlich gibt es noch \"gradientenbasierte\" Verfahren, die die Helligkeitsgradienten des HDR-Bildes abschwächen.\n\nDass viele Tone-Mapping-Operatoren auf Erkenntnissen über die visuelle Wahrnehmung basieren, liegt nicht zuletzt daran, dass der Mensch selbst das Tone-Mapping-Problem scheinbar mühelos löst. So können Operatoren beispielsweise die helligkeitsabhängige Farb- und Schärfewahrnehmung simulieren, was besonders bei Nachtszenen zu realistischeren Ergebnissen führt. Das neuere iCAM06-Modell berücksichtigt eine Vielzahl von Effekten der menschlichen Wahrnehmung. Viele Tone-Mapping-Operatoren setzen absolute Helligkeitswerte voraus.\n\nEin Problem bei der Darstellung von HDR-Bildern sind Haloartefakte, die häufig beim Tone Mapping mit einfachen lokalen Tone-Mapping-Algorithmen entstehen. Moderne Tone-Mapping-Operatoren vermeiden derartige Artefakte; physiologisch basierte Operatoren wie iCAM06 liefern auch bei schwierigen Lichtverhältnissen plausible Ergebnisse.\n\nEinige HDR-Programme enthalten Tone-Mapping-Operatoren, die dem Benutzer absichtlich eine große Freiheit bei Parametereinstellungen lassen. Erik Reinhard kritisiert, dass dies den Benutzer dazu verleiten würde, Tone Mapping als Effektmittel zu missbrauchen. Halos, merkwürdige Kontraste und zu gesättigte Farben, die eigentlich von Unzulänglichkeiten des verwendeten Tone-Mapping-Algorithmus herrühren, würden von einigen Anwendern als künstlerische Effekte missverstanden. Dadurch entstünde der falsche Eindruck, HDRI wäre mit einem bestimmten „Stil“ verbunden. Christian Bloch ermutigt zwar zur kreativen Nutzung von Tone-Mapping-Operatoren, empfiehlt aber, das Resultat „impressionistische Fotografie“ oder „Hyperrealismus“, nicht aber irreführend „HDRI“ zu nennen.\n\nUnter den Bezeichnungen Exposure Blending, Exposure Fusion, „Dynamic Range Increase“ oder „Pseudo-HDR“ wurden Methoden vorgestellt, die unterschiedlich belichtete Bilder ausschließlich per Bildbearbeitung zusammenfügen, um über- und unterbelichtete Bereiche zu vermeiden. HDRI-Techniken können für den gleichen Zweck verwendet werden, indem aus den Einzelbildern ein HDR-Bild erzeugt wird, das anschließend per Tone Mapping in ein LDR-Bild umgewandelt wird. Exposure-Blending-Techniken haben jedoch nichts mit HDRI zu tun, da sie keinerlei HDR-Daten verarbeiten. Im Idealfall ist die Qualität der per Exposure Blending erzeugten Bilder mit denen des HDRI-Prozesses vergleichbar.\n\n→ \"Siehe auch: Liste von HDRI-Software unter HDR-Software\"\n\nHDR-Bilder werden in unterschiedlichem Maße von vollwertigen Bildbearbeitungsprogrammen unterstützt. Adobe Photoshop unterstützt ab der Version CS 2 den Import/Export sowie das Generieren von HDR-Bildern, bietet aber erst in folgenden Versionen Unterstützung für einige Malwerkzeuge und Filter. Das quelloffene CinePaint, eine für die Kinofilmproduktion überarbeitete Version des GIMP, kann ebenfalls mit HDR-Bildern umgehen.\n\nZudem existieren Programme, die sich auf die Anzeige, Generierung oder das Tone Mapping von HDR-Bildern spezialisiert haben. Zu den bekanntesten zählen die kommerziellen Anwendungen FDRTools Advanced und Photomatix, die Freeware-Programme Picturenaut, Photosphere und FDRTools Basic sowie die freie Software Luminance HDR.\n\n\n"}
{"id": "342911", "url": "https://de.wikipedia.org/wiki?curid=342911", "title": "Docuverse", "text": "Docuverse\n\nAls Docuverse bezeichnet Ted Nelson die elektronische universale Bibliothek, die durch einen Hypertext wie Xanadu entstehen soll; die zahllosen Dokumente sind zwar verteilt gespeichert, aber miteinander verbunden, so dass ein Metadokument entsteht:\n\nEr sieht sich selbst wie auch das Xanadu-Projekt dabei als \"\"Literary Machines\"\"; das \"Docuverse\"-Paradigma wurde jedoch nicht in Xanadu umgesetzt, sondern – mit einigen funktionalen Abstrichen – im World Wide Web, das in vielen Aspekten den Gegenentwurf zum Druck-Paradigma bildet.\n\n\n\n"}
{"id": "343121", "url": "https://de.wikipedia.org/wiki?curid=343121", "title": "Distributed Numerical Control", "text": "Distributed Numerical Control\n\nDistributed Numerical Control (DNC) bezeichnet in der Fertigungstechnik die Einbettung von computergesteuerten Werkzeugmaschinen (CNC-Maschinen) in ein Computernetzwerk. Die Bearbeitungsprogramme werden bei Bedarf mit Hilfe des DNC-Systems von einem der angeschlossenen Computer in die Steuerung der Maschine geladen. DNC-Systeme ermöglichen die Abkehr vom Lochstreifen oder von der Diskette als Datenträger. Möglich wurde sie erst durch die Entwicklung einer Netzwerktechnologie, die sich als relativ störunanfällig gegenüber den in Werkshallen herrschenden Verhältnissen (große zu überbrückende Entfernungen, starke elektromagnetische Felder, Öl- und Säurebeständigkeit, unempfindlich gegenüber Temperaturschwankungen) darstellt.\n\nAls Medium werden in den meisten Fällen Twisted-Pair-Kabel verwendet. Auch die Realisierung anhand serieller Schnittstellen auf Basis des EIA-232-Standards wird in der Praxis noch häufig angewandt. Auch Funk-DNC und WLAN-DNC ist heute schon gängig. \n\nHäufig werden handelsübliche PCs verwendet und es können pro PC bis zu 256 CNC-Maschinen verarbeitet werden. Die DNC-Technik ist auch fähig, bei Maschinen mit kleinem Hauptspeicher, bei deren Einsatzzwecken die Neuanschaffung einer neuen Hauptplatine mit mehr Hauptspeicher sich nicht rentiert, kontinuierlich Befehlssätze nachzuladen.\n"}
{"id": "343266", "url": "https://de.wikipedia.org/wiki?curid=343266", "title": "Subdivision Surface", "text": "Subdivision Surface\n\nEine Subdivision Surface (deutsch: \"Unterteilungsfläche\") ist in der Computergrafik eine glatte (in der ersten oder mehrfachen Ableitung stetige) Fläche, die aus einem Ausgangsgitter (auch Kontroll-Polygonnetz genannt) erzeugt wurde. Eine Subdivision Surface ist ursprünglich als der Grenzwert (Limes) eines unendlichen, rekursiven Verfeinerungsschemas definiert. Dieses Verfeinerungsschema wird auch als \"Subdivision Schema\" bezeichnet; der Grenzwert als Limesfläche.\n\n\nVerfeinerungsschemata können grob in zwei Kategorien eingeteilt werden: interpolierende und approximierende. Interpolierende Schemata werden benutzt, wenn die Limesfläche die Punkte des Ausgangsgitters interpolieren soll. Approximierende Schemata leisten dies nicht; die Limesfläche kann innerhalb oder außerhalb des Ausgangsgitters zu liegen kommen. Oft ist bei approximierenden Schemata das Ausgangsgitter die konvexe Hülle der Limesfläche.\nGenerell erzeugen die meisten bekannten approximierenden Schemata ästhetisch ansprechendere Limesflächen.\n\nDas andere Unterscheidungskriterium, das auch Verwendung findet, ist die Kategorisierung in Schemeta, die nur auf Gittern aus Polygonen mit bestimmter Punktzahl bestehen. Einige solcher Schemata benötigen beispielsweise ein Ausgangsgitter, das nur aus Dreiecken oder Vierecken besteht.\n\nViele Schemata sind auch nur auf mannigfaltigen Ausgangsgittern definiert.\n\nApproximierend meint, dass die Limesfläche das Ausgangsgitter approximiert (annähert) und die bei jedem Rekursionschritt neu erzeugten Punkte in der Regel nicht auf der Limesfläche liegen. Beispiele für approximierende Schemata sind:\n\nInterpolierend heißt, dass die Punkte des Ausgangsgitters und die durch jeden Rekursionsschritt neu erzeugten Punkte immer auf der Limesfläche liegen. Beispiele für interpolierende Schemata sind:\n\n"}
{"id": "347428", "url": "https://de.wikipedia.org/wiki?curid=347428", "title": "Norton Commander", "text": "Norton Commander\n\nDer Norton Commander (meist verkürzt NC) ist ein Dateimanager für DOS und Windows. Das Programm diente in seiner Grundkonzeption als Vorbild für eine ganze Gruppe ähnlicher Dateimanager, die sich selbst meist als Norton-Commander-Klone bezeichnen.\n\nDer Hauptkonkurrent in den 1980er Jahren war der DOS-Dateimanger Xtree, der bereits 1985 veröffentlicht worden war. Auch von Xtree gibt es zahlreiche Klone.\n\nDas Programm wurde 1984 vom damaligen Physikstudenten John Socha unter dem Namen \"Visual DOS\", kurz \"VDOS\", entwickelt, der es 1986 unter dem Namen \"Norton Commander\" bei der Firma Peter Norton Computing, die später auch für das bekannte Produkt Norton Utilities verantwortlich war, veröffentlichte. In den darauf folgenden Jahren erschienen noch drei weitere Versionen des Dateimanagers für das Betriebssystem DOS. Nach der Herausgabe des Norton Commander 4 wurde Norton Computing 1991 vom Unternehmen Symantec aufgekauft. Symantec veröffentlichte 1995 Version 5 des Norton Commander und danach 1998 noch Versionen 5.51 für MS-DOS und 2.01 für Windows 95/98/ME und Windows NT. Da aber mittlerweile mehrere andere Dateimanager für Windows als auch für DOS die Norton-Commander-Architektur kopierten und mit allerlei Erweiterungen besser waren als das Original, wurde die Norton-Commander-Reihe aufgrund mangelnder Verkaufszahlen komplett eingestellt.\n\nDer Norton Commander bietet unter DOS eine textbasierte Benutzeroberfläche, die aus zwei Fenstern und einer DOS-Kommandozeile besteht. Diese zweispaltige Präsentation von zwei individuellen Ansichtsfenstern wird auch als Zwei-Panel-Konzept bezeichnet, von oder \"\". Jedes Fenster, oft auch Spalte oder Panel bezeichnet, stellt in der Regel einen Ordner mit Dateien und Unterverzeichnissen dar, die sich auf verschiedene Arten anzeigen und sortieren lassen – etwa nach dem Namen, der Größe oder dem Änderungsdatum. Auf diese Weise lassen sich Dateien und Verzeichnisse leicht kopieren, verschieben und miteinander vergleichen.\n\nDieses zweispaltige Bedienkonzept ist eine große Erleichterung, verglichen mit der DOS-Kommandozeile, wo dafür oft mühsam längere Befehle fehlerfrei einzutippen sind.\n\nTextdateien können mit einem integrierten Editor angelegt und bearbeitet werden. Der integrierte Dateibetrachter kann auf Wunsch Dateien auch im Hexadezimalformat anzeigen. Zusätzlich bieten spätere Versionen neben anderen Funktionen die Möglichkeit, Formate aller Art (wie etwa Bilddateien und Sounddateien) zu öffnen und Archive anzulegen.\n\nTypisch für das Programm ist, dass sich alle Funktionen über eine Vielzahl von Tastenkürzeln auslösen lassen. Oftmals übernahmen die Klone diese Kürzel, um Benutzern den Umstieg zu erleichtern. Damit ist der Norton Commander zum De-facto-Standard für zweispaltige Dateimanager avanciert. Die Hauptfunktionen des originalen Norton Commander sind schnell und einfach über die Funktionstasten abrufbar und beziehen sich in den meisten Fällen auf die Auswahl im aktiven Fenster. Diese sind auch unten eingeblendet:\n\nVor allem die Dateioperationen bis für Anzeigen, Bearbeiten, Kopieren, Umbenennen oder Verschieben, Verzeichnis erstellen und Löschen, wurden in allen Norton-Commander-Klonen übernommen. Die Funktionstaste ist ohnehin in den meisten Betriebssystemen der Standard, um die Hilfefunktion aufzurufen. Zu DOS-Zeiten wurde die Taste oft zum Beenden genutzt. Obwohl das Menü unter Windows auch über das Drücken und wieder Loslassen der -Taste möglich ist, wurde von Klonen auch oft die Taste übernommen. Dennoch, sowie und haben oft neue Funktionen erhalten, um mit den Konventionen aktueller Betriebssysteme nicht zu brechen.\n\nAb Version 4 enthielt der Norton Commander ein Terminalemulationsprogramm mit der Bezeichnung \"Term90\"; in der Version 5.0 lautete die Bezeichnung \"Term95\". \"Term90\" und \"Term95\" waren für viele Modemneulinge die erste Software zur Datenkommunikation mit einem Modem.\n\nIn der Mitte der 1980er Jahre entstanden viele ähnliche Programme, die als Dateimanager die Arbeit unter MS-DOS oder PC DOS einfacher machen sollten. Darunter waren neben dem Hauptkonkurrenten Xtree auch Programme wie Q-DOS oder PathMinder. In den 1990er Jahren wurden zwar sowohl XtreeGold als auch der Norton Commander auf Betriebssysteme mit grafischen Benutzeroberflächen portiert, allerdings aufgrund mangelnden kommerziellen Erfolgs bis zur Jahrtausendwende vollständig eingestellt. Doch gelten der Norton Commander und Xtree bis heute als Synonyme für orthodoxe Dateimanager und wurden daher vielfach für die unterschiedlichsten Betriebssysteme geklont.\n\nDer Norton Commander für Windows war eine Windows 95-Variante des klassischen DOS-Dateimanagers.\n\nDie Version 1.0 wurde erstmals 1996 veröffentlicht. Es unterstützte Windows 95 und Windows NT.\n\nDiese Version ist vollständig mit Windows-Funktionen wie dem Papierkorb und QuickView (Schnellansicht) integriert. Die Quick View-Funktion wurde über die im Lieferumfang enthaltene grundlegende Quick View Plus-Funktion unterstützt.\n\nMit der Version 1.02 des Norton Commander wurde auch Windows 98 unterstützt.\n\nDie Version 2.0 wurde im Jahr 2000 veröffentlicht. Es unterstützt Windows 2000 und funktioniert unter Windows XP, Windows Vista und Windows 7. Der Installer enthielt die Norton Network Utilities, den Norton Commander Scheduler und den Norton Commander.\n\nDie Norton Network Utilities ermöglichten die Anzeige von Geräten und Systemen im Netzwerk, die Verbindung zu Remote-Systemen, die Zuordnung von Netzwerklaufwerken, die Netzwerküberwachung und vieles mehr.\n\nDer Norton Commander unterscheidet sich kaum von früheren Versionen und umfasst Dateikomprimierung- und dekomprimierung verschiedener Formate, Netzwerkdienstprogramme, Datenträgerbereinigung, Dateien- und Ordnervergleich sowie FTP-Verbindungsverwaltung.\n\nDie letzte Windows-Version des Norton Commander war die Version 2.01.\n\nDie Version 1.0 wurde im Dezember 1992 veröffentlicht. Sie unterstützte OS/2 2.0 mit HPFS- oder FAT-Dateisystem.\n\nSie enthielt nicht die Eingabeaufforderung wie andere Versionen des Norton Commander.\n\nIm Juni 1993 senkte Symantec den Preis des Norton Commander für OS/2 auf 49 US-Dollar und stellte bald darauf den Verkauf ein.\n\nDer Norton Commander gehörte zu den populärsten Programmen der DOS-Ära und wurde von verschiedenen Unternehmen, oft als Shareware, nachprogrammiert. Zu den bekannteren dieser Klone gehören (alphabetisch geordnet):\n\n"}
{"id": "347762", "url": "https://de.wikipedia.org/wiki?curid=347762", "title": "Papyrus Autor", "text": "Papyrus Autor\n\nPapyrus Autor (vor 2008 Papyrus) ist ein Schreibprogramm für Schriftsteller und gleichzeitig ein Office-Paket (daher vormals auch Papyrus Office) der Berliner Firma R.O.M. Logicware GmbH. Das Programm wird heute für Windows und macOS angeboten und weiterentwickelt, in der Vergangenheit auch für Atari-TOS (seit 1992) und OS/2 (seit 1995). Es verwendet die Dateiendung codice_1.\n\n\"Papyrus Autor\" (seit 2008) enthält das früher selbständige Papyrus Office und richtet sich an Schriftsteller und Schreiber von „Texten besonderer Wichtigkeit“ wie z. B. Diplomarbeiten, da es eine Steigerung der Stilqualität und Lesbarkeit sowie der Übersichtlichkeit von Texten anstrebt:\n\nDie Basisversion \"Papyrus Office\" (letzte Version 17.10 vom 2. September 2012, danach in Papyrus Autor aufgegangen) ist eine Kombination von Textverarbeitung, Tabellenkalkulation und Datenbank. Sie enthält auch Desktop-Publishing-Funktionalitäten. Von Version 12.5x an kann Papyrus auch PDF-Dateien erzeugen – entweder eine reine PDF-Datei oder ein Hybrid, das neben der Anzeige in herkömmlichen PDF-Anzeigeprogrammen ein Editieren mit Papyrus gestattet.\n\nDie Menüs der Benutzeroberfläche weichen etwas von den gewohnten Schemata ab, lassen sich aber auf Wunsch an die in Microsoft Office übliche Strukturierung anpassen.\n\nAls Light-Variante von Papyrus Office wurde in den frühen 2000er Jahren gelegentlich als Beilage zu Computerzeitschriften eine kostenlose eingeschränkte Version mit der Bezeichnung \"Papyrus Works\" veröffentlicht.\n\n"}
{"id": "348608", "url": "https://de.wikipedia.org/wiki?curid=348608", "title": "BackTrack", "text": "BackTrack\n\nBackTrack (zu Deutsch etwa \"Zurückverfolgung\") war eine Linux-Distribution zur Überprüfung der Sicherheit einzelner Rechner und gesamter Netzwerke sowie zur forensischen Analyse angegriffener Systeme. Die Distribution konnte von einer Live-CD, einem USB-Stick oder über ein Netzwerk gebootet werden. Mit Veröffentlichung der Nachfolgedistribution Kali Linux im März 2013 wurde BackTrack eingestellt.\n\nZu Beginn der Entwicklung existierten zwei voneinander unabhängige Distributionen \"Auditor Security Collection\" und \"Whoppix/Whax\". Am 5. Februar 2006 beschlossen die Entwickler, sie zusammenzuführen. Dadurch entstand BackTrack als neue Distribution. Es basiert auf der Linux-Distribution Slackware mit den Slax-Skripts. Dies geschah hauptsächlich deshalb, weil die Modularität von Slax es ermöglicht, Anpassungen durch einen Modulaustausch besonders leicht vorzunehmen.\nMit Version 4 wurde die Entwicklung auf Basis von Debian fortgesetzt. Damit wurde die Skalierbarkeit (Advanced Packaging Tool) größer und eine Aktualisierungsmöglichkeit hergestellt. Somit ist es nun möglich und gewollt, die Distribution zu installieren und kontinuierlich auf dem neuesten Stand zuhalten. Die am 10. Mai 2011 herausgekommene Version 5 basiert nun auf Ubuntu 10.04 LTS.\n\nBackTrack 2 wurde am 6. März 2007 vorgestellt. Es zeichnet sich durch eine Vielzahl von unterstützten Hardwareplattformen aus und beherbergt in 12 Kategorien über 300 Sicherheitstools. Darunter sind populäre Werkzeuge wie Nmap, Ettercap oder Wireshark, aber auch viele unbekannte Programme. Nessus hingegen erhielt aufgrund von Lizenzierungsproblemen keinen Einzug in das System. Als Desktop-GUI kommt KDE zum Einsatz. Kleine Hilfsprogramme ermöglichen die Installation auf der Festplatte sowie auf einem USB-Stick.\n\nAm 14. Dezember 2007 wurde die erste Betaversion von BackTrack 3 veröffentlicht, am 19. Juni 2008 schließlich die Finalversion. Zuvor gab es erste Einblicke in Form eines Video-Teasers. Erstmals steht neben der 700 MB großen CD-Version auch eine gleich große VMWare-Version und eine etwa 800 MB umfassende USB-Stick-Version zum Download. Das System basiert weiterhin auf Slax. Der Linux-Kernel wurde auf die Version 2.6.21.5 aktualisiert. Es soll nun auch möglich sein, vorgenommene Einstellungsänderungen nachträglich auf die CD brennen zu können. Dazu muss die CD als Multisession-CD gebrannt sein. Überdies wurden eine Cluster-Unterstützung für John the Ripper sowie diverse Leistungsverbesserungen eingepflegt.\n\nAm 11. Februar 2009 wurde die Betaversion von BackTrack 4 veröffentlicht.\nDie neue Version setzt nun auf den Debian-Core und Ubuntu-Pakete. Des Weiteren wurde für beiliegende Programme CUDA-Unterstützung eingebaut. Da BackTrack 4 über 1 GByte groß ist, wird es nur noch als DVD-Abbild angeboten.\nAm 12. Januar 2010 wurde nach einer sehr langen Testphase die offizielle Version freigegeben. Zeitgleich startete eine neue Webpräsenz, welche sich unabhängig vom Hersteller \"Remote Exploit\" präsentiert.\n\n19. November 2010: Backtrack 4 R2 Codename „Nemesis“ veröffentlicht.\n\nAm 10. Mai 2011 wurde BackTrack 5, Codename „revolution“, veröffentlicht. Es basiert auf Ubuntu Lucid mit dem Kernel 2.6.38.\nIn dieser Version wurde \"Armitage\", eine grafische Oberfläche für Metasploit 3.7.0, hinzugefügt. Des Weiteren wurde ein „Stealth-Modus“ hinzugefügt, mit dem kein Netzwerkverkehr generiert wird.\nDer Support von BackTrack 4 wurde mit der Veröffentlichung von BackTrack 5 eingestellt.\nAm 18. August 2011 wurde die korrigierte Version Backtrack 5 R1, am 1. März 2012 Version 5 R2 und am 13. August 2012 Version 5 R3 veröffentlicht.\n\nDie Distribution beinhaltet eine große Anzahl an Werkzeugen für die unterschiedlichsten Netzwerktests und Schulungen. Hier eine kleine Auflistung der wichtigsten Werkzeuge:\n\nBackTrack beinhaltet Softwaretools, die zum Teil Sicherheitsvorkehrungen umgehen und die nach Inkrafttreten des sogenannten Hackerparagrafen (§ 202c StGB) in Deutschland als \"Computerprogramm zum Ausspähen von Daten\" aufgefasst werden. Somit kann bereits der Besitz oder Vertrieb strafbar sein, sofern die Absicht zur illegalen Nutzung nach § 202a StGB oder § 202b StGB besteht.\n\nEnde 2008 stellte der Chefredakteur von \"iX\" gegen sich selbst Strafanzeige wegen des Hackerparagrafen, nachdem er in einem Sonderheft die BackTrack-CD beigelegt hatte. Die Anzeige wurde 2009 von der Staatsanwaltschaft «aus rechtlichen Gründen» abgelehnt.\n\nEinen ähnlichen Ansatz, die Systemsicherheit eines Rechners sowie ganzer Netzwerke zu prüfen, verfolgen Distributionen wie PHLAK und Knoppix STD. Auch das BSI hat eine Distribution namens BOSS veröffentlicht.\nEs gibt bereits auf BT basierende Ableger wie Damn Vulnerable Linux.\n\n\n"}
{"id": "349223", "url": "https://de.wikipedia.org/wiki?curid=349223", "title": "Cyber-Terrorismus", "text": "Cyber-Terrorismus\n\nCyber-Terrorismus ist eine spezielle Form des Terrorismus, der mit Hilfe von Internet-Technologien Angriffe auf Computersysteme verübt. Es gibt sehr kontroverse Meinungen über den Cyber-Terrorismus: Von den gänzlich Überzeugten einer permanenten Bedrohung, bei der Schreckensszenarien mit Tausenden von Toten durch falsch gesteuerte Schleusentore, veränderte Zusammensetzung von Medikamenten oder gar atomare Katastrophen durch Überlisten der Sicherheitssysteme und Manipulieren der bestehenden Programme befürchtet werden, über die gelegentlichen Warner bis zu denen, die von bloßer „Panikmache“ sprechen und den Cyber-Terrorismus für irreal halten.\n\nDie Unterscheidung zwischen unpolitischen kriminellen Aktionen und Aktivitäten mit terroristischer (also politisch- oder religiös-extremistischer) Motivation, aber auch mit nationalstaatlicher (Geheim)kriegsführung ist mitunter schwer oder gar nicht möglich. In diesem Kontext ist zwischen zwei Unterarten des Cyber-Terrorismus unterscheiden: Reiner Cyberterrorismus, der einzig mit Computern arbeitet und rein virtuelle Angriffe startet, und Terrorismus, der mit Hilfe der Computertechnologie andere Attentate ermöglicht, sie unterstützt, propagiert oder auch nur nachrichtendienstlich die (elektronische) Kommunikation zwischen den einzelnen Zellen oder deren Führungskadern sicherstellt.\n\nWährend etwa der deutsche Verfassungsschutz darauf beharrt, das Internet sei das zentrale Instrument zur Propagandaverbreitung und Nachwuchsrekrutierung von Terroristen, erklärte Stephen Cummings, Chef der britischen Behörde zum Schutz kritischer Infrastrukturen, auf einer Cyber-Security-Konferenz in London Mitte April 2008 schlicht: \"Cyberterrorismus ist ein Mythos.\"\n\n\n"}
{"id": "352064", "url": "https://de.wikipedia.org/wiki?curid=352064", "title": "Partikelsystem", "text": "Partikelsystem\n\nEin Partikelsystem ist eine Funktion im Bereich der Computeranimation, mit der sich eine große Anzahl von Objekten animieren lässt. Partikelsysteme werden beispielsweise eingesetzt, um Feuer-, Rauch- oder Explosionseffekte zu simulieren.\n\nBei einem Partikelsystem werden von einem so genannten Emitter Partikel ausgestoßen, deren Bewegung über zahlreiche Parameter beeinflusst werden kann. Diese sind unter anderem:\n\n\nDie Partikel sind zunächst nur logische Elemente (rechts im Bild), denen grafische Eigenschaften zugewiesen werden müssen, damit sie sichtbar werden. Zum Beispiel kann jedes Partikel durch ein geometrisches Objekt ersetzt werden, wodurch sich Schwärme, Asteroidenfelder und Ähnliches darstellen lassen (Mitte im Bild). Mit entsprechenden Materialzuweisungen können Partikelsysteme auch eingesetzt werden, um Rauch, Nebel oder Feuer darzustellen (links im Bild).\n\nWeitergehende Partikelsysteme können auch auf andere Objekte reagieren, von denen sie je nach Einstellung angezogen oder abgestoßen werden oder an denen sie abprallen (physikalische/ereignisbasierende Verhaltensweisen).\n\nDurch die Darstellung der Bewegungspfade der einzelnen Partikel werden Haare, Plüsch, Grasflächen und ähnliche Strukturen realisiert.\n\n"}
{"id": "355888", "url": "https://de.wikipedia.org/wiki?curid=355888", "title": "Turbolinux", "text": "Turbolinux\n\nTurbolinux ist ein japanisches Softwareunternehmen und eine Linux-Distribution.\n\n1992 gründeten Jeff „Cliff“ Miller und seine Frau Iris das Unternehmen Pacific HiTech in Kalifornien. Die Ausrichtung des Unternehmens zielte hauptsächlich auf den asiatischen Markt, zuerst in Japan, dann die Volksrepublik China und Südkorea. Das Unternehmen benannte sich 1999 in \"Turbolinux\" um.\n\nAuf dem Höhepunkt der Dotcom-Blase hatte die Firma viele Investoren und hat darum eine sehr bewegte Historie. Allein im Jahr 2000 flossen insgesamt 87 Millionen US-Dollar Wagniskapital in den Betrieb; sie expandierte unter anderem nach Europa und eröffnete weltweit zahlreiche Niederlassungen. Ein Gang an die Börse schien ganz kurz bevorzustehen.\n\nInnerhalb von Jahresfrist waren aber die meisten Niederlassungen wieder geschlossen. Häufige Wechsel des Managements gingen mit immer wieder anderen Firmenstrategien einher, die eher schadeten statt halfen. Die Gründer Jeff und Iris Miller hatten das Unternehmen schon im Jahr 2000 verlassen. Turbolinux zog sich auch aus Deutschland und Europa zurück. Im Sommer 2001 schien eine Fusion mit dem ebenfalls schon stark angeschlagenen Dienstleister Linuxcare unmittelbar bevorzustehen. Dies scheiterte im letzten Moment jedoch. 2002 wurde Turbolinux von SRA übernommen. SRA (Software Research Associates) ist ein seit 1967 bestehendes Systemhaus mit starken Verbindungen zur freien Software. Einer der bekanntesten Beschäftigten bei Turbolinux ist der PostgreSQL-Kernentwickler Tatsuo Ishii. Turbolinux ist dennoch eine der populärsten Linux-Distributionen überhaupt, besonders in Südasien (China, Japan).\n\nBekannt wurde Turbolinux in Deutschland durch United Linux, eine Beteiligung an der Allianz von SuSE, Conectiva und Caldera.\n\nTurbolinux wird jeweils als Desktop- und als Server-Edition angeboten. Diese Versionen tragen die gleiche Versions-Nummer und werden durch ein nachgestelltes \"D\" oder \"S\" gekennzeichnet.\n\nTurbolinux ist nur für i386-kompatible Computer erhältlich. Standardmäßig ist bei der Desktop-Edition KDE die Desktop-Umgebung. Gnome gehört auch zum Lieferumfang. Die Server-Edition setzt auf Xfce als Desktop-Umgebung. Turbolinux verwendet RPM als Paketmanager und OpenOffice.org als Büro-Suite. Zusätzlich enthält Turbolinux u. a. folgende proprietäre Software:\n\n\n"}
{"id": "358930", "url": "https://de.wikipedia.org/wiki?curid=358930", "title": "Inkscape", "text": "Inkscape\n\nInkscape (Kofferwort aus , „Tinte“ und \"-\" wie in \"\", „Landschaft“) ist eine freie, plattformunabhängige Software zur Bearbeitung und Erstellung zweidimensionaler Vektorgrafiken. Das Programm eignet sich zum Erstellen einseitiger Dokumente wie Logos, Vektorkunst, technischen Diagrammen, Landkarten, Stadtplänen, Flugblättern, CD-Motiven, Postern, Schriftzügen, Comics usw.\n\nArbeitsdateien werden in einem Format gespeichert, welches auf dem quellenoffenen, auf XML-basierenden Scalable Vector Graphics W3C-Standard basiert. Daher lassen sich Arbeitsdateien ohne Konvertierung in vielen Bildbetrachtern darstellen.\n\nObjekte können auf unterschiedlichen Wegen angelegt werden. Es lassen sich grafische Primitive, Bézier-Kurven, bewegliche Verbindungen und Textfelder erzeugen, Rastergrafiken importieren sowie Freihandlinien und kalligraphische Figuren zeichnen. Alle Objekte lassen sich mit booleschen Operationen an anderen Objekten schneiden oder miteinander vereinen. Zudem lassen sich geschlossene Pfade mit Polygonen füllen, welche ihre Form autonom an die Form des Pfades anpassen. Objekte und Pfade lassen sich durch Koordinatentransformationen sowie Verschiebung einzelner Punkte manipulieren. Texte lassen sich hart formatieren oder auf einen beliebig geformten Pfad legen. Fliesstexte können über mehrere Textblöcke verteilt dargestellt werden. Es lassen sich sowohl einzelne Farben als auch Farbverläufe oder Masken auf Objekte projektieren. Farblich komplexere Oberflächen lassen sich mit Hilfe der integrierten Filter erstellen. Es gibt auch diverse Filter, welche künstlerische Effekte erzeugen und die Form von Pfaden verändern. Neben Vektorgrafiken lassen sich auch Rastergrafiken in Inkscape-Projekte importieren und vektorisierten. Seit der Version 0.92 kann Inkscape PDFs und PostScript 3 – welche komplexe Farbverläufe unterstützten – exportieren.\n\n\nInkscape nutzt hauptsächlich das Dateiformat Scalable Vector Graphics (SVG). Alle anderen Dateiformate, die das Programm importiert oder exportiert, werden zu SVG konvertiert. Inkscape nutzt in die SVG-Daten eingebettet Cascading Style Sheets (CSS). Es setzt jedoch weder den gesamten Standard von SVG, noch den von CSS um. Der bedeutendste nicht unterstützte Teil betrifft die im SVG-Standard enthaltenen Möglichkeiten für die Animation.\n\nInkscape kann standardmäßig die folgenden Dateiformate importieren:\n\nMit Hilfe eines Plugins können folgende Dateiformate importiert werden:\n\nInkscape kann standardmäßig die folgenden Dateiformate exportieren:\n\nInkscape entstand 2003 als Abspaltung des Vektorzeichenprogramms Sodipodi auf Grund von Unstimmigkeiten über die Ziele und den Entwicklungsweg. Im Zuge der Aufspaltung wechselte man von der Programmiersprache C zu C++ und von der Bibliothek GTK+ zur C++-Variante gtkmm. Eine weitere große Änderung betraf die Benutzeroberfläche: Die Benutzeroberfläche von Sodipodi orientierte sich an GIMP und CorelDRAW, die neu gestaltete Benutzeroberfläche von Inkscape hingegen an Xara Xtreme. Zudem wurde Inkscape gegenüber Sodipodi um eine Vielzahl neuer Funktionen ergänzt.\n\nDas Autorenteam bestand zum Zeitpunkt der Abspaltung aus drei US-Amerikanern; zusammen mit einem Kanadier bilden sie momentan die Administrationsebene. Die Anzahl weiterer gleichzeitig aktiver Autoren war bisher nicht größer als ein bis zwei Dutzend – insgesamt haben jedoch inzwischen an die hundert Autoren Programm- und Übersetzungsbeiträge geleistet.\n\nZuletzt wurde 2Geom als lib2Geom öffentlich als weiteres Projekt zugänglich gemacht, um hier die Weiterentwicklung zu beschleunigen.\n\n\n"}
{"id": "360097", "url": "https://de.wikipedia.org/wiki?curid=360097", "title": "Microsoft Dynamics AX", "text": "Microsoft Dynamics AX\n\nMicrosoft Dynamics AX (ehemals Axapta) ist ein objektorientiertes ERP-System. Es wird insbesondere in mittelständischen und größeren Unternehmen eingesetzt.\n\nAxapta wurde ursprünglich von der Firma Damgaard entwickelt. Danach verschmolzen die Firmen Damgaard und Navision und nannten sich seither Navision-Damgaard. Da 2002 die neue Navision-Damgaard von Microsoft für 1,4 Milliarden Dollar gekauft wurde, gehört die ERP-Lösung Axapta nun zur Produktreihe von Microsoft Business Solutions. Im Zuge eines Rebrandings der Microsoft-ERP-Produkte, die im Rahmen des sogenannten \"Project Green\" in ein einheitliches Microsoft ERP-System überführt werden sollten, wird Axapta bis zur vollkommenen Ablösung durch das Neuprodukt unter dem Namen Microsoft Dynamics AX vermarktet. Die Version Axapta 3.0 wurde im Juli 2006 durch die Version Microsoft Dynamics AX 4.0 abgelöst. Neben vielen Ergänzungen der Funktionalität wurde der Client überarbeitet und der Oberfläche von MS Office 2003 angepasst. Auf den ersten Blick nicht so sichtbar sind die tiefgreifenden Änderungen in der Systemarchitektur (verstärkter Fokus auf Sicherheit (Trusted Computing), nur noch eine Drei-Schicht-Architektur), der Entwicklungsumgebung (u. a. Einbindung von .NET) und der zugrundeliegenden Produktphilosophie (weg von der integrierenden Software hin zu einem integrierten Bestandteil eines sogenannten Microsoft Stack). Die Version AX 2009 stellte im Jahre 2008 den ersten deutlich sichtbaren Beleg für diese Philosophie dar.\nSeit Ende 2011 ist die aktuelle Version Dynamics AX 2012 auf dem Markt, die rollenbasierte Oberfläche wurde noch stärker\nan die Oberfläche der Office-Produktreihe angepasst. Die Weboberfläche (Enterprise Portal) wurde über SharePoint 2010 ebenfalls\nder Office-Umgebung angeglichen.\n\nVon Anfang an lief der Axapta-Server ausschließlich auf Windows 2000 und Windows 2003 – zumindest gilt das für Produktivsysteme. Im Entwicklungs- oder Testbereich kann für die Version 3.0 auch Windows 2000 oder XP Professional verwendet werden. Seit der Version 4.0 ist die Verwendung von Windows Server (2003 oder 2008) zwingend. Dies liegt u. a. auch daran, dass die Benutzerauthentifizierung über Active Directory vorgenommen wird. Der zugehörige Axapta-Client läuft auf allen gängigen Windows-Systemen.\n\nAxapta bietet mit dem Enterprise Portal Server unter Verwendung des IIS und – ab Version 4.0 – Sharepoint Portal Services auch eine Verbindung über den Internet Explorer an.\n\nAls zugrundeliegende Datenbanken kommen entweder ein Microsoft SQL Server oder ein Oracle-System zum Einsatz. Bei einer Oracle-Anbindung sind die Auslastungsstatistik und die Datendurchsatzauswertung nicht nutzbar, können aber mit dem Oracle Enterprise Manager ermittelt werden. Die Unterstützung von Oracle-Datenbanken wurde mit der Version 2012 eingestellt.\n\nDas ERP-System kann durch die Axapta-Programmiersprache X++ angepasst und erweitert werden. Die Programmiersprache X++ ist eine Mischung aus den Programmiersprachen Java, BASIC, C++ und SQL. Die Tabellen, Objekte und Methoden sind zum größten Teil Open Source und können dadurch angepasst werden. Durch eine Layerstruktur kann ein durch den Kunden angepasstes Objekt trotzdem released werden. Es gibt insgesamt 16 Layer, wovon jeweils zwei Layer logisch zusammengehören. Die ersten vier Layer (codice_1, codice_2, codice_3, codice_4) sind für Microsoft reserviert. Der fünfte und sechste Layer (codice_5, codice_6) dient für die Auslieferung von landesspezifischen Funktionalitäten. Hierfür existiert für mehrere Länder eine Layerversion. Der siebte und achte Layer (codice_7, codice_8) enthält lokale Funktionalitäten, die sich ausschließlich auf die Anfordernisse eines Landes beziehen. Im neunten und zehnten Layer (codice_9, codice_10) werden zertifizierte Lösungen beispielsweise von Partnern ausgeliefert. Der elfte und zwölfte (codice_11, codice_12) Layer dient Beraterfirmen, um Anpassungen und Erweiterungen für Ihre Kunden auszuliefern. Layer 13 bis 16 (codice_13, codice_14, codice_15, codice_16) sind für den Kunden reserviert. Layer, die auf „P“ Enden, dienen generell zur Auslieferung von Aktualisierungen und Service Packs für den zugrundeliegenden Layer. Sind z. B. für ein Service Pack Änderungen im codice_1 Layer notwendig, werden diese im codice_2 Layer ausgeliefert. Die Layer setzen aufeinander auf, d. h. eine Änderung eines Objektes in einem höheren Layer verdeckt das Objekt im darunterliegenden.\n\nBis auf USR/USP werden die Layer durch Passwörter geschützt, so dass sie nur lesbar sind.\n\nIn der Version 4.0 wurden die bisherigen Layer SYS, GLS und DIS zu einem gemeinsamen SYS-Layer zusammengefasst, länderspezifische Funktionalitäten sind in einem GLS-Layer installierbar. Der LOS-Layer wurde (zumindest einstweilen) ersatzlos gestrichen. Des Weiteren wurde Microsoft Visual SourceSafe integriert, was eine Versionsverwaltung des Sourcecodes ermöglicht.\n\nIn Version 2012 wurde die Layerstruktur überarbeitet (SYS/SYP, GLS/GLP, FPK/FPP, SLN/SLP, ISV/ISP, VAR/VAP, CUS/CUP, USR/USP) und diese durch Models erweitert. Models sind Sammlungen von Anwendungsobjekten innerhalb eines Layer, die für Partnerlösungen und Branchenmodule geeignet sind, um sie besser und mit weniger Migrationsaufwand in eine vorhandene Installation zu integrieren.\n\nOftmals wird der Funktionsumfang von Dynamics AX, durch die Vertriebspartner, in Form von Branchenlösungen erweitert.\nEinerseits werden dadurch Funktionslücken der Standardinstallation ganz oder teilweise geschlossen, andererseits wird dadurch der Wechsel zu einem anderen Microsoft-Partner deutlich erschwert.\nDie aktuelle Version AX 2012 wurde durch den Zukauf von ausgewählten Partnerlösungen um Funktionen wie Lean Produktion(KANBAN)\nund Retail(POS) erheblich im Standard erweitert.\n\nSystemanpassungen können in Dynamics AX teils durch Customizing vorgenommen werden.\nEs ist jedoch auch allgemein üblich ein Standard-System zu verändern (modifizieren). Dabei wird durch Veränderungen im Quellcode das Systemverhalten beeinflusst.\nHierbei leistet die Layer-Technologie wertvolle Dienste, da immer nur der oberste modifizierte Layer ausgeführt wird.\nOftmals wird zur Beurteilung einer Modifikation nur der Aufwand der aktuellen Anpassung herangezogen.\nDies ist jedoch falsch, da eine Modifikation auch bei jedem zukünftigen Update der Komponente zu erneutem Aufwand führt.\nWenn jedoch ein tieferer Layer (z. B. Updates für länderspezifische Versionen) ausgetauscht und aktualisiert wird,\nwerden auch diese Aktualisierungen durch die Modifikation überdeckt.\n\nUpdates für das Dynamics AX-Standardsystem werden direkt von Microsoft über die Vertriebspartner ausgeliefert.\nDa fast alle Dynamics AX – Installationen teils umfangreiche Modifikationen des Quellcodes aufweisen, kann das Update nicht einfach installiert werden. Vielmehr muss für jede Codeänderung im Quellcode überprüft werden, ob Modifikationen noch den gewünschten Effekt auslösen.\nDaher sind Update-Projekte nicht trivial und können im Extremfall einen, mit einer Neueinführung vergleichbaren, hohen Entwicklungsaufwand erreichen. Dies gilt vor allem, wenn die Zusatzprogrammierung durch den Partner nicht sauber durchgeführt wurde.\nTeilweise liefern auch Microsoft-Partner Updates für Ihre Branchenlösungen aus. Auch hier ist jede Code-Anpassung und Modifikation nochmals zu prüfen.\n\nAxapta X++ und die Logiken sind in Version 3 nicht ausreichend dokumentiert. Allerdings kann man sich durch die Layerstruktur und durch den offenen Quellcode schnell in die Axaptafunktionen einarbeiten.\n\nDas eingebaute Entwicklungshandbuch bietet einen Überblick über Syntax und ausgewählte Konstrukte, die eigentliche Geschäftslogik ist weitgehend undokumentiert. Dies wird sich nach Ankündigungen in der aktuellen Version Dynamics AX 4.0 grundlegend ändern; geplant ist unter anderem eine umfassende Dokumentation der grundlegenden Klassen, die auch über MSDN abrufbar sein wird. Allerdings scheint Microsoft fast ein Jahr nach der Veröffentlichung der 4.0 dieses Versprechen nicht gehalten zu haben.\n\nEs gibt auf dem Markt zahlreiche Bücher zu Dynamics AX.\n\nTraining wird wie für andere Microsoft-Produkte auch als Reihe von MOC-Kursen von sogenannten CPLS (\"Certified Partner for Learning Solutions\") angeboten. Diese Schulungen, die die unterschiedlichen Module der Software behandeln und Zertifizierungen auf verschiedenen Ebenen ermöglichen, werden von Partnern deutschlandweit angeboten (aktuelle Informationen sind von der jeweiligen Microsoft-Landesvertretung zu erfahren). Um eine gründliche Einarbeitung zu gewährleisten, sind gründliche Kenntnisse der Programmiersprache X++ unumgänglich, welche ebenfalls in verschiedenen Schulungen erworben werden können.\n\nAxapta kann nicht direkt von Microsoft gekauft werden. Stattdessen wird es von externen Beraterfirmen vertrieben, die auch die Implementierung sowie eventuell nötige Anpassungen durchführen. Auch Support und Schulungen werden von Partnern erbracht oder vermittelt. Diese Beraterfirmen müssen diverse Axapta-Zertifikate erworben haben und damit nachweisen, dass sie die notwendige Sachkenntnis besitzen.\n\n\n"}
{"id": "360749", "url": "https://de.wikipedia.org/wiki?curid=360749", "title": "Yellowdog Updater, Modified", "text": "Yellowdog Updater, Modified\n\nYUM (\"Yellowdog Updater, Modified\") ist ein Paketmanagement-System, das für die Linux-Distribution \"Yellow Dog Linux\" entwickelt wurde. Mit dem Kommandozeilenprogramm lassen sich RPM-Software-Pakete suchen, installieren und aktualisieren.\n\nUm RPM-Pakete einfacher zu installieren und zu verwalten, wurde für das Yellow Dog Linux das Programm YUP geschrieben und unter die GNU General Public License gestellt. YUP war aber sehr langsam, da es zur Auflösung der Abhängigkeiten immer alle Pakete herunterlud, statt nur die wichtigen Kopfzeilen jedes Pakets.\nSeth Vidal schrieb eine abgeänderte Version und gab ihr den Namen \"yellowdog updater, modified\". Diese wurde nach und nach bis zum heutigen Stand erweitert.\nNachdem YUM mächtig genug war und genügend Funktionen besaß, wurde es auch von der Distribution Red Hat Enterprise Linux übernommen.\n\nMittlerweile unterstützt auch openSUSE YUM rudimentär, setzt jedoch standardmäßig auf ZYpp.\n\nYUM wurde ebenso auf eComStation und OS/2 portiert.\n\nIn der Datei codice_1 werden alle notwendigen Konfigurationen des Programms gespeichert, die Paketquellen finden sich im Verzeichnis codice_2. Die Paket-\"Repositories\" (englisch für \"Lager, Depot\") können dabei lokale Verzeichnisse, CDs, HTTP- oder FTP-Server sein.\nWird das Programm aufgerufen, so durchsucht es zuerst alle eingetragenen Repositories nach neuen Paketen. Werden welche gefunden, lädt es die Kopfzeilen des Pakets herunter. Zu diesen Kopfzeilen gehören zum Beispiel der Name des Pakets, Abhängigkeiten zu anderen Paketen, enthaltene Dateien, Version etc.\n\nNach der Aktualisierung der Paketdaten und dem Abgleich mit der lokalen Paketdatenbank verhält sich das Programm entsprechend der übergebenen Befehlsoption. Wurde es etwa mit der Option codice_3 übergeben, sucht es dann in den Kopfzeilen nach dem Paketnamen \"firefox\" und lädt das entsprechende Paket zusammen mit allen weiteren dazugehörigen Paketen herunter. Lautet die Option codice_4, so werden alle Pakete des lokalen Systems erneuert, von den es auf den Repositories eine neuere Version gibt. Diese Option wird auch ausgeführt, wenn ein Dienst, wie z. B. PackageKit ein automatisches Systemupdate ausführt.\n\nYUM lädt die für eine Installation oder ein Update nötigen RPM-Pakete von Repositories herunter und speichert sie temporär auf der Festplatte. Für den Installationsvorgang selbst und die Verwaltung der Paketdatenbank, ruft YUM im Hintergrund das Programm RPM auf.\n\nEine Besonderheit von YUM gegenüber anderen Paketmanagern ist, dass YUM so konfiguriert werden kann, dass es bei jedem Aufruf aus einer Liste im Internet einen günstigen Server heraussucht, statt immer nur auf denselben zuzugreifen. Diese dynamische Konfiguration ermöglicht auch das Ausweichen auf andere Server im laufenden Betrieb, falls ein Server ausfällt oder sich als nicht aktuell herausstellen sollte.\n\nDes Weiteren kann YUM durch eine Plugin-Schnittstelle nahezu beliebig erweitert werden. Die Plugins werden dabei in Python geschrieben. Zu den Funktionen, die durch Plugins realisiert werden, gehören unter anderem die Überwachung der Verbindungsgeschwindigkeiten zu den Download-Servern und die Auswahl des jeweils schnellsten Servers sowie eine automatische Aktualisierung von externen Kernelmodulen bei der Installation neuer Kernel.\n\nYUM ist eine Software für die Kommandozeile, die in den ersten Fedora-Versionen ohne grafisches Frontend ausgeliefert wurde. Dies änderte sich erstmals, als die auf Fedora Linux basierende Linux-Distribution Cobind das Programm GYUM integrierte, welches eine an die Gnome-Umgebung angepasste grafische Oberfläche bot. GYUM wurde zunächst von der Fedora-Core-Gemeinde übernommen, aber Version 2.1 von YUM wurde die Weiterentwicklung zu Gunsten von \"pup\" eingestellt.\n\nMittlerweile existieren mehrere grafische Frontends für YUM. Besonders erwähnenswert sind \"Yum Extender\" (yumex) und \"KYUM\", die in die Fedora-Distributionen aufgenommen wurden und aktiv weiterentwickelt werden.\n\nFür Fedora Core 5 bis Fedora 8 steht ein grafisches Aktualisierungswerkzeug namens \"pup\" bereit, das auf \"YUM\" aufbaut. Außerdem gibt es dort auch ein grafisches Paketverwaltungs-Benutzeroberfläche („frontend“) namens \"pirut\", welche neben den Installations-CDs auch auf \"YUM\" und seine Funktionen zurückgreift.\n\nSeit Fedora 9 wird \"PackageKit\" als grafisches standard Frontend für YUM eingesetzt.\n\nFür eComStation und OS/2 wurde als graphisches Frontend das Programm anpm (ArcaNoaePackageManager) entwickelt.\n\nAb Fedora Core 6 gibt es den Dienst yum-updatesd. Dies ist ein Daemon, der das System periodisch auf Updates überprüft und, wenn neue verfügbar sind, diese Information über verschiedene Kanäle ausgeben kann. Mögliche Ausgaben sind per E-Mail, Syslog oder D-Bus. Es gibt Applets, die darauf hinweisen, wenn Aktualisierungen verfügbar sind.\n\n\n"}
{"id": "361071", "url": "https://de.wikipedia.org/wiki?curid=361071", "title": "MuPAD", "text": "MuPAD\n\nMuPAD (\"Multi Processing Algebra Data Tool\") ist ein Computeralgebrasystem. Ursprünglich von der MuPAD-Forschungsgruppe an der Universität Paderborn unter der Leitung von Benno Fuchssteiner entwickelt, erfolgte seit 1997 die weitere Entwicklung durch die Firma SciFace Software GmbH & Co. KG, Paderborn in Kooperation mit der MuPAD-Forschungsgruppe sowie Partnern an anderen in- und ausländischen Universitäten.\n\nMit der Übernahme des Herstellers SciFace durch MathWorks ist MuPAD seit Ende September 2008 nur noch als Bestandteil der \"Symbolic Math Toolbox\" zu MATLAB erhältlich.\n\nBis Herbst 2005 gab es die Variante MuPAD Light kostenlos für den Einsatz in Forschung und Lehre. Als Konsequenz aus der Schließung des Instituts AutoMATH der Universität Paderborn, in dem die MuPAD-Forschungsgruppe beheimatet war, ist MuPAD Light nicht mehr erhältlich, sondern nur noch die kostenpflichtige Variante \"MuPAD Pro\". Die letzte von SciFace entwickelte Version ist 4.0.6.\n\n\"MuPAD Pro\" bietet\n\n\nDie Bedienung des Systems geschieht durch Eingabe von Kommandos. Oft benutzte Kommandos sind direkt per Mausklick zu erreichen. MuPAD Pro bietet ein Arbeitsblatt-Konzept, was es erlaubt, vergleichbar zu Textverarbeitungssystemen, mathematische Problemstellungen in MuPAD Pro zu formulieren, zu berechnen, in Form von eingebetteten 2D/3D-Grafiken und Animationen zu visualisieren und zu dokumentieren. Vorangegangene Kommandos können geändert und erneut ausgeführt werden. MuPAD Pro Arbeitsblätter (Notebooks) können wie ein Rechenschema auf Knopfdruck neu berechnet werden.\n\nMuPAD Pro wird in der Wissenschaft, in Schulen und in der Wirtschaft eingesetzt. Die Anwendungsmöglichkeiten reichen von einfachen Termumformungen und linearer Algebra (auch über endlichen Körpern und anderen algebraischen Strukturen) über lineare Optimierung, Differentialgleichungen bis zu numerischen Rechnungen. Für die Beschleunigung einzelner Teilrechnungen lässt MuPAD Pro sich durch eigene C++-Routinen erweitern. Auch die Anbindung von Java-Code ist möglich.\n\n"}
{"id": "362489", "url": "https://de.wikipedia.org/wiki?curid=362489", "title": "Bit blit", "text": "Bit blit\n\nAls Bit blit (für \"Bit Block Image Transfer\"; auch als \"BitBlt\" oder ähnlich bezeichnet für \"bit block transfer\") bezeichnet man in der Computergrafik eine Operation, die für das schnelle Kopieren und Verschieben von Speicherinhalten (\"Blitting\") zuständig ist. Bit blit kann entweder als Softwareprozedur oder als Funktion einer hardwarebasierten Grafikausgabe (Grafikkarte, -Chip oder Chipsatz) implementiert werden; im Fall eines dedizierten Chips bezeichnet man diesen auch als \"Blitter\".\n\n\"Bit Blit\" geht auf die sogenannte \"RasterOp\"-Spezifikation von Newman und Sproull zurück, eine Funktion zum bitweisen Kopieren von Speicherblöcken, die aber noch stark begrenzt war. Eine Hardware-Implementierung der \"RasterOp\"-Funktion wurde von VLSI Technology 1986 vorgestellt. Dieser Chip hatte keine DMA-Funktion und konnte daher den Hauptprozessor kaum entlasten.\n\nBlitting wird vor allem zur Beschleunigung der Grafikausgabe verwendet. Dabei werden Bildbereiche verschoben, kopiert, bitweise manipuliert oder zwei Rastergrafikteile zu einem kombiniert. Die CPU wird beim Einsatz des Blitters nicht belastet, sofern dieser DMA beherrscht. Das Konzept wurde von Dan Ingalls am Forschungszentrum Xerox PARC für den Xerox-Alto-Computer entwickelt. Bei älteren Computern war die CPU oft zu langsam, um große Datenblöcke schnell im Speicher zu kopieren. Die Aufgabe übernahm ein Spezialchip bzw. Co-Prozessor, der besagte Blitter. Viele Heimcomputer wie der Amiga (\"Amiga Blitter\", schon seit 1985 mit DMA-Beschleunigung) und manche Atari-ST-Modelle (darunter die Mega ST-Baureihe ab 1987 und die STE-Serie ab 1989) besaßen einen Blitter. Hiermit wurde es auch möglich, die CPU mit anderen Aufgaben zu beschäftigen, während der Blitter Bildschirmdaten manipulierte.\n\nAuch in die Welt der PC-kompatiblen X86-Prozessor-Rechner fand Blitting Eingang. Zunächst wurden solche Grafikkarten auch als „Windows-Beschleuniger“ bezeichnet, weil sie den Prozessor von der Notwendigkeit entlasteten, die Daten über den ISA-Bus zu übertragen. Wenn dies innerhalb der Grafikkarte durch den Grafikprozessor in separatem Grafik-RAM erfolgt, wird auch hier die CPU und der normale Arbeitsspeicher entlastet. Man spricht hier auch von 2D-Beschleunigung. Sie obliegt, wie auch die später eingeführte 3D-Beschleunigung, dem Grafikprozessor.\n\nEin klassisches Beispiel für Blitting ist das Verschieben eines Fensters in einer grafischen Oberfläche. Hierbei muss ein großer Speicherblock (der Fensterinhalt) von einer Speicherstelle (Ursprungsposition) in eine andere Speicherstelle (Zielposition) verschoben werden. Geschieht dies ohne Hardwarebeschleunigung, wie beispielsweise im abgesicherten Modus von Windows XP oder unter Verwendung des X11-\"vesa\"-Treibers, so kann man diesen Vorgang nachverfolgen: Das Fenster wird nicht als Ganzes verschoben, sondern stückweise. Zudem ist die grafische Oberfläche währenddessen meist blockiert. Frühere Oberflächen wie Windows 3.1 oder Windows 95 haben daher während des Verschiebevorgangs nur den Rahmen gezeichnet und den Fensterinhalt erst verschoben, wenn die Zielposition feststand. Zudem waren die Auflösungen und damit auch die Datenmengen geringer.\n\n\n"}
{"id": "362643", "url": "https://de.wikipedia.org/wiki?curid=362643", "title": "Pan (Newsreader)", "text": "Pan (Newsreader)\n\nPan ist ein erstmals am 30. Juli 1999 veröffentlichter, freier Newsreader.\n\nPan erlaubt außer dem Anzeigen und Senden von Artikeln auch das Herunterladen von Dateianhängen und das Speichern von Artikelinhalten ganzer Threads oder Gruppen zum Offline-Lesen. Ursprünglich wurde er für Unix-artige Betriebssysteme entwickelt, ist inzwischen aber auf allen gängigen Betriebssystemen lauffähig. Pan benötigt GTK2.\n\nPan wurde ursprünglich von Matt Eagleson entwickelt. Dem Entwicklerteam gehören jetzt auch Charles Kerr und Christophe Lambin an. Der Name war ursprünglich ein Akronym für „Pimp Ass Newsreader“, ist jetzt aber eine Eigenbezeichnung ohne Langform.\n\nNachdem die Weiterentwicklung von Pan Ende 2003 für eine Weile eingeschlafen war, begann Charles Kerr an einer kompletten Neufassung in C++ zu arbeiten. Seit April 2006 werden in kurzen Abständen Beta-Versionen der neuen Fassung veröffentlicht, die sich unter anderem durch eine deutlich erhöhte Geschwindigkeit, geringeren Speicherverbrauch und die gleichzeitige, automatische Unterstützung mehrerer News-Server auszeichnet.\n\n"}
{"id": "365083", "url": "https://de.wikipedia.org/wiki?curid=365083", "title": "Baudot-Art", "text": "Baudot-Art\n\nBaudot-Art ist ähnlich wie ASCII-Art eine künstlerische Methode, mit Hilfe von Codeelementen des 5-Bit-Baudot-Codes eine grafische oder textuelle Gestaltung vorzunehmen. Ein einfaches Beispiel ist es, einen Lochstreifen so zu kodieren, dass er einen für den Menschen lesbaren Text ergibt.\n"}
{"id": "366434", "url": "https://de.wikipedia.org/wiki?curid=366434", "title": "Fedora- und Red-Hat-Versionsnamen", "text": "Fedora- und Red-Hat-Versionsnamen\n\nDie Linux-Distribution \"Red Hat Linux\" (RHL) benannte, wie andere Distributionen auch, die jeweiligen Versionen der Software neben der Versionsnummer auch mit einzelnen Codenamen für jedes größere Release. Diese Tradition wurde auch im \"Fedora\"-Projekt bis zur Version 20 fortgeführt. Die Codenamen von \"Red Hat Enterprise Linux\" (RHEL) folgten nie der hier beschriebenen Tradition. Dort werden Ortsnamen aus den USA als Codenamen verwendet.\n\nAb der Version 3.0.3 der Red-Hat-Linux-Distribution ist der besondere Clou hinter diesen Namen die Tatsache, dass alle Begriffe zweideutig sind. Die zweite Bedeutung eines Begriffes hängt jeweils thematisch mit der ersten Bedeutung des Namens der nächsten Version zusammen (soweit man diese Bedeutungen ordnen kann).\n\nMit Fedora 20 endete diese Namenserie.\n\n"}
{"id": "366747", "url": "https://de.wikipedia.org/wiki?curid=366747", "title": "Microsoft Visio", "text": "Microsoft Visio\n\nMicrosoft Visio ( []) ist ein Visualisierungsprogramm von Microsoft für Windows. Die Software gehört im weitesten Sinne zur Microsoft-Office-Familie, ist aber nicht Bestandteil einer Office-Suite und kann daher nur separat erworben werden.\n\nSeit Office Visio 2003 gibt es nur noch eine Standard- und eine Professional-Edition. Vorherige Editionen wie Technical- und Enterprise-Edition wurden in die Professional-Edition integriert. Mit Einführung von Visio 2010 gibt es zusätzlich eine Premium-Edition, die erweiterte Tools enthält, mit denen Geschäftsprozesse und SharePoint-Workflows modelliert werden können. \n\nDas Unternehmen Visio wurde mit der gleichnamigen Software im Januar 2000 für ca. 1,3 Mrd. US-Dollar von Microsoft gekauft.\n\nVisio dient dazu, mit Hilfe verschiedener Vorlagen mit passenden Werkzeugen und Symbolen grafische Darstellungen zu erzeugen. Die so entstehenden Diagramme lassen sich einfach, beispielsweise per Drag and Drop, aber auch als eigenständige Datei (*.vsd) in andere Dokumente einbetten. Besonders geeignet ist es für Ablaufdiagramme und Geschäftsprozesse, aber auch andere Arten von Diagrammen, beispielsweise lassen sich damit einfache technische Zeichnungen und UML-Diagramme erstellen (siehe UML-Werkzeuge). Das Besondere an Visio-Diagrammen ist, dass die einzelnen Shapes, die sich in sogenannten Schablonen befinden, mit Daten aus beliebigen Datenbanken und Excel-Tabellen verknüpft werden können. Mit den Visio Services können solche Diagramme auch auf dem SharePoint Server eines Unternehmens anderen Mitarbeitern präsentiert werden, ohne dass diese Visio auf dem Rechner installiert haben müssen.\n\nSeit März 2008 wird Office Visio erstmals mittels eines Werbespots bei YouTube beworben.\n\nMit Einführung von Visio 2010 entstanden verschiedene Spots, in denen bekannte Märchen visualisiert wurden.\n\n\n"}
{"id": "367164", "url": "https://de.wikipedia.org/wiki?curid=367164", "title": "System Dynamics", "text": "System Dynamics\n\nSystem Dynamics (SD) oder Systemdynamik ist eine von Jay W. Forrester Mitte der 1950er Jahre an der Sloan School of Management des MIT entwickelte Methodik zur ganzheitlichen Analyse und (Modell-)Simulation komplexer und dynamischer Systeme.\n\nAnwendung findet sie insbesondere im Bereich sozioökonomischer Systeme. So können die Auswirkungen von Management-Entscheidungen auf die Systemstruktur und das Systemverhalten, wie zum Beispiel den Unternehmenserfolg, simuliert und Handlungsempfehlungen abgeleitet werden. In der Praxis findet die Methodik insbesondere bei der Gestaltung von Lernlabors, in der strategischen und operativen Planung sowie der Operationalisierung von Balanced Scorecards Verwendung. Die Analyse und Gestaltung sozioökonomischer Sachverhalte und Problemsituationen erfolgt durch qualitative und quantitative Modelle.\n\nBei der qualitativen Methode geht es hauptsächlich um die Identifikation und Untersuchung in sich geschlossener Wirkungsketten (engl.: \"feedback loops\"). Unterschieden werden dabei Loops mit positiven (\"reinforcing loops\") und negativen (\"balancing loops\") Polaritäten. Ursprünglich soll u. a. nach Forrester die qualitative Methode immer durch eine sich anschließende quantitative Analyse (Simulation) ergänzt werden: Tatsächlich beschränken sich heutige System-Dynamics-Projekte aus konzeptionellen oder finanziellen Gründen teilweise auf qualitative Modelle. Qualitative Modelle wie sog. Kausaldiagramme oder \"influence diagrams\" leisten auch ohne Simulation und Verwendung \"harter Daten\" (Wolstenholme EF. 1993) einen wichtigen Beitrag für die System-Analyse: Sie fassen ein sehr komplexes Problem, bestehend aus unzähligen Erklärungen, in übersichtlicher Weise zusammen und helfen als Vorlage für Diskussionen. Sie identifizieren Rückkopplungen und helfen somit, Probleme und Strukturen zu erklären oder neue Einsichten zu gewinnen. Die Untersuchung des Diagramms mag die Angemessenheit der Modellgrenzen und Annahmen besser erkennen lassen. Und schließlich dienen sie als Grundlage für mögliche quantitative Modelle, zumal sie relativ leicht in Gleichungen transformiert werden können.\n\nDie Darstellung in Flussdiagrammen und deren Simulation ermöglicht tieferes Systemverständnis. Lager (\"Stocks\"), Raten (\"Flows\") und Hilfsgrößen dienen zur Beschreibung der Systemzusammenhänge und zeigen, wie die Wirkungsketten zum Verhalten von Systemen führen, welche teils nicht-linear und kontraintuitiv sind. Dies ist der Hauptvorteil dieser Methode. Spezielle Software wie CONSIDEO, AnyLogic, iThink/STELLA, DYNAMO, Vensim oder Powersim ermöglichen die Simulation der untersuchten Fragestellungen. Die Simulation unterschiedlicher Szenarien (\"Runs\") fördert das Verständnis für das Systemverhalten im Zeitverlauf.\n\nDen immer wiederkehrenden Verhaltensmustern komplexer Systeme liegen bestimmte Strukturen zugrunde, die als vereinfachte Modelle dargestellt werden können, sogenannte Systemarchetypen. Momentan werden 10 verschiedene solcher Systemarchetypen unterschieden. Die Kenntnis dieser Grundstrukturen ermöglicht ein tieferes Verständnis verschiedenster Systeme und schafft somit eine Grundlage für effektivere Eingriffe in diese.\n\nSystem Dynamics wird heutzutage insbesondere in den Bereichen der Volks- und Betriebswirtschaft zur Analyse von dynamischen und komplexen Sachverhalten eingesetzt. Beispiele stammen aus dem öffentlichen und privaten Sektor: Produktionsmanagement, strategische Planung, Analyse und Design von Geschäftsmodellen, Business Forecasting und Szenarioanalyse. Die Methodik bietet sich generell für die Simulation und Erklärung des komplexen Verhaltens von Menschen in sozialen Systemen an. Hier sind typische Beispiele die Überfischung der Weltmeere oder die Entstehung von Katastrophen z. B. die Katastrophe von Tschernobyl. Außerdem war System Dynamics die grundlegende Methodik zur Simulation des Weltmodells World3, das für die Studien zu \"Limits to Growth\" (dt.: \"Die Grenzen des Wachstums\", 1972) unter Leitung von Dennis L. Meadows im Auftrag des Club of Rome erstellt wurde. Wichtige Simulationsmodelle insbesondere für die Umweltforschung wurden von Hartmut Bossel entwickelt.\n\nNeben dem Modell und den Lösungsansätzen an sich, sind die gewonnenen Erkenntnisse und das Verständnis der Prozesse auch Resultate, welche über das Projekt hinaus eingesetzt werden. Weiter führt das Verständnis der Methode zur verbesserten und schnelleren Erkenntnis bei anderen Problemstellungen.\n\n\n"}
{"id": "368108", "url": "https://de.wikipedia.org/wiki?curid=368108", "title": "Mutella", "text": "Mutella\n\nMutella ist ein textbasiertes Filesharingprogramm für das Peer-to-Peer-Netzwerk Gnutella für Unix, entwickelt von Max Zaitsev und Gregory Block.\nEs hat zwei Benutzeroberflächen – eine für den Textmodus und eine für das so genannte „Remote Control“, welches auf einem integrierten Webserver läuft und durch einen Webbrowser benutzt wird.\nDie erste offizielle Version wurde am 6. Oktober 2001 veröffentlicht.\n\nDas Mutella Logo wurde ab der Version 0.4.1 in einen Tintenfisch verändert, davor wurde eine blaue und schwarze Version des Ouroboros verwendet.\n\n"}
{"id": "368203", "url": "https://de.wikipedia.org/wiki?curid=368203", "title": "Metropolis-Algorithmus", "text": "Metropolis-Algorithmus\n\nDer Metropolis-Algorithmus ist ein Markov-Chain-Monte-Carlo-Verfahren zur Erzeugung von Zuständen eines Systems entsprechend der Boltzmann-Verteilung. Der davon abgeleitete, allgemeinere Metropolis-Hastings-Algorithmus ermöglicht es, Folgen von Zufallsvariablen, genauer Markow-Ketten, zu simulieren, die eine gewünschte Verteilung als stationäre Verteilung besitzen, insbesondere in vielen Fällen, bei denen die Verteilungen der Zufallsvariablen nicht direkt simuliert werden können.\n\nDer Metropolis-Algorithmus wurde 1953 von Nicholas Metropolis et al. publiziert. Er wird dazu genutzt, eine Markow-Kette und damit die Zustände eines Systems entsprechend der Boltzmann-Verteilung zu erzeugen. Dabei hängt der neue Zustand des Systems formula_1 nur vom vorherigen Zustand formula_2 ab.\n\nIm Folgenden wird der Algorithmus für den Fall beschrieben, dass das System von einem mehrdimensionalen Ort formula_3 abhängt. formula_3 sei kontinuierlich und der aktuelle Ort nach formula_5 Iterationen wird mit formula_6 bezeichnet. Der Metropolis-Algorithmus ergibt sich dann durch Wiederholung der folgenden Schritte:\n\nKleine Werte von |\"r\"| führen dabei zu großen Akzeptanzraten, haben jedoch den Nachteil hoher Autokorrelationszeiten\n\nGroße Werte von |\"r\"| dagegen verkürzen zwar die Autokorrelationszeit, haben dafür aber nun den Nachteil einer geringeren Akzeptanzrate, so dass in der Praxis stets ein Mittelweg gesucht werden muss.\n\nDas oben beschriebene Verfahren lässt sich einfach auch auf andere Fälle wie beispielsweise diskrete Zustände übertragen. Für Systeme aus vielen wechselwirkenden Teilchen wird der Metropolis-Algorithmus dabei zunächst \"lokal\" für ein einzelnes Teilchen angewandt und anschließend – entweder nacheinander oder zufällig – auf alle Teilchen.\n\nW. Keith Hastings generalisierte 1970 das Verfahren. Der Metropolis-Hastings-Algorithmus kann Zustände für eine beliebige Wahrscheinlichkeitsverteilung formula_26 erzeugen. Voraussetzung ist lediglich, dass die Dichte an jedem Ort formula_3 berechnet werden kann. Der Algorithmus benutzt eine \"Vorschlagsdichte\" formula_28, die vom derzeitigen Ort formula_3 und möglichem nächsten Ort formula_15 abhängt. Beim Metropolis-Hastings-Algorithmus wird ein Vorschlag formula_15 anhand der Vorschlagsdichte zufällig erzeugt und mit der Wahrscheinlichkeit formula_32 akzeptiert.\n\nFür eine Vorschlagsdichte, die symmetrisch ist (formula_33), sowie eine Boltzmann-Verteilung als Wahrscheinlichkeitsverteilung formula_34 ergibt sich hieraus der ursprüngliche Metropolis-Algorithmus.\n\nBei Monte-Carlo-Simulationen werden Konfigurationen mittels des Metropolis-Algorithmus erzeugt und Mittelwerte/Erwartungswerte physikalisch relevanter Größen berechnet, beispielsweise der Erwartungswert des Drucks oder der Dichte:\n\nmit\n\nDazu werden von den Iterationsschritten des Metropolis-Algorithmus zunächst so viele ausgeführt, bis sich das System hinreichend nah an das thermische Gleichgewicht angenähert hat, d. h. bis die Wahrscheinlichkeit der einzelnen Konfigurationen der Boltzmann-Verteilung entspricht. Befindet sich das System im thermischen Gleichgewicht, so entspricht die Wahrscheinlichkeitsverteilung formula_38 der Boltzmann-Verteilung, d. h. die Konfigurationen werden mit der Wahrscheinlichkeit formula_39 erzeugt (\"Importance Sampling\") und es muss lediglich über jeden Messwert, bzw. Messwerte in konstantem Abstand, gemittelt werden: formula_40.\n\nDer Metropolis-Algorithmus erzeugt Systeme im kanonischen Zustand, d. h. mit konstanter Temperatur. Um mikrokanonische Zustände zu erzeugen, können Molekulardynamik-Algorithmen verwendet werden.\n\nIn der Originalarbeit von Nicholas Metropolis et. al. wurde der Algorithmus für die Monte-Carlo-Simulation eines zweidimensionalen Harte-Scheiben-Modells verwendet. Der Algorithmus wurde später für eine Vielzahl unterschiedlichster Monte-Carlo-Simulationen in Bereichen wie z. B. bei der Thermodynamik bzw. der Statistischen Physik, Festkörperphysik, Quantenelektrodynamik oder Quantenchromodynamik eingesetzt. Dabei muss der Algorithmus gegebenenfalls angepasst werden; beispielsweise muss man die Energie durch den Hamiltonoperator oder die Wirkung ersetzen.\n\nDer Metropolis-Algorithmus ist leicht zu implementieren, jedoch nicht immer der effizienteste Algorithmus. Alternativ können andere lokale oder nicht-lokale Verfahren Verwendung finden.\n\nDer Metropolis-Algorithmus kann auch als stochastisches Optimierungsverfahren zum Finden eines globalen Minimums einer Wertelandschaft verwendet werden. Hierzu wird mit einer hohen Temperatur begonnen, damit möglichst ein großes Gebiet der Wertelandschaft besucht wird. Anschließend wird die Temperatur langsam abgesenkt, sodass man sich mit immer höherer Wahrscheinlichkeit einem Minimum nähert. Ein solcher Metropolis-Algorithmus mit von der (Simulations-) Zeit abhängiger Temperatur heißt simulierte Abkühlung (\"simulated annealing\"). Für bestimmte Formen der simulierten Abkühlung konnte bewiesen werden, dass sie das globale Minimum einer Wertelandschaft finden.\n\nDas Verfahren ähnelt dem Bergsteigeralgorithmus (\"hill climbing\"), akzeptiert jedoch im Gegensatz zu diesem auch Schritte weg vom nächsten Minimum, so dass das „Hängen bleiben“ in lokalen Minima vermieden wird, die noch nicht das absolute Minimum ergeben. Der Metropolis-Algorithmus überwindet so kleine Hügel, bevor weiter in Richtung Tal gegangen wird, da der Anstieg in Richtung Hügel klein ist und somit die Akzeptanzwahrscheinlichkeit relativ groß ist.\n\n\n"}
{"id": "370206", "url": "https://de.wikipedia.org/wiki?curid=370206", "title": "Zeroconf", "text": "Zeroconf\n\nZeroconf oder Zero Configuration Networking ist ein Vorhaben für die selbständige Konfiguration von Rechnernetzen ohne Eingriffe durch Menschen.\n\nDie entsprechende Arbeitsgruppe der Internet Engineering Task Force schloss mangels Konsens ohne Ergebnis. Sie arbeitete von 1999 bis 2004. Ihre Ziele waren:\n\nDie Arbeitsgruppe relativierte \"Zero\" (\"Null\") dahingehend, dass \"wenig Konfiguration\" besser als \"ohne Sicherheit\" sei.\n\n2005 wurde nachträglich RFC 3927 über die automatische Konfiguration von IPv4 in Local Area Networks ohne das Dynamic Host Configuration Protocol veröffentlicht. Dieser Text wurde von Mitarbeitern der Unternehmen Apple, Microsoft und Sun Microsystems verfasst und dokumentiert auch verwandte frühere Implementierungen von \"AutoIP\" oder \"Automatic Private IP Addressing\" (APIPA). Verbindungen in andere Netze als das Internet sind bei all dem ausgeschlossen. Die \"IPv6 Stateless Address Autoconfiguration\" nach RFC 4862 ist darin flexibler.\n\nDas einfachste Einsatzszenario ist es, zwei Computer mit einem Crossover-Kabel zu verbinden. Jedes Gerät sucht sich dann automatisch eine freie IP-Adresse und kann anschließend mittels Internet Protocol (IP) unter dieser Adresse angesprochen werden.\n\nDiese Aufgabenstellung ist nicht neu und wurde mit AppleTalk unter Mac OS und NetBIOS unter Microsoft Windows schon weitgehend gelöst. Zur Nutzung muss man allerdings die IP-Adresse der Gegenstelle kennen, weshalb Zeroconf weiter zielt.\n\nDabei handelt es sich um einen auf dem Address Resolution Protocol (ARP) aufbauenden Mechanismus, um für eine Netzwerkschnittstelle automatisch eine freie IP-Adresse auszuwählen. Für diesen Zweck ist von der IANA der Adressbereich 169.254.0.0/16 vorgesehen, wobei die ersten und die letzten 256 Adressen in diesem Bereich für zukünftige Anwendungen reserviert sind.\n\nWenn ein Rechner eine Link-Local-IP-Adresse konfigurieren will, wählt er eine zufällige IP-Adresse zwischen 169.254.1.0 und 169.254.254.255 aus. Bei der Erzeugung der IP-Adresse sollte eine rechnerspezifische Information einfließen, wie etwa die MAC-Adresse der Netzwerkschnittstelle, damit möglichst jedes Mal dieselbe IP-Adresse generiert wird (sie wird also nur \"pseudo\"zufällig gewählt).\n\nNach der Auswahl seiner IP-Adresse muss der Rechner diese für sich beanspruchen und testen, ob diese nicht bereits von einem anderen Rechner verwendet wird. Dieser Test muss durchgeführt werden, bevor die IP-Adresse als Senderadresse in IP- oder ARP-Paketen verwendet wird, und genau dann, wenn eine Netzwerkschnittstelle aktiviert wird. Das kann etwa das Einschalten oder Rebooten des Rechners, das Aufwachen aus einem Sleep-Modus, das Einstecken eines Ethernet-Kabels oder das automatische Einhängen eines Rechners in ein WLAN sein.\nExplizit verboten ist, diese Überprüfung periodisch durchzuführen, da das eine Verschwendung von Netzwerkressourcen darstellen würde und im eigentlichen Test auf Adresskonflikte schon eine Möglichkeit vorgesehen ist, eventuelle Konflikte auch passiv zu erkennen und darauf zu reagieren.\n\nDie Überprüfung von Adressen auf Konflikte erfolgt mit Hilfe von ARP-probes. Ein ARP-probe ist ein ARP-Paket, bei dem die Absender-IP-Adresse auf 0.0.0.0 gesetzt wird und als Empfänger-IP-Adresse die zu überprüfende Adresse verwendet wird.\n\nSobald der Rechner bereit ist, mit der Konfliktüberprüfung zu beginnen, wartet er eine zufällig lange Zeit zwischen 1 und 2 Sekunden, und sendet dann drei ARP-probes mit einem zufälligen Zeitabstand von 1 bis 2 Sekunden aus. Empfängt der Rechner zwischen dem Testanfang und 2 Sekunden nach Versenden der letzten ARP-probe ein ARP-Paket, bei dem die Absender-IP-Adresse der zu überprüfenden IP-Adresse entspricht, so wurde ein Konflikt gefunden. Der Rechner muss diese Prozedur dann mit einer anderen generierten IP-Adresse wiederholen.\n\nWird in diesem Zeitraum ein anderes ARP-probe empfangen, das als Empfänger-IP-Adresse die zu testende IP-Adresse enthält, und deren Absender-MAC-Adresse keiner der MAC-Adressen der Netzwerkschnittstelle des Rechners entspricht, so muss vom Rechner ebenfalls eine neue IP-Adresse generiert und überprüft werden. Das kann beispielsweise passieren, wenn zwei oder mehrere Rechner gleichzeitig versuchen, dieselbe Link-Local-Adresse zu konfigurieren.\n\nUm ARP-Stürme und damit eine Überlastung des lokalen Netzwerkes zu vermeiden, wenn es zu mehreren Konflikten kurz hintereinander kommt, muss jeder Rechner nach zehn Fehlversuchen die Geschwindigkeit, mit der er neue Adressen auswählt und überprüft, auf maximal eine Überprüfung pro Minute reduzieren.\n\nKonnte der Rechner keinen Konflikt feststellen, so hat er erfolgreich die generierte IP-Adresse für sich beansprucht. Er muss diese dann bekannt machen, indem er zwei ARP-announcements mit einem Abstand von 2 Sekunden versendet. Ein ARP-announcement unterscheidet sich von einem ARP-probe nur dadurch, dass als Absender- und Empfänger-IP-Adresse die neu ausgewählte Link-Local-IP-Adresse eingesetzt wird.\n\nDie Erkennung von Konflikten muss dauerhaft auch nach dem Auswählen einer verwendbaren IP-Adresse stattfinden. Empfängt der Rechner ein ARP-Paket, das von einem anderen Rechner versendet wurde, und als Sender-IP-Adresse die eigene IP-Adresse enthält, so handelt es sich um einen Adressenkonflikt.\n\nDer Rechner hat nun zwei Möglichkeiten, nämlich eine neue IP-Adresse auszuwählen oder seine IP-Adresse zu verteidigen. Letzteres ist zu bevorzugen, wenn der Rechner noch TCP-Verbindungen offen hat. Wurden noch keine kollidierenden ARP-Pakete empfangen, erfolgt die Verteidigung der Adresse durch den Rechner dadurch, dass dieser ein ARP-announcement aussendet. Konnte der Rechner jedoch in den letzten Sekunden schon einen Adresskonflikt feststellen, so muss er eine neue IP-Adresse auswählen, um eine Endlosschleife zu vermeiden, wenn zwei Rechner mit gleicher IP-Adresse versuchen, diese gegenüber dem anderen zu verteidigen.\n\nDie Probleme, Namen und IP-Adressen ohne DNS-Server zu übersetzen sowie einen Mechanismus zum automatischen Veröffentlichen und Finden von Netzdiensten verfügbar zu haben, wurden von der Zeroconf Working Group praktischerweise zusammengefasst und in Form der zwei grundsätzlich unabhängigen, jedoch einander gut ergänzenden Protokolle \"Multicast DNS\" (mDNS) und DNS-Based Service Discovery (DNS-SD) zu Papier gebracht.\n\nmDNS ist dabei nichts anderes als eine Beschreibung, wie Clients verfahren müssen, wenn sie DNS-Anfragen an Multicast-Adressen senden, und wie eine Gruppe von Rechnern damit umgeht, sodass die Anfrage richtig und ohne erhöhte Last auf dem Netzwerk beantwortet wird. DNS-SD dagegen spezifiziert eine Konvention für die Verwendung von existierenden DNS-Record-Typen, die das Browsen und Veröffentlichen von Netzwerkdiensten mit DNS ermöglicht.\n\nmDNS legt fest, dass die DNS-Top-Level-Domain „.local.“ link-lokal ist. Anfragen und Antworten, die mit „.local.“ zu tun haben, ergeben ähnlich IP-Adressen aus dem 169.254.0.0/16- bzw. fe80::/10-Bereich nur im lokalen Netz Sinn. Alle DNS-Abfragen für Namen, die auf „.local.“ enden, müssen mit UDP und IP Multicast an die mDNS-Multicast-Adresse (IPv4: 224.0.0.251, IPv6: ff02::fb, UDP-Port 5353) gesendet werden. Ist kein anderer DNS-Server verfügbar, können auch Anfragen, die nicht auf „.local.“ enden, an diese Adresse gesendet werden.\n\nJedem Rechner steht es übrigens frei, sich einen eigenen Rechnernamen aus der „.local.“-Domain auszuwählen. Im Gegensatz zu anderen, öffentlichen Top-Level-Domains gibt es keinerlei Formalitäten, außer dass bereits vergebene Rechnernamen nicht mehr verwendet werden sollten. Natürlich kann es in der Praxis zu Konflikten kommen, von den Erfindern von mDNS wurde das aber als sehr unwahrscheinlich angenommen. Eine Konfliktlösung wird sogar bewusst nicht diskutiert, da es sinnvolle Anwendungen für den Fall geben kann, dass mehrere Rechner den gleichen Namen tragen – etwa für Lastverteilungs- oder Hochverfügbarkeits-Lösungen.\n\nSoll über eine Reverse-Mapping-DNS-Abfrage der Hostname zu einer link-lokalen IP-Adresse ermittelt werden, so muss auch diese an die mDNS-Multicast-Adresse (siehe oben) gesendet werden.\n\nUm den Netzwerkverkehr möglichst gering zu halten, haben sich die Erfinder von mDNS ein paar Verhaltensregeln einfallen lassen, die doppelte oder gar mehrfache mDNS-Abfragen und -Antworten verhindern sollen.\n\nEine Möglichkeit zur Unterdrückung von bereits bekannten Antworten besteht darin, dass der Client, der eine mDNS-Abfrage versendet, schon bekannte Antworten, die noch in seinem Cache zu finden sind, als Answer Records an seine Abfrage anhängt. Sieht nun ein anderer Client, der die Abfrage beantworten könnte, seine vorgeschlagene Antwort in der Liste der bereits gegebenen Antworten und ist die TTL (Time to live) noch mehr als die Hälfte der üblichen TTL, muss er diese nicht mehr versenden. Ist die TTL zu gering, muss der andere Client eine Antwort versenden, um die im Cache des ersten Clients gespeicherte TTL zu aktualisieren.\n\nWenn mehrere Clients in etwa zur selben Zeit dieselbe Abfrage versenden würden, käme es zu redundantem Netzwerkverkehr. Plant ein Client eine Anfrage zu versenden und sieht eine Abfrage eines anderen Clients desselben Inhalts, sollte er die fremde Abfrage als seine eigene betrachten und die Antwort darauf verwenden, anstatt eine eigene Abfrage zu versenden.\n\nWenn ein Server, während er die Versendung einer Antwort vorbereitet, eine Antwort eines anderen Servers desselben Inhalts sieht und die TTL (Time to live) der fremden Antwort nicht kleiner als die der eigenen geplanten ist, dann sollte er seine Antwort als gesendet betrachten (also keine eigene Antwort senden).\n\nDie erste Implementierung von Zeroconf wurde von Apple durchgeführt und heißt Bonjour (früher \"Rendezvous\"). Es ist seit der Version 10.2 („Jaguar,“ 2002) in Mac OS X integriert, und Apple liefert die meisten Programme, soweit es sinnvoll ist, mit Bonjour-Funktionalitäten aus.\n\nEine Komponente von Bonjour ist das mDNSResponder-Projekt, das von Apple im Sourcecode unter der Apache-Lizenz veröffentlicht wurde. mDNSResponder implementiert dabei mDNS und DNS-SD. Es wurde von Apple portierbar entworfen und implementiert, so dass der Quelltext nicht nur unter Mac OS X ab 10.2, sondern auch unter Linux, FreeBSD, NetBSD, OpenBSD, Solaris, VxWorks, Mac OS 9 und Microsoft Windows ohne Anpassungen übersetzt werden kann.\n\nEin bekannter Fehler in der mDNS-Implementierung von Apple führt unter Windows 7 häufig zu dem Problem, dass ein Default-Gateway von 0.0.0.0 eingetragen wird, der zeitlich vor dem per DHCP bezogenen Gateway herangezogen wird und damit die Internet-Verbindung des Rechners effektiv lahmlegt. Eine einfache Lösung ist hier die Deaktivierung des Bonjour-Dienstes per Service-Manager.\n\nMit Avahi existiert weiterhin eine freie (LGPL) und portierfähige Implementierung von mDNS/DNS-SD, die heutzutage in allen Linux-Distributionen Standard ist.\n\nIn Microsoft Windows ist eine automatische IP-Adressen-Vergabe seit Windows 98 implementiert. Sie entspricht jedoch nicht vollständig dem RFC der IETF. Microsoft nennt dieses Verfahren \"Automatic Private IP Addressing\" oder kurz APIPA.\n\nDamit unter Linux die Namensauflösung bei der Top-Level-Domain .local wie gewohnt über den DNS-Server (z. B. BIND) abgewickelt wird, ist bei älteren Implementierungen der Eintrag codice_1 in der Datei codice_2 erforderlich.\n\n\n\n"}
{"id": "372690", "url": "https://de.wikipedia.org/wiki?curid=372690", "title": "Selective Sequence Electronic Calculator", "text": "Selective Sequence Electronic Calculator\n\nDer Selective Sequence Electronic Calculator (SSEC) wurde 1946/47 unter der Leitung des Columbia-Professors Wallace John Eckert vom Watson Scientific Computing Laboratory gebaut und war ein Hybridcomputer, der sowohl aus 12.500 Röhren als auch aus 21.400 mechanischen Relais bestand. Er stand im IBM-Hauptquartier in Manhattan und bedeckte dort die Wände eines 18 × 9 Meter großen Raumes, nach anderen Quellen eine 18, eine 12 und eine 24 Meter lange Wand. Am 27. Januar 1948 nahm er die Arbeit auf.\n\nEine wichtige Aufgabe erfüllte SSEC: Er berechnete Mondpositionen, mit denen die Apollo-Landungen ermöglicht wurden. Jede Position erforderte 11.000 Additionen und Subtraktionen, 9.000 Multiplikationen und 2.000 Suchanfragen an eine Datenbank. In jede Berechnung mussten wiederum 1.600 Beziehungen einbezogen werden. Dieser Vorgang dauerte – zum Erstaunen der Zuschauer – sieben Minuten.\n\nDas Einzelstück wurde im Juli 1952 wieder demontiert, um Platz für den neuen \"Defense Calculator\" (IBM 701) zu machen.\n\nVon technischer Seite (Eckert war Astronom) war bei IBM insbesondere Frank E. Hamilton beteiligt.\n"}
{"id": "374034", "url": "https://de.wikipedia.org/wiki?curid=374034", "title": "Tottaste", "text": "Tottaste\n\nDer Begriff Tottaste () bezeichnet eine Taste auf einer Tastatur, die nach dem Drücken keinen Buchstabenvorschub erzeugt und nicht sofort angezeigt wird, sondern eine Art „Wartemodus“ aktiviert. Erst ein weiterer Tastendruck erzeugt dann ein kombiniertes Zeichen (Grundzeichen und Diakritika – wie è, ĉ, ñ, ś oder ů).\n\nMit Hilfe solcher Tastenkombinationen ist es dem Anwender möglich, beispielsweise auch auf einer Tastatur mit deutscher Tastaturbelegung französischsprachige Buchstaben wie é einzugeben, ohne dass die entsprechenden Tasten auf der Tastatur vorhanden sein müssen. \n\nAuf mechanischen Schreibmaschinen wird beim Druck auf eine Tottaste nur das Zeichen auf das Blatt geschrieben, ohne den Wagen weiter zu bewegen. Als Nächstes kann ein beliebiger Buchstabe getippt werden, der dann akzentuiert auf dem Blatt steht.\nDie Eingabe in umgekehrter Reihenfolge – so wie es in Handschrift üblich ist: erst den Buchstaben und dann den Akzent – wurde selten eingesetzt, da der Wagen ein Zeichen zurücklaufen müsste, was technisch aufwendiger ist. Auf einigen Remington-Schreibmaschinen wird dies aber mit einer Type umgangen, die ein Zeichen weiter links aufschlägt.\n\nAus Gewohnheitsgründen wurde das deutlich weiter verbreitete System mit Tottasten auf moderne Computertastaturen übernommen und ist heute weitgehend etabliert.\n\nSo besitzen Schweizer Tastaturen die Umlaut-Tottaste , mit der es möglich ist, Umlaute in Großbuchstaben wie Ä, welche auf Schweizer Tastaturen fehlen, einzugeben. Ein weiteres Beispiel ist die Akzent-Taste mit »´« und »`« (auf deutschen Tastaturen meist rechts neben „ß“). Wird diese Taste gedrückt, passiert zunächst einmal nichts; drückt man anschließend auf beispielsweise ein , wird das Zeichen „é“ ausgegeben.\n\nIm Normalfall – und in den meisten Eingabestandards auch vorgeschrieben – ergibt die \"Tottaste\" (Beispiel: ) und der anschließende Druck auf die Leertaste das Zeichen selbst (nicht mit dem Apostroph »’« oder dem Minutenzeichen »′« bzw. dem für beide vorgesehenen Ersatzzeichen »'« zu verwechseln).\n\nAuf deutschen Tastaturen sind noch der Zirkumflex (»^«) und die Tilde (»~«) oft als Tottaste ausgeführt. Allerdings stirbt die Tilde als Tottaste auf deutschen Tastaturen aus, da kaum noch ein Tastaturtreiber sie als Tottaste aufführt. Für hauptsächlich (beispielsweise) spanischsprachige Nutzung oder für osteuropäische Sprachen gibt es Tastaturen (bzw. Treibersoftware) mit den jeweiligen spezifischen Tottasten.\n\nBenutzer, die vorwiegend programmieren oder aus anderen Gründen die Sonderzeichen unmittelbar benötigen, verwenden gern eine Tastaturbelegung ohne Tottasten, da Zeichen wie »^« dann oft eine eigene Bedeutung haben und sonst nur mit einem zusätzlichen Leerzeichen eingegeben werden können. Die Eingabe von beispielsweise französischen Texten verkompliziert sich dadurch erheblich, aber Software-Quelltexte lassen sich besser erstellen.\n\nEine spezielle, besonders auf unixoiden Systemen verbreitete Tottaste ist die Compose-Taste.\n"}
{"id": "375257", "url": "https://de.wikipedia.org/wiki?curid=375257", "title": "Iteriertes Funktionensystem", "text": "Iteriertes Funktionensystem\n\nEin iteriertes Funktionensystem (IFS) ist eine Menge formula_1 von Funktionen, die denselben Raum formula_2 als Definitions- und Wertebereich haben und unter Verknüpfung abgeschlossen sind. Also\n\nIterierte Funktionensysteme dienen meist der Konstruktion von Fraktalen, die dann auch als \"IFS–Fraktale\" bezeichnet werden. Bekannte Vertreter dieser Klasse von Fraktalen sind das Sierpinski-Dreieck und die Koch-Kurve wie auch die Grenzmengen von Lindenmayer-Systemen.\n\nDiese Art der Fraktalkonstruktion wurde 1981 von John Hutchinson erfunden und später von Michael F. Barnsley mit seinem Buch \"Fractals Everywhere\" popularisiert. Dort gab Barnsley auch den \"Collage-Satz\" an, welcher die Grundlage der fraktalen Bildkompression bildet. Diese Art, Bilder effizient mittels Datenstrukturen zu kodieren, hat sich jedoch nie richtig durchsetzen können und wird heute im Wesentlichen nur noch als Hybridverfahren in Kombination mit einer Wavelet-Transformation untersucht.\n\nUm für ein IFS Eigenschaften ableiten zu können, muss die Funktionenmenge zusätzliche Voraussetzungen erfüllen. Üblicherweise, wenn von \"IFS\" gesprochen wird, werden diese Voraussetzungen stillschweigend als gegeben angenommen. Diese Voraussetzungen sind \n\nUnter diesen Umständen gibt es eine invariante, selbstähnliche Menge formula_7.\nSelbstähnliche Mengen haben meist keine ganzzahlige Hausdorff-Dimension und werden dann auch als Fraktal bezeichnet, deshalb die Bezeichnung \"IFS-Fraktal\". Man könnte auch weitergehend den Begriff der Selbstähnlichkeit durch die Forderung der Existenz eines IFS definieren.\n\nMathematisch gesehen handelt es sich bei der Theorie der \"iterierten Funktionensysteme\", wie auch die Begrifflichkeit vermuten lässt, um eine direkte Anwendung des\nbanachschen Fixpunktsatzes, wobei mehrere Funktionen statt einer betrachtet werden und, statt eines eindeutigen Fixpunktes, sich eine invariante, meist fraktale, Teilmenge des Raumes formula_2 ergibt. Zur Illustration wird meist das zweidimensionale Einheitsquadrat formula_14 mit dem euklidischen Abstand gewählt.\n\nWir beginnen also mit einer endlichen Menge von Funktionen eines kompakten metrischen Raumes formula_15 in sich selbst:\nvon denen wir voraussetzen, dass es eine Kontraktionskonstante formula_17 gibt mit\n\nDurch Iteration setzen wir formula_19 zu einem IFS formula_20 fort, es sei\nund erhalten schließlich\n\nDer Beweis des Satzes erfolgt dadurch, dass man aus dem metrischen Raumes formula_15 einen neuen Raum konstruiert, dessen „Punkte“ genau die kompakten Teilmengen von formula_2 sind. Hierauf kann man eine Metrik definieren (die Hausdorff-Metrik), bezüglich der dieser Raum vollständig und die Abbildung formula_25 eine Kontraktion ist. Dadurch wird der banachsche Fixpunktsatz anwendbar.\n\nDie Gestalt der fraktalen Menge formula_8 kann durch ein so genanntes \"Chaosspiel\" visualisiert werden. Dabei wird zunächst\nein Fixpunkt formula_27 von formula_28\naufgesucht und auf diesen in zufälliger Reihenfolge die definierenden Funktionen angewandt. Als Algorithmus kann dies wie folgt aussehen:\n\n\nAnmerkung:\n\nEine weitere Möglichkeit der Darstellung, vorzugsweise für die affinen Fraktale, ist die \"rekursive\" Approximation der Menge formula_8. Dies wird meist anschaulich mittels eines Fotokopierers\nerklärt: Man macht verschiedene Verkleinerungen eines Ausgangsbildes, fixiert diese nach Vorschrift\nauf einem neuen Blatt und benutzt dieses dann als Ausgangsbild des nächsten Schrittes.\n\nAuch die \"Turtle-Grafik\", die zur Konstruktion der L-Systeme\nverwendet wird, folgt einer ähnlichen Idee. \n\nAls Algorithmus braucht man dazu eine rekursiv aufrufbare Funktion, welche die Zuordnung \nformula_39 bei einer beliebigen Menge formula_40\nrealisiert. Die Implementierung benötigt einen Stackspeicher, in welchem das jeweils aktuelle Koordinatensystem als affine\nKoordinatentransformationen festgehalten wird. Damit ergibt sich als Algorithmus\nformula_41:\n\nFraktal:\n\nDie erzeugenden Funktionen des IFS seien affine Abbildungen des zweidimensionalen\nEinheitsquadrates in sich selbst. Jede Funktion formula_48 ist gegeben durch eine 2×2–Matrix formula_49 und einen Verschiebungsvektor formula_50.\n\nDas Koch-Fraktal wird z. B. von folgendem System von 2 Funktionen erzeugt:\n\nDie klassische Methode zur Erzeugung der Koch-Kurve benutzt 4 Funktionen\n\nDas rechtwinklige Sierpinski-Dreieck wird erzeugt von\n\nGrundlage für die Begeisterung für solche IFS–Fraktale war das Collage–Theorem von Barnsley. Es besagt, dass jede kompakte Menge – jede Gestalt – durch ein IFS–Fraktal beliebig genau angenähert werden kann. Die Grundlage dafür sind folgende Beobachtungen:\n\nAnschaulicher: Haben wir in einem 100×100–Pixelbild eine Figur von 500 schwarzen Pixelpunkten, so können wir das Bild um den Faktor 100 auf die Größe eines Pixels verkleinern und mit diesem einen schwarzen Punkt dann wieder die Figur malen, indem wir ihn auf jeden der zugehörigen 500 Pixel abbilden. Diese Vorgehensweise ist bei weitem nicht optimal, hier wäre das einfache Speichern der Positionen der 500 Pixel einfacher. Aber wenn wir für den gleichen Zweck mit nur fünf Abbildungen auskämen, wäre eine Datenreduktion erzielt. \n\nWir sind auch nicht auf einfache Schwarzweißbilder eingeschränkt. Bei einem Graustufenbild kann der Grad der Schwärzung als dritte Koordinate des Punktes aufgefasst werden, es ergibt sich eine kompakte Fläche im dreidimensionalen Raum, auf welche wieder das Collage–Theorem angewendet werden kann. Mit systematischen Verfahren zur Konstruktion eines IFS–Fraktals mit möglichst wenigen Funktionen befasst sich die Fraktale Bildkompression sowie die Fraktale Tonkompression.\n\n"}
{"id": "377261", "url": "https://de.wikipedia.org/wiki?curid=377261", "title": "Ubuntu", "text": "Ubuntu\n\nUbuntu (auch Ubuntu Linux) ist eine Linux-Distribution, die auf Debian basiert. Der Name Ubuntu bedeutet auf Zulu etwa „Menschlichkeit“ und bezeichnet eine afrikanische Philosophie. Die Entwickler verfolgen mit dem System das Ziel, ein einfach zu installierendes und leicht zu bedienendes Betriebssystem mit aufeinander abgestimmter Software zu schaffen. Das Projekt wird vom Software-Hersteller Canonical gesponsert, der vom südafrikanischen Unternehmer Mark Shuttleworth gegründet wurde.\n\nUbuntu konnte seit dem Erscheinen der ersten Version im Oktober 2004 seine Bekanntheit stetig steigern und ist inzwischen eine der meistgenutzten Linux-Distributionen. Die Nutzerzahl wird auf etwa 25 Millionen geschätzt. Neben Ubuntu selbst, das von Version 11.04 bis 17.04 standardmäßig die von der Ubuntu-Entwicklergemeinschaft selbst entwickelte Desktop-Umgebung Unity einsetzte und ab Version 17.10 wieder auf Gnome basiert, existieren verschiedene Abwandlungen. Zu den offiziellen Unterprojekten gehören unter anderem Kubuntu mit KDE, Xubuntu mit Xfce, Ubuntu MATE mit MATE, Ubuntu Budgie mit Budgie sowie Ubuntu Studio, das speziell auf die Anforderungen von Audio-, Grafik- und Videobearbeitung ausgerichtet ist. Neue Ubuntu-Versionen erscheinen jedes halbe Jahr im April (04er-Versionen) und im Oktober (10er-Versionen). Die derzeit aktuelle Version ist 18.10 Cosmic Cuttlefish, die nächste Version wird im April 2019 erscheinen.\n\nInitiiert wurde das Ubuntu-Projekt in den frühen 2000er-Jahren durch den südafrikanischen Multimillionär Mark Shuttleworth. Seine Absicht war das Entwickeln eines Betriebssystems, das möglichst allen Menschen zur Verfügung steht. Der Begriff \"Ubuntu\" stammt aus den Sprachen der afrikanischen Völker Zulu und Xhosa und steht für „Menschlichkeit“ und „Gemeinsinn“, aber auch für den Glauben an ein universelles Band des Teilens, das alles Menschliche verbindet. Weitere Ziele des Projekts sind die Verbesserung der Internationalisierung und der Barrierefreiheit, damit die angebotene Software für so viele Menschen wie möglich benutzbar wird. Derzeit kommen hier hauptsächlich die Übersetzungen und Hilfsmittel für Barrierefreiheit aus dem Gnome-Projekt zum Tragen.\n\nShuttleworth finanziert einen Großteil des Projektes, wodurch dieses weitaus größere finanzielle Mittel zur Verfügung hat als die meisten anderen Distributionen und ist selbst als Entwickler tätig. Neben ihm arbeiten ungefähr 40 Personen hauptberuflich an der Weiterentwicklung von Ubuntu. Diese entstammen überwiegend den Debian- und Gnome-Online-Entwicklungsgemeinschaften. Finanziert wird die Entwicklung durch das von Shuttleworth gegründete Unternehmen Canonical, das das System auch vermarktet.\n\nDie erste Version von Ubuntu erschien im Oktober 2004 unter dem Namen Warty Warthog. Seitdem erscheint alle sechs Monate eine neue Fassung des Betriebssystems. Am 1. Juli 2005 wurde von Shuttleworth und Canonical die Ubuntu Foundation mit einem Startkapital von 10 Millionen US-Dollar ins Leben gerufen. Diese soll die Pflege der Ubuntu-Versionen nach deren Erscheinen übernehmen und die Weiterentwicklung unterstützen. Um ihre Ziele zu verwirklichen und neue Versionen zu ermöglichen, soll die \"Ubuntu Foundation\" Mitglieder der Kern-Community einstellen.\n\nEine weitere Finanzierungsquelle entstammt der seit der Fassung 12.10 eingebauten Suchfunktion, die anonymisiert Suchanfragen an den Onlinehändler Amazon verschickt.\n\nUbuntu wurde bereits kurz nach der Veröffentlichung der ersten Version auch über die Fachwelt hinaus in den Medien stark beachtet. Über die Website von Canonical ließen sich kostenlose Ubuntu- und Kubuntu-CDs bestellen; heutzutage ist dieses Angebot kostenpflichtig. Sämtliche Ubuntu-Versionen können jedoch nach wie vor kostenlos im Internet heruntergeladen werden. Die heruntergeladene Installationsdatei kann auf eine DVD gebrannt oder auf einen USB-Stick kopiert werden. Dies ermöglicht die Installation aus einem laufenden Live-System heraus. In den Jahren 2006 und 2007 erhielt Ubuntu in einer Umfrage der Website desktoplinux.com zur Verbreitung verschiedener Linux-Distributionen 30 % der Stimmen und erzielte damit in beiden Jahren den ersten Platz. Auch auf der Website Distrowatch belegte Ubuntu in den Jahren 2005 bis 2010 den ersten Platz, danach wurde es von Linux Mint abgelöst. Linux Mint wird seit 2006 entwickelt und basiert auf Ubuntu. Auf Webservern nimmt Ubuntu nach Debian und CentOS Rang drei unter den Linux-Distributionen ein.Der Hardwareanbieter Dell begann in den USA im Mai 2007, kurze Zeit später auch in Deutschland, Frankreich und Großbritannien, Ubuntu vorinstalliert auf einigen seiner Computermodelle anzubieten.\n\nDie französische Nationalversammlung stellte 2007 die Rechner der Abgeordneten und ihrer Assistenten auf Ubuntu um. Die Förderung von freier Software hatte das Parlament bereits Ende 2006 beschlossen. Diese Umstellung betraf 1154 Rechner. Im Januar 2008 begann auch die französische Gendarmerie mit der Umstellung von 70.000 Arbeitsplatzrechnern von Windows auf Ubuntu.\n\nDie Regierung Mazedoniens kündigte 2016 an, für die Schüler des Landes 20.000 Thin-Client-Systeme auf Basis von Edubuntu beschaffen. Hierbei sollen an einen Server jeweils sieben Clients angeschlossen werden, sodass theoretisch bis zu 140.000 Schüler diese Clients nutzen können.\n\nIn Andalusien werden 220.000 Ubuntu-Desktops in Schulen eingesetzt. AMTRON, ein indischer Telekommunikationsanbieter, übergab jedem Schüler mit überdurchschnittlichen Abschlussklausuren im Bundesstaat Assam einen PC mit der Ubuntu Desktop-Edition, insgesamt 28.000 PCs wurden übergeben. Im Rahmen der Umstellung der Münchner Stadtverwaltung auf Linux (LiMux-Projekt) wurde 2010 auf Ubuntu gewechselt. Die LVM Versicherung setzt seit April 2011 auf ungefähr 10.000 Laptops und Desktop-Rechnern Ubuntu Desktop 10.04.2 LTS ein.\n\nUbuntu wird von zahlreichen Software-Anbietern im Test berücksichtigt. So läuft die ELSTER-Steuersoftware auch auf Ubuntu Linux 17.04.\n\nUbuntu basiert auf Debian, wobei das Paketformat (.deb) und diverse Strukturen übernommen wurden. Zu Beginn eines Entwicklungszyklus wird ein Teil der Pakete mit denen aus \"Debian unstable\" abgeglichen, insbesondere die des \"main\"-Bereichs werden aber vollständig alleine gepflegt. Hierdurch wird der Arbeitsaufwand für die Wartung der weniger wichtigen Programme reduziert. Alle Änderungen und Verbesserungen an Debian-Paketen, die in Ubuntu vorgenommen werden, stehen dem Debian-Projekt als Patches zur Verfügung. Theoretisch ist es aufgrund der strengen Paketdefinitionen auch möglich, Programmpakete aus Debian direkt zu benutzen, in der Praxis gibt es hierbei jedoch insbesondere bei systemnahen Funktionen aufgrund diverser Detailunterschiede oftmals Probleme.\nAb Version 11.10 ist das \"Software Center\" in Ubuntu integriert. Dieses dem Apple App Store und dem Google Play Store nachempfundene Programm dient zur einfachen Installation von Software. Die Entwickler von Ubuntu bezwecken hiermit, die verfügbaren Applikationen für das Betriebssystem langfristig deutlich zu steigern.\n\nNach der Standardinstallation von Ubuntu ist ein Administrator-Benutzerkonto („Root-Account“) zwar vorhanden, dieses ist aber – wie bei macOS – durch ein ungültiges Kennwort deaktiviert. Es ist daher in der Standardkonfiguration nicht möglich, sich direkt als „root“ anzumelden, wodurch das ungewollte Starten von Programmen mit Administratorrechten und eine möglicherweise dadurch verursachte nachteilige Änderung am System verhindert wird. Mithilfe des Befehls sudo kann das mit eingeschränkten Rechten ausgestattete Benutzerkonto allerdings vorübergehend vollständige Systemprivilegien erhalten, was etwa zur Installation mancher Programme notwendig ist.\n\nUbuntu verwendete als Desktop-Umgebung anfänglich Gnome. Mit Version 11.04 wurde diese, bis einschließlich Version 17.04, durch Unity abgelöst. Die später hinzugekommenen Abwandlungen Kubuntu, Xubuntu und Lubuntu verwenden KDE, Xfce beziehungsweise LXDE. Die verschiedenen Abwandlungen unterscheiden sich lediglich durch die Vorauswahl der standardmäßig installierten Software-Pakete und können daher technisch als unterschiedliche Konfigurationen einer Distribution angesehen werden. Grundsätzlich entstammen sämtliche Programme, inklusive der Konfigurationsprogramme, der gleichen Desktop-Umgebung, wodurch die Bedienung konsistent gehalten wird. Die Programme anderer Umgebungen können jedoch über die Paketverwaltung nachinstalliert werden, ebenso weitere aus einem Fundus von über 30.000 Paketen.\n\nUbuntu ist hauptsächlich für die Rechnerarchitektur IA-32 (auch bekannt als x86 32-Bit) inklusive x64 (x86 64-Bit, implementiert durch AMD64 oder Intel 64) verfügbar. Die offizielle Unterstützung für PowerPC wurde mit Erscheinen der Version 7.04 eingestellt, da Apple-Rechner seit 2006 mit Intel-Architektur ausgeliefert werden und das Anpassen der Distribution für die verbliebenen PowerPC-Nutzer nicht mehr als wirtschaftlich angesehen wird. Diese Aufgabe soll jedoch durch die Community weiter geleistet werden. Für die Sun-SPARC-Architektur gab es nur eine Server-Version bis einschließlich Version 7.10.\n\nBei Versionen, die vor 2006 erschienen sind, waren Installations-CD und Live-CD getrennt, sodass Ubuntu nicht von der Live-CD installiert werden konnte. Version 6.06 LTS ermöglichte es erstmals, das System von der Live-CD (auch Desktop-CD genannt) aus zu installieren. Dadurch ist es möglich, während der Installation im Internet zu surfen, E-Mails zu lesen oder andere Aufgaben zu erledigen. Die spezielle Installations-CD (jetzt \"Alternate-CD\" genannt) wird nur noch benötigt, wenn besondere Anforderungen wie beispielsweise LVM-Einrichtung bei der Installation umgesetzt werden sollen oder das System wenig Arbeitsspeicher besitzt. Ab der Version 12.10 gibt es diese nur noch für die Variante Lubuntu. Daneben gab es zeitweise eine DVD-Version, die sowohl einen Live-Modus wie auch eine direkte Installation und mehr Pakete beinhaltete. Die Variante Ubuntu Studio wird dagegen immer nur als DVD-(Installation) angeboten.\n\nAls weitere Besonderheit, insbesondere gegenüber Windows, speichert das System die Hardware-Konfiguration weitgehend nicht auf der Festplatte, sondern erkennt sie automatisch beim Systemstart; ab Version 8.10 gilt dies auch für die Grafikkarte. So ist ein Austausch der Hardware oder sogar ein Einbau einer mit Ubuntu bespielten Festplatte in einem völlig anderen PC teilweise ohne jede Anpassung möglich.\n\nNach eigenem Bekunden liegt Ubuntus Schwerpunkt auf der Benutzerfreundlichkeit. Die Standard-Installation stellt jeweils nur ein Programm für die üblichen Anwendungsgebiete – etwa E-Mail-, Browser- oder Office-Pakete – bereit, wodurch die bei anderen Linux-Distributionen häufigen Redundanzen vermieden werden. Die Auswahl erfolgt überwiegend nach dem eingesetzten GUI-Toolkit zur Programmierung der grafischen Benutzeroberfläche. Kubuntu verwendet hier Qt, alle anderen Versionen GTK+. Eine Ausnahme bildet LibreOffice, das sich nach Meinung der Entwickler gegenüber KOffice und den Gnome-Office-Programmen durch stabileren Betrieb und besseren Umgang mit Microsoft-Office-Dateien auszeichnet. Weitere Kriterien sind die Integration in die Desktop-Umgebung und der Entwicklungsstand der Programme.\n\nEin weiterer Aspekt der Benutzerfreundlichkeit ist die Automatisierung der Konfiguration des Systems. So wird beispielsweise beim Umgang mit Grafikkartentreibern im Idealfall direkt der beste unter einer freien Lizenz verfügbare Gerätetreiber ausgewählt. Falls ein proprietärer Treiber notwendig ist – etwa für die Unterstützung von 3D-Beschleunigung – kann dieser über ein grafisches Konfigurationsprogramm installiert werden.\n\nAußerdem existiert das eigene Online-Übersetzungswerkzeug \"Rosetta\". Als Richtlinie für erlaubte Lizenzen für die Paketquellen \"main\" und \"universe\" (siehe Aufteilung der Programm-Pakete) werden die Debian Free Software Guidelines verwendet, unfreie Softwarepakete jedoch – anders als etwa bei Debian – nicht kategorisch ausgeschlossen. Solche unfreien Pakete werden dort automatisch installiert, wo freie Software noch nicht den vollen Funktionsumfang gewährleisten kann, beispielsweise bei Gerätetreibern. Unter anderem für diesen Pragmatismus wird Ubuntu von der Free Software Foundation und anderen strengen Verfechtern freier Software kritisiert; andere Benutzer sehen jedoch genau darin einen der größten Vorteile.\n\nWie bei allen Debian-Derivaten sind die Programmpakete in mehrere Paketquellen aufgeteilt. Bei Ubuntu erfolgt die Zuordnung anhand zweier Kriterien. Zum einen wird danach unterschieden, ob es sich um freie Software handelt, zum anderen danach, ob das Programm zu einer der Varianten der Grundausstattung gehört oder ob es grundsätzlich optional ist. Die Paketquellen werden unterschiedlich intensiv durch das Ubuntu-Team betreut. Eine Einschränkung des Kundendiensts auf nur einen Teil der Pakete ist eine Besonderheit von Ubuntu und in Debian nicht vorhanden.\n\nDie Paketquelle \"main\" umfasst jene Pakete, die den Ubuntu-Lizenzanforderungen (Debian Free Software Guidelines) entsprechen und direkt durch das Ubuntu-Team unterstützt werden. Die Pakete dieser Quelle sind in einer der Ubuntu-Varianten Bestandteil der Standardinstallation und meist aufeinander abgestimmt. Für alle Pakete in diesem Bereich stellt das Ubuntu-Team kommerziellen Kundendienst und Sicherheits-Korrekturen zur Verfügung.\n\nZu dem Bereich \"restricted\" gehört Software, die von den Ubuntu-Entwicklern wegen ihrer Wichtigkeit unterstützt wird, die aber wegen fehlender geeigneter Lizenz nicht in \"main\" integriert werden kann. Es handelt sich insbesondere um Pakete für nur im Binärformat vorliegende Grafikkarten-Treiber. Die Unterstützung ist geringer als die für \"main\", da die Entwickler keinen Zugriff auf den Quellcode haben.\n\nDer Bereich \"universe\" umfasst ein breites Spektrum an freier Software, die nicht direkt durch das Ubuntu-Team unterstützt wird. Die meisten dieser Pakete entstammen \"Debian unstable,\" werden aber in einer Ubuntu-Version nicht aktualisiert, wenn eine neuere Version des Paketes in Debian unstable vorhanden ist. Daneben gibt es ein gesondertes Team namens \"Masters of the Universe\", das diese Pakete betreut, allerdings werden keine Sicherheits-Aktualisierungen garantiert.\n\nZu \"multiverse\" gehört optionale Software, die entweder nicht unter einer freien Lizenz steht oder aufgrund von Softwarepatenten nicht frei verteilt werden darf. Diese Programme werden nur eingeschränkt gepflegt. Sicherheitsupdates und Korrekturen sind – ähnlich wie bei \"restricted\" – durch den fehlenden Zugriff auf den Quellcode oft nicht möglich.\n\nIm speziellen Teil \"commercial\" befindet sich von Canonical zertifizierte Software kommerzieller Anbieter. Dazu können Software-Hersteller ihre Programme zertifizieren lassen, wenn diese kompatibel mit Ubuntu sind und sich vollständig entfernen lassen. Dabei kann neben freier Software auch nicht freie Software zertifiziert werden. Proprietäre Anwendungen können jedoch nicht Teil der Kern-Distribution von Ubuntu werden. Momentan enthält dieses Verzeichnis unter anderem den Opera-Browser, den RealPlayer, die Anti-Viren-Software Panda DesktopSecure und den VMware-Server.\n\nMultimediaprogramme und proprietäre Codecs, die aufgrund von Patent- oder Urheberrechten nicht in allen Ländern frei vertrieben werden können, wurden bis November 2013 in einer inoffiziellen Paketquelle namens Medibuntu nachgeliefert. Enthalten waren unter anderem Google Earth, gängige Codecs wie DivX und libdvdcss, Windows Media Video und QuickTime. Libdvdcss wird seitdem von VideoLAN.org bereitgestellt.\n\nUbuntu wird offiziell auf den x86-, AMD64- und ARM-Architekturen unterstützt. Inoffiziell kann man Ubuntu auf den PowerPC- (ppc), SPARC- (sparc64) und PA-RISC-Architekturen zum Laufen bringen.\n\nAls minimale Hardware-Anforderungen wurden für 8.10 Ubuntu 256 Megabyte Arbeitsspeicher und ein Prozessor-Modell mit 500 Megahertz von Ubuntu selbst genannt. Auf der Festplatte wird ein freier Speicherplatz von etwa 5 Gigabyte empfohlen, belegt werden rund 2,5 Gigabyte, zudem wird bei Updates temporär rund ein Gigabyte weiterer Platz benötigt. Diese Anforderungen änderten sich mit aktuelleren Versionen nur geringfügig.\n\nJede Version hat einen eigenen Codenamen und eine Versionsnummer, die auf dem jeweiligen Veröffentlichungsjahr und -monat basiert. So steht beispielsweise \"5.10\" für Oktober 2005. Die Codenamen sind Tierarten mit einem vorangestellten Adjektiv, so dass eine Alliteration entsteht. Ab der Version „Dapper Drake“ werden die Alliterationen alphabetisch fortgeführt (\"Dapper, Edgy, Feisty\" usw.). Das Projekt hat sich zum Ziel gesetzt, alle sechs Monate eine neue Version der Distribution zu veröffentlichen, wobei jede Version mindestens neun Monate mit Sicherheitskorrekturen versorgt wird. Des Weiteren wird in etwa zweijährigen Abständen eine Version mit Langzeitunterstützung (englisch: long-term support, kurz LTS) angeboten, die fünf Jahre lang mit Updates versorgt wird. LTS-Versionen bis einschließlich 10.04 wurden nur in der Server-Variante fünf, in der Desktop-Variante hingegen lediglich drei Jahre unterstützt. Außerdem wurden nicht-LTS-Versionen bis einschließlich 12.10 18 Monate lang gepflegt. Die erste LTS-Version ist am 1. Juni 2006 erschienen.\n\nDer Veröffentlichungszyklus von Ubuntu mit neuen Versionen jedes halbe Jahr orientiert sich an dem der Desktop-Umgebung Gnome, sodass immer kurz nach der Veröffentlichung einer neuen Gnome-Version eine neue Ubuntu-Version vorliegt, die diese integriert (Ausnahme: Ubuntu 13.04 mit Gnome 3.6 statt 3.8). Hierdurch versucht Ubuntu, zumindest bezogen auf die grafische Benutzeroberfläche, eine besonders aktuelle Distribution zu sein. Auch als die Unity-Oberfläche noch in Ubuntu enthalten war, diente Gnome nach wie vor als Basis für jede Ubuntu-Version.\n\nDie erste Version 4.10 \"\" (englisch für: \"warziges Warzenschwein\") war im Wesentlichen ein weitgehend fertig konfiguriertes Debian-System. Sie ist am 20. Oktober 2004 erschienen. Das System gab es zur Installation auf IA-32-, AMD64- und PowerPC-Systemen, für IA-32-Systeme gab es zusätzlich eine Live-CD. Bereits mit dieser Version wurde der Versand-Service (siehe Verbreitung) gestartet. Die Version 4.10 wurde bis zum 30. April 2006 unterstützt.\n\nMit der Version 5.04 \"\" (englisch für: \"ergrauter Igel\") am 8. April 2005 wurde eine grafische Aktualisierungsverwaltung eingeführt. Diese Version machte vor allem durch eine weitgehend automatische Hardwareunterstützung auch und gerade für Laptops von sich reden. Seit dieser Version wird in Form der Variante \"Kubuntu\" auch KDE unterstützt. Diese Version wurde bis zum 31. Oktober 2006 unterstützt.\n\nDie Version 5.10 \"\" (englisch für: \"zuversichtlicher Dachs\") erschien am 13. Oktober 2005 und fügte einen Installationsmodus für OEM-Systeme, bei dem Benutzername und Kennwort nicht eingegeben werden müssen, hinzu. Als Compiler dient in dieser Version erstmals ein GCC 4.0, der weitere Optimierungen ermöglichte. Außerdem wird der Start des Systems mit einem Statusbalken, genannt \"usplash\", grafisch aufbereitet. Zudem wurde für die Verwaltung der Systementwicklung das Online-Programm Launchpad und das darin enthaltene Übersetzungsprogramm \"Rosetta\" eingeführt. Version 5.10 wurde bis zum 13. April 2007 unterstützt.\n\nBei der Version 6.06 LTS \"\" (englisch für: \"adretter Erpel\"), die am 1. Juni 2006 erschien, wurde erstmals vom Sechs-Monats-Rhythmus abgewichen, um den Entwicklern zusätzliche Zeit zum Finden und Beheben von Programmfehlern, zum Testen, zur Verbesserung der asiatischen Sprachunterstützung und zur Linux-Standard-Base-Zertifizierung zu geben. Sie ist die erste sogenannte Long-Term-Support-Version: Desktopanwender erhielten bis zum 14. Juli 2009, Serveranwender bis zum 1. Juni 2011 Updates und Bugfixes. Weitere Neuerungen in dieser Version waren der \"Ubiquity\"-Installer auf den Live-CDs, die damit zugleich zum Standard wurden. Mit dieser Version gibt es erstmals den Ableger \"Xubuntu\", der Xfce als Desktop-Umgebung nutzt.\n\nZwei Monate nach der Freigabe von Ubuntu 6.06 entschloss sich das Ubuntu-Team, eine aktualisierte Version \"(6.06.1)\" herauszugeben. Diese beinhaltet im Wesentlichen über 300 Sicherheits- und Fehlerkorrekturen sowie eine aktualisierte Übersetzung. Auch ein großer Teil des Gnome-Desktops 2.14.3 ist hinzugekommen. Ebenso wurden Fehler im grafischen Installationsprogramm der Desktop-CD (Live-CD) beseitigt und alle mitgelieferten Programme aktualisiert. Im Januar 2008 erfolgte eine weitere derartige Aktualisierung als \"6.06.2\".\n\nDie Version 6.10 \"\" (englisch für: \"nervöser Jungmolch\") vom 26. Oktober 2006 ist das erste reguläre Release nach dem \"6.06 LTS\". Auf Grund der zweimonatigen Verzögerungen bei der Fertigstellung der Vorversion wurde die Entwicklungszeit für Edgy auf vier Monate verkürzt und war deshalb als Versuchs-Release gedacht, bei dem nicht gezielt auf Stabilität geachtet wurde. Dennoch sind die sichtbaren Verbesserungen gering, neu sind vor allem das Initialisierungssystem Upstart und automatisierte Fehlerberichte. Diverse weitere Verbesserungen wurden angefangen. Außerdem werden mit dieser Version einige auf dem wegen seiner Nähe zu Microsoft nicht unumstrittenen Mono-Projekt basierende Anwendungen wie Tomboy oder F-Spot integriert. Die offizielle Unterstützung für diese Version endete am 25. April 2008.\n\nMit der Version 7.04 ' (englisch für: \"keckes Rehkitz\") vom 19. April 2007 werden viele Verbesserungen des auf \"6.06 LTS\" folgenden, eher experimentellen ' in der Praxis nutzbar. Hier gibt es unter anderem einen Migrationsassistenten, KVM, einen Installationsassistenten für unfreie Codecs und Treiber, Desktop-Effekte (compiz) und WPA-Unterstützung. Die PowerPC-Version wird, bedingt durch die Umstellung auf IA-32-Prozessoren von Intel bei Apple-Computern, nur noch inoffiziell weiterentwickelt. Die offizielle Unterstützung dieser Version endete am 19. Oktober 2008.\n\nDie Version 7.10 \"\" (englisch für: \"mutiger Gibbon\") ist am 18. Oktober 2007 erschienen. Neu sind hier das freie Flash-Plug-in Gnash, Mozilla Firefox Version 3 Alpha (in universe), teilweise automatisch aktivierte Desktop-Effekte über Compiz Fusion, ein ganz neu gestaltetes Konfigurationsprogramm für die Optik des Desktops (Themes, Effekte, Hintergrundbild), ein grafisches Konfigurationsprogramm für das X Window System, wobei Monitore jetzt dynamisch erkannt werden. Außerdem AppArmor, die Desktopsuche Meta Tracker und eine neue Druckerverwaltung \"system-config-printer\", die aus Fedora stammt. Auch für Drucker gibt es jetzt eine automatische Erkennung und Einrichtung. Für die konventionelle Installation wird jetzt eine Verschlüsselung der Festplatte unterstützt. Die offizielle Unterstützung dieser Version endete am 18. April 2009.\n\nAm 24. April 2008 ist mit der Version 8.04 LTS \"\" (englisch für: \"kühner Reiher\") wieder ein über längere Zeit – bis 12. Mai 2011 als Desktop-System und bis 9. Mai 2013 als Server-System – unterstütztes Release veröffentlicht worden, wobei allerdings die Langzeitunterstützung aufgrund des Erscheinens von KDE 4 nicht für die Kubuntu-Variante galt. Die neuen Funktionen wurden auf der Ubuntu-Entwicklerkonferenz (28. Oktober bis 3. November 2007) festgelegt. Neu sind diverse Sicherheitsfunktionen wie PolicyKit (womit Systemprogramme nur bestimmte Sonderrechte, aber keinen vollen root-Benutzer benötigen), SELinux und ein erweiterter Speicherschutz. An Programmen wurde das Brennprogramm Brasero neu aufgenommen, als Browser diente ursprünglich die Version 3.0 Beta 5 von Mozilla Firefox, da man keine Pflege der Version 2.0.x über die drei Jahre garantieren konnte. Die endgültige Version wurde als Aktualisierungspaket („update“) nachgeliefert und ist auch in 8.04.1 enthalten. Der veraltete Soundserver EsounD wurde durch PulseAudio ersetzt. Auch eine einfache, kommandozeilenbasierte Personal Firewall gibt es jetzt. Neuerdings wird iSCSI (ausdrücklich zu aktivieren) und Active Directory unterstützt. Daneben ist das Installationsprogramm Wubi aufgenommen worden, das die Installation von Ubuntu auf eine Windows-Partition erlaubt. Einige der Änderungen am Kernel entstammen der Linux-Distribution Kanotix, die ihrerseits den Ubuntu-Kernel nutzt. Diese Version unterstützt sowohl eine Aktualisierung von der direkt vorhergehenden Version 7.10 wie auch von der letzten LTS-Version 6.06.\n\nWie bei allen LTS-Releases wurden mehrere aktualisierte Versionen zum Download bereitgestellt, um nicht alle Verbesserungen nachträglich herunterladen zu müssen und die Stabilität als Live-CD zu erhöhen. Edubuntu ist seit dieser Version keine eigenständige Distribution mehr, sondern stellt ein Erweiterungspaket für ein Standard-Ubuntu-System dar.\n\nDie auf Server optimierte Variante der am 24. April 2008 erschienenen Version 8.04 LTS \" wurde noch bis Anfang Mai 2013 mit Sicherheits- und Stabilitätsaktualisierungen versorgt. Die Unterstützung der Desktop-Variante wurde am 12. Mai 2011 beendet.\n\nAm 30. Oktober 2008 ist die Version 8.10 mit Codenamen \" (englisch für \"unerschrockener Steinbock\") erschienen. In dieser finden sich diverse Detailverbesserungen; so wurden die Roaming-Fähigkeiten mobiler Systeme verbessert, um beispielsweise bei ausreichender Netzverfügbarkeit auf dem Weg vom Büro mit dem Zug bis nach Hause nie die Internetverbindung zu verlieren. Auch kann die neue Version des NetworkManager jetzt auch mit UMTS-Verbindungen umgehen. Eine weitere Neuerung ist ein Gastkonto ohne jede Zugriffsrechte auf die Festplatte. Die nicht vom eigentlichen Kernel-Team betreuten Kernel-Module werden jetzt über Dynamic Kernel Module Support verwaltet, wodurch diese automatisch an die jeweilige Kernel-Version angepasst werden, statt dass es hierfür unzählige eigene Pakete geben muss. Zudem lassen sich so Treiber-Pakete über mehrere Ubuntu-Versionen hinweg einsetzen. Des Weiteren kommt der X-Server seit dieser Version ohne eine Konfigurationsdatei aus, wodurch sich das System automatisch an die meisten Hardware-Änderungen anpasst; außerhalb des X-Servers benötigen bereits ältere Ubuntu-Versionen keinerlei manuelle Konfiguration. Ein ganz neues optisches Design wurde unter dem Namen \"DarkRoom\" zwar entwickelt, ist aber in der fertigen Version standardmäßig nicht aktiviert; hier kommt eine leicht veränderte Version des bekannten \"Human\"-Designs zum Einsatz. Die Details zu den Neuerungen wurden auf der Ubuntu-Entwicklerkonferenz vom 19. bis 23. Mai 2008 in Prag besprochen.\n\nDie Version 9.04 ist am 23. April 2009 unter dem Namen \"\" (englisch für \"lebhafte Jackalope\") erschienen. Die meisten Änderungen beziehen sich auf weniger sichtbare Verbesserungen am Unterbau des Systems; vor allem wurde die Startzeit gegenüber den vorherigen Versionen deutlich verbessert. Darüber gibt es eine ganze Reihe kleinerer Verbesserungen an der Benutzeroberfläche; insbesondere ein neues Benachrichtigungs-System. Darüber hinaus sollen in Zukunft für einige Pakete tagesaktuelle Versionen zur Verfügung gestellt werden; ein erster Schritt hierzu sind aktuelle Kernel-Versionen.\n\nDie Version 9.10 ist am 29. Oktober 2009 unter dem Namen \"\" (englisch für \"karmischer Koala\") erschienen. Sie unterstützt das bei Version 9.04 noch im letzten Moment zurückgezogene Dateisystem ext4 und enthält den Webbrowser Mozilla Firefox und die freie Bürosuite OpenOffice.org in ihren jeweils aktuellen Versionen. Diverse bereits in den Vorversionen begonnene Verbesserungen wurden hier erneut fortgesetzt, so setzt das Bootsystem jetzt komplett auf Upstart.\n\nWeitere teilweise deutliche Änderungen betreffen den Bootscreen, die verschiedenen Themes, die erstmals in größerer Anzahl mitgelieferten Wallpaper und das erstmals mitgelieferte \"Ubuntu Software Center\". Die Unterstützung für diese Version wurde am 30. April 2011 beendet.\n\nDie Version 10.04 LTS ist am 29. April 2010 unter dem Namen \"\" (englisch für \"klarer Luchs\") erschienen. Die auffälligste Neuerung ist eine neue optische Gestaltung mit zwei neuen Farboptionen (\"Ambiance\" mit dunklen und \"Radiance\" mit hellen Akzenten) bis hin zu einem überarbeiteten Ubuntu-Logo. Die Schaltfläche für die Fensterfunktionen ist jetzt wie unter Mac OS auf der linken Seite angeordnet. Technisch gibt es vor allem eine weitergehende Integration von Funktionen für soziale Netzwerke und von Ubuntu One. Die Bootzeit wurde weiter optimiert, insbesondere wird jetzt komplett auf HAL verzichtet und unter dem Namen \"plymouth\" die Splash-Screen-Funktionalität neu implementiert. Die Bildbearbeitung GIMP wurde aus Platzgründen durch die einfachere Fotoverarbeitungssoftware F-Spot ersetzt und gehört nicht mehr zum Standardumfang der Distribution, wird aber nach wie vor uneingeschränkt unterstützt. Am 18. August 2010, 18. Februar 2011, 22. Juli 2011 und 16. Februar 2012 sind aktualisierte CD-Images mit allen zwischenzeitlich erschienenen Updates als \"10.04.1\", \"10.04.2\", \"10.04.3\" bzw. \"10.04.4\" veröffentlicht worden. Die Unterstützung der Desktop-Variante wurde am 9. Mai 2013 beendet.\n\nDie Version 10.10 wurde am 10. Oktober 2010 unter dem Namen \"\" (englisch für: \"eigenwilliges Erdmännchen\") veröffentlicht. Zu den wenigen sichtbaren Neuerungen gehören eine eigens für Ubuntu entwickelte Schriftart namens „Ubuntu“ und ein deutlich erweitertes Software-Center, das jetzt auch kommerzielle Software anbietet. Wie bei den Versionen unmittelbar nach einer LTS üblich sind in der Struktur einige Neuerungen (etwa der Abschied von HAL) und es gibt experimentelle Funktionen, wie etwa das Dateisystem btrfs.\nDie Unterstützung für diese Version wurde am 11. April 2012 beendet.\n\nDie Version 11.04 mit dem Namen „“ (englisch für „Schicker Narwal“) erschien am 28. April 2011. Mit dieser Version setzte die Hauptversion standardmäßig nicht mehr auf die Gnome-Oberfläche, sondern auf die von Canonical selbst entwickelte Unity-Oberfläche. Die Unterstützung für diese Version wurde am 28. Oktober 2012 beendet.\n\nDie Version 11.10 mit dem Namen „“ (englisch für „traumhafter Ozelot“) erschien am 13. Oktober 2011. Eingebaut wurde auch der Linux-Kernel 3.0. Zudem ersetzte LightDM den Gnome Display Manager. Unity wurde nach Gnome 3 portiert. Es ersetzte auch auf Rechnern ohne Grafik-Beschleunigung durch OpenGL die Desktop-Umgebung Gnome 2.32. Die Unterstützung für diese Version wurde am 9. Mai 2013 beendet.\n\nDie Version 12.04 LTS (LTS = long term support, englisch für Langzeitunterstützung) ist am 26. April 2012 erschienen und trägt den Namen \"\" (englisch für \"fehlerfreies oder gewissenhaftes Schuppentier\"). Zum ersten Mal wird auch die LTS-Desktop-Variante 5 Jahre unterstützt statt wie bisher nur die LTS-Server-Versionen. Ubuntu 12.04 ist das erste LTS-Release mit der Desktop-Benutzeroberfläche Unity.\n\nDie Version 12.10 ist am 18. Oktober 2012 erschienen und trägt den Namen \"\" (englisch für \"Quanten-Quetzal\"). Neben Aktualisierungen diverser Pakete wurden vor allem Verbesserungen bei der Oberfläche Unity vorgenommen, die bspw. nunmehr eine Vorschau auf Dateiinhalte ermöglicht. Zugleich wurde in die Suche die Anzeige von Produktwerbung der Internetplattform Amazon eingefügt, was auf stark geteilte Meinung stieß (siehe auch Abschnitt \"Rechtliches\"). Ebenfalls wurde Unity 2D wegen der Einführung von LLVMpipe wieder entfernt.\n\nDie Version 13.04 ist am 25. April 2013 erschienen und trägt den Namen \"\" (englisch für \"Enthusiastisches Katzenfrett\"). Cristian Parrino, Canonicals Vizepräsident für Online-Dienste, hat für die Version 13.04 eine weitere Vertiefung der Shopping-Funktion angekündigt. Die Linux-Distribution weitet die Online-Suche aus und wird neben den eigenen Angeboten und Amazon auch weitere Online-Händler und Quellen wie YouTube und Last.fm miteinbeziehen. Laut Parrino wird der Suchbegriff zunächst analysiert, um relevante Anbieter herauszufiltern. Als Ergebnis liefert die Suchfunktion dann einen Mix aus lokalen und externen Fundstellen. Die weiteren Vorschläge sollen zudem auf weitere Online-Händler ausgeweitet werden. Zum Direktkauf \"(Instant Purchasing)\" wird kein Webbrowser mehr benötigt. Stattdessen werden Einkäufe im hauseigenen Software Center und Music Store direkt vom Desktop aus vorgenommen. Die Unterstützung für diese Version wurde am 27. Januar 2014 beendet.\n\nDie Version 13.10 ist am 17. Oktober 2013 erschienen und trägt den Namen \"\" (englisch für \"Naseweiser Salamander\"). Gegenüber der vorigen Version wurde der Code für den Einsatz auf mobilen Geräten optimiert. Die Unterstützung für diese Version wurde am 17. Juli 2014 beendet.\n\nDie Version 14.04 LTS ist am 17. April 2014 erschienen und trägt den Namen \"\" (englisch für \"Treuer Tahr\").\n\nDie Version 14.10 ist am 23. Oktober 2014 erschienen und trägt den Namen \"\" (englisch für \"Utopisches Einhorn\"). Die Unterstützung für diese Version wurde im Juli 2015 beendet.\n\nDie Version 15.04 ist am 23. April 2015 erschienen und trägt den Namen \"\" (englisch für \"Lebhafte Grüne Meerkatze\"). Die Unterstützung für diese Version wurde im Januar 2016 beendet.\n\nDie Version 15.10 erschien am 22. Oktober 2015 unter dem Namen \"\" (englisch für \"hinterlistiger Werwolf\"). Die Unterstützung für diese Version wurde im Juli 2016 beendet.\n\nDie Version 16.04 erschien am 21. April 2016 unter dem Namen \"\" (englisch für \"gastfreundliches Borstenhörnchen\"). Am 19. Juli 2016 erschien die erste Updateversion 16.04.1, am 16. Februar 2017 die zweite Updateversion 16.04.2 und am 2. August 2018 die bislang letzte, fünfte Updateversion 16.04.5.\n\nDie Version 16.10 wurde am 21. April 2016 durch Mark Shuttleworth auf den Namen \" benannt und erschien am 13. Oktober 2016. Die Unterstützung für diese Version wurde am 20. Juli 2017 beendet.\n\nDie Version 17.04 ist am 13. April 2017 erschienen und trägt den Namen \" (englisch für \"Begeisterte Hüpfmaus\"). Dies war die letzte Version mit Unity-Desktop. Die Unterstützung für diese Version wurde am 13. Januar 2018 beendet.\n\nDie Version 17.10 ist am 19. Oktober 2017 erschienen und trägt den Namen \"Artful Aardvark\" (englisch für Kunstvolles Erdferkel). Diese Version wechselt vom Unity-Desktop zurück zum Gnome-Desktop. Die Unterstützung für diese Version wurde im Juli 2018 beendet.\n\nDie Version 18.04 ist am 27. April 2018 erschienen und trägt den Namen \"\" (englisch für \"Bionischer Biber\"). Da es sich um eine LTS-Version handelt, wird sie bis ins Jahre 2023 für private Heimanwender und bis 2028 für professionelle Benutzer im kostenpflichtigen Rahmen des Extended Security Maintenance (ESM) unterstützt werden. Diese Version enthält einen neuen textbasierten Installer für die Server-Version, sowie die Option \"Minimale Installation\" im Desktop-Installer bei der nur grundlegende Tools installiert werden. Aus Stabilitätsgründen wird in dieser Version standardmäßig wieder X-Server verwendet, aber Wayland ist weiterhin Teil der Standard-Installation und kann statt X-Server verwendet werden. Die erste Updateversion 18.04.1 erschien am 26. Juli 2018 und die zweite Updateversion 18.04.2 erschien am 15. Februar 2019.\n\nUbuntu 18.10\n\nDie Version 18.10 ist am 18. Oktober 2018 erschienen und trägt den Namen \"Cosmic Cuttlefish\" (englisch für \"kosmischer\" \"Tintenfisch\"). Es besitzt ein neues Standardtheme mit einer neuen Startmelodie. Die neuen Symbole wirken moderner und sind zweidimensional (\"material design\") und wärmer. Die Version wird bis zum 18. Juli 2019 unterstützt.\n\nMark Shuttleworth plante mit seinem Team, Ubuntu auch auf Smartphones, Tablets und Smart TVs als vollwertige Plattform zu portieren. Als Zeithorizont war hierfür Ubuntu 14.04 genannt worden. Im Zuge dieser Neuausrichtung trat Canonical am 9. Januar 2012 zum ersten Mal auf der Consumer Electronics Show (CES) in Las Vegas auf und präsentierte Ubuntu TV. Am 2. Januar 2013 wurde eine Videopräsentation für Ubuntu auf Smartphones veröffentlicht. Ab April 2015 war mit dem BQ Aquaris E4.5 Ubuntu Edition das erste Smartphone mit Ubuntu Touch auf dem EU-Markt verfügbar, seit Juni 2015 auch das \"BQ Aquaris E5 Ubuntu Edition\" sowie das \"Meizu MX4 Ubuntu Edition\"\n\nAnfang 2013 wurde, unter anderem im Rahmen des \"Ubuntu Developer Summit\" im März, darüber diskutiert, Ubuntu auf Rolling Releases umzustellen. Die Idee sah vor, nur noch alle zwei Jahre eine LTS-Version mit Langzeitunterstützung zu veröffentlichen und zwischen zwei LTS-Versionen die Programmpakete ständig zu aktualisieren. Shuttleworth sprach sich jedoch gegen Rolling Releases aus und verwies Benutzer, die sich ständig aktuelle Programmpakete wünschten, auf die täglichen Entwicklungsversionen (sog. \"Daily Builds\"). Schließlich stimmte auch die technische Leitung von Ubuntu gegen die Einführung von Rolling Releases. Gleichzeitig beschloss man, dass LTS-Versionen in den ersten beiden Jahren der fünfjährigen Support-Dauer nicht nur Patches, sondern auch neue Programmversionen erhalten sollen.\n\nAls weiteres größeres Projekt kündigte Shuttleworth im November 2010 an, dass der Display-Server Wayland den bisher eingesetzten X.Org-Server ersetzen solle. Erklärtes Ziel war es, ein \"Convergence\" genanntes Prinzip zu realisieren, bei dem ein an einen Bildschirm angeschlossenes Smartphone ein vollwertiges PC-System bildet. Da der X.Org-Server technisch als nicht kompatibel erschien, plante und entwickelte Canonical ab 2013 den eigenen Display-Server \"Mir\". Dies stieß bei Teilen der Linux-Community auf Ablehnung und führte zu einer teilweise heftigen Kontroverse, da eine Zersplitterung des Linux-Ökosystems befürchtet wurde. Zudem wurde damit die Zukunft von Ubuntu-Derivaten ungewiss.\n\nAm 5. April 2017 gab Mark Shuttleworth bekannt, dass die Arbeiten an Unity und dem neuen Display-Server Mir zugunsten von Cloud-Lösungen und Internet der Dinge eingestellt werden. Seit Ubuntu 17.10 ist Gnome wieder der standardmäßige Desktop von Ubuntu. Auch Ubuntu Touch wurde eingestellt.\n\nDas Konzept von Ubuntu sieht vor, möglichst wenig redundante Software zu enthalten. Dies führte dazu, dass von den Ubuntu-Entwicklern mehrere Ubuntu-Derivate erstellt wurden, die sich ausschließlich in der Auswahl der Software im Rahmen der Standardinstallation unterscheiden. Schlussendlich beruhen alle diese Neuzusammenstellungen nur auf einer unterschiedlichen Auswahl der bei der Erstinstallation enthaltenen Pakete. Basissystem, Installationsprogramm und Repositories sind hingegen identisch. Somit ist es auch möglich, eine Distribution durch Nachinstallieren um den Funktionsumfang eines anderen Derivats zu erweitern. Die verschiedenen Varianten drücken somit nur Präferenzen des Benutzers aus, auf die sich dieser schon bei der Grundinstallation festlegen möchte, sie können als „Konfigurationsvarianten“ derselben Linux-Distribution angesehen werden. Versionen mit anderer Desktop-Umgebung (wie beispielsweise Kubuntu) weisen jedoch untereinander einen teilweise enormen Unterschied in Optik und Bedienung auf.\nNeben den diversen offiziellen Ausgaben gibt es eine Reihe spezialisierter, von externen Entwicklern herausgegebene, inoffizielle Ubuntu-Derivate.\n\nDie Ubuntu Server Edition ist eine Zusammenstellung mit mehr Serverdiensten statt grafischer Benutzeroberfläche. Vor der Version 12.04 hatte die Server Edition einen anders ausgelegten Kernel als das normale Ubuntu, das bis dahin zwei Jahre weniger \"Long Time Support\" als die Server Edition bekam. Seither führen Installationen der beiden Zusammenstellungen bei Wahl der jeweils minimalen Ausstattung zum gleichen Ergebnis, das mit allem der beiden Zusammenstellungen nachgerüstet werden kann.\n\nUnter der Bezeichnung JeOS \"(Just Enough Operating System)\" gibt es ab Version 8.04.x eine minimalistische Untervariante des Servers, optimiert für den Einsatz in virtuellen Umgebungen wie VMware. Ab Ubuntu 8.10 war diese Version als Option in die normale Server-Version integriert.\n\nMit der zunehmenden Bedeutung von Cloud Computing und Anwendungscontainern wie Docker oder LXC wurde diese Variante 2014 in \"Ubuntu Core\" umbenannt. Sie erhält derzeit eine neuartige Paketverwaltung namens \"Snappy\", die transaktionale Updates und Rollback unterstützt.\n\nSeit Juli 2018 bietet Canonical Ubuntu auch in der Minimal Variante an. Minimal Ubuntu ist eine im Vergleich zu Ubuntu Core nochmals deutlich verkleinerte Version. Erreicht wird die Verkleinerung unter anderem durch eine Minimierung der vorinstallierten Pakete auf das Notwendigste. Daher eignet es sich noch besser als Basis für Container in der Cloud.\n\n\"Kubuntu\" ist eine Ubuntu-Variante mit der Desktop-Umgebung KDE, die erstmals mit der Version \"5.04\" erschienen ist. Das vorangestellte „K“ steht dabei als übliche Abkürzung für „KDE“.\n\nKDE-Anwendungen lassen sich im Vergleich zu Gnome umfangreicher konfigurieren, was einige Benutzer als Vorteil, andere – wegen geringerer Übersichtlichkeit – als Nachteil sehen. Zum Einsatz kommt stets die aktuelle Version des KDE Plasma Workspace zusammen mit anderen Applikationen aus dem KDE-Umfeld, wie dem Audio-Player Amarok oder dem Brennprogramm K3b. Als Office-Paket dient – als einziges nicht auf KDE-Bibliotheken basierendes Programm – LibreOffice.\n\nIn der ersten Ubuntu-Version (4.10) war KDE lediglich in \"universe\" enthalten, in dem sich Programme befinden, die einfach nur aus Debian übernommen sind. Zudem musste universe zu dieser Zeit noch ausdrücklich freigeschaltet werden. Aufgrund starker Nachfrage nach KDE wurde mit Version 5.04 dann eine Variante gestartet, die sich bis auf den anderen Desktop nicht von Ubuntu unterscheidet. Dies ist die erste Variante dieser Art und stellt einen Kompromiss zwischen dem Ziel der Redundanzvermeidung und der Bedienung unterschiedlicher Interessen dar. Auf den ersten Blick sind Kubuntu und Ubuntu völlig verschiedene Systeme, selbst die Boot-Meldungen reden von \"kubuntu\". Dennoch basieren sie auf den gleichen Paketquellen. Diese Vorgehensweise gibt es sonst bei keiner Linux-Distribution.\n\nAnfangs wurde Kubuntu fast nur von einem Team Freiwilliger entwickelt, einzig Jonathan Riddell war direkt bei Canonical angestellt, was für einigen Unmut sorgte. Nach dem ersten LTS-Release wurde angekündigt, dass die KDE-Unterstützung den gleichen Status wie die für Gnome haben soll. Dies spiegelt sich unter anderem darin wider, dass es für Kubuntu einen kommerziellen Kundendienst durch Canonical gibt.\n\nDie Version 8.04 ist keine LTS-Version. Für die KDE-Version 4 konnte keine hinreichende Stabilität, für KDE 3.5 kein Support für drei Jahre gewährleistet werden. Kubuntu 8.04 existiert daher auch in zwei Varianten, der normalen \"kubuntu\" mit KDE 3.5.9 und \"kubuntu-kde4\" mit KDE 4.0.3. Kommerzieller Kundendienst für 18 Monate und der Versandservice stehen nur für die KDE-3-Version zur Verfügung.\n\nVersion 8.10 gibt es nur mit KDE 4.1. Die automatische Aktualisierung („upgrade“) auf diese Version wurde daher für die Version 8.04 standardmäßig deaktiviert, da es eine vollständige Deinstallation von KDE 3 zur Folge hätte. Die Aktualisierung kann aber trotzdem über die Kommandozeile ausgeführt werden.\n\n\"Xubuntu\" ist eine Abwandlung von Ubuntu, die die ressourcensparende Desktop-Umgebung Xfce- verwendet. Daher eignet sich Xubuntu für ältere Computer mit wenig Arbeitsspeicher, bei denen die Installation der Standardversion problematisch ist. Eigene Installations- und Live-CDs von Xubuntu wurden erstmals mit der Ubuntu-Version 6.06 LTS veröffentlicht. Diese Version wird ausschließlich von Freiwilligen entwickelt. Zum Umfang gehören hier etwa die Textverarbeitung AbiWord, die Tabellenkalkulation Gnumeric, Mozilla Firefox und Thunderbird, aber auch eine auf die reine Textverarbeitung reduzierte Version von LibreOffice.org, bzw. OpenOffice.org, falls der Funktionsumfang des wesentlich sparsameren \"Abiword\" nicht ausreicht. Hinzu kommen diverse Hilfsprogramme aus der normalen Ubuntu-Version, etwa die Aktualisierungsverwaltung, der NetworkManager oder die Paketverwaltung Synaptic. Als Hardwareanforderungen werden 128 MB Arbeitsspeicher (256 MB werden jedoch empfohlen) und 1,5 GB freier Festplattenspeicher benötigt. Diese Angaben sind jedoch im Gegensatz zu den Angaben für die anderen Ubuntu-Varianten am technisch Machbaren und nicht an einem tatsächlich benutzbaren System orientiert. Auf der Festplatte werden mit den deutschen Lokalisierungen fast zwei GB belegt, bei Aktualisierungen kommt weiterhin Platzbedarf in der Größenordnung der CD hinzu.\n\n\"Lubuntu\" ist ein Derivat, das für den Einsatz auf leistungsschwächerer Hardware konzipiert ist. Als Desktop-Umgebung setzte es das besonders ressourcensparende LXDE ein, durch den Einsatz von Openbox als Fenstermanager wird nur etwa halb so viel Arbeitsspeicher wie bei Xubuntu benötigt. Lubuntu 11.10 wurde mit der Veröffentlichung im Oktober 2011 ein offizielles Derivat von Ubuntu. Seit Version 18.10 wird LXQt als Desktop-Umgebung verwendet.\n\n\"Edubuntu\" ist eine für die Verwendung in der Schule entwickelte Erweiterung zu Ubuntu, das auf dem Ubuntu Linux Terminal Server Project aufsetzt. Der Name setzt sich aus „education“ (englisch für Bildung) und „Ubuntu“ zusammen. Einige Funktionen des eingestellten Projekts \"Skubuntu\" wurden in Edubuntu integriert. Kernkomponenten von Edubuntu sind die Lernprogramme GCompris, Kalzium (KDE), Tux4Kids und der Schooltool Calendar.\n\nIn den Versionen 5.10 bis 7.10 war Edubuntu eine eigenständige Ubuntu-Version, bei der der normale Funktionsumfang aus Platzgründen zu Gunsten der Bildungsprogramme eingeschränkt war. Ab Version 8.04 LTS ist Edubuntu eine Erweiterung, die auf ein installiertes Ubuntu-System aufsetzt. Dieses kann über den Paketmanager nachinstalliert werden. Ab Version 11.10 ist Edubuntu allerdings auch wieder als eigenständige Version verfügbar.\n\nAm erklärte Projektleiter Stéphane Graber, dass keine Edubuntu-Version auf Basis von Ubuntu 16.04 veröffentlicht wird. Zeitgleich sicherte er eine reguläre Betreuung der LTS-Version Edubuntu 14.04 bis zum Supportende im Jahr 2019 zu.\n\n\"Ubuntu Studio\" ist speziell auf die Anforderungen von Audio-, Grafik- und Videobearbeitung ausgerichtet. Für solche Zwecke wurde den Ubuntu-Quellen ein Kernel mit Echtzeitfunktionen hinzugefügt, der dafür keinerlei Energiesparfunktionen unterstützt. Als Audio-Backend dient das Programm JACK. Außerdem gehören diverse Multimedia-Programme zum Umfang, darunter CinePaint, die Videobearbeitung OpenShot, der Audio-Sequenzer Rosegarden, der Audio-Editor Ardour und das DTP-Programm Scribus. Die erste stabile Version ist am 10. Mai 2007 erschienen und basiert auf Ubuntu 7.04. Diese wurde als einzige Ubuntu-Version als ein DVD-Image mit knapp 900 MB ausgeliefert. Ab der Version 7.10 ist Ubuntu Studio auch für x64 verfügbar. Auf der Ubuntu-Website wird Ubuntu Studio als „Recognized Derivative“ (anerkanntes Derivat) aufgeführt.\n\n\"Mythbuntu\" ist eine Variante, um den PC als HTPC einzusetzen. Hierbei wird ein auf ein Minimum reduziertes Xfce (siehe Xubuntu) verwendet und die HTPC-Software MythTV installiert. Darüber hinaus gibt es ein eigenes Kontrollzentrum, mit dem MythTV konfiguriert werden kann und über das dessen diverse Plug-ins installiert werden können. Auch die Installation eines oder mehrerer der drei vollwertigen Desktop-Systeme ist hierüber möglich. Mythbuntu erfordert einen wesentlich schnelleren Prozessor als andere Ubuntu-Versionen – die Website nennt 1 GHz minimal und empfiehlt je nach Anwendung 2 oder sogar 3 GHz. Eine Hauptursache hierfür ist, dass MythTV für die (nicht abschaltbare) Unterstützung von zeitversetztem Fernsehen das Programm permanent aufzeichnet, was je nach TV-Karte hohe Systemanforderungen stellen kann.\n\nMythbuntu entstand mit der Entwicklung von Ubuntu 7.10 und ist seit 1. Februar 2015 in der aktuellen Version 14.04.02 verfügbar. Als von Canonical als Community-unterstütztes Projekt anerkannt, sind die Pakete Bestandteil der Ubuntu-Paketquellen. Kommerzieller Kundendienst und verlängerte Unterstützung für LTS-Versionen sind aber nicht vorhanden.\n\nDie Variante Ubuntu GNOME (vormals: Ubuntu Gnome Remix) verwendete statt Unity die Gnome Shell sowie einige Gnome-Applikationen, die in Ubuntu standardmäßig nicht installiert waren. Ab Version 13.04 bis Version 18.04 war Ubuntu GNOME eine von Canonical offiziell anerkannte Variante. Die Version Ubuntu GNOME 14.04 ist eine LTS-Version mit einem Support-Zeitraum von 3 Jahren. Seit Version 18.04 LTS ist Ubuntu GNOME kein eigener Spin mehr sein, sondern Ubuntu nutzt Gnome wieder als Standard-Desktop, dessen Entwickler gemeinsam mit dem Team von Canonical an Ubuntu arbeiten.\n\n\"Ubuntu Kylin\" ist eine weitere Variante, die ab Version 13.04 offiziell von Canonical anerkannt ist. Sie soll laut eigenen Angaben besser an die Bedürfnisse chinesischer Benutzer angepasst sein als Ubuntu selbst. Einige Bestandteile von \"Ubuntu Kylin\" basieren auf Kylin. Ubuntu Kylin ist seit März 2013 eine offizielle Ubuntu-Variante, die von Canonical gepflegt wird.\n\nIn einer Behördenmitteilung kündigte die chinesische Regierung am 20. Mai 2014 an, dass sie 200 Millionen Rechner, die noch mit Windows XP arbeiteten, auf Ubuntu Kylin umrüsten wolle. Das Ziel dieser großflächigen Migration sei es, die Abhängigkeit von ausländischen Herstellern von Betriebssystemen zu verringern. Aus Sicherheitsgründen wurde eine Migration zu Windows 8 verboten.\n\nDie Variante Ubuntu MATE verwendet die Desktop-Umgebung MATE an Stelle von Unity. Ab Version 15.04 ist Ubuntu MATE eine von Canonical offiziell anerkannte Variante. Das enthaltene Tool \"MATE Tweak\" bietet vorkonfigurierte Panel/Dock-Varianten mit unterschiedlichen Startmenüs zur Auswahl an. Systemanwendungen wie Dateimanager, Texteditor und Dokumentenbetrachter stehen als eigene, von GNOME abgeleitete, Anwendungen zur Verfügung, deren Namen in MATE hauptsächlich mit spanischen Begriffen wie \"caja\", \"pluma\" und \"atril\" belegt sind.\n\nDiese Variante verwendet \"Budgie\" als Desktop-Umgebung. Budgie basiert auf GTK+ (> 3.x) und nutzt verschiedene Gnome-Anwendungen. Es wurde von den Entwicklern des Solus Projects entwickelt. Ubuntu Budgie begann als Budgie Remix und ist ab Version 17.04 offiziell von Canonical anerkannt. Eine Besonderheit stellt das Slide-Menu \"Raven\" dar, das sich als standardmäßig rechts öffnen lässt und das Notifications sowie Applets wie z. B. einen Kalender oder Einstellungen für die Tonausgabe beherbergt.\n\nEinige Varianten sind inzwischen nur noch Erweiterungen für ein bestehendes Ubuntu-System beziehungsweise in dieses integriert.\n\nBereits ab Version 7.10 ist unter wechselnden Namen von Ubuntu-Varianten die Rede, die speziell auf eher spartanisch ausgestattete Rechner, also beispielsweise Ultra-Mobile PCs oder Netbooks ausgelegt sind.\n\nEine erste, zusammen mit Intel entwickelte Version hiervon erschien mit Ubuntu 8.04 unter dem Namen „Ubuntu MID-Edition“; teilweise auch schlicht „Ubuntu Mobile“. Die abgespeckte Variante der Linux-Distribution enthält neben Software wie einem Webbrowser verschiedene Multimedia-Applikationen, die teilweise in der normalen Desktop-Version nicht enthalten sind. Die Oberfläche wurde auf kleine Displays von 4 bis 8 Zoll Größe angepasst. Diese Version kommt unter anderem bei dem Netbook Dell Inspiron Mini 9 und dem Toshiba NB100 zum Einsatz.\nDas mit Ubuntu 9.04 erschienene „Ubuntu Netbook Remix“ ist eine Version, die für Netbooks mit Displaygrößen bis 10 Zoll entwickelt wurde und von einem USB-Live-System aus installiert werden kann.\n\nMit dem Netbook Remix gibt es einen alternativen Desktop, der alle wichtigen Programme auf einen Blick darstellt. (siehe Bild)\n\nIn Ubuntu 10.04 LTS wurde der Netbook Remix in „Ubuntu Netbook Edition“ umbenannt und Ubuntu 10.10 erschien erstmals mit der eigens entwickelten Oberfläche Unity.\n\nAb Ubuntu 11.04 gibt es keine Netbook Edition mehr, da die Desktop-Version nun Unity als Standard-Shell benutzt.\n\n\"Gobuntu\" war eine Ubuntu-Variante ohne proprietäre Inhalte. Diese enthielt fast nur Programme, Treiber und Dateien, die unter einer freien Lizenz veröffentlicht wurden, Ausnahme waren beispielsweise Logos und Icons des Webbrowsers Firefox. Gobuntu gab es einzig in den Versionen 7.10 und 8.04 (inklusive Update-Release 8.04.1).\n\nMit diesem Projekt versuchte Canonical dem Vorwurf zu begegnen, dass Ubuntu nicht mehr freie Software sei, weil es viele proprietäre Treiber und Programme nutzt oder zumindest anbietet. Die Entwickler sollten mit Gobuntu die Grenzen von freiem Code und sonstigen freien Werken feststellen und falls nötig auch an deren Ausweitung arbeiten. Gemäß Chefentwickler Mark Shuttleworth funktionierte Gobuntu noch mit viel Hardware nicht einwandfrei, existierte aber als ein Zeichen für die Notwendigkeit freier Software und als Beispiel dafür, was heute schon möglich ist.\n\nErste Pläne für ein derartiges Projekt gab es schon im November 2005, damals unter dem Namen \"Gnubuntu\". Dies wurde nach Protesten von Richard Stallman zunächst in \"Ubuntu-libre\" geändert. Die Entwicklung von Gobuntu geschah danach in Zusammenarbeit mit den Entwicklern von gNewSense, das von Stallman gebilligt wurde, mit dem Ziel, zukünftig dessen Basis zu werden. Ab Version 3 basiert gNewSense nicht mehr auf Ubuntu, sondern auf Debian.\n\nWeitere Gobuntu-Versionen nach 8.04.1 wurden nicht mehr veröffentlicht. Als Grund wurde unter anderem Inaktivität der Gobuntu-Community geltend gemacht. Auch ist es seit Ubuntu 8.04 möglich, bei der Installation nur freie Software auszuwählen. Solange man keine Pakete aus \"restricted\" oder \"multiverse\" installiert, hat man ein rein freies System, wie es von Gobuntu beabsichtigt war. Auch hat Shuttleworth darauf hingewiesen, dass er lieber auf gNewSense fokussieren will, weil die Arbeit am gNewSense-Projekt auch der Ubuntu-Community hilft.\n\nInoffizielle Ubuntu-Derivate werden im Gegensatz zu den offiziellen Abwandlungen nicht von Canonical Ltd. veröffentlicht, sondern von externen Softwareentwicklern oder Entwicklergruppen.\n\nSeit Ubuntu 10.04 sind die meisten Programme für den Zugriff auf Mobilgeräte automatisch installiert. Das Live-System unterstützt auch iPhones.\n\nUbuntu TV ist eine Abwandlung von Ubuntu und auf Smart-TV abgestimmt. Es wurde auf der Consumer Electronics Show 2012 von Canonical vorgestellt. Es arbeitet wie Ubuntu mit Unity als Standardoberfläche und ist für Hersteller von Fernsehgeräten kostenfrei nutzbar. Die Entwicklung wurde eingestellt.\n\nUbuntu Touch ist die Mobilversion von Ubuntu für Smartphones. Sie ist an die Version für Desktop und Ubuntu TV angelehnt und soll alle Möglichkeiten und Tools der Desktop-Variante unterstützen: Mit dem Konzept der \"Convergence\" soll ein an einen Bildschirm angeschlossenes Smartphone die Funktionen eines Desktop-Rechners bieten. Die Installation von Applikationen erfolgt über das Ubuntu Software Center. Für die Anwendungsentwicklung stehen dieselben Werkzeuge wie für die Desktop-Variante zur Verfügung. Dies erleichtert es, eine Applikation zeitgleich für mehrere Ubuntu-Varianten zu entwickeln. Im Februar 2013 wurde eine Anleitung veröffentlicht, wie man eine Vorabversion von Ubuntu Touch auf den Android-Geräten Nexus 4, Nexus 7, Nexus 10 und dem Galaxy Nexus testen konnte. Anfang 2016 wurden vier Smartphones mit vorinstalliertem Ubuntu-Touch angeboten: das BQ Aquaris E5 HD, das BQ Aquaris E4.5, das Meizu MX4 Ubuntu Edition sowie das Meizu Pro 5 Ubuntu Edition.\n\nIm Januar 2017 wurde bekanntgegeben, dass Canonical die Entwicklung von Ubuntu Touch einstellt, ab Ende Juni 2017 keine Updates mehr ausliefert und zum Ende des Jahres 2017 den App-Store schließt. Im Juni 2017 waren die meisten mit Ubuntu Touch vorinstallierten Smartphone-Modelle ausverkauft.\n\nDie weitere Entwicklung von Ubuntu Touch hat die UBports-Community übernommen.\n\n\"Ubuntu für Android\" ist eine Ubuntu-Version, die darauf ausgelegt ist, auf Android-Smartphones zu laufen. So kann Ubuntu simultan mit Android laufen, da beide auf dem Linux-Kernel basieren. Ebenfalls soll es möglich sein, das Mobiltelefon mit einem Monitor zu verbinden, um ein voll funktionsfähiges Ubuntu Desktop System zu erhalten. Seit April 2014 wird das System nicht mehr aktiv weiterentwickelt.\n\n„Ubuntu on Tablets“ ist an die Version für Smartphones angelehnt, es wird eine ähnliche Bedienung und Menüführung besitzen. Als Besonderheit wird es möglich sein, ein Ubuntu-Smartphone an das Tablet anzudocken und geöffnete Apps auf dem Tablet weiterlaufen zu lassen. Da das Tablet auf der Desktopversion beruht, wird es möglich sein, mit angeschlossener Maus und Tastatur das Tablet wie einen normalen PC zu nutzen.\n\nCanonical kündigte an, an der Entwicklung eines eigenen Smartphones unter dem Namen \"Ubuntu Edge\" zu arbeiten, das ebenfalls mit dem Betriebssystem Ubuntu ausgestattet wird und im Frühjahr 2014 erscheinen sollte. Im August 2013 wurde bekannt gegeben, dass durch das Crowdfunding zur Finanzierung des Smartphones zwar die höchste jemals erreichte Summe zur Verfügung gestellt wurde, trotzdem nur annähernd 40 Prozent der 32 Millionen US-Dollar eingenommen werden konnten, die zur Realisierung benötigt worden wären. Deshalb wurde das Projekt gestoppt und die Anleger bekamen ihr Geld zurück.\n\nUbuntu ist mit vollständiger deutscher Benutzeroberfläche verfügbar, auch als Live-System.\n\nDie 2012 eingestellte deutsche Fluggesellschaft Contact Air nutzte Ubuntu auf ihren 120 Laptops für Piloten.\n\nAm 12. November 2005 wurde der Verein „Ubuntu Deutschland“ in Nürnberg gegründet, um so die Ubuntu Foundation in Deutschland zu unterstützen. Der Verein unterstützt als juristische Person und Spendenempfänger die verschiedenen Aktivitäten der deutschen Community. Am 20. und 21. Oktober 2007 richtete er eine Benutzerkonferenz unter dem Namen \"UbuCon\" aus, die in der Hochschule Niederrhein in Krefeld stattfand und kostenfrei besucht werden konnte, in der Folge fanden jährlich weitere Ubucons statt; die Ubucon im November 2016 richtet sich als Ubucon Europe erstmals an ein gesamteuropäisches Publikum:\n\n\nDer SPIEGEL schrieb 2005 über die Betreuung der Nutzer: „Es herrscht ein sehr freundlicher Umgangston, der sich deutlich von dem abhebt, was mitunter in bestimmten Linux-Foren üblich war, …“\n\nUbuntu wird von manchen Anhängern des Debian-Projekts kritisiert. Moniert wird, dass Ubuntu technisch nicht vollständig kompatibel zu Debian sei. Eine der unter anderem von Ian Murdock erhobenen Forderungen ist es, weiterhin die jeweils aktuelle Debian-Version als Basis zu nutzen. Auf der Debian-Entwicklerkonferenz \"debconf 6\" im Mai 2006 gab es diesbezüglich eine Aussprache zwischen einigen führenden Debian-Entwicklern und Mark Shuttleworth. Außerdem sind einige Debian-Entwickler mit der Qualität der von Ubuntu zurückgegebenen Patches unzufrieden. Diese enthielten zu viele Ubuntu-spezifische Änderungen, die für Debian nutzlos seien. Außerdem wird die Idee als solche, Pakete aus Debian zu übernehmen, oftmals kritisiert, da hierdurch Debian selbst geschwächt werde.\n\n2006 kritisierte ein Mozilla-Entwickler das Konzept von Ubuntu, nicht als Plattform für Anwendungsprogramme zu agieren, sondern diese vor allem integriert in das Betriebssystem anzubieten. Diese Verwischung der Grenzen zwischen Anwendungen und Betriebssystem durch die integrierte Verwaltung mit der Distribution wird auch als Ursache einiger Desktop-User-Experience-Probleme beschrieben, z. B. die Schwierigkeit der parallelen Installation von mehreren Anwendungsprogrammversionen. Auch der eigentlich banale Fall, dass ein Nutzer lediglich ein Anwenderprogramm aktualisieren möchte, ist häufig leichter durch die Installation einer neueren Ubuntu-Version zu lösen. 2010 wurde dieses Verhalten als Problem in Ubuntus Launchpad-Bugtracker akzeptiert, da auch die existierenden PPA- und Backport-Ansätze dieses Problem nicht vollständig lösen.\n\nUbuntu wurde 2008 von dem Kernel-Entwickler Greg Kroah-Hartman, der damals für den Konkurrenten Novell arbeitete, eine zu geringe Beteiligung an der Entwicklung des Linux-Kernels vorgeworfen. Seine Kritik begründete er mit der geringen Zahl der in den Kernel zurückgegebenen Patches, die in diesem Zeitraum weniger als 1 % betrug. Ubuntu-Entwickler Matt Zimmerman antwortete auf diesen Vorwurf, dass Kroah-Hartmanns Betrachtung des Linux-Ökosystems „seltsam“ sei, da er sämtliche Anwendungen wie Gnome, KDE und jegliche Server-Dienste dabei auslasse. Greg Kroah-Hartman untersuchte ein größeres Linux-Ökosystem mit den Kernkomponenten GCC, ALSA, X.Org-Server und fand erneut heraus, dass die Beiträge von Canonical auch hier gering (< 1 %) oder nicht vorhanden waren.\n\nIm Juli 2010 wurde eine Beitragsstatistik für Gnome veröffentlicht. Canonicals Anteil an der Entwicklung betrug 1 %. Lediglich die Programmsymbole und ein Taschenrechnerprogramm wurden von Canonical-Mitarbeitern betreut. Jono Bacon entgegnete, dass Canonicals Programme innerhalb von Launchpad entwickelt und vom Gnome-Projekt abgelehnt wurden. Mark Shuttleworth äußerte, dass diese Form von Tribalismus kontraproduktiv sei: Auch Beiträge jenseits der Arbeit an Quelltexten wie z. B. der Ubuntu-Verhaltenskodex würden von vielen Open-Source-Projekten adaptiert.\n\nMangelnden Datenschutz kritisierte beispielsweise die Electronic Frontier Foundation wegen der Einbindung des Onlineshops von Amazon im Herbst 2012. Der Vorwurf, Adware zu beinhalten, kam auf, weil die vorher nur auf installierte Programme und lokale Dateien gerichtete Suchfunktion des Desktops auf den Shop von Amazon erweitert wurde, um passende Angebote auszugeben. Das abzuschalten wurde bald ermöglicht und die Provisionen von Amazon als hilfreich zur Entwicklung freier Software verteidigt, doch Richard Stallman bezeichnete Ubuntu als Spyware. Im Oktober 2013 bekam Shuttleworth für die inzwischen ausgebaute Erweiterung einen Big Brother Award in Österreich. Im November 2013 bekam Micah Lee wegen seiner Website zum Abschalten der Erweiterung eine Abmahnung, die zunächst bekräftigt und dann zurückgezogen wurde. Im August 2014 antwortete das britische Information Commissioner's Office auf eine Beschwerde, wegen Hinweisen auf Amazon und die Abschaltbarkeit der Erweiterung verstoße sie nicht gegen europäisches Recht.\n\nMit Ubuntu 16.04 wurde das Dateisystem ZFS in die Distribution integriert. Die für ZFS verwendete Lizenz \"Common Development and Distribution License\" (CDDL) wird von der Free Software Foundation allerdings als nicht mit der GPL vereinbar bezeichnet. Die Organisation Software Freedom Conservancy spricht in diesem Zusammenhang von einer Lizenzverletzung.\n\nIn der Ausgabe vom Juli 2006 wurde Ubuntu von der Zeitschrift PC World mit dem \"PC World 2006 World Class Award\" ausgezeichnet und somit von dieser als eines der 100 besten Produkte des Jahres bezeichnet. Ebenfalls im Juli 2006 hat IT Reviews Ubuntu 6.06 LTS mit ihrem \"„Recommended“ award\" ausgezeichnet.\n\n\n"}
