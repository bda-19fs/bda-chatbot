{"id": "2303820", "url": "https://de.wikipedia.org/wiki?curid=2303820", "title": "Microsoft Windows 1.0", "text": "Microsoft Windows 1.0\n\nMicrosoft Windows 1.0 ist eine vom Unternehmen Microsoft entwickelte grafische Benutzeroberfläche. Sie sollte den Umgang mit dem MS-DOS-Betriebssystem erleichtern sowie das Erscheinungsbild von Anwendungsprogrammen vereinheitlichen und die Nutzung von Peripheriegeräten wie Druckern und Bildschirmen für die Benutzer vereinfachen. Für das am 10. November 1983 von Bill Gates in New York offiziell angekündigte Produkt, an dem 24 Softwareentwickler drei Jahre lang gearbeitet hatten, wurde nach langer Verzögerung am 20. November 1985 die Auslieferung der Einzelhandelsversion bekanntgegeben. Am darauffolgenden Tag wurde das Produkt auf einer Pressekonferenz offiziell vorgestellt.\n\nMicrosoft Windows 1.0 fand kaum Anklang bei den Benutzern, da für einen sinnvollen Einsatz teure Hardwarekomponenten wie Maus, Festplatte, Erweiterungsspeicher und Farbgrafikkarte benötigt wurden. Außerdem gab es nur wenige Anwendungen, die speziell auf die Leistungsmerkmale von \"Windows\" zugeschnitten waren. Es enthielt jedoch bereits viele Merkmale der ab 1990 erfolgreichen Versionen Microsoft Windows 3.0 und Microsoft Windows 3.1.\n\nBis Mitte der 1980er Jahre die ersten grafischen Benutzeroberflächen für Personal Computer erschienen (der Apple Macintosh erschien 1984, andere 1985 oder wenig später), wurden Computer fast ausschließlich mit Kommandobefehlen bedient, deren Befehlssyntax eingehalten werden musste und eine Einarbeitung erforderte. Intuitives Arbeiten war dadurch nicht möglich.\n\nDie meisten grafischen Benutzeroberflächen orientieren sich mehr oder weniger stark am Xerox Alto, dem ersten Computer mit einer Oberfläche dieser Art. Mit diesen Oberflächen kann der Benutzer per Mausklick Dateien verwalten, Programme starten und Dateien anlegen, ohne sich dafür eine exakte Zeichenfolge merken zu müssen. Daten, Anwendungen, Ressourcen und Teile des Computersystems, z. B. die Systemsteuerung, werden als Objekte (in Form visueller Symbole oder als anklickbare Listeneinträge) dargestellt und die Arbeit mit ihnen geschieht durch Kontext- oder Auswahlmenüs, durch Schaltflächen und andere grafische Elemente oder durch direktes Anklicken eines Symbols mit der Maus.\n\nIm September 1981, einen Monat nach der Auslieferung von PC-DOS 1.0 durch IBM, begannen die Arbeiten am Projekt \"Interface Manager\", der eine Zwischenschicht zwischen MS-DOS und den Anwendungsprogrammen bilden und den Umgang mit den am Computer angeschlossenen Druckern und dem Monitor erleichtern sollte.\nDer \"Interface Manager\" sollte hardwareunabhängig sein, im Grafikmodus arbeiten, Unterstützung für WYSIWYG bieten und das Erscheinungsbild der Anwendungsprogramme vereinheitlichen. Das Manager-Magazin schrieb 2005 in einem geschichtlichen Rückblick, Oberfläche und System sollten zu einem Betriebssystem verwoben werden, um sicherzustellen, dass alle Nutzer vom System abhängig waren, die sich für die Oberfläche entschieden. Ursprünglich war wie bei dem im August 1982 erschienenen Microsoft Multiplan eine alphabetisch geordnete Befehlsliste am unteren Bildschirmrand geplant. Unter dem Eindruck der grafischen Benutzeroberfläche des Xerox Stars und Apples Lisa fiel 1982 die Entscheidung, Pull-Down-Menüs und Dialogboxen zu verwenden und die Anzeige von mehreren Dokumenten in verschiedenen Fenstern zu ermöglichen. Als Eingabegerät sollte eine Maus genutzt werden.\n\nNachdem der Softwarehersteller Visicorp im Herbst 1982 auf der COMDEX sein Produkt \"Visi On\" vorgestellt hatte, das nach der Einschätzung von Charles Simonyi dem von Microsoft geplanten Produkt äußerlich sehr ähnelte, machte Bill Gates im Januar 1983 erste Andeutungen über die Entwicklung des \"Interface Managers\". Im Mai 1983 schlug der für das Produkt verantwortliche Manager Jeff Raikes die Bezeichnung \"Microsoft Desktop\" vor, konnte sich damit aber nicht durchsetzen. Da die Entwickler immer von Fenstern sprachen, dachten sich die PR-Manager den einfacheren Namen „Windows“ aus.\n\nNach Beginn der Auslieferung von \"Visi On\" im Oktober 1983 und der Ankündigung von IBM, mit \"TopView\" ebenfalls etwas dem Interface-Manager Entsprechendes entwickeln zu wollen, sah sich Microsoft gezwungen, mit seiner eigenen Produktentwicklung an die Öffentlichkeit zu gehen. Am 10. November 1983 verkündete Bill Gates in New York die Entwicklung einer grafischen Benutzeroberfläche für MS-DOS, die bis Ende 1984 auf mehr als 90 Prozent aller MS-DOS-Rechner im Einsatz sein würde. Der Auslieferungstermin von \"Windows 1.0\" musste jedoch mehrere Male verschoben werden, was dem Produkt den Ruf einer sogenannten Vaporware einbrachte und worunter das Ansehen von Microsoft litt. Die Entwickler hatten Probleme mit der Leistungsfähigkeit des Intel-8088-Prozessors. \"Windows\" benötigte zu viel Speicherplatz und war zu langsam. Mit der multitaskingfähigen Fensterumgebung \"DESQview\" (Juli 1985; Vorgänger DESQ, bereits Mai 1984, war praktisch bedeutungslos) von Quarterdeck, \"Top View\" (Januar 1985) von IBM und der graphischen Benutzeroberfläche \"GEM\" (Februar 1985) von Digital Research erschienen nach \"Visi On\" unterdessen weitere Konkurrenzprodukte. Nach der Umstrukturierung von Microsoft im August 1984 erhielt Neil Konzen die Verantwortung für die Benutzeroberfläche. Er definierte zahlreiche interne Routinen von \"Windows\" neu, um die Anpassung von Macintosh-Anwendungen an \"Windows\" zu erleichtern.\nUm die Leistungsfähigkeit von \"Windows\" zu demonstrieren, wurden nach dem Vorbild der Apple-Programme die Programme Write\" und Paint\" entwickelt. Unter der Leitung von Tandy Trower entstanden weitere Hilfsmittel wie der Kalender, der Taschenrechner, der Kartenmanager, die Uhr und das Spiel Reversi. Der Öffentlichkeit präsentiert wurde \"Windows\" auf der COMDEX im Mai 1985. \"Windows\" ließ sich jetzt sowohl mit einer Maus als auch mit der Tastatur bedienen und es unterstützte die von IBMs \"TopView\" eingeführten PIF-Dateien. Am 28. Juni 1985 lieferte Microsoft eine erste Testversion für Softwareentwickler und Computerhersteller aus.\n\nDie endgültig freigegebene Version 1.01 wurde am 21. November 1985 auf einer Pressekonferenz vorgestellt. Die Auslieferung der Einzelhandelsversion war bereits einen Tag zuvor bekannt gegeben worden. 85 Prozent des Quelltextes waren in der Programmiersprache C geschrieben. Nur kritische Programmteile waren in Assemblersprache implementiert. 24 Programmierer hatten 110.000 Stunden benötigt, um die erste \"Windows\"-Version zu entwickeln. Den Programmierern standen außerdem Test- und Dokumentationsteams zur Seite. Während der dreijährigen Entwicklungszeit gab es insgesamt vier Produktmanager und drei Entwicklungsleiter. Im Mai 1986 folgte die in die wichtigsten europäischen Sprachen, darunter Deutsch, lokalisierte Version 1.02. Die Versionen 1.03 (August 1986) und 1.04 (April 1987) enthielten lediglich kleinere Verbesserungen, Fehlerbereinigungen und aktualisierte Gerätetreiber. Windows wurde in den Preislisten der Händler unter den Hilfsprogrammen aufgeführt und kostete im Dezember 1986 etwa 340 DM.\n\nNach dem Ausführen der Datei WIN.COM auf der DOS-Kommandozeile startet \"Windows\" mit dem Desktop und dem Programmfenster \"MS-DOS\", einem in etwa mit dem späteren Arbeitsplatz vergleichbaren Dateimanager. Es erlaubt den Benutzern beispielsweise, ohne Kenntnis der genauen Syntax der MS-DOS-Befehle Dateien zu kopieren, Verzeichnisse anzulegen oder DOS- und Windows-Programme zu starten. Aktive Programme wurden am unteren Bildschirmrand als Symbole dargestellt. Microsoft nannte sie in dieser Version Sinnbilder. Mit Hilfe der \"Systemsteuerung\" war es möglich, bestimmte Eigenschaften von \"Windows\" an die Vorlieben eines Benutzers anzupassen. So konnten die verwendeten Farben oder die Blinkgeschwindigkeit des Cursors eingestellt werden. Dank des mitgelieferten Druckerspoolers erfolgte der Ausdruck von Dateien während der Arbeit mit \"Windows\" im Hintergrund. Die \"Zwischenablage\" erlaubte den Austausch von Daten zwischen Windowsprogrammen. Mit dem Systemprogramm \"Terminal\" war es möglich, mittels eines Nullmodem-Kabels mit anderen Computern zu kommunizieren.\n\nDa es kaum Windowsprogramme gab, mussten die Benutzer auf ihre gewohnten DOS-Programme zurückgreifen. Diese können zwar ebenfalls unter Windows verwendet werden, jedoch ohne die Vorteile der grafischen Benutzeroberfläche mit ihrer zentralen Grafik- und Treiberverwaltung. Mit dem \"PIF-Editor\" können diese Anwendungen für die Arbeit unter \"Windows\" konfiguriert werden, wie etwa die Verwendung von Speicher oder Peripheriegeräten (siehe nachstehende Abbildung).\n\nDOS-Anwendungen, die ausschließlich DOS-Systemaufrufe verwenden (Microsoft nennt sie „kooperative Programme“), können in einer „DOS-Box“ gestartet werden, einem Fenster unter \"Windows\". Damit steigt der Bedarf an Arbeitsspeicher, da die DOS-Programme zusätzlich zu Windows in den Speicher geladen werden.\n\nDer Leistungsumfang von \"MS-Write\" entsprach in etwa dem von Microsoft für den Macintosh entwickelten \"Microsoft Word\" und führte erstmals auf PCs das mit der Dateierweiterung .DOC gekennzeichnete Dateiformat von Word ein. \"MS-Paint\" war vergleichbaren Anwendungen ebenbürtig und erlaubte wie diese nur Schwarz-weiß-Zeichnungen. \"Windows\" enthielt außerdem das Computerspiel \"Reversi\" und verschiedene Hilfsprogramme, die Dinge repräsentierten, die typischerweise auf einem Schreibtisch zu finden sind: einen Taschenrechner, einen Terminkalender, einen Notizblock, eine Karteikartenverwaltung sowie eine Uhr.\n\nWeiterhin befanden sich im Lieferumfang von \"Windows\" Gerätetreiber für insgesamt 19 Druckermodelle der Firmen Epson, IBM, Oki, NEC, C. Itoh, Star, Toshiba, TI und HP. \"Windows\" unterstützte Speichererweiterungskarten nach dem LIM-EMS-Standard, mit denen der unter DOS auf 640 kByte beschränkte Arbeitsspeicher auf mehrere Megabyte ausgebaut werden konnte. Bestandteil der Installation waren PIF-Dateien mit den Standardkonfigurationen für die damals gängigsten DOS-Anwendungen und ein Editor für die Bearbeitung dieser Konfigurationsdateien.\n\nDer Inhalt der Installationsdisketten war, im Gegensatz zu vielen anderen Betriebssystemen und späteren Windowsversionen, nach Funktion kategorisiert. Die sechs 5,25-Zoll-Disketten heißen WIN1, WIN2, ANWENDGN, FONTS, HILFSPRG und WRITEPRG (Version 1.03). Alle Installationsdateien sind unkomprimiert.\n\nDie Mindestanforderungen für die Installation des auf fünf 5,25-Zoll-Disketten ausgelieferten \"Windows 1.01\" waren ein mit einer 8088-CPU von Intel ausgerüsteter Computer, der über 256 KByte Arbeitsspeicher und zwei Laufwerke verfügte, von denen eines ein Diskettenlaufwerk sein musste, sowie das Betriebssystem MS-DOS 2.x oder höher. Verfügbar war zum Zeitpunkt der Markteinführung bereits die netzwerkfähige MS-DOS-Version 3.1, die seit November 1984 ausgeliefert wurde. \"Windows\" unterstützt die Grafikkarten Hercules sowie die CGA- und EGA-Standards.\n\nWindows unterstützt in diesen Versionen nicht alle genannten Spezifikationen vollständig. Beispielsweise wird MS-DOS erst in einer späteren Version voll unterstützt (Version 3.2) und der CGA-Standard wird nur Schwarz-Weiß unterstützt.\n\nAls \"Windows\" Ende 1985 erschien, gab es auf dem Markt vier Produkte für IBM-kompatible PC mit einem vergleichbaren Funktionsumfang:\n\nEine Umfrage des Magazins \"InfoWorld\" im Dezember 1985 ergab, dass die in den Unternehmen für die Datenverarbeitung Verantwortlichen \"Windows\" den Produkten \"GEM\" oder \"TopView\" vorziehen würden, aber bessere Integrationsmöglichkeiten abwarten wollten. Im Alltagsbetrieb erwies sich \"Windows\" als zu langsam. Die für eine sinnvolle Nutzung notwendigen Hardwarekomponenten Festplatte, Erweiterungsspeicherkarten, Maus, Farbgrafikkarte waren zum damaligen Zeitpunkt noch sehr teuer und daher wenig verbreitet. Nur wenige Hersteller IBM-kompatibler PCs lieferten \"Windows\" mit ihren Produkten aus. Es gab so gut wie keine speziell an die Fähigkeiten von \"Windows\" angepassten Anwendungen. Die erste nicht von Microsoft stammende Windowsanwendung wurde von Paul Grayson von Micrografx entwickelt und war das Zeichenprogramm \"In-A-Vision\". Das für die Entwicklung von Windows-Anwendungen notwendige Software Development Kit bestand aus sieben Disketten. Es enthielt eine spezielle Fassung des Microsoft C-Compilers, weitere Hilfsanwendungen und eine etwa 1000 Seiten umfassende Dokumentation des Windows-APIs und war vielen Programmierern zu kompliziert. Erst im Dezember 1986 erschien mit der DTP-Software \"PageMaker\" von Aldus die erste größere Anwendung für \"Windows\". Auf Grund der geringen Verbreitung von \"Windows\" wurde \"PageMaker\" mit einer speziellen Laufzeitumgebung von \"Windows\" verkauft. Ende Oktober 1987 veröffentlichte Microsoft mit \"Excel 2.0\" das erste eigene speziell an \"Windows\" angepasste Anwendungsprogramm. Es erschien zeitgleich mit \"Windows 2.0\" und setzte einen Computer mit Intel-80286-Prozessor voraus.\n\nDer Journalist Jim Seymour beurteilte \"Windows\" kurz nach dessen Erscheinen in der Fachzeitschrift \"PC Week\" folgendermaßen:\n\n"}
{"id": "2303854", "url": "https://de.wikipedia.org/wiki?curid=2303854", "title": "Pet Alien", "text": "Pet Alien\n\nPet Alien (Untertitel: \"EinFall aus dem All\") ist eine US-amerikanische computeranimierte Kinderserie. In der Serie geht es um den Jungen Tommy Cadle, welcher in einer Küstenstadt namens DeSprayBay an der Küste in einem Leuchtturm wohnt.\n\nEines Nachts landet beim 13-jährigen Tommy ein Raumschiff mit Aliens. Diese ziehen bei ihm ein und stellen bald gemeinsam mit Tommy einigen Unsinn an. Häufig entstehen Missverständnisse, wenn die Aliens das Verhalten der Menschen falsch interpretieren.\n\n\nDie Serie wurde 2005 unter Regisseur Andrew Young von Mike Young Productions produziert. Drehbuchautor war unter anderen Dan Danko. Taffy Entertainment vertrieb die Serie weltweit.\n\nDie Erstausstrahlung der ersten Staffel erfolgte vom 23. Januar 2005 bis zum 26. Mai 2005 in den USA bei Cartoon Network. Später folgten bislang drei weitere Staffeln. Die deutsche Fassung wird seit 10. Februar 2006 beim KI.KA gezeigt. Die Serie wurde unter anderem auch in Großbritannien, Portugal, Italien und der Ukraine im Fernsehen gezeigt.\n\nEin auf der Serie basierendes Puzzelvideospiel erschien im Juli 2007 für Nintendo DS in den USA. Es wurde von Shin'en Multimedia entwickelt und veröffentlicht von The Game Factory.\n\n2006 wurde \"Pet Alien\" für den Daytime Emmy Award in der Kategorie für außergewöhnliche animierte Sendungen nominiert.\n\n"}
{"id": "2305851", "url": "https://de.wikipedia.org/wiki?curid=2305851", "title": "Agentenbasierte Modellierung", "text": "Agentenbasierte Modellierung\n\nAgentenbasierte Modellierung ist eine spezielle, individuen-basierte Methode der computergestützten Modellbildung und Simulation, eng verknüpft mit komplexen Systemen, Multiagenten Systemen, evolutionärer Programmierung und zellulären Automaten.\n\nAgentenbasierte Modellierung hat ihre Wurzeln sowohl in der Modellierung zellulärer Automaten, als auch in den diversen Bereichen künstlicher Intelligenz. Bei vergleichender Betrachtung kann die agentenbasierte Modellierung auch als Erweiterung von zellulären Automaten interpretiert werden. Sie ist ein spezieller Fall einer Mikrosimulation. Agentenbasierte Modelle basieren auf der Theorie von Multiagenten-Systemen.\n\nIm Gegensatz zu anderen Arten der Modellierung (zum Beispiel System Dynamics) haben in der agentenbasierten Modellierung viele kleine Einheiten (Agenten) Entscheidungs- oder Handlungsmöglichkeiten. Das System-Verhalten resultiert aus dem Verhalten der einzelnen Agenten und wird nicht auf Systemebene vorgegeben. Wenn es dabei zu Effekten auf der Systemebene kommt, die nicht unmittelbar aus den Entscheidungsalgorithmen der Individuen ableitbar sind, spricht man von Emergenz. Zusätzlich kann ein von den individuellen Entscheidungen getrenntes Systemverhalten implementiert werden.\n\nZwei entscheidende Aspekte der agentenbasierten Modellierung sind die Möglichkeiten heterogenes Verhalten und Abhängigkeiten von anderen Individuen explizit abbilden zu können.\n\nDiese Art der Modellierung kommt vor allem dann zur Anwendung, wenn der Fokus einer Fragestellung nicht die Stabilität eines Gleichgewichts bzw. die Annahme, dass ein Prozess in ein Gleichgewicht zurückkehrt, ist, sondern die Frage, wie sich ein System veränderten Rahmenbedingungen anpassen kann (Robustheit). Dabei wird der Erkenntnis Rechnung getragen, dass komplexe Probleme es erfordern, die Mikro-Ebene, also die Entscheidungen der Individuen, ihre Heterogenität und ihre Interaktionen, direkt zu untersuchen.\n\nSehr unterschiedliche Anwendungen fallen in den Bereich der agentenbasierten Modellierung. Sie unterscheiden sich zum Beispiel im Grad der modellierten Intelligenz der Agenten und in der Modellierung von physischem oder sozialem Raum. Allen diesen Ansätzen ist gemein, dass das Entscheidungsverhalten auf der Ebene der Individuen implementiert wird.\n\nEinige Beispiele verdeutlichen diese Bandbreite.\n\nIn einer Dimension bewegen sich Fahrzeuge (die Agenten). Die Fahrer bzw. Fahrzeuge haben ein bestimmtes Beschleunigungs- und Bremsverhalten und halten einen Mindestabstand zu dem vor ihnen fahrenden Auto ein. Die Komplexität der simulierten Umwelt ist so gering und die nötige künstliche Intelligenz der Agenten so begrenzt, dass in diesem Fall auch von einer Mikrosimulation gesprochen werden kann. Trotzdem lassen sich mit diesen Modellen interessante Aussagen treffen.\n\nEin Verkehrsmodell mit diskret modelliertem Raum (die Fahrzeuge bewegen sich auf Gitterzellen) ist das Nagel-Schreckenberg-Modell, ein Beispiel für ein Fahrzeugfolgemodell mit kontinuierlichem Raum ist das Wiedemann-Modell.\n\nÄhnlich einfache Intelligenz genügt simulierten Ameisen, die auf der Futtersuche Duftstoffe absondern und den Duftstoffen anderer Ameisen folgen. Die Duftstoffe verlieren sich mit der Zeit. Die zweidimensionale Umwelt kann hier schon sehr viel aufwändiger sein, zum Beispiel Futterquellen und Hindernisse enthalten. Auch wenn das Verhalten der Individuen einfach ist, kann sich hier eine komplexe Schwarmintelligenz bilden. Siehe dazu auch die in NetLogo implementierte Simulation der Entstehung einer Ameisenstraße.\nVon diesem Verhalten wurden auch sogenannte Ameisenalgorithmen abgeleitet zur Lösung von kombinatorischen Optimierungsproblemen.\n\nEtwas aufwändigeres Entscheidungsverhalten zeigen die Agenten in Schellings Segregationsmodell. Dort treffen Agenten aufgrund von unterschiedlichen Präferenzen eine Wahl, in welchen Stadtteil sie umziehen. Zu der räumlichen Umwelt kommt hier die soziale Umwelt. Das Verhalten der Agenten hängt vom Verhalten und den Präferenzen anderer Agenten ab (social embeddedness). Siehe auch hierzu die in NetLogo implementierte Simulation.\n\nRaum kann gänzlich in den Hintergrund treten, wenn das Entscheidungsverhalten der Agenten nicht mehr von dem Ort, an dem sie sich aufhalten, abhängt, sondern von den anderen Agenten, mit denen sie Kontakt haben, wie etwa bei Konsumentenverhalten oder der Ausbreitung kultureller Normen. Dazu werden soziale Netzwerke simuliert. Austausch findet nur mit den Agenten statt, zu denen eine Netzwerkbeziehung besteht. Hier kann das Entscheidungsverhalten der einzelnen Agenten durchaus schon komplizierter und vielschichtiger werden und zum Beispiel, wie bei Consumats, Wiederholung, Imitation, sozialen Vergleich und Nachdenken enthalten.\n\nDie wissenschaftliche Disziplin Agent-based Computational Economics beschäftigt sich mit der Simulation von wirtschaftlichem Entscheidungsverhalten auf der Ebene von Individuen. Untersuchte Fragen reichen dabei von Auktionsverhalten über individuellen Arbeitseinsatz (Moral Hazard) zu Verhalten in sozialen Dilemmata.\n\nDer Bereich der sozialen Simulation umfasst die Modellierung konkreter, beobachtbarer Situationen, die in Fallstudien untersucht werden. Die daraus resultierenden agentenbasierten Modelle bilden das Verhalten der Menschen in den Untersuchungsgebieten, zum Beispiel Landwirte in einem Flusseinzugsgebiet, ab. Gleichzeitig können sie mit mehr oder weniger komplexen Modellen der physischen Umwelt gekoppelt werden und entsprechende Rückkopplungen enthalten.\n\nAgentenbasierte Modellierung zeichnet sich vor allem durch die Möglichkeit aus, die Verbindungen zwischen der Mikro- und der Makro-Ebene explizit zu modellieren bzw. zu untersuchen. Dieser Aspekt wird in unterschiedlichen Fragestellungen benötigt:\n\n\nDie Anwendung in Artificial Economics ist dabei besonders hervorzuheben, denn die Annahme von rational handelnden Individuen (Homo oeconomicus) war stets eine Beschreibung auf der aggregierten Ebene. Das aggregierte Verhalten von wirtschaftlich handelnden Individuen kann so beschrieben werden, als würden die Individuen rational handeln. Für Märkte mit viel Information, vielen Lerngelegenheiten, genügend Zeit und Motivation mag das stimmen. Es gibt aber genügend Beispiele für Situationen, in denen Annahmen rationalen Verhaltens keine guten Prognosen über tatsächliches menschliches Verhalten liefern. Die interessanten wissenschaftlichen Fragestellungen, vor allem in Bezug auf öffentliche Güter und soziale Dilemmata, gehören zu diesen Situationen. Da es aber keine andere Theorie über menschliches Verhalten gibt, die sich auf die gleiche Weise zur Aggregation eignet, wie die der Rationalität, ist es in solchen Fragestellungen nötig, das heterogene, tatsächlich zu beobachtende Verhalten von Menschen zu untersuchen. Agentenbasierte Modellierung ist eine Methode, dieses Verhalten zu simulieren und Hypothesen über die Zusammenhänge zwischen dem Mikro-Verhalten der Individuen und dem Makro-Verhalten des Systems aufzustellen und zu untersuchen.\n\n\n"}
{"id": "2309567", "url": "https://de.wikipedia.org/wiki?curid=2309567", "title": "Sun-1", "text": "Sun-1\n\nDie Sun-1 war die erste Reihe von Unixbasierten Servern und Workstations der Firma Sun Microsystems.\n\nDie Sun-1 wurde von Andreas von Bechtolsheim während seiner Zeit an der Stanford University entwickelt. Da er keine Firma fand, die diesen Rechner in Lizenz bauen wollte, entschloss er sich mit Vinod Khosla, Scott McNealy und Bill Joy die Firma Sun zu gründen, deren erstes Produkt die Sun-1 wurde. Der erste Prototyp wurde am 24. Februar 1982 vorgestellt, ab Mai/Juni erfolgte dann die Auslieferung.\n\nAls Prozessor kam ein mit 10 MHz getakteter Motorola 68000, später auch ein Motorola 68010, zum Einsatz. Ursprünglich verfügte die Sun-1 über 256 KB Hauptspeicher, der später auf 1 MB erweitert wurde. Ein maximaler Speicherausbau auf 2 MB war möglich. Als Bussystem kam der von Intel entwickelte \"Multibus\" zum Einsatz. Der 17\"-Monitor war zusammen mit der Hauptplatine in einem Gehäuse verbaut.\n\nAls Betriebssystem wurde eine Portierung von Version 7 Unix der Firma UniSoft verwendet. Auf den Modellen mit Prozessoren vom Typ 68010 konnte alternativ auch SunOS verwendet werden.\n\nDer Sun-1 folgten mit der Sun-2 und Sun-3 noch zwei weitere Workstation-Familien, die ebenfalls Prozessoren aus Motorolas 68000er-Familie verwendeten, erst mit der Sun-4 kam eine Workstation mit dem von Sun entwickelten SPARC-Prozessor auf den Markt.\n\n"}
{"id": "2312027", "url": "https://de.wikipedia.org/wiki?curid=2312027", "title": "Rasterung von Linien", "text": "Rasterung von Linien\n\nDie Rasterung von Linien ist eine elementare Aufgabe der Computergrafik, bei der eine Linie auf das Punktraster einer Rastergrafik oder eines Raster-Grafikgeräts gezeichnet \"(gerastert)\" wird. Dazu werden diejenigen Punkte oder Pixel eingefärbt, die die ideale Strecke möglichst gut annähern.\n\nGrundlegende Algorithmen rastern Linien nur einfarbig. Eine bessere Darstellung mit mehreren Farbabstufungen ergibt sich bei fortgeschrittenen Verfahren, die Antialiasing (Kantenglättung) unterstützen.\n\nDa in der Computergrafik auch komplexere geometrische Figuren wie Polygone und beliebige Kurven häufig aus Liniensegmenten zusammengesetzt werden, bildet das Rastern von Linien gleichzeitig die Ausgangsbasis für deren Rasterung. Eine weitere Anwendung, bei der oft besonders viele Linien gezeichnet werden müssen, ist die Darstellung von Drahtgittermodellen.\n\nBei der einfarbigen Rasterung werden Linien mit einer einzigen Vordergrundfarbe auf einen Hintergrund gezeichnet. Sie eignet sich für Geräte mit monochromer Darstellung, zum Beispiel Laserdrucker.\n\nDie Anfangs- und Endpunkte der zu rasternden Linie werden üblicherweise in ganzzahligen Koordinaten angegeben, das heißt, sie liegen direkt auf den Punkten des Rasters. Deshalb werden die meisten Methoden nur für derartige Anfangs- und Endpunkte formuliert. Zur Rasterung dicker Linien gibt es mehrere Möglichkeiten, die auch für andere Kurven geeignet sind, siehe dazu den Artikel Rasterung.\n\nDie einfachste Möglichkeit der Rasterung ist die direkte Umsetzung der Gleichung, die die Linie definiert. Wenn (\"x\", \"y\") der Anfangs- und (\"x\", \"y\") der Endpunkt der Linie ist, so erfüllen die Punkte auf der Linie die Geradengleichung formula_1, wobei formula_2 die Steigung ist.\n\nDie Linie wird gezeichnet, indem in einer Schleife für jedes formula_3 von formula_4 bis formula_5 der entsprechende formula_6-Wert gemäß dieser Formel berechnet und auf die nächstliegende Ganzzahl gerundet wird. Das Pixel (\"x\", \"y\") wird dann eingefärbt.\n\nDieses Verfahren ist unnötig langsam, da innerhalb der Schleife eine Multiplikation ausgeführt wird, die auf den meisten Computern wesentlich mehr Rechenzeit als eine Addition oder Subtraktion erfordert. Eine schnellere Methode ergibt sich durch die Betrachtung der Differenz zwischen zwei aufeinanderfolgenden Schritten:\n\nDemnach genügt es, mit (\"x\", \"y\") zu starten und bei jedem Schleifendurchlauf formula_6 um formula_8 zu erhöhen. Dieses Verfahren wird auch als Digital Differential Analyzer (DDA) bezeichnet.\n\nDa die Rundung von formula_6 zur nächsten Ganzzahl dem Abrunden von formula_10 entspricht, lässt sich auch eine zusätzliche Kontrollvariable verwenden, die mit 0,5 initialisiert wird und zu der bei jedem Schleifendurchlauf formula_8 addiert wird. Jedes Mal, wenn die Kontrollvariable den Wert 1,0 erreicht oder übersteigt, wird formula_6 um 1 erhöht und von der Kontrollvariable 1,0 abgezogen. Dadurch ist keine Rundung mehr nötig. Diese Methode kann so umformuliert werden, dass sie nur schnellere Ganzzahl-Operationen verwendet und sich elegant in Assemblersprache implementieren lässt. Dennoch ist weiterhin eine langsame Division (formula_13) zu Beginn nötig, die bei kurzen Linien nicht durch die schnelle Schleife aufgewogen werden kann.\n\nDie soeben beschriebenen Verfahren funktionieren nur bei Liniensteigungen zwischen 0 und 1, was einem Winkel von 0° bis 45° zur Horizontalen entspricht. Bei anderen Steigungen wird die Linie nicht oder falsch gezeichnet. Es genügt jedoch, einen Algorithmus nur für Steigungen zwischen 0 und 1 zu beschreiben, da andere Linien durch die Nutzung von Symmetrien korrekt dargestellt werden können. Dies geschieht durch folgende drei Veränderungen:\n\n\nDurch die Anwendung dieser Verallgemeinerungen kann zur besseren Übersicht die folgende Tabelle erstellt werden:\nWobei i für m <= 1 Werte zwischen 0 und formula_22 und für m > 1 Werte zwischen 0 und formula_21 annimmt. Linien, die parallel zur X- oder Y-Achse werden von dieser Tabelle ebenfalls abgedeckt und müssen nicht gesondert betrachtet werden. Die angegebene Richtung bezieht sich auf die nebenstehende Grafik unter der Annahme, dass x nach rechts größer wird und y nach oben wächst. x1 und y1 sind dabei die Koordinaten des Startpunktes einer Linie und x2 und y2 die Koordinaten des Endpunktes.\n\nFerner kann nur durch die Veränderung des Vorzeichens folgender Pseudocode erstellt werden, der alle acht beschriebenen Fälle abdeckt:\n\nWobei ceil() eine Funktion zum Aufrunden ist und drawPixel eine beliebige Funktion zum setzen eines Pixels sein kann.\n\nDie ältesten veröffentlichten Algorithmen zum Rastern von Linien stammen aus den frühen 1960er Jahren. Sie dienten der Steuerung von Digitalplottern, bei denen sich der Stift nur in festen Abständen horizontal, vertikal oder diagonal auf einem Raster bewegen konnte. Dazu gehörte auch der 1963 von Jack Bresenham präsentierte Bresenham-Algorithmus, der nur Ganzzahl-Berechnungen verwendet. Pitteway gab eine äquivalente Herleitung dieses Algorithmus an, die gegenüber Bresenhams eher geometrischen Formulierung den Vorteil hat, dass sie auch auf andere Kurven als Linien angewendet werden kann. Der resultierende Algorithmus, manchmal „Midpoint-Algorithmus“ genannt, ist genau der gleiche wie in Bresenhams Veröffentlichung.\nDie Idee des Bresenham-Algorithmus besteht darin, bei jedem Schritt zwischen den beiden Pixeln zu wählen, die rechts („östlich“) und rechts oben („nordöstlich“) vom zuletzt gezeichneten Pixel liegen. Es wird dasjenige Pixel gewählt, das näher an der idealen Linie liegt. Dazu betrachtet man in der Midpoint-Formulierung den Mittelpunkt formula_27 zwischen den Pixeln formula_28 und formula_29: befindet sich formula_27 über der idealen Linie, so liegt formula_28 näher, ansonsten formula_29.\n\nUm die Position von formula_27 gegenüber der Linie zu bestimmen, wird eine andere Form der Geradengleichung verwendet:\n\n\"F\"(\"x, y\") ist 0 für Punkte auf der Linie, positiv für Punkte unterhalb und negativ für Punkte oberhalb der Linie. Wenn in diese Gleichung die Koordinaten von formula_27 eingesetzt werden, so erhält man den Wert\n\nJe nach Vorzeichen dieser Kontrollvariable formula_37 wird das Pixel formula_28 oder formula_29 gewählt.\n\nUm einen effizienten Algorithmus zu erhalten, wird die Kontrollvariable inkrementell berechnet, also schrittweise erhöht. Ihre Änderung zwischen zwei aufeinanderfolgenden Schritten hängt davon ab, ob Pixel formula_28 oder formula_29 gewählt wurde. Für jeden dieser Fälle betrachtet man die Differenz zwischen dem Wert der Kontrollvariable beim übernächsten und beim nächsten Pixel:\n\nBei jedem Schritt wird die Kontrollvariable je nach gewähltem Pixel um formula_43 oder formula_44 erhöht. Ist formula_45, so liegt formula_27 oberhalb der Geraden, weshalb formula_28 gewählt wird, ansonsten formula_29.\n\nDer Anfangswert der Kontrollvariable formula_49 lässt sich ebenfalls effizient berechnen, wenn formula_50, der Startpunkt der Linie also genau auf einem Pixel liegt:\n\nUm die Division durch 2 zu beseitigen, werden alle Werte der Kontrollvariable verdoppelt; das entscheidende Vorzeichen bleibt dabei erhalten. Damit lässt sich der Bresenham-Algorithmus für Linien mit einer Steigung zwischen 0 und 1 in nachfolgendem Pseudocode ausdrücken. Der Algorithmus benötigt nur Additionen innerhalb der Schleife; die einfachen Multiplikationen außerhalb der Schleife lassen sich ebenfalls durch eine Addition realisieren.\n\nEine andere Interpretation des Algorithmus geht von der Feststellung aus, dass die gerasterte Linie formula_52 Horizontal- und formula_53 Diagonalschritte enthält. Um diese beiden Schritttypen zu „mischen“, wird bei jedem Schritt entweder formula_8 von der Kontrollvariable abgezogen oder formula_55 addiert. Es wird der entsprechende Schritttyp ausgeführt, bei dem der resultierende Betrag der Kontrollvariable geringer ist. Dies wird auch aus obiger Grafik deutlich, bei der die Kontrollvariable stets so nahe wie möglich an der Nullachse liegt. Thompson beschrieb einen Algorithmus nach dieser Formulierung 1964, auf die Wahl des korrekten Anfangswerts der Kontrollvariable ging er allerdings nicht ein. Noch vor Bresenham hatte Fred Stockton 1963 einen Algorithmus zur Rasterung von Linien veröffentlicht, der ebenfalls nur Ganzzahl-Berechnungen verwendet, aber unnötig kompliziert ist.\n\nLinien, deren Endpunkte mit nicht-ganzzahligen Koordinaten angegeben werden, lassen sich ebenfalls mit dem Bresenham-Algorithmus rastern. Hierzu muss der Anfangswert der Kontrollvariable gemäß ihrer ursprünglichen Definition berechnet werden; pauschale Vereinfachungen sind nicht möglich. Der restliche Algorithmus bleibt gültig.\n\nObwohl der Bresenham-Algorithmus recht effizient ist, zeichnet er nur ein Pixel pro Schleifendurchlauf und benötigt dazu eine Addition. Eine Methode, die alle Pixel einer „Reihe“ – das heißt, Pixel mit gleicher \"y\"-Koordinate – auf einmal zeichnet, wurde zum ersten Mal von Reggiori entwickelt. Reihen mit nur einem Pixel wurden dabei gesondert behandelt. Später stellte Bresenham einen allgemeineren Algorithmus vor, der ohne Tests für diesen Spezialfall auskam.\n\nBei Bresenhams Pixelreihen-Algorithmus wird nicht formula_3 schrittweise erhöht, sondern formula_6. Für jedes formula_6 wird das Ende der aktuellen Reihe berechnet. Das geschieht durch Betrachtung der Punkte, in denen die ideale Linie eine durch den Mittelpunkt zwischen zwei vertikal benachbarten Pixeln verlaufende Horizontale schneidet. Das Ende der Pixelreihe ist der abgerundete Wert der \"x\"-Koordinate dieses Schnittpunkts:\n\nDie Endpunkt-Koordinaten der Pixelreihen lassen sich auch inkrementell berechnen. Da hier bei bestimmten Steigungen einige Punkte falsch berechnet werden können, nutzt der Algorithmus eine Symmetrie zur Geraden der Steigung ½ aus.\n\nIn der innersten Schleife von Bresenhams neuem Algorithmus sind keine Additionen erforderlich, denn alle Pixel einer Reihe werden auf einmal eingefärbt. Allerdings ist eine Division zur Initialisierung erforderlich. Fung ersetzte sie durch ein Suchverfahren und nahm einige weitere Optimierungen vor.\n\nEine andere, erstmals von Wu und Rokne vorgestellte Möglichkeit der Rasterung besteht darin, Schritte von mehreren Pixeln entlang der \"x\"-Achse zu machen und alle dazwischen liegenden Pixel der Linie auf einmal einzufärben. Dazu wird zwischen den verschiedenen möglichen „Pixelmustern“ ausgewählt. Bresenhams Algorithmus kann als Spezialfall dieser Methode angesehen werden, bei dem nur Schritte von je einem Pixel gemacht werden und bei dem nur zwischen zwei „Mustern“ (Pixel rechts oder rechts oben) gewählt wird.\n\nUm zwischen den beim Doppelschrittverfahren vier möglichen Mustern unterscheiden zu können, wird zunächst die letzte Pixelspalte des Musters betrachtet. Befindet sich das zu rasternde Pixel unten oder oben, lässt sich trivial auf das Muster 1 beziehungsweise 4 schließen. Befindet sich das Pixel hingegen in der Mitte, so ist ein zusätzlicher Test der mittleren Spalte nötig, um zwischen den Mustern 2 und 3 wählen zu können.\n\nDiese Tests werden dadurch vereinfacht, dass bei einer Steigung \"m\" ≤ ½ Muster 4 und bei \"m\" ≥ ½ Muster 1 nicht auftreten kann. Ähnlich wie beim Bresenham-Algorithmus lässt sich auch beim Doppelschrittverfahren die Kontrollvariable für die Tests inkrementell berechnen. Im Endeffekt kommt der Algorithmus nur mit Additionen und einer einfachen Multiplikation, die sich mit einer schnellen Bitverschiebung realisieren lässt, aus.\n\nEs wurden auch Algorithmen für Dreifach- und Vierfachschritte entwickelt, die nach dem gleichen Prinzip arbeiten, aber erheblich komplizierter und länger sind. Ein anderer Vierfachschritt-Algorithmus verwendet eine etwas abweichende Formulierung, die systematisch die Bedingungen untersucht, unter denen ein bestimmtes Muster auftritt, und die sich auf beliebig viele Schritte verallgemeinern lässt.\n\nUm die Geschwindigkeit der Rasterung weiter zu erhöhen, liegt es nahe, die Linie bidirektional, also gleichzeitig vom Anfangs- und vom Endpunkt aus bis zum Mittelpunkt zu zeichnen. Hierbei läuft die Schleife nur über eine Hälfte der Linie; bei jedem Schritt werden die beiden beteiligten Pixel auf jeder Seite der Linie eingefärbt. Sowohl der Bresenham-Algorithmus als auch andere Verfahren lassen sich so umändern.\n\nDabei muss aber beachtet werden, dass die mit den normalen Methoden gerasterte Linie nicht unbedingt in sich punktsymmetrisch ist. Das liegt daran, dass es bei der Rasterung uneindeutige Situationen gibt, in denen die ideale Linie genau durch den Mittelpunkt zweier vertikal benachbarter Pixel verläuft, zwischen denen beliebig gewählt werden kann. Der oben aufgeführte Bresenham-Algorithmus zum Beispiel wählt in solchen Fällen (formula_59) immer das Pixel mit kleinerer \"y\"-Koordinate aus. Beim Zeichnen von rechts nach links hingegen wird, bedingt durch die Nutzung der Symmetrie, das Pixel mit größerer \"y\"-Koordinate ausgewählt. Wenn die Linie bidirektional oder von rechts nach links gezeichnet wird, kann sich daher das Erscheinungsbild gegenüber dem normalen Algorithmus ändern, sofern nicht separat auf die uneindeutigen Fälle getestet wird.\n\nPeriodizität\n\nEs lässt sich beweisen, dass sich die Werte der Kontrollvariable beim Bresenham-Algorithmus \"a\"-mal wiederholen, wobei \"a\" der größte gemeinsame Teiler von formula_22 und formula_21 ist. Das bedeutet, dass die Werte der Kontrollvariable nur für einen Teil der Linie berechnet werden müssen und dann auf die anderen Teile angewandt werden können. Hierzu muss allerdings der ggT berechnet werden. Diese Methode lässt sich auch mit der bidirektionalen Rasterung kombinieren.\nHierarchische Pixelreihen\n\nDie Länge der Pixelreihen in einer gerasterten Linie folgt einem bestimmten Muster. Rosenfeld bewies, dass die Länge aller Pixelreihen, außer möglicherweise der ersten und der letzten, höchstens um ein Pixel abweicht. Er stellte außerdem fest, dass die Folge der Pixelreihen selbst diese Struktur aufweist, ebenso wie die Folge dieser Folgen, und so weiter. Gerasterte Linien sind also hierarchisch aus Reihen „\"n\"-ter Ordnung“ aufgebaut, die jeweils nur bestimmte Längen annehmen können. Stephenson beschrieb praktikable Algorithmen, die eine Linie ausgehend von Reihen beliebig hoher Ordnung zeichnen können, sowie einen rekursiven Algorithmus, der von der Reihe höchstmöglicher Ordnung ausgeht. Dadurch wird sowohl der Bresenham-Algorithmus als auch der Pixelreihen-Algorithmus verallgemeinert. Der Algorithmus für Reihen „nullter Ordnung“, bei dem die Pixelreihen ignoriert werden, entspricht dem gewöhnlichen Bresenham-Algorithmus.\n\nStrukturelle Algorithmen\n\nEs wurden noch weitere Algorithmen zur Rasterung vorgeschlagen, die aber nicht inkrementell arbeiten, sondern sich die strukturellen Eigenschaften der gerasterten Linien direkt zunutze machen. Sie basieren auf Überlegungen aus der Bildverarbeitung oder digitalen Geometrie und erreichen in der Praxis nicht die Geschwindigkeit der herkömmlichen Methoden, da sie Zeichenketten manipulieren oder andere langsame Operationen erfordern.\n\nBrons’ Algorithmus etwa repräsentiert die gerasterte Linie durch eine Zeichenkette aus Nullen und Einsen, wobei 0 für einen Horizontal- und 1 für einen Diagonalschritt steht. Der Algorithmus geht von einer Zeichenkette aus, die eine erste Annäherung an die Linie darstellt, fasst Folgen von Nullen und Einsen zusammen und verteilt sie gleichmäßig. Der gleiche Prozess wird auf die resultierende Folge angewandt. Das wiederholt sich so lange, bis keine Verbesserung mehr erzielt werden kann. Die so gerasterte Linie ist allerdings nicht optimal; um die gleiche Linie wie beim Bresenham-Algorithmus zu erhalten, sind zusätzliche Anpassungen nötig.\n\nObwohl zahlreiche Algorithmen entdeckt wurden, die weniger komplex als der Bresenham-Algorithmus sind, ist deren praktischer Geschwindigkeitsvorteil gering. Das liegt daran, dass die Befehle zum Einfärben von Pixeln auf heutiger Hardware verglichen mit der Ausführung des Rasteralgorithmus selbst sehr langsam sind. Einige Grafikkarten stellen jedoch etwas schnellere Funktionen zum Einfärben mehrerer Pixel auf einmal bereit, etwa die \"rectwrite\"-Funktion auf SGI-Systemen. Dies ist von Vorteil für Pixelreihen-Algorithmen, die so eine Reihe schnell auf einmal zeichnen können.\n\nDie Ausführungsgeschwindigkeit der verschiedenen Algorithmen hängt von der Länge der zu rasternden Linie ab. Algorithmen, deren innere Schleife schnell ist, die aber viel Zeit zur Initialisierung benötigen, können nur bei langen Linien einen Geschwindigkeitsvorteil verbuchen. Es wurde daher vorgeschlagen, in Abhängigkeit von der Länge der Linie den jeweils effizientesten Algorithmus zu wählen. Eine statistische Analyse der Linienlängen in verschiedenen Anwendungen wie der Darstellung von Drahtgittermodellen, Kurvensegmenten und Schriftzeichen kam zu dem Ergebnis, dass knapp drei Viertel aller gerasterten Linien weniger als zehn Pixel lang waren. Demnach lohnt es sich, für den Spezialfall der kurzen Linien zu optimieren. Algorithmen, die eher bei der Rasterung langer Linien vorteilhaft sind, eignen sich besser für Ausgabegeräte mit höherer Auflösung als Bildschirme und damit im Durchschnitt längeren Linien, etwa Laserdrucker. Bei manchen Algorithmen hängt die Geschwindigkeit außerdem von der Steigung der Linie ab – Pixelreihen-Algorithmen zum Beispiel sind weniger effizient bei diagonalen Linien, da hier nur ein Pixel pro Reihe gezeichnet werden kann.\n\nEin anderer Faktor bei der Wahl eines Algorithmus ist die Programmlänge. Hersteller von Grafikprozessoren, die die Rasterung direkt auf Hardwareniveau implementieren und daher Platz sparen müssen, bevorzugen kurze Algorithmen wie den Bresenham-Algorithmus. Bei Softwareimplementierungen ist dieser Faktor weniger kritisch.\n\nAlle Algorithmen zur einfarbigen Rasterung können in bestimmten Situationen Probleme verursachen:\n\nUnterschiedliche Helligkeit\n\nBeim Rastern von Linien gleicher Länge, aber unterschiedlicher Steigung wird nicht unbedingt die gleiche Anzahl von Pixeln eingefärbt. Im nebenstehenden Beispiel ist die diagonale Linie länger als die waagrechte, dennoch werden in beiden Fällen die gleiche Anzahl von Pixeln eingefärbt. Dies führt dazu, dass beide Linien auf dem Ausgabegerät unterschiedlich hell erscheinen. Bei monochromen Geräten kann dieses Problem nicht umgangen werden.\n\nLinienstile\n\nDie Nutzung der Symmetrie zum Rastern von Linien mit beliebigem Anfangs- und Endpunkt kann unerwünschte Effekte verursachen, falls bestimmte Linienstile verwendet werden. Wenn gestrichelte oder gepunktete Linien gezeichnet werden sollen, so fängt das jeweilige Muster beim Anfangspunkt der Linie an. Solche Linien werden anders gezeichnet, wenn Anfangs- und Endpunkt vertauscht werden. Sofern die Striche eines Linienstils durch eine bestimmte Anzahl einzufärbender Pixel definiert sind, variiert außerdem die tatsächliche Strichlänge je nach Steigung.\n\nClipping\n\nDas Clipping ist eine Operation, die die Rasterung auf einen bestimmten, meist rechteckigen Bereich einschränkt. Dies geschieht, indem vor der Rasterung die Anfangs- und Endpunkte der zu rasternden Linie zu den Kanten des rechteckigen Bereichs hin verschoben werden, sofern sie herausragen. Im Allgemeinen führt das dazu, dass die Koordinaten dieser Punkte nicht mehr ganzzahlig sind. Wenn diese Koordinaten dennoch gerundet werden, ergibt sich eine Linie mit anderer Steigung und damit möglicherweise auch anderem Erscheinungsbild. Um dies zu vermeiden, sind zusätzliche Tests nach dem Clipping nötig. Der Bresenham-Algorithmus kann auch mit einem Clipping-Algorithmus kombiniert werden.\n\nDas größte Problem bei einfarbig gerasterten Linien ist ihr im Allgemeinen „treppenartiges“ Aussehen, auch Treppeneffekt genannt. Auf Grafikgeräten, die zur Darstellung mehrerer Helligkeitsstufen fähig sind, kann diesem Effekt durch Antialiasing entgegengewirkt werden. Hierbei wird die zu rasternde Linie üblicherweise nicht mehr als eindimensionale Strecke, sondern als zweidimensionale Form, im einfachsten Fall als Rechteck mit gewünschter Dicke, betrachtet. Für die Rasterung müssen die Farbwerte der Pixel, die in der Nähe des Rechtecks liegen, ermittelt werden.\n\nBeim Antialiasing lässt sich der Farbwert eines Pixels ermitteln, indem ein so genannter Glättungskern oder Rekonstruktionsfilter über das Pixel gelegt wird. Gupta und Sproull schlugen als Glättungskern einen Kegel mit einem Radius von einem Pixel vor. Der Farbwert des Pixels ist proportional zum Volumen des Kegelteils, der die zu rasternde Linie (hier also das zu rasternde Rechteck) überlappt. Dieses Volumen wiederum hängt von der Distanz formula_62 zwischen der Mittellinie des Rechtecks und dem Pixel ab.\nDer Gupta-Sproull-Algorithmus basiert auf dem Bresenham-Algorithmus, berechnet aber zusätzlich formula_62 für jedes der Pixel, deren Glättungskern die Linie überlappt. Bei einer Linienstärke von einem Pixel sind dies maximal drei Pixel pro Spalte. Aus Effizienzgründen werden die Distanzen nicht exakt berechnet, sondern nur 24 mögliche Distanzen betrachtet. Die Intensitätswerte, die diesen Distanzen entsprechen, wurden im Voraus berechnet und in einer Tabelle (Lookup-Tabelle) gespeichert, sodass sie schnell abgerufen werden können.\n\nDie Linienenden müssen gesondert behandelt werden, da hier mehr als drei Pixel, insgesamt bis zu sechs, beteiligt sind. Die Intensitäten dieser Pixel hängen von der Steigung der Linie ab. Sie werden für einige Steigungen im Voraus berechnet und auch hier wieder in einer Tabelle gespeichert. Es sind auch andere Formen für die Linienenden denkbar, zum Beispiel abgerundete Endpunkte; die Intensitäten der beteiligten Pixel ändern sich dementsprechend.\n\nDer Gupta-Sproull-Algorithmus eignet sich für Linien mit beliebiger Linienstärke, wobei sich allerdings auch die Lookup-Tabelle ändert. Bei einer Linienstärke größer als einem Pixel muss beachtet werden, dass möglicherweise die Glättungskerne von mehr als drei Pixeln die Linie überlappen.\n\nEin Problem des Gupta-Sproull-Algorithmus ist, dass gerasterte Linien oftmals an verschiedenen Stellen unterschiedlich hell zu sein scheinen. Dieses „seilartige“ Aussehen ist vor allem auf die Unzulänglichkeit des Kegels als Glättungskern zurückzuführen.\n\nWu wählte einen anderen Ansatz für das Antialiasing, der nicht auf der Nutzung eines bestimmten Glättungskerns, sondern auf einem Fehlermaß basiert. Die Methode ist in der Grundform nur auf ideale, unendlich dünne Linien anwendbar.\n\nBeim Bresenham-Algorithmus wird versucht, eine Annäherung an die ideale Linie zu erreichen, indem der „Fehler“, also der Abstand zwischen der idealen Linie und zwei möglichen Pixeln minimiert wird. Wu schlug ein anderes Fehlermaß vor, das auf beliebige Kurven anwendbar ist. Der Fehler im Sinne dieses Fehlermaßes lässt sich vollständig beseitigen, sofern beliebige Farbwerte zugelassen werden. Dazu müssen die beiden Pixel, die direkt über und unter der idealen Linie liegen, Farbwerte proportional zur vertikalen Distanz zur idealen Linie annehmen.\n\nFür Linien gab Wu einen besonders schnellen Algorithmus an. Dank trickreicher Ganzzahl-Operationen benötigt er nur eine Kontrollvariable, die schrittweise verändert wird und sowohl die Position der zwei beteiligten Spaltenpixel als auch deren Intensität bestimmt.\n\nEine andere Möglichkeit des Antialiasing ist die ungewichtete Flächenabtastung \"(unweighted area sampling)\". Hierbei entspricht der Farbwert eines Pixels dem Flächenanteil der Linie innerhalb eines Quadrats von einem Pixel Kantenlänge um das betreffende Pixel, der Glättungskern ist in diesem Fall also ein Würfel. Für diese Methode sind schnelle Algorithmen entwickelt worden. Nachteilig an der ungewichteten Flächenabtastung ist das verschwommene Erscheinungsbild der Linien.\n\nWie auch bei der einfarbigen Rasterung kann die Struktur der Pixelreihen dazu genutzt werden, die Geschwindigkeit der Rasterung zu erhöhen.\n\nNeben den speziell für Linien optimierten Antialiasing-Verfahren können auch allgemeine Verfahren genutzt werden, etwa Whitteds Methode, bei der eine hochaufgelöste Rastergrafik als „Pinsel“ entlang der Linie bewegt wird.\n\nDie Rasterung von Linien lässt sich durch Näherungsverfahren, Nutzung von oder direkte Implementierung in Hardware sowie Parallelisierung noch effizienter gestalten. Dies ist erforderlich, wenn eine sehr große Zahl von Linien in Echtzeit gerastert werden muss.\n\nBoyer und Bourdin stellten ein Näherungsverfahren vor, das immer die direkt unter der idealen Linie liegenden Pixel einfärbt. Eine derartig gerasterte Line verfügt über einige besondere Eigenschaften, die ausgenutzt werden können. So etwa sind in diesem Fall nicht nur die Bresenham-Kontrollvariable, sondern auch die Linienabschnitte periodisch. Zusammen mit weiteren Optimierungen ergibt sich ein Algorithmus, der insbesondere bei längeren Linien erheblich schneller als die präzisen Verfahren ist. Eine Qualitätsverschlechterung ist bei Linien mit sehr geringer Steigung feststellbar.\n\nEine einfache Methode zur Parallelisierung der einfarbigen Rasterung lässt verschiedene Algorithmen parallel laufen, die – jeweils etwas verschoben – mehrere Pixel in bestimmten Abständen zeichnen. Eine andere Möglichkeit besteht darin, die Linie in mehrere ungefähr gleich große Teile aufzuteilen, die jeweils einem Prozessor zur Rasterung zugewiesen werden. Jeder Teil wird mit Hilfe des Bresenham-Algorithmus gerendert; das Hauptproblem ist die Berechnung der korrekten Anfangswerte der Variablen. Weiterhin ist es möglich, mehrere Prozessoren die Endpunkt-Koordinaten der Reihen bei Bresenhams Pixelreihen-Algorithmus berechnen zu lassen. Auch für massiv parallel arbeitende Vektorrechner mit über 1000 Prozessoren wurden Algorithmen vorgestellt. Jedes Pixel der Rastergrafik wird dabei einem Prozessor zugewiesen, der entscheidet, ob dieses Pixel eingefärbt werden soll oder nicht.\n\nUm die langsamen Speicherzugriffe bei der Rasterung zu beschleunigen, wurden spezielle Speicherarchitekturen entwickelt, etwa solche, bei denen der Speicher in Zellen unterteilt wird, in denen unabhängig voneinander ein Teil der Linie gezeichnet werden kann. Auch die Rasterung mit Antialiasing kann durch Hardware unterstützt werden.\n\nLinien können nicht nur wie normalerweise üblich 8-verbunden, sondern auch 4-verbunden \"(4-connected)\" gerastert werden. Das bedeutet, dass keine Diagonal-, sondern nur noch Horizontal- und Vertikalschritte erlaubt sind. Wenn man sich das Punktraster in Quadrate unterteilt denkt, so werden hierbei alle Quadrate, die von der Linie überlappt werden, ausgewählt. Eine Verallgemeinerung dieser Technik auf drei Dimensionen findet bei Voxelgittern, einer Beschleunigungstechnik des Raytracing, Verwendung. Sie dient der Bestimmung der Voxel, durch die ein Strahl beim Raytracing wandert.\n\nBei gerasterten Linien sind die diagonalen Pixelschritte möglichst gleichmäßig verteilt. Algorithmen zum Rastern von Linien lassen sich daher auch dazu verwenden, Punkte mit ganzzahligen Koordinaten gleichmäßig in einem bestimmten Intervall zu verteilen. Mögliche Anwendungen sind die lineare Interpolation oder das Downsampling in der Signalverarbeitung. Weitere Parallelen ergeben sich zum Euklidischen Algorithmus sowie Farey-Reihen und einigen anderen mathematischen Konstrukten.\n\nEinzelne Artikel:\n\n"}
{"id": "2312028", "url": "https://de.wikipedia.org/wiki?curid=2312028", "title": "Rasterung von Polygonen", "text": "Rasterung von Polygonen\n\nDie Rasterung von Polygonen und aneinandergereihten Liniensegmenten (Polygonzügen) ist eine Aufgabe der Computergrafik. Das Rastern von Polygonzügen basiert auf der Rasterung von Linien, erfordert jedoch bei dicker Strichbreite zusätzlichen Aufwand. Dem Rastern von gefüllten Polygonen kommt in der 3D-Computergrafik eine große Rolle zu, da 3D-Szenen gerendert werden können, indem man die auf die Bildebene projizierten Polygone farbig füllt.\n\nBeim Rastern dicker Liniensegmente muss entschieden werden, wie sie miteinander verbunden werden. Dicke Linien als Rechtecke zu betrachten, liefert unschöne Ergebnisse. Besser ist es, Linien mit runden Enden zu zeichnen. Eine andere Möglichkeit ist \"Mitering\" („Gehrung“), bei der die Linienenden schräg gezeichnet werden, sodass sie aneinander angrenzen. Bei sehr spitzen Winkeln ragen die Linienenden zu sehr über die eigentliche Polygonecke heraus, weshalb sie besser abgeschnitten werden.\n\nBei der Rasterung von Polygonen muss entschieden werden, wie deren Koordinaten interpretiert werden. Wenn beispielsweise ein Rechteck mit den Endpunktkoordinaten (1, 1) und (5, 4) gerastert wird, so werden im Normalfall 5×4 Pixel eingefärbt, obwohl das Rechteck nur 4×3 Einheiten groß ist. Dieser unerwünschte Effekt ist eine Konsequenz der endlichen Bildauflösung. Wenn nebeneinanderliegende Polygone gezeichnet werden, so führt das dazu, dass einige Pixel mehrfach eingefärbt werden. Eine Möglichkeit, dieses Problem zumindest bei ganzzahligen Koordinaten zu umgehen, besteht darin, sie bei der Rasterung um einen halben Pixelabstand nach links und nach unten zu verschieben, sodass in Wirklichkeit das Rechteck mit den Koordinaten (0,5, 0,5) und (4,5, 3,5) gerastert wird (siehe Bild). Dadurch wird vermieden, dass bei nebeneinanderliegenden Polygonen Kanten doppelt eingefärbt werden, was bei bestimmten Rasteroperationen wie XOR unerwünschte Resultate hervorrufen würde. Eine alternative Methode, die Gleitkommazahlen vermeidet, ist, das jeweils letzte Pixel einer Zeile oder einer Spalte nicht zu rastern. Dabei wird in Kauf genommen, dass das Polygon etwas verschoben erscheint.\n\nAuch sehr spitzwinklige Polygonecken können dazu führen, dass Pixel jeweils von mehreren Segmenten eingefärbt werden. Ein weiteres Beispiel für Artefakte sind Slivers, Polygonteile, die so dünn sind, dass sie keine Pixel einschließen und bei denen einige Rasteralgorithmen nur einzelne oder gar keine Pixel zeichnen.\n\nWenn ein gerasterter Winkel durch eine weitere Linie halbiert werden soll, etwa um Pfeile darzustellen, so ist es im Allgemeinen nicht möglich, ein exakt symmetrisches Resultat zu erreichen.\n\nPolygone mit mehr als drei Ecken als Teil von Polygonnetzen werden vor der Rasterung oft in Dreiecke umgewandelt. Dabei teilen sich sehr viele Dreiecke ihre Kanten. Wenn diese aneinanderliegenden Dreiecke unterschiedlich gefärbt sind und jede Kante als einfarbige Linie gerastert wird, so hängt das Erscheinungsbild von der Reihenfolge ab, in der die Dreiecke gerastert werden.\n\nUm dieses Problem zu umgehen, färbt man ein Pixel dann und nur dann ein, wenn es innerhalb des zu zeichnenden Dreiecks liegt. Dazu können baryzentrische Koordinaten verwendet werden. Bezogen auf ein Dreieck kann jeder Punkt der Ebene durch die Koordinaten formula_1 beschrieben werden. Ein Punkt oder Pixel befindet sich genau dann strikt innerhalb dieses Dreiecks, wenn sich jede dieser Koordinaten im Intervall formula_2 befindet. Diese Methode ist auch für nicht-ganzzahlige Eckpunktkoordinaten gültig.\n\nEinen Sonderfall stellen Pixel dar, die sich genau auf einer Kante befinden und somit nicht trivial einem der beiden anliegenden Dreiecke zugewiesen werden können. Für diese Fälle wählt man einen außerhalb des Bildes liegenden Referenzpunkt und wählt die Farbe desjenigen Dreiecks, dessen nicht auf der Kante befindlicher Eckpunkt näher an diesem Punkt liegt. Diese Methode funktioniert immer dann, wenn der Referenzpunkt nicht auf der durch die Kante verlaufenden Geraden liegt.\n\nAm einfachsten werden gefüllte Dreiecke und damit Polygone gerastert, indem für jedes Pixel des Bildes der oben beschriebene Test angewendet wird: Pixel werden nur dann eingefärbt, wenn sie innerhalb eines Dreiecks liegen. Eine etwas effizientere Methode testet nur diejenigen Pixel innerhalb eines Rechtecks, welches das zu rasternde Dreieck einschließt. Neben diesen einfachen Methoden sind jedoch schnellere Verfahren entwickelt worden, die im Folgenden beschrieben werden.\n\nKantenlisten-Algorithmen bestimmen für jede Kante des Polygons die Schnittpunkte mit den Bildzeilen. Diese können direkt berechnet werden, oder es können Algorithmen zur Rasterung von Linien verwendet werden. Horizontale Kanten werden ignoriert. Die so ermittelten Schnittpunkte werden in einer Liste abgelegt.\n\nUm wie weiter oben beschrieben zu vermeiden, dass zu viele Pixel an den Kanten eingefärbt werden, werden in Wirklichkeit die Schnittpunkte mit genau zwischen zwei Bildzeilen verlaufenden Geraden berechnet. Für das rechts im Bild dargestellte Beispielpolygon werden somit folgende Schnittpunkte in der Liste abgelegt:\n\nDiese Schnittpunkte werden nun nach \"y\"-Koordinaten absteigend sortiert. Unter Werten mit gleichen \"y\"-Koordinaten wird aufsteigend nach \"x\"-Koordinaten sortiert. Nach der Sortierung sieht die Liste folgendermaßen aus:\n\nIn dieser Liste gibt es immer eine gerade Anzahl von Werten mit gleicher \"y\"-Koordinate. Das Polygon wird gezeichnet, indem nacheinander Punktpaare aus der Liste betrachtet werden. Sie sind stets von der Form (x; y) (x; y); beide Punkte haben also die gleiche \"y\"-Koordinate. Es werden nun alle Pixel der entsprechenden Bildzeile gezeichnet, deren \"x\"-Koordinate sich im Intervall formula_3 befindet.\n\nDas erste Punktpaar ist (1; 6,5) (1,5; 6,5). Das einzige Pixel, das die eben genannte Bedingung erfüllt, ist hier (1; 6). Anschließend wird das nächste Punktpaar, (1; 5,5) (2,5; 5,5), betrachtet. Hier gibt es zwei Pixel, die eingefärbt werden, nämlich (1; 5) und (2; 5). Das Polygon ist fertig gerastert, wenn alle Punktpaare der sortierten Liste abgearbeitet wurden.\n\nDie Vorausberechnung der Schnittpunkte von Polygonkanten und zwischen den Bildzeilen verlaufenden Geraden ist unnötig zeitaufwändig und kann erheblichen Speicherplatz erfordern. Mit einer so genannten aktiven Kantenliste lässt sich die Berechnung der Schnittpunkte inkrementell durchführen und der Speicherplatz reduzieren. Dieses Verfahren wird gelegentlich \"Scanline-Algorithmus\" genannt; allerdings werden mit Scanline-Algorithmen auch darauf aufbauende Verfahren bezeichnet, um im Rahmen der 3D-Computergrafik aus Polygonen aufgebaute Szenen Zeile für Zeile zu rastern.\n\nBei diesem Algorithmus werden für jede Polygonkante nicht die Schnittpunkte mit allen Geraden, sondern nur mit der Geraden mit der größten \"y\"-Koordinate, die die Kante schneidet, ermittelt. Zusätzlich zur \"x\"-Koordinate des Schnittpunkts werden folgende Daten ermittelt:\n\n\nDie Daten werden in einer Tabelle gespeichert, deren Einträge nach der Bildzeile sortiert sind. Für das Beispielpolygon ergibt sich folgende Tabelle:\n\nDie Grundidee des Algorithmus besteht darin, von diesen vorberechneten Daten auszugehen und mit Hilfe der Δx-Werte die Koordinaten der anderen Schnittpunkte fortlaufend zu berechnen. Dabei wird von der höchsten Bildzeile ausgegangen und schrittweise zur niedrigeren Bildzeile gewechselt. Eine \"aktive Kantenliste\" speichert die Kanten, die die zur Bildzeile gehörende Gerade schneiden, sowie für jede Kante die aktuellen \"x\"-, Δx- und n-Werte.\n\nZu Beginn ist die aktive Kantenliste leer. Ausgegangen wird von der höchsten Bildzeile. Für jede Bildzeile wird in der vorberechneten Tabelle gesucht, ob sie Kanten enthält, die noch nicht in der aktiven Kantenliste enthalten sind, und wenn ja, werden die entsprechenden Daten in die aktive Kantenliste kopiert. Nun werden die \"x\"-Werte aller in der aktiven Kantenliste befindlichen Kanten aufsteigend sortiert. Die resultierenden Punktpaare werden wie im grundlegenden Kantenlisten-Algorithmus dazu verwendet, Pixel einzufärben. Anschließend werden die n-Werte in der aktiven Kantenliste um 1 erniedrigt; fällt ein Wert unter 0, so wird die entsprechende Kante aus der aktiven Kantenliste entfernt. Schließlich werden zu allen \"x\"-Werten in der aktiven Kantenliste Δx addiert, und die Prozedur wird mit der nächsten, niedrigeren Bildzeile wiederholt. Die Rasterung ist fertig, sobald alle Bildzeilen abgearbeitet wurden.\n\nBeim Beispielpolygon verändert sich die aktive Kantenliste in den ersten vier Bildzeilen wie folgt:\n\nDer größte Nachteil der Kantenlisten-Algorithmen ist der Aufwand zur Sortierung und Manipulation der Listen. Der sehr einfache Edge-fill-Algorithmus kommt ohne diesen Aufwand aus. Beim Edge-fill-Algorithmus werden für jede Bildzeile, die bei formula_4 eine Polygonkante schneidet, alle Pixel dieser Bildzeile mit einer \"x\"-Koordinate strikt größer als formula_5 invertiert. „Invertierung“ bedeutet hier, dass eingefärbte Pixel in den Ausgangszustand zurückgesetzt werden und umgekehrt. Die Reihenfolge, in der die Polygonkanten abgearbeitet werden, ist beliebig. Wenn der Algorithmus die Kanten des Beispielpolygons gegen den Uhrzeigersinn abarbeitet, so ergeben sich folgende Schritte:\n\nDas gerasterte Polygon unterscheidet sich von dem der Kantenlisten-Algorithmen, denn drei Pixel werden nicht eingefärbt. Der Nachteil des Algorithmus ist, dass viele Pixel mehrmals geändert werden müssen.\n\nDer Fence-fill-Algorithmus ist eine Weiterentwicklung des Edge-fill-Algorithmus, der die Zahl der nötigen Pixelinvertierungen verringert. Dabei wird ein \"Zaun\" (engl. “fence”), eine durch den Schnittpunkt zweier Polygonkanten verlaufende vertikale Gerade, genutzt.\n\n\nDadurch ergeben sich folgende Schritte:\n\nDer Edge-flag-Algorithmus besteht aus zwei Schritten:\n\n\n\nAls Softwareimplementierung sind der Kantenlisten-Algorithmus und der Edge-flag-Algorithmus vergleichbar schnell, implementiert in Hardware ist letzterer jedoch erheblich schneller.\n\n"}
{"id": "2315719", "url": "https://de.wikipedia.org/wiki?curid=2315719", "title": "HeidiSQL", "text": "HeidiSQL\n\nHeidiSQL (vorher bekannt als MySQL-Front) ist ein freier Client für das Datenbanksystem \"MySQL\" sowie inzwischen weitere Datenbanken, der vom deutschen Programmierer Ansgar Becker entwickelt wird.\n\nDie Veröffentlichungen mit Versionsnummern vor 3.0 waren als MySQL-Front (bis 2.5) bekannt. Der Name ist im Jahr 2006 in „HeidiSQL“ geändert worden, als Ansgar Becker im April 2006 seine alten Quellen in ein neues Open-Source-Projekt bei SourceForge einbrachte.\n\nEs gab auch eine Shareware-Anwendung namens „MySQL-Front 3.0“, die von einem anderen Entwickler von 2004 bis 2006 entwickelt wurde und eine neue Codebasis hatte.\n\nProgrammiert wurde HeidiSQL ausschließlich durch Ansgar Becker, nachdem es phasenweise auch Teammitglieder aus Dänemark und Brasilien gegeben hatte. Andere Programmierer haben kleinere Code-Schnipsel eingeschickt und dadurch zum Teil neue Funktionen eingebaut, wie z. B. den LaTeX-Export.\n\nUnterstützung für den Microsoft SQL Server wurde ab März 2011 für die Version 7.0 eingebaut.\n\nÜbersetzungen der Oberfläche existieren momentan in ca. 20 Sprachen, die per Transifex verwaltet werden. Während die deutsche Übersetzung vom Autor selbst erstellt wird, befassen sich ab der Version 8.0 ca. 30 freie Autoren mit den anderen Sprachen.\n\nDie PostgreSQL-Unterstützung ist seit März 2014 in Arbeit und offiziell seit der Version 9.0 nutzbar.\n\n\n\n\n"}
{"id": "2318896", "url": "https://de.wikipedia.org/wiki?curid=2318896", "title": "DATEV", "text": "DATEV\n\nDie DATEV eG ist ein Softwarehaus und IT-Dienstleister für Steuerberater, Wirtschaftsprüfer und Rechtsanwälte, aber auch für deren Mandanten, wie mittelständische Unternehmen, Kommunen, Vereine und Institutionen. Der Schwerpunkt liegt im Steuerberatermarkt.\n\nDas Unternehmen ist eine eingetragene Genossenschaft. Mitglieder können nur Angehörige der steuerberatenden, wirtschaftsprüfenden und rechtsberatenden Berufe werden.\n\nDie DATEV wurde am 14. Februar 1966 von 65 Steuerbevollmächtigten im Kammerbezirk Nürnberg gegründet, um die Buchführung ihrer Mandanten mit Hilfe der EDV zu erledigen. Ihr ursprünglicher Name lautete \"DATEV Datenverarbeitungsorganisation der Steuerbevollmächtigten für die Angehörigen des steuerberatenden Berufes in der Bundesrepublik Deutschland, eingetragene Genossenschaft mit beschränkter Haftpflicht\". Initiatoren waren Heinz Sebiger und Joachim Mattheus. Ende der 1960er Jahre wurde von der DATEV die erste BWA (Betriebswirtschaftliche Auswertung) in Deutschland als sogenannte \"DATEV-Standard-BWA Nr. 1\" eingeführt.\n\nHintergrund für die Gründung waren die damals neuen Einsatzmöglichkeiten der EDV, die Arbeitskräfteknappheit und die für 1968 bevorstehende Einführung der \"Netto-Allphasen-Umsatzsteuer mit Vorsteuerabzug\" (umgangssprachlich: Mehrwertsteuer). Zunächst waren die Dienstleistungen der DATEV vollständig rechenzentrumsbasiert, sie umfassten Eingabe der Daten in Datenerfassungsgeräten vor Ort, Versand der Lochstreifen per Post zum Rechenzentrum (zunächst von IBM, dann seit 1969 eigenes Rechenzentrum) und Rücksendung der Auswertungen per Post an die Kanzlei. Seit 1974 besteht die Möglichkeit, die Daten per DFÜ in das Rechenzentrum zu übertragen. Der Rückversand der Auswertungen (Buchhaltungsauswertungen, Lohnabrechnungen, Steuererklärungen oder Bilanzen) blieb zunächst postalisch, weil auch zumeist noch die Papierform vorgeschrieben war. Ab 1985 wurde mit der sukzessiven Umstellung der Programme auf die sog. „Im-Haus-Verarbeitung“ in der Kanzlei begonnen, jedoch waren die ersten Anwendungen zur kompletten Im-Haus-Verarbeitung ohne Nutzung des Rechenzentrums erst 1989 verfügbar.\n\n1998 stellte die DATEV die Software von MS-DOS auf Microsoft Windows um. Eine ursprünglich in Richtung OS/2 von IBM vorgesehene Portierung wurde aufgrund ausbleibenden Erfolgs dieses Betriebssystems nicht weiter verfolgt.\n\nSeit 1998 erweiterte die DATEV ihre Geschäftsfelder auf Software und IT-Dienstleistungen sowie Consulting und Schulungen. Die \"Im-Haus\"-Verarbeitung der Daten durch die Mitglieder nahm zu. In Reaktion auf die aktuellen Rückkehrtendenzen zur Nutzung zentral vorgehaltener Ressourcen (\"Cloud Computing\") wurde Frühjahr 2011 ein zusätzlicher Rechenzentrumsstandort in Betrieb genommen. DATEV unterstützt heute netzbasierte Lösungen, Vor-Ort-Software und Hybridmodelle.\n\nSoftware für Finanzbuchführung: Die Finanzbuchführungen von rund 2,5 Millionen der meist mittelständischen deutschen Unternehmen werden vom Steuerberater oder im Unternehmen selbst mit DATEV-Software erstellt. Dementsprechend steht DATEV auch als Synonym für einen deutschen Standard EDV-gestützter Buchführung.\n\nLohn- und Gehaltsabrechnung: Im Jahr 2018 wurden monatlich rund 13 Millionen Lohn- und Gehaltsabrechnungen mit DATEV-Software erstellt. Davon werden etwa 80 Prozent im Nürnberger DATEV-Rechen-, Druck- und Logistikzentrum erstellt, schätzungsweise 20 Prozent werden über PC-Programme der DATEV direkt vor Ort in Unternehmen und Kanzleien erzeugt.\n\nDatendistribution und Informationsbereitstellung: Weitgehend automatisiert werden über das DATEV-Rechenzentrum Informationen zwischen mittelständischen Unternehmen und deren Steuerberatern sowie rund 200 Institutionen in Deutschland ausgetauscht – darunter Finanzverwaltungen, Sozialversicherungsträger, Krankenkassen, Banken, Berufsgenossenschaften oder statistische Ämter. Damit unterstützt DATEV mittelständische Unternehmen bei den im Rahmen verschiedener E-Government-Projekte zunehmend elektronisch zu erledigenden Meldepflichten. In Datenbanken stehen den DATEV-Mitgliedern mehr als 530.000 Dokumente zu den Themen Steuer-, Zivil-, Handels- und Gesellschaftsrecht im Volltext zur Verfügung.\n\nSicherheitsdienstleistungen: Ein zentrales Element der DATEV-Unternehmenskultur macht seit jeher der Bereich Datenschutz und -sicherheit aus. Ursprung dieses Schwerpunktes ist die berufliche Verschwiegenheitspflicht, der die Mitgliedsberufsgruppen der DATEV unterliegen. Auf Basis des einschlägigen Know-how entwickelte die Genossenschaft auch eine Reihe von Sicherheitsdienstleistungen, die vom Schutz des Internet-Zugangs sowie von Rechnern und Netzwerken über die Datensicherung und Möglichkeiten zur sicheren Nutzung mobiler Arbeitsmittel bis hin zu differenzierten Beratungsleistungen und dem physischen Schutz von Geschäftsräumen reichen. So können Anwender beispielsweise einen Managed Security Service nutzen, bei dem über die kanzlei- bzw. unternehmenseigenen Kommunikationseinrichtungen (DSL) eine ausschließliche Einwahl per VPN-Tunnel ins DATEV-Rechenzentrum erfolgt. DATEVnet-Nutzer werden über eine zentrale Sicherheitszone bei DATEV ins Internet geleitet. Darin befinden sich gestaffelte und redundant ausgelegte Schutz-Systeme, wie zum Beispiel Virenscanner oder Firewall-Systeme, die permanent von Sicherheits-Spezialisten überwacht und aktualisiert werden. Ebenso bietet die DATEV gesicherten E-Mail-Verkehr mit kaskadiertem, zentralem Virenscan und verschlüsselten Mails. Ein besonderes Alleinstellungsmerkmal ist der so genannte Reverse-Scan, für den Kopien aller E-Mails, die den DATEVnet-Anwendern zugestellt wurden, über den Zeitraum von zwölf Stunden in einem zentralen Speicherpuffer permanent mit den stetig aktualisierten Schutzmechanismen überprüft werden. So können auch Viren, Trojaner oder Keylogger schnell entdeckt werden, die es geschafft haben sollten, sich in der kurzen Zeit ins System zu schleichen, bevor ihr Auftauchen bekannt wurde. Zur Komplettierung des Schutzgürtels wurde DATEVnet mit dem Web-Radar incl. Reverse-Scan eingeführt. Der Web-Radar ist ein mehrstufiges Sicherungskonzept mit statischen und dynamischen Schutzfiltern. Es werden als besonderes Sicherheitsservice alle Seitenaufrufe der vergangenen 24 Stunden auf Web-Adressen geprüft, die auf einen Schädlingsbefall hinweisen.\n\nConsulting: Das Consulting der DATEV berät Kanzleien und in Kooperation mit deren Steuerberatern auch Unternehmen in strategischen und organisatorischen Fragen ebenso wie zum Einsatz von Informationstechnologie sowie zu Sicherheit und Datenschutz.\n\nSoftware für die Wirtschaftsprüfung: Wirtschaftsprüfer unterstützt DATEV speziell mit Lösungen rund um die Abschlussprüfung. Neben Produkten für die Abschlussprüfung mit Arbeitspapieren, Checklisten und Vorlagen für Prüfungsberichte gibt es Angebote für die Qualitätskontrolle und -sicherung sowie für die digitale Datenanalyse.\n\nLösungen für Rechtsanwälte: Seit 1998 engagiert sich DATEV im Rechtsanwaltsmarkt. Kern des spezifischen Angebots ist das Kanzleisystem Anwalt classic pro.\n\nSoftware für Kommunen und kommunale Betriebe: Die Verwaltungen setzen verstärkt auf betriebswirtschaftliche Steuerungskonzepte. Gemeinsam mit dem steuerlichen Berater liefert DATEV speziell auf die kommunalen Belange abgestimmte Beratung sowie ein Software-System rund um das Finanzwesen.\n\nSeit Anfang 2000 gab es interne Auseinandersetzungen unter den Genossenschaftsmitgliedern über die strategische Ausrichtung, was das Anbieten der DATEV-Software auch für Personen, die nicht Berufsträger sind, angeht (Stichwort „Mandantendirektgeschäft“). Mit der Gründung der „IDA“ (Interessengemeinschaft der DATEV Anwender e.V.) bildete sich ein genossenschaftsorientierter Gegenpol innerhalb der Mitglieder. Es trat das Spannungsverhältnis „genossenschaftliche Verpflichtung“ (d. h. die Genossenschaft soll nur das tun, was den Genossen direkt nutzt) und „wirtschaftlicher Erfolg“ (jede sinnvolle Umsatzausweitung ist prinzipiell gut) zutage. Die Auseinandersetzung scheint seit der außerordentlichen Vertreterversammlung vom 18. Februar 2005 beendet. Mit ausdrücklicher Billigung und Unterstützung der IDA wurde die Satzung mit einer Mehrheit von 86,5 % geändert, um das nun sog. „mitgliedsgebundene Mandantengeschäft“ zu ermöglichen. Erforderlich ist aber stets die vorherige Einwilligung des Beraters, die auch zurückgezogen werden kann.\n\nSeit dem Jahr 2000 ist DATEV auch im Ausland aktiv. Das internationale Engagement umfasst derzeit die Länder Tschechien, die Slowakei, Österreich, Italien, Polen, Spanien und Ungarn.\n\nDas Leistungsspektrum der DATEV umfasst vor allem die Bereiche Rechnungswesen, Personalwirtschaft, betriebswirtschaftliche Beratung, Steuern, Enterprise-Resource-Planning (ERP) sowie Organisation und Planung. Es reicht von mehr als 200 PC-Programmen über Cloud-Dienste wie Online-Anwendungen, Datenverarbeitung und -archivierung im Rechenzentrum bis hin zu Outsourcingleistungen sowie Sicherheitsdienstleistungen. Abgerundet wird das Angebot durch Beratungsleistungen und Angebote zur Wissensvermittlung in Deutschland und einigen anderen europäischen Ländern.\n\nDATEV ist Microsoft Windows-Software. Für Apple-Nutzer existieren Emulatoren, die die Programme auf Mac-Rechnern lauffähig machen. Die Datenhaltung erfolgt über den Microsoft SQL Server. Die meisten Programme waren neben der Unterstützung der Microsoft Office-Formate auch mit Schnittstellen zu OpenOffice ausgestattet, 2017 wurde die Unterstützung von OpenOffice aufgrund einer zu geringen Nachfrage eingestellt. Die DATEV-Programme zur Lohn- und Gehaltsabrechnung verfügen darüber hinaus über abgenommene Schnittstellen zu den ERP-Systemen SAP Business One, SAP Business ByDesign und Microsoft Dynamics NAV.\n\nDie DATEV ist Sponsor des Challenge Triathlon Roth. Der Vertrag läuft von 2013 bis 2021.\n\n\n\n"}
{"id": "2322661", "url": "https://de.wikipedia.org/wiki?curid=2322661", "title": "Animal Logic (Australien)", "text": "Animal Logic (Australien)\n\nAnimal Logic ist ein australisches Unternehmen, das 1991 gegründet wurde und visuelle Effekte für Filme, Fernsehproduktionen, Musikvideos, Computerspiele und Werbespots entwirft und produziert.\n\nBegonnen hat die Firma mit Effektdesign für TV-Sendungen und Werbespots, was lange Zeit das Kerngeschäft des Unternehmens war.\n\nSeit 1997 gehört Animal Logic zu den Fox Studios.\n\nAnimal Logic begann in den letzten Jahren immer mehr Aufträge für große kommerzielle Filmprojekte anzunehmen. Beispiele hierfür sind die Filme \"Matrix Reloaded\", \"House of Flying Daggers\", \"World Trade Center\", \"Moulin Rouge\", \"Harry Potter und der Feuerkelch\" und \"Stealth – Unter dem Radar\". Auch produzierten sie mehr Werbespots für große Unternehmen wie Nike, Adidas, Toyota und die Umweltschutzorganisation Greenpeace.\n\nIm Jahre 2002 begann die Arbeit am ersten computeranimierten Zeichentrickfilm \"Happy Feet\" für den Regisseur George Miller und Warner Bros. Animal Logic wurden für die realistischen Effekte und Szenarien herangezogen. Der in den USA am 17. November 2006 erschienene Film sorgte für eine signifikante Expansion des Unternehmens auf 300 Künstler und Techniker auch außerhalb Australiens. \"Happy Feet\" war der erste digital animierte Hauptfilm, der in Australien produziert wurde. Der Film bekam 2007 einen Oscar in der Kategorie Bester animierter Spielfilm.\n\n\n\n"}
{"id": "2324546", "url": "https://de.wikipedia.org/wiki?curid=2324546", "title": "CentOS", "text": "CentOS\n\nCentOS (\"Community Enterprise Operating System\") ist eine Linux-Distribution, die auf Red Hat Enterprise Linux (RHEL) des Unternehmens Red Hat aufbaut. Die Distribution wird von einer offenen Gruppe von freiwilligen Entwicklern betreut, gepflegt und weiterentwickelt.\n\nCentOS ist 2016 laut WTechs hinter Ubuntu und Debian die am dritthäufigsten verwendete Linux-Distribution für Web-Server.\n\nDie kommerzielle Linux-Distribution RHEL kann nur im Zusammenhang mit Supportverträgen erworben werden. Die Firma Red Hat stellt aber alle Quellpakete von RHEL im Internet bereit, um die Anforderungen unterschiedlicher Lizenzen enthaltener freier Software zu erfüllen. Das ermöglicht es, eine zu RHEL binärkompatible Linux-Distribution zu entwickeln. Durch die Binärkompatibilität ermöglicht CentOS, Computer mit einer RHEL-kompatiblen Linux-Distribution zu nutzen, ohne einen Supportvertrag mit Red Hat abschließen zu müssen. Auch ergibt sich, neben finanziellen Ersparnissen, der Vorteil, dass alle Software, die für RHEL angeboten wird, auch direkt und ohne Einschränkungen unter CentOS genutzt werden kann.\n\nAm 7. Januar 2014 gaben Red Hat und das CentOS-Projekt bekannt, dass man sich zusammenschließe. Red Hat stellte vier der CentOS-Entwickler an und ein neues „\"CentOS Governing Board\"“ – dem sowohl Mitarbeiter von Red Hat als auch Community-Mitglieder angehören – soll die zukünftige Entwicklung von CentOS führen.\n\nDer Zweck von CentOS ist, eine vollständig zu Red Hat Enterprise Linux binärkompatible Linux-Distribution zur Verfügung zu stellen. Hinzu kommt eine angestrebte schnelle Reaktionszeit in Bezug auf das Bereitstellen von Updates und die Möglichkeit, zusätzlich Support zu CentOS zu erwerben.\n\nCentOS ist binärkompatibel zu RHEL und daher ebenfalls ein Enterprise-Betriebssystem, also ein Betriebssystem, das auf die Bedürfnisse großer Unternehmen und staatlicher Organisationen ausgerichtet ist. Als Enterprise-Betriebssystem ist es deshalb auf Stabilität und lange Wartungszyklen ausgelegt. Man kann CentOS bis zu zehn Jahre nutzen, ohne Pakete bzw. Softwareversionen migrieren zu müssen, weshalb es für den kommerziellen Einsatz geeignet ist. Für RHEL bieten große Softwarehäuser wie Oracle oder SAP Zertifikate an, die garantieren, dass deren Software auf RHEL problemlos funktioniert, was analog für große Hardwarehersteller gilt. Enterprise-Betriebssysteme findet man daher meist auf Workstations und Servern, wo ein extrem stabiler Betrieb verlangt wird z. B. in der Wissenschaft, Forschung, Börse, Militär oder Raumfahrt. Im Gegensatz zu RHEL gibt es für CentOS von den meisten Software- und Hardwareherstellern weder Zertifikate noch Support. Aufgrund der Binärkompatibilität zu RHEL kann es aber oft von den Voraussetzungen, die für RHEL geschaffen werden, direkt profitieren.\n\nCentOS unterstützt nahezu alle Architekturen, für die auch Red Hat Enterprise Linux zur Verfügung steht.\n\nIn der neusten Version (7) werden folgende Architekturen uneingeschränkt unterstützt:\n\nDie folgenden Architekturen werden von älteren CentOS-Versionen unterstützt:\n\nEin Live-CD-Image von CentOS ist auf der Homepage des Projekts verfügbar. Dieses Image kann auf eine CD-ROM gebrannt werden oder mit dem Befehl dd oder UNetbootin auf einen USB-Stick übertragen und danach gebootet werden.\n\nMöglicherweise entstehen innerhalb des CentOS-Projekts sogenannte \"Special Interest Groups\" (SIG), die eine Portierung auf weitere Architekturen vornehmen werden. An einer i686-Variante von CentOS 7 wurde im Januar 2014 gearbeitet. Fertiggestellt und veröffentlicht wurde diese 32-Bit-Variante im Oktober 2015. Seit Dezember 2015 wird auch die ARM-Architektur unterstützt.\n\nWie bei RHEL wird CentOS mit einem grafischen Installer mit dem Namen \"Anaconda\" installiert, der auch für Einsteiger leicht bedienbar ist. Bei der Softwareverwaltung setzt CentOS auf den Paketmanager RPM und die Software-Verwaltung yum. System-Komponenten sowie Anwendungen werden dabei online auf einem Repository-Server gesucht, von dort als RPM-Package heruntergeladen und installiert.\n\nRepositories anderer Hersteller verfolgen meist andere Ziele oder eine andere Lizenzpolitik als CentOS. Nennenswert sind hier \"Dag Wieers\", \"RPM Fusion\", \"RPMForge\" und \"atrpms\". Die Quellen sind nicht immer zueinander kompatibel. Darüber hinaus stellen immer mehr Softwareprojekte und Firmen, wie das GStreamer-Projekt, Skype oder Adobe Systems, das Mono-Projekt, eigene Repositories zur Verfügung.\n\nEPEL (Extra Packages for Enterprise Linux) ist ein vom Fedora-Projekt gepflegtes Repository, das portierte Pakete bereitstellt, die in Fedora selbst enthalten sind, aber nicht in RHEL, CentOS oder Scientific Linux. Weil diese Enterprise-Distributionen auf der Basis von Fedora entwickelt werden, sind meist nur sehr kleine Anpassungen an den Paketen notwendig. EPEL erweitert somit die Enterprise-Distributionen um viele dort nicht enthaltene Anwendungen und Treiber. Da EPEL allein vom Einsatz der Community abhängt, gibt \"Red Hat\" oder das \"Fedora Projekt\" für EPEL-Pakete keine Garantien, Support oder Zertifizierungen, wie dies für Pakete im offiziellen RHEL-Repository üblich ist.\n\nNormalerweise können Upstream-Updates immer sehr zeitnah (praktisch binnen Ein-Tages-Frist) zur Verfügung gestellt werden. Nur bei den etwa halbjährigen „Point-Releases“ von RHEL ist man einige Wochen (bisher schlimmstenfalls ein bis zwei Monate) davon abgeschnitten, da das Freiwilligen-Team von CentOS dann deutlich mehr zu tun hat. Während der Fertigstellung von CentOS 6.0 und 6.1 kam es zu einer sieben- respektive fünfmonatigen Verzögerung, weil sich das CentOS-Projekt zu jenem Zeitpunkt gerade neu organisierte.\n\n\n"}
{"id": "2325709", "url": "https://de.wikipedia.org/wiki?curid=2325709", "title": "Apollo/Domain", "text": "Apollo/Domain\n\nApollo/Domain war eine Workstation-Serie des Herstellers Apollo Computer Inc., die zwischen 1980 und 1989 produziert wurde. Obwohl die Namen mittlerweile kaum noch bekannt sind, galt Apollo zwischen 1980 und 1987 als größter und bedeutendster Hersteller von vernetzten Workstations. Die meisten Workstations nutzten eine Motorola-68k-CPU mit Apollo eigenem Chipsatz. Eine wesentliche Ausnahme bildet aber die DN10000-Workstation, sie nutzte als Prozessor eine Eigenentwicklung, einen der ersten RISC-Prozessoren, den Apollo-PRISM-Prozessor, welcher auch Pate für die PA-RISC-CPUs von Hewlett-Packard gestanden haben soll.\n\nDas ursprüngliche Betriebssystem, eine Eigenentwicklung, nannte sich Apollo Aegis, Aegis wurde später in Apollo Domain/OS umbenannt. Aegis war ein elegantes eigenständiges Betriebssystem. Ende der Achtziger wurde es POSIX-konform erweitert. Bemerkenswert ist, dass große Teile des Betriebssystems nicht wie sonst üblich in C geschrieben waren, sondern in einer hauseigenen Version von Pascal. Zu den besonders gelungenen und eleganten Funktionen gehörte die Netzwerkimplementierung. Man konnte praktisch alles, was lokal möglich war, auch auf das Netzwerk auslagern und verteilen. Eine Funktionalität, die in dieser Transparenz für die damalige Zeit einmalig ist. Legendär ist auch die gelungene und zeitsparende Systemadministration.\n\nDie Workstations hatten eine eigene Hardware und CPUs, die nicht von Intel stammten. Daher waren zum Erlangen der PC-Kompatibilität Zusätze notwendig. Es gab vergleichsweise langsame Software Emulatoren (vergl. Virtualisierung), aber auch schnelle hardwarebasierte Zusatzkarten mit i286- oder i386-CPU.\n\n"}
{"id": "2330132", "url": "https://de.wikipedia.org/wiki?curid=2330132", "title": "Classmate PC", "text": "Classmate PC\n\nDer Classmate PC ist ein von Intel produzierter Laptop, der ursprünglich nur in Entwicklungsländern erhältlich sein sollte. Er war in verschiedenen Konfigurationsvarianten ab 299 Euro (dritte Generation) erhältlich und wurde als Intels Antwort auf den 100-Dollar-Laptop der gemeinnützigen Gesellschaft One Laptop per Child entwickelt. \nDie neue (vierte) Generation des Classmate-PC wurde preislich angepasst und war zum Verkaufsstart für 429 Euro erhältlich. Gerade durch seine Robustheit (50–70 cm Höhe fallsicher auf den Boden) soll der Laptop Einzug in die Klassenräume bei Grundschulen nehmen. \nDurch die höhere Auflösung der neuen, vierten Generation wird das Gerät aber nun auch für weiterführende Schulen und Privatpersonen oder Firmen interessanter.\n\nDer konkrete Einsatz des Classmate PC und die Weiterentwicklung desselben stellt jedoch ganz klar die Ansprüche aller Kinder auf der ganzen Welt in den Mittelpunkt und wird so ein idealer Begleiter für die Schule. \n\nDie erste Generation enthielt folgende Komponenten:\n\nIn der Presse wird darauf hingewiesen, dass der Intel-Laptop in infrastrukturschwachen Gebieten durch Anschluss an eine Autobatterie betrieben werden muss. Eine Kurbel zur Stromgenerierung wird nicht beigefügt, da ein Nutzer den höheren Strombedarf des Intel-Laptops durch eine Handkurbel nicht mehr befriedigen kann.\n\nIn der dritten Generation sind folgende Komponenten enthalten:\n\nDie vierte Generation wurde nun mit folgenden Punkten verbessert:\n\nDie fünfte Generation wurde auf der IDF 2011 vorgestellt\n\nDer Classmate PC sollte ursprünglich nur in den Entwicklungsländern zum Kauf angeboten werden. Die ersten 800 Exemplare für Tests erreichten Ende März 2007 Brasiliens Schulen, wobei sich Brasilien noch nicht entschieden hat, ob es den Classmate PC oder den XO kaufen möchte. Ende September 2008 bestellte der Präsident Venezuelas, Hugo Chavez, eine Million Classmate PCs zur Verteilung in den Schulen des Landes. Libyen hat 150.000 Geräte bestellt.\n\nDie zweite Generation an Classmate PCs sind auch in Europa und den USA erhältlich. Seit dem Herbst 2008 werden Classmate PCs von verschiedenen Herstellern auch in deutscher Ausführung angeboten.\n\nDer Convertible Classmate PC der dritten Generation ist besonders für den Unterrichtsgebrauch im Klassenverband gedacht und soll das Lernen modernisieren. Ab Juli 2009 war die dritte Generation auf dem deutschen Markt erhältlich.\n\nDie \"Vierte Generation\" ist seit dem dritten Quartal 2010 erhältlich. Hier ist u. a. Bildschirmgröße, SSD-Laufwerk sowie Arbeitsspeicher vergrößert worden. Durch das nun optional erhältliche UMTS-Modul erfreut sich dieser nicht nur bei Schülern reger Beliebtheit, sondern ist auch für Firmen und Außendienstmitarbeiter interessant geworden. Hier könnte er eine günstige Alternative zum iPad aus dem Hause Apple werden.\n\n"}
{"id": "2331078", "url": "https://de.wikipedia.org/wiki?curid=2331078", "title": "Bedeutung-Text-Modell", "text": "Bedeutung-Text-Modell\n\nDas Bedeutung-Text-Modell (früher z. T. auch als Inhalt-Text-Modell bezeichnet) ist ein Sprachmodell, dessen Entwicklung in den 1960er Jahren von einer Gruppe Moskauer Sprachwissenschaftler um I.A. Meľčuk, A.K. Žolkovskij, J.D. Apresjan u. a. begonnen wurde. Ein Modell, das die menschliche Sprache nachbildet, indem es einen Text (in geschriebener oder gesprochener Form) in die ihm entsprechende Bedeutung überträgt und umgekehrt. Das Modell besteht aus zwei Komponenten, einer Grammatik, nämlich einer speziellen dependenzorientierten Transformationsgrammatik, und einem speziellen Wörterbuch.\n\nDieses Sprachmodell basiert auf der Untersuchung vieler verschiedener Sprachen und ist deswegen prinzipiell zur Beschreibung beliebiger natürlicher, d. h. menschlicher Sprachen geeignet.\nIm Bedeutung⇔Text-Modell (BTM bzw. auch MST von Modell ‘Smysl⇔Tekst’ aus dem russ. модель «Смысл⇔Текст» bzw. engl. MTT von ‘Meaning⇔Text’ Theory) sind Wörterbuch und Grammatik eng aufeinander abgestimmt bezüglich der in ihnen enthaltenen Informationen und bezüglich der verwendeten formalen Sprachen. Die wesentlichen Bestandteile dieses Modells sind also eine besondere Grammatik und ein ebenso spezielles Wörterbuch: Die Grammatik fungiert dabei als Translator, der in mehreren Etappen gegebene Texte einer natürlichen Sprache in die diesen Texten entsprechenden Bedeutungen (russ. \"cмыcл\") und umgekehrt überträgt. Im Lexikon finden sich u. a. Informationen über die Bedeutung und die Kombinierbarkeit von Lexemen.\nDas Modell dient dementsprechend zur Translation in zwei Richtungen:\n1. von der Bedeutung zum Text: zur Synthese von Sprache bzw. zur Produktion von Texten, wodurch die menschliche Fähigkeit zu Sprechen modelliert wird.\n2. vom Text zur Bedeutung: zur Analyse von Texten, womit die Fähigkeit des Menschen zum Verstehen von sprachlichen Äußerungen bzw. von geschriebenen Texten modelliert wird.\n\nWichtige Gesichtspunkte des BTM sind die folgenden: 1. der Ansatz des BTM, eine wirklich systematische Beschreibung von natürlichen Sprachen zu liefern; 2. die so genannten Lexikalischen Funktionen, die in allen natürlichen Sprachen anwendbar sind; 3. das Rektionsmodell (systematische Darstellung der Valenzen eines Wortes); 4. die Semantische Metasprache, die im BTM zur Bedeutungserklärung von Wörtern und auch komplexeren Ausdrücken verwendet wird; 5. die Berührungspunkte des BTM mit dem Gebiet der maschinellen Übersetzung.\n\nErste Arbeiten zur Entwicklung des BTM wurden in den 1960er Jahren von einer Gruppe Moskauer Sprachwissenschaftler um Igor’ Aleksandrovič Meľčuk, Aleksandr Konstantinovič Žolkovskij und Jurij Derenikovič Apresjan (später Moskauer Semantische Schule, russ. Московская семантическая школа bzw. Московская школа семантики, engl. Moscow Semantic Circle) geleistet. Im Jahr 1974 erschienen die beiden grundlegenden Monographien von Meľčuk und von Apresjan, die eine recht umfassende Beschreibung des BTM bieten. Modifikationen, die an dem Modell vorgenommen wurden, bzw. Modifikationsvorschläge wurden wiederholt in neueren Publikationen beschrieben.\n\nIm deutschsprachigen Raum sind in erster Linie folgende Namen im Zusammenhang mit dem BTM zu nennen: Tilmann Reuther beschäftigt sich seit Ende der 1970er Jahre mit dem Wörterbuch im BTM, speziell mit bestimmten Lexikalischen Funktionen. Daniel Weiss setzt sich kritisch mit der Theorie des BTM auseinander. Leo Wanner benutzt Lexikalische Funktionen für die automatische Übersetzung und hat u. a. einen Sammelband „Recent Trends in Meaning-Text Theory“ herausgegeben. Klaus Hartenstein beschäftigte sich in den 1980er Jahren ebenfalls vor allem speziell mit den Lexikalischen Funktionen nicht zuletzt hinsichtlich ihrer Nutzung für den Fremdsprachenunterricht, aber auch mit theoretischen Fragen im BTM. Hartenstein und Peter Schmidt publizierten 1983 eine umfangreiche, kommentierte Bibliographie zum BTM.\n\nDie Aktualität des BTM zeigt sich nicht zuletzt darin, dass seit dem Jahr 2003 im Zwei-Jahres-Abstand internationale Konferenzen zum Thema BTM stattfinden. Die erste in Paris, dann in Moskau (2005), Klagenfurt (2007), Montreal (2009), Barcelona (2011) und zuletzt im August 2013 in Prag.\n\nEine praktische Anwendung des BTM stellt u. a. die Realisierung des maschinellen Übersetzungssystems ETAP dar. Dieses System wird seit 1978 zunächst im Moskauer Institut „Informelektro“ und dann am Institut für Probleme der Informationsübertragung an der Akademie der Wissenschaften der UdSSR (später Russische Akademie der Wissenschaften) entwickelt und übersetzt vom Englischen ins Russische und umgekehrt.\n\nDas BTM findet seit 2014 auch in selbst-erklärenden Systemen eine Anwendung. Hier werden unter Verwendung von Methoden der Künstlichen Intelligenz automatische Dekompositionen aufgebaut und somit teile eines Erklärend-Kombinatorisches Wörterbuch automatisch erstellt.\n\nDas Wörterbuch des BTM, das Erklärend-Kombinatorisches Wörterbuch (EKW, russ. \"тoлкoвo-кoмбинaтopный cлoвapь\"), bietet wesentlich umfangreichere Informationen als ein konventionelles Wörterbuch. Diese Informationen sind im EKW auf bis zu zehn übersichtlich strukturierte Zonen für jeden Wörterbucheintrag verteilt. Einige dieser Zonen sind auch aus konventionellen Wörterbüchern bekannt, andere jedoch vermitteln Informationen, die in gewöhnlichen Wörterbüchern nicht zu finden sind.\n\nDrei dieser Zonen sind besonders wichtig: (a) die Bedeutungsexplikation, die mithilfe der Semantischen Metasprache eine exakte Erklärung des Stichwortes liefert, (b) das Rektionsmodell, das alle relevanten Informationen zu den semantischen und syntaktischen Valenzen des Stichwortes, sowie zu dessen syntaktischen Eigenschaften enthält und (c) eine Zone mit der Auflistung aller Lexikalischen Funktionen des Stichworts, wodurch die entscheidenden Angaben zu allen Möglichkeiten der Verbindung des Stichworts mit anderen Wörtern ergänzt werden.\n\nDie Grammatik im BTM ist zum einen eine Dependenzgrammatik Tesnière’scher Prägung, zum anderen eine Transformations- bzw. Translationsgrammatik, die den Übergang von der Bedeutung zum Text und umgekehrt modelliert. Um diesen Übergang vollziehen zu können, bedient sich das Modell verschiedener Zwischenebenen. Im Einzelnen werden im BTM die folgenden vier Hauptebenen verwendet, wobei die erstgenannte Ebene, die Semantische Darstellung, der Bedeutung entspricht, die letztgenannte, die Phonologische bzw. Orthographische Darstellung, dem Text:\n\nDie drei letztgenannten Darstellungsebenen werden dabei in jeweils zwei Unterebenen, nämlich eine Oberflächen- und eine Tiefen-Ebene unterteilt.\n\n\n\n\n\n\n\n\n\n"}
{"id": "2333762", "url": "https://de.wikipedia.org/wiki?curid=2333762", "title": "Microsoft Silverlight", "text": "Microsoft Silverlight\n\nMicrosoft Silverlight ist ein Browser-Plug-in, das die Ausführung von Rich Internet Applications ermöglicht. Außerdem wird Silverlight als Framework für Apps für Windows Phone 7 verwendet. Silverlight ist ein proprietäres, programmierbares Plug-in für die Betriebssysteme Windows und macOS, das für die Webbrowser Internet Explorer und Safari angeboten wird. Silverlight basiert auf einer reduzierten Version des .NET Frameworks.\n\nNeben der offenen W3C-Webplattform Ajax konkurriert Silverlight mit OpenLaszlo, Adobe Flash/Adobe Flex und JavaFX.\n\nMicrosoft wird Silverlight 5 bis Oktober 2021 unterstützen.\n\nSilverlight-Anwendungen werden vom Webserver heruntergeladen und auf dem Client typischerweise im Browser ausgeführt. Die Kommunikation der Anwendung mit dem Webserver erfolgt mittels HTTP-GET. Für die Programmierung mittels .NET eignen sich unter anderem die ADO.NET Data Services, die Datenbanken automatisch als Webdienst für einen Silverlight-basierten RIA-Client bereitstellen können.\n\nSilverlight ist hinsichtlich seiner UI-Präsentationsschicht abgeleitet aus der Windows Presentation Foundation. WPF wurde mit dem .NET-Framework 3.x (3.0/3.5) eingeführt. Hauptbestandteil der vektorbasierten Grafikdarstellung und der Gestaltung von Anwendungsoberflächen ist das universelle und textbasierte XML-Format XAML (\"eXtensible Application Markup Language\"). Während WPF für die grafische Darstellung und Animationen von Windows-Desktop-Anwendungen entwickelt wurde, ist unter dem Codenamen WPF/E (\"E\" für \"Everywhere\") eine webfähige Variante entwickelt worden, die mit einem um Elemente und Funktionen reduzierten XAML ausgestattet ist.\n\nSilverlight 1.0 wurde am 5. September 2007 freigegeben. Bis April 2007 war es unter dem vorläufigen Codenamen „WPF/E“ bekannt (die Abkürzung stand für „Windows Presentation Foundation/Everywhere“). Silverlight 1 ist mit einer JavaScript-API ausgestattet und lässt sich mit JavaScript, aber auch mit anderen Scriptsprachen wie Python und Ruby entwickeln. Silverlight 1 besteht aus dem Kern des Presentation Frameworks, der verantwortlich für das UI (User Interface), Interaktivität und Benutzereingaben, grundlegende Bedienelemente, Grafiken und Animation, Medienwiedergabe, Digitale Rechteverwaltung (DRM) und DOM-Integration ist. Es ist gegliedert in folgende Komponenten:\n\n\nEine Silverlight-Anwendung startet mit dem Aufrufen des Silverlight-Controllers von der HTML-Seite, der dann eine XAML-Datei lädt. Die XAML-Datei enthält ein Canvas-Objekt, das als Platzhalter für andere Objekte dient. Silverlight stellt verschiedene geometrische Grundformen zur Verfügung, aber auch Elemente wie Text, Bilder und andere Medien. Die Elemente können exakt positioniert werden, um das gewünschte Layout zu erreichen. Diese Elemente können animiert werden, indem sogenannte Event Trigger verwendet werden. Einige Effekte sind vordefiniert, andere können als Komposition dieser entworfen werden. Tastatur- und Mausereignisse können ebenfalls von gewöhnlichen Skripten verarbeitet werden.\n\nDas 2008 freigegebene Silverlight 2 enthält eine Vielzahl von aus WPF-Anwendungen bekannten XAML-Controls. Zusätzlich zum Canvas, das in der Version 1.1 als einziges Layout-Panel zu Verfügung stand, werden nun weitere Layout-Controls unterstützt, das StackPanel und das Grid, die als Container für andere Controls dienen und die Positionierung und Größenanpassung der Elemente erleichtern.\nAuch komplexe Funktionalitäten, wie \"Data Binding\", die Verwendung von Vorlagen \"(Templates)\", benutzerdefinierte Steuerelemente und Steuerelemente zur Datenmanipulation und Datenvisualisierung (ListBox, DataGrid) sind dazugekommen. Silverlight 2 unterstützt darüber hinaus Klassen zur Internetkommunikation über REST, POX, RSS und WS, und domainübergreifende Netzwerkzugriffe. Die Windows Communication Foundation (WCF), Ajax und LINQ sind integriert.\n\nSilverlight enthält einen Mediaplayer, der die Formate Windows Media Video (WMV), VC-1 Video Standard, HDV, Windows Media Audio (WMA) und MP3 unterstützt.\n\nSilverlight-Anwendungen werden in der Regel mit Microsoft Expression Blend und Visual Studio entwickelt. Es ist geplant, Silverlight auch für mobile Endgeräten mit z. B. Windows Mobile zu veröffentlichen. Die Plattform unterstützt eine Deep-Zooming-Technologie für hochauflösende Bilder, und mit dem \"Microsoft Streaming Server\" ist es möglich, Videos und Silverlight-Anwendungen als Streams anzubieten.\n\nSeit Anfang Juli 2009 bietet Microsoft Silverlight 3 für Windows und macOS zum Herunterladen an. Neu ist die Unterstützung von Hardware-Beschleunigung bei Videos sowie die Möglichkeit, Anwendungen für den Desktop sowie für das Internet zu entwickeln.\n\n\nAm 13. April 2010 wurde Microsoft Silverlight 4 auf der Microsoft-Entwicklerkonferenz vorgestellt. Zu den Neuerungen gehört ein Drucksystem mit Druckvorschau. Neu sind außerdem die \"WCF RIA Services\", die Netzwerkfunktionen beinhalten, um mehrschichtige Anwendungen zu erstellen. Eine Unterstützung für Webcams und Mikrofone und damit die lokale Aufzeichnung von Audio und Video wurde ebenfalls integriert. Silverlight 4 soll 3 mal schneller sein als der Vorgänger.\n\nIm Juli 2010 meldete Microsoft selbst eine Verbreitungsquote des Silverlight-Plugins von 60 %, mit einem Sprung der Verbreitungsquote von 45 % auf 60 % in nur vier Monaten. Damit sehen Beobachter die „kritische Masse“ erreicht, die Silverlight wohl schneller als allgemein erwartet zum „vollwertigen“ Konkurrenten von Adobes Flash machen dürfte.\n\nAm 9. Dezember 2011 wurde die Version 5 veröffentlicht. Dabei wurde die Ausführungsgeschwindigkeit verbessert. Als Neuerungen wurde die vollständige 64-Bit-Unterstützung sowie die Darstellung von H.264-Daten mithilfe der GPU bekanntgegeben. Außerdem werden die Netzwerkfunktionen in einen separaten Thread ausgelagert, was das Stocken von Anwendungen verhindern soll. Die Integration fremder Webinhalte per HTML5 wird mit Silverlight 5 ebenfalls einfacher. Zudem wurde Microsofts „Trusted Application“-Modell eingeführt, womit es mit einem Zertifikat versehenen Anwendungen möglich ist, bestimmte Aktionen direkt vom Browser aus durchzuführen. Voraussetzung ist neben dem Zertifikat ein passender Eintrag in der Registry. Außerdem bietet Silverlight 5 hardwarebeschleunigte 3D-Unterstützung. Die Unterstützung wurde bis 2021 zugesagt.\n\nEs wird für mehrere, aber nicht alle Betriebssysteme und Webbrowser angeboten.\n\nMicrosofts Entwicklung von Silverlight wurde für die Konzentration auf dessen Betriebssystem Windows kritisiert. Obwohl Microsoft mit Apple und Novell im Rahmen des Mono-Projektes zusammenarbeitet, das auch die alternative Silverlight-Implementation Moonlight beinhaltet, wurden vom Flash-Entwickler Adobe die Anstrengungen Microsofts, Silverlight plattformunabhängig anzubieten, infrage gestellt.\n\nÜberdies gibt es Vorbehalte gegen Microsoft, was die Einhaltung und Nutzung von Web-Standards angeht, da Silverlight beispielsweise nicht den vom World Wide Web Consortium empfohlenen Standard SVG für Vektorgrafiken nutzt. Microsoft hat es vorgezogen, eine eigene, strukturell mit SVG kompatible Implementierung in XAML einzubauen. Laut Aussagen eines Microsoft MVPs sei es jedoch nicht möglich gewesen, SVG zu nutzen, sonst hätte Microsoft SVG als erstes um GUI-Elemente erweitert, und damit wäre es nicht mehr SVG gewesen.\n\nIn Silverlight 4.0 gibt es neben der kontrollierten Ausführung in einer Sandbox optional noch die Möglichkeit einer expliziten lokalen Installation (das steht nur für Out-of-Browser-Ausführung bereit). Wenn der Benutzer diese Installation akzeptiert, werden die Einschränkungen der Sandbox verringert, und es wird möglich sein, COM-Objekte zu verwenden – etwa für die Integration mit Desktop-Anwendungen. Die betroffenen Teile solcher lokal installierten Silverlight-Anwendungen wären dann nicht mehr plattformunabhängig.\n\n\n\nVideos und Tutorials\n"}
{"id": "2338457", "url": "https://de.wikipedia.org/wiki?curid=2338457", "title": "Zarafa (Software)", "text": "Zarafa (Software)\n\nZarafa war eine Groupware-Server-Software für Linux, ähnlich wie Microsoft Exchange Server. Seit 1. Mai 2017 wird Zarafa nicht mehr unterstützt. Der offizielle Fork Kopano führt Zarafa mit veränderter Funktionalität weiter und wird auf Subscriptionsbasis vertrieben.\n\nZarafa ist in einer funktionell eingeschränkten Version (keine Outlook-Clients, einige Enterprise-Merkmale fehlen) als freie Software unter den Bedingungen von Version 3 der GNU Affero General Public License (AGPL) veröffentlicht. Zusätzlich ist eine nicht komplett quelloffene Free Edition erhältlich, die Verbindungen über einen eigenen Outlook-Client („closed source“) für bis zu drei Benutzer ermöglicht. Zarafa ist Mitglied der Open Source Business Alliance, unterstützt die OpenMAPI-Initiative und wird im Rahmen der Deutschen Wolke als SaaS angeboten. Im Januar 2015 gab Zarafa bekannt, dass der Support für Outlook als nativen MAPI32-Treiber ab April 2016 nicht mehr angeboten wird und an dessen Stelle Microsoft Outlook 2013 (und höher) via ActiveSync angebunden werden kann (wobei aber nicht alle bisher verfügbaren Funktionen zur Verfügung stehen). Seit 1. Mai 2017 wird Zarafa nicht mehr unterstützt. Der offizielle Fork Kopano führt Zarafa mit veränderter Funktionalität weiter und wird auf Subscriptionsbasis vertrieben.\n\nZarafa ist von MySQL abhängig, aber auf keinen Mail Transfer Agent oder Webserver festgelegt.\n\nDie für Linux-Server geschriebene Software hat eine MAPI-Anbindung, bildet die Funktionen von Microsoft-Exchange-Servern nach und ermöglicht so, Benutzern von Microsoft Outlook oder anderen geeigneten Clients, gemeinsame Kalender, Kontakte, Notizen und E-Mails zu verwalten. Diese Anbindung wurde bis April 2016 unterstützt. Dabei werden vorhandene Mail-Services wie z. B. \"postfix\" benutzt.\n\nZarafa erweitert zusätzlich PHP um ein Modul zum Aufrufen von MAPI-Befehlen direkt aus PHP-Skripten heraus. Darüber bietet es für den Apache-Webserver einen eigenen Webclient, der in der Bedienung Outlook ähnelt.\n\nMit Hilfe der freien Eigenentwicklung \"z-push\" unterstützt Zarafa Mobilgeräte wie Smartphones und PDAs. Es besteht aus einer Implementierung von ActiveSync over-the-air (OTA), die unter anderem die Kommunikation mit Windows Mobile sowie mit Mobiltelefonen von Nokia und Ericsson, Apple (z. B. iPhone/iPad/iPod) und Android basierenden Mobilgeräten ermöglicht. Außerdem können diese Endgeräte damit Push-Mail über das ActiveSync-Protokoll empfangen.\n\nFür die Integration von Google Maps oder Alfresco existieren z. B. eigene Zarafa-Plugins.\nÜber Z-Merge ist es möglich, weitere Software an Zarafa anzubinden (z. B. SugarCRM).\n\nEs stehen verschiedene Editionen zur Verfügung. Die Quellcode-Basis ist für alle Editionen identisch.\n\n\nZarafa ist ursprünglich ein niederländisches Produkt. Die Plochinger Neuberger & Hughes GmbH mit langjähriger Erfahrung im Linux-Groupware-Markt kooperiert seit Herbst 2006 mit den Niederländern und besetzt bei der deutschen Tochtergesellschaft, der Zarafa Deutschland GmbH, auch die Position eines von zwei Geschäftsführern. Im Oktober 2007 wurde die kommerzielle Unterstützung ihres eigenen Groupware-Servers Exchange4linux zugunsten von Zarafa eingestellt. Ab April 2016 sieht Zarafa keine Outlook-Integration via MAPI-Connector mehr vor.\n\n\n"}
{"id": "2351536", "url": "https://de.wikipedia.org/wiki?curid=2351536", "title": "Zeichengenerator", "text": "Zeichengenerator\n\nEin Zeichengenerator ist derjenige Teil eines Computers oder eines anderen digital gesteuerten Ausgabegeräts, der die Umsetzung von einem Zeichencode − z. B. nach ASCII − in ein für den Menschen lesbares Zeichen (Glyph) erledigt. Dazu braucht man in der Regel sowohl Hard- als auch Software.\n\nAufbauend auf solchen Zeichengeneratoren lassen sich dann Terminals oder Grafikbildschirme und damit Schriftgeneratoren ansteuern, aber auch Schriftzüge per Plotter oder andere mechanische Systeme zu Papier bringen.\n\nDie häufigste Ausführung eines ausschließlich Text darstellenden Bildschirms (siehe Textmodus) verwendet ein Zeichengenerator-ROM, das für jeden Zeichencode eine Matrix fester Größe enthält, wobei jedes gesetzte Bit einem (hell) darzustellenden Pixel entspricht. Zeichenauflösungen von 5×7 (minimal) bis 8×8 (typisch) und sogar 16×16 (selten) kommen hier vor.\n\nDie Pixeldaten dieses ROMs werden entweder von einem entsprechenden Videobaustein wie dem 6845 verwendet oder von einer eigenen, aus diskreten Logikgattern aufgebauten Videologik, wie z. B. im PET 2001 oder dem Apple II.\n\nUm Zeichen beispielsweise in einer 8×8-Punktmatrix darzustellen, muss diese Videologik für eine Textzeile also 8 Einzelzeilen darstellen. Für jede Einzelzeile und in der Zeile für jede Zeichenposition wird zunächst aus dem Videospeicher der Zeichencode geholt, dieser auf den Eingang des Zeichengenerator-ROMs gegeben, zusammen mit dem Zahlencode der Einzelzeile (0–7, gezählt innerhalb eines Zeichens), dessen Ausgang dann in diesem Fall die nächsten 8 Pixel nebeneinander ergeben, die endlich von einem Schieberegister in der Pixelfrequenz auf den Videoausgang gegeben werden.\n\nEinen Sonderweg bestritt man in den Frühzeiten der EDV. Dort arbeitete man im Videospeicher teilweise noch nicht mit RAM-Bausteinen, sondern mit aus heutiger Sicht exotischen Lösungen wie Laufzeitleitungen als Kurzzeitspeicher. Ähnlich wie später beim Zeilenspeicher der frühen PAL-Fernsehgeräte wurden die Daten dabei sequentiell per Ultraschall in eine spiralförmig aufgewickelte Glasleitung gespeist und vom Ausgang wieder verstärkt auf den Eingang zurückgeführt. Konkret wurde diese Technik von der Firma ICL bei ihren Terminals bis in die 1970er Jahre verwendet. Das Problem dabei ist nun, dass man ein Zeichen jeweils nur einmal während eines Bildes auslesen kann und nicht wie bei der Matrixdarstellung nötig achtmal für jede Einzelzeile nacheinander. Das führte dazu, dass ICL in diesen Terminals die Ablenkelektronik aufwendig so ausführen musste, dass der Elektronenstrahl an jeder Zeichenposition zusätzlich ein kleines 8×8-Raster aufbaute, so dass alle Pixel eines Zeichens in einem Zug ausgegeben werden konnten.\n\nWie bei einer älteren Schreibmaschine hatte bei so einer Technik jedes Zeichen seine feste Position in einem Rechteckraster auf der Anzeige. Die Abstände der Zeichen untereinander waren konstant (engl. \"mono spaced\", siehe nichtproportionale Schriftart).\n\nBei späteren Entwicklungen konnten jedem einzelnen Zeichen auf dem Schirm individuelle Attribute beigefügt werden, z. B. Inversdarstellung, verschiedene Helligkeitsstufen oder sogar verschiedene Farben.\n\nDamit man auf so einem Textterminal mit gewissen Einschränkungen auch Grafiken wie z. B. mathematische Kurven oder Spielfiguren und -umgebungen darstellen konnte, stellte der Zeichensatz auch oft Blockgrafik-Zeichen zur Verfügung.\n\nWenn der Anzeigeteil auf die Darstellung beliebiger Bitmuster ausgelegt ist, gewinnt man auch für die Textdarstellung zusätzliche Freiheiten. Insbesondere ermöglicht dies die Darstellung von Proportionalschrift inklusive Kerning, die Variierung der Schriftgröße und der Schriftart selbst (\"Font\"). Bei zusätzlicher Ausnutzung von Graustufen kann man Antialiasing realisieren. Auch farbige oder grafisch kreativ gestaltete Schriftzüge werden möglich.\n\nFür die Darstellung beliebig geformter Schriftzeichen haben sich mehrere Standards entwickelt, die die Zeichen typischerweise stückweise aus Bézierkurven zusammensetzen. Beispiele sind der TrueType-Standard von Microsoft und die Postscript-Schriften von Adobe. Diese Standards legen zusätzlich das Dateiformat fest, in dem diese Schriften vertrieben und von der Festplatte in den Speicher geladen werden.\n\nDiese Darstellungsmöglichkeiten erfordern umfängliche Unterstützung durch Betriebssystemteile. Sie können also erst benutzt werden, wenn das Betriebssystem komplett geladen ist. Bis dahin verwenden PCs daher in ihrem Boot-Vorgang bis heute reine Textdarstellungen wie im vorherigen Kapitel, die komplett aus dem ROM erzeugt werden können.\n\nBei Plottern und ähnlichen mechanischen Ausgabegeräten wird ein einzelnes Zeichen nicht durch eine Pixelmatrix dargestellt, sondern aus Linien zusammengesetzt. Der Zeichengenerator besteht dann zwar weiterhin zuerst einmal aus einem ROM, in diesem sind jedoch keine Pixelbits gespeichert, sondern die Punktkoordinaten der Elementarlinien einer Zeichendarstellung, plus Bits, die z. B. anzeigen, ob der Stift am Ende dieser Einzellinie abgehoben (und erst am Anfang der nächsten wieder abgesenkt) werden muss und ob dies die letzte Linie dieses Zeichens war. Die Punktkoordinaten sind dabei wiederum in einem Grundraster von z. B. 16×16 Punkten angeordnet.\n\nDie Firmware des Plotters setzt die aus dem ROM gelesenen Linienkoordinaten in Fahrbefehle für Linien um. Dabei kann sie noch weitere Aufbereitungen vornehmen, wie eine einstellbare Skalierung der Daten, also der Zeichengröße (wozu nur die Anzahl der Elementarschritte für eine Linienrasterweite des Zeichenrasters gesetzt werden braucht), oder Drehungen der Schrift in jede Richtung.\n\nEine höhere Softwareebene sorgt dann dafür, dass die Buchstaben eines Wortes auch korrekt nebeneinander gesetzt werden. Wie bei Grafikbildschirmen können hier zusätzliche Effekte implementiert werden wie Anordnung der Buchstaben längs einer Kurve oder kontinuierliche Änderungen der Zeichengrößen.\n\nIn Einzelfällen werden Zeichen nicht nur aus geraden Linienstücken, sondern zusätzlich auch aus Kreis- bzw. Ellipsenstücken oder Bézierkurven zusammengesetzt, was die Darstellung natürlicher macht.\n\n"}
{"id": "2352160", "url": "https://de.wikipedia.org/wiki?curid=2352160", "title": "Intertec Superbrain", "text": "Intertec Superbrain\n\nDer Intertec Superbrain war ein Mikrocomputer, der von der US-amerikanischen Firma Intertec Data Systems ab dem Jahre 1979 verkauft wurde. Der Rechner wurde mit dem Betriebssystem CP/M 2.2 gesteuert und hatte zwei parallele Z80 Prozessoren eingebaut. Der Master Prozessor (4 MHz) für den Rechner, der zweite für den Disk Controller. Der Speicher war mit 64 KByte dynamisch, 1 KByte statisch ausgestattet. er besaß eine QWERTY Schreibmaschinentastatur mit 80 Tasten und abgesetzter Ziffernblock. Als Schnittstellen waren zwei RS232, ein 40-poliger Z80-Bus und ein S-100-Adapter vorhanden. Von dem Rechner gab es verschiedene Modelle: Superbrain II, Superbrain II Jr., \"QD\" (quad density disk drives) und \"SD\" (super density).\n\n"}
{"id": "2354634", "url": "https://de.wikipedia.org/wiki?curid=2354634", "title": "HTCondor", "text": "HTCondor\n\nHTCondor (ehemals Condor) ist eine sogenannte Batch-Software, die in großen Rechnernetzen zum Einsatz kommt. Condor wird eingesetzt, um Rechenzeit für numerische Berechnungen zu gewinnen, indem Arbeitsplatzrechner zu einem Condor-Pool zusammengefügt werden.\n\nCondor wird seit 1988 an der University of Wisconsin in Madison (USA) entwickelt und stetig weiter verbessert. Zu Beginn wurde diese Software nur für Unix entwickelt, mittlerweile lässt sich Condor jedoch ebenfalls auf Linux, Solaris, Mac OS und Windows installieren. Es existieren jedoch einige Features von Condor, die nur unter UNIX/Linux zur Verfügung stehen.\n\nVon Condor sind immer zwei Versionen verfügbar, der sogenannte stable release und der development release. Der stable release ist ausreichend getestet und sollte daher in echten Condor-Pools installiert werden, im development release hingegen findet man neue Features, die aber noch nicht ausreichend getestet wurden. Der development release sollte nur in kleinen Test-Pools installiert werden.\n\nEin Computer bzw. Server des Condor-Pools muss die Arbeit des Central Managers übernehmen. Dieser Central Manager hat die Aufgabe den Pool zu verwalten. Die Clients bilden den eigentlichen Condor-Pool. Die Clients haben zwei Aufgaben. Zum einen sollen sie die Jobs bearbeiten und zum anderen soll man von ihnen aus Jobs abschicken können.\n\nCondor kann so konfiguriert werden, dass es nur dann Jobs ausführt, wenn der Benutzer über einen bestimmten Zeitraum keine Eingaben durchgeführt hat, der Rechner also unbenutzt ist. Sobald wieder Maus- oder Tastatureingaben erfolgen, wird ein laufender Job angehalten und an einen eventuell freien Rechner übergeben.\n\n"}
{"id": "2363943", "url": "https://de.wikipedia.org/wiki?curid=2363943", "title": "TOPCASED", "text": "TOPCASED\n\nTOPCASED (\"\"; deutsch \"Quelloffene Werkzeugsammlung für die Entwicklung kritischer Anwendungen und Systeme\") ist ein CAE-Werkzeug.\n\nEs nutzt als IDE die Infrastruktur der Entwicklungsplattform Eclipse zur Anforderungserfassung, Anforderungsanalyse, Modellierung, Simulation, Implementierung, Test, Validierung, zum Reverse Engineering und Projektmanagement komplexer sicherheitskritischer Echtzeitsysteme aus Hard- und Software im Bereich Luft- und Raumfahrttechnik, dem Fahrzeugbau (Automotive) und ähnlich anspruchsvoller technischer Systeme in der Kategorie Systems Engineering.\n\nZunächst wird als standardisierte Methode und Sprache für die Modellierung von Software und anderen Systemen die Unified Modeling Language (UML) der Object Management Group (OMG) unterstützt; später sollen weitere modellgetriebene (MDE / MDA) Modellierungsmethoden und -sprachen hinzukommen, so zum Beispiel die Eclipse-eigene Metamodellierungssprache \"Ecore\", die Systems Modeling Language (SysML) und die \"Architecture Analysis and Design Language\" (AADL), eine Sprache zur Beschreibung der System- und Software-Architektur von Echtzeitsystemen. Andere Modellierungsmethoden sollen sich einfach integrieren lassen. Darüber hinaus soll die CAE-Werkzeugsammlung (IDE) mit Simulatoren, Tools zum Testen der Modelle, Werkzeuge für das Konfigurations-, Änderungs- und Anforderungsmanagement sowie Code-, Test- und Dokumentationsgeneratoren ergänzt werden.\n\nTOPCASED wird nicht mehr weiterentwickelt. Stattdessen wird Papyrus (UML) vom gleichen Hersteller empfohlen.\n\n"}
{"id": "2366255", "url": "https://de.wikipedia.org/wiki?curid=2366255", "title": "Know-how-Computer", "text": "Know-how-Computer\n\nDer Know-how-Computer oder WDR-Papiercomputer ist ein Lehrmittel, das die Arbeitsweise eines Computers mithilfe von Streichhölzern auf vorgedruckten Feldern auf einem Bogen Papier darstellt. Damit können Interessierte lernen, wie ein Computer arbeitet, ohne einen elektronischen Computer zur Verfügung zu haben; zum Zeitpunkt der Veröffentlichung war der Besitz eines Computers nicht so selbstverständlich wie heutzutage. Somit diente dieser „Computer“ als pädagogische Hilfe im Bereich der Informatik. Der Know-how-Computer wurde von Wolfgang Back und Ulrich Rohde entwickelt und in der Fernsehsendung \"Know How Computerclub\" (später \"WDR Computerclub\") im Jahre 1983 erstmals vorgestellt.\n\nEr wurde gleichfalls in den Zeitschriften \"mc\" und \"pc magazin\" veröffentlicht.\n\nDer „Computer“ arbeitet auf Papier; die vorgedruckten Felder repräsentieren einzelne Register, während der Registerinhalt durch die Streichhölzer dargestellt wird. Nur fünf Befehle reichen aus, um daraufaufbauend alle mathematischen Funktionen darstellen zu können. Dieser Übungscomputer auf Papier wurde in über 400.000 Exemplaren verschickt und gehörte damals zu den Computern mit der weitesten Verbreitung. Eine Implementierung als Computerprogramm ist auf Wolfgang Backs Website erhältlich.\n\nDie Arbeitsweise ist an Registermaschinen angelehnt (der Artikel der \"mc\" bezieht sich auf Elmar Cohors-Fresenborg), entspricht aber mehr dem Ansatz von Shepherdson und Sturgis.\n\nEine abgeleitete Version wird als „Know How Computer“ in Namibia im Schulunterricht verwendet.\n\n"}
{"id": "2370076", "url": "https://de.wikipedia.org/wiki?curid=2370076", "title": "Benutzerkontensteuerung", "text": "Benutzerkontensteuerung\n\nDie Benutzerkontensteuerung wurde mit Microsoft Windows Vista eingeführt. Der im Englischen als \"User Account Control (UAC)\" bezeichnete Sicherheitsmechanismus ist dafür verantwortlich, dass bei der Ausführung von administrativen Aufgaben eine Rechteerhöhung des Nutzers erforderlich ist. Eingeführt wurde sie, da die meisten Heimanwender bis einschließlich Microsoft Windows XP als Administratoren arbeiteten und damit eine erhöhte Anfälligkeit für Viren und andere Angriffe aufwiesen.\n\nWenn sich mit aktiver UAC ein Administrator am System anmeldet, so arbeitet er mit normalen Benutzerrechten. Sobald eine Anwendung administrative Berechtigungen für die Ausführung benötigt, wird ein Dialogfeld angezeigt, welches extra zu bestätigen ist, damit diese Anwendung mit Adminrechten ausgeführt werden kann.\n\nUnter UNIX-Systemen existiert mit dem Kommandozeilenbefehl \"sudo\" ein Mechanismus, der „vergleichbar, aber gleichzeitig auch sehr verschieden“ ist. Die Einstellungen der Benutzerkontensteuerung können unter den lokalen Sicherheitsrichtlinien den Wünschen des Benutzers angepasst werden. Hier gibt es auch die Möglichkeit, UNIX-ähnlich das Anmeldepasswort zur Rechteerhöhung einzugeben (Bild rechts, Windows 10 Technical Preview).\n\nEin weit verbreitetes Missverständnis ist, dass die Benutzerkontensteuerung nur ein Bestätigungsklick sei; das dahinter stehende Prinzip der Rechteerhöhung durch Identitätswechsel wird nicht verstanden. Ferner ist die UAC die Grundlage und Voraussetzung für das Sandboxing von Programmen und Verzeichnissen unter Windows. Sie ermöglicht die Vergabe von Privilegien an Prozesse, und sie isoliert Prozesse und Fenster, die auf demselben Desktop mit unterschiedlichen Rechten laufen, voneinander.\n\nVersionen von Windows, die älter als Windows NT sind, beziehungsweise nicht davon abstammen (beispielsweise Windows 3.1, Windows 95, 98 und ME), waren Einzelbenutzersysteme, in denen jeder Benutzer die volle Systemkontrolle besaß. Windowssysteme der NT-Linie sind dagegen Mehrbenutzersysteme, so dass verschiedene Benutzerrollen und -rechte vergeben werden können.\n\nUnter Windows XP erhalten die bei der Installation angelegten Benutzerkonten Administratorrechte. Dies führte dazu, dass viele mit Windows XP ausgestattete Einzel-PCs standardmäßig mit einem Benutzer betrieben wurden, der über volle Administratorrechte verfügten. Dadurch wird jede Software, auch Schadsoftware, mit Administratorrechten gestartet, so dass diese vollständigen Zugriff auf das System besitzt. In vielen älteren Anwendungen wurden eingeschränkte Benutzerrechte nicht berücksichtigt, obwohl Microsoft dies in den mit Windows 95 erstmals veröffentlichten \"Designed for Windows\"-Richtlinien als Minimalanforderung festlegte. Installiert oder startet man solche Software mit eingeschränkten Rechten, treten Fehler auf oder die Software arbeitet nicht ordnungsgemäß. Dazu kam die eingeschränkte Benutzerfreundlichkeit: Um in Windows XP den Kalender durch klicken auf die Uhrzeit abrufen zu können, sind Administratorrechte erforderlich. Diese Probleme wurden früher oft dadurch umgangen, dass an Einbenutzerrechnern stets mit Administratorrechten gearbeitet wurde.\n\nUnix-Systeme (wie macOS oder z. B. Ubuntu) wurden von Anbeginn als Mehrbenutzersysteme konzipiert. Jeder angemeldete Benutzer hat ein Heimatverzeichnis für seine persönlichen Daten, wo er nach Belieben editieren kann. Änderungen ausserhalb des Benutzerkontos können im Allgemeinen nur vom Root-Konto durchgeführt werden. Dieses ist bei manchen Unix-Derivaten wie Ubuntu standardmäßig nicht freigeschaltet. Die Mitglieder der Gruppe „Sudoers“ können aber administrative Aufgaben mit dem Befehl sudo ausführen, was dann im Rechtekontext des Benutzers root geschieht. Um zu verhindern, dass Schadsoftware sudo anwendet, muss zur Ausführung des Befehls das Anmeldepasswort zur Authentifizierung eingegeben werden.\nSonderfall bzgl. unixoider Systeme ist Android; hier wird das Benutzersystem nicht für die Benutzerverwaltung verwendet, sondern für die Abschottung der Apps gegeneinander: Für jede App wird bei ihrer Installation ein eigenes „Nutzerkonto“ eingerichtet, anschließend wird die App (als einzige) in dessen 'home-Verzeichnis' installiert. Die eigentliche Verwaltung des (einen) echten Benutzers geschieht gesondert; dadurch ist Android im Grunde ein Einbenutzer-System. Dieser eine (echte) Benutzer hat im Allgemeinen keine Rechte, die einem Admin-User entsprechen würden – dazu muss Android „gerootet“ werden.\n\nMit Windows Vista, welches erstmals Microsofts Security Development Lifecycle anwendete, wurde mit der Benutzerkontensteuerung ein vergleichbarer Mechanismus eingeführt, um das Prinzip des \"least user access\" oder \"least-privileged user account (LUA)\" umzusetzen. Mitglieder der Gruppe Administratoren erhalten beim Anmelden zwei Token: Einen als Administrator, und einen als Standardnutzer, dem alle administrativen Rechte und Privilegien entzogen sind. Diese Nutzer werden als „Geschützte Administratoren“ (Protected Administrators, PA) bezeichnet. Während die Sudoers unter Unix den “großen Bruder” mit sudo anrufen, haben die Geschützten Administratoren eine “gespaltene Persönlichkeit”, welche sie je nach Aufgabe wechseln. Um eine Manipulation der Rechteerhöhung durch Schadsoftware zu vermeiden, findet diese standardmäßig auf dem sicheren Desktop statt.\n\nDas Endszenario ist in beiden Fällen das Gleiche: Die meisten Benutzer sind nur mit Standardnutzerrechten unterwegs, und können nur in bestimmten Bereichen des Rechners editieren. Die Verwalter des Systems, also die Mitglieder der Gruppe \"Sudoers\" oder \"Administratoren\" arbeiten ebenfalls mit eingeschränkten Rechten, können diese aber bei Bedarf erhöhen. In beiden Fällen ist das Prinzip des \"Least-privileged User Account (LUA)\" umgesetzt: Alle arbeiten nur mit den Rechten, die sie für diese Aufgabe auch wirklich benötigen. Interessant ist dazu das Buch \"Windows Vista Security: Securing Vista Against Malicious Attacks\" von Roger A. Grimes (u. a. CISSP, MCSE: Security, MVP) und Jesper M. Johansson (Senior Security Strategist bei der Security Technology Unit von Microsoft). Auf drei Seiten beschreiben sie detailliert die Unterschiede und Gemeinsamkeiten von sudo und UAC, deren spezifischen Vor- und Nachteile, und warum sich Microsoft gegen die Einführung eines sudo-Befehls entschieden hat.\n\nIn die Benutzerkontensteuerung wurde auch der Befehl Runas implementiert. Der Befehl kann dazu genutzt werden, Verwaltungs- bzw. Administrationsaufgaben durchzuführen, ohne dass sich ein Benutzer mit Administrationsrechten komplett neu anmeldet. Dazu muss das Konto und das Passwort eines Mitgliedes der Gruppe Administratoren eingegeben werden. Diese UAC-Abfrage wird als „Eingabeaufforderung für erhöhte Rechte für Standardbenutzer“ (Over-the-shoulder, OTS) bezeichnet, während die Rechteerhöhung von Geschützten Administratoren als „Administratorbestätigungsmodus“ (Admin Approval Mode, AAM) bezeichnet wird.\n\nNeben einer Zugriffssteuerungsliste (ACL) und den Privilegien, die zur feineren Rechtevergabe (oder -verbot) auch schon unter XP vorhanden waren, kam ab Windows NT 6.0 (Vista) noch der „Integrity Level“ hinzu. Im Deutschen wurde dies etwas unglücklich mit „Verbindlichkeitsstufe“ übersetzt, aber auch „Integritätsebenen“ ist ein geläufiger Begriff. Die Verbindlichkeitsstufe ist ein Sicherheitsmechanismus der Vorrang vor der Zugriffssteuerungsliste hat, also Zugriffe auch dann verhindert, wenn sie die Zugriffssteuerungsliste erlauben würde. Dazu bekommt jeder Prozess in seinem Access Token einen sogenannten Integrity Level (IL) verpasst, der ausdrücken soll, wie vertrauenswürdig er ist. Die hohe Verbindlichkeitsstufe bildet zusammen mit den administrativen Privilegien die beiden Teile des “administrativen Schlüsselbundes”, neben der Zugriffssteuerungsliste.\n\nJedes Objekt im System befindet sich auf einer von fünf Stufen, gekennzeichnet durch ein Label in seinem Security Descriptor. Die Grundidee der Integrity Levels (IL) ist es, dass Prozesse, die auf einer niedrigeren Stufe laufen, Objekte mit höherer Stufe nicht beschreiben können (No Write-up). So kann ein Prozess mit niedriger Verbindlichkeitsstufe den auf „Medium“ stehenden Benutzerdaten nichts anhaben und schon gar nicht den noch höher eingestuften Systemkomponenten. Zugriffe von unten nach oben sind also beschränkt, während auf gleicher Ebene oder von oben nach unten alles erlaubt ist – im Rahmen der Zugriffssteuerungsliste. Die fünf Integritätsebenen sind:\n\n\nDie Hauptziele der Mandatory Integrity Control (MIC) sind die Trennung von Standardnutzerprozessen von Prozessen mit erhöhten Rechten, weswegen auch das Component Object Model die Integritätsebenen beachtet. Ferner wird durch die Verbindlichkeitsstufen ein Schreibschutz im Wurzelverzeichnis des Rechners gewährleistet, während andererseits Anwendungen wie der Internet Explorer nur beschränkte Änderungsmöglichkeiten von Nutzerdaten und -profil haben. Während im Allgemeinen bei Objekten nur „No Write-up“ gilt, setzt das Prozessmanagement im NT-6-Kernel „No Read-up“ und „No Write-up“ bei laufenden Prozessen, um eine Manipulation der höhergestellten Prozesse zu verhindern. Es ist lediglich SYNCHRONIZE, PROCESS_QUERY_LIMITED_INFORMATION und PROCESS_TERMINATE möglich.\n\nNormalerweise bekommt jede Anwendung die Rechte des Prozesses von dem sie gestartet wurde. Das würde aber bedeuten, dass wenn ein Anwender den Internet Explorer startet, dieser auch mit mittlerer Verbindlichkeitsstufe laufen würde. Um das zu verhindern, gibt es im Access Token eines Accounts den Eintrag TOKEN_MANDATORY_POLICY_NEW_PROCESS_MIN, welcher bei Benutzeraccounts gesetzt und bei administrativen Accounts nicht gesetzt ist. Ist dieser Eintrag gesetzt, bewirkt er, dass Prozesse, welche gestartet werden, keine höheres Integrity Level bekommen können als der EXE-Datei zugewiesen wurde. Da dem iexplorer.exe nur das IL „Low“ zugewiesen ist, wird diese Datei bei normalen Benutzern auch nur in dieser Stufe gestartet. Wenn der Administrator UAC abschaltet, laufen alle seine Prozesse mit hoher Verbindlichkeitsstufe, da jedem Prozess das administrative Token zugewiesen wird. Alle Dokumente und Dateien welcher dieser Administrator erzeugt besitzen dann die Integritätsebene „hoch“. Wird nun die Benutzerkontensteuerung wieder aktiviert, bekommt jeder von ihm angeklickte Prozess/Ordner nur das Standardnutzer-Token (IL „mittel“) zu sehen, weswegen er diese Dateien ohne Admin-Rechte nicht mehr öffnen kann.\n\nDie Verbindlichkeitsstufe eines Ordners gilt entweder nur für diesen selbst (object inherit, OI), oder für das komplette Verzeichnis (container inherit, CI). Wird eine Datei z. B. in codice_1 oder ein Unterverzeichnis desselben geschoben, erhält diese die niedrige Verbindlichkeitsstufe, egal welche sie vorher hatte (Abstufung vorausgesetzt).\n\nDie administrativen Privilegien sind der zweite Teil des “Schlüsselbundes”. Unter NT 6.0 wurden die Probleme, die sich bei der Arbeit mit einem Standardnutzeraccount ergaben, entschärft, indem neue Privilegien hinzukamen und manche Aufgaben nicht mehr administrativ sind. Unter XP wurde nicht zwischen Zeitzone und Systemzeit unterschieden, obwohl nur letztere für die Sicherheit relevant ist. Ab Vista gibt es deshalb die Unterscheidung zwischen dem Privileg die Systemzeit zu ändern (SeSystemTimePrivilege) und dem Privileg die Zeitzone zu ändern (SeTimeZonePrivilege). Ferner können drahtlose Internetverbindungen und Energieoptionen des Rechners ohne Adminrechte konfiguriert werden. Die Installation von kritischen Windows-Updates ist nun ebenfalls als Standardnutzer möglich. In Firmennetzen können auch Treiber und ActiveX-Elemente von bestimmten Seiten installiert werden, wenn dies durch die Administratoren in den Gruppenrichtlinien freigegeben wurde. Die Privilegien der einzelnen Gruppen können unter \"Start > secpol.msc > Lokale Richtlinien > Zuweisen von Benutzerrechten\" angesehen oder geändert werden. Die folgende Liste enthält nicht alle Privilegien und Gruppen von NT-6.X-Systemen. Sie dient nur der Veranschauung der Unterschiede zwischen Administratoren und Benutzern.\n\nDie Benutzerkontensteuerung besteht aus dem Application Information Service (AIS), der UAC-Abfrage selbst mit dem sicheren Desktop, der User Interface Privilege Isolation (UIPI), der Installationserkennung und der Anwendungs- /Datenvirtualisierung. Obwohl sich der Login-Prozess von Administratoren äußerlich nicht von dem unter XP unterscheidet, erkennt die Local Security Authority (lsass.exe) bei der Anmeldung eines Mitgliedes der Gruppe Administratoren dies und kreiert zwei Acces Token: Ein User-Token und ein Admin-Token. Der User-Token wird nun zum Starten der Windows-Shell verwendet. Explorer.exe ist wiederum der Vaterprozess, von dem alle anderen Prozesse innerhalb der Shell ihren Access Token vererbt bekommen. Alle Anwendungen laufen so mit Userrechten, wie wenn sich ein Standardnutzer anmelden würde. Wird nun eine Anwendung ausgeführt welche Administratorrechte benötigt, startet der Application Information Service (AIS) eine UAC-Abfrage. Bei Verweigerung wird die Anwendung nicht gestartet, bei Zustimmung wird diese mit dem Admin-Token ausgeführt. Wird diese erhöhte Anwendung beendet, wird auch der Prozess mit erhöhten Rechten beendet.\n\nEine UAC-Abfrage wird entweder provoziert wenn ein Programm in seinem XML-Manifest erhöhte Rechte anfordert, oder wenn die Installationserkennung zuschlägt. Diese verwendet eine Heuristik die Installationsroutinen erkennt, da die typischen Verzeichnisse (codice_4, codice_5) nur von Administratoren beschrieben werden können. Auf dieselbe Weise werden Updateroutinen und Deinstallationsroutinen erkannt. Die Heuristik arbeitet nur bei 32-Bit-Programmen, wenn diese keine manuelle Rechteerhöhung durch requestedExecutionLevel anfordern, und wenn LUA (Standardnutzer/Geschützter Administrator) aktiv ist. Die Heuristik sucht nach Schlüsselwörtern wie \"install,\" \"setup,\" \"update,\" Schlüsselworte wie Anbieter, Firmenname, Produktname, Dateibeschreibung und Namen. Fernern wird nach Schlüsselwörtern im side-by-side Manifest, und in den StringTables der ausführbaren Datei gesucht, ebenso bestimmte Bytesequenzen und bestimmte Eigenschaften in RC Data. Fordert eine Anwendung Adminrechte an, läuft folgender Vorgang ab: Der Befehl ShellExecute(BeispielApp.exe) wird an AIS (codice_6) gesendet. AIS, welche innerhalb von svchost.exe läuft, startet Consent.exe (codice_7). Consent.exe macht einen Screenshot und wendet einen Abdunklungseffekt auf die Bitmap an. Anschließend wird auf einen Virtuellen Desktop gewechselt der nur dem Benutzer Lokales System (SYSTEM) gehört, die Bitmap als Bildschirmhintergrund eingefügt und die Abfragebox der Benutzerkontensteuerung eingeblendet. Dieser Vorgang wird als Sicherer Desktop bezeichnet und verhindert, das Schadware Einfluss auf die Entscheidung nehmen kann.\n\nWenn das abfragende Programm von Microsoft digital signiert wurde und das Image im Windows-Systemverzeichnis liegt, ist die Kopfzeile der Box blau. Grau bedeutet, dass das Programm digital signiert wurde, aber nicht von Microsoft stammt. Gelb steht für nicht signierte Anwendungen/Programme, und rot erscheint bei blockierten Anwendungen. Im Prompt erscheint das Icon, die Beschreibung, der Dateiname und der Publisher wenn signiert. Dies ist als Hürde für Schadsoftware gedacht, um das Vortäuschen von seröser Software zu erschweren. Unter „Details“ kann die Kommandozeile eingeblendet werden, welche zur Rechteerhöhung weitergegeben wird. Bei blauen UAC-Abfragen kann hier auch das Zertifikat angesehen werden. Drückt der Nutzer „Nein“/„Abbrechen“, schickt Windows einen access-denied Fehler an den Prozess, der die Anfrage stellte. Wenn der Benutzer zustimmt indem er auf „Ja“ drückt oder ein administratives Passwort eingibt, ruft AIS CreateProcessAsUser(BeispielApp.exe) auf, um den Prozess mit Administrator-Identität zu starten. Weil der Prozess technisch gesehen von AIS gestartet wurde, wird ein Feature in der CreateProcessAsUser-API genutzt, welches es erlaubt, die Prozess-ID des Vaters auf die desjenigen zu setzen, welcher das Programm (und damit die Anfrage) ursprünglich startete. Deshalb erscheinen Prozesse die mit erhöhten Rechten laufen nicht als Kindprozesse von AIS Service Hosting.\n\nIn der Praxis könnte Schadware die UAC-Abfrage nachbilden, was im AAM aber keine Rolle spielt, da ein Klick auf „Ja“ keine Rechteerhöhung zur Folge hätte. Problematischer sind Passworteingaben, da dies durch Trojaner (Keylogger) abgegriffen werden kann. Microsoft empfahl deshalb bei OTS eine Secure Attention Sequence anzufordern, bzw. diese Art der Rechteerhöhungen generell abzublocken. Versucht ein Programm ohne eine Rechteerhöhung anzufordern in administrative Pfade zu schreiben, werden Verzeichnisse und Registry virtualisiert. Dabei wird eine Kopie derselben vom Programm beschrieben, wobei diese im Nutzerprofil unter codice_8 sowie codice_9 abgelegt wird, sodass jeder Nutzer eine eigene Kopie erhält. Dies soll vor allem älteren Anwendungen das ungestörte Ausführen ermöglichen. Bei 64-Bit Anwendungen ist dies nicht möglich, ebenso, wenn eine UAC-Abfrage negativ beantwortet wurde.\n\nEinige Anwendungen laufen mit erhöhten Rechten im selben Desktop wie niedriger gestufte Anwendungen. Wird von einem Nutzer ein Prozess mit erhöhten Rechten ausgeführt (per OTS oder AAM), läuft dieser Prozess in einem anderen Account. Die tiefer laufenden Prozesse können dadurch keinen Code in den höher liegenden Prozess schreiben. Allerdings könnten die tiefer liegenden Anwendungen den höher Laufenden Fake-Input senden, um diese zu kompromittieren. Das Sandboxing durch die Integritätsebenen soll dies verhindern, indem tieferliegende Prozesse in ihren Rechten gegenüber Höheren beschränkt werden. Dies wird als User Interface Privilege Isolation (UIPI) bezeichnet. Prozesse können nur Prozesse mit gleicher oder niedrigerer Verbindlichkeitsstufe zum Schreiben öffnen. Um den Zugriff auf Geheimnisse im Arbeitsspeicher zu verhindern, haben Prozesse mit niedrigerem IL keinen Lesezugriff auf Prozesse mit höherer Verbindlichkeitsstufe. Tiefergestellte Prozesse können dadurch keine \"window handle validation\" auf einen höheren Prozess ausführen. Die Befehle \"SendMessage\" oder \"PostMessage\" zu höheren Prozessen bekommen von der API Erfolgsmeldungen zurückgeschickt, während die Befehle im stillen verworfen werden. Thread Hooks und Journal Hooks gegen Prozesse mit höherer Verbindlichkeitsstufe sind ebenso wie DLL-Injection nicht möglich.\n\nDie UAC-Abfrage lässt sich über \"Start > Systemsteuerung > Benutzerkonten und Jugendschutz > Einstellungen der Benutzerkontensteuerung ändern\" den persönlichen Vorlieben anpassen. Die Auswahlmöglichkeiten dort beschränken sich aber auf einen Schieberegler, wo zwischen unterster Stufe (Win7: UAC abgeschaltet; ab Win8: UAC erhöht ohne Nachfrage), zwei mittleren Stufen (UAC an, mit Whitelist für Windows-Programme) und der höchsten Stufe (UAC an, ohne Whitelist für Windows-Programme) gewählt werden kann. Die beiden obersten Stufen aktivieren dabei den Sicheren Desktop; standardmäßig ist die zweithöchste Stufe gewählt.\n\nDen Vollzugriff auf die Einstellungsmöglichkeiten der Benutzerkontensteuerung gibt es nur in den Lokalen Sicherheitsrichtlinien (secpol.msc) unter \"Start > secpol.msc > Lokale Richtlinien > Sicherheitsoptionen\". Käufer einer preisgünstigen Windows-Variante wie Home Basic und Home Premium haben keine grafische Benutzeroberfläche wie gpedit.msc und secpol.msc, und müssen die Einstellungen deshalb in der Registrierungsdatenbank (regedit.exe) unter \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\" vornehmen. Die Einstellungsmöglichkeiten sind deshalb nachfolgend auch mit Registrierungs-Werten in Klammern angegeben. Die Standardeinstellungen in Windows NT 6.1 und höher sind fett gedruckt.\n\nInteressant ist, dass der Administratorgenehmigungsmodus auch die Möglichkeit hat, Unix-typisch das Anmeldepasswort zur Rechteerhöhung einzugeben, wie dies bei \"sudo\" der Fall ist. Dabei kann zwischen dem normalen (ConsentPromptBehaviorAdmin = 3) und dem Sicheren Desktop (ConsentPromptBehaviorAdmin = 1) gewählt werden. Interessant ist auch die Option \"ValidateAdminCodeSignatures\", bei der nur Programme Adminrechte erlangen können, die digital signiert und überprüft sind. Bei Windows 8 und später ist dies nicht mehr nötig, da die Funktion im SmartScreen-Filter aufgeht, der Programme nur starten lässt, die diese Anforderungen erfüllen und/oder Reputation besitzen. Alle Eingabehilfen (UIAccess) müssen bereits ab Vista eine PKI-Signatur aufweisen, sonst werden sie nicht akzeptiert. Die “sicheren Verzeichnisse” (EnableSecureUIAPaths) können nur von Administratoren beschrieben werden. Die Rechteerhöhung für Standardbenutzer hatte ursprünglich keinen Sicheren Desktop (wird in älterer Literatur nicht aufgeführt), der Zahlensprung von 1 auf 3 mag dies verdeutlichen.\n\nFür Paranoiker gibt es noch die Möglichkeit, zur Rechteerhöhung die Secure Attention Sequence (SAS) anzufordern. Dazu muss unter \"Start > gpedit.msc > Administrative Vorlagen > Windows-Komponenten > Benutzerschnittstelle für Anmeldeinformationen\" die Option „Vertrauenswürdiger Pfad für Anmeldeinformationseintrag erforderlich“ aktiviert werden. Die Rechteerhöhung behindert den Arbeitsablauf massiv, soll aber das Abgreifen von Passwörtern verhindern. Wird eine Anwendung gestartet die Adminrechte anfordert, erscheint erst eine Dialogbox „Windows-Sicherheit“, in der man den Vorgang fortsetzten oder abbrechen kann. Wird fortsetzen gewählt, erfolgt die Aufforderung die Secure Attention Sequence (SAS) auszuführen. Erst danach steht die UAC-Abfrage zur Rechteerhöhung bereit.\n\nLeo Davidson entdeckte während des Beta-Tests von Windows 7, dass etwa 70 Windows-Programme ohne Nachfrage mit vollen Administratorrechten ausgeführt werden, und demonstrierte die damit mögliche Rechteausweitung.\n\nStefan Kanthak veröffentlichte einen Proof of Concept zur Rechteausweitung mittels der Installationserkennung.\n\nStefan Kanthak zeigte einen weiteren Proof of Concept, der die Ausführung beliebigen Codes sowie die Rechteausweitung mittels der von Leo Davidson entdeckten automatischen Rechteerhöhung und DLL Hijacking erlaubt.\n\n\"Vozzie\" zeigte einen anderen Proof of Concept, der die Rechteausweitung durch Einschleusen eines selbsterstellten Manifests mittels der von Leo Davidson entdeckten automatischen Rechteerhöhung erlaubt.\n\nDas Problem bei Windowssystemen war in der Vergangenheit immer, dass diese zwar eine Trennung von Nutzer- und Adminkonten für Firmen und Kindersicherung zuließen, die meisten Rechner (75 %) aber Einzelplatzrechner sind. Da jede Maschine einen Administrator braucht, und die meisten Nutzer auch die volle Kontrolle über das System haben wollen um Änderungen vorzunehmen, gab es praktisch nur Administratoren als Nutzer. Dies hatte auch Auswirkungen auf die Software, die stets davon ausgehen konnte, dass der Nutzer über Adminrechte verfügt, und systemweit geltende Änderungen vornehmen kann. Software die für diese Umgebung geschrieben wurde arbeitet nicht wenn der Anwender nur ein Standardnutzer ist, was damals nur für Firmen und Kindersicherung relevant war. Eine Software die Administratorrechte bekommt, kann aber das System beschädigen, entweder mit Absicht (Schadware) oder unabsichtlich (schlecht programmierte Software). Firmen umgingen das Problem teilweise, indem sie diese Anwender zur Powerusergruppe hinzufügten, die es unter Vista eigentlich nicht mehr gibt.\n\nDie Benutzerkontensteuerung sollte zwei Ziele erreichen: Die Inkompatibilität der Software beseitigen, und dem Nutzer Änderungen am System deutlich machen. Aus diesem Grund wurde der Geschütze Administrator (Protected Admin, PA) eingeführt, welcher standardmäßig das erste Konto auf dem System ist. Durch die Erzeugung zweier Access-Tokens – einen als Standardnutzer, einen als Administrator – wurde das Problem der unbemerkten Änderungen gelöst. Der Weg vom Administrator zum Benutzer war bei Drittsoftware aber ein steiniger, da die meisten Anwendungen beim Erscheinen von Vista nicht für Standardnutzer ausgelegt waren (Wes Miller im Microsoft TechNet, Mai 2008: „Meine Herausforderung an Sie: Fühlen Sie den Schmerz“). Durch Telemetriedaten von Kunden konnte Microsoft die Entwicklung von Drittanbietersoftware im Angesicht der Benutzerkontensteuerung gut mitverfolgen.\n\nDie Zahl der Anwendungen, welche mit Adminrechten lief, konnte drastisch gesenkt werden. Dies wurde von Microsoft positiv aufgenommen, da die Verwundbarkeit des Betriebssystems reduziert wird. Im ersten August (2007) nach dem Release von Vista hatten die Telemetrie-Kunden in 50 % ihrer Sessions (Zeitraum vom Einloggen bis zum Ausloggen, max. 24 Std.) eine UAC-Abfrage. In diesem Zeitraum forderten 775.312 verschiedene Programme Adminrechte an, wobei die Installationen dazugezählt wurden. Hier zeigte sich, dass die meiste Software noch Adminrechte benötigte. Drei Monate später sank die Zahl bereits auf 350.000 pro Monat. Ein Jahr später, im August 2008, waren nur noch 168.149 UAC-Prompts pro Monat. Der Umstand, dass die Neuinstallation und Einrichtung des Rechners mehr UAC-Abfragen erfordert, wurde ebenfalls erkannt. Die Daten des Customer Experience Improvement Program zeigten auch auf, das von Mai 2007 bis Juli 2008 die Zahl der Sessions mit einer oder mehr UAC-Abfragen von 50 % auf etwa 33 % (Vista SP1) fiel.\n\nDamit tat sich aus Sicht von Microsoft das nächste Problem auf: Da Windows selbst ins Betriebssystem eingreifen kann, erzeugte Windows nun etwa 40 % aller UAC-Prompts. Da die Anwendungsprogrammierer ihre Arbeit taten, verschob sich das Verhältnis der UAC-Abfragen. Windows-Komponenten erzeugten 17 der Top 50 UAC-Abfragen in Vista, und 29 der Top 50 in Vista SP1. Mit Vista SP1 wurden kleinere Verbesserungen eingeführt, um die UAC-Abfragen weiter zu reduzieren. Das neue Betriebssystem Windows 7 wurde als Gelegenheit gesehen, tiefergehende Veränderungen durchzuführen, um die Zahl der UAC-Abfragen des Systems weiter zu reduzieren. Es wurde z. B. auch herausgefunden, dass Benutzer die UAC genervt abschalteten, oder die Abfragen einfach nur durchwinkten. Ferner wurde in einer Testumgebung herausgefunden, dass nur 13 % der Teilnehmer sagen konnten, warum der UAC-Dialog erschien. Bei Telemetrienutzern wurde zudem beobachtet, dass 89 % der Prompts in Vista und 91 % der Abfragen in Vista SP1 positiv beantwortet wurden. Es wurde befürchtet, dass die Nutzer den UAC-Dialog aus Gewohnheit abklicken, und bei kritischen Abfragen nicht bewusst entscheiden. Ein informativerer UAC-Dialog, und eine Reduzierung der UAC-Abfragen von Windows selbst wurden angestrebt, damit die Nutzer besser auf die kritischen UAC-Prompts achten können.\n\nMit Windows 7 wurde deshalb die UAC-Whitelist eingeführt. Die Programme, die auf dieser armlangen Liste stehen, erhalten automatisch Adminrechte, ohne UAC-Abfrage. Vereinfacht gesagt sind dies fast alle EXE-Dateien, die unter codice_10 liegen. Nennenswerte Ausnahmen sind mmc.exe und besonders rundll32.exe, da sich sonst virus.dll mit Adminrechten starten lassen könnte. Das Programm codice_11 ist ebenfalls auf der Whitelist. Nachfolgend sind alle Programme aufgeführt, die sich beim Release von Windows 7 auf der Whitelist befanden. Bis auf kleinere Veränderungen wurde die so modifizierte Benutzerkontensteuerung bei allen nachfolgenden Windows-Versionen übernommen. Die Whitelist kann deaktiviert werden, wenn der UAC-Regler auf die höchste Stufe gestellt wird.\n\nWie oben erwähnt, konnten 2008 in einer Testumgebung mit Vista nur 13 % der Teilnehmer sagen, warum der UAC-Dialog erschien. Unter Vista lautete der lapidare Kommentar in der UAC-Box: „Zur Fortsetzung des Vorganges ist Ihre Zustimmung erforderlich“. Ab Windows 7 wurde die Formulierung „Möchten Sie zulassen, dass durch das folgende Programm Änderungen an diesem Computer vorgenommen werden?“ gewählt, die für unerfahrene Nutzer besser geeignet ist. Die fachlich korrekte Frage lautet:\nKonkret sind unter Windows NT 6.0 und höher drei Gründe ausschlaggebend, warum eine UAC-Abfrage eine Rechteerhöhung anfordert: Die Zugriffskontrolle, die Verbindlichkeitsstufen und die Privilegien. Die Zugriffskontrollliste jeder Datei kann angesehen werden, wenn im Windows-Explorer bei einer Datei mit \"Rechtsklick > Eigenschaften > Sicherheit\" die Berechtigungen der einzelnen Nutzer und Gruppen angesehen werden. Der Benutzer SYSTEM (Lokales System) und die Gruppe der Administratoren haben praktisch überall den Vollzugriff, der Nutzer Mustermann darf im Stammverzeichnis des Systems codice_12 (%systemroot%) und den Installationsverzeichnissen codice_13 bzw. codice_14 (%programfiles%) nur lesen und ausführen. Die Integritätsebenen und Privilegien werden in der normalen grafischen Benutzeroberfläche wie dem Taskmanager leider nirgends angezeigt. Der Process Explorer von Microsoft ist dazu praktisch zwingend erforderlich, da hier bei laufenden Prozessen deren Verbindlichkeitsstufe eingeblendet wird, und unter Details auch die Privilegien des Prozesses angezeigt werden können. Folgende Vorgänge benötigen Administratorrechte, und lösen eine UAC-Abfrage aus:\n\n\nEine besondere Rolle nimmt das Verzeichnis %programdata% ein, das standardmäßig unsichtbar geschaltet ist. Hier haben Benutzer Vollzugriff, codice_18 und seine Unterverzeichnisse können aber nur von Administratoren beschrieben werden. Die Unterorder Microsoft Antimalware und Windows Defender können aus Gründen des Selbstschutzes nur von Administratoren, SYSTEM und TrustedInstaller gelesen und beschrieben werden. Änderungen an MSE bzw. Defender erfordern deshalb Adminrechte.\n\nMit dem Aufkommen der Benutzerkontensteuerung war klar, dass Schadware versuchen würde, allein mit Standardnutzerrechten auszukommen. Hier kommen die Verbindlichkeitsstufen ins Bild: Alle administrativen Pfade werden de facto durch Zugriffssteuerungslisten und Privilegen vor dem Zugriff tieferstehender Anwendungen und Nutzer geschützt. Die Verbindlichkeitsstufen betreffen hier nur die laufenden Prozesse, welche durch UIPI (No Read-up, No Write-up) durch Zugriff von unten geschützt sind. Verzeichnisse, welche mit IL „high“ geschützt sind, gibt es standardmäßig nicht. Bei der Entwicklung von Vista war sich Microsoft schon damals des Problems bewusst, dass ein Webbrowser im gleichen Rechtekontext läuft, wie die Gehaltsabrechnung einer Firma. Die Integritätsstufen „low“ und „untrusted“ werden deshalb zum Sandboxing von Anwendungen und ihren Verzeichnissen eingesetzt.\n\nMit dem Internet Explorer 7 in Vista wurde die erste Anwendung geschaffen, die mit niedriger Verbindlichkeitsstufe läuft. Wie bereits oben erwähnt, gibt es auf der Festplatte unter codice_1 ein Verzeichnis mit niedriger Verbindlichkeitsstufe, in das diese Prozesse schreiben können. In der Registry steht mit codice_2 ein äquivalenter Verzeichnispfad zur Verfügung. Damit Daten aus der Sandbox iexplorer.exe in das Benutzerprofil gelangen können, ist ein Broker-Prozess ieuser.exe mit IL „medium“ nötig. Inzwischen wurde das Sandboxing auch von anderer Software übernommen: Beim Chrome läuft der Broker-Prozess chrome.exe mit IL „medium“, die Registerkarten chrome.exe mit „nicht vertrauenswürdiger Verbindlichkeitsstufe“, d. h. ohne Schreibzugriff auf die Festplatte. Der Adobe Reader hat inzwischen einen „Protected Mode“, Microsoft Office 2010 „Protected View“ usw.\n\nWird aus diesen Sandkästen eine Datei mit IL „low“ in ein Verzeichnis mit IL „medium“ per Drag and Drop verschoben, erscheint die „UAC-Abfrage für Arme“ im Bild rechts, da eine Rechteerhöhung von niedriger auf mittlere Verbindlichkeitsstufe stattfinden würde. Dies hat mit der Benutzerkontensteuerung direkt nichts zu tun, da der Standardnutzer selbst IL „mittel“ besitzt, also keine Adminrechte benötigt, um Dateien “auf sein Niveau” zu erhöhen. Die Benutzerkontensteuerung unter Windows Vista und 7 ist insofern relevant, da das Abschalten derselben (oder das Arbeiten mit dem eingebauten Administratorkonto, was denselben Effekt hat) dieses Sandboxing zerstört. TOKEN_MANDATORY_POLICY_NEW_PROCESS_MIN wird nur bei Benutzeraccounts gesetzt und bewirkt, dass Prozesse keinen höheren Integrity Level bekommen können, als wie der Anwendung zugewiesen wurde. Wenn die Benutzerkontensteuerung abgeschaltet wird, bekommen alle Prozesse Adminrechte zugewiesen. Aus diesem Grund ist der Protected Mode des IE abgeschaltet, wenn UAC deaktiviert ist.\n\nUnter Windows 8 ist die Benutzerkontensteuerung nicht abgeschaltet, wenn der UAC-Regler auf unterster Stufe steht. Die Rechteerhöhung auf Wunsch einer Anwendung findet dann geräuschlos statt. Dies ist notwendig, da für die Windows-Apps Sandboxing erzwungen wird. Da immer mehr Windows-Anwendungen auf niedriger Verbindlichkeitsstufe laufen, wurde zudem ein Weg gesucht, damit diese nicht in die „Low“-Verzeichnisse einer anderen Anwendung schreiben können. Deshalb bekommt jede Windows-App (bzw. ihre Dateien und Anwendungen) einen individuellen Security Identifier (S-1-15-2-...) zugewiesen. Diese Kapselung wird als AppContainer bezeichnet. Diese AppContainer werden unter codice_21 abgelegt, das Verzeichnis besitzt niedrige Verbindlichkeitsstufe. Der Internet Explorer ist unter Windows 8 die einzige Anwendung, die auch als Desktop-Programm in einem AppContainer laufen kann („Erweiterter geschützter Modus“). Mit Windows 10 sollen die AppContainer auch auf dem Desktop Einzug halten. In beiden Betriebssystemen kann die Benutzerkontensteuerung nur abgeschaltet werden, wenn in den Lokalen Sicherheitsrichtlinien (secpol.msc) „Alle Administratoren im Administratorbestätigungsmodus ausführen“ auf „Deaktiviert“ gesetzt wird. Wenig überraschend kommt dann, wenn eine Windows-App gestartet werden soll, eine Fehlermeldung mit der Aufforderung, UAC wieder zu aktivieren.\n\nWenn ein Angreifer bei einem Unix-System Zugang zu einem Account bekommt, der codice_22 ohne Einschränkungen und weitere Authentifizierung ausführen kann, hat er praktisch schon gewonnen, da ihm damit Vollzugriffsrechte gewährt werden. Bei Windows Vista und höher ist dies nicht anders. Bereits beim Erscheinen von Vista 2007 stellten Russinovich und Johansson klar, dass die Benutzerkontensteuerung weder zum Nerven, noch als Schutz gegen ausgeklügelte Angriffe zur Rechteausweitung gedacht ist. Damit hatte man nun dasselbe Problem, an dem UNIX seit 20 Jahren arbeitet. Ferner schützt die UAC nicht vollständig höhere Prozesse gegen Manipulation durch niedere Prozesse, da eben kein vollständiges Sandboxing stattfindet, was auch nicht geplant war. Malware mit Nutzerrechten läuft bei Geschützten Administratoren im selben Account wie erhöhte Prozesse. Da viele Anwendungen in das Nutzerprofil schreiben und lesen, kann sich hier eine Lücke ergeben, die zu einer Rechteausweitung führt. Russinovich nannte z. B. das Anhängen von Schadprogrammen an Shell-Extensions in der Registry, um sich früher oder später Adminrechte zu erschleichen. Ferner teilen sich privilegierte Prozesse denselben Namensraum mit Standardnutzerprozessen. Wenn Schadware weiß, wann ein privilegierter Prozess (IL: high/system) auf einen bestimmten Speicherbereich zugreift, könnte sie durch einen Pufferüberlauf Schadcode in den Prozess injizieren. Die einfachste Möglichkeit ist, unter einem wohlklingenden Namen einen UAC-Prompt auszulösen in der Hoffnung, dass unerfahrene Nutzer auf „Ja“ klicken. Wie Jesper M. Johansson im Microsoft TechNet schon 2007 treffend formulierte:\n\nAufgrund dieser Möglichkeiten wurde die Benutzerkontensteuerung von Microsoft auch nicht als Sicherheitsbarriere gesehen. Als Sicherheitsbarriere wird von Microsoft eine Schranke bezeichnet, durch die Code oder Daten nur passieren können, wenn die Sicherheitsrichtlinien es erlauben. Beispielsweise kann ein Standardnutzer die Daten eines anderen Nutzers nicht manipulieren, oder diesen zwingen, bestimmten Code auszuführen. Wenn es doch möglich wäre, wäre dies eine Sicherheitslücke in Windows (oder einem Drittprogramm). Diese strikte Art der Trennung findet bei der Benutzerkontensteuerung nicht statt, als Kompromiss aus Bequemlichkeit und Sicherheit. Deshalb stellen die Integritätsebenen auch keine Sicherheitsbarriere dar, da standardmäßig eben nur No Write-up gilt. Die Benutzerkontensteuerung war in erster Linie eine Bequemlichkeit, die das Wechseln zwischen den Benutzerkonten zur Rechteerhöhung erleichtern sollte. Ihr Kernanliegen ist es, dass alle User nur mit Standardnutzerrechten arbeiten, um das Prinzip des \" least-privileged user account\" (LUA) durchzusetzen.\n\nIm Jahr 2011, also vier Jahre und eine Version der UAC später (Windows 7), sah Microsoft auch einen Nutzen gegen Malware. Die Schadwareschreiber begannen, ihre Software auf Standardnutzerrechte anzupassen. Für die meisten Schadprogramme stellte dies kein Problem dar. Die Umgehung der UAC stellte sich für Malware aber als sehr schwierig heraus. Ein Teil der Schadware ging deshalb dazu über, die Benutzerkontensteuerung zu deaktivieren, um böse UAC-Prompts der Schadware direkt nach dem Neustart zu vermeiden. Genannt wurden Sality-Viren, Alureon-Rootkits, FakePAV, Autostart-Würmer, Banking-Trojaner usw. Für die Änderung der UAC-Einstellungen benötigt die Malware bereits Adminrechte, was durch Exploits, eine Abschaltung der UAC oder einen „Ja“-Klick zur falschen Zeit möglich ist. Microsoft reagierte darauf, indem Microsoft Security Essentials nun darauf achtet, ob Software die UAC-Einstellungen ändert, und nutzt dies als Verhaltenserkennung. Da normale Software immer weniger UAC-Abfragen stellt, wurde die Verhaltenserkennung einfacher. Da 23 % aller infizierten Rechner die Benutzerkontensteuerung deaktiviert hatten, wurden die Nutzer gebeten, diese aktiviert zu lassen. Es wurde nochmals daraufhingewiesen, dass UAC nicht als Virenschutz gedacht ist, aber die Sicherheit des Betriebssystems verbessert.\n\nDie Benutzerkontensteuerung schützt nicht per se vor Malware, sie sorgt nur für eine strikte Trennung zwischen Benutzer- und Administratorrechten. Jedes Mal wenn die Grenze von unten nach oben durchschritten werden soll, erfolgt eine UAC-Abfrage. Der „Schutz“ besteht also darin, dass man bestimmen kann, welches Programm Adminrechte bekommt, und welches nicht. Alle administrativen Einstellungen, die man von Hand am Rechner vornimmt, können auch von einer Software durchgeführt werden; die Frage ist eben bloß, ob man das will. Bei tanzenden Schweinen, Zahlenkolonnen und chinesischen Schriftzeichen, die Administratorrechte möchten, sollte man besser vorsichtig sein. Der UAC-Prompt im Bild rechts stammt z. B. von einer Schadware, die am 30. Dezember 2014 von der \"Malc0de Database\" zu Testzwecken in ein Windows 10 TP heruntergeladen wurde. Die Malware wird ausgeführt, wenn der Benutzer entweder auf „Ja“ klickt oder die Benutzerkontensteuerung abgeschaltet hat. Knapp zwei Tage später, zu Silvester, erkannte der Windows Defender die Zahlenkolonne als Win32/Sality. Der SmartScreen-Filter war dabei zu Testzwecken deaktiviert, da er die Schadware mangels PKI-Zertifikat geblockt hätte.\n\nWie oben erwähnt, ist die Benutzerkontensteuerung nicht zum Schutz gegen ausgeklügelte Angriffe zur Rechteausweitung gedacht. Das Metasploit-Framework bietet verschiedene Möglichkeiten, die UAC zu umgehen, sowie die Möglichkeit, unter wohlklingendem Namen einen UAC-Prompt auszulösen und dreist um Adminrechte zu bitten. Die Varianten ohne UAC-Pop-up arbeiten alle mit DLL Hijacking und DLL-Injection und nutzen unter Windows 7/8/8.1 die automatische Rechteerhöhung durch die UAC-Whitelist (autoElevate) aus. Vista ist dagegen immun. Aus diesem Grund ist es sinnvoll, den UAC-Regler auf die höchste Stufe zu stellen, bzw. mit getrennten Konten zu arbeiten. Jesper M. Johansson stellte 2007 folgende Liste der Best practices auf:\n"}
{"id": "2370498", "url": "https://de.wikipedia.org/wiki?curid=2370498", "title": "Applesoft BASIC", "text": "Applesoft BASIC\n\nApplesoft BASIC (manchmal auch „Applesoft II“ genannt) war die zweite Version der Programmiersprache BASIC für den Apple II Homecomputer, die dem Integer BASIC folgte. Applesoft BASIC wurde von Microsoft entwickelt, und der Produktname entstand aus den beiden Firmennamen Apple und Microsoft. Applesoft BASIC wurde entwickelt von Marc McDonald und Ric Weiland.\n\nApplesoft BASIC kann auf dem frühesten Modell der Apple-II-Serie, dem eigentlichen Apple II, von einem Datenträger ins RAM geladen oder als ROM-Steckkarte hinzugefügt werden. In allen späteren Modellen der Serie – dem Apple II+, Apple IIe, Apple IIc, Apple IIgs und Apple IIc+ – ist es als ROM fest eingebaut und steht damit sofort nach dem Einschalten zur Verfügung. Die Applesoft-Versionen der einzelnen Modelle unterscheiden sich nur minimal, insbesondere unterstützt keine davon die Verwendung von mehr als 48 KB RAM für BASIC-Programme, auch nicht in den Modellen, die von Haus aus wesentlich mehr RAM-Speicher mitbringen.\n\nDie Kunden von Apple wollten eine Version von BASIC, die auch Berechnungen mit Gleitkommazahlen ermöglichte. Da Steve Wozniak, der Entwickler des Apple II und des Integer BASIC, zu beschäftigt war mit der Entwicklung des Diskettenlaufwerks und des dazugehörigen Controllers für den Apple II, gab man die Entwicklung der BASIC-Variante an Microsoft, die auch schon das BASIC für den Altair 8800 und für den Commodore PET entwickelt hatten; Applesoft BASIC war in der Tat im Wesentlichen identisch mit dem Commodore-PET-BASIC, erweitert um einige Apple-spezifische Befehle z. B. für die Grafikansteuerung. Diese Apple-spezifischen Befehle wurden größtenteils direkt aus dem Integer BASIC übernommen. Neu waren allerdings die Befehle zur Ansteuerung des hochauflösenden Grafikmodus.\n\nDie BASIC-Variante für den Apple war vergleichbar mit anderen BASIC-Programmiersprachen für Rechner mit dem 6502 Prozessor, wie z. B. Commodore BASIC: Die Version verwendete Zeilennummern; Leerzeichen zwischen den einzelnen Befehlen in einer Zeile waren nicht notwendig. Anders als beim Commodore BASIC wurden aber beim Abspeichern von Programmzeilen alle unnötigen Leerzeichen automatisch entfernt und beim Auflisten eines Programmes dann automatisch wieder Leerzeichen zwischen den Befehlen eingefügt. Dadurch sank der Speicherbedarf und die Lesbarkeit erhöhte sich.\n\nApple Business BASIC wurde erstmals mit dem Apple III Computer ausgeliefert. Donn Denman portierte Applesoft BASIC auf SOS und überarbeitete es, um die Vorteile des erweiterten Speichers des Apple III zu nutzen.\n"}
{"id": "2378771", "url": "https://de.wikipedia.org/wiki?curid=2378771", "title": "Soulcage Department", "text": "Soulcage Department\n\nThe Soulcage Department ist ein Unternehmen in Bremen, das sich auf Trickfilm und Computeranimationsfilm spezialisiert hat.\n\nEs wurde 2002 von Mike Meyer, Jo Bub, Elmar Keweloh, Martin Ernsting und Wilhelm Landt nach dem Studium an der Hochschule für Künste Bremen gegründet. Seitdem hat es mehrere computeranimierte Filme produziert.\n\n\n"}
{"id": "2382393", "url": "https://de.wikipedia.org/wiki?curid=2382393", "title": "Mc (Zeitschrift)", "text": "Mc (Zeitschrift)\n\nDie Zeitschrift mc war ein von Januar 1981 bis mindestens Oktober 1996 monatlich erscheinendes deutschsprachiges Computermagazin.\n\n„mc“ stand für Mikrocomputer, was sich auch im Untertitel der 1980er-Jahre \"Die Mikrocomputer-Zeitschrift\" widerspiegelte. Sie wurde von Herwig Feichtinger mitgegründet, der zunächst auch Chefredakteur war, und kostete zu dieser Zeit 6,00 bis 6,50 DM. Entstanden war die \"mc\" aus einer festen Rubrik der \"Funkschau\". Zwischenzeitlich wurde der Untertitel geändert in \"Computerpraxis für den technischen Anwender\", schließlich in \"Systemübergreifendes, technisch orientiertes Know-how\". Mit der Ausgabe Juli 1994 erschien sie nur noch als Beilage zur Zeitschrift \"DOS International\", hatte aber immer noch einen Umfang von etwa 45 Seiten. 1992 bis 1993 erschienen insgesamt fünf Sonderhefte mit dem Titel \"WINbox. Spezial-Magazin für Windows-Anwender\".\n\nRedakteur und später Chefredakteur der Zeitschrift \"mc\" war Ulrich Rohde, welcher 1983 auch den WDR Computerclub mitbegründete. Herausgegeben wurde die \"mc\" anfangs vom Franzis-Verlag, ab Juli 1994 in Form einer Heftbeilage vom DMV Verlag als \"mc extra\". \n\nDie mc befasste sich mit Computerthemen, Netzwerken, Hardware, Datenbanken sowie Programmierung. In den letzten Ausgaben gab es die Rubriken „Grundlagen“, „Programmieren“, „Technologie-Report“ und „Test“, in denen Beiträge als Einführungen bzw. Zusammenfassungen zum jeweiligen Thema im Stil wissenschaftlicher Fachblätter geschrieben wurden. Die Redaktion war mit Naturwissenschaftlern besetzt.\n\nEines der bekanntesten Projekte der Zeitschrift war ein auf dem Mikroprozessor 6504 basierender Einplatinencomputer namens EMUF.\n\n"}
{"id": "2389182", "url": "https://de.wikipedia.org/wiki?curid=2389182", "title": "Tomboy (Software)", "text": "Tomboy (Software)\n\nTomboy ist eine Notiz-Software für Desktop-Betriebssysteme. Das Programm wird häufig als „lokales Wiki“ bezeichnet, da es viele der Funktionalitäten eines solchen Systems mit sich bringt, jedoch keinen Webserver erfordert. So lassen sich zum Beispiel die einzelnen Nachrichten leicht über Links verknüpfen bzw. verweisen automatisch auf existierende Notizen. Tomboy ist Freie Software, die in C# geschrieben wurde und GTK# nutzt. Die Software ist Teil der Desktopumgebung Gnome und wurde unter der LGPL veröffentlicht. Tomboy wird häufig genutzt, um persönliche Informationen zu verwalten.\n\nSeit das Mono-Projekt in Version 2.0 zur Verfügung steht, ist Tomboy auch unter Mac OS X lauffähig. Am 19. November 2018 wurde es aus dem Entwicklungszweig von Debian entfernt, weil es von veralteten GNOME-Bibliotheken abhängt und nicht mehr gepflegt wird. Debian ist u. a. die Grundlage für Ubuntu, die meistverbreitete Linux-Distribution.\n\n\nIn Tomboy geschriebene Notizen lassen sich unter anderem mit folgenden Auszeichnungen versehen:\n\nTomboy unterstützt eine Vielzahl an Plug-ins, die meist andere Programme und Services einbinden; z. B.:\n\nDa das Mono-Projekt in Linux-Distributionen zahlreiche Programmbibliotheks-Abhängigkeiten nach sich zieht, was besonders bei dem begrenzten Speicherplatz für CD-Images eine Rolle spielt, und auch Bedenken gegen den Nachbau von patentierter Microsoft-Technologie seitens der Gemeinschaft um Freie Software bestanden, wurde \"Gnote\" entwickelt. Bei dem Programm handelt es sich um eine C++-Portierung von Tomboy, welche die gesamte Funktionalität nachbildet, ohne auf Mono-Bibliotheken angewiesen zu sein. In Fedora 12 ist das Programm in die Standardinstallation aufgenommen.\n\nDaneben existiert seit 2017 Tomboy-ng, ein Rewrite in der Programmiersprache Pascal.\n\n\n"}
{"id": "2391553", "url": "https://de.wikipedia.org/wiki?curid=2391553", "title": "Audiovisualisierung", "text": "Audiovisualisierung\n\nAudiovisualisierung bezeichnet die dynamische Darstellung von Grafiken und Animationen auf der Basis von Audiodaten, wie Musik, Sprache oder Geräuschen. Das Ziel ist einerseits die Verbildlichung von bestimmten Informationen im Material, um sie technisch beurteilbar zu machen und andererseits entwickelt sich die Visualisierung auch zu einer Kunstform, indem Musik optisch untermalt wird.\n\nDie einfachste Variante ist die Darstellungen per Oszilloskop. Im einfachsten Fall wird die Wellenform des Audiosignals über den Zeitverlauf des Audiosignals aufgetragen. Das bedeutet, man lässt den Elektronenstrahl eines Oszilloskops in einer gewissen Frequenz über den Bildschirm laufen und sieht die Wellenform des Audiosignals.\n\nEine weitere Variante ist die Panoramaanzeige (auch Korrelationsanzeige genannt) zwischen dem linken und dem rechten Audiokanal. In diesem Falle steuert das Audiosignal für die linke Seite die horizontale Auslenkung des Elektronenstrahls, während das rechte Audiosignal die vertikale Auslenkung aussteuert (links und rechts können auch vertauscht werden). Diese Art der Audiovisualisierung wird in der Regel in Musikstudios oder in der TV- oder Radiotechnik eingesetzt, um Stereobreite des gehörten Signals optisch beurteilen zu können. An der Wellenformdarstellung kann man bei leisen Signalen den Rauschanteil optisch beurteilen. In der Panoramaanzeige kann man zudem erkennen, wie sich ein Audiosignal im Raum anordnet. Die Darstellung schwankt dabei zwischen einer diagonale Line im ersten Quadraten und einer solchen Linie im 4. Quadranten, was eine völlige Dekorrelation bedeutet. In der Regel ist die Anzeige bei Stereosignalen pulsieren kreisförmig. In dem Lied \"Die Roboter\" der Düsseldorfer Elektro-Band Kraftwerk zeigt die Panoramaanzeige am Anfang des Liedes für kurze Zeit ein Quadrat.\n\nEine weitere häufige Anzeige ist das sogenannte Peakmeter. Es zeigt die Lautstärke in Form eines Balkens an. Der Klassiker, der noch heute in Radiostudios eingesetzt wird ist das Peakmeter von RTW. Auch sehr bekannt sind die Varianten der Firma NTP. Peakmeter werden eingesetzt, um zu beurteilen, ob man innerhalb eines bestimmten Lautstärkebereichs liegt. Gerade im Radio ist es wichtig, den Lautstärkepegel einzuhalten, da große Schwankungen den Zuhörer stören würden. Ein Peakmeter dient den Moderatoren zur Kontrolle, ob die Lautstärke richtig eingestellt ist und ob wirklich Audiosignale zum Sender transportiert werden. Anderenfalls wurde eventuell ein Regler am Mischpult nicht hochgezogen.\n\nWichtig zur Beurteilung ist die sogenannte Korrelation. Falls ein Kanal verdreht ist (linkes und rechtes Audiosignal schwingen genau gegensätzlich), kann sich das Signal beim Zusammenmischen (Umschalten auf Mono, Radios die nur Mono abspielen) komplett aufheben. Das Resultat wäre, das man nichts mehr hört, obwohl ein Signal vorhanden ist. Das menschliche Gehör kann Phasenverschiebungen nicht wahrnehmen. Solche Phasendreher kann man nur optisch beurteilen. In älteren Radiomischpulten wurden die Signale über Operationsverstärker gemischt. Die darin verwendeten Operationsverstärker neigten dazu, die Phase von Audiosignalen zu verschieben. Sogenannte Korrelationsmesser machen solche Phasendreher optisch sichtbar und sind in modernen Peakmetern integriert. Früher wurden Phasendreher mit Oszilloskopen beobachtet, oder mit einem zusätzlichen Peakmeter, der die Summe beider Signale (Mono) anzeigte. Bewegten sich die Balken für die linke und rechte Seite, jedoch der Balken für das Mono-Signal nicht oder nur kaum, so war ein Phasendreher im Signal. Den Moderatoren steht in diesem Fall eine Taste zur Verfügung, die das rechte Audiosignal auf den linken Kanal oder umgekehrt kopiert. Das Audiosignal ist in einem solchen Fall nur „pseudo-Stereo“, kann aber auch auf Mono-Radios gehört werden. Dieser Effekt ist durch moderne hochqualitative Operationsverstärker (zum Beispiel der Firma BurrBrown) kaum noch vorhanden. Seit Audiosignale digital (mittels DSP) gemischt werden, tritt der Effekt nicht mehr auf.\n\nIn ähnlicher Weise können die einzelnen Spektralanteile eines Klanges nach Frequenzen aufgeschlüsselt grafisch dargestellt werden. In Signalverarbeitungssystemen wird dazu oft eine FFT angewendet und die Amplituden der Signalanteile logarithmisch aufgetragen.\n\nDie Audiovisualisierung fasziniert viele Menschen. In den 1970er Jahren wurden Lichtorgeln populär.\n\nIm Jahr 1976 verkaufte die Firma ATARI ein Produkt mit dem Namen \"Atari Video Music System C-240 Mint\". Dieses Gerät wurde an den Fernseher oder die Stereoanlage angeschlossen und stellte bunte Bilder dar, die sich simultan zur Musik bewegten oder ihre Farben änderten. Das Gerät musste zuerst an die Steckdose angeschlossen werden und anschließend an die Stereoanlage, sonst konnte die Stereoanlage einen Defekt davontragen.\n\nFür den legendären Commodore 64, der Mitte bis Ende der 1980er Jahre als Spielkonsole genutzt wurde, gab es eine große Anzahl von Spielen, viele mit Kopierschutz. Cracker knackten den Kopierschutz und hinterließen als Visitenkarte ein sogenanntes Cracktro auf den Datenträgern. Heute sind diese „Cracktros“ unter dem Namen „Demo“ bekannt. Es hat sich eine eigene Szene um diese Demos herum gebildet (Demoszene). Sehr früh schon waren sich bewegende Balken und optische Darstellungen der im Hintergrund laufenden Musik wesentlicher Bestandteil dieser Demos. Um 1985 herum wurde diese Demoszene auf dem Amiga der Firma Commodore fortgeführt. Die Computer der Serie Amiga verfügten über vier Audiokanäle, die dazugehörigen Formate waren SID oder Mod. In sehr vielen Demos, aber auch in Computerspielen wurden die vier Balken (Peakmeter) für jeden der Audiokanäle als optischer Effekt eingesetzt. Um 1987 tauchten vermehrt Disketten mit kleinen Liedersammlungen auf (\"MusicMags\"), wie zum Beispiel von der Gruppe Kaktus. Diese Disketten wurden innerhalb weniger Sekunden vom AMIGA geladen und zeigten ein buntes Universum von kleinen hektischen Grafiken an. Darüber hinaus konnte aus einer Liste von Hunderten von Liedern ein Stück ausgewählt werden. Abhängig von der Musik liefen verschiedene Grußbotschaften an andere Gruppen über den Bildschirm die von den verschiedenen Elementen der Musik verzerrt wurden, oder sich in der Geschwindigkeit veränderten. Diese Liedersammlungen waren die ersten Programme ihrer Art, die die Elemente der Musik auf Grafiken abbildeten und letztlich nur diesem Zweck dienten. Die Darstellung der Musik wurde durch die Grafik nahezu zelebriert.\n\nSeit die Computer schnell genug sind kann mittels der Schnellen Fourier-Transformation das Frequenzspektrum eines Audiosignals in Echtzeit berechnet werden. Dies begann für die PC Plattform mit MS-DOS in den frühen 1990er Jahren, Software die das bewerkstelligte war z. B. der \"IntertiaPlayer\" oder \"CubicPlayer\", welche eine ausreichende Rechenleistung auf der noch schwachen PC-Hardware durch Assemblerprogrammierung erreichten.\n\nBald nach der Entwicklung des Datenformates \"mp3\" wurde im Mai 1997 ein Audioplayer namens Winamp veröffentlicht, mit einem Plugin basierenden Audiovisualisierungskonzept (siehe auch Winamp#Visualisierungs-Plug-ins). Winamp ist einer der ersten kostenlose mp3-Spieler für PCs gewesen, erreicht eine große Verbreitung und prägte in Design, GUI und Look and Feel das Audioplayergenre. Ein bekanntes Audiovisualisierungs-Plugin war das Geissplugin von 1998, welches erstmals eine Fraktalartige Visualisierung auf Basis der Musik generierte.\n\nSeit Winamp sind praktisch alle Audio- und Medienplayer mit einer Audiovisualisierung ausgestattet, z. B. iTunes der Firma Apple. Der Zusammenhang zwischen Musik und Video ist allerdings willkürlich und nicht immer nachvollziehbar realisiert.\n\nAm Freitag, dem 30. November 2001, installierten die Wiener Klangkünstler sha und GTT eine Stahlkonstruktion von 14 riesigen Klangmonolithen am Dornerplatz in Wien. Es war das größte Klangkunstwerk Europas. Diese Monolithen waren mit sogenannten NXT-Flachlautsprechern bestückt. Diese Lautsprecher bringen Platten in Schwingung und übertragen so den Klang. Die 14 riesigen Klangmonolithen sollten nun als Integrationsversuch für städtische Problemzonen fungieren. Der Begriff „Audiovisualisierung“ fällt in diesem Zusammenhang im Metaphorischen Sinne. Die Probleme der Stadt und die Schatten, die auf diese fallen werden ‚audiovisualisiert‘. Sinngemäß wird mit dieser Begriffssemantik die Wirkung von Klang und Musik auf die Wahrnehmung der Menschen bezeichnet. Der Klang, die Musik visualisiert in den Köpfen der Menschen die Problemstellung.\n\nGerade beim Entwurf von Firmenlogos hat sich in den letzten Jahren viel getan. Es wurden Schriften entwickelt und durch Logos erweitert. Diese Kombination wird für ein Unternehmen einmal festgemacht und lebt dann in der Firmengeschichte als Corporate Identity weiter. Viele Designbüros bieten zusätzlich eine „Audiovisualisierung“ der Corporate Identity an. Dies ist die Gestaltung von Erkennungsmelodien, oder Tonfolgen zusätzlich zum Logo. Sehr bekannt sind die fünf Töne der T-Com, die vier Töne, die bei der Intelwerbung stets auftreten oder die Startmelodie des Betriebssystems Windows der Firma Microsoft. Der Konsument wird mit dem Logo und dem Audiologo stets gleichzeitig konfrontiert. So verbindet das Unterbewusstsein des Konsumenten die Tonfolge (das Akustische Logo) mit dem Firmennamen und dem Logo. Heutzutage spricht man daher auch von „Identity-Mix“ oder „Markenkommunikation“, statt nur von Corporate Identity.\n\n\n\n\n\n"}
{"id": "2402367", "url": "https://de.wikipedia.org/wiki?curid=2402367", "title": "Rasterung von Kreisen", "text": "Rasterung von Kreisen\n\nUnter der Rasterung von Kreisen versteht man in der Computergrafik das Zeichnen (Rastern) eines Kreises auf dem Punktraster einer Rastergrafik oder eines Raster-Grafikgeräts durch Einfärben entsprechender Pixel. Es gibt hierfür sowohl Algorithmen zur einfarbigen Rasterung als auch zum Antialiasing.\n\nEine einfache Möglichkeit, Kreise zu zeichnen, basiert auf der Parameterdarstellung von Kreisen:\n\nFür jedes formula_2 zwischen 0 und formula_3 werden in bestimmten Abständen die (\"x\", \"y\")-Koordinaten gemäß dieser Formel berechnet und die entsprechenden Pixel eingefärbt. Kreise mit beliebigem Mittelpunkt können durch eine einfache Koordinatenverschiebung gezeichnet werden. Diese Methode ist sehr ineffizient, da sie die langsamen Cosinus- und Sinusfunktionen verwendet.\n\nEine andere Möglichkeit ist, die Koordinatengleichung des Kreises (formula_4) nach formula_5 aufzulösen:\n\nHier wird für jedes formula_7 zwischen formula_8 und formula_9 formula_5 berechnet. Diese Methode ist wegen der Wurzel ebenfalls ineffizient und lässt zudem für formula_7 nahe formula_9 Lücken.\n\nDie soeben beschriebenen Algorithmen können durch die Nutzung von Symmetrieeigenschaften verbessert werden. Tatsächlich besitzt jedes Pixel auf dem Kreis sieben weitere symmetrische Pixel, die sich trivial berechnen lassen. Es genügt demnach, nur einen Achtelkreis zu zeichnen und anstatt nur einem Pixel folgende acht Pixel einzufärben:\n\nDiese Methode löst außerdem das Problem der Lücken bei der eben beschriebenen Methode.\n\nEine frühe Methode zum Rastern von Kreisen wurde 1969 von Metzger vorgestellt. Hierbei wird ausgehend vom aktuellen Pixel der Koordinaten formula_13 zwischen den beiden Pixeln, die sich außerhalb formula_14 und innerhalb formula_15 des Kreises befinden, gewählt. Wenn formula_16 der Abstand des inneren und formula_17 der Abstand des äußeren Pixels zum Kreismittelpunkt ist, so wird dasjenige Pixel gewählt, dessen Abstand näher am Kreisradius liegt. Beispielsweise wird das äußere Pixel gewählt, falls formula_18.\n\nUnter Anwendung des Satzes des Pythagoras lässt sich letztere Bedingung folgendermaßen umformen:\n\nMit Hilfe der Dreiecksungleichung, die hier für alle formula_20 gültig ist, ergibt sich:\n\nZur Umsetzung der Quadrierungen sind allerdings langsame Multiplikationen erforderlich. Diese würden sich durch die inkrementelle Berechnung der Bedingung vermeiden lassen; Metzger formulierte allerdings keine derartige Lösung.\n\nEin Algorithmus, der nur Additionen und Subtraktionen verwendet, wurde 1976 von Horn vorgestellt. Bei Horns Verfahren befinden sich die einzufärbenden Pixel innerhalb eines ein Pixel breiten Bereichs um den idealen Kreisbogen. Wenn formula_13 das aktuelle Pixel ist, dann wird die Position des direkt darüberliegenden Pixels formula_14 mit dem rechten Rand dieses Bereichs verglichen. Liegt es innerhalb des Bereichs, wird dieses Pixel gewählt. Liegt das Pixel außerhalb, so wird das links liegende Pixel formula_15 gewählt. Auf letzteren Fall lässt sich unter Einführung der Kontrollvariable formula_25 folgendermaßen testen:\nEin inkrementeller Algorithmus ergibt sich durch die Betrachtung der Differenz formula_27 bei beiden möglichen Fällen. Bei jedem Schritt wird formula_25 um formula_29 erhöht; wenn das linke Pixel gewählt wird, subtrahiert man formula_30. Der Anfangswert der Kontrollvariable beträgt formula_31, kann aber für Kreise mit ganzzahligen Mittelpunkten und Radien auf formula_32 gerundet werden.\n\nDer komplette Algorithmus lautet damit:\n\nOptimierung für den Schritt formula_33:\nWenn man die Zeile formula_34 mit der darüberliegenden vertauscht, kann man die Operation formula_35 einsparen.\nWenn zuvor also formula_7 um 1 erniedrigt wurde, ist das formula_37 automatisch schon enthalten.\nVon formula_7 wird in beiden Versionen 1 abgezogen. Trotz Vertauschung der Zeilen bleibt also das Endergebnis für formula_7 gleich.\nFür formula_25 ändert sich jetzt aber etwas: Statt mit dem einfachen formula_7 wird in dieser Zeile mit dem um 1 reduzierten formula_7 gearbeitet, mit formula_43. Von formula_25 wird jetzt formula_45 abgezogen:\nformula_46\nKlammern entfernen, entspricht:\nformula_47\nMan kann also sehen, dass für formula_25 der gleiche Wert, wie in der unoptimierten Version errechnet wird. Damit ist gezeigt, dass das Endergebnis sich algorithmisch nicht von der unoptimierten Version unterscheidet.\n\nOptimiert sieht das dann so aus:\n\n1964 und 1977 stellte Bresenham einen weiteren Algorithmus vor (siehe auch Bresenham-Algorithmus). Ähnlich wie Metzger wählt er Pixel auf der Basis ihrer Entfernung zum Kreismittelpunkt aus. Ein einfacherer, äquivalenter Algorithmus bedient sich der Midpoint-Formulierung, bei der der Mittelpunkt zwischen den beiden nächsten Pixeln betrachtet wird.\n\nDer Midpoint-Algorithmus rastert den Kreisbogen ausgehend vom Pixel mit größter \"y\"-Koordinate. Ausgegangen wird von einer impliziten Form der Koordinatengleichung des Kreises:\nformula_50 ist 0 auf dem Kreis, positiv außerhalb und negativ innerhalb des Kreises. Bei jedem Schritt wird zwischen dem „östlichen“ und dem „südöstlichen“ Pixel gewählt. In diese Gleichung werden die Koordinaten des Mittelpunkts eingesetzt:\nBei formula_52 wird Pixel \"O\" gewählt, im anderen Fall \"SO.\"\n\nAuch hier ist ein inkrementeller Algorithmus möglich. Die Änderung der Kontrollvariable hängt von der Wahl des Pixels ab:\n\nDer Anfangswert der Kontrollvariable beträgt formula_55. Für die ganzzahlige Rasterung lässt sich der Bruch vermeiden, indem von \"d\" formula_56 abgezogen wird. Dadurch ändert sich der Anfangswert in formula_57 und der Vergleich formula_52 in formula_59, welcher sich durch Rundung in formula_52 umwandeln lässt.\n\nDer resultierende Algorithmus ist Horns Methode sehr ähnlich.\n\nIm Gegensatz zum Midpoint-Algorithmus für Linien (siehe Rasterung von Linien) sind formula_61 und formula_62 nicht konstant, sondern hängen von der aktuellen Position ab. Es ist daher möglich, Differenzen „zweiter Ordnung“ zu betrachten, bei der formula_61 und formula_62 selbst inkrementell berechnet werden. Mit diesem Algorithmus wird die Initialisierung aufwändiger; innerhalb der Schleife spart man im Falle der Wahl von \"SO\" eine Addition. Auf dieses Verfahren hat IBM, Bresenhams damaliger Arbeitgeber, in mehreren Staaten Softwarepatente eingereicht, darunter auch 1982 beim Europäischen Patentamt.\n\nDie Anzahl der arithmetischen Operationen bei Bresenhams Algorithmus lässt sich weiter verringern. Es wurden noch andere, schnellere Methoden zur Rasterung vorgestellt, die mehrere Pixel auf einmal zeichnen. Wu und Rokne stellten 1987 ein Doppelschrittverfahren vor, bei dem je Schleifendurchlauf zwei Pixel eingefärbt werden. Yao und Rokne zeigten 1995, wie auch bei der Rasterung von Kreisen ganze Pixelreihen auf einmal eingefärbt werden können.\n\nEs gibt mehrere Methoden, gefüllte Kreise zu zeichnen. Eine triviale Methode besteht darin, beim Zeichnen eines Oktanten nicht nur ein Pixel pro Schleifendurchlauf, sondern alle Pixel einer Reihe zu zeichnen. Durch die Symmetrie wird der gesamte Kreis gefüllt. Ebenfalls möglich ist das Zeichnen einer minimalen Anzahl von Rechtecken; Nachteil ist hier, dass viele Pixel mehrmals eingefärbt werden.\n\nAnstatt einen Kreis durch seinen Mittelpunkt und seinen Radius zu definieren, ist es auch möglich, einen Mittelpunkt und einen beliebigen auf dem Kreis liegenden Punkt anzugeben. Dabei muss aber beachtet werden, dass bestimmte Punkte des Rasters gar nicht auf einem Kreis mit ganzzahligem Radius liegen. Algorithmen, die Kreise nach diesem Schema zeichnen, müssen auf ungültige Anfangspunkte testen.\n\nField stellte eine Methode zum Antialiasing von Kreisen mittels ungewichteter Flächenabtastung vor, bei der der Kreis für jedes Pixel mit einem Trapez angenähert wird. Der Flächenanteil des Trapezes innerhalb eines Quadrats mit einem Pixel Kantenlänge bestimmt den Farbwert. Dank inkrementeller Berechnung benötigt der Algorithmus nur Multiplikationen und Additionen.\n\nAuch der Gupta-Sproull-Algorithmus für Linien kann auf Kreise erweitert werden. Im Gegensatz zu Linien hängt der Wert des Glättungskerns nicht nur von der Distanz zur Kurve, sondern auch vom Radius ab. Daher sind verschiedene Tabellen für verschiedene Radien notwendig. Für größere Kreise kann eine einzige Tabelle verwendet werden, bei der die Krümmung vernachlässigt wird.\n\n"}
{"id": "2420874", "url": "https://de.wikipedia.org/wiki?curid=2420874", "title": "Wolff-Algorithmus", "text": "Wolff-Algorithmus\n\nDer Wolff-Algorithmus ist ein Monte-Carlo-Algorithmus zur Simulation statistischer Prozesse, insbesondere des Ising-Modells.\nDer Wolff-Algorithmus gehört zu den Cluster-Algorithmen (einem Bereich der MCMC-Verfahren), die besonders effektiv im Bereich von Phasenübergängen sind. Cluster-Algorithmen benötigen in der Nähe der kritischen Temperatur deutlich weniger Rechenzeit als lokale Algorithmen, da sie der Divergenz der Autokorrelationszeiten in der Nähe von Phasenübergängen – dem \"critical slowing down\" – entgegenwirken. \n\nIm Gegensatz zu lokalen Algorithmen, wie beispielsweise dem Metropolisalgorithmus, werden beim Wolff-Algorithmus nicht einzelne lokale Updates nacheinander ausgeführt, sondern ganze Cluster auf einmal verändert. Hierdurch werden die gerade in der Nähe der kritischen Temperatur weitreichenden Korrelationen verkleinert. Für Simulationen weit entfernt vom kritischen Punkt ist der Wolff-Algorithmus jedoch weniger effektiv als lokale Algorithmen. \n\nEntwickelt wurde der Algorithmus 1989 vom deutschen Physikprofessor Ulrich Wolff, zurzeit tätig an der Humboldt-Universität zu Berlin.\n\n\n"}
{"id": "2421345", "url": "https://de.wikipedia.org/wiki?curid=2421345", "title": "SYM-1", "text": "SYM-1\n\nDer SYM-1 war ein Einplatinencomputer, der von der US-amerikanischen Firma Synertek im Jahre 1978 auf den Markt gebracht wurde. Der Rechner trug ursprünglich den Namen VIM-1 (Versatile Input Monitor), der Namen wurde aber aus rechtlichen Gründen im Jahre 1978 abgeändert.\n\nDer SYM-1 war ein Konkurrenzprodukt zum populären MOS Technology KIM-1. Beide Produkte waren in hohem Maße baugleich und kompatibel. Im Vergleich zum KIM-1 konnte der SYM-1 alleine mit einer +5 Volt-Stromquelle betrieben werden, hatte einen umfangreichen Monitor-ROM, besaß drei konfigurierbare ROM/EPROM-Sockel und einen RS-232 seriellen Port.\nSynertek verkaufte ROMs, durch die man auf dem Rechner die BASIC-Programmiersprache und einen Assembler-Editor installieren konnte.\n\n"}
{"id": "2423200", "url": "https://de.wikipedia.org/wiki?curid=2423200", "title": "Checkinstall", "text": "Checkinstall\n\nCheckinstall ist ein Kommandozeilenprogramm für Linux-Betriebssysteme zum Erstellen von Slackware-TGZ-, RPM- oder DEB-Paketen aus Quellcodepaketen von Programmen. Es führt codice_1 aus und erstellt dabei ein Paket, welches nachher mit dem distributionseigenen Paketmanager sauber installiert und später auch deinstalliert werden kann. Checkinstall steht unter der GPL2.\n\nNach dem Kompilieren des Codes durch codice_2 (wobei das Makefile meist durch ein codice_3-Script im Wurzelverzeichnis des Quellcodepakets erstellt wird), führt der Aufruf von codice_4 den Befehl codice_1 aus und registriert die Pfade der installierten Dateien. Anschließend wird hieraus ein RPM- oder DEB-Paket erstellt, welches dann mit dem distributionseigenen Paketmanager installiert werden kann.\n\nIn den häufigsten Fällen werden also folgende Befehle im Quellcodeverzeichnis ausgeführt:\n\nNach der Eingabe optionaler Paketinformationen wie Autor und Beschreibung, die auch direkt beim Programmaufruf als Optionen angegeben werden können, erhält man im Verzeichnis das fertige RPM- bzw. DEB-Paket.\n\nDurch die Anwendung von Checkinstall ergeben sich folgende Vorteile gegenüber einer Installation von Hand mit codice_1:\n\n"}
{"id": "2424164", "url": "https://de.wikipedia.org/wiki?curid=2424164", "title": "Claws Mail", "text": "Claws Mail\n\nClaws Mail (ehemals \"Sylpheed-Claws\") ist ein freies E-Mail-Programm und Newsreader für Unix-ähnliche und Windows-Betriebssysteme. Im Unterschied zu Sylpheed, von dem es abgespalten wurde, bietet es zusätzliche Funktionen wie Skins und Plugins (zum Beispiel für Spamfilter und Virenscanner).\n\nClaws wurde von Chip.de als grafischer Mailclient für Linux empfohlen und ist ebenso als Windows-Port erhältlich.\n\nClaws Mail ist in C geschrieben und verwendet das GIMP-Toolkit (GTK+) für die Benutzeroberfläche.\nNachrichten werden im MH-Format gespeichert, wobei mittels Zusatzmodul alternativ auch das mbox-Format benutzt werden kann.\n\nEs wird als freie Software auch im Quelltext unter den Bedingungen der GNU General Public License (GPL) verbreitet. Claws ist auf Unix-ähnlichen (zum Beispiel GNU/Linux, xBSD, macOS und Maemo) und mit geringen Einschränkungen auch auf Windows-Betriebssystemen lauffähig. Bei gängigen Linux-Distributionen ist es direkt aus den Standard-Paketquellen installierbar.\n\nDie Entwicklung von Claws begann im April 2001 als Entwicklungsversion von Sylpheed, die der Erprobung neuer Merkmale dienen sollte. Am 18. Januar 2005 wurde Version 1.0.0 freigegeben. Im August 2005 wurde es zu einem eigenständigen Software-Projekt abgespalten. Anfangs hieß die Software noch „Sylpheed-Claws“. Am 30. Januar 2006 erschien Version 2.0.0. Am 7. November 2006 wurde sie in „Claws Mail“ umbenannt, um zu unterstreichen, dass der Code unabhängig von Sylpheed weiterentwickelt wird. Am 3. September 2007 erschien Version 3.0.0.\n\n\n"}
{"id": "2424354", "url": "https://de.wikipedia.org/wiki?curid=2424354", "title": "PROFI-5-Mikrocomputerfamilie", "text": "PROFI-5-Mikrocomputerfamilie\n\nDie PROFI-5-Mikrocomputerfamilie beinhaltet verschiedene Varianten eines Einplatinencomputers, der im Jahre 1981 auf den Markt kam und später weiterentwickelt wurde. Er wird vor allem in der Ausbildung und der überbetrieblichen Weiterbildung eingesetzt. Es gibt ihn in den Varianten PROFI-50, PROFI-5, PROFI-50E und PROFI-5E. \n\nFunktionskompatibel zum Profi5E, jedoch mit einem Mikroprozessor Zilog Z80 ausgestattet, ist das MICO-80-Mikrocomputersystem.\n\nDer PROFI-5 wurde vom Ingenieurbüro Kammerer 1981 entwickelt und als Nachfolger des Lerncomputers EZ80-DIT vertrieben. \n\nVon 1994 bis 2011 wurde das Nachfolgegerät PROFI-5E von der IED Kammerer GmbH, Deutschland verkauft. Dieser Einplatinencomputer basiert je nach Variante auf dem 8-Bit-Mikroprozessor Intel 8080 (PROFI-5/PROFI-50) oder auf dem Intel 8085 (PROFI-5E/PROFI-50E). Die Größe des Arbeitsspeichers liegt im Grundausbau bei 2K - 8K Byte ROM und 1K - 4K Byte RAM, erweiterbar bis insgesamt 22 KByte. Als Schnittstellen sind Einzelbit-programmierbare Ein-Ausgabe-Leitungen (Intel 8255), eine parallele Centronics-Schnittstelle (PROFI-5E/-50E), eine serielle V.24-Schnittstelle und für Erweiterungen ein SMP-Bus (PROFI-5E/-50E) verfügbar. Für die analoge Speicherung von Programmen und Daten ist ein Tonbandinterface eingebaut. \n\nDie Systeme der PROFI-5-Mikrocomputerfamilie haben im ROM ein Monitorprogramm. Dieses ermöglicht die Eingabe von Programm und Daten im Hexadezimalcode, Abarbeitung im Einzelschritt- sowie Automatikmodus, Anzeige und Editieren der Registerinhalte. Die Bedienung erfolgt über neun Funktionstasten, die Eingabe der Daten im Hexadezimalsystem über 16 Tasten die mit den Ziffern 0..9 und A..F beschriftet sind. Die Reset-Taste ermöglicht einen Kaltstart.\n\nAls Anzeige steht eine 8-stellige Siebensegment-LED-Anzeige zur Verfügung. Alternativ kann die Bedienung auch über ein an der V.24-Schnittstelle angeschlossenes Terminal erfolgen.\n\nFür die PROFI-5-Mikrocomputerfamilie gibt es Erweiterungsplatinen und Software:\n\n\n\n"}
{"id": "2426352", "url": "https://de.wikipedia.org/wiki?curid=2426352", "title": "Swfdec", "text": "Swfdec\n\nSwfdec (ausgesprochen „swiff deck“) ist eine freie Wiedergabe-Software für das Adobe-Flash-Format (SWF) unter Unix-Systemen, die unter der Softwarelizenz LGPL veröffentlicht wurde.\n\nEs werden Funktionen von SWF Version 7 und ActionScript unterstützt, sowie einige der Videofunktionen des Flash 9 Players. Die Software befindet sich in einer frühen Entwicklungsphase und kann noch nicht als vollwertiger Ersatz für den Adobe Flashplayer benutzt werden.\nDie Wiedergabe von Flash-Videos (z. B. von den Portalen Youtube, DailyMotion, Yahoo, CNN, AOL etc.) ist schon möglich. Ein Blog-Eintrag des Hauptentwicklers deutet darauf hin, dass die Entwicklung mittlerweile eingestellt wurde, auch gab es seit Dezember 2008 keine neuere Version (Stand Dezember 2014).\n\nDie Software besteht aus einer Programmbibliothek, für die es ein Erweiterungsmodul für Mozilla-Browser (und solche mit kompatibler Pluginarchitektur) und ein separates Wiedergabeprogramm für den GNOME-Desktop als Oberfläche gibt.\nEs benutzt die Grafikbibliothek Cairo zum Rendern und PulseAudio, OSS, oder ALSA für Audio-Wiedergabe.\n\n\nDebian GNU/Linux 5.0 (Lenny) verwendet Swfdec standardmäßig als Flash-Plugin für Iceweasel. Ubuntu 8.04 (Hardy Heron) bietet Swfdec bereits im Installationsassistenten für fehlende Firefox-Plugins an. Auch bei Fedora 9 wird Swfdec als Ersatz für proprietäre Lösungen angeboten.\nUbuntu ab Version 9.10 hat es zwar noch in der Repository, aber nicht mehr standardmäßig installiert.\n\n\n"}
{"id": "2431683", "url": "https://de.wikipedia.org/wiki?curid=2431683", "title": "Elilo", "text": "Elilo\n\nelilo war ein Bootloader für Linux auf Computern mit Extensible Firmware Interface (EFI), dessen Entwicklung seit 2014 eingestellt ist.\n\nDie Software wurde ursprünglich für die Architektur IA-64 entwickelt, dann auf IA-32 und schließlich auf x86-64 erweitert. elilo unterstützt anders als der Grand Unified Bootloader (GRUB) nicht weiterhin das BIOS. elilo kann aber eine ähnliche Konfigurationsdatei wie der Linux Loader (LILO) nutzen. \n\nelilo unterstützt das Booten übers Netzwerk mittels DHCP und TFTP. \n"}
{"id": "2432868", "url": "https://de.wikipedia.org/wiki?curid=2432868", "title": "Skinning", "text": "Skinning\n\nAls Skinning wird in der Computeranimation die Zuordnung eines Drahtgittermodells zu einem Bewegungsskelett bezeichnet. Skinning ist der logisch folgende Schritt nach dem Rigging, das die grundlegende Motorik eines animierten Charakters festlegt. Skinning hingegen definiert die tatsächlich sichtbare Verformung des Körpers, die im Wesentlichen den Bewegungsvorgaben des Rigs folgt und, besonders in „Problemzonen“, dessen Einfluss variieren kann, um natürliche Bewegungsabläufe zu erzielen.\n\nDurch die Gewichtung einzelner Teile des Drahtgitters bei der Verknüpfung kann der Einfluss des Skeletts angepasst und so beispielsweise das „Einknicken“ speziell an den Gelenkpunkten vermieden werden. Zudem ist es möglich, weitere Bewegungen zu überlagern. Dies kann unter anderem nötig werden, um beim Beugen eines Arms oder eines Beins die zugehörige Muskelschwellung zu simulieren oder unerwünschte Einflüsse des Rigs auszugleichen.\n\nDurch die zunehmende Bedeutung der Animationsindustrie hat Skinning in seiner heutigen Bedeutung das veraltete Synonym für Texturing praktisch vollständig verdrängt.\n"}
{"id": "2442387", "url": "https://de.wikipedia.org/wiki?curid=2442387", "title": "Rundll32.exe", "text": "Rundll32.exe\n\ncodice_1 ist ein Win32-Dienstprogramm von Microsoft Windows ab Windows 95 und wird verwendet, um Win32-Funktionen aus Programmbibliotheken als eigenständige Routinen auszuführen. Dabei können nur Funktionen ausgeführt werden, die in der Programmbibliothek explizit für das Ausführen mit diesem Dienstprogramm deklariert wurden. In älteren Windows-Versionen (Windows 95 bis Windows Me) ist aus Kompatibilitätsgründen die 16-bit-Version codice_2 zum Ausführen von Win16-Funktionen noch enthalten.\n\nDie Datei befindet sich bei Windows NT bis Windows 10 im Ordner codice_3 (also zum Beispiel codice_4) und bei Windows 95 bis Windows ME direkt im Windows-Verzeichnis (also zum Beispiel codice_5). Bei 64-Bit-Betriebssystemen befindet sich die Datei in der 32-Bit-Version (Windows on Windows) zusätzlich unter codice_6.\n\nDiese Anwendungen sind in der Automatisierung von systemnahen Abläufen wichtig. Daher gehört die rundll32.exe zu den durch Malware gefährdeten Angriffspunkten und ist so vielen Anwendern namentlich bekannt geworden.\n\nEine Programmbibliothek (DLL-Datei) dient dazu, anderen Programmen Funktionen als Programmmodul zur Verfügung zu stellen, kann jedoch normalerweise nicht direkt ausgeführt werden. Die RunDLL erlaubt das Aufrufen einzelner Funktionen einer solchen Schnittstelle beispielsweise an der Kommandozeile, aus Skripten oder als Verknüpfung. Die Ausführung erfolgt in einem eigenen Prozess, daher werden RunDLL-Aufrufe auch von anderen Programmen verwendet, die sich vor Fehlern im aufgerufenen DLL schützen wollen. Auf die gleiche Weise können auch Programmfunktionen in ausführbaren Systemdateien (EXE-Dateien) aufgerufen werden.\n\nBeispiele:\n\nDie \"Control Panels\" (CPL-Dateien), die normalerweise über den virtuellen Ordner \"Systemsteuerung\" aufgerufen werden, können mit der RunDLL alternativ auch direkt über die Kommandozeile zugänglich gemacht werden. Das erfolgt über den Aufruf der codice_12:\n\nCode:\n\nWährend die Applets der einzelnen Funktionen durchaus dokumentiert sind, ist man bei der Steuerung des passenden Reiters eher auf Probieren bzw. auf Tipps in der einschlägigen Literatur und Webressourcen angewiesen.\n\nBeispiele:\n\nDiese Methode kann sowohl von der \"Kommandozeile\" bzw. der Batch-Verarbeitung, aus diversen \"Skriptsprachen\" wie auch mit einfachen \"Verknüpfungen\" (LNK-Dateien) ausgeführt werden.\nDa die Funktionen sehr nah am Betriebssystem laufen, wird in einschlägigen Kreisen bei Experimenten Vorsicht angeraten, und nur dem einigermaßen geübten Benutzer empfohlen.\n\nEs wird erwartet, dass die von Rundll32.exe aufgerufenen Funktionen einer bestimmten Signatur entsprechen:\nÜblicherweise wird diese Einschränkung allerdings missachtet (auch von Beispielen auf dieser Seite). Das führt in jedem Fall zu einer Korruption des Stacks und zu unvorhergesehenem Verhalten, beispielsweise Endlosschleifen.\n\nAufgrund ihrer häufigen Verwendung durch Programme, durch die rundll32 in der Prozessliste häufig auftauchen kann, wird die rundll32 oft von Viren, Spyware und ähnlichem als „Namensgeber“ für deren Schädlingsprogramme genutzt.\n\nWeiterhin ist eine Datei außerhalb von codice_19 mit dem Namen codice_20 in den meisten Fällen ein Virus. Eine bösartig ersetzte Original-RunDLL wird aber von der Windows-Funktion \"Systemwiederherstellung\" abgesichert, die Systemdateien automatisch auf einen verlässlichen Zustand zurücksetzt. Wenn es dem Schädling aber gelingt, Aufrufe der RunDLL auf die bösartige Version umzubiegen, ohne dass das bemerkt wird, handelt es sich um einen sehr bedrohlichen Schadensfall.\n\nFolgend eine Liste typischer Funktionen. Der erste Parameter ist durchwegs eine .dll- oder .exe-Datei, dass die Endung nicht angegeben ist, liegt daran, dass sie über die Path-Variable und den windowstypischen Ergänzungsschlüssel codice_21 gefunden wird. Der zweite Parameter ist der Name der Routine, die weiteren Parameter sind Eingabewerte an diese Routine, z. B. bei einigen Dialogen mit mehreren Registerkarten die Nummer der Registerkarte als \",@1\" oder \",1\".\n\nDiese Aufrufe sind i. d. R. nicht offiziell dokumentiert, daher kann sich die Verfügbarkeit in Abhängigkeit von Betriebssystemsversion und Edition, u. U. auch von Service Pack, Update oder Software Dritter unterscheiden.\n\n"}
{"id": "2443944", "url": "https://de.wikipedia.org/wiki?curid=2443944", "title": "Alcohol 120%", "text": "Alcohol 120%\n\nAlcohol 120% ist ein CD/DVD-Emulator mit Brennfunktion, der von Alcohol Soft entwickelt wurde. Alcohol 52% ist eine reduzierte Version dieser Software, die lediglich die Emulationsfunktionen, aber keine Brennfunktion enthält. Beide Programmvarianten werden in einer kostenpflichtigen Version mit Produktunterstützung vertrieben, für die nach einem Testzeitraum von 15 Tagen eine Lizenz erworben werden muss. Ausschließlich für Privatanwender sind sowohl Alcohol 120% als Alcohol 52% auch in einer kostenfreien Version (\"Free Edition\") erhältlich.\n\nAlcohol benutzt Rootkit-Techniken, um sich vor Kopierschutz-Programmen zu verstecken.\n\nAuch bei den offiziellen, vom Hersteller signierten Installationspaketen wird versucht, die spywareverdächtige Software \"Smart File Advisor\" mitzuinstallieren, die nach Einrichtung permanent im Hintergrund aktiv ist, regelmäßig eine Liste installierter Programme an einen Internet-Server meldet und den Windows-Dialog \"Öffnen mit...\", der zur Verknüpfung eines Dateityps mit einer Anwendung dient, durch einen eigenen Dialog ersetzt. In den kostenpflichtigen Versionen muss der Anwender die zusätzliche Installation bei Nichtinteresse explizit abwählen, sie ist jedoch unabhängig von der getroffenen Auswahl bei jeder Update-Installation standardmäßig aktiviert. In den kostenfreien Versionen wird die Zusatzsoftware zwingend mitinstalliert und kann auf direktem Weg über die Windows-Programmverwaltung nicht deinstalliert werden, ohne auch die Hauptsoftware zu deinstallieren.\n\nAlcohol 120% und Alcohol 52% können Abbilder von CDs und DVDs erstellen, entweder im hauseigenen „Media Descriptor Image“-Format oder in anderen bekannten Formaten.\n\nAußerdem kann Alcohol bis zu 31 (\"Free Edition\": bis zu 6) virtuelle Laufwerke emulieren, in die man CD/DVD-Abbilder einbinden (mounten) kann, wodurch das Brennen auf einen CD/DVD-Rohling sich unter Umständen erübrigt. Alcohol 120% kann Abbilder auf beschreibbare CD/DVD-Rohlinge brennen. Es unterstützt gleichzeitiges Brennen auf mehreren Brennern. Ab der Version 1.9.8.7117 ist das Programm auch in der Lage, auf Blu-ray Disc zu brennen.\n\nAb Version 1.9.7.6221 ist in Alcohol 120% der sogenannte „Xtra Assistent“ integriert. Diese Funktion erlaubt es dem Nutzer, das Programm auch als Brennsuite wie z. B. Nero zu verwenden. Er öffnet dazu den Assistenten und fügt seine Dateien der Zusammenstellung hinzu, um daraufhin ein Abbild im MDS- oder ISO-Format zu erstellen.\n\nAb Version 1.9.8.7117 ist in Alcohol 120% ein Modul namens \"A.C.I.D.\", eine veränderte Version des Programms \"Y.A.S.U.\" (Yet Another SecuRom Utility), integriert. Dieses ermöglicht es dem Anwender, die virtuellen SCSI-Laufwerke zu verstecken und das \"Blacklisting\" einiger Kopierschutzverfahren (SecuROM und SafeDisc) zu umgehen. \"A.C.I.D.\" ist in aktuellen Versionen nur für Kunden von Alcohol Soft erhältlich.\n\nIn Deutschland werden vom Franzis-Verlag verschiedene Kaufhausversionen von Alcohol 120% vertrieben, welche jedoch, um dem deutschen Urheberrecht zu genügen, die meisten heute eingesetzten Software-Kopierschutzverfahren nicht umgehen können. Der Herausgeber verwendet für diese Versionen eigene Bezeichnungen und bietet im Gegensatz zum Entwickler Alcohol Soft keine kostenfreien Aktualisierungen an. So wurde etwa die Programmversion 1.9.6.4719 in Deutschland unter anderem als \"\"Alcohol 120% Classic Edition Vista\"\" vertrieben. Während für Kunden von Alcohol Soft die Nachrüstung der Blu-ray-Funktionen als kostenloses Update (Version 1.9.8.7117) bereitgestellt wurde, ist diese Version in Deutschland nur als neues Vollpreisprodukt mit der Bezeichnung \"„Alcohol 120% 5.0 BluRay“\" erhältlich.\n\nDer Franzis-Verlag nutzt die Bezeichnung \"Alcohol\" oder Variationen des Alcohol-Logos auch für einige andere Programme, die nicht zur Produktpalette von Alcohol Soft gehören (z. B. \"Alcohol 120% Audio-Converter\", \"Alcohol 120% Musik-Recorder\", \"Audio 180%\"). Von einigen dieser Zusammenstellungen distanziert sich der Entwickler und leistet keine Produktunterstützung.\n\n\nAlcohol 120% und mit Einschränkungen auch Alcohol 52% sind in der Lage, bestimmte Kopierschutzverfahren wie SecuROM oder SafeDisc zu umgehen. Da Programme mit derartigen Fähigkeiten in Deutschland nicht beworben oder verkauft werden dürfen, ist dort in Kaufhäusern nur eine dem deutschen Recht angepasste funktionsreduzierte Version erhältlich.\n\nAus rechtlichen Gründen können die Alcohol-Programme weder Sicherheitskopien noch virtuelle Abbilder von Video-DVDs erzeugen, deren Inhalt mit dem Content Scramble System (CSS) verschlüsselt ist.\n\n"}
{"id": "2444350", "url": "https://de.wikipedia.org/wiki?curid=2444350", "title": "Link Layer Topology Discovery", "text": "Link Layer Topology Discovery\n\nLink Layer Topology Discovery (LLTD), d. h. Verbindungsschicht-Topologieerkennung, ist ein Microsoft-Windows-Netzwerkprotokoll, welches eingesetzt wird, um die Topologie eines Netzwerkes zu erfassen und darzustellen sowie auch zur Analyse der Dienstgüte (Quality of Service, QoS).\n\nDas Protokoll wurde von Microsoft im Rahmen der Windows-Rally-Technologien entwickelt und in Windows Vista erstmals eingesetzt. Für Windows XP existiert eine Aktualisierung, mit der Windows-XP-Systeme unter Vista und Server 2008 erkannt werden können. Als Teil des Windows Rally Development Kit wurde auch eine lizenzpflichtige Quelltextversion für Linux veröffentlicht.\n\nUnter Windows Vista und Server 2008 werden die Rechner und Switches in einer Topologiegrafik dargestellt. Netzwerkkomponenten werden durch kleine Piktogramme gezeigt und mit dem Hostnamen als Erkennung versehen. Die Netzverbindungen zwischen den Geräten werden durch Linien verzeichnet. Die Piktogramme können weiter erforscht werden, so dass ein Daten-Popup erscheint, das wichtige Netzdaten, wie z. B. MAC-Adresse und IP-Adresse (IPv4 und IPv6), angibt. Die LLTD Komponente für Windows XP kann aber nur IPv4 (nicht IPv6) Information erzeugen.\n"}
{"id": "2449563", "url": "https://de.wikipedia.org/wiki?curid=2449563", "title": "Shrek der Dritte", "text": "Shrek der Dritte\n\nShrek der Dritte ist ein Computeranimationsfilm der Regisseure Chris Miller und Raman Hui aus dem Jahr 2007. Er lief am 21. Juni 2007 in den deutschen Kinos an. Der Film bildet den dritten Teil der \"Shrek\"-Filmreihe nach \"Shrek – Der tollkühne Held\" und dessen Fortsetzung \"Shrek 2 – Der tollkühne Held kehrt zurück\" und wurde 2010 von \"Für immer Shrek\", dem letzten Teil, fortgesetzt.\n\nAls Shreks Schwiegervater Harold, der Herrscher des Königreichs „Weit Weit Weg“, erkrankt, ernennt er Shrek zu seinem Nachfolger als König. Kurz darauf stirbt Harold. Da Shrek nicht den Wunsch hegt, König zu werden, begibt er sich zusammen mit seinen Freunden Esel und dem gestiefelten Kater auf die Suche nach Fionas Cousin Artus, dem nächsten Familienmitglied in der Thronfolge.\n\nWährend Shrek davonsegelt, um Artus zu suchen, ruft Fiona ihm hinterher, dass sie schwanger sei, was bei Shrek im weiteren Verlauf des Films Albträume hervorruft. Er fürchtet sich vor der Verantwortung, die er als Vater hätte.\n\nWährenddessen versucht Prinz Charming, mittlerweile ein erfolgloser Schauspieler, den Thron von „Weit Weit Weg“ durch einen Staatsstreich zu übernehmen. Unterstützt wird er dabei von Captain Hook, der bösen Stiefmutter von Schneewittchen, einem Zyklopen, Rumpelstilzchen, Mabel, der hässlichen Stiefschwester, und einer Armee aus Hexen, Elben und Ents. Die schwangere Fiona, ihre Mutter Lilian und Doris, die andere hässliche Stiefschwester, rekrutieren vier von Fionas besten Freundinnen: Schneewittchen, Dornröschen, Rapunzel und Aschenputtel. Zusammen mit ihnen, Pinocchio, den drei kleinen Schweinchen, dem großen, bösen Wolf und dem Lebkuchenmann stellen sie sich gegen Charming.\n\nCharmings Gruppe stürmt das Königreich, doch Fiona und ihre Freundinnen können entkommen. Auf der Flucht vor Charming werden sie aber von Rapunzel verraten – Charming hatte ihr angeboten, seine Königin zu werden – und sie geraten hinter Gitter. Währenddessen schickt Charming eine Gruppe los, um Shrek und seine Freunde zu verhaften. Shrek, Esel, der gestiefelte Kater und Artus können die Bösewichte aber vertreiben. Um wieder nach Hause zu kommen, suchen sie den alten und verwirrten Schul-Zauberlehrer Merlin auf, der die vier mit einem Zauberspruch nach „Weit Weit Weg“ befördert, zugleich tauschen jedoch Esel und der gestiefelte Kater durch einen Fehler Merlins ihre Körper. Zurück in „Weit Weit Weg“ wird Shrek kurz darauf verhaftet. Um Artus zu schützen, erklärt Shrek, dass Artus nur zum nützlichen Thron-Lückenbüßer für ihn selbst dienen sollte. Daraufhin lässt Charming den gekränkten Artus gehen.\n\nWährenddessen brechen die Frauen mit Lilians Hilfe aus. Mit „Frauenpower“ gelingt es ihnen, wieder zurück in die Stadt zu kommen. Prinz Charming plant, Shrek während einer Theateraufführung zu töten. Diese nutzt Shrek, um sich über ihn lustig zu machen. Als der Prinz zum Todesstoß ansetzt, wird die Bühne von Fiona und ihren Freundinnen gestürmt. Artus klettert auf die Bühne und überzeugt Charmings Armee davon, dass sie ebenfalls sanfte Seiten haben und gut sind. Zusammen mit dem Drachen gelingt es ihnen, Charming zu überwältigen. Artus nimmt daraufhin die Krone an.\n\nShrek und Fiona kehren zu ihrem Sumpf zurück, wo sie als glückliche Eltern ihre drei Kinder aufziehen.\n\nWie in den vorhergehenden Filmen finden sich zahlreiche Anspielungen auf andere Klassiker. So wird beispielsweise Prinz Charming in der Endszene von dem herabstürzenden Turm erschlagen. Das Szenenbild ist dabei an den Film Hook angelehnt, in dem Captain Hook am Ende auf genau dieselbe Art von dem ausgestopften Krokodil erschlagen wird und ebenso wie Charming nach seiner Mutter ruft.\n\nDer Film erhielt bei Rotten Tomatoes lediglich einen Wert von 42 % und einen IMDB-Wert von 6,1 (9. Juli 2016).\n\n\nFast alle Sprecher der ersten beiden Teile konnten auch für Teil drei wieder gewonnen werden. Nur Randolf Kronberg, die bisherige deutsche Stimme von Esel, verstarb am 2. März 2007 vor der Synchronisation des Films und wurde durch Dennis Schmidt-Foß ersetzt. Die deutsche Synchronisation des Films übernahm auch diesmal wieder die Berliner Synchron AG in Berlin.\n\nIm Film sind u. a. folgende Titel zu hören:\n\n\n\n"}
{"id": "2450534", "url": "https://de.wikipedia.org/wiki?curid=2450534", "title": "Roland MC-303", "text": "Roland MC-303\n\nDie Roland MC-303 ist ein Synthesizer, Drum Machine und Hardwaresequenzer in einem und eröffnete damit den Trend der Grooveboxen, die es ermöglichten, innerhalb kurzer Zeit in einer geschlossenen Arbeitsumgebung bestimmte Arten von Musik zu komponieren und zu arrangieren.\n\nDie Entwicklung von Synthesizern und Drum Machines war Mitte der 1990er Jahre so weit, dass alle Komponenten in einem kompakten und preiswerten Gerät vereinigt werden konnten. Diesem Konzept folgte die MC-303, die 1996 auf den Markt kam. Die Nachfrage stieg innerhalb kurzer Zeit so stark an, dass Roland in Lieferschwierigkeiten geriet.\n\nDie MC-303 etablierte sich binnen kürzester Zeit besonders in der Techno-Szene, vor allem wegen der Drum-Sounds. Wegen der exzessiven Ausnutzung der Klänge des Geräts kam es auch schnell aus der Mode. In den letzten Jahren besinnt man sich aber zunehmend wieder auf seine Qualitäten.\n\nZwei Jahre später, 1998, kam der Nachfolger der MC-303, die Roland MC-505 auf den Markt.\n\nDie Soundsynthese der MC-303 basiert vorwiegend auf gesampleten Klängen aus anderen Roland-Produkten wie der Roland TR-808, Roland TR-909, Roland TB-303 und der \"Juno\"-Serie. Darüber hinaus enthält es auch eine General-MIDI-Soundbank. Die Filter- und Effektsektion ermöglichten im kleinen Rahmen eine Veränderung der Klänge und Geräusche.\n\nWie auch seine Vorbildmodelle arbeitet die MC-303 mit einem patternorientierten Achtspur-Step-Sequencer, der entweder manuell programmiert oder durch die MIDI-Schnittstelle angesteuert werden kann. Das in Form einer Keyboardoktave angeordnete Tastenfeld kann sowohl den Synthesizer als auch den Sequenzer ansteuern.\n\n\n"}
{"id": "2452868", "url": "https://de.wikipedia.org/wiki?curid=2452868", "title": "Mumble", "text": "Mumble\n\nMumble ( „murmeln“, „nuscheln“) ist eine freie Sprachkonferenzsoftware, die sich wegen niedriger Latenzzeit und guter Audioqualität unter anderem für den Einsatz parallel zu Onlinespielen eignet.\n\nDie Software realisiert ein klassisches Client-Server-System. Der Client Mumble stellt eine grafische Oberfläche für Unterhaltungen und zur Administration bereit, der Server, der die Bezeichnung „Murmur“ trägt, realisiert das Back-End, über das die Gespräche laufen.\n\nDer offizielle Client läuft unter Windows, Linux und macOS. Für Android und iOS gibt es mehrere alternative Clients (z. B. \"Plumble\"). Der Server lässt sich auf beinahe beliebigen Systemen kompilieren und ausführen. Die einzige Voraussetzung ist, dass Qt 4.0 ebenfalls kompiliert werden kann. Allerdings existiert auch ein ressourcensparender Server namens uMurmur, der Qt nicht voraussetzt und vor allem für den Einsatz auf Routern geeignet ist, aber einige Funktionen nicht beherrscht.\n\nDie Software nutzt die freien Audiocodecs Constrained-Energy Lapped Transform (CELT) und Opus der Xiph.Org Foundation. Um Kompatibilität zu älteren Clients zu gewährleisten, wird Speex zum Dekodieren mitgeliefert. Mumble nutzt dabei die Fähigkeiten der Codecs zur Echo- und Rauschunterdrückung.\n\nWeitere Funktionen sind:\n\nIm September 2005 erschien die erste Alpha-Version 0.1, die folgende Betaphase wurde am 15. Juli 2007 mit der Freigabe der Version 1.0.0 abgeschlossen.\n\nAm 10. Dezember 2009 wurde die Version 1.2.0 mit zahlreichen neuen Funktionen und Verbesserungen veröffentlicht.\n\nAm 1. Juni 2013 wurde die Version 1.2.4 veröffentlicht, mit der Opus eingeführt wurde.\n\nDie Versionen 1.2.5 bis 1.2.8 beinhalten keine neuen Funktionen, sondern sind reine Sicherheitsupdates.\n\nDie aktuelle Version, 1.2.19, wurde am 27. Januar 2017 veröffentlicht.\n\nDie vollständige Versionsgeschichte ist im offiziellen Blog nachzulesen.\n\n\n"}
{"id": "2454349", "url": "https://de.wikipedia.org/wiki?curid=2454349", "title": "Distanzmatrix", "text": "Distanzmatrix\n\nDie Distanzmatrix zeigt die Abstände, d. h., die Anzahl der Bindungen zwischen den Atomen eines Moleküls an. Die Distanzmatrix beschreibt damit einen wichtigen Aspekt der Topologie einer chemischen Verbindung. Das Molekül wird dabei als ungerichteter Graph ohne Mehrfachkanten betrachtet. Die Bindungsordnungen werden somit ignoriert, eine Distanzmatrix unterscheidet nicht zwischen Einfach- und Mehrfachbindungen.\n\nIn kompakter mathematischer Darstellung (ohne die Atomnummern) werden die Eigenschaften deutlicher:\n\nformula_1\n\nDie Distanzmatrix ist symmetrisch. Da der Graph ungerichtet ist, ist der Abstand von Atom 1 zu Atom 2 gleich dem Abstand von Atom 2 zu Atom 1.\n\nDie Distanzmatrix wird bei der Berechnung topologischer Deskriptoren wie dem Wiener-Index und, in modifizierter Form, dem Balaban-J-Index verwendet.\n\nZur Berechnung kann der Min-Plus-Matrixmultiplikations-Algorithmus, der Algorithmus von Floyd und Warshall oder der Dijkstra-Algorithmus angewandt auf jeden Knoten verwendet werden.\n\n"}
{"id": "2456012", "url": "https://de.wikipedia.org/wiki?curid=2456012", "title": "GeoPort", "text": "GeoPort\n\nGeoPort war eine serielle Schnittstelle, die man an einigen Modellen der Apple Macintosh-Computern finden konnte. Der GeoPort änderte die eigentliche Definition der bisherigen seriellen Schnittstellen des Macs und fügte einen noch schnelleren Direct Memory Access (DMA)-Kanal hinzu, der es beispielsweise der internen Soundkarte erlaubte, andere Geräte, wie beispielsweise ein Modem oder ein Faxgerät zu emulieren. Apple verbaute den GeoPort in vielen 68k-Macs und einigen Power Macintosh-Computern, bevor USB-Anschlüsse integriert worden sind. Die GeoPort-Technologie gilt mittlerweile als überholt und wurde eingestellt.\n\n"}
{"id": "2462847", "url": "https://de.wikipedia.org/wiki?curid=2462847", "title": "I-DEAS", "text": "I-DEAS\n\nI-DEAS (\"Integrated Design and Engineering Analysis Software\") ist eine kommerzielle CAD-Software für die 3D-Konstruktion, die ursprünglich von der Firma Structural Dynamics Research Corporation (SDRC) entwickelt worden ist. SDRC wurde 2001 von dem Unternehmen Electronic Data Systems (EDS) übernommen und mit dem gleichzeitig durch EDS erworbenen Wettbewerber Unigraphics Solutions zusammengeführt. Das CAD-Produkt Unigraphics der Unigraphics Solutions und die von SDRC entwickelte CAD-Software I-DEAS werden in das gemeinsame neue CAD-Programm NX überführt. Zur Zeit (März 2007) wird I-DEAS noch unter dem Namen I-DEAS (NX Series) vertrieben. EDS hat Unigraphics später als UGS Corporation an ein Privat-Equity-Konsortium abgegeben. Die Siemens-Sparte Automation and Drives (A&D) (Aktueller Name: Siemens PLM Software) hat UGS für 3,5 Milliarden US-Dollar (rund 2,7 Milliarden Euro) Anfang 2007 gekauft.\n\nI-DEAS wurde zusammen mit weiteren Programmen als Bestandteil eines Softwarepaketes zur konzernweiten Produktentwicklung und Konstruktion innerhalb der Automobilindustrie eingesetzt. I-DEAS unterstützt parametrische Konstruktion. \n\nI-DEAS ist modular aufgebaut. Die mehr als 30 verschiedenen Module aus den Bereichen Digital Product Design (u. a. Master Modeler, Master Drafting, Master Surfacing, Master Assembly, Viewer und Konverter), Digital Manufacturing und Digital Simulation lassen sich einzeln installieren, lizenzieren und benutzen. Von jedem Programmmodul aus wird jedoch auf die gemeinsame Datenbasis, das sogenannte Team-Verzeichnis, zugegriffen. In einem Master-Modell können die im Team-Verzeichnis in verschiedenen Versionsständen abgelegten Einzelteile zu einem Gesamtmodell des zu konstruierenden Teiles zusammengefasst werden.\n"}
{"id": "2464349", "url": "https://de.wikipedia.org/wiki?curid=2464349", "title": "Senster", "text": "Senster\n\nSenster \nwar ein Roboterkunstwerk von Edward Ihnatowicz. Es wurde von Philips als Ausstellungsstück für das Evoluon in Auftrag gegeben und dort von 1970 bis 1974 ausgestellt und anschließend demontiert.\n\nEs war die erste von einem digitalen Computer kontrollierte robotische Skulptur.\nSie war an ihrer \"Schulter\" etwa 2,5 Meter hoch und 4 Meter lang. Sie bestand aus geschweißten Stahlrohren und wurde durch hydraulische Kolben bewegt. Auf ihrem \"Kopf\" waren vier Mikrofone und zwei Dopplerradar-Sensoren montiert, die die Geräusche und Bewegungen der umgebenden Menschen aufnahmen. Ein Computersystem (Philips P9201 – ein Klon des verbreiteteren Honeywell DDP-416) kontrollierte den Roboter und steuerte sein Verhalten so, dass er von leisen Geräuschen und langsamen Bewegungen angezogen, von lauten Geräuschen und abrupten Bewegungen aber abgestoßen wurde. Die komplizierte Akustik der Halle und das unvorhersagbare Verhalten der Besucher ließ das Verhalten des Senster erheblich komplexer erscheinen, als seine Programmierung vermuten ließ.\n\n\n"}
{"id": "2478440", "url": "https://de.wikipedia.org/wiki?curid=2478440", "title": "Mercurial", "text": "Mercurial\n\nMercurial ist ein plattformunabhängiges, verteiltes Versionskontrollsystem zur Software-Entwicklung. Es wird nahezu vollständig in Python entwickelt: lediglich eine diff-Implementierung, die mit binären Dateien umgehen kann, ist in C umgesetzt. Mercurial wird primär über die Kommandozeile benutzt; alle Kommandos beginnen mit „hg“, dem Elementsymbol von Quecksilber ().\n\nEntwicklungsschwerpunkte von Mercurial sind Effizienz, Skalierbarkeit und robuste Handhabung von Text- und Binärdateien. Bei Mercurial wird das Repository des Projektes, an dem man entwickeln will, „geklont“, also eine lokale Kopie erstellt. Auf dieser lokalen Kopie stehen dann die üblichen Funktionen zur Verfügung, beispielsweise das Erstellen neuer Revisionen, \"\" genannt.\n\nDie Fähigkeit, Entwicklungszweige zu erstellen und zusammenzuführen (engl.: \"„“\" und \"„“\"), ist fester Bestandteil von Mercurial. Eine integrierte Web-Schnittstelle steht zur Verfügung; Drittanbieter stellen grafische Frontends oder Plugins für Entwicklungsumgebungen zur Verfügung.\n\nAm 19. April 2005 wurde Mercurial von Matt Mackall auf der Linux-Kernel-Mailingliste angekündigt. Ausschlaggebend war die Ankündigung der Firma BitMover, die z. B. für den Linux-Kernel als Versionskontrollsystem eingesetzte Software BitKeeper nicht mehr in einer kostenlosen Version bereitzustellen. Ungefähr zur gleichen Zeit hatte Linus Torvalds damit begonnen, ein eigenes Projekt namens Git zu starten, welches ähnliche Ziele verfolgt wie Mercurial.\n\nOffiziell wird für den Linux-Kernel Git benutzt, allerdings gibt es auch Kernel-Entwickler, die Mercurial einsetzen.\n\nMercurial wird von vielen bekannten Software-Projekten und Firmen eingesetzt. Unter anderem findet es Einsatz bei Facebook, Mozilla (Firefox, Thunderbird), SourceForge, Google Inc. (Google Code), Atlassian (Bitbucket), Microsoft (CodePlex), Oracle (OpenJDK), Xen, NetBeans IDE, Python, Dovecot, und Nginx.\n\nFür Microsoft Windows und Gnome/Nautilus steht mit der grafischen Oberfläche TortoiseHg, und macOS mit MacHg und Murky, ein jeweils einfach zu bedienendes Frontend zur Verfügung, das die Benutzung von Mercurial ohne Kommandozeilenbefehle erlaubt.\n\nVerschiedene integrierte Entwicklungsumgebungen wie Netbeans, Eclipse, Android Studio, Delphi oder der Qt Creator unterstützen Mercurial direkt aus der grafischen Oberfläche, in der Regel durch ein Plug-in, welches entweder mitgeliefert oder nachträglich installiert wird. MercurialEclipse erlaubt dabei auch das Arbeiten mit Patch Queues (mq).\n\n\n"}
{"id": "2479616", "url": "https://de.wikipedia.org/wiki?curid=2479616", "title": "Wasserintegrator", "text": "Wasserintegrator\n\nDer Wasserintegrator () war ein früher Analogrechner. Er wurde 1936 in der Sowjetunion von Vladimir S. Lukyanov gebaut. \n\nMittels eines komplexen Netzwerkes aus Rohrleitungen und Wasserbehältern konnten Berechnungen durchgeführt werden. Der Wasserstand in den Behältern entsprach verschiedenen Zahlenwerten. Der Rechner konnte nicht-homogene Differentialgleichungen lösen.\n\nDie ersten Exemplare von Lukyanovs Integratoren waren eher experimentell und bestanden aus Zinn- und Glasröhren. Jeder Integrator konnte nur zur Lösung eines Problems verwendet werden.\n\nIn den 1930er Jahren war es das einzige Rechner in der Sowjetunion um Differentialgleichungen in partiellen Ableitungen zu lösen. \n\n1941 schuf Lukyanov einen hydraulischen Integrator mit modularem Aufbau, der die Montage einer Maschine zur Lösung verschiedener Probleme ermöglichte. Entworfen wurden zweidimensionale und dreidimensionale hydraulische Integratoren.\n\nDer Wasserintegrator wurde in den 1940er Jahren zur Berechnung des Karakumkanals und in den 1970er Jahren zum Bau der Baikal-Amur-Magistrale verwendet. Ein funktionsfähiges Modell dieses Computers ist im Science Museum in London zu sehen.\n\n1949–1955 wurde am wissenschaftlichen Forschungsinstitut für Rechenmaschinenbau (NIIschetmash) ein Integrator in Form einheitlicher Module entwickelt. \n\nIm Jahr 1955 begann die Fabrik für Rechen- und Analysegeräte in Ryazan mit der Serienproduktion von Integratoren mit dem Fabriknamen „IGL“ (Integrator des Lukyanov-Hydrauliksystems). Integratoren wurden weit verbreitet und in die Tschechoslowakei, nach Polen, Bulgarien und China geliefert. \n\nWasserrechner wurden bis in die 1980er Jahre in der Sowjetunion genutzt. Sie wurden in der Geologie, im Bergbau, in der Metallurgie, in der Raketenproduktion und in anderen Bereichen eingesetzt.\n\n"}
{"id": "2480241", "url": "https://de.wikipedia.org/wiki?curid=2480241", "title": "Digital Content Creation", "text": "Digital Content Creation\n\nAls Digital Content Creation wird der Bereich innerhalb einer Produktionspipeline bezeichnet, der sich mit der Erstellung von multimedialen Inhalten beschäftigt. Darunter werden verschiedenste Technologien zusammengefasst.<br>\nGenerell fallen alle Werke, die grafischer Natur sind und computergestützt erzeugt wurden in den Bereich der DCC.<br>\nDa die Erstellung dieser Inhalte meistens ein kreativer Prozess ist, wird eine Fachkraft innerhalb eines Bereiches der DCC fast immer als \"Artist\" bezeichnet (3D-Artist, Compositing-Artist ...).\n\nBranchen:\n"}
{"id": "2481389", "url": "https://de.wikipedia.org/wiki?curid=2481389", "title": "FileVault", "text": "FileVault\n\nFileVault ist eine Funktion von Mac OS X/​OS X/​macOS zum Verschlüsseln von persönlichen Daten. Sie ist seit Mac OS X Panther (10.3, 2003) standardmäßig im Lieferumfang enthalten.\n\nFileVault der 1. Generation verschlüsselte zunächst ausschließlich das Benutzerverzeichnis; weitere Daten wurden nicht erfasst und Daten im Benutzerverzeichnis lassen sich nicht von der Verschlüsselung mit FileVault ausschließen. Es ließen sich jedoch auch Programme damit verschlüsseln, wenn statt des globalen Programm-Ordners ein Benutzer Programm-Ordner im Home-Ordner angelegt und verwendet wird.\n\nMit der Einführung von Mac OS X Lion (10.7, 2011) unterstützt die neue Version \"FileVault 2\" auch Festplattenverschlüsselung, die Verschlüsselungstechnik wurde auf XTS-AES 128 umgestellt. Zudem kann jetzt auch \"on the Fly\" verschlüsselt werden, der Mac kann also während des Verschlüsselungsvorganges weiter verwendet werden, eine Funktion, die auch bei anderer Software üblich ist.\n\nFileVault (1. Generation) verwendet seit Mac OS X Leopard (10.5, 2007) für die Verschlüsselung ein mitwachsendes Bundle-Image, in dem die Benutzerdaten gespeichert werden, und das bei der Benutzeranmeldung transparent als Benutzerverzeichnis eingebunden (gemountet) wird. Bei der Benutzerabmeldung wird das mitwachsende Bundle-Image komprimiert und allenfalls mit Time Machine gesichert.\n\nBis einschließlich Mac OS X Tiger 10.4.11 (2007) verwendete FileVault ein verschlüsseltes Sparse Disk Image, ab Version 10.4.7 (2006) in einer modifizierten Form, welche die Datensicherheit und Stabilität im Fall von Systemabstürzen verbesserte (Header der Verschlüsselung am Anfang statt am Ende des Sparse Disk Images gespeichert). Mit dem Umstieg auf die erwähnten Sparse Bundles wurde die Datensicherheit weiter verbessert, ebenso erlauben Sparse Bundles einen schnellen Zugriff auf die Daten und ermöglichen die differenzielle Datensicherung mit Time Machine.\n\nEin erheblicher Unterschied zwischen \"Sparse Image\" und \"Sparse Bundle\" ist die Wahl der Speicherung der verschlüsselten \"Disk-Daten\". Bei \"Sparse Image\" wird ein „einzelner“ Datenblock gespeichert. Bei \"Sparse Bundle\" wird die „übergeordnete Disk“ vom Betriebssystem in 8-MB-Teile (sog. \"Bands\") gesplittet. Dieser Vorgang erfolgt für den Benutzer transparent. Bei einem Upgrade von einer älteren System-Version auf Mac OS X Leopard (10.5, 2007) wird das ursprüngliche Disk-Format beibehalten. Um das neue Format \"Sparse Bundle\" verwenden zu können, muss für das Benutzerkonto FileVault deaktiviert und neu aktiviert werden.\n\nFileVault 2 benötigt kein \"Sparse Bundle\" mehr, sondern basiert auf dem Logical Volume Manager CoreStorage.\n\nUm FileVault 1 mit Time Machine verwenden zu können, darf der Benutzer zum Zeitpunkt der Sicherung nicht angemeldet sein. Es werden bei der Sicherung die Daten des \"Sparse-Files\" übertragen, wodurch die Datenintegrität gewahrt bleibt.\n\nWährend bei \"Sparse-Image\" die gesamte Datei übertragen werden muss, werden bei Verwendung von \"Sparse-Bundle\" nur geänderte \"Bands\" (die 8-MB-Blöcke) übertragen, was die Dauer der Sicherung erheblich verkürzt.\n\nDiese Einschränkungen gelten ab FileVault 2 ab Mac OS X Lion (10.7, 2011) nicht mehr.\n\nIn einer im Jahr 2008 durchgeführten Studie konnte durch eine Kaltstartattacke Schlüsselmaterial von FileVault ausgelesen werden.\n\n"}
{"id": "2487582", "url": "https://de.wikipedia.org/wiki?curid=2487582", "title": "SlimBrowser", "text": "SlimBrowser\n\nSlimBrowser ist ein kostenloser Webbrowser von FlashPeak, der die Trident Layout Engine von Microsoft verwendet.\n\nSlimBrowser beherrscht Tabbed Browsing und hat einen Sitzungsmanager, mit dem sich bestimmte Konstellationen von Webseiten einfach aufrufen und sich die geöffneten Seiten nach einem Absturz des Browsers wiederherstellen lassen.\n\nDie Software stellt weitreichende Konfigurationsmöglichkeiten zur Verfügung, mit denen z. B. einzelne Elemente einer Webseite (Bilder, Sounds etc.) blockiert werden oder Spuren verwischt werden können. Außerdem ist eine Suchleiste für verschiedene Suchmaschinen integriert.\n\nEbenfalls eingebaut sind:\nSeit version 6 implementiert SlimBrowser eine Multi-Prozess Architektur um Leistungsengpässe zu vermeiden und die Stabilität zu erhöhen.\n\nSlimBrowser war einer der elf Browser, die von Microsoft infolge eines Gerichtsverfahrens mit der EU unter BrowserChoice.eu als Alternative zum Internet Explorer angeboten wurden.\n\nFlashPeak hat auch zwei Klone von SlimBrowser veröffentlicht, die andere Engines als Trident von Microsoft verwenden.: den Browser Slimjet, der Chromium und SlimBoat, der WebKit verwendet, aber nicht mehr unterstützt wird.\n\n"}
{"id": "2499220", "url": "https://de.wikipedia.org/wiki?curid=2499220", "title": "Microsoft Dynamics", "text": "Microsoft Dynamics\n\nMicrosoft Dynamics besteht aus einer Reihe von Unternehmensanwendungen der Firma Microsoft. Es ersetzt die Softwarelinie der Microsoft Business Solutions.\n\nMicrosoft Dynamics umfasst folgende Software:\n\n"}
{"id": "2499599", "url": "https://de.wikipedia.org/wiki?curid=2499599", "title": "Microsoft Dynamics CRM", "text": "Microsoft Dynamics CRM\n\nMicrosoft Dynamics CRM ist eine Customer-Relationship-Management Software von Microsoft. Sie ist ein Teil der Microsoft Dynamics Unternehmenssoftwareanwendungen. Im Gegensatz zu den anderen Dynamics-Produkten basiert Dynamics CRM seit der Version 3.0 vollständig auf Microsoft .NET und lässt sich leicht in Visual Studio .NET Projekten integrieren.\n\nDie erste Version, die für Kunden zur Verfügung stand, war die Version 1.2. Die Version 2 wurde aus Marketinggründen komplett übersprungen. Ab der Version 3.0 gab es einfachere Anpassungsmöglichkeiten und auch das Drucken war jetzt möglich. Außerdem wurde Dynamics CRM komplett in Outlook integriert. E-Mails, Termine und Aufgaben lassen sich mit dem Button „Im CRM verfolgen“ ins Dynamics CRM übertragen. Auch eine Offline Synchronisation ist für Außendienstmitarbeiter vorgesehen. Dabei wird die Menge der Daten nur durch die Größe der lokalen Festplatte begrenzt. Die Begrenzung auf 2 GB, die normalerweise für Datenbanken der Microsoft SQL Server Express Edition gilt, wird durch den installierten Client aufgehoben.\nAktuell ist die Version 2016, die neben einer globalen Bereitstellung als Onlinedienst weitere tiefgreifende Erweiterungen mitgebracht hat.\n\nDie Version 2011 ist durch neue Funktionen, wie Sicherheit auf Feldebene und anderen Erweiterungen gekennzeichnet. Das vielleicht wichtigste Merkmal ist die Mehr-Instanzenfähigkeit, welche es erlaubt mehrere disjunkte Organisationseinheiten auf einem einzelnen Server unterzubringen.\nMicrosoft Dynamics CRM 2011 wurde mit folgenden Verbesserungen veröffentlicht:\n\n\nDie offensichtlichste Änderung der Version 2013 stellt die neue Benutzeroberfläche dar, die in Version 2015 weiter optimiert wurde.\n\nIn der Version 2016 wurde CRM ergänzt durch zusätzliche optionale und teilweise kostenpflichtige Add-Ons, wie den Projektservice (zur Projektakquise und -Abwicklung) und Portals (als Self-Service Portal für externe Benutzer, wie Kunden, Lieferanten, Partner oder Mitarbeiter).\n\nDynamics CRM kann auf drei verschiedene Arten genutzt werden. Der Kunde kann zum einen auf CRM Online zugreifen. Dabei wird die Software von Microsoft gehostet. Zum zweiten können auch Microsoft-Partner Dynamics CRM in einem Rechenzentrum hosten und ihren Kunden von dort aus über das Internet zur Verfügung stellen. Zum dritten lässt sich die Software auch vor Ort im Unternehmen einrichten.\n\nFür den Betrieb vor Ort wird folgende technische Infrastruktur benötigt:\n\n\n\n"}
{"id": "2503460", "url": "https://de.wikipedia.org/wiki?curid=2503460", "title": "Phoner", "text": "Phoner\n\nPhoner [] ist ein Freeware-Programm für Microsoft Betriebssysteme.\n\nPhoner ermöglicht Gesprächsverbindungen in das Festnetz, zu Mobiltelefonen und zu VoIP-Gegenstellen. Das Programm kann als Softphone an analogen Teilnehmeranschlüssen, an digitalen ISDN-Teilnehmeranschlussleitungen und als Softphone auf stationären und mobilen Computern benutzt werden. \n\nDer Eigenname ist eine Ableitung des englischen Verbs \"to phone\" [] und kann in Anlehnung an die englische Bedeutung mit \"Telefonierer\" übersetzt werden. Im sprachlichen Zusammenhang ist die Applikation ein Software-Pendant zu einem Telefon mit integrierten Aufzeichnungs-, Protokollierungs-, Verwaltungs- und Steuerungsfunktionen.\n\nAufgrund der zusätzlichen Unterstützung von IP-Telefonie-Funktionen wird seit dem 1. Juni 2005 vom Entwickler eine eigene Programmvariante mit dem Namenszusatz \"„Lite“\" [] unentgeltlich als Freeware zur Verfügung gestellt.\n\nSeit der ersten Veröffentlichung ist Phoner in den Sprachen Deutsch und Englisch verfügbar. Bulgarische, französische, italienische, griechische, niederländische, spanische und tschechische Übersetzungen sowie vom Entwickler zur Verfügung gestellte Telefonvorwahlen für die Länder Griechenland, Italien, die Niederlande, Österreich, Polen und die Schweiz weisen auf eine weite Verbreitung der Software hin. Transkriptionen der genutzten Texte in arabische Schriftzeichen oder Übersetzungen für arabische Schriftzeichen nutzende Sprachräume sind offiziell nicht verfügbar, jedoch wird die Software in diesen Sprachräumen auf Internetseiten im Zusammenhang mit dem Namen des Entwicklers erwähnt. \n\nDas Programm wurde in unterschiedlichen Fachzeitschriften im deutschsprachigen Raum vorgestellt, besprochen und empfohlen.\n\nDie Entwicklung erfolgte unabhängig von proprietären Leistungsvorgaben, und die Verwendung findenden Kommunikationsprotokolle wurden innerhalb des Rahmens ihrer Spezifikationen umgesetzt. Hierdurch soll eine hohe Kompatibilität zu verfügbaren Netztechniken sowie zu im Zusammenhang mit der Software verwendeter Endgeräte-Hardware erreicht werden.\n\nPhoner und PhonerLite [] basieren auf dem gleichen Programmcode. Der Entwickler verzichtete bei PhonerLite neben den Modi für einen ISDN-Betrieb (CAPI) und die Verwendung an analogen Teilnehmeranschlussleitungen (TAPI) auf Verwaltungs-, Steuerungs-, Protokollierungs- und Aufzeichnungsfunktionen der umfangreicheren \"Phoner\"-Variante.\n\nPhonerLite kann ausschließlich als Softphone für die IP-Telefonie (VoIP) genutzt werden. Da diese Variante über Einstellungshilfen für die Konfiguration der Verbindungsdaten verfügt und durch die Beschränkung der Verwendungsmöglichkeiten Systemressourcen schont, soll hauptsächlich Einsteigern ermöglicht werden, die IP-Telefonie zu nutzen.\n\nPhoner kann anstelle von oder, bei Installation in einem anderen Verzeichnis, gleichzeitig mit PhonerLite verwendet werden. Die Konfiguration einer genutzten \"Lite\"-Variante kann für den SIP-Betriebsmodus übernommen werden.\n\nPhoner kann in drei Betriebsmodi genutzt werden, die einen Betrieb mit systemseitig installierten Endgeräten an analogen Teilnehmeranschlussleitungen, mit Endgeräten an digitalen ISDN-Teilnehmeranschlüssen und als IP-Telefon zulassen.\n\nDie Applikation nutzt als Frontend die technischen Möglichkeiten installierter Modems, ISDN-Geräte und Netzadapter für die computergestützte Telefonie. Über eine grafische Benutzerschnittstelle (GUI), den Aufruf über die Kommandozeile sowie über eine zur Laufzeit verfügbare COM-Schnittstelle stellt das Programm Funktionen für den Aufbau oder die Entgegennahme von Gesprächsverbindungen sowie weitere Funktionen eines Telefonie-Endgeräts zur Verfügung.\n\nFür eine aktive Nutzung von Phoner als Telefonie-Software zum Aufbau und zur Entgegennahme von Gesprächsverbindungen werden eine installierte, vollduplexfähige Soundkarte und ein Mikrofon vorausgesetzt. Möglich ist auch die Verwendung eines Headsets oder eines kompatiblen USB-Telefonhörers.\n\nDie durch Gerätetreiber oder herstellerspezifische Programmbibliotheken verfügbaren Funktionen einer Kommunikations-Hardware werden über standardisierte Schnittstellen, ebenfalls Programmbibliotheken, angesprochen.\n\nDie verfügbaren Betriebsmodi können wahlweise und voneinander unabhängig genutzt werden. Sofern eine gleichzeitige Nutzung der Software in mehreren Betriebszuständen erforderlich ist, kann für jede Aufgabe eine Installation in ein eigenes Programmverzeichnis erfolgen. Es können mehrere Instanzen der Software gleichzeitig aktiv sein, um jeweils einen Betriebsmodus oder jeweils eine zugewiesene Aufgabe zu überwachen.\n\nDer TAPI-Betriebsmodus wird zum Betrieb an einer analogen Teilnehmeranschlussleitung oder analogem Endgerät verwendet. Die Kommunikation mit Modems oder anderen Endgeräten an analogen Teilnehmeranschlüssen erfolgt über eine TAPI-Schnittstelle. TAPI ist eine Programmbibliothek, die zum Betrieb von Kommunikationshardware mit einem Computersystem dient.\n\nEine Nutzung des Programms an analogen Teilnehmeranschlüssen und an ISDN-Teilnehmeranschlüssen ist seit der ersten Veröffentlichung am 1. Februar 1998 möglich.\n\nDer TAPI-Betriebsmodus wird nicht weiterentwickelt, da seit September 1995 in Deutschland flächendeckend ISDN verfügbar ist. Dieser Modus kann für die Telefonie hinter einer TAPI-fähigen Telefonanlage genutzt werden, die an einem ISDN-Basisanschluss oder -Primärmultiplexanschluss betrieben wird.\n\nDer CAPI-Betriebsmodus wird zum Betrieb an einer ISDN-Teilnehmeranschlussleitung verwendet.\nSofern ein ISDN-Basisanschluss oder -Primärmultiplexanschluss genutzt wird, und eine ISDN-Karte oder eine andere CAPI-unterstützte Hardware verfügbar ist, erfolgt die Kommunikation über eine auf dem Rechner installierte CAPI-Schnittstelle.\n\nZur Nutzung von ISDN-Funktionen und ISDN-Leistungsmerkmalen benötigt das Programm im CAPI-Modus eine Programmbibliothek mit dem Namen \"capi2032.dll\".\n\nSofern zusätzlich eine Netzwerkkarte verfügbar ist, kann das Programm in einem LAN als Frontend-Anwendung hinter einem CAPI-Server betriebssystemübergreifend verwendet werden. Der CAPI-Server stellt als unabhängige Applikation die Funktionalität einer serverseitig installierten CAPI-Programmbibliothek Client-Rechnern im Netz zur Verfügung.\n\nDieser Betriebsmodus ermöglicht die Sprach- und Datenübertragung nach den logischen Spezifikationen des seit 1991 europaweit einheitlichen ISDN-Standards DSS1. Sofern durch die genutzte Endgerät-Hardware und CAPI unterstützt, können mit Phoner außer Gesprächs- und Datenverbindungen nach dem DSS1-Standard durch den Netzanbieter bereitgestellte vermittlungstechnische Leistungsmerkmale über den ISDN-D-Kanal oder bei analogen Anschlüssen über das Mehrfrequenzwahlverfahren genutzt werden.\n\nDer VoIP-Betriebsmodus wird für IP-Telefonie nach dem Session Initiation Protocol (SIP) verwendet.\nSeit der Programmversion 1.66 ist ein Betriebsmodus für die VoIP-Telefonie nach dem SIP-Standard implementiert. Sofern eine Netzwerkkarte installiert ist, kann die Software innerhalb eines lokal begrenzten Rechnernetzes oder für Gesprächsverbindungen in andere Rechnernetze innerhalb einer IP-Netzumgebung im SIP-Modus verwendet werden.\n\nIm Programmumfang ist eine Programmbibliothek mit dem Namen \"sipper.dll\" vorhanden, die VoIP-Funktionalität nach dem SIP-Standard ähnlich einer \"capi2032.dll\" im CAPI-Modus zur Verfügung stellt.\n\nDie Implementierung vermittlungstechnischer Leistungsmerkmale für die IP-Telefonie erfolgt streng nach den Vorgaben der RFC 3261 für das Session Initiation Protocol. Hierdurch soll die technische Kompatibilität zu Unternehmen erreicht werden, die IP-Telefonie auf der Basis des Session Initiation Protocols auch für Verbindungen über Gateways in das Festnetz und in das Mobilfunknetz anbieten.\n\n\nDas Programm Phoner wurde im Februar 1998 vom Entwickler Heiko Sommerfeldt im Rahmen seiner Diplomarbeit an der Universität Ulm erstellt. In den folgenden Jahren erfolgte die Weiterentwicklung zusätzlicher Verwaltungs- Aufzeichnungs-, Protokollierungs- und Steuerungsfunktionen.\n\nMit der wachsenden Anzahl von DSL-Anschlüssen nahm die computerbasierende ISDN-Telefonie aufgrund der zusätzlich notwendigen ISDN-Hardware ab. Ein gleichzeitig wachsender Anstieg der IP-Telefonie kann darin begründet sein, dass Gesprächsverbindungen zwischen VoIP-Teilnehmern überwiegend kostenlos und unter freibleibenden vertraglichen Bindungen angeboten wurden.\n\nDas Session Initiation Protocol (SIP) setzte sich für Gesprächsverbindungen gegenüber dem H.323-Protokoll durch, wodurch Phoner und PhonerLite ausschließlich SIP für VoIP-Gesprächsverbindungen unterstützen.\n\nBasierend auf der LGPL-lizenzierten oSIP-Bibliothek entwickelte Heiko Sommerfeldt in Anlehnung an die standardisierte \"capi2032.dll\" die \"sipper.dll\", die im SIP-Betriebsmodus des Programms die VoIP-Funktionalität für Gesprächsverbindungen und Leistungsmerkmale der IP-Telefonie bereitstellt.\n\nAm 1. Juni 2005 wurde auf der Basis des gleichen Quelltextes die Programmvariante PhonerLite veröffentlicht, die ausschließlich Gesprächsverbindungen in einer IP-Netzumgebung ermöglicht.\n\nDie Entwicklung erfolgte mit der Begründung, dass PhonerLite als einfach zu bedienendes Softphone für die ausschließliche VoIP-Benutzung zur Verfügung stehen sollte, während durch die Integration des SIP-Modus in die ursprüngliche Programmvariante Phoner eine zwangsläufig komplexere Bedienung des Programms sowie aufgrund der erhöhten Konnektivität umfangreichere Einstellungsmöglichkeiten entstanden.\n\nDie Weiterentwicklung der VoIP-Funktionalität wurde auf die Übertragungssicherheit ausgedehnt. Es erfolgte die Implementierung der Verschlüsselungstechniken SRTP und TLS.\n\nDie Unterstützung von Wideband-Codecs wie G.722 und Speex-Wideband ermöglichen durch eine höhere Abtastfrequenz von 16 kHz eine bessere Sprachqualität. PhonerLite steuert auch die Soundkarte mit dieser höheren Frequenz an. \n\nIPv6 wird unterstützt\n\nNeben PhonerLite unterstützt nun auch Phoner Wideband – seit Version 2.51 nicht nur für VoIP-Verbindungen, sondern auch über ISDN.\n\nPhoner und PhonerLite unterstützen nun auch ZRTP zur Sprachverschlüsselung. Als weiterer Wideband-Codec wird CELT unterstützt.\n\nPhonerLite kann nun Google Kontakte importieren. Statt des Codecs CELT wird nun der Codec OPUS unterstützt.\n\nFür den Import der Google Kontakte wurde eine entsprechende Zwei-Faktor-Authentifizierung umgesetzt.\n\n"}
{"id": "2505060", "url": "https://de.wikipedia.org/wiki?curid=2505060", "title": "SUPER (Software)", "text": "SUPER (Software)\n\nSUPER (auch \"SUPER ©\" - Akronym für \"Simplified Universal Player Encoder & Renderer\") von eRightSoft ist ein kostenloses Video- und Audiokonvertierungsprogramm. Es nutzt die Bibliotheken des offenen FFmpeg-Projekt zur Dekodierung und Kodierung und bietet eine grafische Benutzeroberfläche, sodass Benutzer nicht auf Kommandozeilenbefehle angewiesen sind.\n\nDie Hauptfunktionen von SUPER ist die Konvertierung von Audio- und Videodateien, aber es besitzt auch die Eigenschaften eines Mediaplayers (hierzu wird der MPlayer verwendet) und kann Videostreams herunterladen. Des Weiteren kann man mit der Software des US Entwicklers eRightSoft auch die Audiospuren aus einem AVI oder MKV Container heraus streamen. Und mit der Funktion \"Mux Video & Audio Streams\" kann man ein Audiofile wieder hinzufügen, allerdings ist das auf nur ein Audiofile beschränkt. VOB Files kann SUPER ebenfalls zusammenfügen, mit der Funktion \"Join Format-Identical Files\". Somit kann man die mehreren VOB Files einer DVD besser an einem Stück neu konvertieren oder die Audiospuren an einem Stück heraus streamen. Gibt es Probleme beim Neukonvertieren, werden etwa Untertitel eingefügt, obwohl die Funktion \"Hide Subtitles\" aktiviert ist, hilft die Funktion \"DeMux Extract Streams\", damit wird der Videostream von allen anderen Streams getrennt. Auch zu laute oder zu leise Audiofiles kann man mit SUPER problemlos unter den Options neu aussteuern lassen und noch einiges mehr kann man mit der Software machen.\n\nSUPER unterstützt die gängigsten Audio- und Videoformate, wie ASF, AVI, FLV, MPEG (MPEG-1 und MPEG-2), QuickTime (MOV und QT), RealVideo (RAM und RM), Matroska, AAC, MP3, Ogg Vorbis und WAV.\n\nEs wird zudem auch oft verwendet, um Videos für mobile Endgeräte anzupassen, z. B. für Mobiltelefone (3gp), iPods und PSPs.\n\n\n\nSUPER beinhaltet Bibliotheken von einigen Open-Source-Programmen, inklusive Cygwin und libavcodec, ohne den Quellcode preiszugeben.\n\nZusätzlich sind einige der verwendeten DLLs, für die eRightSoft keine Erlaubnis hat, mit einem Copyright geschützt. Darunter sind DLLs von Firmen wie Apple, RealNetworks und Vivo Software (gehört heute zu Real Networks).\n\n"}
{"id": "2506663", "url": "https://de.wikipedia.org/wiki?curid=2506663", "title": "VMware Workstation", "text": "VMware Workstation\n\nVMware Workstation ist eine Software des US-amerikanischen Unternehmens VMware zur Virtualisierung von Computern mit x86- bzw. x86-64-Architektur. Ziel des Produktes ist es, ein oder mehrere vollwertige Desktop-Umgebungen mit anderen Betriebssystemen (sogenannte Gastsysteme) bereitzustellen, ohne das native System neu starten zu müssen, um das Betriebssystem zu wechseln. Das Produkt wird dabei auf dem \"Wirtsystem\" (dem sogenannten Hostsystem) installiert und setzt spätestens seit Version 9 auf diesem einen 64-Bit-Prozessor voraus.\n\nMit VMware Workstation kann unter den Betriebssystemen Linux und Windows sowie hierzu kompatiblen Systemen ein kompletter x86-PC bzw. x86-64-PC virtualisiert werden. Auf diesen virtuellen Systemen können unterschiedliche Betriebssysteme wie Windows, Linux und andere installiert werden. Es bestehen aber Restriktionen, abhängig von den technischen Eigenschaften des zugrunde liegenden Betriebssystems. So kann z. B. eine mit Microsoft Windows 2000 eingerichtete virtuelle Maschine, welche auf einem Rechner mit dem älteren Microsoft Windows NT 4.0 läuft, dennoch nicht auf USB-Geräte zugreifen, obwohl VMware 5.5 den USB-Zugriff teilweise ermöglicht. In der Praxis findet man oft den umgekehrten Fall, Benutzer lassen ältere Betriebssysteme innerhalb einer virtuellen Maschine auf modernen Wirtssystemen laufen.\n\nDie mit VMware Workstation virtualisierten Betriebssysteme sind prinzipbedingt etwas langsamer als vergleichbare native Installationen auf identischer Hardware.\n\nBei Einführung einer neuen Version wurde vom Hersteller bisher immer auch ein Freischaltcode für eine Testversion veröffentlicht. Dieser Code – bis Version 6 vier Gruppen von beliebigen Ziffern und Buchstaben, ab Version 7 fünf Gruppen – wechselt täglich, so dass das konkrete Programm, das mit dem Code vom Datum des Downloads freigeschaltet wurde, genau 30 Tage lang ausprobiert werden kann.\n\nEnde Oktober 2009 wurde die Version 7 veröffentlicht. Diese Version bietet u. a. Unterstützung für Windows 7 und verbesserte 3D-Unterstützung für Windows im Allgemeinen. Des Weiteren können damit beispielsweise vier Prozessoren bzw. Kerne den Gästen zugewiesen werden. Virtuelle Festplatten können mittels AES (256 Bit) verschlüsselt werden.\n\nDie Version 8 erschien am 14. September 2011. Die wichtigste Änderung besteht neben erster rudimentärer Unterstützung für Windows 8 darin, dass die VMware Workstation virtuelle Maschinen freigeben kann, damit sie sich ferngesteuert von anderen Rechnern aus nutzen lassen. Voraussetzung dafür ist auf den Clients ebenfalls eine Workstation in der Version 8. Mit diesem Merkmal kann die Workstation Aufgaben übernehmen, für die bisher häufig der kostenlose VMware Server eingesetzt wurde.\nDie Rolle als Client beschränkt sich nicht auf VMs, die von einer Workstation 8 freigegeben werden. Vielmehr kann man von der Workstation aus auch auf VMs zugreifen, die auf einem ESX(i)-Host bzw. unter vSphere laufen. Dabei unterstützt sie auch verschiedene Administrationsfunktionen, beispielsweise das Anlegen, Starten und Herunterfahren von VMs. Daher lässt sich die Workstation 8 für einige Aufgaben als Alternative zum vSphere Client heranziehen. Seit VMware Workstation 8 werden nur noch Hostrechner mit einem 64-Bit-x86-Prozessor unterstützt. Windows-Versionen bis Windows 7 bringen keine generellen USB-3.0-Treiber mit. Workstation 8 unterstützt USB-3.0-Ports, die auf NEC xHCI Chips und Treibern basieren. Nur mit NEC-basierenden USB-3.0-Ports ist bei Workstation 8 eine USB-3.0-Nutzung in einer Windows-VM möglich.\n\nAm 23. August 2012 erschien die Version 9 des Produkts. Sie unterscheidet sich in der grundlegenden Unterstützung von Windows 8 und in der Funktion einer verschlüsselten Virtuellen Maschine, wobei die Eingabe des Passwortes für die Änderung der Einstellungen benötigt wird. Ein geändertes Web-Interface namens WSX ermöglicht Benutzern den Zugriff auf ihre freigegebenen virtuellen Maschinen über den Browser von beispielsweise einem Tablet, Smartphone oder PC. Des Weiteren ist der Upload und Download von vSphere bereitgestellten virtuellen Maschinen möglich. Außerdem können mit Windows-8-VMs alle USB-3.0-Ports des Hosts genutzt werden. Die Unterstützung von Intel VT-x/EPT oder AMD-V/RVI Erweiterungen wurde gegenüber der Vorgängerversion verbessert. \"Microsoft Hyper-V\" wurde in die Liste der Gastsysteme aufgenommen. Die Benutzeroberfläche wurde vollständig überarbeitet, um den Windows-Standards zu entsprechen.\n\nEin vergleichbares Produkt für macOS auf Intel-Prozessoren ist VMware Fusion.\n\nVMware Workstation bietet drei Möglichkeiten, die Netzwerkressourcen des Hosts zu nutzen. Je nach Anforderungen wird man eine dieser Möglichkeiten auswählen.\n\n\nAnmerkung: Mehrere Gastsysteme können separate private Netze nutzen, die nur miteinander kommunizieren können, wenn es jeweils im Hostsystem explizit konfiguriert ist.\n\n\n\n"}
{"id": "2512669", "url": "https://de.wikipedia.org/wiki?curid=2512669", "title": "CPU-Z", "text": "CPU-Z\n\nCPU-Z ist ein Freeware-Systemprogramm, welches dem Benutzer ermöglicht, Betriebssystem- sowie Hardware-Daten auszulesen und anzuzeigen. In Tabs angeordnet stellt CPU-Z Daten über Prozessor, Hauptplatine, Arbeitsspeicher und Cache zusammen.\n\nCPU-Z ist eines der Standardtools seiner Art und wird auch von den Chipherstellern selbst eingesetzt. So verwendete Intel es bei der ersten Vorführung eines auf DDR3-Speicher basierenden Systems auf dem \"Intel Developer Forum\". Des Weiteren findet es in vielen Tests von Fachzeitschriften und Online-Publikationen Verwendung. Dazu gehören Hardwareartikel im Magazin \"PC Games\" und im Hardwareportal \"Tom’s Hardware Guide,\" welches eine der ältesten und bekanntesten Testseiten ist.\n\nAb Version 1.39 unterstützt \"CPU-Z\" auch das Betriebssystem \"Windows Vista\" und ab der Version 1.52 das Betriebssystem \"Windows 7\". In der letztgenannten Version hat die neue Registerkarte \"Graphics\" Einzug gefunden, die Basisinformationen über die verwendete Grafikkarte liefert. Im Juni 2013 erschien erstmals auch eine Version für Android.\n\nFolgende Informationen werden mit CPU-Z angezeigt:\n\n\n\n\n\n\nBesonders beliebt ist CPU-Z bei Übertaktern, da mit diesem Programm genauestmögliche Informationen über die Hardware herausgefunden werden können. Außerdem zeigt das Programm für Übertaktungen wichtige Daten wie beispielsweise den FSB an. Ab Version 1.40.5 wird auch der dynamische FSB von Intel-CPUs unterstützt.\n\n"}
{"id": "2515494", "url": "https://de.wikipedia.org/wiki?curid=2515494", "title": "Deshaker", "text": "Deshaker\n\nDeshaker ist eine Software, die verwackelte Videos stabilisiert. Sie ist als Plugin für das Open-Source-Videobearbeitungsprogramm VirtualDub realisiert. Deshaker ist als Freeware kostenlos verfügbar.\n\nDeshaker analysiert ein Video auf ungewünschte Kamerabewegungen (Verwackeln). Dabei werden die Einzelbilder so positioniert, dass das Motiv stillzustehen scheint. Lässt man das Video so laufen, so ist der Bildrand bei keinem Bild an derselben Position. Anschließend wird der Bildausschnitt bestimmt, der in allen Einzelbildern vorkommt. Wenn von jedem Einzelbild dieser Bildausschnitt verwendet wird, erhält man ein unverwackeltes Video mit reduzierter Bildgröße; die eigentliche Auflösung bleibt jedoch gleich.\n\nMeist ist es jedoch erforderlich, dass das Video wieder in der Originalgröße und -auflösung ausgegeben wird. Dazu unterstützt Deshaker verschiedene Möglichkeiten:\n\n\nGrundsätzlich führt das nachträgliche Entfernen von Verwacklungen immer zu einer schlechteren Qualität, als sie ein von vornherein unverwackeltes Video hätte.\n\nGründe:\n\n\nEs gibt einige kommerzielle Alternativen zu Deshaker, am bekanntesten ist Dynapel SteadyHand. Viele kommerzielle Videoschnittsysteme enthalten vergleichbare Funktionen.\n\n"}
{"id": "2520966", "url": "https://de.wikipedia.org/wiki?curid=2520966", "title": "ROWA-Verfahren", "text": "ROWA-Verfahren\n\nDas Read-One-Write-All-Verfahren ( für \"lies eins, beschreibe alle\") oder ROWA-Verfahren ist ein einfaches Verfahren zur Synchronisation replizierter Daten. Das Prinzip beruht darauf, dass Änderungen an einem Datenobjekt immer synchron auf allen Replikaten durchgeführt werden; eine Änderung kann nur dann erfolgreich sein, wenn sie auch auf allen Replikaten durchgeführt wurde.\n\nDer Vorteil des Verfahrens ist, dass dadurch immer alle Replikate auf dem gleichen Stand sind und keines veraltet ist. Für Leseoperationen ist die Verfügbarkeit eines replizierten Datenobjekts sehr hoch, weil der aktuelle Wert eines Datenobjekts verzögerungsfrei von jedem Replikat abgerufen werden kann. Die Leseverfügbarkeit steigt mit dem Replikationsgrad (Anzahl der Replikate).\n\nUm eine Schreiboperation erfolgreich durchführen zu können, müssen alle Replikate erreichbar sein. Je höher der Replikationsgrad ist, umso geringer ist aber die Wahrscheinlichkeit, dass alle Replikate für die Durchführung einer Änderung auch erreichbar sind. Deswegen besitzt das Verfahren nur eine geringe Schreibverfügbarkeit. Darüber hinaus steigt mit dem Replikationsgrad auch der Aufwand für Änderungsoperationen (zum Beispiel die Anzahl der notwendigen Nachrichten).\n\nAbhilfe für das Problem der geringen Schreibverfügbarkeit bringt das ROWAA-Verfahren. Bei ROWAA (Read-One-Write-All-Available) werden nur die tatsächlich verfügbaren Replikate geändert, so dass eine Änderung auch dann erfolgreich durchgeführt werden kann, wenn nicht alle Replikate erreichbar sind. Für Replikate, die an zwischenzeitlich ausgefallenen Rechnern gespeichert sind, werden die Änderungen protokolliert und bei einem Neustart nachgefahren. Das Verfahren erfordert allerdings einen zusätzlichen Aufwand zur Validierung. Außerdem kann die wechselseitige Konsistenz der Replikate nur bei Rechnerausfällen, nicht aber bei Netzwerkpartitionierungen gewährleistet werden, weil dann in verschiedenen Partitionen Änderungen am gleichen Datenobjekt stattfinden können, die nicht mehr synchronisiert werden können.\n\nEine weitere Möglichkeit, die Schreibverfügbarkeit zu erhöhen, besteht darin, das ROWA-Verfahren mit sogenannten Votierungsverfahren (zum Beispiel gewichtetes Votieren) zu kombinieren. Beispiele für derartige hybride Verfahren sind \"Missing Writes\" und \"Virtual Partition\".\n"}
{"id": "2527976", "url": "https://de.wikipedia.org/wiki?curid=2527976", "title": "Launchd", "text": "Launchd\n\nlaunchd ist ein einheitliches Framework zum Starten, Verwalten und Beenden von Daemons, Programmen und Shell-Skripten im Betriebssystem-Kontext. Eingeführt wurde es mit Mac OS X Tiger (10.4) und \"Darwin v8.0\". Nutzungsrechtlich steht es unter der Apache-Lizenz. Der beim US-amerikanischen Unternehmen Apple angestellte Dave Zarzycki entwickelte und pflegt derzeit launchd.\n\nDer launchd-Daemon soll folgende Funktionen übernehmen:\n\nMit Mac OS X Tiger (10.4) hat Apple die meisten Aufgaben an \"launchd\" übertragen. Durch die Vereinheitlichung des Dienste-Starten auf einem einzigen Prozess beschleunigt \"launchd\" die notwendige Startzeit, insbesondere auf langsamen Computern.\n\nDie Kernbestandteile des launchd-System sind:\n\n\"launchd\" verwaltet die „Daemons“ sowohl auf Nutzer- als auch auf Systemebene. Ähnlich \"xinetd\" kann launchd auf Anforderung „Daemons“ starten. Wie \"watchdogd\" kann auch launchd „Daemons“ überwachen und sicherstellen, dass sie immer laufen. Außerdem hat launchd \"init\" als PID 1 auf Mac OS X ersetzt und ist somit verantwortlich für den Systemstart (Bootvorgang).\n\nDie Parameter der Dienste, welche von launchd gestartet werden können, werden in Konfigurationsdateien definiert. Diese Dateien befinden sich in den Verzeichnissen \"LaunchAgents\" und \"LaunchDaemons\" des Verzeichnisses \"Library\" und basieren auf property List (plist), haben in etwa dreißig editierbare Schlüsselwerte.\n\nlaunchctl ist ein Kommandozeilen-Programm, welches die Aufgabe des Ladens und Entladens von „Daemons“ hat. Weiterhin kann es verwendet werden zum Starten und Stoppen von launchd-gesteuerten Diensten, zum Ermitteln von Statistiken über die Systemauslastung für launchd und seine Kindprozesse und schließlich zum Setzen von Umgebungsvariablen.\n\nlaunchd hat zwei Aufgaben:\n\nDer folgende Abschnitt zeigt eine vereinfachte Darstellung des Systemstarts von Mac OS X 10.4 auf einem PowerPC-Mac (auf einem Intel-Mac ersetzt EFI die Open Firmware, und codice_1 ersetzt BootX):\n\n\nIn Schritt 3 durchsucht launchd einige Verzeichnisse nach Diensten, die ausgeführt werden müssen. Es gibt hierfür zwei Verzeichnisse: Das Verzeichnis LaunchDaemons enthält Kommandos, welche als \"root\" (d. h. mit Systemverwalter-Rechten) ausgeführt werden, üblicherweise sind dies Hintergrundprozesse. Die Verzeichnisse namens LaunchAgents enthalten bestimmte Kommandos, sogenannte \"agent applications\", welche mit Nutzer-Rechten ausgeführt werden. Dies können Skripte sein oder andere Vordergrund-Kommandos (d. h. sichtbare), welche sogar eine Benutzeroberfläche haben können. Diese Verzeichnisse liegen alle in den \"Library\"-Ordnern von Mac OS X.\n\nLaunchd unterscheidet sich sehr von SystemStarter, und zwar dahingehend, dass es tatsächlich nicht notwendigerweise alle „Daemons“ beim Systemstart lädt. Die Grundidee bei launchd ist es, wie auch ähnlich bei xinetd, die „Daemons“ erst dann zu laden, wenn sie benötigt werden. Während launchd beim Systemstart die plist-Dateien mit den Kommandos durchsucht, reserviert es alle dort angeforderten (IP-)Ports und lauscht auf ihnen, d. h. wartet auf Anfragen auf diesen Ports. Wenn in der plist-Datei der Schlüssel \"OnDemand\" definiert ist, wird der „Daemon“ zu diesem Zeitpunkt noch nicht gestartet. Stattdessen „horcht“ launchd an diesem Port und startet den „Daemon“ erst, wenn er benötigt wird, und beendet ihn, wenn er nicht mehr benötigt wird. Nachdem ein „Daemon“ geladen worden ist, wird er von launchd überwacht. launchd stellt dabei sicher, dass er läuft, wann immer er auch benötigt wird. In dieser Hinsicht arbeitet lauchd wie \"watchdogd\" und stellt wie \"watchdogd\" die Anforderung an den Prozess, dass er nicht versucht, selbständig ein \"fork\" oder \"daemonize\" auszuführen. Sobald ein Prozess in den Hintergrund verschoben wird, verliert launchd die Kontrolle über ihn und versucht, ihn neu zu starten.\n\nAls Ergebnis dieses Konzepts startet Mac OS X 10.4 wesentlich schneller als seine Vorgänger. Das System braucht lediglich die „Daemons“ zu registrieren und nicht sofort zu starten. Tatsächlich ist der grafische Fortschrittsbalken beim Startvorgang des Mac lediglich ein „Placebo“-Programm  (namens WaitingForLoginWindow), welches nichts anderes zeigt als den Ablauf einer bestimmten Zeitspanne.\n\nDer am schwierigsten zu bewältigende Aspekt beim Systemstart via launchd sind die Abhängigkeiten der Dienste untereinander. Das bisherige Verfahren über \"SystemStarter\" bot ein sehr einfaches Konzept der Festlegung von Abhängigkeiten, und zwar durch die Schlüsselwörter \"Uses\", \"Requires\" und \"Provides\" in der plist-Datei eines Autostart-Objekts. Dagegen gibt es zwei Hauptstrategien, um Abhängigkeiten in MacOS 10.4 aufzulösen. Die Interprozesskommunikation ermöglicht den „Daemons“, miteinander zu kommunizieren und Abhängigkeiten auszuhandeln, oder man beobachtet Dateien oder Verzeichnispfade bezüglich Änderungen. Die Verwendung von IPC ist sehr viel geschickter und raffinierter als die Schlüsselwörter des SystemStarter-Konzepts, und dies verlangt auch mehr Arbeit bei der Programmentwicklung, aber es kann zu saubereren und schnelleren Systemstarts führen. Der \"SystemStarter\" ist eine Option, welche auch in Mac OS X Tiger (10.4) noch verfügbar ist und unterstützt wird.\n\nEine der wichtigsten Beanstandungen an der Umsetzung anderer Dienste-Verwaltungen ist es, dass sie über das System verstreut sind und es kein zentrales Administrationstool dafür gibt. Apple verwendet launchctl, um dieses Problem zu lösen.\n\nWenn dies eigenständig verwendet wird, akzeptiert \"launchctl\" Kommandos von der Kommandozeile, von der Standardeingabe, oder arbeitet interaktiv. Eine Folge von Kommandos kann dauerhaft gespeichert werden durch Verwendung der Datei codice_5 oder codice_6. In Verbindung mit sudo kann \"launchctl\" verwendet werden, um Änderungen mit globalen Auswirkungen vorzunehmen.\n\nEine Property List (plist) ist eine Dateierweiterung, die von Apple verwendet wird, um Programmeinstellungen zu speichern. Wenn dann \"launchd\" ein Verzeichnis durchsucht oder eine Aufgabe an launchd übermittelt wird, liest es die plist-Datei, die beschreibt, wie das Programm gestartet werden muss.\n\nDie nun folgende Tabelle zeigt eine Liste von häufig verwendeten Schlüsselwörtern. Für weitergehende Informationen siehe Apples \"man page\".\n\nVon einigen Leuten wird kritisiert, dass launchd zu sehr im Hinblick auf Startgeschwindigkeit und zu wenig mit dem Ziel der Korrektheit und Flexibilität entwickelt worden ist. Insbesondere sind dies:\nDies kann zu Problemen führen, wenn z. B. ein NetInfo- oder LDAP-Server für die Authentifizierung verwendet wird oder wenn das private Benutzerverzeichnis (home directory) auf einem Netzwerk-Server liegt. Denn das Anmeldefenster wird nicht blockiert, bis diese Dienste aktiv und verfügbar sind. Andererseits gilt: Wenn in dem genannten Beispiel die vom Anmeldefenster verwendeten APIs zum Ermitteln von Informationen in den Directory Services blockieren, bis die \"Directory Services\" die Verbindung zum NetInfo- oder LDAP-Server hergestellt haben oder feststellen, dass kein solcher Server verfügbar ist, und wenn der Zugriff auf das Benutzerverzeichnis blockiert wird, bis es vom Server eingehängt werden kann, dann ist das kein Problem.\n\nDie Idee dabei ist, dass ein Programm, sofern es erst laufen kann, wenn Dienst \"x\" zur Verfügung steht, solange blockiert, bis Dienst \"x\" tatsächlich zur Verfügung steht; die Abhängigkeit wird also implizit in der Software selbst festgelegt anstatt durch Konfigurationsdateien. (Man beachte, dass in Unix-ähnlichen Systemen, die nicht launchd verwenden, eine Festlegung der Startreihenfolge lediglich verhindert, dass spätere (d. h. abhängige) Dienste gestartet werden, bevor diejenigen Dienste gestartet werden, von dem ersterer abhängt. Jedoch blockiert dieses Konzept nicht notwendigerweise den späteren (abhängigen) Dienst lange genug, bis der benötigte Dienst initialisiert und bereit zur Verwendung ist.)\n\nWenn man beispielsweise zwei „Daemons“ nacheinander durch eine Konfigurationsdatei (rc file) startet, könnte es passieren, dass der zweite Dienst Funktionen des ersteren benötigt, bevor dieser seinen Startvorgang beendet hat.\n\n"}
{"id": "2536918", "url": "https://de.wikipedia.org/wiki?curid=2536918", "title": "QuarkImmedia", "text": "QuarkImmedia\n\nQuarkImmedia war ein Autorensystem innerhalb von QuarkXPress, das es Designern ermöglichte, interaktive Multimediapräsentationen innerhalb zu erstellen und diese für CD-ROM zu exportieren und – mit Einschränkungen – auch fürs Web.\n\nDie erste Version von QuarkImmedia wurde 1996 vom Unternehmen Quark Inc. ausgeliefert.\n\nQuarkImmedia war technisch eine XTension, also ein Plug-in für das DTP-Layoutprogramm QuarkXPress, die es Designer ermöglichte, direkt in dem Programm, das sie bereits kannten, interaktive Präsentationen zu erstellen.\n\nDie Idee war, dass, statt Programmiersprachen wie Lingo (Macromedia Director) oder ActionScript (Flash) zu lernen, der Benutzer Elemente wie Animationen, Schaltflächen, Sounds, Videos etc. direkt in QuarkXPress definiert und mittels vorgefertigter Aktionen animiert bzw. interaktiviert. So konnten auch Designer Interaktivität im Web oder offline erstellen ohne Programmieren oder Coden lernen zu müssen.\n\nDas Autorenwerkzeug gab es nur für QuarkXPress unter Mac OS (Classic), nie für Windows. Abgespielt werden konnte IMD auf beiden Plattformen.\n\nQuarkImmedia kostete damals ca. 2.000 DM (knapp 1.023 Euro), also so viel wie QuarkXPress selbst.\n\n1999 hat Quark das Programm eingestellt. 2006 hat Quark ein konzeptionell ähnliches Programm wie QuarkImmedia mit dem Quark Interactive Designer vorgestellt, das allerdings statt IMD nun SWF (Flash) bzw. einen Projektor exportiert.\n\nNach Fertigstellung wurde das interaktive Layout aus QuarkXPress in ein proprietäres Format (IMD) exportiert, das dann vom QuarkImmedia Player abgespielt wurde. Es gab verschiedene Exportformate, für CD-ROM und für Internet. Während das CD-ROM Format eine IMD Datei erzeugte, bestand das Internetformat aus einer kleinen IMD-Datei, dem Loader und vielen Ressourcendateien, die dynamisch bei Bedarf nachgeladen wurden.\n\nDen Player benötigte man, um exportierte Präsentationen (IMDs) abzuspielen. Optional konnte man auch den Player ins IMD einbetten und so Anwendungen (APPs, EXEs) erzeugen, ähnlich wie Flash das für Projektoren ermöglicht.\n\nDen Player gab es anfangs nur für Mac OS Classic (68k und PPC), später auch für Windows (16- und 32-Bit), und durfte frei verbreitet werden.\n\nDer Player holte sich das IMD entweder lokal vom Filesystem oder über TCP/IP (z. B. übers Internet). Der Player war kein Browser plug-in und wurde über MIME-Types aus dem Browser aufgerufen.\n\nBekannteste Anwendungen, erstellt mit QuarkImmedia, waren:\n\n\n\n"}
{"id": "2537851", "url": "https://de.wikipedia.org/wiki?curid=2537851", "title": "Gemulator", "text": "Gemulator\n\nGemulator ist ein Emulator, mit dem klassische Macintosh-, Atari-ST- und Atari-8-bit-Programme auf einem Rechner mit dem Betriebssystem Windows laufen. Die erste Version von Gemulator wurde 1992 veröffentlicht.\n\nDie Weiterentwicklung des Emulators wurde zunächst am 1. Juli 2004 eingestellt. Seit Juni 2007 hat der Programmierer die Arbeit an dem Projekt wieder aufgenommen. Ende 2008 wurde die Version 9.0 veröffentlicht.\n\n"}
{"id": "2543656", "url": "https://de.wikipedia.org/wiki?curid=2543656", "title": "IBM LUM", "text": "IBM LUM\n\nIBM License Use Management (IBM LUM) ist eine Lizenzmanagementsoftware der Firma IBM. \"IBM LUM für AIX\" wird als Standardkomponente des Betriebssystems AIX ausgeliefert und ist auch für viele andere Unix-Betriebssysteme und für Windows verfügbar. CATIA ist die vermutlich am häufigsten mit diesem Lizenzmanager zusammen verwendete Anwendungssoftware. \n\n"}
{"id": "2544933", "url": "https://de.wikipedia.org/wiki?curid=2544933", "title": "Shared Nothing Architecture", "text": "Shared Nothing Architecture\n\nDie Shared-Nothing-Architektur (oder kurz Shared Nothing, Abk. SN) beschreibt eine Architektur eines verteilten Systems, bei der jeder Knoten unabhängig und eigenständig seine Aufgaben mit seinem eigenen Prozessor und den zugeordneten Speicherkomponenten wie Festplatte und Hauptspeicher erfüllen kann und kein bestimmter, einzelner Knoten für die Verbindung zu einer Datenbank notwendig ist. Die Knoten sind über ein LAN- oder WAN-Netzwerk miteinander verbunden. Jeder Knoten verfügt darüber hinaus über eine Kopie des Datenbank-Management-Systems, der Knoten kann Aufgaben an einen anderen nicht ausgelasteten Knoten weitergeben.\n\nShared-Nothing-Systeme werden oft im Gegensatz zu Systemen verstanden, die eine Vielzahl von Statusinformationen zentral speichern, wie etwa Datenbank- oder Anwendungsserver. Wenn auch der Begriff \"Shared Nothing\" erst im Zusammenhang mit der Weiterentwicklung des World Wide Web in einem größeren Kreis bekannt wurde, so war das Konzept schon weit vorher verwendet worden. Michael Stonebraker erwähnte den Begriff bereits in einer Veröffentlichung zu Datenbanken im Jahre 1986 an der University of California in Berkeley, es ist jedoch durchaus möglich, dass der Begriff schon früher geprägt wurde.\n\n\"Shared Nothing\" ist auf Grund seiner Skalierbarkeit beliebt für Webanwendungen oder parallele Datenbanksysteme. Wie bei Google gezeigt werden konnte, ist ein Shared-Nothing-System nahezu unbegrenzt durch Ergänzung zusätzlicher Knoten in Form preiswerter Computer ausbaubar, weil kein einzelnes Netzwerkelement existiert, dessen begrenzte Leistung die Geschwindigkeit des gesamten Systems vermindert.\n\nEin Shared-Nothing-System kann seine Daten auf viele verschiedene Knoten aufteilen, zum Beispiel durch Zuordnung bestimmter Knoten zu bestimmten Anwendern oder zu bestimmten Webabfragen, oder es kann von jedem Knoten fordern, seine eigene Kopie der Anwendungsdaten zu halten, was aber die Verwendung eines Koordinierungsprotokolls-/Verfahrens zwischen den Knoten notwendig macht. \n\nEs gibt einige Diskussionen darüber, ob eine Webanwendung mit verschiedenen, unabhängigen Netzwerkrechnern, aber einer einzigen, zentralen Datenbank, z. B. in Form eines Computerclusters als ein Shared-Nothing-System betrachtet werden soll.\n\nAndere Architekturen mit davon abweichender Auslegung sind:\n\n"}
{"id": "2545115", "url": "https://de.wikipedia.org/wiki?curid=2545115", "title": "Elmer (Software)", "text": "Elmer (Software)\n\nElmer ist ein freies Finite-Elemente-Programm, mit dem strukturmechanische Simulationen und numerische Strömungssimulationen berechnet werden können. Es wird als freie Software unter der GNU General Public License (GPL, Version 2) verbreitet. Elmer eignet sich dazu, zwei- oder dreidimensionale Finite-Elemente-Analysen durchzuführen. Der Präprozessor ist in der Lage, das Eingabeformat vom Gambit Netzgenerator der Firma \"Fluent\" und weiteren kommerziellen oder freien Präprozessoren für den Gleichungslöser zu konvertieren. Es lassen sich daher zahlreiche Prä- und Postprozessoren nutzen, obwohl zu dem Softwarepaket ein eigener Prä- und Postprozessor gehören.\n\nElmer wurde mit Fördergeldern der finnischen Regierung vom CSC, dem finnischen Computerwissenschaftszentrum, in Kooperation mit finnischen Universitäten und Unternehmen entwickelt.\n\n"}
{"id": "2547715", "url": "https://de.wikipedia.org/wiki?curid=2547715", "title": "X-CD-Roast", "text": "X-CD-Roast\n\nX-CD-Roast war eines der ersten CD-Brennprogramme mit grafischer Oberfläche auf Unix-Systemen.\n\nX-CD-Roast ist ein Frontend für die cdrtools und unterstützt dadurch sowohl SCSI- als auch IDE-CD-Brenner auf allen aktuellen Betriebssystemen. Die meisten der im Handel erhältlichen Geräte werden von X-CD-Roast automatisch erkannt.\n\n\n"}
{"id": "2555883", "url": "https://de.wikipedia.org/wiki?curid=2555883", "title": "FreeWRT", "text": "FreeWRT\n\nFreeWRT war eine Linux-Distribution, die in eingebetteten Systemen wie WLAN-Geräten der Unternehmen Linksys und Asus eingesetzt wurde. Es handelte sich um eine Abspaltung des OpenWrt-Projekts. FreeWRT wird aktuell nicht mehr weiterentwickelt.\n\nDas Projekt trägt den gleichen Namen wie ein anderes, inzwischen eingestelltes russisches Projekt, das die Sveasoft-Firmware kopierte und frei zugänglich machte.\n\nDas FreeWRT-Projekt verfolgte etwas andere Ziele als das OpenWrt-Projekt. Ziel der FreeWRT-Entwickler war die Ausrichtung der Distribution auf professionellen Einsatz im Unternehmensumfeld. Der Fork des OpenWrt-Projekts entstand, um den jeweiligen Interessen und Fähigkeiten der FreeWrt-Entwickler Rechnung zu tragen und weil eine entsprechende Lösung innerhalb von OpenWrt aus ihrer Sicht nicht möglich war. Da zwei der Initiatoren BSD-Nutzer, beziehungsweise -Entwickler waren, hatte insbesondere die Umstellung auf ein Build-System, welches BSD-Ports ähnelte, hohe Priorität.\n\nDie Entwickler widmeten sich vergleichsweise wenigen Modellen, u. a. spezialisierten sie sich auf die Unterstützung der Modelle der Produktfamilien Linksys WRT54G (L/S/3G), Asus WL500g (Deluxe/Premium) und Netgear WGT634u, dafür wurden diese Modelle jedoch umfassender unterstützt. Ein weiteres Ziel des Projektes war es, regelmäßig stabile Versionen zu veröffentlichen, die aktuelle, gut ausgestattete Hardware unterstützten. Zum professionellen Konzept gehörte es auch den Kontakt zu den Hardwareherstellern zu pflegen, um dort Einfluss auf die Produktentwicklung zu nehmen.\n\nDa Linksys für deren Router das unter der GNU General Public License im Quelltext frei verfügbare Linux nutzt und modifiziert, bestand gemäß dieser Lizenz ebenfalls die Verpflichtung, die Firmware wiederum frei zu veröffentlichen. Dadurch war es möglich, das Betriebssystem des Routers zu modifizieren, weiterzuentwickeln und als OpenWrt/FreeWRT zu veröffentlichen.\n\nDie FreeWRT-Distribution erlaubte eine Handhabung, wie sie für Linux-Systeme typisch ist. Damit können auf WLAN-Geräten auch nicht vom Hersteller vorgesehene Funktionen genutzt werden. Die Konfiguration erfolgte über SSH. Durch eine Kooperation mit dem Tntnet-Projekt waren die ersten Konzepte für ein modulares und effizientes Webinterface fertiggestellt. In der Version FreeWRT 1.1 sollte es die ersten Ansätze für Webinterface Frameworks (C++/PHP) geben.\n\nAuf der Projekthomepage war es zudem möglich, sich online eine individuelle Softwarekonfiguration zu bauen, welche dann automatisiert erstellt und als Paket zum Herunterladen angeboten wurde.\n\n"}
{"id": "2556715", "url": "https://de.wikipedia.org/wiki?curid=2556715", "title": "Card 1", "text": "Card 1\n\nDie CAD-Software card_1 ist ein professionelles (deutsch/englisch) System für Planungen im Tief- und Straßenbau, besonders für BIM Verkehrswege.\n\ncard_1 dient der Planung von klassifizierten Straßen (Autobahnen, Bundes-, Landes- und Kreisstraßen) sowie anderen Straßen, Wegen und Plätzen und ihren Verknüpfungen (Knotenpunkte).\nEs wird auch zur Planung von Schienenverkehrswegen (DB AG, Straßenbahn) und zur Trassierung von Magnetschwebebahnen benutzt.\n\nDas Programmsystem ist plattformunabhängig und zeichnet sich durch eine einheitliche Bedienoberfläche aus. Integriert sind Module für die Vermessung, Deckenbuchberechnung, den Grunderwerb, die Entwässerungsberechnung, die Planung von Kanal- und Leitungsnetzen. Es können auch Bebauungspläne und Flächennutzungspläne erstellt werden.\n\nIm System gibt es eine Vielzahl an Schnittstellen für den Im- und Export, u. a. OKSTRA, LandXML, VERM.ESN, ISYBAU, DXF/DWG, BIM (CPIXML, IFC), GIS, (CityGML,ALKIS, Shape, OSM).\n\ncard_1 wird an verschiedenen Hochschulen als Schulungssoftware verwendet.<ref name=\"CARD/1\">Referenzen laut Herstellerangaben</ref>\n\ncard_1 wird von der IB&T Software GmbH entwickelt und vertrieben. Sitz der GmbH ist Norderstedt.\n\nDie card_1 Software wurde für PC entwickelt und läuft unter MS-Windows (Windows 7, Windows 8, Windows 10). Es sind Stand-alone als auch Netzwerklösungen möglich. Seit September 2018 ist card_1 in der Version 9.1 auf dem Markt, aber auch ältere Versionen sind noch in Benutzung.\n\n\n"}
{"id": "2558027", "url": "https://de.wikipedia.org/wiki?curid=2558027", "title": "Befehlstaste", "text": "Befehlstaste\n\nDie Befehlstaste ist eine Taste auf Tastaturen von Apple-Computern.\nUnter Mac-Anwendern wird sie auch \"Apfeltaste\" genannt. Umgangssprachlich finden sich auch Bezeichnungen wie \"Propeller-\", \"Kleeblatt-\" oder \"Blumenkohltaste\".\nIm Text wird sie häufig mit folgenden Symbolen dargestellt: , , .\n\nGegenwärtig (Stand Mai 2017) haben Tastaturen des Herstellers Apple in der deutschen Variante die Beschriftung sowie in der Variante US Englisch die Beschriftung .\n\nAuf Apples frühesten Computermodellen, dem Apple I von 1976, dem Apple II von 1977 und dem Apple II+ von 1979 gibt es eine solche Taste nicht. Sie taucht erstmals auf den 1980 erschienenen Apple III und den 1983 bzw. 1984 erschienenen Apple IIe und Apple IIc\n\nauf. Diese Rechner haben zwei solche Tasten, die offener und geschlossener Apfel genannt werden. Sie sind auf den Tasten durch einen nur im Umriss gezeichneten und einen voll ausgefüllten Apfel dargestellt. Die Tasten waren intern genauso verdrahtet wie die beiden Tasten am Joystick (dies weil die Apple-Tastatur nicht mit Scancodes, sondern direkt mit ASCII-Codes arbeitete, und den Joystick-Tasten kein ASCII-Code zugeordnet werden kann).\nDie Apple Lisa hatte nur eine solche Taste, mit dem „geschlossenen“ Apfelsymbol. Aus dem geschlossenen Apfel wurde später beim Macintosh die Wahltaste.\n1984 erschien mit dem Apple Macintosh zum ersten Mal das noch heute verwendete Schleifenquadrat (⌘) auf der Taste, zunächst ohne das Firmenlogo, den Apfel. Zuerst sollte durch die Verwendung eines speziellen, auf dem Bildschirm für keinen anderen Zweck verwendeten Symbols die eindeutige Zuordnung von Tastaturbefehlen und Menüpunkten erreicht werden. Die Verwendung des Firmenlogos zur Kennzeichnung von Tastaturkürzeln auf dem Bildschirm gefiel Apple-Mitbegründer Steve Jobs wegen der allzu häufigen Verwendung nicht; er fürchtete eine Entwertung des Logos durch inflationären Gebrauch. Daraufhin fand die bei Apple tätige Designerin Susan Kare das schwedische Hinweiszeichen für Sehenswürdigkeiten in einem internationalen Symbollexikon. Dieses Zeichen ist seit 2012 (Amendment 1 des Standards ISO/IEC 9995-7:2009) international als Tastatursymbol „operating system key“ für eine Taste genormt, deren Funktion dem verwendeten Betriebssystem freigestellt ist.\n\nIm Jahre 1986 führte Apple den Apple IIgs ein, der erstmals über den neuen Apple Desktop Bus zum Tastatur- und Mausanschluss verfügte. Denselben Bus wiesen ab diesem Zeitpunkt auch die neuen Macintosh-Modelle auf. Apples Tastaturen wurden dadurch austauschbar zwischen der Apple-II-Baureihe und der Macintosh-Baureihe. Die selten verwendete geschlossene Apfeltaste wurde auf dem IIgs wie zuvor auf dem Macintosh in „Option“ umbenannt, aber die offene Apfeltaste musste erhalten bleiben, da viele Apple-II-Programme diese verwendeten und auch so benannten; andernfalls wäre Verwirrung der Benutzer vorbestimmt gewesen. Daher erhielten die neuen Tastaturen auf ihrer Befehlstaste beide Symbole, den „offenen Apfel“ der II-Reihe und auch das „Kleeblatt“ der Mac-Reihe. Diese Kombination blieb auch erhalten, als die Apple-II-Reihe 1993 eingestellt wurde.\n\nJe nach Modell verfügt der Computer über eine oder zwei der Tasten, jeweils neben der Leertaste. Die Befehlstaste dient bei Betriebssystemen von Apple zum Aufrufen von Funktionen per Tastatur als Bestandteil von Tastenkombinationen \n\nGenerell dient die Befehlstaste der Einleitung von Befehlssequenzen gleich welcher Art. So ist sie die Basis für (fast) alle Tastenkombinationen am Macintosh.\n\nViele Anwender, die mehrere Jahre und nahezu täglich mit dem Macintosh arbeiten, verwenden fast ausschließlich Tastenkombinationen, darunter auch viele mit der Befehlstaste. Damit lässt sich meist eine wesentlich höhere Arbeitsgeschwindigkeit erreichen als mit der Maus.\n\nDas Mac OS lässt sich fast ganz ohne Maus steuern und bietet für praktisch jede Aktion Tastenkombinationen. Durch die von Apple in den frühen 1980er-Jahren definierten Human Interface Guideline, als Vorgabe für die Gestaltung von Benutzeroberflächen, ändern sich diese Tastenkombinationen auch nicht. So ist beispielsweise die wohl bekannteste Tastenkombination + immer noch aktuell und führte schon 1984 auf den ersten Macintosh zum Beenden des aktuellen Programms.\n\nIn Texten lässt sich die Befehlstaste zur schnellen Navigation mit der Eingabemarke nutzen:\n\n\nIm Finder dient die Befehlstaste zur schnellen Navigation in der (hierarchischen) Ordnerstruktur.\nBei Verzeichnissen („Ordnern“):\n\nBei der Anwendung auf Dateien:\n\n\nIm Zuge der Modelleinführung eines neuen iMacs am 7. August 2007 wurde auch eine neue Tastatur, schlicht Apple Keyboard genannt, vorgestellt, welche diese alte Symbolkombination nicht mehr bot.\nNach über 21 Jahren der Nutzung seitens der Mac-Anwender wurde das für diese Taste so typische und namensgebende Firmenlogo aufgegeben und auf deutschen Tastaturen durch „cmd“ ersetzt, innerhalb der USA durch das ausgeschriebene Wort „Command“.\nAndere Modelle folgten dieser Änderung mit der jeweiligen Hardwareaktualisierung.\nDie Entfernung des Apple-Logos löste einigen Wirbel und eine Online-Protestwelle bei eingefleischten Fans der Apple-Produkte aus, die darin eine unnötige Fortnahme eines liebgewonnenen Designelements sahen.\n\nAuch bei Tastaturen von NeXT gab es die Befehlstaste und teilweise mit dem Mac ähnliche Tastenkombinationen. Auch hier wurde Command ausgeschrieben.\n\nDie Symbole von den alten Tastaturen, der Apfel als Umriss und der ausgefüllte Apfel, wurden von der Linux Assigned Names and Numbers Authority den Codepoints U+F812 bzw. U+F813 in der Zone für private Nutzung der Unicode-Tabelle zugeordnet. Apple selbst hat sein ausgefülltes Logo dagegen in eigenen Schriftarten auf die letzte Stelle dieser Zone, U+F8FF, gelegt.\n\n"}
{"id": "2558029", "url": "https://de.wikipedia.org/wiki?curid=2558029", "title": "Schleifenquadrat", "text": "Schleifenquadrat\n\nDas Schleifenquadrat (⌘), auch Tristramsknoten (nach einer Alternativbezeichnung des ähnlichen \"Bowen-Knotens\" aus der englischen Wappenkunde), umgangssprachlich auch \"Propeller,\" \"Kleeblatt\" oder \"Blumenkohl\", ist ein zumeist als Symbol gebrauchtes Ornament sowie ein auf Computersystemen verfügbares Schriftzeichen.\n\nIn Entsprechung zu Bezeichnungen in skandinavischen Sprachen (wie dänisch \"\") findet sich auch die Bezeichnung \"Johannskreuz\", die auf die Verwendung zur Kennzeichnung des Johannistages in mittelalterlichen Kalendern zurückgehen soll. Dieser Begriff bezeichnet jedoch in der Heraldik eine grundsätzlich andere Figur (siehe \"Johanneskreuz\").\n\nIn Unicode ist das Schleifenquadrat als U+2318 („Sehenswürdigkeitssymbol“) im Block \"Verschiedene technische Zeichen\" enthalten.\n\nDas Schleifenquadrat besteht aus einem liegenden Quadrat (also einem mit horizontaler Unterkante), dessen Kanten über die Eckpunkte hinaus je gleich weit gerade verlängert und auswärts von jeder Ecke mit je einem Dreiviertelkreis knickfrei verbunden sind. Es kann in einem Zug gezeichnet werden; beim Gebrauch als Ornament ist häufig der Linienzugverlauf durch Betonung des „später“ gezeichneten Teilzuges an den Kreuzungspunkten sichtbar, dabei ist die Drehrichtung der Schleifen stets gleich.\n\nDer Ursprung des Symbols im europäischen Kulturkreis ist wahrscheinlich die Verwendung als Ornament auf Kunst- und Gebrauchsgegenständen im finnischen und wikingischen Kulturraum. In Schweden ist der etwa 400–600 n. Chr. geschaffene Bildstein aus Stora Havor (Gotland) ein bedeutendes Fundstück. Eines der ältesten Beispiele aus dem finnischen Raum ist ein Paar etwa 1000 Jahre alter mit diesem Symbol verzierter Holz-Skier.\n\nAuf Münzen findet es sich beispielsweise auf der Rückseite der 1963–1990 geprägten finnischen Fünf-Penniä-Münze.\n\nAußerhalb Europas findet sich das Ornament beispielsweise auf Fundstücken der Mississippi-Kultur, in der das Symbol aus zumeist vier, häufig auch drei parallelen Linienzügen gebildet wird.\n\nEin verwandtes Ornament ist der Schildknoten in seiner Grundform. Hier sind die Schleifenränder in einen kreisförmigen Umriss eingepasst.\n\nIn der englischen Heraldik ist das Symbol eine von mehreren unterschiedlichen Darstellungsformen des Bowen-Knotens. Außerhalb des britischen Raums kommt das Symbol, auch um 45° gedreht, gelegentlich als gemeine Figur in Wappen vor, ohne dass sich eine einheitliche Benennung eingebürgert hat.\n\nEine ähnliche gemeine Figur ist das Fensterrautenkreuz oder \"Würfelknotenkreuz\". Hier sind die „Schleifen“ eckig statt rund.\n\nFür andere ähnliche Figuren finden sich in der Heraldik keine speziellen Namen, sie sind daher in der Blasonierung detailliert zu beschreiben. In Einzelfällen können solche Varianten, speziell wenn sie nur gerade Striche enthalten, zu den Hausmarken gezählt werden. \nDas Symbol wurde Anfang der 1950er Jahre zuerst von finnischen Heimstättenverbänden zur Kennzeichnung von Kulturdenkmälern vorgeschlagen und seit den späten 1960er Jahren in den skandinavischen Staaten als Verkehrszeichen verwendet, um auf historische Fundstätten oder auf Sehenswürdigkeiten allgemein hinzuweisen. Auch in anderen Staaten, beispielsweise Deutschland, wird es auf Hinweiszeichen vor allem für vor- und frühgeschichtliche Sehenswürdigkeiten verwendet.\n\nDas Schleifenquadrat wird seit 1984 vom Computerhersteller Apple in den Computermenüs der Macintosh-Rechner und auf deren Tastaturen zur Kennzeichnung der Befehlstaste verwendet, um durch ein spezielles Symbol eine eindeutige Zuordnung von Tastaturbefehlen und Menüpunkten zu erreichen. Diese Verwendung geht auf die seinerzeit bei Apple tätige Designerin Susan Kare zurück. Als Steve Jobs die Verwendung des Apple-Logos dort nicht wollte, da er eine Entwertung des Logos durch inflationären Gebrauch befürchtete, hat die Designerin das schwedische Hinweiszeichen für Sehenswürdigkeiten in einem internationalen Symbollexikon gefunden.\n\nDas Schleifenquadrat ist seit 2012 als Tastatursymbol genormt:\n"}
{"id": "2559947", "url": "https://de.wikipedia.org/wiki?curid=2559947", "title": "European Advanced Multilingual Information System", "text": "European Advanced Multilingual Information System\n\nDas European Advanced Multilingual Information System, Akronym EURAMIS, wurde als Übersetzungsunterstützungssystem für die Generaldirektion Übersetzung der Europäischen Kommission entwickelt. Die Softwareentwicklung begann im Jahr 1995, das System wird aber weiterhin angepasst und erweitert.\n\nDie Generaldirektion Übersetzung der Europäischen Kommission (DGT) musste zu Beginn der Entwicklung von EURAMIS hauptsächlich in die 11 damaligen Amtssprachen übersetzen. Mit den Erweiterungen der Europäischen Union müssen nun insgesamt 24 Amtssprachen und wichtige Sprachen von Handelspartnern (Russisch, Arabisch, Chinesisch und Türkisch) unterstützt werden.\n\nEURAMIS zielt darauf ab, den Übersetzern möglichst viele für die Übersetzung relevante (oder nützliche) Daten in einer Art One-Stop-Shop zur Verfügung zu stellen, damit diese Daten so weit wie möglich wiederverwendet werden können. Dies dient vor allem dazu, die Qualität und Kohärenz der Übersetzungen zu erhöhen, aber auch den Übersetzungsprozess zu optimieren. Dies ist bei der Menge der vorhandenen Daten und der Anzahl der benötigten Sprachenpaare mit den kommerziellen Systemen bis heute nur beschränkt möglich.\n\nEURAMIS wird innerhalb der DGT als Hilfsmittel für die Übersetzung der überwiegenden Mehrheit der Dokumente benutzt. Fast alle Übersetzungsdienste der anderen europäischen Institutionen nutzen EURAMIS vor allem im Zusammenhang mit dem Gesetzgebungsverfahren (Europäisches Parlament, Rat der Europäischen Union, Gerichtshof der Europäischen Gemeinschaften, Europäischer Rechnungshof, Europäischer Wirtschafts- und Sozialausschuss, Ausschuss der Regionen); außerdem wird EURAMIS vom Übersetzungszentrum für die Einrichtungen der Europäischen Union genutzt.\n\nEURAMIS war eines der ersten Übersetzungsunterstützungssysteme, die konsequent als Client-Server-Architektur umgesetzt wurden. Nachdem die Architektur ursprünglich zweischichtig war, ist sie inzwischen dreischichtig, d. h., sie hat eine eigene Datenschicht.\n\nWegen des hohen Bedarfs an Ressourcen bei der Verarbeitung sprachlicher Massendaten werden die meisten Dienste und Funktionen nur in Stapelverarbeitung angeboten: die zu behandelnden Daten werden dem System übergeben, das nach einiger Zeit die angeforderten Ergebnisse zurückschickt. Wo dies sinnvoll und möglich ist, werden vereinzelt auch Online-Dienste angeboten.\n\nEURAMIS war von Anfang multilingual ausgerichtet, es ist also nicht nur sprachenpaarbezogen, sondern es nutzt eine sprachen-übergreifende Datenstruktur, um zwischen allen Amtssprachen der Europäischen Union zu übersetzen. Da die verschiedenen Amtssprachen eine Reihe von spezifischen Sonderzeichen haben, lag es auf der Hand, Unicode durchgehend zur Zeichendarstellung zu verwenden. Auch damit war das System ein Vorreiter in diesem Bereich.\n\nDer zentrale Übersetzungsspeicher ist das Kernstück von EURAMIS. Er enthält fast 1,2 Milliarden Sätze in allen Amtssprachen. Da für jedes Dokument immer alle Sätze gespeichert werden, handelt es sich um etwa halb so viele Typen wie Tokens, d. h., die Zahl der verschiedenen Sätze ist etwa halb so groß wie die Gesamtzahl der Sätze.\n\nDie Verwaltung der multilingualen Daten in einer relationalen Datenbank ermöglicht Funktionalitäten, die in anderen Übersetzungsspeichern nicht verwirklicht sind. Wurde etwa ein Dokument in mehrere Sprachen übersetzt (z. B. aus dem Englischen ins Französische und Deutsche), so werden die Daten – ohne Duplizierung – auch bei der Suche in der umgekehrten Übersetzungsrichtung berücksichtigt, und sogar bei der Suche zwischen den ursprünglichen Zielsprachen (hier zwischen Deutsch und Französisch). Dies ist besonders deshalb von Vorteil, weil sich die Ausgangssprache eines Dokuments von Fassung zu Fassung ändern kann.\n\nWeiterhin besteht z. B. ein virtuelles Dokument aus der Gesamtheit der Sätze (einer Sprache), die die gleiche Meta-Information haben, ohne dass dafür eine physische Unterteilung notwendig wäre. Der Übersetzungsspeicher kann in seiner Gesamtheit, aber auch in seinen virtuellen Unterstrukturen angesprochen werden. Dies wird z. B. dazu genutzt, Einträgen aus verbindlichen Dokumenten Vorrang zu geben.\n\nDie wichtigsten Anwendungen innerhalb von EURAMIS sind\n\n\n\n\n\n\n\n\nFalls eine Weitergabe von Daten zwischen den einzelnen Modulen bzw. Anwendungen notwendig ist, erfolgt diese über eine SGML-basierte sog. Pivot-Datei, in der die aufgerufenen Komponenten ihre Ergebnisse hinzufügen. Am Ende des Prozesses werden die benötigten Informationen herausgefiltert und in das gewünschte Format konvertiert.\n\nDer Zugriff von Client Seite erfolgt über einen Webbrowser. Einige Online-Anwendungen (z. B. Konkordanz) befinden sich auf dem Webserver und interagieren unmittelbar mit der Datenbank.\n\nEinige Anwendungen werden auf einem zentralen Windows-Server (z. B. Konvertierung von proprietären Formaten nach RTF oder Vorbereitung von Dokumenten für das Alignment) durchgeführt.\n\nDie Webschnittstelle bietet Zugang zu etwa 40 Menüs, mit denen die Benutzer ihre Anfragen oder Aufträge formulieren können. Eine Mehrzahl dieser Menüs werden auch in Form von Webservices angeboten. Mit dieser Schnittstelle kann der Benutzer entweder interaktive Abfragen formulieren, oder aber Dokumente zur Stapelverarbeitung übergeben.\n\nDie Übersetzer verwenden die von EURAMIS gelieferten Ergebnisse mit einer Desktop-Software als Frontend (zurzeit das kommerzielle Produkt SDL Trados Studio und die Open-Source Software OmegaT; Datenaustausch über TMX-Dateien), oder aber ausschließlich in einer HTML-Seite, in dem Informationen über die Trefferquote farbig darstellt und Meta-Informationen (Herkunft des gefundenen Satzes, z. B. Dokumentnummer, Auftraggeber) in Form von Kommentaren dargestellt sind. Die zuerst beschriebene Vorgehensweise wird im Allgemeinen bevorzugt, da durch die interaktive Nutzung eines Übersetzungsspeichers auch Wiederholungen oder Ähnlichkeiten innerhalb desselben Dokuments ausgenutzt werden können.\n\nEin Windows-basierter Editor erlaubt die Korrektur etwaiger Fehler in Alignments. Die Darstellung erfolgt in Tabellenform: der Benutzer kann das alignierte Dokument durchsehen und muss nur dort eingreifen, wo Fehler zu beheben sind. Der Alignment-Editor verfügt über eine Reihe von Hilfsfunktionen, z. B. Suchen und Ersetzen, Rechtschreibfehlererkennung und Bearbeitung der Meta-Informationen.\n\nInnerhalb der DGT sind viele dieser Schritte automatisiert: für alle elektronisch eingehenden Übersetzungsaufträge wird eine Behandlung mit Standard-Parametern ausgelöst, die Ergebnisse werden innerhalb eines Workflow-Systems den Übersetzern bereitgestellt. Sofern sie mit Hilfe des Frontends angefertigt wurden, werden die Übersetzungen automatisch in den Übersetzungsspeicher eingestellt, ansonsten erfolgt meist ein Alignment durch eine Hilfskraft.\n\nEine ähnliche automatisierte Einbindung in den Arbeitsprozess findet derzeit bei den anderen teilnehmenden Institutionen unter Zuhilfenahme der Webservices statt.\n\nWährend die Nutzung von EURAMIS sich innerhalb der Europäischen Kommission vor allem wegen der schon sehr weit getriebenen Automatisierung auf hohem Niveau stabilisiert hat, nimmt die Nutzung durch die anderen beteiligten Institutionen immer noch zu. Zurzeit werden jeweils mehrere Millionen Seiten pro Jahr im Übersetzungsspeicher gesucht bzw. aligniert. Weiterhin werden an jedem Arbeitstag mehr als 80 000 interaktive Abfragen mit der Konkordanz getätigt.\n\n"}
{"id": "2584416", "url": "https://de.wikipedia.org/wiki?curid=2584416", "title": "InfraRecorder", "text": "InfraRecorder\n\nInfraRecorder ist ein freies Brennprogramm für optische Datenträger wie CDs und DVDs für Windows-Betriebssysteme. Es ist in der Programmiersprache C++ geschrieben und unter der GNU General Public License veröffentlicht.\n\nInfraRecorder wurde 2006 beim Google Summer of Code von Christian Kindahl ins Leben gerufen. Es nutzt die Kommandozeilenprogramme von cdrtools. Neben \"cdrtfe\" ist es eines der wenigen grafischen Brennprogramme für Windows, die unter einer freien Lizenz veröffentlicht werden.\n\nDas Programm bietet Funktionen rund um das Erstellen und Kopieren von CDs und DVDs, den Umgang mit ISO-Abbildern, CD- und DVD-RWs sowie doppelschichtigen DVDs. Außerdem kann es Audio-CDs einlesen und als MP3, Ogg-Vorbis, WMA und RIFF WAVE speichern. Vorhandene Video-DVD-Dateien kann es ebenfalls brennen. Es unterstützt einfaches Drag and Drop sowohl innerhalb des Programms als auch zwischen verschiedenen Programmen (zum Beispiel aus dem Windows-Explorer heraus).\n\nIm derzeitigen Entwicklungsstand fehlen die Unterstützung von LightScribe und Labelflash.\n\nEs gibt auch eine Portable-Version, die direkt von Wechselmedien aus lauffähig ist.\n\nInfraRecorder läuft unter Windows 2000 oder höher (NT-Kernel), die letzte Version für Windows 98/Me war 0.43.1.\nDas Programm ist in 39 Sprachen verfügbar.\n\n"}
{"id": "2592057", "url": "https://de.wikipedia.org/wiki?curid=2592057", "title": "Mensch Computer", "text": "Mensch Computer\n\nDer Mensch Computer ist ein Computersystem, das auf Western Design Centers WDC 65C265 Mikrocontroller aufsetzt und von der Firma Western Design Center produziert wird. Der Computer wurde benannt nach Bill Mensch, einem der Designer des 6502 Prozessors und Gründers der Firma Western Design Center.\n\nDer Rechner wurde entwickelt für den Hobbybereich und für Kunden, die sich für die Funktionsweise von Computern interessieren, besonders auf der Ebene der Assemblersprache. Ein Großteil der Software, die auf dem Rechner ausgeführt werden kann, stammt von anderen Systemen, die den Befehlssatz des 65816 oder 6502 Prozessors verstehen (z. B. Nintendo Entertainment System, Super Nintendo, oder Apple IIgs).\n\nDer Mensch Computer besitzt einen ROM Monitor und etliche Software Routinen, die sich als Subroutinen auf dem ROM befinden.\n\n"}
{"id": "2592840", "url": "https://de.wikipedia.org/wiki?curid=2592840", "title": "Chaos Computer Club Zürich", "text": "Chaos Computer Club Zürich\n\nDer Chaos Computer Club Zürich (auch \"CCCZH\") ist ein Verein nach Schweizer Recht und einziger Ableger des deutschen Chaos Computer Clubs (CCC) in der Schweiz mit Erfastatus, dem Status eines Erfahrungsaustauschkreises und damit Bestandteil des \"CCC\". Der \"CCCZH\" vertritt den \"CCC\" für den Kanton Zürich und hat zum Ziel seine Forderungen lokal umzusetzen. Dabei stehen vor allem ein konstruktiver und verantwortungsbewusster Umgang mit Technik im Vordergrund, woraus politische Forderungen abgeleitet werden. Der Erfahrungsaustauschkreis veranstaltet wöchentliche Treffen, eine monatliche Radiosendung namens \"Hackerfunk\" und einzelne jährliche oder einmalige Events.\n\nEinige der Mitglieder treffen sich wöchentlich im seit Ende 2014 bestehenden Hackerspace an der Röschibachstrasse 26 im Stadtkreis Wipkingen von Zürich; dies sind die sogenannten Chaostreffs. Dabei werden auch Aktivitäten des Vereins geplant und aktuelle Ereignisse diskutiert; Ziel ist auch das Ausleben einer Hacker-Kultur. Die Treffs werden auf der CCCZH-Webseite öffentlich angekündigt und alle Interessierten sind eingeladen daran teilzunehmen.\n\nEine gewisse Bekanntheit mit Kontakten zu Journalisten erhielt der \"Chaos Computer Club Zürich\" mit der Anprangerung von Sicherheitslücken in der Schweizer Postcard, einer Debitkarte der PostFinance. Zuvor wurden am jährlichen CCC-Kongress 2006, dem 23C3, diese Sicherheitslücken erstmals der Öffentlichkeit vorgestellt. Später wurde ein Artikel mit Namen \"Signatur in Schweizer Postcard geknackt\" in der c't (05/07) abgedruckt. Für weiteren Wirbel sorgte ein Artikel darauf in der SonntagsZeitung.\n\nVon Mitgliedern wurde eine Radiosendung namens \"Hackerfunk\" ins Leben gerufen. Diese wird jeweils am ersten Samstag im Monat auf dem Campusradio der ETH Zürich, Radio Radius, per Webstream live ausgestrahlt und ist auch als Web-Feed erhältlich. Der Hackerfunk ist nationaler Gewinner der Schweiz beim European Podcast Award 2008 in der Kategorie Non-Profit.\nAm 28. Januar 2013 stellten die Moderatoren des Hackerfunks einen Mitgliedschaftsantrag an den Chaos Computer Club Schweiz als akustischer Hackerspace. Am 16. Juni 2013 wurde der Hackerfunk im Rahmen der ordentlichen Generalversammlung als Mitglied aufgenommen.\n\nDer Club veranstaltete in den Jahren 2006 und 2007 zusammen mit der LUGS, der in Zürich ansässigen \"Linux User Group Switzerland\", einen Software Freedom Day. Obwohl der Verein per se nicht Linux-orientiert ist, hat man sich entschlossen daran teilzunehmen und Linux und andere freie Software unter die Leute zu bringen.\n\nIm Jahr 2008 hat der Chaos Computer Club Zürich am \"Tag der Informatik\" auf dem Turbinenplatz in Zürich erstmals einen Stand betreut.\n\nDas Chaosdock ist ein im Frühling oder Herbst stattfindendes Meeting in den eigenen Treffräumen, wobei der \"CCCZH\" nach eigenen Angaben den Erfahrungsaustausch unter den Mitgliedern und Interessierten für einige Tage \"verschärfen\" möchte. Als Aufhänger dienen Vorträge und Workshops, so konnten z. B. 2007 Unerfahrene unter Anleitung eine Geekclock, eine von einem Mikrocontroller gesteuerte Uhr mit binärer Zeitanzeige, selber löten sowie die eigens dafür kreierte Software weiter anpassen lernen. Mangels Treffraum hat das Chaosdock im 2008 erst im Herbst und in einer anderen Räumlichkeit stattgefunden.\n\nDer Club beteiligt sich auch an der Organisation und Ausführung der jährlich stattfindenden Chaos Singularity, kurz auch \"CoSin\" genannt. Die \"CoSin\" fand in den Jahren 2006–2009 im Kulturzentrum Bremgarten in Bremgarten AG statt und ist ein Schweizer Kongress nach Vorbild des Chaos Communication Congress, des jährlich zwischen Weihnachten und Neujahr stattfindenden Kongresses des \"CCC\", jedoch in einem deutlich kleineren Rahmen. Ziel ist der Austausch zwischen allen CCC-Mitgliedern und Interessierten der Schweiz. Seit 2010 findet die CoSin nicht mehr im Kulturzentrum Bremgarten statt. 2010 wurde sie in einer privaten Räumlichkeit in Dübendorf ausgetragen, seit 2011 findet die Durchführung in einer jugendkulturellen Institution in Biel statt.\n\n2009 beteiligt sich der Club mit einem Kurs zum Thema Überwachung an der Autonomen Schule Zürich.\n\n2005:\nAm 28. Juni wird der \"Chaostreff Zürich\" als lockere Gruppierung mit einem ersten Treffen formiert. Von da an trifft man sich regelmässig in Restaurants der Zürcher Innenstadt.\n\n2006:\nDie Treffen in öffentlichen Restaurants werden als unpraktisch angesehen und der Drang einen eigenen Clubraum zu besitzen entsteht. Man begibt sich auf die Suche nach geeigneten Räumlichkeiten. Mehrere Lokalitäten stehen zur Auswahl und man entscheidet sich für das \"DOCK18\" aufgrund potenzieller Synergien zwischen Kunst-, Gestaltungs-, Medien- und Technik-Gruppen.\n\nAm 4. April wird der \"Verein Chaostreff Zürich\" gegründet, um finanzielle Ansprüche wegen Vereinsgeschäften von den Mitgliedern abzuwenden. Die Statuten werden basisdemokratisch konzipiert, dem Vorstand damit nur administrative und keine inhaltlichen Aufgaben auferlegt. Der Verein wird als reiner Trägerverein verstanden, der nur den materiellen Rahmen für die Aktivität der Mitglieder bereitstellen soll.\n\nAm Regiotreff des \"CCC\", welcher am dritten Tag der 23C3-Veranstaltung am 29. Dezember 2006 in Berlin stattfindet, wird mit einer Abstimmung unter allen \"Erfakreisen\" dem \"Verein Chaostreff Zürich\" der Erfastatus zugesprochen.\n\n2007:\nAn der Vereinsversammlung vom 20. Januar wird intern über die Annahme des Erfastatus abgestimmt, wobei der angenommen wird. Der Verein nennt sich fortan \"Chaos Computer Club Zürich\", um seine Nähe zum \"Chaos Computer Club\" offen zu bekundigen. Intern wurde sich auch geeinigt als übliche Abkürzung dafür \"CCCZH\" zu verwenden.\n\n2012:\nIn Bern gründet der CCCZH mit den Chaostreffs Basel, St. Gallen und Bern (Ostermundigen) am 15. Dezember den Chaos Computer Club Schweiz (CCC-CH).\n\n\n"}
{"id": "2601756", "url": "https://de.wikipedia.org/wiki?curid=2601756", "title": "Avidemux", "text": "Avidemux\n\nAvidemux ist eine freie, plattformübergreifende Videoschnittsoftware, die zahlreiche Audio-, Video- sowie Containerformate unterstützt und diese zumeist untereinander konvertieren kann.\n\nDas Projekt stellt eine im Funktionsumfang vergleichbare, teilweise sogar umfangreichere Alternative zu anderen nichtkommerziellen Videobearbeitungsprogrammen wie beispielsweise VirtualDub oder AviSynth dar und ist als solches das meistverbreitete freie Programm seiner Art. So unterstützt das Programm im Gegensatz zu VirtualDub OGM- und MPEG-4-Dateien nativ.\n\nLaut dem Zähler auf der Website des Projektes wurde das Programm über eine Million Mal heruntergeladen, dazu kommen von Linux-Distributionen beigelegte Pakete. Das Augenmerk des Projekts richtet sich auf Plattformunabhängigkeit und eine vergleichsweise hohe Anzahl an standardmäßig unterstützten Dateiformaten, Video- und Audiocodecs sowie Filtern.\n\nDie maximale Anzahl an Tonspuren ist momentan auf vier begrenzt. Die Anzahl der Kanäle pro Tonspur, z. B. zwei Kanäle für stereo oder ein Kanal für mono, hängt von dem verwendeten Audioformat ab.\n\n\"Avidemux\" hat eine grafische Benutzeroberfläche, kann aber auch per Skript auf Kommandozeilenebene bedient werden. Bei letzterem kommt der JavaScript-Interpreter SpiderMonkey zum Einsatz.\n\nDie grafische Benutzeroberfläche existiert in zwei Varianten. Sie basiert wahlweise auf den Bibliotheken GTK+ oder Qt, welche für alle gängigen Betriebssysteme erhältlich sind, wodurch die Plattformunabhängigkeit erreicht wird.\n\n\n\n"}
{"id": "2607148", "url": "https://de.wikipedia.org/wiki?curid=2607148", "title": "AI (Computer)", "text": "AI (Computer)\n\nAI, auch MIT-AI, später ai.ai.mit.edu war ein am Fachbereich für Künstliche Intelligenz des Massachusetts Institute of Technology eingesetzter PDP-10-Computer, auf dem das Betriebssystem ITS lief. Gleichzeitig wurde ITS auf dem Rechner auch entwickelt.\n\nBis zur Einstellung des Betriebs im Mai 1990 war AI ein zentraler Treffpunkt der frühen Hackergemeinschaft. Der Rechner war an das Arpanet (Adresse 2/6, später IP-Adresse 10.2.0.6) sowie das CHAOSnet (Adresse 2026) angebunden und erlaubte darüber jedermann den Zugriff als „Tourist“. Da ITS ursprünglich ganz ohne jegliche Sicherheitsmaßnahmen entworfen worden war, war eine Teilnahme mit vollem Zugriff auf alle Daten anfänglich durch schlichtes Verbinden mit dem Rechner über Telnet möglich. Später wurde auf Druck der Universitätsverwaltung hin ein Login-System eingebaut, das zur Benutzung des Rechners ein Benutzerkonto mit Passwort erforderte. Aus Protest setzten jedoch viele ihr eigenes Passwort auf die leere Eingabe.\n\nEin prominenter Benutzer von AI war Richard Stallman. Die Hackergemeinschaft rund um den Rechner und das ITS-Betriebssystem inspirierte ihn maßgeblich bei der Schaffung des GNU-Projekts.\n\nEin von AI inspirierter Computer kommt in Password Swordfish vor.\n\n"}
{"id": "2610095", "url": "https://de.wikipedia.org/wiki?curid=2610095", "title": "Apple QuickTake", "text": "Apple QuickTake\n\nApple QuickTake war eine Familie von Digitalkameras von Apple.\n\nApple brachte in Zusammenarbeit mit der Firma Kodak das Modell \"QuickTake 100\" 1994 auf den Markt, es war eine der ersten bezahlbaren digitalen Kameras für den Massenmarkt. Das Modell kostete damals umgerechnet etwa 750 Euro.\nDer Nachfolger war die sehr ähnliche \"QuickTake 150\".\n\nDie Kameras \"QuickTake 100/150\" wurden über eine serielle Schnittstelle mit dem Macintosh verbunden. Nach Installation von zusätzlicher Software (Disketten lagen bei) wurde die Kamera über ein Mac-OS-Kontrollfeld angesteuert und die Bilder auf den Mac übertragen.\n\nAls Zubehör lagen Disketten (Inhalt: Systemerweiterungen, Kontrollfeld, eine einfache Bildbearbeitung), ein serielles Kabel sowie Plastiklinsen für den Nahbereich bei. Die drei Mignonzellen mussten extra zugekauft werden.\n\nAn der Rückseite beider Modelle befand sich eine kleine monochrome LCD-Anzeige, die über den Batteriestatus, die Anzahl der Aufnahmen, die gewählte Auflösung und den Blitzmodus informierte. Die Darstellung intern gespeicherter Fotos war nicht möglich.\n\nAn den Ecken des Displays waren Taster für Selbstauslöser, Auflösung, Blitzmodus und (versenkt) Löschfunktion.\n\nDie letzte Digitalkamera von Apple entstand in Kooperation mit Fujifilm und trug die Bezeichnung \"QuickTake 200\". Sie erschien 1996.\n\nDie Produktion der QuickTake-Kameras wurde 1997 kurz nach der Rückkehr von Steve Jobs zu Apple eingestellt.\n\nZum Betrachten und Öffnen der QuickTake-Bilder ist eine Systemerweiterung nötig, die es nur für das klassische Mac OS und Windows 95 gab. Das Datenformat (qtk) ist proprietär, und wird nicht mehr unterstützt. Solche Bilder können also unter neueren Betriebssystemen wie Mac OS X nicht mehr geöffnet werden. Der Apple-Finder stellt immerhin eine Vorschau zur Verfügung.\nDas Programm GraphicConverter kann ab Version 6 auch QuickTake-Bilder unter Mac OS X öffnen.\n\n"}
{"id": "2610310", "url": "https://de.wikipedia.org/wiki?curid=2610310", "title": "Triff die Robinsons", "text": "Triff die Robinsons\n\nTriff die Robinsons (Originaltitel: \"Meet the Robinsons\") ist ein US-amerikanischer Computeranimationsfilm und der 47. abendfüllende Trickfilm der Walt Disney Animation Studios aus dem Jahr 2007. Er basiert auf dem Kinderbuch \"Zu Gast bei Willi Robinson\" (Originaltitel: \"A day with Wilbur Robinson\") von William Joyce.\n\nLewis ist ein zwölfjähriger Waisenjunge, dessen größter Wunsch eine Familie ist. Um herauszufinden, weshalb seine Mutter ihn weggegeben hat und um sie zu suchen, erfindet er einen Gedankenscanner. Sein Zimmergenosse, der immermüde Baseball-Spieler Michael „Goob“ Yagoobian erträgt mehr oder weniger stumm Lewis' explosive Mischung aus Traurigkeit und Erfindungswut. Den Gedankenscanner möchte Lewis auf einer Science Fair vorstellen. Ein geheimnisvoller Junge namens Wilbur, der behauptet aus der Zukunft zu kommen, warnt Lewis vor einem bösen Mann mit Melone, dessen Ziel darin besteht, den Gedankenscanner zu stehlen. Während sich Lewis auf die Präsentation vorbereitet, wird sein Gerät von eben jenem sabotiert. Der Gedankenscanner kollabiert und Lewis ist am Boden zerstört. \n\nLewis zieht sich enttäuscht und wütend auf das Dach des Waisenhauses zurück, wo auch Wilbur bald eintrifft. Dieser will ihn überreden, den Gedankenscanner zu reparieren. Lewis will ihn aber nur reparieren, wenn Wilbur beweist, dass er wirklich aus der Zukunft ist. Kurzerhand bringt er Lewis mit einer Zeitmaschine in die Zukunft. Dort zerstören die beiden die Zeitmaschine unabsichtlich bei einem Streit, weil Lewis in die Vergangenheit will, um seine Mutter davon abzuhalten ihn wegzugeben. In der Zwischenzeit stiehlt der Melonenmann den Gedankenscanner und will ihn als seine eigene Erfindung ausgeben. Das geht gründlich schief, da er keine Ahnung hat, wie das Ding funktioniert. Deshalb plant er, Lewis zu entführen.\n\nDieser jedoch ist in der Zukunft, wo er die Robinsons, Wilburs vollkommen verrückte Familie, kennenlernt. Nach einem Kampf mit einem Dinosaurier, den der Melonenmann aus der Vergangenheit geholt hat, wollen die Robinsons Lewis sofort adoptieren, nehmen es allerdings gleich zurück, als sie seine blonden Haare sehen. Wütend auf Wilbur rennt Lewis davon und trifft auf den Melonenmann, dem er auch folgt. Lewis repariert den Gedankenscanner für ihn. Der Melonenmann fesselt ihn und erzählt Lewis von seinem Leben.\n\nWie sich herausstellt ist der Melonenmann nämlich der erwachsene Goob, für dessen ruiniertes Leben er Lewis verantwortlich macht. Da Lewis laute Erfindungen Goob viel Schlaf gekostet haben, ist er bei einem für ihn sehr wichtigen Baseballspiel eingeschlafen. Lewis ist Wilburs Vater Cornelius. Der Melonenmann gelangt in die Vergangenheit und gibt den Gedankenscanner als seine Erfindung aus, verändert allerdings dadurch die Zukunft extrem. Das Haus der Robinsons ist leer bis auf den Gedankenscanner, auf dem Lewis sehen kann, was passiert ist: Der Hut Doris des Melonenmannes wurde ebenfalls patentiert und vervielfältigt. Dieser Hut hat allerdings ein Eigenleben, versklavt die Menschheit und tötet seinen geschockten Besitzer.\n\nLewis wird von der Melonen-Robinsonfamilie angegriffen und kann nur fliehen, indem er die kaputte Zeitmaschine in Gang bringt. Er reist in der Zeit zurück und verhindert, dass Doris jemals von ihm erfunden wird. Zurück in der Zukunft repariert sich die Welt von selbst. Lewis begegnet auch seinem zukünftigen Ich, bevor Wilbur ihn in die Vergangenheit bringt. Allerdings an den Tag, als er vor dem Waisenhaus ausgesetzt wurde. Lewis begegnet doch noch seiner Mutter, hält sie allerdings nicht davon ab, ihn wegzugeben: die Zukunft ist ihm wichtiger als die Vergangenheit.\n\nAn dieser Stelle wird deutlich, dass die Zeitmaschine nur für diesen Zweck gebaut wurde. Lewis hält seine leibliche Mutter nicht davon ab, ihn wegzugeben, sondern kehrt zurück zum Wettbewerb, um den Gedankenscanner vorzustellen, der diesmal funktioniert. Er wird von einem der Juroren adoptiert. Außerdem trifft er seine zukünftige Frau Franny, die sich in ihn verliebt, und zieht in das Haus, das er in der Zukunft kennengelernt hatte. Von nun an heißt er Cornelius Robinson. Vorher jedoch geht er zu dem Baseballspiel bei dem Goob eingeschlafen war und schreit ihm zu, damit dieser aufwacht und den Ball fängt. \n\nAb dem 23. März 2007 startete der Film in den Kinos zahlreicher Länder. In seinem Produktionsland, den Vereinigten Staaten, lief der Film am 30. März desselben Jahres an und spielte bis Juni etwa 96 Millionen US-Dollar ein. In Deutschland, wo er bereits am 29. März 2007 gestartet war, wurde der Film etwa 250.000 Mal gesehen.\n\nA. O. Scott kritisierte in der \"New York Times\" vom 30. März 2007, der Film sei \"„mit Sicherheit einer der schlechtesten in letzter Zeit im Kino veröffentlichten Animationsfilme, die unter dem Disney-Label herausgekommen sind.“\" Eine positive Meinung zu \"Triff die Robinsons\" vertrat dagegen Todd McCarthy von der \"Variety\". Der Film sei \"„zynisch“\" und \"„reichlich unterhaltend“\". Kinder jeden Alters würden ihm mit großen Augen folgen und würden auf Zack gehalten.\n\nDer \"film-dienst\" bezeichnete die Geschichte als wirr, meinte jedoch auch: \"„Dennoch gelingt es Regie-Neuling Stephen J. Anderson, ein wenig Pixar-Glamour in diese Produktion aus dem Hause Disney zu zaubern – was bewirkt, dass man mehr staunt als denkt und sich in ein Universum entführen lässt, in dem das Unmögliche möglich scheint. Dass die Geschichte nicht wie sonst bei Disney ins allzu Gefühlsselige abrutscht und dass die Gags ein vernünftiges Timing haben und durchaus auch Erwachsene ansprechen, hilft dabei enorm.“\"\n\nDas Lexikon des Internationalen Films urteilt: „\"Ein mitunter etwas überladenes Science-Fiction-Abenteuer, das trotz seiner Anlehnung an bekannte Vorbilder dennoch als eigenständiger 3D-Animationsfilm überzeugt. Unbeschwerte Unterhaltung, die in der Handlung eher auf Action als auf Gefühligkeit aufbaut.\"“\n\n\"nominiert:\"\n\n\"nominiert:\"\n\n\"nominiert:\"\n\n\n\n"}
{"id": "2612664", "url": "https://de.wikipedia.org/wiki?curid=2612664", "title": "Microsoft Windows NT 3.51", "text": "Microsoft Windows NT 3.51\n\nWindows NT 3.51 ist ein Betriebssystem von Microsoft. Es stellt eine technische Weiterentwicklung von Windows NT 3.5 dar. Die System-APIs wurden erweitert und unterstützen damit neuere 32-Bit-Anwendungen.\n\nDas Vorgängerbetriebssystem Windows NT 3.5 sollte ursprünglich auch Unterstützung für die PowerPC-Architektur bieten. Diese fehlte jedoch, da IBM die Auslieferung der PowerPC-Prozessoren wiederholt verschieben musste. Daraufhin entschied sich Microsoft, die PowerPC-Unterstützung nicht für NT 3.5 nachzuliefern, so wie es mit der Alpha AXP-Version von Windows NT 3.1 geschehen war, sondern diese als Teil einer überarbeiteten Version von Windows NT 3.5 unter der Bezeichnung Windows NT 3.51 herauszugeben.\n\nDas Betriebssystem wurde im Februar 1995 erstmals angekündigt. Die endgültige Version NT 3.51 wurde im Mai 1995 in den USA herausgegeben. Ende des Supports war für Endkunden der 31. Dezember 2000, für Firmenkunden am 31. Dezember 2001. Es gab danach keine Sicherheits-Aktualisierungen für neu entdeckte Sicherheitslücken mehr.\n\nEs wurden insgesamt fünf Service Packs veröffentlicht:\n\nNach dem Service Pack 5 wurden noch einige einzelne Hotfixes veröffentlicht. Diese betrafen etwa Unterstützung für das Eurozeichen sowie AGP-Grafikkarten.\n\nDie hauptsächliche Neuerung in Windows NT 3.51 ist die Unterstützung des PowerPC als vierte Prozessorarchitektur. Die weiteren Neuheiten von Windows NT 3.51 gegenüber seinem Vorgänger 3.5 halten sich in Grenzen. So werden etwa PCMCIA-Geräte unterstützt. Wird das System auf einem Intel Pentium mit FDIV-Bug installiert, bietet Windows NT 3.51 das Abschalten des Koprozessors an, um das Problem zu umgehen.\n\nFür das Dateisystem NTFS wurde eine Datenkompression eingeführt, die es erlaubt, einzelne Dateien oder ganze Verzeichnisse zu komprimieren, was allerdings auf Kosten der Leistung geht.\n\nDie größte Änderung ist jedoch die Hinzufügung zahlreicher APIs des sich seinerzeit in Entwicklung befindlichen Windows 95, sodass Windows 95-Programme in den meisten Fällen auch auf Windows NT 3.51 lauffähig sind, sofern sie nicht auf mit Windows NT inkompatiblen Komponenten (wie DirectX) basieren. Windows NT 3.51 enthält letztmals einen Treiber für das HPFS-Dateisystem von OS/2, wenngleich im Gegensatz zu Windows NT 3.5 keine Partitionen mehr mit dem HPFS-Dateisystem formatiert werden können.\n\nZur Vorbereitung auf Windows NT 4.0 bot Microsoft die Benutzeroberfläche von Windows 95 als Download für Windows NT 3.51 an. Diese war zwar ursprünglich für Entwickler gedacht, um ihre Software an die neue Benutzeroberfläche anzupassen, wurde aber auch von zahlreichen Endanwendern verwendet.\n\nIm Lieferumfang von Windows NT 3.51 befinden sich neben einer CD-ROM und drei 5,25\"-Startdisketten noch ein 3,5\"-Diskettensatz, bestehend aus 23 Disketten bei der Workstation und 42 Disketten beim Server.\n\n\n\n"}
{"id": "2612669", "url": "https://de.wikipedia.org/wiki?curid=2612669", "title": "Microsoft Windows NT 3.5", "text": "Microsoft Windows NT 3.5\n\nWindows NT 3.5 ist ein Betriebssystem von Microsoft und Nachfolger der Version Windows NT 3.1. \n\nBereits gegen Ende der Entwicklung des Vorgängers Microsoft Windows NT 3.1 sammelten die Entwickler Ideen, die in diesem Stadium des Entwicklungszyklus nicht mehr realisiert werden konnten und daher für die nächste Version geplant waren, die im Herbst 1994 veröffentlicht werden sollte.\n\nDavid N. Cutler wusste, dass die erste Version eines Produkts nie perfekt ist. Angespornt durch die zahlreichen Kritiken an Windows NT 3.1 wollte er unverzüglich mit der Entwicklung des Nachfolgers beginnen. Damit stand er jedoch in Konkurrenz zu Cairo, einem neuen Projekt von Microsoft unter der Führung von Jim Allchin. Windows NT sollte mitsamt dem Entwicklerteam in das Cairo-Projekt eingegliedert werden und so die Entwicklung unter einer neuen Führung fortsetzen. Cutler akzeptierte diese Entscheidung nicht und verließ das Entwicklerteam. Bill Gates befürchtete, dass Cutler das Unternehmen komplett verlassen würde, und versuchte, ihn zum Bleiben zu bewegen. Cutler akzeptierte dies schließlich, indem er gemeinsam beide Projekte entwickelte und die neue Führung lediglich als Formsache betrachtete.\n\nFür das neue Betriebssystem setzte sich das Entwicklerteam drei grobe Ziele. Das erste Ziel war Leistung, denn Windows NT 3.1 galt weithin als zu langsam. Die sehr hohen Hardwareanforderungen des Vorgängers sollten reduziert und die Leistung des Betriebssystems optimiert werden, was vor allem durch den Einsatz verbesserter Compiler gelang. Im Juni 1994 berichtete Microsoft von einem vier bis acht Megabyte geringeren Speicherverbrauch gegenüber dem Vorgänger sowie 50 bis 100 Prozent schnellere Grafikdarstellung und 20 bis 30 Prozent schnellere Dateizugriffe.\n\nDas zweite Ziel war NetWare-Kompatibilität. Novell versprach zwar einen NetWare-Client für Windows NT, zögerte die Auslieferung aber wiederholt hinaus. So entschied sich Microsoft schließlich, einen eigenen NetWare-Client zu entwickeln und mit Windows NT 3.5 auszuliefern. Das Entwicklerteam wusste, dass Windows NT keine Chance auf dem Markt haben würde, wenn es keine Möglichkeit zur Anbindung an NetWare-Netze böte.\n\nDas dritte Ziel war Kompression. Aufgrund des wachsenden Bedarfs an Festplattenspeicher waren Programme wie DoubleSpace unter MS-DOS weit verbreitet. Windows NT 3.5 sollte eine ähnliche Technologie unterstützen.\n\nEin weiteres Augenmerk richteten die Entwickler auf den TCP/IP-Protokollstapel. Der in Windows NT 3.1 enthaltene Protokollstapel war von einem Drittanbieter namens \"Spider Systems\" lizenziert. Dessen Code basierte auf der in Unix verwendeten STREAMS-Programmierschnittstelle. Dadurch musste STREAMS auf Windows NT portiert werden, was sich negativ auf die Leistung auswirkte. Windows NT 3.5 sollte daher einen neuen, eigens entwickelten TCP/IP-Protokollstapel beinhalten.\n\nErste Informationen über einen Nachfolger von Windows NT 3.1 mit dem Codenamen \"Daytona\" kamen im Dezember 1993 heraus. Im Februar 1994 wurde die Entwicklung offiziell auf der WinHEC bekanntgegeben. Ende März folgte die erste öffentliche Vorstellung des neuen Betriebssystems.\n\nDie endgültige Version NT 3.5 wurde am 13. September 1994 veröffentlicht. Am 13. Januar 1995 kam das erste Service Pack heraus, welches unter anderem eine Methode zur Umgehung des Pentium-FDIV-Bug bot. Das Service Pack 2 folgte am 1. März 1995. Am 29. August 1995 wurde schließlich das Service Pack 3 fertiggestellt.\n\nDie zwei Versionen wurden im Unterschied zu NT 3.1 umbenannt und heißen nunmehr \"Windows NT 3.5 Workstation\" und \"Windows NT 3.5 Server\". Während es noch in Windows NT 3.1 kaum Unterschiede zwischen der Workstation-Variante und dem Server gab, sind die beiden Versionen von Windows NT 3.5 klar voneinander abgegrenzt. Windows NT 3.5 Workstation akzeptiert nur 10 Client-Verbindungen, sodass ein Einsatz als Server abgesehen von kleinen Netzwerken ausgeschlossen ist.\n\nWindows NT 3.5 Workstation war für 319 USD erhältlich. Mit \"Windows NT 3.5 Server\" wurde das bis heute gültige Lizenzierungsmodell von Microsoft eingeführt, bei dem für jeden Client, der auf den Server zugreift, eine Lizenz zu kaufen ist. Kostete die vorherige Version des Servers noch pauschal 1.495 USD, egal wie viele Clients verbunden waren, lag der Preis nun bei 699 USD, wobei für jeden Client 39 USD zu entrichten waren.\n\nWindows NT 3.5 Server wurde nicht nur einzeln, sondern auch unter der Bezeichnung Microsoft BackOffice erstmals zusammen mit zahlreichen Programmen, wie einem Mailserver und einem Microsoft SQL Server vertrieben.\n\nGegenüber dem Vorgänger wurden zahlreiche Detailverbesserungen vorgenommen. So können 16-Bit-Windowsprogramme in separaten Umgebungen laufen, um präemptives Multitasking zwischen ihnen zu ermöglichen und zu verhindern, dass eine fehlerhafte 16-Bit-Anwendung alle anderen 16-Bit-Anwendungen abstürzen lässt. Zudem gibt es die neue Option, ein Nutzerkonto für eine bestimmte Zeit zu sperren, wenn das Passwort zu oft falsch eingegeben wurde.\n\nZur Einstellung der Bildschirmauflösung und Farbtiefe wird in Windows NT 3.5 nicht mehr das alte Programm \"Windows-Setup\", sondern die neue Systemsteuerungsoption \"Anzeige\" verwendet. Diese erlaubt es, die ausgewählten Optionen schon im Vorab zu testen, indem ein Testbild eingeblendet wird, sofern die Grafikkartentreiber dies unterstützen. Um weiters die Entwicklung von Grafiksoftware zu unterstützen, wurde in Windows NT 3.5 eine von Silicon Graphics lizenzierte OpenGL-Grafikbibliothek hinzugefügt. Zur Demonstration lagen dem Betriebssystem einige Bildschirmschoner bei.\n\nDas neu entwickelte VFAT ermöglichte lange Dateinamen nun auch auf FAT-Partitionen, die unter dem Vorgänger NT 3.1 noch auf das 8.3-Format beschränkt waren. Aufgrund dieser Änderungen sowie weiterer Änderungen am NTFS-Dateisystem muss, falls Windows NT 3.1 und Windows NT 3.5 auf einem Rechner parallel betrieben werden sollen, ein Aktualisierungspaket von der Windows NT 3.5-CD installiert werden, um Windows NT 3.1 den Zugriff auf diese Partitionen zu ermöglichen.\n\nWindows NT 3.5 beinhaltet einen überarbeiteten TCP/IP-Protokollstapel, der von Grund auf neu programmiert wurde. Windows NT 3.5 unterstützt DHCP und enthält einen Namensauflösungsdienst namens WINS. Ein mitgelieferter NetWare-Client ermöglicht erstmals den Zugriff auf NetWare-Netzwerke mit Bordmitteln, ohne dass zusätzliche Software installiert werden muss. Der Remote Access Service, der unter Windows NT 3.1 lediglich den Zugriff auf NetBEUI-Ressourcen ermöglichte, unterstützt nun auch TCP/IP sowie IPX, sodass etwa auch auf NetWare-Freigaben zugegriffen werden kann. In diesem Zuge unterstützt RAS einen Verbindungsaufbau über PPP sowie SLIP. \n\nDie Serverversion von Windows NT 3.5 enthält einige Neuerungen. Sie bietet die Möglichkeit, als NetWare-Gateway zu fungieren. So können DOS- und Windows-Clients auf Ressourcen eines NetWare-Servers zugreifen, ohne direkt mit diesem Netzwerk kommunizieren zu müssen. Ein Programm soll helfen, einen NetWare-Server mitsamt seinen Daten wie Verzeichnisse und Benutzer auf Windows NT Server zu migrieren. Andere Rechner im Netzwerk können über den Server entfernt (\"remote\") gestartet werden, was vor allem für Diskless-Workstations gedacht ist. Der RAS-Server unterstützt nun nicht mehr 64, sondern 256 eingehende RAS-Verbindungen.\n\nDie Treiberbibliothek wurde gegenüber dem Vorgänger deutlich erweitert, so wird jetzt der PCI-Bus unterstützt sowie moderne ATAPI-CD-ROM-Laufwerke.\n\nDie Systemvoraussetzungen haben sich gegenüber dem Vorgänger Windows NT 3.1 nicht verändert. Zur Installation auf x86-Systemen wurden ein 80386-Prozessor mit 25 MHz, mindestens 12 MB Arbeitsspeicher, 75 MB Festplattenspeicher sowie eine VGA-Grafikkarte benötigt. RISC-Systeme erforderten stattdessen 16 MB Arbeitsspeicher und 92 MB Festplattenspeicher, sowie ein CD-ROM-Laufwerk. Im Gegensatz zum Vorgänger kann Windows NT 3.5 nicht auf 80386-basierten Mehrprozessorsystemen installiert werden.\n\nDie Installation von Windows NT ist entweder von der CD-ROM möglich, der drei Startdisketten beiliegen, oder auf x86-Rechnern von einem Diskettensatz (bestehend aus 21 3,5\"-Disketten). Es ist zudem möglich, ein bestehendes Windows NT 3.1 zu aktualisieren.\n\nAufgrund eines Fehlers bei der Prozessorerkennung scheitert die Installation des Betriebssystems auf allen Prozessoren seit dem Pentium II. Dieses Problem wurde von Microsoft nie behoben, jedoch sind inoffizielle Patches verfügbar, um die Installation zu ermöglichen.\n\nDie Berichte von Microsoft, wonach Windows NT 3.5 schneller sei als sein Vorgänger, bestätigten sich: die gleichen Benchmarks liefen unter NT 3.5 schneller ab als unter NT 3.1, gemessen wurde dabei eine Leistungssteigerung um bis zu 100 Prozent. Das Betriebssystem verkaufte sich besser als sein Vorgängerprodukt, was nach Analysten nicht alleine auf die Verbesserungen der neuen Version, sondern auch auf Marketingfehler der Konkurrenz zurückzuführen sei. Schätzungen zufolge gab es Anfang 1995 400.000 Workstations und 141.000 Server mit Windows NT. Im Angesicht der Verbesserungen ging die Migration bestehender Systeme mit Windows NT 3.1 auf die neue Version zügig vonstatten. Für Microsoft kam der Erfolg der neuen Version überraschend, das Betriebssystem war teilweise ausverkauft. Das Angebot an 32-Bit-Windowsanwendungen verbesserte sich, so gab es zu dem Zeitpunkt etwa 1.200 Anwendungen. Auch Microsoft selbst unterstützte das Betriebssystem mit 32-Bit-Versionen von Microsoft Word und Microsoft Excel.\n\nVon den verkauften Systemen mit Windows NT beliefen sich auf RISC-Systeme lediglich fünf Prozent. Der Versuch von DEC, Rechner mit Alpha-Prozessoren mittels Windows NT als Desktop-PC zu etablieren, schlug fehl, sodass sich das Unternehmen auf den High-End-Markt konzentrierte, um dort eine Nische zu finden. Sowohl DEC als auch NEC, welches Rechner mit MIPS-Architektur vertrieb, hofften durch das verbesserte Betriebssystem einen neuen Zulauf.\n\nUrsprünglich sollte Windows NT 3.5 auch Unterstützung für die PowerPC-Architektur bieten. IBM musste jedoch die Auslieferung der PowerPC-Prozessoren wiederholt verschieben, sodass man sich entschied, die PowerPC-Unterstützung nicht für NT 3.5 nachzuliefern, so wie es mit der Alpha AXP-Version von Windows NT 3.1 geschehen war, sondern eine überarbeitete Version von Windows NT 3.5 unter der Bezeichnung Windows NT 3.51 herauszugeben. Dies geschah nicht zuletzt aufgrund der großen Zahl an bekannten Fehlern, die Windows NT 3.5 enthielt.\n"}
{"id": "2615097", "url": "https://de.wikipedia.org/wiki?curid=2615097", "title": "Microsoft Windows NT 3.1", "text": "Microsoft Windows NT 3.1\n\nWindows NT 3.1 ist ein von Microsoft entwickeltes 32-Bit-Betriebssystem. Es erschien am 26. Juli 1993 in einer Workstation- und einer Servervariante und war das erste Betriebssystem der Windows-NT-Reihe. Die aus Vermarktungsgründen gewählte Versionsnummer 3.1 sollte die optische Nähe zum damaligen grafischen Aufsatz Windows 3.1 für das Betriebssystem DOS betonen. Windows NT 3.1 wurde als Netzwerkbetriebssystem konzipiert und konkurrierte in diesem Bereich mit Novell NetWare.\n\nWindows NT 3.1 ist ein von Grund auf neu entworfenes Betriebssystem. Es sollte nicht nur auf mehreren Prozessorarchitekturen lauffähig sein, sondern auch eine höhere Sicherheit und Stabilität bieten als die bisherigen, auf MS-DOS basierenden Windows-Versionen, und damit unter anderem als Betriebssystem für sicherheitskritische Anwendungen dienen. Windows NT 3.1 unterstützte dementsprechend nicht nur x86-, sondern auch MIPS und später auch Alpha-AXP-Prozessoren. Durch die Architektur von Windows NT 3.1 konnten fehlerhafte Programme das System nicht länger zum Absturz bringen.\n\nDas Betriebssystem war aufgrund der hohen Hardwareanforderungen für damalige Computer zu langsam. Außerdem gab es nur wenige 32-Bit-Anwendungen, die die Fähigkeiten von Windows NT 3.1 nutzten. Insgesamt konnte sich Windows NT 3.1 auf dem Markt nicht durchsetzen, es legte jedoch den Grundstein für alle späteren Windows-NT-Versionen.\n\nIn den 1980er-Jahren hatte Microsoft mit MS-DOS, dem Betriebssystem der damals vorherrschenden IBM-PC-kompatiblen Computer, eine Vormachtstellung im Personal-Computer-Markt inne. Allerdings befand sich das Unternehmen im Jahr 1988 in einer schwierigen Situation. Microsoft war in einem Urheberrechtsstreit mit Apple verwickelt, dessen Ausgang zur damaligen Zeit noch nicht abzusehen war. Sorgen bereitete dem Unternehmen außerdem das von ihm und IBM gemeinschaftlich entwickelte Betriebssystem OS/2. Dieses sollte ursprünglich MS-DOS als Betriebssystem ersetzen, größere Erfolge blieben jedoch bisher aus.\n\nDer Führungsstil von Microsoft-Gründer Bill Gates äußerte sich unter anderem dahingehend, dass er sich von Mitarbeitern beraten ließ, die er persönlich auswählte. Einer von ihnen war Nathan Myhrvold, der durch die Übernahme seines Unternehmens zu Microsoft kam. Myhrvold erkannte zwei Gefahren, die auf lange Sicht die Stellung von MS-DOS gefährden würden. Zum einen entstanden neue Prozessoren, die auf dem RISC-Prinzip basierten und einen schnelleren Takt erreichten als die vergleichbaren Intel-Prozessoren, auf denen MS-DOS lauffähig war. Zum anderen war ein Jahrzehnt zuvor mit Unix ein Betriebssystem erschaffen worden, das durch seine Multitasking- und Netzwerkfähigkeiten bestach und auf vielen Architekturen lauffähig war. Aufgrund der Tatsache, dass das Betriebssystem früher frei kopiert und verändert werden konnte, existierten zu dieser Zeit viele Unix-Derivate, die jedoch zueinander inkompatibel waren, sodass Anwendungsprogramme für jedes Derivat angepasst werden mussten. Dies verhinderte zu diesem Zeitpunkt zwar eine flächendeckende Verbreitung von Unix, doch die Vorstellung der Symbiose von Unix und RISC überzeugte Bill Gates davon, dass er einen „Unix-Killer“ benötigte. Er beauftragte daraufhin Nathan Myhrvold mit der Entwicklung eines portablen Betriebssystems.\n\nZur gleichen Zeit arbeitete David N. Cutler, ein renommierter Softwareentwickler, der unter anderem an der Entwicklung des Betriebssystems VMS beteiligt war, für DEC, einen Computerhersteller, der zu dieser Zeit ein starkes Wachstum erlebte. 1985 sollte Cutler mit seinem Entwicklerteam eine neue Computerfamilie mit dem Codenamen \"Prism\" entwickeln sowie ein zugehöriges Betriebssystem namens \"Mica\" entwerfen. Die konfliktreiche Entwicklungszeit endete vorzeitig im Juni 1988, als sich DEC entschied, das Projekt einzustellen. Dies überraschte Cutler zwar nicht, aber er war dennoch gezeichnet vom Ende seines Projekts. Zwar wollte Cutler DEC so schnell wie möglich verlassen, das Unternehmen überzeugte ihn aber mit der Hoffnung auf eine andere Stelle zum Abwarten. Gates erfuhr am 4. August 1988 von den Geschehnissen bei DEC. Er kannte Cutler zwar nicht persönlich, wusste aber um seine Erfahrung und wollte ihn zu sich holen. Cutler forderte die Mitnahme eines Teils seines Entwicklungsteams, darunter auch Computerdesigner. Zwar war Microsoft nicht im Bereich der Rechnerarchitektur tätig, jedoch entschied sich das Unternehmen, auf die Forderung einzugehen und vollendete so die Abwerbung Cutlers von Digital. Cutler kam am 31. Oktober 1988 bei Microsoft an. Myhrvolds Entwurf für ein portables Betriebssystem verwarf er, da das Konzept ihn nicht überzeugte. Die Entwickler nutzten die ersten Monate, um das zukünftige Betriebssystem zu planen, Ideen zu sammeln und den Personal Computer kennenzulernen, mit dem Cutlers Entwicklerteam bisher noch keine Erfahrung hatte.\n\nDas neue Betriebssystem betitelte Microsoft gegenüber der Öffentlichkeit zunächst als erweiterte Version von OS/2. Anfang 1989 definierte das Unternehmen die ersten Anforderungen, damit \"NT OS/2\", so der interne Name des Betriebssystems, am Markt erfolgreich sein würde. Es sollte portabel sein, um auf Veränderungen im Prozessormarkt reagieren zu können, sowie die Leistungsfähigkeit von Mehrprozessorsystemen nutzen können, die damals nur von wenigen Betriebssystemen unterstützt wurden. Das Betriebssystem sollte Netzwerkfunktionen beinhalten, da wegen der wachsenden Anzahl an Computern in Unternehmen die Notwendigkeit entstand, diese untereinander zu vernetzen. Zuletzt sollte das Betriebssystem auch Voraussetzungen der US-Regierung erfüllen. Dazu zählten neben der Unterstützung des POSIX-Standards auch Sicherheitsfunktionen, deren Grundlage die Trusted Computer System Evaluation Criteria darstellten. Das Ziel für NT war zunächst die Erfüllung der Stufe \"C2\"; zu den Voraussetzungen dieser Stufe zählten unter anderem separate Benutzerkonten, ein Zugriffsrechtesystem sowie die Protokollierung sicherheitsrelevanter Ereignisse.\n\nIm Hinblick auf die Portabilität des zukünftigen Betriebssystems begann die Entwicklung des Betriebssystems zunächst auf Nicht-x86-Prozessoren, um die unbeabsichtigte Verwendung von x86-spezifischem Code im Betriebssystem zu verhindern. Die Wahl fiel zunächst auf einen Intel-i860-Prozessor; da dieser bei Microsoft nicht vorlag, setzten die Entwickler einen Emulator ein. Der Codename des Intel i860-Prozessors, \"N-Ten\", gab auch dem Betriebssystem NT seinen Namen, die Bezeichnung wurde erst später zu Marketingzwecken zur Abkürzung für \"\" umgedeutet. Da DEC vermutete, dass ein Großteil des Quellcodes des \"Mica\"-Projekts auch im neuen Betriebssystem verwendet wurde, verklagten sie Microsoft. DEC und Microsoft einigten sich später außergerichtlich, dabei sagte Microsoft unter anderem zu, mit dem Betriebssystem den Alpha-Prozessor zu unterstützen. Im April 1989 war der Betriebssystemkern erstmals im Emulator lauffähig, und die Entwickler rechneten mit der Fertigstellung des Betriebssystems innerhalb von 18 Monaten.\n\nIm Juli 1989 kamen erste Exemplare des Intel i860-Prozessors bei Microsoft an. Der Prozessor war jedoch recht unzuverlässig und der Emulator sehr langsam. Erschwerend kam noch hinzu, dass wichtige Entwicklungswerkzeuge wie ein Debugger für diesen Prozessor noch nicht existierten. Zudem sah sich Microsoft gezwungen, die zur Entwicklung eingesetzten Computer selber zusammenzubauen, da die Computer, die NT benötigte, nicht auf dem Markt erhältlich waren. Schnell stellte sich heraus, dass der Intel i860-Prozessor für NT ungeeignet war. Infolgedessen beschlossen die Entwickler im Dezember 1989 auf einen MIPS R3000-Prozessor zu wechseln; Cutler kannte diese Prozessorarchitektur bereits aus seiner Zeit bei DEC. Innerhalb von drei Monaten war das System portiert. Im Februar 1990 schlug Microsoft-Mitarbeiter Paul Maritz vor, NT auf der COMDEX im kommenden Herbst vorzustellen. Dadurch sollte vor allem Konkurrenten begegnet werden, die behaupteten, dass sich die Entwicklung von NT bis ins Jahr 1994 ziehen würde, während Maritz von einer Fertigstellung im Jahr 1992 ausging.\n\nSchon früh machten sich die Entwickler Gedanken, wie das Betriebssystem getestet werden sollte, um Schwachstellen aufzudecken. Zwar gab es bereits bestimmte Tests für OS/2, aber diese, so die Entwickler, legten zu viel Wert auf die Architektur des Betriebssystems. NT hingegen sollte so getestet werden, wie auch ein normaler Benutzer das Betriebssystem benutzen würde. Dabei sollten mehrere dieser Tests gleichzeitig ablaufen, um die Zusammenarbeit verschiedener Komponenten zu überprüfen und Fehler aufzuspüren, die nur auftreten, wenn mehrere Aktionen gleichzeitig ablaufen.\n\nMicrosoft veröffentlichte im Mai 1990 die grafische Betriebssystemerweiterung Windows 3.0, die ein großer Erfolg wurde. Dadurch verschlechterte sich die Partnerschaft zwischen Microsoft und IBM, denn IBM wollte, dass Microsoft sein Windows-Betriebssystem vernachlässigt, um sich auf OS/2 zu konzentrieren. Microsoft beschäftigte zu dieser Zeit 150 Programmierer für die Entwicklung von OS/2, die jährlich 50 Millionen USD kostete; diese Ressourcen waren dementsprechend nicht für andere Projekte wie NT verfügbar. Die parallele Arbeit an mehreren komplett verschiedenen Betriebssystemen kostete nicht nur wertvolle Ressourcen, sondern stellte auch für das Marketing ein großes Problem dar. Softwareentwickler standen vor der schwierigen Wahl, ob sie ihre Anwendungen für Windows oder für OS/2 entwickeln sollten, denn es bestand keinerlei Kompatibilität zwischen den beiden Betriebssystemen und es war zu diesem Zeitpunkt noch ungewiss, welches der beiden Betriebssysteme sich am Ende durchsetzen würde.\n\nDaraufhin beschloss Microsoft im August 1990, das Betriebssystem als „Windows NT“ weiterzuentwickeln. Die OS/2-API sollte nicht mehr Bestandteil des zukünftigen Betriebssystems sein, sondern lediglich gesondert erhältlich sein. Um den Umstieg auf Windows NT für Programmierer einfach zu gestalten, welche bereits mit den 16-Bit-Versionen von Windows vertraut waren, wurden die Funktionen der Win16-API weitestgehend übernommen und um neue Funktionen erweitert. Für NT war ursprünglich die Presentation-Manager-Oberfläche von OS/2 vorgesehen, welche nun im Zuge des Wechsels zur Windows-Produktreihe durch die Windows-Shell ersetzt werden musste. Dadurch war klar, dass eine Demonstration des Betriebssystems auf der COMDEX 1990 nicht mehr zu schaffen war.\n\nIBM sollte zunächst nichts von den Plänen erfahren, um das Unternehmen nicht zu verärgern. Stattdessen sollte IBM weiterhin glauben, dass NT unter dem Namen OS/2 liefe, und lediglich zusätzlich Windows-Programme unterstützen würde. Mit dieser Taktik sollte Zeit gewonnen werden, um aus NT endgültig ein Windows-Betriebssystem zu machen. So einigten sich die beiden Unternehmen im September auf eine Neuausrichtung, bei der sie beschlossen, dass IBM die alleinige Federführung an der nächsten Version von OS/2, Version 2.0 bekam, während Microsoft zur selben Zeit die übernächste Version 3.0 entwickeln sollte. Im Januar 1991, als IBM-Mitarbeiter in einem internen Treffen das wahre Ziel von Microsoft erkannten, zerbrach die Allianz beider Unternehmen endgültig. Im Juli 1991 erreichte diese Nachricht schließlich die Presse.\n\nSetzten die Entwickler bisher OS/2-Maschinen zur Entwicklung von Windows NT ein, verlangte Cutler nun, dass die Entwickler ihre Arbeit fortan auf Windows NT selbst fortsetzten, um sie direkt mit ihrer eigenen Kreation zu konfrontieren und sie dazu zu animieren, das Betriebssystem in einen zumindest ansatzweise nutzbaren Zustand weiterzuentwickeln. Zudem würden Programmfehler im Betriebssystem so schneller entdeckt werden. Im März 1991 fand die Umstellung statt. Von hier an stellte das Entwicklerteam zunächst im Mai die grafische Benutzeroberfläche und im August die Netzwerkfunktionen fertig. Im Anschluss konzentrierten sich die Entwickler auf die Sicherheit des Betriebssystems, die bisher vernachlässigt worden war.\nIm September 1991 bereiteten die Entwickler Windows NT vor, um das Betriebssystem auf der im darauffolgenden Monat stattfindenden COMDEX vorzustellen. Da Microsoft erkannte, dass Windows NT ohne speziell auf seine Fähigkeiten ausgelegten Programme seine Vorteile nicht ausspielen könnte, sollten auf dieser ersten öffentlichen Vorstellung des Betriebssystems Anwendungsentwickler angeworben werden. Aufgrund des immensen Zeitdrucks und der hohen Anzahl an Programmfehlern mussten viele Funktionen weggelassen werden. Dazu zählten etwa das neue Dateisystem NTFS, welches sich noch in der Entwicklung befand, und die Kompatibilität zu DOS- und 16-Bit-Windowsanwendungen. Auch lief diese Version von Windows NT ausschließlich auf der x86-Architektur, die MIPS-Version war zu diesem Zeitpunkt nicht funktionsfähig und fehlte daher. Um Kosten zu sparen, entschied sich Microsoft, Windows NT auf einer CD-ROM auszuliefern.\n\nAuf der COMDEX demonstrierte Microsoft die Mehrprozessorfähigkeiten des Betriebssystems und gab Entwicklungskits für 32-Bit-Software an ausgewählte Entwickler weiter. Als Auslieferungstermin stellte das Unternehmen nunmehr Ende 1992 in Aussicht. Die Reaktionen auf die Vorstellung waren überwältigend, das PC Magazine nannte Windows NT , hielt es jedoch gleichzeitig für unwahrscheinlich, dass die versprochene Abwärtskompatibilität auch ins Endprodukt übernommen würde. Die nicht funktionierende MIPS-Version drohte langfristig das Ziel der Portabilität des Betriebssystems zu gefährden, und so konzentrierten sich die Entwickler auf diese Version und veröffentlichten am 23. Dezember eine Version des Entwicklungskits für den MIPS R4000-Prozessor.\n\nDas Ziel war nun, Entwickler zur Programmierung von Software für das neue Betriebssystem zu bewegen. Microsoft kündigte zu diesem Zweck im März 1992 mit Win32s eine Schnittstelle an, mit der Windows-NT-Anwendungen auch auf Windows 3.1 ausgeführt werden konnten, sodass diese bereits vor der Veröffentlichung von Windows NT nutzbar sein sollten. Indes verzögerte sich die geplante Auslieferung des Betriebssystems immer weiter, denn die Entwicklung dauerte länger als ursprünglich angenommen. Zudem mussten Gerätetreiber erstellt werden um bestehende Hardware ansprechen zu können, da Windows NT nun ein Bestandteil der Windows-Reihe war.\n\nAuf einer vom 6. bis zum 8. Juli 1992 in San Francisco stattfindenden Konferenz präsentierte Microsoft Windows NT erneut und gab CDs mit einer Vorabversion heraus, die unter anderem eine neue Version des Entwicklungskits enthielt. Diese beinhalteten unter anderem ein Programm, das die Portierung von 16-Bit-Windowsprogrammen auf das neue Betriebssystem vereinfachte. Außerdem demonstrierte Microsoft die Lauffähigkeit von Windows NT auf x86- und MIPS-Rechnern. Zwar beinhaltete diese Version nunmehr die Kompatibilität zu DOS- und 16-Bit-Windowsanwendungen sowie das OS/2-Subsystem, jedoch war diese noch sehr unausgereift und die entsprechenden Programme stürzten häufig ab. Das Dateisystem NTFS war ebenfalls vorhanden, galt aber noch als instabil. Das Unternehmen kündigte in diesem Zusammenhang eine 32-Bit-Version des Microsoft SQL Servers für Windows NT an.\n\nDer hohe Speicherbedarf des Betriebssystems war allerdings ein Problem; die \"PC Week\" hielt nach einem eigenen Test Windows NT für praktisch unbenutzbar. Die meisten PCs der damaligen Zeit hatten einen Arbeitsspeicher von 4 Megabyte, die Entwickler hielten für einen typischen Rechner allerdings 16 Megabyte für notwendig. Aufgrund der hohen Preise für Arbeitsspeicher befürchteten die Entwickler, dass der Markt Windows NT wegen seines Speicherbedarfs nicht annehmen werde. Bis zu diesem Zeitpunkt war dies noch kein Thema gewesen, im Vordergrund stand, dass das neue Betriebssystem sich an OS/2 und Unix messen müsse und die Entwickler daher mit Funktionalität punkten wollten. Auch die Leistung des Betriebssystems war nicht optimal. Das Dateisystem NTFS etwa war langsamer als die beiden anderen von Windows NT unterstützten Dateisysteme, ebenso wie die Textausgabe auf dem Bildschirm. Am 12. Oktober 1992 begann der öffentliche Betatest von Windows NT und Microsoft versandte 20.000 Kopien einer Vorabversion an Betatester, um Programmfehler aufzuspüren. Das Betriebssystem galt nun als stabil, sodass sich die Entwickler Geschwindigkeitsoptimierungen annahmen. So nutzten die Entwickler einen Trick im Super-VGA-Treiber, um die Ausgabe deutlich zu beschleunigen. Durch die Auslagerung des Betriebssystemkerns sollte der Speicherverbrauch des Betriebssystems gesenkt werden.\n\nMicrosoft nutzte die COMDEX im November 1992, um eine große Anzahl an Drittanbietersoftware für Windows NT vorzuführen. Noch immer bereitete die Leistung des Betriebssystems den Entwicklern Probleme. Analysten erwarteten bereits einen Marktvorteil des Konkurrenten IBM, denn dessen Betriebssystem OS/2 2.0 war bereits auf dem Markt, während sich der Veröffentlichungstermin von Windows NT ins Jahr 1993 verschob. Bill Gates wusste, dass IBM an einer Version von OS/2 arbeitete, die, ebenso wie Windows NT, DOS- und Windows-3.1-Anwendungen ausführen konnte. Sollte Windows NT nicht schnell genug sein, befürchtete er, dass die Anwender zu OS/2 wechseln würden. Im Februar 1993 schließlich waren die Entwickler so weit fortgeschritten, dass sich Gates von der Leistung des Betriebssystems überzeugt zeigte.\n\nDie Entwickler hofften, Windows NT bis zum 10. Mai fertigzustellen, denn am 26. Mai begann die COMDEX in Atlanta, und an diesem Tag sollte Bill Gates das neue Betriebssystem publikumswirksam vorstellen. Am 8. März 1993 lieferte Microsoft die letzte öffentliche Vorabversion an über 70.000 Personen aus. Gleichzeitig erschienen erstmals Details über die Server-Variante \"Windows NT Advanced Server\", die zuvor nur als \"Windows NT with LAN Manager\" bekannt war. Ursprünglich sollte der LAN Manager, ähnlich wie bei OS/2 und dem LAN Server, ein separater Aufsatz auf Windows NT sein, aber Microsoft arbeitete die Funktionen des LAN Managers letztendlich in das Betriebssystem ein. Zwar war diese Vorabversion wesentlich stabiler und schneller als die letzte Vorabversion, aber aufgrund der weiterhin zahlreichen Programmfehler befürchteten Betatester, dass das Betriebssystem entweder zu früh im unfertigen Zustand herausgegeben, oder die Veröffentlichung sich zum Ende des Jahres 1993 verzögern würde. Die Entwickler realisierten schnell, dass die Behebung dieser Fehler eine längere Zeit in Anspruch nehmen würde, und der geplante Veröffentlichungstermin am 10. Mai nicht zu halten war.\n\nMicrosoft stellte Windows NT auf der COMDEX im Mai 1993 letztmals vor der Veröffentlichung vor. Aufgrund der Neuausrichtung des Betriebssystems in die High-End-Sparte von Seiten Microsofts zeigten sich einige Softwarehersteller verunsichert, ob sich Windows NT auch als Client-Betriebssystem durchsetzen würde, und zögerten daher mit der Portierung ihrer Software auf die 32-Bit-Architektur. Diese Softwarehersteller hatten bereits zuvor schlechte Erfahrungen mit OS/2 gemacht und waren dadurch vorsichtiger geworden. Bill Gates kündigte an, dass das neue Betriebssystem innerhalb von 60 Tagen geliefert werden würde. In den letzten drei Monaten vor der Veröffentlichung behob das Entwicklerteam nur noch Fehler, entwickelte das Betriebssystem in diesem späten Stadium des Entwicklungszyklus allerdings nicht mehr weiter. Die Ideen flossen jedoch in die nächste Version des Betriebssystems ein, dem späteren Windows NT 3.5.\n\nAm 26. Juli 1993 erschien Windows NT 3.1 und Windows NT 3.1 Advanced Server, zunächst nur für x86- und MIPS-Prozessoren, in den Vereinigten Staaten. Im September gleichen Jahres folgte die Portierung auf Alpha-Prozessoren. Die Versionsnummer 3.1 wurde bewusst gewählt, um mit dem 16-Bit-Betriebssystemaufsatz Windows 3.1 hinsichtlich der Versionsnummer gleichauf zu sein. Der Preis in den USA betrug 495 USD für die normale Version; der Advanced Server sollte ursprünglich 2995 USD kosten und nur in den ersten sechs Monaten zu einem reduzierten Preis von 1495 USD erhältlich sein, dieser reduzierte Preis wurde letztendlich bis zur Veröffentlichung des Nachfolgers beibehalten. Knapp 250 Programmierer schrieben insgesamt 5,6 Millionen Codezeilen, die Entwicklung kostete insgesamt 150 Millionen USD. Die Entscheidung, so früh wie möglich auf dem eigenen Betriebssystem zu arbeiten, zahlte sich letztendlich aus, denn so konnten viele Programmfehler im Zuge des Entwicklungsprozesses behoben werden. Alleine im letzten Jahr der Entwicklung behob das Entwicklerteam so über 30.000 Programmfehler.\n\nDer Behebung von Fehlern dienten drei in den folgenden Monaten veröffentlichte Service Packs: Am 8. Oktober 1993 kam das Service Pack 1 heraus, am 24. Januar 1994 folgte das Service Pack 2 und am 29. Oktober 1994 erschien schließlich das Service Pack 3. Die Verbreitung erfolgte über CD und Disketten, aber auch über Mailboxen, Compuserve und auch das damals neue Internet. Ende des Supports war für Endkunden der 31. Dezember 2000.\n\nWindows NT 3.1 war neben Englisch auch auf Deutsch, Französisch, Niederländisch, Japanisch, Spanisch und Schwedisch erhältlich. In Dänisch, Finnisch, Italienisch, Norwegisch und Portugiesisch wurde nur die Workstation-Variante, nicht aber der Server übersetzt.\n\nCutler hatte für Windows NT drei Hauptziele gesetzt. Das erste Ziel war Portabilität: Anders als bisherige Betriebssysteme, die meist eng mit der zugehörigen Architektur verbunden waren, sollte Windows NT auf mehreren Prozessorarchitekturen lauffähig sein. Dies bedeutete, dass das Betriebssystem in einer höheren Programmiersprache geschrieben werden musste; Cutler wählte die Programmiersprache C, da viele Programmierer mit ihr bereits vertraut waren. Er wusste schon während der Planungsphase, dass Windows NT dadurch einen weit größeren Speicherverbrauch haben würde als alle bisherigen Betriebssysteme. Neben dem Grafiksystem und Teilen des Netzwerksystems, die die Programmiersprache C++ verwenden, wurden lediglich Abschnitte, die zwingend Hardwarezugriff benötigen, sowie geschwindigkeitskritische Funktionen in Assemblersprache verfasst; diese Abschnitte wurden isoliert, sodass sie während der Portierung leicht durch entsprechenden Maschinencode der jeweiligen Architektur ersetzt werden konnten.\n\nDas zweite Ziel war Zuverlässigkeit: Das System sollte nicht mehr durch ein fehlerhaftes Programm oder fehlerhafte Hardware zum Absturz gebracht werden können. Dadurch sollten Computer auch für systemkritische Anwendungen attraktiv werden. Um dieses Ziel zu erfüllen, wurde die Architektur von Windows NT so gestaltet, dass der Betriebssystemkern abgeschottet war und Anwendungen nicht direkt auf ihn zugreifen konnten. Der Betriebssystemkern selbst war als Microkernel geplant, auf den verschiedene Bestandteile des Betriebssystemkerns aufbauen; mit diesem Konzept war Cutler bereits aus seiner Zeit bei DEC vertraut. Fehler, auf die das Betriebssystem trifft, werden als Exceptions abgefangen, sodass diese das System möglichst nicht beeinträchtigen. Das Dateisystem NTFS ist als robustes Dateisystem ausgelegt und soll auch durch Festplattenfehler nicht zerstört werden können. Beschädigte Daten, etwa durch fehlerhafte Sektoren, versucht das Betriebssystem so gut wie möglich wiederherzustellen. Zuverlässigkeit beinhaltet auch Sicherheit, denn auch gegen externe Angriffe sollte das Betriebssystem gefeit sein. Auf Mainframes war es längst üblich, dass jeder Benutzer ein eigenes Benutzerkonto besaß, unter dem er sich anmeldete und dem ein Administrator bestimmte Rechte zuwies; so konnte etwa Benutzern der Zugriff auf sensible Dokumente unterbunden werden. Eine virtuelle Speicherverwaltung sollte dem Betriebssystem ermöglichen, den Speicher von Programmen zu verwalten. So sollten Anwendungen eines Benutzers den Zugriff auf fremde Speicherbereiche verboten werden, um so zu verhindern, dass etwa der Speicher von Programmen eines anderen Benutzers ausgelesen oder manipuliert wird.\n\nDas dritte Ziel hieß \"Personalität\": Das Betriebssystem sollte Programme verschiedener Betriebssysteme ausführen können, etwa Windows-, MS-DOS- und OS/2-Programme. Bereits vorher gab es den Mach-Kernel, der ein ähnliches Konzept verfolgte, indem die APIs in Komponenten ausgelagert wurden, die außerhalb des Betriebssystemkerns als Anwendungsprogramme operierten. Diese Komponenten konnten problemlos geändert, oder auch neue hinzugefügt werden. Windows NT übernahm dieses Konzept in Form von sogenannten Subsystemen. Bill Gates war vor allem die Kompatibilität zu Windows-Programmen wichtig, denn er sah, wie OS/2 auf dem Markt scheiterte, weil es keine Rücksicht auf diese Kompatibilität nahm. Er erkannte, dass die Kunden ihre alte Software auch auf einem neuen Betriebssystem weiter benutzen wollen und kam so zu dem Schluss, dass Windows NT alte Windows-Programme unterstützen musste, um erfolgreich zu sein.\n\nNeben all diesen Zielen versuchten die Entwickler, die Geschwindigkeit des Betriebssystems im Auge zu behalten. Sie untersuchten das Laufzeitverhalten wichtiger Funktionen des Betriebssystems und optimierten dieses auf schnelle Ausführungsgeschwindigkeit. Große Teile des Netzwerksystems sind zwecks Steigerung der Netzwerkperformance in den Betriebssystemkern eingebaut.\n\nWindows NT wurde von Anfang an als Netzwerkbetriebssystem geplant. In diesem Bereich war Novell mit seinem Produkt \"NetWare\" Marktführer und Microsoft hatte es bisher nicht geschafft, ein ebenbürtiges Produkt zu entwickeln. Cutler hoffte, mit einem zuverlässigen netzwerkfähigen Betriebssystem weitere Kunden zu gewinnen. Bill Gates hatte bereits in den 1980er-Jahren mit den Produkten MS-DOS und Windows den Markt für Desktopbetriebssysteme dominiert und hoffte, mit Windows NT ähnliches auch im Netzwerkgeschäft zu vollziehen. Vor allem in den damals als neuer Rechnertyp entstehenden Servern sah Gates einen Markt für Windows NT, während er gleichzeitig nicht vor 1995 von einem Erfolg im Desktopmarkt ausging.\n\nIn späteren Interviews, unter anderem mit dem Produktmanager von Windows NT, David Thacher, wurde das Betriebssystem daher als High-End-Betriebssystem positioniert; es sollte Windows 3.1 nicht komplett ersetzen, sondern die Produktpalette mit einem Betriebssystem für sicherheitskritische Anwendungen ergänzen. Microsoft erwartete Verkaufszahlen von 10 bis 20 Prozent aller Windows-Betriebssysteme sowie einen Verbreitungsgrad von 10 Prozent im High-End-Markt im ersten Jahr, was ungefähr einer Auflage von einer Million Kopien entspräche.\n\nDie Benutzeroberfläche von Windows NT 3.1 hat zwar das gleiche Aussehen wie Windows 3.1, intern ist es aber ein von Grund auf neu entwickeltes Betriebssystem. Es basiert dabei nicht auf MS-DOS, sondern ist ein vollkommen eigenständiges 32-Bit-Betriebssystem; zahlreiche Konzepte wurden dabei aus Cutlers vorherigem Betriebssystem VMS übernommen. Die Architektur von Windows NT ist eine Kombination verschiedener Modelle: der Betriebssystemkern ist modular aufgebaut, Anwendungen hingegen werden entsprechend dem Client-Server-Modell als Clients aufgefasst, die mit einem Server kommunizieren, um Funktionen aufzurufen. Systemressourcen wie Speicher, Dateien oder Geräte werden als Objekte betrachtet, auf die stets in gleicher Weise mittels Handles zugegriffen werden kann und die dadurch vor unbefugtem Zugriff geschützt werden können.\n\nDas Betriebssystem ist für Mehrprozessorsysteme konzipiert; es unterstützt präemptives Multitasking sowie Threads, um mehrere Rechenprozesse gleichzeitig laufen lassen zu können. Über symmetrisches Multiprozessing wird die Rechenlast gleichmäßig auf alle verfügbaren Prozessoren verteilt. Die Möglichkeiten der Interprozesskommunikation sind in Windows NT 3.1 vollständig auf Netzwerke ausgelegt. Funktionen wie Pipes lassen sich auf gleiche Weise sowohl lokal als auch über das Netzwerk verwenden. Zwei neu eingeführte Funktionen, der Remote Procedure Call (RPC) einerseits und Netzwerk-DDE, eine Erweiterung von Dynamic Data Exchange (DDE), andererseits, vereinfachen den Zugriff auf sowie den Datenaustausch zwischen Prozessen, die sich auf verschiedenen Computern in einem Netzwerk befinden.\n\nDer Betriebssystemkern ist so konzipiert, dass er bestimmte Elemente eines monolithischen Kernels mit denen eines Microkernels kombiniert; dies wird heute meist als Hybridkernel bezeichnet. Die unterste Schicht bildet die Hardwareabstraktionsschicht, welche das Betriebssystem von der Hardware isoliert, sodass das Betriebssystem leicht auf andere Prozessorarchitekturen portierbar ist. Windows-NT-Programmdateien sind aber immer nur auf einer bestimmten Prozessorarchitektur lauffähig und müssen für jede Plattform neu kompiliert werden. Der darüber liegende Kernel enthält nur grundlegende Funktionen wie Interruptverwaltung und Prozessorsynchronisierung, andere Kernelfunktionen werden durch Module wahrgenommen, die unabhängig voneinander sind und ausgetauscht werden können, ohne den Rest des Betriebssystems zu beeinflussen.\n\nAuf dem Betriebssystemkern bauen die Subsysteme auf. Es gibt zwei Arten von Subsystemen: auf der einen Seite sind dies die sogenannten \"integralen Subsysteme\", die wichtige Basisfunktionen des Betriebssystems durchführen. Dazu zählt unter anderem das Sicherheitssubsystem, welches den Anmeldeprozess durchführt und die Sicherheit des Systems überwacht. Auf der anderen Seite gibt es die \"Umgebungssubsysteme\", die Anwendungsprogrammen die Funktionen des Betriebssystems in Form von Programmierschnittstellen zugänglich macht. Den Grundstein bildet das 32-Bit-Subsystem, welches für Windows NT geschriebene 32-Bit-Programme ausführt. Das 32-Bit-Subsystem enthält darüber hinaus sämtliche Ausgabefunktionen wie das Graphics Device Interface (GDI), sodass die anderen Subsysteme zur Ausgabe von Text oder Grafik das 32-Bit-Subsystem aufrufen müssen. Windows NT 3.1 beinhaltet neben dem 32-Bit-Subsystem zwei weitere Subsysteme. Dies sind zum einen das POSIX-Subsystem, welches das Ausführen von POSIX-kompatiblen Programmen ermöglicht, die für Windows NT 3.1 kompiliert wurden. Zum anderen enthält die x86-Version von Windows NT 3.1 das OS/2-Subsystem, mit dem kommandozeilenbasierte OS/2-1.x-Programme gestartet werden können.\n\nDie sogenannte Virtual DOS Machine (VDM) wird manchmal auch als Subsystem betrachtet, ist strenggenommen aber eine einfache 32-Bit-Windowsanwendung. Sie erlaubt das Starten von Programmen, die ursprünglich für MS-DOS geschrieben wurden. Die Virtual DOS Machine nutzt dazu auf x86-Rechnern den Virtual 8086 Mode, um MS-DOS-Programme direkt auszuführen, auf RISC-Rechnern wird hingegen ein von \"Insignia Solutions\" lizenzierter Emulator verwendet, der einen Intel-80286-Prozessor emuliert. Aufbauend auf der VDM ist Windows on Windows (WoW), welches es ermöglicht, Programme für 16-Bit-Windowsversionen wie Windows 3.1 auszuführen. Während reine DOS-Programme jeweils in einer eigenen VDM ausgeführt werden, wird für 16-Bit-Windowsprogramme eine gemeinsame VDM verwendet, sodass sie sich einen Speicherbereich teilen. Dies geschieht aus Kompatibilitätsgründen, da einige Programme von der Eigenschaft abhängig sind, ihren Speicher gemeinsam zu verwalten (so etwa Microsoft Mail und Schedule+). Dies führt jedoch dazu, dass diese Programme immer noch im kooperativen Multitasking untereinander laufen und ein fehlerhaftes Programm alle anderen 16-Bit-Windowsprogramme (jedoch nicht Windows NT selbst) zum Absturz bringen kann. Nicht alle MS-DOS- und 16-Bit-Windowsprogramme sind unter Windows NT 3.1 funktionsfähig, da das Betriebssystem Programmen nicht gestattet, direkt auf die Hardware zuzugreifen; einige Anwendungen benötigen zudem VxD-Dateien, die in Windows NT 3.1 nicht verwendet werden können.\n\nWindows NT 3.1 enthält mit NTLDR einen Bootmanager, der beim Systemstart auf x86-basierten Rechnern geladen wird. Dieser ermöglicht es, sowohl eine oder mehrere Instanzen von Windows NT 3.1 selbst als auch MS-DOS und OS/2 1.x parallel auf einem Rechner zu betreiben. NTLDR ist nicht in den RISC-Versionen enthalten, da die Firmware dieser Architekturen einen eigenen Bootmanager mitliefert.\n\nJeder Benutzer muss sich in Windows NT 3.1 nach dem Start des Betriebssystems anmelden, indem die Tastenkombination Strg+Alt+Entf gedrückt und Benutzername und Passwort eingegeben werden. Zur Authentifizierung nutzt Windows NT 3.1 das neu eingeführte NTLM-Verfahren, welches gegenüber dem in vorherigen Netzwerkprodukten von Microsoft verwendeten LM-Hash auch die Unterscheidung zwischen Groß- und Kleinbuchstaben im Passwort ermöglicht. Benutzer haben jeweils ein eigenes Konto, auf dem benutzerspezifische Einstellungen gespeichert werden; dazu zählen etwa das Farbschema oder das Hintergrundbild. Benutzern können bestimmte Rechte zugewiesen werden, wie das Ändern der Systemzeit oder das Herunterfahren des Systems. Zur Vereinfachung der Verwaltung ist es auch möglich, Benutzer zu gruppieren und den Benutzergruppen entsprechende Rechte zuzuweisen. Die Gruppen des Programm-Managers sind unterteilt in allgemeine und persönliche Gruppen. Allgemeine Gruppen stehen allen Benutzern des Computers zur Verfügung, persönliche Gruppen nur dem jeweils angemeldeten Benutzer.\nWindows NT 3.1 beinhaltet das neue Dateisystem NTFS. Dieses ist einerseits auf Robustheit ausgelegt und soll auch Hardwarefehler überstehen können, andererseits ermöglicht es die Zuweisung bestimmter Lese- und Schreibrechte an Benutzer oder Benutzergruppen auf Dateiebene. NTFS unterstützt lange Dateinamen sowie Funktionen zur Unterstützung von POSIX-Programmen wie harte Links. Aus Kompatibilitätsgründen unterstützt Windows NT 3.1 weiterhin das Dateisystem FAT16 sowie das von OS/2 übernommene Dateisystem HPFS.\n\nAls Netzwerkbetriebssystem konzipiert, unterstützt Windows NT 3.1 zahlreiche Netzwerkprotokolle. Neben IPX/SPX, DLC und dem MS-eigenen NetBEUI bietet das Betriebssystem das TCP/IP-Protokoll an, mit dem auch eine Verbindung zum Internet möglich ist. Ähnlich wie in \"Windows für Workgroups\" können Ordner und Drucker freigegeben werden, sodass der Zugriff und die Konfiguration dieser Ressourcen, entsprechende Benutzerrechte vorausgesetzt, über das Netzwerk möglich ist. Der Remote Access Service (RAS) erlaubt es einem Client, der sich außerhalb eines Windows-NT-Netzwerks befindet, sich über ein Modem, ISDN oder X.25 mit diesem Netzwerk zu verbinden und auf dessen Ressourcen zugreifen. Die Workstation erlaubt eine RAS-Verbindung, der Server hingegen 64. Zudem beinhaltet Windows NT 3.1 einen FTP-Server nebst Client.\n\nDer Druck-Manager enthält, neben der Verwaltung lokaler und Netzwerkdrucker, einige weitere Funktionen. Ähnlich wie bei Dateien können Benutzern bestimmte Rechte zugewiesen werden, etwa das Recht, auf einem bestimmten Drucker überhaupt drucken zu dürfen. Ebenso lassen sich mehrere Drucker zu einem Druckerpool gruppieren, sodass Druckaufträge automatisch auf alle Drucker verteilt werden. Bei der Installation eines Netzwerkdruckers werden die nötigen Druckertreiber automatisch über das Netzwerk auf den jeweiligen Clientrechner heruntergeladen, sodass das manuelle Installieren der Druckertreiber für jeden Rechner entfällt.\n\nWindows NT 3.1 unterstützt den seinerzeit neuen Unicode-Standard, einen Zeichensatz, mit dem sich mehrere Sprachen darstellen lassen. Dadurch sollte die Lokalisierung des Betriebssystems vereinfacht werden. Sämtliche Zeichenketten sowie Datei- und Ordnernamen werden intern in Unicode verarbeitet, jedoch unterstützen die mitgelieferten Programme, wie etwa der Datei-Manager, Unicode nicht, sodass etwa auf Ordner, die Unicode-Zeichen im Namen enthalten, nicht zugegriffen werden kann. Zu Demonstrationszwecken ist mit \"Lucida Sans Unicode\" eine Unicode-Schriftart im Lieferumfang enthalten, die jedoch standardmäßig nicht installiert wird. Die bisherigen Codepages werden aus Kompatibilitätsgründen weiterhin unterstützt.\n\nMit der Windows-Registry, einer Registrierungsdatenbank, führt Windows NT 3.1 eine zentrale, hierarchisch aufgebaute Konfigurationsdatenbank ein. Diese soll die weit verbreiteten textbasierten Konfigurationsdateien, wie INI-Dateien, AUTOEXEC.BAT und CONFIG.SYS ablösen und die Konfiguration von Rechnern über das Netzwerk erleichtern. Über den undokumentierten Registrierungseditor lässt sich die Registry vom Anwender einsehen und bearbeiten. Die AUTOEXEC.BAT wird vom Betriebssystem bei der Anmeldung weiterhin ausgelesen, um Umgebungsvariablen, die in dieser Datei definiert sind, zu setzen.\n\nDer \"Advanced Server\" ist dafür konzipiert, die Workstations zu verwalten. Er kann dazu als Domain Controller dienen, auf dem sämtliche Benutzer und Gruppen sowie ihre Rechte gespeichert sind. Dadurch kann sich ein Benutzer von jedem Rechner, der sich in der Domain befindet, anmelden, und auch die Benutzerverwaltung kann zentral auf dem Server erfolgen. Zu anderen Domains lassen sich Vertrauensstellungen aufbauen, um domainübergreifend auf Ressourcen zugreifen zu können. Dateien, wie etwa Anmeldungsskripte, können über den Replikatordienst automatisch auf allen Rechnern eines Netzwerks synchronisiert werden. Um die Verbindung mit Macintosh-Computern zu erleichtern, unterstützt der \"Advanced Server\" im Gegensatz zur Workstation auch das AppleTalk-Protokoll. Festplatten können im \"Advanced Server\" zu RAIDs zusammengelegt werden, unterstützt werden dabei die Systeme RAID 0, RAID 1, und RAID 5.\n\nDer Lieferumfang von Windows NT 3.1 bestand hauptsächlich aus 32-Bit-Versionen der Programme, die bereits in Windows 3.1 und \"Windows für Workgroups\" enthalten waren. Zusätzlich gab es einige speziell auf die Bedürfnisse von Windows NT zugeschnittene Dienstprogramme: den Benutzer-Manager, den Systemmonitor, den Festplatten-Manager, die Ereignisanzeige und das Programm \"Bandsicherung\". Für den \"Advanced Server\" standen zusätzliche serverspezifische Dienstprogramme bereit. Da Windows NT 3.1 nicht auf MS-DOS basiert, war mit cmd.exe ein neuer 32-Bit-Kommandozeilenprozessor enthalten, der weitestgehend kompatibel zu MS-DOS 5.0 war. Aus Kompatibilitätsgründen enthielt Windows NT 3.1 einige 16-Bit-Windows- und MS-DOS-Programme, wie Microsoft Write oder EDLIN.\n\nDa Windows NT 3.1 ein völlig neues Betriebssystem war, und somit bisherige Gerätetreiber nicht verwendet werden konnten, waren zahlreiche Treiber für gängige Geräte im Lieferumfang enthalten. Der Treiberumfang umfasste Gerätetreiber für einige verbreitete SCSI-Geräte (wie Festplatten, CD-ROM-Laufwerke, Bandlaufwerke und Scanner) sowie ISA-Peripheriegeräte (wie Grafikkarten, Soundkarten und Netzwerkkarten), ebenso wie Druckertreiber. Ausdrücklich nicht unterstützt wurde der PCI-Bus. Windows NT 3.1 konnte eine vorhandene unterbrechungsfreie Stromversorgung verwalten.\n\nDie Installation von Windows NT 3.1 war auf zwei Wegen möglich: einerseits von der CD-ROM mittels mitgelieferter Startdiskette, andererseits auf x86-Rechnern von einem Diskettensatz bestehend aus 22 3,5″-Disketten (beim \"Advanced Server\" 23 Disketten). Eine Netzwerkinstallation wurde ebenfalls unterstützt. Im Lieferumfang war zudem ein Coupon enthalten, mit dem bei Bedarf ein 5,25″-Diskettensatz bestehend aus 27 Disketten bzw. beim \"Advanced Server\" 28 Disketten nachbestellt werden konnte. Die CD-ROM enthielt gegenüber dem Diskettensatz zusätzliche Treiber und Programme.\n\nWindows NT 3.1 unterstützte mehrere Prozessor-Architekturen. Das Betriebssystem lief sowohl auf der x86-Architektur als auch auf zwei RISC-basierten Architekturen: den Rechnern der MIPS-R4000- und -R4400-Serie sowie Rechner mit Alpha-Prozessor.\n\nDie Mindestvoraussetzung für die Installation auf x86-Systemen waren ein 386-Prozessor mit 25 MHz, mindestens 12 MB Arbeitsspeicher, 75 MB Festplattenspeicher sowie eine VGA-Grafikkarte. Auf RISC-Systemen war die Mindestvoraussetzung mit 16 MB Arbeitsspeicher und 92 MB Festplattenspeicher architekturbedingt höher, auch war ein CD-ROM-Laufwerk zwingend notwendig. Der \"Advanced Server\" erforderte einen 386-Prozessor mit 16 MB RAM sowie 90 MB Festplattenspeicher, bzw. ein RISC-System mit ebenfalls 16 MB Arbeitsspeicher und 110 MB Festplattenspeicher. Windows NT 3.1 unterstützte Mehrprozessorsysteme mit bis zu zwei Prozessoren, der \"Advanced Server\" konnte vier Prozessoren verwalten.\n\nAufgrund eines Fehlers bei der Prozessorerkennung scheitert die Installation des Betriebssystems auf allen Prozessoren seit dem Pentium II. Dieses Problem wurde von Microsoft nie behoben, jedoch sind inoffizielle Patches verfügbar, um die Installation zu ermöglichen.\n\nBis zum Juni 1994 konnte Microsoft weltweit lediglich 250.000 Kopien absetzen, davon zehn Prozent in Deutschland. Obwohl Microsoft selbst hohe Verkaufszahlen von 200.000 Kopien innerhalb der ersten 60 Tage angab, berichteten Händler, dass die Nachfrage nach der anfänglichen Euphorie stark nachließ und sogar hinter OS/2 zurückfiel. Die Hardwareanforderungen galten für die damalige Zeit als sehr hoch; die empfohlene Systemvoraussetzung eines 486-Prozessors mit 16 MB RAM lag weit über dem Durchschnittsniveau damaliger Computer, außerdem erwies sich das Betriebssystem als zu langsam. 32-Bit-Programme, die die Fähigkeiten von Windows NT 3.1 hätten nutzen können, waren kaum vorhanden, sodass auf die alten 16-Bit-Programme zurückgegriffen werden musste; bei der Nutzung solcher Programme unter Windows NT 3.1 waren aber im Vergleich zu Windows 3.x Geschwindigkeitseinbußen zu verzeichnen. Schätzungen zufolge gab es im November 1993, vier Monate nach der Veröffentlichung von Windows NT 3.1, lediglich 150 Windows-NT-Programme, vor allem grundlegende Software, wie ein Office-Paket, war nicht für Windows NT 3.1 erhältlich. Während der Entwicklung des Betriebssystems änderten sich die API-Aufrufe, sodass 32-Bit-Programme, die mit der Vorabversion von Windows NT 3.1 kompiliert wurden, auf der endgültigen Version nicht mehr lauffähig waren. Dies betraf auch kommerzielle Produkte wie Microsoft Visual C++.\n\nRISC-Systeme mit Windows NT 3.1 hatten einen noch größeren Nachteil: Zwar waren sie von der Leistung her der x86-Version überlegen, aber es wurden so gut wie keine 32-Bit-Programme oder Treiber für diese Systeme portiert. In der Ausführung von 16-Bit-Programmen hatten RISC-Systeme das Nachsehen, da hier ein 80286 emuliert werden musste, was die Programme verglichen mit x86-Systemen nochmals massiv verlangsamte. Die Wahl eines 80286-Prozessors als Basis der Emulation bedeutete außerdem, dass MS-DOS- und 16-Bit-Windowsprogramme, die 80386-Befehle verwendeten, überhaupt nicht ausgeführt werden konnten, betroffen waren davon hauptsächlich Grafikprogramme und Entwicklungsumgebungen.\n\nEs gab jedoch nicht nur negative Rezensionen; so bewertete etwa die \"c’t\" die Multitaskingfähigkeiten des Betriebssystems vor allem im Vergleich zu Windows 3.1 sehr positiv. Auch die Installation galt als gut durchdacht. Eine ähnliche Meinung hatte die InfoWorld; die Installation des Betriebssystems gestaltete sich, verglichen mit der Größe des Betriebssystems, als sehr einfach, wenngleich die Installation von Disketten als zeitaufwendig galt. Der \"Advanced Server\", der die Nachfolge des erfolglosen Produkts LAN Manager antreten sollte, war technisch seinem Vorgängerprodukt weit überlegen, und galt als bedienerfreundlicher als Unix, litt aber letztendlich unter den gleichen Problemen wie die Workstation-Variante, etwa dem Geschwindigkeitsverlust bei der Benutzung von 16-Bit-Software. Der \"Advanced Server\" hatte zudem in großen Netzwerken einen deutlichen Preisvorteil, da der Preis pauschal war und nicht von der Anzahl der Clients abhängig.\n\nMit Windows NT betrat Microsoft einen Markt, in dem das Unternehmen vorher nie hatte Fuß fassen können und der dementsprechend überwiegend von den Betriebssystemen Unix, Novell NetWare und OS/2 geprägt war. Bei einem Test des Magazins InfoWorld im November 1993, der zahlreiche damals erhältliche Betriebssysteme auf ihre Netzwerkfähigkeit untersuchte, schnitt Windows NT 3.1 im Vergleich mit der Konkurrenz sehr schlecht ab: Als Client konnte es sich lediglich über NetBEUI mit dem eigenen Serverprodukt verbinden; Versuche, mit Unix, NetWare, und OS/2 Verbindungen aufzunehmen, scheiterten durchweg, was vor allem an fehlender Clientsoftware lag. Mit dem \"Advanced Server\" konnte sich neben dem Client lediglich der Macintosh (via AppleTalk) sowie, wenn auch beschränkt, OS/2 verbinden.\n\nWindows NT 3.1 konnte in Japan einen größeren Erfolg verbuchen als im Rest der Welt. Als das Betriebssystem im Januar 1994 in Japan erschien, setzten viele japanische Unternehmen noch Mainframes ein und waren gerade dabei, PCs einzurichten und diese zu vernetzen. NetWare und Unix waren in Japan, im Gegensatz zu Amerika, kaum verbreitet, sodass sich Windows NT 3.1 konkurrenzlos auf dem japanischen Markt ausbreiten konnte. Microsoft Japan betrieb ein besseres Marketing, das hauptsächlich Finanzunternehmen anvisierte; es arbeitete mit den größten Unternehmen der Computerbranche zusammen, um das Betriebssystem im finanziellen Sektor zu vermarkten.\n\nObwohl der eigentliche Erfolg des Systems nur moderat war, hatte Windows NT nachhaltigen Einfluss auf die weitere Entwicklung von Betriebssystemen: So begannen die Entwickler von Unix-Derivaten, ihre Betriebssysteme zu standardisieren. Auch Novell sorgte sich um seinen Marktanteil und kaufte einen Unix-Händler auf. Die Unix-Händler indes machten sich darüber Sorgen, dass Windows NT ihnen den Markt wegnehmen könnte. Die Portabilität des Betriebssystems hofften sich die Hersteller von Mikroprozessoren zunutze machen zu können, und so kündigten einige von ihnen an, das Betriebssystem auf ihre Architektur zu portieren, etwa auf den Clipper-Prozessor und die Sun-SPARC-Architektur. Windows NT werde den Desktopmarkt dominieren, sobald die Hardware leistungsfähig genug wird, um das Betriebssystem in einer akzeptablen Geschwindigkeit ausführen zu können.\n\n\n"}
{"id": "2624292", "url": "https://de.wikipedia.org/wiki?curid=2624292", "title": "Ubuntu Studio", "text": "Ubuntu Studio\n\nUbuntu Studio ist eine auf Ubuntu basierende Linux-Distribution, die speziell auf die Anforderungen von Audio-, Grafik- und Videobearbeitung ausgerichtet ist.\n\nFür Multimedia-Zwecke wurde den Ubuntu-Quellen ein Kernel mit Echtzeitfunktionen hinzugefügt, der dafür keinerlei Energiesparfunktionen unterstützt. Als Soundserver dient JACK. Außerdem gehören diverse Multimedia-Programme zum Umfang, wie beispielsweise CinePaint, die Videobearbeitung OpenShot, der Audio-Sequenzer Rosegarden, der Audio-Editor Ardour, die 3D-Grafiksoftware Blender, das Rastergrafikprogramm GIMP oder das DTP-Programm Scribus. \n\nDie erste stabile Version erschien am 10. Mai 2007 und basierte auf Ubuntu 7.04. Diese wurde als einzige Ubuntu-Version als ein DVD-Image mit knapp 900 MB ausgeliefert. Auf der Ubuntu-Website wird Ubuntu Studio als anerkanntes Derivat aufgeführt.\n\n\n"}
{"id": "2625421", "url": "https://de.wikipedia.org/wiki?curid=2625421", "title": "DVBViewer", "text": "DVBViewer\n\nDer DVBViewer ist eine proprietäre, kommerzielle Software, die alle grundsätzlichen Funktionen zum Empfang und zur Wiedergabe digitalen Fernsehens und Radioempfanges bietet. Weitere Funktionen sind durch Plug-ins nachrüstbar.\n\nDie Software ist nur über das Internet als Download verfügbar, um die Vertriebskosten gering zu halten.\n\nWichtigster Kunde und Abnehmer von Lizenzen ist der deutsche Hersteller von Fernseh-Digitalempfängern TechniSat. Dieser legt die Software seinen Produkten als \"DVBViewer Technisat Edition\" bei.\n\nNach Aussagen des Entwicklers war der DVBViewer die erste Applikation für den PC, die H.264/MPEG-4 AVC-Übertragungen sowohl live als auch als Aufnahme erkannte und abspielte.\n\n\nDVBViewer wird in drei Varianten, welche sich in ihrem Funktionsumfang unterscheiden, vertrieben:\n\nTransEdit ist ein Scanner für DVB-S (2), DVB-T und DVB-C, der als Zusatzprogramm für den DVBViewer Pro/GE einen Sendersuchlauf mit wesentlich erweiterten Möglichkeiten bietet. TransEdit unterstützt TechniSat-Karten (SkyStar2, AirStar2, CableStar2) sowie alle Geräte mit BDA-Treiber (Broadcast Driver Architecture). Außer dem erweiterten Sendersuchlauf gehören folgende wesentliche Funktionen zu TransEdit:\nTransEdit MMC ist die kostenlose, funktionsbeschränkte Version von TransEdit, welche auf der Website von DVBViewer verfügbar ist. Der Hauptzweck ist die Überprüfung, ob der DVBViewer in Zusammenspiel mit einem bestimmten Gerät funktioniert.\n\n\n\n"}
{"id": "2626542", "url": "https://de.wikipedia.org/wiki?curid=2626542", "title": "PHP Magazin", "text": "PHP Magazin\n\nDas PHP Magazin ist eine Computerzeitschrift mit Schwerpunkt PHP. Die Ausgaben erscheinen in einem zweimonatlichen Rhythmus mit einer Druckauflage von 6000 Exemplaren (Stand: 2. Quartal 2015). Herausgeber ist die Software & Support Media-Gruppe.\n\nUrsprünglich wurde das Themenspektrum von einem anderen Magazin des publizierenden Verlages abgedeckt. Ab dem Jahr 2001 wurde ein eigenes Magazin veröffentlicht. Das PHP Magazin setzt dabei schwerpunktmäßig auf PHP und andere Web-bezogene Themen, wie zum Beispiel Webgestaltung oder den Einsatz von Technologien wie Ajax. Das Thema wird jeweils aus Entwickler- und weniger aus Designer-Sicht dargestellt, wodurch sich das PHP Magazin von anderen Publikationen zum Thema Internet unterscheidet.\n\nAufgrund der guten Absatzentwicklung beschloss der Verlag ebenfalls im Jahr 2001, das Thema mit einer eigenen Konferenz weiter auszubauen. Hieraus ging im Folgenden die International PHP Conference hervor.\n\nBegleitet wird das Konzept des PHP Magazins durch einen umfangreichen Online-Auftritt, der die einzelnen Facetten der Print-Ausgabe, wie zum Beispiel aktuelle News, Buch-Rezensionen oder auch begleitende Fachartikel herausstellen soll. Das angegliederte Web-Forum dient zum Einen als Schnittstelle der einzelnen Publikationen des Verlages, wie auch als Recherche-Quelle, über die Entwickler auf Code-Beispiele und dezidierte Informationen zu einzelnen Fachgebieten zurückgreifen oder sich austauschen können.\n\n"}
{"id": "2626995", "url": "https://de.wikipedia.org/wiki?curid=2626995", "title": "BKM-Algorithmus", "text": "BKM-Algorithmus\n\nDer BKM-Algorithmus ist ein iterativer Algorithmus, mit dessen Hilfe sich die Logarithmus- und Exponentialfunktion effizient in digitalen Schaltungen berechnen lassen. Er wurde 1994 von J. C. Bajard, S. Kla und Jean-Michel Muller entwickelt, wovon sich auch die Bezeichnung ableitet.\n\nDer BKM-Algorithmus ist wie CORDIC-Algorithmus ein so genannter \"Shift-and-add-Algorithmus\", der auf bitweisen Verschiebungen und ganzzahligen Additionen in Addierwerken basiert. Divisionen werden ausschließlich mit negativen Potenzen von 2 durchgeführt, welche sich in digitalen Schaltungen direkt als bitweise Verschiebung implementieren lassen. Der Algorithmus kommt im Gegensatz zu dem CORDIC-Verfahren ohne Skalierungsfaktor aus und verwendet Logarithmentabellen anstelle der bei CORDIC notwendigen Arkustangens-Tabelle.\n\nDie Berechnung eines Funktionswertes erfolgt in einem Iterationsverfahren mit einer Konvergenzrate von ungefähr einem Bit pro Durchlauf. Aufgrund dieses Umstands wird dieser Algorithmus manchmal auch als \"Bitalgorithmus\" bezeichnet.\n\nGegeben sei die Iterationsvorschrift\nmit formula_2 und formula_3.\nDie Iterationsvorschrift ist per Induktion identisch mit\nSind alle formula_5, so sind alle formula_6. Sind alle formula_7 gilt formula_8. Tatsächlich kann mit der Iterationsvorschrift bei geeigneter Wahl der formula_9 jede reelle Zahl formula_10 im Bereich formula_11 als Grenzwert dargestellt werden.\n\nWeiterhin gelte die Iterationsvorschrift\nmit formula_13\noder äquivalent dazu\n\nFür numerische Berechnungen wird formula_15 durch eine vorab berechnete Tabelle realisiert.\n\nEs folgt sofort, dass formula_16 für alle formula_17 gilt.\nMit denselben Überlegungen wie oben ergibt sich für den Logarithmus\nder Bereich formula_18.\n\nUm die Logarithmusfunktion zu berechnen (dies wird beim BKM-Algorithmus auch als \"L-mode\" bezeichnet), wird in jedem Schritt getestet, ob formula_19 ist. Wenn ja, wird formula_20 und formula_21 berechnet. Nach formula_22 Schritten ist der Funktionswert mit einem Fehler formula_23 bestimmt.\n\nBeispiel als C++-Programm (Tabelle codice_1 unten):\n\nAuch andere Logarithmen lassen sich ohne Mehraufwand berechnen. Enthält die Tabelle die Werte für einen anderen Logarithmus als den zur Basis e, dann berechnet die Funktionen ebendiesen Logarithmus (Tabelle codice_2 ebenfalls im Anhang):\n\nDer erlaubte Bereich für das Argument ist der gleiche (1 ≤ Argument ≤ 4,768462058…). Im Fall des Logarithmus zur Basis 2 kann man den Exponenten vorher abtrennen (erhält damit den ganzzahligen Anteil des Logarithmus) und wendet auf das Restargument (welches zwischen 1 und 2 liegt) den Bitalgorithmus an. Da das Argument kleiner als 2,384231… ist, braucht die Iterationsschleife von \"k\" erst bei 1 anzufangen.\n\nUm die Exponentialfunktion zu berechnen (dies wird beim BKM-Algorithmus auch als \"E-mode\" bezeichnet), wird in jedem Schritt getestet, ob formula_24 ist. Wenn ja, wird formula_20 und formula_21 berechnet. Nach formula_22 Schritten ist der Funktionswert mit einem Fehler formula_28 bestimmt.\n\nBeispiel als C++-Programm (Tabelle codice_1 unten):\n\n static const double A_e [] = // A_e[k] = ln (1 + 0.5^k)\n\n\n"}
{"id": "2631024", "url": "https://de.wikipedia.org/wiki?curid=2631024", "title": "Microprofessor II", "text": "Microprofessor II\n\nDer Microprofessor II (MPF II), der im Jahre 1982 auf den Markt kam, war ein Homecomputer der Firma Multitech (heute Acer). Es war nach dem Microprofessor I der zweite Rechner der Firma und einer der ersten Apple-Clone, da er weitestgehend mit dem Apple II Computer baugleich war. Jedoch war von der Betriebssystemoberfläche lediglich das BASIC des Microprofessor II genau gleich mit dem des Apple II. Das Gerät wurde im Jahre 1984 zu einem Preis von 1.200,- DM verkauft.\n\n\nDer MPF II unterschied sich geringfügig vom Apple II:\n\nDer MPF II bot nicht den Textmodus des Apple II. Stattdessen wurde jeder Buchstabe per Software und nicht, wie beim Apple II, durch die Hardware erzeugt. Dies war nötig, da dies der einzige kostengünstige Weg war, auch chinesische Textzeichen auf dem Rechner abzubilden. Bei einer Hardwarelösung wäre das Gerät nicht mehr kostengünstig herzustellen gewesen.\n\nWie auch der Apple II hatte der MPF II zwei Grafik-Puffer. Jedoch startete der zweite Puffer des MPF II bei der Adresse 0xA000, wohingegen der Apple II bei 0x4000 begann.\n\n\n"}
{"id": "2631421", "url": "https://de.wikipedia.org/wiki?curid=2631421", "title": "MediaCoder", "text": "MediaCoder\n\nMediaCoder ist eine Software für Windows, um Audio-/Video-Dateien in Stapelverarbeitung zu transkodieren. Für Linux-Nutzer wird eine speziell für Wine optimierte Version zur Verfügung gestellt. Es verwendet einige Open-Source-Audio- und Video-Codecs und -Werkzeuge, um zwischen verschiedenen Formaten zu transkodieren, und hat viele darüber hinausgehende Funktionen wie etwa das Extrahieren von Audiospuren aus Videodateien. Es werden viele Formate unterstützt, darunter MP3, OGG Vorbis, AAC, Windows Media Audio, RealAudio, WAV, Xvid, DivX 4/5, MPEG-2, AVI, außerdem kann MediaCoder mit CD- und DVD-Datenträgern umgehen.\n\nMediaCoder ist eine grafische Benutzeroberfläche für eine Sammlung an mitgelieferten Kommandozeilen-Tools, -Codecs und -Multiplexern. Im Gegensatz zu vergleichbaren Programmen macht das Programm weitaus mehr, als nur die einzelnen Tools zu starten und auf ihre Ausführung zu warten. MediaCoder schreibt „piped“ die Ausgabe der Decoder in die Eingabe der Encoder anstatt temporäre Dateien zu verwenden. Dies bedeutet, dass große Mengen an Dateien ohne extremen Speicherplatzverbrauch konvertiert werden können. Einige der mitgelieferten Tools wurden gepatcht, um diese zusätzlichen Fähigkeiten zu ermöglichen.\n\nMediaCoder gibt es in mehreren Versionen, die zum Teil auf ganz spezielle Aufgabenbereiche zugeschnitten sind:\n\n\n\n\n\nEs kann nur eine begrenzte Anzahl von Audiodateien zur Konvertierung angegeben werden (\"Eingeschränkter Batch-Modus\"). Außerdem besteht nach Herstellerangabe ein Zwang zur Internet-Verbindung bei Programmstart. Eine Alternative ist der Kauf einer Premium-Ausgabe. Werbung für die Premium-Version wird an verschiedenen Stellen im Programm eingeblendet.\n\nDie Installationsdateien transportieren Drittsoftware, die versehentlich mitinstalliert werden kann. Beim Beenden des Programms wird der Online-Shop Amazon geöffnet (Stand: März 2013)\n\nMediaCoder verletzte die Urheberrechte mehrerer Open-Source-Projekte, darunter libavcodec und MPlayer.\n\n"}
{"id": "2633024", "url": "https://de.wikipedia.org/wiki?curid=2633024", "title": "Game Jackal", "text": "Game Jackal\n\nGame Jackal ist eine Emulationssoftware für Microsoft Windows, mit deren Hilfe die meisten aktuellen Computerspiele ohne das Vorhandensein des Original-Datenträgers gestartet werden können. Nachdem bereits fast 3 Jahre keine Updates veröffentlicht wurden, wurde die Entwicklung im März 2016 mit Schließung der Firma SlySoft eingestellt.\n\nIm Gegensatz zu anderen Emulationsprogrammen wie Virtual CD, Alcohol 120% oder DAEMON Tools erstellt Game Jackal keine möglichst genauen virtuellen 1:1-Abbildungen des Original-Datenträgers, sondern emuliert lediglich diejenigen Teile, die während des Programmlaufs normalerweise von der CD bzw. DVD gelesen würden. Da heutige Computerspiele in der Regel komplett auf der Festplatte installiert werden müssen und der Original-Datenträger nur von der Kopierschutzroutine auf Authentizität überprüft wird, genügt hierfür meist die Emulation weniger Sektoren. Anders als bei virtuellen 1:1-Abbildern, deren Größe in etwa der auf dem Original-Datenträger gespeicherten Datenmenge entspricht, sind von Game Jackal erzeugte Abbilder deshalb meist nur wenige Kilobyte groß.\n\nDer Zweck von Game Jackal ist vorrangig die bequemere Verwaltung der eigenen Spielesammlung, da durch die Software das Heraussuchen und häufige Wechseln von Datenträgern entfällt. Obgleich heutige gepresste CDs und DVDs relativ robust sind, ist die Tatsache, dass die Datenträger durch Einsatz des Programms geschont werden können, nicht zu unterschätzen. Die Hersteller haben diverse Maßnahmen getroffen, die sicherstellen sollen, dass Game Jackal nicht für Softwarepiraterie missbraucht wird. So enthält das Produkt keine Routinen, die den Kopierschutz von Computerspielen entfernen können. Um also eine funktionierende Emulation des Original-Datenträgers zu erstellen, muss dieser überhaupt vorliegen. Weiterhin erhält jede Installation des Programms einen individuellen sogenannten Profilschlüssel, der sich nicht ohne Weiteres ändern lässt. Game Jackal akzeptiert nur Emulationsabbilder, die mit dem eigenen gespeicherten Profilschlüssel erstellt wurden. Auf diese Weise ist sichergestellt, dass die Abbilddatei auf jedem Computer unter Verwendung des Original-Datenträgers neu erzeugt werden muss und es nicht möglich ist, einmal erstellte Abbilder zwischen verschiedenen Computern auszutauschen. Trotz dieser Vorkehrungen sind der Vertrieb und die kommerzielle Nutzung des Programms in Ländern, in denen das sogenannte „Umgehen technischer Schutzvorrichtungen“ verboten ist (z. B. Deutschland), wahrscheinlich nicht erlaubt.\n\nBis Ende des Jahres 2006 wurde Game Jackal von der australischen Firma „Jacal Consulting Pty.“ entwickelt und vertrieben. Im Angebot waren drei im Funktionsumfang geringfügig unterschiedliche Versionen mit den Bezeichnungen \"Game Jackal\", \"Game Jackal Professional\" und \"Game Jackal Enterprise Edition\". Aufgrund einer Änderung des australischen Urheberrechts musste die Firma den Verkauf des Produktes Anfang des Jahres 2007 einstellen. Die im karibischen Inselstaat Antigua und Barbuda ansässige Firma SlySoft erwarb daraufhin sämtliche Rechte an dieser Software und nahm sie in ihre Produktpalette auf. Nach einer mehrmonatigen Pause, die zur Weiterentwicklung und Anpassung erforderlich war, ist Game Jackal seit Juni 2007 wieder verfügbar und wurde von SlySoft bis zu dessen Schließung im März 2016 verkauft und betreut.\n\nNeben \"Game Jackal Pro\" für Einzelplatzrechner bot die Firma SlySoft auch \"Game Jackal Enterprise\" für die Bereitstellung von Computerspielen im Netzwerk an. Diese Version besteht aus einem Servermodul, mit dem die Abbilder erzeugt und verwaltet werden, und beliebig vielen, einzeln zu lizenzierenden Klientmodulen, die auf den gewünschten Arbeitsrechnern installiert werden und ausschließlich den Lesezugriff auf Abbilder ermöglichen. \"Game Jackal Enterprise\" wendet sich hauptsächlich an Internet-Cafés und Firmenkunden, da beim Erstkauf mindestens fünf Klientmodule lizenziert werden müssen. Die Erweiterung auf weitere Arbeitsplätze war während der Vertriebszeit durch Zukauf weiterer Klientmodule jederzeit möglich.\n\nNach der Schließung von SlySoft im März 2016 gab der Nachfolger Red Fox bekannt, dass Game Jackal nicht weiterentwickelt und zukünftig nicht mehr vertrieben wird. Die zuletzt veröffentliche Version vom Oktober 2013 kann auf der Homepage von Red Fox als \"End-of-Life\"-Produkt heruntergeladen werden, die Freischaltung zur Vollversion ist jedoch nur mit einem bereits vorhandenen Lizenzschlüssel möglich.\n\nDas Programm besitzt eine offene Plugin-Schnittstelle und kann daher im vom Hersteller vorgesehenen Rahmen um weitere Funktionen erweitert werden. So entstand etwa die Möglichkeit, auch (sehr kleine) Abbilder nicht kopiergeschützter Computerspiele anzulegen und somit tatsächlich die komplette Spielesammlung über die Oberfläche von Game Jackal verwalten zu können.\n\n"}
{"id": "2633160", "url": "https://de.wikipedia.org/wiki?curid=2633160", "title": "Laser 128", "text": "Laser 128\n\nDer Laser 128 war ein Klon der Apple II Heimcomputer-Serie, der 1984 zum ersten Mal von der Firma VTech produziert wurde. VTech entwickelte seinen Apple-kompatiblen ROM als Cleanroom-Implementierung durch Reverse Engineering, im Gegensatz zu den Apple-II-Kopien von Franklin. Apple Computer versuchte, VTech gerichtlich zu belangen und den Laser 128 vom Markt nehmen zu lassen, scheiterte aber damit, anders als bei Franklin.\n\nWie der Name schon andeutet, hatte der \"Laser 128\" Random-Access Memory in einer Größe von 128 KB. Wie der Apple IIc war er ein einteiliges, semi-portables Gerät mit Tragegriff und einem eingebauten 5¼-Zoll-Diskettenlaufwerk. Im Gegensatz zum Apple IIc, besaß er außerdem ein numerisches Tastenfeld und einen Apple-IIe-kompatiblen Erweiterungsschacht. Damit war der Laser 128 mit besseren Erweiterungsmöglichkeiten ausgestattet als der Apple IIc. Der Endkundenpreis belief sich auf gut 700 US-Dollar und war damit bedeutend geringer als beim Apple IIc.\n\nAls Reaktion auf den Laser 128 entwickelte Apple den \"Apple IIc+\". Dem folgte wiederum VTech mit dem \"Laser 128EX\" und dem \"Laser EX2\".\n\nDer Laser 128 wurde in den USA aggressiv beworben, sowohl von Versandfirmen als auch von Einzelhandelsketten wie Sears. Denn obwohl ihre Firmware nicht direkt vom Apple Pendant abstammte, war die Laser-128-Serie weitestgehend kompatibel zu den Konkurrenzprodukten von Apple.\n\nDazu trug vor allem bei, dass VTech eine Lizenz für Applesoft BASIC von Microsoft erworben hatte, genau wie Apple dies schon zuvor getan hatte. Da es zu Microsofts Politik gehörte, die Rechte an ihrer Software zu behalten, und Apple es versäumt hatte, einen Exklusiv-Vertriebs-Vertrag für Applesoft BASIC abzuschließen, konnte VTech ebenfalls eine Lizenz erwerben. Ein großer Teil der Apple Software beruhte auf verschiedenen Assembler-Routinen welche einen Teil des BASIC im ROM darstellten. Es gilt als sehr wahrscheinlich, dass der Laser nicht so erfolgreich gewesen wäre, wenn er keine kompatiblen ROM-Einstiegspunkte besessen hätte.\n\n"}
{"id": "2634736", "url": "https://de.wikipedia.org/wiki?curid=2634736", "title": "IBM 7030 Stretch", "text": "IBM 7030 Stretch\n\nDie IBM 7030, auch Stretch genannt, ist ein Supercomputer der 700/7000 series von IBM. Er wurde 1961 an das Los Alamos National Laboratory geliefert.\n\nDas ursprüngliche Preisangebot von 13,5 Millionen US-Dollar konnte zur Auslieferung nicht gehalten werden, da die projektierte Leistung nicht erreicht wurde. Der Computer wurde dann für 7,78 Millionen Dollar an diejenigen Kunden geliefert, die schon eine Bestellung gemacht hatten, und aus dem Angebot genommen. Obwohl die 7030 erheblich langsamer als erwartet war, war sie dennoch von 1961 bis 1964 der schnellste Rechner der Welt.\n\nDie Maschine war mit einer Wortbreite von 64 Bit ausgeführt. In jeder Speicherbank konnten 16.384 Wörter adressiert werden, im Maximalausbau standen 262.144 Wörter (2 Megabyte) zur Verfügung.\n\nMit der heutigen Rechnerarchitektur ist der Rechner nur schwer vergleichbar, da binäre und dezimale Festkommazahlen in variabler Länge von 1 bis 64 Bit, auch über Wortgrenzen hinweg, verarbeitet werden konnten, ebenso war die alphanumerische Sichtdarstellung variabel in „Bytes“ von 1 bis 8 Bit Breite. Die Breite der Befehlswörter war schaltbar zwischen 32 Bit und 64 Bit.\n\nDa er regelmäßig die Berechnungen in 64 Bit durchführte, zählt er als erster Vertreter einer 64-Bit-Architektur. Die kleinste Adressierungseinheit des IBM 7030 ist nicht ein Maschinenwort, sondern ein einzelnes Bit. D. h. eine 24-Bit-Adresse besteht aus einem 18-Bit-Block zur Adressierung eines Wortes im Hauptspeicher und aus einem 6-Bit-Block zur Adressierung eines Bits in dem Wort. Dieses Adressierungsschema hat sich aus praktischen Nachteilen in späteren Rechnerarchitekturen nicht durchgesetzt.\n\n"}
{"id": "2635287", "url": "https://de.wikipedia.org/wiki?curid=2635287", "title": "Institut für Computermusik und Elektronische Medien", "text": "Institut für Computermusik und Elektronische Medien\n\nDas Institut für Computermusik und Elektronische Medien (ICEM) an der Folkwang Universität der Künste in Essen wurde 1990 als zentrales Institut gegründet. Hervorgegangen aus dem Elektronischen Studio (gegründet 1971) ist das ICEM lebendiger Ort für Forschung und Lehre und beinhaltet neben den Studios für Elektronische Musik/Medien weitere Fachabteilungen für Audio- und Medientechnik.\n\nGründer und bis Oktober 2011 künstlerischer Leiter des ICEMs ist Dirk Reith.\nSeitdem wird das Institut von Thomas Neuhaus geleitet.\n\nMomentan stehen den Studierenden und Dozenten sieben Studios zur Verfügung:\n\nHauptaufgabe des ICEMs ist die Forschung und Lehre in kunstrelevanten Gebieten aus Informatik und Medientechnologie und deren Verwendung in den musikalisch-akustischen, visuellen und darstellenden Künsten sowie die Betreuung aller Projekte der Hochschule, bei denen Medien in umfassendem Maße eingesetzt werden.\n\nDurch die direkte Nachbarschaft zu Tanz, Theater und Wissenschaft wird den Studenten am ICEM eine Ausbildung geboten, die interdisziplinäre Projekte begünstigt.\n\nDas ICEM gibt in unregelmäßigen Abständen die CD-Reihe \"ex machina\" heraus, so z. B. die 2001 veröffentlichte \"Jubiläumsausgabe\" bestehend aus \"ex machina\" Volume 4-6, auf denen ICEM Produktionen aus drei Dekaden ein umfassendes Klangbild des elektronischen Schaffens am ICEM zeigen.\n\n\n"}
{"id": "2638335", "url": "https://de.wikipedia.org/wiki?curid=2638335", "title": "Adobe Illustrator", "text": "Adobe Illustrator\n\nAdobe Illustrator (kurz \"Ai\") ist ein vektorbasiertes Grafik- und Zeichenprogramm. Es dient also dem Herstellen von Computergrafiken, die man ohne Qualitätsverlust beliebig in ihrer Größe verändern kann – im Unterschied zu pixelbasierten Bildbearbeitungsprogrammen wie z. B. Photoshop. Der Name \"Illustrator\" wies von Anfang auf die Zielgruppe hin: Illustratoren, vor allem aus der Werbebranche. Das Programm wurde 1987 von dem kalifornischen Softwareunternehmen Adobe Systems für den Apple Macintosh entwickelt und wird bis heute von Adobe gepflegt und vermarktet. Es läuft auf den Computerbetriebssystemen Windows sowie Apple macOS und gilt seit Jahren als Standardanwendung auf dem Gebiet der Vektorgrafik.\n\nVon der Klassifizierung her liegt Illustrator zwischen technisch orientierten, ebenfalls vektorbasierten CAD-Programmen wie AutoCAD und künstlerisch ausgerichteten, pixelbasierten Mal- und Bildbearbeitungsprogrammen wie Painter und Photoshop. Alternativen zu Adobe Illustrator sind u. a. Affinity Designer, Corel Draw und Inkscape.\n\nHistorisch war Illustrator die erste kommerzielle Grafikanwendersoftware, die Bézierkurven zur mathematisch exakten Beschreibung geometrischer Formen benutzte. Illustrator speichert Dateien in ein eigenes Format (.ai), das auf der Mitte der 1980er Jahre etablierten Protokollsprache für Laserdrucker, PostScript, aufsetzte. Die Software war dennoch nie ein echter PostScripteditor.\n\nDie erste Illustratorversion verfügte nur über grundlegende Zeichenwerkzeuge. So gab es beispielsweise keine Möglichkeit, Objekte mit Löchern zu erstellen. Zudem gab es einen speziellen Vorschaumodus, in dem die Zeichnung nicht bearbeitet werden konnte.\n\nTrotz einer vereinfachten Benutzerführung und einiger neuer Werkzeuge (Überblendung, Freihandwerkzeug, Maßband, Pausstift sowie Farben, Masken und Muster) war es immer noch nicht möglich, Zeichnungen im Vorschaumodus zu bearbeiten oder Objekte mit Löchern zu erstellen.\n\nDer ersten Illustratorversion für Windows war kein großer Erfolg beschieden. CorelDRAW war damals Illustrator von Funktionsumfang und Bedienerfreundlichkeit her weit überlegen.\n\nAuch in dieser Version war immer noch keine Bearbeitung im Vorschaumodus möglich, doch wurden verknüpfte/zusammengesetzte Pfade (compound paths) eingeführt und somit die Möglichkeit, auch Objekte mit Löchern zu erzeugen. Weitere neue Funktionen waren unter anderem Diagramme, Direktauswahlwerkzeug, Gruppenauswahlwerkzeug, Lupe, Zentrieren, abgerundetes Rechteck, Ellipse vom Mittelpunkt, Transformierwerkzeuge und verbesserte Pfadbearbeitung. Zudem konnte man in dieser Version auch auf nicht-postscriptfähigen Druckern ausdrucken.\nDie Versionen für NeXTStep und andere Unix-Plattform waren kein sonderlicher Erfolg, da sich beide Plattformen im Anwenderbereich nicht durchgesetzt haben.\n\nAuch diese Version brachte für Adobe keinen großen Erfolg in der Windowswelt, da Corel Draw sich schon etabliert hatte und Illustrator 4.0 keine Weiterentwicklung von Version 3.0 für Macintosh war. Trotzdem gab es kleine Verbesserungen wie die Bearbeitung im Vorschaumodus.\n\nNun konnte man auch am Macintosh seine Zeichnungen direkt im Vorschaumodus bearbeiten. Die Benutzeroberfläche wurde verändert, und die schwebenden Paletten erhielten Einzug. Neben der vergrößerten Arbeitsfläche wurde die Undo-Funktion erweitert. Viele noch heute bekannte Werkzeuge (Messer, Seitenpositionierer, kalligraphischer Pinsel, Farbeimer, Farbverläufe, Ebenen) kamen hinzu.\n\nDiese Version lief nativ auf einem Power Macintosh und brachte verbesserte Import-/Export-Filter mit. So konnte man nun PDF-Dateien öffnen und direkt exportieren. Die typographischen Fähigkeiten wurden ebenfalls stark verbessert. So gab es endlich Spalten und eine Tabulatorstopp-Palette.\n\nNeben der besseren Unterstützung von Windows 95 gab es keine nennenswerten Verbesserungen. Alle Grafikprogramme für den PC (und auch Illustrator für den Mac) hatten zu jener Zeit Illustrator für Windows weit hinter sich gelassen.\n\nDiese Version bot weiter verbesserte Importfilter, wodurch es möglich war, TIFF-, PDF-, EPS- und PSD-Dateien zu platzieren. Zudem wurden nun Photoshop-Filter direkt unterstützt und auch einige von ihnen mitgeliefert. Neue Funktionen waren unter anderem \"In Pixelbild umwandeln\", Stern- und Polygonwerkzeuge, Spiralwerkzeug, Wirbel-Werkzeug, Ausrichten-Palette, Farbseparation, mathematische Operationen in den Paletten und Drag-and-Drop-Unterstützung. Die stark veränderte Benutzerführung bei der Pfadbearbeitung (an Photoshop angeglichen) hielt viele Käufer von einem Update ab.\n\nErstmals gab es eine Windows-Version, die den gleichen Funktionsumfang bot wie die Macintosh-Version. Die Benutzeroberfläche wurde noch weiter verbessert (verschachtelbare Paletten), und durch den Siegeszug des WWW kam auch erstmals die Fähigkeit in das Programm, mit RGB-Farben zu arbeiten. Das Textwerkzeug wurde um vertikale Ableger erweitert, und das Diagrammwerkzeug erhielt neue Diagrammarten. Auch das Farbmanagement wurde von Adobe nicht vernachlässigt, und Illustrator wurde mit der Multiple-Master-Palette das erste Programm, das solche Schriften direkt ohne den Adobe Type Manager verwenden und modifizieren konnte.\n\nMit jeder Version kamen Fähigkeiten von Photoshop auch zum Illustrator, und somit enthielt Illustrator nun eine Aktionspalette, Navigatorpalette und die Möglichkeit, Dateien mit Ebenen nach Photoshop zu exportieren. Eine große Neuerung war das Verlaufsgitter, das dank PostScript 3 und PDF auflösungsunabhängige flexible Verläufe ermöglichte. Die magnetischen Hilfslinien und Frei-Verzerren-Werkzeug erhöhten deutlich die Produktivität, und die Bild- und Spezialpinsel machten Zusatzprogramme wie Fractal Design Expression teilweise überflüssig. Zudem gab es endlich nachträglich anpassbare Überblendungen, wie es andere Programme schon länger boten.\n\nDie längere Entwicklungszeit gegenüber vorherigen Versionen machte sich in einem enormen Technologiesprung bemerkbar. Illustrator 9 war das erste Vektorprogramm, das Transparenzen in guter Qualität (und teilweise weiter auflösungsunabhängig) ausgeben konnte. Zwar hatte Corel Draw schon seit geraumer Zeit Transparenzen verfügbar, doch wurden diese in jedem Fall in Pixelbilder umgerechnet, was oft zu Belichtungsproblemen führte. Auch Macromedia FreeHand bot vor Illustrator Transparenzen an und warb sogar damit, das erste Programm zu sein, das Transparenzen als Vektorobjekte ausgibt. Anders als Illustrator 9 war Freehand aber stark in den Kombinationsmöglichkeiten eingeschränkt. Schon acht überlagerte Transparenzen waren für ein sinnvolles Arbeiten am Dokument zu viele. Illustrator 9 hatte zwar noch etwas Probleme bei komplizierten Dokumenten, doch das wurde in der folgenden Version 10 zum größten Teil behoben. Es gab aber noch weitere Neuerungen wie verschachtelte Ebenen, Effekte (Live-Filter), Stile und Photoshop-kompatibles Farbmanagement. Durch die vielen komplexen Funktionen wurde Illustrator 9 aber deutlich langsamer als seine Vorgänger, und viele Anwender stiegen nach dem Update wieder auf Version 8.0 um.\n\nAdobe positionierte Illustrator immer mehr auch als Web-Grafik-Anwendung, und somit lag es nahe, dass in diesem Bereich sehr viel verbessert wurde. Die Symbol-Palette half, Flash-Dateien klein zu halten, und das Slice-Werkzeug machte vielfach andere Anwendungen überflüssig. Der SVG-Export wurde auch stark verbessert, und die Variablen-Palette machte Illustrator-Dokumente anhand von XML-Dateien dynamisch.\nEndlich hielten auch Verzerrungshüllen in Illustrator Einzug, womit Illustrator wohl eines der letzten Grafikprogramme war, welches diese Funktion erhielt. Selbst an lang vermisste Werkzeuge (einfache Linien, Bogen und Gitter) hatte Adobe gedacht. Die meisten Probleme der Version 9.0 mit den Transparenzen wurden behoben, und auch die Arbeitsgeschwindigkeit erhöhte sich weiter.\n\nEnde Dezember 2003 erschien die deutsche Version von Illustrator CS. Seit dieser Version ist Illustrator integrierter Bestandteil der Adobe Creative Suite, wird aber auch weiterhin einzeln angeboten. Unter anderem gibt es erstmals echte 3D-Funktionen, welche dem nicht mehr weiterentwickelten Adobe Dimensions recht ähnlich sind. Die häufig angefragte Extrusionsfunktion wurde damit endlich in Illustrator verwirklicht. Der Umgang mit Schriften und fremdsprachigen Texten wurde komplett überarbeitet, und Illustrator unterstützte nun erstmals OpenType und Unicode ähnlich der Funktionen von Adobe InDesign. Zudem gibt es nun auch Absatz- und Zeichenstile. Die PDF-Unterstützung wurde dem aktuellen Stand (PDF 1.5, Acrobat 6) angepasst und unterstützt PDF-Ebenen, Schnittmarken und Passwortschutz. An eine neue gestalterische Funktion hat Adobe ebenfalls gedacht: Der Scribble-Effekt ermöglicht es, mit wenigen Mausklicks eine Zeichnung wie handgemalt aussehen zu lassen.\nEinige Probleme der Vorversion wurden behoben; so müssen EPS- und PDF-Dateien nicht mehr eingebettet werden, wenn sie mit Transparenzen in Berührung kommen. Auch wurde die Druckfunktion erweitert, der Umfang der Druckeinstellungen entspricht nun etwa den Möglichkeiten, die Adobe InDesign bietet.\n\nDer Funktionsumfang wurde um Möglichkeiten zum interaktiven Abpausen (Autotrace), interaktiven Malen (Füllen von Flächen, die durch mehrere Pfade eingeschlossen sind und Bearbeiten von Konturensegmenten), erweiterte Konturenoptionen und kolorierte Graustufen-Rasterbilder erweitert. Die Steuerungspalette stellt häufig gebrauchte Funktionen aus verschiedenen Paletten im aktuellen Bearbeitungskontext zur Verfügung.\n\nIntegration mit früheren Macromedia-Produkten, vor allem Flash.\n\nNeue Werkzeuge: Schnittbereich-Werkzeug (Crop Area Tool) und Vektor-Radierer\n\nNeue Funktionen: Der in Version CS2 (12) eingeführte Isolierte Bearbeitungsmodus für Gruppen wurde überarbeitet und auf das Editieren von Symbolen erweitert. Die Funktion Interaktive Farbe ermöglicht ein Editieren mehrerer bereits an Objekten verwendeten Farben z. B. durch Zuweisung anderer Farbharmonien oder das Lokalisieren auf dem Farbrad. Das Editieren der Ankerpunkte eines Pfads wurde durch Hinzufügen von Buttons für oft benötigte Funktionen und durch die bessere Hervorhebung der Position eines Punkts etwas vereinfacht.\n\nNeue Bedienfelder (vorher Paletten): Farbhilfe (Color Guide), Adobe Labs/Kuler & Know-how (nur in der US-Version)\n\nIn Illustrator CS4 können mehrere Artboards (Seiten) in unterschiedlichen Größen angelegt werden. Verbessert wurde darüber hinaus die Bearbeitung von Verläufen – diese können jetzt auch transparente Verlaufsfarben enthalten. Im Aussehenbedienfeld ist nicht mehr nur die Übersicht, sondern auch die Bearbeitung der Objekteigenschaften möglich. Als neues Werkzeug wurde der Tropfenpinsel (Blob-Tool) eingeführt, mit dem Benutzer Vektorobjekte \"malen\" können. Auch besitzt Illustrator jetzt eine interne Separationsvorschau.\n\nVerbesserungen gab es bei Schnittmasken, magnetischen Hilfslinien, Grafikstilen und dem Isolationsmodus.\nVollkommen überarbeitet wurde die Benutzeroberfläche: u. a. können offene Dateien getabbt werden.\n\nIn Version CS5 erhielt Illustrator neue Werkzeuge. Das Breitenwerkzeug dient zum Anpassen der Konturstärke an jeder beliebigen Stelle des Pfads. Mit dem Formerstellungswerkzeug werden Formen gebildet. Es funktioniert ähnlich wie Interaktiv malen, die Formen entstehen jedoch sofort. Das neu eingeführte Perspektivenraster hilft bei der Erstellung perspektivischer Szenen und Objekte und bringt ebenfalls eigene Werkzeuge mit. Neu hinzugekommen ist auch der Borstenpinsel, ein transparenter, sehr natürlich wirkender Pinsel mit Aquarellanmutung.\n\nVerbessert wurde die Bearbeitung von Zeichenflächen durch die Verwaltungsmöglichkeit in einer eigenen Palette sowie Symbole durch die Einführung von u. a. 9-Slice-Hilfslinien zur direkten Verwendung in Illustrator. Des Weiteren wurde Transparenz in Verlaufsgittern eingeführt, diverse Verbesserungen bei Pfeilspitzen und gestrichelten Konturen und beim Auswählen und Bearbeiten dahinter liegender und mit Schnittmasken versehener Objekte.\n\nDiese Version unterstützt 64 Bit auf Windows und Mac OS nativ, dadurch kann Illustrator mehr als die bisher möglichen 3 GB RAM nutzen und verarbeitet komplexere Dateien besser und schneller. Zum Geschwindigkeitszuwachs trägt ebenfalls ein überarbeitetes Weichzeichner-Modul bei.\nDarüber hinaus bietet Illustrator CS6 eine Funktion, die die Erstellung von kachelbaren Mustern erheblich vereinfacht und es können Verläufe auf Konturen angelegt werden (über den gesamten Pfad, quer oder entlang dem Pfad). Die Autotrace-Funktion wurde komplett neu geschrieben.\nDie Benutzeroberfläche wurde nicht nur optisch, sondern auch in der Bedienung überarbeitet, z. B. können Ebenennamen direkt im Bedienfeld bearbeitet werden.\n\nIn dieser Version können Pfadsegmente direkt frei bearbeitet werden und Ecken einzeln abgerundet bzw. abgeflacht oder mit einer Innenrundung versehen werden.\nMit dem Touch-Type-Textwerkzeug können bestimmte Zeichen-Attribute intuitiv direkt am Buchstaben durch Verformung zugewiesen werden. Die Eckelemente von Musterpinseln können nach unterschiedlichen Methoden automatisch generiert werden und Spezial-, Bild- und Musterpinsel können Rastergrafik enthalten. Buntstift-, Pinsel- und Radierer-Werkzeug wurden komplett überarbeitet.\nWird bei einem Perspektivenraster nachträglich der Horizont oder die Fluchtpunkte geändert, dann passen sich bestehende Objekte an.\nIn dieser Version hat Illustrator auch eine Verpacken-Funktion erhalten, die automatisch alle zugehörigen Elemente wie Bilder und Fonts sucht und in einen Ordner speichert. Zu verknüpften Bildern wird eine ausführlichere Information eingeblendet und man kann eingebettete Bilder wieder aus dem Dokument lösen und verknüpfen.\nWebdesigner können viele Eigenschaften als CSS ablesen und entsprechend speichern.\nUnter Windows 8 können Touch-Geräte benutzt werden, das Interface einiger Werkzeuge (Frei-transformieren, Touch-Type-Textwerkzeug) ist bereits so angepasst, dass deren Benutzung mit den Fingern möglich ist.\nEinige Funktionen beruhen besonders auf der Anbindung an die Adobe Creative Cloud:\nSo lassen sich Einstellungen (Voreinstellungen, eigene Bibliotheken etc.) zwischen unterschiedlichen Rechnern eines Nutzers synchronisieren. Darüber hinaus hat auch Illustrator einen direkten Zugriff auf Typekit-Schriften.\n\nIn dieser Version wurde unter anderem das Eigenschaften-Bedienfeld überarbeitet. Außerdem ist es nun möglich, mehrere Zeichenflächen gleichzeitig auszuwählen. In ein einzelnes Element passen seit Version 18.0 bis zu 1.000 Zeichenflächen. Texte lassen sich jetzt auch über die Creative Cloud-Bibliotheken verwalten. Beispielsweise können nun Texte zwischen Illustrator und InDesign ausgetauscht werden. Des Weiteren unterstützt Illustrator erstmals die TouchBar des MacBook Pro. In Version 18.1 wurde die Bedienung durch die verbesserte Anzeige von Steuerungen erleichtert. Außerdem können Daten über das Bedienfeld \"Variablen\" einfacher zusammengeführt werden. Beim Import von mehrseitigen PDF-Files können nun alle Seiten oder nur ein bestimmter Bereich ausgewählt werden. Darüber hinaus wurde die Synchronisation mit Dropbox verbessert.\n\n"}
{"id": "2641538", "url": "https://de.wikipedia.org/wiki?curid=2641538", "title": "CMake", "text": "CMake\n\nCMake (\"\") ist ein plattformunabhängiges Programmierwerkzeug für die Entwicklung und Erstellung von Software.\n\nCMake wurde als Teil des Visible Human Project für das Insight Segmentation and Registration Toolkit erstellt.\n\nBill Hoffman von Kitware nutzte neue und eigene Ideen vom pcmaker (einem vorherigen Buildsystem) mit Grundfunktionalitäten des GNU Build Systems, um Mitte 2000 die erste Version zu erstellen, die dann bis Anfang 2001 weiterentwickelt wurde. Danach entstanden viele Verbesserungen durch andere Entwickler, die CMake für ihre eigenen Projekte verwenden wollten.\n\nMit CMake werden aus Skriptdateien (CMakeLists.txt) Makefiles und Projekte für viele integrierte Entwicklungsumgebungen und Compiler erzeugt.\n\nUnterstützt werden:\n\nCMake überprüft dabei automatisch die Abhängigkeiten für C, C++, Fortran und Java und unterstützt parallele Builds.\n\nEs werden viele Bibliotheken, wie z. B. SWIG, Boost und Qt durch die CMake-Skriptsprache unterstützt. Ab der Version 2.6 wird auch Cross Kompilation und cross-platform build unterstützt. Integriert sind Tools für Tests und release: DART, CDash, CTest und CPack.\n\nMit CPack ist es möglich, Installationspakete in folgenden Formaten zu erstellen:\n\nZu den Anwendern von CMake gehören:\n"}
{"id": "2646764", "url": "https://de.wikipedia.org/wiki?curid=2646764", "title": "Microprofessor III", "text": "Microprofessor III\n\nDer Microprofessor III (MPF III) ist ein Heimcomputer, der im Jahre 1983 von der Firma Multitech (heute Acer) auf den Markt gebracht wurde. Es handelt sich dabei auch um eine Apple-Clone des Apple IIe-Computers. Im Gegensatz zu den ersten zwei Computern der Firma war dieser Rechner vom IBM PC beeinflusst worden. Wegen zusätzlicher Funktionen im ROM und bei der Grafik war der Rechner nicht vollständig kompatibel mit dem Apple IIe.\n\nEine der Besonderheiten des MPF III war sein Chinese BASIC, welches eine chinesische Version der Programmiersprache BASIC war und auf dem Applesoft BASIC basierte.\nDer Rechner war ausgerüstet mit einer Z80-Karte. Damit war es auf dem Rechner möglich, den zusätzlichen Z80 Prozessor zu nutzen und Programme unter dem Betriebssystem CP/M laufen zu lassen.\n\n\n\n"}
{"id": "2649493", "url": "https://de.wikipedia.org/wiki?curid=2649493", "title": "Metisse (Desktop-Umgebung)", "text": "Metisse (Desktop-Umgebung)\n\nMetisse ist ein 2½D-Desktop-Umgebung, die auf dem X Window System basiert. Sie ist in den Linux-Distributionen \"Mandriva One 2007\" und \"Sabayon Linux\" enthalten.\n\nMetisse besitzt viele Eigenschaften, die es von dem größeren 3D-Desktop-Projekt \"Compiz\" abheben: So können beispielsweise Fenster dreidimensional in alle Richtungen gedreht werden. Dies ist in Compiz nur mit dem in der Entwicklung befindlichen \"Freewins\"-Modul möglich. Metisse legt seinen Schwerpunkt nicht auf optisch ansprechende Effekte wie beispielsweise die „wackelnden Fenster“ in Compiz. Stattdessen haben Funktionalität und Produktivität eine höhere Priorität für die Entwickler.\n\n"}
{"id": "2654861", "url": "https://de.wikipedia.org/wiki?curid=2654861", "title": "UNIVAC 1101", "text": "UNIVAC 1101\n\nDie UNIVAC 1101, auch ERA 1101 und ATLAS I genannt, war ein UNIVAC-Computersystem, das von Engineering Research Associates (ERA) entworfen und durch Remington Rand 1950 gebaut wurde.\n\nUrsprünglich für die NSA unter dem Code Name \"Atlas\" gebaut, wurde die Maschine im kommerziellen Verkauf als UNIVAC \"1101\" bezeichnet. Der Code Name \"Atlas\" beruht auf einem Comic, der zu dieser Zeit populär war. \n\nDie UNIVAC 1101 besaß folgende Eigenschaften:\n\nEs wurden zwei Atlas-Systeme gebaut und 1950 und 1953 ausgeliefert. Eine dritte Maschine wurde für eigene Zwecke gebaut, man wollte die Rechenressourcen den Kunden auf Basis der genutzten Prozessorzeit berechnen. Dieses Geschäftsmodell war aber noch nicht reif genug, und die dritte Maschine wurde 1954 dem Georgia Institute of Technology geschenkt, wo sie bis 1961 im Einsatz war.\n\n"}
{"id": "2654956", "url": "https://de.wikipedia.org/wiki?curid=2654956", "title": "UNIVAC 1102", "text": "UNIVAC 1102\n\nDie UNIVAC 1102, auch ERA 1102 genannt, war ein UNIVAC-Computersystem, das von Engineering Research Associates (ERA) entworfen und durch Remington Rand zwischen 1954 und 1956 gebaut wurde.\n\nEs wurden drei Maschinen für die United States Air Force gebaut. Die Maschinen wurden zum einen für die Auswertung von Windkanalsimulationen verwendeten, zum anderen wurden sie für Tests von Flugzeugantrieben verwendet.\nDie Maschine interpretierten die Daten des Windkanals und druckten die Resultate auf fünf Druckern und vier Plottern aus.\n\nDie \"Univac 1102\" war eine Variante der UNIVAC 1101 besaß folgende Eigenschaften:\n\n"}
{"id": "2655026", "url": "https://de.wikipedia.org/wiki?curid=2655026", "title": "UNIVAC 1103", "text": "UNIVAC 1103\n\nDie UNIVAC 1103, auch ERA 1103 und ATLAS II genannt, war ein UNIVAC-Computersystem, das von Engineering Research Associates (ERA) entworfen und durch Remington Rand 1953 gebaut wurde. Unter anderem wurde die UNIVAC 1103 von Seymour Cray konzipiert.\n\nRemington Rand wusste bis 1953 nicht, dass die Maschine überhaupt entwickelt wurde. Die UNIVAC 1103 wurde unter Geheimhaltung für die NSA unter dem Codename \"Atlas II\" entworfen und 1952 ausgeliefert. Da das Management von Remington Rand nicht über eine genügend hohe Sicherheitsklassifikation verfügte, wussten nur die direkt involvierten Personen über die Existenz dieses Rechners. 1953 erlaubte die NSA, dass die Maschine kommerzialisiert werden durfte, unter der Bedingung, einige spezielle Anweisungen aus der Rechenmaschine zu entfernen. Remington Rand veröffentlichte die Maschine 1953 unter dem Namen UNIVAC 1103.\n\nInnerhalb kurzer Zeit gingen Bestellungen von Boeing, Lockheed, NASA und White Sands Missile Range ein. Remington Rand hatte aber Schwierigkeiten bei der Umstellung auf die Massenproduktion, und alle Aufträge wurden zu spät abgeschlossen.\n\nDie Maschine wurde mit folgender Software ausgeliefert:\n\n"}
{"id": "2656753", "url": "https://de.wikipedia.org/wiki?curid=2656753", "title": "EZ80-DIT", "text": "EZ80-DIT\n\nDas Mikrocomputersystem EZ80-DIT wurde 1977 entwickelt und ab 1978 als Lerncomputer in der handwerklichen Ausbildung und der überbetrieblichen Weiterbildung eingesetzt.\n\nDer EZ80-DIT besteht aus einer veränderten Version des NEC-Einplatinencomputers TK-80 (TK: Training Kit). Der EZ80-DIT ist ausgestattet mit dem µPD8080A (kompatibel mit dem 8-Bit-Mikroprozessor Intel 8080), einer achtstelligen Siebensegment-LED-Anzeige und einer Tastatur für Hexadezimalziffern.\n\nDas Gerät ermöglicht die Entwicklung und Abarbeitung von Programmen in Maschinensprache.\nDas System wurde mit Schaltplan und Beispielprogrammen, dem Monitorprogramm, ausgeliefert.\nDie zusätzliche Ausstattung gegenüber dem Einplatinencomputer TK-80:\n\nDer EZ80-DIT wurde vom Ingenieurbüro Kammerer 1977 entwickelt und vom Unternehmen Roederstein, Landshut bis Ende 1982 gefertigt. Der Vertrieb erfolgte\ndurch die Ditratherm Halbleiter Vertrieb GmbH, Landshut. Der Mikrocomputer EZ80-DIT wurde in der Handwerklichen Ausbildung und Überbetrieblichen Weiterbildung eingesetzt um das Verständnis zu Aufbau und Arbeitsweise des Mikrocomputers zu vermitteln. Ab 1981 wurde der EZ80-DIT vom Profi-50 Mikrocomputersystem abgelöst.\n\nDer Systemaufbau ist aus dem nebenstehenden Bild ersichtlich. Nach Anlegen der beiden Versorgungsspannungen +12 Volt und +5 Volt und Betätigen der Reset-Taste startet das Monitorprogramm im ROM den Funktionsablauf.\nDas Monitorprogramm dient zur Eingabe der Daten und Befehle in den Speicherbereich des EZ80-DIT. Der Schreib-Lesespeicher ist als CMOS-RAM ausgeführt und durch 2 Batterien bei Stromausfall gepuffert. Die für den Mikroprozessor 8080A benötigte negative Substratspannung von −5 Volt wird auf der Platine erzeugt.\n\nDie Ein-/Ausgabe erfolgt über ein programmierbares Peripherie-Interface (µPD8255). Dieser Baustein wird sowohl vom System für das Kassetteninterface, die Tastatur- und Schalterabfrage genutzt als auch vom Anwender zur Dateneingabe über Schalter und Datenausgabe über Leuchtdioden.\n\nAuf der Rückseite des EZ80-DIT Mikrocomputersystems sind die drei 8-Bit-Ports des µPD8255 über 2mm Buchsen herausgeführt. Zur Eingabe von binären Werten kann die Eingabeeinheit ES80 an einen der Ports angeschlossen werden. Die acht Schalter der ES80-Einheit erzeugen jeweils ein TTL-kompatibles Signal, das als High- bzw. Lowpegel über ein Programm eingelesen werden kann. Die Pegel der Schalterstellung werden über acht rote LEDs im ES80 angezeigt. Zur Ausgabe von binären Werten wird die Ausgabeeinheit AS80 eingesetzt. Die AS80-Einheit verfügt über acht LEDs, die den Wert des Ausgabeports anzeigen. Über die verschieden farbigen Leuchtdioden lässt sich programmtechnisch eine Ampelsteuerung realisieren.\n\nEin Schalterfeld mit 15 stabilen Kippschaltern ermöglicht unabhängig von der Tastatur die Steuerung des EZ80DIT-Systems über 8 Datenschalter und 7 Steuerschalter.\n\nDas Tastaturfeld besteht aus 25 großflächigen Tasten, 16 Tasten für die hexadezimale Eingabe und 9 Steuertasten. Die Abdeckhauben der Tasten sind abnehmbar, so dass auch eine individuelle Beschriftung des Tastenfeldes möglich ist. Über Software kann mit Ausnahme der LÖ-Taste jeder Taste eine beliebige Funktion zugewiesen werden. Die Tastatur ist über Software entprellt. Durch das programmierbare Peripherie-Interface (µPD8255) wird das Tastaturfeld abgefragt. Von den E/A-Kanälen dienen zwei Leitungen der seriellen Datenübertragung.\n\nAcht rotleuchtende Siebensegment-LED-Anzeigen (10 mm hoch, 6,5 mm breit, mit Digitalpunkt) werden zur Darstellung von Adressen, Befehlen und Daten im Multiplexverfahren angesteuert. Über das komfortable Monitorprogramm kann das Anzeigensystem auch zur Darstellung von Symbolen, Alarmsignalen, Quittungsanzeigen sowie festen und beweglichen Schriften verwendet werden. 5 Leuchtdioden signalisieren die Flag-Zustände, die außerdem als Hexadezimalzahl sichtbar gemacht werden können. Eine Leuchtdiode ist zur Anzeige des Eingangspegels bei seriellem Datenverkehr vorgesehen. Die Betriebsspannungen + 5V und + 12V werden ebenfalls durch Leuchtdioden angezeigt.\nDie Umwandlung von Serial- in Paralleldaten und umgekehrt läuft unter Steuerung des Monitorprogramms. Der Zusatz einer Modulator-Demodulator-Einheit erlaubt den Anschluss eines handelsüblichen Kassettenrekorders. Die Datenübertragungsgeschwindigkeit beträgt 110 Bit/s. Der Kassettenrekorder kann dann als externer Speicher genutzt werden. Die Steuerung erfolgt ebenfalls über das Monitorprogramm.\n\nZusammen mit dem Serial-Ein-/Ausgang schaltet der Schalter DE einen NF-Verstärker mit Lautsprecher an den Kanal C, Anschluss 2^1 (Port C Bit 1). Neben der Funktionskontrolle eines laufenden Programmes ist auch die Wiedergabe von elektronischer Musik, Quittungssignalen oder Warnsignalen möglich.\n\n\nBereits 1976 wurde von Joseph Kammerer am Elektronik Zentrum München EZM ein Prototyp erstellt und getestet. Das Gerät mit den Abmessungen 500 × 300 × 100 mm lieferte wichtige Erkenntnisse für die Serienproduktion.\n\n\n"}
{"id": "2657046", "url": "https://de.wikipedia.org/wiki?curid=2657046", "title": "Automator", "text": "Automator\n\nAutomator ist ein Programm von Apple, das standardmäßig mit dem Betriebssystem Mac OS X ab der Version 10.4 ausgeliefert wird. Automator dient dazu, Routineaufgaben automatisiert ablaufen zu lassen. Das Programm übernimmt so in gewisser Weise die Funktionen eines Roboters, was auch durch das Programmsymbol verdeutlicht wird.\n\nVor Automator mussten diese Abläufe mit AppleScript durchgeführt werden, wofür zumindest elementare Programmierkenntnisse erforderlich sind. Auf Basis eines einfachen und intuitiven Assistenten können mit Automator Befehle aneinandergereiht und so sequentiell abgearbeitet werden. Eine Vielzahl an Programmen bietet Standardaktionen, auf die in Automator zugegriffen werden kann.\n\n"}
{"id": "2657538", "url": "https://de.wikipedia.org/wiki?curid=2657538", "title": "UNIVAC II", "text": "UNIVAC II\n\nDie UNIVAC II war ein in den USA hergestellter Großrechner der Baureihe Universal Automatic Calculator. \n\nDie UNIVAC II war eine Weiterentwicklung der UNIVAC I. Ausgebaut wurde vor allem der Hauptspeicher auf 2.000 bis 10.000 Maschinenworte, UNISERVO II Magnetbänder, welche entweder die alten UNIVAC I Magnetbänder oder die neuen auf Mylar basierenden Bänder benutzen konnten. Einige Schaltkreise wurden auf der Basis von Transistoren implementiert (die UNIVAC II war immer noch röhrenbasiert). Die UNIVAC II war vollständig kompatibel zur UNIVAC I, dies sowohl in Bezug auf die Programminstruktionen als auch die Datenstrukturen.\n\n\nDie \"UNIVAC II\" wurde bei verschiedene Kunden eingesetzt: Bei der US Navy, welche die Maschine zur Inventarverwaltung benutzte, dem US Department of Agriculture, welche die Maschine zur Berechnung und Transaktionsverwaltung des „Weizenpreises“ in über 31 Bundesstaaten verwendete, der Metropolitan Life Insurance Company, welche vier Maschinen besaß und die Maschinen zur Gehaltsabrechnung, Debitorenbuchhaltung, Rechnungstellung und sonstigen versicherungstechnischen Berechnungen einsetzte, der Pacific Life Insurance Company mit ähnlichem Einsatzgebiet wie bei der Metropolitan Insurance, sowie bei der United States Steel Corporation, welche die Maschine zur Lösung von mathematischen Aufgabenstellungen verwendete.\n"}
{"id": "2670537", "url": "https://de.wikipedia.org/wiki?curid=2670537", "title": "Disk II", "text": "Disk II\n\nDie Disk II war ein 5¼-Zoll-Diskettenlaufwerk, welches von Steve Wozniak entwickelt und von Apple Computer hergestellt wurde. Das Gerät erschien zum ersten Mal im Jahre 1978 auf dem Markt zu einem Preis von US$ 495 auf Vorbestellungen, später wurde es für $ 595 zusammen mit der Controllerkarte und entsprechenden Kabeln verkauft. Die Disk II wurde als Laufwerk für den Apple-II-Computer entwickelt, wodurch das langsamere Kassettenlaufwerk zur Datenspeicherung abgelöst wurde.\nDie normale Speicherkapazität pro Diskettenseite betrug 114 KB bis zum Apple-Betriebssystem (Apple DOS) DOS 3.2.1, oder 140 KB mit dem 1980 erschienenen DOS 3.3 sowie dem späteren ProDOS. Der Schreib- und Lesezugriff erfolgte nur einseitig. Die Rückseite der Diskette konnte nur benutzt werden, wenn die Diskette gewendet und mit einer zweiten Schreibkerbe versehen wurde (z. B. mit einem Diskettenlocher). (Verdopplung der Speicherkapazität, aber nicht gleichzeitig)\n\nApple veränderte später das Design des Laufwerks, damit es besser zum Gehäuse des Apple IIe passte und bot unter dem Namen \"DuoDisk\" auch eine Version mit zwei Laufwerken in einem Gehäuse an.\n\n"}
{"id": "2674109", "url": "https://de.wikipedia.org/wiki?curid=2674109", "title": "Wildcard (Steckkarte)", "text": "Wildcard (Steckkarte)\n\nDie Wildcard (auch \"Wild Card\", eigene Schreibweise \"WILD CARD\", englisch für „Platzhalter“) ist eine Steckkarte für den Apple II, Apple II+ und Apple IIe, mit der Sicherungskopien auch von kopiergeschützter Software angefertigt werden können. Die Karte erstellt direkt aus dem Arbeitsspeicher eine lauffähige Disketten-Version einer zuvor von Diskette gestarteten Software.\n\nDas Gerät wurde von \"East Side Software Co.\" seit 1982 hergestellt und war in mehreren unterschiedlich ausgestatteten Ausführungen erhältlich. Version 1 kann 48-KB- und 64-KB-Disketten kopieren, Wildcard 2 und Wildcard Plus auf dem Apple IIe auch 128-KB-Disketten. Die Steckkarte war in den 1980er Jahren verbreitet genug, um für Produktpiraten interessant zu sein.\n\nWildcard 2 (anfangs \"Alaska Card\") wurde zeitweise von Central Point Software, Inc. als Ergänzung zur Datensicherungssoftware Copy II+ vertrieben.\n"}
{"id": "2676048", "url": "https://de.wikipedia.org/wiki?curid=2676048", "title": "Macintosh LC 580", "text": "Macintosh LC 580\n\nDer Macintosh LC 580 (LC steht für „Low Cost“ = preisgünstig) ist ein Macintosh-Rechner der Firma Apple. Das 1995 vorgestellte Modell stellt die letzte Generation der Macintosh LC-Reihe dar. Im Vergleich zum „Pizzaschachtel“-Format der ersten LC-Generation hatte sich die Bauhöhe nun fast verdoppelt.\n\nEr besaß einen 32-Bit-Datenbus. Das VRAM seines Vorgängers, des LC 575, war beim LC 580 auf 1 MB DRAM verdoppelt worden und saß fest auf der Hauptplatine. Zudem war nun ein Video In/Out Slot integriert. Dieser Slot war der gleiche wie der TV Tuner Slot, den es kurze Zeit für den Mac TV gab.\n\nDer LC 580 verfügte über einen integrierten Farbmonitor, der 640 × 480 Pixel mit bis zu 32k Farben darstellen konnte. \n\nSerienmäßig hatte der Rechner ein 1,44-MB-SuperDrive-Disketten- sowie ein CD-ROM-Laufwerk. Außer zwei seriellen Schnittstellen hatte der LC 580 auch einen Stereo 8 bit-Tonausgang, einen Mono 8 bit-Toneingang mit einem kleinen mitgelieferten Mikrophon und eine SCSI-Schnittstelle, an die damals oft ein in Agenturen und Graphikbüros beliebtes Wechselplattenlaufwerk angeschlossen wurde. \n\nDer Macintosh LC 580 wurde in Deutschland mit dem Betriebssystem MacOS 7.5. ausgeliefert und konnte bis System 8.1. aufgestockt werden. Seine Bauzeit lag zwischen April 1995 und April 1996. Er wurde auch als Performa 580CD und 588CD verkauft. Der Codename des Rechners war „Optimus“.\n"}
{"id": "2678870", "url": "https://de.wikipedia.org/wiki?curid=2678870", "title": "Microsoft Windows Home Server", "text": "Microsoft Windows Home Server\n\nWindows Home Server ist ein Betriebssystem von Microsoft, das am 7. Januar 2007 auf der Consumer Electronics Show (CES) von Bill Gates angekündigt wurde. Es wurde an Distributoren ausgeliefert und war auch als OEM-Version verfügbar. Nach Herstellerangaben soll das Produkt fortgeschrittene Funktionen wie den Austausch von Dateien und automatisierte Datensicherungen auch technisch wenig versierten Menschen möglich machen. Außerdem soll es den sicheren Zugriff auf die Daten von außerhalb des Heimnetzes gewährleisten. Anfang 2011 wurde die Nachfolger-Version mit dem Namen Windows Home Server 2011 veröffentlicht. Der Support endete am 8. Januar 2013.\n\nDer Windows-Home-Server basiert auf dem Windows-Server-2003-SP2-Kernel, genauer gesagt steckt dahinter der Windows Small Business Server 2003 Service Pack 2. Dadurch ist er sehr stabil, da der eigentliche Home-Server nur ein Aufsatz ist. Wichtiger Unterschied zum Server 2003 ist das Setup, welches über das von Windows Vista bekannte Windows PE läuft. Ferner ist es durch die gemeinsame Basis möglich, die gleichen Treiber zu verwenden. Auch Windows-XP-Treiber lassen sich nutzen.\n\n\n\n\nDas System bietet zudem ein SSL-verschlüsseltes Web-Interface an, welches über das Internet genutzt werden kann. Für dieses wird eine kostenlose Webadresse angeboten, die ein frei wählbares Präfix, gefolgt von einem Punkt und dem Suffix homeserver.com, enthält. Über das Webinterface können Dateien hoch und heruntergeladen werden und man kann den Server und zudem PCs des Heimnetzwerkes über den Webbrowser steuern, sofern auf dem Zielsystem ein Remote-Desktop-Protocol-Server läuft. Daher wird für die Fernbedienung am Zielsystem mindestens Windows XP Professional, Vista Business, 7 Professional bzw. 8 Pro vorausgesetzt.\n\nDie Leistungsmerkmale des Windows-Home-Servers werden durch die Installation einer Software, dem Windows-Home-Server-Connector, in Windows XP, Windows Vista und, seit dem Power Pack 3, auch in Windows 7 integriert. Die Bereitstellung der Daten in einer Windows-Freigabe ermöglicht den Zugriff von fast jedem Betriebssystem.\n\nÜber eine von Microsoft bereitgestellte Schnittstelle können Erweiterungen des Windows Home Servers, sogenannte Add-Ins, von anderen Softwareherstellern entwickelt und zur Verfügung gestellt werden. Dabei wird im Allgemeinen insbesondere auch der Client-Teil, die Windows-Home-Server-Console erweitert, in dem dort auf einem neuen Reiter neue Funktionalität zur Verfügung gestellt wird, z. B. von einem PC aus die anderen hoch oder runter zu fahren oder sich das Event-Log des Servers aufbereitet anzeigen zu lassen.\n\nAm 29. März 2011 wurde der Nachfolger Microsoft Windows Home Server 2011 fertiggestellt. Er ist über Technet und MSDN bzw. als OEM-Gerät im Handel verfügbar. Die erste Version von Windows Homeserver wie oben beschrieben ist dagegen Januar 2013 ausgelaufen.\nNach Microsoft Angaben wird der Homeserver mit der Server Version 2012 eingestellt, die einfachste Version heißt nun “Essentials” aber liegt preislich weit über dem Homeserver.\n\nWindows Homeserver war insgesamt kein großer Erfolg, nach verschiedenen Quellen wurden bis 2009 von der ersten Version nur 100.000 Lizenzen verkauft.\n\n\n"}
{"id": "2683333", "url": "https://de.wikipedia.org/wiki?curid=2683333", "title": "Video Genie", "text": "Video Genie\n\nUnter dem Namen Video Genie wurden die TRS-80 kompatiblen Computer Genie I/II in Europa vertrieben, in anderen Ländern aber unter anderem Namen so z. B. in Australien und Neuseeland unter dem Namen \"system-80\". Produziert wurden die Genie Computer von der Firma EACA in Hongkong und in Deutschland von der Firma Trommeschläger Computer Service vertrieben. TCS bot für die Genie I/II Computer umfangreiche Peripherie an, daher wurde das Video Genie von TCS auch als \"Video Genie \"System\"\" bezeichnet.\n\n\n\n\n\n\n\n\n\n"}
{"id": "2683412", "url": "https://de.wikipedia.org/wiki?curid=2683412", "title": "GNU Parted", "text": "GNU Parted\n\nGNU Parted ist ein freies plattformübergreifendes Partitionierungsprogramm, das mit den grafischen Oberflächen \"GParted\" und \"QtParted\" dazu verwendet werden kann, Partitionen und darin liegende Dateisysteme zu bearbeiten und zu überprüfen.\n\n\"GNU Parted\" besteht aus dem Kommandozeilenprogramm \"Parted\" und der Bibliothek \"Libparted\". Zurzeit ist \"Parted\" nur unter den Betriebssystemen Linux, GNU Hurd, BeOS und FreeBSD ausführbar. Die grafischen Oberflächen \"GParted\" und \"QtParted\" basieren auf \"GNU Parted\" beziehungsweise dessen Bibliotheken.\n\n\"GNU Parted\" bearbeitet lediglich die Rohdaten innerhalb einer Partition, dabei müssen eventuell Änderungen an den Systemeinstellungen vorgenommen werden. Zudem kann mit dem Programm beispielsweise in einem bestehenden System Speicherplatz für weitere Betriebssysteme freigegeben werden, indem die bisher existierenden Partitionen verkleinert werden. Weiterhin können Partitionen innerhalb einer Festplatte neu angelegt, vergrößert, verkleinert, umsortiert sowie auch auf andere Festplatten kopiert oder verschoben werden. Alle Angaben erfolgen mit Binärpräfixen.\n\nNeben den weit verbreiteten Partitionstabellen MBR (die Benutzeroberfläche nennt das aber \"msdos\") und GPT unterstützt GNU Parted noch weitere Spezifikationen: atari, aix, amiga, bsd, dvh, mac, pc98, sun und loop. Vergleiche dazu den Artikel Partitionstabelle.\n\nDas Programm \"GNU Parted\" unterstützt selbst die folgenden Dateisysteme.\n\nFür das Kommandozeilenwerkzeug \"parted\" gibt es verschiedene grafische Oberflächen, die teilweise auch als Direktstart-CD (englisch \"\") verfügbar sind, wie zum Beispiel Parted Magic oder SystemRescueCd. Die meistverbreiteten sind \"GParted\" (Gnome) und \"QtParted\" (KDE). \n\nDer \"Gnome Partition Editor\", kurz \"GParted\" genannt, ist eine GTK+-Oberfläche für libparted.\n\nEs verwendet \"libparted\", um Laufwerke und Partitionstabellen zu erkennen und zu verändern, während verschiedene frei auswählbare Dateisystemwerkzeuge Unterstützung für nicht standardmäßig erkannte Dateisysteme bereitstellen.\n\nGParted ist in C++ geschrieben und benutzt gtkmm als grafisches Toolkit.\n\nEs gibt eine Live-CD- und eine Live-USB-Version, die mit jeder neuen Version von GParted aktualisiert werden. Von dieser CD kann gebootet und die Partitionen direkt bearbeitet werden.\n\nGParted unterstützt folgende Operationen und Dateisysteme, vorausgesetzt, dass das aktuell laufende Betriebssystem über die benötigten Funktionen und Eigenschaften verfügt. Das trifft auch auf das Direktstartsystem \"GParted-Live\" zu.\n\nQtParted, basierend auf der Qt-Bibliothek, ist das KDE-Pendant von \"GParted\".\n\nDie QtParted-Entwickler bieten keine offizielle Direktstart-CD an, allerdings ist QtParted in Knoppix und anderen Direktstart-Linux-Distributionen enthalten. Der Funktionsumfang ist gegenüber GParted stark eingeschränkt.\n\n"}
{"id": "2685117", "url": "https://de.wikipedia.org/wiki?curid=2685117", "title": "Cyberdog", "text": "Cyberdog\n\nCyberdog war ein von Apple entwickelter Webbrowser, der als Beta-Version im Februar 1996 veröffentlicht wurde. Er macht sich die OpenDoc-Softwaretechnologie zunutze und war unter Mac OS bis Version 9 lauffähig. Es handelt sich dabei um eine integrierte Anwendung mit Webbrowser-, Mail-, Nachrichten-, Adressbuch- und FTP-Drag-and-Drop-Funktionalität. Nachdem am 28. April 1997 die Version 2.0 veröffentlicht worden ist, wurde die Entwicklung von Cyberdog und OpenDoc im Mai 1997 eingestellt und durch den Internet Explorer ersetzt.\n\nDie Konzepte, die Cyberdog zugrunde lagen, finden sich beim aktuellen OS X beispielsweise in der FTP-Integration im Finder und im WebKit-Browser Safari wieder.\n"}
{"id": "2688658", "url": "https://de.wikipedia.org/wiki?curid=2688658", "title": "Building Information Modeling", "text": "Building Information Modeling\n\nDer Begriff Building Information Modeling (kurz: \"BIM\"; deutsch: Bauwerksdatenmodellierung) beschreibt eine Methode der optimierten Planung, Ausführung und Bewirtschaftung von Gebäuden und anderen Bauwerken mit Hilfe von Software. Dabei werden alle relevanten Bauwerksdaten digital modelliert, kombiniert und erfasst. Das Bauwerk ist als virtuelles Modell auch geometrisch visualisiert (Computermodell). Building Information Modeling findet Anwendung sowohl im Bauwesen zur Bauplanung und Bauausführung (Architektur, Ingenieurwesen, Haustechnik, Tiefbau, Städtebau, Eisenbahnbau, Straßenbau, Wasserbau, Geotechnik) als auch im Facilitymanagement.\n\nIn der klassischen Bauplanung erstellt ein Architekt einen Entwurf und zeichnet diesen auf, heutzutage mit Hilfe von CAD-Systemen. Die Pläne werden unter anderem Fachingenieuren, Brandschutzgutachtern und Behörden vorgelegt.\n\nZur Kostenkalkulation wird eine Mengenermittlung auf Basis der Zeichnungen erstellt. Dazu ist eine Verknüpfung der Geometrien mit qualitativ und monetär definierten Leistungsbestandteilen erforderlich, sodass die einzelnen Mengendetails in Leistungspositionen bzw. kalkulatorischen Teilleistungen aufsummiert werden können.\n\nTritt eine Änderung der Planung auf, müssen die Zeichnungen geändert werden, die Mengenermittlung muss angeglichen werden, alle Beteiligten erhalten aktualisierte Zeichnungen und müssen diese mit ihren Fachplanungen abgleichen. Dies verursacht einen erheblichen Koordinierungs- und Arbeitsaufwand, der mit BIM deutlich reduziert werden kann.\n\nMit BIM nimmt der Architekt oder Fachplaner Änderungen an der Projektdatei, am Modell (\"engl.\" model) vor. Diese Änderungen sind für alle Beteiligten, sowohl als Zeichnung als auch als Datenpaket, direkt verfügbar. Massen und Stückzahlen, die zum Beispiel als Grundlage zur Kostenkalkulation dienen, werden automatisch abgeglichen. Beispielsweise kann sich aufgrund von Änderungen im Grundriss die Zahl und Beschreibung der Türen in einem Gebäude ändern. Der Architekt ändert die Türen im virtuellen Gebäudemodell. Damit wird automatisch die Türliste verändert und bei entsprechender Verknüpfung sieht man die unmittelbare Auswirkung auf die Kosten.\n\nKennzeichen und Vorteile des Verfahrens sind:\n\nDurch den verbesserten Datenabgleich soll letztlich die Produktivität des Planungsprozesses hinsichtlich Kosten, Termine und Qualität gesteigert werden.\n\nDer Begriff \"Building Information Modeling\" wurde von Autodesk geprägt, um einen „\"dreidimensionalen, objektorientierten, AEC-spezifischen computerunterstützten Design-Prozess\"“ zu beschreiben. Dabei wird zwischen einem \"parametrischen Gebäudemodell\" und einem \"intelligenten Gebäudemodell\" unterschieden. Im parametrischen Gebäudemodell können sämtliche Elemente (Wände, Decken, Bemaßungen, Beschriftungen, Objekte, Schnittlinien, etc.) zueinander in Abhängigkeiten gebracht werden, während beim intelligenten Gebäudemodell die Intelligenz auf einzelne Objekte beschränkt ist.<br>\n\nDie Verabschiedung des \"Stufenplans Digitales Planen und Bauen\" durch das BMVI soll die Umsetzung von BIM in Deutschland vorantreiben. Im Stufenplan fordert das BMVI \"die Einführung von modernen, IT-gestützten Prozessen sowie Technologien zur Planung, für den Bau und das Betreiben von Bauwerken\". Es werden vertragliche Regelungen definiert, die enge Zusammenarbeit der Baubeteiligten erklärt und die teamorientierte Planung im technischen Sinne aufgezeigt. Ab 2020 gelten die Regelungen für alle neu zu planenden Projekte des infrastrukturbezogenen Hochbaus als verpflichtend.\n\nDie internationale Organisation buildingSMART hat das Ziel, offene Standards (openBIM) für den Informationsaustausch und die Kommunikation auf der Basis von Building Information Modeling zu etablieren. Dazu hat buildingSMART ein Basisdatenmodell – die Industry Foundation Classes (IFC) – für den modellbasierten Datenaustausch im Bauwesen entwickelt.\n\nBIM-Verfahren werden von vielen namhaften CAD-Herstellern angeboten. Einige Beispiele:\n\n\nSoftware die das BIM-Verfahren im weiteren Planungsprozess unterstützt:\n\n\nFreie Software die BIM unterstützt:\n\n\n\n\n"}
{"id": "2691054", "url": "https://de.wikipedia.org/wiki?curid=2691054", "title": "T-Flex Parametric CAD", "text": "T-Flex Parametric CAD\n\nT-FLEX Parametric CAD/CAM ist eine featuregestützte, parametrische CAD-Software von der russischen Firma Top Systems. Sie ermöglicht das Erstellen parametrischer Modelle, d. h. alle Teile können durch die Veränderung der Maße in ihrer Geometrie verändert werden. Darüber hinaus kann jedes Attribut (Schraffurtyp, Material, Werkzeug, Aussehen etc.) über Parametrik geändert werden, auch z. B. aus einer Datenbank oder Tabelle heraus. In der Regel werden mehrere Teile zu einer oder mehreren Baugruppen zusammengefasst. Die Parametrik aller Teile ist übergreifend aufgebaut, d. h. auch in einer komplexen Zusammenbaustruktur wirken sich Änderungen am Einzelteil auf das Gesamtteil aus (Beispiel: Bohrung M8 wird zu M10, überall dort, wo diese Bohrung aus solchem Zusammenhang verwendet wird).\n\nT-FLEX Parametric CAD existiert in dieser Form seit den 80er Jahren, ist bisher allerdings vornehmlich in Russland zu finden. Seit der politischen Öffnung hat sich das CAD-System auch in anderen Ländern verbreitet. In Deutschland wird es lokalisiert als TENADO CAD mit optionalem Metallbauaufsatz vertrieben.\n\nT-FLEX Parametric CAD ist eine Unicode-basierte Software, ist also in allen Sprachen auf MS-Windows ab 2000 verfügbar, sowohl als 32-bit als auch als 64-bit-Variante.\n\nDas System ist im Wesentlichen ein Komplettpaket, vergleichbar mit anderen High-End-CAD-Systemen. Besondere erweiterte Module gibt es für den Bereich Dynamik und Analyse sowie CAM.\n\nAls Basis verwendet T-FLEX den 3D-Kern Parasolid. Alle Parametrik von T-FLEX ist auch von außen steuerbar, z. B. durch Anbindung an eine Excel-Tabelle, Microsoft Access oder eine einfache Textdatei. So können Teilefamilien schnell erzeugt und genutzt werden durch Ergänzung oder Änderung von Tabellenwerten.\n\nDie Software ist benutzerdefiniert anpassbar und ermöglicht es dem Nutzer, eigene Dialoge zu erstellen.\n\nEine offene Programmierschnittstelle (API) bietet Möglichkeiten der Anbindung an andere Systeme.\nEinige Standardschnittstellen sind z. B. DXF, DWG, IGES, STEP, Parasolid, SolidWorks, Solid Edge und Autodesk-Inventor.\n\nT-FLEX wurde für die mechanische Konstruktion konzipiert und findet Verwendung in der Blechverarbeitung, im Werkzeug-, Maschinen- und Anlagenbau, aber auch im klassischen Designbereich. Durch die Verwendung von Parasolid als Volumenmodellierer und den Möglichkeiten der Flächenerstellung sowie deren Kombination ist T-FLEX für jeden Bereich einsetzbar.\nSpezielle Bereiche der Software (im Grundpaket enthalten) bieten z. B. spezielle Blechverarbeitung, Express-FEA, Schweißen, Rohrleitungsbau.\n\n\n"}
{"id": "2692234", "url": "https://de.wikipedia.org/wiki?curid=2692234", "title": "N-Steiner", "text": "N-Steiner\n\nn-Steiner, mit n als Parameter aus den natürlichen Zahlen, bezeichnet eine Stellung bei Brettspielen, insbesondere Schach, bei der n Spielsteine auf dem Spielbrett stehen. Beispielsweise stellt die Grundstellung im Schach einen 32-Steiner dar, da sich anfangs 32 Schachfiguren auf dem Brett befinden.\n\nDer Begriff findet vor allem Verwendung in der Schachkomposition, im Computerschach und in Schachdatenbanken.\n\n\n"}
{"id": "2699676", "url": "https://de.wikipedia.org/wiki?curid=2699676", "title": "Growl (Software)", "text": "Growl (Software)\n\nGrowl ist ein globales Benachrichtigungssystem für macOS und Windows. Programme können den Benutzer mittels Growl über Ereignisse wie neue Mails oder Chat-Nachrichten, fertig heruntergeladene Dateien oder den aktuell spielenden Musiktitel informieren. Softwareentwicklern bietet Growl Schnittstellen für die Programmiersprachen Objective-C, C, Perl, Python, Tcl, AppleScript, Java und Ruby.\nDer Benutzer kann vollständig konfigurieren, welche Nachrichten angezeigt werden sollen. Auch verschiedene Dringlichkeitsstufen sind möglich, so dass etwa der aktuelle Musiktitel nur einige Sekunden lang angezeigt wird, während Geburtstagserinnerungen solange auf dem Bildschirm bleiben, bis der Nutzer sie entfernt.\nFür die Benachrichtigungen stehen mehrere Darstellungsstile zur Verfügung.\n\nAb Version 1.3 wurde das Programm kommerziell über den Mac App Store vertrieben, da das Projekt nach Angaben der Entwickler sonst nicht mehr weiterentwickelt werden könnte. Der Quellcode von Growl 1.3 ist jedoch weiterhin frei verfügbar. Seit 2013 erschien keine neue Version mehr, wodurch die Unterstützung neuerer Betriebssysteme fehlt. Durch die in Mac OS X 10.8 eingeführte Mitteilungszentrale ist eine ähnliche Funktionalität jedoch weiterhin gegeben.\n\nIm Jahr 2008 wurde eine Version von Growl für Windows vorgestellt. Diese wurde unabhängig von der Mac-Version entwickelt und unterscheidet sich in der Versionsnummer und einigen Fähigkeiten, auch die Windows-Version wurde nicht mehr weiterentwickelt, 2012 erschien die jüngste Ausgabe.\n\nAuf der Growl-Website sind mehr als 200 Programme aufgezählt, die Growl unterstützen, darunter etwa Opera, Skype oder der VLC media player. Für einige weitere Programme – darunter Mozilla Firefox, iTunes oder Apple Mail – sind Plugins erhältlich, die Growl-Unterstützung nachrüsten.\n\n"}
{"id": "2715583", "url": "https://de.wikipedia.org/wiki?curid=2715583", "title": "Samplitude", "text": "Samplitude\n\nSamplitude ist eine Digital Audio Workstation (DAW) zum professionellen Bearbeiten von MIDI- und Audiodateien. Hersteller ist das Unternehmen Magix.\nSamplitude ist eine Native-Processing-Software, die keine spezielle DSP- oder Audio-Hardware benötigt. Projekte können somit auch auf eine mobile Workstation übertragen werden. Die Software funktioniert auf den Betriebssystemen Windows Vista, Windows 7 und 8 von Microsoft.\n\nSamplitude wurde im Studio für elektronische Klangerzeugung Dresden (SEK'D) in Zusammenarbeit mit der Technischen Universität Dresden entwickelt. 1992 wurde die erste Version von Samplitude fertiggestellt. Von dem Entwickler Titus Tost für den Amiga geschrieben, war sie hauptsächlich Sample-Editor mit 24-Bit-Audioverarbeitung. Die Version Samplitude Pro II mit integriertem Harddisk Recording erschien 1993. 1994 erfolgte die Umstellung auf die Programmiersprache C++, wodurch die Portierung nach Windows 3.11 erleichtert und noch im Sommer desselben Jahres durchgeführt wurde. Als PC-Version wurde Samplitude Studio zum ersten Mal 1995 auf der NAMM Show in Anaheim präsentiert. Den Vertrieb organisierte die Firma Hohner Midia. 1997 wurde die Amiga-Version durch die Firma A.C.T. zu SamplitudeOpus weiterentwickelt und 1998 eine neue Version von Samplitude mit nativer 32-Bit-Fließkommaverarbeitung der Audiodaten vorgestellt.\nDie erste Version von Sequoia wurde 2000 entwickelt. Sequoia ist eine Erweiterung von Samplitude, spezialisiert auf die Bereiche Media- und Postproduktion, Broadcast und Mastering und damit vor allem für den Profi- und Rundfunkbereich relevant. Samplitude und Sequoia sind seit 2000 bei der Magix GmbH & Co. KGaA im Vertrieb.\n\nDie aktuelle Version ist Samplitude Pro X4. Es gibt neben der Standardversion Samplitude Pro X4 (UVP 399,00 €) noch eine erweiterte Version Samplitude Pro X4 Suite (UVP 599,00 €) und Sequoia (UVP 2975,00 €). Die Hauptunterschiede liegen im Funktionsumfang, der Anzahl der Features und im Preis.\nFür den Amateurbereich wird auch eine reduzierte Version, das Music Studio (UVP 49,99 €) und das Samplitude Music Studio 2016 (UVP 99,99 €) angeboten. Das Programm ist als All-in-One-Software für Musiker gedacht, die noch die klassische Studioaufnahme kennen. Einschränkungen bestehen vor allem bei dem Funktionsumfang des Mastering.\n\n\n\nSamplitude arbeitet nichtdestruktiv, das heißt die Originaldateien bleiben auch nach Bearbeitung erhalten. Der Funktionsumfang von Samplitude erstreckt sich über Recording, Editing, Mixing und Mastering von Musik- und Sprachaufnahmen. Alle Schnitte, Crossfades und Effekte werden in Echtzeit errechnet. Die Bearbeitung in Samplitude kann sowohl objekt- als auch spurbasiert erfolgen.\nDer CD-/DVD-Brennprozess erfolgt direkt aus dem Mehrspurprojekt mitsamt allen Effekten, Blenden und Plugin-Einstellungen. Die erzeugten CDs sind Red-Book-kompatibel und können zur Vervielfältigung direkt an ein Presswerk gegeben werden. Sie lassen sich zusätzlich mit Kopierschutz, UPC/EAN, International Standard Recording Codes, Pre-Emphasis und CD-Text ausstatten.\n\nDie Ausstattung umfasst ein digitales Mischpult, Mehrspur-Rekorder, 6-Band-Equalizer (EQ116), Crossfade-Editor, verschiedene Dynamik-Prozessoren, Reverb und Delay, Echtzeit-Raumsimulator sowie High-End-Mastering-Effekte. Hinzu kommen Restaurationswerkzeuge sowie ein Time-Stretching-/Pitchshifting-Algorithmus in verschiedenen Qualitätsstufen. Die Software integriert dabei MIDI-Funktionalität, VST-Instrumente und ReWire.\nHinzu kommt die virtuelle Gitarrenverstärkersimulation Vandal (Pro Version) bzw. die leicht abgespeckte Variante Vandal SE in der Standardversion. Bei letzterer lassen sich bestimmte Einstellungen nur über festgelegte Voreinstellungen anwählen. Außerdem wird mit Samplitude der Sampler Independence mitgeliefert. Die Größe der Sample-Datenbank unterscheidet sich dabei je nach Version.\n\nSamplitude und Sequoia können barrierefrei bedient werden.\n\n\n"}
{"id": "2716768", "url": "https://de.wikipedia.org/wiki?curid=2716768", "title": "HiRes", "text": "HiRes\n\nHiRes (auch \"Hires\" oder \"Hi-Res\") ist die Abkürzung für den Begriff High Resolution (engl. \"hohe Auflösung\").\n\nHiRes wurde von Computerfirmen für die Bildschirmauflösung ihrer Heimcomputer verwendet (z. B. Commodore beim C64 oder Apple beim Apple II). Als Alternative zu HiRes gab es die LoRes (Low Resolution, also niedrige Auflösung).\n\nDie Bildschirmauflösung variierte zwischen den einzelnen Geräteherstellern, im Allgemeinen war aber mit einer Umstellung vom LoRes- zum HiRes-Modus die Umschaltung vom Textmodus in den Grafikmodus gemeint, wodurch von einer Ansteuerung von einzelnen Buchstabenpositionen (Textmodus) dann eine Ansteuerung einzelner Pixel möglich war.\n\nBeim C64 beispielsweise wird die Auflösung nur in der Breite von 160×200 auf 320×200 Pixel erhöht, dabei treten jedoch Begrenzungen der Farbauswahl auf.\n\nIn der Audiotechnik (High Fidelity) werden mit HiRes digitale Musikformate bezeichnet, die eine Abtastrate von mindestens 96 kHz oder eine Samplingtiefe von mindestens 24 Bit aufweisen und damit über den Standard Compact Disc Digital Audio hinausgehen. Musikkonsumenten versprechen sich davon eine höhere Klangqualität.\n\nAus wissenschaftlicher Sicht lässt sich eine hörbare Verbesserung der Klangqualität durch eine höhere Abtastrate jedoch nicht belegen (dies folgt sowohl aus theoretischen Überlegungen (Nyquist-Shannon-Abtasttheorem,) als auch aus empirischen Untersuchungen (Blindtests)). Die Hörbarkeit einer größeren Samplingtiefe als 16 Bit ist nur bei Musik mit leisen Passagen gegeben, die mit sehr hoher Lautstärke wiedergegeben wird.\n\n"}
{"id": "2718985", "url": "https://de.wikipedia.org/wiki?curid=2718985", "title": "Ntop", "text": "Ntop\n\nntop (network top) ist eine quelloffene und freie Software, mit der Netzwerkverkehr mitgeschnitten und analysiert werden kann. Der Name ist an das Unixprogramm top angelehnt, da es alle aktiven Netzwerkverbindungen analysiert und nach diversen Kriterien sortiert darstellen kann. Ntop ist allerdings inzwischen auch fähig, Protokolldateien einzulesen, um diese Protokolle auszuwerten und damit auch grafisch darzustellen.\n\nDas Programm wurde von Luca Deri, einem italienischen Wissenschaftler an der Universität von Pisa im Juni 1998 initiiert, wird aber inzwischen von einer breiteren Entwicklerfront weiterentwickelt.\n\nNetzwerktechnisch ist ntop zwischen OSI-Schicht 2 (MAC) und 3 (IP) anzusiedeln.\n\nDie GUI von ntop ist als Webserver, dessen Verbindung optional mittels OpenSSL verschlüsselt werden kann, ausgeführt und benötigt einen Webbrowser, um die Ausgaben der Analysen anzusehen. Die Ausgabe ist teils aufwendig gestaltet (JavaScript, sprechende grafischen Elemente). Es besteht aber ebenfalls die Möglichkeit, die Ausgabe, ähnlich wie tcpdump, in eine Textdatei umzuleiten.\n\nsFlow und Netflow werden ebenso unterstützt wie RRD und eine Vielzahl weiterer Protokolle, unter vielen anderen auch TCP/UDP/ICMP, (R)ARP, IPX, NetBIOS, AppleTalk, SMTP/POP/IMAP und SNMP.\nJe nach Hardwareschnittstellen des Hosts werden neben Ethernet auch Token Ring oder Fiber Channel unterstützt.\n\nNachteilig von ntop ist die fehlende Speichermöglichkeit der gesammelten Netzwerkdaten. Damit gehen nach einem Neustart des Programms, gesammelte Informationen verloren. Ebenfalls problematisch ist zum einen der damit erhöhte Bedarf an Hauptspeicher, zum anderen kann man ntop selbst nicht dauerhaft als Werkzeug zur grafischen Auswertung, wie das beispielsweise bei MRTG der Fall ist, benutzen. Diese Funktionalität wird allerdings teilweise über vorhandenen Plugins nachgerüstet. Das RRD-Plugin erlaubt hier die regelmäßige Speicherung von Langzeitdaten in RRD-Datenbanken, die dann für langfristige Auswertungen herangezogen werden können.\n\n"}
{"id": "2719192", "url": "https://de.wikipedia.org/wiki?curid=2719192", "title": "Cyberduck", "text": "Cyberduck\n\nCyberduck ist ein freier FTP-Client. Das Produkt der Firma Iterate aus Bern wurde ursprünglich für macOS entwickelt, es erschien am 8. März 2011 mit dem Release 4.0 eine unter Windows lauffähige Version. \n\nCyberduck unterstützt Verbindungen per FTP, FTP/TLS, WebDAV und SFTP. Zusätzlich bietet Cyberduck die Möglichkeit eine Verbindung zu Cloudspeicheranbietern wie \"Amazon S3\", \"Google Cloud Storage\" und \"Rackspace Cloud Files\" herzustellen. Für Cloud Verbindungen wird die API des jeweiligen Anbieters direkt genutzt.\n\nDas Programm unterstützt einfaches Hoch- und Herunterladen mittels Drag and Drop, auch die Synchronisation von Dateien und Ordnern und das Öffnen und Bearbeiten von Dateien in einem Texteditor ist möglich. Cyberduck enthält eine Lesezeichenfunktion und nutzt den Netzwerkstandard Bonjour sowie den Passwortmanager \"Schlüsselbund\". Bestehende Lesezeichen aus anderen FTP-Clients, etwa FileZilla, können importiert werden.\n\nSoweit installiert, wird der Benutzer mittels Growl über den Verbindungsstatus und den Ladefortschritt informiert.\n\nZusätzlich zur grafischen Oberfläche wurde eine CLI-Version duck für macOS, Windows und Linux veröffentlicht.\n\nNeben der klassischen Anwendung für grafische Oberflächen und Konsolen ist inzwischen auch eine Laufwerksintegration namens Mountain Duck erschienen. Diese bildet die entsprechende Quelle als Laufwerk im Mac-Finder oder Windows-Datei-Explorer ab und erlaubt eine Nutzung und Bearbeitung direkt aus diesem heraus.\n\nCyberduck ist in 35 Sprachen verfügbar, darunter Deutsch, Türkisch, Polnisch und Italienisch.\n\n"}
{"id": "2722932", "url": "https://de.wikipedia.org/wiki?curid=2722932", "title": "Könige der Wellen", "text": "Könige der Wellen\n\nKönige der Wellen (Originaltitel: Surf’s Up; deutsch: \"Brandung oben\") ist eine computeranimierte Filmkomödie aus dem Jahre 2007, die in der Art einer Mockumentary erzählt wird. Der Film wurde von Sony Pictures Animation produziert und in Deutschland von Sony Pictures Releasing verliehen. Premiere war am 2. Juni 2007 in Westwood. Am 13. September 2007 kam der Film in die deutschen Kinos.\n\nDer 16-jährige Felsenpinguin Cody Maverick lebt zusammen mit seiner Mutter und seinem Bruder in einer Pinguinkolonie im Heimatort „Buenos Eisig“ (im Original „Shiverpool“) in der Antarktis. Es ist bereits einige Jahre her, da besuchte die Surfer-Legende Zeke „Big Z“ Topanga diesen Ort. „Big Z“ schenkte dem kleinen Cody eine Muschel-Halskette, ermunterte ihn, seinen eigenen Weg zu gehen und niemals aufzugeben. Seitdem ist „Big Z“ sein großes Vorbild und Cody ein leidenschaftlicher Surfer, der davon träumt, auch einmal als Surfer berühmt und respektiert zu werden. Big Z jedoch verstarb bei seinem letzten Surfwettkampf in den „Knochenbrechern“, einem äußerst gefährlichen Teil des Meeres, der von vielen Felsen gesäumt ist.\n\nEines Tages besucht Talentsucher Mike Abromowitz den Ort auf der Suche nach Surfern, die beim Surf-Wettbewerb im Gedenken an den verstorbenen „Big Z“ teilnehmen können. Durch seine Hartnäckigkeit gelingt es Cody, einen Platz zu ergattern. Auf der Fahrt auf die tropische Insel „Pin-Gu Eiland“ (im Original „Pen Gu Island“), dem Veranstaltungsort des Surf-Wettbewerbs, freundet Cody sich mit dem Surfer-Hahn Chicken Joe an, der auch am Wettbewerb teilnimmt. Auf der Insel angekommen, trifft Cody auf die hübsche Rettungsschwimmerin Lani, in die er sich verliebt.\nDa bemerkt er, dass Tank Evans, der 9-malige Gewinner, gegen den auch Big Z bei seinem letzten Kampf antrat, Big Z aufs Gröbste verspottet. Wütend fordert Cody ihn heraus und hat beim Surfen einen Unfall. Dadurch tritt er auf einen Feuerseeigel.\nLani bringt den verletzten Cody daraufhin zu ihrem Onkel „Freak“ (im Original „Geek“), der zurückgezogen im Urwald der Insel lebt. Durch Zufall findet Cody heraus, dass es sich bei „Freak“ um die vermeintlich verstorbene Surfer-Legende „Big Z“ handelt. Dieser hatte seinen Tod nach einer sportlichen Niederlage gegen Tank Evans nur vorgetäuscht, um seinen Fans nicht als Verlierer gegenübertreten zu müssen. Cody will sich für den Wettbewerb von Big Z trainieren lassen, doch von diesem lernt er vor allem, dass es beim Surfen um mehr als nur ums Gewinnen geht.\n\nNeben dem Favoriten Tank Evans erreichen Cody und Chicken Joe überraschend das Finale des Wettbewerbs. Im letzten Lauf wird Cody von Tank Evans hart angegangen, schafft es jedoch sein Surfbrett unter Kontrolle zu behalten und sogar den Weg für Chicken Joe freizusperren, dem es dadurch gelingt, den Wettbewerb zu gewinnen. Cody wird hingegen in den Knochenbrecher abgedrängt, in dem einst „Big Z“ seinen Tod vortäuschte. Dort hat Tank einen Unfall und muss von Lani gerettet werden, Cody aber muss sie zurück im Meer lassen.\n\nDort allerdings kommt ihm Big Z zur Hilfe, der Cody mit seinen unglaublichen Fähigkeiten, Wellen einzuschätzen, aus dem Wasser retten kann. Am Strand angekommen, gibt Big Z bekannt, dass er nicht tot ist. Cody gilt als Held am Strand, wird von Lani umarmt und von den Passanten gefeiert.\n\nDie Vorproduktion begann bereits 2002. Die eigentlichen Dreharbeiten starteten am 23. August 2004. Die Produktionskosten wurden auf rund 85 Millionen US-Dollar geschätzt. Der Film feierte am 2. Juni 2007 seine Premiere im kalifornischen Westwood. Kinostart des von Sony Pictures Animation produzierten und Sony Pictures Imageworks animierten Films war in den USA am 8. Juni 2007 und in Deutschland am 13. September 2007. Der Film spielte in den Kinos weltweit rund 149 Millionen US-Dollar ein, davon rund 59 Millionen US-Dollar in den USA. An den deutschen Kinokassen wurden fast 950.000 Besucher gezählt. Bereits 18 Wochen nach dem Kinostart war der Film als DVD erhältlich.\n\nDer Film war die zweite Animationsproduktion, bei der Jeff Bridges als Synchronsprecher beteiligt war. Die erste Produktion \"Das letzte Einhorn\" lag zu diesem Zeitpunkt bereits 25 Jahre zurück. Die Rolle des Onkel „Freak“ (im Original „Geek“) ist eine Hommage an die ebenfalls von Jeff Bridges verkörperte Rolle des „Dude“ aus dem Film \"The Big Lebowski\".\n\nDie beiden Regisseure Ash Brannon und Chris Buck steuerten die Synchronstimmen der Filmcrew, die die Dokumentation über Cody drehen, bei.\n\nAls Cody zu Beginn gefragt wird, ob er noch andere Talente außer Surfen besitzt, stellt er die Gegenfrage: „Sowas wie singen und tanzen?“. Dies ist eine Anspielung auf einen weiteren computeranimierten Spielfilm, in dem auch Pinguine die Hauptdarsteller sind, der nur ein Jahr zuvor veröffentlicht wurde: \"Happy Feet\".\n\nDer Name des Fernsehsenders SPEN (Sports Penguin Entertainment Network) ist eine Anspielung auf den Sport-Fernsehsender ESPN. Sal Masekela, der im Film den SPEN-Fernsehsprecher spielt, ist auch bei ESPN als Sprecher tätig.\n\nIm Film wurde an einer Stelle die Figur Meeper aus dem animierten Kurzfilm \"Die Chubbchubbs!\" im Hintergrund eingefügt. Der Kurzfilm (Originaltitel: \"The Chubbchubbs!\") befindet sich zudem auf der DVD von \"Könige der Wellen\". Daneben wurden kaum sichtbar an verschiedenen Stellen UFOs im Hintergrund eingefügt – ein Spaß der Animatoren, die wussten, dass Regisseur Ash Brannon ein UFO-Fan ist.\n\nDas Animationsteam wurde an den Strand von Malibu geschickt, um Bewegungsabläufe und das Verhalten von Surfern zu studieren. Der brasilianische Surfer Renato Mendes, sein australischer Kollege Rory Nubbins sowie der Japaner Tatsuhi Kobayashi sind während des Wettkampfes im Film zu sehen. Der englische Originaltitel \"Surf’s Up\" ist ein Begriff aus der Surfer-Sprache und bedeutet so viel wie „hohe Brandung“ im Wasser oder allgemein „gute Surfbedingungen“.\n\nDas deutschsprachige Dialogbuch schrieb Michael Nowka, der ebenfalls die Dialogregie übernahm.\nDie deutsche Synchronisation des Films übernahm die Berliner Synchron AG in Berlin.\n\nAm 10. Oktober 2007 wurde von Sony ein Soundtrack bestehend aus 14 Titeln veröffentlicht:\n\nDie beiden Titel „Holiday“ und „Welcome to Paradise“ der Band Green Day wurden zwar im Film verwendet, befinden sich aber nicht auf dem Soundtrack-Album zum Film.\n\nGernot Gricksch schrieb in der Zeitschrift \"TV Digital\" 19 vom 7. September 2007, dass der sprunghafte Stil des Films Kinder verwirren dürfte, Erwachsene hingegen würden vergebens nach so etwas wie einer Handlung suchen. Es würden Top-Tricks gezeigt, aber insgesamt nicht so gut wie beim Pinguinfilm \"Happy Feet\".\n\nDas Lexikon des internationalen Films urteilt: „Was in Form und Inhalt originell als »Surf-Dokumentation« beginnt, verflacht schnell zur üblichen »Underdog kommt groß heraus«-Geschichte, die von coolen, aber wenig sympathischen Charakteren beseelt ist. Auch die ausgefeilte Tricktechnik kann den auf Bewährtes setzenden, für eine Komödie nur bedingt amüsanten Film nicht retten.“\n\nDer Film wurde im Jahr 2008 für zahlreiche Auszeichnungen nominiert, konnte jedoch lediglich zwei der Preise – beides Annie Awards – gewinnen.\n\nBeim Young Artist Award wurde der Film in der Kategorie „Best Family Feature Film (Animation)“ nominiert. Bei den VES Awards erhielten Rob Bredow, Lydia Bottegoni, Daniel Kramer und Matt Hausman eine Nominierung in der Kategorie „Best Single Visual Effect of the Year“ für die Surf-Szene in der sich brechenden Welle. Mit einer Nominierung in einer ähnlichen Kategorie „Outstanding Effects in an Animated Motion Picture“ wurden Rob Bredow, Daniel Kramer, Matt Hausman und Danny Dimian bei den VES Awards bedacht. Ebenfalls bei den VES Awards wurden David Schaub, Moon-Jung Kang, Brian Casper und Andreas Procopiou für die Rolle des Chicken Joe in der Kategorie „Outstanding Animated Character in an Animated Motion Picture“ nominiert, während David Schaub, Peter Nash, James Crossley und Shia LaBeouf eine Nominierung in derselben Kategorie für die Rolle des Cody erhielten.\n\nBei den Golden Reel Awards erfolgte eine Nominierung in der Kategorie „Best Sound Editing – SFX, Foley, Dialogue & ADR for Feature Film Animation“ für Steven Ticknor, Martin Lopez, Michael J. Benavente, Jason King, Ulrika Akander, Gary A. Hecker sowie Michael J. Broomberg.\n\nBei den Annie Awards gewannen Deborah Carlson in der Kategorie „Best Animated Effects“ und John Clark in der Kategorie „Best Animation Production Artist“, während es zu acht weiteren Nominierungen kam. Diese waren im Einzelnen das „Best Animated Feature“ für den Film und personenbezogene Auszeichnungen für Dave Hardin („Best Character Animation in a Feature Production“), Alan Hawkins („Best Character Animation in a Feature Production“), Sylvain Deboissy („Best Character Design in an Animated Feature Production“), Ash Brannon und Chris Buck („Best Directing in an Animated Feature Production“), Marcelo Vignali („Best Production Design in an Animated Feature Production“), Denise Koyama („Best Storyboarding in an Animated Feature Production“) sowie Don Rhymer, Ash Brannon, Chris Buck und Christopher Jenkins („Best Writing in an Animated Feature Production“).\n\nDie Saturn Awards brachten eine Nominierung des Films in der Kategorie „Best Animated Film“.\n\nBei der Verleihung des Oscar wurden Ash Brannon und Chris Buck mit einer Nominierung in der Kategorie „Best Animated Feature Film of the Year“ bedacht. Dies war für viele Kritiker eine überraschende Nominierung, da neben den beiden nominierten Filmen \"Ratatouille\" und \"Persepolis\" mit einer Nominierung des an den Kinokassen überaus überzeugenden Films \"Die Simpsons – Der Film\" gerechnet wurde.\n\n"}
{"id": "2727131", "url": "https://de.wikipedia.org/wiki?curid=2727131", "title": "MANIAC I", "text": "MANIAC I\n\nDer MANIAC I (Mathematical Analyzer Numerical Integrator And Computer Model I; engl. \"maniac\" „Verrückter“) war ein früher Universalrechner, der unter der Leitung von Nicholas Metropolis Anfang der fünfziger Jahre für das Los Alamos National Laboratory gebaut wurde. Der auf der Von-Neumann-Architektur basierende Rechner wurde bei der University of California gebaut und kostete mit Ausgabegeräten 298.000 US-Dollar.\n\nDer Röhrencomputer MANIAC benötigte eine Standfläche von 1,85 Quadratmetern und war knapp 2 Meter hoch. Der Energiebedarf des Rechners betrug 35 Kilowatt. 2400 Elektronenröhren und 500 Kristalldioden übernahmen die Schaltungen, die Befehle wurden entweder per Lochstreifen oder über Magnetband eingelesen. Die Speicherung erfolgte entweder elektrostatisch (Kapazität: 1024 Wörter) oder auf einer Magnettrommel (Kapazität: 10.000 Wörter). Die Durchführung einer Addition dauerte 80 Millisekunden, Multiplikations- und Divisionsschritte nahmen eine Sekunde in Anspruch. Die Ausgabe erfolgte entweder über einen Anelex-Drucker, einen Fernschreiber, auf Papierband oder einen Magnetstreifen.\n\nMANIAC I lief zum ersten Mal erfolgreich im März 1952, er wurde dann zur Berechnung von Nuklearwaffen eingesetzt, bis er 1957 durch seinen Nachfolger MANIAC II ersetzt wurde. Der Großrechner wurde dann an die University of New Mexico übergeben, wo er weiterverwendet wurde.\n\n1961 wurde MANIAC III an der University of Chicago gebaut.\n\nMANIAC war der erste Computer, der gegen einen Menschen ein Schachspiel gewann. 1956 gewann er in einem 6×6-Spiel gegen eine junge Frau, die erst eine Woche zuvor mit dem Schachspiel begonnen hatte.\n\n"}
{"id": "2740398", "url": "https://de.wikipedia.org/wiki?curid=2740398", "title": "Boost (C++-Bibliothek)", "text": "Boost (C++-Bibliothek)\n\nBoost () ist eine freie C++-Bibliothek, die aus einer Vielzahl von portablen Unterbibliotheken besteht. Die Unterbibliotheken dienen unterschiedlichsten Aufgaben von Algorithmen auf Graphen über Metaprogrammierung bis hin zu Speicherverwaltung.\n\nEs können jederzeit neue Bibliotheken zur Eingliederung in Boost vorgeschlagen werden, diese müssen jedoch einen aufwendigen Review-Prozess durchlaufen. Teile von Boost wurden in einen sogenannten \"Technical Report\" des Standardisierungskomitees für C++ übernommen und später in die Sprache C++ integriert (ISO/IEC 14882:2011).\n\nBoost besteht aus Unterbibliotheken zu verschiedensten Zwecken. Gemeinsam ist ihnen das Ziel der Produktivitätssteigerung beim Programmieren mit C++. Außerdem sollen Boost-Bibliotheken portabel und allgemein anwendbar sein. Ansonsten dienen sie in ihrer Gesamtheit keinem speziellen Zweck.\n\nFolgende Themenbereiche werden durch Boost zurzeit unter anderem abgedeckt:\n\nDas Boost-Projekt wurde im Jahr 2000 ursprünglich von Mitgliedern des C++-Standardisierungskomitees gegründet, um Vorschläge für C++-Erweiterungen öffentlich zu machen und im praktischen Einsatz zu testen. Inzwischen ist Boost eine große Sammlung von C++-Bibliotheken unabhängig von ihrer Relevanz für den nächsten C++-Standard.\n\nAuch bekannte kommerzielle Firmen steuern Code und Bibliotheken bei, etwa die Grafikbibliothek \"GIL (Generic Image Library)\" von Adobe.\n\nEinige der Bibliotheken wurden mittlerweile in den bereits verabschiedeten ' verbindlich aufgenommen und finden sich somit im kommenden C++-Standard wieder. Zehn Boost-Bibliotheken wurden dabei vom C++-Komitee akzeptiert. Außerdem wurden für den geplanten ' (eine vom Standardisierungskomitee anzunehmende Erweiterung des kommenden C++-Standards) einige weitere Boost-Bibliotheken vorgeschlagen.\n\nNoch in der Gemeinschaft aktiver Initiatoren von Boost sind unter anderem Beman Dawes und David Abrahams. Der Autor mehrerer Bücher über C++, Nicolai Josuttis, stellte 2001 die Boost.Array-Bibliothek zur Verfügung. Etwa 3000 Leute haben sich in die Boost-Mailinglisten eingetragen und einige dutzend davon sind auch regelmäßig aktiv.\n\nJede neue Bibliothek, die in Boost aufgenommen werden soll, muss einem sogenannten Review durch die Boost-Community unterzogen werden. Dabei können von interessierten Entwicklern sowohl Bewertungen abgegeben werden, als auch Zustimmung oder Ablehnung zum Ausdruck gebracht werden. Diese werden ausgewertet und haben Einfluss auf die Aufnahme in die Bibliothek. Durch diesen Prozess soll eine hohe Qualität erreicht werden, da diese eventuell den Weg in den C++-Standard finden.\n\nUnabhängig vom Reviewprozess muss eine Bibliothek, die Teil von Boost werden soll unter anderem\n\nAuch muss der Autor aktiv am Qualitätssicherungsprozess teilnehmen und formale Dinge wie z. B. Quellcodeformatierung oder Dokumentation müssen den vereinbarten Richtlinien entsprechen.\n\nUm Teil von Boost zu werden, muss eine Bibliothek eine Lizenz aufweisen, die folgende Bedingungen erfüllt:\n\nAls Ergebnis dieser Bedingungen ist es möglich, alle Boost-Bibliotheken ohne Kenntnis der konkreten Lizenz zu nutzen. Man kann Boost-Bibliotheken in Projekten nutzen, die unter einer Lizenz mit Copyleft-Klausel stehen, ebenso wie in Projekten, die nicht quelloffen sind.\n\nEs wird dringend empfohlen, auf die \"Boost Software License\" zurückzugreifen, da es ein langfristiges Ziel ist, alle Boost-Bibliotheken unter einer einheitlichen Lizenz zur Verfügung zu stellen. De facto ist es unwahrscheinlich, dass eine Bibliothek ohne \"Boost Software License\" akzeptiert wird.\n\nAlle Boost-Komponenten werden regelmäßig auf verschiedensten alten und neuen Compilern getestet. Es werden unterschiedliche Dinge, wie die Kompilierfähigkeit, Funktionsfähigkeit und Vollständigkeit der Lizenzinformationen überprüft. Die Ergebnisse werden regelmäßig auf der Boost-Mailingliste angekündigt und auf der Boost-Webseite veröffentlicht.\n\nBoost enthält ein eigenes Test-Framework, mit dem auch versucht wird, die Qualität der Boost-Bibliotheken aufrechtzuerhalten. Dieses ist lose nach Maximen des Extreme Programming aufgebaut.\n\nBei Bugs bitten die Boost-Entwickler um Meldung auf einer der Mailinglisten oder im Boost-eigenen Trac, allerdings bevorzugt erst nachdem der meldende Programmierer überprüft hat, ob der Fehler in der aktuellen Entwickler-Version behoben ist.\n\nTest und Pflege von Boost wird durch die stark variierende Qualität der unterstützten Plattformen und vor allem Compiler erschwert, was man unter anderem an zahlreichen Varianten und Umgehungslösungen innerhalb des Boost-Codes erkennen kann.\n\nKritisch ist, dass einige Boost-Bibliotheken lange Zeit keine Pflege erhalten oder erhalten haben, was zumindest ihre Tauglichkeit für moderne Anwendungen einschränken kann.\n\nViele Teile von Boost machen umfangreichen Gebrauch von Templates, d. h. generische Programmierung oder Metaprogrammierung. Dies stellt hohe Anforderungen an die Konformität der zu verwendenden Compiler. Es wird jedoch parallel versucht, so viele Compilerfehler wie möglich zu umgehen. Die Entwicklungen decken auch Beschränkungen der bisherigen Compiler auf und können so zu weitergehenden Vorschlägen für die Standardisierung von Compilern und deren Konformitätstests führen.\n\n\n"}
{"id": "2742259", "url": "https://de.wikipedia.org/wiki?curid=2742259", "title": "Gears", "text": "Gears\n\nGears (vorher \"Google Gears\") ist eine inzwischen nicht mehr weiterentwickelte Software des Unternehmens Google. Gears erweiterte unterstützte Webbrowser um einige Features, die von den Browsern zum damaligen Zeitpunkt nicht oder nicht in konsistenter Form angeboten wurden. Etwa wird es ermöglicht, Inhalte von Webapplikationen zum späteren Offline-Lesen zu synchronisieren oder Daten in einer lokalen SQLite-Datenbank abzulegen. Auch ist es möglich, nachdem der Benutzer dies erlaubt hat, die geografische Position anhand eines angeschlossenen GPS-Empfängers, der verfügbaren WLANs und Mobilfunksender oder eines Dienstes zu ermitteln. Ferner ermöglicht es Gears, mehrere Dateien auf einmal auszuwählen und hochzuladen.\n\nGoogle hat die Entwicklung im November 2011 eingestellt, da mit der breiten Unterstützung von HTML5 Gears nicht mehr notwendig sei. Aus dem hauseigenen Browser Google Chrome wurde die Unterstützung für Gears bereits mit der Version 12 entfernt. Der Quellcode ist als Open-Source-Projekt weiterhin verfügbar.\n\n\"Gears\" installiert im Browser eine Erweiterung (Browser-Plug-in), die es einem JavaScript-API ermöglicht, auf den lokalen Datenträger zuzugreifen. Die Erweiterung ist bisher für Firefox unter Windows, macOS und Linux sowie für den Internet Explorer verfügbar. Für Apple Safari gibt es sie nur für Mac OS X.\n\nNeben Google Reader und Google Drive unterstützt \"Gears\" seit Januar 2009 auch Google Mail. Web-Applikationen anderer Anbieter wie Remember The Milk, Zoho oder WordPress (ab der Version 2.6) unterstützen die \"Gears\"-Funktionalität. Allerdings ist eine manuelle Umschaltung zwischen Online- und Offlinemodus erforderlich.\n\nEin Google-Entwickler entwickelte eine Version des \"Google Readers\" während des Firmenprogrammes, das es Arbeitnehmern erlaubt, 20 Prozent der Wochenarbeit – also einen Arbeitstag – für ihre eigenen Projekte zu verwenden. Bret Taylor, der Entwicklungsleiter der Google-Projektgruppe, sagte, dass der Ingenieur den Zugriff auf den \"Google Reader\" während einer Geschäftsreise haben wollte.\n\nAm 28. Mai 2008 entschloss man sich, Google Gears zu Gears umzubenennen. Damit soll gezeigt werden, dass es ein offenes Projekt ist, das nicht nur von Google betrieben wird. Deshalb ging am 11. Juni 2008 auch das neue Logo (ohne Google im Logo) online.\n\nSeit September 2008 gibt es auch eine Gears-Version für Safari auf dem Mac.\n\nAm 19. Februar 2010 wird im Gears-Blog bekannt gegeben, dass Google seine Gears-Unterstützung zugunsten des freien Webstandards HTML5 zurückgefahren hat. Insbesondere werden keine neue Funktionen mehr in Gears aufgenommen. Mit dem Erscheinen von Google Chrome 12 am 7. Juni 2011 wurde Gears aus Chrome entfernt. Zuvor war Chrome der einzige Browser, der Gears bereits in der Standardkonfiguration mitlieferte.\n\nWie man inzwischen auf der Produktseite lesen kann, wird der Download von Gears bis Dezember 2011 angeboten.\n\n\"Gears\" wurde als Open Source unter BSD-Lizenz veröffentlicht.\n\nEs gibt vier Haupt-APIs in \"Gears\":\n"}
{"id": "2744529", "url": "https://de.wikipedia.org/wiki?curid=2744529", "title": "Absynth", "text": "Absynth\n\nAbsynth ist ein von dem Amerikaner Brian Clevinger entwickelter Software-Synthesizer. Sein Konzept basiert auf digitaler Klangerzeugung. Absynth kombiniert klassische Verfahren (wie z. B. die subtraktive Synthese) mit moderneren Verfahren (z. B. Frequenzmodulation, Granularsynthese und Sampling), um komplexe Klangfarben und Klangflächen zu erzeugen.\n\nAbsynth kann direkt von MIDI-fähigen Instrumenten wie z. B. Keyboards oder als Plug-in von DAW-Anwendungen wie Ableton Live, Cubase, Logic oder Sonar aus angesteuert werden. Der Vertrieb von Absynth erfolgt über die Berliner Firma Native Instruments.\n\nDas Grundkonzept von Absynth beruht auf einem dreikanaligen Aufbau der Klangerzeugung. Jeder dieser drei Kanäle verfügt wiederum über drei Module (vgl. auch Modularer Synthesizer):\nDie Parameter von Klangerzeugung, Filtern und Modulation lassen sich bei Absynth über Wellenformen, Hüllkurven, LFOs sowie über MIDI-Controller steuern. Wellenformen und Hüllkurven können vom Anwender selbst editiert werden. Die drei Kanäle werden über individuelle Lautstärke- und Panorama-Einstellungen in einem Master-Kanal zusammengemischt.\n\nGegenüber konventionellen Hardware-Synthesizern, bei denen die jeweils vom Anwender vorgenommenen Einstellungen der Klangparameter oft nur über Kontrollleuchten oder kleine Displays angezeigt werden können, bieten Software-Synthesizer die Möglichkeit, eine Vielzahl solcher Einstellungen über eine grafische Oberfläche auf dem PC-Monitor zu visualisieren. Absynth nutzt diesen Vorteil auf folgende Weisen:\n\nAbsynth wurde ursprünglich von Brian Clevinger für die Mac-Os-Plattform entwickelt und ab Oktober 2000 von der Pariser Firma Rhizomatic Software als Internet-Download vertrieben. Diese erste Version verfügt bereits über die wesentlichen Merkmale, die bis heute das Design von Absynth kennzeichnen (halbmodulares Design, verschiedene Klangsynthese-Modi, grafisches Patch-Feld, Wellenform-Editor, Hüllkurven-Editor, 64-stimmige Polyphonie, MIDI-Steuerung). Clevinger schloss sich im Jahr 2001 mit Native Instruments zusammen und ist bis heute Chefentwickler der Software.\n\n\nTests und Rezensionen\n"}
{"id": "2752606", "url": "https://de.wikipedia.org/wiki?curid=2752606", "title": "Bee Movie – Das Honigkomplott", "text": "Bee Movie – Das Honigkomplott\n\nBee Movie – Das Honigkomplott ist ein Computeranimationsfilm von DreamWorks Animation, der im Dezember 2007 in die deutschen Kinos kam. (Der Titel „Bee Movie“, übersetzt \"Bienenfilm\", ist eine Anspielung auf den Begriff B-Movie.)\n\nDer Drohn Barry B. Benson hat gerade seine College-Ausbildung abgeschlossen und steht vor dem Eintritt in das Berufsleben. Der einzige Beruf, den er ergreifen kann, ist die Arbeit in der Honigfabrik Honex. Da Barry jedoch nicht sein Leben lang dort arbeiten will, verlässt er den Bienenstock, um die Welt zu erkunden. Dort bricht er das eiserne Gebot des Schweigens gegenüber den Menschen, indem er sich mit der Floristin Vanessa anfreundet.\n\nVon ihr erfährt er, dass die Menschen seit Jahrhunderten den Honig essen, für den die Bienen so hart arbeiten müssen. Mit Vanessas Hilfe verklagt er erfolgreich die Honigproduzenten. Dadurch haben die Bienen plötzlich einen immensen Honigüberschuss und stellen ihre Arbeit ein. Dies führt wiederum dazu, dass die Blumen nicht bestäubt werden und beinahe aussterben.\n\nBarry gelingt es, die Bienen von der großen Bedeutung ihrer Arbeit zu überzeugen. Er lässt sich schließlich in Vanessas Blumengeschäft nieder, wo er eine Anwaltskanzlei eröffnet.\n\nDie Synchronisation übernahm die Berliner Synchron GmbH Wenzel Lüdecke. Oliver Rohrbeck führte die Dialogregie und schrieb zusammen mit Andrea Wilhelm das Dialogbuch.\nZusammen mit Cineplex hat die BEEgroup der Universität Würzburg einen kurzen Vorfilm erstellt, um den wissenschaftlichen Hintergrund besser zur Geltung kommen zu lassen. Er war in allen Kinos des Cineplex-Verbunds zu sehen und informierte über die Honigbienen und ihr tatsächliches Leben.\n\nDie Biene \"Barry B. Benson\" präsentierte während der Oscarverleihung 2008 die Nominierten in der Kategorie Bester animierter Kurzfilm.\n\n"}
{"id": "2762241", "url": "https://de.wikipedia.org/wiki?curid=2762241", "title": "Vorbereiten des Ausspähens und Abfangens von Daten", "text": "Vorbereiten des Ausspähens und Abfangens von Daten\n\nVorbereiten des Ausspähens und Abfangens von Daten (umgangssprachlich auch Hackerparagraf oder Hackertoolparagraf) ist ein Tatbestand, der in des deutschen Strafgesetzbuches (StGB) normiert ist. Er wurde Ende Mai 2007 mit großer Mehrheit im Deutschen Bundestag verabschiedet. Der Paragraph stellt die Beschaffung und Verbreitung von Zugangscodes zu zugangsgeschützten Daten sowie auch die Herstellung und den Gebrauch von Werkzeugen, die diesem Zweck dienlich sind, als Vorbereitung einer Straftat unter Strafe (maximal zwei Jahre Freiheitsstrafe). Eine juristische Stellungnahme der European Expert Group for IT Security (EICAR) geht davon aus, dass gutartige Tätigkeiten (im Dienste der IT-Sicherheit) bei ausführlicher Dokumentation nach diesem Paragraphen nicht strafbar sind.\n\nEr wurde durch das \"Einundvierzigste Strafrechtsänderungsgesetz zur Bekämpfung der Computerkriminalität (41. StrÄndG)\" in das StGB eingefügt und trat am 11. August 2007 in Kraft. Die deutsche Rechtsnorm stellt unter anderem die Herstellung und die Verbreitung von sogenannten Hackertools unter bestimmten Umständen unter Strafe. Hierdurch wird das Übereinkommen des Europarats über Computerkriminalität vom 23. November 2001 (Cybercrime Convention, ETS No. 185) sowie der Rahmenbeschluss des Rates der Europäischen Union über Angriffe auf Informationssysteme umgesetzt. Die Höchststrafe in der Urfassung von 2007 war ein Jahr Freiheitsstrafe.\n\nArtikel 9 Abs. 2 der Richtlinie 2013/40/EU fordert eine Mindesthöchststrafe von zwei Jahren. Die Umsetzung erfolgte mit Nr. 5 des \"Gesetzes zur Bekämpfung der Korruption\" durch die Erhöhung des Strafrahmens in Abs. 1 StGB n.F. auf zwei Jahre.\n\n§ 202c Vorbereiten des Ausspähens und Abfangens von Daten\nWelche Software unter Hackertools fällt, ist im Gesetzestext sehr vage formuliert und stößt daher auf erhebliche Kritik insbesondere von Sicherheitsexperten und IT-Branchenverbänden. Vor allem wird kritisiert, dass allein entscheidend sei, dass ein Programm oder eine Information genutzt werden könnte, in fremde Computer einzudringen und keine Ausnahmeregelungen bestehen, die den Einsatz für legale Zwecke erlaubt. So wurde unter anderem gegen das Bundesamt für Sicherheit in der Informationstechnik Strafanzeige erstattet, da das Amt angeblich selbst gegen das Gesetz verstoße. Die Staatsanwaltschaft Bonn stellte das Ermittlungsverfahren ein, da der Tatbestand gemäß § 202c StGB nicht erfüllt sei. Ebenso wurde das Verfahren von der Staatsanwaltschaft Mannheim im Falle der Selbstanzeige von Michael Kubert im Februar 2008 eingestellt.\n\nAls Reaktion auf die wachsende Kritik hat der Rechtsausschuss des Deutschen Bundestages 2007 in einem Bericht darauf hingewiesen, dass der gutwillige Umgang mit Hackertools durch IT-Sicherheitsexperten nicht vom § 202c StGB erfasst werde. Auch die Bundesjustizministerin Brigitte Zypries verwies im Juli 2007 mehrfach darauf, dass dieser Paragraf nur die Vorbereitungshandlungen zu Computerstraftaten unter Strafe stelle.\n\nFraglich war auch die Rechtslage für die Hersteller von Hackertools, wenn sie ihre Software beispielsweise im Internet verbreiten und diese von Kriminellen tatsächlich für Straftaten missbraucht werden. Aus diesem Grund verlagern viele Hersteller ihr Angebot auf ausländische Webseiten bzw. publizieren im Ausland.\n\nEin weiteres Beispiel für die Unklarheit dieses Paragrafen stellt auch der Beschluss der Staatsanwaltschaft des Landgerichts Fulda dar. Denn auch der Fall der Selbstanzeige des Geschäftsführers Herbert Treinen der dit-consulting GmbH, ebenfalls IT-Dienstleister und damit zwangsläufig auf den Gebrauch sogenannter Hacker-Tools angewiesen, wurde im Februar 2008 eingestellt. Als Begründung gab die Staatsanwaltschaft an, dass „die §§ 202a, 202b mangels Rechtswidrigkeit nicht gegeben sind“ und somit eine Verurteilung nicht zu erwarten war. Auch der Tatbestand des § 202c StGB sei nicht erfüllt worden, da die Hacker-Tools nur zu „gutartigen“ Verwendungen beschafft und genutzt wurden, so die Staatsanwaltschaft.\n\nEbenfalls wurde die Ende 2008 erstattete Selbstanzeige des \"iX\"-Chefredakteurs Jürgen Seeger von der Staatsanwaltschaft Hannover „aus rechtlichen Gründen“ im März 2009 abgelehnt. Seeger hatte sich selbst angezeigt, nachdem er ein \"iX\"-Sonderheft „Sicher im Netz“ veröffentlichte. Diese enthält unter anderem einen Datenträger mit der Linux-Distribution BackTrack, die verschiedene \"Hackertools\" beinhaltet. „Aufgrund der erheblichen Rechtsunsicherheit nicht nur bei professionellen Sicherheitsexperten, sondern auch bei Zeitschriften, bleibt uns keine andere Wahl, als die juristische Einordnung des Verteilens derartiger Programme im Rahmen einer Selbstanzeige prüfen zu lassen“, kommentierte Seeger seine Selbstanzeige. Die Staatsanwaltschaft wiederum kommentierte die Ablehnung dahingehend, dass es vor allem auf die subjektive Vorstellung des Handelnden ankäme. Dies spiegele auch der Gesetzesentwurf wider.\n\nDer Chaos Computer Club hat 2008 Auswirkungen der Strafrechtsänderung des Hackerparagrafen untersucht und in einer Stellungnahme Standortnachteile für IT-Betriebe in Deutschland festgestellt. Diese rechtlichen Maßregelungen würden vielmehr dem Ziel des Gesetzgebers entgegenstehen und das Sicherheitsniveau senken: „Sicherheitsforscher und -unternehmen können Leistungen nicht mehr erbringen, ohne sich der Gefahr einer Strafverfolgung auszusetzen.“ Vielmehr zeige sich, dass die Ziele des Gesetzgebers, eine Verbesserung der Sicherheitslage zu erreichen, verfehlt wurden. Die Kriminalisierung von Softwareherstellern und -benutzern führe zu einem Standortnachteil für die deutsche Forschung und Wirtschaft.\n\nIn der Rechtswissenschaft wurde die Vorschrift als Straftatbestand „ohne erkennbaren ‚Unrechtskern‘“ bezeichnet.\n\nAufgrund dieser Unklarheiten haben drei Personen – eine aus der IT-Branche, eine aus dem akademischen Bereich und der Berliner Rechtsanwalt und Strafverteidiger Ulrich Kerner – jeweils eine Verfassungsbeschwerde gegen den sogenannten \"Hackerparagraphen\" (genauer: gegen § 202c Absatz 1 Nr. 2 StGB) eingereicht. Die drei Beschwerden wurden mit Beschluss vom 18. Mai 2009 durch das Bundesverfassungsgericht (BVerfG) als unzulässig abgelehnt.<ref name=\"Nr. 67/2009\"></ref><ref name=\"2 BvR 2233/07\"></ref> Das BVerfG begründete die Ablehnung damit, dass die Beschwerdeführer durch § 202c StGB nicht „selbst, gegenwärtig und unmittelbar“ in ihren Grundrechten betroffen seien. Denn ein Risiko strafrechtlicher Verfolgung sei bei einer verfassungskonformen Auslegung des Gesetzestextes für die von ihnen genannten Tätigkeiten im Umgang mit derartigen Programmen nicht gegeben. Zum einen könne man (insbesondere bei sogenannten „dual use tools“) nicht davon ausgehen, dass die Programme als „Zweck die Begehung einer Straftat“ hätten. Bei den Beschwerdeführern fehle jedenfalls das „\"subjektiv[e] Merkmal\" der Vorbereitung einer Computerstraftat“.\n\nHier stellt Artikel 143bis StGB das unbefugte Eindringen in ein Datenverarbeitungs-System als solches, Art. 143 den Daten-Diebstahl und 144 bis die Daten-Beschädigung unter Strafe. Hinzu kommen die zivilrechtlichen Schadenersatz-Ansprüche.\n\nPer 1. Januar 2011 wurde in der Schweiz Art. 143bis StGB um Bestimmungen analog zum deutschen Hackerparagraphen erweitert.\n\n\n\n"}
{"id": "2764253", "url": "https://de.wikipedia.org/wiki?curid=2764253", "title": "Open Workbench", "text": "Open Workbench\n\nDie Open Workbench ist eine frei verfügbare Projektmanagementsoftware und eine Alternative zu Microsoft Project. Sie unterstützt Projektleiter durch die grafische Aufbereitung des Projektablaufs. Die Anwendung leistet dies wahlweise über eine vorgefertigte Gantt-Darstellung, über Netzplandarstellungen oder aber über individuell sehr fein einstellbare Abwandlungen dieser Darstellungen.\n\nOpen Workbench ermittelt auch Informationen über den kritischen Pfad. Über die Autoplan-Funktion kann der Projektleiter seinen Projektplan automatisiert optimieren. Er kann dabei vorwärts, das heißt ab einem gegebenen, fixen Projektstart arbeiten, oder rückwärts, das heißt bei Beachtung eines fixen Enddatums vorgehen. Für den unternehmensweiten Einsatz (mit einer zentralen Datenbank) benötigen die Benutzer meist das kostenpflichtige Projekt- und Portfoliomanagement-System \"Clarity\" des Herstellers Computer Associates; begrenzt ist aber bei geschicktem Vorgehen auch ohne den Einsatz von \"Clarity\" ein reifes Multiprojektmanagement möglich.\n\nOpen Workbench stellt recht einfach einzustellende Abweichungsanalysen von einer Planbasis in einer druckbaren Übersicht dar und kann dabei je nach Vorlieben des Anwenders auch differenzierte Kosten- und Terminabweichungen sowie Ressourcen-Überlastungen berichten. Wenn erforderlich, können auch graphisch sehr komplexe Projektdarstellungen erzeugt werden: prozentualer Erledigungsgrad, Übereinstimmung mit der ursprünglichen Planung und viele weitere Parameter sind in der Bildschirmansicht und in der Druckversion vermittelbar.\n\nDer Quellcode von Open Workbench ist mit Ausnahmen frei verfügbar. Bis Januar 2011 gab es auf der Hersteller-Website ein Support-Forum. Grund für die Abschaltung sollen Sicherheitsprobleme gewesen sein (Auskunft eines deutschen Partners).\n\n"}
{"id": "2766152", "url": "https://de.wikipedia.org/wiki?curid=2766152", "title": "Nuendo", "text": "Nuendo\n\nNuendo ist ein Sequenzer und vorrangig ein digitales Tonsignalbearbeitungsprogramm der deutschen Firma Steinberg. Es weist die wesentlichen Funktionen des vom selben Hersteller stammenden Programms Cubase auf. Zum Beispiel gibt es direkte Unterstützung für die Raumklang-Produktion, weshalb dieses Programm häufig zum Nachvertonen von Filmen verwendet wird, sowie administrative Netzwerkfunktionen, zusätzliche Automatisierungs- und Control-Systeme, Aufnahmelängen, die über die sogenannten Tagegrenzen des Zeitstrahls hinausgehen, Anpassung von Videomaterial an die Audioaufnahmen, Verwendung mehrerer Aufnahme-Datenträger innerhalb eines Projektes und Einbindung bzw. Kompatibilität mit speziellen, international üblichen Nachvertonungswerkzeugen. Lediglich professioneller Notendruck, Noteneditor, Drumeditor und einige in der Produktreihe Cubase mitgelieferten virtuellen Instrumenten-Plugins müssen mittels eines Expansion-Kits zugekauft werden.\n\nNuendo ist für die Betriebssysteme Microsoft Windows und OS X erhältlich.\n\nFür die Aufnahme und Wiedergabe von Audiomaterial wird eine Soundkarte, möglichst mit schnellem ASIO-Treiber, benötigt. Für die Aufnahme von MIDI-Daten muss ein entsprechendes Interface am Rechner angeschlossen sein. Über MIDI können externe Klangerzeuger (z. B. Synthesizer) sowie MIDI-Eingabegeräte (MIDI-Tastatur, MIDI-Gitarre, MIDI-Controller etc.) mit dem PC und somit der Tonsignal-Bearbeitungssoftware verbunden werden.\n\nBezüglich der Dateiformate herrscht Kompatibilität, ein mit Cubase-Versionen erstelltes Projekt lässt sich also auch mit Nuendo problemlos öffnen, dies gilt auch umgekehrt.\n\nUm eine unberechtigte Weitergabe der Programme zu unterbinden, setzt Steinberg Dongles als Kopierschutz für seine Programme ein. Nuendo benutzt USB-Dongles, die Technologie dazu hat Steinberg im Oktober 2008 von der deutschen Firma Syncrosoft eingekauft.\n\n"}
{"id": "2766614", "url": "https://de.wikipedia.org/wiki?curid=2766614", "title": "Pardus (Linux-Distribution)", "text": "Pardus (Linux-Distribution)\n\nPardus ist eine freie Linux-Distribution, die im Zuge des Pardus-Projekts in der Türkei entwickelt wurde. Das Open-Source-Projekt wurde im September 2003 gegründet und wird vom Nationalen Forschungsinstitut für Elektronik und Kryptologie (UEKAE) als eigenständige Linux-Distribution entwickelt und finanziert; Pardus ist damit kein Derivat.\n\nDer Projektname leitet sich aus der wissenschaftlichen Bezeichnung des anatolischen Leoparden (Panthera pardus) ab.\n\nPardus setzt den Schwerpunkt auf Benutzerfreundlichkeit und Stabilität und will so sicherstellen, dass Pardus flächendeckend in der Türkei eingesetzt wird. Neben dem Einsatz in öffentlichen Institutionen, Organisationen und Behörden soll Pardus insbesondere auch in privaten Haushalten verbreitet werden. Erreicht werden soll dies unter anderem durch eine breite Hardwareunterstützung sowie die gezielte Entwicklung graphischer Komponenten, Programme und Werkzeuge, um Konsoleneingaben weitgehend umgehen zu können.\n\nPardus wird in der Türkei vom Nationalen Forschungsinstitut für Elektronik und Kryptologie (UEKAE) entwickelt. Diese Einrichtung ist dem Türkischen Wissenschafts- und Technologieforschungsamt (TÜBİTAK) unterstellt. Im Jahre 2006 arbeiteten an Pardus 15 Entwickler und eine nicht genau bekannte Anzahl von Freiwilligen. Bis zum Jahre 2012 sollte die Anzahl der Entwickler auf etwa 50 angehoben werden.\nTürkische Studenten haben zudem die Möglichkeit, ein Praktikum beim Pardus-Projekt zu absolvieren.\n\nLaut dem Projektkoordinator Erkan Tekman startete man das Pardus-Projekt im Jahre 2003, um die Sicherheit der eingesetzten Rechner u. a. auf Gebieten wie beim Nachrichtendienst und der Armee sicherzustellen. Um dies zu gewährleisten, musste sichergestellt werden, dass der Quellcode offensteht und frei modifizierbar ist.\nDie Kosteneinsparung durch Verwendung eines eigens entwickelten Betriebssystems basierend auf Linux spielte auch eine Rolle, da Microsofts Windows-Betriebssysteme auf Rechnern der Streitkräfte und zivilen Behörden dazu führte, dass eine Menge Geld für Lizenzen ins Ausland abgeführt werden musste.\n\nPardus wird in einem Pilotprojekt seit April 2007 von der Zentralen Rekrutierungsbehörde der Armee (ASAL) und deren Vertretungen verwendet. Sowohl weitere Verwaltungsstrukturen der Streitkräfte als auch das türkische Verteidigungs- und Bildungsministerium beobachten die Umstellung auf Pardus der Zentralen Rekrutierungsbehörde der Armee und planen eine Umstellung. Parallel hierzu wird das Bildungsministerium noch in diesem Jahr Pardus an vereinzelten Schulen einführen.\nPardus wird vom Hohen Rundfunk- und Fernsehrat der Türkei zur Archivierung, Verwaltung und Analyse des gesendeten Programminhalts von 210 nationalen Fernsehkanälen und 860 nationalen Radiostationen verwendet.\n\nDer Vertreter des Gesundheitsministeriums für die Provinz Manisa erarbeiteten 2007 einen Fahrplan für die Implementierung des e-Gesundheit-Projekts basierend auf Pardus. Durch das Projekt werden Patienten, Ärzte und Apotheker eine zentrale Zugriffsdatei erhalten. Diese beinhaltet beispielsweise die Krankheitsvergangenheit des Patienten, aber auch die Möglichkeit, die Medikamentenrezepte für den Patienten online zu bezahlen.\nIn erster Etappe wurden 360 Rechner an zwei Zentralservern angeschlossen und bisher die Daten von einer Million Patienten erfasst. In der nächsten Etappe werden die Provinzen Antalya, Bursa, Hatay und Eskişehir ans e-Gesundheitssystem angeschlossen.\n\nPardus 2007 wurde bisher (Stand: 1. Februar 2007) 120.000-mal von der Projekt-Homepage heruntergeladen. Dies entspricht einer Zunahme von 300 % gegenüber der Version 1.0. Rechnet man den Datenverkehr von Universitäten wie der Bilgi-Universität Istanbul und anderen hinzu, die Pardus ebenfalls hosten, wurde die Version Pardus 2007 über 200.000-mal heruntergeladen. Pardus 2007 wurde zudem an Computer-Zeitschriften, die sich an der Verbreitungskampagne beteiligen in einer Auflage von 120.000 CDs verteilt. Die Vertretung der Kammer der Elektroingenieure in Ankara verteilt zusätzlich 10.000 Pardus-2007-CDs.\n\nPardus 1.0\nDie Erstversion, Pardus 1.0, wurde am 26. Dezember 2005 veröffentlicht, enthielt unter anderem den Linux-Kernel 2.6.14 sowie KDE 3.5 und unterstützte die englische und türkische Sprache. Die Pardus-Live-CD war das erste Produkt des Pardus-Projekts.\n\nPardus 2007\nNachdem im Jahre 2006 mehrere Alpha-Versionen unter dem Versionsnamen Pardus 1.1 vorgestellt wurden, folgte am 18. Dezember 2006 die Veröffentlichung von Pardus 2007.\n\nPardus 2007.1\nAm 16. März 2007 wurde Pardus 2007.1 \"Felis chaus\" veröffentlicht. Mit Ausnahme der Major Releases (Pardus 2007, 2008, 2009 und 2011) erhalten seit Pardus 2007.1 sämtliche Versionen den Beinamen einer in Anatolien (ehemals) verbreiteten Tierart.\n\nPardus 2007.2\nPardus 2007.2 \"Caracal caracal\" wurde am 11. Juli 2007 veröffentlicht.\n\nPardus 2007.3\nDie Version Pardus 2007.3 \"Lynx lynx\" wurde am 19. November 2007 veröffentlicht und nutzte den Linux-Kernel 2.6.18, KDE 3.5.8, OpenOffice.org in der Version 2.3, Internet-Anwendungen (Browser, E-Mail, Instant Messaging etc.), Multimedia- und Grafikanwendungen (Video, Musik, Bilder etc.), Spiele und vieles mehr. Diese Version gab es als Installations-CD (\"Kurulan\") und als Live-CD (\"Calisan\").\n\nPardus 2008\nDie Veröffentlichung von Pardus 2008 verzögerte sich mehrmals und erfolgte schließlich am 27. Juni 2008. Auch in den folgenden Versionen wurden Verzögerungen des Rahmenterminplans bewusst in Kauf genommen, um in jeder Version die gestellten Entwicklungsziele einhalten und die Stabilität gewährleisten zu können.\n\nPardus 2008.1\nDie Version Pardus 2008.1 \"Hyaena hyaena\" wurde am 15. September 2008 veröffentlicht. In dieser Version kam der Linux-Kernel 2.6.25.16 mit erweiterter Hardware-Unterstützung zum Einsatz, mit Mozilla Firefox 3.0.1, KDE 3.5.10 und KDE 4.1.1 als Live-CD, OpenOffice.org in der Version 2.4.1, einem verbesserten Netzwerk-Manager mit Ad-hoc-Modus- und 802.1x-WLAN-Unterstützung.\n\nPardus 2008.2\nDie Version Pardus 2008.2 \"Canis aureus\" wurde am 30. Januar 2009 veröffentlicht. Trotz der verzögerten Fertigstellung (im neuen Kalenderjahr) entschied man sich, die ursprünglich geplante Bezeichnung 2008.2 beizubehalten. In dieser Version kommt der Linux-Kernel 2.6.25.20 mit erweiterter Hardware-Unterstützung zum Einsatz. Als Desktopumgebung dient wahlweise KDE 3.5.10 oder das wenige Tage zuvor erschienene KDE 4.2.0. Die Software wurde mit Mozilla Firefox 3.0.5, OpenOffice.org in der Version 2.4.1, einem weiterhin verbesserten Netzwerk-Manager und neuen Hardware-Treibern aktualisiert. Pardus 2008.2 gibt es in vier Versionen. Zwei Versionen, die sich für die direkte Installation eignen: eine englische und türkische mit zusätzlichen Entwicklerwerkzeugen und eine internationale, die alle 11 Sprachen unterstützt; dazu noch zwei Live-CDs, die entweder KDE 3 oder KDE 4 als Standardoberfläche anbieten.\n\nPardus 2009\nAm 17. Juli 2009 wurde Pardus 2009 mit dem Linux-Kernel 2.6.30.1 und KDE 4.2.4 veröffentlicht. Des Weiteren bringt diese Version den Mozilla Firefox 3.5.1, OpenOffice.org 3.1 und viele andere Aktualisierungen mit sich. Als Dateisystem kann jetzt auch ext4 verwendet werden. Wie schon in der Vorgängerversion, gibt es wieder eine türkische und eine internationale Version mit elf Sprachen.\n\nPardus 2009.1\nPardus 2009.1 \"Anthropoides virgo\" wurde am 15. Januar 2010 veröffentlicht.\n\nPardus 2009.2\nAm Freitag, den 4. Juni 2010, wurde Pardus 2009.2 \"Geronticus eremita\" veröffentlicht. Aktualisierte Pakete sind unter anderem KDE 4.4.4, Qt 4.6.2, Python 2.6.5, OpenOffice.org Office Suite 3.2.1.3, Firefox Web Browser 3.6.3, Thunderbird 3.0.4, Gimp 2.6.8 und Linux-Kernel 2.6.31.13.\n\nPardus 2011\nDiese Version wurde am 20. Januar 2011 veröffentlicht. Damit wurden erstmals auch 64-Bit-Prozessoren unterstützt.\n\nEbenfalls neu war die Unterstützung der russischen und ungarischen Sprache. Nicht mehr unterstützt werden die Sprachen katalanisch, polnisch und portugiesisch.\n\nZu den weiteren Neuerungen und vorinstallierten Standardanwendungen zählten u. a.: Linux-Kernel 2.6.37, KDE SC 4.5.5, Logical Volume Manager (LVM), GNOME NetworkManager 0.8.2, LibreOffice 3.3.0, Mozilla Firefox 4.0 Beta9, Clementine 0.6.\n\nMit dem Versionssprung von Pardus 2009.2 auf Pardus 2011 stimmten Veröffentlichungsdatum und Versionsbezeichnung wieder begrifflich überein.\n\nPardus 2011.1\nPardus 2011.1 \"Dama dama\" wurde am 12. Juli 2011 veröffentlicht.\nIn dieser Version wurden einige Bugs gefixt und zahlreiche Pakete aktualisiert bzw. ergänzt. Zu den Basiskomponenten gehören Linux-Kernel 2.6.37.6, KDE SC 4.6.5, Libre Office 3.4.1.3, Mozilla Firefox 5.0, Xorg 1.9.5, Gimp 2.6.11, Python 2.7.1, GCC 4.5.3 und Glibc 2.12.\n\nPardus 2011.2\nPardus 2011.2 \"Cervus elaphus\" wurde am 19. September 2011 vorgestellt und beinhaltet unter anderem sicherheitskritische Updates und weitere Aktualisierungen wie z. B.:\nNetworkManager 0.8.5.91, ModemManager 0.5, cups 1.4.8, LibreOffice 3.4.3.\n\nPardus 2011.2 wurde (wie auch schon Pardus 2011 und 2011.1) in vier Versionen angeboten:\nDie Live-DVDs enthalten keine Installationsroutine.\n\nPardus 2013\nDie erste Version, die auf den Paketquellen Debians basiert. TÜBİTAK-ULAKBİM kündigte an, dass Pardus zu Debian gezogen sei und nun somit auf Debian basiere.\n\nAm 27. Januar 2012 wurde in der offiziellen Mailingliste der Pardus Entwickler das End of life der Pardus 2011 Familie bekannt gegeben; von offizieller Seite erfolgen keine Updates mehr. Einzelne Softwarepakete für Pardus 2011.2 werden von freiwilligen Unterstützern, darunter auch die deutschsprachige Pardus Community PardusUser, aktualisiert.\n\nIm Januar 2012 wurde angekündigt, dass der Community-Zweig des türkischen Staats-Linux Pardus eingestellt wird, in Zukunft gibt es die Distribution nur noch in einer Version für Behörden und Unternehmen. Nachfolger der Pardus-Variante für Privatanwender ist die Fork Pisi Linux.\nDie sogenannte Corporate Version wurde speziell auf die Bedürfnisse von Unternehmen und öffentlichen Einrichtungen zugeschnitten. So handelt es sich um eine auf Stabilität und längerfristigen Support (auf 4 Jahre) ausgelegte Distribution, die jedoch im Vergleich zur „normalen“ Pardus-Version, die sich vor allem für den privaten Einsatz eignet, eine geringere Auswahl an Software, Bibliotheken und Einstellungsmöglichkeiten mitbringt.\n\nDie am 16. Februar 2011 veröffentlichte Version Pardus Corporate 2 enthält unter anderem Linux-Kernel 2.6.35.11, KDE 3.5.10, Mozilla Firefox 3.6.13, LibreOffice 3.3.1.1, Xorg 1.7.7, Gimp 2.6.11 und Python 2.6.5.\n\nAuch für Pardus Corporate 2 stehen jeweils ein 32- und 64-Bit-System als Live- und als Installations-Image zum Download zur Verfügung.\n\nIm Frühjahr 2011 wurden unter dem Namen \"Comak\" Pläne für weitere Versionen von Pardus Linux mit alternativen Desktop-Umgebungen bekannt; inzwischen wurden erste Beta-Versionen veröffentlicht. Unter dem Namen Pardus 2011 Nusrat steht ein Linux-System mit der freien Desktop-Umgebung Xfce (Version 4.8) bereit. Pardus 2011 Falco Peregrinus ist mit dem Fenstermanager Fluxbox ausgestattet. Seit April 2011 stehen auch Beta-Versionen mit den Desktop-Umgebungen Gnome und LXDE zur Verfügung.\n\nYALI (Yet Another Linux Installer) ist der Setup-Assistent von Pardus und führt den Benutzer mittels graphischer Oberfläche einfach und schnell durch die Installation. Hier werden die Basiskonfigurationen der Pakete vorgenommen.\n\nMudur ist das Pardus-eigene Init-System, welches die Root-Partition mountet, Module lädt und entsprechende Dienste initialisiert.\n\nÇOMAR (COnfiguration MAnageR) ist der Konfigurationsmanager von Pardus, mit welchem man notwendige Einstellungen unter anderem an der Hardware, dem Bootvorgang, den Netzwerken, den Benutzerkonten, den Zeit- und Ländereinstellungen und der Anzeige vornehmen kann, womit dann sicherstellt wird, dass das System ordnungsgemäß arbeitet.\n\n\"PiSi\" (Packages Installed Successfully as Intended, ) ist das Paketverwaltungssystem von Pardus. Es ist das Primärwerkzeug, um Anwendungen zu installieren, entfernen oder aktualisieren. PiSi speichert und verwaltet die Abhängigkeiten der verschiedenen Pakete, Bibliotheken und COMAR-Abläufe. Als Gründe für die Verwendung eines eigenständigen Paketverwaltungssystems werden von den Entwicklern die fehlende Einfachheit bestehender, ausgereifter Paketverwaltungssysteme, der große Aufwand, bestehende Systeme weiterzuentwickeln, sowie die angestrebte strikte Trennung von Paketspezifikationen und Installationsanweisungen aufgeführt.\n\nDie offiziellen Depots umfassten 2011 über 4.500 Pakete für Pardus 2011; in Community Depots stehen weitere PiSi-Pakete zur Verfügung. Paketwünsche können über das Pardus Bugtracking-System geäußert werden, PiSi bietet aber auch die Möglichkeit, Programme selbst zu kompilieren.\n\nEigenschaften von PiSi:\n\nPiSi selbst ist kommandozeilenbasiert (CLI), mit dem Paket-Manager steht auch eine entsprechende benutzerfreundliche, Qt-basierte grafische Benutzeroberfläche (GUI) zur Verfügung.\nEine umfassende, deutschsprachige Einführung zur Pardus-eigenen Paketverwaltung kann in einer Vorabversion bei der deutschen Pardus Community heruntergeladen werden.\n\nNach der Erstinstallation von Pardus wird der Anwender von Kaptan begrüßt und durch eine Konfiguration geführt, um erste personalisierte Einstellungen vorzunehmen und seine Vorlieben in der Bedienung (wie z. B. bevorzugte Repositories, Optik, Maus-Einstellungen und Klick-Eigenschaften) festzulegen.\n\nZemberek wurde in Pardus integriert, um dem Benutzer Zugang zu einer türkischen Rechtschreib- und Grammatikprüfung in ihrer Textverarbeitung (OpenOffice), Text- und Chat- (Kopete), Kalender- (Kontact) und E-Mail-Software (Kmail) zu ermöglichen.\n\n\n"}
{"id": "2767535", "url": "https://de.wikipedia.org/wiki?curid=2767535", "title": "Flamenco-Metronom", "text": "Flamenco-Metronom\n\nEin Flamenco-Metronom ist ein Hilfsmittel zum Studieren von Flamenco-Rhythmen. Es gibt verschiedene Ausführungen als freie oder proprietäre Software, neuerdings auch für Smartphone, oder als Hardware, die an einen Computer angeschlossen und programmiert werden kann. \n\nDie Audiovisualisierung von Rhythmen der verschiedenen Flamenco-Gattungen (Palos) anhand der Flamenco-Uhr (Reloj Flamenco) trägt dazu bei, die alternierenden Taktarten besser zu verstehen. Mit solchen Metronomen kann z. B. der Compás (Rhythmus-Einheit im Flamenco) auf der Gitarre oder mit den Palmas (Händeklatschen) geübt werden. Je nach Version hat ein solches Metronom weitere Möglichkeiten, wie z. B. stufenloses Verstellen des Tempos, Auswahl verschiedener Klänge für den Metronom-Klick, verschiedene Perkussionselemente (z. B. Palmas, Cajones, Djembés, Darbukas, Panderos, Tacones), unterschiedliche Lautstärke für die betonten und unbetonten Taktteile, Laden eigener Kompositionen oder ein Noten-Player mit zur Musik synchron laufendem Cursor (betrifft nur das Metronom von Gerhard Graf-Martinez), der verschiedene Flamenco-Gattungen (u. a. Bulerías, Soleares, Alegrías, Siguiriyas) in Ton und Animation (Noten und Tabulatur) zeigt.\n\n"}
{"id": "2772151", "url": "https://de.wikipedia.org/wiki?curid=2772151", "title": "Chris Colorado", "text": "Chris Colorado\n\nChris Colorado ist eine 26-teilige französische Animationsserie aus dem Jahr 2000. Erschaffen wurde sie von Thibaut Chatel, Franck Bertrand und Jacqueline Monsigny, der Soundtrack stammt von Fabrice Aboulker. Erstausstrahlung war am 18. Dezember 2000 auf Canal+ und France 3, in Deutschland lief die Serie zum ersten Mal 2001 auf ProSieben.\n\nFür die Produktion der futuristischen Serie wurde Computeranimation mit herkömmlicher Zeichentrickanimation gemischt. Obwohl sie hauptsächlich neben Kinderserien lief, wird sie häufig als Erwachsenenserie bezeichnet, nicht zuletzt aufgrund der apokalyptischen Endzeit-Handlung und der im hart-kantigen Stil gezeichneten Gesichter.\n\nChris Colorado spielt in einer düsteren, nicht allzu fernen Zukunft auf der Erde. Nachdem ein Softwareverarbeitungs-Fehler verhindert, dass die Menschheit vor einem auf die Erde stürzenden Meteoriten gewarnt wird, schlägt dieser mitten in einer der Welthauptstädte ein. Trotz eines Abwehrversuches, der zumindest die vollständige Vernichtung in Form einer frontalen Kollision des Meteoriten mit der Erde verhinderte, wurde durch den Aufprall fast die gesamte menschliche Zivilisation ausgelöscht. Nach einer Weile bildeten die Überlebenden eine Weltregierung, die während des Großteils der Serie unter Weltpräsidenten Richard Jullian steht. Regierungssitz ist die Mayastadt Chichen-Itza.\n\nNach einigen Jahrzehnten des Friedens begann der Diktator Thanatos, ein Mann der sein Gesicht stets unter einer Maske verbirgt, einen Krieg mit Hilfe einer Armee von zu Cyborgs konvertierten Menschen, infolgedessen er die Herrschaft über Europa erlangt. Eine Gruppe von Kämpfern unter der Führung von Richard Julian gelang es jedoch seine Streitmacht zu besiegen, woraufhin er sich in die vom Meteoriten zerstörte Welthauptstadt zurückzog. Dort gelang es ihm mit einer seltsamen Substanz, dem \"Schwarzen Fluss\", die aus den Überresten des Meteoriten austrat eine neue Armee aus den Überlebenden der Stadt aufzubauen. Die Story beginnt, als Weltoberhaupt Julian den ehemaligen Piloten und Kampfsportler Christopher Krantz, der zuweilen Chris Colorado genannt wird, rekrutiert und ihm verrät, dass er der Sohn von Wilhelm-Erwin Krantz ist. Dieser sollte einst Thanatos Armee infiltrieren, doch er lief offensichtlich zum Feind über und wurde von allen, auch seinem Sohn, als Verräter gebrandmarkt.\n\n"}
{"id": "2772564", "url": "https://de.wikipedia.org/wiki?curid=2772564", "title": "Scratch (Programmiersprache)", "text": "Scratch (Programmiersprache)\n\nScratch ist eine erstmals 2007 veröffentlichte erziehungsorientierte visuelle Programmiersprache für Kinder und Jugendliche inklusive ihrer Entwicklungsumgebung und der eng verknüpften Online-Community-Plattform.\n\nIhr Ziel ist es, Neueinsteiger − besonders Kinder und Jugendliche − mit den Grundkonzepten der Programmierung vertraut zu machen. Unter dem Motto \"imagine, program, share\" („Ausdenken, Entwickeln, Teilen“) wird die kreative und explorative Erstellung eigener Spiele und Multimedia-Anwendungen, verbunden mit dem gegenseitigen Austausch darüber, als Motivation genutzt. Kostenlos und werbefrei können die Ergebnisse in einer internationalen Online-Community mit dem Scratch-Player abgespielt, diskutiert und weiterentwickelt werden. Außerdem gibt es einige Beispiele, die Anregungen für Einsteiger schaffen und das Prinzip des Programmierens näher bringen.\n\nDer Name Scratch leitet sich von der DJ-Scratchtechnik ab. Die Gemeinsamkeit zum musikalischen Scratchen ist die leichte Wiederverwendbarkeit von Versatzstücken: In Scratch können alle interaktiven Objekte, Grafiken und Töne leicht von einem in ein anderes Scratch-Projekt übertragen und dort neu kombiniert werden (siehe: Remixingkultur). Dadurch erhalten Anfänger sehr schnell Erfolgserlebnisse, die motivieren, sich vertieft mit der Materie auseinanderzusetzen.\n\nDas erstmals 2007 veröffentlichte Scratch wird unter der Leitung des US-amerikanischen Professors für Lernforschung Mitchel Resnick von einem kleinen Forschungsteam, der Lifelong Kindergarten Group, am MIT Media Lab entwickelt und größtenteils von der National Science Foundation und freiwilligen Spenden finanziert.\n\nAls Schüler von Seymour Papert entwickelt Resnick, basierend auf konstruktionistischen Lerntheorien mithilfe digitaler Technologien, Lern- und Experimentierumgebungen, die die kreativen Potentiale von Kindern und Jugendlichen fördern.\n\nDie erste Implementierung der Scratch-Entwicklungsumgebung basierte auf Squeak (Scratch 1.0 im Jahr 2007 bis Scratch 1.4 im Jahr 2009), der Scratch-Web-Player zunächst auf Java, seit 2011 wahlweise auch auf Flash. Die vorige Version Scratch 2.0 erschien am 9. Mai 2013 und basiert komplett auf Flash.\n\nDas aktuelle Scratch 3.0 basiert auf JavaScript. Unterstützung für Touch-Bedienung gibt es für Mobile Chrome und Safari.\n\nDie Entwicklungsumgebung kann kostenlos aus dem Internet heruntergeladen werden. Da sie mit dem plattformunabhängigen Smalltalk-Entwicklungssystem Squeak erstellt ist, gibt es Versionen für Windows, macOS und potentiell für weitere Betriebssysteme. Die mit Scratch erstellten Programme können direkt aus der Entwicklungsumgebung in einen persönlichen Benutzerbereich der Scratch-Website hochgeladen werden. Dort können die Programme von anderen Mitgliedern der Scratch-Community genutzt, kommentiert und zum Lernen und zur Weiterentwicklung heruntergeladen werden. Zur Ausführung direkt im Webbrowser dient ein Java-Interpreter, der sogenannte Scratch-Player.\n\nDie Entwicklungsumgebung ist auf der Scratch-Website direkt lauffähig und wie der neue Scratch-Player komplett in Flash erstellt. Download und Installation sind daher nicht mehr erforderlich. Eine Offline-Version zum Herunterladen für Mac OS, Windows und einige Linux-Versionen (32 bit) ist ebenfalls verfügbar.\n\nDa Scratch-Web-Player auf Java oder Flash basieren, welche auf den mobilen Geräten von Apple (iPhone/iPad/iPod) nicht unterstützt werden, lassen sich Scratch-Projekte und die Scratch 2.0 Entwicklungsumgebung dort nicht ausführen. Eine eigenständige Scratch-Player-App für diese Geräte war bereits im App-Store gelistet, so dass man dort alle Scratch-Projekte ausführen konnte, wurde jedoch von Apple nach einigen Monaten wieder verbannt. Eine Alternative wird die Scratch-Modifikation BYOB 4.0/Snap! darstellen, deren Web-Player und Entwicklungsumgebung auf HTML5-Canvas und JavaScript basieren, die aber bisher erst im Betastadium und noch nicht voll kompatibel zu Scratch vorliegt.\n\nDas auf JavaScript basierende Scratch 3.0 kann mittels Safari auf dem iPad genutzt werden.\n\nWie oben beschrieben, gibt es Scratch Player für Squeak (Offline), Java und Flash (Online). Ein HTML5-Player (Online) für Scratch, mit dem obige Beschränkung für iPhone/iPad/iPod aufgehoben werden soll, befindet sich im Entwicklungsstadium und kann bereits öffentlich im Web getestet werden.\n\nSeit 2007 breitete sich die Scratch-Idee u. a. durch den Netzwerkeffekt rasch aus. So entstand eine weltweit wachsende Gemeinschaft von Programmieranfängern, Schülern, Studenten, Pädagogen und Hobbyisten, die sich gegenseitig motivieren und unterstützen. Daraus resultierten lokale Gruppen, Ereignisse, Kurse, Übersetzungen, Beispiele, Unterrichtsmaterialien und vieles mehr, um Programmieranfängern zu helfen, ihre Kreativität und Kenntnisse zu entwickeln. Befördert wurde dies durch die intuitive Bedienung und leichte Übersetzbarkeit, sowohl der Scratch-Entwicklungsumgebung als auch der Scratch-Website. Hinzu kommt eine erfolgreiche Kommunikation und Medienarbeit des MIT-Scratch-Teams (u. a. dem weltweiten Scratch Day, der Ausbilder-Plattform ScratchEd, dem Scratch Wiki, sowie vielen wissenschaftlichen Publikationen, Interviews und Beiträgen in verbreiteten US-Medien).\n\nDie Internationalität dieser Benutzergemeinde wird durch die leicht zugängliche Mehrsprachenfähigkeit aller Scratch-Plattformen gefördert, mit der die Online-Community die Entwicklungsumgebung, die Webplattform und auch die Programmierbefehls-Bausteine selbst in alle Sprachen übersetzt, womit Scratch u. a. auch komplett in Deutsch zur Verfügung steht. Auch in Ländern mit nicht auf lateinischen Buchstaben basierenden und von rechts nach links zu schreibenden Sprachen kommt Scratch zum Einsatz, denn bestehende Scratch-Projekte lassen sich per Knopfdruck auch in solche Sprachen umschalten. Bei der Übersetzung wird mit lokaler und/oder serverbasierter Pootle-Technologie gearbeitet.\n\nVon den Anfängen im März 2007 bis Ende 2012 legten mehr als 1.300.000 Anwender – mit einem Altersschwerpunkt zwischen 8 und 16 Jahren – ein Scratch-Onlinekonto an, von denen über 400.000 in dieser Zeit insgesamt mehr als 3.000.000 selbstentwickelte Scratch-Projekte ins Netz stellten und über 1.380.000 Posts schrieben (in 13 englischsprachigen und 17 internationalen Scratch-Community-Foren). Da mit Scratch auch ohne Benutzerkonto gearbeitet werden kann und die aktuelle Scratch-Entwicklungsumgebung in vielen Unterrichtsformen ohne Webeinsatz genutzt wird, beträgt die Anzahl der Scratch-Anwender wahrscheinlich ein Vielfaches der angemeldeten Benutzer.\n\nScratch wird weltweit in verschiedensten Schulformen und Unterrichtsveranstaltungen eingesetzt. Schulen in Deutschland, Schweiz, Österreich und Luxemburg nutzen Scratch zunehmend. Auch an Universitäten werden Scratch oder das auf Scratch basierende Snap! (BYOB) zur Einführung in die Programmierung verwendet, etwa an der Universität Berkeley für Nicht-Informatiker oder an der Universität Hildesheim für Studierende des Lehramts.\n\nNeben dem Einsatz in Bildungseinrichtungen wird die Verbreitung von vielen freien Organisationen vorangetrieben, die sich in Sammelbewegungen wie der deutschen Initiative Jeder kann programmieren oder der amerikanischen Code.org darstellen. So kommt Scratch auch außerhalb von Schulen in Hobby und Familie zum Einsatz. Viele Jugendliche entdecken Scratch selbstständig auf der Suche nach Möglichkeiten, wie sie selber leicht ein Computerspiel entwickeln und es anderen präsentieren können. Speziell Eltern mit IT-nahen Berufen nutzen Scratch häufig, um ihren Kindern schon in jungen Jahren einen motivierenden Einblick in die IT-Welt vermitteln zu können. Zunehmend entwickeln sich auch im deutschsprachigen Raum Clubs oder Vereine für MINT-begeisterte Jugendliche, oder auch zur Förderung von MINT-Erziehungs-benachteiligten Kindern aus bildungsfernen Haushalten (z. B. Computer-Clubhouse-Bewegung in den USA), die ebenfalls Scratch einsetzen. Neben solchen physischen Hobbygemeinschaften haben sich auch viele virtuelle Onlinegruppen von Hobbyisten gebildet, die gemeinsam Projekte realisieren und dazu regelmäßig online zusammenkommen, um sich über ihr Hobby auszutauschen.\n\nDer seit 2009 jährlich stattfindende Scratch Day trägt stark zur Verbreitung der Scratch-Idee bei (zuletzt am 14. Mai 2016.). Der vom MIT-Scratch-Team ausgerufene erste weltweite „Scratch Day“ am 16. Mai 2009 dokumentiert dies: Auf der dazugehörigen Website trugen sich innerhalb von wenigen Wochen weit über 100 regionale Organisationen, wie Schulen oder Computerclubs, ein, die eines der lokalen Scratch-Day-Ereignisse organisierten. Zum ersten deutschen Scratch Day 2009 kamen in Bochum Scratch-Experten aus ganz Deutschland zusammen, um Scratch für Kinder, Eltern, Lehrer und Wissenschaftler unter verschiedensten Aspekten vorzustellen.\n\nScratch@MIT ist eine Konferenz für Lehrer, Professoren, Forscher, Entwickler und alle, die sich mit Scratch oder dem Unterrichten von Kindern beschäftigen. Bei der Konferenz können Erfahrungen und Geschichten ausgetauscht, Workshops besucht und Vorträge angehört werden. Die Konferenz findet alle zwei Jahre (2008, 2010, 2012, 2014) am MIT Media Lab in Cambridge, Massachusetts, USA, statt und folgt jeweils einem ausgewählten Motto. Die Scratch-Konferenz am MIT bildet ein Äquivalent zur neu eingeführten europäischen Scratch-Konferenz (2013), die immer in den Jahren zwischen den Scratch@MIT-Konferenzen stattfindet. Die Scratch@MIT-Konferenz 2014 fand vom 6. bis zum 9. Juli unter dem Motto „Projects, Peers, Passion, Play“ statt.\n\nIn den Jahren 2008, 2010 und 2012 konnten Interessierte an der Scratch-Konferenz im MIT Media Lab in Cambridge, Massachusetts, teilnehmen. Mitglieder der europäischen Scratcher-Gemeinschaft fanden, dass es auch für Scratcher, die nicht die Möglichkeit haben, in die USA zu reisen, eine Gelegenheit zum Treffen mit anderen Scratchern geben sollte. Scratch Connecting Worlds 2013 in Barcelona war die erste europäische Scratch-Konferenz. Sie fand von 25. bis 27. Juli in von der Citilab Barcelona zur Verfügung gestellten Räumlichkeiten statt. Erzieher, Forscher, Entwickler und andere Mitglieder der weltweiten Scratch-Gemeinschaft nutzen die Gelegenheit um die kreativen Möglichkeiten von Scratch zu feiern und zu teilen.\n\nDie insgesamt siebente internationale Scratch-Konferenz hat den Titel \"Scratch2015AMS – Creative communities\" und findet in Amsterdam vom 12. bis 15. August 2015 statt.\n\nScratchEd ist eine zusätzliche Online-Community für Lehrer und andere Ausbilder, die mit Scratch arbeiten. Dort haben die Mitglieder einige hundert Materialien, Quellen und Tipps für die Bildungsarbeit mit Scratch eingestellt. Außerdem können sich die Anwender über die Website austauschen und miteinander in Kontakt treten. Die Hauptsprache ist Englisch aber auch in vielen anderen Sprachen, u. A. Deutsch, wurden bereits für den Unterricht nützliche Materialien hinterlegt. Das Angebot umfasst Anfang 2013 u. a. 193 Storys (d. h. Berichte über Veranstaltungen) und einige hundert Ressourcen, deren Anwendungsbereich vom Kindergarten bis zur Hochschule und über alle Fachbereiche geht.\n\nDas Scratch-Wiki ist ein freies, gemeinschaftlich geschriebenes Wiki auf MediaWiki-Basis, das Informationen über die Programmiersprache Scratch, deren Websites, Geschichte und begleitende Phänomene enthält. Das Wiki wird vom Scratch-Team unterstützt, ist aber im Wesentlichen von Scratchern geschrieben. Das Scratch-Wiki ist eine beliebte Quelle für Informationen über Skripte und Anleitungen die ständig weiter wächst, weil es Scratcher als ihre erste Informationsquelle nutzen. Das erste internationale Schwester-Projekt ist das ebenfalls vom Scratch-Team unterstützte DACH-Scratch-Wiki in deutscher Sprache. Das englischsprachige Scratch-Wiki wurde am 6. Dezember 2008 gegründet und enthält >900 Artikel, das deutschsprachige Scratch-Wiki wurde am 8. Februar 2012 gegründet und enthält über 450 Artikel.\nMittlerweile existieren Wikis auf französisch, niederländisch, russisch, ungarisch, japanisch und indonesisch.\n\nBei Sprachumfang und Bedienung wurde hohe Priorität auf den intuitiven Zugang und auf die Vermeidung von hoher Komplexität oder Abstraktion gelegt. Mächtigen Funktionen wie Multimediaelementen und Multitasking stehen starke Einschränkungen in Peripherienutzung und Sprachumfang gegenüber. Alle Elemente, die den intuitiven Einstieg behindern könnten, wurden in empirischen Studien identifiziert und eliminiert. Alles, was den Einsteiger motiviert, sein Verständnis erleichtert und zur explorativen Entwicklung ermutigt, wurde betont. Spielerisches Lernen steht im Vordergrund. So können bereits Grundschulkinder erste Anwendungen erstellen und Jugendliche schnell hochmotivierende Ergebnisse erhalten. Aber selbst erfahrene erwachsene Programmierer haben sich der Scratch-Community angeschlossen. Sie reagieren – trotz der starken Einschränkungen – positiv darauf, wie schnell und intuitiv Ergebnisse zu erreichen sind, und betrachten die Beschränkung als Herausforderung ihrer Programmierkünste.\n\nScratch unterstützt folgende Programmierparadigmen bzw. Eigenschaften, die einem solchen ähnlich sind:\n\n\nScratch ist so konzipiert, dass bereits junge Kinder und Erwachsene ohne Computerkenntnisse mit Scratch motivierende Programmiererfahrungen machen können. Da es auf einer nahezu rein grafisch gehaltenen Oberfläche basiert, werden potentielle Frustfaktoren wie viel Tipparbeit, komplizierte Sprachsyntax und diesbezügliche Fehler ausgeschlossen.\n\nDie Erstellung von Scratch-Projekten soll sich beim Zusehen und Ausprobieren leichter erschließen lassen, als durch eine schriftliche Anleitung, da neben Textmetaphern (wie Bühne, Spielfigur, Kostüm, Baustein) viele intuitiv verständliche Form- und Farbordnungen verwendet werden, die von Anfängern einfach benutzt werden können, ohne dass darüber bewusst nachgedacht werden muss. Da die Funktion vieler Elemente intuitiv klar wird, spielen ihre genauen Bezeichnungen beim Verständnis keine wesentliche Rolle, zumal diese Bezeichnungen – durch die beliebig austauschbaren Sprachdateien – willkürlich sind.\n\nDie Scratch-Entwicklungsumgebung bearbeitet immer jeweils ein Scratch-Projekt, das aus mehreren Multimedia-Elementen (Bilddateien, Klangdateien) und Programmen besteht. Ein Scratch-Projekt wird als eine einzige Datei abgespeichert oder eingelesen, die alle Multimedia-Elemente, Programme und Startwerte enthält (in Scratch 1.4). Außerdem kann das Projekt per Klick auf der Scratch-Website veröffentlicht werden (mitteilen/to share) oder von dort in die Entwicklungsumgebung geholt werden. Ab Scratch 2.0 ist keine lokale Installation mehr notwendig, auch die Entwicklungsumgebung ist online.\n\nIn einem Scratch-Projekt agieren beliebig viele Spielfiguren (Objekte/Sprites) auf einer Bühne (Hintergrund/Stage). Die Spielfiguren haben Ähnlichkeit mit der Schildkröte in der Programmierlernsprache Logo, jedoch wesentlich mehr Fähigkeiten und Eigenschaften. Spielfiguren und Bühne können wechselnde Kostüme (Grafiken) anziehen und Geräusche, Schlagzeugklänge oder Noten abspielen. Die Spielfiguren können sich auf der Bühne bewegen, drehen, ihre Größe ändern, sich – beeinflusst von verschiedenen grafischen Effekten – zeigen oder verstecken, Malspuren und Stempel-Abdrücke hinterlassen, sprechen und denken (Sprech-/Denkblasen).\n\nDie Spielfiguren haben Werte (X, Y, Ausrichtung, Größe, Lautstärke, Tempo, Stifteinstellungen, …) und zusätzlich erstellbare lokale Variablen und Listen, die bei Bedarf auf der Bühne dargestellt werden können. Neben diesen lokalen können auch globale Variablen und Listen angelegt werden, die in allen der Spielfiguren und der Bühne gelten. Die Werte und Variablenbelegungen im Zeitpunkt des Abspeichern oder Veröffentlichen eines Programms sind Teil desselben und definieren den Startzustand.\n\nIm Abspielmodus der Entwicklungsumgebung und im Internet-Player sieht man vom fertigen Scratch-Projekt nur noch die Bühne. Im Programmiermodus der Entwicklungsumgebung kann das Projekt ebenfalls uneingeschränkt abgespielt werden, jedoch kann dabei jederzeit ändernd eingegriffen werden und die Bühne belegt nur einen Teil des Scratch-Fensters, das dann in vier Bereiche unterteilt ist:\n\nDie Bühne (links oben, vor Version 2 rechts oben), die Spielfiguren-Auswahl (links unten, vor Version 2 rechts unten) und die in acht farbige Fächer eingeteilte Bausteinkiste (rechts, vor Version 2 links), sind drei konstante Bereiche, während sich der vierte Programme/Kostüme/Klänge-Bereich anhand der Spielfiguren-Auswahl ändert. Dieser mittlere Bereich zeigt stets die Programme, Kostüme, Klänge und Werte der ausgewählten Spielfigur oder der ebenfalls auswählbaren Bühne. Im Programme/Kostüme/Klänge-Bereich kann mittels Laschen zwischen den Elementen der gewählten Spielfigur bzw. der Bühne geblättert werden.\n\nIn die Lasche „Programme“ werden die Programmier-Bausteine der Farbe des aktuell gewählten Baustein-Faches gezogen und dort zu Programmen zusammengebaut, die dann frei nebeneinander auf der Fläche liegen. Bausteine, die nicht gebraucht werden, können ohne Funktion liegenbleiben, oder durch Herausziehen weggeräumt werden.\n\nViele Bausteine haben Bedienelemente, bzw. Öffnungen, in die andere Bausteine, Listenauswahlen, Zahlen, Buchstaben oder Farben (mit Bildschirmpipette auswählbar) eingesetzt werden. Die Form dieser Bedienelemente zeigt ihre Verwendung an, bzw. deutet darauf hin, welche Bausteine zusammen passen. Nur was passt, lässt sich „einklinken“, nur existierende Elemente können in den Listen gewählt werden. Dadurch werden viele Fehler automatisch vermieden. Die Farben der Bausteine entsprechen den „Farb-Fächern“ der Bausteinkiste, die ihren Einsatzbereich symbolisieren.\n\nNeben den oben beschrieben Fensterbereichen der Scratch-Entwicklungsumgebung gibt es noch einige Knöpfe, Menüs und Kontextmenüs (rechte Maustaste) und damit verbundene Auswahlboxen und Editoren zum Laden, Speichern, Kopieren, Erstellen und Beeinflussen von Spielfiguren, Medien und Projekten. Die wichtigsten sind die grüne Startflagge und das rote Stoppschild, mit denen alle Programme eines Projektes gestartet und gestoppt werden.\n\nProgramme beginnen immer mit einem der vier Kopfbausteine aus dem gelben Fach Steuerung, der angibt, durch welches Ereignis sie gestartet werden (Klick die Start-Flagge, Mausklick auf ihre Spielfigur, Druck einer bestimmten Taste oder Empfang einer bestimmten Nachricht). Indem ein Programm eine \"Nachricht an alle\" sendet, kann es andere Programme starten, die auf diese bestimmte Nachricht warten. Multitasking und Ereignissteuerung sind so selbstverständliche Konzepte, die vom Programmier-Anfänger nebenbei erfasst werden.\n\nLaufzeit und Programmierzeit gehen fließend ineinander über: Während ein Projekt mit seinen Programmen läuft, kann weiterprogrammiert werden, bzw. wenn es nicht läuft, können Programme durch Anklicken gestartet und so ausprobiert werden, womit die erforschende spielerische Herangehensweise gefördert wird. Laufende Programme sind mit weißem Rand hervorgehoben und Ausführungsfehler (zum Beispiel Division durch 0) mit rotem Rand. Im Einzelschrittmodus werden auch die jeweils ausgeführten Bausteine hellgelb hervorgehoben.\n\nProgramme, Grafiken und Töne können durch Ziehen auf andere Spielfiguren übertragen werden und Spielfiguren und Programme können per Knopf oder Kontextmenü kopiert werden. So kann zum Beispiel nur durch das Erstellen einer einzigen Spielfigur, die mehrfach kopiert wird, eine komplexe Simulation geschaffen werden, bei deren Ausführung die Kopien deterministisch aber dennoch unvorhersehbar interagieren.\n\nEin wesentlicher (namensgebender, s. o.) Aspekt von Scratch ist es, dass sich die Nutzer von vorhandenem Medien-Material inspirieren lassen, daraus lernen, es neu kombinieren, verändern und ergänzen, um eigene Werke daraus zu schaffen. Diese werden dann wiederum anderen zur Verfügung gestellt. Die Installation bringt dafür bereits einen großen Fundus an Beispielprojekten, Spielfiguren, Grafiken und Sounds mit, der für erste Einsteigererfahrungen umfangreiches Ausgangsmaterial liefert. Dieses wird erweitert durch selbstgemachte oder aufgezeichnete Bild- und Tondateien, sowie den Multimediaelementen oder ganzen Spielfiguren und Programmen, die aus bereits veröffentlichten Scratch-Projekten anderer entnommen werden können. Das Wiederverwenden der Spielfiguren in anderen Projekten wird gefördert, indem sie komplett mit allen ihren Kostümen, Klängen und Programmen aus Projekten als Datei exportiert und in andere importiert werden können, was häufig ohne größere Anpassungen zu lauffähigen Kombinationsprojekten führt. Das Herunterladen fremder Projekte, anschließendes Ändern und Erweitern und Hochladen als eigenes Projekt wird in Scratch als Remixing bezeichnet und ist eine beliebte und positiv bewertete Vorgehensweise. Remixing wird daher nicht als Plagiat oder Ideendiebstahl gesehen, sondern als Gemeinschaftserfahrung und als eine Ehre für denjenigen, dessen Material verwendet wird. Remixing wird unterstützt, indem ein Projekt seine Herkunft mit sich trägt und der Weg des Remix auf der Scratch-Website bis zum Ursprungsautor rückverfolgbar ist. Auf der Scratch-Website entstehen so durch eine spezielle Visualisierungsseite Remixing-Ketten und -Bäume, die durchaus hunderte von Projekten umfassen können.\n\nNeben der offiziellen Weiterentwicklung durch das Scratch-Team, deren größter bisheriger Sprung die komplette Neuimplementierung mit Scratch 2.0 als Flash-Anwendung darstellte, existieren eine Vielzahl von Scratch Modifikationen. Die inoffiziellen Erweiterungen wurden von Mitgliedern der Community (meist basierend auf dem offenen Squeak-Smalltalk-Quellcode von Scratch 1.4) erstellt.\n\nSie umfassen sowohl lehrorientierte als auch rein anwendungsorientierte Neuerungen von extrem unterschiedlicher Qualität und Ernsthaftigkeit. Einige offizielle Scratch-Weiterentwicklungen lassen sich auch auf Einflüsse aus \"Modifikationen\" zurückführen. So wurden beispielsweise die sogenannten \"Listen\" (eindimensionale Felder) von Chirp/BYOB übernommen, das sich inzwischen von einer Modifikation zu einer eigenständigen Programmiersprache mit von Scratch-unabhängiger Technologiebasis weiterentwickelt hat.\n\n\"Scratch for Arduino\" (S4A) ist eine Scratch-Modifikation, die eine Programmierumgebung im Scratch-Design für den Arduino-Mikrocontroller zur Verfügung stellt.\n\nPanther ist eine beliebte Modifikation der Scratch Entwicklungsumgebung. Panther arbeitet mit Blöcken, hat zwei zusätzliche Kategorien und kann Projekte in ausführbare EXE-Dateien umwandeln. Die Panther-Website ermöglicht das Hochladen von Projekten.\n\nMit ScratchJr erschien am 20. Juli 2014 eine einführende visuelle Programmiersprache mit dem Ziel, jüngeren Kindern ab 5 Jahren mithilfe eines auf Scratch basierenden Systems einfach und spielerisch eigene interaktive Geschichten und Spiele zu programmieren. Die Kinder können grafische Programmblöcke aneinander fügen und so Figuren bewegen lassen. In einem Editor lassen sich eigene Figuren erstellen und anpassen oder mit Stimmen und Geräuschen versehen, die in die Programmblöcke eingebunden werden können. Entwickelt wurde diese App von der Tufts University in Zusammenarbeit mit dem MIT Media Lab und Zuschüssen der National Science Foundation. ScratchJr ist als kostenlose App für Android, Apple iOS und Chromebook verfügbar.\n\n\"Snap!\", bis zu Version 3.1.1 unter dem Namen \"BYOB\" bekannt (engl. Abk. von \"Build Your Own Blocks\" deutsch: „Bau deine eigenen Blöcke“), ist eine auf Scratch aufbauende und von Scheme und damit auch Lisp inspirierte erziehungsorientierte visuelle Programmiersprache inklusive ihrer Entwicklungsumgebung für fortgeschrittene Schüler und für die Erwachsenenbildung.\n\nMicrosoft MakeCode ist eine grafische Oberfläche für das Microsoft Programming Experience Toolkit (PXT), mit dem Entwicklungsumgebungen für erziehungsorientierte Einplatinencomputer (z. B. , Calliope mini) erstellt werden können. Diese Entwicklungsumgebungen sehen aus wie Scratch, laufen allerdings als JavaScript-Anwendung im Browser. Es kann darin außer mit den Blöcken auch mit JavaScript selbst programmiert werden.\n\nmBlock von Makeblock basiert auf Scratch und erlaubt es, Roboter (mBot, mBot Ranger) einfach grafisch zu programmieren.\n\nNEPO ist eine an Scratch angelehnte visuelle Programmiersprache, welche die freiverfügbare Blockly-Bibliothek nutzt.\n\n\n\n\n\n"}
{"id": "2777839", "url": "https://de.wikipedia.org/wiki?curid=2777839", "title": "Pocket Internet Explorer", "text": "Pocket Internet Explorer\n\nDer Pocket Internet Explorer (PIE) ist ein mit Windows Mobile (auf Windows CE basierend) mitgelieferter Webbrowser, der die Darstellung von Webseiten auf PDAs und Smartphones mit diesem Betriebssystem ermöglichte.\n\nDer PIE wurde unabhängig vom Internet Explorer von Microsoft entworfen und hat deutlich weniger Funktionen als sein großer Bruder.\n\n"}
{"id": "2779435", "url": "https://de.wikipedia.org/wiki?curid=2779435", "title": "Microsoft Dynamics GP", "text": "Microsoft Dynamics GP\n\nMicrosoft Dynamics GP ist ein ERP-System von Microsoft, basierend auf Microsoft SQL Server und ist für den Einsatz im Mittelstand sowie für Unternehmen mit mehreren Standorten bzw. mit internationaler Ausrichtung vorgesehen.\n\nDas Produkt wurde ursprünglich von Great Plains Software entwickelt und Ende 2000 von Microsoft übernommen.\n\nMicrosoft Dynamics GP ist nur im englischsprachigen, kanadischen und lateinamerikanischen Markt verfügbar. Dynamics GP wird in Deutschland nicht vermarktet, doch häufig sind aufgrund von Konzernstrukturen entsprechende Installationen auch in Europa zu betreuen.\n"}
{"id": "2784253", "url": "https://de.wikipedia.org/wiki?curid=2784253", "title": "VideoSuite", "text": "VideoSuite\n\nMovavi VideoSuite ist Programm zur Bearbeitung von digitalem Video der Firma Movavi Ltd. mit einer einfachen Bedienung und breiten Akzeptanz von Videoformaten.\n\nMovavi VideoSuite besteht aus sechs verschiedenen Programmteilen:\n\n\n"}
{"id": "2785903", "url": "https://de.wikipedia.org/wiki?curid=2785903", "title": "Liste von Apple-Software", "text": "Liste von Apple-Software\n\nDiese Liste beinhaltet Software, die vom US-amerikanischen Unternehmen Apple entwickelt wurde.\n"}
{"id": "2788572", "url": "https://de.wikipedia.org/wiki?curid=2788572", "title": "Cyber-Mobbing", "text": "Cyber-Mobbing\n\nMit den aus dem Englischen kommenden Begriffen Cyber-Mobbing, auch Internet-Mobbing, Cyber-Bullying sowie Cyber-Stalking werden verschiedene Formen der Verleumdung, Belästigung, Bedrängung und Nötigung anderer Menschen oder Unternehmen mit Hilfe elektronischer Kommunikationsmittel über das Internet, in Chatrooms, beim Instant Messaging und/oder auch mittels Mobiltelefonen bezeichnet. Dazu gehört auch der Diebstahl von (virtuellen) Identitäten, um in fremden Namen Beleidigungen auszustoßen oder Geschäfte zu tätigen usw. Cybermobbing gehört zu einer der zentralen Gefahren im Umgang mit Internet und neuen Medien.\n\nDer Ausdruck „Cyber-Bullying“ lässt sich nicht eindeutig definieren, aufgrund der unterschiedlichen Übersetzungen des Begriffes in verschiedenen Ländern. Die Studie von Nocentini et. al (2010) beschäftigte sich mit den verschiedenen Auffassungen des Begriffes. Die Ergebnisse zeigten, dass in Deutschland die Bezeichnung „Cyber-Mobbing“, in Italien „Virtual“ oder „Cyber-Bullying“ und in Spanien „harrassment via Internet or mobile phone“ verwendet wird.\n\nEine repräsentative Studie der Universität Münster zusammen mit der Techniker Krankenkasse kam 2011 zu dem Ergebnis, dass mittlerweile 32 % (in NRW sogar 36 %) der Jugendlichen und jungen Erwachsenen als Opfer von Cybermobbing betroffen sind. 21 % der Befragten konnten sich vorstellen, als Täter im Internet aufzutreten.\n\nOpfer werden durch Bloßstellung im Internet, permanente Belästigung, durch entwürdigende Bilder oder durch Verbreitung falscher Behauptungen gemobbt. Die Täter werden in diesem Zusammenhang auch als \"Bullies\" bezeichnet.\n\nDie Motive sind sehr vielschichtig: Außenseiter werden beispielsweise in Chatrooms schikaniert; man versucht, Konkurrenz klein zu halten oder Freunden zu imponieren; unter Umständen werden Mobbingopfer zu Tätern: Sie wehren oder rächen sich.\n\nZunächst gewann das Phänomen vor allem im Zusammenhang mit Schülern, die Videos oder Bilder von Lehrern bearbeiteten und anschließend ins Internet gestellt haben, an Bedeutung.\n\nMittlerweile ist Internet-Mobbing unter Schülern verbreitet und erfolgt per Handy, Chat, sozialen Netzwerken wie VZnet Netzwerke oder Videoportale wie YouTube oder eigens erstellten Internetseiten; 2010 berichten allgemein bereits 25 % der Nutzer eines sozialen Netzwerks von Beleidigungen und Bedrohungen. Einzelne Untersuchungen zeigen, dass in Deutschland mittlerweile über ein Drittel der (Oberstufen-)Schüler innerhalb eines Zeitraumes von zwei Monaten mindestens einmal als Opfer von Cyber-Mobbing betroffen sind und über die Hälfte der Schüler als Täter aktiv werden, beides mit steigender Tendenz.\n\nDie Grenzen sind fließend: Die Hemmschwelle, im Internet andere auszulachen oder zu verhöhnen, ist gering. In der Anonymität des World Wide Web muss ein Täter seinem Opfer nicht in die Augen blicken, eine unmittelbare Rückmeldung für das eigene Verhalten bleibt (zunächst) aus und in der Folge auch das Bewusstsein und Empfinden für das mögliche Ausmaß und die Qualität der Verletzung der Betroffenen. Es ist einfach, Unwahrheiten zu äußern oder herumzuschimpfen. Dieser Effekt wird auch als \"Online Disinhibition Effect\" (dt. \"Online-Enthemmungseffekt\") bezeichnet: Es fällt Menschen, insbesondere Jugendlichen, schwerer, ihre Impulse zu zügeln, wenn soziale Kontrolle wegfällt oder nicht spürbar ist.\n\nIm Jahr 2008 erschien das Buch \"Generation Internet\" von John Palfrey und Urs Gasser, zwei Rechtsprofessoren aus den USA und der Schweiz. Sie stufen das Thema \"Cyberbullying\" als eines der größten Risiken ein, welche den \"Digital Native\" bedrohen, den in die Internetwelt hinein geborenen Jugendlichen. Mädchen werden hierin als besonders Betroffene bezeichnet.\n\nEine Frage, die in Bezug auf Cyber-Mobbing aufkommt, ist, in welchem Ausmaß es sich vom traditionellen Mobbing unterscheidet oder welche Gemeinsamkeiten die beiden Formen aufweisen.\n\nBeide Formen sind Akte, die Aggression seitens des Täters ausdrücken. Des Weiteren treten sowohl Cyberbullying als auch traditionelles Mobbing zwischen Personen auf, bei denen ein Machtungleichgewicht besteht und das Mobbing findet nicht nur einmalig statt, sondern wird vom Täter wiederholend ausgeführt.\n\nAnonymität: Die Täter können sich hinter einem Computer unter einem Pseudonym verstecken und somit anonym das Opfer mobben. Dadurch sind sie hemmungsloser und trauen sich auch bestimmte Sachen zu sagen, die sie nie bei der face-to-face Kommunikation sagen oder tun würden. Täter sehen auch nicht die negativen Folgen ihrer Handlungen auf das Opfer und fühlen somit weniger Empathie.\n\nGrößere Reichweite: Der Unterschied zum traditionellen Mobbing liegt darin, dass man mit Cyber-Mobbing ein sehr viel größeres Publikum erreicht. Traditionelles Mobbing ist oft an bestimmte Orte gebunden, wie beispielsweise Schulen und die Angriffe sind daher meist nur für eine begrenzte Anzahl an Personen sichtbar. Dahingegen ist bei Cyber-Mobbing, zum Beispiel in Form eines Hasskommentars unter einem Bild, der Angriff für alle Nutzer dieser Plattform sichtbar.\n\nLängerer Zeitraum: Cyber-Mobbing kann sich über einen viel längeren Zeitraum hinwegsetzen. Dadurch, dass für das Mobbing kein direktes Aufeinandertreffen von Opfer und Täter nötig ist, ist der Täter an keinen bestimmten Zeitraum oder Ort gebunden und kann daher das Opfer 24h angreifen. Zudem sind die Posts und Kommentare im Internet immer abrufbar, daher ist nicht nur der mögliche Mobbing-Zeitraum länger, sondern auch die Dauer eines Angriffs ist länger als beim traditionellen Mobbing.\n\nZwar kommt es in beiden Fällen selten zu einer Anzeige seitens des Opfers, jedoch sind die Gründe hierfür unterschiedlich. Beim traditionellen Mobbing haben die Opfer Angst vor einer Vergeltung des Täters. Der Grund bei Cyber-Mobbing liegt darin, dass die Betroffenen Angst haben, dass ihre Eltern die technischen Geräte zum Schutz ihres Kindes von ihnen wegnehmen.\n\nKinder, die im virtuellen Medium gemobbt werden, waren oft bereits vorher im wirklichen Leben ein Angriffsziel von Mobbing. Besondere Angriffsflächen bieten dabei Kinder und Jugendliche, die bereits wegen ihres Aussehens (zu dick - zu dünn etc.) stigmatisiert werden.\n\nDie meisten Patienten (Opfer und Täter) in der Kinder- Jugendpsychiatrie im Wilhelmstift in Hamburg sind Schüler zwischen 11 und 16 – einem besonders schwierigen Entwicklungsalter (Pubertät) mit einer hohen Empfindlichkeit für das mögliche Erleiden und auch Zufügen von Verletzungen.\nBetroffene finden unter Umständen keine angemessene Hilfe bei Eltern oder Lehrern, da diesen die Problematik teilweise unbekannt ist bzw. sie die Anzeichen nicht (er)kennen und wahrnehmen.\n\nTäter sind mit einem etwa gleichen Anteil Jungen und Mädchen. Häufig überschneidet sich das Täterprofil in einigen Aspekten mit dem des Opfers, wozu vor allem Depression, hohe Internetnutzung und ein geringes Selbstbewusstsein zählen. Die Täter sind meist hyperaktiv, narzisstisch und gewaltbereit. Sie haben oftmals einen großen Freundeskreis, jedoch weisen sie eher ein geringes Empathievermögen auf, was sich auch in einer Studie von 2008 zeigt. In dieser gaben 16 % der Befragten an, selbst schon einmal im Internet gemobbt zu haben – 40 % von ihnen empfanden dies wie einen Streich.\nWas als Scherz empfunden wird, kann dramatische Folgen nach sich ziehen, wie zum Beispiel soziale Isolierung, Stress, psychische Probleme oder Suizid:\nIm September 2009 hatte sich in Großbritannien ein junges Mädchen das Leben genommen, weil es online gemobbt wurde. Es war bereits der dritte Fall in England innerhalb von zwei Jahren.\n\nEin jüngstes Beispiel für Cyber-Mobbing ist der Fall Amanda Todd: Die 15-jährige Kanadierin hat sich im Oktober 2012 nach jahrelangem Cyber-Mobbing das Leben genommen: In der siebten Klasse begann sie im Internet, neue Kontakte zu Fremden zu knüpfen. Eines Tages bat sie ein \"Cam-Chat\"-(Kamera-Chat)-Partner, ihm vor der Kamera am PC ihre Brüste zu zeigen. In seiner jugendlichen Unbedarftheit folgte der Teenager dem Wunsch des Fremden, anschließend meldete sich dieser via Facebook wieder bei ihr und versuchte, sie mit den Nacktaufnahmen, die er per Screenshot vom \"Cam-Chat\" gemacht hatte, zu erpressen: Als die Jugendliche nicht darauf einging, verschickte der Mann die Bilder an ihre Freunde und Bekannten. Daraufhin fiel Amanda in eine Depression, ihr Umfeld distanzierte sich von ihr; Amanda wechselte mehrfach die Schule und beging nach einem ersten Selbstmord\"versuch\" schließlich tatsächlich Suizid.\n\nCyber-Mobbing kann in verschiedensten Formen auftreten:\nMobbing kann vollkommen unterschiedliche Opfer in den verschiedensten Lebensbereichen treffen. Cybermobbing ist nicht nur auf den Privatbereich beschränkt, sondern kann auch gegen Einzelpersonen im Berufsleben oder gegen Unternehmen gerichtet sein.\nBeim klassischen (realen) Schulmobbing wird das Opfer vor den Augen der ganzen Klasse verprügelt, beschimpft und ausgegrenzt. Hier besteht allerdings die Möglichkeit für einen Entspannungs-, Deeskalationsraum beispielsweise nach der Schule.\n\nIm Cyberspace mobben Kinder und Jugendliche anders, beispielsweise setzen sie hinter dem Rücken ihres Mitschülers anonym per Handy ein Gerücht in die Welt, Betroffene werden per Handykamera gefilmt, unter Umständen in auch aktiv herbeigeführten entwürdigenden, bloßstellenden oder gewalttätigen Situationen. Hier gibt es kein Entkommen, das Internet vergisst nichts, Einträge zu löschen, gestaltet sich außerordentlich schwierig und aufwändig.\n\nMittlerweile gibt es hierzu erste wissenschaftliche Untersuchungen. Dabei wurde festgestellt, dass in Deutschland derweil etwa jeder fünfte Jugendliche beteiligt ist, also entweder als Täter, als Opfer oder als sogenanntes Täteropfer, welches sowohl Täter als auch Opfer wird. Dies wird als ein relativ hoher Wert angesehen, deckt sich aber nach Aussagen der Wissenschaftlerinnen sowohl mit internationalen als auch mit anderen Befunden aus Deutschland.\n\nGemäß einer Umfrage des Bundesamtes für Sicherheit in der Informationstechnik (BSI), die unter privaten Internetnutzern durchgeführt wurde, können auch Erwachsene Opfer von Cybermobbing werden. 12 % der Internetnutzer, die sich in mindestens einem Sozialen Netzwerk engagierten, gaben Mobbing sowie sexuelle Belästigung bezüglich ihrer Person an. Überwiegend waren hiervon weibliche Personen in der Altersklasse von 14 bis 39 Jahren betroffen.\n\nAuf Bewertungsportalen wie Spickmich oder MeinProf können Schüler und Studenten anonym die Arbeit ihrer Lehrer und Professoren beurteilen. Die Meinungen zu diesen Foren sind geteilt. Während sie einerseits lediglich als Rückmeldemöglichkeit für Betroffene bezeichnet wird, fühlen sich andere durch die anonyme Kritik gemobbt. „Könnten Foren eine in Schulen oder Hochschulen fehlende Feedback-Kultur ausgleichen, wäre es nicht notwendig, dass sich kritische Schüler in der Verborgenheit des Internets verstecken und ein Ventil wie \"spickmich\" wäre überflüssig.“\n\nAuch Firmen können Opfer von Cyber-Mobbing-(Rufmord-)Attacken oder -Kampagnen werden. Vor- und Nachsorge können hier sogenannte Reputationsmanager treffen.\n\n\nWer Opfer von Cyberbullying wird, kann zunächst meist nur hilflos reagieren. Mittels Argumenten hat man gegen eine anonyme Gruppe keine Chancen. Mangelndes Selbstbewusstsein verschärft dabei die Situation unter Umständen. Als Außenseiter kann man wie im wirklichen Leben auch hier von der Internetcommunity nur schwerlich bzw. keinen Beistand erwarten: Steht erst einmal ein entwürdigendes Video im Netz, können es schnell Hunderte oder Tausende sehen und allzu schnell und einfach lässt sich ein bereits erfolgtes Stigma nicht wieder entfernen. Hinzu kommt die Ungewissheit der Urheberschaft.\n\nEltern müssen mit den Betroffenen intensiv die Situation erörtern und auf alle Fälle im entsprechenden Fall die Schule informieren.\nErwachsene können bei Cyber-Mobbing gegen Kinder und Jugendliche auch eingreifen, indem sie möglichst schnell die Polizei informieren. Diese kann die Täter unter Umständen identifizieren und eine Strafverfolgung einleiten; das ist allerdings dadurch erschwert, dass entsprechende Server oft im Ausland angemeldet sind und sich damit einer eventuellen Strafverfolgung entziehen. Zivilrechtliche Unterlassungsansprüche wegen Persönlichkeitsrechtsverletzung nach deutschem Recht gelten jedoch weltweit und entsprechende deutsche Gerichtsentscheidungen können auch im Ausland vollstreckt werden.\n\nBei jedem seriösen Netzwerkanbieter bzw. Seitenbetreiber besteht die Möglichkeit, beleidigende, unseriöse, unethische oder sonstwie auffallende Seiten, Profile oder Darstellungen zu melden und ihre Löschung zu beantragen.\n\nDie Verbesserung der Medienkompetenz und des Verständnisses von Eltern, Lehrern und Erziehern steht mit an erster Stelle der Vorsorge. Der gut gemeinte Ratschlag, Computer und/oder Handy einfach auszuschalten und auszulassen, greift in einer medialen und vernetzten Wirklichkeit zu kurz; zudem gelten diese Regeln – sofern sie je von den Verantwortlichen aufgestellt wurden – nur bis zum Unterrichtsende, so dass sich diejenigen Fälle, bei denen \"nach\" der Schule entwürdigende Szenen gegebenenfalls mitgefilmt werden, durch diesen Ratschlag weder beeinflussen noch ausschließen lassen.\n\nSchnelles Handeln und Prävention kann Mobbing im Netz vermindern oder im besten Fall sogar verhindern:\nDie Schulleitung reagierte sofort. Die beiden Klassenkameraden, die unter ihrem eigenen Namen gemobbt hatten, mussten die Schule verlassen. Patrick hat sich also nicht zum Opfer machen lassen und er empfiehlt auch anderen, keine Angst aufkommen zu lassen:\nViele Opfer wagen nicht, sich zu öffnen und andere zu informieren, weil sie fürchten, noch weiter ins (vermeintliche) soziale Abseits zu geraten.\n\nGenerell muss in der digitalen Wirklichkeit wie im analogen Leben das allgemeine Prinzip der Verantwortlichkeit gelten: Alle sind selbst für das verantwortlich, was sie sehen, tun (oder unterlassen), veröffentlichen usw. Für die praktische Umsetzung in Schulklassen gibt es mittlerweile eine Reihe qualitativ guter und evaluierter Arbeitshilfen sowie eine Reihe kostenloser Angebote im Internet.\n\nBisher gibt es noch sehr wenig Forschung zur Prävention von Cyber-Mobbing. Allerdings zeigen erste Studien, dass es möglich ist, das Risiko zu senken, Opfer zu werden. Allgemein anerkannt ist, bei der Bewegung im Internet nicht leichtfertig persönliche Daten und Darstellungen in schriftlicher und/oder bildlicher Form zu hinterlassen, um sich nicht in besonderer Weise angreif- und verletzbar zu machen. Ebenso ist das Mobben von anderen oder das Nutzen von Chaträumen mit extremen Inhalten ein Risikofaktor.\n\nDas Thema und der Erwerb von Medienkompetenz muss strukturell in der pädagogischen Ausbildung verankert werden.\n\nDie allgemeine Stärkung des Selbstbewusstseins von Kindern und Jugendlichen (engl. Empowerment, dt. etwa \"Bestärkung\") sowie die Schaffung eines Problembewusstseins bei den Tätern sowie die Sensibilisierung der Gesellschaft: Der respektvolle und sichere Umgang mit den Neuen Medien muss thematisiert und kann geübt und diskutiert werden, um Selbstachtung, Durchsetzungsvermögen, Eigen- und Mitverantwortlichkeit sowie das Entwickeln von Freundschaften zu unterstützen. Mittlerweile wird das Problem immer mehr erkannt, Fortbildungskonzepte werden entwickelt.\n\nAuch kleinere medienpädagogische Projekte können hier bereits einen Beitrag leisten.\n\nGrundsätzlich haben die Betreiber von sozialen Netzwerken im Internet ein starkes Interesse, Cyber-Mobbing einzudämmen, denn ihr Erfolg hängt entscheidend ab von ihrem guten Ruf und einem guten Klima in ihrer Community. Ihre Mitglieder müssen zum Teil persönliche Daten veröffentlichen, um an den Aktivitäten des Netzwerks teilnehmen zu können, und machen sich damit besonders angreifbar für Cyber-Mobbing. Daher wird mit Aufklärungsaktionen versucht, vor allem Jugendliche zu erreichen; die Aktionen sind häufig mit Werbung für das jeweilige Netzwerk verbunden.\n\nEin Jugendbeauftragter von SchülerVZ sah eine große Chance für mehr Sicherheit im Netz in der Zusammenarbeit zwischen Schülern und Lehrern:\nMittlerweile (Dezember 2011) haben einige große Social Network-Anbieter sogenannte Notfallbuttons auf ihren Seiten installiert, mit denen man unmittelbar etwaige Angriffe, Belästigungen, Beleidigungen, Verletzungen usw. direkt an Verantwortliche melden kann.\n\nSie können gemeinsam mit Eltern einen Verhaltenskodex entwickeln sowie \"Mobbingbeauftragte\" benennen, deren Aufgabenfeld sich auf das Cyber-Mobbing erstreckt. Das sogenannte Streitschlichter-Konzept bietet darüber hinaus auch hier Konfliktbearbeitungsmöglichkeiten.\n\nIn Hamburg wurde im Februar eine Initiative zur Förderung der Datenschutzkompetenz an Hamburger Schulen vorgestellt. Im Rahmen von Unterrichtseinheiten soll mit Schülern das Leben in der virtuellen Welt eingeübt werden: Der Hamburgische Beauftragte für Datenschutz und seine Behörde sind Initiatoren des Projektes:\nIn Deutschland wird der Begriff Cybermobbing (bisher, Dezember 2011) nicht als eigener Straftatbestand aufgeführt; unter Umständen könnte eine entsprechende Installation dazu führen, dass das Problem ernster genommen wird - analog der Entwicklung beim Stalking. Die (Straf-)Gesetzgebung wird ansonsten allgemein als ausreichend erachtet; Missouri hat als erster amerikanischer Bundesstaat eine entsprechende eigene Gesetzgebung erlassen.\n\nIn Großbritannien wurde bereits von staatlicher Seite gegen Cybermobbing vorgegangen. Dort wurden neue, spezielle Richtlinien für den Umgang mit dem Problem erlassen.\n\nGut ausgebildete und ständig betreute Social-Media-Kanäle zählen zu den besten Vorsorgemöglichkeiten gegen Cyber-Mobbing für Unternehmen. Kommt es zu Nutzerkritiken, kann die Kritik auf den eigenen Plattformen kontrolliert, überblickt und kommentiert werden. Vernachlässigen Unternehmen die Pflege der Online-Präsenzen, erhöht sich die Gefahr, dass sich die Kritik auf viele verschiedene Webseiten verlagert und so für Unternehmen zur Gefahr für die Reputation und das Tagesgeschäft wird.\n\nHier wird die Therapie an eventuell vorliegenden (narzisstischen) Persönlichkeitsstörung ansetzen (wie bereits oben erwähnt).\n\nCyber-Mobbing ist in Deutschland kein eigener Straftatbestand. Allerdings sind einzelne Formen von Cyber-Mobbing strafbar und können Gegenstand zivilrechtlicher Ansprüche sein (etwa Unterlassung und Schadenersatz). In Betracht kommen insbesondere Beleidigungsdelikte (Straftatbestände der §§ 185 ff. StGB), Delikte betreffend Verletzung des persönlichen Lebens- und Geheimbereichs (§§ 201 ff. StGB), Straftaten gegen die persönliche Freiheit (§§ 232 ff., insbesondere § 238 StGB (Nachstellung)), Verletzungen des Allgemeinen Persönlichkeitsrechts (Art. 1 Abs. 1 und Art. 2 Abs. 1 GG), des Rechts am eigenen Namen (§ 12 BGB), des Rechts am eigenen Bild (§ 22 ff. KUG) oder des wirtschaftlichen Rufs (§ 824 BGB). Im Fall von Mobbing im geschäftlichen Bereich kommen auch Ansprüche aus dem Gesetz gegen den unlauteren Wettbewerb (UWG) in Betracht.\n\nCybermobbing findet allerdings auch in Foren oder auf Webseiten statt, die sich der deutschen Rechtsprechung völlig entziehen. Ein Großteil der als Cyber-Mobbing bezeichneten Aktivitäten haben sich auf Webseiten und Foren verlagert, die im Ausland angemeldet sind; allerdings ist es nach wie vor auch noch in deutschen sozialen Netzwerken ein Problem. Die Rechtsdurchsetzung innerhalb der EU ist mit der EU-Verordnung über die gegenseitige Anerkennung von Schutzmaßnahmen in Zivilsachen verbessert worden. Danach können Opfer häuslicher Gewalt die in einem Mitgliedstaat erlassenen Gewaltschutzanordnungen auch in andere Mitgliedstaaten übertragen lassen. Solche Gewaltschutzanordnungen können nicht nur in Fällen häuslicher Gewalt, sondern auch in Fällen von Nachstellungen – umgangssprachlich auch Stalking genannt – erlassen werden, d. h. auch in Fällen des klassischen Mobbings per Handy, SMS und E-Mails.\n\nDas Jugendschutzgesetz enthält spezielle Passagen, die sich auf Mediennutzung beziehen. Im Zusammenhang mit gewalthaltigen Medien erscheint der Teilaspekt der Nachahmung, das Aufgreifen und Ausleben einer Idee durch junge Menschen als relevant für das Verständnis auch von Schulschießereien. Darauf deuten dort Nachahmungen von Heldenfiguren durch die Täter aus bekannten Filmen oder Computerspielen hin. Die Gefahr von Nachahmungstaten und Trittbrettfahrern steige zudem durch die Häufung der Fälle und der Medienpräsenz.\n\nBisher wird in Deutschland allerdings im Gegensatz beispielsweise zum Vorgehen in England keine kriminalpolizeiliche Statistik über den Einfluss von Cyber-Mobbing auf Suizide (von Jugendlichen) geführt.\n\nDas OLG Köln stellte im November 2007 fest, dass „eine Bewertung unter den genannten Kriterien durchaus für eine Orientierung von Schülern und Eltern dienlich und zu einer wünschenswerten Kommunikation, Interaktion und erhöhter Transparenz führen kann. Gerade der schulische Bereich und die konkrete berufliche Tätigkeit von Lehrern sind durch Bewertungen gekennzeichnet, so dass es – auch vor dem Hintergrund eines Feedbacks – nahe liegt, diese im Rahmen einer Evaluation zurückzugeben. Sie stellen, obwohl in Notenstufen angegeben, eher gegriffene, subjektive Einschätzungen widerspiegelnde Wertungen dar, die dennoch geeignet sein können, Schülern und Lehrern eine gewisse Orientierung in der Einschätzung der bewerteten Kriterien zu ermöglichen“.\nDie genannten Foren können die Nutzung des Grundrechts auf Meinungsfreiheit unterstützen, da keine direkten Repressalien zu befürchten sind. Beispielsweise würden wahrscheinlich kurz vor anstehenden Beurteilungen wenige Schüler Unterrichtsmethoden ihres Lehrers als gerade ausreichend oder befriedigend bewerten.\nGrundrechtlich geregelt ist die Meinungsfreiheit in Art. 5 GG, welche allerdings ihre Schranken in den Vorschriften der allgemeinen Gesetze, den gesetzlichen Bestimmungen zum Schutze der Jugend und in dem Recht der persönlichen Ehre findet.\n\nSelbst unter Pseudonym wurde die private Meinungsäußerung von Rechtsprechungsseite gewürdigt: „Es steht außer Frage, dass die Möglichkeit, sich unter einem Pseudonym zu äußern, für den Prozess der öffentlichen Meinungsbildung von Nutzen sein kann. Das gilt dann, wenn der Äußernde ohne diese Möglichkeit aus Angst vor ungerechtfertigten Repressalien von einem an sich schutzwürdigen Beitrag zur öffentlichen Meinungsbildung abgehalten werden könnte.“ Der Schutz von Meinungsäußerungen tritt regelmäßig hinter dem Persönlichkeitsrechtsschutz zurück, wenn sich die betreffenden Äußerungen als Schmähung darstellen. Eine Äußerung ist als Schmähkritik anzusehen, wenn sie sich nicht auf eine Auseinandersetzung in der Sache bezieht, sondern jenseits einer polemischen und überspitzten Kritik in der persönlichen Herabsetzung des Betroffenen besteht.\n\nRichten sich die Äußerungen nicht gegen eine bestimmte Person, sondern gegen ein Unternehmen, ist das sog. Unternehmenspersönlichkeitsrecht oder das Recht am eingerichteten und ausgeübten Gewerbebetrieb betroffen. In diesen Fällen gelten andere Grundsätze. Denn der Umfang des Schutzbereichs des Unternehmenspersönlichkeitsrechts geht nach höchstrichterlicher Rechtsprechung nicht so weit wie das allgemeine Persönlichkeitsrecht von natürlichen Personen. Die Entfaltung der Persönlichkeit im Wirtschaftsleben bringt es nämlich naturgemäß mit sich, dass sie sich der Kritik stellen muss (BGH, Urteil vom 24. Oktober 1961 - VI ZR 204/60, NJW 1962, 32, 33). Deshalb haben betroffene Unternehmen auch scharf und überzogen formulierte Kritik zu dulden, soweit der Äußerung eine kritische Auseinandersetzung zugrunde liegt. Eine Schmähung liegt bei einer die Öffentlichkeit wesentlich berührenden Frage nur ausnahmsweise vor und ist eher auf die Privatfehde beschränkt (BGH, Urteil vom 16. Dezember 2014, Az.: VI ZR 39/14).\n\nIn Österreich wird Cybermobbing seit dem 1. Januar 2016 mit Freiheitsstrafe von bis zu einem Jahr Haft bestraft. Voraussetzung ist die Verletzung des höchstpersönlichen Lebensbereiches oder die Verletzung der Ehre im Wege der Telekommunikation oder über ein Computersystem. Die Regelung wurde am 7. Juli 2015 mit dem Strafrechtsänderungsgesetz 2015 im Nationalrat verabschiedet und führt Cybermobbing als Straftatbestand als § 107c in das Strafgesetzbuch ein. Die neue Regelung nennt sich „Fortgesetzte Belästigung im Wege einer Telekommunikation oder eines Computersystems“ und wird in den Materialien zum Gesetz ausdrücklich als Cybermobbing bezeichnet.\n\nHier hat sich Anfang Mai 2011 der Erziehungsminister mit dem Internet-Netzwerk-Anbieter und Unternehmen Facebook zusammengetan: Mobber sollen identifiziert und unter Umständen von Unterricht und/oder Schule ausgeschlossen werden. Lehrer sollen Blog-Einträge kontrollieren.\n\nIn den USA ließ sich 2009 auf gesamtstaatlicher Ebene der Vorwurf des Cyber-Bullyings, auch mit tödlichem Ausgang, unter der dort herrschenden Rechtslage nicht fassen. In einem Präzedenzfall hat ein Bundesrichter schließlich sogar die Verurteilung einer 50-jährigen Mutter wegen unautorisierten Zugangs zu einem Computer (sie hatte sich mit falschen Angaben angemeldet) aufgehoben, weil nach Ansicht des Richters kaum jemand die umfangreichen Nutzungsbedingungen eines Anbieters gründlich lese und beherzige. Gemeinsam mit ihrer 13-jährigen Tochter hatte sie unter falscher Identität eine Bekannte ihrer Tochter im Netzwerk Myspace gemobbt, was zum Suizid des Mädchens führte.\n\nDer Bundesstaat Missouri führte 2008 ein Gesetz gegen Cybermobbing ein. Dort hatte die Selbsttötung des Teenagers große Empörung ausgelöst.\n\nDer Bundesstaat New Jersey erließ nach dem Selbstmord eines Studenten das bislang als in den USA am strengsten geltende entsprechende Gesetz gegen Gewalt und Mobbing an Schulen und Hochschulen.\n\nIm Frühjahr 2011 fand am amerikanischen Regierungssitz im Weißen Haus in Washington, D.C. ein Anti-Mobbing-Gipfel statt. Das Unternehmen Facebook erklärte dort, in Zukunft Streitschlichter einsetzen zu wollen.\n\nSüdkorea hat 2007 ein Gesetz zur Vermeidung von Mobbing im Internet vorgelegt.\n\n\n\n\n\n\n"}
{"id": "2790684", "url": "https://de.wikipedia.org/wiki?curid=2790684", "title": "Ted (Texteditor)", "text": "Ted (Texteditor)\n\nTed ist ein freies Textverarbeitungsprogramm, das Textdateien im Rich Text Format (RTF) unter Linux/Unix bearbeitet.\n\nTed bietet eine Grafische Benutzeroberfläche und WYSIWYG, ermöglicht auch farbliche Formatierungen, Kopf- und Fußzeilen, Fuß- und Endnoten, Tabellen, Zeichnungen und Bildern. Text kann auch als PostScript, PDF und HTML gespeichert werden.\n\nTed setzt ein X Window System voraus und greift per Motif, LessTif oder GTK+ darauf zu.\n\n"}
{"id": "2796077", "url": "https://de.wikipedia.org/wiki?curid=2796077", "title": "Asus Eee PC", "text": "Asus Eee PC\n\nDer Eee PC [] war eine Serie von Netbooks des Computerherstellers Asus. Das Akronym „Eee“ steht für den Werbespruch „Easy · Excellent · Exciting\"“ (einfach, exzellent, aufregend) bzw. „Easy to Learn, Easy to Work, Easy to Play\"“.\n\nDie Geräte der Eee-PC-Serie waren vor allem für den Massenmarkt in Schwellenländern gedacht, wogegen sie in Industrienationen mit deutlicher Verzögerung als Lifestyle-Produkte erschienen sind.\n\nMit dem Erscheinen des ersten iPad 2010 und der darauf folgenden Entwicklung von Tablets wurde die Entwicklung des Eee-PC eingestellt. Inzwischen sind Netbooks allgemein zur Seltenheit geworden.\n\nDas erste Gerät war der \"Eee PC 700\", ein Subnotebook, welches zu einem für diese Kategorie sehr niedrigen Preis angeboten wurde. Von Intel wurde für diese Geräteklasse im Februar 2008 die Bezeichnung \"Netbook\" eingeführt. Das Gerät wurde seit dem 16. Oktober 2007 in Taiwan angeboten, es folgten die USA Anfang und Großbritannien Mitte November. In Deutschland und Österreich begann der Verkauf am 24. Januar 2008, allerdings zunächst nur in einer Version (\"Eee PC 701 4G\") und in zwei verschiedenen Farbausführungen (schwarz und perlweiß).\n\nIn allen Ländern kam es zu Engpässen bei der Auslieferung, da die Nachfrage das Angebot, dessen Umfang Asus nicht bekannt gab, überstieg. Die große Nachfrage führte zu einer ganzen Reihe ähnlicher Geräte von Mitbewerbern.\n\nDas Nachfolgemodell \"Eee PC 900\" kam am 26. Juni 2008 in Deutschland auf den Markt, später, am 18. August 2008, folgte der \"Eee PC 901\", der einen Intel Atom-Prozessor als CPU hat. Am 4. September 2008 erschien der \"Eee PC 1000H\", der über ein 10-Zoll-Display, eine 160-GB-Festplatte, eine 1,3-Megapixel-Webcam, Bluetooth und eine n-WLAN-Schnittstelle verfügt. Bei Markteinführung kosteten die Nachfolgeprodukte (zum Teil erheblich) mehr als der \"Eee PC 701 4G\" bei seiner Markteinführung (299 Euro).\nIm Juni 2009 zeigte Asus eine Variante mit Snapdragon-Prozessor – als erstes PC-ähnliches Gerät auf ARM-Basis (nach Acorn); auf ihm lief Googles Android als Betriebssystem. Das Gerät wurde jedoch nur als Prototyp präsentiert und gelangte nicht in den Verkauf.\n\nDas erste Modell des Eee PC hat ein Display mit einer Bilddiagonalen von 7 Zoll und einer Auflösung von 800×480 Pixeln. Als Prozessor dient bei der Serie \"701\" ein untertakteter Celeron Mobile Prozessor (ULV353), der mit 630 anstatt 900 MHz betrieben wird. Beim \"Eee PC 700 2G surf\", dem einfachsten, in Europa nicht angebotenen Modell, hat der Prozessor nur 570 MHz. Als Grafikchip wird bei allen Modellen ein Intel 915GML eingesetzt, der neben dem internen Display einen VGA-Anschluss hat, über den ein externer Monitor oder Projektor mit einer Auflösung von bis zu 1600×1200 Pixeln betrieben werden kann. Die Modelle haben je nach Ausstattungsvariante zwischen 256 (700er) und 1024 MB Arbeitsspeicher.\n\nFür den Klang sind Stereo-Lautsprecher vorhanden, die durch einen High-Definition-Audio-Chipsatz angesteuert werden. Zudem sind ein Mikrofon sowie Audio-Ein- und Ausgang vorhanden. Ein optisches Laufwerk (CD/DVD) ist nicht vorhanden. Statt einer Festplatte ist ein Solid State Drive mit – je nach Ausstattungsvariante – 2 bis 8 GB Speicherkapazität verbaut. Alle Modelle verfügen über einen 10/100-MBit-LAN-Anschluss, 11/54-MBit-WLAN (Atheros-Chipsatz), drei USB-2.0-Anschlüsse, einen Cardreader für MMC / SD/SDHC- sowie einen PCI-Express-Mini-Card-Steckplatz.\n\nInsgesamt gibt es vom Eee PC im Ausland vier Ausstattungsvarianten und in den Farben Pink, Grün, Blau sowie Schwarz und Weiß. Unterscheidungsmerkmale sind in erster Linie die Kapazität des Arbeitsspeichers, die Größe der Solid State Disk und die Kapazität des Akkus. Zusätzlich verfügen einige Varianten über eine eingebaute Webcam. In Deutschland ist die Verfügbarkeit nach wie vor auf das 4G-Modell mit Webcam in den Farben schwarz und weiß beschränkt.\n\nDas Gerät hat eine Größe von 225 mm × 165 mm bei einer Höhe von 21 mm bis 35 mm und wiegt mit Akku 920 g (Modell \"4G\"). Damit ist der Eee PC wesentlich kleiner und leichter als preislich vergleichbare Notebooks.\n\nAusgeliefert wird der \"Eee PC 700/701\" im \"Easy Mode\" des vorinstallierten Linux-Derivates Xandros. In diesem Modus werden die Anwendungsprogramme über eine in mehrere Kategorien (Internet, Arbeit, Lernen, Spielen, Einstellungen, Favoriten) gegliederte Oberfläche mit besonders großen und bunten Symbolen aufgeteilt. Der \"Easy Mode\" basiert auf dem Fenstermanager iceWM, der für eine Windows-XP-artige Anmutung der Fenster sorgt und ansonsten vor allem eine Statusleiste am unteren Bildschirm beiträgt. Vorinstalliert sind auch der KDE-Desktop und der im Textmodus arbeitende Dateimanager Midnight Commander.\n\nDie Installation eines beliebigen anderen Betriebssystems ist möglich, jedoch durch die Hardware begrenzt. Von den Linux-Distributionen Xubuntu existiert beispielsweise eine speziell angepasste Version; für viele andere Systeme finden sich im Internet Anleitungen. Die meisten davon unterstützen den Asus EeePC \"out of the box\", also ohne spezielle Anpassungen vornehmen zu müssen. Für Microsoft Windows XP stehen Treiber bereit, allerdings wird die Bedienung in der Standard-Bildschirmauflösung erschwert, da Dialogboxen von Windows XP unter Umständen nicht vollständig angezeigt werden können. Die Installation von Microsoft Windows Vista ist zwar auf Modellen mit wenigstens 8 GB Festspeicher grundsätzlich möglich, das System ist aber aufgrund der kaum über den von Microsoft für diese genannten Systemanforderungen liegenden Leistung nur bedingt nutzbar.\n\nNachdem immer wieder Gerüchte über Modelle mit einem größeren Display kursierten und Asus auch derartige Modelle auf Messen ausgestellt hat, wurde auf der CeBIT 2008 unter dem Namen \"Eee PC 900\" eine Modellreihe mit einem deutlich größeren Display von 8,9″ und einer Auflösung von 1024×600 Pixeln vorgestellt. Das Modell verfügt mit Windows-Lizenz über einen 12 GB, mit Linux über einen 20 GB großen Festspeicher. Asus investiert dabei das mit der Windows-Lizenz eingesparte Geld in eine größere Kapazität. Bei beiden Ausführungen sind 4 GB in Form einer SSD eingebaut, die nur durch Öffnung des Gehäuses ausgetauscht werden kann. Darauf wird das Betriebssystem gespeichert. In der Windows-Version sind so zudem 8 GB Flash-Speicher und in der Linux-Version 16 GB Flash-Speicher über einen Mini-PCIe-Steckplatz verbaut. Zur weiteren Ausstattung gehört eine über dem Display integrierte Webcam, die nun 1,3 statt 0,3 Megapixel auflöst. Wie beim \"Eee PC 701\" gibt es weiterhin einen SDHC-Speicherkartensteckplatz, drei USB-Ports sowie einen D-Sub-VGA-Ausgang. Auch der Kopfhörerausgang sowie Mikrofoneingang sind wie gehabt als 3,5-mm-Klinkenbuchse integriert. Dafür kostet das neue Modell in Deutschland ab dem Erscheinungstermin 26. Juni 2008 100 € mehr, also 399 €. Die Lautsprecher sind nun auf der Gehäuseunterseite angebracht. Als Prozessor setzt die 900er-Serie weiterhin auf den Intel Celeron mit 900 MHz.\n\nDas neuere Modell \"Eee PC 900\" mit 8,9\"-Display ist in Deutschland nur in der 12-GB-Version mit Windows XP Home verfügbar, in der Schweiz ausschließlich als 20-GB-Modell mit Linux. Da laut Presseberichten auch in Deutschland ein Linux-Modell eingeführt werden sollte, brachte Asus verspätet das Modell \"900A\" auf den deutschen Markt. Dabei handelt es sich um einen mit Atom-Prozessor ausgestatteten \"Eee PC 900\", der allerdings nur über 8 GB Flash-Speicher verfügt. Die unverbindliche Preisempfehlung des Herstellers lag bei 299 €.\n\nDer \"Eee PC 900A\" wird mit einer Linux-Distribution ausgeliefert, Treiber für Windows XP liegen bei.\n\nInzwischen ist auch der \"Eee PC 901\" mit einem Intel Atom-Prozessor statt des Celeron M, einem Akku mit größerer Kapazität und einem integrierten Bluetooth-Modul verfügbar. Durch den effizienteren Prozessor und die Vergrößerung der Akkukapazität wurde die Akkulaufzeit erheblich verlängert. In Deutschland erschien der \"Eee PC 901\" am 18. August 2008, jedoch wie zuvor der \"Eee PC 900\" nur in der Windows-Variante mit 12 GB Speicher.\n\nIn Deutschland wird seit dem 7. November 2008 auch das Modell \"Eee PC 900HD\" angeboten, ein \"Eee PC 900\" bei dem statt der SSD eine Festplatte mit 160 GB zum Einsatz kommt.\n\nDer \"Eee PC 901Go\" verfügt über ein eingebautes UMTS-Modem mit HSPA (HSDPA+HSUPA), 16 GB SSD und ist wahlweise in weiß oder schwarz, mit Windows XP Home oder Linux erhältlich.\n\nIm November 2008 kam der \"Eee PC 904HD\" auf den deutschen Markt, der eine große Tastatur wie der \"Eee PC 1000(H)\" hat. Er ist wie alle Modelle der 90x-Reihe mit einem 8,9\" -Display ausgestattet, mit einem Rahmen um den Bildschirm. Der \"Eee PC 904HD\" verfügt über einen 900-MHz-Celeron-M-Prozessor.\n\nZur Computex 2008 zeigte Asus zwei Eee PCs mit 10,1-Zoll-Display. Die dadurch einhergehende Vergrößerung des Gehäuses wurde genutzt, um die Tastatur zu vergrößern, womit die 1000er-Serie für das Zehnfingerschreiben besser geeignet ist als die 90x- und 70x-Serien. Die 1000er-Serie wird mit Draft-N-WLAN, Bluetooth, eine 1,3-Megapixel-Webcam und Intels Atom-Plattform ausgestattet. Der \"Eee PC 1000\" ist mit einer 40 GB großen SSD ausgestattet, der \"Eee PC 1000H\" mit einer 2,5-Zoll-Festplatte mit bis zu 160 GB. Der \"Eee PC 1000H\" ist als erstes Modell der Reihe seit dem 4. September 2008 in Deutschland erhältlich.\n\nIm März 2009 wurde der \"Eee PC 1008HA\" im \"Seashell\"-Design veröffentlicht. Das Gehäuse wird wahlweise im schwarzen oder weißen Hochglanzkunststoff angeboten und als CPU kommt ein Intel Atom N280 zum Einsatz. Im Oktober 2009 wurde das Model \"Eee PC 1008P/1008PGO\" ohne Ankündigung veröffentlicht. Es ist ebenfalls mit einem 10,1-Zoll-Display mit einer Auflösung von 1024×600 Pixel ausgestattet, besitzt allerdings einen Intel Atom N450 CPU. Der \"Eee PC 1008PGO\" enthält weiters auch ein integriertes UMTS-Modem.\n\nIm Juni 2008 wurden von der Stiftung Warentest verschiedene Notebooks getestet; wobei der Asus Eee PC 1000H in der Kategorie Netbook mit Note 2,6 am besten abgeschnitten hat. Es wurden zudem noch Geräte der Firmen HP, Dell, MSI, Acer und One getestet.\n\nIm Juni 2009 zeigte Asus auf der Computex die neue 1100er-Serie. Diese besitzt ein 11,6-Zoll-Bildschirm und bietet eine Auflösung von 1366×768 Pixel. Als Prozessor wird ein Atom Z520 mit 1,33 GHz Taktfrequenz eingesetzt. Als Betriebssystem ist Windows XP Home (32bit) vorinstalliert.\n\nDie 1200er-Serie verfügt über einen 12,1-Zoll-TFT und wird mit Windows 7 Home Premium ausgeliefert. Aus diesem Grund sind die Modelle dieser Reihe mit 2 GB Arbeitsspeicher ausgestattet. Zusätzlich verfügt das Modell 1201N über den Nvidia-ION-Chipsatz.\n\nIm vierten Quartal 2008 wurde die neue S-Serie vorgestellt. Bei dieser wollte Asus die „alten“ Eigenschaften der Serie mit einem „neuen“ Äußeren kombinieren. Mit dem S101 stellte Asus sein bislang teuerstes Netbook der EeePC-Reihe vor. Während andere Hersteller ihre Mini-Laptops immer günstiger anboten, peilte Asus für den S101 einen Preis von knapp 700 US-Dollar an. Der S101 ist zusammengeklappt nur halb so dick wie seine günstigen Brüder und wiegt nur 1 Kilogramm. Später kam noch der S101 in einer S101H Ausführung auf den Markt, die sich nur in der Festplatte vom anderen Modell unterschied.\n\nAn das Design und die Hardware des \"Eee PC\" hat Asus für den Fernsehempfang den DVB-T-Tuner \"MyCinema U3000 Mini\" und Zusatzgeräte (z. B. bessere Lautsprecher) angepasst.\n"}
{"id": "2797353", "url": "https://de.wikipedia.org/wiki?curid=2797353", "title": "ADINA", "text": "ADINA\n\nADINA (\"Automatic Dynamic Incremental Nonlinear Analysis\") ist ein Finite-Element-Programmsystem, das von Klaus-Jürgen Bathe ins Leben gerufen wurde und seit 1986 von \"ADINA Research & Development, Inc.\" entwickelt wird. Das Programm bietet Möglichkeiten zur Berechnung linearer und nichtlinearer, quasistatischer und dynamischer Fragestellungen aus Strukturmechanik, Strömungsmechanik, Akustik, Thermodynamik, Elektromagnetismus.\n\nEine der Besonderheiten von ADINA ist die Möglichkeit, Probleme der Fluid-Struktur-Interaktion (FSI) vollständig gekoppelt zu berechnen. Dies ermöglicht Lösungen für offene Fragestellungen, die das flexible Verhalten von Strukturen unter beliebigen Strömungsbedingungen und die wechselseitigen Beeinflussungen betreffen (Bsp.: Fallschirm, Adern, Ventilfeder).\n\nDie von ADINA verwendeten mathematischen Formulierungen sind vollständig beschrieben und damit jedem Interessenten zugänglich. Die zu diesem Thema verfassten Bücher von Klaus-Jürgen Bathe gehören zu den anerkannten Standardwerken der Finite-Elemente-Methode.\n\nADINA bedient sich einer großen Anzahl von 1-, 2-, und 3-dimensionalen Elementformulierungen, für die eine Vielzahl von Materialgesetzen verfügbar ist.\n\nADINA ist modular aufgebaut und kann auch modular angeschafft werden. Neben den drei physikalischen Kernbereichen ADINA-A (Struktur), ADINA-T (Thermodynamik und Feldberechnung) und ADINA-F (Strömungsmechanik) stehen auch die Kopplungen ADINA-FSI und ADINA-TMC sowie eine vollständige Bedienoberfläche und Schnittstellen zur Verfügung. Das System beinhaltet auch Netzgeneratoren inklusive eines automatischen Hexaedervernetzers. Über eine Protokolldatei ist eine Prozessautomatisierung möglich, die Offenheit des Systems lässt Raum für die Integration eigener Lösungen.\n\nZu den Nutzern von ADINA zählen neben ausbildungsbedingten Strukturen, wie z. B der Bundeswehr, Universität München und anderen Universitäten, primär Firmen, welche virtuell entwickeln und simulieren (hierzu gehören beispielsweise Ingenieurbüros und Fahrzeughersteller).\n\nAufgrund einer Kooperation zwischen ADINA Inc. und Siemens PLM ist in NX Nastran ein ADINA Subset als Zusatzmodul Advanced Nonlinear verfügbar. \n\n"}
{"id": "2797864", "url": "https://de.wikipedia.org/wiki?curid=2797864", "title": "Geant4", "text": "Geant4\n\nGeant4 (\"Geometry and Tracking\") ist eine Plattform für die Simulation des Durchtritts von Partikeln durch Materie unter Benutzung von Monte-Carlo-Methoden. Sie ist der modernste Teil der Toolkit-Serie, die am CERN entwickelt wird und benutzt zum ersten Mal Objektorientierte Programmierung (in C++). Die Anwendungsgebiete sind Hochenergiephysik und Experimente zu nuklearen Reaktionen, medizinische Physik, Beschleunigerphysik und Astrophysik. Die Software wird von mehreren Forschungsprojekten weltweit genutzt.\n\nDie Geant4-Software und ihr Quelltext waren immer frei erhältlich, aber bis zur Version 8.1 (30. Juni 2006) gab es keine spezifische Lizenz zur Benutzung. Mittlerweile wird Geant4 unter den Bedingungen der Geant4 Software License verbreitet.\n\nGeant4 besitzt Möglichkeiten zur Bearbeitung von Geometrien, Tracking, Antworten von Detektoren, Run Management, Visualisierung und Benutzerschnittstelle. Bei vielen physikalischen Simulationen kann damit weniger Zeit mit Details auf niedrigem Level verbracht werden und die Forscher können sich auf die wichtigeren Aspekte der Simulation konzentrieren.\n\n\n\nAufgrund seiner universellen Struktur eignet sich Geant4 auch gut für andere Anwendungsbereiche:\n\n\n"}
{"id": "2800589", "url": "https://de.wikipedia.org/wiki?curid=2800589", "title": "Creo Elements/Direct Modeling", "text": "Creo Elements/Direct Modeling\n\nCreo Elements/Direct Modeling ist ein professionelles 3D-CAD-Werkzeug der Parametric Technology Corporation.\n\nBis November 2008 hieß das Produkt \"OneSpace Modeling\", nach der vollständigen Integration der CoCreate Software GmbH wurde der Name zu \"CoCreate Modeling\" geändert. Ende Oktober 2010 wurde die neue Creo-Produktpalette vorgestellt und im selben Zug wurde die Software in \"Creo Elements/Direct Modeling\" umbenannt. Die Entwicklung von Modeling startete Anfang der 1990er Jahre unter dem Namen \"Soliddesigner\". Damals wurde die Entwicklung dieser Software unter Hewlett-Packard in einem Geschäftsbereich des Mutterkonzerns angestoßen.\n\nDer Anwendungsbereich der Software Creo Elements/Direct Modeling ist im High-Tech-Maschinenbau und in der Elektroindustrie. Die Software verfolgt einen Ansatz des dynamischen Modellierens (Dynamic Modeling).\n\nIn der 3D-Volumenmodellierung ist dies eine neue Herangehensweise, da diese Art der Modellierung ohne einen Modellentstehungsbaum auskommt. Damit hebt sich die Software von CAD-Systemen ab, die einen klassischen Ansatz der Modellierung verfolgen, bei dem eine Historie der angewandten Parameter auf das Modell im Konstruktionsprozess mitgeführt wird.\n\n\nVerfügbare Zusatzmodule: Surfacing, Sheet Metal, Advanced Design, Cabling, Mold Base, Finite Element Analysis, Part Library, 3D Access\n\n\n"}
{"id": "2800989", "url": "https://de.wikipedia.org/wiki?curid=2800989", "title": "Ututo", "text": "Ututo\n\nUtuto ist ein aus Argentinien stammendes Projekt, das eine Linux-Distribution veröffentlicht und darauf aufbauend Softwareprojekte entwickelt und Aktivitäten im Bildungsbereich durchführt.\n\nDie Linux-Distribution, Ututo GNU/Linux genannt, wird seit dem Jahr 2000 hauptsächlich von argentinischen Entwicklern vorangetrieben, basiert auf Gentoo Linux und besteht ausschließlich aus freier Software.\n\nDer Name \"Ututo\" kommt von einer in Argentinien weit verbreiteten Gecko-Art.\n\nDie erste Version von Ututo wurde im Oktober 2000 von Diego Saravia an der Universität Salta entwickelt. Ziel war eine Linux-Version, die sich vollständig von einer CD aus starten ließ (Live-CD). Ututo war eine der ersten Distributionen, die dieses Ziel erreichte.\n\n2002 wurde das Projekt Ututo-R gestartet, eine Version des Betriebssystems, die als Ziel hatte, als Software-Router zu funktionieren. Ututo-R wurde in Argentinien auf mehreren Netzwerken in Behörden und Schulen, beispielsweise in Buenos Aires, installiert.\n\nDie Version Ututo-e, eine Desktop-Variante, wurde im Jahr 2004 gestartet. Sie wurde mit dem primären Ziel entwickelt, ein vollwertiges Betriebssystem mit der Basis aus vollständig freier Software anzubieten, das auch auf älterer Hardware lauffähig ist, wie sie in Argentinien insbesondere als Nachwirkung der Argentinien-Krise häufig noch anzutreffen ist.\n\nSeit 2005 wird das Betriebssystem als Ututo XS bezeichnet, da die neueren Varianten ein komplett neues Design, u. a. eine einfache Installationsumgebung, aufweisen. Die Live-CD-Version wird ebenfalls unter diese Bezeichnung, allerdings als Ututo XS Vivo (span. für \"live\") bezeichnet.\n\nDas Ututo-Projekt entwickelte neben dem Betriebssystem auch verschiedene darauf aufbauende Softwareprojekte, darunter ein System zum Betrieb eines Webradios und eines Internet-Fernsehsenders (\"Ututo Free TV\"), ein auf Ututo XS basierendes Supercomputer-Cluster sowie eine Software zum Betrieb eines IPv6-Netzwerkes. Zeitweise arbeitete das Projekt auch an einer Distribution von FreeBSD (\"Ututo GNU/BSD\"), die jedoch wieder eingestellt wurde.\n\nUtuto besteht vollständig aus freier Software. Dies macht sich beispielsweise dadurch bemerkbar, dass selbst bekannte proprietäre Plugins wie Adobe Flash durch freie Alternativen wie Gnash ersetzt werden. Zumindest bis zur Version XS 2006 gab es durch diese strenge Ethik-Konvention auch Nachteile, die sich etwa in Kompatibilitäts- und Leistungsproblemen im Vergleich zu anderen Linuxdistributionen bemerkbar machten.\n\nUtuto ist eine der wenigen Distributionen von Linux, die eine spezielle Version für verschiedene Prozessor-Architekturen bereitstellen. So gibt es z. B. innerhalb der x86-Versionen eine für AMD-Prozessoren und eine für Intels Pentium 4. Dies erlaubt es, die einzelnen Architekturen besser auszunutzen.\n\nAls weitere Besonderheit benutzt Ututo ab der Version XS 2007 eine eigene Paketverwaltung namens UTUTO-Get (heute UGet), die sowohl Debian- als auch RPM-Pakete verwalten kann.\n\nUTUTO-e bzw. UTUTO XS basiert standardmäßig auf der Desktop-Umgebung Gnome. Ab der Version XS 2007 werden jedoch auch die alternativen Umgebungen KDE, Fluxbox und IceWM auf der Installations-CD mitgeliefert.\n\nDas Ututo-Projekt wurde 2006 von der argentinischen Abgeordnetenkammer als Projekt nationalen Interesses ausgezeichnet.\n\nÜber das Ututo-Projekt wurde unter anderem in den großen argentinischen Zeitungen Clarín, Página/12 und \"La Capital\" berichtet. Auch das Wired-Magazin widmete Ututo 2001 einen Artikel.\n\nInternationale Bekanntheit erlangte die Distribution vor allem, da sie von Richard Stallman, Gründer des GNU-Projektes, bis zur Entwicklung von gNewSense längere Zeit genutzt und von der Free Software Foundation wegen des Verzichts auf proprietäre Bestandteile empfohlen wurde.\n\n\n"}
{"id": "2803988", "url": "https://de.wikipedia.org/wiki?curid=2803988", "title": "Keine Zeit für Nüsse", "text": "Keine Zeit für Nüsse\n\nKeine Zeit für Nüsse (Originaltitel: \"No Time for Nuts\") ist ein US-amerikanischer, computeranimierter Kurzfilm aus dem Jahr 2006. Er wurde erstmals auf der DVD zum Kinofilm \"\" veröffentlicht und stellt einen Ableger zu diesem dar. \"Keine Zeit für Nüsse\" ist gleichzeitig eine Fortsetzung des Films \"Scrats neue Abenteuer\" aus dem Jahr 2002. Unter dem Titel \"Ice Age: No Time For Nuts 4D\" existiert eine 4D-Version des Films, die seit 2016 im Movie Park Germany und im Sea Life Melbourne aufgeführt wird.\n\nScrat ist ein Rattenhörnchen und lebt in der Eiszeit. Als er nach einem geeigneten Ort sucht, um seine Eichel zu vergraben, findet er im Eis eine Zeitmaschine, die durch einen Wissenschaftler aus der Zukunft, dessen Skelett im Eis vergraben ist, in die Vergangenheit gelangt ist.\n\nAls Scrat die Zeitmaschine berührt, schickt sie seine Nuss in eine andere Zeit. Er gerät in Panik und berührt die Zeitmaschine ein weiteres Mal, wodurch sie auch ihn in die andere Zeit bringt. Jedes Mal, wenn in einer Zeit eine Gefahr auf ihn lauert oder wenn ihm die Zeit nicht gefällt, versetzt er der Zeitmaschine einen Tritt und er landet in einer anderen fremden Zeit.\n\nSchließlich findet Scrat sich unter einem Baum voller Eicheln wieder. Versehentlich berührt er die Zeitmaschine. Weil er allerdings in der Zeit mit den vielen Eicheln bleiben will, zerstört er die Zeitmaschine. Scrat merkt, dass die Eiche nicht echt ist, sondern dass sie nur einer Eiche nachempfunden ist und er sich in einer hochentwickelten Zukunft befindet, in der weit und breit kein Baum zu sehen ist. Als er wenigstens seine mitgebrachte Eichel nehmen will, wendet die Zeitmaschine ihre letzte Kraft auf und schickt die Eichel in eine andere Zeit, während Scrat in der hochentwickelten Zukunft gefangen bleibt.\n\nDer sechsminütige Film entstand in den Blue Sky Studios. Der Regisseur Chris Renaud begann mit dem Projekt im September 2005. Etwa einen Monat später stieß Mike Thurmeier als zweiter Regisseur dazu. Die beiden arbeiteten bis Dezember desselben Jahres an der Geschichte und dem Storyboard zum Film. Im Januar 2006 begann die Arbeit an der Animation, die im Juni des gleichen Jahres schließlich abgeschlossen war. Als Animatoren an dem Film arbeiteten Aaron J. Hartline, Leif Jeffers, Robin Luera, Wesley Mandell und David Torres.\n\nDer Film existiert außerdem als 4D-Film unter dem Titel \"Ice Age: No Time For Nuts\" und wird seit 2016 im Movie Park Germany in Bottrop in einem extra für den Film umgestalteten Kinosaal gezeigt. Die Vorführung erfolgt im 3D-Verfahren mit spezieller 3D-Brille. Zusätzlich werden einige Effekte, wie Windstöße, Wasserspritze und bewegliche Stühle eingesetzt. Der Film läuft in seiner englischsprachigen Originalversion. Die Effekte und das Konzept stammen von SimEx-Iwerks. Neben dem Movie Park Germany wird der Film auch im Sea Life Melbourne aufgeführt.\n\nBei der Oscarverleihung 2007 war der Film als „bester animierter Kurzfilm“ nominiert, musste sich aber dem Film \"The Danish Poet\" von Torill Kove geschlagen geben. \"Scrats neue Abenteuer\" war ebenfalls bereits für den Oscar nominiert gewesen. Den Annie Award gewann der Film in derselben Kategorie.\n\n"}
{"id": "2809870", "url": "https://de.wikipedia.org/wiki?curid=2809870", "title": "Geschichte der Computergrafik", "text": "Geschichte der Computergrafik\n\nDie Geschichte der Computergrafik, der computergestützten Erzeugung von Bildern, wurde insgesamt eher von neuen Entwicklungen im Hardware- als im Softwarebereich beeinflusst.\n\nIn den 1950er-Jahren wurden einige der ersten Computer mit Bildschirmen ausgestattet. In den 1960er-Jahren kamen erste grafische Computersysteme auf den Markt, die aber äußerst kostspielig waren und erst gegen Ende des Jahrzehnts von erschwinglichen Geräten ersetzt wurden. Betriebsfertige Komplettsysteme, die den Anwender von Softwaredetails abschirmten, erschienen in den 1970er-Jahren. Bedeutsame und zum Teil auch heute noch gebräuchliche Rendertechniken wurden entwickelt. In den 1980er-Jahren stieg die Computerleistung derartig an, dass auch PC-Anwender Computergrafiken selbst herstellen konnten. Neben technischen Anwendungen erlangten Multimedia- und andere nichttechnische Anwendungen zunehmende Bedeutung.\n\nFrühe Computer verwendeten als Ausgabegeräte Fernschreiber und Kettendrucker, mit denen sich Figuren und Diagramme nur sehr grob zeichnen ließen. Der am MIT entwickelte Whirlwind-Computer wird oftmals als Beginn der Computergrafik angesehen. Er verfügte ab 1951 über einen Kathodenstrahlröhren-Bildschirm, der sowohl für den Anwender als auch für Fotoapparate, die das Bild zum späteren Ausdruck aufnahmen, bestimmt war. Außerdem war er mit einem von Bob Everett entwickelten lichtgriffelähnlichen Gerät ausgestattet, mit dem sich direkt auf dem Bildschirm Eingaben machen ließen.\n\nWhirlwind diente als Basis für den Prototyp des Luftraumüberwachungssystems SAGE, das für die US-Luftwaffe entworfen wurde und Radarinformationen interaktiv in Computerdarstellungen umwandelte. SAGE wurde ab 1955 mit Hilfe eines Lichtgriffels bedient: zeigte man damit auf ein Luftfahrzeug, so lieferte das System nähere Informationen.\n\n1951 begann das Forschungslabor von General Motors zu prüfen, welche Rolle künftige CAD-Systeme spielen könnten. IBM stellte das IBM-740/780-System vor, das in Verbindung mit dem IBM-704-Großrechner Punkte oder Linien auf einem Röhrenbildschirm darstellen konnte.\n\nDie Forschungen von General Motors resultierten 1959 im ersten CAD-System, dem DAC-1, das von Don Hart und Ed Jacks in Zusammenarbeit mit IBM entwickelt und erst 1964 der Öffentlichkeit vorgestellt wurde. Dieses bahnbrechende System demonstrierte die Vorteile grafischer Interaktion für den Entwurfsprozess.\n\n1950 zeichnete der Künstler Ben Laposky mit Hilfe von Oszilloskopen, die von analogen Computern gesteuert wurden, abstrakte Figuren. 1958 entwickelte William Higinbotham eines der ersten Videospiele, Tennis for Two.\n\nMitte der 1960er-Jahre gab es bereits einige Forschungsprojekte und kommerzielle Produkte, die sich mit Computergrafik beschäftigten. Da zu dieser Zeit die Ein- und Ausgabe vor allem über Lochkarten lief, war die Hoffnung groß, dass sich interaktive Benutzerschnittstellen allgemein durchsetzen würden. Vorerst konnten sich jedoch nur wenige Firmen interaktive Computergrafik-Systeme leisten; erst gegen Ende des Jahrzehnts erschienen erschwinglichere Produkte. Nachdem akzeptable Ausgabegeräte verfügbar waren, wurden wieder Softwarefragen laut, da die Computergrafik kompliziertere Datenstrukturen als Arrays und verkettete Listen erforderte.\n\nIn den 1960er-Jahren gründeten sich die ersten Verbände von Forschern und Anwendern, die sich mit Computergrafik beschäftigten: Der \"Society for Information Display\" 1963 folgte das \"ACM Special Interest Committee for Computers\", das 1969 zur Themengruppe SIGGRAPH wurde. Ein Großteil der US-amerikanischen Forschung auf dem Gebiet der Computergrafik wie auch der Informatik überhaupt wurde durch die ARPA finanziert.\n\nEin größeres Hardwareproblem der frühen 1960er-Jahre war die Frage, wie sich Schriftzeichen und Linien auf einem Bildschirm ausgeben lassen konnten. Zeichengeneratoren waren nicht standardisiert und kosteten 2000 bis 10.000 US-Dollar.\n\nDie ersten Speicherröhren-basierten grafischen Computerterminals kamen 1968 auf den Markt: Das Unternehmen Computer Displays bot die \"Advanced Remote Display Station\" und Computek die 400er-Serie an. Beide Geräte basierten auf der Tektronix-611-Speicherröhre und kosteten 12.000 bis 15.000 Dollar. Daraufhin brachte Tektronix mit dem ca. 9000 Dollar teuren T4002A, gefolgt vom 4000 Dollar teuren 4010 eigene Produkte auf den Markt. Diese auf so genannten \"Direct-View Storage Tubes\" (DVSTs) basierenden Sichtgeräte konnten eine Grafik anzeigen, ohne dass ein periodisches Neuzeichnen notwendig war. Dreidimensionale Darstellungen waren mit ihnen nicht möglich. Ungeachtet ihrer primitiven Grafik ermöglichten sie zehntausenden von Anwendern den Zugang zur Computergrafik, da vor dem Aufkommen der DVSTs alleine die Hardware von Grafikterminals 50.000 bis 200.000 Dollar gekostet hatte.\n\nIn der Folge stiegen weitere Hersteller mit anderen Technologien wie Vektor- und Plasmabildschirmen in dieses vergleichsweise günstige Preissegment ein. Der erste experimentelle Bildspeicher (Framebuffer) für Rastergrafiken wurde 1969 von Bell Laboratories entwickelt. Er verwendete 3 Bit pro Pixel, entsprechend acht Grautönen.\n\nIvan Sutherlands 1963 in seiner Dissertation vorgestelltes Sketchpad-Zeichensystem war ein Meilenstein in der Geschichte der interaktiven Computergrafik. Dieses Programm erlaubte es, einzelne Zeichnungen als Vorlagen zu speichern und hierarchisch aufzubauen, sodass man eine Zeichnung aus bereits vorhandenen „Modulen“ zusammensetzen konnte. Sutherland führte außerdem neuartige Interaktionsmöglichkeiten ein, von denen viele noch heute in Gebrauch sind, zum Beispiel Kontextmenüs. Mit Tastatur und Lichtgriffel ließen sich Befehle erteilen, Objekte auswählen und Zeichnungen anfertigen. Sutherlands Dissertation legte das Augenmerk auf die Schaffung und Optimierung einer grafischen Datenstruktur.\n\nEinige Jahre später erweiterte Timothy Johnson das System um 3D-Fähigkeiten und nannte es Sketchpad 3. Dabei wurde der Bildschirm in die heutzutage immer noch gebräuchlichen drei Seitenansichten und eine Perspektivansicht unterteilt.\n\nAlan Kay entwickelte 1969 für den Xerox Alto die erste grafische Benutzeroberfläche, die auch den späteren Apple Macintosh erheblich beeinflusste.\n\nWilliam Fetter, ein Angestellter bei Boeing, verwendete 1960 die Bezeichnung \"computer graphics\" für seine Computerzeichnungen von Flugzeugcockpits mit Pilot. Zusammen mit Walter Bernhardt und anderen gab er die Punktkoordinaten eines Flugzeugmodells in eine Datenbank ein und druckte eine automatisch berechnete perspektivische Darstellung auf einem Plotter aus.\n\n1963 fertigte John Lansdown perspektivische Zeichnungen an einem Elliott-803-Computer an und schrieb eigene CAD-Programme. Edgar Horwood entwickelte für das US-Bauministerium ein Kartografiesystem. Am MIT begann Steven Anson Coons Techniken zur parametrischen Modellierung von Flächen zu entwickeln.\n\nEin frühes System für Virtuelle Realität, bei dem das erste Head-Mounted Display zum Einsatz kam, wurde von Ivan Sutherland und seinen Studenten entwickelt. Es zeigte mehr oder weniger einfache Drahtgittermodelle an.\n\nZusammen mit David Evans gründete Sutherland 1968 Evans & Sutherland, das erste Unternehmen, das sich auf Computergrafik spezialisiert hatte. 1969 stellte es LDS-1 (Line Drawing System-1) vor, das erste kommerzielle CAD-Grafiksystem, das Drahtgittermodelle anzeigte.\n\nFür Digitalplotter, bei denen der Stift auf einem Raster bewegt wurde, entstanden in den 1960er-Jahren Rasterungsalgorithmen, etwa 1962 der Bresenham-Algorithmus zur Rasterung von Linien.\n\nZur Lösung des Sichtbarkeitsproblems entwickelte John Warnock 1969 den Warnock-Algorithmus. Ungefähr zur selben Zeit wurde der Raytracing-Algorithmus entdeckt.\n\nVon 1961 bis 1962 entwickelten MIT-Studenten an einem PDP-1 das erste populäre Computerspiel, Spacewar. Ken Knowlton entwickelte 1963 die Programme BEFLIX und EXPOR, mit denen frühe computergenerierte Filme produziert wurden. Frieder Nake fertigte mit dem Z64 Graphomat vierfarbige Zeichnungen an. Im selben Jahr fand der erste Computerkunst-Wettbewerb statt, gesponsert von der Zeitschrift \"Computers and Automation.\" 1965 fand an der Technischen Hochschule Stuttgart die erste Computerkunst-Ausstellung statt, der weltweit viele weitere folgten. Ralph Baer entwickelte ab 1966 Odyssey, die erste kommerzielle Spielkonsole (später von Magnavox vermarktet). Ebenfalls 1966 produzierte John Whitney sr. den ersten digitalen computergenerierten Kurzfilm, \"Permutations\"; es folgten diverse weitere Computeranimationen wie etwa Charles Csuris \"Hummingbird\" (1967).\n\nWährend vorher Anwender die Software für ihre Grafikterminals selber schreiben mussten, änderte sich die Situation allmählich in den 1970er-Jahren. Diverse Softwarepakete, die das Zeichnen von Grafiken ermöglichten oder eine Benutzerschnittstelle bereitstellten, kamen auf den Markt. Außerdem wurden Komplettsysteme verkauft, die den Anwender fast vollständig von den Softwaredetails isolierten. Um der wachsenden Vielfalt der Hersteller und Technologien Herr zu werden, wurden Grafiknormen entwickelt. Die erste Konferenz der ACM SIGGRAPH fand 1974 statt und zog 600 Besucher an.\n\nUngefähr ab Mitte der 1970er-Jahre wurde Speicherhardware günstig genug, um pixelbasierte Rasterbildschirme mit mehreren hundert Bildzeilen anzusteuern. Trotz des unausweichlichen Treppeneffekts boten Rastergrafiken im Vergleich zu Vektorgrafiken eine Reihe von Vorteilen: flimmerarme Darstellung auch bei komplexen Grafiken, gefüllte Flächen und kostengünstige Bildschirme, die auch Farbe darstellen konnten. Gegen Ende des Jahrzehnts waren Rasterbildschirme beliebter als DVSTs und Vektorbildschirme.\n\nDer erste kommerzielle Framebuffer erschien 1973. Er wurde von Evans & Sutherland auf der Basis des experimentellen 3-Bit-Framebuffers von Bell Labs entwickelt. Den ersten 8-Bit-Framebuffer entwickelte 1978 Richard Shoup bei Xerox PARC.\n\nWährend sich die frühen Grafiksysteme nur über Tastaturen und Lichtgriffel bedienen ließen, setzten sich in den 1970er-Jahren eine Vielzahl weiterer Eingabegeräte wie Maus, Trackball, Grafiktabletts und berührungsempfindliche Geräte durch.\n\nUnter den Ausgabegeräten wurden elektrostatische Plotter verfügbar, die schnelle und hochwertige monochrome Ausgaben lieferten. Langsamere elektromechanische Plotter mit mehreren Stiften erlaubten auch Farbausgeben. Außerdem erschienen Diabelichter und Tintenstrahldrucker, die ebenfalls farbige Grafiken unterstützten.\n\nDie Entwicklung des Jahrzehnts, die die vielleicht weitreichendsten Auswirkungen haben sollte, war die Einführung des Personal Computers. Trotz seiner schlechten Grafik bewog er Hersteller dazu, günstige Plotter und Grafiktabletts herzustellen.\n\nHenri Gouraud und Bùi Tường Phong entwickelten für Evans & Sutherland die nach ihnen benannten Shadingverfahren Gouraud Shading und Phong Shading (1971 und 1975 veröffentlicht).\n\nEdwin Catmull beschrieb 1974 den Z-Buffer zur Verdeckungsberechnung sowie Texture Mapping, mit dem sich die Oberfläche von 3D-Modellen mit Bildern ausstatten lassen. Jim Blinn entwickelte 1976 Reflection Mapping, das Spiegelungen ermöglicht, und zwei Jahre später Bumpmapping zur einfachen Simulation von Oberflächenunebenheiten.\n\nUm 1979 entwickelten Douglas Scott Kay und Turner Whitted unabhängig voneinander rekursives Raytracing, eine Raytracing-Erweiterung, mit der sich Spiegelungen und Lichtbrechung simulieren lassen.\n\n1973 entstand das erste reale Objekt, das vollständig am Computer modelliert wurde. Es wurde vom Künstler Ronald Resch entworfen und stellte ein großes Osterei dar. Peter Foldes’ mit Hilfe der Tweening-Technik gezeichnete Kurzfilm \"Hunger\" war der erste vollständig animierte, computerbasierte Film, der ein Lebewesen darstellte. Der erste Spielfilm mit 3D-Computer Generated Imagery (CGI) war \"Futureworld\" (1976), der das Polygonmodell einer Hand und eines Kopfes zeigte. Das Polygonmodell der Hand stammt von Edwin Catmull, der 1972 seine linke Hand digitalisierte und animierte. Zur gleichen Zeit erstellte Fred Parke die erste Computergrafik eines menschlichen Gesichts (dem Gesicht seiner Ehefrau). Beide Filme wurden als Vorreiter der Computeranimation gefeiert und fanden Verwendung in Futureworld.\n\nMit der drastisch gestiegenen Systemleistung von PCs und Workstations hatten in den 1980er-Jahren auch Endbenutzer Zugang zur Computergrafik. In diesem sehr ereignisreichen Jahrzehnt gründeten sich viele bedeutsame Unternehmen mit Bezug zur Computergrafik wie Silicon Graphics (1981), Adobe, Autodesk und Sun Microsystems (1982), Aldus (1984) oder Softimage und Pixar (1986).\n\nIn den 1980er-Jahren waren High-End-Workstations in der Preisklasse 30.000 bis 100.000 Dollar in der Lage, plausible Bilder nahezu in Echtzeit zu rendern. Zur Verbesserung der Grafikleistung wurden Parallelprozessoren und Grafikbeschleuniger genutzt. Der erste kostengünstige Farb-Framebuffer für PCs wurde 1985 mit der \"TARGA\"-Grafikkarte von AT&T eingeführt. True-Color-Bildschirme mit über 1000 Bildzeilen wurden weithin verfügbar; Monochrombildschirme mit 3000 Bildzeilen waren für etwa 5000 Dollar zu haben. Die Maus entwickelte sich zum allgegenwärtigen Zeigegerät; die ersten Datenhandschuhe erschienen. Farbdrucker unterschiedlichster Typen eroberten den Markt.\n\nEffektive stereoskopische Anzeigegeräte wurden erschwinglich. Frühere Geräte waren unhandlich und basierten meistens auf Rot-Grün-Filtern. 1989 konnte man 3D-Brillen oder polarisierende Flüssigkristallbildschirme für 2000 US-Dollar erwerben. Anwendungen fanden derartige Geräte hauptsächlich in der wissenschaftlichen Visualisierung.\n\nGegen Ende des Jahrzehnts entstand ein überlebensfähiger Markt für optische Datenerfassungsgeräte und dazugehöriger Software. Der ursprüngliche Hintergedanke war, technische Zeichnungen einscannen, automatisch vektorisieren und in die damals üblichen CAD-Dateiformate umwandeln zu können. Später entwickelte sich das Einscannen von Bildern zur Hauptanwendung.\n\nCAD/CAM gelangte in den 1980er-Jahren zum vollen Durchbruch. Die Systeme begannen als Großrechner und wurden ab Mitte der 1980er-Jahre allmählich von Workstations abgelöst. Die erste Version der erfolgreichen AutoCAD-Software von Autodesk erschien 1982.\n\nDie ersten PC-Anwendungen wie Textverarbeitung und Tabellenkalkulation waren textbasiert. Mit grafikfähigen PCs kamen auch günstige Grafikanwendungen auf den Markt, darunter CAD/CAM-Anwendungen, Präsentations- und Malprogramme. Die ersten Grafikprogramme für handelsübliche PCs waren MacDraw und MacPaint, die 1984 zusammen mit dem ersten Macintosh eingeführt wurden. Zum Ende des Jahrzehnts war Grafiksoftware für alle Anwendungsbereiche verfügbar.\n\nFür den professionellen Markt entwickelten Alias Research und Wavefront Technologies eines der ersten 3D-Animationswerkzeuge.\n\n1985 verabschiedeten ANSI und ISO die erste 2D-Computergrafik-Norm, GKS, die konformen Anwendungsprogrammen gewisse Mindestanforderungen vorschrieb. Für 3D-Computergrafiken folgten 1988 GKS-3D und PHIGS. Daneben entstanden wichtige Industriestandards wie die Seitenbeschreibungssprache PostScript von Adobe oder X Window System, ein von einem MIT-Konsortium geleitetes System zur Bereitstellung grafischer Benutzeroberflächen.\n\nIBM veröffentlichte für den IBM-PC 1981 den ersten Farbgrafikstandard, CGA, der von EGA (1984) und VGA (1987) ersetzt wurde.\n\n1984 entwickelte eine Gruppe von Forschern der Cornell University Radiosity, zusammen mit Raytracing eines der beiden großen Verfahren zur Berechnung der Lichtverteilung und das erste Verfahren, das unter bestimmten Bedingungen globale Beleuchtung simulieren kann. Zwei Jahre später veröffentlichte Jim Kajiya die Rendergleichung, die die globale Beleuchtung auf ein mathematisches Fundament stellt, sowie den Path-Tracing-Algorithmus.\n\nIn den 1980er-Jahren begann die Computergrafik in größerem Ausmaß Anwendung in der Filmwirtschaft zu finden. Für den Film \"\" entstand 1982 die erste volldigitale, computergenerierte Bildsequenz, inspiriert von Jim Blinns Fly-By-Animationen der Raumsonde Voyager. Der im selben Jahr gezeigte Film \"Tron\" war der erste Spielfilm, der in größerem Umfang 3D-CGI verwendete. Es erschienen die ersten 3D-Videospiele, etwa \"Cube Quest\" (1983) von Simutrek. \"Taran und der Zauberkessel\" (1985) war der erste abendfüllende Zeichentrickfilm, der computergenerierte 3D-Elemente enthielt. Im selben Jahr wurde in \"Das Geheimnis des verborgenen Tempels\" zum ersten Mal ein computergeneriertes Wesen – ein gläserner Ritter – in einem Spielfilm gezeigt. \"Willow\" (1988) war der erste Spielfilm, der eine digitale Morphing-Sequenz enthielt.\n\nAb den 1990er-Jahren verwischten sich zunehmend die Grenzen zwischen Computergrafik und verwandten Gebieten wie Bildverarbeitung. Hatten beide Felder vorher noch unterschiedliche Workstation-Architekturen erfordert, so wurden nun die Rechner schnell genug, um beide Aufgaben zu bewältigen. Zusätzlich wurde es möglich, Video- und Toninhalte zu kombinieren. Nicht zuletzt wegen der Nachfrage nach Notebooks kamen zunehmend Flachbildschirme auf den Markt.\n\nCAD und Visualisierung drangen in immer mehr Anwendungsbereiche vor, etwa in die Architektur oder Medizin.\n\n1992 wurde die erste Version der 3D-Grafik-Programmierschnittstelle OpenGL von Silicon Graphics veröffentlicht.\n\nWährend zu Beginn der 1990er-Jahre noch wissenschaftliche und technische Anwendungen vorherrschten, vorschob sich das Gewicht im Laufe des Jahrzehnts zunehmend auf nichttechnische Anwendungsgebiete.\n\nEinen regelrechten Boom erlebten Computeranimationen für die Werbebranche sowie Animations- und Spielfilme. \"Bernard und Bianca im Känguruhland\" (1990) nutzte ausschließlich Disneys CAPS-Prozess und war somit der erste vollständig digital produzierte Zeichentrickfilm. Die computergenerierten, realistischen Dinosaurier in \"Jurassic Park\" (1993) setzten neue Maßstäbe für CGI. 1995 erschien \"Toy Story\", der erste vollständig computergenerierte Kinofilm.\n\n1996 stellte 3dfx mit \"Voodoo Graphics\" den ersten Grafikprozessor vor, der 3D-Hardwarebeschleunigung für den nichtprofessionellen Bereich anbot.\n\n\n\nDokumentarfilm:\n\n"}
{"id": "2810800", "url": "https://de.wikipedia.org/wiki?curid=2810800", "title": "IBM Roadrunner", "text": "IBM Roadrunner\n\nRoadrunner war der inoffizielle Name eines Supercomputers auf Basis eines IBM BladeCenter QS22 Clusters, der am Los Alamos National Laboratory im US-Bundesstaat New Mexico installiert war.\n\nMit einer Leistung von 1,026 Peta-FLOPS stellte der Roadrunner am 9. Juni 2008 einen neuen Rekord auf und kam auf Platz 1 der TOP500-Liste. Seitdem wurde das System auf den Endausbau von 296 Racks gebracht und behielt mit nun 1,105 Peta-FLOPS den ersten Platz bis zur Liste November 2009, in der er vom \"Jaguar\"-System überholt wurde. Der Zeitplan sah den Start des Alltagsbetriebs für Oktober 2009 vor. Insgesamt kostete das System rund 133 Millionen Dollar.\n\nIBM baute den Supercomputer für das US-Energieministerium (DOE). Er hat ein hybrides Design, bestehend aus 6480 AMD-Opteron-Prozessoren (3240 IBM LS21 Blade-Server, jeder enthält zwei Dual-Core-Opterons, durch HyperTransport miteinander verbunden). Allerdings war Roadrunner kein Zusammenschluss eines Cell-Clusters und eines Opteron-Clusters, sondern ein Opteron-Cluster, bei dem jedem Opteron-Kern ein Cell-Prozessor unterstellt war (durch PCI Express miteinander verbunden), der für ihn die mathematischen Berechnungen übernahm, was in einer Anzahl von 12.960 Cell-Prozessoren resultierte.\n\nDer Großteil der Rechenleistung stammte demnach auch von den Cell-Prozessoren: 1,3 PFLOPS Peak vs. 47 TFLOPS Peak der Opterons. Die dafür benutzte Prozessorarchitektur war die Cell Broadband Engine (Cell B. E.), die in Kooperation von Sony, Toshiba und IBM (STI) entstanden ist und auch in Sonys Spielkonsole PlayStation 3 verwendet worden ist. Im Roadrunner kam allerdings eine neue Cell-Version namens PowerXCell 8i zum Einsatz, die in der Lage war, zwei Double-Precision-Rechnungen in seinen 128-Bit-Registern bei einer Höchstgeschwindigkeit von etwas über 100 GFLOPS zu berechnen (zum Vergleich: die Playstation-3-Variante schaffte nur knapp 15 GFLOPS bei doppelter Genauigkeit).\n\nDer Roadrunner benutzte das Betriebssystem Red Hat Enterprise Linux und Fedora. Die Cell-Prozessoren wurden als IBM Cell-Blades geliefert und durch InfiniBand direkt mit den x3755 Opteron-Knoten verbunden. Das System wurde am 8. Juni 2008 in Betrieb genommen.\nDas US-Energieministerium hat den Roadrunner für Simulationen bezüglich der Alterung radioaktiver Substanzen verwendet, unter anderem auch bei Atomwaffen (vgl. Advanced Simulation and Computing Program). Es wurden Atomwaffentests simuliert und überprüft, ob das alternde Nuklearwaffenarsenal der Vereinigten Staaten sicher und zuverlässig ist. Andere Anwendungsgebiete des Computers schlossen Simulationen und Berechnungen für Wissenschaft, Finanzwirtschaft sowie für die Automobilindustrie als auch die Luft- und Raumfahrt ein.\n\nDas System ging am 31. März 2013 endgültig außer Betrieb. Begründet wurde dieses Vorgehen mit seiner schlechten Energieeffizienz, insbesondere im Vergleich zu anderen Supercomputern.\n\n"}
{"id": "2814410", "url": "https://de.wikipedia.org/wiki?curid=2814410", "title": "Numbers (Software)", "text": "Numbers (Software)\n\nNumbers ist ein Tabellenkalkulationsprogramm von Apple. Es wurde am 7. August 2007 vorgestellt und ergänzt die iWork-Suite, die außerdem die Programme Pages (Textverarbeitung) und Keynote (Präsentationsprogramm) enthält. Seit 2010 ist es auch für das iPad und seit 2011 auch für das iPhone und den iPod touch erhältlich.\n\nIm Gegensatz zu Microsoft Excel arbeitet Numbers nicht mit einer einzigen, großen Tabelle, sondern kann mehrere unabhängige Tabellen, Diagramme oder Bilder auf einer „Leinwand“ nebeneinander darstellen.\n\nNumbers kann Dokumente in das Format .xls umwandeln, sodass diese auch mit Excel geöffnet werden können. Ebenso ist es eingeschränkt möglich, Excel-Tabellen mit Numbers zu öffnen. Dies funktioniert bei einfachen Exceltabellen; aufgrund des geringeren Funktionsumfangs von Numbers kommt es bei komplexen Excelfunktionen aber zu Fehlermeldungen. Umwandlungen in die Formate PDF und CSV sind ebenfalls möglich.\n\n\n"}
{"id": "2817452", "url": "https://de.wikipedia.org/wiki?curid=2817452", "title": "Widerlegung (Schach)", "text": "Widerlegung (Schach)\n\nIm Schachspiel versteht man unter einer Widerlegung die Durchkreuzung einer Absicht durch einen Gegenzug oder eine Gegenstrategie. In der Eröffnungstheorie wird durch eine Widerlegung die bisherige Bewertung einer Variante auf den Kopf gestellt, der bislang nicht oder ungenügend untersuchte widerlegende Zug wird häufig Neuerung genannt.\n\nEine Widerlegung in der Schachkomposition weist einen Fehler im Gedankengang des Komponisten nach und entwertet damit seine Idee vollständig. Bei Verführungen handelt es sich um absichtlich vom Autor ausgelegte Irrwege, die ihrerseits jeweils eine (möglichst einzige) Widerlegung haben. Beispiel: Komposition im Artikel Gerhard Latzel: Sieben Springerabzüge werden widerlegt, nur der achte führt zum Matt.\n\nIm Computerschach spielt Widerlegung in der effektiven Variantenberechnung eine zentrale Rolle. Im Gegensatz zum Minimax-Algorithmus wird in der Alpha-Beta-Suche das Prinzip der Widerlegung integriert. Prinzipiell lassen sich in der Alpha-Beta-Suche zwei Arten von Widerlegung ausmachen:\n\nErsterer Fall ist im Alpha-Beta-Algorithmus durch die Bedingung α > β gegeben, letzterer durch die Bedingung α = β. Da praktisch beide Arten von Widerlegung gleich behandelt werden, wird die Bedingung zusammenfassend auf α ≥ β reduziert.\n\nEin spezieller abstrakter Fall von Widerlegung ergibt sich zudem in der Null-Zug-Suche:\n"}
{"id": "2820419", "url": "https://de.wikipedia.org/wiki?curid=2820419", "title": "Brasero (Brennprogramm)", "text": "Brasero (Brennprogramm)\n\nBrasero ist ein freies Brennprogramm für die GNU/Linux-Oberfläche Gnome. Ursprünglich wurde es unter dem Namen \"Bonfire\" entwickelt und wird von Philippe Rouquier und Luis Medinas betreut. Der Name \"Brasero\" stammt aus dem Spanischen und bedeutet \"Kohlebecken\".\nAb Version 2.26 von Gnome ist es dessen Standard-Brennprogramm.\n\n\nBrasero wurde ursprünglich nur für GNU/Linux entwickelt. Mittlerweile steht es jedoch auch für mehrere BSD-Derivate zur Verfügung.\n\n"}
{"id": "2821623", "url": "https://de.wikipedia.org/wiki?curid=2821623", "title": "Magische S-Abf-Taste", "text": "Magische S-Abf-Taste\n\nAls Magic SysRq Key (kurz für \"Magic Sysrequest key\", engl. „Magische S-Abf-Taste“) wird eine Reihe von Tastenkombinationen des Linux-Kernels mit der -Taste bezeichnet, mit denen sich verschiedene Funktionen – beispielsweise ein Neustart des Computers – ausführen lassen, wobei sich die einzelnen Funktionen auch nacheinander in sinnvoller Reihenfolge ausführen lassen. Das funktioniert auch noch, wenn der Computer auf andere Eingaben nicht mehr reagiert, der Kernel aber noch nicht abgestürzt ist. Häufig benutzt wird diese Art von „Klammergriff“, um einen Neustart durchzuführen, ohne Schäden am Dateisystem zu verursachen oder um einen nicht mehr reagierenden X-Server zu beenden.\n\nEin gerne verwendeter und in IT-Kreisen bekannter Weg, den Computer bei hängendem System neu zu starten, ist es, auf einem Linux-System die Tastenkombination Alt+S-Abf gedrückt zu halten und dabei nacheinander die Tasten , , , , und zu drücken. So werden die Dateisystemcaches sicher geleert (d. h. ihr Inhalt wird auf die Platte geschrieben), alle Dateisysteme sicher ausgehängt und damit eine Beschädigung des Dateisystems unterbunden. Eine weitere bekannte Tastenreihenfolge, RSEIUB, zieht den Sync vor. Ein Argument dagegen ist, dass die terminierten Prozesse eventuell noch Daten in den Cache schreiben, und der Sync somit zu früh kommt. Ein Argument dafür ist, dass der unmount-Befehl ohnehin die Platten synchronisieren sollte. Es existiert kein Konsens darüber, welche Kombination zu bevorzugen ist.\n\nDer gewünschte Befehl wird erteilt, indem man \"gleichzeitig\" auf der Tastatur  +  drückt. ist auf IBM-AT-kompatiblen Tastaturen eine Alternativbelegung der Taste .\n\nAuf Tastaturen für Deutschland drückt man \"gleichzeitig\" die Tasten  +  + .\n\nAuf Tastaturen für die Schweiz und englischsprachige Länder entspricht die Taste dem deutschen Pendant . Die Beschriftung ist zum Beispiel in der Schweiz . Die Kombination auf schweizerischer Tastatur ist: + .\n\nBesonders auf Laptops müssen wegen herstellerspezifischen Tastaturlayouts ggf. noch Zusatztasten gedrückt werden, so etwa auf Dell-Laptops zunächst , anschließend +++; auf z. B. ThinkPads kann folgende Abfolge verwendet werden: drücken und halten, + drücken und wieder loslassen, drücken, alle loslassen.\n\nDie für den entsprechenden Befehl notwendige Taste kann in der folgenden Tabelle abgelesen werden, Groß- und Kleinschreibung spielt keine Rolle. Die Angaben beziehen sich auf die in Deutschland übliche QWERTZ-Tastatur.\n\nDie genannten Befehle können auch über die Shell (und somit auch durch Skripte) ausgelöst werden.\nDazu wird in virtuelle Datei /proc/sysrq-trigger vom Kernel-Proc-API das entsprechende Zeichen geschrieben.\nDies ist nur als root möglich.\nDer folgende Befehl löst beispielsweise einen sofortigen Reboot aus.\n\necho b > /proc/sysrq-trigger\n\n"}
{"id": "2822474", "url": "https://de.wikipedia.org/wiki?curid=2822474", "title": "UNIVAC LARC", "text": "UNIVAC LARC\n\nDie UNIVAC LARC (\"Livermore Advanced Research Computer\") war ein erster Versuch, 1960 einen Supercomputer zu bauen. Die \"LARC\" besaß zwei CPUs. Insgesamt wurden zwei Maschinen gebaut, eine für die US Navy und eine für Lawrence Livermore National Laboratory. 1960 gehörte die \"LARC\" mit 500 kFLOPS zu den schnellsten Maschinen dieser Zeit.\n\nDas Lawrence Livermore National Laboratory setzte die Maschine zur Lösung von Differenzialgleichungen ein. Eine Besonderheit dieser Maschine war, dass die Peripherie (Input/Output) vollständig unabhängig von dem Hauptrechner war. Je eine CPU für die Arithmetischen Berechnungen und eine CPU für das Input/Output System.\n\n\n"}
{"id": "2822530", "url": "https://de.wikipedia.org/wiki?curid=2822530", "title": "UNIVAC 490", "text": "UNIVAC 490\n\nDas UNIVAC 490 Real Time System wurde 1961 durch Sperry Rand gebaut. Die \"Univac 490\" war ein System, welches wie ein Kommunikationsnetzwerk funktionierte. Hunderte von Empfänger- und Sendegeräte (Terminals) konnten geographisch verstreut platziert werden. Diese Kommunikationsgeräte übermittelten nun Daten an die Zentrale Prozessoreinheit, welche die Berechnungen ausführte. Diese Architektur führte zu einer geographisch verteilten, transaktionsbasierenden Echtzeit-Umgebung. \n\nEigenschaften der \"UNIVAC 490 RTS\"\n\n\n"}
{"id": "2824164", "url": "https://de.wikipedia.org/wiki?curid=2824164", "title": "Gjiten", "text": "Gjiten\n\ngjiten ist ein auf GTK+/GNU/Linux basierendes Wörterbuch für Japanisch. Es ist als freie Software zu den Bedingungen der GNU General Public License (GPL) veröffentlicht. Man kann damit sowohl japanische Eingaben nachschlagen als auch in lateinischen Buchstaben geschriebene Abfragen absetzen. Da eine Vielzahl von Wörterbuch-Dateien installiert werden kann, ist Gjiten prinzipiell nicht auf Übersetzungen von und nach japanisch von oder nach einer bestimmten Sprache beschränkt.\n\nEin Kanji-Lexikon ist vorhanden, das umfangreiche Informationen zum nachgeschlagenen Kanji liefert wie gängige und seltene Lesungen, Radikal-Angaben oder den Unicode-Wert (hexadezimal). Zusätzlich ist mit „KanjiPad“ ein Eingabefeld für grafischen Input (Maus | Tablet | Touchpad) vorhanden, das fehlertolerant handgeschriebene Zeichen sucht und Vorschläge für die Zwischenablage unterbreitet, die in den anderen Programmteilen dann verwendet werden können.\n\n\nGjiten ist in diversen Distributionen verfügbar, aber auch als einzelnes Projekt:\n"}
{"id": "2825069", "url": "https://de.wikipedia.org/wiki?curid=2825069", "title": "Open Invention Network", "text": "Open Invention Network\n\nDas Open Invention Network (OIN; deutsch \"offenes Innovationsnetzwerk\") ist ein Industrie-Konsortium, das Softwarepatente erwirbt und diese jedem Unternehmen sowie jeder Privatperson frei zur Verfügung stellt, welche sich bereit erklären, keine Patentansprüche gegen das freie und quelloffene Betriebssystem GNU/Linux, noch gegen irgendwelche andere Software, welche mit Linux in Verbindung steht, geltend zu machen.\n\nDas Konsortium wurde am 10. November 2005 von IBM, NEC, Novell, Philips, Red Hat und Sony in New York City gegründet. Seit 2008 ist der ehemalige Hedgefonds-Manager Keith Bergelt CEO der Open Invention Network LLC.\n\nAm 26. März 2007 schloss sich \"Oracle\" dem OIN an. Am 7. August 2007 schloss sich auch Google Inc. dem OIN an. Seit Dezember 2010 sind auch The Document Foundation (Berliner Stiftung bürgerlichen Rechts und Herausgeberin von LibreOffice) und das KDE-Projekt mit dabei sowie seit Anfang Mai 2011 der europäische Open-Source-Softwarehersteller Univention. Microsoft trat im Oktober 2018 bei.\n\n"}
{"id": "2825639", "url": "https://de.wikipedia.org/wiki?curid=2825639", "title": "Präfixsumme", "text": "Präfixsumme\n\nIn der Informatik ist die Präfixsumme einer Folge von Zahlen \"a\", \"a\", \"a\", … die Zahlenfolge \"s\", \"s\", \"s\", … ihrer Partialsummen:\n\nBeispielsweise ist die Präfixsumme der natürlichen Zahlen die Folge der Dreieckszahlen:\nDie Präfixsumme ist mit einer einfachen Schleife sequenziell berechenbar, indem mit der Formel\nfür \"i\">0 jeder Summenwert sukzessive aufaddiert wird. Die Präfixsumme bildet die Grundlage für Algorithmen wie den Countingsort.\nSie kann statt der Summierung als Basisoperation eine allgemeine assoziative binäre Operation verwenden, womit sie beispielsweise zur Polynominterpolation oder zur Stringbearbeitung eingesetzt\nund in der funktionalen Programmierung auf Funktionen höherer Ordnung angewandt werden kann; in diesem Falle wird sie auch Scan genannt. Da die Präfixsumme mit dem Fork-join-Modell zudem effizient auf Mehrkernprozessorsystemen und Rechnerclustern berechnet werden kann, spielt sie in Betrachtungen zu parallelen Algorithmen eine wichtige theoretische und praktische Rolle, als zu lösendes Testproblem ebenso wie als Subroutine wichtiger paralleler Algorithmen.\n\nMathematisch kann die Berechnung der Präfixsumme von endlichen auf unendliche Folgen verallgemeinert werden. Sie stellt dann eine Reihe dar. Die Präfixsummierung ist ein linearer Operator auf einem Vektorraum endlicher oder unendlicher Folgen. Seine Inverse ist ein Differenz-Operator.\n\nDie eigentliche Präfixsumme basiert auf der binären Operation der Addition. In der funktionalen Programmierung kann die Präfixsumme auf jede binäre assoziative Operation verallgemeinert werden, also statt einer Präfix\"summe\" eine Präfix\"operation\" darstellen. (Allerdings wird oft der Begriff „Präfixsumme“ auch dafür verwendet.) Die aus dieser Verallgemeinerung resultierende Präfixoperation ist eine Funktion höherer Ordnung und wird auch Scan genannt. Zum Beispiel kann die Folge der Fakultäten (\"a\") als Präfixprodukt berechnet werden, indem die Addition durch die Multiplikation ersetzt wird:\n\nDie Scan-Operation ist daher ähnlich der Reduce-Operation, allerdings liefert \"Scan\" die gesamte Folge der Partialoperationen, während \"Reduce\" nur den Wert der letzten Partialoperation als Ergebnis liefert.\n\nIn Haskell gibt es zwei Varianten von \"Scan\", nämlich codice_1 und codice_2. Die prozeduralen MPI-Bibliotheken bieten eine Operation codice_3 an, um eine Scan-Operation zwischen vernetzten Processoreinheiten zu berechnen. Die Programmiersprache C++ hat eine Funktion codice_4 in ihrer Standardbibliothek, welche, trotz ihres Namens, eine Scan-Operation mit einer beliebigen binären Operation ausführt. Die Programmiersprache Java bietet (ab der Version 1.8) in dem Standardpaket codice_5 eine Methode codice_6 in mehreren Varianten an, die neben dem zu bearbeitenden Array einen binären Operator erwartet. Die Präfixsumme eines Arrays codice_7 wird damit durch die folgende Anweisung berechnet und in dem Eingabearray gespeichert:\n\nArrays.parallelPrefix(a, (x,y) -> x + y);\n\nDer binäre Operator ist hier als Lambda-Ausdruck, also eine anonyme Funktion mit zwei Parametern angegeben. Entsprechend kann die Fakultät nach obigem Beispiel als Präfixprodukt\n\nArrays.parallelPrefix(a, (x,y) -> x * y);\n\nprogrammiert werden.\n\nMit Hilfe des Fork-join-Modells kann eine Präfixsumme mit den folgenden Schritten effizient parallel berechnet werden.\nHat die Eingabefolge \"n\" Einträge, so schreitet die Rekursion bis zu einer Tiefe von \"O\"(log \"n\") fort, was ebenso die Begrenzung der parallelen Laufzeit darstellt. Die Anzahl der Schritte des Algorithmus beträgt \"O\"(\"n\") und kann auf einer PRAM mit \"O\"(\"n\"/log \"n\") Prozessoren implementiert werden, indem in Runden, in denen mehr Einträge als Prozessoren vorhanden sind, einem Prozessor einfach mehrere Indizes zugewiesen werden.\n\nParallele Algorithmen für Präfixsummen können auf andere Präfixoperationen (Scans) assoziativer binärer Operationen verallgemeinert werden. Sie laufen effizient auf paralleler Hardware wie Mehrkernprozessoren, GPU's oder Rechnerclustern ab. Viele parallele Implementierungen verwenden dazu zwei Durchgänge: im ersten Durchgang werden die partiellen Präfixsummen auf jeder Prozessoreinheit berechnet; die Präfixsumme dieser Teilsummen wird dann berechnet und zum zweiten Durchgang an die einzelnen Prozessoreinheiten zurückgesendet, die nun mit dem bekannten Präfix als Anfangswert weiterrechnen. Asymptotisch angenähert erfordert diese Methode etwa zwei Lese- und eine Schreiboperation pro Eintrag.\n\n\n"}
{"id": "2826309", "url": "https://de.wikipedia.org/wiki?curid=2826309", "title": "Clipping (Computergrafik)", "text": "Clipping (Computergrafik)\n\nAls Clipping oder Abschneiden (englisch \"to clip\" = „abschneiden“, „kappen“) bezeichnet man in der Computergrafik das Abschneiden von Grundobjekten am Rand eines gewünschten Bildschirmausschnittes oder Fensters. Ein Fenster kann dabei ein beliebiges Polygon sein.\n\nGegeben sind die beiden Endpunkte einer Strecke formula_1 und ein Fenster formula_2. Ziel ist es nun, den Teil der Strecke formula_3 zu bestimmen, der sich innerhalb des Rechteckfensters befindet. Es sind vier Fälle möglich:\n\nSolche Probleme lösen die Algorithmen von Cohen-Sutherland, Cyrus-Beck und von Liang-Barsky.\n\nEs liegt wiederum eine Strecke mit Endpunkte formula_4 und formula_5 wie oben vor. Zur Berechnung der Anteile, die innerhalb eines Polygons liegen, werden die Schnittpunkte der Strecke mit dem Polygonfenster unter Berücksichtigung des Inside Test und des Max-Min Test bestimmt. Diese Vorgehensweise vollzieht der Scanline-Algorithmus zum zeilenweisen Zeichnen von Polygonen. Es werden Zeile für Zeile Punkte formula_4 und formula_5 mit formula_16 gewählt, die sich links oder rechts neben den Polygon befinden. Durch Clipping dieser Linie am Polygon entsteht zeilenweise der zu zeichnende Polygonanteil einer kompletten Zeile.\n\nPolygonclipping ist das Clipping eines beliebigen Polygons bezüglich eines Rechteckfensters oder beliebigen Polygons. Es treten verschiedene Situationen auf:\nDiese Probleme deckt der Algorithmus von Sutherland-Hodgman ab. Er betrachtet dabei ein Polygon als eine Sequenz von Punkten, die er abläuft und dann\n\n\n\n"}
{"id": "2829077", "url": "https://de.wikipedia.org/wiki?curid=2829077", "title": "Magix Music Maker", "text": "Magix Music Maker\n\nDer Magix Music Maker ist eine Digital Audio Workstation, die vom Unternehmen Magix für den Konsumentenbereich konzipiert wurde. Er wurde als Laienversion der von den gleichen Entwicklern stammenden Profi-Musiksoftware Samplitude entworfen und erschien erstmals 1994. Wie Samplitude kann der Music Maker bei der Musikproduktion Funktionen eines Sequenzerprogramms übernehmen, der Fokus der Software liegt inzwischen jedoch stärker auf der Verwendung vorgefertigter Loops. Mit mehr als einer Million verkauften Exemplaren gilt der Music Maker als Europas erfolgreichstes Musikprogramm.\n\nNeben dem Aneinanderreihen von vorgefertigten Sound Loops (meist im OGG- oder WAV-Format) beinhalten die neueren Versionen des Music Maker auch eine ASIO-Schnittstelle. Ebenso lassen sich VST-Synthesizer in das Programm einbinden. Weitere Ausstattungsmerkmale sind:\n\n\n\n\n\n2003 veröffentlichte die deutsche Band Wolfsheim eine besondere Version ihres Nummer-eins-Studioalbums Casting Shadows, welche die Software des \"Magix Music Maker\" beinhaltete.\n\n"}
{"id": "2834379", "url": "https://de.wikipedia.org/wiki?curid=2834379", "title": "Legion of Doom", "text": "Legion of Doom\n\nDie Legion of Doom (später LOD/H) war eine US-amerikanische Hacker- und Phreakergruppe, die von 1984 bis in die frühen 1990er Jahre existierte. \n\nIm Sommer 1984 gründete sich die Gruppe um einen Hacker mit dem Pseudonym „Lex Luthor“. Vorbild für die Namensgebung war eine Gruppe von Superman-Gegnern namens „Legion of Doom“ um die Comic-Figur Lex Luthor. Eine eher Computer-orientierte Untergruppe namens „Legion of Hackers“ wurde später assimiliert, daher stammt auch die dann verwendete Kurzform LOD/H.\n\nDie Legion of Doom war Herausgeber des Untergrund-Magazins \"LOD Technical Journals\", einer elektronischen Publikation, in der Hack-Methoden und -Ergebnisse ebenso vorgestellt wurden wie Phreaking-Tipps bis hin zur Anleitung zum Bau einer Bluebox. Das Journal sollte ursprünglich regelmäßig erscheinen, wurde aber letztendlich zwischen 1987 und 1990 nur viermal fertiggestellt.\n\nDie Vereinigung spaltete sich später in zwei Fraktionen auf. Das Mitglied Mark Abene gründete die Hackergruppe \"Masters of Deception\". Ihm folgten weitere Mitglieder von LoD. In der Folge der Teilung kam es zu Rivalitäten zwischen den beiden Gruppen, die als \"Great Hacker War\" bekannt wurden.\n\nIn den frühen 1990er Jahren begann die USA-weite \"Operation Sundevil\" durch den Secret Service und das FBI gegen Hackergruppen vorzugehen. Im Verlauf wurden viele Mitglieder der „Legion of Doom“ verhaftet – viele für Taten, die für Hacking und Phreaking typisch sind, wie die Übernahme von Computern oder Telefonleitungen, einige allerdings auch für Kreditbetrug zur persönlichen Bereicherung. Letztlich war die Operation jedoch nicht sehr erfolgreich, viele Beschuldigte wurden freigesprochen. In einem Fall warf das Gericht im Urteil dem Secret Service „schlampige“ (\"sloppy\") Vorbereitung vor und empfahl „bessere Schulung“ (\"better education\").\n\n\n"}
{"id": "2835988", "url": "https://de.wikipedia.org/wiki?curid=2835988", "title": "TermBase eXchange", "text": "TermBase eXchange\n\nUrsprünglich ein Standard der Localization Industry Standards Association (LISA), nahm sich die ISO des Standards an und überarbeitete und spezifizierte ihn in ISO 30042, welcher sich auf ISO 12620, ISO 12200 und ISO 16642 stützt.\n\nBasis für die Sprache waren\n\ndie im Format\n\n\nausgetauscht werden.\n\n"}
{"id": "2836363", "url": "https://de.wikipedia.org/wiki?curid=2836363", "title": "Mockingboard", "text": "Mockingboard\n\nDie Soundkarte Mockingboard (Wortspiel mit dem englischen „mockingbird“ – Spottdrossel) war eine Erweiterungskarte der Firma \"Sweet Micro Systems\", für den Heimcomputer Apple II bzw. Apple IIc und eine der weltweit ersten Soundkarten für Privatanwender überhaupt. Sie wurde in verschiedenen Varianten verkauft; die Standardvariante bot sechsstimmige Soundsynthese und ließ sich um eine Sprachsynthese-Erweiterung aufrüsten.\n\nDer Apple II verfügte von Haus aus nur über sehr beschränkte Möglichkeiten zur Sound-Ausgabe. Seine Soundschnittstelle bestand aus einem eingebauten Lautsprecher, dessen Membran per Software-Zugriff auf eine bestimmte Hardware-Adresse zwischen zwei räumlichen Positionen hin- und hergeschaltet werden konnte – im Wesentlichen entsprach das einer noch primitiveren Variante des späteren IBM-PC-Systemlautsprechers. Über geeignete Programme konnte dieser Ausgang in den gewünschten Frequenzen umgeschaltet werden und so Töne ausgeben. Allerdings besaß der Apple II keine programmierbaren Frequenzgeneratoren, Timer oder Interruptquellen, die CPU musste sich daher um die gesamte Tonerzeugung kümmern – was Soundprogramme sehr kompliziert machte und die Ausführung anderer Aufgaben (etwa die Änderung des dargestellten Bildschirminhalts) während einer Tonausgabe praktisch verhinderte.\n\nDas Mockingboard basierte auf dem Soundchip AY-3-8910, der drei unabhängige Rechteckgeneratoren und einen durchstimmbaren Rauschgenerator mitbrachte. Dieser Chip wurde in vielen Spielekonsolen der 1980er und später auch im Atari ST verbaut, so dass ein Mockingboard in der Grundausstattung einen vergleichbaren Sound lieferte. Angesteuert wurde der Soundchip über den Interface-Chip VIA 6522, der zugleich die beim Apple II fehlenden Timer und Interruptquellen nachrüstete. Ein 0,5-W-Ausgangsverstärker zum Anschluss an einen 8-Ohm-Lautsprecher komplettierte die Schaltung.\n\nVarianten des „Mockingboards“ brachten mehr als einen Soundchip mit und boten außerdem die Möglichkeit, den Sprachsynthese-Chip Votrax SC-01 nachzurüsten. Später wurde die weitgehend kompatible Chip-Variante SC-223 eingesetzt.\n\n"}
{"id": "2838608", "url": "https://de.wikipedia.org/wiki?curid=2838608", "title": "&amp;RQ", "text": "&amp;RQ\n\n&RQ (gesprochen: änderkiù) ist ein Open-Source-Client für das OSCAR-Protokoll aus dem Jahr 2000. Die Entwicklung wurde 2002 eingestellt, das Programm wird allerdings von einzelnen Programmierern in unabhängigen Projekten weiterentwickelt.\n\nDie Urversion von &RQ erschien Ende 2000 und fand recht bald großen Anklang in der ICQ-Community. Vor allem in den russischsprachigen Ländern und in Italien wurde der Client zu einem der gängigsten, was ihm aufgrund seines Logos, welches eine Ratte darstellt, auch zum Namen „Крыса“ (dt: Ratte) verhalf. In Deutschland blieb der Client zunächst recht unbekannt. Es erschien nie eine Final-Version, die Entwicklung stoppte bei 0.9.4.16.\n\nDa die Entwicklung an &RQ schon im Jahr 2002 eingestellt wurde, fanden sich weitere Entwickler, die eigenständige Derivate von &RQ aufbauten, um dieses an das neue ICQ-Protokoll anzupassen und weiterhin zu verbessern.\n\nandrq ist ein Derivat aus dem Jahre 2004. Es sollte mit der Genehmigung des ursprünglichen Autors der offizielle Nachfolger sein. Anders als z. B. R&Q behielt andrq die simple Handhabung von &RQ. Der Client beinhaltet neben den typischen Funktionen vom Vorgänger auch eine komplexe Event- und Packetlog und ein erweitertes Pluginsystem. Das Programm richtet sich mehr an fortgeschrittene Benutzer.\n\nIMadering (vormals Blackrat oder CasperICQ) ist ein Derivat aus dem Jahr 2006. Dieser Client hat ein sehr breites Funktionsspektrum.\nNeben den üblichen Funktionen von &RQ bietet das Programm unter anderem die Möglichkeit an, seine UIN endgültig vom Server zu löschen. Eine weitere neue Funktion ist das interne Weiterleiten, mit der sich ein Bot in eine andere UIN einloggt und alle eingehenden Nachrichten an die aktive UIN in IMadering sendet. IMadering verfügt über eine Konsole, welche den gesamten Datenverkehr zwischen Client und Server anzeigt und auch in eine Logdatei speichern kann. Außerdem unterstützt IMadering auch das XMPP-Protokoll. Wie auch R&Q besitzt IMadering kaum Ähnlichkeit mehr zur Grundversion.\n\nR&Q ist das bekannteste Derivat, welches 2004 erschien. Mit R&Q wurde eine konkurrenzfähige Version von &RQ geschaffen, welche die gängigsten Funktionen wie unter anderem xStatus, unsichtbare Kontakte finden und Proxyunterstützung unterstützt. R&Q besitzt außerdem eine Pluginfunktion, mit welcher man eigene Plug-ins oder vorgefertigte von der Entwicklerwebseite einfügen kann. Es besitzt fast keine Ähnlichkeit mehr zur Grundversion.\n\nOpen R&Q ist ein plattformunabhängiger Lazarus-Port von R&Q im Pre-Alpha-Status und noch nicht für den Produktionsbetrieb geeignet. Der Client basiert auf dem Sourcecode von R&Q und ist unter der GPL veröffentlicht. Das Interface besteht entweder aus GTK+ oder WinAPI.\n\n"}
{"id": "2839460", "url": "https://de.wikipedia.org/wiki?curid=2839460", "title": "Remington Rand 409", "text": "Remington Rand 409\n\nDie Remington Rand 409 wurde 1949 entworfen. Sie war einer der ersten Lochkartenmaschinen welche Röhren zur Speicherung von Zwischenresultaten verwendeten. Gebaut wurde die Maschine unter den Namen UNIVAC 60 (1952) und UNIVAC 120 (1953). Die Nummer in der Bezeichnung spiegelte die Anzahl Dezimalzeichen welche als Speicher zur Verfügung standen.\n\nDie Produktion der UNIVAC 60 und UNIVAC 120 wurde 1962 eingestellt und die Modelle wurden durch die UNIVAC 1004 abgelöst.\n\nSämtliche Aufzeichnungen über diese Modelle wurden durch Remington Rand vernichtet.\n\nZahlen wurden mittels Festkommazahlen abgebildet. Die Länge der Zahl hinter dem Komma konnte eine variable Länge bis Maximum 10 Stellen einnehmen. Arithmetische Berechnungen wurden in Gleitkommazahlen abgefertigt und anschließend für die Speicherung in Festkommazahlen gewandelt.\n\nZahlen wurden im \"Bi-Quinary Dezimalcode\" abgebildet. Jede Zahl im Speicher wurde mittels 5 Vakuumröhren abgebildet, welche die Zustände 1,3,5,7 und 9 repräsentierten.\n\n\n"}
{"id": "2839710", "url": "https://de.wikipedia.org/wiki?curid=2839710", "title": "Naval Tactical Data System", "text": "Naval Tactical Data System\n\nNaval Tactical Data System, abgekürzt NTDS, bezeichnet ein elektronisches Computersystem, welches durch Remington Rand (Sperry Rand) und der US Navy 1950–1960 entwickelt wurde und ab 1960 in Kriegsschiffen eingesetzt wurde.\n\nKriegsschiffe besitzen Operationszentralen \"(engl. Combat Information Centers, CIC)\". In diesen Räumen werden alle Informationen bezüglich Positionen von Flugzeugen, Schiffen und U-Booten zusammengeführt, um daraus ein Lagebild zu erstellen und zu pflegen. Dieses Lagebild dient der Schiffsführung als Grundlage für taktische Entscheidungen und wurde bis zum Einsatz von Rechnern z. B. auf einem Plotboard (Plexiglastafel mit Beleuchtung) manuell geführt. Es war auf einem Kriegsschiff üblicherweise nur für eine begrenzte Gruppe - i. d. R. die Schiffsführung und -Einsatzleitung - sichtbar und bewertbar. 1950–1960 wurden mit der Miniaturisierung der Computer Vakuumröhren gegen Transistoren ausgetauscht. In diesem Zuge wurden bis dahin manuell durchgeführte Prozesse automatisiert und die \"NTDS\"-Systeme eingeführt. Diese Rechner ermöglichten die Erstellung eines rechnergestützten Lagebildes, das direkt auf den Konsolen der Entscheidungsträger angezeigt werden konnte und damit einen einheitlichen Informationsstand sicherstellte. Weiterhin konnte der Einsatz von Effektoren (z. B. Feuerleitgeräten, Waffen usw.) automatisiert werden. Mittels des Datenfunks Link 11 tauschten die einzelnen Schiffe in einem Verband Informationen aus und erlaubten eine schnellere Auswertung der Informationen sowie die Ausdehnung des Lagebildes auch auf außerhalb des Erfassungsbereiches der eigenen Sensoren liegenden Bereiche - zumindest solange eine Plattform im Link Netz diesen Raum mit ihren Sensoren abdeckte. Das \"NTDS\" war der Vorgänger des heutigen auf US-Kreuzern und -Zerstörern verwendeten AEGIS.\n\nDer Einsatz der elektronischen Systeme erlaubte vor allem eine schnellere Auswertung der Situation als auch eine Fehlerreduzierung bei der Auswertung der Informationen.\n\nEine Vielzahl von eingebetteten UNIVAC-Computersystemen, typischerweise mit 30-Bit-RISC-Prozessoren und 32K Trommelspeicher, wurden dafür eingesetzt. Die Systeme verfügten über 16 I/O-Peripherieanschlüsse, über welche die entsprechenden Sensoren wie Radarsysteme angebunden werden konnten. Die Computer wurden mittels Wasser gekühlt. Die Systeme wurden auch als \"UNIVAC-NTDS\" oder \"UNIVAC 1206/8\" bezeichnet.\n\nDie letzte Rechnergeneration der NTDS Technologie ist der \"AN/UYK-43\", ein wassergekühlter Multiprozessorkernrechner, der von Lockheed Martin im Auftrag der US Navy produziert wird. Der Rechner dient u. a. zur Steuerung des SPY-1 Radars, in der deutschen Marine wird er im Führungssystem der Fregatte Klasse F123 eingesetzt.\n\nFür das NTDS wurde eine proprietäre Schnittstellenarchitektur gleichen Namens entwickelt, die im US \"MIL-STD-1397\" beschrieben ist. Es handelt sich dabei um eine - aus heutiger Sicht relativ langsame - parallele Schnittstelle, mit der die verschiedenen Komponenten eines NTDS verbunden wurden. Um auch modernere zivile Hardware anbinden zu können, werden NTDS-Schnittstellenadapter mittlerweile auch als PCI-Einbaukarten für PC-Systeme angeboten, z. B. von der Firma GET.\n\nDie NTDS-Schnittstelle gilt mittlerweile als veraltet, da sie im Vergleich zu neueren Technologien (z. B. FDDI, MIL-Bus, ATM oder Ethernet) nicht netzwerkfähig ist und relativ aufwändig zur Anpassung an die jeweilige Anwendung programmiert werden muss. Dennoch werden nach wie vor viele militärische Geräte wie Radaranlagen oder Flugkörpersysteme mit dieser Schnittstelle angeboten.\n\nSeymour Cray hat das Design eines der ersten Systeme, des AN/USQ-17 entworfen. Durch den Abgang von Seymour Cray bei Sperry Rand wurde das ursprüngliche Design neu entworfen und unter der Bezeichnung AN/USQ-20 ausgeliefert. Crays Originaldesign wurde nie in der Produktion umgesetzt.\n\n\n\n"}
{"id": "2842642", "url": "https://de.wikipedia.org/wiki?curid=2842642", "title": "UNIVAC Athena", "text": "UNIVAC Athena\n\nDie UNIVAC Athena, auch ATHENA genannt, war ein UNIVAC-Computersystem, das von Sperry Rand 1955 gebaut wurde.\n\nDas Athena-System, benannt nach der griechischen Kriegsgöttin Athene, wurde für die US Air Force im Zuge des Titan-ICBM-Programms (intercontinental ballistic missile) entwickelt, und als auf dem Boden stationiertes Raketensteuerungssystem eingesetzt. Dabei synchronisierte sich das Computersystem mit den jeweiligen Abschussbasen und koordinierte die Flugbahn der Raketen.\n\nDer Chefentwickler der UNIVAC Athena war Seymour Cray. Es wurden für das Athena-System zwei Prototypen entwickelt. Ein System basierend auf Magnetschalter, genannt \"MAGTEC\" und ein System basierend auf Transistoren, genannt \"TRANSTEC\". Beide Testsystem verfügten über den gleichen Befehlssatz. Nach ausgiebigem Testen entschied sich Seymour Cray für die auf Transistoren basierende Version.\n\nInsgesamt wurden 23 Systeme an die US Air Force geliefert.\n\n"}
{"id": "2843225", "url": "https://de.wikipedia.org/wiki?curid=2843225", "title": "Sprachsteuerung", "text": "Sprachsteuerung\n\nAls Sprachsteuerung bezeichnet man die Übermittlung von Befehlen an technische Geräte, die per Stimme erfolgt.\n\nGrundsätzlich kann das Prinzip der Sprachsteuerung bei einer sehr großen Zahl von Gerätetypen zum Einsatz kommen. Voraussetzung ist, dass es ein Modul für Spracherkennung gibt, das sprachliche Äußerungen aufnehmen und interpretieren kann.\n\nEs gibt Software, mit der das Betriebssystem eines PCs alternativ zu der Steuerung über gewöhnliche Eingabegeräte wie Maus oder Tastatur auch durch Sprachbefehle gesteuert werden kann.\n\nEin aktuell zunehmendes Einsatzgebiet findet sich auf Smartphones in Form von Intelligenten Persönlichen Assistenten.\n\nSprachsteuerung wird außerdem auch bei Navigationssystemen in Pkws eingesetzt. Der Fahrer muss nicht mehr mit der Hand per Tasten, Touchscreen oder Scrollrad seine Zielroute eingeben, sondern kann die Befehle und ebenso die Straßen- und Ortsnamen per Spracheingabe an das Gerät übermitteln.\n\nLaut einer repräsentativen Umfrage nutzten Anfang 2016 52 Prozent der Deutschen, die ein Smartphone verwenden, die Sprachsteuerung ihres Geräts.\nDie häufigsten Einsatzgebiete sind dabei das Anrufen von Kontakten (76 Prozent), Textnachrichten diktieren (54 Prozent) und Suchanfragen zur Internetrecherche (31 Prozent).\n\n\n"}
{"id": "2845785", "url": "https://de.wikipedia.org/wiki?curid=2845785", "title": "Terra (Film)", "text": "Terra (Film)\n\nBattle for Terra (OT: \"Terra\") ist ein Animationsfilm aus dem Jahr 2007. Regie führte Aristomenis Tsirbas, der außerdem das Drehbuch verfasste. Terra war ursprünglich ein Kurzfilm von 2003, der nun für die große Leinwand umgesetzt wurde. Der Film wurde zum ersten Mal am 8. September 2007 auf dem Toronto International Film Festival vorgeführt. Während der Dreharbeiten wurde eine zweite Kamera eingesetzt, aus deren Aufnahmen später eine 3D-Fassung entstand.\n\nAuf einem fremden Planeten, der im späteren Verlauf des Films von den Menschen als Terra bezeichnet wird, lebt eine außerirdische Spezies in einer pazifistischen und naturverbundenen Kultur auf relativ primitivem Niveau. Die einzig bemerkenswerte Errungenschaft sind motorisierte Fluggleiter, mit denen die Außerirdischen ihre natürliche Flugfähigkeit erweitern.\n\nAls eines Tages den Außerirdischen überlegene Raumschiffe auftauchen und Bürger entführen, verfolgt die technikbegabte Mala eines der Raumschiffe, da auch ihr Vater entführt wurde. Sie schafft es durch Nutzung ihrer Ortskenntnisse, das Raumschiff in einem Unfall zu verwickeln und so zur Landung zu zwingen. Als Mala sich dem Raumschiff nähert, wird sie vom Piloten mit einer Waffe bedroht, jedoch wird dieser kurz daraufhin bewusstlos, und es wird ersichtlich, dass es sich bei der angreifenden Spezies um Menschen handelt.\n\nMala bringt den Piloten zu sich nach Hause und wird dort vom Assistenzroboter des Piloten, Giddy, der sie nach Hause verfolgt hat, gebeten den Piloten, Lieutenant James „Jim“ Stanton, zu retten, da seine Sauerstoffversorgung zu versagen droht und die Atmosphäre des Planeten für Menschen giftig ist. Als Bedingung für die Hilfe bei der Rettung bringt Giddy Mala die menschliche Sprache per Download ins Gehirn bei. Daraufhin bauen die beiden ein Zelt und eine Apparatur, die aus einer heimischen Pflanze Sauerstoff generiert.\n\nSpäter kommt Jim zu sich und wird, als er aus dem Zelt ausbricht, weil eine Rettungsmannschaft durch die Stadt fliegt, die er auf sich aufmerksam machen will, erneut von Mala gerettet, als er aus der Tür fällt und beinahe zu Tode stürzt.\n\nWährend Jims neuerlicher Bewusstlosigkeit offenbart Giddy Mala die Hintergründe der menschlichen Operationen auf dem Planeten.\nDie Menschheit expandierte aus Gründen des Ressourcenmangels auf den Mars und die Venus und terraformte die Planeten. Als die Kolonien jedoch ihre Unabhängigkeit forderten, kam es zum Krieg, bei dem alle drei Planeten zerstört wurden. Die Reste der menschlichen Spezies suchten nun seit Jahrhunderten in einem Generationsschiff einen bewohnbaren Planeten und fanden ihn in „Terra“.\n\nMit einem sich bewegenden Modell der Erde mit Mond überzeugt Mala Jim von ihren technischen Fähigkeiten, worauf er ihr einen Handel anbietet. Sie hilft ihm bei der Reparatur seines Schiffs, und er hilft ihr bei der Befreiung ihres Vaters. Nachdem die beiden alle notwendigen Ersatzteile hergestellt haben, besucht Malas Freund Senn sie überraschend und entdeckt Jim, worauf er den Ältestenrat informiert. Mala und Jim müssen fliehen und beginnen mit der Suche nach Jims Schiff, das jedoch verschwunden ist. Sie finden es schließlich in einer anscheinend älteren und versteckten Konstruktion der Terraner, die auf ihre kriegerische Vergangenheit hinweist. Dort treffen sie auch auf feindselige Terraner, die über fortschrittliche Waffen verfügen, können jedoch das Schiff erbeuten und zur irdischen Arche fliegen.\n\nIm Orbit angekommen ist man sehr erfreut, dass Jim noch am Leben ist. Er entschließt sich jedoch, vorerst alleine mit seinen Leuten zu reden und bittet Mala darum, an Bord seines Jägers zu bleiben. Auf dem inzwischen sehr gealterten Generationsschiff, auf dem sich aufgrund des baulichen Zustands ständig tödliche Unfälle ereignen, wird Jim als Held gefeiert, da er dem Militär, der Earth Force, Informationen über die Wehrfähigkeit der Terraner liefern kann, das angeführt von General Hemmer den Planeten erobern will, um das Überleben der Menschheit sicherzustellen. Mala hat inzwischen Jims Schiff verlassen und macht sich, bewaffnet mit Jims Laserpistole, auf die Suche nach ihrem Vater. Sie findet ihn und die anderen Terraner in einer Lagereinrichtung mit angeschlossener Müllverbrennung/Krematorium. Sie versucht, ihn zu befreien, wird jedoch von Soldaten entdeckt. Ihr Vater tötet sich und die Wachen, indem er eine Scheibe zerschießt, sodass die Luft des Raums ins Vakuum des Alls entweicht. Mala im Nebenzimmer wird von Wachen gefangen genommen.\n\nVideoaufnahmen vom Tod der Wachen durch die Hand von Malas Vater nutzt der General als weitere Propaganda gegen die „bösartigen“ Außerirdischen vor dem Rat, der ihm jedoch immer noch die Erlaubnis verwehrt, die Terraner auszurotten und den Planeten zu erobern. Der General führt daraufhin einen einem Militärputsch durch und setzt den Rat ab. Auf die Proteste des Rats erwidert er, dass zukünftige Generationen über seine Taten richten sollen. Zukünftige Generationen gäbe es aber nur dank ihm, so der General.\n\nDer Staatsstreich führt zu einem Zerwürfnis zwischen Jim und seinem Bruder Stewart, der den General unterstützt, während Jim erklärt, dass sie dem Rat als höchstes Staatsorgan und nicht dem General loyal gegenüber sein müssen.\n\nNachdem der General Jim offenbart, dass die Arche nur noch für zwei Monate Sauerstoff hat, sperrt er Mala und Jims Bruder in einem Raum mit Terraner-Atmosphäre und gibt Jim die Möglichkeit, den Raum mit Sauerstoff zu fluten, um so Stewart zu retten, aber damit Mala zu töten. Jim aktiviert die Sauerstoffversorgung, befiehlt Giddy aber, alles Mögliche zu tun, um Mala zu retten, sodass er die Zelle öffnet und Mala ein Atemgerät gibt. Giddy und Mala fliehen von der Arche und kehren nach Terra zurück.\n\nDer Ältestenrat hat inzwischen die modernen Fluggeräte aus den kriegerischen Epochen der Terraner reaktiviert, um sich verteidigen zu können. Unter Verweis auf seinen Befehl, sie zu beschützen, kann Mala Giddy dazu bringen, den Terranern taktische Informationen über die Menschen zu liefern.\n\nAuf der Arche wird inzwischen der Angriff vorbereitet, dessen Hauptziel die Aufstellung und Aktivierung des Terraformers ist, einer Maschine, die bei Normalbetrieb innerhalb von 7 Tagen die Atmosphäre des Planeten umwandelt, jedoch auch die einzige Sauerstoffquelle des Generationsschiffs darstellt und so, einmal abgeworfen, erfolgreich sein muss, da sonst der Sauerstoff auf dem Schiff ausgeht.\n\nDer Angriff der Earth Force beginnt, und die Terraner verteidigen sich mit den reaktivierten Gleitern, die den Menschen in ihren zwar technisch fortgeschrittenen, aber sehr schlecht gewarteten Schiffen schwere Verluste zufügen. Während die Terraner nur vergleichsweise schwache Verluste hinnehmen müssen, zwingt der General seine Truppen in gewaltiger zahlenmäßiger Überlegenheit mit dem Angriff fortzufahren und setzt den Terraformer, dessen Brücke dem General als Kommandobunker dient, nahe dem Bevölkerungszentrum der Terraner ab. Die Schiffe der Menschen verteidigen nun den Terraformer, der selbst über Geschütze verfügt, gegen die verzweifelten Angriffe der Terraner. Der General überlastet den Former, um den Prozess schneller abzuschließen, und lässt weitere Verstärkung einfliegen.\n\nEin Großteil der terranischen Flotte wird zum Rückzug gezwungen. Nachdem jedoch Senns Gleiter von einem irdischen Jäger abgeschossen wurde, macht Mala, getrieben von Rache, Jagd auf den Jäger, der sich jedoch als Stewart herausstellt. Dieser fleht Jim um Hilfe gegen den terranischen Gleiter an. Doch als Jim erkennt, dass er von Mala gesteuert wird, bringt er sein Schiff zwischen Mala und Stewart und beendet so den Kampf zwischen den beiden. Er selbst salutiert Mala und setzt daraufhin Kurs auf den Former. Er durchdringt das Abwehrfeuer und zerstört den Kommandoposten, in dem der General zurückbleibt, mit seinen Bordraketen. Die durch die zu schnelle Sauerstoffproduktion erzeugte lokale Überkonzentration an Sauerstoff verstärkt die Explosion, sodass der Former zerstört wird. Während Mala und Stewart sich retten können, opfert sich Jim damit für die Terraner, zerstört jedoch zugleich die Lebensgrundlage der Menschheit.\n\nIn der Stadt der Terraner ist man froh, dass der Angriff abgewehrt werden konnte. Mala und Senn sehen einander wieder und freuen sich beide, da sie voneinander dachten, sie wären tot. Der Älteste lobt Mala für die Rettung ihres Volkes, doch sie ist traurig, dass die Menschheit nun ihrer Vernichtung entgegensieht. Der Älteste der Terraner ermuntert sie jedoch und erklärt, es gäbe immer Alternativen.\n\nMit Hilfe der Terraner haben die Menschen kurze Zeit später eine Sauerstoffkuppel gebaut, unter der sie eine Siedlung errichten. In ihrer Mitte steht eine Statue für Jim, der es durch sein Opfer ermöglicht hat, dass Terraner und Menschen friedlich auf dem Planeten zusammen leben.\n"}
{"id": "2850765", "url": "https://de.wikipedia.org/wiki?curid=2850765", "title": "MadTracker", "text": "MadTracker\n\nMadTracker ist ein Shareware-Tracker für die Microsoft Windows-Oberfläche, der von dem Belgier Yannick Delwiche (* 1979) programmiert und 1998 erstmals veröffentlicht wurde. MadTracker hat neben dem MadTracker-Modus auch einen FastTracker-Modus und ist in der Lage, FastTracker-Module (XM) weitgehend fehlerfrei wiederzugeben. Die momentan aktuelle Version ist 2.6.1 (Februar 2006). \n\nMadTracker verfügt über einen 64-Track-Sequenzer und einer Reihe von Echtzeiteffekten. Es können auch beliebige VST-Effekte eingebunden werden. MadTracker kann über MIDI angesprochen werden und unterstützt ASIO und ReWire. Neben dem XM-Dateiformat werden noch andere Dateiformate unterstützt: S3M (Scream Tracker), IT (Impulse Tracker), UMX (Unreal Music), MOD (ProTracker). Jedoch ist es zu diesen Formaten nicht 100-prozentig kompatibel, so dass die Wiedergabe mitunter sehr fehlerhaft sein kann. Die Umwandlung eines MadTracker-Moduls in eine 24-Bit-Wave-Datei (WAV oder AIFF) ist nur mit der kostenpflichtigen Version möglich.\n\n"}
{"id": "2852828", "url": "https://de.wikipedia.org/wiki?curid=2852828", "title": "Computermuseum Aachen", "text": "Computermuseum Aachen\n\nDas Computermuseum Aachen im Aachener Ortsteil Melaten, benannt nach dem Gut Melaten, entstand in Zusammenarbeit mit dem Rogowski-Institut für Elektrotechnik der RWTH Aachen. Es hatte bis 2009 seinen Sitz in der Sommerfeldstraße Nr. 32 auf dem Aachener \"Campus Melaten\".\n\nDas Museum entstand in Zusammenarbeit mit dem Rogowski-Institut für Elektrotechnik der RWTH Aachen und bekam von diesem veraltete Hardware als Museumsstücke. Wesentlicher Fürsprecher und Unterstützer war Walter Ameling (1926–2010).\n\nDie ab 1965 mit Unterstützung des Landes Nordrhein-Westfalen, der Deutschen Forschungsgemeinschaft und der Freundesgesellschaft der Aachener Hochschule entstandene umfangreiche Sammlung gehörte zu den frühesten ihrer Art in Deutschland.\n\nDas Computermuseum verstand sich als aktives Museum, in dem der Besucher an zahlreichen Rechengeräten und PCs selbst arbeiten und Erfahrungen mit der EDV-Technik der Vergangenheit sammeln konnte. Das Computermuseum wurde 1987 auf dem RWTH-Erweiterungsgelände in Melaten eingerichtet und Ende 2009 geschlossen. Von 1987 bis 1993 wurde das Museum von dem Historiker Peter Johannes Droste geleitet.\n\nPrunkstück der Ausstellung war eine Rechenanlage vom Typ Zuse Z22 der RWTH aus dem Jahr 1958.\n\nEinige der Exponate\n\nDas Gebäude, in dem das Computermuseum untergebracht war, musste 2010 im Rahmen der Neuausrichtung des Campus Melaten abgerissen werden. Deswegen musste das Museum Ende 2009 schließen, und alle dort vorhandenen Exponate suchten einen neuen Standort.\n\nBereits 2006 wurden die Hintergrundbestände des Museums aufgelöst und zunächst in Castrop-Rauxel deponiert. Von dort gelangte, von SAP finanziert, ein Großteil der Sammlung nach Mountain View in Kalifornien in das dortige Computer History Museum (CHM), darunter auch Mulby-Rechner aus Aachener Produktion (Krantz Computer). Die Reste wurden in eine Lagerhalle auf dem Gelände des Dortmunder Bahnhofs verbracht (Tillmann-Sammlung). Dort wurden die Teile durch die schlechte Lagerung arg ramponiert und waren 2012 von der Verschrottung bedroht. Privaten Sammlern gelang es noch viele Artefakte zu retten und zu restaurieren. Sie finden sich jetzt u. a. im Rechenwerk Computer- & Technikmuseum Halle.\n\nEin weiterer Teil aus dem Aachener Bestand wurde bei einer Spedition in Köln gelagert. Die Kosten hierfür übernahm das CHM bis 2007/2008. Danach wurden auch hier einzelne Artefakte von Sammlern gerettet, der Rest wurde verschrottet.\n\nNach der Schließung Ende 2009 wurden die noch vorhandenen Exponate auf RWTH-Gelände zwischengelagert und 2012 an Museen verteilt, insbesondere an das Konrad-Zuse-Computermuseum in Hoyerswerda und an das Zuseum in Bautzen. Letzteres erhielt auch die Z22.\n\n"}
{"id": "2855796", "url": "https://de.wikipedia.org/wiki?curid=2855796", "title": "UNIVAC 1104", "text": "UNIVAC 1104\n\nDie UNIVAC 1104 war ein 30-Bit Computersystem von Sperry Rand, welches 1957 entwickelt wurde. Dabei handelte es sich um einen abgeänderte Version der UNIVAC 1103. Die Maschine sollte ursprünglich für das BOMARC-Raketenprogramm eingesetzt werden. 1960 bei der Entwicklung des BOMARC Programms wurde die UNIVAC 1104 durch die AN/USQ-20, auch bezeichnet als G-40, ersetzt. \n\n"}
{"id": "2856703", "url": "https://de.wikipedia.org/wiki?curid=2856703", "title": "CHKDSK", "text": "CHKDSK\n\nCHKDSK oder ähnliche Schreibweisen wie chkdsk oder Chkdsk ist der Name verschiedener Kommandozeilenprogramme zur Überprüfung von Dateisystem-Strukturen auf Datenträgern. Je nach Version und des zugrunde liegenden Betriebssystems können FAT-, HPFS oder NTFS-Dateisysteme auf Disketten, Festplatten oder anderen Blockdevices wie Flashspeichern auf Dateisystemfehler überprüft werden.\n\nDer Programmname ist ein Kürzel für den englischen Befehl \"check disk\", „überprüfe Platte“. Das Programm ist das Pendant zum Unix-Programm bzw. -Befehl fsck (Kürzel für \"FileSystem Check\"). Es kann unter DOS, OS/2 und Microsoft Windows NT als selbständiges Programm über die Eingabeaufforderung mit \"chkdsk\" ausgeführt werden.\n\nUnter PC-kompatiblen DOS-Versionen wie MS-DOS oder PC DOS wurde CHKDSK für die Überprüfung von FAT12- und FAT16-Dateisystemen, später auch FAT32-Systemen eingesetzt. Mit MS-DOS 6.2 wurde ScanDisk mitgeliefert, welches gegenüber CHKDSK benutzerfreundlicher war, CHKDSK wurde dennoch bis zur Einstellung der Windows-9x-Linie mitgeliefert.\n\nFür Windows NT 3.1 wurde \"chkdsk\" als 32-bit-Programm neu implementiert, welches NTFS- und HPFS-Dateisysteme unterstützte. Die Unterstützung für die Prüfung von HPFS-Dateisystemen entfiel mit der Einstellung der HPFS-Unterstützung durch Microsoft.\n\nchkdsk kann im Fenster der Windows-Eingabeaufforderung oder vor dem Booten in der \"System Recovery Console\" gestartet werden.\n\nFehlerhafte Datenblöcke werden erkannt. Deren binärer Inhalt wird wenn möglich gesichert. chkdsk ist in der Lage, eine beschädigte Master File Table (MFT) eines NTFS-Dateisystems zu erkennen und zu reparieren.\n\n\n\nCHKDSK legt ein Protokoll in der Ereignisanzeige im Anwendungsprotokoll ab.\nBei Windows XP heißt die Quelle WINLOGON mit EreignisID 1001.\nBei Windows 7 heißt die Quelle WININIT mit EreignisID 1001.\n\n\n"}
{"id": "2857092", "url": "https://de.wikipedia.org/wiki?curid=2857092", "title": "Rigging (Animation)", "text": "Rigging (Animation)\n\nDas Rigging ist eine Arbeitstechnik im Bereich der 3D-Animation. \n\nBeim Rigging wird mithilfe einer entsprechenden Software ein sogenanntes \"Skelett\" bzw. \"Rig\" aus Bones (Knochen) oder auch Joints (Gelenken) konstruiert, das festlegt, wie die einzelnen Teile eines \"Meshes\" (eines Polygonnetzes) bewegt werden können. Nicht selten orientiert man sich bei der Konstruktion an der Beschaffenheit eines tatsächlichen Skelettes, beispielsweise bildet man etwa einen echten Oberschenkelknochen oder ein echtes Kniegelenk nach.\n\nNachdem das Skelett bzw. Rig erstellt wurde, kann es mit dem Polygonnetz gekoppelt werden, in einem weiteren Arbeitsschritt, dem sogenannten Skinning, sind oft noch einige kleinere Fehler auszubügeln, die beim Koppeln des Meshes an das Rig aufgetreten sind.\n\n\n\n"}
{"id": "2857847", "url": "https://de.wikipedia.org/wiki?curid=2857847", "title": "UNIVAC BOGART", "text": "UNIVAC BOGART\n\nDie UNIVAC BOGART, auch X308 genannt und nach dem Journalist John. B. Bogart benannt ist ein Großrechner und wurde 1955–1960 durch Sperry Rand für die NSA gebaut.\nDas BOGART System war ein direkter Nachfolger der UNIVAC 1103 (ATLAS-II) und wurde für Kryptoanalyse eingesetzt um Muster in Kryptographischen Texten zu erkennen. \n\nGebaut wurde das BOGART System durch Seymour Cray und setzte die Magnetschalter-Technologie des \"MAGTEC\" Prototyps der UNIVAC ATHENA ein. Der BOGART Prototyp wurde 1956 fertiggestellt und anschließend durch die NSA getestet. Im ganzen wurden bis 1959 fünf Maschinen ausgeliefert und für einen ersten Test des Remote Job Entry (RJE) Konzeptes für die NSA verwendet. Nachdem Seymour Cray 1957 Sperry Rand verlassen hatte, verwendete er dieselbe logische Design in seinem ersten Computer 1960, dem CDC 1604 für die Control Data Corporation.\n\nEigenschaften des BOGART-Systems\n\n\n"}
{"id": "2859172", "url": "https://de.wikipedia.org/wiki?curid=2859172", "title": "UNIVAC 418", "text": "UNIVAC 418\n\nDie UNIVAC 418, im militärischen Bereich UNIVAC 1218 und UNIVAC 1219 genannt, ist ein Großrechner und wurde 1963–1969 durch Sperry Rand entwickelt. Ein Ziel war es, die Maschine so klein wie möglich zu gestalten, so dass sie auf einem normalen Bürotisch Platz findet. Das System basierte auf Transistoren und eine Maschinenwortbreite von 18 Bit. Der Name 418 bildete sich aus den Eigenschaften, dass die Maschine vier Mikrosekunden Taktzeit als auch 18 Bit Wortbreite besaß.\n\nIm ganzen wurden drei verschiedene Modelle (418-I, 418-II und 418-III) entworfen und im ganzen über 392 Maschinen produziert. Manche Maschinen waren bis Ende 1990 im Einsatz.\n\n\n\n\n\n"}
{"id": "2860670", "url": "https://de.wikipedia.org/wiki?curid=2860670", "title": "K·p-Methode", "text": "K·p-Methode\n\nDie k·p-Methode (auch KP-Methode) ist eine störungstheoretische Methode der Quantenmechanik zur Berechnung der elektronischen Bandstruktur eines Festkörpers. Sie bietet eine Näherung der Lösung der Schrödinger-Gleichung für Elektronen in Halbleitern und anderen kristallinen Festkörpern. Die Methode erlaubt so auch das elektronische Verhalten von Bauteilen der Mikroelektronik zu simulieren.\n\nDie Bezeichnung stammt daher, dass in den Energien der einzelnen Energiebänder ein Ausdruck der Form k·p auftritt, also das Skalarprodukt aus dem quantenmechanischen Impulsoperator p und dem Wellenvektor k .\n\nDie Methode basiert auf einer Beschreibung der Elektronen als \"nicht\" miteinander wechselwirkende Teilchen in einem periodischen effektiven Potential. Dieses beinhaltet die Wechselwirkung des beschriebenen Elektrons mit den Elektronen und Atomkernen des Festkörpers.\n\nIst die Lösung der Schrödinger-Gleichung für \"einen\" Wellenvektor k des Elektrons im reziproken Raum aus anderen Methoden (z. B. der Dichtefunktionaltheorie) bekannt, so kann die Elektronen-Energie für naheliegende k-Werte als Störung dieser Lösung bestimmt werden. Aus der Veränderung der Energie (Eigenwerte der Schrödinger-Gleichung) mit dem Wellenvektor ist dann die gesuchte Bandstruktur des Festkörpers bestimmt.\n\nDie Wellenfunktion des Elektrons genügt in der Ein-Teilchen Näherung der Schrödingergleichung:\n\nmit\n\nBloch's Theorem besagt nun, dass die Lösung einer solchen periodischen Differentialgleichung wie folgt geschrieben werden kann:\n\ndabei ist\n\nSetzt man formula_5 in die Einteilchen-Schrödingergleichung ein, so erhält man die folgende Differentialgleichung für u:\n\nmit dem reduzierten planckschen Wirkungsquantum formula_7.\n\nFür einen Wellenvektor k, für den die Lösungen bekannt sind (oft am Γ-Punkt k=0), behandelt die k·p-Methode nun den Term \n\nin obiger Gleichung als Störung (daher der Name). Ziel der Störungsrechnung ist es, näherungsweise Ausdrücke für die Energieeigenwerte und die zugehörigen Eigenzustände zu finden. \n\nDie Energien und Eigenzustände werden mit zunehmender Ordnung zwar genauer, die Gleichungen jedoch immer komplexer. Man approximiert daher die gesuchten Ausdrücke mit Störungen zweiter Ordnung. Für alle betrachteten Zustände \"n\" erhält man Gleichungen, in denen Wechselwirkungsterme in Form von Übergangsmatrixelementen zwischen den betrachteten Zuständen und allen anderen Zuständen \"n' \" auftreten. Man erhält also \"n\" Gleichungen mit jeweils \"n' \" Wechselwirkungstermen.\n\nFür direkte Anwendungen betrachtet man nur Zustände in der Nähe der Bandlücke, womit die Anzahl der Gleichungen reduziert wird. Des Weiteren nutzt man in kristallinen Schichten die Symmetrieeigenschaften der verschiedenen Kristallsysteme in Form der Gruppentheorie, um mit deren Hilfe viele der Wechselwirkungsterme zu effektiven Termen zusammenzufassen und somit die Anzahl der Wechselwirkungsterme weiter stark zu reduzieren. Schließlich ergeben sich relativ wenige Gleichungen, welche man kompakt als Matrix darstellt, um anschließend die gesuchten Energieeigenwerte \"E\" und die zugehörigen Eigenzustände \"u\" zu berechnen.\n\nAus den Eigenwerten lassen sich dann Ausdrücke für die Dispersion formula_9, die effektive Masse der Elektronen und Auswahlregeln für die Wechselwirkung mit Licht mit weniger Aufwand als bei einer vollständigen Rechnung bestimmen. \n\nWichtig ist sie insbesondere im Fall entarteter Bänder, da der k·p-Term die Bänder miteinander koppelt, die Entartung teilweise aufhebt und neue Auswahlregeln für optische Übergänge zwischen den Bändern bestimmt.\n\n(Anm.: Anspruchsvolles Fachbuch, in der die KP-Theorie systematisch abgeleitet wird)\n"}
{"id": "2861120", "url": "https://de.wikipedia.org/wiki?curid=2861120", "title": "Tight-Binding-Methode", "text": "Tight-Binding-Methode\n\nDie Tight-Binding-Methode (engl. \"enge Bindung\"; abgekürzt TB oder TBM) dient zum Berechnen der elektronischen Bandstruktur von Festkörpern oder Molekülen. Sie ist deutlich weniger rechenintensiv als die Dichtefunktionaltheorie (DFT), da hier meist nur die Valenzelektronen berechnet, die Wechselwirkungen der ersten formula_1 Nachbaratome in Form von Parametern berücksichtigt und Ein-Elektron-Betrachtungen durchgeführt werden. \nEs wird eine atomzentrierte Basis angenommen. Im Gegensatz zur k·p-Methode ist Tight-Binding eine atomistische Methode, wodurch Grenzflächeneffekte (z. B. in Oberflächenchemie und Oberflächenphysik) berücksichtigt werden können.\n\nDie Wechselwirkungsparameter werden typischerweise durch Anpassung an bekannte gemessene oder berechnete Bandstrukturen ermittelt und anschließend für weitere Berechnungen übernommen. Durch den vergleichsweise geringen numerischen Aufwand lassen sich damit dann komplexere Berechnungen durchführen, wie z. B. Berechnung der Gitterschwingungen (Phononen) oder Rechnungen mit nicht elementaren Kristallzellen, wie an dünnen Schichten oder Oberflächen.\n\n\n"}
{"id": "2865564", "url": "https://de.wikipedia.org/wiki?curid=2865564", "title": "PaX", "text": "PaX\n\nPaX ist ein Sicherheitspatch für den Linux-Kernel, der einen „Geringste-Rechte“-Schutz () für Speicherseiten implementiert. Der Ansatz der geringsten Rechte erlaubt Computerprogrammen nur diejenigen Aktionen durchzuführen, die sie für einen ordentlichen, regulären Ablauf benötigen, und verbietet alle darüber hinausgehenden. PaX wurde 2000 erstmals veröffentlicht.\n\nPaX markiert Datenbereiche des Speichers als nicht-ausführbar, Programmbereiche als nichtbeschreibbar und ordnet den Programmspeicher zufällig an. Das erste verhindert direkte Code-Ausführung, während das letztere sogenannte \"return-to-libc\"-Angriffe (ret2libc) stark erschwert, ein Erfolg eines solchen Angriffes hängt dann vor allem vom Zufall ab; es verhindert aber nicht das Überschreiben von Variablen und Pointern.\n\nPaX wurde durch das PaX Team geschrieben. Der Hauptautor von PaX möchte anonym bleiben.\n\nViele, wenn nicht fast alle, Sicherheitslücken bei Computern sind auf Fehler in Programmen zurückzuführen, die es ermöglichen, die Funktion eines Programms zu verändern, effektiv also ein „Umschreiben“ des Programms während dessen Ausführung erlauben. So zeigen die ersten 44 Ubuntu \"Security Notices\", dass 41 % der Angriffsmöglichkeiten aus \"buffer overflows\" resultieren, 11 % aus \"integer overflows\" und 16 % von anderer, falscher Behandlung von nicht erwartungsgemäßen Daten. Diese Typen von Programmfehlern ermöglichen es oft, Schadcode einzuschleusen und auszuführen; im Beispiel machen sie 61 % aus, ohne Berücksichtigung von Überschneidungen. Die oben durchgeführte Analyse ist sehr ungenau, eine umfangreichere Untersuchung würde sicherlich andere Zahlen ergeben (sowohl höher als auch tiefer), zeigt aber grundsätzlich die Problematik.\n\nViele Würmer, Viren und Übernahmeversuche beruhen auf der Veränderung von Speicherinhalten, so dass Schadcode ausgeführt wird, oder auf der Ausführung von Speicherinhalten (durch Falschadressierung), die eigentlich Daten darstellen sollen. Wenn die Ausführung solcher Anweisungen verhindert werden könnte, würde solche Schadsoftware nur noch wenig bis gar keinen Schaden anrichten können, sogar nach Installation auf dem Computer; viele könnten überhaupt nicht mehr installiert werden (z. B. der Sasser-Wurm).\n\nPaX wurde entworfen, um genau dies für eine große Anzahl möglicher Angriffe zu tun, und zwar auf einem sehr allgemeinen, breit anwendbarem Weg. Er verhindert die Ausführung von missbräuchlichem Code durch Kontrolle des Zugriffes auf den Speicher (lesen, schreiben, Zugriff auf Programmcode sowie Kombinationen davon) und dies, ohne die Ausführung von normalem Code zu verhindern. Mit einem kleinen Overhead reduziert PaX dadurch viele Exploits auf \"Denial of Service\"-Angriffe: Exploits, die dem Angreifer normalerweise root-Zugriff, Zugriff auf wichtige Daten oder anderen Schaden anzurichten erlauben würden, würden das betroffene Programm nunmehr abstürzen und das restliche System unbehelligt lassen.\n\nEin DoS-Angriff ist zwar unangenehm und resultiert oft in Verlust von Zeit oder anderen Ressourcen; jedoch werden meistens keine Daten kompromittiert, wenn PaX eingreift. Trotzdem kann ein DoS-Angriff in gewissen Umgebungen nicht akzeptabel sein; es gibt level-of-service-Verträge oder andere Bedingungen, die einen erfolgreichen Einbruch in ein System weniger kostenintensiv machen als die Reduktion oder Verlust der Verfügbarkeit eines Dienstes. In diesem Licht betrachtet ist der Ansatz von PaX nicht überall angemessen. In vielen Fällen jedoch ist es eine brauchbare Methode, um vertrauliche Daten vor Sicherheitsbrüchen zu schützen.\n\nViele Programmierfehler verursachen eine Verfälschung des Speicherinhaltes. Von den Fehlern, die dies ermöglichen und absichtlich herbeigeführt werden können, ermöglichen einige, das Programm verschiedene Dinge durchführen zu lassen, für die es nicht vorgesehen wurde, wie z. B. eine privilegierte Shell zu erhalten. Der Fokus von PaX ist nicht das Finden und Korrigieren solcher Fehler, sondern das Verhindern und Isolieren der Exploits dieser Fehler. Wie im oberen Absatz erwähnt, wird eine Untermenge dieser Fehler in ihrer Schwere heruntergestuft; Programme werden beendet statt ihre Dienste fehlerbehaftet weiterhin anzubieten.\n\nPaX verhindert Pufferüberläufe nicht direkt. Stattdessen verhindert es, dass viele dieser und ähnlicher Programmierfehler ausgenutzt werden, um unberechtigt Zugriff auf ein Computersystem zu erhalten. Andere Systeme wie \"Stack-Smashing Protector\" und \"StackGuard\" versuchen, Pufferüberläufe direkt zu verhindern und das entsprechende Programm bei Entdeckung zu beenden. Dieser Ansatz wird \"Stack-Smashing\" genannt und versucht, die Angriffe abzuwehren, \"bevor\" sie überhaupt durchgeführt werden können. Der allgemeinere Ansatz von PaX hingegen verhindert Schaden, \"nachdem\" der Angriff begonnen hat. Obwohl beide Methoden dieselben Ziele erreichen können, sind sie nicht völlig redundant. Dadurch wird ein Betriebssystem bei Verwendung beider Methoden prinzipiell sicherer. Es gibt bereits Linux-Distributionen, die beide Methoden benutzen.\n\nPaX kann Fehler im Design von Programmen oder des Kernels, die einen Missbrauch der angebotenen Funktionen erlauben, nicht verhindern. Diese Fehler sind im Prinzip so auch nicht feststellbar. Als Beispiel mag man einen Script-Interpreter betrachten, der Zugriff auf Dateien und Netzwerk erlaubt und diese Funktionen ungenügend schützt: Der Angreifer könnte auf Dateien von anderen Benutzern zugreifen, ohne dabei am Programm etwas „verbiegen“ zu müssen. PaX kann auch nicht alle Formatstring-Angriffe abwehren, die beliebiges Lesen und Schreiben in Datenbereiche des Speichers durch bereits existierenden Code erlauben; der Angreifer braucht weder interne Adressen zu wissen noch fremden Code einzuschleusen, um diesen Typ Angriff durchführen zu können.\n\nDie PaX-Dokumentation beschreibt die folgenden drei Klassen von Angriffen, vor denen PaX zu schützen versucht. Sie behandelt sowohl Angriffe, die PaX abwehrt, als auch solche, die nicht abgewehrt werden. Alle setzen ein vollständiges, positionsunabhängiges Programm mit Schutz des Programmspeichers und zufälliger Anordnung des Adressbereiches voraus. Folgende Angriffe können dann blockiert werden:\nEs existiert hierfür ein Patch, der alle Werte für Adressbereiche und Inodes in jeder Informationsquelle, die vom Userland erreichbar ist, ausnullt. Dieser Patch ist zurzeit jedoch nicht in PaX enthalten.\n\nVerantwortungsvolle Systemadministration ist trotz PaX immer noch notwendig. PaX verhindert oben beschriebene Angriffe, dennoch besteht die Möglichkeit eines erfolgreichen Angriffes.\n\nDies ist eine, noch nicht vervollständigte Liste der Geschichte von PaX, sie sollte bearbeitet werden, soweit neue Informationen gegeben sind.\n\nEiner der Hauptmerkmale von PaX ist der Schutz von Programmbereichen des Speichers. Dieser Schutz verwendet das NX-Bit auf bestimmten Prozessoren, um die Ausführung von fremden Code zu verhindern; dies fängt Angriffe ab, die auf Injektion von fremden Code oder Shellcode beruhen. Auf IA-32-CPUs ist das NX-Bit nicht vorhanden, PaX kann in diesem Fall dessen Funktionalität auf verschiedenen Wegen emulieren.\n\nViele Betriebssysteme, einschließlich Linux, nutzen auf x86-Prozessoren dessen NX-Bit, falls vorhanden, um korrekte Einschränkungen auf den Speicher anzuwenden. Fig. 1 zeigt eine einfache Zusammenstellung von Speichersegmenten in einem Programm mit einer geladenen Bibliothek; grüne Segmente sind dabei Daten, blaue sind Programmcode. Unter normalen Umständen sieht der Adressbereich auf AMD64 und anderen derartigen Prozessoren ähnlich wie dieses Bild aus, mit klar definierten Daten- und Codesegmenten. Unglücklicherweise verwehrt Linux standardmäßig einem Programm nicht, seinen Speicherschutz zu ändern; jedes Programm kann Codesegmente als beschreibbar und Datensegmente als ausführbar kennzeichnen. PaX verhindert solche Änderungen, sowie es auch möglichst hohe Einschränkungen garantiert, die eine normale Ausführung erlauben.\n\nWenn die verschiedenen Schutzvorkehrungen des Programmspeichers aktiviert sind, einschließlich der mprotect()-Einschränkungen, garantiert PaX, dass keinerlei Speicherzuordnungen in irgendeiner Weise als ausführbarer Programmcode markiert werden können, nachdem es möglich gewesen wäre, den Zustand dieser Bereiche zu ändern. Der Effekt dieser Maßnahme ist, dass es unmöglich wird, Speicherbereiche auszuführen, während oder nachdem sie verändert werden können, bis diese Bereiche schließlich wieder freigegeben werden; und damit, dass kein Code in das Programm eingeführt werden kann, ob nun schädlich oder nicht, weder von einer internen noch einer externen Quelle.\n\nDie Tatsache, dass Programme Datenbereiche nicht ausführen können, obwohl die dort gespeicherten Daten dafür vorgesehen wären, stellt ein unüberwindliches Problem für ebendiese Programme dar. Ein Beispiel hierfür sind die Just-in-time-Compiler für Java; jedoch können viele dieser Programme vom Programmierer so geändert werden, dass sie nicht auf dieser Funktionalität beruhen. Die anderen können durch den Systemadministrator gekennzeichnet werden, PaX wendet dann die Einschränkungen für diese nicht an.\n\nDas PaX-Team musste einige Entscheidungen im Bezug auf die Behandlung des mmap()-Aufrufes treffen. Diese Funktion wird dazu verwendet, gemeinsamen Speicher (\"Shared Memory\") abzubilden oder dynamische Bibliotheken (\"shared libraries\") zu laden. Daher benötigt der Aufruf beschreibbare oder ausführbare Speicherbereiche, abhängig von den jeweiligen Bedingungen.\n\nDie aktuelle Implementierung von PaX unterstützt beschreibbare anonyme Speicherabbildungen standardmäßig; beschreibbare Abbildungen einer Datei sind nur beschreibbar, wenn der mmap()-Aufruf das Schreibrecht angibt. Es werden jedoch nie Abbildungen zurückgegeben, die beschreib- und ausführbar sind; sogar wenn der Aufruf diese Rechte explizit angibt.\n\n\n\n"}
{"id": "2865613", "url": "https://de.wikipedia.org/wiki?curid=2865613", "title": "Music &amp; PC", "text": "Music &amp; PC\n\nDie Music & PC war ein Musikmagazin, das sich mit dem PC und seinen Fähigkeiten zur Produktion und Bearbeitung von Musik beschäftigte.\n\nVon 1997 bis 2010 informierte Music & PC ihre Leser über allgemeine Themen rund um die Hard- und Software für einen leistungsfähigen Musik-PC.\nHerausgegeben wurde die Zeitschrift vom Fachverlag Schiele & Schön in Berlin, für die Position als Chefredakteur zeichnete seit 2009 Helge Beckmann verantwortlich.\n\nZielgruppen von Music & PC waren sowohl erfahrene Profis als auch ambitionierte Hobby-Musiker ohne großen Erfahrungsschatz, die sich für die nötigen und möglichen technischen Voraussetzungen eines musikbearbeitenden Computers interessieren.\n\nDie bereits 1950 sichtbar werdende Tendenz, mit dem Computer elektronische Musik zu produzieren, gipfelte in den 1980er Jahren in einer beinahe flächendeckenden Fokussierung auf die Digitalisierung von Musik. Diese Entwicklung erlangte Mitte der 1990er Jahre eine völlig neue Bedeutung, als die Produktion von elektronische Musik durch immer leistungsstärkere Computer und im Grunde leicht zugängliche Hard- und Software auch für den Otto Normalverbraucher in erreichbare Nähe rückte.\n\nIn diese Zeit der ‚Demokratisierung’ der elektronischen Musik fiel auch die Publikation der ersten Ausgabe von Music & PC, auf deren Titelblatt jedoch noch ihr alter Name zu lesen war: PC & Musik. Die Tatsache, dass der PC damals noch im Vordergrund, respektive an erster Stelle des Titels stand, erklärt sich aus dem Verständnis, dass die Aufnahme und Produktion von Musik per Computer noch als Besonderheit betrachtet wurde.\n\nAls die Zeitschrift 2007, nachdem sie zehn Jahre lang vom Tetrix-Verlag in Eschwege herausgegeben wurde, vom Berliner Fachverlag Schiele & Schön übernommen wurde, wurde die Umbenennung in Music & PC vorgenommen.\nZiel dieser Aktion war es, den Schwerpunkt der Berichterstattung noch weiter in den Bereich der Musikproduktion zu verschieben. Gleichzeitig symbolisiert der neue Titel jedoch auch den technischen Fortschritt, der die Arbeit an Musikcomputern so weit vereinfacht hat, dass die Musik wieder in den Vordergrund rückt.\n\nMusic & PC behandelte vornehmlich praktische Themen. So wurden technische Mittel und Methoden beschrieben, die für die Produktion und Bearbeitung von Musik am Computer nötig beziehungsweise möglich sind. Im Mittelpunkt der Berichterstattung standen Tests und Beschreibungen von relevanter Hard- und Software – unter anderem digitale Instrumente, Synthesizer, Mikrofone, Kabel, Plug-ins und MIDI-Controller, aber auch Sampling-CDs und Software.\n\nNeben diesen praxisorientierten Artikeln zur Arbeit am Musik-PC werden in Music & PC auch regelmäßige Workshops zu komplexen Themen angeboten, die den Umgang mit den oben genannten Medien näher bringen. Der Rezipient erfährt hier beispielsweise Näheres über das Verfahren des Harddisk-Recordings, die Erstellung von MIDI-Signalen oder die Bearbeitung von Musik mit unterschiedlichen elektronischen Mitteln. Außerdem wurde über Neuheiten in den Bereichen Musik im Internet, Podcast und MP3 berichtet.\n\nBis Anfang 2009 lag dem Magazin eine CD bei, die als Ergänzung der im Heft behandelten Themen dient. Sie enthält multimediale Erweiterungen zu einzelnen Artikel, deren Inhalt sie sowohl visuell als auch akustisch illustriert. So dient sie beispielsweise der Darstellung von klanglichen Unterschieden zwischen verschiedenen im Test befindlichen Röhrenmikrofonen. Die Beilage wurde mit der Ausgabe 03/2009 eingestellt und durch Downloads von der Homepage vom Music & PC abrufbar.\n\nMusic & PC erschien bereits unter seinem Namen PC & Musik alle zwei Monate, dieser Rhythmus wurde auch nach dem Relaunch beibehalten. Das Fachmagazin war deutschlandweit erhältlich, im Abonnement war es auch im Ausland zu beziehen.\n\n"}
{"id": "2866505", "url": "https://de.wikipedia.org/wiki?curid=2866505", "title": "Security Accounts Manager", "text": "Security Accounts Manager\n\nSecurity Account Manager (SAM) bzw. Sicherheitskontenverwaltung ist ein Dienst von Microsoft Windows, mit dem Benutzerinformationen wie Anmeldename und Kennwort als Hashwerte in einer Datenbank gespeichert werden. Diese Datenbank ist verschlüsselt und kann unter Windows nicht geöffnet werden, da sie von internen Prozessen verwendet wird. Sie kann allerdings mit bestimmten Programmen auch während des Betriebs ausgelesen werden. Die Datei wird meist unter codice_1 gespeichert. Bei einem schlecht gewählten Passwort kann dieses, aufgrund des vorliegenden Hashwertes, durch einen Brute-Force-Angriff innerhalb weniger Minuten herausgefunden werden.\n\n"}
{"id": "2871426", "url": "https://de.wikipedia.org/wiki?curid=2871426", "title": "ABC 80", "text": "ABC 80\n\nDer ABC 80 (Advanced BASIC Computer 80) war ein Heimcomputer, welcher von der schwedischen Firma Dataindustrier AB entwickelt wurde. Hergestellt wurde er in den 1970er und 1980er Jahren von Luxor in Motala, ebenfalls in Schweden.\n\nEr basierte auf einem mit 3,58 MHz getakteten Zilog-Z80-Prozessor, hatte jeweils 16 KB RAM und ROM und einen BASIC-Interpreter. Des Weiteren besaß er den Soundchip SN76477 von Texas Instruments. Allerdings gab es keine Möglichkeit, sämtliche Funktionen des Chips zu verwenden, deshalb war die Soundausgabe des ABC auf ein paar wenige Sound-Effekte beschränkt. Der Monitor war ein Schwarz-Weiß-Fernseher, der für diesen Zweck etwas verändert wurde.\n\nDas Gerät wurde auch als \"BRG ABC80\" von Budapesti Rádiótechnikai Gyár in Ungarn hergestellt. Allerdings waren diese Modelle mit Metall- statt Kunststoffgehäusen ausgestattet.\n\nDer ABC 80 war in Schweden ein sehr großer Erfolg und nahm dort schnell einen Großteil des Heimcomputer-Markts ein, was unter anderem daran lag, dass seine Software in Schwedischer Sprache verfügbar war.\n\nIn den frühen 1980ern wurde der Computer durch Spiel-Computer mit besserer Grafik und besserem Sound abgelöst.\n\nLuxor konnte sich noch einige Jahre mit dem neuen ABC 800, der mehr Speicher und eine höhere Grafikauflösung hatte, unter anderen Büro-Computern durchsetzen.\n\n1985 kam der neue Unix-basierte ABC 1600 und die 9000er-Reihe auf den Markt, diese waren jedoch Fehlschläge.\n"}
{"id": "2874535", "url": "https://de.wikipedia.org/wiki?curid=2874535", "title": "Die TeXnische Komödie", "text": "Die TeXnische Komödie\n\nDie TeXnische Komödie (DTK) ist die Mitgliederzeitschrift der Deutschsprachigen Anwendervereinigung TeX e. V. (DANTE). \n\nDie Zeitschrift enthält sowohl vereinsinterne Mitteilungen und Tagungsberichte als auch wissenschaftliche und praktische Beiträge zu allen Aspekten des Textsatzsystems TeX und der begleitenden Software (vor allem zu LaTeX, aber auch zu ConTeXt, LuaTeX, XeTeX oder BibTeX/Biber) sowie Rezensionen neuer Literatur. Die Autoren sind in der Regel Vereinsmitglieder von DANTE, Entwickler oder Anwender.\n\nDie Zeitschrift besteht seit der Gründung des Vereins 1989. Sie erscheint viermal pro Jahr in einer Auflage von 2400 Exemplaren und wird an alle Mitglieder von DANTE kostenlos verteilt. Nichtmitglieder können einzelne Hefte nachbestellen. Einmal im Jahr liegt die TeX Collection mit der aktuellen TeX-Distribution TeXLive sowie einem Abzug des TeX-Repository CTAN auf DVD bei.\n\nBei dem Namen der Zeitschrift handelt es sich um eine Anspielung auf die \"Göttliche Komödie\" des italienischen Dichters und Philosophen Dante Alighieri und den Namen des Herausgebers.\n\n"}
{"id": "2875862", "url": "https://de.wikipedia.org/wiki?curid=2875862", "title": "Computer-aided architectural design", "text": "Computer-aided architectural design\n\nComputer-aided architectural design (kurz CAAD) ist rechnergestützter Entwurf für Architekten. \n\nSchon seit den 1960er Jahren versucht man, mit Hilfe geeigneter Software bei der Modellierung räumlicher Gebilde Zeit einzusparen. Die Anwendung von gewöhnlichen CAD-Programmen war oft mit Schwierigkeiten verbunden, da sie nicht alle notwendigen Planungs- und Entwurfswerkzeuge besaßen, sodass sich eine eigenständige CAD-Anwendung für den Architekturbereich entwickelte.\n\nBasierend auf den CAAD-Dateien können mittels CAAM Modelle hergestellt werden.\n\nDie CAAD-Software besitzt eine Datenbank mit geometrischen Formen, welche mit gewissen Eigenschaften verknüpft sind. Im Gegensatz zur Standard CAD-Software kann der Architekt auf bauspezifische Objekte und Daten zurückgreifen. Die Software kennt sämtliche Bauteile mitsamt Eigenschaften und unterstützt die kreative Nutzung während der Entwurfsphase. \n\nDie Grenzen zwischen CAAD-Software und anderer rechnergestützter Zeichensoftware lassen sich nicht immer klar ziehen. So werden auch klassische Animationsprogramme wie beispielsweise 3ds Max verwendet, um Entwürfe von Architekten darzustellen und zu präsentieren bzw. zu animieren. Auch die Anwendung der Finite-Elemente-Methode kann Bestandteil einer CAAD-Software sein, wird bei der Entwurfsfindung sehr selten eingesetzt und dient nur für rechnerische Nachweise von Gebäudestrukturen.\n\nDer Aufbau typischer CAAD-Software besteht üblicherweise aus mindestens \"zwei\" Ebenen. Die erste Ebene dient der Berechnung und der Darstellung der geometrischen Definition des Entwurfs. Diese innere Repräsentanz kann, je nach Software, aus 2D- oder 3D-Modellen bestehen. Auf dieser Ebene werden die Pläne mit weiteren Daten wie Beschriftungen, Vermassungen und Materialeigenschaften verknüpft. Die Ergebnisse dieses Prozesses können als Pläne, in Grundrissen, Schnitten, Ansichten und bei 3D-Modellen in räumlichen Darstellungen ausgegeben werden. \n\nAuf einer zweiten Ebene werden diese aggregierten, also mit Zusatzinformationen versehenen, geometrischen Körper mit Datenbankoperationen analysiert. Damit lassen sich Metainformationen in Listenform erstellen, etwa Flächenlisten und Listen zur Massenermittlung, Stücklisten, teilweise Bauzeitenpläne. Da der Computer eine hohe Rechenleistung vollbringen kann und somit auch technisch nicht umsetzbare Konstruktionen und Anordnungen berechnen und darstellen kann, begrenzt und überwacht diese Ebene die Auswahl sowie den Umfang und achtet auf die Funktionalität (technische Realisierbarkeit) des Entwurfs.\n\nEine weitere Bezeichnung von Konstruktionssoftware lautet AEC (Architecture, Engineering, Construction). Hier werden oft weitere oder besser ausgebaute Module zur Konstruktion, statischen Berechnung und Ausschreibung von Bauteilen mitgeliefert. Die Grenzen zur CAAD sind aber fließend.\n\nFreeware:\n\nkommerzielle Produkte:\n\nsiehe Liste von CAD-Programmen.\n\n\n"}
{"id": "2876797", "url": "https://de.wikipedia.org/wiki?curid=2876797", "title": "BINAC", "text": "BINAC\n\nDer BINAC \"(Binary Automatic Computer)\" war ein Großrechner, der 1949 durch die Eckert-Mauchly Computer Corporation gebaut wurde.\n\nDer \"BINAC\" war ein direkter Nachfolger der ENIAC, der auf einem binären Zahlensystem aufgebaut wurde (Die ENIAC verwendete ein Dezimalsystem). BINAC wurde durch J. Presper Eckert und John Mauchly der Eckert-Mauchly Computer Corporation entworfen und für die Northrop Corporation gebaut. Der Rechner hatte zwei unabhängige Prozessoren, CPU, jede CPU mit jeweils 512 Datenwort, Verzögerungsleitungspeicher basierend auf Quecksilber. Der BINAC verwendete ca. 700 Vakuumröhren. Die CPU besaß einen Taktzyklus von ca. 4.35 MHz, was sich in einer Zugriffszeit von ca. 10 Mikrosekunden manifestierte. Programme und Daten wurden über eine oktale Tastatur eingegeben.\n\nNorthrop holte den \"BINAC\" Rechner im September 1949 ab und transportierte ihn zu ihrer Niederlassung. Obwohl der BINAC Rechner die Akzeptanztests bestand, die bei Eckert-Mauchly durchgeführt wurden, funktionierte die Maschine bei der Northrop Corporation nie richtig. Wahrscheinlich wurde die Maschine mit ihren empfindlichen Vakuumröhren während des Transportes beschädigt.\n\n\n"}
{"id": "2878305", "url": "https://de.wikipedia.org/wiki?curid=2878305", "title": "EGovernment Computing", "text": "EGovernment Computing\n\neGovernment Computing – die Fachzeitschrift für die Digitalisierung der Verwaltung und Öffentliche Sicherheit ist eine deutsche Fachzeitschrift für Informationstechnik. Sie richtet sich an die IT- und eGovernment-Entscheider in Bund, Land und Kommune, um die öffentlichen Einrichtungen über die digitale Informationsverarbeitung im Public Sector zu informieren. Die erste Ausgabe erschien im November 2001 zur Kongress-Fachmesse „Moderner Staat“.\n\nDie Publikation gibt einen Überblick über aktuelle Technologien und politische Entwicklungen rund um die Themen Verwaltungsdigitalisierung, eGovernment und IT-Sicherheit im Public Sector.\n\neGovernment Computing erscheint bei den Vogel IT-Medien in Augsburg, ein Tochterunternehmen der Vogel Communications Group.\n\n"}
{"id": "2882962", "url": "https://de.wikipedia.org/wiki?curid=2882962", "title": "MicroStation", "text": "MicroStation\n\nMicroStation ist eine CAD-Software des Unternehmens Bentley Systems. Aktuelle Versionen laufen nur auf dem Betriebssystem Windows, obwohl sie früher auch für Macintosh-Plattformen und viele Unix-basierende Betriebssysteme verfügbar waren.\nMicroStation ist das Grundprodukt einer Architektur- und Konstruktionssoftware, die durch Bentley Systems, Incorporated entwickelt wird. Neben vielen anderen Dingen können mit dieser Software 2D/3D Vektorobjekte und -elemente erzeugt werden.\n\nDas Dateiformat ist das DGN (DesiGN file) Format, obwohl MicroStation auch viele zusätzliche Standard CAD Formate lesen und schreiben kann. Dies schließt auch das DWG und DXF ein. Über die zahlreichen verschiedenen Versionen wurden zusätzliche erweiterte Funktionalitäten im Bereich der Modellierung und des Rendering hinzugefügt. Darunter sind auch die Verwendung boolescher Grundkörper, raytracing, Particle Tracing und keyframe animation. Es gibt unter anderem spezielle Arbeitsumgebungen für Architektur, Tiefbau, Kartografie und Anlagenbau.\n\nIm Jahr 2000 wurde das DGN Dateiformat in der Version MicroStation V8 erstmals grundlegend verändert, um zusätzliche Features wie Digitale Rechte und Signaturen, Zeichnungshistorie -eine Art der Revisionsverwaltung- und eine bessere Unterstützung des AutoDesk DWG-Formates zu ermöglichen.\n\nZahlreiche Erweiterungen stehen zur Verfügung, die zusammen mit MicroStation eingesetzt werden können, um dessen Funktionalität zu erweitern. Dies sind zum Beispiel:\n\nZusätzliche Erweiterungen können erzeugt werden unter Verwendung:\n\nMicroStation wurde zunächst als IGDS (Interactive Graphics Design System) für den PC entwickelt. Damals wurde das Produkt noch „Pseudo Station“ genannt.\n\n1987 wurde MicroStation 2.0 herausgegeben. Dies war die erste Version, die das DGN-Format lesen und schreiben konnte.\n\nFast zwei Jahre später kam MicroStation 3.0 auf den Markt, das speziell im Hinblick auf die schnell anwachsende Leistungsfähigkeit der Computer optimiert wurde.\n\nMicroStation 4.0 wurde Ende 1990 fertig und verfügte über zahlreiche neue Funktionen: Ausschneiden und Maskieren von Referenzdateien, ein DWG-Übersetzer, verschiedene Zaun-Modi zur Auswahl von Elementen, die Fähigkeit, Ebenen (Level, Layer) zu benennen und Änderungen des GUI, um nur einige zu nennen.\n\n1993 wurde MicroStation 5.0 veröffentlicht und brachte wiederum zahlreiche neue Funktionen mit sich: Unterstützung binärer Rasterdateien, benutzerspezifische Linienstile, einen Einstellungsmanager und das sog. dimension drive design, eine parametrische Form zur Erstellung von Zeichnungen.\n\n1995 kam Windows 95 auf den Markt. Bentley folgte schon bald mit einer MicroStation-Version für dieses Betriebssystem, der erste Streifzug in die 32-Bit-Welt. Dies war die erste Version, die nicht die Versionsnummer im Namen trug. Sie hieß MicroStation 95, war jedoch intern MicroStation v5.5. \n\nDie letzte Version, die für mehrere Betriebssysteme verfügbar war, war MicroStation SE (SE bedeutet Special Edition). Die interne Versionsnummer lautet MicroStation 5.7 und wurde 1997 veröffentlicht. Diese Version beinhaltete farbige Icons, die – wie in Microsoft Office 97 – in eine randlose Darstellung umgeschaltet werde konnten. Die neuen Funktionen dieser Version beinhalten Mechanismen für das Arbeiten über das Internet. In dieser Version wurde auch eine erweiterte Präzision und der PowerSelector hinzugefügt.\n\nMicroStation/J (auch MicroStation 7.0, bzw. MicroStation V7) wurde ein Jahr nach MicroStation SE veröffentlicht. Das J im Namen steht für Java, da in dieser Version die Programmierschnittstelle MDL um Java erweitert wurde. Diese Erweiterung wurde JMDL genannt. Weitere Verbesserungen sind QuickvisionGL und ein überarbeitetes Hilfesystem. MicroStation/J war die letzte Version, die auf dem Dateiformat IGDS aufbaute. Dieses Dateiformat – nahezu einzigartig in der Softwarebranche – wurde über einen Zeitraum von 20 Jahren nahezu unverändert genutzt. Dieses Dateiformat wird seitdem nach der letzten unterstützenden Version V7-Format genannt.\n\nIm Jahr 2001 wurde MicroStation V8 veröffentlicht. Das neue Dateiformat basiert auf der IEEE-754-Norm und wird DGN V8 genannt. Das neue Dateiformat erlaubt es, eine nahezu unbegrenzte Zahl von Ebenen zu definieren. Mit dem Übergang zur Gleitkomma-Arithmetik steht nun ein wesentlich größerer Zeichnungsraum zur Verfügung. Mit der Optimierung auf die Windows-Betriebssysteme ist nun auch eine wesentlich größere maximale Dateigröße möglich. Weitere Verbesserungen sind AccuSnap, Design History, Modelle (mehrere Zeichnungen in einer Datei), unbegrenztes Rückgängigmachen, VBA-Schnittstelle, True Scale, Standarddefinitionen für Arbeitseinheiten (das neue Format speichert intern in Metern und rechnet auf andere Einheiten um). Die wichtigste Erweiterung ist aber die Unterstützung des sehr verbreiteten DWG-Dateiformats von Autodesks AutoCAD.\n\nMicroStation V8 2004 Edition (V8.5) folgte drei Jahre später mit der Unterstützung neuerer DWG-Formate, Multi-snaps, PDF-generierung und Feature Modeling.\n\nMicroStation V8 XM (V8.9) wurde 2006 veröffentlicht. Die XM-Edition beinhaltet ein komplett überarbeitetes, auf Direct3D basierendes Grafiksubsystem. Diese Version kann PDF-Dateien referenzieren. Die Benutzeroberfläche ist neu gestaltet und beinhaltet Task-Navigation und Tastatur-Mapping.\n\nIn MicroStation ist V8i (V8.11) wurde die Task-Navigation überarbeitet und erweitert. Außerdem war ein Modul zur Bearbeitung von GPS-Daten enthalten.\n\nDie aktuelle Version von Microstation ist die CONNECT Edition (V10).\n\n"}
{"id": "2888532", "url": "https://de.wikipedia.org/wiki?curid=2888532", "title": "Bitboard", "text": "Bitboard\n\nEin Bitboard (Bitmap Board) ist eine Datenstruktur, die häufig Verwendung in Computerprogrammen für Brettspiele findet, insbesondere bei Schachprogrammen.\n\nDie Grundidee der Bitboard-Struktur ist es, dass sich die Frage, ob auf einem bestimmten Spielfeld eine bestimmte Figur vorhanden ist, mit \"ja\" oder \"nein\" beantworten lässt. Aufgrund dessen kann die Belegung eines Spielplans endlicher Größe als Sequenz einzelner Dualzahlen dargestellt werden, die jeweils den Wert 0 oder 1 annehmen können.\n\nDualzahlen („Bits“) stellen die Basis der meisten heutigen Computersysteme dar, weshalb darauf basierende Strukturen grundsätzlich sehr effizient verarbeitet werden können. Einfluss auf den effizienten Einsatz von Bitboard-Strukturen haben insbesondere\n\nGrundsätzlich sind Bitboard-Strukturen für verschiedene Brettspiele geeignet. So gibt es zum Beispiel auch Bitboard-Implementationen für Spiele wie Dame, Othello, Vier gewinnt oder auch Tic-Tac-Toe. Besondere Bedeutung haben Bitboards allerdings im Bereich des Computerschachs erlangt. Ein Schachbrett besteht aus 8×8=64 Feldern, ein zugehöriges Bitboard ist also 64 Bits lang. Diese Größe kann von modernen Computersystemen direkt als Datenwort verarbeitet werden, was einen großen potentiellen Geschwindigkeitsvorteil verspricht. Beispiele für Computerschach-Programme, welche Bitboards verwenden, stellen Crafty und die aktuelle Version 5.0 von GNU Chess dar.\n\nDer Hauptvorteil von Bitboard-Strukturen liegt in der potentiell sehr hohen Verarbeitungseffizienz, sowohl mit Blick auf Speicherplatz als auch vor allem mit Blick auf die Programmgeschwindigkeit. Eine komplette Schachstellung lässt sich z. B. in deutlich unter 150 Bytes kodieren, und viele Spielplan-Operationen können jeweils durch wenige Prozessorbefehle ausgeführt werden.\n\nDer hauptsächliche Nachteil von Bitboards liegt in ihrer „Andersartigkeit“ gegenüber älteren, weiter verbreiteten Darstellungsarten. Robert Hyatt, der Entwickler der bekannten Schachsoftware Crafty, schreibt über seine ersten Erfahrungen mit Bitboards:\n\nDa mittlerweile eine ganze Reihe von quelloffenen Bitboard-Implementationen existieren, dürfte dieses Argument gegen Bitboards allerdings nur noch schwach wiegen und in seiner Bedeutung weiter abnehmen.\n\nWie bereits erwähnt, ist ein Bitboard im Schach-Fall aufgrund der Größe des Schachbretts genau 64 Bits lang. Als Besonderheit gibt es 12 verschiedene Arten Figuren (Bauern, Türme, Springer, Läufer, Dame, König in jeweils beiden Farben). Aus diesem Grund kommen normalerweise mindestens 12, jedoch meistens noch mehr Bitboard-Strukturen gleichzeitig zum Einsatz.\n\nDie eingangs erwähnte Software Crafty beispielsweise speichert neben den Positionen der 12 Figurensorten in weiteren Strukturen die Positionen aller weißen bzw. schwarzen Figuren zusammengenommen, und zudem gedrehte Versionen des Spielbretts für weitere Optimierungen. Eine komplette Datenstruktur, die den momentanen Zustand des Spieles vollständig beschreibt, muss zudem Informationen über den Status z. B. von Rochade-Möglichkeiten, „en passant“-Zügen, der 50-Züge-Regel und gegebenenfalls weiteren Sonderregeln enthalten.\n\nDie Ausgangsposition (siehe Diagramm) führt für die weißen Bauern auf folgende Bitboard-Struktur (für die anderen Figurenarten gelten entsprechende Belegungen):\n\n00000000 00000000 00000000 00000000 00000000 00000000 11111111 00000000 .\n\nAuf welche Art „gezählt“ wird, also welches Feld des Schachbretts welchem Index in der Bit-Darstellung zugeordnet wird, ist letztlich wahlfrei. Bei diesem Beispiel und im Folgenden wird beginnend beim Feld A1 zeilenweise gezählt, also gehört zu A1 das Bit 0, zu A2 das Bit 1, und so weiter bis schließlich zu Feld H8 und Bit 63. Wie bereits erwähnt, verwalten einige Schachprogramme sogar Bitboard-Strukturen in verschiedener Zählweise (zeilen- oder spaltenweise, bzw. auch diagonal) gleichzeitig, da hierdurch bestimmte Operationen effizienter möglich sind.\n\nEin zentrales Problem bei der Umsetzung eines Schachprogramms stellt die Berechnung der möglichen Folgezüge aus einer gegebenen Position heraus dar. Bei Benutzung von Bitboard-Strukturen erfolgt dies durch logische Operationen auf der Spielplan-Belegung. Hierbei müssen zwei Arten von Figuren unterschieden werden: bei den „springenden“ Figuren wie Bauern, Springern und König ist allein die Belegung des Zielfelds entscheidend. Bei den „gleitenden“ Figuren wie Türmen, Läufer und Dame ist die Zugmöglichkeit komplexer, da Figuren bestimmte Zielfelder blockieren können, ohne diese selbst zu belegen.\n\nBei diesen Figuren kommt es lediglich darauf an, ob auf dem Zielfeld eine Figur der eigenen Farbe platziert ist. Tatsächlich stellen die Bauern wiederum einen Sonderfall dar, da sie unterschiedlich ziehen, je nachdem ob sie eine gegnerische Figur schlagen oder nicht. Darauf soll hier jedoch nicht eingegangen werden.\n\nMan betrachte als Beispiel einen Springer auf Feld D4. Die möglichen Zielfelder sind im Diagramm zu sehen. Die Frage, ob der Springer grundsätzlich auf ein bestimmtes Zielfeld ziehen kann, ist wiederum eine mit \"ja\" oder \"nein\" zu beantwortende Frage, deren Antwort als Bitboard kodierbar ist. Für jedes Feld von A1 bis H8 kann ein solches „Angriffs-Board“ im Vorhinein berechnet und abgespeichert werden.\n\nIm gewählten Beispiel ist das Feld D4 das 28. Feld zeilenweise von A1 aus gezählt, also würde in einer Liste von 64bit-Zahlen der Index 27 (wenn 0 der erste Index ist) mit folgender Zahl belegt:\n\n00000000 00000000 00101000 01000100 00000000 01000100 00101000 00000000 .\n\nWenn diese insgesamt 64 möglichen Bitboards (für den Springer) beim Programmstart bereits berechnet werden, ist der Zugriff später also als einfache Index-Operation sehr effizient möglich. Um nun zu entscheiden, auf welche Felder der Springer tatsächlich im vorliegenden Fall ziehen kann, ist noch Information über die aktuelle Spielplan-Belegung in der eigenen Farbe erforderlich. Diese liegt entweder direkt vor oder kann aus den 6 Belegungen der einzelnen Figurensorten (durch bitweise \"ODER\"-Verknüpfung) bestimmt werden. Durch ein bitweises \"NICHT\" angewendet auf diese Belegung bestimmen sich die Felder, die nicht durch Figuren der eigenen Farbe belegt sind.\n\nDie logische Bedingung, die für einen Springer-Zug auf ein bestimmtes Feld erfüllt sein muss, ist nun gerade, dass dort keine Figur der eigenen Farbe stehen darf. In dem eben beschriebenen Komplement-Bitboard ist genau bei jedem Feld das Bit gesetzt, bei dem diese Bedingung erfüllt ist. Das gewünschte Ergebnis ergibt sich so schließlich durch bitweise \"UND\"-Verknüpfung mit dem vorberechneten „Angriffs-Board“ des Springers.\n\nMit leichten Anpassungen kann dasselbe Verfahren verwendet werden, um die Züge für Bauern und für den König zu berechnen.\n\nDiese Figuren bewegen sich grundsätzlich anders als die drei vorgenannten Arten. Bei diesen „gleitenden“ Figuren kommt es zusätzlich zur Belegung des Zielfelds darauf an, ob der Weg dorthin blockiert ist, sei es durch Figuren der eigenen oder der gegnerischen Farbe. Die Dame kann als Kombination aus Turm und Läufer interpretiert werden, was je nach genauer Wahl der Datenstrukturen eine Vereinfachung darstellen kann.\n\n"}
{"id": "2894367", "url": "https://de.wikipedia.org/wiki?curid=2894367", "title": "IPod touch", "text": "IPod touch\n\nDer iPod touch ist ein mobiler Mediaplayer, eine Spielekonsole, ein Organizer und eine WLAN-Plattform des amerikanischen Unternehmens Apple. Er ist Bestandteil der iPod-Produktlinie. Betrieben wird der iPod touch als einziges Gerät in der iPod-Linie mit dem kompletten iOS, wie es auf iPhones und iPads läuft. Der erste iPod touch erschien am 5. September 2007. Inzwischen sind sechs verschiedene Generationen erschienen, die sich in ihren Ausstattungsmerkmalen unterscheiden.\n\nBis Mai 2013 verkaufte Apple mehr als 100 Millionen iPod touch-Geräte.\n\nAm 5. September 2007 stellte Apple den iPod touch bei einem Special Event in San Francisco vor. Das Gerät basierte auf der Technik des iPhones und wurde über einen Multi-Touch-Bildschirm bedient. Dieser nimmt wie beim iPhone einen Großteil der Fläche der Gehäuseoberseite ein. Das Gehäuse ähnelte stark dem des iPhone, ist allerdings flacher und wirkt noch flacher wegen der zur Rückseite hin abgerundeten Ecken. Das Gerät verfügte als erster iPod über WLAN. Der \"iPod Dock Connector\" für eine Kabelverbindung war ebenfalls vorhanden.\n\nDie zweite Generation des iPod touch wurde am 9. September 2008 vorgestellt. Das Design war leicht überarbeitet worden: Die Rückseite war nun leicht gewölbt, hinzu kamen ein integrierter Lautsprecher und ein Lautstärkeregler an der linken Seite. Im Vergleich zur ersten Generation wurde das Gerät um 0,5 Millimeter dicker. Technische Neuerungen sind Bluetooth-Fähigkeit und ein integrierter Empfänger für das Nike+iPod-System. Die Akkulaufzeit erhöht sich bei Musikwiedergabe von 22 auf 36 Stunden, bei Videos von fünf auf sechs Stunden. Außerdem wurde erstmals in einem iOS-Gerät ein System on a Chip („SoC“) eingesetzt, das S5L8720. Auch die Auslieferungsfirmware, iPhone OS 2.1.1, brachte neue Features, beispielsweise den App Store.\n\nDie Geräte wurden mit 8, 16 und 32 GB angeboten. Nach der Veröffentlichung des iPod touch der 3. Generation wurde der iPod touch 2G dann nur noch in der 8-GB-Version angeboten; sein Bootrom wurde leicht modifiziert und war nun mit dem des iPod touch 3G identisch.\n\nAm 9. September 2009 stellte Apple einen technisch modifizierten, äußerlich unveränderten iPod touch vor, der als „iPod touch late 2009“ vertrieben wurde. Umgangssprachlich wurde er „iPod touch 3G“ genannt, da er der dritte bisher erschienene iPod touch war. Bei der Vorstellung des iPhone OS 4.0 nutzte Apple den Namen „iPod touch 3rd generation (late 2009)“. Ein neues SoC, das S5L8922, bot einen neuen Prozessor (ARM Cortex A8) und verdoppelten Speicher (256 MB). Es gab Modelle mit 32 GB und 64 GB Kapazität. Der iPod touch verfügte nun zusätzlich über eine Sprachsteuerung und über „VoiceOver“. Die Laufzeit des Akkus verringerte sich auf 30 Stunden bei Musikwiedergabe. Die Auslieferungssoftware war iPhone OS 3.1. Apple bewarb das Gerät im Fernsehen und auf der eigenen Homepage als „mobile Spielkonsole“.\n\nDie vierte Generation wurde am 1. September 2010 vorgestellt. Sie verfügt über ein Retina-Display mit einer Auflösung von 960 × 640 Pixel, Unterstützung für Wi-Fi 4, eine Kamera auf der Rückseite, die auch HD-Videos aufnimmt und Fotos mit 0,7 Megapixel macht, eine zweite Kamera auf der Vorderseite mit VGA-Auflösung sowie ein Mikrofon und einen Gyroskop. Die Kameras bieten keinen Autofokus und nur eine statische Fokussierung. Das SoC ist ein Apple A4.\n\nDer iPod touch 4G ist mit 7,2 Millimetern fast 1½ Millimeter dünner als seine Vorgängermodelle, hat aber die gleiche Rückwand aus Aluminium. Anders als beim Vorgänger gab es wieder eine Variante mit 8 GB Speicherplatz. Dank der Kamera auf der Vorderseite unterstützt der iPod touch nun „FaceTime“, Apples Videotelefoniedienst. FaceTime-Anrufe sind nur zu Geräten mit der entsprechenden Software möglich – andere iPod-touchs, iPhones, iPads oder Macs. Laut Apple sollte der Akku bei Musikwiedergabe bis zu 40 Stunden, bei Videowiedergabe bis zu sieben Stunden halten.\n\nAb dem 12. Oktober 2011 war der iPod touch der vierten Generation auch in Weiß und mit 16 GB erhältlich. Seit Ende Oktober 2012 wurde er nur noch mit 16 und 32 GB angeboten; am 30. Mai 2013 wurde der Verkauf eingestellt.\n\nAm 12. September 2012 stellte Apple die fünfte Generation vor. Sie verfügt über das Apple-A5-SoC und ein größeres Retina-Display, eine iSight-Kamera auf der Rückseite, die Full HD-Videos und Fotos mit 5 Megapixel (2592 × 1936 Pixel) aufnimmt, eine FaceTime-Kamera auf der Vorderseite sowie ein Mikrofon. Erstmals ist auch ein LED-Blitz für Fotoaufnahmen vorhanden, der auch als Videoleuchte oder Taschenlampe dienen kann. Das Modell ist mit 6,1 Millimetern um 1,1 Millimeter dünner als sein Vorgängermodell. Laut Apple soll der Akku bei Musikwiedergabe bis zu 40 Stunden, bei Videowiedergabe bis zu acht Stunden halten. Der iPod touch ist mit einem Unibody-Gehäuse aus anodisiertem Aluminium in den Farben Pink, Gelb, Blau, Weiß und Schwarz sowie einer roten Product Red-Sonderedition, bei der Apple einen Teil der Erlöse an den \"Global Found to fight AIDS\" in Afrika spendet, erhältlich. Auf der Rückseite kann eine Handschlaufe (\"iPod touch loop\") befestigt werden. Das Gerät führt die Sprachsteuerung Siri auf dem iPod ein. Das Gerät hat keinen Umgebungslichtsensor zur automatischen Bildschirmhelligkeitseinstellung – laut Phil Schiller, Manager und Marketing-Vizepräsident von Apple, aus Platzgründen. Das Modell unterstützt Bluetooth 4.0 und Wi-Fi 4 neben dem 2,4 GHz-Band auch erstmals im 5-GHz-Frequenzband. Auf dem iPod läuft Apples mobiles Betriebssystem iOS 9.3.5. Er steht in Versionen mit 16, 32 und 64 GB internen Speicher zur Verfügung.\n\nAm 30. Mai fügte Apple eine 16 GB-Variante mit gleichen Spezifikationen wie die 5. Generation hinzu, jedoch ohne die rückseitige Kamera, die \"Loop\"-Befestigung und bunt gefärbte Rückseiten; sie wird nur mit einer schwarzen Front- und silberfarbener Rückseite geliefert. Am 10. September 2013 ersetzte Apple die schwarze Variante durch eine graue Version. Am 26. Juni 2014 wurde das bisherige 16-GB-Modell durch eine Version, die baugleich mit der 32- und 64-GB-Version ist, ersetzt. Gleichzeitig wurden die Preise gesenkt, am 9. März 2015 jedoch wegen des Wechselkurses wieder um 30 € (das große Modell um 40 €) angehoben.\n\nDie sechste Generation wurde am 15. Juli 2015 veröffentlicht. Sie gleicht im Aussehen der fünften Generation, besitzt nun das Apple A8-SoC und verzichtet auf die Kameraschlaufe. Die neue Kamera hat eine Auflösung von 8 Megapixeln und eine optionale Zeitlupenfunktion beim Filmen. Die Farben wurden, wie beim iPod nano und iPod shuffle, aktualisiert und entsprechen nun denen des iPhone 6: Silber, Gold und „Spacegrau“ sowie die neuen Farben Dunkelblau und Pink. Auch eine Product Red-Sonderedition gibt es wieder. Weiter wurde eine 128-GB-Variante eingeführt, für einen Aufpreis von 110 € gegenüber der 64-GB-Variante. Die Preise der anderen Ausführungen entsprechen denen des Vorgängers.\n\nNachdem die übrigen iPods am 27. Juli 2017 eingestellt wurden, wurden die Preise des iPod touch gesenkt. Die Ausführungen mit 16 und 64 GB wurden nicht mehr verkauft. Die 32-GB-Variante kostet seitdem 229 € beziehungsweise 229 CHF (vorher 279 € in Deutschland, 289 € in Österreich und 279 CHF in der Schweiz). Die 128-GB-Variante gibt es für 339 € in Deutschland, 349 € in Österreich und 359 CHF in der Schweiz (vorher 449 € in Deutschland, 469 € in Österreich und 449 CHF in der Schweiz).\n\nDas System-on-a-Chip (SoC) im iPod touch ist bis einschließlich zur 5. Generation ein \"Samsung S5L\"-SoC. Seit der vierten Generation wird es offiziell beworben und heißt „Apple Ax“. Die interne Bezeichnung Samsung S5L wurde jedoch beibehalten. Zuvor hat Apple kaum Informationen über seine Geräte preisgegeben. Die Bezeichnung \"Apple Ax\" wird umgangssprachlich fälschlicherweise dem Prozessor gleichgesetzt, ein SoC enthält jedoch neben dem Prozessor auch alle anderen wichtigen Komponenten eines Computers. Seit der 6. Generation setzt Apple das von TSMC hergestellte Apple A8-SoC ein.\n\nDas Betriebssystem des iPod touch, das iOS, ist das gleiche, das auch beim iPhone und beim iPad verwendet wird. Bei der Einführung des iPhones sagte Steve Jobs lediglich, dass das iPhone Mac OS X verwendet, ab der Version 2.0 war die Rede von \"iPhoneOS\". Ab der Version 4.0 hieß es dann \"iOS\". Die Bedienung von iOS erfolgt wie bei den anderen Geräten über einen Touchscreen mit Multi-Touch-Funktion. Für den iPod touch sind Anwendungsprogramme (sogenannte „Apps“) aus dem App Store verfügbar. Grundsätzlich handelt es sich um die gleichen Apps wie beim iPhone. Allerdings sind bestimmte Anwendungen, die auf nicht vorhandene Hardwarekomponenten wie beispielsweise GPS oder Telefondienste zurückgreifen, nicht erhältlich. Ein bekanntes Beispiel für eine App, die nicht auf dem iPod touch erhältlich ist, ist WhatsApp. Vor iOS 2.0 gab es nur Web-Applikationen, die mit dem Browser „Safari“ aufgerufen werden mussten. Apps, die im App Store veröffentlicht werden sollen, werden von Apple zuvor geprüft. Außerdem kann Apple jede App aus dem Store entfernen oder nicht zulassen. Das Installieren von nicht autorisierten Apps ist nur durch einen Jailbreak oder Einstellungsprofile möglich.\n\n"}
{"id": "2903623", "url": "https://de.wikipedia.org/wiki?curid=2903623", "title": "Ohio Scientific Superboard II", "text": "Ohio Scientific Superboard II\n\nDer Ohio Scientific Superboard II, ab 1978 für 279 US$ verkauft, war ein typischer Vertreter der frühen Generation von Mikrocomputern. Basierend auf dem 6502-Prozessor bestand der ganze Computer nur aus einer großen Hauptplatine, auf der sich auch die Tastatur befand (wie damals weit verbreitet ohne Cursortasten und Ziffernblock). Gehäuse und Netzteil waren als Zubehör erhältlich.\n\nDer OS Superboard II verfügte über eine 40 × 25 Zeichen Textausgabe per Videoausgang (damals für Computer dieser Art und Preisklasse nicht unbedingt selbstverständlich) und einen Kassettenport nach Kansas City Standard zur Datensicherung. Im ROM befanden sich ein Microsoft BASIC und ein Monitorprogramm, der RAM-Speicher konnte mit 4 KB oder 8 KB bestückt werden.\n"}
{"id": "2907535", "url": "https://de.wikipedia.org/wiki?curid=2907535", "title": "Zattoo", "text": "Zattoo\n\nZattoo ist eine Streaming-Plattform zur IP-basierten Übertragung von Fernsehprogrammen und Videos auf Abruf auf unterschiedliche Endgeräte. Das Unternehmen hat seinen Hauptsitz in Zürich und weitere Standorte in Berlin, Ann Arbor (USA) und Singapur. Zattoo ist in zwei Geschäftsbereichen aktiv. Neben einem eigenen Endkundengeschäft für Fernsehstreaming in Deutschland und der Schweiz ist Zattoo für Geschäftskunden ein Partner im Telekommunikationsbereich weltweit.\n\nIn beiden Ländern kann ein Basisangebot des Dienstes nach vorheriger Registrierung kostenlos genutzt werden („Zattoo Free“). Mit dem kostenpflichtigen Angebot „Zattoo Premium“ stehen dem Nutzer mehr Fernsehsender und Funktionen wie Aufnahme und zeitversetztes Ansehen zur Verfügung.\n\nZattoo ist mit rund 20 Millionen registrierten Nutzern der größte Fernsehstreaminganbieter in Europa und hat monatlich rund 1,5 Millionen Nutzer auf der Plattform. Unterstützt werden PCs, Smartphones und mobile Endgeräte von Apple, mit Android und Windows. Zattoo bietet zudem Fernseh-Applikationen für internetverbundene Fernseher wie Samsung Smart TV, Xbox One, Amazon Fire TV, Apple TV, Android TV und unterstützt Streaming über Apple Air Play und Chromecast. Das Programmangebot umfasst mehr als 200 Fernsehprogramme, davon mehr als 100 in Deutschland. Dazu gehören öffentlich-rechtliche, private und internationale Sender.\n\nZattoo bietet eine Restart-Funktion an zum Neustart laufender Sendungen, sowie die Replay-Funktion. Bei der letzteren Funktion steht das gesamte Fernsehprogramm der letzten sieben Tage auf Abruf und gilt für den Premium-Tarif. Restart soll neben den Öffentlich-Rechtlichen insbesondere auch für die Sender der großen privaten Sendergruppen RTL und ProSiebenSat.1 funktionieren. Sie steht in den Premium-Tarifen zur Verfügung und bei einzelnen Privatsendern auch im kostenlosen Basistarif. Zudem lassen sich Sendungen mit der Aufnahmefunktion aufzeichnen. Bis zu 30 Sendungen können in der Aufnahmeliste gespeichert werden. Seit Anfang Januar 2018 können Nutzer infolge einer neuen EU-Richtlinie, die Ländergrenzen für Streaminginhalte aufhebt (Portabilitätsverordnung), via Zattoo deutsches Fernsehen auch im EU-Ausland empfangen.\n\nIm Bereich mit Firmenkunden bietet Zattoo seit 2012 einen komplett gehosteten und gemanagten Over-the-top content (vom Internet-Service-Provider unabhängiges Streaming) und IP-basierte Fernsehübertragung für Netzbetreiber und Medienunternehmen, live und als Video on Demand inklusive Encoding, Transcoding, Multi-DRM (Nutzungskontrolle) und Speicherung bis hin zum Playout sowie die Endkunden-Applikationen („TV-as-a-Service-Ansatz“, analog zum SaaS-Ansatz). Der Fokus liegt dabei auf einem White-Label-Produkt mit Standardapplikationen für alle relevanten Endgeräte. Alle Dienstleistungen sind End-to-End-Lösungen. Es werden derzeit 20 Geschäftskunden aus Europa und den USA bedient, die die technische Infrastruktur von Zattoo nutzen und ihrerseits über mehrere Millionen Endkunden verfügen.\n\nZattoo nahm den Betrieb anlässlich der Fußball-Weltmeisterschaft 2006 mit zunächst vier frei empfangbaren Schweizer Fernsehkanälen auf. Das Netzwerk trat im Dezember 2006 ins öffentliche nutzbare Entwicklungsstadium und ist seit September 2007 in Deutschland verfügbar. Der Zattoo-Client wurde für die Betriebssysteme Windows und macOS entwickelt und zum kostenlosen Herunterladen bereitgestellt. Übertragungen fanden dabei nach dem Prinzip P2PTV über ein Peer-to-Peer-Netzwerk statt. Der anfänglich verfügbare Linux-Client wurde eingestellt.\n\nIn der Schweiz war die Nutzung von Zattoo trotz der Verfügbarkeit mehrerer Privatsender kostenlos und legal. Seit dem 1. September 2008 zieht das Schweizer Bundesamt für Kommunikation die Gebühren für Radio- und Fernsehempfang auch bei Internetnutzern ein.\n\nEnde 2009 wurde das Peer-to-Peer-Prinzip aufgrund zu schmaler Uploadraten der DSL-Anschlüsse abgeschafft. Es wurde auf einen direkten Serverstream von Zattoo umgestellt. Nach dem Start mit Programmübertragungen für Desktop-PCs hat Zattoo seine Verfügbarkeiten erweitert. Fernsehen lässt sich mit Zattoo auf Computern, Smartphones und Tablets (iOS, Android, Windows 10) schauen. Seit Anfang 2011 ist der Empfang auch auf mobilen Endgeräten möglich, seit August 2011 auch auf internetverbundenen Fernsehern sowie zahlreichen Streamingboxen und Spielkonsolen (Samsung Smart TV, Xbox One, Amazon Fire TV, Apple TV, Android TV und unterstützt Streaming über Apple Air Play und Chromecast).\n\nDas erste Premium-Bezahlfernsehangebot startete mit Zattoo+ im September 2011. Im Juni 2012 übertrug Zattoo, ebenso wie die Angebote der öffentlich-rechtlichen Sendeanstalten von ARD und ZDF, die Fußball-Europameisterschaft 2012 im Livestream im Internet.\n\nIn Deutschland wurden die Programme der Mediengruppe RTL Deutschland (RTL, RTL II, VOX, Super RTL und n-tv) am 13. Juni 2013 und Sport1 am 29. Januar 2014 aufgeschaltet. Nachdem die ProSieben-Sat1-Gruppe im April 2014 ins Zattoo-Angebot aufgenommen worden war, wurden die Preise für das Premium-Abo aufgrund der gestiegenen Lizenzkosten auf mehr als das Doppelte erhöht. In der Schweiz blieben diese Sender weiterhin kostenlos.\n\nSeit November 2015 bietet Zattoo zusätzlich zu den Fernsehprogrammen in Echtzeit auch Video-on-Demand von ProSiebenSat.1, Studio Hamburg, Spiegel TV und BBC an.\n\nSeit März 2017 bietet Zattoo Werbetreibenden personalisierte Werbeausspielung: Mithilfe des Dynamic-Ad-Insertion-Verfahrens werden Werbeblöcke aus dem Livestream von Fernsehprogrammen ausgeschnitten und über den Adserver in Echtzeit neu befüllt, d. h. lineare Fernsehwerbeblöcke werden erstmals im Unicast durch digitale Spots framegenau ersetzt.\n\nIm Februar 2018 hat Zattoo einen neuen Geschäftssitz in Singapur eröffnet, um von dort aus in der Asien-Pazifik-Region weiter zu expandieren.\n\nPer 28. Januar 2019 übernahm Zattoo das Endkunden-Geschäft von Magine TV Germany, das seinen TV-Streaming-Service in Deutschland einstellt. 150.000 betroffene Nutzer erhalten ein Willkommensangebot für einen Wechsel zu Zattoo.\n\nDer Dienst wird schwerpunktmäßig in der Schweiz und in Deutschland angeboten.\n\nZattoo verfügt über Lizenzen für die ausgestrahlten Sender, weshalb jeder Sender nur für Nutzer aus den Staaten verfügbar ist, für die eine Lizenz besteht. Die Software prüft mittels Geotargeting der IP-Adresse des Benutzer-Computers, aus welchem Land der Dienst in Anspruch genommen wird, und ermöglicht ausschließlich den Empfang der für dieses Land lizenzierten Fernsehprogramme. So ist beispielsweise die deutsche Privatsendergruppe ProSiebenSat.1 zwar in der Schweiz, nicht jedoch in Deutschland kostenfrei zu empfangen.\n\nNach langen Verhandlungen haben sich die öffentlich-rechtlichen Anstalten und Zattoo auf eine einjährige Testphase für ihre gesamte Programmpalette über Zattoo geeinigt. Die öffentlich-rechtlichen Fernsehsender, einige kleinere Privatsender sowie 20 Hörfunkprogramme werden seit dem 1. April 2008 über die deutsche Version von Zattoo verbreitet. Seit dem 1. April 2009 ist der Vertrag mit den öffentlich-rechtlichen Fernsehsendern ausgelaufen. Vorläufig wurde die fortgeführte Ausstrahlung jedoch weiter geduldet. Auf Anfrage wurde im April 2011 von Seiten der „Programmdirektion – Zuschauerredaktion Das Erste“ jedoch per E-Mail bestätigt, dass zwischen der ARD und Zattoo ein entsprechendes Kooperationsabkommen bestehen würde.\n\nBei Wiedergabe mit der eigenen Zattoo-Software sind die übertragenen Inhalte praktisch kopiergeschützt, die Datenströme lassen sich mit dem Zattoo-Player nicht aufnehmen. Seit 2010 gibt es für Schweizer Nutzer die kostenpflichtige Option, alle empfangbaren Kanäle aufzuzeichnen. Sie stehen als „Recalls“ bis zu sieben Tage lang zur Verfügung. Zusätzlich können bis zu 500 Sendungen ohne zeitliche Einschränkung gespeichert werden.\n\nFolgende Sender sind in der Schweiz verfügbar. Einige Kanäle (HD, Zattoo+, Erotik, bosnische, kroatische, polnische, türkische und portugiesische Sender) sind nur mit entsprechendem Abonnement empfangbar.\nDie folgenden Sender sind in Deutschland verfügbar. HD-Sender (auch öffentlich-rechtliche) und alle Sender der RTL Group und die der Pro7-Sat.1-Gruppe (Pro 7, Sat.1, sixx, Kabel1) können nur mit dem kostenpflichtigen Paket Zattoo Premium empfangen werden.\nChip.de hat im Juni 2017 in einem Vergleich von fünf kostenpflichtigen Streamingdiesten \"Zattoo Premium\" mit der höchsten Punktzahl und mit dem besten Preis-Leistungs-Verhältnis ausgezeichnet. Auch in einer Liste der kostenfreien Streamingdienste wurde \"Zattoo Free\" mit „Sehr gut“ angegeben.\n\n"}
{"id": "2908384", "url": "https://de.wikipedia.org/wiki?curid=2908384", "title": "A-Plan", "text": "A-Plan\n\nA-Plan ist eine Software für die Projekt- und Ressourcenplanung. Die derzeit aktuelle Version ist A-Plan 2016.\n\nA-Plan sieht sich als Alternative für konventionelle Projektmanagementprogramme, die für kleinere und mittelgroße Projekte oft zu aufwändig und komplex sind. Das Programm verfügt zwar über alle wesentlichen Funktionen eines Projektmanagementprogrammes, verzichtet aber bewusst auf Features, deren sinnvoller Einsatz entsprechende Vorkenntnisse benötigt. Hierzu gehört zum Beispiel die automatische Optimierung des Projektablaufes und Ressourceneinsatzes, die nur bei vollständiger Eingabe aller relevanten Randbedingungen die erwarteten Ergebnisse liefert.\n\nEin optionales Modul ermöglicht die Berücksichtigung von Istzeiten, die wahlweise auch über eine universelle Schnittstelle von anderen Programmen übernommen werden können.\n\nDie Software wird derzeit im deutschsprachigen Raum von über 120.000 Benutzern eingesetzt (Stand 03/2017), seit 2005 gibt es auch eine englische Version.\n\nAlle laufenden Projekte werden in einer gemeinsamen Datenbank gespeichert. Je nach Anzahl gleichzeitiger User kann hierfür Microsoft Access oder Microsoft SQL Server eingesetzt werden. Ein Datenaustausch mit Microsoft Project und die Synchronisation mit Microsoft Outlook ist möglich.\n\nSeit dem 21. September 2009 steht A-Plan auch in einer Freeware Variante zur Verfügung. Mit diesem Schritt stellt der Hersteller zusätzlich eine einfache und kostenlose Variante für den Einsteiger in die Projektplanung zur Verfügung. Angedacht ist die funktional eingeschränkte Free-Version vor allem für Privatpersonen, Schüler und Studenten, die eine einfache Projektplanung mit Hilfe eines Gantt-Diagramms / Balkenplans durchführen möchten.\n\n"}
{"id": "2910849", "url": "https://de.wikipedia.org/wiki?curid=2910849", "title": "MoviX", "text": "MoviX\n\nMoviX ist eine Sammlung von drei minimalistischen Linux-Distributionen, welche sich als Live-Systeme direkt von CD/DVD starten lassen und ähnlich einem DVD-Spieler auf das Abspielen von Filmen und anderen multimedialen Inhalten spezialisiert sind. Alle drei Varianten setzen dabei auf die Multimediasoftware MPlayer und laufen nach dem Start in einer RAM-Disk des Rechners, so dass das Bootmedium entfernt und beispielsweise gegen eine DVD-Video ausgetauscht werden kann. Das eigentliche MoviX ist eine kleine, wahlweise auch installierbare, kommandozeilenbasierte Multimediadistribution. MoviX² ist dem sehr ähnlich, hat aber eine grafische Benutzeroberfläche. Die eMoviX genannte Variante befindet sich zusammen mit dem abzuspielenden Inhalten auf CD oder DVD und startet direkt nach dem Einschalten des Computers den Abspielvorgang.\n\nDer MoviX-Entwickler Roberto De Leo suchte im Jahr 2002 nach einer Mini-Linux-Distribution, welche nur darauf ausgerichtet ist, multimediale Inhalte auf einem beliebigen Rechner ohne vorherige Softwareinstallation abzuspielen. Es gab zwar bereits eine Reihe funktionsreicher Live-Distributionen wie Knoppix, jedoch fand er keine Mini-Live-Distribution nur für einen Zweck, weshalb er beschloss, in Form von MoviX selbst eine zu erstellen.\n\nDie erste MoviX-Version mit der Nummer 0.2 veröffentlichte Roberto De Leo im September 2002. Damit war es eines der ersten Softwarepakete dieser Art im Bereich freier Software. In rascher Folge wurden neue verbesserte Versionen der Distribution veröffentlicht bis zur bislang letzten Version der ursprünglichen MoviX-Distribution im März 2004 mit der Versionsnummer 0.8.1.\n\nUm auch zum Abspielen eine grafische Benutzeroberfläche anzubieten, wurde MoviX² entwickelt. Die erste Version wurde kurz nach der Kommandozeilenvariante am 10. Oktober 2002 veröffentlicht. Die aktuelle Ausgabe von MoviX² Version 0.3.1rc1 wurde am 17. Juli 2004 freigegeben. Die bislang letzte Version der dritten Variante eMoviX wurde mit Version 0.9 im Juni 2005 veröffentlicht.\n\nMoviX baut in allen seinen Varianten auf der Linux-Distribution Slackware auf, welche um alle Funktionen und Softwarepakete bis auf die zur Wiedergabe multimedialer Inhalte reduziert und durch eigene Anpassungen für diesen Zweck speziell optimiert wurde. Es kann als Live-Systeme direkt von CD/DVD starten (sogenanntes bootfähiges Medium) und Filme und andere multimediale Inhalte direkt abspielen. Es ist somit von seiner Funktion einem DVD-Spieler ähnlich und lässt sich wie dieser auch per Fernbedienung steuern, sofern der Computer über eine per LIRC ansprechbare Schnittstelle verfügt.\n\nDie Medien können nach dem Start ohne das Boot-Medium abgespielt werden, sodass ein weiterer Mediendatenträger eingelegt werden kann. Das funktioniert, weil sich das gesamte Betriebssystem samt Abspielsoftware bereits vollständig im Arbeitsspeicher in einer RAM-Disk befindet. Die zum Abspielen der Multimedia-Dateien zentrale Anwendungssoftware der etwa 28 MB großen Kommandozeilenvariante MoviX ist das Kommandozeilenprogramm MPlayer, welches alle gängigen Multimediadateiformate wiedergeben kann. MoviX verwendet dabei zur Wiedergabe von Filmen keine grafische Oberfläche, wie das unter Linux gängige X Window System, sondern gibt diese direkt über den sogenannten Framebuffer auf dem Bildschirm aus. Daneben kann MoviX auch Bilder mithilfe des Programms fbi darstellen. Andere wichtige verwendete Programme und Bibliotheken sind SYSLINUX als Bootloader, der Linux-Kernel in der Versionsreihe 2.4, das Soundkartentreibersystem ALSA, die Fernbedienungstreibersoftware LIRC und die Kommandozeilenprogramme von BusyBox.\n\nIm Gegensatz zu MoviX startet die Variante MoviX² einen grafischen X-Server und verwendet zum Abspielen \"gmplayer\", ein grafisches Frontend für den Mplayer, verwendet aber ansonsten dieselben Softwarepakete wie MoviX. Aufgrund der grafischen Oberfläche ist MoviX² mit ungefähr 50 MB etwas größer.\n\nDie zur direkten Installation auf einer Multimedia-CD/DVD gedachte Variante eMoviX ist nur 8 MB groß und spielt mit Hilfe von MPlayer die auf den Datenträger enthaltenen Dateien nach dem Start des Rechners automatisch ab – unabhängig von der auf dem Rechner sonst installierten Software einschließlich des Betriebssystems. Bei eMoviX kann jederzeit die CD/DVD ausgeworfen werden und eine andere mit entsprechenden Inhalten abgespielt werden. Jedoch können nicht, wie bei MoviX, auch die Inhalte der PC-Festplatte abgespielt werden.\n\nMittels verschiedener externer Entwicklungen ist es möglich, speziell angepasste und konfigurierte MoviX-Distribution über einfache grafische Hilfsprogramme unter Linux beziehungsweise Unix zu erstellen. So können eMovix-Datenträger mit K3b, einer unter Linux und Unixderivaten sehr häufig eingesetzten Brennsoftware von KDE, direkt gebrannt werden. Mithilfe des Perlprogramms \"MoviXMaker\" ist es ebenfalls möglich sich eine angepasste MoviX-Distribution zu erstellen.\n\nAuch für Microsoft Windows gibt es zum einen direkt vom Movix-Projekt das \"eMoviX win32 Setup\" und von dritter Seite den \"MovixISOCreator\" um Movix-Datenträger einfach zu erstellen.\n\nDaneben haben Projekte wie Geexbox die Idee von MoviX weiterentwickelt. GeeXboX hat jedoch etwas höhere RAM-Anforderungen als die Varianten MoviX und eMoviX.\n\n"}
{"id": "2917164", "url": "https://de.wikipedia.org/wiki?curid=2917164", "title": "System/3", "text": "System/3\n\nDas System/3 war ein Minicomputer für mittelgroße Unternehmungen der Firma IBM. Der Rechner wurde erstmals 1969 ausgeliefert. Es wurden die Modelle 6, 8, 10, 12 und 15 gebaut.\n1978 wurde das System/3 (auch S/3) von dem Nachfolgeprodukt System/38 abgelöst. \n\nDer für damalige Verhältnisse geringe Platzbedarf sowie die Kosten machten das System/3 attraktiv für mittelgroße Unternehmungen. Neu war die 96-Spalten-Lochkarten statt der üblichen 80 Spalten, wobei das Lochkartenformat 1/3 der üblichen Größe hatte. Der Plattenspeicher war zwischen 2,45 und 9,80 Millionen Zeichen groß. Als Programmiersprache diente RPG II. Diese basierte auf RPG des IBM System/360. Auch wurde die Monolithic Systems Technology (MST), eine Hardwaretechnologie der 1970er Jahre, eingesetzt. \n\nEine typische Disk-orientierte Konfiguration bestand aus einem Zentralspeicher mit 12,288 Zeichen, einer 4,9 Millionen Zeichen Festplatte, einer Multifunktionslochkarteneinheit (MFCU), einem offline-Kartensortierer, einem Drucker (200 Zeilen pro Minute) und zwei Datenrecorder (für Lochkarten). Eine typische Lochkarten-orientierte Version hatte einen Hauptspeicher von 8.192 Zeichen, eine MFCU, einen offline-Kartensortierer, einen Drucker (100 Zeilen pro Minute) und einen Datenrecorder. \n\nAm Modell 15 konnten Plattenstapel der Type 3340 (Winchester) angeschlossen werden. \n\n\n\n\n"}
{"id": "2920096", "url": "https://de.wikipedia.org/wiki?curid=2920096", "title": "Stellwerksimulation", "text": "Stellwerksimulation\n\nBei der Stellwerksimulation handelt es sich um eine spezielle Form einer Eisenbahnsimulation, bei der der Eisenbahnbetrieb aus Sicht eines Stellwerkbedieners (Fahrdienstleiter oder Weichenwärter) simuliert wird. Der Anwender stellt in der Simulation die Weichen und Signale und sorgt für eine reibungslose, sichere und pünktliche Abwicklung des Zugverkehrs.\n\nDer Bereich einer Simulation ist räumlich klar abgegrenzt (im Gegensatz zu anderen Eisenbahnsimulationen) und umfasst in der Regel einen vorgegebenen Bahnhof oder einen längeren Abschnitt einer Eisenbahnstrecke. Stellwerksimulationen können sowohl der Freizeitbeschäftigung dienen als auch für Ausbildungszwecke eingesetzt werden.\n\nDie Hersteller von elektronischen Stellwerken setzen im Rahmen ihrer Qualitätssicherung und für die Schulung der Bediener Simulatoren ein, die auf der Originalhardware ihrer Produkte Erprobungen möglichen Einsatzes, auch der fehlerhaften Bedienung (zugelassen wird nur \"gegen vorhersehbare Fehler robuste Software\"), extremer Belastung usw. ermöglichen. Weitere Unterscheidungsmöglichkeiten bezüglich eingesetzter Außenanlagen und ihrer ebenfalls eingesetzten Simulationen sind zwar bekannt, aber mangels Freigabe von Quellen nicht öffentlich zugänglich.\n\nDie Stellwerksimulation für Unterhaltungszwecke stellt den Eisenbahnbetrieb spielerisch dar. Der Anwender muss dabei meist in kürzester Zeit auf neue Betriebssituationen reagieren und die Züge vorausschauend im simulierten Bereich steuern.\nEine beachtliche Zahl von entsprechender Software zur spielerischen Simulation von Stellwerken der verschiedenen Bauformen ist inzwischen verfügbar.\n\nZur Ausbildung von Studenten und zur beruflichen Weiterbildung unterhält die Deutsche Bahn gemeinsam mit Universitäten und Fachhochschulen Modellbahnanlagen, die mit echter Stellwerkstechnik bedient werden. Hierfür sind ehemalige Stellwerke unterschiedlichster Bauart in ein digitales System integriert.\n\n"}
{"id": "2920344", "url": "https://de.wikipedia.org/wiki?curid=2920344", "title": "Füllrate", "text": "Füllrate\n\nDie Füllrate (engl. \"fill rate\", \"fillrate\") ist die Anzahl der Pixel oder Texel, die ein Grafikprozessor in einem bestimmten Zeitraum berechnen und in den Grafikspeicher schreiben kann. Sie wird dementsprechend in Mega- bzw. Gigapixel oder Gigatexel pro Sekunde angegeben.\n\nWährend Ende 1996 erhältliche Grafikchipsätze eine Füllrate im zweistelligen MT/s-Bereich aufwiesen (so hatte z. B. der Voodoo-Graphics-Chipsatz eine Füllrate von 45 MT/s), erzielt man im September 2007 fast um den Faktor 1000 höhere Füllraten (G80 mit etwa 39 GT/s). Eine weitere Dekade später sind die Füllraten um den Faktor 5 gestiegen (GM200 mit etwa 190 GT/s).\n\nBei modernen Grafikchips tritt, wie auch bei anderen Prozessoren, die theoretische Rechenleistung (FLOPS), aber auch die Füllrate immer mehr in den Hintergrund, und sie können daher nicht mehr als direkte Vergleichsbasis herangezogen werden. Die Füllrate ist hierfür zwar am besten geeignet, kann aber aufgrund der verschiedenen Formen, sie zu errechnen, auch nicht als eindeutiger Vergleichswert herangezogen werden. Zudem setzt der Wert voraus, dass der Prozessor immer mit 100 % Effizienz (ungeachtet seiner Anbindung an das Bus-System oder der Leistung der anderen Komponenten im System) arbeitet.\n"}
{"id": "2926588", "url": "https://de.wikipedia.org/wiki?curid=2926588", "title": "Sprachverarbeitung (Medizin)", "text": "Sprachverarbeitung (Medizin)\n\nIn der Medizin werden Methoden der automatischen Sprachverarbeitung zur objektiven Beurteilung von Sprech- und Stimmstörungen (Dysphonie) eingesetzt. Im Gegensatz zu herkömmlichen invasiven Methoden ist für die automatische Sprachverarbeitung (Spracherkennung) kein invasiver Vorgang nötig. Daher eignen sich diese Methoden auch für Kinder und ältere Menschen, da mit der Untersuchung kein Risiko verbunden ist.\n\nIm Wesentlichen gibt es zwei Herangehensweisen an die automatische Untersuchung durch ein Sprachverarbeitungssystem: Die Untersuchung von gehaltenen Lauten (oft Vokale, wie z. B. ein langes \"A\") und die Untersuchung von gelesener Sprache.\n\nDie Untersuchung von gehaltenen Lauten ist schon seit längerem bekannt. Als Vertreter sind hier zu nennen:\n\nAls Nachteil der Methoden wird of genannt, dass ein einzelner Vokal die Sprache nicht repräsentativ darstellen kann. Der DSI versucht hier Abhilfe zu schaffen, indem verschiedene Phonationsparadigmen (maximale Phonationsdauer, höchste erzeugte Frequenz, kleinste Tonintensität und Jitter) zu einer Maßzahl verrechnet werden.\n\nDiese Herangehensweise ist eine neue Methode, die erst durch das Voranschreiten der Spracherkennungstechnik möglich wurde. Dabei wird einem Patienten ein bekannter Text wie z. B. Die Sonne und der Wind vorgelegt, den er in ein PC-Mikrofon vorlesen muss. Der Spracherkenner ermittelt dann die gesprochene Wortkette und kann verschiedene Analysen vornehmen, mit denen sich z. B. die Verständlichkeit ermitteln lässt.\n\n"}
{"id": "2926804", "url": "https://de.wikipedia.org/wiki?curid=2926804", "title": "EXAPT", "text": "EXAPT\n\nEXAPT (\"EXtended Subset of APT\") ist eine fertigungstechnisch orientierte Programmiersprache zur Erzeugung von NC-Programmen, um Steuerungsinformationen für Werkzeugmaschinen zu generieren, und ermöglicht die Berücksichtigung fertigungstechnischer Belange verschiedener Bearbeitungsverfahren.\n\nEXAPT ist nach industriellen Anforderungen historisch gewachsen. Es wurden im Laufe der Jahre diverse Software-Lösungen für die Fertigungsindustrie geschaffen, die heute ein breites, skalierbares Leistungsspektrum mit zukunftsorientierten Produkten und Dienstleistungen bilden.\n\nUnter dem Markennamen EXAPT versteht man heute in erster Linie ein CAD/CAM-System, sowie eine Produktionsdaten- und Toolmanagement-Software der deutschen Firma EXAPT Systemtechnik GmbH mit Sitz in Aachen.\n\nEXAPT ist ein modular aufgebautes Programmiersystem für alle NC-Bearbeitungen wie\nDie Haupt-Produktgruppen EXAPTcam und EXAPTpdo sind durch ihren modularen Aufbau stufenweise ausbaufähig und ermöglichen individuell zugeschnittene Software-Lösungen für die Fertigungsindustrie, die sowohl eigenständig als auch im Verbund mit einem vorhandenen IT-Umfeld einsetzbar sind.\n\nEXAPTcam deckt die Anforderungen zur NC-Planung insbesondere für die spanenden Verfahren wie Drehen, Bohren und Fräsen bis 5-Achs-Simultanbearbeitung ab. Dabei werden fortlaufend neue Verfahrenstechnologien, Werkzeug- und Maschinenkonzepte einbezogen. In die NC-Programmierung können Daten aus unterschiedlichen Quellen wie 3D-CAD-Modellen, Zeichnungen oder Tabellen einfließen. Die Möglichkeiten zur NC-Programmierung reichen von der sprachorientierten bis hin zur featureorientierten NC-Programmierung. Die integrierte EXAPT Wissensdatenbank sowie intelligente und skalierbare Automatismen unterstützen die Anwender. Die EXAPT-NC-Planung deckt auch die Erstellung der Fertigungsinformationen ab, wie Spann- und Werkzeugpläne, Voreinstelldaten oder Zeitberechnungen. Die realitätsnahen Simulationsmöglichkeiten der NC-Planung und NC-Steuerdaten bringen Sicherheit für die Fertigung.\n\nEXAPTpdo (EXAPT ProduktionsDatenOrganisation) liefert eine neutral einsetzbare Technologie-plattform für den Informationsverbund von der NC-Planung – bis in die Werkstatt. Das betrifft alle NC-Fertigungsdaten, die zum Rüsten von NC-Bearbeitungsmaschinen, für die Beschaffung, die Voreinstellung und die Lagerhaltung von Betriebsmitteln erforderlich sind und durch EXAPTpdo in einer zentralen Datenbank bereitgestellt werden. Neben klassischen Funktionen des Toolmanagementsystems (TMS), wie die Verwaltung von Zerspanwerkzeugen, Mess-, Prüf- und Spannmitteln ist auch das Technologiedatenmanagement und Tool Lifecycle Management (TLM) enthalten. Systemgestützte Verwendungsnachweise tragen z. B. dazu bei, den Betriebsmittelkreislauf durch gesicherte Bedarfsermittlung und Bedarfsdeckung in den Griff zu bekommen. Überflüssige Transporte und ungeplante dispositive Umstellungen entfallen, Lagerbestände werden reduziert, Rüstzeiten reduziert und der Durchsatz erhöht. EXAPTpdo synchronisiert primär beteiligte Systeme innerhalb der Wertschöpfungskette. Lagersysteme, MES-Systeme oder ERP-Systeme (z. B. aus den Bereichen Einkauf und Produktion) arbeiten nicht mehr isoliert voneinander, sondern interagieren miteinander. EXAPTpdo liefert die Basis zur Smart Factory, für mehr Flexibilisierung in der Produktion und eine beschleunigte Kommunikation.\n\nAls Spin-Off der Universitäten Aachen, Berlin und Stuttgart wurde 1967 mit Gründung des EXAPT-Vereins die Weiterentwicklung „EXAPT (EXtended Subset of APT)“ der Programmiersprache „APT (Automatically Programmed Tools)“ fokussiert und somit der erste Meilenstein für die EXAPT-Geschichte gelegt. Noch im gleichen Jahr steht das System EXAPT 1 für Bohr- und einfache Fräsaufgaben zur Verfügung.\n\n1969 beginnt die industrielle Anwendung von EXAPT 2 zur Programmierung von NC-Maschinen mit 2-achsiger Strecken- und Bahnsteuerung. Im Folgejahr startet die Entwicklung des EXAPT-Modul-Systems.\n\n1972 erfolgt die Bereitstellung von BASIC-EXAPT zur universellen, einheitlichen Programmierung aller NC-Aufgaben. Die Einsatzunterstützung erfolgt durch die EXAPT-Anwendungsberatung.\n\n1973 erfolgt die Bereitstellung von EXAPT 1.1 zur Programmierung von strecken- und bahngesteuerten Bohr- und Fräsmaschinen sowie von Bearbeitungszentren. Auf der Hannovermesse (IHA 73) wird der interaktive Zugriff zu einem Zentralrechner über ein Time-Sharing-Terminal für die Teileprogrammeingabe und -korrektur vorgeführt und leitet die Ablösung der Lochkarte ein.\n\n1974 werden die Möglichkeiten für den Einsatz von Prozessrechnern zur NC-Datenübertragung ausgelotet. EXAPT bietet als Vorreiter die Möglichkeit zur Ergebnissimulation beim Einsatz von Plottern mit der Darstellung von Werkzeugwegen und Werkzeug in Zuordnung zum Werkstück.\n\nIm April 1975 wird die EXAPT NC Systemtechnik GmbH gegründet mit dem Ziel, kleineren und mittleren Unternehmen durch ein vollständiges Produkt- und Dienstleistungsprogramm den Einstieg in die NC-Technik zu ermöglichen. Im Folgejahr wird das Systemangebot durch weitere Systembausteine und Service-Programme sowie die Bereitstellung von Postprozessoren erweitert.\n\n1978 werden die in den 1970ern begonnenen Entwicklungsarbeiten am EXAPT-Modulsystem abgeschlossen. Die unterschiedlichen Systemteile BASIC-EXAPT, EXAPT 1, EXAPT 1.1 und EXAPT 2 werden unter Nutzung moderner Softwaretechniken zu einem Gesamtsystem verbunden. Systemsupport und Anwendungsberatung entwickeln sich zu einem neuen Arbeitsschwerpunkt.\n\nAnfang bis Mitte der 1980er werden neben Veröffentlichungen neuer portabler Softwarebausteine für CAD/CAM-Anwendungen (wie z. B. CAPEX, NESTEX, CADEX, CADCPL) die erste Version des EXAPT-DNC-Systems und die Erweiterungen des EXAPT-NC-Programmiersystems für die Bearbeitung gekrümmter Flächen präsentiert.\n\n1988 erweitert EXAPT das Software-Produktangebot um Systeme für die Betriebsmittelverwaltung (BMO) und Fertigungsdatenorganisation (FDO). Zusammen mit firmenspezifischen Lehrgängen schult EXAPT allein im Jahr 1988 mehr als 1.300 Lehrgangsteilnehmer.\n\n1992 wird die erste Version der vollständig neuen Produktgeneration EXAPTplus vorgestellt und eine Geschäftsstelle Dresden eröffnet.\n\n1993 wird der Firmenname „EXAPT NC Systemtechnik GmbH“ in „EXAPT Systemtechnik GmbH“ geändert. EXAPTplus wird auf PC unter WindowsNT vorgestellt. Die Dezentralisierung des EXAPT-Systemeinsatzes erweitert den Anwendungsbereich erheblich. Im Folgejahr ist EXAPT-DNC auf handelsüblichen PC unter Windows lauffähig. Es kann auf Spezialhardware verzichtet werden und damit im Verbund mit dem datenbankgestützten EXAPT-Fertigungsdatenorganisationssystem (FDO) eingesetzt werden.\n\n1995 ist EXAPTplus auch für komplexe Einsatzfälle wie Bearbeitung von Formkanälen an Strangpresswerkzeugen einsatzbereit. EXAPT-CADI sorgt für den Transfer von 2D-CAD-Daten nach EXAPTplus. Mit der neuen Geschäftsstelle Gießen wird der Vertrieb gestärkt. Im Folgejahr wird zur direkten Bearbeitung von NC-Steuerdaten mit Werkzeugwegdarstellung und Visualisierung der Werkzeuge der EXAPT-NC-Editor entwickelt.\n\nIm Zuge des Markteintrittes von komfortablen 3D CAD-Systemen für die Solidmodellierung von Bauteilen wird 1997 eine umfangreiche Evaluation marktgängiger Systeme durchgeführt. Es fällt die Entscheidung für SolidWorks als Referenzsystem für die Solid-orientierte NC-Planung mit EXAPT.\n\n1998 wird die erste Lösung für den Geometriedaten-Transfer zwischen SolidWorks und EXAPTplus erstellt. Die EXAPT-Organisationssysteme sind jetzt (neben SQL) auch unter Oracle lauffähig. Die Nutzung von Client-Server-Lösungen unterstützt die Informationsflüsse in der Fertigung.\n\n1999 werden zur Unterstützung einer NC-gerechten Werkstückmodellierung in Verbindung mit EXAPTsolid AFR-Funktionen bereitgestellt. Für alle EXAPT-Systeme wird die Millenniumfähigkeit abgesichert. Mit AFR wird Neuland betreten für die Integration von Third-Party-Produkten.\n\n2002 wird EXAPT-BMG zur Erstellung und Visualisierung von Werkzeugen mit zusätzlichen Funktionen für den Zusammenbau aus Komponenten entwickelt. Die Erfassung von Werkzeugen mit deren geometrischen und technologischen Ausprägungen bietet umfassende Unterstützung der NC-Planung mit den EXAPT-Systemen.\n\n2003 steht EXAPTpdo bereit, um die Prozessketten in Fertigungsplanung und Fertigungsausführung unter den wachsenden Anforderungen sich ändernder Fertigungsbedingungen optimal zu gestalten.\n\n2004 erfolgen vielfältige Systemerweiterungen in EXAPTplus, EXAPTsolid, EXAPT-NC-Editor, EXAPTpdo für die Komplettbearbeitung auf Dreh-/Fräszentren mit Ergebnisabsicherung durch umfangreichere Simulation auf der Basis von realNC (Tecnomatix), für den Einsatz neuer komplexer Werkzeugsysteme und für den Verbundeinsatz zwischen ERP-Systemen wie SAP und intelligenten CNC-Systemen. Im Folgejahr wird EXAPTpdo für die auftragsübergreifende Rüstoptimierung und Betriebsmittelbereitstellung, insbesondere bei Einzel- und Kleinserienfertigung mit Anbindung an Beschaffung und physikalischer Bestandsverwaltung, erweitert.\n\n2006 stehen die EXAPT-Systeme im umfassenden Einsatz als Informationsplattform für die Fertigung, die Zeitwirtschaft und für ähnliche Anforderungen. Für EXAPTsolid erfolgen Erweiterungen für die featureorientierte Fräsbearbeitung und Maschinensimulation. Die NC-Programmierung komplexer Werkzeugmaschinen wie z. B. Dreischlitten-Dreh-/Fräszentren wird durch EXAPT-Systeme unterstützt, ebenso wie die Nutzung von multifunktionalen Werkzeugen.\n\n2007 wird ein Modul für die simultane 3-5-Achsfräsbearbeitung präsentiert.\n\n"}
{"id": "2929701", "url": "https://de.wikipedia.org/wiki?curid=2929701", "title": "Partimage", "text": "Partimage\n\nPartimage ist ein Programm um Speicherabbilder () von Festplattenpartitionen in eine Abbilddatei zu sichern oder daraus wiederherzustellen. Partimage steht als freie Software unter der GNU General Public License (GPL). Über alle Versionen summiert wurde es schon über 545.000 Mal von dem offiziellen Server heruntergeladen, dazu gehört es in vielen Linux-Distributionen zum Standard-Umfang.\n\nPartimage bietet die Möglichkeit, Partitionen über ein Rechnernetz auf andere Rechner wie einem Server oder auf einem Network Attached Storage (NAS) zu sichern oder davon wiederherzustellen, sofern dort ein \"partimaged\"-Server vorhanden ist. Zu sichernde Abbilder von Partitionen können entweder direkt oder durch verschiedene Verfahren komprimiert gesichert werden. Da im regulären Betrieb eingebundene Partitionen, wie die sogenannte \"root-partition\" im Regelfall nicht im Betrieb gesichert werden können, steht Partimage auch in Form einer \"Rettungs-CD\" () zur Verfügung. Dabei wird das zu sichernde System von der Rettungs-CD gebootet und erlaubt so beliebige Partitionen unabhängig vom Funktionszustand des Systems zu sichern bzw. zu restaurieren.\n\nPartimage teilt Abbilddateien standardmäßig in Dateien kleiner als 2 GB auf, da diverse ältere Dateisysteme wie FAT16 keine Dateien größer als 2 GB ermöglichen, diese Einstellung lässt sich jedoch, etwa für CD-R (650 oder 700 MB) oder für modernere Dateisysteme mit höheren Grenzen, ändern. Partimage verfügt zudem über eine Fortschrittsanzeige, die alle diesbezüglichen für Benutzer relevanten Daten anzeigt, so zum Beispiel die Größe der zu sichernden Imagedatei, den zur Verfügung stehender Speicherplatz auf dem Zielmedium oder die verstrichene und verbleibende Zeit.\n\nDas ext4-Dateisystem, Standard in Debian und davon abgeleiteten Distributionen wie Ubuntu, wird so wie ReiserFS-4 nicht unterstützt. Weiterhin ist es nicht möglich, aus den Festplattenabbildern einzelne Dateien zu extrahieren, ohne das Image komplett zu entpacken.\n\n"}
{"id": "2937891", "url": "https://de.wikipedia.org/wiki?curid=2937891", "title": "Distrowatch", "text": "Distrowatch\n\nDistrowatch (Eigenschreibweise: DistroWatch) ist eine seit dem 31. Mai 2001 aktive Website, die allgemeine Informationen, Neuigkeiten und ein Beliebtheitsranking zu verschiedenen Linux-Distributionen sowie zu anderen freien oder quelloffenen Betriebssystemen wie etwa OpenSolaris oder BSD anbietet.\n\nDistrowatch listet ausführlich die Versionsunterschiede einzelner Distributionen sowie die darin enthaltenen Pakete auf. Außerdem werden Informationen über die Prozessorarchitektur und das Dateisystem sowie der Preis angezeigt. Die Seite \"Top-Ten-Distributionen\" soll zudem einen ausführlichen Überblick über die heute führenden Distributionen verschaffen.\n\nDie Seitentreffer-Rangliste \"(Page Hit Ranking)\" von Distrowatch gilt als und wird oft zitiert, wenn es um die Popularität einzelner Distributionen geht. Das Bewertungssystem von Distrowatch ist aber auch anfällig für Betrügereien, und einige Hersteller wurden beschuldigt, dies zumindest versucht zu haben. Auch der Gründer der Seite, Ladislav Bodnar, ist sich dessen bewusst:\n\nErmittelt werden die Zahlen über die Zugriffe auf die Beschreibungsseiten der Distributionen. Diese und weitere, ausführlichere Distributionen-Ranglisten und Statistiken findet man auf Unterseiten von Distrowatch (siehe Weblinks).\n\nDistrowatch ermittelt ob und wenn ja, in welcher Version eine Software in einer Distribution enthalten ist und ermöglicht so, die Distributionen miteinander zu vergleichen. Dabei gelten für alle aufgeführten Systeme die gleichen Regeln; so wird etwa auch auf der Distrowatch-Seite zu OpenBSD angegeben, ob der Linuxkernel enthalten ist.\n\n\n"}
{"id": "2943521", "url": "https://de.wikipedia.org/wiki?curid=2943521", "title": "OLPC XO-1", "text": "OLPC XO-1\n\nDer XO-1 der Initiative „One Laptop Per Child“, deutsch „Ein Laptop pro Kind“, (kurz \"OLPC\") ist ein robuster und speziell Kinderbedürfnissen angepasster Laptop, der für den Einsatz im Schulunterricht, insbesondere in Entwicklungs- und Schwellenländern, vorgesehen ist. Weitere Bezeichnungen für den XO-1 sind 100-Dollar-Laptop, Children’s Machine beziehungsweise OLPC.\n\nDer Leitgedanke ist, den Computer zu einer freien Wissensdatenbank und zu einem kindgerechten und vielseitigen Lernwerkzeug für die Schule umzugestalten und zusätzlich den Zugang zu modernem Wissen über digitalisierte, vielfältige Medien aller Art zu ermöglichen. Die Verwendung von freier Software wird angestrebt. Das Projekt bezeichnet sich als \"Ausbildungsprojekt\", nicht als \"Laptopprojekt\". Gründer und Vorsitzender der Initiative ist der MIT-Professor Nicholas Negroponte.\n\nDer Schülerlaptop soll die Grundlage für sogenanntes E-Learning im weiten Sinn sein. Der Computereinsatz im Rahmen von E-Learning erfordert eine Umgestaltung des Laptopdesigns, weg von der bisherigen Konzeption als Bürogerät für Erwachsene hin zur Gestaltung als Lernwerkzeug. Dazu wurden sowohl die Hardware als auch die Software für die Anforderungen und Bedürfnisse von Schülern maßgeschneidert: Es wurde eine neue grafische Benutzeroberfläche namens Sugar konzipiert.\n\nDer Laptop soll sowohl neues Kommunikationsmedium als auch integriertes Medium für den regulären Unterricht sein (siehe digitale Schulbank). Er kann zum Lesen eines Buches (als E-Book) oder als modernes Kommunikationsmittel (netzbasiertes Videogespräch, Telefongespräch, Chat) verwendet werden. Der Laptop ermöglicht die spontane Bildung von Lernteams (kollaboratives Lernen), indem sich die Computer auf Anforderung seiner Nutzer selbständig miteinander vernetzen (sogenanntes Mesh-Netzwerk). Die mitgelieferten Anwendungen (das Softwarepaket) sind für Gruppenarbeit über das Netz (LAN und Internet) optimiert. Dieses aktive, situierte Lernen kann eine neue Qualität des Wissenserwerbs ermöglichen.\n\nDer Lerncomputer soll Plattform unterschiedlicher Lernkonzepte und Unterrichtsmethoden sein, wobei im Rahmen des Projektes die Konstruktivistische Didaktik im Vordergrund steht. Nach dem Konzept der Konstruktivistischen Didaktik nach Seymour Papert müsse der Lehrer Schülern das Konzept selbständigen Lernens beibringen. Alternativ dazu soll er in seinem Unterricht weiterhin auf Konzepte klassischer Unterrichtsmethoden bis hin zum Frontalunterricht zurückgreifen können.\n\nTräger des Projekts ist die Non-Profit-Organisation „One Laptop per Child“ unter Vorsitz des MIT-Professors Nicholas Negroponte. Sie wurde nach Abschluss eines Forschungsprojekts am MIT Media Lab gegründet. Das Projekt versteht sich als Bildungsprojekt für die Allgemeinheit. Alle interessierten Menschen sind eingeladen sich an dem laufenden Projekt zu beteiligen. Als Open-Source-Projekt stellt OLPC die seit Projektstart im Jahr 2005 entwickelte Software der Allgemeinheit als freie Software uneingeschränkt und kostenlos zur Verfügung. Damit steht es jedem frei, die Software weiterzuentwickeln und an spezifische Bedürfnisse anzupassen.\n\nDie Zielgruppe des Geräts sind Schüler aus Entwicklungs-, Schwellen- und Industrieländern. Als gemeinnützige Gesellschaft ist OLPC nicht auf Gewinnmaximierung ausgerichtet. Die durch eine hohe Stückzahl erreichte Verringerung der Produktionskosten (sogenannter Skaleneffekt) wird unmittelbar an die Abnehmer weitergereicht. Zwar werden Entwicklungs- und Schwellenländer beim Start der Großproduktion besonders berücksichtigt, Industrieländer sind aber auch nicht ausgeschlossen.\n\nNeben der Verbesserung der Schulausbildung zielt das Projekt darauf, die wachsende digitale Kluft der Industrieländer gegenüber den Entwicklungs- und Schwellenländern langfristig zu schließen. Durch seinen geringen Preis soll der Laptop möglichst allen Bevölkerungsschichten den Zugang in das Internet und damit zu modernem Wissen erlauben. Wissen soll für alle zugänglich sein und so Bildung ermöglichen. Daher unterstützen die Vereinten Nationen das Projekt. Bereits im September 2000 hatten sie in ihren Millennium-Entwicklungszielen als zweites Ziel die Bereitstellung einer primären Schulausbildung für alle bis zum Jahr 2015 durch die Weltgemeinschaft beschlossen. In der Abschlusserklärung bestätigten und konkretisierten 174 Staaten dieses zweite Millennium-Ziel auf dem zweiten Weltgipfel zur Informationsgesellschaft im November 2005, in Tunis, Tunesien. „Wir bestätigen, dass es unser Wunsch und unsere Verpflichtung ist, eine am Menschen orientierte, nicht ausschließende und entwicklungsorientierte Informationsgesellschaft zu schaffen, […], so dass Menschen an jedem Ort zu Informationen und Wissen Zugang haben, es benutzen, gebrauchen, schaffen und teilen können, damit jeder seine Möglichkeiten ausschöpft; und um die international vereinbarten Entwicklungsziele, d. h. auch die Millenniumziele, zu erreichen.“ Seitens der 174 WSIS-Teilnehmer wird vom Zugang zu modernen Kommunikationstechniken erwartet, dass sich die sozialen und wirtschaftlichen Entwicklungschancen verbessern und sich damit auf lange Sicht positive Impulse für die Entwicklungshilfe ergeben.\n\nDen Angaben des offiziellen OLPC-Wikis folgend verwendet der Laptop den Prozessor \"AMD Geode LX-700@0,8W\" mit 433 MHz Taktfrequenz, mit zusätzlichem L1- und L2-Cache mit einer Größe von insgesamt 256 kB. Der Geodeprozessor basiert auf der X86-Architektur. Die Zahl 700 gibt an, dass die Rechenleistung zumindest der eines Intel Pentium III (Celeron) mit einer Taktfrequenz von 700 MHz entspricht. Laut Nicholas Negroponte wird ein Wechsel von der x86- zur ARM-Architektur angestrebt.\n\nDie Größe des Hauptspeichers beträgt 256 MB. Anstatt einer vibrationsempfindlichen Festplatte wird ein stoßfester Flash-Speicher mit 1024 MB eingebaut. Größere Datenmengen (derzeit 3 bis 5 GB pro Laptop) sollen auf einem \"100-Dollar-Server\" gespeichert werden. Der Zugriff erfolgt dabei über integriertes WLAN. Der Laptop hat als externe Anschlüsse drei USB-Anschlüsse, ein integriertes Kartenlesegerät für SD-Karten sowie Audioein- und -ausgänge.\n\nDas Display ist 7,5 Zoll groß. Die Bildschirmauflösung beträgt maximal 1200×900 Pixel. Das Display hat eine spezielle Technik, die es ermöglicht, sowohl im Sonnenlicht als auch bei Dunkelheit zu arbeiten. Bei Sonnenlicht wird das Licht hinter dem LCD-Gitter reflektiert. Somit ist zumindest ein monochromes Bild sichtbar („reflective mode“). Bei Nutzung der Hintergrundbeleuchtung („color mode“) ist die Anzeige mehrfarbig. Die Pixeldichte beträgt 200 dpi (vergleiche gedruckte Zeitschrift: 300 dpi).\n\nWeiterhin verfügt der Laptop über eine Videokamera mit einer Auflösung von 640×480 Pixel, ein Mikrofon und zwei Lautsprecher.\n\nÜber den Signaleingang ist der Laptop in der Lage, Messwerte von analogen Sensoren aufzuzeichnen und automatisch in einem Programm zu verarbeiten. Denkbar wären etwa die Messwerte von einem Thermometer, pH-Messgeräte, Oszilloskop oder einem Mikroskop. Der Schülerlaptop hat ein Touchpad mit einer Gesamtlänge von ca. 15 cm, wobei das Touchpad die Eingabe mit einem Eingabestift verarbeiten kann. Somit ist auch das Schreiben von digitalen Briefen möglich. Die Tastatur und das Touchpad können mittels zweier Leuchtdioden beleuchtet werden. Damit ist die Computernutzung auch nachts bzw. bei schlechten Lichtverhältnissen möglich.\n\nDer Laptop hat einen eingebauten Router für ein lokales Funknetzwerk nach dem 802.11s-Standard. Die maximale Übertragungsgeschwindigkeit beträgt 2 Mbit/s. Die Funkreichweite pro Laptop beträgt unter optimalen Bedingungen etwa 2 Kilometer. Zudem kann der XO-Laptop mit anderen Laptops bei der Datenweiterleitung kooperieren. Datenpakete können über weitere 20 Laptops zum Empfänger weitergeleitet werden. Dadurch ergibt sich eine theoretische Reichweite des Funknetzes von 30 bis 40 Kilometer.\n\nJeder Laptop kann ohne eine Vermittlungsstelle, d. h. ohne einen Server, zu jedem anderen Laptop automatisch eine Verbindung aufbauen. Ein Serverausfall kann also durch den Laptop automatisch kompensiert werden. Soweit ein Laptop bzw. ein Server Zugang in das globale Internet hat, stellt er dies auch für alle Teilnehmer im Funknetz bereit, bei Bedarf auch über die eingebauten USB-Anschlüsse in Verbindung mit einer Ethernet Netzwerkkarte. In entlegenen Gebieten kann der Server mittels einer Satellitenschüssel einen Internetzugang bereitstellen.\n\nDer XO-Laptop wurde für den täglichen Einsatz in der Schule konzipiert und ist daher besonders robust konstruiert. Das mobile Computersystem soll zumindest für eine Dauer von fünf Jahren ohne Funktionsausfälle einsetzbar sein (Standardlaptop: Zwei Jahre). Die Tastatur ist wasserdicht, der gesamte Laptop ist beim Transport zum Schutz vor Regen abgedichtet. Je nachdem, welche Anwendung und welcher Akkutyp benutzt wird, beträgt die durchschnittliche Laufzeit des Akkus drei bis sechs Stunden. In der Zukunft sollen mit einer verbesserten Software Akkulaufzeiten von zehn Stunden und mehr erreicht werden.\nEbenso wurde der Schülerlaptop auf eine hohe Energieeffizienz ausgerichtet. Solange der Laptop aktiv benutzt wird, benötigt er lediglich ca. 2,5 Watt. Wenn am Laptop keine Eingaben erfolgen, arbeitet er weiterhin als Netzwerkrouter und nimmt dabei nur noch ca. 0,3 Watt auf (vergleiche Standardlaptops: ca. 20 bis 40 Watt; Desktopcomputer: mindestens 70 Watt). Daher sind zur externen Energieversorgung auch schwache Stromquellen wie etwa Solarzellen oder ein handbetriebener Dynamo (sogenannte \"human power\") ausreichend.\n\nDer Laptop hat die Maße 24,2 cm × 22,8 cm × 3,0 cm. Das Gerät ist in die Klasse der besonders mobilen Subnotebooks einzuordnen. Durch seinen umklappbaren Bildschirm kann das Gerät ähnlich wie ein Tablet-PC verwendet werden. Sein Gewicht beträgt – inklusive Akku – etwa 1,4 Kilogramm. Das Design des Geräts stammt vom renommierten Schweizer Industriedesigner Yves Béhar.\n\nErste Überlegungen bezüglich eines Wissenstransfers in Entwicklungs- und Schwellenländer gingen bereits in den 1970er Jahren von MIT-Professor Seymour Papert aus. In einem Forschungsprojekt brachte er Computertechnologie in ein afrikanisches Dorf. Er beobachtete, inwieweit die Kinder, die vorher keinen Kontakt damit hatten, innerhalb kürzester Zeit lernten, den Computer anzuwenden und sich so neues Wissen anzueignen.\n\nBei weiteren Überlegungen kam am MIT die Idee auf, einen preiswerten Laptop speziell für Entwicklungsländer zu konzipieren. Daraus entstand später das Projekt \"100-Dollar-Laptop\". Am MIT Media Lab, Fakultät der Universität MIT in Cambridge (Massachusetts), entwickelt ein Forschungsprojekt es weiter.\n\nWeitere Tests fanden im Jahr 2001 zusammen mit einer Dorfschule in Kambodscha statt. Jedem kambodschanischen Kind der Projektschule wurde ein damals moderner Laptop für den Schulunterricht zur Verfügung gestellt. Man testete, welche besonderen Anforderungen an ein solches Gerät im Rahmen des Schulunterrichts, insbesondere in einem infrastrukturell schwachen Gebiet gestellt werden, und zog daraus Schlussfolgerungen für das Design eines solchen Laptops.\n\nAls sich abzeichnete, dass das Projekt den Rahmen eines reinen Forschungsprojektes sprengen würde, wurde zu dessen Umsetzung in die Praxis die Non-Profit-Organisation \"One Laptop per Child\" (kurz: OLPC) gegründet und von der Universität organisatorisch ausgegliedert (sogenannter Spin-off).\n\nAls Non-Profit-Organisation ist OLPC nicht auf Gewinnmaximierung ausgerichtet. Vorstandsmitglieder sind unter anderem Nicholas Negroponte (Vorsitzender), Antonio Battro (Chief Education Officer) und Walter Bender (Software and Content). Gemäß dem Kurznamen \"OLPC\" ist es das erwünschte Ziel, jedem Kind für Ausbildung und Unterricht einen Laptop zur Verfügung stellen zu können. Somit steht der Begriff \"100-Dollar\" für den anvisierten Produktionspreis bei großen Stückzahlen. Dieser Preis wurde für Ende 2008 bzw. Anfang 2009 angezielt.\n\nDer Projektstart erfolgte im Januar 2005 auf dem Weltwirtschaftsforum in Davos (Schweiz), als Nicholas Negroponte das Konzept zur Entwicklung eines \"Hundred-Dollar-Laptop-Project\" (HDLP) bekannt gab. Damals konnte als erster Partner AMD gewonnen werden. Noch im selben Monat folgten News Corporation und Google. Zu diesem Zeitpunkt wurden jedoch noch weitere Partner für dieses Projekt gesucht.\n\nAm 16. November 2005 stellte Nicholas Negroponte gemeinsam mit UN-Generalsekretär Kofi Annan auf dem zweiten Weltgipfel zur Informationsgesellschaft (WSIS) in Tunis (Tunesien) erstmals öffentlich funktionsfähige Prototypen des 100-Dollar-Laptops vor. Im Rahmen einer Pressekonferenz äußerte sich Kofi Annan wie folgt: „Er [der Laptop] ist eine eindrucksvolle technische Errungenschaft, in der Lage, fast alles, was größere, teurere Computer können, zu tun. In ihm liegt das Versprechen große Fortschritte in wirtschaftlicher und sozialer Entwicklung zu bewirken. Aber vielleicht am wichtigsten ist die eigentliche Bedeutung von „one laptop per child“. Es geht nicht einfach darum, jedem Kind einen Laptop zu geben, so als würde man ihm ein Zaubermittel überreichen. Die Magie liegt im Inneren – im Inneren jedes Kindes, jedes werdenden Wissenschaftlers, Gelehrten oder einfachen Bürgers. Diese Initiative beabsichtigt sie ans Tageslicht zu bringen.“\n\nSeitdem erhält das Projekt von weiteren Partnern aus der Industrie wie etwa Marvell, Brightstar, Nortel, SES-Astra und Red Hat Unterstützung. In einer Pressemitteilung vom Dezember 2005 gab „One Laptop per Child“ bekannt, dass als Laptop-Hersteller das Unternehmen Quanta mit Sitz in Taiwan gewonnen werden konnte. Auf dem Weltwirtschaftsforum im Januar 2006 wurde schließlich die Zusammenarbeit von „One Laptop per Child“ mit dem Entwicklungsprogramm der Vereinten Nationen bekanntgegeben.\n\nIm April 2006 begann der „Alpha-Test“ mit der Hauptplatine für die Großproduktion und seinen integrierten Komponenten. Im Juni 2006 wurde die konzipierte Hauptplatine (ca. 500 Stück) an alle beteiligten Entwickler für weitere Tests übergeben.\n\nIm Sommer 2006 wurde die Beta-Testphase gestartet. Dieser begann mit Beta-Test 1 im November 2006. Es wurden 875 Laptops mit allen relevanten Komponenten an die Entwickler sowie für praktische Tests in der Schulklasse (für weitere Belastungs-Tests) versendet. Nach Abschluss des Beta-Tests-1 wurden die verwendeten Laptops zusätzlichen mechanischen Belastungstest unterzogen, um weitere Erkenntnisse über die Stabilität und maximale Belastbarkeit von Konstruktion und Design zu erhalten. Mitte Februar 2007 begann der Beta-Test 2 zur weiteren Optimierung des Systems. Etwa 2500 Beta-2-Laptops wurden wieder an Entwickler und zum testweisen Praxiseinsatz in der Schulklasse versandt.\n\nParallel dazu wurden verschiedene Tastatur-Layouts für Sprachen, die nicht so weit verbreitet sind, wie Kinyarwanda, Amharisch, Urdu, Nepali, Kasachisch, Mongolisch, Devanagari, Paschtu oder Darī entwickelt. Bislang war für einige dieser Sprachen keine Computertastatur verfügbar, was jedoch für ein Bildungsprojekt, das sich auf die Grundschulausbildung ausrichtet, wesentliche Voraussetzung ist. Die Entwicklung von Tastaturen für bislang vernachlässigte Sprachen wird seitens der Gesellschaft OLPC als ein Beitrag zur Überwindung der digitalen Kluft angesehen. Daneben wurden auch verfügbare Tastatur-Layouts weit verbreiteter Sprachen wie Libysch, Türkisch oder Englisch angepasst, um den Bedürfnissen der Zielgruppe des XO-Laptops besser zu entsprechen.\n\nIm Mai 2007 wurde ein Beta-Test 3 produziert (Stückzahl: ca. 100 Stück), Ende Juni 2007 wurde der Prototyp der 4. Generation produziert (Stückzahl: ca. 2.000 Stück). Ende Juli 2007 wurde eine kleine Auflage an „pre-production test systems“ hergestellt (ca. 300 Stück), der sogenannte CTest-1, gefolgt von einem CTest-2. Diese Prototypen waren weitgehend identisch zum Endprodukt der Großproduktion.\n\nDie Massenproduktion startete schließlich im November 2007. Laptops werden seit dem Dezember 2007 ausgeliefert. Bis Mitte 2010 wurden etwa 2 Millionen Laptops produziert, die hauptsächlich in Uruguay, Peru, Mexiko, Ruanda, Haiti und in den USA eingesetzt werden.\n\nParallel zur Weiterentwicklung der Prototypen zur Serienreife wurden in Brasilien, Nigeria, Thailand, Uruguay und in Peru Prototypen der ersten, zweiten, dritten bzw. vierten Laptop-Generation in der Schulklasse eingesetzt. Aus den Rückmeldungen konnten dann weitere Wünsche bei der Entwicklung berücksichtigt und die Konstruktion der Schüler-Laptops für die Großproduktion verfeinert werden. Parallel dazu konnte auch in den Abnehmerländern vor Ort festgestellt werden, in welchen Umfang der Einsatz des Laptops das Interesse der Schüler am Unterricht und damit im Ergebnis das Lernniveau erhöhte.\n\nNach einem Zwischenbericht aus der OLPC-Projektschule Galadima in Nigeria gingen die Leistungen der Schüler über das bisherige Niveau hinaus. Laut Aussage eines Lehrers der Projektschule konnte das Konzept des Projekts \"One Laptop per Child\" bestätigt werden:\n“Pupils go even beyond what I can teach in the class. It's a very interesting thing to use. I personally have a better idea about teaching... We discovered that giving them time to discover something and to do it in their own way, they feel more happy and they are so excited in using it.” (deutsch: „Die Schüler gehen sogar über das hinaus, was ich ihnen in der Klasse beibringen kann. Der Einsatz [der Laptops] ist sehr interessant. Ich persönlich habe nun auch eine bessere Vorstellung, wie ich lehre … Wir haben Folgendes herausgefunden: Indem wir den Schülern mehr Zeit geben, etwas selbst zu entdecken, und sie dies auf ihre Weise machen können, sind sie zufriedener und sehr motiviert ihn [den Laptop] einzusetzen.”) \n\nNach der ursprünglichen Planung war als Voraussetzung für den Produktionsstart die vorherige Bezahlung der Laptops durch die Teilnehmerländer vorgesehen. Mittlerweile hat das Projekt OLPC sich für eine andere Art des Vertriebes entschieden.\n\nAufgrund des bisherigen Erfolges im Rahmen der Spendenaktion \"Give 1 Get 1\" werden die Schulen in den verschiedenen Teilnehmerländern erst nach und nach mit XO-Laptops versorgt. Dieses Vorgehen kommt auch der bisherigen Organisationsstruktur des OLPC-Projekts entgegen. Indem die logistischen Kapazitäten für mehrere tausend XO-Laptops auf mehrere hunderttausend Stück in den nächsten Monaten erhöht werden, kann die Organisation die dazu erforderlichen Personalressourcen anpassen und damit auch direkt vor Ort Unterstützung anbieten.\n\nAuch das gelegentlich kritisierte Vorgehen, dass lediglich Staaten die XO-Laptops ordern können, ist so nicht mehr gegeben. Bereits im Rahmen der Aktion \"Give 1 Get 1\" hatten die Endverbraucher in Nordamerika bis zum Jahresende 2007 die Möglichkeit, einen Laptop für 400 US-Dollar zu erwerben und gleichzeitig eine Spende für die am Projekt teilnehmenden Entwicklungs- und Schwellenländer zu tätigen. Ab Ende 2008 war dies auch in Europa möglich. Aufgrund dieser Spenden war der Start neuer Projekte in verschiedenen Ländern möglich.\n\nDer XO-Laptop wird durch den Auftragshersteller Quanta Computer Inc. mit Sitz in Taiwan hergestellt. Quanta Inc. ist Auftragshersteller, u. a. auch für Apple-Computer, und fertigt ca. ein Drittel aller weltweit verkauften Notebooks. Nach derzeitiger Planung ist eine Gesamtproduktion von mehreren Millionen Stück über einen Zeitraum von ca. fünf Jahren geplant.\n\nIm April 2009 kündigte OLPC eine neue, verbesserte Variante des XO-1 an, die als XO-1.5 bezeichnet wurde. Im Vergleich zum Ursprungsmodell haben die neuen Laptops leistungsstärkere Prozessoren und mehr Speicher. Diese Modellreihe wurde ab 2010 mit veränderter Software auch als XO-HS (\"Highschool\") speziell für Schüler weiterführender Schulen angeboten. Ab September des Jahres werden 90.000 Laptops dieser Variante nach Uruguay geliefert.\n\nOLPC hatte eine neue Version des Laptops unter der Produktbezeichnung XO-2 geplant, die für 75 Dollar angeboten werden sollte. Die Entwicklung wurde dann allerdings eingestellt. Als Ersatz wurde der XO Tablet-PC (Bezeichnung: XO-3) entwickelt, der alternativ mit einem 7- oder 10-Zoll-Bildschirm erhältlich ist. Was den Laptop angeht, so gibt es inzwischen – nach XO-1, XO-1.5 und XO-1.75 – als neueste Version den XO-4, der mit einem sehr stromsparenden ARM-Prozessor ausgestattet ist. Den XO-4 gibt es auch mit Touchscreen (Bezeichnung: XO-4 Touch). Der XO-4 wird mit dem Linux-Betriebssystem Fedora 18 und den Benutzungsoberflächen Sugar und Gnome ausgeliefert. Der Preis für das Modell ohne Touchscreen sollte laut Ankündigung vom August 2013 bei 206 US-Dollar liegen.\n\nEinen Laptop mit diesen sehr anspruchsvollen Leistungsmerkmalen zu diesem Preis zu entwickeln, war von Anfang an für die Entwickler vom MIT Media Lab eine große Herausforderung.\n\n„Es gibt zwei Wege um irgendwas kostengünstig zu machen. Der eine Weg ist, billige Komponenten, billige Arbeit sowie billiges Design zu nehmen, um ein ‚billiges‘ Produkt zu machen. Der andere Weg ist, fortgeschrittene Produktionsprozesse, einen hohen Integrationsgrad, sehr große Stückzahlen sowie gutes Design einzusetzen, um ein kostengünstiges, qualitativ hochwertiges Gerät zu bekommen. Wir haben uns ausschließlich auf Letzteres konzentriert […].“\n\nDemnach erfolgt der Vertrieb nur über „Großabnehmer“, d. h. über am Projekt teilnehmende Länder, welche die Laptops in großen Stückzahlen abnehmen. Die Weiterleitung an die Schüler geschieht dabei durch die Schule. Die Bestellungen werden nicht sofort in Gesamthöhe ausgeführt, sondern über mehrere Monate oder wenige Jahre verteilt, um vorhandene Transportkapazitäten in die Abnehmerländer mitzunutzen. Ziel ist es auch, die Transportkosten gering zu halten.\n\nIm September 2007 teilte das Projekt OLPC mit, dass der Laptop beim Start der Großproduktion zu einem Preis von ca. 188 US-Dollar angeboten wird, was umgerechnet etwa 135 Euro entspricht. Es wird jedoch weiterhin an dem Ziel festgehalten, den Preis kontinuierlich zu senken.\n\nOLPC plante Anfang 2007, dass bis Ende 2008 der Preis bei größeren Stückzahlen circa 100 Dollar und im Jahr 2010 nur noch 50 Dollar beträgt. Die Organisation führte an, dass Faktoren wie die Rohstoffpreise für Kupfer und Nickel sowie der Kurs des US-Dollars zu Preisschwankungen führen könnten. Der Preis sollte aufgrund höher integrierter Hardwarekomponenten und größeren Liefermengen sinken. Diese Hoffnung erfüllte sich nicht, und der Preis blieb bei 190 Dollar (Stand Ende 2009). OLPC gibt die für die Total Cost of Ownership (TCO), also die Fixkosten, die Wartung und den Internetzugang insgesamt 1 Dollar pro Woche an (Stand Ende 2009).\n\nSeit Ende September 2007 ist es auch für Privatpersonen möglich, dem Projekt Einzelspenden auf der Homepage XOgiving.org zukommen zu lassen. Möglich ist die Spende eines oder mehrerer XO-Laptops in Form einer Geldspende über das Onlinesystem PayPal.\n\nZudem fand vom 12. November bis 31. Dezember 2007 erstmals eine Kampagne namens \"Give 1 Get 1\" (G1G1) in den USA und Kanada statt. Bei dieser Aktion bekam ein bedürftiges Kind einen XO-Laptop, einen zweiten Laptop erhielt der Besteller Mitte Dezember, also kurz vor Weihnachten, geliefert. Im Rahmen dieser Aktion wurden ca. 300.000 XO-Laptops bestellt, wovon allein bei dieser Aktion 150.000 XO-Laptops den teilnehmenden Projektländern als Spende zukamen.\n\nAm 17. November 2008 wurde eine erneute \"Give One Get One\" Aktion gestartet, die es auch Privatpersonen aus allen Ländern der EU, sowie der Schweiz, Russland und der Türkei erlaubte, je einen XO-Laptop zu erwerben und zu verschenken. Der Vertrieb wurde ausschließlich durch Amazon UK abgewickelt. OLPC Deutschland umwarb dieses Projekt auch unter dem Arbeitstitel \"Dir1Mir1\". Die Aktion ist jedoch im Dezember 2008 abgelaufen.\n\nWährend die Teilnehmer der \"Give One Get One\" Aktion keinen Einfluss darauf haben, in welchem Land der gespendete Laptop zum Einsatz kommt, bietet die OLPC-Stiftung bei Spenden von mehr als 100 Laptops (etwa durch Unternehmen) dem Spender die Möglichkeit, den Einsatzort selbst zu wählen.\nAuf diese Weise könnten auch Kinder in deutschen Bildungsprojekten an einen XO-Laptop kommen. Laut OLPC Deutschland wäre es bei einer Spende dieser Größenordnung auch möglich, den Laptop, der im Rahmen der \"Give One Get One\" Aktion nur mit QWERTY-Tastatur erhältlich ist, mit einem deutschen Tastatur-Layout zu versehen.\n\nEs wird erwogen, einige Zeit nach Start der Großproduktion den Laptop eventuell auch frei verkäuflich für den ausschließlich privaten Gebrauch, also nicht zum Einsatz in der Schule, anzubieten. In diesem Fall würde ein anderer Hersteller die Produktionsrechte erwerben und zudem den XO-Laptop mit zusätzlicher Hardware ausrüsten, wie etwa eine eingebaute Netzwerk-Karte oder mehr Hauptspeicher. Allerdings würde dann ein höherer, evtl. der dreifache Preis anfallen. Überschüsse aus diesem kommerziellen Verkauf sollen dann zur Unterstützung der Entwicklungsländer verwendet werden. Es ist noch offen, inwieweit diese Idee umgesetzt wird. Im Januar 2008 wurde bekanntgegeben, dass die eigens gegründete Gesellschaft \"OLPC America\" die Geräte auch in den USA vertreiben soll. Preise wurden noch keine genannt, die Auslieferung soll jedoch in Zusammenarbeit mit den Regierungen der einzelnen Bundesstaaten noch im Laufe des Jahres erfolgen.\n\nFür eine nachhaltige Entwicklung und einen Zugang für die gesamte Bevölkerung eines Landes ist es erforderlich, auch eine nachhaltige Betreuung bereitzustellen. Daher will das Projekt One Laptop per Child neben der erfolgreichen Übergabe der XO-Laptops an die Abnehmerländer auch die Anbindung an das Internet über spezielle Server, die 100-Dollar-Server, ermöglichen.\n\nDiese 100-Dollar-Server sollen dabei in verschiedenen Konfigurationen bereitgestellt werden. Es werden derzeit drei Varianten seitens des Projekts geplant. Einen erweiterten XO-Laptop, der als Server für eine Kleingruppe dient, einen sogenannten XS-Server und schließlich das Servermodell XSX. Jedes Modell ist auf die Bereitstellung des Internetzugangs für eine bestimmte Gruppengröße ausgelegt und soll an die anspruchsvollen klimatischen Anforderungen insbesondere von Entwicklungs- und Schwellenländern angepasst sein.\n\nFür eine nachhaltige Entwicklung ist es notwendig, dynamische Lernprozesse in den jeweiligen Teilnehmerländern und vor Ort zu fördern. Durch die Benutzung des Laptops und der bewusst offen gestalteten Architektur und Software der Computer sollen die Benutzer motiviert werden, sich Wissen über die Informationstechnologie anzueignen. Beste Voraussetzungen sind bei Verwendung von freier Software gegeben, welche jedem Computerbesitzer die Nutzung und das Recht zur Anpassung des Computersystems erlaubt.\n\nGezielt sollen dabei lokale Initiativen von Bürgern, den Kommunen und Initiativen des Staates gefördert werden. Bereits vor der Auslieferung der Laptops an die teilnehmenden Länder erfolgt die Bildung von Lernteams, um den Gebrauch der XO-Laptops zu fördern. Eine Unterstützung vor Ort erfolgt dabei langfristig durch die \"Stiftung One Laptop per Child\". Diese ortsansässigen Initiativen sollen dabei helfen, digitale Bildung nachhaltig zu gestalten und zu vertiefen. Daneben ist es Ziel der Stiftung, nach Projektstart für besonders benachteiligte Kinder in Entwicklungsländern einen Zuschuss für XO-Laptops zu ermöglichen, wobei der Zuschuss von externen Spenden abhängig sein wird. Insbesondere Flüchtlingskinder, Kinder in besonders schwer erreichbaren Regionen der Erde und Kinder, die nicht durch das Abnehmerland berücksichtigt wurden, sollen auf diese Weise durch die Stiftung unterstützt werden.\n\nDer Laptop ist nicht nur für Kinder aus Entwicklungsländern konzipiert, sondern für das Lernen eines jeden Kindes – explizit auch in den Industrienationen. In Graz wurde im Herbst 2008 eine Volksschulklasse mit 25 XOs ausgestattet, wobei unter wissenschaftlicher Evaluation vier Jahre lang Erfahrungen mit dem Lernen am Laptop in einem westlichen Industrieland gesammelt werden sollen. Nachdem es lange mit „OLPC Austria“ nur eine österreichische Beteiligung an dem Projekt gab, gründete sich im April 2008 der gemeinnützige Verein „OLPC Deutschland e. V.“ Eine deutschsprachige Version für die im Projekt verwendete Software ist zwar gerade in der Entwicklung, jedoch ist die Übersetzung noch nicht abgeschlossen.\n\nDer Deutsche IT-Verband BITKOM fordert gegenüber der deutschen Politik eine Beteiligung an einem Laptop-Projekt. Demnach soll jeder Schüler der fünften Klasse ein Laptop besitzen, ohne dass der Branchenverband mit dieser Forderung ein notwendiges, pädagogisches Gesamtkonzept verbindet.\n\nBundeskanzlerin Angela Merkel hat dazu auf dem Technologiegipfel im Dezember 2006 in Potsdam keine Stellung genommen, obwohl es bei diesem Gipfel um eine Thematisierung solcher Fragen ging. Allerdings fallen Fragen der Schulausbildung, und damit auch die Frage des eingesetzten Lehrmaterials in die Kompetenz der Bundesländer. Ob und inwieweit moderne Informationstechnologie im Schulunterricht angewendet wird, entscheiden die jeweiligen Regierungen der deutschen Bundesländer.\n\nDer XO-1 ist für die flexible Verwendung auch außerhalb des Klassenzimmers konzipiert.\n\nDer Bildschirm hat eine Diagonale von 7,5 Zoll (etwa 19 cm). Aufgrund seines besonderen Aufbaus, insbesondere der Anordnung der verschiedenen Schichten (hier des Farbfilters, der teildurchlässigen Reflexionsschicht und des eigentlichen LCDs), erhält der Bildschirm besondere Eigenschaften. Er arbeitet dadurch transflektiv und kann daher unter verschiedenen Lichtverhältnissen genutzt werden. Der Bildschirm kann Farben dabei nur transmissiv darstellen, während er reflektiv nur Schwarz-Weiß-Darstellung beherrscht. Die Schwarz-Weiß-Auflösung beträgt 1200 × 900 Pixel. Rein rechnerisch beträgt die Auflösung bei Farbanzeige 692 × 520 Pixel. (Das entspricht einer Gesamtfarbpixelzahl von einem Drittel der Schwarzweißpixelzahl.) Aufgrund der ungewöhnlichen Anordnung der Subpixel ist es nicht einfach, die \"wahrgenommene\" Auflösung mit der anderer Displays zu vergleichen. Tests der \"wahrgenommenen Auflösung\" lassen sie im Bereich der XGA-Auflösung erscheinen (entspricht 1024×768 Pixeln).\nIm Farbbild-Modus wird der Bildschirm, wie bei Flachbildschirmen üblich, hintergrundbeleuchtet. Allerdings wird bei herkömmlichen Bildschirmen normalerweise das Bild umso blasser, je heller das einfallende Umgebungslicht ist. Im Extremfall ist bei sehr starkem Lichteinfall die Erkennbarkeit der Anzeige auf dem Bildschirm nur noch minimal. Deshalb wird der Bildschirm des OLPC XO-1 im Schwarz-Weiß-Modus nicht hintergrundbeleuchtet, sondern reflektiert das Umgebungslicht. Durch diese Reflexion erhöht sich der Kontrast der Anzeige. Der Bildschirm ist damit auch bei direktem Einfall von Sonnenlicht einsetzbar und die Anzeige sogar noch besser ablesbar.\n\nZwischen üblicher Grafikhardware des Chipsatzes und dem Bildschirm liegt ein spezieller Grafikprozessor (Display Controller – DCON). Er beinhaltet einen Framebuffer, so dass der Chipsatz komplett abgeschaltet werden kann und trotzdem ein Standbild auf dem Bildschirm aufrechterhalten werden kann. Bei aktivem Chipsatz nimmt er Anpassungen des Signals vor, um den speziellen Eigenarten des Bildschirms Rechnung zu tragen: Bei abgeschalteter Hintergrundbeleuchtung ist er im Schwarzweiß-Modus, wobei aus dem Farbbildsignal ein tonwertrichtiges Schwarzweiß-Signal erzeugt wird. Mit der Hintergrundbeleuchtung wird der DCON in den Farbmodus umgeschaltet, wobei das Bildsignal einer Antialias-Filterung unterzogen wird, um der geringeren Pixeldichte einer jeden der drei Grundfarben Rechnung zu tragen.\n\nAls Innovation ist die Kombination von Hintergrundbeleuchtung und Reflexion in der Anzeige anzusehen. Soweit bei dem verwendeten Display bei eingeschaltetem Farbbild-Modus Sonnenlicht auf den Bildschirm fällt, werden zwar die Farben blasser, weil dann zunehmend – durch die Reflexion des Sonnenlichts – der Anteil des reflektiven Modus überwiegt. Allerdings erhöht sich durch die Reflexion auch der Kontrast der (nun Schwarz-Weiß-)Anzeige. Zudem nähert sich die wahrgenommene Auflösung der physischen Auflösung von 1200×900 Pixeln an, was die Lesbarkeit des Bildschirms erhöht. Bei vollem Reflexionsmodus beträgt die Auflösung 200 dpi (vergleiche Auflösung eines in den 1990er Jahren üblichen einfachen Laserdruckers: 300 dpi).\n\nComputerbasierter Unterricht ist damit auch im Freien möglich, was insbesondere in Entwicklungsländern der Regelfall ist. Zudem wird im Schwarz-Weiß-Modus der Energieverbrauch des gesamten Systems erheblich gesenkt. Der Bildschirm verbraucht dann 0,1 Watt, während ein Standardbildschirm durchschnittlich 7,0 Watt verbraucht.\n\nDas Betriebssystem belegt ca. 140 Megabyte auf dem Flash-Speicher, womit noch ca. 860 Megabyte Speicherplatz für Anwendungen und Daten verfügbar sind. Soweit der Bedarf an einer Erweiterung des Speicherplatzes besteht, können sowohl über den eingebauten SD-Karten-Steckplatz als auch über die drei USB-Anschlüsse weitere Speichermedien angeschlossen werden. Bereits über den SD-Karten-Steckplatz ist eine zusätzliche Speichererweiterung um zumindest acht Gigabyte möglich.\n\nZiel der Entwickler ist es, den Kindern einen robusten und für den Schulalltag tauglichen Computer zur Verfügung zu stellen. Aufgrund seines geringen Stromverbrauchs muss die Abwärme des Prozessors nicht über Entlüftungsschlitze abtransportiert werden. Daher können die relevanten elektronischen Komponenten in einem vollständig umschlossenen, abgedichtetem Gehäuse von äußeren Einflüssen abgekapselt werden. Die externen USB- und Audioanschlüsse werden im geschlossenen Zustand von den integrierten Antennen abgedeckt. Der Laptop ist somit im geschlossenen Zustand unempfindlich gegenüber Regen bzw. Sand oder Insekten. Da die Tastatur des Laptops durch eine Gummimembran abgedichtet wurde, ist der Laptop auch im aufgeklappten Zustand gegen Flüssigkeit und Staub geschützt.\n\nAufgrund seiner geringen Abwärme heizt sich das Computersystem nicht so schnell wie ein Standard-Laptop auf. Daher ist der XO-Laptop selbst bei konstanter Außentemperatur von über 60 Grad Celsius funktionstüchtig, also auch bei Wüstentemperaturen einsetzbar.\n\nUm die Funktionsfähigkeit des Laptops auch bei starken Stößen und Erschütterungen zu gewährleisten, wird statt einer Festplatte ein stoßunempfindlicher Flash-Speicher (Solid-State-Drive) verwendet. Die Hauptplatine ist im Kopfteil hinter dem Bildschirm eingebaut, um die Kabellänge und die Leiterbahnen zu den angeschlossenen Komponenten zu verkürzen und dadurch potentielle Bruchstellen bzw. Wackelkontakte auf ein Minimum zu reduzieren. Zusätzlich wird die Hauptplatine im Gehäuse auf Stoßdämpfer aus Weichgummi gelagert. Der Laptop hat zudem einen breiteren Gehäuserahmen als Standard-Laptops (2-mm-Gehäuse statt üblicherweise 1,3-mm-Gehäuse).\n\nDerzeit (Stand: 2007) wird der XO-Laptop zum Nachweis seiner Unempfindlichkeit gegenüber Regen, Sand und großer Hitze durch die Organisation Underwriters Laboratories zertifiziert.\n\nBei einer möglichen Herstellung von geschätzten 100 bis 150 Millionen XO-Laptops ist es erforderlich, die Auswirkungen auf die Umwelt in die Überlegungen miteinzubeziehen. Derzeit werden weltweit jährlich 230 Millionen Computer ausgemustert. Jedes Jahr werden zum Beispiel 500 Container mit Computerschrott zur Entsorgung nach Nigeria transportiert.\n\nAus diesem Grund arbeitet die gemeinnützige Gesellschaft OLPC mit der Organisation EPEAT zusammen, um Konstruktion und Vertrieb des XO-Laptops möglichst ökologisch zu gestalten. Zur Bewertung wurde das „Electronic Program Environmental Assessment Tool“ nach dem IEEE 1680-Standard eingesetzt. Der XO-Laptop hat das Zertifikat in Gold erhalten. Teilweise geht das Projekt sogar über die Anforderungen der Gold-Auszeichnung hinaus, so dass seitens der Organisation EPEAT bereits über eine nachträgliche Verschärfung der Bewertungskriterien nachgedacht wird. „[Die Gesellschaft] OLPC hat einen neuen Umwelt-Standard mit dem XO[-Laptop] geschaffen.“\n\nDer XO-Laptop ist unter anderem aus folgenden Gründen besonders umweltfreundlich:\n\n\nDer Computer wurde so konstruiert, dass ein Austausch von defekten Komponenten wie etwa der Tastatur oder des Bildschirms in möglichst wenigen Arbeitsschritten erfolgen kann. Dabei wurde darauf geachtet, dass die dazu notwendigen Handgriffe möglichst einfach bleiben und kein spezielles Werkzeug benötigt wird. Ein Test hat ergeben, dass bereits 10-jährige Kinder – ggf. unter fachkundiger Anleitung eines Erwachsenen – in der Lage sind den Komponentenaustausch vorzunehmen.\n\nIn jedem XO-Laptop ist ein Router integriert. Dies ermöglicht den Aufbau eines Funknetzwerkes, ohne dass zusätzliche Hardware benötigt wird. Der Verbindungsaufbau im WLAN zwischen den XO-Laptop erfolgt dabei automatisch. Das WLAN verwendet als Übertragungsprotokolle 802.11b und 802.11g mit Erweiterung gemäß 802.11s.\n\nDurch Verwendung von zwei integrierten WLAN-Antennen (die „Hasenohren“) ergibt sich eine maximale Übertragungsgeschwindigkeit von 2.000 kBit/s. Soweit eine Verbindung zwischen den XO-Laptops über große Distanzen aufgebaut werden soll, können zwei integrierte WLAN-Antennen für bessere Signalqualität ausgeklappt werden.\n\nDamit jeder Laptop als Vermittlungsstelle arbeiten kann, ist die Datenweiterleitung in den XO-Laptops selbst im ausgeschalteten Zustand aktiv. Somit steht ein ständig verfügbarer Router oder eine Bridge für das Funknetz zur Verfügung und stabilisiert damit für die anderen Teilnehmer das gesamte Netzwerk. Die Leistungsaufnahme der Routerfunktion beträgt nur 0,25 Watt.\n\nDie im Laptop eingesetzten und für den Netzbetrieb erforderlichen WLAN-Mikrochips werden vom Unternehmen Marvell produziert und haben eine Übertragungsgeschwindigkeit von maximal 2.000 Kbit/s. Für das Routing und damit zur Steuerung des eingebauten Routers verwendet der Hersteller Marvell einen Mikrochip, der auf der ARM-Architektur basiert.\n\nDie im Mikrochip verwendete Software (Firmware) basiert noch auf nicht-quelloffener Software eines anderen Herstellers. Der Quellcode für diese Firmware kann daher nur nach Unterzeichnung einer Vertraulichkeitsvereinbarung eingesehen werden. Dies entspricht nach Ansicht einiger Kritiker nicht dem an freier Software orientierten Gesamtkonzept.\n\nNach Mitteilung von Jim Gettys, Projektteilnehmer bei „One Laptop per Child“, wird an einem Firmware-Ersatz gearbeitet. Dieser soll dann unter die freie Lizenz „GNU GPL“ gestellt werden. Zusätzlich befindet sich das Projekt in juristischen Verhandlungen mit Rechtsanwälten von Marvell und dem Hersteller der Firmware über eine offenere Lizenz. Der Treiber im Linux-Kernel selbst war von Anfang an GPL-konform und ist mittlerweile im Linux-Kernel integriert.\n\nFür das Mesh-Netzwerk müssen drei Protokolle verwendet werden.\n\nAls WLAN-Protokoll wird der IEEE 802.11b/g-Standard verwendet. Das IEEE-802.11b/g-Protokoll beschreibt allerdings nur den Datenverkehr innerhalb eines Funknetzwerkes, welches nach dem Prinzip eines kabelgebundenen Ethernet-Netzwerkes arbeitet. Nach diesem Protokoll werden die Daten lediglich zwischen Sender und Empfänger direkt ausgetauscht.\n\nWeiterhin können Datenpakete von dem einen zum anderen Rechner weitergeleitet werden. Für diese sogenannten „Hops“ muss zusätzlich das Protokoll IEEE 802.11s eingesetzt werden. Die Datenpakete werden dann so lange weitergeleitet, bis ein Laptop im Mesh-Netzwerk das Datenpaket dem eigentlichen Empfänger zugestellt hat. Somit können Laptops, welche sich eigentlich mit ihrem Funksignal außerhalb ihrer gegenseitigen Reichweite befinden, über Vermittlungsstellen miteinander kommunizieren bzw. in das Internet „einklinken“.\n\nDurch Einbau zweier Antennen (die „Hasenohren“) erhöht sich die Signalqualität und damit die Reichweite des Funksignals beträchtlich. Unterschiedliche Werte ergeben sich, je nachdem, ob Funkkontakt zwischen zwei XO-Laptops oder zwischen einem XO-Laptop und Standardhardware aufgebaut wird. Unter optimalen Bedingungen, d. h. bei Verbindungsaufbau zwischen zwei XO-Laptops mit ausgeklappten Antennen im flachen, dünn besiedelten Gebiet, ergibt sich eine maximale Reichweite von ca. zwei Kilometern. Bei Tests im Outback von Australien wurde eine Reichweite von 1,6 Kilometern gemessen.\n\nDie XO-Laptops sind nach bisherigen Tests, aufgrund ihrer eingebauten Mesh-Netzwerkfunktion, in der Lage, Daten von einem Sender zu einem bestimmten Empfänger über bis zu 20 weitere Laptops weiterzuleiten. Dadurch ergibt sich eine theoretische Reichweite des Mesh-Netzwerkes von 30 bis 40 Kilometern. Somit wird ebenfalls auf besonders effiziente Art und Weise ein Breitbandanschluss in das Internet aufgebaut. Damit kann zugleich das „Problem der Letzten Meile“ in den teilnehmenden Partnerländern gelöst werden.\n\nSoweit Daten über das Funknetz innerhalb eines Gebäudes weitergeleitet werden, können die XO-Laptops untereinander Kontakt zwischen den einzelnen Gebäudeetagen aufnehmen.\n\nÜber das mobile Mesh-Netzwerk (auch als mobiles Ad-hoc-Netz bezeichnet) vernetzen sich automatisch die in Reichweite befindlichen Laptops miteinander über WLAN, ohne dass manuelle Konfiguration erforderlich wäre. Damit entsteht ein lokales Netz.\n\nDas Zuweisen einer IP-Adresse für das ineinandergreifende Netz erfolgt automatisch. Folglich ist kein Administrator oder eine zentrale Verwaltung der IP-Adressen erforderlich. Somit würde der automatische Netzwerkaufbau auch die automatische Einrichtung eines Schulnetzwerkes bzw. eines Netzwerkes für eine bestimmte Unterrichtsstunde ermöglichen, ohne vertiefte Computerkenntnisse über Soft- und Hardware.\n\nZudem ist der Laptop, neben dem unmittelbaren Datenaustausch, für netzwerkbasierte Videogespräche, Telefongespräche, und Netzwerk-Chat geeignet.\n\nDie XO-Nutzer können nicht nur lokal das Funknetz nutzen. Soweit ein zentraler Internetzugang in der Schule vorhanden ist, können sich die Laptops über WLAN in das globale Internet „einklinken“. Der XO-Laptop verwendet dabei bereits das neue Internetprotokoll IPv6.\nDamit ist es jederzeit möglich, das Internet als Informationsquelle heranzuziehen. Wissensaneignung soll daher nicht nur auf reine Datenabfrage beschränkt sein, sondern beinhaltet auch die Nutzung des Internets als Kommunikationsmedium (z. B. Soziale Netzwerke, Chat, E-Mail).\n\nDurch spontane Bildung von beliebigen Netzwerken ermöglicht das technische Netzwerk auch die Bildung und Vertiefung von sozialen Netzwerken. Kinder sind damit in der Lage, durch Verwendung und bei Bedarf spontane Neubildung des lokalen, aber mobilen Mesh-Netzwerkes, die Zusammenarbeit und soziale Interaktion untereinander auf neue Art und Weise kennenzulernen. Kinder sollen in der Lage sein, je nach dem zu lösenden Problem und selbst über eine gewisse Distanz, Arbeitsgemeinschaften durch spontane Neuvernetzung zu bilden.\n\nAufgrund seines geringen Gewichts und der erheblichen Reichweite des Funknetzwerkes können sich die Kinder auch außerhalb der Schule miteinander vernetzen. Somit kann soziale Interaktion mittels Netzwerken und damit die Bildung von Wissensnetzen auch außerhalb der Schule erfolgen. Als weiterer positiver Aspekt wird gemäß den Befürwortern von freien Funknetzen angeführt, dass durch „PicoPeering-Vereinbarungen“ und die gemeinsame Verantwortung eines Wohnviertels für die Funktionsfähigkeit des WLAN die nachbarschaftliche Solidarität und Bürgerinitiative gefördert werden.\n\nAls ein grundsätzliches Problem stellt sich die Energieversorgung in infrastrukturschwachen Gebieten dar. Obwohl der Computer bereits eine sehr geringe Leistungsaufnahme (etwa 2,0 Watt) aufweist, sind in der Praxis weitere Wege der Energieversorgung nötig. Der XO-Laptop kann sich daher über verschiedene Quellen mit elektrischer Energie versorgen:\n\n\n\nEs muss für den effektiven Einsatz der Laptops gewährleistet werden, dass die spätere Softwarepflege möglich ist und die Weiterentwicklung der Software, angepasst an die spezifischen Bedürfnisse in den Abnehmerländern, erfolgen kann. Die unabhängige Veränderung und Weiterentwicklung der Software würde bei proprietärer Software jedoch einen Lizenz-Verstoß und eine Urheberrechts-Verletzung zur Folge haben. Deswegen wird, soweit möglich, freie Software beziehungsweise Open Source verwendet. Das gesamte Softwarepaket für Linux (sogenannte „stabile Version“) wird bereits zum Testen und Ausprobieren zum Herunterladen bereitgestellt. Der Download kann als Live-CD auch für normale PCs erfolgen bzw. kann als Emulator als Anwendung auf der Festplatte installiert werden.\n\nAls Bootloader zum Start des PC-Systems wird Open Firmware (anstatt BIOS) eingesetzt, das unter der freien MIT-Lizenz bzw. BSD-Lizenz verfügbar ist. Open Firmware ist in der Lage, über verschiedene Datenspeicher zu booten:\n\n\nDarüber hinaus wird versucht, die Größe des Codes zum Starten des Computers zu reduzieren. Da die Größe des Firmware-Speichers für den Boot-Loader bei 1024 kB liegt, wäre es möglich, den Boot-Code doppelt im Firmware-Speicher abzulegen. Es wären dann zwei Versionen im Speicher hinterlegt, eine größere Hauptversion und eine kleinere Version als Notfallsystem. Wenn dann bei der Aktualisierung des Boot-Codes ein Fehler passieren würde, könnte Open Firmware automatisch auf seine eigene Kopie zurückgreifen, den PC starten und versuchen, erneut die Aktualisierung erfolgreich durchzuführen. Somit könnte sich der Boot-Code selbst „heilen“, wohingegen bei den meisten Standard-PCs ein BIOS-Fehler zur vollständigen Unbrauchbarkeit des gesamten Computersystems führt. Da die Entwicklung des Boot-Codes durch OLPC noch nicht beendet ist, kann OLPC diese Möglichkeit der Selbstheilung noch nicht zusichern.\n\nAls Betriebssystem wird die Distribution \"Fedora 9\" des freien Open-Source-Systems Linux installiert. Sie basiert auf dem 2.6-Linux-Kernel. Fedora-Linux wird vom Unternehmen Red Hat, einem Distributions-Entwickler, auf besonders intelligenten Ressourcen- und geringen Stromverbrauch optimiert und damit speziell für diesen Laptop weiterentwickelt. Ziel der Softwareentwickler ist es, ein schlankes und damit schnelles Betriebssystem zur Verfügung zu stellen. Die Zeit für einen Kaltstart des Laptops soll auf unter eine Minute, vielleicht sogar auf ca. 30 Sekunden reduziert werden. Erreicht wird dies, indem u. a. Treiber für veraltete und nicht mehr produzierte Hardware aus dem Kernel entfernt bzw. indem systematisch Teile des Linux-Kernels effizienter programmiert werden. Manche dieser Code-Anpassungen sind bereits in den offiziellen Linux-Kernel übertragen worden und sind damit Bestandteil aller aktuellen Linux-Betriebssysteme.\n\nDer Systemstart aus dem Ruhezustand soll, nach Abschluss der System-Optimierung, nur noch 0,1 Sekunden betragen, was für den Benutzer unterhalb der Wahrnehmungsschwelle ist. Dabei sind der Hauptprozessor und sonstige Geräte abgeschaltet, lediglich der Bildschirm bleibt aktiviert. In einem zusätzlichen Chip (\"DCON\"-Chip) werden Daten für die Anzeige auf dem Bildschirm zwischengespeichert.\n\nIst der Zwischenspeicher des DCON-Chips erschöpft, wird der Hauptprozessor wieder aktiviert und aus dem RAM bzw. vom Flashspeicher neue Daten in den DCON-Speicher übertragen. Danach wird das System wieder in den Ruhezustand versetzt. Somit ist bei normaler Benutzung des XO-Laptops das Computersystem für die meiste Zeit und damit zum überwiegenden Teil inaktiv, verbraucht also insgesamt wenig Energie.\n\nBitfrost ist die neuentwickelte Sicherheitsplattform von OLPC. Sie vereinigt verschiedene in der Wissenschaft entwickelte und erfolgreich getestete Sicherheitskonzepte. Bitfrost soll gemäß seiner Spezifikation in der Lage sein, Computerviren und Spyware weitestgehend ohne Virenscanner zu bekämpfen.\n\nDie Sicherheitsplattform soll folgendes ermöglichen:\n\nDem Sicherheitskonzept liegen folgende Prinzipien zugrunde:\n\nDas Referenzmodell unterliegt gemäß der Free-Software-Tradition einer öffentlichen Diskussion. Je nach Einwänden der Beteiligten werden möglicherweise bessere Alternativen diskutiert und Bitfrost daran angepasst.\n\nDie grafische Benutzeroberfläche des Betriebssystems wird auf die Zielgruppe angepasst, d. h. für Schüler ab der Primarstufe, die gegebenenfalls noch keine Kenntnisse in Lesen und Schreiben haben. Diese maßgeschneiderte Oberfläche heißt „Sugar“ und ermöglicht die einfache Bedienung aller Funktionen auch durch Analphabeten. „Sugar“ basiert auf Software-Komponenten von Gnome. Allein durch Anklicken von selbsterklärenden Symbolen wird die Computernutzung, auch ohne Vorkenntnisse über Informationstechnologie, ermöglicht. Dabei soll sich die Computernutzung vorrangig durch Zusammenarbeit der Mitschüler untereinander bzw. zwischen Schüler und seinem Lehrer äußern.\n\nDie Design-Vorgaben liegen auf Bereitstellung von „Aktivität, nicht Anwendung“, Bereitstellung von „Werkzeugen des individuellen Selbstausdrucks“ und „Möglichkeit der jederzeitigen Gruppenarbeit“. Demnach können „Aktivitäten“ nicht nur durch eine einzelne Person gestartet werden, sondern auch mit anderen im Mesh-Netzwerk geteilt werden. Dazu kann der Nutzer die Aktivität für alle oder einen bestimmten Personenkreis veröffentlichen oder bestimmte Personen fragen, ob sie teilnehmen wollen. Jeder Teilnehmer kann die „Aktivität“ aufnehmen oder beenden. „Sugar“ vereinigt verschiedene Konzepte der Kognitionspsychologie um die Benutzer in ihrem Lernprozess zu unterstützen.\nFür den Ideen- und Dateienaustausch wurde ein „Schwarzes Brett“ integriert. Zunächst wird für jede laufende „Aktivität“ ein Schwarzes Brett automatisch eingerichtet. Die hinterlegten Dateien können dann unmittelbar in das gemeinsam erstellte Dokument übertragen werden. Für effektive Zusammenarbeit ist ein Chat-Programm integriert, um Hinweise oder sonstige Informationen mitzuteilen.\n\nDurch einen übersichtlichen und einfachen Aufbau aller Aktivitäten soll der Laptop auch geeignet sein für junge und in Informationstechnologie unerfahrene Kinder. Die grafische Oberfläche „Sugar“ soll nach Beendigung der Schulausbildung durch die Nutzeroberfläche GNOME ersetzt werden können. Damit soll der XO-Laptop nach Beendigung der Schulausbildung auch für berufliche Zwecke geeignet sein.\n\nWie bei einem Standard-PC können jederzeit Anwendungen installiert bzw. deinstalliert werden. Damit das Gerät ohne aufwendige Einrichtung jedes einzelnen Computers verwendbar ist, wird der 100-Dollar-Laptop unter Linux mit bestimmten vorinstallierten Programmen (den sogenannten „Aktivitäten“) in den Abnehmerländern verteilt. Einige Aktivitäten wurden durch die deutsche Community bereits in die deutsche Sprache bzw. in die Sprache der Partnerländer übersetzt (Lokalisierung). Dies wird über ein Onlinesystem namens Pootle ermöglicht, einer Homepage, bei der nach dem Wiki-Prinzip jeder User zur Übersetzung beitragen kann. Nachfolgende Übersicht über Status der Übersetzungsprojekte bezieht sich auf das Linux-Softwarepaket mit Stand September 2007:\n\nNeben installierter Software für den XO-Laptop werden weitere Medien auf dem 100-Dollar-Server zur Verfügung gestellt, wie etwa eine digitale Bibliothek bzw. eine Kopie der Wikipedia.\n\nZur Grundausstattung gehört unter anderem ein Webbrowser, der zur Darstellung die gleiche Software wie Firefox (die Gecko-Engine) verwendet. Ebenso wird auf dem Laptop die Textverarbeitung AbiWord, eine Anwendung zum E-Mail-Versand und ein Chat-Programm mit Videokonferenz-Funktion installiert. Der Laptop unterstützt verschiedene Formate zur Anzeige von Dokumenten, Bildern, zum Abspielen von Audio- und Videodateien (u. a. PDF-Dateien, Word-Dateien, ODF-Dateien, JPEG-Dateien, PNG-Dateien, MP3-Dateien, Ogg-Dateien, Video-Flash).\n\nDie Textverarbeitung \"Abiword\" ermöglicht das parallele Bearbeiten eines Dokuments mit bis zu fünf Personen über das Funknetz und globale Internet. Die Eingabe der ersten Person ist dabei sofort auf dem Bildschirm der anderen beteiligten Mitarbeiter sichtbar. Ebenso gehören zur Grundausstattung Neuentwicklungen, etwa für den Musikbereich. Zu erwähnen sind das digitale Musikinstrument „TamTam“ und das Spiel „Musik-Memory“.\n\n„TamTam“ ist ein Musiksynthesizer, der von Musikern und Software-Entwicklern der Universität Montreal entwickelt wird. Er kann die Töne von Musikinstrumenten, aber auch von sonstigen Tönen aus der Natur verarbeiten. Es ist für die Zusammenarbeit über das Mesh-Netzwerk konzipiert. Folglich kann ein von einem Schüler begonnenes Musikprojekt jederzeit über das Mesh-Netzwerk, zusammen mit anderen Mitschülern, weiterentwickelt werden. Die entstandene Musikdatei kann ggf. über das Mesh-Netzwerk an andere weitergegeben werden. Das Spiel „Musik-Memory“ ist das Spiel „Memory“, allerdings nicht mit Bildern, sondern mit Tönen bzw. Liedern.\n\nZur Förderung der Kreativität wird ein Malprogramm für Kinder beigefügt. Auf einem digitalen Blatt Papier können die Kinder verschiedene Dinge zeichnen. Das Malen kann dabei alleine oder zusammen mit anderen Kindern erfolgen. Die Kinder sind dabei über das Funknetz miteinander verbunden, d. h. mehrere Kinder können gemeinsam ein digitales „Blatt Papier“ bemalen.\n\nLogisches Denken soll durch die Anwendung Squeak gefördert werden. Auf dem Laptop werden dazu einfache, in Squeak entwickelte Bausteine (die \"EToys\") installiert. Auch diese Bausteine können von den Kindern über das Netzwerk getauscht werden. Daneben werden von verschiedenen Communitys Spiele-Klassiker wie Tetris oder auch SimCity auf den XO übertragen.\n\nZudem wurden von verschiedenen Diensteanbietern weitere Nutzungsmöglichkeiten eingeräumt: Laut einem Interview mit Nicholas Negroponte stellt Google digitale Landkarten zur Verfügung, eBay ermöglicht die Benutzung von PayPal (System für Kleingeldzahlungen) und Skype (Videogespräche und Telefongespräche über Internet bzw. Internet-Chat). News Corp stellt die Videoplattform Myspace zur Verfügung. Diese Inhalte stehen jedoch nicht unter einer freien Lizenz.\n\nDas größte Hindernis ist die noch geringe Verfügbarkeit von geeignetem digitalem Inhalt zur Wissensvermittlung. Da der Laptop auch für E-Learning innerhalb und außerhalb des Schulunterrichts konzipiert wurde, bedarf es noch der Generierung großer Mengen digitalen Lehrmaterials für die Schule und die unabhängige Weiterbildung.\n\nEine Bereitstellung von digitalem Unterrichtsmaterial in Verbindung mit dem Mesh-Netzwerk würde die schnelle Verteilung innerhalb einer Schulklasse bzw. innerhalb einer Schule ermöglichen. Rechtliche Bedenken bezüglich Urheberrechtsverletzungen würde durch die Verwendung flexibler freier Lizenzen, wie etwa Creative Commons, ausgeschlossen sein.\n\nAls digitales Unterrichtsmaterial kommen folgende Materialien in Betracht: \n\n\nFür das digitale Inhaltemanagement wird darüber nachgedacht, ein Wiki-System zu verwenden, dessen Software ebenfalls unter der freien GNU General Public License steht. Dies hätte auch den Vorteil, dass jeder Teilnehmer an dem Projekt mit seinem Wissen, ähnlich der Idee von Wikipedia, zum Erfolg beitragen kann. Auf diese Weise könnten freie Unterrichtsmaterialien ausgetauscht werden.\n\nDa das gemeinnützige Projekt auf freier Software basiert und somit der Quellcode frei verfügbar ist, arbeiten auch Freiwillige weltweit an der Entwicklung der Software mit. Es ist festzuhalten, dass ohne die Vorarbeit von freier Software das gemeinnützige Projekt in dieser Form nicht möglich wäre.\n\nNicholas Negroponte teilte Ende 2010 mit, dass 2 Millionen Geräte in 40 verschiedene Länder ausgeliefert wurden. In Uruguay verfügt beinahe jedes Schulkind über einen OLPC-Laptop – auch in entfernt gelegenen Dörfern.\n\nOLPC XO-1 hatte eine Vorreiterrolle in der Entwicklung kostengünstiger, kompakter Notebooks, und nahm somit die Entwicklung der Netbooks vorweg. Die Organisation OLPC sieht in Netbooks jedoch keine Konkurrenz, da sie nicht für Bildungszwecke in Entwicklungsländern ausgelegt seien.\n\nDie Inter-American Development Bank (IDB) veröffentlichte 2012 eine groß angelegte Studie, in der die Auswirkungen des OLPC-Projekts in Peru untersucht wurden. Zwar nutzten die Schüler der Primärschulen die vorinstallierten Anwendungen häufig. Jedoch steigerte sich die Anwesenheit im Unterricht nicht, die Schüler lasen nicht mehr als die Vergleichsgruppe, und die Lernmotivation konnte nicht messbar gesteigert werden. Die Studie schlägt vor, Gelder besser für die Verkleinerung der Klassen und für die Lehrerausbildung einzusetzen.\n\nIntel-Chef Craig R. Barrett sagte im Dezember 2005, dass der Laptop nicht alle Möglichkeiten eines vollwertigen Computers biete, und bezeichnete ihn als „Gadget“ (engl. etwa \"technische Spielerei\"). Er zweifelte am Erfolg des Projekts. Die Presse machte für seine Haltung unter anderem den Umstand verantwortlich, dass die CPU des Laptops von Intels Konkurrenten AMD hergestellt wird. Zudem wies er darauf hin, dass es sich „nicht um ein Laptop-Projekt, sondern um ein Bildungsprojekt handelt“. Negroponte warf Barrett außerdem vor, den Erfolg des 100-Dollar-Laptops gezielt zu unterminieren, indem er Partner für Intels eigenen low-budget-laptop abwerbe.\n\nMicrosoft-Gründer Bill Gates bemängelte im März 2006 die Größe des Bildschirms, das Fehlen der Festplatte und die Handkurbel zur Stromgenerierung („Himmel, nehmt einen vernünftigen Computer.“). Stattdessen stellte er die gemeinsam mit Intel entwickelte 100-Dollar-Alternative \"Origami\" vor. Dieser hat ebenfalls einen Sieben-Zoll-Bildschirm, aber keine Tastatur. Stattdessen erfolgt bei dem vorgestellten „Lifestyle-Gerät“ die Eingabe mit den Fingern bzw. mit einem speziellen Induktivstift über den sieben Zoll kleinen Touchscreen. Die Presse mutmaßte, dass die Konzeption des 100-Dollar-Laptops als Linuxsystem Anlass seiner ablehnenden Haltung war.\n\nTony Roberts von Computer Aid International äußerte im Juni 2006 Zweifel am Konzept des 100-$-Laptops. „Sie [One Laptop per Child] wollen eine nicht standardisierte und ungetestete Plattform einführen, die nur an Regierungen verkauft wird.“ Weiterhin beanstandete er, dass der 100-Dollar-Laptop von anderen Projekten mit ähnlichen Zielen ablenke.\n\"Computer Aid International\" ist ein in London ansässiges Projekt, das gebrauchte, aber von Unternehmen gespendete Desktop-Computer repariert und an Entwicklungsländer weitergibt. Pro PC wird eine Kostenerstattung von 39 Pfund zuzüglich Kosten für den Versand erhoben. Auf diese Weise konnten seit 1998 bereits 80.000 Desktop-Computer in Entwicklungsländer verschifft werden.\n\nDie indische Regierung lehnte nach anfänglicher Zustimmung im Juli 2006 die Teilnahme am Projekt ab. Staatssekretär Sudeep Banerjee zweifelte am pädagogischen Nutzen des Projekts. Wenn jedes Kind für den Schulgebrauch einen Laptop erhielte, wäre dies der Entwicklung von Kreativität und analytischen Fähigkeiten abträglich. Eine Investition des Geldes in traditionelle Schulmittel wie Schulgebäude und Lehrer wäre sinnvoller.\n\nIm März 2007 kritisierte die Onlinezeitschrift Telepolis die Höhe der Kosten für die Entwicklungsländer. Selbst ein Laptop-Preis von 100 US-Dollar würde die finanzielle Leistungsfähigkeit der Entwicklungsländer überfordern. Zudem würde durch die XO-Laptops massenweise Elektronikschrott anfallen, der trotz eines Recyclingprogramms schwerwiegende ökologische Folgen nach sich ziehen könnte. Mittlerweile fordert aber auch Negroponte einen \"Null-Dollar-Laptop\".\n\nIm Juli 2007 wurde bekannt, dass Schülerlaptops dazu benutzt wurden, pornografisches Material im Internet herunterzuladen. Entsprechend reagierte das Projekt OLPC und integrierte einen Softwarefilter, der ein solches Herunterladen unterbinden soll. Seitens der OLPC-Stiftung wurde dieses Phänomen als gesellschaftliche Angelegenheit und explizit als kein spezifisches Problem bezüglich der Laptops eingestuft.\n\nEin Einwand ist auch, dass es Schulen in ärmeren Ländern an viel grundlegenderen Dingen mangelt. So sei es kaum sinnvoll, in Informations- und Kommunikationstechnik zu investieren, wenn es an Infrastruktur und Ausbildung der Lehrkräfte mangelt. Der Erfolg sei zweifelhaft, da diese Projekte und deren Erfolgskontrolle von der Industrie mit unterstützt würden. Projekte wie „Mehrere gute Lehrer pro Schule“ wären sinnvoller. Einige Projektländer hätten zudem Regierungen, die sich kaum um die Bildung kümmerten. Somit sei die Politik die eigentliche Ursache der Bildungsmissstände.\n\n\n\n"}
{"id": "2943560", "url": "https://de.wikipedia.org/wiki?curid=2943560", "title": "MacOS", "text": "MacOS\n\nDas Betriebssystem macOS, früher Mac OS X und OS X, ist das Desktop-Betriebssystem von Apple für dessen Mac-Computer. Außerdem ist macOS kommerziell das erfolgreichste Unix für Personal Computer und UNIX-03-zertifiziert. Es besitzt eine objektorientierte Desktop-Umgebung sowie Unix-typische Schnittstellen. Mit ihm ersetzte das kalifornische Hard- und Software-Unternehmen Apple sein klassisches Mac-Betriebssystem und es wurde zum Fundament weiterer Apple-Entwicklungen wie iPod und iPhone. Dabei bildet Darwin als sog. \"Core Operating System\" die gemeinsame Basis, das den Hybridkernel XNU umfasst.\n\nIm Detail ist macOS ein proprietäres Betriebssystem auf der Basis des freien, ebenfalls von Apple entwickelten Darwin, dem BSD-Unix zugrunde liegt. Überdies ist das Betriebssystem ab Version 10.5 „Leopard“ nach der Single UNIX Specification als UNIX zertifiziert (in Großbuchstaben Warenzeichen).\n\nTrotz elementarer technischer Unterschiede gilt macOS historisch als jüngstes Mitglied der (nicht-unixoiden) Mac-OS-Betriebssystem-Familie, die ab 1984 für die hauseigenen Macintosh-Computer eingeführt worden war.\n\nTatsächlich ist macOS mit seinem Darwin-Kern eine Weiterentwicklung von NeXTStep (und dessen BSD-Unix-Kern), das von Steve Jobs’ Unternehmen NeXT entwickelt und 1988 erstmals veröffentlicht worden war.\n\nIn abgewandelter Form kommt das Betriebssystem als iOS beim iPhone, iPad und iPod touch zum Einsatz, sowie als Apple-TV-Software bei der ersten Generation des Apple TV. Wiederum auf iOS basieren watchOS, das auf der Apple Watch läuft, die Software des Apple TV der zweiten und dritten Generation wie auch tvOS, das ab der vierten Generation des Apple TV zum Einsatz kommt.\n\nSteve Jobs musste 1985 auf Drängen des Managements das von ihm mitgegründete Unternehmen Apple verlassen. Er gründete daraufhin NeXT, wo er zusammen mit einigen vertrauten Apple-Entwicklern, die mit ihm Apple verließen, sowohl an Hardware als auch an Software arbeitete. Das Kernstück der Bemühungen waren die NeXTstation und das Betriebssystem NeXTStep. Doch während das NeXT-Betriebssystem von Fachleuten hoch gepriesen wurde, blieben die erhofften Hardware-Verkäufe der NeXTstation aus. NeXT blieb nichts anderes übrig als die Hardwareproduktion 1993 einzustellen und das Betriebssystem auch für externe Computerarchitekturen anzubieten. Gemeinsam mit Sun wurde dabei ab 1995 die objektorientierte NeXTstep-Programmierschnittstelle als plattformübergreifendes Framework weiterentwickelt, zu OpenStep. Dementsprechend wurde das damit neu implementierte Betriebssystem ab Version 4 als OPENSTEP bezeichnet.\n\nBei Apple hingegen verkaufte sich die Hardware relativ gut, doch galt das ursprüngliche Macintosh-Betriebssystem als technisch veraltet und nicht zukunftsfähig. Apple unternahm daher selbst einige Anstrengungen und Versuche, ein neues Betriebssystem zu entwickeln, das zum ursprünglichen Macintosh-Betriebssystem weitestgehend kompatibel sein sollte. Da jedoch auch Ende 1996 noch kein moderner Nachfolger in Sicht war und gleichzeitig die Hardwareverkäufe immer weiter zurückgingen, war man bei Apple schließlich bestrebt, ein bestehendes Betriebssystem zu übernehmen.\n\nLetztendlich kaufte Apple Ende 1996 das Unternehmen NeXT samt Betriebssystem OPENSTEP auf. Die NeXT-Mitarbeiter wurden ins eigene Unternehmen integriert und Steve Jobs übernahm 1997 wieder die Leitung des von ihm in den 1970er Jahren mitgegründeten Apple.\n\nGleich nach der Übernahme wurde OPENSTEP im Projekt Rhapsody modernisiert und auf die von Apple damals in den Macintosh-Computern genutzte PowerPC-Architektur portiert. Zunächst sollte Rhapsody das ursprüngliche Macintosh-Betriebssystem gänzlich ersetzen, wofür Softwareanbieter ihre Anwendungen hätten komplett neu schreiben müssen – was diese ablehnten. Apple reagierte, indem Rhapsody weiter zu „Mac OS X“ entwickelte wurde. Entwicklern wurde der Umstieg vom originären Macintosh-Baukasten zum neuen Mac OS X durch die Einführung von Carbon erleichtert, und für Anwender blieb mit der Classic-Umgebung die volle Kompatibilität zum bisherigen Betriebssystem gewahrt.\n\nBei der Ankündigung von „Mac OS X“ im Jahr 1998 sollte das X einerseits als römische 10 die Nachfolge auf das klassische Mac OS aufzeigen und andererseits die unixoide Abstammung der neuen Betriebssystemgeneration hervorheben, die im Englischen nicht willkürlich mit „next generation“ beworben wurde. Die einzelnen neuen Hauptversionen wurden anfangs nach Raubkatzen benannt, etwa Jaguar, gegenwärtig nach natürlichen Wahrzeichen Kaliforniens wie dem Yosemite-Nationalpark und El Capitan, einem markanten Felsvorsprung im Park, oder Mojave, einer Wüste, die teilweise in Kalifornien liegt.\n\nAm 13. Juni 2016 wurde während der Keynote der WWDC bekanntgegeben, dass die Version 10.12 des Betriebssystems im Herbst als „macOS Sierra“ veröffentlicht werde und somit eine Umbenennung der Produktlinie von „OS X“ in „macOS“ erfolge. Apple erklärte, dies geschehe, um der Namensgebung der anderen Apple-Plattformen (iOS, tvOS, watchOS) zu entsprechen. Erst 2012 war die Produktlinie von „Mac OS X“ in „OS X“ umbenannt worden.\n\nDie Architektur ist in vier grundlegende Ebenen unterteilt:\n\nDarwin ist der elementare Unterbau, auf dem macOS aufbaut. Durch Darwin und den Kernel XNU verfügt macOS über Fähigkeiten wie Speicherschutz, präemptives Multitasking, Mehrbenutzerfähigkeit, erweitertes Speichermanagement und symmetrisches Multiprocessing (SMP). Darwin wurde unter die quelloffene Lizenz \"Apple Public Source License\" gestellt, welche mit Version 2.0 als Lizenz freier Software von der Free Software Foundation anerkannt wurde. Der XNU (X is Not Unix) getaufte Kernel wurde gegenüber OPENSTEP vollkommen überarbeitet. Während OPENSTEP noch einen Mach 2.5 verwendete, setzt Darwins Kernel auf dem seinerzeit überarbeiteten OSF Mach Kernel 3.0 auf (kurz OSFMK). Bereits im Betriebssystemprojekt Rhapsody wurde der OSFMK mit Teilen des monolithischen FreeBSD-Kernels ergänzt und so als Hybridkernel realisiert. Auch Erfahrungen aus MkLinux flossen bei der Modernisierung mit ein. Außerdem liefert macOS ein größtenteils von FreeBSD und NetBSD stammendes Userland mit, das über das mitgelieferte Programm \"Terminal\" u. a. Terminalemulationen genutzt werden kann. Seit Panther 10.3 (2003) ist als Unix-Shell standardmäßig Bash voreingestellt. Außer Bash (bash, sh) werden auch die Z Shell (zsh), die TENEX-C-Shell (tcsh, csh) sowie die KornShell (ksh) mitgeliefert.\n\nDie auf OpenStep aufbauende vollständig objektorientierte Programmierschnittstelle wurde zu Cocoa weiterentwickelt. Mit Aqua wurde eine völlig neue grafische Benutzeroberfläche entworfen, deren Design-Konzept, wie auch die Programmierschnittstelle Cocoa und Quartz mit Display-PDF sowie das Dock, in großem Maße auf das von NeXT entwickelte NeXTStep zurückgeht.\n\nAus dem klassischen Mac OS wurden vor allem die globale Menüleiste, der Finder und QuickTime sowie einige weitere Bibliotheken übernommen. Der originäre Macintosh-Baukasten (Toolkit) konnte nicht vollständig in Mac OS X integriert werden, da die Funktionen nicht mit den modernen Betriebssystemfunktionen wie Speicherschutz und Multitasking kompatibel waren. Stattdessen wurden mit Carbon 6.000 der rund 8.000 Funktionen des Macintosh-Baukastens und somit ein Großteil in Mac OS X integriert, was die Portierung existierender Anwendungen erleichterte. Da Carbon auch ins klassische Mac OS integriert wurde, mussten Entwickler nur eine Software für beide Betriebssystemlinien entwickeln. Mit der Classic-Umgebung war für Anwender bestehender Programme zudem eine nahezu vollständige Kompatibilität der nicht auf Carbon portierten Anwendungen gegeben.\n\nEs war jedoch auch klar, dass Cocoa die modernere der beiden Programmierschnittstellen ist. Carbon ist zwar weiterhin Bestandteil von macOS, wurde aber seit 2007 nicht mehr weiterentwickelt und bleibt daher auf 32-Bit beschränkt. Waren anfangs noch Teile von Mac OS X selbst unter Verwendung von Carbon geschrieben, so sind inzwischen sämtliche Komponenten mittels Cocoa implementiert worden, zuletzt der mit Snow Leopard 10.6 ausgelieferte Finder (2009). Auch Programme von Drittherstellern wurden schließlich von Carbon nach Cocoa portiert, beispielsweise die Creative Suite 5 von Adobe (2010).\n\nAuf Wunsch kann eine automatische Benutzer-Anmeldung ohne Passwortabfrage aktiviert werden. Bei sicherheitsrelevanten Operationen wird dennoch ein Kennwort verlangt, wie es bei Unix üblich ist.\n\nDie auffälligste Änderung bei macOS gegenüber seinen Vorgängern, dem klassisches Mac OS bis Version 9 und Rhapsody, das noch die Oberfläche von Mac OS 8 hatte, ist die neue Oberfläche Aqua (lat. für Wasser). Sie soll durch Lichteffekte wie Reflexionen und Schlagschatten auf diversen Oberflächenelementen wie Schaltflächen oder eingeblendeten Menüs an Wassertropfen erinnern. Ebenfalls markant ist die Nadelstreifen-Optik der Fensterhintergründe und der Fotorealismus (bis Yosemite) der Icons.\n\nAls ein weiteres Erscheinungsbild für Fenster gab es bis Tiger 10.4 (2005) \"\", gebürstetes Metall. In den Apple empfiehlt Apple die Verwendung dieses Designs für Programme, die einen Teil der Hardware oder ein bestimmtes Gerät darstellen (z. B. eine Digitalkamera oder einen DVD-Spieler). Mit Leopard 10.5 (2007) wurden die unterschiedlichen Erscheinungsbilder optisch vereinheitlicht.\n\nGanz neue Elemente in Aqua gegenüber älteren Oberflächen sind sogenannte \" (dt. ‚Tafeln‘, ‚Platten‘)\" und ' (dt. ‚Schubladen‘). ' sollen einem Benutzer deutlich machen, zu welchem Dokument eine sich öffnende Dialogbox gehört: eine Art Dialogfenster, das direkt an die Titelzeile des betroffenen Dokuments angehängt wird und das somit zu seinem untrennbaren Bestandteil wird. \" sind Schubladen, die durch einen Klick auf die entsprechende Schaltfläche links oder rechts des Hauptfensters ausgefahren werden und Elemente enthalten, die nicht dauerhaft zur Programmbedienung benötigt werden. In der ersten Version von Apples E-Mail-Programm Mail befand sich beispielsweise die Ordnerstruktur in einem , wurde jedoch in Version 2 Teil des Hauptfensters.\n\nEine weitere Besonderheit von Aqua ist die Art der Darstellung der Bildschirminhalte. Hierbei verwendet Apple eine eigene Technik namens '. Diese Darstellung zweidimensionaler Elemente basiert auf dem Portable Document Format (PDF). Die Weiterentwicklung dieser Technik namens ' beschleunigt die Darstellung, indem jedes Fenster als Textur betrachtet und so nicht mehr nur vom Haupt-, sondern vom Grafikprozessor berechnet werden kann. Fenster können dadurch ohne hohe Prozessorlast in Echtzeit skaliert und transformiert werden.\n\nDie native Programmier- und Anwendungsschnittstelle für Aqua-Programme ist Cocoa, welches eine Weiterentwicklung von OpenStep ist. Cocoa-Programme werden vorwiegend in den Sprachen Objective-C und Swift geschrieben. Während die Entwicklung von Objective-C zeitlich über die von NeXTStep hinausreicht, ist Letzteres eine Apple-eigene Neuentwicklung, die 2014 vorgestellt wurde. Sie soll die Vorteile moderner Sprachen vereinen, aber Objective-C nicht ablösen.\n\nVerschiedene Brückenschnittstellen, sogenannte \" (siehe auch Brücke), ermöglichen es zudem, Cocoa mit Ruby, Python und Java zu nutzen. Seit Tiger 10.4 (2005) wird die Java-Bridge nicht mehr aktualisiert. Mit AppleScript Studio besteht darüber hinaus die Möglichkeit, Programme in AppleScript zu schreiben und mit Objective-C oder anderen Sprachen zu erweitern.\n\nFür die klassische und die übliche Hochsprachenprogrammierung stellt macOS außer Cocoa und der Carbon-Bibliothek eine vollständige Java-5.0-Umgebung, eine POSIX- und SUS-konforme BSD-Umgebung sowie mit XQuartz eine X11-Umgebung zur Verfügung. Daneben werden die verbreiteten Unix-Programmiersprachen (z. B. Perl, PHP, Python, Ruby und Tcl und C) zur Verfügung gestellt. Bei Belieben kann auf einen Großteil des FreeBSD-Userlands zugegriffen werden; siehe #Programme anderer Betriebssysteme in macOS.\n\nEin mit dem Macintosh-Baukasten für das klassische System 7 (oder älter) geschriebenes Programm war mit dem neueren Mac OS X nicht kompatibel. Apple veröffentlichte daher die Programmbibliothek \"Carbon\" für das klassische Mac OS ab Version 8 und das neue Mac OS X. Carbon basiert auf einer Teilmenge der originalen \"\". Entwickler mussten ihre Programme nicht nach Cocoa portieren, sondern konnten vom originalen Macintosh-Baukasten auf Carbon portieren. Derart „carbonisierte Programme“ () waren auf beiden PowerPC-Betriebssystemen nativ lauffähig. Carbon existiert nur als 32-Bit-Version, seine Entwicklung wurde 2007 eingestellt.\nUm nichtangepasste Programme (auch solche, die noch für den 68k-Prozessor geschrieben wurden) unter Mac OS X benutzen zu können, gab es bis Tiger 10.4 die Classic-Umgebung. Diese war ein als Laufzeitumgebung geladenes Mac OS 9 innerhalb von Mac OS X, in dem solche Programme transparent in Mac OS X integriert weiterhin benutzt werden können. Technisch gesehen ist die Classic-Umgebung somit eine Virtuelle Maschine für die PowerPC-Architektur (G3, G4 und G5), auf den 2006 eingeführten Intel-Macs läuft die Umgebung nicht mehr. Der Großteil der älteren Software für Mac OS, auch solche für sehr alte Macs (68k-CPUs), konnte auf diese Weise verwendet werden. Die Classic-Umgebung () ist eine Weiterentwicklung der Blue Box von Rhapsody.\n\nKompatibilitätsprobleme innerhalb von Mac OS X gab es vor allem durch den im Januar 2006 vorgenommenen Wechsel der zugrundeliegenden Prozessorarchitektur von PowerPC (CPUs von IBM und Motorola, 32 und 64 Bit) auf IA-32 (CPUs von Intel, sog. Intel-Macs), kurz nach Einführung auch und inzwischen ausschließlich als 64-Bit-Architektur x64 (Intel 64) erhältlich. Für Programme, die für Mac OS X geschrieben wurden und einen PowerPC-Prozessor voraussetzen, gibt es bis Snow Leopard 10.6 auf Intel-Macs die Rosetta-Emulation. Seit Lion 10.7 (2011) ist diese nicht mehr Bestandteil des Betriebssystems.\n\nUniversal-Applikationen enthalten Maschinencode für beide Architekturen und sind somit nativ auf PowerPC- und Intel-CPUs ausführbar. U. a. Xcode unterstützte diese Technik. Inzwischen ist diese Technik nicht mehr von Belang.\n\nAuf Macs mit PowerPC-Prozessor (bis 2006) war es bereits möglich, andere, meist freie Betriebssysteme wie FreeBSD oder Linux zu verwenden. Apple selbst unterstützte MkLinux aktiv, sodass auch andere Linux-Distributionen über die Open Firmware leicht startbar waren. Auch der Mac-OS-X-Bootloader BootX konnte Linux starten.\n\nAuf PowerPC-Macs war es zudem mit Emulatoren möglich, x86-Betriebssysteme als Gast-Betriebssystem unter einem unterstützten PowerPC-Betriebssystem zu nutzen. So gab es z. B. Virtual PC von Connectix (später Microsoft), das Windows auf einem Power Macintosh emulieren konnte. Im Gegensatz zu Virtualisierung ist Emulation jedoch sehr langsam.\n\nMit dem Wechsel von PowerPC- auf Intel-Prozessoren 2006 wurde es möglich, eine Vielzahl an x86-basierenden Betriebssystemen auch auf Macs zu nutzen. Apple unterstützt dies aktiv mit der seit Leopard 10.5 mitgelieferten Software Boot Camp, mit deren Hilfe Windows auf einer separaten Partition installiert und im BIOS-kompatiblen Modus gestartet werden kann. Im EFI-Modus wird Windows jedoch nicht unterstützt. Über die EFI-Bootloader rEFIt und rEFInd ist es jedoch möglich, einige der Einschränkungen, die sich daraus ergeben, zu mindern, indem z. B. von Partitionen gestartet werden kann, die durch den EFI-Startup-Manager eines Intel-Macs nicht auswählbar sind. Neben Windows sind x86-Unices ebenfalls weiterhin nutzbar, z. B. Linux- und BSD-Distributionen.\n\nDurch den Prozessorwechsel ist es zudem mit Virtualisierungssoftware möglich, nahezu jedes andere x86-Betriebssystem auf einem Mac unter einem der (unterstützten) gestarteten Betriebssysteme z. B. in einem Fenster auszuführen. Solche Programme sind u. a. VMware Fusion, Parallels Desktop for Mac und VirtualBox.\n\nEtwa Homebrew, Fink oder MacPorts ergänzen macOS um freie Unix-, BSD- bzw. GNU- und andere Programme.\n\nEine weitere Möglichkeit, Windows-Anwendungen auszuführen, stellt die Windows-kompatible Laufzeitumgebung Wine dar. Vorteil ist hierbei, dass nicht ein vollständiges Betriebssystem zusätzlich laufen muss; vor allem aber dass keine zusätzliche Windows-Lizenz benötigt wird. Des Weiteren können mehrere Umgebungen für unterschiedliche Windows-Versionen gleichzeitig laufen. Vor allem durch das freie Darwine und das kommerzielle CrossOver wurde Wine unter macOS auch Laien zugänglich. Ein weiteres freies Projekt ist PlayOnMac, eine Variante von PlayOnLinux; es bietet eine einfach zu bedienende grafische Oberfläche zum Einrichten von Software.\n\nWeil in Apple-Computern neben dem selbstentwickelten Motherboard Standard-Hardware und Intel-Chips verwendet werden, gibt es Tüftler und kommerzielle Anbieter, die macOS auf anderen als Apple-Computern installieren. Dabei werden macOS und Treiber modifiziert oder Kernel-Erweiterungen geschrieben. Derartige sowohl auf Intel als auch AMD basierende Systeme werden als „Hackintosh“ bezeichnet, gewerblich angebotene Konfigurationen mitunter als „Mac-Klon“.\n\nDie Apple-Lizenzbestimmungen erlauben die Installation von macOS jedoch nur auf Apple-Hardware. Ob den Benutzern und Händlern die Installation und Nutzung des Betriebssystems auf fremder Hardware von Apple rechtswirksam untersagt werden kann, ist nicht geklärt. Apple wehrt sich vor allem gegen gewerblich angebotene Systeme, die wenig technisches Wissen beim Käufer voraussetzen. Kommerzielle Anbieter, die Computer anderer Hersteller für den Betrieb mit macOS umrüsten, wurden von Apple gezwungen, ihre Geräte nur noch ohne macOS auszuliefern. Ob das reine Einrichten von PCs zum Betrieb von macOS bereits illegal ist, wenn der Händler selbst keine lizenzwidrige Installation des Betriebssystems vornimmt, blieb ungeklärt.\n\nSeit Lion 10.7 wird das Betriebssystem nicht mehr auf Datenträgern angeboten, sondern über den Mac App Store. Da beim Kauf bereits darüber informiert wird, dass die Lizenz nur zur Installation auf Apple-Geräten berechtigt, ist die Möglichkeit zur Nutzung von macOS auf anderen Computern mit einer legalen Lizenz de facto nicht mehr gegeben.\n\nFür Computerbastler gibt es Anleitungen auf Websites, wie macOS auf Computern betrieben werden kann, die nicht von Apple produziert sind. Fälle, in denen dieser private Gebrauch rechtlich verfolgt wurde, sind nicht bekannt.\n\nAuch zu Zeiten, in denen Mac OS X ausschließlich für die PowerPC-Architektur verfügbar war, bestand die Möglichkeit, Mac OS X auf Fremdhardware zu nutzen. Allerdings gab es auf dem Markt nur einige wenige frei erhältliche Motherboards, die mit G3- und G4-Prozessoren bestückt werden konnten – beispielsweise das Pegasos-Board, den AmigaOne oder den Teron. Ein Komplettsystem auf Basis dieser Komponenten war allerdings sehr teuer in der Anschaffung und benötigte ferner noch eine Mac-OS-X-Lizenz, die in der damaligen Zeit ca. 100 Euro kostete. Die Konfiguration eines PowerPC-basierten Hackintosh war dadurch eher eine Spielerei als eine lukrative Alternative zur Apple-Hardware.\n\n\"Yellow Box\", ein Vorläufer der heutigen Cocoa-Programmierschnittstelle (siehe auch Rhapsody), wurde speziell dafür konzipiert, unter verschiedenen Betriebssystemen auf unterschiedlichen Hardwareplattformen nutzbar zu sein. Im Jahr 1997 warb Apple damit, dass sich dieses OpenStep-kompatible Framework problemlos in Windows integrieren lasse. Noch vor Erscheinen von Mac OS X wurden diese Möglichkeiten aber wieder eingeschränkt.\n\nDas GNUstep-Projekt macht sich diese grundsätzlich in macOS immer noch vorhandene Möglichkeiten der Portierbarkeit zu Nutzen und entwickelt einen freien Nachbau der macOS-Frameworks und -Bibliotheken. Mit GNUstep lassen sich Anwendungen, die für macOS entwickelt wurden, ohne große Anpassungen für Linux, Unix oder Windows kompilieren und danach nutzen.\n\nDarauf aufbauend wird Darling entwickelt, eine macOS-kompatible Laufzeitumgebung, mit der sich macOS-Anwendungen ohne eine Neukompilation unter Linux nutzen lassen.\n\nVerschiedene lokale Dateisysteme werden von macOS unterstützt. Bevorzugt wird das mit High Sierra 2017 eingeführte APFS, welches das weiterhin unterstützte HFS+ ersetzen soll.\n\nMit macOS Sierra (10.12) entfiel 2016 die Unterstützung für das ursprüngliche Macintosh-Dateisystem HFS komplett.\n\nAls Startvolumen (Partition von der gebootet wird) ist eine mit APFS oder HFS+ formatierte Partition vorgesehen. Bis Tiger 10.4 konnte das Betriebssystem auf eine mit dem Unix-Dateisystem UFS formatierte Partition installiert und davon gestartet werden. Mit Leopard (10.5, 2007) war das nicht mehr möglich und mit Lion (10.7, 2011) entfiel die UFS-Unterstützung gänzlich.\n\nWeitere nativ unterstützte Dateisysteme sind: ISO 9660,\nFAT12, FAT16 und FAT32,\nexFAT (ab 10.6.5),\nNTFS (ab 10.6),\nUDF (nur lesend).\n\nBeschränkt lesender Zugriff auf ZFS war in Leopard 10.5 möglich, wurde aber mit Snow Leopard 10.6 nicht mehr unterstützt.\n\nNativ unterstützte Netzwerkdateisysteme sind AFP, FTP (nur lesend), NFS, SMB/CIFS und WebDAV.\n\nMacFUSE bzw. dessen Nachfolger OSXFUSE stellt eine macOS-Variante von FUSE dar. Damit können plattformfremde Dateisysteme zur Verfügung gestellt aber auch eigene Dateisysteme entwickelt werden; theoretisch kann somit jeder Datenbestand in Form von Verzeichnissen und Dateien bereitgestellt und in den lokalen Verzeichnisbaum eingehängt werden – z. B. als dynamisch generierte In Kombination mit der in macOS integrierten „Ordner überwachen“-Funktion ergeben sich zusätzliche Möglichkeiten der Automatisierung \n\nMittels FUSE kann beispielsweise auf die Dateien eines tragbaren Medienabspielgeräts, einschließlich iPod, derart zugegriffen werden, dass die Titelnamen als Dateinamen anstatt der tatsächlichen Dateinamen aufgelistet werden. Mit SSHFS entstand die Möglichkeit, Dateien auf einem anderen Computer (), auf die durch eine SFTP-Verbindung zugegriffen wird, wie lokale Dateien behandeln zu können. Mit NTFS-3G ist es möglich, auf NTFS-Partitionen zu schreiben. Zwar liefert Mac OS X seit Panther 10.3 (2003) NTFS-Lesezugriff bereits mit,\nder macOS-eigene Schreibzugriff, der ab Snow Leopard 10.6 (2009) vorhanden ist, befindet sich jedoch offiziell noch in einem experimentellen Stadium und ist daher standardmäßig nicht aktiviert.\n\nApples macOS bietet durch Darwin die für Unix typischen Netzwerkeigenschaften. Dadurch können mehrere Macs relativ einfach zu einem Cluster verbunden werden. Unter dem Namen Bonjour unterstützt macOS Zeroconf, das es Benutzern ermöglicht, Netzwerkverbindungen und -dienste zu nutzen, ohne sie vorher konfigurieren zu müssen.\n\nAuch liefert es einen Samba-Server mit und unterstützt SMB bzw. CIFS einschließlich Drucker-Freigaben. Auch das Apple Filing Protocol wird weiterhin unterstützt.\n\nDrucker im Netzwerk können neben CIFS auch via AirPrint, Internet Printing Protocol, Line Printer Daemon Protocol und JetDirect angesprochen werden.\n\nBisher war macOS nur selten Ziel von Angriffen und gilt daher unter Nutzern als vergleichsweise sicher. Auf welche Faktoren dies zurückgeführt werden kann, ist umstritten. Unter Sicherheitsexperten wurde Mac OS X bis 2011 für bestimmte Angriffe unsicherer als etwa Windows Vista eingeschätzt, da Sicherheitsfeatures wie nicht ausführbarer Speicher und Address Space Layout Randomization (ASLR) fehlten oder unvollständig waren. Bei Lion 10.7 (2011) hat Apple die Sicherheitsarchitektur grundlegend überarbeitet. Damit galt es als sicherer als vergleichbare Betriebssysteme. Bekannt gewordene Sicherheitslücken schließt Apple durch Sicherheitsaktualisierungen. Manchmal wird die Zeitspanne zwischen Bekanntwerden und Schließen einer Sicherheitslücke kritisiert.\n\nIm Juni 2015 wurde eine Studie veröffentlicht, die gravierende Sicherheitslücken in iOS und OS X beschreibt, mithilfe derer sich Passwörter und Daten auslesen lassen; die Forscher nutzten dabei fehlende Sicherheitsmechanismen bei der Kommunikation von Apps untereinander \"(Cross-App Resource Access\", kurz \"Xara)\" aus. Entsprechend manipulierte Apps konnten sowohl im iOS-, als auch Mac App Store platziert werden. Apple wurde im Oktober 2015 über die Probleme informiert und erbat sich einen – branchenüblichen – Zeitraum von sechs Monaten für die Fehlerbehebung.\n\nDarwin unterscheidet zwischen normalen Benutzern (user), Systemverwaltern (admin) und dem Superuser (root). Einem normalen Benutzer ist es nicht erlaubt, Änderungen am System vorzunehmen oder Software außerhalb seines Benutzerordners zu installieren. Von ihm gestartete Programme werden nur mit seinen Nutzerrechten ausgeführt. Die Benutzer der Gruppe \"admin\" verfügen über weitergehende Rechte, sie dürfen systemweite Einstellungen vornehmen, Software installieren und verfügen über Schreibzugriff auf diverse Systemverzeichnisse. Nur nach gesonderten Authentifizierungen können tiefergreifende Änderungen am System vorgenommen werden. Ein Root-Benutzerkonto, das dauerhaft über Berechtigungen des \"Superusers\" verfügt, ist standardmäßig deaktiviert.\n\nmacOS enthält die von FreeBSD stammende paketorientierte Firewall ipfw, die seit Leopard 10.5 (2007) standardmäßig ungenutzt bleibt. In Leopard wurde zusätzlich eine programmorientierte Firewall eingeführt, in der eingestellt wird, welche Programme eingehenden Datenverkehr empfangen dürfen. Eine grafische Benutzeroberfläche für ipfw muss separat installiert werden (z. B. WaterRoof oder Flying Buttress).\n\nErste Tests zeigten, dass die zusätzliche Firewall von Leopard 10.5.0 selbst dann noch Daten passieren lässt, wenn in den Einstellungen „alle Verbindungen blockieren“ ausgewählt ist. In Version 10.5.1 wurden mehrere dieser Sicherheitslücken geschlossen. Die Formulierung der Benutzeroberfläche wurde angepasst zu „Nur notwendige Dienste erlauben“ und die Anzahl der in diesem Modus noch zugänglichen Dienste reduziert.\n\nAusgehende Verbindungen können mit den von macOS bereitgestellten grafischen Anwendungen nicht weitergehend konfiguriert werden; hierfür werden Zusatzprogramme wie Little Snitch, GlowWorm oder TCPBlock benötigt.\n\nDie \"App Sandbox\" ist ein im Jahr 2011 mit Lion 10.7 eingeführter Sicherheitsmechanismus, der es Angreifern erschweren soll, Sicherheitslücken in Programmen auszunutzen. Dazu erhält jedes Programm nur die absolut notwendigen Rechte, so dass es – falls es von einem Angreifer gekapert wird – möglichst geringen Schaden anrichten kann.\n\nDas Betriebssystem teilt hierfür jedem Programm einen eigenen Bereich der Festplatte, die sogenannte „Sandbox“, zu. Auf die „Sandbox“ fremder Programme oder auf andere Bereiche des Systems kann das Programm nicht zugreifen. Die Öffnen- und Speichern-Dialoge sind folglich nicht mehr Teil des Programmes, sondern laufen in einem eigenen Systemprozess namens \"Powerbox,\" da das Programm selbst keinen Zugriff mehr auf Dokumente auf der Festplatte hat.\n\nAußerdem muss jedes Programm, das Sandboxing unterstützt, eine vom Entwickler erstellte Liste der benötigten Berechtigungen enthalten. (Insgesamt gibt es etwa ein Dutzend Berechtigungen für Funktionen wie den Aufbau von Netzwerkverbindungen oder die Aufnahme von Fotos mit der eingebauten Webcam.) Falls ein Programm versucht, auf eine Funktion zuzugreifen, für die es keine Berechtigung verlangt hat, blockiert macOS dies.\n\nZudem kommt mit XPC eine Variante der Interprozesskommunikation zum Einsatz, die es Entwicklern vereinfacht, einzelne Funktionen des Programms in einen Prozess auszulagern, der über eine eigene Berechtigungsliste verfügt \"(Privilege Separation)\". So kann etwa bei einem Mediaplayer das Rendering von Videodateien ausgelagert werden. Ein Angreifer, der Sicherheitslücken in diesem Bereich ausnutzt, kann dann nur die Berechtigungen dieses Renderingprozesses erhalten, jedoch nicht weitergehende Berechtigungen des Mediaplayers.\n\nGatekeeper ist eine im Jahr 2012 mit Mountain Lion 10.8 eingeführte Funktion, die sicherstellt, dass nur signierte Software auf dem Rechner gestartet werden kann. Dazu wird geprüft, ob die Software mit der Signatur eines bei Apple registrierten Entwicklers versehen ist. Ist dies nicht der Fall, verweigert das Betriebssystem das Ausführen der Software. Es gibt verschiedene Sicherheitsstufen. Entweder sind nur Programme erlaubt, die aus dem „App Store“ geladen wurden. Dann die Standardeinstellung, die vorsieht, dass alle Programme mit einer von Apple beglaubigten Signatur versehen sein müssen, und zuletzt die „schwächste“ Sicherheitsseinstellung, in der wie bisher jegliche Software installiert werden kann. Jedoch besteht für Nutzer, die nicht die schwächeste Sicherheitseinstellung gewählt haben, die Möglichkeit des Startes „unsicherer“ Software über das Terminal oder über das Kontextmenü im Finder. Für letzteres ist jedoch eine Anmeldung als Administrator vonnöten, sonst wird der Nutzer nach einem Admin-Kennwort gefragt.\n\nGatekeeper war schon in Mac OS X Lion ab Version 10.7.3 vorhanden und konnte per Terminal freigeschaltet werden. Ab Version 10.7.5 ist es offiziell Bestandteil des Betriebssystems.\n\nBis 2010 waren nur neun Prototypen viren- oder wurmähnlicher Schadprogramme für Mac OS X bekannt, verbreiten konnte sich allerdings keiner. Dazu gehört der am 13. Februar 2006 im Forum einer US-amerikanischen Gerüchteseite veröffentlichte erste Computerwurm, der jedoch vom Anwender willentlich ausgeführt werden muss, für Tiger 10.4 – damals noch nur für PowerPC. Virenscanner für macOS dienen im Wesentlichen dazu, das Durchreichen von Viren zu verhindern, die für andere Betriebssysteme geschrieben wurden.\n\nVon mehreren bekannten Trojanischen Pferden für Mac OS X galten bis 2010 nur zwei als allgemein bedeutend und nennenswert gefährlich. Sie können sich bei unvorsichtigem Nutzerverhalten installieren, etwa versteckt in illegalen Downloads oder als angeblich fehlender Codec auf Pornoseiten.\nDie bis zum damaligen Zeitpunkt am weitesten verbreitete Schadsoftware für Mac OS X war im Mai und Juni 2011 unter dem Namen \"MAC Defender\" aktiv und tarnte sich als Antivirus-Programm. Als Reaktion darauf baute Apple in Mac OS X eine täglich aktualisierte Liste mit Malware-Definitionen ein.\nSeit Februar 2012 haben sich mehrere Varianten der sogenannten „Flashback“-Malware über präparierte Webseiten durch Lücken in Java auch auf OS X ausgebreitet. Die Anzahl der betroffenen Systeme wurde zur Höchstzeit auf 600.000 geschätzt. Apple hat die Sicherheitslücke im April 2012 mit einer Java-Aktualisierung geschlossen und ein Programm zur Verfügung gestellt, mit dessen Hilfe die Schadsoftware von betroffenen Computern entfernt wird. Nutzern älterer Systeme bleibt nur das Abschalten des Java-Plugins, da keine Updates für Mac OS X, die älter als Leopard 10.5 (2007) sind, angeboten wurden. Im Browser Mozilla Firefox wurde Java für diese Mac-OS-X-Versionen standardmäßig deaktiviert.\n\nDas erste Betriebssystem mit dem Namen „Mac OS X“ war Mac OS X Server 1.0 (1999). Es war praktisch ein Rhapsody und zählt daher technisch nicht zu dieser Betriebssystemlinie, wird aber vollständigkeitshalber hier mit aufgeführt. Ihm fehlte, genau wie Rhapsody, die Kompatibilität zum ursprünglichen Macintosh-Betriebssystem Mac OS („Classic“), was später durch die Entwicklung der Programmierschnittstelle Carbon kompensiert wurde. Die letzte Version war Mac OS X Server 1.2 v3 (2000, intern Rhapsody 5.6). Das nie offiziell veröffentlichte Rhapsody 1.0 (intern Rhapsody 5.2) sowie Mac OS X Server 1.0 (intern Rhapsody 5.3) bilden den Ausgangspunkt für die Entwicklung von Mac OS X 10.0.\n\nWährend Mac OS X Server 1.0 für die Server-Varianten der Power-Mac-G4-Linie optional vorinstalliert wurde, blieb das klassische Mac OS vorerst das Desktop-Betriebssystem für Macs.\n\nMac OS X Public Beta, Codename „Kodiak“:Ab dem 13. September 2000 gab Apple eine Beta-Version von Mac OS X aus und verkaufte sie zum Preis von 29,95 US-Dollar.\nDiese Version und die zuvor an Entwickler abgegebenen ermöglichten Early Adopters einen Einblick in das kommende Betriebssystem und bot Softwareentwicklern die Möglichkeit, eigene Programme für dieses System zu entwickeln, damit diese ihrerseits Programme für die fertige Release-Version von Mac OS X 10.0 zeitnah freigeben konnten.\n\nMac OS X 10.0, Codename „Cheetah“:Die erste Release-Version von Mac OS X erschien am 24. März 2001 und war in vielerlei Hinsicht noch nicht ganz ausgereift. Sie war sehr langsam (auf älteren G3-Systemen bis zur Unbrauchbarkeit), wurde aber wegen ihrer in einem so frühen Stadium hohen Stabilität gelobt. 10.0.4 war die letzte Version (22. Juni 2001).\n\nMac OS X Server 10.0:Mac OS X Server 10.0 wurde am 21. Mai 2001 veröffentlicht – fast zwei Monate nach Mac OS X 10.0. Das Betriebssystem ist im Wesentlichen genau gleich wie die Desktop-Ausgabe, bietet jedoch zusätzlich umfangreiche Server-Software sowie grafische Hilfsprogramme, die die Konfiguration erleichtern.\n\nMac OS X 10.1, Desktop-Ausgabe:Mac OS X 10.1 erschien am 25. September 2001. Es wurde als kostenlose Aktualisierung von Apple bereitgestellt. Die Geschwindigkeit, insbesondere das Ansprechverhalten der Benutzeroberfläche, wurde wesentlich verbessert, und fehlende Features, wie zum Beispiel das Abspielen von DVDs, wurden hinzugefügt. Die letzte Version war Mac OS X 10.1.5 vom 5. Juni 2002.\n\nMac OS X Server 10.1, Server-Ausgabe: Diese Server-Ausgabe erschien gleichzeitig mit der Desktop-Ausgabe.\n\nMac OS X 10.2: Jaguar wurde am 13. August 2002 veröffentlicht. Es beschleunigte mit Quartz Extreme die Benutzeroberfläche auf geeigneten Grafikkarten. Als Drucksystem wurde CUPS eingeführt, was die Verwendung alternativer Druckertreiber ermöglichte. Die letzte Version von Jaguar war Mac OS X 10.2.8 (veröffentlicht am 3. Oktober 2003); danach gab es jedoch noch einige Sicherheitsaktualisierungen. Seit dieser Version sind die Raubkatzenarten nicht nur Code- und Projektnamen, sondern offizielle Produktbezeichnungen, die auf den Verpackungen und Datenträgern aufgedruckt sind.\n\nMac OS X Server 10.2:Diese Server-Ausgabe von Mac OS X 10.2 erschien ca. 1½ Wochen nach der Desktop-Ausgabe am 24. August 2002.\n\nMac OS X Panther 10.3:Panther wurde am 24. Oktober 2003 eingeführt. Es brachte Funktionen wie Exposé, das Video-Chat-Programm iChat AV und die Benutzerverzeichnisverschlüsselung FileVault mit. Auch die neue Programmierschnittstelle Core Audio wurde hinzugefügt. Der Finder wurde überarbeitet und einige Inkonsequenzen und Inkonsistenzen der Vorgängerversion beseitigt. Außerdem erfuhr die Benutzeroberfläche einige Änderungen, das Design wurde insgesamt etwas schlichter. Die Systemgeschwindigkeit wurde weiter gesteigert. Mit dem neu eingeführten schnellen Benutzerwechsel wurde es ermöglicht, zwischen Benutzern hin und her zu schalten, ohne sich abmelden zu müssen. Am 15. April 2005 wurde die letzte Version, Mac OS X 10.3.9, veröffentlicht.\n\nIm Gegensatz zur Version 10.2 (Jaguar) lässt sich Panther (ohne Drittprogramme wie XPostFacto) nicht mehr auf den beigen G3-Power-Macs installieren, sondern nur mehr auf sogenannten „New-World“-Macs (mit anderer Bus-Architektur, an den fest eingebauten USB-Anschlüssen und am einfarbig gehaltenen Apfel als Unternehmenslogo auf dem Gehäuse erkennbar).\n\nMac OS X Server 10.3: Mac OS X Server 10.3 erschien gleichzeitig mit Panther am 24. Oktober 2003.\n\nMac OS X Tiger 10.4: Tiger erschien am 29. April 2005. Unter den Neuerungen befindet sich eine systemweite Metadatensuche namens Spotlight. Mit Dashboard wurde zu Exposé eine neue Komponente hinzugefügt, die kleine Hilfsprogramme, sogenannte Widgets, einblendet.\nAußerdem enthält Tiger eingeschränkte Unterstützung von 64-Bit-Prozessen (auf 64-Bit-Prozessoren) sowie die neuen Programmierschnittstellen Core Image und Core Video zur Auslagerung grafischer Berechnungen an die GPU der Grafikkarte. Offiziell ab der Version 10.4.4 läuft das System auf Intel-Prozessoren mit i386- bzw. IA-32-Befehlssatzarchitektur. Das neu eingeführte Rosetta ermöglicht das Ausführen von PowerPC-Anwendungen auf Intel-CPUs. Systemaktualisierungen werden getrennt als PowerPC- oder Intel-Version angeboten. Tiger ist mit über zwei Jahren die bisher am längsten gewartete Version von Mac OS X. Am 14. November 2007 erschien die letzte Version von Tiger, 10.4.11.\n\nMac OS X Server 10.4: Diese Server-Ausgabe erschien zusammen mit der Desktop-Ausgabe am 29. April 2005. Mac OS X Server 10.4.7 (2006) war die erste Universal-Version von Mac OS X überhaupt.\n\nMac OS X Leopard 10.5:Leopard erschien am 26. Oktober 2007, nachdem das Veröffentlichungsdatum aus Kapazitätsgründen (zur rechtzeitigen Fertigstellung des Mobiltelefons iPhone) um etwa ein halbes Jahr verschoben wurde.\n\nIn Leopard wurde die Nutzeroberfläche des Systems und insbesondere des Finders deutlich überarbeitet; das Konzept der virtuellen Desktops wurde mit \" von einigen Linux-Varianten übernommen. Integriert sind außerdem die Datensicherungssoftware Time Machine, sowie Boot Camp, das auf Intel-basierten Macs die Installation von Windows parallel zu Mac OS X ermöglicht.\nLeopard ermöglicht den 64-Bit-Betrieb bei Applikationen mit grafischer Benutzeroberfläche. Zudem erfüllt es als erstes BSD-Derivat überhaupt die kommerzielle Zertifizierung Single UNIX Specification UNIX 03 der Open Group und darf daher den Markennamen „UNIX“ (in Großbuchstaben oder Kapitälchen) tragen.\n\nDie Classic-Umgebung wurde mit Mac OS X Leopard (10.5) eingestellt, das Ausführen von Mac OS 9 oder älteren Macintosh-Programmen ist nicht mehr möglich.\n\nMac OS X Server 10.5:Gleichzeitig mit Leopard erschien am 26. Oktober 2007 Mac OS X Server 10.5, ebenfalls als Universal-Version.\n\nMac OS X Snow Leopard 10.6:Snow Leopard erschien am 28. August 2009. Wie schon von Apple auf der WWDC vom 9. Juni 2008 angekündigt, enthält diese Betriebssystemversion außer dem neu hinzugekommenen App Store (Voraussetzung für Upgrades des Betriebssystems) hauptsächlich Verbesserungen und nur wenige neue Funktionen. Der Fokus liegt in der verbesserten Ausnutzung der Computerhardware – Rechner mit PowerPC-Prozessoren werden ab dieser Version nicht mehr unterstützt, ein Intel-Prozessor ist also Bedingung. So soll mit \" und der eine signifikante Leistungssteigerung durch die Vereinigung von Mehrkern-Hauptprozessoren mit leistungsfähigen Grafikprozessoren (GPGPU) unter einer zentralen Programmierschnittstelle erreicht werden. Außerdem gibt es eine erweiterte 64-Bit-Unterstützung im Kernel. Der Finder wurde komplett neu geschrieben und ist nun eine 64-Bit-Anwendung. Die meisten vorinstallierten Programme von Apple laufen nun ebenfalls im 64-Bit-Modus, können jedoch bei Bedarf in 32-Bit gestartet werden. QuickTime X unterstützt moderne Multimediacodecs besser; hinzu kommt eine native Unterstützung von Microsoft Exchange 2007 in den Programmen Mail, Adressbuch und iCal. Seit dem 6. Januar 2011 ist die Anwendung für den Zugriff auf den Mac App Store für Snow Leopard verfügbar.\n\nMac OS X Server 10.6:Die letzte separate Server-Ausgabe war Mac OS X Server 10.6, das wie Snow Leopard am 28. August 2009 erschien. Es ist ebenfalls nur noch auf Macs und Xserve mit Intel-Prozessor lauffähig.\n\nLion wurde am 20. Juli 2011 veröffentlicht. Zu den vorgestellten Neuerungen von Lion gehören das Launchpad, eine an iOS angelehnte Übersicht aller installierten Programme; eine verbesserte Handhabung von Programmen im Vollbildmodus; \"\", eine kombinierte Übersicht über Spaces, Exposé, Dashboard und laufende Vollbildprogramme; FileVault2, welches nun auch Full-Disk-Verschlüsselung ermöglicht. Verschiedene Bestandteile früherer Betriebssystemversionen, insbesondere die Mediencenter-Oberfläche Front Row, eine vorinstallierte Java-Laufzeitumgebung und die Emulationssoftware Rosetta werden nicht mehr unterstützt.\n\nLion ist die letzte Version, die noch „Mac OS X“ im Namen trägt und wurde bereits als „OS X Lion“ beworben.\n\nDie separate Server-Ausgabe wurde eingestellt, stattdessen wird fortan Server- und Konfigurationssoftware als ein Programmpaket über den Mac App Store angeboten.\n\nMountain Lion wurde am 16. Februar 2012 vorgestellt und am 25. Juli 2012 zum Preis von 17,99 Euro beziehungsweise 20 Franken veröffentlicht. \"OS X Mountain Lion\" verzichtet explizit auf den Zusatz „Mac“, den vorherige Versionen des Betriebssystems trugen.\n\nZu den Neuerungen zählen vor allem Funktionen, die vom Tochter-Betriebssystem iOS übernommen wurden, beispielsweise Push-Benachrichtigungen samt Mitteilungszentrale, bessere Anbindung an iCloud, die Ausweitung des iMessage-Protokolls auf den Mac, dedizierte Programme für Erinnerungen und Notizen sowie die systemweite Integration sozialer Netzwerke wie Twitter, Facebook, Flickr und Vimeo.\n\nDie Nachfolgeversion von Mountain Lion wurde am 22. Oktober 2013 veröffentlicht. Sie wurde am 10. Juni 2013 von Craig Federighi auf der WWDC als OS X Mavericks vorgestellt, das nach dem Surfspot Mavericks südlich von San Francisco benannt wurde. Es bietet unter anderem neue Funktionen wie Tabs und Tagging im Finder und verbessert den Umgang mit mehreren Bildschirmen. Das Update auf Mavericks ist für Besitzer von Apple-Computern kostenlos.\n\nDie Nachfolgeversion von Mavericks wurde am 16. Oktober 2014 veröffentlicht. Sie wurde am 2. Juni 2014 auf der Worldwide Developers Conference (WWDC) vorgestellt und trägt den Namen OS X Yosemite, nach dem Yosemite-Nationalpark. Zu den Änderungen zählen u. a. eine verbesserte Zusammenarbeit mit iPhone und iPad sowie eine überarbeitete Betriebssystemoberfläche. Erstmals gab es vor der Veröffentlichung ein öffentliches Beta-Programm für die Software. Das Update auf Yosemite ist für Besitzer von Apple-Computern kostenlos.\n\nDiese Version ist der Nachfolger von Yosemite und wurde am 30. September 2015 veröffentlicht. Sie wurde am 8. Juni 2015 auf der Worldwide Developers Conference vorgestellt. Es wurde nach einem Monolithen im Yosemite-Nationalpark benannt. Neben Verbesserungen des Fenstermanagements und neuen Funktionen in mit dem System mitgelieferten Programmen liegt der Fokus auf Stabilitäts- und Leistungsverbesserungen. Die bereits zuvor mit iOS 8 eingeführte 3D-Grafikschnittstelle Metal ist ab El Capitan auch auf Mac-Modellen ab 2012 verfügbar.\n\nDer Nachfolger von El Capitan wurde am 20. September 2016 veröffentlicht. Das System wurde auf der WWDC am 13. Juni 2016 in San Francisco vorgestellt. Die Namensänderung von OS X zu macOS wurde in Anlehnung an die anderen Betriebssysteme des Herstellers, iOS, watchOS und tvOS, gewählt. In dieser Version gab es nur geringfügige technische Änderungen. Apple stellt die Integration des Spracherkennungs- und -steuerungsdienstes Siri sowie die Möglichkeit heraus, sich auf dem Rechner mittels der Apple Watch einzuloggen. Außerdem wurden ein neues Dateisystem, mit dem man USB-Sticks mit einem Passwort sichern kann und eine verbesserte Speicherverwaltung hinzugefügt. In den angelsächsischen Ländern, in China und in Singapur, später auch in der Schweiz, in Frankreich und in Hongkong soll der Online-Bezahldienst Apple Pay über den Webbrowser Safari eingeführt werden.\n\nDer Nachfolger von Sierra wurde am 5. Juni 2017 auf der WWDC vorgestellt. Auch in dieser Version gibt es vor allem Änderungen an Details und weitere Verbesserungen. Safari erhält eine Unterstützung gegen Tracking und verhindert automatisch abgespielte Videos auf Webseiten. Apple Fotos erhält neue Ansichten für importierte Bilder.\n\nDie großen Änderungen betreffen die Einführung des neuen Dateisystems APFS, die neue Videoschnittstelle Metal 2 und H.265 als Videostandard für 4K-Videos.\n\nMojave wurde am 4. Juni 2018 auf der WWDC vorgestellt und ist seit dem 24. September 2018 verfügbar.\n\nAb Lion 10.7 (2011) gibt es keine separate Server-Ausgabe des Betriebssystems mehr. Erweiternde Server- und -Konfigurationssoftware kann nun über den Mac App Store bezogen werden.\n\nApple nennt keine Zeiträume, für die Aktualisierungen () zugesichert werden. Seit 2011 erschienen jährlich Hauptversionen mit größeren Neuerungen und unter neuem Namen. Diese wurden für ein Jahr weiterentwickelt und anschließend für zwei Jahre mit Sicherheitsupdates versorgt, so dass jeweils die drei jüngsten Hauptversionen abgedeckt waren. Bisweilen wurden darüber hinaus Patches für kritische Sicherheitslücken älterer Hauptversionen bereitgestellt.\n\nDie einer Hauptversion beiliegenden Anwendungen werden nach der Einführung einer neuen Hauptversion meist nicht mehr gepflegt, um einen Anreiz zum Aufrüsten () der Plattform zu bieten. Davon gibt es zwei Ausnahmen: Seit der Veröffentlichung von El Capitan 10.11 wird der Webbrowser Safari, beginnend mit Version 9, bei der Aktualisierung auch für die beiden vorhergehenden Systeme angeboten, jedoch nicht mit den im Vergleich zu den früheren Systemen neu eingeführten Merkmalen, die nur auf dem aktuellen Betriebssystem genutzt werden können. Dieses Vorgehen gilt entsprechend auch für Safari 10, das Teil von macOS Sierra ist, und für Safari 11 bei der Veröffentlichung von macOS High Sierra.\n\nDas Betriebssystem macOS und die darunter laufende Cocoa-nutzende Software sind für die Mehrsprachigkeit implementiert, so dass ein Wechsel der Sprache keine Neuinstallation einer Programmversion erfordert. Meldungen von System und Anwendungen werden in der Sprache dargestellt, die in den Systemeinstellungen als bevorzugt konfiguriert ist. Dort können mehrere Sprachen angegeben und priorisiert werden. Anwendungen, die die erste bevorzugte Sprache nicht anbieten, verwenden dann die zweite oder dritte usw.\n\nDa die auszugebenden Texte bei Cocoa-Programmen üblicherweise nicht in deren Binärcode eingebunden werden, sondern sich in separaten Dateien gebündelt innerhalb des Programmpakets ( ) befinden, lassen sich auch nachträglich weitere Sprachen oder Sprachvarianten ergänzen – freie Projekte können so inoffizielle Übersetzungen bieten.\n\nÄhnlich wie für die Lokalisierung von Anwendungen kann auch für die integrierte Rechtschreibprüfung eine bevorzugte Reihenfolge der Sprachen konfiguriert werden. Nicht für alle Sprachen ist auch eine Rechtschreibprüfung vorhanden. Wörterbücher von Drittanbietern können hinzugefügt werden, um weitere Sprachen zu unterstützen.\n\nSiri ist ein Assistent mit Spracherkennung und -ausgabe (Audio-Benutzungsschnittstelle). Auch vor der Einführung von Siri konnte macOS bereits Sprachkommandos erkennen und Texte sprechen, s. VoiceOver. Diese Funktionen dienen auch der Barrierefreiheit.\n\nAllgemein bieten in macOS die englischen Sprachkomponenten den größten Funktionsumfang.\n\nDas Betriebssystem wird mit den Sprachen (teils in länderspezifischen Varianten)\nArabisch, Chinesisch (traditionell und vereinfacht), Deutsch, Dänisch, Englisch, Finnisch, Französisch, Griechisch, Hebräisch, Hindi, Indonesisch, Italienisch, Japanisch, Katalanisch, Koreanisch, Kroatisch, Malaiisch, Niederländisch, Norwegisch (Bokmål), Polnisch, Portugiesisch, Rumänisch, Russisch, Schwedisch, Slowakisch, Spanisch, Thai, Tschechisch, Türkisch, Ukrainisch, Ungarisch und Vietnamesisch\nausgeliefert.\n\nVoiceOver unterstützt neben Deutsch und Englisch 24 weitere Sprachen.\n\n"}
{"id": "2946328", "url": "https://de.wikipedia.org/wiki?curid=2946328", "title": "Homecomputer (Zeitschrift)", "text": "Homecomputer (Zeitschrift)\n\nDie Homecomputer war eine Computerzeitschrift, die von Anfang 1983 bis ca. Mitte 1986 erschien. Die Verlage, die die Zeitschrift veröffentlichten, waren \"Roeske Verlag\" (Eschwege) und der \"Tronic Verlag\". Erster Herausgeber der Zeitschrift war Ralph Roeske. \n\nDie Zeitschrift war überwiegend mit Listings für die damals gängigen Heimcomputer-Systeme gefüllt, so u. a. C64, Atari XL/XE, VC-20, ZX Spectrum, Apple II. Nur wenige Seiten waren mit Nachrichten oder Artikeln gefüllt.\n\nNach der letzten Ausgabe wurde die Homecomputer mit der Zeitschrift \"Computronic\" zusammengelegt.\n\n"}
{"id": "2947862", "url": "https://de.wikipedia.org/wiki?curid=2947862", "title": "Basis 108", "text": "Basis 108\n\nBeim Basis 108 handelte es sich um einen zum Apple II weitgehend kompatiblen Computer (Apple-Clone), der von der Basis Microcomputer GmbH (nicht mehr existent) hergestellt wurde.\n\nDer Basis 108 integrierte viele Funktionen, die bei einem Apple II erst über Erweiterungskarten nachgerüstet werden mussten. So befand sich neben dem 6502-Prozessor auch ein Z80 auf der Hauptplatine, wodurch die Benutzung von CP/M als Betriebssystem möglich war. Der Textmodus konnte wahlweise 40 x 24 oder 80 x 24 Zeichen darstellen, eine serielle und eine parallele Schnittstelle waren ebenfalls vorhanden. Die Farben des niedrig aufgelösten (\"low resolution\") Grafikmodus waren farblich sortiert, was zwar vom Aufbau logischer strukturiert war, aber aufgrund der Inkompatibilität der Farbtabelle zum Apple II den Grafikmodus für Apple II-Programme praktisch unbrauchbar machte. Der Arbeitsspeicher von 64 KB konnte auf 128 KB aufgerüstet werden.\n\nAuffallend beim Basis 108 war sein massives und schweres Metallgehäuse. Im Gegensatz zum Apple II war die Tastatur nicht im Gehäuse mit eingebaut, sondern als externe Tastatur ausgeführt. Dafür waren bis zu zwei Diskettenlaufwerke im Gehäuse integriert.\n\n"}
{"id": "2959365", "url": "https://de.wikipedia.org/wiki?curid=2959365", "title": "Windowstaste", "text": "Windowstaste\n\nDie Windowstaste (offiziell Windows-Logo-Taste) ist eine Taste auf Computertastaturen, die von Microsoft zusammen mit dem Betriebssystem Windows 95 eingeführt wurde. Ihren Namen verdankt sie dem typischerweise auf ihr befindlichen Windows-Logo, das meist einfarbig ausgeführt ist. Häufig ist das Symbol auf einer kreisförmigen, tastbaren Erhöhung der Tastenoberfläche ausgeführt. Bei Verwendung der Tastatur mit Unix-artigen Betriebssystemen (z. B. Linux) wird die Taste auch als Super- oder Meta-Taste bezeichnet.\n\nDie Einführung von Windows 95 war verbunden mit der Umstellung vom 102-Tasten- zum 105-Tasten-Layout. Die Tastaturbelegung wurde dabei um zwei Windowstasten und eine Menü-Taste erweitert. Auf den meisten Tastaturen finden sich die Windowstasten beidseits jeweils zwischen und , jeweils beidseitig neben und auf Höhe der Leertaste. Abweichend vom vollständigen 105-Tasten-Layout existiert auf manchen Notebook-Tastaturen und neueren Tastaturen für Windows Vista nur noch eine Windowstaste, meist zwischen der linken - und -Taste.\n\nDa auch andere Betriebssysteme wie Linux auf PC-kompatibler Hardware eingesetzt werden können, steht auch dort die Windowstaste zur Verfügung. In diesen Fällen ohne Bezug zu Windows wird dafür jedoch alternativ die Bezeichnung „Supertaste“ verwendet. Ursprünglich – viele Jahre vor der Entwicklung des PC und damit der Einführung der eigentlichen Windowstaste – bezog sich der Begriff „Supertaste“ auf eine ähnliche Zusatztaste auf den Tastaturen, die von Tom Knight am MIT Artificial Intelligence Laboratory entwickelt wurden.\n\nUnter Windows, bis Windows 7 und unter Windows 10, bewirkt ein \"einfacher\" Tastendruck das \"Öffnen des Startmenüs\"; ein zweiter Tastendruck schließt dieses Menü wieder und setzt, je nach System, den Eingabefokus des Benutzers wieder auf die zuvor aktive Anwendung zurück. Bei Windows 8 wird durch einen einfachen Druck der Taste die Startseite geöffnet, durch einen weiteren Druck wird zur zuletzt verwendeten App zurückgewechselt.\n\nAuch ohne diese Taste kann ihre Basisfunktion, das \"Öffnen des Startmenüs\", über die Tastenkombination + aufgerufen werden. Für Windows RT oder Windows 10 S muss, im Gegensatz zu allen anderen Windows-NT-Versionen, mindestens eine Windowstaste vorhanden sein.\n\nTasten\"kombinationen\" bewirken verschiedene Aktionen der Benutzeroberfläche:\n\n\nZusätzliche Kombinationen, wenn Microsoft OneNote installiert ist:\n\nSeit Windows Vista sind zusätzlich folgende Tastenkombinationen möglich bzw. geändert:\n\nFür Windows 7 wurden weitere neue Tastenkombinationen eingeführt:\n\nUnter Windows 8 wurden folgende Tastenkombinationen eingeführt oder geändert:\n\nUnter Windows 10 wurden folgende Kombinationen eingeführt oder geändert:\n\nUnter anderen Betriebssystemen funktioniert die Windowstaste beispielsweise als Befehlstaste (Apple Macintosh, für Tasten\"kombinationen\") und als Compose-Taste (eine Tottaste).\n\n\n\n<keybind key=\"W-F4\">\n</keybind>\n\n\nDie internationale Normreihe ISO/IEC 9995 \"Information technology — Keyboard layouts for text and office systems\" erlaubt (erfordert aber nicht) an spezifischen Positionen eine oder insgesamt zwei \"„operating system keys“\" („Betriebssystemtasten“), deren Funktion ausdrücklich dem verwendeten Betriebssystem freigestellt ist. Diese können mit dem dafür seit 2012 in der Norm vorgesehenen Schleifenquadrat (⌘) oder aber ausdrücklich mit einem Logo eines Betriebssystems beschriftet werden. Die deutsche Tastaturnorm DIN 2137 verweist in diesem Zusammenhang auf ISO/IEC 9995. Insoweit ist die Windowstaste als herstellerspezifische Ausprägung der „Betriebssystemtaste“ in ihrer gängigen Anordnung und Ausführung normkonform.\n\nDie Windowstaste wurde mit Erscheinen von Windows 95 und Windows etabliert und war ursprünglich mit dem Windows-3-Logo beschriftet: .\n\nMit dem Erscheinen von Windows XP wurde ein neues Windows-Logo eingeführt, das sich kurz darauf auch auf den neuen Tastaturen wiederfand .\n\nAuf Geräten mit vorinstalliertem Windows 8 und speziell für solche Geräte angebotenen Tastaturen ist ein erneut verändertes Logo zu finden: .\n\nFür Geräte mit macOS ist die Befehlstaste mit unterschiedlichen Layouts des Herstellers Apple bekannt. Diese wird zur Windowstaste wenn auf dem Gerät eine Windowsinstallation genutzt wird.\n\nFür Linux-Nutzer existieren auch Tastaturen, auf deren (als „Supertaste“ bezeichneter) „Windowstaste“ an Stelle des Windows-Logos das Linux-Maskottchen Tux abgebildet ist. Dies stellt jedoch einen rein optischen Unterschied dar. Der Scan-Code und damit die Funktion der Taste im tatsächlich verwendeten Betriebssystem bleiben gleich. Wird unter Linux eine Mac-Tastaturbelegung verwendet, kann hier ebenfalls die „Supertaste“ als Befehlstaste und die Dritt- und Viertbelegung genutzt werden – jedoch weniger konsistent als unter macOS.\n"}
{"id": "2961258", "url": "https://de.wikipedia.org/wiki?curid=2961258", "title": "GPhoto", "text": "GPhoto\n\ngPhoto ist eine freie Software (unter GPL/LGPL) für den Zugriff auf Digitalkameras unter unixoiden Betriebssystemen. Neben dem Auslesen von Daten (Bilder, Videos, sonstige Multimedia etc.) erlaubt es (abhängig von den Fähigkeiten des Gerätes) auch das Hochladen von Dateien und die Steuerung eines Gerätes (Konfiguration, Bilder aufnehmen).\n\nMittlerweile werden über 2.471 Kameramodelle unterstützt, teils über das vollständig unterstützte Picture Transfer Protocol (PTP), teils über Hersteller-eigene Protokolle. Nicht speziell unterstützte Geräte können oft über die \"Mass Storage Class\" (MSC) des USB trotzdem als USB-Massenspeichergerät angesprochen werden; die Unterstützung dafür ist direkt im Linux-Kernel vorhanden. Da das Media Transfer Protocol (MTP) dem PTP sehr ähnlich ist, werden in neueren Versionen auch entsprechende Medienwiedergabegeräte unterstützt.\n\nAb Version 2, die gegenüber ihren Vorgängern eine Neuentwicklung ist, besteht die Software aus verschiedenen Frontends und einer Bibliothek, deren Funktionalität auch in anderen Programmen genutzt werden kann.\n\nKDEs digikam sowie GNOMEs gThumb und F-Spot setzen als graphische Frontends auf den Funktionalitäten von gPhoto2 (bzw. libgphoto2) auf, da libgphoto2 das einzige System für Linux ist, das die Steuerung von PTP-Kameras erlaubt.\n\n"}
{"id": "2963097", "url": "https://de.wikipedia.org/wiki?curid=2963097", "title": "Lissi und der wilde Kaiser", "text": "Lissi und der wilde Kaiser\n\nLissi und der wilde Kaiser ist ein deutscher 3D-Animationsfilm aus dem Jahr 2007, der unter der Regie von Michael Herbig entstand. Er ist eine Parodie auf die von Ernst Marischka in den 1950er Jahren geschaffene \"Sissi\"-Trilogie.\n\nDer deutsche Kinostart für den Film im Verleih von Constantin Film war der 25. Oktober 2007.\n\nDie österreichische Kaiserin Lissi lebt mit ihrem Gatten Kaiser Franz überglücklich auf Schloss Schöngrün. Die harmonische und wohlgeordnete Welt des Paares, die von finanziellem Überfluss und Dekadenz geprägt ist, wird jäh erschüttert, als Lissi vom ruppigen Yeti entführt wird. Der Yeti wurde vom Teufel sowie dessen Echo beauftragt, dies zu tun, da er ansonsten wegen seiner Arroganz und Boshaftigkeit zur Hölle fahren müsste.\n\nUmgehend nimmt Kaiser Franz, begleitet von seinem treuen Feldmarschall und der liebestollen Kaiserinmutter, die Verfolgung auf, die sie bis über die Landesgrenzen hinweg nach Bayern führt. Auch die Kammerjäger Schwaiger und Ignaz nehmen die Verfolgung auf, da sie sich von der Belohnung ein besseres Leben versprechen.\n\nWährend der Entführung bricht Lissi in einen zugefrorenen See ein, wird jedoch vom Yeti gerettet. Nach und nach freundet sie sich mit dem Yeti an. Es gelingt ihr, den Yeti davon zu überzeugen, dass er mit seiner arroganten Art nicht weiterleben dürfe, und er bessert sich daraufhin. Nachdem der Yeti angeschossen worden ist, beschließen sie, vorerst im Schloss Neuzahnstein von König Bussi von Bayern Schutz zu suchen, der unter einem übertriebenen Sauberkeitswahn und Zahnschmerzen leidet. Derweil findet der Feldmarschall eine Flaschenpost, die Lissi zurückgelassen hat. Der eigentliche Inhalt dieser Nachricht wird von Kaiser Franz nicht erkannt, da die entscheidenden Stellen durch das Wasser unleserlich geworden sind. Dadurch sieht es für Franz so aus, als liebe Lissi einen anderen Mann. Franz ist von diesem Brief derart schockiert, dass er unbedingt einen „Bieeep“ möchte – eine nicht näher genannte Kaffee-„Spezialität“. Das Wort selbst wird im Film jedes Mal ausgepiepst. Diese vermutet er, auch auf dem Schloss von König Bussi zu bekommen.\n\nNachdem sich Lissi und der Yeti erfolgreich gegen die Kammerjäger gewehrt haben, erreicht Franz das Schloss und beschuldigt Lissi des Verrats. Daraufhin erscheint der Teufel, um Lissi an sich zu reißen. Dem Yeti gelingt es zusammen mit Franz, den Teufel auszutricksen. Nachdem sie Lissi befreit haben, kommt es zum Happy End. Der Yeti kann die Zahnschmerzen des Königs Bussi lindern, der daraufhin die Kaiserinmutter heiratet. Zwischen Lissi und Franz klärt sich alles auf, und der Yeti, der in den Himalaya gegangen ist, findet endlich einen Freund – Reinhold Messner.\n\nDie Parodie auf die \"Sissi\"-Trilogie war ursprünglich eine Gagserie, die Herbig für seine Comedyserie \"Bullyparade\" kreiert hatte. Dort liefen die Sketche unter dem Namen „Sissi – Wechseljahre einer Kaiserin“. Anders als bei den Sketchen der Fernsehserie durfte der Name \"Sissi\" jedoch nicht verwendet werden und wurde so in \"Lissi\" geändert. Der Regisseur bezeichnete den Film als „einen Kniefall“ vor den Original-Sissi-Filmen der 1950er Jahre, in denen noch Romy Schneider und Karlheinz Böhm das österreichische Kaiserpaar dargestellt hatten.\n\nHerbig entschied sich, das Projekt als Trickfilm umzusetzen. Aus ästhetischer Sicht, so der Regisseur, sei die Realisierung der ehemaligen Sketche als Animationsfilm die richtige Entscheidung gewesen. An dem Film arbeiteten ungefähr 50 Animatoren, die Produktionskosten betrugen zwölf Millionen Euro.\n\nEs tauchen mehrfach Personen und Orte auf, die als Anspielungen auf reale Personen und Orte gesehen werden können. Franz und Lissi leben auf Schloss Schöngrün, was eine Anspielung auf das Schloss Schönbrunn ist. Auf Neuzahnstein lebt König Bussi von Bayern in Anlehnung an Schloss Neuschwanstein und König Ludwig II.\n\nWeiterhin sind vielfältigste Filmzitate zu finden, darunter Anspielungen auf \"Fluch der Karibik\", \"King Kong\" sowie \"Die Mumie\". Ebenso sind einige Szenen verschiedenen Märchen entlehnt, darunter Rapunzel sowie Die drei kleinen Schweinchen. Eines der Fotos an der Wand von Lissis Schlafzimmer zeigt sie und Franz mit dem an den Martpfahl gebundenen Abahachi aus Der Schuh des Manitu, ebenfalls ein Film von Herbig auf Basis der Bullyparade.\n\nRalf Wengenmayr, der unter anderem auch die Musik zu Herbigs Film \"Der Schuh des Manitu\" schrieb, komponierte die Musik zum Film.\nDer offizielle Song zum Film kommt von Max Mutzke und heißt „Denn es bist du“. Die Single erschien am 2. November 2007. Produziert wurde dieser Titel von Stefan Raab, der auch weitere Songs zum Soundtrack beisteuerte.\n\nDie deutsche Erstausstrahlung erfolgte am 7. September 2009 auf ProSieben. In Österreich wurde der Film bereits am 30. August 2009 auf ORF 1 gezeigt.\n\nDer Film erreichte insgesamt rund 2,8 Millionen Kinobesuche, davon 2.273.804 in Deutschland, 477.535 in Österreich und 35.276 in der Schweiz. Verleih in Deutschland war die Constantin Film, in Österreich deren ehemalige Tochtergesellschaft, die Constantin Film-Holding, und in der Schweiz die Rialto Film. Der Film spielte rund $30.000.000 ein.\n\nDie \"Süddeutsche Zeitung\" schrieb, der Film sei in seinen besten Momenten „großes Kino“. Der Rezensent Fritz Göttler führte aus: „Schön genial wird Bullys Film immer dann, wenn er sich ein bisserl gehen lässt, angenehm degeneriert gibt.“ Weiterhin meinte Göttler, der Film erreiche animationstechnisch nicht die Qualität US-amerikanischer Computeranimationsfilme, was Susanne Schmetkamp in der \"Zeit\" ebenfalls anmerkte. Sie kritisierte auch die Entscheidung, den Film als Animation zu konzipieren: „Doch der Charme der Darsteller, wie man ihn aus den Sissi-Sketchen der TV-Comedyshow kennt, lässt sich halt nicht digital animieren. […] Insbesondere aber die urkomische Mimik von Herbig, Tramitz und Kavanian, die schließlich den Erfolg der Vorgängerfilme ausmachten, fehlt. Eine Handvoll Kalauer und Zoten, strukturlos aneinandergereiht und nur zur Hälfte originell, reicht eben nicht aus.“ Der Film wurde generell schwächer bewertet als alle Herbig-Produktionen zuvor, und auch an der Kinokasse blieb der Film weit hinter den Vorgängerwerken zurück.\n\nDas \"Lexikon des Internationalen Films\" urteilt: „Gag- und einfallsreicher Animationsfilm auf technisch hohem Niveau, der den Vergleich mit internationalen Standards nicht zu scheuen braucht. Liebevoll gestaltete Figuren und die sorgfältige Synchronisation lassen freilich nicht übersehen, dass ihm als Parodie auf die ‚Sissi‘-Filme der Biss fehlt, sodass der Film eher als amüsante Hommage bestehen kann.“\n\n\n"}
{"id": "2963509", "url": "https://de.wikipedia.org/wiki?curid=2963509", "title": "Luminance HDR", "text": "Luminance HDR\n\nLuminance HDR – vormals Qtpfsgui – ist eine Grafiksoftware für die Erstellung und Bearbeitung von High-Dynamic-Range-Bildern. Sie ist unter der GPL veröffentlicht und für die Betriebssysteme Linux, Windows, Mac OS X (nur Intel) erhältlich. Luminance HDR unterstützt mehrere \"High-Dynamic-Range (HDR)\"- sowie \"Low-Dynamic-Range (LDR)\"-Dateiformate.\n\nGrundvoraussetzung der HDR-Fotografie sind Digitalbilder unterschiedlicher Belichtung. Diese legt Luminance HDR übereinander und berechnet aus ihnen ein Bild mit hohem Kontrast. Damit dieses auf üblichen Computerbildschirmen angezeigt werden kann, wandelt Luminance HDR diese mit Hilfe verschiedener zur Auswahl stehenden Methoden (Farbabbildung) in ein darstellbares LDR-Bildformat um.\n\nEin allgemeines Problem ist, dass die als Basis dienenden Bilder exakt übereinander ausgerichtet werden müssen. Bei statischen Motiven kann das mit Hilfe eines Stativs oder einer stabilen Ablage der Kamera sichergestellt werden. Soll mit Bilddaten gearbeitet werden, die nicht ganz exakt dasselbe abbilden, so bietet das Programm die Funktion, die Bilder übereinander auszurichten. Eine automatische Ausrichtung ist über das Hugin-Projekt eingebaut. Sollte diese Automatik nicht das gewünschte Ergebnis liefern, kann aber auch manuell nachgebessert werden.\n\nHDR-Bilder sind Bilder mit einem hohen Kontrastumfang und können mit Luminance HDR sowohl erstellt als auch bearbeitet werden. Folgende HDR-Grafikformate werden unterstützt:\n\nLuminance HDR kann aus mehreren LDR-Bildern ein HDR erstellen und ein HDR-Bild in ein LDR farbabbilden. Folgende LDR-Formate werden unterstützt:\n\n\n"}
{"id": "2965786", "url": "https://de.wikipedia.org/wiki?curid=2965786", "title": "Vue", "text": "Vue\n\nVue (aus dem frz. \"vue\", „Blickwinkel“, „Ansicht“ oder „Sehsinn“, im Deutschen ein veraltendes Bildungs-Fremdwort für „(schöne) Aussicht“ (vgl. „Bellevue“), „(schöner) Anblick“) ist der Name einer Software-Produktlinie des US-amerikanischen Softwareherstellers \"E-on software\". Die Produktlinie wird fortlaufend aktualisiert und liegt gegenwärtig (Mai 2013) in der Version 11.5 vor. Die Software ermöglicht es, natürlich wirkende, realistische 3D-Grafiken von Landschaften und Innenräumen zu erzeugen. Die Versionen \"Infinite\" und \"xStream\" bieten hierbei den größten Funktionsumfang, es existieren jedoch auch abgespeckte Versionen (\"Pro Studio, Esprit, Easel\") für Anwender, welche weniger Funktionen benötigen.\n\n\nDas Hauptaugenmerk in Vue liegt bei der Erstellung fotorealistischer Außenaufnahmen. Hierfür werden diverse vordefinierte Pflanzen- und Baummodelle mitgeliefert. Durch die sogenannte \"\"SolidGrowth\"\"-Technologie ist jede erzeugte Pflanze unterschiedlich. Alle Pflanzen lassen sich in Form und Gestalt stark modifizieren. Ferner besteht die Möglichkeit, weitere Modelle in das Programm einzubinden.\n\nDie Versionen \"Infinite\" und \"xStream\" bieten die Möglichkeit, das Programm mit Hilfe von Python-Skripten zu steuern. Jedoch sind so nicht alle Funktionen und Parameter verfügbar. Zwar lassen sich beispielsweise vordefinierte Atmosphärenmodelle laden, jedoch sind diese nicht modifizierbar. Trotz diverser Aufrufe und Wünsche der Benutzer im entwicklereigenen Forum ist unklar, ob dieser Missstand in absehbarer Zukunft behoben wird.\n\n\"Industrial Light & Magic\" verwendete „Vue 5“, um diverse Szenen in \"Pirates of the Caribbean – Fluch der Karibik 2\" mit pflanzlichem Leben zu füllen und Nebel hinzuzufügen.\n\n"}
{"id": "2969374", "url": "https://de.wikipedia.org/wiki?curid=2969374", "title": "Rechentechnik/datenverarbeitung", "text": "Rechentechnik/datenverarbeitung\n\nrechentechnik/datenverarbeitung (kurz \"rd\") war ein deutsches EDV-Magazin, das ursprünglich in der DDR und nach der Wende auch kurzzeitig in der Bundesrepublik Deutschland erschien.\n\nDas Magazin wurde 1964 unter dem Namen \"Rechentechnik, Rationalisierung, Datenverarbeitung in den Finanzorganen\" gegründet und ab 1966 unter dem Namen \"rechentechnik/datenverarbeitung\" verbreitet. \"rd\" erschien im Verlag Die Wirtschaft, Berlin. Die monatlich erscheinenden Ausgaben kosteten anfangs 1,80 MDN, später 4,50 MDN bzw. Mark je Heft; der Sonderpreis für die DDR betrug 3,60 M[DN]. Später fiel dieser Preis auf 3,40 M. Seit den frühen 1970er Jahren erschienen vierteljährlich zudem die sogenannten rd-Beihefte. Aus diesen entwickelte sich ab 1982 eine eigenständige Publikation namens \"edv aspekte\". Der Preis für diese Hefte betrug 5,00 M.\n\nIn den 1970er- und 1980er-Jahren stieg die Zahl der Abnehmer von \"rechentechnik/datenverarbeitung\" stark an. Nach Angaben des ehemaligen Chefredakteurs, erreichte das Magazin den Gipfel seiner Popularität im Januar 1990 mit 20.000 Abonnenten.\nIm Februar 1991 wurde das Magazin vom IDG-Verlag übernommen. Auf Grund der stark geschrumpften Auflage stellte der Verlag das Blatt jedoch zum Jahreswechsel 1991/92 ein. Die Abonnenten wurden in die \"PC-Welt\" übernommen.\n\n"}
{"id": "2976678", "url": "https://de.wikipedia.org/wiki?curid=2976678", "title": "Windows Small Business Server", "text": "Windows Small Business Server\n\nWindows Small Business Server (SBS) war eine Produktreihe von Microsoft. Sie war auf die Bedürfnisse kleiner und mittelständischer Unternehmen ausgerichtet und basierte auf dem jeweils aktuellen Serverprodukt der Windows-NT-Reihe, enthielt aber zahlreiche zusätzliche Programme.\n\nDer Small Business Server entstand ursprünglich aus der \"Small Business Server\"-Variante des Microsoft BackOffice Servers. Nachdem der BackOffice Server im Jahr 2000 eingestellt wurde, führte man die Small Business Server-Variante als eigenständiges Produkt weiter.\n\n2012 gab Microsoft die Einstellung der Produktreihe bekannt.\n\nDer Small Business Server war in zwei Varianten erhältlich: die \"Standard\"-Variante enthielt neben Windows selbst die Produkte Microsoft Exchange Server, Microsoft Windows SharePoint Services, die \"Premium\"-Variante enthielt zusätzlich auch Microsoft SQL Server. Je nach Version waren noch weitere Produkte enthalten.\n\nDa der Small Business Server nur für kleine Unternehmen vorgesehen war, gab es eine Reihe von Einschränkungen. So unterstützte der Small Business Server nur 75 Clientzugriffslizenzen.\n\nWindows Small Business Server 2011 führte noch eine \"Essentials\"-Variante ein. Diese war auf 25 Clientzugriffslizenzen beschränkt und enthielt außer Windows überhaupt keine Software, da sie zur Verwendung in der Cloud bestimmt war.\n\n\n"}
{"id": "2978699", "url": "https://de.wikipedia.org/wiki?curid=2978699", "title": "Appleseed (Film)", "text": "Appleseed (Film)\n\nAppleseed (jap. , \"Appurushīdo\") ist ein Computeranimationsfilm des renommierten japanischen Anime-Regisseurs Shinji Aramaki aus dem Jahr 2004 und basiert auf Motiven des gleichnamigen Mangas von Masamune Shirow.\n\nDie Produktion ist nach dem 1988 entstandenen 70-minütigen Anime-Kinofilm bereits die zweite Inszenierung der gleichnamigen Manga-Serie, diesmal allerdings mit aktueller Animationstechnologie.\n\nDer Film wurde in Deutschland erstmals am 15. April 2005 im Rahmen des Nippon Connection Filmfestivals gezeigt. Am 12. Dezember 2005 auf deutsch von Universum Anime auf DVD veröffentlicht.\n\nIn der Zukunft tobt ein vernichtender Dritter Weltkrieg mit großer Grausamkeit und Brutalität, dem große Landstriche der Erde zum Opfer gefallen sind. Die Nationen der Welt sind untergegangen. In dieser vom langen Krieg gezeichneten und größtenteils verlassenen urbanen Landschaft des Jahres 2131 kämpfen versprengte Truppenmitglieder wie die ehemalige Polizistin Deunan Knute einen fast aussichtslosen Kampf gegen überlegene Maschinen. Da sie seit Monaten ohne Kontakt zu ihrer Kommandozentrale agiert, erfährt sie nichts von einem bereits unterzeichneten Friedensvertrag der kämpfenden Parteien.\n\nEines Tages wird die legendäre Agentin während eines Feuergefechtes von der olympischen Polizeieinheit E S.W.A.T. in der Außenwelt überwältigt und in die futuristische Metropole Olympus gebracht. Olympus ist eine nahezu gewaltfreie Umgebung, die durch die \"Olympus Regular Army\" und ein automatisches Verteidigungssystem geschützt wird. Die Bevölkerung von Olympus besteht etwa zu gleichen Teilen aus Menschen und aus Bioroiden, die zu einem globalen Neuanfang beitragen und für die Erhaltung einer stabilen menschlichen Gesellschaft Sorge tragen wollen. Bioroiden sind gen-veränderte Klone, die sich nicht selbstständig fortpflanzen können. Bei ihnen sind Gefühle wie Hass, Zorn oder Liebe weitgehend unterdrückt. Die Stadt wird von Bioroiden unter der Aufsicht des lernenden Supercomputers GAIA und eines Ältestenrates aus sieben Menschen regiert.\n\nIn dieser elitären Gesellschaft wird Deunan von ihrem verschollenen Freund und Kampfgefährten Briareos Hecatonchires, der einst seinen schweren Verletzungen während eines Kampfeinsatzes erlag und seitdem als Cyborg existiert, begrüßt. Die Bioroidin Hitomi erklärt Deunan, was Olympus ist und versucht, sie für dessen Ziele zu gewinnen. Die friedliche Co-Existenz von Menschen und Bioroiden wird von Anschlägen bedroht und das Gleichgewicht zwischen Menschen und Bioroiden gerät in eine Schieflage. Besonders das von den Menschen kontrollierte Militär um deren obersten Heerführer General Uranus glaubt daran, dass die Bioroiden zu viel Macht in Olympus gewinnen und wollen diese daher beseitigen. Um dieses Ziel zu erreichen und gleichsam die künstlichen Menschen loszuwerden, verüben die regulären militärischen Truppen heimlich einen Anschlag auf Tartarus. Dort wird die jeweils neue Generation Bioroiden geschaffen, aber auch die existierenden Klone müssen sich in bestimmten Zyklen erneuern, da sie ansonsten verfallen. Mit dem Anschlag auf Tartarus sind alle Bioroiden von ihren lebensverlängernden Maßnahmen abgeschnitten und somit in ihrer Weiterexistenz bedroht. Die Militärs wollen außerdem den auf Tartarus eingebauten D-Tank zerstören. Dieser soll ein Virus enthalten, welches die Bioroiden tötet.\n\nDie olympische Premierministerin Athena, eine Bioroidin, der die Spezialkräfte unterstellt sind, schickt Deunan, Briareos und andere Polizisten auf eine gefährliche Mission, um das bislang verloren geglaubte „Appleseed“-Gen zu finden. Mit diesem Gen soll es den Klonen ermöglicht werden, ihre reproduzierenden Fähigkeiten zu reaktivieren und sich so zu retten. Im Zuge ihrer Mission erfährt Deunan, dass sie bereits im Besitz jener Daten ist. Ihre Mutter war die Wissenschaftlerin, welche die Bioroiden konstruierte und gab Deunan die Daten in einem Amulett mit, kurz bevor sie selbst ermordet wurde.\n\nZurück in Olympus entdeckt Deunan, dass der Ältestenrat ebenfalls in den Konflikt involviert ist und den Supercomputer GAIA dazu nutzt, um gegen die Menschen vorzugehen. Der Rat ist zu der Ansicht gelangt, dass die Menschen zu gewalttätig sind, um weiterexistieren zu dürfen und will den Bioroiden das Feld überlassen. Entgegen der allgemeinen Meinung enthält der D-Tank kein Virus gegen die Bioroiden, sondern gegen die Zeugungsfähigkeit der Menschen – die menschliche Spezies soll altern und aussterben. Das Militär kapituliert in letzter Minute, der Rat verführt allerdings GAIA, die mobilen Festungen gegen den D-Tank in Marsch zu setzen, um das Virus freizusetzen. Deunan gelingt es in letzter Sekunde, die Festungen durch Eingabe eines Passwortes in der kontrollierenden Festung zu stoppen. Durch den Abtritt des Ältestenrates müssen Menschen und Bioroiden ihr Schicksal nun selbst in die Hand nehmen. Unklar bleibt, wer das rettende Passwort mit dem letzten fehlenden Buchstaben vervollständigte. Es wird angedeutet, dass dies ein Abschiedsgeschenk des Ältestenrats ist und eine Wegleitung für die Zukunft signalisieren soll.\n\nIn der letzten Einstellung des Films hört man die Stimme Deunan Knutes, die dem Zuschauer ihre pessimistischen Zukunftsaussichten erörtert und für die nächste Generation, die wahre neue Gattung, weiterkämpfen will.\n\n"}
{"id": "2984913", "url": "https://de.wikipedia.org/wiki?curid=2984913", "title": "Browser Helper Object", "text": "Browser Helper Object\n\nBrowser Helper Objects (BHO) sind Computerprogramme, die die Funktionen des Internet Explorers erweitern. BHOs werden ab Version 4 des Internet Explorers eingesetzt. Sie haben direkten Zugriff auf das Document Object Model (DOM).\n\nBHOs werden etwa von Firmen wie Adobe genutzt; die Erweiterung „Adobe Acrobat add-in“ ermöglicht es dem Internet Explorer, PDF-Dateien direkt im Browser darzustellen. Da sie ungehinderten Zugang zu allen Funktionen des Internet Explorer haben, erstellen aber auch einige Schadprogramme eigene BHOs, von denen z. B. ein Teil das Nutzerverhalten im Internet aufzeichnet und an Marketingunternehmen sendet.\n\nAus der Registrierung können BHOs mit dem Programm Regedit entfernt werden. BHOs sind in der Windows-Registrierungsdatenbank unter dem Schlüssel\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Browser Helper Objects\\\neingetragen.\n\nMicrosoft Edge, der Nachfolger des Internet Explorer, unterstützt keine Browser Helper Objects mehr; stattdessen wird ein Erweiterungssystem genutzt.\n"}
{"id": "2989354", "url": "https://de.wikipedia.org/wiki?curid=2989354", "title": "Okular (Software)", "text": "Okular (Software)\n\nOkular ist ein universeller Dokumentenbetrachter. Er wird als freie Software unter der GNU General Public License (GPL) als Teil der KDE Applications 5 verbreitet, baut auf KPDF auf und löst die Einzelprogramme KPDF, KGhostView, KFax, KFaxview, KView und KDVI in KDE SC 4 ab.\n\nOkular wurde beim Google Summer of Code 2005 von Piotr Szymanski ins Leben gerufen. Gegenüber KPDF bietet Okular nun unter anderem auch ein System für Lesezeichen sowie für Anmerkungen, Unterstützung für Formulare und Multimedia-Inhalte, mehrspaltige Anzeige und das Rotieren der Seiten.\n\nAb Version 0.15 werden Anmerkungen im PDF-Dokument gespeichert. Diese Version wurde zusammen mit KDE 4.9 veröffentlicht. Bei vorherigen Versionen wurden Kommentare und Anmerkungen nicht direkt in der Datei gespeichert, sondern in einer externen XML-Datei.\n\nMit der Version 1.0 vom Dezember 2016 wurde die Portierung auf Qt 5 vollzogen. Die Versionsnummerierung sprang in dem Zusammenhang von 0.26 auf 1.0.\n\nDer Programmkern besteht aus einer Bibliothek, welche die Module für die einzelnen Formate abstrahiert. Deren Funktionalität kann dann mit der zugehörigen grafischen Benutzeroberfläche genutzt oder auch einfach in andere Programme eingebunden werden.\n\nEs unterstützt die folgenden Dateiformate:\n\n\n"}
{"id": "2989711", "url": "https://de.wikipedia.org/wiki?curid=2989711", "title": "Takedown (Film)", "text": "Takedown (Film)\n\nTakedown (Alternativtitel: \"Takedown – Sie dachten, Computer wären sicher\"; Originaltitel: \"Takedown\") ist ein US-amerikanischer Thriller aus dem Jahr 2000. Er thematisiert die Jagd auf den Computerhacker Kevin Mitnick, gespielt von Skeet Ulrich.\n\nRegie führte Joe Chappelle, das Drehbuch schrieben John Danza, David Newman, Leslie Newman und Howard A. Rodman anhand des gleichnamigen Buches von Tsutomu Shimomura (jap. ) und John Markoff.\n\nDer Film ist auch unter dem Namen \"Track Down\" bekannt und verbreitete sich als Bootleg unter den Titeln \"Hackers 2 - Operation Takedown\" und \"Hackers 2 - Takedown\".\n\nKevin Mitnick ist ein Computerhacker; er wird bereits von den FBI-Agenten Mitch Gibson und McCoy Rollins verdächtigt. Ein Anfänger in der Hackerszene soll dem FBI Beweise für Mitnicks Festnahme bringen. Stattdessen kommt Mitnick, dank seiner unfreiwilligen Hilfe, an einen Abhördienst für das FBI. Mitnick verstößt so gegen noch ausstehende Bewährungsauflagen.\n\nMitnick sieht im Fernsehen den Computerexperten Tsutomu Shimomura und versucht, an eine von ihm entdeckte und bekanntgemachte Technologie zu kommen, die es ermöglicht, Handys abzuhören. Wie bei seinem vorhergehenden Hack versucht er es mit Social Engineering, was aber bei Shimomura scheitert. Dieser kommentierte den Versuch mit einer abfälligen Bemerkung, worauf sich Mitnick mit einem Hack in seine Computersysteme rächen will. Dabei lädt er den gesamten Festplatteninhalt herunter und löscht ihn anschließend von Shimomuras Computer. Neben dem Handy-Code, hinter dem er her war, entdeckt Mitnick ein Virus, den Shimomura entwickelt hat, um zukünftige Aufträge für seine Firma sicherzustellen. Shimomura verfolgt in Zusammenarbeit mit dem FBI die Spur von Mitnick, der inzwischen untertauchen musste.\n\nAls Mitnick den Code vollständig entschlüsselt hat und im Internet veröffentlichen will, wird er vom FBI verhaftet. Sein Upload ins Internet wurde von Shimomura abgefangen. Angeblich hat er seinen Virus anschließend selbst vernichtet.\n\nChristopher Null schrieb auf www.filmcritic.com, der Film halte sich nicht strikt an den tatsächlichen Verlauf der Ereignisse; die meisten Namen habe man geändert. Skeet Ulrich sei Kevin Mitnick gar nicht ähnlich. Der Film könne allenfalls die Enthusiasten der Computergeschichte – als Kuriosität – interessieren. Der Kritiker fand, das Zuschauen sei Zeitverschwendung.\n\nDas \"Lexikon des internationalen Films\" schrieb, der Film sei ein „\"High-Tech-Thriller nach einer authentischen Geschichte, der seinen negativen Helden als Guerilla-Kämpfer gegen einen allgegenwärtigen Überwachungsstaat vorstellt, dem der Zugang zur wirklichen Welt entgleitet\"“. Die „\"politisch-ethischen Dimensionen der Geschichte\"“ würden „\"angesichts effektvoll inszenierter Hackeraction in den Hintergrund\"“ rücken.\n\nDer Film wurde im August und September 1998 in Wilmington (North Carolina) gedreht. Er wurde in den meisten Ländern direkt auf Video/DVD veröffentlicht.\n\nIn der Szene, in der Tsutomu Shimomura auf der Konferenz über Computermissbrauch und Manipulation eine Rede hält, sieht man den echten Tsutomu Shimomura rechts neben Alex Lowe sitzen.\n\n"}
{"id": "2990464", "url": "https://de.wikipedia.org/wiki?curid=2990464", "title": "Design-Expert", "text": "Design-Expert\n\nDesign-Expert ist eine kommerzielle, Windows-basierte Software des US-amerikanischen Konzerns \"Stat-Ease\" zur Erstellung von statistischen Versuchsplänen (\"\", DoE). Mit den Methoden der statistischen Versuchsplanung unterstützt Design-Expert die Optimierung von Prozessen und Produkten.\n\nDas Arbeiten mit Design-Expert ist nach den Prinzipien eines experimentellen Zyklus aufgebaut. Am Anfang der Arbeit mit Design-Expert wird ein Versuchstyp geplant, der das Screening wichtiger Einflussgrößen in ihrer Wirkung beschreibt. Die Versuchsergebnisse eines solchen Plans können ausgewertet werden. Auf Grundlage der Auswertung werden Optimierungspläne entworfen, durchgeführt und ausgewertet. Mit Hilfe von Design-Expert können über einen Wünschbarkeitsalgorithmus eine oder mehrere Zielgrößen optimiert werden. Das heißt, es können Stellgrößen für Prozessparameter gefunden werden, bei der sich die Zielgrößen bei einem Optimum befinden.\n\nDesign-Expert ist kein allgemeines Statistikprogramm. Mit Design-Expert sind nur statistische Methoden rechenbar, die im direkten Zusammenhang zu statistischen Versuchsplanung stehen. Der Arbeitsablauf der Software sieht die Schritte 1. Planung eines Versuches 2. Auswertung 3. erneute und optimierte Planung, sowie 4. erneute Auswertung vor. Über die Analyse \"historischer Daten\" können zwar auch nicht geplante Versuche ausgewertet werden, dieses ist allerdings weniger komfortabel als in allgemeinen Statistikprogrammen.\n\nDieser Nachteil wird dadurch ausgeglichen, dass das Planen und Auswerten von statistischen Versuchen äußerst fehlersicher und einfach durchführbar ist. Die Arbeitsschritte der Versuchsplanung und Auswertung sind massiv assistentenbasiert.\n\nFolgende Typen von Versuchsplänen können erstellt und ausgewertet werden:\n\nZu weiteren Funktionen der Software zählen\n\nNeben Design-Expert kann eine Statistische Versuchsplanung auch mit Minitab, Modde, Fusion QbD, Statgraphics, JMP, STAVEX oder Visual-XSel durchgeführt werden.\n\n\n"}
{"id": "2992559", "url": "https://de.wikipedia.org/wiki?curid=2992559", "title": "WinZip", "text": "WinZip\n\nWinZip ist ein von WinZip Computing (ehemals \"Nico Mak Computing\") entwickeltes Packprogramm für Windows und macOS. Standardmäßig erstellt WinZip Archive im ZIP-Format; es unterstützt aber auch verschiedene andere Archivformate.\n\nWinZip 1.0, eine Frontend-Version des von Phil Katz geschriebenen Kompressionsprogramms PKZIP, wurde im April 1991 erstmals als Shareware für die grafische Benutzeroberfläche (GUI) von Windows freigegeben. Bereits zuvor, im Januar 1991, hatte die Firma Nico Mak Computing unter dem Namen PMZIP eine GUI-Frontend-Version für den OS/2 Presentation Manager veröffentlicht. Ursprünglich wurde WinZip auf Compuserve veröffentlicht, stand aber bald darauf auch auf großen Online-Diensten wie GEnie, Prodigy und anderen zur Verfügung.\n\nAb 1993 wurde die zum freien Download bereitstehende Software zu den meistverkauften Windows-Sachtiteln wie etwa dem Windows-3.0-Buch „Windows Secrets“ von Brian Livingston auf Begleitdisketten mitgeliefert. 1994 war WinZip bei CompuServe zum offiziell erforderlichen Kompressions-Werkzeug avanciert.\n\nAb der Version 5.0 im Jahr 1993 integrierten die Macher von WinZip den Kompressionscode aus dem Info-ZIP-Projekt, eine Open-Source-Variante des ursprünglichen PKZIP-Tools von Phil Katz, wodurch die Notwendigkeit der PKZIP-Lauffähigkeit abgeschafft wurde.\n\nIm Mai 2006 verkündete der kanadische Technologiekonzern und Softwarehersteller Corel Corporation, der vor allem für seine WordPerfect- und CorelDRAW-Produktlinien bekannt ist, die Übernahme von WinZip Computing.\n\nWinZip für Mac OS X wurde im November 2010 veröffentlicht. Diese Version funktioniert nur mit dem Betriebssystem Mac OS X Snow Leopard 10.6. Seit Februar 2012 ist auch eine Anwendung für Apple iOS unter der Marke WinZip erhältlich.\n\nWinZip ist in Standard- und Professional-Versionen erhältlich. Allerdings hat die Fähigkeit von Windows ME und späteren Versionen von Microsoft Windows, ZIP-Dateien als komprimierte Ordner zu öffnen und zu erstellen, die Notwendigkeit für zusätzliche Packprogramme eingeschränkt.\n\n\n\n\n"}
{"id": "2994115", "url": "https://de.wikipedia.org/wiki?curid=2994115", "title": "Latent Dirichlet Allocation", "text": "Latent Dirichlet Allocation\n\nLatent Dirichlet allocation (LDA) ist ein von David Blei, Andrew Ng und Michael I. Jordan im Jahre 2003 vorgestelltes generatives Wahrscheinlichkeitsmodell für „Dokumente“. Das Modell ist identisch zu einem 2000 publizierten Modell zur Genanalyse von J. K. Pritchard, M. Stephens und P. Donnelly. Dokumente sind in diesem Fall gruppierte, diskrete und ungeordnete Beobachtungen (im Folgenden „Wörter“ genannt). In den meisten Fällen werden Textdokumente verarbeitet, in denen Wörter gruppiert werden, wobei die Wortreihenfolge keine Rolle spielt. Es können aber auch z. B. Pixel aus Bildern verarbeitet werden.\n\nLDA modelliert Dokumente durch einen Prozess:\n\nZunächst wird die Anzahl der Themen formula_1 durch den Benutzer festgelegt. \n\nDie Dokumentensammlung enthält formula_2 unterschiedliche Terme, die das Vokabular bilden. Zunächst werden formula_1 Multinomialverteilungen über alle formula_2 Terme aus Dirichlet-Verteilungen gezogen, diese Verteilungen werden „Themen“ (englisch \"topics\") genannt.\n\nFür jedes Dokument wird eine Verteilung über die formula_1 Themen aus einer Dirichlet-Verteilung gezogen. Ein Dokument enthält also mehrere Themen. Durch eine generierende Dirichlet-Verteilung mit Parametern formula_6 kann die Annahme ausgedrückt werden, dass Dokumente nur wenige Themen enthalten. Diese Annahme ist die einzige Neuerung von LDA im Vergleich zu vorherigen Modellen und hilft bei der Auflösung von Mehrdeutigkeiten (wie etwa beim Wort „Bank“). Die Steigerung der Themen-Qualität durch die angenommene Dirichlet-Verteilung der Themen ist deutlich messbar. \n\nAnschließend wird für jedes Wort aus einem Dokument ein Thema gezogen und aus diesem Thema ein Term.\n\nIn LDA wird jedes Dokument als eine Mischung von verborgenen Themen (engl. \"latent topics\") betrachtet. Jedes Wort im Dokument ist einem Thema zugeordnet. Diese Themen, deren Anzahl zu Beginn festgelegt wird, erklären das gemeinsame Auftreten von Wörtern in Dokumenten. So kommen in Zeitungsartikeln die Wörter „Euro, Bank, Wirtschaft“ oder „Politik, Wahl, Parlament“ jeweils häufig gemeinsam vor; Diese Mengen an Wörtern haben dann jeweils eine hohe Wahrscheinlichkeit in einem Thema. Wörter können auch in mehreren Themen eine hohe Wahrscheinlichkeit haben. \n\nLDA wird u. a. zur Analyse großer Textmengen, zur Textklassifikation, Dimensionsreduzierung oder dem Finden von neuen Inhalten in Textkorpora eingesetzt. Andere Anwendungen finden sich im Bereich der Bioinformatik zur Modellierung von Gensequenzen.\n\n\n\n"}
{"id": "2997037", "url": "https://de.wikipedia.org/wiki?curid=2997037", "title": "Tivoli System Automation", "text": "Tivoli System Automation\n\nTivoli System Automation ist ein Softwareprodukt des Unternehmens IBM zur Automatisierung der Verfügbarkeit von Anwendungen in einem Rechnerverbund (Cluster).\nDiese Software überwacht die Verfügbarkeit von Computer und Anwendungen und kann einen erwünschten Zustand dieser so bezeichneten Ressourcen automatisch sicherstellen.\n\nTivoli System Automation besteht aus einer Software Komponente die in der Regel mehrere Computer zu einem sogenannten Computercluster zusammenfügt. So zusammengefasste Computer werden in der Regel auch als Knoten (Node) bezeichnet. In diesem Rechnerverbund können Anwendungen nun in der Regel auf jedem der zusammengeschalteten Knoten gestartet werden, was der Ausfallsicherheit des Gesamtsystems dient, sollte ein Knoten ausfallen. Die zweite wesentliche Komponente ist der sogenannte „Automation Manager“, welcher die Ressourcen überwacht und Entscheidungen trifft, wie auf Zustandsänderungen zu reagieren ist.\n\nRessourcen werden in sogenannten Richtlinien für den „Automation Manager“ definiert. Sie werden identifiziert durch einen eindeutigen Namen und definieren als Eingabe für Tivoli System Automation wie eine bestimmte Anwendung gestartet, gestoppt und überwacht wird. So einmal definierte Ressourcen werden in ihrer Richtlinie logisch gruppiert und zueinander in Beziehung gesetzt. So wird unter anderem ausgedrückt, dass, bevor eine bestimmte als Ressource abstrahierte Anwendung vom Automation Manager gestartet werden darf, zunächst eine andere Ressource verfügbar sein muss.\n\nDiese Software kann installiert werden auf den folgenden Betriebssystem:\n\n\n"}
{"id": "3003877", "url": "https://de.wikipedia.org/wiki?curid=3003877", "title": "Completely Fair Scheduler", "text": "Completely Fair Scheduler\n\nDer Completely Fair Scheduler (CFS) ist ein Prozess-Scheduler in der Informatik. Solche \"Scheduler\" werden verwendet, um die Priorität von Programmabläufen auf Kernelebene von Betriebssystemen zu verwalten. CFS wurde von Ingo Molnár entwickelt und ersetzte mit der Linux-Kernelversion 2.6.23 im Oktober 2007 seinen zuvor implementierten O(1)-Scheduler.\n\nDer CFS garantiert eine faire Aufteilung der Prozessorzeit. Er verzichtet im Gegensatz zum O(1)-Scheduler auf Heuristiken und\nStatistiken. Im Idealfall läuft beim CFS jeder Prozess quasiparallel in gleicher Geschwindigkeit. Der CFS kennt keine \"Runqueue\", keine \"Timeslices\" und kein \"Array-Switching\", weil es kein \"expired-Array\" gibt. Stattdessen ist jedem Prozess ein \"wait_runtime\"-Wert zugeordnet, der auf Nanosekunden genau bestimmt ist und eine Aussage darüber macht, wie lange der Prozess auf seine Ausführung wartet. Derjenige Prozess mit höchster \"wait_runtime\" wird gewählt. Als Struktur wird dafür ein nach der \"wait_runtime\" sortierter Rot-Schwarz-Baum verwendet.\n\nMit dem 2.6.24er-Kernel unterstützt der CFS Task Groups. Dies bedeutet, dass mehrere Prozesse zu Gruppen zusammengefasst werden und dadurch eine faire Aufteilung zwischen den Gruppen stattfindet. Das kann sinnvoll sein, wenn mehrere User an einem System arbeiten.\n\nFolgendes Szenario verdeutlicht den Sachverhalt: Benutzer A lässt zwei Prozesse laufen, Benutzer B dagegen einen. Alle drei Prozesse haben die gleiche Priorität. Der alte \"O(1)-Scheduler\" würde dem Benutzer A zwei Drittel und Benutzer B ein Drittel der CPU-Laufzeit zuteilen. Der CFS teilt dagegen beiden Usern 50 % der Rechenzeit zu.\n\n\n"}
{"id": "3005126", "url": "https://de.wikipedia.org/wiki?curid=3005126", "title": "Lisp-Maschine", "text": "Lisp-Maschine\n\nEine Lisp-Maschine (kurz oft \"LispM\") ist ein Computer, dessen Prozessor für die Ausführung von LISP-Programmen optimiert ist und dessen Betriebssystem wie auch typische Anwendungen in LISP geschrieben sind. Lisp-Maschinen boten eine komfortable Entwicklungsumgebung. Manche Lisp-Maschinen wurden aber auch ohne Entwicklungsumgebung für die Nutzung von Lisp-Anwendungen eingesetzt. Lisp-Maschinen wurden in den 1970er und 1980er Jahren entworfen, um Aufgaben im Bereich der Künstlichen Intelligenz (KI) besser erledigen zu können. Auch für Animation wurden einige Maschinen benutzt. Die Anzahl der eingesetzten Lisp-Maschinen war sehr gering. Es gibt Schätzungen zwischen 5000 und 7000 Stück.\n\nTrotz dieser geringen Stückzahl wurden einige Konzepte heutiger Computersysteme auf Lisp-Maschinen erprobt und erstmals produktiv eingesetzt, wie farbige Grafik, Fenstersysteme, Computermäuse, Rechnernetze, Hypertext, inkrementelle Kompilierung und das Konzept von Einzelbenutzer-Workstations.\n\nWegen der großen Fortschritte in der Prozessor-Technologie in den 1990er Jahren, die auch ausreichend schnelle allgemeine Mikroprozessoren ermöglichten, und des KI-Winters stürzten die Lisp-Maschinen-Hersteller in eine Krise und die Produktion von Lisp-Maschinen endete.\n\nFür Forschungsprojekte zur Künstlichen Intelligenz wurde während der 1960er und 1970er Jahre meistens die Programmiersprache Lisp verwendet.\nDie verfügbaren Computer waren darauf optimiert, Programme in Sprachen wie Assembler oder Fortran mit möglichst wenig Speicherverbrauch und Rechenzeit zu verarbeiten.\nDie Ausführung von Lisp-Programmen erforderte dagegen für damalige Verhältnisse erhebliche Ressourcen. Ein Grund dafür waren die komplexer werdenden Lisp-Programme und die großen Datenmengen, die in KI-Anwendungen verarbeitet wurden.\n\nLisp verwendet Dynamische Typisierung und dynamische Speicherverwaltung (Garbage Collection). Die Lisp-Maschinen bieten oftmals generische Operationen. Die Maschinen-Operation + akzeptiert zur Laufzeit beliebige Zahltypen. Der Prozessor ermittelt die Typen der Argumente, überprüft diese auf Anwendbarkeit, führt Konvertierungen durch, wählt die passende Addition aus und wendet diese dann an. Dazu wurden die Datenwörter mit Typ-Informationen versehen \"(getagt)\". Die Typ-Überprüfung konnte parallel erfolgen und war wesentlich schneller als eine Software-Implementierung. Typische Wortlängen für Lisp-Maschinen sind 32 Bit (z. B. TI-Explorer-Microprocessor), 36 Bit (Symbolics 3600) oder 40 Bit (Symbolics Ivory). Mit einer Wortlänge von 36 Bit passen auch Daten mit 32 Bit und Tags mit 4 Bit in ein Datenwort.\n\nAußerdem wurde virtueller Speicher eingeführt und die Garbage Collection durch die Hardware unterstützt. In kommerziellen Lisp-Maschinen wurden auch ganze Lisp-Funktionen in Hardware umgesetzt.\n\nUm den Anwendern möglichst viel Rechenleistung zu bieten, wurden Lisp-Maschinen als Einzelplatz-Rechner (mit Unterstützung für Bitmap-Bildschirme, Tastatur, Maus, Netzwerkschnittstelle, Festplatten, Bandlaufwerke und diverse Erweiterungssteckplätze) entworfen. Dies war für die damalige Zeit unüblich, in der Großrechner über Terminals als Mehrbenutzersystem verwendet wurden. Um das gemeinsame Arbeiten von mehreren Benutzern zu ermöglichen, wurden Lisp-Maschinen mit der Fähigkeit entworfen, Rechnernetze zu bilden (zunächst Chaosnet, später auch Ethernet), was für die Zeit ebenfalls unüblich war.\n\nAuch im Bereich Hypertext waren Lisp-Maschinen damals führend. Das Dokumentationssystem des Lisp-Maschinen-Herstellers Symbolics gewann mehrere Auszeichnungen.\n\nAuf Lisp-Maschinen laufen nicht nur Lisp-Programme. Es existieren auch Compiler für beispielsweise C, Pascal, Fortran, Ada und Prolog. Diese Compiler wurden meist auch in Lisp geschrieben und können ebenso interaktiv verwendet werden wie der Lisp-Compiler.\n\n1973 begannen Richard Greenblatt und Tom Knight mit der Entwicklung eines Prototyps für eine Maschine, die Lisp-Code optimiert ausführen sollte. Die erste Maschine, über die Knight seine Masterarbeit schrieb, wurde \"CONS machine\" (nach der Lisp-Funktion codice_1) genannt und 1976 fertiggestellt. Die \"CONS machine\" hatte eine 24-Bit-Architektur und benötigte noch eine PDP-10 zum Betrieb. Nach einer Vorstellung der Maschine 1978 bei einer Konferenz über Künstliche Intelligenz begann die DARPA damit, das Projekt zu finanzieren, und Firmen äußerten ihr Interesse am Erwerb einer Lisp-Maschine. Dies führte zur Entwicklung der \"CADR machine\" (nach der Lisp-Funktion codice_2), von der 25 Stück produziert wurden. Das große Interesse an Lisp-Maschinen führte dazu, dass die Gründung einer Firma für die Vermarktung geplant wurde.\n\nParallel zur Entwicklung am MIT entwickelte BBN Technologies eine eigene Lisp-Maschine \"(Jericho)\", die jedoch nie vermarktet wurde. Das enttäuschte Team wurde dann von Xerox abgeworben und entwickelte am Xerox PARC 1979 eine Lisp-Maschine mit dem Namen \"Dolphin\". Die Xerox-Lisp-Maschinen basierten auf InterLisp, im Gegensatz zu den MIT-Maschinen, die auf Maclisp basierten.\n\n1979 kam es zum Streit zwischen Russell Noftsker und Greenblatt über das Geschäftsmodell der Firma. Noftsker wollte eine traditionelle Firma aufbauen, während Greenblatt vor allem ein mit der Hacker-Ethik des MIT AI Labs zu vereinbarendes Geschäftsmodell anstrebte, das auf Risikokapital verzichten sollte. Da Noftsker, der das AI Lab 1973 verlassen hatte, um in der freien Wirtschaft zu arbeiten, bereits Erfahrungen in der kommerziellen Welt hatte und es auch aus anderen Gründen zu Streit zwischen Greenblatt und einigen Mitarbeitern am AI Lab kam, gelang es Noftsker viele Mitarbeiter, unter anderem Thomas Knight, für seine Pläne zu gewinnen und er gründete Symbolics Inc. Greenblatt blieb zunächst passiv und war sehr verärgert über Noftsker. Control Data Corporation (CDC) zeigte jedoch großes Interesse, eine MIT-CADR-Maschine zu erwerben; Alexander Jacobson, ein Consultant von CDC, brachte daher Greenblatt dazu, endlich eine eigene Firma zu gründen, die Lisp Machines Inc. (LMI). 1980/1981 brachte Symbolics die LM-2 auf den Markt, die eine „neuverpackte“ MIT-CADR-Maschine war. LMI brachte ebenfalls eine MIT-CADR-Maschine heraus, die LMI-CADR-Maschine.\n\nDer Konkurrenzkampf zwischen LMI und Symbolics führte dazu, dass die Mitarbeiter beider Firmen das AI Lab verlassen mussten. Nur Richard Stallman und Marvin Minsky blieben zurück. Außerdem hatten LMI und Symbolics ihre Technik und Software zwar vom MIT lizenziert und räumten dafür dem MIT ein Nutzungsrecht ihrer Veränderungen ein, aber Symbolics verweigerte dem MIT die Änderungen in den ursprünglichen Prototyp und die Software zu integrieren, damit LMI diese nicht nutzen konnte. Dies verärgerte Stallman, der dadurch zum Advokaten freier Software wurde. Stallman nutzte den Zugang am MIT zu den Lisp-Maschinen, um die Änderungen zu rekonstruieren und LMI zur Verfügung zu stellen. LMI hielt jedoch eigene Änderungen an der Lisp-Maschine ebenso verschlossen.\n\nLMI lizenzierte ihre Lisp-Maschinen an die Firma Texas Instruments, die mit Explorer I/II auf der LMI Lambda basierende Maschinen produzierte.\nGegen Ende der 1980er und Anfang der 1990er Jahre brach der ohnehin kleine Markt der Lisp-Maschinen zusammen. LMI war bereits 1986 insolvent und ein Versuch, die Firma als \"GigaMos Systems\" wiederzubeleben, scheiterte an juristischen Problemen des Investors. Xerox hatte schon relativ früh die Entwicklung weiterer Lisp-Maschinen gestoppt.\n\nGründe für den Zusammenbruch gibt es viele. Zum einen war der Markt sehr klein. Spekulationen gehen von zwischen 5000 und 7000 Maschinen aus. Dies sorgte dafür, dass die Hersteller weniger Geld in die technische Weiterentwicklung der Lisp-Maschinen investieren konnten, während die Hersteller herkömmlicher Computer immer bessere Verfahren entwickelten und Lisp-Maschinen bald an Geschwindigkeit einholten und sogar überholten. Firmen wie Lucid Inc. und Franz Inc. begannen, Lisp-Umgebungen für Microcomputer zu verkaufen. Die Portierung der Symbolics-Betriebssystemsoftware Genera von 1992 auf ein DEC Tru64-UNIX/Alpha-System war dreimal so schnell wie die schnellste Lisp-Maschine.\n\nAußerdem erfüllten sich die überzogenen Erwartungen an die Künstliche Intelligenz nicht, weshalb die Gelder für viele KI-Forschungsprojekte gekürzt wurden (der sogenannte KI-Winter). Besonders die massiven Kürzung der Mittel für das SDI-Projekt (auch \"Star Wars\"-Projekt genannt) trafen den Markt hart. Viele KI-Forschungsprojekte (besonders im Bereich Expertensysteme) waren über SDI-Mittel finanziert worden. Damit brach der wichtigste Markt für Lisp-Maschinen ein.\n\n\n\n"}
{"id": "3006335", "url": "https://de.wikipedia.org/wiki?curid=3006335", "title": "Integrated Woz Machine", "text": "Integrated Woz Machine\n\nDie Integrated Woz Machine oder kurz IWM ist eine Einchipversion des Diskettenlaufwerkscontrollers des Apple II, der auch in den Macintosh-Computern verwendet wurde.\n\nBei der Realisierung eines Diskettenlaufwerkes für den Apple II hatte Apple-Mitbegründer Steve Wozniak entschieden, dass die auf dem Markt vorhandenen Lösungen zu kompliziert, zu teuer und ineffektiv sind. Statt Diskettenlaufwerke mit der damals üblichen Shugart-Schnittstelle einzusetzen, verwendete Wozniak nur die Laufwerksmechanik und entwickelte seine eigene Elektronik sowohl für das Laufwerk als auch für den rechnerseitigen Controller.\n\nTatsächlich gelang es ihm, mit deutlich weniger Bauteileaufwand eine leistungsfähigere Lösung zu schaffen. Statt der damals üblichen 8–10 Sektoren à 256 Byte pro Spur schaffte er es, 13 Sektoren à 256 Byte mit der gleichen Mechanik und den gleichen Medien zu realisieren. In einer späteren Version erzielte er sogar 16 Sektoren pro Spur.\n\nZunächst war der Laufwerkscontroller aus mehreren Logikbausteinen und einem PROM aufgebaut. Mit zunehmenden Stückzahlen bei Apple wurde später eine in einem Chip zusammengefasste Version entwickelt, dies war die IWM. Die IWM wurde u. a. im Apple IIgs und allen Mac-Modellen bis zum Macintosh II verwendet. Danach wurde die IWM um die Option erweitert, auch Disketten im FM- und MFM-Format (auch als DOS-Format bezeichnet) lesen und schreiben zu können. Die Bezeichnung des neuen Chips war \"SWIM\". In späteren Mac-Modellen wurde die SWIM dann in größere Peripheriebausteine mit integriert, bis Apple die Diskettenlaufwerke endgültig aus dem Mac verbannte. Die Funktion blieb noch einige Zeit in den Chipsets, auch wenn kein Laufwerk mehr am Mac vorgesehen war, z. B. hatten die ersten iMacs sogar auf dem Mainboard noch einen Stecker für ein Floppylaufwerk vorgesehen, das auch von Bastlern teilweise nachgerüstet wurde.\n"}
{"id": "3007488", "url": "https://de.wikipedia.org/wiki?curid=3007488", "title": "Microsoft Windows Server 2008", "text": "Microsoft Windows Server 2008\n\nMicrosoft Windows Server 2008 ist ein Betriebssystem von Microsoft. Es ist der Nachfolger von Microsoft Windows Server 2003.\n\nAm 27. Februar 2008 veröffentlichte Microsoft das Betriebssystem zusammen mit Microsoft Visual Studio 2008. Zuvor hatte die Software am 4. Februar 2008 den RTM-Status erreicht.\nUrsprünglich war vorgesehen, den Microsoft SQL Server 2008 zum gleichen Zeitpunkt zu veröffentlichen, allerdings wurde dieser Termin verschoben. Im Gegensatz zum Vorgänger Windows Server 2003 besitzt der Windows Server 2008 dieselbe Codebase wie der dazugehörige Client Windows Vista, denn die interne Versionsnummer lautet wie bei Vista NT 6.0. Durch diese enge Verwandtschaft ist es möglich, dieselben Patches wie z. B. Service Packs anzuwenden.\n\nWindows Server 2008 war bis zum 16. Mai 2007 als \"Windows Server, Codename Longhorn\" bekannt, als von Bill Gates auf der WinHEC der endgültige Produktname bekannt gegeben wurde.\n\nWindows Server 2008 ist in folgenden Editionen verfügbar:\n\nAm 12. November 2008 wurden weitere Versionen für kleine und mittelständische Unternehmen freigegeben:\n\n\nZusätzlich kann der Windows Server 2008 als textbasierter \"Core Server\" installiert werden, der nur Datei- oder Infrastrukturserverfunktionen unterstützt (Stand: Mai 2007).\n\nFür die Verwaltung des Server 2008 stehen mehrere Modi zur Verfügung.\nDer Servermanager ist eine Microsoft Management Console in der Version 3.0. Über den Servermanager können zentral die Rollen und Funktionen verwaltet werden. Zusätzlich gibt der Servermanager einen Überblick über den Systemzustand und anfallende Benachrichtigungen der Komponenten durch eine eingebettete Ereignisanzeige. Der Servermanager fasst damit mehrere Verwaltungstools von Server 2003 zusammen. Zusätzlich können Features und Rollen über den Manager installiert und deinstalliert werden.\n\nDer Server kann auch direkt über die MMC und eingebettete Snap-ins verwaltet werden.\n\nEine weitere Möglichkeit bietet die integrierte WMI-Schnittstelle. Der Host stellt Funktionen zur Verwaltung via Skript zur Verfügung.\n\nDer Server lässt sich textbasiert auch über PowerShell administrieren. cmd.exe und Batch wird jedoch weiterhin unterstützt. Über PowerShell lassen sich auch vorhandene Skripte (.bat, vbs, perl) wiederverwenden.\n\nActive Directory wird zum Verwalten von Ressourcen und Benutzern verwendet. Zu den bereits bekannten DCO-Typen wurde ein RODC hinzugefügt. Der RODC ist ein Host mit reduzierter Funktionalität. Dieser Domänencontroller ist schreibgeschützt und kann damit in ungesicherten Bereichen eingesetzt werden, da bei einem Diebstahl nur zwischengespeicherte Passworthashes auf der Hardware zu finden sind. In RODCs werden keine Kennwörter verwaltet. Für schreibende Funktionen muss der Member eine Verbindung zu einem regulären DCO aufbauen. Delegate für die Verwaltung des RODC können an Gruppen und Domänenbenutzer verteilt werden. Damit können auch Benutzer ohne Domänenadministratorrechte den RODC verwalten.\n\nUmfassende Kennwort- und Sperrrichtlinien können innerhalb einer Domäne verteilt werden. Dadurch minimiert sich die Anzahl der benötigten Domänen in einem Netzwerk.\n\nIm Server 2008 sind die Terminaldienste um einige Zusatzfunktionen erweitert:\n\n\nDurch die Funktionalitäten können am Gastrechner lokale und Remoteanwendungen gemischt benutzt werden.\n\nMit Hyper-V stellt Microsoft eine Virtualisierungsumgebung bereit, die es IT-Unternehmen ermöglicht, komplette Serversysteme unter Windows Server 2008 zu virtualisieren. Hyper-V ermöglicht über eigene Verwaltungstools die Organisation sowohl der physischen als auch der virtuellen Ressourcen.\nDie Hyper-V-Umgebung wird mit Windows Server 2008 angeboten. Microsoft plant aber eine separate Hyper-V-Serverversion, die sich leicht in vorhandene IT-Umgebungen einbinden lässt. Auch im Windows Server 2012 findet sich diese Virtualisierungsumgebung, die bei beiden Versionen via Serverrolle im Server Manager installiert und verwaltet wird.\n\nHyper-V isoliert dabei die Gastsysteme komplett vom Hostsystem. Dies setzt allerdings ein virtualisierungsfähiges Mainboard sowie Prozessor voraus. Dem System können dabei zur Laufzeit veränderbare Ressourcen angebunden werden. Damit können Hardwareressourcen wie z. B. Arbeitsspeicher, Prozessoren und Prozessorzeit sowie Netzwerkadapter dynamisch zur Laufzeit an die Anforderungen des Gastsystems angepasst werden.\n\nHyper-V unterstützt alle gängigen Prozessorarchitekturen (x86, x86-64). Ein Gastsystem kann dabei bis zu vier Prozessoren zugewiesen bekommen.\n\nVirtuelle Server unter Hyper-V unterstützen keine exklusive Zuweisung von PCI- und USB-Geräten.\n\nDurch den \"Pass Through\" auf lokale Speicher können auch Hochlastsysteme in einem Gastsystem abgebildet werden.\n\nWindows Server 2008 ist das erste Windows-Betriebssystem, welches mit Microsofts objektorientiertem Kommandozeileninterpreter Windows PowerShell ausgeliefert wird.\nPowerShell basiert auf dem .NET-Framework in der Version 2.0 und stellt eine moderne Alternative zum bekannten Kommandozeilenprogramm cmd.exe sowie dem Windows Script Host dar, welche dem Anwender weiterhin in vollem Umfang zur Verfügung stehen.\nÄhnlich wie in anderen Betriebssystem-Shells, wie beispielsweise Unix-Shells und dem bereits genannten cmd.exe, kann der Benutzer Befehle interaktiv an einer Kommandozeile ausführen und mit Hilfe von Pipes miteinander verknüpfen. Komplexe Shell-Skripte können in der eigens dafür entwickelten \"PowerShell Scripting Language\" geschrieben werden.\n\nDie Datendurchsatzrate konnte unter Windows Server 2008 und Windows Vista durch ein Redesign der Kommunikations-Stacks erhöht werden. Bei Verbindungen zwischen einem Server 2008 und Vista konnte gegenüber Server 2003 und XP SP2 eine Steigerung um den Faktor 3 gemessen werden.\n\nDas Clustermodell von Server 2003 wurde in das Failover Clustering unter Server 2008 überführt. Dabei können Clusternodes in unterschiedlichen Subnetzen liegen. Der Failovercluster unterstützt GPT mit maximal 128 Partitionen und kann dabei 18 Exabyte verwalten. Das Failover-Clustering kann nur in der Enterprise- und DataCenter-Edition verwendet werden. Zusätzlich kann der Cluster über NFS für unixoide Clients genutzt werden. Der Cluster unterstützt das dynamische Hinzufügen von Ressourcen zur Laufzeit.\n\nDie Netzwerk-Lastverteilung verteilt die Last auf mehrere Systeme. Server 2008 unterstützt dabei IPv4 und IPv6.\n\nDer IIS 7.0 dient als Webserver für statische und dynamischen Webseiten, und unterstützt sowohl HTML, SSL, wie auch PHP.\n\nDer integrierte Netzwerkszugriffschutz (\"Network Access Protection\", NAP) bewertet fortlaufend die Zustände der Clients. Dadurch kann ein unsicherer Zugriff oder Zustand des Clients entdeckt werden und das Netz gesichert werden. Die NAP-Richtlinien verhindern zusätzlich, dass sich ein Client, der gegen diese Richtlinien verstößt, mit dem Server verbinden kann. NAP unterstützt dabei IPsec, VPN, DHCP, TS-Gateway und IEEE-802.1X-Verbindungen.\n\nDie integrierte Firewall unterstützt in dem neuen Verwaltungs-Snap-in sowohl die Richtlinien für IPsec als auch die Firewalleinstellungen. Dadurch sind überlappende Einstellungen durch ein gemeinsames Frontend leichter erkennbar.\n\nFür die Verschlüsselung der Daten hat Server 2008 das Tool BitLocker integriert. BitLocker ist für die Sicherheitsvorschriften für Daten in Unternehmen nach Sarbanes-Oxley und HIPAA geeignet. Dabei wird das gesamte System verschlüsselt. Komponenten, die vor dem Betriebssystemkern gestartet werden, sind durch eine Integritätsprüfung gesichert.\n\nServer 2008 unterstützt , ein für Entwickler kryptographischer Anwendungen entwickeltes SDK. Es ist der direkte Nachfolger der CryptoAPI und unterstützt die gängigen Suite-B-Algorithmen sowie ECC.\n\nDer Domänencontroller von Server 2008 implementiert eine weitere Controllerstruktur in Form eines RODC. Der RODC ist ein Read-only Controller der keine Kennwörter abspeichert. Er kann optional durch zusätzliche Rollen verwaltet werden. Dadurch ist ein Einsatz in einem sicherheitskritischen Bereich möglich.\n\nDurch das Isolationsfeature können Domänen und Ressourcen isoliert verwaltet werden. Somit kann ein logisches Netzwerk innerhalb eines physischen Netzwerks komplett isoliert erstellt werden. Dazu sind zwei Isolationsmöglichkeiten implementiert: Die Domänenisolation verhindert den Zugriff von Clients außerhalb der isolierten Domäne, bei der Serverisolation werden, über IPsec gesteuert, nur Zugriffe von registrierten Clients zugelassen.\n\nServer, welche keine grafische Benutzeroberfläche benötigen, können als Core-Server installiert werden. Dabei wird ein Minimalsystem installiert, welches durch eine textbasierte Oberfläche administrierbar ist. Es werden nur benötigte Features eingebunden. Die Installationsoption ist vor allem zur Installation in die Parent Partition eines Hyper-V-Hosts vorgesehen. Ursprünglich wurde kein Managed Code unterstützt, mit dem Service Pack 1 von Server 2008 R2 wurde jedoch das\n.NET Framework 4 für Core zur Funktionalität hinzugefügt. Der Server Core hat keine Upgrade-Möglichkeit.\n\n\n"}
{"id": "3010686", "url": "https://de.wikipedia.org/wiki?curid=3010686", "title": "Nokia PC Suite", "text": "Nokia PC Suite\n\nNokia PC Suite ist ein Softwarepaket zur Verwaltung von Nokia-Handys an Computern mit dem Betriebssystem Microsoft Windows. Der Nachfolger ist die Nokia Suite, die aber nicht mit Lotus Notes synchronisiert.\n\nFolgende Anwendungen sind in der aktuellen Version der PC Suite zu finden:\n\nNokia PC Sync unterstützt folgende E-Mail-Programme:\n\n\n\n"}
{"id": "3012168", "url": "https://de.wikipedia.org/wiki?curid=3012168", "title": "Atari MegaSTE", "text": "Atari MegaSTE\n\nDer Atari MegaSTE ist eine semi-professionelle Variante des Atari 1040 STE und wurde im Jahr 1991 eingeführt, um den technisch überholten MegaST abzulösen und einen Kompromiss zwischen dem günstigen 1040 STE und dem teuren High-End-Modell TT anbieten zu können. Gegenüber dem 1040 STE wurde der MegaSTE in folgenden technischen Eigenschaften weiterentwickelt:\n\n\nWährend der MegaSTE technisch auf dem 1040 STE basierte, wurde das – recht eigenwillige – Gehäuse vom TT übernommen, wie alle ST-Computer \"mausgrau\" gefärbt. Einerseits verfügte der MegaSTE damit über eine exzellente abgesetzte Tastatur, Platz für interne Erweiterungen und konnte problemlos unter dem Monitor platziert werden, andererseits fehlten dem MegaSTE dadurch die erweiterten Joystick-Anschlüsse des 1040 STE. Dafür verfügte er, im Gegensatz zum TT, über einen HF-Modulator zum Anschluss an einen Fernseher.\n\nDie frühen Modelle wurden mit einem DD-Diskettenlaufwerk und TOS 2.05 ausgeliefert, das eine überarbeitete Desktop-Umgebung, Tastaturkürzel, nachladbare Icons und frei platzierbare Verknüpfungen bot. Auf Diskette war außerdem das modulare Kontrollfeld „XControl“ enthalten.\n\nSpätere Modelle enthielten TOS 2.06 und wurden meist vom Händler mit einem HD-Diskettenlaufwerk ausgerüstet. Eine interne DIP-Schalterleiste ermöglichte das Konfigurieren des Rechners, so dass das Betriebssystem im Dialog „Diskette formatieren“ dann auch die „Hohe Speicherdichte“ anbot.\n\nDer Atari-eigene Hostadapter zum Betrieb einer internen Festplatte war primär für die von Atari selbst angebotenen 48-MB- und 80-MB-Laufwerke gedacht und war nicht für den Anschluss von externen Festplatten geeignet, so dass einige Händler recht früh eigene SCSI-Adapter anboten.\n\nAufgrund seiner recht bescheidenen Leistungsdaten – insbesondere im Vergleich zu den Anfang der 1990er Jahre aufkommenden 80386- und 80486-basierten PCs – fiel der Preis des Atari MegaSTE bereits kurz nach dessen Einführung. Kurzzeitig schien er sich im semi-professionellen und privaten Umfeld als günstiges Arbeitsgerät zu etablieren, zum Beispiel zur Textverarbeitung, DTP und, dank den integrierten MIDI-Schnittstellen, Musikerzeugung. Doch mit der Etablierung von Microsoft Windows als De-facto-Standard für Computer verschwand der MegaSTE vom Markt.\n\nVortex bot eine Adapterkarte an, auf der ein 386SX mit lokalem Speicher verlötet war und die in den Prozessorsockel des STE gesteckt wurde. Die 68000 des STE fand wiederum auf der Adapterkarte Platz. Damit ließen sich DOS und Windows nativ betreiben, mit Zugriff auf die Festplatte des STE (bis zu 16 MiB Partitionsgröße.)\n\nMit Hilfe von Emulatoren wie zum Beispiel STEEM Engine oder Hatari kann ein großer Teil der Software, die für den MegaSTE entwickelt wurde, auf heutigen Computern verwendet werden.\n\n"}
{"id": "3015463", "url": "https://de.wikipedia.org/wiki?curid=3015463", "title": "GNU Core Utilities", "text": "GNU Core Utilities\n\nDie GNU Core Utilities (‚Hauptbetriebsmittel‘), kurz coreutils, sind eine vom GNU-Projekt unter der General Public License veröffentlichte Sammlung von grundlegenden Befehlszeilen-Programmen wie beispielsweise das zum Auflisten von Dateien verwendete ls. Die Sammlung stellt die Funktionalität der üblichen Unix-Kommandos zur Verfügung und ist für viele verschiedene Betriebssysteme verfügbar, insbesondere für GNU/Linux, aber auch für Windows mit dem WSL oder Cygwin. Die Befehlszeilenprogramme orientieren sich am POSIX-Standard, sind aber bewusst nicht vollständig konform. Die Programme sind in der Regel bewusst simpel gehalten und beschränken sich auf eine bestimmte Aufgabe. Dadurch können sie einfach in umfangreichere Befehle oder Programme bzw. Skripte eingefügt werden (siehe Unix-Philosophie).\n\nUnterstützt werden viele Hardwareplattformen, beispielsweise x86, SPARC, ARM und PowerPC.\n\nDie Sammlung fasst die ursprünglich einzeln angebotenen Pakete \"textutils\" (für Texte), \"shellutils\" (für die Shell), und \"fileutils\" (für Dateien) in einem zusammen.\n\n\n"}
{"id": "3018360", "url": "https://de.wikipedia.org/wiki?curid=3018360", "title": "Distributed Transaction Coordinator", "text": "Distributed Transaction Coordinator\n\nDer Distributed Transaction Coordinator, kurz DTC, ist ein Systemdienst auf Microsoft Windows NT-basierten Betriebssystemen, der zur Koordination atomarer Operationen beim Einsatz verteilter Anwendungsszenarien verwendet wird.\n\nEine verteilte Transaktion erstreckt sich dabei über mindestens zwei Ressourcen, wie Datenbanken, Nachrichtenwarteschlangen oder Dateisysteme. Wird der Systemdienst beendet, können abhängige Dienste nicht ausgeführt werden und keine Transaktionen über Systemgrenzen hinweg mehr ausgeführt werden. Hierzu gehört beispielsweise der transaktionsgesicherte Aufruf von Prozeduren auf Fremdsystemen (engl. \"Remote Procedure Call\") oder die Nutzung des mit Windows Vista eingeführten Dienstes \"Kernel Transaction Manager\" (KTM) und dem \"Transactional NTFS\" (TxN).\n"}
{"id": "3025176", "url": "https://de.wikipedia.org/wiki?curid=3025176", "title": "Betacode", "text": "Betacode\n\nBetacode ist ein Verfahren, nach dem die in altgriechischen Texten vorkommenden Schriftzeichen, also die Buchstaben des griechischen Alphabets und die diakritischen Zeichen der polytonischen Schreibweise in umkehrbar eindeutiger Weise durch solche Schriftzeichen dargestellt werden, die auf Rechnern sehr weit verbreitet sind. Damit ist eine Basis für den Austausch rechnergestützter altgriechischer Texte geschaffen worden, ohne dass auf den beteiligten Rechnern eine genormte Darstellung der benötigten griechischen Schriftzeichen nötig ist. Nach seiner Entwicklung durch David W. Packard in den 1970er Jahren entwickelte sich Betacode schnell zum allgemeinen Standard. Der Thesaurus Linguae Graecae war und ist eine der wichtigsten Anwendungen.\n\nIn den 1980er Jahren wurde im Zusammenhang mit der Erstellung eines maschinenlesbaren Textes der hebräischen Bibel eine ähnliche Transliteration des hebräischen Alphabets und seiner Diakritika entwickelt, die dann ebenfalls den Namen \"Betacode\" trug; daneben auch Michigan-Claremont nach den Wirkungsstätten der Projektbeteiligten.\n\nZur Zeit der Entwicklung von Betacode wurden Schriftzeichen auf den Systemen der verschiedenen Hersteller in unterschiedlicher Weise codiert. Nach dem Austausch (durch Versand von Datenträgern) von Betacode-Texten wurden diese auf dem Zielrechner in dessen internen Code konvertiert. Die Zeichen des Betacodes sollten also in möglichst vielen solcher internen Codes enthalten sein. Die Unterscheidung von Groß- und Kleinbuchstaben war weder in allen damaligen Systemen noch auf ihren Ein-/Ausgabegeräten (Lochkarten, Zeilendrucker, Bildschirme) überall unterstützt – deswegen wird sie in Betacode nicht benutzt. Später hat sich ASCII als genormter Code zum Datenaustausch durchgesetzt. Heute, wo mit Unicode ein genormter Code auch für die griechische und andere Schriften mit allen ihren Zeichen zur Verfügung steht, wird Betacode noch verwendet, um mit den auf der üblichen Computer-Tastatur vorhandenen Zeichen Texte zu schreiben, die in einem zweiten Schritt durch einen Konverter in Unicode umgewandelt werden können.\nFolgende Tabelle zeigt die Entsprechungen zwischen Betacode und Unicode:\n\nDie Konsonanten werden nach der oberen Tabelle durch lateinische Großbuchstaben und einige weitere Schriftzeichen dargestellt. Dabei spielt es keine Rolle, ob sie konsonantischen Lautwert haben oder als matres lectionis auftreten. Das Dagesch wird unabhängig von seiner Funktion als Punkt dargestellt wie an einem Beispiel gezeigt. Die Darstellung der Diakritika für die Vokale stehen in der Tabelle auf der linken Seite, deren unterste Zeile häufige Kombinationen von Vokalzeichen und \"mater lectionis\" enthält. Andere Diakritika, insbesondere die Teamim, werden jeweils durch ein Paar von Dezimalziffern dargestellt, so wie in der oberen Tabelle Paseq und Sof pasuq (das zweite und dritte Schriftzeichen). Ein und zwei Sternchen bezeichnen Ketib und Qere (im Original Fuß- oder Randnoten), das Fragezeichen eine neue Zeile in einer Originalhandschrift und der Schrägstrich die Trennstelle zwischen Wortwurzel und Affix (im Original nicht vorhanden). Weitere Codezeichen findet man in der Dokumentation (siehe Weblinks).\nDie Bibelstelle im Beispiel ist Kohelet 12,12.\n\ncodice_1\n\nwejoter mehemma beni hisaher asot sefarim harbe en kez welahag harbe jegiat basar\n\nIm Übrigen, mein Sohn, lass dich warnen! Es nimmt kein Ende mit dem vielen Bücherschreiben und viel Studieren ermüdet den Leib.\n\nAm rechten Rand (mittlere Tabellenspalte) ist derselbe Vers aus dem parallelen Text der hebräischen Bibel und der griechischen Septuaginta dargestellt. Nicht immer gestattet die Wortstellung in den beiden Sprachen wie hier eine solche wortweise Nebeneinanderstellung.\n\n"}
{"id": "3025897", "url": "https://de.wikipedia.org/wiki?curid=3025897", "title": "Höchstleistungsrechenzentrum Stuttgart", "text": "Höchstleistungsrechenzentrum Stuttgart\n\nDas Höchstleistungsrechenzentrum Stuttgart (HLRS) ist ein Rechenzentrum, das Wissenschaft und Industrie Zugang zu Supercomputern bietet. Es wurde 1995 unter dem Dach des Rechenzentrums der Universität Stuttgart (RUS) gegründet und ist seit dem Jahr 1996 das erste deutsche Bundeshöchstleistungsrechenzentrum. Das HLRS ist seit 2003 eine eigenständige zentrale Einrichtung der Universität Stuttgart. Es beherbergte einen der schnellsten Supercomputer Europas (Stand 11/2017), ein Cray XC40-System mit einer Rechenleistung von 5,6 Peta-FLOPS. Das HLRS betreibt Rechner, betreut deutsche und europäische Benutzer und bietet ein intensives Schulungsprogramm für seine Nutzer an. Ab 2019 wird der neue Supercomputer \"Hawk\" mit einer Rechenleistung von bis zu 24 Petaflops in Betrieb gehen. Die Kosten für einen der dann schnellsten Supercomputer Deutschlands liegen bei etwa 38 Millionen Euro.\n\nDarüber hinaus betreibt das HLRS eigene Forschung auf dem Gebiet des Höchstleistungsrechnens. Schwerpunkte liegen auf den Themen der Skalierbarkeit, der Leistungsoptimierung, des Big Data, des Green IT, in den Anwendungsbereichen Gesundheit, Umwelt, Energie und Mobilität, sowie in der Untersuchung sozialer, politischer und philosophischer Aspekte der Simulation. Für seine Forschungsarbeiten wurde das HLRS mehrfach international ausgezeichnet. Das HLRS macht seine Rechner und Services seit 1995 über eine Public-Private-Partnership mit Porsche und T-Systems auch der Industrie zugänglich. Mit der Gründung des Automotive Simulation Center Stuttgart (ASCS) hat das HLRS eine Entwicklungs- und Forschungsplattform mit der Automobilindustrie geschaffen, auf der vorwettbewerbliche Forschung in enger Abstimmung von Automobilherstellern, Zulieferern, Rechnerherstellern und Softwarefirmen zum optimale Einsatz in der Verbesserung der Mobilität beiträgt.\n\nSeit Oktober 2015 ist das Kernsystem des HLRS ein Cray XC40 System mit einer Maximalleistung von rund 5,6 Peta-FLOPs (Peak Performance 7,4 Peta-FLOPs). Der Supercomputer ist (Stand November 2017) der drittschnellste Supercomputer im Gebiet der EU und auf Platz 19 der TOP500.\n\nWeitere Systeme sind der NEC Cluster (Vulcan und Vulcan2). Darüber hinaus wird eine Vielzahl kleinerer Systeme betrieben, die zu Testzwecken (NEC SX-ACE) oder z. B. für die Entwicklung und Visualisierung eingesetzt werden. Zur dreidimensionalen Darstellung von Simulationsergebnissen steht eine Cave mit fünf Projektionsflächen zur Verfügung.\n\nDas Höchstleistungsrechnen in Stuttgart geht auf eine lange Tradition zurück, die in den Bereichen der Luft- und Raumfahrt (John Argyris) sowie der Nukleartechnik (Roland Rühle) ihre Ursprünge hat. Die Eingliederung dieser Aktivitäten in eine zentrale Einrichtung geht auf die Anschaffung des damals leistungsstärksten Supercomputers Cray-2, einem Vektorrechner, durch Lothar Späth im Jahre 1986 zurück.\n\nBei der Gründung des HLRS 1996 verfügte das HLRS mit einer NEC SX-4, ebenfalls ein Vektorrechner, mit einer Spitzenleistung von 64 Gigaflops und einer Cray T3E/512 mit 461 Gigaflops theoretischer Rechenleistung über zwei unterschiedliche Systeme die gleichermaßen weltweit zur Spitze zählten. Erster Direktor des HLRS war Roland Rühle, der das HLRS als Bereich des Rechenzentrums der Universität Stuttgart (RUS) etablierte. Zuständiger Bereichsleiter – und damit erster Leiter des HLRS – war Alfred Geiger. Im Jahr 1999 wurde das HLRS von der amerikanischen National Science Foundation (NSF) für seine Arbeiten auf dem Gebiet des verteilten Höchstleistungsrechnens ausgezeichnet. Im Jahr 2000 wurden eine NEC SX-5/32M2 und eine Hitachi SR-8000 beschafft, die eine theoretische Rechenleistung von jeweils 128 Gigaflops aufwiesen.\n\nIm Jahr 2002 wurde das HLRS nach der Pensionierung von Roland Rühle als eigenständige Einrichtung vom Rechenzentrum der Universität Stuttgart abgetrennt. Die Leitung übernahm Michael M. Resch. Im Jahr 2003 gewann das HLRS die HPC Challenge in den USA. Die 2003 beschaffte NEC SX-6 (sechs Knoten, 440 GigaFlops Peak Performance) war erneut ein Vektorrechner. Im selben Jahr wurden auch ein AMD Opteron-Cluster und ein Intel Xeon-Cluster beschafft. Die Installation des Vektorrechners NEC SX-8 (12,8 TeraFlops Peak) erfolgte im Jahr 2005, wofür auch ein neues Gebäude errichtet wurde. 2007 war das HLRS Gründungsmitglied des deutschen Gauss Centre for Supercomputing (GCS), in dem die drei deutschen Bundeshöchstleistungsrechenzentren zusammengeschlossen sind. 2010 erfolgte der Neubau eines Energieversorgungsgebäudes. 2011 wurde mit der Cray XE6 der erste Rechner mit einer Leistung von mehr als einem Peta-FLOPs installiert, der im Dezember 2014 durch das Cray XC40-System \"Hornet\" ersetzt wurde (Peak Performance: 3,8 Peta-FLOPs). Seit 2012 versorgt das HLRS auch europäische Benutzer im Rahmen von PRACE mit Rechenzeit. Im Oktober 2012 hat das HLRS ein neues Forschungsgebäude in Betrieb genommen, in dem auch eine neue 5-Seiten CAVE installiert wurde.\n\nOrganisatorisch war das HLRS innerhalb der Universität Stuttgart von 2012 bis 2016 Mitglied eines Verbundes von drei Zentren, die im \"Informations- und Kommunikationszentrum der Universität Stuttgart\" (IZUS) zusammengefasst wurden.\n\nDie Großrechner werden vom HLRS betrieben und sind für unterschiedliche Benutzer zugänglich.\n\nWissenschaft: Die Vergabe von Rechenzeit erfolgt über einen wissenschaftlichen Lenkungsausschuss (LA). Dessen Mitglieder werden je zur Hälfte von der Deutschen Forschungsgemeinschaft (DFG) sowie der baden-württembergischen Landesrektorenkonferenz (LRK) nominiert. Anträge werden am HLRS eingereicht und werden von dort an den LA weitergeleitet. Im Rahmen der gesamtdeutschen Kooperation im Gauss Centre for Supercomputing (GCS) werden sehr große Anträge von einem gesamtdeutschen Ausschuss begutachtet, der sich aus den Ausschüssen der drei Mitgliedszentren des GCS zusammensetzt. Herausragende Projekte können über die europäische HPC Partnerschaft PRACE Zugang erhalten. Auch auf diesem Weg erfolgt eine Begutachtung durch ein wissenschaftliches Gremium.\n\nIndustrie: Die Rechner des HLRS werden über die \"Höchstleistungsrechner für Wissenschaft und Wirtschaft\" GmbH (hww) auch an die Industrie vermietet. Neben dem HLRS, das die Rechner betreibt, sowie dem KIT und dem Land Baden-Württemberg, sind T-Systems, T-Systems Solutions for Research GmbH und Porsche weitere Gesellschafter. Reiner Produktionsbetrieb wird über die hww angeboten und vertrieben.\n\nKommerzielle Kooperation / Test: Für kommerzielle Nutzung im Rahmen einer Kooperation oder für Projekte, in deren Rahmen Firmen Zugang zum Rechner testen wollen, kann im Rahmen einer Kooperation ein Zugang erfolgen. In diesem Fall sind die Kosten einer Industrienutzung zu tragen.\n\nDie Nutzung der Rechner für militärische Zwecke ist grundsätzlich ausgeschlossen. Ebenso ausgeschlossen ist die Nutzung von Rechnern für Angelegenheiten die vom deutschen Außenhandelsgesetz und den entsprechenden Embargobestimmungen betroffen sind (Nukleartechnik, Raketentechnik, … für boykottierte Staaten).\n\nDas HLRS betreibt eigene Forschung auf dem Gebiet des Höchstleistungsrechnens. Es wurde 1999 und 2003 für seine Forschungsarbeiten in den USA ausgezeichnet. In rund 35 Projekten arbeiten ca. 60 Wissenschaftler an Problemstellungen aus dem Bereich der Simulation und des Höchstleistungsrechnens. Die wichtigsten Forschungsthemen des HLRS sind\n\nDerzeit ist das HLRS im Rahmen der Exzellenzinitiative des Bundes im Exzellenzcluster „Simulation Technology“ engagiert. Der Direktor des HLRS ist Principal Investigator im Exzellenzcluster. Darüber hinaus ist das HLRS als Projektpartner im Sonderforschungsbereich 716 „Dynamische Simulation von Systemen mit großen Teilchenzahlen“ beteiligt. In Europa ist HLRS ein führender Partner in einer Reihe von Projekten zur Entwicklung neuer Hardware und Softwaretechnologien für Clouds und Höchstleistungsrechner.\n\n\n\nDie auf den Rechnern des HLRS erzielten Forschungsergebnisse aus den unterschiedlichsten Fachgebieten werden jährlich als Buch in der Reihe \"High Performance Computing in Science and Engineering\" im Springer-Verlag veröffentlicht. \n\nÜber die Arbeiten im Rahmen des Teraflop-Workbench bzw. des Workshop on Sustained Simulation Performance wird in der Buchreihe \"High-Performance Computing on Vector Systems\" (2005–2011) bzw. \"Sustained Simulation Performance\" (seit 2012) berichtet, die ebenfalls beim Springer-Verlag verlegt wird.\n\nÜber den gemeinsam mit dem ZIH der TU Dresden durchgeführten Workshop zu Tools in High Performance Computing erscheint ebenfalls im Springerverlag seit 2008 die Reihe \"Tools for High Performance Computing\".\n\nDas HLRS veröffentlicht halbjährlich die Zeitschrift des Gauss Center for Supercomputing namens \"Inside\" (Innovatives Supercomputing in Deutschland), die auch als Onlineausgabe angeboten wird.\n\n"}
{"id": "3026983", "url": "https://de.wikipedia.org/wiki?curid=3026983", "title": "Boot Camp (Software)", "text": "Boot Camp (Software)\n\nBoot Camp ist eine Software von Apple zur Installation von Windows neben macOS auf einem Mac mit Intel-Prozessor.\n\nDer Computer wird so zu einem Dual-Boot-System. Vor Boot Camp war für die Nutzung von Windows auf Macs eine virtuelle Maschine notwendig. Entbehrlich wurde sie durch einen hybriden Modus der Firmware und der Partitionstabelle. Boot Camp unterstützt den Nutzer bei der Partitionierung der Festplatte und bringt die Gerätetreiber für Windows mit.\n\nBoot Camp wird als strategisch wichtiger Schachzug von Apple gesehen, um mehr Kunden zu gewinnen, da diese nun das eigene macOS und Windows wechselweise auf demselben Rechner ohne Geschwindigkeitsverluste betreiben können.\n\nDas Hochfahren von macOS erfolgt mittels Extensible Firmware Interface (EFI) und GUID Partition Table. Um Kompatibilität zu älteren Windows-Versionen herzustellen, ergänzte Apple sein EFI um eine Emulation eines BIOS, das mit einer zweiten Partitionstabelle (Master Boot Record) in der ersten arbeitet.\n\nDer Boot-Camp-Assistent hilft bei der Installation von Windows. Voraussetzung ist, dass macOS auf nur einer einzigen Partition installiert wurde. Der Assistent kann die existierende macOS-Partition (formatiert mit dem Dateisystem HFS+, angezeigt als „Mac OS Extended“) verkleinern und den frei gewordenen Bereich mit einer zusätzlichen, mit dem Dateisystem FAT32 formatierten Partition belegen. Diese erhält die Bezeichnung codice_1 und muss für die Installation von Windows auf der Festplatte als Installationsziel ausgewählt werden, wobei sie als NTFS neu formatiert werden muss. Nach der Installation kann der Nutzer beim Neustart des Rechners durch Drücken der Wahltaste zwischen beiden Betriebssystemen wechseln oder die mitgelieferte, ebenfalls „Boot Camp“ genannte Software unter Windows bzw. den Punkt „Startvolume“ in den Systemeinstellungen von macOS zum Festlegen der Startpartition nutzen.\n\nTeile der Boot-Camp-Software können auch auf Nicht-Apple-PCs installiert werden. Damit sind in Verbindung mit der Apple-Tastatur einige spezielle Apple-Funktionen möglich, etwa Steuerung der Lautstärke, des Media-Players, Ausfahren der Laufwerkschublade usw.\n\nEs ist mit Boot Camp nicht möglich, mehr als zwei Betriebssysteme zu installieren. Das liegt daran, dass Windows auf dem Mac durch Verwendung des im EFI enthaltenen (CSM) gestartet wird. Das CSM stellt BIOS-Funktionen zur Verfügung, sodass auch Windows wie auf einem PC mit BIOS gestartet wird und daher einen (MBR) erwartet. Der Boot-Camp-Assistent legt dafür mithilfe von codice_2 von Mac OS X / OS X / macOS einen Hybrid-MBR an, welcher bedingt durch dessen Aufbau auf vier Partitionen beschränkt ist. Zählt man die (ESP) und die macOS-Wiederherstellungspartition dazu, bleiben lediglich je eine Partition für die Betriebssysteme macOS und Windows.\n\nZum Wechsel zwischen den installierten Betriebssystemen muss der Computer bei Nutzung von Boot Camp neu gestartet werden.\n\nMan kann andere Betriebssysteme auch ohne \"Boot Camp\"-Hilfe installieren und sogar macOS entfernen, was jedoch Systemkenntnisse erfordert. Dazu muss ein alternativer Bootlader, wie z. B. \"rEFIt\" oder \"rEFInd\" installiert werden.\n\nEine weitere Möglichkeit, andere Betriebssysteme auf einem Intel-Mac zu nutzen, bietet sich durch Virtualisierungs-Software. Diese hat gegenüber Boot Camp den Vorteil, dass zwei Betriebssysteme zugleich laufen können, ohne dass ein Neustart erforderlich ist, allerdings müssen sich die Betriebssysteme Ressourcen wie Rechenleistung und Arbeitsspeicher teilen. Bekannte Virtualisierungs-Lösungen sind Parallels Desktop, VMWare Fusion oder VirtualBox. Mit Virtual PC war es möglich, Windows-Betriebssysteme auf PowerPC-Macs zu benutzen; auf Macs mit Intel-Prozessoren läuft dieses Programm jedoch nicht mehr.\n\nAußerdem ist es mit Wine und dessen Forks wie CrossOver möglich, Windows-Programme auf anderen Betriebssystemen auszuführen, ohne Windows zu installieren. Dazu wurden große Teile der Windows-API nachprogrammiert. Allerdings ist dies bisher nicht vollständig geschehen. Einige Multimedia-Funktionen werden noch nicht unterstützt, sodass solche Programme nicht verwendet werden können.\n\n\nBoot Camp war seit April 2006 als zeitlich begrenzte Beta-Version erhältlich; mit dem Erscheinen von Mac OS X Leopard am 26. Oktober 2007 können diese nicht mehr zur Neueinrichtung von Windows-Partitionen genutzt werden. Bestehende Windows-Installationen funktionieren jedoch problemlos weiter. Boot Camp ist seitdem Teil des Betriebssystems macOS und nicht mehr separat erhältlich.\n\nAb August 2010 war Bootcamp in Mac OS X Server nicht mehr enthalten.\n\n"}
{"id": "3027198", "url": "https://de.wikipedia.org/wiki?curid=3027198", "title": "KDE Education Project", "text": "KDE Education Project\n\nDas KDE Education Project (KDE-Bildungsprojekt) stellt freie Software für den Bildungs- und Erziehungssektor bereit. Das Spektrum der Anwendungen reicht von spielerischen Lernprogrammen bis zu ausgebauten Wissenschaftsprogrammen. Diese umfassen Themenbereiche wie Astronomie, Chemie, Geographie, Mathematik und Linguistik. Das Projekt setzt auf die Desktop-Umgebung KDE. Die Programme laufen auf allen gängigen Betriebssystemen, wie z. B. Linux, Mac OS X und MS Windows.\n\n\n\n\n\nBereits in das Projekt aufgenommen, aber noch nicht fertiggestellt sind folgende Programme:\n\n\n"}
{"id": "3028992", "url": "https://de.wikipedia.org/wiki?curid=3028992", "title": "GOS (Betriebssystem)", "text": "GOS (Betriebssystem)\n\ngOS (\"good Operating System\"; deutsch \"gutes Betriebssystem\") war eine Linux-Distribution, das von gOS LLC in Los Angeles entwickelt wurde. Veröffentlicht wurde es erstmals am 1. November 2007 mit dem Verkauf des Everex Green gPC TC2502, eines PCs, der in den USA bei Walmart für 199 $ erhältlich war. Sämtliche gPCs waren innerhalb von zwei Wochen ausverkauft.\n\ngOS basiert auf Ubuntu und ist mit einigen Hyperlinks auf Google-Anwendungen ausgerichtet. Es ist jedoch kein Google-Betriebssystem, wie oft zu lesen ist. Everex, der Hersteller des erwähnten gOS PC, sagt hierzu eindeutig: \"\"Everex makes it easy for everyone with the gPC (Everex and its products, the gPC and gOS, are not affiliated with or sponsored by Google Inc.)\"\". Die Paketverwaltung sowie die Hardware-Erkennung wurden von Ubuntu übernommen. Wie auch Ubuntu, lässt sich das System mittels eines Live-Systems austesten, ohne ein installiertes System zu verändern. Das dazu nötige Medium-Image lässt sich von der Herstellerseite herunterladen.\n\nVon gOS gibt es verschiedene Editionen. Universell besitzt gOS ein macOS-ähnliches Dock, mit dem sich Gmail, Wikipedia, Google Drive und lokale Programme wie Skype und GIMP aufrufen lassen.\n\nDie Version 3.x unterstützt durch die Wine-Technologie auch Windows-Programme und kommt mit Mozilla Firefox 3, Skype sowie OpenOffice.org.\n\ngOS Rocket G nutzt die Desktop-Umgebung Gnome und im Gegensatz zur Erstversion ist diese \"Rocket\" (zu Deutsch \"Rakete\") getaufte Version nicht mehr Everex-gelabelt und somit nicht mehr für den gPC zugeschnitten, sondern für das breite Publikum geeignet. gOS Rocket G ist auf DVD erhältlich.\n\nArbeitet mit Enlightenment als Desktop-Umgebung. Da diese schlanker ist als GNOME, empfiehlt sich diese Edition vor allem für leistungsschwächere PCs. Enlightenment 17 befand sich zum Zeitpunkt der Veröffentlichung jedoch noch in der Entwicklung, weshalb noch Fehler auftreten können. Da Enlightenment sehr klein ist, ist gOS Rocket E auf CD erhältlich.\n\nDiese Edition ist speziell für Nutzer des weit verbreiteten Sozialen Netzwerks Myspace zugeschnitten. Sie enthält teils selbst programmierte und teils externe Programme, die ausschließlich für die Nutzung von Myspace ausgelegt wurden. Es verwendet GNOME und einige Teile von Enlightenment als Desktop-Umgebung.\n\n"}
{"id": "3036391", "url": "https://de.wikipedia.org/wiki?curid=3036391", "title": "Android (Betriebssystem)", "text": "Android (Betriebssystem)\n\nAndroid [] (von Androide, von Mann und Gestalt) ist sowohl ein Betriebssystem als auch eine Software-Plattform für mobile Geräte wie Smartphones, Mobiltelefone, Fernseher, Mediaplayer, Netbooks und Tabletcomputer, die von der von Google gegründeten Open Handset Alliance entwickelt werden.\n\nBei Android handelt es sich um freie Software, Basis ist der Linux-Kernel. Weiterhin teilt sich Android zahlreiche Eigenschaften mit Embedded-Linux-Distributionen. Es unterscheidet sich wesentlich von jenen GNU/Linux-Distributionen, wie man sie für Desktop und Server kennt, weil entscheidende Teile mit alternativen Konzepten wie z. B. Java und der C-Standard-Bibliothek Bionic umgesetzt sind, ähnlich wie z. B. bei der Alpine-Linux-Distribution auch. Ob trotz dieser konzeptionellen Unterschiede der Begriff Linux-Distribution auf Android angewendet werden kann, ist umstritten, da dieser Begriff üblicherweise für Distributionen steht, die neben dem Linux-Kernel auch GNU-Software wie die GNU Core Utilities mitliefern. Diese sind aber nur unvollständig Bestandteil von Android, lassen sich aber nachrüsten. Gleichwohl bezeichnet die Linux Foundation Android als Linux-Distribution, da es sich um eine Distribution mit Linux-Kernel handelt. Zudem sind auch etliche Bestandteile aus NetBSD in Android enthalten.\n\nAndroid hatte als Smartphone-Betriebssystem im dritten Quartal 2016 einen weltweiten Marktanteil von 87,5 Prozent (nach Verkaufszahlen). Seit dem Marktstart 2008 erzielte Google mit Android einen Umsatz von 31 Mrd. US-Dollar (Stand: Januar 2016).\n\nIm Sommer 2005 kaufte Google das im Herbst 2003 von Andy Rubin gegründete Unternehmen Android, von dem nur wenig mehr bekannt war, als dass es Software für Mobiltelefone entwickelte und vorrangig standortbezogene Dienste behandelte. Ursprünglich war Android ausschließlich zur Steuerung von Digitalkameras gedacht. Android wurde größtenteils nicht von Grund auf neu entwickelt. Es wurden vorwiegend mehrere als Open-Source – schon zuvor bestehende – Komponenten neu zusammengestellt. Die wichtigsten davon sind der Linux-Kernel, Java und die C-Standard-Bibliothek \"Bionic\", sowie einige Bibliotheken und Daemons des GNU-Projekts. Am 5. November 2007 gab Google bekannt, gemeinsam mit 33 anderen Mitgliedern der Open Handset Alliance ein Mobiltelefon-Betriebssystem namens Android weiter zu entwickeln. Seit dem 21. Oktober 2008 ist Android offiziell verfügbar.\n\nAls erstes Gerät mit Android als Betriebssystem kam am 22. Oktober 2008 das HTC Dream unter dem Namen \"T-Mobile G1\" in den USA auf den Markt. Dass bereits dieses erste Gerät auf das Global Positioning System (GPS) zugreifen konnte und mit Beschleunigungssensoren ausgestattet war, gehörte zum Konzept von Android.\n\nAm 5. August 2015 kündigte Google an, ab sofort monatlich Sicherheitsupdates für Android bereitzustellen. Diese werden jedoch nur an hauseigene Geräte direkt ausgeliefert. Als Unterstützungszeitraum gibt Google hierbei 3 Jahre für die eigenen Geräte ab Verkaufsstart an. Für andere Geräte entscheidet der jeweilige Hersteller selbst, ob, wie lange und bis wann diese Sicherheitsupdates ausgeliefert werden. Im Zuge dessen kündigte auch Samsung an, künftig monatlich Sicherheitsupdates für die hauseigenen Geräte auszuliefern. Damit reagierte man auf die immer lauter gewordene Kritik aufgrund der im Juli 2015 aufgekommenen schweren Sicherheitslücke Stagefright, die Millionen Android-Geräte weltweit gefährdete.\n\nVon 2010 bis 2016 brachte Google mit der Nexus-Produktreihe in Kooperation mit Hardware-Partnern eigene Android-Tablets und Smartphones auf den Markt. Die Partner übernahmen mit Google die Entwicklung der Geräte, während die Software von Google stammte, ohne Herstelleraufsätze bei der Oberfläche. Dadurch konnte man Geräte mit aktuellen Android-Updates anbieten.\nIm Oktober 2016 wurde die Nexus-Produktreihe durch die Nachfolge-Reihe Pixel ersetzt. Diese werden von Google allein entwickelt. Der Hardwarepartner übernimmt die Fertigung.\n\nNeben dem für Telefone optimierten Android veröffentlicht Google auch angepasste Android-Versionen für Fernsehgeräte – in Form von Android TV, für Uhren bzw. Smartwatches (Android Wear – seit 16. März 2018: Wear OS), Autos (Android Auto) sowie vernetzte Kleingeräte (Android Things).\n\nDer Nutzer steuert Android mittels verschiedener sogenannter Navigationstasten. Bei neuen Geräten (Stand: 2015) bestehen diese aus „zurück“ (geht zur vorherigen Aktivität), „Home“ (öffnet den Startbildschirm) und „Letzte Applikationen“ (erlaubt das schnelle Schließen oder Hin- und Herschalten zwischen gleichzeitig laufenden Apps). Bei älteren Geräten ist die Funktion des Multitasking-Buttons durch ein Gedrückthalten des Home-Buttons zu erreichen. Der Multitasking-Button ist dort ein Options-Button, der bei neueren Geräten bei Bedarf ebenfalls eingeblendet wird. Vor Android 4.0 und bei einzelnen Herstellern gibt es auch „Menü“ oder „Suche“. Samsung setzte lange Zeit in seinen Geräten auf den Menü- statt Multitasking-Button und vertauschte diesen auch von der Position her mit dem Zurück-Button. Erst in neueren Modellen wird der von Google empfohlene Button verwendet. Seit Android 7 wird durch das lange Drücken des Knopfes „Letzte Applikationen“ der Bildschirm geteilt und es lassen sich zwei Apps gleichzeitig benutzen.\n\nMan unterscheidet zwischen Soft- und Hardwaretasten. Erstere befinden sich auf dem Display, letztere als physische Knöpfe auf dem Gerät selbst. Vorteil an virtuellen Tasten ist, dass diese bei Bedarf gedreht bzw. versteckt werden können. Der Vollbild-Modus funktioniert aber nur ab Version 4.4. Davor ist es lediglich beim Betrachten von Fotos und Videos möglich, die Navigationsleiste zu verstecken.\n\nDer Startbildschirm dient in erster Linie zum Starten von Apps. Daher kommt auch die englische Bezeichnung „Launcher“ (englisch „to launch“ = starten). Er besteht aus mindestens einer Seite, je nach Gerät können aber auch weitere Seiten mit beliebigen Elementen hinzugefügt werden. Anders als bei Apples iOS befinden sich damit nicht zwingend alle App-Verknüpfungen auf einer der Seiten des Startbildschirms. Die Übersicht aller installierten Apps ist bis Android 1.6 mit einer Lasche aufzurufen. Ab Android 2.2 öffnet sich eine alphabetisch sortierte Liste aller Apps durch den \"App-Drawer\". Dieser befindet sich in der Standardeinstellung direkt bei den vier Hauptanwendungen im \"App-Dock\", das in der Regel auf allen Seiten sichtbar ist und ebenfalls mit der Version 2.2 eingeführt wurde. Am oberen Rand wird eine Google-Suchleiste angezeigt, die von Google auch einfach durch das Sprechen der Worte „Ok Google“ gestartet und dessen Spracheingabe aktiviert wird. Zwischen Dock und Suchleiste (eine eigene Zeile für das Sucheingabefeld) lassen sich in einem Raster Verknüpfungen (zu Apps, Lesezeichen, Kontakten, etc.) oder Widgets ablegen. Bei diesen handelt es sich um meist etwas größere Anzeigen von allgemeinen Daten, z. B. Wetter, Datum und Uhrzeit, Termine oder auch ein Taschenrechner. Auch werden über Widgets bestimmte App-Informationen übersichtlich präsentiert. So zeigt die Standard-Musik-App beispielsweise Informationen über die gerade laufende Mediendatei mit einem reduzierten Abspielmenü an.\n\nDie Anpassbarkeit ist ein großer Vorteil von Android und spielt beim Startbildschirm als Start- und Übersichtsansicht eine besonders wichtige Rolle. Es lassen sich alternative Apps mit mehr Einstellungen oder einem anderen Bedienkonzept installieren. Beim Druck auf die Hometaste lässt sich auswählen, welche der Apps als Standardanwendung für den Startbildschirm benutzt werden soll. Auch andere Teile des Systems lassen sich ersetzen, bspw. die Tastatur oder der Standardbrowser.\n\nEin weiteres Merkmal des Betriebssystems ist die Benachrichtigungsleiste am oberen Bildschirmrand, die mit einer Geste von ganz oben nach unten geöffnet werden kann. Jede App kann selber bei bestimmten Ereignissen eine Benachrichtigung erstellen, z. B. bei einer neuen E-Mail, einem Termin, einem Downloadvorgang, einer SMS oder einer neu installierten App. Ist die Leiste geschlossen, sieht der Nutzer oben links für jede Benachrichtigung ein passendes Symbol. Die rechte Seite informiert ebenfalls hauptsächlich mit Symbolen über Bluetooth-Verbindung, Klingelmodus, WLAN-Verbindung, Feldstärke und Datenverbindung oder Flugmodus, Akkustand und Uhrzeit (v. l. n. r.). Benachrichtigungen können dauerhaft sein, dann lassen sie sich nicht wie sonst per Geste entfernen, sondern verschwinden von alleine, wenn eine bestimmte Bedingung nicht mehr erfüllt oder ein Vorgang abgeschlossen ist (z. B. die Benachrichtigung „USB-Debugging“ bei PC-Anschluss oder eine Download-Information). Auch gibt es erweiterbare Benachrichtigungen, die per Ziehen nach unten weitere Buttons anzeigen, wie bspw. „Antworten“, „Weiterleiten“ oder „Löschen“ einer E-Mail, oder in der Audiowiedergabe ein größeres Cover, den Albumnamen und einen Zurückspulen-Knopf.\n\nUm die Schnelleinstellungen („Quick Settings“) zu öffnen, gibt es drei Möglichkeiten: Bei geschlossener Leiste einfach durch eine Geste mit zwei Fingern von oben nach unten, sowie im geöffneten Modus (ab Version 5.0) entweder durch einen weiteren Wisch nach unten oder durch Tippen auf die obere Statusleiste (außer dem Datum). Das Einstellungsmenü beinhaltet einen Helligkeitsregler und An-/Aus-Schalter für WLAN, Bluetooth, Farbumkehrung des Bildschirms, Feldstärke, Flugmodus, automatische Bildschirmrotation, Taschenlampe (mithilfe des LED-Blitzes), Standort, Bildschirmübertragung und mobiler WLAN-Hotspot. Durch einen langen Druck gelangt man in die jeweilige Sektion der Systemeinstellungen. Manche Optionen zeigen beim Tippen noch ein kleines Menü an, so erfährt der Nutzer bei einem Druck auf die Feldstärke noch den momentanen mobilen Datenverbrauch und sein Limit und kann auch die mobilen Daten gleich deaktivieren. Bei WLAN und Bluetooth kann (ab Version 5.1) noch das verbundene Netzwerk bzw. Gerät ausgewählt werden.\n\nViele Hersteller von Android-Geräten installieren ihre eigenen Herstelleraufsätze, wie HTC mit HTC Sense, Sonys Xperia UI, Samsung mit der Samsung Experience, LG UX von LG, MIUI von Xiaomi und die EMUI von Huawei. Diese ändern das Design der Oberfläche drastisch, wobei der grundsätzliche Aufbau meist der gleiche bleibt. Die populärsten Geräte mit purem Android, auch „Vanilla Android“ oder „Stock Android“ genannt, sind die Pixel-Geräte, die von Google in enger Zusammenarbeit mit einem anderen Android-Handy-Hersteller entstanden sind und von Google zwei Jahre lang direkt und schnell mit Updates versorgt werden. Einen ähnlichen Status besitzen sogenannte Google-Play-Editionen von bereits existierenden Geräten, die mit Stock-Android statt der Herstelleroberfläche ausgeliefert werden. Bei diesen kommen Updates zwar von den Herstellern selbst, erscheinen aber meist recht schnell.\n\nDie Architektur von Android baute anfangs auf dem Linux-Kernel 2.6 auf, ab Android 4.x wird ein Kernel der 3.x-Serie verwendet. Er ist für die Speicherverwaltung und Prozessverwaltung zuständig und stellt die Schnittstelle zum Abspielen von Multimedia und der Netzwerkkommunikation dar. Außerdem bildet er die Hardware-Abstraktionsschicht für den Rest der Software und stellt die Gerätetreiber für das System.\n\nWeitere wichtige Bausteine sind die auf der Java-Technologie basierende Laufzeitumgebung Android Runtime (ART) sowie die Klassenbibliothek OpenJDK.\n\nBis Version 4.4 wurden Anwendungen in der Dalvik Virtual Machine ausgeführt. Die verwendeten Klassenbibliotheken stammten ursprünglich zum Teil aus dem inzwischen eingestellten Projekt Apache Harmony und orientierten sich stark an der Java Standard Edition.\n\nAnwendungen für die Android-Plattform werden in der Regel in Java geschrieben, jedoch greifen diese in geschwindigkeitskritischen Bereichen auf zahlreiche in C oder C++ geschriebene native Bibliotheken zurück. Darunter befinden sich neben Codecs für die Medienwiedergabe auch ein Webbrowser auf der Basis von WebKit sowie seit Android 4.4 Chromium, die Datenbank SQLite und eine auf OpenGL basierende 3D-Grafikbibliothek.\n\nUm eigene Programme für Android zu entwickeln, benötigt man ein aktuelles Java-Entwicklungswerkzeug und zusätzlich das Android-SDK. Zuerst wird der in Java geschriebene Quelltext mit einem normalen Java-Compiler übersetzt und dann von einem Cross-Assembler für die Dalvik-VM angepasst. Aus diesem Grund können Programme prinzipiell mit jeder Java-Entwicklungsumgebung erstellt werden.\n\nDie fertige Anwendung muss in ein .apk-Paket () verpackt werden, anschließend kann sie über Google Play, App Stores der Gerätehersteller, andere kommerzielle Anbieter (zum Beispiel Amazon) oder auch alternative Paketquellen wie F-Droid bereitgestellt werden. Außerdem lassen sie sich direkt auf dem Gerät mit dem Paketmanager installieren.\n\nDas Framework setzt auf starke Modularität. So sind alle Komponenten des Systems generell gleichberechtigt (ausgenommen die virtuelle Maschine und das unterliegende Kernsystem) und können jederzeit ausgetauscht werden. Es ist also beispielsweise möglich, eine eigene Anwendung zum Erstellen von Kurznachrichten oder zum Wählen von Rufnummern zu erstellen und die bisherige Anwendung damit zu ersetzen.\n\nSeit Mai 2013 stellt Google eine eigene IDE namens Android Studio auf Basis von IntelliJ IDEA bereit, welche die vormals als Eclipse-Plugin entwickelten Android Development Tools ablöste.\n\nEinen weiteren Anwendungsentwicklungs- und Portierungsweg jenseits von Java bietet die SDL-Bibliothek für SDL- und nativen C-Code an. Über einen kleinen Java-basierten Wrappercodeanteil wird über das JNI die Verwendung nativen Codes möglich. Damit ist das Portieren vorhandener SDL-Applikationen auf Android relativ problemlos möglich, wie beispielsweise die Portierung von Jagged-Alliance-2.\n\nDa Android standardmäßig nicht den vollen Umfang der GNU-Bibliotheken umfasst bzw. diesen nicht standardkonform implementiert sowie ein eigenes Fenstersystem besitzt, ist eine Portierung von Software für klassische Linux-Distributionen auf Android schwierig.\n\nAndroid verwendet EGL als Schnittstelle zwischen dem Fenstersystem und OpenGL ES sowie OpenVG.\n\nAndroid ist eine freie Software. Der größte Teil der Plattform steht unter der Apache-Lizenz – Ausnahmen sind der Linux-Kernel, der unter der GPL 2 vertrieben wird, und die Google Play Dienste zusammen mit, von den meisten Herstellern vorinstallierten, Anwendungen von Google, deren Quelltext nicht verfügbar ist. Der Quelltext der ausschließlich für Tablets geschriebenen und zunächst nur für ausgewählte Gerätehersteller verfügbaren Version 3 wurde von Google erst im Zuge der Veröffentlichung der Quellen von Version 4 freigegeben, die die Smartphone- und die Tabletoberfläche vereinte.\n\nAb Version 1.5 tragen alle Versionen neben der Versionsnummer den englischen Namen einer Süßspeise, deren Anfangsbuchstaben jeweils im Alphabet aufsteigend sind.\nZu jeder Android-Version entwirft die Open Handset Alliance oder Google ein Android-Maskottchen mit der jeweils zur Version gehörenden Süßspeise. Dieses kann ab Version 2.3 durch mehrmaliges Berühren der Android-Version in den Systemeinstellungen angezeigt werden.\n\nAlle Versionen bis einschließlich Version 2.3.x „Gingerbread“ waren nur für Smartphones vorgesehen, wurden aber auch in Navigations- und anderen Geräten eingesetzt. Mit der Version 3 mit Beinamen „Honeycomb“ kam ein seitens Google rein für Tablets vorgesehenes Betriebssystem. Die Trennung zwischen den beiden Geräteklassen wurde mit „Ice Cream Sandwich“ aufgehoben, welches von Grund auf für beide Systeme vorgesehen ist.\n\nDa der Tablet-Markt immer stärker wuchs und Google „Honeycomb“ erst recht spät veröffentlichte, brachten einige Hersteller Tablets auf den Markt, die entgegen Googles Empfehlungen unter „Gingerbread“ liefen. Umgekehrt wollte Google verhindern, dass die „Honeycomb“-Version auf Smartphones eingesetzt wird, weswegen der Quellcode erst verspätet veröffentlicht wurde.\n\nZum ersten Mal in der Geschichte wurde auf der Google I/O 2014 eine Vorabversion von Android 5.0 „Lollipop“ mit dem Namen „L Preview“ angekündigt. Für welches Dessert „L“ dabei stand, wurde erst zum fertigen Release verraten. Dieses Vorgehen wird seither auf alle neuen Versionen angewendet. Mit Android Lollipop 5.0 wurde unter anderem ein Energiesparmodus integriert, der unter anderem Hintergrundprozesse anhält, die Taktrate des Prozessors verringert und so den Stromverbrauch reduziert. Auch wurde eine Möglichkeit eingeführt, Benachrichtigungen bestimmter Apps als vertraulich zu behandeln, sodass sie auf dem Sperrbildschirm nur zensiert gezeigt werden.\n\nAm 17. August 2015, gleichzeitig mit der finalen Vorschauversion, wurde „Marshmallow“ als offizieller Name für Android 6.0 angekündigt. Die finale Version erschien am 5. Oktober 2015. Android Marshmallow 6.0 gab erstmals, ohne vorher benötigte Eingriffe in das Betriebssystem, die Möglichkeit, einzelnen Apps Berechtigungen zu entziehen. Unter vorherigen Versionen war dies nur mit technischer Manipulation und einem Garantieverlust möglich. Auch das in Android 5.0 eingeführte Akkumanagement wurde um einen „Schlafmodus“ für Anwendungen ergänzt, der aktiviert wird, sobald der Bildschirm des Geräts ausgeschaltet wird. In der Version wurde sowohl Googles Bezahldienst „Android Pay“ als auch die Erfassung und Verarbeitung biometrischer Daten in das Betriebssystem integriert.\n\nIn Googles App Store „Google Play“ (ehemals „Android Market“) gab es im Mai 2015 mehr als 1.500.000 Apps. Seit 2013 verfügt Google Play über ein größeres App-Angebot als der vorherige marktführende App-Anbieter Apple. Im SDK werden zusätzlich eine Reihe von Anwendungen, darunter ein Webbrowser, die Kartenanwendung Google Maps, eine SMS-, E-Mail- und Adressbuchverwaltung, ein Musikprogramm, eine Kamera- und Galerieapplikation, sowie ein Satz von API-Demoanwendungen mitgeliefert.\nErstellte Software kann von den Entwicklern bei Google Play angeboten werden. Verkaufen kann man sie dort allerdings nur, wenn man in bestimmten Staaten ansässig ist. In den Nutzungsbedingungen sind unter anderem Deutschland, Österreich und die Schweiz aufgeführt. Kostenfreie Software macht etwa 69 Prozent aus.\n\nNeben Google Play stehen Entwicklern und Endanwendern auch noch eine Reihe anderer Märkte und Plattformen für Android-Software offen, wie der Amazon Appstore, Yandex.Store oder F-Droid; manche vermeintlich eigenständige verweisen jedoch wiederum auf Google Play.\n\nGoogle behält eine gewisse Kontrolle über Android-Software. Nur lizenzierte Android-Distributionen dürfen die Google-eigenen (Closed Source) Anwendungen wie Google Mail oder Google Maps verwenden, sowie auf Google Play für weitere Applikationen zugreifen. Verschiedene Tablets verwenden ein unlizenziertes Android 4; „gerootete“ Geräte verwenden oft ein Custom-ROM. Diese haben nach dem Flashen der entsprechenden GApps ebenfalls die Berechtigung für den Zugriff auf den Google Play sowie die anderen GApps der Google Mobile Services (GMS). Es wird nicht offiziell unterstützt, Applikationen aus Google Play auf einem Nicht-Android-System herunterzuladen (z. B. einem normalen PC), um sie dann über USB auf einem Android-Gerät zu installieren. Dies ist aber mit Drittanbietersoftware wie dem Java-Programm \"Raccoon\" oder dem Browser-Addon \"APK Downloader\", erhältlich für Firefox und Chrome, trotzdem möglich. Auch ein Download auf einem Android-Gerät selbst, mithilfe von Apps wie \"YalpStore\", ist möglich. Jedoch wird hierbei, wie bei dem Download mit der offiziellen App, ein Google-Konto benötigt. Einige Software-Hersteller bieten ihre Applikationen auch in alternativen App-Stores oder direkt als Installationsdatei an; diese lässt sich dann auf beliebige Weise herunterladen und auf dem Android-Gerät installieren.\n\nAb Version 6 „Marshmallow“ ist auf purem Android ein rudimentärer Dateimanager vorinstalliert, der über die \"Einstellungen, Speicher, Erkunden\" aufzurufen ist. Seit Android 4.4 „KitKat“ gibt es zudem ein Media-System, über das indirekt auf das Dateisystem zugegriffen werden kann, wenn z. B. ein Hintergrundbild ausgewählt oder ein E-Mail-Anhang verschickt wird; diese Oberfläche direkt zu starten oder Dateien zu löschen, verschieben, kopieren oder umbenennen ist aber dennoch nicht möglich.\n\nAb Version 4.0 \"(Ice Cream Sandwich)\" besitzt das Betriebssystem einen hohen Grad an Barrierefreiheit. Die benötigte Software ist bereits installiert und kann theoretisch von einem Blinden selbständig aktiviert werden. Dabei bietet Android den Vorteil, dass Sprachausgabe (Screenreader) und Bildschirmvergrößerung parallel genutzt werden können. Unterstützung für Braillezeilen bietet Android ab der Version 4.1 und der Erweiterung BrailleBack.\n\nIm Gegensatz zu herkömmlichen Desktop-Computer, bei denen der Nutzer beim Kauf auch die vollen Administrationsrechte besitzt, hat man bei Android-Geräten nicht das vollständige Administrationsrecht. Vom Nutzer unerwünschte Applikationen können von ihm nicht entfernt werden, auch wenn diese Applikationen nicht für den einwandfreien Betrieb des Gerätes erforderlich sind. Die Festlegung, ob z. B. eine bestimmte Anwendung entfernt werden darf, legt der Hersteller der mobilen Endgeräte selbst oder in Absprachen mit den Entwicklerfirmen der Anwendung fest (siehe auch Abschnitt zur Kritik/Kontrolle durch Google, Übermittlung privater Daten). Zur Erlangung von Rootrechten muss entweder eine alternative Betriebssystemversion, etwa ein sogenanntes Custom-ROM installiert werden, oder mittels speziell dafür erstellter Apps vorhandene Mängel im System ausgenutzt werden.\n\nIm ersten Quartal 2010 wurden in den Vereinigten Staaten erstmals mehr Android-Mobiltelefone als iPhones verkauft. Der Marktanteil bei Neugeräten betrug im untersuchten Zeitraum für Google 28 Prozent im Gegensatz zu 21 Prozent bei Apple.\nBei den Internetzugriffen ist Android seit Juli 2014 das führende Smartphone-Betriebssystem.\nIm September 2013 verkündete Sundar Pichai, dass bis dato eine Milliarde Geräte aktiviert worden seien.\nIm Jahr 2014 wurden ca. 1 Milliarde Android-Geräte verkauft. Android hatte als Smartphone-Betriebssystem im dritten Quartal 2016 einen weltweiten Marktanteil von 87,5 Prozent, nach 84,6 Prozent im zweiten Quartal 2014, 79,3 Prozent im zweiten Quartal 2013, 68,1 Prozent im zweiten Quartal 2012, 52,5 Prozent im dritten Quartal 2011 und 25,5 Prozent im dritten Quartal 2010.\n\nDurch die Entscheidung von Google, sein Betriebssystem kostenlos zur Verfügung zu stellen, ist die Nutzung bei Herstellern von Endgeräten beliebt. Diese Entscheidung führte jedoch auch zu einer starken Fragmentierung. Laut einer im Mai 2012 vom Unternehmen Staircase vorgenommenen Untersuchung unter Android-Smartphone-Nutzern, die über einen Zeitraum von einem halben Jahr durchgeführt wurde, konnten 3997 verschiedene Geräte von insgesamt 599 Herstellern gezählt werden.\nAllerdings wird das Ergebnis durch Custom-ROMs, das heißt nicht originale vom Hersteller bereitgestellte Betriebsprogramme, etwas verfälscht.\n\nViele Hersteller, insbesondere in China,\nverwenden eine unlizenzierte Android-Version, die keine Google-Dienste wie Google Maps, den Google Play Store und Gmail enthält. Nach Schätzungen von Analysten betrifft dies zwischen 23 %\nund 41 % aller verkauften Android-Geräte.\n\nDa die Quellen von Android frei verfügbar sind und auch keine rechtlichen Gründe dagegen sprechen, kann es verhältnismäßig einfach auf nahezu jeden ausreichend leistungsfähigen Rechner portiert werden. Wenn eine Plattform vom Linux-Kernel unterstützt wird, besteht eine gute Möglichkeit, dass eine Portierung von Android gelingt. Es gibt Portierungen für Smartphones, die ursprünglich mit einem anderen Betriebssystem ausgeliefert wurden. Andere Projekte portieren Android für einen handelsüblichen Desktop-Computer. Viele dieser Portierungsprojekte werden von einer weltweiten Entwickler-Community vorangetrieben. Die \"Open Handset Alliance\" unterstützt diese Projekte mit einer eigenen Portierungsanleitung.\n\n\n\nGoogle hat die Möglichkeit, sofern die Google Apps installiert sind, Software ohne vorherige Nachfrage beim Nutzer zu löschen und zu installieren. Über etwaige dauerhafte Verbindungen mit Google-Servern könnten Applikationen via Fernzugriff ohne Einwirkung, jedoch mit Wissen des Nutzers gelöscht und installiert werden (Statusmeldung). Falls Google eine kostenpflichtige Software löscht, erhält der Kunde den Kaufpreis zurück. Im Juni 2010 hat Google erstmals Anwendungen auf den Endgeräten der Anwender durch einen Fernzugriff gelöscht, nachdem Sicherheitsexperten ein Schadprogramm in den damaligen Android Market eingeschleust hatten, um auf fehlende Kontrollen aufmerksam zu machen. Außerdem kann Google auch über die Google-Play-Dienste Geräte-Einstellungen ohne Einwilligung des Nutzers aus der Ferne ändern, was sich durch ein versehentliches Aktivieren des Energiesparmodus auf einigen Telefonen durch Google zeigte.\n\nWegen Googles Kontrolle über die Marke \"Android\" sowie über das Betriebssystem sind Gerätehersteller auf die Zusammenarbeit mit Google angewiesen. Diese Kontrolle soll Google in der Vergangenheit unter anderem genutzt haben, um zu verhindern, dass Gerätehersteller die Lokalisierungsdienste von Skyhook Wireless anstatt der Google-eigenen benutzen sowie um zu verhindern, dass Acer Geräte mit dem konkurrierenden Betriebssystem Aliyun vorstellt.\n\nDie SafetyNet Api soll die Kompatibilität und Sicherheit prüfen. Überprüft wird unter anderem, ob der Bootloader entsperrt ist, ob das Gerät gerootet ist und ob Google-Dienste installiert sind; das führt dazu, dass Apps wie z. B. Pokemon GO, Snapchat und auch viele Banking Apps auf gerooteten sowie auf Geräten mit Custom-ROMs ohne Google Apps nicht oder nur teilweise funktionieren.\n\nGoogle verbietet App-Entwicklern, bestimmte Funktionen oder Apps zu integrieren; wenn sich ein Entwickler nicht daran hält, dürfen seine Apps nicht über den Google Playstore verbreitet werden.\nSo werden immer wieder Apps aus dem Playstore verbannt. Z. B. wurden 2013 alle Apps, die systemweit Werbung blockieren wie z. B. AdAway, Adblock Plus, vom Playstore verbannt; die Nahverkehrsapp Öffi war 2 Monate lang nicht verfügbar, da ein Spenden-Button gegen Googles Richtlinien verstoßen hat, und auch TV-Browser musste Funktionen entfernen.\n\nViele Applikationen benötigen Zugang zu privaten Daten wie Kontakte, Lokalisierung sowie Telefonnummer und können diese auch übermitteln. Bis zur Android-Version 6.0 Marshmallow kann man diesen Zugang standardmäßig nur verwehren, indem man die Applikation nicht installiert. Ab Marshmallow können einige grundlegende Rechte, wie Zugriff auf die GPS-Position oder Kontakte, verweigert werden. Custom-ROMs wie z. B. LineageOS (früher CyanogenMod) verfügen über ein weitergehendes Datenschutzmanagement, womit den Apps bestimmte Berechtigungen wieder entzogen werden können (auch ohne Root-Berechtigungen möglich). Es sind zahlreiche Apps bekannt, die deutlich mehr Berechtigungen anfordern und Daten übermitteln, als für ihre Funktion notwendig wäre. Jedoch gibt es auch Hilfsprogramme („Tools“), die den Apps falsche Daten liefern und sie so in dem Glauben lassen, sie hätten die Zugriffsrechte. Diese Tools benötigen aber Root-Rechte. Ab Android 6 wird eine solche Funktion mitgeliefert.\n\nMitunter fragen auch vorinstallierte Applikationen unnötig viele private Daten ab. Dies lässt sich auch kaum verhindern, da diese Applikationen nicht ohne Root-Rechte entfernt werden können. Selbst reine „Sammel-Apps“ gänzlich ohne Funktion für den Nutzer wurden bereits vorinstalliert ausgeliefert.\n\nOft können solche Apps aber wenigstens deaktiviert werden, so dass sie zwar noch vorhanden, jedoch im System nicht mehr aktiv sind. Als Bloatware bezeichnet man Apps, die schon vorinstalliert ausgeliefert werden, ohne einen unmittelbaren Nutzen für das System zu haben.\n\nApplikationen mit Berechtigung zum Internetzugriff können Dateien der SD-Karte ins Internet hochladen. Mit dem Recht „Konten auf dem Gerät suchen“, das sehr viele Spiele anfordern, kann z. B. die E-Mail-Adresse des Google-Kontos gelesen werden und können über Google+ persönliche Informationen abgefragt werden. Auch hinterlegte E-Mail-Adressen, Facebook- und Twitter-Konten können ermittelt werden. Das Recht „WLAN-Verbindungen abrufen“ entspricht in seiner Wirkung dem Zugriff auf den Standort über GPS, da unter anderem Google die SSIDs in frei abfragbaren Geodatenbanken speichert.\n\nIm August 2018 wurde von der Verleger-Organisation „Digital Content Next“ eine Studie veröffentlicht, nach der ein nicht bewegtes Android-Smartphone mit im Hintergrund laufendem Chrome-Browser innerhalb von 24 Stunden rund 340 Mal Standortinformationen an Google versendet. Zudem soll Google anonymisiert erhobene Informationen mit persönlichen Nutzerdaten verknüpfen können, so der federführende Professor Douglas Schmidt von der Vanderbilt University.\n\nMit seinem ständig steigenden Verbreitungsgrad wird Android für Schadsoftware-Autoren immer interessanter. Insbesondere die Möglichkeit, ungeprüfte Apps von Drittanbietern zu installieren (dazu ist die Zustimmung des Nutzers erforderlich), erhöht die Gefahr. Google prüft seit Anfang 2012 alle Apps im Play Store automatisch. Daher ist man, wenn man Apps nur von dort installiert, relativ sicher; es sind jedoch auch Fälle von Schadsoftware-Verbreitung über den Play Store bekannt. Weitere Sicherheit bietet das in Android standardmäßige Sandboxing: Alle Apps laufen getrennt in einer virtuellen Maschine. Angeforderte Berechtigungen der Apps werden angezeigt und bedürfen der Zustimmung des Nutzers.\n\nDiverse Anbieter von Sicherheitssoftware stellen Sicherheits-Apps mit mehr oder weniger guten Resultaten zur Verfügung. Diese Hersteller sind es auch, die oftmals laut und medienwirksam auf eine „Bedrohung“ aufmerksam machen, um ihre Produkte zu vermarkten. Eine weitere Art der Bedrohung über den in den neuesten Geräten vorhandenen Nahbereichsfunk NFC wurde von einem Sicherheitsspezialisten auf der Hackerkonferenz Black Hat 2012 vorgeführt.\n\nAndroid-Smartphones werden oft mit einer älteren Version des Betriebssystems verkauft. Meist haben die Hersteller gegenüber den Kunden keine vertraglichen Verpflichtungen, neuere Versionen zur Verfügung zu stellen; auch den einwandfreien Betrieb der Geräte im Auslieferungszustand muss vor allem der Verkäufer sicherstellen, nicht der Hersteller. Letztlich ist es also dem Hersteller überlassen, ob er den technischen Aufwand, der je nach Situation unterschiedlich hoch ist, betreibt oder nicht. Dass aktuelle Android-Versionen „verspätet“ oder gar nicht für ein bestimmtes Smartphone erscheinen, wird dennoch von Kunden scharf kritisiert.\n\nAuf der Entwicklerkonferenz Google I/O im Mai 2011 wies Google jedoch darauf hin, dass zahlreiche Hersteller bei jedem neuen Gerät mindestens 18 Monate lang die jeweils aktuelle Android-Version bereitstellen würden. Allerdings gab es schon kurz darauf Meldungen, wonach Hersteller diese Selbstverpflichtung brechen. Auf der Konferenz \"Google I/O\" im Juni 2012 veröffentlichte Google ein \"Platform Development Kit\", das es Herstellern frühzeitig ermöglicht, neue Android-Versionen auf ihre Hardware zu portieren.\n\nAuf der anderen Seite ist durch die Quelloffenheit von Android und des Linux-Kernels die technische und juristische Voraussetzung dafür erfüllt, dass herstellerunabhängige Entwicklergruppen „inoffizielle“ Android-Portierungen, sogenannte Custom-ROMs, für Android-Geräte bereitstellen können. Welche Geräte davon profitieren, hängt aber von der Motivation der entsprechenden Entwickler und der Verfügbarkeit benötigter hardwarenaher Software-Komponenten, vor allem Treiber, ab.\n\nWeniger betroffen von der späten bzw. nicht vorhandenen Aktualisierung sind Googles Geräte der Nexus-Reihe (z. B. Nexus 5, Nexus 4, Galaxy Nexus, Nexus 7 (2012), Nexus 7 (2013) und Nexus 10) und der Pixel-Reihe. Diese erhalten aktuelle Android-Versionen mit kürzeren Verzögerungen und bleiben so lange auf dem neuesten Stand, bis die 18-monatige Zeitspanne der „Update-Unterstützung“ endet. Im Zuge der Stagefright-Sicherheitslücke gab Google im August 2015 bekannt, dass Nexus-Geräte künftig monatliche Sicherheitsupdates für drei Jahre nach Erscheinen des Geräts erhalten werden, unabhängig von den Updates der Android-Plattform, die für etwa zwei Jahre verteilt werden sollen.\n\nBei Android-Smartphones mit Qualcomm-Chipsatz lässt sich mit Hilfe extrahierter Informationen und der Brute-Force-Methode das verwendete Passwort knacken und damit die Gerätevollverschlüsselung aushebeln. Ursache ist, dass der Prozess der Schlüsselerstellung durch Software bestimmt wird.\n\nGoogle wird beschuldigt, mit Android eine große Anzahl von Patenten anderer Firmen zu verletzen. Infolgedessen kam es seit 2009 weltweit zu einer Reihe von Rechtsstreiten mit Geräteherstellern. Microsoft hat mit den drei Auftragsfertigern Quanta, Wistron und Compal sowie den Herstellern Samsung, HTC, Acer, ViewSonic, Onkyo, General Dynamics, Itronix und Velocity Micro Lizenzverträge abgeschlossen und verdient dadurch Schätzungen zufolge zwischen 3 und 12,50 US-Dollar pro verkauftem Android-Gerät, insgesamt etwa 500 Mio. US-Dollar im Geschäftsjahr 2012.\n\nZudem wurde Google im August 2010 von Oracle verklagt, weil Android Patent- und Markenrechte von Java verletzen würde. Den Rechtsstreit konnte Google in erster Instanz für sich entscheiden. Das Berufungsgericht gab Oracle jedoch recht und erkannte darauf, dass Urheberrechte an Java-APIs verletzt worden seien. Über den weiteren Fortgang entscheidet ein noch anhängiges Wiederaufnahmeverfahren.\n\n\n\n\nInoffizielle apk-Downloader für den Google Play Store\n\n.NET-Compiler für Android\n"}
{"id": "3043432", "url": "https://de.wikipedia.org/wiki?curid=3043432", "title": "Gravis (Handelskette)", "text": "Gravis (Handelskette)\n\nDie Gravis Computervertriebsgesellschaft mbH (Eigenschreibweise \"GRAVIS\") ist eine deutsche Handelskette mit Sitz in Berlin und Tochtergesellschaft von Mobilcom-debitel, die sich auf den Vertrieb von Apple-Produkten und deren Zubehör spezialisiert hat.\n\nDas Unternehmen wurde 1986 von den damaligen Berliner Studenten Archibald Horlitz und Wilfried Gast als HSD GmbH gegründet. Am 29. Juli 1988 wurde zunächst die Tochtergesellschaft GRAVIS EDV Vertriebs GmbH als reines Versandhandelshaus gegründet. Heute sind etwa 660 Mitarbeiter deutschlandweit bei Gravis beschäftigt. Es gibt momentan in Deutschland 41 Gravis-Filialen. Sitz der Zentrale und des ersten Flagship-Stores ist Berlin. \n\nAm 18. Dezember 2012 gab Gravis bekannt, dass die mobilcom-debitel GmbH, die zur freenet-Gruppe gehört, Gravis übernehmen will. Nach Zustimmung der Kartellbehörden wurde die Übernahme 2013 vollzogen.\n\nAm 5. Mai 2014 gab Gravis bekannt, dass bis zu 12 deutsche Läden der Düsseldorfer reStore GmbH übernommen werden sollen. Die Integrierung fand ab Ende Mai 2014 nach Zustimmung der Kartellbehörden statt. Die Filialen wurden nun vollständig zu Gravis Filialen umgewandelt.\n\nGravis war zusammen mit den Apple-Premium-Resellern neben Apples Online-Store und den deutschen \"Apple Retail Stores\" in Deutschland einer der wenigen Händler, die sich auf Apple spezialisiert haben. Mit der Übernahme 2013 wurde eine Neuorientierung vollzogen, bisher orientierte sich das Sortiment vorwiegend an einer professionellen oder kreativen Zielgruppe, nun an elektronischen Lifestyle-Produkten und Trends. Es werden neben Smartphones, Tablets und Zubehör, auch Mobilfunkverträge und elektronische Gadgets – beispielsweise Spielzeuge, Hauselektronik und Quadrocopter – verkauft. Seit 2017 verkauft Gravis neben Apple-Hardware auch die Surface-Produkte des Herstellers Microsoft.\n\nAlle Produkte werden auch im Online-Shop angeboten. Es existiert des Weiteren ein eigener, zentraler Reparaturservice mit zertifizierten Technikern, ebenfalls mit Sitz in Berlin, sowie Filialen mit eigenem technischem Service. Es gibt insgesamt 41 Filialen deutschlandweit. Seit 2015 haben einige Filialen den „Premium Service Provider“-Status von Apple verliehen bekommen, der für besonders hohe Service-Qualität steht.\n\n"}
{"id": "3043749", "url": "https://de.wikipedia.org/wiki?curid=3043749", "title": "Dolphin (KDE)", "text": "Dolphin (KDE)\n\nDolphin ist der Dateimanager der KDE Software Compilation 4 und Teil der KDE Applications 5.\n\nNach dem Dateimanager \"KFM\" (KDE File Manager) von KDE 1 wurden dessen Funktion ab KDE 2 in den Konqueror übernommen. Nachdem der Konqueror allerdings ab KDE 3 als zu komplex für einfaches Verwalten von Dateien kritisiert worden war, wurde das Projekt \"Dolphin\" ins Leben gerufen. Er ist nun im Unterschied zu Konqueror ein reiner Dateimanager und kann somit an die speziellen Anforderungen angepasst werden.\n\nDie KDE-3-Version von Dolphin ist alltagstauglich, wenngleich sie nie das Beta-Stadium verlassen hat. Nach der Version 0.8 als letzter KDE-3-Version wurde Dolphin auf KDE Platform 4 portiert. In einer Abspaltung namens \"D3lphin\" wurde die Entwicklung einer KDE 3-Version zunächst fortgesetzt, dann jedoch eingestellt.\n\nSeit dem Erscheinen von KDE 4.0 ist Dolphin der Standarddateimanager, der einen großen Teil des Quellcodes als KPart zur Verfügung stellt. Dadurch kann der Browser Konqueror einfach die Ansicht von Dolphin integrieren, wenn darin Verzeichnisse angezeigt werden sollen.\n\nAb KDE Applications 15.08 ist Dolphin auf Basis von KDE Frameworks 5 erhältlich.\n\nFür viele Funktionen greift Dolphin als Front-End auf KDE-Programmkomponenten, die KParts, zurück. Auch Archivinhalte oder entfernte Dateiverzeichnisse können über sogenannte kioslaves des KDE Input/Output bearbeitet werden. Dolphin erlaubt es, die Verzeichnisansicht zu teilen, um zwei Verzeichnisse nebeneinander darzustellen, und in einem Fenster mehrere Verzeichnisse in Tabs zu öffnen. Es können weiterhin Mehrfachumbenennungen von Dateien/Verzeichnisse durchgeführt werden und eine Rückgängig-Funktion wird geboten. Dolphin unterstützt sowohl die Brotkrümelnavigation als auch die direkte Eingabe eines Pfades.\n\n\n"}
{"id": "3043761", "url": "https://de.wikipedia.org/wiki?curid=3043761", "title": "Ambulantes Assessment", "text": "Ambulantes Assessment\n\nAmbulantes Assessment bedeutet Datenerfassung im Alltag der Untersuchten. Die Daten werden ambulant („beweglich“) erhoben im Unterschied zu einer stationären, an Standorte wie Klinik oder Labor gebundenen Messung. Das aktuelle Erleben und das Verhalten werden zeitnah in der jeweiligen konkreten Situation erfasst. Darüber hinaus können die in der Psychophysiologie verwendeten Rekorder auch die Bewegungsaktivität (Accelerometrie) sowie verschiedene physiologische Messwerte registrieren. So ist zu erkennen, wie psychische und körperliche Veränderungen zusammenhängen. In der Medizin hat sich das ambulante 24-Stunden-Monitoring (Überwachung) bewährt, beispielsweise um Patienten mit Bluthochdruck unter alltäglichen Lebensbedingungen zu untersuchen.\n\nAmbulantes Assessment bedeutet Datenerhebung im Alltag der Untersuchten, heute meist mit computer-unterstützter Methodik, um Auskünfte über das momentane Befinden und Verhalten (Selbstberichte), Verhaltensmaße, Bewegungsverhalten und physiologische Messwerte aufzuzeichnen. Außerdem werden meist objektive Rahmenbedingungen (engl. Setting) wie Aufenthaltsort, Tätigkeit, Anwesenheit anderer Personen sowie die subjektive Bewertungen der Situation erfasst.\nDie in den 1970er Jahren von Mihály Csíkszentmihályi und Mitarbeitern in Chicago entwickelte Experience Sampling Method ESM war im Unterschied zu den ersten deutschen Forschungsarbeiten von Kurt Pawlik und Lothar Buse ursprünglich keine computerunterstützte Methode, sondern benutzte eine programmierte Uhr und Fragebogen in gehefteter Form (Booklet).\n\nAmbulantes Assessment ist der im deutschen Sprachraum inzwischen verbreitete und umfassende Begriff für die Datenerfassung im Alltag. Dagegen beziehen sich ähnliche Begriffe wie \"Ecological Momentary Assessment\" EMA, \"Experience Sampling Method\" ESM, \"Time Sampling Diary\" TSD oder \"electronic diary\" (elektronisches Tagebuch) in der Regel nur auf die tagebuchartige Erfassung aktueller Selbstberichte und nicht auf Verhaltensdaten und physiologische Messungen.\n\nDie systematische Erfassung, Beobachtung und Überwachung eines Vorgangs mit der Absicht steuernd einzugreifen, falls der gewünschte Verlauf nicht eintritt, wird als Monitoring bezeichnet (siehe Biomonitoring, Patientenmonitoring). Demgegenüber ist das ambulante Assessment nicht auf Überwachungsfunktionen festgelegt, sondern hat vielseitige Anwendungsmöglichkeiten und steht als „Felduntersuchung“ in einem Gegensatz zum psychologischen Experiment in einem Labor (als „künstlicher“ Situation) oder einer nachträglichen Datenerhebung, wenn die Situation nicht mehr präsent ist (z. B. abendliche Beantwortung von Fragen zum Stress-Erleben in bestimmten Situationen des Tages).\n\nZu festgelegten Zeiten fordert ein kleiner tragbarer Computer, ein Personal Digital Assistant (hand-held PC), zur Eingabe des vereinbarten Berichts auf, beispielsweise über Aufenthaltsort und Tätigkeit, über Befindlichkeit, körperliche Beschwerden und Symptome. Eine andere Version ist, dass die Person den Eingabezeitpunkt selbst bestimmt, sobald sie sich in einer bestimmten Situation befindet.\n\nHeute sind die computerunterstützten Methoden so weit entwickelt, dass sie sich für verschiedene Daten eignen:\n\n\nEin „elektronisches Tagebuch“ hat im Vergleich zu den konventionellen „Papier-und-Bleistift“-Methoden (Fragebogen, Persönlichkeitsfragebogen) den wichtigen Vorzug der zeitliche Nähe zur Situation, die beurteilt werden soll, wobei sich Gedächtniseffekte und Bewertungen weniger auswirken werden als bei einem zeitlichen Abstand. Solche computer-unterstützten Selbstberichte sind aktuelle und situationsbezogene Aussagen, die mit einer genauen Zeitangabe verankert sind. Im Allgemeinen werden sie durch ihre größere Verhaltensnähe gültiger und überzeugender sein als die Antworten in einem später ausgefüllten Fragebogen. Die technische Zuverlässigkeit der computerunterstützten Datenerhebung übertrifft jene der Fragebogenmethodik, und die genaue zeitliche Protokollierung der Eingaben kontrolliert zugleich die Zuverlässigkeit – statt nur darauf zu hoffen, dass alle Untersuchten sich auch im Alltag an den vereinbarten Terminplan für ihre Selbstberichte halten werden. Weitere praktische Vorzüge der computer-unterstützten Selbstberichte sind die flexible Programmierung der Fragen und Antwortmöglichkeiten sowie der einfache Datentransfer. Hinzu kommen die neueren Kommunikationstechniken: die uni- oder bidirektionale Kommunikation mit Untersuchungsleitern oder Therapeuten über Mobiltelefon (Handy) sowie die Webanwendungen mit den Möglichkeiten des Datentransfers und der Datenanalyse in Echtzeit, ggf. mit Rückmeldung an die Untersuchten.\n\nMit modernen Aufzeichnungs- und Auswertungssystemen sind auch wichtige physiologische Funktionen unter Alltagsbedingungen zu registrieren, u. a. das Elektrokardiogramm (Herzfrequenz), Blutdruck, Atmung, Hauttemperatur und motorische Funktionen (Accelerometrie). Inzwischen gibt es eine Vielzahl portabler Datenerfassungs-Systeme mit vielseitiger Software (Ebner-Priemer und Kubiak, 2007; Myrtek, Foerster und Brügner, 2001). Eine vielversprechende Methodik ist die Aufzeichnung von Sprechaktivität und Umweltgeräuschen, wenn die Teilnehmer dazu bereit sind. Auf diese Weise können nach einer Eingewöhnung alltagsnahe Daten, u. a. über die soziale Umgebung, soziale Interaktionen, Gewohnheiten, Hinweise z. B. auf depressive Stimmungsänderungen, gewonnen werden (Mehl & Holleran, 2007).\n\nDie psychologische Beurteilung der ambulant erhobenen Daten ist grundsätzlich auf Informationen über den jeweiligen Kontext (Rahmenbedingungen, engl. setting) angewiesen. Hierzu dienen meist Selbstberichte, seltener Verhaltensbeobachtungen. Darüber hinaus können wichtige Umgebungsbedingungen gemessen werden.\n\nWährend das Ambulante Monitoring in der Medizin vorwiegend der Diagnostik und der Überwachung von Risikopatienten, u. a. bei Herz-Kreislauf-Erkrankungen, dient, sind die Aufgaben des Ambulanten Assessment in der Psychologie vielseitiger. Auch in der Arbeitspsychologie und Klinischen Psychologie gibt es Überwachungsaufgaben, z. B. an riskanten Arbeitsplätzen oder als Selbst-Monitoring bei bestimmten chronischen Gesundheitsstörungen oder Verhaltensproblemen.\n\nFür viele andere Fragestellungen sind Verhaltens- und Erlebnisdaten aus dem Alltag wesentlich. Hier wird das Ambulante Assessment künftig die Methode der Wahl sein statt der gegenwärtig weithin dominierenden Fragebogen. Arbeit und Gesundheit bilden wichtige Forschungs- und Praxisfelder, die eine alltagsnahe Datenerhebung erfordern. Beispiele sind die gründliche Erfassung von Arbeitsbelastungen, das Schmerztagebuch bei chronischer Schmerzkrankheit und die Diagnostik und Therapiekontrolle bei Patienten mit einer Panikstörung (Anwendungen für das Gebiet der Klinischen Psychologie und Psychiatrie, siehe Wilhelm und Perrez, 2008).\n\nBeim Blutdruck-Monitoring hat sich gezeigt, dass die Messungen in der ärztlichen Praxis und im Forschungslabor häufig nicht mit den Messwerten im Alltag übereinstimmen (Blutdruckmessung, Weißkittelhypertonie). Auch auf anderen Gebieten könnte es systematische Fehler mangels alltagsnaher Daten geben.\n\nDie meisten Untersucher berichteten, dass die Akzeptanz der computer-unterstützten Methoden, d. h. der Geräte und der Anforderungen, in der Regel gut ist. Der Sinn von Schmerztagebüchern (pain diaries) im Hinblick auf die Erprobung und Einstellung der optimalen Medikation ist ebenso einleuchtend wie der praktische Nutzen der Blutdruckmessung unter Alltagsbedingungen. Eine unerwünschte, methodenbedingte Reaktivität kann sich – je nach Untersuchungsplan – in unterschiedlichen Effekten äußern. Nur ein Teil der Untersuchten meinte, dass sich die objektive Selbstaufmerksamkeit durch die regelmäßigen Selbstberichte erhöht habe. Von einem elektronischen Tagebuch mehrmals am Tag angepiepst zu werden, kann nach einiger Zeit lästig werden, zumindest in bestimmten Situationen. Deshalb sollte die Programmierung vorsehen, dass der Selbstbericht ggf. aufgeschoben werden kann. Demgegenüber sind die aufgeklebten Elektroden für ein Langzeit-EKG oder die Sensoren der accelerometrischen Bewegungsmessung so unaufdringlich, dass sie lange Zeit einfach vergessen werden können.\nDie Grenzen des Ambulanten Monitoring und Assessment werden deutlich, wenn sich bei einzelnen Personen eine starke methodenbedingte Reaktivität zeigt oder wenn sich im Alltagsleben verschiedene Störeffekte überlagern. Inzwischen wurden Kontrollstrategien entwickelt, um möglichst die interessierenden von den störenden Effekten abzugrenzen.\n\nAlltagsnahe Aufzeichnungen könnten mehr als andere Untersuchungsmethoden in den Bereich der Privatsphäre eindringen, d. h. ein Teilnehmer könnte in nicht vorhersehbare Situationen kommen, deren Registrierung unerwünscht ist. Deshalb sollte diese Möglichkeit bereits angesprochen werden, wenn die informierte Zustimmung des Teilnehmers eingeholt wird. Außerdem könnte es, insbesondere bei Audio-Aufzeichnungen vorkommen, dass unabsichtlich andere Personen einbezogen werden. Solche problematische Aspekte sind rückblickend zu klären – bis zum vollständigen Löschen von Daten.\n\nDas ambulante Assessment kann Fehlschlüsse vermeiden helfen und bleibt die wichtigste Methodik, wenn es auf die psychologische Situation, die Verhaltensunterschiede oder die Verhaltensstörungen im alltäglichen Leben ankommt.\n\n\n\n"}
{"id": "3045796", "url": "https://de.wikipedia.org/wiki?curid=3045796", "title": "MDGRAPE-3", "text": "MDGRAPE-3\n\nMDGRAPE-3 ist ein, vom Forschungsinstitut \"RIKEN\" in Japan hergestellter, Supercomputer. Das System wurde speziell für Moleküldynamik-Simulationen entworfen, genauer für die Berechnung von Proteinstrukturen.\n\nMDGRAPE-3 besteht aus 201 Einheiten mit jeweils 24 MDGRAPE-3-Chips (insgesamt 4824) und zusätzlichen Dual-Core-Intel-Xeon-Prozessoren (Codename „Dempsey“), die als Host-Maschinen dienen.\n\nIm Juni 2006 verkündete RIKEN die Fertigstellung des Systems, das eine Billiarde Gleitkommaoperationen pro Sekunde (1 Petaflop/s) durchführen kann. Das ist vergleichbar mit der Schnelligkeit des IBM Roadrunner, der die TOP500-Liste der schnellsten Supercomputer vom November 2008 anführt. Da MDGRAPE-3 kein System für allgemeine Berechnungen ist, sondern spezielle Aufgaben bearbeitet, konnte er sich wegen der Inkompatibilität zu LINPACK nicht für die TOP500-Liste qualifizieren. Deshalb wird das System oft als ein Beispiel für die negativen Aspekte LINPACKs genannt.\n\n"}
{"id": "3045857", "url": "https://de.wikipedia.org/wiki?curid=3045857", "title": "Nixdorf System 820", "text": "Nixdorf System 820\n\nSysteme vom Typ Nixdorf 820 gehörten einer Modellfamilie an, die ab 1965 vom Labor für Impulstechnik (LFI), dem Vorgängerunternehmen der Nixdorf Computer (NCAG), entwickelt wurden. Zunächst trat das LFI nur als Zulieferer auf; vertrieben wurden die Rechner unter dem Namen Logatronic durch die Wanderer-Werke. Ab ca. 1968 übernahm Nixdorf Computer die Vermarkung selbst.\n\nDrei Produktphasen – Buchungsautomat, Magnetkonten-Computer und Terminalsysteme – lassen sich ausmachen. Das System erschien von 1967 bis 1979 und wurde in verschiedenen Modellvarianten über 40.000 mal verkauft. Zur Anwendung kam das System beispielsweise bei der Lohn- und Gehaltsabrechnung und der Fakturierung. Daneben wurde der Rechner (als 820P) aber auch erfolgreich für die Prozessautomatisierung und -steuerung eingesetzt, z. B. für Spulautomaten der Fa. Schlafhorst in Mönchengladbach.\n\nDurch das im Jahr 1952 in Essen von Heinz Nixdorf gegründete Labor für Impulstechnik verwirklichte der Computerpionier den Bau eines Elektronenrechners. Zunächst trat das Unternehmen nur als Zulieferer elektronischer Rechenwerke für Büromaschinenhersteller wie die Exacta Büromaschinen GmbH – ab 1963 Wanderer-Werke – in Köln und die Compagnie des Machines Bull in Paris auf. Die in den 1950er Jahren entwickelten Rechenanlagen basierten auf Röhrentechnik.\n\nIm Jahr 1965 kam die Wanderer Logatronic, die unter Beteiligung von Otto Müller in Transistortechnik konstruiert wurde, auf den Markt. Dieser Rechner wurde kontinuierlich weiterentwickelt und erschien 1968, nach der Übernahme der Wanderer-Werke durch Nixdorf, unter dem Namen Nixdorf 820.\n\nMit diesem nicht frei programmierbaren Kleincomputer erschloss sich die NCAG einen neuen Markt: die Mittlere Datentechnik, speziell für kleine und mittlere Unternehmen. Die Systemfamilie 820, mittlerweile vertrieben über das übernommene Geschäftsstellennetz der Wanderer-Werke, war ein großer wirtschaftlicher Erfolg und ebnete den Weg der NCAG zum viertgrößten Computerhersteller Europas.\n\nDie technischen Hauptbestandteile des Nixdorf-820-Systems (Baujahr 1968) waren\n\n\n\nIm Foto ist neben dem Schreibwerk außerdem ein Kassettenrekorder erkennbar, der dazu diente, Daten frequenzmoduliert digital zu speichern.\n\nDer Magnetkernspeicher, intern als Leb(end)-Speicher bezeichnet, diente üblicherweise nur zur Aufnahme der Anwenderprogramme während der Entwicklung. War das Programm ausgetestet, wurde dies auf dem Einschubmodul für den Stäbchenspeicher (intern als Festspeicher bezeichnet) „gefädelt“. Spätere Korrekturen und Erweiterungen waren deshalb nur sehr schwierig zu handhaben, indem intern ein „Programmsprung“ an einen unbenutzten Speicherbereich gemacht, dort die gewünschte Programmsequenz eingebaut wurde, und anschließend zur ursprünglichen Programmsequenz zurückgesprungen wurde, und dabei natürlich den fehlerhaften Code zu überspringen. Diese Methode wurde als „Rucksack“ bezeichnet. Da eine Trennung von Programm- und Datenspeicher vorlag, basierte die 820 auf einer Harvard-Architektur. Sie war somit im Gegensatz zu Von-Neumann-Rechnern nicht \"frei programmierbar\". Die verwendeten Festspeicher hatten allerdings den Vorteil, dass der Rechner nach dem Einschalten sofort betriebsbereit war und keine Programme geladen werden mussten.\n\nFür die Datenein- und -ausgabe standen Lochkartenleser/-stanzer, Lochstreifenleser/-stanzer, Lochstreifenkartenleser sowie Magnetkonten zur Verfügung.\n\n\n"}
{"id": "3053381", "url": "https://de.wikipedia.org/wiki?curid=3053381", "title": "JUGENE", "text": "JUGENE\n\nJUGENE (\"Jülich Blue Gene\") war ein Supercomputer vom Typ BlueGene/P im Forschungszentrum Jülich und Nachfolger des dort bis 2008 betriebenen JUBL. Der Rechner gehörte zum Institut \"Jülich Supercomputing Centre\" (JSC) und zum virtuellen \"Gauss Centre for Supercomputing\".\n\nMit 294.912 auf 850 MHz getakteten Power-PC-450-Kernen und 144 Terabyte Hauptspeicher – jeder der 73.728 Knoten enthielt einen Quad-Core-Prozessor sowie zwei Gigabyte RAM – in 72 Schränken erbrachte der Rechner eine Leistung von 825,5 TFLOPS (R).\n\nFinanziert wurde die Anschaffung vom Forschungszentrum, dem Bundesland Nordrhein-Westfalen, dem \"Bundesministerium für Bildung und Forschung\" sowie der \"Helmholtz-Gemeinschaft\". Der Leiter des JSC Thomas Lippert betonte „\"das Besondere an unserem JUGENE ist der im Vergleich zu anderen Systemen äußerst geringe Stromverbrauch bei allerhöchster Rechenleistung\"“ – ein BlueGene/P-System soll nach Herstellerangaben ca. 0,35 GFLOPS/Watt leisten und damit fast doppelt so effizient sein wie das an gleicher Stelle benutzte Vorgängermodell.\n\nDie offizielle Einweihung in Anwesenheit des nordrhein-westfälischen Ministerpräsidenten Jürgen Rüttgers erfolgte am 22. Februar 2008; gleichfalls an diesem Tag ging wenige Stunden später der ebenfalls zivile, aber noch schnellere \"Ranger\" am \"Texas Advanced Computing Center\" (TACC) der \"University of Texas at Austin\" offiziell in Betrieb.\n\nDie damalige erste Version von JUGENE erreichte mit 16 Racks eine offizielle LINPACK-Leistung von 167,3 TFLOPS (R) und sicherte sich als schnellster zivil genutzter Rechner Platz 2 der TOP500-Liste vom November 2007.\n\nDas Upgrade auf die letzte Ausbaustufe wurde am 26. Mai 2009 eingeweiht und ab 1. Juli 2009 produktiv genutzt. Die Marke von 1 Petaflops (R) wurde im Mai 2009 erstmals durchbrochen – JUGENE wird daher als der erste europäische Petaflops-Rechner bezeichnet.\n\nJUGENE wurde 2012 abgeschaltet und ab Mai 2012 durch das Blue Gene/Q-System JUQUEEN ersetzt, das in der 39ten TOP500-Liste vom Juni 2012 den 8. Platz belegt.\n"}
{"id": "3054490", "url": "https://de.wikipedia.org/wiki?curid=3054490", "title": "Split Brain (Informatik)", "text": "Split Brain (Informatik)\n\nSplit Brain ist in der Informatik ein unerwünschter Zustand eines Computerclusters, bei dem alle Zwischenverbindungen zwischen den Clusterteilen gleichzeitig unterbrochen sind.\n\nGrundsätzlich unterscheidet man zwischen\n\nZur Koordination der Transaktionen im Cluster wird in der Regel ein Cluster Interconnect oder ein Quorum verwendet – je nach eingesetzter Technologie. Wird die Verbindung zwischen einem oder mehreren Teilen des Clusters über diesen Weg unterbrochen, kann keines noch unterscheiden ob es sich um einen partiellen Ausfall oder eine Trennung handelt. Alle dieser (nun isolierten) Clusterfragmente arbeiten für sich weiter, um die Bereitstellung des Dienstes (auch „Service“) aufrechtzuerhalten. Da im Normalfall die Netzwerkanbindung in das öffentliche Netz (also in Richtung Benutzer) noch funktioniert, ergeben sich Probleme:\n\nDas Grundproblem von \"Split Brain\" ist der Umstand, dass mindestens zwei Teile noch funktionieren, jedoch keine Koordination zwischen ihnen mehr möglich ist. Während dies bei reinen Lesezugriffen noch nicht unmittelbar problematisch scheint, führt ein Schreibzugriff zu massiven Konflikten: Die Schreibvorgänge verteilen sich über die (zwar funktionierenden aber voneinander isolierten) Teile des Clusters, wobei aber die Logikschicht (engl. \"middle tier\") bzw. der Benutzer nichts Ungewöhnliches bemerkt; das Cluster verhält sich im Normalbetrieb aus Anwendersicht gleich. Dabei kann jedoch durch die unterbrochene Zwischenverbindung der von Knoten/Teil A geschriebene Block durch Knoten/Teil B nicht gelesen werden – und umgekehrt. \n\nDie Datenstände laufen daher auseinander, die Konsistenz der Daten ist nicht mehr gewährleistet. Eine Wiederherstellung aus dieser Situation ist im Normalfall nur unter indiskutablem Zeitaufwand machbar oder gar gänzlich unmöglich.\n\nDie Grundlage aller Gegenmaßnahmen ist der gleichzeitige Einsatz von Quorum und Cluster Interconnect: Die Trennung einer der beiden Koordinationsmöglichkeiten erlaubt noch immer die Unterscheidung zwischen Teilung und partiellem Ausfall. \n\nDie Abdeckung paralleler Ausfälle (gleichzeitiger Verlust \"mehrerer\" betriebskritischer Teile) erhöht den Aufwand enorm – im Falle Split-Brain-Verhütung fängt beispielsweise die Verwendung mehrerer Quoren und der Einsatz parallelisierter / gebondeter Interconnects den Ausfall von Interconnect und einer Storage ab.\n\nIm Zusammenspiel zwischen Quoren und Interconnect ist eine zuverlässig automatisierte Entscheidungsfindung notwendig, so wird beispielsweise bei der Oracle Clusterware die Entscheidung folgendermaßen getroffen:\n\nEs überlebt nach dem Verlust des Interconnects (Reihenfolge wird beachtet):\n\nUm nicht das eben durch mehrere Quoren vermeintlich gelöste Problem zu wiederholen (Ich sehe zwei Quoren, Du siehst zwei Quoren, aber sehen wir zwei verschiedene Paare!) verwendet Oracle eine ungerade Anzahl dieser Quoren. Alle Knoten die sich im Quorum treffen, müssen sich auch im Interconnect sehen. Ist dies nicht der Fall, entscheiden die Last- und Topologieinformationen in der Voting Disk über Leben und Tod des Knotens. Die eben genannte Entscheidungsliste wird erweitert:\n\n"}
{"id": "3054746", "url": "https://de.wikipedia.org/wiki?curid=3054746", "title": "Power Macintosh 7200", "text": "Power Macintosh 7200\n\nDie beiden Rechnermodelle des Power Macintosh 7200 (Code-Name: „Catalyst“) sind Personal Computer des Unternehmens Apple und gehören zur Power-Macintosh-Serie. Sie wurden als Nachfolger der Vorgänger 7100/66 (7100/66 AV) und 7100/80 (7100/80 AV) 1995 in Cupertino (USA) vorgestellt. Der 7200/90 wurde in Japan mit den gleichen Systemprofilen als Power Macintosh 7215 verkauft.\n\nDie 7200er waren die kleinsten PCI-Rechner von Apple und wurden für rund 2.400–2.700 DM angeboten. Aufgrund des günstigen Preises sowie der technischen Ausstattung lagen die 7200er in der Käufergunst auch vor den aktuellen PowerPC-Performas 5200 und 6200 in den eigenen Reihen. Der Power Macintosh 7200 war als günstige Alternative zu der zeitgleich eingeführten Modellreihe Power Macintosh 7500, welche im gleichen Gehäuse erschienen, vorgesehen. Über diesen beiden Rechnern lagen damals der ebenfalls gleichzeitig erschienene Power Macintosh 8500 und das etwas ältere Spitzenmodell Power Macintosh 9500. Dieser teurere 9500 hatte als erster Mac den Umstieg der Firma Apple von NuBus auf PCI eingeläutet, mit dem 7200, 7500 und 8500 wollte Apple jetzt die für Macintosh-Rechner neue Technik günstiger einem breiteren Markt zugänglich machen. Das Unternehmen musste auf den Markt reagieren, hatte man die Entwicklung zu PCI, das damals bei PCs längst zur Regelausstattung gehörte, eigentlich bereits verpasst.\n\nAls zweite Mac-Generation mit auf der Hauptplatine verlöteter 601 CPU, hatten die 7200er Taktfrequenzen von 75–120 MHz, drei PCI Steckplätzen und vier RAM-Steckplätzen, 1 MB VRAM und drei freie Steckplätzen zu dessen Erweiterung. Der 7200 wurde in den USA nur im Desktop-, in Deutschland auch im Minitowergehäuse ausgeliefert. Die Hauptspeicher-Steckplätze waren 1995 mit zwei Modulen von je acht MByte ausgestattet, so blieben also noch zwei für weitere DIMMs frei. Obwohl theoretisch die Möglichkeit bestand, bei einer 21-Zoll-Auflösung 16,7 Mio. Farben darzustellen, schaffte der 7200 die größte Auflösung nur mit 16 Bit Farbe. Dies war auf einen Fehler in der Videologik zurückzuführen. Ohne zusätzlich geordertes VRAM konnte die Videologik zumindest 16-Zoll-Monitore mit 32768 Farben ansteuern. Die Bildwiederholungsrate lag immer bei 75 Hz, was damals auf der Höhe der Zeit war.\n\nDa bei den Power Macintoshs 7200 einige QuickDraw-Funktionen der werksseitig eingebauten Grafikkarte gesondert beschleunigt worden waren, bot auch die mitgelieferte Darstellungsmöglichkeiten gute Leistung. Dazu bot der 7200/90 mit seinem Bustakt von 45 MHz deutlich mehr als die damaligen DOS- und Windows-95-Rechner mit ihrem Standardwert von 33 MHz, so dass sich bei diesem Modell die Durchsatzrate nochmals erhöhte.\n\nDie Power Macintoshs 7200 wurden mit dem Betriebssystem MacOS 7.5.2 ausgeliefert.\n\nGelobt wurde seinerzeit das schnell und problemlos zu öffnende Gehäuse des 7200. Zum Einbau von Steckkarten und zur Aufrüstung des Video- oder Hauptspeichers genügen nun wenige Handgriffe. Im Vergleich zu seinem Vorgänger, dem Power Mac 7100, hatte man noch einen weiteren Platz für ein 3,5-Zoll-Speichermedium in voller Bauhöhe geschaffen.\n\n\"Apple Power Macintosh 7200/75\" (Nachfolger des entsprechenden 7100)\n\"Apple Power Macintosh 7200/90\" (Nachfolger des entsprechenden 7100)\n"}
{"id": "3054841", "url": "https://de.wikipedia.org/wiki?curid=3054841", "title": "Power Macintosh 8100", "text": "Power Macintosh 8100\n\nDer Power Macintosh 8100 (Code-Namen: „Cold Fusion“, „Flagship“) ist ein Personal Computer des Unternehmens Apple und gehört zur Power-Macintosh-Serie. Zusammen mit dem preisgünstigen 6100er und den Mittelklasserechnern Power Macintosh 7100 wurde er im März 1994 am Markt eingeführt. In Japan erschien der hochgerüstete 8100/110 unter der Bezeichnung Power Macintosh 8115/110.\n\nDer 8100er bildete das Spitzenmodell der neuen Serie und wurde im Towergehäuse ausgeliefert. Die Modelle dieser Reihe richteten sich an Graphikstudios, Werbeagenturen und Druckereien, welche mehr Leistung forderten.\n\nDer 8100/80 erschien mit einem Motorola-PowerPC-601-Hauptprozessor und einer Taktrate von 80 MHz. Er kostete in den USA 4200 US-Dollar. Sieben Monate nach seiner Einführung legte Apple mit dem nun parallel gebauten Power Macintosh 8100/110 die Latte nochmals höher. Mit einer Taktrate von 110 MHz, einem Prozessor Motorola PPC 601+ sowie einem Bustakt von 36,7 MHz lag er über den Parametern der handelsüblichen DOS-PCs. Der 8100/110 besaß drei NuBus-Steckplätze für Erweiterungsmöglichkeiten und wurde mit dem Betriebssystem MacOS 7.5 (PowerPC Enabler V1.1.1, AppleTalk 58.1.3) ausgeliefert. Er lässt sich bis MacOS 9 aufrüsten.\n\nOffizieller Nachfolger des 8100/80 wurde ab Januar 1995 der 8100/100, der einen mit 100 MHz getakteten Motorola PPC 601 mit einem Bustakt von 33,3 MHz besaß. Der Power Macintosh 8100/100 war mit dem System MacOS 7.5 ausgestattet und ist bis MacOS 9 verwendbar.\n\nDie lieferbaren AV-Versionen besaßen eine werksmäßig eingebaute Audio-Video-Karte (S-Video und RCA Video in/out).\n\n"}
{"id": "3054859", "url": "https://de.wikipedia.org/wiki?curid=3054859", "title": "Pro Tools Mixer", "text": "Pro Tools Mixer\n\nDer Pro Tools Mixer ist eine Komponente der Audioeditor-Software Pro Tools. Er dient zur Zusammenfassung, Ausgabe und technischen, sowie klanglichen Bearbeitung der Audio- und MIDI-Signale. Der Aufbau kann individuell an den momentanen Bedarf angepasst werden. Für die unterschiedlichen Signale stehen fünf verschiedene Mixerspuren zur Verfügung: Tonspur, Aux Input, Instrumentenspur, MIDI-Spur und Master Fader. Die Mixerspuren, ausgenommen der MIDI-Spur, können einkanalig (Mono) oder mehrkanalig (Stereo, Raumklang) ausgeführt werden. Die Berechnung erfolgt bei Pro-Tools-LE-Systemen nativ und bei den Pro Tools TDM Systemen auf den Audiokarten durch die digitalen Signalprozessoren (DSP). Die Struktur des Pro Tools Mixers ist sehr an den Aufbau eines analogen Mischpultes angelehnt. Die maximale Größe des Mixers ist begrenzt, dabei wird die maximale Anzahl der Tonspuren durch die verfügbaren Voices des Systems bestimmt. Voices werden benötigt, um Audiosignale mittels der Tonspur aufzunehmen oder wiederzugeben. Bei TDM-Systemen werden große Mixer auf mehrere DSP-Prozessoren verteilt.\n\nUnter der Verwendung der Mischumgebung ist die Anzahl der Summierstufen (Mixer-Spuren) variabel. Pro Tools erzeugt beim Öffnen oder Erzeugen einer neuen Sitzung (Projekt) einen TDM-Mixer. Der Ausdruck \"mixer channel\" bezieht sich auf alle Spurtypen, ausgenommen die Master Fader, die keine zusätzliche DSP-Leistung benötigen. Wenn eine bestimmte Anzahl an “mixer channels” überschritten wird, wird ein zusätzlicher DSP eingebunden, um zusätzlich Kapazität für den Mischprozess bereitzustellen. Innerhalb der Pro-Tools-Umgebung ist es möglich, einen eigenen Mixeraufbau hinsichtlich der Anzahl der Summierstufen zusammenzustellen. So passt sich die benötigte DSP-Leistung und die Anzahl der verwendeten Zeitschlitze an den tatsächlichen Bedarf für den Mixer an.\n\nDie Summierstufen werden in Form von Mixer-Modulen von der DAE (Digidesign Audio Engine) nach Bedarf geladen. Es gibt zwei unterschiedliche Mixer-Modul-Typen. Zum einen das Stereo- und zum anderen das Raumklang-Zusatzmodul.\nInsgesamt stehen für die TDM-I-Technologie drei verschiedene Mixer-Module zur Verfügung: das Raumklang-Mixer-Modul, der 24 Bit optimierte Mixer und der 16 Bit optimierte Mixer.\nFür die TDM-II-Technologie stehen vier verschiedene Mixer-Module zur Verfügung:\nStereo, Surround, Stereo Dithered und Surround Dithered. Durch das Dithering wird die maximale Anzahl an Eingängen etwas verringert.\n\nDie Stereo-Version des Mixers kann eine variable Anzahl an Eingängen auf maximal zwei Ausgänge zusammenmischen. Nach dieser Definition liegt der Mixer in der Form Nx2 vor, wobei N für die variable Anzahl an Eingängen steht und die Ziffer \"2\" die maximale Anzahl an Ausgängen definiert. Wenn zum Beispiel acht Mono-Tonspuren auf einen Stereoausgang (zwei Kanäle) zugewiesen werden wird ein einfacher 8(N)x2-Mixer benötigt.\n\nDer Raumklangmixer kann eine variable Anzahl an Eingängen und eine variable Anzahl an Ausgängen besitzen. Bei einer 7.1-Raumklangmischung bestehend aus 10 Mono-Tonspuren und zwei Stereo-Tonspuren wird ein Raumklang-Mixer-Modul mit dem Format \"14(N)x8\" benötigt.\n\nJeder Mixer besitzt eine von der gewählten Abtastrate abhängige Gesamtkapazität (Größe). Jeder Hauptmixer benötigt eine eigene DSP-Prozessorzuweisung. Übersteigt die Anzahl an zu summierenden Kanälen die Gesamtkapazität eines Hauptmixers werden automatisch zusätzliche Hauptmixer und so genannte Submixer erzeugt. Ein Submixer summiert die Ausgänge mehrere Hauptmixer auf die mit den Hauptmixern definierte Anzahl an Ausgangskanälen. Dieses bedeutet wenn maximal 68 Mixereingänge und 2 Mixerausgänge möglich sind (68(N)x2) und die Anzahl an Eingängen überschritten wird, wird ein neuer Hauptmixer (68(N)x2) und ein Submixer (4(N)x2), entsprechend den Ausgängen des Hauptmixers erzeugt. Der zusätzliche Hauptmixer ermöglicht nun die Verwendung von mehr als 68 Eingängen. Der Submixer summiert die beiden Stereoausgänge der Hauptmixer auf den Stereoausgang des Submixers.\nDer maximale positive Bereich der Kanalfader kann vom Anwender gesetzt werden. Hierzu stehen zwei Möglichkeiten (+6 dB und +12 dB) bereit, die beim Erstellen einer Pro Tools Session selbst gewählt werden können. Der maximale Regelbereich der Kanalfader hat direkten Einfluss auf den Headroom des Systems.\n\nDie Mixer-Module bei den Pro-Tools-MIX-Systemen bieten einen Aussteuerungsreserve von 30 dB. Bei diesem Mixer ist es möglich sowohl die Eingänge, als auch die Ausgänge der Summierstufen zu übersteuern. Durch die Verwendung eines Master Faders ist es möglich, den Signalpegel auf den Ausgängen zu reduzieren, und dadurch Übersteuerungen zu verhindern. Jedoch hat die Verringerung des Ausgangspegels keinen Einfluss auf die Eingänge der Summierstufe. Dadurch sind Übersteuerungen, die an den Eingängen der Summierstufe entstehen, nur durch eine Reduzierung der Eingangspegel zu verhindern.\n\nDie Mixer-Module werden auf einem reservierten Bereich der DSPs berechnet. Dadurch ist es möglich die Signale mit konstanter 48-Bit-Auflösung zu summieren. Selbst bei der Verringerung des Signalpegels durch den Fader bleibt die volle Auflösung erhalten. Die 48-Bit-Auflösung ermöglicht einen Aussteuerungsreserve von 48 dB. Sogar bei einer Faderposition von +6 dB kann kein Clipping auf der Eingangsseite des Summierbusses entstehen. Im Gegensatz dazu kann die Ausgangsseite der Summierstufe, welche die Signale auf einen physikalischen Ausgang der Hardware oder den TDM-Bus ausgibt, dennoch übersteuern. Um diese Übersteuerung an den Ausgängen des Mixers zu verhindern, kann ein Master Fader verwendet werden, der keine Qualitätsverluste zur Folge hat.\n\nBei den Dithered Mixer wird an jedem summierten Ausgang, sowohl an den physikalischen Ausgängen, wie auch auf den internen Bussen, ein unkorrelierter Dither verwendet. Der Dither verhindert, dass es zu hörbaren Artefakten an den Ausgängen zu dem 24-Bit-TDM Bus oder 24 Bit physikalischer Ausgang kommt, wenn das Signal die 48-Bit-Mixer-Ebene verlässt. Die möglichen Artefakte können entstehen, wenn zum Beispiel geringe Signalpegel bei der Wandlung von der 48-Bit- auf die 24-Bit-Ebene abgeschnitten werden. Die Artefakte werden nun durch das Dithering mit gleichförmigem weißem Rauschen überdeckt.\n\nDurch die Hinzugabe des unkorrelierten Dither wird mehr DSP-Leistung benötigt. Die maximale Anzahl an Mixerkanälen vermindert sich um etwa 15 %.\nDie Pro-Tools-Software bietet die Möglichkeit selbst zu wählen, welcher Mixer-Modul verwendet werden soll.\n\nDas Dithering des Mixer Plug Ins ist auf 24 Bit ausgelegt. Für den Fall, dass eine weitere Bittiefenwandlung indiziert ist, kann es sinnvoll sein, ein zusätzliches Dither-Modul für die fertige Mischung zu verwenden,\n\nAusgehend von einem Signal, das von einem Analog-Digital-Wandler einer Schnittstelle mit 24 Bit digitalisiert wurde und demnach einen theoretischen Dynamikumfang von etwa 144 dB aufweist (1 Bit ermöglicht die Darstellung von 6,02 dB), kann die Funktion des Mixers näher erläutert werden.\n\nDas nun digitalisierte Signal wird auf den 24-Bit-TDM-II-Bus gelegt und zu den DSP-Prozessoren auf den Audiokarten transportiert. Die Anzahl der zur gleichen Zeit möglichen transportierten Signale wird durch die Anzahl der Zeitschlitze definiert. Auf den DSP-Prozessoren kann das Signal nun durch Zusatzmodule bearbeitet (EQ, Kompressor usw.) werden. Die Module arbeiten nach dem so genannten “double-precision”-Prinzip. Hierbei wird ein 24-Bit-Signal (144 dB) mit 48 Bit (288 dB) bearbeitet. Durch die erweiterte Dynamik sind mehr Möglichkeiten zur Darstellung der Signalbearbeitungen verfügbar. Dieser Effekt macht sich besonders durch die neuberechneten Dezimalstellen des Signals bemerkbar. Bei jeder Signalveränderung, zum Beispiel Klangsteller, Kompressor, Veränderung des Signalpegels usw., wird das ursprüngliche Signal mit den Veränderungen multipliziert. Je mehr Darstellmöglichkeiten bestehen, umso höher kann die Klangqualität sein. Das Signal wird nach dem Modul durch einen Dither und anschließend wieder auf den 24-Bit-TDM-Bus geleitet.\n\nDer Mixer ist ein Zusatzmodul, das von der DAE (Digidesign Audio Engine) geladen wird. Der Mixer bearbeitet das Signal mit 48 Bit und bietet daher eine Bearbeitungsdynamik von 288 dB. Für den Fall, dass der Mixer auf mehrere DSP-Prozessoren verteilt werden muss, bleibt diese Dynamik stetig erhalten. Für die Verbindung von mehreren Mixer-Modulen werden zwei Zeitschlitze auf dem 24-Bit-TDM-Bus für jede Verbindung verwendet.\n\nDie 24 zusätzlichen Bit ermöglichen einen zusätzlichen Dynamikbereich für die Kanalfader über und unter der originalen 24-Bit-Auflösung. Dadurch können 128 korrelierte Spuren mit voller Aussteuerung (24 Bit) und maximaler Kanalfaderstellung (+ 12 dB) summiert werden, ohne den Eingang der Summierstufe zu übersteuern. Im Umkehrschluss ist es möglich, die Kanalfader auf nahe −90 dB zu regeln, bevor das Signal nicht mehr mit vollen 24 Bit dargestellt werden kann.\n\nAm Eingang des Mixers werden die 24-Bit-Originalsignale mit 24-Bit-Pegel und Panoramakoeffizienten multipliziert. Dadurch wird ein 48-Bit-Resultat erzielt. In dem nun kreierten 48-Bit-Wort ist die originale 24-Bit-Information enthalten. Durch das Register des Mixers von 56 Bit ist es nun möglich den Signalpegel begrenzt zu erhöhen oder zu mindern ohne an Qualität zu verlieren. Die Fehler entstehen erst für Signale, die einen Signalpegel von etwa −240 dB aufweisen. Der entstehende Quantisierungsfehler bei der “double presicion”-Technik liegt bei etwa ein Millionstel dB. Durch das 56-Bit-Register sind nicht nur 24 zusätzliche Bits zur Darstellung von leisen Signalen vorhanden, sondern es werden 8 Bit als Aussteuerungsreserve für die Summierung bereitgestellt.\n\nBeim Pro-Tools-Mixer spielt es keine Rolle, ob die Pegel am Eingang (Kanalfader) oder Ausgang (Masterfader) verändert werden, da durch diese Veränderung kein Einfluss auf Signaldarstellung und Weiterleitung ausgeübt wird.\n\nBei einem Mixer mit maximaler Wortlänge von 24 Bit steht keine Aussteuerungsreserve und kein zusätzlicher Bereich für die Darstellung von negativen Pegelveränderungen zur Verfügung. Dadurch wird bei einer Summierung von mehreren Signalen der Bereich der Darstellung für kleine Signalpegel weiter verkleinert. Bei einer Verdoppelung der Signalanzahl reduziert sich der Dynamikbereich um 1 Bit. Der Grund liegt darin, dass der Pegel bei einer Verdoppelung der Anzahl von korrelierten Signalen um 6,02 dB angehoben wird.\n\nBei der 48-Bit-Mixer-Struktur war es möglich 128 korrelierte Signale zu summieren ohne einen Qualitätsverlust zu erzielen. Der Grund liegt im Dynamikbereich in dem das summierte Signal mit der ursprünglichen 24-Bit-Wortbreite dargestellt werden kann. Dieser Bereich erstreckt sich von + 48 dB bis 0 dB (Headroom) und von 0 dB bis −90 dB.\n\nIm Vergleich zum vorherigen Beispiel (48-Bit-Mixer-Struktur), sieht es bei einer 24 Bit Mixerarchitektur mit gleichen Grundvoraussetzungen etwas anders aus. Bei 128 korrelierten Signalen wird der summierte Signalpegel um 42,17 dB ansteigen. Da keine Aussteuerungsreserve zur Verfügung steht, muss der gesamte Signalpegel um 42,17 dB reduziert werden. Es stehen keine zusätzlichen Bits für die richtige Darstellung des pegelreduzierten Signals zur Verfügung, daher werden die letzten acht Bits abgeschnitten und aus dem ursprünglichen 24-Bit-Signal wird ein 16-Bit-Signal.\n\nAm Ausgang des Mixers wird das Signal aus der 48-Bit-Mixerebene wieder in die 24-Bit-Ebene gewandelt. Hierbei wird der originale 24 Bit Signalanteil, der sich zwischen maximal +48 dB und minimal −90 dB befindet, in voller Qualität auf den TDM-II-Bus ausgegeben. Über den TDM-Bus gelangt das Signal entweder direkt zu weiteren DSP-Prozessoren oder zu einem physikalischen Ausgang über einen D/A-Wandler.\n\n\n\n"}
{"id": "3054876", "url": "https://de.wikipedia.org/wiki?curid=3054876", "title": "Power Macintosh 8200", "text": "Power Macintosh 8200\n\nDer Power Macintosh 8200 (Code-Name: „Catalyst“) ist ein Personal Computer des Unternehmens Apple und gehört zur Power-Macintosh-Serie. Er ist eine überarbeitete Ausgabe des Power Macintosh 7200 und wurde ausschließlich im Towergehäuse ausgeliefert. Aufgrund seiner Basis gehört er zur Modellreihe 7200. Als Zielgruppe waren Graphiker, Agenturen und Druckereien angepeilt, die relativ preisgünstig einen anspruchsvolleren Mittelklasserechner erwerben wollten. Der Power Macintosh 8200 wurde nur in Europa vertrieben.\n\nMit Einführung des 8200 stellte Apple alle Bauarten des 7200 ein. Als einzige Ausnahme blieb das 90-MHz-Modell mit vierfachem CD-ROM-Laufwerk im Programm.\n\nDer 8200 wurde in zwei Leistungsvarianten angeboten. Beide PCI-Macs besaßen den bewährten Hauptprozessor Motorola PowerPC 601. Der 8200/100 besaß ohne L2-Cache 8 MB RAM, beim 8200/120 wurden 16 MB RAM und 256 KB L2-Cache verbaut. Da der Prozessor auf die Hauptplatine gelötet ist, gestaltet sich ein Austausch schwierig.\n\nDie Rechner wurden mit dem System macOS 7.5.3 ausgeliefert und sind bis MacOS 9.1 aufrüstbar.\n\nDas Gehäuse ist wie beim 7200 sehr gut zugänglich, und die diversen Steckplätze ermöglichen ein vielfältiges Aufrüsten.\n\n\"Apple Power Macintosh 8200/100\" (erweiterter 7200)\n\n\"Apple Power Macintosh 8200/120\" (erweiterter 7200)\n"}
{"id": "3055003", "url": "https://de.wikipedia.org/wiki?curid=3055003", "title": "SYSTAT", "text": "SYSTAT\n\nSYSTAT ist der Name einer Statistik-Software sowie verschiedener Unternehmen, die dieses Produkt entwickelt und vertrieben haben bzw. dies noch tun.\n\nDie Software Systat wurde ab den späten 1970er Jahren von Leland Wilkinson, einem Lehrbeauftragten für Psychologie an der University of Illinois at Chicago entwickelt. Er gründete zusammen mit seiner Frau im Jahr 1983 die Firma Systat .\n\nLeland veräußerte Systat zum Jahr 1995 an das Konkurrenzunternehmen SPSS, dessen Geschäftsbereich SPSS Science Division das Produkt Systat weiter im wissenschaftlichen Markt vertrieb. Wilkinson selbst übernahm bei SPSS eine leitende Position und den zuletzt 50 Mitarbeitern wurden bei SPSS ebenfalls Arbeitsplätze angeboten. Ab 2002 konzentrierte sich SPSS auf den Bereich der Wirtschaftsanalyseprogramme und verkaufte Systat an das in Bangalore in Indien ansässige und börsennotierte Unternehmen Cranes Software. Für Vertrieb und Anwenderunterstützung gründete Cranes in den Vereinigten Staaten die Systat Software Inc. mit Sitz in San Jose in Kalifornien sowie weltweit weitere Landesgesellschaften. Systat vertreibt neben dem gleichnamigen Produkt auch noch das Softwarepaket SigmaStat, das für statistische Analyse vorgesehen ist und den Anwender mit verschiedenen automatisierten Funktionen unterstützt.\n\nDie Entwicklung von Systat wurde anfänglich auf Wang-Großrechnern durchgeführt. Bei Erscheinen der ersten Einzelplatzrechner auf Mikroprozessorbasis wechselte Wilkinson jedoch auf diese Plattform, da die Kompilationszeiten dort weit geringer waren. Die Software war eines der ersten Produkte aus diesem Marktsegment, das auf einen Kopierschutz verzichtete. Auch eine der Windowsversion weitgehend funktionsgleiche Version für Mac OS war relativ früh auf dem Markt.\n\nWährend der Quellcode der Software des Produktes bis 2005 noch in Fortran geschrieben war, wurde Systat ab jenem Zeitpunkt in der Version 11 erstmals in einer in C++ geschriebenen Version veröffentlicht. Die Version 12 erschien im Frühjahr 2007 mit Verbesserungen bei der Benutzerschnittstelle und verschiedenen neuen Leistungsmerkmalen.\n\nSystat unterstützt verschiedene Funktionen: \n\n\"SYSTAT\" war auch der Name eines Befehls in den Betriebssystemen TOPS-10 und RSTS/E der Firma DEC, der dem Computernutzer den aktuellen Status des laufenden Betriebssystems anzeigte. Dazu gehörten die eingeloggten Benutzer, auf dem Computer laufende Prozesse, laufende I/O-Operationen sowie andere wichtige System Management Informationen. \n\nDer Befehl Systat ist darüber hinaus in Betriebssystemen vorhanden, die auf BSD zurückgehen . \n\n"}
{"id": "3055160", "url": "https://de.wikipedia.org/wiki?curid=3055160", "title": "SICAD/open", "text": "SICAD/open\n\nSICAD/open (Abkürzung für \"Siemens Computer Aided Design\") ist eine professionelle GIS-Software. Sie wird seit 1979 von Siemens entwickelt und in erster Linie im kommunalen Bereich für den bildlichen Teil des Liegenschaftskatasters (ALK) einschließlich Stadtgrundkarte, Baumkataster, Statistik (Kleinräumige Gliederung), Stadtplanung (B-Plan, Flächennutzungsplan, Landschaftsplan) sowie im Bereich Ver- und Entsorgung (Gas, Strom, Wasser, Fernwärme) eingesetzt.\n\nSICAD/open ist ein offenes System, lauffähig auf Unix-, Linux- und Windows-Betriebssystemen. Zur Verwaltung der Geodaten dienen relationale Datenbanksysteme (z. B. Oracle oder Informix). Die Clients kommunizieren über TCP/IP mit den Servern.\n\nSeit 1998 wird das Produkt durch das eigenständige Unternehmen SICAD GEOMATICS vertrieben.\nDie \"SICAD Geomatics GmbH & Co. oHG\", damals eine 100-prozentige Tochter von \"Siemens Business Services\" aus München, wurde 2002 von der \"AED Graphics AG\" aus Bonn übernommen, die seitdem unter dem Namen \"AED-SICAD GmbH\" firmiert.\n\nAED-SICAD hatte sich schon früh entschlossen, für den AdV-Standard ALKIS (Amtliches Liegenschaftskatasterinformationssystem) eine völlig neue Produktlinie auf Basis der Esri- ArcGIS Technologie zu erstellen. Deshalb wird das System SICAD/open nicht mehr weiterentwickelt. Es gab nur noch in längeren Abständen Fehlerberichtigungen, zuletzt Ende 2007 die Korrekturversionen \"SICAD/open 7.1A20\" für Windows- und \"SICAD/open 7.1A25\" für UNIX-Plattformen. Diese Versionen waren bei vielen Kommunen noch lange im Einsatz, sind inzwischen aber fast überall durch die neue 3A Produktlinie von AED-SICAD ersetzt worden.\n\n"}
{"id": "3058022", "url": "https://de.wikipedia.org/wiki?curid=3058022", "title": "Dinosaurier – Im Reich der Giganten", "text": "Dinosaurier – Im Reich der Giganten\n\nDinosaurier – Im Reich der Giganten (Originaltitel: \"Walking with Dinosaurs\") ist eine dokumentarische Fernsehserie der britischen BBC aus dem Jahr 1999 über Dinosaurier und andere Reptilien (Saurier) des Mesozoikums (Erdmittelalter). Die um mehrere Specials erweiterte sechsteilige Miniserie ist der erste Teil einer Trilogie (die Originaltitel beginnen mit \"Walking with\"…) über ausgestorbene Tiere, es folgten die \"Die Erben der Saurier\" und \"Die Ahnen der Saurier\". Die Erstausstrahlung im deutschen Fernsehen erfolgte am 11. November 1999 auf dem Sender ProSieben.\nSprecher der deutschsprachigen Fernsehserie und der DVDs ist der Schauspieler Otto Clemens.\n\n\"Dinosaurier – Im Reich der Giganten\" handelt, auf Grundlage paläontologischer Erkenntnisse, vom Leben auf der Erde zur Zeit des Erdmittelalters (Mesozoikum), dasjenige Erdzeitalter, in der die Dinosaurier das Leben auf dem Festland dominierten. In den sechs der erdgeschichtlichen Chronologie folgenden Teilen werden einige Dinosaurier und ihre Verwandten (Plesiosaurier und Flugsaurier) in Trias (u. a. \"Placerias\", \"Coelophysis\", \"Peteinosaurus\" und \"Plateosaurus\"), Jura (u. a. \"Diplodocus\", \"Allosaurus\", \"Stegosaurus\" und \"Liopleurodon\") und der Kreidezeit (u. a. \"Ornithocheirus\", \"Iguanodon\", \"Leaellynasaura\", \"Anatotitan\", \"Torosaurus\", \"Tyrannosaurus\") beschrieben.\n\nDiese Folge, die in der Obertrias spielt, erzählt von den Anfängen der Dinosaurier und zeigt sie zu dieser Zeit mit den Rauisuchidaen und Therapsiden um die Vorherrschaft um die Erde kämpfen. Im Mittelpunkt stehen die Coelophysis und ein Cynodontia Thrinaxodon-Paar. Der Zuschauer erlebt die erfolgreiche Jagd eines Postosuchus auf eine Herde Placerias und sieht, wie die Reptilien in Form des Peteinosaurus auch die Lüfte erobern. Dank ihrer Anpassungsfähigkeit und dem geringen Wasserbedarf gewinnen die Dinosaurier die Vorherrschaft über die Erde. Am Ende der Folge tauchen mit einer Plateosaurus-Herde die ersten großen Dinosaurier auf.\n\nIn dieser im späten Jura angesiedelten Episode stehen eine Diplodocus-Herde und ihr Sozialverhalten im Mittelpunkt. Ein Muttertier legt seine Eier im Wald ab, damit die Jungen vor großen Raubsauriern wie dem Allosaurus geschützt sind. Dennoch werden einige Jungtiere von einem Ornitholestes, einem kleinen Raubsaurier, gefressen. Einige Jahre später nähert sich die Gruppe aus jungen Diplodocus gefährlich nah dem Waldrand. Dabei werden sie Zeugen, wie ein Stegosaurus erfolgreich den Angriff eines Allosaurus abwehrt. Des Weiteren wird die Symbiose zwischen dem Diplodocus und dem Flugreptil Anurognathus thematisiert. Nach einem Waldbrand haben nur drei Jungtiere überlebt und nur das Erscheinen einer Gruppe Brachiosaurier rettet sie vor einem Angriff der Allosaurier. Ein weiteres Jungtier verschwindet, was mit ihm geschah, wird nicht angegeben. Die letzten beiden Jungtiere können sich einer Herde erwachsener Diplodocus anschließen.\n\n\nDiese Folge widmet sich hauptsächlich dem Leben in den Meeren zur Oberjura. Eine Herde Ophthalmosaurus wird auf dem Weg zu ihren Brutplätzen begleitet. Gefährdet werden sie durch Haie der Gattung Hybodus und den gigantischen Pliosaurier Liopleurodon. Der Cryptoclidus wird mit einer Lebensweise ähnlich der der heutigen Robben dargestellt. Neben den Meeresreptilien wird mit dem Rhamphorhynchus ein mariner Flugsaurier vorgestellt. Dieser ernährt sich von Fischen und dem Laich der Pfeilschwanzkrebse. Der einzige vorkommende Dinosaurier ist der als zwischen kleinen Inseln pendelnder Raubsaurier dargestellte Eustreptospondylus.\n\n\nIn dieser Folge wird ein altes Ornithocheirus-Männchen auf seinem letzten Flug begleitet. Er macht im heutigen Brasilien in der Nähe einer Kolonie von Tapejara Rast. Anschließend macht er sich auf in Richtung Nordamerika. Dabei wird eine an der Küste wandernde Dakotadon-Herde in Begleitung eines Polacanthus gezeigt. In Europa ankommend wird das alte Männchen Zeuge, wie eine Gruppe Utahraptoren einen Iguanodon töten. Mit dem Iberomesornis wird angedeutet, wie die Vögel die Flugreptilien verdrängen. Aufgrund von Hunger und Überhitzung verstirbt das Ornithocheirus-Männchen während des Paarungskampfes um Weibchen.\n\nDer Schwerpunkt dieser Episode liegt auf Flugsauriern und spielt in der Unterkreide.\n\n\nIn dieser Folge wird das Leben der Tiere beschrieben, die während der mittleren Kreidezeit auf dem Kontinent Antarktika leben. Protagonisten sind eine Herde Leaellynasaura, die über einen Zeitraum von einem Jahr begleitet werden. Der Australovenator, bezeichnet als Zwerg-Allosaurus, wird als der größte lokale Räuber präsentiert und erweist sich als andauernde Bedrohung für die Leallynasaura-Herde. Die wie Nomaden lebenden Muttaburrasaurus wandern nur im Sommer in die Gebiete der Leaellynasaura. Der Koolasuchus ist ein Relikt aus vergangenen Zeiten, in denen Amphibien die Welt beherrschten. Er macht Jagd auf kleine Dinosaurier. Das Steropodon zeigt auf, dass auch Säugetiere schon große Spezies hervorgebracht haben.\n\n\nDie letzte Folge handelt vom extremen Vulkanismus am Ende der Kreidezeit und dessen Einfluss auf die Dinosaurier am Beispiel von Tyrannosaurus und Torosaurus und abschließend von dem Meteoriteneinschlag, der die Dinosaurier (bis auf die Vögel) aussterben ließ.\n\nBegleitet wird ein Tyrannosaurusweibchen, das nach einer erfolglosen Brut auf der Suche nach einem Männchen ist. Dieses findet sie am Kadaver eines jungen Triceratops. Nach einer erfolgreichen Paarung hat sie erneut ein Gelege und wird von eierraubenden Säugetieren, den Didelphodons und dem Raptor Dromaeosaurus terrorisiert. Auch wird der spektakuläre Kampf zweier Torosaurier gezeigt. Mit dem Quetzalcoatlus wird einer der letzten Flugsaurier gezeigt, da diese Gruppe von den Vögeln verdrängt wurde. Eine Herde Edmontosaurus, die schließlich auf einen See trifft, wird auf Nahrungssuche gezeigt. Im Wasser leben die Deinosuchus, riesige Krokodile. Das Tyrannosaurus-Weibchen greift die Herde an und erlegt einen Edmontosaurus. Anschließend wird gezeigt, dass die Brut dieses Mal erfolgreich war und das Weibchen ihre drei Jungen mit Nahrung versorgt und beschützt. Später fehlt vom jedoch vom Kleinsten jede Spur und ein Ankylosaurus verletzt das Weibchen tödlich. Am darauffolgenden Morgen schlägt der Meteorit auf der Erde ein.\n\n\n\n2001 und 2002 erschienen zwei Specials zu \"Dinosaurier – Im Reich der Giganten\".\n\n2001 erschien \"Die Geschichte von Big Al\" (englischer Originaltitel: \"The Ballad of Big Al\"). In diesem Special wird die Biografie eines Allosaurus namens Big Al, erzählt.\n\nEin weiteres Special erschien 2002. Unter dem Namen \"Im Reich der Giganten\" (englischer Originaltitel: \"Chased by Dinosaurs\") macht der Zoologe Nigel Marven eine fiktive Zeitreise in das Erdmittelalter, um das Rätsel der „Riesenklaue“ von Therizinosaurus zu lösen und den „Kampf der Giganten“ zwischen Giganotosaurus und Argentinosaurus im heutigen Argentinien mitzuerleben.\n\nIm Jahr 2003 trat Marven auch in der von der BBC produzierten Dokumentarfilmreihe \"Monster der Tiefe\" als Zoologe auf Zeitreise auf. Weiterhin produzierte man mit ihm auch die Serie \"Prehistoric Park\" (2007).\n\n2013 erschien mit \"Dinosaurier 3D – Im Reich der Giganten\" ein Kinofilm. Im Gegensatz zur Serie können die Tiere sprechen.\n\nBei Beginn der Dreharbeiten gab es für Tim Haines, den Produzenten der Serie, die Schwierigkeit einen geeigneten Drehort für die Landschaftsaufnahmen zu finden, denn im Erdmittelalter gab es noch kein Gras, das heute nahezu weltweit verbreitet ist. Deshalb musste auf Gebiete wie Neuseeland, die Bahamas, Neukaledonien, Chile, Kalifornien, die Kanaren (Santa Cruz de La Palma, Teneriffa) und Australien ausgewichen werden.\n\nMit Hilfe von Computer Generated Imagery (CGI) wurden die Tiere animiert, nur bei Nahaufnahmen wurde auf 3D-Animationen verzichtet, da ihre Wirkung nicht als hinreichend realistisch eingeschätzt wurde. An ihrer Stelle kamen Animatronica zum Einsatz. Die Produktion kostete 18 Millionen US-Dollar und ist bis heute eine der teuersten Dokumentarfilmserien der Fernsehgeschichte.\n\nDie Erstausstrahlung im deutschen Fernsehen, gesprochen von Otto Clemens, verfolgten sechs Millionen Zuschauer. In den Vereinigten Staaten verzeichnete die Premiere die beste Quote des Discovery Channels in der Sendergeschichte.\n\nVon den sechs Teilen werden in Deutschland zumeist zwei in einer Sendung gezeigt, die Dauer eine Episode beträgt etwa 30 Minuten. Die Serie wurde auf VHS-Kassette und DVD veröffentlicht. Unter Mitwirkung des Paläontologen Martin Sander als wissenschaftlicher Berater ist ein Buch zur Serie erschienen.\n\nDie auf den Filmen beruhende Live-Show „Walking With Dinosaurs“ beziehungsweise „Dinosaurier – Im Reich der Giganten“ ist 2009 und 2010 in Berlin, Mannheim, Köln, Hannover, Hamburg, München, Nürnberg und Wien aufgeführt worden. Eine weitere Tour machte Anfang 2013 in Leipzig, Oberhausen, Frankfurt, Wien, Stuttgart und München halt.\n\n\n"}
{"id": "3058450", "url": "https://de.wikipedia.org/wiki?curid=3058450", "title": "Mikrocomputer für Ausbildung", "text": "Mikrocomputer für Ausbildung\n\nDer Mikrocomputer für Ausbildung (MFA) ist ein deutscher Mikrocomputer für die Ausbildung. Er wurde z. B. bei der Ausbildung zum Kommunikationselektroniker eingesetzt.\n\nDer MFA ist modular in einem 19\"-Gehäuse eingebaut und kann mit den unterschiedlichsten Erweiterungskarten bestückt werden. Alle Komponenten, auch die Grundkomponenten, wie CPU, RAM oder ROM sind als Steckkarten ausgelegt. Unter anderem gibt es auch Karten, mit denen man direkten Zugriff auf den Daten-, Adress- und Systembus des Systems hat. Der MFA ist dadurch gut geeignet die Vorgänge in einem Computer nachzuvollziehen. Die Programmierung erfolgt meistens in Assembler. Über die RS232-Schnittstelle kann er an einen PC angeschlossen werden, auf dem eine Terminalemulation läuft, welches dann die Tastatureingaben und die Bildschirmausgabe übernimmt. Es kann aber auch eine eigene Tastatur und ein eigener Monitor angeschlossen werden. Später kam die Unterstützung für das Betriebssystem CP/M hinzu. Hierzu war ein Bootstrap-Loader In einem EPROM vorgesehen, der sich selbst in den Speicher kopiert hat und dann das EPROM abgeschaltet hat um dann den ersten Sektor der Diskette zu laden, aus dem weitere Routinen zum Laden des Betriebssystems CP/M hervorgingen. Nach dem Abschalten des EPROM standen bis zu 64KB RAM zur Verfügung, die CP/M benutzen konnte. Die Kommunikation mit dem Bediener lief dann über eine 8251-UART-Karte in Verbindung mit einem Terminal. Mit dem MAT85-System lief die Kommunikation über die SID und SOD Pins des Prozessors 8085. Das serielle Protokoll wurde nur durch Software realisiert. Bei Verwendung des 8251-UARTs gegenüber der Softwarelösung wird eine Menge Rechenleistung frei, in der andere Aufgaben vom Prozessor übernommen werden können.\n\n\nEinstellung der Schnittstelle: COM 1, 4800, N, 8, 1\n\nMonitor-Kommandos stellen Programmabläufe auf den Bildschirm dar oder ermöglicht den Dialog mit dem Rechner.\n\nDie in der folgenden Tabelle aufgeführten Unterprogramme aus dem Betriebssystem können in eigenen Programmen verwendet werden. Um die in der Tabelle angegebenen Namen der Unterprogramme in einem Programm mitbenutzen zu können, müssen diese Namen mit Hilfe der EQU-Anweisung vorher den zugehörigen adressen zugewiesen werden.\n\n\n"}
