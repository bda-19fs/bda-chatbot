{"id": "1035467", "url": "https://de.wikipedia.org/wiki?curid=1035467", "title": "HFS Plus", "text": "HFS Plus\n\nDas Dateisystem HFS Plus oder – gängiger – HFS+, ist eine Weiterentwicklung von HFS. Die Abkürzung steht für , hierarchisches Dateisystem. Es wurde am 19. Januar 1998 mit Mac OS 8.1 eingeführt und ist das Standard-Dateisystem für Macintosh-Rechner mit Mac OS X, das seit 2012 OS X und seit 2016 macOS heißt, und iOS-Geräte (iPhone, iPad, iPod, Apple TV und Apple Watch) und kann für alle internen und externen Speichermedien verwendet werden. In Mac OS X selbst wurde es als Mac OS Extended und sein Vorgänger HFS als \"Mac OS Standard\" bezeichnet.\n\nIm Vergleich mit FAT16/32 sind die Zuordnungseinheiten von HFS+ kleiner – dadurch kann sich bei der Partition bzw. bei der Partitionsverwaltung und Zugriffsgeschwindigkeit eine höhere Effizienz ergeben.\n\nDer Nachfolger von HFS Plus ist das 2016 vorgestellte APFS.\n\nEs gibt mehrere, teils kombinierbare Varianten von HFS Plus. Der Vollständigkeit halber wird auch das ältere HFS hier aufgelistet, wie es unter Mac OS X im Festplatten-Dienstprogramm zur Auswahl steht oder stand. In neueren Versionen von OS X/​macOS sind die älteren Varianten teilweise nicht mehr verfügbar.\n\nAußer in der HFSX-Variante macht bei HFS Plus standardmäßig das Dateisystem (dessen Treiber) keine Unterscheidung zwischen Groß- und Kleinbuchstaben im Dateinamen, sodass z. B. codice_3 (großer Anfangsbuchstabe) dieselbe Datei repräsentiert wie codice_4 (alles in Kleinbuchstaben). Die Normalisierung (Umwandlung von Groß- und Kleinbuchstaben, NFD-Normalisierung bei Unicode) findet dabei im Dateisystem-Treiber statt.\n\nHFS Plus wird seit der Umstellung von der PowerPC- auf die Intel-Architektur 2006 nur noch in der Journaling-Variante genutzt. Wie etwa die freien Dateisysteme ext3/ext4, XFS und ReiserFS oder das kommerzielle NTFS der Firma Microsoft, weist es damit eine höhere Stabilität gegenüber Dateisystemen auf, die kein verwenden (FAT16 und FAT32, ext2, HFS u. a.).\n\nUnter klassischem Mac OS und auf den PowerPC-Versionen von Mac OS X verwendet HFS und HFS Plus dieselbe Partitionskennung codice_5 in der (APM). In der Variante HFSX wird aus Kompatibilitätsgründen hingegen die Kennung codice_6 genutzt, damit ältere Betriebssysteme nicht versehentlich eine modernere (inkompatible) Variante von HFS Plus einbinden, was eventuell zu Systeminstabilität und Datenverlust führen könnte. Dies ist u. a. für die -sensitive Variante von HFSX der Fall, nicht aber bei der -Variante.\n\nSeit der Umstellung auf die Intel-Prozessorarchitektur IA-32 2006 wird (GPT) als Partitionstabelle verwendet. Die alte HFS-Partition ist darin nicht mehr abgebildet – HFSX mit GUID codice_7 ist die Standard-GPT-Kennung für das HFS-Plus-Dateisystem.\n\nFür RAID- und FileVault-Partitionen gibt es abweichende Partitions-GUIDs innerhalb einer GUID-Partitionstabelle (GPT), wie auch für RAID-Partitionen innerhalb einer (APM). Diese Partitionstypen können ebenfalls ein HFS-Plus-Dateisystem enthalten, müssen aber nicht.\n\nUnter den Linux-Distributionen ist das Lesen und Schreiben von HFS/HFS+ oft schon durch einfaches mounten möglich, wenn der Kernel das codice_8-Dateisystem unterstützt; ansonsten sind noch die Softwarepakete codice_9 (nur HFS) sowie codice_8 zum Nachinstallieren verfügbar. Für Schreibunterstützung kann es notwendig sein, codice_11 zu installieren oder das Dateisystem-Journal zu deaktivieren. Auch für BSD-Systeme gibt es entsprechende Software-Pakete. Das heißt, dass die Daten auf dem Datenträger von Unix/Linux-Systemen gelesen werden können, wenn die entsprechende Kernel-Unterstützung installiert wurde.\n\nHFS+ kann von NT-basierenden Windows-Betriebssystemen nur mit Hilfe von zusätzlicher Software genutzt werden. Boot Camp 3.0, welches mit Mac OS X Snow Leopard (10.6, 2009) mitgeliefert wird, bietet die Möglichkeit, lesend auf HFS+-Dateisysteme zuzugreifen.\n\n\n\nDie Classic-Umgebung unter Mac OS X erfordert eine mit HFS+ formatierte Systempartition, das Dateisystem UFS wird nicht unterstützt.\n\nHFS und HFS+ sind so ausgelegt, dass sie den größten freien Speicherblock auf der Festplatte suchen, in dem eine Datei gespeichert werden soll. Erst wenn eine Datei nicht in den größten freien Speicherblock passt, wird die Datei aufgeteilt (fragmentiert), und der noch nicht geschriebene Teil wird in einem weiteren Block gespeichert.\n\nEine solche Vorgehensweise setzt voraus, dass beim Schreiben einer Datei deren Größe bereits vorher bekannt ist. Dies ist unter Mac OS X oft gegeben, da die Systembibliotheken zur Verarbeitung von Dokumenten so ausgelegt sind, dass sie Dateien in der Regel atomar aktualisieren: Beim Speichern einer Änderung wird die aktuelle Version des Dokumentes in einem Rutsch in eine neue Datei geschrieben, danach die frühere Version gelöscht und der Dateiname auf die neue Datei übertragen.\n\nZusätzlich vermeidet es Mac OS X, frei gewordene Speicherblöcke gelöschter Dateien wiederzuverwenden, falls möglich. Ab Mac OS X 10.2 („Jaguar“, 2002) wird außerdem die Abbuchung freier Blöcke verzögert, um die Reservierung mehrerer kleiner Blöcke in eine einzelne Abbuchung eines großen zusammenhängenden Blockes zusammenzufassen.\n\nDiese Fragmentvermeidung ist wirkungslos, wenn Dateien langsam wachsen, also nach dem ersten Erstellen einer Datei später weitere Blöcke angehängt werden. Ab Mac OS X Panther (10.3, 2003) kann das Betriebssystem deshalb auch zur Laufzeit defragmentieren \"(on the fly defragmentation)\". Beim Öffnen einer Datei wird geprüft, ob diese in mehr als acht Teile fragmentiert ist. Ist dies der Fall und alle der nachfolgenden Bedingungen treffen zusätzlich zu, wird die Datei in einen genügend großen freien Speicherbereich verschoben und hierdurch defragmentiert:\n\n\nEin weiteres Verfahren, das ab Version 10.3 von Mac OS X (Panther, 2003) zum Einsatz kommt, ist die automatische Gruppierung intensiv genutzter Dateien \"(adaptive hot file clustering)\": Durch kontinuierliches Führen einer Statistik über die Häufigkeit der Lesezugriffe auf jede Datei identifiziert Mac OS X die am intensivsten genutzten Dateien und verschiebt diese in einen Bereich des Dateisystems, der sich direkt hinter den zentralen Metadaten befindet. Bei dieser Verschiebung werden die Dateien defragmentiert und kommen in direkter Nachbarschaft der am häufigsten benutzten Elemente des HFS-Dateisystems zu liegen, so dass Kopfbewegungen der Festplatte minimiert werden. Die Nutzungsintensität einer Datei wird ermittelt, indem die Anzahl der innerhalb eines Beobachtungsfensters der letzten 60 Stunden gelesenen Bytes durch die Gesamtgröße der Datei geteilt wird. Als Speicherbereich für diese Dateien wird 0,5 % der Gesamtkapazität des Dateisystems verwendet. Die Anzahl der Dateien in diesem Bereich wird auf maximal 5.000 beschränkt, und nur Dateien, die höchstens 10 MiB groß sind, nehmen an dem Verfahren teil.\n\nWeitere Defragmentierungsverfahren sind nicht Bestandteil von Mac OS X. Apple rät davon ab, Programme zur nachträglichen Defragmentierung zu verwenden, da sich der Einsatz in der Regel nicht lohnt.\n\nEin bekanntes Problem bei der Verwendung von HFS Plus ist die Voreinstellung auf , die auf Dateisystemebene erzwungen wird. Durch diese Eigenschaft entstehen ungewollte Inkompatibilitäten im Datenaustausch mit anderen Computersystemen. Anders als z. B. bei NTFS, wo die case-insensitivity von Windows nicht im Dateisystem selbst integriert ist, werden bei HFS Plus die Dateinamen durch den Dateisystemtreiber von Apple direkt umgewandelt.\n\nObwohl von HFS Plus auch eine strikte -sensitive-Variante existiert – sie wurde als „HFSX“ mit Mac OS X Panther (10.3, 2003) eingeführt – wird diese nur sehr selten genutzt, ist daher auch weniger gut getestet und teilweise sogar mit weit verbreiteter Anwendungssoftware inkompatibel. Dies ist jedoch keine Einschränkung von HFS Plus, sondern vielmehr ein Problem des Betriebssystems, da unter macOS (bis 2012 „Mac OS X“, bis 2016 „OS X“) in gleicher Weise auf anderen Dateisystemen (z. B. NTFS mittels NTFS-3G) zu Inkompatibilitäten führt.\n\n\n"}
{"id": "1036360", "url": "https://de.wikipedia.org/wiki?curid=1036360", "title": "Schriftgenerator", "text": "Schriftgenerator\n\nEin Schriftgenerator ist ein Computer, dessen Hauptaufgabe es ist, gesetzte Texte in ein Videosignal umzuwandeln. Er findet bei Einblendungen in Fernsehsendungen wie Bauchbinden oder Abspännen Anwendung und ist damit Teil des On Air Designs eines Fernsehsenders.\n\nWeiter ist es z. B. bei Sportübertragungen üblich, externe Datenquellen wie Zeitmesseinrichtungen an den Schriftgenerator anzuschließen, der die Daten ohne Eingriff eines Operators in Bildschirmgrafiken umsetzt.\n\nBereits vor der Einführung der Computertechnik erhielten Filme und Fernsehsendungen Vor- oder Abspänne, in denen die beteiligten Personen und Unternehmen aufgeführt waren. In erster Linie waren das per Hand gemalte Tafeln, die von einer Film- oder Fernsehkamera abgefilmt wurden und erst später in das fertige Produkt hineingeschnitten wurden.\n\nNach und nach kam es aber in der Filmtricktechnik in Mode, den Titel mit Hilfe einer optischen Bank in das bewegte Filmbild zu kopieren. Dabei handelte es sich jedoch um einen photographischen Effekt, der bei dem elektronischen Medium Video nur sehr umständlich realisierbar war.\n\nNach Einführung der Keying-Technik wurden die Titel auf speziellen Karten oder Folien aufgemalt und von einer Kamera abgefilmt. Dieses Kamerabild wurde dann mit dem eigentlichen Bildsignal elektronisch gemischt. Roll- oder Kriechtitel wurden durch Abfilmen eines auf einer Rolle aufgetragenen Textes realisiert. Ein Beispiel dafür findet man in der populären Fernsehsendung Dinner for One. Für komplexere Animationen kam der Legetrick zum Einsatz, der vorproduziert werden musste.\n\nDie Erfindung eines computergestützten Systems stellte daher eine erhebliche Vereinfachung dar, da die Titel nun kostengünstig und zeitsparend erstellt, flexibel bearbeitet und für spätere Verwendung relativ einfach gespeichert werden konnten.\n\nSchriftgeneratoren sind oftmals speziell aufgebaute Computer, deren Hardware besondere Anforderungen an Stabilität und Signalqualität erfüllen muss. Häufig wird der Schriftgenerator als komplettes Paket aus Software und spezialisierter Hardware ausgeliefert. Es gibt allerdings auch Schriftgeneratoren, die als Software in bestehenden Videoschnittprogrammen integriert sind, so z. B. bei Adobe Premiere, Avid und Final Cut Pro.\n\nBei den Hardware-Schriftgeneratoren werden die Texte am Bildschirm gesetzt, als analoges oder digitales Videosignal an einen Bildmischer weitergegeben und dort mit dem Hauptbild, dem \"Background-Signal\", gemischt. Das geschieht meist „live“, also in direktem zeitlichen Zusammenhang mit einer Fernsehsendung oder -produktion.\n\nDamit das Videosignal des Schriftgenerators, das \"Fill-Signal\", das Hintergrundbild nicht komplett verdeckt, muss es gekeyt, also maskiert werden. Dabei wird das Schriftsignal so „zurechtgeschnitten“, dass nur noch seine relevanten Teile, also Buchstaben und Grafiken, im gemischten Signal zu sehen sind. Die technische Einrichtungen für den Key-Vorgang sind heute meist komplett in den Bildmischer integriert.\n\nFür die Einblendung selbst kommt bei den Bildmischern häufig der sogenannte \"Downstream Key\" zum Einsatz. Dieser bezeichnet jedoch keine besondere Key-Technik, sondern eine spezielle Komponente des Bildmischers. Dabei wird die Einblendung erst am Ende der Signalkette innerhalb des Bildmischers über das Bild gelegt, nachdem also alle anderen Bildbearbeitungen schon erfolgt sind. Dadurch ist es möglich, \"vor\" diesem letzten Bearbeitungsschritt den Cleanfeed abzugreifen. Dabei handelt es sich um ein Bildsignal, dem die Einblendungen des Schriftgenerators fehlen.\n\nBeim Keying können verschiedene Methoden angewendet werden. Häufig geschieht das über einen \"Luminanz-Key\", bei dem die dunkelsten Teile des Schriftgenerator-Signals transparent werden. Das ist jedoch nur sinnvoll, wenn die Schrift relativ hell ist und das Schriftgenerator-Signal keine Partien enthält, die einerseits dunkel, aber auch opak, also \"nicht\" transparent sein sollen. Ist die Kontur eines Buchstabens zum Beispiel schwarz, um die Buchstaben auch auf hellen Hintergründen sichtbar zu machen, so würde der Luminanz-Key auch diese Kontur transparent darstellen.\n\nSo ist es üblich, mit einem sogenannten \"Linear Key\" zu arbeiten: Dabei wird zusätzlich zum eigentlichen Videosignal ein weiteres Signal an den Bildmischer geleitet, das \"Key-Signal\", das die transparenten Teile beschreibt. Dieses Signal ist vergleichbar mit dem Alphakanal in der Bildbearbeitung.\n\nDas Key-Signal muss man sich als voll aufgelöstes Schwarz-Weiß-Fernsehsignal vorstellen. Die Helligkeit eines gegebenen Bildpunktes dieses Signals ist ausschlaggebend für die Transparenz, mit der der korrespondierende Bildpunkt des Fill-Signals über das Background-Signal gelegt wird.\n\nAuf tieferer Ebene basieren alle Schriftgeneratoren auf Zeichengeneratoren, die die prinzipielle Darstellung von (einzelnen) Zeichen auf einem Bildschirm erledigen.\n\nMit der Entwicklung der Technik seit den frühen 1980er Jahren wurde es möglich, auch Grafiken in den gesetzten Text einzubinden, mit mehreren Ebenen zu arbeiten oder die Texte und Grafiken zu animieren. Heute sind die Möglichkeiten, die ein Schriftgenerator bietet, stark gewachsen: So kann man nun auch Videosequenzen einbinden und komplexe 3D-Animationen erzeugen. Damit verbinden sich die Anwendungsbereiche eines Schriftgenerators mit denen eines digitalen Videoeffektgerätes.\n\nDer Commodore Amiga war ein von vornherein auf Videoanwendungen ausgelegtes Computersystem. Vor allem seine Genlock-Fähigkeit − zusammen mit der Overscan-Unterstützung − machte ihn ab Mitte der 1980er Jahre zu einem oft eingesetzten Gerät in Videostudios. Ein eigenes \"Genlock-Interface\" führte dem Amiga dabei von außen den Studiotakt zu, synchronisierte dabei den ganzen Computer auf diesen Takt, so dass dessen Videoausgabe problemlos mit anderen Studiosignalen gemischt werden konnte, letzteres oft im Genlock-Interface selbst. Die Art der Einblendung konnte dabei von harter Überlagerung bis zu weicher Einblende variiert werden. So konnten zum ersten Mal mit einfacher Programmierung eines Mikrocomputers (zur Not in BASIC) Effekte wie Einblendungen von Texten in beliebiger Größe, Farbe und Gestaltung und von sonstigen, beliebigen Grafiken − und das alles auf Wunsch auch animiert − mit einem einfachen Tischgerät verwirklicht werden. Eine spezielle Anwendung war lange Zeit die Verwendung von Amigas zur Generierung der Senderlogos in einer der Bildschirmecken.\n\nHeutige PCs sind von ihrer Rechenleistung her durchaus in der Lage, die Arbeit eines Schriftgenerators zu übernehmen. Das gilt allerdings nur unter der Voraussetzung, dass geeignete Erweiterungskarten installiert sind, die ein qualitativ ausreichendes Videosignal zur Verfügung stellen. Üblicherweise handelt es sich dabei um ein SDI-Signal. Die Qualität des Composite-Signals einer normalen Consumer-Grafikkarte ist \"nicht\" ausreichend. Auch das qualitativ bessere Komponentensignal muss über eine ausreichende Bandbreite verfügen. Wird der Schriftgenerator in Live-Sendungen eingesetzt, so sind die Anforderungen an die Stabilität und Ausfallsicherheit der Soft- und Hardware außerdem wesentlich höher.\n\nIm Allgemeinen ist zurzeit ein Übergang von den reinen Speziallösungen zu auf PC-Technik aufbauenden Systemen zu erkennen. Ein Beispiel dafür sind die Chyron-Systeme, die unter dem Windows-Betriebssystem laufen, aber weiterhin als Komplettsysteme verkauft werden. Allerdings werden auch PCI-Erweiterungskarten angeboten, mit denen man einen normalen PC zu einem Schriftgenerator aufrüsten kann.\n\n"}
{"id": "1036547", "url": "https://de.wikipedia.org/wiki?curid=1036547", "title": "Dazuko", "text": "Dazuko\n\nDazuko (für \"Dateizugriffskontrolle\") ist ein Kernel-Modul, das unter Linux und FreeBSD eine Schnittstelle zwischen dem Dateisystem und Echtzeit-Virenscannern sowie Sicherheitstools anderer Anbieter zur Verfügung stellt.\n\nDie Entwicklung von Dazuko wurde mittlerweile eingestellt.\n\nDas von Avira entwickelte Kernel-Modul war ursprünglich ein Plug-in-Bestandteil von AntiVir für Linux Server und ermöglicht seit 2002 eine Sicherheitsprüfung Linux-basierter Dateien. Ab der Version 1.0.1 steht die Dazuko-Schnittstelle auch anderen Anbietern für Sicherheits- und Monitoring-Applikationen zur Verfügung. Avira will Dazuko damit zu einer universalen Integrationsschnittstelle für die Linux-Welt machen.\n\nSo soll sich das Kernel-Modul beispielsweise zur Einbindung von Tools eignen, die Dateizugriffe statistisch auswerten oder Daten auf der Festplatte verschlüsseln. Bei Zugriffsanforderung werden einzelne Datei-Informationen wie die Art des Zugriffs, Prozess- und Benutzer-ID zunächst zur Prüfung an die jeweilige Applikation umgeleitet, Dateianfragen werden also zunächst via Dazuko an ein Userspace-Programm weitergereicht und erst bei entsprechend positiver Rückmeldung für die Applikation freigegeben.\n\nSoftware, die mit Dazuko arbeitet:\n\nDazuko ist als freie Software verfügbar, wobei die Avira GmbH aber auch eine vorkompilierte kommerzielle Version der Software für Sun Solaris SPARC anbietet. Dazuko ist standardmäßig nicht mit AppArmor von Novell kompatibel, weshalb AppArmor deaktiviert werden muss, wenn Dazuko genutzt werden soll. Alternativ kann man Dazuko mit Syscalls statt LSM (Linux Security Modules) kompilieren. Damit funktioniert der Parallelbetrieb von AppArmor und Dazuko.\n\nNeben Dazuko gibt es noch DazukoFS, das inzwischen in der Version 3.1.4 vorliegt (Stand: 19. März 2011). Es benutzt ein anderes Interface und andere Devices als Dazuko.\n\n"}
{"id": "1037151", "url": "https://de.wikipedia.org/wiki?curid=1037151", "title": "Computerwoche", "text": "Computerwoche\n\nDie Computerwoche ist eine deutsche Wochenzeitung für CIOs und IT-Manager. Sie ist seit 1974 am Markt und wird hauptsächlich im Abonnement vertrieben. Die Zeitung gehört zum IT-Fachverlag International Data Group (IDG), dessen deutsche Niederlassung und die Redaktion der \"Computerwoche\" ihren Sitz in München hat. Chefredakteur ist derzeit Heinrich Vaske.\n\nDie \"Computerwoche\" will technische Trends und wirtschaftliche Situation von Herstellern so darstellen, dass IT-Verantwortliche vor allem in mittleren und großen Unternehmen damit eine Handhabe für ihre Investitionsplanung erhalten. Dazu kommen Analysen, Anwenderberichte, Branchennachrichten, Projektberichte, Personalien und aktuelle Meldungen aus der Welt der IT.\n\nDie Webseite der \"Computerwoche\" wurde 2009 vom Verband der Deutschen Fachpresse als bestes Online-Fachmedium in der Kategorie IT/Telekommunikation/Elektronik ausgezeichnet.\n\nDie amerikanische Zeitschrift \"Computerworld\" erschien erstmals 1967. Im Laufe der Zeit gründete der Verlag International Data Group in insgesamt 46 Ländern Ableger mit unterschiedlicher Erscheinungsweise, von denen manche nur noch digital erscheinen. Die Website von IDG listet diese Print- und Digitalmedien inklusive ihrer Links auf.\n\nDie erste Ausgabe der \"Computerwoche\" erschien am Mittwoch, dem 9. Oktober 1974 mit dem Untertitel \"Die aktuelle Wochenzeitung für die Computerwelt\". Für die zweite Ausgabe benötigten Redaktion und Verlag, der damals noch \"Computerworld GmbH\" hieß, ganze drei Wochen. Ab der dritten Ausgabe, die zwei Wochen später am 13. November erschien, gab es eine einheitliche Gliederung, die in den nächsten Jahren beibehalten wurde. Auf den Nachrichten, ein Thema der Woche, einem Editorial (später Kolumne), Gastkommentar und Leserbriefen folgten die jeweiligen Rubriken Software, Hardware, \"Communications\", EDV-Karriere und Industrie (später in Wirtschaft umbenannt). Das Zeitungsformat war anfangs mit 445 × 315 Millimetern an das Aussehen von Tageszeitungen angelehnt. Auf der linken oberen Ecke war bis September 1989 eine Datenbandspule und eine Erdkugel abgebildet. Ab der vierten Ausgaben, die am 27. November erschien, wurde die Fachzeitschrift dann wöchentlich vertrieben.\n\nMit der Ausgabe 40/1975 erfolge eine Umstellung auf ein kleineres Format mit 390 × 268 Millimetern. Das Layout aller Seiten hatte, bis auf die Seiten 5 bis 7, von nun an vier Spalten mit jeweils 62 Millimetern Breite. Später folgen weitere kleine Anpassungen. Etwa ein Jahr nach dem Erscheinen der Erstausgabe hatte die \"Computerwoche\" durchschnittlich 60 Seiten. Der Druck erfolgte anfangs mittels bleiernen Druckplatten. Ab 1978 standen dem Verlag Texterfassungsgeräte des Unternehmens Linotype zur Verfügung. Redakteure mussten ihre Texte auf einer Kugelkopfmaschine verfassen, der dann abgetippt und auf Verbatimbändern gespeichert wurde. Der Fotosatz erfolge von nun an direkt im Haus. Mitarbeiter kleben die Seiten gemäß den Layoutvorgaben zusammen, die dann bis zu viermal geprüft wurden, bevor es eine Freigabe durch die Redaktion gab. Ab 1981 wurde die \"Computerwoche\" bis zur fertigen Druckvorlage komplett im eigenen Haus hergestellt.\n\nAufgrund von Kritik am Layout seitens der Redaktion und Verlag wurden Mitte des Jahres 1986 Pläne zur Umgestaltung gemacht. Im Februar 1989 entstand die Nullnummer mit dem zukünftigen Layout. Am 29. September 1989 erschien die Ausgabe 40/1989 mit dem neuen Layout pünktlich zur Computermesse Systems. Die Zeitschrift hatte nun 148 Seiten und wurde im Format 381 × 280 Millimeter gedruckt. Auf der Titelseite war eine zweispaltige farbige Grafik abgebildet. Eine Umstellung auf Desktop-Publishing (DTP) erfolge im Herbst 1993 mit der Ausgabe 38/1993. Zuvor wurden einige Macintosh Quadra mit dem Programm QuarkXPress angeschafft und Mitarbeiter umgeschult. Ab der Ausgabe 10/1994, die zur CeBIT-Messe erschien, kamen ein Pressespiegel hinzu und der Nachrichtenteil, Meinungsseiten und Rubriken wurden farbiger gestaltet.\n\nNach einer stetigen Steigerung der verkauften Auflage auf bis zu 48.281 Exemplaren im 1. Quartal 2005 kam es zu einem Skandal. Anfang Juli 2005 hatte der IDG-Verlag die IVW-Zahlen für das 2. Quartal 2005 gemeldet. Die IVW, ein Institut, das für die Auflagenprüfung bei Medien zuständig ist, kündigte eine Nachkontrolle an, weil ein deutlich geringerer Abonnementanteil gemeldet wurde. Am 8. Juli 2005 trat die \"Computerwoche\" mit sofortiger Wirkung aus der IVW aus, womit sich die IVW-Kontrolle, die zum Ausschluss geführt hätte, erübrigt hatte. Die \"Computerwoche\" ließ von da an ihre etwa halb so hohe Auflage von einem Wirtschaftsprüfer bescheinigen. Im dritten Quartal 2005 verkaufte die \"Computerwoche\" insgesamt 28.306 Exemplare, verbreitet wurden 54.742 Exemplare. Diese Zahlen wurden von der Wirtschaftsprüfungsgesellschaft Deloitte entsprechend der branchenüblichen Kriterien testiert.\n\nSeit 2007 befindet sich die \"Computerwoche\" wieder in der IVW.\n\n"}
{"id": "1040300", "url": "https://de.wikipedia.org/wiki?curid=1040300", "title": "FAUmachine", "text": "FAUmachine\n\nFAUmachine ist eine freie virtuelle Maschine, die in vielerlei Hinsicht VMware oder VirtualPC ähnelt.\nDie folgenden Eigenschaften unterscheiden FAUmachine von anderen virtuellen Maschinen:\n\nFAUmachine kann derzeit in einem von drei verschiedenen Modi laufen:\n\nDie Hardwareschicht der FAUmachine besteht im Wesentlichen aus dem Linux-Kernel des Gastgebersystems. Die Hardwarekonfiguration, die die FAUmachine dem Gastsystem zeigt (Größe des Arbeitsspeichers, CDROM, Anzahl und Größe der Festplatten etc.) kann vom Benutzer konfiguriert werden.\n\nFAUmachine unterstützt die transparente Anbindung des Gastes an Netzwerke, mit denen der Gastgeber verbunden ist.\n\nDie virtuelle Maschine von FAUmachine enthält einen Experimentcontroller, mit dem vollautomatisch in Skriptform vorbereitete Experimente ausgeführt werden und dabei Fehler in die virtuelle Hardware injiziert werden können.\n\n"}
{"id": "1042566", "url": "https://de.wikipedia.org/wiki?curid=1042566", "title": "Cray X-MP", "text": "Cray X-MP\n\nDie Cray X-MP war ein Supercomputer, der von Cray Research entwickelt, gebaut und verkauft wurde. Sie war der erste parallele Vektor-Prozessor-gestützte Computer der Firma und war 1982 der Nachfolger der 1976 erschienenen Cray-1. Sie war von 1983 bis 1985 der schnellste Computer der Welt.\n\nDie X-MP führte das „Hufeisen“-Prinzip der früheren Rechner fort und sah von außen fast wie ihre Vorgänger aus. Die Prozessoren wurden mit 100 MHz getaktet (10 Nanosekunden Taktperiodendauer), waren damit 25 % schneller als die der Cray-1A und ermöglichten eine theoretische Rechenleistung von 200 Megaflops pro Prozessor, also eine Gesamtleistung von 800 Megaflops für die Vier-Prozessor-Maschine. Die Prozessoren hatten auch eine verbesserte Unterstützung für verkettete Berechnungen, parallele arithmetische Pipelines und Zugriff auf geteilten Speicher über mehrere Pipes.\n\nAuf dieser Hardware lief anfangs das proprietäre Cray Operating System (COS), wobei Unicos (ein UNIX-System-V-Abkömmling) über eine Gastbetriebssystem-Option ausgeführt werden konnte. UniCOS wurde ab 1984 zum Hauptbetriebssystem.\n\nDie X-MP wurde mit einem, zwei oder vier Prozessoren und ein bis sechzehn Megaworte (8–128 MB) großem Hauptspeicher verkauft. Während der Speicher anfangs durch ein 24 Bit breites Adressregister auf 16 Megaworte begrenzt war, erweiterte die spätere XMP/EA-Speicherarchitektur den nutzbaren Speicher auf theoretische zwei Gigabyte. Der größte in der Praxis hergestellte Speicher hatte jedoch nur 64 Megabyte. Die XMP/EA hatte eine Taktgeschwindigkeit von 8,5 ns, was eine theoretische Höchstrechenleistung von 942 Megaflops ermöglichte. 1982 kostete eine X-MP/48 ohne Massenspeicher um die 15 Millionen US-Dollar.\n\n1985 wurde die Cray-2 mit einer komplett neuen Architektur vorgestellt. Mit einer von der X-MP deutlich abweichenden kompakten Vier-Prozessor-Architektur und 512 MB bis 4 GB Hauptspeicher sollte sie nach ihrer Spezifikation bis zu 488 Megaflops erreichen, war bei manchen Berechnungen jedoch langsamer als die X-MP, da ihr Speicher sehr große Latenzzeiten hatte (1986 wurde bei einem standardisierten LINPACK Test einer X-MP/48 eine Geschwindigkeit von 713 Megaflops gemessen).\n\nDie Nachfolgeserie der X-MP, die Cray Y-MP, wurde ab 1988 verkauft; sie war eine evolutionäre Fortentwicklung der X-MP mit einer Kapazität von bis sechzehn in ihrer Architektur nicht viel veränderten Prozessoren.\n\n"}
{"id": "1045015", "url": "https://de.wikipedia.org/wiki?curid=1045015", "title": "Windows-Indexdienst", "text": "Windows-Indexdienst\n\nDer Windows-Indexdienst ist ein in Windows 2000 und Windows XP enthaltener Dienst, der den Festplatteninhalt indiziert. Mithilfe dieses Dienstes können Suchanfragen innerhalb von wenigen Sekunden ausgeführt werden; nicht nur eine Suche nach Dateinamen, sondern auch in dem Inhalt der Dateien (Volltextsuche). Der Indexdienst berücksichtigt bei der Volltextsuche erst einmal nur Standardformate, z. B. *.txt, sowie einige Microsoft-Formate, wie z. B. *.doc, *.xls. Über so genannte \"iFilter\" kann der Indexdienst aber auch eine Volltextsuche in anderen Dateiformaten, wie z. B. Adobe PDF, RTF, OpenDocument (OASIS OpenDocument Dateiformat), JFIF (Metadaten), XML oder MP3 (ID3-Tags) durchführen.\n\nIm Web finden sich Anleitungen, wie man den als ressourcenhungrig geltenden Indexdienst abschalten kann. Bemerkenswerterweise erfreuen sich die seit 2004 erschienenen Desktopsuchprogramme (wie z. B. \"Google Desktop\", \"Lookeen\" oder \"Copernic Desktop Search\") großer Beliebtheit, obwohl diese im Kern eine zum Indexdienst ähnliche Funktionalität bieten.\n"}
{"id": "1046629", "url": "https://de.wikipedia.org/wiki?curid=1046629", "title": "IPod", "text": "IPod\n\niPod (von ‚Kapsel‘) ist die Bezeichnung einer Serie tragbarer digitaler Medienabspielgeräte des Unternehmens Apple. Sie sind die weltweit meistverkauften tragbaren Musikabspieler. Einige Modelle können zudem Videodateien wiedergeben.\n\nApple sieht die Verknüpfung mit der eigenen kostenlosen Software iTunes vor, um einen iPod mit multimedialen Inhalten (Musik, Bilder, Videos, Adresskontakte und Spiele) zu füllen. Der iPod lässt sich aber auch mit Software anderer Hersteller bespielen und verwalten; das Bespielen mittels betriebssystemeigenen Dateimanagern ist jedoch nicht möglich.\n\nDie Idee, bei Apple einen Musikabspieler mit einer geräumigen Festplatte als Speichermedium zu produzieren, hatte Jon Rubinstein. Derartige Geräte waren zu der Zeit zwar auf dem Markt, jedoch wenig erfolgreich. Tony Fadell brachte das Konzept des Gespanns aus portablem Abspieler und dazugehörigem Onlinemusikvertrieb in die Firma, er arbeitete als Elektronikentwickler am iPod. Das Design der Geräte stammt von Jonathan Ive. 2002 gewann der iPod den \"Red Dot Design Award.\"\n\n2001: Jon Rubinstein zeigte im Februar 2001 dem damaligen Apple-Chef Steve Jobs eine erste, nur 1,8 Zoll messende Festplatte und entwickelte die Idee eines iPod. Jobs antwortete ihm “Go for it!” (deutsch: „Mach es!“) und stellte am 23. Oktober 2001 den ersten iPod mit einer 5-GB-Festplatte vor. Zur Navigation in Abspiellisten, zum Regeln der Lautstärke und zum Anwählen gewünschter Stellen innerhalb eines Liedes besaß dieses Modell ein bewegliches Scrollrad.\n\n2002: Nach der Einführung eines zweiten Modells mit einer 10-GB-Festplatte (am 21. März 2002) wurde am 17. Juli 2002 die zweite Generation des iPods angekündigt, mit einer 10 oder 20 GB großen Festplatte und einer Fernbedienung im Kopfhörerkabel. Das Scrollrad hieß nun \"Touch Wheel,\" denn es war nicht mehr mechanisch beweglich, sondern berührungsempfindlich – mit (modellabhängig) mehr als 100 Sensoren stellte es die Drehrichtung fest. Alternativ waren diese Modelle als Windows-Version erhältlich.\n\n2003: Die dritte Generation wurde am 28. April 2003 vorgestellt und war ab dem 8. September 2003 erhältlich. Sie kam mit neuem Design, berührungsempfindlichen Tasten, Dock Connector (statt FireWire-Buchse), USB-Anschluss und mit einer Verkleinerung der Akkuladung von 10 auf 8 Stunden Laufzeit auf den Markt. Bei der Vorstellung kündigte Apple 10, 15 oder 30 GB große Festplatten an; verkauft wurden iPods mit Festplattenkapazitäten von 20 und 40 GB. Im September 2003 wurde der einmillionste iPod verkauft.\n\n2004: Im Januar 2004 wurde der \"iPod mini\" angekündigt. Bei diesem Gerät waren Touch Wheel und Tasten erstmals unter einer Oberfläche, dem sogenannten „Click Wheel“, vereint; er war in den Farben Silber, Blau, Grün, Pink und Gold erhältlich. Die Nachfrage nach dem iPod mini war so groß, dass am 17. Februar bereits über 100.000 Vorbestellungen eingegangen waren. Am 20. Februar 2004 begann die Auslieferung in den USA, am 24. Juli 2004 in Europa.\n\nSeit 19. Juli 2004 war die vierte Generation des ursprünglichen iPod als 20- und 40-GB-Modell erhältlich. Er besaß nun wie der iPod mini ein \"Click Wheel,\" und die Spieldauer erhöhte sich laut Apple von acht auf zwölf Stunden.\n\nDie ersten Vorbestellungen des \"iPod photo\" und des limitierten \"iPod U2\" wurden am 26. Oktober 2004 entgegengenommen. Ersterer hatte erstmals einen Farbbildschirm, eine verbesserte Akkudauer von 15 Stunden bei Musik- und fünf Stunden bei Diashow-Wiedergabe. Die U2-Special-Edition besaß eine schwarze Front mit einem roten Click Wheel und eine Gravur der Unterschriften der vier Bandmitglieder von U2 auf der Rückseite. Außerdem bekamen Käufer einen Rabatt auf „The Complete U2“ im iTunes Music Store.\n\n2005: Am 11. Januar 2005 wurden zwei Modelle des iPod shuffle mit 512 und 1024 MB Speicherplatz auf Flash-Speicher-Basis vorgestellt. Sie hatten jeweils 18 Stunden Akkulaufzeit, keinen Bildschirm und gaben die Musikstücke entweder in fester oder zufälliger Reihenfolge wieder.\n\nAm 23. Februar 2005 erschienen neue Versionen des iPod mini (mit 4 oder 6 GB erhältlich, Akkulaufzeit jetzt 18 Stunden), des iPods der vierten Generation (nur mit 20 GB erhältlich) und des iPod photo (nun mit 30 oder 60 GB erhältlich). Außerdem wurden die Preise aller Modelle gesenkt.\n\nDer iPod photo war seit dem 28. Juni 2005 nicht mehr erhältlich. Stattdessen wurde der iPod nun mit einem Farbbildschirm und Kapazitäten von 20 und 60 GB angeboten. Auch der iPod U2 Special Edition bekam nun einen Farbbildschirm. Die Preise für iPod, iPod U2 Special Edition und iPod shuffle 1 GB wurden gesenkt.\n\nSoftware-Aktualisierungen zur Unterstützung von Podcasts erschienen im Juli (zusammen mit iTunes Version 4.9, aber nur für iPods der vierten Generation und iPod mini). Der iPod nano mit Zwei- und Vier-GB-Flash-Speicher wurde am 7. September 2005 vorgestellt. iTunes war nun in der Version 5 verfügbar. Die Produktion des iPod mini wurde eingestellt.\n\nApple präsentierte am 12. Oktober 2005 die fünfte iPod-Generation, die erstmals Videos abspielen konnte. Er war mit 30 und 60 GB und ebenso wie der iPod nano in zwei Farben erhältlich. Der iPod U2 Special Edition wurde nun nicht mehr hergestellt. iTunes waren in der Version 6 mit neuer Videofunktion und Videostore erhältlich, in dem (zumindest im amerikanischen ITMS) auch Fernsehserien, Musikvideos sowie Pixar-Kurzfilme zum Kauf angeboten werden.\n\n2006: Apple präsentierte am 10. Januar 2006 das Zubehör \"iPod Radio Remote\" für iPod video und iPod nano. Gleichzeitig erschien eine neue Firmware-Version, die die Unterstützung der Fernbedienung gewährleistete und einige Fehler korrigierte. Am 7. Februar 2006 senkte Apple die Preise der iPod-shuffle-Reihe und führte den iPod nano mit 1 GB ein. Am 6. Juni 2006 präsentierte der Hersteller den neuen iPod U2 Special Edition mit 30 GB. Zusätzlich zu den Besonderheiten der ersten Version erhielt der Käufer nun einen Gutschein für 30 Minuten „exklusives U2-Videomaterial“ aus dem iTunes Store.\n\nApple präsentierte am 12. September 2006 die zweite Generation des iPod nano und aktualisierte den iPod der fünften Generation. Der iPod shuffle bekam ein ganz neues Design. Den iPod gab es nun in einer 30- und einer 80-GB-Version und der Bildschirm war nun deutlich heller. Außerdem gab es im neuen iTunes-Store nun auch Spiele für den iPod. Der iPod nano hatte nun 2, 4 oder 8 GB Speicherkapazität und ein Aluminiumgehäuse in den Farben Silber, Schwarz, Blau, Pink und Grün. Der iPod shuffle war kleiner als die Vorgängergeneration, besaß ein silberfarbenes Aluminiumgehäuse, hatte einen integrierten Clip mit dem er an der Kleidung befestigt werden konnte, und war nur noch mit 1 GB Speicherkapazität erhältlich.\n\nAm 13. Oktober 2006 stellte Bono von U2 den iPod nano \"Product Red\" in rot mit 4 GB Speicher vor, dessen Kauf mit einer Spende an die Hilfestiftung Global Fund verbunden ist.\n\nAm 1. November 2006 startete eine Kooperation mit dem Sportschuhhersteller Nike: Das Nike+iPod-Paket beinhaltete einen Schrittzähler mit Sender für den Schuh und einen Adapter mit Empfänger für den iPod nano.\n\nAm 3. November bot Apple den iPod nano \"Product Red\" auch in einer 8-GB-Version an.\n\n2007: Am 9. Januar 2007 stellte Steve Jobs auf der Macworld das \"iPhone\" vor, und am 30. Januar 2007 bot Apple den iPod shuffle in fünf Farben an: Neu waren Rosa (Pink), Orange, Grün und Blau.\n\nAm 5. September 2007 wurde die gesamte iPod-Linie aktualisiert: Der iPod shuffle war nun auch als \"Product Red\" erhältlich, der iPod nano konnte nun Videos abspielen, der iPod wurde zum iPod classic, der nun mit 80 GB und 160 GB erhältlich war, und der iPod touch wurde vorgestellt.\n\nDas iPhone wurde am 9. November 2007 erstmals in Deutschland verkauft, nachdem es in den USA schon seit dem 28. Juni 2007 verfügbar war.\n\n2008: Seit dem 21. Januar 2008 ist der iPod nano auch in der Farbe Pink erhältlich. Am 5. Februar 2008 wurde der iPod touch zusätzlich mit 32-GB-Speicher und das iPhone mit 16-GB-Speicher angeboten. Am 19. Februar wurde der iPod shuffle im Preis gesenkt und erhielt eine neue 2-GB-Version. Am 9. September 2008 stellte Apple die vierte Generation des iPod nano, die zweite Generation des iPod touch sowie neue Farbvarianten des iPod shuffle vor. Seit diesem Zeitpunkt gab es den iPod classic in der 120-GB-Version.\n\n2009: Am 11. März 2009 wurde der iPod shuffle der dritten Generation vorgestellt. Er verfügt über eine Speicherkapazität von 2 oder 4 GB, lässt sich ausschließlich über die mitgelieferte Kopfhörerfernbedienung steuern und ist in den Farben Schwarz, Silber, Grün, Hellblau und Pink erhältlich, außerdem gibt es eine Special Edition in Edelstahl. Am 9. September 2009 stellte Apple auf einem Special Event mit dem Titel „It’s only rock and roll, but we like it“ die nächste Generation des iPod nano, des iPod classic und eine Überarbeitung des iPod touch sowie des Programms iTunes 9.0 vor.\n\n2010: Am 1. September 2010 stellte Apple eine neue Generation des iPod nano ohne Kamera und Videofunktion – allerdings mit Multitouch – vor. Des Weiteren wurde der iPod touch in der vierten Generation vorgestellt. Dieser ist dünner und hat zwei Kameras an Vorder- und Rückseite für Videoaufnahmen in HD (720p). Weiterhin ist die Nutzung von \"FaceTime\" möglich, das Apple bereits beim iPhone 4 vorgestellt hatte. Er hat ein Gyroskop und Retina-Display, allerdings keinen Blitz. Der iPod shuffle erhielt ein schlankeres Design sowie die bei der vorhergehenden Generation von vielen Kunden vermissten Tasten zurück. Außerdem werden nun Playlists und VoiceOver unterstützt. Weiterhin wurde iTunes in der Version 10 vorgestellt.\n\n2011: Apple stellte am 4. Oktober 2011 eine aktualisierte Version des iPod touch vor, der technisch mit der Vorgängergeneration identisch ist. Er ist nun auch in Weiß erhältlich und wird mit iOS 5 ausgeliefert. Gleichzeitig zeigte man den iPod nano 6G, der durch ein Upgrade eine neue Benutzeroberfläche, neue Uhrendesigns und mehr Fitnessfunktionen besitzt, bei denen sich die letzten erfassten Aktivitäten in Kategorien anzeigen lassen.\n\n2012: Apple stellte am 14. September 2012 eine aktualisierte Version des iPod touch, sowie des iPod nano vor. Der iPod touch hat ein 4″-Retina-Display erhalten, und den A5 Prozessor, der schon im iPhone 4S verwendet worden war. Zum Anderen ist er im Vergleich zu seinem Vorgänger dünner geworden und wird mit iOS 6 ausgeliefert. Der iPod touch ist jetzt in mehreren Farben verfügbar. Der iPod nano hat ein größeres Display erhalten und ähnelt leicht dem iPod touch, läuft jedoch nicht mit iOS.\n\nAm 9. September 2014 wurde der Verkauf des iPod classic eingestellt.\n\nIm Juli 2017 wurde der Verkauf des iPod shuffle und iPod nano eingestellt. Damit ist der iPod touch der letzte noch von Apple produzierte iPod.\n\nJeder iPod (mit Ausnahme früher iPod-shuffle-Generationen, die Apple Lossless und AIFF nicht unterstützten) unterstützt das Abspielen von Musikdaten in den Formaten MP3, AAC, AIFF, WAV, Apple Lossless, Protected AAC sowie das Hörbuchformat Audible. Einige iPods können zudem Videodateien in den Formaten H.264 und MPEG-4 abspielen.\n\nDas iPhone enthält neben der Hauptfunktion des Telefonierens die volle Funktionalität eines iPod touch und kann daher als ein weiteres iPod-Modell angesehen werden. Die Benutzeroberfläche orientiert sich stark an den Gegebenheiten der iTunes-Versionen ab Version 7.5, so werden u. a. Alben im sogenannten „Cover Flow“ dargestellt. Ähnliches gilt für das iPad.\n\nDas iPhone wird mittels USB 2.0 mit iTunes synchronisiert und hat in den Flash-Speichern Platz für bis zu 256 GB (je nach Modell) abzüglich des Betriebssystems (ca. 700 MB). In Deutschland wurde es am 9. November 2007 eingeführt.\n\nDer erste iPod wurde am 23. Oktober 2001 vorgestellt, enthielt eine 5-GB-Festplatte und war nur mit Mac-Computern kompatibel. Er wurde am 21. März 2002 um eine 10-GB-Variante ergänzt. Die iPods der ersten Generation besaßen noch ein bewegliches, drehbares Scrollrad und noch keinen Dock-Anschluss; als Schnittstelle diente eine FireWire-Buchse; das passende Kabel wurde mitgeliefert.\n\nAm 17. Juli 2002 eingeführt, mit 10- oder 20-GB-Festplatte bestückt und wahlweise für Mac oder Windows erhältlich, war bei diesem Gerät das Gehäuse ein wenig flacher als beim Vorgänger. Das mechanische Scrollrad wurde durch ein berührungsempfindliches Touchpad ohne bewegliche Teile ersetzt. Beim Blick von vorne auf das Gerät ist dieser Unterschied zur ersten Generation kaum zu erkennen.\n\nMit der zweiten Generation veröffentlichte Apple den ersten Windows-kompatiblen iPod. Die Datenübertragung erfolgte weiterhin über einen Firewire-Anschluss. Da iTunes für Windows noch nicht existierte, war dem Windows-iPod die MusicMatch Jukebox zum Synchronisieren beigelegt.\n\nDer iPod der dritten Generation wurde am 28. April 2003 mit einer 15- und 30-GB-Festplatte eingeführt. Am 4. September 2003 erhöhte Apple die Kapazitäten auf 20 und 40 GB. Im Januar 2004 wurde das 10-GB-Modell durch das wieder eingeführte 15-GB-Modell abgelöst. Die Tasten waren bei diesem Gerät nicht mehr kreisförmig um das Scrollrad herum angeordnet, sondern oberhalb in einer Reihe.\n\nDie Trennung zwischen Mac und Windows wurde aufgehoben, das gleiche Gerät konnte unter beiden Betriebssystemen verwendet werden. Der bisher oben befindliche FireWire- bzw. USB-Anschluss (beim Windowsmodell) wurde durch einen Multifunktionsbus, genannt \"Dock-Connector,\" unten ersetzt. Mit diesem neuen Anschluss wurde gleichzeitig ein Dock zum aufrechten Stand bei gleichzeitigem Laden und Synchronisieren und ein zusätzlicher Line-Out-Ausgang herausgebracht. Der Dock-Anschluss bietet anderen Herstellern eine Schnittstelle zum iPod. Seit Mitte 2005 ist die Verwendung lizenzpflichtig.\n\nDie vierte Generation wurde am 19. Juli 2004 mit 20 und 40 GB eingeführt. Das Scrollrad wurde durch das vom iPod mini bekannte ClickWheel, das Scrollwheel und Tasten in sich vereint, ersetzt. Der Akku hält nun bis zu zwölf Stunden, was durch Verbesserungen in der Firmware erreicht wurde. Über das USB-2.0-Kabel kann der iPod jetzt nicht nur mit Daten, sondern auch mit Strom versorgt werden. Die Menüführung wurde leicht überarbeitet. Die Abspielgeschwindigkeit von Hörbüchern lässt sich steuern, ohne dass sich die Stimmfrequenzen verschieben. Mehrere On-The-Go-Playlists können verwaltet und auch Titel daraus entfernt werden.\n\nDer iPod photo wurde am 26. Oktober 2004 von Apple vorgestellt und besitzt einen Farbbildschirm (220 × 176 Pixel, 16 Bit). Damit kann er zusätzlich zur Musik auch Bilder anzeigen und an ein Fernsehgerät angeschlossen werden. Seine Akku-Kapazität reicht aus, um 15 Stunden Musik zu hören oder fünf Stunden lang Diashows mit Musik darzustellen.\n\nFotos der Bildformate JPEG, BMP, GIF, TIFF und PNG müssen zur Darstellung auf dem iPod photo mit iTunes in ein proprietäres Format konvertiert und übertragen werden. Eine Kamera enthält der iPod photo jedoch nicht. In der Software unterscheidet sich der iPod photo nur wenig vom normalen iPod. Der Bildschirm ist sieben- bzw. achtzeilig und farbig.\n\nSein Aussehen ähnelt zwar dem normalen weißen iPod, jedoch unterscheiden sie sich leicht in ihren Abmessungen und im Gewicht. Die 30-GB-Version ist um 4 Millimeter dicker und um acht Gramm schwerer. Die 60-GB-Version dagegen ist 5 Millimeter dicker und 23 Gramm schwerer als der normale weiße iPod der vierten Generation.\n\nZuerst wurde der iPod photo in einer 40- bzw. 60-GB-Version geliefert, dann wurde das 40- in ein 30-GB-Modell umgewandelt. Am 28. Juni 2005 wurde die Produktion des iPod photo eingestellt.\n\nEinführung des Farbbildschirms\nAm 28. Juni 2005 erhielten alle iPod-Modelle einen Farbbildschirm, der iPod photo wurde in die iPod-Linie integriert. Die maximale Akkulaufzeit erhöhte sich von zwölf auf 15 Stunden. Die neuen Modelle mit Farbbildschirm sind teilweise etwas schwerer und dicker.\n\nVorgestellt am 12. Oktober 2005 mit 30 und 60 GB (ersetzt durch 80 GB am 12. September 2006), ist er neben der Standardfarbe Weiß auch erstmals in Schwarz erhältlich. Er besitzt einen im Gegensatz zum Vorgänger vergrößerten 2,5-Zoll-Bildschirm (6,4 cm) mit 320 × 240 Pixeln und als erster iPod die Fähigkeit, Videos abzuspielen, deshalb wird er auch mitunter als „iPod video“ bezeichnet. Er unterstützt die Formate MPEG-4 und H.264; die angegebene Akkulaufzeit bei Videowiedergabe beträgt zwei Stunden bei 30 GB beziehungsweise drei Stunden bei 60 GB. Bei reiner Audiowiedergabe reicht eine Akkuladung bis zu 20 Stunden. Mit 11 und 14 Millimeter ist die fünfte Generation deutlich flacher als die Vorgängergeneration. Außerdem besitzt er die neuen Funktionen \"Screenlock,\" Weltzeituhr und Stoppuhr. Im September 2006 wurden neue Modelle des iPods herausgegeben (Generation 5.5). Diese haben einen helleren, weniger blaustichigen Bildschirm und neue Kopfhörer. Das 60-GB-Modell wurde durch ein 80-GB-Modell ersetzt (bei gleicher Größe wie das bisherige 60-GB-Modell). Außerdem verfügen beide Modelle über eine Suchfunktion. Die Akkulaufzeit bei Videowiedergabe wurde beim 80-GB-Modell laut Apple auf sechseinhalb Stunden erhöht.\n\nFür die bisherigen iPod-Modelle der fünften Generation gibt es eine neue Software in der Version 1.2, dadurch besteht jetzt die Möglichkeit, die Bildschirmhelligkeit einzustellen und Spiele aus dem iTunes Store zu installieren. Außerdem ist jetzt auch \"gapless playback\" (deutsch: \"lückenlose Wiedergabe\") möglich. Weiterhin wurde ein Programmfehler entfernt, wodurch die RDS-Kennung von Radiosendern bei Verwendung der iPod Radio Remote nun korrekt angezeigt wird. Außerdem wurde das Scrollen durch lange Interpreten- und Titellisten vereinfacht. Apple stellt diese vereinfachte und beschleunigte Suchfunktion auch für ältere Modelle der fünften Generation durch eine neue Software-Version zur Verfügung. Anfang Dezember 2006 brachte Apple wiederum eine neue Version 1.2.1 mit weiteren Fehlerkorrekturen heraus. Die aktuelle Softwareversion für die fünfte iPod-Generation hat die Bezeichnung 1.3.\n\nDer nun \"iPod classic\" genannte iPod 6G wurde von Steve Jobs am 5. September 2007 auf einer Sonderveranstaltung mit dem Namen \"The Beat Goes On\" in San Francisco vorgestellt. Er ist mit einer Speicherkapazität von 80 oder 160 GB in den Farben Silber und Space Grey erhältlich. Die Laufzeit des Akkus verlängerte sich im reinen Musikbetrieb auf 30 Stunden beim 80-GB-Modell, beziehungsweise 40 Stunden beim 160-GB-Modell. Die Schale besteht aus Metall und ist zu den Seiten hin leicht abgeflacht. Außerdem ist das dreidimensionale Blättern durch Albencover möglich, genannt Cover Flow. Im Hauptmenü ist die Anzeige nun geteilt und zeigt links die Menüpunkte und rechts verschiedene Einblendungen wie zum Beispiel Musik, Video und Extras.\n\nMit der Generation 6.1 wurde er mit 120 GB Speicherplatz angeboten und verfügt über die \"Genius\"-Funktionalität, bei der Wiedergabelisten mit „gut zueinander passenden Titeln“ erstellt werden können (Apple-Aussage: “Songs that go great together”).\n\nDie Generation 6.2, die seit dem 9. September 2009 erhältlich war, wurde ausschließlich mit 160 GB Speicherplatz angeboten.\n\nDer offizielle Verkauf des iPod Classic wurde am 9. September 2014 eingestellt.\n\nDer iPod mini wurde im Jahr 2004 eingeführt und im Herbst 2005 durch den iPod nano ersetzt. Der iPod mini wird heute nicht mehr verkauft.\n\nDas Gerät wurde in den USA im Februar 2004 und in allen anderen Ländern am 24. Juli 2004 eingeführt. Der iPod mini ist ein wesentlich kleinerer iPod, der jedoch nur eine 4-GB-Festplatte enthielt. Er erhielt als erstes Modell das \"ClickWheel,\" das Scrollrad und Knöpfe vereinigt. Den iPod mini gab es in fünf verschiedenen Farben: Blau, Pink, Grün, Silber und Gold. Als Akkulaufzeit gab Apple acht Stunden bei mittlerer Lautstärke und ausgeschalteter Bildschirmbeleuchtung an.\n\nAm 23. Februar 2005 kamen neue Modelle auf den Markt. Zum 4-GB-Modell kam zusätzlich noch ein 6-GB-Modell hinzu, die Akkulaufzeit wurde auf bis zu 18 Stunden erhöht und die Farben waren nun deutlich kräftiger. Allerdings entfiel das goldfarbene Modell, somit verblieben Pink, Blau, Grün und Silber. Das Ladegerät gehörte nicht mehr zum Lieferumfang, man kann den iPod mini nur noch über das beiliegende USB-Kabel über die Schnittstelle am Computer aufladen oder ein Ladegerät als Zubehör erwerben.\n\nDer iPod nano wurde am 7. September 2005 von Steve Jobs auf einem Special Music Event vorgestellt und ersetzte den iPod mini. Er ist in drei Varianten (1, 2 und 4 GB) sowie zwei Farben (Schwarz und Weiß) erhältlich und verwendet wie der iPod shuffle NAND-Flashspeicher. Dadurch ist er mit den Maßen 90 × 40 × 6,9 mm um etwa 60 Prozent kleiner als sein Vorgänger, der iPod mini, und etwa zwei Millimeter flacher als der iPod shuffle. Der iPod nano besitzt wie der iPod der fünften Generation einen Farbbildschirm (1,5 statt 2,5 Zoll), ein ClickWheel und eine Funktion zum Darstellen von Bildern. Der Kopfhöreranschluss befindet sich erstmals an der unteren Gerätekante. Der iPod nano war mit den meisten damals existierenden Erweiterungen (wie iTrip) nicht mehr kompatibel, weil er keine Stromversorgung mehr nach außen führte. Dennoch verfügte er über die meisten Funktionen seines großen Bruders. Hinzugekommen sind eine Weltzeituhr, ein „Screenlock“, eine Stoppuhr und eine Songtext-Anzeige. Der neuen Generation liegen die neuen Apple-iPod-Kopfhörer bei.\n\nIm Gegensatz zum iPod der ersten bis vierten Generation kann der iPod nano nur über USB mit Daten gefüllt werden, über FireWire kann nur noch der Akku geladen werden. Die Akkulaufzeit beträgt, laut Apple, bis zu 14 Stunden. Im Laufe der Zeit reduziert sich die Akkulaufzeit jedoch erheblich. Daher ist es ratsam, die Bildschirmbeleuchtung abzuschalten, da dies die Beanspruchung des Akkus erheblich verringert und somit längere Betriebszeiten ermöglicht.\n\nBei manchen ausgelieferten Geräten machten sich Qualitätsprobleme bemerkbar, insbesondere die schwarze Version ist sehr kratzempfindlich und sorgte stellenweise für Frustration bei Anwendern. Apple zeigte sich allerdings kulant und ersetzt den Schaden, wenn dieser durch den Herstellungsfehler hervorgerufen wurde. Es sind bislang nur ein Prozent der gesamten Nano-Produktion von dem Fehler betroffen. So konnte man bei den späteren Chargen der Produktion sicher sein, ein fehlerfreies Gerät zu erhalten. Den neueren Geräten liegt mittlerweile wie bei der Ausgabe \"iPod video\" eine Schutzhülle aus Stoff zur Vermeidung von Kratzern bei. Die Klangqualität hat sich, insbesondere im Bassbereich, gegenüber dem iPod gesteigert.\n\nAm 11. November 2011 startete Apple eine Rückrufaktion für die Geräte der ersten Generation des iPod nano. Es hatte Hinweise auf überhitzte Akkus bei dieser Produktreihe gegeben. Kunden mit betroffenen Geräten können online ein Formular ausfüllen, mit welchem eine Überprüfung der Seriennummer des Gerätes vorgenommen wird. Innerhalb von sechs Wochen werden schadhafte Geräte von Apple ausgetauscht, einschließlich 90-tägiger Garantie. Geräte mit persönlicher Gravur werden dabei durch unpersonalisierte Standardprodukte ersetzt.\n\nDie zweite Generation des iPod nano wurde von Steve Jobs am 12. September 2006 auf einem Special Event vorgestellt. Das Design des Nachfolgemodells erinnerte mit seinem in unterschiedlichen Farben verfügbaren Eloxal-Gehäuse an den iPod mini. Allerdings sind nicht alle Farben für alle Speichergrößen verfügbar, die 2-GB-Version wurde in Silber angeboten, die 4-GB-Version in Grün, Pink, Orange, Blau und Silber, die 8-GB-Version in Schwarz. Im Oktober 2006 wurde die Serie um ein 4-GB-Sondermodell und ein 8-GB-Sondermodell „iPod nano Product Red“ ergänzt.\n\nIm Vergleich zur ersten Generation verfügte der iPod nano der zweiten Generation, laut Apple, über einen bis zu 40 % helleren Bildschirm und wiegt 40 Gramm bei einer Größe von 90 × 40 × 6,5 mm, somit ist er etwas flacher und leichter als die Vorgängerversion. Daraus ergibt sich ein Volumen von 23,4 cm³. Außerdem wurde laut Apple die Akkulaufzeit um zehn Stunden erhöht und beträgt nun 24 Stunden. Die Anschlüsse an der Unterseite des iPod nano der zweiten Generation wurden verändert, sodass einige Zubehörteile der Vorgängergeneration, wie Lanyard-Kopfhörer, Docking-Station oder Armband, hier nicht verwendbar sind.\n\nDie neue Version des iPod nano benötigte die Version 7 von iTunes. Ältere Versionen (zum Beispiel iTunes Version 6) waren nicht mehr kompatibel.\n\nDie dritte Generation des iPod nano wurde von Steve Jobs am 5. September 2007 auf derselben Veranstaltung vorgestellt wie der iPod classic und der iPod touch. Auffälligstes Merkmal war der auf zwei Zoll vergrößerte Bildschirm mit LED-Hintergrundbeleuchtung und QVGA-Auflösung (320 × 240 Pixel). Der iPod nano konnte nun ebenfalls Videos abspielen und beherrschte auch Cover Flow. Im Hauptmenü ist die Anzeige geteilt, insgesamt erinnert der iPod nano 3G an den iPod classic, auch wenn er weiterhin aus eloxiertem Aluminium in verschiedenen Farben bestand.\n\nEs gab zwei Speichervarianten: Zum einen das silbergraue 4-GB-Modell und zum anderen ein Modell mit 8 GB, das zusätzlich in Blau, Rot \"(Product Red),\" Grün, Schwarz und seit 21. Januar 2008 auch in Pink erhältlich war. Im Vergleich zum Vorgänger war das Gerät kürzer und etwas dünner, dafür aber breiter (69,8 × 52,3 × 6,5 mm). Daraus ergab sich ein Volumen von 23,73 cm³ und ein Gewicht von 49,2 Gramm. Eine Akkuladung reichte laut Apple für 24 Stunden Musik oder fünf Stunden Videowiedergabe.\n\nDer iPod nano der vierten Generation war seit dem 9. September 2008 mit einer Speicherkapazität von 4 oder 8 GB sowie im Unterschied zum Vorgänger mit 16 GB in neun verschiedenen Farben (Silber, Schwarz, Lila, Blau, Grün, Gelb, Orange, Rot und Pink) erhältlich. Er hat nun wie der iPod touch einen Beschleunigungssensor. Beispielsweise startet das Gerät beim Schütteln die Zufallswiedergabe \"(Shake to Shuffle)\" und wechselt durch einen Lagesensor automatisch das Format von Hoch- auf Querformat, wenn das Gerät entsprechend gehalten wird. Neu ist außerdem die sogenannte „Genius Playlist“, die automatisch zueinander passende Musikstücke abspielt. Es war nunmehr auch möglich, die Schriftgröße in den Menüs zu verändern. Außerdem bot er als erster ipod die Möglichkeit, eine art Audio-Guide zu verwenden. Hierbei werden dem Nutzer die Menüinformationen und Informationen über das aktuelle Musikstück angesagt. Die Erzeugung der Sprachsynthese erfolgt am Computer. Der Nutzer hat daher eine große Auswahl an Sprachausgaben. Unter Windows etwa wird auf die Speech-API zurückgegriffen. iTunes synchronisiert die Informationen der Mediathek mit dem Audio-Guide, sodass immer die Möglichkeit besteht, Informationen über das Musikstück, den Podcast, das Video oder das Bild zu erfahren. Bei einem Gewicht von 36,8 Gramm beträgt die Akkulaufzeit laut Hersteller bei der Videowiedergabe bis zu vier Stunden und bis zu 24 Stunden bei reinem Musikbetrieb. Dieser iPod nano benutzt ein Samsung-S5L8720-SoC.\n\nKurz nach der Veröffentlichung im September 2008 tauchten bei diversen Fachhändlern (nicht aber in Apple Stores bzw. Apple Online Stores) Geräte mit 4 GB Speicherkapazität auf, die bis auf die Farbe Rot in allen üblichen Farben erhältlich waren. Kurze Zeit später hat Apple die 4-GB-Modelle offiziell bestätigt und erklärt, dass diese auf einigen Märkten in limitierter Auflage erhältlich seien. Apple hüllt sich in Schweigen, ob diese Geräte möglicherweise aus einer Fehlproduktion stammen oder ursprünglich für die Märkte in Entwicklungsländern vorgesehen waren.\n\nIm Dezember 2008 testete die Stiftung Warentest 20 mobile Multimediaplayer und der iPod nano erhielt die Gesamtnote 2,5 und belegte damit Platz 2 hinter dem Sony NWZ–A828 (Note 2,2).\n\nAm 9. September 2009 stellte Apple im Yerba Buena Center for the Arts den iPod nano der fünften Generation vor. Er hatte eine Videokamera, ein UKW-Radio, ein Mikrofon, einen Lautsprecher, einen Schrittzähler und war für einen Pulsmesser vorbereitet. Das Display hat eine Diagonale von 2,2 Zoll. Seine Oberfläche besteht aus poliertem Aluminium, unterscheidet sich aber im äußeren Erscheinungsbild nicht sehr vom iPod nano der vierten Generation. Er ist mit 8 und 16 GB in den Farben Silber, Schwarz, Lila, Blau, Grün, Orange und Pink erhältlich – im Apple Store zusätzlich in Gelb und Rot \"(Product Red).\"\n\nAm 1. September 2010 wurde der iPod nano der sechsten Generation vorgestellt. Gegenüber der Vorgängergeneration wurde er auf 37,5 mm × 40,9 mm verkleinert, jedoch dicker, da sich auf der Rückseite eine große, dem iPod shuffle entlehnte Klemme befand. Die Verkleinerung der Grundfläche war möglich durch den Einsatz eines Touchscreen-Displays mit 3,91 cm (1,54 Zoll) Bilddiagonale und einer Auflösung von 240 × 240 Pixeln, das die gesamte Grundfläche einnahm. Die Bedienelemente wie das Clickwheel entfielen dadurch bis auf drei Knöpfe an der Längsseite. Weiterhin entfiel die in der Vorgängergeneration eingeführte Kamera, die Videoabspielfunktion und der Lautsprecher.\nDie Oberfläche bestand wie beim iPod der vierten Generation aus eloxiertem Aluminium. Er war in sieben Farben (Silber, Grau, Blau, Grün, Orange und Pink, sowie exklusiv in Rot als \"Product Red\" im Apple-Online-Shop) erhältlich. Die Hardware blieb im Wesentlichen bis zur Einführung der siebten Generation unverändert.\n\nAm 28. Februar 2011 wurde ein Firmwareupdate (Version 1.1) für diese Generation des iPod nano veröffentlicht, das einige kleinere Änderungen der Bedienung enthielt. Ein weiteres Update (Version 1.2) wurde am 4. Oktober 2011 im Rahmen von Apples „Let’s Talk iPhone“ Veranstaltung vorgestellt. Es enthielt größere Änderungen der Benutzeroberfläche, neue Uhrendesigns und mehr Fitnessfunktionen, bei denen sich die letzten erfassten Aktivitäten in Kategorien anzeigen lassen.\n\nMit 5,4 mm Tiefe und 2,5″-Multi-Touch-Display wurde die geänderte Form in der siebten Generation am 12. September 2012 bekannt gegeben. Neben neuen Farben (helle Farbtöne in Graphit, Silber, Violett, Pink, Gelb, Grün, Blau und einer speziellen roten Variante), dem neuen Lightning-Anschluss und der integrierten Radioantenne ist auch ein Bluetooth-4.0-Modul eingebaut, um erstmals kabellos Verbindung zu Zubehör, Lautsprechern und Kopfhörern aufzubauen. Dieser iPod wird vorerst nur in 16 GB Kapazität erscheinen und enthält die neuen EarPod-Kopfhörer. Bei der Entwicklung scannte Apple laut eigener Aussage hunderte von Ohren, um den Komfort der EarPods zu verbessern. Die Entwicklungszeit betrug laut Apple drei Jahre.\n\nAm 10. September 2013 wurde die Farbe Graphit, die vom iPhone 5 bekannt war, durch die neue Farbe Space Gray ersetzt, die vom iPhone 5s, iPhone 6 und iPhone 6 Plus bekannt ist.\n\nAm 9. März 2015 wurde der Preis von 169 auf 179 Euro erhöht (in Österreich kostet er aufgrund der höheren Mehrwertsteuer 189 Euro, in der Schweiz 179 Franken).\n\nIm August 2015 wurde die Farbpalette erneuert und zeigt nun kräftigere Farbtöne sowie eine zart-goldene Variante.\n\nAm 27. Juli 2017 wurde der iPod nano eingestellt.\n\nDer iPod shuffle (von \"to shuffle\" = ‚[Karten] mischen‘) wurde am 11. Januar 2005 vorgestellt und im September 2006 aktualisiert. Er hat ein glänzend weißes Plastikgehäuse und besitzt anstelle einer Festplatte einen Flash-basierten Speicher von 512 MB oder 1024 MB (1 GB). Der iPod shuffle besitzt kein Display. Neben der Funktion als Musikspieler kann er auch als normaler Daten-USB-Stick genutzt werden. Mit Hilfe des USB-Anschlusses lädt man ihn auf und kopiert die Musik oder andere Dateien. Auf der Rückseite befindet sich ein Schalter, mit dem der iPod shuffle bedient wird. Er lässt sich in drei Positionen stellen:\nUnter dem Schalter befindet sich eine Akkuanzeige: Nach einem Knopfdruck zeigt die aufleuchtende Farbe in einem \"Ampelsystem\" den Ladezustand des integrierten Akkus:\n\nMan kann ihn über iTunes wahlweise mit zufälligen Titeln bespielen oder eine bestimmte Playlist auf den iPod kopieren.\n\nAm 12. September 2006 wurde der iPod shuffle der zweiten Generation von Steve Jobs vorgestellt. Den iPod shuffle gibt es seit dem 5. September 2007 mit veränderten Farbtönen, die denen des iPod nano der dritten Generation entsprechen. Erstmals hat auch das kleinste Modell der iPod-Reihe eine \"Product-Red-Serie\". Seit dem 19. Februar 2008 gab es den iPod shuffle neben dem Modell mit 1 GB auch in einer 2-GB-Version. Am 9. September 2008 wurden die lieferbaren Farben des Shuffle geändert.\n\nMit zum Lieferumfang gehört eine kleine Akku-Ladestation, die über den Kopfhöreranschluss des iPod shuffle angeschlossen wird. Auf der Gegenseite befinden sich zwei kleine Schalter, die zum Ein- bzw. Ausschalten und zum Wechsel zwischen den Funktionen „Shuffle“ und „In-Reihenfolge-spielen“ genutzt werden. Durch einen integrierten stabilen Clip lässt sich der iPod shuffle fast überall befestigen. Im Gegensatz zum Vorgänger ist sein Äußeres aus eloxiertem Aluminium gefertigt.\n\nDas Modell ist mit den Abmessungen von 27,3 × 41,2 × 10,5 mm (etwa die Größe der \"Apple Radio Remote,\" aber etwas dicker) und einem Gewicht von 15 Gramm kleiner und leichter als der Vorgänger und galt laut Apple als der derzeit kleinste Audio-Player weltweit.\n\nAm 11. März 2009 wurde der iPod shuffle der dritten Generation vorgestellt. Diesen gibt es wahlweise mit 2 oder 4 GB Speicher. Er lässt sich nur noch durch die mitgelieferte Kopfhörerfernbedienung steuern. Seit dem 9. September 2009 ist der iPod shuffle außer in Schwarz und Silber auch in den Farben Blau, Grün und Pink erhältlich, sowie in einer 2-GB–Version. Es gibt zudem noch eine Sonderedition aus poliertem Edelstahl; dieser verfügt über 4 GB Speicherkapazität und ist ausschließlich im Apple Store erhältlich.\n\nNeu bei den Modellen der dritten Generation ist zudem eine „VoiceOver“ genannte Funktionalität, die auf Knopfdruck an der Kopfhörerfernbedienung den aktuell gespielten Interpreten, den Titel sowie die Wiedergabeliste nennt. Die Sprachsynthese findet dabei auf dem Rechner statt, nicht auf dem iPod. Die Ansagen werden in der gewünschten Sprache als AAC-Dateien erzeugt und bei der Synchronisation auf den iPod kopiert. Mit ihrer Einführung war die Funktion in 14 Sprachen verfügbar. Die eingesetzte Stimme ist unter \"Windows\" und \"Mac OS X Tiger\" weiblich, unter \"Mac OS X Leopard\" männlich.\n\nAm 1. September 2010 wurde der iPod shuffle der vierten Generation vorgestellt. Diesen gibt es mit 2 GB Speicher und er ist in den Farben Silber, Blau, Grün, Gelb und Rosa erhältlich. Er lässt sich nun wieder durch Knöpfe am Gehäuse steuern, verfügt aber nach wie vor über kein Display. Das soll mit der VoiceOver-Funktion kompensiert werden, die in 29 Sprachen den aktuellen Titel und dessen Interpreten ansagt, vor kritischem Ladestand warnt oder Playlists nennt.\n\nAm 12. September 2012 wurden lediglich die Farben des iPod shuffle aktualisiert.\n\nAm 10. September 2013 wurde die Farbe Graphit, die vom iPhone 5 bekannt war, durch die neue Farbe Space Gray ersetzt, die vom iPhone 5s, iPhone 6 und iPhone 6 Plus bekannt ist.\n\nAm 9. März 2015 wurde in Deutschland der Preis von 49 auf 55 Euro erhöht (in Österreich kostet er aufgrund der höheren Mehrwertsteuer 59 Euro, in der Schweiz 59 Franken).\n\nIm August 2015 wurden die Farben des Shuffle erneuert. Sie sind jetzt kräftiger und entsprechen denen des zeitgleich aktualisierten iPod Nano.\n\nAm 27. Juli 2017 wurde der iPod shuffle zusammen mit dem iPod nano eingestellt.\n\nDie iPod shuffle werden ab der 2. Generation mit einem speziellen Kabel, vierpolig 3,5 mm Mini-Klinke auf USB mit dem Computer bzw. der iTunes Software verbunden. Die Kabel für die 3. und 4. Generation können – obwohl sie mechanisch kompatibel sind – nicht mit den iPod shuffles der 2. Generation verwendet werden.\n\nAm 5. September 2007 stellte Apple den iPod touch bei einem \"Special Event\" im Moscone-Center in San Francisco vor. Das Gerät basiert auf der Technik des iPhones und wird über einen Multi-Touch-Bildschirm bedient. Dieser nimmt wie beim iPhone einen Großteil der Fläche der Gehäuseoberseite ein und hat eine Auflösung von 480 × 320 Pixel. Das Gehäuse ähnelt stark dem des iPhones, ist jedoch deutlich flacher. Das Gerät verfügt als erster iPod über WLAN. Der \"iPod Dock Connector\" für eine Kabelverbindung ist ebenfalls vorhanden. Die verwendeten Komponenten stimmen weitgehend mit denen des iPhones überein, mit Ausnahme von Telefon- und Kamerafunktion, Bluetooth und einigen Softwarekomponenten.\n\nAm 9. September 2008 wurde auf dem Apple Special Event „Let’s Rock“ in San Francisco die zweite Generation des iPod touch vorgestellt. Das Design wurde leicht überarbeitet und ähnelt dem iPhone 3G. Die Rückseite ist nun leicht gewölbt, hinzu kamen ein integrierter Lautsprecher und ein an der linken Seite befindlicher Lautstärkeregler. Im Vergleich zum iPod touch der ersten Generation ist das neue Gerät um 0,5 mm stärker geworden, es wirkt aber wesentlich flacher, da es an den Seiten abgerundet wurde. Eine technische Neuerung ist ein integrierter Empfänger für das \"Nike + iPod\"-System. Es braucht nun nur noch der Sensor-Chip und nicht mehr das gesamte \"Nike + Sport Kit\" gekauft zu werden. Darüber hinaus wurde die Laufzeit des Akkus bei Musikwiedergabe von 22 auf 36 Stunden erhöht, bei Videos von fünf auf sechs Stunden. Außerdem wurde die Prozessortaktfrequenz von 412 MHz auf 532 MHz erhöht. Die aufgespielte Firmware bringt diverse neue Features und Fehlerbehebungen mit sich und ist auch für den iPod touch der ersten Generation kostenlos erhältlich, sollte eine 2.x-Firmware bereits installiert sein. Auch der seit Firmware 2.0 enthaltene AppStore, mit dem man neue Programme und Spiele auf den iPod touch laden kann, wird mitgeliefert. Die Geräte werden mit 8, 16, 32 GB angeboten. Der iPod, die Kopfhörer und das USB-Kabel wurden ohne PVC und bromierte Flammschutzmittel hergestellt. Am 18. Januar 2009 ist den Mitgliedern des \"iPhone Dev Teams\" der erste Jailbreak des iPod touch 2G gelungen.\n\nNeuerungen durch Softwareversion 3.0\nIm Frühjahr 2009 wurde für den iPod touch die Softwareversion 3.0 angeboten. Diese basiert auf der Software des \"iPhone 3G.\" Durch das neue Software-Update ist es möglich, mit einem iPod touch der 2. Generation eine Bluetoothverbindung aufzubauen. Der dazu notwendige Chip wurde von Anfang an in diese Geräte eingebaut. Dies ermöglicht unter anderem die Benutzung von Stereo-Bluetooth-Kopfhörern. Außerdem führt die neue Firmware eine systemweite „Cut, Copy and Paste“-Funktion und die Spotlight-Suche ein. Im Gegensatz zum iPhone war das Update eine Zeit lang nicht kostenlos. Diese Features waren auch mit dem iPhone OS 2.2.1 möglich, allerdings musste man einen Jailbreak vornehmen.\n\nAm 9. September 2009 wurde ein technisch leicht modifizierter, von außen aber unveränderter iPod touch vorgestellt. Er wurde von Apple als „iPod touch late 2009“ vertrieben. Umgangssprachlich wurde er „iPod touch 3G“ genannt, da er der dritte bisher erschienene iPod touch war. Bei der Vorstellung des iPhone OS 4.0 nutzte Apple den Namen „iPod touch 3rd generation (late 2009)“. Für die Geräte mit 32 und 64 GB gab es einen neueren Prozessor (ARM Cortex A8 833 MHz, auf 633 MHz getaktet) und doppelt so viel Arbeitsspeicher (256 MB RAM). Das Modell mit 16 GB entfiel. Die Version mit 8 GB wurde nicht verändert, und entsprach daher technisch dem iPod touch 2G. Der iPod touch verfügte nun zusätzlich über eine Sprachsteuerung und das Feature „Voice-over“. Die Laufzeit des Akkus verringerte sich auf 30 Stunden bei Musikwiedergabe. Wesentliche Neuerungen kamen mit der neuen Softwareversion 3.1, die für alle bisherigen iPod touch ebenfalls zur Verfügung stand, ihre volle Funktionalität aber nur auf der „Late-09“-Version entfaltete. Der iPod touch wurde von Apple durch TV-Spots und auf der eigenen Website als „mobile Spielkonsole“ beworben.\n\nDie vierte Generation wurde am 1. September 2010 auf dem Apple Special Event in San Francisco vorgestellt. Sie verfügt über ein Retina-Display mit einer Auflösung von 960 × 640 Pixeln, eine WLAN-Funktion mit Draft-N-Unterstützung, eine hochauflösende Kamera auf der Rückseite, die auch HD-Videos aufnimmt, eine zweite Kamera auf der Vorderseite mit VGA-Auflösung sowie ein Mikrofon. Die Kameras verfügen über keinen Autofokus, der Fokus ist statisch. Der iPod touch 4G ist mit 7,2 Millimetern fast 1½ Millimeter dünner als das Vorgängermodell und seine Rückseite ist komplett aus Edelstahl angefertigt. Während der Speicherplatz zum Vorgängermodell gleich geblieben ist, kommt in der Hardware der leistungsstärkere Apple-A4-Prozessor zum Einsatz sowie ein Gyroskop (3-Achsen-Gyrosensor) für mehr Spielmöglichkeiten. Dank der Kameras unterstützt der iPod touch nun \"FaceTime\", wobei hier nicht eine Telefonnummer, sondern die E-Mail-Adresse des Nutzers zum Anrufen verwendet wird. FaceTime-Anrufe sind sowohl zu anderen iPod touch-Geräten der vierten Generation, zum iPhone 4 und einem Apple Computer mit installierter FaceTime-Software möglich. Allerdings müssen dafür beide Gesprächspartner online sein. Laut Apple soll der Akku bei Musikwiedergabe bis zu 40 Stunden, bei Videowiedergabe bis zu sieben Stunden halten.\n\nAm 4. Oktober 2011 wurde eine aktualisierte Fassung des iPod touch vorgestellt. Der einzige Unterschied zur Vorgängerversion ist die nunmehr auch erhältliche Farbvariation in Weiß.\n\nDie fünfte Generation wurde am 12. September 2012 in San Francisco vorgestellt. Sie verfügt über ein größeres Retina-Display, eine hochauflösende Kamera (iSight) auf der Rückseite, die Full HD-Videos und Fotos mit 5 Megapixel (2592 × 1936 Pixel) aufnimmt, eine zweite Kamera auf der Vorderseite sowie ein Mikrofon. Der neue iPod touch besitzt außerdem erstmals einen Blitz für Fotoaufnahmen. Der iPod touch ist mit 6,1 Millimeter um 1,1 Millimeter dünner als sein Vorgängermodell. In der Hardware kommt der leistungsstärkere ARM Cortex A9 im Apple-A5 SoC zum Einsatz. Laut Apple soll der Akku bei Musikwiedergabe bis zu 40 Stunden, bei Videowiedergabe bis zu acht Stunden halten. Der iPod touch ist durch anodisiertes Aluminium in den Farben Pink, Gelb, Blau, Weiß und Schwarz erhältlich sowie in einer roten Product Red Sonderedition, bei dem Apple einen Teil der Erlöse an den Global Found to fight AIDS in Afrika spendet. Eine weitere Besonderheit stellt iPod touch loop dar, bei dem eine Handschlaufe auf der Rückseite des iPod touch befestigt werden kann. Das Gerät ist der erste iPod touch, auf dem die Sprachsteuerung Siri installiert ist und der keinen Umgebungslichtsensor zur automatischen Bildschirmhelligkeitseinstellung enthält. Phil Schiller, Manager und Marketing-Vizepräsident Apples, antwortete auf eine Anfrage, dass aufgrund der geringen Dicke kein Platz für einen Umgebungslichtsensor sei. Der iPod touch unterstützt WLAN 802.11 a/b/g/n und Bluetooth 4.0. Auf dem Multimediaplayer läuft Apples mobiles Betriebssystem iOS 6. Es stehen Versionen mit 32 GB und mit 64 GB internen Speicher zur Verfügung. Von Haus aus bietet iOS 6 auf dem iPod touch 5G Features wie Siri, AirPlay, iMessage, FaceTime, Facebook- und Twitter-Integration. Am 30. Mai 2013 wurde eine Variante mit 16 GB Speicher veröffentlicht. Bei diesem Gerät muss der Nutzer allerdings auf die rückseitige Kamera sowie den Loop verzichten. Gleichzeitig wurde der Verkauf der vierten Generation endgültig eingestellt.\n\nDie sechste Generation wurde am 15. Juli 2015 veröffentlicht. Sie gleicht im Aussehen der fünften Generation, besitzt nun das Apple A8-SoC und verzichtet auf die Kameraschlaufe. Die neue Kamera hat eine Auflösung von 8 Megapixeln und eine optionale Zeitlupenfunktion beim Filmen. Die Farben wurden, wie beim iPod nano und iPod shuffle, aktualisiert und entsprechen nun denen des iPhone 6: Silber, Gold und „Spacegrau“ sowie die neuen Farben Dunkelblau und Pink. Auch eine Product Red-Sonderedition gibt es wieder. Weiter wurde eine 128-GB-Variante eingeführt, für einen Aufpreis von 110 Euro gegenüber der 64-GB-Variante. Die Preise der anderen Ausführungen entsprechen denen des Vorgängers.\n\nNachdem die übrigen iPods am 27. Juli 2017 eingestellt wurden, wurden die Preise des iPod touch gesenkt. Die Ausführungen mit 16 und 64 GB wurden nicht mehr verkauft. Die 32-GB-Variante kostet somit noch 229 Euro, der 128 GB iPod touch fortan noch 339 Euro.\n\nHewlett-Packard (HP) verkaufte seit dem 27. August 2004 den iPod in der eigenen Produktauswahl. War der Prototyp noch im typischen HP-Grau gehalten, so unterschied sich das Endmodell kaum vom iPod der vierten Generation. Einziger äußerlicher Unterschied war ein HP-Logo auf der Rückseite unterhalb des Apfels und des iPod-Schriftzugs. Verkauft wurde der iPod nur in den USA, unter dem Namen \"Apple iPod from HP\". Er war mit 299 $ bzw. 399 $ genauso teuer wie das Original. Interessant an dieser strategischen Partnerschaft war, dass alle Desktop-PCs und Notebooks von HP mit iTunes für PC vorinstalliert waren, was der Akzeptanz dieser für Apple wichtigen Software gutgetan hat. Ende Juli 2005 beendete Mark Hurd, der neue Unternehmenschef von HP, diese Vereinbarung mit der Begründung, dass diese nicht mehr in die Unternehmensstrategie von HP passe.\n\nDie iPod U2 Special Edition kam im Oktober 2004 auf den Markt. Das erste iPod-Sondermodell ist mit dem iPod der vierten Generation identisch und enthält 20 GB Speicher. Das normalerweise weiße Gehäuse ist in Schwarz, das ClickWheel in Blutrot gehalten. Auf der Rückseite sind die Unterschriften der vier Bandmitglieder von U2 eingraviert. Seit Juli 2005 besitzt der iPod U2 nun auch den bei allen anderen Modellen inzwischen üblichen Farbbildschirm. Der iPod U2 entstand als Teil einer Partnerschaft und einer gemeinsamen Marketingkampagne zwischen Apple, der Gruppe U2 und der Universal Music Group (UMG).\n\nAm 12. Oktober 2005 wurde er zugunsten des schwarzen iPod der fünften Generation eingestellt. Am 6. Juni 2006 stellte Apple eine neue Ausgabe der U2-Edition vor. Das Sondermodell wurde auf die fünfte iPod-Generation mit 30 GB Speicher aktualisiert. Am 22. September 2006 wurde die U2-Edition an die neuen Modelle der Generation 5.5 angepasst und besitzt nun ebenfalls einen helleren Bildschirm, eine Suchfunktion und eine höhere Videoauflösung an externen Geräten.\n\nAm 13. Oktober 2006 wurde der \"iPod nano Product Red\" von U2-Frontmann Bono und der US-Talkmasterin Oprah Winfrey vorgestellt. Je verkauftem Gerät dieser Special Edition werden von Apple 10 $ an die HIV/AIDS-Hilfestiftung „Global Funds“ gespendet. Auch den am 12. September 2006 vorgestellten iPod nano der zweiten Generation gibt es als \"Product-Red-\"Edition, ebenso wie erstmals den iPod shuffle. Aktuell ist diese Farbe für den iPod nano (7. Generation), iPod touch (6. Generation) und den iPod shuffle (4. Generation) ausschließlich im Apple Online Store sowie im Apple Store verfügbar.\n\nIn den USA verkaufte Apple ab dem 7. September 2005 einen 20-GB-iPod mit Farbbildschirm und eingraviertem Hogwarts-Emblem auf der Rückseite. Diese Edition wurde zum Start der Harry-Potter-Hörbücher in iTunes aufgelegt. Der iPod enthielt alle sechs bis dahin erschienenen Harry-Potter-Hörbücher.\n\nIn Österreich wurde dieser iPod mit 30 GB ausgeliefert \"(Harry Potter 30 GB Collector’s iPod).\"\n\nDer BachPod ist ein iPod, der zusammen mit der Kompletteinspielung der Werke von Johann Sebastian Bach durch Helmuth Rilling ausgeliefert wird.\n\nDie Gravis-Handelskette stellte im Jahr 2005 Sondereditionen in Zusammenarbeit mit den Musikern Xavier Naidoo sowie Mousse T. vor.\n\nDer iPod ist seit seiner Einführung im Dezember 2001 besonders auf dem US-Markt für digitale Musikspieler außerordentlich erfolgreich. Im vierten Quartal 2005 wurden 14 Millionen iPods ausgeliefert, womit die Gesamtzahl der verkauften Geräte auf über 42 Millionen stieg. Bis Mitte September 2006 wurden nach Angaben von Apple mehr als 60 Millionen iPods weltweit verkauft, davon zehn Millionen iPod shuffles. Am 9. April 2007 meldete Apple insgesamt 100 Millionen verkaufte iPods. Bis zum dritten Quartal 2007 wurden zehn Millionen weitere iPods verkauft. Der iPod hält, besonders in den USA, einen großen Marktanteil im Bereich der Festplatten-Player, und dies, obwohl Player anderer Hersteller teils niedrigere Preise, längere Akkulaufzeiten und mehr Funktionalität bieten. Ihnen fehlt in der Regel aber eine Software wie iTunes, die Musik, Videos, Hörbücher und Podcasts verwaltet und automatisch mit dem Player abgleicht. Auch die Nutzerführung des iPods selbst gilt vielfach als besonders einfach und durchdacht. Ein weiteres Argument ist die sehr große Auswahl an passendem Zubehör, das vielfach so nur mit dem iPod funktioniert. Konkurrenten sind unter anderem die von Creative angebotenen Modellreihen \"Zen\" und \"MuVo\" und die Player des Unternehmens iriver.\n\nRund um den iPod hat sich eine eigene Branche von Zubehörherstellern entwickelt. Die angebotenen Produkte reichen von Standardzubehör wie Schutzhüllen bis hin zu FM-Transmittern (zum Beispiel Griffin \"iTrip\", Belkin \"Tunecast II\"), mit deren Hilfe der iPod Musik drahtlos an UKW-Radios übertragen kann (in Deutschland nur zugelassen mit CE-Kennzeichnung und einer Sendeleistung bis maximal 50 nW). iPods seit der dritten Generation erlauben mittels zusätzlicher Hardware weitere Funktionen, wie die Aufnahme von Tonmaterial (wie \"iTalk\"), oder stellen weitere Schnittstellen bereit. So konnte beispielsweise der \"iPod Camera Connector\" genutzt werden, um Fotos von einer Digitalkamera auf ältere iPod-Modelle zu übertragen.\n\nVerschiedene Anbieter (unter anderem auch Apple) haben Systeme zur Integration des iPods in HiFi-Anlagen oder „Basis-Stationen“ mit Lautsprechern entwickelt, um den iPod auch ohne Kopfhörer nutzen zu können.\n\nIn den letzten Jahren haben zahlreiche Autohersteller Adapter zum Anschluss verschiedener iPod-Varianten an die HiFi-/Infosysteme in ihre Fahrzeuge integriert. Dabei wird der iPod über entsprechende Tasten des Fahrzeugs bedient. Außerdem lassen sich viele Original-Autoradios und solche für den nachträglichen Einbau mit Wechslersteuerung mit einem Adapter nachrüsten, der den iPod wie einen CD-Wechsler anspricht; somit können alle Funktionen durch das Autoradio gesteuert werden.\n\nDie Ohrhörer sind bekannt für ihre weiße Farbe und ihr Design. Sie sind ein Markenzeichen des iPods geworden und stellen ein Wiedererkennungssymbol dar. „Die weißen Ohrhörer zeigen, dass Sie Ihre Musik mit Stil genießen“, heißt es in Apples Produktbeschreibung des iPod shuffle.\n\nDas Kabel besitzt die Farbe Weiß-Grau und wird mit einem länglichen Klinkenstecker (3,5 mm) am Kopfhöreranschluss des iPods angeschlossen. Die Kabelführung ist beidseitig. Die im Lieferumfang vorhandenen kabelgebundenen Kopfhörer des iPods arbeiten mit 18-mm-Treibern und besitzen Neodym-Magnete als Wandler. Der Frequenzbereich reicht von 20 bis 20.000 Hz. Meist sind im Lieferumfang Ohrpolster vorhanden, die man an den Ohrhörern befestigen kann.\n\nDie Ohrhörer sind an allen iPods mit Dock-Connector zusätzlich mit einer Kabelfernbedienung erweiterbar, die alle wichtigen Steuerfunktionen, wie ‚Play/Stop‘, Vor- und Zurückspulen, die Lautstärkeeinstellung und ‚Hold‘ beinhaltet. Die Betätigung der ‚Hold‘-Taste der Fernbedienung hat keine Auswirkungen auf die am iPod befindliche ‚Hold‘-Taste und sperrt somit lediglich die Tasten der Fernbedienung.\n\nMit der Einführung der iPod-Generation 5.5 (siehe oben) und der zweiten Generation des iPod nano am 12. September 2006 wurde auch das Design der Ohrhörer geringfügig verändert. Sie sind in der gleichen weiß-grauen Farbkombination gehalten wie das Vorgängermodell, ihre Form ist allerdings etwas runder geworden. Durch einen etwas kleineren Durchmesser passen sie jetzt auch in kleinere Gehörgänge. Zudem wurde der Klang laut Apple etwas verbessert. Die technischen Spezifikationen sind gleich geblieben.\n\nSeit 2012 vertreibt Apple sogenannte „EarPods“. Diese wurden nach Angaben von Apple in mehrjähriger Entwicklungszeit mithilfe von 3D-Scans von vielen Ohren entwickelt, damit sie stabiler im Ohr bleiben als die Vorgänger. Da sie aber eine einheitliche Form besitzen, passen sie nicht jedem Nutzer gleich gut. Andere Hersteller lösen dieses Problem mit Gummiaufsätzen. Earpods bieten einen besseren Klang als das Vorgängermodell und beinhalten ein Mikrofon.\n\n\"Nike + iPod\" ist ein Trainingssystem für den iPod nano, iPod touch (ab 2. Generation) und ab dem iPhone 3GS, das von Apple in Kooperation mit Nike entwickelt wurde. Es besteht aus einem Schrittzähler, der in die Sohle spezieller Nike-Schuhe eingelegt wird, und einem Empfänger, der an den iPod angeschlossen wird. Beim iPod touch ist der Empfänger in das Gerät integriert. Das Sensorteil beinhaltet einen Piezosensor und einen Sender, der die ermittelten Daten über Funk zum Empfänger überträgt. Die Trainingsdaten werden auf dem Bildschirm des iPods angezeigt und per Tastendruck auch über den Kopfhörer angesagt. Zudem besteht die Möglichkeit, vor dem Training die angestrebte Streckenlänge, Trainingsdauer oder den gewünschten Energieumsatz einzustellen. In diesem Fall werden regelmäßig die Trainingsdaten angesagt (beispielsweise nach jedem Kilometer).\n\nNach dem Training kann der Benutzer seine Trainingsdaten auch am Computer einsehen und auswerten. Benutzer des \"Nike + iPod Sport Kit\" können auf der Webseite von Nikeplus Ihre Trainingsdaten hochladen, sich mit anderen Sportlern vergleichen und an Wettkämpfen teilnehmen.\n\nKritik wird häufig an der Verkaufsstrategie geübt: Die Batterien im Sensor lassen sich nicht austauschen. Sind diese entladen, muss ein komplettes neues Sport-Kit gekauft werden. Viele Sportler reagieren darauf verärgert, zumal die Batterien nicht besonders langlebig sind und gerade Ausdauersportler so mehrmals jährlich ein neues Gerät kaufen müssten. Apple hat mit der Einführung des neuen iPod touch den Sensor separat in sein Angebot aufgenommen.\n\nSeit 2011 ist im App Store die separate Anwendung \"Nike+\" für iPhone und iPod erhältlich, die keinen separaten Sensor benötigt. Stattdessen erfolgt die Aufzeichnung der Laufstrecke und Geschwindigkeit über den GPS-Empfänger. Zum Jubiläum der Zusammenarbeit zwischen Nike und Apple wurde die App vorübergehend kostenlos angeboten.\n\nIm Jahr 2005 führte Apple das „Made for iPod“-Programm (Mfi) ein, das Kunden helfen soll, Produkte zu finden, die garantiert mit ihrem iPod funktionieren.\n\nDie Teilnahme an diesem Programm ist für Zubehörhersteller, die den Dock-Anschluss des iPods benutzen, verpflichtend. Apple verlangte dafür zunächst Lizenzgebühren von bis zu zehn Prozent des Umsatzes mit diesem Zubehör, 2006 änderte Apple die Bedingungen und verlangt nun einen Festbetrag von 4 US-Dollar pro verkauftem Zubehör.\n\nDie Lizenznehmer erhalten im Gegenzug eine vollständige, im Laufe der Jahre auf mehrere hundert Seiten angewachsene Entwickler-Dokumentation, die neben der mittlerweile im Netz von Dritten offengelegten Pin-Belegung, u. a. Beschreibung der Ansteuerung über den Dock-Anschluss, sowie detaillierte Vorgaben enthält, wie sich ein angeschlossenes Zubehör-Teil gegenüber dem iPod zu verhalten hat (beispielsweise Stromaufnahme, Kommandos, die das Zubehör beim Abziehen einer externen Stromversorgung zu senden hat).\n\nEinige Funktionen des Dock-Anschlusses – wie z. B. die Videoausgabe oder die serielle Kommunikation – sind seit Herbst 2007 nur noch durch den Einsatz eines speziellen Chips, des sogenannten Authentifizierungskoprozessors, verfügbar. Dieser eigens von/für Apple entwickelte Chip steht nur Mfi-Lizenznehmern zur Verfügung.\n\nMit Einführung des iPhone 5 im September 2012 wurde der Dock-Anschluss durch den Lightning-Anschluss ersetzt. Zukünftig verfügen iPhones, iPads und iPods über den sehr viel kleineren, achtpoligen, beidseitig einsteckbaren Anschluss. Dieser Stecker liefert von Haus aus nur noch digitale, keine analogen Signale mehr (wie z. B. Composite Video oder Line Out Audio). Zudem ist jeder Stecker werkseitig mit einem Authentifizierungschip ausgestattet, der es, so die Vorstellung von Apple, unmöglich machen soll, unlizenziertes Zubehör zu verwenden.\n\nBeim Design sämtlicher iPod-Modelle ist der Wechsel der eingebauten internen Akkus durch den Anwender nicht vorgesehen. Je nach Bauform (iPod, iPod mini) sind Akkus von anderen Herstellern lieferbar. Der Einbau durch den Anwender ist bei genügend handwerklichem Geschick möglich. Apple bietet außerhalb der Gewährleistungsfrist einen Tauschservice an, dieser ist jedoch teurer als der Einbau durch den Anwender oder durch einige Drittanbieter.\n\nBei den neueren kleinen iPods (shuffle, nano) erlaubt die große Integrationsdichte der Bauteile und die Ausführung der internen Verkabelung einen Akkutausch nicht ohne Lötarbeiten.\n\nDer US-Börsenaufsicht bestätigte Apple, dass 2003 mehrere Gemeinschaftsklagen gegen das Unternehmen eingereicht wurden. Die Kläger sahen in der kurzen Akkulebenszeit und dem mangelnden Austauschservice für die Akkus des iPods einen Garantiebruch sowie unfairen Wettbewerb. Im Dokumentarfilm \"Kaufen für die Müllhalde\" wird der iPod-Akku gar als Beispiel für geplante Obsoleszenz genannt.\n\nUm den iPod zu benutzen, benötigt man die Software \"iTunes,\" die nicht mehr auf CD-ROM mitgeliefert wird. So können Kunden ohne Internetzugang den iPod nicht umgehend in Betrieb nehmen. Als Hintergrund dieser Entscheidung wird von Apple die Tatsache angegeben, dass Internet und Computer bald überall verfügbar seien und durch den Verzicht wichtige Ressourcen gespart würden, da nur wenige Nutzer auf die CD-ROM angewiesen seien.\n\nDie neueste iPod-Generation verwendet eine Prüfsumme in ihrer Datenbank, was das Bespielen der Geräte mit nicht unterstützter Software zunächst ausschloss. Andere Software wird von Anwendern oft bevorzugt, weil diese auf schwächeren Computern auch neben anderen Anwendungen noch besser funktioniert als iTunes, und weil iTunes als Musikverwaltungssoftware insbesondere bei technisch versierten Anwendern teilweise auf Ablehnung stößt. Unter Linux stellen diese Alternativen sogar die einzige Möglichkeit dar, den iPod mit Musik zu versorgen. Es wird vermutet, dass Apple die Verwendung von iTunes erzwingen will; auch das verwendete DRM kann nur so gewährleistet werden. Möglich ist auch, dass die Prüfsumme von dem fast zeitgleich eingeführten \"iTunes WiFi Store\" für den iPod touch und das iPhone benötigt wird, um die Konsistenz der Datenbank zu gewährleisten.\n\nAllerdings ist diese Prüfsumme innerhalb einiger Tage gehackt worden, sodass zum Beispiel \"libgpod\" in der neuesten Version wieder wie gewohnt mit dem iPod kommunizieren kann.\n\nObwohl die Technik des iPods grundsätzlich fähig ist, verschiedenste Audioformate abzuspielen, hat Apple sich bei der Entwicklung hauptsächlich auf das MP3- sowie dessen Weiterentwicklung, das AAC-Format beschränkt. So ist der Import einer WMA-Datei nach iTunes nur durch Konvertieren möglich. Nutzer, die andere Audioformate (wie das offene Ogg-Vorbis) bevorzugen, müssen die Dateien mit Programmen Dritter vorher in ein kompatibles Format umwandeln. Um Formate ohne vorheriges Konvertieren auf dem iPod abzuspielen, muss man auf alternative Betriebssysteme (beispielsweise Rockbox) zurückgreifen.\n\nMusik, die online erworben wird, ist oft mit einem DRM-Kopierschutz geschützt. Will man derart erworbene Musik auf dem iPod nutzen, ist man auf den hauseigenen iTunes Store angewiesen. DRM-geschützte Musik von Anbietern wie \"Musicload\" kann man auf dem iPod nicht abspielen, da diese auf das Konkurrenzformat WMA mit DRM setzen.\n\nDie Materialien und die Verarbeitung des Gehäuses sind teilweise in die Kritik geraten, da durch die Berührung der Bedienelemente und besonders der hochglänzenden Rückseite Fingerabdruckspuren zurückbleiben, die nur durch eine separat zu erwerbende Schutzhülle verhindert werden können. Die glänzende Metallrückseiten vieler Modelle und die Kunststoffvorderseite des iPod nano der ersten Generation sind zudem sehr kratzanfällig. Ausnahmen sind hier die heute nicht mehr erhältlichen iPod minis und die seit September 2006 erhältlichen iPod nanos der zweiten Generation sowie die Modelle des iPod nano der vierten Generation, da ihre Oberflächen vollständig aus Aluminium bestehen.\n\nÄhnlich wie die Hardware hat Apple auch die Betriebssystem-Software des iPods eingekauft und an die eigenen Bedürfnisse angepasst. In der ersten Generation lag dem Player eine Software namens Pixo OS in Version 2.1 zugrunde, welche vom gleichnamigen Unternehmen Pixo speziell für MP3-Player produziert wurde. Einige ehemalige Apple-Mitarbeiter waren bei Pixo angestellt, was die geschäftliche Verbindung vermutlich erleichtert hat.\n\nWegen einiger Differenzen zwischen Pixo und Apple wurde die Entwicklung der Software angeblich nebst den daran arbeitenden Mitarbeitern von Apple aufgekauft. „Pixo OS“ wird ab der zweiten Generation nicht mehr gesondert im „About“-Menü aufgeführt. Pixo wurde inzwischen von Sun Microsystems gekauft und konzentriert sich auf die Entwicklung von Java-Applikationen.\n\nMit der Einführung der iPods dritter Generation wechselte Apple zu einem Echtzeit-Betriebssystem namens RTXC des texanischen Unternehmens Quadros. Version 3.2 dieses OS hat Apple an Hardware und Design angepasst. RTXC lässt sich wegen seiner Einsatzfähigkeit auf diversen Mikroprozessoren und DSP-Chips auch gut an zukünftige Geräte anpassen, zumal mit der Unterstützung von Multitasking auch künftige Erweiterungen leicht entwickelt werden können. Den Wechsel hat Apple mit einem Versionswechsel auf 2.x markiert.\n\nMit der Einführung des iPhone mit integrierten iPod-Funktionen hielt iPhone OS Einzug, das mit Version 4 in iOS umbenannt wurde. Es basiert auf Mac OS X und ist das Standard-Betriebssystem der Apple-Produkte iPhone, iPod touch, iPad und Apple TV (ab Apple TV 2). Zur Vorstellung des iPhones gab Steve Jobs unter anderem bekannt, dass die Umstellung weiterer oder gar aller zukünftigen iPods auf OS X geplant sei.\n\nMehrere ambitionierte Programmierer haben es sich zum Ziel gesetzt, lauffähige Alternativen zum originalen Apple-Betriebssystem zu kreieren. Oft möchte man bestehende Probleme und Einschränkungen auf diese Weise auf eigenes Risiko umgehen. Dazu zählt insbesondere die fehlende Unterstützung von bestimmten Audioformaten oder fehlenden Überblendungen zwischen Musikstücken.\n\nNeben dem speziell für den iPod entwickelten „iPod Linux“ gibt es auch das Rockbox-Projekt, das zunächst für andere Musikabspielgeräte entwickelt wurde. Auf den iPod touch kann auch der Android Port „iDroid“ installiert werden.\n\niPodLinux ist ein Linux-basierendes System für den iPod. Zurzeit werden nicht alle Generationen und Modelle offiziell vom iPodLinux unterstützt. Unterstützt werden die iPods der ersten bis dritten Generation. Potenziell läuft das System aber auf allen iPods, außer dem iPod shuffle, dem iPod Classic (6. Generation) und dem iPod nano ab der zweiten Generation, wegen der verschlüsselten Firmware, für die keine Unterstützung geplant ist. Für die anderen Modelle befindet sich das Linux-Betriebssystem noch in Entwicklung, lässt sich aber schon installieren. Die Neuerungen sind unter anderem das Darstellen von Bildern und Videos auf dem iPod. Auch die Anzahl der Spiele wurde zum Beispiel um Tetris und Space Invaders erweitert. Zwei der interessantesten Errungenschaften sind wohl der GameBoy-Emulator, der es möglich macht, GameBoy-Spiele auf dem iPod zu spielen, und eine Portierung des Klassikers Doom. Die Unterstützung des Ogg-Vorbis-Formates und diverser MOD-Formate sind nahezu fertiggestellt. Außerdem ist unter Linux eine verbesserte Aufnahmefunktionen verfügbar, die mit 96 kHz Abtastrate eine weitaus bessere Qualität erzeugt als die Apple-Firmware mit nur 8 kHz.\n\nRockbox wurde ursprünglich für die „Archos Jukebox“ entwickelt, da einige unzufriedene Benutzer deren Hardware besser ausnutzen wollten. Nachdem die eigenständige Firmware auch auf den iRiver portiert worden war, führten die Entwickler am 28. Januar 2006 die rudimentäre Unterstützung für viele iPod-Modelle ein.\n\nDie Software unterstützt im Gegensatz zu iPodLinux nicht die iPod-Datenbank, kann jedoch eine eigene erzeugen. Diese Datenbank war ursprünglich auf maximal 10.000 Dateien begrenzt, inzwischen ist diese Beschränkung jedoch aufgehoben. Eine Ordnernavigation ist ebenfalls möglich. Eine Unterstützung für die iPod-Datenbank ist nicht geplant.\n\nEs gibt jedoch diverse Programme, die iTunes-Datenbanken verwalten können. Im Gegensatz zur Original-Firmware des iPods ist es mit der alternativen Software nun auch möglich, die Formate Ogg Vorbis, FLAC, Musepack, MP2 und WAV-Dateien abzuspielen. Zusätzlich wird unterbrechungsfreie Wiedergabe unterstützt.\n\nNeben Apples iTunes-Software erlauben auch Programme anderer Anbieter die Verwaltung von Musik und Daten des iPods, darunter die Open-Source-Programme ipod-sharp und gtkpod, das auf der plattformunabhängigen GTK2-Bibliothek basiert. Beide Programme bieten somit auch Unix- und GNU/Linux-Benutzern eine Möglichkeit, den iPod zu verwenden.\n\nDie aus gtkpod ausgegliederte Programmbibliothek libgpod wird von einigen weiteren Programmen wie den Audiospielern Amarok und Exaile genutzt. Der Audiospieler Banshee nutzt hingegen ipod-sharp. Für die beiden neuesten iPods kann man bereits eine neue Version von libgpod benutzen, womit auch unter Linux der Player bespielt werden kann.\n\nEine andere Lösung stellen die Freeware-Programme YamiPod, Floola und SharePod dar. Es ist damit möglich, Musik zu verwalten, abzuspielen und auf den iPod zu übertragen, wie auch vom iPod Musik auf ein anderes Speichermedium zu übertragen. Hierfür müssen die Tools nicht installiert werden, sodass sie auf jedem Rechner sofort benutzt werden können, auch direkt vom iPod aus. Im Gegensatz zu YamiPod bietet Floola sogar Unterstützung für Album-Artwork; jedem Song und Album auf dem iPod lassen sich Cover zuordnen, die der iPod beim Abspielen anzeigt.\n\nWeitere Alternativen stellen der Windows-MP3-Spieler Winamp und die Musikbibliothek MediaMonkey dar. Mit dem Plugin „ml ipod“ ist in Winamp inzwischen auch der reibungslose Betrieb mit den aktuellen ipod classic und nano-Modellen bis zur vierten Generation möglich. MediaMonkey lässt sich sogar mit dem iPod touch und dem iPhone nutzen. Interessant an diesen Programmen ist die Möglichkeit des Erstellens einer „Query“ (Datenbankabfrage) zum Übertragen von bestimmten Titeln und der Erstellung und des Synchronisierens von sogenannten „Smart Playlists“ (Winamp) oder „Auto Playlist“. In Winamp ist auch das Übertragen von Fotos und Videos implementiert. MediaMonkey bietet viele Möglichkeiten, MP3-Tags automatisiert zu bearbeiten, so können fehlende Tags und Albumbilder nicht nur per CDDB, sondern auch über amazon.de abgefragt werden.\n\nDer zentrale Prozessor des iPods wird von PortalPlayer hergestellt, die auch das Referenzdesign für den portablen MP3-Player entwarfen, der nach Apples Einstieg zum iPod wurde. PortalPlayer war auch an der Entwicklung der Benutzeroberfläche des iPods beteiligt, an der das Unternehmen Pixo mitgearbeitet hat.\n\n\n\nDas Gehäuse des iPods zeigt Ähnlichkeit mit den von Dieter Rams entworfenen, in den 1960er Jahren produzierten Braun-Taschenradios.\n\nDie Aussprache des Gerätes führte unter manchen Nutzern zu der parodierenden Bezeichnung „Ei-Pott“. Dies ist als umgangssprachlicher Scherz zu bewerten. Für den Namensschutz von „iPod“ zog Apple vor Gericht und verklagte das Unternehmen \"koziol\" wegen eines Eierbechers des Designers Michael Neubauer. Das Erbacher Unternehmen vertrieb diesen bislang unter der Bezeichnung „eiPOTT“. Das Hanseatische Oberlandesgericht in Hamburg hat im August 2010 die Verwechslungsgefahr des Namens „eiPott“ mit dem Medienabspielgerät des Herstellers Apple anerkannt. Nach Angaben von \"koziol\" hatte Apple bislang vergeblich versucht, die Herstellung generell und im zweiten Anlauf den Vertrieb des Eierbechers verbieten zu lassen.\n\n\n"}
{"id": "1047479", "url": "https://de.wikipedia.org/wiki?curid=1047479", "title": "Einplatinencomputer", "text": "Einplatinencomputer\n\nEin Einplatinencomputer, oft engl. \"single-board computer\" (SBC), ist ein Computersystem, bei dem sämtliche zum Betrieb nötigen elektronischen Komponenten auf einer einzigen Leiterplatte zusammengefasst sind. Typischerweise ist das Netzteil als einzige Komponente separat untergebracht. Sie werden in vielen Anwendungsgebieten eingesetzt – z. B. in der Industrie, in elektronisch gesteuerten Gebrauchsgegenständen und im Privat- und Hobbybereich.\n\nGeräte wie der KIM-1, der Microprofessor I und der Apple I bildeten ab Mitte der 1970er-Jahre eine der Vorstufen der späteren Heimcomputer und Personal Computer. Sie wurden oft von Computerenthusiasten und Hackern gebaut und verwendet, später ging ihre Verbreitung bei Privatnutzern stark zurück. Konstruktiv gesehen können auch die meisten Heimcomputer der 1980er wie der Sinclair ZX81, der Commodore 64 oder der Atari ST als Einplatinencomputer angesehen werden, wurden jedoch in der Regel nicht so bezeichnet.\n\nEinplatinencomputer werden in der Industrie vorwiegend in der Mess-, Steuer- und Regelungstechnik (MSR) eingesetzt. Sie ersetzen in vielen Bereichen festverdrahtete Steuerungen, da Änderungen der Steuerungsabläufe einfach durch Modifizierung des Programms realisiert werden können, gleichzeitig sind sie preisgünstiger als aufwendige speicherprogrammierbare Steuerungen (SPS). Eine wichtige Anwendung sind auch sogenannte Eingebettete Systeme in elektronisch gesteuerten Gebrauchsgegenständen wie zum Beispiel Haushaltsgeräten, Kraftfahrzeugen, WLAN-Routern, Geräten der Unterhaltungselektronik oder auch der Medizintechnik. Dabei kommen heute häufig angepasste Varianten des Linux-Betriebssystems zum Einsatz (etwa Embedded Linux).\n\nSeit etwa 2012 haben Einplatinencomputer wieder zunehmend Verbreitung im Privatbereich gefunden, vor allem der Raspberry Pi mit 5 Millionen verkauften Exemplaren (Stand Februar 2015). Sie werden zum Beispiel als preisgünstiges Mediacenter, Heim-Server, als Schulrechner oder als Experimentiergerät unter Linux (z. B. Android) eingesetzt.\n\nEin Einplatinencomputer besteht mindestens aus dem eigentlichen Prozessor, einem Taktgenerator, einer Reset-Logik, einem Festspeicher (ROM) für das Programm und Ein- bzw. Ausgabebaugruppen. Mit dieser Ausstattung lassen sich bereits einfache Ablaufsteuerungen realisieren. Für komplexere Aufgaben werden weitere Komponenten erforderlich, vornehmlich RAM, um Zwischenergebnisse ablegen oder Unterprogramme ausführen zu können, und EEPROM- oder Flash-Speicher für veränderliche Parameter, die einen Stromausfall überstehen müssen.\n\nSehr häufig werden Einplatinencomputer mit Überwachungsschaltungen (Stromausfall-Erkennung, Watchdog) versehen, die den Rechner im Falle eines unerwarteten Fehlers im Programm oder in der Stromversorgung in einen definierten Ausgangszustand zurücksetzen.\n\nWeiterhin können Einplatinencomputer mit Analog-Digital-Wandlern, Zählerbausteinen, Kommunikationsschnittstellen und anderen speziellen Schaltungen an die jeweilige Anwendung angepasst werden.\n\nEinplatinencomputer kamen Ende der 1970er-Jahre mit der zunehmenden Verbreitung von Mikroprozessoren auf den Markt. Sie deckten zunächst den Bedarf an preiswerten Entwicklungssystemen ab, waren aber von Beginn an auch dafür konzipiert, in Produktivumgebungen eingesetzt zu werden. Sie werden in der Industrie vorwiegend in der Mess-, Steuer- und Regelungstechnik (MSR) eingesetzt. Sie können dort in vielen Bereichen festverdrahtete Steuerungen vorteilhaft ersetzen, da Änderungen der Steuerungsabläufe in den meisten Fällen durch einfachen Austausch des Programms realisiert werden können. Gleichzeitig sind sie wesentlich billiger als speicherprogrammierbare Steuerungen (SPS) und können daher wirtschaftlich in Bereichen eingesetzt werden, für die eine große SPS überdimensioniert wäre.\n\nMit der fortschreitenden Entwicklung der Mikrocontroller, die neben dem eigentlichen Prozessorkern immer mehr Funktionen in einem Chip vereinigten, erweiterte sich auch das Anwendungsspektrum. Heute stecken die Abkömmlinge der Einplatinencomputer in Waschmaschinen, Automatikgetrieben, Fernbedienungen, Heizungssteuerungen und unzähligen anderen Geräten des täglichen und industriellen Bedarfs. Dabei entwickelten sich die kleinen Rechner sowohl zu immer größerer Leistungsfähigkeit, auch mit 16-Bit- und 32-Bit-Prozessoren, als auch in Richtung winziger Minimalsysteme mit nur wenigen, einfachen Funktionen (BASIC-Briefmarke).\n\nSehr viele Heimcomputer, wie z. B der C64 oder der Sinclair ZX81, sind aus konstruktiver Sicht ebenfalls Einplatinencomputer mit integrierter Videoausgabe, ergänzt durch Tastatur, weitere Bedienungselemente, Anschlüsse für Peripheriegeräte und meistens irgendeine Form von Massenspeicher – ohne diese Erweiterungen sind sie jedoch nicht sinnvoll einsetzbar. Ebenso enthalten auch moderne Smartphones und Tabletcomputer in ein Gehäuse eingebaute Einplatinencomputer – meist auf Basis der ARM-Mikroprozessoren – werden jedoch ebenfalls nicht so bezeichnet.\n\nIn der frühen Computerentwicklung waren diese Einplatinenrechner sehr verbreitet, so zum Beispiel die Modelle Apple I, KIM-1, SYM-1 und AIM-65, alle auf Basis des 6502-Mikroprozessors.\n\nEin Bausatz auf Basis der 6502-CPU wurde im Mai 1980 vom Elektor-Verlag als Junior-Computer vorgestellt und entwickelte sich mit mehreren tausend Exemplaren in Europa zu einem beliebten Selbstbaucomputer. Entwickelt wurde der Junior-Computer bei Elektor von Loys Nachtmann, der später als Redakteur zur Zeitschrift Chip wechselte.\n\nWeitere Beispiele sind der Selbstbaucomputer Cobold und der 1984 in der c’t vorgestellte CEPAC-65, der sich als universeller Steuercomputer eignete.\n\nEinplatinenrechner gab es auch mit dem Intel-8085-Prozessor, zum Beispiel der Experimentiercomputer ECB85 von Siemens, der 1979 auf den Markt kam.\n\nEin früher Selbstbaucomputer wurde vom Elektor-Verlag im Juni 1977 (G 3078EX) auf der Basis des SC/MP (Typ I) veröffentlicht. Ein weiterer Bausatz wurde seitens des Elektronikherstellers und Lehrmaterialanbieters Christiani-Verlag aus Konstanz als SC/MP-Lehrcomputer 1978 auf den Markt gebracht.\n\nAls einer der ersten in Deutschland verfügbaren Einplatinenrechner kam 1977 der Nascom 1 auf Basis der Zilog-Z80-CPU auf den Markt. Die ersten Computer des englischen Herstellers Sinclair Research, der Sinclair ZX80 und der ZX81 sowie die ZX-Spectrum-Serie, waren ebenfalls Einplatinenrechner. Auf der Basis von Z80-kompatiblen CPUs wurden weitere Einplatinencomputer (sog. SBC) realisiert, so u. a. der NDR-Klein-Computer aus dem Jahre 1984 von Rolf-Dieter Klein sowie der Euro-Z80 (Elektor-Verlag) aus dem Jahre 1989. Auch der Z80-EMUF (Einplatinen-Mikrocomputer für universelle Festwertprogramm-Anwendung) ist ein verbreiteter Einplatinentyp im Selbstbau gewesen. Da Zilogs Z80-Prozessor auch heute noch verfügbar ist, werden aktuell wieder neue Systeme mit Taktraten von 20 MHz entwickelt.\n\nBis in die 1990er-Jahre wurden 8-Bit-Einplatinencomputer, z. B. mit Intel-8085-Prozessor, Intel-MCS-51-Mikrocontrollern und ihren Nachfolgern, zu Mess-Steuer-Regel-Zwecken (MSR) eingesetzt. Heute werden Einplatinenrechner mit leistungsfähigeren Microcontrollern bestückt und decken den unterschiedlichen Bedarf für Haushalts-, Industrie-, Automobil- und Militärzwecke als Eingebettetes System ab. Der Trend geht dazu, immer mehr Funktionen (Standardschnittstellen, A/D-Wandler usw.) des Einplatinencomputers direkt auf demselben Chip wie die CPU zu integrieren (siehe System-on-a-Chip). Dadurch kann bei steigender Funktionalität der Stückpreis dieser Rechner gesenkt werden.\n\nZu den bekannten aktuellen Einplatinenrechnern für Endnutzer zählen Arduino, BeagleBoard, Cubieboard, Ethernut, PandaBoard, Raspberry Pi, Tinkerforge, Banana Pi, pcDuino, Orange Pi, ODROID, NanoPC, HummingBoard und der für Bildungszwecke konzipierte . Solche Rechner werden etwa als Musik-Streaming-Client, Media Center, Thin Client oder Server, als Steuerungsplatine in einem Quadrocopter, als Wetterstation, UKW-Radiosender oder als Steuereinheit für dedizierte Bitcoin-Mining-Hardware verwendet.\n\n\n"}
{"id": "1048652", "url": "https://de.wikipedia.org/wiki?curid=1048652", "title": "EMUF", "text": "EMUF\n\nEMUF (\"Einplatinen-Mikrocomputer für universelle Festprogrammierung\") war in den 1980er-Jahren ein verbreiteter Einplatinentyp für den Selbstbau auf Basis der 6504-CPU (650x-Familie), des Z80 und später auch weiterer Prozessorfamilien.\n\nDer erste Einplatinencomputer dieser Bezeichnung wurde vom Franzis-Verlag in der Computer-Zeitschrift mc 1981 veröffentlicht. Die Entwicklung des 6504-EMUF wurde von Herwig Feichtinger verantwortet. Der EMUF war ein Einplatinencomputer im Europakarten-Format (100 × 160 mm). Der damalige Preis betrug für den kompletten Bausatz weniger als 100 DM (rund 50 Euro) und erlaubte es, einen funktionsfähigen Computer für Steuerungszwecke aufzubauen.\n"}
{"id": "1048964", "url": "https://de.wikipedia.org/wiki?curid=1048964", "title": "Tandy TRS-80 Model 100", "text": "Tandy TRS-80 Model 100\n\nDer TRS 80 Modell 100 des Elektronikhändlers Tandy RadioShack aus dem Jahr 1983 gehörte zusammen mit Olivetti M-10, NEC PC-8201 und NEC PC-8300 zu einer Familie von portablen Computern, die auf der Plattform des Kyotronic 85 von Kyocera basierten. Das Design mit einem sparsamen CMOS-Prozessor 8085 und statischem CMOS-RAM (8–32 KiB) ermöglichte mit 4 AA-Batterien eine Betriebszeit von bis zu 20 Stunden. Besonders unter Journalisten war das Gerät wegen seines eingebauten 300 Baud-Modems und der enthaltenen Textverarbeitung sehr verbreitet. Weltweit wurden über 6 Millionen Stück verkauft.\n\nFür das Model 100 wurden verschiedene Erweiterungen angeboten:\nAußerdem existierte die Möglichkeit zusätzliche Software in Form eines Option-ROM zu installieren. Nur ein ROM konnte gleichzeitig installiert sein.\nVerschiedene Tandy Peripheriegeräte, insbesondere aus dem Heimcomputerbereich, wie der 4-Farb-Plotter CGP-115, oder der Cassettenrekorder CCR-82, wurden in speziellen Varianten für das Model 100 angeboten. Meist beschränkten sich die Anpassungen jedoch auf eine Farbgebung in Weiß mit schwarzen Zierlinien, passend zum Model 100.\n\n1984 wurde der Tandy 200 als erweiterte Version des Model 100 vorgestellt. Weitgehend kompatibel bot er eine doppelt so große Anzeige (16×40 Zeichen) sowie einen Speicherausbau bis 72 KiB, organisiert als 3 Banks mit je 24 KiB. Softwareseitig wurde Multiplan als Tabellenkalkulation hinzugefügt.\n\nIm Oktober 1985 wurde der Tandy 600 als letzte Entwicklung mit eigener Struktur und Software im ROM vorgestellt. Spätere Geräte waren handelsübliche IBM-kompatible Laptops. Er basierte nicht mehr auf dem Kyocera Design, sondern verwendete eine 80C88 (3,07 MHz) sowie Microsofts \"Handheld DOS\" (HHDOS). Der Tandy 600 bot 32-224 KiB batteriegepuffertes RAM sowie ein 3½ Zoll Diskettenlaufwerk im Standard MS-DOS Format. BASIC war nicht mehr Teil der Grundausstattung, ein BASIC ROM konnte jedoch anstelle des Multiplan ROMs installiert werden. \n\nDas Model 102 ersetzte 1986 das Model 100 und unterschied sich von diesem nur durch ein flacheres Gehäuse, eine Grundausstattung von 32 KiB RAM, sowie einiger Fehlerkorrekturen im ROM.\n\n"}
{"id": "1049226", "url": "https://de.wikipedia.org/wiki?curid=1049226", "title": "Sirius I", "text": "Sirius I\n\nDer Victor 9000 / Sirius 1 war ein Personal Computer, der von dem US-amerikanischen Ingenieur Chuck Peddle entwickelt wurde. Er hatte zuvor auch schon den Commodore PET 2001 entwickelt. Dieser Computer erschien im selben Zeitraum (1981/82), als auch der erste IBM PC auf den Markt kam. Der Sirius 1 war aber in manchen Punkten weitaus innovativer und besser als das IBM Gegenstück. Der Rechner war mit einem Intel 8088 Prozessor (4,77 MHz) ausgestattet. Der Speicher des Gerätes bestand in der Grundversion aus 128 KByte, konnte aber bis 1 MByte ausgebaut werden. Die hochauflösende Grafik des Sirius konnte erst ab 256 KByte vollständig ausgenutzt werden. Der Sirius 1 hatte zwei RS 232 Schnittstellen und konnte mit den Betriebssystemen CP/M-86 und MS-DOS betrieben werden.\n\nIn Europa war der Sirius 1 sogar relativ erfolgreich, da dieser Rechner vor dem IBM PC dort erschien. Weltweit konnte sich der Rechner nicht durchsetzen und wurde schnell bei den Verkaufszahlen durch den IBM PC überholt.\n\nDer Rechner hatte einen 12 Zoll monochromen Monitor, der eine Anti-Reflexionsschicht besaß. Kontrast und Helligkeit des Monitors konnten direkt von der Tastatur gesteuert werden. Der Computer konnte schon damals die sehr fortschrittliche Auflösung von 800 × 400 Pixeln darstellen.\n\nDer Sirius wurde mit einem internen Sampler geliefert, mit dem man kurze Tonsequenzen aufnehmen und dann wieder abspielen konnte. An den Sirius 1 konnte zusätzlich ein Lichtgriffel angeschlossen werden.\n\nDer Sirius 1 wurde in drei Hardware-Konfigurationen angeboten:\n\nDas Spitzenmodell kostete in Deutschland 1982 19.995,- DM inkl. Mwst., nach heutiger Kaufkraft etwa Euro\n\nDer Sirius 1 konnte folgende Betriebssysteme nutzen:\n\n\nAls Programmiersprachen standen folgende zur Verfügung:\n\nAuf dem Rechner standen weiterhin folgende Software zur Verfügung:\n\n"}
{"id": "1049613", "url": "https://de.wikipedia.org/wiki?curid=1049613", "title": "Xyz-Format", "text": "Xyz-Format\n\nDas xyz-Format wird in der Computerchemie und Bioinformatik verwendet und enthält kartesische Koordinaten in drei Dimensionen zur Beschreibung der Positionen von Atomen in Molekülen. Als Maßeinheit werden meist Ångström, seltener atomare Einheiten verwendet. \n\nDieses Format wird von vielen Programmen verstanden, eine formale Definition wurde aber nie veröffentlicht. Deshalb kann sich das Format je nach verwendetem Computerprogramm leicht unterscheiden, bzw. das jeweilige Programm kann Probleme beim Lesen nicht-konformer xyz-Dateien haben.\n\nWie die meisten in der Computerchemie verwendeten Dateiformate ist auch das xyz-Format zeilenorientiert aufgebaut:\n\nIn seiner allgemeinsten Spielart erlaubt das xyz-Format in den Atomspezifikationen ab der dritten Zeile noch die Angabe einer (Atomladung, Kommazahl), dreier (kartesischer Schwingungsvektor, 3 Kommazahlen) oder vierer (Schwingungsvektor und Atomladung) zusätzlicher Spalten.\n\nAnders als andere weitverbreitete Dateiformate (z. B. pdb-Format) gibt das xyz-Format keine festen Zahlenformate und -positionen vor.\n\nIm Folgenden ist eine Beispieldatei für das Wassermolekül dargestellt. Alle Koordinatenangaben (ab Zeile 3, Spalte 2 bis 4) sind in Ångström, die Ladungen als Vielfaches der Elementarladung angegeben:\n\n"}
{"id": "1053261", "url": "https://de.wikipedia.org/wiki?curid=1053261", "title": "Fdisk", "text": "Fdisk\n\nfdisk – Original-Schreibweise FDISK in DOS-üblicher Form (siehe 8.3) – bezeichnet mehrere Partitionierungsprogramme für Blockgeräte wie Festplatten oder Disketten. Das Wort kommt von , eine von IBM eingeführte englische Bezeichnung für Festplatte.\n\nPC DOS 2.0 (1983) und die von Microsoft für alle IBM-PC-kompatiblen Computer veröffentlichte Version MS-DOS 2.0 enthielten die von IBM entwickelte allererste Version von codice_1, die gemeinsam mit MS-DOS weiterentwickelt wurde. Als Programm auf der Kommandozeile (Konsolenprogramm) kann FDISK sowohl per Kommandozeilenparameter als auch per zeichenorientierter Benutzerschnittstelle (dialog- oder menü-basiert) bedient werden. Andere Betriebssysteme enthalten meist einen an das MS-DOS-FDISK angelehnten Klon, ebenfalls unter dem Namen codice_2, wie beispielsweise PC-Unix und PC-kompatibles DOS.\n\nFDISK kann ursprünglich nur Partitionen des (MBR) bearbeiten. Der MBR wurde gemeinsam mit PC DOS 2.0 und dem „ Version 1.0“ (codice_1, deutsche Übersetzung „Festplatten-Einrichtungsprogramm“) 1983 eingeführt. Der ist ein Bootsektor und enthält neben dem Startprogramm auch die eigentliche Partitionstabelle, die auf vier primäre Partitionen begrenzt ist. Ab MS-DOS 3.2 ist auch eine erweiterte Partition möglich, innerhalb derer weitere Partitionen als logische Laufwerke definiert sein können.\n\nEs gibt auch grafische Varianten von FDISK, etwa codice_4 von OS/2. Einige Weiterentwicklungen von fdisk, z. B. die in util-linux enthaltene Variante, können zusätzlich zum MBR auch mit GUID-Partitionstabellen (GPT, ) umgehen – die GUID-Partitionstabelle ist der Nachfolger des auf IBM-PC-kompatiblen Computern.\n\nDer IBM-PC XT (Typ 5160) von 1983 war der erste Vertreter der später als IBM-PC-kompatible Computer bezeichneten Systeme, der mit einer Festplatte ausgestattet war. Das dazu gelieferte Betriebssystem PC DOS bzw. MS-DOS 2.0 führte für diese Neuerung eine Unterteilung des Festplattenspeichers ein, sogenannte Partitionen, die in der dazu eingeführten Partitionstabelle „“ (MBR) definiert sind. Aus Kompatibilitätsgrunden wurde die Partitionstabelle innerhalb des Bootsektors untergebracht – dieser beinhaltet beim IBM-PC und kompatiblen Computern das Startprogramm, welches im Chainloading-Prinzip das Betriebssystem, z. B. MS-DOS, von der als aktiv markierten Partition startet.\n\nFestplatten waren um ein Vielfaches größer als Disketten. Ausgedrückt in damals üblichen Größenverhältnissen: eine 20 MiB große Festplatte ist um den Faktor 57 größer als eine damals übliche 360‑kB-Diskette. Die Dateisysteme dieser Zeit waren FAT12 für Disketten und FAT16 für Festplatten. Das FAT16-Dateisystem wies anfangs jedoch ein Größenlimit von 32 MiB auf, sodass größere Festplatten (damals 40 MiB und mehr) partitioniert werden mussten. DOS-FDISK folgt dieser Entwicklung: die Limits wurden sowohl beim FAT-Dateisystem als auch bei MBR-Partitionen sukzessive erhöht. Dementsprechend kann codice_1 aus MS-DOS/PC DOS 2.0 nur Partitionen bis 16 MiB anlegen. Ab MS-DOS 3.2 (1985) sind Partitionen bis 32 MiB möglich und die erweiterte Partition wurde eingeführt. Mit MS-DOS 3.3 (1987) wurden innerhalb der erweiterte Partitionen mehr als ein logisches Laufwerk unterstützt, was nun mehr als zwei Laufwerksbuchstaben pro Festplatte ermöglicht, sowie die Unterstützung für eine zweite Festplatte eingeführt.\n\nWeil unter PC-DOS/MS-DOS-kompatiblen DOS-Betriebssystemen die Anzahl der (logischen) Laufwerke auf 26 beschränkt ist – von codice_6 bis codice_7 (siehe Laufwerksbuchstaben) – sind insgesamt nur 24 Partitionen möglich. Die Laufwerksbuchstaben codice_6 und codice_9 sind unter MS-DOS in jedem Fall für Diskettenlaufwerke reserviert. Bei einer primären Partitionen sind somit noch maximal 23 logische Laufwerke innerhalb der erweiterten Partition möglich. Während das anfängliche Limit von maximal vier primären Partitionen ein Hindernis darstellen konnte, ist die Anzahl möglicher (logischer) Laufwerke durch Hinzufügen einer erweiterten Partition nicht nur für die damalige Zeit absolut ausreichend.\n\nIn PC DOS bzw. IBM DOS ist jeweils dasselbe FDISK wie in MS-DOS der gleichen Version enthalten, da IBM das größtenteils unmodifizierte Betriebssystem von Microsoft für die hauseigene IBM-PC-Produktlinie nutzte. Der einzige Unterschied war, dass MS-DOS auch auf Klonen installiert werden konnte – es gab zahlreiche OEM-Versionen – während IBM DOS nur auf dem Original-IBM-PC nutzbar war – bis IBM DOS 3.3 (1986), welches erstmals auch gezielt als Alternative für IBM-PC-kompatible Computer vermarktet wurde.\n\nAufgrund einer mit IBM geschlossenen gemeinsamen Entwicklungsvereinbarung (, ) sind ab MS-DOS 3.3 die von IBM geschriebenen DOS-Programme auch in der von Microsoft veröffentlichten Betriebssystemvariante enthalten.\n\nAb MS-DOS 4.0 wurde FDISK eine EXE-Datei, codice_10. Partitionen können ab dieser Version bis zu 2,1 GiB groß sein, was zugleich auch das Limit von FAT16B („BigDOS“) ist.\n\nAb MS-DOS 6.0 (und dem darin enthaltenen FDISK) werden bis zu acht Festplatten unterstützt; zwei IDE-Festplatten und bis zu sechs SCSI-Festplatten, für die erst über einen Treiber im Betriebssystem ein Zugriff ermöglicht wird.\n\nAb PC DOS 6.1 wurde das Betriebssystem von IBM eigenständig und ohne Microsoft weiterentwickelt.\n\nMS-DOS 7.0 bis 8.0 sind keine eigenständigen Betriebssysteme mehr, sondern Bestandteil der Windows-9x-Betriebssystemreihe (Windows 95 bis Windows Me).\n\nOS/2 ist bis Version 4.0 mit zwei Partitionsmanagern ausgestattet, dem textbasierten codice_10 und dem grafischen Programm codice_4. Beide haben gleiche Funktionen und können FAT- und HPFS-Partitionen erstellen. OS/2-FDISK kann komplett über die Kommandozeile gesteuert werden, inklusive des Löschens von Partitionen, und kann den OS/2-Bootmanager installieren. OS/2-Versionen ab 4.5 unterstützen das Dateisystem JFS und haben FDISK durch einen (LVM) ersetzt.\n\nAufgrund der gemeinsamen Entwicklungsgeschichte von OS/2 und Windows NT hat NTFS den gleichen Partitionstyp wie HPFS.\n\nMS-DOS-basierte Windows-Versionen, auch als Windows 9x bezeichnet, enthalten ein nahezu vollwertiges MS-DOS, sowie ein FDISK, das ab MS-DOS 7.10 (Windows 95b, Windows 98) und 8.00 (Windows Me) FAT32-Partitionen erstellen kann. Für Festplatten, die größer als 32 GiB sind, benötigt MS-DOS 7.10 einen Patch, der nur für Windows 98 zur Verfügung steht. Die FDISK-Programme von Windows 98 bis Windows Me können jedenfalls Festplatten von bis zu 128 GiB verwalten – mehr, wenn das BIOS die 48-Bit-Adressierung LBA-48 unterstützt. Allerdings weisen diese Versionen von FDISK ein Limit von 64 GiB bei der Anzeige der Festplattenkapazität auf, was ebenfalls durch einen Patch von Microsoft behoben werden kann.\n\nIn Windows NT ist codice_10 nicht enthalten. Ab Windows NT 3.1 wird stattdessen die grafische Datenträgerverwaltung () verwendet und ab Windows 2000 steht zusätzlich das Konsolenprogramm diskpart zur Verfügung.\n\nAlternative Betriebssysteme verwenden auf PCs üblicherweise das vorhandene Partitionsschema – auf IBM-PC-kompatiblen Computern mit BIOS (für „“) somit die Partitionstabelle im (MBR). Es gibt daher zahlreiche Programme mit dem Namen codice_2 – oder ähnlichem Dateinamen bzw. Programmnamen – die zum Verwalten von Partitionen auf den jeweiligen Betriebssystemen dienen und dabei in Funktion und Verhalten mehr oder weniger an IBMs PC-DOS-FDISK angelehnt sind.\n\nDadurch dass alternative Betriebssysteme die Partitionstabelle der jeweiligen Computerplattform unterstützen und mit dem enthaltenen FDISK-Klon verwalten können, ermöglichen sie Multi-Boot-Systeme. Folglich ist auf IBM-PCs mit BIOS allen FDISK-Klonen die Unterstützung der MBR-Partitionstabelle gemein. Allerdings wurde seit ca. 2010 das BIOS zunehmend durch UEFI und somit die Partitionstabelle im MBR durch die GUID-Partitionstabelle (GPT) ersetzt, sodass PC-Betriebssysteme nun ebenfalls GPT verwenden können (müssen).\n\nEiner der Hauptkonkurrenten von PC DOS und MS-DOS auf dem IBM-PC war CP/M von Digital Research. Später wurden aus CP/M-86 auch PC-DOS-kompatible Versionen entwickelt, wie Concurrent DOS, DOS Plus und DR DOS. Auch in diesen Variante ist ein von Digital Research entwickelter FDISK-Klon enthalten, der stark an IBMs FDISK angelehnt ist.\n\nWie bei PC DOS und MS-DOS wurde auch DR-DOS-FDISK nicht weiterentwickelt und kann mit der GUID-Partitionstabelle nicht umgehen.\n\nDiese alternativen Betriebssysteme sind nicht selten Unix-Derivate. Auf PCs laufen neben weit verbreiteten Betriebssystemen wie Windows und macOS daher meist diverse Unices wie BSD oder Linux, die zur Installation ihrerseits MBR- und GPT-Partition verwalten können müssen und daher ein entsprechendes Partitionierungsprogramm bereitstellen. Meist ist dies auch eine Variante von fdisk. macOS (ab 2001, ursprünglich „Mac OS X“) selbst, das ein FreeBSD-Userland bietet, enthält ebenfalls codice_2 zum Manipulieren von „PC-Partitionstabellen.“ Auf EFI-PCs kann fdisk zumindest eine vorhandene GPT-Partitionierung erkennen, selbst verwalten oder bietet ein weiteres and fdisk angelehntes separates Partitionierungsprogramm. Unter macOS ist dies etwa das Konsolenprogramm codice_16.\n\nSo wie OS/2-FDISK zusätzlich HPFS und Windows-9x-FDISK zusätzlich FAT32 unterstützt, kennen FDISK-Klone noch Partitionstypen (und Dateisysteme) des jeweiligen Betriebssystems – zusätzlich zu den mit MS-DOS eingeführten Partitionstypen. So kann z. B. ein für Linux geschriebenes fdisk auch Swap- (Partitionstyp codice_17) und ext2-Partitionen (Partitionstyp codice_18) anlegen.\n\nLinux-Distributionen stellen verschiedene Variante von fdisk bereit. Neben dem traditionellen codice_2 ist dies auch oft codice_20 – beide Programme sind kommandozeilen- und dialogbasiert und bieten zahlreiche Optionen. Das Programm codice_21 bietet eine einfache, menübasierte Handhabung. Das Programm codice_22 gilt als eher umständlich und für Experten ausgelegt, bietet aber als einziges mit dem Parameter die Möglichkeit, die Struktur einer Partitionierung (d. h. ohne Bootcode) für ein späteres Neupartitionieren abzuspeichern.\n\nDas mit FreeDOS ausgelieferte \"Free FDISK\" ist ein freies und erweitertes Partitionierungsprogramm, das auch auf anderen PC-kompatiblen DOS-Versionen lauffähig ist. Ebenso kann der erweiterte Partitionsmanager XFDisk als freier Ersatz für das MS-DOS/PC-DOS-codice_23 verwendet werden. XFDisk bietet viele Optionen, die im Standard-FDISK nicht enthalten sind.\n\nDie GUID-Partitionstabelle (GPT) ist das Partitionsschema des BIOS-Nachfolgers UEFI: Design-bedingt enthält jeder GPT auch eine MBR-Partitionstabelle, die eine einzige primäre Partition enthält, welche sich über den gesamten vorhandenen Speicherbereich erstreckt. Diese Partitionstabelle wird als Schutz-MBR, , bezeichnet, denn sie soll die im GPT definierten Partitionen und Daten vor unabsichtlichem Löschen durch ältere Betriebssysteme und Programme schützen. Für alte Software – bei Programmen für den PC kann man seit 1983 davon ausgehen, dass diese eine MBR-Partitionstabelle erkennt – markiert der Schutz-MBR den gesamten Speicherbereich als belegt. Wäre dies nicht so, wären die vorhandenen Daten in Gefahr, weil alte Software den GPT nicht erkennt und den Speicher somit als vermeintlich frei erkennen würde.\n\nIn einigen Varianten von fdisk wurde daher die Unterstützung für die GUID-Partitionstabelle ebenfalls aufgenommen: Neuere Versionen von fdisk erkennen das Vorhandensein von GPT-Datenstrukturen und geben eine Warnung aus. fdisk würde sonst zwar den Schutz-MBR manipulieren, nicht aber die im GPT definierten Partitionen, was zu Datenverlust führt.\n\nAndere Varianten von fdisk wurden um die Funktion erweitert, selbst GUID-Partitionstabellen (GPT) verwalten zu können. Wieder andere an MS-DOS-FDISK angelehnte Partitionierungsprogramme wurden rein für die GUID-Partitionstabelle, ohne Bearbeitungsmöglichkeit für die MBR-Partitionstabelle, neu geschrieben.\n\nGeschichtlich bedingt sind die Limits für das Betriebssystem, für das FDISK geschrieben wurde, und FDISK selbst sehr nahe beieinander. Daher ist das Betriebssystemlimit meist auch das FDISK-Limit.\nObwohl im bis zu vier primäre Partitionen möglich wären, kann FDISK nur eine einzige primäre Partition erstellen. Um eine zweite Festplatte oder mehr als ein logisches Laufwerk nutzen zu können, muss sowohl der DOS-Kernel als auch FDISK erweiterte Partitionen kennen – eine mit dem in MS-DOS 3.3 enthaltenen FDISK partitionierte Festplatte funktioniert daher nicht ohne weiteres auf einer früheren Version von PC DOS/MS-DOS, da dieses die logischen Laufwerke in der erweiterten Partition nicht verwenden kann. Ebenso kann MS-DOS vor Version 5.0 nur eine einzige primäre Partition verwenden.\n\nGenau eine primäre Partition kann als erweiterte Partition definiert sein. Diese kann eine beliebige Anzahl logischer Laufwerke enthalten, wobei MS-DOS nur maximal 24 logische Festplattenlaufwerke verwenden kann. Ist nur eine Festplatte im PC und auf der Festplatten ebenfalls genaue eine primäre Partition (die keine erweiterte Partition ist) vorhanden, so können in der erweiterten Partition noch 23 logische Laufwerke von MS-DOS verwendet werden – bei drei Primärpartitionen (ab MS-DOS 5.0) sind es nur noch 21 logische Laufwerke. Diese müssen innerhalb eines zusammenhängenden Speicherbereichs, der durch die erweiterte Partitionen festgelegt ist, definiert sein. Bei mehr als einer Festplatte reduziert sich die Anzahl der in der erweiterten Partition nutzbaren logischen Laufwerke entsprechend der Gesamtanzahl an Partitionen und logischen Laufwerken auf allen Festplatten.\n\nOS/2 akzeptiert wie MS-DOS (vor Version 5.0) nur eine primäre Partition pro Festplatte.\n\nMS-DOS (und PC DOS) können nicht von einer erweiterten Partition starten. OS/2 hingegen schon – wenn allerdings ein logisches Laufwerk in der erweiterten Partition hinzugefügt oder gelöscht wird und sich damit die Laufwerksbuchstaben (welche automatisch vergeben werden) verändern, kann OS/2 eventuell nicht mehr starten.\n\nWie MS-DOS (bzw. kompatible DOS) und Windows unterliegt auch Unix den BIOS-bedingten Einschränkungen beim Betriebssystemstart. Ältere BIOS-Versionen konnten auf Bereiche oberhalb von 1024 Zylindern nicht zugreifen. Daher legten viele Anwender eine kleinere Startpartition für den Unix-Kernel so an, dass sie unterhalb der 1024-Zylinder-Grenze beginnt. Ebenso verhält es sich mit der 137-GiB-Grenze, wenn LBA nicht als 48-Bit-Zahl (LBA-48) vom BIOS implementiert wurde. Ist die Unix-Startpartition unterhalb der BIOS-bedingten Grenze, ist ein Starten möglich und das gestartete Betriebssystem kann anschließend auch den oberhalb liegenden Bereich nutzen. Meist richten die Installationsprogramme neben der Startpartition zumindest eine root-Partition für das Wurzelverzeichnis ein sowie eine Swap-Partition. Da viele Unix-Bootloader erweiterte Partition unterstützen, können alle für Unix relevanten Partition als logisches Laufwerk definiert sein, jedoch kann es nötig sein, die Startpartition codice_24 wegen der BIOS-Limitierung als Primärpartition erstellen zu müssen.\n\n\n"}
{"id": "1055449", "url": "https://de.wikipedia.org/wiki?curid=1055449", "title": "Die Madagascar-Pinguine in vorweihnachtlicher Mission", "text": "Die Madagascar-Pinguine in vorweihnachtlicher Mission\n\nDie Madagascar-Pinguine in vorweihnachtlicher Mission ist ein elfminütiger Kurzfilm über die Pinguine aus \"Madagascar\", der vor dem Animationsfilm \"\" ab dem 13. Oktober 2005 in den internationalen Kinos lief. Zudem ist er als Bonus auf der Madagascar-DVD enthalten. Der Film wurde in den Dreamworks Animation Studios entwickelt.\n\nWährend die weihnachtlichen Planungen der vier Pinguine \"Kowalski, Private, Rico\" und \"Skipper\", deren Chef, gerade in vollem Gang sind, bemerkt \"Private\", dass \"Ted der Eisbär\" an den Feiertagen ganz allein ist, und will ihm zu Weihnachten etwas schenken. Da ihn die anderen nicht unterstützen, geht er in die Stadt, um ein Geschenk für \"Ted\" zu finden. Als die anderen drei bemerken, dass er fehlt, machen sie sich auf den Weg, um ihm zu folgen.\n\nWährenddessen wird \"Private\" von einer alten Dame als Weihnachtsgeschenk für deren Hund \"Mr. Chew\" gekauft. Um ihn zu retten, dringen die anderen in die Wohnung der alten Dame ein. Dort müssen sie zuerst gegen \"Mr. Chew\" vorgehen, damit dieser \"Private\" nicht frisst. Nachdem der Hund im Weihnachtsstrumpf verstaut ist, machen sie sich auf den Heimweg. Hierzu sprengt \"Rico\", der bereits in drei Szenen zuvor etwas in die Luft sprengen wollte, die Tür auf. Nachdem der Weg frei ist und die alte Lady auch noch glaubt, ihr Hund hätte das Chaos angerichtet, machen sie sich auf den Weg zurück in den Zoo. Schließlich laden sie doch noch \"Ted den Eisbären\" ein, der, nicht gerade zur großen Freude der Pinguine, alle anderen Tiere des Zoos eingeladen hat, welche dann am Ende noch ein Ständchen singen.\n\n\"Die Madagascar-Pinguine in vorweihnachtlicher Mission\" ist ein Ableger des Films \"Madagascar\". Im November 2008 lief die Fortsetzung von \"Madagascar\", Madagascar 2, in dem die Pinguine auch wieder mit von der Partie sind, im Kino an.\n\n2008/2009 wurde eine Zeichentrickserie mit dem Titel \"Die Pinguine aus Madagascar\" produziert, die seit 28. März 2009 im amerikanischen Fernsehen ausgestrahlt wird.\n\n"}
{"id": "1059233", "url": "https://de.wikipedia.org/wiki?curid=1059233", "title": "Karazuba-Algorithmus", "text": "Karazuba-Algorithmus\n\nDer Karazuba-Algorithmus ist ein Algorithmus zur Multiplikation zweier großer ganzer Zahlen. Er wurde 1960 von dem 23-jährigen Anatoli Alexejewitsch Karazuba (engl. Karatsuba, ) entwickelt und 1962 veröffentlicht.\n\nBezeichnet formula_1 die Bit-Anzahl der beiden Zahlen und formula_2 ein Landau-Symbol, so ist der Algorithmus mit einer Laufzeitkomplexität von formula_3 deutlich schneller als die Schulmethode. Diese (und auch deren implizite Übertragung auf das Binärsystem in Form der russischen Bauernmultiplikation) besitzt eine Laufzeitkomplexität von formula_4. Die Methode von Karazuba wurde zum Vorbild für das Teile-und-herrsche-Prinzip in der Informatik. Für hinreichend große Zahlen ist der Karazuba-Algorithmus langsamer als seine Verallgemeinerungen, wie der Toom-Cook-Algorithmus (1965) und der Schönhage-Strassen-Algorithmus (1971), dessen Laufzeitkomplexität formula_5 beträgt und der aus Sicht der Komplexitätstheorie als schnellster Algorithmus zur Multiplikation großer ganzer Zahlen galt, bis 2007 Martin Fürer eine Weiterentwicklung mit einer (bisher nur theoretisch) geringeren Laufzeitkomplexität vorstellte.\n\nMultiplizieren verursacht in der Schulmethode quadratischen Aufwand, während Additionen und Verschiebeoperationen, bei denen mit einer Potenz der Basis des verwendeten Stellenwertsystems multipliziert wird, nur linearen Aufwand benötigen. Die Idee ist, nach dem Teile-und-herrsche-Prinzip die beiden zu multiplizierenden Zahlen in zwei Teile aufzuspalten, und die Multiplikationen soweit möglich durch Additionen und Verschiebeoperationen zu ersetzen. Das Ausmultiplizieren der aufgeteilten Zahlen ergibt drei Teilterme, die durch vier Multiplikationen gebildet werden. Diese können durch Verschiebe- und Additionsoperationen zum Gesamtergebnis zusammengesetzt werden. Einer dieser Terme ist dabei eine Summe zweier Produkte. Dieser Term lässt sich als Differenz mit einem neuen Produkt und der Summe der anderen beiden Teilterme schreiben. Insgesamt spart man so also eine Teilmultiplikation ein. Führt man dieses Verfahren rekursiv durch, so erhält man eine wesentlich günstigere Laufzeit als nach der Schulmethode.\n\nDer hier angegebene Algorithmus gilt für natürliche Zahlen, er lässt sich aber leicht auch auf ganze Zahlen verallgemeinern, indem ihre Vorzeichen gesondert berücksichtigt werden.\nDie Faktoren formula_6 und formula_7 seien im Stellenwertsystem zur Basis formula_8 als Tupel dargestellt. Der Wert von formula_8 ist unerheblich: Etwa in einem Computer mit einem Multiplizierer für 32 Bit breite Zahlen würde formula_10 gewählt werden. Die Beispiele weiter unten verwenden Dezimalzahlen. Um die Rekursion bis formula_11 durchführen zu können, seien die Längen beider Zifferntupel eine Zweierpotenz formula_12 mit formula_13, und es sei formula_14. Das ist immer erreichbar durch geeignet vorangestellte Nullen; an der unten durchgeführten Laufzeitabschätzung ändert sich dadurch nichts Wesentliches.\n\nDie Zifferntupel seien also\nJedes Zifferntupel wird nun in zwei Tupel der Länge formula_1 aufgespalten. Das liefert die vier Zahlen\nDamit ist\nAusmultipliziert ergibt sich\n\nDen Term formula_25 kann man nun in eine andere, hier schneller berechenbare Form bringen:\n\nDamit ergibt sich für das Produkt die Darstellung\n\nin der nur noch die drei „kurzen“ Produkte\n\nerscheinen. Rekursiv berechnet und mit einfachen Verschiebe- und Additionsoperationen verknüpft ergeben sie\n\nVon den vier möglichen Produkten (von X mit Y) X Y, X Y, X Y, X Y wird das erste P und das letzte P direkt im Ergebnis verwendet. Der Term aus der Summe der beiden mittleren Produkten kann als Summe von allen Produkten minus des ersten und letzten Produkt gebildet werden. Die Summe aus allen vier Produkten kann über das neu eingeführte Produkt P mit nur einer Multiplikation erzeugt werden.\n\nEine Multiplikation zweier formula_30-stelliger Zahlen wird zurückgeführt auf drei Multiplikationen von je zwei formula_1-stelligen Zahlen und vier Additionen bzw. Subtraktionen formula_1-stelliger Zahlen eventuell mit Überträgen sowie mit zwei Verschiebungen. Die benötigte Zeit für die Operationen, die keine Multiplikationen sind, ist kleiner als formula_33 mit einer von formula_1 unabhängigen Konstanten formula_35. Bezeichnet formula_36 die Gesamtzahl der Operationen bei der Multiplikation zweier formula_37-stelliger Zahlen, so gilt\n\nDer hier anwendbare erste Fall des Master-Theorems mit formula_39 und formula_40 liefert formula_41 als Laufzeitkomplexität von formula_42\nDie direkte Herleitung mit vollständiger Induktion ermöglicht einen genaueren Einblick:\n\nErsetzen von formula_12 durch formula_1 ergibt dann\n\nDie zu multiplizierenden Zahlen seien\n\nDa es hier nur um die Veranschaulichung der Produktumformung geht, wird mit vorangestellten Nullen auf die nächste gerade und gleiche Länge und nicht auf eine Zweipotenzlänge aufgefüllt. Damit ergeben sich die Zifferntupel\nder Länge formula_51, die in vier Tupel der Länge formula_52 zerlegt werden:\nEs gilt\n\nDie benötigten Produkte sind\n\nDer Algorithmus würde die Produkte formula_63, formula_64 und formula_65 rekursiv bestimmen. Es bleibt das Ergebnis gemäß obiger Formel zusammenzusetzen:\n\nWährend die Schulmethode 110 Ziffernmultiplikationen und 90 Additionen (ohne Überträge) benötigt, sind es hier 92 Multiplikationen und 83 Additionen.\n\nStatt in zwei Teile, können die zu multiplizierenden Zahlen auch in mehr Teile zerlegt werden. Durch geschickte Linearkombination von Teilergebnissen genügen dann bei Zerlegung in formula_67 Teile formula_68 Multiplikationen auf den kleineren Zahlen. Rekursiv angewandt führt dieses Verfahren dann zum Toom-Cook-Algorithmus.\n\n\n"}
{"id": "1059257", "url": "https://de.wikipedia.org/wiki?curid=1059257", "title": "Toom-Cook-Algorithmus", "text": "Toom-Cook-Algorithmus\n\nDer Toom-Cook-Algorithmus ist ein effizienter Algorithmus zur Multiplikation zweier ganzer Zahlen, der nach dem Prinzip \"Teile und herrsche\" arbeitet. Er wurde zuerst von Andrei Toom beschrieben, später durch Cook verbessert und in dessen Doktorarbeit veröffentlicht.\n\nEr existiert in zwei Varianten. Die Variante mit \"fester Teilung\" besitzt eine Laufzeitkomplexität von formula_1, wobei formula_2 eine feste Konstante ist, die nur von der Teilung, aber nicht von der Eingabelänge formula_3 abhängt. Die Variante mit \"variabler Teilung\" besitzt Laufzeitkomplexität formula_4.\n\nDer Algorithmus ist die Verallgemeinerung des Karatsuba-Algorithmus und deutlich schneller als der naive Algorithmus nach der Schulmethode (bzw. der russischen Bauernmultiplikation im Binärsystem), der Laufzeitkomplexität formula_5 besitzt. Für hinreichend große Zahlen ist er aber auch langsamer als der Schönhage-Strassen-Algorithmus, dessen Laufzeitkomplexität formula_6 beträgt und der aus Sicht der Komplexitätstheorie als schnellster, praktisch angewandter, Algorithmus zur Multiplikation ganzer Zahlen gilt.\n\n"}
{"id": "1060401", "url": "https://de.wikipedia.org/wiki?curid=1060401", "title": "Xubuntu", "text": "Xubuntu\n\nXubuntu ist ein von Canonical Ltd. veröffentlichtes Derivat des Betriebssystems Ubuntu und wird von einer freien Community gepflegt. Bei Ubuntu handelt es sich um eine freie und kostenlose Linux-Distribution, die von Debian abstammt. Xubuntu verwendet die Xfce-Desktop-Umgebung, die vor allem für ihre Stabilität und ressourcenschonende Arbeitsweise bekannt ist. Ubuntu hingegen verwendet als Desktopumgebung Gnome.\n\nDer Name der Distribution Ubuntu bedeutet auf Zulu etwa „Menschlichkeit“ und bezeichnet eine afrikanische Philosophie. Xubuntu ist hierbei ein Kofferwort aus \"Xfce\" und \"Ubuntu\".\n\nDie Xubuntu-Gemeinde soll ein Teil der größeren Ubuntu-Community sein, die die Ideale der Ubuntu-Philosophie unterstützt. Die Software sollte kostenlos verfügbar sein; Softwaretools sollen von den Menschen in ihrer Landessprache und trotz aller Behinderungen genutzt werden können. Die Entwickler geben die folgenden Prinzipien ihrer Arbeit an:\n\n\nXubuntu bietet seinem Benutzer bereits vorinstallierte Programme an. Die Auswahl der Standardprogramme unterscheidet sich jedoch im Vergleich zu Ubuntu.\n\n\nXubuntu nutzt ebenfalls das \"Ubuntu Software Center\", über das sich alle Programme aus den von Ubuntu bereitgestellten Repositories über eine grafische Benutzeroberfläche installieren lassen. Canonical versucht somit dem Benutzer ein intuitiv zu nutzendes Werkzeug mitzugeben. Auf zunächst zu erlernende Terminal-Befehle kann somit verzichtet werden.\n\nXubuntu wurde erstmals offiziell am 1. Juni 2006 neben Ubuntu in der Version 6.06 ' veröffentlicht. Die zurzeit aktuelle Version ist 17.04 mit dem Codenamen ', die im April 2017 erschien. Derzeit wird außerdem noch die Version 16.04 LTS ' offiziell mit Aktualisierungen versorgt. „LTS“ bedeutet in diesem Zusammenhang ': Diese Versionen werden drei Jahre anstatt der üblichen neun Monate mit Updates und sicherheitsrelevanten Änderungen versorgt.\n\nXubuntu 12.04 erschien im April 2012 und ist eine LTS-Version, die für insgesamt drei Jahre unterstützt wird. Dies steht im Gegensatz zu den LTS-Versionen von Kubuntu und Ubuntu, die einen fünfjährigen Unterstützungszeitraum erhalten haben.\n\nDie minimalen Systemvoraussetzungen werden mit 512 MiB RAM und 5 GB freiem Festplattenspeicher angegeben.\n\nXubuntu 13.10 erschien im Oktober 2013 und wurde bis Juli 2014 unterstützt. Aufgrund der Größe der Installationsdatei von 1 GB musste das Betriebssystem von einem USB-Stick oder einer DVD installiert werden. Für diese Version wurden Gnumeric und GIMP wieder als vorinstallierte Programme mit aufgenommen, sowie viele Fehler in Catfish und Parole beseitigt. Ebenso wurde das Desktopthema \"Greybird\" neu aufgelegt und neue Hintergrundbilder hinzugefügt. Des Weiteren wurde als Bestandteil der Distribution nun auch eine Dokumentation mitgeliefert.\n\nFür Xubuntu 13.10 gelten die gleichen Systemvoraussetzungen wie für die Version 12.04.\n\nDie im April 2014 erschienene Version 14.04 wird drei Jahre lang unterstützt und gepflegt werden. Zu den Neuerungen zählen die Verwendung von \"\" als Bildschirmsperre sowie des \"Whisker Menu\" als Startmenü. Am 25. Juli 2014 erschien ein Update auf Version 14.04.1. Die Veröffentlichung des zweiten Updates 14.04.2 erfolgte am 19. Februar 2015; dabei wurde der Linux-Kernel von Version 3.13 auf 3.16 aktualisiert. Ein drittes Update wurde am 6. August 2015 freigegeben. Erneut erfolgte eine Anhebung der Kernelversion auf 3.19.\nEin 4. Update wurde am 18. Februar 2016 freigegeben. Erneut erfolgte eine Anhebung der Kernelversion auf 4.2 (wie 15.10). Im August 2016 erfolgt das fünfte und letzte Update des Kernels.\n\n\"\" (englisch für \"Utopisches Einhorn\")\n\n\"\" (englisch für \"hinterlistiger Werwolf\")\n\n\"\" (englisch für \"gastfreundliches Borstenhörnchen\")\n\n\"\" (englisches Wortspiel mit dem Wort Yak, etwa \"Geplapper\")\n\n\"\" (englisch für \"begeisterte Hüpfmaus\")\n\n\"\" (englisch für \"raffiniertes Erdferkel\")\n\n\"Bionic Beaver (\"englisch für \"bionischer Biber)\"\n\n\"Cosmic Cuttlefish\" (englisch für \"kosmischer Tintenfisch\")\n\n\nXubuntu-Versionen werden zweimal im Jahr gleichzeitig mit Ubuntu veröffentlicht. Xubuntu verwendet die gleichen Versionsnummern und Namen, die sich aus dem Veröffentlichungsjahr und Monat zusammensetzen. Beispielsweise wurde Xubuntu 6.06 im Juni 2006 veröffentlicht.\n\nKritiker von Xubuntu bemängeln, dass Versionen ab Xubuntu 8.04 fast den gleichen Speicherbedarf wie ein Ubuntu mit Gnome oder KDE haben. Bei einem von Distrowatch durchgeführten Test wurde festgestellt, dass Xubuntu 9.04 im laufenden Betrieb doppelt so viel Systemspeicher wie Debian 5.0.1 mit XFCE verwendet und beim Laden der Desktopumgebung eine zehnmal höhere Speicherauslastung aufwies. Distrowatch führt dies auf den Einsatz von Applikationen aus der Gnome-Desktopumgebung wie dem grafischen Paketmanager, dem Netzwerkmanager und der Energieverwaltung zurück. Man kam daher zu dem Ergebnis, dass die Standardauswahl der Pakete nicht unbedingt zu einem Low-Memory-System passe. Das \"Linux Magazine\" kam zu einem ähnlichen Ergebnis und zeigte auf, dass Xubuntu etwas mehr Arbeitsspeicher als Ubuntu verwendet, und empfiehlt daher für ältere Systeme die Ubuntu-Variante Lubuntu mit LXDE als Desktop-Umgebung. Für Xubuntu 14.04 LTS und Ubuntu 14.04 LTS werden von den Distributoren die gleichen Systemvoraussetzungen empfohlen. Die Autoren von ubuntuusers.de verweisen auf die Alternative, mit einer Minimalinstallation eine abgespeckte Variante von Xubuntu zu installieren.\n\n\n"}
{"id": "1062943", "url": "https://de.wikipedia.org/wiki?curid=1062943", "title": "Creative Computing", "text": "Creative Computing\n\nCreative Computing war eines der ersten US-amerikanischen Computermagazine, das sich mit dem gerade entwickelnden Homecomputer-Markt beschäftigte. Die Zeitschrift wurde zwischen 1974 und 1985 veröffentlicht.\nDie Zeitschrift beschäftigte sich mit dem Themenspektrum zwischen Homecomputern und Personal Computern. Dabei nutzte sie eine einfachere Form und Sprache als das Konkurrenzprodukt Byte, das wesentlich technischer ausgerichtet war.\n\nDas Magazin wurde gegründet von David H. Ahl, der es in den 1980er Jahren an den Ziff-Davis Verlag verkaufte. In den letzten Jahren der Veröffentlichung fokussierte sich die Zeitschrift auf den professionellen Computermarkt, war aber dabei nicht erfolgreich und stellte dann die Publikation ein.\n\nTed Nelson, bekannt als Erfinder des Hypertext, war kurzzeitig Chefredakteur der Zeitschrift.\n\nBekannt ist auch die Buchreihe BASIC-Computer-Spiele von David H. Ahl, die Programme von Creative Computing enthält und das erste Computerbuch war, das sich mehr als eine Million Mal verkauft hat. Dieses wurde auch in Deutschland verkauft.\n\n"}
{"id": "1063850", "url": "https://de.wikipedia.org/wiki?curid=1063850", "title": "Taxbird", "text": "Taxbird\n\nTaxbird ist ein freier ELSTER-Client, also eine freie Software zur Tätigung der deutschen Umsatzsteuervoranmeldung über das deutsche System zur elektronischen Übermittlung von Steuererklärungen namens ELSTER.\nEs wird von Stefan Siegl in der Programmiersprache \"C\" entwickelt und unter Version 3 oder höher der GNU General Public License (GPL) verbreitet.\nEs ist derzeit die einzige freie Software, die die seit Beginn 2005 vorgeschriebene Umsatzsteuervoranmeldung per Fernmeldeleitung ermöglicht. Die Übermittlung von Jahressteuererklärungen und Einkommensteuererklärungen wird nicht unterstützt, da die hierfür notwendigen Schnittstellen nicht für frei lizenzierte Anwendungen freigegeben sind.\n\nDas Programm ist modular, für die eigentliche Meldung der Steuerdaten wird die Programmbibliothek \"libgeier\" verwendet, \"Taxbird\" selbst ist ein Frontend mit graphischer Benutzeroberfläche (GUI) auf Basis von GTK+ zu der Bibliothek. Die Gestaltung lehnt sich an die proprietäre Freeware ElsterFormular an.\n\nDie Software wurde in die Linux-Distributionen Debian, Ubuntu und Gentoo aufgenommen. Für openSUSE stehen im PackMan-Archiv Pakete bereit.\n2012 wurde die Weiterentwicklung von Taxbird eingestellt. Als Nachfolger entwickelte Siegl die auf HTML und JavaScript basierende Anwendung \"Geierlein\".\n\nAm 26. Oktober 2018 informierte das Bayerische Landesamt für Steuern, dass das Verfahren \"ElsterAnmeldung\" auf der offenen Schnittstelle Mitte Januar 2019 abgeschaltet wird. Dadurch ist die Datenübertragung nur noch über eine proprietäre Schnittstelle möglich, die Möglichkeit freie Software zur Übermittlung der Steueranmeldung zu entwickeln besteht nicht mehr. Damit wurde die Einstellung des Projektes \"Geierlein\" erzwungen.\n\n\n"}
{"id": "1063863", "url": "https://de.wikipedia.org/wiki?curid=1063863", "title": "TaskJuggler", "text": "TaskJuggler\n\nTaskJuggler ist eine Projektmanagementsoftware. Im Gegensatz zu herkömmlichen Projektplanungsprogrammen basiert TaskJuggler auf einer leistungsfähigen Projektbeschreibungssprache, aus der der Projektplan errechnet wird. TaskJuggler unterstützt den Projektmanager bei allen Phasen eines Projektes, von der ersten Idee bis zur Fertigstellung. Es bietet Funktionen zur Zeitplanung, Kostenrechnung, Risikoanalyse, Statusverfolgung und zum Kommunikationsmanagement.\n\nDas Programm verfügt über ein optimierendes Steuerprogramm, das die Berechnung der Zeitintervalle für alle Aufgaben sowie die Ressourcenzuteilung erledigt. Der Ressourcenbalancierer verhindert dabei automatisch eine Überlastung einzelner Ressourcen. Durch die flexible Projektbeschreibungssprache kann der Projektmanager jeweils so viele Details in den Plan aufnehmen, wie gerade bekannt sind. TaskJuggler berechnet dann die restlichen Daten und erlaubt es jederzeit weitere Daten hinzuzufügen, sobald sie bekannt werden.\n\nDas TaskJuggler Project wurde 2001 von Christian Schläger als Open-Source-Projekt gegründet. Zunächst war es ein Kommandozeilenprogramm, das Berichte in HTML erzeugte. Seit August 2004 verfügt es über ein graphisches Frontend auf Basis der KDE-Bibliotheken. In Version 3 wurde dieses graphische Frontend jedoch wieder entfernt. Version 3 ist eine Neuimplementation in Ruby, und damit plattformunabhängig.\n\n"}
{"id": "1064600", "url": "https://de.wikipedia.org/wiki?curid=1064600", "title": "Burrrn", "text": "Burrrn\n\nBurrrn ist ein Brennprogramm, das speziell zum Erstellen von Audio-CDs mit CD-Text dient. Als Quelldateien für die CD können folgende Formate verwendet werden:\n\nWAV, MP3, MPC, Ogg Vorbis, AAC, MP4, Monkey’s Audio, FLAC, OFR, WavPack, True Audio\n\nBurrrn liefert Unterstützung der Playlistformate M3U, PLS, FPL und Cuesheets. Mit Hilfe dieser können die einzelnen Titel eines CD-Images bearbeitet und automatisch auf CD gebrannt werden. Es ist also nicht nötig, die einzelnen Formate vor dem Brennen zu konvertieren. Es werden zudem die Tags aller lesbaren Formate unterstützt. Burrrn nutzt dabei auch die ReplayGain-Einstellungen zur Anpassung der Lautstärke bei der Kompilationen.\n\nAuch wird die unterbrechungsfreie Wiedergabe unterstützt (Gapless Playback), d. h. zwischen den einzelnen Tracks entstehen keine Lücken (Voraussetzung ist natürlich, dass auch das Quellformat dies unterstützt). Auch für das MP3-Format, welches eigentlich nicht dafür konzipiert wurde, ist Unterstützung implementiert, wenn dieses mit LAME encodiert wurde.\n\nBurrrn steht in zahlreichen Sprachen zur Verfügung, unter anderem auch in Deutsch.\n\nZum eigentlichen Brennen benutzt Burrrn Cdrdao.\n\nBurrrn wurde in der c't Ausgabe 4/2009 besprochen und befand sich dort auch in der Beigabe Software-Kollektion 4/2009.\nIn der PCgo wurde die Software bereits 2007 besprochen .\nDie netzwelt stellt ein Tutorial für Burrrn bereit.\n"}
{"id": "1066636", "url": "https://de.wikipedia.org/wiki?curid=1066636", "title": "Epson PX-8", "text": "Epson PX-8\n\nDer Epson PX-8 war ein Laptop-Computer der Firma Epson, der im Jahre 1984 auf den Markt kam. Der Rechner war mit einem kleinen LCD ausgestattet, der in acht Zeilen jeweils 80 Zeichen darstellen konnte.\nDas Betriebssystem des Epson war das damals noch weitläufig verbreitete CP/M (in der Version 2.2). Daneben bot der Rechner im ROM die Programmiersprache BASIC an.\n\nDer Rechner wurde als portabler Business-Computer verkauft, der mit seinen auf jeweils einem ROM abgespeicherten Programmen, wie Textverarbeitung (WordStar), Tabellenkalkulation (Portable Calc) und Terminverwaltung (Portable Scheduler) Geschäftsleuten ein entsprechendes Arbeitsgerät bot.\n\n\n"}
{"id": "1067204", "url": "https://de.wikipedia.org/wiki?curid=1067204", "title": "Iptables", "text": "Iptables\n\niptables ist ein Userspace-Programm zur Konfiguration der Tabellen (\"tables\"), die durch die Firewall im Linux-Kernel (bestehend aus einer Reihe von Netfilter-Modulen) bereitgestellt werden. Diese Tabellen enthalten Ketten (\"chains\") und Regeln (\"rules\"). Verschiedene Programme werden gegenwärtig für unterschiedliche Protokolle verwendet; iptables beschränkt sich auf IPv4, für IPv6 gibt es \"ip6tables\", für ARP ist es \"arptables\", und mit \"ebtables\" gibt es eine Sonderkomponente für Ethernet-Pakete.\n\nDa iptables erweiterte Systemprivilegien benötigt, muss es als root ausgeführt werden. Auf den meisten Linux-Systemen ist iptables als /usr/sbin/iptables installiert. Dokumentation ist in den Manpages mittels man iptables einsehbar, sofern installiert.\n\nDer Begriff \"iptables\" wird auch oft verwendet, um ausschließlich die Kernel-Komponenten zu beschreiben. \"x_tables\" ist der Name des Kernelmoduls, der den gemeinsamen Code aller vier Module (\"v4\", \"v6\", \"arp\" und \"eb\") trägt, und das API für iptables-Erweiterungen bereitstellt. Folglich ist mit \"Xtables\" oft die gesamte Firewall-Infrastruktur gemeint.\n\nNetfilter und iptables wurden ursprünglich zusammen entwickelt, sodass es Überschneidungen in der früheren Entwicklung gab. Siehe dazu den Netfilter-Artikel.\n\nLinux besitzt ab Version 1.0 einen Paketfilter. Dieser stammte zunächst von BSD ab und wurde in der Linux-Version 2.0 unter dem Namen ipfwadm erweitert. Rusty Russell überarbeitete den Paketfilter nochmals und stellte ihn als ipchains zur Verfügung. Er wurde in Linux 2.2 integriert. Gegen 1999 wurde der Kernel und damit auch ipchains komplett überarbeitet. Aus ipchains ging iptables hervor, das seit Kernel 2.4 zum „Lieferumfang“ gehört.\n\niptables behält die ursprüngliche Grundidee von ipfwadm: Listen von Regeln, wovon jede angibt, was in einem Paket überprüft wird und was dann mit diesem Paket geschehen soll. ipchains brachte das Konzept von Ketten (chains) ein, und iptables erweiterte dies hin zu Tabellen (tables). Eine Tabelle ist für NAT zuständig, eine weitere zur Filterung. Zusätzlich wurden die drei Punkte, wo Pakete auf ihrer „Reise“ gefiltert werden, so geändert, dass jedes Paket nur durch einen Filterpunkt gelangt.\n\nDiese Aufteilung ermöglichte iptables wiederum, Informationen zu verwenden, die das Connection-Tracking-Subsystem erarbeitet hatte – diese Information war zuvor an NAT gebunden. Somit hat iptables mehr Möglichkeiten als ipchains, da es zusätzlich den Zustand einer Verbindung überwachen, diese umleiten, oder Datenpakete basierend auf dem Zustand stoppen und manipulieren kann, statt dies nur mittels Quell- oder Zieladresse zu tun. Eine Firewall wie iptables, die diese Voraussetzungen erfüllt, wird als \"stateful\" bezeichnet, während ipchains außer in sehr begrenzten Ausnahmefällen doch nur \"stateless\" war.\n\nDie aktuelle Version 1.6.1 ist am 27. Januar 2017 veröffentlicht worden.\n\nDer Nachfolger von iptables ist nftables, das seit der Linux-Kernelversion 3.13 verfügbar ist.\n\nIptables ermöglicht dem Systemadministrator, Tabellen zu laden, die Ketten von Regeln für die Behandlung von Paketen enthalten. Jede Tabelle dient einem eigenen Zweck. Pakete werden durch sequenzielles Abarbeiten von Regeln innerhalb einer Kette weitergereicht. Eine Regel kann einen Sprung (jump) oder einen Aufruf (goto) in eine andere Kette erwirken, und dies kann mehrfach verschachtelt werden. (Eine Rückkehr (return) kehrt zur nächsten Regel nach dem Sprung zurück.) Jedes Netzwerkpaket, das den Computer erreicht oder diesen verlässt, durchläuft mindestens eine Kette.\n\nDer Ursprung des Pakets bestimmt, in welcher Kette die Abarbeitung beginnt. Es gibt fünf vordefinierte Ketten (die den fünf Netfilter-Hooks entsprechen), auch wenn eine Tabelle nicht unbedingt alle Ketten haben muss. Vordefinierte Ketten haben eine \"Policy\", z. B. DROP, die greift, wenn ein Paket das Ende der Kette erreicht hat (also ohne auf eine Regel gepasst zu haben). Es können weitere benutzerdefinierte Ketten angelegt werden, jedoch haben diese keine Policy; trifft ein Paket auf deren Ende, geht die Abarbeitung in der Kette weiter, die ursprünglich den Sprung ausgelöst hat. Leere Ketten sind zulässig.\n\n\nJede Regel in einer Kette enthält Spezifikationen (\"matches\"), auf welche Pakete sie zutrifft. Regeln können außerdem ein Ziel (\"target\", für Erweiterungen) bzw. Urteil (\"verdict\") enthalten. Mit dem Durchlaufen eines Paketes durch eine Kette werden Regeln nacheinander geprüft. Falls eine Regel auf das Paket nicht zutrifft, wird zur nächsten Regel übergegangen. Trifft sie hingegen zu, wird die mit Ziel/Urteil gelistete Aktion durchgeführt, welche darin resultieren kann, dass das Paket weiter durch die Kette läuft oder nicht. Spezifikationen stellen den größten Teil von Regelwerken dar, da sie die Bedingungen enthalten, auf die ein Paket getestet wird. Diese Tests können für jeden Layer im OSI-Model durchgeführt werden, zu nennen sind die --mac-source und -p tcp --dport Parameter. Jedoch gibt es auch protokollunabhängige Optionen, z. B. codice_1.\n\nEin Paket avanciert in einer Kette, bis entweder eine Regel auf das Paket zutrifft und ein endgültiges Urteil für das Paket gefällt wird (z. B. mittels ACCEPT oder DROP) oder bis eine Regel als Urteil RETURN enthält (wodurch es in der übergeordneten Kette wieder weitergeht) oder bis das Ende der Kette erreicht wird.\n\nZur Erleichterung beim Aufsetzen von Regeln wird Software von vielen Herstellern angeboten. Frontends in textbasierter oder grafischer Manier erlauben es Benutzern, einfache Regelwerke mit wenigen Mausklicks anzulegen; Skripte sind oftmals Shell-Skripte (aber auch andere Sprachen sind möglich), die iptables oder (das schnellere) iptables-restore mit einer Reihe von vordefinierten Regeln aufrufen. Dabei können auch Vorlagen zum Einsatz kommen, die mittels Konfigurationsdateien angelegt werden. Linux-Distributionen verwenden oft Vorlagen, bieten dem Anwender aber auch die Möglichkeit, eigene Regeln zu definieren.\n\nBeispiele:\n\nSolche Frontends, Generatoren und Skripte sind oft durch ihre Vorlagen und Bauweise beschränkt. Hinzu kommt, dass die so generierten Regelwerke meist nicht für den jeweiligen Einsatz der Firewall optimiert sind, da eine automatische Optimierung im Frontend einen hohen Entwicklungsaufwand darstellen würde. Benutzern, die ein gutes Verständnis von iptables haben und ein optimiertes Regelwerk wünschen, wird daher angeraten, die Regeln selbst zu konstruieren.\n\n\n\n"}
{"id": "1070459", "url": "https://de.wikipedia.org/wiki?curid=1070459", "title": "Directory Opus", "text": "Directory Opus\n\nDirectory Opus ist ein frei konfigurierbarer Dateimanager für Windows und AmigaOS, der gegenüber dem vom Betriebssystem zur Verfügung gestellten Dateimanager einen stark erweiterten Funktionsumfang bietet.\n\nDie erste, von Jonathan Potter entwickelte Version von \"Directory Opus\" wurde 1990 für AmigaOS veröffentlicht. Dank des großen Funktionsumfangs, der guten Integration ins Betriebssystem und der extremen Konfigurierbarkeit entwickelte sich das Programm sehr schnell zu einem der beliebtesten Dateimanager unter AmigaOS. Diese erste, bis zur Version 4 (1994) weiterentwickelte Inkarnation von \"Directory Opus\" war mit ihren zwei Verzeichnisfenstern und der darunterliegenden Knopf-Reihe noch stark vom Norton Commander inspiriert. Sie wurde 2000 unter der GNU General Public License freigegeben und wird seitdem von verschiedenen Programmierern weiterentwickelt.\n\nMitte der neunziger Jahre kam mit einem Wechsel des Distributors auch ein zusätzlicher Programmierer an Bord: Dr. Greg Perry. Man entschied sich, das bisherige Konzept aufzugeben und das Programm zu einem vollwertigen Desktop-Ersatz auszubauen. Auch der vollständig überarbeitete \"Directory Opus 5\" wurde ein Erfolg und bis 2000 aktiv weiterentwickelt bzw. unterstützt. Die Rechte der Amiga-Version wurden 2004 an den schwedischen Distributor GuruMeditation verkauft, der jetzt an einer Anpassung von \"Directory Opus\" an die Version 4 von AmigaOS arbeitet.\n\nIm Juni 2001 erschien mit \"Directory Opus 6\" die dritte Inkarnation des Dateimanagers. Das Programm wurde von Jonathan Potter und Greg Perry auf Windows portiert und nochmals umfangreich erweitert. Die Windows-Version wird stetig weiterentwickelt, aktuell ist Version 12 (September 2016).\n\nAlle Versionen von \"Directory Opus\" sind umfangreich konfigurierbar. Für praktisch jede erdenkliche Operation, die der Anwender mit einer Datei durchführen kann, lässt sich die gewünschte Reaktion des Programms festlegen – und zwar für jeden Dateityp einzeln. Auch die Menüs und die Belegung der Toolbars können vom Anwender neu definiert werden.\n\nAb Version 5 lässt sich auch die grafische Benutzeroberfläche den Wünschen des Benutzers anpassen – das beinhaltet die Anzahl der Verzeichnisfenster, die Position und Bedeutung der einzelnen Elemente der Benutzeroberfläche oder auch die in Toolbars verwendeten Grafiken.\n\nWeitere Funktionen von \"Directory Opus\" sind der transparent integrierte FTP-Client, die verschiedenen Anzeige-Modi für ein Verzeichnis (Windows-Version: Verzeichnisbaum, Dokumenten-Voransicht, Liste, …), konfigurierbare Ordner-Register (siehe auch Tabbed Browsing und Registerkarte – nur Windows-Version), die direkte Unterstützung verschiedener Archiv-Formate, Datei-Filter, Funktionen zum Umbenennen zahlreicher Dateien, Suchfunktionen und die Möglichkeit auf Wunsch den Dateimanager des Betriebssystems komplett zu ersetzen.\n\n"}
{"id": "1071787", "url": "https://de.wikipedia.org/wiki?curid=1071787", "title": "Geschichte des Computers", "text": "Geschichte des Computers\n\nDie Geschichte des Computers reicht zurück bis in die Antike.\n\nDie Computertechnologie entwickelte sich im Vergleich zu anderen Elektrogeräten sehr schnell. Die Geschichte der Entwicklung des Computers reicht zurück bis in die Antike und ist damit wesentlich länger als die Geschichte der modernen Computertechnologien und mechanischen bzw. elektrischen Hilfsmitteln (Rechenmaschinen oder Hardware). Sie umfasst dabei auch die Entwicklung von Rechenmethoden, die etwa für einfache Schreibgeräte auf Papier und Tafeln entwickelt wurden. Im Folgenden wird entsprechend versucht, einen Überblick über diese Entwicklungen zu geben.\n\nDas Konzept der Zahlen lässt sich auf keine konkreten Wurzeln zurückführen und hat sich wahrscheinlich mit den ersten Notwendigkeiten der Kommunikation zwischen zwei Individuen entwickelt. Man findet in allen bekannten Sprachen mindestens für die Zahlen eins und zwei Entsprechungen.\n\nAls Weiterentwicklung ist der Übergang von der reinen Anzahlbenennung zum Gebrauch mathematischer Rechenoperationen wie Addition, Subtraktion, Multiplikation und Division anzusehen; auch Quadratzahlen und Quadratwurzel sind hierunter zu fassen. Diese Operationen wurden formalisiert (in Formeln dargestellt) und dadurch überprüfbar. Daraus entwickelten sich dann weiterführende Betrachtungen, etwa die von Euklid entwickelte Darstellung des größten gemeinsamen Teilers.\n\nIm Mittelalter erreichte das Indische Zahlensystem über den arabischen Raum (deswegen fälschlicherweise als Arabisches Zahlensystem bekannt) Europa und erlaubte eine größere Systematisierung bei der Arbeit mit Zahlen. Es erlaubte die Darstellung von Zahlen, Ausdrücken und Formeln auf Papier und die Tabellierung von mathematischen Funktionen wie der Quadratwurzel, des einfachen Logarithmus und trigonometrischer Funktionen. Zur Zeit der Arbeiten von Isaac Newton war Papier und Velin eine bedeutende Ressource für Rechenaufgaben und ist dies bis in die heutige Zeit geblieben, in der Forscher wie Enrico Fermi seitenweise Papier mit mathematischen Berechnungen füllten und Richard Feynman jeden mathematischen Schritt mit der Hand bis zur Lösung berechnete, obwohl es zu seiner Zeit bereits programmierbare Rechner gab.\n\nDas früheste Gerät, das in rudimentären Ansätzen mit einem heutigen Computer vergleichbar ist, ist der Abakus, eine mechanische Rechenhilfe, die vermutlich um 1100 v. Chr. im indochinesischen Kulturraum erfunden wurde. Der Abakus wurde bis ins 17. Jahrhundert benutzt und dann durch die ersten Rechenmaschinen ersetzt. In einigen Regionen der Welt wird der Abakus noch immer als Rechenhilfe verwendet. Einem ähnlichen Zweck diente auch das Rechenbrett des Pythagoras.\nBereits im 1. Jahrhundert v. Chr. wurde mit dem Räderwerk von Antikythera die erste Rechenmaschine erfunden. Das Gerät diente vermutlich für astronomische Berechnungen und funktionierte mit einem Differentialgetriebe, einer erst im 13. Jahrhundert wiederentdeckten Technik.\n\nMit dem Untergang der Antike kam der technische Fortschritt in Mittel- und Westeuropa fast zum Stillstand und in den Zeiten der Völkerwanderung ging viel Wissen verloren oder wurde nur noch im oströmischen Reichsteil bewahrt (so beispielsweise auch das Räderwerk von Antikythera, das erst 1902 wiederentdeckt wurde). Die muslimischen Eroberer der oströmischen Provinzen und schließlich Ost-Roms (Konstantinopel) nutzten dieses Wissen und entwickelten es weiter. Durch die Kreuzzüge und spätere Handelskontakte zwischen Abend- und Morgenland sowie die muslimische Herrschaft auf der iberischen Halbinsel, sickerte antikes Wissen und die darauf aufbauenden arabischen Erkenntnisse langsam wieder nach West- und Mitteleuropa ein. Ab der Neuzeit begann sich der Motor des technischen Fortschritts wieder langsam zu drehen und beschleunigte fortan – und dies tut er bis heute.\n1614 publizierte John Napier seine Logarithmentafel. Mitentdecker der Logarithmen ist Jost Bürgi. 1623 baute Wilhelm Schickard die erste Vier-Spezies-Maschine mit getrennten Werken für Addition/Subtraktion und Multiplikation/Division und damit den ersten mechanischen Rechner, wodurch er zum „Vater der Computerära“ wurde. Seine Konstruktion basierte auf dem Zusammenspiel von Zahnrädern, die im Wesentlichen aus dem Bereich der Uhrmacherkunst stammten und dort genutzt wurden, wodurch seine Maschine den Namen „Rechenuhr“ erhielt. Ein weiteres Exemplar war für Johannes Keplers astronomische Berechnungen bestimmt, verbrannte aber halbfertig. Schickards eigenes Gerät ist verschollen.\n\n1642 folgte Blaise Pascal mit seiner Zweispezies-Rechenmaschine, der Pascaline. 1668 entwickelte Samuel Morland eine Rechenmaschine, die erstmals nicht dezimal addierte, sondern auf das englische Geldsystem abgestimmt war. 1673 baute Gottfried Wilhelm Leibniz seine erste Vierspezies-Maschine und erfand 1703 (erneut) das binäre Zahlensystem (Dualsystem), das später die Grundlage für die Digitalrechner und darauf aufbauend die digitale Revolution wurde.\n1805 nutzte Joseph-Marie Jacquard Lochkarten, um Webstühle zu steuern. 1820 baute Charles Xavier Thomas de Colmar das „Arithmometer“, den ersten Rechner, der in Massenproduktion hergestellt wurde und somit den Computer für Großunternehmen erschwinglich machte. Charles Babbage entwickelte von 1820 bis 1822 die Differenzmaschine (englisch \"Difference Engine\") und 1837 die Analytical Engine, konnte sie aber aus Geldmangel und wegen damals noch zu wenig ausgereifter Feinmechanik nicht bauen. 1843 bauten Edvard und George Scheutz in Stockholm den ersten mechanischen Computer nach den Ideen von Babbage. Im gleichen Jahr entwickelte Ada Lovelace eine Methode zur Programmierung von Computern nach dem Babbage-System und schrieb damit das erste Computerprogramm. 1890 wurde die US-Volkszählung mit Hilfe des Lochkartensystems von Herman Hollerith durchgeführt. 1912 baute Torres y Quevedo eine Schach­maschine, die mit König und Turm einen König matt setzen konnte,– und somit den ersten Spielcomputer.\n\nMechanische Rechner wie die darauf folgenden Addierer, der Comptometer, der Monroe-Kalkulator, die Curta und der Addo-X wurden bis in die 1970er Jahre genutzt. Anders als Leibniz nutzten die meisten Rechner das Dezimalsystem, das technisch schwieriger umzusetzen war. Dies galt sowohl für die Rechner von Charles Babbage um 1800 wie auch für den ENIAC von 1945, den ersten \"vollelektronischen\" Universalrechner überhaupt.\n\nEs wurden jedoch auch nichtmechanische Rechner gebaut, wie der Wasserintegrator.\n\n1935 stellten IBM die \"IBM 601\" vor, eine Lochkartenmaschine, die eine Multiplikation pro Sekunde durchführen konnte. Es wurden ca. 1500 Exemplare verkauft. 1937 meldete Konrad Zuse zwei Patente an, die bereits alle Elemente der so genannten Von-Neumann-Architektur beschreiben. Im selben Jahr baute John Atanasoff zusammen mit dem Doktoranden Clifford Berry einen der ersten Digitalrechner, den Atanasoff-Berry-Computer, und Alan Turing publizierte einen Artikel, der die Turing-Maschine, ein abstraktes Modell zur Definition des Algorithmusbegriffs, beschreibt.\n\n1938 stellte Konrad Zuse die Zuse Z1 fertig, einen frei programmierbaren mechanischen Rechner, der allerdings aufgrund von Problemen mit der Fertigungspräzision nie voll funktionstüchtig war. Die Z1 verfügte bereits über Gleitkommarechnung. Sie wurde im Krieg zerstört und später nach Originalplänen neu gefertigt, die Teile wurden auf modernen Fräs- und Drehbänken hergestellt. Dieser Nachbau der Z1, welcher im Deutschen Technikmuseum in Berlin steht, ist mechanisch voll funktionsfähig und hat eine Rechengeschwindigkeit von 1 Hz, vollzieht also eine Rechenoperation pro Sekunde. Ebenfalls 1938 publizierte Claude Shannon einen Artikel darüber, wie man symbolische Logik mit Relais implementieren kann. (Lit.: Shannon 1938)\n\nWährend des Zweiten Weltkrieges gab Alan Turing die entscheidenden Hinweise zur Entzifferung der Enigma-Codes und baute dafür einen speziellen mechanischen Rechner, Turing-Bombe genannt.\n\nEbenfalls im Krieg (1941) baute Konrad Zuse die erste funktionstüchtige programmgesteuerte binäre Rechenmaschine, bestehend aus einer großen Zahl von Relais, die Zuse Z3. Wie 1998 bewiesen werden konnte, war die Z3 turingmächtig und damit außerdem die erste Maschine, die – im Rahmen des verfügbaren Speicherplatzes – beliebige Algorithmen automatisch ausführen konnte. Aufgrund dieser Eigenschaften wird sie oft als erster funktionsfähiger Computer der Geschichte betrachtet. Die nächsten Digitalrechner waren der in den USA gebaute Atanasoff-Berry-Computer (Inbetriebnahme 1941) und die britische Colossus (1941). Sie dienten speziellen Aufgaben und waren nicht turingmächtig. Auch Maschinen auf analoger Basis wurden entwickelt.\nAuf das Jahr 1943 wird auch die angeblich von IBM-Chef Thomas J. Watson stammende Aussage „Ich glaube, es gibt einen weltweiten Bedarf an vielleicht fünf Computern.“ datiert. Im selben Jahr stellte Tommy Flowers mit seinem Team in Bletchley Park den ersten „Colossus“ fertig. 1944 erfolgte die Fertigstellung des ASCC (Automatic Sequence Controlled Computer, „Mark I“ durch Howard H. Aiken) und das Team um Reinold Weber stellte eine Entschlüsselungsmaschine für das Verschlüsselungsgerät M-209 der US-Streitkräfte fertig. Zuse hatte schließlich bis März 1945 seine am 21. Dezember 1943 bei einem Bombenangriff zerstörte Z3 durch die deutlich verbesserte Zuse Z4 ersetzt, den damals einzigen turingmächtigen Computer in Europa, der von 1950 bis 1955 als zentraler Rechner der ETH Zürich genutzt wurde.\n\nDas Ende des Zweiten Weltkriegs erlaubte es, dass Europäer und Amerikaner von ihren Fortschritten gegenseitig wieder Kenntnis erlangten. 1946 wurde der Electronical Numerical Integrator and Computer (ENIAC) unter der Leitung von John Eckert und John Mauchly entwickelt. ENIAC ist der erste vollelektronische digitale Universalrechner (Konrad Zuses Z3 verwendete 1941 noch Relais, war also nicht vollelektronisch). 1947 baute IBM den Selective Sequence Electronic Calculator (SSEC), einen Hybridcomputer mit Röhren und mechanischen Relais und die Association for Computing Machinery (ACM) wurde als erste wissenschaftliche Gesellschaft für Informatik gegründet. Im gleichen Jahr wurde auch der erste Transistor realisiert, der heute aus der modernen Technik nicht mehr weggedacht werden kann. Die maßgeblich an der Erfindung beteiligten William B. Shockley, John Bardeen und Walter Brattain erhielten 1956 den Nobelpreis für Physik. In die späten 1940er Jahre fällt auch der Bau des Electronic Discrete Variable Automatic Computer (EDVAC), der erstmals die Von-Neumann-Architektur implementierte.\n\n1949 stellte Edmund C. Berkeley, Begründer der ACM, mit „Simon“ den ersten digitalen, programmierbaren Computer für den Heimgebrauch vor. Er bestand aus 50 Relais und wurde in Gestalt von Bauplänen vertrieben, von denen in den ersten zehn Jahren ihrer Verfügbarkeit über 400 Exemplare verkauft wurden. Im selben Jahr stellte Maurice Wilkes mit seinem Team in Cambridge den Electronic Delay Storage Automatic Calculator (EDSAC) vor; basierend auf John von Neumanns EDVAC ist es der erste Rechner, der vollständig speicherprogrammierbar war. Ebenfalls 1949 besichtigte der Schweizer Mathematikprofessor Eduard Stiefel die in einem Pferdestall in Hopferau aufgestellte Zuse Z4 und finanzierte die gründliche Überholung der Maschine durch die Zuse KG, bevor sie an die ETH Zürich ausgeliefert wurde und dort in Betrieb ging.\n\nIn den 1950er Jahren setzte die Produktion kommerzieller (Serien-)Computer ein. Unter der Leitung von Prof. Alwin Walther wurde am Institut für Praktische Mathematik (IPM) der TH Darmstadt ab 1951 der DERA (Darmstädter Elektronischer Rechenautomat) erbaut. Remington Rand baute 1951 ihren ersten kommerziellen Röhrenrechner, den UNIVersal Automatic Computer I (UNIVAC I) und 1955 Bell Labs für die US Air Force mit dem TRansistorized Airborne DIgital Computer (TRADIC) den ersten Computer, der komplett mit Transistoren statt Röhren bestückt war; im gleichen Jahr begann Heinz Zemanek mit der Konstruktion des ersten auf europäischem Festland gebauten Transistorrechners, des Mailüfterls, das er 1958 der Öffentlichkeit vorstellte. Ebenfalls 1955 baute die DDR mit der „OPtik-REchen-MAschine“ (OPREMA) ihren ersten Computer. 1956 nahm die ETH Zürich ihre ERMETH in Betrieb und IBM fertigte das erste Magnetplattensystem (Random Access Method of Accounting and Control (RAMAC)). Ab 1958 wurde die Electrologica X1 als volltransistorisierter Serienrechner gebaut. Noch im selben Jahr stellte die Polnische Akademie der Wissenschaften in Zusammenarbeit mit dem Laboratorium für mathematische Apparate unter der Leitung von Romuald Marczynski den ersten polnischen Digital Computer „XYZ“ vor. Vorgesehenes Einsatzgebiet war die Nuklearforschung. 1959 begann Siemens mit der Auslieferung des Siemens 2002, ihres ersten in Serie gefertigten und vollständig auf Basis von Transistoren hergestellten Computers.\n\n1960 baute IBM den IBM 1401, einen transistorisierten Rechner mit Magnetbandsystem, und DECs (Digital Equipment Corporation) erster Minicomputer, die PDP-1 (Programmierbarer Datenprozessor) erscheint. 1962 lieferte die Telefunken AG die ersten TR 4 aus. 1964 baute DEC den Minicomputer PDP-8 für unter 20.000 Dollar.\n\n1964 definierte IBM die erste Computerarchitektur S/360, womit Rechner verschiedener Leistungsklassen denselben Code ausführen konnten und bei Texas Instruments wurde der erste „integrierte Schaltkreis“ (IC) entwickelt. 1965 stellte das Moskauer Institut für Präzisionsmechanik und Computertechnologie unter der Leitung seines Chefentwicklers Sergej Lebedjew mit dem BESM-6 den ersten exportfähigen Großcomputer der UdSSR vor. BESM-6 wurde ab 1967 mit Betriebssystem und Compiler ausgeliefert und bis 1987 gebaut. 1966 erschien dann auch noch mit D4a ein 33bit Auftischrechner der TU Dresden.\nDer erste frei programmierbare Tischrechner der Welt, der „Programma 101“ von der Firma Olivetti, erschien 1965 für einen Preis von $3,200 (was auf das Jahr 2017 bezogen einer Summe von $24,746 entspricht). \n1968 bewarb Hewlett-Packard (HP) den HP-9100A in der Science-Ausgabe vom 4. Oktober 1968 als „personal computer“, obgleich diese Bezeichnung nichts mit dem zu tun hat, was seit Mitte der 1970er Jahre bis heute unter einem Personal Computer verstanden wird. Die 1968 entstandene Nixdorf Computer AG erschloss zunächst in Deutschland und Europa, später auch in Nordamerika, einen neuen Computermarkt: die Mittlere Datentechnik bzw. die dezentrale elektronische Datenverarbeitung. Massenhersteller wie IBM setzten weiterhin auf Großrechner und zentralisierte Datenverarbeitung, wobei Großrechner für kleine und mittlere Unternehmen schlicht zu teuer waren und die Großhersteller den Markt der Mittleren Datentechnik nicht bedienen konnten. Nixdorf stieß in diese Marktnische mit dem modular aufgebauten Nixdorf 820 vor, brachte dadurch den Computer direkt an den Arbeitsplatz und ermöglichte kleinen und mittleren Betrieben die Nutzung der elektronischen Datenverarbeitung zu einem erschwinglichen Preis. Im Dezember 1968 stellten Douglas C. Engelbart und William English vom Stanford Research Institute (SRI) die erste Computermaus vor, mangels sinnvoller Einsatzmöglichkeit (es gab noch keine grafischen Benutzeroberflächen) interessierte dies jedoch kaum jemanden. 1969 werden die ersten Computer per Internet verbunden.\n\nMit der Erfindung des serienmäßig produzierbaren Mikroprozessors wurden die Computer immer kleiner, leistungsfähiger und preisgünstiger. Doch noch wurde das Potential der Computer verkannt. So sagte noch 1977 Ken Olson, Präsident und Gründer von DEC: „Es gibt keinen Grund, warum jemand einen Computer zu Hause haben wollte.“\n1971 war es Intel, die mit dem 4004 den ersten in Serie gefertigten Mikroprozessor baute. Er bestand aus 2250 Transistoren. 1971 lieferte Telefunken den TR 440 an das Deutsche Rechenzentrum Darmstadt sowie an die Universitäten Bochum und München. 1972 ging der Illiac IV, ein Supercomputer mit Array-Prozessoren, in Betrieb. 1973 erschien mit Xerox Alto der erste Computer mit Maus, graphischer Benutzeroberfläche (GUI) und eingebauter Ethernet-Karte; und die französische Firma R2E begann mit der Auslieferung des Micral. 1974 stellte HP mit dem HP-65 den ersten programmierbaren Taschenrechner vor und Motorola baute den 6800-Prozessor, währenddessen Intel den 8080-Prozessor fertigte. 1975 begann MITS mit der Auslieferung des Altair 8800. 1975 stellte IBM mit der IBM 5100 den ersten tragbaren Computer vor. Eine Wortlänge von 8 Bit und die Einengung der (schon existierenden) Bezeichnung Byte auf dieses Maß wurden in dieser Zeit geläufig.\n\n1975 Maestro I (ursprünglich Programm-Entwicklungs-Terminal-System PET) von Softlab war weltweit die erste Integrierte Entwicklungsumgebung für Software. Maestro I wurde weltweit 22.000 Mal installiert, davon 6.000 Mal in der Bundesrepublik Deutschland. Maestro I war in den 1970er und 1980er Jahren führend auf diesem Gebiet.\n\n1976 entwickelte Zilog den Z80-Prozessor und Apple Computer stellte den Apple I vor, den weltweit ersten Personal Computer, gefolgt 1977 vom Commodore PET und dem Tandy TRS 80. Der ebenfalls im Jahr 1977 veröffentlichte Apple II gilt bislang als letzter in Serie hergestellter Computer, der von einer einzelnen Person, Steve Wozniak, entworfen wurde. 1978 erschien die VAX-11/780 von DEC, eine Maschine speziell für virtuelle Speicheradressierung. Im gleichen Jahr stellte Intel den 8086 vor, ein 16-Bit-Mikroprozessor; er ist der Urvater der noch heute gebräuchlichen x86-Prozessor-Familie. 1979 schließlich startete Atari den Verkauf seiner Rechnermodelle 400 und 800. Revolutionär war bei diesen, dass mehrere ASIC-Chips den Hauptprozessor entlasteten.\n\nDie 1980er waren die Blütezeit der Heimcomputer, zunächst mit 8-Bit-Mikroprozessoren und einem Arbeitsspeicher bis 64 KiB (Commodore VC20, C64, Sinclair ZX80/81, Sinclair ZX Spectrum, Schneider/Amstrad CPC 464/664, Atari XL/XE-Reihe), später auch leistungsfähigere Modelle mit 16-Bit- (Texas Instruments TI-99/4A) oder 16/32-Bit-Mikroprozessoren (z. B. Amiga, Atari ST).\n\nDas Unternehmen IBM stellte 1981 den IBM-PC vor, legte die Grundkonstruktion offen und schuf einen informellen Industriestandard; sie definierten damit die bis heute aktuelle Geräteklasse der \"„IBM-PC-kompatiblen Computer“\". Dank zahlreicher preiswerter Nachbauten und Forführungen wurde diese Geräteklasse zu einer der erfolgreichsten Plattformen für den Personal Computer; die heute marktüblichen PCs mit Windows-Betriebssystem und x86-Prozessoren beruhen auf der stetigen Weiterentwicklung des damaligen Entwurfs von IBM.\n\n1982 brachte Intel den 80286-Prozessor auf den Markt und Sun Microsystems entwickelte die Sun-1 Workstation. Nach dem ersten Büro-Computer mit Maus, Lisa, der 1983 auf den Markt kam, wurde 1984 der Apple Macintosh gebaut und setzte neue Maßstäbe für Benutzerfreundlichkeit. Die Sowjetunion konterte mit ihrem „Kronos 1“, einer Bastelarbeit des Rechenzentrums in Akademgorodok. Im Januar 1985 stellte Atari den ST-Computer auf der Consumer Electronics Show (CES) in Las Vegas vor. Im Juli produzierte Commodore den ersten Amiga-Heimcomputer. In Sibirien wurde der „Kronos 2“ vorgestellt, der dann als „Kronos 2.6“ für vier Jahre in Serie ging. 1986 brachte Intel den 80386-Prozessor auf den Markt, 1989 den 80486. Ebenfalls 1986 präsentierte Motorola den 68030-Prozessor. Im gleichen Jahr stellte Acorn den ARM2-Prozessor fertig und setze ihn im Folgejahr in Acorn-Archimedes-Rechnern ein. 1988 stellte NeXT mit Steve Jobs, Mitgründer von Apple, den gleichnamigen Computer vor.\n\nDie Computer-Fernvernetzung, deutsch „DFÜ“ (Datenfernübertragung), über das Usenet wurde an Universitäten und in diversen Firmen immer stärker benutzt. Auch Privatleute strebten nun eine Vernetzung ihrer Computer an; Mitte der 1980er Jahre entstanden Mailboxnetze, zusätzlich zum FidoNet das Z-Netz und das MausNet.\n\nDie 1990er sind das Jahrzehnt des Internets und des World Wide Web. (\"Siehe auch Geschichte des Internets, Chronologie des Internets\") 1991 spezifizierte das AIM-Konsortium (Apple, IBM, Motorola) die PowerPC-Plattform. 1992 stellte DEC die ersten Systeme mit dem 64-Bit-Alpha-Prozessor vor. 1993 brachte Intel den Pentium-Prozessor auf den Markt, 1995 den Pentium Pro. 1994 stellte Leonard Adleman mit dem TT-100 den ersten Prototyp eines DNA-Computers vor, im Jahr darauf Be Incorporated die BeBox. 1999 baute Intel den Supercomputer ASCI Red mit 9.472 Prozessoren und AMD stellte mit dem Athlon den Nachfolger der K6-Prozessorfamilie vor.\n\nZu Beginn des 21. Jahrhunderts sind Computer sowohl in beruflichen wie privaten Bereichen allgegenwärtig und allgemein akzeptiert. Während die Leistungsfähigkeit in klassischen Anwendungsbereichen weiter gesteigert wird, werden digitale Rechner unter anderem in die Telekommunikation und Bildbearbeitung integriert. 2001 baute IBM den Supercomputer ASCI White, und 2002 ging der NEC Earth Simulator in Betrieb. 2003 lieferte Apple den PowerMac G5 aus, den ersten Computer mit 64-Bit-Prozessoren für den Massenmarkt. AMD zog mit dem Opteron und dem Athlon 64 nach.\n\n2005 produzierten AMD und Intel erste Dual-Core-Prozessoren, 2006 doppelte Intel mit den ersten Core-2-Quad-Prozessoren nach – AMD konnte erst 2007 erste Vierkernprozessoren vorstellen. Bis zum Jahr 2010 stellten mehrere Firmen auch Sechs- und Achtkernprozessoren vor. Entwicklungen wie Mehrkernprozessoren, Berechnung auf Grafikprozessoren (GPGPU) sowie der breite Einsatz von Tablet-Computern dominieren in den letzten Jahren (Stand 2012) das Geschehen.\n\nSeit den 1980er Jahren stiegen die Taktfrequenzen von anfangs wenigen MHz bis zuletzt (Stand 2015) etwa 4 GHz. In den letzten Jahren konnte der Takt nur noch wenig gesteigert werden, stattdessen wurden Steigerungen der Rechenleistung eher durch mehr Prozessorkerne und vergrößerte Busbreiten erzielt. Auch wenn durch Übertaktung einzelne Prozessoren auf über 8 GHz betrieben werden konnten, sind diese Taktraten auch 2015 noch nicht in Serienprozessoren verfügbar. Außerdem werden zunehmend auch die in Computern verbauten Grafikprozessoren zur Erhöhung der Rechenleistung für spezielle Aufgaben genutzt (z. B. per OpenCL, siehe auch Streamprozessor und GPGPU).\n\nSeit ca. 2005 spielen auch Umweltaspekte (wie z. B. Stromsparfunktionen von Prozessor und Chipsatz, verringerter Einsatz schädlicher Stoffe) – bei der Produktion, Beschaffung und Nutzung von Computern zunehmend eine Rolle (siehe auch Green IT).\n\n\n"}
{"id": "1073630", "url": "https://de.wikipedia.org/wiki?curid=1073630", "title": "EAccelerator", "text": "EAccelerator\n\neAccelerator ist eine freie Software zum Einsatz auf Webservern, die als Beschleuniger, Optimierer und Cache für PHP-Seiten dient. Es ist ein Abspaltung von \"TurckMMCache\", der ursprünglich von Dmitry Stogov entwickelt wurde.\n\nDie Beschleunigung des PHP-Codes wird erreicht, indem der PHP-Code im interpretierten Zustand gespeichert wird und somit das wiederholte Interpretieren des PHP-Codes bei jedem Aufruf fast vollständig entfällt. Hierzu speichert eAccelerator den interpretierten PHP-Code im Shared Memory und führt bei einem erneuten Aufruf direkt den interpretierten PHP-Code aus dem Hauptspeicher aus. Zusätzlich werden Optimierungen zur Laufzeit vorgenommen, um die Ausführung des PHP-Codes zu beschleunigen.\n\nIm Mittel reduziert der Einsatz von eAccelerator die Serverlast deutlich und erhöht die Geschwindigkeit bei der Ausführung des PHP-Codes um das ein- bis zehnfache.\n\neAccelerator wurde erfolgreich getestet mit PHP 4.x und PHP 5.0.x; die Unterstützung für PHP 5.1 und 5.2 wurde in die Version 0.9.5.1 vollständig integriert, welche somit nicht mehr mit PHP 4.x unter Windows zusammenarbeitet. Ab Version 0.9.6 wird auch PHP 5.3 unterstützt. Folgende Plattformen und Webserver funktionieren mit eAccelerator: GNU/Linux, FreeBSD, macOS, Solaris und Windows mit dem Apache HTTP Server 1.3, 2.0 und 2.2, lighttpd (über FastCGI) und IIS. Eine Einbindung über CGI wird weder unterstützt noch ist dies von den Entwicklern geplant; eine FastCGI-Unterstützung ist jedoch vorhanden.\n\nSeit Januar 2010 hat es keine aktuelle Version von eAccelerator mehr gegeben. Die letzte unterstützte PHP-Version war 5.3, die bereits 2014 eingestellt wurde.\n\n\n"}
{"id": "1074534", "url": "https://de.wikipedia.org/wiki?curid=1074534", "title": "Windows-Firewall", "text": "Windows-Firewall\n\nDie Windows-Firewall ist eine Personal Firewall von Microsoft, die Bestandteil von Windows (ab Windows XP SP2) sowie der Windows-Server-Betriebssysteme (ab Windows Server 2003 SP1) ist.\n\nDie Windows-Firewall verwirft – wie bei derartigen Anwendungen üblich – unaufgefordert eingehende Netzwerk-Verbindungen und fragt beim Start anderer Anwendungen, die Server-Dienste anbieten, bei Benutzern, die über Administrator-Rechte verfügen, nach, ob eingehende Verbindungen zu den von diesen Anwendungen geöffneten Ports erlaubt werden sollen. Sie kann über das Sicherheitscenter – der ebenfalls mit Service Pack 2 hinzugekommenen zentralen Verwaltungsstelle für sogenannte \"Personal Firewalls\" und Virenscanner – oder (in Windows XP, unter codice_1) über eine Datei im INI-Format Namens codice_2 konfiguriert werden. Dort können in zwei Profilen Ausnahmelisten für bestimmte Ports und Anwendungen erstellt werden.\n\nNeben der Möglichkeit, die Einstellungen über die grafische Oberfläche anzuzeigen und zu ändern, gibt es (ab Windows XP oder früher) auch den Befehl codice_3, mit dessen Hilfe auch die Firewall (im sogenannten „\"netsh firewall\"-Kontext“, also [bis Windows XP] mit der Befehlserweiterung codice_4 oder [ab Vista] codice_5) über ein Befehlsfenster gesteuert werden kann.\n\nBereits die Betriebssysteme Windows NT4 und Windows 2000 bringen bordeigene Möglichkeiten der Paketfilterung mit, die älteren Windows-Versionen fehlen. Einerseits stellt IPsec eine Möglichkeit zur regelsatzbasierten Paketfilterung dar, andererseits können in den Eigenschaften der Netzwerkverbindung Filter für eingehende Verbindungen auf bestimmte Ports definiert werden.\n\nMit Windows XP bis inklusive Service Pack 1 wird die \"Internet Connection Firewall\" (ICF) mitgeliefert. Sie kann für einzelne Netzwerkinterfaces aktiviert werden und prüft eingehende Datenpakete dahingehend, ob diese zuvor angefordert worden sind. In der Standardkonfiguration ist die ICF nicht für alle Netzschnittstellen aktiviert. Dies hatte zur Folge, dass viele nicht rechtzeitig gepatchte Windows-XP-Rechner den Internetwürmern „Blaster“ und „Sasser“ zum Opfer fielen.\n\nMit dem Service Pack 2 für Windows XP kamen weitere Sicherheitsfunktionen hinzu: Das Sicherheitscenter wurde eingeführt. Die Funktionen der Firewall wurden erweitert und sie wurde in \"Windows-Firewall\" umbenannt. Sie wird bei der Installation des Service Packs 2 oder bei der Windows-Installation von einem Datenträger mit integriertem (engl.: \"\") Service Pack 2 automatisch aktiviert. Anfänglich beeinträchtigte ein Programmierfehler die zuverlässige Funktion der Windows-Firewall. Dieser wurde kurz nach Erscheinen des Service Pack 2 in einer Sicherheitsaktualisierung vom 14. Dezember 2004 beseitigt. Nach außen gerichtete Verbindungen kontrolliert die \"Windows-Firewall\" unter Windows XP nicht. Bei nach innen gerichteten Verbindungen können keine Port-Bereiche angegeben werden.\n\nIn Windows Vista wurde die Funktion der Firewall erneut erweitert: Sie kann ausgehende Verbindungen filtern. Außerdem wurden die bisher von der Windows-Firewall unabhängigen IPsec-Richtlinien integriert und sie verfügt über einen Fernwartungszugang.\n\n\n"}
{"id": "1074757", "url": "https://de.wikipedia.org/wiki?curid=1074757", "title": "Zenwalk Linux", "text": "Zenwalk Linux\n\nZenwalk Linux (ehemals \"Minislack\") ist eine in Frankreich zusammengestellte Linux-Distribution, welche auf Slackware basiert. Ihr Fokus liegt auf Schlankheit, Benutzerfreundlichkeit und der Verwendung der neusten stabilen Software. Die Distribution verwendet standardmäßig Xfce als Desktop-Umgebung. In Deutschland wurde Zenwalk erstmals richtig bekannt als die Version 2.6 im Juli 2006 auf der Heft-CD des Magazins LinuxUser erhältlich war.\n\nZenwalk ist nicht zu verwechseln mit \"Zen Linux\", einer inzwischen eingestellten Live-CD-Linux-Distribution, die auf Debian basierte.\n\nZenwalk ist sehr schlank gehalten; die \"Standard\" und \"Live\"-Editionen sind nur je ca. 620 MB, die \"Core\"-Edition sogar nur 202 MB groß. Zum Vergleich: Slackware wird auf drei CDs bzw. einer 3,6 GB großen DVD vertrieben. Erreicht wird der geringe Speicherbedarf, indem im Lieferumfang nur jeweils ein Programm zu einer Software-Kategorie vorhanden ist, andere können jedoch nachträglich aus dem Zenwalk-Repository installiert werden. Bei früheren Versionen wurde auf das speicherplatzintensive OpenOffice.org-Office-Paket verzichtet und stattdessen AbiWord und Gnumeric mitgeliefert. Zenwalk Linux 6.0 enthält stattdessen eine leichtgewichtige Version von OpenOffice.org 3.0.\n\nNeuere Zenwalkversionen setzen ausschließlich auf das GTK+-Toolkit Version 2, andere Toolkits wie GTK+ Version 1 oder Qt sind in der Standardinstallation nicht vorhanden. Der Xfce-Desktop wird standardmäßig durch zahlreiche Gnome-Programme ergänzt. GNOME und KDE lassen sich nachträglich ebenfalls vollständig installieren, ab Version 6.4 gibt es eigene Variante mit Openbox als Fenstermanager. Zenwalk benutzte als eine von wenigen Distributionen LILO als Bootloader, mit Version 6.2 wurde auf GRUB 2 umgestellt.\n\nZenwalk Linux 6.0 (Standard Edition) enthält nach der Installation unter anderem folgende Pakete:\n\nDie neusten offiziellen Software-Pakete werden im \"snapshot\"-Verzeichnis veröffentlicht. Erst nach genügenden Tests der Software auf Programmfehler werden die Pakete ins \"current\"-Verzeichnis verschoben. Die Zenwalk Standard Edition beinhaltet nur Pakete aus dem \"current\"-Verzeichnis.\n\nDie Produkt-Philosophie gleicht der von Slackware in strafferer Ausführung; es werden während der Installation keine grafischen Hilfsmittel eingesetzt, kommerzielle Programme sind auf den Medien nicht enthalten und die Konfiguration muss unter Umständen manuell, ohne Konfigurationsprogramme angepasst werden. Software-Pakete werden Slackware-typisch im tgz-Format installiert.\n\nZenwalk benutzt den Paketmanager \"netpkg\", welcher Programmpakete von Repositories herunterlädt und – im Gegensatz zu Slackwares Paketmanager – Abhängigkeiten der Software auflöst. Die lokale Installation von Paketen erfolgt mittels \"installpkg\" wie unter Slackware auch, außerdem wird ab Version 4.0 der Paketmanager \"gslapt\" unterstützt.\n\nDie Konfiguration von mobilen Netzwerkverbindungen erfolgt durch das in Python geschriebene Tool \"wicd\".\n\nDie Linux-Distribution wurde unter dem Namen Minislack im Mai 2004 von Jean-Philippe Guillemin initiiert und mit Version 1.2, die im August 2005 erschien, in Zenwalk geändert. Mit Version 3.0 liefert Zenwalk erstmals einen eigenen Automounter aus. Dieser meldet das Einlegen optischer Medien (CDs, DVDs) an den bereits ab Version 2.4 benutzten udev-Daemon, welcher die Geräte dann automatisch mountet. Das zentrale Konfigurationstool Zenpanel hielt in Version 4.2 in die Distribution Einzug. Der ab Version 1.1 enthaltende hauseigene Paketmanager netpkg, bekam in der Version 4.0 eine grafische Oberfläche und wurde in Version 5.2 grundlegend überarbeitet. Des Weiteren wurden die Programme Firefox und Thunderbird aus Lizenzgründen in Version 4.8 durch die vom Debian Projekt gepflegten Pakete Iceweasel und Icedove ersetzt. Version 5.0 erfuhr eine weitere Verbesserung der WLAN-Unterstützung, indem das \"Wifi-Radar\" durch \"wicd\" ersetzt wurde. Ab Version 6.2 wurde von ext3 auf ext4 als Standarddateisystem umgestellt.\n\nZenwalk 7.2, basierend auf Linux-Kernel 3.4.8 und zu Slackware 14.0 kompatibel, steht aktuell seit 12. Oktober 2012 in fünf verschiedenen Editionen zum Download bereit:\nStandard Edition, Core Edition, Live Edition, Gnome Edition, Openbox Edition.\n\nDie \"Core Edition\" entstand durch einen Fork von der Standard Edition und wird parallel dazu weiterentwickelt. Sie beinhaltet ein Zenwalk-System ohne grafische Benutzeroberfläche. Es ist für Benutzer gedacht, die ihr Desktop- oder Server-System individuell erstellen möchten oder nur über einen sehr kleinen Speicher auf ihrer Harddisk verfügen.\n\nDie \"Live-CD Edition\" von Zenwalk (ehemals \"Zenlive\") basiert auf der jeweilig aktuellen Standard Edition und entstand im Juli 2006. Die Zenwalk-Live-CD lässt sich ab der Version 5.0 mithilfe des \"Zen Installers\" auf die Festplatte installieren. Die Zenwalk Live-CD unterscheidet sich von anderen Live-CDs, indem sie alle nötigen Programmbibliotheken und Anwendungen enthält, die nötig sind, um Software selbst herzustellen und zu kompilieren, sowie \"Live Clone\", ein Programm, mit dem sich eine den eigenen Bedürfnissen angepasste Variante des Systems erstellen und als ISO-Datei mastern lässt.\n\nDie Server-Version (aktuelle Version: 0.5; veröffentlicht am 14. Juni 2007) ist ein Derivat von Zenwalk mit hierfür optimiertem Kernel. Seine Entwicklung wurde eingestellt.\n\nZenEdu ist eine auf Zenwalk snapshot basierte Live-CD, die sowohl Software für Bildungszwecke als auch Computerspiele enthält.\n\n\n"}
{"id": "1076803", "url": "https://de.wikipedia.org/wiki?curid=1076803", "title": "Schachdatenbank", "text": "Schachdatenbank\n\nEine Schachdatenbank dient zum Speichern von Schachpartien, Schachanalysen oder auch von Schachkompositionen. Zu unterscheiden ist zwischen der Datenbank selbst und der Software zu deren Verwaltung.\n\nSchachdatenbanken sollen die effiziente Suche in Echtzeit nach Stellungen, Teilstellungen, Zugfolgen in einer Anzahl von mehreren Millionen Partien ermöglichen. Obwohl dies ein für Datenbanken typisches Ziel ist, hat es sich gezeigt, dass die Nutzung SQL-basierter relationaler Datenbanken als Basis nicht effizient ist. Deshalb stützen sich erfolgreiche Programme auf eigene Datenformate und Zugriffsalgorithmen, die zum Teil proprietär sind.\n\nIntegrierte und externe Schachprogramme helfen bei der Analyse und Bewertung von Stellungen. Trotzdem sind Schachdatenbanken nicht dafür gedacht, um gegen einen Menschen oder ein Schachprogramm zu spielen.\n\n\n\n\n\n\n\n\nIn der Regel enthalten Schachdatenbanken zahlreiche gespielte Partien, in einigen werden auch Analysen, Übungsbeispiele oder Varianten gespeichert. Sie unterstützen das Studium von Eröffnungen und die spezielle Vorbereitung auf zu erwartende Gegner, sofern die Datenbank geeignete Partien enthält. Darüber hinaus können aus dem Datenbestand Eröffnungsbibliotheken sowie dynamische Bewertungsfunktionen für Schachprogramme generiert werden.\n\n\nKarl-Heinz Milaster entwickelte 1995 eine Online-Schachdatenbank mit ungefähr 37 Millionen Positionen aus etwa einer halben Million Schachpartien plus 700.000 Computer-Analysen, mit der man Schachpartien und Positionen mit Referenz-Partien kommentieren lassen kann, sortiert nach der Spielstärke der Spieler. Die Suchergebnisse stehen innerhalb weniger Augenblicke zur Verfügung, die Abfrage ist kostenlos.\n\nIn einer Schachdatenbank lassen sich auch Schachkompositionen speichern und klassifizieren. Oft wurde Material für derartige Datenbanken bereits vor dem Computerzeitalter zusammengetragen und gesammelt.\n\nDie umfangreichste Studiensammlung verwaltet Harold van der Heijden. Im Juli 2008 bestand sie aus mehr als 73.000 Studien. Sie ist sehr weit vorangeschritten im Bezug auf den Grad der Vollständigkeit. In ihr sind Teilsammlungen mehrerer Sammler integriert. Sie ist ein Beispiel für ein hohes Maß an Kooperation vieler Beteiligter.\n\nÜber 300.000 Probleme aller Art enthält derzeit die Schachdatenbank WinChloe des Franzosen Christian Poisson (Stand Januar 2009). Die Datenbank kann beim Autor erworben werden und lässt sich über das Internet aktualisieren. Das Löseprogramm verfügt über automatische Themenerkennung, jegliche Brettarten und Figurentypen sind grafisch darstellbar.\n\nEin weiteres Beispiel für gute Kooperation bei der Vervollständigung von Sammlungen ist der PDB-Server, welcher per Internet abgefragt werden kann. Auf ihm wurde die Hilfsmattsammlung von John Niemann durch mehrere Schachfreunde dezentral erfasst. Weitere Teilsammlungen aus anderen Bereichen der Schachkomposition wurden hinzugefügt. Allerdings beruht sie nicht auf einer Schachdatenbank, sondern auf einer allgemeinen Datenbank mit der Möglichkeit einer eingeschränkten SQL-Abfrage für jedermann. Ihr Hauptproblem ist die Unzuverlässigkeit der Daten wie zum Beispiel Dubletten oder Ungenauigkeiten. Allerdings kann jedermann durch Kommentare zur Verbesserung der Informationen beitragen. Das eigentliche Problem besteht in der kontinuierlichen Verarbeitung der eingehenden Kommentare, was bislang durch Einzelne per Handarbeit zum Teil geschieht.\n\nWeitere Sammlungen (orthodoxe Miniaturen, Selbstmatts, o. ä.) existieren in den Händen von Einzelpersonen. Es besteht dort kein individueller Zugriff von außen. Dafür dürften die in ihnen enthaltenen Angaben zuverlässiger sein. Auch in ihnen sind bereits Ergebnisse früherer Sammler (wie Albert Heinrich Kniest, später übernommen von Peter Kniest) integriert. Ein typisches Beispiel hierfür war die \"Albrecht-Sammlung\", benannt nach dem Komponisten Hermann Albrecht. Dieser begann 1933, Zweizüger auf Karteikarten zu sammeln und zu klassifizieren. Bei seinem Ableben 1982 umfasste dieses Werk etwa 80.000 Probleme. Inzwischen ist diese Sammlung erweitert und digitalisiert. Sie wird von Udo Degener im Web bereitgestellt.\n\nDie Sammlung orthodoxer Miniaturen des Erfurters Klaus-Peter Zuncke bestand zum Zeitpunkt seines Todes am 15. November 2007 aus 61.807 Aufgaben mit der Forderung \"Matt in n Zügen\". Er hatte die Daten seiner Sammlung aus einer von ihm speziell für diesen Zweck entwickelten BASIC-basierten Datensammlung in eine allgemein verbreitete Schachdatenbank übertragen. Auch in ihr sind frühere Sammlungen integriert, wie zum Beispiel die Karteien von Gerhard Kaiser, Pehr Henrik Törngren und große Teile der \"Sammlung Maßmann\". Inzwischen wurden die Aufgaben dieser Sammlung in die PDB und YACPDB-Wiki übertragen.\n\n"}
{"id": "1079343", "url": "https://de.wikipedia.org/wiki?curid=1079343", "title": "NetMeeting", "text": "NetMeeting\n\nNetMeeting war eine Software von Microsoft für IP-Telefonie (\"Voice over IP\"), Mehrpunkt-Videokonferenzen, Chat und Datentransfer auf Basis des H.323-Protokolls. Des Weiteren kann man Microsoft-Programme (Word, Paint usw.) in Zusammenarbeit nutzen. Ein wesentlicher Bestandteil von NetMeeting ist die Funktionalität eines interaktiven Whiteboards. Es ist darüber hinaus möglich, die Ein- und Ausgabe eines Rechners auf einen anderen umzuleiten. Man kann sich also mit einem anderen PC über das Netzwerk verbinden; die eigene Tastatur und Maus steuert dabei den PC im Netzwerk, der eigene Monitor zeigt dabei die Ausgabe des PCs im Netzwerk an.\n\nEs ist im Installationspaket des Internet Explorer 5 und in der Windows-2000-Standardinstallation enthalten und ab dem Internet Explorer 4+ benutzbar.\nIn Microsoft Windows XP wurde es zwar durch den MSN Messenger abgelöst, wird jedoch unter \"\"%programfiles%\\Netmeeting\\conf.exe\"\" standardmäßig installiert.\nIn Microsoft Windows Vista ist NetMeeting nicht mehr enthalten, kann unter den 32-Bit-Versionen aber nachinstalliert werden.\n\nDas Programm wird von Microsoft nicht mehr weiterentwickelt. Bei Nachfolgeprodukt Office Communications Server wechselte Microsoft das Signalisierungsprotokoll von H.323 auf SIP, aktuell wird es unter dem Namen Skype for Business vertrieben.\n\nIn Zusammenarbeit mit Routern und Firewalls gibt es Verbindungsschwierigkeiten mit NetMeeting, da eine direkt erreichbare IP-Adresse vorausgesetzt wird. Die Weitergabe des TCP-Ports 1503 an den Rechner hinter dem Router erlaubt die Nutzung der Basisfunktionen, Sprach-/Videokommunikation erfordert zusätzlich das Weiterleiten des TCP-Ports 1720.\n\n\n"}
{"id": "1080445", "url": "https://de.wikipedia.org/wiki?curid=1080445", "title": "Basispunkt (CAD)", "text": "Basispunkt (CAD)\n\nBasispunkt ist ein spezieller Referenzpunkt im computerunterstützten Zeichnen (CAD).\n\nBasis- oder Einfügepunkte kommen häufig bei der Makroprogrammierung zur Anwendung. Mehrfach verwendete Elemente oder -Details werden dabei im Makro nur einmal definiert und wiederholt nach der Festsetzung oder Verschiebung des Basispunktes als Makro ausgeführt.\n\nDie Anwendung ist vielfältig; von der Verwendung als Einfügepunkt von Texten bis zum Nullpunkt einer beliebigen komplexen Figur. Meist wird der Basispunkt mit dem Ursprung (Koordinaten 0,0,0) gesetzt, um ein einfacheres Maßsystem beim Einfügen oder Plotten von Details, ohne die sonst erforderliche Maßumrechnung bei einer Verschiebung, zu ermöglichen.\n"}
{"id": "1081636", "url": "https://de.wikipedia.org/wiki?curid=1081636", "title": "QtiPlot", "text": "QtiPlot\n\nQtiPlot ist ein Programm zur Analyse und Visualisierung von Daten. \n\nQtiPlot ist an die Software Origin angelehnt. Mit QtiPlot können Daten in verschiedenen 2D- und 3D-Diagrammen dargestellt werden. Eine Interpolation der Daten kann mit linearen und nichtlinearen Funktionen erfolgen. Neben der englischen Menüführung lässt sich auch eine deutsche installieren. QtiPlot ist ab Version 0.9.9 als proprietäre Software lizenziert und nicht mehr als freie Software verfügbar. Für die Nutzung des Programms muss ein ein- bis dreijähriger Vertrag mit IONDEV abgeschlossen werden. Der Kauf einer Lizenz kostet für eine Privatperson 20 Euro im ersten Jahr und gestattet für den Zeitraum bis zu zwei Stunden technischen Support. \n\nBis einschließlich Version 0.9.8.9 ist QtiPlot freie Software, lizenziert unter der GPL. Der Quelltext des Programms ist bis Version 0.9.8.9 als kostenfreier Download, jedoch ohne technischen Support, erhältlich und kann für alle gängigen Betriebssysteme kompiliert werden.\n\n\n"}
{"id": "1083103", "url": "https://de.wikipedia.org/wiki?curid=1083103", "title": "MacWrite", "text": "MacWrite\n\nMacWrite war das graphische Textverarbeitungsprogramm, das 1984 zusammen mit den ersten Apple Macintosh ausgeliefert wurde. MacWrite und das Zeichenprogramm MacPaint, das ebenfalls mit den ersten Macs ausgeliefert wurde, waren die ersten Programme, die WYSIWYG auf Macintoshs und Personalcomputern überhaupt zu einer weiten Verbreitung verhalfen.\n\nVor MacWrite gab es lediglich zeichenorientierte Textprogramme, die einen beschränkten ASCII-Zeichensatz verwendeten, der sowohl für die Bildschirmdarstellung als auch für den Druck verwendet wurde. Die Formatierung erfolgte über unterschiedliche Tastenkombinationen, für die man oft mehrere Finger gleichzeitig benötigte, und dennoch war das Druckbild erst nach dem Ausdruck auf dem Papier sichtbar und musste dann oft noch korrigiert werden. Ein Beispiel für diese Art der Textverarbeitung ist das damals weitverbreitete WordStar.\n\nMit MacWrite konnten erstmals viele Endanwender ihre Texte mit den neuen Konzepten und Techniken bearbeiten, die heute noch kaum verändert üblich sind. Das Menü am linken oberen Bildschirmrand, die Rollbalken, Lineale, Vorratsbehälter für normale und für Dezimal-Tabulatorstopps, die man mit der Maus an die gewünschte Stelle schieben konnte usw. Im Hauptmenü gab es nicht nur \"Ablage\", \"Bearbeiten\" und \"Auffinden\", sondern auch \"Format\", \"Zeichensatz\" und \"Stil\" für unterschiedliche Lineale, Absatzformate, Zeichensätze – damals standardmäßig enthalten waren \"New York\", \"Geneva\", \"Toronto\", \"Monaco\", \"Chicago\", \"Venice\", \"London\" und \"Athens\" jeweils in 8, 12, 14, 18, 24 Punkten – sowie Standarddruck, \"Fettdruck\", \"Kursiv\", Unterstrichen, Konturschrift und Schattiert.\n\n"}
{"id": "1085151", "url": "https://de.wikipedia.org/wiki?curid=1085151", "title": "MacPaint", "text": "MacPaint\n\nMacPaint war ein bitmaporientiertes Bildbearbeitungsprogramm, das bereits 1984 auf den ersten Macintosh von Apple zusammen mit MacWrite ausgeliefert wurde und in verschiedenen verbesserten Versionen bis 2004 auf dem Markt war. MacPaint war wegweisend für viele andere ähnlich arbeitende Programme wie PixelPaint und Photoshop, das erstmals um 1990 auf dem Macintosh IIfx verfügbar war. \n\nDank graphischer Benutzeroberfläche konnten schon damals mit MacPaint erstellte Bilder ganz einfach in MacWrite-Dokumente eingefügt und auch gedruckt werden. Da der erste Macintosh nur einen Schwarzweißbildschirm hatte, konnte man mit MacPaint nur Schwarzweißbilder (Auflösung nur 576 × 720 Pixel) editieren. Das war aber keine Beschränkung, denn der damals anschließbare ImageWriter konnte standardmäßig nur schwarzweiße Bilder der Größe 203 × 204 mm mit 72 DPI drucken.\n\nMacPaint-Bilddateien lassen sich mit dem Kommandozeilen-Tool macptopbm (Teil des Netpbm-Projektes) in PBM-Dateien umwandeln, dieses Format kann mit heutigen Programmen weiterverarbeitet werden.\n\nIm Jahr 2010 veröffentlichte Apple über das Computer History Museum den MacPaint 1.3 Quelltext, bestehend aus Assembler und Pascal-Code, damit einhergehend wurde auch eine historische Variante des QuickDraw Quelltextes verfügbar.\n\n"}
{"id": "1088604", "url": "https://de.wikipedia.org/wiki?curid=1088604", "title": "SUSE Linux Enterprise Server", "text": "SUSE Linux Enterprise Server\n\nSUSE Linux Enterprise Server (SLES) ist eine Linux-Distribution von SUSE, die speziell auf Unternehmenskunden ausgelegt ist.\n\nDas Produkt ist mit einem entsprechenden Supportangebot und langjähriger Wartungsphase ausgestattet. Die für den unternehmenskritischen Einsatz notwendigen Hardware- und Softwarezertifizierungen spielen eine ebenso große Rolle.\n\nSLES wurde im Sommer und Herbst 2000 auf Basis von Suse Linux von einem sehr kleinen Team unter der Leitung von Marcus Kraft, Bernhard Kaindl und Joachim Schröder bei SuSE in Nürnberg entwickelt und wurde erstmals am 31. Oktober 2000 in einer Version für IBM-Großrechner (S/390) verfügbar. Eine Vorabversion wurde bereits am 13. Juli 2000 veröffentlicht. Im Dezember 2000 wurde mit dem in Schweden führenden Telekommunikationsanbieter Telia ein erster großer Kunde bekannt. Im April 2001 folgte dann SLES für die x86-Architektur, die im Juli 2001 die notwendige Zertifizierung für den Einsatz von mySAP erlangte.\n\nIm 4. Quartal 2005 verkaufte Novell 65.000 Stand-Alone Subscriptions des Suse Linux Enterprise Server.\n\nNovells SUSE-Linux-Enterprise-Produkte basieren auf den Community-Arbeiten aus openSUSE, die mit Novells Unterstützung weiterentwickelt wurden.\n\nMit SUSE Linux Enterprise Desktop (SLED) bekam SLES einen Zwilling im Desktop-Bereich, der parallel mit gleichem Konzept und selber Codebasis vertrieben wird. SLED und SLES werden zusammen auch als Suse Linux Enterprise (SLE) bezeichnet. Parallel zu SLES 10 wechselt Novell auch für den Unternehmensdesktop auf die Verwendung der bekannten Marke SUSE und so heißt der Nachfolger des ehem. Novell Linux Desktop 9 (NLD) Suse Linux Enterprise Desktop (SLED) 10.\n\nUrsprünglich wurde SLES mit dem auch in SUSE Linux verwendeten und sowohl viel gelobten als auch kritisierten Desktop-Theme \"Suse Keramik\" ausgeliefert, das über ein an das KDE-Theme Keramik angepasstes GNOME-Theme ein einheitliches Look & Feel für Gnome und KDE bietet.\n\nMit der Integration von SUSE in Novell dominiert jedoch nicht mehr KDE. Mit SLED 10 wurde bei der Installation nicht mehr gefragt, ob KDE oder GNOME als Desktop-Umgebung installiert werden soll. KDE konnte nur mehr als \"Pattern\" in der Softwareselektion ausgewählt werden. Seit SLED 11 kann wieder zwischen GNOME und KDE gewählt werden.\n\nIm Gegensatz zu Red Hat Enterprise Linux ist in einigen Programmen die Fähigkeit integriert, MP3-Dateien abzuspielen, jedoch nicht über einen von allen Programmen direkt verwendbaren mp3-Codec, sondern über die Verwendung von RealPlayer als Abspielsoftware im Hintergrund, beispielsweise bei Amarok.\n\nSLES ist ein Enterprise OS, also ein Betriebssystem, das auch auf die Bedürfnisse großer Firmen eingeht. Als Enterprise OS ist es folglich auf Stabilität und lange Wartungszyklen ausgelegt. So ermöglicht es SLES, ein System mit uneingeschränktem Herstellersupport für 5 Jahre (oder mehr) nutzen zu können, ohne Pakete bzw. Softwareversionen migrieren zu müssen. Gerade dieser Sachverhalt macht ein Enterprise OS für den kommerziellen Einsatz interessant, denn nur bei langen Supportzeiträumen haben große Softwarehäuser wie Oracle oder SAP ein Interesse, ein Betriebssystem für ihre Anwendungen zu zertifizieren. Gleiches gilt analog für die großen Computer- und Serverhersteller. Enterprise OSes findet man daher meist auf Servern, aber auch dort, wo Rechner extrem stabil laufen sollen (z. B. Börse, Medizin, Raumfahrt).\n\nSLES wird wie SUSE Linux mit dem integrierten grafischen Installations- und Administrationswerkzeug YaST, das auch für Einsteiger leicht bedienbar ist, installiert und administriert. Eine einfache Konfiguration des integrierten Paketfilters ist bereits während der Installation über YaST möglich.\n\nWegen der Zertifizierung und Konzentration auf bestmögliche Stabilität im Enterprise-Umfeld folgen Neuauflagen von SLES einem eher langsameren Rhythmus und werden auch länger gewartet. Der Kunde bekommt meistens eine Wartungsdauer von mind. 5 Jahren (7 bis max. 9 Jahre für SLES10) zugesichert, so dass er zeitlichen Spielraum hat, ein Update sorgsam zu planen und zu terminieren. Durch die langen Wartungszeiträume kann er auch gut einzelne Neuauflagen überspringen und sein Verhalten bezüglich Aktualisierungen an seine individuellen Bedürfnisse anpassen.\n\nDer Suse Linux Enterprise Server wird immer mit der zum Veröffentlichungszeitpunkt aktuellen Version der Linux Standard Base zertifiziert. Bei SLES 10 ist dies 3.0, bei SLES 9 2.0. Ältere Versionen des SLES sind nach älteren LSB-Standards zertifiziert. Einige Versionen von Suse Linux sind ebenfalls nach dem zum Veröffentlichungszeitpunkt aktuellen LSB-Standards zertifiziert.\n\nIm Jahr 2014 entschied SUSE, die Quellcodes von SLE der Community zur Verfügung zu stellen, die darauf basierend das frei verfügbare openSUSE Leap erstellte.\n\nDabei werden openSUSE Leap und SUSE Linux Enterprise parallel weiterentwickelt. Dies ist vergleichbar mit der Red Hat Enterprise Linux (RHEL) Distribution mit ihren frei verfügbaren Varianten wie z. B. CentOS. Außerdem können SLES und SLED in einer kostenlosen Testversion heruntergeladen werden. Diese enthält auf 60 Tage begrenzte Upgrade-Berechtigung. Danach kann die Testversion unbeschränkt ohne Support und Upgrades weiter verwendet werden.\n\nAls freie Alternative existiert die Distribution openSUSE Tumbleweed, auf deren Basis neue Hauptversionen von SLES und SLED entwickelt werden. OpenSUSE Tumbleweed ist in dieser Hinsicht vergleichbar mit Fedora bei Red Hat Enterprise Linux. Neue Versionen von openSUSE werden intensiv von SUSE getestet und auf verschiedene Plattformen wie die IBM zSeries portiert. Die dabei gewonnenen Erkenntnisse gehen zurück an die Community. Für den Produktivbetrieb nicht als geeignet angesehene Pakete der openSUSE-Distribution werden entfernt, weitere frei verfügbare „closed source“-Produkte von anderen Herstellern, mit denen entsprechende Lizenzvereinbarungen bestehen, werden hinzugefügt (z. B. ibm-java).\n\n"}
{"id": "1096735", "url": "https://de.wikipedia.org/wiki?curid=1096735", "title": "Behördendesktop", "text": "Behördendesktop\n\nDer Behördendesktop war eine Debian-basierte, öffentliche Linux-Distribution des Bundesamts für Sicherheit in der Informationstechnik (BSI), die in Zusammenarbeit mit der Credativ GmbH aus Jülich entwickelt wurde. Die Idee war es, öffentlichen Verwaltungen kostengünstige und sichere Softwarelösungen anzubieten.\n\nIm Wesentlichen wurde dabei Debian GNU/Linux|Debian Sarge um OpenOffice.org und KDE 3.3.2 erweitert. Die Installation war stark vereinfacht, und eine automatische Hardware-Erkennung ersetzte ein manuelles Editieren von vielen Konfigurationsdateien.\n\nAndere Desktop-Oberflächen wie Gnome oder Windowmaker fehlten, um die Distribution für die leichte Einrichtung von einheitlichen PCs mit grafischer Benutzeroberfläche, Webbrowser (Firefox) und OpenOffice.org-Büro-Software anwendbar zu machen.\n\nDas Projekt wurde im Jahr 2008 vom BSI für „veraltet“ erklärt und wird nicht mehr weitergeführt.\n\nDie Gründe, warum Verwaltungen Linux einsetzen, sind vielseitig, im Vordergrund stehen vor allem finanzielle Aspekte, da keine Lizenzen erworben werden müssen, und Sicherheitsüberlegungen. Hierbei spielt die Überlegung eine Rolle, dass das häufig verwendete Windows nur begrenzte Zeit von Microsoft unterstützt wird, läuft diese Frist ab, entstehen Sicherheitslücken, wenn nicht für viel Geld neue Lizenzen aktuellerer Versionen erworben werden. Die Funktionalität der nicht mehr unterstützten Software ist im Rahmen des jeweiligen Bedarfs der Behörden häufig noch gegeben, allerdings wird aufgrund von wirtschaftlichen Erwägungen die Unterstützung eingestellt. Diese Gründe waren mit ausschlaggebend für die Entwicklung eines an die speziellen Bedürfnisse öffentlicher Verwaltungsstellen angepasster Software.\n\n\n\n"}
{"id": "1098114", "url": "https://de.wikipedia.org/wiki?curid=1098114", "title": "Blending (Grafik)", "text": "Blending (Grafik)\n\nUnter Blending (engl. für \"vermischen\") versteht man in der Computergrafik einen Vorgang bei der Rasterung, bei dem die Farbe eines darzustellenden Pixels mit der des bereits im Grafikspeicher vorhandenen Pixels vermischt wird.\n\nBeim Alpha Blending wird dieser Mischprozess mit Hilfe des Alphakanals, in dem sich zusätzliche Informationen über die Transparenz eines Bildes befinden, durchgeführt. Für den einfachsten Fall gilt:\n\nDer neue Farbwert C berechnet sich aus dem Farbwert A des Bildes, der mit seinem entsprechenden Alpha-Wert als Faktor in die Gleichung eingeht. Der bereits vorhandene Farbwert B wird dementsprechend verkleinert, so dass die beiden Faktoren zusammen 1 ergeben. Das Ergebnis ist also nur vom Alphawert des neuen Bildes abhängig, die Transparenz des Hintergrundes spielt keine Rolle. Eingesetzt wird diese Technik unter anderem zur Realisierung von Schatteneffekten.\n\nBeim \"Fog-Blending\" wird das darzustellende Bild dahingehend verändert, dass die Farbe alle Punkte entsprechend ihrer Entfernung vom Betrachter im dreidimensionalen Raum mit einem festen Farbton gemischt werden. Diese Technik wird vorwiegend dazu verwendet, weit entfernte Objekte in einem künstlichen Nebel verschwinden zu lassen. Zur Realisierung des Effektes bedient man sich der Entfernungsinformation der Pixel, die im Z-Buffer gespeichert ist. Dieser Wert, zusammen mit einem Blendfaktor, einer Farbe und einer einfachen Funktion bestimmen den resultierenden Farbwert. Gebräuchlich sind unter anderem einfache lineare, exponentielle aber auch logarithmische Funktionen.\n"}
{"id": "1099817", "url": "https://de.wikipedia.org/wiki?curid=1099817", "title": "Computer-Flohmarkt", "text": "Computer-Flohmarkt\n\nDer Computer-Flohmarkt (kurz CF) war in den 1990er Jahren ein populäres deutschsprachiges Kleinanzeigenblatt zum Thema Computer. Herausgeber war der Verlag Thomas Eberle in Maulbronn.\n\nDer CF verstand sich als „Computerzeitschrift für kostenlose Kleinanzeigen“. Er erschien in zweimonatlichem Turnus und war – kleinanzeigentypisch – in Rubriken untergliedert, die hauptsächlich Computerthemen abdeckten. Das Blatt war grob nach Computersystemen und -plattformen untergliedert. Innerhalb einer Plattform gab es wiederum Rubriken für Kauf- und Verkaufsanzeigen und Diskussionsforen zum Erfahrungsaustausch. In diesen Foren konnten Leser private Mitteilungen und Fragen – im Stil von Usenet-Newsgroups oder Webforen – veröffentlichen und mit anderen Lesern diskutieren.\n\nIm Gegensatz zu üblichen, eher regional verbreiteten Anzeigenblättern wurde der CF als Fach-Offertenblatt bundesweit vertrieben. Private Inserate waren gratis, geschäftliche Anzeigen wurden gegen Gebühr geschaltet und mit dem Buchstaben \"G\" gekennzeichnet. Eine typische Ausgabe des Blattes bot Platz für mehrere tausend Kleinanzeigen.\n\nAuf den ersten Blick scheint das Format des Computer-Flohmarktes für den Austausch privater Mitteilungen ungeeignet. Dies gilt im Besonderen für die Führung Usenet-artiger Diskussionen – die Wartezeit zwischen einem „Posting“ und der Antwort des Adressaten betrug zwischen zwei und vier Monaten. Aus der Beschränkung auf wenige Zeilen ergab sich zudem die Notwendigkeit, Anzeigen im Telegrammstil zu verfassen.\n\nFormatbedingt fehlte auch wichtige Meta-Information wie der Absender einer Mitteilung. Die Leserschaft behalf sich damit, Beiträge mit einem Nickname (im Leserjargon \"Pseudonym\", kurz \"Pseudo\") zu versehen.\n\nTrotz dieser Mängel fand das Format in der Leserschaft regen Zuspruch. Der Diskussionsstil war in der Regel knapp und pointiert, aber meist freundschaftlich. Das Konzept führte rasch zur Bildung einer aktiven Offline-Community. Dies trug auch zu Weiterentwicklung und Erhalt des Szene-Gedankens bei.\n\nEin Kuriosum war die Beteiligung der im Verlag angestellten Schriftsetzer an einzelnen Diskussionen. Deren Aufgabe bestand darin, die meist handschriftlich per Post eingereichten Kleinanzeigen in das Redaktionssystem einzupflegen. Einige der \"Säzzer\" (Leserjargon) waren der Leserschaft namentlich bekannt und wurden schnell in die Community integriert.\nDabei beeinflussten sie den Diskussionsverlauf durch sporadische Kommentare, allerdings auch durch Zensur einzelner Mitteilungen, wenn diese den Gegenstand der Rubrik verfehlten oder strafrechtlich relevant waren.\n\nMitte der 1990er Jahre wurde die Erfassung der Anzeigen durch Einführung einer Erfassungssoftware teilautomatisiert. Diese wurde zum Selbstkostenpreis an interessierte Leser verschickt und ermöglichte ihnen, ihre \"Anzis\" (Leserjargon) auf Diskette per Post einzureichen.\n\nIn der EDV-Welt der 1990er Jahre nahm die Relevanz von Urheberrechtsverstößen stark zu. Im Computer-Flohmarkt äußerte sich dies bald auf skurrile Art und Weise. Einige Softwarefirmen vermuteten Umsatzrückgänge durch illegale Kopien von Software und beauftragten den Rechtsanwalt Günter Freiherr von Gravenreuth, gegen illegalen Softwaretausch vorzugehen.\n\nFreiherr von Gravenreuth engagierte daraufhin mehrere Testbesteller, die den CF und andere Zeitschriften nach verdächtigen Anzeigen durchsuchen sollten. Die Testbesteller verschickten dann sporadisch Lockbriefe per Post an die Inserenten. Darin gaben sie sich – unter wechselnden falschen Namen wie „Tanja Nolte-Berndel“ – als weiblicher Teenager aus und baten um Zusendung bestimmter Computerprogramme. Ging man darauf ein, erhielt man kurze Zeit später eine Abmahnung mit strafbewehrter Unterlassungserklärung.\n\nIn einigen Fällen erwirkte Freiherr von Gravenreuth vor Gericht auch die Anordnung von Hausdurchsuchungen bei CF-Lesern.\n\nIm Lauf der Jahre verlor der Computer-Flohmarkt an Bedeutung, während die Nutzung von Onlinediensten und des Internets zunahm. Im Jahr 1999 stellte der Eberle-Verlag den Vertrieb des CF ein. Die Rechte am Markennamen wurden an den Verlag \"Dr. Heide & Partner GmbH\" übertragen.\n\nDer neue Verlag kündigte bald an, den Computer-Flohmarkt wieder auferstehen zu lassen. Im Jahr 2000 erschien die erste neue Ausgabe, enttäuschte aber den Großteil der Leserschaft der ersten Stunde. Dies lag nicht nur an radikalen konzeptionellen Änderungen, sondern auch an qualitativen Mängeln in Inhalt und Erscheinungsbild. Mitte 2001 gab es ein kurzes Joint Venture zwischen dem CF und der \"GO64!\", einem Printmagazin rund um die Commodore-Familie. Beide Magazine wurden im zweimonatlichen Rhythmus zusammen produziert. Diese Doppelausgabe brachte den Vorteil für das Abomagazin \"GO64!\", nun auch am Kiosk erhältlich zu sein. Nach Auflösung der Kooperative verschwand die Zeitschrift CF endgültig vom Markt.\n\nDer Thomas-Eberle-Verlag gab auch zwei Ableger des CF heraus: Die C64-Zeitschrift \"Brotkasten Live\" und die PC-Zeitschrift \"PC-Heimwerker\". Diese Zeitschriften enthielten im Gegensatz zum CF keine Kleinanzeigen, sondern waren reine Foren.\n\nBeim \"PC-Heimwerker\" wurde nach einiger Zeit der Versuch gestartet, die Ausgaben in digitaler Form per Diskette zu vertreiben. Dazu wurden die Texte in dBase-III-Dateien gespeichert und konnten mit Hilfe eines Programms mit grafischer Oberfläche angezeigt werden.\nBeide Zeitschriften wurden nach einigen Ausgaben wieder eingestellt, weil der kommerzielle Erfolg ausblieb.\n\nEine weitere Zeitschrift, die im Thomas-Eberle-Verlag erschien, war die von Konzept und Aufmachung her sehr ähnliche \"Musik-Flohmarkt\".\n\n\n"}
{"id": "1102331", "url": "https://de.wikipedia.org/wiki?curid=1102331", "title": "Cult of the Dead Cow", "text": "Cult of the Dead Cow\n\nCult of the Dead Cow (kurz cDc) ist eine Hackergruppe, die 1984 in Lubbock, Texas, Vereinigte Staaten gegründet wurde. Bekannt wurde sie vor allem durch die (oft auch illegal eingesetzte) Fernwartungssoftware Back Orifice. Ihre neue Software und andere Medien veröffentlicht die Gruppe zuerst über ihren Weblog.\n\nDie Gruppe wurde im Juni 1984 von \"Grandmaster Ratte\" und Frank Gibbe gegründet.\n\nDas Gruppenmitglied Drunkfux etablierte im Dezember 1990 die Hacker-Konferenz HoHoCon, die üblicherweise in Houston stattfand. Diese Hacker-Konferenz war die erste ihrer Art, die auch Journalisten und Gesetzeshüter einlud, sich zu beteiligen.\n\n1991 begann die Gruppe, selbst aufgenommene Musik auf Kassetten über ihr Postfach zu vertreiben. Viele dieser Aufnahmen sind mittlerweile online verfügbar.\n\nDie Usenet-Newsgroup „alt.fan.cult-dead-cow“ wurde im Oktober 1994 gegründet. cDc ist damit die erste Hackergruppe mit eigener Newsgroup. Im November des gleichen Jahres behauptete die Gruppe, Ronald Reagan mittels eines Blasrohrs mit Alzheimer infiziert zu haben.\n\nCult of the Dead Cow erklärte im Jahr 1995 Scientology den Krieg. \n\nDas cDc-Mitglied \"Omega\" gilt als Urheber des Begriffs \"Hacktivism\" (de: Hacktivismus), den er 1998 erstmals in einer Email erwähnte. Von da an prägten die Aktivisten die Definition des Ausdrucks, die sich auf die Entwicklung und Nutzung von Technologie zur Förderung der Menschenrechte und des offenen Informationsaustausches bezieht.\n\n1997, bevor die Verbreitung von Musik über das Internet üblich war, veröffentlichte cDc auf ihrer Webseite eigene Musik im MP3-Format.\n\nIm Februar 2000 war cDc Thema eines 11-minütigen Dokumentar-Kurzfilms namens „Disinformation“. Ebenfalls im Februar 2000 beriet das cDc-Mitglied \"Mudge\" den amerikanischen Präsidenten Bill Clinton zum Thema Internetsicherheit.\n\ncDc communications ist die Dachorganisation von Cult of the Dead Cow. cDc ist eine der drei Gruppen von cDc communications. Die anderen beiden sind die Ninja Strike Force und Hacktivismo.\n\n1996 gründete der cDc die Ninja Strike Force (NSF), eine Gruppe von sogenannten Ninja, die die Ziele des cDc „online und offline“ erreichen sollten. Der cDc eröffnete 2004 das NSF Dōjō. Ein Mitglied der NSF betreibt außerdem ein Streaming-Radio, auf dem Aufnahmen von Hacker-Konferenzen, ein alternatives Bildungsprogramm und eine Vielzahl unterschiedlicher Musikstücke geboten werden.\n\n2006 startete die Ninja Strike Force ihre eigene Website.\n\nHacktivismo wurde 1999 von cDc gegründet und ist eine unabhängige Gruppe unter dem Dach von cDc communications. Ihr Schwerpunkt liegt in der Erstellung von Anti-Zensur-Technologie zur Stärkung der Menschenrechte im Internet. Der Zugang zu Information gehöre zu den grundlegenden Menschenrechten.\n\nDie Zielsetzung der Gruppe ist in der \"Hacktivismo Declaration\" zusammengefasst, welche die Allgemeine Erklärung der Menschenrechte und den völkerrechtlichen Schutz der politischen Opposition auf das Internet übertragen will.\n\nHacktivismo hat außerdem eine eigene Software-Lizenz unter dem Namen \"Hacktivismo Enhanced-Source Software License Agreement\" veröffentlicht.\n\n\n\n\n\n"}
{"id": "1103224", "url": "https://de.wikipedia.org/wiki?curid=1103224", "title": "Macintosh IIsi", "text": "Macintosh IIsi\n\nMacintosh II si ist der Name eines von Apple produzierten Rechners.\n\nDer mit 20 MHz getaktete und auf dem Motorola 68030 basierende Rechner mit einem Arbeitsspeicher von 65 MB bei maximalem Ausbau wurde von Apple im Oktober 1990 vorgestellt. Zur Erweiterung stand ein PDS-Slot zur Verfügung, der mittels eines Adapters zu einem NuBus-Port konvertiert werden konnte. Neben der 8-Bit-Farbgrafikkarte auf der Hauptplatine bot der IIsi wie alle Rechner der II-Serie ein 8-Bit-Stereo-Soundsystem. Am externen SCSI-Bus-Ausgang konnten Festplatten, CD-ROM-Laufwerke, Scanner oder Streamer angeschlossen werden.\n\nDer Mac IIsi war eine abgespeckte Version des Macintosh IIci – als preisgünstige Alternative für private Anwender. Um die Kosten gering zu halten, teilte sich die Grafik wie beim IIci die Speicherbandbreite mit der CPU. Insbesondere die höheren Auflösungen und Farbtiefen waren langsamer als beim IIci. Die On-Board-Grafik konnte mit einer maximalen Auflösung von 640 × 480 Pixeln bei 8 Bit oder 640 × 870 monochrom betrieben werden. Von Apple gab es verschiedene Monitore für den 15-poligen Videoausgang des Gerätes: einen Farbmonitor mit einer Auflösung von 512 × 384 Pixeln und zwei monochrome Monitore mit 640 × 480 bzw. 640 × 870 Pixeln. Einen aufpreispflichtigen NuBus- oder LC-PDS-Steckplatz gab es auf Erweiterungsplatine zusammen mit dem 68882-Koprozessor. 1 MB RAM waren auf der Hauptplatine fest eingelötet. Die CPU des IIsi war mit 20 statt wie beim IIci mit 25 MHz getaktet, obwohl die verwendeten Teile für den 25 MHz Prozessortakt ausgelegt waren. Deshalb war das Übertakten (häufig bis 25 oder 28 MHz) des IIsi weit verbreitet, auch weil der gemeinsame Zugriff von CPU und Grafik auf den Speicher dann wesentlich besser funktionierte und die Grafikausgabe merklich schneller wurde. \n\nDas neu konstruierte Motherboard des Mac IIsi hatte einen PDS-Erweiterungssteckplatz, für den spezielle Karten mit dem mathematischen Koprozessor Motorola 68882 oder angeboten wurden und in den gesteckt werden konnten, sowie einen internen Standard-NuBus-Slot enthielt, der mit den anderen Macs der IIsi-Serie kompatibel war. Angeboten wurde der IIsi mit entweder 40- oder 80-MB-Festplatte sowie einem 1,44-MB-Diskettenlaufwerk. \n\nMit Ausnahme des IIci war der IIsi einer der ersten Macs mit einem Audioeingang, der zuweilen Probleme machte, wenn sich über einen längeren Zeitraum die Lautsprecherkontakte abnutzten, was dann zu einem Ausfall der Tonausgabe führte. Dies konnte nur durch die Reinigung der Kontakte und ihrer Behandlung mit Leitsilber behoben werden. \nDie ersten verfügbaren ROMs waren oft fehlerhaft, die das Starten des Systems sogar unmöglich machten. Obwohl Apple zur Fehlerkompensation bald ein spezielles ROM-SIMM auf späteren Platinen ergänzte, verblieb der Steckplatz für die ROM-SIMM weiterhin erhalten. \n\n"}
{"id": "1104433", "url": "https://de.wikipedia.org/wiki?curid=1104433", "title": "Blue Sky Studios", "text": "Blue Sky Studios\n\nBlue Sky Studios ist ein auf Computeranimation spezialisiertes Unternehmen, das zu 20th Century Fox Filmed Entertainment gehört und sich, ähnlich den Pixar Animation Studios, auf animierte Kinofilme spezialisiert hat.\n\nDie Firma wurde 1987 gegründet und hatte bis Januar 2009 ihren Sitz in New York City. Im Januar verlegte die Firma ihren Sitz nach Greenwich (Connecticut). Die Belegschaft ist laut Firmenwebsite etwa 180 Mitglieder groß.\n\nBlue Sky Studios hatte mit \"Ice Age\", \"Robots\" und \"Ice Age 2 – Jetzt taut’s\" große Erfolge. Die Firma produzierte drei Kurzfilme, die für den Oscar in der Kategorie Bester animierter Kurzfilm nominiert wurden. Der Film \"Bunny\" (1998) hat den begehrten Preis gewonnen. Der zweite Kurzfilm war \"Scrats neue Abenteuer\" mit der Figur Scrat, aus den beiden Ice Age-Filmen. Er wurde 2003 für den Oscar nominiert, verlor aber. Der dritte Kurzfilm mit Scrat, \"Keine Zeit für Nüsse\", wurde 2007 für den Oscar nominiert. \"Ice-Age Teil 3\" kam Anfang Juli 2009 in die Kinos. Weitere Fortsetzungen der Reihe folgten.\n\n\n\n"}
{"id": "1106332", "url": "https://de.wikipedia.org/wiki?curid=1106332", "title": "Grafikmodus", "text": "Grafikmodus\n\nAls Grafikmodus ( \"alle Punkte adressierbar\", APA) bezeichnet man eine Betriebsart von Grafikkarten, bei der das auf dem Monitor angezeigte Bild aus einem Rechteck einzelner Pixel aufgebaut ist. Das Gegenteil ist der Textmodus.\n\nDer Betrieb im Grafikmodus ist auf modernen Computersystemen heutzutage Standard und ist eine Grundlage für grafische Benutzeroberflächen. \n\nGrafikmodi, basierend auf Grafikstandards, unterscheiden sich in folgenden Eigenschaften:\n\n"}
{"id": "1108177", "url": "https://de.wikipedia.org/wiki?curid=1108177", "title": "Origin2000", "text": "Origin2000\n\nOrigin2000 ist ein massiv paralleler Computer von Silicon Graphics (SGI), der 1996 vorgestellt wurde. Es handelt sich um ein ccNUMA-System mit verteiltem, gemeinsam genutztem Speicher (Distributed Shared Memory, DSM) bei dem bis zu 512 MIPS R10000-, R12000- oder R14000-Prozessoren mit bis zu 600 MHz über NUMAlink genannte Hochgeschwindigkeitsverbindungen gekoppelt werden.\n\nDie Prozessoren eines Origin2000 sind paarweise an einen \"Hub\" genannten Crossbar-Switch angeschlossen. Diese Hubs sind untereinander über ein Netzwerk von NUMAlinks verbunden, die einen Hyperwürfel bilden und an deren Knoten Router sitzen. Da die Router-Chips nur sechs NUMAlink-Verbindungen haben, muss ab 64 Prozessoren zu einer Fat-Hypercube-Topologie übergegangen werden. Die NUMAlinks transportieren bidirektional 1,6 Gigabytes.\nZu Anfang wurde statt NUMAlink die Bezeichnung \"CrayLink\" verwendet, um auf die ebenfalls 1996 erfolgte Übernahme von Cray Research hinzuweisen. Cray-Technologie wurde aber erst in der nächsten NUMAlink-Generation verwendet.\n\nDie Hubs verbinden die Prozessoren neben NUMAlink und Speicher auch mit den \"XBow\"- Ein-/Ausgabe-Crossbar-Switches an die insgesamt acht Ein-/Ausgabe- oder Prozessorkarten angeschlossen werden können. Die verwendete \"XIO\"-Verbindung ist elektrisch mit den NUMAlinks identisch, das Protokoll ist aber für Ein-/Ausgabe optimiert und nicht routingfähig. Wie die NUMAlinks überträgt XIO bidirektional 1,6 GB, d. h. eine XIO-Karte kann Daten mit 800 MB/s in den Origin2000 übertragen.\n\nJeder Origin2000 besteht aus einem oder mehreren \"Compute-Modulen\", von denen jeweils zwei in einen 19-Zoll-Schrank eingebaut werden. Jedes Compute-Modul nimmt ein bis vier Prozessor-Boards mit je zwei Prozessoren und bis zu vier Gigabyte Speicher auf. Auf der Backplane der Module sind zwei XBow-Ein-/Ausgabe-Crossbars montiert, die den Anschluss von bis zu 12 XIO-Karten erlauben. Standardmäßig ist eine \"BaseIO\" genannte Karte montiert, die Konsolenanschluss, Netzwerk und Massenspeicheranschluss (SCSI) bereitstellt.\nIn einen überbreiten XIO-Steckplatz kann ein PCI-Cardcage eingesetzt werden, der die Montage von bis zu drei PCI66-64-Karten erlaubt.\n\nWeiterhin können in jedes Computer-Modul zwei Router-Boards eingesetzt werden, um es via NUMAlink mit anderen Computer-Modulen zu verbinden. Jedes Modul kann mit zwei CD-ROM- und/oder DAT-Laufwerken und bis zu sechs SCSI/SCA-Festplatten bestückt werden.\n\nIm Ausbau mit mehr als 64 Prozessoren muss zu den Schränken mit Computer-Modulen ein weiterer \"Meta-Router\"-Schrank hinzukommen, um die Erweiterung auf Fat-Hypercube-Topologie zu realisieren. Somit umfasst ein Origin2000 im Maximalausbau mit 512 Prozessoren 33 19-Zoll-Schränke.\n\nDurch Hinzufügen eines oder mehrerer \"Graphics-Module\" mit je einer oder zwei InfiniteReality-Grafik-Pipes kann ein Origin2000 zu einem Onyx2 erweitert werden. In dieser, von SGI als Grafik-Supercomputer bezeichneten Kombination kann an jedes Compute-Modul über XIO bis zu zwei Graphics-Module mit zwei Pipes angeschlossen werden. Dies ermöglicht den Aufbau komplexer Simulations- und Visualisierungsanwendungen.\n\nWie alle NUMAlink-Systeme von SGI wird der Origin2000 wie ein normales SMP-System programmiert, d. h. es sind keine besonderen Vorkehrungen zur Verteilung der Aufgaben auf die Prozessoren oder zur Kommunikation der Prozesse notwendig. Dies wird automatisch vom IRIX-Betriebssystem in Zusammenarbeit mit der Hardware übernommen. Erst wenn man die letzten 5 % Leistung benötigt, muss man die verwendete Software für das ccNUMA-System optimieren. Aber auch für Cluster-Systeme geschriebene Programme können über eine angepasste MPI-Implementierung leicht portiert werden.\n\n"}
{"id": "1108228", "url": "https://de.wikipedia.org/wiki?curid=1108228", "title": "SGI Origin", "text": "SGI Origin\n\nOrigin ist der Name einer Familie von Hochleistungsservern von Silicon Graphics (SGI) die mit MIPS R10000, R12000, R14000 und R16000 Prozessoren ausgestattet sind. Je nach Modell kann eine Origin bis zu 2048 Prozessoren haben. Als Betriebssystem wird IRIX eingesetzt.\n\nDas erste Modell, die 1996 eingeführte Origin2000, war zugleich das erste ccNUMA-System von SGI. Es wurde 2000 von der Origin 3000 Linie abgelöst, deren wichtigste Neuerungen eine stärkere Modularisierung und die Ausbaubarkeit bis zu 2048 Prozessoren sind.\n\nDie Origin-Line wurde seit 2003 von der Altix-Linie abgelöst, die bei gleicher Systemarchitektur mit Intel-Itanium-Prozessoren ausgestattet war. Altix-Systeme benutzten statt IRIX ein angepasstes Linux als Betriebssystem.\n\nWird ein System der Origin-Familie mit Grafik ausgestattet, wird diese als Onyx bezeichnet, wobei eine Onyx2 einer Origin2000 und eine Onyx 3000 einer Origin 3000 entspricht. Die Onyx (ohne Kennnummer, auch als \"Onyx 1\" bezeichnet) ist ein älteres, von der Challenge abgeleitetes System.\n"}
{"id": "1108257", "url": "https://de.wikipedia.org/wiki?curid=1108257", "title": "SGI Altix", "text": "SGI Altix\n\nAltix ist der Name einer Familie von Hochleistungsservern von Silicon Graphics (SGI), die mit Intel-Itanium-Prozessoren ausgestattet sind. Je nach Modell kann eine Altix bis zu 4096 Prozessoren haben. Als Betriebssystem wird ein angepasstes Linux eingesetzt.\n\nDie Altix-Linie wurde 2003 mit der Altix 3000 eingeführt. Die Systemarchitektur (ccNUMA) und der mechanische Aufbau sind identisch mit der mit MIPS-Prozessoren ausgestatteten Origin 3000. Über die Zwischenstufe der Altix 3700, die im Wesentlichen eine höhere Integrationsdichte einführte, wurde 2005 die Altix 4700 eingeführt, mit einem ganz neuen Modularisierungskonzept, basierend auf Blades.\n\nWerden Systeme der Altix-Familie mit einem Grafikmodul ausgestattet, werden sie als Prism bezeichnet.\n\n"}
{"id": "1108790", "url": "https://de.wikipedia.org/wiki?curid=1108790", "title": "MacDraw", "text": "MacDraw\n\nMacDraw war eine vektorbasierte Zeichenanwendung, die 1984 für die ersten Macintosh-Computer veröffentlicht wurde. Dabei gilt MacDraw als eine der ersten Anwendungen die eine WYSIWYG-Darstellung in Zusammenhang mit einem anderen Programm ermöglichte. Bei Apple war dies das Zusammenspiel mit MacWrite. MacDraw war eine Layoutanwendung, die insbesondere für wissenschaftliche Zeichnungen, technische Diagramme und Flurpläne benutzt werden konnte. Nachfolgeversionen waren MacDraw II, MacDrawPro und ClarisDraw. Unter dem Betriebssystem Mac OS X lief MacDraw noch in der Classic Umgebung. Auf Macintosh-Rechnern mit Intel-Prozessoren wird Classic nicht mehr unterstützt und damit ist auch MacDraw nicht mehr lauffähig.\n"}
{"id": "1112328", "url": "https://de.wikipedia.org/wiki?curid=1112328", "title": "CrypTool", "text": "CrypTool\n\nCrypTool ist ein Open-Source-Projekt.\nHauptresultat ist die freie Lern-Software CrypTool, die die Konzepte der Kryptographie und der Kryptoanalyse erfahrbar macht. Laut \"Hakin9\" ist CrypTool weltweit die am meisten verbreitete Lern-Software im Bereich Kryptologie.\n\nEs sind über 400 Algorithmen implementiert. Diese können mit eigenen Daten und Parametern effizient ausgeführt werden.\n\nDie grafische Benutzeroberfläche und die umfangreiche Online-Hilfe machen es dem Nutzer (auch „Anfängern“) leicht, kryptologische Verfahren kennenzulernen und zu verstehen. Enthalten sind sowohl die meisten klassischen Verfahren (siehe Geschichte der Kryptographie) als auch moderne Verfahren wie asymmetrische Kryptographie (RSA, ECC), digitale Signaturen, Hybrid-Verschlüsselung, homomorphe Verschlüsselung oder das Diffie-Hellman-Schlüsselaustauschverfahren. Auch Verfahren aus dem Bereich der Quantenkryptographie (BB84-Schlüsselaustausch-Protokoll) und der Post-Quanten-Kryptographie (McEliece, WOTS, Merkle-Signaturen MSS, eXtended Merkle Signature Scheme XMSS und SPHINCS) sind enthalten. Viele Verfahren (bspw. Huffman-Code, AES, Keccak, MSS) sind visualisiert.\n\nZusätzlich sind darin integriert: didaktische Spiele (wie der Zahlenhai, das Teilerspiel oder Zudo-Ku) und interaktive Anleitungen (die bspw. in die Primzahlen, in die elementare Zahlentheorie und Gitter-basierte Kryptographie einführen).\n\nVerwendung findet CrypTool sowohl in der Lehre an Hochschulen, im Unterricht an Schulen als auch in der Ausbildung in Firmen und Behörden oder in Fortbildungskursen wie TISP oder CISSP.\n\nDie CrypTool-Software wird in einem Open-Source-Projekt entwickelt. Ursprünglich wurde sie von deutschen Firmen und Hochschulen entwickelt, inzwischen kommen die rund 70 Mitwirkenden aus der ganzen Welt. Zuwendungen in Form von Software-Plugins kamen z. B. von Universitäten oder Schulen aus den folgenden Städten: Belgrad, Berlin, Bochum, Brisbane, Darmstadt, Dubai, Duisburg-Essen, Eindhoven, Hagenberg, Jena, Kassel, Klagenfurt, Koblenz, London, Madrid, Mannheim, San Jose, Siegen, Utrecht, Warschau.\n\nDas Ziel von CrypTool ist, das Verständnis der Benutzer für Kryptologie zu erhöhen, die darunterliegenden Konzepte zu erläutern und die Benutzer für IT-Bedrohungen zu sensibilisieren.\n\nDas Programm CrypTool 1 ist in Deutsch, Englisch, Polnisch, Spanisch, Serbisch und Französisch verfügbar. CrypTool 2 ist in Deutsch, Englisch und Russisch verfügbar. Alle anderen Programme im CrypTool-Projekt (JCrypTool und CrypTool-Online) sind nur in Deutsch und Englisch verfügbar.\n\nCrypTool wurde international als E-Learning-Software ausgezeichnet: TeleTrusT Special Award 2004, EISA 2004, IT Security Award NRW 2004 und Ausgewählter Ort 2008 in „Deutschland – Land der Ideen“ 2008.\n\nWeltweit wird das Paket zurzeit allein von der CrypTool-Seite rund 10.000-mal pro Monat heruntergeladen (knapp über 50 % laden die englische Version).\n\nCrypTool wird seit 1998 entwickelt.\n\nAls herunterladbare und offline ausführbare Programme sind verfügbar:\n\n\n\n\nNeben den genannten produktiven CrypTool-Projekten gab es ausserhalb des eigentlichen CrypTool-Projekts kurze Zeit das Projekt CrypTooLinux: Damit sollte CrypTool 1.x auf Linux portiert werden, aber den Entwicklern dieses Teilprojektes fehlte die dazu notwendige Zeit, und sie bezogen auch keine neuen Entwickler mit ein, so dass CrypTooLinux seit 2008 im Alpha-Stadium ruht.\n\nDie zwei Projekte CT2 und JCT sind seit 2007 die Nachfolger von CT1 und stellen als Pure-Plugin-Anwendungen regelmäßig neue stabile Versionen bereit.\n\nNeben den offline ausführbaren Programmen werden im CrypTool-Projekt auch die beiden im Folgenden aufgeführten Webportale gepflegt.\n\nZum CrypTool-Projekt gehört seit 2009 auch die Webseite CrypTool-Online, die Interessierten die Möglichkeit bietet, Verschlüsselungsverfahren gleich im Browser auf dem PC oder dem Smartphone (mittels JavaScript) auszuprobieren, ohne Software herunterzuladen und installieren zu müssen. Hier wird versucht, das Thema für Einsteiger und junge Leute ansprechend und einfach aufzubereiten. Eine große Nutzergruppe auf CTO sind Geocacher.\n\nEbenfalls zum CrypTool-Projekt gehört der 2010 gestartete internationale Krypto-Wettbewerb MTC3, der aktuell über 200 Aufgaben rund um (alte und neue) Verschlüsselungsverfahren anbietet. Außerdem enthält die MTC3-Webseite ein moderiertes Forum, Benutzerstatistik und eine Hall-of-Fame. Die Aufgaben reichen von einfachen Rätseln für Einsteiger bis zu mathematischen Herausforderungen aus der modernen Kryptoanalyse für Forscher und Experten. Inzwischen engagieren sich über 9000 registrierte Benutzer an der Lösung der Aufgaben. Dabei wurden dort auch bisher ungelöste Rätsel wie das verschlüsselte Tagebuch des italienischen Partisanen Antonio Marzi oder die Doppelwürfel-Challenge von Otto Leiberich geknackt.\n\nCrypTool wird ebenfalls eingesetzt, um Schüler für MINT-Fächer zu motivieren (Schülerkrypto) als auch in Awareness-Veranstaltungen für jedermann (Anti-Prism-Party).\n\n\n\n "}
{"id": "1112350", "url": "https://de.wikipedia.org/wiki?curid=1112350", "title": "Front Row", "text": "Front Row\n\nFront Row ist eine Software von Apple, die Mediencenter-Funktionalitäten unter Mac OS X bietet. Das Programm war von 2005 bis 2011 auf allen Macs vorinstalliert. Ebenfalls basierte die erste Softwareversion von Apple TV auf Front Row, wurde inzwischen jedoch durch eine eigenständige Entwicklung abgelöst.\n\nDas Programm Front Row kann über die Tastatur oder über die spezielle Fernbedienung Apple-Remote gesteuert werden. Die Apple-Remote wird auch nach der Einstellung von Front Row weiter hergestellt und ist inzwischen in einer zweiten Generation erneuert worden.\n\nAm 12. Oktober 2005 stellte Apple-CEO Steve Jobs den iMac G5 (mit integrierter iSight-Kamera) zusammen mit der Software Front Row vor, sowie einige weitere Neuerungen in Apples Entertainment-Bereich (iPod mit Video-Fähigkeiten, iTunes 6 und die Apple-Remote-Fernbedienung).\nSeitdem kann man im iTunes Store nicht nur Musik, sondern auch Musikvideos sowie Kurzfilme von Pixar (mittlerweile auch Spielfilme, und auch von anderen Filmgesellschaften) kaufen.\n\nAm 10. Januar 2006 wurde neben dem neuen iMac (mit Intel-Prozessor) auch der Nachfolger des PowerBooks vorgestellt, das MacBook Pro, welches ebenfalls Front Row unterstützt.\n\nAm 26. Oktober 2007 wurde zusammen mit dem Betriebssystem Mac OS X Leopard 10.5 eine neue Version von Front Row eingeführt. Es kann jetzt als Programm gestartet (/Applications/Front Row.app) und im Dock abgelegt werden. Außerdem hat sich das Design verändert, es entspricht nun in großen Teilen der Apple-TV-Oberfläche „Take 2“.\n\nAm 20. Juli 2011 wurde Front Row mit der Veröffentlichung des Betriebssystem Mac OS X Lion 10.7 ersatzlos aus dem System entfernt. Es ist nicht mehr Bestandteil des Betriebssystems und auch über den App Store nicht erhältlich. Auf älteren Betriebssystemen (z. B. Snow Leopard) läuft Front-Row weiterhin, wird jedoch nicht mehr weiterentwickelt. Durch Kopieren des Programms aus älteren Betriebssystem-Versionen kann Front Row auch unter Mac OS X Lion installiert werden.\n\nIn Front Row können Musik, Filme, Fernsehsendungen, Podcasts, Fotos und sofern der Mac mit dem Internet verbunden ist, Filmtrailer angesehen werden (je nach Verbindung und Verfügbarkeit auch in High Definition).\n\n\nApple stellte am 12. Oktober 2005 auch die erste Generation der Apple Remote, einer Infrarot-Fernbedienung mit nur sechs Tasten (oben, unten, links, rechts, auswählen und Zurück (beschriftet als „MENU“)), vor, mit der man Front Row steuern kann.\n\nAnfänglich lag die Apple Remote serienmäßig allen iMac und MacBook Modellen bei. Seit September 2008 ist das nur noch bei Apple TV und beim Apple Universal Dock der Fall.\n\nDie Apple Remote der ersten Generation wurde 2006 mit dem red Dot Design Award ausgezeichnet.\n\nDie zweite Generation von Apple Remote, vorgestellt am 20. Oktober 2009, besitzt eine zusätzliche Play/Pause-Taste, die von Front Row jedoch als Auswählen erkannt wird. Das Gehäuse ist jetzt nicht mehr aus Plastik, sondern aus einem Aluminiumblock gefräst nach dem Unibody-Design. Bei beiden Generationen dient eine wechselbare CR2032-Knopfzelle als Batterie.\n\nFront Row kann mit der MENU-Taste gestartet werden.\n\nMan kann Front Row auch ohne die Apple Remote steuern. Das Starten erfolgt dann durch die Tastenkombination + . Danach kann Front Row durch die Pfeiltasten, Return und Escape gesteuert werden.\n\nFront Row kann auch mithilfe diverser Remote-Apps gesteuert werden. Dazu müssen sich das Apple iPhone bzw. der iPod touch im selben Netzwerk wie der Mac befinden.\n\n"}
{"id": "1112401", "url": "https://de.wikipedia.org/wiki?curid=1112401", "title": "Microsoft Publisher", "text": "Microsoft Publisher\n\nMicrosoft Publisher ist eine Desktop-Publishing-Software, die das Erstellen von Druckpublikationen, E-Mail-Headlines und Produktpräsentationen ermöglicht. Die Software ist neben Programmen wie Word, PowerPoint oder Excel im Office-Paket von Microsoft enthalten. Sie erreicht nicht die Komplexität von Adobe InDesign, QuarkXPress oder Affinity Publisher. Die Dateinamenserweiterung für mit Publisher erzeugte Dateien lautet \"pub\".\n\nInhalte lassen sich durch Vorlagen im WYSIWYG-Verfahren schnell und einfach veröffentlichen. Dafür stehen Designsets zur Auswahl. Auch Grußkarten und Einladungen können erstellt und bearbeitet werden.\n\nMit dem Publisher können bis zur Version 2007 Websites erstellt, aber keine HTML-Dateien geöffnet werden. Nur mit Publisher erstellte Websites können mit dem Publisher wieder geöffnet werden, erhalten aber auch die übliche HTML-Endung. Die Funktionen zur Erstellung von Webseiten wurden mit Version 2010 entfernt.\n\n"}
{"id": "1112688", "url": "https://de.wikipedia.org/wiki?curid=1112688", "title": "NetBus", "text": "NetBus\n\nNetBus ist ein Fernwartungstool für Microsoft Windows, das meist illegal eingesetzt wird.\n\nIn seiner Funktionalität entspricht es Back Orifice, verfügt allerdings über mehr Möglichkeiten. Es wurde von dem Schweden Carl-Fredrik Neikter in Delphi geschrieben, der die erste Version Mitte März 1998 veröffentlichte. Derzeit liegt das Programm in den Versionen 1.60, 1.70 und als NetBus 2.01 Pro vor.\n\nNetBus besteht wie alle Fernwartungsprogramme aus einem Client und einem Server-Programm, welches zur Kontrolle des entfernten Rechners eingesetzt wird. Die benutzten Ports sind ab der Version 1.7 variabel konfigurierbar. Die Version 1.6 benutzt einen festgelegten Port.\n\nNetBus bietet auf dem installierten Rechner, auf dem der Server installiert wurde, folgende Funktionen:\n\n\n\n"}
{"id": "1113821", "url": "https://de.wikipedia.org/wiki?curid=1113821", "title": "SPBLinux", "text": "SPBLinux\n\nSPBLinux (für St. Petersburg, den Entstehungsort der Software) ist eine extrem kleine Linux-Distribution (je nach Softwareauswahl im einstelligen Megabyte-Bereich), die (bei Installation von einigen Zusatzpaketen, insgesamt knapp 10 MB) sogar eine grafische Benutzeroberfläche mit Webbrowser beinhaltet. Aufgrund seiner Größe ist SPBLinux hauptsächlich für Rettungs-Disketten oder USB-Sticks geeignet. Es gibt zahlreiche Zusatzpakete für unterschiedlichste Anwendungen, zum Beispiel für die Kommunikation mit PDAs und Mobiltelefonen.\n\nSPBLinux wird vom Schweizer Informatiklehrer Christian Ostheimer entwickelt. Die erste, nach Ostheimers Studienort St. Petersburg benannte Version wurde im Mai 2000 veröffentlicht. Version 2 folgte Ende 2003 und wurde in ebenso großen Abständen weiter entwickelt. Die jüngste verfügbare Version 2.2 von Ende 2008 ist zwar nach wie vor als Vorveröffentlichung aber auch als stabil gekennzeichnet.\n\nMit SPBLinux wird ein Toolkit mitgeliefert, das ein Booten von einem USB-Stick ermöglicht. Fast alle anderen Kleindistributionen (wie zum Beispiel Damn Small Linux, etwa 50 MB) verwenden für den USB-Boot den SPBLinux-Bootsektor. SPBLinux bringt einen eigens entwickelten Bootmanager mit, den sogenannten „Smart Boot Manager“. Auch für DOS-basierte Systeme ist der Einsatz möglich. Der Smart Boot Manager ist im ersten Block mit dem SPBLinux-Bootsektor an dessen Anfang untergebracht. Indem er Logical-Block-Adressierung statt Cylinder-Head-Sector-Adressierung verwendet, behebt er einige Probleme aufgrund der Unterschiede zwischen der Plattengeometrie, mit der Windows mit dem USB-Stick kommuniziert und der, mit der das BIOS beim Boot auf den Stick zugreift.\n\nDer extrem sparsame Gebrauch von Speicherplatz wird in dieser Linux-Distribution unter anderem auch dadurch möglich, dass anstelle des schwergewichtigen X-Window-Systems auf ein Client-Server-System verzichtet wird und die grafische Benutzeroberfläche samt Grafikanwendungen auf dem Framebuffer ausgeführt wird. Dies geschieht, indem die in C geschriebene Softwarebibliothek \"DirectFB\" genutzt wird.\n\n"}
{"id": "1115104", "url": "https://de.wikipedia.org/wiki?curid=1115104", "title": "Electronic Delay Storage Automatic Calculator", "text": "Electronic Delay Storage Automatic Calculator\n\nDer EDSAC \"(Abkürzung für Electronic Delay Storage Automatic Calculator)\" war ein britischer Hochleistungsrechner, der im Jahr 1949 fertiggestellt wurde. Der Röhrencomputer wurde von Maurice V. Wilkes und seinem Team in Cambridge entwickelt.\n\nIn dem Rechner wurden einige damals neue Konzepte eingesetzt, so wurde beispielsweise erstmals die Von-Neumann-Architektur implementiert. Außerdem war er der weltweit erste Computer, der gespeicherte Programme nutzte. Der EDSAC basierte auf Neumanns Rechner EDVAC. Es gab eine kommerzielle Version mit dem Namen LEO 1 (Lyons Electronic Office).\n\nAuf ihm lief auch 1952 das wohl erste Computerspiel namens OXO, eine Art von Tic-Tac-Toe.\n\n"}
{"id": "1115621", "url": "https://de.wikipedia.org/wiki?curid=1115621", "title": "Krita", "text": "Krita\n\nKrita (ehemals \"KImageshop\" und \"Krayon\") ist ein freies Malprogramm von KDE. Es war einige Zeit Teil des Office-Pakets KOffice und ist ab Version 2.4 Teil der Calligra Suite. Krita ist hauptsächlich als Malprogramm konzipiert, enthält aber auch Funktionen zur Bildbearbeitung.\nKrita steht für die Betriebssysteme Windows, Linux und macOS zur Verfügung.\n\nNachdem Versuche fehlgeschlagen waren, eine auf Qt basierende KDE-Variante von GIMP zu entwickeln, schlug Matthias Ettrich am 24. Mai 1999 vor, ein Bildbearbeitungsprogramm von Grund auf neu zu entwickeln.\n\nIm Januar 2002 musste dann der ursprüngliche Name KImageshop auf Drängen eines Anwalts geändert werden – das Projekt nannte sich fortan Krayon. 2003 verlief die Entwicklung nur schleppend, bis sich im Oktober eine neue Gruppe von Entwicklern zusammenfand, welche die Entwicklung des Programms unter dem Namen Krita (schwedisch für Kreide) fortführte.\n\nAm 21. Juni 2005 wurde die erste Version von Krita zusammen mit KOffice 1.4 veröffentlicht.\n\nSeit dem Jahr 2014 wird die Entwicklung durch jährliche Spendenkampagnen auf Kickstarter.com finanziert, beginnend mit Version 2.9. So konnten vollzeitlich arbeitende Mitarbeiter angestellt werden, die an umfangreichen Performance-Optimierungen arbeiten.\n\nAb Version 1.5 unterstützt Krita die Farbräume RGB (8 bit, 16 bit und 32 bit), LAB (16 bit), Grayscale (8 bit und 16 bit) und auch CMYK (8 bit und 16 bit). Auch unterstützt Krita das OpenEXR-Format nativ und kann für die Bearbeitung von HDR-Bildern in diesem Format eingesetzt werden.\n\n"}
{"id": "1118391", "url": "https://de.wikipedia.org/wiki?curid=1118391", "title": "Quorum (Informatik)", "text": "Quorum (Informatik)\n\nUnter einem Quorum oder einer Voting Disk versteht man eine Komponente des Cluster Managers eines Computerclusters zur Wahrung der Datenintegrität im Fall eines Teilausfalls. Bei Ausfall des Cluster Interconnects (der Verbindung zwischen den Clusterknoten) besteht das Risiko einer Aufspaltung des Gesamtsystems in unerwünschterweise autonom agierende Einheiten, die fast immer die Datenintegrität bedroht (Split-Brain-Problem). Durch wechselweises oder konkurrierendes Schreiben in die logische Struktur der Voting Disk wird im Falle eines unterbrochenen Interconnects entschieden, welcher Teil des Clusters überleben soll. Die Voting Disk liegt auf Shared Storage.\n\nEin Beispiel für den Fall des Oracle RAC, es überlebt:\n\n\nEine derartige Unterscheidung nach einem Ausfall des Interconnects als Kommunikationskanal wäre ohne eine „Abstimmung“ auf Massenspeicher unmöglich. Da fast alle Clustermanager auf einen Ausfall der Zwischenverbindung mit dem Neustart mindestens eines Knotens reagieren, ist auch die persistente Speicherung des Clusterstatus in der Voting Disk von Vorteil: Es entfällt ein Gutteil der Neuverhandlungen über Verfügbarkeiten und Masterstatus. Diese Verhandlungen bedingen ohne die persistente Voting Disk oft mehrere Neustarts. Damit steigt bei Verwendung eines Quorums die Verfügbarkeit der Einzelknoten durch entfallende Neustartzyklen.\n\nDie Voting Disk selbst ist – sobald sie verwendet wird – integraler Bestandteil des Clusters. Ist ein vormals verfügbares Quorum während des Clusterbetriebes plötzlich nicht mehr greifbar, fällt das gesamte System aus. Dies gilt natürlich insbesondere bei Ausfall einer einzelnen Shared Storage. Den damit entstandenen Single Point of Failure zu vermeiden, ist derzeit Bestrebung aller Hersteller von Clusterware.\n\nDer gebräuchliche Ansatz zur Lösung dieser Probleme ist die Spiegelung der Voting Disk über mehrere physische Medien. Dabei zeigen sich jedoch andere Probleme:\n\nDie Königslösung für Konsistenz-, Latenz- und Verfügbarkeitsprobleme stellt die (u. U. sehr teure) storageseitige Replikation im Storage Area Network (SAN) dar. Sie präsentiert allen Cluster-Membern transparent ein einziges repliziertes Gerät und entlastet dadurch Clusterware, Cluster-Member und Administratoren.\n"}
{"id": "1120309", "url": "https://de.wikipedia.org/wiki?curid=1120309", "title": "Techno-Kunst", "text": "Techno-Kunst\n\nUnter dem Begriff Techno-Kunst (auch \"Techno-Art\") werden Kunstformen zusammengefasst die in direktem Zusammenhang zur Technoszene stehen. Dabei handelt es sich oft um Zeichnungen, Grafiken oder Animationen und Covergestaltungen, so wie futuristischen Metallfiguren, deren Künstler sich mit der Szene identifizieren und das entsprechende Lebensgefühl mit ihren Werken zum Ausdruck bringen möchten.\n\nEine frühe und weit verbreitete Form der Techno-Kunst ist der Flyer, mit dem für Partys und andere Veranstaltungen geworben wird.\n\nGroße Bedeutung haben auch Dekorationen, Projektionen und Rauminstallationen auf Raves und Techno-Veranstaltungen bekommen. So erlangten beispielsweise Berliner Künstler wie \"Gecco\" oder \"Skudi Optics\" Bekanntheit durch ihre experimentellen und großflächigen Projektionen.\n\nAls Techno langsam aus dem Underground hervortrat und die Labels sich Gedanken um den kommerziellen Vertrieb machten, entstanden Anfang der 1990er Jahre die ersten computeranimierten Musikvideos, zu deren Vorreitern die 3Lux-Serie sowie deren Nachfolger X-Mix zählen.\n\nDie erste, große Ausstellung, die sich ausschließlich mit Techno-Kunst befasste, war die Chromapark im E-Werk in Berlin.\n\nDer Gestalten-Verlag aus Berlin veröffentlichte bereits Anfang der 1990er Jahre Bücher, die sich im weitesten Sinne mit Techno-Art befassten.\n\nUnter den Szenemagazinen nahm die Zeitschrift Frontpage und ihr damaliger Grafiker Alexander Branczyk eine Vorreiterrolle bezüglich Gestaltung und Layout ein.\n\nIn den Anfängen war Techno-Kunst noch von bunten Collagen, naiven Malereien, einfachen geometrischen 3-D-Objekten sowie Texten in teilweise schwer lesbarem Typofreistil geprägt. Gerne wurden auch ironisch zu verstehende Fotografien aus früheren Jahrzehnten verwendet, um sich als revolutionäre, neue Kultur abzugrenzen. Mit der technischen Entwicklung und dem Wandel des Lebensgefühls innerhalb der Szene, weg von der „höher-schneller-weiter-Mentalität“, konzentrierte sich die künstlerische Darstellung zunehmend auf minimalistisches und farblich reduzierteres Design, sowie realistischere 3-D Animationen.\n\nIn der Freetekno-Bewegung hingegen werden teils auch heute noch bewusst einfache Flyer erstellt, die häufig mit den Farben schwarz und weiß spielen und psychedelische Muster oder bis nahe an die Unkenntlichkeit verschnörkelte Darstellungen ergeben, in denen dann mehr oder weniger versteckt Informationen zur beworbenen Veranstaltung enthalten sind. Gezeichnet werden meist Motive mit Bezug zu Tekno-Partys an sich oder fiktive Maschinen und Roboter, die in der Regel einen düsteren bis dämonischen Eindruck vermitteln. Typische, immer wiederkehrende Elemente, sind Gasmasken, Totenköpfe, mutierte bis morbide Gestalten und technisches Equipment (in der Regel Boxen und Plattenteller). Die Hintergründe sind üblicherweise dunkel und sollen einen mechanisch-industriellen Eindruck vermitteln. Solche Muster und Grafiken werden in vielfacher Ausprägung auch auf Tücher gezeichnet, die auf den Veranstaltungen als Dekorationsmittel eingesetzt werden, oder auf Kapuzen-Pullover gedruckt.\n\nIn der eng mit Techno verwandten Goa- und Psytrance-Szene etablierte sich bereits vor den 1990er Jahren unter dem Einfluss psychoaktiver Drogen eine ganz eigene Kunstform auf Veranstaltungen, Flyern und Musik-Covern. Die künstlerischen Darstellungen beziehen sich meist auf Natur, Spiritualität, farbenfrohe, psychedelische Muster oder Fraktale und ethnisch-religiöse Symbole, oft aus dem Hinduismus, Shivaismus oder Buddhismus und zeigen einen engen Bezug zur Hippie-Bewegung.\n\nGoa-Partys finden überwiegend als Freiluftveranstaltungen statt und sind oft dekoriert mit liebevoll gebastelten halluzinogenen Pilzen aus Pappmaché, 2- oder 3-dimensionalen Objekten aus gewebten Fäden oder bunten Batiktüchern.\n\nHäufig sind die Objekte in fluoreszierenden Farben gehalten und leuchten unter der Bestrahlung mit Schwarzlicht.\n\n\n"}
{"id": "1121559", "url": "https://de.wikipedia.org/wiki?curid=1121559", "title": "Cluster Interconnect", "text": "Cluster Interconnect\n\nDer Cluster Interconnect dient für Management-, Datenübertragungs- und Lastverteilungszwecke in einem Computercluster und ist eine Komponente des Cluster Managers. In der Literatur wird der Cluster Interconnect manchmal auch sinnreduzierend als Cluster Heartbeat bezeichnet, wobei diese Funktion des „Herzschlages“ nur eine von mehreren ist.\n\nDer Interconnect eines Clusters ist eine Verbindung zwischen den Cluster-Membern, über die alle Arten von relevanten Daten ausgetauscht werden. Sie wird in aller Regel als privates Netzwerksegment konzipiert, um höchstmögliche Betriebssicherheit zu gewährleisten oder auch andere Netzwerkarchitekturen abbilden zu können. Denn diese Zwischenverbindung ist – in Abhängigkeit von der Größe des Clusters – mit möglichst geringer Latenz und hoher Übertragungsrate auszustatten, um keinen Flaschenhals zu schaffen. Dazu häufig verwendete Technologien sind Gigabit-Ethernet oder auch das teurere InfiniBand.\n\nWie eingangs erwähnt, dient der Cluster Interconnect mehreren Zwecken:\n\nUnter \"Heartbeat\" (\"dt. „Herzschlag“\") versteht die Computertechnik ein periodisch zwischen zusammengehörigen Systemen ausgetauschtes Signal, analog dem Fühlen des menschlichen Pulses. Dieses Signal dient zur gegenseitigen Überwachung: Bleibt der Heartbeat eines Systems aus, wird ein Ereignis ausgelöst auf welches die anderen entsprechend reagieren können. Der Cluster Interconnect dient hier der reinen Signalübertragung. Wird die gegenseite Verfügbarkeitsüberwachung nicht durch eine kommerzielle Clusterware realisiert, stehen auch Software-Pakete wie LinuxHA für die Absicherung einzelner Dienste bereit.\n\nFür einen reinen Heartbeat-Interconnect wurde bis vor einiger Zeit auch oft nur ein serielles Kabel verwendet.\n\nEine weitere wesentliche Aufgabe des Interconnects ist das Management des Clusters. Beispielsweise muss das gezielte Umstellen eines Dienstes von einem Knoten auf den anderen koordiniert und mit möglichst geringer Ausfallzeit geschehen. Jede Clusterware verwendet dazu produktspezifisch standardisierte Telegramme von unterschiedlicher Größe.\n\nBei Nutzung des Clusters für die Lastverteilung bei Applikations- oder Datenbankservern ist es darüber hinaus wichtig, in bestimmten Situationen auch Benutzerdaten über den Cluster Interconnect weiterzugeben. Auch hier werden durch die Clusterware spezielle Telegramme mit dem entsprechenden Nutzinhalt übertragen, und für die darüberliegende Applikation eine API angeboten.\n\nBeispiel: Oracle RAC 10g verwendet die sogenannte Cache-Fusion-Technologie. Sie baut darauf auf, dass schon einmal in den Cache eines Clusterknotens eingelesene Blöcke über den Interconnect schneller beschafft werden können als durch erneutes Einlesen vom Massenspeicher. Auch setzt die Hoheitskontrolle für Datenbank-Blöcke dieses Datenbankmanagementsystems auf das hier \"Blockshipping\" genannte Übertragen der Daten über die Zwischenverbindung.\n\n\n\n"}
{"id": "1123288", "url": "https://de.wikipedia.org/wiki?curid=1123288", "title": "Efika", "text": "Efika\n\nEfika (esperanto für \"effizient\") ist eine Gruppe von Einplatinencomputern des Herstellers Genesi. Der erste basierte auf der PowerPC-Architektur, der Efika MX basiert auf einem Freescale i.MX515 SoC (ARM-Architektur).\n\nGenesi adressiert mit den Efika-Boards den Markt für Thin Clients, Unterhaltungselektronik und eingebetteten Systeme.\n\nDer Efika 5200B basiert auf dem CHRP-Standard und ist für Betriebssysteme wie Linux, MorphOS, Symobi und einige mehr geeignet. Im Gegensatz zu vergleichbaren Systemen bietet er eine leistungsfähige Implementierung der Open Firmware.\n\n\n"}
{"id": "1124287", "url": "https://de.wikipedia.org/wiki?curid=1124287", "title": "CIO (Magazin)", "text": "CIO (Magazin)\n\nCIO – IT-Strategie für Manager ist ein monatlich in der International Data Group (IDG) Business Media GmbH erscheinendes IT-Wirtschaftsmagazin, das sich hauptsächlich mit den wirtschaftlichen und strategischen Aspekten des Informationstechnik-Einsatzes in Unternehmen beschäftigt. Benannt ist es nach der Rolle des Chief Information Officer (IT-Leiter eines Unternehmens).\n\nDie Erstausgabe des Magazins erschien im Oktober 2001. CIO zielt auf Führungskräfte aller Branchen, die über den strategischen IT-Einsatz in ihren Unternehmen entscheiden. Exemplarisch zeigt das Magazin, wie IT in Unternehmen implementiert werden kann. Darüber hinaus will CIO wichtige Technologietrends und Management-Methoden vermitteln. Chefredakteur des Magazins ist Heinrich Vaske. Redaktionsmitglieder sind Wolfgang Herrmann, Rolf Röwekamp und René Schmöl.\n\nCIO erscheint derzeit in zwölf Ländern: Australien, China, Deutschland, Frankreich, Indien, Japan, Kanada, Südkorea, Neuseeland, Polen, Schweden und den USA.\n\nDas Magazin kürt seit 2003 zusammen mit der Computerwoche einen „CIO des Jahres“. Ferner wurde 2016 erstmals gemeinsam mit der CIO-Stiftung und der WHU – Otto Beisheim School of Management der CIO Executive Award vergeben, mit dem „herausragende Potentialträger“ geehrt werden sollen. Der Gewinner 2016 war Hans-Martin Hellebrand, Gründer und CFO von Humada Inc., eines in den USA ansässigen IT-Unternehmens.\n\nIm vierten Quartal 2009 lag die durchschnittliche monatlich verbreitete Auflage nach IVW bei 14.427 Exemplaren. Das sind 23,79 Prozent (= 4.503 Hefte) weniger Hefte als im Vergleichsquartal des Vorjahres. Der Verkauf von Abonnements stagnierte in den Jahren 2007 bis 2009.\n\nDas CIO-Magazin wird primär als Controlled Qualified Circulation vertrieben.\n\nIm Internet bietet CIO Online weiterführende Informationen und spezielle Angebote über das Magazin hinaus. Das „CIO-Netzwerk“ ist nicht nur das Alleinstellungsmerkmal der Website, sondern auch mit über 13.300 Usern das größte soziale Netzwerk für CIOs in Deutschland. Insgesamt elf Knowledge-Center angefangen von „Business Intelligence“ über „ERP“ bis „Storage“ ordnen die Online-Beiträge thematisch. In der Top-500-Datenbank gibt es detaillierte IT-Informationen über die 500 größten deutschen Unternehmen ab 1 Mrd. Euro Umsatz pro Jahr.\n\nDas CIO-Jahrbuch „Die IT-Fakten der größten Unternehmen Deutschlands“ erscheint jährlich. Das Nachschlagewerk bündelt die Recherche-Ergebnisse der CIO-Redaktion und ermöglicht einen Einblick in die wichtigsten IT-Kennzahlen der Branchenführer.\n\nWeiterhin veranstaltet CIO zielgruppenspezifische Kongresse und Fachveranstaltungen wie etwa die \"Hamburger IT-Strategietage\", der IT Excellence Benchmark oder die \"Executive Lounge\".\n\n"}
{"id": "1124758", "url": "https://de.wikipedia.org/wiki?curid=1124758", "title": "IMatch", "text": "IMatch\n\nIMatch (engl. \"Match Image Management\") ist eine Software zur Bilderverwaltung für Windows. Bilder lassen sich importieren und in Kategorien einordnen. Die erste Version von IMatch erschien 1998.\n\nIMatch verwaltet über 100 Dateiformate. Rohdatenformate werden über WIC (Windows Imaging Component) Codecs bzw. in IMatch implementierte Routinen gelesen. IMatch unterstützt alle relevanten Metadatenformate, wie unter anderen: IPTC, Exif, GPS, XMP, PDF, ID3. Die meisten Formate können sowohl gelesen als auch geschrieben werden. Es implementiert die Vorgaben der Metadata Working Group und des IPTC Komitees. Dies garantiert einen reibungslosen Austausch von Metadaten mit anderen Anwendungen und Diensten.\n\nDas Programm richtet sich mit seinem Funktionsumfang und der Möglichkeit, Archive mit mehreren hunderttausend Bildern sowohl auf lokalen wie auch auf entfernbaren Speichermedien verwalten zu können, vor allem an anspruchsvolle Amateurfotografen, Profis und Anwender in Firmen, der Forschung und in Behörden.\n\nIMatch hat eine eingebaute Versionsverwaltung und kann Dateien stapeln. Änderungen an Metadaten oder Schlüsselwörtern können automatisch auf alle Versionen einer Datei angewendet werden.\n\nDie Software bietet des Weiteren eine Ähnlichkeitssuche zum leichteren Finden von Duplikaten, ist mit einer eigenen Scriptsprache erweiterbar und kann vom lizenzierten Benutzer auf beliebig vielen Computern benutzt werden.\n\nÜber eine Vielzahl von Import- und Exportmodulen lassen sich Bilder und Daten importieren und exportieren. Eine leistungsfähige Stapelverarbeitung sowie Web Publishing sind ebenfalls integriert.\n\n"}
{"id": "1125663", "url": "https://de.wikipedia.org/wiki?curid=1125663", "title": "Shorewall", "text": "Shorewall\n\nShorewall (Kurzform für \"Shoreline Firewall\"; benannt nach dem Wohnort seines Autors Thomas M. Eastep in Shoreline, Washington, Vereinigte Staaten) ist ein freier Firewall-Konfigurator, der auf den im Linux-Kernel eingebauten \"netfilter\" aufsetzt.\n\nDie Konfiguration erfolgt in Textdateien (unter anderem in /etc/shorewall/). Aus diesen Dateien kompiliert Shorewall mit Hilfe von iptables-netfilter-Regeln, die den durch den Kernel fließenden IP-Datenstrom regulieren. Shorewall ist kein Daemon, es läuft nicht beständig, sondern beendet sich nach der Erstellung der Regeln. Es steht ein Webmin-Plugin als grafisches Frontend zur Verfügung.\n\nDie von Shorewall erstellten iptables-Regeln wirken in der OSI-Schicht 3, also der Verbindungsschicht, auch wenn es möglich ist, andere Schichten zu kontrollieren.\n\nDie Stärken von Shorewall liegen in der Abstraktion der direkt an den Schnittstellen angebunden Netzwerke, die als „Zonen“ bezeichnet werden. Die Anzahl der Zonen und deren Einsatzzweck können beliebig definiert werden. Für die drei wichtigsten Einsatzfälle bringt Shorewall folgende Konfigurationsvorlagen mit:\n\n\nZwischen den Zonen sind Richtlinien (Policy) zu definieren, die das Standardverhalten zwischen den Zonen festlegen. Diese stellen eine Rückfalllösung für die Verbindungen dar, für die keine expliziten Regeln im Regelwerk (Rules) definiert sind. Shorewall beherrscht auch das Erstellen von NAT, Traffic-Shaping, Bridges und vielem mehr.\n\nShorewall ist eher eine Firewall für den professionellen Einsatz und kann nicht mit einer Personal Firewall (OSI-Schicht 7) verglichen werden.\n\n"}
{"id": "1126639", "url": "https://de.wikipedia.org/wiki?curid=1126639", "title": "Gedit", "text": "Gedit\n\ngedit (Kurzform für GNOME Editor) ist ein UTF-8-kompatibler freier Texteditor für die Desktopumgebung Gnome. Er baut auf den GNOME- und GTK+-Bibliotheken auf. Mit Unterstützung für das virtuelle Dateisystem gio in GLib, GNOMEs Hilfesystem, und Drag and Drop mit anderen GNOME-Programmen integriert er sich in die GNOME-Umgebung. Verschiedene Dateien können in separaten Registerkarten simultan bearbeitet werden.\n\nDie erste Version des Programms erschien im April 1998. Das Ziel war, für das kurz zuvor gestartete GNOME-Projekt einen dazugehörigen Texteditor bereitzustellen. Der Texteditor ist mittlerweile auch für Windows-Nutzer verfügbar.\n\npluma ist ein Fork von gedit. Dieser erfolgte im Zuge der Entwicklung des MATE Desktop Environment, das ein Fork von Gnome 2 ist. Der Name „pluma“ ist das lateinische Wort für „Feder“, und lehnt sich an deren historischer Nutzung als Schreibgerät an.\n\ngedit ermöglicht bereits im Lieferumfang mittels gtksourceview die Syntaxhervorhebung für C, C++, CSS, Java, HTML, XML, Python, Perl, PHP sowie einer Reihe weiterer Programmier- und Scriptsprachen. Zudem ist es möglich, mittels einer XML-formatierten Datei Sprachdefinitionsdateien selbst zu erstellen und solche von Drittentwicklern nachzuinstallieren, wie beispielsweise für die Mediawiki-Wikitext-Syntax.\n\nVon Benutzern und Programmierern werden eine Vielzahl von Plugins bereitgestellt, wie etwa Autovervollständigung für LaTeX-Dokumente oder eine Python-Shell. Im Februar 2010 wurde ein Plugin entwickelt, welches Kollaboratives Schreiben ermöglicht. Es baut dabei auf der gleichen Programmbibliothek auf wie der Texteditor Gobby.\n\nAuf Basis des gedit wurde 2003 der Editor gPHPEdit erstellt, der neben PHP auf HTML und CSS spezialisiert ist.\nDie ersten zwei Stellen der Versionsnummer orientieren sich an GNOME.\n\n"}
{"id": "1126688", "url": "https://de.wikipedia.org/wiki?curid=1126688", "title": "Marching Cubes", "text": "Marching Cubes\n\nMarching Cubes ist ein Algorithmus zur Darstellung von Isoflächen in der 3D-Computergrafik. Er nähert eine Isofläche durch eine Polygongrafik an.\n\nMarching Cubes wurde 1987 von William E. Lorensen und Harvey E. Cline als Ergebnis einer Forschungsarbeit für die Forschungsabteilung des Unternehmens General Electric in der Zeitschrift Computer Graphics vorgestellt. Lorensen und Cline beschäftigten sich mit der effizienten Visualisierung von Bilddaten bildgebender Verfahren wie Computertomografie, Magnetresonanztomografie und Single-Photon-Emissionscomputertomographie.\n\nIn der 3D-Computergrafik gibt es verschiedene Methoden, dreidimensionale Objekte zu modellieren. Eine davon ist die Modellierung als polygonale Oberfläche („Drahtgittermodell“): Man fügt eckige Flächen – in der Regel Dreiecke – so aneinander, dass sie die Oberfläche des Objektes nachbilden. Diese Modelle können sehr schnell und einfach in Bilder umgesetzt werden, der Speicherbedarf eines Modells ist relativ gering und durch zahlreiche Raffinessen sind sehr realistische Bilder möglich. Andererseits ist es fast unmöglich, auf diese Weise ein Medium wie Nebel zu modellieren, welches keine klar umrissene Oberfläche aufweist.\n\nEine andere Methode ist die Modellierung als Voxel-Datenmenge: In regelmäßigen Abständen wird an einem einzelnen Punkt aus dem Objekt die Dichte des Materials abgelesen; das Ergebnis ist ein würfelförmiges dreidimensionales Gitter aus Voxeln. Bildgebende Systeme wie die Computertomografie erzeugen von Natur aus solche Modelle. Diese Voxel-Modelle können recht einfach in Bilder umgesetzt werden, die zudem sehr realistisch und detailgetreu wirken. Allerdings benötigt das Modell sehr viel Speicherplatz – in Größenordnungen von mehreren hundert Megabyte bis einigen Gigabyte – und die Visualisierung dauert erheblich länger als bei einem polygonalen Oberflächenmodell vergleichbarer Größe. Zudem sind Manipulationen (zum Beispiel das Deformieren eines Objektes) deutlich schwieriger, teils sogar unmöglich.\n\nMarching Cubes ermöglichte es erstmals, unpraktikable Volumen-Modelle durch praktikable polygonale Oberflächen-Modelle anzunähern, um sie anschließend effizient zu visualisieren. Der Algorithmus befriedigte damit den Wunsch der Radiologie nach einem Verfahren, Daten bildgebender Systeme wie der Computertomografie schnell und aussagekräftig bildlich darzustellen. Auch heute noch ist Marching Cubes als effizienter Transformationsalgorithmus im Einsatz. Zwar hat die Volumengrafik in der Zwischenzeit Fortschritte gemacht und durch 3D-Texturen Eingang in die Computergrafik-Praxis gefunden, jedoch gibt es bisher keine Hardware, welche die Volumengrafik auf ähnliche Weise beschleunigt, wie dies Grafikprozessoren mit Dreiecken tun.\n\nDie Idee von Marching Cubes ist es, das gegebene Voxelmodell eines Objekts zunächst in kleine Würfel (cubes) zu zerlegen und anschließend von einem Würfel zum nächsten zu „marschieren“ und zu bestimmen, wie die Oberfläche des Objekts den jeweiligen Würfel durchschneidet. Dazu wird ein vom Benutzer gewählter Grenzwert verwendet, um zu bestimmen, welche Teile der einzelnen Würfel innerhalb und welche außerhalb des letztendlichen Objekts liegen. Durch Veränderung dieses als „Dichte“ interpretierten Grenzwerts kann der Benutzer bestimmen, welche Bereiche des Objekts dargestellt werden und welche nicht.\n\nDer Algorithmus verarbeitet folgende Eingabeparameter:\n\nDie folgenden Datenstrukturen werden vom Algorithmus verwendet oder erzeugt, aber nicht als Eingabeparameter übergeben:\n\n\n\nDer oben dargestellte Algorithmus für Marching Cubes ist sehr rudimentär. Er nutzt beispielsweise nicht aus, dass bereits berechnete Informationen wieder verwendet werden können: Teilen sich zwei benachbarte Kuben eine Kante, so müssen darauf liegende Knoten nur einmal interpoliert werden; der Nachbar kann die bereits gefundenen Knoten einfach übernehmen.\n\nDa die Laufzeit des Algorithmus nur von der Anzahl der betrachteten Kuben abhängig ist, liegt in der Verminderung dieser Anzahl das größte Einsparpotenzial. Weitere Optimierungsansätze versuchen daher, vor dem Marching Cubes-Durchlauf diejenigen Würfel aus der Datenmenge herauszufiltern, die ohnehin nicht mit der Isooberfläche in Berührung kommen. Dies sind diejenigen Kuben, die vollständig innerhalb oder vollständig außerhalb des Objektes liegen.\n\nEine 1992 von Wilhelms/van Gelder vorgeschlagene Methode besteht darin, das Volumen in einem Octree abzulegen. In einem Octree wird normalerweise jeder Würfel von Voxeln in acht Unterwürfel zerlegt. Zu jedem Würfel wird nun der niedrigste und der höchste Wert abgespeichert, der darin zu finden ist. Sind bei einem Würfel beide Werte gleich, so wird er nicht mehr weiter unterteilt. Das Ergebnis ist eine Hierarchie von kleiner werdenden Würfeln, für die jeweils ihr Werteintervall bekannt ist. Durch eine Traversierung dieser Hierarchie lassen sich nun diejenigen Würfel aussortieren, deren Minimum über oder deren Maximum unter dem Iso-Schwellwert liegt.\n\nDas Verfahren hat die Nachteile, dass bei jeder Änderung des Isowertes die Hierarchie komplett neu durchlaufen werden muss und dass in realistischen Datensätzen, die normalerweise zentriert vorliegen, meist erst auf unteren Hierarchieebenen Würfel ignoriert werden können.\n\nHierbei handelt es sich um eine Vereinfachung des Marching Cube Algorithmus: die in der obigen Beschreibung des Algorithmus genannte Interpolation der Isoflächenschnittpunkte entfällt. Eckpunkte der durch den Algorithmus erzeugten Dreiecke sind dann lediglich die Mittelpunkte der zwölf Kanten des Würfels sowie sein Mittelpunkt. Auch die Flächennormalen müssen dann nicht mehr interpoliert werden, sondern können auch in einer Nachschlagetabelle gespeichert werden. Ein Vorteil dieser Approximation ist, dass nur noch Integer-Berechnungen durchgeführt werden müssen. Außerdem erhält man viele koplanare Dreiecksflächen, was die Anzahl der resultierenden Dreiecksflächen wesentlich reduziert.\n\nDer MC-Algorithmus ist ein Beispiel für die Auswirkungen von Softwarepatenten. Da der Algorithmus patentiert war, konnte er lange Jahre nicht verwendet werden, ohne Gebühren an den Entwickler zu zahlen. Daher wurde als Alternative der \"Marching Tetrahedrons\" entwickelt, welcher die Voxel-Würfel in Tetraeder unterteilte, und sonst gleich funktionierte. Das erteilte Patent US 4,710,876 wurde am 5. Juni 1985 angemeldet und galt in den Vereinigten Staaten 20 Jahre.\n\n"}
{"id": "1128679", "url": "https://de.wikipedia.org/wiki?curid=1128679", "title": "Molekulare Modellierung", "text": "Molekulare Modellierung\n\nUnter molekularer Modellierung ( (AE) bzw. (BE)), werden Techniken für computerunterstütztes Modellieren chemischer Moleküle zusammengefasst. Das Design von neuen Molekülen und deren Modellierung ist ein Teilgebiet der molekularen Modellierung (englisch , CAMD).\n\nDiese Techniken ermöglichen neben der räumlichen Darstellung von einfachsten bis zu hochkomplexen Molekülen auch die Berechnung ihrer physikochemischen Eigenschaften. Besonders in der medizinischen Chemie liegt die Anwendung darin, Strukturen für neue Wirkstoffe zu optimieren (Homologiemodelling). In diesem Bereich erfährt die molekulare Modellierung zunehmend eine Ergänzung durch die kombinatorische Chemie (Virtual Screening, QSAR, CoMFA, CoMSIA).\n\nMechanistische (semiempirische) und statistische Ansätze (Moleküldynamik, Monte-Carlo-Simulation) eignen sich eher für sehr große Moleküle oder Wechselwirkungen zwischen einer großen Anzahl an Molekülen, quantenchemische Berechnungsverfahren sind hingegen weitaus präziser, aber wegen des höheren Rechenaufwandes nur bei Molekülen kleiner und mittlerer Größe erste Wahl. Von der stetig wachsenden Rechenleistung moderner Computer profitiert die molekulare Modellierung.\n\n\n\n\nDie Potentialhyperfläche (PES) ist eine Darstellung, in der die potentielle Energie und die Struktur eines Moleküls einen mehrdimensionalen Raum aufspannen. Die Energie eines Moleküls wird in Abhängigkeit seiner Kernkoordinaten dargestellt.\n\nIn der molekularer Modellierung ist die Wahl eines geeigneten Koordinatensystems wichtig. Es wird generell zwischen globalen und lokalen Koordinatensystemen unterschieden. Unter die globalen Koordinatensysteme fallen z. B. die orthonormierten kartesischen Koordinaten und die Kristallkoordinaten. Im Unterschied zu kartesischen Koordinaten sind die Winkel in Kristallkoordinaten nicht gleich 90°. Die lokalen Koordinatensysteme sind immer bezogen auf bestimmte Atome oder die Beziehungen der Atome zueinander (z. B. Symmetrieeigenschaften), hier wäre u. a. die Z-Matrix oder die Normalkoordinaten der molekularen Schwingungen zu nennen.\n\nDie \"Optimierung\" ist nach \"Roland W. Kunz\" das Auffinden eines vorteilhaften Zustandes eines Systems, also die Suche nach einem lokalen Minimums in der Nähe eines gegebenen Startpunktes. Generell wird der Begriff \"Optimierung\" für die Suche nach \"kritischen Punkten\" nahe der Ausgangsstruktur verwendet. Als kritische Punkte bezeichnet man Minima, Maxima und Sattelpunkte der Potentialhyperfläche. \n\nDie meisten Optimierungsmethoden bestimmen den nächstgelegenen kritischen Punkt, eine multidimensionale Funktion kann allerdings sehr viele verschiedene kritische Punkte derselben Art enthalten.\n\nDas Minimum mit dem niedrigsten Wert wird als globales Minimum bezeichnet, während alle anderen lokale Minima sind.\n\nDas \"Verfahren des steilsten Abstiegs\", auch \"Gradientenmethode\" genannt nutzt die differenziellen Eigenschaften der Zielfunktion aus. Die Suchrichtung ist durch einen festgelegten negativen Gradienten (differentielle Eigenschaft der Hyperfläche) definiert. Die Optimierung erfolgt hier in die Richtung des negativen Gradienten, der die Richtung des steilsten Abstiegs von einem Ausgangswert angibt, bis keine Verbesserung mehr erzielt werden kann.\n\nDa die potentielle Energie als Funktion der Kernkoordinaten die PES bildet sind für die kritischen Punkte die Ableitungen der Kernkoordinaten-Funktionen besonders wichtig. Innerhalb der Gradientenmethode wird zwischen dem numerischen und dem analytischen Gradienten unterschieden. In der Praxis sollte nur mit Methoden gearbeitet werden, für die ein analytischer Gradient vorhanden ist.\n\nBeim Durchlauf der Gradientenmethode werden keine Informationen aus vorhergegangenen Suchschritten verwendet.\n\nEin Hauptproblem der molekularen Modellierung ist nicht das Fehlen einer geeigneten Methode zur Berechnung der Moleküleigenschaften, sondern ein Zuviel an Methoden. Die Wahl der Methode für ein bestimmtes Problem sollte daher sorgfältig getroffen werden. \n\nDas Ziel der Dichtefunktionaltheorie ist ein geeignetes (Energie-)Funktional der Dichte zu finden. Die möglichen Funktionale sind auf drei Dimensionen, unabhängig von der Molekülgröße, beschränkt. \n\nDie Leistung verschiedener Dichtefunktionaltheorie-Methoden (DFT) ähnelt den Hartree-Fock-Ergebnissen (HF) (siehe unten).\n\nEine der am häufigsten verwendeten Verfahren zur Geometrieoptimierung ist \"B3LYP/6-31G*\". Bei \"B3LYP\" (Kurzform für: Becke, 3-Parameter, Lee-Yang-Parr) handelt es sich um eine Approximationen zum Austausch-Korrelations-Energiefunktional in der Dichtefunktionaltheorie (DFT), \"B3LYP\" steht somit für die gewählte Methode. Die weitere Bezeichnung \"6-31G\" bezieht sich auf den verwendeten Basissatz von John Pople. Die allgemeine Bezeichnung der Basissätze ist \"X-YZG.\" Hierbei steht das \"X\" für die Anzahl primitiver Gaussianer (primitiver Gauß-Funktionen), die jede Kernatomorbitalbasisfunktion umfassen. \"Y\" und \"Z\" zeigen an, dass die Valenzorbitale jeweils aus zwei Basisfunktionen zusammengesetzt sind. Die erste Basisfunktion besteht aus einer Linearkombination von Y primitiven Gaußfunktionen. Die zweite Basisfunktion besteht entsprechend aus einer Linearkombination von Z primitiven Gaußfunktionen. Die Verwendung von Linearkombinationen ist notwendig, da die primitiven Gaussianer in Kernnähe ein anderes Verhalten zeigen als Atomorbitale, durch die Linearkombination wird dieser Fehler minimiert. In diesem Fall impliziert das Vorhandensein von zwei Zahlen nach dem Bindestrich, dass dieser Basissatz ein Split-Valenz-Basissatz ist.\n\nEin \"6-31G\"-Basissatz beschreibt somit die inneren Orbitale als eine Linearkombination von 6 primitiven Gauß-Funktionen, die zu einer kontrahiert sind. Valenzorbitale werden entsprechend durch zwei kontrahierte Gaußfunktionen beschrieben. Eine der kontrahierten Gaußfunktionen ist eine Linearkombinationen von drei primitiven Gauß-Funktionen und die andere eine Linearkombination mit einer primitiven Gauß-Funktion.\n\nDas Sternchen in \"6-31G*\" weißt auf eine Korrektur für die räumliche Abhängigkeit der Ladungsverteilung im Molekül hin. Dies geschieht durch sogenannte Polarisationsfunktionen.\n\nIm Unterschied zur Dichtefunktionaltheorie wird bei der Hartree-Fock-Approximation die Vielteilchenwellenfunktion durch eine Slater-Determinante der Einteilchenzustände (Produktzustände) ausgedrückt, dies führt dazu, dass das Pauli-Prinzip automatisch berücksichtigt wird. Ziel ist somit ein geeignetes (Energie-)Funktional der Wellenfunktion zu finden, diese Wellenfunktionen sind meist hochdimensional, was einen, im Vergleich zur DFT erhöhten Rechenaufwand zur Folge hat. Ein zu beachtender Fehler des Hartree-Fock-Produktansatzes ist die Annahme, dass die Gesamt-Wahrscheinlichkeitsdichte ein einfaches Produkt der Einzel-Wahrscheinlichkeitsdichten ist:\n\nformula_1\n\nDies ist eine physikalisch zweifelhafte Annahme, da Elektronen unkorreliert (also unabhängig voneinander) behandelt werden. Für eine genaue Betrachtung dürfen die Wechselwirkungen der Elektronen untereinander nicht vernachlässigt werden. Auch in der Form der Slater-Determinante werden die Elektronen unkorreliert behandelt.\n\nFür die Geometrieoptimierung auf Hartree-Fock-Niveau werden u. a. häufig folgende Basissätze verwendet: \"STO-3G\", \"3-21G\", \"6-31G*\" bzw. \"6-31G**\", \"cc-pVDZ\" und \"cc-pVQZ\".\n\nDie Wahl des verwendeten Basissatzes spielt ebenso wie die Wahl der Methode (HF, DFT, ...) eine wichtige Rolle in der molekularen Modellierung. Ein Basissatz beschreibt eine Reihe von Basisfunktionen, die die Gesamtelektronenwellenfunktion durch Linearkombination der Basisfunktionen annähert. Heute werden als Basisfunktionen fast ausschließlich primitive Gaußfunktionen verwendet, dies liegt vor allem an dem geringeren Rechenaufwand bei der Verwendung von Gauß-Funktionen im Vergleich zu anderen Basisfunktionen. Wenn andere Basisfunktionen verwendet werden sollen ist darauf zu achten, dass sie ein Verhalten aufweisen, das mit der Physik des Problems übereinstimmt; auch sollte die gewählte Basisfunktion schnell zu berechnen sein.\n\nEin Basissatz heißt \"minimal\", wenn er so viele Basisfunktionen enthält, dass alle Elektronen des Moleküls beschrieben werden können und nur ganze Sätze von Basisfunktionen vorkommen. \n\nDie hier gezeigten Basissätze sind klein, eine typische DFT-Rechnungen weist ca. 100.000 Basisfunktionen auf.\n\n\n"}
{"id": "1130602", "url": "https://de.wikipedia.org/wiki?curid=1130602", "title": "Initrd", "text": "Initrd\n\ninitrd ist ein temporäres Dateisystem, das vom Linux-Kernel während des Bootvorgangs verwendet wird.\n\n„initrd“ steht für \"initial ramdisk\" (sinngemäß übersetzt \"Ausgangspartition im Arbeitsspeicher\"). Die initrd ist ein reservierter Bereich im Arbeitsspeicher, der vom Kernel wie eine Festplattenpartition behandelt wird (siehe auch RAM-Disk). Sie enthält das Abbild eines Dateisystems mit den zum Start des Systems benötigten Dateien. Die initrd kann vom Linux-Kernel und anderen Unix-verwandten Betriebssystemen beim Booten als Stammverzeichnis eingehängt werden. Anschließend wird ein auf der initrd vorhandenes Programm (linuxrc) gestartet. Bei eingebetteten Systemen kann die ganze Funktionalität des Systems in der initrd enthalten sein. Personal Computer nutzen die initrd oft nur als einen Zwischenschritt, um Treiber zu laden und andere Vorbereitungen für den Start des eigentlichen Systems zu treffen. Durch die initrd wurde es möglich, den Bootprozess unter Linux flexibler zu gestalten und Funktionalität aus dem Betriebssystemkern in den Userspace auszulagern. \n\nZum Erstellen von initrd-Abbildern wird gewöhnlich das Programm mkinitrd verwendet. Neben diesem gibt es weitere Alternativen wie z. B. Yaird und Dracut. Das Dracut-Projekt ist mittlerweile Teil des Kernel-Projekts.\n\ninitrd ist veraltet bzw. abgelöst von initramfs, welches manche Schwächen von initrd nicht mehr hat:\n\n"}
{"id": "1131052", "url": "https://de.wikipedia.org/wiki?curid=1131052", "title": "DRBD", "text": "DRBD\n\nDRBD (\"Distributed Replicated Block Device\") ist eine freie Netzwerkspeicher-Software für Linux. Als Kernel-Modul zusammen mit einer Management-Applikation im Userspace und einem Skript dient es dazu, ein Blockgerät auf einem produktiven (primary) Server in Echtzeit auf einen anderen (secondary) Server zu spiegeln. Dieses Verfahren wird verwendet, um Hochverfügbarkeit (HA) im Linux-Umfeld zu realisieren und somit eine gute Verfügbarkeit verschiedener Dienste zu erreichen. Alternativen hierzu sind verteilte Dateisysteme wie z. B. GlusterFS.\n\nDie primäre Aufgabe der Software DRBD besteht darin, im Sinne von Hochverfügbarkeit das Vorhandensein ein und desselben Datensatzes auf mehr als einem Blockgerät – etwa einer Festplatte oder einer SSD – sicherzustellen. Die jeweiligen Blockgeräte befinden sich üblicherweise in unterschiedlichen Servern, um Redundanz im Falle des Ausfalls eines Servers zu garantieren. DRBD verfügt über mehrere Funktionen, um die mit Datenreplikation zumeist einhergehenden Probleme zu umgehen oder ihre Effekte abzumildern; es beherrscht etwa neben der vollsynchronen Replikation auch die semi-synchrone sowie die asynchrone Replikation. Ebenso unterstützt es mehrere Netzwerktransportmedien. Das Activity Log stellt sicher, dass nicht der komplette Inhalt eines Blockgerätes zwischen den Knoten eines Hochverfügbarkeitsclusters erneut synchronisiert werden muss, falls die Netzwerkverbindung zwischen den beiden Systemen abbricht.\n\nDRBD nutzt für die Kommunikation mit einem Blockgerät die Blockspeicher-geräteverwaltung des Linux Kernels („Block Device Layer“), sodass ein DRBD-Laufwerk („Ressource“) selbst auf einem Linux-System ebenfalls ein Blockgerät darstellt. Auf Applikationsebene funktioniert der Zugriff auf DRBD-Ressourcen mithin genauso wie jener auf Festplatten oder SSDs; ebenso benötigt eine DRBD-Ressource ein Dateisystem oder eine vergleichbare Vorrichtung, um den koordinierten Lese- und Schreibzugriff zu ermöglichen. DRBD ist mithin agnostisch im Hinblick auf die Software, die es benutzt.\n\nVon der Applikationsseite her eingehende Lese- und Schreibanfragen leitet eine DRBD-Ressource einerseits unmittelbar an das mit ihr verbundene Blockgerät blockweise („Backing Device“) weiter. Andererseits übergibt die DRBD-Ressource dieselben Daten auch an die Netzwerkverwaltung („Network stack“) des Linux-Kernels auf dem System, von wo aus dieser sie an den jeweils anderen Knoten sendet. Dieser Mechanismus garantiert die Synchronisation der Daten in DRBD.\n\nZwar beherrscht DRBD verschiedene Netzwerktransportmedien für die Datenübertragung; die Standardkonfiguration sieht jedoch die Verwendung einer vorhandenen Ethernet-Verbindung mittels TCP/IP-Protokoll vor. Wahlweise lassen sich zwei DRBD-Systeme auch direkt – also ohne zwischenliegende Netzwerk-Switches – verbinden („Cross Link“), falls geeignete Netzwerkkarten in den Systemen verbaut sind.\n\nDRBD beherrscht die automatische Resynchronisation der Backing Devices einer DRBD-Ressource, falls die Netzwerkverbindung dieser Ressource zwischenzeitlich getrennt war. Sobald die Netzwerkverbindung wiederhergestellt ist, prüft DRBD automatisch den Zustand der Ressource auf beiden Hosts und leitet gegebenenfalls eine Resynchronisation ein. Diese gilt als erfolgreich abgeschlossen, sobald der sich auf den Backing-Devices der DRBD-Ressource befindliche Datensatz auf allen beteiligten Servern identisch ist.\n\nEine Sonderform der Resynchronisation ist das erstmalige Anlegen einer DRBD-Ressource: Hierbei unterscheidet sich der Inhalt der Backing Devices zwar vielleicht auf der Ebene der Blockgeräte; dies ist jedoch belanglos, weil bei der Nutzung von DRBD die Daten auf den Backing Devices ohnehin sukzessive überschrieben werden. DRBD bietet deshalb die Möglichkeit, mittels spezieller Befehle die Synchronisation der Backing Devices beim erstmaligen Anlegen der Ressource zu überspringen.\n\nBis einschließlich DRBD 8.4 bot DRBD lediglich die Möglichkeit, denselben Datensatz von einem System auf ein anderes System zu spiegeln. Üblich war insofern der Einsatz in Hochverfügbarkeitsclustern mit zwei Knoten, wobei der schreibende Zugriff etwa durch Applikationen im Normalfall nur auf einem System geschieht.\n\nIn einem Cluster bestehend aus zwei Knoten hält eine DRBD-Ressource auf einem System in aller Regel die Rolle („role“) \"Primary\" und auf dem anderen System die Rolle \"Secondary\". Lese- wie Schreibzugriff sind bei DRBD immer ausschließlich auf jenem Knoten möglich, auf dem die jeweilige DRBD-Ressource die \"Primary\"-Rolle besitzt. Die DRBD-Ressource im \"Secondary\"-Modus empfängt lediglich Updates für den Datensatz des jeweiligen Laufwerks, die die Ressource auf dem \"Primary\"-Modus an sie sendet.\n\nEine bis einschließlich DRBD 8.4 häufig genutzte Funktion zur Umgehung der Limitation auf zwei Knoten war das so genannte „Device Stacking“. DRBD bot dabei die Möglichkeit, mehrere DRBD-Ressourcen auf einem Host innerhalb der Blockgeräteverwaltung des Linux-Kernels so zu kombinieren, dass sie Veränderungen in festgelegter Reihenfolge aneinander vererben. Vorrangiges Ziel dieser Maßnahme war es, die redundante, lokale Replikation einerseits und gleichzeitig die Replikation hin zu einem anderen Standort andererseits zu ermöglichen. Gestapelte Ressourcen waren im Hinblick auf die erreichbare Performance jedoch ihren „normalen“ Pendants unterlegen.\n\nMit dem Funktionsumfang von DRBD 8.4 ist DRBD aktuell offizieller Bestandteil des Linux-Kernels.\n\nBeginnend mit DRBD 9 haben dessen Maintainer die Funktionalität der Software erheblich erweitert. Weggefallen ist insbesondere jene Einschränkung, wonach eine DRBD-Ressource mit der \"Primary\"-Rolle ihre Daten lediglich auf eine weitere DRBD-Ressource mit der \"Secondary\"-Rolle kopieren kann. Aktuelle DRBD-Versionen von DRBD 9 bieten stattdessen die Möglichkeit, Daten von einer \"Primary\"-Ressource auf bis zu 31 \"Secondary\"-Ressourcen gleichzeitig zu kopieren.  \n\nIndirekt lässt sich DRBD seit DRBD 9 durch die Einführung der Management-Software codice_1 als eine Lösung für Software Defined Storage nutzen: DRBD-Ressourcen lassen sich dabei aus Umgebungen wie OpenStack heraus dynamisch auf den Servern eines DRBD-Clusters anlegen, die gerade noch freie Ressourcen – also verfügbaren Speicherplatz – bieten. Die Funktionen von DRBD 9 ermöglichen indirekt also den Betrieb als Speicher, der in die Breite skalieren kann („Scale Out“).\n\nDurch die Einführung der Replikation hin zu mehreren Zielen hat das aus DRBD 8.4 bekannt Stapeln von Ressourcen („Device Stacking“) in DRBD 9 seine Bedeutung de facto eingebüßt.\n\nDRBD unterscheidet bei einer Ressource grundsätzlich zwischen den Rollen \"Primary\" und \"Secondary\".\nDie Rolle einer DRBD-Ressource ist grundsätzlich unabhängig vom aktuellen Status der Ressource im Hinblick auf ihre Netzwerkverbindung. Für die eigene Netzwerkverbindung kennt eine Ressource in DRBD drei Zustände:\nIn Summe ergeben sich für eine DRBD-Ressource auf einem Host mithin die folgenden möglichen Zustände:\nDRBD unterstützt verschiedene Replikationsmodi, die in DRBD als „Protokolle“ bezeichnet werden. Maßgeblich unterscheiden sich die verfügbaren Protokolle im Hinblick auf dem Zeitpunkt, zu dem die DRBD-Ressource im \"Primary\"-Modus einer auf sie schreibend zugreifenden Applikation signalisiert, dass der Schreibvorgang erfolgreich abgeschlossen ist.\n\nDie Standardkonfiguration für ein Laufwerk sorgt für vollsynchrone Replikation (\"Protokoll C\"); hierbei bestätigt in einem Setup mit zwei Knoten die DRBD-Ressource der schreibend auf sie zugreifenden Applikation erst, dass ein Schreibvorgang erfolgreich abgeschlossen ist, sobald dieselbe DRBD-Ressource auf beiden Cluster-Knoten die Veränderung erfolgreich auf ihr lokales Blockgerät geschrieben hat. In DRBD 9 ist die Anzahl der Knoten, auf denen ein Schreibvorgang erfolgreich beendet sein muss, bevor er im Sinne des Protokolls C als erfolgreich gilt, durch den Admin konfigurierbar. Dieser Replikationsmodus bietet als einziger der von DRBD unterstützten Modi Transaktionssicherheit. Er ist deshalb auch der in den meisten Setups anzutreffende Modus.\n\nAls Protokoll B bezeichnet DRBD seine Implementation semi-synchroner Replikation: Hierbei müssen die Pakete lediglich die Netzwerkkarte des gegenüberliegenden Cluster-Knotens erreicht haben, damit die \"Primary\"-Ressource der Applikation die erfolgreiche Beendigung des Schreibvorgangs signalisiert. Der Modus ist nicht transaktionssicher, weil etwa im Falle eines Stromausfalls beim Knoten mit der Ressource im \"Secondary\"-Modus die Daten dort möglicherweise noch nicht auf die Platte geschrieben waren. In der Praxis kommt dem Protokoll B lediglich untergeordnete Bedeutung zu.\n\nDas Protokoll A beschreibt in DRBD das Prinzip der asynchronen Replikation: Die DRBD-Ressource im \"Primary\"-Modus signalisiert der lokalen Applikation den erfolgreichen Abschluss eines Schreibvorgangs, sobald die Daten das der DRBD zugrundeliegende Blockgerät auf demselben Knoten erreicht haben. Das Protokoll A kommt üblicherweise nicht für die lokale Replikation zwischen zwei Knoten zum Einsatz; es eignet sich stattdessen besonders für die Replikation zwischen verschiedenen geographischen Standorten, falls die über das Netzwerk zu erreichende Latenz dort unzufrieden stellend ist.  \n\nDRBD bis einschließlich Version 8.4 bietet grundsätzlich die Möglichkeit, zueinander gehörende DRBD-Ressourcen auf unterschiedlichen Hosts parallel in der \"Primary\"-Rolle zu betreiben. Der zuverlässige Betrieb einer DRBD-Ressource in der \"Primary\"-Rolle auf mehreren Systemen stellt jedoch fast immer erheblich höhere Anforderungen an das Setup als der klassische Betrieb nach Primary-Secondary-Schema; er bedingt etwa die Nutzung eines Sperrmechanismus, um konkurrierende Schreibvorgänge zu unterbinden, wofür unter Linux üblicherweise der \"Distributed Lock Manager (DLM)\" zum Einsatz kommt. Für DRBD 8.4 raten die DRBD-Entwickler bereits für die Mehrzahl der Fälle von entsprechenden Setups ab; der einzige legitime Einsatzzweck ist demnach der kurzzeitige parallele Betrieb im Dual-Primary-Modus zum Zwecke der Live-Migration virtueller Maschinen.\n\nDRBD 9 unterstützte den Betrieb einer Ressource im Dual-Primary-Modus zwar grundsätzlich, die Entwickler der Software messen diesem Einsatzszenario jedoch mit Ausnahme der erwähnten Live-Migration virtueller Maschinen keine praktische Relevanz mehr bei. Eine Weiterentwicklung der Funktion ist seitens des Anbieters augenblicklich nicht vorgesehen.  \n\nFür seine ordnungsgemäße Funktionalität benötigt DRBD sogenannte Metadaten auf den Backing Devices einer Ressource. Beim Anlegen einer Ressource in DRBD 8.4 oder vorherigen Versionen erstellt der Admin die Metadaten mittels codice_2 selbst auf allen beteiligten Systemen; in DRBD 9 besteht bei der Nutzung von codice_1 alternativ die Möglichkeit, dieses die Metadaten automatisch anlegen zu lassen.\n\nDer Bereich mit den Metadaten befindet sich üblicherweise am Anfang oder am Ende des Backing Devices; alternativ lassen sich die Metadaten auch auf einem externen Blockgerät ablegen. Dieses Setup kommt vorrangig zum Einsatz, um die Replikation von Blockgeräten mittels DRBD im Nachhinein zu ermöglichen. Das Auslagern der Metadaten neu anzulegender DRBD-Ressourcen ist hingegen unüblich und kommt nur in besonderen Szenarien zur Anwendung.\n\nDer Metadatenbereich hat eine variable Größe, die im Wesentlichen von der Größe des Backing Devices insgesamt abhängt. Im betrieblichen Alltag kann das zu Problemen führen, wenn eine Ressource interne Metadaten nutzt und durch den Admin vergrößert werden soll. In solchen Fällen ist es notwendig, die Metadaten der DRBD-Ressource zunächst an das Ende des Backing Devices zu verschieben, nachdem die neue Größe des Backing Devices bestimmt ist.\n\nZu den in den Metadaten zu findenden Informationen gehören insbesondere das DRBD Activity Log sowie Informationen über Rollenänderungen der jeweiligen DRBD-Ressource in der Vergangenheit.\n\nIm betrieblichen Alltag eines Hochverfügbarkeitsclusters ist der Abbruch der Netzwerkverbindung zwischen den verschiedenen Clusterknoten ein gängiges Fehlerszenario. Der Ausfall eines Clusterknotens etwa, mit dem der Ausfall der Netzwerkverbindung zwangsweise einhergeht, ist gar das klassische Einsatzszenario für einen Hochverfügbarkeitscluster überhaupt. Aus Sicht von DRBD wie aus Sicht jeder netzwerkbasierten Replikationslösung stellt der Abbruch der Kommunikationsverbindung hin zum Clusterpartner jedoch ein handfestes Problem dar.\n\nDas trifft insbesondere dann zu, wenn der nunmehr ausgefallene Clusterknoten zuvor DRBD-Ressourcen in der \"Primary\"-Rolle betrieben hat. Trotz vollsynchroner Replikation per Protokoll C ist es nicht zu verhindern, dass auf jenem Clusterknoten Änderungen des Datensatzes des Backing Devices der DRBD-Ressourcen unmittelbar vom Ausfall des Knotens stattfinden, die nicht mehr erfolgreich zum Clusterpartner synchronisiert werden können. Im ungünstigsten Fall befinden sich auf dem Backing Device der Ressource des dann ausgefallenen Knotens also aktuellere Daten als auf dem Backing Device der Ressource des verbliebenen Systems.\n\nKommt ein Cluster-Manager wie Pacemaker zum Einsatz, aktiviert dieser unmittelbar nach dem Ausfall eines Knotens im Normalfall alle Ressourcen auf dem verbliebenen Knoten für die dortige Nutzung; DRBD-Ressourcen schaltet er dort dann in die \"Primary\"-Rolle um. Weil zumindest im Protokoll C etwaige Transaktionen auf dem vorherigen \"Primary\"-Knoten nicht als erfolgreich abgeschlossen galten, handelt es sich ausdrücklich nicht um eine Split-Brain-Situation. Sobald der zwischenzeitlich ausgefallene Clusterknoten dem Cluster wieder beitritt, muss der verbliebene Clusterknoten jedoch für die Löschung der dann nicht mehr korrekten Daten auf den Backing Devices der DRBD-Ressourcen auf dem zwischenzeitlich ausgefallenen System sorgen.\n\nHierfür existierenden verschiedene Ansätze: Einerseits könnte der Knoten mit den DRBD-Ressourcen in der \"Primary\"-Rolle den gesamten Inhalt des Backing Devices vom primären auf den sekundären Knoten synchronisieren. Gerade bei großen DRBD-Ressourcen würde dies jedoch einige Zeit in Anspruch nehmen, während der die Performance der DRBD-Ressourcen nicht optimal wäre. DRBD nutzt deshalb stattdessen eine Technik namens „Activity Log“: Im Activity Log, das in den Metadaten einer DRBD-Ressource liegt, verzeichnet DRBD, welche Extents des Backing Devices zuletzt verändert worden sind. Die Anzahl der im Activity Log verzeichneten Extents lässt sich per Konfigurationsdatei an lokale Gegebenheiten anpassen. Sobald die Netzwerkverbindung wieder zustande gekommen ist, synchronisiert der Knoten mit den DRBD-Ressourcen in der \"Primary\"-Rolle lediglich jene Extents, die im Activity Log der Ressource auf dem anderen Knoten verzeichnet sind. DRBD umgeht auf diese Weise die vollständige Resynchronisation der Backing Devices.\n\nWaren in DRBD 8.4 noch sowohl die Replikationslogik als auch die Logik für den Transport der zu replizierenden Daten Teil des gemeinsamen DRBD-Kernelmoduls drbd.ko, so haben die Entwickler die Netzwerklogik der Lösung in DRBD 9 erheblich modifiziert. Das eigentliche Kernelmodul für DRBD kümmert sich seither ausschließlich um die Replikation von Daten und übergibt diese über eine interne, generische Schnittstelle an ein ebenfalls in den Kernel zu landendes Modul für den Netzwerktransport der DRBD-Daten. Das neue Design ermöglicht die Nutzung verschiedener Netzwerktransportschichten; neben Ethernet unterstützt DRBD 9 etwa auch Replikation via Infiniband. Die Unterstützung für zusätzliche Transportmedien ist seitens des Herstellers geplant und befindet sich in Vorbereitung.\n\nAnders als verteilte Speicherlösungen wie Ceph oder GlusterFS nimmt DRBD selbst keine Reorganisation der Daten vor. Als Teil des Block Device Layers des Linux-Kernels leitet es eingehende Schreib- und Leseanfragen lediglich an das jeweilige Backing Device einerseits und an den Netzwerkstack des lokalen Systems andererseits weiter. Weil das Verschieben von Daten innerhalb des Block Device Layers des Linux-Kernels praktisch in Echtzeit geschieht, entspricht der zu erreichende Durchsatz auf einer DRBD-Ressource in etwa jenem des darunter liegenden, physischen Blockgerätes. Entsprechend lässt sich dieser Durchsatz erhöhen, indem DRBD RAID-Verbünde mit vielen Spindeln nutzt, deren Bandbreite entsprechende RAID-Controller bündeln.\n\nHinsichtlich der zu erwartenden Latenz bei Schreibzugriffen auf DRBD-Ressourcen wirkt sich die durch DRBD gewährleistete Replikation hingegen negativ aus. Die DRBD-Ressource in der \"Primary\"-Rolle kann den erfolgreichen Abschluss einer Schreiboperation an die schreibende Applikation erst vermelden, nachdem die Bestätigung vom Clusterpartner über das erfolgreiche Schreiben der Daten auf dem dortigen Blockgerät eingelangt ist. Dadurch entsteht eine Gesamtlatenz bestehend aus der Latenz der beiden Datenträger sowie der doppelten Netzwerklatenz zwischen den Systemen. Durch den Einsatz alternativer Netzwerklösungen wie Infiniband, deren implizite Latenz deutlich geringer als jene von Ethernet ist, lässt sich dieser Parameter jedoch optimieren.\n\nIn Summe liegt die Latenz von Schreibzugriffen auf DRBD-Ressourcen selbst bei der Nutzung von Ethernet deutlich unter jener von verteilten Storage-Lösungen. Jene bieten im Gegenzug meist deutlich höhere Bandbreiten, weil sie die Bandbreite vieler Knoten durch das Aufteilen der zu schreibenden Daten in binäre Objekte miteinander kombinieren.\n\nDRBD selbst bietet lediglich rudimentäre Funktionen für die knotenübergreifende Funktionalität in Hochverfügbarkeitsclustern. Seit DRBD 9 besteht etwa die Möglichkeit, eine Ressource durch DRBD automatisch in die \"Primary\"-Rolle umschalten zu lassen, falls auf diese lokal schreibend zugegegriffen werden soll.\n\nFür komplexe Installationen genügt diese Funktionalität jedoch nicht. So reicht es auf Systemen in der Regel etwa nicht, eine Ressource in die \"Primary\"-Rolle zu schalten; ergänzend dazu muss meist auch ein auf der DRBD-Ressource beheimatetes Dateisystem in das Dateisystem des Hosts eingehängt werden. Neben einem allfällig zu startenden Dienst wie einer Datenbank, die das eingehangene Dateisystem anschließend nutzt, brauchen klassische Hochverfügbarkeitscluster zudem besondere Dienste wie eine spezielle IP-Adresse auf Clusterebene, die zusammen mit den Diensten zwischen den Clusterknoten hin- und herschwenkt („Failover“). Nur so lässt sich HA-Funktionalität nämlich erreichen, ohne die Clients, die auf einen von einem Hochverfügbarkeitscluster angebotenen Dienst zugreifen sollen, umzukonfigurieren.\n\nDRBD kommt im Kontext von Hochverfügbarkeitsclustern deshalb regelmäßig mit dem Cluster Resource Manager (CRM) des Linux-HA-Projektes, Pacemaker, sowie dessen Cluster Communication Manager (CCM), Corosync zum Einsatz. LINBIT bietet als DRBD-Betreuer einen „OCF Resource Agent“ an, mittels dessen sich DRBD-Ressourcen in Pacemaker integrieren und verwalten lassen.\n\nDie Werkzeuge, die auf der Kommandozeile dienen, um DRBD-Ressourcen zu verwalten, haben sich im Laufe der Versionsgeschichte von DRBD mehrfach geändert.\n\nBis einschließlich DRBD 8.4 standen Administratoren vier Werkzeuge zur Verfügung, um DRBD-Ressourcen zu verwalten:\nDie Datei codice_8 innerhalb von procfs enthält grundsätzliche Details über die lokal vorhandenen DRBD-Ressourcen. In ihr verzeichnet der DRBD-Kerneltreiber alle aktiven Ressourcen sowie deren Rollen auf dem lokalen System und ggf. auf dem Clusterpartner.\n\nDas Werkzeug codice_5 liest codice_4 aus und korreliert die dort gefundenen Daten mit den Inhalten der Konfigurationsdateien von DRBD; schließlich zeigt es eine entsprechend aufbereitete Übersicht aller Ressourcen an. codice_5 ist allerdings obsolet und sollte nicht länger zum Einsatz kommen.\n\ndrbdsetup ist das Low-Level-Werkzeug zum Management von DRBD-Ressourcen. Der Admin nutzt es selten direkt; das Werkzeug codice_2 ruft codice_6 im Hintergrund jedoch mit den richtigen Parametern auf. codice_6 ist außerdem der empfohlene Weg, Informationen über die DRBD-Ressourcen aus codice_4 zu beziehen.\n\ncodice_2 ist in DRBD 8.4 das Hauptwerkzeug für Admins um DRBD-Ressourcen anzulegen, zu verwalten und zu löschen.\n\nAufgrund der in DRBD 9 im Vergleich mit DRBD 8 stark gestiegenen Komplexität der grundsätzlich möglichen Setups hat der Hersteller LINBIT zusammen mit DRBD 9 auch ein neues Management-Werkzeug namens codice_1 vorgestellt. codice_1 basiert auf einer Server-Client-Architektur und ermöglicht das cluster-weite Anlegen, Verwalten und Löschen von DRBD-Ressourcen. Die erste Version von codice_1 basiert auf der Skriptsprache Python; aktuell arbeitet LINBIT jedoch an einer neuen Version von codice_1, die auf Java basieren soll. \n\nEbenfalls haben die DRBD-Entwickler im Rahmen der Einführung von DRBD 9 die Entwicklung von DRBD und den dazugehörigen Verwaltungswerkzeugen getrennt, so dass sie nun unterschiedlichen Releasezyklen folgen.\n\nFür Kontroversen sorgte LINBIT Ende 2016, als es die Lizenz von codice_1 von der freien GPL v3 hin zu einer kommerziellen Lizenz änderte, die mit den Anforderungen der Definition freier Software des GNU-Projektes nicht in Einklang zu bringen war. LINBIT revidierte die Entscheidung wenig später jedoch, so dass für drbdmanage nun wieder die Bestimmungen der GNU GPL v3 gelten.\n\nLINBIT ersetzte codice_1 im Juli 2018 durch Linstor, um den neuen Anforderungen im Storage-Management gerecht zu werden.\n\nRegelmäßig findet DRBD insbesondere in der Version 9 gleichzeitig mit anderen Speicherlösungen wie Ceph oder GlusterFS Erwähnung; auch OCFS2 oder GFS2 sind Begriffe, die im DRBD-Kontext regelmäßig fallen. Von all jenen Lösungen unterscheidet sich DRBD jedoch merklich.\n\nAnders als DRBD handelt es sich bei Lösungen wie Ceph oder GlusterFS um massiv verteilte Speichersysteme. Diese zeichnen sich dadurch aus, dass sie anhand eines Algorithmus – etwa eines Hash-Algorithmus – Daten auf Basis bestimmter Kriterien auf eine beliebige Anzahl physischer Speichergeräte verteilen. Replikation ist dabei üblicherweise impliziter Teil der Lösung.\n\nCluster-Dateisysteme wie OCFS2 oder GFS2 ermöglichen es, innerhalb eines Clusters auf denselben Datensatz von mehreren Klienten aus konkurrierend schreibend wie lesend zuzugreifen. DRBD ist mithin keine Alternative zu Clusterdateisystemen, kann im Dual-Primary-Modus jedoch die Basis für solche Ansätze bilden.\n\nKonventionelle Computer-Cluster-Systeme benutzen in der Regel eine Art gemeinsamen Speicher, der für die Clusterressourcen benutzt wird.\nDieser Ansatz hat jedoch eine Reihe von Nachteilen, die DRBD umgeht.\n\nDRBD arbeitet innerhalb des Linux-Kernels auf Blockebene und ist damit für darauf aufsetzende Schichten transparent. DRBD kann somit als Grundlage verwendet werden für:\n\nDRBD-basierende Cluster werden eingesetzt, um z. B. Dateiserver, relationale Datenbanken (wie PostgreSQL oder MySQL) und Hypervisor/Server-Virtualisierung (wie OpenVZ) um synchrone Replikation und Hochverfügbarkeit zu erweitern.\n\nIm Juli 2007 stellten die DRBD-Autoren die Software der Linux-Entwicklergemeinde für eine mögliche zukünftige Aufnahme von DRBD in den offiziellen Linux-Kernel zur Verfügung. Nach zweieinhalb Jahren wurde DRBD in den Kernel 2.6.33, der am 24. Februar 2010 veröffentlicht wurde, aufgenommen.\n\nDie kommerziell lizenzierte Version DRBD+ wurde in der ersten Hälfte des Dezembers 2008 mit der Open-Source-Version zusammengeführt und unter der GNU General Public License freigegeben. Seit der daraus resultierenden Version 8.3 ist es möglich, den Datenbestand auf einen dritten Knoten zu spiegeln. Die Höchstgrenze von 4 TiByte pro Gerät wurde auf 16 TiByte erhöht.\n\nSeit 2012 gibt es keine Größenbeschränkung mehr pro DRBD-Device. Die offizielle Nutzungsstatistik zählt rund 30.000 regelmäßig aktualisierte Installationen mit Device-Größen von bis zu 220 TB.\n\nIm Juli 2018 hat Linbit codice_1 durch Linstor ersetzt. \n\n"}
{"id": "1131836", "url": "https://de.wikipedia.org/wiki?curid=1131836", "title": "Systemwiederherstellung", "text": "Systemwiederherstellung\n\nUnter der Systemwiederherstellung versteht man eine bei den Betriebssystemen der Windows-Familie erstmals im Microsoft Windows ME vorgestellte Funktion, welche es dem Benutzer mit Hilfe sogenannter Wiederherstellungspunkte ermöglicht, das System in Hinsicht auf System- und Konfigurationsdateien in einen früheren Zustand zurückzuführen. \n\nDer Wiederherstellungspunkt ist eine durch die Systemwiederherstellungsfunktion angelegte Schattenkopie. Beim Auftreten von Systemproblemen kann das System durch einen vorhandenen Wiederherstellungspunkt auf einen früheren Systemstatus zurückgesetzt werden. Vor der Installation einer tief ins System eingreifenden Software (Treiber, systemnah operierende Software etc.) ist es daher ratsam, einen Wiederherstellungspunkt anzulegen. Bei der Installation bestimmter Software wird dies automatisch erledigt.\n\nDurch die Systemwiederherstellung werden lediglich System- und Konfigurationsdateien zurückgesetzt, eigene Dateien des Computerbenutzers wie Musik, Bilder, Videos etc. sind hiervon nicht betroffen.\n\nÄnderungen an der Partitionstabelle, selbst wenn diese nicht die Windows-Boot- oder Systempartitionen betreffen, können mittels dieser Systemwiederherstellung auch unter Zuhilfenahme von Windows-Recovery-Datenträgern nicht wieder rückgängig gemacht werden.\n\nEin durch das Betriebssystem automatisch angelegter Wiederherstellungspunkt wird als Systemprüfpunkt bezeichnet.\n\nNeben der Möglichkeit, Wiederherstellungspunkte zu erstellen, ist es seit Windows 7 auch möglich, sogenannte Systemabbilder zu erstellen. Ein Systemabbild enthält dabei den gesamten Inhalt eines (oder mehrerer) Laufwerke und kann verwendet werden, um diese Laufwerke vollständig in den gesicherten Zustand zurückzusetzen. Es ist nicht möglich, einzelne Elemente (wie z. B. versehentlich gelöschte Dokumente) für die Wiederherstellung auszuwählen; alle Dateien werden vollständig ersetzt. Obwohl bei diesem Sicherungstyp persönliche Dateien eingeschlossen sind, wird empfohlen, regelmäßig z. B. mit der Windows-Sicherung die eigenen Dateien zu sichern, damit bei Bedarf einzelne Dateien oder Ordner wiederhergestellt werden können.\n"}
{"id": "1136472", "url": "https://de.wikipedia.org/wiki?curid=1136472", "title": "Fermi-Pasta-Ulam-Tsingou-Experiment", "text": "Fermi-Pasta-Ulam-Tsingou-Experiment\n\nDas Fermi-Pasta-Ulam-Tsingou-Experiment (häufig auch Fermi-Pasta-Ulam-Experiment genannt) untersucht das Schwingungsverhalten komplexer Systeme. Das überraschende Ergebnis dieses Experiments zählt zu den wesentlichen Beiträgen der Chaosforschung. Als eines der ersten Computerexperimente beeinflusste es das Verfahren der Simulation als Experimentiertechnik wesentlich. \n\nDieses Experiment wurde im Sommer 1953 von Enrico Fermi, John R. Pasta, Stanislaw Ulam und Mary Tsingou durchgeführt und 1955 in einem Bericht des Los Alamos National Laboratory publiziert. Es war eines der ersten Computerexperimente; die Versuchsanordnung war ein im Computer, dem MANIAC I, simuliertes Modell. Untersucht wurde die Energie einer schwingenden Saite, deren Verhalten mit einem nichtlinearen Teilterm (quadratisch und kubisch) beschrieben wird.\n\nBei einem linearen Schwinger stellen sich gleiche Zustände nach gleichen zeitlichen (oder örtlichen) Abständen wieder ein, es sind einzelne Frequenzen (Schwingungsmodi) bestimmbar. Bei nichtlinearer Kopplung erwartete Fermi ein ergodisches Verhalten: Die bestimmende Frequenz schwächt sich in ihrer Auswirkung ab, alle Modi können gleich angeregt werden → die Anordnung verhält sich zufällig.\n\nStatt des zufälligen stellt sich ein fast periodisches (quasi-periodisches) Verhalten ein. Daraus wird gefolgert:\n\n1965 konnten Norman Zabusky und Martin Kruskal zeigen, dass die Korteweg-de-Vries-Gleichung den kontinuierlichen Grenzfall darstellt, und damit eine erste Erklärung für das quasi-periodische Verhalten geben.\n\nNeben diesen Erkenntnissen zur Komplexität nichtlinearer Systeme ist die Benutzung eines Computers zur Untersuchung mechanischer und physikalischer Vorgänge eine Pioniertat.\n\n"}
{"id": "1137065", "url": "https://de.wikipedia.org/wiki?curid=1137065", "title": "Rxvt", "text": "Rxvt\n\nrxvt ist eine Terminalemulation für das X Window System („X“) und soll einen im Funktionsumfang reduzierten und folglich speicherschonenderen Ersatz für xterm darstellen. „urxvt“ (oder „rxvt-unicode“) wiederum ist ein Klon von „rxvt“, der im Gegensatz zum Original intern alle Daten in Unicode verarbeitet. Mit der Verbreitung von Compiz und Beryl gewann „urxvt“ neue Bedeutung, da er kompatibel zur neu eingeführten Transparenz von X ist.\n\n"}
{"id": "1137838", "url": "https://de.wikipedia.org/wiki?curid=1137838", "title": "Internet Professionell", "text": "Internet Professionell\n\nDie Internet Professionell war eine Fachzeitschrift mit Praxis-Artikeln, Tests und Ratgebern aus den Bereichen Webdesign, Programmierung, Server-Technik, IT-Sicherheit und E-Commerce. Aufgrund nicht vorhandener wirtschaftlicher Perspektiven hat der neue Eigentümer der Zeitschrift \"3i\" beschlossen, diese zur Ausgabe 6/2007 einzustellen.\n\nInternet Professionell richtete sich mit Artikeln und Tests an Webentwickler, Webdesigner, IT-Administratoren, Business-Entscheider und Onlineshop-Betreiber. In den Rubriken Design, Web Dev und Technik & Sicherheit stellten die Redakteure Skripte vor, testeten aktuelle Hardware und Software und gaben Tipps und Tricks rund um Webentwicklung, Provider und aktuelle Trends im Internet.\n\nDas Themenspektrum der Zeitschrift umfasste PHP, CSS, JavaScript, Gebrauchstauglichkeit (\"usability\"), Webserver, XML, Photoshop, RSS, Online-Shops, Webhosting und ASP.NET, Antispamsoftware, Content-Management-Systemen (CMS), Webservices, Online-Recht und Netzwerk-Hardware.\n\nInternet Professionell erschien monatlich im Verlag VNU Business Publications Deutschland. Ursprünglich hob 1995 der amerikanische Verlag Ziff-Davis das Vorläufer-Magazin \"pl@net\" aus der Taufe. Das Internet-Lifestyle-Magazin nach dem Vorbild der amerikanischen Zeitschrift \"Wired\" wurde 1997 zu Internet Professionell, die dann anstatt Netzkultur die Webentwicklung und Technik in den Mittelpunkt stellte. Im Jahr 1999 verkaufte der Medienkonzern Ziff-Davis den Verlagsbereich, zu dem auch Internet Professionell gehört, an das US-amerikanische Investmenthaus Willis Stein und Partners. 2000 übernahm VNU Business Publications Deutschland in München die Zeitschrift. Mit der Aufspaltung von VNU in The Nielsen Company und VNU Business Media wurde VNU Deutschland an \"3i\" weiterverkauft und anschließend in eine Auffanggesellschaft Volnay GmbH eingebracht. Dieser Verlag wurde zum Ende Juni 2007 geschlossen, weil sich hierfür kein Käufer finden konnte.\n\n"}
{"id": "1140654", "url": "https://de.wikipedia.org/wiki?curid=1140654", "title": "Corel Photo-Paint", "text": "Corel Photo-Paint\n\nCorel Photo-Paint ist ein Bildbearbeitungsprogramm zur Bearbeitung von Rastergrafiken. Hersteller ist das Unternehmen Corel Inc., ein Softwareunternehmen mit Hauptsitz in Ottawa (Kanada). \n\nDas Programm wird nicht einzeln verkauft, sondern ist Bestandteil der CorelDraw Graphics Suite. Die aktuelle Version ist Corel Photo-Paint X8, das entspricht der Version 18. Photo-Paint wird vor allem für Bildmanipulationen benutzt. \n\nPhoto-Paint hat keinen großen Marktanteil, ist aber im Druckvorstufenbereich mit Adobe Photoshop vergleichbar. So besitzt Photo-Paint ebenfalls eine voll funktionsfähige CMYK-Unterstützung mit Farbmanagement als Voraussetzung für die Umsetzung im Vierfarbdruckbereich. Auf der einen Seite war jede neue Version von Photo-Paint bemüht, mit den Fähigkeiten von Photoshop und dessen Werkzeugen gleichzuziehen, auf der anderen Seite erschienen mit Photo-Paint auch Neuerungen, beispielsweise \"an einer Kurve orientierten Text\", zwei Jahre, bevor auch Photoshop diese Funktion anbot.\n\nDa nahezu alle Bearbeitungsfunktionen und Effekte in beiden Programmen mittlerweile identisch sind, können die neueren Versionen die aus dem gleichen Jahr stammenden Photoshop-Dateien (mit den Endungen PSD) unter Beibehaltung aller Ebenen (bei Photo-Paint „Objekte“ genannt) öffnen und auch als solche wieder speichern. Die Version X5 kann dementsprechend Photoshop CS4-Dateien nativ öffnen und bearbeiten.\n\nIn der Version X5 ist das Farbmanagement vollständig neu programmiert worden und entspricht nun dem weltweiten Industriestandard. Dies führt teilweise zu Farbunterschieden beim Bearbeiten von Dateien aus vorherigen Versionen. \n\nPhoto-Paint kann durch photoshopkompatible Plugins erweitert werden.\n\nWie Corel Draw bietet auch Photo-Paint die Möglichkeit, mit Programmiersprachen die Funktionalität zu erweitern. Zum einen kann seit der Version 9 Visual Basic genutzt werden, zum anderen die Corel-eigene Programmiersprache Corel Script, die jedoch seit der Version 9 nicht mehr dokumentiert ist und ein Basic-Dialekt ist. Bis dahin gab es auch einen eigenen Script-Editor mit einer ausführlichen Hilfe zu allen Befehlen.\nZeichnet man in Photo-Paint mit dem enthaltenen Rekorder eine Befehlskette auf und speichert diese ab, so kann man diese Datei problemlos mit einem Texteditor öffnen und bearbeiten.\nDiese Scripte lassen sich dann auch in die Stapelverarbeitung von Photo-Paint integrieren und somit auf viele Bilder anwenden.\n\n\nCorel Photo-Paint bietet das proprietäre Dateiformat CPT mit der Extension .cpt an. Bis Version 6 handelt es sich dabei um TIFF-Dateien. Bei höheren Versionen können die Dateien nur noch mit Werkzeugen von Corel betrachtet, konvertiert und bearbeitet werden.\n\nDie Formatversion kann mittels eines Text- oder Hex-Editors herausgefunden werden. Bei Dateien, die beispielsweise mit Corel Photo Paint 9 oder 10 geschrieben wurden, steht CPT9FILE in der ersten oder zweiten Zeile.\n\n"}
{"id": "1142964", "url": "https://de.wikipedia.org/wiki?curid=1142964", "title": "Digitales Malen", "text": "Digitales Malen\n\nDigitales Malen (engl.: \"Digital Painting\"), oder digitales Zeichnen, bezeichnet die Erstellung von Bildern am Computer mit Hilfe von Grafiksoftware und Eingabegeräten. Dabei kann die Anmutung traditioneller Maltechniken, wie beispielsweise Aquarell, Ölmalerei oder Impasto imitiert werden.\n\nEinen ersten Höhepunkt hatte das digitale Malen in der so genannten „Paintbox-Ära“, als ab Mitte der 1980er Jahre leistungsfähige und bezahlbare Computer wie auch Malprogramme erstmals für jedermann zur Verfügung standen und vor allem junge Leute die neuen Möglichkeiten digitaler Bilderstellung erkundeten. In dieser Zeit wurde die Behauptung aufgestellt, mit dem digitalen Malen würde eine neue Kunstrichtung entstehen. Heute ist digitales Malen eine von vielen Möglichkeiten, Bilder auf digitalem Weg zu erschaffen. Gute Bildbearbeitungsprogramme und spezielle Mal- und Zeichenprogramme gibt es in großer Auswahl. Der Begriff \"digitales Malen\" ist heute vor allem in der kommerziellen Illustration, im Grafikdesign sowie in der Populärkunst geläufig.\n\nVerbreitet ist das digitale Malen unter Illustratoren und Gestaltern z. B. von Bildwelten der Computerspiele und gemalter Szenerien für Kinofilme. Hier werden die schnelle Arbeitsweise, die Korrigierbarkeit jedes Arbeitsschrittes und die Kopierbarkeit der Bilder geschätzt. Die Bearbeitung digitaler Dateien ermöglicht die direkte Erstellung druckfähiger Versionen der grafischen Arbeit. Ein wichtiger Vertreter dieses Genre ist der Amerikaner Craig Mullins, der 1994 als einer der ersten Grafikdesigner eine kommerzielle Produktion ausschließlich digital erstellt hat.\n\nDigitales Malen ist ebenso wie die traditionellen Mal- und Zeichenverfahren eine Technik, die erst im künstlerischen Prozess zu einem Mittel der Produktion von Kunst wird. Die Möglichkeiten des digitalen Malens werden heute neben anderen Bildbearbeitungsmethoden von vielen Künstlern verwendet. Der Begriff des „digitalen Malens“ taucht dabei im Kunstmarkt nicht auf, stattdessen sind Angaben zur Technik des Ausgabemediums gebräuchlich, darunter \"Digitalprint\", \"Giclée Print\", \"Pigment Print\" oder \"C-print digital.\"\nKünstlerisch geht es bei der Arbeit mit den Mal- und Zeichenprogrammen weniger um eine Imitation traditioneller Maltechniken, als um die Nutzung der speziellen Möglichkeiten dieser Programme. So hat etwa Julian Opie in seinen reduzierten Personendarstellungen das Moment der Bewegung eingeführt, das sich auf Flachbildschirmen als Animation darstellen lässt. Das Künstlerduo Bittermann & Duka setzt ab circa 2002 am Computer hergestellte Malerei als Teil eines größeren künstlerischen Konzeptes ein. Einige Arbeiten sind unmittelbar auf der Website eines ihrer künstlerischen Projekte zu sehen, andere sind nur ein Schritt hin zu einer auch plastisch umgesetzten Materialisation z. B. im Gartenprojekt Hentzelpark in Rolandswerth. Mit den Mitteln digitaler Bildproduktion arbeiten heute zudem die Mehrzahl der ursprünglich fotografisch arbeitenden Künstler. Ein prominentes Beispiel ist die Serie „Nudes“ des Fotografen Thomas Ruff, in der Ruff Bildvorlagen aus dem Internet (Pornobilder) mit einer Reihe bekannter Mal- und Fotofilter so bearbeitet hat, dass die (großformatigen) digitalen Fotoprints dieser Serie schließlich der Malerei Gerhard Richters aus den 1960er Jahren ähneln (Verwischungen).\n\nWaren bis in die 1980er Jahre Malvorlagen, z. B. Malen nach Zahlen, unter künstlerischen Laien ein beliebter Einstieg in eine bildnerische Gestaltung, so wird diese Zielgruppe heute von Softwareherstellern wie Corel mit Slogans wie „Aus Fotos werden Gemälde“ umworben. Computer gehören inzwischen zur Grundausstattung eines Haushalts und entsprechend gibt es für die Mal- und Zeichenprogramme inzwischen einen großen Absatzmarkt. Neben den gerade für den Laien wichtigen technischen Vorteilen des digitalen Gestaltungsprozesses (s. u.) ist für viele Anwender die direkte Veröffentlichungsmöglichkeit der digitalen Dateien über das Internet ein wichtiger Aspekt. Während für den ambitionierten Laien früher eine öffentliche Präsentation seiner Malergebnisse ein großes Problem darstellte, stehen insbesondere für den Anwender der digitalen Maltechniken heute eine Vielzahl von Websites bereit, die meist gegen Bezahlung die digitalen Malereien über das Internet veröffentlichen. Ein zweiter Weg, der zunehmend an Bedeutung gewinnt, ist die Veröffentlichung einer eigenen Website. Eine Sichtung dieser Sites, die meist ein „art“ oder „Kunst“ als Namensbestandteil führen, zeigt dabei in diesem neu entstehenden Sektor der Populärkunst erkennbare stilistische Schwerpunkte. Besonders beliebt sind in diesem Sektor der digitalen Malerei dem Impressionismus und dem Surrealismus nachempfundene Bilder sowie jede Art von Fantasykunst.\n\nSoftware für digitales Malen gibt es sowohl in Form spezieller Malprogramme wie auch als integraler Bestandteil von Bildbearbeitungs-Programmen (für eine Liste von Grafikprogrammen siehe dort).\n\nInnerhalb der Programme lassen sich zwei grundlegende Anwendungsmöglichkeiten unterscheiden. \nIn den Mal- und Bildbearbeitungsprogrammen werden Werkzeuge zum Zeichnen/Malen von Details oder zum kompletten Aufbau eines Bildes benutzt, deren Werkzeugspitzen („Malpinsel“ und „Zeichenstift“ und andere) in Technik (z. B. „Aquarell“ oder „Ölmalerei“) und Stärke in einer großen Bandbreite variiert werden können. Teilweise werden diese Werkzeuge in einer Ausstattungsform angeboten, die den Hilfsmitteln eines traditionellen Malers nachgeahmt sind: eine „Leinwand“, Zeichenwerkzeuge, Mischpaletten und eine Vielzahl von Farboptionen. \n\nDa die Arbeit an einer Vorlage grundsätzlich in übereinander gelagerten Ebenen erfolgen sollte, können Veränderungen immer wieder radiert, gelöscht oder verändert werden. Über das Kopieren eines Bildes können Variationen effektiv angefertigt werden.\n\nDaneben können mit Mal-Filtern ganze Bildvorlagen (z. B. Fotos) komplett in einem Arbeitsgang in eine Grafik, die z. B. eine traditionelle Maltechnik imitiert, umgewandelt werden. Hierbei gibt es jeweils eine Reihe von Variationsmöglichkeiten, wie stark und in welchem Duktus die Effekte angewendet werden sollen. \n\nBeide Wege, die Arbeit mit Werkzeugspitzen und diejenige mit Filtern, werden technisch als Rastergrafiken umgesetzt. Dies hat zur Folge, dass einmal gewählte Bildgrößen nicht beliebig vergrößert werden können, ohne die Bildqualität durch das Entstehen von Pixeltreppen oder unscharfen Kanten zu mindern. Dementgegen stehen Vektorgrafikprogramme, die das Erstellen verlustfrei skalierbarer Grafiken erlauben. Genauere Informationen hierzu finden sich im Artikel Grafiksoftware.\n\nDas Digitale Malen kann mit Hilfe der Maus oder aber spezielle Eingabegeräte durchgeführt werden. Für genaues Arbeiten ist ein Tablett empfehlenswert, das mit einer Sensoroberfläche genau auf darauf ausgeführten Stiftbewegungen und den ausgeübten Druck reagiert und diese Informationen an das Programm weiterleitet. Dadurch sind fließende Striche und auch feine verlaufende Lasuren möglich. Eine weitere Möglichkeit ist die Verwendung von Tablet PCs. Diese verfügen über ein Grafiktablett, welches direkt im Display der Rechner integriert ist und erlauben somit ein direktes Malen/Zeichnen auf der Zeichenoberfläche der Grafikprogramme.\n\nPenabled Display – sogenannte „Stift-Displays“ ermöglichen dem Anwender direkt auf dem Monitor zu malen. Diese Geräteart lässt sich sowohl an Mac- als auch PC-Systemen anschließen. Unterschiedliche Stifte simulieren z. B. Airbrushgerät, Marker, Bleistift und Filzer.\n\nDie digitalen Bilddateien können als Fotoprint wie auch als Inkjetprint auf einer großen Bandbreite von Materialien, darunter auch Leinwand, ausgegeben werden. Die genaue Materialisation ist dabei von einer großen Zahl von Faktoren (u. a. Farbprofil) abhängig, so dass erst im Prozess dieser Ausgabe, ähnlich wie im Offsetdruck, die exakte Wirkung bestimmt wird. Für eine Publikation über das Internet sind die digitalen Bilddateien direkt verwendbar.\n\n\n\n"}
{"id": "1145479", "url": "https://de.wikipedia.org/wiki?curid=1145479", "title": "Blockgrafik", "text": "Blockgrafik\n\nAls Blockgrafik bezeichnet man eine aus eigens dafür vorgesehenen Zeichen zusammengesetzte Grafik.\n\nAuf vielen Heimcomputern der späten 1970er und frühen 1980er Jahre konnten einzelne Bildschirmpixel nicht direkt angesteuert werden, vielmehr las der Grafikchip die auszugebenden Pixel selbst aus einem Zeichensatz-ROM, s. a. bei Zeichengenerator. Eine Grafikausgabe im heutigen Sinne war damit unmöglich. Um dennoch eine eingeschränkte Form der Grafikdarstellung zu erlauben, nahmen die Entwickler spezielle Zeichen in den Zeichensatz auf, die beispielsweise eine Linie, eine Ecke oder eine mit einem Muster gefüllte Flächen darstellten. Auch der Zeichensatz des C64 (s. a. CBM-ASCII) und der VGA-Karten (siehe bei Codepage 437) enthielt noch solche Zeichen zur Darstellung von Blockgrafiken. Heute findet man ähnliches im Unicode-Standard, beispielsweise den Unicodeblock Rahmenzeichnung (U+2500..U+257F) und den darauf folgenden Unicodeblock Blockelemente. Auch Programme, die in einer Terminalemulation laufen, setzen teilweise Blockgrafik ein.\n\nMit Blockgrafikzeichen konnte zur Not auch Einzelpunktgrafik dargestellt werden, allerdings in wesentlich geringerer Auflösung. Die Commodore-8-Bit-Computer hatten alle 16 Varianten von Zeichen in ihrem PETSCII-Zeichensatz, um je Zeichenposition eine 2×2-Punktmatrix (also „Viertelkästchen“) darstellen zu können. Bei einer Zeichenauflösung von 40×25 bzw. 80×25 ergab das Punktauflösungen von bescheidenen 80×50 bzw. 160×50. Der Tandy TRS-80 Model 1 bot Grafiksymbole mit einer 2x3 Matrix bei einer Zeichenauflösung von 64×16 an, was zu einer Punktauflösung von 128×48 führte. Dies reichte immerhin aus, die erste Version des \"Flight Simulator\" von Sublogic zu implementieren. Anders als bei Commodore wurde die Grafik auch durch entsprechende Basicbefehle unterstützt. Auch die Sinclair ZX-80/81 Rechner boten eine 2x2-Grafik mit Basicunterstützung.\n\nDie genannten Commodore-Computer boten zusätzlich Grafikzeichen an, mit denen man wahlweise in x- oder in y-Richtung die volle Punktauflösung ausreizen konnte, in der anderen Koordinatenrichtung war man dann aber auf die Zeichenauflösung beschränkt. Konkret konnte man damit Darstellungen von 40×200 bzw. 80×200 und 320×25 bzw. 640×25 erreichen. Besonders die 80×200-Auflösung bot schon praktische Hilfe z. B. bei der Untersuchung von Details mathematischer Kurvenverläufe.\n\nDarüber hinaus enthielten die meisten Zeichensätze von Heimcomputern noch Spielkartensymbole und andere Zeichen, die in Computerspielen von Nutzen waren. Dagegen waren die Grafiksymbole im IBM-PC mehr auf Rahmen- und Fenstergestaltung ausgelegt, also eindeutig Richtung Büroanwendungen.\n\nBlockgrafik ist nicht zu verwechseln mit ASCII-Art, bei der auf Sonderzeichen außerhalb der im ASCII-Standard definierten Zeichen explizit verzichtet wird, um eine plattformunabhängige Darstellung zu erreichen. Blockgrafik-Zeichen dagegen waren herstellerabhängig unterschiedlich (s. o.).\n"}
{"id": "1147958", "url": "https://de.wikipedia.org/wiki?curid=1147958", "title": "IMSAI 8080", "text": "IMSAI 8080\n\nDer Mikrocomputer IMSAI 8080 war ein früher Personal Computer mit Intel-8080-Mikroprozessor und S-100-Bus, der seit 1975 vom US-Unternehmen IMSAI, einer Abkürzung für Information Management Science Associates, Incorporated, hergestellt wurde.\n\nDa der IMSAI vollständig kompatibel mit seinem stärksten Konkurrenten MITS Altair 8800 war, sind die technischen Details identisch. Der IMSAI hatte lediglich ein anderes Design mit großen roten und blauen Schaltern sowie ein stärkeres Netzteil. Als Betriebssystem verwendete er eine stark veränderte CP/M-Version namens IMDOS.\n\nDer Preis des Bausatzes betrug anfangs 439 US-Dollar, wurde aber wegen der großen Nachfrage auf 499 Dollar erhöht. Zwischen 1975 und 1979 wurden etwa 17.000 bis 20.000 Einheiten produziert.\n\nIm Mai 1972 hat William Millard die Geschäfte von seiner Privatwohnung gestartet. 1973 gründete er IMS Associates und schloss mehrere Software-Verträge ab. 1974 gab es einen wichtigen Vertrag mit General Motors, die mehrere Workstations mit Peripherie, teils mit Festplatten, bestellten. Zu dieser Zeit wurde der neue Intel 8080 angekündigt, zuvor gab es den sehr einfachen Intel-4004-Prozessor. Am 16. Dezember 1975 wurden die ersten IMSAI-8080-Bausätze ausgeliefert.\n\nIm Oktober 1979 meldete IMSAI Konkurs an. Der Markenname IMSAI wurde an die Firma Fischer-Freitas verkauft.\n\nEinen IMSAI 8080 benutzte die Hauptfigur David L. Lightman im Spielfilm \"WarGames – Kriegsspiele\" aus dem Jahr 1983. Dieses Exemplar war noch bis mindestens 2015 lauffähig und stand damals zum Verkauf. \n\n\n"}
{"id": "1148897", "url": "https://de.wikipedia.org/wiki?curid=1148897", "title": "Microsoft Windows NT 4.0", "text": "Microsoft Windows NT 4.0\n\nWindows NT 4.0, häufig abgekürzt als \"NT4\", ist ein Betriebssystem von Microsoft und der Nachfolger von Windows NT 3.51. Es wurde am 29. Juli 1996 veröffentlicht und war zunächst in den Varianten Workstation und Server verfügbar. Es folgten die Enterprise Edition für große Netzwerke, die Terminal Server Edition für den Einsatz als Terminalserver und Embedded für spezielle Rechner. Zur Behebung von Programmfehlern veröffentlichte Microsoft insgesamt sechs Service Packs. Windows NT 4.0 war das letzte Betriebssystem der Windows-NT-Reihe, welches für MIPS-, PowerPC- und Alpha-AXP-Prozessoren erhältlich war.\n\nDie Bedienung von Windows NT 4.0 wurde durch die Verwendung der grafischen Benutzeroberfläche von Windows 95 und den Einsatz von Assistenten zur Konfiguration vereinfacht. Das Betriebssystem war auf die wachsende Bedeutung des Internets orientiert; die Workstation-Variante enthielt den Internet Explorer, der Server beinhaltete mit dem Internet Information Server erstmals einen Webserver direkt im Lieferumfang. Verbessert wurde zudem die Integration des Betriebssystems in Unix- und Netware-Netzwerke. Durch die Verlagerung der Grafikkomponenten in den Betriebssystemkern konnte Windows NT 4.0 eine Geschwindigkeitssteigerung gegenüber früheren Versionen erzielen.\n\nWenngleich Windows NT 4.0 zunächst als unzuverlässig galt und Probleme mit den Service Packs dem Betriebssystem einen schlechten Ruf einbrachten, so konnte das Betriebssystem seinen Marktanteil dennoch ausbauen. Es wurden über 25 Millionen Lizenzen von Windows NT Workstation 4.0 verkauft, der Server konnte seinen Marktanteil auf über 40 Prozent ausbauen. Auch nach der Veröffentlichung des Nachfolgers Windows 2000 bzw. dessen Nachfolgers Windows Server 2003 waren zahlreiche Computer mit Windows NT 4.0 in Betrieb. Das eigentliche Ziel von Microsoft, mit dem Betriebssystem auch im oberen Segment des Servermarkts präsent zu sein, konnte jedoch nicht erreicht werden.\n\nWindows NT 4.0 wurde im Juli 1995 unter der Bezeichnung \"Windows NT 3.6\" zum ersten Mal offiziell angekündigt. Zu den größten geplanten Neuerungen der Version zählte die Benutzeroberfläche von Windows 95, die ursprünglich erst für das objektorientierte Betriebssystem Cairo vorgesehen war. Das Betriebssystem sollte Anfang 1996 veröffentlicht werden. Im November 1995 kündigte Microsoft an, die ebenfalls erst für Cairo vorgesehene Technologie \"Netzwerk-OLE\" zusammen mit dem neuen Betriebssystem zu entwickeln. Sie solle aber nicht zusammen mit dem Betriebssystem, sondern erst später als Teil eines Service Packs veröffentlicht werden.\n\nDie Industrie erwartete drei zentrale Punkte vom neuen Betriebssystem: es sollte so stabil sein wie die Vorgängerversionen von Windows NT, mit dem versprochenen Netzwerk-OLE eine hohe Leistung erzielen können, aber gleichzeitig nicht mehr als 16 Megabyte Arbeitsspeicher verbrauchen. Besonders die Funktion Netzwerk-OLE stand dabei im Fokus der Industrie, um darauf aufbauend Anwendungen entwickeln zu können. Entwickler, die bereits eine erste nichtöffentliche Vorversion von Windows NT 4.0 erhielten, beschrieben die Funktion noch als zu unausgereift. Weiterhin war unklar, ob diese Funktion bereits in Windows NT 4.0 oder, wie zunächst angekündigt, erst zusammen mit einem Service Pack kommen sollte. Später stellte Microsoft klar, dass Netzwerk-OLE mit dem zweiten Betatest Teil des Betriebssystems sein würde. Ende Januar 1996 fand der erste öffentliche Betatest von Windows NT 4.0 statt; über 120.000 Kopien wurden an Tester verschickt.\n\nIm Mai 1996 startete der zweite Betatest, und 200.000 Tester erhielten eine neue Vorversion. Diese enthielt auch das versprochene Netzwerk-OLE. Im Release Candidate entschied sich Microsoft, Windows NT Workstation 4.0 weiter zu beschränken, indem nur zehn verschiedene IP-Adressen innerhalb von zehn Minuten eine Verbindung aufbauen konnten. Vor allem Netscape Communications vermarktete zu dieser Zeit Webserver für die Workstation-Version von Windows NT, was durch diese Beschränkung gestoppt werden sollte. Nach massiver Kritik an dieser Beschränkung, die das Produkt im Internet unbenutzbar mache, kündigte Microsoft wenig später an, die Beschränkung wieder aufzuheben. Am 29. Juli 1996 wurden Windows NT Workstation 4.0 und Windows NT Server 4.0 schließlich veröffentlicht. Das Betriebssystem besteht aus 16,5 Millionen Codezeilen, und bis zu diesem Zeitpunkt wurden 400 Millionen USD in die Entwicklung von Windows NT gesteckt.\n\nDer \"Mainstream Support\" für Windows NT 4.0 endete am 30. Juli 2002 (31. Dezember 2002 beim Server). Der \"Extended Support\" sollte ursprünglich 2003 beendet werden, wurde aber um ein Jahr verlängert und lief letztendlich am 30. Juni 2004 (31. Dezember 2004 beim Server) aus. Bei Windows NT 4.0 Embedded gestaltete sich der Supportzeitraum aufgrund der späten Veröffentlichung anders; der \"Mainstream Support\" endete am 30. Juni 2003, der \"Extended Support\" am 11. Juli 2006.\n\nMit der Zeit stellte Microsoft die Unterstützung alternativer Architekturen schrittweise ein. Allen diesen Architekturen war gemein, dass es ihnen an Software und Treibern mangelte. Zwar waren sämtliche unterstützten Architekturen von Windows NT zueinander quelltextkompatibel, aber da Microsoft keine Cross-Compiler bereitstellte, mussten Entwickler einen PC einer bestimmten Architektur besitzen, um Windows-NT-Anwendungen auf ihr portieren zu können.\n\nZunächst entschied sich Microsoft im Oktober 1996, die MIPS-Version von Windows NT nicht länger weiterzuentwickeln. Auch NEC als größter OEM-Kunde der MIPS-Version von Windows NT kündigte an, die MIPS-Plattform aufzugeben und auf x86-basierte Rechner umzusteigen. Anfang 1996 waren unter 1,5 Millionen Rechnern, die Windows NT einsetzten, lediglich 23.000 MIPS-Rechner, deren Verbreitung sich hauptsächlich auf Japan beschränkte. Zum Zeitpunkt der Veröffentlichung von Windows NT 4.0 wurden, selbst unter alleiniger Betrachtung von RISC-Rechnern, 99 % aller Exemplare des Betriebssystems zusammen mit Alpha- oder PowerPC-Rechnern verkauft, die Verkaufszahlen für MIPS-Rechner mit Windows NT beliefen sich auf weniger als 1 %.\n\nIm Dezember 1996 beendeten IBM und Motorola die Auslieferung von PowerPC-Computern mit Windows NT, und im Februar 1997 stellte Microsoft nunmehr auch die PowerPC-Version von Windows NT ein. Offiziell begründeten die Firmen diesen Schritt zwar mit dem geringen Erfolg der PowerPC-Plattform, aber nur wenige Monate zuvor äußerten zahlreiche Unternehmen ihre Absicht, ihre Windows-NT-Anwendungen auch auf den PowerPC portieren zu wollen. Um diesen Schritt verbreiteten sich daher schnell Gerüchte, etwa dass Intel auf Microsoft Druck ausgeübt haben soll und Microsoft daraufhin die weitere Unterstützung der PowerPC-Plattform von einer millionenschweren Zahlung abhängig gemacht haben soll.\n\nDie Version für Alpha-Prozessoren konnte sich wegen seiner Verbreitung länger am Markt halten. Der größte Vorteil gegenüber den anderen Architekturen war FX!32, ein Programm von DEC, dem Entwickler der Alpha-Prozessoren, mit dem auch 32-Bit-Windowsanwendungen auf dem Alpha-Prozessor ausgeführt werden konnten, indem das Programm bei seiner ersten Verwendung übersetzt wurde. Aufgrund der hohen Leistung der Alpha-Prozessoren zeigten diese übersetzten Programme kaum Geschwindigkeitseinbußen gegenüber der x86-Version von Windows NT, obschon sie nativen Alpha-Anwendungen leistungsmäßig weiterhin unterlegen waren. DEC wurde dann jedoch von Compaq aufgekauft. Zwar versprach Compaq, den Alpha-Prozessor weiterhin zu unterstützen, aber am 20. August 1999 kündigte das Unternehmen völlig überraschend an, die Entwicklung von Windows NT 4.0 und Windows 2000 für den Alpha-Prozessor, das sich bereits in der Release-Candidate-Phase befand, einzustellen. Microsoft folgte diesem Schritt umgehend. Beide Unternehmen einigten sich dennoch, zumindest das Service Pack 6 für Windows NT 4.0 auf den Alpha-Prozessor zu portieren.\n\nIm Oktober 1995 wurden erste Details über eine Unterstützung von Clustering bekannt. Diese sollte das Ziel von Windows NT als Betriebssystem für sicherheitskritische Anwendungen unterstützen. Dabei sollten zwei Rechner so zusammengeschaltet werden, dass bei einem Ausfall eines Rechners der andere Rechner dessen Aufgaben übernehmen kann. Im März 1996 bekamen ausgewählte Entwickler erste APIs dieser Technologie, die nun unter dem Codenamen \"Wolfpack\" bekannt war. Das Betriebssystem sollte mit dieser Funktion in direkter Konkurrenz zu Großrechnern gestellt werden, die in der Regel die Betriebssysteme Solaris und OpenVMS verwendeten. Zwei Versionen waren geplant; die erste Version sollte die Verbindung von zwei Rechnern ermöglichen, eine spätere Version sollte schließlich Unterstützung für 16 Rechner bieten.\n\nIm Dezember 1996 wurde eine Vorversion der Clusteringsoftware \"Wolfpack\" an Serverhersteller ausgeliefert. Im März 1997 hieß es, dass Microsoft bei der Entwicklung auf Probleme stieß und die Vorversion noch unausgereift sei. Dies könnte die Veröffentlichung des Produkts bis ins nächste Jahr verzögern. Zwei Monate später kündigte man die Veröffentlichung des Produkts für den Sommer an. Die Erwartungen an die spätere Version, die ursprünglich die Verbindung von bis zu 16 Rechnern ermöglichen sollte, wurden indes deutlich zurückgeschraubt; diese sollte nun lediglich eine Verbindung von vier Rechnern zu einem Cluster ermöglichen. Im September 1997 folgte die Herausgabe des Produkts als \"Windows NT Server 4.0 Enterprise Edition\".\n\nAm 12. Mai 1997 lizenzierte Microsoft eine Mehrbenutzertechnologie vom Unternehmen Citrix. Diese kam zuvor im von Citrix entwickelten Produkt WinFrame, einer modifizierten Version von Windows NT 3.51, vor. Mithilfe dieser Technologie sollte die Entwicklung zu sogenannten Thin Clients angestoßen werden, die sich lediglich mit einem Server verbinden und alle Programme auf dem Server ausführen. Microsoft und Citrix beschlossen zudem eine gemeinsame Entwicklung am resultierenden Produkt, welche als Bestandteil von Windows NT Server 4.0 sowie Windows NT Server 5.0, dem späteren Windows 2000 Server, vermarktet werden sollte. Microsoft wollte mit der Möglichkeit, Anwendungen auf einem Server laufen zu lassen und diese von einem beliebigen Client zu steuern, unter anderem mit Suns Java-Technik konkurrieren.\n\nIm Juni 1997 kündigte Microsoft diese Technologie unter dem Codenamen \"Hydra\" an. Am 17. November 1997 erhielten etwa 1.000 Tester eine Vorversion des Produkts. Eine zweite Vorversion folgte im März 1998. Das Produkt wurde schließlich unter dem Namen \"Windows NT Server 4.0 Terminal Server Edition\" am 16. Juni 1998 veröffentlicht.\n\nMit \"Windows NT 4.0 Embedded\", das im November 1998 angekündigt wurde, wagte Microsoft den Einstieg in den Markt der Embedded Systeme. Nach einem Betatest im Februar 1999 kam das Produkt am 9. August 1999 heraus. Man erwartete, dass im Jahr 2002 15 % aller Embedded-PCs der oberen Preisklasse Windows NT 4.0 Embedded einsetzen werden, und dass sich dieser Wert bis zum Jahr 2005 auf 30 % erhöhen wird. Compaq kündigte sogleich an, künftig Terminals mit Windows NT 4.0 Embedded auszustatten.\n\nInsgesamt wurden sechs Service Packs für Windows NT 4.0 veröffentlicht. Diese konnten sowohl kostenlos aus dem Internet heruntergeladen, als auch auf einer CD-ROM bestellt werden, die zusätzlich noch einige weitere Programme enthielt. Die Service Packs waren kumulativ, sodass nur das neueste Service Pack installiert werden musste, um das Betriebssystem auf den neuesten Stand zu bringen. Die Service Packs konnten sowohl für die Workstation als auch für den Server und die Enterprise Edition verwendet werden. Lediglich die Terminal Server Edition benötigte eigene Service Packs, die nicht mit den anderen Varianten von Windows NT 4.0 kompatibel waren.\n\nIn vorherigen Versionen von Windows NT dienten Service Packs allgemein als Aktualisierungen, die Fehler behoben und neue Funktionen hinzufügten. Angesichts der Tatsache, dass für den Vorgänger Windows NT 3.51 bereits vier Service Packs veröffentlicht wurden und ein fünftes in Entwicklung war, entwarf Microsoft ein neues System für Windows NT 4.0. Service Packs sollten lediglich Fehler beheben, neue Funktionen sollten ausschließlich in Form von sogenannten \"Option Packs\" erscheinen. Letztendlich gab es aber nur ein Option Pack für Windows NT 4.0, und die Service Packs, vor allem das Service Pack 4, enthielten wie zuvor auch neue Funktionen.\n\nNach der Veröffentlichung von Windows NT 4.0 wurden schwere Fehler im Betriebssystem bekannt. Unter bestimmten Umständen konnten Daten beim Zugriff beschädigt werden. Daraufhin veröffentlichte Microsoft am 12. Oktober 1996 das erste Service Pack. Dieses Service Pack ist auf späteren Versionen der CD bereits integriert, sodass es nicht mehr installiert werden muss.\n\nAm 19. Dezember 1996 folgte das zweite Service Pack. Dieses behob über 100 Programmfehler, unter anderem ein Speicherleck im IIS sowie Probleme mit DHCP. Als erstes Service Pack bot es eine Deinstallationsroutine an. Nach der Veröffentlichung häuften sich Beschwerden über Systeme, die nach dem Installieren dieses Service Packs nicht mehr funktionierten. Microsoft gab zwar zu, dass das Service Pack inkompatibel mit Norton AntiVirus sei und es ein Problem mit der RAS-Verbindungsfunktion gab, aber Anwender fanden zahlreiche weitere Fehler, die auf das Service Pack 2 zurückzuführen seien. Microsoft reagierte mit der Ankündigung, in Zukunft auch bei Service Packs einen Betatest durchzuführen, bei der das Service Pack von Kunden auf Fehler geprüft wird. Später stellte sich heraus, dass das Service Pack 2 mehrere Fehler enthielt, die unter anderem zu Datenkorruption führen konnten.\n\nNach den Erfahrungen mit dem letzten Service Pack testete Microsoft im März 1997 das sich in Entwicklung befindliche Service Pack 3 zunächst zusammen mit 300 Kunden. Die Veröffentlichung folgte am 15. Mai 1997. Dieses Service Pack enthielt eine vollständige Implementierung von DirectX 3.0, einschließlich der 3D-Komponente Direct3D, die bisher in Windows NT 4.0 fehlte.\n\nDie Sicherheit des Betriebssystems wurde auch in der Presse zu einem zunehmend bedeutenderen Thema. Ein Kritikpunkt an Windows NT 4.0 war, dass die Daten des Security Accounts Manager (SAM), darunter die Passwörter der Benutzer, im MD4-Algorithmus gespeichert sind, der als unsicher galt. Um die Sicherheit dieser Datenbank zu verbessern, führte Microsoft das optionale Programm SYSKEY.EXE ein. Dieses verschlüsselt die SAM mit einem Schlüssel, der selbst verschlüsselt ist und nur mithilfe eines Passworts entschlüsselt werden kann. Dieses Passwort kann entweder auf dem Rechner gespeichert werden, oder beim Starten des Rechners von Benutzer verlangt werden, entweder durch manuelle Eingabe oder mittels einer Diskette.\n\nEnde 1997 erschien das erste und einzige \"Windows NT 4.0 Option Pack\", um die Zeit bis zur Veröffentlichung eines Nachfolgers von Windows NT 4.0 zu überbrücken. Dieses Paket setzte ein installiertes Service Pack 3 voraus, es aktualisierte bestehende Komponenten von Windows NT 4.0 und installierte zusätzliche Komponenten. Zu den aktualisierten Komponenten zählen etwa der Internet Information Server 4.0 oder auch die RAS-Verbindungsverwaltung. Unter den neuen Komponenten befinden sich die Microsoft Management Console zur effizienteren Computerverwaltung, der Microsoft Script Debugger und der Windows Script Host.\n\nDie Installation des Option Packs auf der Enterprise Edition des Windows NT Servers 4.0 ist problematisch, sofern zwei Computer zu einem Cluster zusammengeschlossen sind. Einige Komponenten des Option Packs funktionieren nicht innerhalb eines Computerclusters und bei bestimmten Konstellationen von Option Pack und Cluster Server kann es beim Ausfall eines Rechners zu Datenkorruption kommen. Das Option Pack kann zwar prinzipiell auch auf der Terminal Server Edition installiert werden, dies wird aber von Microsoft nicht unterstützt, da es auch hier zu Problemen mit bestimmten Komponenten kommt.\n\nDas Service Pack 4 folgte am 21. Oktober 1998, die Version für die Terminal Server Edition wurde am 5. April 1999 nachgereicht. Es behob zahlreiche Jahr-2000-Probleme und fügte die Unterstützung des Eurozeichens hinzu. Im Falle eines zuvor installierten Option Packs korrigierte das Service Pack auch Programmfehler dieses Paketes. Erstmals konnten mit dem Service Pack 4 Festplatten benutzt werden, die größer sind als 8 GB. Zudem enthielt das Service Pack 4 einen aktualisierten NTFS-Treiber, mit dem auf Partitionen des Nachfolgers Windows 2000 zugegriffen werden kann, zulasten der Kompatibilität mit älteren Versionen von Windows NT 4.0. Funktionen, die erst mit Windows 2000 eingeführt wurden, wie verschlüsselte Dateien, können in Windows NT 4.0 jedoch nicht verwendet werden. Um die Sicherheit der Authentifizierung zwischen Client und Server zu verbessern, führte das Service Pack 4 die NTLMv2-Authentifizierung ein, welche unter anderem MD5 anstelle von DES verwendet.\n\nUrsprünglich sollte das Service Pack 4 das letzte Service Pack für Windows NT 4.0 werden, aber als das Betriebssystem am Kryptografietest der US-Regierung nach FIPS 140-1 scheiterte, sah sich Microsoft gezwungen, ein neues Service Pack zu entwickeln, um nicht die US-Regierung als Kunden zu verlieren. Zudem erschienen erneut Berichte über Probleme nach der Installation des Service Pack 4, ähnlich wie es zuvor beim Service Pack 2 vorgekommen war. Gerüchte, wonach ein Service Pack 4a geplant sei, dementierte Microsoft jedoch.\n\nDie Probleme des vorhergehenden Service Packs behob Microsoft mit dem Service Pack 5. Dieses erschien am 2. Mai 1999, die Version für die Terminal Server Edition folgte am 19. November 1999. Im Gegensatz zu den vorherigen Service Packs enthielt das Service Pack 5 keine Neuerungen, was Microsoft mit dem geringen zeitlichen Abstand zum letzten Service Pack begründete.\n\nDas Service Pack 6, welches am 28. Oktober 1999 erschien, löste die Probleme, die zum Scheitern des Kryptografietests führten. Kurz nach der Veröffentlichung traten allerdings erneut Probleme bei Benutzern auf, da der im Service Pack 6 enthaltene TCP-Protokollstapel defekt war; dies äußerte sich unter anderem dahingehend, dass die Verbindung des Programms Lotus Notes, eines der am weitesten verbreiteten Drittanbieterprogramme der damaligen Zeit, mit dem Server fehlschlug, sofern der Benutzer keine Administratorrechte besaß. Daraufhin warnte Microsoft vor der Installation des Service Pack 6 und gab einige Tage später einen Hotfix heraus, der dieses Problem für Systeme, auf denen das Service Pack 6 bereits installiert war, behob. Am 23. November 1999 schließlich veröffentlichte Microsoft ein überarbeitetes SP6 unter der Bezeichnung \"Service Pack 6a\". Dieses behob zugleich einen Y2K-Bug im Internet Information Server sowie einen Fehler, der beim Zugriff auf Macintosh-Server zu einem Blue Screen führte. Im Mai 2000 wurde die Version des Service Packs 6 für die Terminal Server Edition herausgebracht.\n\nAls die Veröffentlichung von Windows 2000 bevorstand und spekuliert wurde, ob das Service Pack 6a das letzte Service Pack sein würde, veröffentlichte Microsoft eine gegenteilige Aussage auf seiner Website. Die Veröffentlichung des \"Service Packs 7\", welches nunmehr das letzte Service Pack sein sollte, war ursprünglich Ende 2000 geplant. Es sollte unter anderem einen Active-Directory-Client für Windows NT 4.0 beinhalten. Dieser Termin verzögerte sich jedoch bis in das dritte Quartal des Jahres 2001, und schließlich entschied sich Microsoft, die Entwicklung im April 2001 einzustellen. Die offizielle Begründung für diesen Schritt lag im fehlenden Bedarf, da die Anzahl an entdeckten Problemen seit dem Service Pack 6 stark zurückgegangen ist; zudem seien die Kunden bereits mit den bestehenden Service Packs zufrieden. Als Ersatz für das Service Pack 7 veröffentlichte Microsoft am 26. Juli 2001 ein sogenanntes \"Security Rollup Package\", welches das Service Pack 6a voraussetzte und alle seit diesem Service Pack herausgebrachten Hotfixes enthielt. Der Active Directory-Client wurde indes separat zum Download angeboten. Am 24. April 2002 folgte eine Version des Security Rollup Package für die Terminal Server Edition.\n\nWindows NT Workstation 4.0 und Windows NT Server 4.0 sind sich zwar ähnlich und basieren auf demselben Code, haben aber einige bedeutende Unterschiede. Die Workstation-Version kann lediglich zehn Clientverbindungen aufnehmen und unterstützt nur zwei parallele Dateiübertragungen, um einen Einsatz als Server zu unterbinden. Die Speicherverwaltung verhält sich je nach Variante anders; während bei Windows NT Workstation 4.0 möglichst viel Speicher gespart wird, um auch bei vielen gleichzeitig laufenden Anwendungen noch ein schnelles Arbeiten zu ermöglichen, ist der Server auf hohe Netzwerkleistung optimiert und priorisiert daher Datei- und Netzwerkzugriffe. Zudem bietet nur die Server-Version die Möglichkeit, Laufwerke zu RAIDs zusammenzuschalten. Zahlreiche Anwendungen, wie Microsoft BackOffice oder der Internet Information Server, verweigern die Installation auf der Workstation-Version, sie lassen sich nur auf dem Server installieren. Auch die Anzahl der unterstützten Prozessoren unterscheidet sich; bei der Workstation sind es zwei, beim Server vier.\n\nWindows NT Server 4.0 Enterprise Edition beinhaltet zwei CDs. Auf der ersten CD befindet sich das Betriebssystem selbst sowie das Service Pack 3. Auf der zweiten CD befinden sich neben dem Microsoft Transaction Server, dem Microsoft Message Queue Server, dem Internet Information Server 3.0 und Microsoft FrontPage 97 auch das Hauptaugenmerk des Produkts, der \"Cluster Server\". Dieser erlaubt es, zwei Rechner zu einem Cluster zusammenzuschalten, sodass ein Rechner beim Ausfall des anderen seine Aufgaben übernehmen kann.\n\nDie Enterprise Edition von Windows NT Server 4.0 enthält einige Änderungen gegenüber der normalen Variante. Die größte Änderung ist ein in der x86-Version enthaltener Modus, der es erlaubt, für bestimmte speicherintensive Anwendungen mehr Arbeitsspeicher bereitzustellen. Sind normalerweise von den vier Gigabyte Arbeitsspeicher, die Windows NT 4.0 verwalten kann, zwei Gigabyte für Anwendungen reserviert und zwei Gigabyte für das System, so erhöht sich durch diesen Modus der verfügbare Speicher für Anwendungen auf drei Gigabyte, im Gegenzug reduziert sich der dem System vorbehaltenen Speicher auf ein Gigabyte. Allerdings muss eine Anwendung darauf ausgelegt sein, den zusätzlichen Arbeitsspeicher verwalten zu können, zusätzlich muss ein spezielles Flag gesetzt sein, das mithilfe einer auf der CD vorhandenen Anwendung vom Nutzer für jede ausführbare Datei geändert werden kann. Zudem unterstützt der Enterprise Server Systeme mit bis zu acht Prozessoren.\n\nUm die Enterprise Edition aufzuwerten, kaufte Microsoft im August 1998 den Softwarehersteller \"Valence Research\", der eine TCP/IP-basierte Clustering-Lösung anbot. Im Januar 1999 erschien schließlich der \"Windows NT Load Balancing Service\" (WLBS) als kostenloser Download für die Windows NT Server 4.0 Enterprise Edition. Dieser ermöglicht es, bis zu 32 Rechner zusammenzuschalten, sodass sie von außen unter einer einzigen IP erreichbar sind.\n\nWindows NT Server 4.0 Terminal Server Edition wurde von Microsoft in Zusammenarbeit mit Citrix entworfen. Die \"Terminal Server\"-Funktion, das zentrale Merkmal dieses Produkts, besteht aus einem modifizierten Kernel, der es ermöglicht, dass sich mehrere Benutzer gleichzeitig mit dem Server verbinden und auf dem Server Programme ausführen. Die Verwaltung des Terminal Servers wird durch zusätzliche Anwendungen ermöglicht. Neben speziell für diese Aufgabe erhältlichen Windows-Terminals sind im Lieferumfang Client-Programme für Windows für Workgroups 3.11, Windows 95, Windows NT Workstation 3.51 und 4.0 enthalten. Das von Citrix separat erhältliche Produkt MetaFrame, das unter anderem Load Balancing sowie Unterstützung von Nicht-Windows-Clients bot, erweiterte die Funktionalität des Terminal Servers stark. Das Service Pack 3 ist im Betriebssystem integriert, zudem ist der Internet Explorer 4.0 dem Betriebssystem beigelegt.\n\nZur Herstellung einer Verbindung zwischen Client und Server wird das neu entwickelte Remote Desktop Protocol verwendet. Dieses basiert auf dem ITU-Standard T.120, welcher Microsoft schon zuvor für das Konferenzprogramm NetMeeting verwendete. Einige Änderungen am Betriebssystem sind auf die Optimierung der Netzwerkleistung zurückzuführen, so ist etwa standardmäßig die Uhr deaktiviert.\n\nWindows NT 4.0 Embedded, das sowohl als Workstation als auch als Server konfiguriert werden kann, basiert auf Windows NT 4.0 Service Pack 5 und unterstützt spezielle Funktionen, um den besonderen Anforderungen der Embedded-PCs gerecht zu werden. Es kann sowohl ohne Monitor, als auch ohne Festplatte betrieben werden. Das Betriebssystem kann auch ohne Eingabegerät benutzt und ausschließlich remote über das Netzwerk oder den seriellen Anschluss bedient werden. Über den \"Target Designer\" und den \"Component Designer\" kann das Betriebssystem auf das jeweilige Gerät zugeschnitten werden, indem Windows-Komponenten entfernt oder Drittanbieterprogramme integriert werden.\n\nDer unterschiedlich große Funktionsumfang der einzelnen Versionen schlug sich stark im Verkaufspreis nieder. Windows NT Workstation 4.0 kostete 319 USD, der Server kostete 809 USD und beinhaltete fünf Clientzugriffslizenzen. Die Enterprise Edition war für 3.999 USD erhältlich und enthielt 25 Clientzugriffslizenzen. Der Terminal Server war zunächst für 1.129 USD erhältlich und beinhaltete 10 Clientzugriffslizenzen, allerdings musste für jeden Nutzer, der sich mit dem Terminal Server verband, zusätzlich zu den Clientzugriffslizenzen eine Lizenz für Windows NT Workstation 4.0 erworben werden, die 269 USD kostete. Nach massiver Kritik änderte Microsoft die Preispolitik für den Terminal Server; statt einer kompletten Lizenz für Windows NT Workstation 4.0 musste fortan lediglich für eine gesonderte Lizenz 109 USD pro Client entrichtet werden, für Unternehmen, die den Terminal Server im Internet einsetzten, gab es zudem einen Pauschaltarif, der Verbindungen von bis zu 200 Benutzern für 9.999 USD ermöglichte. Für 1.299 USD konnte eine Version des Terminal Servers für fünf Clients erworben werden, die bereits alle nötigen Lizenzen beinhaltete. Windows NT 4.0 Embedded war nicht im Handel erhältlich, sondern wurde ausschließlich zusammen mit passender Hardware verkauft.\n\nWindows NT Workstation 4.0 wurde in 19 Sprachen übersetzt, der Server in 11 Sprachen. Beide Varianten waren neben Englisch auch in Chinesisch (vereinfacht & traditionell), Deutsch, Französisch, Japanisch, Koreanisch, Niederländisch, Portugiesisch, Spanisch und Schwedisch verfügbar, die Workstation-Variante gab es zusätzlich auch auf Dänisch, Finnisch, Italienisch, Norwegisch, Polnisch, Russisch, Tschechisch und Ungarisch.\n\nWindows NT 4.0 verwendete wie bei allen Vorgängerversionen die gleiche Grundstruktur der Architektur von Windows NT. Die unterste Schicht bildet die Hardwareabstraktionsschicht, durch die der Kernel von der Hardware abgeschottet wird. Der Microkernel in der nächsthöheren Schicht übernimmt nur grundsätzliche Funktionen wie die Prozessorsynchronisierung. Weitere Funktionen des Kernels sind in Modulen oberhalb des Mikrokernels implementiert. Im Benutzermodus sorgen Subsysteme dafür, dass Programme ausgeführt werden können, indem sie die nötigen Programmierschnittstellen bereitstellen. Neben dem Win32-Subsystem, das für 32-Bit-Windowsprogramme zuständig ist, gibt es noch die POSIX- und OS/2-Subsysteme, mit denen textbasierte OS/2-1.x- und POSIX-kompatible Programme ausgeführt werden können. MS-DOS und 16-Bit-Windowsprogramme werden innerhalb einer speziellen Umgebung, der Virtual DOS Machine, ausgeführt. Unter x86-kompatiblen Prozessoren wird der Virtual 8086 Mode ausgenutzt, unter RISC-Prozessoren kommt hingegen ein Emulator zum Einsatz. In Windows NT 4.0 bildet die Basis des Emulators im Gegensatz zu vorherigen Versionen von Windows NT nicht mehr ein 80286-Prozessor, sondern ein i486-Prozessor. Dadurch können erstmals Anwendungen ausgeführt werden, die mindestens einen 80386 voraussetzen.\n\nDer Weg, wie Daten auf dem Bildschirm ausgegeben werden, wurde jedoch grundsätzlich geändert. In vorherigen Versionen des Betriebssystems liefen die Grafikfunktionen als Teil des Win32-Subsystems im Benutzermodus. Dies hatte jedoch zur Folge, dass beim Aufrufen dieser Funktionen zahlreiche Kontextwechsel vorgenommen werden mussten, um zwischen dem Benutzer- und dem Kernelmodus zu wechseln, was sich negativ auf die Leistung des Betriebssystems auswirkte. Daher wurden in Windows NT 4.0 die Grafikfunktionen wie der Fenstermanager oder das Graphics Device Interface (GDI) in den Kernel verlagert, was die Grafikleistung des Betriebssystems spürbar erhöhte. Lediglich die Konsolenfunktionen sowie weitere Funktionen, wie etwa das Erstellen von Prozessen, verblieben weiterhin im Benutzermodus. Diese Änderungen glichen den Mehrverbrauch an Arbeitsspeicher, der durch die neue Benutzeroberfläche entstand, größtenteils aus. Als Folge davon ist es jedoch in Windows NT 4.0 möglich, dass fehlerhafte Grafikkartentreiber das gesamte System zum Absturz bringen. Diese Änderung ist eine direkte Folge der geänderten Philosophie im Designs des Betriebssystems. Waren anfangs noch alle Subsysteme gleichwertig und sich gegenseitig gleichgestellt, wird mit diesen Änderungen der besonderen Wichtigkeit von Win32-Anwendungen Rechnung getragen, indem die Ausführungsgeschwindigkeit dieser Anwendungen optimiert wird.\n\nZahlreiche neue Programmierschnittstellen können in Windows NT 4.0 verwendet werden. Die Technologie, die während der Entwicklungszeit unter der Bezeichnung Netzwerk-OLE bekannt war, heißt in der fertigen Version von Windows NT 4.0 Distributed Component Object Model (DCOM). Mit ihr ist es möglich, OLE-Objekte, die sich auf einem anderen Computer im Netzwerk oder im Internet befinden, via Remote Procedure Call (RPC) anzusprechen. Weiters enthält Windows NT 4.0 das Telephony-API, mit dem Anwendungen mit Telefonen, Modems und Faxgeräten kommunizieren können, das Cryptography API, um Anwendungen die Nutzung von Funktionen zur Ver- und Entschlüsselung sowie digitale Signaturen und Zertifikate zu ermöglichen, sowie auch ein, wenngleich undokumentiertes API, mit dem erstmals Defragmentierungsprogramme für Windows NT 4.0 erstellt werden können, ohne das System selbst modifizieren zu müssen. Windows NT 4.0 enthält zudem eine unvollständige Implementierung der Grafikbibliothek DirectX, der die 3D-Komponente Direct3D fehlt. Bei ihr handelt es sich technisch gesehen um einen Kompatibilitätslayer, der DirectX-Aufrufe in GDI, Sound- und Winsock-Befehle übersetzt, da DirectX unter Windows NT 4.0, anders als unter Windows 95, architekturbedingt nicht direkt mit der Hardware oder den Treibern kommuniziert. Microsoft reichte später mit dem Service Pack 3, das DirectX aktualisierte, das fehlende Direct3D nach.\n\nUm mehrere gleichzeitig eingeloggte Benutzer zu unterstützen, mussten für die Terminal Server Edition Änderungen am Kernel von Windows NT 4.0 vorgenommen werden. Da das Win32-Subsystem in Windows NT 4.0 nicht darauf ausgelegt ist, mehr als ein Mal ausgeführt zu werden, wird es in der Terminal Server Edition in ein sogenanntes \"session space\" geladen, ein getrennter Speicherbereich, auf die nur die jeweilige Sitzung zugreifen kann. Für jede Sitzung wird ein eigener \"session space\" mit einer eigenen Instanz des Win32-Subsystems eingerichtet. Um die Sitzungen voneinander unterscheiden zu können, besitzen alle Objekte in der Terminal Server Edition eine Session-ID, die die zugehörige Sitzung identifiziert. GDI-Befehle, die innerhalb einer Remote-Sitzung ausgeführt werden, werden an einen virtuellen Grafikkartentreiber weitergeleitet, der die Daten im Netzwerk überträgt und auf dem Client darstellt.\n\nFühren mehrere Benutzer dieselbe Win32-Anwendung aus, wird die Anwendung nur ein Mal in den Speicher geladen. Alle Sitzungen erhalten daraufhin einen Zeiger, der auf den gleichen Speicherbereich zeigt. Wenn eine Sitzung versucht, in den Speicherbereich der Anwendung zu schreiben, z. B. um einen Text im Arbeitsspeicher zu speichern, wird die jeweilige Speicherseite kopiert und so eingerichtet, dass nur die jeweilige Sitzung darauf zugreifen kann. Alle anderen Speicherseiten bleiben unberührt und zeigen weiterhin auf den gemeinsam genutzten Speicherbereich. Diese gemeinsame Verwendung von Ressourcen wird nur bei 32-Bit-Anwendungen angewandt, 16-Bit-Windowsanwendungen profitieren nicht von diesen Funktionen.\n\nBei der Terminal Server Edition stellte sich die Frage der Anwendungskompatibilität, da zahlreiche Anwendungen nicht darauf ausgelegt sind, in einer Mehrbenutzerumgebung verwendet zu werden. Um Anwendungen so gut wie möglich in die Umgebung des Terminal Servers einzubinden, wurden einige Vorkehrungen getroffen. Häufig werden etwa wichtige Informationen nur für den aktuell angemeldeten Benutzer in die Registrierungsdatenbank geschrieben, sodass andere Benutzer die Software nicht verwenden können. Um dieses Problem zu beheben, kann das Betriebssystem zwischen zwei Modi, dem Ausführungs- und dem Installations-Modus, umgeschaltet werden. Im Installationsmodus werden Informationen, die in den Hive codice_1 geschrieben werden, auf einem benutzerunabhängigen Ort gespiegelt und von dort bei Bedarf an die jeweiligen Benutzer verteilt. So stehen diese Informationen allen Benutzern zur Verfügung. Auch Startmenüeinträge werden im Installationsmodus entsprechend angepasst. Für einige häufig benutzte Programme wie Microsoft Office befinden sich Skripts im Lieferumfang, um diese Programme an die Mehrbenutzerumgebung anzupassen. Um die Erstellung eigener Skripts zu vereinfachen, ist die Terminal Server Edition mit zusätzlichen Kommandozeilenprogrammen ausgestattet. Diese dienen zum Beispiel dazu, Einträge der Windows-Registrierung abzufragen oder automatisiert Zugriffsrechte auf Dateien zu setzen.\n\nDie größte Neuerung im Vergleich zum Vorgänger ist die neue grafische Benutzeroberfläche: Windows NT 4.0 benutzt die Oberfläche von Windows 95, bestehend aus Taskleiste und Windows-Explorer. In diesem Zuge übernahm Windows NT 4.0 einige Funktionen von Windows 95, etwa den Aktenkoffer, DFÜ-Netzwerkunterstützung, AutoPlay oder in der Server-Version den Richtlinien-Editor, nicht aber den Geräte-Manager. Dafür enthält das Betriebssystem zahlreiche Neuerungen, die vorher nur in dem kostenpflichtigen Paket Plus! für Windows 95 enthalten waren, etwa Schriftglättung oder die Streckung von Hintergrundbildern. Wie in Windows 95 helfen Assistenten bei der Konfiguration des Betriebssystems. Mit Ausnahme der Terminal Server Edition kann die Benutzeroberfläche von Windows NT 4.0 mit den im Internet Explorer 4 enthaltenen Weberweiterungen aktualisiert werden.\n\nWindows NT 4.0 beinhaltet einen überarbeiteten Taskmanager. Er zeichnet die CPU-Auslastung und den Speicherverbrauch des Systems auf. Es ist außerdem möglich, diese Werte separat für jeden Prozess anzuzeigen. Prozesse, die nicht mehr reagieren, können von hier beendet werden. Der Netzwerkmonitor, welcher mit Windows NT Server 4.0 erstmals Bestandteil des Betriebssystems ist, zeichnet sämtliche Netzwerkaktivitäten auf, was die Überwachung des Netzwerks vereinfacht. Zur Diagnose von Computern enthält Windows NT 4.0 ein dem DOS-Programm MSD.EXE ähnliches Programm, das auch Daten von Computern im Netzwerk auslesen kann. Als Webbrowser beinhaltet Windows NT 4.0 den Internet Explorer in der Version 2.0.\n\nDie Netzwerkfähigkeiten des Betriebssystems wurden durch neue Funktionalitäten verbessert. Einerseits unterstützt Windows NT 4.0 erstmals das Domain Name System (DNS), was die Verwaltung vor allem in Netzwerken mit Unix-Rechnern vereinfacht. Andererseits enthält Windows NT 4.0 eine neue Version des Novell NetWare-Clients, die unter anderem die Novell Directory Services (NDS) unterstützt. Auch die RAS-Funktion des Betriebssystems wurde aufgewertet, einmal durch das Point-to-Point Tunneling Protocol (PPTP), welches die Herstellung einer sicheren RAS-Verbindung ermöglicht, sowie durch Multilink PPP, mit dem mehrere Verbindungen zur Erhöhung der Datenübertragungsrate zusammengeschaltet werden können. Allerdings unterstützt Windows NT 4.0 nicht länger das HPFS-Dateisystem von OS/2 und kann daher nicht mehr auf solche Partitionen zugreifen. Mithilfe eines inoffiziellen Workarounds ist es möglich, diese Funktionalität wiederherzustellen.\n\nDie Server-Version von Windows NT 4.0 enthält erstmals den Internet Information Server in der Version 2.0, mit dem ohne zusätzliche Programme ein Webserver oder FTP-Server betrieben werden kann. Der Internet Information Server kann auch über das Internet mithilfe eines Browsers überwacht und konfiguriert werden. Zum Erstellen von Webseiten ist dem Server Microsoft FrontPage 1.1 beigelegt. Eine in der Funktionalität beschränkte Variante, der unter anderem FrontPage fehlt, ist der Workstation-Variante unter der Bezeichnung \"Peer Web Services\" beigelegt.\n\nWindows NT 4.0 lässt sich anders als vorherige Versionen nicht mehr auf 80386-Prozessoren installieren. Die Mindestvoraussetzungen für die Workstation-Version waren ein 486-Prozessor mit 25 MHz, 12 MB Arbeitsspeicher (16 MB für RISC-Computer), 110 MB Festplattenspeicher und ein CD-ROM-Laufwerk. Die Server-Version benötigte einen 486-Prozessor mit 33 MHz, 16 MB Arbeitsspeicher, 125 MB Festplattenspeicher (für die RISC-Versionen 160 MB) und ein CD-ROM-Laufwerk. Eine ältere Windows NT-Version kann auf Windows NT 4.0 aktualisiert werden, eine Aktualisierung von Windows 95 ist hingegen nicht möglich.\n\nFür die Enterprise Edition nannte Microsoft als Systemvoraussetzung einen Pentium-Prozessor mit 90 MHz, 64 MB Arbeitsspeicher, 500 MB Festplattenspeicher sowie ein CD-ROM-Laufwerk. Die Enterprise Edition kann entweder neu installiert, oder von einer bestehenden Installation von Windows NT Server 4.0 aktualisiert werden. Microsoft gab als Systemvoraussetzung für die Terminal Server Edition einen Pentium-Prozessor ohne Angabe einer Taktrate, 32 MB Arbeitsspeicher, 128 MB Festplattenspeicher sowie eine TCP/IP-Netzwerkverbindung an. Eine Aktualisierung auf die Terminal Server Edition ist lediglich von WinFrame 1.6 oder 1.7 möglich.\n\nWindows NT 4.0 kann erstmals auch ohne Diskettenlaufwerk installiert werden, da die Windows NT 4.0-CD den seinerzeit neuen El-Torito-Standard für bootfähige CDs verwendet. Für Rechner, die nicht von einer CD booten können, stehen weiterhin drei Startdisketten zur Verfügung. Nach Angaben von Microsoft unterstützt Windows NT 4.0 über 6.000 Hardwareplattformen und enthält über 4.000 Gerätetreiber. Infolge der Änderungen an der Architektur sind Grafikkartentreiber für ältere Versionen von Windows NT nicht mit Windows NT 4.0 kompatibel.\n\nDie Reaktionen auf Windows NT 4.0 waren gemischt. Einerseits wurde die Benutzerfreundlichkeit des Betriebssystems positiv bewertet. Der Schritt, die neue Benutzeroberfläche einzuführen und damit vom Programm-Manager abzukehren, wurde einhellig begrüßt. Andererseits war Windows NT 4.0 vor allem anfangs für seine Unzuverlässigkeit berüchtigt. Das Betriebssystem enthielt zahlreiche teils schwere Programmfehler, die erst nach einigen Service Packs behoben wurden. Noch im Januar 1997, mehr als vier Monate nach der Veröffentlichung, setzten 70 bis 80 Prozent aller Betriebe Windows NT 3.51 ein, da die Fehler des neuen Betriebssystems sie abschreckten und sie sich daher entschieden, die Veröffentlichung des Nachfolgers abzuwarten. Das Service Pack 2 war bei seiner Veröffentlichung so fehlerhaft, dass die Presse von der Verwendung dieses Service Packs abriet.\n\nWindows NT 4.0 hatte es schwer, mit den Entwicklungen der Zeit mitzuhalten. Es unterstützte weder Advanced Power Management (APM) noch Plug and Play, was bereits während der Entwicklung zu Kritik führte. Neuere Schnittstellen wie der gegen Ende der 1990er-Jahre aufstrebende Universal Serial Bus (USB) oder IrDA konnten nicht mit Windows NT 4.0 verwendet werden. Das mit Windows 95 B bzw. Windows 98 eingeführte Dateisystem FAT32 konnte Windows NT 4.0 nicht lesen, was beim Parallelbetrieb von Windows 9x und Windows NT einen großen Nachteil darstellte. IDE-Festplatten größer als 8 GB unterstützte Windows NT 4.0 erst mit dem Service Pack 4, was bei einer Neuinstallation zu Problemen führen konnte.\n\nWenngleich Windows NT Workstation 4.0 und Windows 95 auf völlig unterschiedliche Zielgruppen ausgelegt werden, wurden sie häufig miteinander verglichen. Windows NT 4.0 zeigte bei Rechnern, die leistungsfähig genug sind, teilweise eine erheblich höhere Leistung im Vergleich zu Windows 95; sogar Microsoft selbst zeigte sich von diesem Ergebnis überrascht. Auf solchen Rechnern kamen auch die anderen Stärken von Windows NT 4.0 zum Zuge, bei Rechnern mit wenig Arbeitsspeicher hingegen war Windows 95 das geeignetere Betriebssystem. Durch die fehlende Plug & Play-Unterstützung war Windows NT 4.0 für Notebooks allgemein schlechter geeignet als Windows 95.\n\nMit den in Windows NT Server 4.0 eingebauten Internetkomponenten erkannte Microsoft die wachsende Bedeutung dieses Mediums im Netzwerkmarkt, was überwiegend positiv bewertet wurde. Für über die Grundbedürfnisse hinausgehende Einsatzzwecke fehlten allerdings sowohl ein E-Mail- und Usenet-Server als auch Sicherheitsmechanismen wie eine Personal Firewall, um den Server vor unbefugtem Zugriff zu schützen. Im Netzwerkbetrieb war Windows NT Server 4.0 leistungsfähiger als sein Vorgänger. Die Netzwerktransferrate war um bis zu 60 Prozent höher, der Internet Information Server war 30 Prozent schneller als die vorherige Version. Der bei weitem größte Kritikpunkt war das Fehlen eines Verzeichnisdienstes, da das bisherige Domain-System von Windows NT 4.0 den Einsatzzwecken großer Netzwerke nicht mehr gewachsen sei.\n\nDas Betriebssystem konnte trotz aller Widrigkeiten seinen Marktanteil kontinuierlich ausbauen, hauptsächlich zu Lasten des Konkurrenten Novell NetWare. Es profitierte dabei hauptsächlich von der großen Anzahl an Windows-Anwendungen. Hatte Windows NT im Vergleich sämtlicher Betriebssysteme 1997 noch einen Anteil von 9,2 %, so stieg dieser Anteil im nächsten Jahr auf 11 % und lag damit hinter Windows 95 und Windows 98 auf Platz 3. Nach Angaben von Microsoft hatte das Unternehmen bis zum November 1997 11 Millionen Lizenzen des Betriebssystems Windows NT Workstation 4.0 verkauft, bis zum Oktober 1998 erhöhte sich diese Zahl auf 20 Millionen. Im März 1999 erreichte das Unternehmen die Zahl von 28 Millionen verkauften Lizenzen. Der Windows NT Server 4.0 konnte innerhalb des ersten Monats 150.000 verkaufte Exemplare verbuchen. Unter alleiniger Betrachtung der Serverbetriebssysteme brachte es Windows NT bis zum Jahr 2000 auf 41 % Marktanteil.\n\nAm Ende stellte sich für Microsoft das Problem, dass viele Nutzer nicht auf neuere Windows-Betriebssysteme migrierten. Als der Windows Server 2003 veröffentlicht wurde, war Windows NT 4.0 noch in zahlreichen Betrieben im Einsatz, was sich nach Meinung von Analysten negativ auf dessen Verkaufszahlen auswirken würde. Es wurde auch damit gerechnet, dass einige Unternehmen nicht auf eine neuere Windows-Version, sondern auf ein alternatives Betriebssystem wie Linux wechseln würden. Noch Ende 2004, als Microsoft den Support für Windows NT 4.0 einstellte, benutzten Schätzungen zufolge 20 Prozent aller Server und 10 Prozent aller Workstations Windows NT 4.0.\n\nWährend NetWare und OS/2 als Konkurrenten ausfielen, blieb jedoch Unix als starker Gegner. Das Ziel von Microsoft, Windows NT auch für sicherheitskritische Anwendungen attraktiv zu machen, konnte nicht erreicht werden. Eine Umfrage unter 200 Firmen aus der Fortune 500, einer Liste der 500 größten Unternehmen der USA, ergab, dass viele Firmen zunächst Windows NT einsetzten, später jedoch wieder zurück zu Unix wechselten. Vor allem die Beschränkung in der Anzahl der Prozessoren machte Windows NT für leistungsfähige Rechner unattraktiv. Analysten erwarteten, dass Unix im High-End-Segment weiterhin eine Rolle spielen wird und Microsoft eine lange Zeit brauchen würde, um Unix angreifen zu können. Am 20. Mai 1997 unterhielt Microsoft eine Demonstrationsveranstaltung unter dem Titel „Scalability Day“. Sie sollte die Skalierbarkeit von Windows NT, und damit die Eignung für sehr anspruchsvolle Aufgaben unter Beweis stellen. Die Mehrheit der Unternehmen zeigte sich aber unbeeindruckt. Windows NT sei noch weit entfernt davon, an die tausend Benutzer zu verwalten und mit sehr großen Datenbanken umzugehen. Ein weiterer Kritikpunkt betraf den Kundensupport, den 40 Prozent der Unternehmen als unzureichend bewerteten. In diesem Zusammenhang wurde auch die Abkehr vom Alpha-Prozessor im August 1999 kritisiert. Die sehr hohe Leistung der Alpha-Prozessoren, die auch Microsoft selbst als Marketingargument verwendete, sei nicht durch Intel-Prozessoren ersetzbar, auch nicht durch neue Achtkernsysteme. Zwar verwendeten nur sieben Prozent aller Rechner mit Windows NT Alpha-Prozessoren, aber darunter handelte es sich meist um große Server. Mit der Aufgabe des Alpha-Prozessors würde Windows NT den Anschluss an den Enterprisemarkt verlieren.\n\nDie Enterprise Edition des Windows NT Servers 4.0 erhielt durchschnittliche Rezensionen. Zwar sei die Clusteringfunktion an sich gut und funktioniere auch einwandfrei, mit nur zwei möglichen Rechnern falle sie jedoch hinter den Möglichkeiten Unix-basierter Produkte zurück. Durch das fehlende \"load balancing\" müssten die Serverzugriffe manuell durch den Administrator gleichmäßig auf beide Rechner verteilt werden. Trotz dieser Einschränkungen zeigten sich Nutzer des Produkts überwiegend zufrieden. Der Windows NT Load Balancing Service erhielt nach seiner Fertigstellung zwar positive Bewertungen, ein Kritikpunkt war jedoch die fehlende Integration mit dem bereits bestehenden Cluster Server. Während Webserver dank dem neuen Produkt nun besser erreichbar seien, helfe das Produkt nicht, wenn etwa ein Datenbankserver abstürze.\n\nDas Ziel, mit der Enterprise Edition auf dem von Unix dominierten Segment aufzuschließen, wurde verfehlt. Bei einem Test, der Windows NT mit verschiedenen Unix-Varianten verglich und auch die Enterprise Edition berücksichtigte, schnitt das Produkt in allen Kategorien außer der Clientunterstützung am schlechtesten ab. Bereits während der Entwicklung sagten Kritiker, dass Unix-Betriebssysteme bereits seit 1990 die Funktionen unterstützten, die die Enterprise Edition einführte.\n\nBereits während der Entwicklungszeit gab es heftige Kritik an der Terminal Server Edition von Windows NT 4.0. Das Produkt verfolge keine klare Linie, und Microsoft hielte sich mit wichtigen Details etwa zum Preis bis zuletzt zurück. Zwar sollten durch dieses Produkt Verwaltung und Kosten für Unternehmen sinken, aber da ein sinnvoller Einsatz nur mit dem separat zu erwerbenden Produkt \"MetaFrame\" von Citrix möglich sei, lägen die Kosten beim Einsatz des Terminal Servers höher als bei einem traditionellen Netzwerk. Kritisiert wurde zudem die Entscheidung, anstelle des bereits verbreiteten ICA-Protokolls von Citrix ein neues, proprietäres Protokoll einzuführen, dessen Datendurchsatz für einen sinnvollen Gebrauch zu niedrig sei. Unabhängig vom verwendeten Protokoll sei das Hauptproblem, dass beim Einsatz grafikintensiver Anwendungen die Netzwerkleistung massiv einbreche. Außerdem unterstütze Kritikern zufolge Microsoft die Administratoren nicht ausreichend bei der Anwendungskompatibilität. Zwar seien einige vorgefertigte Skripts bereits vorhanden, aber für andere, nicht durch diese Skripts abgedeckten Anwendungen müssten eigene Skripts geschrieben werden, was als sehr umständlich galt.\n\nDie Situation auf dem Hardwaremarkt war zum Zeitpunkt der Veröffentlichung ungünstig, denn Thin Clients galten als zu teuer, zudem böten sie keinen Mehrwert zu regulären PCs. Nutzer warfen Microsoft vor, das Produkt kaum zu vermarkten und zu wenige Informationen preiszugeben. Der Einsatz des Terminal Servers im Internet, zum Beispiel um Anwendungen bereitzustellen, würde durch Microsofts Lizenzpolitik, die für jede Verbindung eine separate Lizenz fordere, verhindert. Microsoft reagierte auf die Kritik an der Preispolitik und änderte die Preise daraufhin, um den besonderen Verwendungszwecken des Terminal Servers gerecht zu werden.\n\nDie Reaktionen auf die Embedded-Version von Windows NT 4.0 waren gemischt. Einerseits berichteten einen Monat nach der Veröffentlichung bereits 12 % aller IT-Manager, Windows NT 4.0 Embedded in ihrem Betrieb einzusetzen. Andererseits besitze Microsoft mit Windows CE bereits ein Produkt im Embedded-Markt, sodass das Unternehmen nur mit sich selbst konkurriere. Aufgrund des erwarteten höheren Preises gegenüber Windows CE gäbe es nur wenige Anreize, Windows NT 4.0 Embedded zu bevorzugen.\n\n\n"}
{"id": "1151427", "url": "https://de.wikipedia.org/wiki?curid=1151427", "title": "UNIVAC III", "text": "UNIVAC III\n\nDie UNIVAC III wurde als Weiterentwicklung der UNIVAC I und UNIVAC II 1962 ausgeliefert; er arbeitete im Gegensatz zu seinen Vorgängern mit Transistoren. \nObwohl die UNIVAC III kompatibel mit ihren Vorgängern war, wurde das Layout des Befehlssatzes und die Maschinenwortbreite komplett überarbeitet. Dies führte dazu, dass bestehende Programme umgeschrieben werden mussten, welches wiederum viele Kunden veranlassten, ihre Hardware zu wechseln.\n\nDa zu dieser Zeit der Hauptspeicher die größten Kosten verursachte, wurde die UNIVAC III so entworfen, dass sie so wenig Hauptspeicher wie nur möglich verwenden musste.\n\nSperry Rand lieferte ab 1962 96 Systeme aus.\n\n\n"}
{"id": "1152043", "url": "https://de.wikipedia.org/wiki?curid=1152043", "title": "Gruppensimulation", "text": "Gruppensimulation\n\nDie Gruppensimulation (englisch: crowd simulation) beschreibt die Simulation des Verhaltens einer großen Anzahl von Objekten bzw. Figuren/Personen. In der 3D-Computeranimation findet die Gruppensimulation spätestens seit dem Film \"Herr der Ringe\" verstärkt Anwendung, da herkömmliche Animationstechniken – bei denen jede Figur individuell animiert wird – in Massenszenen zu aufwändig (und damit zu teuer) sind. \n\nEine Massenszene besteht aus zahlreichen ähnlichen Figuren. Für diese Figuren werden Bewegungsbibliotheken angelegt, die sämtliche möglichen Bewegungen der Figur bzw. ihrer „Körper“teile beinhalten. Die Bewegungsbibliotheken werden entweder individuell erzeugt oder können prozedural auf Basis von bekannten Bewegungsschemata oder biologischen Grundlagen automatisch erstellt werden.\n\nDie eigentlichen Bewegungen der Figuren und die Interaktion der Figuren untereinander können auf zwei Arten simuliert werden:\n\n\nDie Gruppen-KI erzeugt sehr viel realistischere Ergebnisse, ist aber sehr aufwendig zu implementieren und wurde bislang meist individuell für ein Animationsprojekt programmiert. Mit dem für Weta Digital entwickelten Softwarepaket Massive bzw. Softimage Behaviour sind kommerzielle Produkte erhältlich, die Gruppensimulation für eine größere Benutzergruppe zugänglich machen.\n\nEine verwandte Anwendung ist die Planung von Gebäuden und Plätzen mit starkem Publikumsverkehr um das Verhalten von Menschen bei einer Evakuierung zu simulieren und bauliche Maßnahmen in den Entwurf einzuarbeiten. Solche Evakuierungssimulationen nutzen Erkenntnisse aus der Gruppendynamik und Gruppenpsychologie. Der Fokus liegt hier weniger auf der Visualisierung, sondern mehr auf dem Verhalten an sich. Die grundlegende Technik ist auch hier die Gruppen-KI, jedoch mit komplexeren Regelsätzen.\n\n\n"}
{"id": "1156177", "url": "https://de.wikipedia.org/wiki?curid=1156177", "title": "Liferea", "text": "Liferea\n\nLiferea (Linux Feed Reader) ist ein freier Feedreader. Es wurde basierend auf GTK+ entwickelt und ist für zahlreiche Linux-Distributionen unter der GNU-GPL-Lizenz als kostenloses Paket verfügbar. Derzeit ist Liferea für folgende Feed-Formate einsetzbar:\n\nDaneben unterstützt Liferea auch Podcasting. Episoden werden als Anhang zur Beschreibung angezeigt und können sowohl in Liferea abgespielt, als auch heruntergeladen oder ihre URLs an andere Programme übergeben werden.\n\nLiferea ist in C programmiert und verwendet keine Interpretersprachen.\n\nUm das Lesen von verlinkten Artikeln oder Blogkommentaren zu erleichtern, erlaubt Liferea das Laden von Webseiten im eingebetteten HTML-Browser, der WebKit zur Darstellung verwendet. Zusätzlich kann der Benutzer eine Vielzahl von vordefinierten externen Browserprogrammen zum Öffnen von Links benutzen (Mozilla, Firefox, Netscape, Opera, Epiphany, Konqueror).\n\nDurch das Speichern von Feed-Abonnements in verschiedenen Ordnern kann auf die Schlagzeilen der Abonnements zugegriffen werden. Mit einer entsprechenden Konfigurationsoption zeigt Liferea nur die ungelesenen Schlagzeilen an. Auf diese Weise erhält man eine schnelle Übersicht über alle neuen Schlagzeilen in einer Kategorie.\n\nÄhnlich wie Evolution unterstützt Liferea sogenannte \"Suchordner\", die eine permanente Suchfunktion darstellen. Ein Suchordner enthält jeweils alle Schlagzeilen aller Abonnements die seinen Suchregeln entsprechen.\n\nLiferea erlaubt die Synchronisierung mit Web-Anwendungen wie Tiny Tiny RSS, InoReader, Reedah und TheOldReader.\n\nIn Epiphany können mit der \"News Feed Subscription\"-Erweiterung von der jeweiligen Webseite angebotene Feeds selektiv zu Liferea hinzugefügt werden.\n\nIn Firefox kann das mitgelieferte Skript „liferea-add-feed“ in den Firefox-Einstellungen konfiguriert werden, so daß Feed automatisch zu Liferea hinzugefügt werden.\n\nSeit Version 1.10 kann Liferea über Plugins erweitert oder verändert werden. Mitgelieferte Plugins erlauben etwa Benachrichtigungen, Schlüsselbund-Integration und ein Benachrichtigungssymbol (Tray-Icon) hinzuzufügen.\n\n"}
{"id": "1158397", "url": "https://de.wikipedia.org/wiki?curid=1158397", "title": "Free DV", "text": "Free DV\n\nAvid Free DV war eine gratis-Version der professionellen Videoschnittsoftware von Avid für Mac OS X und Windows XP.\n\nDas Programm konnte kostenlos von der Avid FreeDV Website heruntergeladen werden. Es wurde zum 4. September 2007 abgekündigt, eine für die Installation notwendige Registrierung des Programms ist nicht mehr möglich. Die Installationsdatei ist über Suchmaschinen im Internet zu finden (wichtig für diejenigen, die ihre Kopie der Installationsdatei verloren haben, aber die Registrierungsdaten noch haben).\n\nVideo kann nur über die FireWire-Schnittstelle eingeladen und ausgespielt werden, und es stehen jeweils maximal zwei Video- und zwei Audiospuren zum Editieren zur Verfügung.\n\nDie Effektpalette ist zwar relativ umfangreich, jedoch fehlt zum Beispiel 3D-Warp, und nur ein kleiner Teil der bei den professionellen Versionen verfügbaren Effekte ist vorhanden. Neben dem Ausspielen auf ein DV-Band kann man das Video auch als QuickTime exportieren. Alle anderen Kompatibilitäten sind gesperrt, obwohl die Oberfläche fast identisch mit den auf den professionellen Einsatz ausgerichteten Programmen Xpress DV und Xpress Pro von Avid ist.\nWeiterhin kann das in Avid Free DV erstellte Material in der Vollversion nicht genutzt werden, so dass die „free version“ in diesem Fall nutzlos bleibt.\n\nDas Programm eignet sich jedoch gut zum Schnitt von Amateurfilmen (Urlaubs- oder Hochzeitsvideos) auf DV und um die Oberfläche der gesamten AVID-Familie kostenfrei kennenzulernen.\n\nAndere Gratis-Schnittprogramme:\n"}
{"id": "1159097", "url": "https://de.wikipedia.org/wiki?curid=1159097", "title": "MindManager", "text": "MindManager\n\nMindManager ist eine kommerzielle Anwendungssoftware zur Erstellung von Mindmaps.\n\nDie Entwicklung der Software MindManager begann 1994 in einer Onkologie-Station durch Mike Jetter. Mike Jetter wurde damals gegen Leukämie behandelt und wollte etwas Selbsterschaffenes hinterlassen. Zu diesem Zeitpunkt hatte er sich bereits länger mit der Mindmapping-Methode beschäftigt. Daher lag es nahe, eine Software zu entwickeln, um Ideen und Informationen zu sammeln, zu ordnen und darzustellen.\nDie ersten drei Versionen der Software liefen unter dem Namen MindMan, bevor mit dem Release 3.5 der Name zum heutigen MindManager geändert wurde.\nHersteller von MindManager ist Mindjet in San Francisco/Kalifornien, USA. Die Europa-Zentrale hat ihren Sitz in Alzenau. Im August 2016 wurde Mindjet vom vorherigen Eigentümer \"Spigit\" an die Corel Corporation verkauft.\n\nMindManager ist eine Software auf Mindmapping-Basis, mit der Informationen, Aufgaben und Zusammenhänge visuell dargestellt und bearbeitet werden können. In Mindmaps werden Informationen, Bilder oder Hyperlinks hierarchisch um ein zentrales Thema angeordnet. Sie ermöglichen ein besseres Verständnis und einen schnellen Überblick.\n\nMindManager ist sowohl für Windows (Version 2019) als auch für MAC (Version 11) erhältlich.\n\nNeben der Einzelplatzlizenz gibt es auch die Enterprise-Version, welche als Collaboration-Tool eingesetzt wird und u. a. die Zusammenarbeit mit SharePoint vorsieht.\n\nDie derzeitige Produktlinie ist fett gekennzeichnet.\nMit MindManager erzeugte Mind Maps können in eine Reihe von Formaten exportiert werden:\n\nDie gesamte Mindmap kann mit Microsoft Outlook synchronisiert werden. Fertige Projektpläne können in Microsoft Project importiert werden und dort als Vorlage für Projektmanagement-Aufgaben dienen.\n\n\n"}
{"id": "1159515", "url": "https://de.wikipedia.org/wiki?curid=1159515", "title": "Eurographics (Vereinigung)", "text": "Eurographics (Vereinigung)\n\nEurographics \"(European Association for Computer Graphics)\" ist eine europäische Vereinigung von Forschern und Geschäftspersonen im Bereich Computergrafik.\n\nDie 1980 gegründete Organisation veranstaltet jedes Jahr in einer anderen europäischen Stadt eine gleichnamige Computergrafik-Tagung. Außerdem gibt sie vierteljährlich die Fachzeitschrift \"Computer Graphics Forum\" () heraus, in der auch die Proceedings der Eurographics-Tagung veröffentlicht werden.\n\n\n"}
{"id": "1168553", "url": "https://de.wikipedia.org/wiki?curid=1168553", "title": "MacBook Pro", "text": "MacBook Pro\n\nDas MacBook Pro (MBP) ist ein Macintosh-Notebook des Unternehmens Apple. Die Produktreihe wurde von Steve Jobs am 10. Januar 2006 auf der Macworld Expo vorgestellt.\nDie neuen Laptops lösten das 15- und 17-Zoll-Modell des PowerBook G4 ab. Das Design basiert auf dem von Jonathan Ive entwickelten Design der G4-PowerBooks. 2005 kündigte Apple an, von PowerPC-Prozessoren zu Intel-CPUs zu wechseln. Das MacBook Pro markierte den ersten mobilen Mac, der mit dem Core Duo einen Intel-Prozessor besaß.\n\nDas MacBook Pro richtet sich im Gegensatz zum MacBook und MacBook Air auch an professionelle Anwender, was unter anderem durch die Möglichkeit eines 15-Zoll großen Bildschirms und bei diesem Modell auch einer eigenen Grafikkarte unterstrichen wird. Es stellt das obere Ende der MacBook-Reihe dar.\n\nDie erste Generation des MacBook Pro wurde von Steve Jobs auf der Macworld Expo am 10. Januar 2006 vorgestellt und war ab April 2006 mit einer Bildschirmdiagonale von 15,4″ im Handel erhältlich. Am 24. April 2006 wurde ein Modell mit 17″-Bildschirm ergänzt.\n\nNach dem 2006 angekündigten Wechsel zu Intel ist das MacBook Pro der erste Macintosh mit einer Intel-CPU, in dem Fall \"Core Duo\"-Prozessoren.\nDer Einführungspreis des 15,4″-MacBook Pro mit 1,83 GHz-Prozessor lag bei 2099 €/2999 CHF, der des 17″-MacBook Pro bei 2599 €/3699 CHF. Kurz darauf wurde es ohne Mehrpreis mit einer schnelleren Prozessortaktung von 2,0 oder 2,16 GHz ausgeliefert.\n\nWesentliche Neuerungen erfuhren auch die Bildschirme. Sie sind im Breitbildformat und erreichen mit 300 cd/m² nahezu die Helligkeit der \"Apple Cinema Displays\". Eine spiegelnde („glossy“) Ausführung des Bildschirms war bei beiden Größen erhältlich.\n\nEine weitere Neuerung ist der MagSafe-Ladeanschluss: Das Netzkabel wird nicht mehr mechanisch mit seinem Stecker im Notebook gehalten, sondern mit einem Magneten. Sollte man versehentlich am Stromkabel hängenbleiben, wird lediglich der Kontaktstecker abgezogen, ohne dass der Rechner mitgerissen oder der Stecker bzw. die Buchse beschädigt wird. Im Unterschied zum 15,4″-Modell, welches 2 USB-Anschlüsse hat, verfügt das Notebook mit 17″-Bildschirm über drei USB-Anschlüsse und die beim kleineren Modell erst ab Oktober 2006 verbaute FireWire-800-Schnittstelle. Für den Anschluss eines externen Bildschirms war ein DVI-auf-VGA-Adapter im Lieferumfang enthalten. Ein Mikrofon und zwei Lautsprecher sind eingebaut. Über die Anschlüsse können digitale Audiogeräte angeschlossen werden. In der Mitte des oberen Bildschirmrahmens befindet sich eine iSight-Kamera (mit 640 × 480 Pixeln Auflösung).\n\nDer bislang übliche PC-Card- oder PCMCIA-Slot wurde mit dem MBP auf den ExpressCard/34-Slot umgestellt. Vorhanden sind darüber hinaus Bluetooth 2.1, Wi-Fi 4, eine Festplatte mit Sudden Motion Sensor (Bewegungssensor) und ein Infrarot-Sensor für die Fernbedienung Apple Remote. Der DVD-Brenner besaß im 15,4″-Modell keine Double-Layer-Fähigkeit (nur bei den 17-Zöllern) und nur vierfache Brenngeschwindigkeit.\n\nDie Tastatur ist je nach Umgebungshelligkeit wahlweise hintergrundbeleuchtet. Der Bildschirm passt sich dank der in den seitlichen Lautsprechern verborgenen Lichtsensoren ebenfalls der Umgebungshelligkeit an.\n\nAls einfaches optisches Unterscheidungsmerkmal der ersten Versionen kann die kleine grün-leuchtende Statusanzeige für die iSight-Kamera dienen: Sie ist seit Oktober 2006 hinter der Oberfläche des Displayrahmens verbaut und ausgeschaltet nicht zu erkennen. Im Vorgängermodell sitzt die grüne Leuchtdiode gut sichtbar im Gehäuse.\n\nAm 24. Oktober 2006 wurde die verbesserte Folgeversion des MacBook Pro vorgestellt. Statt des Core Duos wurden nun \"Core 2 Duo\"-Prozessoren verwendet. Außerdem erhielt jetzt auch das 15″-Modell eine FireWire-800-Schnittstelle. Der Arbeitsspeicher wurde verdoppelt und auch der Festplattenspeicher wurde vergrößert.\n\nDie 15,4″-Varianten des MacBook Pro besaßen nun einen Bildschirm mit LED-Hintergrundbeleuchtung. Der große Bildschirm (17 Zoll) mit einer Auflösung von 1920 × 1200 Bildpunkten verwendete weiterhin die zuvor übliche Kathodenröhre (engl. CCFL). Die Vorteile der LED-Beleuchtung sind ein geringerer Stromverbrauch, bessere Farbwiedergabe und verbesserte Umweltverträglichkeit. Die Notebooks hatten einen schnelleren Grafikchip (Nvidia Geforce 8600M GT) sowie einen schnelleren Intel-Chipsatz erhalten.\n\nAb 1. November 2007 konnten neue Prozessoren der neuen Santa-Rosa-Generation optional konfiguriert werden. Die optionalen Festplattenkonfigurationen wurden ebenfalls überarbeitet.\n\nDiese überarbeiteten MacBook Pros waren mit den neuen \"Penryn\"-Intel-Chips in 45-nm-Bauweise ausgestattet. Diese brachten 6 MB Cache-Speicher, eine höhere Taktfrequenz und den neuen Befehlssatz SSE4 mit sich. Außerdem verbrauchen die neuen Prozessoren weniger Energie als ihre Vorgänger und sind so besser für den mobilen Einsatz geeignet. Der VRAM der Grafikchips wurde im Vergleich zu den Vorgängermodellen verdoppelt.\n\nAlle MacBook Pros waren nun serienmäßig mit 2 GB Arbeitsspeicher ausgestattet und besaßen eine 200-GB- oder eine 250-GB-Festplatte. Im 17-Zoll-Modell stand außerdem eine 300-GB-Festplatte zur Auswahl. Der Bildschirm des 17-Zoll-MacBook-Pro konnte nun optional mit einer Auflösung von 1920 × 1200 Pixeln und LED-Hintergrundbeleuchtung bestellt werden, wodurch sich die angegebene Akkulaufzeit um eine halbe Stunde auf fünf Stunden verlängerte.\n\nNeu war außerdem das Trackpad mit Multi-Touch-Funktionalität. Damit ist es möglich, das MacBook Pro – ähnlich wie beim iPhone – durch Gesten zu steuern.\n\nDie Preise wurden im Vergleich zum Vorgänger deutlich gesenkt.\n\nMit der Einführung des Unibody-Designs Ende 2008 wurde die 15,4″-Variante eingestellt, während das 17″-Modell, für das es noch keinen Nachfolger gab, im alten Design und mit weniger Konfigurationsmöglichkeiten weiterverkauft wurde. Optional war erstmals eine SSD-Festplatte erhältlich.\n\nMit der zweiten Generation wurde das Design überarbeitet.\n\nDas Gehäuse ist aus einem Stück Aluminium gearbeitet (gefräst, gebohrt und gelasert). Apple bezeichnet diese Konstruktion als Unibody. Ein neues Großformat-Trackpad aus Glas kommt ohne separate Taste aus, denn das ganze Trackpad ist die Taste (ähnlich wie beim Click Wheel des iPods). Es unterstützt Multi-Touch-Gesten mit bis zu vier Fingern. Zum Aufklappen verwendet es keine Taste mehr, sondern eine Einkerbung im Gehäuse, an der man das Display anhebt. Diese war schon vom MacBook Air bekannt und wurde auch in den folgenden Generationen verwendet.\n\nUm den Hochglanz-LCD-Bildschirm mit LED-Hintergrundbeleuchtung befindet sich ein schwarzer Rahmen. Eine Glasscheibe bedeckt die komplette Vorderseite.\n\nIn diesen Modellen wird eine neue Grafikkarten-Generation eingeführt. Erstmals werden gegen Aufpreis SSD-Massenspeicher verbaut, welche eine weitaus höhere Geschwindigkeit als herkömmliche Festplattenlaufwerke aufweisen. Dadurch werden Aufgaben wie das Hochfahren des Computers oder das Öffnen von Programmen schneller und mit weniger Verzögerungen absolviert. Die Schreibgeschwindigkeit ist ebenfalls höher, wodurch auch aufwendige Aufgaben wie der Export von Videos weitaus schneller abgeschlossen werden.\n\nZeitgleich wurde eine kleinere 13″-Variante eingeführt, welche die meisten Eigenschaften des größeren Modells übernahm, jedoch nur „MacBook“ hieß. Die aktualisierte Auflage zur WWDC 2009 hieß „MacBook Pro“.\n\nDas MacBook Pro 15,4″ hat einen leistungsstärkeren Prozessor bekommen. Im Zuge dessen wurde auch eine 17″-Ausgabe eingeführt. Diese kann nach Wunsch mit entspiegeltem Display geordert werden. Bei dieser Option wurde, wie bei den Vorgängermodellen und beim MacBook Air, ein Aluminiumrahmen verwendet. Bei diesem Modell wird der Akku erstmals fest verbaut.\n\nZur WWDC stellte Apple am 9. Juni 2009 eine aktualisierte Version vor. Der Akku wird nun in allen Modellen fest verklebt. Die Lebensdauer wird mit etwa 1000 Zyklen angegeben. Gegen Aufpreis war nun auch das 15″-Gerät mit einem entspiegelten Bildschirm lieferbar.\n\nDas 13″-Alu-MacBook wurde durch das 13″ MacBook Pro ersetzt, welches nun ebenfalls einen FireWire-800-Anschluss besitzt. Anstelle des Express-Card-Slots wurde bei den beiden kleineren Modellen ein SD-Kartenleser verbaut. Nur beim 17″-Modell wurde dieser nicht ersetzt.\n\nDie Prozessoren sind etwas schneller.\n\nIn den 15″- und 17″-Modellen wurden nun Prozessoren der neuen Intel-Core-i-Serie verbaut. Das 13″-MacBook-Pro erhält leicht schnellere Prozessoren, die jedoch weiterhin der Core 2 Duo-Generation angehören. Die Akkulaufzeit erhöhte sich auf 10 Stunden beim 13″-Gerät, 8 bis 9 Stunden beim 15″-Gerät, und 9 Stunden beim 17″-Gerät. Das MagSafe-Ladekabel ist nun nach hinten abgewinkelt.\n\nDie Grafikprozessoren wurden durch neue Nvidia-Chips (320M oder GT 330M) und die integrierten Grafikeinheiten der Intel-Prozessoren ersetzt; optional war eine höhere Bildschirmauflösung von 1680 × 1050 Pixeln (spiegelnd oder matt) beim 15″-Modell erhältlich. Das Multi-Touch-Trackpad unterstützte nun systemweit „Scrollen mit Nachlauf“, bei dem die Scrollbewegung wie zuvor beim iPhone oder der Magic Mouse nachscrollt.\n\nDer Mini-DisplayPort führt nun auch das Audio-Signal. Mit einem Adapter auf HDMI kann über ein einziges Kabel Bild und Ton an Fernseher oder Projektoren übertragen werden.\n\nIn dieser Version wurden neue Intel Core i5- bzw. i7-Prozessoren der Sandy-Bridge-Mikroarchitektur verwendet. Bei den 15″- und 17″-Modellen wurde dabei die Anzahl der Kerne auf 4 verdoppelt. In allen Modellen kam die integrierte Intel HD Graphics 3000 zum Einsatz; in den 15″/17″-Modellen arbeitet zusätzlich eine AMD Radeon HD 6490M bzw. 6750M Grafikkarte.\n\nDie Akkulaufzeiten wurden aufgrund realistischerer Mess-Methoden mit sieben Stunden angegeben, es sollten aber die gleichen Laufzeiten wie bei den älteren Modelle erreicht werden. Die neue FaceTime-HD-Kamera unterstützte nun die HD-Auflösung von 720p (1280 × 720 Pixel).\n\nDer Mini-DisplayPort wurde durch einen funktionaleren Thunderbolt-Anschluss ersetzt, welcher äußerlich die gleiche Form besitzt und auch mit älteren Kabeln und Adaptern arbeitet. Es konnten 2 × 8 GB Arbeitsspeicher statt maximal 8 GB RAM eingesetzt werden. Die Taktrate dessen wurde von 1066 MHz auf 1333 MHz erhöht.\n\nApple verwendete nun leicht höher getaktete Prozessoren. Die Ausstattung der Grafikkarten wurde verbessert. Beim 13″-Modell wurden auch die Festplattenoptionen verbessert. Alle Komponenten basieren weiterhin auf der bisherigen Technik.\n\nDiese Modelle unterstützen erstmals AirPlay.\n\nAb diesem Modell unterstützen die MacBooks offiziell nur noch Mac OS 10.7 (Lion) und höher. Entgegen den Herstellerangaben kann Mac OS X 10.6 (Snow Leopard) auf diesem Modell aufgrund der großen Ähnlichkeiten zur Hardware von Anfang 2011 ab Version 10.6.7 jedoch weiterhin genutzt werden.\n\nDiese verbesserten Modelle wurden zeitgleich mit der dritten Generation vorgestellt. Sie waren eine kostengünstigere Alternative und boten weiterhin ältere Anschlüsse. Ihre Prozessoren wurden ebenfalls auf die Ivy-Bridge-Architektur aktualisiert. Die USB-Schnittstellen beherrschen jetzt USB 3.0. Alle MacBook Pro von 2012 verfügen über eine Intel-HD-Graphics-4000-Grafikeinheit. Die 15″-Geräte besitzen zusätzlich eine NVIDIA GeForce GT 650M Grafikkarte. Das leistungsschwächste MacBook Pro mit 13″ verfügt über einen Intel-Core-i5-Prozessor. Alle übrigen Modelle sind mit einem Intel-Core-i7-Prozessor ausgestattet.\n\nDie 17″-Variante entfällt.\n\nDas 13″-Modell wurde noch bis zur Einführung der vierten Generation im Oktober 2016 verkauft. Zu Beginn betrug der Grundpreis 1399 €. Ende 2013 wurde der Preis auf 1199 € und Mitte 2014 auf 1099 € gesenkt. Von Anfang 2015 bis zur Einstellung kostete es wieder 1199 €.\n\nBei den 15″ und 17″-Modellen, die zwischen Februar 2011 und Dezember 2013 verkauft wurden, können Probleme bei der Videowiedergabe auftreten. Für dieses Problem bietet Apple ein Reparaturprogramm an. Es ist bis 4 Jahre nach Kaufdatum gültig.\n\nDie dritte Generation wurde in Form des 15-Zoll-MacBook Pro mit Retina Display am 11. Juni 2012 auf der WWDC von Phil Schiller vorgestellt. Gleichzeitig wurden die Modelle ohne Retina-Display mit verbesserter Technik weiterverkauft. Das Modell mit 13 Zoll folgte am 23. Oktober.\n\nAlle MacBook Pro von 2012 verfügen über Intel-Prozessoren der Ivy-Bridge-Generation und über eine Intel-HD-Graphics-4000-Grafikeinheit. Die 15″-Geräte besitzen zusätzlich eine NVIDIA GeForce GT 650M Grafikkarte. Das leistungsschwächste MacBook Pro mit 13″ verfügt über einen Intel-Core-i5-Prozessor. Alle übrigen Modelle sind mit einem Intel-Core-i7-Prozessor ausgestattet. Die USB-Schnittstellen beherrschen jetzt USB 3.0.\n\nDas 15″-Retina-MacBook-Pro ist 25 % flacher und leichter.\n\nDie Geräte werden ausschließlich mit Flash-Speicher angeboten. Der Arbeitsspeicher basiert auf der sparsameren DDR3L-Technik anstelle von DDR3. Er ist fest verlötet und nicht erweiterbar, im Gegensatz zum nur eingesteckten Flash-SSD-Speicher, für den es im freien Markt Ersatzmodule gibt.\n\nAnschlüsse für Ethernet-Netzwerkkabel und FireWire entfallen, optische Laufwerke werden nicht mehr verbaut (Thunderbolt-Adapter für Gigabit-Ethernet und FireWire sowie ein externes optisches Laufwerk werden angeboten). Als Stromanschluss wird MagSafe 2 verbaut, welcher flacher und länger ist. Ältere MagSafe-Netzteile können über einen separat erhältlichen Adapter verwendet werden.\n\nDie Geräte verfügen über einen Umgebungslichtsensor, der eine automatische Regelung der Bildschirmhelligkeit sowie Tastaturbeleuchtung ermöglicht.\n\nAlle Prozessoren takten 0,1 GHz schneller. Bedeutender ist, dass die Preise flächendeckend gesenkt wurden.\n\nIm Gegensatz zu den Modellen von Anfang 2013 gab es Ende 2013 tiefgehendere Änderungen. Dazu zählt die Verwendung der neuen Haswell-Prozessoren von Intel. Das 13″-Modell ist jetzt 1 mm dünner (vorher 19 mm, jetzt 18 mm) und daher rund 50 g leichter (vorher 1,62 kg, jetzt 1,57 kg). Ansonsten ist das Design unverändert. Bei den 13″-Modellen wird der integrierte Intel Iris Grafikchip und bei den 15″-Modellen der integrierte Iris Pro Grafikchip verwendet. Optional kann beim 15″-Modell eine zusätzliche NVIDIA GeForce GT 750M mit 2 GB bestellt werden. Die Akkulaufzeit wurde bei beiden Modellen verlängert.\n\nIn Deutschland wurden die Preise gegenüber dem Vorgänger um 200 € gesenkt:\n\nAlle 13″-Modelle haben nun 8 GB DDR3L-RAM und können optional mit 16 GB konfiguriert werden. Die 15″-Modelle sind jetzt mit 16 GB DDR3L-RAM ausgestattet. Die Taktrate der Prozessoren wurde jeweils um 0,2 GHz angehoben (Haswell-Refresh).\n\nPreisanpassungen:\n\nDas sogenannte „Force Touch Trackpad“ wurde vom neuen MacBook übernommen. Außerdem wurden beim 13″-Modell Intel Prozessoren der Broadwell-Generation verwendet. Die Taktraten der Prozessoren in den 15″-Modellen wurden leicht erhöht, jedoch wurden hier aufgrund von Lieferproblemen seitens Intel weiterhin ältere Haswell-Prozessoren verbaut. Anstelle von DDR3L Arbeitsspeicher bietet jedes MacBook Pro von 2015 LPDDR3 Arbeitsspeicher, welcher noch einmal sparsamer ist. Beim 13″-Modell wurde dessen Taktrate von 1600 MHz auf 1866 MHz erhöht, was beim 15″-Modell nicht der Fall ist. Die dedizierten Grafikkarten basieren auf einer neuen Generation, welche nicht mehr von Nvidia, sondern von AMD geliefert wird. Der eingebaute Flash-basierte SSD-Speicher ist nun doppelt so schnell. Die Akkulaufzeit wurde um jeweils eine Stunde erhöht. Aufgrund der größeren Akkus sind die Geräte minimal schwerer.\n\nDie Preise wurden aufgrund des Wechselkurses in Deutschland und Österreich angehoben:\n\n\nDie 15″-Variante wurde noch bis zum 12. Juli 2018 verkauft und zusammen mit der Einführung neuer Touch Bar-Modelle eingestellt.\n\nBei allen MacBook Pro der dritten Generation kann sich bei der Nutzung die Antireflexbeschichtung lösen. Im September 2016 begann Apple ein Reparaturprogramm, das für Geräte bis 3 Jahre nach dem Kaufdatum gilt. Im November 2017 wurde das Programm auf 4 Jahre verlängert.\n\nBei den 15″-Modellen, die bis Dezember 2013 verkauft wurden, können Probleme bei der Videowiedergabe auftreten. Für dieses Problem bietet Apple ein Reparaturprogramm an. Es ist bis 4 Jahre nach Kaufdatum gültig.\n\nZur Keynote am 27. Oktober 2016 wurden drei neue Modelle des MacBook Pro vorgestellt. Die auffälligste Neuerung der beiden besser ausgestatteten Modelle ist die individualisierbare OLED-Leiste „Touch Bar“ mit Touch ID. Einheitlich hingegen ist die Einführung der „Butterfly“-Tastatur, wie sie aus dem MacBook bekannt ist, hier in der zweiten Generation. In allen Modellen werden Intel-Skylake-Prozessoren verbaut. Im Vergleich zum Vorgänger ist das Force-Touch-Trackpad beim 13″-Modell 46% größer und beim 15″-Modell sogar doppelt so groß. Das Display ist 25 % heller (500 cd/m²) als zuvor. Mit dem P3-Farbraum kann es mehr Farben darstellen und auch der Kontrast wurde erhöht. Der Speicher besitzt nun eine Lesegeschwindigkeit von 3,2 GB pro Sekunde und eine Schreibgeschwindigkeit von 2,2 GB pro Sekunde. Auch die Lautsprecher wurden verbessert, wobei die Löcher seitlich der Tastatur beim 13″-Modell vorwiegend dekorativen Zwecken dienen, während sich dahinter beim 15″-Modell die Lautsprecher befinden. Beide Modelle sind leichter, dünner und besitzen ein kleineres Gehäuse bei gleicher Displaygröße, wobei das 13″-Modell mit 14,9 mm noch etwas dünner als das 15,5 mm hohe 15″-Modell ist. Neben dem bisherigen Silber ist es nun auch in Spacegrau erhältlich. Mit der neuen Generation wurden viele der bisherigen Anschlüsse entfernt, darunter ältere Versionen wie Thunderbolt 2 und USB A, aber auch HDMI, der magnetische Ladeanschluss MagSafe 2 und der SD-Karten-Steckplatz. Das Logo auf der Rückseite ist nicht mehr beleuchtet, sondern besteht aus Metall und ist farblich an das Gerät angepasst. Der Akku kann über einen der Thunderbolt-Anschlüsse geladen werden. Auch der Lieferumfang wurde reduziert. So ist kein Verlängerungskabel und kein Mikrofasertuch mehr enthalten. Die Möglichkeit, das Kabel am Netzteil aufzuwickeln, entfällt. Ladekabel und Netzteil können nun unabhängig voneinander verwendet werden. Durch die Nutzung des USB-C-Standards kann das MacBook Pro nun auch beispielsweise über eine Powerbank geladen werden.\n\n\n\n\nEine verbesserte Version wurde am 5. Juni 2017 auf der WWDC vorgestellt. Die größte Änderung sind die Core-i-Prozessoren der siebten Generation. Inoffiziell sollen auch kleinere Probleme behoben worden sein, zum Beispiel soll eine verbesserte Tastatur verhindern, dass die Tasten wie bei der vorherigen Generation vereinzelt klemmen.\n\nDie 15″-Modelle erhielten neue, besser ausgestattete Grafikkarten (die erste Ziffer benennt die Generation, die anderen beiden die Ausstattung):\n\nDas 13″-Modell ohne Touch Bar ist nun mit 128 GB Flash-Speicher für 1499 € erhältlich. Der Preis mancher Varianten wurde erhöht:\n\n\n\nAm 12. Juli 2018 wurde ohne gesonderte Pressekonferenz eine überarbeitete Ausgabe veröffentlicht. Die 13″-Modelle ohne Touch Bar blieben unverändert. Zu diesem Anlass wurde das 15″-MacBook Pro von 2015 eingestellt, welches bis dahin noch verkauft wurde.\n\nZu den wichtigsten Neuerungen zählt die Einführung von Intel-Core-Prozessoren der 8. Generation, genannt Coffee-Lake, und eine erneut überarbeitete Tastatur, welche den „Butterfly“-Mechanismus der dritten Generation verwendet. Die neuen Prozessoren beinhalten zwei Kerne mehr als die der Vorgänger. Das 13″-Modell erhält damit zum ersten Mal mehr Kerne, beim 15″-Modell wurde die Anzahl der Kerne zuletzt 2011 erhöht. Laut Apple sei die neue Tastatur leiser beim Tippen, wurde offiziell aber nicht mit dem Ziel entwickelt, die mit den Modellen von 2016 und 2017 weiter anhaltenden Probleme bezüglich der Zuverlässigkeit zu beheben. Beim Zerlegen des Gerätes fand iFixit heraus, dass dieser Effekt durch den Einsatz einer Silikon-Dichtung erzielt wird, welche auch die Zuverlässigkeit verbessern könnte.\n\nWeiterhin erhalten alle neuen MacBooks die vom iPad Pro bekannte True-Tone-Technologie, bei der sich die Farben des Displays an das Umgebungslicht anpassen. Der Akku beider Geräte wurde vergrößert. Die Lautsprecher wurden überarbeitet. Beide Versionen sind geringfügig dünner als ihre Vorgänger. Wie bereits beim iMac Pro der 1. Generation werden die neuen MacBook Pro mit einem Apple T2-Koprozessor ausgestattet. Dieser steuert den Startvorgang und fügt weitere Sicherheitsmerkmale hinzu, er kümmert sich um die Anbindung der Kamera, regelt den Zugriff auf die SSD und gegebenenfalls deren Verschlüsselung, steuert den System Management Controller (SMC), regelt die Anbindung an den Audio-Controller und ist für die sichere Speicherung der Daten des Fingerabdrucksensor zuständig. Außerdem ermöglicht der T2-Koprozessor das aktivieren von Siri mit den Worten „Hey Siri“ ohne eine Taste zu betätigen. Des Weiteren unterstützen die neuen MacBook Pro Bluetooth 5.0. Die 15″-Variante nutzt statt LPDDR3 nun den DDR4 Arbeitsspeicher, wodurch dieser optional auf 32 GB erweitert werden kann. Diese Technik verbraucht allerdings mehr Strom. Beide Modelle können mit höchstens doppelt so viel Flash-Speicher als bisher bestellt werden. Das 15″-Modell erhält neue, dedizierte AMD-Grafikkarten, welche etwas höher getaktet sind und in der Grundausstattung mehr Speicher besitzen.\n\nSeit Ende November 2018 konnte das 15″-Modell mit 512 GB Flash-Speicher gegen Aufpreis mit Radeon-Pro Vega-Grafikkarten bestellt werden.\n\nDie neue Butterfly-Tastatur konnte zu diversen Problemen führen. Am verheerendsten ist dabei, dass die Tasten durch Schmutz- und Staubteilchen blockiert werden können. Mit Druckluftspray ließ sich das Problem manchmal, jedoch nicht immer beheben. Seit der Einführung im MacBook 2015 wurde die Tastatur mit jeder Revision verbessert, weswegen bereits vier verschiedene Versionen existieren. Nach mehreren Petitionen und Sammelklagen stellte Apple ein Austauschprogramm für alle MacBooks mit einer Butterfly-Tastatur aus den Modelljahren 2015 bis 2017 bereit.\n\nNutzer der ersten Modelle mit Touch Bar beschwerten sich über ein Knacken unbekannter Herkunft. Betroffene Modelle wurden von Apple innerhalb der Garantiezeit repariert oder ausgetauscht. Im Januar 2019 wurde bekannt, dass das im MacBook Pro mit Touch Bar ab 2016 verbaute Flachbandkabel zur Verbindung des Retina-Displays mit dem Display-Controller zu kurz und zu dünn ist, was im normalen Alltagsbetrieb zu Kabelbrüchen und Displayschäden führen kann. Um dieses Problem zu beheben, muss die gesamte Displayeinheit ausgetauscht werden.\n\nDie MacBook Pro von 2018, insbesondere die Core-i9-Variante, erhitzten laut ersten Erfahrungsberichten unter Last sehr stark und konnten dadurch Aufgaben teilweise langsamer abschließen als der Vorgänger. Symptomatisch waren große Schwankungen des Prozessortaktes, was laut Apple auf ein softwareseitiges Problem zurückzuführen ist und mit einem ergänzenden Update für macOS 10.13.6 speziell für dieses Modell behoben wurde.\n\n\"Daten, die kursiv und grau geschrieben sind, können nur auf der Internetseite optional konfiguriert werden. Geräte mit solchen Sonderausstattungen werden oftmals als built-to-order (BTO) bezeichnet, weil sie pro Bestellung extra angefertigt werden, wodurch die Lieferzeit in der Regel länger ist.\"\n\n"}
{"id": "1168584", "url": "https://de.wikipedia.org/wiki?curid=1168584", "title": "IWeb", "text": "IWeb\n\niWeb ist ein Programm des Unternehmens Apple zum einfachen Erstellen von Websites. iWeb war Bestandteil des Softwarepaketes iLife Version ’06 bis Version ’11. Die Entwicklung wurde inzwischen jedoch eingestellt.\n\niWeb richtet sich vor allem an Nutzer, welche auch ohne Kenntnisse von HTML schnell gute Ergebnisse bei der Erstellung von Webseiten erzielen wollen. iWeb ist sehr gut für Einsteiger geeignet.\n\niWeb benötigt keine serverseitige Scriptsprache und keine Datenbanken. Für den vollen Funktionsumfang muss die Webseite auf MobileMe veröffentlicht werden.\n\niWeb ist ein WYSIWYG-Editor. Hierfür nutzt iWeb vorgefertigte Vorlagen (Themes), welche überwiegend auf grafischen Elementen und Bilddateien basieren. Diese Vorlagen lassen sich schnell und einfach anwenden und austauschen. Ein Abändern der Vorlagen oder Erstellen von individuellen Vorlagen ist mit Bordmitteln nicht möglich. Die erstellten Seiten lassen sich aber sehr einfach individuell anpassen. Durch Kopieren (Duplikate) der individualisierten Seiten lassen sich diese wie Vorlagen verwenden.\n\nEine Quellcodebearbeitung fehlt. Eine teilweise Ergänzung von Quellcode ist aber durch die HTML-Snippet-Funktion möglich. Die geschickte Nutzung der Möglichkeiten des Programms erlaubt andererseits weitgehende grafische Freiheiten.\n\nDie Webseiten werden in einem festen Layout erstellt. Der Inhalt wird in einen statischen Bereich dargestellt, der standardmäßig exakt 700px breit ist und im Browserfenster (Viewport) zentriert wird. Alle verwendeten Bestandteile (Textfelder, Formen und Grafiken) werden mit festen Größen und Positionen auf der Webseite angelegt. Die verwendete Technologie sichert eine einheitliche Darstellung auf allen Internet-Browsern. Wird im Browser eine Skalierungsfunktion verwendet, so wirkt sich diese auf den gesamten Inhalt aus. Alle Bestandteile werden also gleichmäßig skaliert und das optische BIld – der visuelle Eindruck – bleibt erhalten.\n\nDas Verfahren hierzu ist vergleichbar dem heute im E-Book-Bereich genutzten Fixed Layout EPUB.\n\nAuch wenn iWeb keine Resposives Webdesign nutzt, kann das Ergebnis sowohl auf Mobilgeräten, Tablets wie auch Desktopsystem gute bis sehr gute Ergebnisse erzielen.\n\nFolgende Bearbeitungsfunktionen sind verfügbar:\nHilfsfunktionen:\nDie grafische Oberfläche unterstützt die Gestaltung mit intelligenten Hilfslinien, die man sonst nur in professionellen Satzprogrammen findet.\n\nEinige Funktionen sind allerdings nur für Webseiten unter einem MobileMe-Account verfügbar:\ndie über iWeb in Webseiten integriert werden können, wie z. B. ein Besucherzähler oder Kommentare in Blogs, sind nur ausführbar.\n\nMit iWeb lassen sich Webseiten direkt auf\n\nEin Export in einen lokalen Ordner ist auch möglich.\n\niWeb wurde im Januar 2006 vorgestellt und ist Teil des Softwarepaketes iLife '06 von Apple.\n\nIn iWeb '08 wurden neue Widgets hinzugefügt, wie Google Maps, Google AdSense, Videos und HTML-Baustein. Als Funktion kam das Fotoalbum hinzu. Neue Themen wurden auch bereitgestellt.\n\nIn iWeb ’09 wurden neue Widgets hinzugefügt, wie iSight Video (heute FaceTime Video), iSight Foto (heute FaceTime Foto), Countdown, YouTube Video und RSS-Feeds. Mit der neu integrierten FTP-Veröffentlichung können nun beliebige Host-Server zur Veröffentlichung der Webseiten verwendet werden.\n\nMit iLife ’11 wurde iWeb letztmals ausgeliefert. Mit der Einführung des Mac App Store wurde der Vertrieb von iWeb endgültig eingestellt und zum 1. Juli 2012 wurde auch der MobileMe-Dienst geschlossen.\n\nEinen Teil der Funktionen von iWeb findet man in der Software iBooks Author von Apple wieder.\n\niWeb funktioniert auch unter dem aktuellen Apple Betriebssystem macOS.\n\nDie Konfiguration der Veröffentlichung von Webseiten auf beliebige Webservern, außerhalb von MobileMe, wurde nie vollständig implementiert. Die Suchfunktion ist hier nicht möglich. Blogs sind nur eingeschränkt nutzbar. Mancher Rest-Code für MobileMe ist in den Webseiten noch enthalten. So werden Seitenaufrufe auch an MobileMe (www.me.com) gesendet:\n<script type=\"text/javascript\" src=\"http://www.me.com/1/up/comments/scripts/search.js\"></script>\n\n"}
{"id": "1168719", "url": "https://de.wikipedia.org/wiki?curid=1168719", "title": "Microsoft Office 2007", "text": "Microsoft Office 2007\n\nMicrosoft Office 2007 (offiziell 2007 Microsoft Office System genannt) ist ein Office-Paket von Microsoft aus der Microsoft-Office-Serie. Nachfolger von Microsoft Office 2007 ist \"Microsoft Office 2010.\" Der Mainstream-Support endete am 9. Oktober 2012, der erweiterte Support z. B. mit Sicherheitsaktualisierungen endete am 11. Oktober 2017.\n\nDie klassische Menüstruktur und die üblichen Symbolleisten, die man von vorangegangenen Office-Versionen kennt, wurden durch eine Multifunktionsleiste (\"Ribbon\") ersetzt. Oft benötigte Funktionen sollen so schneller erreichbar sein. Die Multifunktionsleiste enthält in Gruppen gefasste Befehle. Diese werden ergänzt durch Listenfelder und Dialogfelder. Diese Elemente sind in einzelnen Registern zusammengefasst. Die Standard-Register einer Office-Anwendung werden durch weitere Register ergänzt, die situationsabhängig verfügbar sind. Von Letzteren gibt es zwei Typen: \"Programmregisterkarten\" stehen in bestimmten Ansichtsmodi zur Verfügung, \"Kontextregisterkarten\" zeigen Befehle, die nur in bestimmten Situationen zur Verfügung stehen, beispielsweise bei der Arbeit mit Grafiken, Tabellen und Diagrammen.\n\nEin weiteres neues Feature sind die sogenannten \"Live Previews\": Formatfunktionen, Vorlagen und Designs werden in einer Echtzeitvorschau im Dokumentenfenster angezeigt.\n\nMit OpenXML führt Microsoft ein neues, auf XML basierendes Standard-Dateiformat ein. Ältere Versionen von Microsoft Office (Microsoft Office-Versionen 2000, XP und 2003) können dieses Format mit einem kostenlosen Konverter (\"Compatibility Pack\") verarbeiten. Für Mac OS X existiert ein entsprechender Formatumwandler für Microsoft Office 2004; darüber hinaus sind die freien Office-Pakete NeoOffice und OpenOffice.org in der Lage, Office-2007-Dateien zu öffnen.\n\nMicrosoft Office 2007 kann Dokumente auch im sogenannten „Kompatibilitätsmodus“ in den mit Office 97 eingeführten, alten Dateiformaten speichern, wodurch sie mit älteren Office-Versionen für Windows und Mac OS ohne Hilfsmittel lesbar sind. In diesem Modus stehen Funktionen, die es in der alten Word-Version noch nicht gab, nicht zur Verfügung. Das betrifft z. B. Alternativtexte zu Tabellen oder neue WordArt-Effekte.\n\nMicrosoft Office 2007 führte ein neues Exportformat namens XPS (XML Paper Specification) ein, das Ähnlichkeiten zu PDF aufweist. Nach Differenzen mit Adobe wurde die PDF-Exportfunktion, die in der Beta 2 noch integriert war, in der finalen Version wieder entfernt. Es kann jedoch ein kostenloses Add-In installiert werden, das das Speichern in den Formaten PDF und XPS aus den Anwendungen heraus ermöglicht. Seit dem Service Pack 2 ist es möglich, Dokumente im OpenDocument-Format zu speichern; der PDF- und XPS-Export wurde wieder integriert.\n\n2005 kündigte Microsoft eine neue Version von Office unter dem Codenamen „Office 12“ als Nachfolger von Office 2003 an. Office 2007 wurde am 30. Januar 2007 gleichzeitig mit dem Betriebssystem Windows Vista auf den Markt gebracht. Für Geschäftskunden wurde Office 2007 wie auch Vista bereits im November 2006 bereitgestellt.\n\nAb dem 23. Mai 2006 wurde die Beta 2 von Bill Gates anlässlich der WinHEC vorgestellt. Sie wurde im Rahmen des bisher umfangreichsten Microsoft-Betaprogramms zum Testen zugänglich gemacht und im Zeitraum von Ende Mai bis Ende Oktober 2006 weltweit mehr als 3,5 Millionen Mal heruntergeladen, davon in Deutschland allein 500.000 Mal.\n\nAm 14. September 2006 erschien ein Technical Refresh zur Beta 2. Damit wurden noch einige Änderungen an der Benutzeroberfläche vorgenommen, die nun dem fertigen Produkt entspricht. Auch wurden Programmfehler der Beta 2 behoben, so dass das Produkt nunmehr stabiler läuft.\n\nDie Betaphase ist seit dem 30. Oktober 2006 beendet, die Betaversionen können nicht mehr heruntergeladen werden. Bereits installierte Beta-2-Versionen ohne Technical Refresh konnten im vollen Funktionsumfang nur noch bis 1. Februar 2007 genutzt werden, Beta-2-Versionen mit Technical Refresh waren bis 31. März 2007 voll nutzbar. Seit diesem Stichtagen laufen die in der Version enthaltenen Programme nur noch mit funktionellen Einschränkungen.\n\nMit dem „Release to Manufacturing“ (RTM) am 8. November 2006 ist die Entwicklung abgeschlossen. Für Unternehmen ist Office 2007 seit Ende November 2006 verfügbar, für Einzelanwender seit 30. Januar 2007.\nSeit dem 26. Oktober gibt Microsoft eine Technologiegarantie, dass ab diesem Datum gekaufte Rechner mit einer Office-2003-Vollversion auf Office 2007 aktualisiert werden können (es fallen Bearbeitungsgebühren an). Über die Microsoft Office 2010 Technology Garantie erhielt bis zum 31. Oktober 2010 der Käufer von Office 2007 automatisch eine kostenlose Upgrade-Lizenz auf Microsoft Office 2010, sofern dieser Office 2007 nach dem 5. März 2010 erworben hat.\n\nFür die verschiedenen Nutzergruppen ist Office in verschiedenen Zusammenstellungen erhältlich:\nIn Office 2007 ist FrontPage nicht mehr enthalten. Es wird abgelöst von Microsoft Expression zur Webseiten-Erstellung für das Internet und SharePoint-Designer zur Gestaltung von SharePoint-Seiten vornehmlich für das Intranet.\n\n\n"}
{"id": "1169339", "url": "https://de.wikipedia.org/wiki?curid=1169339", "title": "Höchstleistungsrechner Bayern II", "text": "Höchstleistungsrechner Bayern II\n\nDer Höchstleistungsrechner Bayern II, kurz \"HLRB II,\" war ein Supercomputer, der im Neubau des Leibniz-Rechenzentrums in Garching bei München betrieben wurde. Der Rechner war ein System von Silicon Graphics auf Basis der Altix-4700-Plattform. Er wurde am 21. Juli 2006 in Betrieb genommen. Im Juni 2011 war der HLRB II in der Liste der 500 schnellsten Supercomputer auf Platz 15 der deutschen Rechner und stand weltweit auf Platz 198.\n\nIn der Ausbauphase 1 verfügte er über 4096 Prozessoren mit einer Spitzenleistung (Rpeak) von 26,2 Tera-FLOPS. Als Prozessoren wurden Intel-Prozessoren vom Typ Itanium 2 Madison 9M mit 1,6 GHz eingesetzt. Das verwendete Betriebssystem war SUSE Linux Enterprise Server 10.\n\nIm April 2007 wurden im Zuge der zweiten Ausbaustufe die Prozessoren durch den Typ Itanium 2 Montecito Dual Core ersetzt, der Rechner verfügte damit über 9728 Prozessorkerne und erreichte eine Spitzenleistung von 62,3 Tera-FLOPS. Er besaß außerdem 39 TByte Hauptspeicher, das Storage Area Network wurde auf 600 TByte erweitert.\n\nDie Anschaffungskosten betrugen ca. 38 Millionen Euro und wurden vom Bund und dem Freistaat Bayern getragen. Die Betriebskosten von ca. 3 Millionen Euro pro Jahr trug der Freistaat Bayern alleine.\n\nDas gesamte System wog 103 Tonnen und benötigte bei Volllast ca. 1,1 MW an elektrischer Leistung.\n\nIm Juli 2009 wurde die Erweiterung des Rechenzentrums um einen zweiten Höchstleistungsrechner, den SuperMUC, beschlossen. Dabei wurde ein zweites, ebenfalls würfelförmiges Gebäude direkt an dem 2006 fertiggestellten HLRB II gebaut, in dem der Rechner installiert wurde. Die Kosten für die Erweiterung sollten 135 Millionen Euro betragen und wurden vom Bund und dem Freistaat Bayern getragen. Der Bau wurde im Frühjahr 2011 abgeschlossen. Bis zur offiziellen Einweihung des SuperMUC im Juli 2012 wurde der HLRB II vollständig abgebaut.\n\nDie Hauptanwendungen des Rechners sind Large-Eddy-Simulation, Lattice-Boltzmann-Applikation, Quantenchromodynamik, Astrophysik, Hochenergiephysik und Quantenchemie.\n"}
{"id": "1170349", "url": "https://de.wikipedia.org/wiki?curid=1170349", "title": "PYTHIA", "text": "PYTHIA\n\nPythia ist ein Computerprogramm, das in der Teilchenphysik verwendet wird, um Kollisionen an Teilchenbeschleunigern zu simulieren. Pythia ist der älteste und meistverwendete Monte-Carlo-Ereignisgenerator.\n\nUm herauszufinden, durch welche Signale sich verschiedene, insbesondere vom Standardmodell abweichende, Physikmodelle an Teilchenbeschleunigern bemerkbar machen könnten, ist es oft hilfreich, diese \nModelle im Vorfeld numerisch zu simulieren. Beispielsweise kann für den Spezialfall, dass das Physikmodell ein neues instabiles Teilchen voraussagt, folgende Vorgehensweise gewählt werden:\n\nDamit können bereits im Vorfeld von Experimenten Hinweise auf die zu suchenden Signale gewonnen und gegebenenfalls die Detektoren auf diese Signale optimiert werden.\n\nEs werden typischerweise einige tausend bis Millionen von Teilchen simuliert. Die Zerfallskanäle werden dabei vorgegeben. Das heißt, man kennt oder wählt die Wahrscheinlichkeit, mit dem es in bestimmte andere Teilchen zerfällt. Per Zufall wird dann jeweils der eine oder andere Zerfall simuliert. Realisiert wird das durch eine Monte-Carlo-Simulation. Die Zerfallsprodukte können gegebenenfalls weiter zerfallen. Man hat damit pro Zerfall immer nur eine bestimmte Zerfallskette. Außerdem muss simuliert werden, wie sich die Zerfallsprodukte im Detektor verhalten, dies wird durch andere Programme wie Geant4 übernommen. Dazu kommen üblicherweise Simulationen von anderen Prozessen, die im Detektor ähnlich aussehen können und somit bei der Analyse der Daten berücksichtigt werden müssen.\n\nAnhand diesen Daten kann man abschätzen, wie sich das Teilchen am besten suchen lässt. Um die korrekte Funktionsweise des Programms zu überprüfen, werden Vergleiche mit anderen Programmen (z. B. \"Herwig\" oder \"Alpgen\") und mit Messdaten alter Experimente angestellt.\n\nPythia wurde ursprünglich unter dem Namen \"Jetset\" an der Universität Lund entwickelt. Die aktuelle Version Pythia 8 liegt in der Programmiersprache C++ vor, während der Vorgänger, Pythia 6, noch in Fortran geschrieben wurde.\n\n"}
{"id": "1170515", "url": "https://de.wikipedia.org/wiki?curid=1170515", "title": "Active Template Library", "text": "Active Template Library\n\nDie Active Template Library (ATL) ist eine Sammlung von Visual-C++-Programmbibliotheken zur Erstellung und Nutzung von COM-Komponenten, einschließlich ActiveX-Steuerelementen. Der Namensbestandteil \"Template\" (dt.: Vorlage) rührt von der ausgiebigen Nutzung von C++-Klassenvorlagen her. Im Vergleich zu unter Verwendung der MFC generierten Programmen sind die mit ATL erzeugten Komponenten kleiner und damit schneller über das Internet zu laden. Wie bei der Nutzung von Klassenbibliotheken üblich, erfolgt die Verwendung der ATL über das Einbinden der Header-Dateien und Linken mit den eigentlichen Bibliothek-Binärdateien. Visual C++ bietet jedoch spezielle ATL-Projektvorlagen, bei denen die Einbindung bereits vorgegeben ist. Es handelt sich um ein kommerzielles Produkt von Microsoft, welches mit dem VC++-Compiler vertrieben wird.\n\nDie COM-Unterstützung in VC++ gestattet es Entwicklern, eine Vielzahl von COM-Objekten, OLE-Servern und ActiveX-Controls auf einfache Art und Weise zu erstellen. Die Quellcodes der ATL sind der Standard Template Library (STL) ähnlich strukturiert; ein „Wizard“ übernimmt zusätzlich das Erstellen von Klassen und Interfaces und erleichtert dem Entwickler damit die Arbeit.\n\nControls, die in Webseiten eingebettet werden können, könnten genauso mit den Microsoft Foundation Classes erstellt werden, allerdings sollte auf die Größe der herunterzuladenden Daten geachtet werden, wo die ATL den MFC gegenüber Vorteile aufweist. Die Abhängigkeiten der MFC summieren sich je nach Verwendung auf mehrere Megabyte, wohingegen alle ATL-Abhängigkeiten in eine Bibliotheksdatei (DLL) eincompiliert werden, die nur wenige hundert Kilobyte groß ist.\n\nEin konkreter Anwendungsbereich für ATL ist Microsofts Active Server Pages, wo VBScript als Programmiersprache zum Einsatz kommt, deren Funktionsumfang durch ActiveX-Controls und COM-Objekte beliebig erweiterbar ist. Weiterhin ist ATL die Grundlage der Windows Template Library (WTL). Hierbei handelt es sich um ein von Microsoft quelloffen freigegebenes Framework zur Erstellung von Benutzeroberflächen (Dialoge, Views) für Windowsprogramme (Windows Desktop Applications/WDA).\nVon der ATL gibt es auch eine Variante namens ATL Server, um serverseitige Software zu entwickeln.\n\nDie Entwicklung von ATL begann Ende 1995, als man bei Microsoft neue Klassenbibliotheken für die \"Visual C++ 4.2 Enterprise Edition\" entwickeln wollte. Während der Entwicklungszeit wurden diese Bibliotheken zunächst als \"Microsoft Enterprise Classes\" (MEC) bezeichnet. Die erste Version, ATL 1.0, wurde im Frühsommer 1996 zum Herunterladen im Internet angeboten. Im Spätsommer folgte ATL 1.1, das neben Fehlerbehebungen auch Neuerungen wie Connection Points, NT Services, RGS Registry Support und Sicherheitsfunktionen enthielt. Mit dem darauffolgenden ATL 2.0 konnte man ActiveX-Steuerelemente erzeugen. Es wurde im Dezember 1996 zusammen mit VC 5.0 ausgeliefert. Es folgte bald Version 2.1, die lediglich Fehlerbehebungen für Alpha-, MIPS- und PowerPC-Prozessoren enthielt. ATL 2.1 wurde zusammen mit der Alpha-Version von Visual C++ 5.0 ausgeliefert; außerdem konnte es über das Internet für VC 4.2 heruntergeladen werden.\nIm Juni 1998 wurde VC 6.0 gemeinsam mit ATL 3.0 ausgeliefert. Mit der Visual-Studio-Version 2003 wurde ATL in der Version 7 ausgeliefert. Aktueller Versionsstand ist ATL 9.0. Im November 2014 veröffentlichte Microsoft das kostenlose Visual Studio Community 2013, das die aktuelle ATL beinhaltet.\n\n"}
{"id": "1170518", "url": "https://de.wikipedia.org/wiki?curid=1170518", "title": "Assemblierung", "text": "Assemblierung\n\nUnter dem Begriff Assemblierung versteht man den Zusammenbau verschiedener Komponenten.\n\nVorwiegend wird dieser Begriff in der Kerntechnik, der Elektronik, der Nanotechnik (Nanoassemblierung) oder für Computer verwendet.\n\nIn der Kerntechnik den letzten Teilschritt bei der Herstellung von Brennelementen für Kernreaktoren. Vereinfacht ausgedrückt bedeutet die Assemblierung den Zusammenbau von einzelnen Kernbrennstäben zu einem Brennelement.\n\nVor der Assemblierung stehen bei der Fertigung von Brennelementen für die am häufigsten verwendeten Leichtwasserreaktoren die Verfahrensschritte\n\nZum Abschluss werden im Verfahrensschritt „Assemblierung“ jeweils bis zu 250 Brennstäbe (im Falle von Druckwasserreaktoren) unter Verwendung von Abstandshaltern, Kopf- und Fußstücken zu fertigen Brennelementen zusammengefügt.\n"}
{"id": "1174368", "url": "https://de.wikipedia.org/wiki?curid=1174368", "title": "Textursynthese", "text": "Textursynthese\n\nTextursynthese nennt man die automatische Erzeugung von Texturen, also zweidimensionalen digitalen Bildern, die Oberflächenstrukturen oder vergleichbare Inhalte zeigen. Es gibt zwei grundlegend verschiedene Arten der Textursynthese: \"Prozedurale\" Textursyntheseverfahren erzeugen aus dem Nichts eine neue Textur, \"vorlagenbasierte\" Verfahren ahmen eine gegebene Bildvorlage nach. Ziel beider Verfahren ist es, Bilder zu erzeugen, die von realen Vorbildern nicht zu unterscheiden sind.\n\nDie Textursynthese bildet zusammen mit der \"Texturanalyse\" den gemeinsamen Forschungsbereich der Richtungen Bildverarbeitung, Computergrafik und Maschinelles Sehen, der sich den besonderen Eigenschaften von Texturen widmet. Neben der Textursynthese werden in diesem Fachbereich drei weitere Problemkreise untersucht: \"Texturklassifikation\", die Unterscheidung verschiedener Texturarten anhand messbarer Eigenschaften, \"Textursegmentation\", das Zerlegen eines Bildes in einheitlich texturierte Flächen und die \"Shape from Texture\" genannte Instanz der allgemeineren Fragestellung \"Shape from X\", bei der aus einem zweidimensionalen Bild auf die dreidimensionale Form des abgebildeten Objekts geschlossen werden soll.\n\nAnwendung findet die Textursynthese in der Bildbearbeitung, z. B. wenn unerwünschte Details in einem digitalen Foto unauffällig retuschiert werden sollen oder die Materialbeschaffenheit eines abgebildeten Objekts verändert werden soll. In der 3D-Computergrafik dient sie dazu, dreidimensionale Modelle mit möglichst realistisch wirkenden Oberflächen zu überziehen und gleichzeitig den Speicherbedarf für Texturen zu reduzieren.\n\nMenschen erfassen den Begriff der Textur intuitiv, eine präzise und damit maschinell nachvollziehbare Ausformulierung gestaltet sich jedoch schwierig und ist eines der noch unerreichten Ziele der Texturanalyse. Eine brauchbare Definition von Textur ist: \"Textur ist die Schwankung von Daten in einer Größenordnung, die kleiner ist als die Größenordnung, für die sich der Betrachter interessiert\". Ob eine Abbildung Textur ist oder nicht, ist danach eine Frage der \"Skalierung\" und des Standpunkts des Betrachters.\n\nIn der Textursynthese unterscheidet man zwei Texturartenextreme, die fließend ineinander übergehen: \"Regelmäßige\" oder \"deterministische\" Texturen zeigen ausgeprägte Strukturen, die sich mit geometrischer Regelmäßigkeit wiederholen, \"stochastische\" Texturen zeigen überhaupt keine Struktur, sondern erinnern an Bildrauschen. Für die Anwendungspraxis sind vor allem die in der Natur häufig anzutreffenden \"fast regelmäßigen\" und \"fast stochastischen\" Texturen von Interesse. Die Texturklassifikation kennt weitere Texturarten und unterteilt beispielsweise die regelmäßigen Texturen noch in geometrisch und farblich regelmäßige Texturen.\n\nProzedurale Textursyntheseverfahren erzeugen Texturen aus dem Nichts. Der Benutzer wählt lediglich eine Reihe von numerischen Parametern – man spricht daher auch von \"parametrischer\" Textursynthese –, die an eine mathematische Funktion oder einen Algorithmus übergeben werden und die Ausgabe des Verfahrens beeinflussen.\n\nDer Ursprung der parametrischen Textursynthese ist \"Perlin-Noise\", eine 1982 von Ken Perlin für den Film Tron entwickelte mathematische Funktion, die Texturbilder durch zufällige Verzerrung abwechslungsreicher gestaltet. Eine systematische Forschung auf diesem Gebiet findet nicht statt, prozedurale Verfahren werden durch Versuch und Irrtum oder durch Zufall entdeckt.\n\nStatt der eigentlichen Textur muss bei der prozeduralen Textursynthese nur das Verfahren gespeichert werden, mit dem die Textur erzeugt wird. Dies spart in der Regel extrem viel Speicherplatz und ist damit besonders für solche Szenarien von Interesse, in denen nur wenig Speicher zur Verfügung steht, z. B. in der Demoszene, wo aus nur 64 KB Speicher möglichst vielseitige Grafiken erzeugt werden sollen. Ein zweiter Pluspunkt ist, dass die prozedurale Textursynthese mitunter sehr schnell große Texturen erzeugt und sich damit zur Texturerzeugung \"on the fly\" für 3D-Computerspiele und ähnliche Anwendungen eignet, wo schnelle Reaktionszeiten garantiert werden müssen.\n\nFür die Textursynthese sind hauptsächlich \"stationäre\" Texturen von Interesse: Wo immer der Blick gerade in einem solchen Bild umherwandert, man hat immer das Gefühl, das Gleiche zu sehen – gerade so als bewegte sich der Blick überhaupt nicht von der Stelle. In der Fachsprache sagt man, die \"Bildinformation\" sei gleichmäßig über das gesamte Bild verteilt.\n\nEine der wichtigsten mathematischen Grundlagen der Textursynthese ist die Erkenntnis, dass Texturen als Produkt einer Markow-Kette begriffen werden können. Die Aussage dieses Modells der Wahrscheinlichkeitstheorie lautet im Zusammenhang der Textursynthese: Jeder Bildpunkt einer Textur hängt nur von den Bildpunkten in seiner unmittelbaren Umgebung ab, jedoch nicht von den Bildpunkten außerhalb dieses fest eingrenzbaren Bereichs. Diese sogenannte \"Markow-Eigenschaft\" übersetzt die Eigenschaft der Stationärität in ein handfestes Berechnungsmodell und ist der mathematische Unterbau nahezu aller Textursyntheseverfahren.\n\nIn der Anfangszeit setzte die Textursynthese auf das folgende geometrische Prinzip: Ein Bild, dessen rechter und linker sowie oberer und unterer Rand sauber zueinander passen, kann ohne sichtbare Nahtstellen beliebig oft aneinander angefügt werden. Weil sich dies besonders deutlich zeigt, wenn man das flache Bild zu einem Torus verformt, spricht man hier auch von \"Torusgeometrie\". Der Geometrie der ebenen kristallographischen Gruppen \"pm\" und \"pmm\" zufolge lässt sich auch ein Bild mit beliebigen Rändern durch geeignetes Spiegeln nahtlos aneinanderreihen. Heute sind diese geometrischen Ideen von untergeordneter Bedeutung.\n\nAlle bislang entwickelten Algorithmen zur Textursynthese verfolgen entweder den Mosaikansatz oder den Pixelansatz. Beim Mosaikansatz wird das Ergebnisbild aus größeren Stücken der Vorlage zusammengesetzt, während es beim Pixelansatz Bildpunkt für Bildpunkt aufgebaut wird.\n\nEin Textursynthesealgorithmus erhält als Eingaben ein digitales Bild, die gewünschte Größe des Ausgabebildes sowie eventuell einige algorithmusspezifische Parameter, mit denen seine Funktionsweise näher bestimmt werden kann. Die Ausgabe des Algorithmus besteht aus dem synthetisierten Bild.\n\nBei der stochastischen Synthese wird das Ausgabebild Bildpunkt für Bildpunkt zusammengesetzt. Der Farbwert des gerade betrachteten Bildpunkts wird festgelegt, indem der Farbwert eines zufällig ausgewählten Pixels im Vorlagenbild übernommen wird. Diese Methode ist sehr schnell, funktioniert aber nur bei hochgradig stochastischen Texturen, d. h. wenn die Einzelteile der Textur höchstens einen Bildpunkt groß sind.\n\nVon \"Kacheln\" (englisch \"tiling\") spricht man, wenn die Vorlage wiederholt dupliziert und aneinandergefügt wird. Diese Methode ist schnell, erzeugt aber höchstens bei strukturierten Texturen befriedigende Ergebnisse; und nicht einmal dort immer. Erfüllt die Vorlage nicht spezielle Symmetriebedingungen, so ist im Endresultat deutlich erkennbar, wo die Duplikate aneinandergrenzen. Da sich der Bildinhalt in regelmäßigen Abständen wiederholt, ist das Ergebnis immer eine strukturierte Textur. Kacheln ist also für andere Texturklassen gänzlich ungeeignet. Spiegelt man die Duplikate wiederholt, so verschwinden zwar die Grenzen zwischen den Kacheln, der Wiederholungseffekt bleibt aber bestehen. Kacheln wird häufig zur Erstellung einfacher Hintergrundbilder für Websites und grafische Benutzeroberflächen angewandt.\n\nDiese Technik wurde 1999 von Alexei A. Efros und Thomas K. Leung vorgestellt und wird gemeinhin „Synthese nach Efros und Leung“, gelegentlich aber auch \"Image Growing\" genannt.\n\nHier wird zunächst ein leeres Bild mit den gewünschten Maßen erzeugt. In dieses Bild hinein wird die sogenannte \"Saat\" kopiert, ein kleines, zufällig ausgewähltes Stückchen der Vorlage. Dann wird das Bild, ausgehend von der Saat, in mehreren Durchgängen Pixel für Pixel befüllt. In jedem Durchgang werden zunächst alle an den bereits ausgefüllten Bildbereich angrenzenden Pixel bestimmt. Dann wird für jeden dieser Pixel eine Reihe von Bildpunkten in der Vorlage ermittelt, deren Umgebungen der Umgebung des neuen Bildpunkts möglichst ähnlich sind. Aus diesen wird ein Bildpunkt zufällig ausgewählt und dessen Farbe dem neuen Bildpunkt zugewiesen.\n\nQualität der Ergebnisse und Geschwindigkeit des Verfahrens sind von der Größe der betrachteten Umgebung, der sogenannten \"Randbreite\", abhängig. Je strukturierter eine Textur ist, desto weiträumiger müssen die Umgebungen der Bildpunkte verglichen werden, bei stochastischen Texturen hingegen ist eine möglichst kleine Umgebungsgröße angezeigt, um keine unerwünschten Strukturen zu erhalten. Je größer die betrachtete Umgebung ist, desto langsamer ist der Algorithmus. Wird die Umgebungsgröße auf null gesetzt, so entspricht das Verfahren der stochastischen Synthese (s. o.).\n\nZahlreiche Nachfolgearbeiten (z. B.) haben das Verfahren seitdem verfeinert, indem sie die vorhandenen Such- und Vergleichsaktionen algorithmisch optimierten.\n\nBeim Quilten wird das Ergebnisbild Flicken für Flicken aus der Vorlage zusammengesetzt. Für jede Flickenposition des Ausgabebildes werden in der Vorlage Bildteile gesucht, deren Umgebung der Umgebung des neuen Flickens möglichst ähnlich ist. Aus den ermittelten Teilen wird dann zufällig einer ausgewählt. Um die Übergänge zwischen den Flicken zu verstecken, wird der neue Flicken vor dem Einfügen zurechtgeschnitten, das heißt sein Rand wird so beschnitten, dass er sich möglichst gut in das bislang erzeugte Bild einfügt.\n\nQuilten ist die qualitativ hochwertigste aber auch zeitaufwändigste Mosaiktechnik. Sie eignet sich für teilstrukturierte und teilstochastische Texturen sowie teilweise für strukturierte und stochastische Texturen. Liegt eine unstrukturierte Mustertextur vor, also eine mit nicht rein geometrisch begründeter Struktur, so wird das Ergebnis nur bedingt zufriedenstellend: Da strukturierte Objekte willkürlich zerschnitten und neu zusammengesetzt werden, können befremdliche Effekte entstehen; so mögen beispielsweise aus Tomaten der Güteklasse A zerstückelte oder verwachsene Tomaten werden. Hochgradig strukturierte Vorlagen liefern nur bei genügend großer Flickengröße befriedigende Ergebnisse, während hochgradig stochastische Vorlagen nach einer möglichst kleinen Flickengröße verlangen. Wird die Flickengröße auf einen Bildpunkt festgelegt, so entspricht Quilten der Technik des \"Image Growing\" (s. u.).\n\nChaosmosaik wurde 2000 von Ying-Qing Xu, Baining Guo und Harry Shum in einem technischen Report der Forschungsabteilung der Firma Microsoft vorgestellt. Die Technik verwendet eine „chaotische“ Form des Kachelns mit einer frei wählbaren untergeordneten Synthesetechnik; die Originalarbeit verwendete die pixelbasierte Textursynthese nach Efros und Leung.\n\nDer erste Schritt von Chaosmosaik besteht darin, durch Kacheln ein Bild mit den gewünschten Maßen zu erstellen. Im zweiten Schritt werden in diesem Kachelbild zufällig ausgewählte Blöcke zufälliger Größe an zufällig ausgewählte Positionen kopiert. Ohne weitere Behandlung enthielte das Ergebnis sichtbare Fugen, wo der Bildinhalt des verschobenen Mosaikstücks nicht mit dem Untergrund zusammenpasst. Bei jedem dieser Kopiervorgänge wird daher um das neu eingefügte Mosaiksteinchen herum ein schmaler Rand ausgeschwärzt. Der geschwärzte Bereich wird in einem Unterschritt mit Hilfe des untergeordneten Syntheseverfahrens ausgefüllt. Diese Form des Chaosmosaik liefert gute Ergebnisse und benötigt wenig Speicherplatz. Die Geschwindigkeit wird hauptsächlich vom untergeordneten Syntheseverfahren bestimmt und ist für die Technik nach Efros und Leung langsam.\n\nXu, Guo und Shum präsentierten daher gleichzeitig eine abgewandelte Form, um den Forderungen nach einem schnellen Syntheseverfahren für 3D-Computergrafiken entgegenzukommen. In dieser Variante wird kein Ausschwärzen und nachfolgendes Wiederauffüllen verwendet. Stattdessen werden die Ränder der Mosaikteile mit einem Glättungsfilter geglättet, wodurch harte Kanten und scharfe Farbübergänge verschwimmen. Die Ergebnisse sind schlechter als im ursprünglichen Verfahren, werden aber mit einem erheblichen Geschwindigkeitszuwachs belohnt, da Filter sehr effizient angewandt werden können.\n\nDieser Algorithmus wurde 2000 von Li-Yi Wei und Marc Levoy vorgeschlagen. Er erweitert das Verfahren von Efros und Leung um einen Multiskalenansatz und verwendet einen effizienteren Suchalgorithmus.\n\nZunächst wird ein Bild mit den gewünschten Ausgabemaßen erzeugt, das mit zufälligen Farbwerten (weißes Rauschen) aufgefüllt wird; im Folgenden wird dieses Bild so verändert, dass es am Ende die synthetisierte Textur enthält. Nun wird aus dem Vorlagebild und dem Rauschbild jeweils eine \"Multiskalenpyramide\" erstellt: Durch wiederholtes Filtern werden aus dem Ursprungsbild nacheinander Bilder erzeugt, die jeweils etwa halb so groß sind wie ihr Vorgängerbild; der Größe nach angeordnet bilden diese eine Pyramide, mit dem Ursprungsbild zuunterst. Die Art des Filters hängt vom Anwendungszweck ab, Wei und Levoy verwendeten einen Gaußfilter, denkbar sind aber auch Laplace- oder Waveletfilter; einen Überblick über Filterung bietet der Artikel Bildverarbeitung.\n\nBei der Synthese werden von oben nach unten die Bilder der Rauschbildpyramide mit dem Verfahren nach Efros und Leung gefüllt. Dabei beinhaltet die Umgebung eines Pixels nicht nur die Pixelumgebung im selben Bild, sondern auch Pixel aller darüberliegenden Pyramidenbilder, die an etwa derselben Stelle im Bild liegen. Dieser Vergleich auf mehreren Auflösungsstufen zur gleichen Zeit ist der Inbegriff der Multiskalenanalyse und sorgt dafür, dass sich das Syntheseverfahren selbständig Strukturen verschiedener Größe in der Vorlage anpasst.\n\nDieser Multiskalenansatz wird durch die Binärbaum-basierte Vektorquantisierung (\"tree-structured vector quantization\", TSVQ) ergänzt. Zentraler Bestandteil dieses Verfahrens, das eigentlich der Datenkompression dient, ist ein Binärbaum, in dessen Knoten Vektoren untergebracht sind. Jeder Vektor enthält neben einem Pixel eines Bildes der Vorlagepyramide alle Pixel der Umgebung dieses Pixels (inklusive denen auf höheren Pyramidenebenen), jeder Knoten enthält den mittleren Durchschnittsvektor aller seiner Kinder. In dieser Datenstruktur kann deutlich schneller nach ähnlichen Kandidaten gesucht werden als im Ursprungsbild.\n\nObwohl bei dieser Technik statt einzelner Bilder ganze Bilderpyramiden synthetisiert werden müssen und vor der ersten Synthese zunächst der Binärbaum aufgebaut werden muss, ist dieser Algorithmus erheblich schneller als sein Vorgänger. Zusätzlich deckt er durch die Multiskalenanalyse einige Texturarten besser ab.\n\n"}
{"id": "1175275", "url": "https://de.wikipedia.org/wiki?curid=1175275", "title": "GvSIG", "text": "GvSIG\n\ngvSIG (kurz für ) ist ein freies Geoinformationssystem. Das Programm stellt für die Verwaltung von Geodaten eine Vielfalt an Werkzeugen bereit. Entwickler und Anwender kommunizieren über User-Listen und können so aktiv an der Weiterentwicklung teilnehmen.\n\nDas in der spanischen Provinz Valencia beheimatete \"Regionale Amt für Infrastruktur und Transport\" (CIT) veröffentlichte im Jahr 2003 eine Ausschreibung für die Entwicklung einer Software zur Verarbeitung von geographischen Daten. Das Privatunternehmen Iver gewann die Ausschreibung und entwickelte den Prototyp von gvSIG, dies in Zusammenarbeit mit der städtischen Regierung und der Universität Jaume I in Castellón de la Plana.\n\ngvSIG ist in der Programmiersprache Java entwickelt und wird unter der GNU General Public License vertrieben. Die Software wird als Desktop Client eingesetzt, kommt aber auch häufig als Client für GDIs zum Einsatz. Folgendes ist die Charakteristik der Software:\n\ngvSIG ist in der Lage, Daten von externen Servern zu verarbeiten, beispielsweise Web Map Service, Web Coverage Service, Web Feature Service, Web Catalogue Service, Gazetter- und ECWP-Service.\nDie freie Software gvSIG-Sextante und eine Schnittstelle zu GRASS GIS erweitern den Funktionsumfang von gvSIG um über 500 Module zur Raster- und Vektordatenanalyse. 2008 wurde der erste Prototyp von gvSIG Mobile veröffentlicht.\n\nEine auf gvSIG basierende Variante ist gvSIG Oxford Archaeology Edition 2010, die über eine leicht veränderte Oberfläche verfügt und bereits mehrere Erweiterungen, darunter SEXTANTE integriert hat.\n\n"}
{"id": "1181309", "url": "https://de.wikipedia.org/wiki?curid=1181309", "title": "Flock (Browser)", "text": "Flock (Browser)\n\nFlock ( für \"Herde\", \"Schar\") war ein freier Webbrowser auf der Codebasis von Chromium, früher auf der Codebasis von Mozilla Firefox. Die Entwickler des Browsers bezeichneten ihn als einen „sozialen Browser“ (\"„social browser“\"), was bedeutet, dass er als Soziale Software in der Lage war, die Nutzung populärer Sozialnetzdienstleistungen zu vereinfachen.\n\nDie weitere Entwicklung des Browsers wurde zum 26. April 2011 eingestellt und das Unternehmen widmet sich zusammen mit Zynga der Spieleentwicklung.\n\nIn der Version 1.0 unterschied sich Flock unter anderem in folgenden Eigenschaften von Firefox 2.0:\n\nDie Version 2.0 basierte auf der Codebasis von Mozilla Firefox 3.0 und bot unter anderem folgende zusätzliche Funktionen:\n\nDie Version 3.0 basierte nicht mehr auf Codebasis des Firefox, sondern auf Chromium. Es gab folgende Neuerungen:\n\nFolgende Sozialnetzdienste wurden von der Version 2.0 unterstützt:\n\nAb Version 3.0 wurden nur noch folgende Dienste unterstützt:\n\nBart Decrem und Geoffrey Arone gründeten das Unternehmen \"Round Two\", die sie 2005 aber in \"Flock\" (nach ihrem Hauptprodukt) umbenannten. Der Firmensitz ist in Redwood, Kalifornien.\n\n"}
{"id": "1181585", "url": "https://de.wikipedia.org/wiki?curid=1181585", "title": "AIM-65", "text": "AIM-65\n\nDer Rockwell-Computer AIM-65 war ein Schulungs- und Entwicklungssystem auf der Basis eines 6502-Mikroprozessors von MOS Technology und wurde 1976 vorgestellt. Der AIM-65 wirkte wie ein Nachfolger des KIM-1-Computers und ein Bruder des Ohio Scientific Superboard II. Als zusätzliche Hardware existierte ein Disketten-Controller und ein rückseitiger Erweiterungsstecker. 1981 stellte Rockwell ein verbessertes Modell, den AIM-65/40, mit 40-Zeichen-Anzeige vor.\nDie Firma Siemens baute den AIM-65 unter dem Namen PC 100 in Lizenz nach.\n\nDie verfügbare Software für den AIM-65 enthielt ein Monitor-Programm mit Assembler/Disassembler, einen BASIC-Interpreter, Assembler, Pascal, PL/65 und ein FORTH-Entwicklungssystem. Die Standard-Software gab dem System seinen Namen und enthielt das Monitor-Programm im ROM, den sog. Advanced Interactive Monitor (AIM).\n\n\n\n"}
{"id": "1184285", "url": "https://de.wikipedia.org/wiki?curid=1184285", "title": "Internetkriminalität", "text": "Internetkriminalität\n\nInternetkriminalität sind Straftaten, die auf dem Internet basieren oder mit den Techniken des Internets geschehen. Dies ist nicht zu verwechseln mit Computerkriminalität, bei der primär der Computer, auch ohne Internetnutzung, als Tatwaffe eingesetzt wird. Den Schutz vor Internetkriminalität nennt man auch Onlinesicherheit. Dieser Begriff ist zu unterscheiden von Internetsicherheit, zu dem auch der Schutz der Infrastruktur selber gehört, also auch der Schutz vor Straftaten gegen das Internet selbst, online oder materiell, aber auch vor sonstigen Gefahren, als Teil der IT-Sicherheit.\n\nDie Erscheinungsformen sind sehr vielfältig; Beispiele sind Internetbetrug, das Ausspähen von Daten, Verstöße gegen das Verbreitungsverbot oder den Jugendmedienschutz, Identitätsdiebstahl, Urheberrechtsverletzung, Cyber-Terrorismus, Cyber-Mobbing, Volksverhetzung sowie das Verbreiten von Kinderpornographie. \n\nDer Übergang zu Methoden und Verfahren des Cyberwar („Netzkrieg“) ist mittlerweile fließend geworden; im Einzelfall ist durchaus nicht ohne weiteres auszumachen, ob ein Angriff im Netz einschlägig kriminellen Zielen dient oder militärisch bzw. politisch intendiert ist (etwa bei einer weitreichenden Sabotage des stark ITK-abhängigen Finanzsystems oder der Webpräsenzen öffentlicher Einrichtungen im weitesten Sinn). Den Beobachtungen zufolge professionalisiert sich die „Malware-Branche“ zunehmend, die Angriffe auf Rechner und Rechnernetze werden immer ausgefeilter. \n\nLaut Antivirensoftware-Entwickler stieg z. B. die Infektion von Rechnern mit Schadsoftware, die zum Identitätsdiebstahl dient (etwa der Ausspähung von Bankkontendaten), vom ersten zum zweiten Halbjahr 2008 um 800 Prozent.\n\nIm Bankenbereich nehmen neben Online-Attacken auf die Konten von Privatkunden vor allem die Angriffe direkt auf das Interbankengeschäft mit Hilfe gefälschter Versionen von Zahlungsaufträgen zu. Die dabei verwendete Schadsoftware dringt dabei in das Netz der angebundenen Institute ein und verursacht jährliche Schäden von zig Millionen Dollar.\n\nZielobjekt des CEO Fraud sind Firmen, die laut einer FBI-Verlautbarung von 2015 um über 740 Mio. Dollar geprellt wurden.\n\nBeinahe schon seit der allgemeinen Etablierung des Internets seit den 1990er Jahren und der zunehmenden Elektronisierung weiter Felder des öffentlichen Lebens, vor allem auch auf wirtschaftlichem Gebiet, liefern sich Kriminelle und Sicherheitsexperten einen Hase- und Igel-Wettlauf auf den unterschiedlichsten Feldern, das bislang meist mit einem „positiven Patt“ für die Sicherheit ausging. In jüngster Zeit (2010er Jahre) werden die Methoden der Cyberverbrecher zusehends raffinierter und elaborierter (was z. B. auch für viele Virenprogrammierer gilt, deren Produkte unterdessen ein erstaunliches, im Einzelfall beängstigendes technisches Niveau erreicht haben).\n\nNach Aussage des US-Telekommunikationsdienstleisters Verizon Business sei es Kriminellen in den Vereinigten Staaten gelungen, die Verschlüsselung beim Übertragen von PIN-Codes zu knacken. Dabei konnten die Hacker sowohl an verschlüsselte als auch an unverschlüsselte PINs gelangen. Fachleute gehen Medienberichten zufolge davon aus, dass das Problem nur gelöst werden kann, wenn die Finanzindustrie den elektronischen Zahlungsverkehr insgesamt überholt.\n\nFür seinen von HP Enterprise Security gesponserten Bericht 2014 zu den Kosten von Internetkriminalität befragte das Ponemon Institute ein repräsentatives branchenübergreifendes Sample von 46 Betrieben in Deutschland. Die Umfrage ergab eine durchschnittliche Schadenhöhe von 6,1 Mio. €, mit einer Spannbreite von 425 Tsd. € bis 20,2 Mio. € pro Jahr. Phishing, Social Engineering und Web-Attacken machten mehr als 35 % der Kosten aus.\n\nDas Bundeskriminalamt (BKA) hat im Jahr 2012 in Deutschland 229.408 Straftaten festgestellt, auf die das Merkmal „Tatmittel Internet“ zutraf.\n\n„Phishing“ bildet laut BKA trotz eines Rückgangs der Fallzahlen um 46 % weiterhin „im Hinblick auf das vorhandene Schadenspotenzial und die Lukrativität für die Täterseite weiterhin einen Schwerpunkt im Bereich Cybercrime.“ Knapp 3.500 Fälle will das BKA 2012 ermittelt haben, bei einer durchschnittlichen Schadenshöhe von ca. 4.000 Euro pro Fall. Für den Rückgang wird Sensibilisierung der Anwender, verstärkte Schutzmaßnahmen und effektives IT-Management verantwortlich gemacht.\n\nAktuell sind Smartphones weiterhin ein interessantes Ziel für Kriminelle. Nutzer setzen diese immer vielfältiger wie z. B. für Onlinebanking, zur Autorisierung von Transaktionen, zum unmittelbaren Zugriff auf E-Mail-Konten und Konten sozialer Netzwerke oder gar zur Nutzung geschäftliche Daten ein und seien sich der mobiler Betriebssysteme unzureichend bewusst.\n\nDie Zahl der Straftaten, die mit dem Tatmittel Internet begangen wurden, ist im Jahr 2016 leicht gestiegen. Im Vergleich zum Vorjahr (244.528 Fälle) wurden 2016 insgesamt 253.290 Fälle erfasst. Dazu gehören Delikte wie Waren- und Warenkreditbetrug, Computerbetrug, Leistungs- und Leistungskreditbetrug, die Verbreitung pornografischer Schriften und Straftaten gegen die Urheberrechtsbestimmungen. Wie die Infografik der Polizei zeigt, entfielen allein 27,8 Prozent der Fälle auf den Warenbetrug; beachtenswert ist auch der Anteil von 20,7 Prozent beim Warenkreditbetrug. Waren- und Warenkreditbetrug machen somit fast die Hälfte aller Fälle aus. Von den 101.654 Tatverdächtigen waren 68,7 Prozent männlich und 31,4 Prozent weiblich. Die Aufklärungsquote lag bei 65 Prozent. Die Polizeiliche Kriminalstatistik wies 2007 allerdings auch einen erheblichen Anstieg bei Urheberrechtsverletzungen aus: um 54,6 Prozent auf 32.374 Fälle. Dafür wurde vor allem das verschärfte Vorgehen der Musikindustrie gegen illegales Herunterladen von urheberrechtlich geschützten Daten verantwortlich gemacht.\n\nEs gibt auf Online-Plattformen eine Zunahme von betrügerischen Fakeshops, bei denen bezahlte Ware nicht oder nicht in der bestellten Qualität geliefert wird. Die Kriminalstatistik der Polizei für das Jahr 2015 weist für Deutschland fast 75.000 Fälle von Warenbetrug im Internet aus. Das deutsche Bundeskriminalamt nennt dies ein „Massenphänomen, das die Strafverfolgungsbehörden vor große Herausforderungen stellt“.\n\nNach einer BITKOM-Studie sind bis Mitte 2008 fast vier Millionen Deutsche schon einmal Opfer von Computer- oder Internetkriminalität geworden. Sieben Prozent aller Computernutzer ab 14 Jahren haben demnach bereits einen finanziellen Schaden etwa durch Viren, bei Online-Auktionen oder durch Datenmissbrauch beim Onlinebanking erlitten. Beklagt wurde gleichwohl das geringe Sicherheitsbewusstsein der Nutzer. In seinem auf der CeBIT vorgestellten Bericht \"Die Lage der IT-Sicherheit in Deutschland 2009\" hat das Bundesamt für Sicherheit in der Informationstechnik (BSI) seine Besorgnis über die wachsende Internetkriminalität ausgedrückt; die Situation wurde als „überaus ernst“ und „schlimmer als befürchtet“ eingeschätzt. \n\nDer unbesorgte Umgang mit Daten in den „Mitmach“-Anwendungen des Webs, vor allem in den immer beliebter werdenden sozialen Netzwerken, schreckt Sicherheitsexperten demnach besonders auf. „Bedenkenlos geben Anwender in ihren Benutzerprofilen detailliert private Informationen preis. Dabei vergessen sie oft, dass Informationen im Netz praktisch jedermann zugänglich sind und es auch bleiben“, heißt es in der BSI-Studie.\n\nBotnets, mittels derer Cyber-Kriminelle oftmals hunderttausende gekaperter Privat- und Bürorechner ohne Wissen der Benutzer vernetzen und missbrauchen, laut BSI „Teil einer professionell und international aufgestellten Schattenwirtschaft“, haben sich unterdessen zu einer herausragenden Bedrohung entwickelt. Seit 2007 kam es zu einer Reihe prominenter Aufdeckungen (\"vgl.\" GhostNet; Trojaner).\n\nBei einer Forsa-Umfrage im Auftrag der Schufa aus dem Jahr 2018 gaben 39 Prozent der Befragten an, selbst schon einmal Opfer von Internetkriminalität geworden zu sein, 12 Prozent wurden dabei konkret Opfer von Identitätsmissbrauch im Internet.\n\nWegen der erheblich gestiegenen Gefahren hat die Europäische Kommission Ende März 2009 deshalb einen Fünf-Punkte-Plan zum Schutz kritischer Informationsinfrastrukturen in den Mitgliedsstaaten der Europäischen Union vorgestellt.\n\nNeben einer forcierten Koordination zwischen den Mitgliedstaaten sieht er vor:\n\nDie Europäische Agentur für Netz- und Informationssicherheit (ENISA) soll laut EU-Kommission die Initiative vorantreiben. Die Kommission werde zudem zusammen mit den Mitgliedstaaten „einen Fahrplan zur Förderung von Grundsätzen und Leitlinien auf globaler Ebene ausarbeiten. Als Mittel zur globalen Konsensbildung wird die strategische Zusammenarbeit mit Drittstaaten gefördert, vor allem in den Dialogen zu Themen der Informationsgesellschaft.“\n\nBereits am 23. November 2001 unterzeichneten die 26 Länder des Europarats neben den USA, Kanada, Japan und Südafrika das „Übereinkommen über Computerkriminalität“, auch „Budapester Konvention gegen Datennetzkriminalität“ oder kurz \"Cybercrime-Konvention\" genannt, um die länderspezifischen Computerstrafrechtsregelungen anzugleichen. Unter anderem sollen Internetanwender oder Domain-Besitzer grenzüberschreitend identifiziert oder Web-Sites, deren Inhalte gegen die Konvention verstoßen, grenzüberschreitend aus dem Netz entfernt werden können. Rechte unverdächtiger Dritter sind nicht gesondert geschützt. Rassistische bzw. fremdenfeindliche Inhalte sind auf US-Wunsch mit Hinweis auf die Meinungsfreiheit nicht berücksichtigt. Das Übereinkommen repräsentiere zudem \"„einen Markstein im Vertragssystem des Europarates zur Bekämpfung von Terrorismus und organisiertem Verbrechen“\" (Hans Christian Krüger, damals stellvertretender Generalsekretär des Europarates).\n\nEin rund um die Uhr tätiges internationales Kontaktnetzwerk zur raschen Amtshilfe wurde eingerichtet. \n\nZur Bekämpfung der Internetkriminalität wurde in Deutschland beim Bundeskriminalamt das Technische Servicezentrum Informations- und Kommunikationstechnologien (TeSIT) aufgebaut, dessen vornehmliche Aufgabe es nach Angaben des Innenministeriums ist, „technische Unterstützung bei Exekutivmaßnahmen und Ermittlungen in Datennetzen zu leisten“. Dem TeSIT ist zudem die Anfang 1999 eingerichtete \"Zentralstelle für anlassunabhängige Recherchen in Datennetzen\" (ZaRD) zugeordnet. Das Bundeskriminalamt wertet eigenen Angaben zufolge das Internet . Es wird auf „eine beachtliche Zahl“ von Fahndungserfolgen verwiesen. Hervorgehoben wird auch, . Bei der Verfolgung sei es jedoch ein großes Problem, dass Täter global agieren könnten, Behörden jedoch nur national und regional begrenzt.\n\nAuch die Polizeien der einzelnen Bundesländer sind an der Bekämpfung der Internetkriminalität beteiligt. Beispielsweise besteht seit 2009 beim Landeskriminalamt Niedersachsen eine „Zentralstelle Internetkriminalität“ mit 20 Mitarbeitern. \n\nDer Bund Deutscher Kriminalbeamter (BDK) und die Stiftung Deutsches Forum für Kriminalprävention (DFK) haben unter Mitarbeit von Vertretern aus Forschung und Wirtschaft im Juni 2009 der Bundesregierung ein fertiges und sofort umsetzbares Konzept für mehr Sicherheit im Internet vorgelegt. Es handelt sich um das Online-Angebot \"Web Patrol\" unter dem Motto \"Der 8. Sinn im Netz\". Grund dafür sind ungefilterter Internetzugänge mit Inhalten wie Pornographie, Pädophilie, Islamismus, Rechts- und Linksextremismus, Terrorismus, die für Kinder und Jugendliche frei verfügbar sind. \"Web Patrol\" beinhaltet ein Informationsportal das zielgruppenorientiert über Fragen der Sicherheit und des Verhaltens informieren soll und ein Programm, das in der Lage ist verdächtige Inhalte direkt zu melden. Internetnutzer sollen so durch einfaches Anklicken eines zusätzlichen Buttons im Browser mittels einer automatisch generierten Meldung Erkenntnisse über suspekte Inhalte, fragwürdige Umtriebe, Datenklau, Übergriffe in Chatrooms und strafrechtlich relevantes Material an eine unabhängige Clearingstelle, die sich aus einem interdisziplinären Team aus Kriminalisten, Psychologen und Soziologen zusammensetzt, übermitteln können. Diese bewertet eingehende Meldungen gibt eine erste Rückmeldung und leitet den Vorgang an zuständige Institutionen weiter. Da das Internet zunehmend als Medium für die Vorbereitung und die Ausführung abweichenden Verhaltens bis hin zur Durchführung krimineller Taten genutzt werde und nicht sensibilisierte Personen ein leichtes Opfer für Internetkriminelle würden oder selbst durch die Möglichkeiten des Internets zu kriminellen Taten angeregt würden, fordert der BDK, das Modell noch vor der nächsten Legislaturperiode umzusetzen.\n\n\n\n"}
{"id": "1185856", "url": "https://de.wikipedia.org/wiki?curid=1185856", "title": "Tatung Einstein", "text": "Tatung Einstein\n\nDer Tatung Einstein TC-01 war ein 8-Bit Home- und Personal Computer, der von der taiwanischen Firma Tatung im englischen Bradford entwickelt wurde, jedoch in Telford, England produziert wurde. Der Rechner war im Wesentlichen für den Betrieb in kleinen Unternehmen ausgelegt. Obwohl der Rechner von der Hardware Ausstattung nah an den MSX-Standard angelehnt war, war das Gerät nicht mit diesem kompatibel.\n\nDer Einstein wurde in England im Oktober 1984 auf den Markt gebracht, 5.000 Exemplare wurden später im Jahr nach Taipeh exportiert. Als Zubehör wurden ein Tatung-Monitor (Monochrome oder Color) und ein Drucker angeboten. Er war vergleichsweise groß und das Gehäuse bot die Möglichkeit, in das Gerät ein oder zwei Floppy-Disk-Laufwerke (Größe: 3 Inch) einzubauen, die von der Firma Hitachi hergestellt wurden. Zur damaligen Zeit war ein Floppylaufwerk unüblich, da die Daten eines Homecomputers zumeist auf einem Kassettenlaufwerk gespeichert wurden. Der Rechner besaß ein CP/M-kompatibles Betriebssystem (Xtal) und wurde mit einem BASIC-Interpreter ausgeliefert. In die Oberseite des Gehäuse war eine Vertiefung eingelassen, in die ein entsprechender Monitor gestellt werden konnte, um so einen optimalen Bildschirmabstand zum Benutzer des Rechners zu gewährleisten. Das Gerät wurde zum Start zu einem Preis von 499 englischen Pfund verkauft.\n\nWeil er teurer war als die Konkurrenzprodukte und weil er keine Anwendungsnische bediente, wurde der \"Tatung Einstein\" wirtschaftlich kein Erfolg. Auch das Nachfolgegerät Tatung Einstein 256 war nicht erfolgreich.\n\n\n"}
{"id": "1188370", "url": "https://de.wikipedia.org/wiki?curid=1188370", "title": "Machfeld", "text": "Machfeld\n\nMACHFELD ist ein österreichisches Künstlerduo. Es wurde 1999 von den Medienkünstlern Michael Mastrototaro und Sabine Maier in Wien gegründet. Ihre Tätigkeitsfelder sind Fotografie, Netzkunst, Kurz- und Experimentalfilm, Streamingprojekte, interaktive Installationen, Literatur, On- und Offline Performances sowie Arbeiten für den öffentlichen Raum. Seit 2004 betreibt MACHFELD ein interdisziplinäres Medienkunstlabor in Wien. Projekte, Ausstellungen und Installationen/Screenings in Afrika, Asien, Europa, Mittelamerika, Nordamerika und in den USA.\n\n\n\n\nGalerie Jünger Wien.\n\n\n\n\n"}
{"id": "1189889", "url": "https://de.wikipedia.org/wiki?curid=1189889", "title": "System/38", "text": "System/38\n\nSystem/38 ist ein Minirechner der Firma IBM. Das System wurde 1978 angekündigt und erstmals August 1979 ausgeliefert. Das System/38 (auch S38, System38) wurde von Frank Soltis im IBM-Labor in Rochester entwickelt. Es ist als Mehrbenutzer- und Multithreadsystem für kaufmännische Anwendungen konzipiert.\n\nEs war als Nachfolger des IBM System/3 vorgesehen und hatte eine fortschrittliche Objekt-basierte Architektur. Nach einer Marktanalyse im Jahr 1983 benutzten 69 Prozent der Kunden zuvor das IBM System/3. Das Ziel, diesen Kundenkreis zum Wechsel zum neuen Rechner zu bewegen, wurde erreicht. Das System/38 wurde 1988 von dem Nachfolgesystem AS/400 abgelöst, das heute unter der Bezeichnung System i firmiert.\n\nDas System/38 war mit einer relationalen Datenbank für die Dialogverarbeitung bestimmt. Das System hatte auch eine gute Performance bei Batchverarbeitungen wie z. B. Tagesabschlüsse, Inventuren u. Ä.\n\nDas Betriebssystem, genannt Systemsteuerprogramm CPF, war genau auf diese Maschine und Hardware zugeschnitten. Die Bedienung konnte über das Steuersprachenprogramm CL erfolgen. Es wurden ursprünglich nur zwei Programmiersprachen mit ausgeliefert. Das waren COBOL und RPG III. Später kam noch PL/1 und BASIC als Programmiersprache hinzu. Die relationale Datenbank war in das Betriebssystem integriert.\nDas Ziel war es, einen Rechner zu bauen, bei dem die Programmierer von der Hardware unabhängig waren. Das soll bedeuten, dass die Hardware und ihre Schnittstelle zu den Anwendungen ausgetauscht werden konnten, ohne dass die Programme neu kompiliert zu werden brauchten. Das war damals keineswegs selbstverständlich. Dafür wurde ein mehrstufiges Schichtenmodell entwickelt.\n\nDas System/38 hatte einen Arbeitsspeicher von 512K, 768K oder 1024K (Model 3XX und 5XX) respektive 1280K oder 1536K (Model 5XX). Der Rechner konnte einen 48-Bit-Adressraum adressieren und konnte über alle Speicher (Platten und RAM) in einem Adressraum verfügen.\nDas System war mehrbenutzerfähig und hatte damit auch eine Benutzerverwaltung.\nEine Objektberechtigungsarchitektur ermöglichte eine Rechtevergabe auf Objektebene. Dadurch konnte man ein Berechtigungswesen aufbauen, das genau festlegte, in welcher Art ein Programm, Datei oder sonstiges Objekt von einem Anwender verwendet werden durfte.\n\nAls Bildschirmgeräte wurden Twinax-Terminals verwendet. Es wurden standardmäßig 12 und maximal 40 IBM 5250 verwendet. Twinax wurde nur von der IBM verwendet. Das Twinaxsystem erlaubt sieben Geräte an einer Leitung in Serie geschaltet. Es gab Twinax-Bildschirme und Twinax-Drucker. Weiterhin konnten Daten auch über 8-Zoll-Disketten gelesen werde. So wurde zum Beispiel das Betriebssystem auf mehreren Disketten ausgeliefert. Die Sicherung des Systems erfolgte ebenfalls auf Disketten. Dazu wurden im Regelfall Diskettenmagazine verwendet. Jedes dieser Magazine konnte 10 Disketten beinhalten.\n\n\n"}
{"id": "1193708", "url": "https://de.wikipedia.org/wiki?curid=1193708", "title": "Apple IIgs", "text": "Apple IIgs\n\nIm September 1986 wurde der Apple II als Nachfolger des Apple IIe eingeführt. Es war das fünfte und letzte Modell der Apple II Baureihe, wenn man den Apple //c+, einen lediglich schnelleren, nur in den USA angebotenen Apple //c, nicht mitzählt. Der Apple II war der letzte Computer, den Apple-Mitgründer und Vater der Apple II-Serie, Steve Wozniak, für Apple konzipiert hatte, bevor er im Februar 1985 seine aktive Ingenieurtätigkeit bei Apple beendete (er blieb weiterhin Angestellter mit einem nominellen Gehalt, nahm aber für Apple nur noch PR-Aufgaben wahr).\n\nDas \"GS\" steht für engl. \"Graphics\" und \"Sound\". Der Apple II war eine Hybridmaschine, die einerseits Soft- und Hardware der Apple II-Reihe nutzen konnte und auf der anderen Seite (allerdings nicht pünktlich zur Markteinführung) eine ähnliche graphische Benutzeroberfläche wie die vom Apple Macintosh mitbrachte. Im Gegensatz zur schwarz-weißen Darstellung der Macintoshs der damaligen Zeit wurde hier erstmals auf einem Apple-Rechner ein farbiges GUI geboten; die damaligen Macs hatten allerdings eine höhere vertikale Bildauflösung und waren deutlich schneller.\n\nDer II besaß im Vergleich zu allen vorigen Rechnern der Apple-II-Serie eine echte 16-Bit-Architektur, eine höhere Taktfrequenz, einen direkten Zugriff auf mehrere Megabyte Speicher, ein neues Design, eine Unterstützung von 3,5-Zoll-Diskettenlaufwerken mit mehr Speicherkapazität und eine Maus als Standardausstattung (Extrakosten für Apple IIe/IIc ca. 300 DM). Tastatur und Maus wurden über den neuartigen Apple Desktop Bus (ADB) angeschlossen, der als einer der technischen Vorfahren des Universal Serial Bus gilt. \n\nApple musste zur damaligen Zeit ein konkurrenzfähiges Produkt in Preis und Leistung gegen den Atari ST und den Commodore Amiga (der IBM-PC hatte zum damaligen Zeitpunkt noch keine nennenswert verbreitete graphische Oberfläche) auf den Markt bringen, ohne dabei den hochpreisigen Apple Macintosh zu gefährden. \n\nFür Nutzer, die keine alte 8-Bit-Software, sondern nur neue 16-Bit-Software anwenden wollten, war das Preis/Leistungs-Verhältnis des Apple II im Vergleich zu seinen direkten Mitbewerbern Atari ST und Commodore Amiga deutlich schlechter – der Apple bot weniger Geschwindigkeit und weniger Grafikbeschleunigungs-Möglichkeiten für Spiele zu einem höheren Preis, so dass dann auch wesentlich weniger Spiele für das System erschienen. Nur auf den Nebenschauplätzen Qualität der Verarbeitung, Qualität der Dokumentation und Kompatibilität zu bestehender Software war der Apple seinen Konkurrenten überlegen. Erst später konnte mit dem verzögert fertiggestellten Betriebssystem GS/OS auf einem weiteren Gebiet Boden wieder gut gemacht werden, da dieses weitaus stabiler lief als die zur gleichen Zeit vorhandenen Konkurrenzprodukte.\n\nZur damaligen Zeit gab es bei Apple intern so viel Streit, dass beide Gründer ihre aktive Mitarbeit beendeten, Steve Jobs verließ das Unternehmen sogar vollständig. Der „Volksmund“ war der Meinung, dass sich Apple damit selbst aus dem Markt geschleudert hat, da bei Apple nicht die richtigen Leute die Firma gelenkt haben. Der Apple II wurde kaum noch weiterentwickelt – es erschien 1989 eine „ROM 3“ genannte, um diverse Fehler bereinigte Version mit mehr fest eingebautem Speicher und etwas verbessertem Betriebssystem, die aber keine Geschwindigkeitssteigerung und keine echten neuen Features bot – die Firma hat sich ganz auf den Macintosh konzentriert. Die Apple II-Linie wurde bis 1992 hergestellt.\n\nDer Hauptprozessor war ein 16-Bit-65816/65C816-Prozessor von Western Design Center (WDC), der eine Softwarekompatibilität zum 8-Bit-6502/65C02 von MOS Technology des Apple II/IIe/IIc herstellen konnte. Ein Softwareschalter im Prozessor entschied, ob der Prozessor im 8-Bit-Emulationsmodus (emulation mode) oder im 16-Bit-Modus (native mode) lief. Im 8-Bit-Modus konnte er mit denselben Befehlen wie ein Apple II/IIe/IIc angesprochen werden und lief damit auch mit dessen Software (mit der Wahl zwischen den beiden Geschwindigkeiten 1 bzw. 2,8 MHz), allerdings auch mit demselben Speicherlimit von 64 KiB Adressraum (emulierter 16-Bit-Adressbus), welches durch Bank Switching nur indirekt umgangen werden konnte. Der Prozessor wird heute noch hergestellt. \n\nDie Nummer \"65\" in der Produktbezeichnung des WDC 65816/65C816 bezeichnet die Kompatibilität zum 6502/65C02 von MOS Technology. Die Nummer \"816\" bedeutet, dass der Prozessor im 8- und 16-Bit-Modus arbeiten kann. Der Prozessor kann im 16-Bit-Modus (mit 24-Bit-Adressbus) maximal 16 MiB RAM adressieren. Hardwarebeschränkungen verhindern allerdings im II einen Speicherausbau über 8,25 MiB hinaus (beim späteren Modell 9,125 MiB).\n\nDer II von 1986 verfügte über 256 KiB RAM, das Modell von 1989 hatte 1,125 MiB. Der RAM war aufgeteilt in jeweils 128 KiB (jeweils 4 Chips) \"schnellen\" und \"langsamen\" RAMs. Der \"schnellere\" Speicher wurde nur für Programme verwendet. Der \"langsamere\" Speicher, der dem Speicher des Apple IIe entsprach, wurde für die Bildschirmausgabe, I/O-Verarbeitung und als Systemspeicher verwendet. Beim 1989er-Modell gab es weiterhin 128 KiB langsamen Speicher, aber 1 MiB schnellen. Für 16-Bit-Programme standen bei 256 KiB RAM maximal 176 KiB RAM zu Verfügung (128 KiB schneller RAM + Systemspeicher, je nach Auflösung und Ausstattung (Erweiterungskarten)). Der RAM-Speicher konnte durch Erweiterungskarten auf maximal 8,25 MiB (1989er-Modell: 9,125 MiB) erweitert werden, wobei Erweiterungen über die von der Firma Apple maximal offiziell unterstützten 4,25 bzw. 5,125 MiB Änderungen im Betriebssystem erfordern. Die Erweiterungen vergrößern nur den \"schnellen\" Speicher, der \"langsame\" bleibt immer bei 128 KiB.\n\nFast alle II-spezifischen Programme, einschließlich aller außer der frühesten Versionen des GS/OS-Betriebssystems, erfordern einen Speicherausbau von insgesamt mindestens 1 MiB. Auf dem 1986er-Modell lassen sich ohne Speichererweiterungskarte fast nur die herkömmlichen 8-Bit-Programme des Apple IIe/IIc nutzen. Ab 4 MiB laufen praktisch alle existierenden Programme problemlos, ein darüber hinausgehender Speicherausbau dient dann nur noch dazu, die Häufigkeit und Dauer von Laufwerkszugriffen zu reduzieren.\n\nDer II verfügte über 128 KiB ROM (1986er-Modell) bzw. 256 KiB ROM (1989er-Modell). Der ROM konnte durch Erweiterungskarten auf maximal 1,125 MiB bzw. 1,25 MiB erweitert werden, wovon aber kaum Gebrauch gemacht wurde.\n\nDer Apple II hatte – im Gegensatz zu seinen Vorgängern, die ohne zusätzliche Soundkarten nur eine noch einfachere Variante des PC-Systemlautsprechers boten – auf der Hauptplatine einen Ensoniq ES-5503-DOC (digital oscillator chip)-Soundchip vom Entwickler des MOS Technology SID, wie ihn z. B. auch Ensoniq selbst für seine Synthesizer Mirage, ESQ-1 und SQ-80 verwendete. Dieser Chip hatte einen eigenen Speicher von 64 KiB, um digitale Musik zu speichern und zu bearbeiten. Der Chip hatte einen Digital-Analog-Umwandler und 32 Oszillatoren, von denen Apple in der Firmware des II zwei für interne Zwecke verwendete. Die restlichen wurden für die Tonerzeugung verwendet, entweder als 15 Stereo-Stimmen oder als 30 Mono-Stimmen. Weiterhin war der Chip in der Lage, Töne über die so genannte Wavetable-Synthese zu erzeugen. Hierzu waren zwei Digital-Analog-Umsetzer (DAC – Digital Analog Converter) vorhanden, die eine Auflösung von 8 Bit hatten und auf einen internen 64-KiB-Wavetable-Speicher zugreifen konnten. Somit war der Ensoniq 5503 in der Lage, ähnlich dem damals größten Konkurrenzprodukt Commodore Amiga, digitalen Stereo-Ton zu erzeugen. Apple allerdings beschränkte die Tonausgabe, indem nur ein Mono-Anschluss für Kopfhörer/Lautsprecher eingebaut wurde. Mit Hilfe einer zusätzlichen (passiven) Erweiterungskarte konnte ein Stereo-Ausgangssignal gewonnen werden.\n\nApple Corps, die Rechtefirma der Beatles, strengte wegen dieser Soundmöglichkeiten eine Klage gegen Apple Computer an, da sie nach deren Ansicht gegen die Abmachung zwischen den beiden Firmen verstießen, dass Apple Computer den Namen \"Apple\" nur tragen durfte, solange sie nicht im Musik-Business tätig wurden. Als Konsequenz verbaute Apple Computer danach viele Jahre in den Macintosh-Modellen keine dedizierten Soundchips mehr.\n\nDer integrierte Grafikchip verfügte über eine 12-bit-Farbpalette (4096 Farben) und konnte zusätzlich zu den Grafikmodi des Apple IIe folgende weitere Modi anzeigen:\n\n\nEine ½AA-Batterie mit 3,6 V, bei frühen Modellen fest verlötet, bei späteren Hauptplatinenversionen austauschbar.\n\n"}
{"id": "1196394", "url": "https://de.wikipedia.org/wiki?curid=1196394", "title": "SquidGuard", "text": "SquidGuard\n\nsquidGuard ist ein Internetfilter, der in Verbindung mit dem Web-Cache-Proxy Squid über den Redirector-Mechanismus eingesetzt werden kann. SquidGuard ist freie Software und unter der GNU General Public License lizenziert.\n\nSquidGuard kann nicht gegen Viren oder andere Malware eingesetzt werden. Surft ein Benutzer eine Website an, so schickt Squid den Seitenaufruf an squidGuard zur Überprüfung. Findet squidGuard die Domain oder die URL, so wird der Benutzer auf eine Seite umgelenkt, die ihm sagt, dass er keinen Zugriff auf diese Seite hat.\n\nSquidGuard wird über eine zentrale, textbasierte Datei konfiguriert. Hier wird hinterlegt, welche Ziele wann für wen gesperrt sind und welche Seite statt der blockierten angezeigt werden soll. Die Informationen über die gesperrten Domänen und URLs sind dabei üblicherweise in verschiedene Klassen unterteilt. Neben dem Einsatz von Domain- oder URL-Listen kann der Zugang auch mit Hilfe von Regulären Ausdrücken verhindert werden. Hier kann die Fehlerquote sehr hoch sein, also mehr gesperrt werden als notwendig. Wird zum Beispiel jede URL gesperrt, die das Wort \"sex\" enthält, werden auch Seiten über das Staatsexamen nicht angezeigt. Es ist möglich, weiße Listen zu hinterlegen, auf deren Einträge stets zugegriffen werden kann.\n\nDer squidGuard-Administrator kann die Zugriffsbedingungen sehr differenziert gestalten. Der Zugang kann je nach Benutzer, Uhrzeit, Ursprungsseite und anderen Kriterien erlaubt oder verweigert werden.\n\nFür squidGuard existieren zahlreiche Plug-ins, die weitere Funktionen ergänzen. Mit Hilfe eines webmin-Plugins kann squidguard über ein Webinterface konfiguriert werden.\n\n\n"}
{"id": "1197354", "url": "https://de.wikipedia.org/wiki?curid=1197354", "title": "Medialinx", "text": "Medialinx\n\nDie Medialinx AG ist ein Medienunternehmen mit Sitz in München und Niederlassungen in fünf Ländern weltweit. Sie wurde 1994 als Articon GmbH gegründet und ist 2012 durch Namensänderung aus der Linux New Media AG hervorgegangen.\n\nDie Medialinx AG ist in sechs Geschäftsbereichen aktiv: \"Linux New Media\", \"Business-IT-Fachmedien\", \"mobile Betriebssysteme\", \"Events & Learning\", \"Verlagsservices\" sowie \"International Media\":\n\nDie Medialinx AG bietet nationale wie internationale Vermarktungsmöglichkeiten, wie z. B. crossmediale Pakete, Online-Kampagnen, Anzeigenschaltungen gedruckt und online, Sponsoring von Events und Veranstaltungen sowie Kooperationen an.\n\n"}
{"id": "1197614", "url": "https://de.wikipedia.org/wiki?curid=1197614", "title": "Dreifachpufferung", "text": "Dreifachpufferung\n\nDie Dreifachpufferung (englisch \"triple buffering\") beschreibt ein Konzept in der Computergrafik, bei dem der Framebuffer des Video-RAM bei Grafikkarten in drei Bereiche unterteilt wird. Ziel des Verfahrens ist es, die bei gleichzeitiger Verwendung von VSync (vertikale Synchronisation) und Doppelpufferung (\"double buffering\") auftretenden Nachteile während des Bildaufbaus zu kompensieren.\n\nDer Unterschied zwischen Doppel- und Dreifachpufferung liegt in der Einteilung des Framebuffers. Während bei der Doppelpufferung der Framebuffer aus zwei Pufferspeichern (Front- und Backbuffer) besteht, sind es derer drei bei der Dreifachpufferung (ein Frontbuffer und zwei Backbuffer).\n\nIm Frontbuffer liegt das Bild, das gerade auf den Bildschirm gebracht wird. Im Backbuffer 1 wird das nächste Bild gerendert (berechnet). Der weitere Ablauf ist davon abhängig, wann der \"Swap\"-Befehl umgesetzt wird. Dieser wird von der GPU initiiert, wenn der nächste Frame zur Ausgabe bereitsteht und bewirkt das Vertauschen der Speicheradressen von Front- und Backbuffer (\"Page Flip\").\n\n\nDurch den Einsatz von VSync wird der \"Swap\"-Befehl, der Front- und Backbuffer tauscht, so lange nicht umgesetzt, bis das aktuelle Bild aus dem Frontbuffer komplett auf dem Bildschirm dargestellt wurde. Ist die Berechnung des neuen Frames im Backbuffer abgeschlossen, muss die GPU also bis zum nächsten VSync warten, um mit dem nächsten Frame weiterzumachen. Die effektive Rechenzeit pro Bild ist somit immer gleich der oder ein ganzes Vielfaches der Zeit, die der Monitor zum Anzeigen eines Bilds benötigt. Wenn die Bildrate der GPU also zum Beispiel knapp unter der des Monitors liegt, ist die effektive Bildrate nur die Hälfte der Monitorbildrate (da die GPU bei jedem zweiten VSync noch nicht mit dem Rendern des Bilds fertig ist). Wenn sie unter der Hälfte liegt wird sie ein Drittel der Monitorbildrate usw.\n\nDurch einen weiteren Backbuffer kann die Bildrate des Monitors von der GPU entkoppelt werden. Wenn ein Bild fertiggestellt ist, muss die GPU nun nicht mehr warten, bis der Swap-Befehl ausgeführt wird, sondern kann direkt im anderen Backbuffer weiterarbeiten. Beim Auftreten des Swap-Befehls werden also die beiden Backbuffer vertauscht, während beim Auftreten von VSync der Frontbuffer und der (letzte) fertig berechnete Backbuffer vertauscht werden. Es kommt also zu keinem Leistungsverlust gegenüber dem ungepufferten Fall.\n\n\n\n"}
{"id": "1198023", "url": "https://de.wikipedia.org/wiki?curid=1198023", "title": "Capablanca-Random-Chess", "text": "Capablanca-Random-Chess\n\nCapablanca-Random-Chess (auch CRC) ist eine Schachvariante, die auf einem 10x8-Brett gespielt wird. Zu Capablancas erweiterten Figurensatz mit Kanzler (auch Marschall oder Zentaur) und Erzbischof (auch Kardinal, Janus oder Erzengel) wird eine Startstellung aus 48.000 Möglichkeiten ausgelost, solche mit ungedeckten Bauern werden verworfen. Rochaden bleiben wie beim Chess960 unverändert Elemente des Spiels. Der Kanzler kann sich wie Turm und Springer bewegen, der Erzbischof kann sich wie Läufer und Springer bewegen.\n\n\nAufgrund der enormen Zahl von Möglichkeiten sind programmgestützte Verfahren sehr anzuraten. Dabei empfiehlt sich folgende Vorgehensweise (welche theoretisch auch für einen Würfel gelten könnte):\nTatsächlich gibt es 12118 verschiedene zulässige Startpositionen.\n\n\nBeim Capablanca-Random-Chess wird zur Darstellung von Positionen die X-FEN verwendet.\n\nEin Beweggrund für das CRC ist, Schachprogrammierern ein abwechslungsreiches und zukunftsträchtiges Testfeld ohne riesige Eröffnungsbibliotheken zu eröffnen.\n\nInzwischen kann Capablanca-Random-Chess z. B. mit SMIRF gespielt werden.\n\n"}
{"id": "1200936", "url": "https://de.wikipedia.org/wiki?curid=1200936", "title": "Mozilla Firefox", "text": "Mozilla Firefox\n\nMozilla Firefox (amerikanisch-englische Aussprache []), kurz \"Firefox\" genannt, ist ein freier Webbrowser des Mozilla-Projektes. Er wurde im September 2002 veröffentlicht. Laut \"StatCounter\" gehörte Firefox im Juli 2017 mit einem Anteil von 11,23 Prozent an der weltweiten Internetnutzung ohne mobile Geräte zu den zwei meistgenutzten Webbrowsern.\n\nDas Mozilla-Projekt Firefox, damals noch unter dem Namen Phoenix, wurde von Dave Hyatt und Blake Ross als experimentelle Abzweigung aus dem Programmpaket Mozilla Application Suite initiiert, das auf dem Quellcode des Netscape Communicators basiert. Die erste lauffähige Version des Webbrowsers Phoenix 0.1 wurde am 23. September 2002 veröffentlicht.\n\nIm Jahr 2003 entschieden sich die Entwickler der Mozilla Application Suite zu einer strategischen Kehrtwende. Das lag unter anderem an der Entscheidung von Apple, den eigenen Webbrowser Safari auf dem KHTML-Renderer und nicht auf dem Gecko-Renderer, der von Mozilla entwickelt wird, aufzubauen.\n\nWährend die Mozilla-Entwickler zuvor für einen geringeren Speicherbedarf möglichst alle wichtigen Internetfunktionen wie Webbrowser, E-Mail-Programm, Adressbuch und HTML-Editor in einer Mozilla Application Suite zusammenfassten, strebten sie nun die Veröffentlichung einzelner, voneinander unabhängiger Komponenten an. Die Entwicklung wurde in einzelnen Komponenten vorangetrieben. Ein geringerer Speicher- und Rechenzeitbedarf ermöglichte einen schnelleren Programmstart.\n\nDie Webbrowser-Funktion übernahm \"Firefox\". Die E-Mail-Funktion wurde unter dem Namen \"Thunderbird\" ausgelagert. Der Kalender wurde unter dem Namen \"Sunbird\" entwickelt, welcher jedoch 2010 zugunsten der Thunderbird-Erweiterung \"Lightning\" eingestellt wurde. Der HTML-Editor wurde bis 2006 als \"Nvu\", bis 2010 als \"KompoZer\" weiterentwickelt. Die offizielle Mozilla Application Suite 1.7 erhielt nur noch Sicherheitsaktualisierungen. Seit Mitte 2005 arbeitet eine unabhängige Programmierergruppe an der Weiterentwicklung der Application Suite unter dem Namen \"SeaMonkey\".\n\nMit XULRunner existiert eine Plattform, die die von Firefox, Thunderbird und anderen Programmen gemeinsam genutzten Funktionen enthält und damit den benötigten Speicherplatz und die Größe der Installationspakete verringern soll. Dieses Ziel wird für die offiziellen Release-Versionen nicht weiterverfolgt, stattdessen installiert sich ab Firefox 3 jede Anwendung eine eigene, private XULRunner-Umgebung.\n\nMit der im November 2017 im Rahmen von Firefox 57 erneuerten Browser-Engine \"Quantum\" und der neuen Benutzeroberfläche \"Photon\" verbesserten sich Geschwindigkeit und Speicherverbrauch deutlich. Add-ons auf Basis von XUL/XPCOM wurden ab diesem Zeitpunkt nicht mehr unterstützt.\n\nUrsprünglich wurde Mozilla Firefox unter dem Namen \"Phoenix\" entwickelt, allerdings musste dieser Name aufgrund einer Klage des US-amerikanischen BIOS-Herstellers Phoenix Technologies geändert werden. Zunächst wurde der Browser in \"Mozilla Firebird\" und schließlich – aufgrund der Namensgleichheit mit der Open-Source-Datenbank Firebird – mit Version 0.8 (9. Februar 2004) in \"Mozilla Firefox\" umbenannt.\n\n\"Firefox\" ist die wörtliche englische Übersetzung der chinesischen Bezeichnung . Damit wird im Allgemeinen der Rotfuchs (\"red fox\"), aber auch der rotbraune kleine Panda \"(red panda)\" bezeichnet. Zunächst wurde der Name gewählt, ohne sich auf eine der beiden Tierarten festzulegen. Der kleine Panda hatte jedoch in den Augen von Designer Jon Hicks keinen besonderen optischen Reiz, daher ließ er sich bei der Gestaltung des Logos von einer japanischen, mit „Firefox“ untertitelten Rotfuchszeichnung inspirieren.\n\nBis Version 1.5 von Firefox wurde \"Fx\" bzw. \"fx\" als die bevorzugte Abkürzung für den Browser in den Release-Notes genannt, in späteren Release-Notes wurde dieser Hinweis weggelassen. Darüber hinaus wird jedoch auch die inoffizielle Abkürzung \"FF\" genutzt.\n\nNach einem Namensstreit mit dem Debian-Projekt über die Nutzungsbedingungen für die Warenzeichen von Mozilla erhielt die in der Debian-Distribution enthaltene und leicht angepasste Firefox-Fassung den Namen \"Iceweasel\". Andere Mozilla-Programme erhielten ebenfalls neue Namen, die allesamt mit \"Ice\" beginnen. Erst Anfang 2016 wurden diese Streitigkeiten beigelegt, und das Debian-Projekt verwendet seitdem wieder die ursprünglichen Namen.\n\nEbenso wie die Mozilla Application Suite verwendet Firefox den Gecko-Renderer, ein programmübergreifendes Modul zur Darstellung von HTML-Seiten, und die XML-basierte Beschreibungssprache XUL zur Gestaltung der grafischen Benutzeroberfläche. Möglicherweise wird Firefox in Zukunft nicht mehr XUL verwenden.\n\nFirefox kann durch Motive (), die die komplette Benutzerschnittstelle verändern können, und durch \"Personas\", die zwar gegenüber den Motiven in ihrem Funktionsumfang beschränkt, aber besonders leicht zu installieren und verwenden sind, optisch an den Geschmack des Anwenders angepasst werden. Mit Erweiterungen (englisch „Add-ons“) können zudem zahlreiche Funktionen, wie z. B Mausgesten, Werbeblocker und Webentwickler-Werkzeuge, hinzugefügt werden.\n\nAb Version 0.9 enthält Firefox ein neues Standardmotiv namens „Winstripe“, das „Qute“ als Standardmotiv ersetzt, um so allen Versionen von Firefox ein einheitliches, allenfalls an die Gegebenheiten der jeweiligen Plattform angepasstes Erscheinungsbild zu verleihen. „Winstripe“ basiert auf dem ab Version 0.8 unter Mac OS X eingesetzten „Pinstripe“-Motiv, das von Kevin Gerich und Stephen Horlander im Hinblick auf Apples Vorgaben zur Benutzerschnittstelle \"Apple Human Interface Guidelines\" entwickelt wurde.\n\nFirefox unterstützt Tabbed Browsing, worunter die Darstellung von mehreren, jeweils mit Tabs versehenen Webseiten innerhalb eines einzelnen Anwendungsfensters verstanden wird. Ab der Version 4.0 ist es möglich, über die Funktion \"Panorama\" Tabs zu gruppieren und diese Gruppen visuell darzustellen. Mit Version 45 wurde dieses Feature aufgrund geringer Nutzung wieder entfernt.\n\nUrsprünglich wurden RSS-Web-Feeds in Form von \"dynamischen Lesezeichen\" unterstützt. Dieses Feature wurde in der Version 64.0 aus verschiedenen Gründen entfernt. Mit Hilfe von Add-ons lassen sich diese anzeigen, aber ein abonnieren wie davor ist nicht mehr möglich.\n\nFirefox wird gegenwärtig in 86 Sprachen angeboten, darunter auch Deutsch. Das Programm ist freie Software und wird unter der GNU General Public License (GPL) veröffentlicht. Für die kompilierten Installationspakete gilt die GPL jedoch aus Sicht der Mozilla Foundation nur eingeschränkt. Ab Version 3 wurde bei der Installation des Browsers der Benutzer aufgefordert, einer Endbenutzer-Lizenzvereinbarung (EULA) zuzustimmen. Nach massiver Kritik an diesem Vorgehen, insbesondere aus dem Linux-Lager, wurde die EULA durch eine „Lernen Sie Ihre Rechte kennen“-Informationsleiste ersetzt, die bei der ersten Installation eingeblendet wird. Diese kann auch über die Adresszeile durch Eingabe von \"about:rights\" aufgerufen werden.\n\nUm Suchvorgänge vereinfacht über die Adressleiste abzuwickeln, können Schlüsselwörter (Shortcuts) für das Suchen definiert werden. Die Schlüsselwortsuche ist eine leichte Abwandlung der Möglichkeit, Lesezeichen Shortcuts/Schlüsselwörter zuzuweisen.\n\nIn Firefox ist ein Easter Egg eingebaut. Wenn in die Adressleiste codice_1 eingegeben wird, erscheint Das Buch Mozilla.\n\nAb Version 3.5 bietet Firefox einen privaten Modus an, in dem kein Browserverlauf oder sonstige Daten, die während des Surfens anfallen, gespeichert werden. Im normalen Modus generierte Browserdaten können nachträglich mithilfe der Funktion „Neueste Chronik löschen“ selektiv entfernt werden – entweder für einen gewissen Zeitraum oder komplett. Noch bis Ende September 2017 traf das für Daten, die mit der IndexedDB-Schnittstelle erstellt wurden, nicht zu. Nachdem öffentlich über dieses acht Jahre bekannte Datenschutzproblem berichtet wurde, änderte Mozilla dieses Verhalten in Firefox 56, sodass künftig auch solche von Webseiten angelegte Datenbanken nachträglich gelöscht werden können.\n\nIm privaten Modus werden Inhalte blockiert, die den Benutzer über verschiedene Webseiten hinweg nachverfolgen lassen. Dazu gleicht Firefox URLs mit einer Liste des Unternehmens \"Disconnect.Me\" ab.\n\nBei einem Absturz des Programms wird ein Bericht erstellt. Er enthält unter anderem die Namen der installierten Add-ons und Informationen zum verwendeten System (Prozessorbezeichnung, verwendetes Betriebssystem usw.) Die Angabe der Website, um die es konkret geht, kann durch den Benutzer hingegen unterdrückt werden. Dieser Bericht ist für die Entwickler bestimmt, um mögliche Fehler auszubessern. Das Versenden muss vom Benutzer explizit bestätigt werden.\n\nZum Schutz vor Phishing und Malware werden besuchte Internetseiten und Signaturen vom Benutzer heruntergeladener Anwendungsprogramme automatisch mit einer lokal gespeicherten Liste verdächtig gemeldeter Seiten und Dateien eines anderen Herstellers, in der Voreinstellung Google, abgeglichen, die ungefähr halbstündig aktualisiert wird.\n\nMittels einer in JavaScript umgesetzten Geolocation API können Webseiten – nach Erlaubnis des Nutzers – über Firefox den Standort des Nutzers bestimmen. Dazu wird ein Dienst von Google verwendet, welcher die anonymen Verbindungsdaten auswerten und speichern kann.\n\nFirefox bietet die Möglichkeit, verschiedene Erweiterungen (englisch \"Add-ons\") und „Themes“ zu installieren, um Anwendungsfunktionen oder weitere Optionen bereitzustellen, die nicht vom eigentlichen Browser angeboten werden, oder um sein Erscheinungsbild zu verändern. Einige dieser Zusatzfunktionen werden absichtlich nicht in den Funktionsumfang des Browsers aufgenommen, damit ein verhältnismäßig schlanker Browser erhalten bleibt und auch um das Firefox-Projekt übersichtlicher zu gestalten. Bei den Erweiterungen handelte es sich bisher um Dateien im ZIP-Format, die in XUL und JavaScript geschriebene Programme enthalten. Die Verwendung von XUL und JavaScript macht diese Funktionen plattformunabhängig. Mit Firefox 4 wurde eine neue \"Add-on-API\" (bisher „Jetpack“) genannte Schnittstelle für Erweiterungen eingeführt, die sich ausschließlich auf die Websprachen HTML, JavaScript und CSS stützt und die Installation sowie das Entfernen von Erweiterungen ohne Browserneustart ermöglichen soll. Mit \"WebExtensions\" gibt es ab Version 48 ein neues Erweiterungsformat, das eine weitgehende Kompatibilität mit anderen Browsern besitzt.\n\nUnter der Bezeichnung \"Firefox Marketplace\" wurde im Oktober 2012 eine Plattform gestartet, auf der Firefox-Nutzer Anwendungen für ihren Browser herunterladen können. Der Marketplace war zunächst nur unter Firefox für Android verfügbar, soll später aber auch auf anderen Betriebssystemen nutzbar sein. Ähnlich dem Chrome Web Store basieren alle dort erhältlichen Programme auf Webstandards, sind also nicht abhängig von der jeweiligen Architektur. Zur Vorstellung waren im Firefox Marketplace unter anderem Twitter und SoundCloud erhältlich.\n\nFirefox steht aktuell für die Betriebssysteme \"Windows\" (ab \"Microsoft Windows 7\"), \"Linux\" und \"macOS\" (ab Version 10.9) zur Verfügung. Seit Firefox 53 werden Windows XP und Windows Vista nur noch in der 52 ESR-Version (Extended Service Release) unterstützt. Januar 2013 lieferte ESR 10 letztmals Nutzern von Windows 2000 und XP (SP2) Nachbesserungen zu kritischen und sicherheitsrelevanten Fehlern. Ebenfalls als ESR werden noch Versionen für Mac OS X ab 10.6 angeboten. Die Linux-Version benötigt zusätzlich verschiedene Pakete und Bibliotheken Dritter, die zusammen mit Firefox selbst bei der Mehrzahl der Distributionen bereits mitgeliefert werden.\nNach Herstellerangaben sind die empfohlenen Systemvoraussetzungen für Windows und Linux ein Intel Pentium 4, der SSE2 unterstützt, 512 MB RAM und 200 MB Festplattenspeicher. Ab Firefox 49 wird eine SSE2-fähige CPU zwingend vorausgesetzt. Die Versionen für Mac benötigen einen Intel x86-Prozessor und ebenfalls 512 MB RAM und 200 MB Festplattenspeicher.\n\nAußer den oben genannten offiziell von Mozilla unterstützten Versionen gibt es Portierungen für Solaris (sowohl für x86- als auch Sun-SPARC-Prozessoren), OS/2 und AIX von IBM, FreeBSD, OpenBSD, PC-BSD, SkyOS, BeOS und ZETA sowie RISC OS. Eine Portierung auf weitere Plattformen ist durch die Quelloffenheit möglich. Mittlerweile ist auch eine Portable Edition für die Nutzung auf externen Speichermedien (zum Beispiel externen Festplatten, USB-Sticks oder Speicherkarten) für Windows erhältlich. Darüber hinaus sind für verschiedene Plattformen optimierte Versionen verfügbar, die eine schnellere Reaktionszeit und geringeren Speicherbedarf zu erreichen versuchen. Für Smartphones hat Mozilla Firefox Mobile entwickelt, welcher unter Android und iOS läuft. Seit Dezember 2017 gibt es auch \"Firefox for Fire TV\", eine Version für die Verwendung mit Amazon Fire TV.\n\nUm Mozilla Firefox besser auf Windows-8-Tablets und anderen Windows-8-Geräten nutzen zu können, hatte Mozilla außerdem eine Modern-UI-Version, vorher als \"Metro\" bekannt, angekündigt. Diese sollte, wie alle Apps für Windows 8, für Touchscreens optimiert sein. Aufgrund des geringen öffentlichen Interesses an Modern UI und somit an einer für diese Oberfläche optimierten Version und einer geringen Anzahl an Beta-Testern wurde die Entwicklung eingestellt und das Projekt abgebrochen.\n\nBei einer Untersuchung der Unterstützung der W3C-Standards (sogenannte Empfehlungen) erreichte Firefox in der Version 35 eine Quote von 90 % der getesteten Eigenschaften. Zum Vergleich: Safari 8 erreichte 90 % der Eigenschaften, 92 % Google Chrome 40 sowie Opera 26 und 85 % Internet Explorer 11.\n\nFrüher wurde jeweils nach der Veröffentlichung einer neuen Hauptversion die vorherige noch eine Zeit lang mit Aktualisierungen versorgt. Im März 2010 wurde mit Version 3.0.19 die Unterstützung und Weiterentwicklung des Zweiges 3.0 und im April 2012 mit Version 3.6.28 die des Zweiges 3.6 eingestellt und ein neuer Versionszyklus eingeführt.\n\nSeitdem erschienen Sicherheits- und Stabilitätsaktualisierungen nur noch in Ausnahmefällen. Stattdessen wurde die nächste Hauptversion mit neuen oder aktualisierten sowie geänderten Funktionen zugleich als Sicherheits- und Stabilitätsaktualisierung behandelt. Dieses Vorgehen rief Kritik hervor, da es „Unternehmen nicht zumutbar (sei), alle sechs Wochen auf eine neue Browser-Version umzustellen, nur um Sicherheitslecks zu stopfen“. Hingegen sahen Befürworter dieses Vorgehens in der schnellen Versorgung mit neuen Funktionen und Unterstützung neuer Webtechniken auch Vorteile, vor allem für Heimanwender.\n\nAls Reaktion auf die Kritik veröffentlichte die Mozilla Corporation für Unternehmen, Bildungseinrichtungen und Behörden Anfang 2012 mit Version Firefox 10 ESR wiederum eine sogenannte \"\"-Ausgabe (deutsch: Version mit verlängerter Unterstützung). Sie enthielt, wie davor Version 3.6, keine neuen Funktionen, sondern allein Fehler- und Sicherheitsupdates. Mit jeder siebten Firefox-Hauptversion – also im Abstand von etwa 54 Wochen – wird das bisherige ESR durch eine neue Version ersetzt. So erschienen bislang die Firefox-ESR-Versionen 17 (Ende 2012), 24 (Herbst 2013), 31 (Juli 2014), 45 (April 2016), 52 (März 2017) und 60 (Juni 2018). Anfang September (2019) folgt die Firefox Version 69, die ohne Flash-Plugin arbeiten soll. \n\nFolgende Tabelle enthält nur die wichtigsten Versionen von Mozilla Firefox, ab Version 4 insbesondere die sogenannten ESR-Versionen \"(Extended Support Release),\" die über einen längeren Zeitraum mit Sicherheitsaktualisierungen versorgt werden. Eine detaillierte Auflistung aller veröffentlichten Hauptversionen, einschließlich der Vorabversionen und der Sicherheits- und Stabilitätsaktualisierungen, ist im Hauptartikel verfügbar.\n\nDie offizielle Ausgabe von Mozilla Firefox erfordert im Normalfall eine Installation auf dem Zielsystem. Das erschwert allerdings den rechnerübergreifenden Einsatz mit Wechseldatenträgern (beispielsweise mit einem USB-Stick), die es unter anderem ermöglichen, Firefox mit dem gleichen Profil, das heißt den gleichen Lesezeichen, Verlauf, Formulareingaben, Passwörtern, Einstellungen und so weiter, an Büro- und Heimcomputer einzusetzen. Aus diesem Grund wird von PortableApps eine inoffizielle, portable Ausgabe von Firefox angeboten – Mozilla Firefox, Portable Edition –, die so weit angepasst wurde, dass sie direkt auf einen Wechseldatenträger entpackt werden kann und auf diesem nutzbar ist. Dabei verbleiben die sensiblen Daten auf dem Wechseldatenträger und müssen nicht auf der lokalen Festplatte des genutzten Rechners zurückbleiben.\n\nBereits im Jahre 2009 existierten von Linux-Distributoren erstellte 64-Bit-Varianten von Firefox für Linux.\n\nNeben der offiziellen 32-Bit-Variante von Firefox bot Mozilla für Windows zu Testzwecken eine 64-Bit-Variante der \"Developer Edition\" zum Download an. Die erste Beta-Version dieser Variante wurde mit Firefox 38 am 3. April 2015 veröffentlicht. Gleichzeitig boten Projekte wie \"Waterfox\", \"Cyberfox\" oder \"Pale Moon\" aus dem Quellcode von Firefox erzeugte inoffizielle 64-Bit-Versionen an.\n\nAb Version 42 veröffentlicht Mozilla stabile 64-Bit-Versionen von Firefox für Windows. Diese unterstützen, anders als die 32-Bit-Varianten, keine NPAPI-Plugins – mit Ausnahme von Flash – mehr. Waterfox unterstützt diese Plugins weiterhin.\n\nMit Versionsnummer 56.0.1 werden Nutzer der 32-Bit-Version von Firefox, sofern ihr System über mindestens 2 GB Arbeitsspeicher verfügt, automatisch auf die 64-Bit-Version umgestellt. Ende September 2017 waren noch rund 70 Prozent aller Firefox-Installationen auf 64-Bit-Windows-Systemen 32-Bit-Versionen von Firefox.\n\nMozilla selbst nimmt keine Codeoptimierungen für spezielle Prozessoren vor. Von Mozilla Firefox existieren, ebenso wie von Mozilla Thunderbird, jedoch zahlreiche inoffizielle Versionen (Builds) für verschiedene Betriebssysteme.\n\nEin Grund ist die Optimierung für bestimmte Plattformen (etwa SSE2-Builds für Pentium-4-Prozessoren). In Kombination mit Techniken wie der profilgesteuerten Optimierung, die neuere C++-Compiler unterstützen, lassen sich Reaktionszeit und Speicherbedarf deutlich verbessern, mithin steigt die Geschwindigkeit des Browsers. Dennoch ist die Verwendung solcher inoffizieller Versionen kein Garant für schnelleres Surfen, vereinzelt kommt es gar zu Geschwindigkeitseinbußen.\n\n2015 passte Mozilla den Firefox-Browser für Windows 10 an, das unter anderem eine integrierte Internetsuche bietet. Die von dieser Funktion verwendete Suchmaschine lässt sich nicht direkt über die Windows-Einstellungen auswählen. Mozilla wurde dafür kritisiert, dass diese Suchmaschine umgestellt wird, sobald Firefox als Standardbrowser in Verwendung ist und dessen eigene Standardsuchmaschine von der von Windows bevorzugten abweicht. Der Benutzer kann dieser Änderung nicht zustimmen oder sie ablehnen, er wird noch nicht einmal auf diese Umstellung hingewiesen.\n\n2014 wurde Mozilla dafür kritisiert, dass eine Unterstützung für Encrypted Media Extensions, einen Kopierschutz für Multimedia, in Firefox für Windows integriert wurde. Mozilla argumentierte, dieser Kopierschutz werde bei vielen großen Streaming Media-Diensten genutzt und es sei nötig, ihn zu unterstützen, damit Firefox im Vergleich mit anderen Webbrowsern, in denen diese Unterstützung bereits durchgängig implementiert sei, nicht ins Hintertreffen gerate. Kritiker verwiesen darauf, dass für die Implementierung des Kopierschutzes Quellcode verwendet werde, der nicht quelloffen sei. Dies verletze die Grundsätze von Open-Source-Software.\n\nDie Mozilla Foundation wurde auch dafür kritisiert, dass sie für hohe finanzielle Zuwendungen von Google ihre früheren Ideale gefährde oder bereits aufgegeben habe. \"(Siehe auch Finanzierung und Werbung)\"\n\nÄltere Versionen von Mozilla Firefox wurden wegen eines als zu hoch empfundenen Bedarfs an Systemressourcen kritisiert. Die Entwickler gaben an, dass dieses Verhalten zum Teil gewollt sei, so würden ab Version 1.5 zur schnelleren Navigation mehr geöffnete Seiten zwischengespeichert, wenn auf dem System ausreichend Ressourcen verfügbar seien. Dieser Umstand verbesserte sich jedoch mit neueren Versionen des Browsers. Die Version 3.0 wurde unter anderem erfolgreich auf die Verringerung des Ressourcenverbrauchs hin optimiert.\n\nFür Kritik bei Nutzern sorgte Ende 2017 die Bündelung mit dem Add-on Cliqz, das testweise zunächst bei einem Prozent der Firefox-Downloads aus Deutschland mitgeliefert und standardmäßig aktiviert wurde. Kritisiert wurden mögliche Einschränkungen beim Datenschutz, da zur Anzeige von Surfvorschlägen beim Tippen in die Adressleiste die Benutzereingaben an die Server der hauptsächlich zur Hubert Burda Media gehörenden Cliqz GmbH gesendet werden.\n\nFür heftige Kritik von Benutzern und prominenten Mitarbeitern sorgte im Dezember 2017 die ungefragte Installation eines Add-ons zur Promotion einer US-amerikanischen Fernsehserie. Dieses Add-on hatte einige Nutzer irritiert, da sie es für eine von einer Schadware heimlich installierten Komponente hielten. Die Kritiker sahen dieses Verhalten als widersprüchlich zu den Zielen der Mozilla-Stiftung an.\n\nDie genaue Verbreitung eines Webbrowsers lässt sich kaum verlässlich messen. Unterschiedliche Statistiken haben für die Verbreitung von Firefox folgende Werte ergeben:\n\nLaut einer Untersuchung des französischen Unternehmens \"AT Internet Institute\" benutzten im März 2008 35 Prozent der deutschen Surfer Mozilla Firefox. In Österreich konnte sich der Browser einen Anteil von 32,6 Prozent und in der Schweiz einen Anteil von 26,4 Prozent sichern. Spitzenreiter waren Finnland (45,9 Prozent), Polen (44 Prozent) und Slowenien (43,7 Prozent).\n\nDas deutsche IT- und TK-Nachrichtenportal Golem.de ermittelte im Oktober 2009 unter seinen Lesern einen Marktanteil von 58,3 Prozent. Nach eigenen Angaben richtet sich das Angebot von Golem.de aber primär an „\"professionelle Computeranwender\"“, die Zahlen sind also nicht repräsentativ.\n\nAuch der IT-Nachrichtendienst Heise online verzeichnet einen überproportional hohen Anteil alternativer Webbrowser unter seinen Lesern. Im Juli 2007 bevorzugten erstmals mehr als die Hälfte der Heise-online-Leser Mozilla Firefox.\n\nIm deutschsprachigen Raum war Firefox ab Mitte 2009 der meistgenutzte Browser. Im April 2013 hielt er in Deutschland einen Marktanteil von 39,3 Prozent und lag damit vor dem Internet Explorer von Microsoft (22 Prozent). In Deutschland war er mehrere Jahre bis September 2016 der meistgenutzte Webbrowser (mobile Geräte ausgenommen). Im Mai 2017 belegte er dort laut StatCounter mit einem Marktanteil von 32 Prozent den zweiten Platz hinter Google Chrome. Im Oktober 2018 belegt er mit 15 Prozent Marktanteil den dritten Platz hinter Safari.\n\nIm Jahr 2010 hatte Firefox weltweit einen Marktanteil von über 30 Prozent. Seitdem geht der Anteil kontinuierlich zurück; im Mai 2015 betrug er 16,3 Prozent; im Oktober 2018 betrug dieser 10,1 Prozent.\n\nUm die Verbreitung des freien Browsers Firefox zu fördern und sich im Browserkrieg behaupten zu können, veranstaltete Mozilla mehrere sogenannte „Download Days“, die das Ziel hatten, möglichst viele Downloads in einer möglichst kurzen Zeit zu erreichen. Mehrmals konnte sich das Projekt dabei selbst übertreffen. Es hält den Weltrekord der meisten Software-Downloads innerhalb von 24 Stunden.\n\nAnlässlich der Herausgabe der ersten Vorschauversion des Firefox 1.0 im September 2004 setzte sich Mozilla das Ziel, eine Million Downloads in zehn Tagen zu erreichen; bereits nach vier Tagen gelang dies. Mit der Veröffentlichung der endgültigen Version 1.0 im November 2004 wurde diese Marke bereits am ersten Tag durchbrochen. Knapp ein halbes Jahr später, am 19. Oktober 2005, wurden insgesamt 100 Millionen Downloads gezählt.\nDie Version 2.0 wurde nach Angaben des Herstellers innerhalb von 24 Stunden nach Freigabe von über zwei Millionen Menschen heruntergeladen.\n\nIm Zuge der Veröffentlichung der Version 3.0 rief Mozilla im Sommer 2008 zum „Download Day“ auf, um damit den Weltrekord der meisten Software-Downloads innerhalb von 24 Stunden aufzustellen. Dieses Ziel wurde mit weltweit 8.002.530 Downloads erreicht und in das Guinness-Buch der Rekorde aufgenommen.\n\nWährend die Zahl der Downloads zur Veröffentlichung der Version 1.0 noch exponentiell stieg, entwickelte sie seitdem einen eher linearen Verlauf. Bis April 2010 wurden über 1,3 Milliarden Downloads verzeichnet.\n\nLaut Mozillas „Firefox Public Data Report“ sank die Zahl aktiver Firefox-Installationen weltweit von 899 Millionen 2017 auf 861 Millionen 2018; im gleichen Zeitraum stieg die Zahl der aktiven Installationen in Deutschland von 71,7 auf 75,7 Millionen.\n\nDie Mozilla Foundation hatte 2010 Jahreseinnahmen von 123 Millionen US-Dollar, davon etwa 100 Millionen aus einem Sponsoringvertrag mit Google. Im Dezember 2011 wurde der Vertrag bis zum Jahr 2014 verlängert. Festgelegt ist eine jährliche Zahlung in Höhe von etwa 300 Millionen Dollar. Im Gegenzug veröffentlicht Mozilla seine Browser mit Google als Standardsuchmaschine. Darüber hinaus wirkten IT-Fachkräfte von Google bei der Entwicklung von Mozillas Webbrowsern mit.\n\nZur offiziellen Freigabe der Version 1.0 schaltete die US-Initiative Spread Firefox am 16. Dezember 2004 eine doppelseitige Anzeige in der New York Times, die über Spenden finanziert wurde. Mehr als 10.000 Personen oder Familien spendeten innerhalb von zehn Tagen jeweils zwischen 10 und 45 Dollar, wofür sie namentlich in der Anzeige erwähnt wurden. Insgesamt kamen so 250.000 Dollar zusammen.\n\nIn Deutschland wurde durch einen Aufruf unter dem Namen „Firefox kommt“ eine Werbeaktion durch Spenden finanziert. Die Anzeige erschien am 2. Dezember 2004 in der Frankfurter Allgemeinen Zeitung und anderen überregionalen Zeitungen.\n\nAb 2014 versuchte Mozilla, seine finanzielle Abhängigkeit von Google zu verringern. Anfang 2014 kündigte Darren Herman an, Firefox werde in Zukunft Werbung in den Tiles anzeigen, sie solle allerdings klar gekennzeichnet werden. Mitte November 2014 zeigte daraufhin die stabile Version des Firefox erstmals auch bei bestehenden Nutzern Werbung in den Tiles an, bei Nutzern der Vorabversionen und neuen Nutzern war das schon früher der Fall. Auch wurde Ende November 2014 die Standardsuchmaschine von Firefox in den Regionen USA (Yahoo), Russland (Yandex) und China (Baidu) geändert.\nWährend Ende November 2015 im Firefox (und SeaMonkey) für Europa weiter Google als Suchmaschine voreingestellt war, sagte CMO Denelle Dixon-Thayer, im Moment bestünde keine Geschäftsbeziehung zu Google.\n\n\n"}
{"id": "1204578", "url": "https://de.wikipedia.org/wiki?curid=1204578", "title": "Cadmus Workstations", "text": "Cadmus Workstations\n\nUnter dem Produktnamen Cadmus vertrieb der deutsche Rechnerhersteller PCS Computer Systeme (siehe engl.) UNIX-Rechner, die in Forschung und Lehre sehr verbreitet waren.\n\nEnde der 1970er, Anfang der 1980er Jahre entwickelte der deutsche Rechnerhersteller PCS Computer Systeme eine Reihe von Mikrocomputersystemen zunächst auf der Basis des LSI11 (siehe engl.) Boards mit einem Echtzeit-Betriebssystem der Digital Equipment Corporation. 1982 entwickelte PCS ein neues System auf Basis eines Motorola 68000-Prozessors. Das System ließ noch die Herkunft aus dem Digital-Umfeld erkennen: Q-Bus (siehe engl.) als Systembus, Terminal-Schnittstelle, Platten- und Magnetband-Interface von Digital usw. Es wurde zunächst als „QU68000“ vertrieben, einbaufähig in 19-Zoll-Schränke.\n\nDie Nachfolgemodelle verwendeten die schnelleren Prozessoren 68020 und 68030 – ebenfalls in CISC-Architektur – und wurden unter dem Produktnamen Cadmus vermarktet. Unter diesem Namen gründete PCS eine amerikanische Tochterfirma, die nach wenigen Jahren wieder vom Markt verschwand.\n\nAls 1985 die ersten RISC-Prozessoren auf den Markt kamen, wurden die Cadmus Workstations auf die MIPS-Architektur umgestellt, die unter anderem bei den Silicon Graphics-Workstations zum Einsatz kamen. Das Betriebssystem war ein eigenes UNIX-Derivat namens MUNIX.\n\nDie Rechenleistung solcher RISC-Workstations im Preisbereich von über 20.000 DM lag Mitte der 80er Jahre bei rund 16 MIPS (Millionen Instruktionen pro Sekunde) bei 20 MHz Taktfrequenz und maximal 64 MByte Arbeitsspeicher.\n\nDie Cadmus-Workstations wurden bis Mitte der 1990er Jahre weltweit vertrieben. Nach mehreren Übernahmen durch große Konzerne wurde PCS 1996 durch ein Management-Buy-out wieder eigenständig. PCS entwickelt heute Terminals für Zutritt, Zeit- und Betriebsdatenerfassung.\n"}
{"id": "1205368", "url": "https://de.wikipedia.org/wiki?curid=1205368", "title": "Millennium-Simulation", "text": "Millennium-Simulation\n\nDie Millennium-Simulation (im englischen Sprachraum auch als Millennium Run bekannt) ist ein Projekt des \"Virgo-Konsortiums\", einer Gruppe von Kosmologen aus Deutschland, Großbritannien, Kanada, Japan und den USA unter Federführung des Max-Planck-Instituts für Astrophysik in Garching bei München. Ziel war die Erstellung einer Computersimulation zur Klärung der kosmologischen Frage, wie sich aus dem direkt nach dem Urknall weitgehend strukturlosen Universum die heute beobachtbaren Großstrukturen, Galaxien und Sterne bilden konnten. Im Sommer 2005 konnten Ergebnisse vorgestellt werden, die die Entstehung von großen Unregelmäßigkeiten aus kleinen eingebrachten Inhomogenitäten zeigen.\n\nIm Zentrum der Simulation steht nicht gewöhnliche Materie, sondern Dunkle Materie, die nach der gängigen Meinung etwa 80 Prozent der Masse des Universums ausmacht. Diese Art der Materie konnte offensichtlich von der starken elektromagnetischen Strahlung des heißen frühen Universums nicht auseinandergetrieben werden und verklumpte so früher als die „normale“ Materie. Daher spielte Dunkle Materie für die Strukturbildung des Universums wohl die wichtigste Rolle.\n\nSelbst mit Supercomputern ist es nicht möglich, die Vorgänge im gesamten bekannten Universum zu modellieren. Daher beschränkte man sich auf einen würfelförmigen Ausschnitt von 650 MPc bzw. 2 Milliarden Lichtjahren Kantenlänge. In diesen Bereich wurde Dunkle Materie von 10 Trillionen Sonnenmassen \"eingebracht\", die auf 2160³ ≈ 10 Milliarden virtuelle Teilchen gleichmäßig verteilt wurde. Zum Start der Simulation wurden der Verteilung der dunklen Materie winzige Dichteschwankungen aufgeprägt. Auch in der Realität müssen solche Unregelmäßigkeiten vorhanden gewesen sein, wie aus der Beobachtung der kosmischen Hintergrundstrahlung bekannt ist. Die Stärke der Unregelmäßigkeiten entspricht etwa der im realen Universum 10 Millionen Jahre nach dem Urknall. Das Programm berechnete nun die Bewegung eines jeden Teilchens aufgrund der Schwerkraft mit einer Schrittlänge von etwa einer Million Jahren. Wie dem realen Universum war auch der Simulation ein expandierender Raum zugrunde gelegt. Die Simulation endete nach etwa 11.000 Zeitschritten, was einer Zeitspanne von 14 Milliarden Jahren, also dem Alter des heutigen Universums, entspricht. Die Simulation lief 28 Tage auf 512 Prozessoren.\n\nDie Simulation beginnt 397.000 Jahre nach dem Urknall, als die heute empfangbare kosmische Hintergrundstrahlung emittiert wurde. Die kosmische Hintergrundstrahlung wird seit Jahren von astrophysikalischen Satelliten vermessen (z. B. vom Satelliten Cobe). Die dabei festgestellten Inhomogenitäten wurden als Ausgangspunkt für die zu beobachtenden Strukturen der Materieverteilung angenommen. Aus dieser Anfangsverteilung der Materie und unter Anwendung der physikalischen Gesetze, deren Gültigkeit für das aktuell beobachtbare Universum vorausgesetzt wird, wurde die Entwicklung der räumlichen Verteilung der Materie am mathematischen Modell studiert. Da sich im Ablauf der Simulation die heute beobachtbaren großräumigen Strukturen (schwammartige Verteilung von Galaxien/Galaxienhaufen mit Filamenten, »Walls« und »Voids«) ergaben, konnte man davon ausgehen, dass die Grundannahmen der Simulation korrekt waren.\n\nIn einem zweiten Simulationsschritt wurde normale Materie in die Simulation entsprechend der Verteilung von dunkler Materie hineinmodelliert, wodurch aufleuchtende Sterne und Galaxienformen visualisiert werden konnten.\n\nAm 2. Juni 2005 wurden erste wissenschaftliche Ergebnisse veröffentlicht. Nachdem der Sloan Digital Sky Survey aktuelle Annahmen der Kosmologie durch das Aufspüren von Schwarzen Löchern in sehr hellen Quasaren in großer Entfernung (und damit zugleich in einem unerwartet frühen Stadium des Universums) in Frage gestellt hatte, konnte die Millennium-Simulation durch die ebenfalls sehr frühe Bildung von derartigen Quasaren in ihrem Modell nachweisen, dass das nicht im Widerspruch zu gängigen Annahmen der Kosmologie steht.\n\n2009 führte dieselbe Gruppe von Astrophysikern die Millennium II-Simulation (MS-II) aus, die einen kleineren Raumwürfel mit einer Kantenlänge von 400 Millionen Lichtjahren untersuchte. Dabei wurden ebenfalls 2160³ »Teilchen« betrachtet, wobei jedes allerdings nur 6,9 Millionen Sonnenmassen repräsentierte.\nDiese Aufgabe war sogar noch aufwendiger zu programmieren als die ursprüngliche Simulation, weil das Verteilen der Rechenleistung zwischen den Prozessoren aufwendiger wird, wenn es um dichte Klumpen von Materie geht. Zu dieser stärkeren räumlichen Zusammenballung der »Teilchen« kommt es durch ihre geringere Masse. Mit MS-II waren auf dem Power-6-Computer in Garching bei München 2048 Prozessoren ungefähr einen Monat lang beschäftigt.\n\nEine weitere Simulation wurde ausgeführt, bei der mit weniger »Teilchen« bei gleichen Ausgangsbedingungen untersucht wurde, ob die Ergebnisse der hochaufgelösten Simulation auch bei niedrigeren Auflösungen bestätigt werden können.\n\n2010 wurde die bislang aufwendigste Simulation Millennium XXL (MXXL) ausgeführt. Diesmal wurde ein Kubus mit 12 Milliarden Lichtjahren Kantenlänge gewählt, in dem 6720³ »Teilchen« mit jeweils 7 Milliarden Sonnenmassen untersucht wurden. MXXL umfasste damit ein 216-fach größeres Volumen als die ursprüngliche Millennium-Simulation.\nDie Simulation wurde auf JUROPA, einem der damaligen Top-15-Supercomputer ausgeführt. Er verfügte über 12.000 Cores, 30 TiBytes Arbeitsspeicher und gab mehr als 100 Terabytes Daten aus.\nKosmologen haben die Daten der MXXL-Simulation genutzt, um die Verteilung von Galaxien und Halos von Dunkler Materie in sehr großen Dimensionen zu studieren und um weiter zu klären, wie die größten Strukturen im Universum entstanden sind.\n\nDas Millennium Run Observatorium (MROb – etwa: mit Millennium-Daten gefüttertes Observatorium) ist ein theoretisches, virtuelles Observatorium, das die Vorhersagen zur Verteilung von dunkler Materie und von Galaxien aus den Millennium-Simulationen nutzt, um mit einem virtuellen Teleskop simulierte »Beobachtungen« zu ermöglichen und diese dann mit den tatsächlichen Beobachtungen abzugleichen. Astrophysiker planen damit reale Beobachtungssitzungen und können somit die Zeit reduzieren, in denen sie knappe Teleskopzeit benötigen. Durch den fortgesetzten Abgleich von simulierten »Beobachtungen« und tatsächlichen Beobachtungen lassen sich die Vorhersagen der Millennium-Simulationen überprüfen und verfeinern.\nEine erste Tranche von virtuellen »Beobachtungen« durch MROb wurde den Astronomen weltweit zur Analyse auf der MROb-Website zur Verfügung gestellt. Das virtuelle MROb-Universum kann mit einem Online-Tool, dem MROb-Browser durchsucht werden. Dem Benutzer ist es damit möglich, mit der MROb-Datenbank, in der die Daten von Millionen von Dunkle-Materie-Halos und ihren Galaxien gespeichert sind, zu interagieren. Gegenwärtig sind Aktualisierungen des MROb-Netzwerks geplant.\n\nDie Simulation zeigt eindrucksvoll, wie sich eingebrachte Dichteschwankungen allmählich verstärken, sodass schließlich eine klumpige Struktur entsteht, wie sie das heutige Universum aufweist. Am Ende der Simulation hatten sich Massenansammlungen in der Größe von Galaxien und Galaxienhaufen gebildet. Es ist eine schwammartige Struktur mit fraktalen Eigenschaften entstanden.\n\nDie Simulationsergebnisse von Millennium stimmen sehr gut mit den Beobachtungen des Universums überein, sodass damit erstmals ein gültiges Modell des ganzen Universums erzeugt worden ist. Allerdings haben neuere Simulationen wie Bolshoi auf der Grundlage neuer Daten teilweise abweichende Resultate geliefert.\n\n\n\n"}
{"id": "1207958", "url": "https://de.wikipedia.org/wiki?curid=1207958", "title": "Himmel und Huhn", "text": "Himmel und Huhn\n\nHimmel und Huhn ist ein US-amerikanischer, computeranimierter Kinofilm des Regisseurs Mark Dindal aus dem Jahr 2005 und der 46. abendfüllende Trickfilm von Disney. Der Film ist nach \"Dinosaurier\" ein weiterer selbst produzierter computeranimierter Film der Walt Disney Pictures. Die Grundidee basiert auf dem Disney-Zeichentrick-Kurzfilm \"Chicken Little\" aus dem Jahr 1943. Der Film startete am 26. Januar 2006 in den deutschen Kinos.\n\nIn Oakey Oaks läutet Hühnchen Junior eines Tages aus heiterem Himmel die Schulglocke. Mit der Warnung, jeder solle um sein Leben rennen, versetzt er die ganze Stadt in Panik. Nachdem sich alle wieder etwas beruhigt haben, erklärt Hühnchen Junior, dass ihm ein sechseckiges Stück des Himmels auf den Kopf gefallen sei, als er unter der großen Eiche saß. Beschämt muss sein Vater Bruno Hühnchen, der legendäre Ex-Superstar des lokalen Baseballclubs, seinem Sohn klarmachen, dass es sich dabei nur um eine Eichel hat handeln müssen und keinerlei Gefahr droht.\n\nEin Jahr später wird Hühnchen Junior immer noch für die Himmel-stürzt-ein-Geschichte verspottet. Allein seine Freunde, die ebenfalls allesamt Außenseiter sind, halten zu ihm: Die geschwätzige Ente Susi Schnatter, das gefräßige Hausschwein Ed von Speck und der neugierige Austauschfisch Luigi Forello.\n\nUm die Eichel-Schmach von damals endgültig vergessen zu lassen und seinen Vater stolz zu machen, bemüht sich Hühnchen Junior ein Star seines Baseballteams zu werden. Und tatsächlich gelingt ihm in einem wichtigen Spiel nach zwei Fehlversuchen ein Home Run. In der folgenden Nacht fällt erneut ein merkwürdiger Gegenstand vom Himmel auf seinen Kopf. Um nicht sofort wieder von allen als verrückt dargestellt zu werden, vertraut er sich diesmal nur seinen Freunden an. Als Luigi einen versteckten Knopf an dem Objekt entdeckt und ihn betätigt, fliegen beide plötzlich zurück in den Himmel. Die erstaunten Freunde folgen dem unbekannten Gegenstand um Luigi zu befreien. Sie müssen dabei feststellen, dass der ominöse Gegenstand Teil eines unsichtbaren Alien-Raumschiffs ist. Auf der Suche nach Luigi gelangen die drei Freunde auch in das Innere des Raumschiffs. Dort entdecken sie unter anderem auch eine seltsam aussehende Kreatur, die ein orangefarbenes Plüschfell und drei Augen besitzt. Sie werden von roboterähnlichen Maschinen entdeckt und flüchten aus dem Raumschiff. Auf ihrer Flucht läuten sie die Schulturm-Glocke und verursachen damit eine erneute Panik in Oakey Oaks. Wieder einmal kommen die Erwachsenen zu spät und niemand sieht das Raumschiff. Hühnchen Junior ist frustriert, dass ihm nicht einmal sein Vater glauben will. Allerdings gibt es noch ein weiteres Problem: Niemand hat mitbekommen, dass auch das orangefarbene Alien auf der Erde zurückgeblieben ist. Wie sich herausstellen soll, ist dieses Alien noch ein Kind und die Aliens im Raumschiff glauben, es wurde entführt. Also machen sie sich mit einer ganzen Flotte zurück auf den Weg nach Oakey Oaks, um das Kind zurückzuholen. So kommt es zu einer Invasion von zahlreichen Raumschiffen und die Bewohner müssen erkennen, dass Hühnchen Junior die Wahrheit erzählt hat.\n\nHühnchen Junior und seine Freunde beschließen, den Aliens bei der Suche nach ihrem Kind behilflich zu sein. Auch Juniors Vater unterstützt seinen Sohn bei dieser Aufgabe. Letztendlich wendet sich alles zum Guten und die Bewohner von Oakey Oaks erfahren, dass die Roboter nur ein Transportsystem der Aliens sind. Auch stellt sich heraus, dass sie schon länger immer mal wieder in Oakey Oaks vorbeischauen, weil sie sich mit Eicheln eindecken wollen.\n\nDie Geschichte um Hühnchen Junior wird sogar von Hollywood verfilmt und zum Ende des Films kann man die Freunde dabei sehen, wie sie diesen Kinofilm im Kino anschauen. Hühnchen Junior hat sich außerdem in Susi Schnatter verliebt.\n\nBei der deutschen Synchronbearbeitung führte Kai Taschner Dialogregie. Es wurde sogar eigens eine österreichische Fassung hergestellt, bei der teilweise andere Sprecher zum Einsatz kamen.\n\n\nIm Film sind unter anderem zu hören:\n\n\n„\"Mit \"Madagascar\" und \"Die Unglaublichen\" kann das Anti-Alien-Kampfküken nicht mithalten. Aber es gibt immer noch genug zu gackern.\"“ Cinema\n\n„\"Herrlich verrückt! In quietschbunter Kulisse entzündet ‚Himmel und Huhn‘ ein Gag-Feuerwerk mit irre komischem Slapstick und viel Gefühl – vor allem durch die herzerwärmende Vater-Sohn-Beziehung, die stark an ‚Findet Nemo‘ erinnert. Klar, die Geschichte des liebenswerten Versagers, der zum Helden mutiert, ist wenig originell; auch die Alien-Handlung kommt unvermittelt. Doch Kids werden bei cooler Musik (u. a. R.E.M.), dem zuckersüßen Huhn und den Stimmen von Kim Frank und Verona Pooth trotzdem auf ihre Kosten kommen.\"“ TV Movie\n\n\n\n\n"}
{"id": "1208623", "url": "https://de.wikipedia.org/wiki?curid=1208623", "title": "Linux-Verband", "text": "Linux-Verband\n\nDer Linux-Verband e.V. (LIVE) vereinte die im Bereich freie Software und insbesondere im Bereich Linux tätigen Unternehmen und Personen bzw. deren Unterstützer. Gegründet wurde er auf der Multimediamesse MediaLive im April 1997 in Arnsberg, Vereinssitz war Frickenhausen am Main, die Geschäftsstelle war in Berlin. 1999 übernahm LIVE das Internetportal linux.de.\n\nNachdem bereits ab 2010 drei zusätzliche Vorstandsmitglieder des Verbandes in den Vorstand von Lisog e.V. aufgenommen worden waren, beschlossen am 21. und 22. Juli 2011 die beiden Vereine LIVE Linux Verband e.V. und Lisog e.V. zu fusionieren. Der Name der neuen Organisation ist \"Open Source Business Alliance\" (OSBA), mit Sitz in Stuttgart. Die Verschmelzung wurde im September 2011 vollzogen.\n\nMitglieder sind IT-Unternehmen wie Linux-Distributoren, Systemhäuser, Softwarehersteller, aber auch Anwenderunternehmen und einzelne Personen. Die Mitgliedschaft setzte sich (Stand Juni 2010) zusammen aus:\n\nDer Linux-Verband verstand sich als Interessenvertretung von Open-Source-Unternehmen in Deutschland und Europa. Das Ziel ist die weitere Verbreitung von freier Software und offenen Standards. Den Mitgliedern bietet er ein Netzwerk zum Informationsaustausch, zur Zusammenarbeit und zum gemeinsamen Ausbau des Geschäfts mit freier Software. Der LIVE tritt gegenüber Medien, Wirtschaft, Verwaltung und Politik als Berater auf. Er bezieht Stellung gegen computerimplementierte Erfindungen (Softwarepatente) und wirbt für offene Standards in der IT. Der Verband betrieb in Kooperation mit dem Heise Verlag, Hannover, eine Anbieterdatenbank. \n\nVorsitzender war Elmar Geese, Tarent GmbH, Bonn, Stellvertretender Vorsitzender Rico Barth, c.a.p.e. IT GmbH, Chemnitz (Stand Juni 2010).\n"}
{"id": "1209739", "url": "https://de.wikipedia.org/wiki?curid=1209739", "title": "Apple IIe", "text": "Apple IIe\n\nDer Apple IIe war eine Weiterentwicklung des Apple II, der nach dem Scheitern des Apple III innerhalb sehr kurzer Zeit als „Verluste-Stopper“ von Apple entwickelt und Anfang 1983 veröffentlicht wurde. Er hatte serienmäßig 64 KB RAM eingebaut und verhielt sich im Wesentlichen wie ein Apple II+ mit bereits eingebauter Apple II Language Card. Eine Aufrüstung (mittels einer einzigen Steckkarte) auf 128 KB RAM, Videx-kompatible 80-Zeichen-Anzeige und verdoppelte Grafikauflösung war im Grundgerät bereits vorgesehen und wurde von fast allen Benutzern auch bald durchgeführt.\n\nErstmals in der Apple-II-Serie war ohne Aufrüstung die Ein- und Ausgabe von Kleinbuchstaben möglich; die Tastatur konnte sämtliche 7-Bit-ASCII-Zeichen inklusive aller Steuerzeichen erzeugen, in der deutschen Variante erstmals auch deutsche Umlaute, die nach ISO 646 kodiert waren. Zwischen amerikanischer und nationaler Tastaturbelegung und Zeichensatz konnte bei den nicht-amerikanischen Modellen mittels eines Schalters vorne unter der Tastatur gewählt werden. Aus Softwaresicht war die Umschaltung anders als beim IBM PC vollkommen transparent, es wurden keine Treiber oder ähnliches dafür benötigt.\n\nDie Tastatur des Apple IIe konnte auch im Gegensatz zum Apple II/II+ mit einem Nummernblock erweitert werden; sie verfügte zudem als erste der Apple-II-Baureihe über die vom Apple III stammenden Open-Apple- und die Solid-Apple-Taste (später auf dem Macintosh zu „Command“ und „Option“ umbenannt). Da diesen Tasten kein ASCII-Code zugeordnet werden konnte, wurden sie so verdrahtet, dass sie aus Softwaresicht wie die beiden Feuerknöpfe eines Apple-Joysticks erschienen.\n\nDer IIe bestand aus weniger Bestandteilen und war somit trotz seiner verbesserten Eigenschaften nicht teurer als der Apple II+, brachte Apple aber zugleich deutlich mehr Gewinn ein. Die Produktion des II+ wurde folglich sofort nach dem Erscheinen des IIe eingestellt. Da im IIe mehrere größere ASICs verwendet wurden statt der Vielzahl kleiner Standard-ICs der älteren Modelle, reduzierte sich die Zahl der Apple-Klone, da deren Hersteller diese ASICs nicht erwerben konnten und Eigenentwicklungen entsprechend teuer waren; allerdings bedeutet dies auch, dass ein defekter Apple IIe anders als der Apple II/II+ heute nicht mehr ohne weiteres repariert werden kann, da diese ASICs nicht mehr erhältlich sind.\n\nAls Disketten-Betriebssystem diente zunächst noch das übernommene Apple DOS der älteren Modelle, ab Ende 1983 dann Apple ProDOS.\n\nDer Apple IIe war ein Riesenerfolg, im Schnitt wurden im Jahr 1983 ca. 60.000–70.000 Computer im Monat verkauft.\n\n1985 erhielt er erneut ein Hardware-Upgrade, das als „Enhanced IIe“ bekannt wurde: Der MOS 6502-Prozessor wurde durch den WDC 65C02 ersetzt, er erhielt einen neuen Zeichengenerator, der auch Grafikzeichen generieren konnte, und zwei Firmware-ROM-Chips wurden durch neue ersetzt. Dieses Update beseitigte mehrere bekannte Fehler der Firmware und machte ihn kompatibler zum Apple IIc und in geringerem Maße auch zum alten Apple-II-Urmodell von 1977, dessen Mini-Assembler und Step- und Trace-Befehle nun wieder vorhanden waren. Apple verkaufte auch Update-Kits mit den vier benötigten ICs und einem neuen Typenaufkleber, wodurch Benutzer der älteren Variante ihr Gerät selbst oder in einer Vertragswerkstatt zum „Enhanced IIe“ upgraden konnten. Der „Enhanced IIe“ ist an der Einschaltmeldung „Apple //e“ zu erkennen, die ältere Variante zeigt „APPLE ][“ bzw. bei aktiviertem deutschen Zeichensatz das eher komische „APPLE ÜÄ“. Ein Enhanced IIe und 128 KB RAM ist die Mindestausstattung, um praktisch alle 8-Bit Apple-II-Programme ablaufen lassen zu können. Dies gilt nicht für die Apple-IIgs-spezifischen.\n\n1986 erschien nochmals eine neue Version, der „Platinum IIe“, der softwareseitig keine Neuerungen brachte, aber ein moderneres Gehäusedesign bot, die Chip-Zahl weiter reduzierte (wodurch der Preis sank) und einen bereits eingebauten Nummernblock hatte. In Deutschland wurde dieser Rechner allerdings nicht mehr angeboten, da Apple dort auf den Macintosh setzte. Diese Version wurde noch bis Ende 1993 gefertigt, wodurch der Apple IIe das am längsten hergestellte Computermodell in der ganzen Apple-Firmengeschichte wurde. Einige wenige Geräte sind auch heute noch bei Privatpersonen und in Schulen als Anschauungsmodell in Betrieb.\n\n\n"}
{"id": "1209799", "url": "https://de.wikipedia.org/wiki?curid=1209799", "title": "Apple IIc", "text": "Apple IIc\n\nDer Apple IIc war das vierte Modell in der Apple-II-Linie und war Apples erster portabler Computer. Das \"„c“\" im Namen stand für \"compact\" (kompakt). mit einem MOS-65C02-Prozessor mit 1,02 MHz Taktfrequenz ausgerüstet wog er 3,4 Kilogramm. \n\nDer Apple IIc+ wurde 1988 vorgestellt und nur in den USA angeboten. Er verfügte im Gegensatz zum IIc über ein 3.5\"-Laufwerk und einen 65C02-Prozessor mit 4 MHz. Aufgrund des günstigen Preises von 675 US-Dollar verkaufte sich das Modell sehr gut.\n\nDer Apple IIc wurde im April 1984 auf den Markt gebracht. Seine Vorstellung in einer Veranstaltung unter dem Titel „Apple II Forever“ wurde als Apples Bekenntnis zu der Apple-II-Serie verstanden, die man nicht zugunsten des soeben eingeführten Macintosh fallen lassen werde.\n\nDa der Apple IIc im Wesentlichen einem Apple IIe mit einer Ansammlung von üblichen Zusatzsteckkarten entsprach, wurde er nicht als Nachfolger, sondern als ergänzende tragbare Version angesehen; der IIe wurde auch weiterhin gebaut und überlebte am Ende den IIc um einige Jahre.\n\nMit dem Computer wurde Apples \"Snow White design language\" mit einem für damalige Verhältnisse eleganten und geschmeidigen Aussehen eingeführt, das bald für nahezu ein Jahrzehnt zum Standard für die meisten Apple-Computer und deren Zubehör wurde.\n\nObwohl der Apple IIc relativ leicht und kompakt war, wurde er doch nicht als echter „portable“ betrachtet, da er weder eine Batterie noch ein Display eingebaut hatte.\n\nTechnisch gesehen war der Apple IIc aus Anwendersicht ein Apple IIe in einem kleineren Gehäuse, mit Ausnahme von ein paar wenigen, kleineren Verbesserungen, die die Kompatibilität zum ursprünglichen IIe nicht wesentlich beeinflussten.\n\"Notiz am Rande: Ein Jahr später konnten alle diese Features auch auf dem IIe eingesetzt werden; möglich machte dies ein Upgrade, Enhanced IIe genannt.\"\n\nIntern arbeitete der Rechner jedoch in mancher Hinsicht deutlich anders als der Apple IIe, beispielsweise enthielt er ein doppelt so großes und wesentlich komplexeres ROM, und es kamen erstmals in der Apple-II-Baureihe von Haus aus Interrupts zum Einsatz.\n\nIn die Hauptplatine des IIc wurden fünf übliche Erweiterungskarten integriert. Diese umschlossen:\nDamit hatte der Apple IIc 128 KiB RAM, 80-Spalten-Darstellung, und die sogenannte \"doppelt hochaufgelöste\" Grafik mit 560 × 192 Pixeln standardmäßig an Bord, im Unterschied zu seinem Vorgänger IIe, allerdings fiel die Möglichkeit weg, allerlei weitere Erweiterungskarten einzubauen, da keine echten Slots mehr vorhanden waren. Die integrierten Karten wurden auf virtuelle Slots gelegt, damit Software von früheren Apple-II-Modellen ohne zusätzliche Modifikationen die Karten erkennen konnten. Interessant ist die Kontrollerkarte für Diskettenlaufwerke; diese wurde auf einen einzelnen Chip verkleinert, der „IWM“ genannt wurde. Das Kürzel stand für Integrated Wozniak Machine.\n\nAn der Rückseite des Rechner waren die Schnittstellen angebracht. Die DE9-Schnittstelle für den Joystick wurde auch als Anschluss für die Maus verwendet und war zu den Mäusen von den Lisa- und frühen Macintosh-Modellen kompatibel. Zwei serielle Schnittstellen waren ebenfalls vorhanden (zwei, damit ein Drucker und ein Modem simultan angeschlossen werden konnten), ebenfalls eine Schnittstelle für ein zweites 5.25\"-Diskettenlaufwerk (später konnten auch 3.5\" und Harddisks darüber angeschlossen werden). Außerdem war eine komplexere Video-Schnittstelle für zusätzliche Adapter vorhanden, dieser Anschluss jedoch konnte nur rudimentäre Signale generieren; selbst war er nicht in der Lage, ein Videosignal zu erzeugen. Apple stellte ein LC-Display und einen RF-Modulator für diesen Ausgang her; der Modulator wurde später mit dem IIc ausgeliefert. Der Composite-Video-Ausgang der früheren Apple-II-Modelle blieb, der interne \"DIP-16 Game port\" und Anschluss für Kassettenspieler wurden aber entfernt. Der Netzteilanschluss wurde mit internen Spannungswandlern verbunden und benötigte daher nur eine einzige externe Spannung; damit konnte der IIc mittels eines simplen Adapterkabels an einer Autobatterie betrieben werden.\n\nCPU\n\nArbeitsspeicher\n\nMassenspeicher\n\nBASIC und Betriebssystem\n\nGrafik\n\"*\" Die horizontale Auflösung war bei farbiger Darstellung halbiert.\n\nDie Textmodi konnten mit den grafischen Modi gemischt werden, dabei wurden die untersten 4 Textzeilen angezeigt und darüber Grafik.\n\nAudio\n\nInterne Anschlüsse\n\nInterne spezielle Controller\n\n\nExterne Anschlüsse\n\n\nDie Revisionsnummer des Apple IIc kann über den BASIC-Befehl codice_1 erfragt werden.\n\nDies war das erste Modell. Diese Firmware ist als einzige nur 16 KiB groß, alle späteren Versionen umfassen 32 KiB.\n\nDie erste Baureihe konnte die Baudraten an den seriellen Ports nicht exakt einhalten, was besonders bei schnellen Geräten oft Übertragungsfehler verursachte. Die erste Revision behob diesen Fehler. Das ROM blieb unverändert, weshalb es keine neue Revisionsnummer gab.\n\nDiese Revision unterstützte die Verwendung eines externen 3,5-Zoll-Diskettenlaufwerks mit 800 KiB Speicherkapazität neben dem 5,25-Zoll-Modell. Das Laufwerk „UniDisk 3.5“ hatte dabei einen eigenen Prozessor, da der 1-MHz-Prozessor des IIc zu langsam war, um die im Vergleich zu 5,25-Zoll-Laufwerken wesentlich schneller einlaufenden Daten der 3,5-Zoll-Laufwerke zu dekodieren. Die Vorversion konnte mit einem neuen Firmware-Chip auf diesen Stand gebracht werden.\n\nDiese Revision konnte intern auf 1,125 MiB Speicher erweitert werden. Die Speichererweiterung war aus Softwaresicht kompatibel zu der „Apple II Memory Expansion Card“ für den Apple IIe. Da die Hauptplatine deutlich verändert war, war ein Upgrade älterer Revisionen auf diesen Stand über einen Firmware-Austausch nicht möglich, sondern nur über einen Wechsel der Hauptplatine.\n\nDiese Version behob lediglich einige relativ obskure Fehler in der Firmware. Upgrade der Revision 3 auf diesen Stand war über Firmware-Austausch möglich.\n\nDer Prozessor dieses serienmäßig schnellsten aller Apple-II-Modelle lief mit vierfachem Tempo (abschaltbar), das interne Laufwerk war ein 3,5-Zoll-Modell. Die Farbe des Rechners wurde auf grau geändert, das Netzteil war nun fest eingebaut. Es konnten nun auch simplere und preisgünstigere 3,5-Zoll-Laufwerke ohne eigenen Prozessor angeschlossen werden, wie sie auf dem Apple IIgs üblich waren. Dieses Modell wurde offiziell nur in den USA verkauft und existiert nur mit amerikanischer Tastatur, 110-Volt-Netzteil und NTSC-Videonorm. Upgrade älterer Modelle auf diesen Stand war nicht möglich.\n\n"}
{"id": "1209826", "url": "https://de.wikipedia.org/wiki?curid=1209826", "title": "Apple II+", "text": "Apple II+\n\nDer Apple II+ war ein leicht erweiterter Apple II, der 1979 erschien. Er verfügte immer über volle 48 KB RAM und bot als erster Apple-Computer – statt des älteren Integer BASIC von Steve Wozniak – das Applesoft BASIC, ein gleitkomma-fähiges BASIC aus dem Hause Microsoft, fest eingebaut an. Auch das „Autostart-ROM“, das ein angeschlossenes Diskettenlaufwerk ohne Benutzereingriff booten, sowie selbständig zwischen Warm- und Kaltstarts unterscheiden konnte, war jetzt standardmäßig vorhanden. Es entfielen dagegen einige eher „hackerfreundliche“ als „anwenderfreundliche“ Komponenten des ROMs, darunter der eingebaute Mini-Assembler, die „Step“- und „Trace“-Befehle des Monitors, und der Sweet-16-Interpreter. Bis auf den Sweet 16 kamen diese später auf dem Apple IIc und dem erweiterten Apple IIe wieder zurück.\n\nDer Apple II+ war der erste Rechner, der von Apple auch in Europa gezielt vermarktet wurde; das hiesige Modell hieß Apple II Europlus. Bis auf die Umstellung von 110 auf 220 Volt Netzspannung und die von 60 auf 50 Hertz Bildwiederholrate war der Europlus identisch mit dem amerikanischen Apple II+. Während die Hardware für Farbdarstellung auf dem amerikanischen NTSC-Videosystem bereits auf der Hauptplatine vorhanden war, musste für Farbdarstellung auf dem komplizierteren europäischen PAL-System eine Steckkarte zugekauft und nachgerüstet werden, viele dieser PAL-Karten erzeugten aber eine relativ schlechte Bildqualität (Bildzittern, laufende Moiré-Muster), eine rühmliche Ausnahme war die PAL-Karte der Firma IBS (nicht mehr existent). Ohne eine solche Karte erhielt man auf europäischen Fernsehgeräten und Videomonitoren nur eine Schwarz-Weiß-Darstellung.\n\nDie Elektronik des Apple II+ wurde im Laufe der folgenden Jahre immer wieder leicht aktualisiert, meistens ohne dass sich dies sichtbar auf den Betrieb auswirkte. Die Motherboard-Revisionen 7 und höher konnten, wenn der Benutzer einen ROM-Chip gegen ein entsprechend programmiertes EPROM tauschte, auch Kleinbuchstaben, Umlaute, und andere nicht-amerikanische Zeichen anzeigen. Deren Eingabe war allerdings immer noch nicht ohne weiteres möglich, da die Tastatur nur Großbuchstaben erzeugte. Auch wurde der dafür einzusetzende Chip nicht von Apple selbst angeboten, sondern musste anderweitig besorgt werden.\n\nDer Apple II+ wurde sehr erfolgreich und sehr oft als Schulcomputer eingesetzt und des Öfteren geklont. Er war leicht zu klonen, da in ihm nur Standard-ICs verwendet wurden. Allerdings waren die Klone fast alle illegal, weil sie die Copyright-geschützte Firmware des Apples einfach kopierten und nicht, wie später die IBM-PC-Klone, bei gleicher Funktionalität neu programmierten. Die Apple-Firmware enthielt noch keine saubere Einsprungtabelle, sondern verwendete feststehende Einsprungspunkte irgendwo „mittendrin“. Zugleich musste der Firmware-Code ungewöhnlich kompakt sein, um den für ihn reservierten knapp bemessenen Adressraum von 2 KB nicht zu sprengen. Beides zusammen machte die funktionsgleiche Neuprogrammierung sehr viel schwieriger als beim IBM PC. Als die „Wild-West-Zeiten“ im Microcomputerbereich allmählich endeten, verschwanden die Apple-Klone daher schnell wieder vom Markt.\n\n1982 wurde der Apple II+ vom Apple IIe abgelöst.\n"}
{"id": "1214044", "url": "https://de.wikipedia.org/wiki?curid=1214044", "title": "LF-Parser", "text": "LF-Parser\n\nEin LF-Parser () ist ein Top-Down-Parser, der ausschließlich auf der Grundlage der k nächsten Eingabe-Token entscheidet, zu welcher Alternative ein Nichtterminalsymbol ersetzt wird. Von einem LL-Parser unterscheidet ihn, dass die Entscheidungsmengen, die beim Vorausschauen verwendet werden, für jedes Nichtterminalsymbol paarweise disjunkt sein müssen, das heißt, zu jedem Zeitpunkt muss jedes Tupel von Metasymbol und k-lookahead-Token eindeutig auf eine Alternative verweisen. Daher funktioniert dieses Verfahren nur für spezielle kontextfreie Grammatiken, die LF(k)-Grammatiken. LF-Parser werden auch als strong-LL-Parser bezeichnet.\n\nEin LF-Parser heißt LF(k)-Parser, wenn er während des Parsens \"k\" Token vorausschauen kann. Diese Token werden auch als \"lookahead-Token\" bezeichnet.\n\nDie Klasse der LF(1)-Grammatiken ist gleich der Klasse der LL(1)-Grammatiken. Für formula_1 unterscheiden sich jedoch die Klassen, wie man an folgender Grammatik erkennt, die in LL(2), nicht jedoch in LF(2) liegt:\nDiese Grammatik kann mit einem LF(2)-Parser nicht umgesetzt werden, denn sind die nächsten beiden Tokens \"ab\", so könnte sowohl eine ε-Ableitung notwendig sein, wenn von \"A\" aus abgeleitet wurde als auch eine Ableitung nach \"a\", wenn von \"B\" aus abgeleitet wurde. Es liegt eine LL-Grammatik vor, da zwischen zwei möglichen Ableitungen mit demselben Kontext stets mittels der Lookaheads unterschieden werden kann. Trivialerweise ist jede LF-Grammatik eine LL-Grammatik. Das Beispiel lässt sich auf beliebige formula_2 ausweiten, etwa:\nLF-Parser lassen sich wesentlich einfacher implementieren, etwa mittels rekursiven Abstiegs, der Begriff LL-Parser ist jedoch wesentlich häufiger verwendet, da LL(1)/LF(1)-Parser am verbreitetsten sind, und dort kein Unterschied besteht. Jede LL(1)-Grammatik ist auch eine LF(1)-Grammatik, denn wenn der Kontext zur Auswahl einer Ableitung entscheidend wäre, so wäre dies höchstens ein Token \"a\" im Lookahead, d. h. eine ε-Ableitung oder eine mit \"a\" beginnende Ableitung wären möglich, dies geschähe dann jedoch auch unter Berücksichtigung des Kontexts.\n\n"}
{"id": "1215553", "url": "https://de.wikipedia.org/wiki?curid=1215553", "title": ".Mac", "text": ".Mac\n\n.Mac (dotMac) war ein Online-Service von Apple, der die Funktionalität eines Macintosh-Computers mit dem Betriebssystem Mac OS X auf das Internet erweiterte. .Mac wurde am 10. Juli 2008 durch MobileMe ersetzt, welches am 1. Juli 2012 wiederum durch iCloud ersetzt wurde.\n\nDer .Mac-Service war ursprünglich als kostenloser Dienst namens iTools von Apple gestartet. Er beinhaltete anfangs lediglich die Benutzung einer E-Mail-Adresse und wurde mit dem Slogan „Free e-mail for life“ beworben. Nach einem Jahr wurde der Dienst erweitert und kostete nun Gebühren. Damals wurde ein Preis von 79 € im Jahr erhoben. In Verbindung mit der iLife-Produktlinie wollte Apple Internet-Technologien und den Mac für Heimanwender näher zusammenbringen.\n\nEin .Mac-Benutzer erhielt eine E-Mail-Adresse mit der Domain „mac.com“ (z. B. username@mac.com) sowie bis zu fünf weitere E-Mail-Alias-Adressen. Der E-Mail-Dienst beinhaltete Funktionen wie eine automatische Antwort und den Zugriff per Web-Oberfläche. Dabei wurden die Daten von Mail (Apples E-Mail-Anwendung für Mac OS X) immer synchron gehalten, abgesendete E-Mails via Webinterface landeten also auch automatisch im Ordner „Gesendet“ des Programmes, was via IMAP realisiert wurde.\n\niDisk war Apples Bezeichnung für eine WebDAV-basierte Speicherplatz-Lösung, die sich ähnlich wie ein lokaler Datenträger eines Rechners verhielt. Auf Dateien in einem „Public“ genannten Ordner der iDisk konnten auch andere Benutzern zugreifen. Der Zugriff ließ sich auch mit einem Passwort einschränken. Die iDisk hatte seit dem 7. August 2007 einen Speicher von 10 Gigabyte und war dafür gedacht, mit Freunden und Kollegen oder zwischen eigenen Computern Daten auszutauschen oder auch Dateien zu sichern. Zuletzt betrug der Speicherplatz für eine Einzellizenz 20 GB, für eine Familienlizenz 40 GB.\n\nPhotocasting wurde mit iPhoto '06 eingeführt. Mit der Software iPhoto konnte ein Fotoalbum als Photocast eingerichtet werden. Die Bilddateien wurden dabei auf den .Mac-Server geladen. Benutzer von iPhoto '06 konnten sich bei einem solchen Photocast anmelden. Dieser erschien in der Liste von iPhoto 06 in einem Photocast-Ordner. Hier befanden sich dann die aktuellen Bilder des abonnierten Albums. Mit diesen Bildern konnte man alles machen, was mit Bildern aus anderen iPhoto-Bibliotheken auch gemacht werden konnte. Wer kein iPhoto '06 hatte, konnte die Bilder nur betrachten. Ein Photocast war eigentlich nicht mehr als ein RSS-Web-Feed, der mit jedem bilderfähigen Feedreader gelesen werden konnte, beispielsweise Safari, NetNewsWire oder Sarge. Photocasting war für Leute interessant, die im Familien- und Bekanntenkreis oft Fotos austauschen. Der Photocast konnte durch ein Passwort geschützt werden.\n\nMit einem Klick wurde eine in iWeb erstellte Website auf den .Mac-Server geladen und dort bereitgestellt oder aktualisiert.\n\nMit .Mac Sync konnten Termine und Aufgaben aus iCal, Adressbücher, Lesezeichen, Passwörter, E-Mails-Postfächer, Dashboard-Widgets, Dock-Objekte, Notizen und Mac OS X Systemeinstellungen mit dem .Mac-Server synchronisiert werden. Diese Daten konnten entweder mit einem anderen Mac OS X-Computer oder zum Teil über eine Web-Oberfläche mit jedem beliebigen Webbrowser abgerufen werden. Die Daten konnten auch auf mehreren Rechnern synchron gehalten werden, wobei in Mac OS X immer ein sauberer Abgleich erfolgte, das heißt, es wurde nicht einfach alles überschrieben, was auf dem lokalen Rechner vorlag, sondern die Änderungen miteinander kombiniert.\n\nMit .Mac kam die hauseigene Backup-Software Backup 3.1. Diese bot die Möglichkeit, Daten für Sicherungszwecke auf CD/DVD zu brennen, auf einer externen Festplatte zu sichern oder auf den .Mac-Speicherplatz zu übertragen. Backup 3.1 übertrug zumindest auf die iDisk nur kleine Datenmengen wie Termine, Adressen oder Dokumente. Filme konnten nicht hochgeladen werden.\n\nDie .Mac-Gruppenfunktion bot die Möglichkeit der einfachen Kommunikation zwischen .Mac-Mitgliedern. Ein .Mac-Benutzerkonto war Voraussetzung für eine Gruppenmitgliedschaft, dazu genügte aber bereits ein Benutzerkonto auf Probe; dieses konnte auch nach Ablauf der Probezeit weiterverwendet werden. Auf der Gruppenseite konnten „Schwarze Bretter“, Kalender, Ankündigungen, Mitgliederlisten und ähnliches für die gesamte Gruppe zugänglich gemacht werden, und auch Fotos und Videos ausgetauscht werden. Jede Gruppe hatte einen eigenen Bereich auf der iDisk, dem flexibel Speicherplatz, zugeteilt werden konnte. Die Gruppen-Funktion wurde für den privaten Gebrauch konzipiert und stellte keine Geschäftsanwendung dar.\n\nDie E-Mail-Adresse (s. o.) war gleichzeitig Benutzerkonto für die Verwendung von iChat, einem Instant Messenger, der mit dem AOL Instant Messenger kompatibel war.\n\nVerbindungen mit dem Webinterface von .Mac waren nur unverschlüsselt möglich. Teile des Angebots (z. B. Unterhaltungen über iChat AV oder der Zugriff auf die iDisk mittels WebDAV) konnten auf Wunsch verschlüsselt genutzt werden.\n\nIn der WWDC 2008 eröffnenden Keynote wurde die Ersetzung von .Mac durch den neuen Dienst MobileMe angekündigt. Die bisherigen Dienste wurden erweitert, und neue Dienste auch für mobile Geräte werden angeboten.\n\n"}
{"id": "1219140", "url": "https://de.wikipedia.org/wiki?curid=1219140", "title": "Amiga-Magazin", "text": "Amiga-Magazin\n\nDas Amiga-Magazin war eine von 1987 bis 2009 vom Markt & Technik Verlag herausgegebene Computerzeitschrift, die sich mit dem Heimcomputer Commodore Amiga beschäftigte.\n\nIm Gegensatz zu \"Amiga Games\" oder \"Amiga Joker\", die sich mit aktuellen Amiga-Spielen befassten, beschäftigte sich das Amiga-Magazin mehr mit der Hardware sowie den Programmen des Amiga. Die erste Ausgabe erschien 1987, kurz nach der Markteinführung des Amiga 500 in Deutschland. Das Magazin, das zeitweise einen Umfang von mehr als 300 Seiten erreichte, erschien bis zur Ausgabe 01/1998 als eigenständige Zeitschrift. Seitdem erschien das Blatt als zunächst 32-, dann kurzzeitig 8- und zuletzt wieder 16-seitige Beilage zur Zeitschrift PCgo, die im selben Verlag herausgegeben wird.\n\nDie letzte Ausgabe war die 07/2009, danach wurde das Magazin aus wirtschaftlichen Gründen eingestellt.\n\nNeben Vorstellung und Tests von Hard- und Software gab es oft Programmierwettbewerbe oder Programmlistings zum Abtippen.\n\n"}
{"id": "1221514", "url": "https://de.wikipedia.org/wiki?curid=1221514", "title": "Phex", "text": "Phex\n\nPhex ist ein Peer-to-Peer-Filesharing-Client für das Gnutella-Netzwerk. Es ist eine freie Software und ohne Spy- oder Adware. Phex wird unter der GNU General Public License veröffentlicht.\n\nPhex basiert auf Java-Technologie und ist daher auf vielen verschiedenen Systemen verfügbar, die eine J2SE-1.4-Laufzeitumgebung anbieten. Dazu gehören unter anderem Windows, Mac OS X, Linux und diverse weitere Unix-Systeme.\n\nPhex unterstützt Downloads mittels \"„swarming“\", was bedeutet, dass Teile der Datei von verschiedenen Peers gleichzeitig heruntergeladen werden können. Dadurch beschleunigt sich der Ladevorgang wesentlich. Außerdem werden in Voreinstellung seltenere Dateiteile bevorzugt. Für fortgeschrittene Benutzer bietet Phex durch viele Möglichkeiten der Konfiguration einen hohen Grad an Kontrolle.\n\nPhex bietet auch Dateientausch in geschlossenen Gruppen – sogenanntes privates Filesharing.\n\nIm April 2001 hat Konrad Haenel den veralteten Gnutella-Client Furi abgespaltet und in Phex umbenannt. Nachdem Konrad Haenel am Phex-Projekt bis Ende 2001 beteiligt war, hat er das Projekt verlassen. Die Entwicklung wird von Gregor K. bis heute weitergeführt.\n\nSeit dem Jahr 2006 gibt es auch eine I2Phex–Variante des Clienten, der einen anonymen Dateitausch ermöglicht, indem das Gnutella-Protokoll über das I2P-Netzwerk geroutet wird. Dazu werden Base-64-Keys genutzt, die die IP-Adresse schützen. Ziel ist es, den Code für das I2P-Netzwerk in einer der kommenden Versionen von Phex zu integrieren, so dass der Client sowohl mit IP-Adressen als auch über das Layer-Netzwerk I2P Dateien übertragen kann.\n\nVerschiedene akademische Studien nutzen Phex: \n\n"}
{"id": "1223537", "url": "https://de.wikipedia.org/wiki?curid=1223537", "title": "/dev/random", "text": "/dev/random\n\ncodice_1 ist unter vielen unixoiden Betriebssystemen eine zeichenorientierte virtuelle Gerätedatei, über das Programme auf einen systemweiten Zufallszahlengenerator von hoher Qualität zugreifen können. Da für netzwerkorientierte Systeme wie Unix Kryptographie-Dienste und damit Zufallszahlen eine bedeutende Rolle spielen, kommt dieser Datei und dem dahinterstehenden Treiber eine wichtige Bedeutung zu.\n\nDer Zufallsgenerator sammelt Umgebungsrauschen von Gerätetreibern und anderen Quellen in einem Entropie-„Pool“. Der Generator speichert auch eine Abschätzung über die Anzahl der Bits im Entropie-Pool. Aus diesem „Pool“ werden die Zufallszahlen generiert. Beim Lesen gibt codice_1 nur solange Zufallszahlen zurück, bis die abgeschätzte Entropiemenge erschöpft ist; dann blockieren Lesezugriffe auf codice_1, bis zusätzliches Umgebungsrauschen erhalten wurde. codice_1 sollte ausreichend sein für Anwendungszwecke, die auf eine sehr hohe Qualität der Zufälligkeit angewiesen sind, wie etwa Verschlüsselung (beispielsweise One-Time-Pads oder Schlüsselerzeugung). Aus Geschwindigkeitsgründen wird in der Praxis oft nur der „Seed“ eines Pseudo-Zufallszahlengenerators von codice_1 gelesen (z. B. in OpenSSL, PGP und GnuPG).\n\nDer aktuelle Füllstand des Entropie-Pools lässt sich unter Linux aus der Datei codice_6 ermitteln. Eine Ausgabe der Datei liefert die verfügbare Entropie in Bit, wobei das Maximum von 4096 Bit einem vollständig gefüllten „Pool“ entspricht.\n\nAus codice_7 (von engl. ) können wie aus codice_1 Zufallszahlen gelesen werden. Im Gegensatz zu letzterem blockiert es jedoch nicht, wenn eine definierte Entropieschwelle unterschritten wird. In diesem Fall könnte es theoretisch möglich sein, dass die erzeugten Pseudozufallszahlen im Nachhinein von einem Angreifer berechnet werden könnten.\n\ncodice_1 (wie auch codice_7) ist weder im Filesystem Hierarchy Standard 2.3 noch in der Single UNIX Specification 3.0 spezifiziert.\n\ncodice_1 ist unter Solaris seit Solaris 9 (2002) Bestandteil des Kernels. Der Linux-Kernel stellt ein codice_1-\"Device\" mindestens seit 2002 bereit. In FreeBSD findet ein auf dem Yarrow-Algorithmus basierender Generator Verwendung, codice_1 wird seit Juni 2000 (FreeBSD 2.2) unterstützt. AIX bietet sowohl codice_1 als auch codice_7 und verwendet seit AIX 5.2 ebenfalls eine Yarrow-Implementation.\n\nÜber Software, wie z. B. codice_16, codice_17, codice_18, lässt sich die Entropie verbessern sowie bzw. der Entropie-Pool vergrößern, damit mehr Zufallszahlen zur Verfügung stehen. Mit der GNU-Software codice_19 lassen sich unter Linux und ähnlichen Betriebssystemen physikalische Zufallszahlengeneratoren einbinden.\n\n\n"}
{"id": "1223861", "url": "https://de.wikipedia.org/wiki?curid=1223861", "title": "Polycomputer 880", "text": "Polycomputer 880\n\nDer Polycomputer 880 war ein in der DDR weit verbreiteter Lerncomputer auf Basis des Mikroprozessors U880. Der Computer wurde ab 1983 vom Volkseigenen Betrieb \"VEB Polytechnik Karl-Marx-Stadt\" für den Einsatz als Lernmaschine an allgemein bildenden Schulen und höheren Bildungseinrichtungen produziert.\n\nDas Gerät wurde betriebsfertig eingebaut in einem Diplomatenkoffer (48 × 38 × 12 cm) geliefert und hatte ein Gesamtgewicht von 5 kg. Zur Dateneingabe verfügte der Polycomputer über eine hexadezimale Tastatur sowie einige Funktionstasten. Die Ausgabe erfolgte über eine 8-stellige Siebensegmentanzeige. Der Zustand sämtlicher Leitungen des Daten- und Adressbusses konnte über Leuchtdioden abgelesen werden.\nDas Betriebssystem des Polycomputers befand sich auf einem 4 KByte großen ROM, für die Datenspeicherung war 1 KByte RAM vorgesehen. Aufgrund der unvollständigen Adressdecodierung war eine Aufrüstung auf volle 64 KByte Speicher nicht möglich. Für externe Datenspeicherung konnte ein Kassettentonbandgerät über eine DIN-Buchse angeschlossen werden. Zur Druckausgabe konnte über einen Lautsprecherstecker ein Fernschreiber angeschlossen werden. Ein Treiber für den Schleifenstrom des Fernschreibers war bereits eingebaut.\nDie Datenbusse waren über einen Steckverbinder erreichbar. Somit konnten zahlreiche Bastelobjekte für Zubehör realisiert werden. Intern war eine Steckfassung für einen weiteren ROM oder EPROM vorhanden.\nAus Preisgründen kam als Prozessor eine 2. Wahl-Variante der U880 CPU mit nur 1 MHz Taktfrequenz zum Einsatz.\n\nMit zum Lieferumfang gehörten außerdem ein Bedienhandbuch, ein Systemhandbuch sowie ein Arbeitsbuch. Im Jahr 1984 betrug der Neupreis 3.449 Mark.\nParallel dazu war der Einplatinenrechner LC80 auf dem Markt.\n\n\n"}
{"id": "1226702", "url": "https://de.wikipedia.org/wiki?curid=1226702", "title": "System i", "text": "System i\n\nIBMs Systeme i haben ein proprietäres Betriebssystem namens IBM i for Business und eine eigene Datenbank namens DB2, worauf in der überwiegenden Anzahl der Installationen kaufmännische Anwendungen zur Verwaltung der typischen Geschäftsprozesse eines Unternehmens, als Server- oder Client/Serveranwendung, betrieben werden. Typisch für dieses System ist die enge Verknüpfung zwischen Betriebssystem und Datenbank (vgl. Middleware), welche erste Merkmale einer Appliance aufweist. Diese Ideen wurden bei IBM \"PureSystems\" wieder aufgegriffen.\n\nDie aktuellen IBM-Systeme i sind skalierbar, das heißt, man kann diese als relativ kleine Maschine (z. B. fünf Benutzer) betreiben, aber auch in Großrechner-Dimensionen (mit tausenden Benutzern). Die installierten Anwendungen können mehrere tausend Benutzer parallel bedienen, sofern die Hardware richtig dimensioniert ist.\n\nDas Anfang 2016 größte verfügbare \"System i\" Modell E880 kann bis zu 16 CPUs mit 192 Power8-Prozessorkernen enthalten und bis zu 16 TB RAM haben.\n\n1986 initiierte IBM das Projekt „Silverlake“ unter Leitung von Frank G. Soltis, Chef-Entwickler bei IBM in Rochester und Professor für Computer Engineering (auch: Technical Computer Science oder Technische Informatik genannt) an der Universität von Minnesota. Soltis entwickelte die fundamentalen Design-Konzepte des Systems AS/400, die heute noch immer in einer OS/400-Partition gültig sind.\n\nSie gehörte ursprünglich zu den Minirechnern, wird aber hauptsächlich kommerziell verwendet und bietet eine breite Schicht an Anwendungsprogrammen. Der Begriff Minirechner wird heute (2007) nicht mehr verwendet und hat heute eine andere Bedeutung. Sie gehört in die Kategorie der Midrange-Rechner; ein Begriff, der sich Mitte der 1980er Jahre bis Ende der 1990er Jahre etablierte. Hiermit ist die Größe zwischen Servern, die auf Intel-kompatiblen Prozessoren basieren, und Großrechnern (engl. \"mainframes\") gemeint.\n\nDie AS/400 wurde am 21. Juni 1988 als gemeinsame Weiterentwicklung der IBM-Systeme System/36 und System/38 auf den Markt gebracht. Genauer gesagt handelte es sich um den Nachfolger der S/38 und war zu dieser sogar objektcodekompatibel, d. h. bisherige S/38-Anwendungen konnten ohne oder mit nur geringen Modifikationen wiederverwendet werden. Die /36-Anwendungen konnten in einem speziellen Modus unverändert laufen (Befehl: STRS36), was zwar zu Geschwindigkeitseinbußen führen konnte, jedoch viele Anwender dazu bewog, auf die AS/400 zu wechseln. Im Jahre 2000 wurde die \"AS/400\" in \"iSeries\" umbenannt, im Oktober 2003 wurde der Name \"iSeries i5\" geprägt, und seit 2006 gibt es die Modelle \"IBM System i5.\" Der neueste Name seit 2007 ist \"System i.\" Die neuen Modelle haben auch wesentlich erweiterte Möglichkeiten, wie z. B. den Betrieb von Linux und Unix (AIX), Lotus Domino, Java und den Einsatz als Web-Server. Fast alle IP basierten Dienste können auch unter i5/OS laufen (wie z. B. SNMP, FTP, DNS, Telnet, SMTP, POP3, LDAP, VPN).\n\nDie ersten Modelle waren die sogenannten B-Modelle (B10, B20, B30, B35, B40, B50), danach folgten die D-, E- und F-Modelle. IBM ging danach zu numerischen Modellbezeichnungen über. In der Pinakothek der Moderne in München befindet sich ein Modell 520, welches im Jahre 2000 einen Design-Preis erhielt.\n\nIBM \"PureSystems\" greift Ideen des \"System i\" bzw. der \"AS/400\" wieder auf und erweitert sie in Richtung Appliance.\n\nDie Architektur der System-i-Modelle ist durch die Besonderheit geprägt, dass das Betriebssystem von der Hardware durch eine Isolationsschicht getrennt ist. Wenn bei Systemen ohne diese Isolationsschicht das Betriebssystem die Hardware direkt anspricht, verlangt eine Modernisierung der Hardware eine intensive Anpassung des Betriebssystems. Diese ist oft so kostenintensiv, dass vorgezogen wird, eine neue Systemreihe zu entwickeln und anzukündigen.\n\nDie Isolationsschicht der Systeme i hingegen erlaubt, Hardwaremodernisierungen ohne Eingriff in das Betriebssystem vorzunehmen (Siehe LIC weiter unten). Dies erklärt, warum die AS/400, die vor bald 20 Jahren angekündigt wurde, sich bis zum System i mit der modernsten Hardware wandeln konnte, ohne dass die Anwendungsprogramme umgeschrieben werden mussten. Denn Anwendungsprogramme basieren auf dem Betriebssystem und nicht auf der Hardware. Selbstverständlich wurde das Betriebssystem ebenfalls weiterentwickelt, aber ohne die Zwänge, die üblicherweise mit der Modernisierung der unterliegenden Hardware verbunden sind.\n\n\nBei der Gehäuseform unterscheidet man zwischen Standalone und Rackmounted. Kleinere Modelle werden auch noch als einzeln stehende Gehäuse angeboten. Ab dem Modell 550 bis zur 570 gibt es nur 19-Zoll-Gehäuse, die in ein Rack verbaut werden, das Enterprise-Modell 595 wird in speziellen 24-Zoll-Rahmen geliefert. Je nach Prozessoranzahl und Prozessorbaureihe werden die Hauptgehäuse durch eine oder mehrere Erweiterungseinheiten ergänzt. In diesen Erweiterungseinheiten sitzen dann Festplatten, IOPs, IOAs, HSL-Adapter und sonstige Adapter. Es gibt auch Erweiterungseinheiten für Prozessoren.\n\nMan kann diese Systeme (Server) auch per iSCSI oder SAN-Adapter festplattenlos betreiben. Die Festplattenkapazitäten werden dann von einer SAN-Lösung, also einem zentralen Storage-System bereitgestellt.\n\nEin optionaler virtual I/O-Server (VIO), eine eigene Partition innerhalb des System i, kann physikalisch vorhandene Ethernet-, SAS-, SCSI- und FC-Adapter als virtuelle Geräte an andere Partitionen auf dem gleichen System weitergeben. So können mehrere Partitionen Adapter gemeinsam nutzen.\n\nSpezielle Aufgaben werden nicht vom Hauptprozessor direkt, sondern von Spezialprozessoren durchgeführt, was den Durchsatz extrem erhöht. Da sind z. B. die IOPs (Input/Output-Prozessor) zu erwähnen. Die entsprechenden Adapterkarten (IOAs) für Netzwerk, Festplatten etc. werden über diese IOPs angesteuert und entlasten somit den Hauptprozessor. Bis 2006 hatte fast jedes System einen oder mehrere IOPs. Diese IOP-Karten kosten ungefähr so viel wie zwei normale Büro-PCs. Wegen der steigenden Prozessorleistung, schnellerer Bussysteme (z. B. PCI-X) und um die Systeme günstiger anbieten zu können, ist IBM dazu übergegangen, IOP-lose IOAs zu verwenden.\n\nFür die Ver-/Entschlüsselung gibt es eigene Kryptografie-Co-Prozessoren auf PCI- oder PCI-X-Adapterkarten, welche ebenfalls den Vorgang beschleunigen und den Hauptprozessor entlasten.\n\nEin IBM System i ist ein „Integrationssystem“. Das bedeutet, es unterstützt nicht nur OS/400 bzw. i5/OS, sondern auch Linux (native PowerPC- oder Intel-Architektur über eine PC-Steckkarte) und das hauseigene Unix-Derivat AIX, sowie Windows (über eine PC-Steckkarte). Die PC-Steckkarte (auch IPCS genannt) ist ein eigenständiger PC mit CPU und Hauptspeicher, der Festplattenplatz auf dem System i verwendet. Es können je nach Modell bis zu 60 dieser PC-Karten eingebaut werden. Zusätzlich können viele Linux-Partitionen installiert sein. Somit kann man bspw. 50 einzelne Server auf einem einzigen System i zusammenfassen, was die Administration und das Speichermanagement erheblich vereinfacht. Das Storage-System des System i kann mit einem SAN verglichen werden, welches dynamisch benötigten Platz einer jeweiligen Partition (LPAR genannt) zuweisen kann (ab V5R4).\n\nInnerhalb einer Partition (oder eines System mit nur einer Partition) mit OS/400 (bis V5R3), i5/OS (ab V5R4) oder IBM i for Business (ab V6R1) laufen die meisten Programme, die ab 1988 entwickelt wurden, auch heute noch. Und das, obwohl sich die Hardware und die Software (Betriebssystem und Compiler) gravierend verändert haben.\n\nDas OS/400 ist ein herstellerspezifisches (andere Bezeichnung: proprietäres) Betriebssystem, das grundsätzlich nur auf dieser Hardware von IBM lauffähig ist. Es kann aber auch in einer Partition auf einem eServer pSeries (p5) betrieben werden. Neben dem integrierten Datenbankverwaltungssystem bietet das OS/400 u. a. integrierte Sicherheitsfeatures und Netzwerkunterstützung. Viele Systemelemente sind hier in einer Umgebung zusammengefasst. Das \"i\" im abgelösten Namen iSeries bedeutet laut IBM \"integrated,\" da die Integration verschiedener Software-Elemente wie Datenbank, Sicherheitsverwaltung, Betriebssystem, Programmierumgebung etc. die Anwendung und Administration vereinfacht.\n\nDie System i gilt als ein sehr stabiles System; in vielen Unternehmen laufen diese Systeme seit Jahren rund um die Uhr ohne Systemabstürze. Gemeinsam mit der vergleichsweise hohen Bedienerfreundlichkeit ergibt das einen niedrigen TCO-Wert. Über Viren- und Trojanerangriffe ist bisher nicht viel bekannt. In großen Unternehmen betreut oft ein einziger Administrator eine iSeries, die mehreren Tausend Anwendern Daten und Programme bereitstellt.\n\nNeben dem Betriebssystem OS/400 laufen auch GNU/Linux und AIX nativ auf dem System i. Zusätzlich ist es möglich, über sogenannte IXS-Karten (Integrated xSeries Server for iSeries) sowie IXA-Adapter (Integrated xSeries Adapter) und deren x86-Architektur Windows auf die Hardware-Ressourcen der Maschine zugreifen zu lassen. Seit 2007 ist es auch möglich, IBM Blade Center mit entsprechenden Blade Server per iSCSI an eine i5/OS-Partition anzuschließen. In diesem Falle fungiert das System i als Plattenspeicher mit hoher Sicherheit.\n\n\"Objektbasiert\" ist nicht in jedem Falle gleichzusetzen mit dem Begriff \"objektorientiert,\" wie er bei Programmiersprachen häufig gebraucht wird. Grundsätzlich wird jedes Element im System – ob Bibliothek, Benutzerprofil, Gerätekonfiguration etc. – als Objekt mit bestimmten Funktionen und Attributen angesehen. Ein Objekt gliedert sich in einen \"Header\" und in die eigentlichen Informationen, wobei der Header das Objekt allgemein beschreibt (z. B. durch Eigentumsrechte, Name, Objektart, Schnittstellen). So wird durch den Header z. B. definiert, auf welche Art und Weise das Objekt bearbeitet werden kann. So verfügen Objekte über Eigenschaften (oder \"Parameter\"), die verändert werden können. Nicht definierte Eigenschaften können nicht angelegt oder geändert werden. Beispielsweise kann einem Objekt „Benutzerprofil“ nicht die Eigenschaft „Farbe“ angehängt oder modifiziert werden. Dateien sind ebenso stets ein Objekt. Reine Textdateien gibt es nur zur Emulation von Kompatibilitäten mit anderen Betriebssystemen; in der Regel ist eine AS/400-Datei aber ein Datenbankobjekt. Es ist jedoch möglich Dateien in das Internal File System zu transferieren welche z. B. JavaCode enthalten und diese zu kompilieren.\n\nDie relationale Datenbank ist fest in das Betriebssystem OS/400 integriert. Es gilt: „Ohne Betriebssystem keine Datenbank und umgekehrt“. Die DB2/400 bietet eine hohe Funktionalität und Leistung, weshalb sie zu den führenden Datenbanksystemen gehört. Durch die feste Integration ist keine zusätzliche Berechtigungsverwaltungssoftware (wie z. B. RACF) nötig, sondern es kann mit den vorhanden Objektrechten autorisiert werden. Durch die permanente Weiterentwicklung durch IBM ist diese Datenbank heute auch für eine Vielzahl von externen Schnittstellen wie JDBC (Java), ODBC, FTP o. Ä. offen.\n\nDie Datenbank hatte am Anfang (1988) gar keinen Namen, wurde später DB2/400 genannt und heißt seit iSeries DB2/UDB (Universal Database).\n\nFür Abfragen und Datenmanipulation wird SQL (Structured Query Language) verwendet. Mit Programmiersprachen, die nativ im OS/400 compiliert werden können, kann auch mit programmspezifischen Zugriffsmethoden die Datenbank abgefragt und manipuliert werden.\n\nIm April 2007 hat IBM eine Kooperation mit MySQL AB angekündigt, um DB2 UDB for iSeries als Database-Engine für MySQL verfügbar zu machen. Dadurch kann die freie Datenbank MySQL auch auf dem System i5 eingesetzt werden. IBM erhofft sich davon, neue Einsatzbereiche des Systems i5 für MySQL- und PHP-Anwendungen zu eröffnen.\n\nDer Arbeitsspeicher und der Festplattenspeicher werden zu einem großen virtuellen Speicher zusammengefasst, das heißt, der Adressraum wird durchgängig adressiert.\n\nSomit gibt es keine Segmentierung des Adressraums, die Programme nutzen während der Ausführung absolute Adressen. Wenn ein Objekt bearbeitet wird, existiert keine vollständige Kopie im Arbeitsspeicher, sondern es werden nur Teile (sog. „Pages“) geladen. Wird ein Objekt gelöscht, markiert das System den darauf zeigenden Pointer als ungültig und nicht wiederverwendbar, sodass hier das Ausnutzen von Sicherheitslücken unterbunden wird.\n\nKenner der Windows- oder Unix-Systeme müssen sich das Konzept so vorstellen: Der Hauptspeicher wird als Read-Cache für die Festplatten verwendet, alle Festplatten stellen eine Art „Auslagerungsdatei“ dar, in der alle Objekte (temporär oder permanent) abgelegt werden. Diese Daten werden beim Systemstart nicht gelöscht, sodass „permanente“ Objekte wieder zur Verfügung stehen.\n\nDurch die einstufige Speicherverwaltung ist für den Anwender (und auch für das Betriebssystem) nicht nachvollziehbar, welche Objekte auf welcher Platte abgelegt werden. Vorteilhaft ist, dass sich der Anwender nicht um eine Aufteilung des Plattenplatzes zu kümmern braucht. Bei Plattenspeicherengpässen muss die Plattenkapazität nur erweitert werden, um die Verteilung der Daten kümmert sich das System. Die AS/400 hatte von Anfang an das sogenannte RAID-Prinzip integriert. Bei Ausfall einer Platte kann das System durch die \"checksum\"-Methode (bereits im S/38 vorhanden) fehlende Daten innerhalb des Plattenstapels ermitteln. Bei Ausfall von mehr als einer Platte ist allerdings diese Berechnung nicht mehr möglich (erst neuere Modelle ab 2006 unterstützen RAID6). Daher ist für Hochverfügbarkeitsumgebungen eine Plattenspiegelung ratsam, welche auch Controller- und Bus-übergreifend möglich ist. Defekte Magnetplatten konnten seit der Betriebssystemversion 3.2 während des laufenden Betriebs gewechselt werden. Dies wird auch als \"Hot-Swapping\" bezeichnet.\n\nDie iSeries war von Anfang an auf einen 128-Bit-Adressraum ausgelegt. Die ersten Prozessoren (CISC-Eigenentwicklungen von IBM) waren 48-Bit-Bipolar-Systeme, Ende 1994 stieg IBM auf die 64-Bit-POWER-Prozessoren um. Derzeit werden im Betriebssystem 80 Bit für die Adressierung verwendet (unabhängig von der CPU-Architektur), ein Umstieg auf 128 Bit ist mit einfachen Mitteln machbar.\n\nDer 128-Bit-Adressraum erlaubt eine direkte Adressierung von 18 Quintillionen Bytes. Bei einem Wechsel der Prozessorarchitektur auf 128-Bit-CPUs ist durch das objektbasierte Konzept und dem Vorhandensein von Objektcode in den Programmen ein problemloser Umstieg möglich, siehe nachfolgendes Kapitel. Systemintern sind alle Pointer 16 Byte breit.\n\nDer lizenzierte interne Code (auch \"LIC\" genannt) ist das Herzstück jeder OS/400 (i5/OS) Partition (Systems), da diese Software-Komponente zwischen der Hardware und der Anwendungssoftware vermittelt (das Betriebssystem OS/400 wird hier als Anwendungssoftware angesehen). Nur dieser LIC kann eine Hardware direkt ansprechen, und er stellt der Anwendungssoftware alle nötigen Programmierschnittstellen zur Verfügung. Es kann keine Funktion unter Umgehung dieser APIs verwendet werden, und diese prüfen neben Plausibilität auch Berechtigungen. Dies ist auch ein Grund, warum OS/400 auf der gleichen Hardware etwa 3–5 % langsamer als beispielsweise AIX ist, da diese Prüfungen Rechenzeit benötigen.\n\nBei einem Hardware-Wechsel ändert IBM diesen internen Code, behält die Schnittstellen aber bei, sodass die Anwendungsprogramme nicht modifiziert zu werden brauchen. Bei gravierenden Änderungen in der Hardware (Wechsel der CPU-Architektur) wird weiterhin auf die in den Programmen üblicherweise vorhandene Zwischencode-Schicht zurückgegriffen. In einem Programmobjekt ist nicht nur ein (fast) direkt ausführbarer Code enthalten, sondern auch ein Objektcode. Ruft man ein Programm auf, welches für eine andere CPU erstellt wurde, erkennt dies das Betriebssystem und wandelt das Programm automatisch um. Auf diese Weise wurde auf der iSeries der Wechsel von proprietären 48-Bit-CISC-Prozessoren auf 64-Bit-RISC-Prozessoren vollzogen. Die Anwender haben auf dem alten System alle Programme auf ein Band gesichert und auf dem neuen System zurückgeladen. Den Rest erledigte das Betriebssystem.\n\nDieser hardwareunabhängige Objektcode kann aus dem Programmobjekt entfernt werden, um Speicherplatz zu sparen. Dann muss aber das Programm unter Verwendung des Quellcodes neu umgewandelt werden, wenn es eine neue Hardware-Architektur gibt.\n\nDiese Möglichkeit der hardwareunabhängigen Software ist in der IT-Branche recht einzigartig, da hier nicht ein menschenlesbarer Quelltext mit der (oft ja kommerziell vertriebenen) Software mitgegeben werden muss, obwohl dies im AS/400-Umfeld traditionell häufig vorkommt.\n\nMit der klassischen AS/400-Programmierung soll allgemein folgendes ausgedrückt werden:\n\nAls Datenbank wird die DB2 benutzt, welche im OS/400 fest verankert ist. Diese Datenbank kann man über die spezielle Datenbankbeschreibungsprache DDS oder über normierte SQL-Befehle verwalten und administrieren.\n\nUm das OS/400 zu steuern, wird die Steuersprache CL benutzt, die wie eine Shell-Sprache funktioniert, aber wesentlich komfortabler zu benutzen ist, da es bildschirmgestütztes Prompting gibt. Die CL-Befehle können zu einer Art Script zusammengefasst werden, welche hier als CL-Programme bezeichnet werden und die compiliert werden müssen. Eine Interpretation von CL-Programmen ist nicht vorgesehen.\n\nDie eigentlichen „Greenscreen“-Applikationen oder zeichenorientierten Applikationen wurden in der großen Masse in RPG geschrieben. Auch COBOL spielte eine Rolle. Historisch betrachtet ist es bis heute möglich, alle RPG-Versionen (RPG-II, RPG-III, RPG/400 etc.) zu verwenden. Der in der jetzt aktuellen Version V7R3 verfügbare RPG-Compiler kann auch noch die „alte“ Syntax (Tabulator-orientiert, also Lochkarten-ähnlich) verarbeiten und lauffähige Programme/Module erstellen. Zum Teil laufen noch Anwendungen, die vor 15 Jahren entwickelt wurden, und Anwendungen der neuesten Art parallel.\n\nAndere Programmiersprachen wie z. B. COBOL, C, C++, oder Java sind ebenfalls verfügbar und können hier compiliert werden. Darüber hinaus sind Interpretersprachen wie Net.Data, Rexx, als auch Perl und PHP benutzbar. Innerhalb des OS/400-Systems wird mit dem Befehl STRQSH eine Shell gestartet, in der sh-Scripte ausgeführt werden können, die AIX-kompatibel sind. Dadurch können AIX-Anwendungen parallel zu klassischen Programmen in derselben Partition ausgeführt werden.\n\n\n\n\n\nDie iSeries Systeme verwenden heutzutage IBM Power-Prozessoren. Dieser RISC-Mikroprozessor hat eine Verarbeitungsbreite von 64 Bit und wird von IBM selbst entwickelt und hergestellt. Die Prozessormodelle ab sStar und Power 5/6 sind hyperthreadfähig, das heißt, sie können simultan mehrere Threads abarbeiten - bis zu 8 pro Kern bei Power 8. Die Power-Prozessoren ab Modell 4 enthalten zwei Prozessorkerne im Core. Sie sind darüber hinaus auch als Multichip-Module (MCM) erhältlich und haben hier vier Power-Prozessoren (acht Cores) bzw. acht Power-Prozessoren (16 Cores) auf einem MCM.\n\nDie IBM hat ihre Maschinen immer in Modellen gruppiert, die jedes Jahr leistungsstärker wurden. Die Modellbezeichnung änderte sich im Laufe der Jahre von Anfangsbuchstaben in reine Zahlen. Die Prozessorgruppe ist hier mit aufgeführt, weil sie Aufschluss über die Kosten des Betriebssystems und der Lizenzprogramme gibt. Je höher die Zahl, desto teurer auch das Betriebssystem. Der CPW-Wert ist die Maßeinheit der Leistungsfähigkeit einer AS/400. Je höher die Zahl, desto schneller und leistungsfähiger ist der Rechner. Die Systemversion (“release”) ist eine Mindestanforderung auf der entsprechenden Hardware, neuere funktionieren oft bis zu einem gewissen Grad.\n\nDie POWER-basierten Plattformen des IBM System i und System p sind seit den Ankündigungen von i5 und p5 physikalisch nahezu baugleich. Da ein IBM-System i Server bei großen Modellen immer nach Kundenwunsch konfiguriert wird, sind die verbauten Teile in dem fertigen Computer sehr selten identisch zu einer anderen Konfiguration. Den Unterschied machen die charakteristischen Eigenschaften des gewählten Betriebssystems i5/OS, AIX oder Linux aus, denn man benötigt für die verschiedene Betriebssysteme verschiedene Features, wie zum Beispiel Plattenkontroller, Netzwerkadapter, CPU-Gehäuse.\n\nGenerell kann man die Hardware kaufen. Ein Leasing über die IBM sowie über andere Leasinggesellschaften ist möglich und wird in der Praxis sehr oft in Anspruch genommen.\n\nEine Besonderheit ist das Verfahren “Capacity on Demand”, bei dem ein Server in vollständigem Hardware-Ausbau geliefert wird, aber nur ein Teil der Leistung (CPU und RAM) zur Verfügung stehen. Wird die zusätzliche Leistung später vorübergehend oder dauerhaft gebraucht, kann das durch zusätzliche Lizenzcodes freigeschaltet werden. Die Aktivierung geschieht dabei im laufenden Betrieb, ohne dass Umbauten an der Hardware oder gar ein Abschalten des Servers notwendig werden.\n\nNach der Garantiezeit, oder auch schon während der Garantiezeit, benötigt man bei einem produktiv eingesetzten System einen Wartungsvertrag, der mit dem Hersteller oder mit einem anderen Anbieter abgeschlossen werden kann. Darin wird in der Regel eine Reaktionszeit und eine Bereitschaftszeit (zum Beispiel 24 × 7) vereinbart.\n\nUm eine in Produktion befindliche Maschine mit neuen Software Fixes (Program Temporary Fixes, kurz PTFs) zu versorgen, benötigt man einen Softwarewartungsvertrag. Dieser deckt sowohl Softwareprobleme als auch neue Versionen ab.\n\n\n"}
{"id": "1227666", "url": "https://de.wikipedia.org/wiki?curid=1227666", "title": "Functional Grammar", "text": "Functional Grammar\n\nDie funktionale Grammatik oder engl. functional grammar (FG) ist eine linguistische Theorie, die Ende der 1970er Jahre von Simon C. Dik in Amsterdam entwickelt wurde, ausdrücklich als Gegenmodell zum Standardmodell der Transformationsgrammatik von Noam Chomsky. Nach dem Tod Diks 1995 wurde die Theorie vor allem durch seinen Mitarbeiter Kees Hengeveld fortentwickelt und ist in ihrer heutigen Form der ursprünglichen Formulierung noch sehr nah. Seit 2004 wird die Theorie unter der Bezeichnung Functional Discourse Grammar (FDG) vor allem durch Kees Hengeveld und Lachlan Mackenzie weiterentwickelt (Hengeveld & Mackenzie 2008).\n\nDie zentrale Annahme Diks über Sprache ist ihr zweckgebundener Charakter als Mittel zur Kommunikation. Dik rückt damit die Funktion der Sprache in den Mittelpunkt. In diesem Sinne ist die Bezeichnung \"Functional Grammar\" zu sehen: Ein sprachliches Modell, das von der Funktion der Sprache statt von ihrer äußeren Form ausgeht. Mit dieser zentralen Annahme fordert Dik eine Abkehr von der früher häufig angewandten heuristischen Reduktion der Ausblendung der Pragmatik. Konkret ist jedoch in der Behandlung der Pragmatik bei Dik nicht die allgemeine Pragmatik im Sinne von Sprechakten und Sprache als Handlung gemeint, sondern der Bereich der Diskurspragmatik, im Wesentlichen also das Verhältnis der Informationsstruktur eines sprachlichen Ausdrucks zu seiner Realisierung, etwa bei der Behandlung von Topik und Fokus (Dik 1991:267ff.).\n\nDer von Dik beschriebene Grammatikformalismus ist in diesem Sinne pragmatikbasiert. Die nächst wichtigste sprachliche Ebene ist aus Diks Sicht die Semantik, die, selbst von der Pragmatik beeinflusst, ihrerseits Einfluss auf die Syntax hat. Ein Beispiel für eine solche Beeinflussung der Syntax wäre etwa eine Aktiv-Passiv-Alternation, die von den semantischen Rollen der Mitspieler in der Äußerung bestimmt wird und in diesem Sinne semantisch motiviert ist.\n\nZugleich ist FG ein formales Modell, da es Methoden der formalen Semantik (etwa Prädikatenlogik) verwendet und den Anspruch der Implementierbarkeit als Computerprogramm und damit der Testbarkeit erhebt.\n\nAls Grammatiktheorie ist die funktionale Grammatik eine allgemeine Theorie über die grammatische Organisation natürlicher Sprachen. Ihre wichtigsten Kennzeichen sind:\n\nDer Grammatikformalismus der FG besteht im Wesentlichen aus der Beschreibung abstrakter Ausdrücke, der \"Underlying Clause Structures\" (UCS), die schrittweise aus Prädikaten und Termen gebildet werden und die durch Ausdrucksregeln zu konkreten sprachlichen Äußerungen in Bezug gesetzt werden oder diese erzeugen.\n\nDie UCS werden aus Prädikaten und Termen gebildet. Einige elementare Prädikate und Terme sind Teil des Lexikons, andere werden aus diesen elementaren Prädikaten und Termen erstellt (durch \"predicate formation\" und \"term formation\"). So wäre das Prädikat für \"throw back\" ein aus den elementaren Prädikaten für \"throw\" und \"back\" abgeleitetes Prädikat. Alle Prädikate und Terme zusammen bilden den Fundus (\"fund\") einer Sprache.\n\nPrädikate sind Ausdrücke für Eigenschaften oder Relationen. Es handelt sich hier um Prädikate im Sinne der Prädikatenlogik, nicht um die grammatische Relation des Prädikates aus der lateinischen Schulgrammatik. In diesem Sinne sind nicht nur Verben Prädikate, sondern alle Inhaltswörter einer Sprache. So ist \"haus(x)\" etwa ebenso ein Prädikat wie \"schlagen(x,y)\".\n\nEin Unterschied der Prädikate in der FG zur klassischen Prädikatenlogik ist die Verwendung sogenannter Restriktoren. Wenn in der FG Prädikate zusammengesetzt werden, geschieht dies durch die Verwendung dieser Restriktoren, geschrieben als \":\", etwa in der Form \"japanisch(x):buddhistisch(x)\". Dies lässt sich paraphrasieren mit \"Die Menge der x, für die gilt: x ist japanisch, eingeschränkt auf die Menge der x, für die gilt: x ist buddhistisch\". Die entsprechende prädikatenlogische Form wäre \"japanisch(x) & buddhistisch(x)\", wobei das \"&\" ein prädikatenlogisches \"UND\" ist. Der entsprechende Sachverhalt ist in beiden Fällen gleich. Der Unterschied besteht darin, dass das prädikatenlogische \"&\" umkehrbar ist, dass also \"japanisch(x) & buddhistisch(x)\" äquivalent ist zu \"buddhistisch(x) & japanisch(x)\". Bei den Restriktoren ist dies nicht der Fall und sie sind damit in der Lage, den Unterschied der sprachlichen Äußerungen \"Der japanische Buddhist\" und \"Der buddhistische Japaner\" zu erfassen (Dik 1997, Kap. 6.2).\n\nPrädikate sind stets Teil eines Prädikatrahmens, der die Eigenschaften des Prädikats beschreibt. Ein Beispiel für den Prädikatrahmen eines transitiven Verbs wäre etwa:\n\nZunächst erscheint die Wortform (throw), anschließend die Wortart (V). Im Folgenden sind die Argumentpositionen des Verbs beschrieben. Das Argument in der Mitspielerposition mit der semantischen Rolle des Agens muss belebt (animate) sein, der vom Sachverhalt betroffene Mitspieler (\"Goal\", allgemein hat sich für die Rolle, die Dik \"Goal\" nennt, die Bezeichnung \"Patient\" bzw. Patiens durchgesetzt), hier der geworfene Gegenstand, muss konkret (concrete) sein und das dritte Argument (mit der semantischen Rolle \"Location\") unterliegt keiner solchen Selektionsbeschränkung.\n\nZu solchen nuklearen Prädikaten können nun die fakultativen sogenannten Satelliten hinzukommen, die Positionen einnehmen, die nicht vom Prädikatrahmen spezifiziert werden, etwa zu einer zeitlichen Präzisierung des Prädikats mit Hilfe von Wörtern wie \"gestern\" oder \"bald\". Einen solchen um Satelliten erweiterten Prädikatrahmen nennt Dik einen erweiterten Prädikatrahmen (\"extended predicate frame\").\n\nDer zweite wesentliche Bestandteil einer Underlying Clause Structure (UCS) sind neben Prädikaten die Terme. Formal sind Terme die Argumente der Prädikate, semantisch sind es Ausdrücke, die Entitäten referenzieren (Genau genommen schreibt Dik (1991:255), dass Terme den Adressaten instruieren, eine Entität zu identifizieren, die dem Profil des Terms entspricht). Beispiele für Terme wären \"Das Haus\" oder \"Die lila Plastiktüte\". Es existieren nur sehr wenige elementare Terme, so sind lediglich Eigennamen und Personalpronomina als elementare Terme vorhanden, andere Terme, wie \"Die lila Plastiktüte\" werden aus Prädikaten erstellt. Terme sind also die Entitäten, die durch ein Prädikat zueinander in Beziehung gesetzt werden.\n\nEine Prädikation, die zwei Terme (\"the garden\" und \"the dog\") enthält, wäre z. B.:\n\nInnerhalb der Underlying Clause Structure (UCS) werden in Form von Funktionen drei verschiedenen Ebenen unterschieden:\n\n\nIn diesem Sinne nehmen einzelne Elemente einer sprachlichen Äußerung auf den verschiedenen Ebenen verschiedene Kategorien an. In dem Satz \"Peter kauft ein Eis\" etwa ist \"Peter\" zugleich Agens, Topik und Subjekt, während \"Eis\" zugleich Goal (Patiens), Fokus und Objekt ist.\n\nDer sprachliche Ausdruck, der durch die Ausdrucksregeln auf die UCS\n\nbezogen (oder in einer Implementierung des Formalismus auch aus der UCS erzeugt) werden kann, ist so noch nicht eindeutig. Der UCS entspricht etwa die Äußerung \"The dog is in the garden\". In einer bestimmten Äußerungssituation (etwa in einer Aufzählung der für einen Einbruch zu überwindenden Hindernisse) wäre aber folgende Äußerung denkbar, die ebenfalls mit der UCS übereinstimmt: \"There is the dog in the garden\". Dieses Beispiel verdeutlicht die Möglichkeiten, die eine Berücksichtigung der pragmatischen Ebene bietet, denn durch die Kennzeichnung des Aufzählungscharakters ist es möglich, die beiden sprachlichen Äußerungen in der zugrunde liegenden Struktur zu unterscheiden (Dik 1997, Kap. 8.7.2).\n\nDie Unterscheidung der Ebene der semantischen Rollen, d. h. der Mitspieler und der syntaktischen (grammatischen) Relationen ermöglicht die Beschreibung syntaktischer Alternationen wie der Passivierung ohne dass dabei die eine Form aus der anderen abgeleitet werden müsste. So gibt es in einem Aktivsatz eine Übereinstimmung von Subjekt und Agens, während bei einer Übereinstimmung von Subjekt und Goal (Patiens) in der UCS dieser ein Passivsatz entspräche.\n\nWenn wie beschrieben die Terme in die Prädikatrahmen eingesetzt wurden, haben wir eine Prädikation, die die vollständige Proposition oder den Sachverhalt (\"State of Affair\", SoA) des Satzes enthält, jedoch noch nicht weiter spezifiziert ist. Dazu werden nun Operatoren auf die gesamte Prädikation angewandt, etwa in der Underlying Clause Structure (UCS) oben der Operator \"present\", der selbst wieder als ein Prädikat mit der vollen Prädikation als Argument gesehen werden kann. Ebenso werden in diesem Schritt Operatoren zum Modus (etwa Interrogativ oder Deklarativ) eingefügt.\n\nDiese nun voll spezifizierte Prädikation wird schließlich mit Hilfe von Ausdrucksregeln zu Form, Reihenfolge und Intonation spezifiziert und damit zu einer konkreten sprachlichen Äußerung in Bezug gesetzt (bei der Beschreibung) bzw. in eine solche umgewandelt (bei der Generierung).\n\nEs handelt sich damit bei FG um ein monostratales Modell, da zwar zwischen den Underlying Clause Structures (UCS) und den sprachlichen Ausdrücken unterschieden wird und diese durch Ausdrucksregeln aufeinander bezogen werden, jedoch werden keine verschiedenen Ebenen angenommen, auf denen konkrete sprachliche Äußerungen stehen, so sind etwa keine syntaktischen Derivationsmechanismen vorhanden. In diesem Sinne findet die Bildung der sprachlichen Äußerungen schrittweise innerhalb einer Prozesskette, auf einer einzigen Ebene statt.\n\nDie Pragmatikorientiertheit macht FG zu einem deszendenten Grammatikmodell, das von der Gesamtsituation ausgeht, in der eine Äußerung getätigt wird, im Gegensatz zu einem aszendenten Grammatikmodell, das von den kleinsten Teilen ausgeht, etwa von der Phonologie über die Morphologie zur Syntax.\n\nDie Bedeutung von sprachlichen Daten setzt Dik im Allgemeinen sehr hoch an: \"Whenever there is some overt difference between two constructions X and Y, start out on the assumption that this difference has some kind of functionality in the linguistic system\" (Dik 1997, Kap. 1.6).\n\nDamit hat FG einen induktiven Charakter, da sie ähnlich dem Bloomfieldschen Deskriptivismus von konkreten sprachlichen Daten ausgeht, im Gegensatz zu einem deduktiven Modell wie der Generativen Grammatik nach Chomsky, wo eine ideale, vom konkreten Sprachgebrauch abstrahierte Sprachkompetenz im Mittelpunkt der Theorie steht.\n\nIn den zentralen Bereichen Pragmatik und Semantik ist die FG vor allem auf die Befragung von Informanten (Elizitierung) sowie die Konsultation der eigenen muttersprachlichen Einsichten (Introspektion) angewiesen. Andere Quellen wie Experimente oder Korpora sind nicht ohne weiteres (eine Generierung von semantischem Wissen wäre eventuell durch eine automatische Verarbeitung von Korpora, etwa zur Ermittlung paradigmatischer oder syntagmatischer Relationen möglich) zur Ermittlung semantischen Wissens (etwa für die Selektionsbeschränkungen in Prädikatrahmen) verwendbar.\n\nZur Evaluierung des Gesamtmodells können dagegen auch in der FG Korpora und damit spontansprachliche Daten verwendet werden, etwa zur Überprüfung, ob Äußerungen in Korpora durch den FG-Formalismus beschrieben werden können.\n\nDie Zielsetzung der FG ist sehr umfassend, Dik (1997, Kap. 1) formuliert folgende zentrale Frage: \"How does the natural language user (NLU) work?\". Diese Fragestellung kennzeichnet FG klar als Modell mit mentalistischem Anspruch.\n\nDik identifiziert im Anschluss an die Formulierung dieser zentralen Fragestellung fünf Fähigkeiten des NLU, die essentielle Rollen für die menschliche Kommunikation spielen:\n\n\nDarüber hinaus formuliert Dik in Anspielung auf die von Chomsky geforderten drei Adäquatheitskriterien der Beschreibungs-, Erklärungs- und Beobachtungsadäquatheit drei eigene, völlig andere Adäquatheitskriterien:\n\n\nInsbesondere durch den Anspruch der typologischen Offenheit erhält das Modell einen stark beschreibungsorientierten Charakter, da es eine solche Offenheit zu einem universellen Beschreibungswerkzeug machen würde, sowie einen universalistischen Anspruch, der es als Ziel sieht, allgemeingültige Aussagen über Sprache insgesamt, nicht über eine bestimmte Sprache oder Sprachfamilie zu machen.\n\nDer Anspruch der psychologischen Adäquatheit kennzeichnet FG, wie schon im Zusammenhang mit der zentralen Fragestellung erwähnt, als ein mentalistisches Modell, das wie etwa die generative Syntaxtheorie ein Modell für die menschliche Sprachfähigkeit sein will, im Gegensatz zu rein anwendungs- bzw. beschreibungsorientierten Ansätzen wie HPSG.\n\nFG geht im Gegensatz zur nativistischen Hypothese Chomskys davon aus, dass sprachliche Universale nicht angeborenen Eigenschaften entspringen, sondern den Notwendigkeiten der menschlichen Kommunikation (in diesem Sinne wäre etwa die Tatsache, dass alle Sprachen eine Unterscheidung zwischen Funktions- und Inhaltswörtern haben, in der Notwendigkeit begründet, die Inhalte einer sprachlichen Äußerung zueinander in Bezug zu setzen) sowie der physischen und psychischen Konstitution des Menschen (etwa eine Einschränkung der Schachtelungstiefe von Nebensätzen durch die begrenzten Möglichkeiten des menschlichen Kurzzeitgedächtnisses), und kann damit als nicht-nativistisches Modell charakterisiert werden.\n\nZiel der Forschung im Rahmen der FG ist die Entwicklung eines sprachunabhängigen Formalismus zur Sprachbeschreibung. Dazu ist eine ausgiebige Anpassung der bestehenden Formalismen an viele verschiedene Sprachen nötig (Dik 1991:248). In diesem Sinne ist die Sprachbeschreibung ein zentraler Gegenstand der FG-Forschung.\n\nAus dem Anspruch der Formalisierbarkeit ergibt sich ein weiteres Forschungsgebiet: Die Implementierung von FG auf einem Computer. Dik selbst hat seit den 1980er Jahren vor allem auf diesem Gebiet gearbeitet. Diks eigene und auch andere Implementierungen (etwa Samuelsdorff 1989) verwenden die logikorientierte Programmiersprache Prolog (\"Programming in Logic\"), die sich aufgrund ihrer starken Orientierung an Prädikatenlogik besonders zu eignen schien, eine Umsetzung ist jedoch auch in jeder anderen Programmiersprache möglich. Die Arbeiten in diesem Bereich konzentrieren sich stark auf das Gebiet der Generierung und abstrakten Darstellung von sprachlichen Ausdrücken, nicht auf die Verarbeitung (Parsing), die ebenso wie die Generierung Teil der \"linguistic capacity\" ist.\n\nDarüber hinaus legen die Forderungen nach pragmatischer und psychologischer Adäquatheit eine gewisse Offenheit und interdisziplinäre Zusammenarbeit nahe, wenn Erkenntnisse relevanter Fächer wie Psychologie und Soziologie berücksichtigt werden sollen.\n\nDiks \"Functional Grammar\" scheint viele in anderen Modellen vernachlässigte, jedoch zur vollständigen Sprachbeschreibung wichtige Aspekte von Sprache zu berücksichtigen:\n\n\nDie Verwendung merkmalsemantischer Primitive zur Selektionsbeschränkung (siehe Semantische Relation) bestimmter Argumentpositionen in den Underlying Clause Structures, (UCS) könnte in der Praxis zu den mit diesem semantischen Modell bekannten Problemen führen, etwa bei relationalen Eigenschaften wie Verwandtschaftsverhältnissen, sowie bei Verben, graduellen Unterschieden und Farben. Die Kodierung der Feinsemantik im Lexikon stellt jedoch allgemein ein ungelöstes Problem dar.\n\nDas Vorgehen, Verletzungen der Selektionsbeschränkungen nicht als ungrammatisch zu bezeichnen, sondern z. B. als Metapher zu behandeln, stellt eventuell eine Immunisierungsstrategie dar, etwa wenn keine spezielle Interpretationsstrategie ausgearbeitet wurde, die in diesem Fall Testbarkeit, Anwendbarkeit und den wissenschaftlichen Wert des Modells verringern könnte.\n\nAuch die sich aus der semantischen Ausrichtung ergebende Konzentration auf Introspektion und Elizitierung zur Datengewinnung für die Bestimmung von Selektionsbeschränkungen von Argumentpositionen in Prädikatrahmen könnte Probleme verursachen und den wissenschaftstheoretischen Wert der gewonnenen Daten schmälern, da elizitierte und aus Introspektion gewonnene Daten durch die vorgegebene Fragestellung leicht missinterpretiert werden können, etwa wenn Einflussfaktoren, die über die Fragestellung hinausgehen, nicht berücksichtigt werden.\n\nAuch im Bereich der Operatoren zur zeitlichen Spezifizierung der Prädikation gehen der universelle Anspruch und die praktischen Erfordernisse auseinander, denn die auf dieser Ebene von Dik genannten Operatoren wie „present“ und „progressive“ sind keine universellen Kategorien, doch die Underlying Clause Structures, (UCS) hat den Anspruch, vor Anwendung der Ausdrucksregeln sprachunabhängig kodiert zu sein.\n\nZusammenfassend lässt sich Simon C. Diks „Functional Grammar“ somit als ein\n\n\n\n"}
{"id": "1228481", "url": "https://de.wikipedia.org/wiki?curid=1228481", "title": "Vektormodell", "text": "Vektormodell\n\nVektormodelle bauen auf Punkte und Linien auf. Ein geschlossener Linienzug stellt dabei eine Fläche dar. Vektormodelle bezeichnet man auch als lineale Modelle, während Rasterdaten ein areales Modell darstellen. Im 3-dimensionalen Raum spricht man auch vom „Drahtmodell“ (Vektor) versus „Volumenmodell“ (Raster).\n\nVektormodelle basieren auf der Darstellung linienhafter Strukturen durch eine geordnete Folge von Punkten formula_1 die einen Polygonzug darstellen. Die Distanz zwischen zwei Objekten entspricht dabei dem kürzesten Abstand zwischen zwei Punkten. Diese Vektordaten eignen sich insbesondere für die Darstellung von einfachen, linienbasierten Objekten wie Straßen und Flüssen. Im Rastermodell werden regelmäßige Maschen (Flächen, Raster) verwendet.\n\nBeim Vektormodell wird zwischen topologischen und nichttopologischen Datenstrukturen unterschieden. Letztere Datenstrukturen bilden nur die Lage und Form des Objektes ab (Geometriedaten). Topologische Vektordaten bieten zusätzlich Informationen über die räumlichen Beziehungen der Objekte zueinander. Ein topologisches Datenmodell ist das „arc-node data model“. Hierbei werden einzelne Objekte durch Linienstücken (Segmente, arcs) erfasst. Der Ort an dem zwei oder mehr Segmente zusammentreffen wird als Knoten (node) bezeichnet.\n\n\n\n"}
{"id": "1231896", "url": "https://de.wikipedia.org/wiki?curid=1231896", "title": "SHOUTcast", "text": "SHOUTcast\n\nSHOUTcast ist ein Freeware-Streaming-Server für Audiostreams/TV Streams welcher 1999 von der Firma Nullsoft entwickelt und 2014 von der Firma Radionomy abgekauft wurde. Shoutcast benutzt MP3 für die Audiodatenkompression, AAC+ für Audiodateien sowie HTTP als Transportprotokoll zum Kontakt des Streams. Ermöglicht wird dadurch der Betrieb eines Internetradios unter den Betriebssystemen Windows, FreeBSD, Linux, macOS und Solaris.\n\nDie Software, die in der Basisversion als Freeware erhältlich ist, erlaubt es auch unerfahrenen Benutzern, unproblematisch einen Server für Internetradios einzurichten. Der ausgehende Audiostream wird von vielen weitverbreiteten Clients unterstützt. Einige andere Mediaplayer erlauben mit Hilfe eines Plugins die Wiedergabe eines SHOUTcast-Streams.\n\nEin SHOUTcast-Stream kann, da er über HTTP läuft, direkt über einen Webbrowser aufgerufen werden („HTTP GET“-Befehl). Die Antwort des Servers enthält im Header Informationen über den Stream und erlaubt das Abspielen in einem geeigneten Mediaplayer. Des Weiteren besteht die Möglichkeit, bei wechselnden gestreamten Audiodateien, z. B. während des Radiobetriebs, den Zuhörern den Titel des aktuellen Stücks bekanntzugeben.\n\nIm Internetradio-Verzeichnis von SHOUTcast sind strukturiert mehrere tausend verschiedene Webradio-Stationen diverser Stilrichtungen eingetragen.\n\nIm Medienplayer Winamp gibt es daneben ab der Version 5.1 die Funktion „Shoutcast Wire“. Damit wurde ein Podcatcher integriert mit dem es möglich ist, Podcasts zu abonnieren, anzuhören und zu verwalten. Dieser Dienst hat mit dem eigentlichen Shoutcast relativ wenig zu tun, da Podcasts in der Regel nicht live gesendet werden und somit nicht auf Streaming-Server angewiesen sind.\n\n\"(Auswahl)\"\n\n\"Mit Erweiterung:\"\n\n\n\n"}
{"id": "1239379", "url": "https://de.wikipedia.org/wiki?curid=1239379", "title": "FM Towns", "text": "FM Towns\n\nDer FM Towns (\"Fujitsu Micro\") ist ein zwischen 1989 und 1997 in Japan verkauftes Computersystem.\n\nEs wurde von Fujitsu gebaut und orientierte sich an der Hardware der damals gängigen PCs, ohne jedoch zu diesen kompatibel zu sein. Obwohl das Gerät für die Zeit um 1990 eine sehr gute technische Ausstattung hatte, unter anderem bereits ein CD-ROM-Laufwerk, lange bevor sich dies im restlichen PC-Markt durchsetzen sollte, wurde es nie über Japan hinaus bekannt und ist heute eine extreme Rarität. Die Bedeutung des Gerätes zeigt sich aber daran, dass viele Computerspiele der frühen 90er auf das System portiert wurden – für gewöhnlich in höherer Qualität (z. B. mehr Farben, besserer Ton).\n\nDer Name „FM Towns“ stammt noch vom verwendeten Entwicklungs-Codenamen: „Townes“. Dieser war eine Hommage an den Nobelpreis-Gewinner von 1964 in Physik, Charles H. Townes. Damit folgte Fujitsu der eigenen Tradition, PC-Produkte nach Nobelpreis-Gewinnern zu benennen. Als das System in die Produktion ging, wurde das „e“ in „Townes“ fallen gelassen, um eine Aussprache als „Towns“ anstelle von „Tau-Ness“ zu unterstreichen; das „FM“ stand für „Fujitsu Micro[computer]“.\n\n"}
{"id": "1239692", "url": "https://de.wikipedia.org/wiki?curid=1239692", "title": "Starship Troopers (Serie)", "text": "Starship Troopers (Serie)\n\nStarship Troopers (Alternativtitel: Starship Troopers Chronicles, Originaltitel: Roughnecks: The Starship Troopers Chronicles) ist eine US-amerikanische Animationsserie von 1999, basierend auf dem Roman \"Starship Troopers\" von Robert A. Heinlein. Produzenten waren Paul Verhoeven sowie Richard Raynis.\n\nDie Serie greift viele Elemente von Paul Verhoevens Film Starship Troopers auf, beinhaltet aber wesentlich mehr Elemente aus dem Roman. Die kontroverse Politik aus dem Buch wird aber nicht behandelt.\n\nBei der Produktion von Adelaide Productions führte Andre Clavel Regie. 40 Folgen waren für die Serie geplant. Es wurde mit drei Teams gearbeitet, die je drei Wochen Zeit hatten, um eine Folge fertigzustellen. Als die Produktion jedoch in Verzug kam, wurden nur 36 Folgen produziert. Die Serie wurde nie abgeschlossen und endet mitten im Handlungsbogen. \n\nDie Erstausstrahlung erfolgte ab dem 30. August 1999 durch The Sci-Fi-Channel in den USA. Jede Woche wurden fünf Folgen ausgestrahlt. In Deutschland lief die Serie zunächst auf Premiere und wurde von 2001 bis 2003 auf sieben DVDs veröffentlicht. Die DVDs sind nach den jeweiligen Handlungsbögen geordnet.\n\n"}
{"id": "1240511", "url": "https://de.wikipedia.org/wiki?curid=1240511", "title": "UClibc", "text": "UClibc\n\nuClibc (auch µClibc) ist eine für Linux-Embedded-Systeme konzipierte, kleine C-Standard-Bibliothek. Sie ist freie Software, lizenziert unter der GNU Lesser General Public License (LGPL).\n\nDie Standardbibliothek uClibc wurde ursprünglich erstellt um μClinux zu unterstützen, eine portierte Version des Linux-Kernels für Prozessoren ohne Speicherverwaltungseinheit (MMU). Deshalb eignet diese sich gut für Mikrocontroller, woher das „µC“ im Namen stammt.\n\nuClibc ist viel kleiner als die GNU-C-Bibliothek (glibc), welche die C-Standard-Bibliothek der meisten Linux-Distributionen für klassische Computer ist. Während glibc entwickelt wurde, um alle relevanten C-Standards auf einer möglichst breiten Auswahl an Plattformen voll zu unterstützen, ist uClibc auf Embedded-Linux-Systeme spezialisiert. Funktionen können aktiviert oder deaktiviert werden, je nachdem wie viel Speicherplatz zur Verfügung steht.\n\nuClibc läuft auf Standard- und auf MMU-losen Linux-Systemen. Es unterstützt Alpha, i386, i960, x86-64, ARM (Big/Little Endian), AVR32, Blackfin, cris, Renesas H8 (h8300), HP PA-RISC, Motorola m68k, MIPS (Big/Little Endian), IBM PowerPC, SuperH (Big/Little Endian), Sun SPARC, Altera Nios und v850 Prozessoren.\n\nDas Build-System Buildroot, ein Teilprojekt von uClibc, besteht aus einer Sammlung von Makefiles und Patches, welche uClibc mit einem kleinen Linux-System und einer Cross-Compile-Werkzeugkette erstellen.\n\nDas Projekt wird beziehungsweise wurde von Erik Andersen geleitet. Sein aktivster Helfer ist Manuel Novoa III. Eine aktive Weiterentwicklung der Bibliothek findet seit 2015 in dem davon abgespalteten Nachfolgeprojekt uClibc-ng statt. Auf dieses Projekt verweist der Originalautor, welcher nach 2012 selbst keine neuen Ausgaben mehr veröffentlichte.\n\n\n"}
{"id": "1242586", "url": "https://de.wikipedia.org/wiki?curid=1242586", "title": "Enterprise (Heimcomputer)", "text": "Enterprise (Heimcomputer)\n\nDer Enterprise war ein Zilog Z80 basierter Heimcomputer, der erstmals im Jahre 1985 erschien. Es wurden zwei Varianten produziert: der \"Enterprise 64\" mit 64 KByte RAM und der \"Enterprise 128\" mit 128 KByte. In dem Gerät gab es zwei Koprozessoren für den Sound (Audiochip \"Dave\") und die Grafik (Videochip \"Nick\"). Beide Chips wurden nach deren Designern Nick Toop und Dave Woodfield benannt.\n\nDer Rechner hatte einen Z80 CPU mit 4 MHz, 64 KByte oder 128 KByte RAM und 48 KByte ROM, in dem das EXOS Betriebssystem und die Programmiersprache BASIC untergebracht waren. Im ROM war zusätzlich ein Textverarbeitungsprogramm eingebaut. Das Gehäuse des Rechners war auf seine Weise einzigartig, da dieses sowohl die Tastatur mit programmierbaren Funktionstasten als auch einen kleinen Joystick enthielt.\n\nIn Deutschland wurde der Rechner unter dem Namen \"Mephisto PHC 64\" von der Firma Hegener + Glaser ab 1985 vertrieben, die zuvor im deutschen Markt schon erfolgreich elektronische Schachcomputer vertrieben hatte. Das Gerät wurde im Mai 1985 zu einem Preis von 1.198,00 DM (inkl. MwSt.) verkauft.\n\nIn Ungarn hatte die Firma Videoton ab dem Jahr 1984 eine Lizenz und erstellte eine auf die nationalen Umstände angepasste Version des Geräts, die ab 1988 unter dem Namen TV Computer für ca. drei Jahre produziert wurde. Die Serie umfasste zwei Grundmodelle und eine überarbeitete Version, kam jedoch insgesamt gerade so in den 5-stelligen Stückzahlenbereich.\n\n\n"}
{"id": "1245899", "url": "https://de.wikipedia.org/wiki?curid=1245899", "title": "Vendor Independent Messaging", "text": "Vendor Independent Messaging\n\nVendor Independent Messaging (VIM) ist eine Programmierschnittstelle zum Versenden von E-Mails aus Windows-Anwendungen heraus.\n\nSie wurde Anfang der 1990er Jahre von Lotus Software, Borland, IBM und Novell für Windows 3.1 entwickelt. VIM unterlag in der Marktdurchsetzung der von Microsoft entwickelten Schnittstelle \"MAPI\" und wird nicht mehr weiterentwickelt.\n\n"}
{"id": "1247882", "url": "https://de.wikipedia.org/wiki?curid=1247882", "title": "Praat", "text": "Praat\n\nPraat (Niederländisch \"die Rede\", \"das Gesprochene\") ist ein freies Programm für phonetische Analysen auf Signalbasis. Es wird von Paul Boersma und David Weenink am Institute of Phonetic Sciences an der Universität Amsterdam entwickelt. Praat ist bekannt für seine vielseitigen und modernen Analysemethoden und hat sich im Bereich der Sprach- und Kommunikationswissenschaften als Quasistandard etabliert. Praat wird ständig erweitert und verbessert, wobei 10 bis 20 Programmversionen pro Jahr keine Seltenheit sind.\n\nPraat bietet eine große Vielzahl an Optionen, vor allem zur skriptbasierten Bearbeitung, wobei eigene Skripte relativ problemlos in die grafische Oberfläche integriert werden können. Eine einfache Experimenterstellung und Ergebniserfassung sind ebenso möglich wie die vektorbasierte grafische Darstellung von Ergebnissen. Neben einer recht intuitiven und übersichtlichen Benutzbarkeit, der Darstellung von Spektrogrammen und der großen Funktionsvielfalt (erwächst aus dem Open-Source-Gedanken, daher auch die regelmäßigen und zahlreichen Aktualisierungen), spricht für Praat die kostenlose Nutzbarkeit. Nachteile von Praat sind unter anderem, dass nur immer eine Datei auf einmal eingelesen werden kann, lange Soundfiles sind nur eingeschränkt bearbeitbar, Stereofiles ebenso. Es gibt ein übersichtliches Online-Handbuch und eine rege User-Group (siehe Weblinks).\n\nPraat ist für Windows, Mac OS und Linux verfügbar. In Debian-Systemen ist Praat über den Paketmanager zu beziehen.\n\n"}
{"id": "1249968", "url": "https://de.wikipedia.org/wiki?curid=1249968", "title": "Pause (Tastatur)", "text": "Pause (Tastatur)\n\nDie Taste befindet sich auf einer PC-Computertastatur meist rechts neben den Tasten (englisch: ) und (englisch: ) und dient für sich alleine zum Anhalten, in Kombination mit (englisch: ) dagegen zum Abbrechen von Computerprogrammen.\n\nDie Funktion der Tastenkombination + wird durch die meist auf der Taste zu findende zweite Beschriftung (englisch / schweizerisch: ) bezeichnet. Einige wenige Computer haben dafür eine eigene -Taste auf ihrer Tastatur.\n\nBei anderen Computern, vor allem Notebooks mit verkleinerter Tastatur, werden beide Funktionen durch verschiedene Tastenkombinationen, oft mit der -Taste, implementiert, die je nach Rechner variieren können; gewöhnlich sind die Funktionen auf den entsprechenden Tasten aufgedruckt.\n\nAuf IBM-kompatiblen Tastaturen mit PS/2 oder DIN-Schnittstelle ist die -Taste die einzige Taste, die beim Loslassen überhaupt keinen Scancode generiert. Daher kann keine Software auf dem Rechner erkennen, ob die -Taste gedrückt gehalten wird oder nicht. Beim Drücken sendet die Taste eine längere Scancode-Folge, die im Wesentlichen dem Drücken und Loslassen der Tastenkombination +, ergänzt um zusätzliche 0xE1-Scancodes, entspricht; dies ist so, weil bei den ältesten PC-Tastaturen, die noch keine eigene -Taste hatten, die Pause-Funktion tatsächlich durch Eingabe von + erreicht werden konnte. Die Kombination + (= ), sendet Scancodes, die der Kombination + ähnlich sind, denn so wurde auf den ältesten Tastaturen die Abbrechen-Funktion erreicht.\n\nDie Pause- bzw. Untbr/Break-Taste ist ein Relikt auf der Computertastatur. In den meisten heute verwendeten PC-Anwendungen kommt der Pause-Taste keine oder nur noch geringe Bedeutung zu, in bestimmten Situationen wird sie aber auch heute noch benötigt.\n\nSo kann mit der -Taste der Start des BIOS unterbrochen und mit einer beliebigen Taste wieder fortgesetzt werden. Zudem können DOS-Batch-Dateien durch Eingabe von + beendet werden. Auch in Programmiersprachen wie z. B. BASIC oder Pascal hat diese Tastenkombination (bzw. die eigenständige -Taste) eine wichtige Bedeutung, da mit ihr getestete Programme zum Beispiel bei Endlosschleifen oder sonstigen Problemen vorzeitig abgebrochen werden können. Außerdem findet sie in vielen Spielen Verwendung, um das Spiel zu unterbrechen oder anzuhalten.\n\nBefindet man sich in einer Kommandozeilenumgebung, wird oft auch die Tastenkombination + als Signal zum Abbrechen erkannt. Unter MS-DOS unterscheidet sich + dadurch von +, dass Letzteres nur von DOS oder der zu unterbrechenden Anwendung selbst überwacht wird, während + direkt vom Tastaturtreiber überwacht wird und daher oft noch funktioniert, wenn + durch einen Anwendungsfehler bereits außer Funktion gesetzt wurde. Bei neueren Betriebssystemen gibt es diesen Unterschied nicht mehr.\n\nUnter Windows ermöglicht die Taste , die Eigenschaften des Arbeitsplatzes (Systemeigenschaften) aufzurufen, wenn sie gleichzeitig mit der -Taste gedrückt wird.\n"}
{"id": "1250239", "url": "https://de.wikipedia.org/wiki?curid=1250239", "title": "Rasterung", "text": "Rasterung\n\nIn der 2D-Computergrafik bezeichnet Rasterung, auch \"Rendern\" oder \"Scankonvertierung\" genannt, die Umwandlung einer Vektor- in eine Rastergrafik.\n\nEs gibt eine Vielzahl von Algorithmen zur Rasterung von grafischen Primitiven, wie Linien, Kreisen und dergleichen mehr, siehe dazu:\n\nEin bekanntes Problem der Rasterung ist der Treppeneffekt. Steht für die zu erzeugende Rastergrafik eine Farbtiefe von mehr als 1 bpp zur Verfügung, so kann dieser Effekt mittels „Kantenglättung“ (Antialiasing) vermindert werden. Dazu gibt es mehrere Methoden, die teils ungewichtet arbeiten, teils einen speziellen Rekonstruktionsfilter verwenden. Bei der Rasterung von Text treten spezielle Probleme auf, die mittels Hinting vermieden werden können.\n\nBei der Rasterung von Primitiven mit einer bestimmten Dicke gibt es, sofern sie nicht bereits vom verwendeten Antialiasing-Algorithmus unterstützt wird, mehrere Möglichkeiten. Bei Polygonen müssen hierbei auch die Verbindungsstellen zwischen den einzelnen Liniensegmenten beachtet werden, siehe hierzu Rasterung von Polygonen.\n\n\nBereits gezeichnete Figuren können selektiv gelöscht werden, indem sie nochmals mit der Hintergrundfarbe gezeichnet werden. Das funktioniert jedoch nicht, wenn sie andere Figuren überschneiden, da hierbei auch unerwünschte Bildteile gelöscht werden können. Eine effiziente Möglichkeit, dies zu vermeiden, sind \"Minimax-\" oder \"Boxing-Tests.\" Hierbei wird zunächst geprüft, ob sich in dem von der zu löschenden Figur aufgespannten Rechteck andere Figuren befinden. Nur wenn dies der Fall ist, muss auf Schnittpunkte getestet und gegebenenfalls der gesamte Bereich neu gezeichnet werden.\n\n"}
{"id": "1252112", "url": "https://de.wikipedia.org/wiki?curid=1252112", "title": "Hauptwerk (Software)", "text": "Hauptwerk (Software)\n\nHauptwerk ist ein Software-Sampler zur Simulation des Klanges einer Pfeifenorgel, benannt nach deren Teilwerk, dem „Hauptwerk“.\n\nDie Ansteuerung der Software erfolgt über MIDI. Eine fotorealistische Umsetzung eines Spieltisches ermöglicht die Bedienung der Register am Bildschirm oder über einen Touchscreen; auch eine Steuerung per MIDI ist möglich. Die Software arbeitet mit Aufnahmen (Samples) echter Orgelpfeifen.\n\nSie wurde bis 2008 von der Firma Crumhorn-Labs Ltd. in Birmingham hergestellt und vertrieben; danach wurde Hauptwerk vom amerikanischen Unternehmen Milan Digital Audio übernommen. Crumhorn Labs war 2006 mit dem Erscheinen der Programmversion 2 von dem Programmierer Martin Dyde gegründet worden. Schon Anfang 2004 war die Version 1.00 herausgekommen. Es gibt eine kostenfreie, im Funktionsumfang und Leistung eingeschränkte Version.\n\nInzwischen sind zahlreiche Samplesets für Hauptwerk erhältlich, darunter bedeutende Instrumente von Arp Schnitger, Gottfried Silbermann, Andreas Silbermann, Wilhelm Sauer, Henry Willis, Jonathan Bätz und Aristide Cavaillé-Coll. Entsprechend vorprogrammierte historische Stimmungen können eingestellt werden. Einige Hersteller von Samplesets unterstützen auch die 4-kanalige-Audioausgabe, ähnlich dem Surround-Effekt. Hier werden die hinteren Raumlautsprecher (rear) mit einem halligeren Signal (wet) gespeist als die vorderen Lautsprecher (front). Neuerdings gibt es sogar Samplesets für 6-kanalige Audioausgabe. Eine solche Installation kann mehr als 64 GB Arbeitsspeicher erfordern.\n\nHauptwerk ahmt verschiedene Eigenschaften einer realen Pfeifenorgel nach. Dies betrifft zum Beispiel Windschwankungen einer Pfeifenorgel, die sich je nach Register, Anzahl und Geschwindigkeit der gespielten Töne auf die Lebendigkeit des Klangs auswirken. Aus patentrechtlichen Gründen war diese wesentliche Programmfunktion in den USA jedoch bis Mai 2015 nicht verfügbar. Die Verwendung von Release-Trigger-Samples, die mit dem Loslassen jeder Taste die Originalakustik des Raumes hinzufügt, erzeugt eine hohe Authentizität. Ab Version 2.21 ist die Verwendung von bis zu drei Release-Trigger-Samples, in direkter Abhängigkeit der vorhergegangenen Tonlänge oder der MIDI-Dynamik von diesen Triggern, möglich. Auch verwendet die Version mehrfache und variierende Multiloops, um eine noch höhere Lebendigkeit des Klanges zu erzeugen.\n\nZur Einrichtung und zum Anschluss an einen MIDI-fähigen Orgelspieltisch sind ab Version 4.0 keine speziellen MIDI-Kenntnisse mehr notwendig.\n\nDie große Verbreitung der Software beruhte auf der Version 1 (\"HW1\") (ohne ASIO), mit der sich selbst auf relativ preiswerten, älteren PC-Konfigurationen eine gute und preiswerte Alternative zu einem Hardwaresampler realisieren ließ. Die Version 1 wird nicht mehr gepflegt und unterstützt. Während in der ersten Version Orgelsets in Form von Organ-Dateien (im ASCII-Format) vom kundigen Anwender erstellt und individualisiert werden konnten, ist dieses ab der Version 2 ohne fundierte XML-Kenntnisse kaum noch möglich.\n\nDie Version 3 benötigte dagegen mehr Hardwareressourcen. Ab ca. 18 Registern wird ein Doppelkernprozessor mit 2,66 GHz und 8 MB L2-Cache empfohlen, um auch große Samplesets mit annehmbarer Latenz spielen zu können. Sample-Sets von zweimanualigen Instrumenten lassen sich ohne Einschränkungen auch noch mit Pentium-IV-Prozessoren der letzten Generation bespielen. Für ältere PC-Systeme gibt es zahlreiche Einstellmöglichkeiten, die den Betrieb auch mit solchen ermöglicht. Hauptwerk 3 bietet die Option, alte Samplesets zu importieren und somit weiterzuverwenden. Bei der Hardwaredimensionierung kommt der Größe des zur Verfügung stehenden Hauptspeichers eine wichtige Bedeutung zu. Hauptwerk 4 macht die Erkennung und Einrichtung von MIDI-Befehlen, wie sie von der Registersteuerung eines MIDI-fähigen Orgelspieltisches gesendet werden, mit einer „Lern-Funktion“ sehr schnell und leicht möglich. In den Niederlanden sind Installationen mit Hauptwerk inzwischen auch schon in zahlreichen Kirchen zu finden.\n\nDie Software bietet – in der Freeware-Edition – eine Reihe voreingestellter Stimmungen, die durch weitere, selbsterstellte Stimmungen ergänzt werden können:\n\nEin weiterer Orgelsampler, vergleichbar der Hauptwerk-Version 1.3, ist \"MyOrgan\". Er ist Open-Source-Software und besitzt eine deutsche Benutzeroberfläche. Seit Juni 2006 wird der Quellcode jedoch nicht mehr gepflegt. Eine weitere, davon abgeleitete und erweiterte Version ist \"GrandOrgue\" für Windows, Linux und OS X.\n\n\"Aeolus\" bietet für Linux eine ähnliche Funktionalität und ist ebenfalls frei unter der GPL erhältlich. Es benutzt keine Samples, sondern mathematische Modelle zur Klangsynthese.\n\n"}
{"id": "1255465", "url": "https://de.wikipedia.org/wiki?curid=1255465", "title": "MoodLogic", "text": "MoodLogic\n\nDas Musikverwaltungsprogramm moodLogic gab dem Nutzer für erworbene Credits die Möglichkeit, Musik nach Genre, Geschwindigkeit, Erscheinungsjahr, Ähnlichkeit und sechs voreingestellten Gefühlsstimmungen auch in Kombination zu filtern.\n\nVon jedem Musikstück im MP3-Format wird ein akustischer Fingerabdruck erstellt und in eine lokale Datenbankdatei übertragen. Diese Daten werden über das Internet mit einem zentralen Server abgeglichen, auf dem, sofern bereits vorhanden, die Bewertungen anderer Nutzer zum selben Musikstück stehen. Nach dieser Erkennungsphase ist das Programm auch ohne Internetanbindung nutzbar, um die Musik nach genannten Kategorien zu filtern. Dateinamen und ID3-Signaturen können nach erfolgreicher Erkennung korrigiert werden, eine Einordnung in artistspezifische Ordner auf der Festplatte ist nach Bedarf automatisch möglich.\n\nFür in der Erstphase angemeldete Nutzer ist es durch das Bewerten von 5 Musikstücken nach einem Multiple-Choice-Verfahren zu Ähnlichkeit, Geschwindigkeit und Klanggefühl möglich, 100 freie Credits zu erwerben. Bei 50 Bewertungen gibt es 250 Credits. Pro einsortiertes Musikstück ist ein Credit einmalig fällig, eine wiederholte Analyse dieses Musikstückes ist unbegrenzt möglich. Das Programm konnte ohne Netzanbindung betrieben werden, wenn die Datenbank einmal via Serververbindung erstellt wurde. Eine Installation ohne Server ist nicht möglich.\n\nAus den lokalen Musikdaten wird ein sogenannter „Fingerprint“ erstellt und zur Erkennung auf den Server der Firma geschickt, um die zugehörigen Beschreibungsdaten aus der Datenbank der Firma zu erhalten.\n\nSeit 2004 gibt es kein Lebenszeichen mehr von den MoodLogic-Entwicklern. Die Datenbank mit den Liedinformationen wurde schon seit über zwei Jahren nicht mehr aktualisiert. Die einzige Änderung, die an der Internetseite in den letzten Jahren stattgefunden hat, ist, dass das Forum, in dem User von dem Kauf der Software abrieten, eingestellt wurde. Im Mai 2006 wurde MoodLogic von All Music Guide gekauft.\n\nDer Dienst wurde Anfang März 2008 durch den neuen Eigentümer Macrovision beendet.\n\n\"Siehe auch:\" MusicBrainz\n"}
{"id": "1257535", "url": "https://de.wikipedia.org/wiki?curid=1257535", "title": "Amstrad PC1512", "text": "Amstrad PC1512\n\nDer Amstrad PC1512 ist ein 1986 vorgestellter Heimcomputer.\nEr war damit einer der ersten in Europa privat genutzten IBM-PC-kompatiblen Computer.\nHersteller war die Firma Amstrad, im deutschsprachigen Raum wurde der PC – wie schon die vorherigen Amstrad-Computer – durch die Schneider Computer Division unter eigenem Namen vertrieben.\nDer PC1512 verwendete einen Intel 8086 mit 8 MHz und wurde mit 512 KByte RAM ausgeliefert, \nkonnte auf dem Motherboard aber auf 640 KByte RAM aufgerüstet werden.\n\nEin PC1512 SD besaß ein, ein PC1512 DD zwei 5,25 Zoll Diskettenlaufwerke. Außerdem wurde eine 20-MB-Festplatte angeboten.\nEs gab die Wahl zwischen einem Schwarz-Weiß und einem Farb-Monitor. Auf dem S/W-Monitor wurden die Farben als Graustufen angezeigt.\nDer 1512 verfügte über einen CGA-Adapter mit einem zusätzlichen 640x200x16 Modus. \nErst sein Nachfolger PC1640 konnte neben CGA auch HGC und EGA Grafik darstellen.\n\nNeben der CGA besaß das Motherboard eine parallele und eine serielle Schnittstelle, eine Echtzeituhr (mit zugeordnetem Interrupt Kanal 2) und ein 16 KB BIOS (mit Basissegment 0xFC00), \ndas die Konfiguration (Laufwerke, Schnittstellen, Coprozessor, Gameport sowie Speicher) automatisch erkannte,\nim Ausrüstungsinterrupt allerdings Bit 11 statt Bit 12 für einen Gameportadapter setzte.\nIn der ersten Biosversion erfolgte in der Systemzeit mitternachts kein 24 Stundenüberlauf.\n\nEine Besonderheit des 1512 waren die mitgelieferten Betriebssysteme: Neben Microsofts MS-DOS lag dem PC auch das auf CP/M basierende DOS Plus von Digital Research sowie die vom Atari ST-Heimcomputer bekannte grafische Benutzeroberfläche GEM (Graphical Environment Manager) bei. \nDeswegen verfügte dieser Rechner bereits standardmäßig über eine 2-Tasten-Maus\nmit Anschluss auf der linken PC-Seite.\n\nBeim 1512 befand sich das Netzteil im Monitor, ein Lüfter war deshalb nicht nötig.\nGeräusch verursachten daher lediglich Laufwerke.\nAndererseits konnte dadurch der Monitor nicht durch einen handelsüblichen ersetzt werden.\n\nAls Puffer-Batterien für die persönlichen BIOS-Einstellungen kamen \nvier handelsübliche AA-Batterien \nund nicht wie heute üblich Lithium-Batterien zum Einsatz. \nDie Batterien befanden sich unter der Monitor-Aufnahme im PC-Gehäuse und konnten so leicht und ohne Werkzeug gewechselt werden.\nUhrzeit und Bios-Einstellungen gingen bei ausgeschaltetem Rechner beim Batteriewechsel verloren.\n\nWeitere Besonderheit war ein ohne Werkzeug zu öffnender Deckel über den Slots für drei Erweiterungen.\nEine Zusatzkarte (weitere Grafikkarte, COM-Anschlusskarte usw.) konnte daher eingebaut werden,\nohne dafür den ganzen PC zu öffnen.\nAllerdings war der Anschlag für das Befestigungsblech der Zusatzkarten zu tief. \n\nEin Schneider 1512 mit einem Diskettenlaufwerk und S/W-Monitor kostete 1986 knapp unter 2000 DM,\nAufpreis für zweites Diskettenlaufwerk 500 DM, Aufpreis für Farb- statt S/W Monitor 500 DM.\n"}
{"id": "1258028", "url": "https://de.wikipedia.org/wiki?curid=1258028", "title": "Softwarevisualisierung", "text": "Softwarevisualisierung\n\nSoftwarevisualisierung beschäftigt sich mit der Visualisierung von Informationen über Softwaresysteme. Es kommen vorrangig statische, interaktive und animierte 2-D- und 3-D-Visualisierungsverfahren zum Einsatz.\n\nSoftwarevisualisierung bildet im Allgemeinen Informationen wie zum Beispiel die Implementierungsstruktur, den Entwicklungsverlauf oder das dynamische Systemverhalten ab. Typischerweise werden in der Visualisierung Softwaremetrik-Informationen einbezogen, wie zum Beispiel die Größe oder Komplexität einer Systemkomponente (z. B. abgeleitet aus Quellcode-Analysen).\n\nSoftwarevisualisierung stellt grundlegende Konzepte und Werkzeuge für den Softwareentwicklungsprozess bereit, zum Beispiel in Form von Softwarekarten, die innerhalb von entscheidungsunterstützenden Systemen eingesetzt werden. Softwarevisualisierung richtet sich nicht nur an die initiale Phase einer neuen Systementwicklung (z. B. graphisches Programmieren), sondern vor allem an die (zeitlich meist unbefristete) Maintenance-Phase.\n\nSoftwarevisualisierung ist von Natur aus keine Methode zur Software-Qualitätssicherung, kann aber dazu verwendet werden manuell Anomalien (z. B. Zyklen) aufzuspüren oder Defekte zu erkennen. Dieser Prozess wird auch \"visuelles Data Mining\" genannt.\n\nDie Ziele der Softwarevisualisierung beinhalten das Verstehen von Softwaresystemen (z. B. Aufbau und Struktur) und Algorithmen (z. B. die Animation von Suchalgorithmen), die Analyse von Softwaresystemen zur Entdeckung von Anomalien (z. B. durch Darstellung von Klassen mit (zu) hoher Kopplung) sowie das Überwachen von Code-Qualität in Verbindung mit Aktivitäten des Entwicklungsteams.\n\nDie Softwarevisualisierung bildet einen Themenbereich zwischen der Softwaretechnik und der Informationsvisualisierung. In den meisten Arbeiten, die sich damit beschäftigen, stehen weniger neue Visualisierungsmethoden oder neue Softwareanalyseverfahren im Mittelpunkt, sondern die Anwendung bereits bekannter Darstellungsformen auf bestimmte Eigenschaften einer Software. Das Gegenstück zur Softwarevisualisierung ist das sogenannte \"visuelle Programmieren\", bei dem aus einer Visualisierung erst die Software generiert wird. Dieser Ansatz kommt etwa bei Lernprogrammen für Kinder oder beim Rapid Prototyping zum Tragen.\n\nWerkzeuge für die Softwarevisualisierung können eingesetzt werden um einen Entwickler direkt während der Softwareentwicklung und -wartung zu unterstützen. Ziel dabei ist die automatische Entdeckung und Visualisierung von Qualitätsdefekten in objektorientierten Systemen oder Diensten. Als Plugin in Entwicklungsumgebungen wie Eclipse visualisieren sie die Beziehungen einer Klasse mit anderen Klassen im Softwaresystem und markieren potentielle Probleme. Ein Nebeneffekt stellt die visuelle Navigation durch das Softwaresystem dar.\n\nSoftVis-Werkzeuge visualisieren Softwaresysteme oder größere Gruppen von Klassenverbänden, um Architekturen zu analysieren oder die Einhaltung von Architekturvorgaben oder der Codequalität zu überprüfen. Beispiele solcher Werkzeuge sind:\n\n\n\n"}
{"id": "1258031", "url": "https://de.wikipedia.org/wiki?curid=1258031", "title": "Tone Mapping", "text": "Tone Mapping\n\nTone Mapping, Tone Reproduction oder Dynamikkompression sind synonyme Begriffe, die die Kompression des Dynamikumfangs von Hochkontrastbildern (high dynamic range images) bezeichnen, also von digitalen Bildern mit hohem Helligkeitsumfang. Beim Tone Mapping wird der Kontrastumfang eines Hochkontrastbildes verringert, um es auf herkömmlichen Ausgabegeräten darstellen zu können.\n\nIn der Natur kommt ein Dynamikumfang (Verhältnis von größter und kleinster Leuchtdichte) von über 10:1 vor, wenn man das Sonnenlicht mit dem Sternenlicht vergleicht. Der zu einem bestimmten Zeitpunkt typischerweise beobachtete Dynamikumfang liegt in der Größenordnung von 1:10.000. Die menschliche visuelle Wahrnehmung löst das Tone-Mapping-Problem, da sie in der Lage ist, sich den vorherrschenden Helligkeitsverhältnissen anzupassen. Auf unterschiedliche absolute Helligkeitsbedingungen (photopisch, mesopisch, skotopisch) reagiert das Auge nichtlinear.\n\nViele Tone-Mapping-Verfahren basieren auf den Erkenntnissen über die menschliche visuelle Wahrnehmung, da ihr Ziel darin besteht, ein möglichst naturgetreu wirkendes Bild zu berechnen. Die wichtigste Rolle spielen dabei die Fotorezeptoren, deren Adaptation sich nach der Naka-Rushton-Gleichung wie folgt beschreiben lässt:\n\nHierbei ist formula_2 die Fotorezeptor-Reizstärke, formula_3 ist die maximale Reizstärke, formula_4 ist die Lichtstärke und formula_5 die Lichtstärke, die die halbe Reizstärke bei der vorherrschenden Hintergrundintensität hervorruft. Mehrere Tone-Mapping-Verfahren basieren auf einer Gleichung, die dieser ähnelt.\n\nEs existieren zahlreiche Tone-Mapping-Operatoren, die sich jedoch nur in wenige grundlegend verschiedene Klassen einteilen lassen. Sogenannte \"globale\" Operatoren verwenden eine Funktion, die jedem HDR-Wert einen dynamikkomprimierten Wert zuweist und die auf jedes Pixel angewandt wird. Im Gegensatz dazu wird bei \"lokalen\" Operatoren diese Funktion für jedes Pixel je nach lokalem Adaptationsniveau variiert. \"Frequenzbasierte\" Operatoren nutzen eine grundlegend andere Technik, bei der der Dynamikumfang von Bildregionen je nach Ortsfrequenz reduziert wird. Schließlich gibt es noch \"gradientenbasierte\" Operatoren, die die Helligkeitsgradienten des Ausgangsbildes für jedes Pixel abschwächen, um das LDR-Bild (Bild mit geringem Helligkeitsumfang) zu erzeugen.\n\nViele Operatoren erwarten, dass die Werte des Ausgangsbildes als Leuchtdichte in einer bestimmten Einheit (cd/m²) kalibriert sind. Das liegt daran, dass die nichtlineare Wahrnehmung von absoluten Helligkeiten berücksichtigt wird; eine Tageslichtszene wird demnach anders dargestellt als eine Nachtszene. Es ist jedoch oft möglich, die originalen Lichtverhältnisse direkt anhand des HDR-Bildes zu rekonstruieren, indem das Histogramm ausgewertet wird. Die meisten Tone-Mapping-Verfahren ignorieren die Farbwahrnehmung weitgehend und wenden den neuen Helligkeitswert auf alle Kanäle gleich an.\n\nGlobale Operatoren verarbeiten die Pixel des Ausgangsbildes unabhängig voneinander. Sie sind schneller als andere Verfahren und können oft in Echtzeit ausgeführt werden. Allerdings eignen sie sich weniger gut für Szenen mit sehr großem Dynamikumfang, da sie eher dazu neigen, in sehr hellen und sehr dunklen Bereichen Details zu verlieren.\n\nViele globale Operatoren basieren auf Adaptationsmodellen, bei denen die Hintergrundintensität bekannt sein muss. Diese Intensität kann abgeschätzt werden, indem das arithmetische Mittel der Pixelwerte berechnet wird, das geometrische Mittel ist jedoch die bevorzugte Methode.\n\nDer einfachste globale Operator rechnet die Werte des Ausgangsbildes linear auf den Dynamikumfang des LDR-Bildes herunter. Dieses Verfahren ist jedoch unzureichend, da Details und Kontrast verlorengehen.\n\n\nLokale Operatoren können eine große Klasse von HDR-Bildern verarbeiten, da sie einen größeren Dynamikumfang darstellen können, ohne Details zu verlieren. Sie gehen davon aus, dass die menschliche Helligkeitswahrnehmung sich nicht dem gesamten Bild anpasst, sondern nur kleineren Regionen.\n\nUm den lokalen Helligkeitswert für jedes Pixel zu berechnen, kann ein radialer Filter verwendet werden, der auf die Nachbarpixel angewandt wird. Diese Methode führt jedoch zu Halo-Artefakten und Kontrastumkehrungen nahe Kanten, da dort zu große Helligkeitsunterschiede innerhalb des Filterradius vorherrschen. Um dieses Problem zu umgehen, können unterschiedliche Methoden verwendet werden:\nZur Bestimmung des optimalen Filterradius kann eine Reihe von Tiefpass-gefilterten Versionen des Ausgangsbildes verwendet werden.\n\nFrequenzbasierte Operatoren teilen das Ausgangsbild in ein gefiltertes HDR-Bild mit geringen Ortsfrequenzen und ein ungefiltertes LDR-Bild mit hohen Frequenzen auf, die anschließend kombiniert werden. Allerdings kann das gefilterte Bild auch so interpretiert werden, dass jedes Pixel einen lokalen Adaptationswert liefert. Daher lässt sich nicht immer klar zwischen lokalen und frequenzbasierten Operatoren trennen.\n\n\nDiese Klasse von Tone-Mapping-Operatoren berechnet die Gradienten des Ausgangsbildes und schwächt sie ab.\n\n\nTone-Mapping-Operatoren unterscheiden sich in Geschwindigkeit, Vorhandensein und Stärke von Artefakten, Beibehaltung von Bilddetails sowie der Fähigkeit, HDR-Bilder mit sehr großem Dynamikbereich komprimieren zu können. Einige Studien befassen sich mit dem Vergleich von Tone-Mapping-Verfahren. Die Internationale Beleuchtungskommission hat das Arbeitskomitee TC8-08 gebildet, um Methoden zur Validierung von Tone-Mapping-Operatoren zu entwickeln. Beim visuellen Vergleich verschiedener Operatoren ergibt sich die Schwierigkeit, dass Änderungen an Parametern große Auswirkungen auf das Ergebnis haben können.\n\n\n"}
{"id": "1258247", "url": "https://de.wikipedia.org/wiki?curid=1258247", "title": "Share (P2P)", "text": "Share (P2P)\n\nShare (jap. , \"Shea\") ist ein Closed-Source-Filesharingprogramm. Entwickelt wird sie in Japan von einem anonymen Autor als Nachfolger von Winny, dessen Entwickler am 10. Mai 2004 verhaftet wurde, und setzt auf hohe Sicherheit und Anonymität. Share funktioniert fast genauso in der Art und Weise wie Winny, arbeitet also mit verschlüsselten IP-Adressen, Dateinamen sowie Zwischenspeicher und basiert außerdem auf derselben Node-Architektur wie Winny.\n\nDas Programm existiert in zwei Varianten: Share EX2 verwendet TCP und Share NT5 UDP als Netzwerkprotokoll.\n\nDer Nutzer von Share legt zuerst seine Downstream- und Upstreamgeschwindigkeit (mindestens 50 KB/s, eine für japanische ISPs durchschnittliche Geschwindigkeit), Port und optional eine auf einem RSA-Verschlüsselungsalgorithmus (hier: \"Tripcode\") basierende ID fest. Die ID des Benutzers, über den intern die neuesten Versionen von Share verbreitet werden lautet .\n\nIm zweiten Schritt werden vom Nutzer in den Cluster-Einstellungen Schlüsselwörter definiert, von denen maximal fünf gleichzeitig aktiv sein können. Aktive Schlüsselwörter führen zu gezielten Suchaktionen von Share bei anderen Benutzern mit identischen aktiven Schlüsselwörtern. Die Mengen der Benutzer mit identischen Schlüsselwörtern werden als \"Cluster\" bezeichnet. \n\nDie Verbindung mit anderen Benutzern führt über Nodes (Rechner, die online sind und auf denen Share läuft), deren Adressen verschlüsselt sind, sich aber auch nach Kontaktaufnahme mit anderen Nodes über diese verbreiten. Die Node-Datenbank wächst dadurch ständig weiter. Außerdem werden auf diversen privaten Seiten Listen mit verschlüsselten Adressen der Nodes zur Verfügung gestellt.\n\nNeue Dateien sowie häufig angefragte Dateien werden automatisch in die Caches der verschiedenen Nodes geladen. Alle übertragenen Dateien sind dabei immer in mehrere Blöcke aufgeteilt, welche zudem bis zum Zeitpunkt der Vervollständigung beim Endnutzer verschlüsselt sind. Bei der Übertragung werden zwischen demjenigen Node, der eine Datei anbietet und demjenigen Node, der diese Datei anfordert, noch mehrere weitere Nodes zwischengeschaltet.\n\nShare benutzt eine auf RC6 im ECB Mode und SHA-1 basierende Verschlüsselung, um die Identität der Nodes (IP-Adresse) sogar vor den Share-Benutzern selbst geheim zu halten. Für den normalen Benutzer ist also weder ersichtlich, mit welchen anderen IP-Adressen er verbunden ist, noch, welche Daten (abgesehen von den selbst angebotenen und angefragten) er von diesen erhält, an diese sendet oder welche von ihm abgefragt werden.\n\nShare ist durch die Nodes dezentralisiert aufgebaut und kann deshalb nicht einfach abgeschaltet werden. Es existiert kein zentraler Server, mit dem sich das Netzwerk abschalten ließe, ein Ausfall einzelner Nodes schadet dem Netzwerk ebenfalls nicht.\n\nShare kannte ein Belohnungssystem, für diejenigen die viele Daten mit den anderen Nutzern geteilt haben. Die sogenannten Points werden beim Node zu dem man hochlädt gespeichert und diese Informationen mit anderen geteilt. Nutzer mit vielen Points haben eine höhere Uploadpriorität als die ohne; hat man viele Points, so wird man von den Nodes von denen Daten angefordert werden gegenüber denen ohne Points bevorzugt. Seit einiger Zeit ist dieses allerdings nicht mehr aktiv, man kann aber davon ausgehen, dass es in einer der nächsten Builds wieder aktiviert sein wird.\n\n"}
{"id": "1264525", "url": "https://de.wikipedia.org/wiki?curid=1264525", "title": "PLD Linux", "text": "PLD Linux\n\nPLD ist eine – seit 1998 – ursprünglich von polnischen Linux-Enthusiasten entwickelte RPM-basierte Linux-Distribution. Der Name PLD bedeutete ursprünglich „Polish(ed) Linux Distribution“ – heute steht er für ein englisches rekursives Akronym von „PLD Linux Distribution“. Die Distribution richtet sich an fortgeschrittene Benutzer, die bereits Erfahrung mit anderen Linux-Distributionen haben.\n\nPLD wurde 1998 als ein Paketarchiv für Red Hat Linux ins Leben gerufen. Schon bald darauf entwickelte es sich zu einer vollständigen, eigenständigen Distribution. Ausgehend von seinen polnischen Wurzeln wird PLD heute von etwa 200 Linux-Enthusiasten (davon 50 aktive) aus der ganzen Welt weiterentwickelt.\n\nSeit 26. Februar 2007 wird die aktuelle Version 3.0 entwickelt.\n\nPLD Linux läuft auf x86- (jeweils separat optimiert für i386, i586 und i686), PowerPC-, Alpha-, AMD64- und SPARC-Rechnerarchitekturen. Darüber hinaus unterstützt es neue Netzwerktechnologien wie IPv6, im Bereich Sicherheit Grsecurity, PAM, GSSAPI und TLS/SSL und kommt hauptsächlich auf Servern zum Einsatz. Für die stabile Version 3.0 sind weit mehr als 10.000 Softwarepakete zum Download erhältlich. Dabei wurden große Pakete (wie X.Org, KDE, etc.) stark modularisiert, so dass der Benutzer nur die wirklich benötigten Pakete installieren muss.\n\nDie Distribution stellt zwei spezielle RPM-Paketmanager bereit: einen Klon des vom Debian-Projekt entwickelten APT und ein eigenes Programm namens „poldek“. Poldek ist wie APT ein konsolenbasierter Paketmanager, mit dessen Hilfe sich auf einfache Weise neue Softwarepakete herunterladen und installieren lassen.\n\nVon PLD sind CD- und DVD-ISO-Abbilder zum Download erhältlich, unter anderem auch sogenannte Mini-ISOs für CD, die nur die wichtigsten Pakete enthalten – der Rest lässt sich mit dem Paketmanager über das Internet herunterladen und installieren. Ebenfalls als ISO verfügbar sind eine PLD Live-CD und eine Rescue-CD, die den eigenen Bedürfnissen angepasst werden können.\n\n\n"}
{"id": "1264988", "url": "https://de.wikipedia.org/wiki?curid=1264988", "title": "Aeolus (Software)", "text": "Aeolus (Software)\n\nAeolus ist eine freie Software zur Simulation einer Pfeifenorgel. Es benutzt keine Samples, sondern erstellt den Klang synthetisch.\n\nAnhand von mathematischen Rechenmodellen simuliert es das natürliche Verhalten einer Pfeifenorgel. Es ist unter der GPL als freie Software veröffentlicht worden und läuft unter Linux. Für die Klangausgabe verwendet Aeolus den freien Soundserver JACK.\n\nAeolus verbraucht durch den Verzicht auf Sampling wenig Systemressourcen und läuft bereits auf einem 1-GHz-Rechner mit 256 MByte RAM.\n\nDie Disposition ab der Version 0.6.6 ist folgende:\n\n\nAeolus unterstützt folgende Stimmungen:\n\n\nKammerton a = 400–480 Hz\n\n\n"}
{"id": "1267576", "url": "https://de.wikipedia.org/wiki?curid=1267576", "title": "FastStone Image Viewer", "text": "FastStone Image Viewer\n\nFastStone Image Viewer ist ein für private Anwender sowie Bildungseinrichtungen kostenloses Programm zur Betrachtung, Bearbeitung und Verwaltung von Bildern vieler verbreiteter Formate.\nEs wird wegen seiner Benutzerfreundlichkeit beim Betrachten und effizienten Durchführen von einfachen Bildbearbeitungsfunktionen geschätzt.\nEin vergleichbares Programm (auch) für Linux-basierte Betriebssysteme ist XnViewMP.\n\nDie Oberfläche nutzt im Vollbildmodus die gesamte Bildschirmfläche und wird nicht durch Menüleisten überdeckt. Erst wenn man den Mauszeiger an eine der vier Bildschirmkanten bewegt, erscheinen die entsprechenden Menüs, die Bearbeitungs- und Organisationsfunktionen anbieten.\n\nIm Hauptfenster des Programmes ist ein Dateibrowser enthalten, mit dessen Hilfe die Ordner mit digitalen Bildern auf dem Datenträger ausgewählt werden können. Das Programm erzeugt nach dem Öffnen eines Ordners eine Ansicht mit Miniaturbildern, die in einer internen Datenbank gespeichert werden und somit den nächsten Zugriff auf diesen Ordner beschleunigen.\n\nFastStone Image Viewer unterstützt neben den gebräuchlichsten Bildformaten JPG, BMP, GIF, PNG, TIFF, PCX, TGA, JPEG2000, PSD, WMF, CUR und ICO auch die RAW- und Spezialformate verschiedener Hersteller wie Canon, Nikon, Olympus, Fuji, Adobe, Minolta, Pentax, Panasonic und Sony.\n\nAllerdings unterstützt es bis jetzt keinen Unicode.\n\nBeim Abspeichern von Bildern in einem verlustbehafteten Dateiformat (z. B. JPG) kann der Einfluss des Kompressionsgrads auf die Bildqualität kontrolliert werden.\n\nEin Klick auf eine der Miniaturansichten zeigt ein vergrößertes Bild an, mit einem Doppelklick wird der Vollbild- und damit gleichzeitig auch der Bearbeitungsbildschirm geöffnet.\n\nDas Programm kennt die windowsüblichen Tastenkombinationen sowie zur schnellen Bedienung viele zusätzliche Funktionstasten und kann komfortabel über Kontext- und die oben erwähnten ausklappbaren Seitenmenüs bedient werden.\n\nSeit Version 2.50 sind auch Texteinfügungen, Freihandzeichnen, Zeichnen von geometrischen Elementen sowie das Einfügen von Wasserzeichen möglich, Bilder können für die Stapelverarbeitung markiert werden.\n\nIm Stapelbetrieb können die Bilder gelöscht, verschoben, kopiert, konvertiert, umbenannt und umgewandelt werden. Ebenso kann man Bilder drucken, mit Wasserzeichen versehen oder zwei bis vier Bilder in einer vergrößerten Darstellung miteinander vergleichen.\nDas Programm unterstützt auch TWAIN-kompatible Scanner und gestattet somit die direkte Einfügung und Bearbeitung gescannter Bilder.\n\nExif-Informationen können aus jeder Ansicht heraus aufgerufen werden, inklusive eventueller GPS-Daten. Dort ist es auch möglich, Kommentare in JPEG-Dateien einzufügen.\n\nAus dem Programm heraus können externe Bildbearbeitungsprogramme gestartet werden, die eine weitergehende Bearbeitung ermöglichen.\n\nDie Bilder eines Ordners können in einer Diaschau mit vielfältigen Konfigurationsmöglichkeiten, wahlweise auch mit Hintergrundmusik, abgespielt werden.\n\nKurze Zeit war Version 2.70 des Image Viewers als Standard- und als kostenpflichtige „Pro“-Version erhältlich. Seit Version 2.8 existiert vom FastStone Image Viewer wieder nur eine Version, die für private Nutzung kostenlos ist.\n\nMit Version 3.0 wurde der Viewer Vista-kompatibel. Weitere Neuerungen sind die Möglichkeiten zum Abspeichern von PDF-Dateien (auch mehrseitige) sowie zum Erzeugen mehrseitiger TIFF-Dateien. Weiterhin können jetzt Bilder im Stapelbetrieb beschnitten und auch nach den Exif-Daten (Datum, Zeit) umbenannt werden.\n\nSeit der Version 3.1 gibt es neben der englischen eine mehrsprachige Version in Dänisch, Niederländisch, Französisch, Deutsch, Ungarisch, Italienisch, Norwegisch, Polnisch und Spanisch.\n\nMit Version 3.3 wurden weitere Bildbearbeitungsfunktionen wie Helligkeit, Kontrast, Gammakorrektur, Sättigung, Schärfen und ähnlich hinzugefügt. Des Weiteren wurde die Diashowfunktion erweitert und eine Downloadfunktion von Speicherkarte integriert.\n\nIn der im November 2007 erschienenen Version 3.4 wurden Verbesserungen in der Bedienungsoberfläche hinzugefügt, Farbanpassungfunktionen verbessert und Stapelverarbeitung für RAW-Dateien eingeführt. Die im Januar 2008 veröffentlichte Version 3.5 führte kleine Verbesserungen, wie optimierte Bedienungsoberfläche und erweitere Textformatierung ein. Version 3.6 vom September 2008 brachte dann unter anderem Unterstützung für 64-Bit-Windowssysteme, erweitere Stapelverarbeitungsfunktionen und verbesserte Videounterstützung. In Version 3.7 wurden einige Funktionen verbessert, Fehler beseitigt und die Unterstützung des RAW-Formates Panasonic RW2 hinzugefügt. Weitere Verbesserungen und Fehlerbeseitigungen gibt es in der Version 3.8, die erstmals als einheitliche Version (englisch und 14 weitere Sprachen) veröffentlicht wurde. Neu in dieser Version sind Sprechblasen, wählbare Anzahl der Kopien im Druckdialog sowie Farben-Subsampling (auch \"Chroma Subsampling\" genannt). Version 4 verfügt u. a. über eine verbesserte Bedienung und neue Rahmeneffekte und verarbeitet im Bild enthaltene GPS-Informationen. In Version 4.3 wurden folgende neue Funktionen hinzugefügt: Kopier- und Reparaturstempel, automatische Farbkorrektur, Schatten oder Lichter aufhellen/abdunkeln, Tonwertkorrektur, Gradationskurven verändern, unscharf maskieren, Bleistiftzeichnung- und Ölfarben-Filter, neue Zeichenelemente für Textobjekte, Einstellung der Reihenfolge von Bildern, Unterstützung des EPS- sowie des Sony-SR2-Formates, überarbeitetes Speichermanagement.\n\nNeu in Version 4.7: Touch-Interface in der gesamten Anwendung, neue Mausgesten zum Bilderwechsel, verbesserte Dialoge für Kopieren/Verschieben, Begradigen/Drehen, Bildschirmhintergrund, Bildvergleich und Drucken, zusätzlicher Browsermodus, Tool zum Reduzieren des Bildrauschens, aktualisierte RAW-Format-Bibliothek, wahlweise neue Oberfläche\n\nNeu in Version 4.9:\n„Design und Print“ hinzugefügt, womit es möglich ist, mehrere Bilder mit Text und Effekte auf eine Seite zu drucken.\nVerbesserte „Gradationskurven“. Durch Klicken und Ziehen im Bild können die Kurven direkt eingestellt werden. Die Endpunkte der Kurven sind einstellbar.\nOption „Papiergröße“ für die Registerkarte Stapelkonvertierung hinzugefügt.\nOption „Automatische Größe“ zum Tool „Bilder einscannen“ hinzugefügt.\nZusätzliche Unterstützung von Videodateien (avi, mpeg, mpg, wmv, mov, mp4) bei der Stapelumbenennung.\n„Anzeigen in Google Maps“ und „Anzeigen in Google Earth“-Tasten im „Bild-Eigenschaften“-Fenster hinzugefügt. Ist dann sichtbar, wenn das Bild GPS-Informationen enthält.\nWeitere kleine Verbesserungen und Bugfixes.\n\n"}
{"id": "1269242", "url": "https://de.wikipedia.org/wiki?curid=1269242", "title": "GBR-Code", "text": "GBR-Code\n\nDer GBR-Code (nach Guy, Blandford und Roycroft) beschreibt die Stellung auf einem Schachbrett durch eine einfache Zeichenkette. Er kann als Index in Datenbanken und Publikationen von Studiensammlungen verwendet werden, da sich Stellungen mit vergleichbarem Figurenmaterial in ihrem GBR-Code stark ähneln. Hierbei entfallen häufig die Figurenpositionen (3. Teil).\n\nDer GBR-Code ist für Stellungen mit üblichem Material eindeutig. Für Stellungen mit drei oder mehr Figuren einer Sorte werden zusätzliche Angaben benötigt, um die Stellung exakt zu beschreiben.\n\nDer GBR-Code gliedert sich in vier Teile, die jeweils von einem Punkt voneinander getrennt werden:\n\nDie Grundstellung:\n\n4888.88. e1e8d1d8a1h1a8h8c1f1c8f8b1g1b8g8. a2b2c2d2e2f2g2h2a7b7c7d7e7f7g7h7\nEine Stellung mit dem weißen König auf a1, einem schwarzen auf a8 und schwarzen Bauern auf a2 und b2 (zweites Beispiel):\n\n0000.02. a1a8. a2b2\n\n"}
{"id": "1269268", "url": "https://de.wikipedia.org/wiki?curid=1269268", "title": "Dichtefunktionaltheorie (Quantenphysik)", "text": "Dichtefunktionaltheorie (Quantenphysik)\n\nDie Dichtefunktionaltheorie (DFT) ist ein Verfahren zur Bestimmung des quantenmechanischen Grundzustandes eines Vielelektronensystems, das auf der ortsabhängigen Elektronendichte beruht. Die Dichtefunktionaltheorie wird zur Berechnung grundlegender Eigenschaften von Molekülen und Festkörpern, wie beispielsweise von Bindungslängen und -energien, verwendet.\n\nDie große Bedeutung dieser Theorie liegt darin, dass es mit ihr nicht notwendig ist, die vollständige Schrödingergleichung für das Vielelektronensystem zu lösen, wodurch der Aufwand an Rechenleistung stark sinkt bzw. Berechnungen von Systemen mit deutlich über zehn Elektronen überhaupt erst möglich werden.\n\nFür die Entwicklung der Dichtefunktionaltheorie wurde 1998 der Nobelpreis für Chemie an Walter Kohn vergeben.\n\nGrundlage der Dichtefunktionaltheorie ist das Hohenberg-Kohn-Theorem: Der Grundzustand eines Systems von formula_1 Elektronen (als Wellenfunktion also von formula_2 Koordinaten abhängig) ist durch die ortsabhängige Elektronendichte formula_3 eindeutig festgelegt. In der Dichtefunktionaltheorie wird nun die Elektronendichte im Grundzustand bestimmt, daraus können im Prinzip alle weiteren Eigenschaften des Grundzustandes bestimmt werden. Diese Eigenschaften, beispielsweise die Gesamtenergie, sind also Funktionale der Dichte.\n\nDie Rechnungen mit der Dichtefunktionaltheorie werden normalerweise in der Born-Oppenheimer-Näherung durchgeführt, es werden also nur die Elektronen quantenmechanisch behandelt.\n\nUm die Elektronendichte zu bestimmen, werden formula_1 Einelektronen-Wellenfunktionen, die sogenannten Kohn-Sham-Funktionen formula_5 angesetzt (benannt nach Kohn und Lu Jeu Sham), die formula_1 Lösungen der Schrödingergleichung in einer effektiven Potentialfunktion formula_7 sind. Diese Rechenweise ist wesentlich weniger aufwändig als die Lösung der Schrödingergleichung mit formula_1 Elektronen zugleich, weil es sich um voneinander unabhängige Lösungen \"einer\" Schrödingergleichung handelt (keine Slater-Determinante). Diese Einelektronen-Schrödingergleichungen werden auch als Kohn-Sham-Gleichungen bezeichnet:\n\nDie Dichte erhält man aus der Summe der Elektronendichten der Kohn-Sham-Funktionen:\n\nDas effektive Potential ist von der Dichte abhängig:\n\nHierbei ist der erste Term, formula_13, das externe Potential, das im Wesentlichen die Anziehung der Elektronen durch die Atomkerne beschreibt, und der zweite Term beschreibt die elektrostatische Wechselwirkung der Elektronen untereinander (Hartree-Term). Der dritte Term formula_14, das sogenannte Austausch-Korrelationspotential („formula_15“ für englisch „, „formula_16“ für „“), soll für die korrekte Behandlung des Vielelektronensystems sorgen.\n\nDa das effektive Potential formula_17 einerseits in den Kohn-Sham-Gleichungen vorkommt, andererseits von der Dichte formula_3 und somit von den Lösungen dieser Gleichungen abhängt, müssen die Lösungen iterativ gefunden werden. Es wird also mit dem neu gefundenen Potential (oder einer Linearkombination des vorigen und des neuen Potentials) die Kohn-Sham-Gleichung wieder gelöst, daraus ein neues Potential bestimmt usw., bis eine stabile (\"selbstkonsistente\") Lösung gefunden wird.\n\nStreng genommen sind die Kohn-Sham-Funktionen reine Rechengrößen und haben für sich alleine keine physikalische Bedeutung. In der Praxis können sie jedoch oft als Näherung für tatsächliche Elektronenzustände herangezogen werden, und ihre Energien formula_19 werden zum Beispiel zur Berechnung der Bandstruktur herangezogen.\n\nMit dem Kohn-Sham-Formalismus wurde das Problem des Vielelektronensystems eigentlich nur auf den Austausch-Korrelationsterm formula_20 verlagert, und noch nicht gelöst. Streng genommen hängt formula_20 von der Elektronendichte an allen Orten und nicht nur am Punkt formula_22 ab, und lässt sich nur für sehr wenige triviale Fälle genau berechnen. Es zeigt sich aber, dass es oft ausreicht, eine genäherte Lösung für diesen Term zu finden:\n\n\n\n\nDie meisten Einschränkungen und Probleme bei der Verwendung der Dichtefunktionaltheorie hängen mit dem Austausch-Korrelations-Potential zusammen. So liefern beispielsweise die verschiedenen GGA-Potentiale Bindungsenergien von einfachen Molekülen, die sich voneinander und von den experimentellen Werten um mehr als 20 Prozent unterscheiden können. Van-der-Waals-Bindungen werden von den „semilokalen“ Funktionen wie GGA überhaupt nicht beschrieben, weil sie auf langreichweitigen Korrelationen der Ladungsverteilung beruhen. Ein weiteres Problem liegt darin, dass die Bandlücken und HOMO-LUMO-Energiedifferenzen, die aus den Kohn-Sham-Funktionen berechnet werden, bei LDA und GGA generell zu niedrig sind.\n\nBerechnungen komplexer Strukturen mittels Dichtefunktionaltheorie erfordern hohe Computerleistung, daher kommt einer effizienten Durchführung der Rechnungen große Bedeutung zu. Die Rechenverfahren können nach den Basisfunktionen für die Kohn-Sham-Gleichungen eingeteilt werden:\n\n\"Atomare Wellenfunktionen\" (engl: ) in einer kugelförmigen Umgebung um den Atomkern (sog. \"Muffin-Tin\"-Bereich) sind gut zur Beschreibung der Elektronen in Kernnähe geeignet. Der Vorteil der atomaren Wellenfunktionen ist, dass bei für ihre Anwendung geeigneten Problemen meist sehr kleine Basissätze (eine Funktion pro Elektron und Drehimpulscharakter) zur Beschreibung genügen. Allerdings ergeben sich Probleme, die fast freien Elektronen zwischen den Atomen (z. B. Leitungselektronen in Metallen, Elektronen an Oberflächen etc.) beziehungsweise den Überlappungsbereich zwischen den Atomen konsistent zu beschreiben.\n\n\"Ebene Wellen\" sind gut zur Beschreibung der Valenzelektronen und Leitungselektronen in Festkörpern geeignet, jedoch können die räumlich wenig ausgedehnten Wellenfunktionen nahe an den Atomkernen schlecht beschrieben werden. Ebene Wellen haben den Vorteil, dass effiziente Algorithmen zur Fouriertransformation eingesetzt werden können und dadurch die Lösung der Kohn-Sham-Gleichungen sehr rasch erfolgen kann. Zudem sind sie sehr flexibel, da zum Beispiel auch fast freie Elektronen an Oberflächen gut beschrieben werden können.\n\nVor allem für Berechnungen in der Festkörperphysik werden daher diese Verfahren kombiniert, indem man ebene Wellen verwendet, für den Bereich nahe den Atomkernen aber zusätzliche Maßnahmen trifft. Dieser Bereich kann entweder vollständig getrennt behandelt (engl. ), es können dort zusätzliche Wellenfunktionen addiert (engl. ) oder es kann ein sogenanntes Pseudopotential, das nur im Bereich der Außenelektronen korrekte Wellenfunktionen ergibt, aber nicht in der Nähe der Atomkerne, verwendet werden.\n\nZur Behandlung molekularer Systeme werden in der Regel atomzentrierte \"Gauß-Funktionen\" als Basis für die zu generierenden Kohn-Sham-Orbitale verwendet. Diese Funktionen werden für jedes Atom voroptimiert und orientieren sich an den analytisch bekannten Lösungen der elektronischen Wellenfunktionen des Wasserstoffatoms. Diese Atomorbitale (Einelektronenwellenfunktionen) werden unter Berücksichtigung ihrer Symmetrie durch geeignete Linearkombinationen zur Konstruktion von Molekülorbitalen herangezogen (LCAO-Ansatz). Durch selbstkonsistentes Lösen der KS-Gleichungen (effektive Ein-Elektronen-Schrödingergleichungen) erhält man einen Satz an KS-Orbitalen als Eigenfunktionen der KS-Operatoren, der in Form einer (antisymmetrischen) Slater-Determinante zusammengefasst wird. Diese Slater-Determinante dient lediglich zur Konstruktion der korrekten Elektronendichte und stellt keine vernünftige Wellenfunktion dar. Hieraus lassen sich die kinetische Energie der Elektronen, das externe Potential sowie die klassische Coulomb-Wechselwirkung der Elektronen berechnen. Bis hierhin unterscheidet sich die DFT-Methode nicht wesentlich von einer Wellenfunktions-Methode. Die moderne KS-DFT profitiert demnach von der bereits lang umforschten und effizient implementierten Hartree-Fock-Maschinerie. Der bedeutende Unterschied der DFT liegt in der Berechnung der Austausch-Korrelations-Energie. Diese wird in der Regel numerisch für ausgewählte Gitterpunkte berechnet, da moderne Dichtefunktionale teilweise sehr abstruse Formen annehmen und eine analytische Behandlung sich demnach sehr schwierig gestaltet.\n\nMit leistungsfähigen Computern können heute Systeme von bis zu rund 1000 Atomen mittels DFT-Rechnungen behandelt werden. Für größere Systeme müssen andere Näherungsverfahren wie die Tight-Binding-Methode oder auf DFT-Ergebnissen basierende Näherungsverfahren verwendet werden.\n\nDFT-Verfahren können – genau wie traditionelle quantenchemische \"ab initio\" Verfahren – zur Strukturbestimmung von mehratomigen Systemen herangezogen werden. Sind Atompositionen, Kernladungen und Zahl der Elektronen bekannt, so kann man mithilfe des Kohn-Sham-Formalismus für eine gegebene Kerngeometrie die Totalenergie des Systems berechnen. Auf diese Weise lassen sich Potentialenergieflächen abbilden und über eine sogenannte Geometrieoptimierung Minimums- oder Übergangszustandsgeometrien charakterisieren. Da die Dichtefunktionaltheorie jedoch in der Praxis lediglich eine Näherung zur Lösung der exakten Schrödingergleichung darstellt, entstehen dabei zwangsläufig Fehler. Diese sind im Allgemeinen aber einerseits weitgehend systematisch (abhängig von der Art der Bindung und des gewählten Funktionals) und andererseits gut dokumentiert bzw. quantifizierbar. Auch wenn die systematische Verbesserbarkeit der Genauigkeit weniger stark ausgeprägt ist als bei \"Ab-initio\"-Verfahren, hat sich mit der sogenannten \"Jacob’s Ladder\" dennoch ein Art Hierarchie der DFT-Methoden etabliert.\n\nWeiterhin können eine Reihe von spektroskopischen Eigenschaften von Molekülen berechnet oder gar ganze Spektren vorhergesagt werden. Sorgfältige experimentelle Daten dienen unter anderem zur Kalibrierung von DFT-Methoden. Die Dichtefunktionaltheorie kann also zur Verifizierung oder Interpretation von bestehenden Daten oder sogar zur Vorhersage genutzt werden. Letzteres verlangt jedoch oftmals die Verwendung empirischer Korrekturschemata, um verlässliche Ergebnisse zu erzielen. Der nachfolgenden Liste können einige beispielhafte (spektroskopische) Anwendungen der DFT entnommen werden:\nÜber statistische Modelle basierend auf den Ergebnissen einer DFT-Rechnung sind zudem thermodynamische Größen wie etwa die Enthalpie oder Entropie eines Systems zugänglich. Darüber hinaus können thermochemische Größen wie Atomisierungs-, Reaktions- und Bindungsenergien berechnet werden. Eine weitere Anwendung finden DFT-Methoden in der Bestimmung von Molekülstrukturen in Festkörpern, die zur rechengestützten Kristallstrukturanalyse genutzt werden können.\n\nEs gibt zahlreiche Erweiterungen der Theorie, z. B. Spindichte- oder Stromdichtefunktionaltheorien, oder etwa sog. dynamische Dichtefunktionaltheorien, die zwar alle erwähnenswert sind, aber hier im Einzelnen nicht besprochen werden können, zumal das Gebiet nach wie vor sehr im Fluss ist.\n\n\n"}
{"id": "1271040", "url": "https://de.wikipedia.org/wiki?curid=1271040", "title": "Windows Image Acquisition", "text": "Windows Image Acquisition\n\nWindows Image Acquisition (WIA) ist eine Bildverarbeitungsschnittstelle, die mit TWAIN verglichen werden kann. WIA dient dem Datenaustausch zwischen Grafiksoftware und Scannern, Digitalkameras sowie anderen Digital-Video-Geräten. Die meisten neuen Scanner unterstützen WIA, auch wenn sie nicht ausdrücklich in der von Microsoft zur Verfügung gestellten Liste (siehe unten) aufgenommen sind.\n\nWIA erfährt derzeit noch keine Unterstützung durch das .NET-Framework. Softwareentwickler, welche die WIA-Funktionalität aus .NET heraus benutzen wollen, müssen daher auf direkte DLL-Aufrufe oder ActiveX-Komponenten zurückgreifen.\n\n\n\n"}
{"id": "1284853", "url": "https://de.wikipedia.org/wiki?curid=1284853", "title": "GNU/Linux-Namensstreit", "text": "GNU/Linux-Namensstreit\n\nAls GNU/Linux-Namensstreit wird eine Debatte zwischen den Anhängern der Freie-Software-Bewegung und jenen des Open-Source-Lagers darüber bezeichnet, ob Betriebssysteme, die auf dem\nund den\nbasieren, als \"Linux\" oder als \"GNU/Linux\" zu bezeichnen sind. Während sich zumeist die kürzere Bezeichnung \"Linux\" durchgesetzt hat, verwenden Projekte wie Debian, Knoppix u. v. a. die Bezeichnung \"GNU/Linux\".\n\nIm Rahmen des GNU-Projekts rund um seinen Gründer Richard Stallman wurde bereits in den frühen 1980er-Jahren mit der Entwicklung des freien Betriebssystems GNU begonnen. Einen Kernel gab es hierfür zunächst nicht, erst im Jahr 1990 begann die Entwicklung des projekteigenen Betriebssystemkerns GNU Hurd, die jedoch sehr viel schleppender verlief als jene des 1991 von Linus Torvalds erstmals angekündigten Linux-Kernels.\n\nIn den Anmerkungen zur Erstveröffentlichung von Linux schrieb Torvalds: „Leider bringt ein Kernel selbst einen nicht weiter. Um ein funktionierendes System zu bekommen, benötigt man eine Shell, Compiler, Bibliotheken usw. Dies sind separate Teile und können unter einem strengeren (oder lockereren) Copyright stehen. Die meisten Tools, welche mit Linux verwendet werden, sind GNU Software […].“ Aus der Perspektive Torvalds sowie vieler früher Linux-Nutzer wurde das Betriebssystem also durch diverse GNU-Pakete komplettiert – während aus der Perspektive des GNU-Projekts das jahrelang entwickelte Betriebssystem GNU durch den Kernel Linux vervollständigt wurde.\n\nRichard Stallman und das GNU-Projekt plädieren dafür, den Namen GNU/Linux zu verwenden. Das Hauptziel ist nicht, einer Verwechslung zwischen dem Systemkernel \"Linux\" und dem gesamten System \"GNU/Linux\" vorzubeugen. Es geht dabei vielmehr darum, die Rolle des GNU-Projektes und seines Idealismus für den Aufbau der freien Software-Gemeinschaft anzuerkennen, sowie der Öffentlichkeit die Bedeutung dieser Ideale in Erinnerung zu rufen. Immerhin gibt es Nutzer, die das GNU/Linux-System verwenden, ohne dass ihnen \"GNU\" ein Begriff ist. Stallman, das von ihm initiierte GNU-Projekt und die später von ihm gegründete Free Software Foundation (FSF) verfolgen bestimmte politische Ziele (nämlich die Propagierung und Verbreitung von Freier Software), wohingegen Torvalds seinen Linux-Kernel lieber als \"apolitische\" Open-Source-Software begreift, ohne dass diesem die \"freiheitskämpferischen\", \"netzpolitischen\" und \"bürgerrechtlichen\" Aspekte des Konzepts der Freien Software inhärent sein sollen. Somit sieht Stallman durch das Weglassen des „GNU“ in GNU/Linux die Gefahr, dass das GNU-Projekt verwendet wird, um die politischen Ziele der Open-Source-Szene statt der eigenen zu fördern.\n\nAus diesem Grund bezeichnen einige Linux-Distributionen, die sich als Teil der Freie-Software-Bewegung begreifen, ihre Distributionen oft als „GNU/Linux“ (z. B. Debian, Trisquel, gNewSense), während Distributionen, die keinen Wert auf diese politische Festlegung legen, ihre Distributionen meist lediglich „Linux“ nennen (z. B. openSUSE, Ubuntu, Fedora). Dieses Detail ist oft ein guter Indikator für die generelle Philosophie einer Linux-Distribution. So wird man bei Distributionen, die sich GNU/Linux nennen, nur selten Softwarepakete vorfinden, die unter einer Lizenz verbreitet werden, die nicht als kompatibel mit den Zielen der Free Software Foundation angesehen wird, wie z. B. Flash-Plugins oder MP3-Codecs; viele bieten sogar als Alternative zum Distributions-Standardkernel eine Version des Linux-Kernels an, die keine Komponenten enthält, die unter einer nicht von der FSF anerkannten Lizenz stehen, genannt Linux-libre. Im Kontrast dazu tendieren Distributionen, die sich lediglich als „Linux“ bezeichnen, oft dazu, eine Vielzahl von nicht-FSF-konformen Paketen anzubieten und installieren diese oft sogar automatisch in ihrer Standardeinstellung (z. B. Linux Mint). Als weiteres Beispiel sei die Distribution Ubuntu angeführt, dessen integrierte Amazon-Online-Suche nach Stallmans Meinung Spyware darstellt – was die Ideale der Freien Software-Bewegung verrate.\n\nLinus Torvalds hingegen schreibt: „Es spielt eigentlich keine Rolle, wie die Leute Linux nennen, solange dem Ehre entgegengebracht wird, dem Ehre gebührt (auf beiden Seiten). Persönlich werde ich weiterhin ‚Linux‘ sagen […] Die GNU-Leute versuchten, es GNU/Linux zu nennen, und das ist ok.“\n\nAls Richard Stallman den Namensstreit initiierte, war nahezu jedes linuxbasierte Gerät auch gleichzeitig GNU-basiert. Seit etwa 2004 boomen jedoch zahlreiche eingebettete Systeme wie Router oder Network Attached Storage für den SOHO-Bereich, die zwar Linux als Kernel verwenden, aber keine oder nur sehr wenige GNU-Komponenten enthalten. Um mit wenig Haupt- und Permanentspeicher auszukommen, werden Alternativen verwendet, die zwar nicht alle Funktionen der GNU-Versionen bieten, aber mit relativ geringen Hardwareressourcen auskommen. Oft wird uClibc anstelle von glibc als C-Bibliothek und BusyBox statt der Kommandozeilenbefehle von GNU und anderen Quellen eingesetzt. Einige eingebettete Systeme verwenden zwar die GNU-Bestandteile glibc oder die davon abgeleitete eglibc, nutzen jedoch BusyBox für die Kommandozeilenbefehle. Sie haben einen vergleichsweise geringen Anteil an GNU-Software.\n\nDas Smartphone- und Tabletbetriebssystem Android verwendet als C-Programmbibliothek Bionic sowie die Android toolbox für Kommandozeilenbefehle, die ähnlich dem BusyBox-Konzept als Multi-Call-Binary konzipiert ist, aber nur wenige Befehle und Optionen beinhaltet.\n\nSolche Systeme ohne oder mit nur wenig GNU-Programmcode bezeichnet auch die FSF nicht als GNU/Linux. Aufgrund der steigenden Verbreitung von internetfähigen Geräten, die nicht klassische PCs oder Notebooks sind (Internet der Dinge), nimmt der Prozentsatz von linuxbasierten Geräten, die tatsächlich einen relevanten Anteil an GNU-Software haben, kontinuierlich ab.\n\nNeben dem reinen Namensstreit kommt dem Begriff GNU/Linux daher immer mehr eine echte Bedeutung als Oberbegriff für klassische desktop- oder serverbasierte Linux-Distributionen zu. Trotzdem bleibt der Begriff im Alltagsgebrauch unüblich. Während Android meist als eigenes Betriebssystem wahrgenommen wird, bezeichnet man die Betriebssysteme in linuxbasierten Geräten mit uClibc und Busybox häufig einfach als \"Linux\", obwohl auch \"embedded Linux\" in Abgrenzung zu Desktop-Linux-Systemen üblich ist.\n\n"}
{"id": "1285744", "url": "https://de.wikipedia.org/wiki?curid=1285744", "title": "Trojanisches Pferd (Computerprogramm)", "text": "Trojanisches Pferd (Computerprogramm)\n\nAls Trojanisches Pferd (), im EDV-Jargon auch kurz Trojaner genannt, bezeichnet man ein Computerprogramm, das als nützliche Anwendung getarnt ist, im Hintergrund aber ohne Wissen des Anwenders eine andere Funktion erfüllt.\n\nTrojanische Pferde zählen zu den unerwünschten bzw. schädlichen Programmen, der sogenannten Malware. Der Begriff wird umgangssprachlich häufig synonym zu Computerviren sowie als Oberbegriff für Backdoors und Rootkits verwendet, ist davon aber klar abzugrenzen.\n\nDer Name ist metaphorisch vom Trojanischen Pferd der Mythologie abgeleitet. Der Legende nach konnte die unbezwingbare Stadt Troja nur durch einen Trick eingenommen werden: Die Angreifer präsentierten den Bewohnern ein riesiges Holzpferd als Friedensangebot. Im Inneren verbargen sich feindliche Soldaten, die so Zugang zum Stadtinneren erlangten. Seither ist der Begriff „Trojanisches Pferd“ gleichbedeutend mit „etwas Vortäuschen“. Analog dazu beabsichtigt ein Trojaner, als etwas Nützliches getarnt, durch den Angegriffenen selbst in den geschützten Bereich des Systems gebracht zu werden.\n\nDas ehemals verbindliche Regelwerk des Dudens ordnet die beiden gleichlautenden Begriffe, nach der Rechtschreibreform von 1996, merkwürdiger Weise unterschiedlich ein. So ist die Empfehlung des EDV-Begriffs – als „\"fester Begriff\"“ – die Kleinschreibung \"trojanisches Pferd\" und die des mythologischen „\"Namens\"“ die Großschreibung, welche die übliche ist.\n\nTrojanische Pferde sind Programme, die gezielt auf fremde Computer eingeschleust werden, aber auch zufällig dorthin gelangen können, und dem Anwender nicht genannte Funktionen ausführen. Sie sind als nützliche Programme getarnt, indem sie beispielsweise den Dateinamen einer nützlichen Datei benutzen, oder neben ihrer versteckten Funktion tatsächlich eine nützliche Funktionalität aufweisen.\n\nViele Trojanische Pferde installieren während ihrer Ausführung auf dem Computer heimlich ein Schadprogramm. Diese Schadprogramme laufen dann eigenständig auf dem Computer, was bedeutet, dass sie sich durch Beenden oder Löschen des Trojanerprogramms nicht deaktivieren lassen. So können u. a. eigenständige Spionageprogramme auf den Rechner gelangen (z. B. Sniffer oder Komponenten, die Tastatureingaben aufzeichnen, sogenannte Keylogger). Auch die heimliche Installation eines Backdoorprogramms ist möglich, die es gestattet, den Computer unbemerkt über ein Netzwerk (z. B. das Internet) fernzusteuern.\n\nTrojanische Pferde müssen jedoch nicht notwendigerweise ein Schadprogramm installieren. Jedes Programm, dem eine wichtige Funktionalität hinzugefügt wurde, die mit dem offensichtlichen Teil des Programms in keinem Zusammenhang steht, ist definitionsgemäß ein Trojanisches Pferd, solange die Funktion dem Anwender nicht genannt wird. Deshalb ist es sogar möglich, dass der versteckte Teil des Programms keinen direkten Schaden verursacht.\n\nZahlreiche Trojanische Pferde entstehen durch den Verbund zweier eigenständiger Programme zu einer einzelnen Programmdatei. Dabei heftet ein \"Linker\" (auch \"Binder\" oder \"Joiner\" genannt) das zweite Programm an eine beliebige ausführbare Wirtsdatei, ohne dass dieser Vorgang einen Einfluss auf die Funktionalität beider Programme hat. Durch den Start des ersten Programms wird so das versteckte zweite Programm unbemerkt mitgestartet. Der Autor des Trojanischen Pferdes kann mithilfe eines entsprechenden Dienstprogrammes jede beliebige ausführbare Datei als Wirtprogramm missbrauchen, ohne Programmierkenntnisse besitzen zu müssen.\n\nEs gibt Trojanische Pferde, die heimlich eine Installationsroutine starten. Diese Trojanerart wird häufig dafür eingesetzt, um unbemerkt Malware auf ein System zu installieren, sobald das Trojanische Pferd ausgeführt wird. Daher nennt man sie „Dropper“ (vom englischen \"to drop\" – etwas im System „ablegen“). Ein Autostartmechanismus sorgt in der Regel dafür, dass die Malware nach jedem Neustart des Rechners automatisch geladen wird. Für den Start der Malware ist das Trojanische Pferd auf diesem System nicht mehr erforderlich.\n\nDemgegenüber gibt es auch Trojanische Pferde, die die geheimen Funktionen in sich selbst bergen. Wird das Trojanische Pferd beendet oder gelöscht, so stehen auch die heimlichen Funktionen nicht mehr zur Verfügung. Ein Beispiel dafür sind zahlreiche Plug-ins. Bei einem Plug-in handelt es sich um eine Art Erweiterungsbaustein für ein bestimmtes Programm, mit dem weitere Funktionen hinzugefügt werden können. So kann ein als nützliches Browser-Plug-in getarntes Trojanisches Pferd auf einem Webbrowser laufen, um beispielsweise über den Browser mit dem Internet zu kommunizieren, wodurch es auf einfache Weise eine Firewall umginge.\n\nAllgemein ist es auch möglich, dass ein Trojanisches Pferd sich die externe Schnittstelle eines Programms zunutze macht. Ähnlich wie ein Plug-in-Trojaner benötigt auch diese Trojanerart ein bereits vorhandenes Programm des Anwenders. Oft nutzt es dabei die Möglichkeiten des Betriebssystems, das Programm in seiner Arbeit zu beeinflussen. So kann ein solches Trojanisches Pferd den vorhandenen Browser starten und ein unsichtbares Fenster öffnen, darüber eine Internetverbindung aufbauen und so Daten an den Angreifer schicken. Eine Firewall kann auch hier den heimlichen Verbindungsaufbau nicht verhindern, wenn die Verbindung zum Internet für den Browser erlaubt wurde. Der Vorteil dieser Methode gegenüber einem Plug-in-Trojaner ist, dass sie selbständig eine Internetverbindung aufbauen kann, also nicht erst, wenn der Webbrowser vom Anwender gestartet wurde.\n\nTrojanische Pferde können über jeden Weg auf einen Computer gelangen, mit dem Daten auf den Computer gebracht werden. Dies sind insbesondere Datenträger oder Netzwerkverbindungen wie das Internet (z. B. Tauschbörsen, präparierte Webseiten (\"siehe auch\" Drive-by-Download), Versand durch E-Mails). Die Verbreitung des Trojanischen Pferdes erfolgt danach durch den Anwender des Computers selbst. Je nach Attraktivität des Scheinprogramms steigt die Wahrscheinlichkeit, dass der Anwender das Programm an weitere Anwender weitergibt.\n\nFür die Verbreitung mittels E-Mails wird meistens ein Computerwurm verwendet, der das Trojanische Pferd transportiert. Der Trojaner selbst wird dadurch, dass er sich augenscheinlich verbreitet, jedoch nicht zu einem Virus. Vielmehr kommen hier zwei Schädlinge in Kombination zum Einsatz: Ein Wurm, der im Anhang das Trojanische Pferd transportiert.\n\nIm Jahr 2006 waren 55,6 % der vom Informationsverbund des Bundes registrierten Schadprogramme Trojanische Pferde, nur 9,9 % hingegen Viren. Schwachstellen in Browsern und Büroanwendungen werden mitunter schon am Tag des Bekanntwerdens ausgenutzt. Moderne Trojaner sind von Virenscannern nur noch schwer erkennbar.\n\nIn der Regel wird das Trojanerprogramm auf direktem Weg durch den Anwender eines Computers gestartet, wodurch es die Zugriffsberechtigung erhält, alle Funktionen zu nutzen, auf die auch der angemeldete Benutzer zugreifen darf. Die Schadroutine kann demnach selbstständig oder ferngesteuert alle Aktionen unentdeckt ausführen, die auch der Benutzer des Computers willentlich ausführen könnte (gleiches gilt für Schadprogramme aller Art, die ein Trojanisches Pferd heimlich auf dem Computer installieren). Da zahlreiche Nutzer aus Bequemlichkeit oder Unkenntnis dauerhaft mit Administrationsrechten arbeiten, ist das Spektrum an Manipulationsmöglichkeiten durch die Schadroutine unbegrenzt.\n\nHier einige typische Schadfunktionen:\n\n\nEs ist denkbar, dass der versteckte Programmteil des Trojanisches Pferdes keinen direkten Schaden verursacht. Sendet beispielsweise das Programm ohne Wissen des Anwenders unsensible Daten an den Programmierer, die in keinem Bezug zu dem Programm stehen, und lässt der offensichtliche Teil des Programms keinen Rückschluss auf die versteckte Funktionalität zu, so erfüllt das Programm alle Bedingungen, um als Trojanisches Pferd klassifiziert zu werden, obgleich es keinen direkten Schaden anrichtet. Dagegen kann eine geheime Funktion auch zu einer Schadroutine werden, ohne dass der Entwickler des Programms das beabsichtigt hat. Bezogen auf dieses Beispiel wäre das der Fall, wenn das Programm in einem vom Entwickler nicht vorhergesehenen Umfeld eingesetzt wird. Dort könnte die heimliche Datenübermittlung beispielsweise zum Aufbau einer Internetverbindung führen und so ungefragt Kosten verursachen.\n\nUnter Unix werden häufig verwendete Befehle wie \"ls\" (Auflisten von Dateien) oder \"ps\" (Anzeige der laufenden Prozesse) gerne durch Trojanische Pferde ersetzt. Zum einen fallen sie so lediglich bei einem Vergleich ihrer Checksummen auf, zum anderen erhöht sich dadurch die Wahrscheinlichkeit, dass ein Administrator das Trojanische Pferd startet, wodurch sie erweiterte Zugriffsrechte erlangen, ohne durch manipulierte Dateirechte aufzufallen.\n\nAnders als unter Unix wird bei einem Microsoft-Windows-Betriebssystem ein ausführbares Programm (Executable) nicht an seinen Dateirechten erkannt. Vielmehr legt hier die Endung des Dateinamens fest, ob und wie die Datei ausgeführt wird. Da Trojanische Pferde nur funktionieren können, indem jemand ihren Code startet, sind auch sie gezwungen, eine dementsprechende Dateiendung zu verwenden, wie beispielsweise codice_1 oder codice_2. In der Standardkonfiguration zeigt das Betriebssystem diese Dateiendungen im Explorer jedoch nicht an. Dadurch kann ein Trojanisches Pferd als Datei beliebiger Art maskiert sein. Viele ausführbare Dateiformate erlauben zusätzlich das Zuordnen von Icons zu einer Datei, so dass eine schädigende Datei „codice_3“ dem Benutzer namentlich nicht nur als „codice_4“ angezeigt wird, sondern auch noch das Icon einer Bilddatei erhalten kann und somit bei der oben genannten Windows-Konfiguration auf den ersten Blick nicht von einer ungefährlichen Bilddatei zu unterscheiden ist.\n\nEine weitere beliebte Möglichkeit der Maskierung besteht darin, eine Dateiendung mit Hilfe zahlreicher Leerzeichen zu kaschieren. So erscheint eine Datei namens „codice_5“ dem Anwender auf den ersten Blick wie eine Textdatei, wobei der restliche Dateiname von ihm oft nur als Hinweis interpretiert wird. Abhängig von dem Programm, das die Datei anzeigt, kann es auch vorkommen, dass nicht der komplette Dateiname zu sehen ist, wodurch der Anwender die *.exe-Endung der Datei gar nicht erst zu Gesicht bekommt. Da vielen Benutzern die Möglichkeit der Maskierung nicht geläufig ist, werden Trojanische Pferde häufig unbemerkt ausgeführt.\n\nEine weitere Möglichkeit, ausführbaren Code unter einer „harmlosen“ Dateiendung zu verstecken, bieten Programme, die den Dateityp unabhängig von seiner Endung selbst analysieren und sie entsprechend ihrem tatsächlichen Typ behandeln. Als Beispiel ist es zwar theoretisch nicht möglich, in einer RTF-Datei ausführbaren Makrocode zu hinterlegen, da dieses Dateiformat keine Makros unterstützt. Jedoch wird eine Datei namens „codice_6“, die man in „codice_7“ umbenennt, von Office anhand des Dateiinhalts als DOC-Datei erkannt, woraufhin der darin hinterlegte Makrocode trotz der Dateiendung codice_8 ausgeführt wird.\n\nTrojanische Pferde, die auf einem Exploit basieren, bilden hier ebenfalls eine Ausnahme. Sie nutzen Programmierfehler oder anderweitige Schwachstellen eines Programms aus, um ihren Code zur Ausführung zu bringen. Abhängig von dem Programm, auf dessen Schwachstelle das Trojanische Pferd basiert, kann es sich in jedem Dateityp verbergen, also auch in Dateien, die normalerweise nicht ausführbar sind. So gibt es beispielsweise Trojanische Pferde, deren Code in einer Grafikdatei hinterlegt wurde. Eine Schwachstelle des jeweiligen Browsers vorausgesetzt, ist es auch möglich, eine Internetseite derart zu präparieren, dass ein bloßer Aufruf der Seite zur Ausführung des Trojanercodes führt. Auch bei E-Mail-Programmen, die den HTML-Code einer Nachricht automatisch anzeigen, besteht die Gefahr, dass bösartiger Code bereits beim Lesen der Nachricht zur Ausführung gelangt. Der Trojanercode kann jedoch nur dann gestartet werden, wenn die belastete Datei tatsächlich mit dem Programm geöffnet wird, für das das Trojanische Pferd bestimmt ist.\n\nOftmals verwenden Trojanische Pferde auch Dateinamen, die es schwer machen, sie von wichtigen Systemdateien zu unterscheiden. Dazu legen sie sich meistens in unübersichtliche Verzeichnisse, wie z. B. im Systemordner von Windows. Werden sie über einen Autostarteintrag der Registry geladen, nutzen sie gerne auch Verschleierungstechniken wie diesen Eintrag: \"„c:\\windows\\system32\\userinit.exe \\\\localhost\\IPC$ -n“\". Bei einer Überprüfung aller Autostarteinträge wird eine mögliche Recherche im Internet ergeben, dass codice_9 ein regulärer Bestandteil des Betriebssystems ist. Und die Überprüfung der Datei wird dem Anwender bestätigen, dass es sich um das Original handelt (sogar mit möglichem Zertifikat). Auch \"„\\\\localhost\\IPC$“\" ist eine reguläre, vom System erstellte Standardfreigabe für interne Zwecke. Alles scheint in Ordnung zu sein, bis auf die Tatsache, dass hier nicht \"„c:\\windows\\system32\\userinit.exe“\" geladen wird, sondern \"„IPC$ -n.exe“\", welche im Verzeichnis \"„c:\\windows\\system32\\userinit.exe \\localhost\\“\" liegt (wobei unter den aktuellen Versionen von Windows das vermeintliche Leerzeichen vor „\"\\localhost\\\"“ tatsächlich ein Sonderzeichen sein muss, welches sich mit Alt+255 erzeugen lässt). Zusätzlich zu abweichenden Speicherorten einer Datei kann auch die Schreibweise des Dateinamens vom „Original“ abweichen, so soll etwa der Dateiname codice_10 an die Datei codice_11 erinnern.\n\nIm Unterschied zu einem Computervirus fehlt dem Trojanischen Pferd die Eigenschaft, sich selbständig zu verbreiten. Wird ein Dateivirus aufgerufen, so reproduziert er sich, indem er sich in fremde Dateien einschleust. Eine durch einen Virus \"infizierte Datei\" besteht somit aus zwei Komponenten: Aus der \"Wirtsdatei\" (einem beliebigen Programm) und dem dort angehängten \"Virus\".\n\nDadurch, dass das Wirtprogramm infiziert wurde, enthält es also eine versteckte Komponente, die nämlich beim Programmstart unbemerkt den Virus in das System lädt. Damit erfüllt die Wirtsdatei (nicht jedoch der Virus) alle Bedingungen, um auch als Trojanisches Pferd klassifiziert zu werden. Genau genommen ist somit jede durch einen Virus infizierte Datei ein Trojanisches Pferd. Die Virusdefinition hingegen umschließt lediglich den sich vermehrenden Virencode und seine Schadroutine, nicht jedoch die infizierte Datei, welche den Virus beherbergt.\n\nDiese exakte Unterscheidung wird in der Fachwelt selten vorgenommen. Ein zu klassifizierendes Programm bezeichnet man dort in der Regel erst dann als Trojanisches Pferd, wenn es nicht zufällig durch einen Virus, sondern gezielt durch seinen Entwickler oder mit Hilfe eines Tools um eine böswillige Komponente erweitert wurde. Damit wird der Sprachgebrauch jedoch nur zum Teil der parallel verbreiteten Definition gerecht.\n\nWenn der Programmierer des heimlichen Programmteils es vorgesehen hat, können Trojanische Pferde auch für die Verbreitung von Viren eingesetzt werden. So könnte ein als Spiel getarntes Trojanerprogramm mithilfe der Schadroutine z. B. Makroviren an Officedateien hängen, während das Spiel ausgeführt wird. Auf dem infizierten System würde das Trojanische Pferd nicht mehr benötigt, da sich der Virus nun automatisch verbreiten kann, sobald eine der infizierten Dateien geöffnet wird. Das Trojanische Pferd hat den Virus lediglich in das System geschleust.\n\nSchwer fällt die Unterscheidung zwischen Trojanischem Pferd und Virus, wenn beispielsweise die Schadroutine neben seiner sonstigen Funktion das Trojanische Pferd kopiert. Auf diese Weise kann es unbemerkt auf andere Datenträger gelangen. Dadurch, dass der eigene Programmcode heimlich reproduziert wird, erfüllt das Programm alle Bedingungen, um auch als Virus klassifiziert zu werden. Daher handelt es sich bei einer solchen Datei um ein Trojanisches Pferd und um einen Virus vereint in einem Programm.\n\nMitunter wird auch die durch ein Trojanisches Pferd heimlich installierte Malware als „Trojanisches Pferd“ bezeichnet. Bezogen auf den assoziativen Ursprung des Begriffs aus der griechischen Mythologie wäre laut dieser These nicht der zur Tarnung dienende Holzrahmen das Trojanische Pferd, sondern auch die darin versteckten Soldaten.\n\nAls Beispiel könnte ein Trojanisches Pferd heimlich ein Backdoor-Programm installieren. Ein Eindringling greift nun auf das installierte Programm zu, und nicht auf das Trojanische Pferd, was in diesem Fall lediglich als Hilfsprogramm für die heimliche Installation fungierte. Es kann danach jederzeit gelöscht werden, ohne dass dies einen Einfluss auf die weitere Funktion des Backdoor-Programms hat. Solche Hilfsprogramme sind definitionsgemäß Trojanische Pferde, weil sie sich als nützliche Anwendung ausgeben (z. B. als ein Spiel oder ein Bildschirmschoner) aber dabei dem Anwender nicht genannte Funktionen ausführen, die in keinem Zusammenhang mit dem offensichtlichen Teil des Programms stehen (hier die heimliche Installation des Backdoors).\n\nDie Mehrheit der als „verbreitet“ bezeichneten Trojanischen Pferde installieren oder beinhalten zwar Backdoorprogramme oder Rootkits, müssen diese jedoch nicht notwendigerweise enthalten. Es gibt verschiedene weitere Programme, die als Trojaner bezeichnet werden (z. B. solche, deren Schadroutine Anwenderdaten versendet).\n\nKnapp drei Jahre nachdem Daniel Edwards 1972 ein von ihm als „Trojan horse“ betiteltes theoretisches Konzept vorgestellt hatte, um eine besondere Rechnersicherheitsbedrohung zu charakterisieren, bewahrheitete sich seine Hypothese. Das Spiel „Pervading Animal“ aus dem Jahr 1975 wurde für die Univac 1108 geschrieben und wird als das erste bekannte Trojanische Pferd bezeichnet. Die Spielregeln sahen vor, dass der Spieler an ein Tier denken musste, welches das Programm durch gezielte Fragen zu erraten versuchte. Konnte das Tier noch nicht ermittelt werden, so aktualisierte das Programm sich selbst und stellte eine neue Frage, wobei jedes Mal die alte Version des Programms durch die aktualisierte Version überschrieben wurde. Zusätzlich kopierte sich das Programm aber heimlich auch in andere Verzeichnisse, sodass nach einer gewissen Zeit das komplette System mit Kopien dieses Programms vollgeschrieben wurde. Die Frage, ob es sich hierbei um einen Programmierfehler oder um eine beabsichtigte Schadensroutine handelte, ist bis heute unbeantwortet geblieben.\n\nDas Programm kopierte sich zwar in jedes Verzeichnis, doch es war klein, stopfte die Festplatte auch nicht zu, wie oben behauptet und wurde von den Systemadministratoren als Amusement gesehen:\n\nIn seinem Buch \"At the Abyss\" („Am Abgrund“) beschreibt Thomas C. Reed, früherer Sekretär der United States Air Force, einen Trojaner, der durch die Vereinigten Staaten heimlich industrieller Steuerungssoftware beigefügt wurde, die in die Sowjetunion geliefert wurde. Nach der Installation der Anlage an der Transsibirischen Gasleitung im Juni 1982 kam es zur Fehlfunktion, die eine große Explosion verursachte. Dies dürfte der erste Fall sein, wo ein Trojaner als Waffe in kybernetischer Kriegsführung im Rahmen des Kalten Krieges eingesetzt wurde.\n\n1984 stellte der Computer-Pionier Ken Thompson während seiner Turing-Award-Rede ein klassisches Beispiel eines Trojanischen Pferdes vor, das sicherheitstechnisch bedenklich und darüber hinaus schwer aufzuspüren wäre. Die Rede war von einem Login-Programm für Unix, das derart verändert wird, dass es zusätzlich zum normalen Passwort auch ein Generalpasswort akzeptiert. Diese Hintertür könne, so Thompson, ein entsprechend manipulierter C-Compiler beim Übersetzen des Login-Programms automatisch hinzufügen, wodurch der Quelltext des Login-Programms keinen Hinweis auf eine Manipulation liefert. Wenn der Compiler des C-Compilers entsprechend präpariert würde, wäre die Manipulation nicht einmal mehr aus dem Quellcode des C-Compilers ersichtlich.\n\nIm Dezember 1989 erschien das erste Trojanische Pferd, das seine Opfer erpressen sollte, womit es weltweite Aufmerksamkeit auf sich zog. Dr. Joseph W. Popp, ein damals 39 Jahre alter Wissenschaftler aus Cleveland, verschickte 20.000 belastete Disketten mit der Aufschrift „AIDS Information Introductory Diskette“ an Adressen in Europa, Afrika, Asien und der WHO. Sein Trojaner versteckte nach einiger Zeit sämtliche Verzeichnisse, verschlüsselte die Dateinamen und hinterließ auf dem Rechner eine Aufforderung, für die Wiederherstellung 378 US-Dollar an eine fiktive „PC Cyborg Corporation“ auf ein existierendes Postfach in Panama zu schicken. Obwohl der Täter in England für unzurechnungsfähig erklärt wurde, hat ihn ein italienisches Gericht in Abwesenheit zu zwei Jahren Haft verurteilt.\n\nIm August 2000 erschien das erste bekannte Trojanische Pferd für PDAs. Der auf den Namen „Liberty Crack“ getaufte Schädling wurde von Aaron Ardiri, dem Co-Entwickler des gleichnamigen Palm Game Boy Emulators, entwickelt. Er tarnt sich als Crack für den Emulator, löscht heimlich die installierte Software und initialisiert wichtige Einstellungen des Palms. Als das Trojanische Pferd außer Kontrolle geriet, half Ardiri, die Verbreitung einzudämmen.\n\nIm Oktober 2005 machte der renommierte Systemspezialist Mark Russinovich die Entdeckung, dass sich heimlich ein Rootkit auf seinem System installierte, als er eine kurz zuvor gekaufte Musik-CD von SONY BMG auf seinem Computer abspielte. Dank einer parallel laufenden Systemanalyse entdeckte er so per Zufall das erste Trojanische Pferd, das über legal erworbene Musik-CDs den Weg auf den Rechner fand. Der bewusst von SONY BMG in Umlauf gebrachte „XCP“-Trojaner war Teil einer sehr aggressiven Kopierschutzkampagne. Die heimlich installierte Malware sammelt Informationen über den Benutzer und schickt diese über das Internet an den Konzern. Zudem schafft sie neue Sicherheitslöcher und bremst aufgrund einer Designschwäche das System auch dann aus, wenn keine CD abgespielt wird. Bereits zwei Wochen nach dieser Entdeckung erschien „Ryknos“, das erste Trojanische Pferd, das sich der Sicherheitslücken von „XCP“ bediente und ein Backdoor-Programm auf den befallenen Rechnern installierte.\n\nSpätestens seit dem Jahr 2006 entwickelt das Bundeskriminalamt ein im Netzjargon „Bundestrojaner“ genanntes Programm zum Ausspähen von Daten zum Zwecke der Strafverfolgung.\n\nDen einzig wirkungsvollen Schutz vor Trojanischen Pferden bietet der Verzicht auf die Benutzung von Programmen aus unbekannten oder unsicheren Quellen. Als besonders gefährlich einzustufen sind hierbei, wie bei jeder Malware, Anbieter von Programmen bzw. Dienstleistungen am Rande der Legalität.\n\nViele Antivirenprogramme erkennen neben Computerviren auch weitere Malware, darunter eine Vielzahl bekannter Trojanischer Pferde. Ihre Erkennungsrate erhebt jedoch keinen Anspruch auf Vollständigkeit. Wird ein Trojanisches Pferd erkannt, bevor der Anwender es startet, ist der Schutzmechanismus recht wirkungsvoll, wohingegen bereits ausgeführte Trojanische Pferde von der Antivirensoftware nur bedingt zuverlässig aus dem System entfernt werden können. Gleiches gilt für die Schadsoftware, welche eventuell durch ein Trojanisches Pferd installiert wurde. Auch gelingt es zahlreichen Trojanischen Pferden, die Antivirensoftware zu deaktivieren oder das System derart zu manipulieren, dass sie von der Software nicht mehr entdeckt werden.\n\nPersonal Firewalls oder andere Programme zur Netzwerküberwachung bieten keinen Schutz vor der Installation eines Trojanischen Pferdes, können unter Umständen aber nach einer Infektion auf unautorisierte Netzwerkkommunikation aufmerksam machen. Einige Personal Firewalls bieten als zusätzlichen Schutz auch eine Überwachung der Autostarteinträge des Systems, was dem Anwender einen Hinweis auf eine Trojanerinstallation liefert, wenngleich auch die Firewallsoftware von zahlreichen Trojanischen Pferden deaktiviert und nicht selten überlistet werden kann.\n\nAls neuen Weg zum Schutz gegen Trojanische Pferde und Computerviren allgemein kann man die Bestrebungen der Trusted Computing Group (TCG) ansehen, die das Ausführen von ungeprüfter, d. h. nicht vertrauenswürdiger Software, technisch unterbindbar machen will bzw. die Funktionsaufrufe geprüfter und ungeprüfter Software voneinander zu isolieren versucht. Es bleibt aber zu bedenken, dass auf Grund des Prinzips Trojanischer Pferde, das menschliche Vertrauen oder die Unerfahrenheit auszunutzen, man auch auf diese technische Weise nur das bei der Installation von Software aufgebrachte Vertrauen auf eine andere Instanz verlagert.\n\nWird ein bereits installiertes Trojanisches Pferd erkannt, so ist es ratsam, die Bereinigung des Systems über die Einspielung des letzten „sauberen“ Abbildes der Festplatte (Image) vorzunehmen, da ein Softwareprodukt (z. B. Virenscanner) diese Aufgabe nur bedingt zuverlässig erledigen kann.\n\nEs gibt eine Testdatei, bekannt unter dem Namen EICAR, die man herunterladen kann, um festzustellen, wie detailreich ein Antivirenprogramm prüft. Dabei kann man die Datei als \".exe\", \".txt\", \".zip\" und als \".zip\" in einer \".zip\" -Datei verpackt finden.\nDer Code dieses Testvirus lautet: X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H*, wobei der durch „$“-zeichen abgegrenzte Teil nur Kommentar, der Rest Anzeichen von Schadcode ist.\n\n\n"}
{"id": "1287117", "url": "https://de.wikipedia.org/wiki?curid=1287117", "title": "Endian Firewall", "text": "Endian Firewall\n\nDie Endian Firewall ist eine Linux-Distribution des Südtiroler Unternehmens \"Endian\". Sie ist spezialisiert auf die Funktionen Router-, Firewall- und Gateway-Sicherheit. Wahlweise ist das Produkt als freie Software, als kommerzielle Software mit garantierten Supportleistungen oder auch komplett installiert als Hardware (Appliance) inklusive Supportleistungen erhältlich.\n\nDie Endian Firewall ist eine schlüsselfertige Linux Security Distribution, die sich als eigenständige, vereinheitlichte Sicherheitsmanagements-Lösung (Unified Threat Management) versteht. Die Endian Firewall basiert auf einem abgesicherten Linux-Betriebssystem. Das System wird über eine Boot-CD auf einem PC installiert. Nach wenigen Grundeinstellungen beginnt die Installation, welche die Partitionierung der Festplatte vornimmt und die Dateien überspielt. Danach kann der Rechner auch ohne Monitor (headless) betrieben werden. Eine Tastatur und ein Monitor werden nicht mehr benötigt, da die komplette Konfiguration des Servers über eine Webschnittstelle (siehe Abbildung „Die Webschnittstelle der Endian Firewall“) mittels eines anderen Computers erfolgt, der wahlweise über das Netzwerk verbunden wird – alternativ kann auch über die serielle Schnittstelle auf das System zugegriffen werden.\n\nDie Hauptaufgabe der Endian Firewall ist es, als Gateway, Router und Firewall sowie als Proxy für Web, E-Mail, FTP, SIP sowie DNS zu fungieren. Hierbei werden durch Endian bis zu vier unterschiedliche Netzwerkzonen verwaltet, siehe Abbildung „Schema der Netzwerktopologie“. Für jedes dieser Netzwerkzonen muss eine Netzwerkkarte in den Computer eingebaut werden. Auch sie werden über die Webschnittstelle konfiguriert. Bei Endian werden diese durch farbliche Kodierung differenziert (siehe hierzu auch Abbildung „Schema der Netzwerktopologie“):\n\n\nEs können jedoch noch weitere, zusätzliche Netzwerke verwaltet werden. Die Endian Firewall unterstützt hierbei auch Load-Balancing, d. h. man kann eine weitere Verbindung zum Internet dem roten Netz hinzufügen; Endian Firewall verteilt die Netzwerklast dann auf beide Schnittstellen.\n\nHinter der Endian Firewall steht die italienische Endian Spa aus Eppan, Südtirol und eine Community aus freiwilligen Entwicklern und Helfern. Das Lizenzmodell von Endian sieht eine kommerzielle Version und eine freie Version vor:\n\nDie aktuelle Version beinhaltet folgende Hauptfunktionen:\n\n\n\n\n\n\n\n\n\n\n\nDer Ursprung der Endian Firewall ist die Linux-Firewall IPCop, welche ihrerseits eine Abspaltung von Smoothwall ist. Bedingt durch zahlreiche Weiterentwicklungen wird laut Endian gegenwärtig nur noch ein Fünftel des ursprünglichen IPcop-Code verwendet. Beispielsweise nutzt Endian heute den RPM Package Manager, was eine vereinfachte Wartung mit sich bringt und häufige langwierige Kompilierzeiten, wie sie bei LFS-basierten Distributionen wie IPcop üblich sind, vermeidet. Neuere Versionen basierten zunächst auf Linux From Scratch und ab Version 2.2 auf RHEL bzw. auf CentOS. Mit der kommenden Version 3.0 soll die Endian Firewall praktisch „Smoothwall-“ und „IPcop-frei“ werden.\n\nDer größte Unterschied zu IPCop ist, dass sich die Endian Firewall nicht mehr lediglich als reine Router/Firewall-Kombination versteht, sondern als umfassende Gateway-Sicherheitslösung (Unified Threat Management). Hierzu sind ein Virenscanner und ein Spam-Blocker fest in die Distribution integriert worden. Damit kann der Verkehr von HTTP, FTP, POP3 und SMTP in Echtzeit gescannt und gegebenenfalls gefiltert werden. Als völlig neue Merkmale wurden weiterhin mehrere WAN-Verbindungen (für einfache Lastverteilung, Failover) und eine WLAN-Hot-Spot-Funktion integriert.\n\nDarüber hinaus wurden im Vergleich zu IPCop die Menüs um viele Punkte verfeinert, womit eine genauere Konfiguration der einzelnen Dienste ermöglicht wird, was allerdings auch die Komplexität erhöht.\n\nZusammenfassend kann man sagen, dass die Endian Firewall verglichen mit IPCop in Bezug auf Gateway-Sicherheit die umfassendere Lösung ist, als Kompromiss dazu jedoch eine etwas umfangreichere Konfiguration und spürbar höhere Anforderungen an die Hardware-Ausstattung in Kauf genommen werden müssen (es werden für die Nutzung der kompletten Funktionalität 512 MB Arbeitsspeicher sowie 1 GHz Prozessortakt empfohlen, beziehungsweise 256 MB und 500 MHz als Mindestausstattung genannt).\n\nBezüglich der Entwicklung und des Geschäftsmodells unterscheidet sich Endian zu seinen Ursprüngen Smoothwall und IPCop wie folgt:\n\nSeit den Community-Versionen 2.3 und 2.4 werden in den einschlägigen Userforen rund um Endian zunehmend kritische Stimmen laut. Es werden in der Hauptsache drei Themen bemängelt:\n\n\n"}
{"id": "1287456", "url": "https://de.wikipedia.org/wiki?curid=1287456", "title": "BOSS-CUBE", "text": "BOSS-CUBE\n\nBOSS-CUBE ist ein elektronisches Handelssystem der Deutsche Börse AG bzw. der Deutsche Börse Systems AG für den Einsatz an der Börse. Der Name setzt sich aus den Abkürzungen für „Börsen-Order-Service-System“ und „Computer-Unterstütztes-Börsenhandels-und-Entscheidungssystem“ zusammen.\n\nMit dem System BOSS werden Orders durch den Händler erstellt und an den Kursmakler weitergeleitet, der den Preis der gehandelten Aktie feststellt. Das System zeigt dem Kursmakler auch die Orderhistorie und unterstützt damit die Kursbestimmung. CUBE ist ein Verfahren zur Kursbestimmung. Im Jahr 2000 wurde BOSS-CUBE in Xontro umbenannt. Das System stellt den Teilnehmern Marktinformationen in Echtzeit zur Verfügung.\n\nZuvor wurde seit 1992 das Order-Routing-System BOSS/Böga an allen deutschen Börsen eingesetzt, \n\n\n"}
{"id": "1295409", "url": "https://de.wikipedia.org/wiki?curid=1295409", "title": "Depth-Sort-Algorithmus", "text": "Depth-Sort-Algorithmus\n\nDer Depth-Sort-Algorithmus (englisch wörtlich „Tiefensortierungs-Algorithmus“) ist in der Computergrafik ein Algorithmus zur Verdeckungsberechnung. Er wurde 1972 von den Brüdern Martin E. Newell und Richard G. Newell sowie Tom Sancha vorgestellt.\n\nDer Grundgedanke besteht darin, die zu zeichnenden Polygone nach ihrer Entfernung vom Betrachter zu sortieren und sie dann, mit dem am weitesten entfernten Polygon beginnend, alle nacheinander zu zeichnen. Dabei werden bereits gezeichnete Teile von näher liegenden Objekten überschrieben, wenn sie sich ganz oder teilweise überlappen. Wenn das Sortieren ordnungsgemäß ausgeführt wurde, liefert diese Vorgehensweise eine korrekte Ansicht verdeckter Oberflächen.\n\nDas Sortieren von zwei Polygonen \"P\" und \"Q\" nach Tiefe (Z-Richtung) geschieht in mehreren Schritten.\n\nDie Polygone müssen planar sein, das heißt, alle Eckpunkte müssen auf einer Ebene liegen. Die Prüfung, ob sich alle Eckpunkte auf einer Ebene befinden, wird durch Einsetzen der Koordinaten aller Punkte in die Ebenengleichung durchgeführt.\n\nDie Reihenfolge der Schritte ist so gewählt, dass die einfachen Tests zuerst und die komplexeren Prüfungen zum Schluss angewendet werden, um weniger Rechenzeit zu benötigen.\n\nDer Depth-Sort-Algorithmus verwendet viel weniger Speicherressourcen als beispielsweise der häufiger verwendete Z-Buffer-Algorithmus zum Berechnen verdeckter Oberflächen, ist diesem aber in der Geschwindigkeit deutlich unterlegen.\n\n\n"}
{"id": "1296504", "url": "https://de.wikipedia.org/wiki?curid=1296504", "title": "Liste von Computeranimationsfilmen", "text": "Liste von Computeranimationsfilmen\n\nDiese Liste zeigt Filme in Spielfilmlänge, die komplett computeranimiert sind. Damit ist hier ausschließlich 3D-Animation gemeint, als Abgrenzung zu 2D-Animationsfilmen, die inzwischen ebenfalls zum größten Teil oder vollständig im Computer entstehen. 3D-Animation bezieht sich nicht auf 3D-Filme, die mit einer speziellen Brille im Kino gesehen werden, sondern auf eine konkrete Animationstechnik, mit der die Filme produziert werden.\n\nDie folgende Tabelle ist eine Zusammenstellung der erfolgreichsten Computeranimationsfilme. Als Kriterium wird ihr jeweiliges gerundetes Einspielergebnis in US-Dollar aus verkauften Kinokarten herangezogen. Die Einspielergebnisse sind nicht inflationsbereinigt, ebenso wurde der schwankende Dollarkurs bei der Umrechnung der Ergebnisse von den jeweiligen Landeswährungen in Dollarbeträge nicht berücksichtigt. Die Zahlen sind daher relativ und repräsentieren nicht die tatsächlichen Besucherzahlen.\n"}
{"id": "1297117", "url": "https://de.wikipedia.org/wiki?curid=1297117", "title": "Dragon 32, Dragon 64", "text": "Dragon 32, Dragon 64\n\nDragon 32 und Dragon 64 sind auf dem 6809E-Mikroprozessor basierende Heimcomputer des walisischen Herstellers Dragon Data Ltd. Die Unterschiede in der Namensgebung der beiden Modelle beziehen sich auf die Konfiguration des Arbeitsspeichers (RAM) im Auslieferungszustand, nämlich 32 oder 64 Kilobyte (KB).\n\nDer Dragon 32 wurde Ende 1981 als überdurchschnittlich leistungsfähiges Gerät für das untere Preissegment des rasch wachsenden britischen Heimcomputermarktes konzipiert. Im Gegensatz zu seinen direkten Konkurrenzmodellen Sinclair ZX Spectrum und Acorn BBC Micro mit Z80- bzw. 6502-Mikroprozessor setzte Dragon Data Ltd. seine Hoffnungen auf den in Europa wenig beachteten 6809-Mikroprozessor. Sein Einsatz war hauptsächlich durch die größere Leistungsfähigkeit und nicht zuletzt durch günstige Verkaufskonditionen seitens des Herstellers Motorola motiviert. Die große Ähnlichkeit der ersten Prototypen des Dragon 32 mit dem US-amerikanischen Tandy Color Computer sorgte anfänglich für patentrechtliche Verstimmungen, die bis zum Produktionsstart durch kleinere technische Modifikationen ausgeräumt werden konnten. Zudem versah Dragon Data Ltd. seine Computer mit einer erweiterten Version der Programmiersprache Microsoft BASIC, die eine einfache Benutzung der umfangreichen Grafikmöglichkeiten erlaubt.\n\nDie Markteinführung des Dragon 32 erfolgte im August 1982 in Großbritannien. In Deutschland wurde das Gerät durch die Noris Computer-Vertriebs-GmbH (Norcom) ab Anfang 1983 in den Handel gebracht. Der Dragon 64 kam im September in den USA und im November in Europa hinzu. Beide wurden bis 1984 von Dragon Data Ltd. und dessen US-amerikanischem Lizenznehmer Tano Microcomputer Products Corp. produziert. Nach der Insolvenz von Dragon Data Ltd. führte das spanische Unternehmen Eurohard S.A. die um modifizierte Geräte erweiterte Produktion bis Mai 1986 fort.\n\nDragon 32 und 64 sind zu den verschiedenen Modellen des Tandy Color Computers weitestgehend softwarekompatibel.\n\nZur Abwendung wirtschaftlicher Schwierigkeiten beschloss der britische Spielzeughersteller Mettoy Anfang der 1980er-Jahre, mit einem eigenen Gerät in den rasch wachsenden lukrativen Markt der Heimcomputer einzusteigen. Mit der Umsetzung des Projekts wurde die 1981 eigens dafür gegründete Tochtergesellschaft Dragon Data Ltd., die auch namensgebend für das zu entwickelnde Produkt sein sollte, betraut.\n\nEine der Vorgaben für die von Mettoy im Oktober 1981 beauftragten Entwickler von PA Technology (PAT) of Cambridge war die Forderung nach einer leistungsfähigen und preiswerten Alternative zu den damals in Großbritannien populären Heimcomputern mit einer auf dem Z80- oder 6502-Mikroprozessor basierenden Systemarchitektur. Die Entscheidung fiel auf den von der europäischen Computerbranche wenig beachteten 6809E-Mikroprozessor von Motorola, der herausragende Grafikfähigkeiten des zu entwickelnden Computers versprach. Des Weiteren sollte aus Effizienzgründen zu dessen Betrieb auf einen Standardchipsatz mit Speicherverwaltungs- und Peripheriebausteinen von Motorola zurückgegriffen werden, der auch im bereits 1980 in den USA eingeführten Tandy Color Computer zum Einsatz kam. Zur Vermeidung absehbarer patentrechtlicher Probleme mit diesem Nachbau und im Hinblick auf leichte Bedienbarkeit änderten die Entwickler einige technische Details. Beispielsweise wurde die Druckerschnittstelle parallel ausgeführt, die Benutzung von 64 KB Arbeitsspeicher ermöglicht und Tandys unhandliche Gummitastatur durch eine vollwertige Schreibmaschinentastatur ersetzt. Analog den Produkten anderer Hersteller sollte der Rechner über eine eingebaute höhere Programmiersprache verfügen. Die Wahl fiel dabei auf ein speziell angepasstes Extended Microsoft BASIC, das den einfachen Einsatz von hochaufgelöster Grafik erlaubt.\n\nDas fertige Vorserienmuster des Dragon 32 mit dem internen Codenamen „Pippin“ und 16 KB RAM wurde nach etwa dreimonatiger Entwicklungszeit Weihnachten 1981 der Geschäftsführung von Mettoy vorgestellt, abgenommen und anschließend durch PA Technology of Cambridge zur Serienreife gebracht. Nachdem bereits ein Großteil der ersten 10.000 Hauptplatinen durch den Zulieferer Race-Electronics hergestellt worden war, erfolgte insbesondere mit Hinblick auf den konkurrierenden ZX Spectrum von Sinclair und den BBC Micro von Acorn kurz vor Auslieferung aus Mettoys Endmontagefabrik im walisischen Swansea eine nachträgliche Aufstockung des Arbeitsspeichers um weitere 16 KB auf insgesamt 32 KB.\n\nDie ersten als „Familiencomputer“ beworbenen Geräte kamen im August 1982 für 199 Pfund Sterling in die britischen Verkaufsfilialen verschiedener Warenhausketten. Aufgrund der guten technischen Ausstattung des Dragon 32 und durch Lieferschwierigkeiten bei Commodore, Sinclair und Acorn konnten in den ersten sechs Verkaufsmonaten insbesondere durch das Weihnachtsgeschäft etwa 32.000 Geräte abgesetzt werden. Beflügelt von diesem großen Erfolg wurde zwischenzeitlich an der Erschließung des gesamten europäischen Marktes gearbeitet. Eigens zu diesem Zweck wurden Niederlassungen wie beispielsweise Dragon Data (France) gegründet und Verträge mit externen Distributoren wie der für den Deutschlandvertrieb gewonnenen Noris Computer Vertriebs-GmbH (Norcom) geschlossen. Eine Besonderheit stellte der finnische Lizenznehmer Finlux dar, der seine Geräte mit einem eigenen Finlux-Typenschild versah.\n\nTrotz gut gehender Computerverkäufe hatten sich die wirtschaftlichen Schwierigkeiten für Mettoy gegen Ende des Jahres 1982 weiter verschärft, so dass die Mehrheitsanteile der Heimcomputersparte Dragon Data Ltd. im November 1982 an ein Konsortium unter Führung des Technologie-Investors Pru-tech veräußert werden mussten; lediglich 18,61 Prozent der Anteile der neu gegründeten Aktiengesellschaft Dragon Data Ltd. verblieben bei Mettoy. Mithilfe des eingebrachten frischen Kapitals in Höhe von 2,4 Mio. Pfund Sterling (etwa 9 Mio. DM) konnte die Produktionskapazität durch ein neues Werk in Kenfig nahe Port Talbot erweitert und an der Entwicklung von Nachfolgemodellen und eines Diskettenlaufwerks für den Dragon 32 weitergearbeitet werden. Daneben fasste Dragon Data Ltd. beispielsweise durch Vorstellung seiner Geräte auf internationalen Fachmessen neue außereuropäische Absatzmärkte und Produktionsstätten ins Auge und forcierte deren Erschließung. Bis Mitte des Jahres 1983 fanden so seit Markteinführung insgesamt über 100.000 Geräte, hauptsächlich in Großbritannien, ihre Abnehmer. Der Dragon 32 hatte sich damit bereits ein Jahr nach seinem Erscheinen als feste Größe auf dem britischen Heimcomputermarkt etabliert, die Verkaufszahlen im Juli 1983 wurden lediglich vom unangefochtenen Marktführer Sinclair ZX Spectrum übertroffen.\n\nAb Herbst 1983 führte die Steigerung des Produktionsvolumens, mangelnder Nachschub an zeitgemäßer Software und zwischenzeitlich aufgekommene starke Konkurrenz durch den Commodore 64 und Atari 600XL jedoch zu einer Marktsättigung; die angepeilten Verkaufszahlen wurden deutlich verfehlt. Bereits vorproduzierte Geräte konnten nicht verkauft werden und eine erneute Kapitalspritze in Höhe von 2,5 Mio. Pfund Sterling durch Pru-tech, die mit personellen Änderungen in der Leitung von Dragon Data Ltd. einherging, war im September 1983 notwendig geworden. Die neue Führungsebene beschloss umgehend eine Erweiterung der Produktpalette, wobei die baldige Fertigung des bereits in Entwicklung befindlichen Dragon 64 mit 64 KB RAM und RS-232-Anschluss und die des Dragon-Diskettensystems besondere Priorität genoss.\n\nDer Verkauf des Dragon 64 begann im September 1983 in den USA und einige Monate später auch in Europa. Vom Dragon 32 wurde daraufhin eine um zusätzliche 32 KB RAM aufgerüstete Variante angeboten und Käufern des alten Modells eine entsprechende Aktualisierung ihrer Geräte bei den Vertragshändlern vor Ort angeboten. Die Aufrüstung erwies sich sowohl in den Fabriken als auch bei den Kundendienstmitarbeitern als zu kostenintensiv. Ab Ende Dezember, rechtzeitig zum Weihnachtsgeschäft, ermöglichte Dragon Data Ltd. den Besitzern eines Dragon 32 daher, ihr Gerät gegen Aufpreis in einen Dragon 64 umzutauschen. Unterstützt werden sollte die Umtauschaktion durch die Veröffentlichung technischer Dokumentationen in Form des Buches \"Inside the Dragon\" sowie die Einführung des lange erwarteten UNIX-nahen Betriebssystems OS-9, zu dessen Betrieb ein 64-KB-Dragon-Computer mit Diskettenlaufwerk unumgänglich ist. Trotz nun erhältlicher leistungsfähiger Programmiersprachen und Anwendungsprogramme standen viele Dragon-32-Besitzer dem Umtauschangebot ablehnend gegenüber.\n\nDurch die geringer als erwartet ausgefallene Akzeptanz des Dragon 64 im Heimcomputermarkt brach der Verkauf bis Anfang des Jahres 1984 drastisch ein. Zur Vermeidung weiterer absehbarer Einbußen von Marktanteilen wurde daraufhin im Februar 1984 auf Druck der Hauptanteilseigner von Dragon Data Ltd. die Vermarktung der Dragon-Produkte der externen, Pru-tech nahestehenden General Electric Company (GEC) übertragen. Von GEC angestoßene Werbeoffensiven in Zeitschriften und der neue in Großbritannien für die Dragon-64-Computer eingeführte Markenname \"GEC DRAGON\" sollten die Wende bringen.\n\nIn Westdeutschland kamen der Dragon 64 und das Dragon-Diskettenlaufwerk im Frühjahr 1984 für 1290 DM bzw. 1300 DM zunächst im Exklusivvertrieb von Friedrich M. Hunold auf den Markt; der Aufpreis für den ebenfalls in Westdeutschland angebotenen Umtausch eines Dragon 32 lag zu diesem Zeitpunkt bei 348 DM. Einem breiteren Publikum vorgestellt wurde der Dragon 64 in Westdeutschland jedoch erst im Mai 1984 auf der \"Hobby-tronic\"-Messe. Wohl infolge der damit verbundenen angestiegenen Nachfrage übernahm kurz darauf das Unternehmen Norcom mit seinem bereits für den Dragon 32 etablierten Vertriebsnetz auch den Verkauf des Dragon 64 in Westdeutschland.\n\nDen verstärkten Vermarktungsbemühungen von GEC nicht vertrauend, strichen wichtige britische Großabnehmer wie die Warenhauskette British Home Stores Ltd. die Dragon-Computer entweder ganz aus dem Verkaufsprogramm oder aber ergänzten dieses wie etwa bei der Drogeriekette Boots UK Ltd. um weitere Konkurrenzprodukte. Auch lagen die Verkäufe in den USA durch den Lizenznehmer Tano weit hinter den Erwartungen von GEC zurück – es wurden weniger als 5000 Geräte abgesetzt und die wirtschaftlichen Ziele damit deutlich verfehlt. Gleichzeitig fielen enorme Kosten durch den laufenden Produktionsbetrieb und die von GEC vorangetriebene Entwicklung neuer Computermodelle an, so dass sich innerhalb kurzer Zeit erneut hohe Schulden anhäuften. Deren Bedienung durch weitere finanzielle Hilfen verweigerten sich die Hauptanteilseigner um Pru-tech jedoch, so dass Mitte Juni 1984 die Insolvenz von Dragon Data Ltd. unausweichlich wurde.\n\nObgleich bis zum Zeitpunkt der Insolvenz weltweit insgesamt etwa 200.000 Dragon-Computer ihre Abnehmer gefunden hatten und somit genug potentielle Kundschaft für weitere Produkte und Programme zur Verfügung stand, gestalteten sich die Übernahmeverhandlungen mit den zahlreichen Interessenten schwierig. Die Insolvenzmasse übernahm schließlich Ende Juli 1984 die bis dahin in der Heimcomputerbranche völlig unbekannte spanische Eurohard S.A. für geschätzte 1 Mio. Pfund Sterling, nachdem Verhandlungen beispielsweise mit Tandy Corporation kurz zuvor gescheitert waren. Sämtliche Produktionslinien und Entwicklungsabteilungen aus Port Talbot wurden vom Neubesitzer nach Spanien verlagert. Der Ausverkauf der Lagerbestände von etwa 13.000 Geräten (Computer und Diskettenlaufwerke) im Wert von geschätzten 4 bis 6 Mio. Pfund Sterling oblag GEC, denen auch der zukünftige Vertrieb für Großbritannien bis Frühjahr 1985 vorbehalten war. Als Servicedienstleister für die bestehende Kundschaft und Ansprechpartner für Softwareentwickler fungierte die zuvor von ehemaligen Dragon-Mitarbeitern ausgegründete Touchmaster Limited.\n\nDie spanischen Fertigungsstätten nahmen die Produktion im November 1984 auf und belieferten den europäischen Markt zunächst mit technisch unveränderten Dragon-Geräten. Mehrere Neuerscheinungen wie der Dragon 200, Dragon 200-E und Dragon MSX (Neuentwicklung mit Z80-CPU, inkompatibel zu Dragon 32 und 64) schlossen sich 1985 an und rundeten die Produktpalette ab. Von April 1985 an übernahm der Distributor Compusense Ltd. die Versorgung des neben Spanien größten Absatzmarktes Großbritannien. Im ersten Produktionsjahr verkaufte Eurohard S.A. europaweit etwa 17.000 Geräte, rund weitere 20.000 Geräte gingen an öffentliche Bildungseinrichtungen in Spanien. Durch die im Sommer 1985 aufgekommenen 16-Bit-Heimcomputer wie Amiga und Atari ST erlahmte das Interesse an den technisch veralteten Dragon-Geräten zunehmend. Infolge der damit verbundenen rückläufigen Verkäufe wurde die Produktion mit einem seit 1984 laufenden täglichen Ausstoß von 500 Geräten schrittweise gedrosselt und nach umfangreichen Ausverkäufen ab Ende 1985 schließlich im Mai 1986 ganz eingestellt.\n\nDie Grundgeräte enthalten zwei Platinen mit den verschiedenen Baugruppen, Tastatur, Peripherieanschlüsse, Bildschirmausgabe und Spannungsregelung für das externe Netzteil. Auf der Hauptplatine befinden sich die Rechnereinheit mit 6809E-CPU (engl. \"central processing unit\"), der Arbeits- (RAM) und Festwertspeicher (ROM), Peripherieanschlüsse und die beiden parallelen Schnittstellen für Erweiterungen.\n\nDie Hardware-Architektur der Dragon-Computer basiert auf einem 6809E-Mikroprozessor von Motorola. Durch die interne Verarbeitungsbreite von 16 Bit ist diese CPU insbesondere im Bereich arithmetischer Operationen deutlich leistungsfähiger als die zudem höher getakteten Z80- oder 6502-Mikroprozessoren, wie sie etwa in den direkten Konkurrenzprodukten Sinclair ZX81, Apple II oder Atari 400/800 verbaut wurden.\n\nDie 6809E-CPU kann auf einen Adressraum von 65536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobytes (KB) festlegt. Aus praktischen Gründen ist es üblich, für Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Dieser wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer von der CPU benutzbare Adressraum unterteilt sich beim Dragon 32 im Wesentlichen in vier unterschiedliche Sektionen: 32 KB RAM ($0000–$7FFF), 16 KB ROM ($8000–$BFFF), einen etwa 16 KB umfassenden freien Adressbereich ($C000–$FEFF) und einen Block mit den Hardwareregistern des Speicherverwaltungsbausteins 6883-SAM (engl. \"Synchronous Address Multiplexer\") und der beiden Ein-/Ausgabe-Bausteine ($FF00–$FFFF). Sämtliche Adressierungsvorgänge der 6809E-CPU laufen über den 6883-SAM, die zweite zentrale Verarbeitungseinheit im Dragon. Diesen vom Anwender programmierbaren IC obliegen zudem die Erzeugung des Systemtaktes und die Zugriffe des Standard-Grafikchips 6847-VDC (engl. \"Video Display Controller\") auf den im RAM befindlichen Videospeicherbereich.\n\nDas RAM enthält die Systemvariablen, den Videospeicher und Bereiche, die zur Programmierung mit der im ROM befindlichen Kombination von Betriebssystem („BIOS“) und BASIC genutzt werden können. Unmittelbar auf das ROM folgen fast 16 KB unbelegten Speichers, der beim Dragon 32 für Steckmodule oder andere Hardware am Erweiterungssteckplatz („Expansionsport“ oder auch „Steckmodulschacht“), einer parallelen Schnittstelle, reserviert ist.\n\nDer Dragon 64 verfügt über 64 KB RAM und zwei jeweils 16 KB große ROMs, in denen geringfügig voneinander verschiedene BASIC-Versionen untergebracht sind. Nach dem Einschalten wird zunächst das erste ROM für den 32-Modus aktiviert. Es stehen dann 30 KB freies RAM zur Verfügung. Per Befehlseingabe wird beim Dragon 64 das zweite ROM in den RAM-Bereich $C000–$FEFF kopiert, im Bereich $8000–$BFFF RAM eingeblendet und die kopierten ROM-Inhalte werden ausgeführt („Bootstrapping“). Im 64-Modus stehen damit 45 KB RAM für die Programmierung mit BASIC zur Verfügung. Zur visuellen Unterscheidung der beiden Betriebsarten blinkt der Bildschirmcursor im 64-Modus blau, im 32-Modus dagegen im gewohnten Schwarz auf grünem Grund.\nDer 6847-VDC übernimmt das Auslesen und die Darstellung der Grafikdaten am angeschlossenen Fernsehgerät oder Monitor. Dabei stellt er verschiedene Grafikmodi wie Text, hochaufgelöste Pixelgrafik und die sogenannte Semigrafik zur Verfügung. Von den acht möglichen Vollgrafikmodi werden lediglich fünf durch das eingebaute BASIC unterstützt. Die höchstmögliche Auflösung beträgt 256 × 192 Bildpunkte in zwei Farben (auswählbar aus zwei vorgegebenen Farbgruppen), bei geringeren Auflösungen sind bis zu vier Farben (aus zwei vorgegebenen Farbgruppen) möglich. Im Semigrafik-Modus können alle acht verfügbaren Farben gleichzeitig dargestellt werden: Im Textmodus (32 × 16 Zeichen) werden den Textzeichen mit einer Auflösung von 8 × 12 Punkten dabei farbige Grafikblöcke geringerer Auflösung (vier Blöcke à 4 × 6 Punkte oder sechs Blöcke à 8 × 3 Punkte) zugeordnet.\n\nDie vom Grafikchip benötigten elektronischen Kontrollsignale werden mithilfe zweier ebenfalls zu Motorolas 6883-Standardchipsatz gehörender 6821-PIA-Bausteine (engl. \"Peripheral Interface Adapter\") erzeugt. Zum Aufgabenbereich dieser beiden Ein-/Ausgabeschaltkreise zählen auch das Auslesen der Tastatur, Interruptbehandlung, das Betreiben der Ein- und Ausgabeports, die Tonerzeugung (Rechteckgenerator) und die Ansteuerung von Massenspeichergeräten.\n\nIm Dragon 64 befindet sich ein weiterer Spezialbaustein zur Implementierung des RS-232-Protokolls, der 6551-ACIA (engl. \"Asynchronous Communications Interface Adapter\").\n\nAls Verbindungen zur Außenwelt sind ein Erweiterungssteckplatz, ein Kassetteninterface, zwei Joystickanschlüsse für analoge Joysticks, eine RGB-Monitorbuchse, ein TV-Anschluss und beim Dragon 64 eine RS-232-Schnittstelle vorhanden.\n\nIn Zusammenhang mit Heimcomputern der frühen 1980er-Jahre kamen als Massenspeicher hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die günstigste Variante der Datenaufzeichnung durch Audiokassetten hat i. A. den Nachteil geringer Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren. Bei Erscheinen des Dragon standen diesem lediglich Kassettenrekorder als Massenspeicher zur Verfügung, Diskettensysteme kamen erst einige Monate später hinzu.\n\nIn den folgenden Abschnitten sollen lediglich die jeweils bekanntesten Systeme Erwähnung finden.\n\nDie Dragon-Computer verfügen ab Werk über ein Kassetteninterface zum Aufzeichnen und Auslesen von Daten durch handelsübliche Kassettenrekorder. Als Speichermedien dienen entsprechende Kompaktkassetten. Die Übertragungsrate beträgt standardmäßig – wie beim konkurrierenden Sinclair ZX Spectrum auch – durchschnittlich 1500 Bit/s, ein im Vergleich zu anderen zeitgenössischen Mitbewerbersystemen beispielsweise von Commodore (300 Bit/s) oder Atari (600 Bit/s) hoher Wert.\n\nAnfang 1984 veröffentlichte das Unternehmen IKON Computer Products einen auf Minikassetten basierenden Datenrekorder. Dieses bis auf einen Auswurfknopf ohne weitere Bedientasten ausgelieferte \"Ultra Drive\" wird mithilfe eines am Expansionsport angeschlossenen Interfaces und darin enthaltener Software nebst BASIC-Befehlen betrieben. Bei einer Übertragungsrate von 1200 Bit/s können pro Kassette bis zu 200 KB Daten gespeichert werden.\n\nWie im September 1982 bekannt geworden, sollte sich die für Oktober 1982 geplante Auslieferung des Diskettensystems von Dragon Data Ltd. aufgrund technischer und wirtschaftlicher Schwierigkeiten erheblich verzögern. Die bestehende große Nachfrage einer Vielzahl von Dragon-Benutzern veranlasste daher die Drittanbieter Premier Microsystems Ltd. und Compusense Ltd. bis Februar 1983 eigene Diskettensysteme zu entwickeln. Insbesondere das günstigere, von Premier Microsystems Ltd. ab April 1983 verkaufte Delta Disc System erfreute sich großer Beliebtheit und konnte sich rasch am Markt etablieren. Bis zum Verkaufsstart des Diskettensystems von Dragon Data Ltd. im September 1983 fanden etwa 1000 Einheiten von Premier Microsystems Ltd. und einige weitere seines Lizenznehmers Cumana Ltd. in Großbritannien ihre Abnehmer, womit Dragon Data Ltd. nicht zu vernachlässigende Marktanteile abspenstig gemacht wurden.\n\nDie beiden zueinander nicht kompatiblen Diskettensysteme von Dragon und Premier Microsystems werden jeweils durch Einstecken eines Hardware-Interfaces („Disc Controller“) am Expansionsport der Dragon-Computer betrieben. Das Interface beinhaltet die Ansteuerungselektronik für die über ein Flachbandkabel angeschlossene externe Laufwerksmechanik und das zugehörige Diskettenbetriebssystem wie beispielsweise Dragon DOS oder Delta DOS. Pro 5¼-Zoll-Diskette können mit dem System von Dragon maximal 180 KB und mit dem von Premier Microsystems bzw. Cumana Ltd. je nach Ausführung der Laufwerksmechanik (40 oder 80 Spuren, einfache oder doppelte Schreibdichte, einseitiges oder doppelseitiges Beschreiben) bis zu 720 KB an Daten gespeichert werden. Das von Premier Microsystems bis Anfang 1985 auch separat angebotene Interface erlaubt neben der Nutzung von 5¼-Zoll-Mechaniken zudem die Anbindung von damals ebenfalls gebräuchlichen 8- und 3-Zoll-Geräten.\n\nDurch die Verwendung eines Interfaces mit fest installierter Systemsoftware konnte der Arbeitsspeicherbedarf zum Betrieb des Diskettensystems sehr gering gehalten werden, allerdings um den Preis eines für weitere Steckmodule oder Peripherie blockierten Erweiterungsschachtes. Die ab 1984 zur Verfügung stehenden Diskettenbetriebssysteme OS-9 und FLEX werden dagegen von Diskette in den Arbeitsspeicher geladen, wonach der Expansionsport wieder zur freien Verfügung steht. \n\nNeben der QWERTY-Schreibmaschinentastatur mit 53 Tasten (keine Escape-, Caps- und Control-Taste) und den analogen Joysticks von Dragon Data Ltd. waren weitere Geräte zur Eingabe erhältlich. Dazu zählt die am Expansionsport anzuschließende Maltafel \"Touchmaster Tablet\" oder der Lichtgriffel \"Trojan Light Pen\" von Drittherstellern.\n\nIm Laufe der Zeit erschienen für den Dragon 32 und dessen Nachfolger viele Erweiterungen und Umrüstbausätze unterschiedlichen Umfangs, wobei im Folgenden nur die wichtigsten aufgezählt werden.\n\nDas ab August 1983 von J.C.B. Microsystems als Steckmodul erhältliche \"Sound Extension Module\" enthält den in vielen Spielautomaten und Heimcomputern verbauten AY-3-8910-Synthesizerbaustein. Er verfügt über umfangreiche Möglichkeiten zur Tonerzeugung und -beeinflussung und ist den ab Werk im Dragon vorhandenen Möglichkeiten weit überlegen. Die Ansteuerung erfolgt bequem über einen vom Steckmodul für das Extended BASIC bereitgestellten speziellen Befehl. Das etwas früher auf den Markt gekommene Sprachsynthesemodul \"Speech Synthesis Module\" erlaubt durch fünf zusätzliche BASIC-Befehle die Übergabe der zu sprechenden Worte in Textform, wobei der Umfang der erzeugbaren Wörter durch die verwendete Allophon-Technik unbegrenzt ist.\n\nDiese von Compusense Ltd. hergestellte und vertriebene Erweiterungskarte wird mit wenigen Handgriffen direkt an die Hauptplatine der Computer angeschlossen. Nach der Umrüstung stehen zusätzliche 64 KB RAM und ein 6845-Grafikchip mit separatem Videospeicher nebst zusätzlicher Monitorbuchse zur Darstellung eines 80-Zeichen-Textmodus (80 × 24 Zeichen) zur Verfügung. Der nachgerüstete Arbeitsspeicher dient bei Benutzung der Betriebssysteme OS-9 oder FLEX als virtuelles Diskettenlaufwerk („Ramdisk“) und ermöglicht so im Zusammenspiel mit den erweiterten Textmöglichkeiten ein schnelles und bequemes Arbeiten, beispielsweise mit dem Textverarbeitungsprogramm \"Edit+\".\n\nMithilfe dieser im Oktober 1985 vorgestellten Erweiterung von Lucidata of Cambridge werden zusätzliche Erweiterungsmöglichkeiten für die Dragon-Computer bereitgestellt. Dazu zählen zwei 20-polige Parallel-Schnittstellen die mit entsprechender Softwareunterstützung den Betrieb von Peripheriegeräten des damals in Großbritannien weit verbreiteten BBC-Microcomputer-Systems am Dragon 32 oder 64 erlauben. Damit stehen jedem Dragon anstatt nur eines nun mehrere, mit entsprechender Software wie OS-9 oder FLEX simultan nutzbare Erweiterungsschnittstellen zur Verfügung. Dies ermöglicht u. a. den Einsatz leistungsfähigerer Entwicklungsumgebungen durch den gleichzeitigen Betrieb von Diskettenlaufwerken und Steckmodulen, auf denen sich beispielsweise Programmierhilfen wie etwa Assembler oder auch Compiler befinden können.\n\nGleichzeitig mit der Markteinführung des Dragon 32 im Sommer 1982 waren auch hochwertige Softwaretitel, darunter mehrheitlich Adaptionen beliebter Arcade-Automatenspiele, erhältlich. Bereits im Herbst 1982 kamen viele Programme unabhängiger Dritthersteller hinzu, was in erster Linie auf die wenig restriktive Softwarepolitik von Dragon Data zurückzuführen ist. Entgegen den Gepflogenheiten der damaligen Zeit verzichtete Dragon Data auf gängige Lizenzierungsmodelle und gab Drittherstellern freie Hand bei der Entwicklung und dem Vertrieb eigener Dragon-Software, verbunden mit der Hoffnung auf erhöhte Computerabsätze. Trotz des entwicklerfreundlichen Umfelds und der Leistungsfähigkeit des im Dragon verbauten Mikroprozessors stießen die Dragon-Computer nicht bei allen etablierten europäischen Softwareherstellern auf Gegenliebe. Ursächlich hierfür waren vor allem mangelnde Erfahrungen in der Programmierung des kaum verbreiteten 6809-Mikroprozessors, die eine Portierung damaliger Verkaufsschlager gängiger Computersysteme als wirtschaftlich unrentabel erschienen ließen. Aus diesem Grund bestand die Software der Dritthersteller bis ins zweite Quartal des Jahres 1983 hinein zum größten Teil aus Lizenzversionen von Programmen der in Amerika weitverbreiteten und weitestgehend softwarekompatiblen Modelle des Tandy Color Computers. Mit Einführung des Dragon 64 weiteten die Hersteller, allen voran Dragon Data Ltd. und Compusense Ltd., das Angebot für die ebenfalls neu veröffentlichten Systemprogramme OS-9 und FLEX massiv auf leistungsfähige Programmiersprachen und Anwendungsprogramme aus.\n\nWie bei anderen Heimcomputern der 1980er-Jahre auch erfolgte der Vertrieb kommerzieller Dragon-Software auf verschiedenen Datenträgern. Die insbesondere bei Spieleherstellern beliebten preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes allerdings sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Bei den in der Herstellung vielfach teureren Steckmodulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen von großem Vorteil war. Den besten Kompromiss zwischen Ladezeit, Verlässlichkeit und Speicherkapazität erzielten die Disketten, wobei die zur Verwendung benötigten kostspieligen Diskettenlaufwerke jedoch erst etwa ein Jahr nach Markteinführung des Dragon 32 erhältlich waren.\n\nEine weitere Quelle und beliebte Art zur Verbreitung von Software für Dragon-Computer waren die in Zeitschriften enthaltenen Programmtexte („Listings“) zum Abtippen. Daneben waren für die Dragon-Computer eine Vielzahl von Büchern unterschiedlicher Qualität, hauptsächlich mit BASIC-Übungen und BASIC-Programmsammlungen erhältlich.\n\nVon der in Umlauf befindlichen Software machten illegale Kopien („Raubkopien“) stets einen großen Teil aus und stellten damit kleinere Softwareentwickler häufig vor existentielle wirtschaftliche Schwierigkeiten. Daraufhin wurden zunehmend Kopierschutzsysteme insbesondere bei Spielen als der meistverkauften Software eingesetzt.\n\nIn den nachfolgenden Abschnitten wird lediglich die jeweils bekannteste Software vorgestellt.\n\nDie Konfiguration der Dragon-Hardware, wozu auch die Ansteuerung der Kassettenschnittstelle gehört, fällt in den Aufgabenbereich des Betriebssystems – der „Firmware“. Zur Optimierung des Zusammenwirkens mit dem ebenfalls auf den 16 KB umfassenden Dragon-ROMs befindlichen Extended-BASIC-Interpreter sind beide programmtechnisch sehr eng miteinander verwoben. Für den Betrieb von Diskettensystemen sind weitere Systemprogramme („Disc Operating System“, DOS) nötig. Diese wurden entweder auf ROM (Delta DOS, Dragon DOS) oder Diskette (OS-9, FLEX) ausgeliefert.\n\nDem BIOS der Dragon-Computer liegt Motorolas Standardsoftware zur Steuerung eines 6809-basierten Referenzsystems zugrunde. Neben Anpassungen zur Unterstützung von technischen Eigenheiten der Dragon-Computer, die auch den Einsatz des Extended-BASIC-Interpreters betrafen, wurden von den Dragon-Entwicklern hauptsächlich Verbesserungen zur Leistungssteigerung vorgenommen. Beispielsweise konnte durch eine optimierte Abfrage der Tastatureingaben gegenüber dem Tandy Color Computer, der ebenso weitestgehend auf Motorolas 6809-Referenzsystem basiert, eine um bis zu 15 Prozent höhere Gesamtverarbeitungsgeschwindigkeit der Dragon-Computer erzielt werden.\n\nBeide DOS wurden auf ROMs jeweils fest in den Hardware-Schnittstellen („Interfaces“) der Diskettensysteme ihrer Hersteller, Premier Microsystems und Dragon Data Ltd., verbaut. Jedes der Systeme erlaubt den gleichzeitigen Betrieb von maximal vier Diskettenlaufwerken, wobei das Delta DOS erweiterte Konfigurationsmöglichkeiten für deren Mechaniken unterstützt. Gemeinsam ist ihnen ebenfalls die einfache Integration in die Systemumgebung der Dragon-Computer durch das Bereitstellen neuer BASIC-Befehle ohne dabei nennenswerten zusätzlichen Arbeitsspeicher zu belegen. Allerdings sind Delta DOS und Dragon DOS durch die in den Interfaces verbauten unterschiedlichen elektronischen Diskettencontrollerbausteine nicht zueinander kompatibel: mit Dragon DOS beschriebene Disketten können nicht mit Delta DOS verwendet werden und umgekehrt.\n\nDelta DOS zeichnet sich durch flexiblere Nutzungsmöglichkeiten aufgrund des standardmäßig vorhandenen Direktzugriffs auf einzelne Bytes von Diskettensektoren („Random Access“) aus. Zudem verfügt es über einen größeren Befehlsumfang, eine bessere Dokumentation und die intuitivere Bedienung. Das mit Dragon DOS betriebene Diskettensystem dagegen ist schneller und wurde preiswerter angeboten, erschien jedoch erst einige Monate später als sein Konkurrent. Dragon DOS erfordert zum Betrieb den 32-Modus bei Dragon-64-Computern, womit nur noch 23 KB freier Arbeitsspeicher zur Verfügung stehen. Sollen die vollen 64 KB RAM des Dragon 64 zusammen mit der Dragon-Diskettenstation benutzbar sein, ist entweder OS-9 oder FLEX erforderlich. \n\nKurz nach dem Markteintritt des Dragon 64 veröffentlichte Dragon Data Ltd. 1983 das von Microware Systems Corporation lizenzierte UNIX-nahe Betriebssystem OS-9. Zu dessen Gebrauch wird mindestens ein Dragon-Computer mit 64 KB Arbeitsspeicher und das Dragon-Diskettenlaufwerk nebst Dragon DOS benötigt, wobei das Dragon DOS lediglich zum Laden der OS-9-Diskette dient und danach abgeschaltet wird. Mithilfe von OS-9 war es erstmals möglich, mehrere Programme simultan auf den Dragon-Computern auszuführen („Multitasking“). Damit konnte beispielsweise die Ansteuerung eines Druckers („Spooling“ als Hintergrundprozess) und die Eingabe neuer Daten in ein Textverarbeitungsprogramm (als Vordergrundprozess) gleichzeitig erfolgen. Zusammen mit der Netzwerkfähigkeit („Multiuser“) und durch die Verfügbarkeit von nahezu 64 KB Arbeitsspeicher war die Effizienzsteigerung bei der Computerarbeit beträchtlich. Zusätzlich wurde die Bildschirmdarstellung um Kleinschreibung und diverse Grafikzeichen erweitert, was für die Benutzerfreundlichkeit ebenso von großem Vorteil war. Damit und durch eine umfangreiche Programmbibliothek hoffte Dragon Data Ltd. weitere Märkte, vor allem im professionellen Geschäftsumfeld, erschließen zu können.\n\nDer Drittanbieter Compusense Ltd. stellte 1984 seine in direkter Konkurrenz zu OS-9 stehende Dragon-Adaption des bereits 1977 in den USA entwickelten Betriebssystems FLEX vor. Das auf Diskette befindliche Programmpaket enthält neben dem Diskettenbetriebssystem, kleineren Hilfsprogrammen und Treibern für verschiedene Drucker auch einen leistungsstarken Makroassembler, der anfänglich für viele Dragon-Benutzer den eigentlichen Grund zum Erwerb von FLEX darstellte. Bei Erscheinen war eine Fülle weiterer Programme beispielsweise zum gemeinsamen Betrieb von FLEX mit dem Extended BASIC der Dragon-Computer sowie Textverarbeitungen und Datenbankanwendungen erhältlich. Durch die Einsteigerfreundlichkeit, die umfangreichen Dokumentationen, die dem 6809-Prozessor besser angepasste Software-Architektur, die Unterstützung von Computerterminals via RS-232-Schnittstelle und nicht zuletzt durch das umfangreiche Programmangebot gab Eurohard S.A. als Nachfolger von Dragon Data Ltd. alsbald FLEX als neuem offiziellen Betriebssystem für Dragon-Computer den Vorzug und stellte seine Unterstützung für OS-9 daraufhin ein.\n\nAufbauend auf der Systemsoftware kam dem benutzerspezifischen Einsatz der Dragon-Computer in unterschiedlichsten Anwendungsgebieten große Bedeutung zu. War dabei die Bearbeitung einer Aufgabenstellung mit z. B. käuflich zu erwerbenden Programmen aus technischen oder wirtschaftlichen Gründen nicht möglich oder sollte beispielsweise neuartige Unterhaltungssoftware produziert werden, so musste dies mithilfe von entsprechenden Programmiersprachen in Eigenregie geschehen.\n\nDie Erstellung zeitkritischer Actionspiele und Anwendungen in der Regelungstechnik erforderten Anfang der 1980er-Jahre eine optimale Nutzung der Hardware insbesondere des Arbeitsspeichers. Im Heimcomputerbereich war dies ausschließlich durch die Verwendung von Assemblersprache mit entsprechenden Übersetzerprogrammen, den Assemblern, möglich. Die Auslieferung von Assemblern erfolgte in vielen Fällen mit einem zugehörigen Editor zur Eingabe der Programmanweisungen („Sourcecode“), häufig auch als Programmpaket mit Debugger und Disassembler zur Fehleranalyse. Im professionellen Entwicklerumfeld kamen vielfach Cross-Assembler zum Einsatz. Damit war es möglich, ausführbare Programme für Heimcomputer auf leistungsfähigeren und komfortabler zu bedienenden Fremdcomputerplattformen zu erzeugen. Beispielsweise erfolgte die Entwicklung des Dragon-BIOS auf einem PDP-11-Computer von DEC.\n\nZu den am weitesten verbreiteten 6809-Assemblern für die Dragon-Computer zählen Editor/Assembler (Dragon Data Ltd, 1983), Dream Assembler (Dragon Data Ltd, 1983, 1984 als Alldream Assembler neu aufgelegt), Encoder09 (Premier Microsystems Ltd., 1983, auch als Aufrüst-ROM-Baustein für das Delta-DOS-Interface erhältlich), DASM (Compusense Ltd., 1983, Inline-Assembler zum Gebrauch mit Extended BASIC), MACE (Windrush Micro Systems Ltd., 1983, auch Auslieferung als Steckmodul mit kombiniertem EPROM-Brenner) und der Editor Assembler von Alligata Software.\n\nProgrammiereinsteiger bevorzugten in vielen Fällen die übersichtlichen und einfach zu bedienenden, dafür aber weniger leistungsfähigen Programmier-Hochsprachen.\n\nDas zusammen mit den Dragon-Computern ausgelieferte und leicht zu erlernende Extended Microsoft BASIC ermöglicht durch seinen äußerst leistungsfähigen Befehlssatz und die verfügbaren 32 KB Arbeitsspeicher eine einfache Programmierung selbst schwieriger Probleme. Nachteilig auf die Einsetzbarkeit wirkten sich die in der Natur des Interpreters liegenden prinzipiellen Beschränkungen wie etwa die geringe Ausführungsgeschwindigkeit und der große Arbeitsspeicherbedarf aus. Erwähnenswert ist die Inkompatibilität des im Dragon verbauten Extended Microsoft BASIC mit dem der ansonsten weitestgehend softwarekompatiblen Tandy Color Computer, so dass BASIC-Programmtexte beider Systeme nicht ohne Weiteres untereinander austauschbar sind.\n\nDem Extended Microsoft BASIC stand mit Dragon-Logo eine weitere, leicht erlernbare Interpretersprache zur Seite.\n\nAls Mittelweg zwischen Interpreter-Hochsprache (langsam in der Ausführung, aber gut lesbare Sourcecodes und einfache Fehleranalyse) und Assemblersprache (schwer zu erlernen und umständlich zu handhaben, aber Anfang der 1980er-Jahre alternativlos zur Erzeugung schneller und speichereffizienter Programme) etablierten sich auch im Heimcomputerbereich im Laufe der 1980er-Jahre die Compiler-Hochsprachen. Die Ausführungsgeschwindigkeit der damit erzeugten Maschinenprogramme war im Vergleich zu interpretierten Programmen wie beim eingebauten Extended Microsoft BASIC sehr viel größer, reichte aber nicht ganz an die von Assemblern erzielte heran. Die Geschwindigkeitsnachteile gegenüber assemblierten Programmen wurden jedoch vielfach zugunsten eines leichter zu wartenden Sourcecodes in Kauf genommen.\n\nMit Einführung des Dragon 64 und der Diskettenbetriebssysteme OS-9 und FLEX stand dem Anwender ein großes Angebot von Compilersprachen wie Pascal, COBOL, Forth, Fortran und C zur Verfügung.\n\nDie Programmpalette für die Dragon-Computer umfasste neben den Programmiersprachen zum Erstellen eigener Applikationen auch eine große Auswahl an vorgefertigter kommerzieller Anwendungssoftware. Sie deckte verschiedenste Themenbereiche wie Textverarbeitung (u. a. Editext, Telewriter, Rainbow Writer, Edit+, Stylograph), Tabellenkalkulation (u. a. Dynacalc), Datenbanksysteme (u. a. RMS) und Bildbearbeitung (Picture Writer) ab. Zusätzlich existierte eine Vielzahl an Programmen zum Kopieren von Datenträgern, Ansteuern von Peripherie oder zum Einsatz in sehr speziellen Gebieten wie beispielsweise zur Prozessverwaltung in der Landwirtschaft (FarmFax).\n\nDen mit Abstand größten Teil der sowohl kommerziellen als auch frei erhältlichen Dragon-Software stellen die Spiele dar. Am beliebtesten waren in erster Linie Action-Spiele wie \"Chuckie Egg\" (1983, A&F Software), \"Donkey King\" (1983, Microdeal), \"Jet Set Willy\" (1985, Software Projects) und \"Airball\" (1989, Microdeal).\n\nZu den bekanntesten Herstellern von Dragon-Spielen zählte neben Dragon Data Ltd. das in Großbritannien ansässige Unternehmen Microdeal. Zwischen 1982 und 1988 brachte allein Microdeal etwa 200 verschiedene Spiele unterschiedlicher Qualität in den Handel, wobei es sich bei einem nicht geringen Teil davon um detailgetreue Nachahmungen bekannter Arcade-Vorbilder handelt. Zur Vorbeugung von absehbaren Streitigkeiten mit den Rechteinhabern wurden diese Titel häufig unter einem dem Original ähnelnden Namen angeboten: \"Donkey Kong\" wurde zu \"Donkey King\" (wegen drohender Klagen später abgeändert in \"The King\"), \"Galaga\" wurde zu \"Galagon\", \"Scramble\" wurde zu \"Skramble\" usw. Weitere Bekanntheit erlangte Microdeal durch sein Maskottchen „Cuthbert“, dem eine Reihe von beliebten, aber ebenfalls häufig anderen Bestsellern nachgeahmten Jump-’n’-Run-Spielen gewidmet sind. Der Vertrieb von \"Cuthbert in the Jungle\" beispielsweise musste wegen allzu auffälliger Ähnlichkeit mit \"Pitfall II!\" auf Betreiben Activisions eingestellt werden.\n\nInsgesamt über einhundert verschiedene Hersteller trugen neben den unzähligen Publikationen in Zeitschriften ihren Anteil an der umfangreichen Spielepalette für die Dragon-Computer bei. Abgesehen von den von Dragon Data Ltd. produzierten Steckmodultiteln wurden kommerzielle Dragon-Spiele meist auf Kompaktkassette mit mehr oder weniger wirksamen Kopierschutzmechanismen ausgeliefert.\n\nIn den 1980er-Jahren spielten neben den Fachbüchern die Computerzeitschriften für viele Heimcomputerbesitzer eine große Rolle. Die häufig monatlich erschienenen Ausgaben enthielten Testberichte zu Neuheiten, Programmieranleitungen und Software zum Abtippen. Sie dienten weiterhin als Werbe- und Informationsplattform sowie zur Kontaktaufnahme mit Gleichgesinnten. Zu den bekanntesten in Westdeutschland erschienenen Zeitschriften zählen \"Happy Computer\", \"ASM – Aktueller Software Markt\", \"Computer Kontakt\", \"Computronic\" (Roeske-Verlag, später Tronic-Verlag) und \"Mein Home-Computer\" (Vogel-Verlag).\n\nFür die Dragon-Benutzer in Großbritannien waren seit 1983 verschiedene, speziell auf ihre Bedürfnisse zugeschnittene Publikationen erhältlich. Zu den bekanntesten zählen das ab 1983 von Dragon Data Ltd. an registrierte Dragon-Besitzer verschickte Magazin \"Stop Press\" und dessen Nachfolger \"Dragon World\". Im Mai 1983 kam das bei Sunshine Books in großer Auflage verlegte unabhängige und sehr beliebte \"Dragon User Magazin\" an die Kioske; sein Erscheinen wurde erst im Januar 1989 eingestellt. Ab 1985 waren für einige Zeit auch spanischsprachige Zeitschriften wie \"Dragon Software\" und \"Video Dragon\" erhältlich. Neben den Kioskzeitschriften erschien im Laufe der Zeit eine Unmenge weiterer lokaler Clubzeitschriften („Fanzines“) in begrenzter Auflage.\nNach dem Ende der Heimcomputer-Ära Anfang der 1990er-Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Ende der 1990er-Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripherie entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reicht mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit u. a. ein verstärktes Transferieren von sonst möglicherweise verlorengegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird.\n\nAls leistungsfähigster Emulator der Dragon-Computer gilt XRoar, der für verschiedene Computer- und Videokonsolenplattformen erhältlich ist. Aufgrund der technischen Nähe der Dragon-Geräte zu den Modellen des Tandy Color Computers wird deren Emulation ebenfalls durch XRoar abgedeckt. Weitere Emulatoren sind die für verschiedene Systeme erhältlichen DREaM, MESS, T3 und PCDragon II.\n\nBei Erscheinen des Dragon 32 fielen die Beurteilungen der Fachpresse großteils positiv aus. Hervorgehoben auf Hardwareseite wurden insbesondere die im Vergleich zu den direkten Konkurrenten Tandy Color Computer und Sinclair ZX Spectrum komfortablere Schreibmaschinentastatur mit Leertaste und der für damalige Verhältnisse mit 32 KB großzügig bemessene Arbeitsspeicher. Ebenso konnten der leistungsfähige Mikroprozessor, die ab Werk verbaute Centronics-Schnittstelle und die analogen Joystickports die Kritiker überzeugen. Auf der Softwareseite fand das eingebaute Extended Microsoft BASIC mit seinem großen Befehlsumfang und umfangreichen Grafikmöglichkeiten vielfach großen Anklang. Getrübt wurde der positive Gesamteindruck hauptsächlich durch die als überarbeitungsbedürftig eingestufte Anleitung, augenunfreundliche Bildschirmfarben, eingeschränkte Texteditorfähigkeiten und den im Wettbewerberfeld höchsten Verkaufspreis. Trotzdem war es im Heimcomputersegment der Dragon 32, dem häufig die höchsten Bewertungen zuteilwurden und dem damit vielfach Kaufempfehlungen insbesondere für Programmiereinsteiger und Hobby-Benutzer ausgesprochen wurden. Begründet wurde dies nicht zuletzt durch das gegenüber den direkten Konkurrenten Tandy Color Computer und Sinclair ZX Spectrum bessere Preis-Leistungs-Verhältnis sowie die gute Vermarktung seitens Dragon Data, die Lieferengpässe, wie sie etwa beim Commodore VC 20 oder Sinclair ZX Spectrum auftraten, vermeiden konnte.\n\nDer Dragon 64 stieß dagegen auf etwas weniger Gegenliebe, zumal bei der Markteinführung das Mitbewerberfeld bereits angewachsen war und mit dem Commodore 64 zudem an Qualität gewonnen hatte. Neben geringfügigen Softwareinkompatibilitäten zum Vorgängermodell war hauptsächlich das Gesamtkonzept als bloßer erweiterter Dragon 32 Gegenstand der Kritik, obgleich der Computer dennoch als zeitgemäß und wettbewerbstauglich eingeordnet wurde. Neben dem günstigen Preis als ursächlich dafür sah man das überarbeitete BASIC, behobene Tastaturprobleme, diverse Hardwareerweiterungen (z. B. RS-232-Schnittstelle) und vor allem das Betriebssystem OS-9, das den Computer für den Betrieb im Geschäftsumfeld und für UNIX-Einsteiger, jedoch weniger für den Heimanwender interessant machte.\n\nTrotz des bereits 1985 beginnenden Niedergangs erfreuten sich die Dragon-Computer aufgrund ihrer robusten Verarbeitung, des für Modifikationen gut geeigneten großen Gehäuses und nicht zuletzt wegen der leichten Portierbarkeit der Programme des Tandy Color Computer als günstiges Einsteigermodell in Europa großer Beliebtheit, wobei 1985 die Verkaufszahlen insgesamt speziell in Deutschland deutlich hinter denen der Marktführer Commodore 64 (ca. 60 Prozent Marktanteil) und Atari 800XL sowie Sinclair ZX Spectrum lagen.\nDer Dragon 32 und Dragon 64 erfährt mittlerweile wieder verstärkte Wahrnehmung im Internet, in Zeitschriften und Büchern. Der Dragon 32 wird dabei mehrheitlich als Klon des Tandy Color Computers eingeordnet, dem seine gleichsam leistungsfähige wie exotische 6809-Systemarchitektur durch fehlende Softwareunterstützung in Europa mit zum Verhängnis werden sollte. Differenziertere Charakterisierungen im Rahmen der Konkurrenzsituation des Jahres 1982 reichen von „Ausgezeichnetes Allzweckgerät für verschiedenste Nutzergruppen“ über „leistungsfähigstes Gerät seiner Preisklasse“ bis hin zu Bezeichnungen wie „Gerät mit revolutionärem Design“.\n\nNeben der im Vergleich zu anderen Systemen als spärlich angesehenen Softwareversorgung gibt auch die wechselvolle Geschichte des Herstellers Dragon Data Ltd. Anlass zu Spekulationen über die Ursache für das rasche Verschwinden der Dragon-Computer vom Heimcomputermarkt. Die innerhalb nur kurzer Zeit erreichte Marktdominanz im hart umkämpften Heimcomputermarkt Großbritanniens gehörte bereits ein Jahr später der Vergangenheit an. Dieser Übergang hin zu einem Nischendasein wird dabei von vielen Autoren im Wesentlichen auf wirtschaftliche Fehleinschätzungen und -entscheidungen der wechselnden Unternehmensleitungen von Dragon Data Ltd. zurückgeführt. Insbesondere das Unterschätzen der Schnelllebigkeit des Heimcomputermarktes und der damit verbundenen kurzen Produktzyklen habe 1983 zu einer massiven und verlustreichen Überproduktion im Zeitalter schnell veraltender Hardware geführt. Neue und nachgefragte Erweiterungen seien nur zögerlich oder gar nicht zur Marktreife gebracht worden. Ein attraktives und die Benutzer ansprechendes Nachfolgemodell für den Dragon 32 habe nicht verwirklicht werden können, stattdessen sei ein lediglich aufgerüsteter Dragon 32 in Form des Dragon 64 erschienen. Dazu kämen handwerkliche kaufmännische Fehler in der Vertriebspraxis, die beträchtliche Differenzen zwischen vorbestellter und tatsächlich abgenommener Ware seitens der Großvertreiber nie thematisierten und so ebenfalls Überproduktionen und damit Verluste nach sich zogen. Wären wenigstens die zur massiven Überproduktion führenden Fehler erkannt und frühzeitig abgestellt worden, so hätte das Unternehmen Dragon Data Ltd. nach Ansicht des Wirtschaftswissenschaftlers David Clutterbuck gerettet werden können. Mitunter fließen auch allgemeine Entwicklungen der Heimcomputerbranche wie der Video Game Crash ab Ende 1983 in den USA und seine starken Auswirkungen auch auf Europa in die Überlegungen zum Verschwinden der Dragon-Computer vom Heimcomputermarkt ein.\n\n\n"}
{"id": "1301031", "url": "https://de.wikipedia.org/wiki?curid=1301031", "title": "Notepad++", "text": "Notepad++\n\nNotepad++ ist ein freier Texteditor für Windows und kompatible Betriebssysteme. Als Zeichensätze werden ASCII und verschiedene Unicode-Kodierungen unterstützt, so können leicht auch fremdsprachige Textdateien verfasst werden. Die Bearbeitung von Quelltext wird besonders unterstützt: Für viele Programmiersprachen werden Syntax und Struktur durch typographische Mittel bzw. Code-Faltung hervorgehoben. Das Projekt basiert auf Scintilla, ist in der Programmiersprache C++ geschrieben und steht unter der GPL-Lizenz.\n\nMit der am 25. Oktober 2008 veröffentlichten Version 5.1 wurde die Software, neben der bereits bestehenden ANSI-Unterstützung, mit einer Unicode-Unterstützung ergänzt. Diese hat jedoch den Nachteil, dass ältere, bereits vorhandene Programmerweiterungen sogenannte Plug-ins teilweise nicht mehr verwendet werden können, da sie noch nicht Unicode-fähig sind. Beim Start der Anwendung werden alle ANSI-kompatiblen Plug-ins deaktiviert und können nicht mehr verwendet werden. Der Programm-Installer enthält nur die Unicode-Version, in der Version ohne Installer ist jedoch auch eine ANSI-Version enthalten. Mit dieser können die nicht für Unicode aktualisierten Plug-ins auch weiterhin verwendet werden. Sie unterstützt jedoch keine Unicode-Plug-ins.\n\nAm 21. September 2009 wurde die Version 5.5 veröffentlicht, welche mit einer verbesserten Suchfunktion sowie einer Textauswahl per Aufziehkasten erweitert wurde. Zudem ist das Laden von Plug-ins nun auch ohne den Neustart des Programms möglich. Darüber hinaus wurden kleinere Verbesserungen und eine Reihe von Fehlerkorrekturen vorgenommen. Des Weiteren wurde bei den Änderungen auch die integrierte Komponente Scintilla von Version 1.78 auf Version 2.01 aktualisiert.\n\nAm Freitag den 13. Juni 2014 wurde die Version 6.6.6 veröffentlicht. Das Chamäleon im Logo wurde zu diesem Anlass rot eingefärbt, sowie mit kleinen schwarzen Hörnern und einem Teufelsschwanz versehen. Der Blog-Eintrag zur Veröffentlichung der Version 6.6.6 endet mit den Worten \"\"Enjoy your 666 Friday the 13th edition, and do some evil things with it ;)\"\".\n\nAm 10. Januar 2015 erschien die Version 6.7.4, welche als \"Je suis Charlie edition\" bezeichnet wird. Nach Installation schreibt Notepad++ ein politisches Statement mit Bezug auf den Anschlag auf Charlie Hebdo in das Textfenster. Nach Erscheinen dieser Version wurde die Webseite von Notepad++ von Unbekannten verschandelt.\n\nNotepad++ ist nativ als \"portable\" Version erhältlich.\nEs gibt auch noch eine gepflegte Version im .paf.exe Format.\n\nDie deutsche Rechtschreibprüfung lässt sich per Einbindung der verschiedenen Wörterbücher bewerkstelligen.\n\nZurzeit stehen sechs deutsche Wörterbücher zur Auswahl: German (Additional), German (Austria), German (Germany), German (New Spelling), German (Old and New Spelling) und German (Switzerland).\n\nNotepad++ bietet Syntaxhervorhebung für folgende Programmier- und Auszeichnungssprachen (ohne Plug-ins):\n\nZusätzlich lassen sich Syntaxdefinitionen für weitere Sprachen über eine grafische Oberfläche einstellen. Dabei ist es möglich Schlüsselwörter, Kommentare und Operatoren selbst festzulegen und in verschiedenen Kategorien unterschiedlich zu formatieren.\n\nNotepad++ stellt eine Schnittstelle für Plug-ins zur Verfügung, über die das Programm um zusätzliche Funktionen erweitert werden kann. Einige der nachfolgend aufgeführten Funktionen sind im Programm selbst implementiert, andere liegen dem Installations-Paket als Plug-ins bei.\n\n\n\n\n"}
{"id": "1301558", "url": "https://de.wikipedia.org/wiki?curid=1301558", "title": "Adobe Photoshop Lightroom", "text": "Adobe Photoshop Lightroom\n\nUnter Adobe Photoshop Lightroom werden Programme und Dienste zur Bilderverwaltung und Bearbeitung von Digitalfotos zusammengefasst. Die Elemente des Lightroom-Systems unterstützen verschiedene Fotoformate, darunter Rohdaten, wie sie typischerweise von digitalen Spiegelreflex- (DSLR), aber auch spiegellosen Systemkameras sowie manchen Kompaktkameras erzeugt werden. Seit Version 3 werden auch Videos unterstützt.\n\nDie Apps des cloudbasierten Dienstes „Adobe Photoshop Lightroom CC“ zeigen eine Oberfläche für Import, Archivierung, Bearbeitung und Teilen, organisiert in Boxen, die für die verschiedenen Schritte aufgeklappt werden.\n\nDie Oberfläche des desktopbasierten Computerprogramms „Adobe Photoshop Lightroom Classic CC“ ist in sieben Module gegliedert:\n\nLightroom unterstützt den Datenimport der Grafikformate JPG, PSD, TIFF sowie das von Adobe selbst entwickelte offene Rohdatenformat Digital Negative, kurz DNG. Darüber hinaus ermöglicht das interne Entwicklungsmodul den Import der nativen Rohdatenformate der verschiedenen Kamerahersteller wie bspw. Canon Raw (*.crw, *.cr2), Pentax Raw (*.pef) oder Nikon Raw (*.nef).\n\nBeim Bildimport steht dem Fotografen zudem eine Vielfalt von Sortierungs- und Umbenennungssystematiken wie beispielsweise Festlegung von Schlagwörtern (Metadaten) zur Verfügung.\n\nDas Programm arbeitet, wie alle Raw-Konverter, nicht-destruktiv, das heißt, es wird nie die Originaldatei verändert (einzig die Aufnahmezeit kann optional direkt in der Originaldatei verändert werden), sondern alle Änderungen werden getrennt von der ursprünglichen Datei in einer Datenbank oder einer zusätzlichen XMP-Metadatei (Filialdatei) gespeichert. Wahlweise können die XMP-Daten auch in die Dateien hineingeschrieben werden. Erst bei der Konvertierung (Export) in JPEG oder TIFF werden die Änderungen wirksam.\n\nAdobe Photoshop Lightroom verwendet Technologien des mittlerweile eingestellten Raw-Programms \"RawShooter\" des dänischen Unternehmens Pixmantec. Adobe Systems hatte das Softwarehaus im Jahr 2006 übernommen.\n\nAdobe Photoshop Lightroom wird im Abonnement-Modell der Adobe Creative Cloud vertrieben. Die aktuelle Version 7 der Desktop-Version heißt offiziell „Adobe Photoshop Lightroom Classic CC“ und wurde am 18. Oktober 2017 veröffentlicht.\n\nEbenfalls am 18. Oktober 2017 stellte Adobe seinen neuen cloudbasierten Fotoservice „Adobe Photoshop Lightroom CC“ vor, mit dem plattformübergreifend Fotos verwaltet, bearbeitet und geteilt werden können und als Gegenstück zu Apples Fotos angesehen werden kann. Lightroom CC ist mit der rein desktopbasierten Software Lightroom Classic CC insofern kompatibel, als dass mit Lightroom CC erstellte Alben in Lightroom Classic CC als veröffentlichte Sammlung angezeigt und bearbeitet werden können und andersherum. Für den kompletten Wechsel von der Desktopvariante Lightroom Classic CC zur Cloudvariante Lightroom CC bietet Adobe einen Migrationsassistenten an. Lightroom CC bietet gegenüber Lightroom Classic CC einen geringeren Funktionsumfang, u. a. hinsichtlich der Bearbeitungsmöglichkeiten und es fehlen Kartendarstellung und die Möglichkeit zum manuellen Geotagging bzw. mittels Log Matching.\n\nNeben dem von Adobe bereits integrierten Veröffentlichungsdienst, z. B. für Flickr, ermöglicht das Plugin WP/LR Sync die Synchronisation der Foto-Verwaltung von Lightroom mit der Mediathek des Content-Management-Systems WordPress.\n\n\n"}
{"id": "1301657", "url": "https://de.wikipedia.org/wiki?curid=1301657", "title": "Layertechnik", "text": "Layertechnik\n\nLayertechnik ist ein im Bereich des computer-aided design (CAD) verbreiteter Begriff für ein Verfahren zur Strukturierung innerhalb von Zeichnungsdateien. \n\nZeichnungen werden hierbei in mehreren Ebenen (englisch „Layer“ oder „Level“) aufgebaut. Diese Ebenen enthalten beispielsweise die Kontur von Objekten, Schraffuren oder Bemaßungen. \n\nDurch das einfach zu handhabende Aus- und Einblenden der einzelnen Layer kann die grafische Darstellung auf den Verwendungszweck der Zeichnung zugeschnitten werden. \n\nIn vielen CAD-Programmen ist die Layertechnik zudem die einzige Möglichkeit, eine Eigenschaft, zum Beispiel eine Farbe, einer vordefinierten Gruppe von Objekten zuzuweisen. \n"}
{"id": "1301838", "url": "https://de.wikipedia.org/wiki?curid=1301838", "title": "JUBL", "text": "JUBL\n\nJUBL (Jülicher Blue Gene/L) war ein Supercomputer im Forschungszentrum Jülich. Der am 6. März 2006 eingeweihte Rechner wurde für komplexe Simulationen in der Astrophysik, der Biologie, Klimaforschung oder Physik benutzt.\nJUBL wurde im Mai 2008 abgebaut.\n\nMit 16.384 Prozessoren (8192 Knoten mit je zwei Prozessoren) und einem Hauptspeicher von 4,1 Terabyte (512 Megabyte pro Knoten) erbrachte der Rechner eine Spitzenleistung (Rpeak) von 45,87 TFLOPS. Die LINPACK-Leistung (Rmax) betrug 37,33 TFLOPS. In die TOP500-Liste stieg JUBL im Juni 2006 auf Platz 8 ein und befand sich in der Juni-2007-Ausgabe auf Platz 18.\n\nAls Prozessoren kamen 32-bit-PowerPCs 440d mit 700 MHz Takt zum Einsatz.\n\nZusätzliche Infrastruktur bildeten ein Service- und ein Login-Knoten mit je acht Power5-1,6-GHz-Prozessoren und 18 bzw. 8 GB Hauptspeicher. Als Betriebssystem kam \"SUSE Linux Enterprise Server\" (SLES 9) zum Einsatz.\n\nAls Nachfolger wurde im Forschungszentrum Jülich ein Rechner vom Typ \"BlueGene/P\" aufgebaut. Dieser JUGENE genannte Computer erbrachte bei Messungen im November 2007 bereits eine Leistung von 167 TFLOPS.\n"}
{"id": "1305381", "url": "https://de.wikipedia.org/wiki?curid=1305381", "title": "D1 (Computer)", "text": "D1 (Computer)\n\nDer D1 – eigentlich \"Programmierbarer Rechenautomat D1\" (Dresden 1) – war ein früher, in Eigenentwicklung entstandener Computer der DDR. Im Gegensatz zum Relaisrechner OPREMA von Zeiss, der 1955 in Betrieb genommen wurde, basierte der D1 bereits auf Vakuumröhren.\n\nEr wurde zwischen 1950 und 1956 an der TH Dresden zusammen mit dem VEB Robotron-Meßelektronik „Otto Schön“ Dresden entworfen und gebaut. Sein Konstrukteur war Nikolaus Joachim Lehmann, Vater vieler Computer aus Dresden. Der D1 war ein Röhrencomputer. Mit seinen etwa 760 Elektronenröhren konnte er 100 bis 200 Rechenoperationen pro Sekunde ausführen. Zur Datenhaltung diente ein Magnettrommelspeicher, welcher 2100 Wörter speicherte. Ein Wort entsprach dabei 72 Bit oder drei Befehlen. Außerdem verfügte der D1 bereits über eine assemblerähnliche Programmierung.\n\nNachfolger des D1 waren D2, D3 und D4.\n\n"}
{"id": "1305413", "url": "https://de.wikipedia.org/wiki?curid=1305413", "title": "D2 (Computer)", "text": "D2 (Computer)\n\nDer programmierbare Rechenautomat D2 (Dresden 2) war ein Computertyp aus der DDR. \n\nEr wurde zwischen 1956 und 1959 an der TH Dresden entworfen und gebaut. Sein Konstrukteur war Prof. N. J. Lehmann, Vater vieler historischer Computer aus Dresden.\n\nDer D2 war, wie sein Vorgänger D1, ein Röhrencomputer. Er verfügte aber bereits über 1400 Elektronenröhren, damit erreichte er eine Verarbeitungsgeschwindigkeit von 1000 FLOPS. Sein Trommelspeicher konnte 4096 Wörter speichern, ein Wort entsprach dabei 56 Bit. Der D2 hatte 11 Register und 320 Schnellspeicher. Außerdem verfügte er über eine assemblerähnliche Programmierung.\n\nNachfolger des D2 waren die Digitalrechner D3 und D4.\n\n"}
{"id": "1305740", "url": "https://de.wikipedia.org/wiki?curid=1305740", "title": "Moppel", "text": "Moppel\n\nDer MOPPEL, Kurzbezeichnung für „Modulares Prozessor-Programm der ELO“, war ein Mikrocomputer-Selbstbauprojekt, das in den Jahren 1982 bis 1984 in zahlreichen Ausgaben der deutschen Elektronik-Fachzeitschrift ELO beschrieben wurde. Autor war Reinhard Gößler.\n\nDer MOPPEL war modular um einen mit 4 MHz getakteten Intel 8085A organisiert und besaß in der Grundversion eine achtstellige Siebensegmentanzeige nebst Hex-Tastatur, 2 kB RAM und 4 kB ROM. Als Massenspeicher wurde ein Kassettenrekorder verwendet.\n\nNach über einem Jahr Projektlaufzeit konnte der MOPPEL mit einer 80x25-Zeichen-Grafikausgabe für einen BAS-Monitor, bis zu 32 kB RAM, einem lizenzierten CP/M-Betriebssystem auf Diskette und einem BASIC-Interpreter ausgestattet werden. Die Materialkosten des Vollausbaus beliefen sich auf über 2000 DM.\n\nDie Veröffentlichungen erschienen in folgenden ELO-Heften:\n\n"}
{"id": "1306828", "url": "https://de.wikipedia.org/wiki?curid=1306828", "title": "Connection Machine", "text": "Connection Machine\n\nDie Connection Machine war eine Baureihe von Parallelrechnern, die von 1983 bis 1991 von dem amerikanischen Unternehmen Thinking Machines (dt. denkende Maschinen) hergestellt wurde.\n\nDas Konzept der \"Connection Machine\" stammt von Danny Hillis vom Massachusetts Institute of Technology (MIT). Zusammen mit Sheryl Handler gründete er 1983 das Unternehmen \"Thinking Machines\", die Entwicklung der Rechner wurde durch Venture-Kapital und das amerikanische Verteidigungsministerium (DARPA) finanziert und später durch das Programm \"High Performance Computing and Communication\" (HPCC) gefördert. Im August 1993 musste \"Thinking Machines\" Insolvenz nach Chapter 11 anmelden.\n\nDie erste \"Connection Machine\" war ein massiv paralleles System mit bis zu 65536 1-Bit Prozessoren. Jeder Prozessor konnte mit 20 anderen über ein Hypercube-Verbindungsnetzwerk direkt kommunizieren. Die Prozessorknoten verfügten über eigenen Hauptspeicher und arbeiteten zunächst nach dem SIMD-Prinzip.\n\nDie CM-1 (1983) war vorwiegend zur Lösung von Problemen aus dem Bereich der Künstlichen Intelligenz konzipiert. Deswegen wurde \"*Lisp\" (auch \"Star Lisp\", eine parallele Erweiterung von Common Lisp) zur Programmierung verwendet. \nMit der CM-2 (1987) wurde die Connection Machine auch für numerische Verfahren interessant, je 32 Prozessorknoten teilten sich einen Koprozessor (Weitek 3132), als Interface zu diesem kam ein sogenannter \"SPRINT\"-Chip zum Einsatz, welcher u. a. die Fähigkeit hatte, aus den 32 1-Bit-Prozessorknoten eine 32-Bit-Zahl zu generieren. Zusammen erbrachten die Prozessorknoten eine Leistung bis neun (theoretisch 20) GFLOP (zum Vergleich, ein normaler PC mit einem Pentium-4-Prozessor bei einer Taktfrequenz von drei Gigahertz kann nach Angaben von IBM etwa sechs GFLOPS erreichen). Die CM-2a war eine kleinere Variante mit 4096 bzw. 8192 Prozessoren, die CM-200 war eine Weiterentwicklung der CM-2.\nAußerdem konnte mit der Einführung der CM-2 diese auch über C* (eine parallele Erweiterung von C) und CM Fortran programmiert werden.\n\nEin Wechsel der Rechnerarchitektur in Richtung MIMD erfolgte dann 1991 mit der CM-5. Sie bestand aus einem Fat Tree-Verbindungsnetzwerk von SPARC-Prozessoren. Bei der CM-5E wurden die SPARC-V7-Prozessoren schließlich durch SuperSPARC-Prozessoren ersetzt.\n\nWährend Connection Machines über Frontend-Computer (Symbolics, VAX und später auch SPARCstations) genutzt wurden, war CM-5 die erste Connection Machine, auf der auch ein eigenes Betriebssystem, genannt \"CMost\", lief, welches auf SunOS basierte, dieses wurde jedoch nur auf Kontrollprozessoren eingesetzt, welche Benutzer Logins und Netzwerkservices bereitstellten. Auf den einzelnen Prozessorelementen lief ein kleines Mikrokernel-Betriebssystem, das beim Start aus einem ROM geladen wurde und Basisfunktionen zum Annehmen und Ausführen von Jobs bereitstellte.\n\nDie \"Connection Machine\" fiel auch durch ihr Design auf. Das Gehäuse war ein großer Block, meist würfelförmig. An der Vorderseite befanden sich Gruppen von roten Leuchtdioden; für jeden Prozessor stand eine Leuchtdiode. Das Blinken der Leuchtdioden signalisierte die Aktivität der einzelnen Prozessorknoten. Eine CM-5 sieht man im Film Jurassic Park (1993).\n\n\n"}
{"id": "1307160", "url": "https://de.wikipedia.org/wiki?curid=1307160", "title": "Ultra-Mobile PC", "text": "Ultra-Mobile PC\n\nUnter der Bezeichnung Ultra-Mobile PC (früherer Projektname: \"Origami\"), abgekürzt UMPC, lancierten Microsoft und Intel im Frühjahr 2006 auf der CeBIT eine neue Geräteklasse von tragbaren Computern. Einen ersten Versuch in diese Richtung machte Sony bereits 2004 mit der Vaio-U-Serie, die allerdings in Europa nie auf den Markt kam und nur in Asien verkauft wurde. UMPCs konnten nie einen bedeutenden Marktanteil am PC-Markt erreichen und wurden in den 2010er-Jahren weitgehend durch Smartphones, Phablets, Tablets und Convertibles abgelöst.\n\nUltra-Mobile PCs sind kleiner als Subnotebooks, verfügen über ein diagonal etwa 12,7 bis 17,8 Zentimeter (5–7 Zoll) großes TFT-Display und werden, ähnlich wie Tablet-PCs, hauptsächlich über den berührungsempfindlichen Bildschirm oder mit Stylus-Stiften bedient. Tastaturen können extern angeschlossen werden oder sind teilweise sogar integriert. Weit verbreitet sind darüber hinaus auch bei Laptops übliche Features wie eine eingebaute Kamera für Videotelefonie oder Fotografie und Fingerabdrucksensoren zur Authentifizierung. Auch GPS-Module und Navigationssoftware werden gerne integriert. \n\nDie Geräte der ersten Generation sind vollwertige PCs, deren Betriebssystem Linux oder eine angepasste Windows XP Tablet-Edition ist.\n\nDie Geräte der zweiten Generation sind einerseits stromsparender sein und erreichen eine längere Akkulaufzeit (bis zu 5 Stunden), andererseits unterstützen sie Windows Vista, teilweise mit AeroGlass-Oberfläche. Als Prozessoren werden VIA C7-M-ULV-Prozessoren mit VIA-VX700-Chipsatz oder Intel-A110-Prozessoren eingesetzt.\n\nZwischen den Begriffen Subnotebook und Ultra-Mobile PC gibt es keine festgelegte Abgrenzung.\n\nUMPCs wurden für Geschäftsanwender beworben, die mit dieser Geräteklasse ihre Geschäftssoftware ohne wesentlichen Einschränkungen mobil nutzen können. Sie sind also leichter/mobiler als Notebooks, aber funktioneller als ein zu jener Zeit üblichen PDA. Ultra-Mobile PCs wurden ebenfalls als Multimedia-Abspielgeräte oder Navigationssysteme angepriesen. Durch Bluetooth, UMTS- und WLAN-Internetzugang kann man damit mobil kommunizieren, auf Webseiten zugreifen, E-Mails lesen oder Online-Spiele spielen.\nDie angepeilte Nische hatte Überschneidungen mit Subnotebooks und PDAs, wodurch selbst die Hersteller den Einsatzzweck nicht immer klar mitteilen können.\n\nDie ersten Geräte gab es von Asus (Modell „R2H“), Founder und Samsung (Modell „Q1“). Asus, Samsung und Gigabyte Technology (wurde unter dem Label Medion verkauft) haben Geräte der zweiten Generation auf der CeBIT 2007 vorgestellt, die alle mit einer modifizierten Version von Windows Vista betrieben werden.\n\nDer erste Ultra-Mobile PC mit Schwerpunkt auf Videospiele ist die in Deutschland hergestellte Pandora.\n\n"}
{"id": "1307161", "url": "https://de.wikipedia.org/wiki?curid=1307161", "title": "Cray T3D", "text": "Cray T3D\n\nDer T3D (Abkürzung für Torus, 3-dimensional) war Crays erster Versuch auf dem Feld der massiv-parallelen Supercomputer-Architektur. Er wurde 1993 vorgestellt und war zugleich Crays erster Supercomputer, der auf einer nicht proprietären Mikroprozessor-Architektur aufsetzte. \n\nDer T3D bestand aus 32 (bis zu 2048) \"Processing Elements (PEs)\" (zu Deutsch etwa: Verarbeitungselement, Rechenknoten), jedes davon schloss einen mit 150 MHz getakteten DEC Alpha 21064 (EV4) Mikroprozessor und RAM, bestehend aus 16 oder 64 MB DRAM, ein. Die PEs wurden in sog. \"Nodes\" paarweise gruppiert und mit einem 6-Wege-Switch verbunden. Diese Switches stellten eine maximale Bandbreite von 300 MB/Sekunde in beide Richtungen zur Verfügung und wurden so miteinander verbunden, dass die Topologie dieses Netzwerkes einem dreidimensionalen Toruskörper entsprach.\n\nDer T3D wurde so entwickelt, dass ein Cray Y-MP Modell E, M90 oder C90 als \"Front-End\"-System bzw. als Host agierte und sämtliche I/O und sonstige Systemdienste über dessen UNICOS-Betriebssystem abgewickelt wurden. Die PEs selbst liefen mit einem einfacheren UNICOS/MAX-Mikrokernel.\n\nDer T3D war in verschiedenen Konfigurationen erhältlich: \n\nDer T3D wurde 1995 durch den schnelleren und weiter entwickelten T3E abgelöst.\n"}
{"id": "1310791", "url": "https://de.wikipedia.org/wiki?curid=1310791", "title": "Windows Essentials", "text": "Windows Essentials\n\nWindows Essentials (früher \"Windows Live Essentials\") ist der Markenname einer Sammlung verschiedener Online-Dienste und Programme des Software-Konzerns Microsoft.\n\nDie Suite bündelt E-Mail, Instant Messaging, Foto-Sharing, Blog-Veröffentlichungs- und Sicherheitsdienste.\n\nDie Programme sind für die optimale Integration ineinander sowie mit Windows und anderen webbasierten Diensten Microsofts wie OneDrive oder Outlook.com gestaltet.\n\nAm 10. Januar 2017 wurde der Support seitens Microsoft eingestellt und die Installationsdateien im Download-Center entfernt.\n\nWindows Essentials beinhaltet die folgenden Programme:\n\nDie Programme lassen sich auf Windows 7, Windows 8, Windows Server 2008 R2 und Windows 10 installieren. Die aktuelle Version vom 16. April 2014 trägt die Versionsnummer 16.4.3528.\nBei der Installation kann gewählt werden, ob alle oder nur einzelne Programmteile installiert werden sollen.\n\nDie erste Version der Software-Sammlung wurde im Jahr 2006 unter dem Namen \"Windows Live Dashboard\" veröffentlicht. Kurz nach der Veröffentlichung war diese Suite nicht mehr verfügbar. Der \"Windows Live Installer\" (2007) war Microsofts zweite Software-Sammlung. Mit \"Windows Live Essentials 2009\" wurden neue Programme hinzugefügt und bisherige erweitert. Mit der inzwischen nicht mehr herunterladbaren Version \"Windows Live Essentials 2011\" für Windows Vista wurde u. a. die Ribbon-Oberfläche eingeführt.\n\nAm 7. August 2012 veröffentlichte Microsoft die \"Windows Essentials 2012\" für Windows 7 und Windows 8. Der Namensbestandteil \"Windows Live\" wurde bei einigen Programmen, darunter der \"Bing Bar\", entfernt.\n\nWindows 8 enthielt eine Reihe neuer Apps, die Alternativen für einige der Funktionalitäten der \"Windows Essentials\" darstellen. Diese Anwendungen sind Metro-Stil-Apps und laufen im Vollbildmodus. Darunter befinden sich zum Beispiel Mail, OneDrive und Fotos.\n\nMit Windows 10 erneuerte man diese Apps grundlegend. Jedoch bieten die Apps unter Windows 8 und Windows 10 einen geringeren/ veränderten Funktionsumfang.\n\n\n\n"}
{"id": "1310872", "url": "https://de.wikipedia.org/wiki?curid=1310872", "title": "IBM-PC-kompatibler Computer", "text": "IBM-PC-kompatibler Computer\n\nAls IBM-PC-kompatible Computer oder IBM-kompatible PCs bezeichnete man historisch gesehen Personal Computer bzw. Mikrocomputer, die dem technischen Design des IBM-PC von 1981 und dessen Nachfolgemodellen wie dem IBM PC/AT nachempfunden waren. Seltener wurden die Nachbauten auch als IBM-Nachbauten (oder -klone) bezeichnet, da sie vor allem in den Anfangsjahren tatsächlich technisch weitgehend identische Kopien waren. Später wurde die Bezeichnung \"IBM-kompatibel\" seltener und wesentlich allgemeiner für Systeme verwendet, die einen Mikroprozessor vom Typ x86 nachbilden und ein entsprechendes BIOS starten. Letzteres wird von dem moderneren EFI abgelöst.\n\nIBM-PC-kompatible Computer wurden auch kurz als \"PC\" bezeichnet, als Plattform in Abgrenzung gegenüber Apple-Macintosh-Modellen, der Atari-ST-Reihe und anderen. Da der Begriff \"Personal Computer\" jedoch bereits früh auch für andere als x86-basierte Plattformen verwendet wurde, war diese Bezeichnung nicht immer eindeutig. Durch die massive Verbreitung kompatibler x86-PCs anderer Hersteller ging die Bedeutung von IBM als anfänglicher Marktführer bereits in den 1980er Jahren stark zurück. 1987 brachte IBM die neuentwickelte PS/2-Reihe auf den Markt, die mit dem bisherigen PC-Konzept vollständig brach und proprietäre Technologien wie das Microchannel-Bussystem einführte, die auch für andere Hersteller bewusst nicht mehr frei verfügbar waren.\n\nDadurch waren die PCs von IBM selbst paradoxerweise nicht mehr \"IBM-PC-kompatibel,\" womit IBM den selbstgesetzten Industriestandard unterlief. Dies wurde jedoch ein wirtschaftlicher Misserfolg, denn große PC-Hersteller wie Compaq machten diesen Schwenk nicht mit. Sie setzten stattdessen erfolgreich eigene technische Standards wie den EISA-Bus als Konkurrenz zum Microchannel-Konzept. Durch den Verkauf seiner PC-Sparte an Lenovo im Jahr 2005 hat sich IBM schließlich vollständig aus dem PC-Bereich zurückgezogen.\n\nAus diesen Gründen ist der Begriff \"IBM-PC-kompatibel\" überholt und auch technisch seit der PS/2-Linie nicht mehr korrekt, weshalb er mittlerweile kaum mehr verwendet wird. Aktuelle Begriffe für x86-basierte Computer sind hauptsächlich die Begriffe \"PC,\" \"Windows-PC\" oder \"Windows-Rechner,\" nach dem dominanten Betriebssystem, oder seltener Wintel-Rechner als Kunstwort aus Windows und Intel, dem marktführenden x86-Prozessor-Hersteller.\n\nDa der IBM-PC – im Gegensatz zu seinen damaligen Konkurrenzerzeugnissen – ausschließlich mit handelsüblichen Standardkomponenten aufgebaut war, führte dies zu zahlreichen Nachbauten. Der IBM-PC entwickelte sich zu einem inoffiziellen Industriestandard, weil er ohne Lizenzierung von IBM nachgebaut werden konnte. Bei IBM selbst wurden solche Nachmachungen etwas herabschauend als \"IBM-PC-Kompatibler\" bezeichnet.\n\nBei der Einführung neuer Prozessoren ab i286 und allen späteren Modellen mit 32-Bit Wortbreite integrierte der Chiphersteller Intel einen (virtuellen) 8086 auf dem Chip, damit weiterhin x86-kompatible Computer gebaut werden konnten. Um den Stromverbrauch heutiger Chips möglichst gering zu halten, wird die 8086-Funktionalität heute durch im Prozessor integrierte Firmware emuliert. Je nach Hersteller und CPU tragen die zur Emulation benutzten Microcodes Namen wie ROP, Micro-Op oder µOp.\n\nSofern das EFI keine BIOS-Kompatibilität bereitstellt, ist ein Computer ohne das IBM-kompatible BIOS nicht mehr als IBM-kompatibel im „engeren Sinn“ zu verstehen. Nur ein IBM-kompatibles BIOS ermöglicht das Starten eines IBM-PC-kompatiblen DOS-Betriebssystems, und damit die \"prinzipielle Möglichkeit, den jeweiligen Computer genau so wie einen IBM-PC von 1981 zu nutzen.\" Die DOS-Betriebssysteme benötigen das hauptplatinenspezifische BIOS als Hardwareabstraktionsschicht für den Zugriff auf dem BIOS bekannte Ressourcen der Hauptplatine und standardisierter Erweiterungskarten. Somit können DOS-Betriebssysteme ohne ein IBM-kompatibles BIOS nicht starten und Programme für den IBM-PC nicht mehr nativ auf dem jeweiligen System ausgeführt werden. Ein Computer ohne IBM-kompatibles BIOS ist somit im Grunde nicht IBM-kompatibel.\n\nUnter modernen Betriebssystemen mit Multitaskingfähigkeit müssen Hardwarezugriffe koordiniert werden. Diese erfolgen hier unter Umgehung des BIOS direkt über die Gerätetreiber; das BIOS wird nur noch beim Booten zum Starten des Betriebssystems benötigt.\n\nAus diesem Grunde wurde im Laufe der Jahre das BIOS auch von als überflüssig empfundenen Ballast befreit. So wurde bereits in den 1980er Jahren von zahlreichen PC-Herstellern auf das beim originalen IBM-PC vorhandene ROM-Basic verzichtet, welches der IBM-PC startete, wenn er keine Diskette mit Betriebssystem vorfand. Eine Implementation hätte zusätzlichen Aufwand und möglicherweise die Erlaubnis von IBM erfordert. Auch die Hardwareabstraktionsschnittstelle für den Datasetten-Port verschwand mangels Nutzung relativ schnell aus den BIOS-Codes der PC-Clone-Hersteller. Bei heutigen PC-Hauptplatinen mit BIOS wurde die Floppyschnittstelle auch auf BIOS-Seite eingespart oder stark eingeschränkt, oft ist nur noch ein Laufwerk konfigurierbar. Auch ist die Unterstützung älterer Floppylaufwerkstypen durch aktuelle BIOS-Varianten fraglich.\n\nStreng genommen sind diese Systeme daher als nur noch \"teilweise IBM-PC-kompatibel\" zu betrachten, da der originale IBM-PC beispielsweise den Anschluss von Datasette und mehreren 5,25\"-Floppylaufwerken sowohl auf der Hard- als auch auf der Softwareseite her vorsah, was moderne Computer eben nicht mehr bieten, da sich Hardware und Betriebssysteme weiterentwickelt haben. An diese Entwicklungen wurde die PC-Architektur durch das Hinzufügen und Weglassen von Elementen über die Jahre kontinuierlich angepasst. Daher lässt sich konstatieren, dass der Grad der Kompatibilität zum IBM-PC zum Teil bereits in den 1980ern, aber auch nach der Ablösung von DOS durch Windows 95 mit der damit einhergehenden schwindenden Bedeutung des BIOS immer weiter gesunken ist.\n\nAls Kernelemente eines \"IBM-PC-kompatiblen Computers\" blieben über die Jahre ein x86-kompatibler Hauptprozessor, welcher ein BIOS startet, welches in wesentlichen Zügen nach wie vor die Funktionalität des BIOS des originalen IBM-PCs, angepasst auf die Erfordernisse moderner Hard- und Software, abbildet. Dadurch ist es auch auf modernen Systemen grundsätzlich noch möglich, DOS-Betriebssysteme zu nutzen und damit auch zahlreiche Programme für den originalen IBM-PC, sodass die Kompatibilität moderner Systeme zu ihrem „Urahn“ immer noch gegeben und als hoch zu betrachten ist.\n\nMittelfristig ist allerdings ein weiteres Absinken dieses Kompatibilitätsniveaus zu erwarten, da viele klassische BIOS-Funktionen immer weniger nachgefragt werden. Mit dem BIOS-Nachfolger EFI ist ein endgültiges Ende der Ära der in irgendeiner Weise „IBM-PC-kompatiblen Computer“ abzusehen, auch wenn für den Anfang möglicherweise noch EFIs mit BIOS-kompatiblen Funktionen für ältere Betriebssysteme realisiert werden. Allerdings ist abzusehen, dass auch diese Restfunktionen dann immer seltener durch Software nachgefragt und damit allmählich aus der Firmware verschwinden werden. Durch EFI und die Ankündigung Microsofts, mit Windows 8 auch EFI statt eines BIOS für den Betriebssystemstart nutzen zu können, ist das langsame Ende des BIOS und damit des IBM-PC-kompatiblen Computers endgültig eingeläutet.\n\nDa moderne Betriebssysteme in verschiedensten „Schichten“ und durch aufgesetzte Laufzeitumgebungen immer mehr Hardware-Interna abstrahieren, ist die Bedeutung dieser Änderungen, welche sich auf den unteren Ebenen abspielen, heutzutage kaum noch relevant. Heute können die Betriebssysteme mit verschiedenen Firmwares oder Loadern, oft auch mit ganz unterschiedlichen Hardwarearchitekturen wie z. B. ARM-SoCs umgehen. Der Begriff „IBM-PC kompatibler Computer“ setzt auf einer Ebene an, welche heute nur noch für wenige Anwender von Interesse ist und wird daher kaum noch verwendet. Für Anwender von Interesse ist heute die Frage zur Kompatibilität zu bestimmten Betriebssystemen, was heute noch – beispielsweise im Falle von Windows- meist die Kompatibilität zum IBM-PC mit einschließt.\n\nDamit die für \"IBM-PC-kompatible Computer\" verfügbaren Betriebssysteme mit unterschiedlichen Hardware-Erweiterungen zusammenarbeiten, bedarf es außer dem x86-kompatiblen Prozessor auch einheitlicher Schnittstellen. Der Ur-PC verfügte intern über 8-Bit-ISA-Steckplätze so wie einem Anschluss für bis zu zwei Diskettenlaufwerke. Extern gab es serielle und parallele Schnittstellen und einen Tastaturanschluss, so wie einen digitalen MDA-Monitoranschluss für monochrome Textdisplays. Diese Schnittstellen wurden jedoch im Laufe der Entwicklung erweitert und modernisiert.\n\nSiehe dazu auch:\n\nTeilweise werden diese Schnittstellen auch von anderer Hardware verwendet. Eine Verwendung angeschlossener Erweiterungen setzt u. U. jedoch die Verfügbarkeit entsprechender Treiber voraus.\n\nFür IBM-PC-kompatible Computer stehen fast alle Betriebssysteme zur Verfügung. Gebräuchlich sind:\n\n\n\n"}
{"id": "1312112", "url": "https://de.wikipedia.org/wiki?curid=1312112", "title": "Hugi (Zeitschrift)", "text": "Hugi (Zeitschrift)\n\nHugi ist eines der langlebigsten Demoszene-, Computerkultur- und Underground-Disketten-Magazine (kurz Diskmag) für den IBM-PC und eines der über die Demoszene hinaus bekanntesten des Genres.\n\nDie ersten, noch rein deutschsprachigen Ausgaben erschienen 1996 in Anlehnung an die Buchhandlung Hugendubel unter dem Namen „Hugendubelexpress“ (HDE). Später wurde die von den Lesern selbst geprägte Kurzform „Hugi“ als offizielle Bezeichnung übernommen. Ab Ausgabe 11 erschien das Magazin zweisprachig in Deutsch und Englisch. Mit Ausgabe 18 wurde der deutschsprachige Teil abgespalten und der unabhängige \"Hugi.GER\" entstand. Zusätzlich gab es zwischen 1998 und 2000 den wöchentlichen \"Hugi Newsletter\", der die Reihe früherer Formate wie zum Beispiel \"Demonews\" inoffiziell fortführte.\n\nInhaltlich entwickelte sich der Hugi vom Stil einer Schülerzeitung zu einem der erfolgreichsten und langlebigsten digitalen Demoszene- und Underground-Magazine. Die Inhalte werden zum größten Teil von den Lesern beigesteuert und lediglich redaktionell aufbereitet. Thematisch umfassen die Artikel alle Bereiche der digitalen Kunst und Netzkunst. Schwerpunkte sind die Programmierung und Rezension von Grafikdemos, Berichte von Demopartys und die Erstellung von Computermusik. Daneben werden politische, literarische und philosophische Themen behandelt, so gibt es beispielsweise Zeitschriften-Splitter, Kurzgeschichten, Erfahrungsberichte und Tests anderer elektronischer Magazine.\n\nBis Juni 2014 erschienen in unregelmäßigen Abständen 38 Hauptausgaben, 17 davon ganz oder teilweise in deutscher Sprache. Zwölf Ausgaben wurden auch ins Russische übersetzt. Der Umfang der Ausgaben wird in Byte angegeben; durchschnittlich war etwa 1 Megabyte Text enthalten. Daneben wurden sieben Ausgaben des deutschsprachigen Ablegers \"Hugi.GER\", 38 Newsletter und vier Spezialausgaben mit den Schwerpunkten Programmierung, Musik und Interviews veröffentlicht. Die Ausgaben 11 bis 38 und die Spezialausgaben wurden in online im Webbrowser lesbarer Form republiziert.\n\nMan kann die 38 Hauptausgaben von Hugi grob in vier Perioden einteilen, wobei jeweils zehn Ausgaben die ersten drei Perioden bilden und die übrigen acht Ausgaben die vierte Periode:\nDerzeit (Stand: August 2017) sind bis auf weiteres keine neuen Ausgaben mehr geplant.\n\n\nDer Hauptherausgeber von Hugi, der Österreicher Claus D. Volko, ist in der Demoszene unter dem Pseudonym „Adok“ bekannt. Die Hugi-Redaktion wird in „Hugi Core“ (aktive Mitglieder) und „Royal Family“ (Ehrenmitglieder) unterteilt. Viele weitere Personen tragen zu Hugi bei, ohne der Redaktion anzugehören.\n\nDie seit Ausgabe 18 vom Dezember 1999 genutzte \"Panorama\"-Engine wurde vom polnischen Programmierer Chris Dragan für das Magazin geschaffen. Die Engine bildet die Basis für zahlreiche andere elektronische Magazine außerhalb der Demoszene.\n\nDie Hugi-Redaktion organisierte auch eine Serie von Assembler-Programmierungs- und Größenoptimierungs-Wettbewerben namens \"Hugi Size Coding Competition\". Das Ziel der Wettbewerbe war, ein vorgegebenes Programm in möglichst wenigen Bytes nachzubilden. Dabei entstanden ausführbare Dateien von meist weit unter einem Kilobyte Größe. Von 1998 bis 2009 wurden 29 Wettbewerbe abgehalten. Üblicherweise nahmen 20 bis 80 Personen aus aller Welt (unter anderem Nordamerika, Ostasien, Südafrika, Australien) daran teil. Nach jedem Wettbewerb wurden die Einsendungen mit ihren Quelltexten veröffentlicht. In einer anschließenden Diskussion konnte die Gültigkeit einzelner Beiträge angezweifelt werden. Wurde einem solchen Einwand stattgegeben, so erhielt der Autor Strafpunkte. Einmal im Jahr wurde eine „Weltrangliste“ mit den Gesamtpunktzahlen generiert, welche die Teilnehmer in den einzelnen Bewerben erreicht hatten.\n\n"}
{"id": "1313219", "url": "https://de.wikipedia.org/wiki?curid=1313219", "title": "Embedded Linux", "text": "Embedded Linux\n\nAls Embedded Linux (deutsch: „eingebettetes Linux“) bezeichnet man ein eingebettetes System mit einem auf dem Linux-Kernel basierenden Betriebssystem. Dies impliziert nicht den Gebrauch bestimmter („Mindest“-)Bibliotheken oder Anwendungen mit diesem Kernel.\n\nEmbedded-Linux-Systeme werden normalerweise aufgrund ihrer verschiedenen Systemeigenschaften und nicht aufgrund ihrer Einsatzorte eingeteilt. Das können u. a. die Skalierbarkeit, die Unterstützung für bestimmte Prozessoren, der Stromverbrauch, das Zeitverhalten (Echtzeitfähigkeit), der Grad der möglichen Nutzerinteraktionen oder andere wesentliche Faktoren sein.\n\nEin Embedded-Linux-System ist grob in drei Schichten unterteilbar.\nDie unterste Schicht wird durch die zugrunde liegende Hardware gestellt. Hierbei wird von den Treibern der folgenden Schicht so gut wie alles an Hardware abgedeckt, was ein 32-Bit Prozessor bietet. Die zweite Schicht besteht aus dem eigentlichen Kernel, welcher wieder dreifach geschichtet ist. Die unterste Schicht hiervon ist eine Low-Level-Schnittstelle, welche eine erste Hardware-Abstraktion mit kleiner API für darüberliegende Schichten bietet. Danach folgen kleine Module zur (Vorab-)Interpretation von strukturierten Daten aus den Filesystem- und Netzwerkprotokollen, welche der Kernel empfängt und sendet. An dieser Stelle können schon erste Weichen zu Standards auf höheren Schichten gestellt werden. Die letzte Kernelschicht wird auch High-Level-Abstraction-Layer genannt und ist schon bei vielen Linux-Derivaten und Unixen gleich oder ähnlich. Diese Schicht ist bis auf Ausnahmen hardwareunabhängig; hier werden z. B. Prozesse, Threads, Dateien, Sockets und Signale generiert bzw. verarbeitet.\nIn der letzten Schicht des Embedded-Linux-Systems sind verschiedene C-Bibliotheken (oder speziell für eingebettete Systeme speicheroptimierte Ersatzbibliotheken) und auch die Anwendungssoftware angesiedelt. Die Bibliotheken werden meist dynamisch mit den Anwendungen verlinkt.\n\nBei der Entwicklung von Embedded-Linux-Systemen kann grob zwischen \"Cross-Development\" und \"Standalone Setups\" (also Nicht-Cross-Development) unterschieden werden. Cross-Development lässt sich außerdem in die beiden Untergruppen \"Linked Setup\" und \"Removable Storage Setup\" unterteilen. Ein Beispiel für Cross-Development ist OpenWrt.\n\nLinked Setup ist die Entwicklungsumgebung, die das klassische Cross-Development darstellt. Man benutzt ein Host-System auf dem eine größere IDE (\"Integrated Development Environment\") laufen kann zum Implementieren des eigentlichen Quellcodes. Dieser Code wird über die Links (daher der Name Linked Setup) auf das Target übertragen, auf dem sich dann ein Bootloader, der Kernel und ein (minimales) Rootfilesystem oder ein Networkfilesystem befinden. Die Links sind serielle Verbindungen wie RS232 oder aber Ethernet oder beides zusammen. Aufgrund der höheren Geschwindigkeit wird häufig Ethernet für den Upload des Codes auf das Target benutzt und die Verbindung des RS232 für die Rückrichtung zum Debugging mit geringerem Datenaufkommen.\n\nDer Aufbau des Removable Storage Setup lehnt sich grundsätzlich an den des Linked Setup an, jedoch wird bei der Entwicklung ein Zwischenspeicher (Storage) zur Verfügung gestellt, auf dem der zweite Bootloader, der Kernel sowie das Rootfilesystem vom Host abgelegt werden. Auf dem Target befindet sich vorerst lediglich der erste Bootloader, welcher danach den Rest des Systems vom Zwischenspeicher bootet.\n\nBei der Entwicklung mit sogenanntem Standalone Setup verzichtet man auf eine große, separate Entwicklungsplattform zu Gunsten eines alleinstehenden, entwicklungsfähigen Targets. Offensichtlich deutet dies auf eine gewisse Größe des Targets hin, denn die Entwicklungsumgebung ist nun auf ihm selbst. Bei gegebener Hardware ist diese Möglichkeit populär, da dann alle zu entwickelnden Komponenten von vornherein in ihrer \"natürlichen\" Arbeitsumgebung laufen.\n\nIn Embedded-Linux-Systemen kann eine Vielzahl unterschiedlicher Hardware Anwendung finden. Dies gilt nicht nur, weil sich schließlich bei noch nicht unterstützter Hardware auch Treiber \"from scratch\" (also von Grund auf selbst) schreiben lassen oder weil selbst fertige \"off the peg\" Distributionen schon einen Großteil der potentiellen Hardware ohne Anpassungen nutzen können, sondern weil schlicht eine überwältigende Mehrzahl der potentiell zur Verfügung stehenden Hardware von Linux unterstützt wird.\nJohn Lombardo schrieb im Jahr 2001:\n\nDiese Aussage bezog sich lediglich auf verwendbare Ein- und Ausgabegeräte beziehungsweise Schnittstellen, jedoch kann man wohl mit gutem Gewissen behaupten, dass dies mit Ausnahme der Prozessoren auf den allergrößten Teil der Hardware anwendbar ist. An dieser Stelle werden die unterstützten Hardwarekomponenten ohne Anspruch auf Vollständigkeit genannt. Wegen Lombardos Aussage kann die Betrachtung der restlichen (also Nicht-CPU-) Hardware wegfallen und somit der Schwerpunkt auf die Prozessorfamilien gelegt werden. Nichtgenannte Prozessortypen werden nicht automatisch nicht unterstützt, sondern schlicht seltener verwendet und stellen meist Einzellösungen dar.\n\nEine sehr häufig verwendete Prozessorfamilie ist die x86. Die Linuxunterstützung beginnt prinzipiell mit allen Versionen des 386ers. Wie schon weiter oben erwähnt, wird durch das Projekt ELKS (Embedded Linux Kernel Subset) auch die Verwendbarkeit von Linux auf 286ern sichergestellt, um Embedded-Linux-Systeme besser an die Größenerfordernisse von kleineren Eingebetteten Systemen anzupassen. Nach dem 486er wurden auch Intels Pentiums sowie die entsprechenden Fabrikate anderer Hersteller unterstützt, was technologisch und historisch gesehen den Sprung auf superskalare CISC-Prozessoren für Linux darstellte. Insgesamt kann man feststellen, dass der Familie der x86er eine nahezu lückenlose Unterstützung durch Linux erfährt, was jedoch nicht vordergründig aus dem Embedded-Bereich herrührt, sondern von Workstations und Desktop-PCs. Damit lässt sich begründen, warum die Reihe der x86er trotz ihrer breiten Linuxkompatibilität im traditionellen Umfeld der Eingebetteten Systeme nur einen kleinen Anteil an der verwendeten Architektur stellt. Häufiger finden ARM, MIPS sowie PowerPC wegen geringerer Komplexität und günstigerer Kosten den Weg in die Produktion.\n\nDie ARM-Architektur (Advanced RISC Machine) sowie die des StrongARM sind im Bereich der Eingebetteten Systeme beliebte Prozessoren und zwar in erster Linie auf Grund ihrer geringen Stromaufnahme. Daher kamen sie schon früh für Embedded-Linux-Systeme in Betracht und wurden unterstützt. Obwohl ARM auch eine Firma ist, werden von ihr nur die Lizenzen zum Bau ihrer Prozessoren vertrieben. Alle bekannten Prozessorhersteller sind Lizenznehmer bei/für ARM. Die Projekte RTAI und RTLinux von der Firma FSMLabs (es werden hier eine freie sowie eine kommerzielle Variante angeboten) haben sich spezifischer mit der Nutzung von ARM-Architekturen mit Hinblick auf harte Echtzeitanwendungen für Embedded Linux befasst. Die Tatsache, dass es sich hierbei um militärische End-Anwendungen mit sehr kostenintensiven Produkten handelt, zeigt übrigens äußerst deutlich, dass man sehr hohe Erwartungen an Embedded Linux als hartes Echtzeitbetriebssystem stellt und nicht von einem großen Kostenaufwand zurückschreckt.\nAls zukunftsweisend ist zu erwähnen, dass in den Embedded-Linux-Systemen auch häufig schon die Java-Coprozessoren der ARMs verwendet werden können.\n\nETRAX CRIS bezeichnet eine Prozessorfamilie von Axis Communications. Dabei steht ETRAX für Ethernet, Token Ring, Achse (engl. AXis), auch wenn der Begriff nicht länger korrekt ist, weil der Support für Token Ring bei den jüngeren Modellen entfernt wurde. Diese Prozessoren basieren auf dem Code Reduced Instruction Set (CRIS). Aktuelle Modelle sind ETRAX 100LX mit 100 MHz 32 bit RISC CPU und ETRAX 200FS mit 200 MHz 32 bit RISC CPU, welche über einen Kryptographie-Beschleuniger und ein über Microcode steuerbaren I/O-Prozessor verfügt. ETRAX CPUs sind für den Einsatz unter Embedded Linux optimiert und eignen sich durch ihre hohe Konfigurierbarkeit hervorragend zur Netzwerkintegration elektronischer Geräte. Der italienische Hersteller ACME Systems hat auf Basis des 100LX einen voll funktionsfähigen Linux-Einplatinen-Computer mit LAN und USB unter dem Namen Fox Board auf den Markt gebracht.\n\nFür die Einbindung des PowerPC wird ähnlich ambitionierte Arbeit betrieben wie bei der ARM-Architektur. Dieses Akronym besteht aus zwei Teilen: Power ist die Abkürzung für \"Performance optimization with enhanced RISC\", also in etwa Leistungsoptimierung durch erweitertes RISC, und PC steht hierbei für Performance Chip, also Hochleistungs-Chip. In einigen Fällen wird diese Architektur auch AIM genannt, was aus den Anfangsbuchstaben der Entwicklerfirmen Apple, IBM und Motorola zusammengesetzt wird. Es existieren 32-Bit- und 64-Bit-Versionen, welche gleichermaßen durch Linux unterstützt werden. Insbesondere sind auch hier die Echtzeit-Projekte RTAI und RTLinux federführend um den PowerPC in harten Echtzeitumgebungen unter einem Linuxbetriebssystem zu verwenden sowie die Java-Unterstützung sicherzustellen.\n\nObgleich die MIPS-Architektur, \"Microprocessor without interlocked pipeline stages\" bedeutet so viel wie Mikroprozessor ohne Pipeline-Sperren, sich ebenfalls durch geringere Komplexität und damit geringere Kosten auszeichnet, ist die Linuxunterstützung hierfür noch nicht ausgereift. Es mag eine Frage der Zeit sein, bis die 32-Bit oder die 64-Bit Version in Embedded-Linux-Systemen Einzug hält; momentan beschränkt sich die Unterstützung auf eine Portierung einzelner Distributionen, da der Befehlssatz von Lizenznehmer zu Lizenznehmer mitunter sehr stark variiert. Da die MIPS-Architektur ursprünglich im Workstation- und Serverbereich „groß geworden“ ist, häufig auf ein spezielles Motherboard zugeschnitten war und die Java-Unterstützung nicht sichergestellt war, ist der Einsatz als Grundlage eines Embedded-Linux-Systems nicht unumstritten beziehungsweise zumindest aufwändig und führt zu proprietären Lösungen.\n\nHitachi SuperH ist eine RISC-artige Prozessorarchitektur, welche ursprünglich von Hitachi entwickelt und später auch in Lizenz gebaut wurde. Sie wird dank ihrer „Bandbreite“ von 8 bis 64 Bit (eigentlich nur 32 und 64 Bit; 8-Bit und 16-Bit sind die ähnlichen Vorgänger) häufig in Eingebetteten Systemen genutzt. Der Einsatz von Linux auf dieser Architektur wurde allerdings erst nach der Entwicklung einer Version mit MMU beziehungsweise nach der Anpassung von μCLinux an MMU-lose Prozessoren häufiger. Ähnliches lässt sich für die Adressbreite sagen: Linux fand zunächst nur auf den 32-Bit-Versionen Anwendung, erst nach der Entwicklung von μCLinux auch auf 16-Bit breiten Datenpfaden. Für diese Architektur spricht, dass sie wenig Strom verbraucht und relativ günstig herzustellen ist.\n\nDie 68000er-Familie von Motorola, auch m68k genannt, wird mit 8, 16 und 32 Bit als CISC-Prozessor eines Embedded-Linux-Systems mit Echtzeitanwendungen verwendet. In Nicht-Linux-Systemen fand sie ebenfalls oft Verwendung, wenn Echtzeitfähigkeit gefordert war; hier wurden oftmals zusätzlich mathematische Coprozessoren verwandt. Zu dieser Architektur ist jedoch zu sagen, dass Linux für sie momentan nur geringen Support bietet, was auch an der fehlenden Java-Unterstützung liegen kann.\n\nWie schon oben erwähnt, ist die Linux-Kompatibilität für unterschiedlichste Hardware quasi garantiert. Dies trifft sowohl für Busse, Schnittstellen, Speicher und Netzwerke als auch für I/O- und andere Peripheriegeräte zu. Generell betrifft diese Formulierung zunächst die konventionellen Geräte. Aber selbst wenn noch keine Unterstützung für neuere Hardware wie zum Beispiel Satellitenempfänger vorliegt, dann bedeutet es einen verhältnismäßig geringen zeitlichen Aufwand, eigene Treiber zu implementieren.\n\nErwähnenswert ist ferner, dass angebotene Treiber häufig Hardwaremonitoring unterstützen. Dies geschieht entweder über Watchdogs in der Software oder als separates Hardwareteil oder über die Interpretation besonderer Monitoringsignale der jeweiligen Hardware selbst.\n\n"}
{"id": "1314069", "url": "https://de.wikipedia.org/wiki?curid=1314069", "title": "Sprachtechnologie", "text": "Sprachtechnologie\n\nDie Sprachtechnologie setzt die theoretischen Forschungsergebnisse der sprachorientierten Grundlagenforschung in praxisgerechte und technologisch verwertbare Anwendungen um.\nZu den Applikationen zählen etwa Spracherkenner, Übersetzungssysteme, Fahrgastinformationssysteme, Kontexttechnik (Auswertung des Inhalts elektronischer Dokumente), Diktiersysteme, Internet-Suchmaschinen, Rechtschreibprüfung, Volltextsuche und Expertensysteme. Die theoretischen Grundlagen liefern die sprachbezogenen Bereiche der Künstlichen Intelligenz und die Computerlinguistik/linguistische Datenverarbeitung.\n\n\n"}
{"id": "1316167", "url": "https://de.wikipedia.org/wiki?curid=1316167", "title": "ProFTPD", "text": "ProFTPD\n\nProFTPD (Abkürzung für \"Pro FTP Daemon\") ist ein freier FTP-Server, der auf Unix-Betriebssystemen und Windows mit Cygwin läuft. Er gehört zu den am häufigsten verwendeten FTP-Servern unter Unix-Betriebssystemen.\n\nDie Idee zu dem FTP-Server kam den Entwicklern aufgrund der hohen Verbreitung von wu-ftpd, welcher aber wenig Features besaß. Diese nachträglich einzuarbeiten schien den Programmierern zu ineffizient, woraufhin sie ein eigenes Projekt starteten – den ProFTPD.\n\nDer ProFTPD ist vielseitig über die Datei /etc/proftpd/proftpd.conf (bei früheren Systemen wie Ubuntu 6.10 über /etc/proftd.conf) konfigurierbar und ähnelt in der Konfiguration dem Webserver Apache. Unter anderem können auch virtuelle Server erstellt und verwaltet werden. Der Server kann als \"standalone\" oder per inetd gestartet werden. IPv6-Unterstützung ist ebenfalls vorhanden. Ein modularer Aufbau erlaubt SSL/TLS-Verschlüsselung, oder Erweiterungen wie LDAP oder SQL.\n\nDie Bedienung des Programms ProFTPD erfolgt, wie die meisten Serverprogramme unter Unix/Linux, über die Kommandozeile. Es gibt jedoch eine Auswahl von Zusatzprogrammen, welche eine grafische Benutzeroberfläche (GUI) für ProFTPD bereitstellen. Der Sinn solcher grafischen Benutzeroberflächen ist es, die Konfiguration und die Bedienung des Servers sowie insbesondere das Monitoring der Geschehnisse auf dem Server bequemer und übersichtlicher zu gestalten. Eine Auswahl vorhandener GUIs für ProFTPD ist: \n\nAnmerkung: Die Verwendung eines GUIs schließt die parallele Bedienung des Servers über die Kommandozeile nicht aus, ebenfalls können auch mehrere GUIs parallel betrieben werden.\n\n"}
{"id": "1316734", "url": "https://de.wikipedia.org/wiki?curid=1316734", "title": "Siemens 2002", "text": "Siemens 2002\n\nDer Siemens 2002 war ein Mitte der 1950er-Jahre von der Firma Siemens & Halske hergestellter Computer. Zum ersten Mal in Deutschland wurden nur noch Transistoren als aktive Bauelemente verwendet.\nNachdem die Firma Siemens & Halske 1954 beschlossen hatte, in die Datenverarbeitung einzusteigen, wurde mit der Entwicklung entsprechender Geräte begonnen. Bereits 1956 war ein erster Prototyp fertiggestellt. Das System war als Universalrechner für den kommerziellen wie für den technisch-wissenschaftlichen Einsatz konzipiert. 1959 wurde mit der Auslieferung der Geräte begonnen. Der Siemens 2002 wurde bis 1966 gefertigt.\n\nBereits 1963 gab es ein Nachfolgemodell \"Siemens 3003\", und 1968 eine \"Siemens 4004-45\". Letztere war jedoch ein Lizenznachbau des RCA Spectra 70 (ab 1965, später UNIVAC Series 70), deren nicht-privilegierter Maschinenbefehlssatz kompatibel zum System/360 von IBM war.\n\nObwohl die Fertigung 1966 eingestellt wurde, waren im Herbst 1971 immerhin noch 39 Anlagen installiert, unter anderem bei folgenden Einrichtungen:\nHeute stehen Exponate, die aber nicht mehr funktionsfähig sind, in folgenden Museen:\n\nDas System benutzte Speicherwörter und Register mit einem Vorzeichen und 12 Dezimalziffern. Intern wurde jede Dezimalziffer durch 4 Bits dargestellt. Da mit 4 Bits 16 verschiedene Zustände dargestellt werden können, für eine Dezimalziffer aber nur 10 gebraucht werden, bedeutete dies eine gewisse \"Verschwendung\" gerade zu einer Zeit, in der Hardware noch extrem teuer war.\nDer Hauptspeicher war als Magnetkernspeicher realisiert, es gab ihn mit Kapazitäten von 1.000, 5.000 oder 10.000 Wörtern, die Zugriffszeit betrug 14 µs. Als optionale Erweiterung gab es einen Trommelspeicher mit 10.000 Wörtern und einer mittleren Zugriffszeit von 19 ms. Verschiedene Peripheriegeräte wie Lochstreifengeräte, Blattschreiber, Lochkartengeräte, Magnetbänder und Schnelldrucker konnten angeschlossen werden.\n\nEin Wort konnte auf 4 verschiedene Weisen interpretiert werden:\n\nDas System hatte folgende Rechen- und Steuerregister mit Wortbreite (d. h. mit Vorzeichen und 12 Dezimalstellen):\nund daneben einige Register mit nur 5 Dezimalstellen, für die Adressierung:\n\nDie über 80 verschiedenen Maschinenbefehle hatten 3-stellige Zifferncodes, die mit drei Großbuchstaben dargestellt wurden. Es gab eine Reihe spezieller Befehle für die Ein-/Ausgabe von Lochkarten und -streifen. Einige typische Befehle waren:\nAls Betriebssystem wurde ORG 2002 eingesetzt, und als Programmiersprachen konnten PROSA 2002, POESIA, MAGNUS 2002 oder ALGOL 60 verwendet werden.\nDer von Ursula Hill-Samelson und Hans Langmaack geschriebene ALGOL-Übersetzer ALCOR MAINZ 2002 erlaubte ab dem Frühjahr 1962 den praktischen Einsatz dieser Sprache, vor allem im Wissenschaftsbereich.\n\n\n"}
{"id": "1316835", "url": "https://de.wikipedia.org/wiki?curid=1316835", "title": "Schachcomputer in der DDR", "text": "Schachcomputer in der DDR\n\nDie Entwicklung von Schachcomputern in der DDR verlief in mehreren Stufen vom Prototyp bis zur Serienreife.\n\nIm VEB Mikroelektronik „Karl Marx“ des Kombinats Mikroelektronik Erfurt („VEB Funkwerk Erfurt“) wurde untersucht, wie der entstandene Rückstand zum westlichen Niveau der Konsumgüter verringert werden kann. \n\nDas Kombinat hatte zwei wesentliche Produktlinien: \n\nDie Entwicklung und Produktion von Schachcomputern erfolgte in den Betriebsteilen des Messgerätewerkes. Spezialisten des Bauelementewerkes wurden zunächst beauftragt, mit den im Werk produzierten mikroelektronischen Bauelementen vergleichbare Produkte zu applizieren – darunter auch Schachcomputer. Die Schachcomputer waren seinerzeit auf dem westlichen Markt bereits als Massenproduktion vorhanden, in den sozialistischen Ländern jedoch nur vereinzelt.\n\nDie Mikroelektronik, dann auch die Konsumgüterindustrie mit mikroelektronischen Bauelementen, waren wirtschaftlich gesehen Zuschuss-Segmente der Volkswirtschaft der DDR.\n\nDer Schachcomputer SC1, von dem etwa ein Dutzend gebaut wurden, war eine Kleinstserie zur Demonstration der Leistungsfähigkeit mikroelektronischer Bauelemente aus der Produktion der DDR.\n\nDas Gehäuse bestand aus einem Holzrahmen, das Schachbrett war ein Aluminiumblech, auf dem die Schachfelder aufgedruckt waren. Die Eingabe der Schachzüge wurde mittels einer Tastatur analog zu der eines Taschenrechners realisiert. Die Ausgabe der Schachzüge erfolgte mit einer LED-Anzeige.\n\nDas Programm des Prototyps war noch keine Eigenentwicklung.\n\nDer SC1 fand bei den für die Aufnahme einer Serienproduktion Zuständigen sofort eine rege Zustimmung und führte zu dem Auftrag, diesen Schachcomputer sofort bis zur Serienreife weiterzuentwickeln. Dabei ergaben sich folgende Schwerpunkte für die Entwicklungstätigkeit:\n\nDas Gehäuse des SC1 mit Holzrahmen und die Montage mit vielen Schrauben wäre für eine Serienfertigung sehr kostenintensiv gewesen. Außerdem hatte das Funkwerk keine Fertigungskapazität für Holzerzeugnisse.\n\nUm kostengünstig produzieren zu können, wurde entschieden, ein Kunststoffgehäuse zu konstruieren. Das erforderliche Werkzeug sollte schnell und billig entstehen, was zu der Entscheidung führte, ein Gehäuse aus Polyurethan einzusetzen. Dieses konnte einfach gefertigt werden. Nachteilig war die dicke Materialstärke, wodurch auch keine hohen gestalterischen Ansprüche erfüllt werden konnten. Dafür war die Lösung schnell umgesetzt und es entstand der SC2.\n\nVom SC2 wurden in den Jahren 1981 bis 1983 einige hundert Exemplare, vorwiegend im Inland, verkauft. Die Vorstellung des SC2 auf Messen und die Marktforschung ergaben keine guten Exportmöglichkeiten in das westliche Ausland, was jedoch eine wichtige Aufgabe für die Produzenten von Konsumgütern in der DDR war. Für die Inlandnachfrage war der SC2 wiederum zu teuer.\n\nMan setzte den Export in westliche Länder als Devisenbeschaffung als ausdrückliches Ziel. Daraufhin wurde der Schachcomputer für den Export in westliche Länder weiterentwickelt.\n\nWegen der Sättigung der westlichen Märkte mit Billigprodukten an Schachcomputern wurde als Zielgruppe speziell der gehobene Bedarf definiert. Dies erforderte insbesondere ein niveauvolles Gehäuse und anspruchsvollere Schachprogramme.\n\nSLC1 (Schach- und Lerncomputer 1) ist ein Schachcomputerbausatz der 1989 erschien.\n\nIm Funkwerk wurde daraufhin im Jahr 1985 eine spezielle Abteilung für die Entwicklung der Schachcomputer und anderer Konsumgüter gegründet. Für diese Abteilung wurden zwei ausgezeichnete Schachspieler mit Programmierfähigkeit gewonnen. Für die Gehäusegestaltung der Schachcomputer und weiterer Konsumgüter wurden die besten Konstrukteure und Entwickler eingesetzt und zwei Designer eingestellt. Insgesamt haben bis zu 28 Ingenieure und Facharbeiter Konsumgüter, primär Schachcomputer, entwickelt. Damit wurden Ressourcen für eine selbständige Produktlinie gebildet, denen auch eine eigene Fertigungsstätte unweit von Erfurt in Plaue zugeordnet wurde.\n\nDie meisten Produkte auf dem westlichen Markt waren Massenfertigung und in Plastgehäusen untergebracht. Beim gehobenen Bedarf ging man bei der Zielgruppe auch von aktiven Schachspielern als potentielle Kunden aus, die sicher ein Schachbrett aus Holz bevorzugen würden. Die Normgröße der Schachbretter wurde also unter diesem Aspekt als wichtig angesehen und realisiert.\n\nAls weitere Zieleigenschaft wurde die Eingabe direkt durch die Figuren auf das jeweilige Schachfeld angesehen. Für die Eingabe über die Schachfigur wurde ein magnetisches Wirkprinzip gewählt. Alle Schachfiguren wurden unten aufgebohrt und bekamen kleine, runde Magnete eingeklebt.\n\nEin technisch zu lösendes Problem war nun, das Magnetfeld durch das 12 mm dicke Schachbrett auf die Leiterplatte zu den dort befindlichen Sensoren (Hall-Generatoren) zu bekommen. Diese Wirkung konnte nur erzielt werden, indem in das Schachfeld für jedes einzelne Feld ein Stahlstift eingelassen wurde. Danach kamen mit einer präzisen Technologie die dünnen Furniere beidseitig auf die hölzerne Trägerplatte. Die Erfinder erhielten für diese Lösung Patente.\n\nDie Anzeige der berechneten Schachzüge erfolgte wiederum mit LEDs auf den betreffenden Schachfeldern.\n\nDas Gehäuse dieses Schachcomputers war also eine anspruchsvolle Holzkonstruktion mit in feinwerktechnischer Präzision eingearbeiteten Stahlstiften. Als Produzent hierfür wurde eine Möbel-Produktionsgenossenschaft (PGH) gewonnen, die mit der geforderten Präzisionsarbeit eine außergewöhnliche Herausforderung gemeistert hat.\n\nEntstanden ist ein hochwertiger, auch ins westliche Ausland verkaufter Schachcomputer: der Chess Master – CM mit neuem Programm, Holzgehäuse, magnetischer Eingabe und Ausgabe mit LED-Anzeigen.\n\nDer CM wurde später mit Bauelementen mit höherer Arbeitsgeschwindigkeit unter gleicher Bezeichnung weiterentwickelt, wobei die übrigen Eigenschaften erhalten blieben. Er wurde 1984 auf Messen vorgestellt und anschließend zu Tausenden ins westliche und östliche Ausland sowie im Inland verkauft. Der Verkaufspreis in der DDR betrug 1580 Mark. Bei einer Schachcomputerweltmeisterschaft in Budapest belegte er den zweiten Platz.\n\nDieser Schachcomputer stand mit seinem Namen Pate für das DDR-Computerspiel Video Chess-Master.\n\nFür die Weiterentwicklung des CM wurde für die Entwickler vorgegeben, zusätzliche Programme schnell für Kunden zugänglich zu machen. Die gewählte technische Lösung hierfür waren von außen einschiebbare, auswechselbare Kassetten. Zum optionalen Lieferumfang gehörte je ein Eröffnungs- und Endspielmodul, die unterhalb der Tastatur eingesteckt werden konnten.\n\nWichtigstes äußeres Unterscheidungsmerkmal in Bezug auf den CM war ein zusätzliches Kommunikationsfeld rechts oben (siehe Abbildung), das als vierstellige LED-Anzeige ausgeführt war. Das Schachprogramm konnte hier unter Verwendung von 16-Segment-Anzeigen aktuelle Informationen zum Spielstand und zur Bedienung ausgeben.\n\nIn Hinblick auf den Verkauf ins westliche Ausland wurde der Name des CM klangvoll ergänzt: CM diamond.\n\nDem Export geschuldet ist auch die sehr attraktive Gestaltung der Verpackung und der Bedienungsanleitung des Schachcomputers, damals vorgenommen von einem auf dem westlichen Markt erfahrenen Designer.\n\nDer CM diamond wurde ab 1987 im In- und Ausland, nun auch stärker ins westliche Ausland verkauft.\n\nWurde mit dem CM schon erfolgreich der gehobene Bedarf gedeckt, ging die Zielrichtung für die Weiterentwicklung an einen speziellen, solventen Kundenkreis, der auch die repräsentative Wirkung des Schachspiels schätzte. Die Konstrukteure entwickelten einen Schachtisch, in dem die komplette Elektronik integriert war. Dies war eine Weltneuheit, die dann nach kurzer Bewertung des Marktes auch sofort umgesetzt wurde.\n\nAls Elektronik wurde die des CM verwendet. Der Tisch selbst war eine spezielle Konstruktion unter Berücksichtigung der konkreten Anforderungen. Damit die Funktion als Tisch gesichert werden konnte, waren die Bedienelemente seitlich einschiebbar gehalten, so dass im eingeschobenen Zustand ein normaler Schachtisch sichtbar war. Das Tischbein war hohl, um darin das Netzkabel unterbringen zu können. Unter der Tischplatte nahmen Aluminiumprofile die Kräfte der Tischplatte auf und übertrugen diese auf das Tischbein.\n\nFür den Transport wurden das Tischbein und die Füße abgeschraubt, so dass eine relativ transportfreundliche Größe entstand (das gleiche Prinzip, das auch bei IKEA-Möbeln angewandt wird). Als Umverpackung wurde eine Holzkiste verwendet, die auch die Stoßsicherheit des Schachcomputertisches beim Transport gewährleistete.\n\nVom Schachtisch wurden einige Dutzend produziert und zumeist in westliche Länder exportiert.\n\nMit der Massenproduktion von mikroelektronischen Bauelementen wurden diese auch preisgünstiger, so dass die Produktion von Schachcomputern zu erschwinglichen Preisen ins Auge gefasst werden konnte. Für dieses Anliegen wurden Designer tätig. Für einen völlig neuen Schachcomputer wurde ein erstes Gestaltungsmuster, noch ohne jegliche elektrische Funktion, hergestellt. Die Umsetzung dieses Erzeugnisses wurde mit dem Niedergang der DDR nicht mehr realisiert.\n\n"}
{"id": "1323502", "url": "https://de.wikipedia.org/wiki?curid=1323502", "title": "Taskmanager", "text": "Taskmanager\n\nDer Windows Taskmanager (in den Medien wird oft auch die Eigenschreibweise von \"Microsoft\" übernommen: Task-Manager) ist ein Programm, in der Regel als Bestandteil des Betriebssystems, das als Prozessmanager unter anderem die laufenden Programme und Prozesse anzeigt und verwaltet. Für die Verwendung dieses Programms auf Prozessebene ist Fachwissen vonnöten.\n\nDer Taskmanager ist in allen NT-basierten Windows NT-Versionen integriert, in seiner heutigen Form allerdings ab Windows NT 4.0. Darunter fallen auch Windows 2000, Windows XP, Windows Server 2003, Windows Server 2008, Windows Vista, Windows 7, Windows 8 und Windows 10. In den neueren Betriebssystemversionen wurden allerdings zum Teil neue Funktionalitäten hinzugefügt. Zuvor wurde das Dialogfenster \"Windows NT-Sicherheit\" vorangestellt, von wo auch grundlegende Systemfunktionen wie etwa Herunterfahren, Kennwort ändern oder eben auch der Taskmanager (dort \"Task-Liste\" genannt) wählbar waren. Diese beiden Dialogfenster wurden in Windows NT 4.0 zusammengefasst.\n\nDas Programm zeigt die auf dem Computer laufenden Programme und Prozesse in jeweils einer Übersicht an. Der Vorteil liegt darin, dass auch solche angezeigt werden, die auf der Windows-Oberfläche nicht sichtbar sind. Das ist vor allem dann hilfreich, wenn es zu Problemen kommt, wenn ein Programm beispielsweise nicht mehr reagiert. Man kann damit dann Prozesse gezielt beenden, auch wenn sie auf der Oberfläche nicht zu sehen sind. Dabei ist zu beachten, dass auch Systemprozesse mit aufgelistet werden. Diese lassen sich, auch mit einem Administratorenkonto, wegen Standard-Sicherheitseinstellungen in der Regel nicht ohne weiteres direkt beenden.\n\nAls Zusatzinformation wird die Prozessor- und Speicherauslastung angezeigt. Ab Windows XP wird auch der Netzwerkverkehr angezeigt, und es kann zu einem anderen angemeldeten Benutzer gewechselt werden. Mit Windows 8 wurde das Design grundlegend geändert, und es wurden neue Reiter wie beispielsweise „Autostart“ eingeführt.\n\nDas Programm kann folgendermaßen gestartet werden:\n\nBeim Absturz des Windows-Explorers (erkennbar am Verschwinden oder Aufhängen der Taskleiste) kann mit Hilfe des Taskmanagers dieser neugestartet (Datei > Ausführen... > explorer) bzw. andere Programme aufgerufen werden.\n\nBei Windows 9x gibt es lediglich das Dialogfenster \"Anwendung schließen\", welches durch einmaliges Drücken der Tastenkombination ++ angezeigt werden kann, solange keine Windows-Meldung über den Absturz eines Programms geöffnet ist. Es wird meist ebenfalls als Taskmanager bezeichnet. Die meisten Systemprozesse werden dort allerdings nicht angezeigt, standardmäßig lediglich die Einträge \"Systray\" und \"Windows Explorer\". Zudem lässt sich Windows von hier aus ohne Nachfrage sofort herunterfahren.\n\nUnter Windows 3.x zeigt der „Affengriff“ ohne vorherigen Programmabsturz lediglich die Bluescreen-ähnliche Warnmeldung an, dass ein erneutes Betätigen dieser Tastenkombination zu einem Windowsneustart führt, ohne zuvor Daten zu speichern. Das Dialogfeld \"Task-Liste\", mit dem auch Symbole und Fenster neu anordnen lassen, ist über den \"Ausführen…\"-Dialog „Taskman“ (oder bei einer nicht reagierenden Anwendung über den Affengriff) erreichbar. Unter noch früheren Windowsversionen (1.x und 2.x) führt diese Tastenkombination, wie unter DOS, zum sofortigen Computerneustart ohne Datenspeicherung und Rückfrage.\n"}
{"id": "1329636", "url": "https://de.wikipedia.org/wiki?curid=1329636", "title": "AppImage", "text": "AppImage\n\nAppImage ist ein System zur einfachen Nutzung von Software auf Linux-Systemen. Es stellt eine distributionsübergreifende Alternative zu den zentral verwalteten Paketmanagersystemen der Linux-Distributionen dar. Vorgänger war das 2004 geschaffene System klik. \"klik\" wurde zwischenzeitlich durch das Nachfolgeprojekt \"PortableLinuxApps\" mit vergleichbaren Zielen ersetzt.\n\nAppImages kommen ohne jegliche Installation auf dem System aus, sie können sogar direkt von CD-ROM oder vom USB-Stick portabel benutzt werden.\n\nDa AppImages, soweit möglich, alle verwendeten Programmbibliotheken mitführen, laufen diese auf allen verbreiteten Desktop-Distributionen wie Ubuntu, openSUSE, Fedora, Debian, Arch Linux oder Red Hat Linux gleichermaßen ohne spezifische Anpassungen. Mit AppImage können Programme wie LibreOffice, Firefox, Blender, DigiKam, Kdenlive oder Gimp in beliebigen Versionen verwendet werden, unabhängig von oder parallel zu der Version, die die Distribution selbst über ihr Repository vertreibt.\n\nDas Kopieren und Starten von AppImages erfordert keine root-Rechte. Es reicht aus, die entsprechende Datei in das Nutzerverzeichnis oder auf einen beliebigen Datenträger zu kopieren, sie ausführbar zu machen und anschließend zu starten. Viele AppImages fragen beim ersten Start, ob sie in ein Menü eingebunden werden sollen. Einige AppImages suchen nach Aufforderung nach neueren Versionen und schlagen ggf. einen entsprechenden Download vor.\n\nMöchte man die jeweilige Applikation nicht mehr nutzen, muss nur die entsprechende .app- oder .AppImage-Datei gelöscht werden.\n\nFür jede Anwendung wird nur eine einzige Datei, meist mit der Endung .AppImage (oder .app) benötigt. Diese stellt ein komprimiertes Dateisystem-Abbild dar, ähnlich einem ISO-Abbild. Die Datei wird beim Start der eingebetteten Anwendung zeitweilig in das Dateisystem eingebunden (engl. ) und mittels eines Wrapper-Skripts gestartet. Auf diese Weise kann ein AppImage-Benutzer sogar verschiedene Versionen derselben Anwendung gleichzeitig auf demselben System nutzen. Die Datei enthält neben der eigentlichen Programmdatei alle nötigen Bibliotheken und andere Komponenten, von denen das Hauptprogramm abhängt.\n\nDie AppImage-Datei kann entweder von einem entsprechenden Server aus dem Internet heruntergeladen oder selbst erzeugt werden. Dazu stehen auf der AppImage-Homepage sogenannte „Rezepte“ zur Verfügung. Diese laden automatisch alle benötigten Komponenten von den Seiten der Hersteller herunter und verpacken sie in eine Image-Datei.\n\nUm die Sicherheit der Anwendung zu erhöhen, kann diese in einer Sandbox wie FireJail, AppArmor oder BubbleWrap ausgeführt werden.\n\nAnwendungen mit AppImage laufen zwar unabhängig von der Distribution, sind aber eher schlecht ins Gesamtsystem integrierbar. Da alle Abhängigkeiten mit eingebettet sind, versagen hier die Aktualisierungssysteme der Distributionen, da sie nichts innerhalb des AppImages aktualisieren können. Die Sicherheit des Systems hängt also mit jedem installierten AppImage auch von einer weiteren Aktualisierungsinstanz ab. Allerdings gibt es Werkzeuge, die es ermöglichen, Updates aus dem Programm und den eingebetteten Bibliotheken, Strukturen und Diensten automatisch zu bauen und über einen Delta-Update-Mechanismus an die Nutzer zu verteilen.\n\nDie AppImages benötigen längere Startzeiten im Vergleich zu herkömmlich über Paketmanagement installierte Anwendungen. Die mitgebrachten Abhängigkeiten werden unabhängig instanziert, belegen also Speicher, selbst wenn die gleichen Bibliotheken bereits von anderen Programmen geladen und instanziert wurden. Das Teilen von Ressourcen, ein wesentlicher Bestandteil des Betriebskonzeptes, wird somit unterlaufen.\n\nFür die Isolation eines AppImage-Programms muss ein externes Sandboxprogramm verwendet werden.\n\nDer AppImage-Vorgänger klik wurde seit 2004 vorwiegend von Simon Peter entwickelt. Anfang 2010 schlief die Entwicklung ein und die Projekt-Homepage war nicht mehr zu erreichen. Diese wurde für einige Zeit wieder online gestellt, um als Referenz zu dienen, ist mittlerweile aber wohl endgültig vom Netz.\n\nDer Hauptentwickler arbeitete zwischenzeitlich am Nachfolgeprojekt, das sich \"PortableLinuxApps\" nannte und vergleichbare Ziele verfolgte. Dazu gehören Einfachheit, Binärkompatibilität, Distributionsunabhängigkeit, Nutzbarkeit ohne Installation, auch von Wechseldatenträgern wie USB-Sticks aus und ohne Veränderungen am installierten Linux-Betriebssystem vorzunehmen sowie die Nutzung als normaler User ohne Administratorrechte.\n\nEtwa 2013 wurde die Software erneut umbenannt, heißt nun AppImage und läuft unter einer MIT-Lizenz. AppImage ist das neue Format der Abbild-Dateien und das \"AppImageKit\" dient zu deren Erstellung. Die Entwicklung wird nun in einem GitHub-Verzeichnis dokumentiert und ist gegenwärtig (2018) aktiv.\n\nSeit August 2017 werden AppImages in einem dafür eingerichteten Hub zur Verfügung gestellt.\n\n\n"}
{"id": "1333501", "url": "https://de.wikipedia.org/wiki?curid=1333501", "title": "Windows Embedded", "text": "Windows Embedded\n\nWindows Embedded ist die zusammenfassende Bezeichnung der Produkte für eingebettete Systeme von Microsoft.\n\nWindows Embedded Standard basiert auf herkömmlichen Windows-Systemen, wird jedoch in stark modularisierter Form angeboten. Damit können Hardwarehersteller für bestimmte Zwecke angepasste Versionen von Windows herstellen und in ihre Hardware integrieren. \n\nWindows Embedded Standard 7 (Codename: Quebec) basiert auf Windows 7.\n\nWindows Embedded Standard 2009 basiert auf Windows XP und besteht aus ca. 12.000 Komponenten. \n\nWindows XP Embedded besteht aus ca. 10.400 Komponenten. \n\nWindows NT 4.0 Embedded wird auch kurz Windows NT Embedded genannt. Es basiert auf Windows NT 4.0 mit Service Pack 5. Die erweiterten Möglichkeiten im Vergleich zu anderen Varianten von Windows NT 4.0 sind erheblich und umfassen die komplette Fernwartung mit Systemstart ohne Tastatur sowie von Solid-State-Drives. \n\nWindows Embedded Enterprise entspricht im Wesentlichen den Verkaufsversionen von Windows und wird für OEMs mit speziellen Integrationswerkzeugen und Lizenzmodellen bereitgestellt. Microsoft wirbt für diese Systeme mit vollständiger Anwendungskompatibilität. Derzeit werden folgende Versionen angeboten:\n\n\nDiese Produktlinie wurde ehemals als Windows Embedded Classic Line bezeichnet.\n\nWindows Embedded Server greift den gleichen Gedanken wie die Windows Embedded Enterprise-Linie auf und stellt Versionen der Server-Betriebssysteme von Microsoft bereit. Die wesentlichen Unterschiede sind auch hier die Lizenzbedingungen, zu denen das Produkt angeboten wird. Zu den verfügbaren Systemen zählen hier unter anderem:\n\n\nMicrosoft bietet die Grundsysteme auch teilweise mit verschiedenen Softwareausstattungen und in verschiedenen Ausführungen an, z. B. als Standard, Enterprise, Appliance oder Compute Cluster Edition, mit ISA Server oder mit Data Protection Manager.\n\nWeiterhin gehören auch Produkte wie Microsoft SQL Server for Embedded Systems in diese Produktlinie, obwohl sie kein eigenständiges Betriebssystem darstellen.\n\nWindows Embedded Compact ist die neue Bezeichnung für Windows CE. Es handelt sich dabei um eine eigenständige Entwicklungslinie, die nicht auf den Desktop- oder Server-Systemen von Microsoft beruht. Kompatibilität ist nur bedingt vorhanden. Es sind spezielle Treiber nötig, und herkömmliche Windows-Software ist nicht ohne Weiteres lauffähig. Microsoft bietet das .NET Compact Framework zur einfachen Entwicklung von Software an.\n\nAufbauend auf den genannten Windows Embedded-Systemen bietet Microsoft weitere Produkte an, die in diese Sparte gehören. Dazu zählen:\n\n\nWindows Embedded lässt sich bis zu einem vollwertigen Betriebssystem ausbauen (im Falle von Windows Embedded Standard 2009 bis zu XP Professional SP2 mit Treibern von SP3), doch gibt es einige Einsatzeinschränkungen in den Lizenzbestimmungen;\nso dürfen z. B. keine Desktopsysteme mit Windows Embedded Standard 2009 ausgestattet werden.\n\nUm Laufzeitlizenzen zu beziehen, muss ein Vertrag mit Microsoft abgeschlossen werden, das sogenannte „Microsoft OEM Customer License Agreement for Embedded Systems“. Erst dann ist der Kauf möglich.\n\n"}
{"id": "1333516", "url": "https://de.wikipedia.org/wiki?curid=1333516", "title": "Cryptographic File System", "text": "Cryptographic File System\n\nDas Cryptographic File System (CFS) ist ein auf einen NFS-Daemon aufsetzendes verschlüsseltes Dateisystem, das unter verschiedenen unixähnlichen Betriebssystemen (wie z. B. BSD und Linux) eingesetzt werden kann. Es wurde von Matt Blaze bei AT&T Research entwickelt und unter der BSD-Lizenz freigegeben. Die von der aktuellen CFS-Version von Matt Blaze unterstützten Verschlüsselungsverfahren sind DES im Hybridmodus mit zwei Schlüsseln, 3DES mit drei Schlüsseln, Blowfish mit 128-Bit-Schlüssel, MacGuffin und SAFER-SK128.\n\nEigenschaften von CFS:\n\n"}
{"id": "1334725", "url": "https://de.wikipedia.org/wiki?curid=1334725", "title": "OCFS2", "text": "OCFS2\n\nBei OCFS2 (Oracle Cluster File System 2) handelt es sich um ein Open-Source Cluster-Dateisystem von der Firma Oracle für Linux, welches in einem Computercluster konkurrierenden Zugriff auf eine Shared Storage ermöglicht. Die Koordination erfolgt über den Distributed Lock Manager.\n\nEs ist seit Kernel 2.6.16 offizieller Bestandteil des Linux-Kernels.\n\n\n"}
{"id": "1336456", "url": "https://de.wikipedia.org/wiki?curid=1336456", "title": "Mehrkörpersimulation", "text": "Mehrkörpersimulation\n\nDie Mehrkörpersimulation (MKS) ist eine Methode der numerischen Simulation, bei der reale Mehrkörpersysteme durch mehrere unverformbare Körper abgebildet werden. Zusätzlich wird die Bewegungsfähigkeit der Körper zueinander durch idealisierte kinematische Gelenke eingeschränkt.\n\nGrundsätzlich wird bei der Mehrkörpersimulation zwischen dynamischer und kinematischer Simulation unterschieden. Bei der einfacheren Kinematik haben die Systeme keine dynamischen Freiheitsgrade. Meist wird der letzte Freiheitsgrad eines mechanischen Systems mit einer Zwangsbewegung gesperrt, wodurch dieser verschwindet. Eine Kinematik lässt sich dadurch charakterisieren, dass jeder Betriebspunkt als Funktion der gegebenen Zwangsbewegung betrachtet werden kann. Dabei ist die Zeit bzw. das Geschwindigkeitsprofil, das zu dieser Position geführt hat, unerheblich.\n\nIn einem dynamischen Modell kann ein Betriebspunkt nur durch Auflösen einer Differentialgleichung ermittelt werden. Nur bei extrem einfachen Systemen, die sich im Normalfall durch lineare Bewegungsgleichungen oder einen einzelnen Freiheitsgrad auszeichnen, kann dies analytisch geschehen. Daher besitzen Mehrkörpersimulationsprogramme immer ein oder mehrere Lösungsverfahren zur numerischen Integration, z. B. Runge-Kutta-Verfahren.\n\nDie Mehrkörpersimulation ist eine sehr grobe Vereinfachung der realen Welt. Um ein System detaillierter und genauer abzubilden, wird das Verfahren daher oft mit anderen Simulationsverfahren kombiniert. Dabei werden die Methoden der Finite-Elemente-Methode (FE), numerischen Strömungssimulation, Thermodynamik, Regelungstechnik, wie auch spezielle Programme für Reifen, Gummielemente, Hydrolager und weitere Konstruktionssimulationen in das Mehrkörpermodell integriert.\n\nEine besondere Methode in Verbindung mit der Mehrkörpersimulation ist die \"modale Reduktion\". Hierbei wird ein Körper, dessen Flexibilität nicht zu vernachlässigen ist, anhand seiner externen Eigenschaften abgebildet. Hierzu muss jedoch vor der eigentlichen Simulation festgelegt werden, wo die Anschlusspunkte an das restliche System sind. Die Bewegung des flexiblen Körpers wird anschließend durch ein Reduktionsverfahren auf die Starrkörperbewegung sowie eine definierte Anzahl von Bewegungsfreiheitsgraden reduziert (z. B. Eigenformen aus einer Modalanalyse beim Craig-Bampton-Verfahren). Dank schneller Prozessoren und moderner Formulierungen des Gleichungssystems wird jedoch die direkte Integration von flexiblen Körpern immer beliebter. Dabei werden die Netze, wie sie aus der FE bekannt sind, und die Mehrkörpersysteme direkt in einem Gleichungssystem zusammengefasst.\n\nKinematische Systeme sind Bestandteil unseres täglichen Lebens. Sie reichen von einfachen Pendeln bis zu kompletten Fahrzeugen. Mit der Mehrkörpersimulation kann der Bewegungsablauf solcher Systeme berechnet und analysiert werden. Die Simulation liefert Ergebnisse über Kräfte, Geschwindigkeiten, Beschleunigungen und Kontakte der Körper.\n\nIm Automobilbereich werden MKS-Systeme seit mehreren Jahren intensiv eingesetzt, z. B. zur Analyse von Fahrwerken. Hierfür gibt es besondere Erweiterungen der MKS-Programme.\nEin weiteres Beispiel für den Einsatz der MKS ist die Analyse von Ladespielen bei Löffelbaggern. Es werden z. B. die dynamischen Belastungen in den Lagerpunkten berechnet.\n\nDas MKS-Modell kann zusätzlich durch die Integration des Hydrauliksystems erweitert werden. Kräfte für die Bewegung des Auslegers werden dann aus der Hydrauliksimulation bereitgestellt.\nDie Integration einer FE-Analyse in das MKS-Modell ermöglicht eine Berechnung der Bauteilbelastungen während der Bewegung.\n\n\n\n"}
{"id": "1336898", "url": "https://de.wikipedia.org/wiki?curid=1336898", "title": "Whirlwind", "text": "Whirlwind\n\nDer Whirlwind-Computer (von engl. \"\"Whirlwind\"\" für \"\"Wirbelwind\"\") wurde von 1945 bis 1952 am Massachusetts Institute of Technology nach einem während des Zweiten Weltkrieges vergebenen Auftrag der US-Marine durch Jay Forrester und Robert Everett entwickelt. Der Whirlwind sollte ein Flugsimulator werden, in dem Piloten der US-Marine mit überraschenden Situationen umgehen lernen sollten. Dazu musste er Fähigkeiten als Echtzeitrechner haben. Forrester und seine Kollegen Perry Crawford und Robert Everett bauten zunächst einen Analogrechner, kamen aber nach einer der ersten ENIAC-Demonstrationen auf die Idee, einen Digitalrechner einzusetzen. Es war der erste Rechner mit Echtzeitverarbeitung, die nach einer Idee von Forrester durch einen Array von Kernspeichern realisiert wurde, und der einen Bildschirm (Kathodenstrahlröhre) als Ausgabegerät verwendete. Das System startete erstmals am 20. April 1951. Eingesetzt wurden zuerst Williamsröhren (Williams-Kilburn CRT-Speicher-Röhren) und später ein (schnellerer) magnetischer Kernspeicher als Speicherbausteine. Ein Lichtgriffel, der am MIT Lincoln Laboratory entwickelt wurde, diente als Eingabegerät. Der Rechner selbst bestand im Wesentlichen aus 5000 Röhren und 11000 Halbleiter-Dioden und nahm fast eine ganze Halle ein.\n\nNach der mehrjährigen Entwicklungsphase wollte die Navy ihn nun nicht mehr als Flugsimulator, sondern für numerische Aufgaben einsetzen, während die MIT-Forscher ihn weiter für Bahnverfolgung von Flugzeugen und Raketen benutzen wollten und sich durchsetzten. Das Whirlwind-Projekt war Ende der 1940er Jahre das größte Computer-Entwicklungsprojekt mit einem jährlichen Etat von 1 Million Dollar und 175 Mitarbeitern. Noch mehr Geld stand zur Verfügung, als die USA als Antwort auf die nukleare Bedrohung durch sowjetische Langstreckenbomber das SAGE-Projekt der US Air Force starteten, das ebenfalls am MIT entwickelt wurde. Whirlwind wurde Teil des Projekts, diente dort als Basis der Entwicklung besserer Computer und trug mit seinen Innovationen indirekt zur Entwicklung fast aller Computer in den 1960er Jahren bei. Whirlwind war der erste Computer mit einer Wortlänge von 16 Bit, was sich dann bei den Minicomputern in den 1960er Jahren durchsetzte.\n\nNachfolger waren der TX-0 (bereits transistorbasiert) am MIT und die PDP-1 bei der Digital Equipment Corporation (DEC).\n\nNach der Verwendung bei SAGE wurde es 1959 von einem der Projektmitglieder (Bill Wolf) für einen symbolischen Dollar pro Jahr gemietet. 1974 retteten Ken Olsen und Robert Everett wesentliche Teile für das Digital Computer Museum bei DEC, über das es zum Computer History Museum in Mountain View kam. Eine Einheit des Kernspeichers ist auch im Charles River Museum of Industry in Waltham (Massachusetts) zu sehen.\n\nIn Anlehnung an den Whirlwind (Wirbelsturm) erhielt der von Heinz Zemanek entwickelte Transistorrechner Mailüfterl seinen Namen.\n\n\n"}
{"id": "1339981", "url": "https://de.wikipedia.org/wiki?curid=1339981", "title": "ArCon", "text": "ArCon\n\nArCon ist ein 2D-/3D-CAD-Programm für PCs und dient der Konstruktion, Visualisierung und Kostenschätzung von Bauvorhaben jeder Art.\n\nArCon hat umfangreiche Funktionen, um Bauvorhaben zu planen und insbesondere dreidimensional zu präsentieren. Aus den 3D-Modellen lassen sich auch 2D-Planungen und Massenermittlungen generieren. Das Programm verfügt über verschiedene Schnittstellen zum Datenimport bzw. -export zu anderen CAD-Systemen. Die Programm-Versionen ArCon + (das \"+\" kennzeichnet die Profiversionen) laufen unter der zum Release-Zeitpunkt aktuellen Windows-Version und sind abwärtskompatibel. Die Systemvoraussetzungen für die aktuelle Version lauten für das Betriebssystem mindestens Windows 7 und für die Grafikkarten 3D-fähig, DirectX/OpenGL, 128 MB (Stand Mai 2014).\n\nNach der Insolvenz des Erstentwicklers \"mb Software AG\" im Jahr 2001 wird ArCon in zwei Produktlinien weiterentwickelt. Die Produktlinien werden mit ArCon Eleco und ArCon planTEK bezeichnet. Zudem werden unter dem Namen \"ArCon\" diverse „Bauherrenversionen“ verkauft, diese haben gegenüber den professionellen Versionen einen stark eingeschränkten Funktionsumfang. Abhängig von der Version des jeweiligen Rechteinhabers wird eventuell ein Dongle als Kopierschutzsystem benutzt. Die Versionen von ArCon planTEK laufen generell ohne Kopierschutzsystem.\n\nArCon verbirgt sich auch hinter vielen Haus- und Garten-Planungs-Programmen wie z. B. \"RTL 3D Software – Einsatz in 4 Wänden\" etc.\n\n\n"}
{"id": "1344998", "url": "https://de.wikipedia.org/wiki?curid=1344998", "title": "Konversation (IRC-Client)", "text": "Konversation (IRC-Client)\n\nKonversation ist ein freier grafischer IRC-Client, der auf der KDE-Plattform aufbaut.\n\nKonversation ermöglicht mehrere simultane Verbindungen zu IRC-Servern (wahlweise mit SSL und/oder IPv6). Darüber hinaus unterstützt Konversation eine Blowfish-Verschlüsselung. Das Programm kann automatisch erkennen, ob in einem Chat UTF-8 verwendet wird und unterstützt diese Kodierung vollständig. Es kann mehrere Identitäten verwalten, so dass man für unterschiedliche Server ggf. unterschiedliche Einstellungen verwenden kann.\n\nWeitere Funktionen sind ein On-Screen-Display, Lesezeichen für Server und Channels sowie DCC-Verbindungen.\n\nKonversation basiert auf kdebase und integriert sich daher optimal in die KDE Oberfläche. Benutzer, die viel mit der Kommandozeile arbeiten und viel chatten, können Konsole in einem Tab dieses IRC-Clients laufen lassen. Chatpartner können mit Einträgen im KAddressbook verknüpft werden. Dadurch wird dann das in KDE global für einen Kontakt festgelegte Bild auch in Konversation angezeigt. KDE wiederum zeigt dann an verschiedenen Stellen an, ob der betreffende Kontakt derzeit im Chat verfügbar ist, z. B. wenn eine E-Mail von ihm mit KMail angezeigt wird.\n\nDurch die Nutzung von KDE Input/Output (KIO) können mittels DCC Dateien an einem Chatpartner versendet werden. Mit dem Kommando\n\nkann der Text dieses Wikipedia-Artikels direkt an den Benutzer \"Nick\" geschickt werden. Auf gleiche Weise ist der Zugriff auf FTP-, SSH- und viele weitere Dienste möglich.\n\nÜber Shell-Skripte und D-Bus lassen sich der Funktionsumfang von Konversation erweitern und Programmfunktionen in automatisierte Abläufe integrieren. Beispielsweise wäre es denkbar, mit Hilfe eines solchen Skripts, bei Auftreten eines Alarms in der Terminverwaltung KOrganizer automatisch eine Nachricht in einen bestimmten Channel schicken zu lassen.\nEine Skript-funktionalität wie z. B. bei irssi ist jedoch noch nicht möglich.\n\n"}
{"id": "1347749", "url": "https://de.wikipedia.org/wiki?curid=1347749", "title": "Konr@d", "text": "Konr@d\n\nKonr@d (Eigenschreibweise: KONR@D) war ein deutsches Lifestyle-Magazin, das zwischen August 1997 und Dezember 1999 mit dem Untertitel „Der Mensch in der digitalen Welt“ im Verlagshaus Gruner + Jahr erschien und sich als „Special des Stern“ verstand. golem.de bezeichnete die Zeitschrift bei ihrer Einstellung als „renommiertestes deutsches Magazin um Internetkultur und Lifestyle“.\n\nMitte der 1990er Jahre beabsichtigte das Hamburger Verlagshaus Gruner + Jahr eine Beteiligung an der geplanten deutschen Ausgabe des US-amerikanischen Technologiemagazins Wired; im Oktober 1996 scheiterten jedoch die Verhandlungen mit dem Condé Nast Verlag. Der Hamburger Verlag entschied sich deshalb, eine entsprechende Zeitschrift auf dem deutschen Markt etablieren zu wollen. Dazu konnte man Peter Glaser als Kopf der Redaktion gewinnen. Ende August 1997 erschien nach sieben Monaten Entwicklung die erste Konr@d-Ausgabe in einer Auflagenstärke von 120.000, in der Folgezeit erschien das nach Konrad Zuse benannte Magazin in unregelmäßigen Abständen etwa alle zwei Monate. Die Zeitschrift befasste sich mit Computer, Lifestyle und Zukunft. Die inhaltlich schwankende Qualität sowie die Einführung von besser vermarkteten Zeitschriften mit ähnlichem Spektrum wie Tomorrow führten zur Einstellung von Konr@d nach der 13. Ausgabe, die am 16. Dezember 1999 erschien. Die Themenbereiche des Magazins sollten fortan in die Rubrik „Internet und Computer“ des Stern integriert werden.\n\nUrsprünglich geplant war eine Auflage von etwa 60.000, wie Erwin Jurtschitsch Mitte 1996 angab. Nach Verlagsangaben verkaufte sich Konr@d bis zu 220.000-mal, bei den letzten Ausgaben soll die verkaufte Auflage bei 115.000 gelegen haben. Der Preis der Zeitschrift lag bei 4,80 DM („Einführungspreis“ der ersten Ausgabe) bis 5,80 DM (zwölfte Ausgabe).\n\n"}
{"id": "1351346", "url": "https://de.wikipedia.org/wiki?curid=1351346", "title": "Industrie-PC", "text": "Industrie-PC\n\nEin Industrie-PC (kurz IPC) ist ein Computer, der für Aufgaben im industriellen Bereich eingesetzt wird. Im engeren Sinn geht es dabei um Rechner, die einem IBM-kompatiblen Personal Computer ähneln und insbesondere mit Software für solche Geräte betrieben werden können.\n\nTypische Bereiche sind Prozessvisualisierung, Robotik, Industrieautomation, Test- und Prüfstände für die Industrie oder Sicherheitstechnik sowie die Qualitätssicherung. Ein Industrie-PC muss gegenüber den Geräten für den Bürobereich (Office-PC) besonderen Anforderungen genügen und wird in der Regel besonders robust z. B. gegenüber Umwelteinflüssen oder elektromagnetischen Störungen und insgesamt weitgehend ausfallsicher ausgelegt.\n\nKonventionell konzipierte PCs haben infolge der Massenproduktion viele Vorzüge, z. B.: hoher Standardisierungsgrad – sowohl hinsichtlich Hardware als auch Software –, Flexibilität, großes Angebot an Peripheriekomponenten und Anwendungssoftware, günstiger Preis. Daraus entstand der Wunsch, diese auch für die Automatisierung einzusetzen. Aufgrund der hohen Flexibilität lässt sich ein PC für die Bedienung, Programmierung, Visualisierung, Langzeit-Archivierung und Simulation von Prozessen einsetzen und darüber hinaus mit herkömmlichen industriellen Steuerungen oder SPS kombinieren.\n\nEine allgemeine Einordnung der verschiedenen Anforderungen wird durch die Schutzart angegeben.\n\nIn industrieller Umgebung, also in Fertigungen oder gar an der freien Luft, muss die Elektronik gegen äußere Einflüsse wie Staub, Dreck, extreme Temperaturen und Feuchtigkeit (Schutzart IP 64) geschützt werden. Das wird vor allem durch angepasste, hochdichte Gehäuse und spezielle Filter in den Kühllüftern erreicht. Genauso müssen alle Steckverbindungen nach außen entsprechend robust und dicht ausgeführt sein.\n\nBei noch stärkeren Einflüssen muss die Elektronik ggf. hermetisch gekapselt werden, so dass man bei der Prozessorkühlung statt Lüftern mit Frischluftzufuhr von außen auf geschlossene Systeme mit Heatpipes und ähnlichen Elementen und beispielsweise als Kühlkörper ausgebildete Außenwände des Gehäuses (also \"passive Kühlung\") übergehen muss.\n\nWenn die Umgebung starke elektromagnetische Störungen (EMV) produziert, muss das System dagegen abgeschirmt werden. Auch dies führt zu speziell elektrisch abgedichteten Gehäusen und Steckverbindern, ggf. mit zusätzlichen \"Entstörgliedern\" in Zuleitungen.\n\nProduziert die Umgebung extreme mechanische Erschütterungen oder Vibrationen, müssen Gehäuse und Steckverbindern wieder entsprechend robust ausgeführt sein. Zusätzlich kann es erforderlich sein, so weit wie möglich auf bewegte mechanische Komponenten zu verzichten, insbesondere auf Lüfter und Festplatten. Wie oben kann man die Lüfter durch passive Kühlsysteme ersetzen; eine Festplatte neuerdings durch eine Solid State Disk.\n\nTastaturen sind ebenfalls mechanisch ziemlich empfindlich. Sie werden daher oft durch Touchscreens ersetzt, die dabei auch gleich die Maus mit ersetzen, wobei die Anwendungssoftware nur geringfügig angepasst werden muss. Siehe dazu auch bei Panel-PC.\n\nUmgekehrt kann es auch vorkommen, dass die Umgebung selbst besonders empfindlich gegenüber elektromagnetischen oder mechanischen Störungen ist, beispielsweise in speziellen Messapparaturen. Hier werden ähnlich wie oben besonders gut abgeschirmte Gehäuse und Steckverbindungen verwendet, hier nur mit dem Ziel, die Störungen nicht nach außen dringen zu lassen.\n\nWenn die Umgebung empfindlich gegenüber mechanischen Erschütterungen oder Vibrationen ist, muss wie oben zu Lüfter- und Festplatten-losen Varianten gegriffen werden.\n\nIn einigen Bereichen (z. B. chemische Industrie) sind zusätzlich noch gesonderte Vorschriften zu beachten (z. B. Explosionsschutz), die eine besondere Kapselung des IPC-Gehäuses erfordern.\n\nDamit die Umgebung auch nicht durch erhöhte Wärmeentwicklung der Elektronik gestört wird, muss ggf. auf besonders leistungssparende Ausführung geachtet werden. Dazu greift man auf Techniken zurück, wie sie in Notebooks und Laptops eingesetzt wird, damit lässt sich der Energieverbrauch etwa um die Hälfte verringern.\n\nBei manchen Anwendungen muss die ganze Elektronik auf extrem kleinem Raum untergebracht werden. Hierzu gibt es beispielsweise komplette PCs für Hutschienenmontage und Gehäuse, wie man sie sonst für eingebettete Systeme verwendet.\n\nVon einem industriell eingesetzten Rechner werden besonders hohe Standzeiten ohne Software- oder Hardware-Ausfall erwartet. Auf der Hardwareseite werden entsprechend robuste Komponenten eingesetzt, bei der Software wird oft zu speziell optimierten Linux-Distributionen gegriffen.\n\nViele IPC-Betreiber legen großen Wert auf Wartungsfreundlichkeit und bevorzugen Systeme mit einfacher Zugänglichkeit und möglichst geringer Anzahl von Verschleißteilen (z. B. Lüfter). So sind bei aktuellen Systemen Festplatten binnen Sekunden austauschbar. Zusätzlich wird der Lieferant danach selektiert, ob er Langzeit-Verfügbarkeit der Geräte und Ersatzteile gewährleisten kann. Für den IPC-Nutzer, der große Stückzahlen – etwa in einer Montagelinie – mit überwiegend identischer Software nutzt, ist es dabei wichtig, auch nach einigen Jahren noch denselben Mainboardtyp mit dem gleichen Chipsatz zu erhalten, da häufig ein anderer Chipsatz ein neues Speicherabbild (Image) aufgrund anderer Gerätetreiber erfordert. Durch den schnellen Wechsel zu immer leistungsstärkeren Systemen müssen die Hersteller der IPC daher einen größeren Lageraufwand auch für bereits abgekündigte Produkte betreiben als Hersteller konventioneller PC.\n\nHäufige Anforderungen sind:\n\n\nEs gibt zwei Ausführungen, wenn ein Industrie-PC als Automatisierungsgerät genutzt wird:\n\n\nDa die Hardware den handelsüblichen Personal Computern von der Struktur her verwandt ist, sind die verbreiteten Betriebssysteme wie Microsoft Windows und Linux einsetzbar. Der große Vorteil dabei ist, dass man auf ein breites Fundament an verfügbaren Softwarelösungen und Entwicklungswerkzeugen zurückgreifen kann. Insbesondere im Bereich Linux gibt es darüber hinaus die Möglichkeit, dank des Open-Source-Charakters eigene Modifikationen und Optimierungen einzuführen, die auch die Betriebssystemebene an das Einsatzgebiet anpassen können.\n\nDie Hardware eines Industrie-PCs unterscheidet sich meist von einem herkömmlichen Personal Computer. Oft reicht eine wesentlich niedrigere Performance, da die Steuerung von industriellen Maschinen keine Hochleistungsprozessoren benötigt. Im Bereich der Prozessvisualisierung werden jedoch durchaus leistungsfähige Prozessoren und Grafiklösungen, insbesondere Mehrschirmsysteme, eingesetzt.\n\nEinige Firmen produzieren IPCs mit einem modularen Aufbau. Das bedeutet, dass die konventionelle Hauptplatine (\"mother board\") durch eine Backplane (\"Busplatine\") und eine Slot-CPU ersetzt wird. Ein Vorteil darin ist, dass der IPC dadurch in mehreren verschiedenen Variationen zu erhalten ist. Insbesondere ist der Einsatz einer größeren Anzahl Erweiterungskarten zur Ansteuerung von Peripheriegeräten als auf üblichen Hauptplatinen möglich. Herkömmliche Hauptplatinen weisen z. B. oft nur vier bis sechs PCI-Steckplätze auf, bei IPCs sind über entsprechende Bridges zehn und mehr möglich, auch können bei Bedarf noch Einsteckkarten für den ISA-Bus unterstützt werden.\n\nAuf der Slot-CPU sind alle Komponenten, die auf einer Hauptplatine auch zu finden sind. Es befinden sich dort unter anderem mindestens ein Prozessorsockel, sowie ein Steckplatz für den Arbeitsspeicher, Anschlüsse für Festplatten und andere Laufwerke, meist ein VGA-Chip und mindestens ein Netzwerk-Controller.\n\nDie Busplatine ist eine Erweiterung der Slot-CPU. Auf dieser werden die Busse ausgeführt.\nDie maximale Anzahl beträgt in der Regel 20 Steckplätze und kann durch die Vielfalt der verschiedenen Busplatinen auf die kundenspezifischen Anforderungen angepasst werden. Die gängigsten Busse für Slot-CPUs sind PICMG 1.0 (PCI/ISA), PCISA oder PCI-Express (PICMG 1.3).\n\nDas Gehäuse eines Industrie-PCs ist in der Regel für den Einbau in einem 19-Zoll-Schrank konzipiert. Weiterhin gibt es noch Box PC – kompakte und robuste Industrie-PC für den universellen Einsatz (z. B. im Schaltschrank, Steuerpult, etc.) – und Panel PC – robuste Industrie-PC mit Displays.\n\nFür die mobile Datenerfassung im Feld, der Logistik, im Service oder in der Hospitality werden zunehmend mobile Industriecomputer eingesetzt.\n\nDurch die aufwändigere Konstruktion, speziellen Anforderungen (beispielsweise erweiterter Temperaturbereich), die hochwertigeren Materialien, so wie die Erfüllung vieler Zulassungen, Richtlinien und Normen, ist der Preis eines Industrie-PC höher als der eines gewöhnlichen Personal Computers im Office-Bereich.\n\nWenn die Kompatibilität zu Personal Computern keine entscheidende Rolle spielt, kann die Hardware noch gezielter auf den Einsatzzweck hin optimiert werden. Auch hier kann man von der Softwareseite her beispielsweise noch Linux-Varianten einsetzen, die aber ihrerseits dann sehr auf die Aufgabe angepasst sind.\n\nIn den allermeisten Fällen läuft das auf Lösungen hinaus, die wesentlich kompakter und kleiner sind als ein voll ausgebauter Industrie-PC, man spricht dann von Embedded-PCs oder von eingebetteten Systemen.\n\n"}
{"id": "1356033", "url": "https://de.wikipedia.org/wiki?curid=1356033", "title": "Chart-Parser", "text": "Chart-Parser\n\nEin Chart-Parser, auch Chartparser geschrieben, ist ein Parser für kontextfreie Grammatiken, der sich Teilanalysen (Teilkonstituenten) in einer Tabelle (Chart) merkt. Diese Zwischenspeicherung und Wiederverwendung von Teilanalysen verbessert die Effizienz erheblich und macht das Parsen von kontextfreien Sprachen zu einem in polynomieller Zeit lösbaren Problem.\n\nChartparsing ist ein Überbegriff für alle Parsverfahren, die eine solche Tabelle benutzen. Nach dem verwendeten Parsalgorithmus unterscheidet man verschiedene Subtypen:\n\nDer Chart ist eine n x n-Matrix, wobei n die Länge des zu analysierenden Satzes ist. Die Zwischenräume zwischen den Wörtern dieses Satzes sind von 0 bis n durchnummeriert.\nIn den einzelnen Chartzellen befinden sich sog. gepunktete Regeln (\"dotted rules\", vgl. LR-Parser).\n\nFormal lässt sich ein Chart als eine Menge von 3-Tupeln < i,j, A → α. β > beschreiben, wobei gilt:\n\nEin einzelner Charteintrag kann beispielsweise so aussehen:\n\n< 2, 5, VP → V NP . NP >\n\nDies bedeutet:\n\nChart-Parser verwenden während der Analyse im Normalfall drei verschiedene Operationen:\n\n\nIst < \"i\", \"j\", A → α . B β > ∈ Chart und\n\nist B → γ eine Regel der Grammatik,\n\ndann füge\n\n< \"j\", \"j\", B → . γ >\n\nin den Chart ein, falls dieses Tupel noch nicht vorhanden ist.\n\nAb der Satzposition \"j\" wird also eine Konstituente der Kategorie B erwartet. Zur Expansion von B existiert eine Regel mit rechter Seite γ. Man generiert also eine neue Erwartung, γ beginnend ab der Position \"j\" zu finden.\n\nIst < \"i\", \"j\", A → α . w β > ∈ Chart (w ist ein Terminalsymbol bzw. Präterminal) und\n\nist \"w\" das \"j\"-te Wort des zu analysierenden Satzes s = ww ... w,\n\ndann füge\n\n< \"i\", \"j+1\", A → α w . β >\n\nin den Chart ein, falls dieses Tupel noch nicht vorhanden ist.\n\nDie Analyse ist somit soweit vorangeschritten, dass nach der Position \"j\" ein Terminalsymbol bzw. eine Wortkategorie (wie Verb) erwartet wird. Ist das \"j\"-te Wort tatsächlich gleich w (bzw. von der Wortart w), dann kann dieses Wort in die Analyse integriert werden. Der Punkt wird dann über das erkannte Wort verschoben.\n\n\"Hinweis\": die hier beschriebene Kombinationsoperation ist diejenige des Top-Down-Chart-Parsers. Andere Methoden des Chart-Parsings gehen hier etwas anders vor.\n\nIst < \"i\", \"j\", A → α . B β > ∈ Chart (B ist ein Nichtterminalsymbol) und\n\nist auch < \"j\", \"k\", B → γ . > ∈ Chart\n\ndann füge\n\n< \"i\", \"k\", A → α B . β >\n\nin den Chart ein, falls dieses Tupel noch nicht vorhanden ist.\n\nWährend der Analyse wurde eine vollständige Konstituente B zwischen den Positionen \"j\" und \"k\" gefunden. Im Chart existiert ein weiteres Tupel, das die Erwartung einer Konstituente B ab Position \"j\" reflektiert. Also können beide zu einem neuen Tupel kombiniert werden, welches die Positionen \"i\" bis \"k\" überdeckt. Der Punkt wurde dabei über die erkannte Konstituente B weitergesetzt.\n\nEingabe: Ein Satz s = ww ... w.\n\n\nAusgabe: \"yes\", falls <0, n, S' → S . > ∈ Chart, andernfalls \"no\".\n\n\"Hinweis\": Eigentlich ist das lediglich ein Erkennungsverfahren. Die tatsächlichen Satzstrukturen können aber mit etwas zusätzlicher Verwaltungsinformation aus dem Chart rekonstruiert werden (sog.\nshared packed parse forest).\n\nDie Schritte unter 2. sind in ihrer Reihenfolge nicht geordnet. Ihre Reihenfolge kann mit Hilfe verschiedener Suchverfahren (Tiefensuche, Breitensuche, Bestensuche) systematisiert werden.\n\nGegeben sei eine kontextfreie Grammatik mit folgenden Produktionsregeln:\n\nLexikonregeln\n\nDer zu parsende Satz sei \"Donald beobachtet Daisy mit dem Fernglas\"\n\n\"Erläuterung\":\n\n\nDie Tatsache, dass Eintrag 33 auch durch Kombination von Eintrag 1 mit Eintrag 32 hätte gebildet werden können, zeigt, dass der Satz auf zwei Arten geparst werden kann (also zweideutig ist).\n\nTilgungsregeln sind u. a. Produktionsregeln der Form A → ε. Solche Regeln werden meist durch spezielle Vorarbeitungsstrategien in der Chartparser integriert.\n\nDas Erzeugen von überflüssigen Charteinträgen kann durch Integration von \"k\" Lookahead-Symbolen in die Charttupel verhindert werden. Diese Technik wird auch bei LR(k)-Parsern verwendet.\n\nZur Parsen von natürlichen Sprachen werden in der Regel sog. separierte Grammatiken verwendet, bei denen Lexikon und Phrasenstrukturregeln voneinander getrennt sind. Die rechten Regelseiten der kontextfreien Grammatik enthalten somit entweder nur Terminalsymbole (Alphabetsymbole) oder Nichtterminalsymbole. Dies macht den Predict- und Scan-Vorgang effizienter, da sie nur bis zur Ebene der Präterminale (Wortarten) voranschreiten.\n\nDa die Eingaben des Parsers nicht immer im Sinne der Grammatik wohlgeformt sind (vgl. die Anwendung der Grammatikprüfung), ist es nützlich, den Parser robust, d. h. unanfällig für Grammatikfehler zu machen. Dies betrifft beispielsweise unbekannte Wörter, für die dann im Scan-Schritt alle wahrscheinlichen Wortarten eingetragen werden, oder fehlende oder überzählige Wörter, die mit speziellen Fehlerproduktionen erkannt werden.\n\nO(n) für beliebige kontextfreie Grammatiken, O(n) für nicht-ambige kontextfreie Grammatiken.\n\nChart-Parser werden meist im Zusammenhang mit der syntaktischen Analyse natürlicher Sprachen eingesetzt, da sie – neben dem Tomita-Parser – die beste Zeitkomplexität für beliebige (d. h. auch ambige) kontextfreie Grammatiken aufweisen. Beispielsweise verwendet die Grammatikprüfung von Microsoft Word einen Chartparser.\nFür Programmiersprachen, deren Syntax spezielle Eigenschaften aufweist, werden meist effizientere Parser wie LR(k)- bzw. LL(k)-Parser eingesetzt.\n\n\n"}
{"id": "1356640", "url": "https://de.wikipedia.org/wiki?curid=1356640", "title": "Bottom-Up-Parser", "text": "Bottom-Up-Parser\n\nDer Begriff Bottom-Up-Parser bzw. Aufwärtsparser bezeichnet ein Analyse-Werkzeug für natürliche und formale Sprachen.\n\nIm Regelfall wird ein Parser als Teil eines Übersetzungsprogramms von einer Sprache in eine andere eingesetzt. Bei Programmiersprachen heißt ein solches Übersetzungsprogramm auch Compiler.\nEin Parser prüft auch die Konformität bzw. das Einhalten des Regelwerks einer Sprache: Er gibt Warnungen und Fehlermeldungen aus, wenn der Eingangstext nicht regelkonform ist.\n\nEin Bottom-Up-Parser arbeitet ausgehend von der kleinsten vorgefundenen Einheit (\"Bottom\") in Richtung des größeren Zusammenhangs (\"Up\").\n\nDer Bottom-Up-Parser implementiert die Strategie des Bottom-Up-Parsings (datengeleitetes Parsing).\nBei dieser wird von den Token (Wörtern) des Eingabesatzes ausgehend versucht, nach und nach größere syntaktische Strukturen aufzubauen, bis man schließlich beim Startsymbol der Grammatik angelangt ist.\n\nWichtige Unterklassen sind\n\nGegeben sei eine kontextfreie Grammatik mit folgenden Produktionsregeln:\n\nDas Startsymbol sei S.\n\nDer Satz, der durch den Bottom-Up-Parser analysiert werden soll, sei \"Daisy liebt Donald\". Der Stapel des Parsers ist anfänglich leer. Die Schritte eines Shift-Reduce-Parsers sehen so aus:\n\nEs gibt keine weiteren Wörter mehr im Eingabesatz, auf dem Stapel liegt das Startsymbol, der Satz wurde daher durch den Parser unter Ausgabe der Regelfolge \"4 5 3 2 1\" akzeptiert.\n"}
{"id": "1358505", "url": "https://de.wikipedia.org/wiki?curid=1358505", "title": "DD-WRT", "text": "DD-WRT\n\nBei DD-WRT handelt es sich um eine quelloffene (GPL) Linux-Distribution, die auch proprietäre Anteile enthält. Sie wurde für Consumer-WLAN-Router und Access-Points der Unternehmen Asus, ALLNET, Belkin, Buffalo, Linksys, Netgear, Motorola, Siemens u. v. m. mit Atheros-, Broadcom- oder Ralink-Chipsatz entwickelt.\n\nEnde 2006 wurde die Unterstützung für professionelle WLAN-Geräte ständig erweitert. Vornehmlich kommen die Geräte bei Wireless Internet Service Provider (WISP), Internetdienstanbieter (ISP) oder Campus WLANs zum Einsatz. Als Plattformen stehen x86, Intel IXP, Atheros MIPS, Infineon ADM MIPS und PowerPC zu Verfügung. Für einige Geräte muss man eine kostenpflichtige Lizenz im \"DD-WRT\"-Shop erwerben.\n\nWährend OpenWrt ganz klar die Basar-Entwicklungsmethode verfolgt, verfolgt \"DD-WRT\" die Kathedralen-Methode. Das mächtige Webinterface und die Tatsache, dass \"DD-WRT\" ein semi-kommerzielles Projekt ist, machen dies deutlich. Siehe dazu eine Übersicht der \"DD-WRT\"-Features unter Weblinks.\n\nLinksys verwendete für ihre Router modifizierten Quelltext des Linux-Kernels und anderer Software, die als frei verfügbarer Code unter der GNU General Public License (GPL) stehen. Gemäß dieser Lizenz muss der modifizierte Quellcode ebenfalls öffentlich zur Verfügung gestellt werden. Wie einige Entwickler im Juni 2003 feststellten, hatte Linksys jedoch den modifizierten Quellcode nicht der Allgemeinheit zur Verfügung gestellt. Sie appellierten an den Hersteller, die Quelltexte frei zugänglich zu machen. Durch die Veröffentlichung der Quelltexte im Oktober 2003 war es möglich, das Betriebssystem des Routers wiederum zu modifizieren und weiterzuentwickeln. Inzwischen gibt es mehrere unterschiedliche Fassungen (Abspaltungen) dieser Firmware von unterschiedlichen Entwicklern und mit unterschiedlichen Schwerpunkten, die zum Teil auch proprietär genutzt werden.\n\nNeben \"DD-WRT\" basieren auch einige andere Projekte ursprünglich auf der inzwischen kommerziellen \"Alchemy\"-Firmware von Sveasoft. Alchemy basiert wiederum auf der Originalfirmware von Linksys. Sebastian Gottschall aka \"BrainSlayer\" und einige andere entwickelten sie weiter, fügten Funktionen hinzu und stabilisierten den Code. Bis zur Version v.22 aus dem Juli 2005 basierte \"DD-WRT\" noch auf der \"Alchemy\"-Firmware. Ab der im Dezember 2005 veröffentlichten v.23 wurde der Code stark weiterentwickelt. Dabei wurde unter anderem auf die Unterstützung des OpenWrt-Projekts zurückgegriffen (\"JFFS2 and Kernel support by OpenWRT Project\").\n\nIm Mai 2008 folgte die stabile Version 24. Im Juli 2008 erschien v24 SP1 und im Juli 2013 v24 SP2 pre. Seitdem erscheinen in regelmäßigen Abständen neue Beta-Versionen. Am 1. Februar 2017 hatte die Anzahl der Revisionen 31221 erreicht.\n\nEinige Unternehmen bieten unter anderem kommerziellen Support für \"DD-WRT\" in Kombination mit ihren Produkten an.\nBuffalo Technology bietet seit 2010 ihre WLAN-Router der \"High Power\"-Serie mit vorinstalliertem \"DD-WRT\" inklusive Support an.\nAsus gestattet den Kunden seit 2011 \"DD-WRT\" auf ausgewählten Routern zu installieren und stellt entsprechenden kommerziellen Support zur Verfügung.\n\n"}
{"id": "1358608", "url": "https://de.wikipedia.org/wiki?curid=1358608", "title": "Sosumi", "text": "Sosumi\n\nSosumi ist einer der System-Töne in dem Betriebssystem Apple Macintosh System 7. Es ist ein extrem kurzes Sample eines Xylophons. Der System-Ton wird noch in den neueren Versionen des Mac OS verwendet, einschließlich Mac OS X.\n\nDer ungewöhnliche Name des Tons rührt von einem Rechtsstreit mit Apple Corps, der Holding der Beatles her, die mit Apple Computer eine weit zurückreichende Prozess-Geschichte aufgrund des Namens verbindet. Mit der Veröffentlichung von System 7 wurde ein Rechtsstreit über die neu in die Produkte von Apple Computer integrierten MIDI-Fähigkeiten beigelegt.\n\nDie Anwälte von Apple Corps prüften jeden Ton-Aspekt des Computers. Während der Entwicklung von System 7 wurde der Name eines der neuen System-Töne als „\"zu musikalisch\"“ beanstandet.\n\nDer Schöpfer der neuen Töne für System 7 und des Macintosh Start-Tons, Jim Reekes, wurde immer frustrierter wegen der rechtlichen Überprüfungen und wollte sich rächen. Zuerst meinte Reekes zum Spaß, der Ton sollte \"Let It Beep\" heißen, eine Anspielung auf das Beatles-Lied \"Let It Be\", nannte ihn dann aber \"Sosumi\", was man auf Englisch \"so sue me\" ausspricht (auf Deutsch so viel wie \"Verklag mich doch\").\n\nDie beiden Apple-Firmen kamen im Jahr 1991 zu einer Einigung. Dennoch strebte Apple Records ein Jahrzehnt später wieder einen Prozess an, nachdem Apple Computer anfing iPods und Musik über den iTunes Music Store zu verkaufen. Die Kernbedingungen des Abkommens aus dem Jahr 1991 wurden bei einer Anhörung im Februar 2005 veröffentlicht.\n\nDer Name des Tons ist heute auch noch als kleiner Scherz auf der Website von Apple versteckt. Dort trägt eine CSS-Klasse, verwendet für rechtliche Hinweise wie z. B. die Copyright-Information, diesen Namen; dies ist aber nur im Quellcode der Seite zu sehen:\n\nDer Name der CSS-Klasse ist ein offensichtlicher Verweis auf den Ton und seine Geschichte.\n\nApple - Entstehung des Namens\n"}
{"id": "1362791", "url": "https://de.wikipedia.org/wiki?curid=1362791", "title": "Picopolo", "text": "Picopolo\n\nPicopolo war eine Software zur georeferenzierten Verwaltung und Betrachtung von digitalen Bilddaten.\n\nNeben typischen Eigenschaften von Bildverwaltungssoftware wie thematischer Organisation, Präsentation in HTML und/oder Flash u. a., können den Bildern geographische Koordinaten zugeordnet werden. Über ein im Programm integriertes Kartenmodul kann das Bild dann als Vorschaubild auf einer Karte angezeigt werden. Neben im Programm enthaltenen Karten lassen sich eigene Karten einbinden, diese können selbst gezeichnet und eingescannt oder von einem fremden Hersteller kommend eingebunden werden. Fotos können aus Picopolo auch direkt zu einem Ausbelichter geschickt und von dort als Abzüge auf Fotopapier bestellt werden.\nIst ein Bild georeferenziert, d. h. es hat Koordinaten, dann werden diese beim Export in den Exif-Header der Datei geschrieben.\n\nMit der Erweiterung Picopolo Plus lassen sich die Kartenprodukte der Landesvermessungsämter der BRD – Top50 – als Kartenhintergrund in Picopolo verwenden. Am 1. Januar 2009 wurde Picopolo vom Hersteller eingestellt.\n"}
{"id": "1363861", "url": "https://de.wikipedia.org/wiki?curid=1363861", "title": "Morphologische Analyse (Computerlinguistik)", "text": "Morphologische Analyse (Computerlinguistik)\n\nUnter morphologischer Analyse versteht man in der Computerlinguistik ein Verfahren, welches die morphologischen, syntaktischen und evtl. semantischen Eigenschaften von Wörtern ermittelt. Im Einzelnen können morphologische Analyseverfahren die folgenden Teilaufgaben lösen:\n\n\n\nDie meisten Verfahren zur morphologischen Analyse basieren auf endlichen Automaten, genauer endlichen Transduktoren. Das verwendete theoretische Modell ist meist das sog. \"Two-Level-Modell\" (Koskeniemi), bei dem quasi-kontextsensitive Regeln zwischen der lexikalischen Form eines Morphems und seiner Oberflächenform (Morph) vermitteln. Eine solche Regel für das Deutsche könnte z. B. folgendermaßen aussehen:\n\n\nDiese Regel erlaubt die Ersetzung des leeren Wortes durch \"e\" (effektiv also ein Einfügen von \"e\") nach einem Verbstamm auf \"ppn\", \"chn\", \"tm\", \"d\" oder \"tt\" (\"wappnen\", \"rechnen\", \"atmen\", \"gründen\", \"retten\") vor den verbalen Flexiven \"n\", \"t\" oder \"st\". \"Beispiel\": \"rechn\" + \"n\" → \"rechnen\".\n\n\n"}
{"id": "1366318", "url": "https://de.wikipedia.org/wiki?curid=1366318", "title": "SME Server", "text": "SME Server\n\nDer SME Server (früher: \"e-Smith\") ist eine freie, auf CentOS Linux basierende Small Business Server-Distribution für Netzwerke von kleinerer bis mittlerer Größe, die eine Alternative zu Zentyal ist. SME Server kann – teilweise durch Zusätze anderer Anbieter – sehr viele Serveraufgaben aus den Bereichen Routing/Gateway, Sicherheit, Netzwerkinfrastruktur, Kollaborationssoftware und Kommunikation wahrnehmen. Die meisten Dienste sind über eine komfortable Weboberfläche einzurichten und zu konfigurieren. Daher sind für die üblichen Einstellungen und Änderungen keine oder nur grundlegende Linuxkenntnisse erforderlich.\n\nSeit Februar 2014 ist der aktuell stabile \"SME Server 8.1\" in 32-bit (nur mit PAE) und 64-bit Versionen verfügbar, die auf CentOS 5.10 basieren.\n\nDie Netzwerkinfrastruktur unterstützt SME Server mit folgenden Produkten:\n\n\nFür die Datensicherheit sorgen:\n\nSME Server kann als vollwertiger Router dienen:\n\n"}
{"id": "1366442", "url": "https://de.wikipedia.org/wiki?curid=1366442", "title": "Java-Spektrum", "text": "Java-Spektrum\n\nJava-Spektrum (auch \"JavaSPEKTRUM\" geschrieben) ist eine zweimonatlich erscheinende Computerzeitschrift des Verlags „Sigs Datacom“ (mit Sitz in Troisdorf), die erstmals im Mai 1996 als erste deutschsprachige Zeitschrift für die Programmiersprache \"Java\" erschien. Die Themenschwerpunkte sind Softwareentwicklung und Integration mit Java und XML. Sie erscheint sowohl als gedruckte wie auch als Online-Ausgabe.\n\n"}
{"id": "1367079", "url": "https://de.wikipedia.org/wiki?curid=1367079", "title": "White dune", "text": "White dune\n\nwhite_dune ist freie und Open-Source-Software zur Erstellung/Bearbeitung von 3D-Modellen/Animationen in VRML97 bzw. X3D. Sie ist für verschiedene Betriebssysteme wie Windows, MacOS/X11 oder Linux verfügbar.\n\nIm Gegensatz zu allgemein einsetzbaren Modellierungswerkzeugen wie Blender oder Wings 3D ist white_dune ein Spezialwerkzeug für das Echtzeit-3D-Dateiformat VRML97 bzw. X3D. Dabei können alle VRML97/X3D-Befehle erzeugt und verändert werden. Obwohl versucht wird, das Rendering bzw. die Beleuchtung innerhalb des Programms genauso wie im VRML/X3D-Standard vorgeschrieben zu gestalten, ist das korrekte Rendering noch nicht für alle X3D Befehle (es gibt es über 300 X3D Befehle) voll implementiert.\n\nZur Zeit (Stand Juli 2018) hat white_dune als Mesh-Modellierungswerkzeug nur die Werkzeuge \"subdivide\" (für catmull-clark Subdivision), \"Flaeche herausziehen\", \"konvexe Hülle\" und CSG. Allerdings kann über den VRML-Befehl „Inline“ der VRML-Export von anderen 3D-Modellierungswerkzeugen in white_dune (bzw. eine VRML-Welt) eingebunden werden, ohne dass es dabei zu einem Bruch der Werkzeugkette kommt. Außerdem lassen sich 3D-Objekte auch über NURBS- oder Superformel-Modellierung in white_dune erstellen und später (z. B. per Exportbefehl) in Mesh-Objekte wandeln.\n\nAußer dem Standard-Userinterface existiert auch eine Kommandozeilenoption für ein anderes Userinterface. Die Option \"-4kids\" startet eine Oberfläche, die besonders einfach aufgebaut ist und für Anfänger gedacht ist.\n"}
{"id": "1368039", "url": "https://de.wikipedia.org/wiki?curid=1368039", "title": "Monitorkalibrierung", "text": "Monitorkalibrierung\n\nDie Monitorkalibrierung ist das exakte Einstellen der Farb- und Helligkeitsdarstellung eines Bildschirms insbesondere mit Hilfe des Farbmanagements.\nFür farbkritische Anwendungen benötigt man eine verlässliche Farbdarstellung. Fotografen und Bildbearbeiter müssen der Bilddarstellung an ihrem Monitor vertrauen können, um Bildeigenschaften richtig beurteilen und ggfs. Korrekturen vornehmen zu können. Dasselbe gilt für die farblich vorhersehbare Erstellung von Druckvorlagen, die Gestaltung von Grafik aller Art und die Beurteilung der Farbwiedergabe von Videofilmen.\n\nDie meisten Monitore werden jedoch mit recht willkürlichen Einstellungen von Farben, Kontrasten und Helligkeitsverteilung ausgeliefert. Selbst wenn ein Monitor vom Hersteller gut voreingestellt wurde, wird aufgrund der Alterung nach gewisser Zeit eine Nachbesserung der Einstellungen nötig.\n\nMan unterscheidet zwei Arten der Monitorkalibrierung:\n\nDie Variante 1 wird nur dort verwendet, wo die Nutzung von Farbmanagement nicht möglich ist. Das ist insbesondere dort der Fall, wo kein Computer als Zuspielgerät verwendet wird, z. B. auf Fernsehgeräten und digitalen Projektoren mit TV-Receivern und Videoplayern als Bildquelle. Meist werden sie heute auf den Rec.709-Farbraum für HDTV-Wiedergabe eingestellt.\n\nSeltener genutzt wird Farbraumemulation für Computeranwendungen. Interessant wird diese Variante, wenn genaue Farbwiedergabe gefordert ist, aber die verwendete Software kein Farbmanagement unterstützt. Insbesondere gilt das heute für Videoschnittprogramme. Aber auch für Office-Software und andere nicht farbmanagementfähige Anwendungen, die Bilder einbinden können, kann die Emulation eines Standardfarbraumes (meist sRGB) nützlich sein.\n\nGrößter Nachteil der Farbraumemulation ist die Festlegung auf einen einzigen Arbeitsfarbraum; die Fähigkeiten des Monitors (besonders, wenn er einen großen nativen Farbraum besitzt) werden dadurch von vornherein beschnitten. Zu beachten ist auch, dass preiswertere Computermonitore gar nicht alle nötigen Einstellmöglichkeiten für eine vollständige Farbraumemulation bieten.\n\nVariante 2 ist heutiger Standard für Computeranwendungen und kann grundsätzlich mit jedem Monitor angewendet werden; was nicht direkt am Monitor eingestellt werden kann, wird einfach über die Grafikkarte korrigiert.\n\nDie Methode besteht aus zwei Schritten: der eigentlichen Kalibrierung und der Profilierung.\nBei der Kalibrierung wird der Farbraum des Monitors linearisiert (d. h. es wird für gleichmäßige Graustufen gesorgt), der Weißpunkt (Helligkeit und Farbtemperatur) des Monitors auf bestimmte Werte gebracht und eine Helligkeitsverteilung (z. B. ein bestimmter Gammawert) festgelegt. Die Größe des Farbraums bleibt jedoch im Rahmen des Möglichen unangetastet, um die Fähigkeiten des Monitors nicht unnötig zu beschneiden.\n\nNach erfolgter Kalibrierung wird ein Profil des Monitors ausgemessen und im Betriebssystem als Standard-Monitorprofil hinterlegt. Anwendungsprogramme, die Farbmanagement beherrschen, können das Profil nutzen, um die Farben der darzustellenden Bilder an den Monitorfarbraum anzupassen. Dies entspricht der grundlegenden Arbeitsweise des heute üblichen ICC-Farbmanagements, für das jederzeit zwei Profile (Quellprofil und Zielprofil) nötig sind.\n\nDie Kalibrierung ist bei Variante 2 also nur ein vorbereitender Schritt (der in Ausnahmefällen sogar ganz entfallen kann), während die eigentliche Farbraumanpassung in die jeweiligen Anwendungsprogramme verlagert ist – wofür das gemessene Profil benötigt wird. Vorteil des Verfahrens ist, dass der native Farbraum des Monitors voll ausgeschöpft werden kann – wichtig insbesondere für die Nutzung großer Arbeitsfarbräume und sogenannter Wide-Gamut-Monitore. Aber leider bleibt die farbrichtige Darstellung auf Anwendungsprogramme beschränkt, die Farbmanagement beherrschen; das gilt zwar heute für nahezu alle Bildbearbeitungs-, Grafik- und Druckvorstufen-Programme, aber noch nicht für Videowiedergabe, Videoschnitt, Office und Spiele. Im Bereich der Browser gibt es teilweise Farbmanagement, aber meist noch unvollständig.\n\nRöhrenmonitore verfügen über Einstellungen für Kontrast und Helligkeit. LCD-Monitore bieten zusätzlich Steuerelemente für Hintergrundbeleuchtung, Gamma und Farbtemperatur. Das Grafikkarten-Konfigurationsprogramm verfügt über Steuerelemente für Kontrast und Helligkeit und ermöglicht das individuelle Anwenden dieser Steuerelemente auf die drei Grundfarben. Das Betriebssystem hat eine Einstellmöglichkeit für Gamma. Mit diesen vielen Steuerelementen ist ein Einstellungsverfahren für eine gute Wiedergabe hilfreich.\n\nDie Kontrasteinstellung ist eine Verstärkungseinstellung. Das Eingangssignal des Monitors wird mehr oder weniger vergrößert. Bei zu viel Verstärkung oder Kontrast wird eine hellgraue Farbe am Eingang des Monitor als Weiß dargestellt. Die Helligkeitseinstellung stellt den Schwarzwert ein. Eine falsche Einstellung des Schwarzwerts zeigt entweder eine schwarze Farbe aufgrund einer zu hohen Helligkeit als dunkelgraue Farbe an, oder aufgrund einer zu geringen Helligkeit wird eine dunkelgraue Farbe als schwarz dargestellt.\n\nAuf einem LCD-Monitor werden die Einstellungen für Kontrast, Helligkeit und Gamma einmalig eingestellt. Die Hintergrundbeleuchtung ist die einzige Einstellung im täglichen Gebrauch. Die Hintergrundbeleuchtung wird bei einigen Monitoren als Helligkeitseinstellung bezeichnet und es gibt keine Schwarzwert-Einstellung. \n\nDer erste Schritt besteht darin, die werkseitigen Standardeinstellungen für Monitor, Grafikkarte und Betriebssystem wiederherzustellen. Die Hintergrundbeleuchtung auf 50 % Intensität und die Farbtemperatur auf 6500K einstellen. Im obersten Bereich werden acht hellgraue Zahlen auf weißen Grund dargestellt, im zweiten Bereich werden acht dunkelgraue Zahlen auf schwarzen Grund dargestellt. Im zweiten Schritt muss meistens der Kontrast in der Grafikkarte und im Monitor verkleinert werden und die Helligkeit vergrößert werden um im weißen und schwarzen Bereich möglichst viele Zahlen zu sehen. Es gibt weitere Testbilder für Kontrast und Helligkeit. Der dritte Schritt besteht darin, das Gamma einzustellen. Dadurch wird auch die Farbtemperatur in einen akzeptablen Bereich gebracht.\n\nDie Gamma-Einstellung beeinflusst die mittleren Helligkeitswerte. Das Gamma von sRGB beträgt 2,2. Der Apple Macintosh benutzte eine Gamma von 1,8. Ein Apple-Macintosh-Bild mit Gamma von 1,8 wird an einem sRGB-System mit Gamma 2,2 zu hell dargestellt.\n\nFarbtemperatur und Weißpunkt haben für den Monitor gleiche Bedeutung. Eine hohe Farbtemperatur macht das Monitorbild bläulich, eine niedrige Farbtemperatur macht es gelblich. Die Standard-sRGB-Farbtemperatur beträgt 6500 K. Wenn der Monitor nicht über eine Farbtemperatur-Einstellung verfügt, wird die Farbtemperatur mit der Helligkeitseinstellung der Primärfarben der Grafikkarte eingestellt.\n\nNach der Grundeinstellung kann die Monitoranzeige verbessert werden, indem zuerst der Kontrast erhöht, dann der Gammawert korrigiert wird, dann die Helligkeit geändert und erneut der Gammawert korrigiert wird. Bei Monitoren mit TN-LCD-Panel ändert sich der Gammawert schnell mit dem vertikalen Blickwinkel. Das Ändern des vertikalen Winkels eines solchen Monitors ist oft die einfachste Möglichkeit, den Gammawert einzustellen. Die Unterschiede in den Primärfarben-Gamma werden mit der Farbbalance des Betriebssystems oder der Helligkeitseinstellung der Primärfarben der Grafikkarte ausgeglichen. Weitere Informationen zum Testbild gibt es unter Gammakorrektur.\n\nIn den meisten Fällen kann dasselbe Ergebnis durch Einstellung am Monitor (Kontrast, Helligkeit, Gamma), an der Grafikkarte (Kontrast, Helligkeit) oder am Betriebssystems (Gamma) erzielt werden. In einigen Fällen ist der nutzbare Bereich der Einstellung unterschiedlich. In diesen Fällen ergibt die Einstellung am Monitor häufig einen größeren nutzbaren Bereich.\n\nEin Tristimulus-Kolorimeter automatisiert den Einrichtungsvorgang. Qualitativ hochwertige Monitore verfügen über ein Hardware-Farbmanagement und ermöglichen eine Hardware-Kalibrierung, d. h. die Ergebnisse des Kolorimeters werden in dem Monitor gespeichert. Ein anderer Ansatz ist, dass das Kolorimeter das ICC-Profil im Betriebssystem einstellt. Der erste Ansatz liefert die besten Ergebnisse. Normalerweise müssen Monitor und Kolorimeter jedoch von demselben Hersteller stammen. In den Anfängen des Farbmanagements wurden Vorlagen auf Fotopapier verwendet, die der Anwender als Referenz neben den Monitor halten sollte, um dann nach Augenmaß die Farben des Monitors einzustellen. Einzelne Fotolabore vertreiben solche Testbilder bis heute. In der Praxis haben sie sich jedoch nicht bewährt; man kann damit allenfalls die Helligkeitsverteilung beurteilen, aber niemals den Farbumfang eines Monitors korrekt erkennen (und selbst wenn man es könnte, hätten die meisten Monitore nicht genügend Einstellmöglichkeiten).\n\nEs gibt keine absolut korrekte Monitordarstellung, da das menschliche Auge sich ungewollt an verschiedene Helligkeiten und Farbreferenzen anpasst. Die Darstellung kann allenfalls in sich selbst korrekt sein, d. h. in Relation zu einem Mittelgrau des Monitors. Aus diesem Grund kann man vor einer Kalibrierung/Profilierung als Anwender selbst festlegen, auf welchen Weißpunkt und welche Helligkeitsverteilung man kalibriert.\n\nWer sich stets ganz auf das Monitorbild konzentriert und einen grauen Bildschirmhintergrund als Referenz für das Auge verwendet, kann die Werte fast beliebig wählen; wird derselbe Monitor bei verschiedenen Lichtverhältnissen benutzt, hat man sowieso keine andere Wahl.\n\nAngenehmer ist es jedoch, den Monitor an das Umgebungslicht anzupassen. In Grafikstudios und Druckereien ist es sogar üblich, den Raum mit speziellem Normlicht auszustatten und dann den Monitor auf dessen Werte zu kalibrieren. Dies hat den Vorteil, dass das Auge sich nicht ständig zwischen Umgebungslicht und Monitor umstellen muss, und dass auch Laien (die nicht gelernt haben, die Farben bewusst innerhalb des Monitors zu beurteilen) eine korrekte Darstellung bekommen.\n\nDie gängigsten Lichtnormen sind D50 (5000 K Farbtemperatur) und D65 (6500 K Farbtemperatur), wobei die Helligkeit meist zwischen 80 cd/m² (reine Bildbearbeitungs-Umgebung, eher dunkel) und 120 cd/m² (normale Raumhelligkeit, auch zum Beurteilen von Papiervorlagen tauglich) eingestellt wird.\n\nEntspricht das Raumlicht keiner Norm, wird man den Monitorweißpunkt zumindest annähern. Wichtig ist, dass die Helligkeit einigermaßen passt. In sehr hellen Umgebungen können auch deutlich höhere Werte als 120 cd/m² nötig werden. Wird es zu hell (z. B. Tageslicht aus großen Fensterflächen) können selbst lichtstarke Monitore mit ihren 300 bis 400 cd/m² nicht mithalten; dort sind dann keine farbkritischen Arbeiten möglich.\n\nEin weiteres Kalibrierungsziel neben dem Weißpunkt ist die Helligkeitsverteilung – oft vereinfacht als Gamma bezeichnet. Meist empfiehlt sich ein Gammawert von 2,2 oder die sogenannte sRGB-Kurve, weil dies auf den Großteil der unprofilierten Bilder passt.\nSobald das Anwendungsprogramm Farbmanagement beherrscht, wird die Helligkeitsverteilung ohnehin automatisch angepasst; auch wenn der Monitor auf Gamma 2,2 kalibriert war, werden Bilder mit Gamma 1,8 (z. B. ProPhotoRGB) oder L* (z. B. ECI-RGB V2) trotzdem korrekt dargestellt.\n\nEine Kalibrierung und Profilierung im Zusammenspiel mit Farbmanagement sorgt dafür, dass Farben im Sinne der Farbraumdefinition korrekt dargestellt werden. Dies entspricht jedoch nicht automatisch dem Aussehen eines Papierabzuges; Fotopapier kann technisch bedingt nämlich nicht alle Farben der gängigen Arbeitsfarbräume darstellen, und es besitzt einen geringeren Kontrastumfang als die meisten Monitore.\n\nUm das Aussehen eines Druckes oder eines ausbelichteten Fotos am Bildschirm zu simulieren, bedient man sich eines sogenannten Softproofs (auch Drucksimulation genannt); viele Bildbearbeitungsprogramme bieten so eine Funktion. Neben dem Profil des Arbeitsfarbraums und dem Monitorprofil ist hierzu auch noch ein Profil des Druckers oder Belichters notwendig.\n\nDer Softproof zeigt eine gute Vorschau des zu druckenden Bildes. Sollen aus irgendeinem Grund Papierbilder oder Gegenstände zum Vergleich direkt neben den Monitor gehalten werden, muss zusätzlich das Umgebungslicht an den Weißpunkt des Monitors angepasst sein (oder umgekehrt). Idealerweise verwendet man hierfür Normlicht. Falls der Aufwand für die Einrichtung des Normlichtes im ganzen Raum zu hoch wäre, kann man ersatzweise auf einen Normlicht-Kasten zurückgreifen; darin wird die Papiervorlage mit einer Helligkeit und Farbtemperatur beleuchtet, die mit dem kalibrierten Weißpunkt des Monitors übereinstimmt.\n\n\n"}
{"id": "1370619", "url": "https://de.wikipedia.org/wiki?curid=1370619", "title": "Operation Center", "text": "Operation Center\n\nOperation Center ist ein Shareware-Dateimanager für Microsoft Windows.\n\nOperation Center ist ein Programmpaket, bestehend aus einem Dateimanager, einer Fotoverwaltung sowie Wartungsfunktionen. Daneben verfügt es über einen FTP-Klienten, einen ZIP-Packer, einen Editor – welcher auch als Textverarbeitungsprogramm benutzt werden kann – sowie zahlreiche Dateimanagement-Funktionen wie beispielsweise Kopieren, Verschieben, Löschen, Festplatte bereinigen usw.\n\nOperation Center wurde von Jochen Moschko (* 1979) geschrieben. Die erste Version erschien im August 1997, die zweite im Mai 1998. Seit Version 4.5 (Mai 1999) verfügt das Programm über zwei Dateifenster. Seitdem sind die zwei Dateifenster plus Schnellansichtsleiste für das Programm charakteristisch. Es existierten auch Versionen für Windows CE (3.0 – 4.2). Diese Reihe wird allerdings nicht mehr weiterentwickelt. Version 7.7, 8.5 und 9.X waren auch im Retail-Handel erhältlich.\n\nSeit Version 11 (erschienen im Mai 2013) wurde die Software auch für Touchscreens und Tablets optimiert. Es wird auch eine eingeschränkte SE-Version angeboten, die z. B. der \"ComputerBild\" 02/14 beilag. Jedem Major-Release geht in der Regel ein öffentlicher Beta-Test voraus, so auch der Version 15, die der Hersteller als „Operation Center x64 Professional“ vermarktet. Für Android ist eine stark eingeschränkte Version erhältlich, die allerdings nur wenige Funktionen aus der Variante für Windows erhält.\n\nDer Hersteller bietet Programmierern und Softwareentwicklern die Möglichkeit, sogenannte „Plug-ins“ für Operation Center zu schreiben, die auf dieser Plattform aufbauen können. Welche Programmiersprache dafür verwendet wird, ist unerheblich. Die Spezifikationen finden sich in der Hilfefunktion unter „Informationen für Entwickler“, einschließlich eines Codebeispiels für Visual Basic 2013.\n\n"}
{"id": "1370669", "url": "https://de.wikipedia.org/wiki?curid=1370669", "title": "Lisog", "text": "Lisog\n\nDie Lisog (früher LiSoG – \"Linux Solutions Group e. V.\") war ein Verein von Open-Source-Anbietern und -Anwendern in Deutschland, Österreich, der Schweiz, Serbien, Kanada und den USA.\n\nDie Linux Solution Group (LiSoG) wurde am 7. März 2005 von 29 Unternehmen, darunter IBM, MySQL, Novell, Red Hat und Siemens Business Services, und Institutionen gegründet und bestand im Mai aus 121 Mitgliedern. Die Geschäftsstelle der Lisog war angesiedelt bei der MFG Baden-Württemberg in Stuttgart. Daneben unterhielt der Verein Kontaktbüros in Zürich (seit 2005), Wien (seit 2006), Hamburg (seit 2008), Berlin (seit 2009), Palo Alto und Toronto.\n\nIm November 2007 nahmen der LIVE Linux Verband und Lisog Verhandlungen zu einem Zusammenschluss auf.\n\nAb 2010 wurden drei zusätzliche Vorstandsmitglieder des LIVE Linux Verbandes in den Vorstand von Lisog aufgenommen.\n\nAm 21. und 22. Juli 2011 beschlossen die Vereine \"LiVe Linux Verband e.V.\" und \"Lisog\" zu fusionieren. Der Name der neuen Organisation ist \"Open Source Business Alliance\" (OSBA). Die Verschmelzung wurde im September 2011 vollzogen.\n\n"}
{"id": "1370788", "url": "https://de.wikipedia.org/wiki?curid=1370788", "title": "Adobe Photoshop Elements", "text": "Adobe Photoshop Elements\n\nAdobe Photoshop Elements ist ein Bildbearbeitungsprogramm für Pixelgrafiken des US-amerikanischen Softwarehauses Adobe Systems, die speziell für den Heimanwenderbereich zugeschnitten ist.\n\nPhotoshop Elements ist der kleine Bruder von Adobe Photoshop, von dem viele Funktionen, Menüpunkte und Werkzeuge übernommen wurden. Diese Funktionen sind über den Experten-Modus abrufbar und somit ist eine manuelle Bildbearbeitung möglich. Des Weiteren stellt Photoshop Elements den Benutzern auch einen Schnell- und Assistent-Modus zur Verfügung. Im Schnell-Modus kann in Fotos die Belichtung, die Beleuchtung, die Farbe, die Farbbalance und die Schärfe ohne weitere Kenntnisse der Materie einfach mit Schieberreglern angepasst werden. Im Assistent-Modus können Retuschen, Foto- und Kameraeffekte mit Hilfe eines Assistenten durchgeführt werden, der den Anwender durch die Bearbeitung führt. Sämtliche Änderungen in den beiden letztgenannten Programmpunkten können im Experten-Modus weiter bearbeitet und verfeinert werden.\n\nNeben der Bildbearbeitung ist in das Programm auch eine Bilderverwaltung, der Elements-Organizer, integriert. Mit diesem können Mediendaten in Katalogen, Alben und Ordnern verwaltet werden.\n\nPhotoshop CC (CS) hat viele Funktionen, die in Photoshop Elements nicht verfügbar sind. Durch Drittanbieter können viele Funktionen in Photoshop Elements verfügbar gemacht werden, die sonst dem Benutzer nur in Photoshop angeboten werden. Durch die Funktionserhöhungen verringert sich die Kluft zwischen beiden Programmen, so dass auch ambitionierte Bildbearbeiter professionelle Werkzeuge zur Verfügung gestellt bekommen. Folgende Erweiterungen stehen zur Verfügung:\n\n\n"}
{"id": "1371189", "url": "https://de.wikipedia.org/wiki?curid=1371189", "title": "Spybot – Search &amp; Destroy", "text": "Spybot – Search &amp; Destroy\n\nSeit 2015 entstanden eine Reihe weitere Spybot-Programme, die zusätzlichen Schutz der Privatsphäre und mehr Datensicherheit bieten sollen. \n\nMicrosoft Windows 7 bis Windows 10 sowie Windows PE ab 3.0 werden von der aktuellen Version 2.7 unterstützt. Für ältere Windows-Varianten von XP bis Vista und PE 1.0-3.0 ist noch die Version 2.4 verfügbar. Die ebenfalls noch unterstützte Version 1.6.2 läuft auch unter noch älteren Windows-Versionen ab Windows 95. \n\nDas Programm ist in der Grundversion für Privatanwender kostenlos. Der volle Funktionsumfang steht sowohl für private Anwender als auch für Unternehmen und Institutionen nur mit jährlich zu verlängernden, kostenpflichtigen Lizenzen zur Verfügung. Ab Version 2.1 tragen diese Versionen den Zusatz \"+AV\" im Produktnamen und enthalten einen zusätzlichen Antivirenschutz, der auf der Bitdefender-Engine basiert.\n\nSeit Februar 2018 gibt es auch die portable Version 2.7, die nicht installiert werden muss. \n\n"}
{"id": "1371866", "url": "https://de.wikipedia.org/wiki?curid=1371866", "title": "Xara Designer Pro", "text": "Xara Designer Pro\n\nXara Designer Pro (früher \"Xara Xtreme\") ist ein vektorbasiertes Grafik- und Zeichenprogramm. Es wird von der britischen Softwarefirma \"Xara Group Ltd.\" entwickelt, welche seit dem 30. Januar 2007 eine 100%ige Tochter der deutschen Magix AG ist.\n\nXara Designer Pro bietet eine hohe Verarbeitungsgeschwindigkeit, die durch die Programmierung in Assembler in Xaras Renderer erreicht wird.\n\n\nEs werden zurzeit fünf verschiedene Programmversionen veröffentlicht:\n\nAm 11. Oktober 2005 kündigte die Xara Ltd. an, eine Open-Source-Version von Xara Xtreme für die Betriebssysteme Linux und Mac OS X zu entwickeln. Am 20. März 2006 schließlich wurde die Entwicklerversion 0.3 unter der GPL freigegeben. Sie erlaubte zunächst nur das Laden von Grafiken im hauseigenen .XAR-Dateiformat. Ab der Version 0.5 beherrschte die Open-Source-Version auch Funktionen zum Speichern. Mit der zurzeit aktuellen Version 0.7 verfügt Xara Xtreme auch über Funktionen zum Importieren von SVG-Dateien.\n\nNur die Linux-Version wird als Open-Source-Version entwickelt und freigegeben werden. Da die Windows-Version lizenzbehafteten Code anderer Hersteller (Plugins) beinhaltet, wird die Windows-Version nach wie vor unter einer proprietären Lizenz vertrieben.\n\nDer Hersteller hat eine Zusammenarbeit mit dem Open-Source-Vektorgrafikprogramm Inkscape angekündigt, um gemeinsam ein leistungsfähiges freies Vektorprogramm für Linux zu entwickeln.\n\nLaut kontinuierlichen Informationen aus dem Jahr 2007 von Pro-Linux ist die 100%ige Umsetzung als Open-Source-Version nach der Übernahme durch die Firma Magix derzeit unwahrscheinlich und die Zukunft des Open-Source-Projektes ungewiss (die proprietäre Version ist davon nicht betroffen).\n\n"}
{"id": "1374178", "url": "https://de.wikipedia.org/wiki?curid=1374178", "title": "Poc32", "text": "Poc32\n\nPoc32 ist ein von dem deutschen Funkamateur \"Detlef Fliegl\" für das Betriebssystem Windows entwickeltes Computerprogramm zur Encodierung und Decodierung von FLEX- und POCSAG-Signalen, wie sie von Pager-Betreibern ausgesendet werden.\n\nGroßes Aufsehen erzielte das Programm bei seiner ersten Veröffentlichung 1997, als diverse Magazine über die Abhörmöglichkeit der damals beliebten und heutzutage zum größten Teil eingestellten Pagerdienste wie etwa Telmi oder Scall mittels Poc32 berichteten.\n\nDem Anwender ist es mit Poc32 möglich, mit einem vergleichsweise geringen technischem Aufwand alle in seinem Funkgebiet ausgesendeten Textnachrichten im Klartext und mit Adressangabe mitzulesen. In den Blütezeiten der Pagerdienste war es so möglich, täglich teilweise viele tausend Textnachrichten illegal mitzulesen und abzuspeichern.\n\nZum Betreiben des Dekoders genügt ein Funkscanner, dessen Tonausgang mit dem Toneingang (ggf. über ein Audio-Interface) des Computers verbunden wird. Ein Diskriminatorausgang, der ein besonders sauberes Signal liefert, ist nicht dringend erforderlich. Das Programm dekodiert das ankommende Signal und stellt das Ergebnis tabellarisch dar. Angezeigt werden:\n\n\nDer Benutzer hat die Möglichkeit, Filterfunktionen anzuwenden oder bei bestimmten Adressen andere Programme aufzurufen.\n\n\nDas unbefugte Abhören von kodierten Botschaften über Telekommunikationswege wird in Deutschland nach Abs. 1 Satz 1 Telekommunikationsgesetz mit Freiheitsstrafe bis zu zwei Jahre bestraft. Der einzige legale Anwendungsbereich des Programms erstreckt sich auf den Amateurfunk.\n\n"}
{"id": "1374560", "url": "https://de.wikipedia.org/wiki?curid=1374560", "title": "Glyphensonde", "text": "Glyphensonde\n\nGlyphensonden (engl. \"glyph probes\") sind in der wissenschaftlichen Visualisierung eine Methode, um hochdimensionale Daten der Strömungslehre – genauer gesagt Diffusionstensorfelder – bildlich darzustellen.\n\nEin Diffusionstensorfeld gibt für jeden Punkt eines Raumes einen Vektor an. Dieser beschreibt, in welche Richtung sich ein Partikel von diesem Punkt aus bewegen würde. Die Länge des Vektors gibt dabei an, wie stark der Partikel dabei beschleunigt würde. \n\nDa sich an jedem Punkt des dreidimensionalen Raumes ein dreidimensionaler Vektor findet, gilt es insgesamt, sechsdimensionale Datensätze bildlich darzustellen. Willem C. de Leeuw und Jarke J. van Wijk stellten dafür 1993 das Prinzip der Glyphensonde vor. Die Glyphensonde wird an einem Punkt in den Raum eingeblendet und stellt über ihre verschiedenen geometrischen Einzelteile Größen wie Teilchenbahn, Tangentialbeschleunigung und Scherkraft an diesem Punkt dar.\n"}
{"id": "1377546", "url": "https://de.wikipedia.org/wiki?curid=1377546", "title": "Apple Studio Display", "text": "Apple Studio Display\n\nDie Studio Displays sind TFT- und CRT-Computerbildschirme der Firma Apple.\n\nDas erste Studio Display wurde am 17. März 1998 eingeführt. Dies war ein 15,1\"-Modell mit Active Matrix TFT Panel, 24 Bit Farbtiefe, einer Helligkeit von 180 cd/m² und einer Auflösung von 1024 × 768 Pixeln.\nDas Design des Displays war revolutionär, es war das erste Apple Produkt mit dunkelblauem transluzenten Gehäuse, welches später auch beim Designklassiker iMac verwendet wurde. Aufgestellt wurde das Display durch einen Teleskopstandfuß oder einen Bilderbügel.\n\nAls Schnittstelle zur Bildübertragung verwendeten die Studio Displays DB-15, VGA, DVI und den Apple Display Connector, kurz ADC, eine Modifikation von DVI, die DVI selbst, USB und Strom in einem Anschluss bündelt. Das abgebildete 17-Zoll-TFT besitzt wie das 15-Zoller nur einen ADC-Anschluß.\n\nIm Januar 1999 kamen die CRT-Monitore heraus. Sie waren in den Größen 15, 17 und 21 Zoll im semitransluzenten bondi-blue-Design des iMac zu haben.\n\nApples letzter CRT-Monitor war das 17\"-CRT-Display, welches im Juli 2000 zusammen mit einem 15\"-TFT eingeführt wurde. Das Design beider Displays wurde erneut vollkommen verändert. Sie waren im Acryl-Design des G4 gehalten.\n\nMitte Mai 2001 kam mit einem 17\"-TFT-Display die letzte Version des Studio Displays heraus.\nDie externen Bildschirme wurden dann als Cinema Displays weitergeführt.\n"}
{"id": "1378732", "url": "https://de.wikipedia.org/wiki?curid=1378732", "title": "Grafikfilter", "text": "Grafikfilter\n\nUnter Grafikfiltern versteht man Funktionen in einer Grafiksoftware, beispielsweise einem Bildbearbeitungsprogramm, die ein bestehendes digitales Bild (meistens Rastergrafik) mit einem vorprogrammierten Algorithmus, der häufig in einigen Parametern konfigurierbar ist, gezielt verändern. Der Begriff lehnt sich an die Filter im Bereich der Fotografie an, etwa an Farbfilter oder Effektfilter.\n\nTeilweise wird zwischen „Filtern“ und „Effekten“ unterschieden, die Bedeutung der beiden Begriffe ist allerdings nicht klar voneinander abgegrenzt. Jeder Filter oder Effekt erzeugt eine bestimmte Wirkung. So lässt sich z. B. ein Bild schnell mit Strukturen versehen, ein Foto in ein digitales Kunstwerk verwandeln, oder es lassen sich Lichtquellen hinzufügen. \n\nGängige Filter und Effekte sind beispielsweise:\n\n\nOft hat man die Möglichkeit, ein Vorschaubild des Effektes zu betrachten, bevor man den Effekt tatsächlich anwendet. \n\nPlugins: Zusatzfilter\n\nBildbearbeitungsprogramme bieten zwar meistens von Haus aus schon zahlreiche Filter und Effekte, dennoch lassen sie sich in ihrer Funktionalität über eine Plug-in-Schnittstelle für Filter und Effekte erheblich erweitern. Meist findet sich diese in einem Programmordner mit dem Namen „Plugin“ bzw. „Zusatzmodule“. Dort werden dann diese Module (8bf-Dateien) installiert und können schließlich nach einem Neustart des Programmes im Programmenü aufgerufen werden. Ein solcher Programmordner befindet sich in jedem leistungsfähigen Bildbearbeitungs- oder Vektorgrafikprogramm.\n\nBekannte Bildbearbeitungs-Filter\n\nEinige Bildbearbeitungsprogramme spezialisieren sich auf Filter und Effekte.\n\n\n\n"}
{"id": "1383775", "url": "https://de.wikipedia.org/wiki?curid=1383775", "title": "Ju-Te-Computer", "text": "Ju-Te-Computer\n\nDer Ju-Te-Computer (alias: Tiny, JU+TE-Computer, \"Comp JU+TE r\") ist ein Heimcomputer-Bausatz, der in diversen Ausbaustufen in der DDR-Zeitschrift Jugend+Technik ab Juli 1987 von Helmut Hoyer vorgestellt wurde. Die Rechnerarchitektur basiert auf dem 8-Bit-Einchipmikrorechner UB8830M mit integriertem Tiny-MP-Basic. \n\nFür den Ju-Te-Computer gibt es mit JTCEMU einen Emulator.\n\n\n"}
{"id": "1385101", "url": "https://de.wikipedia.org/wiki?curid=1385101", "title": "Twinkle (Software)", "text": "Twinkle (Software)\n\nTwinkle ist eine freie Software für IP-Telefonie (\"Voice-over-IP\") und ist mit der GNU General Public License (GPL) lizenziert. Neben dem Session Initiation Protocol (daher \"SIP-Telefon\") verwendet Twinkle eine Reihe anderer Protokolle, die aber durchweg frei und öffentlich dokumentiert sind. Die Oberfläche benutzt die Bibliothek Qt.\n\nDas Projekt wurde vom Entwickler M. de Boer initiiert, die letzte Veröffentlichung in diesem Rahmen erfolgte im Februar 2009 (Stand September 2015).\n\nDer Entwickler Luboš Doležel nahm daraufhin eine Abspaltung vor, die insbesondere das Ziel hat, die Software von der mittlerweile überholten Qt-Version 3 bzw. 4 auf die aktuelle Version 5 zu portieren.\n\nTwinkle ist bei vielen Linux-Distributionen als fertiges Software-Paket enthalten und lässt sich daher meistens mit wenigen Mausklicks installieren. Die aktuelle Twinkle-Version lässt sich im Bedarfsfall mit geringem Aufwand aus dem Quelltext übersetzen. Eine Windows-Version von Twinkle existiert bislang nicht.\n\nDie wichtigsten Merkmale von Twinkle sind\n\n"}
{"id": "1385575", "url": "https://de.wikipedia.org/wiki?curid=1385575", "title": "Wingz", "text": "Wingz\n\nWingz (Eigenschreibweise \"WingZ\") ist ein Tabellenkalkulationsprogramm, das von Informix entwickelt wurde. Es war als Hypertextanwendung auch dazu gedacht, mittels einer Scriptsprache (HyperScript, die sich aus der Sprache HyperTalk entwickelt hat) sich mit der Informix-Datenbank zu verbinden und die Ergebnisse in einem Windows-Fenster als Text oder als Graphik anzuzeigen.\n\nSeit dem 30. Januar 1995 hat Informix die exklusiven Vertriebsrechten an die Investment Intelligence Systems Group (IISG) abgegeben.\n\n\n\n"}
{"id": "1386061", "url": "https://de.wikipedia.org/wiki?curid=1386061", "title": "AC1", "text": "AC1\n\nDer AC1 (Abkürzung für „Amateurcomputer 1“) war ein Heimcomputer-Bausatz-Projekt, das vom \"Haus des Radioklubs der DDR\" betreut wurde. Die Beschreibung des 8-Bit-Computer-Bausatzes wurde erstmals im Jahre 1983 in der DDR-Zeitschrift „Funkamateur“ veröffentlicht.\n\nEntwickelt wurde der AC1 vom Referat Technik des Präsidiums des Radioklubs der DDR am Anfang der 1980er Jahre. Der Vorschlag für die Grundstruktur des Rechners stammte von Frank Heyder, dessen Konzept auf dem Prozessor U880 basierte. Von ihm stammte auch das erste Betriebssystem des Rechners.\n\nTrotz der Knappheit der entsprechenden Bauteile für den Rechner konnte durch die Kooperation verschiedener Unternehmen und Privatpersonen der Ansturm auf den Bausatz bewältigt werden, sodass bis zu 5.000 Geräte nachgebaut wurden. Im Jahre 1988 wurde für den Rechner sogar eine Version des Betriebssystems CP/M veröffentlicht, was die Softwareauswahl für diesen Rechner deutlich erhöhte.\nWobei die Mehrzahl der Nachbauer keinen Bausatz bekommen konnte, sondern die Bauelemente in Eigeninitiative beschaffen musste. Die Hauptplatine musste von Hand durchkontaktiert werden.\n\nDaneben wurde der Rechner besonders von Funkamateuren nachgebaut, da er Anwendungen wie Funkfernschreiben, Telegrafie und Logbuchführung bei Amateurfunkcontesten bewältigte. Der größte Aufschwung zum Nachbau kam auf, als die Betriebsart Packet Radio (nach dem X.25-Protokoll; siehe AX.25-Protokoll) unter den Funkamateuren aufkam.\nSpäter wurde das Gerät mit einem Vollgrafiksystem und einem Diskettenbetrieb mit 256-KByte-RAM-Karte erweitert.\n\nDer LLC2 war softwareseitig größtenteils kompatibel zum AC1.\n\n\n\n"}
{"id": "1387256", "url": "https://de.wikipedia.org/wiki?curid=1387256", "title": "Junior-Computer", "text": "Junior-Computer\n\nDer Junior-Computer war ein erweiterungsfähiger Einplatinencomputer-Bausatz auf Basis des 6502-Prozessors.\n\nEntwickelt wurde der Rechner von Loys Nachtmann für die Zeitschrift Elektor, die den Bausatz in einer mehrteiligen Zeitschriftenserie ab Mai 1980 vorstellte.\nObwohl der Elektor-Junior-Computer nicht der erste Einplatinen-Computer auf Basis des 6502-Prozessors war (in den USA gab es damals schon den KIM-1), war es besonders in Westeuropa ein sehr erfolgreiches Modell, das mehrere tausend Mal nachgebaut wurde. Die Entwicklung des Junior-Computers war inspiriert vom KIM-1.\n\nIm Grundausbau verfügte das System über 1 kByte ROM und 1 kByte RAM. Zur Bedienung des Monitorprogramms im ROM standen lediglich sechs 7-Segment-Anzeigen und ein kleines Tastenfeld mit 23 Tasten zur Verfügung. Der Grundausbau konnte nachträglich um eine Interfacekarte ergänzt werden, die mit Abstandshaltern unter der Basisplatine montiert wurde. Die Interfacekarte enthielt Steckplätze für eine Speichererweiterung (4 kByte EPROM, 1 kByte RAM), einen parallelen Interface-Baustein und ein Interface um Programme auf Musikkassetten speichern zu können.\n\nAn die Interfacekarte konnte eine Buskarte angesteckt werden über welche das System mit weiteren Steckkarten aufgerüstet werden konnte. Es gab mehrere verschiedene Speicherkarten, ein Input/Output Interface, ein Disketteninterface, EPROM Programmierinterface, Video Terminal und vieles mehr. Neben den 4 von Elektor herausgegebenen Büchern zum Junior-Computer entstand eine ganze Reihe von „Paperware“ Druckausgaben mit Tipps und Tricks, Programmbeispielen und Listings.\n\n\n"}
{"id": "1387651", "url": "https://de.wikipedia.org/wiki?curid=1387651", "title": "Santa Fe Institute", "text": "Santa Fe Institute\n\nDas Santa Fe Institute (SFI) ist ein privates gemeinnütziges Forschungs- und Lehrinstitut in Santa Fe, New Mexico.\n\nEs wurde 1984 gegründet, um in interdisziplinärer Grundlagenforschung eine Theorie komplexer adaptiver Systeme in Physik, Biologie, Technik und Sozialwissenschaften zu erarbeiten. Die Gründungsmitglieder waren George Cowan, David Pines, Stirling Colgate, der Nobelpreisträger Murray Gell-Mann, Nicholas Metropolis, Herbert L. Anderson, Peter Carruthers und Richard Slansky, bis auf Pines und Gell-Mann alle Wissenschaftler des Los Alamos National Laboratory.\n\nDas Institut sieht sein ursprüngliches Ziel als erreicht an, da die Theorie komplexer Systeme inzwischen ein etablierter Forschungsgegenstand ist, dem sich weltweit eine Anzahl wissenschaftlicher Institute widmen. Als gegenwärtige Arbeitsthemen nennt das Institut Kognitive Neurowissenschaft, Computersimulation in Physik und Biowissenschaften, ökonomische und soziale Wechselwirkungen, evolutionäre Dynamik, Netzwerkdynamik und Robustheit.\n\n\n\n\n"}
{"id": "1388374", "url": "https://de.wikipedia.org/wiki?curid=1388374", "title": "Blinn-Beleuchtungsmodell", "text": "Blinn-Beleuchtungsmodell\n\nDas Blinn-Beleuchtungsmodell (auch \"Blinn-Phong-Modell\") ist in der Bildsynthese ein lokales Beleuchtungsmodell zur Lichtreflexion an Oberflächen. Als Grundlage wird das Phong-Beleuchtungsmodell verwendet. Durch Nutzung sogenannter Halfway-Vektoren werden die notwendigen Berechnungen beschleunigt, ohne das Ergebnis auf merkbare Weise zu beeinflussen. Das Modell wurde 1977 von James F. Blinn (\"Jim Blinn\") beschrieben, der auch das Bumpmapping entwickelte.\n\nIn der Praxis wird das Blinn-Beleuchtungsmodell zum Beispiel in OpenGL verwendet, da es die Berechnung des Reflexionsvektors vermeidet. Stattdessen wird die Winkelhalbierende formula_1 verwendet:\n\nformula_2\n\nmit\n\n\nMit diesem kann nun der Cosinus des Winkels formula_5 zwischen der Normalen formula_6 und der Winkelhalbierenden formula_1 berechnet werden:\n\nformula_8\n\nDie oben genannte Formel gilt natürlich nur unter der Voraussetzung, dass formula_9 ist. Diesen Cosinus kann man nun anstelle von formula_10 in der aus dem Phong-Beleuchtungsmodell bekannten Formel zur Berechnung des spekularen Anteils einsetzen:\n\nformula_11\n\nmit\n\n\nDer Winkel formula_5 ist angenähert halb so groß wie der\nWinkel formula_16 im Phong-Modell. (Diese Beziehung ist exakt, wenn \"L\", \"V\" und \"N\" in einer Ebene liegen.)\n\nUm mit dem Blinn-Modell vergleichbare Ergebnisse zu erzielen wie mit dem Phong-Modell, muss man den Exponenten \"n\" in der obigen Formel viermal so groß wählen wie den Exponenten \"n\" beim Phong-Modell.\n\n"}
{"id": "1391562", "url": "https://de.wikipedia.org/wiki?curid=1391562", "title": "Computer-aided styling", "text": "Computer-aided styling\n\nComputer-aided styling (CAS) \"(deutsch: computergestützte Gestaltung)\" bezeichnet den virtuellen Entwicklungsprozess im Design. Objekte werden auf der Basis von Skizzen (Renderings), mündlichen Angaben oder Tastdaten (Punktewolke) in einer 3D-Software modelliert.\n\nDer CAS-Prozess ersetzt in zunehmendem Maße die klassische Formfindung mit Clay (Industrieplastilin) oder Modellbau (z. B. Hartschaum, Ureol, Spachtelmasse). Zwar bietet der CAS-Prozess viele Vorteile wie Modellerstellung auf Konstruktionsdaten (Package), Variantenvielfalt, Zurückgehen auf ältere Varianten und kostengünstigere Modellierung, allerdings fällt die Beurteilung des Designs am Bildschirm schwieriger aus als beim klassischen Modell. Dies liegt an der künstlichen (virtuellen) Beleuchtung, der Verkleinerung bzw. Vergrößerung auf einen Computermonitor, der meistens unüblichen Betrachtungswinkel und der Zweidimensionalität auf dem Schirm. Zum Teil werden diese Nachteile durch aufwändigere Visualisierungen auf Beamern, großen Plasmabildschirmen oder sogar dreidimensional (z. B. CAVE) ausgeglichen. Diese Mittel stehen jedoch nur bei kostenintensiven Projekten zur Verfügung. Stellt man die Daten physikalisch (z. B. durch STL, 3D-Druck oder CNC-Fräsen) dar, büßt man einen Teil des Kostenvorteils ein.\n\nDie größte Anwendung von CAS findet sich in der Automobilindustrie sowie Luftfahrtindustrie. In frühen Designphasen kommen Polygonmodellierer wie Autodesk Maya (Software) oder Cinema 4D zum Einsatz. Diese Modelliermethode erlaubt schnelle Designänderungen und viele Varianten in sehr kurzer Zeit. Die Flächenqualität ist durch die Facettierung allerdings noch nicht sehr hoch. Dennoch können die Daten für erste Hartmodelle (Stereolithografie) oder als Basis für ein Claymodell verwendet werden. Dazu wird aus dem Polygonmodell ein CNC-Programm errechnet und aus dem Clay herausgefräst.\n\nIn späteren Designphasen kommen NURBS-Modellierer wie Autodesk Alias Studio (\"siehe auch Alias (Unternehmen)\"), ICEM Surf oder Rhinoceros zum Einsatz. Rhinoceros findet sich auf Grund des relativ günstigen Anschaffungspreises größtenteils in Industriedesignbüros und mittelständischen Unternehmen wieder. ICEM Surf ist in der Automobilbranche stark verbreitet und richtet seinen Fokus stark auf den Strak.\n\nDie erzeugte Geometrie kann je nach Qualität von der Konstruktion für Voruntersuchungen und Konzepterstellung bis zur Serienkonstruktion verwendet werden. Mithilfe spezieller Visualisierungsprogramme wie \"Autodesk VRED Professional\" oder \"RTT DeltaGen\" werden aus CAS-Modellen statische Renderings, Animationen und Filme erzeugt. Diese Visualisierungen werden von der firmeninternen Kommunikation bis zu Werbeauftritten eingesetzt und ermöglichen die Verkürzung von Produktzyklen durch frühzeitiges Einsetzen des Marketings. Echtzeitvisualisierung wird vorwiegend zur Designentscheidung sowie für interaktive Anwendungen wie Spiele und Produktkonfiguratoren verwendet.\n"}
{"id": "1392063", "url": "https://de.wikipedia.org/wiki?curid=1392063", "title": "QuickBASIC", "text": "QuickBASIC\n\nQuickBASIC ist eine BASIC-Entwicklungsumgebung für MS-DOS, Mac OS und OS/2.\n\nQuickBASIC ermöglicht das Kompilieren von Basic-Code in ausführbare Dateien sowie in Module und Bibliotheken. Die letzte erschienene Version ist \"Quickbasic Extended 7.1 PDS\" (Professional Development System), die unter anderem mit Unterstützung für das Dateiformat ISAM und einer verbesserten Speicherverwaltung ausgeliefert wird. QBasic, welches mit MS-DOS ab Version 5 mitgeliefert war, basierte auf QuickBasic 4.5.\n\nFür den Apple Macintosh existierte das Produkt \"Microsoft QuickBASIC version 1.00 for the Apple Macintosh\".\n\nDie 1985 erschienene erste Version des QuickBASIC-Compilers enthielt noch keine integrierte Entwicklungsumgebung, diese wurde mit der Version 2 mitgeliefert. Es unterstützte EGA-Grafikkarten. Die Version 3 kam mit zwei EXE-Dateien, codice_1 und codice_2. Letztere unterstütze den mathematischen Coprozessor. Mit QuickBASIC 3 wurden neue Kontrollstrukturen eingeführt, codice_3, codice_4, dazu wurden Konstanten (codice_5) unterstützt. Die IDE wurde um einen Debugger erweitert, mit der das Setzen von Breakpoints und eine Variablen-Inspektion möglich war. Die IDE der 1987 ausgelieferte Version 4 enthielt eine Quelltext-Syntaxüberprüfung während des Bearbeitens, einen ausgebauten Debugger und eine neue Online-Hilfe. Der Compiler unterstützte mit codice_6-Aufrufe von externen, in Hochsprachen (QuickBASIC, C++ etc.) entwickelten Bibliotheken. Die letzte Version, 4.5 enthielt eine komplette Befehlsreferenz, die mit der kontext-sensitiven Hilfe aufgerufen werden konnte.\n\nQuickBASIC wurde durch BASIC PDS 7 und 7.1 ergänzt, welches parallel zu QuickBASIC 4.5 vermarktet wurde. Die IDE aus QuickBASIC wurde bei 7.1 weiterverwendet. Die darauffolgende und letzte für DOS erschienene Basic-Version seitens Microsoft war Visual Basic für DOS 1.0, welches beide Produktlinien ablöste. QuickBASIC-Programme laufen auf diesen neueren Versionen.\n\nQuickBASIC für den Apple Macintosh bundelte den Basic-3.0-Interpreter und den Basic-1.0-Compiler, welche vorher separat verkauft worden waren. System 4.1 bis System 6 wurden unterstützt. Mit dem Update auf \"1.00e\" wurde System 7 auf 68000 und 68020-Macintoshs unterstützt, wobei die 32-Bit-Adressierung deaktiviert werden musste (was bei 68040 nicht möglich war).\n\nEinige weitere Funktionalitäten heben QuickBASIC von ab:\n\nEs war bis von einigen Ausnahmen abgesehen weitgehend kompatibel zu GW-BASIC. QBasic-Programme laufen auch auf QuickBasic 4.5. Die meisten QuickBasic-Programme laufen auch unter Visual Basic für MS-DOS 1.00.\n\nFür die Unterstützung von Hercules-Grafikkarten musste unter QuickBASIC 4.5 das TSR-Programm \"msherc.com\" im Voraus gestartet sein.\n\nNach der Einführung von Visual Basic wurde QuickBasic nicht mehr weiterentwickelt. Es fehlt Unterstützung für viele Möglichkeiten moderner Betriebssysteme wie zum Beispiel nebenläufige Prozesse, hohe Bildschirmauflösungen im Grafikmodus oder das automatische Erstellen von Dialogfenstern. Letzteres wurde mit Visual Basic für MS-DOS eingeführt.\n\nTrotzdem wird QuickBasic wie auch QBasic auch heute noch verwendet, und zahlreiche private Webseiten beschäftigen sich mit dem Thema. Dies ist vor allem auf die leichte Erlernbarkeit und die große Verbreitung in der Vergangenheit zurückzuführen. Mittlerweile gibt es eine Menge Basic-Dialekte, auf die viele der ehemaligen QuickBasic-Programmierer umgestiegen sind.\n\nQuickBasic sowie QBasic können in der Virtual DOS Machine, die 32-Bit-Betriebssystemen der Windows-NT-Familie enthalten ist, ausgeführt werden.\n\nDarüber hinaus gibt es einen quelloffenen 32-Bit-Compiler namens FreeBASIC, der auf QuickBASIC aufbaut, und insbesondere mit der Compileroption (\"-lang qb\") auch die meisten QuickBasic oder QBasic Programme kompiliert. Außerdem gibt es das QB64-Projekt, das gewissermaßen eine Portierung von QuickBasic für moderne Betriebssystem darstellt und das ausführbare Programme erstellt, die auf 32- und 64-Bit-Betriebssystemen lauffähig sind. Im Gegensatz zu anderen freien Basicversionen führt QB64 die meisten Programme, die für Quickbasic 4.5 oder QBasic geschrieben wurden, ohne weitere Anpassungen aus, ebenso GW-Basic-Programme. QBX-Dateien von Basic PDS werden nicht unterstützt. Hinzu kommen umfangreiche Erweiterungen, um moderne Betriebssystemfunktionen zu unterstützen, beispielsweise ein Netzwerkstack, Grafikfunktionen bis 32-Bit-Farbtiefe und Unterstützung der Zwischenablage. QB64 gibt es auch in Versionen für Linux und Mac OS X.\n\n"}
{"id": "1393621", "url": "https://de.wikipedia.org/wiki?curid=1393621", "title": "Darcs", "text": "Darcs\n\nDarcs (rekursives Akronym für \"Darcs advanced revision control system\") ist ein System zur verteilten Versionsverwaltung von Softwareprojekten und wurde von David Roundy geschrieben. Im Gegensatz zum populären CVS oder SVN kennt Darcs kein zentrales Quelltextarchiv. Jede Kopie des Ordners mit dem Darcs Projekt stellt ein eigenständiges „Repository“ dar. Kern von Darcs sind die „Patches“ (engl. für \"Flicken\", \"Ausbesserung\"; vgl. Patchwork), mit denen die Unterschiede zwischen den einzelnen Versionen repräsentiert und die verschiedenen Repositories auf denselben Stand gebracht werden. Unter bestimmten Umständen kann die Reihenfolge der „Patches“ geändert werden bzw. Änderungen durch einzelne Patches zurückgenommen werden.\n\nTypischerweise erstellt man eine lokale Kopie eines Archivs mit dem Befehl \"get\", führt die Änderungen durch und erstellt mit dem Befehl \"record\" einen „Patch“, den man mit \"push\" oder \"send\" an andere Archive weitergibt, dies kann auch über E-Mail erfolgen. Mit \"pull\" kann man Patches von weiteren Archiven holen und so das lokale Archiv aktualisieren.\n\nDarcs wurde in Haskell geschrieben und greift für die Datenübermittlung auf bewährte Technologien wie SSH, HTTP und E-Mail zurück. Für die Datenübermittlung via HTTP bietet sich das in Python geschriebene \"darcsweb\" an, welches sich als CGI an die gängigen Webserver anbinden lässt.\n\nIm Unterschied zu allen anderen Versionsverwaltungen verwaltet Darcs nicht einen Baum von Revisionen, sondern Patches und Abhängigkeiten zwischen diesen. Aus der gleichen Menge Patches lassen sich so viel mehr denkbare Revisionen generieren, etwa indem Patches weggelassen werden. Das geht bei Darcs im Prinzip überall, bei herkömmlichen Versionsverwaltungen kann immer nur der letzte Patch wieder entfernt werden.\n\n"}
{"id": "1395517", "url": "https://de.wikipedia.org/wiki?curid=1395517", "title": "Coccinella (Instant Messenger)", "text": "Coccinella (Instant Messenger)\n\nCoccinella (von ‚Marienkäfer‘) ist ein freier Instant-Messaging-Client für das Protokoll XMPP.\n\nDas Programm wurde in Tcl/Tk verfasst, und ist unter BSD, Linux, macOS, Solaris und Windows sowie auf allen anderen Plattformen, für die es bereits Tcl/Tk gibt, lauffähig. Mats Bengtsson veröffentlichte das Programm im Jahr 1999 ursprünglich unter dem Namen \"Whiteboard\". Die Umbenennung in Coccinella erfolgte 2003.\n\n\n\n"}
{"id": "1396498", "url": "https://de.wikipedia.org/wiki?curid=1396498", "title": "Cloop", "text": "Cloop\n\ncloop () ist ein Modul für den Linux-Kernel. Es ist eine Art Erweiterung des Loop devices, das Unterstützung für transparent entkomprimierte Blockgeräte schafft. Es ist jedoch kein komprimiertes Dateisystem, sondern bezieht sich nur auf Loop devices, zum Beispiel Dateien, die ein Dateisystem enthalten.\n\ncloop wurde ursprünglich von Rusty Russell für die \"Linuxcare Bootable Business Card\" geschrieben, wird heutzutage jedoch von Klaus Knopper verwaltet, der es für die Live-Linux-Distribution Knoppix verwendet. Wegen der durchschnittlichen Kompressionsrate von etwa 2,5 zu 1 (für normale Software) eignet sich das Verfahren sehr gut für LiveCDs. Das Knoppix Cloop-Abbild ist beispielsweise 700 MB groß und umfasst etwa 1,8 GB komprimierte Software.\n\nEin cloop-Abbild enthält:\n\nDie Datenblöcke sind separat komprimiert. Damit ist es möglich, einzelne Blöcke auszulesen, ohne das gesamte Abbild vorher zu dekomprimieren. Dies spart erheblich Arbeitsspeicher, geht jedoch auf Kosten der Zugriffsgeschwindigkeit sowie der Gesamtkompressionsrate. LiveCD-Abbilder haben in der Regel eine Blockgröße von 256 KB, was einen guten Kompromiss zwischen Zugriffszeit und Speichereffizienz darstellt.\n\nAls Kompressionmethoden werden zlib (LZ77 Deflate) und 7zip (LZMA) unterstützt.\n\n"}
{"id": "1401889", "url": "https://de.wikipedia.org/wiki?curid=1401889", "title": "Photo Booth", "text": "Photo Booth\n\nPhoto Booth ( für \"Fotoautomat\") ist eine Software von Apple, die die Aufnahme von Fotos und Videos mit Hilfe einer (eingebauten oder externen) iSight-Kamera oder mit Webcams anderer Hersteller ermöglicht. Die einfache Hinzufügung verschiedener visueller Effekte gehört zum Programmumfang.\n\nApple stellte die Software am 12. Oktober 2005 zusammen mit dem iMac G5 für Mac OS X 10.4 (Tiger) vor. Seitdem wurde Photo Booth als Bestandteil dieses Rechners, der über eine integrierte iSight-Webcam verfügt, kostenlos mitgeliefert.\n\nSeit Mac OS X 10.5 (Leopard) beherrscht Photo Booth nicht nur den Umgang mit Fotos, sondern auch die Aufnahme und Manipulation von Webcam-Videos. Das Programm gehört seither zur Apple-Standardausstattung, auch wenn nicht alle Computer eine eingebaute iSight-Kamera besitzen.\n\nDie aufgenommenen Schnappschüsse können u. a. mit einem Klick per E-Mail versendet, in iPhoto verwendet oder als Icon in iChat bzw. Nachrichten und im Apple Adressbuch genutzt werden. Seit OS X 10.8 Mountain Lion können sie auch per Facebook, Twitter oder Flickr geteilt werden. Um während der Aufnahme für entsprechende Lichtverhältnisse zu sorgen, kann der Bildschirm als Blitzlichtersatz für die Aufnahme auf reines Weiß geschaltet werden.\n\nAb der Version 4.3 des iOS ist das Nutzen von Photo Booth auch auf dem iPad möglich. Die App läuft auf dem iPad 2 und neuer.\n\nNeben der unveränderten Normalansicht gibt es bei Photo Booth folgende Effekte:\n\nAußerdem kann der Benutzer eigene Hintergrundbilder verwenden, in die das Webcam-Benutzerportrait integriert wird.\n"}
{"id": "1405041", "url": "https://de.wikipedia.org/wiki?curid=1405041", "title": "Pacemaker", "text": "Pacemaker\n\nPacemaker (englisch für \"Schrittmacher\") ist eine gebräuchliche freie Software für die Ressourcenverwaltung in mit Linux betriebenen Hochverfügbarkeits-Clustern (Cluster Ressourcen Management, CRM). Es ist in viele gebräuchliche Linux-Systeme integriert oder wird als Erweiterung angeboten, wie mit der High Availability Extension des SUSE Linux Enterprise Server.\n\nPacemaker wird als freie Software auch im Quelltext verbreitet, wobei es unter der GNU General Public License (GPL) steht – ausgenommen von den Programmbibliotheken und Header-Dateien, die unter der etwas liberaleren GNU Lesser General Public License (LGPL) stehen.\n\nEs verwaltet den Betrieb von (per Skript gestarteten) Server-Diensten auf einem Rechnercluster zur Gewährleistung der jeweils höchstmöglichen Verfügbarkeit, überwacht Server und Dienstprogramme.\n\nZur Kommunikation der Knoten untereinander wird Corosync empfohlen.\nPacemaker unterstützt die Steuerung von STONITH-Geräten, mit denen Knoten im Falle einer Abtrennung vom Cluster-Betrieb automatisch abgeschaltet werden können.\n\nEs ist eine Linux-Software, die eventuell auch auf anderen unixoiden Systemen funktioniert.\n\nPacemaker war früher Teil des Linux-HA-Projektes; es ist der 2008 neugeschriebene Nachfolger von Heartbeat, der in ein neues Projekt abgespalten wurde. Es wurde von SUSE-Entwicklern (hauptsächlich in der Programmiersprache C) entwickelt, wodurch auch als erstes für den Betrieb mit SUSE kommerzielle technische Unterstützung verfügbar ist.\n\nSeit geraumer Zeit arbeitet Red Hat an einer Implementierung von Pacemaker in ihre \"Red Hat Cluster Suite\". In Red Hat Enterprise Linux 6.X ist Pacemaker als \"technical preview\" enthalten. Ab RHEL 7 ist Pacemaker der Standard Cluster Resource Manager bei Red Hat.\n\n"}
{"id": "1405298", "url": "https://de.wikipedia.org/wiki?curid=1405298", "title": "Fluid-Struktur-Kopplung", "text": "Fluid-Struktur-Kopplung\n\nAls Fluid-Struktur-Kopplung () wird im Ingenieurwesen die Berücksichtigung der gegenseitigen Beeinflussung von Struktur und Strömung bezeichnet. Dabei werden numerische Verfahren zur Strömungs- und Strukturberechnung miteinander gekoppelt.\n\nDie wechselseitige Beeinflussung von Struktur und Strömung ist ein interessantes, in Natur und Technik häufig vorkommendes Phänomen. Sie tritt an elastischen, leicht verformbaren, schwingfähigen, drehbar oder verschiebbar gelagerten, umströmten oder durchströmten Strukturen auf. \nDie Ablösung von Wirbeln kann die umströmte Struktur in merkliche Schwingungen versetzen. Strömungsinduzierte Schwingungen kommen z. B. an Flugzeugflügeln, an Propellerblättern, aber auch bei der Umströmung von Bauwerken vor. Sind die Schwingungen groß genug, beeinflussen sie im Gegenzug die Strömung. Weitere populäre Beispiele für eine Fluid-Struktur-Wechselwirkung sind die Strömung in Blutgefäßen oder die Umströmung von Herzklappen. \n\nMit einer numerischen Simulation kann das reale Verhalten eines Bauteils oder einer Strömung modellhaft wiedergegeben werden. Gebräuchliche numerische Verfahren zur Strömungs- und Strukturberechnung sind das Finite-Volumen-Verfahren und die Finite-Elemente-Methode. Beiden Verfahren beruhen auf der Lösung partieller Differentialgleichungen. Das Berechnungsgebiet wird dazu mit Hilfe eines Rechengitters in einzelne Zellen unterteilt (räumliche Diskretisierung), in denen die Differentialgleichungen unter Berücksichtigung geeigneter Randbedingungen gelöst werden. Die dabei entstehenden großen Gleichungssysteme werden direkt oder iterativ numerisch gelöst. \nDie betrachteten Prozesse können stationär (zeitunabhängig) oder instationär (zeitabhängig) sein. Bei instationären Prozessen wird die Lösung an diskreten Zeitpunkten berechnet, wobei die Lösung zur aktuellen Zeit von den Lösungen zu früheren Zeitpunkten abhängt (zeitliche Diskretisierung).\n\nFür die Lösung von gekoppelten Problemen stehen prinzipiell zwei Lösungsmethoden zur Verfügung:\n\nFür die Fluid-Struktur-Kopplung müssen Randbedingungen zwischen den Programmen zur Strömungs- und Strukturberechnung ausgetauscht werden. Die aus der Strömung resultierenden Druckkräfte und Wandschubspannungen wirken auf die angrenzenden Wände und stellen eine Belastung für die Struktur dar. Sie werden an den Grenzflächen auf das Rechengitter der Struktur interpoliert. Die Rechengitter auf der Strömungsseite und auf der Strukturseite können unterschiedlich sein. Führen die strömungsinduzierten Kräfte zu einer Verschiebung bzw. Verformung der Struktur, geht im nächsten Berechnungsschritt die veränderte Position des Rechengitters als neue Randbedingung in die Strömungssimulation ein. Dieser Prozess wird solange iterativ durchlaufen, bis das geforderte Konvergenzkriterium bzw. die vorab festgelegte maximale Anzahl an Iterationen erreicht ist. Bei instationären Prozessen wird anschließend die Lösung zum neuen Zeitpunkt bestimmt.\n\nNeben der iterativen Kopplung werden bei starken instationären Wechselwirkungen direkte Lösungsverfahren verwendet, wobei eine einzige Lösungsmatrix für das Gesamtsystem (Fluid und Struktur) aufgestellt und gelöst wird.\n\nEine echte, zweiseitige Fluid-Struktur-Kopplung liegt vor, wenn die Strömung durch die Strukturänderung spürbar beeinflusst wird. Oft ist aber die Rückwirkung der Strukturänderung auf die Strömung so schwach, dass sie vernachlässigt werden kann. In diesem Fall spricht man von einer einseitigen Fluid-Struktur-Kopplung. \n\n"}
{"id": "1406476", "url": "https://de.wikipedia.org/wiki?curid=1406476", "title": "Procmail", "text": "Procmail\n\nProcmail ist ein Mail Delivery Agent (MDA), der zur serverseitigen Filterung von E-Mail-Nachrichten verwendet wird.\n\nFür gewöhnlich wird Procmail von einem Mail Transfer Agent (MTA) wie zum Beispiel Sendmail oder Postfix aufgerufen. Durch Procmail kann dann eine Vorsortierung eingehender E-Mail-Nachrichten in Ordner realisiert werden. Dabei können E-Mails anhand diverser Eigenschaften gefiltert werden, wie zum Beispiel:\n\nZu diesem Zweck bedient sich Procmail unter anderem regulärer Ausdrücke. Procmail verfügt des Weiteren über die Fähigkeit, externe Programme aufzurufen oder aber automatisiert auf E-Mails zu antworten; so lassen sich beliebte Features wie Weiterleitungen, Autoresponder oder Urlaubsschaltungen implementieren.\n\nUm E-Mails in bereits bestehende Mailboxen zu filtern, liefert Procmail das Hilfsprogramm \"formail\" mit.\n\nDas folgende Beispiel einer Procmail-Steuerdatei (meist .procmailrc) scannt eingehende Mails mit SpamAssassin und filtert sie anschließend.\n\ncodice_1\ncodice_2 leitet eine Filterregel ein. Das angefügte codice_3 der ersten Regel sorgt dafür, dass Procmail wartet, bis das Programm fertig ist. Das angefügte codice_4 der beiden folgenden Regeln sorgt dafür, dass nur die Header der E-Mail durchsucht werden. Ein abschließender Doppelpunkt weist Procmail an, File-Locking zu verwenden. Das ist immer dann notwendig, wenn nicht sichergestellt ist, dass die Regel mehrfach parallel ausgeführt werden kann. Zum Beispiel wenn eine Mail in eine bestimmte mbox-Datei einsortiert werden soll, bei der durch gleichzeitige Zugriffe ein Datenverlust entstehen würde.\n\nZur Erklärung: Zuerst wird die E-Mail mittels einer Pipe an das Programm codice_5 gesendet und gewartet, bis das Programm fertig ist. Die nun gescannte Mail durchläuft die nächsten Filterregeln.\nSpamAssassin fügt gesichteten E-Mails den Header codice_6 hinzu, der je nach eingestellter Schwelle codice_7 (für erkannten Spam) oder codice_8 (für nicht als Spam erkannte Nachrichten) lautet.\nDes Weiteren fügt SpamAssassin einen Header hinzu, der aus codice_9-Zeichen besteht. Die Anzahl der Zeichen steht dabei für den abgerundeten Score (d. h. Wahrscheinlichkeit, dass die Nachricht Spam ist) der E-Mail. Der Score, nach dem eine Nachricht als Spam bewertet wird, liegt standardmäßig bei 5.\n\nDas Beispiel filtert nach der Anzahl der Zeichen. Findet Procmail zehn oder mehr codice_9-Zeichen, so wird die E-Mail im Nulldevice abgespeichert und somit verworfen. Wird eine Nachricht generell als Spam erkannt, so wird die Nachricht im Verzeichnis \"Spam\" abgespeichert. Alle anderen E-Mails werden in der normalen Inbox abgeliefert.\n\n\n"}
{"id": "1406821", "url": "https://de.wikipedia.org/wiki?curid=1406821", "title": "Pro Tools", "text": "Pro Tools\n\nPro Tools ist eine sogenannte Digital Audio Workstation (DAW), also eine professionelle Audioeditor-Software, die vom ehemaligen amerikanischen Unternehmen Digidesign entwickelt worden ist und nach dessen Übernahme durch Avid und seit Version 9.0 unter der Marke \"Avid ProTools\" weiterentwickelt und vertrieben wird. \"ProTools\" verfügt über eine 64-Bit-Architektur mit interner 64-Bit-Gleitkommaverarbeitung. In der Version \"Pro Tools HD\" ermöglicht es Abmischungen bis zu 7.1 Surround Sound. Das MIDI-Interface von Pro Tools verfügt über einen Noteneditor und bietet über ReWire die Implementierung einer beidseitig synchronisierten Sibelius-Partitur, Sample Libraries können so beispielsweise mit dem Kontakt 5 Player von Native Instruments direkt eingebunden werden. Mit der \"Avid Video Engine\" können Videos direkt mit der Tonspur synchronisiert werden.\n\nPrinzipiell ähnelt die Handhabe von Pro Tools – wie bei anderen Digital Audio Workstations auch – der Kombination von Mehrspurrekorder und Mischpult, wobei die Besonderheit in den zusätzlich an die Hand gegebenen Werkzeugen liegt, die sich so nur digital verwirklichen lassen. Entsprechend finden sich im Programm zwei Haupt-Ansichten: die des \"Edit\"-Fensters und die des \"Mix\"-Fensters. Neben standardmäßig beinhalteten Funktionen zum Aufnehmen von MIDI und Audio, zum Schneiden, Nachbearbeiten und Mischen kann das Programm modular um Plug-ins für EQ, Hall, Tonhöhenkorrektur, Sampling oder Ampmodeling erweitert werden, jedoch muss hier oft auf Fremdanbieter zurückgegriffen werden. Bedientechnische Eigenart ist u. a. das sogenannte \"Smart Tool\", was es ermöglicht, mit ein und demselben Werkzeug zu trimmen, zu markieren, zu verschieben und Fades zu setzen.\n\nPro Tools in der aktuellen Version 12.7 vom 21. Dezember 2016 ist in verschiedenen Versionen erhältlich:\n\n\nPro Tools unterstützt Sampleraten bis zu 192 kHz und Auflösungen von 16, 24 und 32 Bit. Standardmäßige Medienformate sind WAV-, AIFF-, MP3- und SDII-Audiodateien, sowie QuickTime-Videos. Weitere Dateiformate können über Plug-ins verarbeitet werden (siehe Abschnitt Pro Tools Plug-ins).\n\nVormalige Varianten waren:\n\n\nDarüber hinaus gab es folgende fünf Varianten, welche nur mit Hardware des Unternehmens Avid bzw. M-Audio (M-Powered (Essential)) benutzt werden konnten:\n\nPro Tools wurde ursprünglich von Peter Gotcher und Evan Brooks entwickelt, welche an der Universität Berkeley ein Studium der Elektrotechnik und Informatik absolvierten. Die Anfänge von Pro Tools liegen im Jahr 1984, als beide mit der Programmierung der Software \"Sound Designer\" begannen, deren Verwendungszweck darin lag, Samples für das Keyboard \"E-MU Emulator\" editieren zu können.\nIhre Software wuchs an Funktionalität und wurde nach einiger Zeit entsprechend in \"Sound Tools\" umbenannt. Als deren Integration in das 1987 veröffentlichte \"Emulator III\"-Keyboard jedoch von \"E-MU\" abgelehnt wurde, gründeten Gotcher und Brooks die Firma Digidesign, um ihr Programm eigenständig zu vertreiben. Der erste offizielle Release von \"Sound Tools\" wurde am 20. Januar 1989 auf der amerikanischen Musikmesse \"NAMM\" vorgestellt.\n\nDie erste Veröffentlichung von Pro Tools erfolgte 1991. Zu diesem Zeitpunkt kostete das Programm etwa 6000 $ (USD), wobei bis zu vier Spuren gleichzeitig bearbeitet und über die Hardware (NuBus-Steckkarte und Interface 442 I/O) ausgegeben werden konnten. 1995 wurde Digidesign von Avid aufgekauft, seine Produkte jedoch vorerst unter der Marke Digidesign weitervertrieben.\n\nPro Tools war zu Beginn eine reine Audiosoftware mit eigener Hardware und wurde schrittweise um MIDI-Funktionen erweitert. Alternative Programme wie Cubase, Logic und Digital Performer begannen dagegen als MIDI-Software, die um Audiofunktionen erweitert wurde. Daraus resultieren unterschiedliche Stärken und Schwächen im Bedienungskonzept. Pro Tools gilt aufgrund samplegenauer Bearbeitung und ebenso genauer Automation bis heute als vorbildlich für die Audiobearbeitung, jedoch bis Erscheinen der Version 8 als weniger praktisch für MIDI-Editing. Viele Musikproduzenten (u. a. \"Just Blaze, Firzt Play\") kombinierten Pro Tools deshalb mit Logic oder Digital Performer, zumal beide die Digidesign-TDM-Hardware direkt unterstützen.\n\nGroße Bedeutung hat Pro Tools in der Filmindustrie im Bereich Postproduktion erlangt und gilt dort neben Nuendo als Quasi-Standard für Tonschnitt und Sounddesign. Im Jahr 2004 erhielt der Hersteller Avid (damals noch unter dem Namen \"Digidesign\") einen Oscar (die Sonderauszeichnung \"Academy Award of Merit\") für sein Produkt Pro Tools.\n\nAb Version 9 wird Pro Tools erstmals auch als reiner Sequenzer vertrieben, nachdem das Produkt viele Jahre lediglich als hochintegrierte DAW erhältlich war und nur in Verbindung mit entsprechender Hardware erworben werden konnte.\n\nDie neueren HD-Systeme benutzen das Plug-in-Format AAX HD. Hierbei wird die Berechnung von TDM-Plug-ins, anders als bei den LE-Systemen nicht von der/den CPU(s) übernommen, sondern von DSPs die sich auf einer oder mehreren speziellen Erweiterungskarten des Computers befinden. Es gibt verschiedene Ausbaustufen des HD-Systems.\n\nIm Gegensatz zu den HDX bzw. HD Accel-Systemen bietet das HD Native-System keine Berechnung mittels DSP, die Nutzung von Pro Tools HD ist allerdings trotzdem möglich. Die HD Native-Umgebung ist als Thunderbolt-Audiointerface oder als PCIe-Karte mit zusätzlichen externen Interfaces (HD I/O, HD MADI, HD Omni, PRE), welche auch für die HDX-Systeme verwendet werden, verfügbar. HD Native-Systeme benutzen das AAX Native-Plug-in-Format; auch sie unterstützen das Vorläuferformat, hier RTAS. Bei nativen Systemen muss das Hostsystem auch alle Audioberechnungen durchführen, was eine nicht unerhebliche Systemlast mit sich bringt.\n\nDas HDX-System besitzt mindestens eine Pro Tools|HDX-Karte an welcher spezielle AVID-Audiointerfaces angeschlossen werden; auf ihnen befinden sich aber auch die DSPs zur Audioberechnung. Auf HDX-Karten basierte Systeme benutzen das Plug-in-Format AAX HD; unterstützen aber auch das ältere TDM-System. Sämtliche Prozesse zur Audioverarbeitung sowie alle Plug-in-Berechnungen finden auf den DSPs der HDX-Karten statt. Dadurch wird das Hostsystem nur durch die minimalen Leistungsanforderungen der grafischen Oberfläche belastet und die Latenz der Audiosignale bei Verwendung von Effekten (EQ,Dynamics etc.) signifikant reduziert.\n\nVersion Pro Tools 10 bietet auch Unterstützung für das alte HD Accel-System, dieses besteht aus einer HD Core sowie optional mehreren HD Accel-Karten sowie externen Audiointerfaces, welche an diese Karten angeschlossen werden. Diese Unterstützung wird jedoch AVID zufolge in Nachfolgeversionen nicht mehr vorhanden sein. Von Digidesign selbst war das System als HD 1 (ohne Accel-Karte), HD 2 (eine Accel-Karte) und HD 3 (zwei Accel-Karten) verfügbar. Weitere Accel-Karten konnten jedoch separat hinzugekauft werden; die begrenze Anzahl an PCI(e)-Steckplätzen machte jedoch meist ein externes PCI(e)-Extender-Gehäuse notwendig, dieses wurde von Digidesign als Magma-Chassis vertrieben.\n\n\"Pro Tools\" ermöglicht das modulare Einbinden von nativen, mit der Rechner-CPU berechneten Plug-ins zur Realisierung verschiedenster Audio-Effekte in Echtzeit. \"Pro Tools HD\" ermöglicht ausschließlich auf Pro Tools HD- und HDX-Systemen über die TDM-Schnittstelle das Einbinden von DSP-Plug-ins in Echtzeit. Die Rechenleistung für native Plug-ins wird bei HDX-Systemen ebenfalls auf die DSP ausgelagert, was jedoch zu Latenzen führen kann. Darüber hinaus werden AudioSuite-Plug-ins für dateibasierte, nicht in Echtzeit gerenderte Effekte bereitgestellt.\n\nZur digitalen Kommunikation mit den Plug-ins bedient man sich des eigenen Standards der Real Time Audio Suite, kurz \"RTAS\". Die proprietäre, von Digidesign entwickelte Plug-in-Schnittstelle ist im Gegensatz zu VST nicht Open Source. Mit Version 10 führte Avid das neue Plug-in-Format \"Avid Audio Extensions\", kurz AAX ein, das es ermöglicht, Pro Tools-Sessions, die auf einem HD-System erstellt wurden, ohne Klangverluste auf einem nativen System zu öffnen und umgekehrt. Das Programm erkennt automatisch, ob zur Berechnung des AAX-Plug-ins die DSP-Leistung eines HDX-Systems zur Verfügung steht, oder, ob auf die Rechenleistung der Host-CPU zugegriffen werden muss. Pro Tools unterstützt somit ausschließlich AAX-, RTAS- und TDM-Plug-ins mit RTAS-Entsprechungen in 64-bit Architektur.\n\nFür den Datentransfer zwischen Aufnahmegeräten und Pro Tools benötigt man spezifische Hardware, je nach System kann dies eine interne Soundkarte, ein externes Audio-Interface oder auch ein DAW-Mischpult sein. Diese werden je nach Version und Preiskategorie über Thunderbolt, FireWire, USB oder Ethernet angeschlossen. Neben einem AD-Wandler kann solche Hardware auch Vorverstärker für Mikrofon- und Kopfhörerbuchsen, sowie DSPs zur Auslagerung von Rechenprozessen für die Signalverarbeitung beinhalten.\n\nSeit November 2011 ist das Pro Tools-System offen und unterstützt somit Hardware von Fremdherstellern. Zuvor war ausschließlich Hardware von Digidesign und M-Audio (beides Marken von Avid) zugelassen, die neben dem iLok-USB-Key wie eine Art Dongle fungierten.\n\nDie Entwicklung von neuen Features und Implementierung bei der Hardware benötigte durch die proprietäre Hardware, mangels Wettbewerb, länger als bei Konkurrenzprodukten. Diesen Umstand nutzten Unternehmen, wie Solid State Logic aus, um eigene Interfaces mit offiziell nicht unterstützten Features zu entwickeln und zu vertreiben. Auf diese Weise konnte MADI mittels Pro Tools genutzt werden. Die Fremdhardware simulierte in Pro Tools vier virtuelle \"192 I/O\"-Interfaces von Digidesign. Auf diese Weise konnten bis zu 64 Ein- und Ausgänge über Lichtwellenleiter genutzt werden.\n\nFolgende Hardware-Controller (mit visuellen Elementen der Pro Tools-Oberfläche und sensitiven Schaltflächen, Schiebe- und Drehreglern, die real bedient und haptisch erfasst werden), finden als Bedienhilfe für Pro Tools Anwendung:\n\n\n\n\n\n"}
{"id": "1408041", "url": "https://de.wikipedia.org/wiki?curid=1408041", "title": "WinCC", "text": "WinCC\n\nWinCC (\"Windows Control Center\") ist ein PC-basiertes Prozessvisualisierungssystem des Unternehmens Siemens. Es wird als eigenständiges SCADA-System oder als Mensch-Maschine-Schnittstelle für Prozessleitsysteme wie SIMATIC PCS 7 eingesetzt. Ein erster Prototyp und grundlegende Konzepte wurden 1993 erstellt, ab 1996 wurde die Software im deutschsprachigen Raum breit vermarktet.\n\nDas Visualisierungssystem ist modular aufgebaut und ermöglicht die Überwachung und Steuerung technischer Prozesse von Maschinen und Anlagen. WinCC ist als Client-Server-System ausgeführt, das auf verschiedenen Versionen des Betriebssystems Microsoft Windows läuft. Mit WinCC sind sowohl einfache Einplatzanwendungen als auch komplexe Mehrplatzlösungen mit verteilten Clients und Servern realisierbar.\n\nWesentliche Produktmerkmale sind die frei projektierbare Bedienoberfläche zur Visualisierung und Bedienung von Maschinen und Anlagen, die Erfassung und Langzeitdatenhaltung von Messwerten, die Erfassung, Speicherung und Visualisierung von Alarmen und Meldungen sowie das Bereitstellen von Datenschnittstellen zu externen Systemen.\n\nDie Basissoftware ist grundsätzlich branchenneutral konzipiert und wird für verschiedene Industrieanwendungen als SCADA-System eingesetzt. Typische Einsatzbereiche sind die Fertigungstechnik im Maschinenbau, die Automatisierung von verfahrenstechnischen Prozessen und die Steuerung und Visualisierung von Prozessen in Logistik-Systemen. Durch offene Schnittstellen, verfügbare Software-Optionen und projektspezifische Implementierung kann die Software an branchenspezifische Anforderungen angepasst werden. Typische Einsatzbereiche in Branchen sind:\n\nZur Prozessführung werden für den Einsatzzweck individuell projektierbare Bedienoberflächen eingesetzt. Die Software stellt hierfür einen Grafikeditor und eine Auswahl von Standardobjekten zur Verfügung (z. B. Grafikobjekte, Buttons, Check- und Radio-Boxes und Slider, Eingabe- und Ausgabefeld, Textlisten oder konfigurierbare ActiveX Controls für Alarme, Kurven und Tabellen). Diese Standardobjekte können um anwenderspezifische Objekte oder ActiveX Controls erweitert werden.\n\nObjekte werden im Grafikeditor mit Prozesswerten und Befehlen verknüpft. Es werden z. B. Messwerte visualisiert oder Zustandsmeldungen von Ventilen angezeigt. Gleichzeitig ist es möglich, durch Benutzereingaben per Maus oder Tastatur diese Objekte zu steuern. Für die Zuweisung von Prozesswerten und Befehlen stellt die Software verschiedene Mechanismen zur Verfügung. Die einfachste Variante ist das direkte Verschalten von Prozessvariablen an ein Objekt zur Anzeige eines Wertes oder einer Statusinformation. Weitere Möglichkeiten sind das dialoggeführte Verschalten von Variablen über sogenannte „Wizards“ bis hin zur komplexen Szenarien, die über die integrierten Script-Sprachen C-Script und Visual Basic Scripting realisierbar sind. Vom Betriebssystem zur Verfügung gestellte oder andere externe Funktionen können dabei verwendet werden (Windows API). Über „globale“ Scripte und Aktionen können kontextunabhängige Funktionen wie zyklische Abfragen, getriggerte Ereignisse bis hin zu Script Libraries realisiert werden.\n\nTexte in der Bedienoberfläche können mehrsprachig und zur Laufzeit umschaltbar ausgeführt werden. Es werden bis zu 34 verschiedene Sprachen in einer Visualisierungslösung unterstützt.\n\nWinCC realisiert eine integrierte Benutzerverwaltung, mit der Zugriffsrechte sowohl bei der Projektierung als auch zur Laufzeit (z. B in der so genannten \"Runtime\"-Bedienoberfläche) kontrolliert werden können. Es können 128 Benutzergruppen mit je bis zu 128 einzelnen Benutzern angelegt und verwaltet werden. Dabei können 999 verschiedene Berechtigungsstufen definiert werden, die wiederum den Benutzern und/oder Benutzergruppen zuzuweisen sind.\n\nDie Software erfasst und speichert Prozessmeldungen und lokale Ereignisse in Archiven und stellt diese bei Bedarf gefiltert und sortiert zur Verfügung. Meldungen können direkt aus binären Informationen oder als Resultat einer Grenzwertüberschreitung bei Analogwerten gebildet werden. Die Archivierung, Protokollierung und die Meldestruktur sind frei parametrierbar.\n\nWerteverläufe können in Prozesswertarchiven gespeichert werden. Diese Archivierung wird in einer Microsoft SQL Server-Datenbank realisiert. Die Werte werden verlustfrei komprimiert in der Datenbank gespeichert. Die Darstellung in der Bedienoberfläche kann über integrierte Objekte wie das Trend Control realisiert werden. Der direkte Zugriff auf diese Daten über optionale Schnittstellen durch externe Anwendungen ist möglich.\n\nDas Protokollsystem ermöglicht den layoutgesteuerten Ausdruck der erfassten Daten. Es umfasst verschiedene Protokolltypen wie z. B. Alarmprotokolle, Bedienprotokolle oder Anwenderberichte. Die Berichte können als Datei abgelegt und über eine Vorschau am Bildschirm angezeigt werden.\n\nWinCC wurde in den Sprachen Englisch, Deutsch, Französisch, Spanisch, Italienisch, Chinesisch, Taiwanisch, Koreanisch und Japanisch veröffentlicht. Die Hauptabsatzmärkte der Software sind in Europa und Asien. Siemens nennt sich selbst in verschiedenen Druckschriften und Presseveröffentlichungen mit WinCC als Marktführer in Europa und weltweit als zweithäufigst eingesetztes HMI-System.\n\nWinCC wird von Siemens im Prozessleitsystem SIMATIC PCS 7 als Basis eingesetzt. SIMATIC PCS 7 ist ein System mit skalierbarer Architektur für mittlere bis große Anlagen (bis zu 100.000 Ein- bzw. Ausgabe-Datenpunkte), welches Engineering Tools, Massendatenverarbeitung, Alarm Management und Asset Management integriert.\n\nWinCC wird seit 1995 kontinuierlich weiterentwickelt. Die nachfolgende Tabelle bietet einen Überblick über veröffentlichte Versionen mit ihren jeweiligen Hauptmerkmalen. Dabei wurden Patches und Hotfixes nicht berücksichtigt. Im Jahr 2008 hat Siemens WinCC-Version 7.0 veröffentlicht.\n\nIm Juli 2010 wurde ein Computerwurm mit dem Namen Stuxnet entdeckt, der für Angriffe auf WinCC- und PCS 7-Systeme spezialisiert ist. Nach Angaben der Computersicherheitsunternehmens Symantec handelt es sich dabei um den ersten Wurm, der Industriesysteme nicht nur ausspionieren, sondern auch deren Funktionsweise manipulieren kann. Der Wurm nutzt dazu die in WinCC fest einprogrammierten Zugangsdaten für den Microsoft SQL Server und vier verschiedene, ungepatchte Sicherheitslücken in Windows aus.\n\nIm September 2010 erklärte der iranische Kommunikationsminister Resa Taghipur, dass im Iran rund 30.000 Computer von Stuxnet befallen seien, darunter auch Rechner des Kernkraftwerks Buschehr.\n\nDa dieser Wurm nur unter großem Aufwand zu programmieren war, wird von Fachleuten wie Jewgeni Kasperski und anderen angenommen, dass er nicht von Privatpersonen, sondern von staatlichen Organisationen stammt.\n\n"}
{"id": "1408667", "url": "https://de.wikipedia.org/wiki?curid=1408667", "title": "Salome (Software)", "text": "Salome (Software)\n\nSalome ist ein freies Computerprogramm mit dem man dreidimensionale Tätigkeiten im CAE-Bereich bearbeiten kann.\nEinsatzgebiet ist das Pre- und Postprocessing bei Numerischen Simulationen wie zum Beispiel der FEM.\n\nDie Software verfügt unter anderem über einen interaktiven Geometrie-Editor.\n\nFür den Datenaustausch verfügt Salome über Schnittstellen wie STEP (AP203/214 Schema), IGES (5.3) und BREP (einem internen Open-CASCADE-Format).\n\nDas Programm, dessen Quelltext unter der LGPL steht und von der französischen Regierung gefördert wird, läuft auf Linux- und Unix-Betriebssystemen. Eine Portierung nach Windows ist ebenfalls verfügbar. Salome stellt zusammen mit Code Aster eine Kernkomponente von CAE-Linux dar.\n\n"}
{"id": "1408796", "url": "https://de.wikipedia.org/wiki?curid=1408796", "title": "Open CASCADE Technology", "text": "Open CASCADE Technology\n\nOpen CASCADE Technology ist ein Software Development Kit (SDK) um 3D-Anwendungsprogramme im Bereich CAD, CAM, CAE zu entwickeln. Open CASCADE ist unter einer LGPL-ähnlichen Lizenz für Linux, Solaris und Windows erhältlich.\n\nAb der Version 6.7.0 wird die Software unter den Bedingungen der LGPL Version 2.1 veröffentlicht und ist somit kompatibel mit der GPL (Version 2 und spätere). Zuvor wurden die Versionen unter einer eigenen Lizenz veröffentlicht, welche nicht von der OSI anerkannt war und sich somit Probleme in der Nutzung durch andere Projekte ergaben.\n\nTechnische Unterstützung und Entwicklung für Open CASCADE werden vom Unternehmen OPEN CASCADE SAS geleistet, das Produkt wird von verschiedenen Kunden u. a. in der Luftfahrt- und Automobilindustrie eingesetzt.\n\nDas CAD-Programm GraphiteOne und das Pre-/Post-processing-Programm Salome basieren auf Open CASCADE. Open-Source-Projekte, die Open CASCADE-Techniken nutzen, sind beispielsweise HeeksCAD und FreeCAD.\n\nFür den Datenaustausch verfügt Open CASCADE über Schnittstellen wie STEP (AP203/214 Schema), IGES (5.3) und BRep (einem reinen Open-CASCADE-Format).\n\nDie Programmbibliothek hat einen Umfang von etwa 200 MB, dazu kommen weitere 200 MB Dokumentation.\n\nEs gibt auch die Möglichkeit, Open CASCADE mit der Programmiersprache Python (pythonOCC) zu nutzen.\n\n\n"}
{"id": "1409276", "url": "https://de.wikipedia.org/wiki?curid=1409276", "title": "OpenFOAM", "text": "OpenFOAM\n\nOpenFOAM \"(Open Source Field Operation and Manipulation)\" ist ein in C++ geschriebenes, numerisches, freies Simulationssoftwarepaket für kontinuumsmechanische Probleme. Das Hauptaugenmerk liegt auf dem Lösen von Strömungsproblemen (Numerische Strömungsmechanik (englisch) CFD).\n\nNeben vielen vordefinierten Standardlösern für Strömungsmechanik inklusive Mehrphasenströmungen existieren weitere Löser, unter anderem für folgende physikalische Problemstellungen:\nWeitere Löser können in der OpenFOAM-eigenen Syntax hinzugefügt werden.\n\nTurbulenz kann mittels RANS, Grobstruktur-Simulation oder direkter numerischer Simulation abgebildet werden.\n\nUm Systeme von partiellen Differentialgleichungen lösen zu können, sind in OpenFOAM folgende Diskretisierungsschemen als C++-Programmbibliothek implementiert:\n\nDas Vorbereiten einer Simulation (Preprocessing) erfolgt in OpenFOAM mit Hilfe beigelegter Tools auf der Kommandozeile ohne GUI. Es ist aber auch möglich, externe Programme zu benutzen und die erzeugten Netze anschließend in OpenFOAM zu importieren. Das Einstellen der Simulationsparameter erfolgt auch hier über die Kommandozeile.\n\nDie grafische Aufbereitung der Simulationsergebnisse (Postprocessing) geschieht standardmäßig mit Hilfe der Software ParaView. Alternativ können die Ergebnisse in Formate einiger weit verbreiteter, kommerzieller Visualisierungsprogramme wie etwa Tecplot360 oder FieldView exportiert werden.\n\nOpenFOAM wird in zunehmendem Maße an Universitäten in Lehrveranstaltungen verwendet.\n\nDie Entwicklung von FOAM \"Field Operation And Manipulation\" wurde um 1989 von Henry Weller am Imperial College London in der Forschungsgruppe von Professor A.D. Gosman begonnen. Doktoranden wie Hrvoje Jasak aus der Forschungsgruppe nutzten und erweiterten FOAM im Rahmen ihrer Doktorarbeit . Weller, Jasak und weitere Mitglieder aus der Forschungsgruppe wie Greenshields und Janssens gründeten 2000 die Firma Nabla Ltd. um FOAM kommerziell anzubieten. 2004 wurde die Firma aufgelöst, und die Entwickler Weller (OpenCFD Ltd.) und Jasak (Wikki Ltd.) arbeiteten fortan getrennt an dem nun unter der GNU General Public Licence stehenden \"OpenFOAM\" weiter.\n\nAm 15. August 2011 wurde die Übernahme der Entwicklerfirma OpenCFD Ltd. durch SGI bekanntgegeben. Zu gleicher Zeit wurde in Delaware, USA das aktienlose Unternehmen bzw. eine Nonprofit Organisation mit dem Namen OpenFOAM Foundation, mit den Direktoren Mark Barrenechea (CEO, SGI) und Henry Weller (Begründer von OpenFOAM), gegründet, mit dem Auftrag, die gegenwärtige und zukünftigen Versionen von OpenFOAM unter der GPL zu verbreiten. Etwas mehr als ein Jahr später, am 12. September 2012, wurde bekannt gegeben, dass die ESI Group die Firma OpenCFD Ltd., die OpenFOAM Foundation sowie sämtliche Markenrechte übernimmt. 2014 verließen Henry Weller und Chris Greenshields OpenCFD Ltd., um die nach UK verschobene OpenFOAM Foundation als Direktoren zu leiten und mit der Firma CFD Direct Schulungen und Softwareentwicklungen für OpenFOAM anzubieten.\n\n\n\n"}
{"id": "1411165", "url": "https://de.wikipedia.org/wiki?curid=1411165", "title": "Autorun", "text": "Autorun\n\nAutorun ist die Funktion von Windows, Datenspeicher automatisch zu durchsuchen und Dateien automatisch zu öffnen beziehungsweise auszuführen, sobald die Datenträger für das Betriebssystem verfügbar sind. Dadurch kann beispielsweise beim Einlegen einer CD oder DVD automatisch ein Installationsprogramm starten. Für Dateien auf Disketten und in Rechnernetzen war die Funktion zunächst gesperrt.\n\nWindows sucht im Stammverzeichnis des Datenspeichers nach eindeutigen Informationen über dessen zugedachten Verwendungszweck. Dazu kann eine bekannte Dateinamenserweiterung dienen oder eine Konfigurationsdatei mit dem Namen \"autorun.inf\". In einer solchen Datei wird ausdrücklich festgelegt, wie das System verfahren soll. Diese Funktion sollte nicht mit der \"Autoplay\" Funktion verwechselt werden, welche, unabhängig von der \"Autorun- Funktion\" lediglich beim Einlegen einer CD oder DVD deren Mediendateien automatisch abspielt.\n\nDer Autostartmodus birgt ein hohes Sicherheitsrisiko. Durch ihn können völlig unbemerkt Schadprogramme installiert werden, da statt der unten beschriebenen Konfigurationsdatei auch ausführbare Programme unkritisch vom System verarbeitet werden, auch wenn diese nicht als Textdateien vorliegen, sofern diese nur den Dateinamen \"autorun.inf\" tragen. Die Benutzerprivilegien des Systems liegen dabei noch über den Privilegien des Administrators. Dieses Szenario ist nicht auf USB-Sticks und andere beschreibbare mobile Datenspeicher beschränkt, die zum Datenaustausch mit verschiedensten Computern verbunden werden. Selbst scheinbar harmlose Audio-CDs haben sich im Fall des Sony-BMG-Kopierschutzskandals als gefährlich erwiesen.\n\nAutorun kann in der Windows-Registry abgeschaltet oder eingeschränkt werden. Außerdem kann Autorun durch Halten der Umschalttaste während des Verbindens mit dem Datenspeicher temporär verhindert werden. Die Standardkonfiguration bei der Installation des Betriebssystems erlaubt jedoch die Ausführung dieser Funktion.\n\nBei dieser Konfigurationsdatei handelt es sich um ein gewöhnliches Textformat, in welchem Windows Shellkommandos durch Zuweisungen von Parametern notiert werden. Der notwendige Aufbau einer „autorun.inf“-Datei sieht wie folgt aus (Beispiel):\n\nZeile 1 gibt an, dass dieses der auszuwertende Abschnitt der Autorun-Datei ist, Zeile 2 sagt aus, welche Programmdatei ausgeführt werden soll und Zeile 3 beschreibt, welches Icon für das sonst übliche Laufwerksicon benutzt werden soll. Werden keine Pfade angegeben, werden die Dateien im Hauptverzeichnis desselben Laufwerks erwartet. Es können auch relative Pfade verwendet werden. Die Option codice_1 generiert den Eintrag „Programm starten“ in der ersten Zeile des AutoPlay-Auswahlfensters.\n\nMit folgender Zeile kann dem Datenträger ein Name zugewiesen werden:\n\nAußerdem können zusätzliche Kontextmenüs (erscheinen üblicherweise durch Betätigen der rechten Maustaste) definiert werden. Dazu muss folgender Aufbau verwendet werden:\n\nJedes Kontextmenü muss eine eigene eindeutige Bezeichnung haben. Die erste Zeile legt fest, welcher Text im Kontextmenü angezeigt werden soll, die zweite Zeile enthält das zu startende Programm.\n\nlegt fest, welche Bezeichnung die Standard-Aktivität sein soll – die vollautomatisch oder durch einen Doppelklick auf das Laufwerksymbol gestartet werden soll.\n\ncodice_2\"Kontext-Standard\" bekommt offenbar Vorrang vor dem älteren open/shellexecute. Beide Varianten können gleichzeitig angegeben werden. Dann sollte jedoch die Beschriftung von codice_4\"Kontext-Standard\"codice_5 und codice_6 sowie die resultierende Wirkung gleich sein, um Nutzer nicht durch unterschiedliches Verhalten auf verschiedenen Systemen zu verwirren.\n\nWie bei anderen Menüs in Microsoft Windows auch lassen sich Hotkeys definieren. Dabei wird dem gewünschten Kennbuchstaben ein codice_7 vorangestellt. Bei der Auswahlanzeige wird dieser Buchstabe dann unterstrichen dargestellt und das Menüfeld kann durch Eingabe dieses Buchstabens auf der Tastatur ausgewählt und das Programm unmittelbar gestartet werden.\n\nsorgt dafür, dass beim Drücken von der erste Eintrag und bei Drücken des der zweite Befehl gestartet wird. Mehrere Befehle mit gleichem HotKey in diesem Kontextmenü sollten vermieden werden; das Verhalten ist dann nicht vorhersagbar und es könnte zu unerwünschter Programmausführung kommen.\n\nOft sollen HTML-Dateien angezeigt werden – jedoch mit dem Standardbrowser des momentanen Benutzers und nicht zwangsläufig mit codice_8 (Microsoft Internet Explorer). Auch Audio/Video-Dateien sollen mit dem Standardplayer des gerade benutzten PC abgespielt werden; das ist nicht immer der Windows Media Player – der bei Windows XP codice_9 hieß und inzwischen als codice_10 anzugeben wäre.\n\nHinweis: codice_13 erhält Vorrang vor codice_14 –\nes sollte also nur eines angegeben werden.\n\nDas codice_15 im Kontextmenü ist wie auch codice_13 immer auf eine ausführbare Datei (EXE, CMD, BAT) angewiesen. Um das zu erreichen, gibt es folgende Möglichkeiten:\n\nEs lassen sich nicht nur Wechselmedien, sondern auch Festplatten mit autorun.inf ausstatten. Damit wären Icons auf dem „Arbeitsplatz“ darzustellen, also beispielsweise ein auffallendes Symbol für die Systempartition codice_19 und andere dagegen zur Charakterisierung von Datenpartitionen sowie freigegebener Ordner im Netzwerk.\n\nDiese erscheinen auch, wenn Ordner zur Abspeicherung angeboten werden, erleichtern damit die schnelle Orientierung und vermeiden eine Auswahl des falschen Ordners.\n\nAuch Aktionen (Doppelklick und Kontextmenü im Arbeitsplatz) können genauso definiert werden; der codice_20 gewinnt meist die Oberhand über Partitions- und Freigabenamen.\n\nÄnderungen werden wirksam, wenn die Verbindung erfolgt bzw. der PC gebootet wird.\n\nBei Disketten wird autorun.inf allerdings nicht „von selbst“ Aktionen ausführen können.\n\nLetzten Endes sind die autorun.inf (wie auch die desktop.ini) immer nur Hinweise \"(hints)\" für das Betriebssystem. Ob eine bestimmte Windows-Version dem folgen kann oder ob diese Funktionalitäten bewusst – beispielsweise aus Sicherheitsgründen – abgeschaltet sind, entscheidet sich im konkreten Einzelfall.\n\nDie AutoRun-Datei ist im Klartext für andere Betriebssysteme (wie Linux oder Mac) lesbar. Mittlerweile ist die Erkennung der Microsoft-autorun.inf oft schon integriert. Zumindest sind Werkzeuge vorhanden, die auf das Einlegen eines Mediums hin nach einer autorun.inf-Datei suchen, ihren Abschnitt [AutoRun] lesen und je nach Wunsch reagieren; entsprechende Skripte können leicht neu geschrieben werden.\n\nDazu eignen sich insbesondere die neutralen Einträge\nMSWindows-Icon-Dateien haben ein altbekanntes, einfaches Format, das andere Betriebssysteme meist problemlos anzeigen oder vorübergehend geeignet konvertieren können. Hier ist eine einzelne .ICO-Datei zu verwenden, kein Abschnitt aus einer .EXE oder .DLL.\n\nAuf codice_14 hin wäre, sofern er bekannt ist, der entsprechende Dateityp zu öffnen; Dokumentenformate wie HTML, TXT, PDF, Multimedia, DOC oder DOCX sind hier aussichtsreich.\n\nDiese Dateien sollten sicherheitshalber im Hauptverzeichnis des Mediums stehen, da ein fremdes Betriebssystem Schwierigkeiten mit dem Pfadtrennzeichen codice_25 haben könnte (es wird codice_26 erwartet).\n\nEs ist jedoch ohnehin empfehlenswert, dass im Hauptverzeichnis stets ein README/LIESMICH, index.html oder Ähnliches vorhanden ist, wenn der Datenträger in andere Hände gegeben wird – so dass das Medium leicht „von Hand“ gelesen werden kann, wenn Probleme mit Auto-Optionen auftreten oder unbekannte Automatismen unerwünscht sind.\n\n\n"}
{"id": "1412922", "url": "https://de.wikipedia.org/wiki?curid=1412922", "title": "Macromedia HomeSite", "text": "Macromedia HomeSite\n\nHomeSite ist ein HTML-Editor. Er bietet viele Hilfsfunktionen („Wizards“), um Standard-Aufgaben lösen zu können.\n\nHomeSite wurde 1995 ursprünglich von Nick Bradbury entwickelt und im März 1997 von der Allaire Corp. übernommen. 2001 wurde Allaire wiederum von Macromedia und Macromedia 2006 von Adobe übernommen. Am 26. Mai 2009 gab Adobe bekannt, dass die Entwicklung und der Vertrieb von HomeSite eingestellt wird; als Upgrade-Pfad wird Adobe Dreamweaver empfohlen.\n\nDen Typ eines Dokumentes erkennt HomeSite am codice_1-Eintrag. Ist der Typ einmal erkannt, bietet es die entsprechenden Befehle. Zu den unterstützten Formaten gehören unter anderem XHTML, JavaScript, VBScript, Perl, Active X, ColdFusion, VTML, Java, JSP, WML, ASP, CSS, PHP, SQL, DHTML und mehr.\n\nEr unterstützt ebenfalls \"Code-Completion\" und \"Insight-Tags\". Mittels Code-Completion werden zu öffnende Tags automatisch wieder geschlossen. Die Insight-Tags zeigen dem Programmierer die möglichen Attribute eines Tags, die er auswählen kann.\n\nZur einfachen Bearbeitung von CSS-Dateien ist das Programm TopStyle-Lite mit im Paket enthalten.\n\nIst nur als englischsprachige Grundversion verfügbar, deutsches Sprachpaket jedoch vorhanden.\n\n\n"}
{"id": "1413207", "url": "https://de.wikipedia.org/wiki?curid=1413207", "title": "Num-Taste", "text": "Num-Taste\n\nDie Num-Taste oder Num-Lock-Taste ist eine Taste auf einer Computertastatur. Sie befindet sich links oben am Ziffernblock. Sie wurde speziell für den IBM PC eingeführt und existiert daher nicht auf jeder Tastatur.\n\nDie Num-Lock-Taste ist eine Feststelltaste ähnlich der Rollen-Taste oder der Umschaltsperre. Ihr Zustand wird meist durch ein kleines LED-Lämpchen dargestellt, bei manchen Notebooks auch auf einem LCD. Wenn sie eingeschaltet ist, kann der Ziffernblock zur Eingabe von Ziffern verwendet werden; ist sie ausgeschaltet, kann der Cursor mit dem Ziffernblock gesteuert werden.\nAuf schmaleren Tastaturen (wie bei kleinen Laptops üblich) ist häufig kein Platz für einen Ziffernblock. In diesem Fall schaltet die Num-Lock-Taste einige Buchstaben-Tasten um, so dass sie (zur schnellen Zahleneingabe) als Ziffernblock genutzt werden können. Die alternativ aktiven Zeichen sind meist auf den Tasten in einer anderen Farbe markiert (siehe Bild). \n\nMit der Tastenkombination \"Linke Umschalttaste + Linke Alt-Taste + Num-Lock\" kann unter Windows und in einigen anderen Betriebssystemen die Tastaturmaus an- oder ausgeschaltet werden.\n\nDie ursprüngliche Tastatur des IBM PC hatte lediglich 83 Tasten. Daher hatte sie keinen separaten Cursorblock, und stattdessen konnte der Ziffernblock wahlweise zur Zahleneingabe (Num-Lock ein) oder als Cursorblock verwendet werden (Num-Lock aus); dabei werden aus den Tasten , , und beispielsweise die vier Richtungstasten, und aus wird die Ende-Taste usw.\n\nSpätere Tastaturen hatten einen separaten Cursorblock, doch statt nun die Num-Lock-Taste einzusparen, wie es logisch gewesen wäre, wurde sie beibehalten, und nun konnte man aus dem Ziffernblock einen zweiten Cursorblock machen.\n\nHäufig wird die Num-Taste als ein Anachronismus wahrgenommen – PC-Tastaturen ohne den separaten Cursorblock, die sie benötigten, waren nur fünf Jahre lang der Standard, während seitdem bis heute über 20 Jahre vergangen sind.\n\nDer „normale“ Zustand einer Feststelltaste wird gewöhnlich durch ein ausgeschaltetes Lämpchen signalisiert, wie bei der Umschaltsperre oder der Rollen-Taste, bei Num-Lock ist es aber seit 1986 umgekehrt, da dort (zumindest auf Desktop-Computern) der eingeschaltete Zustand der Normalzustand ist. Manche Windows-Computer starten ohne Num-Lock, das heißt mit ausgeschaltetem Lämpchen, bei einigen kann der Zustand im BIOS konfiguriert werden, um dann beim Start des Betriebssystems von diesem wieder geändert zu werden. Neuere Windows-Versionen merken sich beim Herunterfahren den derzeitigen Zustand der Taste und stellen diesen beim nächsten Systemstart wieder her.\n\nEin falscher Zustand führt schnell zu Verwirrung und Fehleingaben. Gerade auf Notebooks muss man auf den Status der Num-Lock-Taste achten, denn die Funktion wurde exakt übernommen: aktiver Num-Lock liefert Zahlen, inaktiver Num-Lock liefert die Buchstaben. Dies ist ein Problem bei gelegentlicher Nutzung einer vollen Tastatur an diesen Mobilgeräten. Ohne externe Tastatur muss Num-Lock AUS sein, um die Buchstaben schreiben zu können; mit externer Tastatur möchte man die Zahlen des Ziffernblocks nutzen und benötigt daher Num-Lock AN.\n"}
{"id": "1414323", "url": "https://de.wikipedia.org/wiki?curid=1414323", "title": "Google Desktop", "text": "Google Desktop\n\nGoogle Desktop war ein Suchprogramm für den Desktop. Es wurde vom Unternehmen Google Inc. entwickelt und ermöglichte die Suche nach Dateien, E-Mails, Chatlogs etc. auf dem eigenen Computer. Für Suchen bot es eine der hauseigenen Suchmaschine Google ähnliche Oberfläche und einige Erweiterungen für die \"Windows-Taskleiste\" und den Desktop an.\n\nDie Entwicklung am Google Desktop sowie der Support und die API wurden am 14. September 2011 eingestellt.\n\n\nund zahlreiche andere (teilweise durch Plugins erweiterbar). Dabei ist zu beachten, dass bei den meisten Datenquellen nur die ersten 10.000 Zeichen durchsucht werden. Maximal wird die Erstindizierung 100.000 Dateien pro Laufwerk aufnehmen. Zusätzliche Dateien werden nur durch Echtzeitindizierung beim Öffnen und Kopieren in den Index aufgenommen.\n\nDatenschützer warnen vor der Suchfunktion über mehrere Rechner, die schon Version 3 von Google Desktop anbot. Bei dieser Funktion wird der Suchindex über die Dokumente, die sich auf dem eigenen Rechner befinden, auf zentralen Servern von Google gespeichert. Damit könnten z. B. Ermittlungsbehörden oder Regierungen leicht Zugriff auf private Dokumente bekommen. Für Dateien auf einem Server gelten in den USA geringere Datenschutzbestimmungen als für den Zugriff auf einen PC.\n\nAuch wenn diese Funktionen ausgeschaltet sind, sendet Google Desktop eine eindeutige ID an Google, mit der sich theoretisch alle Daten, wie getätigte Suchen, besuchte Websites (über Google Analytics) und mehr, miteinander verknüpfen lassen und umfangreiche Nutzungsprofile angelegt werden können.\n\nSiehe Liste von Desktop-Suchprogrammen\n\n\n"}
{"id": "1414688", "url": "https://de.wikipedia.org/wiki?curid=1414688", "title": "Spotlight (Software)", "text": "Spotlight (Software)\n\nSpotlight ist eine von Apple entwickelte Desktopsuche für macOS. Sie ist darauf ausgelegt, möglichst schnell Dateien des Benutzers zu finden, unter anderem Dokumente, Bilder, Musik, Programme, Kontakte, Mails, und viele weitere Dateien.\nSpotlight basiert auf Indizes, die es für alle verfügbaren Datenträger anlegt und transparent aktualisiert. Jede Datei wird zusammen mit ihren Metadaten indiziert. Sobald der Benutzer eine Suche anfängt, werden die Indizes der Datenträger auf passende Ergebnisse hin durchsucht (Vorschlagssuche).\n\nSpotlight wurde auf der WWDC 2004 vorgestellt und im April 2005 zusammen mit Mac OS X Tiger ausgeliefert.\n\nEine erneuerte Version von Spotlight wurde auf der WWDC 2014 als Teil von OS X Yosemite vorgestellt.\n\nApple bietet zwei APIs für Spotlight, eine zum Suchen von Dateien (SearchKit) und eine zum Erstellen von Spotlight-Import-Modulen (auf verschiedene Cocoa-Frameworks verteilt).\n\nVon Mac OS 8.5 bis Mac OS X Panther diente ein Programm namens Sherlock zum Suchen von Dateien. Der Such-Code war aus dem Programm AppleSearch übernommen worden, das Apple Anfang der 1990er entwickelt hatte.\n\nIm März 2002 stellte Apple den Entwickler und Dateisystem-Designer Dominic Giampaolo ein, um Spotlight zu entwickeln. Auf der Keynote der WWDC 2004 wurde Spotlight zum ersten Mal der Öffentlichkeit vorgestellt.\nSherlock wurde nicht weiter unterstützt; Stand Ende 2010 waren fast alle der Dienste von Sherlock nicht mehr verfügbar.\n\nSpotlight ist in zwei Komponenten aufgeteilt: zum einen das Backend mit dem Metadaten-Server und zum anderen alle Clients (Programme, die auf den Index zugreifen).\n\nDas Spotlight-Backend besteht aus dem Daemon mds (Metadata Server, wird mit dem System gestartet) und mdworker (wird beim Anmelden gestartet; eine Instanz pro Benutzer).\nDer Metadaten-Server wird aktiv, wenn er von Clients angesprochen wird, oder wenn Dateien angelegt oder verändert werden.\nInformationen über die Dateien auf angeschlossenen Festplatten und USB-Medien (CDs/DVDs werden nicht indiziert, da auf den Datenträgern kein Index angelegt werden kann) erhält der Metadaten-Server über den Daemon mdimport. Dieser versucht, für den Typ der Datei einen Importer zu bekommen. Der Importer muss dann die Datei auf ihre Metadaten hin untersuchen, und gibt dann eine Sammlung von Metadaten für die Datei an den Server zurück, der diese Informationen im Index abspeichert\n\nBeim ersten Start des Betriebssystems werden alle Dateien einmalig indiziert. Das dauert einige Zeit, danach ist Spotlight einsatzbereit, und alle Änderungen am Index geschehen von da an inkrementell und im Hintergrund.\nWenn das System feststellt, dass der Datenträger von einem System unter Mac OS X 10.4 oder einem Nicht-Mac-System verändert wurde, wird der Index verworfen und neu aufgebaut.\n\nIn der oberen rechten Ecke befindet sich ein Icon mit einer Lupe. Ein Klick darauf öffnet das Spotlight-Menü. Es kann auch durch einen Tastatur-Shortcut geöffnet werden, standardmäßig ist das +.\nIm Spotlight-Menü kann der Benutzer eine Suchanfrage eingeben. Spotlight versucht daraufhin, Dateien zu finden, die am besten zu der Anfrage passen. Die Reihenfolge der Ergebnisse kann in den Systemeinstellungen festgelegt werden.\nDabei gibt Spotlight nur Dateien zurück, für die der Benutzer eine Leseberechtigung hat.\n\nDie Suche nach Dateien ist auch über den Finder zu erreichen. Dort kann nach Dateiname oder Inhalt gesucht werden und der Nutzer kann festlegen, ob nur im aktuellen Ordner oder auf der ganzen Festplatte gesucht werden soll.\nWeiterhin können zusätzliche Bedingungen für die Suche festgelegt werden (z. B. „zuletzt geöffnet: gestern“ oder „Dateityp: Bild“).\nJede Spotlight-Suche kann auch als sogenannte „intelligente Suche“ (\"Smart Search\") gespeichert werden; dabei wird die Abfrage mit allen Parametern gespeichert, nicht jedoch die Ergebnisse. Diese gespeicherten Suchen können später einfach wieder aufgerufen werden; sie liefern dann das jeweils aktuelle Ergebnis zum Zeitpunkt des Aufrufs.\n\nStandardmäßig werden Spotlight-Anfragen mit mehreren Begriffen so behandelt, als sei zwischen jeden Begriff der boolesche Operator UND gestellt.\nBei Verwendung des API kann auch eine Anfrage in Textform gestellt werden, dabei können auch Vergleichsoperatoren aus C verwendet werden (codice_1) sowie logische Operatoren (codice_2) und Wildcards (codice_3).\nBei Text-Anfragen in der Benutzeroberfläche (per Spotlight-Menü) können nur die booleschen Operatoren codice_4 und codice_5 verwendet werden.\n\nApple bietet Entwicklern ein API für den Zugriff auf den Spotlight-Index (SearchKit). In Kombination mit eigenen Importern für bestimmte Dateitypen kann ein Entwickler damit auch komplexe Spotlight-Suchen in seinem Programm ermöglichen.\nEin Beispiel dafür ist etwa das Mail-Programm von Apple, das einen eigenen Spotlight-Importer besitzt (zum Auslesen der Metadaten in E-Mails) und es dem Benutzer ermöglicht, sein gesamtes Mail-Archiv nach ebendiesen Metadaten zu durchsuchen.\nEin weiteres Beispiel sind die Systemeinstellungen: wenn der Benutzer z. B. nach „Tasten“ sucht, werden als Ergebnisse u. a. die Maus- oder Tastatureinstellungen angeboten.\n\nLokal installierte Programme, die Spotlight findet, können direkt aus der Ergebnisliste heraus gestartet werden.\n\nJeder Benutzer kann in den Systemeinstellungen festlegen, ob es Ordner oder Datenträger gibt, die Spotlight nicht durchsuchen soll; ebenso kann jeder Nutzer festlegen, welche Kategorien von Dateien er im Spotlight-Menü sehen möchte, und in welcher Reihenfolge sie auftauchen sollen.\n\nApple liefert mit Mac OS X vier Kommandozeilen-Tools für Spotlight mit, die auch unter Darwin verwendet werden können:\n\n\nMit Mac OS X Leopard kamen einige Neuerungen in Spotlight hinzu.\n\nWenn ein anderer Mac Dateien im Netzwerk freigibt, können diese per Spotlight durchsucht werden.\nBoolesche Operatoren werden jetzt offiziell unterstützt, zudem können mit Klammern verschachtelte Suchanfragen erstellt werden.\nMit Apple Safari besuchte Webseiten werden ebenfalls indiziert; die URL, die Metadaten und der Inhalt der Webseite werden dabei gleichermaßen indiziert.\nEs ist möglich, im Spotlight-Menü Berechnungen durchzuführen oder Wörter im integrierten Oxford-Wörterbuch nachzuschlagen; dazu müssen der Rechenterm oder das Wort in das Suchfeld eingegeben werden.\n\nIn Mac OS X Lion wurde das Spotlight-Menü erweitert, nicht jedoch Spotlight an sich.\n\nMan kann aus dem Spotlight-Menü heraus direkt eine Suchanfrage im Internet oder in der Wikipedia starten, zudem kann für viele Dateien (z. B. Texte, Bilder, PDFs, aber auch Wörterbuch-Ergebnisse) direkt eine Vorschau angezeigt werden.\n\nEine erneuerte Version von Spotlight wurde auf der WWDC 2014 als Teil von OS X Yosemite vorgestellt. Die Vorschlagssuche beinhaltet jetzt Wikipedia-Zugriff und Kartenanzeige. Die Funktion ist jedoch in vielen Regionen außerhalb Westeuropa und angelsächsischen Ländern nicht verfügbar.\n\nNeu hinzugekommen war in El Capitan die Möglichkeit, Suchanfragen an Spotlight in gewissen Grenzen in natürlicher Sprache zu formulieren. Beispielsweise kann Spotlight nach „Emails von Peter“ oder nach „Emails gestern“ suchen und in den Suchergebnissen die anderen Dateitypen und die zu anderen Zeitpunkten bearbeiteten Dateien ausfiltern. Auch die Mac App Store kann durchsucht werden. Auch Umrechnungen von Währungen oder von Temperaturwerten sind in Spotlight möglich.\n\nSeit iOS 3.0 (vormals: iPhone OS) besitzt das System ebenfalls eine Suche, die Spotlight genannt wird. Sie war bis iOS 6 links des ersten Homescreens zu finden. Seit der Einführung des Betriebssystems iOS 7 im Jahr 2013 ist die Spotlight-Suche durch eine Wischgeste von der Mitte eines beliebigen Screens aus nach unten zu aktivieren.\n\nDer Suchumfang ist drastisch reduziert, es können im Grunde nur die Liste der installierten Apps sowie Daten aus den mitgelieferten Apps durchsucht werden (Apps, Kontakte, Nachrichten, Musik, etc.), allerdings können aus dem Spotlight-Bildschirm heraus ebenfalls Internet-Suchanfragen und Wikipedia-Suchen gestartet werden.\nDie Suche in einigen integrierten Apps (z. B. Nachrichten oder Mail) basiert ebenfalls auf Spotlight.\n"}
{"id": "1417309", "url": "https://de.wikipedia.org/wiki?curid=1417309", "title": "TV-Browser", "text": "TV-Browser\n\nTV-Browser ist ein freier plattformunabhängiger elektronischer Programmführer (EPG). Die Software ist in Java geschrieben, wird seit dem Jahr 2002 entwickelt und hat nach eigenen Angaben über 80.000 aktive Nutzer. Es sind die Programme von mehr als 1000 deutschen und ausländischen Fernseh-, mehr als 100 Hörfunksendern sowie von einigen Kinos verfügbar .\n\nÜber Schnittstellen und Plug-ins können andere Dienste eingebunden werden, um weitere Programmdaten darzustellen. Die Programmdaten werden von den jeweiligen Sendern/Kinos geliefert, eine redaktionelle Nachbearbeitung findet oftmals nicht statt. Ebenso sind durch Plug-ins viele weitere Funktionen geschaffen. Beispielsweise können Lieblingssendungen durch Stichworte („John Wayne“) gefunden werden; diese werden dann farblich hervorgehoben und durch Erinnerungsfenster angezeigt.\n\nDie Version für Android ist bei Google Play nicht mehr verfügbar. Ein Download bei GitHub ist weiter möglich.\n\nDie Entstehungsgeschichte von TV-Browser begann im Jahr 2002, als Martin Oberhauser eine Erweiterung für einen kommerziellen EPG schrieb, damit dieser auf seinem Mac laufen konnte, und ihn dem Betreiber anbot. Das Unternehmen lehnte jedoch ab, sodass Oberhauser beschloss, sein Programm auf Open-Source-Basis anzubieten und weiterzuentwickeln. Zunächst gab es jedoch keine Programmdaten von Sendern für diese Software. Durch die Entwicklung einer Schnittstelle, die es jedem ermöglicht, Programmdaten in TV-Browser einzubinden, wurde das Anbieten vereinfacht, sodass die Zahl der Sender rasch wuchs und die Beliebtheit des Programms stieg.\n\nHohe Wellen schlug es dementsprechend auch, als ProSiebenSat.1 Media im April 2004 ihre Geschäftsbedingungen dahin gehend änderte, dass TV-Browser die Daten der Sender nicht mehr anbieten konnte. Die Nutzerzahl des Programms sank dadurch deutlich. Im Juli 2004 gestattete das Unternehmen die Nutzung der Daten jedoch wieder. Zwei Monate darauf wurde die Version \"1.0\" von TV-Browser veröffentlicht. Mit der Veröffentlichung der Version \"2.2\" im Mai 2006 versuchten die Entwickler, insbesondere Benutzerfreundlichkeit und Optik zu verbessern. Ab der Version 2.5.3 werden Bilder zu den Sendungen sowie der Export nach Outlook, Apple Kalender und Google Kalender unterstützt.\n\nSeit dem 1. Januar 2008 konnte TV-Browser die EPG-Daten von 16 Sendern nicht mehr anzeigen, da die VG Media eine Gebühr für die Nutzung der Daten in elektronischen Programmführern erhoben hat. Betroffen sind Sender wie ProSieben, RTL II, RTL Television und Sat.1. Es wurde versucht, diese Einschränkungen juristisch anzugreifen. Darüber hinaus konnten Benutzer des TV-Browsers eine Petition gegen diese Auflagen unterschreiben. Des Weiteren sind mehrere Plug-ins entstanden, die kostenpflichtig diese Daten aus anderen Quellen bereitstellen.\n\nSeit dem 20. Februar 2008 ist klar, dass die Anzeige von Titel und Uhrzeit des Fernsehprogramms von der VG Media derzeit nicht mit Gebühren belegt wird. Programmbeschreibende Texte sind davon jedoch ausgenommen. Die Programmbeschreibungen werden inzwischen von der \"Open Media Database\" übernommen und können auch von Community-Mitgliedern beigesteuert werden.\n\nDas Programm existiert auch als portable Version.\n\n"}
{"id": "1418622", "url": "https://de.wikipedia.org/wiki?curid=1418622", "title": "TUSTEP", "text": "TUSTEP\n\nTUSTEP ist die Abkürzung für \"TUebinger System von TExtverarbeitungs-Programmen\"; ein seit 1978 unter diesem Namen vertriebenes und seitdem ständig weiterentwickeltes Programmpaket des Zentrums für Datenverarbeitung der Universität Tübingen.\n\nZweck des Programmsystems ist die wissenschaftliche Textdatenverarbeitung innerhalb der Geisteswissenschaften. Spezifische Aufgaben, für die TUSTEP verwendet werden kann, sind etwa:\n\nAls besonders leistungsfähiges Element des Programmsystems lassen sich u. a. seine komplexen Suchen-und-ersetzen-Routinen nennen. Die Suchmöglichkeiten sind auf philologische Problemstellungen ausgerichtet, aber auch korpuslinguistische Projekte lassen sich in TUSTEP durchführen (z. B. programmgestützte Annotation, verlustfreie Umwandlung von Datenformaten).\n\nTUSTEP zeichnet sich durch Flexibilität (modularer Aufbau) und seine hohe Verarbeitungsgeschwindigkeit aus. Besonders geeignet ist es für die Verarbeitung XML-strukturierter Daten bzw. Erzeugung XML-strukturierter Daten aus ursprünglich anders strukturierten Daten. Die Bedienung kann auf Skriptebene erfolgen. Eine grafische Benutzeroberfläche existiert in der Commandline-Umgebung des jeweiligen Systems und ist hochgradig konfigurierbar. Zahlreiche renommierte Großprojekte (unter anderem erfolgte die Digitalisierung des Grimmschen Wörterbuchs mit TUSTEP), aber auch Einzelprojekte sind auf der Programmhomepage verlinkt.\n\nDer Aus- und Weiterbildung von TUSTEP-Anwendern sowie der Förderung des Informationsaustausches innerhalb der TUSTEP-Community widmet sich die \"International Tustep User Group e.V.\" Zu ihren Aufgaben gehört außerdem die Wartung, Portierung, Weiterentwicklung und Verbreitung von TUSTEP. \n\nDas Programmsystem ist in Fortran und C geschrieben und steht für die Betriebssysteme Windows, Linux und Mac OS zur Verfügung. \n\nTUSTEP gibt es 1.: als 'klassisches' TUSTEP, eine zeilenbasierte, parametergestützte Programmiersprache; 2.: als TUSCRIPT, eine moderne, hochentwickelte Skriptsprache; 3.: als TXSTEP, das neue XML-Frontend von TUSTEP.\n\n\n"}
{"id": "1421871", "url": "https://de.wikipedia.org/wiki?curid=1421871", "title": "Home Planet", "text": "Home Planet\n\nHome Planet ist ein freies Astronomieprogramm, das unter Microsoft Windows 9x, NT, 2000, XP läuft.\n\nDas Programm erlaubt den Blick auf die Erdkarte, Teleskop-Ansicht, Horizont-Ansicht, eine Ansicht des Sonnensystems und zeigt Planeten, über 5000 Asteroiden und periodische Kometen, die Fixsterne des \"Yale Bright-Star-Katalogs\" oder des \"SAO-Katalogs\", sowie alle \"Messier\"– und die meisten \"NGC\"-Objekte und etwa 1000 Satelliten. Weiterhin verfügt es über eine Teleskop-Schnittstelle und eine Kuckucksuhr. Programmiert wurde Home Planet von John Walker, dem Gründer von Autodesk. Der Name bedeutet übersetzt „Heimatplanet“, er ist im Englischen eine Bezeichnung für die Erde (den Heimatplaneten der Menschheit).\n\nDas Programm ist – nach heutigen Begriffen – sehr klein (Version 3.1 mit der einfachen Erdkarte 19,5 MB, Version 3.2 mit der \"Blue Marble: Cloudless Earth, high resolution [8192 × 4096], natural colour\" 47,3 MB) und schnell, verfügt aber über keine 3D-Grafik.\n\nDas Programm hat eine ganze Familie von Modulen als eigenständige Anwendungen hervorgebracht:\nInteraktive Server-Varianten:\n\n"}
{"id": "1423990", "url": "https://de.wikipedia.org/wiki?curid=1423990", "title": "Superminicomputer", "text": "Superminicomputer\n\nSuperminicomputer sind seit den späten 1970er-Jahren Nachfolger der industriell genutzten 16-Bit-Rechner geworden.\n\nSie wurden oft auch „32bit-Superminis“ genannt. Sie hatten ihre Hauptanwendung im technischen Bereich, im Einsatz zu CAD-, CAM-, FEM- und anderen CA-Techniken. Eine ganze Generation Ingenieure lernte sie an der Universität kennen und arbeitete mit ihnen ca. 15 Jahre lang, bis dann preisgünstigere PC-Netzwerke die Superminicomputer ablösten.\n\nDie Superminis dienten oft als Abteilungs- oder Gruppen-Rechner. Normale Konfigurationen erlaubten ca. zehn bis dreißig Anwendern, an alphanumerischen oder grafischen Terminals mit dem sternförmig verkabelten Rechner zu arbeiten. Gängige Aufgabenbereiche solcher Rechnersysteme waren die Einführung von CAD, die teilautomatisierte Erstellung von Stücklisten und Arbeitsplänen im Maschinenbau und in der Elektrotechnik, das Layouten von Elektronik-Platinen, das Simulieren und Rechnen von Strömungsvorgängen im Turbinen- und Flugzeugbau, oder das CNC-Programmieren von numerisch gesteuerten Werkzeugmaschinen.\n\nBekannteste Anbieter dieser Computer waren die beiden Firmen DEC und Pr1me. Auch Rechner von Data General („Eclipse“-Serie) sind als Superminis bekannt geworden. Häufigst eingesetzte Programmiersprache war zu jener Zeit noch Fortran in den Ausprägungen FORTRAN IV und FORTRAN 77. Als dann C als Programmiersprache Mitte der 1980er Jahre immer gängiger wurde, kam der Übergang zu Unix als Betriebssystem für Superminis. Später Anbieter von Unix-Superminis wurde dann Siemens-Nixdorf.\n"}
{"id": "1424620", "url": "https://de.wikipedia.org/wiki?curid=1424620", "title": "Macintosh XL", "text": "Macintosh XL\n\nDer Macintosh XL war ein technisch verbesserter Nachfolger des Apple Lisa. Er kam 1985 auf den Markt. Aufgerüstet wurde er durch\n\nDie Auflösung des Bildschirmes betrug zunächst 720 × 364 Pixel. Daher gab es keine quadratischen Pixel. Eine optionale Hardwareumrüstung erlaubte dem Macintosh XL die Anwendung von quadratischen Pixeln für eine bessere Darstellung von Mac-Software, wobei dadurch die Kompatibilität mit der Lisa-Software verlorenging. Nach der Umrüstung betrug die Bildschirmauflösung 608 × 431 Pixel.\n\nAusgeliefert wurde der Macintosh XL mit der Software MacWorks XL und einem Emulationsprogramm, um das Betriebssystem Lisa OS verwenden zu können.\n\nMacWorks XL ist ein Port des Macintosh-Plus-ROMs auf den Macintosh XL und ermöglicht so die Nutzung der Macintosh System Software ab System 1.1. Neuere Versionen als System 3.2 sind allerdings nicht damit kompatibel. Eine neuere Version mit dem Namen MacWorks Plus ermöglicht die Nutzung von System 6 bis Version 6.0.3. MacWorks Plus II Basic schließlich ist kompatibel bis System 6.0.8. Mit MacWorks Plus II Pro kann auf dem Macintosh XL sogar System 7 bis Version 7.5.5 ausgeführt werden.\n\nDie Produktion des Macintosh XL wurde 1986 eingestellt. Siehe auch den Artikel zur Geschichte der Apple Lisa.\n\nDie Namenserweiterung XL des Macintosh XL führt hinsichtlich der zeitlichen Einordnung dieses Computers oft zu Irrtümern. So ist der Macintosh XL kein technisch verbesserter oder erweiterter Macintosh als Nachfolger des Mac, sondern eben, als Nachfolger der Apple Lisa 2 umbenannt, eigentlich als der Urvater aller Computer mit dem Namen Macintosh anzusehen.\n\n\n\n"}
{"id": "1427718", "url": "https://de.wikipedia.org/wiki?curid=1427718", "title": "DAI (Computer)", "text": "DAI (Computer)\n\nDer DAI war ein Personal Computer, damals noch Heimcomputer genannt, des belgischen Herstellers \"Data Applications International\" aus dem Jahr 1980. Der DAI war bei Markteinführung ein sehr leistungsfähiger Rechner, der aber zu teuer angeboten wurde (im Dezember 1982 für 2880 DM), sodass er sich in der Verbreitung nicht durchsetzen konnte.\n\nDie Hardware des DAI-Computers basierte auf dem Mikroprozessor 8080 A und war als Einplatinencomputer ausgelegt. Das futuristische weiße Gehäuse enthielt eine für damalige Verhältnisse hochwertige Tastatur. Als Massenspeicher wurde ein Kassettenrekorder eingesetzt. Der DAI konnte an konventionelle Fernsehgeräte angeschlossen werden und hatte bereits eine Farbgrafik. Das dynamische RAM (max. 48 KiB) war unterteilt in drei separate Speicherbereiche (\"memory banks\"), die 0, 4 KiB oder 16 KiB Arbeitsspeicher enthalten. Ferner besaß der DAI fünf programmierbare Intervalltimer, zwei externe Interrupts und zwei serielle I/O-Interrupts. Als Schnittstellen waren RS-232 und eine parallele Schnittstelle (DCE-Bus von Data Applications) vorhanden. Das BASIC des Rechners arbeitete halbcompilierend. Das bedeutet, dass das BASIC-Programm gleich bei der Eingabe auf syntaktische Fehler überprüft und in einen Zwischencode übersetzt wurde, den der Interpreter dann deutlich schneller abarbeiten konnte.\n\nZusätzlich konnte bei Bedarf ein Mathematik-Chip AMD-9511 nachgerüstet werden.\n\nWeitere technische Daten:\n\n"}
{"id": "1432238", "url": "https://de.wikipedia.org/wiki?curid=1432238", "title": "Apple-Clone", "text": "Apple-Clone\n\nApple-Clones sind Kopien (sogenannte Klone) des Apple II Homecomputers des Herstellers Apple, der ab 1977 gebaut wurde.\n\nDer Original-Apple besaß kaum hoch integrierte Chips, die meisten Bausteine, selbst die Videoelektronik, waren Logikgatter der 74xx-Reihe. Nur der 6502-Mikroprozessor und die 4116-DRAM-Speicherchips waren für die damalige Zeit schon recht hoch integriert. Gleichzeitig waren diese beiden Bausteine aber auch gut beschaffbar, so dass viele Unternehmen sich anhand des über \"Reverse Engineering\" erhaltenen Schaltplans an Nachbauten wagten.\n\nDie Nachbauten hatten zum Teil die Erweiterungen, die man für den Original-Apple-II auf Steckkarten erwerben musste, bereits eingebaut. Bekannte Beispiele sind die 80-Zeichen-Erweiterung und die Z80-Erweiterung für den Betrieb von CP/M.\n\n\n\n"}
{"id": "1432431", "url": "https://de.wikipedia.org/wiki?curid=1432431", "title": "Adobe Premiere Elements", "text": "Adobe Premiere Elements\n\nPremiere Elements [] ist eine Software zur Videobearbeitung, die speziell für den Heimanwenderbereich zugeschnitten ist. Premiere Elements ist der kleinere Bruder von Adobe Premiere, von dem technisch viele Funktionen übernommen wurden. Premiere Elements hat jedoch eine andere Benutzeroberfläche, die die für Einsteiger interessanten Funktionen schnell auffindbar machen soll, ohne Fortgeschrittenen Benutzern zu wenig Funktionen zu bieten. Die Software ist für Windows XP, ab Version 3.0.2 auch für Windows Vista und Windows 7 verfügbar. Ab der Version 9 sind in der Verpackung zwei gesonderte DVDs für die Version Mac OS X enthalten.\n\nÄhnliche Programme im Segment sind z. B. Magix Video deluxe, Pinnacle Studio, Corel VideoStudio etc.\n\nZu den grundlegenden Funktionen gehören Videoschnitt (auch in HD), Audiobearbeitung, zahlreiche Videoeffekte, vielfältige Überblendungen, Effekte (z. B. für Farbkorrektur und Blue-/Greenscreen), die Möglichkeit zur Erstellung von DVD- oder Blu-Ray-Menüs (auch DVD-Authoring genannt) und andere nützliche Hilfsmittel wie die Erzeugung von Texten usw. Daneben kann die Software zahlreiche Videoformate wie zum Beispiel AVCHD, MPEG, MOV und AVI öffnen bzw. einlesen und auch in unterschiedlichen Videoformaten speichern.\n\nDie Oberfläche des Programms ist an den Bedürfnissen von Videoschnitt-Einsteigern orientiert und besteht aus andockbaren Fenstern. Die wichtigsten sind:\n\n\n\n\n"}
{"id": "1432485", "url": "https://de.wikipedia.org/wiki?curid=1432485", "title": "Corel VideoStudio", "text": "Corel VideoStudio\n\nCorel VideoStudio (vormals \"Ulead VideoStudio\") ist eine Software der Corel Corporation. Ursprünglich entwickelt von der taiwanischen Firma Ulead Systems, hat mit der Akquisition Uleads im Jahre 2006 die Corel Corporation die Software mit übernommen. Als Einsteigerprodukt für den Bereich der Videobearbeitung können vielfältige Aufgaben mit Hilfe von Assistenten erledigt werden. Zu den grundlegenden Funktionen gehören der Videoschnitt, der Einsatz von Überblendungen, Videoeffekte, Texterstellung, Audiountermalung etc. Die Software unterstützt dabei viele verschiedene Videoformate (auch in HD), wie MPEG oder AVI, aber auch Audio- und Bildformate. Darüber hinaus ermöglicht Corel VideoStudio die Erstellung von DVD-Menüs (DVD-Authoring). Hierfür werden zahlreiche Vorlagen mitgeliefert. In den neueren Versionen wird auch die Blu-ray- und 3D-Bearbeitung ermöglicht.\n\n\"Corel VideoStudio\" ist als Pro- und Ultimate-Version erhältlich. Die Ultimate-Version ergänzt dabei die Pro-Version um die Effektpakete\n\n"}
{"id": "1432575", "url": "https://de.wikipedia.org/wiki?curid=1432575", "title": "Pinnacle Studio", "text": "Pinnacle Studio\n\nPinnacle Studio [] ist eine Videoschnittsoftware des Unternehmens Pinnacle Systems. Pinnacle bietet die Software in drei Ausführungen an: „Studio“, „Studio Plus“ und „Studio Ultimate“. Im Juli 2012 übernahm Corel Pinnacle Systems. Ab der Version 16 basiert die „Pinnacle Studio“-Produktlinie auf Code von „Avid Studio“.\n\nSie wird hauptsächlich von Einsteigern und semiprofessionellen Nutzern verwendet und bietet dabei viele Funktionen. Mit der Software lassen sich Videos einspielen, schneiden (Videoschnitt) und ausspielen. Das Einspielen ist in verschiedenen Qualitätsstufen möglich (volle (H)DV-Qualität, MPEG-Qualität und Vorschauqualität). Im Schnitt-Modus kann man zwischen einer Storyboard-, einer Zeitstrahl- und einer Listenansicht wählen, abhängig vom entsprechenden Arbeitsschnitt. Es ist möglich, Überblendungen einzubauen, wozu es zahlreiche Effekte gibt. Weiterhin können Titel eingebaut werden, entweder als vollständiges Bild oder nur zum Einblenden von Schrift und Ähnlichem in einen Film. Diese Titel können auch benutzt werden, um einen Abspann hinzuzufügen. Videos können auch mit Musik untermalt werden, wobei die Software zahlreiche Formate unterstützt. Für solche Effekte gibt es drei Audiospuren, sodass sich auch nahtlose Tonübergänge schaffen lassen. Daneben können Videos mit zahlreichen Effekten versehen werden, wie z. B. einer Geschwindigkeitsänderung und einer schwarz-weiß-Sequenz. Außerdem bietet die Software auch die Möglichkeit zum DVD-Authoring und Blu-ray-Authoring, d. h. Filme können mit navigierbaren Menüs erstellt werden. Die Software besitzt die Möglichkeit, einen Film in mehreren Formaten auszuspielen: auf Band, als Datei (z. B. AVI, MPEG, für Webvideos auch in niedriger Qualität) oder auch direkt als Disk (u. a. (S)VCD, DVD, HD DVD, Blu-ray Disc und AVCHD).\n\n\n\n"}
{"id": "1436089", "url": "https://de.wikipedia.org/wiki?curid=1436089", "title": "Galileo (CRS)", "text": "Galileo (CRS)\n\nDas Computerreservierungssystem (CRS) Galileo ist ein Buchungsportal der Tourismuswirtschaft.\n\n1971 initiierte United Airlines das System \"Apollo CRS\", hiermit wurden rechnergestützt Buchungen von Flügen und Sitzreservierungen zunächst für die eigenen Verkaufsbüros möglich. Ab 1976 wurde dieses System unter dem Namen \"Apollo Travel Services\" (ATS) auch an Reisebüros in Nordamerika und Japan vertrieben.\n\n1987 wurde in Swindon, Großbritannien, die Galileo Company Ltd von den Anteilseignern British Airways, Swissair, KLM Royal Dutch Airlines, Alitalia und Covia (vormals Apollo Travel Services) gegründet.\n\nIm Jahr 1997 fand der Börsengang der Galileo International Corporation an der New York Stock Exchange statt. Von Oktober 2001 bis September 2006 war die Cendant Corporation alleiniger Anteilseigner und Galileo in dieser ein wichtiges Mitglied der Cendant Travel Distribution Services, in welchem die touristischen Aktivitäten der Cendant Corporation gebündelt waren.\nNach der Aufspaltung der Cendant Corporation in vier eigenständige Unternehmen im Jahre 2006 wurde der Bereich TDS an den Finanzinvestor Blackstone Group verkauft und firmiert seitdem unter dem Namen \"Travelport\". Nach Freigabe durch die Kartellaufsichtsbehörden in den USA und der EU hat Travelport am 21. August 2007 den Galileo-Mitbewerber Worldspan übernommen und mit der Zusammenführung der beiden Reservierungssysteme begonnen.\n\nGalileo führt den IATA-Airline-Code 1G, das weiterhin parallel betriebene Apollo-System 1V.\n\nWeltweit arbeiten 43.500 Reisebüros mit dem Reservierungssystem und können hierüber auf Tarife und Verfügbarkeiten von über 460 Fluggesellschaften, 23 Autovermietern, 58.000 Hotels und 430 Veranstaltern zugreifen.\n\nDie deutsche Niederlassung wurde 1989 in Frankfurt am Main gegründet und betreut mit 21 Mitarbeitern derzeit ca. 900 Kunden.\n\n"}
{"id": "1438837", "url": "https://de.wikipedia.org/wiki?curid=1438837", "title": "Babylon Translator", "text": "Babylon Translator\n\nBabylon Translator ist ein automatisches Übersetzungsprogramm für Microsoft Windows. Es verfügt über eine Texterkennung, die es ermöglicht, die Übersetzung durch einen Mausklick auf ein beliebiges Wort in Texten zu starten. Dazu verwendet das Programm eine integrierte OCR-Software. Nachzuschlagende Wörter werden direkt vom Bildschirm erkannt − das Prinzip funktioniert selbst in Bilddateien. Verglichen mit den zum Zeitpunkt seiner Veröffentlichung bekannten Programmen wurde damit eine neue Benutzerschnittstelle popularisiert.\n\nMit Babylon Translator wurde 1997 das von Bill Gates propagierte Motto „\"Information at your fingertips\"“ umgesetzt, das eine allgemeine, schnelle Zugänglichkeit von Informationen mit Hilfe von an Netzwerke angeschlossenen Computern als Vision enthielt. Ein einfacher Mausklick in Babylon auf ein Schlüsselwort liefert durch Integration unterschiedlichster Nachschlagewerke dessen Übersetzung in andere Sprachen, die Bedeutung von Abkürzungen usw.\n\nDie Geschichte von Babylon begann 1995, als Amnon Ovadia die Idee hatte, ein englisch-hebräisches Wörterbuch zu erstellen, das den Lesevorgang am Bildschirm des PCs nicht mehr unterbricht. Das Unternehmen Babylon wurde 1997 in Or Jehuda, Israel durch Ovadia und Shuki Preminger, finanziell unterstützt durch Mashov Computers (heute \"Formula Vision Technologies Ltd.\"), gegründet.\nNoch im Herbst desselben Jahres wurde ein erstes US-Patent für den innovativen Übersetzungsansatz angemeldet.\n\n1998, ein Jahr nach dem Start des Unternehmens rühmte sich die Firma, mehr 2 Millionen Anwender zu haben, vorwiegend in Deutschland und Brasilien, wobei sich die Zahl der Anwender alleine in diesem Jahr von 420.000 auf 2,5 Millionen steigerte.\nIm Jahr 2000 sprach man bereits von über 4 Millionen Anwendern, und das Programm zählte in Bezug auf die Zahl der heruntergeladenen Dateien zu den meistfrequentierten bei ZDnet Frankreich, AOL Deutschland und Tucows.\n\nAnfang 2000 geriet das Unternehmen in finanzielle Schwierigkeiten. Im Frühjahr 2000 misslang der Versuch einer Privatplatzierung von Babylon Ltd., der in Vorbereitung eines Börsengangs 20 Millionen US-Dollar einbringen sollte, womit die Hoffnungen des Unternehmens auf den späteren Gang an die Börse aufgegeben werden mussten. Weiterhin verlor Babylon Ltd. in diesem Jahr 15 Millionen Schekel.\nNoch weiter bergab ging es mit der unter dem Namen Dotcom-Blase bekannt gewordenen Finanzkrise. 2001 setzten sich die finanziellen Verluste für Babylon Ltd. fort, was die Muttergesellschaft Formula Vision weitere 4,7 Millionen Schekel kostete.\n\nAls Resultat der finanziellen Verluste änderte Babylon das Geschäftsmodell weg vom Freeware-Angebot hin zur kostenpflichtigen Lizenz, woraufhin sich laut Wired einige Anwender mit Aussagen wie \"„Untergrabung des Geistes des Internets“\" beschwerten. Auf Computern mit Internetverbindung ohne eine installierte Firewall stellte der freie Client seinen Dienst ein und zwang die Anwender zum Kauf der kostenpflichtigen Software. Ohne Internetverbindung war die Installation des Offline-Wörterbuchs \"dict50.exe\" nötig. Außerdem verwendete das Unternehmen in früheren Adware-Versionen die kontrovers diskutierte, weil oft als Spyware betrachtete Cydoor-Komponente, die z. B. auch im 2005 eingestellten eXeem-Client für Peer-to-Peer-Tauschbörsen verwendet wurde. Bis Version 4.0 wurde so ein Benutzer-Tracking durchgeführt.\n\n2007 stieg der Unternehmer und Investor \"Noam Lanir\" (* 18. Februar 1967) mit mehrheitlicher Kontrolle in das Unternehmen ein, und Entwicklungen wie die neue Version des Babylon Builder und der in Vorbereitung der Veröffentlichung von Version 7.0 eingeführte Online-Community-Dienst LingoZ machen den aktuellen Wandel in der Philosophie des Unternehmens Babylon sichtbar. Die FAQ von LingoZ sprechen dazu eine eindeutige Sprache: \"„Secondly, we give loud and clear credit to contributors“\".\nZum lizenzierten Software-Client stehen neben den kommerziell vermarkteten Inhalten und den online für jeden zugänglichen, von der früheren Community geschaffenen Babylon-Inhalten, nun online erweiterbare LingoZ-Glossare von Babylon Ltd. zur freien Verfügung.\n\n\"Babylon Ltd.\" wurde 1997 als Entwicklungsfirma für die gleichnamige Übersetzungssoftware gegründet. Der Hauptsitz des Unternehmens, das ca. 80 Mitarbeiter beschäftigt, befindet sich in Israel. Weiterhin existiert eine Tochtergesellschaft in Deutschland.\n\nIm Februar 2007 wagte das Unternehmen den Schritt an die Israelische Börse in Tel Aviv.\n\nHauptanteilseigner von Babylon Ltd. sind Reed Elsevier (LSE: REL, NYSE: RUK), Formula Vision Technologies, eine Tochtergesellschaft von Formula Systems (NASDAQ: FORTY), The Monitin Group (Eliezer Fishman) und der Unternehmensgründer Amnon Ovadia.\n\nDie Testversion des Babylon-Client ist in ihrer Laufzeit auf 30 Tage beschränkt. Von 1997 bis Anfang des Jahres 2001 wurde die Babylon-Software kostenlos zum Herunterladen angeboten. Im Februar 2001 mit der Version 4.0 ist Babylon Ltd. dazu übergegangen, kommerziell Softwarelizenzen für Privatanwender zu vertreiben, wobei ältere Versionen des Babylon-Clients anschließend teilweise nicht mehr einwandfrei funktioniert haben.\n\nFür Unternehmen bietet Babylon seit einiger Zeit eine spezielle, kostenpflichtige \"Babylon-Enterprise-Version\", speziell für den Einsatz in Unternehmen.\n\nDer aktuelle Babylon-Client bietet ein integriertes System zur Sprachausgabe, bei dem zwischen unterschiedlichen Stimmen gewählt und die Sprechgeschwindigkeit angepasst werden kann.\n\nBabylon bietet heute zahlreiche, kostenpflichtige Inhalte von namhaften Wörterbuchverlagen wie PONS, Wahrig, Langenscheidt, Oxford, Britannica und viele anderen Verlagen zur Implementierung und Nutzung im Babylon Translator an, es ist jedoch auch eine große Anzahl kostenloser Glossare zur Nutzung im Babylon Translator verfügbar.\n\nZu den für lizenzierte Babylonnutzer kostenlos verfügbaren Angeboten gehören u. a. Wetterdaten, Fahrpläne und Devisenkurse, wobei letztere im Babylon-Client bei vorhandener Onlineverbindung automatisch über den Dienstleister XE.com auf Forex-Kurswerte aktualisiert werden und so jederzeit aktuell zur Umrechnung zwischen verschiedenen Währungen zur Verfügung stehen.\n\nWeiterhin können mit dem Babylon Translator verschiedene physikalische Messgrößen und Einheiten in andere SI-Einheiten und Nicht-SI-Einheiten umgerechnet werden.\n\nAuch die Ortszeit und der Zeitunterschied zweier Orte, sowie die Differenz zwischen verschiedenen Zeitzonen kann mit Hilfe der Babylon-Software bestimmt werden.\n\nDer Babylon Translator verbreitete sich nach seiner Veröffentlichung zunächst recht schnell, da die Nutzer mit dem kostenlos verfügbaren \"Babylon Builder\" Wörterbücher zu zahlreichen Gebieten erstellten und zum freien Herunterladen anboten.\n\nDiese Wörterbücher und Verzeichnisse werden von Babylon zwar frei angeboten, zu deren Nutzung ist heute jedoch eine Softwarelizenz für den Babylon Translator erforderlich. Diese indirekte Kommerzialisierung von kostenlos erstellten Inhalten führte seinerzeit zu erheblicher Verärgerung in der Babylon-Gemeinschaft, weil das freiwillige Engagement der Nutzer zu kommerziellen Zwecken genutzt wurde.\n\nMitte August 2007 hat Babylon die dritte Version seiner Software zur Erstellung von Benutzerwörterbüchern, den \"Babylon Glossary Builder\" veröffentlicht. Diese neue Version beruht auf XML und wird für Privatanwender kostenlos, für Unternehmen jedoch kostenpflichtig angeboten.\n\nGlossare, die mit Microsoft Word oder anderen Texteditoren erstellt wurden, konnten bei früheren Versionen des Babylon Builders nach dem Abspeichern als HTML-Datei importiert werden. Diese Funktionalität wird vom neuen Babylon Glossary Builder nicht mehr in derselben Art und Weise unterstützt.\n\nEin mit dem Babylon Glossary Builder erstelltes Glossary-Projekt (GPR) zur Erstellung eines Babylon Glossary (BGL) basiert entweder auf Babylon-GLS- bzw. TXT-Dateien oder aber auf XLS- bzw. XLSX-Dateien (Microsoft Excel), wobei die Software verschiedene Optionen zur Erstellung der Wörterbücher anbietet und eine Verlinkung von Online-Inhalten ermöglicht.\n\nDa der Babylon Glossary Builder das Erzeugen von Glossaren mit beliebigem Inhalt ermöglicht, existieren auch exotische \"Wörterbücher\" wie beispielsweise\n\n\nSeit 2006 bietet Babylon Zugriff auf Wikipedia in 13 Sprachen (Arabisch, Chinesisch, Niederländisch, Englisch, Französisch, Deutsch, Hebräisch, Italienisch, Japanisch, Koreanisch, Portugiesisch, Spanisch und Türkisch). Babylon stellt den ersten Abschnitt eines Wikipedia-Artikels im Client dar und dem Nutzer einen Link zum gesamten Artikel zur Verfügung.\n\nNachdem der Translator durch seine Kommerzialisierung zunächst einen erheblichen Teil seiner früheren Bedeutung eingebüßt hatte, scheint das Übersetzungswerkzeug in jüngerer Zeit wohl auch wegen der kostenlosen Plug-ins zur Nutzung von Wikipedia in Verbindung mit Babylon wieder an Bedeutung zu gewinnen.\n\nAußerdem ist im aktuellen Babylon-Client eine Übersetzungsfunktion integriert, die einen frei eingegebenen Text zur Online-Maschinenübersetzung mittels Drittsoftware an einen Server überträgt und die Übersetzung dieses Textes in wenigen Sekunden zurückliefert, wobei Ausgangs- und Zielsprache aus einem Angebot von 17 bzw. 18 möglichen Sprachen gewählt werden können.\n\nAuf der Homepage von Babylon ist heute neben den vielen kommerziell und nicht kommerziell über Download vertriebenen Wörterbüchern mit dem \"Babylon Online Dictionary\" ein Onlinedienst verfügbar, der die Recherche nach Worten bzw. Übersetzungen in den für Babylon verfügbaren 1300 Wörterbüchern in über 50 Sprachen ermöglicht.\nDer Onlinedienst bietet zudem die Möglichkeit, unter Nutzung von Google nicht nur das Web zu durchsuchen, sondern unter dem Stichpunkt \"Definition\" auch nach Wortdefinitionen in den bei Babylon verfügbaren Glossaren zu suchen.\n\nMit Version 7.0 hat Babylon Ltd. unter dem Namen LingoZ ein neues Online-Wörterbuch eingeführt, das ähnlich wie ein Wiki gepflegt werden kann und aufgrund der bereits vorhandenen, integrierten Babylon-Inhalte heute bereits fast 4,5 Millionen Definitionen enthält.\n\nAb Version 4 ist ein Upgrade des Babylon-Client auf die aktuelle Version möglich.\n\nProgramme, die das volle Funktionsspektrum von Client und Glossary Builder abdecken, existieren noch nicht. Für die verschiedenen Funktionalitäten gibt es jedoch unterschiedliche Alternativen. Dazu zählen u. a. folgende Werkzeuge:\n\n\nIm August 2010 stufte Microsoft die damals aktuelle Version als Malware ein. Diese Einstufung wurde innerhalb weniger Tage für eine aktualisierte Version widerrufen.\nBei der Babylon Toolbar handelt es sich um Adware und um einen Browser-Hijacker, der den installierten Browsern nicht nur eine Toolbar hinzufügt, sondern auch die Startseite auf \"Babylon Search\" ändert. Spybot hat diese Toolbar als Malware eingestuft, und empfiehlt deren Deinstallation.\n\n"}
{"id": "1440039", "url": "https://de.wikipedia.org/wiki?curid=1440039", "title": "MFchi", "text": "MFchi\n\nMFchi ist ein kommerzielles Bibelprogramm für Windows, das mit einer Vielzahl von CD-ROMs/DVDs aus dem christlichen Bereich geliefert wird. Diese CD-ROMs werden unter verschiedenen Bezeichnungen angeboten, wie beispielsweise \"ELBIWIN\", \"CD-ROM Bibel Edition\" oder \"bibeldigital\". Grund hierfür ist, dass die Software von verschiedenen Verlagen herausgegeben wird, die meist auf eine eigene Produktbezeichnung Wert legen.\n\nMFchi wird seit Anfang der 1990er Jahre entwickelt. Erstmals wurde es unter dem Namen „ELBIWIN 3.0“ zusammen mit der revidierten Elberfelder Bibel als Produkt angeboten. Mittlerweile ist die Produktpalette wesentlich erweitert worden. Die Produkte, die MFchi enthalten, sind nun auch für die Bereiche wie Bibellese, Bibelstudium, Gottesdienstvorbereitung, Kinder- und Jugendarbeit sowie Religionsunterricht konzipiert. Hierbei wird inhaltlich ein breites Spektrum christlicher Richtungen abgedeckt.\n\nDank des modularen Aufbaus von MFchi können alle Produkte unter einer Programmoberfläche benutzt werden. Die Texte sind vielfach miteinander verknüpft, so dass beispielsweise ein Bibelstellenverweis in einem Lexikon direkt per Mausklick nachgeschlagen werden kann.\n\nMFchi wird in zwei verschiedenen Versionen angeboten: MFchi kompakt und MFchi pro. Die Pro-Version bietet gegenüber der Kompakt-Version eine größere Anzahl an Funktionen. Wenn man sowohl Produkte der Kompakt- wie der Pro-Version erworben hat, stehen alle Texte unter der Pro-Version zur Verfügung.\n\nMFchi pro besitzt eine Importfunktion, mit der sich auch eigene Bibeltexte importieren lassen. Das hierbei verwendete Importformat entspricht dem Exportformat der Online-Bibel. Hierdurch ist es unter anderem möglich, mit MFchi auch einen Großteil der fremdsprachigen Bibelübersetzungen zu nutzen, die für die Online-Bibel angeboten werden.\n\n„MF“ steht für den Autor der Software: Matthias Frey. Der griechische Buchstabe „chi“ soll auf Christus verweisen, der als Zentrum der Software gesehen wird.\n\nDer Hauptvorteil von MFchi gegenüber anderen Produkten liegt in der großen Anzahl deutschsprachiger urheberrechtlich geschützter Texte. Als Nachteil steht dem gegenüber, dass man für Produkte mit MFchi – anders als für einige anderer Programme – bezahlen muss. Für eine Arbeit mit den altsprachlichen Bibeltexten eignen sich zudem besser hieran angepasste Bibelprogramme mit besonderen Funktionen wie der grammatikalischen Bestimmung von Wörtern. Siehe unter dem Artikel Bibelprogramm für Hinweise auf Alternativen.\n\n"}
{"id": "1440577", "url": "https://de.wikipedia.org/wiki?curid=1440577", "title": "Scenarist", "text": "Scenarist\n\nScenarist ist eine professionelle Authoring-Software für DVDs und Blu-rays und gilt seit 1995 als weltweiter De-facto-Standard für Datenträgerproduktionen in der Film- und Musikbranche. Die aktuellen Versionen sind \"Scenarist SD\" (für DVDs) sowie \"Scenarist BD Professional\" und \"Scenarist BD Professional Plus\" für BDs. Neben DTS:X, dem 3D-Surround-Sound-Format von DTS, unterstützt Scenarist auch Dolby TrueHD und DTS-HD Master Audio bis zu 7.1 Surround Sound. Im September 2015 stellte Scenarist auf der International Broadcasting Convention in Amsterdam die Unterstützung für Ultra HD Blu-rays vor.\n\nIm Gegensatz zu vielen anderen Authoring-Programmen unterstützt Scenarist lückenlos alle Möglichkeiten der DVD- und BD-Video-Standards, wodurch es zweifellos dem professionellen Bereich zuzuordnen ist. Die jeweiligen Programmsuiten enthalten u. a. das Adobe-Photoshop-Plug-in \"Scenarist Designer PS\" mit Grafikvorlagen zum Export von HDMV-Grafiken (High Definition Movie Mode, 8 bit Blu-ray Standard mit 256 Farben) sowie als BD-J Grafiken (24 bit True Color) für das Menü der BD. Mit \"Scenarist BD-J\" wird eine Eclipse-Plattform zur weitergehenden Java-basierten Erstellung der grafischen Menüführung bereitgestellt. Die eigentlichen Audio- und Videoinhalte der BD, die mit der \"Dolby Media Producer Suite\" für Dolby-Audio, der \"DTS-HD Master Audio Suite\" für DTS-Audio oder Audioeditoren wie Adobe Audition und Pro Tools bzw. Videoschnittprogrammen wie Adobe Premiere, Adobe After Effects, Avid Media Composer oder Final Cut Pro zu codieren sind, werden mit Hilfe weiterer Tools im Blu-ray Standard importiert.\n\nBranchenbekannte Alternativen zu Scenarist BD sind Sonys \"DoStudio Authoring\" und \"Blu-print\".\n\nScenarist wurde bis zur Version 2.7 von Daikin Industries sowie seit 2001 von Sonic Solutions entwickelt und seit der Firmenauflösung im Jahr 2010 von der Rovi Corporation geführt. Im September 2014 wurde bekannt gegeben, dass die \"Scenarist LLC\", ein neu gegründetes Entertainment-Technologie-Unternehmen im kalifornischen Novato, die Rechte an den Hollywood-Standard Blu-ray Disc-Technologien und Produkten erworben hat.\n\nLegende Bildfrequenz: p = progressiv, Vollbilder pro Sekunde / i = interpoliert, Halbbilder pro Sekunde\n\nDie Versionen \"Scenarist BD\" unterstützen somit alle Formate der Blu-ray Spezifikationen für Video- und Audio Streams.\n\n"}
{"id": "1442206", "url": "https://de.wikipedia.org/wiki?curid=1442206", "title": "Design for Six Sigma", "text": "Design for Six Sigma\n\nDesign for Six Sigma (DfSS) ist eine Methode des Qualitätsmanagements für robuste, also möglichst fehlerarme Produkte und Prozesse.\n\nDfSS wird eingesetzt für die Ausgestaltung oder Wiedergestaltung eines Produktes, Prozesses oder einer Dienstleistung. Das zu erwartende Prozess-Sigma-Level für ein DfSS-Produkt oder einen entsprechenden Prozess bzw. eine Dienstleistung sollte mindestens 4,5 sigma (entsprechend etwa 1 Promille = 1 Fehler pro 1000 Möglichkeiten) betragen, kann aber auch 6 sigma (3,4 Fehler pro Million Möglichkeiten) oder bei Bedarf noch höher sein.\n\nEin Produkt oder Dienstleistung mit einer solch niedrigen Fehlerhäufigkeit setzt voraus, dass die für den Kunden entscheidenden Erwartungen und Bedürfnisse (englisch \"\", abgekürzt CTQ), gänzlich verstanden werden müssen, bevor ein Produkt oder eine Dienstleistung vervollständigt und eingeführt werden kann.\n\nIm Gegensatz zum Six-Sigma-Kernprozess DMAIC sind die Phasen oder Schritte für DfSS-Kernprozesse nicht universell anerkannt oder definiert, da fast jedes Unternehmen oder Trainingorganisation DfSS unterschiedlich definiert. Das liegt unter anderem daran, dass oftmals eine Unternehmung DfSS einführt, um ihr Geschäft, Industrie und Kultur anzupassen. Manchmal wird eine Version DfSS von einer Beratungsfirma, zur Unterstützung im Personaleinsatz, eingeführt.\n\nEine beliebte DfSS-Methode ist das DMADV. Die fünf Phasen des \"DMADV\" sind ähnlich wie bei DMAIC wie folgt definiert:\n\n\nEine Variante der DMADV Methodik ist \"DMADOV\": Definieren, Messen, Analysieren, Design, Optimieren und Prüfen (englisch \"\").\nEs gibt einige andere „Arten“ von DfSS: DCCDI, IDOV und DMEDI.\nDCCDI ist durch Geoff Tennant bekannt geworden und ist definiert als Definieren, Kundenkonzept (englisch '), Design, Einführung (englisch '). Es gibt viele Ähnlichkeiten zwischen diesen Phasen und denen des DMADV.\n\n\n\"IDOV\" ist eine bekannte Designmethode speziell im Herstellungsbereich. Die Abkürzung IDOV leitet sich aus\nIdentify (Ermitteln), Design, Optimieren und Validate (Bestätigen)\nab.\n\n\nDa die Kosten der DMAIC-Methode hoch sind, können kleinere Unternehmen meistens nicht von Six Sigma profitieren. Dieses Problem sucht die ICRA-Methode Generation III zu umgehen.\nDer Hauptgedanke von ICRA besteht in der Entwicklung innovativer Ideen.\n\nICRA (Innovate, Configure, Realize, Attenuate) hilft, offene Fragen aller Art durchzudenken.\n\n\nWährend ICRA innovative Ideen erzeugen soll, ist DMAIC für die robuste Durchführung der innovativen Ideen besser geeignet.\n\nIm DfSS kommen ähnlich wie beim DMAIC-Projekt von Six Sigma verschiedenste Werkzeuge zum Einsatz. Dazu zählen unter anderem Toleranzanalyse, Toleranzdesign, House of Quality und Quality Function Deployment. Weiterhin kommen bei DfSS spezielle Designwerkzeuge, CAD Tools und Simulationen wie die Monte-Carlo-Simulation in Verbindung mit statistischen wie auch nicht statistischen Berechnungsmethoden zum Einsatz. Eine mögliche Berechnungsmethode nicht statistischer Natur die beispielsweise (neben der statischen Analyse) bei statischen Problemen eingesetzt werden kann wäre die Finite-Elemente-Methode.\n\n\n"}
{"id": "1442723", "url": "https://de.wikipedia.org/wiki?curid=1442723", "title": "Elephants Dream", "text": "Elephants Dream\n\nElephants Dream ist ein surrealer, computer-generierter Kurzfilm, der fast ausschließlich mit freier Software erstellt und unter einer freien Creative-Commons-Lizenz veröffentlicht wurde. Aufgrund der positiven Reaktionen sowie den Erfahrungen aus diesem Projekt wurde am 10. April 2008 ein zweiter Film unter Creative-Commons-Lizenz veröffentlicht: \"Big Buck Bunny\".\n\nDie Sprache des Filmes ist Englisch, es gibt allerdings Untertitel in über 30 Sprachen.\n\nDer Film wurde erstmals im Mai 2005 von Ton Roosendaal, dem Vorsitzenden der Blender Foundation und Chefprogrammierer von Blender, angekündigt. Blender ist das hauptsächlich genutzte Programm bei der Produktion des Films. Kurz nach der Ankündigung begann die Blender Foundation Vorbestellungen der DVD zum Film entgegenzunehmen. Jeder, der die DVD vor dem 1. September 2005 vorbestellt hatte, wird in den Credits genannt.\n\nDie Produktion begann im September 2005 durch das \"Orange Projekt\", eine Gruppe aus sechs Künstlern und Animatoren aus der ganzen Welt. Der Film hieß ursprünglich \"Machina\", bevor er in \"Elephants Dream\" umbenannt wurde. Der neue Titel basiert auf einer niederländischen Tradition von Gute-Nacht-Geschichten und hat absichtlich kein englisches Genitiv-Apostroph. Die Uraufführung war am 24. März 2006 in Amsterdam.\n\nDas Ziel des Films ist in erster Linie, die Möglichkeiten von quelloffener Software und von Blender als professionellem Werkzeug für Filme zu zeigen. Während der Produktion des Films wurden mehrere neue Funktionen, wie Haar- und Fell-Rendering, in Blender eingebaut.\n\nDer Inhalt des Films (einschließlich der Produktionsdateien) ist unter der Creative-Commons-Attribution-Lizenz (CC-BY) veröffentlicht.\n\nElephants Dream sowie die Produktionsdateien wurden am 3. Mai 2006 auf DVD veröffentlicht. Am 18. Mai wurden der Film und die Produktionsdaten auch im Internet veröffentlicht. Elephants Dream ist als erster deutscher Film auf HD DVD seit dem 14. August 2006 offiziell erhältlich, hiermit ist er zugleich der erste europäische Titel in diesem Format.\n\nDer Film spielt ausnahmslos in einer riesigen surrealen Maschine, einer Traumwelt.\nDer alte Mann \"Proog\" (Tygo Gernandt) führt den jungen \"Emo\" (Cas Jansen) durch eine Maschine. Der Zuschauer sieht diesen Rundgang aus den Augen Proogs, welcher dabei offensichtlich unter heftigen Wahnvorstellungen leidet, aber stets sicher und erfahren wirkt, während Emo hingegen ängstlich wirkt und immer nahe bei Proog bleibt. Die beiden gehen durch mehrere immer unterschiedlich aussehende Räume der Maschine, die meistens keinen Übergang zueinander haben und Proogs abstrakte Halluzinationen darstellen.\n\nZu Beginn befinden sie sich in einem Raum bestehend aus einer gigantischen Telefonschalttafel. Proog rettet Emo vor umherfliegenden Steckern und betont, dass es in der Maschine nicht sicher sei. Der nächste Raum ist sehr dunkel; es befinden sich tausende elektrischer Kabel sowie vogelähnliche Roboter darin. Emo und Proog gehen den einzigen Weg entlang, als plötzlich die vogelähnlichen Maschinen die Verfolgung aufnehmen. Hals über Kopf flüchten sie und landen sofort im nächsten Raum. Ein Telefon klingelt, woraufhin Emo den Hörer abheben möchte. Proog hindert ihn daran, und erklärt ihm, dass es eine Falle ist. Im Raum befindet sich ebenfalls ein roboterähnliches Wesen, das zum Teil aus einer Schreibmaschine besteht und auf sich selbst tippt. Emo kann darüber nur lachen.\n\nDer nächste Raum ähnelt dem zweiten: Es ist wieder dunkel. Ein riesiger Abgrund lässt eine Durchquerung des Raumes scheinbar nicht zu. Als Proog jedoch in das Nichts geht, erscheinen von unten metallene Stützen, die ihn tragen. Proog bewegt sich durch den Raum mit Leichtigkeit, er tanzt und erklärt Emo währenddessen, dass die Maschine wie ein Uhrwerk sei, ein falscher Schritt und man würde zerquetscht. Emo hingegen läuft fast gelangweilt durch den Raum und scheint die Stelzen, die ihn tragen, gar nicht zu bemerken. Die beiden betreten eine Art Lift, der beschleunigt und durch mehrere bedrohliche Blendenlamellen katapultiert wird. Proog fordert Emo auf, seine Augen zu schließen. Als er dies tut, werden beide plötzlich in Dunkelheit gehüllt. Proog fragt Emo, was er sehe; Emo antwortet ihm wahrheitsgemäß, dass er nichts sehe. Dies scheint Proog zu bestätigen, sie stürzen rasant in die Tiefe und kommen in den nächsten Raum. Ein Projektor wirft das Bild einer Tür an die Wand, aus der Musik dringt. Emo möchte durch die Tür gehen, Proog jedoch meint, es sei gefährlich. Er nimmt seinen Stock und drückt auf einen roten Knopf, der sich daran befindet. Die Projektion wird kurz erschüttert, verschwindet jedoch nicht. Erst nach dem dritten Knopfdruck verblasst der Raum und wird durch einen engen und erdrückenden ersetzt.\n\nProog fragt Emo, warum er nicht die Schönheit, die Vollkommenheit der Maschine erkenne. Emo spricht nun erstmals seine Gedanken aus, nämlich, dass die Maschine gar nicht existiert. Proog gibt Emo eine Ohrfeige, auf die Emo erschrocken reagiert und sich von ihm entfernt. Kurz bevor er gegen die Wand zu stoßen droht, bewegt diese sich von ihm weg. Er imitiert Proog nun und versucht die Absurdität der Maschine zu zeigen. Er behauptet, die ganze Maschine sei nur alleine für Proog da und dass jetzt die hängenden Gärten von Babylon an der Reihe seien. Der Raum löst sich allmählich auf und überall sprießen metallene Schlingpflanzen hervor. Nun erklärt er Proog spöttisch, dass jetzt der Koloss von Rhodos käme, und das nur für Proog. Die von Proog konstruierte Fantasiewelt zum Schutz vor der realen Welt droht zu zerfallen. Zwei riesige Hände erscheinen und werfen Proog zu Boden. Proog fühlt sich bedroht und schlägt Emo mit seinem Stock nieder, um diesen zum Schweigen zu bringen. Anschließend verschwinden die Hände und Arme wieder, die Maschine ist wieder in Ordnung. Proog beteuert, dass die Maschine existiere.\n\n\n\n"}
{"id": "1443177", "url": "https://de.wikipedia.org/wiki?curid=1443177", "title": "Nachrichtendienst (Windows)", "text": "Nachrichtendienst (Windows)\n\nDer Nachrichtendienst ist ein Systemdienst, der Bestandteil der Betriebssysteme der Windows-Familie ist. Er dient dazu, in einem Computernetzwerk kurze Nachrichten zu versenden.\nEr basiert auf NetBIOS (heutzutage in der Regel per NBT) und ist nicht mit dem IPv6-Protokoll kompatibel.\n\nSeit Windows NT kann der Nachrichtendienst über das Kommandozeilenprogramm \"cmd.exe\" aufgerufen werden. Die Datei, in der der Nachrichtendienst implementiert ist, befindet sich im system32-Ordner (%windir%\\system32\\) und heißt \"net.exe\". Der Nachrichtendienst ist ab Windows Vista durch das Programm \"MSG.exe\" ersetzt.\n\nDas Programm wird über die Eingabeaufforderung bedient. Es erhält als ersten Parameter den Befehl \"SEND\". Daran schließt sich die Bezeichnung des Empfängers bzw. Empfängerkreises der Nachricht an. Direkt im Anschluss wird der Text der zu versendenden Nachricht eingegeben.\n\nBeispiele für mögliche Eingaben:\nDer Nachrichtendienst benutzt die UDP-Ports 135, 137 und 138 sowie die TCP-Ports 135, 139 und 445.\n\nTechnisch ist es auch möglich, Nachrichten anonym zu senden. \"NetSendFaker\" ist dafür das bekannteste Programm.\n\nUrsprünglich war der Nachrichtendienst für das Versenden kurzer Mitteilungen in lokalen Netzwerken gedacht. So konnte beispielsweise ein Systemadministrator auch größere Nutzerkreise schnell über Ausfälle oder Wartungsarbeiten informieren.\n\nDa mittlerweile in den meisten Computernetzwerken eine E-Mail-Infrastruktur vorhanden ist, hat der Nachrichtendienst auf diesem Gebiet rapide an Bedeutung verloren.\n\nDer Versand von Nachrichten ist nicht nur in lokalen Netzwerken, sondern auch über das Internet möglich.\nDiese Methode wurde in der Vergangenheit jedoch häufig von Spammern als Medium zum Verbreiten eigener Werbung genutzt.\nAus diesem Grund hat Microsoft den Nachrichtendienst in Windows Server 2003 sowie mit dem Service Pack 2 von Windows XP deaktiviert. Über den Service Control Manager kann der Dienst bei Bedarf wieder aktiviert werden.\n\nFür ältere Systeme existieren Anleitungen zum Deaktivieren des Dienstes.\n\nIn den Windows Vista-Versionen Business und Ultimate, sowie in Windows 2008 und Windows 7 wurde der NET SEND-Befehl durch das Programm MSG.exe ersetzt. Die Versionen Home Basic und Home Premium enthalten keinen systemeigenen Nachrichtendienst.\n\n"}
{"id": "1443892", "url": "https://de.wikipedia.org/wiki?curid=1443892", "title": "Skyglobe", "text": "Skyglobe\n\nSkyglobe ist ein Astronomieprogramm zur Simulation des Sternenhimmels einschließlich der Objekte aus dem Messier-Katalog, Planeten, Sonne und Mond. Es berücksichtigte die Präzession der Erdachse (25.750 Jahre). Es war unter Astronomie-Interessierten sehr beliebt. Es wurde von \"Mark A Haney\" bzw. seiner Firma \"KlassM Software Inc.\" in Ann Arbor, Michigan als Shareware angeboten. Mark schrieb es 1989, nachdem er seinen Abschluss in \"Computer Science\" an der Michigan State University gemacht hatte.\n\nDie letzte offizielle Version 3.6 stammte vom Oktober 1993 und beinhaltete in der Shareware-Version die Daten von 29.000 Sternen. Ursprünglich für Systeme unter DOS entwickelt, lief sie ebenfalls unter Windows 95/98 und größtenteils auch unter Windows NT/2000. Für 20 USD wurde eine CD mit einer größeren Sternendatenbank angeboten.\n\nAnlässlich des Ereignisses Shoemaker-Levy 9 gab Haney im Juli 1994 die DOS-Version 4.0 und testweise \"Skyglobe for Windows 1.0\" heraus, das unter Windows 3.11 lauffähig war. Das mitgelieferte Ergänzungsprogramm \"CircumSpace\" erlaubte es, kleine Flüge durch den unserem Sonnensystem nahegelegenen Weltraum zu machen. Es simulierte die Umgebung der 7.700 nächstgelegenen Sterne.\n\n"}
{"id": "1446886", "url": "https://de.wikipedia.org/wiki?curid=1446886", "title": "MK 14", "text": "MK 14\n\nDer MK 14 ist ein Homecomputer-Bausatz, der vom englischen Anbieter Science of Cambridge ab dem Jahre 1978 auf den Markt gebracht wurde. Das Gerät wurde bis 1980 hergestellt und zu einem Preis von £39.95 verkauft. Hinter dem Unternehmen Science of Cambridge steckten der Entwickler des MK 14 Ian Williamson und die Unternehmensbesitzer Clive Sinclair und Chris Curry, die später auch die Unternehmen Sinclair und Acorn gründeten. Vom MK 14 wurden zwischen 15.000 und 50.000 Stück verkauft.\n\nZentraler Bauteil des MK 14 ist der National Semiconductor Microprocessor ISP-8A/600 (SC/MP); er ist nicht besonders leistungsfähig, dafür aber einfach zu programmieren. Die Programmierung erfolgt über eine hexadezimale Tastatur, für die Ausgabe wurde eine acht- bzw. neunstellige 7-Segment-Anzeige verwendet. Der Speicher war auf 256 Bytes begrenzt, konnte aber auf 640 Bytes erweitert werden.\n\n"}
{"id": "1450314", "url": "https://de.wikipedia.org/wiki?curid=1450314", "title": "Lighttpd", "text": "Lighttpd\n\nLighttpd (zumeist gesprochen als \"Lighty\") ist ein von Jan Kneschke entwickelter freier Webserver. Er implementiert alle wichtigen Funktionen eines Webservers und kann, ähnlich wie Apache, durch Module erweitert werden.\n\nDer Server setzt auf asynchrone Kommunikation und bearbeitet mehrere Anfragen in einem einzigen Betriebssystem-Prozess. Dadurch ist der Webserver in der Lage, mehrere parallele Anfragen effizienter zu bearbeiten und CPU und Arbeitsspeicher weniger zu belasten als ein Multi-Prozess-Design etwa beim Apache Webserver. Dies lohnt sich besonders beim gleichzeitigen Zugriff auf große Dateien von vielen Benutzern.\n\nPHP wird mittels FastCGI angebunden. Perl, Python oder Ruby können auch über die SCGI-Schnittstelle angesprochen werden. Seiten können über das Modul CML (ursprünglich: Cache Meta Language) in Lua automatisch erzeugt werden, auch Server Side Includes werden durch ein Modul unterstützt.\n\nLighttpd verbreitet sich laut Netcraft schnell und schaffte im „Web Server Survey“ vom März 2007 mit ca. 1,4 Millionen Domains den Sprung auf den 4. Platz der Rangliste. Bei dieser Wertung gehen allerdings \"auch\" etliche „geparkte“ Domains ein, von denen oft Tausende auf einem Server liegen, wodurch der Wechsel großer Domainhändler sich stark auf die Zahlen auswirken kann. Im August 2010 lag die Verbreitung bei 1,8 Millionen Domains.\n\n\n\n"}
{"id": "1450440", "url": "https://de.wikipedia.org/wiki?curid=1450440", "title": "Guadalinex", "text": "Guadalinex\n\nGuadalinex ist eine auf Ubuntu basierende Linux-Distribution, die von der Regierung von Andalusien (Spanien) unterstützt und eingesetzt wird. Guadalinex ist speziell auf die Bedürfnisse in Andalusien lebender Spanier zugeschnitten und wird deshalb nur in Spanisch vertrieben.\n\nEs wird benutzt in:\n\n\nEs wurden bisher mehr als 500.000 Exemplare ausgeliefert.\n\nDie aktuelle stabile Ausgabe ist Version 8.0, die am 13. März 2012 veröffentlicht wurde und auch als 64-bit-System verfügbar ist. Während frühere Versionen auf einem Debian-System basierten, baut Guadalinex seit der im Juni 2006 erschienenen Version 3.0 auf Ubuntu auf.\n\nDie Wahl des Maskottchens wird für jede Version durch eine Umfrage durchgeführt. Alle Tiere, die in der Umfrage vertreten sind, gibt es in den verschiedenen Provinzen von Andalusien.\n\n\n"}
{"id": "1452106", "url": "https://de.wikipedia.org/wiki?curid=1452106", "title": "OpenZaurus", "text": "OpenZaurus\n\nOpenZaurus ist eine auf OpenEmbedded basierende Embedded-Linux-Distribution für fast die gesamte Sharp Zaurus PDA-Reihe. An graphischen Desktops werden GPE und OPIE unterstützt (experimentelle Unterstützung existiert außerdem für Xfce und E17). Des Weiteren wird auch ein reines Textkonsolen-Image ohne graphischen Desktop angeboten.\n\nOpenZaurus wurde als alternatives Betriebssystem-Image für den Sharp Zaurus PDA entwickelt. Die ursprüngliche Absicht des Projektes war es, ein Image zu erstellen, das den Wünschen der Entwickler-Community eher entsprach als das originale Sharp-Image. Anfangs wurde einfach ebendieser SharpROM als Ausgangsbasis genommen und auf dessen Grundlage Veränderungen gemacht, Fehler beseitigt, Software hinzugefügt oder auch entfernt, um das gesamte Paket freier zu machen.\n\nKurze Zeit später wurde es von Grund auf neu entworfen und war lange Zeit eine an Debian angelehnte Embedded-Linux-Distribution. In dieser Hinsicht war und ist OpenZaurus recht ähnlich wie andere Debian-basierten Embedded-Linux-Distributionen wie z. B. Familiar Linux für den iPaq. Tatsächlich basieren beide Projekte mittlerweile auf dem OpenEmbedded-Buildsystem, das für beide Distributionen (und viele andere eingebettete Systeme) die Softwarebasis und eben das \"build system\" bereitstellt.\n\nAm 26. April 2007 wurde bekanntgegeben, dass die Arbeit an OpenZaurus zugunsten des Ångström-Projektes eingestellt wird.\n\n"}
{"id": "1457951", "url": "https://de.wikipedia.org/wiki?curid=1457951", "title": "Floyd-Steinberg-Algorithmus", "text": "Floyd-Steinberg-Algorithmus\n\nDer Floyd-Steinberg-Algorithmus ist ein erstmals 1976 von Robert W. Floyd und Louis Steinberg veröffentlichter Dithering-Algorithmus. In der Bildbearbeitung findet er häufig Einsatz, um die Farbtiefe eines Bildes zu verringern (zum Beispiel beim Abspeichern einer GIF-Datei) ohne dabei den ursprünglichen Farbeindruck vollständig zu verlieren.\n\nDer Algorithmus arbeitet nach dem Fehlerdiffusionsverfahren (\"error diffusion\"), d. h. der bei der Quantisierung auftretende Fehler (die Differenz zwischen Ausgangswert und quantisiertem Wert) eines jeden Pixels wird nach einem festen Schema auf die umliegenden Pixel verteilt. Dadurch erreicht der Algorithmus eine bessere Detailgenauigkeit als \"Ordered-Dither-\"Verfahren mit einer starren Maske. Der Fehler jedes Pixels \"P\" wird anteilig nach dem folgenden Schema auf die umliegenden Pixel verteilt:\n\nMit dieser Verteilung kann der Algorithmus ohne separaten Puffer die gesamte Eingabe in einem einzigen Durchlauf abarbeiten. Bereits verarbeitete Pixel werden nicht geändert, während noch abzuarbeitende Pixel entsprechend den auftretenden Quantisierungsfehlern beeinflusst werden.\n\nIn Pseudocode formuliert:\n\nDie Diffusionskoeffizienten haben die Eigenschaft, dass im Falle mehrerer Pixel, die genau in der Mitte zwischen den zwei nächstgelegenen Farben der Zielpalette liegen, ein Schachbrett-artiges Muster entsteht. Ein Schwarz-Weiß-Dithering einer zu 50 Prozent grauen Fläche ergäbe demnach ein richtiges Schachbrett-Muster.\n\n"}
{"id": "1459315", "url": "https://de.wikipedia.org/wiki?curid=1459315", "title": "A 5120", "text": "A 5120\n\nDer A 5120 der Firma Robotron war ein DDR-Bürocomputer zur Text- und Datenverarbeitung. Das Gerät wurde ab dem Jahre 1982 im Volkseigenen Betrieb \"VEB Buchungsmaschinenwerk Karl-Marx-Stadt\" hergestellt.\nDer Preis für diesen Computer lag – abhängig von der Ausstattung – zwischen rund 27.000 und 40.000 Mark. Er wurde in der DDR nur an Betriebe und Institutionen ausgeliefert.\n\nAb 1986 kam eine neue Version des Gerätes als A 5120.16 mit dem 16-Bit-Mikroprozessor U8000 (Zilog-Z8000-Nachbau) auf den Markt. In diesem Gerät fungierte das Originalsystem als 8-Bit-Subsystem. Der 16-Bit-Teil bestand aus zwei zusätzlichen Steckkarten (CPU und 256kByte DRAM) und ermöglichte die Nutzung des leistungsfähigen Betriebssystems MUTOS8000 (Unix-System III-Derivat). Der Preis für dieses Modell lag zwischen etwa 32.000 und 48.000 Mark.\n\nInsgesamt wurden vom A 5120 und A 5120.16 17.000 Exemplare hergestellt.\n\nAls Massenspeicher dienten anfänglich Kassettenlaufwerke und 8-Zoll-Diskettenlaufwerke. Diese Modelle wurden recht schnell durch Modelle mit zwei bis drei 5,25-Zoll-Laufwerken abgelöst. Eingebaut ins Gerät war ein Grün-Monitor, der anfangs 64×16 Zeichen, später 80×25 Zeichen darstellten konnte. Grafikfähig war der Rechner nicht. Der Computer basierte auf dem 8-bit-K-1520-Bus-System, als Prozessor kam ein U880 (Z80-Nachbau) zum Einsatz. Der Arbeitsspeicher hatte eine Größe von bis zu 64 kByte. Zusätzlich gab es IFSS- und V24-Schnittstellen.\n\nAuf Seiten des Betriebssystems waren die K1520-Computer, zu denen auch der A5120 gehörte, auf das Betriebssystem SIOS1526 zugeschnitten, eine Eigenentwicklung des Kombinates Robotron. Außerdem kam anfänglich das Betriebssystem BCU880 (ein CP/M-artiges Betriebssystem) zum Einsatz. BCU880 wurde allerdings sehr schnell durch SCP1526 (ebenso ein CP/M-System) abgelöst, das zum meisteingesetzten Betriebssystem auf diesem Computer wurde.\nVon einigen Betrieben wurden auch eigene CP/M-Abwandlungen entwickelt, wie beispielsweise CP/A (Akademie der Wissenschaften, Berlin) oder DAC1526 (VEB Dampferzeugerbau). Als weiteres Betriebssystem gab es UDOS1526, ein Betriebssystem, das hauptsächlich für die Programmentwicklung gedacht war.\n\nAls Software im Büro konnte der Anwender unter SCP beispielsweise REDABAS (Datenbanksoftware, entspricht dBASE) und TP (Textverarbeitung, entspricht WordStar) verwenden.\n\n"}
{"id": "1460675", "url": "https://de.wikipedia.org/wiki?curid=1460675", "title": "OrCAD", "text": "OrCAD\n\nOrCAD ist ein von der in San José (Kalifornien) angesiedelten Firma Cadence Design Systems vertriebenes Programmpaket, das im Bereich EDA zum Entwickeln von elektronischen Schaltungen eingesetzt wird. \n\nEs umfasst im Wesentlichen ein Programm für die Schaltplaneingabe (\"OrCAD Capture\"), ein Programm zum manuellen und automatischen Entflechten von Leiterplatten (\"OrCAD PCB Designer\") und einen Schaltungsimulator PSpice für das Simulieren der Schaltungen. Die ältere Version OrCAD Layout wurde 2007 durch den zu Allegro kompatiblen OrCAD PCB Editor ersetzt. Dabei kam der Autorouter Specctra dazu. Seit 2010 wurden viele Funktionen aus Allegro, wie z. B. ein SI-Simulator, in die OrCAD-Produkte eingebunden und der Preis erheblich gesenkt.\nAuf dem OrCAD Marketplace gibt es viele Apps, die zum Teil kostenlos angeboten werden. Damit lassen sich z. B. Barcodes generieren und spezielle Reports generieren.\nSeit dem Release 16.6 veröffentlicht Cadence über seine Distributoren jedes Quartal eine neue Version mit neuen Funktionen. Die sogenannten QIRs (quarterly incremental releases) können einfach nachgeladen werden. Es sind neue Funktionen für Capture, PCB Editor und PSpice enthalten. Mit der Skriptsprache TCL sind neue Funktionen in OrCAD Capture, z. B. Intelligente PDF-Ausgabe von Stromlaufplänen verfügbar.\n\nDer Name ist eine zusammengesetzte Abkürzung aus den ersten beiden Buchstaben des Ursprungs-Bundesstaates der Entwicklerfirma Oregon und der Abkürzung für Computer-aided design.\n\n"}
{"id": "1462471", "url": "https://de.wikipedia.org/wiki?curid=1462471", "title": "Singular (Computeralgebrasystem)", "text": "Singular (Computeralgebrasystem)\n\nSingular (Eigenschreibweise: Singular) ist ein Computeralgebrasystem für polynomiale Berechnungen \nmit Schwerpunkt auf den Gebieten kommutative Algebra, algebraische Geometrie und Singularitätentheorie. Es enthält eine intuitive C-ähnliche Programmiersprache mit verschiedenen Datentypen für das Rechnen in Polynomringen. Dies ermöglicht dem Benutzer, eigene Bibliotheken zu schreiben. Das Softwarepaket enthält eine Vielzahl solcher Bibliotheken für verschiedene Anwendungen. Elementare Algorithmen sind aus Effizienzgründen im Singular-Kernel in C++ oder C implementiert.\n\nSingular wird beim Fachbereich Mathematik der Technischen Universität Kaiserslautern unter der Leitung von Wolfram Decker, Gert-Martin Greuel, Gerhard Pfister und Hans Schönemann entwickelt und steht unter der freien GNU General Public License.\nDabei ist es für die meisten Hard- und Softwareplattformen verfügbar, neben verschiedenen Unix-Varianten wie Linux und macOS auch für Windows (mittels Cygwin oder Virtualisierung).\n\n"}
{"id": "1466362", "url": "https://de.wikipedia.org/wiki?curid=1466362", "title": "Farbreduktion", "text": "Farbreduktion\n\nFarbquantisierung oder Farbreduktion nennt man Quantisierungsverfahren der Computergrafik, die die Anzahl der Farben einer Rastergrafik verringern. \n\nFarbreduktion wird angewandt, um von dem bei geringerer Farbtiefe kleineren Speicherplatz einer Grafikdatei zu profitieren. Insbesondere indizierte Farben ermöglichen relativ kompakte Dateien. Farbreduktion verliert an Bedeutung, da heutige Grafikkarten und Bildschirme Echtfarben unterstützen. \n\nEs gibt verschiedene Algorithmen, um die formula_1 besten Farben (Repräsentanten) für das zu bearbeitende Bild zu ermitteln. Dazu zählen:\n\nDie Verringerung der Farben erzeugt Kanten, die vom Auge bevorzugt wahrgenommen werden (Machsche Streifen) und den Bildeindruck stören. Das sogenannte Dithern verschmiert die Übergänge durch selektives Verrauschen und verbessert subjektiv die Bildqualität.\n\nDie Farbreduktion ist beim Speichern in einem Grafikformat, das nur eine geringere Farbtiefe als das Originalbild unterstützt, notwendig. Beispielsweise unterstützt das GIF-Format 256 unterschiedliche Farben. Graustufenbilder mit einer Farbtiefe von 8 Bit können verlustfrei gespeichert werden, Farbfotos mit einer Farbtiefe von 24 Bit pro Pixel hingegen müssen vor der Speicherung auf 256 Farben reduziert werden.\n\n"}
{"id": "1472424", "url": "https://de.wikipedia.org/wiki?curid=1472424", "title": "MagSafe", "text": "MagSafe\n\nMagSafe ist die Bezeichnung des proprietären Netzteilanschlusses für Notebooks des Herstellers Apple. Er wurde am 10. Januar 2006 zusammen mit dem MacBook Pro auf der Macworld Expo vorgestellt und wird seitdem im MacBook Pro, MacBook Air und in vor 2015 verkauften MacBook-Modellen verbaut. Primäres Ziel dieser schnell zu lösenden Steckverbindung ist das Vermeiden von Sturzunfällen im Zusammenhang mit dem Stolpern über das Netzteilkabel.\n\nAb Juni 2012 verbaut Apple einen \"MagSafe 2\" genannten, flacheren Anschluss, um die Dicke von MacBook Pro und MacBook Air reduzieren zu können. Diese zwei Typen sind – rein geometrisch – nicht kompatibel.\n\nMit der Erneuerung des MacBook Pro im Oktober 2016 wurde auch hier der MagSafe-Anschluss durch USB-C ersetzt, wobei der Stolperschutz nun nicht mehr gegeben ist. \n\nDas Halten des Steckers in der zugehörigen Buchse geschieht mittels eines Dauermagneten, der ringförmig um die Kontakte angeordnet ist. Der Netzteilsteckers wird nicht eingesteckt wie bei gewöhnlichen Steckverbindern, sondern in eine seichte Vertiefung am Notebook eingelegt und dort von einem mit einer dünnen Kunststoffschicht belegten Dauermagneten gehalten. Dabei werden die stromführenden, kabelseitig federgelagerten und vergoldeten Kontakte mit denen der Buchse in Berührung gebracht und etwas einfedernd angedrückt. Bei einem kräftigen Zug am Kabel, gleich in welche Richtung, löst sich die Verbindung und eliminiert damit ein Unfallrisiko mit meist größerer Schadensfolge, bei dem ein Computer mit festsitzendem, gesteckten Kabel vom Tisch gerissen würde. Das reduzierte Unfallrisiko gilt nicht für die l-förmige Bauform.\n\nDiese Konstruktion verhindert außerdem, dass die Strombuchse im Inneren an der Platine des Rechners ausbricht und einen Wackelkontakt verursacht. Dies kann bei herkömmlichen Verbindern vorkommen, wenn der Stecker außen gekippt wird und geschieht insbesondere dann, wenn der Stecker vor dem Einpacken eines Notebook in eine Tasche nicht abgezogen wird. Ähnliches erleidet die Steckverbindung bei dem Betrieb des Notebooks auf einer weichen Unterlage, auf der das Gerät einsinkt, der Stecker jedoch nicht gleichermaßen.\n\nDer rechteckige Stecker ist symmetrisch aufgebaut, er kann also in beide Richtungen eingelegt werden. Auf den seitlichen Flächen des Steckers sind LEDs angebracht, die den Zustand des Ladevorgangs signalisieren: orange bedeutet ladend, grün kein Ladevorgang. Durch die zwar in einer Mulde, doch recht offen liegenden Kontakte eignet sich das Steckersystem nur für Kleinspannungen.\n\nDie Leuchtdioden leuchten nicht zwangsweise, wenn Ladestrom fließt – wie bei frühen Batterieladegeräten, wo sie direkt im Stromkreis lagen. Dunkle Dioden können auf einen Fehler beim Mittelkontakt zurückgehen und die Stromversorgung des Geräts kann eventuell dennoch funktionieren.\n\nApple hält ein Patent auf diese magnetische Steckverbindung. Eine ähnliche Konstruktion fand bereits früher bei verschiedenen Küchengeräten Verwendung. Im August 2011 wurde bekannt, dass Apple ein Patent für einen MagSafe-Anschluss für das iPad beantragt hat. Wie bei Apple-Patenten üblich wurde es jedoch nie in Seriengeräten umgesetzt, ein magnetischer Anschluss zur Stromversorgung ist nur bei MacBooks bis 2016 zu finden.\n\nDas Netzteil mit dem MagSafe-Stecker hat sekundärseitig das fest angeschlossene MagSafe-Kabel, primärseitig einen wechselbaren und länderspezifischen Netzstecker. Im Lieferumfang des MagSafe-Netzteils befinden sich in Kontinentaleuropa ein kleiner Eurostecker und ein langes Anschlusskabel mit einem Stecker mit Schutzkontakt, in Deutschland und Österreich ein Schukostecker, in der Schweiz ein T12-Stecker nach SEV 1011. Es ist in verschiedenen elektrischen Leistungsstufen, je nach Notebookmodell, von Apple verkauft worden.\n\nDie Verbindung hat fünf Pins, die linear und symmetrisch zur Mitte angeordnet sind. Der mittlere Pin (3) dient zur Steuerung der LEDs am Stecker. Auf den inneren Pins (2) und (4) befindet sich der Pluspol mit ca. 14,5 V bis 20 V je nach Modell. Ganz außen jeweils der Masseanschluss (1) und (5). Der äußere Metallrand dient der magnetischen Verbindung und zusätzlich der elektrischen Abschirmung.\n\nÜber den Stecker wird eine serielle Datenleitung geschaltet, um Netzteildaten auszutauschen und die LEDs anzusteuern. Der Chip hierzu befindet sich im MagSafe-Stecker selbst. Über das Macbook kann die Seriennummer und die Leistung des Netzteils ausgelesen werden, diese Informationen befinden sich im Chip des MagSafe-Steckers.\n\n2007 musste Apple die Netzadapter zurückrufen, da es immer wieder zu Kabelbrüchen kam. Teilweise waren die Kabel so dünn, dass es an diesen Stellen zu beginnenden Kabelbränden kam und die Kunststoffisolierung schmolz. Apple hat daraufhin die mechanische Festigkeit und Leiterführung verbessert. Das Computermagazin c't bezeichnet die Probleme als „Sollbruchstellen“ und im Internet finden sich zahlreiche Anleitungen, wie man bei abgelaufener Garantie selbst mittels eines Lötkolbens das Gerät reparieren kann. Die Probleme betreffen jedoch nicht das Stecksystem selbst, sondern die zu schwach dimensionierte Kabelzuführung.\n"}
