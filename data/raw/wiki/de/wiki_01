{"id": "11979", "url": "https://de.wikipedia.org/wiki?curid=11979", "title": "Atari ST", "text": "Atari ST\n\nAtari ST ist eine Serie von Heim- bzw. Personal Computern der Atari Corporation, die von 1985 bis 1994 produziert wurde. Die ST-Serie eignete sich durch die grafische Oberfläche \"GEM\" unter anderem für professionelle Büroanwendungen und wurde wegen der serienmäßig vorhandenen MIDI-Schnittstelle \"der\" Standardcomputer in kleinen und großen Tonstudios genutzt. Die Abkürzung „ST“ steht dabei für \"Sixteen/Thirty-Two\" (16/32), da der verwendete Hauptprozessor, der Motorola 68000, einen 16 Bit breiten Datenbus hat und intern mit 32 Bit arbeitet.\n\nDie ST-Serie war eine Alternative zu den wesentlich teureren Apple-Macintosh-Modellen und stand in direkter Konkurrenz zu IBM-PC-kompatiblen Computern und der Amiga-Reihe der Firma Commodore.\n\nDer Atari ST war eines der ersten verbreiteten Modelle mit einer grafischen Benutzeroberfläche, dem GEM von Digital Research. Die Hauptspeichergröße lag zwischen 512 KiB (520ST) und 4 MiB (Mega ST4), diese Zahl wurde, nach Aufrunden, Teil der Modellbezeichnung (520ST – 512 KiB; 1040ST – 1024 KiB = 1 MiB).\n\nEinzige Ausnahmen bildeten der 260ST (wurde mit 512 KiB ausgeliefert) und der 520ST+ (1 MiB). Der 260ST sollte – getreu seiner Bezeichnung – nur mit 256 KiB ausgeliefert werden, in der Endphase der Entwicklung stellte sich jedoch heraus, dass 256 KiB definitiv nicht ausreichen würden, um den Rechner mit TOS sinnvoll zu betreiben. Da jedoch die Werbung bereits angelaufen war, wurde er kurzerhand mit 512 KiB ausgeliefert. Bei den ersten in Europa erhältlichen 260ST wurde wie beim ersten amerikanischen 520ST das TOS von Diskette nachgeladen, was die Größe des verfügbaren Speichers stark reduzierte. Der 260ST unterschied sich kaum vom 520ST. Eigentlich bestand der einzige Unterschied darin, dass der 520ST das TOS in Festwertspeichern (ROM) mitbrachte, wobei die ersten 520ST und ST+ noch ohne solche Speicherbausteine geliefert wurden, weil diese zum Produktstart noch nicht verfügbar waren. Die Nachrüstung war aber auch beim 260ST einfach, da die Sockel bereits vorhanden waren. Der Ur-520ST verschwand bereits ein halbes Jahr nach Erscheinen vom Markt, ihm folgte ein Jahr später der 260ST. 1989 wurden sämtliche STs ohne Diskettenlaufwerk sowie der 1040STF eingestellt, Ende 1990 der MegaST und im Januar 1994 alle übrigen Geräte (zu dem Zeitpunkt bestand die Modellpalette aus 1040STFM, 1040STE, MegaSTE und den High-end-Rechnern Falcon 030 und TT 030).\n\nDie Konzeptänderungen nach Abschluss der Entwicklungsphase zeigen sich auch beim Modell 520ST+. Hier fanden die zusätzlichen Speicherbausteine auf der Hauptplatine keinen Platz, so dass sie von Hand huckepack auf die Speicherschaltkreise der regulären Bestückung gelötet werden mussten. Einzelne Pins (RAS, CAS) dieser zusätzlichen Schaltkreise wurden nach oben gebogen und frei verdrahtet.\n\nZusätzliche Buchstaben gaben weitere Ausstattungsmerkmale an: „F“ im Namen gab ein internes Diskettenlaufwerk an, „M“ einen HF-Modulator. „+“ bedeutete eine Speichererweiterung (520ST+) oder andere Erweiterungen wie beim nicht veröffentlichten 1040 STE+. „E“ kommt von \"enhanced\", diese Rechner waren um einige Funktionen, beispielsweise die Fähigkeit, Töne in Stereo wiederzugeben, und eine auf 4096 Töne erweiterte Farbpalette erweitert worden. Die Modelle „LST“ (Stacy) und „NST“ (ST Book) bezeichnen tragbare Geräte.\n\nBis November 1985 wurde der Atari ST mit dem Betriebssystem auf Diskette ausgeliefert (TOS 1.0), spätere Modelle hatten das Betriebssystem im Festwertspeicher eingebaut.\n\nDer Software-Emulator CPMZ80 für den Zilog-Z80-Hauptprozessor und das Betriebssystem CP/M 2.2 wurden kostenlos von Atari mitgeliefert. Dadurch konnten, vor allem zu Beginn, die damals weitverbreiteten 8-Bit-Programme für CP/M, wie z. B. WordStar, dBASE, Microsoft Multiplan, Turbo Pascal und andere, auf dem Atari ST weiterbenutzt werden, und die mit diesen Programmen erzeugten Dokumente, Datenbanken, Quellen und Daten blieben weiter verwendbar.\n\nDie MegaST-Serie besaß eine abgesetzte Tastatur und einen Hauptspeicher von bis zu 4 MiB. Festplattenlaufwerke waren ebenfalls verfügbar (anfangs mit MFM-Verfahren und 20 MB) und direkt an den Atari ST anschließbar (DMA-Port, auch ACSI-Port (SCSI-Variante, steht für „Atari Computer System Interface“) genannt).\n\nDer Atari ST besaß die Möglichkeit, entweder einen hochauflösenden Schwarzweiß- oder einen geringer auflösenden Farbbildschirm anzuschließen. Die Farbauflösung betrug 320×200 Bildpunkte bei 16 Farben und 640×200 Bildpunkte bei vier Farben, jeweils aus einer Palette von 512 Farben (bzw. 4096 beim STE).\n\nDer weit verbreitete monochrome Monitor SM124 hatte eine Auflösung von 640×400 Bildpunkten bei 70 Hz Bildwiederholfrequenz. Dies waren für die damalige Zeit hervorragende Werte, im Bereich der IBM-PC-kompatiblen Computer gab es gerade CGA, HGC und für besonders teure Rechner EGA, das nur 640×350 Bildpunkte und 60 Hz Bildwiederholfrequenz bot. Die Nachfolgemodelle SM125 und SM14x stellten auch 640×400 in 70 Hz dar.\n\nDer Rechner wurde besonders im CAD- oder Desktop-Publishing-Bereich populär, insbesondere als Atari und andere Hersteller die hochauflösenden 19″-Monochrom-Monitore auf den Markt brachten. Im deutschsprachigen Raum überwogen auch ansonsten eher Büroanwendungen wie Textverarbeitung oder Tabellenkalkulation. Insbesondere für die Naturwissenschaften stand mit dem Textverarbeitungsprogramm Signum! eine Software zur Verfügung, die in dieser Zeit nahezu Alleinstellungsmerkmale aufwies.\n\nIn den Vereinigten Staaten wurde der ST vorwiegend mit Farbmonitor eingesetzt und galt eher als Spiele- und Demomaschine (siehe: Atari Demos). Weltweit brachte dem Atari ST eine fest eingebaute MIDI-Schnittstelle eine weite Verbreitung bei Musikern und in Tonstudios ein. Dazu gehörte auch Mike Oldfield, der den Atari ST überwiegend für die Arrangements seiner Songs nutzte. Auf dem Album \"Earth Moving\" wurde die Verwendung des Atari ST auf der Rückseite des Covers angegeben. Auch Depeche Mode und Fleetwood Mac nutzten in dieser Zeit den ST. Außerdem war der Atari ST sozusagen ein Mittler zwischen den Welten. Das Dateisystem der Disketten war mit dem von MS-DOS weitgehend kompatibel, so dass man beispielsweise Zugriff auf Textdateien hatte, die auf einem PC erstellt wurden. Es gab auch einen Apple-Emulator sowie einen CP/M- und Z80-Emulator, und er wurde – mit entsprechender Software versehen – als intelligentes Terminal und Entwicklerstation an verschiedensten Großrechnern und Mini-Computern von Hewlett-Packard sowie Workstations von Texas Instruments und Hewlett-Packard eingesetzt. Bemerkenswert am Betriebssystem der ST-Familie war die Vielfalt der bereitgestellten Schnittstellen. So war von Anfang an an den Anschluss von Grafiktabletts und Kameras gedacht, was zur damaligen Zeit einzigartig war, zumal entsprechende Hardware auf dem Markt überhaupt nicht verfügbar war.\n\nDer Atari ST war zum im Jahr 1984 dreimal so teuren Apple Macintosh eine für breite Massen erschwingliche Alternative und stand in Konkurrenz zum etwas später auf den Markt gekommenen Amiga von Commodore.\n\nMehrere Fachzeitschriften wie \"ST-Computer\", \"ST-Format\", \"ST Magazin\", \"TOS\", \"XEST\", \"ATOS\", \"68000er\" oder \"Atari Inside\" versorgten die Nutzer mit Informationen zu diesem Rechner.\n\n\nStandardmäßig bei allen ST-Varianten vorhanden sind:\n\nEinige Modellvarianten haben darüber hinaus noch zusätzliche Anschlüsse:\n\n\n\n\n\n\n\n\nIm Internet kursiert eine Fülle von Emulatoren für Atari ST. Die meisten dieser Emulatoren benötigen für ihre Funktion jedoch ein Abbild originaler Atari-ROMs, daher dürfen sie aus lizenzrechtlichen Gründen nur dann benutzt werden, wenn der Eigentümer des Rechners selbst auch Eigner eines Atari ST (bzw. seiner ROMs) ist. Als legaler Ausweg bleibt die Verwendung nachprogrammierter, frei erhältlicher TOS-Versionen wie EmuTOS.\n\nAtari-Disketten können meist vom PC (oder einem älteren Apple Macintosh) gelesen werden. Schwierigkeiten bereiten jedoch Umlaute in Pfad- und Dateinamen aufgrund der unterschiedlichen Zeichensätze sowie höher formatierte Disketten (d. h. mit mehr als 80 Spuren und/oder mehr als 9 Sektoren pro Spur); diese funktionieren zuverlässig nur mit einem Original-Atari-Rechner und müssen daher umkopiert werden. Falls kein entsprechender Atari-Rechner zur Verfügung steht, kann dies z. B. mit der Live-CD von ARAnyM erfolgen. Dieser Weg, der lizenzrechtlich unproblematisch ist und der keinerlei Installation auf dem „Wirts-PC“ voraussetzt, macht es möglich, auf einer PC-Hardware auch ältere Atari-Diskettenformate zu lesen und die Dateien auf DOS-formatierte Disketten zu kopieren. Eine weitere Möglichkeit: 2DD-Disketten, die unter MS-DOS mit dem Befehl \"format a: /u /f:720\" formatiert werden, können sowohl vom Atari als auch von Windows gelesen und geschrieben werden und daher als Transportmedium zwischen beiden Computerwelten dienen. Windows XP unterstützt den Parameter \"/f:720\" nicht mehr, deswegen muss man auf \"format /t:80 /n:9 a:\" ausweichen.\n\nDa aktuellen PCs das Diskettenlaufwerk fehlt, bleiben hier die Lösungen, die Daten entweder auf CD zu brennen, ein Diskettenlaufwerk nachzurüsten bzw. ein USB-Laufwerk anzuschließen, Wechselmedien wie ZIP-, JAZ- oder DVD-RAM-Laufwerke, Speicherkarten wie CF und SD (der Atari kann mit geeigneter Software wie dem HDDRiver oder BIGDOS PC-kompatible Fest- und Wechselplatten sowie Speicherkarten lesen und schreiben) oder den Atari und den aktuellen Computer mittels Netzwerkkarte am Atari (EtherNEC bzw. EtherNEA) und meist vorhandenem Netzwerkanschluss zu verbinden.\n\nEin Open-Source-Emulator ist Hatari.\n\nEin professioneller Atari-Emulator für Apple Macintosh ist MagiCMac(X). Dieser Emulator stellt Calamus u. a. eine Drucker-Schnittstelle zur Verfügung, so dass Calamus jeden Apple-seitig installierten Drucker erkennen und ansprechen kann. Auf der Intel-MagicMac-Version funktioniert dieser Druckertreiber jedoch nicht mehr. Calamus-SL-Anwender behelfen sich mit dem PDF-Export und drucken die erzeugten PDF-Dateien dann unter OS X aus.\n\nFür Windows-Betriebssysteme gibt es das Pendant MagiC PC. Auf Windows 7-32 Bit läuft der STemulator.\n\nFür die Apple-Computer \"Performa 450\", \"475\" und \"630\" existierte zudem (1995) mit \"Mac STout\" eine MagicMac-kompatible Schnittstellenkarte, die die beim Macintosh nicht vorhandene parallele (Centronics-/Drucker-)Schnittstelle sowie eine serielle Schnittstelle mit originalen Atari-Chips nachrüstete. In Verbindung mit MagiC Mac konnten Programme wie Wordplus, Signum!, Script oder Calamus so mit PC-Druckern drucken, und Atari-Programme liefen auf dem Macintosh mit etwa vierfacher Geschwindigkeit eines Atari TT. McSTout existierte zudem in einer Variante, die zudem auch noch MIDI-Ports bereitstellte.\n\n\n\nSiehe auch .\n\n\n"}
{"id": "12243", "url": "https://de.wikipedia.org/wiki?curid=12243", "title": "AbiWord", "text": "AbiWord\n\nAbiWord ist ein freies, GPL-lizenziertes Textverarbeitungsprogramm, das unter Linux verfügbar ist. Für andere Betriebssysteme wie macOS, SkyOS und AmigaOS 4 wird AbiWord nicht weiter gepflegt. Aufgrund des Mangels an Windows-Entwicklern ist die aktuelle Version für Windows noch Abiword 2.8.6. Der Name „AbiWord“ ist abgeleitet von der Wurzel des spanischen Wortes „abierto“ für „offen“.\n\nDie Entwicklung des Programms wurde Anfang 1998 von einem jungen Unternehmen in Illinois begonnen.\n\nAbiWord war das erste Textverarbeitungsprogramm, das Dokumente in XML speicherte. Die erste offizielle Version wurde im April 2002 freigegeben und veröffentlicht.\n\nAbiWord beschränkt sich auf die Grundfunktionen der Textverarbeitung wie Formatvorlagen, Tabellen, Fußnoten und ein automatisch generiertes Inhaltsverzeichnis. Die Bedienoberfläche orientiert sich an Microsoft Word. Die Software ist plattformübergreifend, durch Plug-ins erweiterbar und bietet einen einfachen Datenaustausch mit anderen Office-Anwendungen. AbiWord ist in vielen Sprachen verfügbar; es existieren Wörterbücher in 30 verschiedenen Sprachen. Das Programm unterstützt MathML zum Einfügen mathematischer Ausdrücke und enthält zudem ein Wikipedia-Plug-in, um Einträge nachzuschlagen.\n\nAbiWord verfügt über Import/Export-Filter für die Dateiformate RTF, MS Word, OpenOffice.org Writer, HTML und LaTeX. Ab Version 2.4.2 gibt es auch einen vollständigen Import-/Export-Filter für das OpenDocument-Format.\n\nEine portable Version für Windows wird auch angeboten. Diese ist ohne Installation von Festplatte oder USB-Stick startbar. Ab Version 2.4.2 unterstützt AbiWord das Nokia 770 Internet Tablet.\n\nIm Rahmen des Projektes „One laptop per child“ am MIT soll AbiWord als Standardtextverarbeitung des OLPC XO-1 (sog. „100-Dollar-Laptop“) installiert werden. Falls die geplante Gesamtproduktion von 100–200 Millionen Laptops erreicht wird, ist zukünftig in den teilnehmenden Entwicklungsländern von einer hohen Verbreitung des Programmes auszugehen.\n\nAb der Version 2.8.1 werden für AbiWord keine GNOME-Abhängigkeiten (z. B. gnome-print) mehr benötigt.\n\n\n"}
{"id": "13167", "url": "https://de.wikipedia.org/wiki?curid=13167", "title": "C’t", "text": "C’t\n\nDie c’t – \"magazin für computertechnik,\" gegründet 1983, gehört zu den auflagenstärksten und einflussreichsten deutschen Computerzeitschriften. Die \"c’t\" wird in deutscher und niederländischer Sprache (\"c’t – magazine voor computertechniek\") herausgegeben. \n\nDie c’t erscheint im Heise Zeitschriften Verlag in Hannover; Herausgeber sind Christian Heise, Ansgar Heise und Christian Persson, Chefredakteur ist Jürgen Rink.\n\nDie Bezeichnung „c’t“ stand ursprünglich für \"computing today,\" die Bezeichnung für einen 16-seitigen Bestandteil der früher durch den Heise-Verlag herausgegebenen Elektronik-Zeitschrift elrad. Im Oktober 1983 wurde \"c’t\" als eigene, unabhängige Zeitschrift ausgegliedert. Die Erstausgabe 12/83 erschien am 19. Oktober auf der Systems in München. Zunächst erschien die Zeitschrift monatlich, 1997 wurde auf 14-tägliche Erscheinungsweise umgestellt.\n\nIn den Niederlanden wird die \"c’t\" von der \"F&L Publishing Group\" in Nijmegen publiziert. Chefredakteur ist Patrick Smits. \n\nZwischen Mai und Dezember 2008 erschien unter dem Namen \"c’t – Журнал о компьютерной технике\" eine russische Lizenzausgabe in Moskau.\nHerausgeber der russischen Lizenzausgabe war der Verlag \"Business Media Communications\".\n\nDie \"c’t\" präsentierte sich auf dem Markt der Computerzeitschriften zunächst als „hardwareorientiertes“ Magazin, das technikbegeisterten Besitzern von „Heimcomputern“ der ersten Generation neben Softwareentwicklung die Modifikation gekaufter bis hin zum Selbstbau eigener Hardware erklärte. Vor allem in den 1980er Jahren wurden in der \"c’t\" viele komplexe Hard- und Softwareprojekte veröffentlicht, die den gesamten Bereich der Computer-Hardware abdeckten, ein jüngerer Vertreter dieser Art ist der c’t-Bot. Mit dem Siegeszug der IBM-PC-Architektur und der verschiedenen Microsoft-Betriebssysteme nahmen diese einen zunehmend breiteren Raum in der Berichterstattung ein. Allerdings versteht sich die \"c’t\" nach wie vor als plattform- und betriebssystemunabhängig und widmet sich weiterhin, wenn auch in bescheidenerem Umfang, den Betriebssystemen Linux, macOS, Android und Solaris.\n\nNeben praxisbezogenen Computerthemen richtet die \"c’t\" ihren Fokus seit jeher auf die politischen und gesellschaftlichen Dimensionen der Technikentwicklung. Schon in den Anfangsjahren berichteten kritische Reportagen über pornografische Mailboxinhalte oder die Auswirkungen des damaligen Postmonopols auf Modems und andere Telekommunikationsgeräte.\n\nKontinuierlich gepflegte Themen sind Datenschutz, Zensur und die Rechtsproblematik von Software-Patenten. Kritisch berichtet die \"c’t\" auch über die Trusted Computing Platform Alliance und die Bestrebungen der Industrie, Digital-Rights-Management-Systeme durchzusetzen. Ebenso werden wichtige IT-Großprojekte in Deutschland wie das Autobahnmautsystem, die Software zur Berechnung des 2005 im Zuge der Hartz-IV-Reform eingeführten „Arbeitslosengeldes II“ oder das Thema Wahlcomputer kritisch begleitet und auf Schwachstellen untersucht. Ein halbseitiger Einklinker im Artikel zum 25-Jahre-Jubiläum der Zeitschrift zitiert den Autor von Fefes Blog stellvertretend für die typische Leserschaft u. a. zur Wechselwirkung zwischen Printmedien und Blogs: „Ich bin Heise dankbar, dass sie die ganzen Forenhaftungs-Kämpfe stellvertretend für die armen Blogger geführt haben.“\n\nFester Bestandteil des Magazins ist außerdem die \"„c’t Story“,\" eine Kurzgeschichte, die schon oft von namhaften Science-Fiction-Autoren wie Andreas Eschbach, Helmuth W. Mommers oder Peter Schattschneider verfasst wurde. Die \"„c’t Story“\" ist seit 2010 nahezu die einzige verbliebene professionelle Veröffentlichungsmöglichkeit für Science-Fiction-Kurzgeschichten in einem Magazin in Deutschland.\n\nDie \"c’t\" wendet sich gleichermaßen an fortgeschrittene und ambitionierte Computeranwender, Profis und Spezialisten. Die Leser von \"c’t\" sind „hochqualifizierte Computer-Profis und fungieren außergewöhnlich oft als Ratgeber und Entscheider in Sachen Technik.“\n\nWie in nahezu jeder Computerzeitschrift sind Hardware- und Softwaretests fester Bestandteil der c’t. Im Unterschied zur Konkurrenz nennt das Magazin jedoch keine Testsieger, sondern stellt in einem Test-Fazit Stärken und Schwächen der untersuchten Produkte zusammen. Dem Leser soll damit eine je nach Budget und Einsatzzweck bedarfsgerechte Entscheidung ermöglicht werden.\n\nDer \"c’t\" liegen regelmäßig DVDs bei, die von der \"c’t\"-Redaktion zusammengestellte Programmsammlungen enthalten. Diese „Software-Kollektionen“ von Freeware, Shareware und freier Software sind seit 2003 jeweils Themenschwerpunkten wie „Office“, „Sicherheit“ oder „Linux“ gewidmet. Außerdem erscheint einmal im Jahr eine Sammlung von Computerspielen, die die Redaktion für Kinder und Jugendliche für empfehlenswert hält. Zuvor erschienen zweimal pro Jahr CDs mit einem meist breitgefächerten und themenübergreifenden Spektrum. Vereinzelt werden auch nicht von der Redaktion erstellte Datenträger über die Zeitschrift verteilt, wie z. B. Sicherheits-Tools von Microsoft.\n\nNeben der 14-täglich erscheinenden c’t haben sich Sonderheftausgaben entwickelt. Viermal jährlich erscheinen die \"c’t Digitale Fotografie\" und die \"c’t special Mac & i,\" die beide auch abonniert werden können. Darüber hinaus werden die Sonderhefte \"c’t kompakt, c’t medien\" und \"c’t extra\" herausgegeben. Diese befassen sich schwerpunktmäßig mit einem Thema (z. B. Linux oder Netzwerke). Ende des Jahres 2011 erschien erstmals das Magazin \"c’t Hardware Hacks\" aus dem die eigenständige Zeitschrift hervorging. Seit 2012 erscheint einmal jährlich ein Sonderheft zum mobilen Betriebssystem Android \"c’t Android\". Im Oktober 2018 wurde ein Sonderheft \"Retro\" veröffentlicht, in dem Hard- und Software aus der Zeit vor dem Jahr 2000 beschrieben wurde.\n\nIm ersten Aprilheft jedes Jahres ist ein Aprilscherz-Artikel versteckt, der wegen der differenzierten Darstellung sehr glaubhaft wirkt. So erläuterte im April 2006 ein Artikel die angebliche Verhinderung des Abfilmens von Kinoleinwänden durch Infrarotsequenzen, die Digitalkameras und Camcorder außer Gefecht setzen. Der Aprilscherz der Ausgabe 4/1995 beschrieb einen rot blinkenden Mainboard-Chip, der /dev/null effizienter machen sollte. Im April 2016 wurde ein angebliches Tool vorgestellt, das bei Copy & Paste vor Urheberrechtsverletzungen schützen soll.\n\nWeit über das übliche Publikum hinaus wurde \"c’t\" 1995 bekannt, als zunächst ein Kurztest des Programms „SoftRAM“ der amerikanischen Firma Syncronys mit dem Resultat „wirkungslos“ und der Einschätzung „Placebo-Software“ abgedruckt wurde. Der deutsche Distributor Softline erwirkte eine einstweilige Verfügung, der zufolge der Testbericht in dieser Form nicht mehr verbreitet werden dürfe. c’t-Redaktion und Heise-Verlag konterten eine Ausgabe später mit dem Artikel „Placebo forte“, in dem „SoftRAM“ detailliert auseinandergepflückt und teilweise disassembliert wurde. In diesem Artikel wurde dargelegt, dass die vorgebliche Fähigkeit, den tatsächlich vorhandenen Hauptspeicher unter Windows (durch Laufzeitkompression der Speicherinhalte) zu „verdoppeln“, ein einziger Bluff und das Programm völlig wirkungslos war. Das darauffolgende Presseecho fegte das Produkt nicht nur vom deutschen, sondern auch vom US-Markt.\n\nSchlagzeilen machte \"c’t\" im Jahr 2005 weiterhin mit dem Leserwettbewerb „Hommingberger Gepardenforelle“. Ziel war hierbei, Strategien einer Suchmaschinenoptimierung zu entwickeln und dadurch Rankingmechanismen von Suchmaschinen aufzudecken.\n\nIn der Ausgabe 24/2007 berichtete die \"c’t\" von im Handel befindlichen USB-Speichersticks, die nur Teilbereiche statt der angegebenen ein oder zwei Gigabyte Daten speichern konnten. Die alten Daten wurden beim fortlaufenden Beschreiben von neuen Daten überschrieben.\n\nIm September 2015 enthüllte \"c’t\" unlautere Praktiken des Datingportalbetreibers Lovoo. Nachdem der Zeitschrift etwa 50 GB an E-Mail-Fächern, Screenshots und Quellcode zugespielt worden waren, wurde aufgedeckt, wie der Hersteller mithilfe von Fakeprofilen Nutzer zu In-App-Käufen animiert. Ohne diese hätte die Kommunikation mit vermeintlich interessierten Kontaktpersonen nicht oder nur schwerlich aufrechterhalten werden können.\n\nIm Oktober legte die \"c’t\" nach, indem sie Lovoos Einsatz von Chat-Bots offenlegte.\n\nDie deutschsprachige \"c’t\" erschien seit Ausgabe 12/1983 zunächst monatlich und aufgrund des gewachsenen Umfangs (die bis heute seitenstärkste \"c’t\" war die Aprilausgabe 1997 mit 614 Seiten) seit Ausgabe 11/1997 alle 14 Tage. Die niederländische \"c’t\" erscheint monatlich in Lizenz bei F&L, die auch die niederländische iX und Chip herausgibt.\n\nDie \"c’t\" will sich durch ihre sachliche Berichterstattung und ihre Aufmachung ohne reißerische Schlagzeilen von anderen Computermagazinen wie \"Computer Bild\", \"PC-Welt\" oder \"Chip\" abheben. So wird darauf verzichtet, eine Meldung oder einen Report als Sensation auszulegen und Interessenten so möglicherweise zum Kauf zu animieren.\n\nIm vierten Quartal 2014 lag die durchschnittliche verbreitete Auflage pro Ausgabe nach IVW bei 270.560 Exemplaren. Das sind durchschnittlich 6.315 weniger Hefte pro Ausgabe (–2,3 %) als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um durchschnittlich 7.317 Abonnenten pro Ausgabe auf 221.452 ab (−3,2 %).\n\nDie höchste verkaufte Auflage hatte die \"c’t\" im Jahr 2001 mit fast 390.000 Heften. Seitdem sank die Anzahl der verbreiteten Hefte auf etwa 69 % dieses Wertes. Bis ins Jahr 2009 konnte jedes Jahr ein Anstieg der verkauften Abonnements verzeichnet werden. Seitdem sinkt diese Zahl leicht.\n\nUngewöhnlich ist für eine Publikumszeitschrift die vergleichsweise hohe Abonnentenzahl von derzeit 82,6 Prozent der verbreiteten Auflage. Damit ist die \"c’t\" ein Computermagazin mit einem der höchsten Abonnentenstämme in Europa.\n\nVon 2003 bis Juni 2011 wurde wöchentlich im hr-fernsehen das c’t magazin.tv ausgestrahlt. Die Sendung griff Themen der gedruckten Zeitschriftenausgabe auf; moderiert wurde sie von Mathias Münch und \"c’t\"-Redakteur Georg Schnurer. Zum 25. Juni 2011 wurde die Sendung eingestellt.\n\nGeorg Schnurer stand in der Radio-Eins-Sendung \"Escape – Der Experte\" (RBB) als Experte zur Verfügung. Der letzte Beitrag dieser Reihe erschien am 28. Januar 2012.\n\n"}
{"id": "13181", "url": "https://de.wikipedia.org/wiki?curid=13181", "title": "Gzip", "text": "Gzip\n\ngzip ist ein freies Kompressionsprogramm, das, ebenso wie das entsprechende Dateiformat gzip, praktisch für alle Computerbetriebssysteme verfügbar ist (unter den Bedingungen der GPL auch im Quelltext).\n\nAllgemein ist \"gzip\" die Kurzform für „GNU zip“, wobei „zip“ vom englischen Wort für den Reißverschluss entlehnt wurde. OpenBSD hat eine BSD-lizenzierte Reimplementierung unter den Namen codice_1 sowie codice_2 vorgenommen, die völlig kompatibel zu den GNU-Werkzeugen ist.\n\ngzip bietet einen guten Kompressionsgrad und ist frei von patentierten Algorithmen. Es wurde ursprünglich von Jean-Loup Gailly entwickelt, um das unter Unix verwendete compress zu ersetzen. Mark Adler schrieb das Dekompressionsprogramm gunzip.\n\ngzip basiert auf dem \"Deflate\"-Algorithmus, der eine Kombination aus LZ77 und Huffman-Kodierung ist. Deflate wurde als Reaktion auf die Patente entwickelt, die auf LZW und anderen Kompressionsalgorithmen bestanden. Auch das ZIP-Dateiformat verwendet hauptsächlich Deflate zur Komprimierung, darf aber ansonsten nicht mit gzip verwechselt werden.\n\nDie Fenstergröße bei gzip beträgt 32 KiB. Wenn eine Abfolge von Bytes sich in den vorherigen 32 KiB nicht wiederholt, wird sie unkomprimiert in der .gz-Datei gespeichert. Diese Fenstergröße ist gegenüber modernen Kompressionsprogrammen (z. B. bzip2 mit 100 bis 900 KiB Blockgröße, rzip als Extremfall mit 900 MiB Fenstergröße) veraltet, jedoch ist gzip immer noch eines der schnellsten Kompressions-Programme und kann vielseitig eingesetzt werden, zum Beispiel in Verbindung mit einer sogenannten \"Pipeline\" – die Ausgabe („standard out“) eines Programms kann die Eingabe („standard in“) von gzip darstellen und umgekehrt.\n\nUm die Entwicklung von Software zu vereinfachen, die Datenkompression nutzt, wurde die \"zlib\"-Bibliothek geschrieben. Sie unterstützt das gzip-Dateiformat und die Deflate-Kompression. Die Bibliothek ist weit verbreitet, da sie klein, effizient und vielseitig ist.\n\nQuelle: \"RFC 1952 GZIP File Format Specification version 4.3.\"\n\nDie ersten beiden Bytes bilden den sogenannten „Identification-Code“ des Formats, dieser ist beim gzip-Format immer derselbe. Genormt ist dieser Header mit den Bytes 0x1f und 0x8b (Hexadezimal) oder auch 31 und 139 (Dezimal). Diese Bytes sind zur Verifizierung des Datei-Formats (gzip) und um erste, auffällige Mängel der Datei aufzuzeigen. Wenn diese Initialisierung falsch oder gar nicht erfolgt, wird es Fehler geben, die entweder einen Fehler verursachen oder falsche End-Dateien (nach der Dekompression) hervorbringen.\n\nDas Byte auf dem Index (von 0 beginnend) 2 gibt an, um welche Kompressionsmethode es sich handelt, bzw. welche Aktion verwendet wurde, um die Datei(en) abzuspeichern.\nAuf dem Index 3 liegt das Byte, das für besondere Informationen genutzt wird. Hier gibt es wieder bestimmte Werte, die eine festgelegte Bedeutung haben.\n\nWichtig ist, dass hier immer die Bits beachtet werden. Das bedeutet, dass z. B. „10111000“ (binär) (19 [dezimal]; 0x13 [hexadezimal]) folgendes aussagt: Die Datei ist ASCII-Text, ist eine einzige Datei, hat eine CRC-16-Nummer, besitzt extra Informationen und der originale Name ist bekannt.\nDieser Wert wird von 4 Bytes bestimmt und gibt eine Zeit in Unixzeit an.\n\nDie Definition ist analog zu „Spezielle Informationen“ auf Byte 3.\n\nBeispiel für Kompressionsmethode „Deflate“:\nDieses Byte gibt an, auf welchem Betriebssystem die Datei komprimiert wurde.\nEine Datei packen:\n\nEine gepackte Datei entpacken:\noder\n\nRekursiv alle Dateien in einem Verzeichnis packen und die Kompressionsrate angeben:\n\nEine komprimierte Text-Datei ausgeben:\n\nEine defekte komprimierte Datei bis zur Fehlerstelle entpacken:\n\nDie übliche Dateiendung für gzip-komprimierte Dateien ist heute codice_3, früher auch codice_4.\n\nDa gzip nur einzelne Dateien komprimieren kann, werden mehrere Dateien bzw. Verzeichnisbäume üblicherweise zunächst mit tar zu einer \"Tarball\" genannten Archivdatei zusammengefasst, welche anschließend mit gzip komprimiert wird.\nSolche komprimierten Archivdateien tragen dann meist die doppelte Endung codice_5 oder auch einfach codice_6.\nDiese Methode ermöglicht insgesamt bessere Komprimierung, da so Redundanzen zwischen den einzelnen Dateien ausgenutzt werden können (progressive Kompression), erschwert aber den Zugriff auf die einzelnen Bestandteile.\n\nUnter Unix ist die Komprimierung mit gzip heute Standard, weil sie für viele Aufgaben einen guten Kompromiss aus hoher Geschwindigkeit und guter Datenreduktion ermöglicht. Wo es weniger auf Geschwindigkeit als auf minimale Dateigrößen ankommt (etwa bei der breiten Verteilung von Daten über relativ langsame Netze), werden allerdings zunehmend bzip2 und LZMA verwendet (ebenso wie bei gzip in Kombination mit tar).\n\nDas zlib-komprimierte Dateiformat, der Deflate-Algorithmus und das gzip-Dateiformat wurden 1996 als Request for Comments RFC 1950, RFC 1951 und RFC 1952 standardisiert.\n\n\n"}
{"id": "13313", "url": "https://de.wikipedia.org/wiki?curid=13313", "title": "Turbo Pascal", "text": "Turbo Pascal\n\nTurbo Pascal ist eine integrierte Entwicklungsumgebung (IDE) des Unternehmens Borland für die Programmiersprachen Pascal und Object Pascal.\nDer Compiler basierte auf dem \"Blue Label Software Pascal\" Compiler, der von Anders Hejlsberg ursprünglich für das Kassetten-basierte Betriebssystem NAS-SYS des Mikrocomputers Nascom entwickelt wurde.\nDieser Compiler wurde zunächst als \"Compass Pascal\" Compiler für das Betriebssystem CP/M und dann als Turbo Pascal Compiler für MS-DOS und CP/M weiterentwickelt. Kurzzeitig war 1986 auch eine Version für den Apple Macintosh verfügbar.\n\nDie erste Version von Turbo Pascal erschien im November 1983, zu einer Zeit, als das Konzept der integrierten Entwicklungsumgebungen noch recht neu war. Ein Programmierer hatte zu dieser Zeit auf einem IBM-kompatiblen PC im Wesentlichen die Wahl zwischen dem mit DOS mitgelieferten Microsoft BASIC-Interpreter oder einem professionellen und teuren BASIC-, C-, Fortran- oder Pascal-Compiler (UCSD Pascal). Die Compiler waren eher umständlich zu benutzen: Mangels Multitasking unter MS-DOS bestand jeder Testlauf aus dem Verlassen, Starten und Neuladen der verschiedenen Tools (Editor, Compiler, Linker, Debugger), die für die Softwareentwicklung notwendig sind. Da die meisten PCs zu dieser Zeit keine Festplatten hatten (eine solche kostete zum damaligen Zeitpunkt 2000 US-Dollar und mehr), musste oft sogar noch mehrmals die Diskette gewechselt werden.\n\nIn diese Situation hinein kam Turbo Pascal mit dem Konzept einer integrierten Entwicklungsumgebung, das die verschiedenen Tools in einem Programm vereinte. Es war zudem gerade einmal rund 60 KB groß und lief damit auf jedem damaligen PC in hoher Geschwindigkeit. Selbst auf einem PC mit nur einem Diskettenlaufwerk konnte auf Diskettenwechsel verzichtet werden, da auf der Turbo-Pascal-Diskette noch genug Platz für das gerade bearbeitete eigene Programm war. Schließlich war das System preislich selbst für Schüler und Studenten erschwinglich – mit dem Ergebnis, dass es im Laufe der 1980er Jahre auf dem PC zum Quasistandard wurde.\n\nOhne Turbo Pascal hätte die Sprache Pascal sicher das gleiche Schicksal ereilt wie viele an Universitäten vorher und nachher geborene „Kunstsprachen“, z. B. Modula-2 oder Oberon (beide auch von Niklaus Wirth), die heute praktisch verschwunden sind. Hejlsberg entwickelte die Sprache und das System pragmatisch weiter: Von Anfang an wurde die Laufzeitbibliothek um Routinen (Unterprogramme) ergänzt, die Zugriff auf das System ermöglichten – ganz entgegen dem ursprünglichen Konzept von Wirth. Dabei wurde – anders als z. B. bei der Sprache C – die für Pascal typische strenge Typprüfung etc. beibehalten (beides hat Vor- und Nachteile: Eine strenge Prüfung vermindert die Gefahr ungewollten Fehlverhaltens eines Programms, macht den Quelltext aber meist länger und zwingt dazu, bewusst Funktionen zur Typumwandlung zu nutzen). Je umfangreicher ein Programmpaket wird, desto wichtiger werden solche Funktionen, weshalb auch andere Programmiersprachen, wie beispielsweise C++, Java und C, diese Konzepte in unterschiedlicher Strenge übernommen haben.\n\nIm September 1986 kam in der MS-DOS-Version Grafik dazu. Dies war die letzte Version, die auch noch für CP/M erschien, allerdings ohne die Grafikmöglichkeiten (ausgenommen die \"Graphix Toolbox\" des Schneider CPC), da die meisten CP/M-Rechner nicht grafikfähig sind. Es gab drei unterschiedliche Versionen für MS-DOS, die es ermöglichten, unterschiedlichen Code zu generieren, und zwar für emulierten Gleitkomma-Code, Coprozessor-orientierten Code, und BCD-Code. Es werden kommerzielle Programmbibliotheken angeboten, diese müssen allerdings mittels INCLUDE im Quelltext eingebunden werden.\n\nIm Dezember 1987 kam das Unit-Konzept dazu, das Bibliotheken und große Projekte ermöglichte. Das Einfügen von Assemblerteilen in den Quelltext wurde mit Inline-Codes unterstützt.\n\nIm Oktober 1988 wurde der Debugger in die Entwicklungsumgebung integriert. Damit wurde es möglich, innerhalb der IDE zu debuggen, Haltepunkte zu setzen und Variablenwerte zu beobachten.\n\nIm Mai 1989 kam die Objektorientierung hinzu.\n\n\nIm November 1990 kam eine objektorientierte GUI-Bibliothek hinzu (Turbo Vision), ähnlich den späteren MFC für Windows. Turbo Vision war für den Textmodus des PCs konzipiert, enthielt aber bereits Steuerelemente wie Fenster, Befehlsschaltflächen und Bildlaufleisten, die durch Textsymbole dargestellt wurden. Außerdem konnten (auch umfangreichere) Assemblerfunktionen in Intelsyntax direkt im Quelltext realisiert werden. Die Entwicklungsumgebung wurde entsprechend erweitert, so dass auch Assemblerteile im Einzelschrittmodus bei gleichzeitiger Kontrolle aller Flag- und Registerinhalte ausgeführt werden konnten.\n\nkam parallel zu Version 6 auf den Markt. Das GUI war komplett als Windows-Anwendung ausgelegt; es wurde in Version 7 (Borland Pascal) übernommen und ausgebaut.\n\nIm Oktober 1992 wurde in der professionellen Variante (Borland Pascal) die Entwicklung von Protected-Mode-Anwendungen innerhalb der IDE möglich – allerdings ohne integrierten Debugger. Im April 1993 folgte noch eine nachgeschobene/fehlerbereinigte Version 7.01; dies war zugleich auch die letzte von Borland veröffentlichte Pascal-Version.\n\nAnfang der 1990er-Jahre wurde Turbo Pascal auf Windows portiert. Dies war allerdings eine Sackgasse. Die Programmierung war unter Turbo Pascal für Windows ähnlich aufwendig wie in C – mit dem zusätzlichen Nachteil, dass Windows selbst in C programmiert ist, weshalb die Schnittstellen zwischen Windows und Pascalprogramm mindestens grundlegende C-Kenntnisse erfordern. Borland adaptierte in der Folgezeit das Rapid-Application-Development-Prinzip, das sich vorher schon bei Visual Basic von Microsoft sehr bewährt hatte: Die grafischen Elemente einer Windows-Anwendung werden mit der Maus zusammengestellt, der zugehörige Code wird automatisch erzeugt. Dieses Produkt wurde Delphi genannt, die zugrundeliegende Sprache ist \"Object Pascal\" von Borland.\n\nBorland veröffentlichte 2002 mehrere Turbo-Pascal-Versionen als Freeware auf der eigenen Webseite, nachdem sie zu „antique software“ (Abandonware) wurden. Die neueste veröffentlichte internationale Version ist TP 5.5, jedoch wurde von dem neueren TP 7.01 die französische Version veröffentlicht. Die Downloads sind weiterhin auf der Nachfolgerwebseite von Embarcadero verfügbar.\n\nMit Free Pascal und GNU Pascal gibt es zwei Turbo-Pascal-kompatible freie Compiler, die für zahlreiche Betriebssysteme zur Verfügung stehen. Die Entwicklung von Virtual Pascal wurde hingegen eingestellt, obwohl es noch eine große Gemeinschaft gibt.\n\nprogram HalloWelt;\nbegin\nend.\n\n\n"}
{"id": "13633", "url": "https://de.wikipedia.org/wiki?curid=13633", "title": "Vi", "text": "Vi\n\nvi („vi“ für „visual“) ausgesprochen [] oder [], im deutschen Sprachraum gelegentlich auch [], jedoch nicht „sechs“ oder „six“ (wie die römische Zahl VI) ist ein 1976 von Bill Joy auf einem ADM-3A-Computerterminal für eine frühe BSD-Version geschriebener und von POSIX standardisierter Texteditor. Der Name stammt vom Befehl codice_1 des Editors ex. Mit diesem Befehl konnte man den Zeileneditor in einen visuellen Modus umschalten.\n\nBis Anfang der 1970er Jahre wurden hauptsächlich zeilenorientierte Editoren benutzt, wobei ein weit verbreiteter ed war. Joy baute auf diesem auf, zunächst mit ebenfalls einem Zeileneditor, \"ex\". Darauf baute später wiederum der Editor vi auf. vi wurde schnell zum De-facto-Standardeditor unter Unix.\n\n1991 benutzten ungefähr die Hälfte aller Teilnehmer einer Usenet-Umfrage den vi. Auch heutzutage ist die Verwendung von vi bzw. dessen Erweiterungen zumindest in der Unix- und Linuxwelt sehr verbreitet. Außerdem kann man mit diesem Editor in Kombination mit ssh (früher mit Telnet oder rsh) im Netzwerk auf anderen Rechnern arbeiten.\n\nAufgrund ihrer relativen Ressourcenfreundlichkeit starten vi bzw. fast alle seine Klone schneller und benötigen deutlich weniger Speicherplatz als etwa Emacs. Auf einer „Rettungsdiskette“ hat vi auch heute noch seinen Platz, so dass er Bestandteil fast aller Unix-/Linux-Distributionen ist.\n\nDie originale Version von Bill Joy war ursprünglich weder im Quelltext noch sonst frei verfügbar, so dass mittlerweile eine Reihe von Klonen mit zum Teil wesentlichen Erweiterungen existiert, wie z. B. Vim, Nvi, elvis und WinVi, die teilweise auch für nicht Unix-artige Systeme verfügbar sind.\n\nvi besitzt drei grundsätzlich unterschiedliche Arbeitsmodi. Die drei Modi sind:\n | Start mit\n\nAufgrund der verschiedenen Arbeitsmodi ist die Bedienung von vi, verglichen mit anderen Terminaleditoren wie GNU nano oder heute üblicheren grafischen Editoren, zunächst gewöhnungsbedürftig. Ein großer Vorteil von vi ist hingegen, dass mehrere Befehle nacheinander ohne gleichzeitiges Betätigen der -, - oder sonstiger Modifikator-Tasten abgesetzt werden können. So ist es auch möglich, mit einem einzigen Befehl mehrere Wörter oder Sätze zu löschen.\n\nIm Zuge des sogenannten Editor Wars gründeten die Anhänger von vi den „Cult of Vi“ als Reaktion auf die von Richard Stallman alias St. IGNUcius gegründete Church of Emacs. Daraufhin wurden sie von den Emacs-Anhängern als Nachahmer („ape their betters“) bezeichnet.\n\n\n"}
{"id": "14785", "url": "https://de.wikipedia.org/wiki?curid=14785", "title": "Microsoft Access", "text": "Microsoft Access\n\nMicrosoft Access [] (kurz MS Access, nach ) ist eine Anwendung innerhalb der Microsoft-Office-Familie und ist als einzelne Büroanwendung oder als Bestandteil von \"Office Professional\" (Office-Paket) erhältlich.\n\nAccess kombiniert die Microsoft Jet Engine als relationales Datenbankmanagementsystem mit den Werkzeugen einer integrierten Entwicklungsumgebung, die mit ihren grafischen Benutzeroberflächen insbesondere für die Zielgruppe Endbenutzer zur Herstellung von Datenbankanwendungen geeignet ist.\n\nAccess unterstützt (mit Einschränkungen) die Datenbank-Programmiersprache SQL-92.\n\nDer Erfolg von Desktop-Datenbankanwendungen wie dBASE und Foxpro veranlasste Microsoft schon Mitte der 1980er Jahre zu der Entscheidung, eine eigene Datenbankanwendung für das damals neue Betriebssystem Windows zu entwickeln. Die Entwicklungsarbeiten unter dem Projektnamen \"Omega\" verzögerten sich immer wieder, bis Anfang der 1990er Jahre die noch fehlerhafte Version 1.0 und kurz darauf die stabile Version 1.1 auf den Markt kam.\n\nDie aktuelle Version ist Access 2016. Im Gegensatz zu den anderen Office-Programmen Word, Excel und PowerPoint, die auch für das Betriebssystem Mac OS X von Apple angeboten werden, ist Access nur für Windows erhältlich.\n\nStandardmäßig speichert Access alle Daten einer Datenbankanwendung in einer einzigen Datei des eigenen mdb-Dateiformates (bis zur Version Access 2003) bzw. des accdb-Dateiformates (ab Version Access 2007) ab. Dieses schließt sowohl Elemente der Oberfläche, als auch die Datenbanktabellen ein. Alternativ ist es sehr einfach möglich, die Daten (Tabellendefinitionen und den Datenbestand) im Unterschied zur Oberfläche (und weiteren VisualBasic Modulen und Makros, Reports usw.) in verschiedenen Dateien zu halten (Front- bzw. Backend). Beim Einbinden bzw. Verknüpfen von externen Datenquellen (Tabellen) können verschiedene Access-Versionen, aber auch Access-fremde Formate wie dBASE, sowie viele gängige Datenquellen z. B. über ODBC angesprochen werden.\n\nIm Gegensatz zu früheren PC-basierenden Datenbanksystemen unterstützt Access ein relationales Datenbank-Modell mit referentiellen Integritätsprüfungen. Um extern auf Access-Datenbanken zuzugreifen, eignet sich unter anderem die ebenfalls von Microsoft entwickelte ODBC-Programmierschnittstelle. Ferner kann auf Access-Datenbanken auch von anderen Programmiersprachen, zum Beispiel Delphi, Visual Basic etc. durch den Einsatz von ADO oder dem etwas älteren, aber auf MDBs zugeschnittene DAO zugegriffen werden. Um lediglich dieses Format einzubinden, braucht Access weder lizenziert noch installiert zu sein. Ab Windows 2000 ist ADO als Teil von MDAC ein Bestandteil des Betriebssystems. Für frühere Windowsversionen kann es kostenfrei nachinstalliert werden.\n\nAccess, das auf der Microsoft Jet Engine als Datenbank-Backend basiert, eignet sich gut für kleinere bis mittlere Datenbanken bei (etwa) bis zu zehn gleichzeitig zugreifenden Benutzern. Darüber hinaus empfiehlt Microsoft die recht einfache Migration zum MS-SQL-Server. Um den Mehrbenutzerzugriff innerhalb des mdb-Dateiformats einfach zu gestalten, erfolgen Schreibzugriffe bei älteren Versionen einer Access-mdb-Datenbank immer am Dateiende. Gelöschte oder abgeänderte Elemente bleiben als „Löcher“ in der Datei stehen, bis die Access-Datei komprimiert (im engeren Sinne ist es ein Defragmentieren der Datenbankdatei selbst) wird. Bei neueren Versionen ist ein regelmäßiges Komprimieren nicht mehr erforderlich. Durch schrittweise erweiterte, unter anderem Cache-basierte Zugriffstechniken, war es bereits ab Version 1.0 möglich, auch in Netzwerken beachtliche Zugriffsgeschwindigkeiten zu erreichen.\n\nDer Betrieb von Access-Anwendungen im LAN ist an eine stabile Netzwerkumgebung gebunden. Bereits kleine Aussetzer lassen die Verbindung zum Backend abreißen. Access bleibt dabei meist stabil, lediglich die betroffenen Anwender müssen ihre Datenbankapplikation neu starten. Bei stabiler Hardware und Netzwerk wird nur in seltenen Fällen eine Reorganisation (Reparatur und Komprimierung) der Datenbank erforderlich. Access ist mit seinem mdb-Dateiformat im Vergleich zu anderen dateibasierten Datenbanken stabil, wenngleich klassische SQL-Server üblicherweise weit stabiler sind. In der Praxis sind in standardmäßig eingerichteten Netzwerken, zum Beispiel auf Ethernet-Basis, keine Probleme zu erwarten.\n\nFür den Einsatz in heterogenen Netzwerken oder im WLAN ist Access allerdings – wie alle dateibasierenden (statusgebundenen) Zugriffsverfahren, die konkurrierend über das Dateisystem erfolgen – weniger geeignet, hier sollten Datenbankserver (DB2-Server, MS-SQL-Server, MySql-Server usw.) bevorzugt werden.\n\nUm diese Schwächen zu überwinden, basiert der Datenbankzugriff von Access schon seit den ersten Versionen auf einer SQL-Engine, die eine einfache Migration zu einem SQL-Server ermöglicht. Dazu wurde Access ab der Version 2000 so erweitert, dass Anwendungen direkt auf einer Datenbank, die auf einem Microsoft SQL Server betrieben wird, aufgebaut werden können, anstatt sie über ODBC einzubinden. Für diesen Zweck wurde ein neues Dateiformat mit der Endung adp entwickelt und der Dateizugriff von DAO (Data Access Objects) auf das vielseitigere ADO (Active Data Objects) umgestellt.\nADO abstrahiert wesentlich stärker von den verwendeten Datenbankquellen als sein Vorgänger und hat daher eine höhere Zugriffsgeschwindigkeit.\n\nMit Access wird ab Version 2000 eine kostenfreie Desktop-Version des Microsoft SQL Server ausgeliefert, die sich direkt über die Access-Oberfläche verwalten lässt. Daher ist es nicht nötig, eine zusätzliche Programmlizenz zu erwerben. Allerdings sind die Verwaltungsmöglichkeiten der SQL-Server-basierten Funktionen (z. B. Berechtigungen) im Vergleich zur Vollversion stark eingeschränkt, weswegen für größere Projekte die Verwendung des MSSQL-Servers zweckmäßig ist. Seit SQL Server 2005 trifft dies aber nur noch bedingt zu, da mit dieser Version das Management Studio Express kostenlos mitangeboten wird.\n\nDurch die Bereitstellung von visuellen Programmierobjekten, die speziell im Hinblick auf den Datenbankzugriff optimiert sind, ist es mit Access möglich, innerhalb von kurzer Zeit datenbankbasierte Anwendungen zu erstellen (Rapid Prototyping), ohne umfangreiche Programmierarbeiten (wie sie z. B. in C oder C++ notwendig sind) durchführen zu müssen. Dabei ist es möglich, auch auf Skripte zurückzugreifen, die in einer speziellen Makro-Sprache erstellt wurden. Um auch komplexe Anwendungen erstellen zu können, ist in Access eine Entwicklungsumgebung für Visual Basic for Applications (VBA) integriert. Zur Verbesserung der Geschwindigkeit der Programmausführung kann der auf Basis von VBA erstellte Quelltext kompiliert und als optimierter Programmcode, auch „P-Code“ genannt (von „Pseudocode“ abgeleitet, hier jedoch andere Bedeutung) in der Datenbankdatei (identisch wie in der *.MDE) gespeichert werden.\n\nZur Weitergabe entwickelter Datenbanken an Benutzer, die kein Access besitzen, gibt es die \"Runtime-Versionen\". Diese werden mit der \"Office Developer Edition\" erstellt, welche je nach Access-Version kostenfrei oder kostenpflichtig ist.\n\nZur Erstellung einer Datenbank werden vom Entwickler mehrere Objektarten erstellt:\n\n\nDie Daten über diese Objekte („Metadaten“) sind in sogenannten ‚Systemtabellen‘ gespeichert. Dies sind Tabellen, die Access in derselben Datenbank wie die zu speichernden Daten führt; sie sind jedoch im Regelfall für den Benutzer nicht sichtbar.\n\nAngaben zum Datum des jeweiligen Releases, weitere Update-Versionen und inhaltliche Hinweise zu Erweiterungen finden sich in.\n\n\n\n"}
{"id": "14786", "url": "https://de.wikipedia.org/wiki?curid=14786", "title": "MySQL", "text": "MySQL\n\nMySQL [] ist eines der weltweit verbreitetsten relationalen Datenbankverwaltungssysteme. Es ist als Open-Source-Software sowie als kommerzielle Enterpriseversion für verschiedene Betriebssysteme verfügbar und bildet die Grundlage für viele dynamische Webauftritte.\n\nMySQL wurde seit 1994 vom schwedischen Unternehmen MySQL AB entwickelt. Im Februar 2008 wurde MySQL AB vom Unternehmen Sun Microsystems übernommen, das seinerseits im Januar 2010 von Oracle gekauft wurde.\n\nDer Name \"MySQL\" setzt sich zusammen aus dem Vornamen \"My\", den die Tochter des MySQL-AB-Mitbegründers Michael Widenius trägt, und SQL.\n\nEin bevorzugtes Einsatzgebiet von MySQL ist die Datenspeicherung für Webservices. MySQL wird dabei häufig in Verbindung mit dem Webserver Apache bzw. Nginx und der Skriptsprache PHP eingesetzt. Man spricht dann von einem Lamp - Stack (Linux / Apache / MySQL / PHP) oder eben Lemp - Stack (das \"e\" steht dann für [e]nginx). Viele Webdienste bedienen sich dieser Architektur und betreiben je nach Größe und Bedarf eine Vielzahl von MySQL-Servern, über die die Zugriffe aus dem Netz abgewickelt werden. MySQL wird unter anderem verwendet von Flickr,\nYouTube, Facebook und Twitter. Daneben wird MySQL in vielen Produkten als eingebettetes Datenbanksystem eingesetzt.\nInzwischen wird MySQL zunehmend von seinem Fork MariaDB abgelöst. MariaDB ist zu MySQL kompatibel, so dass MySQL-Datenbanken recht einfach migriert werden können. Der Begründer von MySQL Monty Widenius hat nach dem Kauf von MySQL durch Oracle den neuen Fork entwickelt und erweitert diesen gemeinsam mit einem Team ehemaliger Core-Entwickler von MySQL, so dass weitere Features inzwischen verfügbar sind. Eine Rückmigration ist nach Nutzung dieser Features teilweise nicht mehr so ohne Weiteres möglich. Dies gilt insbesondere, wenn Speicher-Engines wie ColumnStores verwendet werden, die MySQL so nicht kennt.\n\nMySQL-Server und offizielle Bibliotheken sind zur Erzielung einer möglichst guten Performance hauptsächlich in ANSI C und ANSI C++ implementiert.\n\nMySQL ist auf vielen Unix-Varianten, macOS und Linux, aber auch auf Windows, OS/2 und i5/OS (ehemals OS/400) lauffähig. Seit Anfang 2008 gibt es auch eine Symbian-Variante.\n\nMySQL sieht grundsätzlich einen MySQL-Server vor, auf dem Daten gespeichert sind, und einen oder mehrere MySQL-Clients, die Anfragen an den Server schicken, die dieser beantwortet. Der Standardport für den MySQL-Server ist codice_1 im Transmission Control Protocol (TCP).\n\nAuf dem Datenbankmanagementsystem, dem MySQL-Server, können mehrere Datenbanken erstellt werden. In einer Datenbank können mehrere Tabellen angelegt werden. Praktisch erstellt MySQL dabei für jede Datenbank einen Ordner auf der Festplatte, in dem Dateien für die Struktur und die Daten der einzelnen Tabellen abgelegt werden. Das genaue Format dieser Dateien hängt von der für die jeweilige Tabelle verwendeten Speicherengine ab.\n\nDie Tabellen können jeweils von einem unterschiedlichen Typ sein. Der Tabellentyp legt fest, welche Speicherengine (Speichersubsystem) für Anfragen an eine Tabelle verwendet wird. Jede Tabelle kann Spalten enthalten, in denen Daten eines festgelegten Datentyps gespeichert werden können (z. B. Integer (ganze Zahlen) oder Strings (Zeichenketten)). Die maximale Größe der Tabellen wird grundsätzlich nur durch das Betriebssystem limitiert.\n\nEin Client kann Datenbankanfragen an einen MySQL-Server schicken. Dieser ist dafür zuständig, jede Anfrage möglichst performant zu bearbeiten. Dabei wird zunächst der Query-Cache befragt und bei nicht vorhandenem Ergebnis die Anfrage geparst, optimiert und schließlich ausgeführt; das Ergebnis wird dann zurückgegeben.\n\nUm die Performance zu verbessern, kann MySQL die Ergebnisse von Anfragen in einem Zwischenspeicher, dem Query-Cache, ablegen. Sollte später eine identische Abfrage an den Server geschickt werden, ohne dass sich in der Zwischenzeit die Daten in der Datenbank verändert haben, wird sie aus dem Cache beantwortet. Es muss dann nicht auf die Datenbank selbst zugegriffen werden.\n\nSoll eine Query ausgeführt werden, wird zunächst geprüft, ob ihre Syntax gültig ist. Sie wird dazu in ihre einzelnen Komponenten zerlegt. Zugleich werden einige grundlegende Informationen über die Query gesammelt, wie etwa die Art der Query (z. B. SELECT, INSERT, SET oder GRANT), die betroffenen Tabellen oder die Inhalte der WHERE-Klausel. Am Ende dieses Schrittes kennt MySQL den Parse-Baum, der danach optimiert werden kann.\n\nIst eine Abfrage syntaktisch gültig, dann wird sie als Nächstes optimiert. Dabei sucht der Optimizer nach dem effizientesten Weg, die Query zu bearbeiten. Dazu wird hauptsächlich versucht, die Anzahl der zu lesenden Datensätze möglichst gering zu halten. Dies wird etwa, wenn Datensätze aus mehreren Tabellen gelesen werden müssen, durch eine geschickte Abfragereihenfolge (JOIN) der Tabellen erreicht; nicht benötigte Tabellen werden komplett aus dem JOIN entfernt. Der Optimizer berücksichtigt unter anderem, ob es sinnvoll ist, zur Lokalisierung der gesuchten Datensätze einen Index zu verwenden (und wenn ja, welchen), oder ob es besser wäre, stattdessen einen Table-Scan durchzuführen, d. h., die komplette Tabelle zu durchlaufen.\n\nDie zur Verfügung stehenden Alternativen werden vom Optimizer bewertet. Dabei wird aufgrund von Heuristiken für jede Möglichkeit geschätzt, wie lange die Ausführung benötigen würde. Diejenige Alternative, die die geringsten Kosten hat, wird anschließend tatsächlich durchgeführt.\n\nDie Arbeit des Optimizers lässt sich mit dem MySQL-Kommando codice_2 nachvollziehen. Dieser Befehl (gefolgt von einer bestimmten Query) beauftragt den Optimizer, seinen \"Query-Plan\" zurückzugeben. Als Ausgabe erhält der Nutzer Informationen für jeden Schritt, den der Optimizer zur Beantwortung der Query durchführen würde. Auf diese Art lässt sich mit etwas Hintergrundwissen ermitteln, inwiefern die Query optimiert werden kann, so dass sie schneller ausgeführt werden kann.\n\nMichael Widenius und David Axmark begannen 1994 mit der Entwicklung von MySQL. Es wurde zunächst als Klon für mSQL entwickelt, um Datenbanken des maskengesteuerten Datenbanksystems UNIREG in Web-Anwendungen verfügbar zu machen. (UNIREG war 1979 von Michael Widenius entwickelt und ca. 1986 in die Programmiersprache C umgeschrieben worden, damit es auch unter UNIX-Systemen lief.) MySQL war daher sowohl zu mSQL als auch UNIREG voll kompatibel.\n\nNach einem internen Release am 23. Mai 1995 wurde die Software im Jahr 1997 sofort unter der Versionsnummer 3.1 veröffentlicht, um zu signalisieren, dass sie auf einem Kern basiert, der schon eine lange Geschichte hat. Sie war von Anfang an für große Datenmengen und sehr gute Performance ausgelegt, teils auf Kosten von Stabilität und Verfügbarkeit. Der Funktionsumfang hingegen war zunächst beschränkt. So gab es nur wenige Tabellentypen und keine Transaktionen. Typischerweise werden neue Eigenschaften auf Wunsch der Anwender implementiert, die dadurch ein sehr großes „Mitspracherecht“ haben.\n\nMit der im Januar 2001 veröffentlichten Version 3.23 verfügte MySQL über zwei Tabellentypen mit Transaktionen, wobei der eine (InnoDB) den ACID-Kriterien genügt. Alle Operationen, die allgemeine SQL-Eigenschaften betreffen, sind für alle Tabellentypen gleich, während die Eigenschaften der Tabellentypen aufgrund der unterschiedlichen Architektur sehr verschieden sein können. So besitzt der Typ MyISAM bereits seit der frühen Version 3.23 eine sehr leistungsfähige Volltext-Suche, die beim Typ InnoDB nicht implementiert ist. Neu in Version 3.23 ist ebenfalls das Replikationssystem. Es ist für den Einsatz in einem Rechnerverbund ausgelegt und bietet sich für einen unterbrechungsfreien Betrieb an. Dabei sind dem Datenbankmanagementsystem (DBMS) mehrere Datenbanken auf unterschiedlichen Rechnerknoten zugeordnet. Eine der Datenbanken fungiert als Master; hier werden die Datenbankinhalte verändert. Das Replikationssystem verteilt anschließend die datenverändernden SQL-Kommandos auf die anderen Datenbanken, die diese Änderungen lokal auf ihren Tabellen nachvollziehen. Es handelt sich hierbei also um eine asynchrone Replikation der SQL-Kommandos. Mit dem MySQL Cluster steht ein Tabellentyp zur Verfügung, bei dem die gesamte Datenbank im Arbeitsspeicher als In-Memory-Datenbank vorgehalten werden kann. Es wird synchrone Replikation zwischen allen Clusterknoten und Transaktionsverarbeitung unterstützt, jedoch keine Volltextsuche.\n\nMySQL 4.0, das im März 2003 freigegeben wurde, erlaubt die Nutzung von \"Unions\" und führte einen Query-Cache ein, der die Ergebnisse häufig benutzter SQL-Anfragen zwischenspeichert.\n\nIm Oktober 2004 wurde MySQL 4.1 veröffentlicht. Es bietet eine Datenspeicherung in unterschiedlichen Zeichensätzen pro Tabelle an; unter anderem wird auch Unicode unterstützt. Unterabfragen (Subqueries/Subselects) sind möglich.\n\nVersion 5.0 wurde im Oktober 2005 freigegeben. Sie unterstützt alle im SQL3-Standard definierten Objekttypen. Neu in Version 5 ist dabei die Unterstützung von Views, Triggern, Stored Procedures und User Defined Functions.\n\nIm Februar 2008 übernahm Sun Microsystems MySQL AB. Als Kaufwert wird eine Milliarde Dollar genannt; davon 200 Millionen in Aktienoptionen.\n\nIm November 2008 wurde MySQL 5.1 freigegeben. Zu den Neuerungen zählen außer Partitionsfunktionen, mit denen sehr große Tabellen in mehrere physikalische Dateien aufgeteilt werden können, ein Event-Scheduler, mit dessen Hilfe zuvor definierte SQL-Kommandos in regelmäßigen Zeitabständen ausgeführt werden können, sowie Funktionen des Instanzenmanagers. Die API wurde überarbeitet, so dass nun das Laden und Entladen von Komponenten während der Laufzeit und ohne Neustart des Servers möglich ist.\n\nIm Januar 2010 wurde Sun Microsystems von Oracle gekauft. Wenige Monate später gab Oracle bekannt, dass die bereits von MySQL AB begonnene Entwicklung der Datenbank-Engine Falcon eingestellt wird.\n\nEnde 2010 wurde MySQL 5.5 veröffentlicht. InnoDB wurde zur Standard-Speicherengine. Die Performance wurde durch die Nutzung von asynchronem I/O verbessert. Neu sind auch die Kommandos SIGNAL/RESIGNAL, die standardkonforme Fehlerbehandlung in Stored Procedures erlauben.\n\nDas im Jahr 2012 veröffentlichte MySQL 5.6 umfasst u. a. Memcached APIs, globale Transaktions-IDs und Verbesserungen am PERFORMANCE_SCHEMA.\n\nIm Januar 2009, bereits bevor Oracle MySQL AB kaufte, startete Monty Widenius einen GPL-Fork, MariaDB. MariaDB basiert auf der Code-Basis von MySQL server 5.5 und hat sich das Ziel gegeben, weitgehend die Kompatibilität zu den von Oracle herausgegebenen MySQL-Versionen zu erhalten.\n\nDie Entwicklung von MySQL 5.7 hat insgesamt über zwei Jahre in Anspruch genommen, bis es schließlich Ende Oktober 2015 veröffentlicht werden konnte. Die neue Version zeichnet sich insbesondere durch eine gesteigerte Performance, verbesserte Replikation und Logging aus. So erweiterten die MySQL Entwickler beispielsweise den Abfragenoptimierer um ein dynamisches Kostenmodell, was eine schnellere Ausführung ermöglicht und dem Nutzer mehr Eingriffsmöglichkeiten bietet.\n\nSeit der Übernahme von MySQL AB von Sun durch Oracle steht das Datenbanksystem immer häufiger in der Kritik. Der Unterschied zwischen der freien und kommerziellen Version von MySQL wird immer gravierender. Neue Funktionen sind häufig nur noch in der kommerziellen Version von MySQL enthalten, dazu kommen die nicht-öffentliche Datenbank mit Fehlermeldungen, veraltete Bazaar-Archive und fehlende Tests für die Fehlersuche.\n\nSeit Ende 2012 erodiert auch der Rückhalt in der OpenSource-Gemeinschaft für MySQL. Nach Fedora und OpenSUSE haben Anfang 2013 auch Slackware und Arch Linux das MySQL-Paket durch MariaDB ersetzt, einen Fork des Mitbegründers Widenius. Auslöser seien der mangelnde Respekt gegenüber der Gemeinschaft und die zunehmend abgeschottete Entwicklung des RDBMS. Ebenso wechselte die Wikimedia-Foundation Anfang 2013 zu MariaDB.\n\nJaroslav Reznik, Red Hats Fedora-Projekt-Manager, erklärte, dass sich MySQL in Richtung eines geschlossenen Projektes entwickle. Alle nützlichen Informationen zu Sicherheitsfragen (CVEs) würden nicht mehr veröffentlicht. Es existierten keine vollständigen Regressionstests mehr und ein sehr großer Teil der MySQL-Bug-Datenbank sei nun nicht mehr öffentlich.\n\nMichael Widenius, der ehemalige Gründer von MySQL AB kritisiert Oracle scharf: Oracle habe klargemacht, dass sie kein Interesse an Open Source hätten, die Zusammenarbeit mit der Community ablehnten und auch MySQL im Allgemeinen nicht mögen würden. Als Beispiele für die Missachtung der Open-Source-Prinzipien nennt er die kommerziellen Erweiterungen für MySQL, die inzwischen nichtöffentliche Fehler-Datenbank und den Mangel an Testfällen für neuen MySQL-Code. Vorzeige-Funktionen, wie das Online-Backup und Fremdschlüssel für alle Speicher-Engines, die für MySQL 6.0 versprochen wurden, seien nicht veröffentlicht worden, obwohl sie fertig entwickelt und bereit seien. Statt Fehler zu beheben, entferne Oracle Funktionen. Die meisten der ursprünglichen MySQL-Entwickler hätten Oracle verlassen. Als weitere Beweise für die „Verachtung“ der MySQL-Anwender nennt er den „scharfen“ Anstieg der Lizenz- und Support-Gebühren und das Fehlen einer offenen Roadmap.\n\nMySQL bietet verschiedene Speichersubsysteme (Engines) an. Jede Engine ist für ein spezielles Einsatz-Szenario optimiert. Im Vergleich zu der traditionellen Mehrschichtenarchitektur von Datenbanksystemen sind die Engines kein reines Speichersubsystem, sondern bieten mehr Funktionalität. So liegt die Verwaltung von Transaktionen, von Indizes und referenziellen Integritäten in der Hand der Engine.\n\nDie einzelnen Speicherengines unterstützen unterschiedliche Funktionen und weisen je nach Einsatzgebiet eine unterschiedliche Performance auf. Je nachdem, wozu eine Tabelle benutzt wird (z. B. hauptsächlich lesende SELECT-Anfragen oder hauptsächlich schreibende INSERT/UPDATE-Anfragen), sollte eine passende Speicherengine gewählt werden. Ein anderes Kriterium für die Wahl der Speicherengine kann auch die Notwendigkeit sein, eine bestimmte Funktion zu nutzen, wie z. B. Transaktionen oder referenzielle Integrität.\n\nMySQL kann auch um eigene Speicherengines erweitert werden. Neben den von MySQL veröffentlichten und mit MySQL mitgelieferten Engines gibt es auch Engines anderer Hersteller.\n\nMyISAM bietet schnellen Zugriff auf Tabellen und Indizes ohne Transaktionssicherung. Parallele Datenbankzugriffe (Concurrency) verwaltet MySQL auf Tabellenebene, das heißt die komplette Tabelle wird je nach Sperrungsart für bestimmte Operationen gesperrt (Read- oder Write-Lock). Eine Vielzahl von simultanen Lesezugriffen ist möglich, da Lesezugriffe nur sogenannte READ-Locks akquirieren. Diese erlauben es anderen „READERN“, auf den gleichen Datensatz gleichzeitig zuzugreifen. Schreibzugriffe müssen allerdings warten, bis alle gegenwärtigen „READER“ ihre Leseoperationen abgeschlossen haben und damit ihren READ-Lock freigeben. Ein „READER“ muss somit nur andere „WRITER“ blocken.\n\nEin Datensatz, der geändert wird, kann weder gelesen noch anderweitig geschrieben werden. Somit muss ein „WRITER“ (schreibender Zugriff auf Daten) andere „READER“ und „WRITER“ blocken. Dies geschieht durch einen „Write-Lock“. Auch dieser findet auf Tabellenebene statt, somit kann in dieser Zeit weder lesend noch schreibend auf die gesamte Tabelle zugegriffen werden, bis der „Write-Lock“ aufgehoben wird.\n\nFür einen Querymix, der vor allem aus Lesezugriffen besteht, ist MyISAM eine sehr effiziente Speicher-Engine. Weitere Vorteile von MyISAM sind:\n\nMySQL verwaltet die Zugriffsrechte über Grant-Tabellen in der Datenbank mysql, die (auch in der neuesten Version) ausschließlich in MyISAM-Tabellen gespeichert werden. MyISAM kann beim Kompilieren oder Serverstart deshalb nicht ausgeschlossen werden. Bis MySQL Version 5.1 war MyISAM außerdem die Standard-Speicher-Engine, danach nahm InnoDB diesen Platz ein.\n\nInnoDB bietet transaktionssichere Lese- und Schreibzugriffe, d. h., es bietet Begin-, Commit- und Rollback-Funktionen. Dadurch wird sichergestellt, dass eine Abfrage oder ein Satz zusammengehöriger Abfragen entweder ganz oder gar nicht, nicht aber unvollständig ausgeführt wird. Die gewünschte Isolationsebene der Transaktionen kann eingestellt werden. Dadurch kann die Sicherheit der vollständigen und korrekten Ausführung verringert werden, was sich positiv auf die Ausführungsgeschwindigkeit auswirkt.\n\nInnoDB bietet ferner die Möglichkeit, Fremdschlüssel-Beziehungen zu überprüfen. Seit MySQL 5.5 ist InnoDB die Standard-Speicher-Engine. Ab MySQL 5.6 wird auch in InnoDB-Tabellen eine Volltextsuche möglich sein. Jedoch erfüllt InnoDB den SQL3-Standard nicht vollständig: Fremdschlüssel werden nur eingeschränkt unterstützt.\n\nInnoDB speichert die Tabellenstruktur in einzelnen frm-Dateien, Nutzdaten und Indizes in einem Tabellenraum. Der Tabellenraum wird vor Beginn der Arbeit mit dem Datenbankserver eingestellt und kann sich über eine oder mehrere Dateien erstrecken. Die Dateien des Tabellenraums können auf verschiedene Verzeichnisse verteilt werden. Die Konfiguration des Tabellenraums kann nicht nachträglich angepasst werden, ohne Datenverlust zu riskieren. Nach einer Änderung der Konfiguration des Tabellenraums wird die gesamte Datenbank von einer Sicherungskopie wiederhergestellt.\n\nMERGE bietet die Möglichkeit, mehrere Tabellen vom Typ MyISAM mit gleicher Struktur zu einer Tabelle zusammenzufassen und die Zugriffe darauf auszuführen. Dabei können komprimierte MyISAM- mit nicht-komprimierten MyISAM-Tabellen zusammengefasst werden. Auf diese Weise lässt sich Datenarchivierung realisieren.\n\nManagement von temporären Tabellen. Die Definition der Tabellen wird auf der Festplatte permanent gespeichert. Die Daten werden im Arbeitsspeicher gespeichert. Es werden jedoch nicht alle Datentypen unterstützt. Bei einem Neustarten des Servers sind die Tabellenstrukturen noch vorhanden, die Inhalte müssen neu eingelesen werden, beispielsweise aus permanenten Tabellen. Es sind spezielle Verfahren zur Speicherplatz-Verwaltung implementiert, um den Platz von gelöschten Sätzen bei der nächsten Einfügung wiederzuverwenden.\n\nCode-Beispiel für die Entwicklung einer eigenen Speicher-Engine. EXAMPLE hat Funktionen zum Erstellen einer Tabelle, die Funktionen zum Schreiben und Lesen der Datensätze sind nur angedeutet. Ein SELECT-Statement liefert immer eine leere Ergebnismenge.\n\nBDB ist die Abkürzung für Berkeley DB. Diese Speicher-Engine wurde von Sleepycat Software entwickelt und später an Oracle verkauft. Die BDB bietet Transaktionssicherheit und besondere Vorkehrungen, damit bei einem Systemausfall die gespeicherten Daten erhalten bleiben.\n\nDie BDB-Speicher-Engine wird ab Version 5.1 nicht mehr weiter unterstützt.\n\nDie FEDERATED-Engine bietet Zugriff auf Tabellen, die auf einem anderen Server liegen. Wenn man eine Tabelle vom Typ FEDERATED erstellt, dann muss die entfernte Tabelle auf dem anderen Server bereits existieren. Der lokale Server verhält sich wie ein Client, der auf den entfernten Server zugreift. Die FEDERATED-Engine verhält sich wie ein föderiertes Informationssystem, das bedeutet, dass sie die Daten selber nicht speichert, sondern Zugriff auf den fernen Server gewährt, während auf diesem ebenfalls auf die Daten zugegriffen werden kann. Bei der Version 5.0 kann nur auf andere MySQL-Server zugegriffen werden.\n\nDie Zugangsdaten zu dem entfernten Datenbankserver werden dabei unverschlüsselt in der lokalen frm-Datei gespeichert. Der Zugriff auf das Datenverzeichnis muss auf der Betriebssystemebene eingeschränkt werden um das Auslesen der Zugriffsrechte durch Unbefugte zu verhindern.\n\nDie ARCHIVE-Engine ist für die Speicherung von großen Datenmengen bei einem möglichst sparsamen Umgang mit dem zur Verfügung stehenden Speicherplatz konzipiert. Es können keine Indizes erstellt werden. Nur die Zugriffe INSERT und SELECT werden unterstützt. Der schnelle Zugriff auf die Daten steht hier nicht im Vordergrund.\n\nVor dem Speichern der Daten auf dem Speichermedium werden diese zunächst in einem Kompressionspuffer gesammelt. Wenn eine Serie von Einfüge-Operationen beendet wird, wird der optimale Kompressionsalgorithmus ermittelt und die Daten werden komprimiert ausgegeben.\n\nFalls während einer Sequenz von Einfüge-Operationen von einem anderen Benutzer eine SELECT-Anfrage kommt, wird eine vorzeitige Kompression und Ausgabe der im Kompressionspuffer gespeicherten Daten erzwungen.\n\nBei der CSV-Engine werden die Daten im CSV-Format gespeichert: Zahlen werden nicht binär, sondern als Ziffernfolgen gespeichert; die einzelnen Werte werden durch Kommata getrennt.\n\nBLACKHOLE wurde entwickelt, um die Syntax von SQL-Statements zu prüfen und ein Binärlog zu schreiben. Die Daten werden nicht gespeichert. Dadurch können Syntaxprüfungen von SQL-Statements ausgeführt werden, ohne dass Speicherplatz zum Speichern der Daten bereitgestellt werden muss. Die Ausgabe des Binärlogs kann über einen Parameter aktiviert und deaktiviert werden.\n\nDie BLACKHOLE-Engine ist ideal für die folgenden Aufgaben:\n\nNDB ist die Abkürzung für Network Data Base.\nDie NDB-Speicher-Engine ist eine unabhängige Komponente, die die persistente Speicherung von Daten ermöglicht und für die Koordination aller Zugriffe auf Datenknoten in einem MySQL Cluster zuständig ist. Anwendungen können direkt auf die NDB-Speicher-Engine über die NDB-API oder über einen MySQL-Knoten zugreifen. Der Zugriff über einen MySQL-Knoten ist für Anwendungsprogrammierer wesentlich einfacher zu gestalten, da in diesem Fall Standard-SQL-Befehle verwendet werden können und das Erlernen der NDB-Spezialitäten nicht notwendig ist.\n\nDie NDB-API ist eine multithreading-fähige Schnittstelle zur Annahme aller ankommenden Datenanfragen. Für jede Anfrage werden ein oder mehrere Threads gestartet. Auf die NDB-API ist nur ein sequenzieller Zugriff möglich, wodurch die Leistung des Clusters bei sehr vielen ankommenden Anfragen vermutlich eingeschränkt wird.\n\nNeben den offiziellen Engines bieten mehrere Hersteller auch andere Engines mit anderen Eigenschaften oder Zusatzfunktionen. Einige seien hier beispielhaft erwähnt.\n\nDie Revision Engine von \"DDEngine\" fügt eine automatische Versionierung als Plugin auf Ebene einer Speicherengine hinzu. Neben dem reinen Speichern von Daten kann diese Engine dadurch gewährleisten, dass Daten so wieder herstellbar sind, wie sie zu einem bestimmten Zeitpunkt waren. Diese Eigenschaft kann z. B. genutzt werden um den Verlauf von Produkteigenschaften zu speichern oder um gesetzliche Auflagen zu erfüllen. Um die Daten physisch zu speichern, werden die mitgelieferten Storage-Engines benutzt.\n\nDie Firma Infobright stellt die Brighthouse Engine zur Verfügung. Sie ist für Data-Warehouse-Anwendungen konzipiert und auf die Verarbeitung besonders großer Datenmengen ausgerichtet. Indizes werden nicht unterstützt. Die Daten werden komprimiert gespeichert, wodurch nach Angaben des Herstellers bis zu 90 % des Speicherplatzes eingespart werden können.\n\nAb der Version 5.1 können MySQL-Tabellen partitioniert werden. Es stehen mehrere Partitionierungsarten zur Auswahl.\n\nBei der Range-Partitionierung werden Wertebereiche für die einzelnen Partitionen definiert.\n\nIn dem Beispiel wird eine Tabelle mit drei Partitionen erstellt. Die Spalte 'region' darf bei dieser Syntax nur Werte kleiner als 30 erhalten.\n\nCREATE TABLE `kunde` (\nPARTITION BY range(region) (\n\nDie Partitionierung kann auch durch einen Ausdruck ermittelt werden. Der Ausdruck muss einen Integer-Wert als Ergebnis generieren. Wenn die letzte Partition mit dem Wert 'maxvalue' definiert wird, damit kann man in die Spalte 'region' (Beispiel oben) alle Integer-Zahlen einfügen bzw. in der Spalte 'ed' (Beispiel unten) alle Datumswerte einfügen.\n\nCREATE TABLE `kunde` (\nPARTITION BY range(year(ed)) (\n\nBei der List-Partition werden die Werte einzeln aufgezählt.\n\nBeispiel:\n\nCREATE TABLE `kunde` (\nPARTITION BY list(region) (\n\nBei der Hash-Partitionierung wird die Verteilung der Sätze auf die einzelnen Partitionen vom DBMS ermittelt. Bei der regulären Hash-Partitionierung wird die Modulo-Funktion als Hashfunktion verwendet (Region modulo 4). Sie hat den Vorteil, dass in der Spalte 'region' alle Integer-Zahlen eingefügt werden können.\n\nCREATE TABLE `kunde` (\nPARTITION BY hash(region) PARTITIONS 4;\nEs gibt auch eine 'Lineare' Hash-Partitionierung. Dabei kommt eine andere Hashfunktion zum Einsatz.\n\nBei der Key-Partitionierung wird implizit eine Hashfunktion verwendet. Als Input für die Funktion dient der Primärschlüssel der Tabelle.\n\nCREATE TABLE `kunde` (\nPARTITION BY key() PARTITIONS 4;\nDie Key-Partitionierung wird bei der Speicherengine \"NDB\" implizit bei allen Tabellen verwendet. Das erleichtert die interne Koordination der Replikation.\n\nBei jeder Art von Partitionierung können zusätzlich 'Subpartitions' definiert werden. Dadurch ist eine noch granularere Aufteilung der Daten möglich.\n\nUnter Linux installiert sich MySQL in das Verzeichnis \"/var/lib/mysql/\". Unter Windows legt der Nutzer den Ablageort der Datendateien fest – Standard ist der Ordner \"%ProgramFiles%\\MySQL\". Grundeinstellungen werden durch den Administrator in der Datei \"my.cnf\" vorgenommen.\n\nZur Verwaltung von MySQL-Datenbanken dient der mitgelieferte Kommandozeilen-Client (Kommandos codice_3 und codice_4). Zum Funktionsumfang gehören außerdem die folgenden Kommandozeilenwerkzeuge:\n\n\nAls grafische Verwaltungssoftware bietet Oracle die Software MySQL Workbench an. Sie ist für die Betriebssysteme Windows, macOS und Linux verfügbar.\n\nEine von vielen Alternativen ist die in der Skriptsprache PHP geschriebene Open-Source-Anwendung phpMyAdmin. Die grafische Benutzeroberfläche lässt sich über einen Browser bedienen. phpMyAdmin wird hauptsächlich zur Verwaltung von MySQL-Datenbanken auf Webservern verwendet, auf denen die einzelnen Benutzer keine Rechte haben, codice_3 und codice_4 direkt auszuführen. Zum Erstellen und Verwalten von Backups der Datenbanken auf Webservern werden – wenn keine Rechte für die Ausführung von codice_7 vorliegen – häufig die ebenfalls in PHP geschriebenen Open-Source-Anwendungen phpMyBackupPro oder MySQLDumper eingesetzt.\n\nFür den Vertrieb von MySQL Server verwendet Oracle ein duales Lizenzsystem: Einerseits ist das Programm eine freie Software, die unter der General Public License (GPL) steht, andererseits wird es auch unter einer kommerziellen Lizenz angeboten.\n\nUnterstützung für den Einsatz von MySQL bietet zunächst das offizielle Handbuch. Außerdem gibt es mehrere Foren und IRC-Channels, in denen Fragen kostenlos beantwortet werden.\n\nDaneben bietet Oracle auch kostenpflichtige Support-Lizenzen in drei Leistungsstufen an: MySQL Enterprise Silver, Gold und Platinum. Sie unterscheiden sich in Leistungsumfang und Preis.\n\n\n\n"}
{"id": "14863", "url": "https://de.wikipedia.org/wiki?curid=14863", "title": "Konqueror", "text": "Konqueror\n\nKonqueror ist der freie Webbrowser des K Desktop Environment (abgekürzt KDE). Er kann auch als Dateimanager, FTP-Client oder Dateibetrachter eingesetzt werden.\n\nDer Name \"Konqueror\" beruht auf einem Gedankenspiel ausgehend von den Namen der vorigen Generationen von Browsern: Zuerst gab es den „Navigator“ von Netscape, danach den „Erforscher“ alias Internet Explorer von Microsoft und nun den „Eroberer“ – Konqueror.\n\nDas englische Wort wird korrekt \"Conqueror\" geschrieben. Ein \"K\" anstelle ähnlicher Laute hat sich zum Kennzeichen von KDE-Programmen entwickelt, beispielsweise auch beim Dokumentenbetrachter \"Okular\" oder der digitalen Fotoverwaltung \"digiKam\".\n\nKonqueror nutzt KHTML als Rendering Engine, deren Abspaltung WebKit auch Verwendung in Apples Webbrowser Safari und in Form der WebKit-Abspaltung Blink in Googles Chrome und Opera ab Version 15 findet. Dank QtWebKit (seit Qt 4.4) bzw. QtWebEngine (seit Qt 5.4) kann Konqueror jedoch auch die WebKit- bzw. die Blink-Portierung von Qt als Rendering Engine nutzen. Auch mit KHTML ist Konqueror ein HTML-4.01- sowie HTML-5-konformer Webbrowser mit Unterstützung für JavaScript (ECMA-262) und CSS2 & CSS3 (Cascading Style Sheets) sowie bidirektionaler Schriften wie Arabisch.\n\nEr unterstützt den sicheren Betrieb von Java-Miniprogrammen (Applets), Netscape-Plugins für Flash, RealAudio und Real Video sowie SSL für sichere Verbindungen. Zu den fortgeschrittenen Funktionen gehören die automatische Vervollständigung von URIs, das automatische Ausfüllen von Formularen, die Verwendung einer Rechtschreibkontrolle mit freier Sprachwahl, die Fähigkeit, Lesezeichen anderer Browser zu importieren, und die auch aus anderen Browsern bekannte Registernavigation.\n\nDer Konqueror stellt eine Schnittstelle bereit, über die mittels KDEs KParts-Technologie zahlreiche KDE-Programme gleichsam als Komponenten (\"KParts\") eingebettet werden können. Diese Komponenten stellen dann die eigentliche Funktionalität bereit.\n\nAuf diese Art und Weise stellt der Konqueror einen Dateimanager für die \"K Desktop Environment\" (KDE) mit Funktionen für die Dateiverwaltung zur Verfügung. Diese reichen u. a. von einfachen Ausschneide-/Kopier- und Einfügeoperationen bis hin zum fortgeschrittenen Durchsuchen der Datei- und Verzeichnisstruktur. Dieses ist sowohl im Netzwerk als auch lokal möglich. Verzeichnisinhalte können auf vielfältige Weise im Text- und Symbol-Anzeigemodus dargestellt werden. Dies umfasst Minibildvorschauen von Dateiinhalten. Datei- und Verzeichniseigenschaften können auf einfache Weise untersucht und verändert werden, und Programme lassen sich mit einem Klick der linken Maustaste starten.\n\nSeit KDE 4 (in einzelnen Distributionen – zum Beispiel Kubuntu – auch früher) wurde Konqueror als Standard-Dateimanager von Dolphin abgelöst, der speziell auf Dateiverwaltung optimiert ist und von Konqueror als Komponente (KPart) für die Dateiverwaltungsfunktionalitäten genutzt wird.\n\nÜber KParts lässt sich auch WebKit zum Anzeigen (\"Rendern\") von HTML- und SVG-Dateien einbinden, wodurch die Unterstützung von (neueren) Webstandards gebessert wird.\n\nKonqueror ist auch ein vollwertiger FTP-Client, kann problemlos mit Windows-Freigaben umgehen (sofern Samba im System installiert ist) und Dateien mittels Secure Shell mit einem entfernten Rechner austauschen. Außerdem kann er mit seinem Cervisia-Modul automatisch auf CVS-Repositories zugreifen.\n\nEine hervorragende Eigenschaft von Konqueror ist die Möglichkeit, jedes Fenster oder Unterfenster zwei- oder mehrfach horizontal oder vertikal zu unterteilen. In jeden Bereich passen beliebige Inhalte. So können zum Beispiel per „Drag and Drop“ Dateien zwischen verschiedenen Maschinen verschoben oder Texte und Bilder von Websites kopiert werden.\n\nDer Konqueror ist ein universeller Dateibetrachter, der mittels als KPart eingebundener Betrachter Bilder und andere Dokumente verschiedenster Dateiformate darstellen kann, ohne dass man hierzu erst ein anderes Programm starten müsste. Dies wird ebenfalls durch die Einbettung von Komponenten (KParts) erreicht, die von anderen Programmen bereitgestellt werden; KView für die Betrachtung von Bildern, kdvi für die Betrachtung von DVI-Dateien, Kghostview für die Betrachtung von PostScript-Dokumenten und diverse KOffice-Anwendungen für ihre jeweiligen Dokumenttypen. Außerdem gibt es die Möglichkeit, OpenOffice.org als Engine zur Dokumentbetrachtung einzubinden, da manche Distributionen standardmäßig auf Koffice verzichten. Ab der Version 3.2 von KDE ist mit KSVG auch ein Betrachter für SVG-Dateien integriert.\n\nKonqueror ist vielfältig an die Benutzerbedürfnisse anpassbar. Dies betrifft den allgemeinen Stil, die Schrift- und Symbolgrößen, die Auswahl der in der Menüzeile angezeigten Symbole, die Anzahl und der Platz der Werkzeugleisten und die Definition neuer Tastenkürzel. Die verschiedenen Einstellungsprofile können gespeichert werden, um bei Bedarf schnell darauf zugreifen zu können.\n\nDie KHTML-Engine des Konqueror besteht ab Version 3.5 fehlerfrei den Acid2-Browsertest. Beim Acid3-Browsertest erreicht Konqueror in Version 4.14.2 die vollen Punkte, allerdings ist der Schriftzug „“ in der linken oberen Ecke sichtbar.\n\nÜber bestimmte Pseudoadressen kann mittels eingebundener KIO-Slaves auf viele weitere Ressourcen neben HTTP zugegriffen werden.\nDazu können neben der Protokollangabe \"http://\" folgende weitere Kürzel als Adresspräfixe in Konquerors Adresszeile bzw. KDEs Dateidialog eingegeben werden:\nDiese Liste ist erweiterbar durch weitere Zusatzpakete, zum Beispiel für ftps:// oder package://\n\nEin weiteres Merkmal sind die Web-Kurzbefehle (Web-Shortcuts), mit denen eine Suchmaschinenanfrage direkt aus der Adressleiste gestartet werden kann. Neben einigen voreingestellten Befehlen kann man zusätzlich auch eigene Suchbefehle erstellen.\n\nKonqueror wurde in KDE in dessen Version 2 eingeführt, welche am 23. Oktober 2000 erschien.\nDarin löste Konqueror den KDE file manager (KFM) als Standard-Dateimanager ab.\nIn KDE 4 (veröffentlicht am 18. August 2006) wurde Konqueror als vorgegebener Dateimanager durch Dolphin ersetzt. Konqueror bietet weiterhin Dateimanagerfunktionalität, indem nun Dolphin als KPart eingebunden wird.\n\n"}
{"id": "14865", "url": "https://de.wikipedia.org/wiki?curid=14865", "title": "Opera (Browser)", "text": "Opera (Browser)\n\nOpera ist ein kostenloser proprietärer Webbrowser. Er basierte bis einschließlich Version 12.18 auf einem eigenen HTML-Renderer. Seit der 2013 freigegebenen Version 15 nutzt er den Renderer der jeweils aktuellen Version des Webbrowsers Chromium. Opera ist für viele Plattformen verfügbar. Alleiniger Entwickler der geschlossenen Softwarekomponenten ist das norwegische Unternehmen Opera Software, das sich auch an der Fortentwicklung der quelloffenen Rendering-Engine beteiligt.\n\nNach einer enttäuschenden Geschäftsentwicklung und einer anschließenden Neubewertung der Geschäftsstrategie ab 7. August 2015 stimmten Ende Mai 2016 die erforderlichen über 90 % der Aktionäre kurz vor Ablauf der verlängerten Angebotsfrist einem Verkauf des Betreibers des Internetbrowsers (\"Opera Software\") an das chinesische Konsortium \"Golden Brick Silk Road Equity Investment Fund II LLP\" zu, bestehend unter anderen aus dem chinesischen Investmentfonds \"Golden Brick Silk Road Fund Management LLP\" aus Shenzhen, den auf Mobilspiele bzw. Antivirensoftware spezialisierten Unternehmen \"Beijing Kunlun Tech\" und \"Qihoo 360 Software Co. Ltd.\" aus Peking sowie der \"Yonglian Investment Co. Ltd.\" aus Yinchuan. Das Konsortium hatte im Februar 10,5 Mrd. Kronen (1,1 Mrd. Euro) geboten.\n\nBis einschließlich Version 4 im Dezember 2000 war Opera grundsätzlich kostenpflichtig; danach zeigte er ein Werbebanner an, solange er nicht kostenpflichtig registriert war (Adware). Ab Version 8.5 (September 2005) ist Opera kostenlos und werbefrei (Freeware).\n\nDas Programm bot bereits seit der ersten Version das aus dem Browser \"InternetWorks\" bekannte sogenannte Tabbed Browsing an und führte früh Mausgesten ein. Beide Funktionen wurden auch von anderen Browsern übernommen.\n\nDas Projekt Opera begann im April 1994 als Forschungsprojekt des norwegischen Telekommunikationsunternehmens Telenor und wurde 1995 als eigenständiges Unternehmen unter dem Namen Opera Software ausgegliedert. Im März 2004 ging Opera an die Osloer Börse. Im Mai 2004 gab Opera bekannt, durch einen Vergleich in einem Rechtsstreit mit einem nicht näher bezeichneten internationalen Unternehmen 12,75 Millionen US-Dollar erhalten zu haben.\n\nDie aktuelle Version von Opera im Desktopbereich ist für folgende Betriebssysteme verfügbar: Microsoft Windows, Mac OS X, Linux, Solaris und FreeBSD. Ältere Versionen von Opera gibt es für BeOS, QNX und OS/2.\nOpera steht in 50 verschiedenen Sprachen zur Verfügung. Die offizielle deutschsprachige Version ist auch bezüglich der eingebauten Suchfunktion (siehe Bedienung) an den Sprachraum angepasst.\n\nOpera ist für Linux sowohl dynamisch als auch statisch gegen die Qt-Bibliothek gelinkt verfügbar. Die statisch gelinkte Version ist jedoch meist nur noch über den FTP-Server verfügbar.\n\nZudem gibt es eine portable 64-bit-Programmversion von Opera, die sich ohne Installation von verschiedenen Speichermedien unter Microsoft Windows verwenden lassen. Außerdem existiert eine Version für die U3-Plattform.\n\nSeit Februar 2006 veröffentlicht Opera Software sogenannte Schnappschüsse („Snapshots“) der aktuell in Entwicklung befindlichen Version des Browsers, welche am 3. Mai 2011 in Opera Next umbenannt wurde um eine unabhängige Installation gegenüber einer vorhandenen stabilen Version zu gewährleisten. Diese werden in unregelmäßigen Abständen veröffentlicht und enthalten meist kleinere Änderungen, die die Entwicklung des Browsers aus Anwendersicht nachvollziehbar machen sollen. Da es sich um Vorabversionen handelt, enthalten Snapshots häufig Bugs.\n\nOpera bestand bis einschließlich Version 12 aus drei Modulen: dem Browser selbst, Opera Mail und Dragonfly.\nDie drei Module umfassten unter anderem folgende Funktionen:\n\nAb Version 15 wurde Opera Mail nicht mehr weiterentwickelt, man kann es als eigenes Programm jedoch immer noch auf der Website des Herstellers herunterladen, Opera Dragonfly wurde komplett herausgenommen. Neu hinzugekommen sind:\n\nBei der Anzeige von Webseiten erfüllt Opera weitgehend die aktuellen Standards für HTML und CSS. Ab Version 8.0b3 enthält Opera eine Unterstützung für eine Teilmenge des SVG-Grafikformates und unterstützt ab Version 9.50 APNG. Im Bereich der neuen Webtechnologien schneidet Opera vergleichsweise gut ab und unterstützt bereits jetzt viele sich noch in der Entwicklung befindende Standards.\nSeit 2003 wird die eigens entwickelte und plattformübergreifende HTML-Rendering-Engine Presto verwendet, seit 2013 allerdings nur noch im Opera Mini (siehe weiter unten). Sie unterscheidet sich von ihrem Vorgänger vor allem im Hinblick auf ihre dynamische Arbeitsweise: Aufgerufene Seiten oder deren Teile können jederzeit neu gerendert werden. Presto unterstützt zudem das Document Object Model (DOM) des W3C vollständig. Des Weiteren interpretiert Presto sogenanntes \"Street HTML\" (abwertend sinngemäß: „Gassen-Quelltext“). Damit bezeichnet der Hersteller nicht standardkonforme, für den damals weit verbreiteten Internet Explorer optimierte Webseiten.\n\nUrsprünglich verwendete man für die Rendering-Engine keine interne Bezeichnung. Nachträglich erhielt die Engine der Opera-Versionen 3.5 bis 6 den Namen „Elektra“, was ursprünglich der Projektname der vierten Version des Webbrowsers war. Ab Opera-Version 7 wurde die Engine formal als „Opera Presto“ bezeichnet.\n\nAm 13. Februar 2013 kündigte Opera an, in Zukunft auf WebKit zu setzen und die Produkte im Laufe des Jahres darauf umzustellen. Am 3. April 2013 gab Google bekannt, seine Mitentwicklung von WebKit zugunsten der Webkit-Abspaltung Blink einzustellen und Blink in Chromium zu verwenden. Opera gab am gleichen Tag bekannt, Blink für zukünftige Versionen von Opera zu benutzen.\n\nDie Zoom-Funktion von Opera vergrößert nicht nur Texte, sondern skaliert und glättet auch Bilder und Plug-ins. Die Größenänderung ist von 20 % (ein Fünftel der normalen Darstellung) bis 1000 % einstellbar. Der „Benutzermodus“ dient zur besseren Lesbarkeit von Webseiten (barrierefreies Internet). Mit seiner Hilfe kann der Nutzer das Aussehen einer beliebigen Webseite mit eigenen CSS-Dateien anpassen; beispielsweise lassen sich Bilder ausblenden und Farben verändern (zum Beispiel in weißen Text auf schwarzem Hintergrund). Bei der Navigation mit der Maus über die Tabs wird eine Miniaturvorschau der bereits geladenen Inhalte angezeigt. Der Vollbildmodus zeigt die aktuelle Seite ohne Menüs oder Paneele an. Ab Version 10 lässt sich zudem die Tab-Leiste verbreitern beziehungsweise aufklappen, wodurch die Tabs selbst breiter werden und im erscheinenden Leerraum eine Seiten-Vorschau anzeigen.\n\nBis Version 12 gab es auch einen Kioskmodus, der Opera im Vollbild ohne Menüs und ohne Hinweis auf das verwendete Betriebssystem zeigt. Allerdings kann man relativ leicht aus dem Browser „ausbrechen“. Der Kioskmodus schützt also nicht vor Vandalismus. Außerdem lassen sich mit der Funktion „An Seitenbreite anpassen“ Webseiten, die für große Auflösungen geschrieben wurden, auch auf kleinen Bildschirmen lesen, und mit der „Small screen“-Funktion kann die Webseite wie mit dem Opera-Mini-Mobilbrowser betrachtet werden, was besonders für Webdesigner von Vorteil ist, da dies die Überprüfung der Kompatibilität erleichtert.\n\nMithilfe der Multiple-Document-Interface-Oberfläche können mehrere Seiten innerhalb eines Opera-Fensters geöffnet werden. Um zwischen den Seiten zu wechseln, steht eine Leiste ähnlich der Windows-Taskleiste zur Verfügung. Mit speziellen Befehlen können alle außer der aktuellen Seite geschlossen oder geschlossene Seiten wieder angezeigt werden. Die Navigation zwischen den Tabs erfolgt über die Tastatur, das Mausrad oder durch Anklicken der Tabs.\n\nMittels Mausgesten kann Opera besonders schnell mit der Maus bedient werden, größtenteils ohne dass Schaltflächen angeklickt werden müssen. Gleichzeitig ermöglichen aber auch Tastenkürzel eine weitgehende Bedienung ohne Maus.\n\nAb Version 8.00 unterstützt Opera die Steuerung über Sprachbefehle und das Vorlesen von Webseiten. Dazu wird die Technologie VoiceXML verwendet. Diese Funktion ist derzeit nur auf Englisch verfügbar und muss aufgrund der Dateigröße zusätzlich heruntergeladen werden.\n\nEine in die Adressleiste eingebaute Suchfunktion mit der Möglichkeit, Kürzel für verschiedene Seiten festzulegen, vereinfacht Suchvorgänge bei diversen Suchmaschinen und Onlinehändlern. Eine weitere Funktion im entsprechenden Kontextmenü ermöglicht es, ein Eingabefeld auf einer Webseite als Suchanbieter hinzuzufügen, der dann sofort verwendet werden kann. Die Suche in der Seite findet Ergebnisse bereits während der Eingabe. Suchen innerhalb aller Bereiche von Opera ist möglich.\n\nAlle momentan aufgerufenen Seiten und deren Anordnung konnten bis Version 12 als „Sitzung“ gespeichert werden, um später damit fortzufahren. Man konnte beim Programmstart eine bestimmte Sitzung laden, jedoch auch die jeweils letzte Sitzung fortsetzen. Ab Version 15 kann man nur Wahlweise mit der Schnellwahl oder der letzten Sitzung beginnen.\n\nMit der Schnellwahl-Funktion „Speed Dial“ ab Version 9.2 kann man in einer neuen Seite bis zu neun Favoriten voreingestellt anzeigen lassen oder per Tastaturkürzel laden; ab Version 10 wurde diese Zahl auf 25 erhöht.\n\nDer Passwort-Manager automatisiert Anmeldevorgänge auf Seiten, die Benutzernamen und Passwort erfordern. Diese Funktion wurde in Opera bis Version 9.64 als „Wand“ (von „magic wand“, englisch für ‚Zauberstab‘) bezeichnet und lässt sich auf Wunsch auch mit einem Opera-eigenen Master-Passwort schützen. Das Master-Passwort kann auch für das E-Mail-Programm Opera Mail verwendet werden.\n\nDie Oberfläche von Opera lässt sich zum großen Teil verändern. Der Nutzer kann sämtliche Symbolleisten nach Belieben anpassen oder ganz ausblenden. Das grafische Erscheinungsbild lässt sich mit verschiedenen Skins (ab Opera 7) verändern, die von der Opera-Website heruntergeladen werden können.\n\nAb Version 9.0 ist außerdem ein neuer Suchmaschineneditor eingebaut, der das Anpassen von Suchmaschinen im GUI ermöglicht.\n\nBis einschließlich Version 10.10 setzt Opera zum Zeichnen der grafischen Benutzeroberfläche für unixoide Betriebssysteme auf die Qt-Bibliothek und für Windows auf die Windows APIs, Mobilgeräte auf Java. Ab Version 10.50 der Variante für Desktopcomputer übernimmt diese Aufgabe die hauseigene Vega-Vektorgrafik-Engine.\n\nOpera konnte bis Version 12 ebenfalls Seiten in beliebigen Intervallen automatisch neu laden. Zur Validierung (Prüfen auf Fehler) des HTML-Quelltextes mittels Übergabe an den \"W3C Markup Validation Service\" existiert eine entsprechende Funktion im Kontextmenü.\n\nNeben dem normalen Vor- und Zurückgehen beim Surfen enthielt Opera „schnelle“ Vor- und Rücklauf-Schaltflächen. Erkennt Opera in einer Webseite eine wiederkehrende Formatierung (beispielsweise Vorschaubilder oder eine Auflistung von Bildern) kann man mit dem Vorlauf-Button alle Bilder oder Dokumente ansehen, ohne jedes Mal zurückgehen und neu klicken zu müssen. Mit der Rücklauf-Schaltflächen kommt man mit einem Klick zum Beispiel auf die Hauptseite eines Diskussionsforums zurück, auch wenn man sich bereits im dritten Unterforum befindet.\n\nEbenfalls enthalten ist eine Funktion zum erneuten Öffnen von geschlossenen Tabs (es wird dort fortgesetzt, wo das Tab geschlossen wurde). Der Seitenverlauf wird nach dem Schließen von Opera aus der Liste entfernt.\n\nBei einem Absturz merkt sich der Browser die geöffneten Seiten, nach dem Neustart kann man an derselben Stelle weiterarbeiten.\n\nIn Texten eingebettete Internetadressen (URLs) können durch einen Doppelklick markiert werden. Darauf öffnet sich ein Kontextmenü, über das man die Seite direkt aufrufen kann.\n\nEbenfalls im Kontextmenü sind Direktlinks auf eine Enzyklopädie (Wikipedia), ein Wörterbuch, ein Untermenü zur Auswahl einer Suchmaschine und derzeit (August 2009) 19 Übersetzungsmöglichkeiten integriert, mit denen einzelne, markierte Wörter oder ganze Sätze nachgeschlagen werden können.\n\nOpera kann jede offene Seite inklusive ihrer kompletten Historie auf Tastendruck in den Hintergrund duplizieren. Text, der in Felder eingegeben wurde, bleibt dabei erhalten.\n\nOpera kann sich Webservern gegenüber auch als Mozilla oder Internet Explorer ausgeben, um Browserweichen zu umgehen, die Opera-Benutzer abweisen. Er kann sich diese geänderte Identifikation für individuelle Seiten merken und wendet sie beim nächsten Besuch automatisch an.\n\nJede Seite kann individuelle Einstellungen wie zum Beispiel die Verwendung von JavaScript, Cookies, Popupblocker usw. erhalten.\n\nOpera bietet die Möglichkeit, Pop-ups zu blockieren, an; in der Standardinstallation ist diese Funktion bereits aktiviert. Dabei werden in erster Linie Pop-ups blockiert, die von einer anderen Domain als der, von der aus das Pop-up aufgerufen wird, stammen. Diese Methode ist natürlich nicht sehr effektiv zur Blockierung von Werbung, stellte jedoch in der Vergangenheit, als Pop-ups noch exzessiv Anwendung fanden, eine sichere Möglichkeit dar, dem Anwender das Surfen einfacher zu gestalten.\n\nDer eigentliche Inhaltsfilter und Werbeblocker von Opera ist in Form einer einfach strukturierten Initialisierungsdatei realisiert. Diese codice_1 genannte Datei befindet sich im Profil-Verzeichnis von Opera. Sie ist in der Standardinstallation von Opera nicht enthalten. Bei der ersten Verwendung der integrierten „Inhalte blockieren“-Funktion wird die codice_1 von Opera automatisch im Profil-Verzeichnis angelegt. Inhalte können über die Programmoberfläche oder durch direktes Editieren der codice_1-Datei blockiert werden. Die Verwendung von Wildcards zum Blockieren ganzer Domains oder einzelner Pfade ist möglich. Im Internet finden sich diverse Websites mit vorgefertigten Filterdateien.\n\nBeim Programmstart von Opera wird die Datei, so vorhanden, eingelesen. Alle darin enthaltenen URLs mit allen Unterseiten werden blockiert. Auch lassen sich ganze IP-Bereiche sperren, wie auch Zugriffe auf Server, die sich außerhalb eines bestimmten Netzes befinden.\n\nZusätzlich zum Inhaltsfilter besteht die Möglichkeit, Werbung mit \"Cascading Style Sheets\" auszublenden. Man kann eine oder mehrere User-CSS-Dateien aktivieren (vgl. Seitendarstellung).\n\nMit Opera 37 wurde ein nativer Werbeblocker unter der Verwendung der Filterliste \"EasyList\" wieder eingeführt.\n\nIn Opera integriert war das E-Mail-Programm „Opera Mail“ (früher „M2“). E-Mails werden in Opera Mail in einer zentralen Liste gespeichert und mithilfe sogenannter „Filter“ sortiert. So ist es möglich, sämtliche E-Mails an und von einer bestimmten Adresse zu sehen, indem der Nutzer einmalig einen Filter mit entsprechenden Bedingungen erstellt. Jede E-Mail kann in beliebig vielen Filtern erscheinen, ohne selbst kopiert oder verändert zu werden.\n\nVerschiedene Filter legt Opera Mail automatisch an: So gibt es „Aktive Adressen“-Filter, mit denen man auf alle Mails von Personen zugreifen kann, mit denen man kürzlich Mails ausgetauscht hat, verschiedene „Aktuelle Korrespondenz“-Filter sammeln automatisch aktuelle Mail-Korrespondenzen (bestehend aus empfangenen Mails und eigenen Antworten). Auch Mailinglisten werden automatisch erkannt und es gibt Filter für verschiedene Dateianhänge (zum Beispiel alle Mails mit Bildern). Mailinglisten können in Baumdarstellung angezeigt werden.\n\nGespeichert werden E-Mails in mbox-Dateien, die sich auch mit einem Texteditor betrachten lassen. Diese besitzen in der Regel die Dateiendung \".mbs\". Außerdem enthalten ist ein lernfähiger Spamfilter (Bayes Spamfilter) sowie Unterstützung von POP3-, IMAP- und Newsgroups. Seit der Opera-Version 7.5 ist es auch möglich, RSS-Feeds zu verwalten.\n\nOpera kann Text- und HTML-Mails darstellen. Das Erstellen von HTML-Mails ist ab Version 10 möglich.\n\nSeit Opera 12.0 besaß Opera eine MAPI-Schnittstelle zur vollständigen Integration des E-Mail-Programms in das Betriebssystem Windows. Um E-Mails aus Drittanwendungen versenden zu können, benötigt diese Funktionalität keine zusätzlich zu installierenden Plugin mehr.\n\nMit Version 15 wurde das E-Mail-Modul aus dem Browser ausgegliedert und wird seitdem als separates Programm weitergeführt.\n\nDragonfly wurde am 6. Mai 2008 als Alpha-Version der Öffentlichkeit präsentiert. Es handelt sich um ein Entwicklertool, mit dem sich DOM-Elemente als auch JavaScripts auf deren Aufbau hin untersuchen lassen und zeigt etwaige Fehler auf. Dies ermöglicht es Webdesignern und Programmierern unter anderem herauszufinden, welche CSS-Eigenschaften auf welches DOM-Element wirken.\nIn den Versionen 15 ff ist Dragonfly nicht mehr enthalten\n\n\nMit dem neu entwickelten Opera 15 wurde der Mail-Client, das Entwicklertool Dragonfly und weitere Funktionen, die in Opera 12.18 noch vorhanden waren, entfernt.\n\nStatt Presto wird Blink als HTML-Rendering-Engine verwendet.\n\nHinzu kamen in Version 15 die erweiterte Schnellwahl sowie die Funktion „Stash“ (). Damit lassen sich Webseiten mit einem Klick in einer Liste in Form von Abbildern (Screenshots) speichern. Auf Wunsch kann man gespeicherte Seiten zusammenziehen, so dass man nur Links sieht.\n\nDie Schnellwahl erhielt eine Ordnerfunktion. Werden zwei Schnellwahl-Felder übereinandergelegt, erzeugt das einen Ordner, in dem man oft besuchte Seiten gruppieren kann. Die neue Suchfunktion bietet in Schnellwahl-Ordnern schnellen Zugriff auf gespeicherte Inhalte.\nDie Funktion „Discover“ zeigt ausgewählte Inhalte wählbarer Nachrichtenseiten geordnet nach bevorzugten Themen an.\n\nEinige Merkmale der Version 12 wurden in späteren Opera-Versionen integriert – darunter ein Modul zur RSS-Anzeige.\n\nAm 23. Januar 2007 wurde eine spezielle „Opera OLPC Edition“ für den 100-Dollar-Laptop vorgestellt.\nDabei handelt es sich fast um die normale Desktopversion. Sie wurde mittels gcc 4.1 kompiliert (anstatt 2.95, ebenso wurde Qt 3.3.7 anstatt 3.3.5 verwendet) und hat ein verändertes Aussehen.\n\nOpera ist auch in zwei Versionen für Smartphones und PDAs erhältlich. Opera Mobile wird für Windows Mobile, für Smartphones mit Android (ab Version 1.6), MeeGo, Symbian OS und Windows Mobile sowie für Tabletcomputer mit Android (ab Versionszweig 3 \"Honeycomb\") und MeeGo plattformabhängig angeboten. „Opera Mini“ ist für fast alle Java-fähigen Mobiltelefone sowie für das iPhone (iOS) erhältlich.\n\nIm Herbst 2005 veröffentlichte Opera Software die ersten Versionen seines MIDP-kompatiblen Browsers Opera Mini für Java-fähige Mobiltelefone und PDAs.\n\nOpera Mini nutzt einen speziellen Proxyserver, der von Opera Software betrieben wird und der die Seiten und Bilder der Website mit der Small-Screen-Rendering-Technik aufbereitet, komprimiert und an das Mobiltelefon weiterschickt. Dadurch werden die Seiten nicht nur an die meist kleinen Displays angepasst, sondern auch das Datenvolumen und der Rechenaufwand werden reduziert. Auch beim Besuch einer verschlüsselten Seite entschlüsselt der Opera-Mini-Transcoder-Server die Verbindung, verschlüsselt sie erneut und bricht damit die Ende-zu-Ende-Verschlüsselung.\n\nAnfang Mai 2006 erschien Opera Mini in Version 2.0 und Ende November 2006 in Version 3.0. Die Version 4.2 war seit dem 25. November 2008 verfügbar und zeigte die Seite zunächst stark verkleinert an, so dass sie in voller Breite auf das Display passte. Danach konnte man einzelne Teile der Seite auswählen und in voller Größe betrachten. Ab 20. November 2009 wurde Benutzern aus China nur noch eine spezielle \"Opera Mini China Version\" angeboten, die ein Umgehen der chinesischen Internetzensurmaßnahmen verhindert. Ein voll funktionsfähiger „Opera Mini™-Simulator“ wird als Java-Applet angeboten. Seit dem 13. April 2010 wurde Opera Mini 5.0 auch für das iPhone angeboten. Innerhalb des ersten Tages wurde der neue Opera-Mini-Browser über eine Million Mal als iPhone-App heruntergeladen. Damit führte die Opera-App die Downloadcharts für das iPhone an.\n\nSeit Juli 2011 war Version 6.0.1 verfügbar. Am 27. Februar 2012 wurde mit Opera Mini Next die neue Version 7 des mobilen Browsers angekündigt, die im Laufe des Jahres erschien. Im Jahr 2014 wurden die Versionen 8 und 9 veröffentlicht und schon im Jahr darauf folgte Opera Mini 10.\n\nIm Unterschied zu Opera Mini ist Opera Mobile ein kompletter Browser ohne Abhängigkeit von einem speziellen Server. Opera Mobile wird für Pocket PCs mit Microsoft Windows Mobile oder Microsoft Windows 7, für Smartphones mit Android (ab Version 1.6), MeeGo, Symbian OS und Microsoft Windows Mobile sowie für Tabletcomputer mit Android (ab Versionszweig 3 \"Honeycomb\") und MeeGo plattformabhängig angeboten. Am 11. Oktober 2011 veröffentlichte Opera Software die Version 11.50 von Opera Mobile für Android. Diese wurde am 27. Februar 2012 durch die aktuelle und finale Version 12 für Android und Symbian ersetzt. Laut Expertenmessungen übertrifft Opera Mobile 12 andere Browser wie Apple Safari oder Google Chrome in seiner Geschwindigkeit, insbesondere bei der Darstellung von HTML5-Inhalten, und kann als der derzeit  schnellste Webbrowser für mobile Endgeräte gelten.\n\nBei Mobiltelefonen mit der Symbian-Benutzeroberfläche UIQ 3 ist Opera Mobile in der Version 8.65 integriert.\n\nOpera entwickelte in Partnerschaft mit Nintendo einen Nintendo-DS-Browser, mit dem es möglich ist, auf diesem Gerät herkömmliche Webseiten zu besuchen. Allerdings werden weder Flash noch Java-Inhalte unterstützt. Mitgeliefert wird eine RAM-Erweiterung für den zweiten Slot des Nintendo DS (Lite). Der Browser erschien am 24. Juli 2006 in Japan, am 6. Oktober 2006 in Europa und am 8. Juni 2007 in den USA. In Europa wurde der Browser für den empfohlenen Preis von 39,99 € angeboten.\n\nDes Weiteren wurde am 22. Dezember 2006 ein Opera-Browser für Nintendos Spielekonsole Wii veröffentlicht (genannt wurde der Browser „Internet Kanal“). Als Speicherplatz wird ein eingebautes Flash-ROM \"(Read-only-Memory)\" verwendet. Alle Webstandards der Desktopversion des Browsers werden auch auf Wii unterstützt, im Gegensatz zur DS-Version auch Adobe Flash. Die Navigation erfolgt mit der „Wiimote“, dem Einhand-Controller des Wii. Der Browser kann nun wieder gratis bezogen werden, nachdem er zwischenzeitlich für 500 Wii-Punkte (entspricht 5 €) gekauft werden musste.\n\nAm 3. April 2009 erschien gemeinsam mit der überarbeiteten Nintendo-DS-Version, dem Nintendo DSi, eine „Nintendo DSi Browser“ genannte Version des Opera-Browsers, die über den Download-Dienst „DSiWARE“ erstmals kostenlos bezogen werden kann.\n\nUnter dem Geschäftsfeld Opera Devices (zu dem eigentlich auch der Bereich „Spielkonsolen“ gehört) bietet Opera Herstellern von vernetzten Geräten die Möglichkeit, einen vollwertigen Webbrowser zu integrieren. Die bekannteste Partnerschaft ist die mit Nintendo. Weitere Einsätze von auf Opera-Technologie basierenden beziehungsweise von Opera entwickelten Browsern sollen in den Bereichen „Set-Top-Boxen“, „Mobile Internetgeräte“, „IP-Telefone“, „Fahrzeuge“, „Mobile Navigationsgeräte“, „Business Terminalgeräte“ und „Haushaltsgeräte“ erfolgen.\n\nIm September 2013 veröffentlichte Opera Software den speziell für iOS entwickelten Webbrowser Opera Coast.\nDie Bedienung des Browser ist für Apples mobile Hardware optimiert und setzt vollständig auf berührungssensitives Navigieren mittels Touchscreen.\n\nAm 25. April 2018 erschien der Browser Opera Touch für Android im Google Play Store. Seit Oktober 2018 ist Opera Touch auch für Apples iOS verfügbar. Die Benutzeroberfläche wurde mit dem Red Dot Communication Design Award 2018 ausgezeichnet. Der Browser unterstützt eine einhändige Nutzung und kann mit dem Opera Flow des Browser am PC verbunden werden, um Bilder oder Notizen zwischen PC und Smartphone verschlüsselt auszutauschen.\nDer interne Werbeblocker kann ein- und ausgeschaltet werden.\n\nAls Opera Ende 1996 in Norwegen vorgestellt wurde, war ihm aufgrund der Konkurrenz durch den Internet Explorer von Microsoft und den Netscape Navigator zunächst kein großer Erfolg beschieden, zumal zu diesem Zeitpunkt zwischen den beiden kostenlosen Wettbewerbern der sogenannte Browserkrieg geführt wurde.\n\nWenngleich Opera gegenüber der Konkurrenz einen vergleichsweise geringen Marktanteil besaß, konnte er auch 2001, als die Software noch mit Werbeeinblendungen als Shareware vertrieben wurde, ein gutes Wachstum verbuchen. Der Marktanteil nahm jedoch in den darauf folgenden Jahren, trotz der Umstellung auf den kostenlosen Vertrieb, nicht signifikant zu.\n\n\"Market Share\" stellte im Januar 2013 einen Marktanteil des Browsers um 1,75 Prozent (mit geringer Aufwärtstendenz seit Mai 2012 mit 1,57 %) fest. \"The Counter\" stellte mangels Nachkommastellen im Dezember 2008 einen Anteil von einem Prozent fest. StatCounter gibt den Nutzeranteil des Opera Desktop Browsers 2015 mit 1,81 % an. Da sich solche Statistiken immer nur auf eine mehr oder minder große Stichprobe beziehen und zudem je nach Land und Publikum oft große Unterschiede bestehen, sind solche Aussagen mit Vorsicht zu genießen. Auf den häufig von Technikbegeisterten besuchten Websites Heise online und Golem.de lag der Anteil der Opera-Benutzer beispielsweise 2008 zwischen 6 und 8 %. Bei einer Untersuchung von 15 Exploit-Websites zeigte sich, dass 26 % der Administratoren dieser 15 Websites Opera verwendeten. Auch in Afrika ist Opera überaus beliebt und – über alle Plattformen betrachtet – in den meisten Ländern der dominierende Browser.\nAngaben über die Verbreitung waren zu jener Zeit wegen der Voreinstellung eines Features von Opera sehr ungenau, der Browser gab sich längere Zeit als Internet Explorer aus und konnte praktisch nicht richtig erkannt werden. Inzwischen ist die Vorgabe-Einstellung wieder auf „als Opera ausgeben“ eingestellt und der Nutzer hat die Wahl, sich entweder als Internet Explorer oder Firefox auszugeben oder zu maskieren. Während beim „maskieren“ die originale Kennung beziehungsweise User Agent des ausgewählten Browsers verwendet wird, fügt der Browser im „ausgeben“-Modus zusätzlich zum imitierten User Agent auch die eigene Kennung hinzu.\n\nLaut Opera ist der Opera Mini der \"„weltweit beliebteste Browser mit über 30 Millionen Benutzern“\" Diese Zahlen können allerdings nicht verifiziert werden. Aufgrund der geringen Anzahl von kostenlosen und frei zugänglichen Webbrowsern für Mobiltelefone und der guten Noten für Opera Mini erscheint zumindest eine weite Verbreitung plausibel. Mittlerweile wird Opera Mini auf einigen Mobiltelefonen (zum Beispiel Samsung) vorinstalliert.\n\nOpera Mobile wurden nach Angaben von Opera seit 2004 auf über 125 Millionen Geräten installiert. \"StatCounter\" gibt Operas Nutzeranteil auf Mobiltelefonen für 2018 mit 4,26 % an.\n\nDurch die Partnerschaft mit Nintendo hat Opera ein Monopol für Browser in den aktuellen Nintendo-Spielekonsolen. Lediglich auf dem Nintendo 3DS kommt der NetFront-Browser von Access zum Einsatz.\n\nNintendo DS & DSlite:\nDer Browser musste mangels integriertem Speicher als Hardware (wie ein gewöhnliches Spiel) käuflich erworben werden. Offizielle Verkaufszahlen wurden nicht veröffentlicht. Die Unterstützung für Nintendo DS/DS Lite wurde mittlerweile eingestellt.\n\nNintendo DSi & Nintendo DSi XL:\nDa der Opera-Browser in jedem Nintendo-DSi-Modell vorinstalliert ist, hat er damit eine Verbreitung von über 1,7 Millionen Installationen erfahren (Stand: Juli 2009).\n\nNintendo Wii:\nDie Nintendo-Wii-Konsole wurde, Stand September 2016, weltweit über 100 Millionen Mal verkauft. Der Opera-Browser (genannt „Internet-Kanal“) ist bei Auslieferung nicht vorinstalliert, kann aber im Wii-Shop-Kanal heruntergeladen werden. Der Download war zunächst kostenlos, wurde aber Mitte 2007 kostenpflichtig. Seit September 2009 ist der Browser wieder kostenfrei erhältlich. Downloadzahlen wurden bislang nicht veröffentlicht.\n\n\n"}
{"id": "14949", "url": "https://de.wikipedia.org/wiki?curid=14949", "title": "RGB-Farbraum", "text": "RGB-Farbraum\n\nEin RGB-Farbraum ist ein additiver Farbraum, der Farbwahrnehmungen durch das additive Mischen dreier Grundfarben (\"Rot,\" \"Grün\" und \"Blau\") nachbildet. Das Farbsehen des Menschen ist von drei Zapfentypen geprägt. Dieser Farbraum basiert im Prinzip auf der Dreifarbentheorie.\n\nNach ersten Untersuchungen und Überlegungen zum Phänomen „Farbsehen“ im 18. Jahrhundert führten vor allem wissenschaftliche Untersuchungen im 19. Jahrhundert zu ersten quantitativen Theorien. Eine davon ist die Dreifarbentheorie. Danach können fast alle Farbreize durch das Mischen dreier Primärfarben nachgebildet werden. Das aus den drei Primärfarbreizen zusammengesetzte Lichtspektrum kann sich vom Spektrum des ursprünglichen Reizes stark unterscheiden, ohne dass das menschliche Auge einen Unterschied wahrnimmt: die beiden Farbreize sind metamer. Können beide Farbreize nicht unterschieden werden, so ist es nicht nötig die genaue spektrale Verteilung für eine Rekonstruktion der Farbtöne zu speichern. Um diesen Farbreiz nachzubilden, reicht es aus, ein Zahlentripel zu speichern, das die Menge an rotem, grünem und blauem Licht beschreibt. Genau so wird eine Farbe im RGB-Raum beschrieben. Ist ein Rot, ein Grün und ein Blau in maximaler Intensität definiert, so können der Rotanteil R, der Grünanteil G und der Blauanteil B die Farbe beschreiben: Farbe = \"(R, G, B)\"\n\nDie Wertebereiche für die Farbreize (R, G, B) können unterschiedlich festgelegt sein. Die klassische Darstellung lässt Werte zwischen 0 und 1 (d. h. 0 Prozent und 100 Prozent) zu. Dies orientiert sich an der praktischen klassischen Realisierung mittels Dämpfung vorhandenen Lichts. Computerorientierte Anwendungen verwenden häufig die an der klassischen Form der Abspeicherung angelehnte Schreibweise, es werden Ganzzahlen zwischen 0 und einer Maximalzahl wie 255 abgespeichert.\n\nDa die Intensitätswahrnehmung des Menschen nach der Weber-Fechner-Regel nichtlinear ist, wird meist eine nichtlineare Kodierung für die Luminanz vorgenommen. Diese wird häufig als Gamma-Funktion bezeichnet, da die ersten Implementierungen die Potenzfunktion \"Y ~ L\" als Ansatz nutzten. Der Koeffizient Gamma mit \"γ > 1\" beschreibt die Krümmung der Kurve. Die inverse Funktion ist \"L ~ Y \".\n\nDas Koordinatensystem hat neben dieser nichtlinearen Kodierung insgesamt 9 Freiheitsgrade, die für einen konkreten RGB-Raum festzulegen sind. Die Angaben können unterschiedlich erfolgen, was beim Anwender zu Verwirrungen führen kann. Für alle drei Primärvalenzen gibt es verschiedene Möglichkeiten\n\n\nModerne computerorientierte Applikationen und Schnittstellen verwenden zumindest intern immer mehr Gleitkommazahlen, die sowohl aus dem Intervall [0,1] ausbrechen als auch größere Wertebereiche mit gleicher relativer Genauigkeit von Haus aus darstellen können (16 Bit ≈ 12 Größenordnungen, 32 Bit ≈ 83 Größenordnungen). So entfällt die Festlegung einer Maximalhelligkeit, die absoluten Helligkeiten werden abgespeichert. Die Anzahl der Freiheitsgrade reduziert sich auf 6, der Farbwürfel wird zu einem Vektorraum.\n\nDas farbige Feld des XYZ-Raumes steht für die Menge aller sichtbaren Farben. Das CIE-Normvalenzsystem wird anschaulich durch den Farbkörper nach Rösch wiedergegeben. Über ICC-Profile werden für die Farbeingabe- und Farbausgabegeräte, wie Monitor, Scanner, Drucker, die jeweils notwendigen Farbräume (RGB, CMYK) transformiert. Diese Transformation ist aber nicht eindeutig möglich. Der materiell jeweils realisierbare RGB-Farbraum liegt auf der Farbarttafel, genauer im CIE-Farbraum innerhalb eines Dreiecks. Ein solches Dreieck ist in der nebenstehenden Darstellung schwarz umrandet. Durch unterschiedliche Umformungen (meist als 3×3-Matrix) der Zahlenwerte und mittlerweile bessere technische Verfügbarkeit gibt es unterschiedlich definierte und normierte Varianten (s-RGB, Adobe-RGB, Bruce-RGB).\nDer RGB-Farbraum wird für selbstleuchtende (farbdarstellende) Systeme benutzt, die dem Prinzip der Additiven Farbmischung unterliegen, daher auch als \"Lichtmischung\" bezeichnet. Nach Graßmanns Gesetzen lassen sich Farben durch drei Angaben definieren, im RGB-Farbraum sind dies der Rot-, der Grün- und der Blauanteil. Die konkrete Form des Farbraums hängt vom jeweils konkreten technischen System ab, für das der jeweilige Farbraum bestimmt wurde, unter anderem von den konkreten Wellenlängen der Grundfarben.\n\n\"sRGB\" (Standard-RGB) wurde für Monitore entwickelt, deren farbgebende Basis drei Phosphore (Leuchtstoffe) sind. Solch ein Stoff gibt beim Auftreffen von Elektronen ein Spektrum von Licht ab, dabei sind geeignete Leuchtstoffe solche mit schmalbandigen Emissionen bei Wellenlängen im Bereich der Wahrnehmungsqualitäten Blau, Grün, Rot. Der Betrachter bekommt den im RGB-Farbraum definierten Farbeindruck (bei genügendem Abstand vom Bildschirm gehen die Pixel additiv ineinander über). Die Intensität des Anregungsstrahls entspricht dem Tripel im RGB-Farbraum und kann beispielsweise als Dezimalbruch (0 bis 1 oder 0 bis 100 %) oder diskret mit 8 Bit pro Kanal (0…255) angegeben werden (8-Bit-TIFF). Je nach Anwendungsart sind dabei bestimmte Wertdarstellungen bevorzugt.\n\nMit größeren Speichermedien wurden Tonstufen von 16 Bit pro Kanal möglich. So sind dreimal von 0 bis 65535 (formula_1) möglich, also insgesamt 281 Billionen Farben, beispielsweise beim 16-Bit-TIFF und 16-Bit-PNG. Gute technische Ausgabesysteme können mehr Farben wiedergeben als der Mensch unterscheiden kann, selbst der trainierte Mensch kommt nur auf etwa 500 000 Farbnuancen. Für spezielle Anwendungen sind 16-Bit-Werte allerdings durchaus sinnvoll. Bei Auswertungen in der Röntgendiagnostik sind so exaktere Betrachtungen möglich.\n\nDie Farbwiedergabe in Fällen wie Farbbilder vom PC-Drucker, Farbfotos auf Silberhalogenidbasis, der Druck einer Illustrierten, Farbbilder in Büchern geschieht durch Remission auf der präsentierenden Fläche. Hier gelten somit die Gesetze der subtraktiven Farbmischung, für die der CMY-Farbraum entwickelt wurde, wegen der Farbtiefe üblicherweise mit Schwarz für Farbtiefe als CMYK-Farbraum.\n\nDie Darstellung des RGB-Farbraumes erfolgt (weniger anschaulich als bei anderen Farbräumen) im kartesischen Koordinatensystem als Würfel. Die Abbildung zeigt links den Blick auf die Rückwand, in der Mitte den Aufblick, rechts einen Einblick ins Innere. Rot-, Grün- und Blau-Anteile folgen den Achsen; in den Ecken sind Gelb, Magenta, Cyan zu finden. Am Koordinatenursprung mit R=G=B=0 befindet sich Schwarz, entlang der Raumdiagonalen Grau bis zum Eckpunkt in Weiß.\n\nRGB-Farbräume als additive Farbräume dienen als Grundlage zur Darstellung von Farbbildern mittels Bildwiedergabegeräten, die Farben aus drei oder mehreren Farben additiv zusammenstellen. Neben CRT- und TFT-Displays sind dies auch Videoprojektoren. Dabei ist es unerheblich, wie die einzelnen Farbkanäle angesteuert werden, ob durch ein analoges oder ein digitales Signal mit 5, 8, 10 oder 16 Bit pro Farbkanal.\n\nÜblicherweise werden die drei Grundfarben Rot, Grün und Blau zur Darstellung genutzt. Zur Vergrößerung des Gamuts oder der Maximalhelligkeit können mehr „Farben“ zum Einsatz kommen. So können durch das Vieleck abgedeckte Farben besser dargestellt werden, zumindest bei geringeren Helligkeiten. Die Beschränkung auf das vom Hufeisen umschlossene RGB-Dreieck entfällt. Zur Vergrößerung der Maximalhelligkeit kann Weiß als weitere Grundfarbe genutzt werden. So sind größere Helligkeiten darstellbar, jedoch unter weiterem Verlust von Gamut. Beide Möglichkeiten werden bei DLP-Projektoren benutzt.\n\nAllerdings ist in diesen Fällen eine weitere Verarbeitung der RGB-Daten der Grafikkarte durch das Ausgabegerät notwendig. Im Fall der Mehrfarbprojektion ist ein geeigneter Arbeitsfarbraum der Grafikkarte notwendig, um die Vorteile nutzen zu können.\n\nDie Eckpunkte des RGB-Farbartdreiecks können willkürlich gewählt sein, sie sind \"nicht\" durch die Verfügbarkeit von Leuchtstoff-Kristallen beschränkt. Es besteht kein untrennbarer Zusammenhang zu den drei (Grund-)Lichtfarben, die die Leuchtstoffe des Ausgabegerätes erzeugen können. Farbwerte außerhalb des durch die Eckpunkte bestimmten Dreiecks können nicht dargestellt werden. So fehlen bei einer Bildröhre viele der kräftigen, satten Grün- und Blautöne, die in der Natur vorkommen, auch das spektralreine Rot und Violett fehlen im RGB-Raum.\n\nWerden die Leuchtstoffe eines Bildschirms durch LED oder ähnliche Elemente für Rot, Grün, Blau ersetzt, ändert sich an der farblichen Wirkung gegenüber dieser Beschreibung nichts vorausgesetzt, sie können den verwendeten RGB-Raum abdecken. Beispielsweise besitzen Flachbildschirme keine Bildröhre und erzeugen die Farben durch elektrische Feldanregung. Andere Leuchtstoffe bedingen eine andere Lage des RGB-Dreiecks (dargestellt auf der xy-Farbsohle). Technische Anforderung ist es, die Lage der Diagramm-Eckpunkte für LC-Displays möglichst an die Lage in Bildröhren anzupassen. Gelingt dies nicht, muss eine mathematische Umrechnung erfolgen, wodurch jedoch Farben wegfallen können, da die Koordinaten keine negativen Werte haben können. Unterbleibt die Umrechnung, werden die Farben verzerrt dargestellt. So werden möglicherweise Farbnuancen zwischen Rot und (Gelborange) auf verschiedenen Geräten merklich unterschiedlich dargestellt.\n\nObwohl es dem ersten Anschein nach so aussieht, als unterläge die Bildaufnahme den gleichen Gesetzmäßigkeiten wie die Bildwiedergabe, so gibt es für die Bildaufnahme grundlegende Unterschiede zur Bildwiedergabe:\n\nIm Prinzip gibt es unendlich viele Farbräume, die durch Definition der Primärvalenzen, des Weißpunkts und der Gradationkurve (Gamma) festgelegt werden (genau das erfolgt in Matrix-ICC-Profilen). Die Primärvalenzen legen das Farbdreieck der bei geringen Helligkeiten darstellbaren Farben fest, der Weißpunkt das Intensitätsverhältnis für Farbtripel mit drei identischen Komponenten, damit indirekt das Verhältnis von maximalem Rot zu maximalem Grün und Blau.\n\nDie folgende Aufstellung gibt einen Überblick über die Historie der üblichen RGB-Farbräume.\n\n\nDieser XYZ-Farbraum aus dem Jahr 1931 ist der erste Normierungsversuch, weltweit ein einheitliches Darstellungssystem zu finden. Der Ausgangspunkt dafür waren die experimentell ermittelten Zapfenempfindlichkeiten. Die angewandte Messtechnik und die Versuchsauswertung entspricht dem Stand der Technik der 1920er Jahre. Dennoch wird der Farbraum noch häufig in der Praxis eingesetzt. Die Farbmessung zu dieser Zeit nutzte den „Trick“, dass bei Lichtfarben durch Zumischen von Licht auf der „Istseite“ sozusagen negative Farbreize auf der „Sollseite“ erzeugen werden können. Für den XYZ-Farbraum bestand der Anspruch, dass er alle vom Menschen wahrnehmbaren Farben umfasst. Zwar ist der XYZ-Farbraum vorrangig ein Messfarbraum, aber er kann zur Darstellung von Farben genutzt werden.\n\nDa letzterer das gesamte „Hufeisen“ aller Farbarten umschließt, werden durch ihn alle existierenden Farben erfasst. Das Hauptproblem besteht in seiner Ungleichmäßigkeit. Im Grün sind die als gleich empfundenen Farbabstände größer als im Rot und im Blau. Die Primärvalenzen sind so gewählt, dass die Farbkoordinaten einfach darzustellen sind. Es sind deshalb keine real existierenden Farben. Es gibt also keine wirklichen Farbkörper in RGB, die diesen Farbraum wiedergeben könnten.\n\nDer \"reelle\" CIE-RGB-Farbraum entsteht durch die Umrechnung des \"virtuellen\" CIE-XYZ-Farbraums (der auf nicht darstellbaren Farbreizen beruht) auf die Eichreize von gut darstellbaren Spektrallinien:\n\n\nDamit wurde eine fast perfekte Abdeckung von Rot, Orange, Gelb und im Blau- und Violett-Bereich erreicht. Deutliche Schwächen liegen allerdings im Türkis- und Grünbereich durch die ungünstige Wahl des Grünreizes. Insbesondere sind nicht alle CMYK-Farben darstellbar, wiederum vor allem im Grün- bis Türkisbereich (480 nm bis 510 nm).\n\nBei Einführung des NTSC-Farbfernsehens im Jahr 1953 wurden als Primärvalenzen die (damals) verwendeten Farbphosphore verwendet:\n\n\nDie Primärvalenzen ergeben sich aus den Emissionsspektren der verwendeten Phosphore. Der klassische NTSC-Farbraum wurde 1979 von der ATC (Vorgänger der ATSC) durch einen dem EBU-Farbraum ähnlicheren SMPTE-C-Farbraum ersetzt.\n\nParallel zur Normierung der Farbdarstellung für Computermonitore mit sRGB wurden die Farbfernsehnormen überarbeitet und angepasst. Da grundsätzlich für beide technische Systeme die gleichen elektronisch angeregten Ausgangssubstanzen zur Verfügung stehen, sind die Möglichkeiten zur Darstellung von Farben fast gleich. Wie beim sRGB-Farbraum wurden besonders die Farbwiedergaben im Grün gegenüber einer besseren Rot- und Blaudarstellung zurückgestellt. Es kam zu parallelen Normungen, sodass neben dem EBU/ITU-R-Farbraum ein geringfügig abweichender SMPTE-C-Farbraum besteht. Mit der Einführung von HDTV setzt sich (wohl) der sRGB-Farbraum für Fernsehanwendungen durch.\n\nDer sRGB-Raum entstand im Jahr 1996 durch eine Kooperation von Hewlett-Packard und der Microsoft Corporation.\n\nBei direkter Darstellung der gespeicherten Farbtripel sollte es ohne Farbmanagement möglich sein, eine gute Farbwiedergabe zu erzielen. Die Zielgröße war ein direkter Zusammenhang zwischen Anregung und wiedergegebener Farbe. Der sRGB ist in CCIR Rec 701 (XA/11) beschrieben.\n\nDieses Farbmodell orientierte sich an den verfügbaren Leuchtstoffen und besitzt Schwächen bei der Darstellung gesättigter Rot-, Grün- und Blau-Töne. Es sind nicht alle mittels CMYK-Siebenfarbendruck druckbaren Farben darstellbar. Vor allem im Grün- bis Türkisbereich (480 nm bis 510 nm) gibt es größere Defizite, die durch den folgenden Farbraum größtenteils behoben wurden.\n\nDurch Adobe wurden 1998 Überlegungen umgesetzt, durch die es möglich werden sollte, alle beim Druck relevanten Farben des CMYK-Farbmodells im neuen Adobe-RGB-(1998)-Gamut darzustellen.\n\nGegenüber sRGB ergeben sich deutliche Verbesserungen bei den Türkis- und Grün-Tönen. Allerdings wurden die Primärvalenzen so gelegt, dass die Darstellung von gesättigten Rottönen sich kaum verbessert hat, die von gesättigten Blautönen sogar leicht verschlechtert sind. Auf die Darstellung der häufiger auftretenden weniger gesättigten Töne wirkte sich die Umstellung aber nicht aus.\n\nDer Kompromiss bestand in einem Ausgleich in den in der Praxis häufigsten Farbwiedergaben. Bei der Wiedergabe realer Bilder treten die hochgesättigten Farben seltener auf als die weniger gesättigten. Die Bildqualität bei der überwiegenden Anzahl von Farbwiedergaben ist hinreichend gut. So konnten beinahe alle Farben des CMYK-Siebenfarbendrucks im RGB-Farbraum reproduziert werden.\n\nDer Adobe-RGB war eine Weiterentwicklung, entspricht aber noch nicht den gestiegenen Anforderungen der Praxis. So ließen sich etwa Firmenfarben in der Werbung im Workflow nicht durchgehend von einer Geräteart zur anderen weitergeben. Deshalb wurde der sogenannte Wide Gamut entwickelt, wiederum unter Federführung von Adobe.\n\nDer Wide-Gamut-RGB arbeitet mit den Primärfarben 700 nm, 525 nm und 450 nm, und höheren Farbsättigungen an der technischen Machbarkeitsgrenze. Somit wird eine perfekte Abdeckung von Rot, eine fast perfekte Abdeckung von Violett und Blau und eine sehr gute Abdeckung von Grün-Tönen erreicht. Leichte Fehler im Bereich der extrem gesättigten Farben im Türkis und Grün zwischen 470 nm und 520 nm werden zugunsten der Anforderungen des praktischen Farbmanagements in Kauf genommen.\n\nAlle mittels CMYK-7-Farbendruck druckbaren Farben sind im Adobe-Wide-Gamut-Farbraum darstellbar.\n\nDie \"European Color Initiative\" (ECI) wurde im Juni 1996 auf Initiative der Verlagshäuser Bauer, Burda, Gruner+Jahr und Springer gegründet. Sie beschäftigt sich mit der medienneutralen Verarbeitung von Farbdaten in digitalen Publikationssystemen. Dabei soll in allen eingesetzten Ein- und Ausgabemedien ein durchgehendes Farbmanagement möglich werden. Die Entwicklung der Druckmedien am Computer verlangt, dass das Druckergebnis dem Entwurf entspricht.\nEs wurde im Jahr 2002 die Version 1 entwickelt. Im Gegensatz zu Version 1 wurde für Version 2 das Gamma 1.8 durch eine L*-Charakterisierung abgelöst. Dadurch ergibt sich eine optimierte Kodierungseffizienz, insbesondere bei nur 8-bittigen Daten in den Tiefen. Die aktuelle Version 2 ist in ISO 22028-2:2007 definiert.\nHierzu liegen allerdings keine öffentlich zugänglichen Werte vor.\nDer ProPhoto-RGB-Farbraum (auch bekannt als ROMM-Farbraum, von englisch: \"Reference Output Medium Metric\") ist eine andere Weiterentwicklung des Wide Gamut, wobei vor allem die Anforderungen der Digitalfotografie beachtet wurden, insbesondere zur anschließenden Weiterverarbeitung. Dafür wurden neue Überlegungen, Forschungsergebnisse (wie der LMS-Farbraum) und praktische Forderungen herangezogen. Er bringt eine sehr gute Abdeckung \"fast aller\" wahrnehmbaren Farben. Ähnlich wie Wide-Gamut-RGB sind nur wenige sehr gesättigte Farben im Bereich von Türkisgrün und im Bereich des Violetts nicht darstellbar.\n\nDie festgelegten Primärfarben für Blau und Grün sind allerdings wiederum keine real existierenden Farben.\n\nDer SMPTE ST2084:2014-Farbraum ist ein HDR-Farbraum, entwickelt von Dolby Labs. Es nutzt als Primärvalenzen Licht der Wellenlängen 467 nm, 532 nm und 630 nm gemäß ITU-R-Empfehlung BT.2020/BT.2100.\n\nDer Hybrid Log Gamma-Farbraum (HLG) ist ein HDR-Farbraum. Es nutzt die Primärvalenzen von BT.2020/BT.2100. Entsprechende Dateien können mit der Dateinamenerweiterung \"HSP\" gespeichert werden.\n\nDer RGB-Farbraum ist eine abstrahierte Darstellung für (Licht-)Farben. Durch geeignete Transformationen lassen sich alle Farbräume ineinander überführen. Bei einigen Transformationen werden Bereiche der umfassenderen Farbräume allerdings auf den Rand des begrenzteren Farbsystems abgebildet, und die Transformation ist nicht immer umkehrbar. Der RGB-Farbraum lässt sich auf das Farbrhomboeder abbilden, aber diese Abbildung ist nicht umkehrbar.\n\nWerden RGB-Farben durch Gleitkommazahlen beschrieben, so kann auf notwendige nichtlineare Verzerrungen für Bilder und Bildkonvertierungen verzichtet werden, die Farbraumkonvertierungen werden zum größten Teil überflüssig. Moderne Programmierschnittstellen rechnen mit linearen Beziehungen im sRGB-Raum, so dass mit Unterstützung von Gleitkomma kein Gamut-Clipping nötig ist.\n\nJedes der oben genannten Farbmodelle kann um einen oder drei Alphakanäle für Transparenzen erweitert werden.\n\nBei der Erweiterung von einem Alphakanal wird davon ausgegangen, dass (teil-)transparente Medien alle drei Spektralfarben gleichmäßig durch ihre eigene Farbe ersetzen oder dämpfen. Mit diesem einfachen und allgemein üblichen Modell lässt sich allerdings gefärbtes Glas nicht darstellen. Es gibt zwei Farbmodelle, die den Alpha-Kanal im Vordergrund entweder mit berücksichtigen (\"straight\") oder nicht berücksichtigen (\"pre-multiplied\").\n\nModelle mit einem Alphakanal (straight):\n\nModelle mit einem Alphakanal (pre-multiplied):\nModelle mit drei Alphakanälen (straight):\nModelle mit drei Alphakanälen (pre-multiplied):\n\nDas \"RGBA-Farbmodell\" ist im eigentlichen Sinn kein Farbmodell, sondern eine Erweiterung des RGB-Modells durch den (vierten) Alphakanal. Diese α-Komponente bestimmt die Transparenz eines Pixels, die für Überblendeffekte eine Rolle spielt. Wird ein Bild mit einem neuen Bild überschrieben, fließen die Informationen des vorhergehenden Urbildes mit in das neue Zielbild ein. Die Alphakomponente bestimmt, wie durchsichtig das entsprechende Pixel des Bildes sein soll. α = 0 steht für völlige Transparenz, α = 1 für völlige Lichtundurchlässigkeit.\n\nZur Umrechnung zwischen zwei beliebigen RGB-Farbräumen sind folgende Operationen auszuführen:\n\n\nFür die Umrechnung von R-, G-, B-Koordinaten in X-, Y- und Z-Werte der CIE gelten für jeden konkreten RGB-Farbraum spezielle Abbildungsmatrizen. Dabei sind X ein virtuelles Rot, Y ein virtuelles Grün und Z ein virtuelles Blau. Für einen dieser Farbräume (hier sRGB und Lichtart D65) gilt folgende Abbildung:\n\nund für die Rückrechnung die inverse Matrix\n\nFolgende Beziehungen zwischen sRGB- und XYZ-Farbraum lassen sich daraus herleiten:\n\nFür unterschiedliche Geräteklassen wurden abweichende RGB-Räume genormt, die alle den gleichen Grundaufbau mit Rot-, Grün- und Blaukomponente besitzen. Entsprechend wurden die Umrechnungsmatrizen vom besonderen RGB-Raum und der gewählten Lichtart beeinflusst.\n\nEigentlich hat jedes Gerät seinen eigenen Geräte-RGB-Farbraum, der aber üblicherweise zulässig innerhalb des genormten Farbraumes liegt. Individuelle Farbdifferenzen können durch Gerätetyp, Hersteller, Verarbeitungs- und Produktionseinflüsse, sowie durch Alterung entstehen. Hierfür gibt es (in bestimmten Grenzen) Möglichkeiten der Anpassung. Diese Methoden werden als Colormanagement zusammengefasst. Eine Mindestanpassung ist die Gammakorrektur. Soweit sich die Geräteparameter nachstellen lassen ist eine Anpassung des Gerätes an die genormten Größen möglich. Für höherwertige Anforderungen wird das Gerät individuell ausgemessen und über 3×3-Matrizen oder spezielle Listen (englisch: \"Look-up-Table\", LUT) die Zuordnung von Geräte-RGB-Tripel mit dem Forderungstripel verknüpft.\n\nFür digitale Bilddaten eignet sich der RGB-Farbraum lediglich zur Darstellung am Bildschirm und den verwandten Gerätetypen. Farbdefinitionen sowie Kontraste der Farben untereinander im Internet zur Darstellung auf einer Vielzahl unterschiedlichster Monitore mit einem breiten Spektrum verwendeter Grafikkarten sind Web-sicher, wenn sie den Empfehlungen des W3C entsprechen. Bilddaten für den Druck (Offsetdruck, Siebdruck, Digitaldruck) sind in einem subtraktiven Farbmodell zu reproduzieren (CMY, CMYK). Die Umrechnung von RGB in CMY ist dabei ein Wissensgebiet, das durchaus noch in der Entwicklung ist (verwiesen sei hierfür auf die ICC-Profile).\n\nGrenzen in der Anwendung findet der RGB-Farbraum mit wahrnehmungsphysiologischen Problemen.\n\n\nEs existieren zwei technische Angaben, die für eine exakte Wiedergabe eines Farbtones erforderlich sind. Zum einen die Lage der Grundfarben (Rot, Grün, Blau) bei voller Anregung aller Kanäle, also die „Mitte“ des xy-Farbartdiagrammes, bei x = y = 1/3 oder den Werten R = G = B = 1. Diese Farbe wird als Referenzweiß bezeichnet. Zum anderen ist es die Beziehung zwischen der Spannung der Anregungsstrahlung (etwa Kathodenstrahlung) zum Farbergebnis und der abgegebenen Lichtleistung (angenähert durch Gamma, genaue Angabe durch eine Funktion abhängig von der angelegten Spannung). Der logarithmische Zusammenhang zwischen Farbvalenz und Farbreiz, wie er von Ewald Hering bestimmt wurde, geht in diese Formel ein.\n\nEs ist also für eine gute Farbdarstellung wichtig zu wissen, welche RGB-Norm eingesetzt wurde.\n\nDie beiden ersten technischen Angaben sind in Normen für alle Hersteller festgelegt. Allerdings sind die Normungen der RGB-Farbräume in verschiedenen Gremien in Amerika (FCC, ATSC), Europa (EBU) und Japan unterschiedlich.\n\nEin RGB-Farbraum ist ein auf wenige, definierte Parameter begrenzter Ausschnitt der Wirklichkeit. Die Wahrnehmung eines „bunten“ Lichtes, einer „Oberfläche“, umfasst weitere Effekte. So könnte die Definition einer Farbe durch drei Zahlen die falsche Erwartung wecken, eine Farbe wäre in ihrer Wahrnehmung absolut bestimmt. Tatsächlich ist die Farbwirkung einer numerisch bestimmten RGB-Farbe dagegen vom konkret vorhandenen technischen System abhängig, das diese Farbe wiedergibt oder aufnimmt, und von den internen und externen Umgebungsbedingungen.\n\n\"Ein Beispiel:\"\n\nSind der genaue Farbraum des Aufnahmesystems und der Farbraum des Wiedergabesystems bekannt und bleiben sie konstant, kann durch eine Umrechnung des Farbraumes eine dem Original weitgehend angenäherte Darstellung erreicht werden. Probleme bereiten Displays, die eine variierende, wie richtungs- oder temperaturabhängige Farbdarstellung aufweisen.\n\nUm vorhersagbare Farben in RGB-Systemen zu erhalten, sind Farbkorrekturmethoden nötig. Es finden Profile Verwendung, die beschreiben, wie Farben aussehen und damit den Farbraum für verschiedene Geräte umrechenbar machen. Typische Farbprofile, Betriebs-RGB-Räume, sind sRGB (standard RGB) oder Adobe-RGB für allgemeine Computerperipherie wie Monitore und Digitalkameras und ECI-RGB für den Einsatz im grafischen Gewerbe, zum Beispiel in der professionellen Bildbearbeitung. Ein angestrebtes Ziel ist der Wide-Gamut-RGB, der einen maximal erreichbaren Farbumfang definiert, der zu seiner Darstellung noch der Lösung harrt. Für Transformation innerhalb des RGB-Farbraumes, also zwischen Betriebs-RGB-Räumen oder zwischen Geräte-RGB-Räumen werden 3×3-Matrizen genutzt. Eine andere Möglichkeit sind LUT (Look-up-Tables) die in Listenform Wertezuordnungen (Transformationstabellen) von (R,G,B) auf (R,G,B) enthalten. Zwischen den Stützstellen kann linear interpoliert werden. ICC-Profile sind solche (standardisierten) Hilfsmittel.\n\n\n"}
{"id": "15460", "url": "https://de.wikipedia.org/wiki?curid=15460", "title": "Microsoft Windows 3.0", "text": "Microsoft Windows 3.0\n\nMicrosoft Windows 3.0 ist die dritte Version des von der Firma Microsoft entwickelten damaligen DOS-Betriebssystemaufsatzes Microsoft Windows. Sie wurde am 22. Mai 1990 veröffentlicht.\n\nIm Sommer 1988 erhielt der Programmierer Murray Sargent von Microsoft die Aufgabe, einen mit dem 80286 kompatiblen DOS-Extender zu schreiben. Dieser sollte dazu dienen, Microsofts Debuggingprogramm \"CodeView\" außerhalb des konventionellen Speichers und damit frei von Einflüssen durch andere DOS-Anwendungen benutzen zu können. Sargent schrieb die dazu notwendigen Routinen zunächst in einem von ihm selbst entwickelten Debugger, um sie bei Fertigstellung in ein eigenes Programm auszulagern.\n\nEnde Juni traf sich Murray mit dem Windows-Entwickler David Weise. Auf die Anmerkung, dass das damals neu erschienene Windows /286 mit der High Memory Area lediglich 64 kB mehr als normale DOS-Anwendungen nutzen könne, beschlossen beide, Windows im Schutzmodus laufen zu lassen, um ähnlich wie bei DOS-Extendern nicht länger auf den konventionellen Speicher beschränkt zu sein. Da Microsofts Augenmerk zu dieser Zeit auf OS/2 lag, hielten die beiden Entwickler das Projekt über einen Zeitraum von einem Monat geheim. Als die ersten Verwerfungen zwischen Microsoft und IBM bezüglich OS/2 entstanden, nutzte Weise die Chance und zeigte Steve Ballmer seine Fortschritte beim Windows-Projekt. Ballmer zeigte sich überzeugt und gab daraufhin die Entwicklung dieser Windows-Version offiziell in Auftrag.\n\nDie ersten Informationen zu Windows 3.0 wurden Anfang 1989 bekannt. Das Speichermanagement, welches in früheren Versionen von Windows als ineffizient galt, sollte in der neuen Version stark verbessert werden. Die Benutzeroberfläche sollte verändert und dem Presentation Manager in OS/2 1.x angeglichen werden. Im Mai 1989 wurde eine erste Testversion an über 900 Hardwarehändler ausgeliefert.\n\nDie grafische Benutzeroberfläche wurde in Windows 3.0 komplett überarbeitet und an den Presentation Manager angepasst. Das MS-DOS-Fenster, das in früheren Versionen von Windows nach dem Start erschien, wurde durch zwei Programme ersetzt. Der Programm-Manager erscheint direkt nach dem Start und enthält Verweise auf Programme, die auf dem Computer installiert sind, in Form von Icons. Diese Programme sind in sogenannten Programmgruppen gruppiert. Der Datei-Manager dient dem Verwalten von Dateien und Ordnern auf den Laufwerken des Computers. Dazu zeigt er in einem Fenster den Verzeichnisbaum des Laufwerks. Durch einen Doppelklick auf ein einzelnes Verzeichnis öffnet sich ein neues Fenster, in dem alle Dateien innerhalb dieses Verzeichnisses aufgelistet werden. Durch Dateizuordnungen können Dateierweiterungen einem Programm zugeordnet werden, um diese Dateien direkt aus dem Datei-Manager heraus zu öffnen. Die Oberfläche selbst verwendet 3D-Schaltflächen, und Icons sind nicht mehr auf die Farben Schwarz und Weiß beschränkt, sondern können nun Farben beinhalten.\n\nDie Installation des Betriebssystems findet bereits unter der grafischen Benutzeroberfläche statt. Das Setup-Programm kann auch nach der Installation aufgerufen werden, um Einstellungen wie den Grafikmodus zu ändern, was in vorherigen Versionen noch ein komplettes Neuinstallieren des Betriebssystems erforderte. Während der Installation werden bereits installierte Anwendungen gesucht und automatisch Icons im Programm-Manager angelegt, sodass diese Programme direkt nach der Installation benutzt werden können.\n\nWar unter DOS ein Netzwerk installiert, wurde dies von Windows 3.0 während der Installation erkannt. Funktionen wie das Erstellen von Netzlaufwerken können von der grafischen Benutzeroberfläche durchgeführt werden, ohne zurück zum DOS-Prompt wechseln zu müssen. Windows 3.0 beinhaltet im Lieferumfang Unterstützung unter anderem für Novell NetWare, Banyan Vines, LAN Manager und 3Com 3+Share.\n\nWindows 3.0 änderte die Systemsteuerung komplett. Bei ihr handelt es sich nun um ein Fenster ähnlich dem Programm-Manager, in dem Icons erscheinen. Diese Icons rufen bestimmte Fenster auf, die es ermöglichen, die Einstellungen von Windows anzupassen. Die Konfigurationsmöglichkeiten wurden deutlich erweitert, so bietet das Betriebssystem nun die Möglichkeit, ein Hintergrundbild auf dem Desktop darzustellen.\n\nDas Zubehör wurde in Windows 3.0 erweitert. Das in früheren Versionen von Windows verwendete Zeichenprogramm Paint wurde durch \"Paintbrush\" ersetzt, welches erstmals Farbunterstützung bietet. Das neue Programm \"Rekorder\" ermöglicht das Aufzeichnen von Makros, welche aus Tastatur- und Mauskommandos bestehen. Diese können gespeichert und beliebig wiedergegeben werden. Mit Windows 3.0 hielt ein neues, Hypertext-basiertes Hilfesystem Einzug, in dem verschiedene Hilfethemen untereinander verlinkt sind. Lesezeichen können gesetzt werden, um Hilfeseiten später schneller wiederzufinden, und mit der eingebauten Suche kann nach bestimmten Begriffen im Hilfedokument gesucht werden. Der Rechner wurde um einen wissenschaftlichen Modus erweitert, der komplexere Berechnungen und die Umrechnung zwischen verschiedenen Zahlensystemen ermöglicht.\n\nWindows 3.0 wurde, im Unterschied zu seinem Vorgänger Windows 2.x, nicht in verschiedenen Versionen je nach Prozessortyp verkauft, sondern nur in einer einzigen Version für sämtliche Prozessortypen. Das Betriebssystem kann in drei verschiedenen Modi je nach Prozessortyp ausgeführt werden. Beim Start wird automatisch der für die jeweilige Computerkonfiguration optimale Modus ausgewählt, es ist jedoch möglich, auf der Befehlszeile mittels Parameter einen bestimmten Modus zu erzwingen.\n\nDer \"Real-Modus\" dient hauptsächlich der Kompatibilität mit älteren Windows-Anwendungen. Er unterstützt zudem alte 8086-Prozessoren und Rechner mit weniger als einem Megabyte Arbeitsspeicher. Der \"Standard-Modus\" nutzt den Protected Mode des 80286-Prozessors, um Arbeitsspeicher über 1 MB zu adressieren. Der \"Erweiterte Modus für 386-PCs\" benötigt einen 80386-Prozessor mit mindestens 2 MB Arbeitsspeicher. Ähnlich wie in Windows 2.x kann der \"virtual 8086 mode\" ausgenutzt werden, um MS-DOS-Programme unabhängig voneinander auszuführen. Windows 3.0 unterstützt auf einem 386-PC die Verwendung einer Auslagerungsdatei, um den nutzbaren Arbeitsspeicher zu erhöhen, indem die Festplatte als Zwischenspeicher genutzt wird. Zudem können Einstellungen zur Verteilung der Prozessorzeit in einem Menüpunkt \"386 erweitert\" der Systemsteuerung getätigt werden.\n\nWindows 3.0 beherrscht präemptives Multitasking nur sehr eingeschränkt. Es wird nur für MS-DOS-Programme unterstützt, und das auch nur dann, wenn es auf Systemen ausgeführt wird, die kompatibel zum 80386-Prozessor sind. Windows-Programme werden im kooperativen Multitasking ausgeführt, bei dem Programme explizit die Kontrolle an die anderen abtreten müssen.\n\nWindows 3.0 wurde am 22. Mai 1990 veröffentlicht und bis 1992 vertrieben. In den ersten vier Monaten wurden eine Million Exemplare zu einem Preis von je 150 USD abgesetzt. Es ist das erste Windows, das VGA unterstützt, aber weiterhin Treiber für sehr alte XT-Rechner und deren Grafikstandards mitliefert.\n\nWindows 3.00a wurde am 31. Oktober 1990 veröffentlicht. Diese Version diente einzig und allein der Behebung zahlreicher Programmfehler, ansonsten gibt es keine Änderungen zu Windows 3.0.\n\nEnde 1991 veröffentlicht, ist es das erste Windows überhaupt, das in der Lage ist, Soundkarten anzusprechen. Im Grunde handelt es sich jedoch nur um ein überarbeitetes Windows 3.0. Der Real-Mode und die Unterstützung für 8086-Prozessoren fielen weg. Hinzugekommen sind:\n\nDiese Windows-Version wurde ausschließlich auf CD-ROM vertrieben (als erstes Windows überhaupt) und war nicht weit verbreitet, da Multimedia-PCs (Rechner mit CD-ROM-Laufwerken und Soundkarte mit Midi-Schnittstelle) damals sehr teure Geräte waren. Die übrigen Hardware-Voraussetzungen mit mindestens 10 MHz 286er-Prozessor, 2 MB RAM und 30 MB Festplatte entsprechen dem damaligen Technikstand, dazu wird MS-DOS ab V3.1 (oder ein kompatibles Betriebssystem, wie das damals weit verbreitete DR-DOS) benötigt.\n\nDie Systemvoraussetzungen des Systems sind vom jeweiligen Modus abhängig. Die absolute Mindestvoraussetzung, um das Betriebssystem im Real-Modus zu starten, war ein Intel 8088/8086 und 384 kB freier Arbeitsspeicher. Der Standardmodus setzte einen 80286-Prozessor mit 192 kB freiem erweiterten Speicher voraus. Um Windows in erweiterten Modus für 386-PCs zu starten, war ein 80386 mit 1 MB freiem erweiterten Speicher notwendig. Dabei ist zu beachten, dass die Angaben zum Arbeitsspeicher gerundet sind; die eigentlichen Speichervoraussetzungen des Betriebssystems hängen von der jeweiligen Konfiguration ab und sind nicht fest definiert.\n\nWindows 3.0 wurde sowohl auf 5,25\"- als auch auf 3,5\"-Disketten ausgeliefert. Im ersten Fall befanden sich fünf 1,2-MB-Disketten im Lieferumfang, im zweiten Fall sieben 720-kB-Disketten.\n\nWindows 3.0 brachte für Microsoft den Durchbruch. Nach der Veröffentlichung gaben 72 Prozent aller Unternehmen bekannt, Windows 3.0 einsetzen zu wollen. Da zahlreiche Entwickler zwischenzeitlich Anwendungsprogramme für Windows bereitstellten, gab es zum Zeitpunkt der Veröffentlichung ausreichend Anreize, auf die grafische Benutzeroberfläche umzusteigen. In den ersten sechs Monaten wurden drei Millionen Kopien des Betriebssystems verkauft.\n\nEs gab aber auch Kritik. Bereits der Installationsprozess erhielt Kritik, da er schlecht durchdacht sei, die meisten Disketten müssten während der Installation mehrmals eingelegt werden. Zwar sei Windows 3.0 auch auf einem IBM XT lauffähig, ein sinnvoller Einsatz sei aber erst mit einem 286-Computer möglich.\n\n"}
{"id": "15472", "url": "https://de.wikipedia.org/wiki?curid=15472", "title": "Darwin (Betriebssystem)", "text": "Darwin (Betriebssystem)\n\nDarwin ist ein freies Unix-Betriebssystem des Unternehmens Apple und die Basis für die proprietären Betriebssysteme von Apple, die aus Mac OS X entstanden sind: macOS für Personal Computer der Marke Mac, iOS für die Mobilgeräte iPad, iPod und iPhone, tvOS für die Apple-TV-Set-Top-Box, und watchOS für die Apple Watch. Es wurde als Darwin 0.1 am 16. März 1999, gemeinsam mit Mac OS X Server 1.0, erstmals verfügbar gemacht.\n\nIm Januar 1997 wurde NeXT von Apple übernommen und damit auch das bis Version 3 noch NeXTStep genannte Betriebssystem OPENSTEP, das gerade in Version 4.0 veröffentlicht worden war. Dieses wurde unter dem Namen Rhapsody u. a. um die Macintosh-Oberfläche im Platinum-Design, wie es auch in Mac OS 8 verwendet wurde, sowie die Virtualisierungsumgebung Blue Box, unter der Mac OS 8.1 virtualisiert ausgeführt werden konnte, erweitert. NeXTStep, OPENSTEP und Rhapsody nutzen große Teile von BSD-Unix als Grundlage für ein auf mehreren Plattformen und Rechnerarchitekturen lauffähiges Betriebssystem – auch Rhapsody hätte auf mehreren Plattformen laufen sollen, doch stoppte Apple 1998 die Veröffentlichung des fertiggestellten Rhapsody für PowerPC-Macintosh- und x86-PC-Systeme, da eine Multi-Plattform-Strategie als am Markt gescheitert erkannt worden war. Auf der WWDC 1998 kündigte Apple die Verschmelzung von Mac OS (damals aktuell in Version 8, bis 1997 noch System 7) mit Rhapsody an, welches den Namen „Mac OS X“ tragen sollte. Laut Steve Jobs sollte Mac OS X bereits 1999 erscheinen. Da es jedoch 1999 noch nicht fertig war, wurde Rhapsody als reines Macintosh-Server-Betriebssystem unter dem Namen „Mac OS X Server 1.0“ veröffentlicht – sowie dessen quelloffener BSD-Kern als Darwin 0.1.\n\nAuf dieser Basis – Rhapsody und Darwin als dessen quelloffener Teil – wurde mit der Entwicklung von Mac OS X 10.0 begonnen und mit den Veröffentlichungen der Developer Previews und der Public Beta wurden ab 1999 auch einige Versionen von Darwin veröffentlicht, die auf einem Macintosh-Computer mit laufendem Mac OS installierbar waren. Doch nach der fertigen Version von Mac OS X 10.0 „Cheetah“ 2001 verlor Apple das Interesse an einer offiziellen Darwin-Distribution und stellt seither nur mehr den Quelltext für Darwin, der die Basis der jeweiligen Version des proprietären Betriebssystems bildet, zur Verfügung. Die Weiterentwicklung von Darwin ist in dieser Form eng mit der Entwicklung der Apple-Betriebssysteme verwoben.\n\nDa integrale Teile von macOS und iOS nicht quelloffen verfügbar sind, fehlt Darwin sowohl die grafische Benutzerschnittstelle Aqua, als auch Quartz, OpenGL, QuickTime sowie die Programmierschnittstellen Cocoa und Carbon, weshalb Programme für Mac OS X/​OS X/​macOS auch nicht lauffähig sind. Daher nutzen die wenigen verfügbaren Darwin-Distributionen freie (FreeBSD-kompatible) Desktop-Umgebungen und z. B. mittels MacPorts ist eine Vielzahl freier Software auch auf Darwin ohne großen Aufwand nutzbar.\n\nDie Quelltextbasis von Darwin geht auf 4.4BSD-Lite zurück, das nach dem Streit um Rechte am UNIX-Code, der 1994 endete, von den letzten noch bestehenden originalen System‑V-Quelltextzeilen gesäubert worden war. Es steht unter freier Lizenz, der APSL. Zugleich ist Darwin ein Nachfahre des von NeXT entwickelten Betriebssystems NeXTStep, das 1996 in OPENSTEP umbenannt wurde und noch auf 4.3BSD basierte. Apple kaufte NeXT gegen Ende 1996 auf und entwickelte das eingekaufte System daraufhin zu Rhapsody weiter. Dabei wurde das BSD-Grundsystem von 4.3BSD- auf 4.4BSD-Lite-Quelltext und der Kernel von Mach 2.5 auf Mach 3 portiert. Mit der Veröffentlichung der Mac OS X Developer Preview 1 wurde dieser Kernel erstmals unter dem Namen XNU bekannt und als Teil des Darwin-Quelltextes veröffentlicht. Mach 3 wird jedoch nicht vollständig umgesetzt, sondern um Teile des FreeBSD-Kernels zu einem Hybridkernel ergänzt, sodass er die Vorteile eines monolithischen Kernels mit den Vorteilen eines Microkernels verbindet. Dabei wurden Teile der Mach-Umsetzung aus MkLinux, bei der auch Apple beteiligt war, wiederverwendet.\n\nDarwin ist in verschiedenen Versionen auf PowerPC-Prozessoren, x86-Prozessoren und auf ARM-Prozessoren lauffähig. Während alle Programme für Darwin auch auf Mac OS X laufen, funktionieren auf Mac OS X/​OS X/​macOS zugeschnittene Programme nicht unbedingt auf Darwin. Die Mobilbetriebssysteme iOS für iPhone, iPad und iPod touch, tvOS für Apple TV und watchOS für die Apple Watch basieren ebenfalls auf Darwin, wobei der Nutzer jedoch weder Zugriff auf das Dateisystem noch auf die Kommandozeile hat. Diese Beschränkung lässt sich in einigen Fällen durch einen Jailbreak aufheben.\n\nDarwin wird unter der Apple Public Source License herausgegeben, die ab der Version 2.0 von der Free Software Foundation als Lizenz für freie Software anerkannt wird.\n\nDas Maskottchen von Darwin ist Hexley, das Schnabeltier.\n\nVon Apple selbst wird nur mehr der Quelltext der zur Erstellung eines Darwin-Betriebssystems nötigen Teile herausgegeben. Aufgrund des von Apple genutzten internen Build-Systems, das nicht öffentlich verfügbar ist, sind die Quellen jedoch nicht so einfach zu kompilieren. Bei Projekten wie OpenDarwin und PureDarwin war es daher die erste Aufgabe, den Quelltext auch mit öffentlich verfügbaren Compilern übersetzbar zu machen. Auch sind die Quelltexte spezifisch auf die Apple-Hardware angepasst und zudem nicht sehr gut dokumentiert, was eine Anpassung schwierig macht. Beim XNU-Kernel beispielsweise veröffentlichte Apple stets nur die für die von den aktuellen Macs genutzte Prozessorarchitektur notwendigen Teile für macOS, nicht aber jene für iOS. Da mit Mac OS X Snow Leopard (10.6, 2009) die Unterstützung für PowerPC eingestellt wurde, ist auch keine PowerPC-Unterstützung mehr im XNU-Kernel für Darwin 10.0 und neuer zu finden. Quelltexte von iOS sind bis auf wenige Ausnahmen nicht verfügbar.\n\nAnfangs wurde von Apple ein als „Darwin OS“ bezeichnetes Paket für Mac OS veröffentlicht, womit Darwin auf einem Mac installiert werden konnte. Eigenständige Distributionen wurden teilweise von externen Entwicklern angeboten. Ab 2002 wurde diese Aufgabe, das Darwin-Betriebssystem als Distribution verfügbar zu machen, an das OpenDarwin-Projekt abgegeben. Als dieses eingestellt wurde, gab es unabhängige Bestrebungen, eine auf Darwin basierende Distribution eines vollwertigen Unix-Betriebssystems zu veröffentlichen, die jedoch nur von mäßigem Erfolg geprägt waren.\n\nAb 1999 begann Apple die ersten von Mac OS X an eine limitierte Anzahl von Entwicklern abzugeben. Dessen Kern basierte auf dem von NeXT entwickelten OPENSTEP (bis 1995 NeXTStep bzw. NeXTSTEP) und wurde nach dessen Übernahme Ende 1996 von Apple als Nachfolge für System 7 weiterentwickelt. Nachdem 1998 das Projekt Rhapsody (das als Version 5.x von OPENSTEP angesehen werden kann) eingestellt wurde, veröffentlichte Apple 1999 eine darauf direkt basierende Version als Mac OS X Server 1.0. Der Kern dieses ersten „Mac OS X“ genannten Betriebssystems wurde die Basis von Mac OS X 10.0 (2001, Alpha- und Beta-Versionen 1999–2000), gleichzeitig wurde der quelloffene Teil jedoch von Apple in der Hoffnung veröffentlicht, die Mitarbeit von freien Entwicklern anzuregen. Da sich Apple ab 1997 für das Ende der Multi-Platform-Strategie entschied (und das für Macs und PCs entwickelte Rhapsody als gescheitert bezeichnete) konzentrierte sich die weitere Entwicklung rein auf die PowerPC-Architektur. Die Freigabe der Quelltexte ermöglichte jedoch eine Rückportierung auf die Intel-x86-Architektur durch freie Entwickler.\n\nDarwin OS 0.3 (1999) hatte die gleichen Anforderungen an die Hardware wie Mac OS X Server 1.0 (1999, Rhapsody 5.3–5.6) und, da es mit Mac OS X weiterentwickelt wurde, bald dessen Anforderungen und Systemkompatibilität. So läuft Darwin 1.2.1 (2000) auf allen Macintosh-Systemen, auf denen auch die Mac OS X Public Beta läuft und auf einigen Intel-x86-PCs. Allerdings wurde die Unterstützung der x86-Version („Darwin x86“) von Apple nicht gerade aktiv verfolgt.\n\nDas von Apple veröffentlichte Paket ließ sich nur von Mac OS aus installieren, doch gab es externe Quellen für eine Art Darwin-Distribution als startfähige Installations-CD.\n\nWie schon NeXTStep und OPENSTEP profitierte auch Darwin (und damit auch Mac OS X bzw. OS X bzw. macOS) von einer Vielzahl quelloffener Projekte. Apple ist damit Teil der weltweiten Open-Source-Community, da Weiterentwicklungen oft lizenzbedingt veröffentlicht werden müssen (wie z. B. WebKit).\n\nOpenDarwin wurde im April 2002 von der ISC und Apple gegründet. Es stand jedermann frei, zum Projekt beizutragen. Das Ziel war eine eigenständige Darwin-Distribution, als Plattform sollten sowohl PowerPC- als auch x86-Architekturen unterstützt werden. Damit war OpenDarwin eine Verkörperung des Darwin-Betriebssystems, die es auch externen Entwicklern möglich machte, am Quelltext direkt beizutragen, ohne Apple-Mitarbeiter zu sein oder sich bei Apple registrieren zu müssen. Wie schon bei Darwin OS (ab 1999) war es die einzige Möglichkeit für externe Entwickler, an den Kernkomponenten wie dem Systemkernel mitarbeiten zu können, da der CVS-Tree in einigen Teilen immer auf dem Live-Darwin-CVS beruhte. Andere Teile wurden von Apple hingegen nur periodisch aktualisiert, was Beiträge außerhalb von Apple zusätzlich erschwerte. OpenDarwin wurde jeweils aufgrund des aktuellen CVS-Tree verfügbar gemacht.\n\nStatt der erhofften Offenheit einer eigenständigen Distribution war es jedoch zunehmend schwieriger, die von Apple zur Verfügung gestellten Quellen ohne einen Computer von Apple, auf dem Mac OS X lief, zu kompilieren. Apple nutzte ein nur intern verfügbares Build-System und passte die Quellen nur an die eigene Hardware an, größere Beiträge von unabhängigen Entwicklern wurden hingegen in den meisten Fällen nicht in Darwin integriert. Im Gegenteil wurde von Apple bereits existierende Unterstützung für ältere Hardware entfernt, was die freien OpenDarwin-Entwickler irritierte, da ein solches Vorgehen als unüblich in der Open-Source-Welt gilt. Mit dem Erfolg von Mac OS X begann Apple in zunehmendem Maße damit, neue Versionen der Quelltexte sowie neue Treiber vom proprietären Mac OS X abhängig zu machen, was eine eigenständige Darwin-Distribution ohne zusätzlichen Aufwand wie Eigenentwicklungen oder Rückportierungen unmöglich machte. Schließlich wurden die aktivsten OpenDarwin-Entwickler von Apple übernommen. Da diese einen Geheimhaltungsvertrag (NDA: ) unterzeichnen mussten, konnten sie nichts mehr für das freie Projekt beitragen. OpenDarwin konnte schließlich mangels aktiver Entwickler nicht mehr weiter gepflegt werden.\n\nAm 26. Juli 2006 wurde das Projekt „OpenDarwin“ eingestellt. Apple war offenbar nicht daran interessiert, OpenDarwin nach den Grundsätzen einer Open-Source-Community zu unterstützen, sondern nutzte Darwin lediglich als Basis für das proprietäre Mac OS X. In diesem Licht erscheinen die quelloffenen Betriebssystembestandteile nur noch als Mittel, Mac OS X erfolgreicher zu vermarkten. Auf der offiziellen Webpräsenz, die bis zum 28. Juni 2007 erreichbar war, gaben die Administratoren zuletzt als Grund für das OpenDarwin-Ende an, dass die gesetzten Ziele des Projektes in vier Jahren nicht erreicht wurden.\n\nNachdem OpenDarwin eingestellt wurde, gab es ab Ende 2007 mit PureDarwin ein Nachfolgeprojekt, dessen Ziel es war, ein startfähiges Darwin-ISO herzustellen. Allerdings gab es seit 2012 keine neuen Bemühungen und die letzte Version ist PureDarwin 1.3 Beta, die auf Darwin 9.8 (analog Mac OS X Leopard 10.5.8 von 2007–2009) basiert.\n\nMit dem „Darwin on ARM Project“ gibt es seit ca. 2013 ein auf Darwin 12.x (OS X Mountain Lion, 10.8, 2012) basiertes Projekt eines einzelnen Entwicklers, das den Darwin-Kernel XNU – und alles andere, was dazu benötigt wird – auf ARM-Geräten lauffähig machen soll (AArch64, ARMv7, ARMv6-A).\n\nTrotz der Bestrebungen, Darwin abseits von macOS – das von 1998 bis 2012 Mac OS X und von 2012 bis 2016 OS X hieß – verfügbar zu machen, gibt es das Darwin-Betriebssystem hauptsächlich als Unterbau und somit als integralen Bestandteil von macOS und iOS. Daher nimmt diese somit unvollständige Versionsgeschichte auch nur auf die Betriebssysteme von Apple Bezug, nicht aber auf die oben erwähnten Projekte oder mögliche weitere Inkarnationen von Darwin.\n\nWelche Version auf einem System läuft kann mit codice_1 ausgelesen werden, wenn Zugang zu einer Kommandozeile vorhanden ist, was bei iOS ohne Jailbreak grundsätzlich nicht der Fall ist. Unter Mac OS X Lion 10.7.2 gibt codice_1 beispielsweise codice_3 aus.\n\n\n"}
{"id": "15554", "url": "https://de.wikipedia.org/wiki?curid=15554", "title": "QCad", "text": "QCad\n\nQCAD ist ein CAD-Programm für zweidimensionale Zeichnungen der Firma RibbonSoft, welches für Microsoft Windows, macOS und Linux verfügbar ist. Es wird sowohl in einer proprietären, lizenzkostenpflichtigen Professional-Edition, als auch in einer sogenannten Community-Edition im Quelltext als freie Software unter der GPL vertrieben. Die Professional-Edition bietet zusätzliche Funktionen sowie Unterstützung für das DWG Format. QCAD wird derzeit als das umfangreichste freie CAD-Programm angesehen. QCAD wird auch für CNC-Fräsen verwendet.\n\nDer Hauptentwickler von QCAD ist Andrew Mustun. Als er mit der Entwicklung begann, war QCAD ein Teil seines CAM-Systems namens \"CAM Expert\". Mit der Zeit verfügte CAM Expert jedoch über viele eigene CAD-Funktionen. Somit war es sinnvoll, QCAD im Oktober 1999 als eigenes Programm abzusplitten. Einige Zeit später legte er den Quelltext von QCAD jedem unter den Bedingungen der GPL offen. Im März 2002 wurde mit der Entwicklung von QCAD 2 begonnen, das Dokumente in das Format DXF 2000 (AC1015) exportieren kann. QCAD 3 wurde im Juli 2012 veröffentlicht und bietet eine sehr komplette Skriptschnittstelle und Unterstützung für das DWG-Dateiformat.\n\nQCAD bietet etwa 40 Konstruktions- und 20 Modifikationswerkzeuge für Punkte, Linien und Bögen, Kreise, Ellipsen, Splines, Polylinienzüge, Texte, Bemaßungen und Schraffuren. Weiterhin kann das Programm mit Ebenen und Blöcken (Gruppierungen) umgehen. Maßstabgetreues Drucken, Objekt-Fang, Symbolbibliothek und Mess-Werkzeuge gehören ebenfalls zum Funktionsumfang.\n\nAls Dateiformat verwendet QCAD DXF. Die zu verwendende Maßeinheit kann zwischen dem metrischen und britischen Einheitensystem umgeschaltet werden.\n\n\n"}
{"id": "15590", "url": "https://de.wikipedia.org/wiki?curid=15590", "title": "Microsoft Windows 2.x", "text": "Microsoft Windows 2.x\n\nMicrosoft Windows 2.x ist eine vom Unternehmen Microsoft entwickelte grafische Benutzeroberfläche, die Ende 1987 veröffentlicht wurde. Als Nachfolger von Microsoft Windows 1.0 war sie die zweite Version von Windows. Die Veröffentlichung von \"Windows 2.03\" löste einen Rechtsstreit mit dem Unternehmen Apple aus, der bis 1992 andauerte.\n\nWindows 2.x ist weiterhin eine erweiterte grafische Oberfläche für das Betriebssystem MS-DOS und bietet die Möglichkeit mehrere Programme gleichzeitig grafisch in Fenstern getrennt zu starten. Neu im Unterschied zu Windows 1.x ist, dass die Fenster sich überlappen können und in der Größe veränderbar sind. \"Windows 2.x\" bietet darüber hinaus neue Funktionen, wie z. B. die Kommunikation zwischen einzelnen Programmen mit Dynamic Data Exchange (DDE), Smartdrive, Warntöne (über den PC-Lautsprecher), VGA-Unterstützung, neue Bildschirmschriften und erstmals die Möglichkeit zur Umschaltung via Alt-TAB zwischen Programmen. Für 1987 neue Eingabegeräte, wie die PS/2-Maus, werden nun auch unterstützt.\n\n\"Windows 2.03\" ist das letzte Windows, welches sich auf Disketten installieren und ohne Festplatte verwenden lässt. \"Windows 2.1\" bietet als Neuerung erneuerte und mehrere neue Treiber (Grafik, Computer, Drucker) und unterstützt Erweiterungsspeicher (XMS und EMS) besser. Mit \"Windows 2.11\" gibt es erneut minimale Verbesserungen bei Speicher- und Druckerzugriffen.\n\nWindows 2.x ist auf die Eigenschaften des Prozessors zugeschnitten und wurde entsprechend in den Varianten \"Windows/286\" und \"Windows/386\" vertrieben. Die Version /386 nutzt erstmals den Schutzmodus des Intel-386er-Prozessors, ist zu früheren Prozessoren nicht kompatibel und unterstützt mit Version 2.1 hochauflösendere Bildschirme. \n\n\"Windows /386 2.01\" wurde noch vor der ersten, im freien Handel verfügbaren, \"Version 2.03 (/286 und /386)\" ab September 1987 als OEM-Version vertrieben, da Compaq zu dieser Zeit Systeme wie den Compaq Portable 386 mit 80386-Prozessoren herausbrachte und diese mit einem Windows ausstatten wollte, welches die Vorteile der damals neuartigen Prozessoren auch nutzen kann. Eine öffentliche Version 2.02 ist nicht bekannt.\n\nDie Fortschritte von Windows selbst, welches in verschiedenen Sprachversionen erschien, hielten sich damals in Grenzen: Mithilfe der grafischen Benutzeroberfläche, die zu diesem Zeitpunkt noch immer weitgehend das Aussehen einer textorientierten Benutzerschnittstelle hat, wurden meist nur DOS-Programme gestartet, so dass das Wechseln zwischen einzelnen Tasks der einzig wirkliche Vorteil von Windows war.\n\nDa die meisten Softwarehersteller nicht bereit waren, ihre Programme auf Windows zu portieren, begann Microsoft selbst, die ersten Windows-Anwendungen zu entwickeln. Ein Teil des Erfolgs von Windows 2 ist dem Programm PageMaker in seiner ersten Windows-Version zu verdanken, welches bereits für Windows 1.x erhältlich war. Dieses wurde mit einer auf eine Kompatibilitätsschicht eingeschränkten Windows-Runtime ausgeliefert und bot damit auch auf Computern ohne Windows-Lizenz die Möglichkeit, diese Anwendung auf einem DOS-Rechner zu starten.\n\nZeitgleich mit Windows 2.03 erschien das Tabellenkalkulationsprogramm Microsoft Excel 2.0, das erste von Microsoft entwickelte Anwendungsprogramm, das speziell auf die Fähigkeiten von Windows ausgerichtet war und zu einer stärkeren Verbreitung von Windows beitrug. 1989 folgte gleichzeitig mit Windows 2.11 das Textverarbeitungsprogramm Word. Alle drei Programme wurden von Mac OS auf Windows portiert und danach für beide Betriebssysteme weiterentwickelt. Insbesondere Excel verhalf Windows zum Durchbruch, führte aber auch zum im Folgenden beschriebenen Rechtsstreit mit Apple.\n\nAm 17. März 1988 verkündete die Firma Apple Computer Inc., man wolle gegen \"Windows 2.03\" und gegen Hewlett-Packards auf Windows beruhende objektorientierte graphische Benutzeroberfläche \"NewWave\" gerichtlich vorgehen. Apple beschuldigte Microsoft, das gemeinsame Lizenzabkommen von 1985 verletzt und das Look and Feel der Benutzeroberfläche des Apple Macintoshs (beispielsweise überlappende Fenster) kopiert zu haben. Apple verlangte die Einstellung des Verkaufes von \"Windows 2.03\" und die Untersagung der Freigabe von Hewlett-Packards \"NewWave\". Richter William Schwarzer (* 1925) ließ am 25. Juli 1989 nur zehn der insgesamt 189 von Apple angeführten Punkte für das Gerichtsverfahren zu. Erst am 15. April 1992 entschied Richter Vaughn Walker (* 1944) in San Francisco, dass die übrigen fraglichen Elemente nicht urheberrechtsfähig seien.\n\nWindows war auf unterschiedlichen Diskettenformaten mit verschiedenen Speicherdichten erhältlich. Das direkt nach der Installation ausgeführte Tool MEMSET.EXE ist ferner für die Speicherkonfiguration (RAM, Smartdrive) von Windows 2.x verantwortlich. Wird die Speicherkonfiguration des Computers geändert, muss Windows 2.x - im Gegensatz zu heutigen Versionen - neu installiert werden, worauf bei jedem Start der grafischen Oberfläche hingewiesen wird.\n"}
{"id": "15938", "url": "https://de.wikipedia.org/wiki?curid=15938", "title": "IMac", "text": "IMac\n\nDer iMac ist ein vom US-amerikanischen Unternehmen Apple produzierter Computer. Grundsätzliche Idee des iMacs war ein sogenanntes „All-in-one“-Gehäuse mit möglichst einfacher Bedienbarkeit für eine technisch weniger versierte Zielgruppe. Mit diesem Gerät knüpfte Apple an frühere, in den 1980er-Jahren produzierte, Rechner an, in denen Rechner und Minibildschirm in einem Gerät integriert waren.\n\nDas „i“ in iMac hat, laut Keynote von 1998, folgende Bedeutungen: internet, individual, instruct (instruieren, anleiten), inform (informieren) und inspire (inspirieren). Es wurde in den folgenden Jahren zum Präfix vieler Apple-Produkte vom MP3-Player iPod über das Laptop iBook bis zum Smartphone iPhone.\n\nPrimäres Merkmal der ersten iMac-Generation war das halbdurchscheinende (semitransluzente) Gehäuse aus Polycarbonat, in dem Bildschirm und PC samt Lautsprechern integriert waren. Die externe Tastatur und kreisrunde Maus hatten das gleiche Design. Er wurde am 7. Mai 1998 vorgestellt und am 15. August 1998 ausgeliefert. Der Preis in Deutschland betrug bei der Markteinführung 2.999 DM (bzw. in den USA 1.299 Dollar zuzüglich Mehrwertsteuer), was heute, inflationsbereinigt, rund Euro (in den USA Dollar exkl. Steuer) entspräche.\n\nEs wurde auf alle Schnittstellen verzichtet, bis auf das zu dieser Zeit noch nicht weit verbreitete USB in der Version 1.2 mit maximal 12 Mbps. Abgesehen davon war neben Kopfhörer- und Mikrophonbuchse noch ein Modem- und ein FastEthernet-Anschluss eingebaut, womit das Ziel, einen Rechner für das „Surfen im Internet“ anzubieten, erreicht wurde. Ungewöhnlich war der Verzicht auf ein integriertes Diskettenlaufwerk (externe Laufwerke können seit dem Jahr 2000 separat erworben werden) und auf die vormals in allen Apple-Modellen eingebaute SCSI-Schnittstelle. Die eingebaute Festplatte und das eingebaute CD-ROM-Laufwerk waren per IDE angeschlossen, zum Datenaustausch gab es Netzwerk und USB-Schnittstelle. Disketten und Peripheriegeräte von vorherigen Macs, wie ursprünglich per Parallelport angeschlossene Drucker, konnten nicht mehr genutzt werden oder mussten durch USB-Hardware ersetzt werden.\n\nBei den ersten beiden Revisionen gab es auf der Hauptplatine einen Steckplatz („Mezzanine Slot“), der von Apple nur für Tests der Hauptplatinen vorgesehen war und nicht offiziell propagiert wurde. Die deutsche Firma Formac fand heraus, dass es sich dabei um einen vollwertigen PCI-Steckplatz handelte, und brachte passende Erweiterungskarten (iPro RAID und iPro RAID TV mit SCSI bzw. SCSI + TV-Tuner) heraus. Bei der dritten Revision des iMacs wurde die entsprechende Buchse nicht mehr verbaut, konnte aber laut einem Artikel der Computerzeitschrift c't nachträglich aufgelötet werden. Grundsätzlich waren die iMacs somit etwas schlechter nachrüstbar als Notebooks. Der Austausch der RAM-Module war einfach, der Einbau einer Festplatte mit größerer Kapazität hingegen schwierig, da das Gehäuse nicht leicht zu öffnen war.\n\nAufsehen erregte vor allem das Design des iMacs. Das erste Modell kam in der Farbe \"„Bondi Blue“\" auf den Markt. Bis zu diesem Zeitpunkt waren Computer meist in einem neutralen Beige, Grau oder Schwarz produziert worden, fast alle in quaderförmigen, kantigen Gehäusen. Apple präsentierte nun einen „bunten“, kugelartigen und durchsichtigen Computer und erweiterte kurz danach die Palette um weitere Farben bis hin zu einem mit einem Blumendekor bedruckten Gehäuse.\n\nDer iMac-Designer Jonathan Ive löste mit dem bunten und transluzenten Gehäuse einen Trend in der Computerindustrie aus. Das halbdurchsichtige Gehäuse des ersten iMacs hat viele Designer inspiriert, ähnliche Entwürfe für eine Vielzahl von Computern und deren Peripheriegeräte zu machen. Bei Komponenten wie Mäusen, Modems und Routern wurde \"durchsichtig und bunt\" schick, und das nicht nur für den Mac-Markt.\n\nAuf der Apple Expo, der hauseigenen Messe des Herstellers, wurde der iMac immer wieder ein Überraschungsprodukt. Mit der Idee einer einfachen Installation des Rechners und der benutzerfreundlichen Möglichkeit, das Internet auf einfache Weise zu nutzen, meldete sich Apple – mit großem Erfolg – zurück auf dem Konsumentenmarkt, den das Unternehmen lange vernachlässigt hatte. Der iMac war der Beginn des kommerziellen Comebacks der Firma Apple.\n\nDer Journalist und Autor Steven Levy beschrieb den neuen iMac in Newsweek, Ausgabe 131/1998: \"Ein Stück Hardware, der Science-Fiction-Schimmer mit dem kitschigen Spleen eines Cocktailschirmes vermischt\" (A piece of hardware that blends sci-fi shimmer with the kitsch whimsy of a cocktail umbrella).\n\nDer erste iMac, erhältlich ab dem 15. August 1998, wurde anfangs nur in der Farbe „Bondi Blue“ angeboten. Wie auch Revision B wird er, weil später diese Farbe durch andere ersetzt wurde, auch so bezeichnet. Er hatte einen G3-Prozessor mit einer Taktfrequenz von 233 MHz, einen 15-Zoll-Röhrenmonitor (CRT), eine ATI-Rage-IIc-Grafikkarte mit 2 MB SDRAM-Grafikspeicher, ein CD-Laufwerk, zwei USB-Schnittstellen, ein Modem, Ethernet 10/100, eine Festplatte mit einer Kapazität von 4 GB und ab Werk 32 MB PC-100-SDRAM. Die Auflösung des Bildschirms betrug maximal 1024×768, wobei die Bildschärfe bei dieser Auflösung oftmals kritisiert wurde.\n\nAuf der Hauptplatine befand sich ein „Mezzanine Slot“ genannter PCI-Slot; das CD-Laufwerk war ein Schubladenlaufwerk, wie es in Notebooks zu finden war. Außerdem verfügte der iMac noch über eine Infrarot-Schnittstelle.\n\nAm 17. Oktober 1998 erschien die erste überarbeitete Version des iMac, die sich nur durch eine bessere Grafikkarte, einer ATI Rage Pro mit 6 MB RAM, vom Vorgängermodell unterschied.\n\nMit der am 5. Januar 1999 erschienenen Version wurde ein deutlicher Schritt vollzogen. Der iMac hatte nun einen mit 266 MHz getakteten G3-Prozessor und eine ATI Rage Pro Turbo mit 6 MB RAM. Der „Mezzanine Slot“ war nicht mehr aufgelötet und die Infrarotschnittstelle wurde weggelassen. Der iMac war nun in fünf verschiedenen, neuen Farben erhältlich: „Strawberry“ (Rosa), „Blueberry“ (Blau), „Lime“ (Tiefgrün), „Grape“ (Violett) und „Tangerine“ (Orange).\n\nAm 14. April 1999 wurde mit Revision D die Taktfrequenz des G3-Prozessors auf 333 MHz angehoben.\n\nMit dem iMac DV („Digital Video“) ab dem 5. Oktober 1999 wurde ein weiterer großer Schritt vollzogen. Dies war nun das erste Modell mit einer FireWire-Schnittstelle, einem DVD-Laufwerk und dem neuen iMovie von Apple, geeignet zur Bearbeitung digitaler Videos von Videokameras beziehungsweise Geräten basierend auf dem MiniDV-Standard. Ferner wich das alte Schubladenlaufwerk einem Slot-In-Laufwerk. Außerdem konnte man erstmals eine WLAN-Karte von Apple nachrüsten. Das neue Hardwaredesign konnte durch Ausnutzen der freien Konvektion nun auf Lüfter verzichten, die Prozessoren wurden passiv über Kühlkörper gekühlt.\n\nNeben dem iMac DV gab es eine ähnliche, aber billigere Variante, die nur in der Farbe „Blueberry“ erhältlich war. Der G3-Prozessor war statt mit 400 MHz mit 350 MHz getaktet, zudem fehlte die FireWire-Schnittstelle und es war ein Slot-In-CD-ROM-Laufwerk verbaut. Beide hatten als Grafikkarte eine ATI Rage 128 VR mit 8 MB RAM. Der Arbeitsspeicher wurde auf 64 beziehungsweise 128 MB erhöht. Zu den bisherigen Farben kam eine „Special Edition“ des iMac DV in „Graphite“ genanntem Grau.\n\nDie am 19. Juli 2000 erschienenen iMacs hatten nun alle ein DVD-ROM-Laufwerk, eine neuere ATI Rage 128 Pro mit 8 MB RAM und bis auf das billigste Modell einen FireWire-Anschluss und nachrüstbares WLAN. Außerdem wurden die verfügbaren Gehäusefarben geändert in „Graphite“ (Grau), „Ruby“ (Rubinrot), „Snow“ (undurchsichtiges Weiß), „Indigo“ (Dunkelblau) und „Sage“ (Hellgrün). Das billigste Modell mit 350 MHz gab es nur in der Farbe „Indigo“, die anderen hatten einen 400, 450 und 500 MHz getakteten G3.\n\nMit den ab dem 22. Februar 2001 angebotenen Modellen wurden die Gehäusefarben wieder geändert. Neben den bewährten „Graphite“ und „Indigo“ wurden nun mit den Mustern „Blue Dalmatian“ und „Flower Power“ bedruckte iMacs angeboten. Die CPU-Taktfrequenz wurde auf 400, 500 und 600 MHz erhöht, bei allen Modellen wurde allerdings statt des DVD-ROM-Laufwerks nun ein CD-Brenner eingebaut, das billigste Modell wiederum ausschließlich in Indigo bekam sogar nur ein CD-ROM-Laufwerk. Alle Modelle ab 500 MHz hatten zudem nun eine ATI Rage 128 Ultra mit 16 MB als Grafikkarte.\n\nAm 18. Juli 2001 erschien dann die letzte Revision der ersten Generation, mit dem gleichen Röhrenmonitor und Abmessungen wie Revision A. Die G3-Prozessoren waren nun mit 500, 600 und 700 MHz getaktet, die Grafik bei allen die ATI Rage 128 Ultra und der Arbeitsspeicher 64, 128 oder 256 MB. Das teuerste Modell wurde bereits im Januar 2002 zugunsten der neuen iMac-Generation eingestellt, die anderen Modelle waren noch als billige Alternative bis März 2003 erhältlich.\n\nDas erfolgreiche Design wurde im Januar 2002 komplett ersetzt. Die Vielfarbigkeit wich einem schlichten Weiß. Der Röhrenmonitor wurde durch einen an einem dreifach beweglichen Arm (Goose Neck) befestigten Flüssigkristallbildschirm (LCD) ersetzt, in dessen halbkugelförmigem Fuß sich der gesamte Computer samt Netzteil, Festplatte, Schnittstellen und optischem Laufwerk befindet. Vorher erdachte Ideen, den gesamten Rechner in der Vertikalen im Gehäuse eines Flachbildschirms unterzubringen, wurden beim iMac G4 aus technischen Gründen verworfen, beim iMac G5 letztendlich aber trotzdem umgesetzt. Das ursprüngliche iMac-Design wurde im eMac fortgeführt.\nDie zweite Generation wurde mit Prozessorgeschwindigkeiten zwischen 700 MHz und 1,25 GHz angeboten.\nDie Größen der LC-Bildschirme begannen bei 15 Zoll und endeten mit für damalige Verhältnisse sehr großen 20 Zoll.\n\nSeit der dritten Generation befindet sich der eigentliche Rechner komplett hinter dem Flachbildschirm.\n\nAm 31. August 2004 wurde der neue „iMac G5“ (später als Rev A bezeichnet) auf der Messe „Apple Expo“ in Paris vorgestellt. Ab Mitte September desselben Jahres wurde das Gerät ausgeliefert. Das Design wurde erneut völlig verändert: Der Computer befindet sich nun zusammen mit dem LC-Display (17″ und 20″) in einem Gehäuse und ist mit jeweils einem 64-Bit-G5 bestückt. Obwohl auch das optische Laufwerk und das Netzteil integriert sind, ist dieser neue iMac nur 5,5 cm dünn.\n\nTechnisch basiert der iMac G5 (Rev A) auf dem damals neuen U3lite/Shasta-Chipsatz, der auch im (Basismodell) Power Mac G5 „Late 2004“ mit Einzelprozessor eingesetzt wurde. Die Revision A ist bekannt für etwas lautere Lüfter, die Apple allerdings im Rahmen der Garantiefrist ersetzte (Lüfter und/oder Platine). Es wurden Festplatten mit 80 GB oder 160 GB verbaut. Als Grafikausgang wurde Mini-VGA eingesetzt.\n\nIm Mai 2005 erhielt der iMac G5 (Rev B) eine leicht überarbeitete Hauptplatine, u. a. mit Gigabit-Ethernet, und eine schnellere Grafikkarte.\n\nWeiterhin wurde in der Rev B eine neue Version des Shasta-I/O-Controllers eingesetzt. Im neuen Shasta wurden Datentransferprobleme („Flaschenhals“) der internen S-ATA-Schnittstelle behoben. Diese Modelle wurden mit 160 GB und 250 GB Festplatten ausgeliefert.\n\nMitte Oktober 2005 kam die letzte Version des iMac G5 heraus (Rev C). Nun wurde eine iSight-Kamera im Gehäuse integriert. In Verbindung mit dem neuen Programm \"Photo Booth\" wurde der iMac zum Fotoautomaten mit vielen Möglichkeiten zur Bildbearbeitung. Die zweite Neuheit wird als „Front Row“ bezeichnet: Mit einer kleinen mitgelieferten Fernbedienung kann man direkt auf seine Bilder, Songs, Videos und DVDs zugreifen.\nAußerdem ist der iMac nochmals etwas schlanker(~4 cm), leiser und dank DDR2-RAM und neuer PCI-Express-Grafikkarte schneller geworden.\n\nTechnisch basiert der iMac in der Rev C nun auf dem gleichen Chipsatz K2/U4 wie die gleichzeitig vorgestellten neuen Dual Core Power Macs. Dieser ist eine Weiterentwicklung des vorherigen Power-Mac-Chipsatzes K2/U3, womit Apple den „Billig-Chipsatz“ Shasta/U3lite endgültig aufgab. Andererseits hat der letzte Chipsatz für G5-Rechner die SMU von der Shasta/U3lite-Kombination geerbt, alle anderen Rechner hatten eine PMU.\n\nIm Januar 2006 wurde der erste Apple Macintosh mit einem Intel-Prozessor vorgestellt. Dabei bietet das Gerät etwa den gleichen Funktionsumfang wie das im Oktober 2005 vorgestellte Modell, jedoch ist es mit einem Intel Core Duo bestückt und laut Apple-Benchmarks zwei- bis dreimal schneller als ein G5-iMac.\n\nAls weitere Neuerung wurde der mini-VGA-Anschluss durch einen mini-DVI-Anschluss ersetzt, der nun auch den Betrieb eines zweiten Monitors zur Erweiterung des Desktops erlaubt. Die mit Ende der dritten Generation (Rev C) eingeführte eingebaute Webcam iSight ist auch in dieser Generation wieder vorhanden.\n\nIm Juli 2006 wurde eine besondere Version des 17-Zoll-iMacs eingeführt, die über eine leistungsschwächere Grafikhardware (die Ausführung besitzt keine Grafikkarte, stattdessen wird die Grafik im Intel-Chipsatz verwendet) verfügt, außerdem ist statt des DVD-Brenners („super drive“) ein „combo drive“ eingebaut. Im Gegensatz zum Standardmodell fehlen außerdem die Fernbedienung Apple Remote und das integrierte Bluetooth-Modul. Aufgrund dieser Einsparungen beläuft sich der Preis für den iMac auf 938 Euro. Er wird als iMac für Kunden im Bildungsbereich beworben, also zum Beispiel für Schulen.\n\nNachdem der neue iMac anfangs einige Tage lang im „Bildungs-Store“ von Apple zu bestellen war, war er kurz darauf nur noch direkt von Bildungseinrichtungen zu beziehen.\n\nIm September 2006 stellte Apple erneut überarbeitete Modelle des iMac vor. Erstmals wurde ein Gerät mit einem 24-Zoll-Bildschirm angeboten. Die Geräte mit 17″ und 20″ Bilddiagonale bleiben auch weiterhin im Programm. Der \"Education iMac\" ist nun als offizielles kostengünstiges Einsteiger-Gerät (engl. „low end“) auch für Privatkunden erhältlich. Die Geräte sind alle mit dem Intel Core 2 Duo-Prozessor ausgestattet, der die 64-Bit-Befehlserweiterung Intel 64 enthält. Als echte Neuerung sind, vorerst nur beim iMac 24″, alle wichtigen Teile des Rechners austauschbar, was bedeutet, dass man das 24″-Modell mit verschiedenen Komponenten bestellen kann. Allerdings werden Komponenten wie z. B. die Grafikkarte gelötet, wodurch es dem Endbenutzer nicht möglich ist, die Komponenten nachträglich auszutauschen (RAM-Speicher ausgenommen).\n\nAm 7. August 2007 stellte Steve Jobs eine neue iMac-Generation vor. Die Hülle des iMacs besteht nun aus Aluminium und Glas. Im Vergleich zu den Vorgängermodellen ist die neue Generation dünner und nur noch mit einer Schraube (statt bisher zwei) am Gehäuse, mit der der Arbeitsspeicher ein- und ausgebaut werden kann, versehen. Die Rückseite besteht aus mattem schwarzem Kunststoff, in dem das Apple-Logo glänzend eingeprägt ist. Der iMac ist mit einem 20- oder 24-Zoll-Breitbildmonitor erhältlich und wird mit einer neuen, flachen USB-Tastatur in Aludesign und mit besonderen Tasten für erweiterte Funktionen ausgeliefert.\nIm neuen iMac ist es nun möglich 4 GB RAM zu verbauen (statt vorher 3 GB) und das 20″-Gerät verfügt erstmals neben einem Firewire-400-Port auch über einen Firewire-800-Port.\n\n\nAm 28. April 2008 erschien eine neue Version des iMacs, die unter anderem bis zu 3,06 GHz im Spitzenmodell bietet. Als Grafikkarte kommen eine ATI Radeon HD 2400 XT (im 20″-Modell mit 2,4 GHz), eine ATI Radeon HD 2600 PRO (20″ mit 2,66 GHz und 24″) oder optional eine nVIDIA GeForce 8800 GS (die in Wirklichkeit eine 8800M GTS ist) zum Einsatz. Die Preise für das Einstiegsmodell mit 20″-Bildschirm wurden auf 999 Euro gesenkt und der CPU-Takt von 2,0 auf 2,4 GHz angehoben. Zudem kommen jetzt Penryn-Prozessoren zum Einsatz, die einen von 4 auf 6 MB erhöhten L2-Cache beinhalten.\n\nAm 3. März 2009 ist eine weitere Version des iMacs erschienen. Der CPU-Takt des Einstiegsmodells wurde auf 2,66 GHz erhöht, das Spitzenmodell bietet weiterhin 3,06 GHz. Als Grafikkarten kommen eine Nvidia GeForce 9400M (20″ und 24″-Modell mit 2,66 GHz), eine Nvidia GeForce GT 120 (im 24″-Modell mit 2,93 GHz) und eine nVIDIA GeForce GT 130 (im 24″-Modell mit 3,06 GHz) hinzu. Eine ATI Radeon HD 4850 512 MB ist gegen Aufpreis ebenfalls erhältlich.\nDie Preise für das 20″-Modell wurden wieder auf 1.099 Euro angehoben. Weiterhin kommen im iMac die Penryn-Prozessoren zu Einsatz. Der Arbeitsspeicher (RAM) wurde standardmäßig auf 2 GB (20-Zoll-Modell) bzw. 4 GB (24-Zoll-Modelle) angehoben. Weiterhin wurde der Arbeitsspeicher von DDR2 auf DDR3 umgestellt, somit kann der Arbeitsspeicher auf bis zu 8 GB RAM, zuvor waren nur 4 GB möglich, erweitert werden. Abgesehen vom nun flach zulaufenden Standfuß und den Anschlüssen, statt FireWire 400 jetzt 800, an der Rückseite, hat sich das Äußere des iMacs nicht verändert.\n\nAm 20. Oktober 2009 stellte Apple Inc. eine neue Generation der iMacs vor. Das kleinste Modell bietet nun ein 21,5″ (1920×1080) großes Display und eine 500 GB Festplatte, das größte hat ein 27″ (2560×1440) Display und eine 1 TB oder 2 TB Festplatte. Alle Modelle verfügen über IPS-Displays mit LED-Hintergrundbeleuchtung. Der 21,5″-iMac sowie das günstigste 27″-Modell verfügen standardmäßig über einen 3,06 GHz oder einen optionalen 3,33 GHz Intel Core 2 Duo Prozessor. Das zweite 27″-Modell ist mit dem 2,66 GHz Core i5 oder dem wahlweise erhältlichen 2,8 GHz Core-i7-Prozessor von Intel der erste iMac mit einer Quad-Core CPU.\n\nAlle iMacs verfügen nun über vier Steckplätze für den Arbeitsspeicher und werden ab Werk mit 4 GB RAM ausgestattet, der sich im 21,5″-Modell und im 27″-Modell auf bis zu 16 GB erweitern lässt. Zudem wurde die Grafikleistung mit dem Nvidia GeForce 9400M von Nvidia im günstigsten iMac und dem ATI Radeon HD 4670 Grafikprozessor in den anderen Modellen verbessert. Die ATI Radeon HD 4850 Grafikkarte ist im Quad-Core iMac ab Werk schon verbaut.\n\nDas Gehäuse ist ab dieser Baureihe komplett aus einem Stück Aluminium gefräst („Unibody“), so dass die Rückseite des iMacs nun nicht mehr schwarz, sondern ebenfalls aluminiumfarbig ist.\n\nStandardmäßig wird der iMac mit einer kabellosen Tastatur und der Magic Mouse ausgeliefert. Apple baut ab dieser Version außerdem einen SD-Kartenslot in den iMac ein.\n\nAm 27. Juli 2010 wurde eine neue Revision des iMacs vorgestellt. Äußerliche Änderungen gab es keine. In allen Modellen werden nun Intel Core i-Prozessoren verbaut. Im günstigsten Modell ist eine 500 GB-Festplatte verbaut, in allen anderen hat diese eine Kapazität von 1 TB. Im leistungsstärkeren Modell mit 27″-Bildschirm kommt standardmäßig ein Intel Core i5 (Quad-Core) mit 2,8 GHz und eine ATI Radeon 5750 zum Einsatz. Ein Intel Core i7 mit 2,93 GHz ist hier optional gegen Aufpreis erhältlich. In allen anderen Modellen ist ein Core i3 (Dual-Core) mit 3,2 GHz und einer ATI Radeon 4670 (Low-End Modell), bzw. Core i5 (Dual-Core) mit 3,6 GHz und einer ATI Radeon 5670 verbaut. Neu bei diesem Modell ist die Möglichkeit zusätzlich zur konventionellen Festplatte und zum Superdrive eine SSD verbaut zu bekommen, die bei darauf installierten Betriebssystem ein deutlich schnelleres Starten von Programmen und des Rechners selbst bietet.\n\nAm 3. Mai 2011 wurde eine neue Revision des iMacs vorgestellt. Äußerliche Änderungen gab es keine, erneut konzentriert sich die Modellpflege auf die Aktualisierung der eingebauten Komponenten. In allen Modellen werden nun aktuelle Intel Quad-Core i-Prozessoren der Sandy-Bridge-Mikroarchitektur verbaut sowie die Grafikchips auf die derzeitige AMD GPU-Generation aktualisiert. Weiterhin wurde anstelle des Mini DisplayPort Intels universelle Thunderbolt-Schnittstelle verbaut. Nach dem MacBook Pro ist der iMac damit nun die zweite Modellserie von Apple mit diesem Anschluss. Außerdem verfügt der iMac nun über eine integrierte FaceTime-HD-Kamera.\n\nEs wurde der Standard von SATA III (6 GBit/s) eingeführt. Zwei Anschlüsse für HDD/SSD stehen zur Verfügung. Der dritte Datenbus (SATA II – 3 GBit/s) wird für das SuperDrive verwendet.\n\nAm 23. Oktober 2012 stellte Apple Inc. eine neue Generation der iMacs vor. Das kleinste Modell bietet ein 21,5″ (1920×1080) großes Display und eine 1 TB Festplatte, das größte Modell besitzt ein 27″ (2560×1440) Display und wahlweise eine 1 TB oder 3 TB Festplatte, 1 TB oder 3 TB Fusion Drive oder 768 GB Flash-Speicher. Alle Modelle verfügen über IPS-Displays mit LED-Hintergrundbeleuchtung. Der kleinste 21,5\"-iMac verfügt standardmäßig über einen 2,7 GHz Core i5, steigert sich dann auf 2,9 GHz Core i5 mit der Option auf 3,1 GHz Core i7. Der kleinste 27\"-iMac startet bei 2,9 GHz Core i5, das Spitzenmodell verfügt standardmäßig über einen 3,2 GHz Core i5, der im Apple Store zum 3,4 GHz Core-i7-Prozessor von Intel umkonfiguriert werden kann.\n\nAlle iMacs verfügen über vier Steckplätze für den Arbeitsspeicher und werden ab Werk mit 8 GB RAM ausgestattet, der sich im 21,5″-Modell auf 16 GB und im 27″-Modell auf bis zu 32 GB erweitern lässt. Nur bei dem 27″-Modell lässt sich der Arbeitsspeicher ohne Entfernen des Displays nachträglich erweitern. Zudem wurde die Grafikleistung mit dem Nvidia GeForce GT 640M von Nvidia im günstigsten iMac und dem GeForce GT 675MX Grafikprozessor in den anderen Modellen verbessert. Wahlweise kann im 27\" Modell auch eine GeForce GT 680MX mit 2 GB dediziertem Speicher gewählt werden.\n\nStandardmäßig wird der iMac mit einem Wireless Keyboard und einem Eingabegerät (Magic Mouse oder Trackpad) ausgeliefert. Aufgrund des dünneren Designs des Displays wurde auf ein optisches Laufwerk verzichtet.\n\nAm 14. März 2013 erweiterte Apple Inc die aktuelle iMac-Generation um eine neue Version, die über eine VESA-Mount-Befestigungsoption verfügt. Sie werden standardmäßig ohne Standfuß ausgeliefert und sind in den beiden Standardvarianten mit 21,5- und 27-Zoll-Bildschirm, um jeweils 40 Euro Aufpreis gegenüber der Version mit Standfuß erhältlich.\n\nAm 24. September 2013 aktualisierte Apple die iMacs mit neuen WLAN-Karten, die den Standard 802.11ac unterstützen. In allen Modellen befinden sich nun Intel-Core-i5-Prozessoren der Intel-Haswell-Mikroarchitektur (optional auch mit Intel Core i7 erhältlich). Eine ebenfalls optionale SSD oder das Fusion Drive basieren bei den neuen Modellen auf PCIe. Der kleinste 21,5″-iMac verfügt über eine Intel-Iris-Pro-Grafikeinheit, alle anderen Modelle haben eine Nvidia Nvidia-GeForce-Grafikkarte. Das Spitzenmodell mit 21,5″ hat eine Nvidia GeForce GT 750M, das kleinste 27″-Modell eine Nvidia GeForce GT 755M und der größte 27″-iMac wird von einer NVIDIA GeForce GTX 780M mit 4 GB dediziertem GDDR5-Grafikspeicher betrieben. Weiterhin ist OS X Mavericks seit dem 22. Oktober 2013 serienmäßig vorinstalliert und Versionen mit einer VESA-Mount-Befestigungsoption sind erhältlich, welche ohne einen Standfuß ausgeliefert werden.\n\nAm 18. Juni 2014 wurde ein günstigeres Einstiegsmodell vorgestellt, welches ein 21,5 Zoll großes Display, einen schwächeren 1,4-GHz-Intel-Core-i5-Zweikernprozessor, nur eine 500-GB-Festplatte und keine zusätzliche Grafikkarte verbaut hat.\n\nDie 27″-Modelle ohne Retina-Bildschirm wurden aus dem Verkauf genommen. Bis auf das aktualisierte Einstiegsmodell besitzen alle iMacs ein Retina-Bildschirm. Dieses Modell ohne Retina-Bildschirm erhält neue Intel-Prozessoren der 5. Generation und schnelleren Arbeitsspeicher.\n\nDas Einstiegsmodell erhält neue Prozessoren der 7. Generation und DDR4-Arbeitsspeicher.\n\nAm 16. Oktober 2014 präsentierte Apple mit dem neuen iMac-27\"-Retina-5K-Display ein Modell ausgestattet mit einem 5K Retina Screen und verbesserten Hardware-Spezifikationen.\n\nSeit dem 13. Oktober 2015 verkauft Apple den iMac 21,5″ mit einem Retina-4K-Display, das eine Auflösung von 4096x2304 Pixeln hat. Neben dem neuen 4K-iMac hat Apple weitere Veränderungen in der iMac-Reihe vorgenommen und die 27\"-iMacs ohne 5K-Display aus dem Verkauf genommen. Die Hardware der Modelle wurde verbessert. Der Arbeitsspeicher bietet bei allen Modellen eine Taktung von 1867 MHz. Das 27\"-Modell besitzt nun 4 SO-DIMM-Steckplätze und lässt sich somit – wenn auch nicht offiziell von Apple unterstützt – auf 64 GB RAM aufrüsten. Alle 21,5″-Modelle bieten nun Intel-Prozessoren der Broadwell-Generation, alle 27″-Modelle welche der neueren Skylake-Architektur. Die Modelle mit Retina-Display bieten einen 3P-Farbraum, der 25 % mehr Farben als sRGB bietet. Der Einstiegspreis des 5K-Modells wurde auf 2.099 € gesenkt und eine dritte, günstigere Variante hinzugefügt.\n\nDamit einhergehend hat Apple auch die Magic Mouse 2, das Magic Keyboard und das Magic Trackpad 2 vorgestellt. Alle drei erhalten ihren Strom nun nicht mehr über AA-Batterien, sondern über einen integrierten Akku, der seine Energie über den auch beim iPhone und iPad verwendeten Lightning-Anschluss erhält und besitzen Bluetooth 4.2. Dadurch sind alle Geräte insgesamt dünner geworden. Die Magic Mouse erhielt zudem eine überarbeitete Unterseite, welche für flüssigeres Bedienen und mehr Stabilität sorgen soll. Sie ist auch länger und leichter geworden. Die Tasten des Magic Keyboards wurden angepasst und seine Größe um 13 % verringert. Außerdem wurde die Stabilität der Tasten durch einen verbesserten Mechanismus um 33 % erhöht. Die Eingabefläche des Magic Trackpad 2 wurde um 29 % vergrößert. Es erhielt die vom MacBook 2015 und bekannte Funktion „Force Touch“. Bei jedem neuen iMac sind ein Magic Keyboard sowie eine Magic Mouse 2 im Lieferumfang enthalten (Magic Trackpad 2 gegen Aufpreis).\n\nDer iMac 2017 wurde am 5. Juni von Apple auf der WWDC vorgestellt und bietet die 7. Generation der Intel-Core-i5- und i7-Prozessoren (Kaby Lake) mit DDR4-SODIMM-Arbeitsspeicher (2400 MHz) und den neuen Grafikkarten von AMD (AMD Radeon Pro 5-Serie). Bis auf das Einstiegsmodell mit 2,5 GHz und 21,5\" verfügen die iMacs über ein überarbeitetes 4K- und 5K-Display von LG. Die höher auflösenden Displays leuchten nun mit 500 Nit/500 cd (1 Nit/cd = 1 Kerze), bieten den großen P3-Farbraum und 10-Bit-Dithering. Erstmals seit 2012 wurden auch die Anschlüsse leicht verändert und die zwei Thunderbolt-2-Anschlüsse vom Vorgängermodell durch Thunderbolt 3 im USB-C-Format getauscht. Der Arbeitsspeicher des 27\" ist erweiterbar, beim 21,5″-Modell wurde wie beim Vorgänger modularer SODIMM-RAM verbaut, sodass nach Lösen des Displays der Arbeitsspeicher ebenfalls getauscht werden kann. Die CPUs beider Modelle sind theoretisch ersetzbar durch jeden i5 und i7 der 7. Generation (Kaby Lake; Sockel 1151). Nicht alle iMacs aus diesem Jahr haben in der Standardkonfiguration Apples Fusion-Drive-Technologie. Die 2 kleinsten 21,5″-Modelle sind beide jeweils mit einer 1-TB-2,5″-Festplatte mit 5.400 Umdrehungen/Minute ausgestattet. Die höher konfigurierten Geräte haben Fusion Drive 1 oder 2 TB in Verbindung mit 32 GB SSD, die konfigurierbare SSD-Kapazität bei dem 21,5″-iMac ist von 256 GB bis 1 TB, beim 27″-iMac von 1 bis 2 TB. Das vorhandene Mikrofon wurde in der Lautsprecher-Leiste statt wie vorher auf der Rückseite platziert. Auch die beigelegten Eingabegeräte haben sich leicht geändert. Standardmäßig im Lieferumfang enthalten sind Magic Mouse 2 und Magic Keyboard. Zu einem Aufpreis von 30 € bekommt man das Magic Keyboard mit Ziffernblock, weiterhin ist auch das Magic Trackpad 2 für 60 € Aufpreis erhältlich.\n\nDer iMac 2019 wurde am 19. März von Apple in einer aktualisierten Version ohne Event präsentiert. Neu bekommt der iMac 21,5“ einen 6-Core Prozessor der 8. Generation i5 oder Intel Core i7. Der 27“ iMac wurde mit einem 8‑Core Intel Core i9 Prozessor aufgerüstet. Beide Modelle werden neu mit einer Radeon Pro Vega Grafikkarte ausgerüstet.\n\nIm Juni 2017 veröffentlichte Apple, ebenfalls auf der WWDC, ein neues Modell des iMac: Den \"iMac Pro\", der sich besonders an professionelle Anwender richten soll. \n\nIm äußeren Design unterscheidet sich der iMac Pro, bis auf die Farbe in Space Grau, nicht besonders von der Siebten Generation. Jedoch hat Apple die Innenarchitektur des Desktop-Computers deutlich überarbeitet. So kann ein iMac Pro mit bis zu einem 18-Core 2,3 GHz Intel Xeon W Prozessor ausgestattet werden und der Arbeitsspeicher ist erstmals in 128 GB DDR4 ECC Variante mit 2666 MHz erhältlich. Die Slots lassen sich aber auch von Apple oder einem von Apple autorisierten \"Repair Center\" im Nachhinein nachrüsten. Die Festplattenkonfiguration liegt bei 1 TB bis 4 TB, ausschließlich SSD. Die Monitorgröße beträgt 27 Zoll bei einer Anzahl von 5120 x 2880 Pixeln und es lassen sich noch bis zu zwei weitere externe 5K Monitore (mit entsprechenden Adaptern) anschließen. Weitere Neuerungen sind überarbeitete Anschlüsse (10 GB Ethernet Anschluss, SD-Kartenslot mit UHS-II Unterstützung), sowie die Möglichkeit einer Vega 56- oder Vega 64 Radeon Pro Grafik.\n\nDer iMac Pro wurde besonders für die leise Aktivität, die aus der verbesserten Lüftung hervorgeht, gelobt, aber auch für die hohe Performance. Kritisiert wurden die komplizierte Reparatur des iMac Pro, die wenigen Möglichkeiten zur Aufrüstung, sowie der hohe Preis.\n\n\n"}
{"id": "16282", "url": "https://de.wikipedia.org/wiki?curid=16282", "title": "Qt (Bibliothek)", "text": "Qt (Bibliothek)\n\nQt (lies []) ist ein Anwendungsframework und GUI-Toolkit zur plattformübergreifenden Entwicklung von Programmen und grafischen Benutzeroberflächen. Darüber hinaus bietet Qt umfangreiche Funktionen zur Internationalisierung sowie Datenbankfunktionen und XML-Unterstützung an und ist für eine große Zahl an Betriebssystemen bzw. Grafikplattformen wie X11 (Unix-Derivate), macOS, Windows, iOS und Android erhältlich. Qt wird insbesondere vom KDE-Projekt in den Bibliotheken der KDE Plasma Workspaces und der KDE Frameworks verwendet, die gleichzeitig die prominentesten Vorzeigebeispiele darstellen.\n\nQt ist für kommerziell ausgerichtete Programmierung sowohl unter einer proprietären Lizenz, als auch für Open-Source-Programmierung unter der GNU General Public License (GPL) und ab Version 4.5 zusätzlich unter der GNU Lesser General Public License (LGPL) zur Verfügung (siehe Duales Lizenzsystem) nutzbar.\n\nQt ist in C++ entwickelt und verwendet einen Präprozessor, genannt \"moc\" (meta object compiler), womit C++ um zusätzliche Elemente erweitert wird, beispielsweise Signale und Slots sowie Introspektion. Der so erzeugte Quelltext folgt dem C++-Standard, so dass er mit handelsüblichen Compilern übersetzt werden kann. Es gibt auch Anbindungen für andere Programmiersprachen, die größtenteils von der Community bzw. von Drittanbietern zur Verfügung gestellt werden, unter anderem für Python (PyQt, PySide), Ruby (QtRuby), C# (Qyoto-Projekt, QtSharp), Java (Qt Jambi), PHP (PHP-Qt), D (QtD), Haskell (Qtah), Perl (PerlQt), Pascal (Qt4Pas) und Ada (QtAda).\n\nHaavard Nord und Eirik Chambe-Eng begannen mit der Entwicklung von Qt im Jahre 1991. Drei Jahre später gründeten sie das Unternehmen \"Quasar Technologies\", welches später in \"Trolltech\" umbenannt wurde.\n\nDer Name \"Qt\" stammt daher, dass das \"Q\" nach Haavard Nords Geschmack besonders schön in seinem Emacs anzusehen war und das \"t\" an Xt (das X-Toolkit) erinnerte. Ausgesprochen wird \"Qt\" offiziell wie das englische Wort . Dieses Wort soll die Ansicht der Entwickler ausdrücken, dass der Quelltext und die API von Qt eben seien, was auf Deutsch unter anderem so viel wie \"süß\", \"hübsch\" usw., aber auch \"pfiffig\" heißt.\n\nAnfang 2008 wurde \"Trolltech\" von Nokia für 150 Mio. US-Dollar aufgekauft und die Entwicklung in der Sparte Qt Development Frameworks fortgeführt. Im Jahre 2011 hat Nokia das Projekt unter dem Namen Qt-Project als freie Software in die Hände der Open-Source-Community gegeben und das Geschäft mit der kommerziellen Lizenzierung und die Serviceverträge für eine unbekannte Summe an Digia verkauft. Im August 2012 wurden auch die restlichen Teile, wie das Servergeschäft und die Entwicklungsabteilung, für 4 Mio. € an Digia abgegeben. Im August 2014 lagerte Digia die Entwicklung von Qt in ein Tochterunternehmen namens \"The Qt Company\" aus, um die bis dahin getrennt stattfindende Entwicklung der \"Open Source Edition\" und der kommerziellen Editionen wieder zu vereinen.\n\nKontroverse Auseinandersetzungen entstanden um das Jahr 1998, als sich abzeichnete, dass KDE sich als Standard-Desktop unter Linux durchsetzen würde. Da KDE zum großen Teil auf Qt basiert, machte sich ein großer Teil der Open-Source- und Freie-Software-Gemeinde Sorgen, dass ein so wichtiger Teil des Systems unter kommerzieller Kontrolle stand. Das führte zu zwei Entwicklungen: Erstens wurde das Harmony-Toolkit entwickelt, das die Funktionen von Qt exakt kopieren, jedoch unter einer Freie-Software-Lizenz stehen sollte. Zweitens begann die Arbeit an der Desktop-Umgebung Gnome, die ähnliche Funktionen wie KDE bieten sollte, allerdings das freie, in C geschriebene Toolkit GTK+ verwendet.\n\nBis Version 1.45 verwendete Trolltech die FreeQt-License für Qt. Diese war weder eine Open-Source- noch eine freie Lizenz. Es war zwar erlaubt, den Quellcode einzusehen, modifizierte Versionen durften allerdings nicht veröffentlicht werden. Mit dem Erscheinen von Version 2.0 wechselte Trolltech zur Q Public License (QPL), einer Open-Source-Lizenz, die aber von der Free Software Foundation als inkompatibel zur GPL eingestuft wurde. Darauf (im Jahre 1998) wurde die \"KDE Free Qt Foundation\" ins Leben gerufen, um zu verhindern, dass Qt im Falle einer Insolvenz oder Übernahme von Trolltech unter eine restriktivere Lizenz als die QPL fiele. Sie besteht aus jeweils zwei Vertretern von Trolltech und dem KDE e. V., wobei letztere im Zweifelsfall bei Abstimmungen die Mehrheit haben. Falls Trolltech länger als zwölf Monate keine Aktualisierungen (Updates) unter einer Open-Source-Lizenz liefern sollte, ist die „KDE Free Qt Foundation“ berechtigt, die letzte freie Qt-Version unter der BSD-Lizenz freizugeben.\n\nAls der Druck auf Trolltech größer wurde und das Debian-Projekt sich wegen Lizenzproblemen weigerte, KDE zu vertreiben, änderte Trolltech im Jahre 2000 die Lizenz für die Linux-Variante des Toolkits. Ab Version 2.2 gab es fortan die Linux-Variante unter zwei Lizenzen: der GPL und der QPL (siehe Duales Lizenzsystem).\n\nDie Windows-Variante war jedoch nach wie vor ausschließlich unter einer kommerziellen Lizenz verfügbar. Auch die gegen Ende des Jahres 2001 veröffentlichte Mac-OS-X-Variante war nur unter einer kommerziellen Lizenz erhältlich, bis im Juni 2003 Trolltech die Version 3.2 der Mac-OS-X-Variante auch unter die GPL stellte. Im Februar 2005 kündigte Trolltech schließlich an, Qt ab der Version 4.0 auch für die Windows-Plattform unter die GPL stellen zu wollen. Das bestätigte sich, als im Juni 2005 Trolltech ein einheitliches duales Lizenzsystem für alle unterstützten Plattformen veröffentlichte. Heute gibt es für jede Plattform proprietäre und Open-Source-Edition (GPL-Edition).\n\nAb der Version 4.3.1 vom 9. August 2007 räumt Trolltech Ausnahmen bei der durch die GPL lizenzierten Open-Source-Version ein, die es ermöglicht, Programme, die Qt benutzen, unter einer nicht-GPL-kompatiblen Lizenz zu veröffentlichen. Die akzeptierten Lizenzen sind namentlich in einer separaten Liste aufgeführt. Weiterhin muss der Quellcode des Programms des Unternehmens Trolltech im selben Ausmaß zugänglich gemacht werden, wie er auch anderen Benutzern zur Verfügung steht, und die Rechte des Autors, Diskussionen über das Programm zu führen und den Quellcode für jeden zugänglich zu machen, dürfen nicht durch jegliche Rechtsmittel (wie beispielsweise besondere Verträge) beschnitten werden.\n\nSeit Anfang 2008 werden die Versionen von Qt auch unter der dritten Version der GPL veröffentlicht.\n\nIm März 2009 wurde Qt in Version 4.5 erstmals unter der LGPL veröffentlicht. Durch die LGPL ist es möglich, auch ohne eine kostenpflichtige Lizenz proprietäre Software mit Qt zu entwickeln, ohne den Quellcode veröffentlichen zu müssen. Lediglich bei Änderungen am Quellcode von Qt selbst müssen diese Änderungen als Quellcode veröffentlicht werden. Darüber hinaus sind die lizenzbedingten Einschränkungen zu beachten. So darf beispielsweise nur unter strengen Bedingungen statisch gelinkt werden. Auch muss das Framework in unveränderter Form verwendet werden.\n\nSeit September 2014 wurde eine kostengünstige Edition von Qt namens \"Indie Mobile\" angeboten, die es erlaubte, mit Qt erstellte mobile Anwendungen in Verkaufsportalen wie Google Play und Apples App Store zu vertreiben, was aufgrund technischer Einschränkungen und den Nutzungsbedingungen der Verkaufsportale häufig nicht unter den Bedingungen der LGPL möglich ist. Der Quellcode durfte im Gegensatz zu den teureren kommerziellen Editionen aber nur in unveränderter Form verwendet werden. Mit dem Erscheinen von Qt 5.5 im Juli 2015 wurde die \"Indie Mobile\" Edition jedoch eingestellt.\n\nMit Qt 4.0 vom 28. Juni 2005 hat Trolltech fünf neue Techniken eingeführt:\n\nQt 4.1 wurde am 19. Dezember 2005 veröffentlicht und brachte SVG-Tiny-Unterstützung, ein PDF-Backend zum Qt-Drucksystem, und weitere Erweiterungen und Verbesserungen.\n\nQt 4.2 erschien am 4. Oktober 2006 und brachte native CSS-Unterstützung zum Gestalten von Widgets. Auch wurden die QCanvas-Klassen von Qt 3.x zur Darstellung von zweidimensionalen Grafikobjekten durch ein Framework namens QGraphicsView zum Rendern von Grafikobjekten auf dem Bildschirm ersetzt.\n\nSeit dem 30. Mai 2007 steht Version 4.3 zur Verfügung. Sie brachte erweiterte Windows-Vista-Unterstützung, eine verbesserte OpenGL-Engine, sowie die Möglichkeit, SVG-Dateien zu erzeugen. Außerdem wurden eine ECMAScript-Engine namens \"QtScript\" und die Unterstützung von SSL-Verbindungen hinzugefügt.\n\nQt 4.4 wurde am 6. Mai 2008 veröffentlicht. Sie enthält erstmals Unterstützung für Windows CE, verwendet WebKit als HTML-Rendering-Engine und eine verbesserte API zur Programmierung von Anwendungen mit mehreren Threads.\n\nSeit dem 3. März 2009 gibt es mit Qt 4.5 die neue Entwicklungsumgebung \"Qt Creator\", womit Anwendungen für Linux, Mac OS X und Windows ohne zusätzliche Entwicklungswerkzeuge erzeugt werden können.\n\nAm 1. Dezember 2009 wurde Qt in der Version 4.6 veröffentlicht, welche nun auch Multi-Touch und die Plattformen Symbian OS und MeeGo unterstützt.\n\nSeit dem 21. September 2010 ist die Version 4.7 verfügbar. Mit dieser Version hält die deklarative Qt Meta Language oder Qt Modeling Language (QML) Einzug in die Bibliothek.\n\nAm 15. Dezember 2011 wurde Qt in der Version 4.8 veröffentlicht. Die neue Version bietet die \"Qt Platform Abstraction\", Thread-Unterstützung für OpenGL und Multithread-Unterstützung für HTTP sowie einen optimierten Dateisystemzugriff. Diese Version hat Long Term Support (LTS) und wurde hier erst von Version 5.6 LTS in diesem abgelöst.\n\nQt 5.0 wurde am 19. Dezember 2012 veröffentlicht. Die neue Hauptversion bringt u. a. einen OpenGL-basierten Szenengraphen, der das Zeichnen von Oberflächen, die in Qts eigener Programmiersprache \"QML\" – auch \"Qt Meta Language\" oder \"Qt Modeling Language\" genannt – geschrieben sind, beschleunigen soll. Zudem wurde QtQuick um neue Möglichkeiten für grafische Effekte auf Basis von OpenGL sowie ein Canvas-basiertes System zum imperativen Zeichnen in QtQuick erweitert. Für Linux bietet Qt 5 neben dem X Window System auch Unterstützung für das neuere Wayland. Weiterhin bietet die neue Version Funktionen zum Umgang mit JSON und bessere Unterstützung für IPv6. Die in Qt integrierte Web-Rendering-Engine QtWebkit soll nun volle Unterstützung für HTML5 haben. Die wohl wichtigste Änderung ist jedoch die weitere Modularisierung der Bibliothek, die einfachere Handhabung des Codes ermöglichen und einen geringeren Speicherverbrauch für Anwendungen bringen soll, die nur spezielle Teile der Bibliothek verwenden. Vor allem die weitere Modularisierung hat zur Folge, dass Qt 5.0 nicht ABI- und nicht API-kompatibel zu Qt 4.8 ist. Es wurde jedoch versucht, eine möglichst weitreichende API-Kompatibilität zu erreichen, der Sprung von Qt 4 auf Qt 5 stellt also keinen so starken Bruch dar wie der Versionssprung 3 auf 4.\n\nQt 5.1 wurde am 3. Juli 2013 präsentiert. Gegenüber der alten Version sind 3000 Verbesserungen eingeflossen, außerdem gibt es eine vorläufige iOS- und Android-Unterstützung. Deren vollständige Unterstützung wurde in Qt 5.2 fertiggestellt. Qt 5.3 erhielt eine experimentelle Unterstützung für WinRT.\n\nIn Version 5.4 wurde das Modul \"Qt WebEngine\" basierend auf Chromium zur Darstellung von Webseiten hinzugefügt, welches das dazu bisher vorhandene Modul \"Qt Webkit\" auf Basis der WebKit-Engine ablösen soll. Bis mindestens Qt 6.0 sollen jedoch noch beide Engines parallel unterstützt werden. Darüber hinaus wird WinRT seit dieser Version vollständig unterstützt.\n\nIn Version 5.5 wurde die Aufteilung der Editionen überarbeitet.\n\nIn Version 5.6 steht 3 Jahre Long Term Support (LTS) zur Verfügung und löst damit Qt 4.8 LTS ab. High-dpi in User Interfaces und Unterstützung für Windows 10 und OSX 10.11, sowie OpenGL ES 3 sind hier Schwerpunkt der Verbesserungen. Aktuell wird der Umstieg auf 5.9 LTS empfohlen.\n\nDie am 16. Juni 2016 veröffentlichte Version 5.7 baute vollständig auf C++11 auf, zudem wurde die Multithreading-3D-Engine Qt 3D, die davor nur als technische Preview vorhanden war, hinzugefügt.\n\nVersion 5.8, die am 23. Januar 2017 präsentiert wurde, enthielt als wichtigste Änderung eine neue Grafikarchitektur, die das Qt-Quick-Modul von der Grafikbibliothek OpenGL entkoppelte, außerdem wurde das Qt-Lite-Projekt eingeführt. Dabei handelt es sich um ein Projekt, das sich primär auf die Entwicklung für Endgeräte mit geringem Speicher konzentriert.\n\nIn Version 5.9, die am 31. Mai 2017 veröffentlicht wurde, wurde das Qt-Gamepad-Modul eingeführt, das einen direkten Zugriff auf Spielsteuergeräte ermöglichen soll und bisher nur als Preview vorlag. Dazu hat diese Version 3 Jahre Long Term Support (LTS).\n\nMit Qt 5.10 wurde erstmals die Vulkan-API unterstützt, außerdem wurden eine neue Sprach- und Handschrifterkennungen im Qt Virtual Keyboard, Unterstützung für OAuth 1 und OAuth 2 und Text-zu-Sprach-Funktionalität hinzugefügt.\n\nVersion 5.11 wurde am 22. Mai 2018 veröffentlicht, darin wurde die Compiler-Pipeline für die Qt QML Engine, die den Code der Qt Meta-object Language (QML) liest und kompiliert, komplett neu geschrieben, diese Änderung würde, laut den Entwicklern, einen großen Geschwindigkeitsvorteil mit sich bringen, außerdem wurde im Zuge des Releases auch \"Qt for Python\" 5.11 veröffentlicht, dabei handelt es sich um eine Überarbeitung von PySide 2 seitens der Qt Company.\n\nVersion 5.12 wurde am 6. Dezember 2018 veröffentlicht. Es handelt sich dabei um eine Long Term Support (LTS) Version, die drei Jahre lang unterstützt wird.\n\nEs gibt derzeit folgende Varianten des Qt Frameworks:\n\nMit neueren Versionen von Qt wurde der Support für manche Plattformen eingestellt. Dazu gehören:\n\nNachdem Nokia den Qt-Quellcode veröffentlicht hat, sind noch verschiedene Portierungen für folgende Plattformen entstanden, die teilweise noch sehr experimentell sind: OpenSolaris, Haiku, OS/2 eCS platform, AmigaOS 4, HP webOS (Palm Pre), Amazon Kindle DX.\n\nDerzeit gibt es die Editionen \"Qt Open Source\", \"Qt for Application Development\" und \"Qt for Device Creation\". Die \"Qt Open Source\" Edition kann kostenlos unter den Bedingungen der GPL- oder LGPL auch für kommerzielle Zwecke genutzt werden. Für die Nutzung der restlichen Editionen fallen monatliche Gebühren an, dafür werden Käufer dieser Editionen weitere Freiheiten eingeräumt. So können Änderungen an Qt vorgenommen werden, ohne dass diese veröffentlicht werden müssen. Darüber hinaus enthalten die kommerziellen Editionen offizielle Produktunterstützung seitens des Herstellers von Qt.\n\nBis einschließlich Version 4.4 wurde zwischen den Versionen \"Qt Console\" (für die Entwicklung von Programmen ohne GUI), \"Qt Desktop Light\" (es fehlen Netzwerk-, Datenbank- und OpenGL-Unterstützung sowie das Graphics View), \"Qt Desktop\" (die vollständige Bibliothek) und der \"Open Source Edition\" (die vollständige Bibliothek zur Entwicklung von Open-Source-Anwendungen) unterschieden. Diese Unterteilung existiert heute nicht mehr.\n\nDa es sich bei Qt um Freie Software handelt, gibt es auch Editionen anderer Anbieter:\n\nAb Version 4 ist die gesamte Bibliothek in Module gegliedert, wodurch kleinere und schnellere Programme möglich sind, da nur die verwendeten Module eingebunden werden müssen. In Version 5 wurde die Modularisierung weiter vorangetrieben, zudem wird nun zwischen \"Qt Essentials \"(Basiskomponenten) und \"Qt Add-Ons \"(zusätzliche Module für spezielle Einsatzzwecke) unterschieden.\n\nIn \"Qt Essentials\" sind die folgenden Module enthalten:\n\nZu den \"Qt Add-Ons\" gehören unter anderem Module für zusätzliche Bildformate (\"Qt SVG\" / \"Qt Image Formats\"), Module zum Zugriff auf Hardware in Smartphones und Tablets (\"Qt Bluetooth\" / \"Qt NFC\" / \"Qt Sensors\"), sowie das in Version 5.4 vorgestellte Modul \"Qt WebEngine\" zur Darstellung von Webseiten basierend auf der Chromium-Engine.\n\nIn den kommerziellen Editionen sind – je nach Edition – zusätzliche Module enthalten, zum Beispiel zur Visualisierung von Daten (\"Qt Charts\" / \"Qt Data Visualisation\").\n\nDas folgende Beispiel erzeugt ein Fenster mit einem Titel, der aufgrund der geringen Fenstergröße allerdings versteckt ist, einem Beschriftungsfeld (\"Label\") und einer Schaltfläche (\"Button\"). Die Funktionen werden mittels „signals“ und „slots“ verbunden, so dass das Programm bei einem Klick auf den Button beendet wird. Die beiden Widgets werden anschließend im Hintergrund gezeichnet und das Fenster schließlich angezeigt. Das Beispielprogramm wird mit dem Aufruf von codice_1 innerhalb des Ordners kompiliert, in dem sich die Datei befindet.\n\nint main(int argc, char **argv) {\n\nBenutzeroberflächen können mit Qt entweder explizit programmiert oder mit dem \"Qt-Designer\" gestaltet werden.\n\nEine Besonderheit ist die Verwendung von „signals“ und „slots“, die auf einfache Art und Weise die Kommunikation zwischen einzelnen Objekten ermöglichen.\nEin Objekt sendet (emittiert) ein Signal, was zum Aufruf einer Methode eines anderen Objekts führt, wenn diese als Empfänger (Slot) registriert ist. Das Konzept kann als Anwendung des Beobachter-Entwurfsmusters angesehen werden.\n\nAnders als bei der Verwendung von Funktionszeigern bzw. Rückruffunktionen ermöglicht das Konzept von Qt die lose Kopplung zwischen Modulen, d. h. Aufrufer und aufgerufenes Objekt müssen sich nicht zwangsläufig kennen. Dadurch können die beteiligen Objekte vollständig unabhängig voneinander gehalten werden. Ein weiterer Vorteil ist die Gewährleistung von Typsicherheit und Threadsicherheit. Bezüglich der Ausführungsgeschwindigkeit ist der Aufruf einer Methode durch Signale und Slots jedoch geringfügig langsamer als beim Aufruf mittels Rückruffunktion.\n\nVor Qt 5 wurde die Typsicherheit von Aufrufparametern lediglich zur Laufzeit überprüft. Mit der Vorstellung von Qt 5 wurde eine alternative Methode vorgestellt, die dies nun bereits zur Kompilierzeit vermag. Die aus dem C++11-Standard bekannten Lambda-Ausdrücke können nun auch als Slots verwendet werden.\n// Die alte connect Methode\nQObject::connect(button, SIGNAL(clicked()), this, SLOT(pushButtonClicked());\n\n// Die neue Variante\nQObject::connect(button, &QPushButton::clicked, this, &ExampleObject::pushButtonClicked, Qt::AutoConnection);\n\n// Verwendung einer Lambda-Funktion (C++11) als Slot\nQObject::connect(sender, &QObject::destroyed, [=](){ ... });\nBeachtung verdient der letzte Aufrufparameter des Beispiels „neue Variante“. Durch die optionale Angabe des Verbindungstyps lässt sich die threadübergreifende Kommunikation beeinflussen.\nStandardmäßig wird AutoConnection verwendet. Befinden sich Sender und Empfänger im gleichen Thread, verhält sich diese Variante wie DirectConnection. Andernfalls wird QueuedConnection verwendet. Diese dynamische Entscheidung findet auch bei UniqueConnection statt, jedoch ist zu einem Signal lediglich ein Slot erlaubt. BlockingQueuedConnection wartet auf die vollständige Abarbeitung des Slots im anderen Thread und kann daher, wenn sich beide im gleichen Thread befinden, zur Deadlock-Situation führen.\n\nQt verwendete bis zur neuesten Version eine eigene Zeichenengine sowie Steuerelemente. Es simulierte das unterschiedliche Aussehen auf den verschiedenen Plattformen (GUI-Simulation). Das machte das Portieren einfacher, da nur sehr wenige Klassen in Qt von der verwendeten Plattform abhängig waren. Der Nachteil bestand allerdings darin, dass Qt das Aussehen jeder Plattform präzise nachahmen musste, was nicht immer möglich war. Seit Qt 4.0 gibt es allerdings die Möglichkeit (wie auch in vielen anderen Toolkits, z. B. wxWidgets oder SWT), die betriebssystemeigenen Routinen zum Zeichnen der Elemente zu verwenden. So gibt es nun die nativen QWindowsVistaStyle, QWindowsXPStyle und den QMacStyle. Diese „Styles“ funktionieren nur auf dem passenden Betriebssystem (und sind dort auch der Standard). Es gibt nach wie vor aber auch plattformunabhängige „Styles“, diese lassen sich einfach (z. B. per Kommandozeile) aktivieren. Ab Version 5.0 wurde ein einheitliches Fallbackstyle namens Fusion eingeführt.\n\nZusätzlich zu den Bibliotheken enthält Qt noch eine Reihe weiterer zusätzlicher Hilfsprogramme, von denen einige unersetzlich für die Arbeit mit Qt sind.\n\n\n\n\nFür die Benutzung der Online-Dokumentation wird der \"Qt-Assistant\" verwendet, der auch in eigene Projekte eingebunden werden kann.\n\nDer \"Meta-Object-Compiler\", auch bekannt als \"moc\", ist ein Werkzeug, welches die Header-Dateien eines Projektes untersucht und Meta-Informationen über Klassen und sonstige Programmteile sammelt. Das geschieht mit Hilfe von „Markierungen“, welche später vom Standard-Präprozessor entfernt werden. Aus diesen Informationen erstellt er wiederum C++-Code, in dem Funktionen implementiert werden, die ohne weitere Bibliotheken in C++ nicht vorhanden sind, wie Introspektion und das Signal-Slot-Konzept. Weitere Makros werden in selbst geschriebenen Header-Dateien expandiert, um deren Funktionsdeklarationen bereitzustellen.\n\nDer Gebrauch eines zusätzlichen Werkzeuges wurde von einem Teil der C++-Programmierer kritisiert. Sie behaupteten, dass Qt-Programmierung keine C++-Programmierung mehr sei. Tatsächlich basiert die Implementation auf C-Makros, die bekanntlich nicht typsicher sind und den Namensraum verunreinigen. Aus der Sicht von Trolltech ist das aber nötig, um das Signal-Slot-Konzept elegant zu implementieren. Als Qt 1.x veröffentlicht wurde, waren die Compilerunterschiede bezüglich generischer Programmierung noch zu groß, als dass man sich auf Vorlagenklassen (Templates) hätte verlassen können. Auch lässt sich die Verunreinigung des Namensraumes durch Verwendung des Makros QT_NO_KEYWORDS vermeiden.\n\nDie folgende Tabelle zeigt die Anbindungen für verschiedene Programmiersprachen:\n\nDie KDE Plasma Workspaces können als größtes und prominentestes Softwareprojekt, das auf Qt basiert, gelten. Daneben findet die Bibliothek aber auch in anderen Anwendungsgebieten breite Verwendung.\n\n\n\n\n\n"}
{"id": "16283", "url": "https://de.wikipedia.org/wiki?curid=16283", "title": "KDevelop", "text": "KDevelop\n\nKDevelop ist eine freie grafische Entwicklungsumgebung für Unix-basierte Betriebssysteme und Microsoft Windows. Der Fokus liegt auf der Unterstützung von C++ als Programmiersprache, einige weitere Sprachen werden aber durch Zusatzmodule („Plugins“) ebenfalls unterstützt.\n\nDas KDevelop-Projekt wurde 1998 am Institut für Informatik an der Universität Potsdam begonnen, um eine einfach zu bedienende integrierte Entwicklungsumgebung für C++ und C auf Unix-basierten Betriebssystemen bereitzustellen. Seit damals ist die KDevelop-IDE öffentlich unter der GPL erhältlich und unterstützt u. a. Qt-, KDE-, Gnome-, C++- und C-Projekte.\n\nAb der Version 3.4 beschränkte sich KDevelop auf eine neue grafische Oberfläche namens Simple-IDEAl, konnte freie und kommerzielle Module (Plug-Ins) einbinden und beherrschte viele neue Programmiersprachen.\n\nDie komplett überarbeitete 4.x Entwicklungslinie setzt auf KDE Platform 4 auf.\n\nDie 5.x Entwicklungslinie verwendet KDE Frameworks und Qt 5 und ist eine kontinuierliche Weiterentwicklung von KDevelop 4.\n\nFür unterstützte Sprachen besitzt KDevelop die Fähigkeit, den vom Benutzer geschriebenen Code in (nahezu) Echtzeit im Hintergrund zu analysieren. Mit den so gesammelten Informationen wird dann eine Vielzahl von Funktionen angeboten, darunter zum Beispiel:\n\nWeitere nennenswerte Funktionen, die nicht direkt zur Sprachunterstützung gehören, sind zum Beispiel\n\nFür die unterstützen Sprachen wird beim Laden eines Projekts, oder sobald der Benutzer im Editor-Fenster eine Änderung durchführt, der Code in den betroffenen Dateien (neu) analysiert. Dabei wird eine abstrakte Repräsentation des Programmcodes (genannt \"definition-use-chain\", oder kurz \"DUChain\") erzeugt, welche zum Beispiel Informationen darüber enthält, wo Variablen und Funktionen deklariert werden und welche Typen diese besitzen. Viele der sprachbezogenen Funktionen (zum Beispiel Code-Hervorhebung, Schnellöffner, Liste von Klassen und Funktionen, …) werden von der IDE mithilfe dieser Informationen zur Verfügung gestellt; oft völlig ohne zusätzlichen Code im Plugin für die betreffende Sprache. Für andere Funktionen, deren Verallgemeinerung nicht in diesem Umfang möglich ist – wie zum Beispiel Code-Vervollständigung und Vorschläge („tool tips“) – steht eine umfassende API zur Verfügung, welche das Einbinden neuer Sprachen erleichtert.\n\nKDevelop implementiert keinen eigenen Texteditor, sondern benutzt KatePart durch das KParts-Framework. Die in KatePart selbst, oder als Plugins für KatePart implementierten Funktionen, zum Beispiel der vi-Eingabemodus stehen somit auch in KDevelop zur Verfügung.\n\nKDevelop ab Version 4 basiert auf KDevPlatform, welches umfangreiche APIs zum Beispiel zur statischen Codeanalyse, zur Einbindung von Versionskontrollsystemen, oder zum Einbinden grafischer Debugger zur Verfügung stellt. KDevPlatform wird von denselben Entwicklern verwaltet wie KDevelop; die Trennung dient der besseren Wiederverwendbarkeit. KDevelop selbst ist im Wesentlichen eine Sammlung von Plugins für KDevPlatform. Die Entwicklungsumgebung KDevelop wird dementsprechend für die Verwendung vieler Plugins nicht zwingend benötigt. Es ist also möglich, eine andere auf KDevPlatform basierende Anwendung zu erstellen, welche dann dieselben Plugins benutzt, um zum Beispiel Integration von Versionskontrolle oder Code-Hervorhebung anzubieten. Eine solche Anwendung war die für Web-Entwicklung optimierte IDE Quanta, deren Entwicklung aber Ende 2012 eingestellt wurde.\n\nSprachunterstützung für C++, CMake und JavaScript/QML ist fester Bestandteil von KDevelop 4 und 5. Unterstützung für PHP und Python ist in Form zusätzlicher Plugins verfügbar. An der Unterstützung weiterer Sprachen, zum Beispiel Ruby und Go, wird gearbeitet. Für alle anderen Sprachen stehen lediglich die Grundfunktionen, wie einfache Syntax-Hervorhebung und textbasierte Code-Vervollständigung zur Verfügung. Für das Entwickeln von C++-Projekten, die die Bibliothek Qt verwenden, sind einige zusätzliche Funktionen vorhanden, zum Beispiel Unterstützung für das von Qt genutzte Signal-Slot-Konzept.\n\nKDevelop 3 unterstützte die Sprachen Ada, Bash, C, C#, C++, D, Fortran, Haskell, Java, Objective-C, Pascal, Perl, PHP, Python, Ruby, SQL und XUL.\n\n\n"}
{"id": "16467", "url": "https://de.wikipedia.org/wiki?curid=16467", "title": "Logikus", "text": "Logikus\n\nDer Lerncomputer Logikus wurde 1968 vom Kosmos-Lehrmittelverlag, Stuttgart, herausgebracht. Mit dem sehr einfachen „Computer“ konnten auf einem Programmierfeld Logik-Schaltungen für \"und\", \"oder\" und \"nicht\" durch Drahtbrücken aufgebaut werden. 10 Schaltschieber und eine Taste steuerten entsprechend dieser Programmierung bis zu 10 Glühlämpchen, vor die eine Transparentpapier-Schablone mit Symbolen zur einfacheren Interpretation der Programmabläufe angebracht werden konnte.\n\nVom einfachen Leuchtband bis zur Mengenlehre, Schaltalgebra und Booleschen Algebra wurden im Begleitbuch und im Fortsetzungsband \"Wir programmieren weiter\" viele Programmierbeispiele vorgestellt, welche die Grundlagen der Computertechnik erläuterten. Bemerkenswert ist der Verzicht auf jegliche elektronische Bauteile, die Funktionalität wird ausschließlich über Schiebeschalter, die Steckplatine und Lämpchen erreicht.\n\nDas Gerät erzielt immer noch respektable Preise auf Auktionen. Gelegentlich werden auch ungebaute Logikus-Experimentierkästen im Neuzustand sowie die Ergänzungspackung mit dem oben genannten Fortsetzungsband angeboten. In den USA erschien damals eine lizenzierte Variante unter der Bezeichnung \"LOGIX\". Ein dem Logikus bauähnliches, jedoch durchaus eigenständiges Gerät wurde in der DDR unter dem Namen \"Piko dat\" produziert.\n\n"}
{"id": "16472", "url": "https://de.wikipedia.org/wiki?curid=16472", "title": "Grafiktablett", "text": "Grafiktablett\n\nEin Grafiktablett (auch Digitalisiertablett, Digitizer, Pen Tablet) ist ein Zeigegerät für Computereingaben. Die Spitze eines speziellen Zeigewerkzeuges, meistens ein \"Stylus\" genannter Stift oder eine puckförmige Computermaus mit Fadenkreuz, wird auf dem Tablett bewegt. Vom Zeigegerät gehen Impulse aus, über die das Tablett die Information über die Koordinaten, den Stiftdruck und zusätzlich gedrückte Tasten an der jeweiligen Position erhält. Diesen Vorgang Daten über das Verfolgen des Stiftes oder durch Druck zu erfassen, nennt man auch Digitalisierung. Bei fortgeschrittenen Modellen können weitere Informationen wie Stiftneigung, Stiftdrehung, Fingerdruck oder mehrere Werkzeuge erkannt werden. Im Gegensatz zu einem Touchscreen, den es auch für Stiftbedienung gibt, kann auf einem Grafiktablett nichts vom Computer dargestellt werden, es ist ein reines Eingabegerät.\n\nJe nach System und Hersteller kommen zur Positionserkennung unterschiedliche Techniken zu Anwendung – es ist dabei auch eine Kombination dieser Techniken möglich.\n\nBei kapazitiven Systemen wird durch eine Veränderung der elektrischen Kapazität in bestimmten Bereichen der Oberfläche eine Positionsbestimmung ermöglicht. Bei resistiven Tabletts erfolgt die Positionsbestimmung durch Druck auf die Oberfläche, welche eine Veränderung des elektrischen Widerstand in bestimmten Bereichen der Oberfläche auslöst und so eine Positionsbestimmung erlaubt. Eine bei Grafiktabletts verbreitete Methode ist das im Folgenden näher dargestellte induktive Übertragungssystem mit folgenden Eigenschaften:\n\n\nDas induktive System basierend auf RFID-Technik vermeidet Fehlerkennung wie beispielsweise durch die Hand oder Finger welche sich im Bereich oder auf dem Tablett befinden. Der wesentliche Unterschied aus technischer Sicht zu einem RFID-System besteht dabei in der Fähigkeit zur genauen Positionsbestimmung des Stiftes, eine Eigenschaft, die bei RFID-Transponder nicht notwendig ist.\n\nBei der induktiven Positionsbestimmung befinden sich im Tablett unter der Oberfläche eine Sensorleiterplatte mit mehreren, horizontal und vertikal ausgeführten Leiterbahnen, wie in nebenstehender Abbildung dargestellt. Dabei werden die vertikalen Leiterbahnen auf einer Seite geführt, die horizontalen Leiterbahnen auf der gegenüberliegenden Seite. Diese Leiterbahnen sind zu Gruppen von mehreren 10, durch die Elektronik des Tablett umschaltbaren Spulen geschaltet. Die Spulen sind auf der Position auf der Leiterplatte exakt bekannt und durch zyklische Umschaltung kurzer Wechselstromsignale lässt sich in der so gebildeten Anordnung die Position des Stiftes bestimmen.\n\nAlternativ zum Stift kann auch ein sogenannter \"Puck\" verwendet werden, der wie eine Computermaus über das Tablett bewegt wird und mit Hilfe eines Fadenkreuzes genaueres Zielen auf dem Tablett und damit die Digitalisierung von Vorlagen ermöglicht. Hierbei ist die Spule meist sichtbar um das Fadenkreuz herum gelegt. Einige Hersteller bieten verschiedene Stiftspitzen für eine an das simulierte Werkzeug angepasste haptische Wahrnehmung an, bis hin zu Werkzeugen, die einer Airbrush nachempfunden sind.\n\nWerden unterschiedliche Werkzeuge angeboten, so wird häufig automatisch ein Identifikationssignal des Werkzeugs mit an das Tablett übertragen, was das Umschalten des Werkzeugs per Hand erspart und individuelle Einstellungen für jeden Stift ermöglicht, ganz wie bei klassischen Zeichenwerkzeugen.\n\nDie meisten Grafiktabletts verfügen außerdem über Tastenfelder neben der kontaktsensitiven Fläche, teilweise auch virtuelle Tastenfelder am Rande der Fläche, um schnell auf Programmfunktionen zurückgreifen oder Einstellungen des aktuellen Werkzeugs ändern zu können.\n\nDas Steuern des Cursors durch einen Stift bringt einige Vorteile mit sich:\n\n\nGrafiktabletts oder deren Stifte können die Druckintensität auswerten, um zum Beispiel in einer Grafiksoftware Pinselgröße oder Deckkraft zu steuern (siehe Bild) sowie das Auf- und Abtragen von Material bzw. dessen Verformung im dreidimensionalen Raum zu steuern.\n\nAnders als bei der PC-Maus, die nur relative Bewegungen erkennt, hat man beim Grafiktablett absolute Koordinaten, wobei ungefähr jeder Punkt auf dem Tablett einem Punkt auf dem Bildschirm entspricht. Bei der Maus hingegen wird der Mauszeiger jeweils von seiner aktuellen Position „weiter geschoben“.\n\nBeispiel: Will man einen Punkt auf dem Bildschirm auswählen, muss man die Maus zunächst dorthin schieben. Bei einem Grafiktablett hingegen „drückt“ man direkt auf die entsprechende Stelle. Dabei wird der Zeiger sofort platziert, wenn sich der Stift ohne Druck auf dem Tablett befindet oder in kurzer Entfernung drüber schwebt. Etwas mehr Druck setzt dann einen Klick um.\n\nIm professionellen Einsatz ermöglicht dies das exakte Nachführen der Striche von Papiervorlagen, um Zeichnungen wie beispielsweise Baupläne zu digitalisieren. Eine solche Arbeit ist mit der Maus überhaupt nicht möglich.\n\n\n\n\n\n\nGrafiktabletts haben teilweise Tätigkeiten (z. B. Malen, Zeichnen am PC, etc.) erst möglich gemacht und sind in folgenden Bereichen ein bedeutendes Hilfsmittel geworden:\n\n\n\nGrundsätzlich kann man mit einem Grafiktablett jede Art von Software mit grafischer Benutzeroberfläche bedienen, die sich auch mit einer Maus bedienen lässt, doch folgende Anwendungen beispielsweise nutzen die technischen Möglichkeiten entsprechend aus:\n\nFür Tablet-PCs wurden von Microsoft-Windows-Betriebssystemen entsprechende Varianten entwickelt, die auf ein tastaturloses Arbeiten mit dem Stift optimiert sind. Es ist eine Handschrifterkennung integriert, die auch biomechanische Eigenschaften wie Schreibdruck und Stiftneigung auswertet (sofern vorhanden), allgemein wurden die Betriebssysteme durch verschiedene Besonderheiten auf die Bedienung ohne die sonst übliche Tastatur angepasst. Software, die auf die Bedienung mit einem Stift abgestimmt ist, gibt es auch von anderen Herstellern. Für Apple Macintosh wurde in das Betriebssystem Mac OS X die Handschrifterkennung Inkwell integriert.\n"}
{"id": "16719", "url": "https://de.wikipedia.org/wiki?curid=16719", "title": "Alt (Taste)", "text": "Alt (Taste)\n\nDie Alt-Taste ist eine Taste auf PC-Tastaturen und befindet sich meistens links neben der Leertaste. Die Bezeichnung \"Alt\" entstammt dem Englischen und steht als Kurzform für \"alternate\" („wechseln“); im Deutschen kann die Abkürzung auch als „Alternative“ verstanden werden. Mit der Alt-Taste lässt sich bei gleichzeitigem Drücken einer weiteren Taste eine zusätzliche Funktion jener anderen Taste aktivieren.\n\nDie Alt-Taste ähnelt in ihrer Verwendungsweise der Umschalttaste (auch: Hochstelltaste oder Shift-Taste von englisch: \"shift\" „die Schicht, Ebene wechseln“) und ermöglicht neben dieser und der Strg-Taste eine zusätzliche Ebene der Tastaturbelegung.\n\nDie Alt-Taste wird üblicherweise nicht allein betätigt, sondern wird nur zusammen mit einer anderen Taste wirksam. Ausnahmen davon finden sich unter anderem in Computerspielen oder zum Aktivieren der Menüleiste bei CUA-konformen Systemen wie zum Beispiel Windows oder OS/2.\nAuf Unix-Systemen mit X11 ist Alt als \"Meta-Taste\" belegt und löst meist den Modifier \"Mod1\" aus, der wie unter Windows Menüeinträge und häufig gebrauchte Befehle auslöst. Unter OS X entspricht die Alt-Taste der Options- oder Wahltaste, mittels derer in die Dritt-Belegung umgeschaltet wird.\n\nDie englische PC-Tastatur hat eine zweite Alt-Taste auf der rechten Seite der Leertaste. Bei den meisten nichtenglischen Tastaturen, so auch der deutschen, wurde diese in eine Alt-Gr-Taste \"(alternate graphic)\" umgewidmet, um die für die meisten nichtenglischen Sprachen notwendige höhere Zahl von Zeichen mit der gleichen Anzahl vorhandener Tasten eingeben zu können.\n\nBekannte Kombinationen mit der Alt-Taste in verbreiteten Betriebssystemen sind folgende:\nBei den drei oben genannten Funktionen verhält sich die Strg-Taste analog für MDI-Unterfenster.\n\n\nUnter Windows (und vorher MS-DOS) ist es möglich, durch Drücken der Alt-Taste und gleichzeitige Eingabe einer Zahl auf dem Nummernblock Zeichen einzugeben, auch solche, die auf dem aktuell gewählten Tastaturlayout gegebenenfalls nicht zur Verfügung stehen. Diese Technik nennt sich Alt-Numpad-Eingabemethode oder Alt-Code.\n\nWenn man ein Zeichen wie beispielsweise das Copyright-Zeichen „©“ eingeben möchte, das nicht auf der Tastatur zur Verfügung steht, im normalen Zeichensatz aber durchaus vorhanden ist, sucht man zunächst nach seiner Codenummer. Da bei Windows in Deutschland meist die Codepage 850 verwendet wird, findet man über die Zeichentabelle dafür die Nummer 184 (dezimal, hex 0xB8). Für das „©“ gibt man also bei gedrückter Alt-Taste im Nummernblock der Tastatur die Zahl 184 ein und lässt dann die Alt-Taste los, das Zeichen erscheint.\n\nAuf diese Weise lassen sich alle Zeichen der entsprechend gemappten Codepages schreiben – etwa Windows-1252 (mit vorangestellter 0 für „ANSI“-Codepages unter einer graphischen Oberfläche) und Codepage 850 (ohne vorangestellte 0 für OEM-Codepages ohne grafische Oberfläche wie in DOS-Boxen oder unter purem DOS). Gängige Zeichen sind diesbezüglich Währungssymbole und akzentuierte oder andere Sonderzeichen von Einzelsprachen wie etwa das ß des Deutschen oder das ç des Französischen.\n\nIn den meisten Programmen können nur Codezahlen von 0 bis 255 eingegeben werden. Gibt man größere ein, wird der Modulo 256 davon genommen, also der Rest bei Division durch 256. Einige Programme (nicht zuletzt die Zeichentabelle selbst) erlauben aber die Eingabe von sämtlichen Unicode-Zeichen auf diese Art und Weise, wobei hier eine sonst allenfalls notwendige vorangestellte 0 nicht erforderlich ist.\n\n"}
{"id": "17001", "url": "https://de.wikipedia.org/wiki?curid=17001", "title": "Macintosh-Modelle", "text": "Macintosh-Modelle\n\nListe von Apple Macintosh-Modellen sortiert nach CPU-Typ:\n\nDie Bezeichnung der einzelnen Modelle liefert Anhaltspunkte zur verbauten Hardware und zur Gehäuseform.\n\nDreistellige Nummern (z. B. LC 475) verweisen auf einen 68k-Prozessor, vierstellige Nummern (z. B. PPC 6100) auf einen PowerPC-Prozessor. Die jeweils erste Ziffer kennzeichnet die Bauform:\nDie zweite Ziffer lässt Rückschlüsse auf die verwendete Hauptplatine und den Prozessor zu, die dritte Ziffer gibt Hinweise auf eventuell installierte Zusatzhardware.\n\nAus Software-Sicht kann das Modell ab System 6.0.4 aus dem Rückgabewert der Funktion codice_1 ermittelt werden, das dem Benutzer auch als „“ im angezeigt wird. So hat der Macintosh 128K und 512K den Typ () codice_2, der Macintosh XL die ID codice_3, usw. Erst mit den „New World“-Macs ab 1997 ist die Gestalt-ID immer codice_4, obwohl es danach noch weitere „Old World“-Modelle gab, etwa die Power-Macintosh-G3-Desktop-Modelle mit Gestalt-Typ codice_5. Unter Mac OS X wurde die Funktion in das Carbon-Framework übernommen und bis Mac OS X Lion (10.7, 2011) unterstützt. Innerhalb der Blue Box, ursprünglich ein virtualisiertes Mac OS 8 unter Rhapsody (ab 1997), ist die „Machine ID“ mittels codice_1 immer codice_7.\n\nMit Einführung des G3 wurde bei Apple die Build-to-Order-Option eingeführt. Der Kunde konnte sich dabei sein Modell selbst zusammenstellen. Die alte Namenskonvention wurde zugunsten von Modellnummern und aufgegeben.\n\nDie Modellnummer muss allerdings nicht eindeutig sein. Z. B. steht Modellnummer A1176 für alle Mac mini mit Intel-Core-Prozessor (die ersten zwei Modellreihen nach der Umstellung von PowerPC auf Intel) von 2006 und 2007, verkauft bis 2009.\n\nEbenso ist der , übersetzt mit „Modell-Identifikation“, nicht eindeutig, beschreibt jedoch oft größere Unterschiede bei der Hardware. Z. B. steht codice_8 für die Modelle mit Core-Solo/Core-Duo-Prozessor (Mac mini der 2. Generation), während für das Nachfolgemodell mit Core-2-Duo-Prozessor codice_9 verwendet wird (Mac mini der 3. Generation).\n\nEine weitere Nummer, die zur Identifikation des Macintosh-Modells dienen kann, ist die , wobei für \"\" steht und die elektromagnetischen Eigenschaften des Gerätes im Bezug auf andere elektronische Geräte in dessen Nähe beschreiben soll. Z. B. ist bei allen Mac minis mit der Modellnummer A1176 die EMC-Nummer 2108. Bei anderen Modellen, z. B. den Power-Mac-G5-Modellen mit Modellnummer A1047, differiert die EMC-Nummer mit jeder Serie: alle Power Mac G5 von 2003 mit codice_10 weisen die EMC-Nummer 1969 auf, während jene von 2004 mit codice_11 die EMC-Nummer 1969C und jene von 2005 mit demselben codice_11 die EMC-Nummer 2061 tragen.\n\nMit diversen Programmen, etwa dem bzw. Systeminformation, kann neben der für jedes einzelne Gerät individuellen Seriennummer auch der ausgelesen werden, nicht jedoch die Modellnummer oder die EMC-Nummer. Auf den meisten Macinstosh-Computern ist an einer unauffälligen Stelle ein Aufkleber angebracht, auf dem die Modellnummer, die EMC-Nummer sowie meistens die individuelle Build-to-Order-Konfiguration und oft auch die Seriennummer und die MAC-Adressen aufgedruckt sind.\n\nDie jeweils leistungsstärksten 68k-Modelle ihrer Zeit, die vor allem im professionellen Bereich genutzt waren:\n\n\n\n\n\nDer von IBM und Motorola entwickelte PowerPC 600 wurde im Rahmen der Apple-IBM-Motorola-Allianz (AIM-Allianz) 1992 gestartet und war als PowerPC 601 ab 1993 verfügbar. Der Prozessor war für Personal Computer als 32-Bit-Einkernprozessor aus IBMs POWER-Serie hervorgegangen. Die Weiterentwicklung PowerPC 603 gilt als die zweite Generation und ist der direkte Vorgänger des PowerPC G3 750. Die Varianten 604 und 604e waren in der Gleitkommaeinheit beschleunigt und im L1-Cache vergrößert, sodass der 604 im direkten Vergleich zum 601 bei gleicher Taktung etwa eineinhalb Mal so schnell arbeitet.\n\nEine ab Version 7.1.2 in Mac OS integrierte transparente Emulation (nicht zu verwechseln mit Rosetta) macht m68k-Maschinencode auch auf darauffolgenden Mac OS-Versionen weiter ausführbar. In Mac OS X ist bis Version 10.4 die Classic-Umgebung integriert, so dass eine Vielzahl an Anwendungen, die ursprünglich für den Motorola 68000 geschrieben und übersetzt wurden, auf allen PowerPC-basierten Macs laufen.\n\n\n\n\n\n\nDer von IBM und Motorola gemeinsam entwickelte PowerPC 750 sollte in der dritten Generation einige der Unzulänglichkeiten des Vorgängers PowerPC 600 aufheben. Er zeichnet sich durch in der damaligen Zeit im Vergleich sehr geringen Energieverbrauch bei etwa gleicher Leistung aus.\n\n\n\n\nDer von Motorola entwickelte PowerPC 7400 entstammt der vierten Generation der Entwicklung. Die Neuerung gegenüber dem Vorgänger G3 ist vor allem dessen 128-Bit-Vektoreinheit AltiVec, die von Apple unter dem Namen vermarktet wurde. Außerdem verwendet der G4 eine schnellere und mit 64-Bit doppelt so breite Gleitkommaeinheit, die aus dem PowerPC 604/604e der zweiten Generation abgeleitet wurde.\n\n\n\n\n\n\n\n\n\n\n\n\nDer von IBM aus dem POWER4+ entwickelte PowerPC 970 stellt die fünfte Generation der Entwicklung dar. Die Neuerung gegenüber dem Vorgänger G4 von Motorola ist dessen 64-Bit-Architektur mit voller 32-Bit-Kompatibilität und der SIMD-Einheit VMX, das IBM-Äquivalent zu Motorolas AltiVec. Die verbesserte Variante 970FX erzeugt weniger Verlustleistung. Mit dem 970MP wurde der erste PowerPC-Doppelkernprozessor eingeführt.\n\nAufgrund seines relativ hohen Strombedarfs und des erforderlichen Kühlaufwands wurde der G5 nie in Notebooks verwendet.\n\n\n\n\nAuf der WWDC 2005 kündigte Steve Jobs den Schwenk zu Intel-Prozessoren an. Bereits im Januar 2006 waren die ersten Intel-basierten Mac-Computer verfügbar. Wie schon bei der Umstellung von m68k- auf PowerPC-Prozessoren kümmert sich in den ersten drei Intel-Versionen von Mac OS X (Tiger 10.4, Leopard 10.5 und Snow Leopard 10.6) die transparente Software-Emulation Rosetta darum, dass für PowerPC-Macs geschriebene Programme auf Intel-Macs ausführbar bleiben.\n\nMit den Prozessoren der IA-32-Architektur ist es möglich, diverse x86-Betriebssysteme nativ auszuführen, sofern diese EFI unterstützen. Apple selbst erleichtert diese Möglichkeit durch das in Mac OS X integrierte Programm Boot Camp für das Betriebssystem Microsoft Windows, obwohl dieses in einigen unterstützten Versionen (Windows XP und 32-Bit-Versionen von Windows Vista und 7) EFI selbst nicht unterstützt. Zusätzlich werden von Apple für diese Windows-Betriebssysteme passende Treiber bereitgestellt, etwa für die Apple-spezifischen Tastaturen und Touchpads.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "17107", "url": "https://de.wikipedia.org/wiki?curid=17107", "title": "Computerzeitschrift", "text": "Computerzeitschrift\n\nEine Computerzeitschrift ist eine Zeitschrift, die sich in der Regel mit Themen zu Hard- und Software von Computern beschäftigt. Sie enthält aber auch Artikel über verwandte technische Themen wie beispielsweise Digitalfotografie, Multimedia oder über digitale Daten betreffende politische Diskussionen. Daneben gibt es spezialisierte Zeitschriften über das Internet. Die ersten Computerzeitschriften sind als Beilagen zu Elektronik-Zeitschriften entstanden.\n\nIm Jahr 2006 wurden erstmals mehr Exemplare von Computer-Zeitschriften unverkäuflich remittiert (also von den Händlern zurückgegeben und im Normalfall anschließend vernichtet) als tatsächlich durch den Einzelhandel verkauft (Quelle: IVW). Seit 2001 ist die Einzelverkaufsauflage damit um über 2 Millionen Exemplare gesunken, was einem Verlust von 40 % entspricht. Im gleichen Zeitraum konnte der Anteil unverkäuflicher Zeitschriften um immerhin 12 % reduziert werden.\nVon 2011 bis 2016 ist die verkaufte Auflage in Deutschland um 60 % zurückgegangen und lag unter 1,2 Mio. Exemplaren, die allein der Marktführer Computerbild im Jahr 1999 verkauft hat.\n\nEine Haupterlösquelle für die Computerzeitschriften sind die werblichen Inserate von Händlern, Herstellern und Serviceunternehmen. Händler inserieren zunehmend in Preisvergleichsseiten sowie direkt in den Suchmaschinen wie Google, Yahoo und MSN. Die Hersteller konzentrieren sich auf interaktive Werbeformen wie Webcasts und Whitepaper-Integrationen, ihre Hausmessen und andere Branchenevents.\n\nDie werbetreibende Industrie reagiert auf diesen lang anhaltenden Negativtrend und buchte bereits 2005 und 2006 Zeitschriften nur noch verhalten. Dieser Effekt setzt sich auch 2007 fort, wie der Verband der Zeitschriftenverleger dokumentierte: „Für die Zeitschriften begann das Werbejahr 2007 sehr zurückhaltend. Im Vergleich zum Vorjahr bilanzierten die Publikumszeitschriften im Januar 2007 bei den Anzeigenseiten ein Minus von 4,5 Prozent. Das geht aus der am Freitag veröffentlichten Zentralen Anzeigenstatistik (ZAS) des VDZ Verband Deutscher Zeitschriftenverleger hervor.“\n\nAufgrund der Überzahl von diesbezüglichen Zeitschriften werden hier nur Magazine mit eigenem Wikipedia-Artikel aufgelistet.\n\nDas Marktsegment ist sowohl im Vertriebsmarkt als auch im Anzeigenmarkt einer ständig zunehmenden Segmentierung unterworfen und muss sich mit stetig steigenden Druckkosten, Vertriebskosten und sinkenden Einnahmen auseinandersetzen. Die steigenden Reichweiten von Onlinemedien und die Tatsache, dass eine Mehrheit der Arbeitnehmer in industriellen Ländern einen Computer für ihre Arbeit verwenden, erzeugen zunehmenden Veränderungsdruck. Dies kommt auch im Bewusstsein der Redaktionen an.\n\nDer letzte Chefredakteur vor der Einstellung von PC Professionell äußert sich dazu in seinem Relaunchblog:\n\nAnlässlich der VDZ-Tagung zur Zukunft der Printmedien äußerte sich der Geschäftsführer der PC-Welt, Mac-Welt und einiger anderer Publikationen dazu, dass IDG in den USA gerade erst eine Print-Publikation einstellen musste: „Ich als IT-Verleger habe keine Zeit.“ Seine Zielgruppe wandere viel schneller ins Netz ab als der Durchschnitt der Bevölkerung.\n\nLaut einem Bericht des t3n Magazin liegt die Zukunft der Computerzeitschriften eher in einer klaren Positionierung sowie Differenzierung gegenüber anderen Medien.\n"}
{"id": "17159", "url": "https://de.wikipedia.org/wiki?curid=17159", "title": "Supercomputer", "text": "Supercomputer\n\nAls Supercomputer oder Superrechner werden die schnellsten Computer ihrer Zeit bezeichnet. Dabei ist es unerheblich, auf welcher Bauweise der Computer beruht, solange es sich um einen universal einsetzbaren Rechner handelt.\nEin typisches Merkmal eines modernen Supercomputers ist seine große Anzahl an Prozessoren, die auf gemeinsame Peripheriegeräte und einen teilweise gemeinsamen Hauptspeicher zugreifen können. Supercomputer werden häufig für Computersimulationen im Bereich des Hochleistungsrechnens eingesetzt.\n\nSupercomputer spielen eine essentielle Rolle im wissenschaftlichen Rechnen und werden in diversen Disziplinen eingesetzt, etwa Simulationen im Bereich der Quantenmechanik, Wettervorhersagen, Klimatologie, Entdeckung von Öl- und Gasvorkommen, Molekulardynamik, Biologischen Makromolekülen, Kosmologie, Astrophysik, Fusionsforschung, Erforschung von Kernwaffentests bis hin zur Kryptoanalyse.\n\nIn Deutschland sind Supercomputer überwiegend an Universitäten und Forschungseinrichtungen wie etwa den Max-Planck-Instituten zu finden. Wegen ihrer Einsatzmöglichkeiten fallen sie unter deutsche Gesetze zur Waffenexportkontrolle.\n\nSupercomputer spalteten sich in der Geschichte der Computerentwicklung in den 1960er Jahren von den wissenschaftlichen Rechnern und den Großrechnern ab. Während Großrechner eher auf hohe Zuverlässigkeit hin optimiert wurden, wurden Supercomputer in Richtung hoher Rechenleistung optimiert. Der erste offiziell installierte Supercomputer Cray-1 schaffte 1976 130 MegaFLOPS. \n\nUrsprünglich wurde die herausragende Rechenleistung durch maximale Ausnutzung der verfügbaren Technik erzielt, indem Konstruktionen gewählt wurden, die für größere Serienproduktion zu teuer waren (z. B. Flüssigkeitskühlung, exotische Bauelemente und Materialien, kompakter Aufbau für kurze Signalwege), die Zahl der Prozessoren war eher gering. Seit geraumer Zeit etablieren sich vermehrt sogenannte Cluster, bei denen eine große Anzahl von (meist preiswerten) Einzelrechnern zu einem großen Rechner vernetzt werden. Im Vergleich zu einem Vektorrechner besitzen die Knoten in einem Cluster eigene Peripherie und ausschließlich einen eigenen, lokalen Hauptspeicher. Cluster verwenden Standardkomponenten, deshalb bieten sie zunächst Kostenvorteile gegenüber Vektorrechnern. Sie erfordern aber einen weit höheren Programmieraufwand. Es ist abzuwägen, ob die eingesetzten Programme sich dafür eignen, auf viele Prozessoren verteilt zu werden.\n\nModerne Hochleistungsrechner sind vor allem Parallelrechner. Sie bestehen aus einer großen Anzahl an miteinander vernetzten Computern. Zusätzlich verfügt jeder Computer in der Regel über mehrere Hauptprozessoren (CPUs). Auf einem Supercomputer können nicht unmodifiziert die gleichen Programme wie auf einem gewöhnlichen Computer laufen, sondern speziell abgestimmte Programme, die die einzelnen, parallel arbeitenden Prozessoren beschäftigen. Supercomputer sind (wie auch heutzutage jeder handelsübliche Computer, im unteren Preissegment) Vektorrechner. Dominierend sind mittlerweile Standardarchitekturen aus dem Bereich von Personalcomputern und Servern, etwa x86-64 von Intel (Xeon) und AMD (Threadripper). Sie unterscheiden sich von gewöhnlicher Personalcomputer-Hardware nur geringfügig. Es gibt aber auch immer noch Spezialhardware wie IBM BlueGene/Q und Sparc64.\n\nDie Verbindungen zwischen Einzelcomputern werden bei Supercomputern mit speziellen Hochleistungsnetzwerken umgesetzt, verbreitet ist dabei unter anderem InfiniBand. Computer werden oft mit Beschleunigerkarten ausgestattet, etwa Grafikkarten oder der Intel Xeon Phi. Grafikkarten eignen sich zum Einsatz im High Performance Computing, weil sie exzellente Vektorrecheneinheiten darstellen und Probleme der Linearen Algebra effizient lösen. Die zugehörige Technik nennt sich General Purpose Computation on Graphics Processing Unit (GPGPU).\n\nBei Clustern werden die einzelnen Computer oft \"Knoten\" (englisch \"nodes\") genannt und mittels Clustermanagament-Werkzeugen zentral konfiguriert und überwacht.\n\nWährend noch in den 1990er Jahren diverse Unix-Varianten bei Supercomputern verbreitet waren, hat sich in den 2000er Jahren die Freie Software Linux als Betriebssystem durchgesetzt. In der TOP500-Liste der schnellsten Computersysteme (Stand Juni 2012) werden insgesamt 462 ausschließlich unter Linux betriebene Systeme und 11 teilweise (CNK/SLES 9) unter Linux betriebene Systeme aufgelistet. Damit laufen 92,4 % der Systeme vollständig unter Linux. Fast alle anderen Systeme werden unter Unix oder Unix-artigen Systemen betrieben. Der im Desktop-Bereich größte Konkurrent Windows spielt im Bereich der Höchstleistungsrechner kaum eine Rolle (0,4 %).\n\nVerwendete Programmiersprachen zur Programmierung von Programmen sind vor allem Fortran und C bzw. C++. Um möglichst schnellen Code zu generieren, wird meist auf Compiler der Supercomputer-Hersteller (etwa CRAY oder Intel) zurückgegriffen. Programme im High Performance Computing (HPC) werden typischerweise in zwei Kategorien eingeteilt:\n\nIn der Praxis findet man oft die Kombination beider Parallelisierungstechniken, die oft \"Hybrid\"-Parallelisierung genannt wird. Sie ist deswegen populär, weil Programme oft nicht gut genug skalieren, um alle Kerne eines Supercomputers mit reinem \"message passing\" auszulasten.\n\nWenn Supercomputer mit Beschleunigerkarten (Grafikkarten oder Rechenkarten) ausgestattet sind, zergliedert sich die Programmierung nochmals auf die des Hostcomputers und die der Beschleunigerkarte. OpenCL und CUDA sind dabei zwei Schnittstellen, die die Programmierung derartiger Komponenten ermöglichen.\n\nHochleistungsrechner werden in der Regel nicht von einem einzigen Benutzer bzw. Programm genutzt. Stattdessen werden Job-Scheduler wie \"Simple Linux Utility for Resource Management\" (SLURM) oder IBMs LoadLeveler verwendet, um einer großen Anzahl an Benutzern zu ermöglichen, kurzzeitig Teile des Supercomputers zu verwenden. Die Zuteilung erfolgt dabei exklusiv auf Ebene von Knotenzuordnung oder Prozessorzuordnung. Die verbrauchte Prozessorzeit wird dabei in Einheiten wie \"CPU-Stunden\" oder \"Knoten-Stunden\" gemessen und ggf. abgerechnet.\n\nDie Herstellungskosten eines Supercomputers aus der TOP10 bewegen sich derzeit in einem sehr hohen zweistelligen, oftmals bereits dreistelligen Euro-Millionenbetrag.\n\nDie heutigen Supercomputer werden überwiegend zu Simulationszwecken eingesetzt. Je realitätsnäher eine Simulation komplexer Zusammenhänge wird, desto mehr Rechenleistung wird in der Regel benötigt. Ein Vorteil der Supercomputer ist, dass sie durch ihre extrem schnelle und damit große Rechenleistung immer mehr Interdependenzen berücksichtigen können. Dies erlaubt das Einbeziehen weiterreichender, oftmals auch unscheinbarer Neben- oder Randbedingungen zur eigentlichen Simulation und gewährleistet dadurch ein immer aussagekräftigeres Gesamtergebnis.\n\nDie derzeitigen Haupteinsatzgebiete der Supercomputer umfassen dabei die Bereiche Biologie, Chemie, Geologie, Luft- und Raumfahrt, Medizin, Wettervorhersage, Klimaforschung, Militär und Physik.\n\nIm militärischen Bereich haben Supercomputer es z. B. ermöglicht, neue Atombombenentwicklungen durch Simulation, ohne Stützdaten durch weitere unterirdische Atombombenversuche, durchzuführen. Die Bereiche kennzeichnen sich dadurch, dass es sich um sehr komplexe Systeme bzw. Teilsysteme handelt, die in weitreichendem Maße miteinander verknüpft sind. So haben Veränderungen in dem einen Teilsystem meist mehr oder minder starke Auswirkungen auf benachbarte oder angeschlossene Systeme. Durch den Einsatz von Supercomputern wird es immer leichter möglich, viele solcher Konsequenzen zu berücksichtigen oder sogar zu prognostizieren, wodurch bereits weit im Vorfeld etwaige Gegenmaßnahmen getroffen werden könnten. Dies gilt z. B. bei Simulationen zum Klimawandel, der Vorhersagen von Erdbeben oder Vulkanausbrüchen sowie in der Medizin bei der Simulation neuer Wirkstoffe auf den Organismus. Solche Simulationen sind logischerweise, ganz unabhängig von der Rechenleistung, nur so genau, wie es die programmierten Parameter bzw. Modelle zur Berechnung zulassen.\nDie enormen Investitionssummen in die stetige Steigerung der FLOPS und damit die Entwicklung von immer schnelleren Supercomputern werden vor allem mit den Nutzenvorteilen und dem eventuellen „Wissensvorsprung“ für die Menschheit gerechtfertigt, weniger aus den Aspekten des allgemeinen technischen Fortschritts.\n\nDas wissenschaftliche Hochleistungsrechnen ist in Deutschland durch das Gauss Centre for Supercomputing (GCS) organisiert, welches Mitglied im europäischen Partnership for Advanced Computing in Europe (PRACE) ist. Die Mehrzahl der 16 deutschen Bundesländer unterhalten Landeshochrechnerverbände, um die Nutzung ihrer Hochleistungsrechner zu organisieren. In der Wissenschaftswelt wird in der Regel ein Kontingent an CPU-Stunden ausgeschrieben und unter Bewerbern verteilt.\n\nDie schnellsten Superrechner nach Leistung werden heutzutage halbjährlich in der TOP500-Liste aufgeführt. Als Bewertungsgrundlage dient der Linpack-Benchmark. Die schnellsten Superrechner nach Energieeffizienz bzw. MFLOPS/W werden seit November 2007 in der Green500-Liste geführt. Den größten Anteil (117) der Top 500 leistungsstärksten Rechner weltweit konnte 2018 Lenovo installieren.\n\nDiese Green500-Liste vom November 2014 weist länderweise gemittelte Effizienzen von 1895 MFLOPS/W (Italien) bis hinunter zu 168 MFLOPS/W (Malaysia) auf.\n\nStand früher als Juni 2017 (2016?). Jedoch Piz Daint, Schweiz nachgetragen.\n\nDie jeweils 3 schnellsten Rechner aus der Schweiz und Österreich. Daten aus \"Top500 List 2017\" Einträge Pos. 3, 82, 265, 330, 346, 385. In der Liste der 500 schnellsten Supercomputer der Welt findet sich keiner aus Liechtenstein. (Stand Juni 2017)\n\nNachfolgende Tabelle (Stand Juni 2017) listet einige der schnellsten Superrechner ihrer jeweiligen Zeit auf:\n\nTrägt man die FLOPs der schnellsten Computer ihrer Zeit gegen die Zeit auf, erhält man eine exponentielle Kurve, logarithmisch in etwa ein Gerade, wie im folgenden Graph dargestellt.\n\nMit einer Executive Order hat US-Präsident Barack Obama die US-Bundesbehörden angewiesen, die Entwicklung eines ExaFlops-Supercomputers voranzutreiben. Im Jahr 2018 soll Intels Aurora-Supercomputer eine Rechenleistung von 180 PetaFlops erreichen. Im Jahr 2021 will das DOE einen ersten Exascale-Supercomputer aufstellen und 9 Monate später in Betrieb nehmen.\n\nChina will bis 2020 einen Supercomputer mit einer Geschwindigkeit im Exaflops Bereich entwickeln. Der Prototyp von „Tianhe-3“ soll bis Anfang 2018 fertig sein, berichtete „China Daily“ am 20. Februar 2017.\nIm Mai 2018 wurde er vorgestellt.\n\n2011 starteten in der EU zahlreiche Projekte mit dem Ziel Software für exascale Supercomputer zu entwickeln. Das CRESTA projekt (Collaborative Research into Exascale Systemware, Tools and Applications), das DEEP Projekt (Dynamical ExaScale Entry Platform), und das Projekt Mont-Blanc. Das MaX (Materials at the Exascale) ist als weiteres wichtiges Projekt zu nennen. Im März 2015 startete das SERT Projekt unter Beteiligung der Universität von Manchester und der STFC in Cheshire.\n\nSiehe auch: Europäisches Hochleistungsrechnen.\n\nIn Japan begann 2013 das RIKEN die Planung eines Exascale Systems für 2020, mit einem Stromverbrauch von weniger als 30 MW. 2014 wurde Fujitsu beauftragt die nächste Generation des K computer zu entwickeln. 2015 verkündete Fujitsu auf der International Supercomputing Conference, dass dieser Supercomputer Prozessoren der ARMv8 Architekture verwenden wird.\n\n\n\nKorrelatoren sind spezielle Geräte in der Radiointerferometrie deren Leistung man ebenfalls in Einheiten von FLOPs messen kann. Sie fallen nicht unter die Kategorie der Supercomputer, weil es sich um Spezialcomputer handelt mit denen sich nicht jede Art von Problemen lösen lässt.\n\n\n\n"}
{"id": "17247", "url": "https://de.wikipedia.org/wiki?curid=17247", "title": "LinuxUser", "text": "LinuxUser\n\nLinuxUser ist eine deutsche Computerzeitschrift für Linux-Benutzer aus dem Verlag der Computec Media, die auch das \"Linux-Magazin\" herausgibt. Sie erscheint seit Januar 2000 – zunächst als Sonderheft, seit Juni 2000 regelmäßig. LinuxUser beschäftigt sich hauptsächlich mit dem praktischen Einsatz von Linux und richtet sich im Gegensatz zum \"Linux-Magazin\" vorwiegend an Endbenutzer.\n\nDie Zeitschrift erscheint an jedem dritten Donnerstag eines Monats. Vierteljährlich wird ein Sonderheft mit Spezialthemen wie Netzwerk, Sicherheit oder Hardware herausgegeben.\n\nChefredakteur war bis Oktober 2004 Hans-Georg Eßer, Jörg Luther ist sein Nachfolger.\n\nDie Zeitschrift behandelt in erster Linie Software, die für den klassischen Heimanwender relevant ist. Das Spektrum reicht von der ausgewachsenen Office-Suite bis hin zum kleinen Konsolenprogramm. Jedes Heft hat einen thematischen Schwerpunkt, der bis zu einem Viertel der Ausgabe umfasst. Daneben finden sich die Artikel in Rubriken wie Praxis, Im Test, Know-how oder Netz&System, womit dem Leser gleichzeitig eine Orientierung in Bezug auf die Ausrichtung des Artikels gegeben wird.\n\nBei der Produktion der LinuxUser-Ausgaben wird hauptsächlich freie Software eingesetzt. Die Heft-CD wird mit mkisofs / cdrecord gemastert und gebrannt. Einzig für das Layout kommen mit Adobe InDesign auf Mac OS X proprietäre Komponenten zum Einsatz.\n\nVom gleichen Verlag wird die easyLinux und das Linux-Magazin herausgegeben.\n\n"}
{"id": "17303", "url": "https://de.wikipedia.org/wiki?curid=17303", "title": "Steuerungstaste", "text": "Steuerungstaste\n\nDie Steuerungstaste, beschriftet mit Strg, in der Schweiz Control-Taste, beschriftet mit Ctrl, ist eine Taste zum Aufrufen von Zusatzfunktionen:  bzw. \n\nAuf üblichen PC-Tastaturen befindet sie sich ganz links in der untersten Tastenreihe direkt unter der Umschalttaste (\"Shift\"-Taste) und meist ein zweites Mal rechts unten, links von den Pfeiltasten. Die meisten Anwendungen unterscheiden nicht zwischen linkem und rechtem Tastendruck, bei Computerspielen wird jedoch davon Gebrauch gemacht.\n\nMitunter wird die Taste in Deutschland fälschlicherweise als „String“-, „Strong“- oder sogar „Strange“-Taste bezeichnet, häufiger jedoch werden die einzelnen Buchstaben „S-t-r-g“ anstelle des Wortes „Steuerung“ ausgesprochen. In der Schweiz wird die Taste gewöhnlich als „Control“ ausgesprochen, selten buchstabiert.\n\nDie Steuerungstaste wird üblicherweise nicht allein verwendet, sondern führt meist in Kombination mit einer anderen Taste einen Befehl aus oder ruft eine Programmfunktion auf (eine Ausnahme ist in neueren Windows-Versionen die – allerdings standardmäßig ausgeschaltete – Möglichkeit, durch Drücken der Taste den Mauszeiger wiederzufinden). So kann man beispielsweise mit + Text kopieren und ihn mit + einfügen (Kopieren und Einfügen), ohne das Tippen unterbrechen und die Maus benutzen zu müssen. Die Alt-Taste und Alt-Gr-Taste sind weitere, computergeschichtlich etwas neuere Tasten, die nach diesem Prinzip funktionieren.\n\nDrücken der Steuerungstaste zusammen mit einer anderen Taste führt bei PCs dazu, dass die ASCII-Entsprechung der zweiten Taste mit 1F (00011111) bitweise UND-verknüpft wird. Das bewirkt, dass das sechste bis achte Bit der ASCII-Codierung der entsprechenden Taste abgeschnitten werden und als Rückgabewert eine Zahl herauskommt, die einer der ersten 32 ASCII-Codierungen, das heißt einer bestimmten Steuerungsfunktion, entspricht.\n\nJedem der ASCII-Großbuchstaben wird eine Funktion zugeordnet, deren Code genau um 64 kleiner ist als der Code des jeweiligen Buchstabens. Kleinbuchstaben aktivieren dieselben Funktionen wie die entsprechenden Großbuchstaben, nur wird von deren Code nicht 64, sondern 96 abgezogen, da sich die Codes der Klein- und Großbuchstaben um jeweils 32 unterscheiden.\n\nHeute werden die meisten dieser Steuerungsfunktionen nicht mehr für die Steuerung von Geräten, sondern für die Steuerung verschiedener Softwarepakete durch Tastaturkombinationen verwendet.\n\nViele Programme enthalten die Möglichkeit, Funktionen über die Tastatur statt per Maus und Menüs auszulösen. In den Menüs steht oft rechts neben der jeweiligen Funktion, wie sie schneller über eine Tastenkombination ausgeführt werden kann. Je nach Programm können sich die Kombinationen allerdings unterscheiden. Die folgende Tabelle beschreibt die Kombinationen mit deutsch/österreichischer und mit Schweizer Tastaturbelegung.\n\nBei älteren PC-Typen (original IBM-PC, IBM XT, IBM AT), und bei vielen Nicht-Personal-Computern, zum Beispiel Commodore 64, Atari ST, Apple II und manchen Unix-Rechnern, findet sich die Taste oberhalb der Umschalttaste, entweder links vom „A“ oder links vom „Q“.\n\nAuf alten Rechnerarchitekturen wurde die Strg-Taste benutzt, um die ersten 32 Codes (000 bis 031) der ASCII-Tabelle zu erzeugen, die sogenannten Steuerzeichen. Diesen 32 nicht druckenden Zeichen entsprachen auf den Ausgabegeräten (ursprünglich waren das meist Fernschreiber) keine symbolischen Darstellungen, sondern Geräte steuernde Funktionen wie beispielsweise BEL (Bell, Oktalcode: \\007, Ctrl+G), was die Klingel des Geräts (heute einen Piepton) ertönen lässt, LF (Zeilenvorschub, engl. \"Line Feed\", Oktalcode: \\012, Ctrl+J), was einen Zeilenvorschub erzeugt, BS (Backspace, Oktalcode: \\010, Ctrl+H), was den Druckkopf einen Schritt zurück nach links bewegt usw.\n\n"}
{"id": "17331", "url": "https://de.wikipedia.org/wiki?curid=17331", "title": "Leertaste", "text": "Leertaste\n\nDie Leertaste (auch \"Leerschritttaste\") ist eine Taste der Computer-Tastatur und Schreibmaschinen­tastatur. Sie befindet sich auf der PC-Tastatur in der Mitte der untersten Tastenreihe, hat keinen Aufdruck und ist die breiteste Taste. Im Laufe der Zeit wurde sie jedoch durch das Einführen diverser anderer Tasten (Verlegung der Strg-Taste nach links unten, zweite Strg-Taste, zweite Alt-Taste bzw. AltGr-Taste, Windows-Taste, zweite Windows-Taste, Windows-Menütaste) immer mehr verkleinert. Die Leertaste fügt ein Leerzeichen in einen Text ein. Dieses hat den ASCII-Code dezimal 32, hexadezimal 0x20.\n\nIn den Dialogfenstern des Betriebssystems Windows kann man mit der Leertaste die Schaltfläche mit dem Fokus betätigen.\n\nIn Webbrowsern oder anderen Anzeigeprogrammen kann man mit der Leertaste oft seitenweise weiterblättern.\n\nIn Computerspielen ist meist die Funktion \"Springen\" damit belegt.\n\nIn Teilen der Schweiz wird die Leertaste auch umgangssprachlich als \"Leerschlagtaste\" bezeichnet. Diese Bezeichnung beruht darauf, dass man beim Schreiben auf der Schreibmaschine anstelle vom \"Tippen\" auch vom \"Anschlagen\" der Tasten spricht.\n\nIm englischsprachigen Bereich spricht man von \"space bar\", \"spacebar\", \"space key\" oder abgekürzt nur von \"space\".\n\n"}
{"id": "17724", "url": "https://de.wikipedia.org/wiki?curid=17724", "title": "Amiga", "text": "Amiga\n\nDer Commodore Amiga (spanisch \"amiga\": ‚Freundin‘) war ein von Mitte der 1980er bis Anfang/Mitte der 1990er weit verbreiteter Computer, der besonders in seinen Einsteigermodellen (A500 und A1200) als Heimcomputer beliebt war. Für seine Zeit hatte er ausgeprägte Multimediafähigkeiten und ein leistungsfähiges, präemptives Multitasking-Betriebssystem. In der Commodore-Zeit arbeitete er durchgängig mit Prozessoren der Motorola-68000er-Familie.\n\nJay Miner war der Schöpfer und geistige Vater des Amiga. Er stieg 1981 bei Atari aus, dort war er u. a. für die Entwicklung der Spielkonsole Atari 2600 und der Heimcomputer Atari 400 und 800 zuständig gewesen. Danach gründete er das Unternehmen Hi Toro, das etwas später zur Amiga Corporation wurde. Anfangs lieferte Amiga Spielmodule und Controller für die Atari-2600-Konsole, etwas später wurde eine eigene Amiga-Spielkonsole geplant. Aus der Spielkonsole wurde in den Köpfen der Entwickler ein Computer.\n\nAtari – damals im Besitz von Warner Communications und unter der Führung von Ray Kassar – war per Vereinbarung vom Juli 1983 Geldgeber und wollte den Amiga als Nachfolger der mittlerweile veralteten XL-Computer-Serie auf den Markt bringen. Mit dem Börsenskandal vom Dezember 1982 im Nacken musste Kassar noch im Juli 1983 zurücktreten. Der neue CEO Morgan verfolgte weiter das Ziel, das Projekt \"Lorraine,\" wie der Amiga intern genannt wurde, als Nachfolger des XL zur Marktreife zu bringen.\n\nAm 2. Juli 1984 verkaufte Warner Communications die Konsolen- und Computerabteilung von Atari an Jack Tramiel, den zu diesem Zeitpunkt bereits entlassenen Gründer von Commodore. Tramiel versuchte Amiga endgültig zu kaufen und bot den Aktionären 0,98 $ pro Aktie. Commodore (unter Irving Gould) bot kurz vor Ende der 24-Stunden-Frist letztendlich 4,25 US-Dollar pro Aktie und bekam den Zuschlag, wonach Commodore die Entwicklungsrichtung des Amiga immer stärker beeinflusste – nach Ansicht von Kritikern nicht immer im Geiste der Erfinder oder zum Vorteil des Amiga. Commodore überhob sich an dieser Übernahme und der folgenden Produkteinführung beinahe und geriet in eine ernste finanzielle Krise.\n\nDas erste Amiga-Modell – später Amiga 1000 genannt – wurde am 23. Juli 1985 in New York im Rahmen einer großen Show vorgestellt, bei der der Pop-Art-Künstler Andy Warhol ein eben erstelltes Abbild der Sängerin der Band Blondie, Deborah Harry, am Amiga neu einfärbte, während die Zuschauer live dabei waren. Die Entwickler demonstrierten des Weiteren die besonderen Eigenschaften, die den Amiga von den zeitgenössischen Konkurrenten IBM PC, Macintosh und Atari ST abhoben:\n\n\nNach Ansicht des ehemaligen Amiga-Entwicklers Dave Haynie war der Amiga der bedeutendste Einfluss von Commodore auf die Entwicklung der Personal Computer. Der Amiga führte demnach in den 1980er Jahren folgende Innovationen im Bereich der Homecomputer ein:\n\n\nIn Deutschland fand eine ähnliche Veranstaltung am 21. Mai 1986 in der Alten Oper in Frankfurt am Main statt, die von Frank Elstner moderiert wurde.\n\nAls ein Kaufargument wurde die potenzielle IBM-PC-Kompatibilität herausgestellt, zunächst in Form einer Software-Emulation namens \"Transformer,\" später dann per A1060 Sidecar. Das Sidecar ermöglichte mithilfe seiner eigens dafür vorgesehenen Intel-8088-CPU den Betrieb von MS-DOS parallel zum Amiga-Betriebssystem. Entwickelt wurde das Sidecar von Commodores Entwicklungsabteilung in Braunschweig.\n\nEin besonderes Merkmal des Amiga 1000 war seine sogenannte Keyboard-Garage: Die Tastatur war etwas schmaler ausgelegt, konnte dadurch unter das Rechnergehäuse geschoben werden, welches dafür extra etwas hochbeinig gestaltet war. Bei späteren Modellen wurde darauf wieder verzichtet, vor allem damit man eine Tastatur mit (näherungsweise) standardmäßiger Belegung einsetzen konnte.\n\nWährend der A1000 noch eine teure und vom Anwendungsgebiet her unklare (und damit schwer verkäufliche) Mischung aus Heim- und Berufsgerät war, wurden 1987 die Modelle Amiga 500 und Amiga 2000 eingeführt. Ersterer sollte eine Art Nachfolger des legendären Heimcomputers C64 werden, letzterer glich den professionellen PCs. Der Amiga 500 wurde der nach verkauften Einheiten erfolgreichste Amiga und galt in der sich schnell entwickelnden Szene als Kult und Computer für Millionen.\n\nAb A2000 und A500 boten die Amiga-Modelle dem IBM-PC-Standard entsprechende parallele und serielle Schnittstellen an; beim A1000 hingegen waren die Pinbelegung sowie das Geschlecht der jeweiligen Buchse anders.\n\nFür professionelle Anwender wurde 1988 ein Rechner namens Amiga 2500/UX angeboten, auf dem parallel ein UNIX-Betriebssystem (AMIX) verfügbar war. Technisch gesehen waren die Amiga-2500-Modelle mit dem normalen Amiga 2000 nahezu identisch. Wie schon beim Amiga 2000 konnten sie durch den Einbau eines zusätzlichen Prozessorboards (mit einer 68020-, 68030-, 68040- bzw. 68060-CPU) deutlich beschleunigt werden.\n\nAls Bindeglied zwischen Amiga und der IBM-PC-Welt besaß der Amiga 2000 sowohl die Amiga-eigenen Zorro-2-Slots als auch IBM-PC-typische ISA-Steckplätze. Diese konnten mit einem \"bridge board\" (Brückenkarte bzw. PC-Emulator, dem Nachfolger des Sidecar) aktiviert werden. Damit besaß man dann einen vollwertigen IBM-PC im Amiga, auf den man von Amiga-Seite aus zugreifen konnte.\n\nDem Amiga 2000 folgte 1990 der Amiga 3000 in einer Desktop- und Tower-Variante (Amiga 3000T), die zum ersten Mal ein neues Betriebssystem in einem modernen 3D-Look mit sich brachten. Das erstmals mit dem A3000 ausgelieferte Betriebssystem AmigaOS 2.0 wies zahlreiche Neuerungen und Optimierungen auf, die noch heute in modernen Betriebssystemen wiederzufinden sind. Der Amiga 3000 wurde – nicht zuletzt dank des fortschrittlichen Betriebssystems – ein Erfolg. Er ist heute vergleichsweise selten, und Liebhaber zahlen dafür deutlich höhere Preise als beispielsweise für seinen Nachfolger, den Amiga 4000.\n\nMit dem Amiga 500 Plus wurde 1991 für den Amiga 500 ein technisch fast identischer Nachfolger geschaffen, der wiederum 1992 durch den kaum veränderten Amiga 600 ersetzt wurde. Beide Folgemodelle waren kommerziell erfolglos, auch da nach wie vor die originären Amiga-500-Modelle günstiger angeboten wurden.\n\nMehr Erfolg – aber nicht annähernd wie der 500er – hatte der ebenfalls 1992 eingeführte, technisch stark verbesserte Amiga 1200, der die Amiga-Familie erweiterte. Der Amiga 1200 wurde als kostengünstige Variante seines großen Bruders, des Amiga 4000 (sowie Tower-Variante Amiga 4000T) eingeführt. Dieser seinerseits kam als Nachfolger des Amiga 3000 auf den Markt. Die wesentliche Gemeinsamkeit zwischen Amiga 1200 und Amiga 4000 besteht in der Verwendung der gleichen Kickstart-Version, der gleichen Workbench und des AGA-Grafikchipsatzes (in Deutschland „AA“, s. u.). Ansonsten ist der Amiga 4000 dem A1200 technisch überlegen, weil er über einen 32 Bit breiten Adressbus verfügte (im Gegensatz zu den 24 Bit des Amiga 1200), durch die Zorro-Steckplätze erweiterungsfähig war und in der Regel den leistungsfähigeren Prozessor MC68040, in preiswerteren Versionen immerhin noch einen MC68EC030, verwendete.\n\nCommodore versuchte bereits Anfang der 1990er Jahre, mit dem CDTV (einem Amiga im Design eines CD-Spielers mit der vereinigten Funktionalität beider) den Amiga als Multimedia-Plattform zu positionieren und in die Wohnzimmer zu bringen. Zu dieser Zeit entstand das Autorensystem AmigaVision. Kurz vor dem Niedergang Commodores folgte 1993 dann das CD³², dem trotz aufwendiger Fernsehwerbung der große Durchbruch versagt blieb, weil Commodore nicht die georderten Mengen produzieren und ausliefern konnte. Das CD³² basiert auf der Amiga-1200-Hardware, die um ein CD-ROM-Laufwerk sowie einen Customchip (\"Akiko\") erweitert worden war. Tastatur, Floppy und Festplatte ließen sich optional nachrüsten.\n\nDie Modelle Amiga 500/600/1200 waren die kostengünstigen Varianten der großen Amiga-Desktop-Modelle (Amiga 2000/4000). Tastatur, Floppy (Diskettenlaufwerk), Erweiterungsschnittstellen und die Hauptplatine bilden eine Einheit. Beim A600 und A1200 ist unter anderem noch Platz für eine 2,5″-ATA-Festplatte. Im A1200 lässt sich mit ein wenig technischem Geschick und einem passenden Adapterkabel eine 3,5″-Festplatte einbauen. Zudem gab und gibt es für den Amiga 500 sogenannte Turbokarten, diese enthielten einen 32-Bit-Prozessor, wie 68020, 68030, 68040, und teilweise eine FPU. Da keine speziellen Steckplätze für solche Karten vorhanden waren, wurde der Prozessor entfernt und an dessen Stelle die Erweiterungskarte gesteckt. Eine einfache Möglichkeit, den A500/A2000 etwas schneller zu machen, war der Austausch der CPU 68000 gegen einen 68010-Chip. Dieser ist pinkompatibel zum 68000, benötigt aber für einige Befehle weniger Taktzyklen und hat einen 6-Byte-Befehls-Cache.\n\nDie Modelle Amiga 2000/3000/4000 sind erweiterbare Systeme, in denen zusätzliche Laufwerke und Erweiterungskarten integriert werden können. Die Modelle 3000 und 4000 wurden auch als Tower-Versionen angeboten und waren im oberen Preissegment angesiedelt – vergleichbar mit heutigen High-End-Rechnern. Auch die Tastaturrechner haben einen Expansion-Slot, in den Erweiterungskarten eingesetzt werden können.\n\nDie Amiga 3000/4000 wurden mit verschiedenen Prozessoren angeboten. Die Palette reicht vom 68020 bis hin zum 68040. Es gab sogar eine Sonderanfertigung des Amiga 4000 mit einem 68060-Prozessor, die durch die Insolvenz von \"Amiga Technologies\" nur kurzzeitig in den USA ausgeliefert wurde. Dieses Modell war allerdings nur ein gewöhnlicher Amiga 4000T, der durch eine \"QuikPak 4060\" erweitert worden war.\n\nZum Anschluss von Festplatten verfügen A2500, 3000(T) und A4000T über eine interne SCSI-Schnittstelle, Amiga 600, 1200 und 4000 besitzen dagegen einen ATA-Controller. Für die restlichen Modelle waren Festplattencontroller als Erweiterung erhältlich – am populärsten war SCSI, in der Anfangszeit wurde teilweise die ST506-Schnittstelle verwendet. Ein Grund für den Einsatz des teuren SCSI-Standards ist die geringe Belastung des Prozessors bei den Ladevorgängen.\n\nFür praktisch alle Amiga-Modelle gab es in Form sogenannter \"Turbokarten\" Steckkarten mit schnelleren oder ganz abweichenden Prozessorvarianten. Dazu wurde bei den frühen Geräten der 68000-Chip entfernt und durch eine Prozessorkarte ersetzt. Diese enthielt dann entweder einen 68020- oder einen 68030-Prozessor und oft auch entsprechendes RAM (32 Bit Datenbusbreite). Spätere Modelle hatten einen dafür vorbereiteten \"CPU-Slot\". Zuletzt wurden Varianten angeboten, die PowerPC-Prozessoren enthielten.\n\nFür den Erweiterungsport des Amiga 500/500+ brachte Commodore das externe CD-ROM-Laufwerk A570 heraus. Im Grunde handelte es sich dabei um die zusätzliche Komponente, die das Commodore CDTV vom Amiga 500 unterschied. So konnte das A570 die speziellen CDTV-Datenträger wiedergeben und bot dieselbe CD-Player-Oberfläche. Mangels einer direkten Anbindung an den Soundprozessor des Amiga 500 besaß das A570 eigene Audioausgänge. Bei dem CD-Laufwerk handelte es sich um ein Single-Speed-Gerät. Zusätzlich bot das A570 den vom CDTV bekannten SCSI-Steckplatz sowie einen Steckplatz für Speichererweiterungen bis 2 MB.\n\nFür die ersten Amiga-Modelle mit Original Chip Set (s. u.) wurde speziell der Monitor A2024 herausgebracht, der die professionelle Anwendung der Rechner im Büro ermöglichen sollte. Dazu stellte er mit erheblichem Hardwareaufwand eine wesentlich höhere Bildauflösung zur Verfügung – auf Kosten von Farbanzahl und Darstellungsgeschwindigkeit.\n\nBesonders in den frühen Amiga-Produkten verewigten sich die Entwickler mehr oder weniger offen außerhalb des offiziellen Rahmens. Bekannt war die sogenannte \"Guru Meditation\". Diese bezeichnet den Zustand eines durch das Amiga-Betriebssystem abgefangenen schweren Programmfehlers. Sie ist vergleichbar mit dem \"Blue Screen Of Death\" der auf Windows NT basierenden Systeme oder mit dem \"Bomben-Bus-Error\" des Atari ST. Zusätzlich zu diesem konnte man mittels eines Rechtsklicks der Maus einen internen Debugger aufrufen und den Amiga-Speicher über einen weiteren Computer, der an der seriellen Schnittstelle angeschlossen wurde, durchsehen und so genau feststellen, was den Fehler verursacht hatte. Auch wenn diese Fehlermeldung später durch ein nüchternes „Software Failure“ ersetzt wurde, hielt sich die Bezeichnung umgangssprachlich. Daneben gibt es auch mittels diverser Aktionen abrufbare Easter Eggs und nur mit einem Speichermonitor finden sich eine Reihe versteckter Botschaften im Betriebssystem-ROM.\n\nWichtige Bauteile bekamen eigene Namen: Zorro, Big/Fat Agnus, Denise und Paula sind einige davon. Die Innenseite des Amiga-1000-Deckels zieren in den Kunststoff gegossene Unterschriften der Entwickler sowie ein Pfotenabdruck des Hundes Mitchy von Jay Miner. Ebenso findet sich auf der Platine des Amiga 500 der Schriftzug „B52/ROCK LOBSTER“ eingeätzt, was eine Hommage an einen Song der Band The B-52s darstellt.\n\nAuch bei der Betriebssystemsoftware zeigten die Entwickler Humor. Bearbeitete man mit dem Programm \"Diskdoctor\" eine Diskette und konnten nur Teile wiederhergestellt werden, so bekam diese Diskette den Namen \"Lazarus.\"\n\nGerade diese persönliche Note wurde von manchen Benutzern als Kaufargument gegenüber den sterilen IBM-PCs angeführt.\n\nSehr bekannt wurden Musiker, die mit dem Amiga Musik machten (siehe auch Chiptune):\n\n\nsowie Künstler, die den Amiga für Computergrafik, Videoinstallationen und Pop Art nutzten:\n\n\nAußerdem kam der Amiga als Requisite in Spielfilmen und Serien der 1980er Jahre zum Einsatz. Zum Beispiel sind in einigen Folgen von Miami Vice sowohl ein Amiga 1000 (ab der vierten Staffel durch einen Amiga 2000 ersetzt) als auch Bildschirmaufnahmen von der CLI-Oberfläche zu sehen. Ebenso wurde er in einigen Folgen der deutschsprachigen Version der Sesamstraße eingesetzt.\n\nBeim ersten Spielfilm der Jurassic-Park-Reihe wurden mit Amigas die schnellen Vorentwürfe für Dinosaurieranimationen erstellt, bevor diese mit größeren Workstations endgültig gerendert wurden.\n\nDie Computeranimationen der Serien seaQuest DSV und Babylon 5 wurden auf Amiga-Systemen erstellt und gerendert.\n\nZu Zeiten von Commodore wurden durchweg Prozessoren der Motorola-68000-Familie verwendet. Es begann mit dem 68000, der als 16/32-Bit-System angesehen werden kann. Später kamen die weiterentwickelten Mitglieder der Familie wie 68020, 68030, 68040 und 68060 zum Einsatz, die den Amiga zum 32-Bit-System machten.\n\nParallel dazu wurden – zunächst auf Prozessor-Steckkarten, in der Nach-Commodore-Ära auch als Hauptprozessor – CPUs aus der PowerPC-Familie (PPC) eingesetzt.\n\nWichtigstes Kennzeichen der klassischen Amiga-Hardware sind die spezialisierten \"Custom-Chips\". Diese stellen eigenständige Co-Prozessoren dar, die den Hauptprozessor bei Grafikoperationen, Interruptverwaltung, Sounderzeugung und diversen Ein-/Ausgabeoperationen entlasten, wofür insgesamt 25 DMA-Kanäle zur Verfügung stehen. Dieser Custom-Chipsatz begründete die seinerzeit im Vergleich zu anderen konkurrierenden Systemen als sehr fortschrittlich angesehenen Grafik- und Soundeigenschaften des Amiga.\n\nDer Chipsatz setzt sich aus den folgenden Custom-Chips zusammen:\n\n\nAnfang der 1990er Jahre wirkte sich jedoch diese Fixierung auf den Chipsatz von Seiten des Betriebssystems zunehmend negativ auf die Flexibilität der Architektur aus, da man lange Zeit keine Möglichkeiten hatte, leistungsfähigere Erweiterungskarten wie z. B. Grafik- oder Soundkarten systemkonform zu nutzen. Das führte in Kombination mit der stagnierenden Weiterentwicklung des Chipsatzes dazu, dass die Architektur in zunehmendem Maße veraltete und bis spätestens zur Mitte der 1990er Jahre bereits technisch überholt war.\n\nIm Laufe der Zeit wurden drei verschiedene Versionen des Chipsatzes entwickelt: Amiga 1000, Amiga 500 und Amiga 2000 nutzen die erste Version von 1985, die später auch als \"Original Chip Set\" (OCS) bezeichnet wurde. Im \"HiRes-Modus\" können damit Auflösungen von 640 × 256 beziehungsweise 640 × 512 Bildpunkten im Zeilensprungverfahren (PAL) mit einer Palette von maximal 16 aus 4096 möglichen Farben dargestellt werden. Ungleich flexibler ist der \"LoRes-Modus\" mit einer Auflösung von 320 × 256 beziehungsweise 320 × 512 Bildpunkten im Zeilensprungverfahren (PAL), der nicht nur die Möglichkeit bietet, eine frei wählbare Palette von 32 Farben zu nutzen, sondern im sogenannten \"HAM6-Modus\" auch alle 4096 Farben gleichzeitig (bei gewissen Einschränkungen) darzustellen. Zusätzlich gibt es noch den \"EHB-Modus\" (Extra-Halfbright-Modus), der eine Palette von 64 Farben ermöglicht, wobei allerdings nur die ersten 32 frei wählbar sind und die restlichen aus diesen mit halber Helligkeit erzeugt werden. Dieser Modus war bei den ersten in den USA verkauften Amigas noch nicht vorhanden. Mittels Overscan können diese Auflösungen geringfügig angepasst werden, indem die oberen und seitlichen Bildschirmränder eliminiert werden, um die Fläche des Videomonitors bzw. Fernsehers besser auszunutzen. Mit dem Soundchip \"Paula\" ist es außerdem möglich, vierstimmigen 8-Bit-Ton abzuspielen (zwei Stimmen pro Stereokanal), wobei Samples mit jeweils frei wählbarer Samplingfrequenz von bis zu 28 Kilohertz (OCS) bzw. 56 Kilohertz (ECS, AGA) abgespielt werden können.\n\nDas im Amiga 600, Amiga 500 Plus und Amiga 3000 verwendete \"Enhanced Chip Set\" (ECS) aus dem Jahr 1990 wurde marginal um einen \"SuperHiRes-Modus\" mit 1280 × 256 beziehungsweise 1280 × 512 Bildpunkten im Zeilensprungverfahren (PAL) bei maximal vier Farben sowie um freier programmierbare Zeilenfrequenzen, die auch höhere vertikale Auflösungen ohne Zeilensprung erlaubten, und die Möglichkeit, 2 MB Chip-RAM zu adressieren, ergänzt.\n\nDie letzte verkaufte Variante, der in der \"Advanced Graphics Architecture\" (AGA) verwendete AGA-Chipsatz aus dem Jahr 1992, kam erstmals im Amiga 4000 und später im Amiga 1200 zum Einsatz. (In Deutschland musste er als AA-Chipsatz bezeichnet werden, weil es eine Namenskollision mit einer Grafikkarte aus der Anfangszeit der IBM-kompatiblen Commodore-PCs gab.) AGA erweitert die Farbtiefe von 12 Bit (4096 Farben) auf 24 Bit (16,8 Mio. Farben). Die Farbpaletten können mit AGA durchgehend 256 Einträge umfassen. Der HAM-Modus wurde ebenfalls erweitert, so dass mehrere hunderttausend Farben gleichzeitig dargestellt werden können. Auch bei AGA sind maximal 2 MB Chip-RAM möglich.\n\nDer klassische Amiga unterscheidet beim Arbeitsspeicher (RAM) zwischen zwei verschiedenen Varianten: Das sogenannte Chip-Memory oder Chip-RAM, auf das der Prozessor und die Custom-Chips zugreifen können, sowie das sogenannte Fast-Memory oder Fast-RAM, das allein dem Prozessor zur Verfügung steht und daher deutlich schneller arbeitet. Das Chip-Memory entspricht in etwa dem Shared Memory, das häufig für Onboard-Grafikkarten in heutigen PCs und Laptops verwendet wird. Im Gegensatz zu dieser Architektur ist beim Amiga jedoch kein festgelegter Teil des Chip-RAMs als Grafikspeicher deklariert, sondern der gesamte Bereich kann je nach momentanem Bedarf für Grafik oder generische Daten genutzt werden. Vorteil dieses Konzeptes beim Amiga ist vor allem, dass keine feste Trennung vorliegt, sondern der Prozessor direkt in den Speicher des Grafikchips schreiben kann.\n\nDas Bussystem des Amiga für Erweiterungssteckkarten ist der sogenannte Zorro-Bus mit 24-Bit- (Zorro 2) bzw. 32-Bit-Adressraum (Zorro 3). Der A500 und der A1000 haben seitlich einen 86-poligen Anschluss mit Zorro-2-Bus, der A2000 hat intern mehrere 100-polige Zorro-2-Steckplätze (mechanisch ähnlich PC-Steckkarten). Der A3000 und der A4000 haben intern mehrere 100-polige Steckplätze mit Zorro-3-Bus. Die 32-Bit-Adressbreite bei Zorro 3 wird durch Multiplexen einiger Signalleitungen erreicht. Durch einen Adapter kann man Zorro-3-Karten an einem Amiga 500/1000 betreiben. Selbstverständlich laufen alle Zorro-2-Karten noch am Zorro-3-Bus, da am Bus selbstständig erkannt wird, ob es sich bei der Karte um eine Zorro-2- oder Zorro-3-Karte handelt. Mit einer PC-Emulator-Karte oder einer Bridge-Karte sind auch handelsübliche IBM-kompatible 16-Bit-ISA-Steckkarten nutzbar.\n\nDer Amiga hat bereits ein Autokonfigurationssystem (ähnlich dem späteren Plug and Play), das es dem Betriebssystem ermöglicht, Adressen und Interrupts den Karten variabel zuzuweisen. Daher gibt es weit weniger Konfliktpotenzial als bei den ISA-Steckplätzen der IBM-kompatiblen Systeme.\n\nNeben diesen Erweiterungssteckplätzen weisen viele Amiga-Modelle einen CPU-Steckplatz auf, in den eine neue Prozessorkarte eingesteckt werden kann, deren Prozessor den Prozessor auf der Hauptplatine ablöst. Dadurch konnten diese Amigas auf neuere, schnellere Prozessoren aktualisiert werden, ohne einen ganz neuen Rechner kaufen zu müssen.\n\nEine der gängigsten Erweiterungen für den internen Amiga-1200-Erweiterungsbus (einen abgespeckten Zorro-3-Bus) war die Blizzard-Turbokarte mit den Prozessoren 68030 bei 50 MHz, 68040 bei 25 MHz oder 68060 bei 50 MHz.\n\nDie größeren Amiga-Modelle A2000, A3000 und A4000 bieten außerdem einen Video-Steckplatz, in den eine Grafikerweiterungskarte gesteckt werden kann. Dementsprechend sind auf ihm die eigenen Amiga-Videosignale verfügbar, um von so einer Karte weiterverarbeitet zu werden.\n\nEine weitere damals herausragende Möglichkeit des klassischen Amiga ist die Genlock-Fähigkeit. Die Synchronisation auf das Videosignal einer externen Quelle ermöglicht (Chroma-)Keying, also das Ersetzen einer bestimmten Farbe im Videosignal des Computers durch das externe Signal, die Vertitelung oder aufwendige Blenden. Deshalb wurde der Amiga oft zum privaten oder halbprofessionellen Videoschnitt benutzt. Auch professionelle Blue-Box-Anwendungen waren verfügbar. Diverse Sendeanstalten benutzten den Amiga lange zur Einblendung ihrer Logos in das laufende Programm oder als Schriftgenerator zur Einblendung z. B. von Sportergebnissen und Zwischenständen.\n\nSpäter wurden für den Amiga Grafikkarten (zunächst reine Flickerfixer) angeboten, die die beim Original vorhandenen Videofähigkeiten um eine flimmerfreie (sozusagen bürotaugliche) Darstellung ergänzen sollten. Zu dieser Zeit hatte allerdings der IBM-PC-kompatible Computer zusammen mit Microsoft Windows schon seinen Siegeszug in die Büros der Welt angetreten.\n\nDie wohl bekannteste Hardwareerweiterung für den Amiga ist neben einer 512 KB großen Speichererweiterung für den A500 der sogenannte Scandoubler (mit integriertem Flickerfixer). Die ersten Modelle des Amiga (1000, 500 und 2000) können ausschließlich Videosignale entsprechend dem PAL- oder NTSC-Standard erzeugen, die mit einer Zeilenfrequenz von 15,625 kHz arbeiten. Höhere vertikale Auflösungen als ca. 256 Pixel sind damit nur durch Verwendung eines Zeilensprungverfahrens möglich, was zu heftigem Flimmern der Darstellung führt. Um dennoch VGA-Monitore ansteuern zu können, wurde der Scandoubler/Flickerfixer entwickelt, der die Zeilenfrequenz der PAL-Modi verdoppelt, die beiden in den Interlace-Auflösungen ausgegebenen Halbbilder zu einem Einzelbild zusammenfügt und sie mit der für diese Monitore erforderlichen doppelten Zeilenfrequenz von 31 kHz ausgibt.\n\nAmiga 500 Plus, 600 und 3000 konnten durch ihren erweiterten ECS-Chipsatz von sich aus mit unterschiedlichen Zeilenfrequenzen umgehen, müssen dabei allerdings mangels Speicherbandbreite Kompromisse eingehen, was die Auswahl der darstellbaren Farben angeht. Um den Amiga 3000 auch in Büroumgebungen einsetzen zu können, wurde hier ein Scandoubler/Flickerfixer bereits ab Werk eingebaut. Auch bei den neueren Modellen Amiga 1200 und 4000 war der Scandoubler/Flickerfixer eine beliebte Erweiterung. Aufgrund des nochmals erweiterten AGA-Chipsatzes waren diese Rechner zwar in der Lage, VGA-ähnliche Bildschirmmodi darzustellen, aber wegen des fehlenden Flickerfixers flimmerten die Interlace-Modi stark, was ein professionelles Arbeiten praktisch unmöglich machte, und zahlreiche Spiele, die direkt auf den AGA-Chipsatz zugriffen, zwangen den Amiga in einen 15,625-kHz-PAL-Modus.\n\nVorrangig in den USA benutzten Filmstudios und Fernsehsender den Amiga zusammen mit einer \"Video Toaster\" genannten Hardwareerweiterung für die tägliche Arbeit z. B. für die Einblendung von Logos. Der \"Video Toaster\" war jedoch nur für den NTSC-Betrieb ausgelegt und konnte wegen spezifischer Hardwarebesonderheiten nicht für den PAL-Betrieb entwickelt werden.\n\nBekannt ist die Fernsehserie \"Babylon 5,\" deren mit einem Emmy ausgezeichneten Spezialeffekte teilweise mit Amiga-Rechnern und dem Programm Lightwave 3D ebenso wie bei der Fernsehserie \"SeaQuest DSV\" erzeugt wurden.\n\nDie ersten Festplatten für den Amiga waren ST506-, später dann SCSI- und ATA-Festplatten.\n\nBei schnellen Amigas können bei einigen internen ATA-Festplatten Probleme auftreten: Nach einem Reset fragt der Amiga die Hardware ab, noch bevor die Festplatte dem Rechner antworten kann, da sie noch nicht bereit ist. Daher wird die Festplatte vom System nicht erkannt. Mit einem Trick schaffen sich manche Benutzer Abhilfe: Die Resetleitung zur Festplatte wird durchtrennt. Sofern die anderen Pins dabei unversehrt bleiben, führt die Festplatte nach dem Einschalten selbsttätig einen Einschalt-Reset durch, nicht jedoch beim Reset beim Drücken beider Amiga-Tasten und der Ctrl-Taste. Führt man dann nach dem Einschalten des Amiga nach kurzer Wartezeit (eine Sekunde ist ausreichend) einen Tastatur-Reset durch, wird so die Festplatte erkannt. Je nach Festplatte ist das Durchtrennen überflüssig, da manche Festplatten nur nach dem Einschalten längere Zeit für das Einrichten ihrer Schnittstelle benötigen, nach einem Reset-Signal des Rechners jedoch schneller bereit sind.\n\nTechnisch war der Amiga vielen Computern seiner Zeit voraus. Neben den herausragenden technischen Eigenschaften (z. B. Plug and Play in Form des \"Auto-Config-Mechanismus\"), unterstützte das Betriebssystem bereits präemptives Multitasking im priorisierten Round-Robin-Verfahren.\n\nDas Betriebssystem des Amiga, das AmigaOS, ist modular aufgebaut und in einigen Aspekten Unix ähnlich. AmigaOS hat dynamisch nachladbare Geräte-Treiber sowie \"Shared Libraries\" und unterstützt bereits viele Konzepte moderner Betriebssysteme (Streams, Pipelining, Signals, Message-Queues usw.). Der Kommandozeileninterpreter (CLI, ) wurde später, nach diversen Erweiterungen, in die bei Unix übliche Bezeichnung \"Shell\" umbenannt. Von Anfang an war AmigaOS ein 32-Bit-taugliches Betriebssystem, obwohl die anfängliche Hardware eher ein 16/32-Bit-System war.\n\nDas gesamte Betriebssystem des Amiga passte zunächst auf zwei Disketten, die Kickstart- und die Workbench-Diskette. Als der Kickstart soweit stabilisiert war, dass er in ein ROM verlegt werden konnte, reichte eine einzige Diskette von 880 Kilobyte Kapazität. AmigaOS bietet eine farbige grafische Oberfläche mit Multitasking und relativ kurzen Reaktionszeiten z. B. auf Benutzereingaben. Bedingt durch die CPU-Architektur, gibt es keinen Speicherschutz, d. h. die Prozesse sind nicht untereinander abgeschottet, und jedes Programm kann bei einem schweren Fehler das gesamte System zum Absturz bringen. Auf der anderen Seite ermöglicht das einfache Speichermodell eine schnelle Interprozesskommunikation durch einfache Übergabe von Zeigern, ohne Daten zu kopieren. Die Geschwindigkeit des Betriebssystems wurde über die Jahre durch diverse Verbesserungen noch gesteigert.\n\nDas AmigaOS bietet seit 1986 eine dynamische RAM-Disk, die als \"RAM:\" wie ein gewöhnliches Laufwerk ansprechbar ist. Durch die RAM-Disk können Dateioperationen enorm beschleunigt werden, da die langsamen Zugriffe auf Disketten oder Festplatten entfallen. Das AmigaOS benutzt die RAM-Disk standardmäßig für das temporäre Verzeichnis, für Umgebungsvariablen und als Zwischenablage, ansonsten kann sie frei verwendet werden. Ab Kickstart 1.3 gibt es die Möglichkeit, eine resetfeste RAM-Disk \"RAD:\" einzubinden, die nicht dynamisch ist, also eine feste Größe hat, bootfähig ist und nach einem Neustart mit allen vorher gespeicherten Daten zur Verfügung steht. Genügend Arbeitsspeicher vorausgesetzt, kann \"RAD:\" z. B. exakt die Größe einer Diskette haben.\n\nDer Amiga kann verschiedene Dateisysteme verwenden. Ursprünglich wurde das Amiga File System genutzt (später \"OFS\" mit O von engl. \"old\" oder \"original\"). Mit der Version 1.3 des Betriebssystems wurde eine verbesserte Version namens Fast File System (FFS) ausgeliefert. Beide gelten als sehr robust. Da das Betriebssystem modular aufgebaut ist, ist es leicht, Unterstützung für weitere Dateisysteme hinzuzufügen; neben einem Treiber für das von MS-DOS verwendete FAT-System wurden von anderen weitere Dateisysteme insbesondere für die Verwendung mit Festplatten entwickelt. Bei Disketten wurde nicht nur das Einlegen und Entfernen automatisch erkannt, sondern auch das genutzte Dateisystem. Disketten sowie jedes andere Laufwerk können über den Namen des Datenträgers angesprochen werden. Ein weiterer Vorteil des Multitasking-Betriebes war, dass man bis zu vier Disketten gleichzeitig formatieren konnte.\n\nAls Festplatten noch sehr teuer waren und eher die Ausnahme bildeten, wurden Daten hauptsächlich auf 3,5-Zoll-DD-Disketten mit einer Speicherkapazität von 880 Kilobyte gespeichert. Mit dem FFS können auch HD-Disketten mit der doppelten Kapazität beschrieben werden. Allerdings sind beim Original-Controller dazu spezielle Laufwerke erforderlich, die HD-Disketten mit halber Drehzahl antreiben, da er nicht die normalerweise mit HD-Disketten verbundene doppelte Datenrate unterstützt.\n\nMit der Bibliothek \"translator.library\" und dem Treiber \"narrator.device\" wurde die Möglichkeit integriert, von höheren Programmiersprachen aus Sprachausgabe zu verwirklichen. Der Amiga war einer der ersten Rechner, der serienmäßig mit Software zur Sprachsynthese ausgeliefert wurde. Das wurde möglich, weil die Audioausgabe des Amiga auf Pulse Code Modulation (PCM) basiert und somit (abgesehen von der Ausgabequalität), wie heutige PCs, jeden beliebigen Klang ausgeben kann – die meisten anderen Rechner der damaligen Zeit boten, wenn überhaupt, nur Synthesizer-Chips, die auf bestimmte Klänge beschränkt waren.\n\nWegweisend war später der konsequente Einsatz sogenannter \"DataTypes\" – das sind Codecs, die eine einheitliche Schnittstelle zum Laden und Speichern aller gängigen Dateiformate anbieten. Bei Entwicklung eines neuen Dateiformates braucht der entsprechende \"Datatype\" nur dem Betriebssystem bekanntgemacht werden. Sämtliche Programme, die die \"Datatypes\"-Schnittstelle unterstützen, können dann dieses Dateiformat lesen bzw. schreiben.\n\nDie Grafische Benutzeroberfläche (GUI) des AmigaOS zeichnete sich durch eine – für damalige Verhältnisse – sehr intuitive Bedienung aus.\n\nInsbesondere die aus der Public-Domain-Szene stammende GUI-Erweiterung Magic User Interface (MUI) war beliebt; mit ihr standen auf dem objektorientierten BOOPSI-System basierende Gadgets (entspricht: Widget) zur Verfügung. Das machte alle Elemente der Oberfläche inkl. der Beschriftungen z. B. beliebig in der Größe skalierbar, also an jede Grafikkartenauflösung automatisch anpassend.\n\nAls grundlegender Text-Zeichensatz wurde der 8-Bit-Zeichensatz ISO 8859-1 gewählt, wodurch der internationale Einsatz ermöglicht wurde und eine zumindest teilweise Kompatibilität zu Windows hergestellt wurde. Durch ladbare andere Zeichensätze (engl. \"fonts\") konnten weitere Schriften unterstützt werden.\n\nGerade in der Anfangsphase des Amiga wurden viele Programme über Tauschbörsen oder Amiga-Magazine vertrieben. Besonders ein Mann, Fred Fish, hat sich große Verdienste durch seine regelmäßig erscheinenden \"AmigaLibDisks\" erworben, allgemein als \"Fish-Disks\" bekannt.\n\nNennenswerte Software für den Amiga ist unter anderem:\n\nBeim nicht in Deutschland erschienenen AmigaOS 1.0 wurde noch der Basic-Interpreter \"ABASIC\" von MetaComCo mitgeliefert. Mit AmigaOS 1.1 bis 1.3 wurde AmigaBASIC ausgeliefert, das einzige Programm, das Microsoft jemals für den Amiga entwickelte. Unter späteren Systemversionen versagte es teilweise den Dienst.\n\nAls systeminterne Skriptsprache wird REXX verwendet (seit AmigaOS 2.0 Teil des Amiga-Betriebssystems). Die \"ARexx\" genannte Amiga-Version dieser Skriptsprache bietet unter anderem die Möglichkeit, Programme extern über ARexx-Scripts zu steuern. Beispielsweise kann man damit den Ablauf bestimmter Funktionen eines Programms über ein Skript angeben und somit dem Programm neue Funktionen zuordnen, wie beim Eagleplayer geschehen.\n\nSehr früh wurden auf dem Amiga Vernetzung (LAN) und Internet (TCP/IP-Stacks) eingeführt. Der erste Webbrowser für den Amiga war AMosaic, ein Port des bekannten Webbrowsers Mosaic. AMosaic wurde später in IBrowse umbenannt. Als zweiter Browser kam Voyager auf den Markt, von Amiga Technologies auch zusammen mit dem Amiga 1200 im sogenannten \"Surfer Bundle\" als \"Mindwalker\" verkauft. Als dritter Browser trat schließlich AWeb auf.\n\nDer Amiga wurde, außer zum Spielen (was beim Amiga 500/1200 eher der Fall war), hauptsächlich zum Bearbeiten von Videos benutzt. Hier stellte das Schnittprogramm MovieShop lange Zeit einen Quasi-Standard dar, entsprechende Kurse wurden z. B. an der Münchener Akademie der Bildenden Künste angeboten. Weitere wichtige Anwendungen waren 3D-Animation (s. u.), Musik (Tracker wie The Ultimate Soundtracker, FutureComposer u. ä. genießen heute noch Kultstatus). In den letzten Jahren kamen auch noch Anwendungen wie das Authoring hinzu. Bekannteste Vertreter: AmigaVision, eine Autorensoftware für die Erstellung von interaktiven CDs, zur Wiedergabe von Laserdiscs und für Karaoke-Anwendungen und Scala, dessen leistungsfähigste Version, Info Channel, auch heute noch in Kabelfernsehanlagen eingesetzt wird.\n\nDie heute noch (auf Windows und MacOS) erfolgreichen 3D-Grafiksoftware Maxon Cinema 4D und LightWave 3D sowie das Audio-Programm Samplitude (und viele andere) hatten ihren Ursprung auf dem Amiga.\n\nVor allem bei den genannten Grafikanwendungen stellte es sich als Vorteil heraus, dass Amiga frühzeitig in einer Kooperation mit dem Unternehmen Electronic Arts einen übergreifenden Standard für Dokumentdateien definiert hatte, das Interchange File Format, kurz IFF. Mit ihm konnten nicht nur Grafikdaten, sondern auch Audio-, Text- oder komplexe Multimediadaten in einer logischen und sinnvoll gleich strukturierten Weise gespeichert werden. Die Vorteile des Formats waren so offensichtlich, dass kaum ein Softwarehersteller Sonderwege einschlug und es nicht benutzte. Als einige Jahre nach Erscheinen des Amiga die Grafikanimation immer bedeutender wurde, wurde der Standard organisch auf entsprechende Inhalte erweitert. Spätere Versionen des AmigaOS enthielten auch Unterstützung zur Verarbeitung dieses Formats, so dass Programmierer auf einer soliden Basis aufsetzen konnten.\n\nFür die Amiga-Plattform erschienen im Laufe der Jahre über 3000 kommerzielle Spiele sowie Hunderte von Public-Domain-Spielen. Vor allem die populärste aller Amiga-Varianten, der Amiga 500, galt als der Spielecomputer schlechthin.\n\nAls erstes Spiel wird oft \"Mindwalker\" bezeichnet, da es den ersten Amiga 1000 beilag. Allerdings erschienen zeitgleich auch u. a. Textadventures des Unternehmens Infocom für den Amiga. 1986 wurde mit Defender of the Crown ein Spiel mit herausragender Grafik veröffentlicht, das zum ersten Mal von den Fähigkeiten des Systems Gebrauch machte. 1987 erschienen Umsetzungen der beliebten Adventurespiele King’s Quest, Police Quest und Space Quest des Unternehmens Sierra On-Line, die so erfolgreich waren, dass sie jeweils viele Fortsetzungen nach sich zogen. The Great Giana Sisters vom deutschen Spieleentwickler Rainbow Arts stand 1988 wegen eines Rechtsstreits aufgrund seiner Ähnlichkeit zu Super Mario Bros. von Nintendo nicht länger als eine Woche in den Regalen der Geschäfte und dürfte damit das am kürzesten auf dem Amiga-Markt erhältliche Spiel gewesen sein. Im selben Jahr erschienen der Maßstäbe setzende Flugsimulator Falcon und David Brabens Kultklassiker vom BBC, Elite. Shadow of the Beast legte 1989 die Messlatte durch ruckelfrei in mehreren Ebenen bewegte Grafik (sogenanntes Parallax-Scrolling) nochmals höher, konnte spielerisch jedoch nicht überzeugen. Dafür sorgten die Konvertierung des Kultadventures Maniac Mansion von Lucasfilm Games und das Fußballspiel Kick Off sowohl für Spielspaß als auch hohe Verkaufszahlen. Die Anzahl der zuerst für den Amiga entwickelten Spiele nahm in den folgenden Jahren jedoch immer weiter ab, vor allem aufwendige Produktionen erschienen in den 1990er Jahren fast immer zuerst für MS-DOS oder Microsoft Windows und wurden allenfalls danach auf den Amiga konvertiert.\n\nFür ein innovatives Spielkonzept stand 1990 insbesondere Lemmings, für Lobeshymnen sorgten zudem das Rennspiel Lotus Esprit Turbo Challenge und Speedball 2. Bahnbrechend waren 1991 die Konvertierung des IBM-PC Adventures The Secret of Monkey Island und das Actionspiel Turrican II; zudem brachte das deutsche Softwarehaus Software 2000 mit dem Bundesliga Manager Professional ein Spiel heraus, das sich über 100.000 Mal verkaufte. Das Fußballspiel Sensible Soccer, das Grafikadventure sowie die beiden sehr erfolgreichen Flippersimulationen Pinball Dreams und Pinball Fantasies, die alle 1992 erschienen, gelten noch heute als herausragende Beispiele an Spielwitz und zogen mehrere Fortsetzungen – auch für andere Plattformen – nach sich. Zudem erschien im selben Jahr die zuvor für technisch nicht möglich gehaltene Umsetzung des PC-Hits Wing Commander, die jedoch auf dem verbreiteten Amiga 500 wegen des starken Ruckelns nicht spielbar war. Ab 1993 erschienen vermehrt grafisch verbesserte Versionen für die ein Jahr zuvor neu hinzugekommenen Amiga-Modelle A1200 und A4000 mit AGA-Chipsatz. In diesem Jahr sorgten das innovative Aufbauspiel Die Siedler, die Actionspiele und The Chaos Engine sowie das Hit-Adventure Indiana Jones and the Fate of Atlantis für Höchstwertungen der Fachpresse und verkauften sich entsprechend. Letzteres war – auch aufgrund der Schwarzkopier-Problematik – das letzte Spiel von LucasArts für den Amiga. Im Jahr 1994 erschienen mit Sensible World of Soccer, Theme Park und SimCity 2000 die letzten großen Spielehits. Biing! erschien 1995 wegen der damals noch recht geringen Verbreitung von CD-Laufwerken auch auf 19 Disketten (in der AGA-Version) – das Installationsprogramm zum Spiel enthielt selbst ein kleines Spiel, um die Wartezeit erträglich zu machen. Im selben Jahr erschienen mit Alien Breed 3D und Gloom zwei Spiele, die vom Erfolg des PC-Ego-Shooters Doom profitieren wollten, sowie das erfolgreiche Rennspiel Super Skidmarks und das Actionspiel Virocop von Entwickler-Legende Andrew Braybrook. Ab 1996 hatten sich die meisten bekannten Softwarehäuser vom Amiga abgewendet und entwickelten ausschließlich für PC und Konsolen. Selbst qualitativ hochwertige und von der Fachpresse gelobte Spiele wie der Knobel-Plattformer The Humans III, das Beat ’em Up Fightin’ Spirit, die Wirtschaftssimulation Mag!!!, das Fußballspiel Sensible World of Soccer 96/97 und die Flippersimulation Slamtilt konnten nur noch in geringen Stückzahlen verkauft werden, insbesondere wegen des anhaltenden Siegeszuges von Sonys PlayStation. Bis Ende der 1990er Jahre erschienen zwar noch vereinzelt kommerzielle Spiele kleinerer Entwickler, allerdings nur noch für AGA-Amigas, insbesondere z. B. vom Publisher Vulcan Software. und der kanadischen Firma clickBOOM. Letztere portierte unter anderem die bekannten PC-Spiele Quake und Myst für AGA-Amiga-Rechner (1200 und 4000) bzw. Amiga-Rechner mit einer Grafikkarte.\n\nEine Aufzählung weiterer populärer Spiele-Titel findet sich in der .\n\nZwar war der Commodore Amiga vor allem mit den ersten Modellen 1000, 2000 und 500 seiner Zeit in puncto Grafik, Sound und Multitasking voraus – dennoch konnte er sich gegen den PC im Bürobereich nie durchsetzen.\n\nSpätestens bei der Markteinführung des Amiga 500 erhielt der Amiga den Ruf eines in erster Linie für Spiele geeigneten Computers. Er wurde sehr beliebt unter Jugendlichen, die zwar viel mit dem Amiga spielten, aber eher selten für die Software bezahlten. Der unzulässige Austausch von Kopien von Hand zu Hand, z. B. auf Schulhöfen, erlebte damals eine Hochkonjunktur. Für ernsthaftere Anwender belastete dieses Bild den Amiga allgemein; er wurde es trotz mancher Korrekturversuche nie richtig los; das für professionelle Anwendungen gedachte Parallelmodell Amiga 2000 änderte daran wenig. Viele Nutzer des PCs würdigten damals zwar die Vorzüge, hielten jedoch gebührenden Abstand zum Amiga. Das eher schwache Marketing von Commodore tat sein Übriges.\n\nDie vergleichsweise einfache Erstellung unzulässiger Kopien führte neben Imageproblemen insbesondere zu einer abnehmenden Zahl an Spieleveröffentlichungen. Spielehersteller konnten mit Konsolenversionen für das Mega Drive und das SNES deutlich mehr Spiele verkaufen, da hier das private Erstellen von Kopien praktisch unmöglich war. Mitte der 1990er lohnte sich selbst das Konvertieren eines Konsolenspiels auf den Amiga wegen zu geringer Verkaufszahlen nur noch in Einzelfällen.\n\nAußerdem war der Amiga nur im Bereich der kombinierten Grafik wirklich schnell. Bei den klassischen Aufgaben in der Bürowelt zählte die Rechengeschwindigkeit mehr als grafische Fähigkeiten. Hier konnte der Amiga zwar gut mithalten (z. B. gegenüber dem Intel 80286), war aber nicht so viel schneller, um einen Umstieg zu begründen. Viel Standardsoftware war für den Amiga nicht oder zu spät verfügbar. Selbst im Bereich Grafik hatte es z. B. der Bereich Konstruktion schwer, denn die für diesen Zweck nötigen hohen Bildauflösungen konnten zunächst nur per Zeilensprungverfahren dargestellt werden.\n\nDie Modelle mit dem sogenannten OCS (Original Chip Set) – die Basismodelle 1000, 500 und 2000 – erreichten nur Bildwiederholfrequenzen bis 50 Hz, da sie speziell für die Benutzung mit nach der PAL-Norm arbeitenden Standardmonitoren konzipiert waren. Die maximale Auflösung von 640 × 512 Pixel wäre zu dieser Zeit ausreichend gewesen, die Zeilenzahl von 512 war jedoch nur eine theoretische Angabe. Sie erforderte den Bildaufbau mittels abwechselnder Halbbilder (Zeilensprungverfahren oder Interlace), es wurden also – wie beim Fernsehen – abwechselnd die geraden und die ungeraden Zeilen dargestellt, um auf die nötige Frequenz von 50 Hz zu kommen. Das Interlacing machte sich vor allem bei kontrastreichen horizontalen Linien, wie sie auf Programmoberflächen häufig vorkommen, negativ bemerkbar, weshalb lange Bildschirmarbeit äußerst anstrengend und ermüdend war. Deshalb waren die Oberflächen professioneller Werkzeuge entsprechend angepasst, d. h. auf kontrastreiche Bildpartien und Bedienelemente wurde weitestgehend verzichtet.\n\nBei den amerikanischen Modellen sind die Zahlen leicht abweichend (NTSC-Norm). Die maximale Auflösung lag bei 640 × 400 bzw. 200 Pixeln, die maximale Bildwiederholfrequenz bei 60 Hz.\n\nWegen der Zeilenfrequenz von nur 15,6 kHz war das Anschließen von VGA-Standardmonitoren nicht ohne Zusatzgeräte möglich. Das konnte mit einem Scandoubler (der bei den meisten erhältlichen Geräten mit einem Flickerfixer kombiniert war) erreicht werden, der die Zeilenfrequenz verdoppelte. Alternativ konnten die teureren Multisync- bzw. Multiscan-Monitore verwendet werden.\n\nCommodore war sich der Mängel beim Büroeinsatz bewusst und entwickelte deshalb einen speziellen sogenannten Hedley-Monitor (A2024), der mittels Digitizer, internem Framebuffer und einem speziellen Monitortreiber ein hochauflösendes Graustufen-Bild aus vier (bzw. sechs) Video-Einzelbildern des Amiga aufbaute. Wegen des vergleichsweise hohen Preises und der eingeschränkten Verwendbarkeit (keine Farbe) fand dieser Monitor keine weite Verbreitung.\n\nEin Manko des Amiga 1200 bestand seinerzeit darin, dass er lediglich ein DD-Diskettenlaufwerk für 880-Kilobyte-Disketten besaß, was dem Speicherplatzbedarf damaliger Spiele nicht angemessen war.\n\nEtwa jedes zweite Amiga-500-Spiel hatte damals einen Umfang von ca. 2 bis 3 Disketten, viele auch mehr, die während des Spiels irgendwann gewechselt werden müssen. Die Installation auf eine Festplatte war eher die Ausnahme. Die favorisierte Amiga-HD-Diskette mit 1,76 MB Kapazität wäre evolutionär genau in diese Nische gefallen und wurde von vielen erwartet; weniger das optionale CD-ROM-Laufwerk oder gar PCMCIA-Speicherkarten, die damals noch eher revolutionär anmuteten. CD-ROMs haben zudem den Nachteil, nur gelesen werden zu können, was ein Speichern von Spielständen kompliziert gemacht hätte.\n\nDer nötige universelle Floppy-Controller, der mindestens das 1,76-MB-Format unterstützt (in zudem auch höherer Geschwindigkeit), der das Problem hätte lösen können, erschien erst später in Gestalt der Zusatzhardware Catweasel MK2 mit voller Amiga-3½″-880/1760-kB-Unterstützung bis zu Catweasel Extra 3½″ 1160/2380 kB. Diese Ersatzlösungen von Drittherstellern waren jedoch nie bootfähig und funktionieren nur über die Workbench. Commodore selbst hatte den Einsatz von 4-MB-Rohdisketten (2,88 MB IBM-formatiert) erst für den AAA, den Nachfolge-Chipsatz des AGA, eingeplant.\n\nDie Abwärtskompatibilität des A1200 zum A500 wird über ein Bootmenü hergestellt, das startet, wenn beide Maustasten beim Booten des Rechners gleichzeitig gedrückt werden. Vollständige Kompatibilität ist dabei jedoch nicht gegeben und einige der alten Amiga-500-Programme liefen daher darauf nicht. Findige Hacker stellten die Kompatibilität inkompatibler A500-Spiele zum A1200 mitunter durch Patches her, die sogenannten \"AGA-Fixes.\"\n\nEin weiteres Problem waren die lediglich 2 MB ChipRAM; dies war zwar eine Vervierfachung des Amiga-500-Speichers, jedoch nicht genug, um das AmigaOS sowie zusätzlich ein installiertes Spiel zu starten und im RAM zu halten. Das nachträgliche Installieren von Spielen funktioniert praktisch erst reibungslos mit WHDLoad, für das 8 MB, besser 16 MB FastRAM empfohlen werden.\n\nLetzten Endes lag der Hauptgrund am Scheitern des Amiga an Fehlern des Commodore-Managements. So wurden die hohen Gewinne, die Commodore mit dem Amiga eine Zeit lang machte, nicht in erfolgversprechende Neuentwicklungen reinvestiert.\n\nDaraus entstanden dann am Markt vorbeizielende Entwicklungen wie der Amiga 500+ und der Amiga 600, die sich beide technisch nicht genug von den Vorgängermodellen abhoben, um den Erfolg des Amiga 500 fortsetzen zu können. Beim CDTV war die Entwicklung innovativ, aber nicht zu Ende gedacht und ebenfalls technisch zu schwach. Zudem kam es zu spät, um den schon etablierten Konsolen Marktanteile abzuluchsen. Diese Fehleinschätzungen der Verantwortlichen kosteten viel Kapital und Marktanteile. Die Einführung der technisch besseren Modelle A1200 und A4000 erfolgte zu einem Zeitpunkt, an dem sich viele Kunden schon vom Amiga abgewandt hatten. Verlorene Marktanteile und die hohen Entwicklungskosten für neue Geräte mündeten schließlich in eine Krise.\n\nEin weiteres Problem für den Amiga war die zunehmende Verbreitung des IBM-kompatiblen PC in Privathaushalten. Die (geplante) Nutzung von Bürosoftware war meist kein Verkaufsargument mehr für den Amiga. Neben den Rechen- entwickelten sich auch die Grafik- und Soundfähigkeiten von DOS-PCs deutlich schneller als die der Amiga-Familie. Spätestens mit VGA-Grafik und Sound Blaster-Sound kamen IBM-kompatible PCs dem Amiga auch bei den Spieleigenschaften sehr nahe, boten aber zusätzlich noch geeignete Auflösungen und Bildwiederholraten für Textverarbeitung und Office. Wurden Strategiespiele und Simulationen in der Blütezeit des Amiga oft zuerst für diesen herausgebracht, so erschienen diese in den 1990ern zunehmend als erstes für den DOS-PC.\n\nObwohl das Mutterunternehmen Commodore bereits 1994 liquidiert wurde, wurde die Entwicklung des Amiga nie ganz beendet.\n\nDurch die auf Commodore folgende Rechteinhaberin, die deutsche ESCOM AG, eine PC-Handelskette aus Heppenheim, wurde das neue Unternehmen \"Amiga Technologies\" als GmbH in Bensheim gegründet, das die Modelle Amiga 1200 und Amiga 4000T neu auflegte und vertrieb. Das geplante Nachfolgemodell Walker (inoffiziell auch als Amiga 1300 bezeichnet), das bereits auf der CeBIT 1996 vorgestellt wurde, ist jedoch nicht mehr erschienen; ESCOM ging in Konkurs. Auch das geplante Advanced Amiga Architecture Chip Set (AAA oder Triple-A Chip Set) hat daher nur noch legendenhaften Charakter.\n\nEin Übernahmeversuch von VisCorp – unter der Leitung des heutigen Genesi-CEO Bill Buck – scheiterte nach langwierigen Bemühungen.\n\n1997 übernahm der PC-Direktversender Gateway 2000 die Amiga-Rechte und vertrieb die vorhandene Hardware über dessen neu gegründete Tochtergesellschaft \"Amiga International\" in Deutschland weiter. Bis zu seiner Vorstellung auf der World of Amiga ’99 in London entstand unter Führung des Amiga-International-Präsidenten Jim Collas das Modell eines neuen Amiga-Rechners, des Amiga MCC (Amiga Multimedia Convergence Computer). Geplant war ein Rechner mit ATX-Motherboard, der mit einer Transmeta-CPU ausgestattet werden und durch Standard-Hardware erweiterbar sein sollte. Das AmigaOS sollte durch ein von der Firma QNX entwickeltes AmigaOE (Amiga Operating Environment) ersetzt werden, bevor im Juli 1999 entschieden wurde, stattdessen Linux einzusetzen. QNX stellte daraufhin die hinter dem neu entwickelten Betriebssystem steckende Technologie für Amiga-Entwickler zur Verfügung. Das Gehäuse des Amiga MCC wurde von der US-amerikanischen Design-Firma Pentagram entworfen, die unter anderem auch für Apple, die Coca-Cola Company und Disney gearbeitet hatte. Unvermittelt wurden Ende September 1999 die Offenen Briefe an die Amiga Community, die Collas regelmäßig auf der Amiga-Website veröffentlicht hatte, entfernt. Fast gleichzeitig verschwanden die E-Mail-Adressen der amerikanischen Mitarbeiter von der Website, und alle Diskussionsforen wurden geschlossen. Kurz darauf erklärte Amiga International, dass Collas aus persönlichen Gründen von seinem Amt als Präsident der Firma zurückgetreten sei.\n\nIm Jahr 2000 wurde Amiga, mit Ausnahme der Rechte an den Ideen zum Amiga MCC und den Amiga Objects sowie der Patente, an ein Unternehmen ehemaliger Gateway-Mitarbeiter namens \"Amino Development\" verkauft, das dann in \"Amiga Inc.\" (in USA) umfirmiert hat.\n\nVon ehemaligen Entwicklungsingenieuren und Managern der Amiga Technologies wurde auch das Unternehmen \"PIOS Computer AG\" gegründet, das später zur \"Metabox AG\" umfirmiert wurde. Zunächst wurde bei PIOS-Metabox die Idee von PowerPC-basierten Rechnern bzw. Powermac-Clones auf CHRP-Basis verfolgt. Diese Idee scheiterte an der geänderten Lizenzpolitik von Apple, so dass später lediglich noch Turbokarten für Mac-Rechner gefertigt wurden. Inspiriert von phase5, die ab 1996 ebenfalls PPC-Turbokarten für Powermacs und Amiga-Rechner entwickelten, startete Metabox mit der \"AmiJoe\" (basierend auf der \"joeCard\") eigene Anstrengungen. Später wurde der Einstieg in den Set-Top-Box-Markt versucht, der in einem Fiasko für Metabox und deren Entwickler endete. Teilweise floss die begonnene Software-Entwicklung jedoch in MorphOS ein – der Open-Source-AmigaOS-Clone AROS stellt hier eines der Bindeglieder dar.\n\nDie PowerPC-Anstrengungen von phase5 endeten zwar mit dem Konkurs des Unternehmens – die Karten wurden danach aber noch eine Zeit lang von DCE gefertigt und verkauft. Die bereits verkauften Cyberstorm- und Blizzard-Prozessorkarten können mit den verschiedensten Betriebssystemen betrieben werden. Neben OS3.9, das diese Hybride unterstützt, gab es das Powerup System, das von Phase5 für genau diese Karten entwickelt wurde (eine Art System-Plug-In ins vorhandene AmigaOS 3.x). Das konnte sich – trotz der besseren Speicherverwaltung – nicht gegen das aufkommende WarpOS von Haage&Partner durchsetzen. Angepasst wurden u. a. auch AmigaOS4, NetBSD, Linux, Morphos.\n\nDas Unternehmen Amiga, Inc. konzentrierte sich zunächst auf die Entwicklung des AmigaDE \"(Digital Environment)\" auf Basis von TAO/Intent sowie dem zugehörigen SDK für Windows- und Unix/Linux-Rechner. Danach folgte AmigaAnywhere unter anderem für Pocket-PC-basierte PDA-Systeme. Um dem ungebrochenen Interesse einer Weiterführung der jetzt \"Classic Amiga\" getauften Produktlinie nachzukommen, suchte sich Amiga, Inc. Partner für eine Neubelebung der Amiga-Plattform: Eyetech und Hyperion Entertainment.\n\nIm Jahr 2003 erschien schließlich die Hardware eines offiziellen Nachfolgers – des AmigaOne von Eyetech –, die statt der veralteten 680x0-CPUs von Motorola moderne PowerPC-CPUs enthält. Anfangs stand für diesen Computer nur Linux/PPC zur Verfügung – die erste öffentliche Version des von Hyperion Entertainment entwickelten neuen AmigaOS 4.0 wurde erst später, im Juni 2004, als Developer Pre-Release an die bisherigen Käufer ausgeliefert. Als inoffizielle Konkurrenz zum AmigaOne hat sich der – ebenfalls CHRP-basierte – Pegasos-Rechner von Genesi etabliert, der allerdings eher in der Tradition von phase5 und VisCorp zu sehen ist.\n\nMitte 2003 wurden die Rechte am Amiga-Betriebssystem durch das Unternehmen \"KMOS\" gekauft, im Juli 2004 wurde auch Amiga, Inc. von KMOS übernommen.\n\nAm 24. Dezember 2006 stellte das Unternehmen Hyperion nach fünf Jahren Entwicklung AmigaOS 4.0 fertig. Diese Betriebssystemversion läuft nativ auf PowerPC-Systemen.\n\nIm Mai 2007 kündigte Amiga Inc. zwei neue Rechner an. Das Einstiegsmodell sollte 489 US$ kosten, der große Rechner 1.498 $. Der Verkauf sollte ab Winter 2007 erfolgen. Dies geschah allerdings nicht.\n\nIm August 2008 veröffentlichte \"Hyperion Entertainment\" AmigaOS 4.1, wobei es sich um die erste Version von AmigaOS handelt, die eine reine PPC-Hardware voraussetzt und dadurch nicht mehr auf klassischen Amigas mit PowerUP-Erweiterung läuft. Die Version 4.1 war damit vorerst nur mit \"AmigaOne\"-Hardware zu gebrauchen, obwohl dieser zum Zeitpunkt der Veröffentlichung nicht mehr erhältlich war.\n\nWenige Monate später gaben die Unternehmen Acube Systems und Hyperion bekannt, dass AmigaOS 4.1 auf Rechnern mit einer aktualisierten Version des SAM440-Motherboards von Acube lauffähig ist. Da die Rechte der Marke Amiga bei dem Unternehmen Amiga, Inc. liegen und die Unterstützung der SAM440-Hardware von AmigaOS 4.1 nur durch die erfolgreiche Zusammenarbeit zwischen Hyperion und ACUBE entstanden war, gibt es Zweifel darüber, ob diese Lösung rechtens ist. Amiga, Inc. gab mehrfach bekannt, neue Amiga-Hardware von anderen Herstellern produzieren zu lassen, ohne diese Ankündigung umzusetzen, so dass Hyperion als Hersteller des Betriebssystems selbstständig nach einem Anbieter von geeigneter Hardware für sein Betriebssystem suchte. Inzwischen (Dezember 2009) hat Hyperion eine Klage gegen Amiga, Inc. gewonnen und hat damit die vollen Nutzungsrechte am AmigaOS 3.1, sowie den eigenentwickelten Versionen 4.0 und Folgesystemen, sowie an der Nutzung der Namen Amiga und AmigaOS, sowie des Boingball-Logos. Im Januar 2010 gab Hyperion bekannt, über die neu gegründete Tochterfirma A.Eon einen neuen Computer namens AmigaOne X1000 zu bauen. Bekannt ist bereits, dass dieser neue Rechner einen FPGA namens \"Xena\" haben wird, außerdem einen Prozessor aus der Familie des PowerPC. Verfügbar soll der neue Amiga-Nachfolger ab Ende 2011 sein. Acube hingegen stellte im September 2011 mit dem AmigaOne 500 ebenfalls ein Komplettsystem auf Basis des SAM460ex-Mainboards und AmigaOS 4.1 vor.\n\nDa es passieren kann, dass die originalen Diskettenlaufwerke kaputtgehen, haben Bastler Umbauanleitungen für diverse PC-Laufwerke erstellt.\n\nSeit 2014 existiert auch ein Amiga auf Basis einer Emulation auf einem ARM-System, genannt \"Armiga.\" Er verfügt, je nach Modell, über ein eingebautes 3,5-Zoll-Diskettenlaufwerk und kann einen Amiga 500 sowie einen Amiga 1200 nachbilden. Die enthaltenen 1.3-Kickstart-Roms sind offiziell lizenziert.\n\nAuf der Basis von frei reprogrammierbaren FPGAs gibt es auch immer wieder Versuche von Hobbyprogrammierern, Amiga-Rechner weitestgehend in Hardware nachzubilden. Ein Ableger dieser Versuche ist z. B. der \"Minimig\" von Dennis van Weeren, der heute für ca. 150 € angeboten wird. Dabei handelt es sich um einen mit 7,09 MHz (via OSD umschaltbar auf 49,63 MHz) getakteten MC68SEC000 mit 2 MB oder optional 4 MB S-RAM, wobei alle Zusatzchips des Amiga 500 in einem 400 Kgate Spartan-3 (XC3S400)-FPGA nachgebildet wurden. Als Laufwerk dient ein MMC-/SD-Flash-Kartenleser. Zusätzlich verfügt der Minimig über einen nachprogrammierten Scandoubler (Amber-Chip aus dem Amiga 3000) und ist somit wahlweise tauglich für VGA 31 kHz und PAL 15 kHz. Die meisten TFT-Bildschirme können das 31-kHz-Signal sauber anzeigen.\n\nIn Verbindung mit einem zusätzlichen ARM-Miniboard unterstützt der Minimig mittlerweile auch bis zu vier virtuelle Laufwerke, die optional mit mehrfacher Geschwindigkeit betrieben werden können, einen Turbo-Modus mit 4 kB CPU Cache und beschleunigtem Blitter sowie maximal 2 MB Chip- und 1,5 MB Slow-RAM. Die Kompatibilität wurde bei jeder neuen Version (frei verfügbar) verbessert, es wird auch aktuell (Stand 2013) weiterentwickelt.\n\nDie aktuelle Minimig-Firmware bietet dazu eine Unterstützung von \"Hard Drive Files\" (HDF) und der ECS-Agnus + Denise.\n\nMit Hilfe von FPGAs wurden auch schon der C64 im C-One oder das MSX im \"One Chip MSX\" neu aufgelegt. Der aktuelle C-One bildet zudem seit 2008 mit dem \"FPGA extender\" die Basis für einen leistungsfähigeren Minimig, der für die Amiga-Nachbildung keinen ARM-Chip mehr benötigt und dafür mit einem 68k-Soft-Core, dem TG68 von Tobias Gubener in einem größeren Cyclone-3-FPGA auskommt, der auch höhere Leistungen erzielt als der Original-68000.\n\nEin auf Minimig basierendes Projekt ist der \"FPGA Arcade\" bzw. \"MikeJ's Replay FPGA Board,\" mit dem auch versucht wird, zusätzlich zum \"Minimig Core\" den Original-AGA-Chipsatz des Amiga 1200 kompatibel nachzubilden, auch dafür wird ein \"TG68 Softcore\" eingesetzt, der trotz noch fehlender Befehle des originalen 68020 die fast doppelte MIPS-Leistung des A1200 erbringt. Auch auf dem MIST FPGA (für aMIga und atari ST) Board sind Minimig und Minimig AGA lauffähig. Im aktuellen MIST 1.3 Plus werden auch die Midi-Anschlüsse des Atari ST unterstützt. Der MIST ist in zahlreichen Onlineshops erhältlich und gegenwärtig die Referenz unter den Amiga-FPGA-Computern. Die Cores werden auf GitHub entwickelt.\n\nEin Nachfolgeprojekt der 'MISTer' setzt hingegen mit einem ca. vier mal stärkeren FPGA einem Intel (ex-Altera) Cyclone V mit 110K LEs (MIST: Cyclone III mit 25K LEs) dort an, wo dem MIST die Leistung ausging, so z. B. bei der Nachbildung des Atari Falcon, X68000 und auch Super Nintendo etc. Der Standard hierfür ist ein normales FPGA-Entwicklerbord, ein Terasic DE10-nano.\n\nMit FPGAs bietet sich heute ein großes Spielfeld, viele Originalfunktionen nachzubilden und die Abwärtskompatibilität beizubehalten, für Zusatzfunktionen und Geschwindigkeitssteigerungen z. B. auch der von Apollo entwickelte 68080 Core und Super AGA (SAGA-Chipsatz) für Amiga-Turbokarten. Eine Standalone-Version der Apollo Turbokarte wurde im Oktober 2018 vorgestellt. Diese besitzt gegenwärtig die Leistungskrone noch vor dem MIST, da hiermit nebst AGA und CD32 auch Ports von früheren PC-Spielen wie z. B. Doom oder Quake erstmals auf dem Amiga flüssig laufen. Der bekannte Benchmark SysInfo weist für die Apollokarte ca. die 4,04 bis 6,61-fache Leistung eines Amiga 4000 mit 25 MHz aus.\n\nReverse Engineering, die freie Kickstart-Alternative AROS wie auch Open-Source-Hardware gegenüber der originalen 68000er-Familie ersparen dabei den Erwerb der Original-Lizenzen und Rechte. Je nach Zielsetzung treibt man einfache Low-Cost-FPGAs aber auch schnell an die Grenzen, wodurch der Optimierungsaufwand steigt.\n\nNeben dem standardmäßigen AmigaOS existiert auch eine Anzahl von alternativen Betriebssystemen für den Amiga:\n\n\nAnmerkung zu MorphOS und zu dem dahinter stehenden Konflikt:\n\nDie Unternehmen Genesi und bplan haben die Pegasos-Mainboards auf den Markt gebracht, für die das Betriebssystem MorphOS geliefert wird. MorphOS ist ein Amiga-ähnliches PowerPC-Betriebssystem auf Microkernel-Basis. Es stellt neben MorphOS-spezifischen neuen Funktionen den größten Teil der AmigaOS3-API bereit und ist dadurch weitgehend Sourcecode-kompatibel und – soweit es die neue Hardware zulässt – binär-kompatibel zu AmigaOS 3 und AmigaOS-3-Anwendungen (zu AmigaOS-4-PPC-Anwendungen besteht eine eingeschränkte Binärkompatibilität über die alten AmigaOS-PPC-Kernel-Erweiterungen wie PowerUP und die os4emu-API-Emulation). MorphOS war ursprünglich als Nachfolger von AmigaOS 3 geplant, da eine offizielle Weiterentwicklung eine Zeit lang nicht sicher schien. Die Verhandlungen mit Amiga, Inc. über die Verwendung von MorphOS als neues PPC-AmigaOS scheiterten jedoch, und Amiga, Inc. entschied sich, AmigaOS 3 von Hyperion Entertainment auf die PowerPC-Plattform portieren zu lassen.\n\nMit DragonFly BSD existiert ein Abkömmling von FreeBSD, der von einem ehemaligen Guru der Amiga Community, Matthew Dillon, entwickelt wird. Das Dateisystem Hammer wurde laut Dillon ursprünglich vom Smart File System des AmigaOS inspiriert. Variant (context-sensitive) symlinks sind funktional vergleichbar mit Assignments in AmigaDOS, also der Zuweisung von Laufwerksbuchstaben an Dateipfade, und die Messaging Fähigkeiten des Kernels wären theoretisch gut geeignet um auch AmigaOS/AROS-Subsysteme im Kernel zu implementieren oder User-Mode-Prozesse Nachrichten in den Kernel senden zu lassen.\nAROS und DragonFly BSD würden theoretisch exzellent zusammenpassen, eine Kommerzialisierung wäre aber lizenzrechtlich nicht unproblematisch.\n\nDie Classic-Amiga-Reihe wird so korrekt wie möglich durch Emulatoren nachgebildet.\n\nDie ausgelieferten Amiga-Modelle wurden oben in der Historie behandelt. Eine vollständige Liste mit Links zu den ausführlichen Einzelartikeln findet sich in der Commodore-Produktübersicht. Hier seien nur noch Modelle mit PPC-CPU von Fremdherstellern oder Nachfolgern aufgeführt:\n\n\n\n\n\n"}
{"id": "17726", "url": "https://de.wikipedia.org/wiki?curid=17726", "title": "Lynx (Browser)", "text": "Lynx (Browser)\n\nLynx ist ein textbasierter Webbrowser, der unter Unix entwickelt wurde, inzwischen aber für fast alle Betriebssysteme verfügbar ist. Er eignet sich unter anderem für den Einsatz an reinen Text-Terminals ohne Maus.\n\nDer Browser wurde um 1992 an der University of Kansas entwickelt, ist heute noch verbreitet und zählt damit zu den Browsern mit der längsten Entwicklungsgeschichte. Bevor das Programm an das World Wide Web und dessen Protokoll http angepasst wurde, war Lynx ein Client für ein proprietäres Hypertextprotokoll. Dabei wurden Elemente von Gopher verwendet. Lynx wurde 1995 unter der Freie-Software-Lizenz GPL veröffentlicht.\n\nIn aktuellen Versionen erscheinen anstelle von Grafiken deren Alternativtexte im „codice_1“-Attribut des „codice_2“-Elements (soweit vorhanden) oder ihre Dateinamen. Wahlweise können Grafiken auch als Links dargestellt und über ein externes Programm betrachtet werden. Frames erscheinen als eine Reihe einzeln anwählbarer Links, zusammen mit dem Text des „codice_3“-Elements. Lynx unterstützt auch Cookies, clientseitige verweissensitive Grafiken (\"imagemap\") und Maus-Bedienung, Tabellen allerdings nur eingeschränkt und JavaScript überhaupt nicht.\n\nTextbrowser sind im Vergleich zu ihren grafikbasierten Konkurrenten sehr schnell.\n\nBeliebt ist auch die Möglichkeit, Lynx als HTML-Interpreter in Shell-Skripten zu nutzen, um bestimmte Webseiten automatisiert zu durchsuchen. Lynx findet auch Verwendung als Bestandteil von Webcrawlern. Beispielsweise können mit Lynx Skripte aufgezeichnet werden, die dann anschließend angepasst und erneut abgespielt werden können.\n\nIn Zeiten statischer Webseiten eignete sich Lynx gut, um Internetseiten auf ihre Lesbarkeit mittels Braillezeile oder Screenreader zu überprüfen, wie blinde Menschen sie verwenden. Heute sollte Lynx jedoch nicht mehr zum Prüfen verwendet werden. Auch im Bereich der Screenreader ist die Entwicklung vorangeschritten. Um einen realistischen Eindruck davon zu bekommen, wie eine Internetseite für blinde Menschen ausgegeben wird, sollte heute besser NVDA oder JAWS verwendet werden. Es kann sonst zu verfälschten Ergebnissen kommen.\n\n\n"}
{"id": "17731", "url": "https://de.wikipedia.org/wiki?curid=17731", "title": "Earth Simulator", "text": "Earth Simulator\n\nAls Earth Simulator (jap. , \"Chikyū shimyurēta\", dt. „Erdsimulator“) sind zwei japanische Supercomputer bekannt, die auf Vektorprozessoren basieren. Das erste Parallelrechnersystem wurde 2002 am \"Yokohama Institute for Earth Sciences\" (YES) von der Firma NEC in Betrieb gesetzt.\n\nFür den Earth Simulator wurde ab 1999 ein speziell für ihn konzipiertes 3.250 m² großes Gebäude im Stadtbezirk Kanazawa der Stadt Yokohama (Präfektur Kanagawa) errichtet. Das ursprüngliche System, 2002 fertiggestellt, basierte auf der SX-6-Architektur von NEC und bestand aus 640 Rechenknoten, zu denen je acht Vektorprozessoren und 16 GB Hauptspeicher gehörten, sowie 130 Kommunikationsknoten mit je einer Crossbar-Switchmatrix und Anschlüssen an alle 640 Rechenknoten. Die Kommunikation fand über ein 12,3 Gbps schnelles Verbindungsnetz statt. Insgesamt standen 5120 CPUs, 10 TB Hauptspeicher, 700 TB Festplatten- und 1,6 PB Bandlaufwerkspeicher zur Verfügung.\n\nIn der TOP500-Liste lag der Earth Simulator von Juni 2002 bis Juni 2004 mit einer Rechenleistung von 35,86 Teraflops als schnellster Rechner der Welt auf Platz 1. In der Liste vom November 2004 wurde er jedoch von gleich zwei Rechnern (IBMs BlueGene/L sowie SGIs Columbia) überholt und lag zuletzt im November 2008 nur noch an 73. Stelle. Seit Juni 2011 ist er nicht mehr gelistet.\n\nMitte November 2008 wurde der erste Earth Simulator stillgelegt und durch seinen Nachfolger mit dem offiziell gleichen Namen (ES2) ersetzt. Dieser basiert auf der SX-9/E-Architektur von NEC und besteht aus 160 Rechenknoten, zu denen je acht Vektorprozessoren und 128 GB Hauptspeicher gehören. Die Kommunikation erfolgt nun über ein Switchnetz mit Fat-Tree-Topologie, das 128 GB/s übertragen kann. Insgesamt können die 1280 CPUs auf 20 TB Hauptspeicher zurückgreifen.\n\nDie Neuauflage debütierte in der TOP500-Liste im Juni 2009 mit 122,4 TFLOPS auf Platz 22. Seit Juni 2014 ist er nicht mehr gelistet.\n\nLeitender Entwickler des Rechners war Tadashi Watanabe.\n\nDer ES2 wurde im März 2015 durch den Earth Simulator 3 (ES3) ersetzt, einem NEC SX-ACE-System mit 5120 Knoten.\n\n"}
{"id": "17732", "url": "https://de.wikipedia.org/wiki?curid=17732", "title": "Links (Browser)", "text": "Links (Browser)\n\nLinks ist ein freier (unter der GPL stehender) textbasierter Webbrowser für verschiedene Unix Betriebssysteme, Linux, OS/2 und Windows.\n\nDer tschechische Programmierer Mikuláš Patočka veröffentlichte 1999 eine erste Version des Webbrowsers \"Links\" und ist seitdem für das Projekt verantwortlich. Ursprünglich war der Name als Anspielung auf den Textbrowser Lynx gedacht, der im Englischen gleich ausgesprochen wird. \"Links\" ist in der Lage, Tabellen und Frames darzustellen, was im Vergleich zu \"Lynx\" bereits eine Verbesserung war.\n\nTextmodus-Browser, wie die der \"Links\"-Familie, \"Lynx\" und w3m, sind in der Regel auf zwei verschiedene Arten nutzbar:\n\nLaut Angaben des Entwicklers funktioniert der Browser zwar unter Windows, eine Garantie dafür oder Support bietet er jedoch nicht.\n\nDer originale \"Links\" (Version 1.x) wird nicht mehr weiterentwickelt. Als Nachfolger entstanden u. a. die Projekte Links2 und ELinks, die jeweils eigene Verbesserungen an \"Links\" vorgenommen haben. \"Links2\" kann im Grafikmodus auch Bilder anzeigen und wird von selben Entwickler (mit Karel Kulhavý, Petr Kulhavý und Martin Pergel) wie Links entwickelt.\n\n"}
{"id": "17751", "url": "https://de.wikipedia.org/wiki?curid=17751", "title": "Miranda IM", "text": "Miranda IM\n\nMiranda IM (meist nur „Miranda“) ist ein freier Instant Messenger für Windows für verschiedene Protokolle. Der frühere SourceForge-Projektname lautete „Miranda-ICQ“, da das Projekt inzwischen aber neben ICQ auch viele weitere Protokolle unterstützt, wurde der Name 2003 geändert.\n\nDas Programm ist von Haus aus in seiner Optik schlicht gehalten und hat dadurch manch anderem Instant-Messenger-Client eine geringere Beanspruchung der Betriebsmittel voraus. Miranda IM liefert bereits in seiner Standardausführung Plug-ins für gängige Protokolle (AOL Instant Messenger, Gadu-Gadu, ICQ, IRC, XMPP, MSN, Yahoo Messenger) mit, auf der Homepage finden sich zudem über 350 optionale Plug-ins, die weitere Protokolle oder Funktionen nachrüsten. Sie werden in den allermeisten Fällen von Drittpersonen programmiert. Um den Einstieg in die modulare Software zu erleichtern, bieten einige Privatpersonen inoffizielle „Komplettpakete“ von Miranda IM an, welche von Haus aus über ausgewählte Plug-ins und Konfigurationen verfügen, die in der Standardvariante nicht vorhanden sind.\n\nNeben der Vielseitigkeit durch die Plug-ins ist Miranda IM durch Sprachdateien in einer Vielzahl von Sprachen übersetzbar. Viele Plug-ins nutzen diese Möglichkeit ebenfalls, so dass Miranda IM – falls gewünscht – fast komplett auf Deutsch arbeitet.\n\nDie Entwicklung von Miranda IM ist stets in drei Hauptversionen aufgeteilt. Dabei handelt es sich um eine stabile (momentan 0.10.x), eine Beta- und eine Alpha-Version. Die Weiterentwicklung der stabilen Version konzentriert sich auf Fehlerkorrekturen und kleinere Optimierungen, während in die Alpha-Versionen neue Funktionen sowie größere Änderungen einfließen. So sind in den Alpha-Versionen neben den Standard-Plug-ins zusätzliche Kontaktlisten- und Nachrichtenfenstererweiterungen enthalten. Diese ermöglichen zum Beispiel das Anpassen der Kontaktliste mittels Skins.\n\nVon jeder Version erscheint sowohl eine ANSI- als auch eine Unicode-Variante, womit Miranda IM immerhin bis zur Version 0.8.27 auf allen Windowsversionen ab Windows 95, später ab 98 funktioniert. Ferner wurde mit der Alpha-Version 0.9.x auch eine 64-Bit-Variante in den Entwicklungszweig aufgenommen und der Code unwissend insofern verändert, dass der Messenger in der ANSI-Version mit Windows 95 nicht mehr funktioniert. Hinsichtlich Plug-ins Dritter muss der Benutzer in allen Fällen auf Kompatibilität achten.\n\nIm Sommer 2012 entstand mit Miranda NG ein Fork von Miranda IM, zu dessen Entwicklung inzwischen die meisten ehemaligen Miranda-IM-Entwickler beitragen. Als Grund für die Abspaltung wurden Unstimmigkeiten zwischen den Hauptentwicklern genannt.\n\nMiranda IM ist keine plattformunabhängige Software, sondern wurde einzig für Windows geschrieben. Zwar gab es in der Vergangenheit Portierungsversuche seitens unabhängiger Programmierer, sie wurden aber allesamt mit dürftigem Ergebnis abgebrochen.\n\nMittlerweile wird jedoch von offizieller Seite die Standardinstallation von Miranda IM unter der Laufzeitumgebung Wine als nutzbar bezeichnet. Die Entwickler bekundeten Interesse daran, die Kompatibilität mit Wine auch von ihrer Seite aus weiter zu fördern.\n\n"}
{"id": "17794", "url": "https://de.wikipedia.org/wiki?curid=17794", "title": "Mutt", "text": "Mutt\n\nMutt (englisch für „Köter“) ist ein schlanker, textbasierter E-Mail-Client für Unix und andere Unix-artige Betriebssysteme. Es unterliegt der GNU General Public License (GPL). Mutt arbeitet im Textmodus und ist damit in Textfenstern, auf Rechnerkonsolen und mit ssh oder telnet in vielen Umgebungen einsetzbar. Andere Medien und Formate als Text werden durch einfache Aufrufbarkeit unabhängiger Programme zu deren spezieller Verarbeitung unterstützt.\n\nDie Benutzung ist ausschließlich tastaturgesteuert, daher für einen an grafische Oberflächen gewöhnten Benutzer eher gewöhnungsbedürftig, aber sehr gut dokumentiert. Mutt ist flexibel konfigurierbar. Dank Funktionen wie Sortierung nach Ursprungs-/Antwort-E-Mail (threading) oder regelbasierte Wertkennzeichnung (scoring) ist es für große Mengen von E-Mails besonders geeignet. Kryptografische Software wie GnuPG und OpenSSL (für S/MIME) zum Verschlüsseln und Signieren von E-Mails kann von Mutt eingebunden werden. Entsprechende Konfigurationsoptionen, Ausgaben und Tastaturkürzel sind integriert. Optional lässt sich Mutt mit Unterstützung für den vielsprachigen Zeichensatz Unicode im Format UTF-8 kompilieren und ist nach wie vor ein modernes Programm zur E-Mail-Kommunikation.\n\nMutt bekam 2004 zusammen mit KMail von der Linux New Media AG den Linux New Media Award in der Kategorie „Bester Mail-Client“. Michael Elkins kreierte die Software im Jahr 1995.\n\nDer Wahlspruch von Mutt lautet \"„All mail clients suck. This one just sucks less.“\" (deutsch: „Alle Mailprogramme nerven. Dieses hier nervt einfach ein bisschen weniger.“)\n\n"}
{"id": "17820", "url": "https://de.wikipedia.org/wiki?curid=17820", "title": "Computergrafik", "text": "Computergrafik\n\nDie Computergrafik ist ein Teilgebiet der Informatik, das sich mit der computergestützten Bilderzeugung, im weiten Sinne auch mit der Bildbearbeitung befasst. Mit den Mitteln der Computergrafik entstandene Bilder werden \"Computergrafiken\" genannt.\n\nDie Computergrafik umfasst zum einen die Erzeugung von Grafiken, deren Bestandteile sich zweidimensional in der Ebene beschreiben lassen. Weitere Teilbereiche beschäftigen sich mit der Frage, wie sich komplexe Formen geometrisch modellieren lassen und wie aus daraus aufgebauten virtuellen Umgebungen Bilder oder Animationen berechnet (gerendert) werden können.\n\nIhren Ursprung hat die Computergrafik in den 1950er-Jahren, als Rechner mit grafischen Ausgabegeräten ausgestattet wurden. In der Folge wurden Eingabegeräte entwickelt, die eine interaktive Bedienung von Computern ermöglichten und vor allem wissenschaftlichen und technischen Anwendungen wie CAD und CAM den Weg bereiteten. Heute sind Computergrafiken allgegenwärtig; ihre Anwendungen reichen von grafischen Benutzeroberflächen über Druckerzeugnisse, Computerspiele und Filmtechnik bis hin zur Medizin.\n\nAls Beginn der Computergrafik gilt oft der Anfang der 1950er Jahre am MIT entwickelte Whirlwind, der als erster Rechner über einen Kathodenstrahlröhren-Bildschirm sowie über ein lichtgriffelähnliches Eingabegerät verfügte und direkt zum interaktiven Luftraumüberwachungssystem SAGE führte. Außerdem kamen die ersten kommerziellen Vektorbildschirme auf den Markt, die Punkte und Linien anzeigen konnten. Parallel dazu entwickelte General Motors ein erstes interaktives CAD-System, das DAC-1.\n\nBesonderen Einfluss auf die Entwicklung der interaktiven Computergrafik hatte Ivan Sutherlands 1963 vorgestelltes Sketchpad-Zeichensystem, das viele auch in moderner Grafiksoftware gebräuchliche Interaktionsmöglichkeiten aufzeigte. Da Anfang der 1960er Jahre noch überwiegend mit Lochkarten gearbeitet wurde, bestand zwar eine Nachfrage nach interaktiven Benutzerschnittstellen, doch waren entsprechende Systeme äußerst kostspielig. Dies änderte sich gegen Ende des Jahrzehnts, als erste Computerterminals mit Speicherröhren-Grafikbildschirmen \"(Direct-View Storage Tubes)\" auf dem Markt erschienen und zehntausenden Anwendern einen erschwinglichen Zugang zur Computergrafik ermöglichten. Zu dieser Zeit wurden außerdem wichtige Verfahren der 3D-Computergrafik wie Raytracing und Scanline-Algorithmen entwickelt.\nIn den 1980er Jahren setzten sich weitere Eingabegeräte wie Maus und Grafiktablett durch; auch Farbdrucker kamen auf den Markt. Gleichzeitig wurden Vektorbildschirme allmählich von Rasterbildschirmen, die farbig gefüllte Flächen anzeigen konnten, verdrängt. 1974 hielt ACM SIGGRAPH, heute die größte Vereinigung von Forschern und Branchenvertretern im Bereich Computergrafik, ihre erste Konferenz ab. Der ab Ende der 1970er Jahre massenhaft produzierte Personal Computer verfügte zwar über vergleichsweise schlechte Grafikmöglichkeiten, trieb jedoch die Hersteller dazu an, preisgünstige Plotter und Grafiktabletts herzustellen und trug so zur Verbreitung der Computergrafik entscheidend bei.\n\nDie Systemleistung von PCs und Workstations stieg in den 1980er Jahren derartig an, dass nun auch Endbenutzer mit einfach zu bedienenden Programmen Computergrafiken erzeugen konnten. Außerdem gelangten die wichtigen Anwendungsgebiete CAD und CAM zum Durchbruch. Sowohl für diese als auch für andere Anwendungen kam erste 3D-Grafiksoftware auf den Markt. Künstlerische Anwendungen hatten zwar schon immer die Computergrafik begleitet, erschlossen nun aber als Computer Generated Imagery für die Film- und Werbebranche einen Markt.\n\nIn den 1990er Jahren erreichten übliche Rechner ausreichend Leistung, um unterschiedliche Techniken, etwa Computergrafik und Bildverarbeitung oder Bild- und Toninhalte, kombinieren zu können. Unter dem Schlagwort Multimedia verschwammen die Grenzen zwischen reiner Computergrafik und anderen Gebieten. Im Laufe des Jahrzehnts verschob sich außerdem das Gewicht von wissenschaftlichen und technischen zunehmend auf nichttechnische Anwendungsgebiete. Dies machte sich unter anderem darin bemerkbar, dass vollständig computergenerierte Kinofilme in Spielfilmlänge produziert wurden. Mitte der 1990er Jahre wurde auch im nichtprofessionellen Bereich Hardwarebeschleunigung für 3D-Computergrafik eingeführt.\n\nSeit Beginn des 21. Jahrhunderts besteht sowohl in der Fotografie als auch in der Bildsynthese ein lebhaftes Interesse an Hochkontrastbildern (High Dynamic Range Images). Die Echtzeitgrafik profitiert von leistungsfähigen Grafikprozessoren, die mittels programmierbarer Shader vielfältige Effekte erzeugen können. Bestrebungen, die für eine realistische Wirkung oft unverzichtbare globale Beleuchtung in Echtzeit zu berechnen, gelangen bisher (2010) jedoch nur teilweise.\n\nSeit den 1980er Jahren sind nur noch Rasterbildschirme üblich, bei denen das anzuzeigende Bild durch ein Raster aus Bildpunkten (Pixeln) repräsentiert wird, denen jeweils ein Farbwert zugeordnet ist. Bilder, die in dieser Form vorliegen, werden Rastergrafiken genannt. Sie können sowohl von Software erzeugt werden, als auch das Ergebnis einer Digitalisierung sein. Ein wichtiger Vorteil von Rasterbildschirmen ist die Fähigkeit, farbig gefüllte Flächen anzuzeigen. Auch viele Drucker und weitere Ausgabegeräte verwenden ein Punktraster. Allerdings leiden Rastergrafiken an Darstellungsproblemen wie dem Treppeneffekt, der ein Ergebnis der begrenzten Bildauflösung (Pixelanzahl) ist. Der sichtbare Teil des Framebuffers, ein besonderer Speicherbereich der Grafikkarte, enthält das auf dem Bildschirm angezeigte Bild in einem geräteabhängigen Datenformat.\n\nFür die permanente Speicherung und den systemübergreifenden Austausch von Rastergrafiken wurden mehrere standardisierte Dateiformate entwickelt. Diese Grafikformate variieren erheblich in ihren Eigenschaften, etwa in der Unterstützung diverser Bildkompressionsverfahren. In Grafikdateien werden aus Speicherplatzgründen oft indizierte Farben verwendet, bei denen die im Bild verwendeten Farben in einer separaten Tabelle gespeichert sind. Rastergrafiken können außerdem einen Alphakanal enthalten, der die „Durchsichtigkeit“ jedes Pixels angibt.\n\nManche Arten von Bildern, etwa Strichzeichnungen oder Diagramme, werden besser als Vektorgrafiken gespeichert. Hierbei werden keine Pixel, sondern die grafischen Grundobjekte (Primitive), aus denen sich das Bild zusammensetzt, gespeichert. Diese Art der Repräsentation ist unabhängig von der Bildauflösung und erlaubt die verlustfreie Bearbeitung der Bildinhalte. Um eine Vektorgrafik auf Rasterbildschirmen anzeigen zu können, muss sie zunächst in eine Rastergrafik umgewandelt werden. Dieser Vorgang wird Rasterung genannt.\n\nBei 2D-Vektorgrafiken sind verschiedene grafische Grundobjekte gebräuchlich. Hierzu zählen Linien, Kreise, Ellipsen, Polygone und andere Kurven. Alle diese Grundobjekte müssen erst gerastert werden, um sie auf einem Rasterbildschirm darstellen zu können. Dabei müssen auch Parameter wie Füll- und Linienfarben, Linienstärken und Linienstile berücksichtigt werden. Bei der Rasterung von Linien, Kreisen, Ellipsen und Polygonen werden oft iterative (schrittweise arbeitende) Algorithmen verwendet, die ausgehend vom Anfangspunkt entscheiden, welches Pixel jeweils als nächstes eingefärbt werden soll.\n\nOft muss die Rasterung einer Vektorgrafik auf einen bestimmten Bereich, zum Beispiel ein rechteckiges Fenster, begrenzt werden. Dies geschieht am einfachsten, indem die Grundobjekte vollständig gerastert, aber nur diejenigen Pixel eingefärbt werden, die sich tatsächlich innerhalb des gewünschten Bildausschnitts befinden. Es wurden jedoch verschiedene effizientere Methoden entwickelt, bei denen ein Grundobjekt vor der Rasterung auf den Bildausschnitt zugeschnitten wird. Derartige Clipping-Algorithmen kommen sowohl bei der 2D- als auch bei der 3D-Computergrafik zum Einsatz.\n\nZur Füllung beliebiger zusammenhängender Farbflächen werden Floodfill-Algorithmen angewandt. Bei der einfarbigen Rasterung kann es neben dem unausweichlichen Treppeneffekt zu weiteren Problemen wie etwa „fehlenden“ Pixeln kommen, die von der endlichen Auflösung des Ausgabegerätes herrühren. Besonders bei Schriftzeichen ist dies ein Problem. Hier muss so genanntes Hinting angewandt werden, um die Zeichen auch bei kleiner Schriftgröße oder auf niedrig auflösenden Bildschirmen in bestmöglicher Qualität darstellen zu können.\n\nBeim Rastern eines Bildes muss jedem Pixel ein Farbwert zugeordnet werden, der die ideale Bildbeschreibung an diesem Punkt möglichst gut repräsentiert. Komplexere Bildbeschreibungen lassen sich nur an einzelnen Punkten auswerten, was im Sinne der Signalverarbeitung als Abtastung interpretiert werden kann. Hierbei können kleine Figuren durch das Pixelraster fallen oder Aliasing-Effekte auftreten, bei denen regelmäßig angeordnete, sehr kleine Bilddetails völlig falsch dargestellt werden. In der Computergrafik tritt dieses Problem besonders zutage, da das ideale Bild meist harte Objektkanten enthält.\n\nMethoden, die die als Folge der Abtastung auftretenden unerwünschten Effekte abzuschwächen suchen, werden Antialiasing genannt. Dazu werden die ein Pixel umgebenden Bildregionen in die Rasterung mit einbezogen. Selbst kleine Details fließen so in die Farbe eines Pixels ein, auch wenn sie zwischen zwei Pixeln liegen sollten. Der Treppeneffekt lässt sich durch Antialiasing ebenfalls deutlich reduzieren.\n\nDer Farbwert jedes Pixels einer Rastergrafik wird üblicherweise mittels Rot-, Grün- und Blau- (RGB-)Werten angegeben. Die Anzahl der Werte, die jeder dieser drei Farbkanäle annehmen kann, wird durch die Farbtiefe angegeben; viele Bildschirme erlauben 256 Werte pro Farbkanal (True Color). Zur Auswahl von Farben durch den Anwender ist der RGB-Farbraum jedoch nicht der benutzerfreundlichste. In Zeichenprogrammen sind andere Farbräume wie der HSV-Farbraum gebräuchlich, bei dem eine Farbe durch Farbton, Sättigung und Helligkeit definiert wird. Beim Vierfarbendruck wird mit dem CMYK-Farbmodell gearbeitet.\n\nDie Helligkeit der Bildschirmpixel ist nicht proportional zu den im Framebuffer angegebenen Farbwerten. Ein Graustufenwert von 50 % wird auf dem Bildschirm nicht als Grau mit 50 % Helligkeit dargestellt, sondern dunkler. Um korrekte Helligkeiten auszugeben, müssen daher computergenerierte Bilder stets eine Gammakorrektur oder – je nach Bildschirmtyp – andere Helligkeitsanpassungen durchlaufen. Damit eine möglichst konsistente Farbdarstellung auf verschiedenen Geräten gewährleistet ist, können zusätzlich Farbmanagement-Techniken angewandt werden.\n\nManchmal ist es nötig, die Anzahl der Farben einer Rastergrafik zu verringern. Dies geschieht mittels Farbreduktion, bei der die für die Grafik repräsentativsten Farben ausgewählt werden müssen. Zusätzlich kann Dithering angewandt werden, um den bei der Farbreduktion entstandenen Fehler zu streuen. Für die Ausgabe auf Druckern gibt es spezielle Methoden, um Druckraster zu erzeugen.\n\nEinige Grafikformate erlauben es, die Farbwerte eines Bildes mit einem sehr hohen Helligkeitsumfang anzugeben. Derartige High Dynamic Range Images (HDRIs) speichern einen wesentlich naturgetreueren Bildeindruck als herkömmliche Formate und ermöglichen nachträgliche Helligkeitsänderungen ohne Qualitätsverlust. Allerdings sind aktuell (im Jahr 2010) nur wenige Bildschirme zur annähernden Wiedergabe von HDRIs in der Lage.\n\nDie geometrische Modellierung ist die computergestützte Beschreibung sowohl von zweidimensionalen Kurven als auch von dreidimensionalen Flächen und Körpern. Neben ihrer Verwendung in der Computergrafik bildet sie die Basis von ingenieurtechnischen und wissenschaftlichen Anwendungen, zum Beispiel physikalischen Simulationen.\n\nKörper können auf verschiedene Weise repräsentiert werden; jedes Darstellungsschema besitzt Vor- und Nachteile im Hinblick auf Speicheranforderungen, Präzision und Komplexität. Nicht jedes Schema garantiert, dass immer physikalisch realisierbare Körper repräsentiert werden. Man unterscheidet zwischen direkten Darstellungsschemata, bei denen das Volumen des Körpers selbst beschrieben wird, und indirekten Schemata, bei denen die Beschreibung eines Körpers über dessen Kanten und Oberflächen erfolgt. Diese beiden Methoden können auch kombiniert werden.\n\nDie beiden am häufigsten verwendeten Darstellungsschemata sind Constructive Solid Geometry (CSG) und Oberflächendarstellungen. CSG ist ein direktes Darstellungsschema, bei dem Objekte mit Hilfe von Grundkörpern wie Kugeln, Quadern oder Zylindern modelliert werden. Auf diese Grundkörper werden Operationen wie Vereinigung, Schnitt und Differenz angewandt, sodass sich am Ende eine Formel ergibt, die beschreibt, wie die Grundkörper miteinander verknüpft werden. CSG ist besonders im CAD-Bereich gebräuchlich, da sich damit viele dort verwendete Objekte auf natürliche Weise beschreiben lassen.\n\nBei der Oberflächendarstellung \"(Boundary Representation)\" hingegen wird ein Körper anhand seiner Oberfläche beschrieben; es handelt sich also um ein indirektes Darstellungsschema. Die mittels Oberflächendarstellung modellierten Objekte werden meist aus so genannten Freiformflächen zusammengesetzt, die sich an Kontrollpunkten verformen lassen. Lokale Änderungen am Modell sind damit einfach möglich. Eine weitverbreitete Art von Freiformflächen sind Non-Uniform Rational B-Splines (NURBS). Vor der Darstellung werden NURBS aus Effizienzgründen üblicherweise in Polygon- oder Dreiecksnetze umgewandelt (trianguliert). Auch allgemeine zweidimensionale Kurven werden meist mittels Splines beschrieben, deren Kurvenverlauf durch Kontrollpunkte festgelegt wird. Hier sind Bézierkurven gebräuchlich, die für die Rasterung in Polygonzüge umgewandelt werden.\n\nDas Ergebnis der 3D-Modellierung ist eine Szene, die neben der Objektgeometrie Materialeigenschaften, Lichtquellen sowie die Position und Blickrichtung eines virtuellen Betrachters enthält.\n\nAusgehend von einer Szene werden durch Bildsynthese, auch \"Rendern\" genannt, 3D-Computergrafiken berechnet. Dieser Prozess läuft im Gegensatz zur Modellierung automatisch ab. Man unterscheidet zwischen der interaktiven Synthese von Bildern in Echtzeit, bei der meist Hardwarebeschleunigung zum Einsatz kommt, und der realistischen Bildsynthese, bei der vor allem auf hohe Bildqualität oder physikalische Korrektheit Wert gelegt wird.\n\nBeim Echtzeitrendern wird eine Reihe von Bildern mit hoher Bildfrequenz berechnet und die zugrundeliegende Szene vom Anwender interaktiv verändert. Mit wenigen Ausnahmen unterstützt Grafikhardware als grafische Grundobjekte nur Punkte, Linien und Dreiecke, andere Darstellungsschemata müssen daher erst in Dreiecksnetze umgewandelt werden.\n\nBeim Echtzeitrendern beschreibt die Grafikpipeline modellhaft den Weg von der Szene bis zum fertigen Bild. Dieser Prozess wird von heutigen Grafikkarten größtenteils direkt in Hardware ausgeführt. Eine Ausnahme sind Techniken wie Kollisionserkennung, Animation oder Morphing, die die Szene verändern und für die das Anwendungsprogramm verantwortlich ist.\n\nViele Schritte der Grafikpipeline dienen dazu, die Grundobjekte der Szene schrittweise in anderen Koordinatensystemen auszudrücken, die die Berechnung vereinfachen. Dazu zählt meist eine Zentralprojektion, um eine perspektivische Abbildung zu erhalten. Außerdem kommen Clipping- und Culling-Techniken zum Einsatz, um außerhalb des sichtbaren Volumens liegende Flächen zu beschneiden oder zu entfernen. Nach Abschluss der verschiedenen Koordinatentransformationen befinden sich die projizierten Grundobjekte an der richtigen Bildschirmposition. Dort werden sie gerastert, indem die zu ihnen gehörenden Pixel eingefärbt werden. Da bei überlappenden Dreiecken nur die vom Betrachter aus sichtbaren Teile angezeigt werden sollen, wird mittels Z-Buffering eine Verdeckungsberechnung ausgeführt.\n\nZur Ansteuerung von Grafikpipelines werden üblicherweise Grafik-APIs verwendet, die die Grafikhardware abstrahieren und dem Programmierer viele Aufgaben abnehmen. Im professionellen Bereich hat der Standard OpenGL die größte Bedeutung, während unter Microsoft Windows laufende Programme, insbesondere Computerspiele, vornehmlich DirectX nutzen.\n\nAls \"Shading\" („Schattierung“) wird im Allgemeinen die Berechnung der Farben an Oberflächen anhand der zugehörigen Materialeigenschaften und dem direkt von den Lichtquellen eintreffenden Licht bezeichnet. Shading kommt sowohl beim Echtzeitrendern als auch beim realistischen Rendern zum Einsatz. Die indirekte Beleuchtung durch Lichtreflexion zwischen Objekten bleibt dabei zunächst unberücksichtigt. Einen Spezialfall stellt sogenanntes Non-photorealistic Rendering dar, bei dem zum Beispiel aus ästhetischen Gründen Verfremdungen erzeugt werden.\n\nZur Beschreibung des Reflexionsverhaltens einer Oberfläche werden lokale Beleuchtungsmodelle verwendet, mit denen sich diverse Materialien simulieren lassen. Das lambertsche Gesetz eignet sich beispielsweise für matte Flächen wie Wände, während mit dem Phong-Beleuchtungsmodell glänzende Oberflächen dargestellt werden können. Beim Echtzeitrendern werden häufig für die Eckpunkte eines Dreiecks die Farben berechnet und diese anschließend innerhalb des Dreiecks interpoliert (Gouraud Shading). Dadurch ergibt sich ein weniger kantiges Erscheinungsbild als beim Flat Shading, bei dem Dreiecke einheitlich mit einer Farbe gefüllt werden.\n\nUm Oberflächendetails zu simulieren, werden Mapping-Techniken eingesetzt, die die Material- oder Geometrieeigenschaften an jedem Punkt der Oberfläche anhand einer Funktion oder Rastergrafik variieren. So etwa dient Texture Mapping dazu, ein zweidimensionales Bild \"(Textur)\" auf einer Oberfläche abzubilden. Eine weitere Technik ist Bumpmapping, mit dem Oberflächenunebenheiten dargestellt werden können. Spiegelungseffekte lassen sich beim Echtzeitrendern mit Environment Mapping erzielen.\n\nOb ein gerendertes Bild realistisch wirkt, hängt maßgeblich davon ab, inwieweit die Verteilung des Lichts innerhalb einer Szene simuliert wird. Während beim Shading nur die direkte Beleuchtung berechnet wird, spielt bei der indirekten Beleuchtung die Reflexion von Licht zwischen Objekten eine Rolle. Dadurch werden Effekte wie Räume, die nur durch einen schmalen Lichtspalt insgesamt erhellt werden, möglich. Werden alle Arten der Lichtreflexion berücksichtigt, so spricht man von globaler Beleuchtung. Die globale Beleuchtung muss für ein realistisches Ergebnis berücksichtigt werden und ist bei Echtzeitverfahren nicht oder nur sehr eingeschränkt möglich. Mathematisch wird sie durch die Rendergleichung beschrieben.\n\nEines der in der realistischen Bildsynthese verwendeten Verfahren ist Raytracing. Raytracing ist zwar in erster Linie ein Algorithmus zur Verdeckungsberechnung, der auf der Aussendung von Strahlen basiert, kann aber in erweiterten Formen auch Lichtreflexionen und -brechungen berechnen. Um die globale Beleuchtung zu simulieren, werden Verfahren wie Path Tracing oder Photon Mapping verwendet, die auf Raytracing basieren.\n\nEin weiterer Algorithmus der realistischen Bildsynthese ist Radiosity. Er basiert auf der Unterteilung der Oberflächen in kleine Teilflächen, für die die Rendergleichung numerisch gelöst wird. In seiner Grundform ist Radiosity nur auf ideal diffuse Flächen anwendbar. Im Gegensatz zu Raytracing wird die Lichtverteilung unabhängig vom Blickpunkt berechnet. Dadurch eignet sich Radiosity besonders zum Rendern statischer Szenen in Echtzeit, bei denen eine zeitaufwändige Vorausberechnung vertretbar ist.\n\nIn der Filmindustrie wird vor allem das REYES-Verfahren verwendet, bei dem die Oberflächen der Szene in sehr kleine Polygone zerlegt werden, für die einzeln die Farbe berechnet wird.\n\nBei der Volumengrafik werden die zu rendernden Objekte nicht als Oberflächen beschrieben, sondern als räumliche Datensätze, so genannte Voxelgitter. Es handelt sich dabei um gitterförmig angeordnete Werte, die die „Dichte“ eines Objektes an einem bestimmten Punkt beschreiben. Diese Form der Datenrepräsentation ist besonders geeignet für Objekte wie Wolken, die keine klaren Umrisse haben. Zum Rendern von Voxelgittern wurden spezielle Techniken wie Splatting entwickelt. Da viele bildgebende Verfahren Voxeldaten erzeugen, ist die Volumengrafik insbesondere für die Medizin von Bedeutung.\n\nDie computergestützte Erzeugung von Animationen erfordert Techniken, die über die statische Modellierung und Synthese von Einzelbildern hinausgehen. Viele Computeranimationen basieren auf der Schlüsselbildanimation, bei der nur für einige Einzelbilder Parameter der Szene wie die Position und Form von Objekten oder die Kamerablickrichtung gesetzt werden. Die Werte dieser Parameter werden für die dazwischen liegenden Bilder mit Hilfe geeigneter Splines interpoliert. Auch die Rotation von Objekten kann mit mathematischen Verfahren interpoliert werden.\n\nOft sind Objekte für die Animation hierarchisch modelliert, sodass sich die Bewegung eines in der Hierarchie höher liegenden Objekts auf alle Unterobjekte auswirkt. Ein Beispiel ist ein Planetensystem, in dem Monde um Planeten und diese wiederum um die Sonne rotieren. Zum Teil werden Objekte ähnlich wie in der Robotik als zusammenhängende Struktur modelliert, deren Einzelteile durch Gelenke miteinander verbunden sind. Derartige Strukturen können unter Anwendung der direkten oder inversen Kinematik animiert werden. Für die Character animation bieten sich auch aufwändigere Motion-Capture-Techniken an.\n\nDie Animation mehrerer Objekte erfordert häufig Techniken zur Kollisionserkennung. Besonders in Computerspielen wird zusätzlich auf die künstliche Intelligenz zurückgegriffen, um im Rahmen der Gruppensimulation Objekte so zu animieren, dass sie ein bestimmtes Ziel ausführen. Bei größeren Gruppen von dutzenden Objekten entsteht oft ein Schwarmverhalten, das mit vergleichsweise einfachen Techniken plausibel simuliert werden kann. Eine noch höhere Anzahl von Objekten kann durch Partikelsysteme beschrieben werden. Dabei werden auf viele Tausend sehr kleine Partikel Kräfte ausgeübt, um Erscheinungen wie Explosionen, Flüssigkeiten, Rauch oder Feuer zu simulieren. Wirklich realistische Ergebnisse lassen sich jedoch nur mit physikalisch basierten Techniken erzielen, bei denen komplexe Berechnungen ausgeführt werden müssen.\n\nEine wichtige Anwendung findet die Computergrafik in der Mensch-Computer-Interaktion, dem Teilgebiet der Informatik, das die benutzergerechte Gestaltung von interaktiven Systemen und ihren Benutzerschnittstellen untersucht. Dieses Gebiet wurde nicht nur deshalb oft mit der Computergrafik zusammen behandelt, weil es sich mit grafischen Benutzeroberflächen oder Prozessvisualisierung befasst, sondern auch, weil Forscher aus dem Bereich der Computergrafik bereits früh Zugang zu damals neuartigen Ein- und Ausgabegeräten hatten. Allgegenwärtig sind Computergrafiken in Form von Präsentationsgrafiken oder in Desktop-Publishing-Dokumenten.\n\nEin mit der Computergrafik verwandtes, aber abgegrenztes Gebiet ist die Bildverarbeitung. Sie beschäftigt sich mit der Aufbereitung von bereits vorhandenen Bildern, um deren Inhalte automatisch auszuwerten, führt also die umgekehrte Aufgabe der Computergrafik aus. Die Techniken der Bildverarbeitung finden zum Teil auch bei der Bildbearbeitung Anwendung.\n\nDie visuelle digitale Kunst entwickelte sich schon mit Beginn der Computergrafik. Anwendung findet die künstlerische Gestaltung von Computergrafiken in Computerspielen, die immer komplexere 3D-Modelle und fortgeschrittene Renderalgorithmen verwenden. In der Filmtechnik und Werbung kommen fast alle Arten von Computergrafik-Technologien zum Einsatz, um Trickfilme und Computer Generated Imagery zu erzeugen. Viele moderne Filme verwenden digitale Compositing-Techniken, um separat gefilmte Vordergrundszenen über einen Hintergrund zu legen.\n\nCAD nutzt die Computertechnologie als „virtuelles Zeichenbrett“, um Maschinenteile und andere Produkte zu entwerfen oder die anschließenden Fertigungsprozesse mittels Computer-aided manufacturing zu steuern. In der Kartografie und in Geoinformationssystemen wird die Computergrafik dazu verwendet, um sowohl präzise als auch schematische Darstellungen aus geografischen und anderen raumbezogenen Messdaten zu erzeugen.\n\nSimulationen und Visualisierungen virtueller Umgebungen reichen von der Architekturdarstellung bis hin zu wissenschaftlichen und sicherheitskritischen Bereichen. Sowohl die virtuelle als auch die erweiterte Realität legen ihr Hauptaugenmerk auf die Interaktion mit dem Betrachter, etwa auf das Verändern der Perspektive bei einer Drehung des Kopfes. Dabei kommen sowohl 3D-Computergrafik als auch hochentwickelte Darstellungstechnologien zum Einsatz. Während in der virtuellen Realität die Welten komplett im Rechner generiert werden, beschäftigt sich die erweiterte Realität mit dem Einbinden von künstlichen Objekten in die Realität, zum Beispiel über Head-Mounted Displays.\n\nIn der Medizin werden möglichst aussagekräftige Bilder aus Daten, die durch bildgebende Verfahren gewonnen wurden, generiert. Dies geschieht hauptsächlich mit den Mitteln der Volumengrafik.\n\nDie Computergrafik ist Teil der Studiengänge Computervisualistik, Visual Computing, Medieninformatik und Informatik.\n\nBücher:\nZeitschriften (Auswahl):\n\n"}
{"id": "17945", "url": "https://de.wikipedia.org/wiki?curid=17945", "title": "Microsoft Foundation Classes", "text": "Microsoft Foundation Classes\n\nDie Microsoft Foundation Classes (MFC) sind eine Sammlung objektorientierter Klassenbibliotheken (GUI-Toolkit), die von Microsoft für die Programmierung von Anwendungen mit grafischen Benutzeroberflächen für Windows mit C++ entwickelt wurden. \n\nMicrosoft führte die MFC 1992 mit dem hauseigenen Compiler für C++ und C ein. Die MFC werden mit diversen Microsoft-C++-Compilern ausgeliefert und sind Teil der Vollversionen der integrierten Entwicklungsumgebung Microsoft Visual Studio. Die kostenlosen Visual-Studio-Express-Editionen enthalten die MFC nicht. Im November 2014 veröffentlichte Microsoft jedoch das kostenlose Visual Studio Community 2013, das – wie Visual Studio 2013 Professional und höher – die MFC beinhaltet.\nSeit Windows 2000 sind oft eine oder mehrere MFC-Programmbibliotheken in einer Betriebssysteminstallation enthalten.\n\nZur Nutzung von übersetzten MFC-Programmen ist oft keine Installation einer Laufzeitbibliothek notwendig, da auf vielen Windowssystemen schon die „Visual Studio C++ 20XX Runtime“ installiert ist, die wiederum die MFC-Laufzeit enthält.\n\nEin Merkmal von MFC ist die Verwendung von „Afx“ als Präfix vieler Funktionen, Makros und sein Vorkommen beim standardmäßig vorkompilierten Headernamen „stdafx.h“, was daher rührt, dass in der frühen Entwicklung der MFC diese zunächst „Application Framework Extensions“ genannt und mit „Afx“ abgekürzt wurden. Erst viel später wurde „Afx“ in MFC umbenannt – so spät, dass diese Bezüge nicht mehr verändert werden konnten.\n\nNeben der Weiterentwicklung durch den Hersteller bei weitgehender Abwärtskompatibilität existieren zahlreiche Ergänzungen durch kommerzielle Anbieter sowie eine ausführliche Dokumentation. Alle in Windows verwendeten Steuerelemente können verwendet werden. So sind aktuelle Komponenten aus Windows und anderen Microsoft-Produkten (z. B. Ribbons) in den MFC verfügbar, sowie das aktuelle Aero-Design.\n\nDie MFC dienen als Schnittstelle zu der nicht objektorientierten WinAPI und sollen den Umgang mit den vom Betriebssystem zur Verfügung gestellten Ressourcen erheblich vereinfachen. Die MFC stellen somit eine Umsetzung des Adapter-Entwurfsmusters dar – anstatt nur direkt die Win32-API-Funktionen verwenden zu können, kann der Programmierer nun als Adapter die Klassen der MFC verwenden.\n\nDie MFC unterstützen eine Abwandlung des Model-View-Controller-Architekturmusters. Es werden die Klassen CDocument und CView zur Verfügung gestellt, wobei mit CDocument das Modell, das heißt der Datenbehälter und mit CView sowohl Ansicht als auch Steuerung implementiert werden. Der in Microsoft Visual Studio enthaltene Assistent ermöglicht es, Frameworks unter Verwendung dieser Architektur automatisch zu erstellen.\n\nBekannte Alternativen zu den MFC sind zum Beispiel die Active Template Library (ATL, ebenfalls Microsoft), auch in Kombination mit der Windows Template Library (WTL, von Microsoft als Open Source freigegeben), und die Visual Component Library (Borland). Die .NET-Plattform verwendet ein System namens Windows Forms, das von den MFC (und dem Thunderforms-System von Visual Basic) beeinflusst wurde. Im Gegensatz zu MFC wird zur GUI-Entwicklung dabei kein reines C++, sondern die Microsoft-C++-Spracherweiterung C++/CLI verwendet. Plattformübergreifende Alternativen sind Qt und wxWidgets – beide basierend auf C++ – oder GTK+ – basierend auf C. Sie haben außerdem Sprachanbindungen für Java, Perl, Python, Ruby und andere Sprachen.\n\n\n"}
{"id": "18022", "url": "https://de.wikipedia.org/wiki?curid=18022", "title": "Bildauflösung", "text": "Bildauflösung\n\nDie Bildauflösung ist ein umgangssprachliches Maß für die Bildgröße einer Rastergrafik. Sie wird durch die Gesamtzahl der Bildpunkte oder durch die Anzahl der Spalten (Breite) und Zeilen (Höhe) einer Rastergrafik angegeben.\n\nDer Begriff \"Auflösung\" wird in der Praxis mehrdeutig und in vielen Bereichen verwendet, wodurch es zu Missverständnissen kommen kann. Auflösung im physikalischen Sinn \"(Bildelemente pro Länge)\" bezeichnet die Punktdichte einer \"Wiedergabe\" oder Bildabtastung und ist damit – neben der Farbtiefe – ein Maß für die Qualität.\n\nBei Rastergrafiken selbst, die z. B. als Datei vorliegen, kann diese Qualität nicht angegeben werden, da zunächst unklar ist, wie die Wiedergabe erfolgt. So kann eine kleinere, beispielsweise nur 200 Byte große Favicon-Grafikdatei eine durchaus „exzellente und 100 % perfekte“ Wiedergabequalität liefern.\n\n\"Auflösung\" im technischen Sinn ist wiedergabebezogen. Solange die Wiedergabe auf physikalisch immer gleichen Medien erfolgt, beispielsweise einem großen Fotoabzug oder identischen Fernsehern, hängt die dort erreichte Qualität auch von der Größe der ursprünglichen Rastergrafik ab. Da im Allgemeinen jedoch nicht klar ist, wie die Ausgabe in allen späteren Fällen genau erfolgt, kann die „Bildauflösung“ nicht als direktes Maß für eine allgemeine Wiedergabequalität dienen.\nFür technische Prozesse, die eine Rastergrafik wiedergeben, gilt: Je größer die Grafik ist („Bildauflösung“ bzw. Bildgröße in Pixeln),\nBedingt durch diverse technische Faktoren ist der Zusammenhang \"nicht linear\" bzw. nur eingeschränkt linear. In der Praxis versucht man oft, eine möglichst große Grafik zu verwenden, um eine spätere Wiedergabequalität zumindest nicht von vorneherein einzuschränken. Im Einzelfall kann dies aber auch unverhältnismäßig aufwendig werden.\n\nDie Grafikgröße kann in zwei Varianten dargestellt werden:\n\nIn der zweiten, ausführlicheren Variante wird auch das Verhältnis zwischen Breite und Höhe ersichtlich, so dass man eine Vorstellung vom Seitenverhältnis bekommt.\n\nSind in einer Grafik die Bildpunkte nicht in einem geometrisch regelmäßigen Raster, sondern willkürlich angeordnet, oder besitzt das Bild selbst gar keine Rechteck- (oder andere regelmäßige) Form, so sind als Auflösung nur die Gesamtzahl der Bildpunkte und eventuell ihre lokale Dichte (pro Längeneinheit oder Fläche) bestimmbar. Eine Angabe des Pixelzahl-Produkts ist dann meist nicht möglich. So etwa in der Silberhalogenid-Fotografie oder bei LED-Verkehrsleitsystemen, die mit diskreten – nicht bildfeldfüllenden – Lichtpunkten nur wenige festgelegte Zeichendarstellungen signalisieren können, z. B. – als Tempolimit – „80“, „100“ oder Diagonale im Doppelkreis. Auch viele LCDs verwenden Anordnungen, die nicht einer Punktmatrix entsprechen, insbesondere in einfachen Geräten wie z. B. Wetterstationen oder Digitaluhren. Diese Anordnungen können aber durchaus bildfüllend sein, da LCD-Pixel nicht rechteckig sein müssen. Bei Zeilendisplays, die hauptsächlich für die Darstellung von Text verwendet werden, wird die Auflösung oft in angegeben, wobei jede Spalte ein Zeichen darstellen kann. Dasselbe gilt für den Textmodus im Bereich der Computergrafik.\n\nBei herkömmlichen analogen Kathodenstrahlröhrenbildschirmen ist das Format des Eingangssignals (Größe in Bildpunkten) identisch mit der wiedergegebenen Größe, die Bildpunkte werden \"eins zu eins\" übertragen. Abhängig von der Wiedergabeelektronik können vom Gerät auf der Bildröhre verschiedene Grafik- bzw. Videoformate mit unterschiedlichen Bildgrößen (in Pixeln) wiedergegeben werden. Durch die Steuerelektronik werden diese auf dem Bildschirm immer mit gleicher Breite und Höhe (im Allgemeinen als bildschirmfüllendes Vollbild) dargestellt.\n\nAuf einem modernen Bildanzeigegerät hingegen, wie zum Beispiel einem Plasma- oder Flüssigkristallbildschirm, ist das Wiedergaberaster bauartbedingt fest vorgegeben. Es kann sich von dem Format des Eingangssignals unterscheiden.\nZur möglichst korrekten Darstellung müssen die Pixelzahlen von Breite und Höhe des Signals dann auf das Ausgaberaster transformiert \"(skaliert)\" werden. Hierbei kommt es – insbesondere bei einer Verkleinerung, aber auch bei einer Vergrößerung – zu Verlusten von Bildinhalten. Es können Bildinformationen verlorengehen oder Bildartefakte entstehen. Die Ausführung und der technische Aufwand, der bei der Transformation betrieben wird, bestimmen die Wiedergabequalität, insbesondere auch die \"wahrgenommene Qualität\".\n\nDie Farbtiefe gibt die \"Feinheit\" der Abstufungen an, mit der die Farbe einzelner Bildelemente einer Rastergrafik wiedergegeben werden kann. Neben der Pixelanzahl ist sie eine der bestimmenden Größen einer Rastergrafik.\n\nEine Auflösung, die exakt der physikalischen digitalen Auflösung (Pixelzahl) eines Anzeigegerätes entspricht, wird als native Auflösung bezeichnet.\n\nDie Kenntnis der nativen Auflösung eines Anzeigegeräts ist deshalb wichtig, weil sich fast jede Auflösungsänderung negativ auf die Bildqualität auswirkt (eine Ausnahme ist z. B. die Vervierfachung der Auflösung, bei der die Qualität unverändert bleibt). Wenn möglich, sollte das digitale Bild nur an einer Stelle des Signalwegs in seiner Auflösung geändert werden, und zwar direkt in die native Auflösung des Anzeigegeräts.\n\n\"Beispiel: Ein Foto mit 6 Megapixeln soll auf einem Beamer mit WXGA angezeigt werden. Der Laptop zur Wiedergabe hat eine Auflösung von Bildpunkten. Würde man am Laptop diese Auflösung für den Ausgang zum Beamer nutzen, müsste das Foto zweimal umgerechnet werden: erst von 6 Megapixeln auf und dann nochmal im Beamer von auf Besser ist es, direkt am PC-Ausgang ebenfalls einzustellen, da beim einmaligen Umrechnen weniger Qualität verloren geht.\"\n\nBei Röhrenmonitoren gibt es keine „native Auflösung“, und auch eine optimale Auflösung kann nicht objektiv berechnet, sondern nur grob abgeschätzt werden. Dies hat folgende Gründe:\n\nBei Röhrenmonitoren wird die Auflösung in erster Linie begrenzt durch die Fokussierung des Elektronenstrahls sowie durch die Bandbreite, die die Elektronik zulässt. Die Fokussierung des Elektronenstrahls ist zumeist über ein im Inneren des Monitors verborgenes Poti beliebig einstellbar. Aus dem Lochabstand kann man keine Rückschlüsse auf die werksmäßig oder nachträglich eingestellte Fokussierung ziehen.\n\nAuch wenn die Dicke des Elektronenstrahls bekannt ist, kann daraus nicht die maximale Auflösung errechnet werden, da sich die einzelnen Zeilen leicht überlappen dürfen. Sie sollten zumindest aneinanderstoßen, denn sonst wäre das Zeilenraster störend erkennbar. Überlappen sie sich hingegen zu stark, wirkt das Bild unscharf. Die optimale Auflösung wird somit zur Geschmacksfrage.\n\nNeben Schwarzweißbildröhren, deren Leuchtschirm keine Strukturen aufweist, gibt es drei Arten von Farbbildröhren: solche mit Lochmaske, mit Schlitzmaske und mit Streifenmaske. Bei Schlitzmasken und Lochmasken muss die Fokussierung des Elektronenstrahls so gewählt sein, dass er immer gleich mehrere Bildpunkte überstreicht, da es sonst zu Moiré-Effekten käme. Die einzelnen Bildpunkte einer Schlitz- oder Lochmaskenröhre sind in keinem Fall individuell ansprechbar so wie etwa die eines TFT-Displays. Die einzelnen Bildpunkte stellen daher auch keinesfalls die „native Auflösung“ dar. Die erreichbare Auflösung liegt vielmehr etwas darüber. Bei einer Streifenmaskenröhre kann es nicht zu Moiré-Effekten kommen, so dass die Fokussierung des Strahls wie bei einer Schwarzweißbildröhre nicht darauf Rücksicht nehmen muss. Die Streifenmaske begrenzt die Auflösung somit nur in X-Richtung. Aber auch hier gilt, dass die einzelnen Unterteilungen nicht individuell angesteuert werden können und die erreichbare Auflösung etwas höher ist als die Anzahl der Streifen.\n\nIm IT-Bereich existieren diverse (de facto) standardisierte Grafikmodi. Diese wurden entweder durch die technischen Eigenheiten bestimmter Grafikstandards oder durch die \"Video Electronics Standards Association\" \"(VESA)\" definiert.\n\nIn der Praxis gibt es bei Desk- und Laptopmonitoren nur die Seitenverhältnisse 5:4 (1,25), 4:3 (1,33), 16:10 (1,6) und 16:9 (1,78). Bei abweichenden Seitenverhältnissen wird die Grafik bei der Wiedergabe in der Breite gestaucht oder gestreckt, insbesondere wird aus 15:9 meist 16:9.\nManche Bezeichnungen, wie zum Beispiel \"XGA\", wurden durch die gängige Werbepraxis so verwaschen, dass es mittlerweile vielfältige Interpretationen der ursprünglichen Standards – meist ergänzt durch weitere Buchstaben – gibt, die nicht Teil des jeweiligen Standards sind.\n\"Quad\" (Q) steht für eine Vervierfachung der Pixelzahl der Basisgröße (Verdopplung von Breite und Höhe), \"Quarter\" (ebenfalls Q) für eine Viertelung (Halbierung von Breite und Höhe) und \"Hex\" (H) für eine Versechzehnfachung (Vervierfachung von Breite und Höhe). Bei den Breitbildgrößen wird das \"W\" manchmal auch mit einem Bindestrich abgetrennt und/oder nach hinten gestellt, z. B.: WXGA, W-XGA, XGAW, XGA-W. Die Auflösungen unterhalb der ursprünglichen VGA-Auflösung kommen heute vor allem in Mobiltelefon- und PDA-Anzeigen vor – häufig auch hochkant. Moderne Computermonitore und -grafikkarten unterstützen Bildgrößen bis zu proprietäre Systeme auch eine Auflösung von Pixeln.\n\"DAR = Anzeige-Seitenverhältnis\" | \"PAR = Pixel-Seitenverhältnis\"\n\nEinige Videoformate:\n\"Alle Videoformate siehe Videoauflösung\"\n\nFür das Auflösungsvermögen von Filmen und Optiken siehe Auflösung (Fotografie).\nIn der Digitalfotografie wird die gerundete Gesamtzahl der Bildpunkte in Megapixeln \"(MP)\" als Anhaltspunkt für die theoretisch erreichbare Qualität angegeben. Die \"tatsächliche\" Bildqualität hängt aber von vielerlei Faktoren ab – die Pixelanzahl allein lässt keine Aussage zu. Tatsächlich war in den Anfangsjahren der Digitalfotografie die Zahl der Pixel aus Kostengründen sehr stark eingeschränkt und damit der bestimmende Qualitätsfaktor. Heute sind hingegen oft die Optik und das Rauschverhalten des Sensors qualitätsbestimmend.\n\nNeben einem Seitenverhältnis von 4:3, welches früher oft vorherrschte, gibt es nun zunehmend auch das 3:2-Format des klassischen Kleinbilds. Fotokameras mit nativem -Format sind weiterhin selten.\n\nEine Liste gängiger Pixel-Anordnungen findet sich unter \"Bildauflösungen in der Digitalfotografie\".\n\n\n\n"}
{"id": "18070", "url": "https://de.wikipedia.org/wiki?curid=18070", "title": "Sinclair ZX81", "text": "Sinclair ZX81\n\nDer Sinclair ZX81 ist ein auf dem Z80-Mikroprozessor basierender Heimcomputer des britischen Herstellers Sinclair Research Ltd. Die Zahl in der Namensgebung bezieht sich auf das Jahr des Markteintritts am 5. März 1981.\n\nDer ZX81 ist eine Weiterentwicklung des ZX80 und wurde von vornherein als Massenware für Einsteiger konzipiert. Ursprünglich nur als Versandware erhältlich, eroberte der massiv als günstiger Lerncomputer beworbene ZX81 ab Ende 1981 auch die Ladengeschäfte Großbritanniens. Daneben starteten die Verkäufe ebenfalls in Kontinentaleuropa, Nordamerika und Japan. Innerhalb kürzester Zeit wurden weltweit große Mengen – auch des von Sinclair mittlerweile auf den Markt gebrachten Zubehörs – abgesetzt. Aufgrund der schlechten Verarbeitung und fehlerhafter Systemsoftware der ersten Produktionsserien wurde anfänglich nahezu jedes vierte Gerät reklamiert, was der weiteren enormen Nachfrage jedoch keinen Abbruch tat.\n\nIm Zuge des großen Erfolgs wurden sowohl legale als auch nicht autorisierte Nachbauten hergestellt. Der von Timex Corporation in Lizenz für den nordamerikanischen Markt produzierte \"Timex Sinclair 1000\" (kurz \"TS1000\") konnte weitere große Marktanteile insbesondere in den USA erobern. Dem später erschienenen \"Timex Sinclair 1500\" (kurz \"TS1500\") war dagegen kein Erfolg beschieden. Die meisten der nicht lizenzierten Produkte erschienen in Südamerika und Asien. Alle Nachbauten mit eingerechnet, wurden bis Produktionsende 1984 weltweit insgesamt etwa zwei Millionen Geräte abgesetzt. Der ZX81 ebnete nach dem ZX80 damit endgültig den Weg für das Zeitalter des Computers als Massenware.\n\nDer erste unter Federführung von Clive Sinclair und seinem Unternehmen \"Science of Cambridge\" für den englischen Markt entwickelte Computer, der gehäuselose Einplatinencomputer \"MK 14\" mit Leuchtdioden-Siebensegmentanzeige, wurde ab Juni 1978 in Großbritannien als Versandware angeboten. Die mit geschätzten 15.000 Stück innerhalb nur kurzer Zeit erreichten hohen Verkaufszahlen ermöglichten weitergehende Entwicklungsaktivitäten. Diese mündeten schließlich in der Herstellung eines preiswerten Kompaktgerätes mit der Bezeichnung \"ZX80\". Der ausschließlich auf Kostenminimierung bei Material und Herstellung ausgelegte, dennoch profitable Computer wurde ab Januar 1980 für 99,95 £ als erstes Gerät unterhalb der vermarktungspsychologisch bedeutsamen Grenze von 100 £ in Großbritannien zum Verkauf angeboten. Mit etwa 100.000 innerhalb von 18 Monaten verkauften Exemplaren entwickelte sich der ZX80 rasch zu einem Kassenschlager. Noch vor dessen Markteintritt wurde ab September 1979 bereits an einem Nachfolgemodell, dem ZX81, gearbeitet.\n\nDas zu entwickelnde Gerät sollte noch leistungsfähiger als der ZX80 bei gleichzeitig niedrigerem Verkaufspreis sein. Dazu wurde die erfolgreiche Systemarchitektur des ZX80 übernommen und in neueste Technik zur weiteren Senkung der Material- und Herstellungskosten investiert.\n\nVon den im ZX80 verbauten kostenintensiven Bauteilen konnten mit Hilfe der noch jungen ULA-Technologie durch das Unternehmen Ferranti Limited die Funktionen von insgesamt 18 integrierten Schaltkreisen auf einem einzigen Spezialbaustein nachgebildet werden. Die damit verbundenen Entwicklungs- und Herstellungskosten relativierten sich bei den von Sinclair angepeilten Stückzahlen rasch und führten in der Summe zur gewünschten Kostenreduktion. Der weiterentwickelte, mit maximal fünf elektronischen Schaltkreisen bestückte Computer konnte daraufhin zu einem Preis von knapp unter 70 £ dennoch profitabel angeboten werden. Daneben erlaubte das Ausschöpfen weiterer zur Verfügung stehender Kapazitäten des Spezialbausteins die Beseitigung technischer Probleme des ZX80. So ermöglichte ein neu implementierter Betriebsmodus die Eliminierung des störenden Bildschirmflackerns.\n\nNeben einer Reduktion des Bauteileumfangs sollte gleichzeitig die Bedienbarkeit verbessert und die Leistungsfähigkeit der auch im ZX80 bereits enthaltenen, leicht zu erlernenden Programmiersprache \"Sinclair-BASIC\" gesteigert werden. Die dafür verantwortlich zeichnende \"Nine Tiles Networks Ltd.\" integrierte daraufhin Fließkommaroutinen, erweiterte den Befehlsumfang und die Fehlerbehandlung des BASIC-Interpreters, verbesserte die Editieroptionen und behob zahlreiche bekannte Probleme in der Systemsoftware des ZX80. Dem selbstgestellten Anspruch, mit dem ZX81 in die Riege der vollwertigen Computer-Hersteller aufzusteigen, sollte im November 1980 auch nach außen hin durch eine Umbenennung des Unternehmens in \"Sinclair Computers\" demonstrativ Rechnung getragen werden.\n\nWährend der Entwicklungsphase des ZX81 sondierte Sinclair Computers weitere Vermarktungsoptionen, um das Image des reinen Versandhändlers ablegen zu können. Davon sollte die britische Bücherkette W. H. Smith & Son profitieren. Zur Ankurbelung ihrer stagnierenden Geschäfte hatte sie kurz zuvor ein Programm zur Heranführung der Bevölkerung an die Computertechnik aufgelegt. Die dazugehörige neue Wissensabteilung „Computer Know-How“ sollte durch einen gleichzeitig direkt vor Ort käuflich erwerbbaren Computer überzeugender wirken und mehr Interessenten anziehen. Der ZX80 passte aufgrund seiner einfachen Bedienbarkeit und seines geringen Preises ausgezeichnet in dieses Konzept. Die bereits erzielten hohen Verkaufszahlen und damit die große Bekanntheit des Rechners versprachen zudem auch profitable Margen für W. H. Smith & Son. Entsprechende Vertriebsanfragen für den ZX80 an Sinclair folgten. Sinclair setzte daraufhin W. H. Smith & Son von der Neuentwicklung in Kenntnis und empfahl stattdessen, diesen \"ZX81\" zu vermarkten. Die Verantwortlichen von W. H. Smith & Son willigten ein, allerdings unter der Bedingung, sich zuvor an einem Prototyp von den versprochenen Qualitäten überzeugen zu können.\n\nAls bekannt wurde, dass der britische Fernsehsender British Broadcasting Corporation (BBC) für seine geplante neue Sendungsreihe über Heimcomputer ein entsprechendes Gerät auch zum späteren Vertrieb suchte, entwickelte Sinclair Computers auch in diesem Umfeld rege Vermarktungsbemühungen. Eilends wurde Ende 1980 ein erster vorführbarer Prototyp – noch ohne Gehäuse – eigens zur Vorstellung im Dezember 1980 bei der BBC gefertigt. Jedoch folgte man bei der BBC nicht Sinclairs Philosophie eines ausschließlich auf günstigen Anschaffungspreis ausgelegten Massenproduktes. Stattdessen setzte man bei BBC auf vollwertige Technik, Verlässlichkeit, Benutzerfreundlichkeit und leichte Erweiterbarkeit. Damit ging der Zuschlag an das Konkurrenzunternehmen Acorn, die ihren hochwertigeren \"BBC Micro\" im Januar 1982 einführte.\n\nWährend der Verhandlungen mit der BBC wurde von Nine Tiles Networks Ltd. an der Erweiterung der Systemsoftware zur Ansteuerung eines ebenfalls zu entwickelnden Elektro-Erosionsdruckers gearbeitet. Durch die späte Entscheidung von Sinclair zugunsten des Druckers und den damit verursachten Zeitdruck schlichen sich etliche Fehler in die umfangreichere Systemsoftware des ZX81 ein. Ende 1980 folgten erste Arbeiten zur Überführung der Computerprototypen in die Serienreife und an der auszuliefernden Dokumentation. Nach dem Abschluss der Revisionen von Hard- und Software und den damit feststehenden Abmessungen des Innenlebens ersetzten die Sinclair-Ingenieure im letzten Entwicklungsabschnitt das zu Beschädigungen neigende Gehäuse des ZX80. Das neue pultförmige und in Spritzgusstechnik gefertigte Chassis war kleiner, robuster und dem Zeitgeschmack besser angepasst. Ein Prototyp mit diesem neuen Gehäuse wurde daraufhin im Januar 1981 den Verantwortlichen von W. H. Smith & Son vorgeführt. Beeindruckt von den technischen und wirtschaftlichen Daten wurde man mit Sinclair Computers schnell handelseinig. Teil des Vertrags war dabei ein Exklusivverkaufsrecht des ZX81 für Ladengeschäfte durch W. H. Smith & Son. Beide Vertragspartner sahen damit ihre nahe Zukunft gesichert. Sinclair verstärkte daraufhin seine Forschungstätigkeiten unter anderem zum Nachfolgemodell ZX82 – dem späteren \"Sinclair ZX Spectrum\" – und änderte im März 1981 dazu passend seinen Namen in \"Sinclair Research\".\n\nDie Herstellung des ZX81 übernahm wie bei großen Teilen des ZX80 auch die bis dahin kriselnde schottische Zweigstelle des US-amerikanischen Uhrenherstellers \"Timex Corporation\" in Dundee.\n\nDer offizielle Verkaufsstart des ZX81 erfolgte in Großbritannien am 5. März 1981, zunächst – wie bei allen vorherigen Sinclair-Geräten auch – ausschließlich als Versandware. Der Markteintritt wurde durch aggressive und allgegenwärtige Werbeoffensiven begleitet, die großteils auf die Person des Clive Sinclair zugeschnitten waren und ihn als genialen Heilsbringer der Computertechnologie ins rechte Licht zu setzen suchten. Beinahe täglich erscheinende großformatige Anzeigen in bekannten Zeitungen und Zeitschriften priesen gebetsmühlenartig Sinclairs Produkte als essentiellen und unverzichtbaren Bestandteil des neuen Computerzeitalters an. Technikinteressierten und der künftigen Generation, den Kindern und Heranwachsenden, wurde dabei ein schneller und leichter Einstieg in die zukunftsträchtige Computerwelt zu einem „unvergleichlich günstigen Preis“ von 69,95 £ versprochen. Ein Argument, das angesichts der teilweise um ein Vielfaches teureren Konkurrenzprodukte Acorn Atom (Bausatz ab etwa 120 £), Apple II (ab etwa 549 £), Atari 400 (ab etwa 295 £), Commodore VC20 (ab etwa 160 £) und TRS-80 (Model III ab etwa 559 £) bei vielen Hobbyisten und um die Zukunft ihrer Kinder besorgten Eltern seine Wirkung nicht verfehlte.\n\nDie von der massiven Werbung entfachte, unerwartet große Nachfrage führte alsbald zu Lieferengpässen – Besteller mussten bis zu neun Wochen auf ihre Geräte warten. Hinzu kam eine Vielzahl an Reklamationen durch fehlerhafte Geräte, deren kostenminimierte Bestandteile der Dauerbelastung im Alltag oftmals nicht standhielten. Insbesondere die in den Geräten verbaute Spannungsstabilisierung und das externe Netzteil erwiesen sich als unterdimensioniert und sehr fehleranfällig. Hinzu kamen Probleme mit der Systemsoftware, die zwar einen generellen Betrieb des Computers erlaubten, ambitioniertere Projekte durch fehlerhafte Fließkommaroutinen jedoch unnötig verkomplizierten. Diese Probleme in Verbindung mit einem völlig überforderten Kundendienst taten der ungebremsten Nachfrage aber keinen Abbruch, im Gegenteil – ein Ende der Verkaufszuwächse war nicht abzusehen.\n\nZwischenzeitlich wandte sich Sinclair Computers auch der prestigeträchtigen Versorgung von Bildungseinrichtungen zu. Ein bereits bestehendes Regierungsprogramm zur Förderung des Computereinsatzes in Schulen subventionierte zum Unmut von Sinclair lediglich Konkurrenzmodelle, darunter solche von Acorn. Die kaum verwundene Niederlage bei der BBC aber auch den großflächigen Werbeeffekt und Imagezuwachs im Auge, initiierte daraufhin Sinclair ein eigenes Verkaufsprogramm für Schulen mit einem generellen Preisnachlass auf alle Geräte der ZX81-Reihe. Ein aus ZX81 und Speichererweiterung bestehendes Komplettpaket (später auch mit Drucker) wurde daraufhin für 60 £ (beziehungsweise 90 £) angeboten – 40 Prozent billiger als der preisgünstigste geförderte Computer des Regierungsprogramms. Daraufhin nahmen 2300 Schulen das Angebot wahr und ab Mitte 1981 wurde der ZX81 nach Angaben Sinclairs der in britischen Schulen meistverwendete Computer.\n\nDurch im Produktionsbetrieb vorgenommene Optimierungen hatte Timex Corporation währenddessen die Herstellungsabläufe wesentlich gestrafft. Damit konnten die langen Wartezeiten von anfänglich mehr als zwei Monaten auf die in den Werbeanzeigen versprochenen drei Wochen verkürzt werden. Mit den nun ausreichend produzierbaren Geräten erhielt ab September 1981 auch W. H. Smith & Son die zugesagten Kontingente für seine Filialen. Durch die üppige Gewinnmarge von 40 Prozent des Verkaufspreises und weitere mit dem ZX81 verbundene Nebeneinkünfte konnte die Bücherkette im Laufe des ersten Verkaufsjahres die erhofften deutlichen Gewinne erwirtschaften. Daneben etablierten die im selben Zeitraum von W. H. Smith & Son erzielten Absätze von 350.000 Computern den ZX81 endgültig als dominierende Größe auf dem britischen Heimcomputermarkt, eine Entwicklung ganz im Sinne von Sinclair Research.\n\nNeben der Versorgung von W. H. Smith & Son erlaubten die mittlerweile erweiterten Produktionskapazitäten auch den Export des Erfolgsgerätes. Ab Oktober erfolgten in Zusammenarbeit mit American Express erste Testverkäufe in den USA, dem damals größten Computerabsatzmarkt der Welt. Nach der erfolgreich verlaufenen Testphase startete ab November 1981 der reguläre Versandvertrieb des ZX81. Mit einem konkurrenzlos günstigen Preis von 149,95 $ für das Fertiggerät und 99,95 $ für den Bausatz schnellten die Verkaufszahlen innerhalb von Wochen mit etwa 15.000 monatlich abgesetzten Geräten in unerwartete Höhen. Damit entwickelte sich im Laufe des ersten Halbjahres 1982 die USA zum Hauptabsatzmarkt. Mit dem Erfolg im Rücken konnten zuvor weitere Vertriebsdienstleister auch für Kontinentaleuropa und Japan (Mitsui Exklusivvertrieb) gewonnen werden. In Westdeutschland kam der ZX81 daraufhin Ende 1981 für 398 DM in den Handel, zunächst ausschließlich als Versandware wenig später auch im eigenen Ladengeschäft von Sinclair Deutschland in München. Nachdem der ZX81 unter anderem durch seine Dauerpräsenz auf Messen und Fachausstellungen schnell größere Popularität in Westdeutschland erlangte, wurde ab Mitte 1982 der Vertrieb um Partner wie beispielsweise den bekannten Elektronikanbieter Vobis erweitert.\nEin Vertrieb in die Länder des Ostblocks war aufgrund des CoCom-Embargos untersagt. Diesem fielen auch anfänglich gutgehende zollfreie Verkäufe des ZX81 auf größeren britischen Flughäfen zum Opfer. Nach Ablauf der Exklusivfrist für W. H. Smith & Son wurden weitere bekannte britische Warenhausketten wie \"Boots\", \"John Menzies\" and \"Currys\" in den Vertrieb miteinbezogen. Bis Juli 1983 konnten so weltweit etwa 1,5 Millionen Computer abgesetzt werden. Dieser große Erfolg rief zwischenzeitlich auch zahlreiche Nachahmer – mit oder ohne Lizenz – auf den Plan.\n\nTrotz gut gehender Geschäfte im US-Versandhandel verfügte Sinclair Research nicht über die nötigen Ressourcen zur Erschließung des nordamerikanischen Einzelhandelsmarktes. Als Kooperationspartner für den Ladenvertrieb des ZX81 bot sich das in den USA etablierte Unternehmen Timex Corporation, dessen Zweigstelle in Schottland den ZX81 herstellte, an. Das im Januar 1982 zwischen beiden vereinbarte Lizenzierungsmodell sah eine Gebühr in Höhe von fünf Prozent des Verkaufswertes für Sinclair Computers vor. Timex wurde im Gegenzug zugestanden, das von ihnen in Lizenz zu produzierende und auf die nordamerikanischen Bedürfnisse anzupassende Gerät unter einem eigenen Namen zu vertreiben. Das daraufhin um zusätzliche 1 KB RAM und ein Abschirmblech erweiterte, in einem der Timex-Werke nahe New York City hergestellte Gerät wurde fortan unter dem Namen \"Timex Sinclair 1000\" (kurz \"TS1000\") angeboten. Die Markteinführung in den etwa 70.000 interessierten US-amerikanischen Einzelhandelsgeschäften erfolgte ab Juli 1982. Mit einem Verkaufspreis von 99,95 $ wurden im zweiten Halbjahr 1982 bereits 550.000 Geräte abgesetzt und die Versandverkäufe des ZX81 vom ersten Halbjahr somit deutlich übertroffen. Der Gewinnanteil von Sinclair Research belief sich dabei auf etwa 1,2 Millionen Dollar.\n\nUm den ab 1983 eingetretenen Verkaufseinbrüchen entgegenzuwirken, wurde eilends der \"Timex Sinclair 1500\" (kurz \"TS1500\") – im Wesentlichen ein ZX81 mit 16 KB Arbeitsspeicher und verbesserter Tastatur in größerem Gehäuse – zur Serienreife gebracht. Bei dessen Markteintritt im August 1983 bot die US-amerikanische Konkurrenz bereits wesentlich leistungsfähigere Geräte in Form des Commodore VC 20 und des TI-99/4A ebenfalls zu sehr günstigen Preisen an. In diesem Umfeld gelang es dem technisch veralteten und als Lerncomputer vermarkteten TS1500 nicht, die mittlerweile eher an Unterhaltung und Geschäftscomputern interessierte amerikanische Kundschaft zu gewinnen. Die Verkäufe blieben weit hinter den hochgesteckten Erwartungen zurück, woraufhin die Produktion im Februar 1984 eingestellt wurde.\n\nNeben den offiziellen Nachbauten von Timex existierte eine Vielzahl weiterer Geräte, die dem ZX81 technisch und optisch sehr ähnlich waren. Häufig wurden lediglich Kleinigkeiten geändert oder die Systemsoftware ersetzt. Vielfach wurde der ULA-Spezialbaustein durch Standardchips nachgebildet. So geschehen mit den meisten in Südamerika entstandenen Klonen, wie etwa den in Brasilien in großen Stückzahlen verkauften \"TK82-C\" und \"TK85\" von Microdigital Eletrônica Ltda. Ein daraufhin von Sinclair Research 1983 angestrengtes Gerichtsverfahren wegen Urheberrechtsverletzungen wurde durch ein brasilianisches Gericht jedoch zugunsten von Microdigital Eletrônica Ltda. entschieden, womit dem brasilianischen Hersteller beträchtliche Nachzahlungen an Lizenzgebühren erspart blieben.\n\nTeilweise wurden dem ZX81 nachempfundene Geräte um zusätzliche Funktionalitäten erweitert, wobei durch Austausch der auf Festwertspeicher befindlichen Systemsoftware mit der von Sinclair eine 100-prozentige Abwärtskompatibilität zum Original erreicht wird. In diese Kategorie fällt der \"Lambda 8300\" mit 16 KB RAM ab Werk, Tonausgabe, verbesserter Tastatur und einem Atari-2600-kompatiblen Joystickanschluss (D-Sub: zweireihig 9-polig). Daneben wurden vom Hersteller Lambda Electronics Ltd. aus Hong Kong diverse Erweiterungen wie etwa Farb- und Speicheraufrüstungen angeboten. Vom Lambda 8300 selbst wurden ebenfalls Klone gefertigt, die beispielsweise auch in Westdeutschland in Form des \"Power 3000\" ihre Abnehmer fanden. \n\nÜbersicht der bekanntesten von Sinclair bzw. Timex nicht autorisierten Nachbauten\n\nDie einfache und überschaubare Architektur des Systems ermöglicht den miniaturisierten Nachbau des ZX81 mit heutigen technischen Mitteln bei gleichzeitig überschaubarem Aufwand, obgleich die zugrundeliegende Systemsoftware nach wie vor Urheberrechtsbeschränkungen durch deren Entwickler Nine Tiles Networks Ltd. unterliegt. Eine solche moderne Realisierung erfolgte erstmals 1997 – wie bei anderen Heimcomputersystemen auch – als Implementierung auf einem programmierbaren Logikschaltkreis (FPGA) nebst Einbettungssystem. Die Nachbildungen mittels FPGA-Technologie waren aufgrund der geschützten Systemsoftware lediglich als technische Machbarkeitsstudien gedacht.\n\nDas Gehäuse des ZX81 enthält eine einzelne Platine mit der Rechen- und Speicherbaugruppe, den Peripherieanschlüssen, dem nach außen geführten Systembus für Erweiterungen, der Bildschirmausgabe an einem Fernseher und der Spannungsregelung für das externe Netzteil. Die elektronischen Hauptbestandteile bilden die Z80-CPU (engl. \"central processing unit\"), der \"Sinclair Logic Chip\" in ULA-Technologie und der Arbeits- (RAM) sowie Festwertspeicher (ROM). Zum Lieferumfang gehörten neben dem Computer ein Netzteil (9 Volt, Gleichspannung), das Antennenkabel, ein Kabel zum Verbinden mit einem Kassettenrekorder (zur Aufnahme einer Kompaktkassette, die als magnetisches Speichermedium diente), und die Bedienungsanleitung \"ZX81 BASIC Programming\". Später erfolgte die Auslieferung zusammen mit der 16 KB Speichererweiterung von Sinclair. Ein Netzschalter existiert nicht, die Inbetriebnahme erfolgt durch Einstecken des Netzteils in die Steckdose. Beim Bausatz muss die Bestückung der Leiterplatte und die Endmontage aller Baugruppen durch den Anwender vorgenommen werden.\n\nDie Systemarchitektur der ZX81-Computer basiert auf dem Z80-Mikroprozessor von Zilog beziehungsweise einer Lizenzversion von NEC in der mit 3,5 MHz getakteten Variante. Die Z80-CPU kann auf einen Adressraum von 65536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobytes (KB) festlegt. Aus praktischen Gründen ist es üblich, für Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Dieser wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer in ULA-Technologie gefertigte Spezialbaustein, ein Vorläufer heutiger FPGA-Bauelemente, fasst die im Vorgängermodell ZX80 noch durch separate integrierte Schaltkreise realisierten Funktionen und ZX81-spezifische Zusätze zusammen. Dazu zählen große Teile der Bildschirmansteuerung, die Systemtakterzeugung, die Kassettenschnittstelle, eine rudimentäre Speicherverwaltung und die Schaltungslogik zur Tastaturabfrage. Der in älteren ZX81-Computern verwendete \"ULA2C184E\" wurde später durch das revisionierte Modell \"ULA2C210E\" ersetzt. Das verbesserte Bildsignal konnte damit auf allen damals erhältlichen Fernsehgeräten korrekt dargestellt werden. Die Ausgaben für die verschiedenen Fernsehnormen SECAM, NTSC und PAL wurden durch verschiedene, je nach Absatzmarkt vorgenommene feste Beschaltungen des ULA-Bausteins auf der Platine realisiert.\n\nDer vom Hauptprozessor ansprechbare Adressraum segmentiert sich beim ZX81 in Abschnitte unterschiedlicher Größe. Die 8 KB umfassende und auf Festwertspeicher untergebrachte Systemsoftware nebst BASIC-Interpreter befindet sich im untersten Adressbereich von $0000 bis $1FFF. Daran schließt sich ein freier Block bis $3FFF an, der jedoch von 64-KB-Speichererweiterungen genutzt werden kann. Im Bereich von $4000 bis $43FF befindet sich der im ZX81 verbaute statische Arbeitsspeicher mit einer Kapazität von 1 KB, beim TS1000 mit seinen 2 KB RAM entsprechend von $4000 bis $47FF. Beim Einsatz einer steckbaren RAM-Erweiterung wird diese ebenfalls bei $4000 eingeblendet und reicht je nach Größe bis zur Adresse $FFFF. Dabei kann jedoch nicht der gesamte Adressraum benutzt werden, da die Systemsoftware ab $C000 die Videodaten zum Auslesen durch die CPU einblendet. Der im Computer verbaute Arbeitsspeicher wird bei Benutzung von Speichererweiterungen abgeschaltet. Im RAM ab Adresse $4000 hält die Systemsoftware 125 Bytes an Variablen, den Eingabepuffer, den Systemstack, die vom Benutzer einzugebenden BASIC-Programme und daran unmittelbar anschließend die Bildschirmdaten (minimal 25 Bytes, maximal 793 Bytes) vor.\n\nDie Schwarzweiß-Bildschirmausgabe des ZX81 erfolgt standardmäßig mit schwarzen Zeichen auf weißem Grund über einen koaxialen HF-Antennenanschluss. Dabei koordiniert die Systemsoftware das Zusammenarbeiten der CPU mit dem ULA-Spezialchip, der die eigentliche Bildaufbereitung übernimmt und über den TV-Modulator ausgibt. Die CPU dient lediglich zum Übergeben der im Arbeitsspeicher hinterlegten Bilddaten, da einzig und allein die CPU auf die Inhalte des RAM zugreifen kann.\n\nDer im ULA-Spezialchip enthaltene Bildgenerator ermöglicht für 192 Fernsehzeilen jeweils die Ausgabe von 256 Bildpunkten. Die Systemsoftware nebst BASIC-Interpreter unterstützt dabei nur Elemente mit einer Größe von 8×8 und 4×4 Bildpunkten, ausreichend für einen Textmodus mit 32×24 Zeichen und Blockgrafik mit 64×48 Pixeln (engl. \"picture cell\", Grafikblock). Dazu stellt der ZX81 neben den alphanumerischen auch grafische Zeichen bereit. Dieser Zeichensatz ist jedoch nicht ASCII-konform, was den Austausch mit Grafikdaten anderer Computer und die Ansteuerung von Druckern deutlich erschwert. Bei Computer-Konfigurationen mit mindestens 16 KB RAM kann der Zeichensatz vom Anwender ersetzt werden.\n\nBei Computermodellen mit weniger als 4 KB Arbeitsspeicher werden die Bildschirmzeilen verkürzt im RAM abgelegt, indem alle rechtsstehenden Leerzeichen einer Zeile weggelassen werden. Diese von der Systemsoftware automatisch durchgeführte Datenreduktion ermöglicht eine effizientere Nutzung des Arbeitsspeichers. Durch entsprechende Steuerzeichen fallen für einen vollständig geleerten Bildschirm damit lediglich 25 Bytes an Daten an. Ein ausgefüllter Bildschirm ohne Leerzeichen dagegen führt zu 793 Bytes an Bildschirmdaten, womit bei Geräten mit 1024 Bytes RAM nicht viel Platz für das Programm des Anwenders bleibt. Neben der Belegung des ohnehin knappen Speichers wird zudem die CPU durch die Bilderzeugung stark beansprucht, so dass je nach Darstellungsmodus bis zu 75 Prozent der Rechenleistung dafür aufgebraucht werden. Im \"Slow Mode\" genießt die interruptgesteuerte Bilderzeugung höchste Priorität. Anwenderprogramme werden nur dann ausgeführt, wenn die Bilddarstellung bereits abgeschlossen worden ist. Dieser Modus führt zu einem flackerfreien Bild, es verbleiben jedoch lediglich 25 Prozent der CPU-Rechenleistung für die Aufgaben des Anwenders. Im \"Fast Mode\" wird dagegen zunächst das auszuführende Programm abgearbeitet und erst nach spezieller Freigabe mit der – nun nicht mehr flackerfreien – Bilddarstellung begonnen beziehungsweise sie wird fortgesetzt. Der Anwender kann je nach Aufgabenstellung zwischen diesen beiden Modi wählen. Eine Ausnahme bildet ein Datentransfer über die Kassettenschnittstelle. In diesem Fall schaltet die Systemsoftware automatisch in den Fast Mode, um eine stabile Datenübertragungsrate zu gewährleisten.\n\nDie von der Systemsoftware nicht unterstützten hochaufgelösten Grafikmodi mit 192×256 Pixeln erfordern entsprechende Treiberprogramme von Drittherstellern und zusätzlich etwa 6 KB Arbeitsspeicher, womit zum Betrieb eine Speichererweiterung unumgänglich ist (mit Ausnahme des \"WRX Hi-Res\"-Treibers). Die beiden Betriebsarten \"True Hi-Res\" und \"Pseudo Hi-Res\" basieren auf Grafikzeichen. Lediglich bei True Hi-Res sind die Positionen der Pixel frei wählbar. Das in der Positionierung der Pixel eingeschränkte \"Pseudo Hi-Res\" funktioniert dagegen ohne Hardware-Modifikationen an der Speichererweiterung.\n\nDurch das Fehlen eines PIO-Bausteins sind beim ZX81 keine seriellen oder parallelen Schnittstellen vorhanden. Lediglich der nach außen geführte Systembus kann als 46-poliger Erweiterungssteckplatz genutzt werden. Klinkenbuchsen dienen zum Anschluss eines handelsüblichen Kassettenrekorders und als Netzteileingang.\n\nIn Zusammenhang mit vor allem westlichen Heimcomputern der 1980er-Jahre kamen zur Datensicherung hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die günstigste Variante der Datenaufzeichnung durch Kompaktkassetten hat im Allgemeinen den Nachteil niedriger Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren. Bei Erscheinen des ZX81 standen diesem lediglich Kassettenrekorder als Massenspeicher zur Verfügung.\n\nDer ZX81 verfügt ab Werk über eine Kassettenschnittstelle (Mono-Klinkenbuchse) zum Aufzeichnen und Auslesen von Daten durch handelsübliche Kassettenrekorder. Als Speichermedien dienen entsprechende Kompaktkassetten. Die Datenübertragungsrate beträgt standardmäßig durchschnittlich 300 Bit/s. Zur Gewährleistung einer verlässlicheren Aufzeichnung und damit auch zur Austauschbarkeit von Kassetten verschiedener Geräte wird die Benutzung entweder von Mono-Geräten oder von aussteuerbaren Stereogeräten mit korrekt eingestelltem Magnetkopf empfohlen. Zur Steigerung der Datenübertragungsrate wurden von Drittherstellern verschiedene Zusätze angeboten. Beispielsweise können mit \"QSAVE\" – einem zwischen Kassettenbuchse des Computers und Datenrekorder geschalteter Hardware-Bandfilter – Daten mit Raten von bis zu 3000 Bit/s übertragen werden.\n\nDas kurz nach Markteintritt des ZX81 bereits erhältliche Kassetteninterface \"ZX-99\" von Data-Assette ermöglicht die Ansteuerung von zwei bis zu vier Kassettenrekordern gleichzeitig. Es sind direkte und gepufferte Kopien zwischen den einzelnen Kassettenrekordern möglich, wobei die Steuerung über BASIC-Befehle des ZX81 erfolgt. Entsprechende Treibersoftware wird durch ein 2 KB umfassendes ROM von der Erweiterung bereitgestellt. Die Übertragungsrate von 300 Bit/s entspricht der der ZX81-Systemsoftware. Daneben enthält die Erweiterung eine RS-232-Schnittstelle mit integriertem ASCII-Konverter zur Textausgabe auf entsprechenden Druckern.\n\nBereits im Jahre 1982 kamen von Drittherstellern einfache Diskettensysteme zur Benutzung mit dem ZX81 auf den Markt. Das \"Macronics Disc System\" ist ein Disketteninterface, das auf der Shugart-Schnittstelle basiert. Es erlaubt die Benutzung von entsprechenden 5¼-Zoll-Laufwerken (SA-400) nebst hardsektorierten Disketten. Die Steuerung erfolgt über auf einem 2 KB EPROM bereitgestellte Systemsoftware mit zehn verschiedenen Befehlen. Durch die einfache elektronische Ansteuerung ohne speziellen Floppy-Disk-Controller beträgt die Speicherkapazität pro Diskettenseite lediglich 42,5 KB bei 35 Spuren und 48,75 KB bei 40 Spuren verbunden mit einer mittleren Datenübertragungsrate von etwa 3000 Bit/s. Daneben verfügt die Erweiterung über eine RS232-Schnittstelle zur Ansteuerung von entsprechenden Druckern. Das \"LarKen 1000\"-Diskettensystem besitzt mit einer Speicherkapazität von etwa 46 KB pro Diskette etwa die gleichen Leistungsmerkmale wie das Macronics Disk System, umfasst jedoch eine umfangreichere und leichter zu bedienende Systemsoftware. Später herausgebrachte Geräte wie beispielsweise die vom Unternehmen \"Metrimpex\" erzielen Speicherkapazitäten von bis zu 200 KB pro Datenträger.\n\nZu dem von Sinclair für den ZX81 gelieferten Zubehör zählt ein am Expansionsport anzuschließender Elektro-Erosionsdrucker. Dessen Ausdrucke mit einer maximalen Breite von 32 Zeichen à 8×8 Druckpunkte erfolgen auf schwarzem, einseitig metallisiertem Rollenpapier. Beim Drucken wird das Papierband zwischen einem Paar gegenüberliegender Nadelelektroden, die sich auf einem beweglichen Schlitten befinden, hindurchgeführt. An der Position eines zu setzenden Druckpunktes wird anschließend ein elektrischer Funkenüberschlag (Lichtbogen) zwischen den beiden Elektroden erzeugt. Die helle metallische Beschichtung wird dabei an der Durchschlagstelle verdampft (erodiert) und das dahinter befindliche und unversehrt gebliebene Papier kommt in Form eines schwarzen Punktes zum Vorschein.\n\nDie Ansteuerung des Druckers ist gänzlich mit Hilfe der Systemsoftware des ZX81 möglich. Vermittels des durchgeschleiften Expansionsportes am Stecker des Druckers können mit dem Gerät gleichzeitig weitere Zusätze wie etwa Speichererweiterungen betrieben werden. Durch die minderwertige Mechanik und Qualitätsschwankungen beim Druckpapier sind die erzielten Ergebnisse von unterdurchschnittlicher Qualität. Dennoch ist das Schriftbild lesbar und für den Ausdruck von Programmen zu Archivierungszwecken geeignet, solange das Papier nur geringer Luftfeuchtigkeit ausgesetzt bleibt. Textverarbeitungstaugliche Drucker waren zur selben Zeit um ein Vielfaches teurer, so dass der von Sinclair Computers Ende 1981 anfänglich für nur 49,95 £ angebotene Drucker trotz seiner sehr eingeschränkten Möglichkeiten bis zum Produktionsende Anfang 1984 weite Verbreitung fand. Die Herstellung erfolgte durch Timex Corporation im schottischen Dundee.\n\nEine Weiterentwicklung des Sinclair-Druckers stellt der ursprünglich nur für den nordamerikanischen Vertrieb gedachte \"Timex Sinclair 2040\" dar, der später auch in Europa als \"Alphacom 32\" für 60 £ verkauft wurde. Durch die Benutzung weißen Thermopapiers und ausgereifterer Ansteuerungstechnik wird ein besseres Druckbild in dazu noch kürzerer Zeit als beim Vorgänger erzielt. Daneben ist der Normalpapier verwendende, aber langsame und laute Nadeldrucker \"GP-50S\" von Seikosha ebenfalls mit dem ZX81 ohne Zusatzsoftware benutzbar.\n\nDie im ZX81 verbaute Folienflachtastatur verfügt über 40 Tasten gleicher Größe in QWERTY-Anordnung. Die Leertaste befindet sich im Gegensatz zu den meisten Computertastaturen am unteren rechten Rand. Neben den gebräuchlichen alphanumerischen Zeichen sind durch \"Shift\"- und \"Function\"-Taste 20 grafische Sonderzeichen und BASIC-Befehlszeichen ansprechbar.\n\nDie Tastatur besteht aus drei übereinandergeklebten Kunststofffolien. An der oberen und unteren Folie befinden sich unter den Tasten metallische Kontakte, die nach einem vorgegebenen Schema miteinander verschaltet sind. Die mittlere dicke und elastische Folie dient als elektrische Trennschicht und Rückstellfeder. An der Position der Tasten weist sie Löcher auf, deren Abmessungen groß genug sind, um bei Tastendruck einen Stromfluss zwischen oberer und unterer Folie zu ermöglichen. Da die als Schließer arbeitenden Tasten unergonomisch sind und über keinerlei Druckpunkt verfügen, ist ein effizientes und längeres Arbeiten mit der Tastatur nahezu unmöglich. Viele Benutzer rüsteten diese daraufhin mit Hilfe von Aufsetzerweiterungen um oder benutzten in vielen Fällen eine extern angeschlossene, vollwertige Tastatur von Drittherstellern.\n\nFür den ZX81 erschienen viele Erweiterungen und Umrüstbausätze unterschiedlichen Umfangs, wobei im Folgenden nur die wichtigsten aufgezählt werden.\n\nDer dem Preisdruck geschuldete, sehr knapp bemessene Arbeitsspeicher von lediglich 1 KB ist für die meisten Programmierprojekte bei weitem nicht ausreichend. Neben den Programmdaten des Anwenders befinden sich im RAM die Bildschirmdaten (minimal 25 Bytes, maximal 793 Bytes) und die Systemvariablen (125 Bytes). Bei vollständig gefülltem Bildschirm stehen somit lediglich etwa 100 Bytes zur freien Verfügung, was selbst bei kleineren Anwendungen Speicheraufrüstungen unumgänglich macht. Das kurz nach Verkaufsstart des ZX81 von Sinclair für 49,95 £ angebotene 16-KB-Speichermodul wird über den Erweiterungssteckplatz genutzt. Allerdings ist durch die mangelnde Passgenauigkeit und durch oxidativ verursachte Kontaktprobleme selbst bei kleinsten Erschütterungen – wie sie beim normalen Computergebrauch auftreten können – häufig ein vollständiger Ausfall und damit Datenverlust verbunden. Zudem verfügt Sinclairs Erweiterung nicht über einen durchgeschleiften Steckplatz, so dass eine Hintereinanderschaltung unmöglich ist und damit maximal 16 KB RAM aufgerüstet werden können. Aus diesem Grund und wegen gesunkener Preise griffen viele Anwender zu Speichererweiterungen von Drittherstellern.\n\nEine hochwertiger verarbeitete Speichererweiterung wurde beispielsweise vom Dritthersteller Memotech angeboten. Dessen Erweiterungen verfügen zudem über einen durchgeschleiften Erweiterungsport, womit mehrere Speichermodule à 16 KB hintereinandergesteckt oder mit anderen Erweiterungen von Memotech kombiniert werden können. Allerdings muss der Adressraum, in dem sie jeweils eingeblendet werden sollen, zuvor vom Benutzer über am jeweiligen Modulgehäuse befindliche DIP-Schalter eingestellt werden. Neben den 16-KB-Erweiterungen lieferte Memotech wenig später auch Versionen mit 32 KB und 64 KB RAM.\n\nZur Verbesserung der Grafikfähigkeiten waren ab 1982 eine Vielzahl an Erweiterungen unter anderem für farbige Darstellung und zur Benutzung von hochaufgelöster Grafik oder Kombinationen davon erhältlich. Das am Expansionsport zu betreibende und mit eigenem Farbspeicher versehene \"Color Board\" von Haven Hardware kam 1982 in den Handel. Mit Hilfe entsprechender POKE-BASIC-Befehle können die Zeichen des ZX81 in 16 verschiedenen Farben ausgegeben werden. Dieser Standardzeichensatz kann mit Hilfe des von Kayde Electronic Systems ab 1982 angebotenen \"4K Graphics ROM Board\" auf 900 Grafiksymbole erweitert werden. Die darin enthaltenen Symbole waren zusammen mit einer 16-KB-Speichererweiterung häufig unerlässlich zum Spielen der von Kayde vertriebenen Arcade-Umsetzungen \"Peckman\" (\"Pac-Man\"), \"Centipede\" und \"Space Invaders\". Zur Darstellung hochaufgelöster Grafik waren verschiedene Zusätze erhältlich. Durch das \"HRG-Interface\" von Memotech beispielsweise ist es möglich, hochaufgelöste Grafik mit 192×248 Pixeln auf dem ZX81 darzustellen. Die ab 1982 verkaufte Erweiterung enthielt in einem 2-KB-EPROM die notwendige Treibersoftware mit entsprechenden BASIC-Befehlen zum Setzen von Punkten, Ziehen von Linien usw. Zudem ist horizontale und vertikale Feinverschiebung (Scrolling) möglich.\n\nDer ZX81 wurde ab Werk ohne Möglichkeiten zur Tonerzeugung ausgeliefert. Da die Verfügbarkeit von Spielen und damit verbundener Toneffekte auch für ZX81-Benutzer eine nicht unerhebliche Rolle spielte, wurden von Drittherstellern Erweiterungen unterschiedlichsten Umfangs angeboten. Viele dieser Zusätze benutzen die programmierbaren Spezialbausteine der AY-3-891X-Familie von General Instrument und gängige Sprachsyntheseelektronik. Zu den bekanntesten Erweiterungen für Musik- und Geräuscherzeugung gehören die \"ZXM Soundbox\" (AY-3-8912, eingebauter Verstärker und Lautsprecher) und der \"ZXS Speech Synthesizer\" von Timedata Ltd., die in Zeitschriften oft beworbene \"SON X-81\" (AY-3-8912, eingebauter Verstärker und Lautsprecher) von Bi-Pak und – als eines der ersten angebotenen – das \"QS Sound Board\" (AY-3-8910, Tonausgabe extern) von Quicksilva\n\nDer ZX81 verfügt über keine elektronischen Baugruppen zur Steuerung von Ein- und Ausgaben, wie sie beispielsweise Joystick- oder Standard-Peripherieanschlüsse benötigen. Entsprechende Erweiterungen beispielsweise zum Ergänzen von RS232- oder Centronics-Schnittstellen können nachgerüstet werden. Zu den bekanntesten zählen \"I/F Centronics\" von Memotech, das später auch mit Tastaturpuffer \"Buffer Pak\" für Memotechs externe Tastatur erhältlich war. Daneben existieren weitere wie der als Bausatz erhältliche \"Input-Output Port\" von Maplin, der \"ZX81 Input/Output Controller TE10\" von Thurnall Electronics und beispielsweise ein von Kempston Micro Electronics umgerüsteter Competition-Pro-Joystick zum Einsatz mit Spielen.\n\nWie bei anderen Heimcomputern der 1980er-Jahre auch erfolgte der Vertrieb kommerzieller Software auf verschiedenen Datenträgern. Die insbesondere bei Spieleherstellern beliebten preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes allerdings sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Bei den in der Herstellung vielfach teureren Steckmodulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen von großem Vorteil war. Den besten Kompromiss zwischen Ladezeit, Verlässlichkeit und Speicherkapazität erzielten die Disketten. Die im Jahr 1982 noch sehr kostspieligen Diskettenlaufwerke wurden für den ZX81 kaum unterstützt.\n\nDie Programmpalette für die ZX81-Computer umfasste neben der von Sinclair Research vertriebenen Auswahl kommerzieller Programme aller Art auch eigenentwickelte und in Zeitschriften und Büchern publizierte Software (Listings) zum Abtippen. Die meisten der kommerziellen Programme wurden auf Kompaktkassette angeboten, einige wenige waren auf Steckmodul erhältlich.\n\nVon der in Umlauf befindlichen Software machten illegale Kopien („Raubkopien“) stets einen großen Teil aus und stellten damit kleinere Softwareentwickler häufig vor existentielle wirtschaftliche Schwierigkeiten. Daraufhin wurden zunehmend Kopierschutzsysteme insbesondere bei Spielen als der meistverkauften Software eingesetzt.\n\nDie Konfiguration der ZX81-Hardware, wozu auch die Ansteuerung der Kassettenschnittstelle gehört, fällt in den Aufgabenbereich des Betriebssystems \"Sinclair OS\". Es handelt sich dabei um eine von Nine Tiles Networks Ltd. vorgenommene Weiterentwicklung des Betriebssystems vom ZX80. Zur Optimierung des Zusammenwirkens mit dem ebenfalls auf dem ZX81-ROM befindlichen Sinclair-BASIC-Interpreter und des Druckertreibers sind alle Einzelbestandteile programmtechnisch sehr eng miteinander verwoben.\n\nZur Vermarktung des ZX81 als massentauglicher Lerncomputer legte man bei der Entwicklung besonderes Augenmerk auf eine im Gerät integrierte und leicht zu erlernende Programmiersprache. Das bereits im ZX80 verbaute Sinclair-BASIC, eine unvollständige Adaption des ANSI Minimal BASIC, wurde daraufhin grundlegend überarbeitet und der das BASIC enthaltende Festwertspeicher auf 8 KB vergrößert. Die Ausführung der Programmierarbeiten wurde dem Unternehmen Nine Tiles Networks Ltd. übertragen, die umfangreiche Erweiterungen sowohl am Interpreter als auch am Editor zum Eingeben und Korrigieren der Programmdaten vornahm. Zu den grundlegenden Neuerungen gehörten vereinfachte Editiermöglichkeiten und die Fehlerauswertung bereits nach Eingabe einer Programmzeile. Letzteres stellte ein Novum für Interpretersprachen im Heimcomputerbereich dar, für die bis dahin eine Fehleranalyse selbst für erfahrene Programmierer mit größerem Aufwand verbunden war. Daneben wurde eine vollwertige Fließkommaarithmetik und Kommandos zum Bedienen des Sinclair-Druckers integriert. Das Sinclair-BASIC entspricht nicht dem damals gängigen De-facto-Standard in Form des Microsoft BASIC – es handelt sich vielmehr um einen eigenen Dialekt. Auf dem ZX81 erstellte Sinclair-BASIC-Programme sind ohne zusätzliche Modifikationen damit nicht auf anderen Computern lauffähig.\n\nZur Erhöhung der Ausführungsgeschwindigkeit von BASIC-Programmen können diese durch Compiler in ausführbare, d. h. ohne BASIC-Interpreter lauffähige Maschinencodeprogramme umgewandelt werden. Der Umfang der unterstützten BASIC-Befehle und Variablen ist jedoch eingeschränkt. Einer der bekanntesten Compiler ist der \"M-Coder\" von P.S.S.\n\nNeben dem Sinclair-BASIC stehen für den ZX81 die Programmiersprachen \"ZX Forth\" von Sinclair Research und der auf Steckmodul erschienene \"Z80-Assembler\" von Memotech zur Verfügung.\n\nDen mit Abstand größten Teil der sowohl kommerziellen als auch frei erhältlichen ZX81-Software stellen die Spiele dar. Darunter befinden sich in erster Linie Arcade-Umsetzungen, aber auch genredefinierende Originale wie \"3D Monster Maze\" als Begründer der dreidimensionalen Labyrinthspiele. Insgesamt erschienen etwa 100 Spiele unterschiedlicher Hersteller, darunter \"Manic Miner\", \"Mazogs\", \"Pimania\" sowie einfache Schach-, Text- und Schießspiele.\n\nIn den 1980er-Jahren spielten neben den Fachbüchern die Computerzeitschriften für viele Heimcomputerbesitzer eine große Rolle. Die häufig monatlich erschienenen Ausgaben enthielten Testberichte zu Neuheiten, Programmieranleitungen und Software zum Abtippen. Sie dienten weiterhin als Werbe- und Informationsplattform sowie zur Kontaktaufnahme mit Gleichgesinnten.\n\nFür die ZX81-Benutzer waren verschiedene auf ihre Bedürfnisse zugeschnittene Kiosk- und Abonnement-Publikationen erhältlich. Zu den bekanntesten kommerziellen Computermagazinen zählen \"Sinclair User\", \"Sinclair Programs\" und \"Sinclair Projects\". Speziell für deutsche Interessenten erschien in Westdeutschland die deutschsprachige Zeitschrift \"ZX User Club\".\n\nNach dem Ende der Heimcomputerära Anfang der 1990er-Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Ende der 1990er-Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripherie entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reicht mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit u. a. ein verstärktes Transferieren von sonst möglicherweise verloren gegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung „digitaler Kultur“ geleistet wird.\n\nAls leistungsfähigste Emulatoren gelten der im Webbrowser lauffähige Java-basierte \"Timex/Sinclair 1000 Emulator\" und der für Linux und andere Systeme erhältliche \"ZEsarUX\".\n\nBei Erscheinen des ZX81 fielen die Beurteilungen unterschiedlich aus. Die Kritiken reichten dabei von vollständiger Ablehnung als „überteuerte Spielkonsole“ über „als Lehr- und Lernhilfsmittel wird der ZX 81 voll verwendungsfähig“ bis hin zu euphorischen Betitelungen als „Meilenstein, was Integration von Computertechnologie betrifft.“\n\nPositiv aufgenommen wurde die Beseitigung der Mängel des Vorgängermodells bei gleichzeitig kompakt gebliebenen Abmessungen und besserer Verarbeitung. Auch das erweiterte BASIC mit seiner einsteigerfreundlichen Fehlerbehandlung und ausführlichen Dokumentation wusste zu gefallen. Bemängelt wurde nahezu einstimmig die für längeren Gebrauch nicht geeignete und auch nicht ASCII-konforme Tastatur. Ebenso kritisiert wurde die geringe Ausführungsgeschwindigkeit des BASIC und seine Inkompatibilität zum damaligen De-facto-Standard des Microsoft BASIC. Aufgrund der minimalen Ausstattung und einer nur bedingten Erweiterbarkeit wurde der ZX81 hauptsächlich als Einsteigermodell charakterisiert. Für einen derart niedrigen Preis könne man mit einem Kauf eigentlich nichts falsch machen und beispielsweise Kindern („Children will love the ZX81“) damit eine gute Gelegenheit zum Heranführen an die Computerwelt bieten, so die einhellige Meinung aller Rezensenten.\n\nBereits kurz nach Produktionsende Mitte der 1980er-Jahre wurde der Rechner als „ein Klassiker unter den Heimcomputern“ bezeichnet, der „absolut billig“ sei und dennoch eine „große Leistung“ erbringe.\n\nDer ZX81 wird mittlerweile von Internetseiten, Zeitschriften und Büchern wieder verstärkt wahrgenommen und rückblickend übereinstimmend neben dem ZX80 als erster preiswerter Heimcomputer eingeordnet („Den ZX81 kann sich jeder leisten“). Selbst „ohne Farbe, Ton und Joystick“ sei der „Minirechner“ und „verbesserte ZX80 zum reduzierten Preis“ sogar „als Spielgerät von Bedeutung gewesen.“ Der ZX81 habe den Computermarkt den Massen und damit völlig neue Perspektiven eröffnet:\n\nUrsächlich für diesen großen Erfolg sei die korrekte Marktanalyse seitens Sinclair Research gewesen, dass die Zukunft der Heimcomputerbranche in der Massenproduktion von einsteigerfreundlichen Geräten mit entsprechend niedrigen Verkaufspreisen liege. Die Kombination von technischen Innovationen (ULA-Baustein) und Billigkomponenten zur kompromisslosen Preissenkung, eine eingebaute höhere Programmiersprache und massive Werbung als Zukunftsprodukt hätten sich dabei als die richtige Vorgehensweise zur erfolgreichen Vermarktung erwiesen.\n\nDarüber hinaus entwickelte sich im Fahrwasser des ZX81-Erfolgs eine ganze Industrie von Zulieferern. Gedruckte Publikationen speziell zu Computerthemen kamen auf und viele bekannte Softwarehersteller unternahmen ihre ersten Gehversuche. Der bis dahin eher Spezialisten vorbehaltene Computermarkt war zu einer ernstzunehmenden Branche mit Millionenumsätzen in der Unterhaltungsindustrie aufgestiegen („More importantly, a new consumer electronics category was born“). Der Erfolg des ZX81 etablierte Sinclair Research endgültig als festen Bestandteil der bis dahin ausschließlich aus US-Unternehmen wie Apple, Atari, Commodore und Tandy bestehenden Riege von Heimcomputerherstellern. Für den Gründer von Sinclair Research und treibende Kraft hinter all den Neuerungen, Clive Sinclair, sollte sich der Erfolg nicht nur finanziell auszahlen. Neben Gewinnbeteiligungen in Millionenhöhe wurde ihm im Juni 1983 zudem die englische Ritterwürde zuteil.\n\n\n"}
{"id": "18231", "url": "https://de.wikipedia.org/wiki?curid=18231", "title": "SIGGRAPH", "text": "SIGGRAPH\n\nDie SIGGRAPH (\"Special Interest Group on Graphics and Interactive Techniques\") ist eine Themengruppe der Association for Computing Machinery (ACM), die sich mit Computergrafik beschäftigt. \n\nEine Tagungsreihe desselben Namens findet seit 1974 jährlich in den USA statt. Seit 2008 gibt es zudem mit der SIGGRAPH Asia eine entsprechende Konferenz in Asien, die ebenfalls jährlich durchgeführt wird.\n\nSIGGRAPH ist die größte und bekannteste Konferenz zum Thema. Eine vergleichbare europäische Vereinigung ist Eurographics.\n\n"}
{"id": "18271", "url": "https://de.wikipedia.org/wiki?curid=18271", "title": "Pixel", "text": "Pixel\n\nMit Pixel, Bildpunkt, Bildzelle oder Bildelement (selten \"Pel\") werden die einzelnen Farbwerte einer digitalen Rastergrafik bezeichnet sowie die zur Erfassung oder Darstellung eines Farbwerts nötigen Flächenelemente bei einem Bildsensor beziehungsweise Bildschirm mit Rasteransteuerung. „Pixel“ (Nominativ Singular: das Pixel; Genitiv: des Pixels; Plural: die Pixel) ist ein Kunstwort aus den Abkürzungen der englischen Wörter \"picture\" (umgangssprachlich verkürzt „pix“) und \"element\" und wird oft mit px abgekürzt.\n\nDie Pixel einer Rastergrafik sind rasterförmig angeordnete Punkte, denen eine Farbe zugeordnet ist.\n\nOft werden Pixel als rechteckig oder quadratisch betrachtet. Dies ist jedoch eine nicht allgemeingültige Modellvorstellung. Im Sinne der digitalen Signalverarbeitung ist ein Pixel ein diskreter Abtastwert; über andere Punkte als die Pixel lassen sich keine Aussagen treffen. Deutlich wird dies bei der Vergrößerung von Rastergrafiken: Das Erscheinungsbild variiert je nach gewählter Skalierungsmethode, und die Pixel des Ausgangsbildes erscheinen in der Vergrößerung nicht zwangsläufig als Quadrate (siehe Bild rechts). Ein vergleichbarer Fehler wäre es, die Abtastwerte eines digitalen Audiosignals als über ein bestimmtes Zeitintervall gleichbleibende Werte zu interpretieren, weil das Signal vom Audioeditor in der Vergrößerung treppenartig dargestellt wird.\n\nDie Modellvorstellung eines quadratischen Pixels ist unangemessen, weil sie die unterschiedlichen Möglichkeiten bei der Umwandlung von Bildinhalten zu Rastergrafiken vernachlässigt. In der Computergrafik, bei der künstliche Bilder erzeugt werden, können die gewünschten Bildinhalte als Vektorgrafik, 3D-Szenenbeschreibung oder eine andere Art der Bildbeschreibung vorliegen. Diese Bildbeschreibung definiert ein kontinuierliches Signal, das in eine Rastergrafik umgewandelt (\"gerastert\" oder \"gerendert\") werden muss, indem die Bildinhalte abgetastet werden. Der verwendete Rekonstruktionsfilter bestimmt, wie die Farben der ursprünglichen Bildbeschreibung in der Nähe eines Pixels gewichtet werden und in die Pixelfarbe einfließen. Bei einem bilderfassenden System, das ein natürliches Bildsignal auf einer optischen Bildebene oder -zeile digitalisiert, bestimmt sich der entsprechende theoretische „Rekonstruktionsfilter“ (die Punktspreizfunktion) durch die optischen und elektronischen Elemente des Systems.\n\nEs ist zwar möglich, Pixel als Quadrate zu betrachten, dies ist jedoch allenfalls dann sinnvoll, wenn als Rekonstruktionsfilter ein Box-Filter gewählt wurde, denn hier würde der Farbwert eines Pixels dem Mittelwert aller Farbbeiträge innerhalb des Quadrates entsprechen. Für die nachfolgende Bildbearbeitung muss jedoch das resultierende Pixel allgemein als diskreter Abtastwert betrachtet werden. Wenn von „einer Pixelbreite“ die Rede ist, dann ist damit tatsächlich der Abstand zwischen zwei benachbarten Pixeln gemeint; der „Mittelpunkt“ eines Pixels bezeichnet in Wirklichkeit das Pixel selbst. Mit Formulierungen wie „Überdeckung eines halben Pixels“ ist die Überdeckung der Bildinhalte durch den verwendeten Rekonstruktionsfilter gemeint.\n\nDie im Pixel verwendete Kodierung der Farbe definiert sich unter anderem über den Farbraum und die Farbtiefe. Der einfachste Fall ist ein Binärbild, bei dem ein Pixel einen Schwarzweiß-Wert speichert.\n\nNeben Farbinformationen können Rastergrafiken auch einen sogenannten Alphakanal enthalten, der Transparenzinformationen enthält. Die Speicherung beliebiger weiterer Informationen ist denkbar; das genaue Format ist vom verwendeten Grafikformat abhängig. Nur bei der Ausgabe auf dem Bildschirm muss ein bestimmtes Format beachtet werden, das vom Bildspeicher der Grafikkarte vorgegeben ist.\n\nSowohl die Bildauflösung (und damit die örtliche Abtastrate) als auch die Größe der im Pixel gespeicherten Informationen (etwa die Farbtiefe) ist in der Praxis begrenzt, weshalb ein Pixel nur eine Annäherung der Wirklichkeit darstellen kann.\n\nDie Begrenztheit der örtlichen Abtastrate führt dazu, dass Bildinformationen verlorengehen. Gemäß dem Nyquist-Shannon-Abtasttheorem kann es bei bestimmten Bildinhalten und zu geringer Abtastrate oder Auflösung zu Alias-Effekten oder dem Treppeneffekt („pixelige“ Darstellung) kommen. Diesen Effekten kann durch Antialiasing entgegengewirkt werden; das Antialiasing in der Computergrafik bedient sich dazu unterschiedlicher Methoden. Das Antialiasing optischer Signale kann durch einen Tiefpass erfolgen, kombiniert mit einer Aperturkorrektur.\n\nDas Verkleinern, Vergrößern oder Drehen einer Rastergrafik kann zu unscharfen oder fehlerhaft wirkenden Bildern führen.\n\nDie Pixel eines Bildsensors oder Bildschirms bestehen üblicherweise aus Flächen jeweils einer Grundfarbe (Rot, Grün und Blau). Bei Flüssigkristall-Bildschirmen (LCD= liquid crystal display) wird jedes sichtbare Bildelement mit einem Farbwert angesteuert. Die für die Grundfarben des Pixels zuständigen Flächen, Subpixel genannt, sind oftmals aneinander anliegend angeordnet. Die im Vergleich zum Pixel feinere Subpixelstruktur kann dazu genutzt werden, um die horizontale Auflösung bei der Rasterung zu erhöhen (Subpixel-Rendering). Ferner sind auch dreieckige, unregelmäßig große, alternierend angeordnete oder zusätzliche weiße Subpixel möglich, zum Beispiel bei den \"PenTile\"-Pixelgeometrien von Samsung. Bei manchen, besonders älteren, Flachbildschirmen können herstellungsbedingt sogenannte Pixelfehler auftreten.\n\nRöhrenbildschirme projizieren das Bildsignal mittels Elektronenstrahlen auf eine Leuchtstoffmatrix mit festgelegter Auflösung. Die vor der Leuchtstoffschicht montierte Schlitz-, Streifen- oder Lochmaske garantiert zwar, dass nur die zu den jeweiligen Elektronenstrahlen gehörenden Grundfarben angeregt werden. Wegen des relativ breiten und angenähert normalverteilten Intensitätsprofils der Elektronenstrahlen sowie Verzeichnung und Streulicht stimmen die Bildpunkte der Leuchtstoffmatrix jedoch nicht genau mit den zu erwartenden Pixeln überein, selbst wenn die ausgegebene Auflösung der physischen Auflösung des Röhrenbildschirms entspricht.\n\nDie physische Größe eines Pixels hängt vom Gerät ab. Die Pixeldichte eines Bildschirms oder Scanners wird in \"pixel per inch\" (ppi) bzw. \"dots per inch\" (dpi) angegeben. Handelsübliche Computerbildschirme erreichen eine Pixeldichte von ungefähr 100 ppi, entsprechend 0,3 Millimeter pro Pixel. Bei Fernsehern ist die Pixeldichte meist niedriger und bei neueren Smartphones um ein Vielfaches höher, während die Sensoren von Scannern und Digitalkameras mehrere Tausend ppi erreichen können. Die Anzahl der in Bildsensoren maximal verwendbaren Pixel wird oft in Megapixeln angegeben, wobei aber meist nur die Farbpunkte eines Bayer-Sensors gemeint sind und nicht die Bildpunkte. Das Seitenverhältnis eines Pixels auf dem Bildschirm (englisch \"pixel aspect ratio\") muss nicht zwingend 1:1 sein; die meisten SDTV-Videonormen schreiben unregelmäßige Pixel-Seitenverhältnisse vor. Die Pixelgröße sowie der Pixelabstand im Verhältnis zur Bildauflösung haben entscheidenden Einfluss auf die Lesbarkeit und Erkennbarkeit von Texten und Grafiken auf Computermonitoren und Fernsehern.\n\nDie Bezeichnung „Bildpunkt“ im Sinne einer kleinen Anzeigeeinheit eines Gerätes wurde zuerst 1884 in Paul Nipkows Patentschrift für sein \"Elektrisches Teleskop\" verwendet, allerdings war der Begriff bereits vorher in der Optik üblich.\n\nDie Bezeichnung „“ wurde ab 1911 in diversen US-amerikanischen Patentschriften verwendet. Als in den 1950er und 1960er Jahren das Einscannen, die Bearbeitung und die Anzeige von Bildern mittels Computern möglich wurde, verwendete die Fachliteratur meist andere Begriffe wie „“, „“, „“, „“ oder „“.\n\nDie ältesten bekannten Dokumente, in denen der Begriff „Pixel“ vorkommt, sind Fred C. Billingsleys 1965 veröffentlichte Artikel \"Digital Video Processing at JPL\" und \"Processing Ranger and Mariner Photography\" in den Proceedings Vol. 0003 bzw. 0010 der SPIE. Die weniger gebräuchliche Bezeichnung \"Pel\" wurde von William F. Schreiber als Teil seines Artikels \"Picture Coding\" in den IEEE-Proceedings Vol. 55 im März 1967 veröffentlicht.\n\nDer Begriff „Pixel“ wird auch in Bezeichnungen für bestimmte Anwendungen von Rastergrafiken verwendet, etwa Pixelfonts, Pixel-Art und Pixel-Banner. Von „Pixel“ abgeleitet ist der Begriff \"Voxel,\" der unter anderem das dreidimensionale Äquivalent eines Pixels bezeichnet, sowie der in der Bildsynthese verwendete Begriff \"Texel\" für Pixel einer Textur.\n\n\n"}
{"id": "18371", "url": "https://de.wikipedia.org/wiki?curid=18371", "title": "Heimcomputer", "text": "Heimcomputer\n\nHeimcomputer (vom engl. \"home computer\") war eine in den 1980er-Jahren gebräuchliche Bezeichnung für eine Klasse von Mikrocomputern, die vor allem in Privathaushalten genutzt wurden. Per Definition handelt es sich bei diesen Geräten im engeren Sinn um Personal Computer, wobei der Heimcomputer den Teil solcher Geräte umschließt, die im unteren Preissegment zu finden und eher für Unterhaltungszwecke und zum Programmieren durch den Anwender vorgesehen waren. Durch die rasant wachsende Verbreitung der Heimcomputer im Privatbereich kamen erstmals breitere Bevölkerungsschichten mit Computern in Kontakt, die noch wenige Jahre zuvor nur Fachpersonal in Unternehmen zugänglich gewesen waren.\n\nZu den verbindenden Merkmalen fast aller frühen Heimcomputer der 1980er gehörte, dass sie eine ins Gehäuse integrierte Tastatur hatten, mit der sie über Kommandozeilen-Befehle bedient wurden, zum Anschluss an einen Fernseher konzipiert waren, und dass das auf einem ROM-Speicherchip fest installierte Betriebssystem auch einen Interpreter für die einfach zu lernende Programmiersprache BASIC aufwies. Die meistverbreiteten Mikroprozessoren waren die der 6502-Familie und der Zilog Z80, die meist mit etwa 1 MHz bis 4 MHz getaktet waren und auf anfangs 1 bis 16, später bis zu 64 kByte Arbeitsspeicher zurückgriffen.\n\nAb etwa Ende der 1980er-Jahre hatten Modelle mit dem Prozessor Motorola 68000 großen Erfolg, die bereits grafische Benutzeroberflächen mit Maus-Bedienung aufwiesen und mit Speicher im Megabyte-Bereich ausgerüstet waren. Mitte der 1990er-Jahre verschwanden jedoch die meisten dieser untereinander nicht kompatiblen Systeme vom Markt, und PCs mit dem Windows-Betriebssystem und x86-Prozessoren setzten sich auch im Privatbereich als Standard durch.\n\nDie Entwicklung der Heimcomputer ging einher mit der Entwicklung der Spielkonsolen und Computerspiele. Zu einigen Heimcomputermodellen gab es technisch fast identische Geräte als Spielkonsole, denen lediglich die Tastatur fehlte.\n\nDer erste digitale, programmierbare Computer für den Heimgebrauch war der bereits 1949 von Edmund Berkeley, dem Begründer der ACM, vorgestellte Relais-Rechner Simon. Simon bestand aus nur 50 Relais und verfügte über keinen Mikroprozessor. Im Handel erhältlich war lediglich ein Bauplan für Simon, von dem in den ersten zehn Jahren seiner Verfügbarkeit über 400 Exemplare verkauft wurden.\n\nDer Micral N war ein weiterer Vorläufer auf dem Weg zum Heimcomputer; der erste seiner Art mit einem Mikroprozessor, in diesem Fall ein Intel 8008. Die Produktion begann im Frühjahr 1973. Mit dem \"Altair 8800\" des Anbieters MITS kam 1975 ein in Serie produziertes Gerät auf den Markt, das als Bausatz für 397 US-Dollar, als Komplettgerät für 695 US-Dollar zu erwerben war. Zukunftsweisend war die Ausstattung mit einem Bus-Stecksystem für Erweiterungskarten nach dem S-100-Bus-Standard. Mit seinen Kippschaltern als Eingabeeinheit und Leuchtdioden als Ausgabeeinheit entspricht jedoch auch dieses Gerät technisch nicht dem, was man heute unter einem Heimcomputer versteht. Ähnlich war es mit dem im selben Jahr erschienenen \"KIM-1\" des Unternehmens MOS Technology, der immerhin schon eine 24-Tasten-Eingabeeinheit im Taschenrechnerformat zur direkten Eingabe von HEX-Code hatte sowie eine 6-stellige 7-Segment-LED-Anzeige als Ausgabeeinheit. Daher waren diese Geräte für durchschnittliche Privatanwender weitgehend untauglich und auch kaum attraktiv; sie wurden hauptsächlich von Hobby-Elektronikbastlern und den wenigen frühen Computer-Enthusiasten gekauft. Die verkauften Stückzahlen waren daher gering.\n\nInnerhalb der frühen Hacker-Szene rund um den Homebrew Computer Club, die die Entwicklung des Heimcomputers entscheidend vorantrieb, erfreute sich der \"MITS Altair 8800\" großer Beliebtheit und diente den Mitgliedern des Clubs als Kernstück für eigene Erweiterungen.\n\nDie Anfänge der Heimcomputer im modernen Sinn, mit schreibmaschinenähnlicher Tastatur als Eingabeeinheit und einen Bildschirm (zunächst in Form eines umfunktionierten Fernsehgerätes) als Ausgabeeinheit, begannen 1976 mit dem Apple I. Er war als Einplatinencomputer ausgeführt, wobei der Anwender selbst noch Teile wie Netzteil, Gehäuse, Bildschirm und Tastatur besorgen und aufwändig konfigurieren musste. Als erstes Gerät der Welt war er mit 666 US-Dollar für Privathaushalte erschwinglich und entsprach zugleich den modernen bedientechnischen Vorstellungen eines Heimcomputers.\n\nAuf der Consumer Electronics Show, die im April in Las Vegas (Nevada) stattfand, wurden 1977 erstmals gleich drei Modelle der interessierten Öffentlichkeit vorgestellt, die nicht als Bausatz, sondern als fertig montierte Geräte sofort für den häuslichen oder betrieblichen Einsatz einsatzbereit waren. Dazu zählen der preisgünstige TRS-80 (599 US-Dollar) der großen Elektronikkette RadioShack, der mit einer offenen Architektur aufwartende, aber teure Apple II (1298 US-Dollar) und das All-in-one-Modell Commodore PET 2001 (795 US-Dollar).\n\nIm Jahre 1979 erschien mit dem Atari 400 der erste Heimcomputer auf dem Markt, der \"Custom Chips\" enthielt und damit technologisch den Konkurrenzgeräten voraus war. Im gleichen Jahr verbaute Texas Instruments im TI-99/4 erstmals einen 16-Bit-Prozessor, wenngleich sich der Rechner nicht durchsetzen konnte. Im Jahr 1980 kam in Großbritannien der ZX80 von Sinclair in den Handel, der auf dem Hauptprozessor Z80 von Zilog basierte und erstmals eine Heimcomputerindustrie außerhalb Nordamerikas zu etablieren half.\n\nIn den Jahren von 1977 bis 1980 dominierte Tandy mit dem TRS-80 den Markt der Mikrocomputer in den USA; den damals allgegenwärtigen RadioShack-Ladengeschäften hatten die Mitbewerber kein gleichwertiges Vertriebsnetz entgegenzusetzen. Außerhalb der USA war der Markt für Mikrocomputer damals noch sehr klein, was auch mit den dort viel höheren Preisen für solche Geräte zusammenhing. In Deutschland war Commodore der Marktführer, gefolgt von Atari und Sinclair.\n\nIBM war einer der führenden Produzenten von Großrechnern; lange Zeit hatte die Unternehmensführung den neuen Markt der persönlichen Computer für nicht lukrativ gehalten und vernachlässigt. 1981 änderte sich dies mit der Veröffentlichung des IBM-PCs. Nachdem ein Vertrag mit Digital Research gescheitert war, dem damals führenden Anbieter von Mikrocomputer-Betriebssystemen, wurde auf ein Angebot von Microsoft zurückgegriffen, das bei IBM fortan PC-DOS genannte MS-DOS. Die Marktposition von IBM sorgte dafür, dass sich der IBM-PC im Bürobereich schnell durchsetzte – viele Unternehmen wollten ihre Arbeitsplatzcomputer aus einer Hand beziehen. Für den Heimgebrauch war der IBM-PC mit seinen 3.005 US-Dollar (in der Grundausstattung) zu teuer. Das Unternehmen IBM legte jedoch die Grundkonstruktion seines PC offen und schuf einen informellen Industriestandard; es definierte damit die bis heute aktuelle Geräteklasse der \"„IBM-PC-kompatiblen Computer“\". Zahlreiche preiswerte Nachbauten und Fortführungen der IBM PCs durch andere Unternehmen machten die Plattform sowohl am Arbeitsplatz als auch im Heimbereich sehr erfolgreich; die heute marktüblichen Computer mit Windows-Betriebssystem und x86-Prozessoren beruhen auf der stetigen Weiterentwicklung des damaligen Entwurfs von IBM.\n\nEnde 1982 brachte Commodore den C64 als Nachfolger des VC20 auf den Markt. Aufgrund seines im Vergleich mit den „professionellen“ Computern wie dem Apple II und dem IBM PC wesentlich günstigeren Preises wurde der sogenannte „Brotkasten“ schnell zum meistverkauften Homecomputer aller Zeiten. Zur etwa selben Zeit vermarktete auch Sinclair Research mit großem Erfolg seine beiden populären Modelle, den Sinclair ZX81 und dessen Nachfolger ZX Spectrum. Im Gegensatz zu diesen beiden führte Sinclairs letzter technisch fortschrittlicher Rechner, der seit 1984 erhältliche Sinclair QL („Quantum Leap“, Quantensprung), wegen Produktionsverzögerungen, Qualitätsmängeln und Fehlern bei der Vermarktung nur noch ein reines Nischendasein im Computermarkt.\n\nAtari brachte 1982 aufgrund des wachsenden Konkurrenzdrucks mit der XL-Serie eine intern nur geringfügig veränderte Version der 400/800er Serie heraus. Diese Computer waren preisgünstiger als ihre Vorgänger und weitgehend softwarekompatibel. Ihre Verkaufszahlen lagen in Deutschland hinter Commodore auf dem zweiten Platz.\n\nIn den Jahren 1984 und 1985 wurde die von Amstrad lizenzierte CPC-Serie von der Schneider Computer Division (einer Abteilung der Schneider Werke) in Deutschland auf den Markt gebracht. Die Computer dieser Serie waren in Deutschland, im Ursprungsland Großbritannien und insbesondere in Frankreich und Spanien sehr erfolgreich. Die beiden Modelle CPC 464 und CPC 6128 wurden in Deutschland in den Jahren 1985 und 1986 jeweils zum Computer des Jahres gekürt.\n\nApple konzentrierte sich mittlerweile nach einigen Misserfolgen mit neuen Modellen wie dem Apple III auf den avantgardistischen, extrem bedienungsfreundlichen und auch teuren Personal Computer Apple Macintosh. Es errang mit diesem im High-End-Bereich eine führende Position, insbesondere beim zukunftsträchtigen Desktop Publishing und der Computer-gestützten grafischen Gestaltung.\n\n1985 kam mit dem Commodore Amiga und dem Atari ST jedoch bereits eine neue Generation von Heimcomputern auf den Markt, die in der damaligen technologischen Spitzenklasse mitspielte. Beide verwendeten den Motorola-68000-Prozessor, der auch im Macintosh eingesetzt wurden, und boten im Heimbereich bis dahin unbekannte grafische Möglichkeiten. Prozessor- und Speicherausstattung konnten leicht mit dem Spitzenmodell der IBM-PC-Serie, dem IBM AT, mithalten. Neu war auch die grafische Benutzeroberfläche. Beim Atari ST lehnte sich das „Look and Feel“ stark an den Apple Macintosh an, der Amiga zeigte hier mehr Eigenständigkeit. Windows dagegen steckte damals noch in den Kinderschuhen; kein PC-Benutzer kam ohne DOS-Kenntnisse aus, das per Texteingabe über eine Kommandozeile bedient wurde.\n\nAufgrund dieser Vorteile und in Verbindung mit dem günstigen Preis erreichte der Atari ST in Europa in den ersten beiden Verkaufsjahren schnell hohe Verkaufszahlen und wurde dank seines hochauflösenden Schwarz/Weiß-Monitors auch im professionellen Bereich eingesetzt (Desktop-Publishing, Buchhaltung, Sekretariat, Kassencomputer). Durch die eingebauten MIDI-Schnittstellen eroberte er sich aber vor allem eine führende Stellung in der Musikproduktion.\n\nAb 1987, nach Erscheinen des preiswerten Amiga 500, überstieg der Marktanteil des Amiga die Verkaufszahlen des Atari ST bei weitem. Die erheblich verbesserte Grafik- und Soundausgabe des Amiga (gegenüber dem C64) machte den Rechner zu einem preiswerten Computer vor allem für Spieler, ebenso, dass Commodore für den Amiga vor allem Farbmonitore anbot. Während Atari die Produktion von Computern bereits 1994 einstellte, wurden Amigas noch bis 1996 produziert.\n\nEtwa zur selben Zeit erschien auch der erste Archimedes-Computer des britischen Herstellers Acorn, der auf 32-Bit-RISC-Prozessoren beruhte und damit technologisch seiner Zeit weit voraus war. Er erreichte aber nie eine ausreichende Marktdurchdringung, das Software-Angebot blieb klein. Bis 1998 hatte Acorn jedoch die Entwicklung beibehalten, bevor die Produktion eingestellt wurde. Die damals bei Acorn entwickelte, sehr fortschrittliche Prozessortechnologie wird bis heute als ARM-Architektur von der Acorn-Nachfolgefirma ARM Limited weiterentwickelt und an Prozessor-Hersteller wie etwa Samsung lizenziert. Fast alle heutigen Smartphones und Tablet Computer, wie das Apple iPhone und die Gerätereihe Samsung Galaxy, basieren auf einem Prozessor mit ARM-Technologie.\n\nAls Massenspeicher wurden außerhalb der USA vor allem handelsübliche Kompaktkassetten (Audiokassetten) genutzt, teilweise mit speziellen einfachen Kassettenrekordern, im Falle des C64 Datasette genannt, teilweise über gewöhnliche Musik-Kassettenrekorder. Diskettenlaufwerke, gewöhnlich im Format 5¼ Zoll, gab es meist als Zubehör, wobei diese oft den Preis des Grundgeräts erreichten oder übertrafen. In den USA waren sie dennoch verbreiteter als die langsamen, fehleranfälligen und unpraktischen Kassetten. Als Bildschirm diente meist der Fernseher statt eines speziellen Computermonitors, weshalb auch von den meisten Heimcomputer-Modellen leicht unterschiedliche PAL- und NTSC-Modelle existierten, je nach der Fernsehnorm des Verkaufslandes. Die Heimcomputer waren meist mit einem Grafikchip und einem Soundchip bestückt und dadurch in der Lage, einfache Grafiken darzustellen sowie Klänge zu erzeugen. Die ersten Heimcomputer nutzten 8-Bit-Prozessoren, in der großen Mehrzahl entweder den Z80 oder 6502-Derivate, gegen Mitte bis Ende der 1980er-Jahre wurden diese von 16/32-bit-Typen wie dem Motorola 68000 verdrängt. Die Grafik- und Soundfähigkeiten wurden komplexer und der Anschluss von Festplatten und anderer PC-Peripherie wurde möglich.\n\nBetriebssystem und BASIC als Programmiersprache waren oft im ROM gespeichert und bildeten eine Einheit, mussten also nicht beim Start geladen werden, weshalb die meisten Heimcomputer nach dem Einschalten innerhalb weniger Sekunden einsatzbereit sind. Mit MSX wurde durch Microsoft und Sony der Versuch unternommen, Betriebssystem und BASIC zu standardisieren und einen Programmaustausch zwischen Computern unterschiedlicher Hersteller zu ermöglichen. Der MSX-Standard war unter anderem in Südamerika und Japan erfolgreich, konnte sich in Deutschland aber nicht durchsetzen.\n\nDer Markt für Heimcomputer war Ende der 1980er-Jahre in viele nicht zueinander kompatible Systeme zersplittert. Von Anfang bis etwa Mitte der 1990er-Jahre folgte eine Konsolidierungsphase, nach der keine Heimcomputer im engeren Sinne mehr angeboten wurden – es gewannen die unter Microsofts Windows-Betriebssystemen laufenden, zum IBM-PC kompatiblen Personal Computer und zu kleineren Anteilen die Apple Macintosh sowie in noch kleinerem Maßstab Linux-Rechner, die meistens auf IBM-PC-kompatibler Hardware aufsetzen.\n\nSchon Ende der 80er verschwanden die älteren Systeme vom Markt, die noch auf einer 8-Bit-Architektur beruhten und den neuen 16- oder 32-Bit-Prozessoren mit ihrem größeren Adressraum unterlegen waren. (Die 8-Bit-Systeme hatten meist einen 16-Bit-Adressraum mit maximal 64 KiB Ram, die 16- oder 32-Bit-Systeme hatten meist einen 20-, 24- oder 32-Bit-Adressraum; 20-Bit-Adressen erlauben bis zu 1 Megabyte Ram.)\n\nKlare Sieger waren zunächst die Atari- und Amiga-Systeme. Den beteiligten Firmen unterliefen jedoch einige unternehmerische Fehler – unter anderem durch Vernachlässigung des professionellen und des US-Markts und erfolglose Konzentration auf den europäischen Markt –, die sie letztlich in die Verlustzone führten und eine Weiterentwicklung der Technologie verhinderten.\n\nAnfang der 1990er-Jahre setzte sich zunehmend der IBM-PC im Heimanwenderbereich durch. Einige maßgebliche Gründe hierfür waren:\n\nNachdem Microsoft Anfang der 1990er nicht nur den Rückstand der Windows-Oberfläche gegenüber den Betriebssystemen der Mitbewerber (Apple Macintosh, Apple IIGS, Atari ST- und Commodore Amiga-Familien) aufholen konnte, sondern gleichzeitig offensiv die Entwicklung von Spielen für das eigene Betriebssystem forcierte und Hardware-Hersteller bei der Entwicklung von Grafik- und Sounderweiterungen förderte, die die auf dem IBM-PC basierenden „Personal Computer“ zu attraktiven Unterhaltungsgeräten machten, wurde Windows schnell zum beliebtesten Betriebssystem für Personal Computer, die somit zu „Heimcomputer im weiteren Sinne“ wurden.\n\nDadurch wiederum wurde die Marktmacht von Microsoft stark genug, dass auch die Versuche von IBM und anderen Hardware-Herstellern, durch die Entwicklung eigener Betriebssysteme unabhängiger zu werden, scheiterten; OS/2 und andere Neuentwicklungen erreichten kaum den PC-Markt, der mittlerweile ebenso bedeutend für die Weiterentwicklung des PC-Bereiches geworden war wie die Anwendung als Bürocomputer. Ausnahme (vor allem in den USA) war Apple damals mit Mac OS Classic. Einen nicht mehr zu vernachlässigenden Anteil hat sich aber auch die Linux-Architektur erobert, auch macOS baut auf einem Unix-artigen Kern auf.\n\nDer Heimcomputer war das erste programmierbare Computersystem, das Anfang der 1980er-Jahre in den privaten Haushalten weite Verbreitung fand. Viele Menschen waren von den neuen Möglichkeiten fasziniert und begannen, als Freizeitbeschäftigung selbst Programme zu schreiben. Technik-affin waren damals überwiegend erwachsene Männer und männliche Jugendliche. Ein Phänomen dieser Zeit waren die sogenannten „Computerkids“, Jugendliche zwischen etwa 12 und 18 Jahren, die durch intensive Beschäftigung mit dem Heimcomputer Kenntnisse und Fähigkeiten entwickelten, die die ihrer eigenen Elterngeneration teils weit übertrafen. Unter anderem wegen des zunächst noch geringen Angebots an fertiger Software war der Anteil der Selbstprogrammierer unter den Benutzern viel höher, der Anteil der Nur-Benutzer viel geringer als heute. Auch die damals entstehenden Computerzeitschriften wandten sich zunächst zu einem wesentlichen Teil an Programmierer und enthielten oft seitenlange Programmausdrucke (Listings), die abzutippen oft mehrere Stunden dauerte. Dabei erwarben viele Benutzer weitergehende Programmierkenntnisse. Junge Computer-Enthusiasten aus dieser Zeit bildeten auch die erste Hacker-Generation. Der US-amerikanische Spielfilm WarGames – Kriegsspiele von 1983 stellte diesen Zusammenhang in dramaturgischer Überhöhung dar.\n\nIn den Schulen gelang es vielfach Mathematik- und Physiklehrern, ihre Schüler für den Umgang mit Computern zu begeistern. Informatikunterricht als Wahlfach, oft nachmittags abgehalten, erlebte einen starken Aufschwung. Gleichzeitig richteten viele Schulen, zunächst noch weitgehend in Eigenregie und durch engagierte Lehrer, eigene Computer-Räume ein. Entgegen der späteren Entwicklung ging man damals weithin davon aus, dass elementare Computerkenntnisse gleichbedeutend mit dem Programmieren, also dem Schreiben eigener Programme seien, weshalb Programmiersprachen und das Erlernen des Programmierens den Großteil des Lehrstoffs ausmachten. Dies ist auch insofern erklärbar, als dass es – mit Ausnahme des für durchschnittliche Privatnutzer deutlich zu teuren Apple Macintosh – praktisch noch keine Computer mit grafischer Bedienoberfläche und auch keine weit verbreitete Standardsoftware wie Microsoft Windows oder Office-Pakete gab, deren Bedienung hätte vermittelt werden können.\n\nIm Ostblock fand eine ähnliche Entwicklung von Heimcomputern wie im Westen statt. Vor der Wende bereits begehrt, fanden ab 1990 noch viele westliche Heimcomputer ihren Weg nach Osteuropa, da sie gegenüber PCs wesentlich preisgünstiger waren.\n\nIn der DDR wurden die Heimcomputer in \"Kleincomputer\" umbenannt, als sich abzeichnete, dass die heimische Produktion nur die Nachfrage in Schulen und Betrieben decken konnte. Die Kleincomputer in der DDR basierten alle auf dem U880 als Prozessor, einem Nachbau des Z80.\n\nDie ersten Computer waren der Polycomputer 880 und der LC80 (LC für Lerncomputer). Diese verfügten nicht über einen Bildschirmanschluss und waren vor allem dazu geeignet, die Funktionsweise eines Mikrorechners zu erlernen. Zur Anzeige von Daten waren jeweils Segmentanzeigen integriert.\n\nAus dem VEB Mikroelektronik „Wilhelm Pieck“ Mühlhausen/Thüringen kam der HC 900. Er wurde später als KC 85/2 verkauft. Weiterentwicklungen dieser Serie waren der KC 85/3 und KC 85/4.\n\nParallel dazu wurde in Dresden vom Kombinat Robotron der Z 9001 entwickelt, der später als KC 85/1 sowie nahezu unverändert als KC 87 verkauft wurde.\n\nWeiterhin gab es eine Reihe von Eigenbaucomputern, deren Baupläne teils in Zeitschriften veröffentlicht wurden.\n\n\n1989/1990 wurden noch die Computer KC Compact und BIC A 5105, Letzterer als Bildungscomputer für Schulen und Berufsausbildung gedacht, vorgestellt, erreichten aber keine große Verbreitung mehr.\n\nTeilweise wurden Computer auch aus dem Westen als Geschenke von Verwandten in die DDR geschickt. Die westdeutsche Computerzeitschrift 64’er informierte mehrmals darüber, was dabei zu beachten war, und ermöglichte auch ein von Westdeutschen bezahltes Geschenkabonnement an DDR-Bürger. Datenträger durften allerdings nicht geschickt werden, da man Propagandamaterial darauf vermutete. Während Audiokassetten noch halbwegs erhältlich waren, waren Disketten in der DDR nur sehr schwer zu bekommen und exorbitant teuer. Westliche kommerzielle Software war praktisch überhaupt nicht legal zu bekommen.\n\n\nDurch die weitere wirtschaftliche Öffnung Ungarns im Gegensatz zu den restlichen Ländern des Ostblocks gab es in Ungarn eine größere Anzahl Commodore Plus4, der dort als Schulcomputer genutzt wurde. In größeren Unternehmen waren häufig C64 zu finden, und auch bei wohlhabenden Privatpersonen war der Besitz eines solchen Computers nicht unüblich.\n\n\n\n"}
{"id": "18378", "url": "https://de.wikipedia.org/wiki?curid=18378", "title": "Denial of Service", "text": "Denial of Service\n\nDenial of Service (DoS; engl. für „\"Verweigerung des Dienstes\"“) bezeichnet in der Informationstechnik die Nichtverfügbarkeit eines Internetdienstes, der eigentlich verfügbar sein sollte.\n\nObwohl es verschiedene Gründe für die Nichtverfügbarkeit geben kann, ist die häufigste Art die Folge einer Überlastung des Datennetzes. Dies kann durch unbeabsichtigte Überlastungen verursacht werden oder durch einen konzentrierten Angriff auf die Server oder sonstige Komponenten des Datennetzes.\n\nIm Fall einer durch eine Unmenge von Anfragen verursachten Dienstblockade spricht man von einer durch Vielanfragen \"verbreiteten Verweigerung des Dienstes\" (engl. Distributed Denial of Service; DDoS).\n\nWird eine Überlastung mutwillig herbeigeführt, geschieht dies in der Regel mit der Absicht, einen oder mehrere bereitgestellte Dienste funktionsunfähig zu machen. War dies ursprünglich vor allem eine Form von Protest oder Vandalismus, werden Denial-of-Service-Attacken mittlerweile von Cyber-Kriminellen zum Kauf angeboten, um Konkurrenten zu schädigen. Ebenso werden Serverbetreiber zu einer Geldzahlung erpresst, damit ihr Internetangebot wieder erreichbar wird.\n\nDoS-Angriffe wie SYN-Flooding oder der Smurf-Angriff belasten den Internetzugang, das Betriebssystem oder die Dienste eines Hosts, beispielsweise HTTP, mit einer größeren Anzahl Anfragen, als diese verarbeiten können, woraufhin reguläre Anfragen nicht oder nur sehr langsam beantwortet werden. Wenn möglich, ist es jedoch wesentlich effizienter, Programmfehler auszunutzen, um eine Fehlerfunktion (wie einen Absturz) der Serversoftware auszulösen, worauf diese auf Anfragen ebenfalls nicht mehr reagiert. Beispiele sind WinNuke, die Land-Attacke, die Teardrop-Attacke oder der Ping of Death.\n\nIm Unterschied zu anderen Angriffen will der Angreifer beim DoS-Angriff normalerweise nicht in den Computer eindringen und benötigt deshalb keine Passwörter oder Ähnliches vom Zielrechner. Jedoch kann der Angriff Bestandteil eines anderen Angriffs auf ein System sein, zum Beispiel bei folgenden Szenarien:\n\n\nEine besondere Form stellt der \"Distributed-Reflected-Denial-of-Service-Angriff\" (DRDoS-Angriff) dar. Hierbei adressiert der Angreifer seine Datenpakete nicht direkt an das Opfer, sondern an regulär arbeitende Internetdienste, trägt jedoch als Absenderadresse die des Opfers ein (IP-Spoofing). Die Antworten auf diese Anfragen stellen dann für das Opfer den eigentlichen DoS-Angriff dar. Durch diese Vorgehensweise ist der Ursprung des Angriffs für den Angegriffenen nicht mehr direkt ermittelbar. Ein Beispiel für einen solchen Angriff ist die DNS Amplification Attack, bei der das Domain Name System als Reflektor missbraucht wird.\n\nWeitere bekannte Methoden sind der Smurf- und der Fraggle-Angriff, bei denen ein Paket mit der IP-Adresse des Opfers als Absender an die Broadcast-Adresse eines Netzwerks gesendet wird. Das bewirkt, dass das Paket um die Anzahl der Geräte im Netzwerk vervielfacht und an das Opfer zurückgeschickt wird.\n\nMutwillige DDoS-Angriffe werden oft (aber nicht ausschließlich, siehe DDoS als Protestaktion) mit Hilfe von Backdoor-Programmen oder Ähnlichem durchgeführt. Diese Backdoor-Programme werden in der Regel von Trojanern auf nicht ausreichend geschützten Rechnern installiert und versuchen selbstständig, weitere Rechner im Netzwerk zu infizieren, um so ein Botnetz aufzubauen. Je größer das Botnetz, desto wahrscheinlicher ist, dass der Angriff selbst gegen gut geschützte Systeme durchdringt. Die Steuerung des Angriffs erfolgt über IRC, HTTP oder mittels eines Peer-to-Peer-Netzes.\n\nMit zunehmender Bedeutung des Internets der Dinge werden für DDoS-Angriffe auch Geräte missbraucht, die auf den ersten Blick harmlos wirken: Internet-fähige Fernsehrekorder, Set-Top-Boxen, Fernseher, Überwachungskameras oder Uhren. Die Geräte werden oft mit Standard-Passwörtern ausgeliefert und ihre Firmware selten aktualisiert, was sie zu attraktiven Zielen für automatisierte Angriffe aus dem Internet macht. Einmal infiziert können sie ähnlich wie Rechner eines Botnetzes orchestriert werden.\n\nAls Form des Protestes sind DDoS-Attacken immer populärer geworden. Einfach zu bedienende Werkzeuge wie zum Beispiel die populäre Low Orbit Ion Cannon ermöglichen es nun auch nicht computerversierten Personen, den Betrieb fremder Computer, Webseiten und Dienste mit Denial-of-Service-Angriffen zu stören.\n\nBefürworter dieser Form des Protestes argumentieren, dass bei Online-Demonstrationen die Protestierenden nur ihre eigenen Ressourcen verwenden und deren Aktionen somit weder das Tatbestandsmerkmal der Gewalt, noch eine Drohung mit einem empfindlichen Übel aufweisen. Daher sei diese politische von der wirtschaftlich motivierten Form des DDoS zu unterscheiden.\n\nIn Deutschland ist bereits der Versuch der Störung als Computersabotage strafbar, siehe dazu Abschnitt Rechtliche Situation.\n\nAuch Staaten nutzten DDoS Attacken um unliebsame Websiten, zumindest vorübergehend, lahm zu legen. Die Volksrepublik China hat dazu die sogenannte Great Cannon of China erstellt und greift Websiten an welche Tools anbieten um die Great Firewall zu umgehen.\n\nIm Folgenden werden acht bekannte Beispiele zu absichtlich herbeigeführten Serverüberlastungen aufgeführt.\n\n\nDas Content Delivery Network Akamai stellte eine Steigerung der Angriffe vom vierten Quartal 2013 zum ersten Quartal 2014 um 39 % fest, zum Vorjahresquartal sind es 47 %. Der Sicherheitsspezialist Imperva berichtet, dass ein Drittel aller Netzwerk-DDoS-Ereignisse ein Volumen von mehr als 10 Gbit/s haben. Zweck solcher Angriffe sind meist Erpressung, Schädigung eines Konkurrenten oder Infiltration des Zielsystems. Es gibt über Stunden gehende Angriffe mit 180 Gbit/s, die selbst Providernetze überfordern. Manche Angreifer geben sich als Suchmaschinen-Bots aus. Mehr als ein Viertel der angreifenden Bot-Netze befinden sich in China, Indien und dem Irak.\n\nFührt der sprunghafte Anstieg von Anfragen an eine bisher nur gering frequentierte Webseite aufgrund der Berichterstattung in einem publikumswirksamen Medium zu deren Überlastung und damit zur Dienstverweigerung, wird das bei dortigen Lesern im Netzjargon auch „Slashdot-Effekt“ genannt und gelegentlich scherzhaft mit einem DDoS-Angriff verglichen. Ein weiteres bekanntes Beispiel dafür im deutschsprachigen Raum ist die IT-Nachrichtenseite heise online und der dort gelegentlich auftretende „Heise-Effekt“. Außerdem kann es bei Tweets populärer Nutzer des Netzwerks Twitter und Retweets ihrer Follower zu serverseitigen Ausfällen kommen.\n\nUm Überlastungen von kritischer IT-Infrastruktur zu verhindern oder solche zu begrenzen, wurden mit der Zeit einige Gegenmaßnahmen entwickelt:\n\n\n\nAngriffe mit breiten Auswirkungen haben sich zwischen 2015 und 2016 nahezu verdoppelt. Vor allem unsichere IoT-Geräte stellen eine zunehmende Gefahr dar. Ein Mirai-Ableger sorgte 2016 für eine Großstörung im Netz der Telekom. Im selben Jahr gab es breit angelegte Attacken auf die Webseiten der Kandidaten im US-Präsidentschaftswahlkampf sowie Angriff auf den DNS-Dienstleister Dyn, durch den ein Wochenende lang unter anderem Twitter, Netflix und Spotify nicht erreichbar waren.\nDie zunehmende Vernetzung von immer mehr Geräten stellt neue Herausforderungen an die IT-Sicherheit. Das Prinzip „Security by Design“, wonach IT-Sicherheit bei der Soft- und Hardwareentwicklung vom Anfang an mitgedacht wird, kann hier Abhilfe schaffen. Auch die Installation von Sicherheitsupdaten, um Sicherheitslücken rechtzeitig zu schließen, ist eine wichtige Komponente.\n\n\n\n"}
{"id": "18381", "url": "https://de.wikipedia.org/wiki?curid=18381", "title": "Exploit", "text": "Exploit\n\nEin Exploit ( ‚ausnutzen‘) ist in der elektronischen Datenverarbeitung eine systematische Möglichkeit, Schwachstellen auszunutzen, die bei der Entwicklung eines Programms entstanden sind. Dabei werden mit Hilfe von Programmcodes Sicherheitslücken und Fehlfunktionen von Programmen (oder ganzen Systemen) ausgenutzt, meist um sich Zugang zu Ressourcen zu verschaffen oder in Computersysteme einzudringen, bzw. diese zu beeinträchtigen. \n\nEin \"Exploit\" wird oft auch nur zum Aufzeigen einer Sicherheitslücke entwickelt und dokumentiert. Damit soll erreicht werden, dass Softwarehersteller eine Sicherheitslücke schneller erkennen und schließen können. Oft bezeichnet man die reine Beschreibung eines Exploits bereits als Exploit.\n\nExploits machen sich zum Beispiel die Tatsache zunutze, dass Computer mit Von-Neumann-Architektur, das sind nahezu alle Heim- und Bürorechner, nicht zwischen Programmcode und Nutzdaten unterscheiden. So wird zum Beispiel bei einem Pufferüberlauf der Code des Angreifers in einen nicht dafür vorgesehenen Speicherbereich geschrieben, wodurch die Ausführung der Anwendung manipuliert werden kann. Eine andere Möglichkeit sind Formatstring-Angriffe, bei denen ungefiltert Benutzereingaben an Formatierungsfunktionen wie codice_1 übergeben werden. Ein Angreifer kann oft einen eigenen Code zur Ausführung bringen, der ihm beispielsweise eine Shell mit den Privilegien der ausgenutzten Anwendung liefert.\n\nMan bezeichnet Exploits zumeist wie folgt:\n\nLokale Exploits können beim Öffnen an sich scheinbar völlig harmloser Dateien (zum Beispiel Bilddateien) aktiviert werden, sofern die dem Dateityp zugeordnete Anwendung durch fehlerhafte bzw. unsaubere Verarbeitung der Datei eine Sicherheitslücke aufweist. Meistens versucht ein Exploit (beispielsweise in einem PDF-Dokument oder als Makro in einer Word- oder Excel-Datei) zunächst, Sicherheitslücken in dem Programm auszunutzen, mit dem die Datei eingelesen wurde, um dadurch eine höhere Privilegienstufe zu erreichen und so schädlichen Code in das Betriebssystem zu laden und auszuführen. Die eigentliche Aktion, die der Exploit ausführt, bezeichnet man als \"Payload\" (deutsch: Nutzlast). Bei vielen Exploit-Frameworks (etwa Metasploit) kann die Payload separat konfiguriert werden. Sie kann allerdings auch fest im Exploit verankert sein.\n\nEine aktive Form des Exploits sind Angriffe aus dem Internet mittels manipulierter Datenpakete oder spezieller Datenströme auf Schwachstellen in Netzwerksoftware. Solche Exploits werden mitunter auch als Remote-Exploits bezeichnet.\n\nMeist sind die ersten für eine bekanntgewordene Sicherheitslücke veröffentlichten Exploits sogenannte DoS-Exploits, die zwar die betroffene Anwendung überlasten, allerdings keine Ausführung von fremdem Programmcode und keine Privilegien-Eskalation beinhalten.\n\nCommand-Execution-Exploits kennzeichnen das Merkmal einer vom Angreifer steuerbaren Ausführung von Programmcode auf dem Zielsystem. Um ein solches Exploit erfolgreich zur Ausführung bringen zu können, muss der Programmierer über diverse Eigenheiten der Aufteilung des Speichers der Zielanwendung Bescheid wissen. Dieses Wissen bezieht er durch offene Quellen des Programmcodes oder durch bloßes Testen. Er muss seinen Code geschickt platzieren, um ihn zur Ausführung bringen zu können. Command-Execution-Exploits sind zumeist sehr gefährlich, da die betroffenen Anwendungen meist über erhebliche Rechte auf dem System verfügen und der Code des Angreifers mit ebendiesen Rechten gestartet wird.\n\nSQL-Injection-Exploits sind eine spezielle Art von Exploits und finden hauptsächlich Einsatz bei Webanwendungen, die eine SQL-Datenbank nutzen, da sie über das Internet sehr leicht zugänglich sind, sie können jedoch prinzipiell für jede Anwendung, die auf eine SQL-Datenbank zugreift, eine Gefahr darstellen. Hierbei werden Anfragen in einer Schichtenarchitektur so gestellt, dass die fehlerhaft bzw. unsauber arbeitende Präsentationsschicht Daten zurückliefert oder schreibt, die sie weder für den Lesezugriff oder den Schreibzugriff verfügbar machen sollte. Beispielsweise können Eingaben in einem Loginformular so gestaltet werden, dass die betroffene Anwendung einen ungültigen Benutzer dennoch erfolgreich einloggt oder es können gezielt Datenfelder aus der Datenbank ausgegeben werden um z. B. die Passwörter oder E-Mail Adressen aller registrierten Benutzer auszugeben. Werden Benutzereingaben in Programmoberflächen nicht ausreichend auf Gültigkeit geprüft (z. B. dass sie keine SQL-Kommandos und auch keine Teile davon enthalten) und gefiltert, kann eine SQL-Injection-Lücke entstehen.\n\nZero-Day-Exploit nennt man einen Exploit, der eingesetzt wird, bevor es einen Patch als Gegenmaßnahme gibt. Entwickler haben dadurch keine Zeit („null Tage“ ), die Software so zu verbessern, dass der Exploit unwirksam wird, um so deren Nutzer zu schützen. Entdeckt eine Person eine Sicherheitslücke und meldet sie nicht dem Software-Hersteller, sondern entwickelt stattdessen einen Exploit, um diese auszunutzen, so wird die Schwachstelle der Software oft erst lange nach dem ersten Angriff bekannt. Von Hackern werden Zero-Day-Exploits oft lange geheim gehalten, damit sie weiterhin ausgenutzt werden können. Unter der Hand werden Zero-Day-Exploits auch, je nach Komplexität, Umfang und Marktwert des betroffenen Systems, unter Hackern gehandelt oder den Herstellerfirmen zu hohen Summen angeboten. Die Preise sind seit 2012 etwa um den Faktor 10 angestiegen. Insbesondere seit staatliche Organe sich auch auf offensive Szenarien in einem Cyberwar vorbereiten, tritt auf dem Markt für Zero-Day-Exploits die Situation auf, dass legale staatliche und privatwirtschaftliche Akteure ein Interesse haben, Exploits zu kennen, um einerseits durch die Veröffentlichung von Patches Systeme abzusichern. Gleichzeitig haben dieselben oder im gleichen Auftrag handelnden Akteure das Bestreben, Exploits geheim zu halten, um sie für Angriffe auf feindliche Systeme nutzen zu können.\n\nEin Angriff über einen Zero-Day-Exploit wird als Zero-Day-Attacke bezeichnet. Zero-Day-Attacken sind gefährlich, weil sie eine neue Sicherheitslücke ausnutzen, bevor für die betroffenen Systeme Patches bereitstehen.\n\nVorbeugend versuchen Experten mit verschiedenen Testmethoden, Sicherheitslücken im Voraus aufzuspüren und dem Software-Hersteller aufzuzeigen. Dies wird aber in Fachkreisen meist heftig kritisiert, da sich die Tester oftmals nicht im legalen Rahmen bewegen oder gegen Richtlinien von Herstellern verstoßen.\n\n\nOft wird der Speicherschutz als Gegenmaßnahme genannt, das ist jedoch nicht korrekt, denn eingefrorene Speicher können mit verschiedenen Programmen ausgelesen werden. Ebenso kann mittels Intrusion-Detection-Systemen ein Angriff auf Basis bestehender Funktionalitäten festgestellt oder mittels Intrusion-Prevention-Systemen auch verhindert werden; ein solches System schützt indessen ebenso wenig gegen das Ausnutzen eines systematischen, unbekannten Fehlers in einer Software. Das Grundproblem ist oft unsaubere Programmierung (z. B. durch die Verwendung von hängenden Zeigern) oder, noch schwerer zu entdecken, ein systematischer, meist sehr komplexer Fehler in der Architektur des Programms oder eines ganzen Systems.\nDie einzige Lösung für solche Probleme wäre, die durch Verarbeitungsfehler entstehenden Sicherheitslücken schon bei der Entwicklung zu vermeiden, was bei heutigen Systemen aber praktisch unmöglich ist. Managed Code bietet einen gewissen Schutz; so werden zum Beispiel Pufferüberläufe effektiv verhindert. Dies ist aber nur eine Teillösung der Gesamtproblematik. Komplexe Systeme, die von unterschiedlichen Herstellern und Sublieferanten zusammengefügt werden, bestehen aus vielen Schichten von Hard- und Software, was die Schwachstellensuche während der Entwicklung enorm erschwert. Deshalb wird dann meist noch im Betrieb, weit nach der Beta-Phase, die Suche nach Schwachstellen fortgeführt. Existenziell wichtig ist diese Suche bei extrem kritischen Systemen, bei denen Menschenleben auf dem Spiel stehen, z. B. bei Autos, Zügen, Flugzeugen und Schiffen, die alle Software enthalten (meist in Form von Firmware), welche prinzipiell angegriffen werden kann.\n\n\n\n"}
{"id": "18409", "url": "https://de.wikipedia.org/wiki?curid=18409", "title": "Gentoo Linux", "text": "Gentoo Linux\n\nGentoo Linux (englische Aussprache []) ist eine quellbasierte Linux-Distribution für fortgeschrittene Linux-Benutzer, die ihr System komplett individuell einrichten möchten. Voraussetzung dafür ist die Bereitschaft, sich mit den Abläufen eines Linux-Systems und der ausführlichen Dokumentation auseinanderzusetzen. \"Gentoo\" ist ein Warenzeichen der \"Gentoo Foundation, Inc.\", einer Non-Profit-Organisation. Anfang Dezember 2010 ist die \"Gentoo Foundation, Inc.\" dem Open Invention Network beigetreten, welches sich für die freie Verfügbarkeit von Softwarepatenten einsetzt. Im europäischen Raum ist der deutsche \"Förderverein Gentoo e. V.\" der Inhaber der Markenrechte. Der Name Gentoo wurde nach einer besonders schnellen Pinguinart, dem Eselspinguin (englisch \"gentoo penguin\"), gewählt, wobei der Name Bezug nimmt auf das offizielle Maskottchen Tux (das einen Pinguin darstellt) des freien Betriebssystemkerns Linux.\n\nGründer und langjähriger Chef des Gentoo-Projekts war der US-amerikanische Programmierer Daniel Robbins. 1999 begann er mit der Entwicklung einer eigenen Linux-Distribution, die er zunächst \"Enoch\" nannte. Der Namenswechsel fand am 4. Oktober 1999 mit der Registrierung der Domain \"gentoo.org\" statt. Dieses Datum wird heute offiziell als „Geburtstag“ Gentoos aufgefasst. Im Jahr 2004 verließ Robbins das Projekt. Seit seinem Weggang wird Gentoo von dem Kuratorium (\"Board of Trustees\") der Gentoo Foundation geleitet. Entscheidungen über technische Aspekte und Richtlinien trifft ein siebenköpfiger \"Council\". Trustees und Council werden von den Mitgliedern der Foundation bzw. den aktiven Entwicklern gewählt.\n\nGentoo unterscheidet sich in mehreren Punkten entscheidend von vielen anderen Linux-Distributionen. So ist Gentoo eine quellbasierte Distribution, bei der in der Regel alle Pakete vor der Installation übersetzt werden. Der dafür nötige Zeit- und Rechenaufwand, aber auch der so mögliche tiefe Eingriff in Konfigurations- und Optimierungsmöglichkeiten ist bei auf binären Paketen basierten Distributionen nicht gegeben. Gleichwohl lassen sich auch wie bei letzteren vorkompilierte Programme nutzen.\nEbenso gibt es nur wenig automatisierte Abläufe, was eine hohe Kontrolle des Systems ermöglicht, die aber auch entsprechende Kenntnisse voraussetzt.\n\nDie Tatsache, dass die Arbeitsweise des Gentoo-Projekts nicht versionsorientiert ist, führt zu einem kontinuierlichen Aktualisieren des Systems, im Gegensatz zu einer stufenartigen Aktualisierung, wie dies bei den meisten klassischen Distributionen der Fall ist. Auf diesem Weg ergeben sich Migrationsprobleme immer nur für einzelne Programmpakete, nicht aber für eine ganze Distributionsversion. Auch kann der Nutzer so über jede verwendete Version eines Programms selbst entscheiden.\n\nHinzu kommt, dass mit vergleichsweise einfachen Mitteln eigene Distributionen auf der Basis von Gentoo erstellt und distributiert werden können, um zum Beispiel für Spezialanwendungen wie Cluster oder Rechnerpools zu genügen. Gentoo kann als Distributionsbaukasten eingesetzt werden. Zum Beispiel basiert Google Chrome OS auf Gentoo. Gentoo wurde benutzt, um Linux auf Macintosh-Rechner mit einer Intel-CPU zu portieren.\n\nBei Gentoo Linux gibt es keine Versionen im eigentlichen Sinn, sondern Veröffentlichungen (\"engl.\" ) eines Entwicklungsstandes (\"engl.\" ), sogenannte Rolling Releases, auf dessen Basis unter anderem die codice_1-Archive und Live-Systeme erstellt werden.\n\nBei einem installierten Gentoo-System gehen die einzelnen Versionen bei regelmäßigem Aktualisieren des Portage-Trees ohne größere Umstellungen ineinander über. Die Version des Basissystems (\"engl.\" ) entspricht der des Pakets codice_2 und kann auch der Datei codice_3 entnommen werden. Es ist die Grundlage des Betriebssystems und als die eigentliche Version einer Gentoo-Installation anzusehen. Das Basissystem unterliegt jedoch anderen Freigabezyklen als die Gesamt-Distribution und deren Veröffentlichung als codice_4 beziehungsweise als Live-System.\n\nPortage ist die Paketverwaltung von Gentoo Linux und ermöglicht den automatischen Bau der einzelnen Pakete aus ihren Quelltexten. Dabei stützt es sich auf den sogenannten \"Portage tree\", einen Verzeichnisbaum, der sich normalerweise unter codice_5 befindet und Informationen zu jedem einzelnen Paket in Form von sogenannten \"ebuild\"-Skripten bereitstellt. Diese Skripte steuern den kompletten Ablauf – Download der Quelltexte, Verifikation der Unverfälschtheit der Dateien mit Hilfe von Prüfsummen, Anwendung von distributionsspezifischen Patches sowie die Berücksichtigung der sogenannten \"USE-Flags\", um letztendlich das Paket in einer Sandbox zu kompilieren und dann zu installieren. Dabei werden etwaige Abhängigkeiten zu anderen Paketen beachtet und diese, falls nötig, ebenfalls aktualisiert oder neu installiert. Der Portage-Baum wird mit Hilfe von rsync auf den aktuellen Stand der Distribution gebracht.\n\nPortage wählt die jeweils aktuelle stabile oder, je nach Konfiguration, die aktuelle instabile Version für die jeweilige Prozessorarchitektur aus. Je nach Paket gibt es noch weitere maskierte Versionen, von deren Installation aber außer zu Entwicklungs- und Testzwecken abgesehen werden sollte. Darunter fallen z. B. die sogenannten \"Live\"-Versionen von Paketen, die direkt den aktuellen Entwicklungsstand aus dem Versionsverwaltungssystem der jeweiligen Software beziehen. Mittels Konfigurationsdateien ist es möglich, einzelne Pakete oder einzelne Versionen von Paketen zu maskieren, um die Installation eines Pakets bzw. einer Version zu verbieten, oder sie zu demaskieren, um eine aktuellere Version als vorgesehen zu installieren.\n\nDie USE-Flags bilden eine Abstraktionsschicht für die Konfiguration der Funktionalität der einzelnen Pakete für Optionen, die sich nur während des Kompiliervorgangs aktivieren lassen. So bestimmt beispielsweise das USE-Flag \"bluetooth\" den Einbau der Bluetoothunterstützung für den Fall, dass das jeweilige Paket diese Unterstützung mitbringt. Eine Funktionalität lässt sich auch mittels USE-Flag komplett abschalten, im Beispiel durch \"-bluetooth\". Der Vorteil einer solchen Möglichkeit liegt darin, dass die kompilierten Programme genau auf die Bedürfnisse des Anwenders angepasst sind, wodurch diese weniger Speicher benötigen und die Installation von nur wirklich notwendigen Bibliotheken voraussetzt. Die Implementierung des An- und Abschaltens von Funktionen kann dabei vom „ebuild“-Skript individuell umgesetzt werden. In der Regel geschieht dies mit Hilfe von Configure-Optionen oder Patches. Die USE-Flags lassen sich mit Hilfe von Konfigurationsdateien sowohl zentral für das gesamte System als auch speziell für einzelne Pakete konfigurieren.\n\nMöchte man Pakete installieren, welche sich nicht im offiziellen Portage-Tree befinden, so gibt es die Möglichkeit, sogenannte \"Overlays\" zu nutzen. Diese werden von Gentoo offiziell nicht unterstützt, bieten aber oft eine größere Auswahl an Software oder aktuellere Versionen. Viele der Overlays beinhalten Pakete, die dort vom Entwickler getestet werden, bevor sie in den offiziellen Baum aufgenommen werden.\n\nGentoo besitzt im Gegensatz zu anderen Linux-Distributionen keinen eigenen Installer. Stattdessen führt der Benutzer die Installation selbst mit einer Serie von Shell-Befehlen aus einem anderen laufenden System heraus durch. Zu diesem Zweck bietet das Gentoo-Projekt spezielle Live-Images an, prinzipiell können jedoch beliebige Linux-Systeme dafür verwendet werden, egal, ob fest installiert oder von Live-Medien gebootet.\n\nZur Installation wird ein vom Gentoo-Projekt bereitgestellter sogenannter \"stage3\"-Tarball an den Zielort entpackt. Dieser enthält ein Grundsystem, einschließlich der für den weiteren Installationsprozess benötigten Werkzeuge, wie z. B. einer Toolchain. Die weiteren Installationsarbeiten finden mittels chroot innerhalb dieses Verzeichnisbaums statt. Auf der Gentoo-Website existieren Installationshandbücher, die Hinweise über die notwendigen Installationsschritte geben. Dem Benutzer werden dabei der Projektphilosophie entsprechend alle Freiheiten gelassen; so kann er beispielsweise selbst entscheiden, welche syslog- und cron-Implementierung und welchen Mail Transfer Agent er nutzen möchte.\n\nFrüher standen neben den \"stage3\"-Archiven auch \"stage1\"- und \"stage2\"-Archive für die Installation zur Verfügung. Diese sind Zwischenprodukte des Prozesses, mit dem \"stage3\"s erzeugt werden. Ihr Anwendungszweck bestand im Erstellen von besonders stark optimierten Systemen. Da inzwischen keine offiziellen \"stage1\"- und \"stage2\"-Archive mehr zum Download angeboten werden, verwendet man nun auch für diesen Zweck die \"stage3\"-Archive.\n\nGentoo ist unter diversen Architekturen lauffähig. Dazu zählen Alpha, AMD64, ARM, Itanium, M68k, MIPS, PA-RISC, PowerPC, S/390, SH, UltraSparc und x86. Gentoo ist ebenfalls auf der Xbox, der Wii und auf der PlayStation 3 lauffähig.\n\nEs gibt auch Projekte, bei denen der Linux-Kernel und einige GNU-Bibliotheken/Programme durch einen FreeBSD- (Gentoo/FreeBSD), NetBSD bzw. OpenBSD-Kernel und deren Basis-Bibliotheken/Programme ersetzt wurde. Zudem kann man Gentoo unter verschiedenen Unix-ähnlichen Betriebssystemen in ein Unterverzeichnis installieren. Diese Installationsvariante wird Gentoo Prefix genannt. Unterstützt werden unter anderem Mac OS X/​OS X/​macOS, Solaris und Windows mit Hilfe der Microsoft Windows Services for UNIX.\n\n\n\n\n"}
{"id": "18607", "url": "https://de.wikipedia.org/wiki?curid=18607", "title": "Bildbearbeitung", "text": "Bildbearbeitung\n\nDie Bildbearbeitung ist die Veränderung von Fotos, Negativen, Dias oder digitalen Bildern. Sie ist zu unterscheiden von der Bildverarbeitung, mit deren Hilfe die Inhalte von Bildern verarbeitet werden. Oft wird die Bildbearbeitung dazu angewandt, um Fehler zu beseitigen, die beim Fotografieren oder anderen Bilderfassungen entstehen können. Dazu gehören beispielsweise Über- und Unterbelichtung, Unschärfe, Kontrastschwäche, Bildrauschen, Rote-Augen-Effekt und stürzende Linien. Durch diese Fehler wirken Bilder oft zu dunkel, zu hell, zu unscharf oder anderweitig mangelhaft. Die Ursachen können technische Probleme oder mindere Qualität der Aufnahmegeräte (Digitalkamera, Objektiv, Scanner), deren Fehlbedienung sowie ungünstige Arbeitsbedingungen oder mangelhafte Vorlagen sein. Die beiden Bilder auf der rechten Seite zeigen einige Möglichkeiten der Bildbearbeitung: Das obere Bild wirkt überbelichtet, farbstichig, der Text unscharf, das Objekt zeigt oben einen Lichtreflex und liegt außerhalb der Mitte. Das untere, korrigierte, nun farbreine Bild dagegen sieht viel klarer und deutlicher aus. Denn der Bildgegenstand wird betont, steht er doch größer, entzerrt und mittig im Bildausschnitt, das Format wurde in der Höhe geringfügig angepasst.\n\nDie Bildbearbeitung umfasst verschiedene Techniken wie Retusche, Unscharfmaskierung, Abwedeln und andere Optimierungsmethoden.\n\nBei der \"digitalen\" Bildbearbeitung handelt es sich um die computergestützte Bearbeitung von digitalen Bildern, üblicherweise von Rastergrafiken, zumeist Fotos oder gescannten Dokumenten. Die nötige Hard- und Software sind relativ preisgünstig, daher ist diese Art der Bildbearbeitung weit verbreitet. Die Möglichkeiten digitaler Bildbearbeitung sind extrem vielfältig und oft nur durch die mangelnden Kenntnisse des Bearbeiters beschränkt.\n\nDie \"traditionelle\" Bildbearbeitung beinhaltet Foto-, Negativ- und Diabearbeitung. Für diese Art der Bildbearbeitung sind spezielle Geräte, Materialien, Chemikalien und Kenntnisse notwendig. Diese Art der Bildbearbeitung ist eine Domäne für Spezialisten. Die Möglichkeiten sind sehr vielfältig; durch den oft sehr großen Aufwand jedoch ökonomisch heute nicht mehr sinnvoll.\n\nDie \"hybride\" Bildbearbeitung beinhaltet Methoden der traditionellen und der digitalen Bildbearbeitung und wurde in den 1990er Jahren entwickelt. Während der Film im Vergrößerungsgerät ausbelichtet wird, befindet sich eine spezielle lichtdurchlässige Scheibe zwischen Film und Foto. Diese Scheibe kann teilweise durch elektrischen Strom lichtundurchlässig gemacht werden. In Sekundenbruchteilen kann dadurch nachbelichtet, abgewedelt oder unscharf maskiert werden. Heutzutage findet diese Technik eine massenhafte Anwendung bei der Herstellung von Papierabzügen vom Negativfilm.\n\nDie heutige digitale Form der Bildbearbeitung ist ein inhomogenes Konglomerat aus verschiedenen Vorläufern. Exemplarisch können als Vorläufer folgende Hauptbereiche genannt werden:\n\n\nAlle Bereiche haben ihre eigenen Spuren hinterlassen, beispielsweise:\n\n\nDie heutige Form der digitalen Bildbearbeitung ist noch sehr jung. In ihr finden sich, bedingt durch die Spuren der vielen Vorläufer, zahlreiche Begriffe, die dasselbe bedeuten. Als Beispiel sei das (teilweise) Einfärben von Schwarz-Weiß-Bildern genannt. In Wikipedia finden sich hierzu folgende Begriffe:\n\n\nGleiche Begriffe, die verschiedene Dinge meinen:\n\n\nBegriffe, die nicht eindeutig sind:\n\n\nZahlreiche Nachschlagewerke, sogar Lehrbücher für die aktuelle Fotografenausbildung, enthalten diese Mehrdeutigkeiten. Auch in Wikipedia sind zahlreiche Begriffe zu finden, die nicht einheitlich definiert sind (einige Beispiele sind in diesem Kapitel verlinkt).\n\nAuf die Software, die im Bereich der Bildbearbeitung eingesetzt wird, wird im Artikel Grafiksoftware eingegangen.\n\nDie Software, mit der die Bildbearbeitung erfolgt, nennt man \"Bildbearbeitungsprogramm\". Sie bietet zahlreiche Bearbeitungsfunktionen, die sich zumeist in einem Programmmenü, einer Symbolleiste oder einer Werkzeugleiste befinden. Beispiele hierfür sind:\n\n\n\nDie digitale Bildbearbeitung hat sich aufgrund der Möglichkeiten der Digitalfotografie, die schnelles Fotografieren und Übertragen der Bilder auf den PC ermöglicht, weit verbreitet. Viele Bildbearbeitungsprogramme („Grafiksoftware“) sind auf den Heimanwenderbereich zugeschnitten, indem sie unter anderem vereinfachte Korrekturmöglichkeiten bieten. Im professionellen Bereich wird die Bildbearbeitung unter anderem von Fotografen, Designern und Mediengestaltern im Desktop-Publishing-Bereich und in der Druckvorstufe eingesetzt. Die in der Bildbearbeitung veränderten Bilder werden in Publikationen – Print- und Digitalmedien – aller Art veröffentlicht, zum Beispiel in Zeitschriften, Katalogen und Büchern.\n\nBildbearbeitung lässt sich auch gezielt zur Fotomanipulation einsetzen. Typische Beispiele hierfür sind die Bearbeitung von Gesichtern, um Falten und andere „Schönheitsfehler“ verschwinden zu lassen, und das Entfernen unerwünschter Personen aus einem Bild. Die Grenzen zwischen legitimer Bildbearbeitung und illegitimer Fotomanipulation sind fließend; eine Beurteilung muss im Einzelfall vorgenommen werden.\n\nDie Bildbearbeitung ist außerdem eine Möglichkeit der künstlerischen Entfaltung – u. a. in den Bereichen Digitale Kunst und Medienkunst.\n\n\n"}
{"id": "18640", "url": "https://de.wikipedia.org/wiki?curid=18640", "title": "Binärbild", "text": "Binärbild\n\nEin Binärbild ist eine digitale Rastergrafik, deren Pixel nur die zwei Farben Schwarz und Weiß annehmen können. Jedes Pixel kann also mit einem Bit gespeichert werden. Die Kodierung der Pixel erfolgt meist mit dem Wert 0 für Schwarz und 1 für Weiß, der umgekehrte Fall existiert allerdings auch.\n\nBinärbilder finden ihre Anwendung vor allem\n\nFür die Umwandlung eines Grauwertbildes in ein Binärbild (Farbreduktion) benutzt man das Halbtonverfahren.\n\nZur Datenreduktion von Binärbildern existiert eine Reihe spezieller Kompressionstypen. Dazu gehören die zwei Verfahren \"CCITT Group 3\" und \"CCITT Group 4\", die bei TIFF-Dateien verwendet werden können. Diese sind auf bei Faxgeräten oft vorkommende Dokumente optimiert, also eine geringe Schwärzung der Seite und relativ wenige Änderungen zwischen zwei aufeinanderfolgenden Zeilen. Tatsächlich sind diese beiden Verfahren dieselben, die auch bei Faxgeräten selbst eingesetzt werden.\n\nAußerdem existieren die Standards JBIG und JBIG2, die Binärbilder allerdings nur als einen Spezialfall von Graustufenbildern behandeln.\n\nStandard-Kompressionsverfahren wie LZW oder Deflate können ebenfalls auf Binärbilder angewendet werden. Meist werden hierzu erst jeweils acht nebeneinanderliegende Binärpixel in einem Byte untergebracht, und diese \"gepackten Bytes\" werden dann an die Kompressionsroutine weitergegeben. In einem Dateiformat ist es dabei wichtig zu berücksichtigen, ob der am weitesten links gelegene Pixel dem nieder- oder dem höchstwertigen Bit innerhalb des Bytes entspricht, da die Reihenfolge der Ausgabe der Pixel auf dem Schirm nach dem Entpacken dem Original entsprechen soll. Die meisten Dateiformate legen dies innerhalb der Spezifikation fest, bei TIFF kann es im Headerteil der Datei frei definiert werden.\n\nAuch bei Grafikkarten mit – heutzutage kaum mehr verwendeten – Schwarz-Weiß-Grafikmodi zur Anzeige von Binärbildern ist der Ansatz der gepackten Bytes üblich, um keinen Grafikspeicher zu verschwenden. Der lesende oder schreibende Zugriff auf einzelne Pixel ist etwas erschwert, da man immer nur mehrere Pixelwerte auf einmal einer Speicherstelle entnehmen oder auf diese schreiben kann.\n\nBinarisierung bezeichnet die Erzeugung eines Binärbildes aus einem Graustufen- oder einem in einem Zwischenschritt zum Graustufenbild umgewandelten Farbbild. Eine einfache Binarisierungsmethode ist das Schwellenwertverfahren. Je nachdem, ob der Grauwert eines Pixels über oder unter einem bestimmten Schwellenwert liegt, wird er Schwarz oder Weiß. Maßgeblich ist dabei die Wahl des Schwellenwertes; dieser muss nicht notwendigerweise bei 50 % liegen. Das Schwellenwertverfahren kann mit Dithering-Techniken kombiniert werden, um den Eindruck von gleichmäßigeren Übergängen zu erreichen.\nUm automatisiert einen sinnvollen Schwellenwert für ein Bild zu generieren, wird das Histogramm des betreffenden Bildes mittels Klusterverfahren wie K-Means- oder dem IsoData-Algorithmus in zwei möglichst gleich große Teile zerlegt. Das Zentrum dieser zwei Cluster bildet den gesuchten Schwellenwert.\n\n"}
{"id": "18642", "url": "https://de.wikipedia.org/wiki?curid=18642", "title": "Sprachsynthese", "text": "Sprachsynthese\n\nUnter Sprachsynthese versteht man die künstliche Erzeugung der menschlichen Sprechstimme. Ein Text-to-Speech-System (TTS) (oder \"Vorleseautomat\") wandelt Fließtext in eine akustische Sprachausgabe.\n\nGrundsätzlich lassen sich zwei Ansätze zur Erzeugung von Sprachsignalen unterscheiden. Zum einen kann durch die sogenannte Signalmodellierung auf Sprachaufnahmen (Samples) zurückgegriffen werden. Zum anderen kann das Signal aber auch durch die sogenannte physiologische (artikulatorische) Modellierung vollständig im Rechner erzeugt werden. Während die ersten Systeme auf Formantsynthesen beruhten, basieren die zurzeit industriell eingesetzten Systeme vorwiegend auf Signalmodellierung. \n\nEin besonderes Problem für die Sprachsynthese ist die Erzeugung einer natürlichen Sprachmelodie (Prosodie).\n\nSchon lange vor der Erfindung der elektronischen Signalverarbeitung versuchten Wissenschaftler Maschinen zu konstruieren, die menschliche Sprache erzeugen können. Gerbert von Aurillac (1003) wird ein aus Bronze gefertigter „Sprechender Kopf“ zugeschrieben, von dem berichtet wurde, dass er „ja“ und „nein“ sagen konnte. Wohl eher in den Bereich der Legenden gehören die Apparate von Albertus Magnus (1198–1280) und Roger Bacon (1214–1294). \n\nDer deutsche, in Kopenhagen tätige Wissenschaftler Christian Kratzenstein baute 1779 aufgrund eines Preisausschreibens der St. Petersburger Akademie eine „Sprachorgel“, die durch freischwingende Lingualpfeifen mit dem menschlichen Vokaltrakt nachgebildeten Resonatoren fünf lange Vokale (a, e, i, o und u) synthetisieren konnte. Wolfgang von Kempelen entwickelte bereits seit ca. 1760 eine Sprechmaschine, die er 1791 in seiner Veröffentlichung „Mechanismus der menschlichen Sprache nebst der Beschreibung seiner sprechenden Maschine“ darstellte. Diese Synthese basierte wie Kratzensteins auf einem Blasebalg als Lungenäquivalent, die eigentliche Anregung geschah aber erheblich anatomienäher durch eine einzelne, aufschlagende Lingualpfeife. Damit waren einige Vokale und auch Plosive möglich. Darüber hinaus ließen sich über verschiedene Mechanismen einige Frikative darstellen. An den Stimmbändern schloss ein Ledertubus an, der durch eine Hand verformt werden konnte, und so die variable Geometrie und Resonanzverhalten des Vokaltrakts nachbildete. Von Kempelen schrieb:\nCharles Wheatstone baute 1837 eine \"Speaking Machine\", die auf diesem Entwurf beruht, ein Nachbau findet sich im Deutschen Museum. 1857 baute Joseph Faber die \"Euphonia\", die ebenso diesem Prinzip folgt.\n\nEnde des 19. Jahrhunderts entwickelte sich das Interesse weg vom Nachbau menschlicher Sprachorgane (genetische Sprachsynthese), hin zur Simulation des akustischen Raumes (gennematische Sprachsynthese). So synthetisierte Hermann von Helmholtz erstmals Vokale mit Hilfe von Stimmgabeln, die auf die Resonanzfrequenzen des Vokaltraktes in bestimmten Vokalstellungen abgestimmt waren. Diese Resonanzfrequenzen werden als Formanten bezeichnet. Sprachsynthese durch Kombination von Formanten war technischer Mainstream bis in die Mitte der 1990er Jahre.\n\nIn den Bell Labs wurde in den 1930ern der \"Vocoder\", ein tastaturgesteuerter elektronischer Sprachsynthesizer entwickelt, über den gesagt wurde, dass er klar verständlich war. Homer Dudley verbesserte diese Maschine zum \"Voder\", der in der Weltausstellung 1939 präsentiert wurde. Der Voder benutzte elektrische Oszillatoren zur Erzeugung der Formantfrequenzen.\n\nDie ersten computerbasierten Sprachsynthesesysteme wurden in den späten 1950ern entwickelt, das erste komplette Text-To-Speech-System 1968 fertiggestellt. Der Physiker John Larry Kelly, Jr entwickelte 1961 bei den Bell Labs eine Sprachsynthese mit einem IBM 704 und ließ ihn das Lied \"Daisy Bell\" singen. Der Regisseur Stanley Kubrick war davon so beeindruckt, dass er es in den Film \"\" integrierte.\n\nWährend frühe elektronische Sprachsynthesen noch sehr roboterhaft klangen und teilweise schwer verständlich waren, erreichen sie etwa seit der Jahrtausendwende eine Qualität, bei der es mitunter schwierig ist, sie von menschlichen Sprechern zu unterscheiden. Das ist hauptsächlich der Tatsache zu verdanken, dass sich die Technologie von der eigentlichen Synthese des Sprachsignals abgewandt hat und sich darauf konzentriert, aufgenommene Sprachsegmente optimal zu verketten.\n\nSprachsynthese setzt eine Analyse der menschlichen Sprache voraus, bezüglich der Phoneme, aber auch der Prosodie, weil eben ein Satz allein durch die Satzmelodie unterschiedliche Bedeutungen haben kann.\n\nWas den Syntheseprozess an sich betrifft, gibt es verschiedene Methoden. Gemeinsam ist allen Methoden, dass sie auf eine Datenbank zurückgreifen, in der charakteristische Informationen über Sprachsegmente hinterlegt sind. Elemente aus diesem Inventar werden zur gewünschten Äußerung verknüpft. Sprachsynthesesysteme lassen sich anhand des Inventars der Datenbank und insbesondere der Methode zur Verknüpfung klassifizieren. Tendenziell fällt die Signalsynthese umso einfacher aus, je größer die Datenbank ist, da diese dann bereits Elemente enthält, die der gewünschten Äußerung näher sind und weniger Signalbearbeitung notwendig ist. Aus dem gleichen Grund gelingt bei einer großen Datenbank meistens eine natürlicher klingende Synthese.\n\nEine Schwierigkeit der Synthese liegt in dem Aneinanderfügen von Inventarelementen. Da diese aus verschiedenen Äußerungen stammen, unterscheiden sie sich auch in der Lautstärke, der Grundfrequenz und der Lage der Formanten. Bei einer Vorverarbeitung der Datenbank oder beim Verbinden der Inventarelemente müssen diese Unterschiede möglichst gut ausgeglichen werden (Normalisierung), um nicht die Qualität der Synthese zu beeinträchtigen. \n\nDie Unit Selection liefert die beste Qualität besonders bei einer eingeschränkten Domäne. Die Synthese verwendet eine große Sprachdatenbank, in der jede aufgenommene Äußerung in einige oder alle der folgenden Einheiten segmentiert wird:\nDiese Segmente werden mit einem Verzeichnis von einer Reihe akustischer und phonetischer Eigenschaften wie Grundfrequenzverlauf, Dauer oder Nachbarn gespeichert. \n\nFür die Synthese werden durch spezielle Suchalgorithmen, gewichtete Entscheidungsbäume, eine Reihe von möglichst großen Segmenten bestimmt, die der zu synthetisierenden Äußerung hinsichtlich dieser Eigenschaften möglichst nahekommen. Da diese Reihe ohne oder mit wenig Signalverarbeitung ausgegeben wird, bleibt die Natürlichkeit der gesprochenen Sprache erhalten, solange wenige Verkettungsstellen erforderlich sind.\n\nAnfang des 21. Jahrhunderts durchgeführte Experimente haben gezeigt, dass die korrekte Wiedergabe der Lautübergänge wesentlich für die Verständlichkeit der Sprachsynthese ist. \nUm alle Lautübergänge zu speichern, wird eine Datenbasis mit etwa 2500 Einträgen verwendet. Darin ist jeweils der Zeitbereich des stationären Teils, die Phonemmitte eines Phonems, bis zum stationären Teil des folgenden Phonems abgespeichert. Für die Synthese werden die Informationen entsprechend zusammengefügt (konkateniert).\n\nWeitere Koartikulationseffekte, die viel zur Natürlichkeit der Sprache beitragen, können durch umfangreichere Datenbasen berücksichtigt werden. Ein Beispiel dafür ist Hadifix, das Halbsilben, Diphone und Suffixe enthält.\n\nDie Signalerzeugung gibt die gewünschten Segmente aus der Datenbank mit dem vorgegebenen Grundfrequenzverlauf wieder. Diese Ausprägung des Grundfrequenzverlaufs kann auf verschiedene Arten geschehen, worin sich die folgenden Verfahren unterscheiden.\n\nBei Synthesen, die eine Quelle-Filter-Separation verwenden, wird eine Signalquelle mit periodischer Signalform verwendet. Deren Periodenlänge wird passend zur Grundfrequenz der zu synthetisierenden Äußerung eingestellt. Dieser Anregung wird je nach Phonemtyp zusätzlich Rauschen beigemischt. Die abschließende Filterung prägt die lautcharakteristischen Spektren auf. Vorteilhaft bei dieser Klasse von Verfahren ist die einfache Grundfrequenzsteuerung der Quelle. Ein Nachteil ergibt sich durch die in der Datenbank gespeicherten Filterparameter, deren Bestimmung aus Sprachproben schwierig ist. Je nach Art des Filters bzw. der zugrunde liegenden Sichtweise des Sprechens unterscheidet man die folgenden Verfahren: \n\nDie Formantsynthese beruht auf der Beobachtung, dass es zur Unterscheidung der Vokale genügt, die ersten zwei Formanten treffend zu reproduzieren. Jeder Formant wird durch einen in der Mittenfrequenz und Güte steuerbaren Bandpass, ein Polfilter 2. Ordnung, nachgebildet. Die Formantsynthese ist durch analoge elektronische Schaltungen vergleichsweise einfach realisierbar.\n\nDas akustische Modell bildet die gesamten Resonanzeigenschaften des Vokaltrakts durch einen geeigneten Filter nach. Häufig wird der Vokaltrakt dazu vereinfacht als Rohr variablen Querschnitts betrachtet, wobei Quermoden vernachlässigt werden, da die seitliche Ausdehnung des Vokaltrakts klein ist. Die Querschnittsänderungen werden weiterhin durch äquidistante Querschnittssprünge approximiert. Ein häufig gewählter Filtertyp ist der Kreuzgliedketten-Filter, bei dem eine direkte Beziehung zwischen Querschnitt und Filterkoeffizient besteht.\n\nDiese Filter sind eng verwandt mit dem Linear Predictive Coding (LPC), das ebenfalls für Sprachsynthesen eingesetzt wird. Bei dem LPC werden ebenfalls die gesamten Resonanzeigenschaften berücksichtigt, es besteht jedoch kein direkter Zusammenhang zwischen Filterkoeffizienten und Querschnittsverlauf des Vokaltrakts.\n\nDie artikulatorische Synthese stellt gegenüber dem akustischen Modell eine Beziehung zwischen der Stellung der Artikulatoren und dem daraus resultierenden Querschnittsverlauf des Vokaltrakts her. Hier kommen zur Nachbildung der Resonanzcharakteristik neben zeitdiskreten Kreuzgliedkettenfiltern auch Lösungen der zeitkontinuierlichen Horngleichung zum Einsatz, aus denen das Zeitsignal durch Fouriertransformation gewonnen wird.\n\nPitch Synchronous Overlap Add, abgekürzt PSOLA, ist ein Syntheseverfahren, bei dem sich in der Datenbank Aufzeichnungen des Sprachsignals befinden. Sofern es sich um periodische Signale handelt, sind diese mit einer Information über die Grundfrequenz (Pitch) versehen und der Anfang jeder Periode ist markiert. Bei der Synthese werden diese Perioden mit einem bestimmten Umfeld mittels einer Fensterfunktion ausgeschnitten und dem zu synthetisierenden Signal an passender Stelle hinzuaddiert: Je nachdem ob die gewünschte Grundfrequenz höher oder tiefer liegt als die des Datenbankeintrags, werden sie entsprechend dichter oder weniger dicht als in dem Original zusammengefügt. Zur Anpassung der Lautdauer können Perioden entfallen oder doppelt ausgegeben werden. Dieses Verfahren wird auch als TD-PSOLA oder PSOLA-TD (TM) bezeichnet, wobei TD für Time Domain steht und hervorhebt, dass die Verfahren im Zeitbereich arbeiten.\n\nEine Weiterentwicklung ist das Multi Band Resynthesis OverLap Add – Verfahren, kurz MBROLA. Hier werden die Segmente in der Datenbank durch eine Vorverarbeitung auf eine einheitliche Grundfrequenz gebracht und Phasenlage der Harmonischen wird normalisiert. Dadurch entstehen bei der Synthese eines Übergangs von einem Segment auf das nächste weniger perzeptiv wahrnehmbare Störungen und die erzielte Sprachqualität ist höher.\n\nDiese Syntheseverfahren sind mit der Granularsynthese verwandt, die in Klangerzeugung und -verfremdung bei elektronischer Musikproduktion zum Einsatz kommt.\n\nDie parametrische Sprachsynthese ist eine Gruppe von Verfahren, welche auf stochastischen Modellen basieren. Bei diesen Modellen handelt es sich entweder um Hidden Markov Modelle (HMM), um stochastische Markov Graphen (SMG), oder neuerdings auch um eine Kombination dieser beiden. Grundprinzip ist, dass die aus einer Textvorverarbeitung gewonnenen, symbolischen Phonemfolgen eine statistische Modellierung durchlaufen, indem sie zunächst in Segmente zerlegt und jedem dieser Segmente sodann ein bestimmtes Modell aus einer bestehenden Datenbasis zugeordnet wird. Jedes dieser Modelle wiederum wird durch eine Reihe von \"Parametern\" beschrieben und schließlich mit den anderen Modellen verkettet. Die Verarbeitung zu einem künstlichen Sprachsignal, das sich an den besagten Parametern orientiert, schließt die Synthese dann ab. Im Falle der Verwendung flexiblerer, stochastischer Markov Graphen, lässt sich ein solches Modell sogar insofern optimieren, als dass ihm vorab und mittels Zuführung von Beispielen natürlicher Sprache eine gewisse Grundnatürlichkeit antrainiert werden kann. Statistische Verfahren dieser Art entstammen dem konträren Bereich der Spracherkennung und motivieren sich durch Erkenntnisse über den Zusammenhang zwischen der Wahrscheinlichkeit einer bestimmten, gesprochenen Wortfolge und der dann zu erwartenden, ungefähren Sprechgeschwindigkeit, oder ihrer Prosodie.\n\nDie Nutzung von Sprachsynthese-Software muss kein Selbstzweck sein. Menschen mit Sehbehinderungen – z. B. Grauem Star oder Altersbedingter Makuladegeneration – nutzen TTS-Softwarelösungen, um sich Texte direkt am Bildschirm vorlesen zu lassen. Blinde Menschen können einen Computer mittels einer Screenreader-Software bedienen und bekommen Bedienelemente und Textinhalte angesagt. Aber auch Dozenten nutzen die Sprachsynthese für die Aufzeichnung von Vorträgen. Ebenso nutzen Autoren TTS-Software, um selbst geschriebene Texte auf Fehler und Verständlichkeit hin zu prüfen.\n\nEin weiteres Einsatzgebiet findet sich in Form von Software, die das Erzeugen von MP3-Dateien erlaubt. Hierdurch kann Sprachsynthese-Software für die Erzeugung einfacher Podcasts bzw. Audioblogs genutzt werden. Erfahrungsgemäß kann die Produktion von Podcasts bzw. Audioblogs sehr zeitintensiv sein.\n\nBei der Arbeit mit US-amerikanischer Software ist zu beachten, dass die vorhandenen Stimmen von unterschiedlicher Güte sind. Englische Stimmen haben eine höhere Qualität als deutsche. Eine 1:1-Kopie der Texte in eine TTS-Software kann nicht empfohlen werden, eine Nachbearbeitung ist in jedem Fall nötig. Dabei geht es nicht nur um das Ersetzen von Abkürzungen, auch das Einfügen von Satzzeichen – auch wenn diese grammatikalisch nicht korrekt sind – kann helfen, um auf das Satztempo Einfluss zu nehmen. Deutsche „Übersetzungen“ mit Anglizismen stellen für die Sprachsynthese i. d. R. ein unüberwindbares Problem dar.\n\nHäufige Anwendungen sind Ansagen in Telefon- und in Navigationssystemen.\n\n\n\n\n\nGeschichte\n\nSysteme\n\nWebinterfaces\n\n"}
{"id": "18815", "url": "https://de.wikipedia.org/wiki?curid=18815", "title": "Zuse Z3", "text": "Zuse Z3\n\nDie Z3 war der erste funktionsfähige Digitalrechner weltweit und wurde 1941 von Konrad Zuse in Zusammenarbeit mit Helmut Schreyer in Berlin gebaut. Die Z3 wurde in elektromagnetischer Relaistechnik mit 600 Relais für das Rechenwerk und 1400 Relais für das Speicherwerk ausgeführt.\n\nDie Z3 verwendete wie die Z1 die von Konrad Zuse in die Rechnertechnik eingeführte binäre Gleitkommaarithmetik. Im Gegensatz zum Entwurf und der Benutzung des ENIAC genügte der Entwurf der Z3 nicht der späteren Definition eines turingmächtigen Computers und sie wurde auch nie so genutzt. Erst 1998 fand man heraus, dass sie rein theoretisch gesehen durch trickreiche Nutzung aufwendiger Umwege dennoch diese Eigenschaft hatte. Die Z3 gilt besonders in Deutschland als erster funktionsfähiger Universalrechner der Welt. Sie wurde 1944 bei einem Bombenangriff zerstört.\n\nDer Entwicklung der Z3 ging die Entwicklung der vollmechanischen Z1 und des Übergangsmodells Z2 voraus. Die Deutsche Versuchsanstalt für Luft- und Raumfahrttechnik hatte sich die Z2 angeschaut und gab Zuse 25.000 Reichsmark, damit er die Z3 bauen konnte. Am 12. Mai 1941 wurde die Z3 schließlich einer Gruppe von Wissenschaftlern (darunter Alfred Teichmann und Curt Schmieden) vorgestellt. Als Zuse 1941 kurzzeitig in den Krieg eingezogen wurde, schrieb er einem Freund: \n\nDie original Z3 Rechenmaschine wurde im Zweiten Weltkrieg durch Bombenangriffe auf Berlin 1944 zerstört. Für Zuse war das ein tragischer Moment, da er keinen Beweis mehr hatte, dass es wirklich eine funktionsfähige Z3 gegeben hatte. Ein funktionsfähiger Nachbau, der 1962 von der Zuse KG zu Ausstellungszwecken angefertigt wurde, befindet sich im Deutschen Museum in München. Am ehemaligen Standort, an der Ruine des Hauses in der Methfesselstraße im Berliner Stadtteil Kreuzberg, erinnert eine Gedenktafel an Zuses Wirkungsstätte. Seit Konrad Zuses 100. Geburtstag am 22. Juni 2010 ist zudem ein Nachbau der Z3 im Konrad-Zuse-Museum in Hünfeld ausgestellt.\n\nNeben der Tatsache, dass sie der erste voll funktionsfähige programmierbare Digitalrechner war, enthielt die Z3 sehr viele Merkmale moderner Rechner:\n\n\nAuch die Z1 verfügte über fast alle der oben angeführten Merkmale, erlangte allerdings nicht so viel Aufsehen, da ihr Rechenwerk aufgrund des mechanischen Aufbaus nicht sehr zuverlässig arbeitete. Allgemein ähneln der Aufbau von Z1 und Z3 einander sehr, was insbesondere für das Rechenwerk gilt.\n\nDie Z3 bestand aus\n\nDer Rechner sah aus wie eine Schrankwand und füllte einen ganzen Raum. Er wog ca. eine Tonne.\n\nDie Z3 ist eine getaktete Maschine. Die Taktung wird von einem Elektromotor übernommen, der eine Taktwalze antreibt. Diese ist eine Trommel, die sich ca. 5,3 Mal pro Sekunde dreht und während einer Drehung die Steuerung der einzelnen Relaisgruppen übernimmt. Die Drehgeschwindigkeit der Trommel entspricht dabei dem Verarbeitungstakt moderner Hauptprozessoren, womit bei diesem Rechner eine Geschwindigkeit von 5,3 Hz gegeben ist. Der Arbeitsspeicher des Z3 umfasst 200 Byte.\nDie Z3 verfügt über folgende Maschinenbefehle:\n\nDie Eingabe numerischer Daten muss über die Tastatur erfolgen, das heißt, Zahlen können nicht auf dem Lochstreifen kodiert werden. Über die Tastatur können alle Operationen außer den Speicherzugriffen (Pr und Ps) direkt ausgeführt werden. Der Lochstreifen kann nur Befehle enthalten, wobei jeder Befehl mit 8 Bit kodiert wird. Die genaue Kodierung auf Lochstreifen findet sich im Artikel Opcode.\n\nDie Z3 kennt keine Sprungbefehle, ist jedoch mit Hilfe geschickter Ausnutzung der endlichen Rechengenauigkeit turingmächtig, wie Raúl Rojas 1998 zeigte. Allerdings ist dieses Resultat nur von theoretischer Bedeutung, da Programme mit Sprunganweisungen umständlich transformiert werden müssen und die Programmlaufzeit steigt.\n\nJede Rechenoperation der Z3 basiert auf der Addition zweier natürlicher Zahlen. Diese Basisoperation der Addition wird durch XOR(XOR(x, y), CARRY(x, y)) berechnet, wobei CARRY(x, y) die Übertragsfunktion ist, z. B. CARRY(0011011, 1010110) = 0111100.\n\n\nAllgemein besteht das Rechenwerk aus zwei Teilen, einem Werk für die Rechnung mit Exponenten und ein Werk für die Rechnung mit Mantissen. Für Befehle, bei denen iterative Methoden zum Einsatz kommen (Lm, Li, Lw, Lu, Ld), wird ein Sequenzer benutzt, um einzelne Teile des Rechenwerks anzusteuern. Dies entspricht grob modernen Mikroprogrammen.\n\nFür die Z3 wurden einige Prüfprogramme und ein \"Programm für die Berechnung einer komplexen Matrix\" geschrieben, das entsprechend einer Lösung von Hans Georg Küssner zur Berechnung von kritischen Flatterfrequenzen bei Flugzeugen verwendet wurde. Der Einsatz des Rechners wurde aber damals nicht als \"dringlich\" eingestuft, so dass es nie zu einem Routinebetrieb kam.\n\nIn den USA und weiten Teilen der Welt wird der 1944 gebaute ENIAC als der erste Computer angesehen, was sich damit begründen lässt, dass die beiden Rechner unterschiedliche Eigenschaften besitzen und zur Definition des Begriffs Computer unterschiedliche Kriterien herangezogen werden.\n\nDie Z3 war der erste Digitalrechner und gleichzeitig der erste binäre, programmierbare und turingmächtige. Allerdings war sie im Gegensatz zum ENIAC, der Röhren benutzte, nicht elektronisch (ein Förderantrag durch Helmut Schreyer für ein elektronisches Nachfolgemodell wurde von der Reichsregierung als nicht kriegswichtig abgelehnt); außerdem wird die Turingmächtigkeit nur dank eines vom Konstrukteur nicht vorhergesehenen Tricks ermöglicht. Der ENIAC war der fünfte Digitalrechner der Geschichte und der erste, der die Kriterien elektronisch, programmierbar und turingmächtig gleichzeitig erfüllte. Er arbeitete mit dem Dezimalsystem, das heißt, er war kein Binärcomputer wie die Z3 und wie alle modernen Computer. In Deutschland wird im Allgemeinen aufgrund ihres höheren Alters und ihrer binären Arbeitsweise, mit der auch heute noch alle Computer arbeiten, der Z3 dieser Titel zugesprochen, wohingegen man dem Aspekt der Hardwareausführung eine geringere Bedeutung zuweist.\n\nDie historische Präferenz für den ENIAC mag auch darin begründet liegen, dass diesem nach dem Zweiten Weltkrieg in den USA eine ungleich größere Aufmerksamkeit zuteilwerden konnte als der Z3, die bei einem Bombenangriff zerstört wurde.\n\n\n\n\n"}
{"id": "19068", "url": "https://de.wikipedia.org/wiki?curid=19068", "title": "Sinclair ZX Spectrum", "text": "Sinclair ZX Spectrum\n\nDer Sinclair ZX Spectrum 16/48k ist ein Heimcomputer, der am 23. April 1982 von dem in Großbritannien ansässigen Unternehmen Sinclair Research zum Preis von 125 £ bzw. 175 £ auf den Markt gebracht wurde. 1986 erwarb Amstrad den Markennamen und vermarktete den Rechner und Nachfolgemodelle bis 1992.\n\nEr ist der Nachfolger des Sinclair ZX81. Im Gegensatz zu seinem Vorgänger arbeitet er mit dem ASCII-Zeichensatz. Als Prozessor kommt ein Zilog Z80 (Z80A mit 3,5 MHz) zur Anwendung. Er verfügt über 16 oder 48 KB Arbeitsspeicher und 16 KB Festwertspeicher (ROM). Als Datenspeicher wird ein handelsüblicher Kassettenrekorder angeschlossen. Über einen Steckplatz können Hardwareerweiterungen angeschlossen werden.\n\nDer Aufbau des Rechners war, wie bei allen Sinclair-Rechnern dieser Zeit, auf Preisgünstigkeit ausgelegt. Insbesondere ist die Anzahl der Bauteile minimiert, und es wurden möglichst billige verwendet: So kamen bei der Version mit 48 KB Speicher defekte 64-Kilobyte-RAM-Bausteine zum Einsatz, bei denen die jeweils defekte Bank ausgeblendet wurde.\n\nEin spezieller („Custom-“)Chip, die \"ULA,\" kümmert sich um die Bildschirmdarstellung, die Tonausgabe und die Kassettenrecorder-Schnittstelle. Im Gegensatz zum ZX81 übernimmt die ULA sämtliche Aufgaben der Bilddarstellung, so dass der Prozessor damit nicht belastet wird.\n\nDie Grafikauflösung beträgt 256 × 192 Bildpunkte (Pixel). Für die Farbdarstellung werden jeweils 8 × 8 Pixel in Blöcke zusammengefasst, so dass effektiv nur ein Farbraster von 32 × 24 Blöcken zur Verfügung steht. In jedem Block kann die Vorder- und Hintergrundfarbe aus 8 Farben ausgewählt werden. Zusätzlich können die Farben eines Blocks aufgehellt oder blinkend geschaltet werden. Die Rahmenfarbe kann separat aus 8 Farben ausgewählt werden. Als Ausgabegerät wird gewöhnlicherweise ein Fernsehgerät über den Antenneneingang angeschlossen. Am Erweiterungsanschluss liegt auch ein Basisband-Videosignal an, dies muss jedoch mittels einer Bastellösung entnommen und nachträglich verstärkt werden.\n\nÜber einen eingebauten Lautsprecher bietet der Spectrum eine sehr eingeschränkte Tonausgabe; der Tonkanal des Fernsehers wird nicht benutzt. Zudem ist der Lautsprecher insbesondere bei den frühesten Modellen sehr leise. Es besteht jedoch die (inoffizielle) Möglichkeit, externe Lautsprecher am Tonkanalausgang für den Kassettenrekorder anzuschließen. Der Lautsprecher kann lediglich vom Prozessor ein- und ausgeschaltet werden, der auch die Tonfrequenz erzeugen muss. Dadurch blockiert die Tonausgabe den Prozessor. In manchen Spielen wurde dennoch durch trickreiche Programmierung zwei- oder sogar mehrstimmige Musik erzeugt, letztere jedoch mit ziemlich dürftiger Qualität. In den späteren 128 KB-Modellen wurde zusätzlich der Soundchip AY-3-8910 eingebaut, um dieses Manko zu beheben.\n\nIm ROM ist hauptsächlich ein BASIC-Interpreter mit integrierter Eingabeaufforderung und Betriebssystem untergebracht. Die Tastatur des ursprünglichen Spectrum besteht aus Gummitasten über einer Kontaktfolie. Im Gegensatz zu anderen Rechnern werden die BASIC-Befehle nicht Buchstabe für Buchstabe eingetippt. Dazu ist jede Taste mit bis zu sechs Zeichen und Befehlen belegt, so dass für einen BASIC-Befehl ein Tastendruck bzw. eine Tastenkombination genügt. Dadurch ist eine relativ schnelle und tippfehlerfreie Programmierung möglich. Weiterhin kann so der BASIC-Interpreter die Befehle sehr schnell erkennen.\n\nDie Gesamterscheinung ist gegenüber anderen Computern dieser Zeit klein. Das Netzteil ist als externes Gerät ausgeführt worden. Die Farbe des Gerätes ist schwarz, die Beschaffenheit der Tastatur erinnert an Radiergummis. Im Betrieb erwärmt sich das Gerät sehr stark, was die Lebensdauer der Kontaktmatte der Tastatur beeinträchtigt.\n\nZahlreiche Erweiterungen von Drittanbietern und ein großes Sortiment an Spielen machten den Spectrum zum erfolgreichsten Computer von Sinclair.\n\nAn Software waren vor allem Spiele, aber auch Textverarbeitung, Datenbanken, diverse Programmiersprachen, Assembler und Debugger erhältlich. In den zeitgenössischen Heimcomputerzeitschriften waren noch keine Datenträger beigelegt, sondern beispielsweise Spiele als BASIC-Programm (Programmausdruck) abgedruckt und konnten so von geduldigen Anwendern per Gummitastatur eingegeben werden. Genauso waren im Verhältnis dazu größere Programme über eigene Bücher durch Eintippen zugänglich. Im Vergleich zum marktbeherrschenden, etwas teureren C64 von Commodore war die Verfügbarkeit und Vielfalt der Software auf dem deutschen Markt deutlich geringer, auch deshalb, weil der ZX Spectrum im amerikanischen Markt so gut wie gar nicht präsent war, weswegen dortige Unternehmen ihre Software meist nicht für den Spectrum anboten. Auf dem britischen Markt war der ZX Spectrum lange der meistverkaufte Heimcomputer mit dem größten Angebot an Software.\n\nIn einigen Ländern des ehemaligen Ostblocks, beispielsweise in Ungarn, wurden in Fernseh- und Radiosendungen, die sich mit Computertechnik befassten, gelegentlich Programme für den Spectrum, später auch für den Commodore 16 und Commodore 64, in Form von Audiosignalen übertragen. Diese konnten mit einem Kassettenrecorder aufgezeichnet und dann in den Heimcomputer geladen werden.\n\nProgrammiert wurde hauptsächlich unter BASIC (serienmäßig im ROM), Assembler und Pascal. Mit der Verfügbarkeit von entsprechenden Peripheriegeräten kam auf den späteren Modellen auch teilweise das Betriebssystem CP/M zum Einsatz (der Original-Spectrum kann ohne Zusatzmodule kein CP/M ausführen, da dieses RAM-Speicher an der Adresse 0 voraussetzt, der Spectrum dort aber nur ROM hat).\nBereits ein minimales System besteht aus dem Computer und mehreren Peripheriegeräten: das zugehörige externe Netzteil (9 V Gleichspannung), das auch weitere Peripheriegeräte (z. B. Interfaces und Microdrive) mit Strom versorgt, das Handbuch, das das Sinclair-eigene BASIC und die Handhabung des Gerätes ausführlich erklärt, sowie die Kassette mit Beispielprogrammen, die die Fähigkeiten des Rechners demonstrieren und deren BASIC-Quelltexte Einblick in die Programmiermöglichkeiten gewähren. Weiterhin benötigt ein funktionsfähiges System auch Geräte, die der Benutzer bereits besitzen muss. Ein handelsüblicher Fernseher mit Antenneneingang wird als Monitor benötigt. Bei Verwendung eines Schwarzweißfernsehers werden die Grautöne (zusammen mit der Helligkeitsstufe \"BRIGHT\" sind 8 Farben bei 16 Farbnuancen möglich) kontrastreich abgestuft. Ein Kassettenrekorder erlaubt das Laden der Beispielprogramme. Anders als bei einigen andern Heimcomputern wurden keine speziellen Computer-Kassettenrecorder („Datasetten“) für den ZX Spectrum angeboten. Stattdessen wurde ein ganz gewöhnlicher Recorder für Compact Cassetten verwendet, der nur Mikrofon- und Kopfhörer-Anschlussbuchsen haben musste, in die das beim ZX Spectrum mitgelieferte Anschlusskabel eingesteckt werden konnte. Das rechtzeitige Starten und Stoppen des Bandes war Aufgabe des Benutzers.\n\nObwohl damals meist noch nicht zwischen internen und externen Peripheriegeräten unterschieden wurde, sind bereits in diesem Gerät interne Zubehörteile verbaut, so z. B. der HF-Wandler, der aus den Videosignalen das Fernsehbild erzeugt, oder das auf der Hauptplatine aufgebrachte Interface für den Anschluss des Kassettenrekorders.\n\nAb 1983 bot der Hersteller Bandlaufwerke mit der Bezeichnung \"ZX Microdrive\" an. Diese werden über den Controller ZX Interface 1 mit dem Computer verbunden. In den Steckmodulen läuft ein Endlosband, welches an einem Tonkopf vorbeigeführt wird und etwa 85 Kilobyte Speicherkapazität bietet. Ein vollständiger Durchlauf des Bandes dauert 7,5 Sekunden. Diese Microdrives werden ebenfalls im Sinclair QL verwendet, dort allerdings mit ca. 110 Kilobyte Speichervermögen. Dieses Speichermedium ist nicht mit den Microdrives von IBM zu verwechseln.\n\nDas \"ZX Interface 1\" für den Spectrum wurde erstmals im Jahre 1983 veröffentlicht. Ursprünglich war es als Local-Area-Network-Interface für den Gebrauch in Schulen gedacht. Das Gerät wurde jedoch vor der Veröffentlichung überarbeitet und dann als Controller für bis zu acht ZX-Microdrive-Laufwerke benutzt. Weiterhin bot das Gerät eine RS-232-Schnittstelle, um daran hauptsächlich Drucker anzuschließen. Das Interface 1 enthielt einen 8-K-ROM. Da der Spectrum den gesamten Adressraum bereits voll belegt hatte, wurde der ROM des Interface 1 mittels Bank Switching in den Adressraum des 16-K-ROMs eingeblendet.\n\nDas Interface ist so ausgeführt, dass es sich mit dem Computer zu einer Einheit fest verschrauben lässt. Dazu wurden zwei Schrauben aus dem Grundgerät herausgedreht und das Interface nach dem Anstecken an der Hauptplatine am hinteren Teil des Computers fest mit diesem verschraubt, ohne ihn jedoch zuvor öffnen zu müssen. Das Grundgerät steht dadurch leicht schräg, ähnlich einer ergonomisch nach vorn gekippten Tastatur.\n\nDas Gerät bietet einen Netzwerkanschluss, mit dem man bis zu 64 ZX-Spectrums per Netzwerkleitung zusammenschließen kann. Dieses Netzwerk wurde ZX Net genannt und hatte eine Datenrate von bis zu 100 kbit/s.\n\n\"Interface 1\" und \"Microdrive\" wurden als Paket verkauft. Mit enthalten war Software, darunter einige Spiele und das Textverarbeitungsprogramm Tasword II, bei welchem zur besseren Anpassung an die Hardware des Benutzers Drucker- und Dateifunktionen in BASIC-Routinen ausgeführt waren. Es enthielt einen eigenen Zeichensatz, der jedes Standardzeichen in zwei schmale Zeichen mit lediglich 7×3 Bildpunkten (Standardzeichen ohne Unterlängen) teilte und war damit das einzige Textverarbeitungsprogramm, das die begrenzten Möglichkeiten des Spectrum für normale Büroarbeit nutzbar machte.\n\nDas \"ZX Interface 2\" ist ein Peripheriegerät, das im September 1983 erstmals verkauft wurde. Es war ausgerüstet mit zwei Joystick-Ports und einem ROM-Steckmodulschacht, der für Spiel-Module vorgesehen war. Die Joystick-Anschlüsse waren zwar die gleichen, vom Atari 2600 übernommenen 9-poligen D-Sub-Buchsen wie bei fast allen anderen damaligen Computer- und Konsolensystemen, aber ihre Ansteuerung über Software war nicht kompatibel mit der weit verbreiteten Kempston-Schnittstelle, wodurch die meisten Spiele, die vor dem Interface veröffentlicht worden waren, nicht funktionierten.\n\nDie Anzahl der veröffentlichten Spiel-Steckmodule war begrenzt: Die Herstellung war fast doppelt so teuer wie der Verkauf auf Compact-Cassetten, und der Speicherplatz auf den Modulen war auf 16 KB beschränkt, obwohl der Spectrum einen Speicher von 48 KB hatte.\n\nZehn Spiele wurden veröffentlicht:\n\nAm Spectrum können alle für den Sinclair ZX81 geeigneten Drucker angeschlossen werden. Der preiswerte – allerdings nicht zur professionellen Textverarbeitung gedachte – Drucker aus dem Hause Sinclair druckte auf ein mit Aluminium beschichtetes Papier mit der Auflösung des Bildschirms (256 Punkte pro Zeile). Das Papier ist etwas breiter als der Ausdruck eines heutigen Kassenbons, was ein sehr kompaktes Gerät – ungefähr so groß wie eine Faust – ermöglichte. (zu diesem und kompatiblen Modellen siehe Beschreibung: Drucker für den ZX81)\n\nNeben diesen Geräten gab es eine Vielzahl von Erweiterungen, wie Joystick-Schnittstellen (z. B. ZX Interface 2 oder Kempston), das Multiface 1/128 oder auch verschiedene Systeme zum Anschluss von Diskettenlaufwerken (z. B. Beta Disk Interface) für den Sinclair ZX Spectrum. Auch gibt es Bastellösungen für den Anschluss von Festplatten.\n\nSinclair selbst brachte zunächst den \"ZX Spectrum Plus\" heraus, dessen Elektronik identisch mit der des 48-KB-Spectrum ist. Im Unterschied zu diesem ist der Spectrum Plus in ein größeres Kunststoffgehäuse eingebaut und bietet eine Tastatur mit harten Tastenkappen. Zusätzlich befindet sich an der Seite ein Reset-Taster. Die zusätzlichen Tastenkappen betätigen durch eine durchdachte Folientastatur unter den Kappen elektrisch mehrere einzelne Tasten, wodurch einige häufig verwendete Tastenkombinationen des Vorgängermodells jetzt durch Einzeltasten erreichbar sind. Form und Größe des Gehäuses sind so gewählt, dass Original-Zubehör für den ZX Spectrum auch am ZX Spectrum Plus angeschlossen werden kann.\n\nAls nächstes Modell von Sinclair erschien der \"ZX Spectrum 128K.\" Er kann in einen Kompatibilitätsmodus umgeschaltet werden, so dass fast alle Hard- und Software der vorherigen ZX-Spectrum Versionen verwendet werden kann. Dieser Computer sieht dem ZX Spectrum Plus ähnlich, bietet aber 128 KB Speicher und einen Dreikanal-Soundchip (was die Programmkompatibilität trotz Kompatibilitätsmodus beeinträchtigt). Zusätzlich hat er noch eine MIDI-Schnittstelle.\n\nDanach erschien noch das auf Basis eines Motorola 68008 und Intel 8049 aufgebaute Computersystem Sinclair QL (Abkürzung für Quantum Leap, deutsch Quantensprung) mit 128 KB Arbeitsspeicher. Zwei Microdrives sind als Massenspeicher bereits in das Gehäuse integriert. Im Grafikmodus können mit dem QL maximal 512×256 Pixel dargestellt werden, im Textmodus 85×25 Zeichen. Besonders interessant waren die eingebauten Netzwerk- und Multitaskingfähigkeiten des Computers. Leider wurde der Sinclair QL zu früh angekündigt, was dazu führte, dass die Wartezeit für potentielle Kunden sehr lang ausfiel. Obwohl das Preis-/Leistungs-Verhältnis sehr gut war, konnte sich der QL nicht gegen die Konkurrenz in diesem Marktsegment, wie etwa die Apple-Macintosh-Serie durchsetzen, da bei diesen Konkurrenzgeräten die angebotene Software deutlich fortschrittlicher war als beim QL.\n\nEin von Sinclair angekündigter „Super Spectrum“, der Spectrum LOKI heißen sollte, als Commodore-Amiga-Konkurrenz gedacht war und mit zwei Z80H-Prozessoren laufen sollte, kam nie über das frühe Prototypstadium hinaus. Das Projekt wurde nach dem Verkauf von Sinclair an Amstrad eingestellt.\n\nZahlreiche Klone wurden weltweit gebaut, u. a. von Timex in den USA, aber auch (ohne Lizenz) von Universitäten in der DDR wie z. B. den GDC1, HCX, KuB 64K oder Spectral.\n\nNachdem der Hersteller Sinclair Research für 5 Mio. britische Pfund an Amstrad verkauft worden war, kamen unter deren Leitung noch die Modelle Spectrum +2/+2A (mit integriertem Kassettenrekorder) und +3 (mit integriertem 3-Zoll-Diskettenlaufwerk) auf den Markt, die wie der Spectrum 128K jeweils über 128 Kilobyte Speicher, eine MIDI-Schnittstelle und einen Dreikanal-Soundchip verfügen; letzterer wurde auch im Schneider bzw. Amstrad-CPC-Computer eingesetzt. Diese Modelle kennen zwei Betriebsarten, erweitert und kompatibel mit dem Original-Spectrum.\n\nIn der Bundesrepublik war der Sinclair ZX Spectrum, bis zum Erscheinen des Schneider/Amstrad CPC, nach dem Commodore C64 der zweitmeist verkaufte Computer. Sein Preis betrug 1983 ca. 400,- DM für das 16KB Modell und 530,- DM für 48KB. Der ZX Spectrum erlangte seine größten Erfolge in Großbritannien selbst, wo der Computer erheblich früher als der Commodore 64 am Markt erschien. Auch in der DDR hatte der Spectrum einige Erfolge, hauptsächlich wegen der Kompatibilität zum dort verbreiteten U880-Mikroprozessor und der unlizenzierten Nachbaumodelle.\n\nWeltweit wurden über 5 Millionen ZX-Spectrum-Exemplare verkauft – es gab insgesamt acht Modellvarianten. In Spanien war der ZX Spectrum z. B. lange Zeit ausgesprochen beliebt; das Modell mit 128 KB Speicher wurde sogar zuerst in Spanien auf den Markt gebracht. Es gab viele Nachbauten des ZX Spectrum. Insbesondere das Grundmodell des ZX Spectrum war relativ leicht nachzubauen, weil es außer der ULA keine speziellen Chips enthielt. Wegen der Vielzahl der Modelle ist es kaum möglich, eine annähernd genaue Anzahl der weltweit gebauten Spectrum-kompatiblen Computer zu ermitteln.\n\nFür andere Computersysteme existieren eine Vielzahl von Emulatoren, die die Funktion des ZX Spectrum nachbilden. Fortgeschrittene Emulatoren können mehrere ZX-Spectrum-Modelle mit hoher Präzision simulieren, einschließlich des Soundchips, der exakten Geschwindigkeit des Originalmodells und undokumentierter Befehle der Z80-CPU. Einige Emulatoren (selbst für den C64) erlauben es sogar, die originalen Kassettenaufnahmen per Toneingang oder -Datei in den Emulator zu laden.\n\n\n"}
{"id": "19086", "url": "https://de.wikipedia.org/wiki?curid=19086", "title": "GNU Privacy Guard", "text": "GNU Privacy Guard\n\nGNU Privacy Guard (englisch für \"GNU-Privatsphärenschutz\"), abgekürzt \nGnuPG oder GPG, ist ein freies Kryptographiesystem. Es dient zum Ver- und Entschlüsseln von Daten sowie zum Erzeugen und Prüfen elektronischer Signaturen.\n\nDas Programm implementiert den OpenPGP-Standard nach RFC 4880 und wurde als Ersatz für PGP entwickelt. Versionen ab 2.0 implementieren auch den S/MIME- und den PGP/MIME-Standard.\nGnuPG benutzt standardmäßig nur patentfreie Algorithmen und wird unter der GNU-GPL vertrieben. Es kann unter GNU/Linux, MacOS und diversen anderen unixoiden Systemen sowie unter Microsoft Windows betrieben werden.\n\nDas Projekt wurde 1997 von Werner Koch begonnen, der immer noch der Hauptentwickler ist. Ende 2014 rief Koch zum Crowdfunding auf, im Januar 2015 wurde das Thema von überörtlicher Presse aufgegriffen und innerhalb weniger Tage war die Finanzierung durch Spenden für zwei bis drei Jahre gesichert.\n\nGnuPG hat sich zum Ziel gesetzt, einer möglichst großen Benutzergruppe die Verwendung von kryptographischen Methoden zur vertraulichen Übermittlung von elektronischen Daten zu ermöglichen.\n\nGnuPG unterstützt dazu folgende Funktionen:\n\nBeide Funktionen können kombiniert werden. In der Regel wird dabei zuerst die Signatur gebildet und an die Daten angehängt. Dieses Paket wiederum wird dann verschlüsselt an die Empfänger versandt. Die Kombination beider Aktionen in einem Aufruf unterstützt GnuPG nur in dieser Reihenfolge. Beim Versand von E-Mails (als PGP/MIME nach RFC 3156) sind zwar beide Varianten möglich, aber durch Beschränkungen der Mailclients ist das in der Praxis die einzige mögliche Reihenfolge; die Möglichkeit, eine E-Mail zuerst zu verschlüsseln und dann mit einer Klartextsignatur zu versehen (die dann z. B. ein Virenscanner oder Spamfilter auswerten könnte, der die eigentliche Nachricht nicht entschlüsseln kann), ist nicht vorgesehen. Man kann allerdings Dateien unabhängig vom E-Mail-Versand verschlüsseln, an eine E-Mail anhängen und die E-Mail dann als PGP/MIME signieren lassen.\n\nGnuPG wird von zumindest den meisten Linux-Distributionen und verwandten Systemen im Rahmen ihres Paketmanagers zur Sicherung der Integrität der verteilten Softwarepakete verwendet und ist deshalb in den meisten Installationen bereits enthalten. Deshalb stellt das Booten von einem authentischen Installationsmedium eines solchen Systems eine Möglichkeit dar, GnuPG in einer sicheren Umgebung (d. h. frei von Schadsoftware) zu starten, etwa für die Erzeugung oder Verwendung von Schlüsseln mit hohen Sicherheitsanforderungen.\n\nGPG ist ein Public-Key-Verschlüsselungsverfahren, das heißt, dass zum Verschlüsseln von Nachrichten keine geheimen Informationen nötig sind. Jeder GPG-Nutzer erstellt ein Schlüsselpaar, das aus zwei Teilen besteht: dem privaten Schlüssel und dem öffentlichen Schlüssel. Auf den privaten Schlüssel darf nur der Eigentümer Zugriff haben. Daher wird dieser in der Regel auch mit einem Passwort geschützt. Mit diesem können Daten entschlüsselt und signiert werden. Der öffentliche Schlüssel dient dazu, Daten zu verschlüsseln und signierte Daten zu überprüfen. Er muss jedem Kommunikationspartner zur Verfügung stehen, der diese beiden Aktionen durchführen will. Die Daten können mit dem öffentlichen Schlüssel weder signiert noch entschlüsselt werden, daher ist seine Verbreitung auch mit keinem Sicherheitsrisiko behaftet. Die öffentlichen Schlüssel können mit anderen Nutzern über eine Vielzahl von Kanälen ausgetauscht werden, z. B. Internet-Schlüsselserver. Sie (bzw. die Kombination aus öffentlichem Schlüssel und User-ID) sollten vor der Verwendung unbedingt verlässlich geprüft werden, um Identitätsmanipulationen vorzubeugen, da die in öffentliche Schlüssel eingetragenen Identitätsinformationen (meist Name und E-Mail, ggf. auch ein Kommentar) trivial gefälscht werden können. GPG kann nur feststellen, ob die Daten mit einem bestimmten Schlüssel signiert bzw. verschlüsselt wurden. Ob der Schlüssel selbst vertrauenswürdig ist, muss der Anwender entscheiden, schließlich kann jeder einen Schlüssel mit den Angaben fremder Anwender erstellen und ihn auf einen Keyserver laden. Einem aus einer unsicheren Quelle (z. B. dem Internet) geladenen Schlüssel sollte man also zunächst nicht vertrauen. Zur Überprüfung besorgt man sich den Fingerabdruck (Hash-Wert) des Schlüssels über einen sicheren Kanal (z. B. Telefon) und vergleicht ihn mit dem lokal erzeugten des heruntergeladenen Schlüssels. Dies ist sicher, weil es nicht möglich ist, einen passenden Schlüssel für einen gegebenen Fingerabdruck zu erzeugen. Diese Sicherheit hängt an der Stärke der Hashfunktion (und der Menge möglicher Schlüssel). In der Version 4 des OpenPGP-Schlüsselformats ist dafür die Verwendung der Hashfunktion SHA-1 festgeschrieben, für die derzeit (2012) zwar Kollisionsangriffe, nicht aber die für die Imitation von Schlüsseln entscheidenden Second-Preimage-Angriffe möglich sind. Durch die kürzlich erfolgte Festlegung der SHA-3-Hashfunktion ist mit dem baldigen Beginn der Entwicklung des nächsten OpenPGP-Schlüsselformats zu rechnen.\n\nUm die Daten zu verschlüsseln oder zu signieren, stehen unterschiedlich starke Schlüssel zur Verfügung. Üblich sind momentan (2014) 2048- bis 4096-Bit starke Schlüssel, mit 2048 Bit empfohlener Länge. GPG verwendet derzeit nur nicht-patentierte Algorithmen, um mit diesen Schlüsseln Daten zu verschlüsseln, wie etwa RSA, Elgamal, CAST5, Triple-DES (3DES), AES (Rijndael) und Blowfish.\n\nGnuPG unterstützt mit Hauptschlüsseln ein Sicherheitsfeature, das über den OpenPGP-Standard hinausgeht, und daher nicht verlässlich funktioniert, wenn solche geheimen Schlüssel in eine andere OpenPGP-Applikation importiert werden sollen. Der Hauptschlüssel wird nicht für das alltägliche Signieren und Entschlüsseln verwendet, sondern für die Verwaltung der eigenen Schlüsselkomponenten (User-IDs und Unterschlüssel) und die Zertifizierung anderer Schlüssel. Diese Aktionen fallen vergleichsweise selten an, so dass man den Hauptschlüssel besonders sichern kann. Die Vorteile dieser Vorgehensweise sind:\nDer technische Ansatz ist, die geheimen Schlüssel ohne den Hauptschlüssel zu exportieren (nach einem Backup des Hauptschlüssels), dann alle geheimen Schlüssel zu löschen und anschließend nur die Unterschlüssel zu importieren. Leider wird diese GnuPG-Funktion bisher von der GUI nicht unterstützt, so dass man die nötigen Schritte selbst in der Konsole durchführen muss.\n\nMittels eines Web of Trust (Netz des Vertrauens) versucht PGP/GnuPG dem Problem zu begegnen, dass man sich persönlich meist nicht der Echtheit der Schlüssel aller Kommunikationspartner versichern kann. Benutzer können andere Schlüssel mit ihrem eigenen Schlüssel signieren und bestätigen Dritten damit, dass sie sich von der Echtheit des Schlüssels überzeugt haben. Zudem kann man festlegen, wie sehr man den Signierungen der Person vertraut. Dadurch entsteht das beschriebene Vertrauensnetzwerk. Wenn Alice beispielsweise mit ihrer Signatur die Echtheit des Schlüssels von Bob bestätigt hat, kann Cloey der Echtheit des Schlüssels von Bob auch dann trauen, wenn sie selbst sich davon nicht direkt überzeugen konnte, weil sie ihn beispielsweise aus dem Internet bezogen hat. Voraussetzung dafür ist natürlich, dass sie den Schlüssel von Alice kennt und ihr vertraut. Es gibt einige Zertifizierungsstellen (engl. \"certification authority\", CA), die die Echtheit von Schlüsseln beispielsweise durch persönlichen Kontakt mit Überprüfung des Personalausweises feststellen. Kostenlos wird dies zum Beispiel von der Zeitschrift c’t und von CAcert angeboten. Bei diesen Organisationen können Interessenten sich beispielsweise auf Computer-Messen wie der CeBIT persönlich ausweisen und ihre öffentlichen Schlüssel bestätigen lassen.\n\nDas Web of Trust von PGP wurde eingehend von Wissenschaftlern untersucht und detailliert visualisiert. Dabei wurde herausgefunden, dass ein großer Teil der Nutzer einer Teilmenge angehört, die durch gegenseitige Bestätigungen vollständig miteinander verbunden ist, dem sogenannten \"Strong Set\" des Web of Trust. Untersuchungen haben außerdem ergeben, dass die c't Krypto-Kampagne einen deutlichen Beitrag dazu geleistet hat, die Verbindungen zwischen den Teilnehmern zu stärken.\n\nEinen weiteren wichtigen Beitrag zum Web of Trust leistet das Debian-Projekt, das für die Aufnahme von Beiträgen digitale Signaturen erfordert.\n\nWie der ssh-agent (bei OpenSSH) dient der gpg-agent, der seit GnuPG 2.0.x integraler Bestandteil ist, unter anderem dazu, die Passphrase für einen konfigurierbaren Zeitraum im Arbeitsspeicher zu halten, so dass eine erneute Eingabe entfällt; gpg-agent speichert die Passphrase aber nicht nur, sondern übernimmt auch ihre Abfrage vom Anwender (über ein konfigurierbares Hilfsprogramm). Anders als bei OpenSSH ist der gpg-agent allerdings ab Version 2 von GnuPG zwingender Bestandteil; in der künftigen Version 2.1 werden sogar alle Operationen, die private Schlüssel beinhalten, an gpg-agent ausgelagert, was es ermöglicht, die Schlüssel auf einem anderen Rechner zu speichern und nur noch indirekt zu verwenden. Die zweite wichtige Aufgabe von gpg-agent bei GnuPG 2.0.x ist der Zugriff auf Smartcards.\n\nDer Kontakt zu einer Instanz von gpg-agent, die nicht den Standardsocket verwendet (es können mehrere gleichzeitig laufen, was aber meist nicht sinnvoll ist), wird über eine Umgebungsvariable ermöglicht. Ein Beispiel: Über den Befehl codice_1 wird der Mailclient Thunderbird gestartet, und zwar mit der Umgebungsvariable codice_2. Diese enthält eine Zeichenkette wie codice_3 (Pfad des Socket, PID von gpg-agent (wird ignoriert) und Versionsnummer des Protokolls). Hierdurch kann das Mailprogramm Kontakt mit dem jeweiligen gpg-agent aufnehmen und ihm den Umgang mit den privaten Schlüsseln (und deren Passphrase) überlassen.\n\nBei den meisten unixoiden Desktopumgebungen wird der gpg-agent gleich beim Start aktiviert. Dadurch, dass das Startscript der Desktopumgebung diese Umgebungsvariable exportiert, haben alle Programme Zugriff darauf. Wenn gpg-agent nicht läuft (oder nicht gefunden wird), wird es von gpg, gpgsm und gpgconf automatisch gestartet. Zugriff auf einen laufenden gpg-agent kann man in der Konsole über das Programm \"gpg-connect-agent\" bekommen. Mit dem Server kann man dann über das textbasierte Assuan-Protokoll kommunizieren. Man kann gpg-agent daher auch unabhängig von OpenPGP und OpenSSH zur Verwaltung von Passphrasen für eigene Programme nutzen. Über die Konfigurationsdatei gpg-agent.conf wird u. a. festgelegt, über welchen Zeitraum eine Passphrase gecacht wird.\n\nMan kann nicht direkt abfragen, welche Einträge gpg-agent gerade speichert, aber man kann testen, ob ein konkreter Eintrag vorhanden ist (und sich den auch anzeigen lassen). GnuPG legt die Passphrase eines Schlüssels unter dessen Fingerprint ab (ggf. dem des betroffenen Unterschlüssels):\n\nEbenso kann man eine Passphrase setzen, so dass sie nicht vom Anwender abgefragt werden muss (etwa bei automatisierten Prozessen). Dafür gibt es sogar eine eigene Anwendung: gpg-preset-passphrase.\n\nMit gpg-connect-agent kann man auch auf einfache Weise testen, ob gpg-agent überhaupt läuft (bzw. so läuft, dass es unmittelbar verwendet werden kann):\n\nUm zu überprüfen, ob die Anwendung korrekt funktioniert, kann man den Mailbot Adele (adele@gnupp.de) des GNU Privacy Projekt nutzen. Hierzu sendet man eine E-Mail mit dem eigenen öffentlichen Schlüssel als Anhang an Adele und erhält eine mit diesem Schlüssel verschlüsselte Mail zurück, die den öffentlichen Schlüssel von Adele als Textblock in der Mail enthält. Nun kann man diesen Schlüssel in die Schlüsselverwaltung importieren und damit selbst eine verschlüsselte Mail an Adele schreiben. Adeles Antwort enthält den Inhalt der gerade verschlüsselten Nachricht, und dass die Nachricht entschlüsselt werden konnte.\n\nDie Portierung von GnuPG auf Windows wurde zwischen 2001 und 2002 vom Bundesministerium für Wirtschaft und Arbeit (BMWA) und Bundesministerium des Innern (BMI) im Rahmen der Aktion „Sicherheit im Internet“ unterstützt (siehe GNU Privacy Projekt), um eine frei verfügbare Verschlüsselungssoftware für jedermann zur Verfügung zu stellen. Inzwischen ist die Unterstützung ausgelaufen. Auf Grundlage dieses Projekts initiierte das Bundesamt für Sicherheit in der Informationstechnik 2006 Gpg4win. Mit Pretty Easy privacy soll der Einsatz von PGP automatisiert und somit massiv vereinfacht werden.\n\nUm GnuPG in verschiedenen Anwendungskontexten zu benutzen, sind zahlreiche Frontends erstellt worden. Hier können die folgenden Frontend-Typen unterschieden werden:\n\nDaneben gibt es noch weitere Schnittstellen für die Nutzung von GnuPG aus verschiedenen Skriptsprachen wie Perl, PHP oder Python.\n\nAufgrund eines Fehlers bei der Optimierung des Verfahrens der digitalen Signatur in GPG tat sich 2003 eine Sicherheitslücke auf. Diese betraf lediglich das Verfahren zum digitalen Signieren von Nachrichten in den GPG-Versionen 1.02 bis 1.2.3. Angeblich sollen weniger als 1000 solcher Schlüssel auf den Schlüsselservern gelistet worden sein. Von der Verwendung dieses Verfahrens wurde abgeraten und nur wenige Benutzer setzten es ein. Über Schäden wurde nicht öffentlich berichtet. Dieses Verfahren wird ab der Version 1.2.4 nicht mehr angeboten.\n\nZwei weitere Sicherheitslücken wurden Anfang 2006 entdeckt – bei der ersten hätten GPG-Skripte beim Überprüfen von Signaturen Fehler der 2. Art (false negative) ergeben können, bei der zweiten waren nicht-MIME Nachrichten gegenüber der Einspeisung von Daten anfällig, die als von der digitalen Signatur gedeckt aufschienen, dies tatsächlich aber nicht waren. Beide Schwachstellen wurden zum Zeitpunkt ihrer Ankündigung bereits durch neue GPG-Versionen behoben.\n\nIn Version 2 von GPG wurde die Verwaltung der privaten Schlüssel in einen Daemon-Prozess (\"gpg-agent\") ausgelagert. Eine ähnliche Struktur (\"ssh-agent\") gibt es bei OpenSSH, dort allerdings optional. Da der Signaturalgorithmus RSA sowohl von OpenPGP als auch von SSH verwendet wird, sind die jeweiligen Schlüssel prinzipiell austauschbar. Deshalb kann \"gpg-agent\" dafür verwendet werden, mittels eines OpenPGP-Schlüssels (der die normalerweise nicht verwendete Fähigkeit Authentisierung besitzen muss) eine SSH-Verbindung aufzubauen. Dieser Umstand eröffnet zwei nützliche Möglichkeiten:\n\n"}
{"id": "20003", "url": "https://de.wikipedia.org/wiki?curid=20003", "title": "Eingabetaste", "text": "Eingabetaste\n\nDie Eingabetaste, auch Zeilenschalter oder \"Rückführtaste mit Zeilenschaltung\" (englisch ) genannt, ist eine spezielle Taste auf Tastaturen von elektrischen Schreibmaschinen und Computern. Sie hat eine ähnliche Bedeutung wie die Entertaste, so dass ihre Bezeichnungen teilweise synonym verwendet werden.\n\nDie \"Eingabetaste\" befindet sich auf PC-Standardtastaturen im Haupttastaturfeld (im Bild rot markiert). Die \"Entertaste\" gehört zum Ziffernblock (im Bild grün markiert), der erst seit den MF2-Tastaturen auf PC-Tastaturen vorhanden ist. Auf alten XT- und AT-Tastaturen mit 83 und 84 Tasten gab es keine Entertaste.\n\nDas übliche Zeichen, das die Eingabetaste auf einer \"Windows\"-Tastatur kennzeichnet, ist ein nach unten hin abgeknickter Pfeil, welcher nach links zeigt (). Auf den \"Apple\"-Tastaturen wird dagegen für den Zeilenschalter ein geschwungener Pfeil () verwendet. Die Eingabetaste (im Ziffernblock) ist dort mit einem waagerechten Strich und diesen unterbrechend einem auch sogenannten \"Caret\" darunter () oder allgemein (herstellerunabhängig, englisch) mit oder (seltener auch mit beiden Wörtern) beschriftet.\n\nAuf IBM-Terminal-Tastaturen der Baureihen 3270 und 5250 ist die Taste noch mit oder (für \"Datenfreigabetaste\") beschriftet, bei Host-Systemen wird sie verwendet, um die Daten der Eingabemaske nach Ausfüllen aller benötigten Felder an den Host zu senden, also freizugeben. Auf ICL-Großrechnertastaturen gab es dagegen eine rote \"Send-Taste,\" die sehr ergonomisch ganz rechts oben angeordnet und damit auch für Ungeübte blind erreichbar war. \n\nEine früher übliche Beschriftung der Entertaste bzw. der Eingabetaste war (Abkürzung für End of Line).\n\nIhre ursprüngliche Funktion auf Fernschreibern und elektrischen Schreibmaschinen war als \"Wagenrückholtaste\", so wie bei mechanischen Schreibmaschinen der Wagenrückholhebel, der den Walzenwagen wieder an den Zeilenanfang stellte (CR ) und einen Zeilenvorschub (LF \"\") auslöste.\n\nBeide Tasten haben oftmals dieselbe Bedeutung:\n\n"}
{"id": "20127", "url": "https://de.wikipedia.org/wiki?curid=20127", "title": "Escape-Taste", "text": "Escape-Taste\n\nDie Escape-Taste (von ‚fliehen‘, ‚entkommen‘, ‚aussteigen‘) oder Abbruchtaste befindet sich auf herkömmlichen Rechner-Tastaturen in der linken oberen Ecke. Das Betätigen der Taste bewirkt meist den Abbruch einer momentan durchgeführten Aktion des Rechners. Als Erfinder der Taste gilt Bob Bemer.\n\nDie Escape-Taste ist neben der Alt-Gr-Taste und der Enter-Taste im Ziffernblock eine der wenigen Tasten auf deutschen Tastaturen, deren Beschriftung nicht eingedeutscht wurde.\n\nEin Tastatursymbol für die Escape-Taste ist standardisiert in ISO/IEC 9995-7 als Symbol 29 \"„Escape“\" sowie in DIN ISO 7000 \"„Graphische Symbole auf Einrichtungen“\" als Symbol ISO-7000-2029. Dieses Zeichen ist in Unicode im Block \"Verschiedene technische Zeichen\" als U+238B (⎋) enthalten. Dieses Symbol wird regelmäßig in macOS verwendet.\n\nUnter PC-kompatiblem DOS wird die Taste meist mit „Esc“ abgekürzt, vor allem im technischen Kontext aber auch mit der Steuersequenz „^<nowiki>[</nowiki>“ oder dem Zeichen „←“ symbolisiert, das in den unter DOS gebräuchlichen Codepages an der Position des Esc-Steuerzeichens liegt.\n\n"}
{"id": "20282", "url": "https://de.wikipedia.org/wiki?curid=20282", "title": "Pufferüberlauf", "text": "Pufferüberlauf\n\nPufferüberläufe () oder – im Besonderen – auch Stapelüberläufe (englisch \"‚‘\") genannt, gehören zu den häufigsten Sicherheitslücken in aktueller Software, die sich u. a. über das Internet ausnutzen lassen können. Im Wesentlichen werden bei einem Pufferüberlauf durch Fehler im Programm zu große Datenmengen in einen dafür zu kleinen reservierten Speicherbereich – den Puffer oder Stapel – geschrieben, wodurch nach dem Ziel-Speicherbereich liegende Speicherstellen überschrieben werden.\n\nDreht es sich nicht um einen ganzen Datenblock, sondern um eine Zieladresse eines einzelnen Datensatzes, wird auch von einem \"‚‘\" (vom englischen \"\", für „Zeiger“) gesprochen, der anzeigt, wo der Datensatz im Puffer hingeschrieben werden soll.\n\nEin Pufferüberlauf kann zum Absturz des betreffenden Programms, zur Verfälschung von Daten oder zur Beschädigung von Datenstrukturen der Laufzeitumgebung des Programms führen. Durch Letzteres kann die Rücksprungadresse eines Unterprogramms mit beliebigen Daten überschrieben werden, wodurch ein Angreifer durch Übermittlung von beliebigem Maschinencode beliebige Befehle mit den Privilegien des für den Pufferüberlauf anfälligen Prozesses ausführen kann. Dieser Code hat in der Regel das Ziel, dem Angreifer einen komfortableren Zugang zum System zu verschaffen, damit dieser das System dann für seine Zwecke verwenden kann. Pufferüberläufe in verbreiteter Server- und Clientsoftware werden auch von Internetwürmern ausgenutzt.\n\nBesonders begehrtes Ziel ist bei Unix-Systemen der Root-Zugang, der dem Angreifer sämtliche Zugriffsrechte verleiht. Das bedeutet aber nicht, wie oft missverstanden, dass ein Pufferüberlauf, der „nur“ zu den Privilegien eines „normalen“ Benutzers führt, ungefährlich ist. Das Erreichen des begehrten Root-Zugangs ist oft viel einfacher, wenn man bereits Benutzerrechte hat (Rechteerweiterung, engl. \"privilege escalation\").\n\nAngriffe mit Pufferüberläufen sind ein wichtiges Thema in der Computersicherheit und Netzwerksicherheit. Sie können nicht nur über jegliche Art von Netzwerken, sondern auch lokal auf dem System versucht werden. Behoben werden sie in der Regel nur durch kurzfristig gelieferte Fehlerkorrekturen (Patches) der Hersteller.\n\nNeben Nachlässigkeiten bei der Programmierung werden Pufferüberläufe vor allem durch auf der Von-Neumann-Architektur basierende Computersysteme ermöglicht, gemäß der Daten und Programm im gleichen Speicher liegen. Durch diese Hardwarenähe sind sie auch nur unter assemblierten oder kompilierten Programmiersprachen ein Problem. Interpretierte Sprachen sind, abgesehen von Fehlern im Interpreter, in der Regel nicht anfällig, da die Speicherbereiche für Daten immer unter vollständiger Kontrolle des Interpreters sind.\n\nMit dem Protected Mode, der beim 80286 eingeführt wurde, lässt sich durch die Segmentierung des linearen Speichers der Programm-, Daten- und Stapelspeicher physikalisch voneinander trennen. Der Zugriffsschutz erfolgt über die Speicherverwaltungseinheit der CPU. Das Betriebssystem muss nur sicherstellen, dass gleichzeitig nicht mehr Speicher zur Verfügung gestellt wird, als der lineare Adressraum groß ist. Als einziges weitverbreitetes Betriebssystem nutzte OS/2 die Speichersegmentierung.\n\nDie wesentlichste Ursache für Pufferüberläufe ist die Verwendung von Programmiersprachen, die nicht die Möglichkeit bieten, Grenzen von Speicherbereichen automatisch zu überwachen, um eine Bereichsüberschreitung von Speicherbereichen zu verhindern. Dazu gehört besonders die Sprache C, die das Hauptgewicht auf Performance (und ursprünglich Einfachheit des Compilers) legt und auf eine Überwachung verzichtet, sowie die C-Weiterentwicklung C++. Hier ist ein Programmierer teilweise gezwungen, den entsprechenden Code von Hand zu generieren, wobei oft absichtlich oder aus Nachlässigkeit darauf verzichtet wird. Die Überprüfung ist häufig auch fehlerhaft implementiert, da während der Programmtests diese Programmteile meist nicht oder ungenügend getestet werden. Daneben stellen der (im Fall von C++) komplexe Sprachumfang und die Standardbibliothek sehr viele fehleranfällige Konstrukte zur Verfügung, zu denen es in vielen Fällen kaum eine Alternative gibt.\n\nDie häufig verwendete Programmiersprache C++ bietet nur eingeschränkte Möglichkeiten zur automatischen Überprüfung von Feldgrenzen. Als Weiterentwicklung der Programmiersprache C übernimmt sie sämtliche Eigenschaften von C, wobei sich aber das Risiko von Pufferüberläufen bei Benutzung von modernen Sprachmitteln (u. a. automatische Speicherverwaltung) weitestgehend vermeiden lässt. Aus Gewohnheit, Kompatibilitätsgründen zu vorhandenem C-Code, Systemaufrufen in C-Konvention sowie aus Performancegründen wird von diesen Möglichkeiten aber nicht immer Gebrauch gemacht. Laufzeitüberprüfungen sind im Gegensatz zu Sprachen wie beispielsweise Pascal oder Ada nicht Bestandteil der Sprache, lassen sich aber in einigen Anwendungsfällen (z. B. mit Smart Pointern) nachrüsten.\n\nDa die meisten Programmiersprachen auch Standardbibliotheken definieren, bedeutet die Wahl einer Sprache meist auch die Verwendung der entsprechenden Standardbibliotheken. Im Fall von C und C++ enthält die Standardbibliothek eine Anzahl von gefährlichen Funktionen, die zum Teil gar keine sichere Verwendung zulassen und zu denen zum Teil keine Alternativen bestehen.\n\nAuf Programmiersprachenebene kann die Gefahr von Pufferüberläufen durch die Verwendung von Programmiersprachen, die konzeptionell sicherer als C++ oder C sind, verringert oder ausgeschlossen werden. Ein sehr viel geringeres Risiko besteht zum Beispiel in Programmiersprachen der Pascal-Familie Modula, Object Pascal oder Ada.\n\nFast ausgeschlossen sind Pufferüberläufe beispielsweise in der Programmiersprache Java, da die Ausführung im Bytecode überwacht wird. Aber auch in Java gibt es einerseits Pufferüberläufe, deren Ursache im Laufzeitsystem liegt und von denen mehrere JRE-Versionen betroffen sind. Andererseits wirft die Java-Laufzeitumgebung einen codice_1, wenn durch eine fehlerhafte Endlos-Rekursion der Methoden-Aufruf-Stapel überläuft. Hierbei handelt es sich um einen logischen Programmierfehler des Anwendungsprogrammierers, nicht des Laufzeitsystems.\n\nWeitere Eigentümlichkeiten von C und C++ sowie der am häufigsten eingesetzten Prozessoren machen das Auftreten von Pufferüberläufen wahrscheinlich. Die Programme in diesen Sprachen bestehen zum Teil aus Unterprogrammen. Diese besitzen lokale Variablen.\n\nBei modernen Prozessoren ist es üblich, die Rücksprungadresse eines Unterprogramms und dessen lokale Variablen auf einen als Stack bezeichneten Bereich zu legen. Dabei werden beim Unterprogrammaufruf \"zunächst\" die Rücksprungadresse und \"danach\" die lokalen Variablen auf den Stack gelegt. Bei modernen Prozessoren wie dem Intel Pentium wird der Stack durch eingebaute Prozessorbefehle verwaltet und wächst zwingend \"nach unten\". Werden Felder oder Zeichenketten in den lokalen Variablen verwendet, werden diese meist \"nach oben\" beschrieben. Wird die Feldgrenze nicht geprüft, kann man damit durch Überschreiten des Feldes die Rückkehradresse auf dem Stack erreichen und gegebenenfalls absichtlich modifizieren.\n\nDas folgende Programmstück in C, das in ähnlicher Form oft verwendet wird, zeigt einen solchen Pufferüberlauf:\nvoid input_line()\n\nBei Prozessoren, die den Stack nach unten beschreiben, sieht dieser vor dem Aufruf von \"gets\" (Funktion der Standard-Bibliothek von C) so aus (wenn man vom eventuell vorhandenen Base Pointer absieht):\n\n\"gets\" liest eine Zeile von der Eingabe und schreibt die Zeichen ab \"line[0]\" in den Stack. Es überprüft die Länge der Zeile nicht. Gemäß der Semantik von C erhält \"gets\" nur die Speicheradresse als Pointer, jedoch keinerlei Information über die verfügbare Länge. Wenn man jetzt 1004 Zeichen eingibt, überschreiben die letzten 4 Bytes die Rücksprungadresse (unter der Annahme, dass eine Adresse hier 4 Bytes groß ist), die man auf ein Programmstück innerhalb des Stack richten kann. In den ersten 1000 Zeichen kann man gegebenenfalls ein geeignetes Programm eingeben.\n\nFalls das Programm höhere Privilegien besitzt als der Benutzer, kann dieser unter Ausnutzung des Pufferüberlaufs durch eine spezielle Eingabe diese Privilegien erlangen.\n\nEine sehr nachhaltige Gegenmaßnahme besteht in der Verwendung typsicherer Programmiersprachen und -werkzeuge, wie zum Beispiel Java oder C#, bei denen die Einhaltung von zugewiesenen Speicherbereichen gegebenenfalls schon beim Übersetzen in Maschinensprache mit dem Compiler kontrolliert, aber spätestens zur Laufzeit mit entsprechendem Programmcode überwacht wird. Es ist hierbei unerlässlich, dass das Verändern von Zeigervariablen nur nach strengen, einschränkenden Regeln erfolgen darf, und es ist in diesem Zusammenhang auch hilfreich, wenn ausschließlich das Laufzeitsystem automatische Speicherbereinigungen durchführt.\n\nBei der Erstellung von Programmen muss also auf die Überprüfung aller Feldgrenzen geachtet werden. Dies liegt bei veralteten, nicht-typsicheren Programmiersprachen in der Verantwortung des Programmierers. Allerdings sollte vorzugsweise die Verwendung von Programmiersprachen, die automatisch Feldgrenzen überwachen, in Erwägung gezogen werden, was jedoch nicht immer ohne weiteres möglich ist. Bei Verwendung von C++ sollte die Verwendung von Feldern im C-Stil so weit wie möglich vermieden werden.\nvoid input_line()\n\nSpezielle Überprüfungswerkzeuge erlauben die Analyse des Codes und entdecken mögliche Schwachstellen. Allerdings kann der Code zur Feldgrenzenüberprüfung fehlerhaft sein, was oft nicht getestet wird.\n\nIn C und C++ steht eine sehr große Auswahl bestehender Programme zur Verfügung. Moderne Compiler wie neue Versionen des \"GNU C-Compilers\" erlauben die Aktivierung von Überprüfungscode-Erzeugung bei der Übersetzung.\n\nSprachen wie C erlauben aufgrund ihres Designs nicht immer die Überprüfung der Feldgrenzen (Beispiel: \"gets\"). Die Compiler müssen andere Wege gehen: Sie fügen zwischen der Rücksprungadresse und den lokalen Variablen Platz für eine Zufallszahl (auch „Canary“ genannt) ein. Beim Programmstart wird diese Zahl ermittelt, wobei sie jedes Mal unterschiedliche Werte annimmt. Bei jedem Unterprogrammaufruf wird die Zufallszahl in den dafür vorgesehenen Bereich geschrieben. Der erforderliche Code wird vom Compiler automatisch generiert. Vor dem Verlassen des Programms über die Rücksprungadresse fügt der Compiler Code ein, der die Zufallszahl auf den vorgesehenen Wert überprüft. Wurde sie geändert, ist auch der Rücksprungadresse nicht zu trauen. Das Programm wird mit einer entsprechenden Meldung abgebrochen.\n\nDaneben kann man manche Compiler auch veranlassen, beim Unterprogrammaufruf eine \"Kopie\" der Rücksprungadresse unterhalb der lokalen Felder zu erzeugen. Diese Kopie wird beim Rücksprung verwendet, die Ausnutzung von Pufferüberläufen ist dann wesentlich erschwert:\n\nFür die GNU Compiler Collection existieren beispielsweise zwei verbreitete Erweiterungen, die Maßnahmen wie die oben beschriebenen implementieren:\n\nEin \"Heap-Überlauf\" ist ein Pufferüberlauf, der auf dem Heap stattfindet. Speicher auf dem Heap wird zugewiesen, wenn Programme dynamischen Speicher anfordern, etwa über malloc() oder den \"new\"-Operator in C++. Werden in einen Puffer auf dem Heap Daten ohne Überprüfung der Länge geschrieben und ist die Datenmenge größer als die Größe des Puffers, so wird über das Ende des Puffers hinausgeschrieben und es kommt zu einem Speicherüberlauf.\n\nDurch \"Heap-Überläufe\" kann durch Überschreiben von Zeigern auf Funktionen beliebiger Code auf dem Rechner ausgeführt werden, insbesondere wenn der Heap ausführbar ist. FreeBSD hat beispielsweise einen Heap-Schutz, hier ist das nicht möglich. Sie können nur in Programmiersprachen auftreten, in denen bei Pufferzugriffen keine Längenüberprüfung stattfindet. C, C++ oder Assembler sind anfällig, Java oder Perl sind es nicht.\n\nz. B. wurde am 23. Juni 2015 von Adobe bekannt gegeben, dass durch solch einen Pufferüberlauf beliebiger Schadcode auf Systemen ausgeführt werden könne und so die Kontrolle über das System übernommen werden könnte, auf denen der Flash Player installiert sei.\n\n\nchar * copy_string(const char *s)\n\nDa strcpy() die Größen von Quelle und Ziel nicht überprüft, sondern als Quelle einen null-terminierten ('\\0') Speicherbereich erwartet, ist auch die folgende Variante unsicher (sie wird allerdings nicht über \"buf\" hinausschießen, sondern ggf. über das Ende des \"s\" zugewiesenen Speicherbereichs).\nchar * buf;\n\nbuf = malloc(1 + strlen(s)); // Plus 1 wegen des terminierenden NUL-Zeichens\nif (buf)\nDer strncpy-Befehl dagegen kopiert maximal n Zeichen von der Quelle zum Ziel und funktioniert somit, wenn s nullterminiert oder größer als BUFSIZE ist.\nchar *buf;\n\nif ((buf = malloc(BUFSIZE)) != NULL) { // Überprüfung des Zeigers.\nreturn buf;\nEinige Betriebssysteme, z. B. OpenBSD, bieten die Funktion \"strlcpy\" an, die ihrerseits sicherstellt, dass der Zielstring nullterminiert wird und das Erkennen eines abgeschnittenen Zielstrings vereinfacht.\n\n\n\n"}
{"id": "20311", "url": "https://de.wikipedia.org/wiki?curid=20311", "title": "Computer-integrated manufacturing", "text": "Computer-integrated manufacturing\n\nCIM, von engl. computer-integrated manufacturing, dt. rechnergestützte Produktion bzw. rechnerintegrierte Fertigung ist ein Sammelbegriff für verschiedene Tätigkeiten, die in einem Unternehmen durch den Computer unterstützt werden, und daher auch unter „CAx“ zusammengefasst (\"computer-aided …\" oder \"computer-assisted …\") sind.\n\nDie Bestandteile von CIM sind:\n\nDie Technik, die sich hinter den Kürzeln CAD und CAM verbirgt, ist schon seit etwa 1965 bekannt. Man versteht darunter rechnerunterstütztes Zeichnen und Konstruieren von Produkten (\"CAD\") und das anschließende Programmieren der Maschinen zur Produktherstellung (\"CAM\"). Die Integration kann so weit gehen, dass CAD-Daten automatisch in ein CAM-System übernommen werden.\n\nIm Jahre 1973 stellte Joseph Harrington das Konzept des Computer Integrated Manufacturing vor. Damit wollte er die Bedeutung von Informationen in der Produktion sowie die Synergiepotentiale bei der Verknüpfung der Insellösungen hervorheben. Er sprach von \"pieces of puzzles\", damit meinte er die Insellösungen, wie CAD, NC, CAM usw., welche in einem Betrieb alleine, ohne jede EDV-Anbindung untereinander, angewandt wurden.\n\nNeben der großen Anzahl an Insellösungen erschwerten sicherlich auch die Anzahl an publizierten CIM-Konzepten eine Umsetzung. Eine visuelle Übersicht (Bilder der Konzepte als auch ein Zeitstrahl über die Zeitpunkte der Veröffentlichung) über die existierenden Konzepte geben die Autoren Meudt, Pohl und Metternich.\n\n\"CIM is the integration of total manufacturing enterprise by using integrated systems and data communication coupled with new managerial philosophies that improve organizational and personnel efficiency.\" (Auf deutsch etwa: \"CIM ist die Integration des gesamten Fertigungsunternehmens durch integrierte System- und Datenkommunikation gepaart mit einer neuen Managementphilosophie zur Verbesserung der organisatorischen und personellen Leistungsfähigkeit\").\n\nAnfang der 80er wurde das \"CIM Wheel\" (CIM-Rad) von der CASA/SME (\"Computer and Automated Systems Association\" of the \"Society of Manufacturing Engineers\" of the United States of America) entwickelt. Hauptidee war die ganzheitliche Betrachtung des Unternehmens ausgehend vom CIM. Im Zentrum des CIM-Wheel steht die Integrierte Systemarchitektur (integrated system architecture) mit einer gemeinsamen Datenbasis (common data) und der Informationsverwaltung und -kommunikation (information ressource management & communication).\n\nAuf der zweiten Ebene wurden Unternehmensfunktionen aus den Bereichen Fabrikautomation und Produktionsplanung und -steuerung über die Bestandteile der Integrierten Systemarchitektur miteinander verknüpft. Neu an diesem Konzept war, dass darüber hinausgehende administrative Aufgaben auf einer dritten Ebene berücksichtigt wurden. Dabei handelte es sich um Betriebsführung, Personalwesen, Marketing, Strategische Planung und Finanzwirtschaft. Die Weiterentwicklung des CIM Wheel ist das \"Manufacturing Enterprise Wheel\". Im Zentrum dieses Konzeptes steht der Kunde.\n\nDefinition laut AWF (Ausschuss für wirtschaftliche Fertigung, 1985):\n\"CIM beschreibt den integrierten EDV-Einsatz in allen mit der Produktion zusammenhängenden Betriebsbereichen. Es umfasst das informationstechnische Zusammenwirken zwischen CAD, CAP, CAM, CAQ und PPS. Hierbei soll die Integration der technischen und organisatorischen Funktionen zur Produkterstellung erreicht werden. Dies bedingt die gemeinsame Nutzung aller Daten eines EDV-Systems, auch Datenbasis genannt.\"\n\nDas AWF-Konzept basiert auf dem von August-Wilhelm Scheer entwickelten \"Y-CIM-Modell\".\n\nWährend der CIM-Hochphase hatte jeder Hersteller wie DEC, HP, IBM oder Siemens eine eigene Definition. Bei Siemens beispielsweise sprach man von \"CAI\" (\"computer-aided industry\"). In einer umfassenden Literaturrecherche zum Thema CIM konnte ein Überblick gegeben werden über die unterschiedlichen CIM Konzepte. Insgesamt wurden 37 verschiedene Konzepte gefunden, wobei 20 Konzepte aus Deutschland und 14 Konzepte aus den Vereinigten Staaten stammen.\n\nBesonders in Japan wurde in den 1980er Jahren in CIM-Projekten auch die vollständige Verdrängung des Menschen aus dem teilweise hochgefährlichen Produktionsbetrieb (die sogenannte \"Menschenleere Fabrik\") versucht, wobei offenbar wegen der Komplexität der modernen Industrie nur einfache, eher theoretische Prototypen entstanden.\n\nVolker Spanier, Leiter Factory Automation von Epson, äußerte in einem Interview mit der Zeitschrift \"Produktion\", dass \"Industrie 4.0\" nur eine Umschreibung für den Begriff des \"Computer integrated Manufacturing\" sei.\n\n"}
{"id": "20312", "url": "https://de.wikipedia.org/wiki?curid=20312", "title": "Computer-aided planning", "text": "Computer-aided planning\n\nCAP auch CAPP (von engl. \"computer-aided process planning\", dt. \"computergestützte Arbeitsplanung\") baut auf konventionell oder mit CAD erstellten Konstruktionsdaten auf, um Daten für die Teilefertigungs- und Montageanweisungen zu erzeugen. CAP ist als Bestandteil der computerintegrierten Produktion (Computer-Integrated Manufacturing, CIM) in vielen Systemen zum Enterprise Resource Planning (ERP) implementiert.\n\n"}
{"id": "20315", "url": "https://de.wikipedia.org/wiki?curid=20315", "title": "Computer-aided quality", "text": "Computer-aided quality\n\nCAQ (von engl. \"computer-aided quality assurance\", zu Deutsch rechnerunterstützte Qualitätssicherung) ist die EDV-unterstützte Festlegung einer Qualitätspolitik und deren Ziele im Unternehmen und ist ein Element des Qualitätsmanagements. Es begleitet den gesamten Produktionsprozess, mit allen operativen und dienstleistenden Bereichen. CAQ-Systeme analysieren (Prozessdatenanalyse, PDA), dokumentieren und archivieren qualitätsrelevante Daten zu Fertigungsprozessen. CAQ umfasst computergestützte Maßnahmen zur Planung und Durchführung der Qualitätssicherung.\n\nBeispiele sind die Untersuchung der Radreifen eines ICE auf Haarrisse oder die automatische Prüfung integrierter Schaltkreise und Leiterplatten (siehe hierzu auch Starrnadeladapter). CAQ umfasst meist neben der Messwerterfassung auch eine Digitalisierung, Übertragung und langfristige Speicherung der Daten.\n\nDie Analyse, Dokumentation und Archivierung qualitätsrelevanter Daten ist für Unternehmen zur Minimierung der Risiken nach dem Produkthaftungsgesetz von sehr hoher Bedeutung. Dazu werden Daten aus messenden, attributiven und visuellen Prüfungen vom CAQ-System mit statistischen Methoden ausgewertet. Die CAQ-Systeme zeigen auch die Prozessfähigkeit von Produktionsprozessen an (siehe auch Statistische Prozesslenkung, kurz SPC), d. h. wie stabil und wie gut reproduzierbar Produktionsprozesse sind. Die Verknüpfung solcher qualitätsrelevanter Daten mit der Reklamationsbearbeitung kann zu einer deutlichen Kostenreduzierung führen.\n\nCAQ ist Bestandteil der computerintegrierten Produktion (CIM) und kann über die QDX zwischen Unternehmen ausgetauscht werden.\n\nDisziplinen von CAQ sind unter anderem:\n\n"}
{"id": "20316", "url": "https://de.wikipedia.org/wiki?curid=20316", "title": "Computer-aided manufacturing", "text": "Computer-aided manufacturing\n\nComputer-aided manufacturing (CAM, dt. \"rechnerunterstützte Fertigung\") bezeichnet die Verwendung einer von der CNC-Maschine unabhängigen Software zur Erstellung des NC-Codes. Im Unterschied zur Erstellung des NC-Codes in der Werkstatt (WOP), wird mit dem CAM-System das NC-Programm bereits in der Arbeitsvorbereitung erstellt. CAM ist ein wesentlicher Bestandteil der computerintegrierten Produktion CIM (Computer-integrated manufacturing).\n\nAn Stelle herkömmlicher Zeichnungen soll das NC-Programm für das zu erstellende Teil direkt auf Basis der am Computer hergestellten CAD-Daten erstellt werden. Die notwendigen Instruktionen für die CNC-Maschine und die Anweisungen für die Bediener sollen nicht mehr ausgedruckt werden müssen, sondern in elektronischer Form an die Fertigung übergeben werden.\n\nWeil keine Geometriedaten aus einer Zeichnung abgeschrieben werden müssen, können dabei auch keine Fehler gemacht werden. Während des Arbeitens am CAM-System bleibt die CNC-Maschine produktiv. Beim CAM-Programmieren kann man im Büro sitzen, statt in der Werkstatt zu stehen. Unabhängig davon, für welche CNC-Maschine ein Programm erstellt wird, ist die dafür verwendete Software immer gleich zu bedienen. Im CAM-System können eigene Funktionen und Erfahrungen hinterlegt werden, damit wiederkehrende Aufgaben schneller und sicherer zu lösen sind. Wenn im NC-Programm doch ein Fehler ist, muss man NICHT zu dessen Behebung zurück in die AV ans CAM-System. Hierfür kann ein Shopfloor Editor verwendet werden, welcher sicherstellt, dass gerade die Produktion in der 2. und 3. Schicht weiter laufen kann. Das CAM-System kann außer mit dem CAD auch mit anderen Anwendungen Informationen austauschen (z. B. mit der Werkzeugverwaltung und dem PPS-System). Mit einem CAM-System können NC-Programme für Freiformoberflächen erstellt werden (z. B. für eine Motorhaube). Das erstellte NC-Programm kann mit dem CAM-System schnell und im Voraus auf Kollisionen und andere Fehler überprüft werden. Die Liste der benötigten Werkzeuge kann im Voraus erstellt werden. Beim Einsatz einer Werkzeugverwaltung können die Sollwerte direkt an das Voreinstellgerät zur Vermessung der Werkzeuge übergeben werden. Nachträgliche Änderungen in der Konstruktion können schnell erkannt und aus dem CAD übernommen werden. Mit einem CAM System können auch Arbeitspläne erstellt werden, die, zusammen mit Fotos oder Filmsequenzen, Werkzeugen, Zykluszeiten des Bauteils und Spannsituationen mit in die Fertigung gegeben werden, was das Einrichten und die Erledigung des Auftrages vereinfacht und beschleunigt.\n\nDas CAM-System benötigt für jede CNC-Maschine einen angepassten Postprozessor. Wenn keine CAD-Daten vorhanden sind, muss man die Geometrie des gewünschten Fertigteils selber erfassen. Die Zerspanungsleistung der CNC-Maschine wird durch NC-Programme, die mit Hilfe eines CAM-Systems erstellt worden sind, deutlich erhöht, wenn diese den optimalen Werkzeugweg berechnen, Luftschnitte vermeiden und die Werkzeuggeometrie optimal an die vorgesehene Bearbeitung anpassen (z. B. wellenförmiges Schruppen). Durch neue, auf die Maschinenumgebung bezogene Simulationen ist ein großer Sicherheitsfaktor gegeben. Die Arbeit in der Werkstatt ist weniger anspruchsvoll, dadurch auch weniger interessant. Bei älteren CNC-Maschinen ohne Netzwerkanschluss müssen die am Computer erstellten NC-Programme mit einer speziellen DNC-Software an die Maschinensteuerung übergeben werden.\n\nEin CAM-System kostet Geld und der Umgang mit ihm muss geschult werden, es erspart im Betrieb jedoch deutlich an Zeit, da der Maschinenbediener nicht im Maschinenlärm Satz für Satz eintippt, was Flüchtigkeitsfehler mit sich bringt. Der wohl größte Nachteil ist, dass CAM-Systeme nur den neutralen Code vor dem Postprozessor simulieren, nicht aber den Original-NC-Code nach dem Postprozessorlauf. Um also wirklich sicher zu sein, sollte man die Simulation in einem speziellen Simulations-System ablaufen lassen.\n\nDer Gewinn aus einem CAM-System schließt auch die grafische Darstellung ein, am Computerbildschirm oder auch für mehrere Betrachter in Projektion. Die Anzahl Fräs- oder Drehschnittdurchgänge kann deutlich gemacht werden und die unterschiedlichen Bewegungen mit bestimmter Richtung und Geschwindigkeit oder im Eilgang sind in verschiedenen Farben klar zu erkennen.\n\nIm CAM-System werden die Geometriedaten für Rohteil, Fertigteil und Aufspannvorrichtung aus dem CAD eingelesen. Gelegentlich ist es erforderlich, dass die Geometrie verändert oder neue Geometrie (Modelle) erstellt werden müssen. Der Werkstoff und die CNC-Maschine werden aus Tabellen ausgewählt, damit das CAM-System die Randbedingungen kennt und geeignete Verfahrbewegungen und Schnittwerte vorschlagen kann.\n\nDie zur Bearbeitung erforderlichen Werkzeuge werden aus der Werkzeugverwaltung übernommen oder im CAM erfasst.\n\nDie zur Bearbeitung des Teils erforderlichen Operationen werden nacheinander festgelegt. Dabei wird in der Grafik die zu bearbeitende Geometrie und das gewünschte Werkzeug ausgewählt. Die vom CAM-System vorgeschlagenen Parameter für die Bearbeitung werden bei Bedarf manuell angepasst. Die bereits definierten Operationen können zur Kontrolle simuliert werden.\n\nWenn alle Operationen festgelegt sind, wird das maschinenspezifische NC-Programm mit dem Postprozessor generiert und gespeichert. Die Anweisungen für die Bediener und die Werkzeugliste werden ausgedruckt oder gespeichert. Die im CAM definierten Operationen werden zusammen mit allen Parametern als „Quellcode“ im CAM-internen Format gespeichert.\n\nNeben der jeweiligen auftragsbezogenen Bearbeitung von Daten ist die Archivierung und Standardisierung ein Aufgabenfeld des CAM.\n\n\n"}
{"id": "20722", "url": "https://de.wikipedia.org/wiki?curid=20722", "title": "PostgreSQL", "text": "PostgreSQL\n\nPostgreSQL ( []), oft kurz Postgres genannt, ist ein freies, objektrelationales Datenbankmanagementsystem (ORDBMS). Seine Entwicklung begann in den 1980er Jahren, seit 1997 wird die Software von einer Open-Source-Community weiterentwickelt.\n\nPostgreSQL ist weitgehend konform mit dem SQL-Standard SQL:2011, d. h. der Großteil der Funktionen ist verfügbar und verhält sich wie definiert. PostgreSQL ist vollständig ACID-konform (inklusive der Data Definition Language), und unterstützt erweiterbare Datentypen, Operatoren, Funktionen und Aggregate. Obwohl sich die Entwicklergemeinde sehr eng an den SQL-Standard hält, gibt es dennoch eine Reihe von PostgreSQL-spezifischen Funktionalitäten, wobei in der Dokumentation bei jeder Eigenschaft ein Hinweis erfolgt, ob dies dem SQL-Standard entspricht, oder ob es sich um eine spezifische Erweiterung handelt. Darüber hinaus verfügt PostgreSQL über ein umfangreiches Angebot an Erweiterungen durch Dritthersteller, wie z. B. PostGIS zur Verwaltung von Geodaten.\n\nPostgreSQL ist in den meisten Linux-Distributionen enthalten. Apple liefert von der Version Mac OS X Lion (10.7) an PostgreSQL als Standarddatenbank aus.\n\nPostgreSQL, zuvor bekannt unter dem Namen Postgres, später Postgres95, entstand aus einer Datenbankentwicklung der University of California in Berkeley. Den Anfang stellte dabei das Ingres-Projekt dar, wobei der Hauptverantwortliche Michael Stonebraker 1982 die Universität verließ, um Ingres kommerziell zu vertreiben. Nachdem Stonebraker 1985 wieder zur Universität zurückgekehrt war, startete er das Post-Ingres-Projekt, um die Probleme der damaligen Datenbankmanagementsysteme auszumerzen. Die Codebasis des ersten Postgres ist dabei eine andere als die von Ingres.\n\n1989 wurde die erste Version von Postgres fertiggestellt.\n\nIm Jahre 1994 wurde Postgres von den Studenten Andrew Yu und Jolly Chen um einen SQL-Interpreter erweitert und die Software als Open Source unter dem Namen \"Postgres95\" freigegeben. Der Postgres95-Code entsprach dem ANSI-C-Standard und wurde um 25 % verkleinert, weiterhin wurden Leistung und Zuverlässigkeit verbessert. Postgres95, Version 1.0.x war im Wisconsin Benchmark 30 bis 50 Prozent schneller als Postgres, Version 4.2.\n\nIm Zuge der Entwicklung des World Wide Web verstärkte sich das Interesse an Datenbanken. 1996 erhielt Postgres seinen neuen Namen PostgreSQL. Die erste unter diesem Namen veröffentlichte Version ist 6.0. Seither wurde PostgreSQL fortlaufend weiterentwickelt.\n\n\nEine Reihe von durch den Benutzer zu installierenden Zusatzmodulen steht zur Verfügung, darunter mit GiST (Generalized Search Tree) eine universelle Schnittstelle, um Such- und Sortierverfahren in weiten Grenzen selbst definieren zu können. Eine Anwendung davon ist PostGIS, das geografische Objekte und Datenstrukturen verwalten und damit als Datenbank für Geoinformationssysteme (GIS) dienen kann. Eine andere GiST-Anwendung ist OpenFTS (Open Source Full Text Search), das Volltextsuche in DB-Objekten erlaubt.\n\nEine Reihe von Erweiterungen beschäftigt sich mit Clustering und Replizierung, dem parallelen Einsatz und Abgleich verteilter DB-Server.\n\nIn diesem Abschnitt werden einige Beschränkungen der aktuellen Version beschrieben. Wenn zukünftige Releases bereitgestellt werden, könnten einige davon weggefallen sein.\n\nDie hier beschriebenen Beschränkungen betreffen die Versionen 8.4 bis 9.5:\n\n\nJede Nebenversionsnummer wird von der PostgreSQL-Entwickler-Community fünf Jahre lang weitergepflegt. Revisionen enthalten Leistungsverbesserungen und Fehlerbereinigungen, aber niemals neue Funktionen. Die älteste derzeit noch gepflegte Versionslinie ist 9.3, die im September 2013 erschien.\n\nBei einer Software-Aktualisierung müssen bestehende Datenbanken vorher gesichert, danach in der neuen PostgreSQL-Version neu angelegt und die Daten aus der vorherigen Sicherung eingespielt werden. Dies ist dann erforderlich, wenn sich die zweite oder erste Stelle der Versionsnummer geändert hat, nicht jedoch bei Änderungen der dritten Stelle.\n\nAlle heute unterstützten Versionen liefern ein Werkzeug namens pg_upgrade mit, welches das Datenverzeichnis ohne das vorher notwendige Sichern und Wiedereinspielen der Datenbank aktualisieren kann.\n\nVon der Version 8.0 an unterstützt PostgreSQL Windows. Eine weitere Neuerung ist die Unterstützung von Sicherungspunkten (Savepoints). Mit diesen ist es möglich, eine Transaktion an einem vorher gespeicherten Sicherungspunkt fortzusetzen, falls diese durch einen Fehler abgebrochen wurde.\n\nVersion 8.3 wurde im Februar 2008 veröffentlicht und brachte eine Reihe von Leistungsverbesserungen sowie Funktionserweiterungen, darunter:\n\nVersion 9.0 wurde im September 2010 veröffentlicht und brachte Funktionen für Replikation und Hot-Standby und Verbesserungen bei Sicherheit, Monitoring und speziellen Datentypen.\n\nVersion 9.1 wurde am 12. September 2011 veröffentlicht. Diese Version brachte eine Verbesserung der Hochverfügbarkeit bei Betrieb von mehreren Servern mit Hilfe synchroner Replikation, Zugriff auf fremde Datenquellen, spaltenweise linguistisch korrekte Sortierbarkeit, Einbindung von Erweiterungen sowie eine Reihe weiterer Verbesserungen.\n\nVersion 9.2 wurde am 10. September 2012 veröffentlicht und enthält in erster Linie neue Funktionalitäten zur Verbesserung der Performance.\n\nVersion 9.3 wurde am 9. September 2013 veröffentlicht, wesentliche Verbesserungen sind im Bereich der Unterstützung von JSON, bei materialisierten Views und beschreibbaren Views.\n\nVersion 9.4 wurde am 18. Dezember 2014 veröffentlicht. Beschleunigte Verarbeitung von JSON-Daten durch das binäre Format JSONB.\n\nVersion 9.5 wurde am 7. Januar 2016 veröffentlicht. Row-level security control, Block Range Indexes (BRIN), IMPORT FOREIGN SCHEMA, pg_rewind hinzugefügt.\n\nVersion 9.6 wurde am 29. September 2016 veröffentlicht. Die wichtigsten Änderungen betreffen vertikale („scale-up“) als auch horizontale („scale-out“) Skalierung, parallelisierte Abfragen, Phrasensuche und Verbesserungen bei synchroner Replikation, sowie generelle verbesserte Performance und Benutzerfreundlichkeit.\n\nVersion 10 wurde am 5. Oktober 2017 veröffentlicht. Die wichtigsten Änderungen betreffen deklarative Partitionierung, logische Replikation, verbesserte Abfrage-Parallelisierung und bessere Password-Hashes.\n\nVersion 11 wurde am 18. Oktober 2018 veröffentlicht.\n\nPostgreSQL selbst läuft als Hintergrundprozess und kann auf verschiedene Weise interaktiv bedient werden.\nFür die kommandozeilenorientierte Bedienung wird die Konsolenanwendung \"psql\" mit der Installation mitgeliefert. Programme mit grafischer Benutzeroberfläche zur Bedienung und Verwaltung der Datenbank müssen oft gesondert installiert werden: freie Produkte sind etwa PgAdmin und phpPgAdmin. Daneben werden eine Vielzahl von kommerziellen Werkzeugen angeboten, die auch grafische CASE-Modellierung, Im- und Export-Funktionen oder DB-Monitoring bieten.\nEin MySQL-Migrationshilfswerkzeug ist in jedem freien PostgreSQL-Downloadpaket von EnterpriseDB enthalten.\n\nAn PostgreSQL wurden in der Vergangenheit zahlreiche Preise vergeben. Zuletzt erhielt das Projekt den \"„Linux New Media Awards 2012“\" als die beste Open-Source-Datenbank.\n\n\n\n\n"}
{"id": "21590", "url": "https://de.wikipedia.org/wiki?curid=21590", "title": "Adobe Acrobat Reader DC", "text": "Adobe Acrobat Reader DC\n\nAdobe Acrobat Reader DC („DC“ für „Document Cloud“) (bis 2015 und Version 11 Adobe Reader) ist ein Computerprogramm des Unternehmens Adobe zum Anzeigen von PDF-Dokumenten, also ein Dateibetrachter. Es ist Teil der Adobe-Acrobat-Produktfamilie.\n\nAdobe Acrobat Reader kann als Freeware kostenlos aus dem Internet heruntergeladen werden und wird von Softwareherstellern häufig zusammen mit der Dokumentation ihrer Programme geliefert.\n\nOffiziell unterstützt werden Windows Server 2008 R2, 2012 (und R2), Windows 7, Windows 8, Windows 10 und macOS seit 10.9. Die Unterstützung für Linux/Unix ist ausgelaufen.\n\nZur Nutzung auf Smartphones und Tablets gibt es den Adobe Acrobat Reader als offizielle App für die Betriebssysteme Android, iOS, Windows Phone und Windows 10 Mobile.\n\nFür den Adobe Reader 9, der insbesondere für Windows (ab Windows 2000), macOS (ab Mac OS X Tiger), Linux und Solaris verfügbar war, endete die offizielle Unterstützung am 26. Juni 2013. Der Download dieser und älterer Versionen wird nur noch über einen FTP-Server angeboten. Jedoch raten Linux-Distributoren wie OpenSUSE oder Red Hat wegen fehlender Sicherheitsupdates von einer weiteren Nutzung ab, obwohl es sich um die letzte auf Linux/Unix laufende Version handelte. Es gibt eine Reihe von Alternativen.\n\nVon der Version 5.1 bis zur Version 6 war das Programm in zwei Versionen erhältlich, einer kompakten Basisversion und einer erweiterten Version, die neben dem Anzeigen von Dokumenten auch Volltextsuche (mit beigefügtem Index auch von PDF-Dokumentensammlungen) und die Wiedergabe eingebetteter Multimediaobjekte unterstützt. Ab der Version 7 können auch dreidimensionale Grafikobjekte angezeigt werden. Bestimmte Funktionen stehen nur in Dokumenten zur Verfügung, die mit dem kostenpflichtigen Adobe Reader Extension Server freigeschaltet wurden.\n\nNeben den Funktionen Anzeigen und Drucken von Dokumenten unterstützt der Adobe Reader auch das Ausfüllen von Formularen. Diese können ausgedruckt werden und, wenn der Verfasser des Dokuments dies freigeschaltet hat, auch gespeichert oder als E-Mail an eine vorgegebene Adresse geschickt werden. Die Funktionen Anmerken und Kommentieren müssen ab Adobe Reader X nicht mehr gesondert freigeschaltet werden.\n\nAdobe Reader kann keine PDF-Dokumente erzeugen. Hintergründe zur Erstellung und Bearbeitung von PDF-Dokumenten siehe im Artikel Portable Document Format, dazu geeignete Software im Artikel Liste von PDF-Software.\n\nDie PDF-Version ist nicht gleichbedeutend mit der Version von \"Adobe Reader\". Beispielsweise benötigt das als ISO 32000-1:2008 veröffentlichte PDF 1.7 den Reader 8 oder höher (Merkhilfe 1+7=8).\n\nMit der kostenlosen Verfügbarkeit des Adobe Readers fand das Portable-Document-Format Verbreitung.\nDurch die Offenlegung des Portable-Document-Formats, dessen ausführlicher Dokumentation und der Normierung durch die ISO existieren heute auch viele andere PDF-Betrachter, so dass der Adobe Reader nicht mehr zwingend erforderlich ist. Dennoch bleibt die Popularität des Adobe Reader groß, auch weil von Adobe eingeführte nicht standardisierte PDF-Erweiterungen wie das 3D-PDF nur von Adobe Reader unterstützt werden.\n\nIn die Kritik geraten ist der Acrobat Reader in den letzten Jahren durch verschiedene Sicherheitslücken. Unter anderem war die Unterstützung von JavaScript fehlerbehaftet. Auch wird der enorme Ressourcenbedarf kritisiert, unter anderem durch die Vielzahl von in der Praxis selten verwendeten Plug-ins.\n\nDie macOS-Version installiert ohne Rückfrage ein Plug-in für den Browser Apple Safari, so dass dieser Adobe Reader zur PDF-Betrachtung verwendet, statt des schlankeren \"Vorschau\".\n\nBei den ersten Ausgaben der Version 9 gab es Probleme mit Benutzerprofilen, die sich nicht am Standardspeicherort befinden, z. B. servergespeicherten Benutzerprofilen (englisch Roaming User Profiles), die zu einem sofortigen Beenden durch die Laufzeitbibliothek führten. Auch beim Adobe Reader X treten Probleme mit symbolischen Verknüpfungen auf.\n\nIn Adobe Acrobat Reader DC hat der Nutzer keine Möglichkeit mehr, automatische Programm-Updates zu unterbinden oder angekündigte Aktualisierung auf einen späteren, ihm passenden Zeitpunkt zu verschieben.\n\nUnbekannte Täter haben einer Mitteilung von Adobe vom 3. Oktober 2013 zufolge den Sourcecode von Adobe Acrobat, Adobe ColdFusion und möglicherweise auch von anderen Programmen gestohlen. Außerdem wurden Kreditkartendaten von 2,9 Mio. Kunden entwendet.\n\nMit Einführung der zwölften Generation wich Adobe vom gewohnten Schema der Nummerierung ab, um mit der Bezeichnung „DC“ und seiner Vermarktung als Bestandteil des neuen Namens \"Adobe Acrobat Reader DC\" besonders auf die Einführung der sogenannten „Document Cloud“ hinzuweisen. Schon 2003 hatte es mit Einführung der sechsten Generation einen Namenswechsel von \"Acrobat Reader\" zu \"Adobe Reader\" gegeben.\nMit der Einführung von \"Adobe Acrobat Reader DC 2015\" wurden zwei Updatepfade, \"Continuous track\" und \"Classic track\", eröffnet. Im \"Continuous track\" stellt Adobe die jeweils aktuelle Basis und das aktuelle Update bereit, im \"Classic track\" werden mehrere Basisversionen und ihre vierteljährlichen Updates bereitgehalten.\n\nAlternativen zum Adobe Reader sind beispielsweise die ebenfalls kostenfreien Programme Ghostscript/Ghostview, der Foxit Reader, PDF-XChange Viewer, STDU Viewer, Xpdf und Sumatra PDF. Eine weitere Alternative ist Evince, ein Dokumentenbetrachter der Linux-Arbeitsumgebung Gnome, der ab Version 2.28 auch für Windows erhältlich ist. Unter KDE wird standardmäßig der universelle Dateibetrachter Okular für die Anzeige von PDF-Dateien verwendet. Einige alternative Readerprogramme sind nicht vollständig mit PDF kompatibel. Beispielsweise lassen sich bei Foxit verschachtelte Dateien, also eine PDF-Datei, die in eine andere PDF-Datei eingebettet ist, nicht öffnen. Der Internet-Browser Google Chrome hat ab Version 8 einen PDF-Reader integriert und kann somit auch zum Betrachten von PDF-Dateien verwendet werden. Firefox bietet diese Möglichkeit ab Version 19.0 ebenfalls. Der mit Windows 10 eingeführte Browser Microsoft Edge beherrscht ebenfalls die Anzeige von PDF-Dokumenten und ist dazu im Betriebssystem standardmäßig voreingestellt.\n\n"}
{"id": "21748", "url": "https://de.wikipedia.org/wiki?curid=21748", "title": "Alt Gr", "text": "Alt Gr\n\nDie Taste Alt Gr (auch \"Alt Graph\", engl. \"alternate graphic\" – alternative Grafik bzw. alternativer Schriftsatz) liegt auf PC-Tastatur mit AT-Layout normalerweise rechts neben der Leertaste und verändert, während sie gedrückt gehalten wird, die Funktionen der anderen Tasten. Eine Ausnahme bildet die US-amerikanische Tastatur, ihr fehlt diese Taste.\nWie die Umschalttaste () die Zweitbelegung der Tasten (z. B. + von »a« auf »A«) steuert, gibt diese Taste die Drittbelegung, also etwa Tastenkombination + = €, + = ² (deutsche und österreichische Tastaturen). Mit der Kombination + (deutsche und österreichische Tastaturen) bzw. + (Schweizer Tastaturen) erhält man beispielsweise das @-Zeichen.\n\nBei manchen Layouts erreicht man durch gemeinsames Betätigen von Alt Gr, Umschalttaste und einer Zeichentaste deren Viertbelegung: So erzeugt man im polnischen Layout das Zeichen Ł mit ++. Bis Windows 10 existierte für die Layouts des deutschsprachigen Raums keine solche Viertbelegung, seit Windows 10 liegt auf ++ das große ß; unter Linux bringt beispielsweise die Tastenkombination ++ das Zeichen × auf den Bildschirm.\n\nUrsprünglich hatten auch in deutschsprachigen Ländern PC-Tastaturen keine Alt-Gr-Taste. IBM wollte aus Kostengründen jedoch nicht verschieden große oder elektronisch unterschiedliche Tastaturen in verschiedene Länder liefern, sondern nur den leichter ersetzbaren Aufdruck der Tastenkappen ändern. Einige in deutschen Texten nur selten verwendete Zeichen, darunter die eckigen und geschweiften Klammern, der senkrechte Strich, der Backslash und das im Vor-E-Mail-Zeitalter im deutschen Sprachraum ebenfalls kaum übliche @-Zeichen, fehlten daher einfach auf solchen Tastaturen. Stattdessen gab es unter anderem die Umlaute, in Deutschland das ß, in der Schweiz französische Zeichen wie das ç. Für die Textverarbeitung und andere Büroanwendungen stellte dies kein größeres Problem dar, wohl aber für professionelle Programmierer, da eine Reihe dieser Zeichen in verschiedenen Programmiersprachen häufig verwendet werden. Hiesige Programmierer benutzten daher damals oft amerikanische Tastaturen oder stellten die Belegung softwaremäßig um und tippten dann „blind“ die entsprechenden Zeichen mit den Umlauttasten. Der fehlende Backslash wurde aber sogar für Nicht-Programmierer ein Problem, als ab 1983 in MS-DOS Unterverzeichnisse eingeführt wurden und dieses Zeichen zur Trennung der Verzeichnisebenen eingeführt wurde. \n\nEin Ausweg aus dieser unbefriedigenden Situation bot sich an, als IBM ab dem Jahr 1985 seine neue MF2-Tastatur einführte. Auf dieser befand sich an der gleichen Stelle, an der heute die AltGr-Taste liegt, erstmals eine zweite Alt-Taste, während ältere Tastaturen nur eine einzige Alt-Taste geboten hatten. Für nicht US-amerikanische Tastaturen wurde diese zweite Alt-Taste zur „AltGr“-Taste umdeklariert, um mehr Zeichen mit der gleichen Tastenzahl eingeben zu können. Dies wurde durch die Drittbelegung einiger Tasten bei gleichzeitigem Gebrauch der AltGr-Taste ermöglicht. Diese „Umdeklaration“ erklärt auch den ähnlichen Namen von Alt und AltGr, obwohl die Funktionen der beiden Tasten sehr verschieden sind.\n\nBei Windows-Systemen ist die Kombination + mit meist gleichwertig; ++ erzeugt auf deutschen und schweizerischen Tastaturen also ebenfalls das €-Zeichen. Das Gleiche gilt für die veralteten PC/XT- und PC/AT-Tastaturen, welche keine AltGr-Taste besitzen. Allerdings lässt sich umgekehrt die auf vielen Systemen zur Anmeldung und zum Herunterfahren benötigte Kombination ++ (Klammergriff) nicht durch + ersetzen. Es ist jedoch möglich, die Tastenkombination ++ durch das gleichwertige ++ zu ersetzen, wobei sich Letzteres mit nur einer Hand eingeben lässt.\n\nWird auf einem Apple Mac mit einer Apple-Tastatur das Betriebssystem Windows ausgeführt, so kann die Taste Alt Gr durch drücken der Tasten Alt-Taste + Ctrl-Taste nachgebildet werden.\n\n"}
{"id": "21987", "url": "https://de.wikipedia.org/wiki?curid=21987", "title": "Computer Bild", "text": "Computer Bild\n\nComputer Bild aus dem Axel-Springer-Verlag ist eine Computerzeitschrift, die in mehreren Ländern, teils unter anderem Namen, herausgegeben wird.\n\nIn Deutschland ist Computer Bild die auflagenstärkste Computerzeitschrift, zudem ist sie die meistverkaufte Computerzeitschrift in ganz Europa sowie in Russland. Sie erscheint seit 1996.\n\nDie Eigenschreibweise in Deutschland ist \"COMPUTERBILD\" oder \"COMPUTER BILD\", auch kurz \"CoBi\" und \"CB\".\n\nDie Zeitschrift richtet sich an PC-Einsteiger, die das Betriebssystem Microsoft Windows benutzen. Es werden stets alle Fachbegriffe erklärt und die Erläuterungen mit vielen Bildschirmfotos illustriert, die dem Leser jeden Mausklick detailliert verdeutlichen. Es sind in jeder Ausgabe Tests zu verschiedenem elektronischen Zubehör (z. B. Speicherkarten) oder Standard-Hardware enthalten.\n\nComputer Bild ist bei den meisten Zeitschriftenhändlern in Deutschland, Österreich und der deutschsprachigen Schweiz in vier verschiedenen Varianten verfügbar: ohne Datenträger, mit CD oder mit DVD.\n\nDie CD enthält das meiste der im Heft besprochenen Software und zusätzlich eine oder mehrere Gratisversionen des Programms für das Titelthema.\n\nAuf der DVD ist zusätzlich zu den Inhalten der CD fast immer ein Film vertreten, der sich auch auf einem DVD-Player abspielen lässt, oder drei bis vier weitere (große) Programme. Auf jedem Medium sind (seit 2014) die ESET Internet Security und der \"COMPUTER BILD-Abzock-Schutz\" enthalten. Die Medien enthalten meist eine bis fünf Kaufprogramme, nur Sondereditionen enthalten mehr solche Programme.\n\nComputer Bild ist in Deutschland im Abonnement in drei Varianten verfügbar: als Heft, mit CD und mit DVD. Frei Haus geliefert kosten sie genauso viel wie im Kioskverkauf.\n\nHarald Kuppek hat die Zeitschrift gegründet und war bis Dezember 2006 auch Herausgeber und Chefredakteur. Von 2007 bis 2016 war der Herausgeber der ehemalige Bild-Chefredakteur (bis 2015) Kai Diekmann.\n\nSeit Oktober 2006 war Hans-Martin Burr Chefredakteur, er verließ das Unternehmen am 28. Februar 2012 auf eigenen Wunsch. Von 2012 bis 2017 war Axel Telzerow Chefredakteur. Auf ihn folgte 2018 Dirk Kuchel.\n\n\"Computer Bild\" veröffentlicht regelmäßig Sonderhefte.\n\n\"Computer Bild\" bot in Zusammenarbeit mit Kaspersky Lab die \"Kaspersky Security Suite CBE\" (Computer Bild-Edition) an, welche sich in fast jeder CD-/DVD-Ausgabe bis Oktober 2012 befand und bis Mai 2013 gültig war. Im Mai 2013 wurde Kaspersky jedoch gegen \"Norton Internet Security CBE\" getauscht, welche wiederum Anfang 2014 durch die \"G Data Security Suite CBE\" ersetzt wurde. Die Kaspersky Security Suite CBE war immer die Vorgängerversion der aktuellen Kaspersky Internet Security und war für Windows ab XP geeignet. Außerdem wird eine Software mit dem Namen \"Computer Bild-Abzockschutz\" angeboten, die vor Betrügern im Internet warnen soll.\n\nIm Ullstein-Verlag sind Taschenbücher zu verschiedenen Computerthemen unter dem Namen \"Computer Bild\" erschienen, die im Buchhandel erwerbbar oder über den Online-Shop bestellbar sind.\n\nAm 28. Juni 2012 veröffentlichte \"Computer Bild\" in der Ausgabe 15/2012 Rechercheergebnisse, die nahelegten, dass das Leipziger Unternehmen Unister das Vertrauen von potentiellen Kunden mit unsauberen Methoden zu erlangen versucht und sie hernach „abzockt“, beispielsweise mit den Webseiten fluege.de, Flug24.de, Billigfluege.de, ab-in-den-urlaub.de, Travel24.com, Reisen.de, Preisvergleich.de oder Partnersuche.de. Nach einer nicht erfolgreichen Abmahnung seitens Unister Anfang Juli 2012 erwirkte Unister einen Gerichtsbeschluss am Landgericht Leipzig, um eine weitere Verbreitung der Computer-Bild-Ausgabe zu verhindern.\n\nZu den Vorwürfen der Computer Bild gegen Unister gehörten unzulässig aufgeschlagene Service-Pauschalen, ein undurchsichtiges Firmenkonstrukt, erfundene gestrichene höhere Vergleichspreise bei Flugangeboten, erkaufte scheinbare Verbraucherschutzsiegel, konstruierte Vergleichsportale, die nur Konzernfirmen miteinander verglichen, ohne diese Zusammengehörigkeit offenzulegen und anderes.\n\n\"Computer Bild\" hat in den vergangenen Jahren erheblich an Auflage eingebüßt. Sie beträgt gegenwärtig Das entspricht einem Rückgang von Stück. Der Anteil der Abonnements an der verkauften Auflage liegt bei Prozent.\n\nIm Juli 2007 hat \"Computer Bild\" ein Internet-Portal gestartet, das zugleich die Online-Inhalte von \"Computer Bild Spiele\" und \"Audio Video Foto Bild\" beinhaltet. Das Online-Portal wird von der Computerbild Online Dienstleistungs-GmbH betrieben. Neben Inhalten der Druckzeitschrift gibt es tagesaktuelle Meldungen, einen interaktiven Preisvergleich und ein Forum zu den Themen Hardware, Software und Telekommunikation. Chefredakteur des Onlineangebots ist Christian Bigge, früherer Chefredakteur der\nPC Action. Im Februar 2012 wurde aufgrund stark gesunkener Verkaufsauflage bekanntgegeben, dass die Redaktion der Druckausgabe mit der Online-Ausgabe unter dem Dach der \"Computer Bild Digital GmbH\" zusammengelegt wird (Teilbetriebsübergang). Da diese Tochterfirma im Unterschied zur bisherigen direkten Arbeitgeberin Axel Springer AG nicht tarifgebunden ist, protestieren Mitarbeiter der Print-Redaktion gegen das Vorgehen.\n\nIm August 2013 unterzog die Axel Springer AG der Webseite von Computer Bild einen Relaunch. Laut dem Unternehmen sollen dadurch neue Stammnutzer und neue Zielgruppen erreicht werden.\n\nDie Webseite von Computer Bild konnte laut AGOF im Mai 2013 13,56 Millionen Menschen (Unique User) erreichen. Die IVW wies im Juli 2013 42,67 Millionen Gesamtbesuche (Visits) aus.\n\nAls Ableger erschien zuerst 1999 Computer Bild Spiele. Danach folgte 2004 Audio Video Foto Bild, die aus der Unterhaltungselektronik-, Musik- und Film-Redaktion der Computer Bild heraus entwickelt wurde.\n\nInzwischen wurden, vorwiegend in Osteuropa, ausländische Ableger der COMPUTERBILD gegründet:\n\n"}
{"id": "22217", "url": "https://de.wikipedia.org/wiki?curid=22217", "title": "Apple Desktop Bus", "text": "Apple Desktop Bus\n\nDer Apple Desktop Bus (ADB) ist ein von Apple definierter Schnittstellen-Standard für den Anschluss von externen Geräten an einen Computer, der von 1986 bis 1999 verwendet wurde. ADB gilt als einer der Vorläufer des Universal Serial Bus (USB).\n\nEs handelt sich um einen seriellen Bus mit geringer Übertragungsrate (10 kbit/s) zum Anschluss von Eingabegeräten, wie z. B. Tastaturen, Mäusen, Joysticks, Softwaredongles und Grafiktabletts. Maximal konnten an diesem Bus 15 Geräte adressiert werden, sie werden über einen 4-poligen Mini-DIN-Stecker angeschlossen.\n\nVon den vier Leitungen des Busses werden zwei für eine 5 V-Stromversorgung verwendet, eine Leitung dient der Kommunikation und die vierte zum Einschalten des Computers.\n\nDie Geräte und der Computer kommunizieren über eine einzelne bidirektionale Leitung. Generell wird die Kommunikation vom Computer ausgelöst, indem dieser eines der Geräte adressiert und diesem Daten sendet oder Daten von ihm abfragt. Jedes Gerät kann vier logische Register haben, auf die auf diese Weise zugegriffen werden kann. Will ein Gerät Daten senden, so kann es am Ende der Übertragung eines jeden Kommandos auf dem Bus einen \"Service Request auslösen\", woraufhin der Computer alle Geräte nacheinander abfragt, bis der \"Service Request\" nicht mehr auftritt.\n\nGeräte am ADB werden durch die Schnittstelle mit Strom versorgt, zumindest sofern sie relativ geringe Leistungen brauchen. Insgesamt stehen für alle Geräte an einem ADB bis 500 mA zur Verfügung, es gibt aber keine Verwaltung des Stromverbrauches, so dass es zu nicht funktionierenden Konfigurationen kommen kann, wenn zu viele oder zu stromhungrige Geräte angeschlossen werden.\n\nDie Geräte am ADB werden in 7 Klassen eingeteilt: Tastaturen, Mäuse, Dongles, Absolute Devices (d. h. Geräte, die absolute Koordinaten übermitteln, wie z. B. Grafiktabletts), Data Transfer (Datenübertragung), Verschiedenes, und eine reservierte Klasse. Durch die Adresse, an der sich das Gerät anmeldet, wird die Klasse festgelegt. Jedes Gerät kann zudem eine 8-Bit-Kennung enthalten, die von Apple an die Hersteller vergeben wurde. Einige der Geräteklassen haben fest vorgeschriebene Datenformate, in denen die Geräte kommunizieren. Damit war es möglich, in vielen Fällen Geräte ohne spezielle Treiber zu verwenden. Diese Grundidee ist heute mit den Geräteklassen am Universal Serial Bus weitergeführt worden.\n\nDer erste Rechner mit ADB war kein Apple Macintosh, sondern der Apple IIgs von 1986. Ab den Macintosh SE und II wurden dann auch alle Mac-Modelle mit dem ADB ausgerüstet, bis dieser ab 1997, zuerst im iMac, schrittweise durch den Universal Serial Bus (USB) abgelöst wurde. Der blauweiße Power Mac G3, eingeführt 1999, verfügte sowohl über ADB als auch USB, der erste G4 hatte den ADB noch im Chipsatz vorgesehen, aber keine Buchse mehr dafür, allerdings war diese auf der Hauptplatine noch vorgesehen und konnte nachgerüstet werden.\n\nFür Softwareentwickler hatte die Option, den ADB zu benutzen, noch einige Zeit eine gewisse Bedeutung, da bei älteren Versionen von MacOS nur mit einer ADB-Tastatur der Low-Level-Debugger bedient werden konnte.\n\nDer ADB wurde auch bei den Workstations der Firma NeXT verwendet.\n\n\n\n"}
{"id": "22324", "url": "https://de.wikipedia.org/wiki?curid=22324", "title": "XnView", "text": "XnView\n\nXnView ist eine Bilderverwaltungssoftware und Bildbetrachter, mit dem auch eine einfache Bildbearbeitung möglich ist und Bilder in andere Dateiformate konvertiert werden können. Das Programm gilt als ausgereift; im Frühjahr 1998 erschien die Version 1.05.\n\nXnView bietet eine Skriptsprache, mit der z. B. Bilder konvertiert und gleichzeitig umbenannt werden können. Weitere Features sind Größenänderung, Farbraum-Manipulation, verschiedene Filter und eine Diashow.\n\nJPEG-Dateien können verlustfrei geändert werden – XnView kann JPGs drehen (auch nach Exif-Ausrichtung), spiegeln und zuschneiden (Cropping), ohne das Bild neu codieren zu müssen.\n\nDas Programm ist für den privaten Gebrauch kostenlos, einfach zu bedienen und benötigt nur wenig Systemressourcen. Es kann mehr als 500 Grafikformate lesen und über 70 schreiben.\n\nDas Originalprogramm gibt es nur für Windows. Der Nachfolger namens XnViewMP (Multi-Plattform) ist zusätzlich für macOS und Linux verfügbar und besitzt neben einer auf Qt basierenden einheitlichen Oberfläche auch Unterstützung für Unicode. XnViewMP besitzt eine verbesserte Leistungsfähigkeit, da es nun mehrere Prozessorkerne nutzt und auch als 64-Bit-Version zur Verfügung steht.\n\nDas Programm wurde mehrfach von verschiedenen Websites, die kostenlose Software zum Download anbieten, mit fünf von fünf Sternen ausgezeichnet.\n\nObwohl das Programm als Bildbetrachter gilt, kann es auch RAW-Dateien lesen, verarbeiten und konvertieren.\n\n"}
{"id": "22665", "url": "https://de.wikipedia.org/wiki?curid=22665", "title": "Linux Loader", "text": "Linux Loader\n\nLinux Loader, kurz LILO, ist ein freier Bootloader für den Linux-Kernel, der durch seine Flexibilität auch andere Kernel starten kann.\n\nDie Entwicklung von LILO begann im Jahre 1992, der damalige Hauptentwickler war Werner Almesberger; 1999 übergab er das Projekt an John Coffman. In diesem Zeitraum hatte sich LILO bereits zum Standard-Bootloader in vielen Linux-Distributionen entwickelt, litt jedoch zusehends unter einigen Einschränkungen. So ist LILO nicht in der Lage, Dateisysteme zu lesen, sondern muss wissen, auf welchen Datenblöcken der zu startende Kernel liegt. Dies hat zur Folge, dass bei einem Austausch des Kernels, zunächst ein spezielles Programm die entsprechenden Blöcke wieder aufwändig ermitteln muss. Dies birgt zwar zugleich den Vorteil, dass LILO dadurch unabhängig von Dateisystemen arbeiten kann, diese Fähigkeit ist aber oftmals trivial. Um 2002 herum verlor LILO seine Stellung als Standard-Bootloader immer mehr an die Neuentwicklung GRUB, welche Dateisysteme lesen und mit Verschlüsselung, Kompression sowie RAID-Systemen umgehen kann.\n\nDie Entwicklung geriet ins Stocken, wurde 2010 jedoch von Joachim Wiedorn wieder aufgenommen und wurde planmäßig bis zum Dezember 2015 von ihm betreut.\n\nDie Konfiguration erfolgt über eine Konfigurationsdatei, welche üblicherweise unter \"/etc/lilo.conf\" zu finden ist.\nDas folgende Beispiel zeigt eine LILO-Konfigurationsdatei, die Linux und FreeBSD zur Auswahl anbietet.\nlarge-memory\nlba32\nboot=/dev/hda\ninstall=menu\nmap=/boot/map\nprompt\n\ndefault=Linux\nimage=/boot/vmlinuz-2.6.26\nlabel=\"Linux\"\nroot=/dev/hda1\nappend=\"\"\nread-only\noptional\n\nother=/dev/hda3\nlabel=\"FreeBSD\"\n\n"}
{"id": "23201", "url": "https://de.wikipedia.org/wiki?curid=23201", "title": "Apache HTTP Server", "text": "Apache HTTP Server\n\nDer Apache HTTP Server [] ist ein quelloffenes und freies Produkt der Apache Software Foundation und einer der meistbenutzten Webserver im Internet.\n\nEine Gruppe von acht Entwicklern begann 1994 den Webserver NCSA HTTPd zu erweitern. Dies waren im Einzelnen: Brian Behlendorf, Roy T. Fielding, Rob Hartill, David Robinson, Cliff Skolnick, Randy Terbush, Robert S. Thau und Andrew Wilson mit Unterstützung von Eric Hagberg, Frank Peters und Nicolas Pioch.\n\nSie gaben dem Ergebnis ihrer Arbeit den Namen Apache HTTP Server und veröffentlichten diesen im April 1995. Er war das Gründungsprojekt der Apache Software Foundation.\n\nDer Name wurde aus Respekt vor dem nordamerikanischen Indianerstamm der Apachen gewählt. Nicht korrekt ist, dass der Name eine Umdeutung von \"„“\" sei, was so viel wie „ein zusammengeflickter Server“ bedeutet. Diese Deutung entstand durch den Umstand, dass der Apache HTTP Server ursprünglich eine gepatchte Erweiterung des alten NCSA HTTP Servers war.\n\nNeben Unix und Linux unterstützt Apache Win32, Netware sowie eine Vielzahl weiterer Betriebssysteme. In Apache 2.4 wurde der Support für ältere, lange schon nicht mehr weiter entwickelte Betriebssysteme wie BeOS, TPF und A/UX beendet. In Version 2.0 wurde die Stabilität und Geschwindigkeit des Servers – vor allem auf Nicht-Unix-Systemen – erheblich verbessert: Die Bibliothek Apache Portable Runtime (APR) stellt eine Verallgemeinerung wichtiger Systemaufrufe zur Verfügung, sodass die individuellen Stärken des jeweiligen Betriebssystems ausgenutzt werden können. Hinzu kommen verschiedene Multiprocessing-Module (MPM), die je nach Plattform unterschiedliche Lösungen für die gleichzeitige Bedienung mehrerer Client-Anfragen anbieten: Beispielsweise setzt das MPM prefork für klassische Unix-Systeme auf Forking von Prozessen, während mpm_winnt für die unter Windows empfehlenswerteren Threads optimiert ist.\n\nDer Apache-Webserver ist modular aufgebaut: Durch entsprechende Module kann er beispielsweise die Kommunikation zwischen Browser und Webserver verschlüsseln (mod_ssl), als Proxyserver eingesetzt werden (mod_proxy) oder komplexe Manipulationen von HTTP-Kopfdaten (mod headers) und URLs (mod rewrite) durchführen.\n\nDer Apache bietet die Möglichkeit, mittels serverseitiger Skriptsprachen Webseiten dynamisch zu erstellen. Häufig verwendete Skriptsprachen sind PHP, Perl oder Ruby. Weitere Sprachen sind Python, JavaScript (z. B. V8CGI), Lua, Tcl und .NET (mit ASP.NET oder Mono). Diese sind kein Bestandteil des Webservers, sondern müssen ebenfalls entweder als Module eingebunden werden oder über das CGI angesprochen werden, da Apache im Gegensatz zu beispielsweise nginx modulbasiert ist. Die Module können jederzeit aktiviert oder deaktiviert werden. Über das bei der Apache-Installation enthaltene mod_include kann Server Side Includes (SSI) ausgeführt werden. Damit ist es möglich, einfache dynamische Webseiten zu erstellen und den Verwaltungsaufwand von statischen Webseiten zu minimieren.\n\nDer Apache HTTP Server ist, wie alle Programme der Apache Software Foundation, eine freie Software. Derzeit wird noch die stabile Version 2.4.x unterstützt und somit beispielsweise mit Sicherheitsupdates versorgt. Die Apache-Entwickler empfehlen die Version 2.4.x für den Produktiveinsatz.\n\nDer Apache HTTP Server ist in fast allen Linux-Distributionen und in macOS standardmäßig enthalten. Eine beliebte Entwicklungs-Distribution für Windows, Linux und Mac OS X ist XAMPP.\n\n\nDer Apache-Server kann mit sogenannten Modulen erweitert werden, die bestimmte Zusatzfunktionen mitbringen und immer mit 'mod_' beginnen (z. B. mod_rewrite, mod_php7.0, mod_ssl). Es gibt Module u. a. für\n\n\n"}
{"id": "23866", "url": "https://de.wikipedia.org/wiki?curid=23866", "title": "Cracker (Computersicherheit)", "text": "Cracker (Computersicherheit)\n\nCracker (vom englischen \"crack\" für „knacken“ oder „[ein]brechen“) umgehen oder brechen Zugriffsbarrieren von Computersystemen und Rechnernetzen.\n\nDas umschließt im Allgemeinen Scriptkiddies und Hacker, die ihre Aktivitäten betont auf die Umgehung von Sicherheitsmechanismen legen (siehe Abgrenzungen). Im Speziellen umschließt das Wort Programmierexperten, die Schutzmechanismen einer Software durch Cracking aushebeln (von der widerrechtlichen Manipulation von Software, beispielsweise als Teil der Warez-Szene, bis hin zu einer legalen Crackerszene begeisterter Programmierer, die mithilfe von CrackMes einen Sport auf geistiger Ebene praktizieren).\n\nDarüber hinaus ist die Abgrenzung des Crackerbegriffs nicht einheitlich, weshalb seine Bedeutung stark vom jeweiligen Kontext abhängig ist:\n\n\nIn der journalistischen und politischen Öffentlichkeit werden diese Ausdrücke gewöhnlich nicht unterschieden. Daneben gibt es auch Hacker, die eine moralische Abgrenzung aus Ermangelung einer klaren Trennlinie zwischen „gut“ und „böse“ ablehnen.\n\nNeben diesem Gebrauch gibt es eine weitere Verwendung, in der speziell jemand als (Software-)Cracker betitelt wird, der sich darauf versteht, Schutzmechanismen einer Software auszuhebeln. Kulturübergreifend gilt dies ungeachtet von dessen Motivation, also auch dann, wenn das Cracken von Software als legaler Sport betrieben wird, indem der Cracker den Programmschutz selbstgeschriebener und eigens für diesen Zweck freigegebener Software (CrackMe) aushebelt.\n\nAnmerkungen:\nDie akademische Hackerkultur unterscheidet sich von der Computersicherheitshackerkultur dahingehend, dass bei der akademischen Hackergemeinschaft die Schaffung neuer und die Verbesserung bestehender Infrastrukturen im Vordergrund steht, insbesondere des eigenen Softwareumfelds. Computersicherheit ist dabei kein relevanter Aspekt. Ein Grundwissen zu Computersicherheit ist allerdings auch in der akademischen Hackergemeinschaft üblich. Zum Beispiel merkte Ken Thompson während seiner Turing-Award-Rede 1983 an, dass es möglich ist, in das UNIX-Login-Programm eine Hintertür einzubauen, sodass es zwar die normalen Passwörter akzeptiert, aber zusätzlich auch ein Generalpasswort. Er nannte dies ‚Trojanisches Pferd‘. Thompson argumentierte, dass man den C-Compiler zur Verschleierung des Ganzen so ändern könnte, dass er beim Übersetzen des Login-Programms diese Hintertür automatisch hinzufügte. Da der C-Compiler selbst ein Programm ist, das mit einem Compiler übersetzt wird, könnte man schließlich diese Compileränderung automatisch beim Übersetzen des Compilers selbst einfügen, ohne dass diese Manipulation noch aus dem Compilerquelltext ersichtlich wäre. Sie wäre somit nur noch in übersetzten Compilern vorhanden und in den übersetzten Programmen ohne irgendeine Spur in der Quelltextbasis zu hinterlassen.\n\nThompson distanzierte sich aber deutlich von den Tätigkeiten der Computersicherheitshacker: „I would like to criticize the press in its handling of the 'hackers', the 414 gang, the Dalton gang, etc. The acts performed by these kids are vandalism at best and probably trespass and theft at worst. … I have watched kids testifying before Congress. It is clear that they are completely unaware of the seriousness of their acts.“\n\nEin weiterer prominenter Fall zur Überschneidung zwischen diesen beiden Kulturen ist Robert T. Morris, der zur Hackergemeinschaft am „AI“-Rechner des MIT gehörte, trotzdem aber den Morris-Wurm schrieb. Das \"Jargon File\" nennt ihn daher „a true hacker who blundered“ („einen echten Hacker, der versagt hat“).\n\nDie akademische Hackergemeinschaft sieht die nebensächliche Umgehung von Sicherheitsmechanismen als legitim an, wenn dies zur Beseitigung konkreter Hindernisse bei der eigentlichen Arbeit getan wird. In besonderen Formen kann so etwas auch ein möglicher Ausdruck von einfallsreicher intellektueller Experimentierfreudigkeit sein. Trotzdem tendieren die Anhänger der akademischen Szene dazu, die Beschäftigung mit Sicherheitslücken negativ zu bewerten und sich davon zu distanzieren. Üblicherweise bezeichnen sie Leute, die dies tun, als \"Cracker\" und lehnen jede Definition des Hackerbegriffs grundsätzlich ab, die eine Betonung auf Aktivitäten im Zusammenhang mit der Umgehung von Sicherheitsmechanismen einschließt.\n\nDie Computersicherheitshackerkultur andererseits unterscheidet im Allgemeinen nicht so streng zwischen den beiden Szenen. Sie beschränken die Verwendung des Cracker-Begriffs stattdessen auf ihre Kategorien der „Scriptkiddies“ und „Black-Hat-Hacker“. Aus dem Bereich der Computersicherheit sehen zum Beispiel Teile des CCC die akademische Hackerbewegung als konservative Fraktion einer einzelnen größeren, verwobenen und allumfassenden Hackerkultur.\n\n\n\n"}
{"id": "23898", "url": "https://de.wikipedia.org/wiki?curid=23898", "title": "Social Engineering (Sicherheit)", "text": "Social Engineering (Sicherheit)\n\nSocial Engineering [] (engl. eigentlich „angewandte Sozialwissenschaft“, auch „soziale Manipulation“) nennt man zwischenmenschliche Beeinflussungen mit dem Ziel, bei Personen bestimmte Verhaltensweisen hervorzurufen, sie zum Beispiel zur Preisgabe von vertraulichen Informationen, zum Kauf eines Produktes oder zur Freigabe von Finanzmitteln zu bewegen. Social Engineers spionieren das persönliche Umfeld ihres Opfers aus, täuschen Identitäten vor oder nutzen Verhaltensweisen wie Autoritätshörigkeit aus, um geheime Informationen oder unbezahlte Dienstleistungen zu erlangen. Häufig dient Social Engineering dem Eindringen in ein fremdes Computersystem, um vertrauliche Daten einzusehen; man spricht dann auch von Social Hacking [] (vgl. Hacker).\n\nEine frühe Form des Social Engineering wurde in den 1980er Jahren mit Phreaking praktiziert. Phreaker riefen unter anderem bei Telefongesellschaften an, gaben sich als Systemadministrator aus und baten um neue Passwörter, mit denen sie schließlich kostenlose Modemverbindungen herstellten.\n\nDas Grundmuster des Social Engineering zeigt sich bei fingierten Telefonanrufen: Der Social Engineer ruft Mitarbeiter eines Unternehmens an und gibt sich als Techniker aus, der vertrauliche Zugangsdaten benötigt, um wichtige Arbeiten abzuschließen. Bereits im Vorfeld hat er aus öffentlich zugänglichen Quellen oder vorangegangenen Telefonaten kleine Informationsfetzen über Verfahrensweisen, tägliches Bürogerede und Unternehmenshierarchie zusammengetragen, die ihm bei der zwischenmenschlichen Manipulation helfen, sich als Insider des Unternehmens auszugeben. Zusätzlich verwirrt er sein technisch ungebildetes Opfer mit Fachjargon, baut mit Smalltalk über scheinbar gemeinsame Kollegen Sympathie auf und nutzt Autoritätsrespekt aus, indem er droht, bei vom Opfer unterlassener Kooperation dessen Vorgesetzten stören zu müssen. Unter Umständen hat der Social Engineer bereits im Vorfeld Informationen gesammelt, dass ein bestimmter Mitarbeiter sogar wirklich technische Hilfe angefordert hat und bereits tatsächlich einen derartigen Anruf erwartet.\n\nTrotz ihrer scheinbaren Banalität gelingen mit der Methode immer wieder spektakuläre Datendiebstähle. So gelang es einem amerikanischen Schüler 2015, den privaten E-Mail-Account des damaligen CIA-Direktors Brennan zu öffnen und drei Tage lang darauf zuzugreifen.\n\nBeim automatisierten Social Engineering, auch Scareware genannt, kommen spezielle Schadprogramme zum Einsatz, die den Nutzer verängstigen und so zu bestimmten Handlungen bewegen sollen.\n\nEine bekannte Variante des Social Engineering ist das Phishing. Bei dieser unpersönlichen Variante werden fingierte E-Mails mit vertrauenserweckender Aufmachung an die potentiellen Opfer versendet. Inhalt dieser Nachrichten kann zum Beispiel sein, dass ein bestimmter Dienst, den man nutzt, eine neue URL hat und man sich auf dieser von nun an einloggen soll, wenn man ihn in Anspruch nehmen will. Bei dieser fingierten Seite handelt es sich, von Layout und Aufmachung her, um eine Kopie der originalen Webseite des Serviceanbieters. Dies soll dazu beitragen, das Opfer in Sicherheit zu wiegen. Fällt man darauf herein, so gelangen Kriminelle in den Besitz des Loginnamens und -passworts. Eine weitere Möglichkeit besteht darin, dass das Opfer von einem vermeintlichen Administrator dazu aufgefordert wird, die Logindaten als Antwort zurückzusenden, da angeblich technische Probleme vorliegen. Das Grundmuster ist ähnlich dem fingierten Telefonanruf, denn auch hier gibt sich der Social Engineer in der Regel als technischer Mitarbeiter aus, der zur Datenüberprüfung oder -wiederherstellung die Geheiminformation benötigt. Anders als dort verfügt der Angreifer hier meist über nicht viel mehr als die E-Mail-Adresse des Empfängers, was die Attacke weniger persönlich und damit auch weniger wirkungsvoll macht.\n\nEffizienter ist das \"Spear-Phishing\" (abgeleitet von der englischen Übersetzung des Begriffs \"Speer\"), worunter ein gezielter Angriff zu verstehen ist. Hierbei beschafft sich der Angreifer z. B. über die Studentenvertretung einer Hochschule die Mailadressen der dort eingeschriebenen Studenten, um an diese gezielt eine Phishing-Mail einer lokal ansässigen Bank oder Sparkasse zu übersenden. Die „Trefferquote“ bei dieser Art von Phishing-Attacken ist höher als bei normalen Angriffen, da die Wahrscheinlichkeit, dass ein Student seine Bankverbindung bei diesem Institut unterhält, sehr groß ist.\n\nHierbei wird der Müll des Opfers durchwühlt und nach Hinweisen und Anhaltspunkten über das soziale Umfeld gesucht. Diese können dann in einem darauf folgenden Anruf dazu verwendet werden, um das Vertrauen des Opfers zu erschleichen.\n\nDie Sicherheitsfirma Kaspersky Lab deckte auf, dass eine unbekannte Hackergruppe seit 2001 rund 500 Firmen mit USB Drops angegriffen hatte. Dabei erhielten zufällige Mitarbeiter infizierte USB-Sticks, deren Verwendung ihren PC infizierte und den Hackern Zugriff auf das interne Netzwerk der Firma gewährte. Eine Möglichkeit für Angreifer ist es, die infizierten USB-Sticks vor dem Firmengelände als Werbegeschenk zu verteilen.\n\nDie Abwehr von Social Engineering ist nicht einfach zu bewerkstelligen, da der Angreifer im Grunde positive menschliche Eigenschaften ausnutzt: Den Wunsch etwa, in Notsituationen unbürokratisch zu helfen oder auf Hilfe mit Gegenhilfe zu reagieren. Generelles Misstrauen zu schüren, würde auch die Effektivität und die vertrauensvolle Zusammenarbeit in Organisationen negativ beeinflussen. Den wichtigsten Beitrag zur Bekämpfung von Social Engineering liefert deshalb im konkreten Fall das Opfer selbst, indem es Identität und Berechtigung eines Ansprechenden zweifellos sicherstellt, bevor es weitere Handlungen vornimmt. Bereits die Rückfrage nach Name und Telefonnummer des Anrufers oder dem Befinden eines nicht existierenden Kollegen kann schlecht informierte Angreifer enttarnen. Höflich um Geduld zu bitten, wenn eine heikle Anfrage auch noch so dringend vorgetragen wird, sollte deshalb gezielt trainiert werden. Auch scheinbar geringfügige und nutzlose Informationen sollten Unbekannten nicht offengelegt werden, denn sie könnten in folgenden Kontaktaufnahmen zum Aushorchen anderer missbraucht werden oder zusammen mit vielen anderen für sich genommen nutzlosen Angaben zum Abgrenzen eines größeren Sachverhalts dienen. Wichtig ist eine schnelle Warnung aller potenziellen weiteren Opfer; erste Ansprechpartner sind die Sicherheitsabteilung des Unternehmens, die Kontaktadresse des E-Mail-Providers und Mitmenschen und Institutionen, deren Angaben zur Vorspiegelung falscher Tatsachen missbraucht wurden. Folgende Punkte sollten unbedingt beachtet werden:\n\nDer US-Sicherheitsspezialist Bruce Schneier zweifelt angesichts der Komplexität und der möglichen Nebenwirkungen von präventiven Maßnahmen gegen Social Engineering sogar generell an deren Wert und schlägt stattdessen Strategien vor, die auf Schadensbegrenzung und schnelles Recovery bauen.\n\nSchulungen der Mitarbeiter sind zwar notwendig, aber nur begrenzt hilfreich, wie Studien an der US-Militärakademie West Point gezeigt haben. Im Vorfeld können sogenannte Social-Engineering-Penetrationstests durchgeführt werden.\n\nÖffentlich bekannt wurde die Methode vor allem durch den Hacker Kevin Mitnick, der durch seine Einbrüche in fremde Computer eine der meistgesuchten Personen der Vereinigten Staaten war. Mitnick selbst meinte, Social Engineering sei die bei weitem effektivste Methode, um an ein Passwort zu gelangen, und schlage rein technische Ansätze in Sachen Geschwindigkeit um Längen.\n\nBekannt wurde 2010 der US-IT-Experte Thomas Ryan mit seiner Kunstfigur Robin Sage. Die virtuelle Internetschönheit stellte über soziale Netzwerke Kontakte zu Militärs, Industriellen und Politikern her und entlockte ihnen vertrauliche Informationen. Ryan ging nach einem Monat mit den Ergebnissen des Experiments an die Öffentlichkeit, um vor allzu großer Vertrauensseligkeit in sozialen Netzwerken zu warnen.\n\nDer für die Computersicherheit tätige Hacker \"Archangel\" zeigte in der Vergangenheit, dass Social Engineering nicht nur bei der Offenlegung von Passwörtern wirksam ist, sondern auch bei der illegalen Beschaffung von Pizzen, Flugtickets und sogar Autos funktioniert.\n\nWeitere bekannte Social Engineers sind der Scheckbetrüger Frank Abagnale, Miguel Peñalver, David „Race“ Bannon, der sich als Interpol-Agent ausgab, der Grundstücksbetrüger Peter Foster, der Hochstapler Steven Jay Russell und der Hochstapler Gert Postel, der mit einem weiteren Hochstapler, Reiner Pfeiffer, eine Rolle in der Barschel-Affäre gespielt hat.\n\n\n\n"}
{"id": "24052", "url": "https://de.wikipedia.org/wiki?curid=24052", "title": "Systemabfrage-Taste", "text": "Systemabfrage-Taste\n\nDie Systemabfrage-Taste, kurz S-Abf, auf Schweizer- und US-Tastaturen SysRq (kurz für engl. ), ist ein Relikt alter Terminal-Tastaturen, auf denen die Taste das Zurücksetzen der Tastatur oder das Wechseln von einer Sitzung zur nächsten bewirkt. Auf PC-Tastaturen wurde diese Funktion zunächst 1984 mit dem IBM Personal Computer/AT als eigene Taste eingeführt, wodurch die Tastenanzahl von 83 auf 84 anwuchs. Nachdem die Taste von Software wider Erwarten kaum verwendet wurde, hat IBM die Funktion 1987 mit der 101/102-tastigen MF2-Tastatur als Zusatzfunktion auf die Druck-Taste verlegt; die Systemabfrage-Funktion wird nun und bis heute mittels der Tastenkombination + bzw. bei internationalen Tastaturen + verwirklicht. Die Systemabfrage-Taste hat aber weiterhin ihren eigenständigen Scancode, der nicht dem der Druck-Taste entspricht, und kann daher bei Notebook-Tastaturen auch auf einer anderen Tastenkombination liegen oder ganz fehlen.\n\nUnter Microsoft Windows (die -Taste ist allgemein das Bildschirmfoto direkt ohne Interface in die Zwischenablage) ist + (also die S-Abf-Belegung der deutsch-/österreichischen Standardtastatur) ein Foto nur des aktuell aktiven Fensters.\n\nUnter Windows kann man mit der Tastenkombination und zweimaligem Drücken von den Kerneldebugger starten, nachdem diese Funktion in der Registry aktiviert wurde.\nIn Windows 10 ist diese Funktion nicht mehr verfügbar.\n\nBei neueren Linux-Versionen kann man diese Taste so konfigurieren, dass man auch bei einem durch einen Software- oder Hardwarefehler blockierten Terminal bzw. X-Server noch bestimmte Aktionen (z. B. Reboot mit vorherigem Schreiben gepufferter Daten auf die Datenträger) durchführen kann. Nur wenige Teile des Kernels müssen noch lauffähig sein, um diese Funktion zu nutzen. Das wird \"Magic SysRq key (Magische S-Abf-Taste)\" genannt.\n"}
{"id": "24416", "url": "https://de.wikipedia.org/wiki?curid=24416", "title": "Maus (Computer)", "text": "Maus (Computer)\n\nDie Maus ist ein Eingabegerät (Befehlsgeber) bei Computern. Der allererste Prototyp wurde 1963 nach Zeichnungen von Douglas C. Engelbart gebaut; seit Mitte der 1980er Jahre bildet die Maus für fast alle Computertätigkeiten zusammen mit dem Monitor und der Tastatur eine der wichtigsten Mensch-Maschine-Schnittstellen. Die Entwicklung grafischer Benutzeroberflächen hat die Computermaus zu einem heute praktisch an jedem Desktop-PC verfügbaren Standardeingabegerät gemacht.\n\nDie Bewegung der Maus, ausgeführt mit der Hand auf dem Arbeitsplatz – gegebenenfalls mit einer geeigneten Unterlage, wie dem Mauspad – wird über einen Sensor in der Maus aufgenommen, digitalisiert und über eine Schnittstelle an den angeschlossenen Computer übertragen. Das Betriebssystem setzt diese zweidimensionale Bewegungsinformation in eine gleichartige Bewegung des Mauszeigers auf dem Bildschirm um. Durch Betätigung der Tasten oder zusätzlicher Elemente der Maus kann der Nutzer verschiedene Aktionen in dem Betriebssystem oder Anwendungsprogramm durchführen. Die Einführung der Computermaus kann als ein entscheidender Durchbruch in der Verbesserung der Benutzerfreundlichkeit von Computern angesehen werden. Im Jahre 2005 wurden schätzungsweise mehr als eine Milliarde Mäuse weltweit verkauft.\n\nDie Alternativen sind die Bedienung des Rechners über einen Trackball, einen Trackpoint, einen Touchscreen, ein Touchpad oder ein Grafiktablett.\n\n1963/1964 arbeitete ein Team um Douglas C. Engelbart und William English am Augmentation Research Center (ARC) des Stanford Research Institute (SRI) an verschiedenen experimentellen Zeigergeräten, unter anderem auch an einer Computermaus. Im Dezember 1968 wurde sie auf der Herbsttagung der American Federation of Information Processing Societies (AFIPS) der Öffentlichkeit präsentiert. Sie fand wenig Beachtung, da es noch keine grafischen Benutzeroberflächen gab und die Menschen, die mit Computern zu tun hatten, mit der Eingabe von Kurzbefehlen per Tastatur vertraut waren. Für das auf zwei rechtwinklig zueinanderstehenden Rädern basierende Prinzip beantragte Engelbart am 21. Juni 1967 ein Patent, das am 17. November 1970 als Patent US3541541 zugewiesen wurde.\n\nAm 2. Oktober 1968, also kurz vor Engelbarts Präsentation seiner Maus, veröffentlichte die deutsche Telefunken ein neues Eingabegerät für ihre TR-440-Rechner, das als „Rollkugelsteuerung für das SIG-100 am TR-86“ (kurz RKS 100-86) bezeichnet wurde. Erfinder dieser Steuerung war Rainer Mallebrein, entwickelt wurde sie um 1966. Bereits im Mai 1966 bewarb Telefunken die Rollkugelsteuerung in einem Prospekt zum TR-440. Auch wenn unklar ist, wer zuerst die Idee zu einer Maus hatte, so ist Telefunken in jedem Fall die erste Firma, die eine solche kommerziell herstellte und die im Gegensatz zu Engelberts Maus durch die Kugel eine freie Positionierung erlaubte. Das Prinzip blieb in allen nachfolgenden Kugelmäusen identisch. \n\nDie Weiterentwicklung der Engelbart’schen Maus erfolgte in den 1970er Jahren am Palo Alto Research Center (PARC) der Firma Xerox. 1971 verließ William English das SRI und wechselte zu Xerox PARC. Dort entwickelte er eine Kugelmaus, die vom Prinzip her identisch mit der Mallebrein'schen Maus ist (Kugel, Positionscodierung durch zwei Drehencoder). Sie wurde 1973 zum ersten Mal beim Xerox Alto eingesetzt, der auch erstmals eine grafische Benutzeroberfläche besaß. Durch seine Tätigkeit am PARC war auch Niklaus Wirth angeregt worden, im Laufe seiner weiteren Arbeit an der ETH Zürich eine grafisch orientierte Workstation mit Mausbedienung zu entwickeln. Die Lilith wurde 1980 vorgestellt. Kommerziell verwendet wurde die Maus 1981 im Rechner Xerox Star, doch dem System wurde kein wirtschaftlicher Erfolg zuteil, weil die Maus 400 US-Dollar kostete und die entsprechende Schnittstelle im Computer 300 US-Dollar.\n\nDer Computerhersteller Apple lizenzierte diese Technik, als Steve Jobs die Rechte von Xerox für 1000 $ erwarb und beauftragte das kalifornische Design- und Ingenieurbüro Hovey-Kelley Design (heute IDEO) mit der Entwicklung einer verbesserten, industriell herzustellenden Maus für 25 US-Dollar. Die von Apple und IDEO entwickelte Kugelmaus wurde zum vorherrschenden Funktionsprinzip für Mäuse während der 1980er und 1990er Jahre. Apple brachte diese Maus 1983 zusammen mit dem Rechner Lisa auf den Markt. Dieser hatte wegen seines hohen Preises keinen Markterfolg. Das Nachfolgemodell, der 1984 eingeführte Macintosh (SE Serie), war dagegen erfolgreich. Erstmals in der PC-Geschichte basierte die Mensch-Maschinen-Schnittstelle von Lisa auf dieser Maus, und zwar so exklusiv, dass der Computer ohne Maus praktisch nicht zu bedienen war. Dass der Apple Lisa mit seiner Mausbedienung einen Meilenstein darstellte, wird auch anhand eines enthusiastisch geschriebenen Erfahrungsberichts eines Computerredakteurs von 1983 deutlich:\n\nNoch bevor der Macintosh erschien, sah die Londoner Sunday Times im Januar 1983 „die Maus aus ihrem Loch kommen“:\n\n1985 brachte eine Ausgründung der École polytechnique fédérale de Lausanne (EPFL; Eidgenössische Technische Hochschule Lausanne), die Firma Logitech, die erste populäre Drei-Tasten-Kugelmaus LogiMouse C7 mit RS-232-Anschluss auf den Markt. Mit der Einführung der PS/2-Systeme durch IBM im Jahre 1987 wurden Mäuse mit PS/2-Anschluss vorgestellt. Die Maus wurde von IBM auch in Deutschland oft als „Pointing Device“ bezeichnet.\n\n1980 begann die Entwicklung optischer Mäuse. Steve Kirsch bei der Firma Mouse Systems und Richard Francis Lyon bei Xerox entwickelten unterschiedliche Ansätze. Deren Durchbruch kam aber erst mit günstigen und leistungsfähigen Chips zur Bildverarbeitung. Ende der 1990er Jahre begannen die optischen Mäuse, die auf Kugelmechanik basierenden Mäuse zu verdrängen. Ab Ende 1998 tauchten auch die ersten Mäuse auf, die über den 1996 im Wesentlichen von Intel spezifizierten USB-Anschluss mit dem Computer verbunden und in Windows 95 (OSR2.1), Windows 98 auf PCs oder MacOS auf Apple iMac betrieben werden können: Primax Navigator, Logitech Pilot.\n\nDas mitunter die Bewegungsfreiheit einschränkende Kabel führte zur Entwicklung drahtloser Mäuse. 1984 stellte Logitech eine Maus basierend auf Infrarottechnik vor. Seit 1991 sind kabellose Mäuse verfügbar, die über Funk mit dem Computer kommunizieren. Ende 2002 wurde von Microsoft und Logitech eine Maus vorgestellt, die über Bluetooth per HID-Profil mit dem PC kommuniziert, seit 2003 kann mit einem Produkt der Firma Belkin die Verbindung auch verschlüsselt werden.\n\n1995 stellte Genius die Mouse Systems ProAgio und die Genius EasyScroll vor, die zwischen den beiden Maustasten ein zusätzliches Scrollrad aufwiesen, um zum Beispiel innerhalb eines Fensters schneller auf- und abscrollen zu können. Seit der 1996 von Microsoft vorgestellten Intellimouse und der Unterstützung innerhalb der Microsoft-Software kommen derartige Mäuse auf einen höheren Marktanteil.\n\nSeit 1998 gibt es von Sun Microsystems Lasermäuse für die Sun Sparc Workstations. Im Herbst 2004 stellte Logitech zusammen mit Agilent Technologies im Markt der Personal Computer eine erste Lasermaus vor, die Logitech MX 1000 Laser. Die MX1000 erreichte eine Bildverarbeitung von 5,8 Megapixeln/Sekunde bei einer Auflösung von 800 dpi. Mittlerweile sind auch Lasermäuse anderer Hersteller mit mehr als 5600 dpi verfügbar, beispielsweise die Razer Mamba, eine Maus für Computerspiele (engl.: gaming mouse).\n\nSeit 2011 gibt es eine erste Maus – die M440 ECO – von Fujitsu, bei der Umweltaspekte eine Rolle spielen (Gehäuse aus 100 % ökologischem Kunststoff und PVC-freies Kabel). Die M440 ECO gleicht hinsichtlich Haltbarkeit und Ergonomie einer konventionellen Standard-Maus. Seit 2012 gibt es vom Verein Nager IT e. V. sogar eine Fairtrade-Maus. Diese wird nicht nur besonders ökologisch produziert, sondern es werden auch soziale und ethische Aspekte, wo immer möglich, in der gesamten Zulieferkette beachtet. Die Maus von Nager IT gilt in Fachkreisen als die ökologischste und „fairste“ Maus der Welt. Sie ist als Projekt mit dem Ziel konzipiert zur Nachahmung anzuregen und aufzuzeigen, wie sozial und ökologisch eine alltagstaugliche Maus produziert werden kann. Daneben gibt es auch von diversen Nischenherstellern z. B. Holz- oder Bambusmäuse, bei denen aber Kriterien wie sozial-ethische Herstellung und Umweltschutz nebensächlich sind oder Ergonomie und Haltbarkeit stark zu wünschen übrig lassen.\n\nDer Anwender bewegt die Maus auf einer glatten Oberfläche, die Bewegungsinformation wird an den Rechner übertragen. Über Betriebssystem-Routinen wird eine Markierung (Mauszeiger) auf dem Bildschirm entsprechend der Mausbewegung bewegt. Zumeist wird diese grafische Markierung als kleiner Pfeil dargestellt.\n\nDie Maus ist mit Tasten ausgestattet, die auf Tastendruck (Mausklick) eine für die entsprechende Software registrierbare Aktivität übermittelt. Bei einem solchen Ereignis werden normalerweise die aktuellen Bildschirmkoordinaten berechnet und eine entsprechende Reaktion ausgelöst. Beispielsweise kann ein Anwender auf ein Dateisymbol zeigen und es mit einem Tastendruck auswählen und aktivieren. Das Programm registriert das und hebt dieses Dateisymbol grafisch hervor. In einem Textverarbeitungsprogramm kann ein Anwender den Mauszeiger im Text bewegen und mit einem Tastendruck eine Schreibmarke (Cursor) darin platzieren. Wenn der Anwender zu tippen beginnt, wird der Text an dieser Stelle eingefügt.\n\nMan kann verschiedene Verfahren zur Aufnahme der Mausbewegung unterscheiden:\n\nDie ersten Mäuse funktionierten mit mechanischen Kontakten. In der allerersten Generation wurden noch Schleifkontakte zur Koordinatenermittlung verwendet, die jedoch starkem Verschleiß unterlagen, dafür aber sehr stromsparend auswertbar sind.\n\nEtwas später waren optomechanische Mäuse üblich, bei denen die Mausbewegungen über eine Rollkugel, zwei Lochscheiben und zugehörige Lichtschranken in elektrische Signale umgewandelt werden. Die Rollbewegung der Kugel wird über zwei Walzen auf zwei gelochte Segmentscheiben übertragen, aus deren Drehrichtung und Geschwindigkeit werden über Inkrementalgeber mit kleinen Lichtschranken elektrische Impulse („Mickies“) erzeugt. Die relativen Koordinaten zur Darstellung des Mauszeigers werden im Computer mit einer entsprechenden Software (Maustreiber) errechnet. Kugelmäuse sind jedoch anfällig für Verschmutzung, da die Kugel, eine meist mit Gummi überzogene Stahlkugel, immer wieder Partikel ins Mausinnere zieht und diese hauptsächlich an der Mechanik haften bleiben, was die Präzision und Wiederholgenauigkeit der Maus vermindern kann. Auch starke Sonneneinstrahlung kann manche Mäuse mit leicht transluzenten Gehäusen durch Störung der Lichtschranken beeinträchtigen. Vorteilhaft gegenüber optischen Mäusen mit einem bildverarbeitenden Prozessor ist der geringere Strombedarf (25 mA zu 100 mA bei einer optischen Maus) und der durch den Aufbau bedingte Umstand, dass auch der Betrieb auf einem transparenten Untergrund wie einer Glasplatte möglich ist.\n\nNeuere Mausgenerationen beleuchten die Oberfläche, auf der die Maus bewegt wird, mit einer eingebauten Lichtquelle, beispielsweise mit einer Leuchtdiode, und nehmen die Reflexionen mit einem optischen Sensor auf. Ein eingebauter Mikroprozessor berechnet aus den Unterschieden zwischen nacheinander aufgenommen Bildern Richtung und Geschwindigkeit der Bewegung der Maus. Man nennt diese Art „optische Maus“. Ausfallerscheinungen durch verschmutzte Kugeln und vor allem Rollachsen können konstruktionsbedingt nicht mehr auftreten. Dafür wird manchmal das optische Signal bei feinen Bewegungen auch auf gut strukturierten Unterlagen falsch interpretiert und der Zeiger falsch bewegt.\n\nDie ersten Mäuse dieser Art benötigten spezielle Mauspads, auf denen ein Gitter oder Punkte aufgezeichnet waren, an denen sich der optische Sensor orientieren konnte (System von Steve Kirsch, Mouse Systems). Mit höherer Leistung der in den Mäusen verbauten Mikroprozessoren können heute rechenintensivere Algorithmen zur Bildverarbeitung eingesetzt werden. So funktionieren moderne optische Mäuse auf fast allen Unterlagen. Nur Flächen, die eine sehr geringe oder keine Struktur aufweisen, z. B. Spiegel, Glas und viele lackierte Flächen, sind prinzipbedingt ungeeignet. Die hohe Präzision moderner optischer Mäuse macht sich besonders in grafischen Anwendungen und in Computerspielen positiv bemerkbar.\n\nDie Lasermaus stellt eine verbesserte Variante der optischen Maus dar. Dabei wird statt der normalen Leuchtdioden eine Laserdiode, die auf Infrarot-Technik basiert, als Lichtquelle eingesetzt. Das ergibt durch den Speckleeffekt einen besseren Kontrast auch auf sehr glatten Oberflächen, transparente Oberflächen bleiben jedoch problematisch.\n\nDie BlueTrack-Technologie, 2011 von Microsoft entwickelt und vorgestellt, soll die bisher gängigen Methoden der Bewegungsfeststellung ablösen. Eine große blaue Lichtquelle unterhalb der Maus soll von der Oberfläche in Kombination mit dem optischen Sensor und der Pixelgeometrie von Microsoft ein Bild mit besserem Kontrast machen können, sodass ein genaueres Tracking möglich ist. Dabei soll diese Technik auf weiteren, bis dato ungeeigneten Oberflächen funktionieren. Durchsichtiges Glas oder spiegelnde Oberflächen stellen aber weiterhin den wunden Punkt sämtlicher nicht-mechanischer Techniken dar.\n\nDie von Logitech entwickelte Darkfield-Technologie verwendet Laserdioden zur Bewegungserkennung, jedoch wird zusätzlich das Prinzip der Dunkelfeldmikroskopie eingesetzt, um selbst Verschmutzungen der Oberfläche und minimale Unregelmäßigkeiten zu erkennen und zur Bewegungsfeststellung zu verwenden. Vorteil dieser Technologie ist die Einsatzmöglichkeit auf bisher ungeeigneten Oberflächen wie Glas oder anderen spiegelnden Oberflächen mit einer vergleichsweise hohen Zuverlässigkeit.\n\nJe nach verwendetem Bewegungsaufnehmer (mechanisch-elektrisch, optomechanisch, optisch mit LED oder Laserdiode) unterscheidet sich die Empfindlichkeit der Maus, also die Strecke, die auf der Unterlage mit der Maus gefahren werden muss, um eine bestimmte Strecke mit dem Mauszeiger auf dem Bildschirm zurückzulegen. Dabei resultiert eine hohe Empfindlichkeit in einem kurzen Fahrweg auf der Unterlage. Einfluss hat darauf die manchmal bei Mäusen angegebene Auflösung in dpi: Je mehr Punkte auf einer bestimmten Strecke aufgelöst werden können, desto empfindlicher ist die Maus.\n\nManche Betriebssysteme und auch manche Programme bieten die Möglichkeit, die Empfindlichkeit individuell zu beeinflussen. Zusätzlich ist die Beschleunigung im neueren Betriebssystem einstellbar (sogenannte „Mausbeschleunigung“), wenn zusätzlich zur zurückgelegten Strecke die Bewegungsgeschwindigkeit ausgewertet wird. Bei Bewegung auf großen Flächen ist es hilfreich, wenn die große Strecke mit einer schnellen Bewegung überwunden werden und die genaue Annäherung an den gewünschten Punkt dann mit normaler Geschwindigkeit erfolgen kann.\n\nNeben der Fähigkeit, eine zweidimensionale Position zu übermitteln, können mit Mäusen über Tasten Aktionen ausgelöst werden. Hinter den Maustasten, mit denen ein Mausklick ausgelöst wird, verbergen sich meist Mikrotaster, die bei Überschreitung einer bestimmten Kraft einen Kontakt schließen. Diese Änderung wird als Bit in einem Teil des Mausprotokolls an den Rechner übertragen und löst über Maustreiber, Betriebssystem und das Anwendungsprogramm eine damit verbundene Aktion aus. Die Taster weisen oft eine Art Knackfroscheffekt auf: Bei Überschreitung der erforderlichen Kraft erhält der Benutzer sowohl eine taktile als auch eine akustische Rückmeldung der Betätigung (daher auch der Begriff Mausklick).\n\nDie erste Maus von Engelbart hatte lediglich eine Taste. Xerox konstruierte schon früh eine Variante mit drei Tasten. Apple nutzte wieder nur eine Taste. Viele Mäuse an Unix-Workstations besaßen drei Tasten. Im PC-Bereich waren lange Zeit Mäuse mit zwei Tasten dominierend. Jüngere Modelle haben oft zusätzliche Tasten, die fest programmierte Funktionen haben oder deren Funktionalität sogar frei programmiert werden kann.\n\nEine weitere Entwicklung war das Rollrad (Wheel-Maus). Dieses Rad hat meist die Funktion, bequemes Scrollen zu ermöglichen, kann jedoch auch anders belegt sein. Bei den meisten Modellen ist das Scrollrad heutzutage klickbar und fungiert somit als mittlere Maustaste. Manche Modelle haben zwei Räder, um gleichzeitiges horizontales und vertikales Scrollen zu ermöglichen.\nDie Tasten werden vom System über Eventnummern (button) abgefragt. Eine typische Standardbelegung unter Linux und Windows ist:\n\nWeitere Maustasten sind in der Regel nutzlos, bis ein zugehöriger Gerätetreiber installiert wird.\n\nKabellose Mäuse übertragen ähnlich wie Funktastaturen ihre Informationen nicht mehr durch das manchmal störende Kabel. Stattdessen werden die Daten via Infrarot (selten) oder Funk (beispielsweise Bluetooth oder in einem anderen ISM-Band [2,4 GHz ist heute das meistgenutzte]) von der Maus zu einer Basisstation übertragen. Sofern die Basisstation nicht bereits fest im Computer verbaut ist, wird das Signal dann per Kabel über die serielle, PS/2- oder eine USB-Schnittstelle an den Computer weiterleitet.\n\nKabellose Mäuse benötigen eine eigene Stromversorgung, üblicherweise durch Batterien oder Akkumulatoren. Dadurch entsteht ein etwas höheres Gewicht der Maus und die Notwendigkeit des gelegentlichen Batteriewechsels oder Nachladens. Neuere höherwertige Modelle kommen nach Herstellerangaben Monate bis Jahre ohne Batteriewechsel bzw. Nachladen aus. Während manche zum Laden in eine Ladestation gestellt werden und somit während des Ladens nicht genutzt werden können, gibt es auch Modelle, die über ein Kabel geladen werden und auch während des Ladens einsatzfähig sind. Es sind auch kabellose Mäuse ohne interne Stromversorgung verfügbar, die per Induktion von einem speziell mitgelieferten, an einem USB-Anschluss angeschlossenes Mauspad mit Energie versorgt werden.\n\nKabellose Mäuse können, sofern die Daten nicht verschlüsselt werden, meist relativ leicht abgehört werden und haben teilweise eine geringfügig längere Reaktionszeit als kabelgebundene Modelle.\n\nDiese Art von Maus ist speziell für Computerspiele entwickelt worden. Ein grundlegendes Merkmal von Gaming-Mäusen ist der Verbau eines optischen Hochleistungssensors wie z. B. PixArts Modell PMW3360. Dieser hochauflösende Sensor verfügt über eine Punktdichte von 12.000 DPI, die meist durch Software oder Hardware eingestellt werden kann. Die Form der Maus ist entweder ergonomisch oder beidhändig gestaltet, wobei die beidhändige Form symmetrisch ist und sich für Rechts- sowie Linkshänder eignet. Die ergonomische Form hingegen soll eine natürliche Halteposition für die Hand des Nutzers bereitstellen und ist entweder für Rechtshänder oder Linkshänder vorgesehen. Einige Gaming-Mäuse, welche vor allem für MMORPGs konzeptioniert wurden, verfügen über eine höhere Anzahl von Seitentasten um die Bedienung von zahlreichen Fähigkeiten zu vereinfachen. Im Shooter Genre wird viel Wert auf Gewicht der Maus gelegt, wobei ein niedriges Gewicht das Handgelenk bei längerem Gebrauch entlastet, und schnelle Bewegungen mit weniger Kraftaufwand erledigt werden können. Unter den Gaming-Mäusen gibt es eine diverse Auswahl von Produkten, welche sich in Qualität, sowie Preis unterscheiden. Oft werden Gaming-Mäuse durch ihre extravaganten Funktionen wie z. B. RGB-LEDs klassifiziert. Diese sind jedoch nicht ausschlaggebend für die tatsächliche Leistung der Maus.\n\nDa die Maus erst in den späten 1980er Jahren ihren Weg zu den IBM-PC-kompatiblen Rechnern gefunden hat, musste dort im Gegensatz zu Computern, die im Grundkonzept bereits eine Maus vorgesehen hatten, beispielsweise Macintosh, Amiga, Atari ST, erst eine geeignete Schnittstelle gefunden werden.\n\nAnfangs wurden Mäuse für IBM-kompatible Computer über eigene Schnittstellenkarten („Busmaus“) betrieben. Die Anschlussbelegung des Mauskabels war nicht standardisiert, Maus und Karte mussten daher zusammenpassen.\n\nEine verbreitete Methode war der Anschluss am seriellen Port (RS232) über einen neun- oder 25-poligen D-Sub-Stecker. Diese serielle Schnittstelle war ursprünglich für die Datenfernübertragung mit Fernschreibern, Modems und Akustikkopplern entwickelt worden. Da die Maus ein einfaches und in der Datenübertragung langsames Gerät ist, das nur für Koordinateninformation und Status der Tasten Daten übermittelt und außerdem seinen Strom mittels eines dafür zweckentfremdeten Handshake-Pins über die Schnittstelle erhalten konnte, war diese Schnittstelle eine recht langlebige Lösung, bei der die Maus auch problemlos im laufenden Betrieb angeschlossen werden kann. Diese serielle Lösung hielt sich bei PCs seit Mitte der 1980er Jahre über zehn Jahre lang und verschwand erst ab 1996 allmählich mit dem Aufkommen der ATX-Hauptplatinen und der PS/2-Maus (siehe unten). Ein Nachteil aus Herstellersicht besteht in den für RS-232 verwendeten hohen Spannungen von +/− 12 V statt der im PC-Inneren üblicheren 0 V / 5 V, wodurch sich der Schaltungsaufwand erhöht. Frühe Modelle benötigten gar noch ein eigenes Netzteil.\n\nFrühe Apple-Computer-Modelle der Apple-II-Baureihe waren noch ohne Maus konzipiert worden. Für diese kam nur ein Busmouse-Konzept mit einer speziellen Steckkarte für den Mausanschluss zum Einsatz. Die Apple-Macintosh-Rechner stellten bereits bei ihrer Einführung einen eigenen (proprietären) Anschluss für die Maus bereit. Zunächst (Ur-Macintosh, Macintosh 512 und Macintosh Plus) wurden die unverarbeiteten Signale der Achsen und der Taste über einen neunpoligen D-Sub-Stecker übertragen, der 1987 beim Macintosh II und Macintosh SE durch den universelleren Apple Desktop Bus (ADB) ersetzt wurde. Mit dem ADB wurde die Maus an die Tastatur und die Tastatur mit einem zweiten ADB an den Computer angeschlossen. Auch andere Kleingeräte wurden mittels ADB angeschlossen. Der ADB wurde bis zum Power Macintosh G3 von 1998 verwendet, bei dem Apple den ADB durch die USB-Schnittstelle ablöste.\n\nAuch bei SUN Unix Workstations war bis etwa 2003 die Maus mit der Tastatur verbunden, und beide Geräte wurden mit einem gemeinsamen Kabel an die Workstation angeschlossen (SUN Ultra).\n\nDie Firma Commodore verwendete für ihre Amiga-Computer ähnlich wie frühe Apple-Modelle eine Schnittstelle, die unverarbeitete Signale über einen neunpoligen D-Sub-Stecker überträgt. Die Pinbelegung orientierte sich dabei jedoch am Atari-2600-„de-facto-Standard“ für Joystickbuchsen, damit die gleichen Buchsen Mäuse und Joysticks unterstützen konnten. Für den Commodore 64 und den Commodore 128, bei denen zunächst keine Maus vorgesehen war und es somit auch keine dafür vorgesehene Empfängerhardware im Gerät gab, wurde eine Maus entwickelt, die ihre Signale analog über die Paddle-Schnittstelle übertrug (Commodore 1351) sowie eine Maus, die ihre Signale digital über die Joystick-Schnittstelle übertrug (Commodore 1350).\n\nBei IBM-kompatiblen Rechnern setzte sich mit dem breiten Aufkommen der mausgesteuerten Betriebssysteme mit dem PS/2-Anschluss eine gesonderte Schnittstelle nur für die Maus durch. Diese wurde bereits 1987 durch IBM in den technischen Referenzhandbüchern zum PS/2-System definiert. Die Übertragungsprotokolle und die Pinbelegungen von Tastatur und Maus sind dabei identisch: Es handelt sich um ein synchrones, serielles Protokoll, das ursprünglich speziell für Computer-Tastaturen entwickelt wurde, und arbeitet mit einer 5-Volt-Spannungsversorgung.\n\nDie PS/2-Schnittstelle ist anders als die serielle und die USB-Schnittstelle nicht Hotplug-fähig spezifiziert: Eine Maus muss bereits beim Einschalten des Computers daran angeschlossen sein, um verwendet werden zu können. Ein Ausstecken während des Betriebs führt regelmäßig zum Systemhalt und kann sogar die Schnittstelle zerstören. Besonders transportable Computer mit PS/2-Schnittstelle unterstützten Hotplug oftmals dennoch.\n\nAuch wenn die Pinbelegung für die grundsätzliche Kommunikation von Tastatur und Maus identisch ist, sind die Anschlüsse eindeutig zugeordnet. Eine Maus arbeitet nicht am Tastaturport, selbst wenn der Stecker passt. Teilweise haben weitere Pins eine Sonderbelegung, beispielsweise bei der Tastatur zum Ausschalten des Computers. Zur Verdeutlichung hat sich mit der Zeit eine farbliche Kodierung der Anschlüsse durchgesetzt: Anschlussstecker und -buchse sind für die Maus türkis, für die Tastatur violett gefärbt.\n\nHeute hat sich der universell einsetzbare USB-Anschluss für Mäuse und andere Peripherie für IBM-kompatible Rechner, Unix-Workstations (SUN, IBM) sowie für Apple-Macintosh-Rechner gegenüber dem PS/2-Anschluss weitgehend durchgesetzt. Moderne PCs verfügen selten, Notebooks fast nie, über einen PS/2-Anschluss. Für die Übertragung werden beim USB-Anschluss neben Masse- und 5-V-Versorgungsspannungsleitung zwei Datenleitungen verwendet, über die Daten differenziell übertragen werden. Außerdem sind Geräte am USB-Anschluss durch die Reihenfolge, in der die Verbindungen beim Steckvorgang hergestellt werden (zuerst Masse und Spannung), dafür ausgelegt im laufenden Betrieb entfernt oder wieder angeschlossen werden zu können (Hotplug).\n\nUnabhängig von der verwendeten physikalischen Schnittstelle findet eine serielle Datenübertragung zwischen dem Computer und der daran angeschlossenen Maus statt. Dabei werden verschiedene Verfahren zur Übertragung benutzt, die als Protokolle bezeichnet werden. Abhängig vom verwendeten Protokoll und der Maus muss innerhalb des Betriebssystems auf dem Computer der entsprechende Maustreiber installiert sein, über den gegebenenfalls auch Anpassungen für Empfindlichkeit und Beschleunigung der Maus vorgenommen werden können.\n\nBei einer Busmaus oder Busmouse sitzt die komplette Elektronik zur Dekodierung von Mausbewegung und Tastenbetätigung auf einer PC-Einsteckkarte für den ISA-Bus. Es werden neun Signale über einen Neun-Pin-Mini-DIN Adapter (auch Hosiden-Adapter genannt) übertragen. Neben dem Massepotential sind das die Kontaktsignale der drei Mausschalter und jeweils zwei für die versetzten Lichtschranken der Drehsensoren für X- und Y-Richtung. Diese Mäuse waren zum Beispiel von Logitech, Microsoft oder ATI zusammen mit den passenden ISA-Bus-Steckkarten erhältlich und sind heute kaum mehr im Betrieb.\n\nFür Mäuse mit serieller Schnittstelle nach RS-232 können Anschlüsse mit neun- oder 25-poligen D-Sub-Buchsen am Mauskabel verwendet werden. Die Signale Masse, RxD, TxD, RTS, DTR werden benutzt, CTS und DSR werden nicht genutzt, dürfen aber auch nicht gebrückt sein. Aus DTR und RTS wird in der Maus die positive Versorgungsspannung für die Leuchtdioden der Bewegungssensoren und zur Bedienung der Schnittstelle gewonnen, über DTR erfolgt auch ein Reset. TxD liefert die negative Spannung. Die verwendeten Signalspannungen liegen entsprechend RS232 zwischen −12 V und +12 V. Die Daten werden bei jeder Zustandsänderung, etwa durch Mausklick oder Bewegung, über den Anschluss RxD asynchron mit 1200 bit/s von der Maus an den Computer übertragen.\n\n\nEine PS/2-Maus wird an einem dem PS/2-Tastaturanschluss vergleichbaren, oftmals grünen sechspoligen Mini-DIN-Anschluss angeschlossen und über ein serielles, bidirektionales, synchrones Protokoll angesteuert. Diese Aufgabe übernimmt der Tastaturcontroller bzw. der Eingabegerätecontroller.\n\nAn IBM-kompatiblen PCs angeschlossene Mäuse verwenden in der Regel nicht mehr als vier physikalisch verbundene Drähte: 5-Volt-Speisespannung (maximal 275 mA Last), Masse, eine Daten- und eine Taktleitung. Takt- und Datenleitung werden von der Rechner- und der Mausseite über Open-Collector-Treiber angesteuert, der Ruhepegel liegt auf 5 Volt. Maus und Computer können jede der beiden Leitungen auf den Pegel von 0 V ziehen. Die Seite, die die Taktleitung auf 0-Volt-Pegel zieht, kann gültige Daten über die Datenleitung übermitteln. Das Taktsignal zwischen 10 kHz und 16,7 kHz wird von der Maus erzeugt. Sie darf nur Daten senden, wenn das Taktsignal nicht vom Computer zur Unterbrechung der Kommunikation auf 0 V gelegt wurde.\n\nDie Übertragung erfolgt mit Startbit (immer 0), acht Datenbits (niedrigstwertiges Bit zuerst), ungerader Parität und einem Stopbit (immer 1), Daten werden bei hohem Pegel der Taktleitung auf die Datenleitung geschrieben und nach Pegelabfall des Taktes vom Computer gelesen. Daten werden vergleichbar dem Microsoft-Protokoll in drei Acht-Bit-Zeichen, aber unverschachtelt übermittelt. Zusätzlich zu den im Microsoft-Protokoll für seriell angeschlossene Mäuse enthaltenen Daten werden noch ein Vorzeichen- und ein Überlaufbit jeweils für X- und Y-Wert übertragen. Der Computer kann verschiedene Befehle an die Maus übermitteln und sie in verschiedene Übertragungsmodi versetzen: „Streammode“ (Standard: jede Änderung wird übertragen), „Remote Mode“ (Änderungen werden nur auf Abfrage übertragen), „Reset Mode“ und „Wrap Mode“ (Echo-Modus)).\n\nDurch Befehle des Computers, genauer: des Tastatur-/Mauscontrollers, lassen sich auch Auflösung (Schritte/mm), Abtastrate (Abtastungen/s) und Skaling (Vergrößerungsfaktor der übermittelten Zählerstände) der Maus beeinflussen. Eine angeschlossene PS/2-Maus wird während des Bootens des IBM-kompatiblen PCs erkannt und kann normalerweise im laufenden Betrieb nicht entfernt, neu angeschlossen und benutzt werden, ist also nicht Hotplug-fähig.\n\nFür die Benutzung der Intellimouse hat Microsoft das PS/2-Protokoll auf ein Vier-Byte-Paketformat zum IMPS/2-Protokoll erweitert. Im vierten Datenpaket werden die Bewegungsinformationen des Scrollrades und die Zustände der beiden zusätzlichen Tasten übermittelt. Die Intellimouse verhält sich zum Zeitpunkt des Einschaltens wie eine PS/2-Maus, übermittelt aber nach Reset eine andere Device-ID. Diese bewirkt, dass der Maustreiber die dann übermittelten Vier-Byte-Pakete verarbeitet.\n\nEinige PS/2-Mäuse können über einen zugehörigen Adapter auch an einem seriellen Anschluss betrieben werden: die PS/2-Serial Maus. Da diese Adapter jedoch nicht zwischen den unterschiedlichen Pegeln und Protokollen wandeln können, muss die Elektronik in der Maus erkennen, an welchem Anschluss sie betrieben wird und sich darauf einstellen. Das wäre etwa über die Versorgungsspannung möglich, die am seriellen RS-232-Anschluss höher ist.\n\nAn den USB-Bus angeschlossene Mäuse verwenden im Gegensatz zu PS/2-Mäusen kein proprietäres Protokoll mehr, sondern ein vom USB Interface Forum standardisiertes Busprotokoll. Die Daten werden seriell, differentiell auf den beiden Datenleitungen mit Sync-Signal, NRZI-Kodierung und Bit-Stuffing übertragen, daher ist eine separate Taktleitung wie bei PS/2-Mäusen nicht erforderlich.\n\nDer USB-Gerätetreiber muss sicherstellen, dass häufig genug (siehe USB Software-Architektur) über einen USB-Treiber, den Treiber des USB-Hostcontrollers und den USB-Hostcontroller selbst der Status des USB-Slave-Clients, in diesem Falle: der Maus, abgefragt wird. Die Maus wird über eine vom USB-Hostcontroller nach Identifizierung vergebene sieben Bit lange Kennung als „Human Interface Device“(HID)-Gerät adressiert (siehe USB-Konfiguration) und als solches nach der Norm USB 1.0 bedient. Bei der Initialisierung informiert sie den Hostcontroller oder den dahinterliegenden USB-Treiber aus ihrem Pufferspeicher 0 über ihre Fähigkeiten und Eigenschaften (Anzahl: maximal vier, Richtung: in oder out, Abfragehäufigkeit, die „Sample Rate“, und Größe der Pufferspeicher der sogenannten Endpunkte: maximal 64 Byte, Geräteart, Hersteller, „Class Code“, Gerätekennung, Protokoll, benötigte Bandbreite und anderes). Dieser speichert die Informationen und reserviert die entsprechenden Zeitslots auf dem Bus. Dann fragt der Hostcontroller im Auftrag des Gerätetreibers, der die Abfragen beim USB-Hostcontroller-Treiber in die Queue einstellt, im Interrupt-Transfer-Modus alle 10 ms die zu übermittelnden Daten aus den Endpunkten ab, die Übertragung erfolgt mit höchstens acht Byte pro Transfer prüfsummengesichert (CRC16) von der Maus an den Computer. Der Endpunkt bildet also faktisch im Hostdevice (der Maus) einen gemeinsam von Hostcontroller (dem Computer) und Hostdevice (der Maus) einsehbaren Speicherbereich, der innerhalb eines garantierten Zeitabstandes regelmäßig vom Computer ausgelesen wird. Falls bei IBM-kompatiblen PCs das BIOS Legacy-Unterstützung bietet, können USB-Mäuse durch Emulation des 8042-Tastaturcontrollers über den USB-Hubcontroller wie PS/2-Mäuse benutzt werden. Seit Einführung des Apple iMac ist USB der Standard-Eingabegeräteanschluss für Apple-Macintosh-Computer.\n\nVergleichbar den PS/2-Serial-Mäusen gibt es auch solche, die sich wahlweise an einem USB- oder einem PS/2-Anschluss betreiben lassen. Dafür gibt es Adapterstecker, die mit der Maus mitgeliefert werden, und vom USB-Anschluss auf den PS/2-Anschluss oder umgekehrt umsetzen. Die Mitlieferung eines Adapters lässt dabei einen Rückschluss auf die Fähigkeiten der Maus zu, denn auch hier muss sie anhand der gegebenen Verhältnisse entscheiden, ob sie sich wie eine PS/2-Maus oder wie eine Maus am USB-Anschluss verhält. Kriterium dabei kann z. B. das Verhalten der Datenleitungen nach dem Einschalten sein: Bei einem PS/2-Rechner darf man davon ausgehen, dass zu einem bestimmten Zeitpunkt nach dem Einschalten zur Übermittlung des Reset-Befehls die Takt- und Datenleitung nach einem bestimmten Verfahren umgeschaltet wird, am USB ist von differentieller Datenübertragung auszugehen, was nicht den PS/2-Verfahrensweisen entspricht. Eine Maus oder Tastatur, die ohne einen USB-zu-PS/2-Adapter ausgeliefert wird, kann in der Regel auch mit einem nachgerüsteten Adapter nicht an einem PS/2-Anschluss betrieben werden, denn das Gerät muss diese Funktion von vornherein unterstützen.\n\nMit dem Apple Desktop Bus (ADB) hatte Apple bereits 1986 ein ähnliches Konzept wie beim USB verfolgt, wenn auch in kleinerem Maßstab. Verwendet wird ein vierpoliger Mini-DIN-Stecker, ähnlich einem S-Video-Stecker. Belegt sind normalerweise drei Verbindungen: 5 V Speisespannung, Masse und die Datenleitung. Eine vierte, zusätzliche Verbindung zum Netzteil des Computers blieb für Mäuse unbenutzt. Die Verbindung ist nicht für Hotplug-Funktion vorgesehen, weil für die einwandfreie Funktion nach dem Anstecken einer Maus der ADB initialisiert werden muss. Diese Initialisierung wird vom Betriebssystem nur während des Startvorgangs automatisch durchgeführt, kann aber mittels eines zusätzlichen Programms auch ohne Neustart des Rechners durchgeführt werden. Die Steckverbindung ist in der serienmäßigen Ausführung nur für maximal 400 Steckvorgänge ausgelegt. Das ist verglichen mit den heutigen USB-Verbindungen sehr wenig. Adressiert werden konnten bis zu 16 Geräte, die Datenübertragungsrate beschränkt sich auf 10 kBit/s. Die Steuerung erfolgt ausschließlich durch den Computer und die Geräte (in diesem Fall die Maus) liefern Daten nur bei Abfrage über einen definierten Speicherbereich (Register, vergleichbar dem Endpunkt) zurück. Die Definition erlaubte auch gleiche Adressen am Bus, was gelegentlich zu Problemen führte.\n\nBei den ersten IBM-kompatiblen PCs mussten der Maus Hardware-abhängige Ressourcen zugewiesen werden. Die Steckkarte der Busmaus erfordert in der Regel die Zuweisung eines eigenen der wenigen verfügbaren Interrupts sowie eines passenden I/O-Adressbereiches (Port). Die Einstellung erfolgt über Jumper oder etwas moderner im BIOS des Rechners.\n\nSeriellen Mäusen ist in der Regel eine eigene serielle, physikalisch vorhandene Schnittstelle inklusive einer Schnittstellengeschwindigkeit und -protokoll (beispielsweise 8N0, 7N1) zuzuweisen. Das wird genauso wie die Angabe des notwendigen, passenden Maustreibers mit seinen Optionen über Konfigurationsdateien vorgenommen.\n\nBei PS/2-Mäusen und bei USB-Mäusen entfallen derartige Angaben, da diese Schnittstellen bereits in der Hardware des Rechners oder aufgrund des verwendeten Protokolls arbeitsfähige Vorgaben zur Verfügung stellen. Spätestens bei seit Ende der 1990er Jahre ausgelieferter Hardware und Betriebssystemen muss sich der Anwender nur noch selten mit derartig hardwarebezogenen Details befassen.\n\nDafür hat aber die Anzahl der Einstellungsmöglichkeiten über Maustreiber und Betriebssystem erheblich zugenommen. Erwähnt sei nur die bereits beschriebene Empfindlichkeitseinstellung der Maus, Zeitabstand für Doppelklick sowie die Zuweisung von programmabhängigen Funktionen zu zusätzlichen Maustasten. Dieses sind aber weitestgehend nur Anpassungen, um dem Anwender die Benutzung angenehmer zu gestalten. Die grundlegende Funktion der Maus (also Zeigen und Klicken) ist in der Regel auch ohne derartige Anpassungen gegeben.\n\nJede gängige grafische Benutzeroberfläche für Computer, die zurzeit für Endanwender existiert, kann mit der Maus bedient werden. Die übliche Anzahl der Maustasten und weiterer Elemente zur Interaktion (Scrollrad) hat sich im Laufe der Zeit gewandelt:\n\n\nEnde der 1990er Jahre hat sich bei Computermäusen das sogenannte „Scrollrad“ etabliert. Es befindet sich meist zwischen den beiden Maustasten und dient zum Auf- und Abrollen des Fensterinhalts. Darüber hinaus wird es mitunter auch zur Einstellung von grafisch simulierten Schiebereglern eingesetzt. Viele Mäuse verknüpfen das Scrollrad mit der Funktion einer dritten Taste, sodass ein Druck auf das Rad das entsprechende Signal an den Computer gibt. Bei einigen Modellen kann das Scrollrad zusätzlich nach links oder rechts bewegt werden, um auch horizontales Scrollen des Fensterinhalts per Maus zu ermöglichen. Im Juli 2005 stellte Apple nach fast drei Jahrzehnten konsequenter Ein-Tasten-Maus-Philosophie erstmals die USB-Mehrtastenmaus Mighty Mouse vor, die mit den Betriebssystemen Windows XP und Mac OS X gleichermaßen kompatibel ist und neben drei zusätzlichen, programmierbaren Tasten eine neuartige 360-Grad-Scrollkugel bietet, die freies vertikales wie auch horizontales Scrollen (letzteres jedoch nur in Mac OS X) ermöglicht.\n\nEtwa die Hälfte seiner Arbeitszeit hat der durchschnittliche Computernutzer Kontakt mit der Maus. Eine 2010 veröffentlichte niederländische Studie ermittelte folgende durchschnittliche Zeitanteile:\n\nDurch Bewegen der Maus kann der Mauszeiger an die gewünschte Stelle bewegt werden, und durch Betätigen einer Maustaste kann der Benutzer Aktionen auslösen. Die Funktion der Maustasten und der Mausbewegung in einem Programm oder Betriebssystem lässt sich oft betriebssystemabhängig durch Betätigung zusätzlicher Tasten auf der Tastatur des Computers modifizieren, beispielsweise über die Apple-Taste, die Umschalt-, Ctrl- oder Alt-Taste. Im Wesentlichen unterscheidet man drei verschiedene Mausaktionen:\n\n\nWeitere Mausaktionen sind:\n\nJe nach Programm und Zustand im Programm kann eine Mausaktion Unterschiedliches bewirken: Ein Klick kann die Einfügemarke in einem Text bewegen, ein Menü öffnen oder beim Klick auf eine Schaltfläche eine Programmfunktion auslösen. Ein Doppelklick kann ein Programm starten oder ein Wort in einem Text markieren.\n\nFür spezielle Anwendungen und Einsatzorte haben sich Variationen der Standardmaus oder auch alternative Zeigegeräte etabliert. Mäuse für Anwendungen, in denen viel quer gescrollt werden muss, verfügen über ein zusätzliches, horizontal wirkendes Scrollrad, oder gar einen kleinen Scrollball. Zur Vermeidung von maustypischen gesundheitlichen Problemen kann der zur Maus verwandte Trackball verwendet werden, bei dem die früher übliche Mauskugel anstatt auf der Unterseite an der Oberseite angebracht ist und mit den Fingern bewegt wird, während das Gerät selbst an seinem Platz bleibt. Dadurch eignet er sich auch für den Einsatz an beengten Arbeitsplätzen. Eine weitere Alternative stellt die vor der Tastatur liegende Rollstangenmaus (englisch: \"Barmouse\") dar.\n\nAn transportablen Rechnern (Notebook, Laptop) sind meist berührungsempfindliche Touchpads und bei höherwertigen Geräten (meist Business-Rechner) zusätzlich Trackpoints fest eingebaut. Freilich ist der Zeigekomfort einer echten Maus größer, sodass viele Anwender oft noch eine zusätzliche kleine Notebook-Maus anschließen.\n\nTouchscreens sind Bildschirme, die dank ihrer Berührungsempfindlichkeit zugleich als Eingabegerät dienen und somit ein separates Zeigegerät überflüssig machen, gegebenenfalls auch die Tastatur. Für Computer im öffentlichen Bereich wie Informationsterminals und Bankautomaten sind Touchscreens schon seit Jahren eine Alternative zu fest eingebauten Tastaturen. Bei Smartphones haben sich kleine Touchscreens in letzter Zeit (2010) etabliert und beginnen sich seit dem ersten Massenmarkt-Einsatz im iPad auch in größeren, PC-typischen Auflösungen durchzusetzen.\n\nIm professionellen Grafikdesign werden oft Grafiktabletts eingesetzt.\n\nFür Konstruktion und Robotik ist die Benutzung einer 3D-Maus möglich, die neben der Bewegung in der Ebene eine weitere Bewegungsdimension sowie Rotationen um die Raumachsen erfasst.\n\nIm Spielebereich kann man unter anderem elektronische Lenkräder, Joysticks, Gamepads als Mausalternative betrachten.\n\nBei körperlichen Einschränkungen können Mehrfachsensoren die Funktion der Maus (Zeigen und Klicken) nachbilden. In schwierigen Fällen sind auch aufwendige Lösungen durch Kameraaufnahmen von Kopf- und Augenbewegungen zur Computerbedienung möglich, die dann aber nicht nur die Mausfunktion ersetzen.\n\nIm industriellen Umfeld wird eine Vielzahl von Alternativen zur herkömmlichen Maus eingesetzt, da hier die Anforderungen durch die Anwendung und die Umgebung bestimmt werden. Häufig werden hier staub- oder wasserdichte Geräte oder sterilisierbare Oberflächen gefordert.\n\nDie Funktion der Maus kann weitgehend durch Benutzung der Tastatur ersetzt werden, obwohl das zwar nicht immer intuitiv ist, aber bei Erfassen großer Datenmengen produktiver ist als der ständige Wechsel zwischen Maus und Tastatur. Dabei ist manchmal die gleichzeitige Betätigung mehrere Tasten, etwa Alt-Esc zum Menüaufruf, oder die häufig wiederkehrende Betätigung bestimmter Tasten zur Bewegung auf dem Bildschirm, etwa mit Tab von einem Bildschirmsymbol zum nächsten Springen, notwendig. Auch die sogenannte Tastaturmaus ist in diesem Zusammenhang zu erwähnen, die die Bewegung des Mauszeigers über die Zweitbelegung des numerischen Tastenblocks durchführt.\nBevor sich die Maus durchsetzte, wurden die Benutzeroberflächen von Programmen (abgesehen von reinen Kommandozeilen-Steuerbefehlen) mit einer Vielfalt von Tastaturbefehlen gesteuert. Manche Programme verfügten über Menüs und Ausklappmenüs, die per Tastatur geöffnet und bedient wurden. Beispielsweise wurde mit dem Tabellenkalkulations-Programm Lotus 1-2-3 eine fette Linien-Umrandung am oberen Rand eines Bereichs mit folgender Tastenfolge erzeugt:\n\nwährend bei Verwendung der Maus folgende Klicks erforderlich waren:\n\nZuerst die Markierung des zu formatierenden Bereichs, Klick im Menü oben \"Bereich\" und dann \"Bereich:Eigenschaften\", Klick auf die Icon-Schaltfläche \"Linien\", Klick auf das Icon \"Linienstil\", Klick auf das Icon \"Linienfarbe\", Klick auf das Abbruch-Icon, Klick ins Arbeitsblatt, um die invers dargestellte Markierung auszuschalten und das Ergebnis zu betrachten.\n\nMit der Tastatur wurden also 8 Tasten meist im Zehnfinger-Blindschreiben ohne Hinsehen betätigt, mit der Maus dasselbe mit 8 Klicks erledigt, wobei der Mauszeiger über den Bildschirm zu führen ist. Der Vorteil der Maus liegt in ihrer ausgesprochen intuitiven Verwendungsweise als Zeigegerät: Anhand der darauf basierenden Schaltflächen erlaubte sie es, die Zeichenformatierungs-Möglichkeiten in textbasierten und grafischen Anwenderprogrammen auszuweiten, ohne zunehmend unübersichtliche und umfangreiche Ausklappmenüs einrichten und sich merken zu müssen.\n\nSolange in den Anfangstagen die Maus oft nur zur Aktivierung von Fenstern und Auswahl von Checkboxen benutzt wurde, war ihre Form weitgehend nebensächlich. Dies änderte sich jedoch mit dem Aufkommen von grafischen Benutzeroberflächen deutlich. Der andauernde Gebrauch einer Computermaus, insbesondere bei Fehlhaltungen, kann Schmerzen im Handgelenk (Sehnenscheidenentzündung) oder das Karpaltunnelsyndrom auslösen. Ebenso können Schmerzen an den längere Zeit unnatürlich gestreckten Fingern auftreten, bekannt als sogenannter „Mausfinger“. Wenn die Symptome über den Handbereich hinaus auf Ellenbogen bis in die Schulter reichen, spricht man umgangssprachlich von einem „Mausarm“. Diese beiden Belastungsformen werden zusammengefasst als RSI bezeichnet (Repetitive Strain Injury Syndrom – Verletzung durch wiederholte Beanspruchung). An Computerarbeitsplätzen entstandene Krankheiten können eine Berufskrankheit darstellen.\n\nVorbeugend gegen solche Beschwerden können die abwechselnde Verwendung unterschiedlicher Eingabegeräte wie Maus, Trackball und Rollstangenmaus sowie der Einsatz ergonomisch gestalteter Tastaturen bis hin zur Sprachsteuerung wirken. Als Abhilfe wurden auch ergonomischere Formen für die Maus selbst entwickelt, um sie den natürlichen Haltungen der Hand und möglichst belastungsfreien Bewegungsabläufen von Fingern, Gelenken und Arm anzupassen. Beispielsweise wird die Zeigefinger- oder Ballenseite der Maus oft länger gestaltet. Entwickelt wurden auch alternative Formen wie die Vertikalmaus, bei der die Tasten in vertikaler Ebene angeordnet sind, was eine entspanntere Armhaltung bedingt, da der Arm nicht zur Körperseite hin verdreht werden muss. Bei derart ergonomisch gestalteten Mäusen ist es natürlich erforderlich, seitenverkehrte Ausführungen für Rechts- und Linkshänder zu verwenden. Auch die Benutzung von Handballenauflagen verspricht Erleichterung durch Entlastung des Handgelenks, das weniger stark nach oben abgewinkelt werden muss.\n\nBei zeitlich angemessener Nutzung von \"optischen Funkmäusen\" ist eine für den Menschen gefährliche Strahlung nicht nachweisbar.\n\nAktuelle, ergonomisch geformte Mäuse haben häufig das Problem, dass sie lediglich für die Nutzung durch Rechtshänder konstruiert wurden. Linkshänder können solche Mäuse meist überhaupt nicht oder nur unter extremer Fehlhaltung benutzen. Da es von den meisten ergonomisch geformten Mäusen keine Linkshändervariante gibt, lernen viele Linkshänder, ihre Maus auch mit der rechten Hand zu verwenden. Bei den symmetrischen und dennoch ergonomisch geformten Mäusen gibt es dieses Problem nicht. Abhängig vom Betriebssystem besteht auch oft die Möglichkeit, die Funktion der rechten und linken Maustaste zu vertauschen, sodass Linkshänder mit einer normalen Maus linkshändig arbeiten können. Abhängig von der Art der Mausabfrage, z. B. bei Computerspielen, kann es sein, dass die im Betriebssystem gespeicherte Tastenvertauschung umgangen wird und der linkshändige Nutzer wieder die Finger vertauschen muss.\n\nBei der typischen Bauart der Tasten und der akustischen Rückmeldung der Betätigung mit Hilfe des Knackfroscheffektes beim Mausklicken kann es zu störenden Geräuschentwicklungen für das Umfeld (z. B. im Tonstudio, Wohnzimmer) kommen. Einige Hersteller bieten deshalb andere Tastentechniken an.\n\nDas Mauspad ist wohl das bekannteste Mauszubehör, das von den meisten Computernutzern eingesetzt wird. Es besteht häufig aus Stoff oder Kunststoff und bietet eine ebene Oberfläche, über die die Maus bewegt werden kann. Vor allem Kugelmäuse lassen sich oft nur in Verbindung mit Mauspads einsetzen, da der Mausball auf die hohe Reibung eines Mauspads angewiesen ist, um sich problemlos zu drehen.\n\nMauspads gibt es mit unterschiedlichen Oberflächen, zum Beispiel Stoff, Papier oder Kunststoff. Diese haben unterschiedliche Eigenschaften hinsichtlich Verschmutzung. Stoffoberflächen wirken teilweise reinigend auf die Mauskugeln.\n\nMittlerweile bietet die Industrie auch spezielle Mauspads für Computerspieler an, die besonders wenig Reibung erzeugen sollen, um eine präzise Bewegung zu ermöglichen. Diese bestehen dann häufig aus Hartplastik, speziellen Kunststofffasern oder Glas und sind oft nur für die Verwendung von optischen bzw. Lasermäusen konzipiert. Optische und Lasermäuse funktionieren teilweise schlecht auf spiegelnden Oberflächen.\n\nErgonomische Mauspads haben eine spezielle erhöhte Auflagefläche für die Handwurzel und entlasten die Hand.\n\nSkatez sind dünne Folienstreifen aus Kunststoffen mit sehr geringen Reibungskoeffizienten wie beispielsweise Teflon, die auf die Unterseite der Maus geklebt werden. Sie sind bei vielen Modellen schon Standard und werden im Handel zum Nachrüsten als Glidetape oder Speedtape angeboten. Skatez sorgen dafür, dass Gleitreibung und Haftreibung herabgesetzt werden und die Maus somit leichter über die Unterlage gleitet.\n\nOptisch die Unterlage abtastende Computermäuse an Workstations für strukturierte, gläserne oder metallene Mauspads werden zur Verbesserung der Gleitfähigkeit an der Unterseite mit austauschbaren, nicht kratzenden Filzstreifen beklebt.\n\nKabelhalter sollen bewirken, dass Mauskabel bei der Bewegung nicht störend im Weg liegen. Falls die (USB-) Maus nicht an der (USB-) Tastatur, sondern direkt am Rechner angeschlossen ist, soll außerdem verhindert werden, dass durch das Kabelgewicht herunter zum Rechner unter dem Tisch zusätzlicher Zug am Kabel entsteht.\n\nEinige Kabelhalter sind bereits ins Mauspad integriert, andere müssen beispielsweise am Tisch befestigt werden. Gängig ist eine anklebbare Öse für die Tischkante, an der die frei auf dem Tisch verfügbar bleibende Mauskabellänge durch festes Einklemmen bestimmt wird. Als einfachste Form des Kabelhalters hat sich ein festes Textilklebeband bewährt. Eine Weiterentwicklung stellt das Mouse Bungee dar. Mit ihm kann die Kabellänge der Maus optimal der überstrichenen Fläche des Mauspads angepasst werden.\n\nEine Handballenauflage ist ein kleines, gepolstertes Kissen oder \"Pad\". Die Füllung kann aus Gel (Silikon, Neopren) oder aus natürlichen Materialien bestehen. Es sorgt dafür, dass das Handgelenk bei der Arbeit nicht \"abknickt\" und eine Mausbetätigung mehr von oben erfolgt, was die Belastung der Hand und so die Ermüdung beim Arbeiten mindern soll.\nDa Mäuse vom Aufbau her ein sehr stark standardisiertes Produkt sind, werden sie von verschiedenen Computerherstellern nicht mehr selbst gefertigt, sondern von spezialisierten Herstellern zugekauft. Bekanntere Hersteller von Computermäusen sind:\n\n\n"}
{"id": "25643", "url": "https://de.wikipedia.org/wiki?curid=25643", "title": "Morphix", "text": "Morphix\n\nMorphix war eine Linux-Distribution, die als Live-System verfügbar war. Es war ein abgewandeltes Knoppix, das wiederum auf Debian GNU/Linux aufbaute. Die letzte Version wurde im September 2007 veröffentlicht.\n\nAlex de Landgraaf begann das Projekt 2003 und war bis 2007 der einzige Entwickler. Sein Ziel war es, das wegen seiner guten Hardwareerkennung für die Produktion von Live-CDs sehr geeignete Knoppix in Module aufzuteilen, so dass Live-CDs lokalisierter oder zweckgebundener Distributionen damit leicht herstellbar waren. Um dafür nützliche Eigenschaften anderer Distributionen zu integrieren wurde Knoppix entscheidend verändert, es entstand Morphix.\n\nDas Konzept des modularen Aufbaus bei Morphix hat die weitere Entwicklung modularer Distributionen gefördert. So diente die Morphix Live-CD als Basis für die Ubuntu-Live-CDs. Auch Knoppix hat das Konzept der Modularisierung übernommen. Die Module vieler unterschiedlicher Debian Derivate wurden kombinierbar. Dreamlinux z. B. hat Module, Kernelpatches und andere Software aus Morphix, aber auch Module aus anderen Distributionen übernommen.\n\n\n"}
{"id": "26475", "url": "https://de.wikipedia.org/wiki?curid=26475", "title": "Red Hat", "text": "Red Hat\n\nDas Unternehmen Red Hat (eng.: \"roter Hut\") ist ein US-amerikanischer Softwarehersteller mit Sitz in Raleigh, North Carolina, der unter anderem die weit verbreitete Linux-Distribution Red Hat Enterprise Linux (RHEL) vertreibt und am Fedora-Projekt beteiligt ist. Früher wurde auch Red Hat Linux (RHL) von Red Hat vertrieben. Darüber hinaus bietet Red Hat Lösungen aus den Bereichen Middleware, Virtualisierung, Cloud Computing, Storage, Mobile, Container, Management sowie Support, Schulungen und Consulting-Services. Die Aktien der Red Hat Inc. werden bis zur Übernahme durch IBM an der US-Börse NYSE gehandelt.\n\nDas Unternehmen Red Hat wurde 1993 von Marc Ewing gegründet und schloss sich 1995 mit dem Unternehmen ACC des Kanadiers Bob Young zusammen. Young übernahm im Unternehmen das Amt des CEO, das 1999 auf Matthew J. Szulik überging, der bis Ende 2007 dem Unternehmen vorstand.\n\nIm Juli 1999 wurde das in Stuttgart ansässige Unternehmen Delix übernommen, Hersteller der Deutschen Linux-Distribution (DLD), aus der dann die Red Hat GmbH hervorging. Am 11. August 1999 ging Red Hat mit einem Initial Public Offering von sechs Millionen Aktien zum Preis von 14 $ pro Aktie an die Börse NASDAQ. Am 15. November 1999 gab Red Hat bekannt, dass sie den Open-Source-Entwickler Cygnus Solutions übernehmen würden. Red Hat ist seitdem auch für Cygwin verantwortlich. Andere Übernahmen folgten, darunter unter anderem ArsDigita, Sistina und die des Netscape-Directory-Servers.\n\n2003 beschloss Red Hat, sich nur noch auf Unternehmenskunden zu konzentrieren. Daher wurde die Weiterentwicklung von \"Red Hat Linux\" (RHL) formell an das Community-Projekt Fedora abgegeben. Das Kernteam von Fedora besteht weiterhin aus von Red Hat bezahlten Entwicklern. Möglichkeiten der Beteiligung durch Freiwillige wurden allerdings umfangreicher als früher. Auf dem Fedora-Code basiert Red Hats Premium-Produkt \"Red Hat Enterprise Linux\". (RHEL) Am 19. Oktober 2005 verließ der Unternehmensmitbegründer Bob Young Red Hat, um sich privaten Projekten zu widmen.\n\nHeute ist Red Hat Marktführer im Bereich der Linux-Distributionen für Server. Red Hat hat weltweit näherungsweise 10.700 Mitarbeiter und 90 Büros. Der Stammsitz liegt in Raleigh (North Carolina) in den USA.\n\nDas Unternehmen ist aktiv auf den Gebieten Entwicklung, Einführung und Management von Linux- und Open-Source-Lösungen für Netzwerk-Infrastrukturen. Das Produktangebot reicht von eingebetteten Systemen bis zu Webservern und umfasst zusätzliche Support-, Trainings- und Managementangebote.\n\nRed Hat schloss im Juni 2006 den Erwerb von JBoss als weltweit führenden Anbieter von Open Source Middleware ab, um die Entwicklung hin zu Serviceorientierten Architekturen (SOA) zu beschleunigen. \nRed Hat ermöglicht so auch den Betrieb webfähiger Anwendungen auf einer Open-Source-Plattform. \n\nNach dem Rücktritt von Szulik trat Jim Whitehurst zum Jahr 2008 dessen Nachfolge als CEO und Präsident an.\n\nRed Hat-Entwickler liefern regelmäßig viele Beiträge zum Linux-Kernel. Gemäß der von der Linux Foundation 2016 veröffentlichten Analyse trug Red Hat mit 8 Prozent der Commits zu den Kernel 3.19 – 4.7 am zweitmeisten (hinter Intel) bei. Auch im Report 2017 steht Redhat mit 7,2 % der Commits zu den Kernels 4.8-4.13 auf Platz 3.\n\n2017 ist Red Hat mit Platz 23 in der Forbes Liste der 100 innovativsten Firmen der Welt erneut unter den Top 25.\n\nAm 28. Oktober 2018 gab Red Hat bekannt, dass die Firma IBM Red Hat für 190 US-Dollar je Aktie übernimmt - vorbehaltlich der Zustimmung der Red Hat-Aktionäre. Das entspricht einem Unternehmenswert von ungefähr 34 Milliarden US-Dollar. Die Unternehmen erwarten, dass die Übernahme nach Genehmigung der zuständigen Behörden in der zweiten Jahreshälfte 2019 abgeschlossen wird. Ende November 2018 wurde bekannt, dass Red Hat seinerseits im Zuge der Übernahme durch IBM das israelische Start-up-Unternehmen NooBaa gekauft hat.\n\nDie Produkte werden als \"kommerzielle Software\" entwickelt und vertrieben, die meist eng mit dem Fedora-Projekt zusammenhängen.\n\nZu den angebotenen Produkten gehört zum einen ein Linux-Betriebssystem. Die Entwicklung erfolgt im Rahmen des Fedora-Projektes (früher Fedora Core). Die vermarktete Variante mit Anpassungen und besser erprobten Versionen von Softwarepaketen trägt die Bezeichnung \"Red Hat Enterprise Linux\" (RHEL).\n\nMit der JBoss Enterprise Middleware Suite (JEMS) bietet Red Hat Lösungen in den Bereichen Java-EE-Applikationsserver (JBoss EAP auf Basis des WildFly Application Server), Objekt-/relationale Persistenz (Hibernate), Portal-Plattform (JBoss Portal), Workflow / Business Process Management / BPEL (JBoss \"jBPM\"), Business Rules (JBoss Rules), Objekt-Daten-Cache (JBoss Cache), Verteiltes Transaktions-Management (JBoss Transactions), Enterprise Messaging (JBoss Messaging) sowie Entwicklungstools (JBoss Eclipse IDE) an.\n\nDer von Netscape erworbene LDAP-kompatible Verzeichnisdienst wird im Rahmen des Fedora-Directory-Server-Projektes entwickelt und lautet kommerziell Red Hat Directory Server. Das ebenfalls erworbene Zertifizierungssystem (Red Hat Certificate System) betreut das Projekt Dogtag Certificate System.\n\nMomentan wird an einer umfassenden, zentralisierten Verwaltungsplattform für verschiedene Betriebssysteme und Plattformen gearbeitet: FreeIPA. Sie vereint Techniken wie Kerberos, LDAP, DNS und NTP und beinhaltet die Bereiche „Identitätskontrolle“ (Identity), „Richtlinien“ (Policy) und „Überwachung“ (Audit) einer heterogenen Client/Server-Computerlandschaft.\n\nErwähnenswert sind auch das Global File System (GFS) und die Cluster Suite.\n\nDes Weiteren unterstützt Red Hat viele andere Open-Source-Projekte wie Samba und OpenJDK in Form von IcedTea. Red Hat ist Gründungsmitglied der Open Source Business Alliance, früher Lisog.\nDas Produktportfolio umfasst folgende Bausteine:\n\nAllen Produkten ist gemeinsam, dass sie für Geschäftskunden ausgelegt und nur mit Support-Verträgen zu erwerben sind. \nEine Kernkomponente der Kunden-Unterstützung stellt das Red Hat Network dar. Produkte für Endnutzer bietet Red Hat nicht an. Red Hat finanziert sich auch durch Beratungen, Schulungen sowie Management bestehender Lösungen und Zertifizierungen.\n\nBeispiel: Um Enterprise-Kunden, Systemintegratoren und Softwarehersteller bei der Maximierung des Return on Investments durch den Einsatz der JBoss-Enterprise-Middleware-Suite-Produkte zu unterstützen, bietet Red Hat in Form des Tochterunternehmens JBoss ein umfassendes Leistungs-Portfolio aus technischem Support (JBoss Subscription), Consulting sowie Training und Zertifizierung an.\n\nIn China ging Red Hat eine Partnerschaft mit der Distribution Red Flag Linux ein.\n\nRed Hat bietet Zertifizierungen für Linux-Administratoren an, zum Beispiel:\n\n\nRed Hat sponsert das Fedora-Projekt, eine von der Community getragene Linux-Distribution, deren Fokus auf dem Einsatz freier Software liegt. Einige Führungspositionen im Fedora-Projekt besetzen Red-Hat-Mitarbeiter. So lässt man neue Techniken in Fedora von der Community testen, ehe sie in Red Hat Enterprise Linux, welches auf Fedora basiert, zum Einsatz kommen.\n\n"}
{"id": "26692", "url": "https://de.wikipedia.org/wiki?curid=26692", "title": "IrfanView", "text": "IrfanView\n\nIrfanView ist ein Freeware-Programm zur Betrachtung und in kleinem Umfang auch zur Bearbeitung von Bildern unterschiedlicher Formate für die Betriebssystemplattform Microsoft Windows.\nDer Name ist aus dem Vornamen seines Entwicklers Irfan Škiljan und dem englischen Wort \"view\" (‚Anzeige‘, ‚Ansicht‘, ‚Blick‘, ‚Sichtweise‘) abgeleitet. Laut Entwickler wird die Software pro Monat mehr als eine Million Mal heruntergeladen.\n\nErstmals erschien das Programm in einer Vorabversion im Jahr 1995. Die erste stabile finale Version des Programms, Version 1.70, die am 1. Juni 1996 veröffentlicht wurde, gab es nur in englischer Sprache. Version 2.63 vom 25. Januar 1998 war die erste, die auch auf Deutsch verfügbar war. Seither entwickelt Škiljan IrfanView auf Deutsch und Englisch weiter. Seit Version 3.35 aus dem Jahr 2001 ist jede Version in 29 Sprachen verfügbar. Englisch ist integriert, die 28 weiteren Sprachen können als Sprachpakete heruntergeladen werden.\n\nDas Programm kann eine Vielzahl freier Grafikformate – wie zum Beispiel JPG, GIF, TIFF, PNG und ICO – lesen und in andere Formate konvertieren. Durch Kooperationen und separat erhältliche Plug-ins wird das Programm um einige Zusatzfunktionen und Unterstützung von Grafikformaten wie WebP erweitert.\n\nIrfanView war laut Angaben des Entwicklers weltweit das erste Windows-Programm, das das GIF-Format inklusive GIF-Animationen unterstützte sowie mehrere Unterseiten aus ICO- und TIF-Dateien laden konnte („Multipage“-Unterstützung).\n\nNeben den eigentlichen Bildformaten werden auch andere Multimedia-Formate für Video- und Musikdateien unterstützt, darunter MPEG, MP3, Ogg und FLV. IrfanView bietet grundlegende Bildbearbeitungsfunktionen wie Scannen, Helligkeit, Kontrast, Ausschneiden, Weichzeichnen, Schärfen, Negativ, Farbänderung und Spezialfunktionen wie das Beseitigen des Rote-Augen-Effekts oder die Erstellung von Panoramabildern. Mithilfe der 8BF-Filter, die im Plug-in-Paket enthalten sind, können auch Photoshop-Plug-ins eingebunden werden. Auch eine Bildschirmfoto- und Kopiererfunktion ist im Programm enthalten, die einen Screenshot mit Mauszeiger erstellt. Außerdem kann man statische HTML-Bildergalerien mit Vorschaubildern (engl. „thumbnail“) aus HTML-Vorlagen (engl. „template“) anlegen.\n\nViele der angebotenen Grafikmanipulationen können auch als Stapelverarbeitung ausgeführt werden. Auf diese Weise ist es z. B. möglich, mehrere Dateien in einem Schritt zu drehen, zu verkleinern oder umzubenennen. Die Batchfunktionalität kann über die grafische Benutzeroberfläche oder über Parameter der Kommandozeile gesteuert werden.\n\nDie für Bildarchive nützliche Bearbeitung und Anzeige von IPTC-IIM-Standard-Informationen ist mit einem IPTC-Plug-in realisiert. Mittels eines anderen Plug-ins kann IrfanView JPEGs verlustfrei bearbeiten.\n\nDas Programm kann RAW-Dateien lesen, verarbeiten und konvertieren.\n\nFür IrfanView gibt es Plug-ins, die dem Programm zusätzliche Funktionen bieten. Die meisten standardmäßig installierten Plug-ins existieren in Form von DLL-Dateien, eines als EXE. Die nachzuinstallierenden Plug-ins sind Exe-Dateien, darunter auch eine mit E-Mail-Funktion. Standardmäßig enthält IrfanView die Plug-ins \"ANSI2UNICODE\" (für die Unterstützung von Unicode), \"Effects\" (Bildeffekte), \"Icons\", \"Paint\" (Funktion zum einfachen Bearbeiten durch Zeichnen mit der Maus), \"RegionCapture\" (einfache Screenshot-Funktion), \"Slideshow\" (zur Bildwiedergabe in Form einer Präsentation) und \"Video\" (Funktion zur Wiedergabe von Video- und Audiodateien).\n\n\"IrfanView Thumbnails\" (von engl. \"Thumbnail\" = Daumennagel, steht für die Anzeige von mehreren Bildern als Miniaturansichten) ist eine Funktion von IrfanView. Es ist keine getrennte Anwendung, öffnet sich allerdings in einem neuen Fenster. Es handelt sich um eine Art Dateimanager mit Miniaturansichten zum Verwalten von Bildern, die mehr Funktionen als der Windows-Explorer bietet. In Thumbnails lassen sich Bilder selektieren, um sie zu kopieren, verschieben, löschen, per E-Mail zu verschicken oder mit IrfanView (oder einem anderen Programm, wie der Windows-Fotoanzeige,) zu öffnen.\n\nIrfanView läuft unter allen Windows-9x- und NT-Versionen (NT, 2000, XP, 2003, 2008, Vista, Windows 7, Windows 8 und Windows 10). Ab Version 4.40 ist es als 32- und 64-Bit-Version verfügbar.\n\nNeben dem üblichen zu installierenden Programm wird IrfanView auch vom Entwickler selbst als portable Version veröffentlicht.\n\nIrfanView ist auch als App (32 und 64 Bit) im Windows Store (Windows 10) für PC verfügbar.\n\nFür private Nutzung, für Schulen und Universitäten sowie in Wohltätigkeits- und humanitären Organisationen ist das Programm Freeware. Für kommerzielle Nutzung ist eine Registrierung und der Kauf des Programms erforderlich. Auch von privaten Nutzern nimmt der Entwickler gerne Spenden an.\n\nDas Programmlogo stellt eine überfahrene Katze dar – ein Scherz.\n\n\n"}
{"id": "26758", "url": "https://de.wikipedia.org/wiki?curid=26758", "title": "Rollen-Taste", "text": "Rollen-Taste\n\nDie Rollen-Taste bzw. Scroll-Lock-Taste ist auf jeder IBM-PC-kompatiblen Computertastatur zu finden. Das Wort kommt von \"„to scroll“\" und \"„lock“\" (engl. Sperre oder Schloss). Auf einigen Tastaturmodellen befindet sie sich auch mit der Beschriftung „Bildlauf“.\n\nIn den 1980er Jahren, als man Texte am PC noch unter DOS verfasste und keine Maus hatte, wurde die Taste benutzt, um bei Betätigung der Pfeiltasten nicht die Schreibmarke (engl. \"Cursor\") durch den Text zu bewegen, sondern stattdessen den sichtbaren Bildausschnitt im Text auf und ab zu bewegen. Das hatte den entscheidenden Vorteil, dass man nicht mit dem Cursor bis ans Ende des Texts gehen musste, um zu sehen, was dort steht, sondern der Cursor an der Stelle bleiben konnte, wo man zuletzt getippt hatte. Heute gibt es Mäuse mit Scrollrad und Bildlaufleisten, die ebenfalls mit der Maus gesteuert werden können. So wurde die Bedeutung der Rollentaste von der Maus verdrängt. Da einige Benutzer aber bis heute Programme verwenden, die die Benutzung dieser Taste verlangen, blieb sie auf den Tastaturen erhalten.\n\nDas erste Mal erschien die Scroll-Lock-Taste auf der Tastatur des ersten IBM-PC. In der originalen Bedienungsanleitung heißt es auf Seite 5–20 (engl.):\nZu deutsch:\n\nHeute wird die Rollentaste von den meisten Programmen ignoriert, nur in wenigen modernen Programmen bewirkt sie immer noch etwas:\n\nDie Tastenkombination - (-) bewirkt dasselbe wie - (-). In Microsoft Visual Basic 5.0 und 6.0 hält diese Kombination beispielsweise das momentan in der Entwicklungsumgebung laufende Programm an. Der Grund hierfür liegt darin, dass die ersten PC-Tastaturen keine eigenständige Taste / hatten, sondern - für und - für verwendeten.\n\nBei manchen KVM-Switches (Computer-Umschalter) wird über zweimalige Betätigung von „Scroll Lock“ (und je nach Modell anschließend einer Ziffer oder Cursortaste) zwischen den angeschlossenen Computern umgeschaltet. Hier kommt die Taste gerade wegen ihres seltenen Gebrauchs zum Einsatz: Fast kein Programm würde auf eine solche Tastenkombination reagieren, daher kann sie ohne Schaden für den Switch reserviert werden.\n\nDie Bezeichnungen unterscheiden sich je nach Tastaturhersteller und Sprachversion:\nBei deutschen Tastaturen mit 101 und mehr Tasten etwa findet sich die Scroll-Lock-Taste meist mit der Beschriftung „Rollen“ und einem hohlen Pfeil nach unten zwischen der /- und der -Taste oben im Mittelblock der Tastatur. Ihr Zustand wird unter Microsoft-Betriebssystemen wie bei der Feststelltaste und der Num-Lock-Taste meist durch eine kleine LED dargestellt. Auch Linux-Konsolen nutzen die LED standardmäßig in dieser Weise; sie kann jedoch auch von Programmen für andere Zwecke verwendet werden.\n"}
{"id": "27203", "url": "https://de.wikipedia.org/wiki?curid=27203", "title": ".NET Framework", "text": ".NET Framework\n\n.NET Framework (auch mit dotNetFx oder nur mit NetFx abgekürzt) ist ein Teil von Microsofts Software-Plattform .NET und dient der Entwicklung und Ausführung von Anwendungsprogrammen. Das .NET Framework besteht aus einer Laufzeitumgebung \"(Common Language Runtime)\", in der die Programme ausgeführt werden, sowie einer Sammlung von Klassenbibliotheken, Programmierschnittstellen und Dienstprogrammen \"(Services)\". .NET Framework ist auf verschiedenen Plattformen verfügbar und unterstützt die Verwendung einer Vielzahl von Programmiersprachen. .NET-Programme werden zum Kompilierungszeitpunkt zunächst in eine Zwischensprache \"(Common Intermediate Language)\" übersetzt. Werden die so entstandenen Kompilate ausgeführt, wird der Code von der .NET-Laufzeitumgebung in die eigentliche Maschinensprache des Zielsystems übersetzt. Diese Übersetzung geschieht mit Hilfe eines Just-In-Time-Compilers. Für die Entwicklung von .NET-Programmen vertreibt Microsoft die Entwicklungsumgebung Visual Studio.\n\n.NET Framework ist ein monolithisches Framework, wohingegen das aus ihm hervorgehende, neu erscheinende Framework .NET Core modular aufgebaut ist.\n\nDie Entwicklung der .NET-Plattform wurde als notwendig angesehen, um die in die Jahre gekommenen Konzepte von Windows durch neue zu ersetzen, war jedoch auch das Ergebnis des Rechtsstreits von Microsoft mit Sun über Java. Microsoft hatte das von Sun entwickelte Java-System adaptiert und es nach eigenen Bedürfnissen erweitert, was die Plattformunabhängigkeit von Java-Applikationen beeinträchtigte. Als Sun das unter anderem durch Gerichtsverfügung unterband, änderte Microsoft seine Strategie. Zudem war es Microsoft bis zur Entwicklung von .NET nicht gelungen, im lukrativen Markt für mobile Kleingeräte Fuß zu fassen.\n\nZudem hatten sich mit der Zeit verschiedene, zueinander inkompatible Softwaresysteme für Windows entwickelt. Die drei für Windows meistverwendeten Programmiersprachen C++, Visual Basic sowie die Microsoft-Implementierung einer Java-Syntax, J++, waren zueinander nicht kompatibel und die Zusammenarbeit über verschiedene Brücken erwies sich als sehr kompliziert.\n\nDie Datentypen für Zeichenketten (engl. „strings“) waren nicht binärkompatibel zueinander. Wollte man solche über zwei Softwaresysteme hinweg schreiben, so musste man Laufzeiteinbußen wegen Konvertierungsfunktionen hinnehmen. Verschärfend kam die Koexistenz von ANSI und Unicode hinzu. Viele Programme unterstützten kein Unicode oder wurden dafür noch nicht ausgerüstet. .NET verwendet einheitlich Unicode für Zeichenketten.\n\nJede Entwicklungsplattform besaß ein eigenes System für die Verwaltung des Speichers. J++ und Visual Basic besaßen eine automatische Speicherverwaltung; das heißt, der Programmierer überließ (weitgehend) dem System die Verwaltung des Speichers. Visual C++ hingegen besaß keine Speicherverwaltung, der Programmierer musste sich selbst darum kümmern.\n\nAm 17. Januar 2008 veröffentlichte Microsoft den Quelltext des Frameworks unter der restriktiven Microsoft Reference License. Zu diesem Schritt entschloss sich Microsoft bereits im Oktober 2007, als Sun Microsystems sein Produkt \"Java\" unter der GNU GPL mit eigenen Zusatzklauseln zur Verfügung stellte.\n\nEnde 2013 gründeten Microsoft, Xamarin (Mono) und andere die \".NET Foundation\" als neuer Rechteinhaber und Lizenzgeber des .NET Frameworks. Seitdem sind fast alle Rechte an der .NET-Klassenbibliothek von Microsoft an die \".NET Foundation\" übertragen worden. Unter dem Dach der \".NET Foundation\" werden mit Stand 2014 30 Projekte verwaltet.\nTeile der Open-Source-Community sahen in der Offenlegung unter der damaligen restriktiven Lizenz eine Gefahr für das Projekt Mono, welches .NET-Anwendungen unter Linux teilweise verfügbar macht. Microsoft hatte 2007 noch behauptet, das Projekt enthalte Quellcode aus dem .NET-Framework. Da das Framework und \"Mono\" gleichermaßen \".NET\" implementieren, befürchtet man nun zwangsweise starke Ähnlichkeiten im Quellcode.\n\nDas umstrittene Patentabkommen zwischen Microsoft und Novell (dem ehemaligen Projektträger von \"Mono\") schützt derzeit sowohl die Community unter Novell als auch Microsoft vor gegenseitigen Patentansprüchen.\n\nMit der Gründung der .NET Foundation und der Übertragung der Rechte und Quellcodes an die Foundation arbeitet Microsoft mit Xamarin (Mono) aktiv zusammen, um .NET auf unterschiedlichen Plattformen bereitzustellen. Durch die Offenlegung der Quellcodes unter der MIT-Lizenz bzw. Apache 2.0 Lizenz ist der Quellcode des .NET-Frameworks nahezu beliebig – sprich auch in Closed-Source-Projekten – verwendbar. Lizenz- und patentrechtliche Auseinandersetzungen sind somit kaum noch möglich und somit auch nicht mehr zu befürchten.\n\nDie .NET-Plattform ist die Umsetzung des Common-Language-Infrastructure-Standards (CLI) und stellt mit diesem eine Basis zur Entwicklung und Ausführung von Programmen dar, die mit unterschiedlichen Programmiersprachen auf verschiedenen Plattformen erstellt wurden. Hauptbestandteile sind die (objektorientierte) Laufzeitumgebung \"Common Language Runtime (CLR)\", die \"Base Class Library (BCL)\" sowie diverse Hilfsprogramme zum Beispiel zur Rechteverwaltung.\n\nMit .NET löste Microsoft zuvor eingesetzte Softwareentwicklungskonzepte wie das Component Object Model ab, bis es COM in einer erweiterten Form unter dem Namen Windows Runtime reaktivierte. Seitdem plant Microsoft den parallelen Einsatz beider Frameworks für die Betriebssystemgeneration um Windows 8.\n\nDie \"Common Language Runtime (CLR)\" ist die Laufzeitumgebung des .NET Framework und enthält den JIT-Compiler für den standardisierten Zwischencode, die Common Intermediate Language (CIL). Die CIL hieß früher \"Microsoft Intermediate Language (MSIL)\", wurde aber im Rahmen der Standardisierung durch die Ecma International umbenannt. Für sie wurde ein sprachübergreifendes System von objektbasierten Datentypen definiert, so dass für alle Hochsprachen, die sich an den \"Common Language Infrastructure-Standard (CLI)\" halten, gültiger CIL-Bytecode erstellt werden kann.\n\n.NET wurde von Anfang an dafür entwickelt, dass Programmierer in unterschiedlichen Programmiersprachen arbeiten können. Jede dieser Hochsprachen wird von .NET dann in die CIL übersetzt.\n\nDas Besondere an der CLR ist weniger die technische Innovation als vielmehr die strategische Entscheidung von Microsoft für ein laufzeitbasiertes System. Es soll unter anderem helfen, Systemabstürze zu vermindern, da die Runtime Applikationsfehler abfangen kann. Damit entschied sich Microsoft erstmals \"gegen\" die bisher angewandte direkte Kompilierung in den Maschinencode des Zielsystems. Zusammen mit der Marktmacht von Java und dem Erfolg von Skriptsprachen ist damit ein Trend zu identifizieren. Dieser stellt einen Bruch mit den direktkompilierenden Programmiersprachen (insbesondere C++ und C) dar.\n\nMittels Reflection ist es möglich, zur Laufzeit Programmcode über ein Objektmodell zu generieren und es direkt im Speicher in lauffähigen Code zu überführen.\n\nDie .NET-Terminologie unterscheidet dabei zwischen Bytecode, welcher von der CLR verwaltet und in Maschinensprache umgesetzt wird (\"verwalteter Code\"), und Teilen, die nicht innerhalb der CLR ausgeführt werden (\"nicht verwaltet\"). Daneben gibt es noch die Möglichkeit in .NET sogenannten \"unsicheren Code\" (oder \"Code im unsicheren Kontext\") zu schreiben, um weiterhin z. B. klassische Zeiger-Operationen unmittelbar auf einem Speicherbereich durchführen zu können.\n\nMit Hilfe der \"Interop-Technik\" lassen sich alle klassischen, binär kompilierten Windows-Bibliotheken mit .NET-Kapseln (oder auch mit sogenannten \"Wrappern\") versehen und danach deren Programmfunktionen wie \"normale\" .NET-Programmfunktionen aufrufen. Technisch gesehen gibt die CLR allerdings im Moment des Aufrufs einer Funktion einer nicht überwachten DLL einen großen Teil der Kontrolle über den Programmfluss ab.\n\nUmgekehrt lassen sich auch .NET-Funktionen wie COM-Funktionen aufrufen. Damit soll eine fließende Migration von Software-Projekten auf .NET ermöglicht werden und die Integration von .NET-Modulen in eine bestehende Umgebung erleichtert werden.\n\nEines der wichtigsten Konzepte von .NET ist die Sicherheit.\nDas Sicherheitskonzept beginnt bei Mechanismen, die die Identität des Programmherstellers gewährleisten sollen (Authentizität), geht über in solche zum Schutz der Programme vor Veränderung (Integrität) und reicht bis hin zu Techniken, die den Ort der Herkunft bzw. Programmausführung (zum Beispiel das Internet) einbeziehen. Es gibt sowohl ein codebasiertes \"(Code-based Security)\" als auch ein nutzerbasiertes \"(Role-based Security)\" Sicherheitsmodell.\n\nEine programmiertechnisch interessante Neuerung von .NET ist die Einführung von \"Attributen\": gekennzeichnete Metadaten als Bestandteil der Programmiersprache. Beispielsweise können im Rahmen der komponentenbasierten Programmierung Komponenteneigenschaften ausgedrückt werden. Für die Verteilung, Installation und Konfiguration, für die Sicherheit, für Transaktionen und andere Programme können dem Code beschreibende Eigenschaften hinzugefügt werden.\n\nInnerhalb eines Programmes kann mit Hilfe von Reflection auf die Attribute eines .NET-Programms und die in ihr enthaltenen Elemente zugegriffen werden.\n\nDieses Konzept wurde später unter anderem in Java übernommen, wo es in Form sogenannter Annotations verwirklicht ist.\n\n.NET ab Version 3.0 enthält die Windows Communication Foundation zur Kommunikation in verteilten Systemen. Diese geben Entwicklern die Möglichkeit, Probleme, die bis dahin mit folgenden Technologien des .NET-Frameworks gelöst werden konnten, über ein einheitliches Programmiermodell zu lösen.\n\nDie \"Common Language Specification\" (CLS) definiert als eine gemeinsame Untermenge den Bytecode der CIL, der von der virtuellen Laufzeitumgebung (VM) in den Maschinencode der Zielmaschine übersetzt und ausgeführt werden kann. Somit ist es möglich, .NET mit verschiedenen, an die CLR angepassten Sprachen zu programmieren. Von Microsoft zum Beispiel schon im \"Visual Studio\" mitgeliefert sind das, neben der von Microsoft für .NET eingeführten Sprache C#, die Sprachen C++/CLI, das proprietäre Visual Basic .NET sowie J# (eine Portierung von Microsofts veränderter Java-Implementierung) und abschließend – nicht zu verwechseln mit J# – JScript .NET. Außerdem wurde mit Visual Studio 2010 die funktionale Programmiersprache F# eingeführt.\n\nInsbesondere das vereinheitlichte Typsystem \"(Common Type System)\", das eine gemeinsame Schnittmenge an Datentypen beschreibt, sorgt für ein reibungsloses Zusammenspiel beim Aufruf von in einer anderen Sprache geschriebenen Komponenten. Das stellt einen wichtigen Fortschritt dar, da man unter Visual Basic 6 unter Umständen gezwungen war, Funktionen, die nicht in Visual Basic implementiert werden konnten, in Visual C++ zu programmieren. In diesem Fall gab es immer Schwierigkeiten bei der Zuordnung der Datentypen von Visual Basic zu den entsprechenden Datentypen unter C++. Auch bei der Programmierung von COM-Komponenten in C++ musste man als Programmierer mit einem eingeschränkten Satz von Datentypen auskommen, die zur Automation benutzt werden konnten. Außerdem wurden Zeichenketten unter C++ und Visual Basic 6 intern unterschiedlich gespeichert, was die Programmierung erschwerte.\n\nDie Vorteile der Unterstützung gemischtsprachiger Programmierung von .NET sind nicht unumstritten. Beispielsweise ist die Wartbarkeit eines Projektes, welches in mehreren Sprachen implementiert wurde, schlechter als bei der Entwicklung mit nur einer Sprache.\n\nNeben den von Microsoft für die .NET-Plattform angepassten Sprachen \"C#\", \"Visual Basic .NET\", \"F#\" und \"C++/CLI\" \"(Managed C++)\" werden weitere .NET-Sprachen von Drittanbietern zur Verfügung gestellt (zum Beispiel Delphi Prism von Embarcadero, aber auch weniger bekannte Sprachen wie APL von Dyalog).\n\nDie für .NET bereitgestellte IDE von Microsoft, das Visual Studio .NET, bietet die Möglichkeit, weitere Sprachen von Drittanbietern in ein Projekt einzubinden und somit deren Funktionalität zu nutzen. Dass die Entwicklung in einer konsistenten Entwicklungsumgebung stattfindet und es nicht für jede Sprache eine eigene IDE gibt, ist zusätzlich von Vorteil.\n\nDie angestrebte Plattformunabhängigkeit wäre unter .NET grundsätzlich möglich, Microsoft selbst hatte 2002 für die erste Version von .NET eine eingeschränkte (und nicht mehr aktuelle) .NET-Variante namens \"Shared Source CLI\" \"(SSCLI)\" für macOS und FreeBSD zur Verfügung gestellt. 2006 folgte die Version 2.0 von SSCLI für .NET 2.0, die aber nur noch auf Windows XP SP2 lauffähig ist.\n\nMehrere von Microsoft quasi unabhängige Open-Source-Projekte haben sich einer entsprechend flexiblen Implementierung der Rahmenkomponenten auf Grundlage des ECMA-Standards angenommen. Das am weitesten entwickelte Projekt ist Mono, das vom Hersteller Ximian initiiert wurde. Das dotGNU-Projekt, welches an einer Portable.NET genannten Laufzeitumgebung arbeitete, wurde dagegen eingestellt.\n\nBeide Implementierungen sind jedoch noch nicht auf dem Entwicklungsstand des heutigen .NET. Zwar hat Mono mit der Version 2.0 einen wichtigen Meilenstein, nämlich die Kompatibilität mit den nicht-windowsspezifischen Bibliotheken von .NET 2.0, erreicht. Auf der anderen Seite gibt es viele Programme, die P-Invoke oder COM Interop benutzen, d. h. auf Bibliotheken zugreifen, die nicht in IL-Code, sondern in normalem, prozessorspezifischen Maschinencode vorliegen. Zwar kann auch Mono auf Bibliotheken zugreifen, die in C oder C++ geschrieben sind, allerdings sind die meisten dieser Bibliotheken plattformabhängig. Weiterhin hat Microsoft mit .NET 3.0 und .NET 3.5 gravierende Weiterentwicklungen des Frameworks veröffentlicht, die von Mono bzw. dotGNU bis dato noch nicht oder nur teilweise implementiert wurden, aber in Arbeit sind. Explizit ausgenommen wurde die Windows Presentation Foundation, die auf absehbare Zeit nicht reimplementiert werden wird. Allerdings wird es trotzdem Unterstützung für XAML geben.\n\nVerwalteter oder auch \"managed Code\" wird, wie oben erwähnt, von der Laufzeitumgebung \"Common Language Runtime\" (CLR) verwaltet. Diese virtuelle Maschine übernimmt die Anforderung und Freigabe von Speicher und anderen Ressourcen (automatische Speicherbereinigung, engl. \"garbage collection\") und stellt sicher, dass geschützte Speicherbereiche nicht direkt angesprochen oder überschrieben werden können.\nWie oben unter Sicherheit beschrieben, können auch Zugriffe auf Dienste, Dateisystem-Funktionen oder Geräte überwacht und, sofern sie gegen Sicherheitsrichtlinien verstoßen, von der CLR abgelehnt werden.\n\nFür den Erfolg von .NET war und ist es wichtig, die Entwicklergemeinde von C++ für .NET zu gewinnen. Daher war Geschwindigkeit bei .NET von Anfang an ein wesentliches Entwurfsziel.\n\nDurch verschiedene Techniken wird versucht, den negativen Einfluss der CLR auf die Ausführungsgeschwindigkeit möglichst klein zu halten. Zum Beispiel wurden analog zu Java sogenannte JIT-Compiler eingeführt, die einen Mittelweg zwischen Interpretation und Kompilierung gehen. Außerdem kann man mit .NET als Neuerung auch Programme in bereits kompiliertem Code, als sogenanntes \"natives Image\" installieren. Das wirkt sich insbesondere auf die erstmaligen Ladezeiten bei Programmen mit größeren Klassenmengen aus. Weiterhin kann der Speicherbedarf reduziert werden, wenn mehrere Programme dieselbe Assembly nutzen bzw. das Programm mehrfach gestartet wird (Terminalserver), da die nativen Images im Gegensatz zu JIT-Code zwischen den Programmen über gemeinsam genutzten Speicher (engl. \"shared memory\") geteilt werden. Der Gewinn an Ausführungsgeschwindigkeit durch native Images muss durch sorgfältige Messungen („profiling“) analysiert werden. Der Einsatz von nativen Images erfordert weitere Planungsschritte bei der Entwicklung der Software, zum Beispiel eine sorgfältige Auswahl der DLL-Basisadresse der Assemblies, um eine Relokation der DLLs zu verhindern. Schließlich müssen die Assemblies auch im GAC installiert werden, um anhand der Identität die Integrität der Images garantieren zu können. Wird das nicht beachtet, führt die Relokation bzw. die Identitätsprüfung der Assembly zu weiteren Ausführungszeiten, die den Vorteil der nativen Images wieder zunichtemachen.\n\nDie automatische Ressourcenverwaltung und die verbesserte Sicherheit haben dennoch ihren Preis – die Ausführung von \"managed code\" hat einen erhöhten Ressourcenbedarf und benötigt mehr Zeit. Außerdem sind die Antwortzeiten auf Programm-Ereignisse wesentlich schwieriger zu kalkulieren und zum Teil deutlich größer, was die Anwendbarkeit in Echtzeitsystemen stark einschränkt.\n\nEin Grund dafür ist die automatische Speicherbereinigung, die Garbage Collection, die automatische Freigabe nicht mehr benötigten Speichers und anderer Ressourcen. Im Regelfall entscheidet der Garbage Collector, wann der Speicher aufgeräumt werden soll. Der Entwickler kann aber den Zeitpunkt der Speicherbereinigung auch selbst festlegen.\nWährend das einerseits, durch die Zusammenfassung der Freigabeoperationen, die Ausführungsdauer von Programmläufen verringern kann, können andererseits die Antwortzeiten auf Ereignisse dadurch in Mitleidenschaft gezogen werden. Das ist besonders für kleinere Maschinen nachteilig und stellt, vor allem im Hinblick auf die Marktausrichtung zu mobilen Kleingeräten, ein Problem dar.\n\n.NET wird inzwischen auch bei performancekritischen Programmen, zum Beispiel Computerspielen (zum Beispiel mit dem XNA Framework), Animationsprogrammen, Konstruktionsprogrammen und ähnlichen, hochaufwendigen Programmen genutzt, da viele Programmierer der Meinung sind, dass aktuelle Systeme durch ihre höhere Geschwindigkeit den durch .NET bedingten Leistungsverlust ausgleichen.\n\nAuf der anderen Seite steht die Meinung, dass die Qualität und Effizienz der traditionellen Softwareentwicklung zu wünschen übrig lassen und dass die diesbezüglichen Vorteile durch obige Verfahren deren Nachteile in der Regel aufwiegen. Im Allgemeinen wird dabei von einer asymmetrischen Verteilung ausgegangen, dass zum Beispiel 90 Prozent einer durchschnittlichen Anwendung problemlos „managed“, das heißt, auch mit automatischer Speicherbereinigung ausgeführt werden können, und lediglich 10 Prozent (zum Beispiel einzelne Funktionen oder Klassen) optimiert werden müssen.\n\nNicht zuletzt können Programme auch in Hinblick auf die Ausführungsgeschwindigkeit im Zusammenspiel mit automatischer Speicherbereinigung optimiert werden.\n\nDie \"Framework Class Library\" (FCL) umfasst z. B. in der Version 3.5 bereits etwa 11.400 Klassen und andere Datentypen, die in mehr als 300 sogenannte Namensräume (Namespaces) unterteilt sind. Im Vergleich zur ersten Version 1.0 mit 3.581 Datentypen in 124 Namensräumen ist das ein deutlicher Anstieg. Die Klassen erfüllen Aufgaben wie das Formatieren von Text, das Verschicken von E-Mails, aber auch das Generieren von Code. Die Unterteilung in Namensräume dient dazu, die große Menge an Informationen übersichtlicher zu gestalten. Beispielsweise befinden sich Klassen zum Generieren von Code in dem Namensraum codice_1.\n\nDie Dokumentation der Klassen liefert der Hersteller in seinem \"Software Development Kit\" (SDK) mit (siehe unten).\n\nDer Compiler für .NET-Sprachen erzeugt keinen Maschinencode, der direkt vom Prozessor ausgeführt werden kann. Stattdessen wird ein prozessorunspezifischer Zwischencode, der sogenannte Intermediate Language Code (Zwischen-Sprachcode, IL-Code) erzeugt. Dieser besteht aus Befehlen, die auf der stackbasierten virtuellen Maschine (VM) ausgeführt werden. Die resultierenden Programme („.exe-Dateien“) besitzen wie native Windows-Programme den PE-Header. Eine kleine Routine am Anfang des Programms ermöglicht den Start der virtuellen Maschine, welche wiederum den Zwischencode ausführt.\n\nWenn das Programm ausgeführt wird, übersetzt ein JIT-Compiler, der in der Laufzeitumgebung \"Common Language Runtime (CLR)\" enthalten ist, den Zwischencode in Maschinencode, der dann vom Prozessor direkt ausgeführt werden kann.\n\nDa Code aus allen .NET-Sprachen in dieselbe Zwischensprache übersetzt wird, können Funktionen und Klassenbibliotheken, die in verschiedenen .NET-Sprachen geschrieben sind, problemlos gemeinsam in einem Programm verwendet werden.\n\nÜbersetzte Programmklassen werden als ausführbare Programme in sogenannten Assemblies zusammengefasst und bereitgestellt (vergleichbar mit JAR-Dateien in Java). Diese haben typischerweise die Endungen .exe oder .dll und sind gültige Portable Executables, werden jedoch anders strukturiert. Insbesondere sind im sogenannten \"Manifest\" alle notwendigen Metadaten aufgeführt, so dass für reine .NET-Programme in der Regel die Registrierung entfällt (Ausnahme zum Beispiel \"COM+/Enterprise Services\").\n\nAssemblies können entweder \"privat\", \"gemeinsam\" \"(shared)\" oder \"global\" sein. Private Assemblies befinden sich in demselben Verzeichnis wie das auszuführende Programm. Daher wird angenommen, dass die Version des Assemblies kompatibel zum Programm ist und daher nicht von der CLR geprüft wird.\n\nEin gemeinsames \"(shared)\" Assembly kann sich in einem Verzeichnis befinden, auf das von mehreren Programmen zugegriffen wird.\nDaher wird für ein gemeinsames Assembly ein sogenannter \"Strong Name\" benötigt, bestehend aus dem Dateinamen des Assemblies, seiner Version, der \"Culture\" – die die Lokalisierung definiert – und einem kryptografischen Schlüssel. Durch eine Konfigurationsdatei, die sich in dem Verzeichnis des Programms befindet, kann der Anwendung der Speicherort der gemeinsamen Assemblies mitgeteilt werden. Ein \"Strong Name\" kann mit Hilfe des Werkzeugs \"sn\" erzeugt werden.\n\nEin globales Assembly wird im globalen Assembly-Zwischenspeicher (Global Assembly Cache, GAC) gespeichert. Mit Hilfe des Werkzeugs \"gacutil\" können Assemblies dem GAC hinzugefügt werden. Innerhalb des GAC können Assemblies mit unterschiedlichen Versionen, Kulturen gespeichert werden. Mit Hilfe von Konfigurationsdateien kann festgelegt werden, welche Versionen eines Assemblies von der Anwendung benutzt werden sollen. Erfolgt keine Angabe, so wird nur die Version benutzt, die bei der Erstellung der Anwendung benutzt wurde. Wenn diese nicht vorhanden ist, wird beim Start des Programms eine Fehlermeldung ausgegeben.\n\nAktuelle Windows-Versionen besitzen eine Explorer-Erweiterung, die eine aussagekräftige Anzeige des Inhalts des GAC im \"Windows Explorer\" ermöglicht.\n\n.NET ist im vollen Umfang nur für Windows verfügbar. Am 17. Januar 2008 veröffentlichte Microsoft Teile des Quelltextes für Windows-Entwickler. Große Teile von .NET, insbesondere die Laufzeitumgebung und die Klassenbibliotheken, wurden unter dem Namen Common Language Infrastructure (CLI) als ECMA-Standard normiert. Durch die Standardisierung der Laufzeitumgebung gibt es alternative Produkte, die Software, die mit .NET erstellt wurde, ausführen beziehungsweise Software für .NET erstellen können. Viele Programme, die mit .NET erstellt wurden, laufen beispielsweise dank der durch das Open-Source-Projekt Mono zur Verfügung gestellten Software auch auf Unix-basierten Betriebssystemen wie z. B. Linux oder Mac OS X.\n\nDer Hersteller Microsoft bietet .NET in verschiedenen Formen an. Als reine Laufzeitumgebung samt benötigter Klassenbibliotheken (Framework), als kostenloses SDK für Entwickler, als kostenpflichtige integrierte Entwicklungsumgebung (IDE) in Form des \"Microsoft Visual Studio .NET\". Speziell für Einsteiger und Studenten gibt es die kostenlosen \"Microsoft Visual Studio Express Editions\" mit Einschränkungen gegenüber den kostenpflichtigen Standard- oder Professional-Varianten. Eine ebenfalls kostenfreie IDE für .NET (und Mono) unter Windows findet man im Open-Source-Projekt SharpDevelop. Studenten bietet Microsoft weiterhin die Möglichkeit, über das \"DreamSpark\"-Programm kostenfrei die Professional-Variante des Visual Studios zu beziehen.\n\nSeit \"Windows Server 2003\" bietet Microsoft darüber hinaus Server-Betriebssysteme an, die bereits eine .NET-Laufzeitumgebung integriert haben. Bei Vorversionen muss diese manuell installiert werden, sofern die betreffende Windows-Variante unterstützt wird. .NET ist erst ab Windows NT 4.0 beziehungsweise Windows 98 einsetzbar, die Programmierung von Webanwendungen (ASP.NET) etwa läuft nur ab Windows 2000. Ab Windows Vista ist .NET ein Kernbestandteil des Systems. Auf Nicht-Windows-Systemen wird .NET von Microsoft offiziell nicht unterstützt – so verbleibt die Plattformunabhängigkeit in der Liste der Möglichkeiten von .NET. Allerdings existieren die bereits erwähnten Open-Source-Projekte, die .NET auch für andere Plattformen (zum Beispiel Linux) verfügbar machen, wenn sie auch nicht den vollen Funktionsumfang des .NET-Frameworks unter Windows bieten können.\n\nMicrosoft begann mit der Entwicklung des .NET Frameworks in den späten 1990ern, ursprünglich unter dem Namen der \"Next Generation Windows Services (NGWS)\". Gegen Ende des Jahres 2000 wurden die ersten Betaversionen von .NET 1.0 veröffentlicht.\n\nVersion 1.0 stellt die erste Veröffentlichung des .NET Frameworks dar. Es wurde am 13. Februar 2002 für Windows 98, NT 4.0, 2000 und XP veröffentlicht. Der Support von Microsoft für diese Version endete am 10. Juli 2007, der erweiterte Support lief noch bis zum 14. Juli 2009.\n\nDie erste Erweiterung von .NET wurde als Installer am 3. April 2003 veröffentlicht. Es wurde gleichzeitig als integraler Bestandteil der Entwicklungsumgebung Visual Studio .NET 2003 vertrieben. Version 1.1 war die erste Version von .NET, die zusammen mit einem Betriebssystem, nämlich dem \"Windows Server 2003\", ausgeliefert wurde. Dieser hieß bis zum Freigabekandidat sogar \"Windows .NET Server\". Die offizielle Unterstützung für diese Version endete am 14. Oktober 2008, die erweiterte Unterstützung endete am 8. Oktober 2013. Da .NET 1.1 eine Komponente des \"Windows Server 2003\" darstellt, lief die erweiterte Unterstützung zusammen mit der Unterstützung für dieses Betriebssystem am 14. Juli 2015 aus.\n\n\n.NET 2.0 wurde zusammen mit Visual Studio 2005, Microsoft SQL Server 2005 und Microsoft BizTalk 2006 veröffentlicht.\n\n\n\n\".NET Framework 3.0\", ehemals \"WinFX\" genannt, erweitert die \"Managed\"-API, die einen integralen Bestandteil der Betriebssysteme Windows Vista und Windows Server 2008 darstellt. Seit dem 6. November 2006 ist das .NET Framework 3.0 für Windows XP ab \"Service Pack 2\" und für Windows Server 2003 verfügbar, um Entwicklern rechtzeitig die Entwicklung und Portierung von Programmen nach Vista zu ermöglichen. In der dritten Hauptversion von .NET wurden tiefgreifende Änderungen an der Architektur vorgenommen. Dazugekommen sind Funktionalitäten, die vor allem unter \"Windows Vista\" zum Einsatz kommen sollen. Das .NET Framework 3.0 greift auf die CLR aus .NET 2.0 zurück.\n\nDas .NET Framework 3.0 beinhaltet vier neue Hauptkomponenten:\n\nFür die Vorab-Demonstration des neuen .NET Framework präsentierte Microsoft den Fotodienst \"Microsoft Max\". Mit Herausgabe der endgültigen Version wurde der Dienst eingestellt.\n\nVersion 3.5 des .NET Framework wurde am 19. November 2007 veröffentlicht. Es verwendet die CLR aus Version 2.0. Mit Version 3.5 werden gleichzeitig das \".NET Framework 2.0 SP1\" und \".NET Framework 3.0 SP1\" installiert.\n\nDie Version 3.5 SP1 (11. August 2008) ergänzte die Bibliothek um das ADO.NET Entity Framework 1.0 und die ADO.NET Data Services. Mit der Version 3.5 SP1 werden gleichzeitig das \".NET Framework 2.0 SP2\" und \".NET Framework 3.0 SP2\" installiert. Am 18. Dezember 2008 wurde zudem ein General Distribution Release veröffentlicht, das lediglich Fehlerbehebungen beinhaltet.\n\nDer Quellcode der Klassenbibliothek (BCL) wurde teilweise unter der \"Microsoft Reference Source License\" freigegeben.\n\n\nMicrosoft gab Informationen zum .NET Framework 4 erstmals am 29. September 2008 und auf der \"Professional Developers Conference (PDC 2008)\" bekannt.\nDie erste Beta-Version des .NET 4 wurde am 18. Mai 2009 veröffentlicht. Am 19. Oktober 2009 folgte eine zweite Beta-Version. Ursprünglich war die Veröffentlichung des .NET-Frameworks zusammen mit der Entwicklungsumgebung Microsoft Visual Studio 2010 für den 22. März 2010 geplant. Um jedoch mehr Zeit für weitere, von Nutzern der Beta 2 des Microsoft Visual Studio 2010 geforderte, Optimierungen zu erhalten, kündigte Microsoft im Dezember 2009 eine Verschiebung des Releases von .NET 4 und Visual Studio 2010 um einige Wochen an. Am 10. Februar 2010 erschien das „Release Candidate“. Die endgültige Version von .NET 4 und Visual Studio 2010 in der englischen Sprachfassung wurde von Microsoft schließlich am 12. April 2010 veröffentlicht.\n\nZu den wichtigsten Neuerungen bei .NET Framework 4 gehörten unter anderem:\n\nDie ersten Informationen zu .NET Framework 4.5 gab Microsoft auf der \"BUILD Windows Konferenz\" am 14. September 2011 bekannt. Die endgültige Version erschien am 15. August 2012.\n\nMit Version 4.5 hat Microsoft die Bereitstellung zweier separater Installationspakete, einem \"Full Package\" und einem im Funktionsumfang reduzierten \"Client Profile\", wieder eingestellt. Als Grund dafür gilt, dass das Client Profile-Installationspaket nur unbedeutende 7 MB an Download einspart, dafür aber häufig Verunsicherung über die richtige Wahl beim Benutzer verursachte.\nNeben einigen kleinen Verbesserungen (u. a. Performance des JIT-Compilers) wurde die Unterstützung asynchroner Methodenaufrufe durch neue Schlüsselwörter in C# (async, await) und Visual Basic (Async, Await) hinzugefügt.\n\nMit Version 4.5.1 wurden erneut einige kleinere Verbesserungen vorgenommen, außerdem erschien eine neue Version 2013 von Visual Studio.\n\nMit Version 4.5.2 wurden kleinere Verbesserungen bei der High-DPI-Darstellung vorgenommen.\n\nMit Version 4.6 wurden u. a. die Performance des 64 Bit JIT-Compilers verbessert sowie umfangreiche Änderungen an Basisklassenbibliotheken vorgenommen.\n\nVersion 4.6 ist die letzte Version, die Windows Vista und Windows Server 2008 unterstützt.\n\nVersion 4.6.1 bringt Fixes und neue Features.\n\nVersion 4.6.2 bringt Fixes und neue Features.\n\nVersion 4.7 bringt Fixes und neue Features:\n\n\nEine Vorschauversion wurde im Windows-Insider-Programm von Windows 10 ab Januar 2017 ausgeliefert.\n\nDie finale Version wurde zusammen mit dem Creators Update für Windows 10 ausgeliefert, welches am 11. April freigegeben wurde.\n\nAm 2. Mai 2017 wurde .NET 4.7 für Windows 7 mit Service Pack 1, Windows 8.1, Windows 10 mit dem Anniversary Update, Windows Server 2008 R2 mit Service Pack 1, Windows Server 2012, Windows Server 2012 R2 und Windows Server 2016 veröffentlicht.\n\nAm 13. Oktober 2017 wurde .NET 4.7.1 für Windows 7 mit Service Pack 1, Windows 8.1, Windows 10 mit dem Fall Creators Update (Version 1709), Windows Server 2008 R2 mit Service Pack 1, Windows Server 2012, Windows Server 2012 R2 und Windows Server 2016 veröffentlicht.\n\n.NET Framework 4.7.1 unterstützt den .NET Standard 2.0, wenn zusätzliche .NET Standard-Unterstützungsdateien (dotnet-standard-support-vs2015-2.0.0-win-x86.msi) installiert werden.\n\nAm 30. April 2018 wurde .NET 4.7.2 für Windows 7 mit Service Pack 1, Windows 8.1, Windows 10 mit dem Spring Creators Update (Version 1803), Windows Server 2008 R2 mit Service Pack 1, Windows Server 2012, Windows Server 2012 R2 und Windows Server 2016 angekündigt.\n\n.NET Framework 4.7.2 baut auf früheren Versionen von .NET Framework 4.x auf. Visual Studio-Anwendungen ab 2012 werden unterstützt.\n\nFür Handhelds und Mobiltelefone, die unter Windows CE bzw. Windows Mobile laufen, existiert eine funktional reduzierte Version der .NET-Laufzeitumgebung in Form des .NET Compact Frameworks. Es lässt sich aber nur unter Verwendung des kostenpflichtigen \"Visual Studio .NET 2003\" oder neuer für diese Plattform entwickeln. Zeitgleich mit der Version 3.5 von .NET wurde das \".NET Compact Framework 3.5\" veröffentlicht.\n\nIm September 2006 stellte Microsoft zusätzlich das .NET Micro Framework vor. Es stellt eine nochmals eingeschränkte Version des .NET Frameworks speziell für Embedded-Geräte dar. Je nach Plattform soll das Framework zwischen 512 KByte und 1 MByte auf dem Gerät beanspruchen und lässt sich direkt aus dem Flash-Speicher oder dem ROM starten. In diesem Falle arbeitet das Micro Framework als Betriebssystem, es kann aber auch auf ein vorhandenes Windows-Betriebssystem aufsetzen.\n\n\"Silverlight\" (vormals \"WPF/E\") enthält eine stark verkleinerte Untermenge des .NET Frameworks und soll im Wesentlichen Webbrowser befähigen, reichhaltige Internetanwendungen auf Basis der WPF auszuführen. Normale Programme auf Basis der WPF sind ebenfalls „webfähig“, benötigen aber das vollständige .NET 3.0, welches derzeit nur für Windows verfügbar ist. \"Silverlight\" jedoch soll auch für Mac OS, ältere PCs mit Windows sowie Linux bereitgestellt werden.\n\nAm 12. November 2014 wurde eine Teilmenge des Reference Source Quellcodes auf GitHub gehostet und unter der MIT-Lizenz veröffentlicht. Das geschah auch, um das Mono-Projekt zu unterstützen, damit Lücken zwischen Mono und .NET durch Verwendung desselben Codes geschlossen werden können. Dieses Repository bezieht sich auf das .NET Framework 4.6 und hat deshalb nur Lesezugriff. Gleichzeitig veröffentlichte Microsoft die überarbeiteten Komponenten des Frameworks unter der Bezeichnung \".NET Core\" auf GitHub und zwar auch unter MIT-Lizenz. \".NET Core\" erlaubt die Beteiligung durch die Community und ist von Microsoft an die 2014 gegründete \".NET Foundation\" überstellt worden. Durch die Verwendung der MIT-Lizenz gibt es faktisch keine Einschränkungen, wie der Quellcode von \".NET Core\" verwendet werden darf.\nDie veröffentlichten Komponenten umfassen unter anderem weitere Werkzeuge für die Softwareentwicklung wie den C# und VB-Compiler Microsoft Roslyn sowie den kompletten ASP.NET Webstack. \".NET Core\" ist auch unter Linux und MacOS X lauffähig.\n\n\n\n"}
{"id": "27940", "url": "https://de.wikipedia.org/wiki?curid=27940", "title": "Amstrad CPC", "text": "Amstrad CPC\n\nDie Amstrad CPC-Serie, im deutschsprachigen Raum besser als Schneider CPC bekannt, war eine in den 1980er Jahren populäre Baureihe untereinander weitgehend kompatibler 8-Bit-Heimcomputer, die auf der damals weit verbreiteten Z80-CPU basierte und u. a. in Westeuropa größere Verbreitung fand. Entwickelt wurden die CPCs von der britischen Firma Amstrad, die sie in Fernost, u. a. von Orion, als Auftragsarbeit bauen ließ. Die Bezeichnung \"CPC\" leitet sich vom englischen \"Colour Personal Computer\" ab.\n\nDie Rechner wurden als Komplettpaket mit umfangreicher Hardwareausstattung verkauft: Enthalten waren der eigentliche Rechner mit integrierter Tastatur und Laufwerk (Kompaktkassette beim CPC464 und 464Plus, 3″-Diskette bei den anderen Modellen), ein Farb- oder ein Monochrommonitor (grün bei den klassischen und schwarzweiß bei den Plus-Modellen) mit integriertem Netzteil, mehrere kurze Verbindungskabel, ein ausführliches Handbuch, eine CP/M-Bootdiskette sowie eine Diskette mit Programmen bzw. eine Demokassette. Ein Fernseher konnte über einen als Zubehör erhältlichen Adapter angeschlossen werden. Jedoch lieferte der mitgelieferte RGB-Monitor ein wesentlich besseres Bild als ein Fernseher. Je nach Modell und Ausstattung war der Verkaufspreis vergleichbar oder deutlich niedriger als der eines C64, bei dem Monitor und Disketten-Laufwerk in der Regel als Zubehör erworben werden mussten.\n\nDa Amstrad in Deutschland über keine Vertriebsstrukturen verfügte, übernahm die Schneider Computer Division, eine eigens zu diesem Zweck gegründete Tochter der Schneider Rundfunkwerke AG, den Vertrieb unter der Bezeichnung \"Schneider CPC\" für die Länder Deutschland, Österreich und die Schweiz. Nachdem Amstrad und Schneider 1988 die Zusammenarbeit beendet hatten, verkaufte Amstrad auch in diesen Ländern die CPC-Serie unter eigenem Namen, was mit erheblichen Anlaufschwierigkeiten verbunden war, da Amstrad bis zu diesem Zeitpunkt nach wie vor nicht selbst in Deutschland vertreten war. Deshalb und weil der Zenit der CPCs bereits überschritten war, stammen die meisten in Deutschland verkauften CPCs noch von Schneider. In den meisten anderen Ländern wurden CPCs bereits zuvor unter der Bezeichnung \"Amstrad CPC\" verkauft.\n\nDie Technik entsprach durchgehend dem Stand der Zeit, und so war der Computer in den meisten Aspekten dem direkten Konkurrenten Commodore 64 ebenbürtig. In Teilbereichen (z. B. Anzahl darstellbarer Farben, Sprachumfang des eingebauten BASIC-Interpreters, Leistung der CPU, Speichermedien) war der CPC diesem sogar überlegen, in anderen (Fehlen von Hardware-Unterstützung für Sprites) dagegen unterlegen.\n\nDer Rechner startete direkt ins (samt ausführlichem Handbuch) mitgelieferte, auf dem ROM enthaltene Locomotive BASIC. Weitere Software konnte über Kassette oder Diskette nachgeladen werden. Mit den beigelegten Disketten konnte das Betriebssystem CP/M 2.2, bei den Modellen mit 128 KB RAM auch CP/M 3.0, nachgeladen werden; weitere Programme und Programmiersprachen, etwa Logo und Turbo Pascal 3.01A waren erhältlich. Programmierung in Maschinensprache war mittels der üblichen PEEK/POKE/CALL-Befehle vom BASIC aus direkt möglich, aber auch echte Assembler standen zur Verfügung.\n\nDer BASIC-Editor des CPC wich vom reinen Bildschirmeditor-Konzept des direkten Konkurrenten C64 insofern ab, als dass eine an sich zwar nur zeilenorientierte, aber mit dem \"Copy-Cursor\" zur Übernahme vorhandener Bildschirminhalte dennoch recht komfortable Funktion zum Kopieren oder Editieren des Quelltextes bereitstand. Der für die Zeit komfortable BASIC-Interpreter wies einen recht guten Befehlsumfang auf. Module und Funktionen fehlten entsprechend der Entstehungszeit beinahe ganz, die automatisierte Bearbeitung des zeilennummerierten Listings (Neunummerierung, Verschmelzen einzelner Listing-Teile etc.) war möglich.\n\nNicht nur das BASIC, sondern auch das interne Betriebssystem (für Assembler-Programmierer) waren im Vergleich zu anderen Heimcomputern schnell und geradezu luxuriös ausgestattet. So enthielt das ROM des CPC unter anderem eine umfangreiche Bibliothek für Gleitkommazahlen sowie ein ausgeklügeltes Interrupt-System, das teilweise sogar von BASIC aus nutzbar war (sogenannte \"Events\").\n\nDas zur Ablösung des Kassettenlaufwerkes ab dem Modell CPC 664 eingebaute 3″-Diskettenlaufwerk brachte einen erheblichen Geschwindigkeitsvorteil. Die Disketten waren sehr stabil, wurden einzeln in Pappschubern oder Jewel Cases geliefert, und sind den später erschienenen 3½″-Disketten ähnlich, mit ihrem starren, schwarzen Plastikgehäuse aber wesentlich teurer. Die von Hitachi-Maxell entwickelte 3″-Diskette war als Nachfolger der in den 1970er Jahren gängigen 8″-Diskette konzipiert, wurde jedoch vom Markt kaum angenommen. Außer von Schneider/Amstrad und später Sinclair, bedingt durch den Aufkauf durch Amstrad, wurde dieses Format nur von wenigen, exotischen Computerherstellern verwendet. Die Medienpreise waren auch wegen der geringen Verbreitung dieses Formates relativ hoch, für eine einzelne Diskette wurden beim Einzelhändler zwischen 7 und 20 DM verlangt.\n\nDas logische Format der 3″-Diskette war beim CPC doppelseitig (engl. \"double sided\") mit doppelter Schreibdichte (\"double density\"), benutzte die Aufzeichnungsmethode MFM, verwendete je Seite 40 Spuren à 9 Sektoren à 512 Bytes, und hatte somit eine Kapazität von 360 KB für Nutzdaten (180 KB je Seite). Die Diskette musste zum Beschreiben der zweiten Seite auch physisch gedreht werden, da es sich um Einzelkopflaufwerke handelte. Spätere 3″-Doppelkopflaufwerke fanden sich nur beim verwandten PCW-Rechner, nicht aber beim CPC, wohingegen Disketten höherer Speicherkapazität (dank veränderter Formatierungsdichte) durchaus möglich waren.\n\nObwohl die 3″-Diskette der früheren 5¼″-Diskette überlegen war, konnte sie sich nicht durchsetzen. In der Zwischenzeit eroberte bereits Sonys 3½″-Diskette dank besserer Leistungsdaten (720 KB), stärkerer Vermarktung, und Verwendung im populären Apple Macintosh den Markt. Der 34-polige Daten-Anschluss des Laufwerks entsprach dem eines PC-XT-Diskettenlaufwerks, nur der Netzteil-Anschluss war anders belegt. So ließen sich mittels eines Netzteil-Adapters auch für den PC konzipierte Laufwerke anschließen, deren Speichermedien wesentlich preiswerter erhältlich waren. Das Betriebssystem AMSDOS konnte grundsätzlich nur eine Seite der Diskette ansprechen. Daher wurden beim Anschluss von 3½″-Laufwerken am CPC zumeist manuelle Kippschalter zur Seitenwahl mit eingebaut, da sich die unsymmetrisch aufgebauten 3½″-Disketten prinzipiell nicht wenden lassen.\n\nDie verschiedenen CPC-Modelle waren – in diesem Preissegment eine Neuerung – untereinander kompatibel. Die Software ließ sich weitgehend austauschen. Das Betriebssystem CP/M war von den mittleren 1970er bis in die mittleren 1980er Jahre der De-facto-Industriestandard für professionell genutzte 8-Bit-Rechner, so dass im Bereich Office-Anwendungen (z. B. WordStar, dBase, Microsoft Multiplan) und Programmiersprachen auch Software genutzt werden konnte, die nicht speziell für den CPC erstellt worden war. Das CP/M-Softwareangebot war wegen des von praktisch allen anderen CP/M-Rechnern abweichenden Diskettenformates allerdings nicht direkt zugänglich; Mailboxen, über die Programme heruntergeladen werden konnten, kamen in Europa gerade erst auf und waren den wenigsten bekannt bzw. zugänglich. Allerdings war es, je nach persönlichen Interessen, auch möglich, ohne den Einsatz von CP/M zu arbeiten. Im Bereich der Computerspiele spielte CP/M ohnehin kaum eine Rolle.\n\nIn der CPC-Serie verrichtete ein Zilog-Hauptprozessor (Z80 oder Z80A) (mit einer Datenwort- und Datenbus-Breite von 8 Bit) seinen Dienst. Dieser Prozessor war den meisten anderen damaligen 8-Bit-Prozessoren in vielerlei Hinsicht überlegen (großer Befehlsumfang erlaubte sehr kompakte Programme, maximale Taktfrequenz von bis zu 4 MHz, integrierter IO-Controller, integrierter DRAM-Refresh, virtuelle 16-Bit-Register, weite Verbreitung und niedriger Preis). Im CPC wird die Z80-CPU mit einer Taktfrequenz von 4 MHz betrieben, wobei jeder Maschinenbefehl auf Vielfache von 4 Zyklen gestreckt wird, um Zugriffskonflikte zwischen Hauptprozessor und Videoelektronik (\"Snowing\") zu vermeiden. Da viele der meistverwendeten Z80-Befehle drei oder vier Zyklen erfordern, ist die durch dieses Verfahren entstehende Leistungseinbuße relativ gering. Es ergibt sich eine theoretische Rechenleistung von bis zu 1 MIPS, in praktischen Anwendungen sind es jedoch eher unter 0,5 MIPS.\n\nDie CPCs waren je nach Modell mit 64 KB RAM (464, 664, 464plus, GX4000) oder 128 KB RAM (6128, 6128plus) ausgestattet, wovon dem Benutzer unter BASIC ca. 42 KB für Programme und/oder Daten zur Verfügung standen. Da der Adressbus der Z80-CPU mit 16 Bit Breite maximal 65.536 verschiedene Adressen (also Bytes, also 64 KB) ansprechen konnte, wurde der erweiterte Speicher des CPC6128 (und auch ggf. mit Speichererweiterungen bestückter CPCs) per Bank Switching verfügbar gemacht. Hierbei wurde ein 16 KB großer Speicherblock aus dem zweiten 64-KB-Block (bzw. weiteren Blöcken) in dem vom Hauptprozessor adressierbaren Speicherbereich (hier zwischen 0x4000 und 0x7fff) eingeblendet (wodurch der normalerweise dort liegende Speicher ausgeblendet wurde, also temporär nicht zur Verfügung stand). Insgesamt waren so (durch die Ansteuerung der 16-KB-Blöcke begrenzt) Speichererweiterungen auf bis zu 576 KB möglich und auch zur damaligen Zeit erhältlich. Erst 2007 kamen Erweiterungen auf den Markt, mit denen auch eine Erweiterung bis 4 MB möglich wurde.\n\nDie CPCs waren je nach Modell mit 32 KB (464) bzw. 48 KB (664, 6128) ROM ausgestattet, wobei sich diese zu jeweils 16 K auf Betriebssystem/Kernel (unteres ROM), BASIC-Interpreter (oberes ROM) und (bei Vorhandensein eines Diskettenlaufwerks bzw. Controllers) das AMSDOS (auch oberes ROM, siehe unten) mit einem Teil der Programmiersprache Logo verteilten. Abweichend hierzu hatten die Modelle 464plus/6128plus/GX4000 kein eingebautes ROM, sondern einen Steckplatz für Steckmodule, wobei das Modul für den 464plus und 6128plus neben einem mitgelieferten Spiel auch alle o. g. ROMs enthielt. Durch ein u. a. speziell für diesen Zweck entworfenes Uncommitted Logic Array konnten diese ROMs im CPC (an verschiedenen Adressbereichen: unteres ROM zwischen 0x0000 und 0x3fff und oberes ROM zwischen 0xc000 und 0xffff) beliebig ein- und ausgeblendet werden. Im oberen ROM waren theoretisch sogar bis zu 252 verschiedene, jeweils 16 K große ROMs (ein- und ausblendbar) möglich.\n\nDer Basic-Interpreter der CPC-Serie (Locomotive BASIC 1.0 im 464 bzw. 1.1 in 664, 6128, 464plus und 6128plus) war für damalige Verhältnisse aus mehreren Gründen außergewöhnlich. Zum einen gab es umfangreiche Befehle zur Erzeugung von Grafik, Tonausgabe, Text-Fenstern sowie UDGs (benutzerdefinierten Grafiken, engl. \"user defined graphics\") etc., zum anderen war es z. B. möglich, den Befehlssatz per RSX (eng. \"resident system eXtension\") beliebig zu erweitern. Der Interpreter unterzog, wie die meisten 8-Bit-BASICs, jede im (zeilenbasierten) Editor eingegebene Programmzeile sofort einer lexikalischen Analyse (Tokenisierung), bevor sie gespeichert wurde. Dadurch sank der Speicherverbrauch und die Geschwindigkeit der Programme stieg.\n\nDie Grafik-Darstellung in der CPC-Serie wird von dem Gespann CRTC (engl. \"cathode ray tube controller\", einem Motorola 6845-Controller, der baugleich auch in CGA-Grafikkarten verwendet wurde) sowie dem Gate Array gesteuert. Interessant ist hierbei die Kombination dieser beiden integrierten Schaltkreise, da der CRTC hauptsächlich zur Darstellung von Textmodi (mit \"Character ROM\", d. h. Zeichensatz-ROM, ähnlich dem ersten IBM PC, dem C64 oder auch dem heutigen Videotext) ausgelegt ist, im CPC aber auf ziemlich ausgeklügelte Art und Weise zusammen mit dem Gate Array die Steuersignale und Bildinhalte erzeugt sowie die Auflösung der Farbpaletten steuert. Dadurch erst sind die drei unterschiedlichen Grafikmodi (\"Mode 0\": 160×200 Pixel bei 16, \"Mode 1\": 320×200 Pixel bei 4 und \"Mode 2\": 640×200 Pixel bei 2 aus jeweils 27 Farben) der CPC-Reihe möglich. Das Fehlen eines echten Textmodus und der Hardware-Sprites schränkten jedoch den CPC (im Vergleich zum C64) im Spielbetrieb deutlich ein – der nötige Befehlsaufwand zur Anzeige von bewegter Grafik ist im CPC wesentlich höher, so dass Spiele entweder langsamer ablaufen oder viel stärker auf größtenteils statische, unbewegte Bildinhalte setzen müssen.\n\nUm Kosten zu senken, wurden in späteren Modellen der klassischen CPCs ein höher integriertes IC (ASIC) als Ersatz für CRTC und Gate-Array verwendet (zu sehen an der verkleinerten Hauptplatine).\n\nDie Modelle 464plus, 6128plus und GX4000 erhielten einen verbesserten Grafikchip, der Sprites, Scrolling und sogar 32 Farben (aus 4096) integriert hatte. Zudem war der neue Grafikchip in der Lage, sich kompatibel zum alten Chipsatz zu verhalten, was er beim Einschalten im klassischen Modus auch standardmäßig tat. Die Umschaltung in den verbesserten Modus erfolgt durch für die Öffentlichkeit undokumentierte Maschinensprachebefehle, diese wurden jedoch bereits 1992 in der Zeitschrift CPC Amstrad International (sowie in Englischen und Französischen Computermagazinen) offengelegt. Trotzdem nutzen nur relativ wenige Diskettenspiele diese Umschaltung und reizen somit auf Plus-Modellen die Fähigkeiten aus.\n\nAmstrad hat sich für den CRTC verschiedener Zulieferer bedient. Bei den dokumentierten Funktionen sind diese vollständig kompatibel, bei sehr hardwarenaher Programmierung, insbesondere in Demos, gibt es jedoch Inkompatibilitäten, so dass nicht alle Tricks mit allen CRTC-Typen möglich sind. Daher ermitteln solche Programme häufig den verwendeten CRTC-Typ, um diesem entsprechend andere Routinen zu verwenden oder die entsprechenden Programmteile ganz auszublenden. Weit verbreitet (aber inoffiziell) ist folgende Zählung:\n\n\nBei der Zählung ist zu berücksichtigen, dass diese nicht chronologisch ist: CRTC 4 kam vor CRTC 3 zur Verwendung, wurde aber von der Szene erst nach CRTC 3 entdeckt. Durch die Abweichungen bei der CRTC ist es in Szenekreisen heute auch gängig, mehrere CPCs mit verschiedenen CRTCs zu besitzen. Die inkompatibelste CRTC ist Typ 2, die kompatibelste Typ 0, gefolgt von Typ 1.\n\nDer Soundgenerator im CPC ist ein AY-3-8912 (ursprünglich von General Instrument), ein Chip, der 3-Kanal-Stereo-Sound erzeugt sowie integrierte Hüllkurven- und Rausch-Generatoren besitzt (sowie \"IO\"-Kanäle, über die z. B. im CPC auch die Tastatur abgefragt und das Lesen und Schreiben von Kassette gesteuert wird). Im Vergleich zu den Möglichkeiten eines Synthesizers (wie dem SID, der z. B. dem C64 seine Stimme gibt) sind die des AY-3 relativ eingeschränkt, aber auch hier war wieder der günstige Preis ausschlaggebend für die Wahl. Dieser Tongenerator (bzw. Abwandlungen davon) wurde z. B. auch in vielen Arcade-Automaten, im Atari ST, im Sega Master System, Sega Mega Drive, Nintendo Game Boy und anderen Konsolen der frühen 1990er Jahre verbaut.\n\n\nDie Tastaturen wurden teilweise auf lokale Standards angepasst (spanische, französische oder englische Belegung). Eine deutsche Tastatur soll es, gerüchteweise, ebenfalls gegeben haben.\n\nDurch drei Lötbrücken auf der Hauptplatine konnte die Produktion zwischen den Handelsmarken Amstrad, Orion, Schneider, Awa, Solavox, Saisho (Dixon UK), Triumph und ISP umgestellt werden. Genutzt wurde, soweit bekannt, Amstrad, Awa (bei den durch Mitsubishi Electric nach Australien exportierten Rechnern; die Gehäusebeschriftung blieb gleich) und Schneider (Deutschland).\n\nEine weitere Lötbrücke legt fest, ob der Monitor mit den europäischen 50 Hz oder den amerikanischen 60 Hz angesprochen werden soll.\n\nÜber eine weitere Lötbrücke konnte festgelegt werden, ob der Rechner beim Einschalten den BASIC-Interpreter oder das CP/M startet. Findige Bastler haben diese Lötbrücke durch einen von außen zugänglichen Schalter ersetzt.\n\nWährend bei den Amstrad-Modellen die externen Stecker zum Teil als kostengünstige Platinenstecker ausgeführt wurden, analog etwa zu den heutigen Steckkarten, waren bei dem besser geschirmten Schneider CPC6128 Micro-Ribbon-Schnittstellen bzw. -Stecker verbaut worden, besser bekannt unter dem inoffiziellen Namen \"Centronics-Stecker\" (es handelt sich um den gleichen Steckertyp, der bei Druckern mit Parallelschnittstelle druckerseitig zum Einsatz kommt).\n\nBei den Amstrad-Modellen waren einige Tasten des CPC464 und CPC664 farbig hervorgehoben:\n\n\nDemgegenüber waren die Tasten der gleichen Rechner in Deutschland einheitlich schwarz (CPC464), bzw. grau (CPC664), um den Computern ein seriöseres Äußeres zu geben. Ab dem CPC6128 waren die Tasten länderübergreifend einheitlich grau (CPC6128) bzw. weiß (Plus-Serie).\n\nDie Unterschiede zwischen den länderspezifischen Modellen lagen also in\n\n\nDer CPC464 ist das ursprüngliche Modell der CPC-Serie. Im Gegensatz zu den anderen Modellen besitzt der CPC464 ein Kassettenlaufwerk zum Speichern von Daten sowie das \"Locomotive BASIC\" in der Version 1.0. Das 3″-Diskettenlaufwerk mitsamt Controller war separat erhältlich. Er wurde mit 64 KB Speicher ausgeliefert. Im Jahr 1985 wurde der CPC464 zum Computer des Jahres gewählt.\n\nDer CPC472 wurde in einer kleinen Serie von Amstrad/Indescomp für den spanischen Markt hergestellt. Genauer betrachtet ist der CPC472 kein eigenes Modell, sondern entspricht technisch dem CPC464. Der Grund für die Einführung des CPC472 war ein spanisches Gesetz, nach dem jeder Rechner, der 64 KB Speicher besitzt, mit einer spanischen Tastatur ausgeliefert werden musste. Um dieses Gesetz zu umgehen, wurden in den CPC464 auf einer kleinen Zusatzplatine zusätzliche – aber völlig funktionslose – 8 KB RAM eingebaut, so dass er weiterhin mit einer englischen Tastatur ausgeliefert werden konnte. Nachdem das Gesetz auf beliebige Speichergrößen ausgeweitet worden war, wurden die Restbestände des CPC472 mit einer spanischen Tastatur ausgeliefert.\nDer CPC664 besaß eine verbesserte ergonomische Tastatur mit leichter Krümmung sowie ein 3″-Diskettenlaufwerk und 64 KB Speicher. Der CPC664 war allerdings nicht sehr lange auf dem Markt, da Amstrad bereits das Nachfolgemodell CPC6128 angekündigt hatte, welches neben dem 3″-Diskettenlaufwerk 128 KB RAM enthielt.\nDer CPC6128 hatte eine verbesserte, flachere Tastatur ohne Krümmung, 3″-Diskettenlaufwerk, 128 KB und war ursprünglich für den amerikanischen Markt gedacht. Da der Z80-Prozessor des CPC nur 64 KB RAM ansteuern konnte, war ein Zugriff auf den restlichen Speicher nur über das Einblenden von 16- oder 64-KB-Blöcken des oberen Speicherbereiches in den unteren möglich, eine Technik namens Bank Switching.\nNeben dieser ursprünglichen Serie gab es später einige einfacher produzierte und zugleich verbesserte, aber zu spät entwickelte Nachfolgemodelle von Amstrad, auf den Spielekonsolenmarkt abzielend. Die Gehäusefarbe wechselte hierbei zum üblichen Beige-Grau, und es wurde eine Schnittstelle für Steckmodule eingebaut. Ferner wurden die Centronics-Buchsen des Schneider CPC6128 jetzt für alle Länder übernommen. Dadurch, und weil alle ROMs (inkl. der evtl. für Frankreich oder Spanien anzupassenden Zeichentabelle) jetzt in dem Steckmodul steckten, konnten alle Modelle ohne Modifikationen der Platine in allen Ländern verkauft werden, da nur noch die Tastenkappen und das Steckmodul unterschiedlich waren.\n\nÄnderungen gab es auch beim Soundchip und beim Grafikchip, der Sprites, Scrolling und sogar 32 Farben (aus 4096) integriert hatte. Die Kompatibilität blieb durch das CPC-ROM in dem Steckmodul weitgehend erhalten. Aufgrund der weiterentwickelten Konkurrenz (Atari etc.) konnten sich die Modelle aber nicht durchsetzen.\n\nWie CPC464, zzgl. o. g. Verbesserungen. Obwohl ein Expansionsport vorhanden ist, ist der Anschluss eines Diskettenlaufwerks erschwert, da zum einen die Erweiterungsschnittstelle auf Centronics umgestellt wurde und der DDI-1-Controller noch für den alten Platinensteckeranschluss entwickelt wurde. Man benötigt also genau das Gegenstück des Adapters, den zuvor die dt. Schneider-CPC6128-Besitzer brauchten. Außerdem enthält die Cartridge ein AMSDOS, welches mit dem des Controllers kollidiert.\n\nWie CPC6128, zzgl. o. g. Verbesserungen. Ggü. dem klassischen CPC6128 fehlt jedoch der Anschluss für den Kassettenrecorder.\nDie GX4000 entspricht technisch weitestgehend dem 464plus, besitzt allerdings kein Kassettenlaufwerk und keine Anschlüsse für Drucker und Erweiterungen. Um sie an den Fernseher anschließen zu können, besitzt sie sowohl einen Composite-Video- und einen SCART-Anschluss. In den Hochzeiten des CPCs fand sich ein Tüftler (Wolfgang Noisternig, auch bekannt als \"The Cranium\"), der die Spielkonsole zu einem vollwertigen 6128plus \"erweiterte\".\nDer CSD oder \"Cartridge Software Demonstrator\" war eine Werbemaßnahme für die Plus-/GX-Reihe. Aufbauend auf CPC-Ersatzteilen, mit einem kastenähnlichen Gehäuse, waren auf einem zweiten Board bis zu zwölf Spielecartridges untergebracht, die zeitlich begrenzt gespielt werden konnten. Die Auswahl der einzelnen Cartridges erfolgte im Bildschirmmenü. Der CSD war für Demonstrationszwecke in Kaufhäusern gedacht und nicht frei erhältlich.\n\nEin Nachfolgeprojekt der ursprünglichen CPC-Reihe lief unter dem Codenamen \"Arnold\" bzw. \"ANT\" (Arnold Number Two).\n\nGerüchten zufolge sollen Amstrads Computer firmenintern allerdings alle \"Arnold\" geheißen haben, benannt nach dem Chefentwickler Roland Perry (\"Arnold\" ist ein Anagramm von \"Roland\").\n\nDa wenige Informationen vorliegen, führt vermutlich ein Vergleich mit der PCW-Technik am weitesten, realistisch für das Projekt erscheinen folglich 256 oder 512 KB RAM sowie ein 3½″-Laufwerk. Auch die Verwendung des kompatiblen, mit 8 MHz doppelt so schnell getakteten Z80H könnte zum Plan gehört haben.\n\nDie Nummerierung ist jeweils auf dem Startbildschirm, z. B. als (V3), sichtbar:\n\n\nFür den CPC472 gibt es keine eigene Arnold-Nummer, da es sich nicht um einen eigenständigen Rechner handelt. Die gesamte Plus-Reihe teilt sich eine Arnold-Nummer, die angezeigte Version 4 differiert jedoch von der während der Entwicklung verwendeten Nummer V (römisch 5).\n\nEin anderer, offiziell bestätigter, allerdings vermutlich sehr inoffiziell benutzter Name innerhalb der CPC-Serie war IDIOT, er stand als Abkürzung für \"Insert Disc Instead Of Tape\".\n\nDer \"KC compact\", ein am 7. Oktober 1989 eingeführter und wegen des fast unmittelbar folgenden Zusammenbruches der DDR seltener Klon des CPC464 auf Basis DDR- bzw. sowjetischer Chipsätze. Fast kompatibel, mit 64 KB bzw. optionalen 128 KB RAM ausgestattet, wurde er mit einem Kassettenrecorder oder einem optionalen (und noch selteneren) externen 5,25″-Robotron-Diskettenlaufwerk und einem Fernseher betrieben, das Gehäuse erinnerte entfernt an den C64.\n\n1993 entstand mit dem Aleste 520EX bei einer russischen Firma in Omsk (Sibirien) ein Klon, der zu einem CPC6128 kompatibel war. Zusätzlich bestand die Möglichkeit, das Gerät in einen MSX-Modus umzuschalten. Im CPC-Modus gab es, außer 192 KB RAM, keine Unterschiede zum CPC. Über eine Erweiterungskarte namens \"Magic Sound\" war es möglich, Scream-Tracker-Dateien direkt mit diesem Computer abzuspielen.\n\nMittlerweile existiert eine leicht veränderte Version von Caprice, die die Emulation des Aleste 520EX ermöglicht.\n\nDer (fiktive) \"CPC5512\" war ein Scherz des französischen Computermagazins \"Hebdogiciel\" im Jahre 1985, der zwar in der nächsten Ausgabe widerrufen wurde, aber ein gerichtliches Nachspiel nach sich zog.\n\nUnter der Projektbezeichnung CPC 6512 veröffentlichte die deutsche Computerzeitschrift c't im Oktober 1987 eine Anleitung, wie ein CPC 6128 mit 512 KB Hauptspeicher ausgestattet werden kann.\n\nIn der auf versierte Hobby- und Amateuranwender ausgelegten Bauanleitung wurden die (fest eingelöteten) RAM-Bausteine im Computer durch solche mit höherer Kapazität ersetzt und die neuen Adressleitungen über zusätzlich einzubauende ICs angesprochen. Wie beim CPC 6128 war der erweiterte Speicher logisch über Bankswitching ansprechbar und erreichbar. Aufgrund fehlender Unterstützung in den (sowieso nur auf 64 KB Hauptspeicher ausgelegten) AMSDOS- und CP/M-Betriebssystemen des CPC konnte der Zusatzspeicher nur in selbstgeschriebenen oder im Rahmen des Projektes veröffentlichten Programmen genutzt werden. Wichtigste Standardanwendung dürfte die sowohl unter AMSDOS als auch unter CP/M ansprechbare RAM-Disk gewesen sein.\n\nDas Projekt weist eine gewisse Ähnlichkeit mit dem CPC 5512 auf, den das französische Computermagazin Hebdogiciel in einem Aprilscherz beschrieb. Anders als dort handelt es sich beim CPC 6512 aber um einen tatsächlich realisierbaren, funktionsfähigen Umbau des CPC 6128.\n\n\nFür den 464plus/6128plus gab es nur 27 Spiele auf Steckmodul und somit für die GX4000 auch insgesamt.\n\nMit Hardware gebündelt:\n\n\nEinzeln im Handel erhältlich:\n\n\nAuf Diskette bzw. Kassette (und damit nicht ohne Umwege für die GX4000 nutzbar):\n\n\nEine Kuriosität stellte der französische Markt dar. Hier war die Nachfrage nach CPC wesentlich größer als erwartet, während sie in Deutschland und England eher verhalten war, insbesondere bei der Plus-Reihe. Dadurch gelangten auch viele Schneider/Amstrad CPC sowie Amstrad Plus mit englischer Tastaturbelegung (gegebenenfalls als Grauimport) nach Frankreich, teilweise mit weißen, französischen Farbtabellenaufklebern auf dem Diskettenlaufwerk. Beim Schneider CPC6128 bedeutete dies für die Käufer zusätzlich, mit den bei regulärer französischer Ware nicht vorhandenen Centronics-Buchsen konfrontiert zu werden.\n\n\nNeben dieser Bücherauswahl gab es verschiedene Computerzeitschriften die die CPCs zum Hauptthema hatten.\nDie wichtigsten deutschsprachigen Publikationen waren:\n\n\n\n\nPer Software:\n\n\nPer Hardware:\n\n"}
{"id": "28407", "url": "https://de.wikipedia.org/wiki?curid=28407", "title": "Amiga 500", "text": "Amiga 500\n\nDer Amiga 500 ist der meistverkaufte Amiga-Computer von Commodore. Er wurde 1987 auf der CeBit zusammen mit dem Amiga 2000 vorgestellt und besitzt ähnliche Leistungsdaten wie dieser. Zielgruppe für den Rechner waren vor allem Privatanwender mit kleinerem Geldbeutel. Zu Lasten der Aufrüstbarkeit wurde auf ein Desktop- bzw. Towergehäuse verzichtet und stattdessen ein Design gewählt, das dem des Commodore 128 ähnelte. Der Amiga 500 war – mehr noch als der Amiga 2000 – zu seiner Zeit besonders als Spielecomputer beliebt, da Bild- und Tonqualität dem damals üblichen PC deutlich überlegen waren.\n\nMarktpolitisch war der Amiga 500 vor allem als Antwort auf den erfolgreichen Konkurrenten Atari ST konzipiert. In der Praxis erfüllte der Amiga 500 die ihm zugedachte Rolle und hängte den Atari ST im Wettbewerb deutlich ab. Allerdings verschob er das allgemeine Image der Amiga-Modellreihe weiter deutlich in Richtung eines „Spielcomputers“, wobei in den Augen des Marktes die konzeptionellen Stärken der Architektur zu wenig wahrgenommen wurden.\n\nDer Amiga 500 besitzt eine mit 7,09 MHz (PAL-Variante) bzw. 7,16 MHz (NTSC-Variante) getaktete Motorola 68000-CPU und 512 KByte Arbeitsspeicher. Dieser kann mittels zweier Erweiterungs-Slots um 512 KByte und 8 MByte ausgebaut werden, so dass maximal 9 MByte Speicher zur Verfügung stehen. In den Jahren 1987 und 1988 wurde der Amiga 500 mit dem Amiga-Betriebssystem AmigaOS 1.2 (Kickstart) ausgeliefert, ab 1988 wurde dieses durch die verbesserte und leicht umfangreichere Version 1.3 abgelöst.\n\nWie beim Amiga 1200 muss an den Amiga 500 ein spezielles, mit eigenem Netzschalter versehenes Netzteil angeschlossen werden, da der Amiga 500 keinen Netzschalter hat.\n\nDer A500 verfügt über keine eingebaute Festplatte und auch keinen Controller dafür, es gibt allerdings Anschlussmöglichkeiten, wie bspw. den linken Expansionsport, welcher für externe Festplatten genutzt werden kann. Der Betrieb erfolgt ursprünglich nur über das Diskettenlaufwerk. Dabei gibt es Spiele, für die man mehr als zehn Disketten nach und nach einlegen musste.\n\nErst mit reichlich Verspätung bot u. a. Commodore das externe Festplattenlaufwerk \"A590\" (inklusive HD-Controller) an, das seitlich an den Erweiterungsanschluss gesteckt wurde. Weiterhin gab es später dann Angebote von Fremdherstellern für Controller mit Festplatten, von denen manche auch in das Gehäuse des A500 hineingezwängt werden konnten – oder auch extern durch selbstgeschaffene „Gehäuselücken“ betrieben werden können. Selbst heute noch wird fleißig an solchen Erweiterungen gebastelt. So gibt es USW-Laufwerke und sogar neue WLAN Lösungen, um auf Medien-Server, auf welchen quasi die ganze Amiga Software-Bibliothek gesichert ist, zuzugreifen und somit als externes Laufwerk fungieren.\n\nAb 1991 gab es mit dem Amiga 500 Plus eine neue Variante des Amiga 500. Er ist mit doppelt so viel Arbeitsspeicher (1 MByte) und mit dem neuen Enhanced Chip Set (ECS) statt dem Original Chip Set (OCS) ausgestattet, welches erstmals VGA-Monitore ansteuern konnte. Der Amiga 500 Plus wurde von Beginn an mit Kickstart Version 2.04 ausgeliefert. Dieses Betriebssystem war zwar technisch ein großer Schritt nach vorne, hatte aber den Nachteil, dass viele bisherige Programme nicht mehr funktionierten, wenn bzw. weil sie direkt – das API missachtend – auf Kickstart-Routinen zugriffen, deren Struktur sich nun geändert hatte. Betroffen waren hier insbesondere Spiele. Sehr bald kamen deshalb Hardware-Erweiterungen auf den Markt, die es erlaubten, zwischen Kickstart 1.3 und 2.0 zu wechseln. Der Amiga 500 Plus fand im Amiga 600 eine wenig erfolgreiche Fortsetzung.\n\nAls interessantes Zwischenspiel wurde der Amiga 500 sogar zum Urahn aller Case-Moddings. Er wurde von Commodore in zwei poppigen Designs (Ball-Design, siehe Bild, und Leoparden-Design) als limitierte Version (10.000 Stück) vertrieben, für die die Fernsehmoderatorin Stefanie Tücking als Namensgeberin gewonnen wurde.\n\n"}
{"id": "28410", "url": "https://de.wikipedia.org/wiki?curid=28410", "title": "Amiga 600", "text": "Amiga 600\n\nDer Amiga 600 ist ein im Jahre 1992 von Commodore eingeführter Computer.\n\nSeine Leistungsdaten entsprachen weitgehend dem des Amiga 500 Plus, er wurde wie dieser mit dem neuen „Enhanced Chip Set“ des Amiga 3000 ausgerüstet, besaß mit einem MB RAM doppelt so viel Arbeitsspeicher wie die ursprüngliche Variante des Amiga 500, eine PCMCIA-Schnittstelle und einen ATA-Controller für eine 2,5-Zoll-Festplatte. Wie auch der Amiga 500 besaß er kein Tower-Gehäuse, sondern war in einem Tastaturgehäuse ähnlich dem Commodore 64 eingebaut.\n\nAls Betriebssystem wurde AmigaOS 2.0 mitgeliefert, bestehend aus Workbench 2.0 und dem Kickstart-ROM mit der Bezeichnung 2.05, von welchem verschiedene Versionen existierten (Versionsnummern 37.299, 37.300 und 37.350). Die ersten Revisionen des Amiga 600 (bis Hauptplatinen-Rev. 1.3) wurden noch mit der Kickstart-Version 37.299 ausgeliefert, welche kurioserweise keinen ATA- und PCMCIA-Support hatte. Man konnte zwar die nötigen Treiber von Diskette nachladen, aber ein direktes Starten des Systems von Medien, die an diesen Bussen angeschlossen werden konnten, war nicht möglich. Erst spätere Modelle des Amiga 600 und insbesondere das Modell Amiga 600HD (mit eingebauter Festplatte im Bundle) wurden mit Kickstart-Versionen ab 37.300 ausgestattet, die sowohl die PCMCIA-Schnittstelle als auch den internen ATA-Controller bereits beim Start des Betriebssystems einbinden konnten. Dabei war die Version 37.300 durch diverse Programmfehler auf die Nutzung von Festplatten bis 40 MB beschränkt (Systeme mit mehr als 40 MB waren instabil), während Version 37.350 mit Platten bis zu vier GB zurechtkam. Optional konnte man später die Workbench 2.1 erwerben, die als Neuerungen eine Lokalisierung des Betriebssystems in verschiedene Sprachen sowie einen „CrossDOS“-Treiber für mit dem FAT-Format (MS-DOS) versehene Disketten/Festplatten mitbrachte. Die Workbench 2.1 lief auf allen Kickstart-ROMs der 2.0x-Familie, ein Kickstart mit der Bezeichnung 2.1 gab es nie.\n\nDer Rechner wurde zu einem der größten Fehlschläge der Amiga-Geschichte. Gründe dafür waren unter anderem die nicht mehr zeitgemäßen Leistungsdaten, der zu hohe Preis und das Fehlen des Nummernblocks auf der Tastatur. Angesichts der Einschränkungen gegenüber dem Vorgängermodell sollte er ursprünglich A300 heißen, wurde aber im letzten Moment in A600 umbenannt und weckte damit falsche Hoffnungen einer allgemeinen Leistungssteigerung. Viele Beobachter meinen, dass diese Entscheidung einer der Sargnägel für die Firma Commodore geworden sei. Außerdem litt der Amiga 600 wie schon sein direkter Vorgänger Amiga 500 Plus an dem Problem, dass einige ältere beliebte Spiele nicht mit Kickstart-Versionen ab 2.0 zurechtkamen. Dies war insofern ein Nachteil, als das Gerät damit für einen großen Teil der angepeilten Zielgruppe – der Spielergemeinde – unattraktiv wurde.\nEin weiteres Manko war die mangelhafte (und von Commodore nie geplante) Aufrüstbarkeit des Amiga 600. Im Gegensatz zum Amiga 500 waren alle Platinenbausteine mit Ausnahme des Kickstart-ROMs fest in SMD-Technik verlötet und nicht mehr gesockelt, so dass viele wichtige Amiga-500-(Plus)-Erweiterungen nicht passten.\n\n\n\"Siehe auch:\" Amiga-Computer.\n\nAufgrund des Alters gibt es mittlerweile das Problem, dass die Elektrolytkondensatoren auslaufen und der Elektrolyt die Leiterbahnen und ICs beschädigt. Daher ist allen Besitzern anzuraten, diese auszutauschen.\nWährend der Amiga 600 in der damaligen Zeit eher unattraktiv war, hauptsächlich wegen einiger Inkompatibilitäten zu Spielen die den Kick1.3 und/oder \"Fastram\" benötigen, gewann er letztlich an Attraktivität und ist heute ein begehrter Amiga in den Retro-Computerforen – hauptsächlich aufgrund von Eigenschaften wie der integrierten PCMCIA-Schnittstelle, die ideal dafür geeignet ist, dort eine Compactflash-Karte mit darauf befindlicher Festplatte einzurichten. Aber auch wegen des kleineren platzsparenden Gehäuses und des bereits integrierten Composite-Video-Ausgangs (es wird also kein TV-Modulator benötigt). Des Weiteren findet man mittlerweile fast immer Diskettenversionen aller klassischen Amiga-Spiele, die auch mit dem Kick2.05 funktionieren, sofern sie keinen Fastram benötigen. Aufgrund guter Aufrüstmöglichkeiten, was den Speicher des Amiga 600 betrifft, ist er auch gut als WHDLoad-Maschine geeignet. Man muss hinzufügen, dass andere Aufrüstmöglichkeiten, aufgrund gelöteter (anstatt gesteckter) ICs, eher beschränkt waren. Im Erweiterungsschacht könnte man den Amiga 600 auf bis zu 2 MB Chipram aufrüsten, sehr viele Programme/Spiel benötigten aber Fastram welchen man im Amiga 600 nicht nachrüsten konnte ohne zu löten. Eine Turbokarte konnte man nicht einbauen, weshalb der ältere Amiga 500(+), mit fast gleicher Hardwareausstattung, durch bessere Aufrüstmöglichkeiten attraktiver war. Später konnte man seltene Turbo- oder Fastram-Karten erwerben, die über bestehende ICs gesteckt werden konnten; dies war jedoch mit Inkompatibilitäten verbunden, außerdem gab es durch diese Verbindung viele Probleme mit Wackelkontakten oder Materialermüdungen. Die letzte Möglichkeit Fastram nachzurüsten gab es über den PCMCIA-Port, aber dies war inkompatibel mit Kick1.3, der ebenfalls von vielen älteren Spielen benötigt wird.\n\n"}
{"id": "28517", "url": "https://de.wikipedia.org/wiki?curid=28517", "title": "Microsoft Word", "text": "Microsoft Word\n\nMicrosoft Word (abgekürzt MS Word) bezeichnet ein Textverarbeitungsprogramm von Microsoft. Es wurde 1983 als \"Multi-Tool Word\" für die Xenix-Plattform eingeführt, darauf aber auch portiert auf PC DOS/MS-DOS des IBM PC (1983), Apple Macintosh (1984), AT&T Unix (1985), Atari ST (1986), SCO UNIX, OS/2 und Windows (1989).\n\nWord gehört zum Microsoft Office-Paket und ist sowohl für Windows als auch für macOS verfügbar. Die aktuelle Version für beide Betriebssysteme ist \"Microsoft Word 2019\".\n\nWord ist das mit Abstand meistverwendete Textverarbeitungsprogramm der Welt.\n\nWord von Microsoft basierte auf dem Konzept der GUI-Textverarbeitung Bravo, die auf dem Xerox Alto lief. Der Autor von Bravo, Charles Simonyi, wechselte 1981 von PARC zu Microsoft und wurde der leitende Entwickler von \"Multi-Tool Word\". 1983 wurde \"Multi-Tool Word\" für Xenix und MS-DOS angekündigt. Microsoft machte Werbung für das Produkt durch Demonstrationsdisketten, die in der US-amerikanischen Computerzeitschrift \"PC-World\" verteilt wurden. Word wurde mit der stetigen Weiterentwicklung zu einer weit verbreiteten Software, nachdem es unter MS-DOS anfänglich unter der ungewohnten Bedienung im Vergleich zu WordPerfect unpopulär war. Bereits in der ersten Version wurde Word für die Bedienung mit einer Maus konzipiert und konnte in Verbindung mit einer Grafikkarte, die Grafikausgabe erlaubte (wie CGA oder Hercules), auf IBM-kompatiblen Geräten Textauszeichnungen wie Fett oder Kursiv, nicht aber die Schriftart oder Schriftgrößen, direkt darstellen. 1985 erschien die Portierung auf den Macintosh. Im Gegensatz zur DOS-Version unterstützte die Macintosh-Version ähnlich wie die Apple-eigene Textverarbeitung MacWrite echtes WYSIWYG. Die Version 1.05 wurde in Zusammenarbeit mit Atari für den Atari ST portiert. Die unter dem Namen Microsoft Write verkaufte Version wurde nicht mehr aktualisiert. Während die DOS-Variante mehrere Konkurrenten wie WordStar, WordPerfect oder IBM PC Text hatten, wurde Word für Macintosh spätestens mit der Version 5.1 im Jahre 1992 marktbeherrschend. Die Macintosh-Version blieb so auch für Jahre teurer als die DOS-Variante. Die erste Windows-Version erschien 1989, die zweite 1991. Mit Word 5.5 für MS-DOS glich Microsoft die Bedienung der DOS-Version an die Windows-Version an, die zu diesem Zeitpunkt die Version 2.0 hatte. Die DOS-Variante verfügte nun ebenfalls über Pulldown-Menüs und eine Symbolleiste und konnte über Dialoge bearbeitet werden. Vom Funktionsumfang her dagegen glich Word 5.5 annähernd der Version 5.0, die ein eigenes, auch mausbasiertes Bedienerkonzept hatte. 1993 wurden Word 6 für DOS, Windows und Macintosh freigegeben. Für MS-DOS war es die letzte Version. Die DOS-Versionen wurden bis 1995 vertrieben. Es existierten auch Versionen für OS/2 und SCO Unix.\n\nWord für DOS war eine der ersten populären Textverarbeitungen für den IBM-PC. In Verbindung mit einer Grafikkarte konnte Word auf IBM-kompatiblen Geräten Textauszeichnungen wie Fett oder Kursiv, nicht aber die Schriftart oder Schriftgrößen, direkt darstellen. Konkurrenten wie WordStar, WordPerfect oder IBM PC Text konnten dagegen lediglich reinen Text darstellen. Formatierungen erschienen dort entweder als farbiger Text, oder es wurden Markups verwendet.\n\nDie erste Windows-Version erschien 1989. Im Gegensatz zu Word für DOS basierte \"Winword\", wie das Programm in den 1990er Jahren umgangssprachlich (nach der ausführbaren Datei codice_1) bezeichnet wurde, auf WYSIWYG. Frühere Windows-Computer stießen an ihre Leistungsgrenzen, daher unterstützte Word für Windows auch eine Entwurfsansicht mit einer Standardschriftart und ausgeblendeten Objekten.\n\nAm 25. März 2014 stiftete Microsoft den Quellcode zu Microsoft \"Word for Windows 1.1a\" dem Computer History Museum. Von dort ist der Quelltext unter einer Microsoft-spezifischen Lizenz für wissenschaftliche Zwecke frei herunterladbar.\n\nDie für Word bis zur Version 2003 bzw. 2004 verwendeten Dateinamenerweiterungen heißen codice_2 für Dokumente und codice_3 für Dokumentenvorlagen. Bemerkenswert dabei ist die Inkompatibilität insbesondere der Versionen bis einschließlich 7 (Microsoft Office 95) und der Versionen ab 8 (Microsoft Office 97). Sie beruht darauf, dass Microsoft die Backup-Funktionalität grundlegend änderte, was der Einführung eines neuen Dateiformates gleichkam, auf eine Änderung der bekannten Dateinamenserweiterung jedoch verzichtete.\n\nWord kann mit verschiedenen Dateiformaten arbeiten, jedoch wurden einige wichtige Fremdformate bis zum Service Pack 2 von Word 2007 beziehungsweise Word 2010 für Windows auf Word für macOS (Mac OS X, OS X) bis heute nicht unterstützt. Inzwischen wurden die Spezifikationen der „alten“ Formate von Microsoft freigegeben. Da der Austausch von Informationen verschiedener, unabhängiger Informationssysteme in zunehmendem Maße immer wichtiger wird und verschiedene Regierungen aufgrund des proprietären Formates gedroht hatten, das Microsoft-Office-Paket nicht mehr länger einzusetzen, wurde Ende Januar 2005 ein neues interoperables Dokumentformat vorgestellt und freigegeben: WordprocessingML. Neuere Microsoft Word Versionen unterstützen das auf XML basierende Format WordprocessingML, und Dokumente in diesem Format lassen sich insbesondere für Fremdprogramme einfacher lesen, verarbeiten und erstellen. Diese Dokumente waren dann aber um einiges größer, da XML einen großen Overhead produziert.\n\nSeit der Windows-Version 2007 bzw. der Mac-Version 2008 werden Dokumente standardmäßig im Format Office Open XML abgespeichert. Aus Kompatibilitätsgründen wird jedoch auch das alte Format weiterhin unterstützt. Dokumente im neuen Format tragen die Dateiendung codice_4 und codice_5 (Dokumente mit Makros). Da dieses Format auf XML basiert, lassen sie sich insbesondere für Fremdprogramme einfacher lesen, verarbeiten und erstellen. Dokumentvorlagen weisen die Endungen codice_6 bzw. codice_7 auf. Da es sich um ein auf ZIP basierendes komprimiertes Dateiformat handelt, sind diese Dateien relativ kompakt. Nach der Installation eines kostenlosen „“ für Microsoft Office in Word für Windows ab Version 2000 (offiziell; das  4 vom 6. Januar 2010 funktioniert aber auch mit Word 97) können diese neuen Dateitypen auch mit älteren Versionen von Word geöffnet, bearbeitet und gespeichert werden. Für macOS bietet Microsoft für Word 2004 einen separat herunterzuladenden Konverter an, der OpenXML-Dokumente in RTF-Dateien konvertiert.\n\nDer Internet Media Type für Dokumente im alten Format ist codice_8. Für Dokumente im Format Office Open XML lautet er codice_9.\n\nBei digitaler Veröffentlichung oder Datenaustausch von Dokumenten im Word-Format sind, falls beim Bearbeiten die Funktionen Schnellspeicherung, Versionsverfolgung oder Änderungsverfolgung aktiviert waren, Einblicke in die letzten Änderungen am Text möglich. Des Weiteren kann es mitunter möglich sein, die ursprünglichen Autoren und den Dateiablageort im lokalen Computer zu ermitteln. Diese Informationen lassen sich jedoch leicht aus dem Dokument entfernen.\n\nWord bietet an, ein Dokument mit Lese- und/oder Schreibschutz zu versehen. Freigeschaltet wird eine so geschützte Datei über ein vom Benutzer einzugebendes Passwort. Bei alten Versionen von Word (2003 und früher) konnte dieser Schutz von einem Angreifer relativ leicht ohne Kenntnis des Passwortes ausgehebelt werden. Neuere Versionen von Word nutzen einen wesentlich besseren Leseschutz. Der Schreibschutz kann noch immer relativ einfach umgangen werden.\n\nDie Automatisierung und Erweiterung von Microsoft Word kann durch \"Visual Basic for Applications\" (VBA) oder unter Windows durch das Verwenden der \"Visual Studio Tools for Office\" bewerkstelligt werden. Vor Word 97 konnte die Automatisierung durch das Verwenden der eigenen Makrosprache (WordBasic) bewerkstelligt werden.\n\nBis zur Version 2004 unterstützte Word für Macintosh VBA. In Version 2008 war eine Automatisierung nur noch mit AppleScript möglich. Word 2011 wurde wieder mit VBA ausgeliefert. Auch in Word 2016 ist VBA fester Bestandteil.\n\nWord 2013 für Windows RT unterstützt VBA nicht und erlaubt dort somit Automatisierungen nur im Rahmen der mit Office 2013 erstmals eingeführten Office-Apps (in Office 2016 in \"Add-Ins\" umbenannt).\n\n\n\n\n\n\n\n\nAb der Version 7.0 werden die Versionsnummern gleich mit den anderen Office-Produkten wie Microsoft Excel, Microsoft PowerPoint, Microsoft Access usw. geführt.\n\n\n\n\n\n"}
{"id": "28535", "url": "https://de.wikipedia.org/wiki?curid=28535", "title": "Amiga 1000", "text": "Amiga 1000\n\nDer Amiga 1000 ist das erste Modell der populären Amiga-Computerreihe, die von Commodore veröffentlicht wurde. Amiga und Atari ST waren es, die 1985 im Bereich des Heimcomputers den Schritt von 8-Bit auf 16-Bit (im Betriebssystem selbst sogar auf 32-Bit) maßgeblich voranbrachten. Der Amiga war für die damalige Zeit eine Revolution am Computermarkt, da seine Architektur viele Merkmale bot, die andere Systeme erst sehr viel später enthalten sollten. Die Systemarchitektur über anwendungsspezifische integrierte Schaltungen und die grafische Leistungsfähigkeit suchten im Heimcomputerbereich damals ihresgleichen.\n\nDer Amiga 1000 wurde am 23. Juli 1985 von Commodore in den USA vorgestellt und wenig später dort in den Handel gebracht. Weil er als Profimaschine vermarktet wurde trug er nicht den blauen Commodore-Schriftzug, da die Firma in den USA nicht den hohen Status der Mitbewerber IBM und Apple besaß, sondern eher als Hersteller von Spielzeugcomputern galt; bei der Markteinführung in Deutschland ein Jahr später sah die Lage schon anders aus. Die Technik des Amiga 1000 stammt maßgeblich von Jay Miner, einem der Gründer der Firma Amiga, weitere Details zur bewegten Vorgeschichte siehe im allgemeinen Amiga-Artikel.\n\nDie von Frank Elstner moderierte Vorabpräsentation in Deutschland wurde in Frankfurt am Main am 21. Mai 1986 in der \"Alten Oper\" für die Presse und ausgesuchte Besucher abgehalten. Als Präsentation der Fähigkeiten des Amiga 1000 gab es unter anderem die Animation einer Tänzerin in 4096 Farben und den berühmten Amigaball zu sehen und zu hören. \n\nIn Deutschland wurde der Amiga 1000 auf der CeBIT 1986 offiziell vorgestellt und kam kurz darauf in die Läden. Kennzeichen war der blaue Commodore-Schriftzug auf der linken Vorderseite. Eine Besonderheit hierbei war, dass die ersten Amiga 1000 in Deutschland noch keine deutsche Tastatur besaßen. Sie wurden mit einer amerikanischen Tastaturbelegung ausgeliefert, die man mit einem mitgelieferten Aufklebersatz in die in Deutschland übliche umetikettieren konnte. Später waren die „1000er“ auch mit deutscher Tastatur erhältlich.\n\nAbgelöst wurde der Amiga 1000 durch den Amiga 500 und den Amiga 2000.\n\nDie Grundausstattung des Amiga 1000 besteht aus dem eigentlichen Rechner, einem 14-Zoll-Farbmonitor (A1081, A1080 in den USA), der Tastatur, die platzsparend unter den Rechner geschoben werden kann (\"Keyboard-Garage\"), und einer 2-Tasten-Maus.\n\nEr besitzt 256 kB Kickstartspeicher und 256 kB Arbeitsspeicher, der mit einem unter der Frontblende einzubauenden Modul auf 512 kB erweitert werden kann, und ein internes 3,5\"-Diskettenlaufwerk mit einer Kapazität von 880 kB. Die CPU MC68000 von Motorola ist mit 7,14 MHz (NTSC-Version) bzw. 7,09 MHz (PAL-Version) getaktet. Besonders die Grafikleistung und Soundfähigkeiten des Amiga 1000 konnten dank der speziellen Custom-Chips des Chip-Designers Jay Miner zur damaligen Zeit überzeugen. Die Ur-Version des Amiga 1000 basierte noch auf dem ICS-Chipsatz, der nur NTSC unterstützte und den EHB-Modus noch nicht kannte, der die Darstellung von 64 Farben erlaubt.\nDie Schnittstellen des Amiga 1000 galten zur damaligen Zeit als modern: Parallelport ähnlich Centronics-Norm, Serielle RS-232-Schnittstelle, Anschluss für zusätzliche Diskettenlaufwerke, Tastaturport, Audio (Stereo), TV-Ausgang, RGB-Video, 2 Maus- bzw. Joystick-Ports, einen proprietären 86-poligen DMA-fähigen Bus-Port und den Frontport für die 256-kB-Speichererweiterung. Der Parallelport ist nicht konform zum parallelen Port des PCs, so dass ein direkter Anschluss von PC-Druckern am Amiga 1000 zum Schaden führen kann. Nur beim Amiga 1000 ist das Geschlecht (Stift/Buchse) der parallelen und seriellen Schnittstellen genau umgekehrt wie bei PCs. Hinsichtlich der Belegung der Schnittstellen gibt es zu anderen Amiga-Modellen einige Unterschiede gerade bei den spannungsführenden Pins, so dass zum Anschluss externer Hardware wie zum Beispiel eines Midi-Interfaces spezielle Adapter notwendig sein können. \n\nWeiterhin kann am RGB-Anschluss ein Genlock angeschlossen werden, mit dem man den Rechner mit einer TV-Quelle synchronisieren und so das Computerbild mit dem Videobild überlagern konnte, um damit z. B. Videoeinblendungen zu erstellen. So wurde der Amiga zu seiner besten Zeit zur Videonachbearbeitung auch professionell rege genutzt.\nZusatzkarten, die über den Erweiterungsslot an der rechten Seite angeschlossen werden, wird vom Betriebssystem ein sogenanntes Autoconfig-Protokoll zur Verfügung gestellt, welches ihnen erlaubt, sich sauber ins System einzuklinken und notwendige Treiber selbst zur Verfügung zu stellen, ohne dass der Anwender von dem irgendetwas bemerkt.\n\nDie ersten in Deutschland verfügbaren Amiga 1000 haben intern ein Piggiback, also eine zusätzliche Leiterplatte, die huckepack auf die Grundplatine des Rechners aufgesteckt ist und das Kickstart-RAM beinhaltet.\nSpätere Modelle haben dieses Piggiback nicht mehr, da das Kickstart-RAM in die Grundplatine integriert wurde.\nDer Amiga 1000 in NTSC-Version hat einen Systemtakt von 28,6 MHz, der CPU-Takt beträgt 7,14 MHz. Die PAL-Version hat einen Systemtakt von 28,36 MHz und die CPU wird hier mit 7,09 MHz getaktet.\n\nDer Schaltplan des Amiga 1000 passt in lesbarer Größe auf ein einzelnes DIN-A3-Blatt.\n\nDer Amiga 1000 besitzt als einziger Rechner der Amiga-Reihe einen 256 kB großen zusätzlichen resetfesten RAM-Bereich, in welchen der Kickstart geladen wird.\nDieser Umstand ist der Tatsache zu verdanken, dass zum Zeitpunkt der Herstellung des A1000 noch keine fertige Kickstartversion zu Verfügung stand und daher das Kickstart noch nicht in einem ROM verfügbar war.\nDen ersten Geräten lag zunächst die Kickstartversion 0.9 und 1.0 bei, kurz nach Markteinführung bereits die Version 1.1. Daher muss dieser Teil des Betriebssystems nach jedem Einschalten von einer Bootstrap-Diskette geladen werden, verbleibt aber auch nach einem Reset im Rechner (im sog. WOM=Write Once Memory). Dieser Umstand hat aber auch Vorteile, z. B. existiert eine Kickstart-Version, die mit einem Antivirenprogramm gepatcht wurde und so Bootviren erkennen kann.\n\nDas 32-Bit-Betriebssystem AmigaOS zeichnet sich durch präemptives Multitasking und die fensterorientierte grafische Benutzeroberfläche „Intuition“ aus.\nWeiterhin besitzt das Betriebssystem zusätzlich ein CLI Command Line Interface. \n\nAls Besonderheit sind auf der Innenseite des Gehäusedeckels des Amiga 1000 die Unterschriften der wichtigsten an der Entwicklung beteiligten Personen eingraviert – einschließlich eines Pfotenabdrucks des Hundes des geistigen Vaters des Amigas, Jay Miner.\n\nAls Antwort auf die zunehmenden Marktanteile durch MS-DOS entwickelte Commodore in Braunschweig für den Amiga 1000 die PC-Erweiterung Sidecar (A1060) mit eigenem 8088-Prozessor und XT-Steckplätzen. Dieses Konzept, dem Amiga einen PC-Mitläufer zu geben, wurde bei nachfolgenden Amiga-Modellen konsequent weitergeführt, so dass selbst der Amiga 4000 noch sogenannte Brückenkarten aufnehmen kann. Ob dieses Konzept zur Zufriedenheit aller Commodore-Mitarbeiter und Entwickler war, liest sich in dem Teil des Amiga 2000-Schaltplans, der die PC-Slot-Erweiterung beinhaltet, wie folgt: \n\n"}
{"id": "28564", "url": "https://de.wikipedia.org/wiki?curid=28564", "title": "SIMpad", "text": "SIMpad\n\nDas SIMpad ist ein von Siemens Schweiz im Jahre 2001 auf den Markt eingeführter Tabletcomputer. Die Entwicklung stammt von der Firma Keith & Koep.\n\nEr ist mit einem berührungssensitiven Touchscreen für die Bedienung mit einem Stift ausgelegt und kann beispielsweise als Webpad, E-Text-Reader oder zum Wardriving eingesetzt werden. Es ist als Universal-Fernbedienung auch Bestandteil der Technik im Futurelife-Haus.\n\nEs gibt mehrere Modellvarianten, von denen jedoch keine mehr produziert wird:\n\nAlle Versionen enthalten:\n\nAlle Geräte haben ein Gewicht von etwa 1 kg bei Maßen von 180×263×28 mm.\n\nAls Betriebssysteme gehörten Windows CE Version 2.12 (CL4), 3.0 (alle Modelle) oder 4.0 (nur SL4 und SLC, auch bekannt als \"CE.net\") zum Lieferumfang. Updates auf Windows CE 4.1 und 4.2 sind im Internet verfügbar, bzw. kommerziell erhältlich.\n\nDa es sich bei allen SIMpads um ausgelaufene Modelle handelt, ist inzwischen kein Herstellersupport mehr verfügbar und es werden keine Updates mehr erscheinen. Im Gebrauchtmarkt sind Geräte und Zubehör jedoch noch gut erhältlich und es existieren viele weiterführende Informationen im Web. Des Weiteren bietet das Familiar-Projekt eine auf OpenEmbedded basierende Linux-Portierung für alle SIMpad-Modelle.\n\n"}
{"id": "29028", "url": "https://de.wikipedia.org/wiki?curid=29028", "title": "WWWOFFLE", "text": "WWWOFFLE\n\nWWWOFFLE (Abkürzung für World Wide Web Offline Explorer) ist eine freie Server-Software für die Betriebssysteme Linux und BSD. Es wird als Proxy eingesetzt und ist außerdem als Cache zu gebrauchen.\nWWWOFFLE kann mit HTTP, FTP und Finger umgehen.\n\nBesonders geschätzt wurde WWWOFFLE von Besitzern teurer Internetanbindungen, da er Webanfragen sammeln und zu einem späteren Zeitpunkt gebündelt abarbeiten kann. Dies reduziert die Zeit, in der man zum Surfen mit dem Internet verbunden sein muss.\n\nWWWOFFLE ist freie Software unter den Bedingungen der GNU General Public License (GPL).\n\nTeilweise diente der Internet Junkbuster als Vorbild für WWWOFFLE. Selber diente es wiederum als Vorbild für Polipo, das aus Unzufriedenheit über die Unzulänglichkeiten von WWWOFFLE geschrieben wurde.\n\n"}
{"id": "29405", "url": "https://de.wikipedia.org/wiki?curid=29405", "title": "Tandy TRS-80 Model 1", "text": "Tandy TRS-80 Model 1\n\nDer TRS-80 Modell 1 von RadioShack gehörte neben dem Apple II und dem Commodore PET 2001 zu den ersten Heimcomputern, die in nennenswerten Stückzahlen als Fertigexemplare produziert wurden. Er kam 1977 auf den Markt und war mit einer Z80-CPU von Zilog ausgestattet, die mit 1,77 MHz Taktfrequenz lief. Der TRS-80 hatte besonders in den USA gute Verkaufserfolge. Der Neupreis betrug in Deutschland 3000 DM.\n\nZunächst wurde er nur mit 4 KiB RAM (der innerhalb der Tastatur auf 16 KiB aufgerüstet werden konnte) ausgeliefert. Typisch für die damalige Zeit war das im ROM gespeicherte Betriebssystem in Form des BIOS und das integrierte BASIC. Mit Hilfe eines \"Expansion Interface\" ließ er sich um weitere 32 KiB RAM, einen Diskettencontroller für bis zu vier 5,25\"-Laufwerke sowie eine serielle und eine parallele Schnittstelle erweitern.\n\nDer TRS-80 Model 1 besaß eine Grafikauflösung von 128 × 48 Pixeln. Erzeugt wurde diese Auflösung durch die Zeichenzellen des Bildschirms, die in 6 kleine Blöcke aufgeteilt waren. In Wirklichkeit handelte es sich also um Blockgrafik aus Sonderzeichen des 64 × 16 Zeilen Textmodus, die geschickt eingesetzt werden mussten. Als Bildschirm konnte über einen zuzukaufenden HF-Modulator jedes handelsübliche Fernsehgerät benutzt werden, sofern man keinen Wert auf hohe Bildqualität legte; ursprünglich wurde ein Monitor mitgeliefert, der mit schwarz-weißer oder schwarz-grüner Anzeige erhältlich war.\n\nEs gab vom Model 1 einen Level I mit 4 KiB ROM und einem simplifizierten BASIC, und einen Level II mit 12 KiB ROM und einem BASIC, das bereits dem damals populären Microsoft-Standard entsprach (und auch von dort lizenziert war) und sich mittels des Expansion Interface um Diskettenbefehle erweitern ließ.\n\nEines der Kultprogramme der damaligen Zeit war trotz der Klötzchengrafik das Spiel \"Dancing Demon\", in dem ein kleiner niedlicher Teufel auf dem Bildschirm tanzte und der Monosound der Cassettenrecorderschnittstelle den Benutzer unterhalten sollte.\n\nIn der Originalausstattung wurde aus Kostengründen am Bildschirmspeicher pro Byte ein Bit gespart. Dadurch war der TRS-80 nicht in der Lage, Kleinbuchstaben darzustellen. Wollte man dies erreichen, musste man einen Speicherbaustein nachträglich einbauen, der dann das Bit Nummer 6 in jedem Byte des Bildschirmspeichers zur Verfügung stellte.\n\nAnfangs wurden die Daten noch auf Compact-Cassette gespeichert, doch sehr bald kamen auch diverse Disk Operating Systeme (DOS) hinzu. Am bekanntesten waren das TRSDOS von Tandy und das sehr fortschrittliche NewDos 80 von Apparat. TRSDOS und NewDos 80 unterstützten zunächst 35 Track Single Sided Single Density 87,5 KiB Disketten im 5,25\" Format, später mit neueren Laufwerken und Controller auch 40 und 80 Track Double Sided / Double Density, so dass 720 KiB speicherbar waren.\n\nDas NewDos 80 ermöglichte schon damals im Heimcomputer-Bereich ungewöhnliche Projekte wie das Fernsteuern eines Tandys mittels Modem oder Akustikkoppler über das öffentliche Telefonnetz von einem anderen Rechner aus. Die ersten Mailboxen bzw. BBS' entstanden, in denen Benutzer Beiträge in virtuellen schwarzen Brettern veröffentlichen konnten. Es gab auch schon Anfang der 1980er Jahre Kataloge mit Dutzenden Büroprogrammen von unterschiedlichsten Herstellern wie Text, Kalkulation, Fibu, Lager etc. für den TRS 80, was sich aber ab ca. 1982 nach der Einführung des IBM-PCs mit MS-DOS und danach diversen \"Kompatiblen\" schnell erledigte.\n\nBasierend auf dem TRS-80 Model 1 gab es auch die Modelle III und 4 (in dieser Schreibweise; das erste und das vierte Modell wurden im Katalog mit einer arabischen Ziffer versehen), sowie den Portable 4P. Alle wurden mit 15/32/48 oder 64 KiB RAM, einem integrierten s/w-Monitor sowie der Möglichkeit zum Einbau von bis zu zwei 5,25-Zoll-Diskettenlaufwerken angeboten und aufgrund der Preislage und Ausstattung bereits zu den Bürocomputern gezählt.\n\nDer Video Genie war ein von Firma EACA in Hongkong produzierter und in Deutschland von Trommeschläger Computer Service vertriebener Nachbau des Model 1. Der Rechner wurde in Australien als System 80, in den USA als PMC-80/81, in Ungarn als HT-1080Z und in Südafrika als TRZ-80 vertrieben. Der Nachbau war praktisch zu 100 % kompatibel und wies nur einige Unterschiede bei der Hardwareansteuerung sowie ein abweichendes Businterface auf. Im Gegensatz zum TRS-80 Model 1 wurde das Video Genie niemals mit einem Monitor ausgeliefert, hatte dafür aber einen Videomodulator und einen Kassettenrekorder integriert. Das Video Genie II wurde als Profirechner vermarktet. Das Kassettenlaufwerk wurde dabei durch eine numerische Tastatur ersetzt. Das Color Genie enthielt eine Erweiterung für Farbgrafik, war jedoch anders als der TRS Color Computer weiterhin zum TRS-80 Model 1 kompatibel.\n\nEin weiterer Nachbau des TRS-80 Model 1 wurde unter dem Namen KOMTEK in Deutschland verkauft, verschiedentlich existieren private Nachbauten auf der Basis von modifizierten Z80-Einplatinenrechnern.\n\nDie polnische Firma Mera-Elzab baute Anfang der 1980er die Computerserie Meritum (zuerst Meritum I und seit 1985 Meritum II), die auf dem TRS-80 basierte. Der Unterschied zum TRS-80 lag hauptsächlich darin, dass bei der Produktion nur im Ostblock gefertigte Bauteile (einschließlich U880D) verwendet wurden. Dieser Klon eines TRS-80 wurde in Polen schnell sehr populär und hatte einen entscheidenden Einfluss auf die Computerisierung in Schulen, verlor jedoch später gegen die westlichen Mitbewerber Atari, Commodore und Sinclair.\n\nAuch in Brasilien entstanden verschiedene Nachbauten – z. B. die Dismac Serie D8000/D8001/D8002 (Model 1 kompatibel), die Digitus DGT-100 and DGT-1000 und die erfolgreichen CP300 und CP500 (Model III) von Prológica.\n\nBis zum Erscheinen des ersten PC-kompatiblen Tandy 2000 wurde die Marke TRS-80 für alle Computerprodukte der Firma Tandy verwendet. Dies umfasste sowohl weitere Rechner auf Z80-Basis mit Ähnlichkeit zum TRS-80 Model 1, als auch völlig andere, meist eingekaufte Serien, wie den Color Computer (Motorola), den Handheld-Rechner Model 100 (Kyocera) oder die Taschencomputer PC-1 bis PC-8 (Sharp/Casio).\n\nModel II war nicht zum Model 1 kompatibel, erlaubte dafür aber ohne Veränderungen an Soft- oder Hardware den Einsatz von CP/M anstelle TRS-Dos und verfügte über die damals im Profibereich vorherrschenden 8\"-Laufwerke. Das Model 12 war eine erweiterte Version des Model II. Das Model 16 war ein mit einem 6-MHz-68000-Prozessor erweitertes Model 12. Die Z80 diente dabei als I/O-Prozessor. Mit TRS-Xenix, einer OEM-Version von Microsoft Xenix waren das Model 16B (8 MHz 68.000) und der Nachfolger Model 6000 1984 mit 40.000 Einheiten die meistverkauften Unix-Rechner in den USA. Das Modell II wurde z. B. bei bayerischen Sparkassen zur Kreditberechnung eingesetzt.\n\nEin weiterer, nicht mit dem Model 1 verwandter Rechner war der gemeinsam mit Motorola auf Basis des Motorola 6809 entwickelte Tandy TRS-80 Color Computer (CoCo) sowie der TRS-80 M10.\n\nFragwürdig war die damalige Marketingpolitik von Radio Shack, die zum Beispiel in ihrem Katalog diese recht teuren Bürocomputer (Modell II über 10.000 DM) neben Plüschtieren mit eingebautem Radio platzierten. Auf diese Art disqualifizierte sich Tandy/Radio Shack gegenüber professionellen Kaufinteressenten. Die Geräte wurden in Europa in eigenen TANDY-Läden ähnlich wie heute bei Media Markt neben Hifi und Elektronik vertrieben. Daneben gab es diverse Computer-Franchiser und reine Tandy-Computer-Stores.\n\nEbenfalls fragwürdig war die Konstruktion der für den deutschen Markt angebotenen Netzteile. Diese basierten auf den Netzteilen für den US-Markt und waren mangelhaft angepasst, wodurch sie erhebliche Mengen Abwärme produzierten. Einige Anwender betrieben ihre Rechner zur Abwärmereduktion und Schonung der Hardware – über Regeltransformatoren – problemlos mit einer Netzspannung von ca. 180 V (rund 20 % unter der damaligen Netzspannung von 220 V).\nZugutehalten muss man TANDY (Radio Shack), dass sie Ende der 1970er in Europa einen funktionierenden Vertrieb mit eigenen Filialen aufzogen und so auch Leuten ohne Zugang zur Computerszene den Erwerb ermöglichten. Leider wurde der deutsche Vertrieb aus steuerrechtlichen Gründen ab 1985 wieder abgewickelt, obwohl die meisten Filialen noch hohe Gewinne erwirtschafteten.\n\nBereits damals schlossen sich Benutzergruppen intensiv zusammen, auch als noch keine preiswerten (bzw. in Deutschland erlaubten) Modems bzw. Akustikkoppler erhältlich waren. Es gab größere bekannte Benutzerzeitschriften namens 80 micro und 80-NW, die später in der überregionalen 80-US aufging.\n\n"}
{"id": "29745", "url": "https://de.wikipedia.org/wiki?curid=29745", "title": "Digitale Kunst", "text": "Digitale Kunst\n\nDigitale Kunst oder Digitalkunst, oft gleichbedeutend mit Computerkunst gebraucht, sind im allgemeinen Sprachgebrauch Sammelbegriffe für Kunst, die digital mit dem Computer erzeugt wird. Im engeren Sinn ist es Kunst, die nur durch die spezifischen Eigenschaften digitaler Medien möglich geworden ist, zum Beispiel die Zählbarkeit aller Information, ihre Trennbarkeit von einem bestimmten Datenträger oder den Einsatz von Algorithmen. – Erst in den 1990er-Jahren wurde der Ausdruck \"digitale Kunst\" gebräuchlich.\n\nDie digitale Kunst zählt zur Medienkunst. Im Zusammenhang mit digitaler Kunst werden teils sich überschneidende Begriffe verwendet:\n\n\"Elektronische Kunst\" können alle künstlerischen Arbeiten und Werke genannt werden, die funktionierende analoge oder digitale Elektronik enthalten, sei es in Kunstgattungen wie Architektur, Performance, Tanz, Bildhauerei und Musik, oder in neuen Bereichen wie Robotik oder Computeranimation. Elektronische Kunst war in ihren Anfängen analog und muss heute nicht notwendig digital sein.\n\n\"Computerkunst\" war anfangs eine Sammelbezeichnung für alle mit einem Computer in zentraler Funktion hervorgebrachte Kunst. Sie war vor allem in ihren Anfängen nicht immer digital, es konnten Analogrechner eingesetzt werden. Als Audio-Computerkunst galt beispielsweise die Verwendung von Computern als Live-Musikinstrument (vgl.)\n\n\"Digitale Kunst\" beruht auf digital kodierter Information. Die Information wird meist über Computer digital verarbeitet und künstlerisch verwendbar präsentiert. Die Digitalisierung von Information kann durch Eingabegeräte, wie Scanner, Tastaturen und Messgeräte erfolgen. Viele Formen digitaler Kunst sind durch den Unterschied zwischen dem rein digitalen Werk, den binären Dateien, und ihrer hörbaren und sichtbaren Repräsentation geprägt. Liegen Werke in digital kodierter und gespeicherter Form vor, etwa als Bilddateien, Algorithmen, Hypertexte, ausführbare Programme oder Code für Internetseiten, kann von digitaler Kunst im engeren Sinne gesprochen werden. In bestimmten Fällen sind die Dateien und digitalen Vorgänge sogar das eigentliche Kunstwerk. Ein künstlerisch bedeutender digitaler Code handschriftlich notiert, kann bereits digitale Kunst sein.\n\nSind Werke digitaler Kunst ohne ihre Repräsentation in einem Medium nicht vollständig, kann durch ausgesuchte Hardware und Software die Repräsentation künstlerisch beeinflusst werden. Mit Software, die bei Veränderungen von Variablen zu nicht genau vorhersehbaren künstlerischen Ergebnissen führt, ist sogar ergebnisoffene Gestaltung möglich. Die Repräsentation einer Datei muss nicht auf die Wiedergabe durch Lautsprecher, Monitore und Projektionen beschränkt sein: Sie kann dreidimensionale materielle Plastiken, Lichtinstallationen oder multimediale Ereignisse umfassen. Bei biennale.py, dem Virus für die 49. Biennale Venedig, bestand die Repräsentation aus einer kollektiven multimedialen Performance, in der Publikum und Medien, oft ohne es zu bemerken, zu unverzichtbaren Akteuren wurden.\n\n\"Computergenerierte digitale Kunst\" und \"Mediale digitale Kunst\" werden in der kunsthistorischen Betrachtung unterschieden (siehe Literatur, Christiane Paul, \"Digital Art\"):\n\nComputergenerierte Kunst entsteht auf Basis traditioneller Kunstgattungen und Kunstformen mit dem Computer als Werkzeug. Vormals analoge Bereiche der Bildkunst wie Druck, Malerei, Fotografie werden digital weitergeführt. Ebenso werden Kunstwerke im Bereich der bewegten Bilder, Video und Film und der Übertragungsmedien Radio und Fernsehen, nach Konventionen aus der analogen Tradition der Medienkunst, zunehmend digital erzeugt. Gleiches gilt für „elektronische oder digitale Musik“ (siehe unten: Computergenerierte Kunst).\n\n„Mediale digitale Kunst“ hingegen, nutzt Computer und Netzwerke, oder andere digitale Geräte, wie Mobiltelefone, als eigenständiges Medium, unter anderem auf Basis des Internets, so dass eigene Kunstformen wie Netzkunst (\"Net Art\"), (Software Art), digitale Installationen und Kunstwerke im Bereich Virtuelle Realität entstehen (siehe unten: Mediale digitale Kunst).\n\nDas Festival Ars Electronica in Linz geht auf entscheidende Entwicklungen in allen Bereichen elektronischer Kunst ein. Computerkunst und digitale Kunst sind ständige Schwerpunkte. Die Ars Electronica inspiriert künstlerische Arbeiten und fördert Künstler durch den manchmal als „Oscar der digitalen Kunst“ bezeichneten Prix Ars Electronica.\n\nDigitale Bildkunst, meist kurz „digitale Kunst“ genannt, fällt weitgehend unter „computergenerierte Kunst“ und wird vorwiegend auf zweidimensionale Medien (Web/Print/Projektion) ausgegeben. In allgemeinerem Sinn werden visuelle Ausgaben eines Computers als Computergrafik bezeichnet.\n\nDifferenzierungen digitaler Bildkunst:\n\nGFX ist eine spielerische Abkürzung für \"Graphical Effects\". Ähnlich analogen Kunstformen, wie Comics oder Graffiti, die mit Jugendszenen und Jugendkultur verbunden sind, ist GFX mit der GFX-Szene beziehungsweise der Demoszene verbunden, einer digitalen Jugendkultur oder Netzkultur, in der grafische Effekte ursprünglich für Spielecomputer und Spielkonsolen wie C64, Atari ST, Commodore Amiga oder Xbox programmiert wurden. Auch heute beruht die Programmierung von GFX Grafiken und Animationen auf Verfahren, die besonders geeignet sind, vom Computer direkt ausgegebene Grafik und Klang in außergewöhnliche Qualität aufeinander abzustimmen und damit die Möglichkeiten des Computers zu demonstrieren. GFX ist eine eigenständige Kunstform, die einerseits als angewandte Kunst untrennbar mit Computerspielen verbunden ist, andererseits aber auch eigenständige Werke hervorbringt, die als zeitgenössische Kunst gesammelt werden (etwa Yehoshua Lakners AVZGs).\n\nElektronische Musik ist anfangs, ähnlich früher visueller Computerkunst, mit analogen Generatoren wie Oscillions, Melochord, Trautonium, Ringmodulatoren, Rauschgeneratoren und Filter erzeugt worden. Computermusik und elektronische Musik sind heute weitgehend digitalisiert. Eine der ersten musikalischen Anwendungen eines digitalen Computers war 1957 Lejaren Hillers digitale Musikkomposition ILLIAC-Suite (für ILLIAC siehe Liste der Röhrencomputer). Iannis Xenakis entwickelte 1961 seine Kompositionstheorie auf Grundlage des neuen Verfahrens der Klangsynthese mit Klangquanten. Der Physiker Wilhelm Fucks gilt mit seiner computergenerierten Komposition von 1963 \"Quatro Due\" als Pionier der digitalen Musik in Deutschland. Aufgrund langer Rechenzeiten und geringer Speicherkapazität setzte sich die voll digitalisierte Musikausgabe erst gegen Ende der 1970er-Jahre mit einer schnelleren Computergeneration durch.\n\nMediale digitale Kunst tendiert zu Interaktivität und Multimedialität. Es geht um die Interaktion zwischen Kunstwerk und Mensch, wobei die Teilnehmer mit dem Kunstwerk ein komplexes Netz und eine künstliche Welt bilden können. Als vorwiegend visuelle und auditive interaktive Benutzerschnittstelle dienen Webseiten, Programmoberflächen, Spieloberflächen, und softwaregesteuerte Installationen. Für haptische Interaktion werden raumorientierende Zusatzgeräte eingesetzt.\n\nZu den neuen Kunstformen zählen:\n\nUnter dem Sammelbegriff Netzkunst wird künstlerische Arbeit in analogen oder digitalen Netzen mit Arbeit in künstlerischen Netzwerken zusammengefasst. Internet- oder Webpräsenzen, die entweder durch ihr programmiertes Verhalten oder die Interaktion mit dem Besucher am Bildschirm eigenständige Kunstwerke bilden, können unter beide Kategorien fallen.\n\nMit dem Beginn der globalen Verbreitung des World-Wide-Webs, ab 1993 mit dem Mosaic Browser, wurden Webseiten als künstlerisches Medium interessant. Es entstand eine Form digitaler Netzkunst als Kunst mit Webseiten, für die ab 1994 im englischen Sprachraum Bezeichnungen wie Internet Art und Web Art üblich wurden. Besonderen Einfluss hatte eine Gruppe von Künstlern unter dem Namen net.art. Dazu zählten Vuk Ćosić, Jodi.org, Alexei Shulgin, Olia Lialina, Heath Bunting, Rachel Baker, Minerva Cuevas, Daniel García Andújar und Marcus Valentin. Die Werke der Künstler hatten inhaltlich wenig gemeinsam. Die Gruppe wurde von Autoren wie Tilman Baumgärtel, Josephine Bosma, Hans Dieter Huber und Pit Schultz als Parodie einer Avantgardebewegung beschrieben.\n\nNetzkunst und digitale Kunst überschneiden sich in weiten Bereichen, vor allem dort, wo digitale Kunst telematische Netze einbezieht oder gemeinsame Kunstwerke durch die Vernetzung von Teilnehmern ermöglicht.\n\nAls einer der Ursprünge der Softwarekunst gelten kreativ kodierte Computeroperationen, die konventionelle Programmierungen übertreffen und manchmal sogar durch Regelbrüche besser funktionieren. Zuerst am Massachusetts Institute of Technology wurde dafür die Bezeichnung „Hack“ verwendet. Software Art als bewusst eigene Kunstform wurde jedoch erst gegen Ende der 1990er-Jahre zum Beispiel durch de-programmierte Computerspiele (vergleiche) oder andere herunterladbare Programme (Beispiele: runme.org) allgemein bekannt, die den Nutzer dazu anhalten, sein Verhältnis zu Internet, Rechner und eigenem Nervensystem zu überdenken.\n\nDigitale Poesie begann einerseits mit dem Computer als bloßes Werkzeug für Dichtung, andererseits jedoch als Untersuchung der nur im Medium Computer entwickelbaren Möglichkeiten, Zeichen, Worte und Sprache zu generieren. Frühe digitale Poesie schrieb bereits Raymond Queneau 1962. 1980 entwarf Jean-Pierre Balpe den ersten algorithmischen Gedichtgenerator „Poèmes d’Amour“. Heutige digitale Poesie geht über Wortspiele, Dichtungen, visuelle Poesie und programmierte Dialoge oft weit hinaus und kann in allen geeigneten Erscheinungsformen medialer digitaler Kunst auftreten.\n\nEin Ereignis auf der Biennale 2001 Venedig war Net Poetry online im „Parallel-Action-Bunker“ von Caterina Davinio. Ebenso für Net Poetry seit den 1990er-Jahren zu nennen: jodi.org (Joan Heemskerk und Dirk Paesmans).\n\nIn Computersprachen wie Algol oder Perl verfasste Code Poetry ist nicht nur computergeneriert, sie kann unter Umständen als Code tatsächlich ausführbar sein. Gleiches gilt für Scriptsprachen (wie JavaScript, Perl, Python und AppleScript), die zur Bedienung von Computern verwendet werden, deren Scripte jedoch gleichzeitig als poetische Mitteilung verfasst sein können.\nDer Besucher wird zum Benutzer und interagiert in einer Kunstinstallation durch Körperbewegung, Geräusche, Laute, Sprache oder über andere Medien (Mobiltelefone und Anderes) mit Programmen, die meist visuell und akustisch antworten. Typische digitale „Interaktive Installationen“ in dieser Art sind einige Werke von Michael Saup und von Projekt Blinkenlights.\n\nDie Virtuelle Realität kann wesentliches Element begehbarer softwaregesteuerter Installationen und Projektionen sein. Virtuelle Räumlichkeiten und andere digitale Kunstwelten haben bildhauerische und architektonische Aspekte. Sie können mit Text, Grafik, Animation, Sprache, virtueller Audio-Realität und sogar körperlicher Erfahrung als virtuelle Existenz erlebt werden. Durch den Eintritt in virtuelle Räume als virtuelle Persönlichkeit oder Avatar, kann sich eine gelebte Traumwelt entwickeln.\n\nUnter Virtuelle Realität fallen:\n\n\nDer Futurismus zu Beginn des 20. Jahrhunderts begrüßte den Einsatz von Maschinen. Elektronische Musikinstrumente wie das Trautonium (1924) sind aus dieser Zeitstimmung hervorgegangen. Mit der Tontechnik, der Funktechnik und später der Fernsehtechnik konnte seit Beginn des 20. Jahrhunderts elektronische Kunst erzeugt werden, die noch nicht digital war. Schon viel länger gab es dagegen Kunst, die auf spielerisch-numerischen Gestaltungsmitteln aufbaute wie dem Spielwürfel. Sie ist im Wortsinn digital, ohne dass Maschinen zu Hilfe genommen werden müssten.\n\nPioniere des künstlerischen Einsatzes elektronischer und digitaler Geräte gab es an Institutionen wie den Bell Laboratories, dem Massachusetts Institute of Technology (MIT), dem Kölner Studio für elektronische Musik und an der Technischen Hochschule Stuttgart, an der Max Bense unterrichtete.\n\nStudios für elektronische Musik gab es früher als Studios für Computerkunst. Am Beginn elektronischer Kunst standen Musiker. Ben F. Laposky, der die ersten elektronischen Grafiken erzeugte, bezeichnete sie als Visuelle Musik. Der Beginn der Computerkunst ist in mehrfacher Hinsicht mit elektronischer Musik verbunden:\n\n\nAllerdings blieb analoge Elektronik in der Musik noch Grundlage der Wiedergabe bis Computer für die Klangwiedergabe in Echtzeit schnell genug wurden.\n\nSeit Anfang der 1950er-Jahre bis in die 1960er benutzten audiovisuell orientierte Künstler Kathodenstrahlröhren, um durch Ablenkung des Kathodenstrahls auf einem Bildschirm Bilder zu erzeugen. Anfangs waren die Bilder stark davon geprägt, dass es sich bei den Geräten im Prinzip um Oszillografen handelte. Ben F. Laposky, der ab 1950 auf solchen Bildschirmen Bilder generierte, nahm diese animierten Kurvengrafiken mit Hochgeschwindigkeitskameras auf und nannte sie „Oscillons“. Herbert W. Franke nutzte 1953 bis 1956 ebenso analog gesteuerte Geräte, und nannte einen Teil der entstandenen Werkgruppe „Oszillogramme“.\n\nWissenschaftlern an den Bell Laboratories gelang es in den 1950er-Jahren, sämtliche Instruktionen zur Bilderzeugung, vor dem Abfotografieren von einem Bildschirm, durch einen digitalen Computer zu erzeugen und zu kontrollieren. Die Geräteanordnung wurde als „automatischer Plotter“ bezeichnet. Die Ausgabe von Vektorgrafik mit Stiftplottern auf Papier wurde erst später möglich.\n\nEin Schritt zu komplexerer Computergrafik begann mit dem Kathodenstrahl-Bildschirm des am MIT entwickelten Whirlwind-Computers (siehe Geschichte der Computergrafik).\n\nZu den Pionieren grafischer Computerkunst im deutschsprachigen Raum zählen Personen, die entweder von der Informationsästhetik (Max Bense) beeinflusst waren oder in Verbindung zu Informatikern standen (z. B. Kurd Alsleben). Arbeiten konnten sie nur in Verbindung mit Rechenzentren, wie dem DESY in Hamburg, da Computer in den 1950er- und 1960er-Jahren noch raumfüllende Anlagen waren. In dieser Entstehungszeit knüpft sich an die Entwicklung der theoretischen Grundlagen und deren technologisch- künstlerischer Umsetzung die Erwartung, hiermit eine neue Ästhetik zu entwickeln („So ist die Ästhetik als \"objektive\" und \"materiale\" Ästhetik gedacht, die nicht mit spekulativen sondern mit rationalen Mitteln arbeitet.“ – Bense 1969).\n\nMit der Verbesserung der Ausgabetechniken über Plotter (Mikrofilmplotter und Stiftplotter) interessierten sich zunehmend mehr Künstler für Computer. Während erste Programme für Stiftplotter zur Anwendung durch Architekten und ähnliche Berufe konzipiert waren, erstellten Künstler wie Frieder Nake und Herbert W. Franke eigene Programme für Plotter. Mit Programmcode und Beschreibungssprachen für grafische Formen und Effekte stand \"erstmalig in der Geschichte der bildenden Kunst ein grafisches Beschreibungssystem zu Verfügung; es war den Noten der Musik vergleichbar, übertraf diese aber durch die Tatsache, dass aus dem Programmcode auch das generative Prinzip, die in den Bildern manifestierte Ordnung, zu ersehen ist.\"\n\nNeben Informatikern interessierten sich anfangs meist Künstler für die Arbeit mit Computern, die wie Vera Molnár aus der konkreten und abstrakten Kunst kamen. Einen anderen Hintergrund hat Manfred Mohr, der vom Action Painter und Jazzmusiker zum Pionier der Computerkunst wurde.\n\n1965, beinahe zeitgleich in den USA und in Deutschland, stellten nicht Künstler, sondern Wissenschaftler das erste Mal Computerkunst in Galerien aus: Bela Julesz und A. Michael Noll in New York, Georg Nees und Frieder Nake in Stuttgart.\n\nGeorg Nees, der die ersten Grafikbefehle in Algol 60 schrieb, produzierte 1968, mit Fräsmaschine und Siemens 4004 Rechner, eine der frühesten komplett computerkontrolliert erzeugten Skulpturen. Sie wurde 1970 auf der Biennale Venedig ausgestellt.\n\nIn den 1970er-Jahren wurden die Großrechner durch kleinere und interaktivere Rechenanlagen abgelöst, die über Tastatur und Monitor steuerbar, Rastergrafik ausgeben konnten. Laurence Gartel führte ab 1975 Methoden und Techniken aus der Videokunst ein: Videosynthesizer und erste Video-Malprogramme zur Manipulation von Grafik auf dem Monitor. Neue Möglichkeiten zur Bildmanipulation wurden von Künstlern aufgegriffen, die bislang mit analogen Medien gearbeitet hatten und gewöhnlich nicht als Vertreter der digitalen Kunst genannt werden: Peter Greenaway für Film, zusammen mit István Horkay; Jeff Wall, Thomas Ruff und Andreas Gursky für fotografische Kunst.\n\nEnde der 1970er-Jahre entstanden die ersten Ansätze zu digitalen Kommunikations-Netzwerken, und damit einer medialen digitalen Kunst, die Netzwerke als künstlerisch gestaltbare Medien begreift. Die Basis bilden zunächst Online Computer Konferenzen, dann erste Mailbox-Systeme wie ARTBOX und FAX-Netzwerke (vgl. ARTEX).\n\nMit zunehmender Rechengeschwindigkeit und Speicherkapazität wurde die interaktive Bearbeitung von Grafik am Monitor Standard. Grafik konnte als Proof auf Desktop-Druckern ausgegeben werden. Yoichiro Kawaguchi begann in den 1980er-Jahren damit, 3D Modeling Programme für digitale Kunst zu nutzen.\n\nMit Bildbearbeitungsprogrammen, wie Paintbox in den 1980er-Jahren und Adobe Photoshop seit den 1990er-Jahren, kann vorhandenes fotografisches Bildmaterial künstlerisch manipuliert werden.\n\nEine Liste von Essays sowie eine fundierte Liste der Künstler, die Entwicklungen der digitalen Kunst geprägt haben, bietet das Digital Art Museum (DAM) (siehe Weblinks).\n\nDie Digitalisierung verschiedenster, mittlerweile „traditioneller“ Medien, führte zu deren digitaler Neuformulierung. Beispielsweise wird Videokunst, ursprünglich analog erstellt, nun digital aufgenommen. Daraus hat sich eine neue Kultur und Kunst auf Webseiten präsentierter Videoclips entwickelt.\n\nHeute sind zweidimensionale und dreidimensionale Bilder aller Art mit entsprechenden Programmen manipulierbar und herstellbar. Sie können um eine zeitliche Dimension ergänzt werden. Digitale Bildkunst kann so in Videokunst übergehen, wie in den Arbeiten von Yves Netzhammer. Einerseits computergenerierte Kunst und klassische Kunstdisziplinen, andererseits die mediale digitale Kunst im engeren Sinne differenzieren sich aus, fließen jedoch auch ineinander.\n\nDie digitale Bildkunst von Julian Opie besteht zu einem großen Teil aus bewegten digitalen Großformaten in Museen und im öffentlichen Raum. Auf großen Flachbildschirmen an geeigneten Gebäuden werden digital gezeichnete Menschen oder digital erzeugte Landschaften in ständig fließender Bewegung zeigt.\n\nNeben der Geschichte und Gegenwart der weitgehend als „computergeneriert“ verstandenen digitalen Kunst gibt es eine Geschichte und Gegenwart der „medialen“ digitalen Kunst, die mit Medienkunst und Netzkunst verbunden ist.\n\n\n\n"}
{"id": "30090", "url": "https://de.wikipedia.org/wiki?curid=30090", "title": "ZoneAlarm", "text": "ZoneAlarm\n\nZoneAlarm ist der Name einer proprietären Personal Firewall, die von der Firma Check Point (früher Zone Labs) entwickelt wird.\n\nZoneAlarm erschien 2000 und entwickelte sich zu einer der beliebtesten Personal Firewalls und wurde anfangs zu den besten ihrer Art gezählt.\n\nSeit Version 6.5 werden die Betriebssysteme Windows 98 SE und ME nicht mehr unterstützt. Seit Version 8.0 wird Windows 2000 nicht mehr unterstützt. Eine auf 32-Bit-Versionen von Windows Vista lauffähige ZoneAlarm-Version ist seit dem 14. Juni 2007 auf Englisch und seit dem 22. Juni 2007 auch auf Deutsch erhältlich. Die Unterstützung von 64-Bit-Versionen der Windows-Betriebssysteme ist – wie den Systemvoraussetzungen der kostenfreien Version zu entnehmen ist – nur für Windows Vista und Windows 7 möglich.\n\nDer Schwerpunkt der Personal Firewall \"ZoneAlarm\" liegt auf einfacher Installation und Konfiguration. Das Produkt gibt es als kostenfreie Version für Privatanwender und als kommerzielle Version unter anderem in Form von \"ZoneAlarm Pro\", die einen größeren Funktionsumfang bietet.\n\nDer Name \"ZoneAlarm\" kommt daher, dass die Personal Firewall getrennte Sicherheitseinstellungen für zwei verschiedene Sicherheitszonen – eine für das lokale Netz und eine für das Internet – erlaubt.\n\nMinimale Systemanforderungen\n\nSowohl die kostenlose als auch die Pro-Edition der ZoneAlarm Firewall wurden 2017 als Editor's Choice des englischsprachigen PCMag ausgezeichnet.\n\n\n\n\n"}
{"id": "30183", "url": "https://de.wikipedia.org/wiki?curid=30183", "title": "Ars Electronica", "text": "Ars Electronica\n\nDie Ars Electronica () ist ein Festival zur Präsentation und Förderung von Kunst in enger Verbindung mit (digitaler) Technik und gesellschaftlichen Fragestellungen, das alljährlich in Linz (Oberösterreich) stattfindet.\n\nDie erste Ars Electronica wurde am 18. September 1979 im Rahmen des internationalen Brucknerfestes gemeinsam mit der ersten Linzer Klangwolke und der Musik von Bruckners achter Sinfonie eröffnet. Als Projekt-Vorläufer wird manchmal das forum metall genannt. Initiatoren des Festivals waren Hannes Leopoldseder vom ORF-Oberösterreich und Horst Stadlmayr, künstlerischer Leiter des Brucknerhauses in Linz. Die Konzeption der Ars Electronica als Forum zum Austausch von Ideen und Ort der Präsentation von Projekten an der Schnittstelle von Kunst, Technologie und Gesellschaft ging auf Hannes Leopoldseder, Intendant des ORF-Landesstudios Oberösterreich, den Elektronikmusiker Hubert Bognermayr, den Musikproduzenten Ulrich Rützel und den Kybernetiker und Physiker Herbert W. Franke zurück. Weltweit war diese Veranstaltungsreihe eine der ersten, die sich mit digitaler Kunst und ihren Möglichkeiten sowie den gesellschaftlichen Auswirkungen der Digitaltechnik auseinandersetzte.\n\nSowohl in der realen wie auch in der virtuellen Welt umfasst die Ars Electronica folgende Teile:\n\nDas Ars Electronica Festival als Grundstein der Ars Electronica wird seit 1979 jährlich in Linz durchgeführt, begleitet von einem großen Medienecho. Es ist das international bedeutendste Festival der digitalen Kunst, das Trends und langfristige Entwicklungen zukunftsorientiert in Form künstlerischer Werke, Diskussionsforen und wissenschaftlicher Begleitung vorstellt. Bis 1995 wurde es geleitet durch Peter Weibel, seit 1996 durch Gerfried Stocker.\n\nAusstellungen und Performances finden im Ars Electronica Center, im Brucknerhaus, im Lentos, in den Räumen der Universität für künstlerische und industrielle Gestaltung und an verschiedenen weiteren Orten in Linz statt. Nur 2010 wurden alle Ausstellungen, Symposien und auch die Verleihung des Prix Ars Electronica an einem Ort, den Gebäuden der ehemaligen Linzer Tabakfabrik, zusammengeführt.\n\nSchwerpunktmäßig gibt es verschiedene Kooperationen mit Forschungseinrichtungen wie der Universität Tokio (2008), dem MIT Media Lab (2009) oder dem Japan Media Arts Festival (2009), die ihre Tätigkeit im Rahmen des Festivals präsentieren. Von der Ars Electronica geförderte Medienkunstprojekte finden regelmäßig in Zusammenarbeit mit Unternehmen und Forschungsinstitutionen wie dem CERN statt.\n\nWährend des Festivals werden neben den Ausstellungen auch Symposien zu techno-kulturellen Phänomenen und den philosophisch-theoretischen Aspekten der technologischen Entwicklung unter Beteiligung internationaler Experten durchgeführt.\n\nDas Festival steht jedes Jahr unter einem anderen Motto:\n\n\n"}
{"id": "30827", "url": "https://de.wikipedia.org/wiki?curid=30827", "title": "Vim", "text": "Vim\n\nVim (Vi IMproved) ist eine Weiterentwicklung des Texteditors vi. Das freie Open-Source-Programm wurde 1991 von Bram Moolenaar veröffentlicht. Seitdem wird der Editor aktiv weiterentwickelt. Wie vi zeichnet sich Vim durch seine verschiedenen Betriebs-Modi aus, während viele andere gebräuchliche Editoren nur einen kombinierten Modus für Eingabe und Befehle kennen, in dem Befehle über Tastenkombinationen und grafische Oberflächen ausgeführt werden.\n\nVim funktioniert wie der vi-Editor im Textmodus auf jedem Terminal. Die Bedienung erfolgt dann üblicherweise über die Tastatur, eine Maus wird zwar auf vielen Terminals unterstützt, ihre Verwendung ist aber limitiert. Das ist zum einen historisch bedingt durch die zu Ur-vi-Zeiten Mitte der 1970er Jahre üblichen Bildschirmterminals mit einer langsamen, seriellen Verbindung zum Hauptrechner, zum anderen schätzen erfahrene Benutzer die Effizienzvorteile der mächtigen Tastatursteuerung („vi is at your fingertips“). Aus der Limitierung damaliger Terminals resultiert sein modales Konzept. Vim ist nahezu vollständig abwärtskompatibel zu vi, hat jedoch eine Vielzahl an Weiterentwicklungen und eine moderne grafische Benutzerschnittstelle mit Menüs (GVim) sowie eine vereinfachte Version für Einsteiger (eVim).\n\nVim kann auf vielen Betriebssystemen genutzt werden und ist auf fast jedem GNU/Linux-Rechner zu finden. In der Regel wird beim Aufruf \"vi\" unter Linux Vim über eine Verknüpfung oder einen Alias aufgerufen. Welcher vi-Ableger gestartet wurde, lässt sich im Kommando-Modus über den Befehl \":version\" überprüfen.\n\nAnfang der 1970er Jahre war \"ed\" von Ken Thompson der Unix-Standardeditor.\nEr arbeitete zeilenorientiert, d. h. die Anzeige des Textes erfolgte nicht wie heute gewohnt mehrzeilig, sondern sie musste über explizite Ausgabebefehle angewiesen werden (da die Ausgabe häufig nicht auf einem Bildschirm, sondern einem Fernschreiber erfolgte). Um mit den begrenzten Möglichkeiten einer Zeile arbeiten zu können, war es notwendig, einen Editier- und einen Kommandomodus zu verwenden (\"modaler\" Editor).\n\nDie ed-Kommandos bestanden normalerweise aus einem Buchstaben, dem ein Zeilenbereich vorangestellt werden konnte. Als Bill Joy ab 1976 \"vi\" entwickelte, stellte dieser – als \"visueller\" Editor – einen wesentlichen Fortschritt gegenüber ed dar, da er Änderungen am editierten Text im Kontext umgebender Zeilen darstellte. Dabei erbte vi das modale Konzept, die Zeilenorientierung und die Kommandos. Diese ed-Kommandos finden sich in allen aktuellen vi-kompatiblen Editoren, wie auch in Vim, wieder.\n\nBram Moolenaar wollte Ende der 1980er Jahre gerne auf einem Amiga-Computer den Editor benutzen, welchen er von Unix her kannte und gewohnt war. Allerdings gab es damals keinen vi für den Amiga. So entwickelte er auf Basis des vi-Klones \"Stevie\" 1988 die Version 1.0 von Vim. Dieser hieß zu der Zeit noch \"vi IMitation\", da das Hauptziel zunächst darin bestand, die Funktionalität von vi nachzubilden. Am 2. November 1991 wurde Vim mit Version 1.14 erstmals auf der sogenannten \"Fred Fish disk #591\", einer Sammlung freier Software für den Amiga, veröffentlicht. Die Version 1.22 wurde 1992 nach Unix und MS-DOS portiert. Zu dieser Zeit wurde die Langform der Abkürzung in \"Vi IMproved\" geändert. Die Großbuchstaben sollen dabei das Kürzel Vim erklären: VIM ist \"V\"i \"IM\"proved (improved, engl. für verbessert).\n\nIn den folgenden Jahren erfuhr Vim umfassende Verbesserungen. Ein Meilenstein war die Einführung der mehrfachen Editier-Fenster in der Version 3.0 (1994) (Bild 1). Mit vi konnte man zwar auch mehrere Dateien mit einem Aufruf editieren, aber man konnte immer nur eine Datei davon sehen und nicht mehrere gleichzeitig. Mit der Version 4.0, die 1996 erschien, war erstmals auch eine grafische Benutzeroberfläche verfügbar, an der Robert Webb großen Anteil hatte. Seit 1998 beherrscht Vim (in der Version 5.0) Syntax-Einfärbung (Bild 2a).\n\nAls bisher letzter großer Schritt wurden 2001 mit der Version 6.0 Code-Faltung, Plug-ins, Unterstützung für Mehrsprachigkeit und vertikal aufgeteilte Fenster eingeführt (Bilder 2 bis 5). Die Version 6.4 (erschienen im Oktober 2005) behebt viele Fehler, fügt aber keine neuen Funktionen hinzu. Dies war Version 7.0 im Mai 2006 vorbehalten; sie verfügt über eine integrierte Rechtschreibprüfung und unterstützt Reiter (Tabs). Die Version 7.3 erschien im August 2010. Sie enthielt als größere Neuerungen (neben allen Patches, die sich ab Version 7.2 angesammelt haben) komplette Undo-Verzweigungen (mehrstufiges Undo beherrscht vim seit langem), Blowfish-Verschlüsselung, Lua- und Python-3-Skriptfähigkeit sowie die Möglichkeit, Texte zu verbergen. Im August 2013 wurde die Version 7.4 veröffentlicht. Neu in dieser Version ist ein verbesserter RegEx-Parser. Die bislang letzte Version, Vim 8, erschien am 12. September 2016. Sie enthält unter anderem neue Features wie die Unterstützung von JSON.\n\nWährend zwischen kleinen Versionssprüngen (zum Beispiel von Version 7.1 auf Version 7.2) oft mehrere Monate vergehen, wird Vim dennoch kontinuierlich entwickelt. Die Vim-Benutzergemeinschaft steuert Fehlerbehebungen oder kleine Verbesserungen in Form von Patches bei, die dann mehrmals im Monat in den Vim-Code einfließen.\n\n\nDie Leistungsfähigkeit von Vim soll an einigen markanten vi-Verbesserungen dargestellt werden.\n\nVim besitzt eine umfassende Dokumentation. Der Nutzer wird bei der Suche nach Lösungen zu seinem Problem durch verschiedene Funktionen unterstützt. Durch Syntaxhervorhebung und eine eigene Hilfe-Syntax werden Schlüsselbegriffe farbig hervorgehoben (Bild 1). Im Bild sind diese Begriffe grün dargestellt und werden in der Kommandozeilenversion in zwei senkrechte Striche eingeschlossen. Über Tastenkürzel kann zwischen diesen Schlüsselwörtern vorwärts und rückwärts navigiert werden, ähnlich wie in einem Webbrowser. Das funktioniert in der grafischen Oberfläche bzw. bei entsprechender Unterstützung im Terminal auch mit der Maus per Doppelklick oder rechter Maustaste. Es gibt noch weitere Funktionen, die dem Nutzer das Suchen erleichtern.\nEine wichtige davon ist das \":helpgrep\"-Kommando. Dabei kann der Nutzer nach einem Begriff in der gesamten Hilfe suchen, die Treffer in einem weiteren Fenster darstellen lassen und von diesem dann zu den entsprechenden Stellen in der Hilfe wechseln (Bild 3). Die Trefferliste (Bild 3 unten) kann mit der Suchfunktion von Vim nach weiteren Begriffen durchsucht werden. Ergänzt wird die Hilfe durch eine HTML-Version dieser Hilfe im Internet, eine umfangreiche Sammlung der häufig gestellten Fragen (FAQ), Literatur auf Englisch und Deutsch und vieles mehr.\n\nVim ist ein Editor, der ebenso wie vi für Programmierer geschrieben wurde. Daher gibt es die Möglichkeit, über Plugins \"edit-compile-fix\" (deutsch „Editieren-Übersetzen-Fehlerkorrektur“)-Funktionalitäten nachzurüsten. Ähnlich wie bei einer integrierten Entwicklungsumgebung wird der Quelltext editiert und dann mittels eines Compilers direkt aus Vim heraus übersetzt. Falls beim Kompilieren Fehler aufgetreten sind, werden diese in einem weiteren Fenster angezeigt. Von der Fehlermeldung kann direkt ins andere Fenster zur fehlerhaften Stelle im Quelltext gesprungen und dieser korrigiert werden. Danach kann ein weiterer Zyklus gestartet und es können gegebenenfalls weitere Fehler korrigiert werden. Der Programmierer wird dabei durch die Funktionen Syntaxhervorhebung und Textfaltung unterstützt.\n\nEine weitere häufige Aufgabenstellung besteht im Vergleichen von zwei Versionen einer Datei. Vim bietet hier eine Möglichkeit, die Unterschiede zweier Dateiversionen nebeneinander in zwei Fenstern darzustellen und die Differenzen farbig zu markieren (Bild 4), die typische Aufgabe eines Merge-Programms. Dabei werden geänderte und eingefügte Zeilen farbig hervorgehoben und Bereiche, die in beiden Versionen gleich sind, durch Text-Faltung ausgeblendet. Im Beispiel sind geänderte Stellen rot gekennzeichnet und eingefügte Zeilen blau bzw. fehlende Zeilen hellblau markiert. \"Gefaltete\" Zeilen sind hier grau unterlegt mit Angabe der Zeilenanzahl, die unverändert sind.\n\nVim ist über die Skriptsprache \"Vimscript\" erweiter- und programmierbar. Damit lassen sich komplexe Vorgänge automatisieren, die für ein Makro zu kompliziert wären. Ein Beispiel für ein Vim-Skript ist die Vim-Start-Datei codice_1 unter Unix und GNU/Linux oder codice_2 unter Windows, DOS und OS/2, in der hauptsächlich Konfigurations-Einstellungen vorgenommen werden. Diese wird beim Start von Vim automatisch ausgeführt. In der Vim-Skriptsprache sind alle Kommandozeilen-Befehle verfügbar und über das Kommando ':normal' auch alle Normalmodus-Befehle. Es gibt mehrere Datentypen: Fließkommazahlen, ganze Zahlen, Zeichenketten, Listen und assoziative Arrays. Boolesche Werte werden über ganze Zahlen realisiert, wobei eine Null als \"falsch\" gewertet wird und alle anderen Zahlen als \"wahr\". Es sind die wichtigsten Operatoren für Vergleiche, logische Verknüpfungen und Grundrechenarten vorhanden. Als Kontrollstrukturen stehen die Entscheidung 'if – then – elseif – else – endif' und die while-Schleife zur Verfügung. Der Nutzer kann eigene Funktionen definieren und über einhundert vordefinierte Funktionen verwenden, die im Wesentlichen den Unix-Systemaufrufen entsprechen. Die Skripte können mit einem Debug-Modus getestet werden. Wer plant, ein Vim-Skript zu schreiben, sollte vorher auf der Vim-Skript-Seite nachsehen, ob es nicht schon ein Skript mit der entsprechenden Funktionalität gibt. Obwohl es dort viele Skripte zur Unterstützung von Programmieraufgaben gibt, lohnt sich auch für Nicht-Programmierer ein Blick auf diese Seite. Nicht zuletzt kann jeder Nutzer, der ein Problem von allgemeinem Interesse gut gelöst hat, sein Skript anderen zur Verfügung stellen. Als Beispiel für ein Vim-Skript soll das Kalender-Skript dienen (Bild 5).\n\nIm Laufe der Jahre wurden für Vim einige Modifikationen entwickelt, welche die Benutzung dieses Editors für ungeübte Benutzer einfacher machen sollen. Die bekanntesten sind der in Vim integrierte \"easy-Vim\"-Modus und das GUI \"GVim\".\n\nMit \"GVim\" steht Vim unter den meisten aktuellen Betriebssystemen eine grafische Oberfläche zur Verfügung, die viele Befehle des Programms ähnlich den heute gängigen grafischen Texteditoren auch über Menüeinträge und eine Werkzeugleiste zugänglich macht. Über seine Konfigurationsdateien lässt sich GVim, ähnlich Vim, umfassend konfigurieren, auch das Einbinden von Vim-Skripten ist hier möglich. Im Gegensatz zum Vim lassen sich jedoch mehr Farben und Schriftarten für die Darstellung von Textdateien einstellen sowie fertige Farbschemata erstellen und nutzen.\n\n\"eVim\" (kurz für \"easy Vim\") ist ein einfacher Modus, in dem Vim betrieben werden kann. Wird Vim in diesem Modus gestartet, so ist sofort der Einfüge-Modus aktiv; Befehle können nur über einen speziellen Befehlsmodus eingegeben werden. Auf diese Weise kann Vim fast wie ein nicht modaler Editor verwendet werden.\n\nMit der GVim-Erweiterung \"Cream\" ist es schließlich möglich, Vim vollständig auf eine Weise zu nutzen, die bisherigen Nutzern einfacherer Editoren (wie gedit oder dem Microsoft Notepad) vertraut ist. Es handelt sich um eine Reihe von Skripten zur weiteren Vereinfachung von GVim, so können praktisch alle wichtigen Kommandos mit der Maus über Menüs erreicht werden und die Betriebs-Modi entfallen in der Defaultausführung gänzlich (können aber nach Belieben hinzugeschaltet werden). \"Cream\" ist weder in Vim noch in \"GVim\" integriert, sondern setzt auf beiden auf, ohne sie dabei zu ersetzen, d. h. auch die gleichzeitige Verwendung von \"Cream\" und \"GVim\" ist problemlos möglich.\n\nVim ist ein \"modaler Editor\". Dies bedeutet, dass man in unterschiedlichen Modi arbeitet, was Einsteigern oft Probleme bereitet. Die sechs Grundmodi sollen hier nur kurz erklärt werden. Zu den ersten drei Modi siehe auch vi: Arbeitsmodi. Es gibt noch fünf weitere Modi, auf die hier aber der Einfachheit halber nicht eingegangen werden soll. Diese sind Varianten der Grundmodi.\n\nVim startet im Normalmodus, oft auch als Kommando- oder Befehlsmodus bezeichnet. Hier kann man über Tastenkürzel zum Beispiel Zeilen kopieren und verschieben oder Text formatieren. Dies ist der \"zentrale Modus\", von dem in alle anderen Modi gewechselt wird. Durch das zweifache Drücken der -Taste gelangt man immer in den Normalmodus zurück. Falls man sich nicht sicher ist, ob man sich bereits im Normalmodus befindet, kann man jederzeit die -Taste drücken. War man bereits im Normalmodus, wird dies im Normalfall, je nach Vim-Konfiguration und Terminal-Einstellungen, mit einem Piepton signalisiert.\n\nIm Einfügemodus verändern Tastatureingaben den editierten Text, so wie man es von anderen Editoren kennt.\nMan erreicht diesen Modus vom Normalmodus aus zum Beispiel über i (für engl. insert = einfügen). Es gibt eine Vielzahl anderer Befehle, um in diesen Modus zu gelangen.\n\nIm Einfügemodus werden (fast) alle Tastatureingaben in den editierten Text übernommen, und nur ganz wenige Tasten und -kombinationen haben eine andere Wirkung. Die Wichtigste ist die -Taste zur Beendigung des Einfügemodus. Anschließend sind die Normalmodus-Befehle wieder verfügbar.\n\nEine weniger verbreitete, alternative Sichtweise verzichtet darauf, das Einfügen von Text als eigenständigen Modus zu betrachten. Gemäß dieser Sichtweise existieren lediglich eine Reihe von \"Textänderungs-Befehlen\", denen unmittelbar eine Eingabe folgt, welche mit abzuschließen ist. Ein Vorteil dieser Sichtweise ist, dass niemals Zweifel aufkommen, ob momentan der \"Normalmodus\" gewählt ist, und auch das Potenzial der Befehlswiederholung erschließt sich in dieser Sichtweise eher.\n\nDieser Modus wird durch den einleitenden Doppelpunkt : erreicht. Hinter dem Doppelpunkt können dann komplexe Kommandos wie beispielsweise zum Suchen und Ersetzen eingegeben werden. Nach dem abschließenden \"Enter\" wird der Befehl ausgeführt und man befindet sich wieder im Normalmodus. Zu diesem Modus zählen des Weiteren die Kommandos / (Textmuster vorwärts suchen), ? (Textmuster rückwärts suchen) und das Filter-Kommando !, mit dem die UNIX-Kommandos (zum Beispiel sort) aufgerufen und auf den Text angewandt werden können.\n\nDieser Modus ist eine Verbesserung von Vim und ähnelt dem Normalmodus. Mit der Maus oder bestimmten Tastenkürzeln wird ein Bereich zeilenweise, zeichenweise oder blockweise markiert und visuell hervorgehoben. Auf diesen Bereich können dann die Kommandos des Normal- sowie des Kommandozeilen-Modus angewandt werden. Die Arbeit mit diesem Modus ist einfacher als mit dem Normalmodus, da man genau sieht, welchen Bereich man gerade bearbeitet.\n\nDieser Modus wird aus dem visuellen Modus heraus gestartet, in dem man einen Bereich markiert. Anschließend wechselt man über \"-\" in den Selektions-Modus. Gibt man nun ein druckbares Zeichen ein, so wird der selektierte Bereich gelöscht und gleichzeitig in den Einfüge-Modus gewechselt. Das heißt, der selektierte Bereich wird mit dem eingegebenen Text überschrieben. Einfacher ist der Auswahlmodus über die Tastenkombination Umschalttaste-Pfeiltasten erreichbar. Allerdings muss Vim dazu entsprechend konfiguriert werden. Beendet wird dieser Modus wie üblich mit der -Taste. Auch dieser Modus ist eine Vim-Erweiterung.\n\nDieser Modus ähnelt dem Kommandozeilenmodus, mit dem Unterschied, dass nach Ausführung eines Kommandos nicht in den Normalmodus zurück gewechselt wird. Dieser Modus stammt aus dem UNIX-Zeileneditor \"ex\" (wie auch der traditionelle vi-Editor).\n\nVim ist unter vielen Betriebssystemen lauffähig. Vor allem für unixoide Systeme existieren Portierungen, beispielsweise für die UNIX- bzw. BSD-Derivate AIX IRIX, HP-UX, macOS und iOS und freie BSD-Distributionen wie FreeBSD sowie für Linux-basierende Systeme einschließlich Android. Windows wurde seit 3.1 unterstützt, Vim 8 benötigt Windows XP und aufwärts. Auch für VMS, RISC OS, AmigaOS, Atari MiNT, Mac OS Classic, Haiku und BeOS, NeXTStep, OS/2 sind (oder waren) Portierungen verfügbar.\n\nvi und Vim unterscheiden sich hinsichtlich ihrer Bedienung von anderen heute üblichen Editoren. Mitunter wird die Trennung der Modi sowie die Notwendigkeit des Erlernens zahlreicher Tastenkürzel kritisiert, unter der die Zugänglichkeit des Programms leide. Mithilfe von \"GVim\", \"eVim\" oder \"Cream\" können diese Schwierigkeiten zu Beginn des Lernprozesses gemindert werden.\n\nViele Nutzer schätzen diese Editoren aufgrund der hohen Produktivität, die sich im Vergleich zu einfachen Editoren nach Bewältigung der Lernkurve einstellt. Die meisten Tastaturbefehle kann man sich einfach einprägen, da es sich meist um den Anfangsbuchstaben des entsprechenden Befehles in englischer Sprache handelt. Diese Eigenart geht auf das Konzept des \"ed\" zurück. Der höhere Aufwand zum Erlernen dieser Befehle wird oft durch ein effizienteres Arbeiten bei häufigem, intensivem Gebrauch gerechtfertigt. Geübte Anwender können mit wenigen Tastatureingaben Text kopieren, formatieren oder sortieren, wobei Varianten möglich sind, die bei gängigen, einfacheren Editoren meist mit größerem Aufwand verbunden sind. Mit einem hinreichenden Überblick sind, durch die Kombination mehrerer Befehle, vor allem komplexere und wiederkehrende Textbearbeitungen mit Vim einfach zu erledigen. Darüber hinaus erlauben die grafischen Versionen zusätzlich eine eingeschränkte Bedienung über mit der Maus erreichbare Menüs, was eine von gewöhnlichen Editoren her bekannte Arbeitsweise ermöglicht.\n\nVim war Gewinner des \"Readers’ Choice Awards\" in der Kategorie \"Favorite Text Editor\" in den Jahren 2001 bis 2005 und erhielt 2000 den \"Slashdot Beanie Award\" als \"Best Open Source Text Editor\" sowie 1999 den \"Linuxworld Editors' Choice Award\". Im Mai 2008 erhielt Bram Moolenaar für seine Arbeit an Vim den \"NLUUG Award\", eine Auszeichnung der niederländischen Unix User Group.\n\nVim steht unter einer GPL-kompatiblen Charityware-Lizenz (charity, engl.: Nächstenliebe). Das bedeutet, dass Vim frei verteilt werden darf, jedoch erbittet der Entwickler bei Gefallen eine Spende für Waisenkinder in Uganda durch das ICCF Holland.\n\nVim wird hauptsächlich von Bram Moolenaar und einer Community von vielen Freiwilligen entwickelt. Die Hilfeseite der aktuellen Version nennt über 60 Mitwirkende namentlich. Es gibt eine große Anzahl ungenannter Helfer, die sich nicht nur um die Weiterentwicklung des Vim kümmern, sondern auch um die Portierung auf andere Betriebssysteme, um Programmtests und das Sammeln von Bugs, die Vervollständigung der Dokumentation und die Übersetzung der Hilfeseiten. Außerdem übernehmen sie die Beantwortung von Nutzeranfragen und die Einbindung des Vim in andere Projekte.\n\nAndererseits entstand, wie auch im Fall von Emacs, über die Zeit eine ganze Reihe von Abspaltungen, häufig mit recht deutlichen Unterschieden in Motivation und Zielsetzung, Pflege und Weiterentwicklung wie auch dem Versuch, Kompatibilität zu Vim und seinen Erweiterungen zu wahren und sich gleichzeitig voneinander abzugrenzen. Größere Aufmerksamkeit in diesem Sinne erlangte Neovim, das sich in erster Linie als Weiterentwicklung und Refactoring-Projekt versteht und darum bemüht ist, den Editor durch Aussparen so verstandener Legacy-Teile, oder heute seltener benutzten Funktionen, zu verschlanken. Verglichen mit dem Original, dessen GUI u. a. auf GTK aufsetzt, bieter es auch eine Version, die auf dem von vielen als moderner empfundenen Qt basiert.\n\nWährend sich die meisten Befehlszeilenprogramme mit + oder + beenden lassen, muss der Vim-Nutzer im Normalmodus, den er ggf. erst durch Drücken von erreichen muss, einen Befehl eingeben. Dieser lautet codice_3 (Beenden und eventuelle Änderungen ohne Nachfrage verwerfen), wobei es zahlreiche andere gibt. Die Frage, wie man Vim beendet, ist eine der häufigsten aufgerufenen auf der Softwareentwickler-Website Stack Overflow. Mehrere Prozent ihres Traffics zu Vim und rund 0,005 % des Gesamttraffics entfallen auf diese Frage.\n\n\n"}
{"id": "31148", "url": "https://de.wikipedia.org/wiki?curid=31148", "title": "Hidden Markow Model", "text": "Hidden Markow Model\n\nDas Hidden Markow Model, kurz HMM ( \"verborgenes Markow-Modell\") ist ein stochastisches Modell, in dem ein System durch eine Markow-Kette – benannt nach dem russischen Mathematiker A. A. Markow – mit unbeobachteten Zuständen modelliert wird.\nEin HMM kann dadurch als einfachster Spezialfall eines dynamischen bayesschen Netzes angesehen werden.\n\nDie Modellierung als Markow-Kette bedeutet, dass das System auf zufällige Weise von einem Zustand in einen anderen übergeht, wobei die Übergangswahrscheinlichkeiten nur jeweils vom aktuellen Zustand abhängen, aber nicht von den davor eingenommenen Zuständen. Außerdem wird angenommen, dass die Übergangswahrscheinlichkeiten über die Zeit konstant sind. Bei einem HMM werden jedoch nicht diese Zustände selbst von außen beobachtet; sie sind \"verborgen\" (englisch \"hidden\"). Stattdessen sind jedem dieser inneren Zustände beobachtbare Ausgabesymbole, sogenannte \"Emissionen\", zugeordnet, die je nach Zustand mit gewissen Wahrscheinlichkeiten auftreten. Die Aufgabe besteht meist darin, aus der beobachteten Sequenz der Emissionen zu wahrscheinlichkeitstheoretischen Aussagen über die verborgenen Zustände zu kommen.\n\nWichtige Anwendungsgebiete sind Spracherkennung, Computerlinguistik und die Bioinformatik, aber unter anderem auch Spamfilter, Gestenerkennung in der Mensch-Maschine-Kommunikation (Robotik), Schrifterkennung und Psychologie.\n\nGegeben seien zwei zeitdiskrete Zufallsprozesse formula_1 und formula_2, von denen nur der letzte beobachtbar sei.\nDurch ihn sollen Rückschlüsse auf den Verlauf des ersten Prozesses gezogen werden; hierfür wird ein mathematisches Modell benötigt, das die beiden Prozesse miteinander in Beziehung setzt.\n\nDer hier beschriebene Ansatz zeichnet sich durch die folgenden beiden Annahmen aus:\nDer aktuelle Wert des ersten Prozesses hängt ausschließlich von seinem letzten Wert ab:\n\nDer aktuelle Wert des zweiten Prozesses hängt ausschließlich vom aktuellen Wert des ersten ab:\n\nHaben die beiden Prozesse nun noch jeweils einen \"endlichen\" Wertevorrat, so lässt sich das so gewonnene Modell als probabilistischer Automat auffassen, genauer als \"Markow-Kette\".\nMan sagt auch formula_5 ist ein \"Markow-Prozess\".\nAngelehnt an den Sprachgebrauch in der theoretischen Informatik – insbesondere der Automatentheorie und der Theorie formaler Sprachen – heißen die Werte des ersten Prozesses \"Zustände\" und die des zweiten \"Emissionen\" bzw. \"Ausgaben\".\n\nEin \"Hidden Markov Model\" ist ein 5-Tupel formula_6 mit:\n\nEin HMM heiße \"stationär\" (oder auch \"zeitinvariant\"), wenn sich die Übergangs- und Emissionswahrscheinlichkeiten nicht mit der Zeit ändern. Diese Annahme ist oft sinnvoll, weil auch die modellierten Naturgesetze konstant sind.\n\nDas Bild zeigt die generelle Architektur eines instanziierten HMMs. Jedes Oval ist die Repräsentation einer zufälligen Variable formula_22 oder formula_23, welche beliebige Werte aus formula_24 bzw. formula_25 annehmen kann. Die erste Zufallsvariable ist dabei der versteckte Zustand des HMMs zum Zeitpunkt formula_26, die zweite ist die Beobachtung zu diesem Zeitpunkt. Die Pfeile in dem Trellis-Diagramm bedeuten eine bedingte Abhängigkeit.\n\nIm Diagramm sieht man, dass der Zustand der versteckten Variable formula_22 nur vom Zustand der Variable formula_28 abhängt, frühere Werte haben keinen weiteren Einfluss. Deshalb ist das Modell ein Markov-Modell 1. Ordnung. Sollten höhere Ordnungen benötigt werden, so können diese durch das Einfügen neuer versteckter Zustände stets auf die 1. Ordnung zurückgeführt werden. Der Wert von formula_23 hängt weiter ausschließlich von formula_22 ab.\n\nEin Gefangener im Kerkerverlies möchte das aktuelle Wetter herausfinden. Er weiß, dass auf einen sonnigen Tag zu 70 % ein Regentag folgt und dass auf einen Regentag zu 50 % ein Sonnentag folgt.\nWeiß er zusätzlich, dass die Schuhe der Wärter bei Regen zu 90 % dreckig, bei sonnigem Wetter aber nur zu 60 % dreckig sind, so kann er durch Beobachtung der Wärterschuhe Rückschlüsse über das Wetter ziehen (das heißt, er kann die Wahrscheinlichkeit für Regenwetter gegenüber sonnigem Wetter abschätzen). Hier bildet das tatsächlich vorhandene, aber nicht sichtbare Wetter den zu ermittelnden versteckten Zustand, die Prozentwerte 70 % und 50 % sind (über längere Zeiten hinweg ermittelte) Trainingsdaten des Modells, und die tatsächlich beobachtbaren Zustände liegen im jeweiligen Aussehen der Schuhe.\n\nZur Untersuchung von DNA-Sequenzen mit bioinformatischen Methoden kann das HMM verwendet werden. Beispielsweise lassen sich so CpG-Inseln in einer DNA-Sequenz aufspüren. Dies sind Bereiche eines DNS-Einzelstrangs mit einem erhöhten Anteil von aufeinanderfolgenden Cytosin- und Guanin-Nukleinbasen. Dabei stellt die DNS-Sequenz die Beobachtung dar, deren Zeichen formula_31 bilden das Ausgabealphabet. Im einfachsten Fall besitzt das HMM zwei verborgene Zustände, nämlich \"CpG-Insel\" und \"nicht-CpG-Insel\". Diese beiden Zustände unterscheiden sich in ihrer Ausgabeverteilung, so dass zum Zustand \"CpG-Insel\" mit größerer Wahrscheinlichkeit Zeichen formula_32 und formula_33 ausgegeben werden.\n\nIn der automatischen Spracherkennung mit HMM werden die \"gesprochenen\" Laute als versteckte Zustände aufgefasst und die tatsächlich \"hörbaren\" Töne als Emission.\n\nIm Zusammenhang mit HMMs existieren mehrere grundlegende Problemstellungen.\n\nGegeben sind die beobachtbaren Emissionen formula_25.\nEs ist zu klären, welche Modelleigenschaften – insbesondere welche orthogonale Dimensionalität – den Schluss auf die nicht direkt beobachtbaren Zustände erlauben und gleichzeitig eine sinnvolle Berechenbarkeit zulassen.\nInsbesondere ist zu entscheiden, welche Laufzeit für die Modellrechnungen erforderlich werden darf, um die Verwendbarkeit der Schätzungen zu erhalten.\n\nDie Berechnung der Schätzwerte der nicht beobachtbaren Zustände aus den beobachtbaren Ausgabesequenzen muss die erreichbaren numerischen Genauigkeiten beachten. Weiter müssen Kriterien zur Klassifizierung der statistischen Signifikanz implementiert werden.\nBei Verwendung eines HMM für einen bestimmten Merkmalsvektor bestimmt die Signifikanz die Wahrscheinlichkeit einer zutreffenden oder falschen Modellhypothese sowie deren Informationsgehalt (Entropie, Bedingte Entropie) bzw. deren Informationsqualität.\n\nGegeben sei ein HMM formula_35 sowie eine Beobachtungssequenz formula_36 der Länge formula_37.\nGesucht ist die Wahrscheinlichkeit formula_38, dass der momentane verborgene Zustand zum letzten Zeitpunkt formula_37 gerade formula_13 ist.\nEin effizientes Verfahren zur Lösung des Filterungsproblems ist der Forward-Algorithmus.\n\nGegeben sei wieder ein HMM formula_35 und die Beobachtungssequenz formula_42 sowie ein formula_43.\nGesucht ist Wahrscheinlichkeit formula_44, also die Wahrscheinlichkeit, dass sich das HMM zum Zeitpunkt formula_45 im Zustand formula_13 befindet, falls die betreffende Ausgabe beobachtet wurde.\nPrädiktion ist dabei gewissermaßen wiederholtes Filtern ohne neue Beobachtungen und lässt sich auch einfach mit dem Forward-Algorithmus berechnen.\n\nErneut seien formula_35, formula_42 und ein formula_49 gegeben.\nGesucht ist die Wahrscheinlichkeit formula_50, also die Wahrscheinlichkeit, dass sich das Modell zu einem früheren Zeitpunkt in einem bestimmten Zustand befand, unter der Bedingung, dass formula_42 beobachtet wurde.\nMit Hilfe des Forward-Backward-Algorithmus kann diese Wahrscheinlichkeit effizient berechnet werden.\n\nSeien wieder formula_35 sowie formula_42 gegeben.\nEs soll die wahrscheinlichste Zustandsfolge aus formula_54 bestimmt werden, die eine vorgegebene Ausgabesequenz erzeugt haben könnte.\nDieses Problem lässt sich effizient mit dem Viterbi-Algorithmus lösen.\n\nGegeben sei nur die Ausgabesequenz formula_42.\nEs sollen die Parameter eines HMM bestimmt werden, die am wahrscheinlichsten die Ausgabesequenz erzeugen.\nDies ist lösbar mit Hilfe des Baum-Welch-Algorithmus.\n\nGegeben seien nur die möglichen Ausgaben formula_25.\nEs sollen die Zustände im Modellsystem und die korrespondierenden Effekte im realen System identifiziert werden, die die Zustandsmenge formula_24 des Modells beschreibt.\nDazu muss vorweg die Bedeutsamkeit der einzelnen Emissionen bestimmt werden.\n\nAnwendung finden HMMs häufig in der Mustererkennung bei der Verarbeitung von sequentiellen Daten, beispielsweise bei physikalischen Messreihen, aufgenommenen Sprachsignalen oder Proteinsequenzen. Dazu werden die Modelle so konstruiert, dass die verborgenen Zustände semantischen Einheiten entsprechen (z. B. Phoneme in der Spracherkennung), die es in den sequentiellen Daten (z. B. Kurzzeit-Spektren des Sprachsignals) zu erkennen gilt. Eine weitere Anwendung besteht darin, für ein gegebenes HMM durch eine Suche in einer Stichprobe von sequentiellen Daten solche Sequenzen zu finden, die sehr wahrscheinlich von diesem HMM erzeugt sein könnten. Beispielsweise kann ein HMM, das mit Vertretern einer Proteinfamilie trainiert wurde, eingesetzt werden, um weitere Vertreter dieser Familie in großen Proteindatenbanken zu finden.\n\nHidden-Markov-Modelle wurden erstmals von Leonard E. Baum und anderen Autoren in der zweiten Hälfte der 1960er Jahre publiziert. Eine der ersten Applikationen war ab Mitte der 1970er die Spracherkennung. Seit Mitte der 1980er wurden HMMs für die Analyse von Nukleotid- und Proteinsequenzen eingesetzt und sind seitdem fester Bestandteil der Bioinformatik.\n\n\n"}
{"id": "31169", "url": "https://de.wikipedia.org/wiki?curid=31169", "title": "Multimedia", "text": "Multimedia\n\nDer Begriff Multimedia bezeichnet Inhalte und Werke, die aus mehreren, meist digitalen Medien bestehen: Text, Fotografie, Grafik, Animation, Audio und Video.\n\nEs gibt keine eindeutige Definition für den Begriff \"Multimedia\". Die Präsentation von Inhalten durch verschiedene Kanäle (Tonfilm bedient zum Beispiel Auge und Ohr) ist nichts Neues. Die Konvergenz (das Zusammenlaufen) von Medien beschäftigte Wissenschaftler und Publizisten seit dem Kino und in verschärftem Maßen seit den ersten erfolgreichen Fernsehübertragungstests. 1926 schrieb der Journalist Ludwig Kapeller:\n\nDer Begriff \"Multimedia\" kam mit der digitalen Vermittlung von Inhalten auf. Außerdem spielt das Vorhandensein unterschiedlicher Interaktionsmöglichkeiten eine wichtige Rolle, z. B. aktive Navigation, Manipulation von Inhalten oder Steuerung von Wiedergabeparametern. Aufgrund des technischen Fortschritts der Digitalisierung und der gesteigerten Leistungsfähigkeit von Computern erlebte Multimedia eine stürmische Entwicklung.\n\nBernd Weidenmann (2001) nennt neben der Interaktivität zwei weitere Eigenschaften, die Medien erfüllen müssen, damit man sie als multimedial bezeichnen kann. Zum einen müssen mehrere Kodierungsformen verwendet werden (Multikodalität). Texte verwenden beispielsweise eine symbolische Kodierungsform (verbal), unabhängig ob sie gedruckt sind oder gesprochen werden. Ein Bild benutzt hingegen eine abbildhafte bzw. imaginäre (realgetreu oder schematisch/typisierend) Kodierungsform. Zum anderen müssen verschiedene Sinnesmodalitäten eingesetzt werden (Multimodalität). Darunter versteht man die angesprochenen Sinne des Menschen. Die häufigsten Sinne sind der auditive und der visuelle Sinn. Teilweise ist es auch schon heute mithilfe der Force-Feedback-Technik möglich, den Tast- oder Geruchssinn anzusprechen.\nEin Text auf dem Computermonitor ist somit monokodal (symbolisch / verbal) und monomodal (visuell). Wird dieser jedoch durch Originalsounds (auditiv und abbildhaft / realgetreu) untermalt, sind die Eigenschaften Multimodalität (visuell und auditiv) und Multikodalität (abbildhaft / realgetreu und symbolisch / verbal) erfüllt. Ein Film ist ebenfalls multimodal (visuell und auditiv) und meist auch multikodal (abbildhaft und symbolisch).\n\nBei P. Kneisel (zitiert nach Steinmetz 1999) findet man folgende Definition: \"„Ein Multimediasystem ist durch die rechnergestützte, integrierte Erzeugung, Manipulation, Darstellung, Speicherung und Kommunikation von unabhängigen Informationen gekennzeichnet, die in mindestens einem kontinuierlichen und einem diskreten Medium kodiert sind.“\"\n\nEine weitere Definition wird von Klimsa gegeben (L. J. Issing, P. Klimsa: \"Information und Lernen mit Multimedia und Internet.\" S. 3f):\n\nUnter Multimedia-Kommunikation ist die Planung, Organisation, Durchführung und Kontrolle sämtlicher Maßnahmen zu fassen, die dazu dienen Botschaften, die durch Kombinationen von Text-, Grafik-, Ton-, Bild- und Bewegtbildelementen gestaltet sind, durch elektronische Medien abzusenden, um mit dem Konsumenten in Interaktion zu treten und die Kommunikationsziele des Unternehmens zu realisieren.\n\nDie Internetwerbung hat in den letzten Jahren im Zuge der Modernisierung der Haushalte immer mehr an Bedeutung gewonnen. Kommunikative Werbebotschaften werden über Banner und Bilder auf Internetseiten und auch über auftauchende Pop-ups übermittelt. Aber auch über die Ergebnisse von Suchmaschinenanfragen wird Internetwerbung praktiziert, indem die betroffenen Unternehmen und ihre Produkte bei einer Suchanfrage eher, als Treffer, angezeigt werden als andere.\n\n\"Vorteile:\"\n\n\"Nachteile:\"\n\n\"Vorteile:\"\n\n\"Nachteile:\"\n\nVerbreitet ist die Auffassung, dass die Nutzung verschiedener Medien dem Betrachter die Wissensaufnahme von Inhalten erleichtert, da der Benutzer die Informationen mit verschiedenen Sinnesorganen aufnimmt. Dem liegt die vordergründig einsichtige Annahme zugrunde, dass Informationen besser „gespeichert“ werden, wenn sie über möglichst viele „Kanäle“ vermittelt werden. In diesem Zusammenhang tauchen Grafiken auf, ähnlich den hier vorgestellten, die den kausalen Zusammenhang zwischen Lerneffekt (grüne Kurve) und Medienmenge illustrieren sollen.\n\nDer Psychologe Bernd Weidenmann bezeichnet diese Argumentation als naive Summentheorie, der jeder empirische Beweis fehle. Vermutlich gründet die Annahme in einer Fehlinterpretation einer Aussage des Begründers der Mediendidaktik, Comenius. Dieser forderte in seiner Schrift „E Scholasticis Labyrinthis Exitus in planum“ unter anderem: \"„Also sollen auch die Schulen alles den eigenen Sinnen der Lernenden darbieten: damit sie alles selbst sehen, hören, riechen, schmecken, berühren, was gesehen usw. werden kann und muss“\".\nEmpirisch belegt ist, dass es Vorteile beim Lernen gibt, wenn Informationen auf verschiedenen Kanälen präsentiert werden, z. B. ein Bild + Audio-Beitrag. Dies wird damit begründet, dass ein Kanal nur eine begrenzte Kapazität besitzt. Bild + geschriebener Text würde also den visuellen Kanal überfordern, während Bild + Audio gleichzeitig wahrgenommen werden kann.\n\nDer interaktive Aspekt von Multimedia ermöglicht dem Betrachter eine individuell zugeschnittene Wissensvermittlung sowie die erfahrungsorientierte Aufnahme von Inhalten. Diese Vorteile werden insbesondere durch Lernprogramme (E-Learning, Computer Based Training) erschlossen.\n\n\n\n\n"}
{"id": "31644", "url": "https://de.wikipedia.org/wiki?curid=31644", "title": "Xfce", "text": "Xfce\n\nXfce ist eine modular aufgebaute, freie Desktop-Umgebung für Unix und unixartige Betriebssysteme.\nSie ist als freie Software unter GPL, LGPL und 2-Klausel-BSD-Lizenz veröffentlicht. Nach einer Umfrage von LinuxQuestions.org ist Xfce hinter KDE die am zweitweitesten verbreitete Desktop-Umgebung auf dem Linux-Desktop (Stand: 2017).\n\nXfce basiert wie Gnome, LXDE und ROX auf dem GUI-Toolkit GTK+, unterstützt derzeit über 40 Sprachen und kann mittels Themes im Aussehen angepasst werden. Da Xfce 4 zu den Standards von freedesktop.org konform ist, kann es auch mit Gnome- und KDE-Programmen umgehen – und umgekehrt.\n\nDer Name \"Xfce\" stand ursprünglich für \"XForms Common Environment\" („Allgemeine XForms-Umgebung“), da Xfce anfangs auf dem GUI-Toolkit XForms basierte.\nDa neuere Versionen von Xfce nicht mehr auf XForms aufbauen, ist das Akronym inzwischen ohne Bedeutung.\nDer Name wird als einzelne Buchstaben in deutsch oder englisch ausgesprochen: [].\n\nDie Entwicklung von Xfce als Panel für FVWM wurde 1996 von Olivier Fourdan begonnen.\nAnfangs basierte Xfce auf dem GUI-Toolkit XForms.\n\nMit der 1997 erschienenen Version 2.0 wurde erstmals der eigens für Xfce entwickelte Fenstermanager \"xfwm\" mitgeliefert.\n\n1999 erschien mit Version 3 dann eine unter der Verwendung von GTK+ neu geschriebene Version. Bis zu dieser Version lehnte sich Xfce an das Common Desktop Environment (CDE) an, das mit vielen kommerziellen Unix-Betriebssystemen ausgeliefert wurde.\n\nVersion 4.0 ist datiert auf den September 2003, wobei fast die gesamte Architektur von Xfce geändert wurde. Dabei wurde die optische Erscheinung verbessert und die Modularität erhöht. Seit dieser Version steht auch ein Framework zur Entwicklung neuer Xfce-Applikationen zur Verfügung.\n\nMit der nach zweijähriger Entwicklung am 21. Januar 2007 vorgestellten Version 4.4 von Xfce hielt die Unterstützung der neuen Composite-Funktionen von X.Org, wie Schatten und echte Transparenz, Einzug (wobei schon Version 4.2 diese teilweise unterstützte). Neu ist ebenfalls die Unterstützung von Icons auf dem Desktop und der auf Leafpad basierende Texteditor \"Mousepad\". Der bisherige Dateimanager xffm wurde durch Thunar ersetzt.\n\nVersion 4.6 wurde am 27. Februar 2009 veröffentlicht. Diese brachte als Neuerung z. B. mit Xfconf ein auf D-Bus aufbauendes Konfigurationssystem.\n\nVersion 4.8 wurde am 16. Januar 2011 nach zwei Jahren Entwicklungsarbeit freigegeben. Es wurde stark an aktuelle Desktop-Frameworks angepasst, so dass ThunarVFS und HAL durch konsequenten Einsatz von GIO, udev, ConsoleKit und PolicyKit nicht mehr benötigt werden. Der Dateimanager Thunar unterstützt zudem nun nativen Zugriff auf SFTP-, SMB- und FTP-Freigaben. Dazu kommt eine Reihe von anderen kleineren Neuerungen.\n\nVersion 4.10 wurde am 28. April 2012 freigegeben. Mit dieser Version können Fenster gekachelt werden und Anwendungen oder Dateien auf dem Desktop mit nur einem Mausklick geöffnet werden. Die Leiste kann nun vertikal angezeigt werden und der Schreibtisch Thumbnails anzeigen. Ein neuer MIME-Typen-Editor erleichtert die Zuordnung von Anwendungen zum Öffnen von Dateien und der Einstellungseditor wurde ausgebaut. Eine weitere Änderung ist, dass der Anwendungsfinder komplett neu geschrieben wurde, wodurch die Pakete xfce4-appfinder und xfrun4 mit ihren Funktionalitäten vereint wurden.\n\nVersion 4.12 wurde am 28. Februar 2015 nach zwei Jahren und zehn Monaten Entwicklung freigegeben. Mit dieser Version wurde der Wechsel zu GTK+ 3 vorangebracht. Der Fenstermanager ist jetzt anpassbar und die Tiling- und Snapfunktion wurde verbessert. Ab Version 4.12 versteckt sich das Panel, wenn ein Fenster an es herangezogen wird. Eine Überarbeitung fand auch bei der Unterstützung mehrerer Monitore statt, für jeden Desktop kann zusätzlich ein eigenes Theme genutzt werden. Thunar unterstützt nun mehrere Tabs, individuelle Shortcuts und eine graphische Anzeige des freien Speichers durch einen Balken. Ab Version 4.12 wird von der Energieverwaltung auch logind und UPower unterstützt.\n\nVersion 4.14 soll in GTK+ 3 laufen. Mit der Version 4.13 ist die zugehörige erste öffentliche Beta-Version im Mai 2017 nun fertig.\n\nZuzüglich der Kernkomponenten (Sitzungsverwaltung, Programmleiste usw.) gehören zum Xfce-Projekt noch die folgenden Programme:\n\nHinzu kommen noch die folgenden Community-Anwendungen aus \"Xfce Goodies\":\nAußerdem mehrere Plugins zur Erweiterung der obigen Komponenten.\n\nIn den allermeisten Linux-Distributionen und BSD-Derivaten kann Xfce nachinstalliert werden, wenn es nicht sowieso, wie in den folgend aufgelisteten, schon die Standard-Umgebung ist oder bei der Installation als Option angeboten wird:\n\n"}
{"id": "31709", "url": "https://de.wikipedia.org/wiki?curid=31709", "title": "NeXT", "text": "NeXT\n\nDas Unternehmen NeXT, Inc. [] (später auch NeXT Computer, Inc. bzw. NeXT Software, Inc.) war ein US-amerikanischer Computer- und Softwarehersteller. Er wurde 1985 von Steve Jobs gegründet, nachdem dieser Apple infolge interner Auseinandersetzungen verlassen musste.\n\nDas Unternehmen mit Sitz in Redwood City, Kalifornien, entwickelte und produzierte eine Reihe von Workstations für den Einsatz in Hochschulen und der Wirtschaft. Vorgestellt wurde der erste NeXT Computer 1988, die kleinere NeXTstation 1990. Die beiden Geräte fanden zwar ihre Nische im Forschungsbereich, konnten sich jedoch darüber hinaus am Markt nicht durchsetzen, was auch auf die hohen Preise ab etwa 6.000 US-Dollar zurückgeführt wurde. Insgesamt wurden Schätzungen zufolge nur 50.000 Geräte verkauft.\n\nDas Betriebssystem NeXTStep mit seiner neuartigen grafischen Benutzeroberfläche gilt in vielen Aspekten als wegweisend. Auch der Aufbau, die Ausstattung und das gesamte technische Design der auch optisch herausragenden Computer hatten viel Einfluss auf heutige Personal Computer, Betriebssysteme und Desktop-Umgebungen.\n\nEnde 1996 wurde NeXT von Apple für 429 Millionen US-Dollar und 1,5 Millionen Apple-Aktien aufgekauft. Teil des Vertrags war, dass Jobs nach elf Jahren Abwesenheit zu Apple zurückkehrte. Die Software von NeXT wurde von Apple weiterentwickelt und bildet so teilweise die Basis der heutigen Betriebssysteme macOS, iOS, watchOS und tvOS.\n\nNachdem Steve Jobs mit fünf Kollegen Rich Page, George Crow, Bud Tribble, Dan'l Lewin und Susan Barnes Apple 1985 verlassen hatte, gründeten sie zusammen mit Randy Heffner und Gary H. Moore 1986 die \"NeXT Computer, Inc.\" Steve Jobs steuerte 7 Millionen US-Dollar aus seinem eigenen Vermögen zur Gründung bei. 1987 erwarb Ross Perot für 20 Millionen US-Dollar einen Anteil von 16 Prozent an NeXT. Im Oktober 1988 stellte Steve Jobs den ersten NeXT-Computer in San Francisco vor. Ab September 1989 wurde dieser dann auch als NeXTcube ausgeliefert. Ebenfalls 1989 erwarb Canon einen Anteil von 16,6 Prozent an NeXT für 100 Millionen US-Dollar.\n\n1990 wurde die NeXTstation vorgestellt, eine flache „Pizzabox“, die unter den Monitor gestellt wurde.\n\nEnde 1990 etablierte sich in Deutschland eine Vertriebsorganisation aus NeXT Deutschland GmbH (München), drei NeXTcentern (d'ART Hamburg, AMG Dortmund und DCS Waldbronn bei Karlsruhe) und zahlreichen NeXTpartnern, darunter auch die Computerabteilung des Karstadt-Oberpollingers in München.\n\n1993 verabschiedete sich NeXT vom Hardware-Geschäft und entwickelte die plattformunabhängige Software NeXTStep für Intel, NeXTstep für Motorola (NeXT-Hardware) und für spezielle Unix-Workstations (PA-RISC) von HP und SPARC, sowie – zumindest im Labor – auch für IBM PowerPC.\n\nIm Zuge der Umorientierung wurde der Unternehmensname 1995 in \"NeXT Software, Inc.\" geändert. Gemeinsam mit Sun Microsystems war OpenStep spezifiziert worden, eine Spezifikation für eine objektorientierte Programmschnittstelle (API) und NeXT implementierte diese in der Version 4.0 seines NeXTStep, das ab da an unter dem Namen OPENSTEP (in Großbuchstaben) vermarktet wurde, um auf die Portabilität hinzuweisen. Als „OPENSTEP for Mach“ lief NeXTs Betriebssystem auf der eigenen Motorola-basierenden Hardware sowie auf Intel-Rechnern und als „OPENSTEP Enterprise“ als Aufsatz auf Windows NT. Parallel dazu entstand mit WebObjects eine objektorientierte Entwicklungs- und Laufzeitumgebung für dynamische Webseiten.\n\nSchließlich wurde das Unternehmen am 20. Dezember 1996 für 402 Millionen US-Dollar von Apple Computer aufgekauft. Steve Jobs kehrte zu Apple zurück und hatte von 1997 bis zu seinem Rücktritt im August 2011 die Position des CEO inne. Das Betriebssystem NeXTStep/OPENSTEP, der Mikrokernel Mach und die Spezifikation OpenStep bildeten zusammen mit FreeBSD die Grundlagen für das heutige Apple-Betriebssystem macOS. Der deutsche Geschäftsführer Gerhard Tauschl führte bei Apple als Sales Manager Enterprise Business Europe das WebObjects-Geschäft weiter, auf der Basis von WebObjects entstand u. a. der iTunes Store.\n\nOpen-Source-Projekte wie die Fenstermanager \"AfterStep\" und \"GNU Window Maker\" machen das Look and Feel der NeXT-Benutzeroberfläche auch für andere Betriebssysteme verfügbar. Eine OpenStep-kompatible API für Unix-Systeme wird mit dem Projekt GNUstep entwickelt.\n\nAuf den Workstations von NeXT lässt sich auch NetBSD als alternatives modernes Betriebssystem einsetzen.\n\nDas Unternehmen NeXT hat bedeutende Beiträge für die Fortschritte im Bereich der Informationstechnologie geleistet. In NeXTStep und den NeXT-Rechnern wurden erstmals Konzepte einer breiteren Öffentlichkeit zugänglich gemacht, die später zum Standard wurden. Beispiele:\n\n\nDie Workstations von NeXT wurden vom Unternehmen frog design des Produktdesigners Hartmut Esslinger entworfen. Wegen des außergewöhnlichen Designs sind einige NeXTcubes in Museen ausgestellt, etwa in der \"Neuen Sammlung\" der Pinakothek der Moderne in München und im Museum of Modern Art in San Francisco.\n\n\n\n\n"}
{"id": "31965", "url": "https://de.wikipedia.org/wiki?curid=31965", "title": "NeXT Computer", "text": "NeXT Computer\n\nDer NeXT Computer (auch NeXT Computer System genannt) ist eine Computer-Workstation, die vom US-amerikanischen Hersteller NeXT Inc. entwickelt, vermarktet und verkauft wurde. Als Betriebssystem verwendet er das von Mach und BSD abgeleitete, Unix-basierte NeXTSTEP-Betriebssystem.\n\nDas Motherboard ist quadratisch und passt in einen von vier identischen Steckplätzen im Gehäuse. Dieses besteht aus einer 305 mm großen, würfelförmigen, schwarzen Magnesium-Hülle, was dazu führte, dass der Rechner als \"\"The Cube\"\" bezeichnet wurde. 1988 wurde der NeXT Computer zu einem Preis von 6 500 US-Dollar (entspricht 11 800€ im Jahr 2017) eingeführt.\n\nDer NeXT Computer wurde 1990 durch den NeXTcube abgelöst.\n\n"}
{"id": "32104", "url": "https://de.wikipedia.org/wiki?curid=32104", "title": "A/UX", "text": "A/UX\n\nA/UX (Apple Unix) war ein kommerzielles Unix-Betriebssystem des Unternehmens Apple und wurde entwickelt, um unter der Macintosh-Benutzeroberfläche eine bedienerfreundliche Anbindung an die Unix-Welt herzustellen.\n\nBasierend auf System V Release 2 (teilweise \"4.2BSD\"), wurde A/UX gegen Ende der 1980er Jahre entwickelt und 1988 erstmals auf den Markt gebracht. Später wurde die Codebasis auf aktuellere Versionen von \"UNIX System V\" umgestellt und es flossen Teile von BSD in das System mit ein.\n\nAls POSIX-kompatibles Betriebssystem entworfen erreichte A/UX volle Unix-Funktionalität, das Hauptmerkmal war aber die Verbindung dieser Fähigkeiten mit dem damaligen Macintosh-Betriebssystem System 6 und später System 7. So war es möglich, auf einem Rechner \"gleichzeitig\" Macintosh- und Unix-Anwendungen auszuführen. Dabei liefen die Unix-Prozesse mit präemptivem Multitasking, während die Macintosh-Anwendungen weiterhin kooperatives Multitasking verwendeten. Um auch auf anderen Rechnerarchitekturen Macintosh-Applikationen verwenden zu können, wurde mit dem Macintosh Application Environment (MAE) eine virtuelle System-7-Umgebung geschaffen, die auch auf anderen Unix-Betriebssysteme portiert wurde und aus der später die Blue Box (für Mac OS 8 unter Rhapsody) und daraus die Classic-Umgebung (für Mac OS 9 unter Mac OS X) entstand.\n\nDa A/UX keine Marktbedeutung erlangen konnte, wurde die Weiterentwicklung bereits 1995 eingestellt. Mit Mac OS X folgt Apple seit dem Jahr 2000 allerdings erneut der Tradition der Unix-Derivate.\n\nUnterstützte Rechner basieren auf der Motorola-68000er-Familie mit erweiterter PMMU (68020 oder neuer) und somit auf Macintosh-Modellen seit dem Macintosh II. Die grafische Oberfläche X11 und Mac OS, welches als Betriebssystem für den Bootloader benötigt wurde, waren Bestandteil des Lieferumfangs.\n\nEin wesentlicher Vorteil von A/UX gegenüber anderen Unix-Systemen bestand in einem sehr vereinfachten Installationsvorgang, der sog. \"Ein-Klick\"-Installation.\n"}
{"id": "32283", "url": "https://de.wikipedia.org/wiki?curid=32283", "title": "Atari 800", "text": "Atari 800\n\nDer Atari 800 ist ein auf dem 6502-Mikroprozessor basierender Heimcomputer des US-amerikanischen Herstellers Atari, Inc.\n\nDer Atari 800 wurde ab Ende 1979 zunächst nur im US-amerikanischen Versandhandel angeboten und wegen seiner vielseitigen Möglichkeiten zur Erweiterung und damit Zukunftsfähigkeit massiv als „zeitloser Computer“ angepriesen. Nach verschiedenen von Atari angestoßenen Kooperationen im Bildungssektor, der Veröffentlichung von Spiele-Kassenschlagern wie \"Star Raiders\" und dem Ausbau des Atari-Händlernetzes gelang es, die Bekanntheit kontinuierlich zu steigern. Verkaufsfördernd kam die ab Mitte 1981 vollzogene Expansion nach Europa hinzu, die schließlich in der bis Ende 1982 währenden Marktführerschaft Ataris gipfelte.\n\nDurch den Misserfolg seines Anfang 1983 parallel eingeführten Computermodells Atari 1200XL und den seinen Höhepunkt erreichenden Preiskrieg mit anderen Herstellern, verlor Atari binnen eines Jahres wieder viele seiner Marktanteile hauptsächlich an Commodore. Etwa zeitgleich mit Ankündigung der Modelle Atari 600XL und Atari 800XL stellte man Mitte 1983 die Produktion des Atari 800 ein. Bis etwa Anfang 1985 währende Lagerverkäufe miteingerechnet wurden von den beiden Computermodellen Atari 400 und 800 zusammen insgesamt etwa zwei Millionen Einheiten verkauft.\n\nBereits kurz nach der Veröffentlichung galt der Atari 800 als Meilenstein in der Heimcomputergeschichte: Er habe nach Meinung vieler Autoren durch seine auf Benutzerfreundlichkeit ausgelegte Konstruktion und die robuste Verarbeitung auch völlig unerfahrenen Benutzern einen leichten Einstieg in die bis dahin eher Spezialisten vorbehaltene Computertechnik eröffnet.\n\nNoch während der letzten Entwicklungsphase für die Videospielekonsole Atari 2600 begann Atari Anfang 1977 mit den Planungsarbeiten für ein Nachfolgemodell. Die Bemühungen der Ingenieure konzentrierten sich dabei hauptsächlich auf die Erweiterung der Grafikfähigkeiten des im Atari 2600 verbauten hochintegrierten Spezialschaltkreises \"Television Interface Adapter (TIA).\" Die Verbesserungen versprachen anspruchsvollere Spiele bei gleichzeitig verringertem Aufwand zu ihrer Entwicklung.\n\nEin noch handverdrahteter früher Prototyp des \"Alphanumeric Television Interface Controller (ANTIC)\" wurde der Leitung von Atari kurz darauf vorgestellt. Anschließende Machbarkeitsstudien zu möglichen Kombinationen des neuen Spezialbausteins mit weiteren elektronischen Baugruppen zeigten rasch über den Einsatz in einer reinen Spielkonsole hinausgehende Potentiale auf. So schienen eine integrierte Tastatur für Programmierzwecke und die Ansteuerung externer Geräte beispielsweise zum Datentransfer sowohl technisch als auch ökonomisch möglich.\n\nEin modularer Aufbau und die Fähigkeit zur Programmierung waren damals lediglich den in Industrie und Forschung eingesetzten teuren Computern von IBM oder DEC und mit deutlichen Abstrichen den wesentlich günstigeren Heimcomputern wie Altair 8800, TRS-80, PET 2001 und Apple II vorbehalten. Insbesondere letztere krankten jedoch an der Umständlichkeit der Bedienung, der Unzuverlässigkeit der Technik und im Vergleich zu Spielkonsolen der damals neuesten Generation immer noch an der Höhe der Anschaffungskosten. Technisch wenig versierte, jedoch elektronischer Datenverarbeitung gegenüber aufgeschlossene Interessengruppen mit schmalem Geldbeutel blieben so außen vor. Diese Zielgruppe im Auge, verwarfen die Verantwortlichen von Atari rasch die ursprünglichen Pläne für eine auf dem ANTIC basierende neue Spielekonsole zugunsten eines eigenen, preisgünstigen und konzeptionell neuartigen Heimcomputers. Die Benutzung hatte einfach und sicher auch für Anfänger zu sein und das Gerät musste ohne technische Detailkenntnisse des Anwenders mit handelsüblichen Fernsehern betrieben werden können. Daneben sollte die Möglichkeit zum schnellen und bequemen Laden von Spielen und Anwendungsprogrammen ähnlich den von Spielekonsolen bekannten Steckmodulen vorhanden sein.\n\nNeben der angestrebten leichten Bedienbarkeit spielten insbesondere niedrige Herstellungskosten des zu entwickelnden Gerätes eine große Rolle; die zunächst geforderte Kompatibilität mit Spielen der Atari-VCS-2600-Konsole verwarfen die Verantwortlichen bereits nach kurzer Zeit. Die daraufhin von den Hauptentwicklern vorgelegten technischen Eckpunkte des neuen Systems wurden von der Firmenleitung im August 1977 für gut befunden und weitere finanzielle Mittel auch zur Aufstockung des Entwicklungspersonals zur Verfügung gestellt. Damit einhergehend erhielt das Heimcomputerprojekt den firmeninternen Codenamen \"Colleen.\"\n\nMit fortschreitendem Stand der Arbeiten entschieden sich die Verantwortlichen, die Entwicklung zweier unterschiedlicher Ausbaustufen des Heimcomputers zu verfolgen: eine stark abgerüstete Variante hauptsächlich für Zwecke der Unterhaltung und ein anwendungsorientiertes Gerät mit Schreibmaschinentastatur und Möglichkeiten zur Erweiterung. Die Entwicklungsarbeiten für die erste Variante wurde im November in ein separates Projekt mit dem Namen \"Candy\" – dem späteren Atari 400 – ausgegliedert, die für das hochwertige Gerät unter dem Namen \"Colleen\" weitergeführt.\n\nErste Entwürfe sahen 4 KB Arbeitsspeicher, zwei Steckmodulschächte, eine parallele Schnittstelle für Peripheriegeräte, eine Tastatur und diverse Erweiterungsmöglichkeiten vor. Nachdem die Konstruktion des ANTIC im Januar 1978 abgeschlossen worden war, konzentrierten sich die weiteren Bemühungen auf die Fertigstellung der Spezialbausteine \"Color Television Interface Adapter (CTIA)\" und \"Potentiometer and Keyboard Integrated Circuit (POKEY).\" Die Entwicklungsarbeiten an den als handverdrahteten Steckplatinen vorliegenden Spezialbausteinen zogen sich bis Ende März hin und verschlangen insgesamt mehr als zehn Millionen US-Dollar.\n\nNachfolgenden Abstimmungen der Spezialbausteine auf den zwischenzeitlich ausgewählten Hauptprozessor 6502 von MOS wurden mithilfe des Computersystems \"Z-2\" von Cromenco durchgeführt. Die damit verbundene Entwicklung der Leiterplatten für den neuen Computer dauerte bis Mitte Juni an; letzte Arbeiten, die insbesondere die Tastatur betrafen, konnten im August abgeschlossen werden. Das äußere Erscheinungsbild des Computers war bereits Ende April festgelegt und nur wenig später das Gehäuse nebst integrierter elektromagnetischer Abschirmung fertiggestellt worden.\n\nParallel zu den noch verbliebenen Arbeiten an einigen mechanischen Komponenten des Computers erfolgte die Sondierung des Marktes für höhere Programmiersprachen. Die Verantwortlichen entschieden sich dabei für BASIC, eine einsteigerfreundliche Sprache, mit der das neue Computersystem durch den Benutzer für eigene Zwecke programmiert und eingesetzt werden kann. Eine Eigenentwicklung durch Atari schied wegen fehlender Kapazitäten bei einer nur kurz zur Verfügung stehenden Frist von sechs Monaten aus. Nachdem der Einsatz des damals marktbeherrschenden Microsoft BASIC an Ataris technischen Erfordernissen gescheitert war, wurde Anfang Oktober 1978 Shepardson Microsystems, Inc. mit der Erstellung eines eigenen, speziell auf die Atari-Computer zugeschnittenen BASIC-Dialektes betraut.\n\nNach Festsetzung der Konfiguration des Arbeitsspeichers auf marktübliche 8 KB änderte Atari im November 1978 den inoffiziellen Namen \"Colleen\" in den direkt an die Speichergröße angelehnten offiziellen Produktnamen \"Atari 800.\" Die der Ziffer \"8\" nachgestellte Doppelnull klassifiziert dabei den Computer als Basisgerät der ihm zugehörigen Peripheriegeräte. Kurz darauf, am 6. Dezember 1978, erfolgte die Verkündung des Heimcomputerprojektes mit seinen beiden Geräten Atari 400 und Atari 800 publikumswirksam in einem Artikel der auflagenstarken \"New York Times.\"\nEinen ersten Blick auf seine neue Produktlinie gewährte Atari Interessenten erstmals im Januar 1979 auf der Winter Consumer Electronics Show in Las Vegas. Der Atari 800 war dort zusammen mit dem dazu passenden Diskettenlaufwerk \"Atari 810\" und dem Drucker \"Atari 820\" zu sehen. Einem größeren Publikum war der Atari 800 erstmals im Mai im Rahmen der \"4th West Coast Computer Faire\" in San Francisco zugänglich. Auf der Summer CES in Chicago wurde die unverbindliche Preisempfehlung in Höhe von 1000 US-Dollar bekanntgegeben.\n\nIm Juni wurden letzte Arbeiten abgeschlossen und der Abnahmetest zur elektromagnetischen Verträglichkeit durch die US-amerikanische Federal Communications Commission im August erfolgreich absolviert – eine maßgebliche Voraussetzung zur Verkaufbarkeit des Gerätes in Nordamerika. Die Fertigung der Computer, deren Entwicklung bislang etwa 100 Millionen US-Dollar gekostet hatte, wurde Ataris Fabrik im kalifornischen Sunnyvale übertragen. Die Produktion konnte jedoch erst im Oktober 1979 aufgenommen werden, da die rasch wachsende Heimcomputerbranche ab Spätsommer 1979 unter einer anhaltenden Teileknappheit litt.\n\nBereits geraume Zeit vor dem Verkaufsstart pries der Hersteller seinen Atari 800 unter Anspielung auf die universelle Erweiterbarkeit und damit die langwährende Nutzbarkeit als „Timeless Computer“ an, der für Einsteiger und Spezialisten gleichermaßen geeignet sei („[…] can be used by people with no previous computer experience, although it doesn’t compromise capability for the sophisticated user“).\n\nDie erste Serie von Geräten wurde ab November 1979 im Rahmen einer Testvermarktung sowohl in der Weihnachtsausgabe des Versandkatalogs als auch in den Fotoabteilungen einiger Ladengeschäfte der Handelskette Sears Roebuck angeboten. Neben dem Computer mit Netzteil, Anschluss- und Anleitungsmaterial erhielt der Käufer für 999,99 US-Dollar einen Programmrekorder \"Atari 410\" und weiteres Zubehör. Dazu zählte die Grundausstattung für das \"Educational System\" und die Programmiersprache BASIC beide jeweils in Form eines Steckmoduls nebst zugehörigem Anleitungsmaterial.\n\nKurz nach dem Verkaufsstart begann Atari, seine Geräte und dazugehörige Unterhaltungssoftware wie das Spiel \"Star Raiders\" auf Fachmessen vorzustellen. Neben allgemeiner Produktwerbung gelang es damit auch, neue Vertriebskanäle zu erschließen. Begleitet wurden die Präsentationen ab dem zweiten Quartal 1980 durch weitere umfangreiche und langfristig geplante Werbeoffensiven. Nach einer zwischenzeitlichen Preiserhöhung auf 1080 US-Dollar änderte Atari am 1. Juni 1980 zudem die Vermarktungsstrategie für den Atari 800 weg vom Bündelangebot hin zum Einzelgerät. Programmrekorder und Educational System waren nun nicht länger im Lieferumfang enthalten, dafür wurde der ab Werk verbaute Arbeitsspeicher auf zeitgemäße 16 KB erhöht.\n\nAb Mitte 1980 war die Bekanntheit der Atari-Computer so gestiegen, dass auch Dritthersteller vielversprechende Absatzpotentiale sowohl für Hard- als auch Software sahen und ihrerseits Produkte auf den Markt brachten.\n\nErgänzend zur Herstellung und zum Vertrieb von Unterhaltungssoftware verstärkte Atari die Bemühungen zur Platzierung seiner Heimcomputer in nordamerikanischen Bildungseinrichtungen, einem bislang von Apple II und Commodore PET dominierten Bereich. Dem lag das Kalkül zugrunde, dass Schüler und Studenten im Rahmen von späteren Privatanschaffungen auf das bereits aus der Schule Bekannte und Vertraute – einen Computer von Atari – zurückgreifen würden. Neben speziellen Verkaufskonditionen für das Bildungswesen war mit der Programmreihe \"Talk & Teach Cassette Courseware\" bereits frühzeitig auch die passende Software aufgelegt worden. Zudem setzte Atari ab Mitte 1980 verstärkt auf die Zusammenarbeit mit der zu IBM gehörigen Organisation \"Science Research Associates,\" die sich der Förderung des computergestützten Unterrichts verschrieben hatte und den Vertrieb für Atari im Bildungssektor übernahm. Im Rahmen dieser Kooperation finanzierte IBM einen Rabatt, der Bildungseinrichtungen von der Grundschule bis hin zur Universität beim Kauf eines Atari-800-Computers einen zusätzlichen kostenfreien Atari 400 gewährte. Atari selbst legte für Schulen wenig später eine ähnliche Preisaktion in Form des \"3 for 2 deal\" auf: Beim Kauf zweier Atari-800- oder Atari-400-Computer erhielt der Käufer einen weiteren Atari 400 gratis dazu.\n\nDie für die Jahre 1979 und 1980 angegebenen Verkaufszahlen für die Modelle Atari 400 und Atari 800 zusammengenommen schwanken zwischen 50.000 und 300.000 Geräten. Die Umsätze allein für 1980 beliefen sich auf etwa 20 Millionen US-Dollar.\n\nBereits im Laufe des ersten Halbjahres 1981 konnten sich die Atari-Computer trotz permanenter Lieferschwierigkeiten und einiger technischer Probleme bei Zubehörteilen als feste Größen auf dem bislang hauptsächlich von Tandy, Apple und Commodore beherrschten Heimcomputermarkt etablieren. Die von Ataris Computersparte erzielten Umsätze lagen Mitte des Jahres 1981 bei zehn Millionen Dollar – die Summe der durch die laufende Produktion verursachten Verluste belief sich jedoch auf einen ähnlich hohen Betrag. Zur Bewältigung der zunehmenden Nachfrage und zur zügigen Umsetzung der geplanten weltweiten Vermarktung nahm Atari im April personelle Erweiterungen im Firmenmanagement vor. Damit einhergehend führte Ataris individuell auswählbare und speziell auf Techniklaien zugeschnittene Erweiterungspakete für seine Computer ein. Diese „Starter Kits“ enthielten jeweils aufeinander abgestimmte, anschlussfertige Hard- und Software für die Einsatzbereiche Programmieren \"(Atari Programmer),\" Unterhaltung \"(Atari Entertainer),\" Bildung \"(Atari Educator)\" und Netzwerk-Aktivitäten \"(Atari Communicator).\" Nur wenig später im August 1981 gelang es bereits, den Umsatz auf 13 Millionen Dollar zu steigern, womit erstmals die Gewinnzone erreicht wurde.\n\nAußer in den Ausbau des Hardwaresektors investierte Atari auch in die Fortbildung seines Kundendienstes und der Vertragshändler sowie in die Softwareunterstützung für die Heimcomputer. Dazu zählten die beinahe monatlich erfolgenden Veröffentlichungen neuer hauseigener Programme und Spiele, die von Drittherstellern langerwartete Publikation technischer Dokumentationen und die Unterstützung unabhängiger Programmautoren. Letzteres umfasste die Ausrichtung von offenen Programmierwettbewerben mit entsprechend hoch dotierten Preisen, technische Schulungen in Ataris \"Acquisition Centers\" und die Gründung der Publikationsplattform \"Atari Program Exchange (APX).\" Durch die Gründung von APX ermöglichte Atari den betriebswirtschaftlich häufig gänzlich unerfahrenen Softwareherstellern den Vertrieb ihrer Programme durch das mittlerweile in Nordamerika voll ausgebaute Atari-Händlernetz.\n\nIm Fahrwasser der amerikanischen Verkaufserfolge startete Atari im Sommer 1981 die Erschließung des lukrativen europäischen Marktes. Wie in den USA auch wurde die Veröffentlichung in Großbritannien (645 £), Italien (1.980.000 ₤) und den Benelux-Staaten von umfangreichen Werbemaßnahmen im Printbereich und von Präsentationen auf speziellen Ausstellungen begleitet. In Frankreich dagegen begann der Verkauf (7500 F) vermutlich wegen zeitaufwendiger Hardware-Anpassungen an die SECAM-Fernsehnorm erst im September 1982.\n\nIn Westdeutschland übernahm ab August 1981 die bereits seit 1980 für die Atari-2600-Vermarktung zuständige \"Atari Elektronik Vertriebsgesellschaft mbH\" den Vertrieb und den Kundendienst. Die Vermarktung der „Privatcomputer“, so die offizielle Bezeichnung von Atari Deutschland, erforderte erhebliche Investitionen insbesondere für die Werbung, Verkäuferschulungen und Serviceaktivitäten. Analog den Promotionsbemühungen im Videospielebereich schaltete Atari entsprechende Werbung in Printmedien. Neben dem Verkauf im Versandhandel und in Fachgeschäften waren die Rechner auch in größeren Kaufhausketten wie Horten und Karstadt erhältlich. Die unverbindliche Preisempfehlung des Atari 800 mit 16 KB Arbeitsspeicher lag bei 2995 DM, das Diskettenlaufwerk Atari 810 kostete knapp 2000 DM und das BASIC-Steckmodul konnte für 272 DM erworben werden. Vor dem offiziellen Verkaufsstart bot Telectron GmbH bereits im Jahr 1980 die US-amerikanische Ausführung des Atari 800 mit 8 KB Arbeitsspeicher für 4200 DM an.\n\nWährend der internationalen Expansionsphase reagierte Atari auf die sich immer weiter zuspitzende Konkurrenzsituation vor allem in Nordamerika unter anderem mit technischen Überarbeitungen seiner Computer in Form eines revisionierten Betriebssystems für Neugeräte („OS Version B“) und einer fehlerbereinigten Version der BASIC-Programmiersprache. Im Geschäftsjahr 1981 konnte Atari so nach eigenen Angaben etwa 300.000 Heimcomputer absetzen, womit sich diese endgültig als Massenware etabliert hatten und Atari zum US-amerikanischen Marktführer aufsteigen ließen.\n\nDie Einführung diverser Billigcomputer wie dem Sinclair ZX81 trotzten auch Atari erhebliche Preisreduktionen ab. Einen ersten Nachlass in Höhe von 16 Prozent gewährte Atari im Januar 1982, womit der unverbindliche Verkaufspreis des Atari 800 auf 899 US-Dollar sank. Darüber hinaus erfolgte die Auslieferung fortan in einer silberfarbenen Hochglanzverpackung, wie sie für den Atari 400 bereits ein Jahr zuvor eingeführt worden war. Auch in Westdeutschland zeitigte die aggressive Preispolitik von Commodore ihre Wirkung: Atari Deutschland sah sich im August 1982 zu einer ersten aber drastischen Senkung des Verkaufspreises von 2995 auf 1995 DM gezwungen.\n\nAb Frühherbst 1982 - vermutlich mit dem von Texas Instruments im amerikanischen Heimcomputermarkt losgetretenen Preiskrieg - sah Atari von weiteren direkten Preisnachlässen ab und schwenkte vielmehr auf kaufbegleitende Rabattaktionen um: Beim Erwerb von Ataris Hard- und Software wurden den Käufern durch „Softwarecoupons“ Ersparnisse von bis zu 60 US-Dollar auf viele Produkte aus Ataris Programmsortiment ermöglicht. Daneben erhielten Käufer des Atari 800 ab Oktober zwei zusätzliche 16-KB-Speichererweiterungen gratis, womit Atari den Rechner faktisch nur noch in der höchsten Ausbaustufe mit 48 KB Arbeitsspeicher anbot. Parallel zu seinen Rabattaktionen baute Atari im Laufe des Jahres 1982 vor allem in Nordamerika den Kundendienst massiv aus. Die in den USA landesweit eingerichteten \"Atari Service Center\" übernahmen fortan Beratungs- und Reparaturdienstleistungen, aber auch die Umrüstung älterer Computer auf den neuen GTIA-Grafikbaustein und das revisionierte Betriebssystem. Sie ermöglichten zudem die durch Ataris Firmenleitung angestrebten profitträchtigen Verkäufe durch große Handelsketten wie J.C. Penney, K-Mart und Toys “R” Us, die aufgrund fehlenden qualifizierten Personals keinerlei Beratung oder Garantiedienstleistungen anzubieten in der Lage waren. Diese mittlerweile hauptsächlich auf Massenvermarktung ausgerichtete Verkaufspolitik bescherte Atari im Laufe des Jahres 1982 annähernd 600.000 Heimcomputerverkäufe, wovon auf den Atari 800 allein etwa 200.000 Einheiten entfielen. Mit insgesamt etwa 1,2 Millionen verkauften Geräten der Modelle 400 und 800 konnte Atari damit seine Marktführerschaft erfolgreich verteidigen.\n\nTrotz Ataris weltmarktbeherrschender Stellung konnten in Westdeutschland im Laufe des Jahres 1982 nur etwa 2000 Atari-800-Computer verkauft werden. Aufgrund der Absatzprobleme und des damit verbundenen hohen Preisdrucks amortisierten sich die Investitionen von Atari Deutschland nur schleppend und die Heimcomputersparte entwickelte sich allmählich zum ungeliebten Stiefkind des nationalen Videospiele-Marktführers.\n\nIm März 1983 brachte Atari ein Nachfolgemodell mit zeitgemäßen 64 KB RAM und neuem Gehäusedesign in den Handel. Aufgrund mangelnder Kompatibilität zu seinen Vorgängern war diesem Atari 1200XL jedoch kein großer Erfolg beschieden, sodass er über eine nur sehr kurzzeitige Veröffentlichungsphase in den USA nicht hinauskam. Umsomehr schnellten die Verkäufe des Atari 800 in unerwartete Höhen, da dessen Preis mit Einführung des neuen Gerätes auf 500 US-Dollar gesenkt worden war und er zudem keine Programminkompatibilitäten befürchten ließ.\n\nMit Ankündigung des offiziellen Nachfolgers \"Atari 800XL\" auf der Summer CES in Chicago und der damit verbundenen Produktionseinstellung im August beschleunigte sich der Preisverfall immer weiter; im September 1983 schließlich wurden die Geräte für 165 US-Dollar verramscht. Die Modelle 400 und 800 zusammengenommen, verkaufte Atari insgesamt etwa 2 Millionen Geräte.\n\nDie überschaubare Architektur des Systems und umfangreiche Dokumentationen des Herstellers ermöglichen den miniaturisierten Nachbau der Elektronik des Atari 800 und dazu kompatibler Modelle mit heutigen technischen Mitteln bei gleichzeitig überschaubarem Aufwand. Eine solche moderne Realisierung erfolgte erstmals 2014 – wie bei anderen Heimcomputersystemen auch – als Implementierung auf einem programmierbaren Logikschaltkreis (FPGA) nebst Einbettungssystem. Die Nachbildung mittels FPGA-Technologie war zunächst lediglich als technische Machbarkeitsstudie gedacht, stellte jedoch im Nachhinein auch ihren praktischen Nutzen unter Beweis: Durch die Miniaturisierung und die Möglichkeit des Batteriebetriebs ist sie eine leicht verstaubare, zuverlässig arbeitende und transportable Alternative zur originalen schonenswerten Technik.\n\nDas Gehäuse des Atari 800 enthält insgesamt drei Leiterplatten und ein stabiles Aluminiumgussgehäuse zur Abschirmung der vom Computer verursachten elektromagnetischen Störfelder.\n\nDie Hauptbestandteile der größten Platine bilden der Spezialbaustein POKEY sowie die Ein-/Ausgabebaugruppen nebst Peripherieanschlüssen. Daneben stellt sie als Bauelementeträger Steckplätze für die kleineren Platinen bereit. Diese enthalten die Prozessor-Baugruppe mit 6502-CPU (engl. \"Central Processing Unit\") nebst den Spezialbausteinen GTIA sowie ANTIC und die Baugruppen zur Spannungsregelung plus Fernsehsignalerzeugung. Der Festwertspeicher (ROM) wie auch der Arbeitsspeicher sind im Erweiterungsschacht in Form von Steckkarten untergebracht. Zur Grundausstattung gehörte neben dem Computer ein externes Netzteil, ein Antennenkabel nebst Antennenschaltbox und die Bedienungsanleitung für das Gerät.\n\nDer Atari 800 basiert auf dem 8-Bit-Mikroprozessor \"MOS 6502,\" der häufig in zeitgenössischen Computern eingesetzt wurde. Die CPU kann auf einen Adressraum von 65536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobytes (KB) festlegt. Der Systemtakt beträgt bei PAL-Geräten 1,77 MHz, für solche mit NTSC-Ausgabe dagegen 1,79 MHz.\n\nWesentlicher Bestandteil der Rechnerarchitektur sind die drei von Atari entwickelten Spezialbausteine \"Alphanumeric Television Interface Controller (ANTIC), Graphic Television Interface Adapter (GTIA)\" mit seinem Vorläufer \"Color Television Interface Adapter (CTIA)\" und \"Potentiometer And Keyboard Integrated Circuit (POKEY).\" Sie sind funktionell derart konzipiert, dass sie innerhalb ihres Aufgabenbereiches flexibel einsetzbar sind und gleichzeitig die CPU entlasten.\n\nDie beiden Grafikbausteine ANTIC und CTIA/GTIA erzeugen das am Fernseher oder Monitor angezeigte Bild. Dazu sind zuvor vom Betriebssystem oder den Benutzer im Arbeitsspeicher entsprechende Daten in der Form der „Display List“ zu hinterlegen. Der CTIA/GTIA erlaubt unter anderem das Integrieren von maximal acht unabhängigen aber jeweils einfarbigen Grafikobjekten, den Sprites. Diese im Atari-Jargon auch „Player“ und „Missiles“ genannten Objekte werden gemäß benutzerdefinierbaren Überlappungsregeln in das vom ANTIC erzeugte Hintergrundbild kopiert und einer Kollisionsprüfung unterzogen. Dabei wird festgestellt, ob sich die Sprites untereinander oder bestimmte Teile des Hintergrundbildes („Playfield“) berühren. Diese Fähigkeiten wurden – wie sich bereits anhand der Namensgebung „Playfield“, „Player“ und „Missiles“ abzeichnet – zur vereinfachten Erstellung von Spielen mit interagierenden Grafikobjekten und schnellem Spielgeschehen entwickelt. Die Fähigkeiten der beiden Spezialbausteine ANTIC und CTIA/GTIA zusammengenommen, verleihen den Darstellungsmöglichkeiten der Atari-Rechner eine von anderen damaligen Heimcomputern unerreichte Flexibilität. Im dritten Spezialbaustein POKEY sind weitere elektronische Komponenten zusammengefasst. Diese betreffen im Wesentlichen die Tonerzeugung für jeden der vier Tonkanäle, die Tastaturabfrage und den Betrieb der seriellen Schnittstelle \"Serial Input Output (SIO)\" zur Kommunikation des Rechners mit entsprechenden Peripheriegeräten.\n\nDurch die hochintegrierte Ausführung (LSI) vereinen die Spezialbausteine viele elektronische Komponenten in sich und senken dadurch die Anzahl der im Rechner benötigten Bauteile, was wiederum eine nicht unerhebliche Kosten- und Platzersparnis mit sich bringt. Nicht zuletzt weil ihre Konstruktionspläne nie veröffentlicht wurden, waren sie mit damaliger Technik nicht wirtschaftlich zu kopieren, womit der in der Heimcomputerbranche durchaus übliche illegale Nachbau von Computern für den Atari 800 ausgeschlossen werden konnte.\n\nDie Bildschirmnormen PAL, NTSC und SECAM werden durch unterschiedliche externe elektronische Beschaltungen der CPU, entsprechend modifizierte Spezialbausteine ANTIC (NTSC-Version mit Teilenummer C012296, PAL-Version mit C014887) und GTIA (NTSC-Version mit Teilenummer C014805, PAL-Version mit C014889, SECAM-Version mit C020120) sowie verschiedene darauf abgestimmte Versionen des Betriebssystems realisiert.\nÜbersicht der vom Betriebssystem des Atari 800 bereitgestellten Grafikstufen\n\nDer von der CPU und ANTIC ansprechbare Adressraum segmentiert sich beim Atari 800 in verschiedene Abschnitte unterschiedlicher Größe. Aus praktischen Gründen ist es üblich, für deren Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Ihr wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer 32 KB große Bereich von $0000 bis $7FFF ist ausschließlich für Arbeitsspeicher vorgesehen und in der kleinsten Ausbaustufe des Atari 800 mit 16 KB RAM ausgestattet. Darüber hinaus sind Erweiterungen bis beispielsweise 48 KB möglich, wobei die belegten Speicheradressen dann bis $BFFF reichen. Nach dem Einfügen eines Steckmoduls wird der 8 KB große, inmitten des Arbeitsspeichersegments gelegene Bereich von $8000 bis $9FFF abgeschaltet und dort die im Steckmodul befindlichen ROMs eingeblendet. Damit stehen bei der Verwendung steckmodulbasierter Programme wie beispielsweise von Atari-BASIC etwa 8 KB Arbeitsspeicher weniger zur Verfügung. Die Adressen der Spezialbausteine und anderer Hardwarebestandteile befinden sich innerhalb eines von $D000 bis $D7FF reichenden Segmentes, unmittelbar gefolgt von den mathematischen Fließkommaroutinen ($D800 bis $DFFF) und dem Betriebssystem ($E000 bis $FFFF). Der Bereich von $C000 bis $CFFF ist für später durch Atari zu ergänzende Systemsoftware vorgesehen, kann aber auch durch Arbeitsspeicher oder alternative Betriebssystemkomponenten genutzt werden.\n\nNach dem Einschalten des Rechners liest die CPU zunächst die Inhalte der ROM-Bausteine mit dem Betriebssystem aus, womit der Atari 800 nebst angeschlossenen Peripheriegeräten initialisiert wird. Sind keine Steckmodule mit ausführbaren Inhalten vorhanden, wird vom Betriebssystem das sogenannte \"Memo Pad\" gestartet. Es handelt sich dabei um ein rudimentäres Texteingabeprogramm ohne weitere Möglichkeiten wie etwa die des Speicherns.\n\nAls Verbindungen zur Außenwelt stehen vier Kontrollerbuchsen an der Vorderseite des Gehäuses, ein koaxialer HF-Antennenanschluss für den Fernseher, ein Schacht zur ausschließlichen Verwendung von ROM-Steckmodulen sowie eine Buchse der proprietären seriellen Schnittstelle (\"Serial Input Output,\" kurz \"SIO\") zur Verfügung. Letztere dient dem Betrieb von entsprechend ausgestatteten „intelligenten“ Peripheriegeräten mit Identifikationsnummern. Dabei kommt ein von Atari speziell für diesen Zweck entwickeltes Übertragungsprotokoll und Steckersystem zum Einsatz. Drucker, Diskettenlaufwerke und andere Geräte mit zwei SIO-Buchsen können so mit nur einem einzigen Kabeltyp „verkettet“ angeschlossen werden. Dabei dient jeweils eine der beiden Buchsen zur Kommunikation des Geräts mit dem Computer \"(serial bus input)\" und die verbleibende zum Anschluss und Verwalten eines weiteren Geräts \"(serial bus extender).\" Die in vielen anderen zeitgenössischen Computersystemen verwendeten Standardschnittstellen RS-232C (seriell) und Centronics (parallel) werden durch die extra zu erwerbende Schnittstelleneinheit \"Atari 850\" zur Verfügung gestellt.\n\nDer Atari 800 ist grundsätzlich mit allen von Atari auch später veröffentlichten Peripheriegeräten für die XL- und XE-Reihe betreibbar, die zum Anschluss nicht den bei XL- und XE-Computern herausgeführten Parallelbus benötigen. Im Folgenden wird ausschließlich auf die von Ende 1979 bis Ende 1983 erhältlichen eingegangen.\n\nIn Zusammenhang mit vor allem westlichen Heimcomputern der 1980er Jahre kamen zur Datensicherung hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die günstigste Variante der Datenaufzeichnung durch Kompaktkassetten hat im Allgemeinen den Nachteil niedriger Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren. Bei Veröffentlichung des Atari 800 standen ihm Kassetten- und wenig später auch Disketten- und Festplattensysteme als Massenspeicher zur Verfügung.\n\nIm Gegensatz zu anderen zeitgenössischen Heimcomputern wie beispielsweise dem TRS-80 oder dem Sinclair ZX81 kann der Atari 800 zum Speichern von Daten nicht mit handelsüblichen Kassettenrekordern betrieben werden. Vielmehr benötigt er ein auf seine serielle Schnittstelle abgestimmtes Gerät – den Atari-410-Programmrekorder. Die durchschnittliche Datenübertragungsrate beträgt dabei 600 Bit/s; auf einer 30-Minuten-Kassette finden 50 KB an Daten Platz. Daneben verfügt der Atari 410 noch über die Besonderheit eines Stereo-Tonkopfes, wodurch parallel zum Lesevorgang das Abspielen von Musik oder gesprochenen Benutzungsanweisungen möglich ist. Aus Gründen der Kosten- und Platzersparnis ist im Gerät kein Lautsprecher verbaut, die Audiosignale werden vielmehr über das SIO-Kabel via POKEY am Fernsehgerät ausgegeben. Auch ist keine SIO-Buchse im Atari-410-Programmrekorder verbaut, so dass er stets als letztes Glied in der Kette von Peripheriegeräten anzuschließen ist.\n\nZusammen mit dem Atari-410-Programmrekorder war kurz nach Markteinführung von Atari 400 und 800 auch ein auf Ataris SIO-Schnittstelle abgestimmtes Diskettenlaufwerk erhältlich, die Floppystation Atari 810. Mit dem Atari-810-Diskettenlaufwerk können 5¼″-Disketten einseitig in einfacher Schreibdichte mit 720 Sektoren à 128 Bytes beschrieben werden, womit sich pro Diskettenseite 90 KB Daten abspeichern lassen. Die mittlere Datenübertragungsrate beträgt etwa 6000 Bit/s, das Zehnfache dessen, was der Datenrekorder Atari 410 in derselben Zeit zu übertragen in der Lage ist. Während des gesamten Produktionszeitraumes wurden vom Hersteller an den Laufwerken mehrfach Änderungen vorgenommen. So existieren beispielsweise Ausführungen mit teilweise fehlerhafter Systemsoftware und solche mit verschiedenen Laufwerksmechaniken.\n\nNeben der Diskettenstation 810 war für kurze Zeit in Nordamerika ein weiteres Gerät in Form des wesentlich leistungsfähigeren Atari-815-Diskettenlaufwerks erhältlich. Es verfügt über zwei Laufwerksmechaniken, wobei jede zudem mit doppelter Schreibdichte operiert und so pro 5¼″-Diskettenseite 180 KB Daten gespeichert werden können. Aufgrund der damit verbundenen komplizierten Konstruktion war lediglich eine manuelle Herstellung möglich. Durch den daraus resultierenden hohen Preis von 1500 US-Dollar bei gleichzeitig großer Fehleranfälligkeit wurde das Gerät nach Auslieferung nur geringer Stückzahlen in Höhe von etwa 60 Exemplaren von Atari aus dem Sortiment genommen.\n\nAb Mitte 1982 erschien eine Vielzahl von Atari-kompatiblen Diskettenlaufwerken diverser Dritthersteller. Dazu zählen unterschiedlich leistungsstarke Geräte von Percom, Laufwerke mit zusätzlicher Datenspuranzeige von Rana und auch Doppellaufwerke von Astra.\n\nEtwa Mitte des Jahres 1982 stellte das US-amerikanische Unternehmen Corvus 5¼″-Festplattenmodelle mit Speicherkapazitäten von 5 bis 20 MB für den Atari 800 vor. Im Gegensatz zu Ataris Peripheriegeräten wie beispielsweise dem Diskettenlaufwerk 810 erfolgt der Anschluss nicht über die serielle Schnittstelle. Vielmehr werden zwei der vier Joystickbuchsen durch entsprechende Hard- und Software von Corvus für den Datenaustausch mit dem Festplattenlaufwerk zweckentfremdet. Durch die Verkettung von bis zu vier Corvus-Laufwerken kann eine maximale Speicherkapazität von 80 MB erreicht werden. Neben der deutlich erhöhten Speicherkapazität bieten die Festplatten im Vergleich zum Diskettenlaufwerk Atari 810 eine deutlich kürzere mittlere Zugriffszeit und eine wesentlich größere Verlässlichkeit, was ein effektiveres Arbeiten ermöglicht. Daneben erlaubt eine damals separat von Corvus vertriebene Erweiterung namens \"Corvus Multiplexer local network\" den gleichzeitigen Anschluss mehrerer Atari-800-Computer an ein und dieselbe Festplatte. Diese Netzwerkfähigkeit nutzten beispielsweise der computergestützte Unterricht in diversen Schulen und größere Mailboxen. Der Preis des günstigsten Corvus-Laufwerkes betrug zusammen mit der benötigten Ansteuerelektronik und Software bei Markteinführung 3195 US-Dollar.\n\nAufgrund der damals eingesetzten vielfältigen Kopierschutzmechanismen funktionierten nur die wenigsten Programme ohne zusätzliche Modifikationen zusammen mit den Festplatten von Corvus. Das 1983 von einem weiteren Drittanbieter vorgestellte \"Integrator board\" behob diese Schwierigkeiten und erlaubt zudem das Benutzen der Festplattenlaufwerke, ohne zuvor deren Ansteuerungssoftware von einem Diskettenlaufwerk laden zu müssen.\n\nDie Bildausgabe am Atari 800 kann an einem Monitor oder via eingebautem HF-Modulator an einem handelsüblichen Farb- oder Schwarz-Weiß-Fernsehgerät erfolgen.\n\nZur schriftlichen Fixierung von Text und Grafik dienen der Thermodrucker \"Atari 822\" und die nadelbasierten Modelle \"Atari 820\" und \"Atari 825.\" Drucker von Fremdherstellern können nur mithilfe von Zusatzgeräten betrieben werden, da der Atari 800 nicht über entsprechende Standardschnittstellen verfügt. Abhilfe lässt sich durch die Zwischenschaltung eines Atari-850-Schnittstellenmoduls schaffen, womit RS-232- und Centronics-Drucker von Epson, Mannesmann und weiteren betrieben werden können.\n\nDaneben existieren von Fremdherstellern eine Fülle von Ausgabezusätzen: Angefangen bei der zur Sprachausgabe gedachten \"The Voicebox\" von The Alien Group über eine selbstzubauende 3D-Brille zum Betrachten von stereografischen Inhalten am Fernseher bis hin zum programmierbaren Robotergreifarm werden alle damals interessierenden Teilbereiche abgedeckt.\n\nDie Schreibmaschinentastatur des Atari 800 enthält insgesamt 56 Einzeltasten, eine Leer- und vier Funktionstasten. Als Erweiterung zur Tastatur bot Atari einen externen Ziffernblock mit der Bezeichnung \"CX85\" zur vereinfachten Eingabe von Ziffern zum Gebrauch mit diversen Anwenderprogrammen wie beispielsweise Tabellenkalkulationen oder Buchhaltungsprogrammen an.\n\nSämtliche weitere Eingabegeräte werden wie der Ziffernblock auch an eine oder mehrere der vier an der Vorderseite des Computergehäuses vorhandenen Kontrollerbuchsen angeschlossen. Dazu zählen Joysticks verschiedenster Hersteller, Paddle-Controller, spezielle Kleintastaturen, der Trackball-Controller von TG Products und Grafiktabletts von Kurta Corporation und Koala Technologies Corp.\nDer Atari 800 wurde von vornherein als erweiterbares System konzipiert. Dazu steht ein leicht zugänglicher Erweiterungsschacht mit insgesamt vier Steckplätzen zur Verfügung, wobei einer der Steckplätze durch die Karte mit dem Betriebssystem ständig belegt ist. Die restlichen drei erlauben die Aufnahme von Speicheraufrüstungen oder 80-Zeichen-Karten. Die nachfolgenden Ausführungen beschränken sich auf die am häufigsten in zeitgenössischen Fachzeitschriften vorgestellten kommerziellen Produkte.\n\nMit dem anfänglich verbauten Arbeitsspeicher in Höhe von 8 KB war kaum mehr als Spielen möglich, denn bei der Benutzung von BASIC reicht der Speicherplatz nicht einmal für die Einbindung der höchstaufgelösten Grafikstufe. Wenn zum Laden und Abspeichern der erstellten BASIC-Programme ein Diskettenlaufwerk benutzt werden soll, wird mit den später ausgelieferten 16 KB RAM ebenfalls schnell die Kapazitätsgrenze erreicht. Ursächlich hierfür ist das speicherintensive Diskettenoperationssystem (DOS), das neben dem BASIC-Programm des Anwenders einen großen Teil des Arbeitsspeichers für sich beansprucht. Beim Atari 800 kann jedoch mithilfe der leicht zugänglichen Erweiterungsschächte und den von Atari bereitgestellten, mit maximal 16 KB RAM bestückten Karten problemlos auf komfortable 48 KB Arbeitsspeicher aufgerüstet werden.\n\nDer Nachteil des maximalen Speicherausbaus mit ausschließlich 16-KB-Steckkarten ist die damit verbundene vollständige Belegung des Erweiterungschachtes. Es stehen somit keine weiteren Steckplätze für beispielsweise 80-Zeichen-Karten zur Verfügung. Aus diesem Grunde brachten Anfang 1981 Dritthersteller wie Mosaic und Axlon erste 32-KB-RAM-Karten auf den Markt. Ende 1981 kamen Modelle hinzu, die mithilfe technischer Raffinessen (Speicherbankumschaltung) bis zu 128 KB Arbeitsspeicher bereitstellten. Diese RAM-Disk-Systeme emulieren ein oder mehrere Diskettenlaufwerke mit einer Datenübertragungsrate, die die des Atari-810-Diskettenlaufwerkes um das Zwanzigfache übersteigen können.\n\nFür eine übersichtlichere und weniger ermüdende Anzeige der Bildinhalte dienen die für den Atari 800 produzierten 80-Zeichen-Karten. Aufgrund der hohen horizontalen Auflösung von 560 Bildpunkten sind diese nicht zum Betrieb mit einem Fernseher geeignet, sondern erfordern entsprechende Computermonitore. Die Ende 1982 von der Firma Bit3 veröffentliche Karte \"Full-View 80\" wird im letzten der Erweiterungschächte platziert. Per Befehlsaufruf kann der 80-Zeichen-Modus aktiviert werden, wobei ANTIC und GTIA abgeschaltet werden und der auf der Steckkarte befindliche Grafikprozessor \"Synertek 6545A-1\" die Bilderzeugung übernimmt. Die entsprechende Software ist im Festwertspeicher der Steckkarte enthalten, im Gegensatz zu der später von Austin Franklin Associates herausgebrachten Erweiterung \"Austin-80 Video Processor.\" Deren Ansteuerungssoftware ist auf einem für den rechten Schacht bestimmten Steckmodul untergebracht.\n\nWie bei anderen Heimcomputern der 1980er Jahre auch erfolgte der Vertrieb kommerzieller Software auf verschiedenen Datenträgern. Die insbesondere bei Spieleherstellern beliebten preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes allerdings sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Zudem sind mit Datasetten bestimmte Betriebsarten wie die beispielsweise zum Betrieb von Datenbanken vorteilhafte relative Adressierung nicht möglich. Bei den in der Herstellung vielfach teureren Steckmodulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen von großem Vorteil war. Den besten Kompromiss zwischen Ladezeit, möglichen Betriebsarten, Verlässlichkeit und Speicherkapazität erzielten die Disketten, deren Verwendung bei Veröffentlichung des Atari 800 durch das 810-Diskettenlaufwerk unterstützt wurde.\n\nDie Programmpalette für den Atari-800-Computer umfasste neben der von Atari und APX vertriebenen Auswahl kommerzieller Programme auch von Drittherstellern entwickelte und in Zeitschriften und Büchern publizierte Software (Listings) zum Abtippen. Die kommerziellen Programme wurden auf Steckmodul, Diskette und Kassette angeboten.\n\nVon der in Umlauf befindlichen Software machten illegale Kopien („Raubkopien“) stets einen großen Teil aus und stellten damit kleinere Softwareentwickler häufig vor existentielle wirtschaftliche Schwierigkeiten. Daraufhin wurden zunehmend Kopierschutzsysteme insbesondere bei Spielen als der meistverkauften Software eingesetzt.\n\nDie Initialisierung und Konfiguration der Atari-800-Hardware fällt in den Aufgabenbereich des im Festwertspeicher untergebrachten \"Operating System (OS),\" des Betriebssystems. Nachdem zahlreiche Fehler bekannt geworden waren, veröffentlichte Atari mit \"OS-B\" im Jahr 1982 eine fehlerbereinigte Version. Die Unterprogramme des 10 KB umfassenden Betriebssystems steuern verschiedene Systemprozesse, die auch vom Benutzer angestoßen werden können. Dazu gehören die Durchführung von Ein- und Ausgabeoperationen wie etwa die Tastatur- und Joystickabfrage, Fließkommaberechnungen, die Abarbeitung von Systemprogrammen nach Unterbrechungen (Interrupts) und die Bereitstellung eines Bildschirmtreibers zum Erzeugen der verschiedenen Grafikmodi. Die Startadressen der einzelnen Unterprogramme sind in einer Sprungtabelle zusammengefasst, um die Kompatibilität mit späteren Betriebssystem-Revisionen oder neuen Versionen zu wahren. Zur Abgrenzung vom Betriebssystem der später erschienenen XL- und XE-Modelle wird das OS des Atari 400 häufig auch als \"Oldrunner\" bezeichnet.\n\nDie Bearbeitung benutzerspezifischer Aufgabenstellungen erfordert häufig speziell darauf zugeschnittene Softwarelösungen, die Anwendungsprogramme. Existieren diese nicht oder können sie aus technischen oder wirtschaftlichen Gründen nicht eingesetzt werden, kommen geeignete Programmiersprachen zum Einsatz. Insbesondere in den ersten Jahren nach Markteinführung des Atari 800 mussten viele Programme durch den Benutzer in Eigenregie erstellt werden.\n\nDie Erstellung zeitkritischer Actionspiele und beispielsweise Anwendungen in der Regelungstechnik erforderten Anfang der 1980er Jahre eine optimale Nutzung der Hardware. Im Heimcomputerbereich war dies ausschließlich durch die Verwendung von Assemblersprache mit entsprechenden Übersetzerprogrammen, den Assemblern, möglich. Die Auslieferung von Assemblern erfolgte in vielen Fällen mit einem zugehörigen Editor zur Eingabe der Programmanweisungen („Sourcecode“), häufig auch als Programmpaket mit Debugger und Disassembler zur Fehleranalyse. Im professionellen Entwicklerumfeld kamen vielfach Cross-Assembler zum Einsatz. Damit war es möglich, ausführbare Programme für Heimcomputer auf leistungsfähigeren und komfortabler zu bedienenden Fremdcomputerplattformen zu erzeugen.\n\nKurz nach Veröffentlichung der Atari-Computer war lediglich der auf Steckmodul ausgelieferte langsame \"Assembler Editor\" von Atari erhältlich. Er bot wenig Komfort und konnte daher nur für kleinere Projekte sinnvoll eingesetzt werden. Im Gegensatz zu anderen Assemblern erlaubte er jedoch das Abspeichern der erstellten Quelldateien und ausführbaren Programme auf Kassette, was insbesondere für viele Atari-800-Benutzer ohne Diskettenstation von Vorteil war und sie so über die Nachteile leicht hinwegsehen ließ. Die für professionelle Programmentwicklung benötigten Assembler standen erst später mit \"Synassembler\" (Synapse Software), \"Atari Macro Assembler\" (Atari), \"Macro Assembler Editor\" (Eastern Software House), \"Edit 6502\" (LJK Enterprises) und dem leistungsfähigen \"MAC 65\" (Optimized Systems Software) zur Verfügung.\n\nProgrammiereinsteiger zogen in vielen Fällen die übersichtlichen und einfach zu bedienenden, dafür aber weniger leistungsfähigen Programmier-Hochsprachen vor.\n\nDem von Atari veröffentlichten BASIC standen zwei weitere zur Seite: Das den damaligen Quasi-Standard bildende Microsoft BASIC und ein zum Atari BASIC abwärtskompatibles Produkt mit dem Namen \"BASIC A+\" von Optimized System Software. Insbesondere BASIC A+ enthält erweiterte Editiermöglichkeiten, Vereinfachungen in der Befehlsstruktur und es ergänzt viele im Atari- und Microsoft-BASIC nicht implementierte Leistungsmerkmale. Dazu zählt beispielsweise eine bequeme Benutzung der Sprites („Player-Missiles-Grafik“) durch eigens dafür bereitgestellte Befehlswörter. Im Gegensatz zum Atari 400 erlaubt der Atari 800 den gleichzeitigen Betrieb zweier, jeweils für die verschiedenen Schächte speziell ausgelegter Steckmodule. So kann beispielsweise mithilfe des Programms \"The Monkey Wrench II\" das Atari BASIC um verschiedene Befehle erweitert werden.\n\nNachteilig auf die Einsetzbarkeit von BASIC-Programmen wirkten sich die in der Natur des Interpreters liegenden prinzipiellen Beschränkungen wie etwa die geringe Ausführungsgeschwindigkeit und der große Arbeitsspeicherbedarf aus. Diese Nachteile können durch spezielle Programme, BASIC-Compiler, abgemildert werden. Dabei werden ausführbare Maschinenprogramme erzeugt, die ohne BASIC-Interpreter lauffähig sind und damit häufig eine schnellere Ausführung erlauben. Für das Atari BASIC stehen mit \"ABC BASIC Compiler\" (Monarch Systems), \"Datasoft BASIC Compiler\" (Datasoft) und \"BASM\" (Computer Alliance) verschiedene Compiler zur Verfügung.\n\nNeben der Programmiersprache BASIC in ihren verschiedenen Dialekten war mit Verkaufsstart des Atari 800 die Interpretersprache \"Logo\" erhältlich. Unterstützt durch Elemente wie die \"turtle graphics\" (Schildkrötengrafik) ist damit eine kindgerechte und interaktive Einführung in die Grundlagen der Programmierung möglich. Ähnlich gelagert in ihren Eigenschaften ist die später in den Handel gebrachte Programmiersprache \"Atari PILOT.\" Mit \"QS-Forth\" (Quality Software), \"Extended fig-Forth\" (APX) und \"Data-Soft Lisp\" (Datasoft) reihen sich weitere Programmiersprachen in die Produktpalette für den Atari 800 ein.\n\nAls Mittelweg zwischen Interpreter-Hochsprache (langsam in der Ausführung, aber gut lesbare Sourcecodes und einfache Fehleranalyse) und Assemblersprache (schwer zu erlernen und umständlich zu handhaben, aber Anfang der 1980er Jahre alternativlos zur Erzeugung schneller und speichereffizienter Programme) etablierten sich auch im Heimcomputerbereich im Laufe der 1980er Jahre die Compiler-Hochsprachen. Die Ausführungsgeschwindigkeit der damit erzeugten Maschinenprogramme war im Vergleich zu interpretierten Programmen wie beim eingebauten BASIC sehr viel größer, reichte aber nicht ganz an die von Assemblern erzielte heran. Die Geschwindigkeitsnachteile gegenüber assemblierten Programmen wurden jedoch vielfach zugunsten eines leichter zu wartenden Quelltextes in Kauf genommen.\n\nIm Laufe der Produktlebenszeit bis Ende 1983 war für die Atari-800-Anwender als Compilersprache lediglich APX Pascal erhältlich.\n\nDie Programmpalette für die Atari-Computer umfasst neben den Programmiersprachen zum Erstellen eigener Applikationen eine im Vergleich zum zeitgenössischen Konkurrenten Apple II lediglich kleine Auswahl an vorgefertigter kommerzieller Anwendungssoftware. Zu den bekanntesten Anwendungsprogrammen zählen \"VisiCalc\" (Visicorp, Tabellenkalkulation), \"The Home Accountant\" (Continental Software, Buchführung), \"Atari Writer\" (Atari, Textverarbeitung), \"Bank Street Writer\" (Broderbund, Textverarbeitung) und \"Letter Perfect\" (LJK Enterprises, Textverarbeitung).\n\nDaneben wurde der Atari 800 auch für Online-Anwendungen eingesetzt, wozu vor allem Banking mit der Pronto-Software und der Betrieb von Mailboxen durch diverse auch selbstgeschriebene Programme zu zählen ist. Darüber hinaus ermöglichte vermutlich eigenentwickelte Anwendungssoftware einen Einsatz als offiziellen Computer der Tennisorganisation ATP, im Logistikbereich des Flugzeugträgers USS \"Nimitz\", zur Erzeugung von Bühnenbildern für die deutsche Musikgruppe Kraftwerk und als Simulationscomputer zur Ausbildung von Mitarbeitern eines kalifornischen Meeresforschungsinstituts.\n\nEs existiert eine Vielzahl an Programmen, die dem computergestützten Vermitteln von Lehrinhalten und seiner anschließenden interaktiven Abfrage dienen. Das zu vermittelnde Wissen wird in spielerischer Form mit ständig steigendem Schwierigkeitsgrad präsentiert, um den Lernenden anhaltend zu motivieren. Dabei wird großer Wert auf eine altersgerechte Darbietung gelegt, die von Kleinkindern bis hin zu Studenten reicht. Bei den Jüngsten kommen häufig animierte Geschichten mit comicartigen Charakteren als begleitende Tutoren zum Einsatz, bei Jugendlichen werden abzufragende Lehrinhalte in Abenteuerspiele oder actionsreiche Weltraumabenteuer gekleidet, bei den höherstufigen Lehrinhalten für Studenten und Erwachsene überwiegt hingegen meist lexikalisch präsentiertes Wissen mit anschließender Abfrage nebst Erfolgsbilanzierung. Die von der Software abgedeckten Lerngebiete erstrecken sich auf Lesen und Schreiben, Fremdsprachen, Mathematik, Technik, Musik, Geographie, Demografie, Tippschulen und Informatik. Zu den bekanntesten Herstellern zählen Atari, APX, Dorsett Educational Systems, Edufun, PDI und Spinnaker Software.\n\nDen mit Abstand größten Teil der sowohl kommerziellen als auch frei erhältlichen Atari-Software stellen die Spiele dar. Zu den frühen Shoot-’em-up-Spielen wie etwa \"Star Raiders\" oder der Brettspieleumsetzung \"3-D Tic-Tac-Toe\" kamen bereits ein Jahr später weitere Actionspiele, Adventures und Arcade-Umsetzungen hinzu. Sowohl professionelle Hersteller als auch Hobbyprogrammierer profitierten dabei von der Veröffentlichung technischer Dokumentationen seitens Atari, den Programmieranleitungen in den Computermagazinen und -büchern sowie von den mittlerweile aufgekommenen leistungsfähigen Entwicklungswerkzeugen. Unter den publizierten Titeln befanden sich jedoch auch viele schlechte Portierungen von beispielsweise Apple-II-Spielen ohne den unverwechselbaren „Atari-Look“, nämlich eine Mischung verschiedener „farbenprächtiger“ und weichverschobener Grafiken, ergänzt um die typische POKEY-Musik nebst Geräuscheffekten.\n\nUnter den für die Atari-Computer veröffentlichten Spielen befinden sich viele, die bereits in den frühen 1980er-Jahren als Videospieleklassiker galten: \"Star Raiders\" (vermutlich 1979), \"Asteroids\" (1981) und \"Pac-Man\" (1982). Insbesondere das 3D-Spiel \"Star Raiders\" galt vielen Spieledesignern der damaligen Zeit als prägendes Erlebnis und Grund, sich für einen Atari-Computer und nicht etwa einen Apple II oder Commodore PET zu entscheiden. In der Folge entstandene Werke wie \"Miner 2049er\" (Bill Hogue, Big Five Software, 1982), \"Eastern Front (1941)\" (Chris Crawford, APX, 1982), \"Capture the Flag\" (Paul Edelstein, Sirius Software, 1983), \"Archon\" (John Freemann, Electronic Arts, 1983) und \"M.U.L.E.\" (Daniel Bunten, Electronic Arts, 1983) zählen zu den herausragenden Titeln ihrer Zeit und ermöglichten Softwarehäusern wie beispielsweise Microprose und Electronic Arts den raschen Aufstieg zu Branchenriesen.\n\nZu den beliebtesten Spielen für die Atari-Computer gehören neben den Infocom-Abenteuern großteils Shoot-’em-up-Spiele wie \"Crossfire\" (Sierra On-Line, 1981) und \"Blue Max\" (Synapse Software, 1983), Rennspiele wie \"Pole Position\" (Atari, 1983), Kriegssimulationen wie \"Combat Leader\" (SSI, 1983), aber auch Grafik-Adventures wie \"Excalibur\" (APX, 1983) und \"Murder on the Zinderneuf\" (Electronic Arts, 1983).\n\nIn den 1980er Jahren spielten neben den Fachbüchern die Computerzeitschriften für viele Heimcomputerbesitzer eine große Rolle. Die häufig monatlich erschienenen Ausgaben enthielten Testberichte zu Neuheiten, Programmieranleitungen und Software zum Abtippen. Sie dienten weiterhin als Werbe- und Informationsplattform sowie zur Kontaktaufnahme mit Gleichgesinnten.\n\nSpeziell mit den Atari-Heimcomputern befassten sich die englischsprachigen Magazine \"Antic, Analog Computing, Atari Connection\" und \"Atari Age;\" gelegentliche Berichte und Programme für die Atari-Rechner veröffentlichten unter anderem auch die auflagenstarken \"Byte Magazine, Compute!\" und \"Creative Computing.\" Während der Atari 800 in Deutschland verkauft wurde, waren Informationen und Programme unter anderem in den Zeitschriften \"Chip, Happy Computer, P.M. Computermagazin, Computer Persönlich\" und \"Mein Home-Computer\" zu finden.\n\nNach dem Ende der Heimcomputerära Anfang der 1990er Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Ende der 1990er Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripheriegeräten entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reichte mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit u. a. ein verstärktes Transferieren von sonst möglicherweise verloren gegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird.\n\nAls leistungsfähigste Emulatoren für Windows- und Linux-Systeme gelten \"Atari++, Atari800Win Plus, Mess32\" und \"Altirra.\"\n\nDas Erscheinen des Atari 400 und 800 wurde durchweg positiv aufgenommen. Die auflagenstarke Zeitschrift \"Compute!\" schrieb von einer neuen Generation von Computern:\n\nVon denselben Rezensenten wird zudem ausgeführt, dass die Einordnung der neuen Geräte am ehesten mit der eines Hybriden zwischen Videospiel und Computer zu umschreiben sei. Sie enthielten das Beste beider Welten, was sie damit zu einem Personalcomputer und Heimgerät gleichermaßen mache. Diese Eigenschaften prädestinierten den Atari 800 geradezu für Lern- und Unterhaltungszwecke. Da die beste Hardware ohne entsprechende Software zu ihrem Gebrauch jedoch nutzlos sei, habe Atari aus den Fehlern der Konkurrenz gelernt und dem Benutzer mit der Programmiersprache Atari BASIC einen ausgesprochen leichten Zugang zu den farbenprächtigen Grafik- und Toneigenschaften seiner Geräte zur Seite gestellt. Diese Vermarktung von aufeinander abgestimmter Hard- und Software – auch beim direkt auf die Atari-8-Bit-Computer zugeschnittenen äußerst populären Spiel \"Star Raiders\" – stelle ein Novum dar.\n\nDurch das modulare Konzept wären jedoch mehr Anschlusskabel als etwa beim kompakten Commodore PET vonnöten, was unter Umständen von Nachteil sein könne ebenso wie das nicht-validierende Abspeichern von Programmen auf Kassette. Ab Sommer 1980 wurden vor allem Lieferschwierigkeiten und das Ausbleiben von anwendungsorientierter Software bemängelt und den Rechnern von Adam Osborne keine große Zukunft vorausgesagt.\n\nAls sich die Atari-Computer entgegen den Voraussagen Osbornes dennoch etablieren konnten und sogar zum Marktführer aufgestiegen waren, wurden von der Fachpresse weiterhin Empfehlungen hauptsächlich für preisbewusste Haushalte ausgesprochen:\n\nÜbereinstimmend mit der Fachpresse sahen auch Spieleautoren wie David Fox (Programmierer bei Lucasfilm-Games) und Scott Adams (Gründer von Adventure International) in den Ataris die grafisch und tontechnisch leistungsfähigsten Geräte des gesamten Heimcomputermarktes:\n\nIm Laufe der Zeit geriet Ataris Vermarktungskonzept aber auch in die Kritik, da die Fähigkeiten als Anwendungscomputer nicht klar genug herausgestellt und unterstützt würden. Obwohl die Atari-Computer seit ihrer Einführung einen guten Ruf auch als leistungsfähige Personal Computer genossen hätten, sei spätestens mit der Produktionseinstellung des leistungsfähigen Diskettenlaufwerks Atari 815 der Einsatzschwerpunkt der Geräte auf den Heimbereich mit besonderem Augenmerk auf den Unterhaltungs- und Bildungssektor verschoben worden. Dazu kämen Fehler bei der Wahl der Vertriebswege. Die Verlagerung des Verkaufs durch große Ladenketten hätte kleinere Fachgeschäfte mit entsprechender Kompetenz und Serviceleistungen bewogen, mangels Konkurrenzfähigkeit die Atari-Rechner aus dem Angebot zu nehmen. Damit wäre ein weiteres wichtiges Standbein zur Versorgung der Rechner mit leistungsfähiger Anwendungssoftware entfallen, so dass auch der Atari 800 letztlich nur noch als reine Spielekonsole wahrgenommen und gekauft wurde.\n\nKurz nach seinem Erscheinen in Deutschland wurde der Atari 800 vom damals auflagenstärksten Computermagazin \"Chip\" als Gerät für den fortgeschrittenen Anwender charakterisiert, „der neben seiner Hobbyanwendung auch den professionellen Bereich bei seiner Kaufentscheidung zugrundelegt.“ Positiv hervorgehoben wurden zudem die stabile Geräteausführung, die grafischen Möglichkeiten, die Farbausgabe, eine ausführliche Dokumentation, die bereits vorhandene große Programmbibliothek nebst verschiedenen Programmiersprachen wie \"Atari PILOT\" und \"Atari Assembler.\"\n\nBereits kurz nach der Ablösung durch die technisch kaum veränderten Nachfolgemodelle 600XL und 800XL wird dem Atari 800 eine exzellente Konstruktion bescheinigt, die einen neuen Standard auf dem Heimcomputermarkt gesetzt habe. Die phantastische Grafik spiegele sich vor allem in den guten Spielen wider, einer der Stärken des Atari 800. Einer der wenigen Kritikpunkte bildete nach Meinung von Michael S. Tomczyk und Dietmar Eirich der bei Einführung zu hohe Preis:\n\nRückblickend verstand es Atari laut Bill Loguidice und Matt Barton erstmals, die Eigenschaften einer reinen Spielemaschine mit den Fähigkeiten damaliger Heimcomputer bei gleichzeitig leichter Bedienbarkeit zu kombinieren. Als einer der Hauptgründe für das Gelingen dieser anspruchsvollen Aufgabe gelten den beiden Autoren die in die Entwicklung einfließenden Erfahrungen der bereits am Bau der erfolgreichen VCS-2600-Spielekonsole beteiligten Atari-Ingenieure. Als Ergebnis waren erstmals in einem Heimcomputer elektronische Spezialbausteine zur Entlastung des Hauptprozessors zur Anwendung gekommen. Deren grafische Raffinessen in Form von beispielsweise der Player/Missile-Grafik seien wegweisend für spätere Geräte gewesen. Auch die Soundeigenschaften hätten durch Verwendung eines Spezialbausteins zur damals obersten Qualitätskategorie gehört und der Atari 400 habe den Apple II damit als besten Spiele-Computer abgelöst.\n\nAls entscheidenden Grund für die innerhalb kürzester Zeit ansteigende Popularität der Atari-Computer sehen die Autoren der Internetplattform Gamasutra die Veröffentlichung des Spiels \"Star Raiders:\"\n\nFür den permanenten Mangel an leistungsfähiger Anwendungssoftware macht Tomczyk Ataris ursprüngliche und umstrittene Praktiken bezüglich der Veröffentlichung technischer Dokumentationen verantwortlich:\n\nEine spätere Änderung der restriktiven Informationspolitik hätte den bereits entstandenen Rückstand nicht mehr aufholen helfen können. So seien mit fortschreitender Zeit hauptsächlich Spiele für die Atari-Heimcomputer erschienen, womit diese nun mehr und mehr als reine Spielemaschinen wahrgenommen wurden:\n\nDurch die damit von Atari selbstgeschaffene Konkurrenz zur hauseigenen Spielekonsole VCS 2600 und hauptsächlich infolge aufkommender Konkurrenz durch Texas Instruments und Commodore mit ihren umfangreichen Programmbibliotheken im Anwendungsbereich hätten die Verkaufserfolge nicht weitergeführt werden können. Entscheidende Marktanteile wären damit ab 1983 wieder dem Apple II und vor allem dem neu erschienenen Commodore 64 zugefallen.\n\n\n"}
{"id": "32284", "url": "https://de.wikipedia.org/wiki?curid=32284", "title": "Atari 400", "text": "Atari 400\n\nDer Atari 400 ist ein auf dem 6502-Mikroprozessor basierender Heimcomputer des US-amerikanischen Herstellers Atari, Inc.\n\nDer Atari 400 wurde ab Ende 1979 in Nordamerika zunächst nur im Versandhandel angeboten und massiv als preiswertes Einsteigergerät zum Spielen und Lernen beworben. Durch diverse von Atari angestoßene Kooperationen im Bildungssektor, die Veröffentlichung von Spiele-Kassenschlagern wie \"Star Raiders\" und den Ausbau des Atari-Händlernetzes gelang es, die Bekanntheit kontinuierlich zu steigern. Verkaufsfördernd hinzu kam die ab Mitte 1981 vollzogene Expansion nach Europa, die schließlich in der bis Ende 1982 währenden internationalen Marktführerschaft Ataris gipfelte.\n\nDurch den Misserfolg seines Anfang 1983 parallel eingeführten Computermodells Atari 1200XL und den seinen Höhepunkt erreichenden Preiskrieg mit anderen Herstellern verlor Atari binnen eines Jahres wieder viele seiner Marktanteile hauptsächlich an Commodore. Etwa zeitgleich mit Ankündigung des Nachfolgemodells Atari 600XL stellte man Mitte 1983 die Produktion des Atari 400 ein. Bis etwa Anfang 1985 währende Lagerverkäufe miteingerechnet, wurden von den beiden Computermodellen Atari 400 und 800 insgesamt etwa zwei Millionen Einheiten verkauft.\n\nBereits kurz nach der Markteinführung galt der Atari 400 als wegweisend in der Heimcomputergeschichte: Er habe durch seine auf Benutzerfreundlichkeit ausgelegte Konstruktion und die robuste Verarbeitung auch völlig unerfahrenen Benutzern einen leichten Einstieg in die bis dahin eher Spezialisten vorbehaltene Computertechnik eröffnet.\n\nNoch während der letzten Entwicklungsphase für die Videospielekonsole Atari 2600 begann Atari Anfang 1977 mit den Planungsarbeiten für ein Nachfolgemodell. Die Bemühungen der Ingenieure konzentrierten sich dabei hauptsächlich auf die Erweiterung der Grafikfähigkeiten des im Atari 2600 verbauten hochintegrierten Spezialschaltkreises \"Television Interface Adapter\" \"(TIA)\". Die Verbesserungen versprachen komplexere und grafisch ausgereiftere Spiele bei gleichzeitig verringertem Entwicklungsaufwand.\n\nEin früher, noch handverdrahteter Prototyp des \"Alphanumeric Television Interface Controller\" \"(ANTIC)\" wurde der Firmenleitung von Atari kurz darauf vorgestellt. Anschließende Machbarkeitsstudien zu möglichen Kombinationen des neuen Spezialbausteins mit weiteren elektronischen Baugruppen zeigten rasch über den Einsatz in einer reinen Spielkonsole hinausgehende Potentiale auf. So schien eine integrierte Tastatur für Programmierzwecke und die Ansteuerung externer Geräte beispielsweise zum Datentransfer sowohl technisch als auch ökonomisch möglich.\n\nEin modularer Aufbau und die Fähigkeit zur Programmierung waren zum damaligen Zeitpunkt lediglich den in Wirtschaft und Forschung eingesetzten teuren Computern von IBM oder DEC und mit deutlichen Abstrichen den wesentlich günstigeren Heimcomputern wie Altair 8800, TRS-80, PET 2001 und Apple II vorbehalten. Insbesondere letztere krankten jedoch an der Umständlichkeit ihrer Bedienung, der Unzuverlässigkeit der Technik und im Vergleich zu Spielkonsolen der damals neuesten Generation immer noch an der Höhe der Anschaffungskosten. Technisch wenig versierte, jedoch elektronischer Datenverarbeitung gegenüber aufgeschlossene Interessengruppen mit schmalem Geldbeutel blieben so außen vor. Diese Zielgruppe im Auge, verwarfen die Verantwortlichen von Atari rasch die ursprünglichen Pläne für eine auf dem ANTIC basierende neue Spielekonsole zugunsten eines eigenen, preisgünstigen und konzeptionell neuartigen Heimcomputers. Die Benutzung hatte einfach und sicher auch für Anfänger zu sein und das Gerät musste ohne technische Detailkenntnisse des Anwenders mit handelsüblichen Fernsehern betrieben werden können. Daneben sollte die Möglichkeit zum schnellen und bequemen Laden von Spielen und Anwendungsprogrammen ähnlich den von Spielekonsolen bekannten Steckmodulen vorhanden sein.\n\nNeben der angestrebten leichten Bedienbarkeit spielten insbesondere niedrige Herstellungskosten des zu entwickelnden Gerätes eine große Rolle; die zunächst geforderte Kompatibilität mit Spielen der Atari-VCS-2600-Konsole verwarfen die Verantwortlichen bereits nach kurzer Zeit. Die daraufhin von den Hauptentwicklern vorgelegten technischen Eckpunkte des neuen Systems wurden von der Firmenleitung im August 1977 für gut befunden und weitere finanzielle Mittel auch zur Aufstockung des Entwicklungspersonals zur Verfügung gestellt. Damit einhergehend erhielt das Heimcomputerprojekt den firmeninternen Codenamen \"Colleen\".\n\nMit fortschreitendem Stand der Arbeiten entschieden sich die Verantwortlichen, die Entwicklung zweier unterschiedlicher Ausbaustufen des Heimcomputers zu verfolgen: ein anwendungsorientiertes Gerät mit Schreibmaschinentastatur und Möglichkeiten zur Erweiterung und eine stark abgerüstete Variante hauptsächlich für Zwecke der Unterhaltung. Die Entwicklungsarbeiten für die zweite Variante, die \"Entertainment Machine\", wurde im November in ein separates Projekt mit dem Namen \"Candy\" ausgegliedert.\n\nErste Entwürfe beschränkten die Technik des Geräts auf das Ausführen von steckmodulbasierten Spielen mithilfe von Joysticks und griffen damit die ursprüngliche Idee einer reinen Spielekonsole wieder auf. Eine Realisierungsstudie vom November 1977 sah keinerlei Schnittstellen für Peripheriegeräte und Erweiterungsmöglichkeiten wie beim Projekt Colleen vor. Selbst der Einbau einer Tastatur wurde zunächst in Frage gestellt. Erst als letzte Arbeiten am ANTIC bis Januar 1978 abgeschlossen worden waren und sich die weiteren Bemühungen auf die Fertigstellung der Spezialbausteine \"Color Television Interface Adapter\" \"(CTIA)\" und \"Potentiometer and Keyboard Integrated Circuit\" \"(POKEY)\" konzentrierten, einigte man sich schließlich auf die Integration einer seriellen Schnittstelle für externe Peripherie sowie einer Tastatur. Der Einbau dieser Bestandteile war unter anderem der Bedienbarkeit komplexer Spiele und dem zwischenzeitlich angepeilten zusätzlichen Einsatzgebiet als Lerncomputer geschuldet. Insbesondere in Hinblick auf die dabei anvisierte Zielgruppe der Kinder sollte die interne Tastatur spritzwassergeschützt und ohne verschluckbare Tasten sein – eine zudem günstige Folienflachtastatur erwies sich dabei als Mittel der Wahl. Letzte Arbeiten an den Spezialbausteinen, den \"Custom Chips\", und deren Abstimmung auf den zwischenzeitlich ausgewählten Hauptprozessor \"6502\" von MOS zogen sich bis Ende März hin. Die gesamten Entwicklungskosten beliefen sich auf mehr als zehn Millionen US-Dollar.\n\nParallel zu den noch verbliebenen Arbeiten am Gehäuse begann die Sondierung des Marktes für höhere Programmiersprachen. Die Verantwortlichen entschieden sich dabei für BASIC, eine einsteigerfreundliche Sprache, mit der das neue Computersystem durch den Benutzer für eigene Zwecke programmiert und eingesetzt werden kann. Eine Eigenentwicklung durch Atari schied wegen fehlender Kapazitäten bei einer nur kurz zur Verfügung stehenden Frist von sechs Monaten aus. Nachdem der Einsatz des damals marktbeherrschenden Microsoft BASIC an den Atari-Erfordernissen zur Integration in einem Steckmodul mit nur 8 KB ROM gescheitert war, wurde am 6. Oktober 1978 die externe Firma Shepardson Microsystems mit der Erstellung eines eigenen, speziell auf die Atari Computer zugeschnittenen BASIC-Dialektes betraut.\n\nNach Festsetzung der Konfiguration des Arbeitsspeichers auf marktübliche 4 KB änderte Atari im November 1978 den inoffiziellen Namen \"Candy\" in den direkt an die Speichergröße angelehnten offiziellen Produktnamen \"Atari 400\". Die der Ziffer \"4\" nachgestellte Doppelnull klassifiziert dabei den Computer als Basisgerät der ihm zugehörigen Peripheriegeräte. Kurz darauf, am 6. Dezember 1978, erfolgte die Verkündung des Heimcomputerprojektes mit seinen beiden Geräten Atari 400 und Atari 800 publikumswirksam in einem Artikel der auflagenstarken \"New York Times\".\n\nEinen ersten Blick auf seine neue, teilweise noch unfertige Produktlinie gewährte Atari Interessenten erstmals im Januar 1979 auf der Winter CES in Las Vegas. Der Atari 400 war dabei als Prototyp mit provisorischem Gehäuse zusammen mit dem dazu passenden Programmrekorder Atari 410 zu sehen. Ein serienreifer Atari 400 war kurz darauf im Mai im Rahmen der \"4th West Coast Computer Faire\" in San Francisco einem größeren Publikum zugänglich. Auf der Summer CES in Chicago schließlich gab man die unverbindliche Preisempfehlung in Höhe von 550 US-Dollar bekannt.\n\nIm Juni wurden letzte technische Arbeiten abgeschlossen und der Abnahmetest zur elektromagnetischen Verträglichkeit durch die US-amerikanische Federal Communications Commission erfolgreich absolviert – eine maßgebliche Voraussetzung zur Verkaufbarkeit des Gerätes in Nordamerika. Kurz darauf ermöglichten allgemein gesunkene Bauteilekosten eine Vergrößerung des ab Werk verbauten Arbeitsspeichers von 4 auf 8 KB, wovon der Produktname des Geräts jedoch unberührt blieb. Die Fertigung der Computer, deren Entwicklung bislang etwa 100 Millionen US-Dollar gekostet hatte, wurde Ataris Fabrik im kalifornischen Sunnyvale übertragen. Die Produktion konnte jedoch erst im Oktober 1979 aufgenommen werden, da die rasch wachsende Heimcomputerbranche ab Spätsommer 1979 unter einer anhaltenden Teileknappheit litt.\n\nBereits geraume Zeit vor dem Verkaufsstart pries der Hersteller seinen Atari 400 als Allzweckgerät („general purpose home computer“) insbesondere für die junge und finanziell weniger gut aufgestellte Einsteigergeneration an, da keinerlei Computerkenntnisse oder sonstiges technisches Vorwissen notwendig sei („The affordable home computer that’s easy to use even for people who’ve never used a computer before“).\n\n Die erste Serie von Geräten wurde ab November 1979 im Rahmen einer Testvermarktung sowohl in der Weihnachtsausgabe des Versandkatalogs als auch in den Fotoabteilungen einiger Ladengeschäfte der Handelskette Sears Roebuck angeboten. Neben dem Computer mit Netzteil, Anschluss- und Anleitungsmaterial erhielt der Käufer für 549,99 US-Dollar die Programmiersprache BASIC in Form eines Steckmoduls nebst Programmierhandbuch.\n\nKurz nach dem Verkaufsstart begann Atari seine Geräte und dazugehörige Unterhaltungssoftware wie das Spiel \"Star Raiders\" auf Fachmessen vorzustellen. Neben allgemeiner Produktwerbung gelang es damit, auch neue Vertriebskanäle zu erschließen. Begleitet wurden die Präsentationen ab dem zweiten Quartal 1980 durch weitere umfangreiche und langfristig geplante Werbeoffensiven. Ab Mitte 1980 war die Bekanntheit der Atari-Computer so gestiegen, dass auch Dritthersteller vielversprechende Absatzpotentiale sowohl für Hard- als auch Software sahen und ihrerseits Produkte auf den Markt brachten.\n\nErgänzend zur Herstellung und zum Vertrieb von Unterhaltungssoftware verstärkte Atari die Bemühungen zur Platzierung seiner Heimcomputer in nordamerikanischen Bildungseinrichtungen, einem bislang von Apple II und Commodore PET dominierten Bereich. Dem lag das Kalkül zugrunde, dass Schüler und Studenten im Rahmen von späteren Privatanschaffungen auf das bereits aus der Schule Bekannte und Vertraute – einen Atari-Computer – zurückgreifen würden. Neben speziellen Verkaufskonditionen für das Bildungswesen war mit der Programmreihe \"Talk & Teach Cassette Courseware\" bereits frühzeitig auch die passende Software aufgelegt worden. Zudem setzte Atari ab Mitte 1980 verstärkt auf die Zusammenarbeit mit der zu IBM gehörigen Organisation \"Science Research Associates\", die sich der Förderung des computergestützten Unterrichts verschrieben hatte und den Vertrieb für Atari im Bildungssektor übernahm. Im Rahmen dieser Kooperation finanzierte IBM einen Rabatt, der Bildungseinrichtungen von der Grundschule bis hin zur Universität beim Kauf eines Atari-800-Computers einen zusätzlichen kostenfreien Atari 400 gewährte. Atari selbst legte für Schulen wenig später eine ähnliche Preisaktion in Form des \"3 for 2 deal\" auf: Beim Kauf zweier Atari-800- oder Atari-400-Computer erhielt der Käufer einen weiteren Atari 400 gratis dazu.\n\nDie für die Jahre 1979 und 1980 angegebenen Verkaufszahlen für die Modelle Atari 400 und Atari 800 zusammengenommen schwanken zwischen 50.000 und 300.000 Geräten. Die Umsätze allein für 1980 beliefen sich auf etwa 20 Millionen US-Dollar.\n\nNach einer zwischenzeitlichen Preiserhöhung auf 629,95 US-Dollar senkte Atari im Rahmen seiner mittlerweile offensiv ausgerichteten Vermarktungskampagne Anfang 1981 den Preis für den Atari 400 mit 8 KB RAM auf 499,95 US-Dollar. Neben dem stets währenden Kampf um Marktanteile mit den direkten Konkurrenzmodellen Apple II+ (16 KB RAM, 1195 US-Dollar), Tandy Color Computer (Versionen mit 4 und 16 KB RAM für 399 bzw. 599 US-Dollar) und Texas Instruments TI-99/4 (16 KB RAM, 1150 US-Dollar) war hierfür insbesondere der aufkommende günstige Commodore VIC 20 (5 KB RAM, 299 US-Dollar) ursächlich. Gleichzeitig fand eine erweiterte Variante des Atari 400 mit 16 KB RAM ab Werk für 630 US-Dollar Aufnahme ins Verkaufsprogramm, um mit den höherwertigen Konkurrenzmodellen in puncto Arbeitsspeicherausstattung gleichzuziehen. Da der Atari 400, im Gegensatz zum Atari 800, über keine dem Benutzer zugänglichen Erweiterungsschächte verfügt und daher zur nachträglichen Aufrüstung vollständig zerlegt werden muss, ermöglichte Atari den Käufern der alten Version eine Aufrüstung ihrer Geräte in zertifizierten Fachwerkstätten.\n\nBereits im Laufe des ersten Halbjahres 1981 konnten sich die Atari-Computer trotz permanenter Lieferschwierigkeiten und einiger technischer Probleme bei Zubehörteilen als feste Größen auf dem bislang hauptsächlich von Tandy, Apple und Commodore beherrschten Heimcomputermarkt etablieren. Die von Ataris Computersparte erzielten Umsätze lagen Mitte des Jahres 1981 bei zehn Millionen US-Dollar – die Summe der durch die laufende Produktion verursachten Verluste belief sich jedoch auf einen ähnlich hohen Betrag. Zur Bewältigung der zunehmenden Nachfrage und zur zügigen Umsetzung der geplanten weltweiten Vermarktung nahm Atari im April personelle Erweiterungen im Firmenmanagement vor und änderte daraufhin im Mai 1981 auch seine Vermarktungsstrategie: Der bislang ausschließlich zusammen mit BASIC und Anleitungsbuch verkaufte Atari 400 war ab sofort nur noch einzeln, dafür aber zum deutlich geringeren Preis von 399 US-Dollar erhältlich. Ergänzt werden konnte das Grundgerät vom Käufer unter anderem mit Ataris neu eingeführten, individuell auswählbaren und speziell auf Techniklaien zugeschnittenen Erweiterungspaketen. Diese „Starter Kits“ enthielten jeweils aufeinander abgestimmte, anschlussfertige Hard- und Software für die Einsatzbereiche Programmieren \"(Atari Programmer)\", Unterhaltung \"(Atari Entertainer)\", Bildung \"(Atari Educator)\" und Datenfernübertragung \"(Atari Communicator)\". Bis August 1981 war es gelungen, den Umsatz auf 13 Millionen US-Dollar zu steigern. Damit konnte erstmals die Gewinnzone erreicht werden.\n\nAußer in den Ausbau des Hardwaresektors investierte Atari auch in die Fortbildung seines Kundendienstes und der Vertragshändler sowie in die Softwareunterstützung für die Heimcomputer. Dazu zählten die beinahe monatlich erfolgenden Veröffentlichungen neuer hauseigener Programme und Spiele, die von Drittherstellern langerwartete Publikation technischer Dokumentationen und die Unterstützung unabhängiger Programmautoren. Letzteres umfasste die Ausrichtung von offenen Programmierwettbewerben mit entsprechend hoch dotierten Preisen, technische Schulungen in Ataris \"Acquisition Centers\" und die Gründung der Publikationsplattform \"Atari Program Exchange\" (Kürzel \"APX\"). Durch die Gründung von APX ermöglichte Atari den betriebswirtschaftlich häufig gänzlich unerfahrenen Softwareherstellern den Vertrieb ihrer Programme durch das mittlerweile in Nordamerika voll ausgebaute Atari-Händlernetz.\n\nIm Fahrwasser der amerikanischen Verkaufserfolge startete Atari im Sommer 1981 die Erschließung des lukrativen europäischen Marktes. Wie in den USA auch wurde die Veröffentlichung in Großbritannien (345 £), Frankreich, Italien (985.000 ₤) und den Benelux-Staaten von umfangreichen Werbemaßnahmen im Printbereich und von Präsentationen auf speziellen Ausstellungen begleitet.\n\nIn Westdeutschland übernahm ab August 1981 die bereits seit 1980 für die Atari-2600-Vermarktung zuständige \"Atari Elektronik Vertriebsgesellschaft mbH\" den Vertrieb und den Kundendienst. Die Vermarktung der „Privatcomputer“, so die offizielle Bezeichnung von Atari Deutschland, erforderte erhebliche Investitionen insbesondere für die Werbung, Verkäuferschulungen und Serviceaktivitäten. Analog den Promotionsbemühungen im Videospielebereich schaltete Atari entsprechende Werbung in Printmedien. Neben dem Verkauf im Versandhandel und in Fachgeschäften waren die Rechner auch in größeren Kaufhausketten wie Horten und Karstadt erhältlich. Die unverbindliche Preisempfehlung des Atari 400 lag bei 1495 DM, der Datenrekorder Atari 410 kostete 289 DM und das BASIC-Steckmodul konnte für 272 DM erworben werden.\n\nWährend der internationalen Expansionsphase reagierte Atari auf die sich immer weiter zuspitzende Konkurrenzsituation vor allem in Nordamerika u. a. mit technischen Überarbeitungen seiner Computer in Form eines revisionierten Betriebssystems für Neugeräte (OS-Version B) und einer fehlerbereinigten Version der BASIC-Programmiersprache. Im Geschäftsjahr 1981 konnte Atari so nach eigenen Angaben etwa 300.000 Heimcomputer absetzen, womit sich Atari 400 und 800 endgültig als Massenware etabliert hatten und Atari zum US-amerikanischen Marktführer aufsteigen ließen.\n\nDie Einführung diverser Billigcomputer wie Sinclair ZX81 brachte auch Atari in Zugzwang. Das Unternehmen senkte den Preis für den Atari 400 zunächst im Juni 1982 um 50 US-Dollar und schon im Juli folgte ein weiterer Nachlass des unverbindlichen Verkaufspreises auf 299 US-Dollar. Auch in Westdeutschland führte die aggressive Preispolitik von Commodore zu einer ersten, aber drastischen Senkung des unverbindlichen Verkaufspreises von 1495 auf 995 DM durch Atari Deutschland im August 1982.\n\nAb Frühherbst 1982 sah Atari von weiteren direkten Preisnachlässen ab und schwenkte vielmehr auf kaufbegleitende Rabattaktionen um: Beim Erwerb von Atari Hard- und Software wurden den Käufern durch „Softwarecoupons“ Ersparnisse von bis zu 60 US-Dollar auf Produkte aus Ataris Programmsortiment ermöglicht. Daneben baute Atari im Laufe des Jahres 1982 vor allem in Nordamerika den Kundendienst massiv aus. Die in den USA landesweit eingerichteten \"Atari Service Center\" übernahmen fortan Beratungs- und Reparaturdienstleistungen, aber auch die Umrüstung älterer Computer auf den neuen GTIA-Grafikbaustein und das revisionierte Betriebssystem. Sie ermöglichten zudem die durch Ataris Firmenleitung angestrebten profitträchtigen Verkäufe durch große Handelsketten wie J.C. Penney, K-Mart und Toys “R” Us, die aufgrund fehlenden qualifizierten Personals keinerlei Beratung oder Garantiedienstleistungen anzubieten in der Lage waren. Diese mittlerweile hauptsächlich auf Massenvermarktung ausgerichtete Verkaufspolitik bescherte Atari im Laufe des Jahres 1982 annähernd 600.000 Heimcomputerverkäufe, wovon auf den Atari 400 allein etwa 400.000 Einheiten entfielen. Mit insgesamt etwa 1,2 Millionen verkauften Geräten der Modelle 400 und 800 gelang es Atari, seine Marktführerschaft erfolgreich zu verteidigen.\n\nTrotz Ataris weltmarktbeherrschender Stellung konnten in Westdeutschland im Laufe des Jahres 1982 nur etwa 6000 Atari-400-Computer verkauft werden – Commodore setzte im selben Zeitraum rund 10.000 VC-20-Heimcomputer ab. Die Verantwortlichen bei Atari Deutschland legten daraufhin ab Mitte 1983 eine verkaufsfördernde Maßnahme in Form des Bündelangebots \"Computer Compact Paket\" auf. Damit erwarb der Käufer neben dem Computer ein BASIC-Modul, den Atari 410 Datenrekorder, einen von Dagmar Berghoff eingesprochenen Programmierkurs sowie eine Spielesammlung auf Kassette für insgesamt unter 1000 DM. Aufgrund des permanenten Preisdrucks amortisierten sich die hohen Investitionen von Atari Deutschland jedoch nur schleppend und die Heimcomputersparte entwickelte sich allmählich zum ungeliebten Stiefkind des nationalen Videospiele-Marktführers.\n\nIm zweiten Quartal des Jahres 1983 stellte Atari ein Nachfolgemodell der neu aufgelegten XL-Reihe mit zeitgemäßen 64 KB RAM und neuem Gehäusedesign vor. Aufgrund mangelnder Kompatibilität zu seinen Vorgängern war dem Atari 1200XL jedoch kein großer Erfolg beschieden, sodass er über eine nur sehr kurzzeitige Veröffentlichungsphase in den USA nicht hinauskam. Umsomehr schnellten die Verkäufe der alten Modelle 400 und 800 in unerwartete Höhen, da diese mit Einführung des neuen Gerätes durch Rabattaktionen weiter im Preis gesenkt worden waren und zudem keine Programminkompatibilitäten befürchten ließen. Im Mai 1983 schließlich war mit einer unverbindlichen Preisempfehlung von weniger als 200 US-Dollar der Ausverkauf des Atari 400 eingeleitet worden, der sich mit der Ankündigung des offiziellen Nachfolgers Atari 600XL auf der Summer CES in Chicago noch weiter beschleunigte. Im August 1983 wurde die Produktion des Atari 400 eingestellt und Räumungsverkäufe zugunsten des Atari 600XL wurden eingeleitet. Die Modelle 400 und 800 zusammengenommen verkaufte Atari insgesamt etwa zwei Millionen Geräte.\n\nDie überschaubare Architektur des Systems und umfangreiche Dokumentationen des Herstellers ermöglichen den miniaturisierten Nachbau der Elektronik des Atari 400 und dazu kompatibler Modelle mit heutigen technischen Mitteln bei gleichzeitig überschaubarem Aufwand. Eine solche moderne Realisierung erfolgte erstmals 2014 – wie bei anderen Heimcomputersystemen auch – als Implementierung auf einem programmierbaren Logikschaltkreis (FPGA) nebst Einbettungssystem. Die Nachbildung mittels FPGA-Technologie war zunächst lediglich als technische Machbarkeitsstudie gedacht, stellte jedoch im Nachhinein auch ihren praktischen Nutzen unter Beweis: Durch die Miniaturisierung und die Möglichkeit des Batteriebetriebs ist sie eine leicht verstaubare, zuverlässig arbeitende und transportable Alternative zur originalen schonenswerten Technik.\n\nDas Gehäuse des Atari 400 enthält insgesamt vier Leiterplatten und ein stabiles Aluminiumgussgehäuse zur Abschirmung der vom Computer verursachten elektromagnetischen Störfelder. Die Hauptbestandteile der größten Platine bilden der Spezialbaustein POKEY, der Festwertspeicher (ROM) sowie die Ein-/Ausgabebaugruppen nebst Peripherieanschlüssen. Daneben stellt sie als Bauelementeträger Steckplätze für die kleineren Platinen bereit. Diese enthalten die Prozessor-Baugruppe mit 6502-CPU (engl. \"Central Processing Unit\") nebst den Spezialbausteinen GTIA sowie ANTIC, die Speicher-Karte mit Arbeitsspeicher (RAM) und die Baugruppen zur Spannungsregelung und zur Fernsehsignalerzeugung. Zur Grundausstattung gehörte neben dem Computer ein externes Netzteil und die Bedienungsanleitung für das Gerät.\n\nDer Atari 400 basiert auf dem 8-Bit-Mikroprozessor \"MOS 6502\", der häufig in zeitgenössischen Computern eingesetzt wurde. Die CPU kann auf einen Adressraum von 65536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobytes (KB) festlegt. Der Systemtakt beträgt bei PAL-Geräten 1,77 MHz, für solche mit NTSC-Ausgabe dagegen 1,79 MHz.\n\nWesentlicher Bestandteil der Rechnerarchitektur sind die drei von Atari entwickelten Spezialbausteine \"Alphanumeric Television Interface Controller\" \"(ANTIC)\", \"Graphic Television Interface Adapter\" \"(GTIA)\" mit seinem Vorläufer \"Color Television Interface Adapter\" \"(CTIA)\" und \"Potentiometer And Keyboard Integrated Circuit\" \"(POKEY)\". Sie sind funktionell derart konzipiert, dass sie innerhalb ihres Aufgabenbereiches flexibel einsetzbar sind und gleichzeitig die CPU entlasten.\n\nDie beiden Grafikbausteine ANTIC und CTIA/GTIA erzeugen das am Fernseher oder Monitor angezeigte Bild. Dazu sind zuvor vom Betriebssystem oder den Benutzer im Arbeitsspeicher entsprechende Daten in der Form der „Display List“ zu hinterlegen. Der CTIA/GTIA erlaubt unter anderem das Integrieren von maximal acht unabhängigen aber jeweils einfarbigen Grafikobjekten, den Sprites. Diese im Atari-Jargon auch „Player“ und „Missiles“ genannten Objekte werden gemäß benutzerdefinierbaren Überlappungsregeln in das vom ANTIC erzeugte Hintergrundbild kopiert und einer Kollisionsprüfung unterzogen. Dabei wird festgestellt, ob sich die Sprites untereinander oder bestimmte Teile des Hintergrundbildes („Playfield“) berühren. Diese Fähigkeiten wurden – wie sich bereits anhand der Namensgebung „Playfield“, „Player“ und „Missiles“ abzeichnet – zur vereinfachten Erstellung von Spielen mit interagierenden Grafikobjekten und schnellem Spielgeschehen entwickelt. Die Fähigkeiten der beiden Spezialbausteine ANTIC und CTIA/GTIA zusammengenommen, verleihen den Darstellungsmöglichkeiten der Atari-Rechner eine von anderen damaligen Heimcomputern unerreichte Flexibilität. Im dritten Spezialbaustein POKEY sind weitere elektronische Komponenten zusammengefasst. Diese betreffen im Wesentlichen die Tonerzeugung für jeden der vier Tonkanäle, die Tastaturabfrage und den Betrieb der seriellen Schnittstelle \"Serial Input Output\" \"(SIO)\" zur Kommunikation des Rechners mit entsprechenden Peripheriegeräten.\n\nDurch die hochintegrierte Ausführung (LSI) vereinen die Spezialbausteine viele elektronische Komponenten in sich und senken dadurch die Anzahl der im Rechner benötigten Bauteile, was wiederum eine nicht unerhebliche Kosten- und Platzersparnis mit sich bringt. Nicht zuletzt weil ihre Konstruktionspläne nie veröffentlicht wurden, waren sie mit damaliger Technik nicht wirtschaftlich zu kopieren, womit der in der Heimcomputerbranche durchaus übliche illegale Nachbau von Computern für den Atari 400 ausgeschlossen werden konnte.\n\nDie Bildschirmnormen PAL, NTSC und SECAM werden durch unterschiedliche externe elektronische Beschaltungen der CPU, entsprechend modifizierte Spezialbausteine ANTIC (NTSC-Version mit Teilenummer C012296, PAL-Version mit C014887) und GTIA (NTSC-Version mit Teilenummer C014805, PAL-Version mit C014889, SECAM-Version mit C020120) sowie verschiedene darauf abgestimmte Versionen des Betriebssystems realisiert.\nÜbersicht der vom Betriebssystem des Atari 400 bereitgestellten Grafikstufen\n\nDer von der CPU und ANTIC ansprechbare Adressraum segmentiert sich beim Atari 400 in verschiedene Abschnitte unterschiedlicher Größe. Aus praktischen Gründen ist es üblich, für deren Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Ihr wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer Bereich von $0000 bis $7FFF ist ausschließlich für Arbeitsspeicher vorgesehen. Der Bereich von $0000 bis $3FFF entspricht mit seiner Größe von 16 KB der größten im Atari 400 werkseitig verbauten RAM-Konfiguration. Darüber hinaus sind jedoch auch Erweiterungen bis beispielsweise 48 KB möglich, wobei die belegten Speicheradressen dann bis $BFFF reichen. Nach dem Einfügen eines Steckmoduls wird der 8 KB große, inmitten des Arbeitsspeichersegments gelegene Bereich von $A000 bis $BFFF \"(left-hand cartridge slot)\" abgeschaltet und dort die im Steckmodul befindlichen ROMs eingeblendet. Damit stehen bei der Verwendung steckmodulbasierter Programme wie beispielsweise von Atari-BASIC etwa 8 KB Arbeitsspeicher weniger zur Verfügung. Die Adressen der Spezialbausteine und anderer Hardwarebestandteile befinden sich innerhalb eines von $D000 bis $D7FF reichenden Segmentes, unmittelbar gefolgt von den mathematischen Fließkommaroutinen ($D800 bis $DFFF) und dem Betriebssystem ($E000 bis $FFFF). Der Bereich von $C000 bis $CFFF ist für später durch Atari zu ergänzende Systemsoftware vorgesehen, kann aber auch durch Arbeitsspeicher oder alternative Betriebssystemkomponenten genutzt werden.\n\nNach dem Einschalten des Rechners liest die CPU zunächst die Inhalte der ROM-Bausteine mit dem Betriebssystem aus, womit der Atari 400 nebst angeschlossenen Peripheriegeräten initialisiert wird. Sind keine Steckmodule oder Massenspeicher mit ausführbaren Inhalten vorhanden, wird vom Betriebssystem das sogenannte \"Atari Memo Pad\" (auch \"OS Blackboard Mode\" genannt) gestartet. Es handelt sich dabei um ein rudimentäres Texteingabeprogramm ohne weitere Möglichkeiten wie etwa die des Speicherns.\n\nAls Verbindungen zur Außenwelt stehen vier Kontrollerbuchsen an der Vorderseite des Gehäuses, ein koaxialer HF-Antennenanschluss für den Fernseher, ein Schacht zur ausschließlichen Verwendung von ROM-Steckmodulen sowie eine Buchse der proprietären seriellen Schnittstelle (\"Serial Input Output\", kurz \"SIO\") zur Verfügung. Letztere dient dem Betrieb von entsprechend ausgestatteten „intelligenten“ Peripheriegeräten mit Identifikationsnummern. Dabei kommt ein von Atari speziell für diesen Zweck entwickeltes Übertragungsprotokoll und Steckersystem zum Einsatz. Drucker, Diskettenlaufwerke und andere Geräte mit zwei SIO-Buchsen können so mit nur einem einzigen Kabeltyp „verkettet“ angeschlossen werden. Dabei dient jeweils eine der beiden Buchsen zur Kommunikation des Geräts mit dem Computer \"(serial bus input)\" und die verbleibende zum Anschluss und Verwalten eines weiteren Geräts \"(serial bus extender)\". Die in vielen anderen zeitgenössischen Computersystemen verwendeten Standardschnittstellen RS-232C (seriell) und Centronics (parallel) werden durch die extra zu erwerbende Schnittstelleneinheit \"Atari 850\" zur Verfügung gestellt.\n\nDer Atari 400 ist grundsätzlich mit allen von Atari auch später veröffentlichten Peripheriegeräten für die XL- und XE-Reihe betreibbar, die zum Anschluss nicht den bei XL- und XE-Computern herausgeführten Systembus benötigen. Im Folgenden wird ausschließlich auf die von Ende 1979 bis Ende 1983 erhältlichen eingegangen. Zum Gebrauch einiger Peripheriegeräte werden mindestens 16 KB RAM vorausgesetzt, da entsprechend speicherintensive Ansteuerungssoftware zum Betrieb vonnöten ist.\n\nIn Zusammenhang mit vor allem westlichen Heimcomputern der 1980er-Jahre kamen zur Datensicherung hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die günstigste Variante der Datenaufzeichnung durch Kompaktkassetten hat im Allgemeinen den Nachteil niedriger Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren. Bei Veröffentlichung des Atari 400 standen ihm Kassetten- und wenig später auch Diskettensysteme als Massenspeicher zur Verfügung.\n\nIm Gegensatz zu anderen zeitgenössischen Heimcomputern wie beispielsweise dem TRS-80 oder dem Sinclair ZX81 kann der Atari 400 zum Speichern von Daten nicht mit handelsüblichen Kassettenrekordern betrieben werden. Vielmehr benötigt er ein auf seine serielle Schnittstelle abgestimmtes Gerät – den Atari 410 Programmrekorder. Die durchschnittliche Datenübertragungsrate beträgt dabei 600 Bit/s; auf einer 30-Minuten-Kassette finden 50 KB an Daten Platz. Daneben verfügt der Atari 410 noch über die Besonderheit eines Stereo-Tonkopfes, wodurch parallel zum Lesevorgang das Abspielen von Musik oder gesprochenen Benutzungsanweisungen möglich ist. Aus Gründen der Kosten- und Platzersparnis ist im Gerät kein Lautsprecher verbaut, die Audiosignale werden vielmehr über das SIO-Kabel via POKEY am Fernsehgerät ausgegeben. Auch ist keine SIO-Buchse im Atari-410-Programmrekorder verbaut, so dass er stets als letztes Glied in der Kette von Peripheriegeräten anzuschließen ist.\n\nZusammen mit dem Atari 410 Programmrekorder war kurz nach Markteinführung von Atari 400 und 800 auch ein auf Ataris SIO-Schnittstelle abgestimmtes Diskettenlaufwerk erhältlich, die Floppystation Atari 810. Mit dem Atari-810-Diskettenlaufwerk können 5,25″-Disketten einseitig in einfacher Schreibdichte mit 720 Sektoren à 128 Bytes beschrieben werden, womit sich pro Diskettenseite 90 KB Daten abspeichern lassen. Die mittlere Datenübertragungsrate beträgt etwa 6000 Bit/s, das Zehnfache dessen, was der Datenrekorder Atari 410 in derselben Zeit zu übertragen in der Lage ist. Während des gesamten Produktionszeitraumes wurden vom Hersteller an den Laufwerken mehrfach Änderungen vorgenommen. So existieren beispielsweise Ausführungen mit teilweise fehlerhafter Systemsoftware und solche mit verschiedenen Laufwerksmechaniken.\n\nNeben der Diskettenstation 810 war für kurze Zeit in Nordamerika ein weiteres Gerät in Form des wesentlich leistungsfähigeren Atari-815-Diskettenlaufwerks erhältlich. Es verfügt über zwei Laufwerksmechaniken, wobei jede zudem mit doppelter Schreibdichte operiert und so pro 5,25″-Diskettenseite 180 KB Daten gespeichert werden können. Aufgrund der damit verbundenen komplizierten Konstruktion war lediglich eine manuelle Herstellung möglich. Durch den daraus resultierenden hohen Preis von 1500 US-Dollar bei gleichzeitig großer Fehleranfälligkeit wurde das Gerät nach Auslieferung nur geringer Stückzahlen in Höhe von etwa 60 Exemplaren von Atari aus dem Sortiment genommen.\n\nAb Mitte 1982 erschien eine Vielzahl von Atari-kompatiblen Diskettenlaufwerken diverser Dritthersteller. Dazu zählen unterschiedlich leistungsstarke Geräte von Percom, Laufwerke mit zusätzlicher Datenspuranzeige von Rana und auch Doppellaufwerke von Astra.\n\nDie Bildausgabe an einen Monitor ist dem Atari 400 im Unterschied zum Atari 800 aufgrund einer standardmäßig fehlenden Anschlussbuchse nicht möglich; es kann via eingebautem HF-Modulator lediglich ein handelsübliches Farb- oder Schwarz-Weiß-Fernsehgerät angesteuert werden.\n\nZur schriftlichen Fixierung von Text und Grafik dienen der Thermodrucker \"Atari 822\" und die nadelbasierten Modelle \"Atari 820\" und \"Atari 825\". Drucker von Fremdherstellern können nur mithilfe von Zusatzgeräten betrieben werden, da der Atari 400 nicht über entsprechende Standardschnittstellen verfügt. Abhilfe lässt sich durch die Zwischenschaltung eines Atari-850-Schnittstellenmoduls schaffen, womit RS-232- und Centronics-Drucker von Epson, Mannesmann und weiteren betrieben werden können.\n\nDaneben existieren von Fremdherstellern eine Fülle von Ausgabezusätzen: Angefangen bei der zur Sprachausgabe gedachten \"The Voicebox\" von The Alien Group über eine selbstzubauende 3D-Brille zum Betrachten von stereografischen Inhalten am Fernseher bis hin zum programmierbaren Robotergreifarm werden alle damals interessierenden Teilbereiche abgedeckt.\n\nDie Tastatur des Atari 400 besteht aus drei übereinandergeklebten Kunststofffolien. An der oberen und unteren Folie befinden sich unter den Tasten metallische Kontakte, die nach einem vorgegebenen Schema miteinander verschaltet sind. Die mittlere dicke und elastische Folie dient als elektrische Trennschicht und Rückstellfeder. An der Position der Tasten weist sie Löcher auf, deren Abmessungen groß genug sind, um bei Tastendruck einen Stromfluss zwischen oberer und unterer Folie zu erwirken. Da die als Schließer arbeitenden Tasten unergonomisch sind und über keinerlei Druckpunkt verfügen, ist ein effizientes und längeres Arbeiten mit der Tastatur nahezu unmöglich. Sie enthält insgesamt 56 Einzeltasten, eine Leer- und vier Funktionstasten.\n\nSämtliche weitere Eingabegeräte werden an eine oder mehrere der vier an der Vorderseite des Computergehäuses vorhandenen Kontrollerbuchsen angeschlossen. Dazu zählen Joysticks verschiedenster Hersteller, Drehregler, spezielle Kleintastaturen, der Trackball-Controller von TG Products und Grafiktabletts von Kurta Corporation und Koala Technologies Corp.\n\nDer Anschluss von externen, nicht auf der SIO-Schnittstelle basierenden Erweiterungen war von vornherein vom Hersteller nicht vorgesehen. Der Atari 400 lässt sich dennoch durch interne Modifikationen, beispielsweise durch Austausch der ab Werk verbauten 8-KB- und 16-KB-RAM-Karten, erweitern. Zum Installieren der Zusatzhardware muss der Computer geöffnet und im Inneren müssen häufig Lötverbindungen hergestellt werden, was mit etwas technischem Geschick jedoch leicht zu bewerkstelligen ist, zur damaligen Zeit aber auch den Garantieverlust für den Computer zur Folge hatte. Die nachfolgenden Ausführungen beschränken sich ausschließlich auf kommerzielle Produkte der beiden wichtigsten Erweiterungsgebiete der Tastatur und des Arbeitsspeichers.\n\nFür den Gebrauch durch die ursprünglich gedachte Gruppe der Kinder und Jugendlichen erfüllt die strapazierfähige und spritzwassergeschützte Flachtastatur durchaus ihren Zweck – für über Spiele hinausgehende Anwendungen war sie dagegen vollkommen ungeeignet. Dass ergonomische Eigenschaften und ein Druckpunkt fehlten, erschwerte die effiziente Eingabe von Daten. Aus diesem Grunde entwickelte sich ab 1982 ein rege wachsender Markt für Austauschtastaturen mit mechanischen Tasten. Modelle wie \"B Key 400\" von Inhome Software und \"KB 400\" von Atto-Soft konnten anstelle der Folientastatur im Rechner fest montiert oder aber wie der \"Joytyper 400\" von Microtronics und der \"Sidewriter\" von Screen Sonics zum Aufsetzen beziehungsweise äußeren Beistellen angeschlossen werden; ihr Komfort entsprach in vielen Fällen dem der Schreibmaschinentastatur des Atari 800.\nMit dem anfänglich verbauten 8-KB-Arbeitsspeicher war kaum mehr als Spielen möglich, denn bei der Benutzung von BASIC reichte der Speicherplatz nicht einmal für die Einbindung der höchstaufgelösten Grafikstufe. Selbst die größte von Atari angebotene Ausbaustufe von 16 KB führte den Anwender schnell an seine Grenzen. Insbesondere dann, wenn zum Laden und Abspeichern der erstellten BASIC-Programme ein Diskettenlaufwerk benutzt werden sollte. Ursächlich hierfür ist das speicherintensive Diskettenoperationssystem (DOS), das neben dem BASIC-Programm des Anwenders einen großen Teil des Arbeitsspeichers für sich beansprucht. Beim Atari 800 kann mithilfe der leicht zugänglichen Erweiterungsschächte und den von Atari erhältlichen, mit maximal 16 KB RAM bestückten Karten problemlos auf komfortable 48 KB Arbeitsspeicher aufgerüstet werden. Beim Atari 400 hingegen steht lediglich ein einzelner und tief im Gehäuseinneren verborgener Steckplatz für eine RAM-Karte zur Verfügung, weswegen seine Aufrüstung zwangsläufig Steckkarten mit mehr als 16 KB RAM erfordert. Aus diesem Grunde brachten Anfang 1981 Dritthersteller wie Mosaic und Axlon erste 32-KB-RAM-Karten auf den Markt. Später kamen Modelle hinzu, die mithilfe technischer Raffinessen wie Speicherbankumschaltung bis zu 64 KB RAM bereitstellten. Mit solchen und weiteren Aufrüstungen für die Tastatur und mit zusätzlicher Monitorbuchse versehen, unterschieden sich die möglichen Anwendungsgebiete des Atari 400 – bis auf den fehlenden rechten Modulschacht – nicht mehr von denen des Atari 800. Diese Verwischung der Anwendungsgrenzen zugunsten des wesentlich günstigeren Atari 400 widerstrebte jedoch Ataris Vermarktungskonzept zweier komplementärer Geräte, weswegen offizielle Produktbeschreibungen als maximale Ausbaustufe des Atari 400 stets die Grenze von 16 KB angaben und vor einem Einbau von mehr als 16 KB RAM unter anderem mit dem Erlöschen der Garantie für den Computer warnten. Höhere Aufrüstungen von Atari selbst wurden mit dem \"Atari 400 Home Computer 48K RAM Expansion Kit\" erst nach dem Produktionsende des Atari 400 ab Herbst 1983 angeboten.\n\nWie bei anderen Heimcomputern der 1980er-Jahre auch erfolgte der Vertrieb kommerzieller Software auf verschiedenen Datenträgern. Die insbesondere bei Spieleherstellern beliebten preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes allerdings sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Zudem sind mit Datasetten bestimmte Betriebsarten wie die beispielsweise zum Betrieb von Datenbanken vorteilhafte relative Adressierung nicht möglich. Bei den in der Herstellung vielfach teureren Steckmodulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen von großem Vorteil war. Den besten Kompromiss zwischen Ladezeit, möglichen Betriebsarten, Verlässlichkeit und Speicherkapazität erzielten die Disketten, deren Verwendung bei Veröffentlichung des Atari 400 durch das 810-Diskettenlaufwerk unterstützt wurde.\n\nDie Programmpalette für den Atari-400-Computer umfasste neben der von Atari und APX vertriebenen Auswahl kommerzieller Programme auch von Drittherstellern entwickelte und in Zeitschriften und Büchern publizierte Software (Listings) zum Abtippen. Die kommerziellen Programme wurden auf Steckmodul, Diskette und Kassette angeboten.\n\nVon der in Umlauf befindlichen Software machten illegale Kopien („Raubkopien“) stets einen großen Teil aus und stellten damit kleinere Softwareentwickler häufig vor existentielle wirtschaftliche Schwierigkeiten.\n\nDaraufhin wurden zunehmend Kopierschutzsysteme insbesondere bei Spielen als der meistverkauften Software eingesetzt.\n\nDie Konfiguration und Initialisierung der Atari-400-Hardware nach dem Einschalten bzw. nach einem Reset fällt in den Aufgabenbereich des im Festwertspeicher untergebrachten Betriebssystems (engl. \"Operating System\", kurz \"OS\"). Nachdem zahlreiche Fehler in der ersten OS-Version bekannt geworden waren, veröffentlichte Atari mit \"OS-B\" im Jahr 1982 eine fehlerbereinigte Version. Die Unterprogramme des 10 KB umfassenden Betriebssystems steuern verschiedene Systemprozesse, die auch vom Benutzer angestoßen werden können. Dazu gehören die Durchführung von Ein- und Ausgabeoperationen wie etwa die Tastatur- und Joystickabfrage, Fließkommaberechnungen, die Abarbeitung von Systemprogrammen nach Unterbrechungen (Interrupts) und die Bereitstellung eines Bildschirmtreibers zum Erzeugen der verschiedenen Grafikmodi. Die Startadressen der einzelnen Unterprogramme sind in einer Sprungtabelle zusammengefasst, um die Kompatibilität mit späteren Betriebssystem-Revisionen oder neuen Versionen zu wahren. Zur Abgrenzung vom Betriebssystem der später erschienenen XL- und XE-Modelle wird das OS des Atari 400 häufig auch als \"Oldrunner\" bezeichnet.\n\nAufbauend auf der Systemsoftware kam dem benutzerspezifischen Einsatz des Atari 400 in unterschiedlichsten Anwendungsgebieten große Bedeutung zu. War dabei die Bearbeitung einer Aufgabenstellung mit z. B. käuflich zu erwerbenden Programmen aus technischen oder wirtschaftlichen Gründen nicht möglich oder sollte beispielsweise neuartige Unterhaltungssoftware produziert werden, so musste dies mithilfe von entsprechenden Programmiersprachen in Eigenregie geschehen.\n\nDie Erstellung zeitkritischer Actionspiele und Anwendungen in der Regelungstechnik erforderte Anfang der 1980er-Jahre eine optimale Nutzung der Hardware insbesondere des Arbeitsspeichers. Im Heimcomputerbereich war dies ausschließlich durch die Verwendung von Assemblersprache mit entsprechenden Übersetzerprogrammen, den Assemblern, möglich. Die Auslieferung von Assemblern erfolgte in vielen Fällen mit einem zugehörigen Editor zur Eingabe der Programmanweisungen („Sourcecode“), häufig auch als Programmpaket mit Debugger und Disassembler zur Fehleranalyse. Im professionellen Entwicklerumfeld kamen vielfach Cross-Assembler zum Einsatz. Damit war es möglich, ausführbare Programme für Heimcomputer auf leistungsfähigeren und komfortabler zu bedienenden Fremdcomputerplattformen zu erzeugen.\n\nKurz nach Veröffentlichung der Atari-Computer war lediglich der auf Steckmodul ausgelieferte langsame \"Assembler Editor\" von Atari erhältlich. Er bot wenig Komfort und konnte daher nur für kleinere Projekte sinnvoll eingesetzt werden. Im Gegensatz zu anderen Assemblern erlaubte er jedoch das Abspeichern der erstellten Quelldateien und ausführbaren Programme auf Kassette, was insbesondere für viele Atari-400-Benutzer ohne Diskettenstation von Vorteil war und sie so über die Nachteile leicht hinwegsehen ließ. Die für professionelle Programmentwicklung benötigten Assembler standen erst später mit \"Synassembler\" (Synapse Software), \"Atari Macro Assembler\" (Atari), \"Macro Assembler Editor\" (Eastern Software House), \"Edit 6502\" (LJK Enterprises) und dem leistungsfähigen \"MAC 65\" (Optimized Systems Software) zur Verfügung.\n\nProgrammiereinsteiger zogen in vielen Fällen die übersichtlichen und einfach zu bedienenden, dafür aber weniger leistungsfähigen Programmier-Hochsprachen vor.\n\nDem von Atari veröffentlichten BASIC standen zwei weitere zur Seite: Das den damaligen Quasi-Standard bildende Microsoft BASIC (als Adaption von Atari) und ein zu Ataris BASIC abwärtskompatibles Produkt mit dem Namen \"BASIC A+\" von Optimized System Software. Beide Interpreter setzen mindestens 32 KB RAM und ein Diskettenlaufwerk zum Betrieb voraus. Insbesondere BASIC A+ enthält erweiterte Editiermöglichkeiten, Vereinfachungen in der Befehlsstruktur und es ergänzt viele im Atari- und Microsoft-BASIC nicht implementierte Leistungsmerkmale. Dazu zählt beispielsweise eine bequeme Benutzung der Atari-Sprites („Player“ und „Missiles“) durch eigens dafür bereitgestellte Befehlswörter.\n\nNachteilig auf die Einsetzbarkeit von BASIC-Programmen wirkten sich die in der Natur des Interpreters liegenden prinzipiellen Beschränkungen wie etwa die geringe Ausführungsgeschwindigkeit und der große Arbeitsspeicherbedarf aus. Diese Nachteile können durch spezielle Programme, BASIC-Compiler, abgemildert werden. Dabei werden ausführbare Maschinenprogramme erzeugt, die ohne BASIC-Interpreter lauffähig sind und damit häufig eine schnellere Ausführung erlauben. Für das Atari BASIC stehen mit \"ABC BASIC Compiler\" (Monarch Systems), \"Datasoft BASIC Compiler\" (Datasoft) und \"BASM\" (Computer Alliance) verschiedene Compiler zur Verfügung.\n\nNeben der Programmiersprache BASIC in ihren verschiedenen Dialekten war mit Verkaufsstart des Atari 400 die Interpretersprache \"Logo\" erhältlich. Unterstützt durch Elemente wie die \"turtle graphics\" (Schildkrötengrafik) ist damit eine kindgerechte und interaktive Einführung in die Grundlagen der Programmierung möglich. Ähnlich gelagert in ihren Eigenschaften ist die später in den Handel gebrachte Programmiersprache \"Atari PILOT\". Mit \"QS-Forth\" (Quality Software), \"Extended fig-Forth\" (APX) und \"Data-Soft Lisp\" (Datasoft) reihen sich weitere Programmiersprachen in die Produktpalette für den Atari 400 ein.\n\nInterpreter-Hochsprachen sind langsam in der Ausführung, dafür ist ihr Quelltext gut lesbar und somit die Fehleranalyse einfach. Die Assemblersprache dagegen ist schwer zu erlernen und zu beherrschen, war aber Anfang der 1980er-Jahre zur Erzeugung schneller und speichereffizienter Programme unumgänglich. Als Mittelweg etablierten sich daraufhin im Laufe der 1980er im Heimcomputerbereich die Compiler-Hochsprachen. Die Ausführungsgeschwindigkeit der damit erzeugten Maschinenprogramme war im Vergleich zu interpretierten Programmen wie beim Atari BASIC sehr viel höher, reichte aber nicht ganz an die von Assemblern erzielte heran. Diese Geschwindigkeitsnachteile wurden jedoch vielfach zugunsten eines leichter zu wartenden Quelltextes in Kauf genommen.\n\nIm Laufe der Produktlebenszeit bis Ende 1983 war für die Atari-400-Anwender mit aufgerüsteten Geräten als Compilersprache lediglich APX Pascal erhältlich.\n\nDie Programmpalette für die Atari Computer umfasst neben den Programmiersprachen zum Erstellen eigener Applikationen eine im Vergleich zum zeitgenössischen Konkurrenten Apple II lediglich kleine Auswahl an vorgefertigter kommerzieller Anwendungssoftware. Zu den bekanntesten Anwendungsprogrammen zählen \"VisiCalc\" (Visicorp, Tabellenkalkulation), \"The Home Accountant\" (Continental Software, Buchführung), \"Atari Writer\" (Atari, Textverarbeitung), \"Bank Street Writer\" (Broderbund, Textverarbeitung) und \"Letter Perfect\" (LJK Enterprises, Textverarbeitung).\n\nEinen weiteren Teil der Anwendungen bilden von Benutzern in Eigenregie erstellte Anwendungsprogramme für die unterschiedlichsten Einsatzorte wie etwa in Arztpraxen, Fotostudios, Bekleidungsgeschäften und Museen.\n\nEntsprechend der Ausrichtung des Atari 400 als Spiel- und Lerncomputer existiert eine Vielzahl an Programmen, die dem computergestützten Vermitteln von Lehrinhalten und seiner anschließenden interaktiven Abfrage dienen. Das zu vermittelnde Wissen wird in spielerischer Form mit ständig steigendem Schwierigkeitsgrad präsentiert, um den Lernenden anhaltend zu motivieren. Dabei wird großer Wert auf eine altersgerechte Darbietung gelegt, die von Kleinkindern bis hin zu Studenten reicht. Bei den Jüngsten kommen häufig animierte Geschichten mit comicartigen Charakteren als begleitende Tutoren zum Einsatz, bei Jugendlichen werden abzufragende Lehrinhalte in Abenteuerspiele oder actionsreiche Weltraumabenteuer gekleidet, bei den höherstufigen Lehrinhalten für Studenten und Erwachsene überwiegt hingegen meist lexikalisch präsentiertes Wissen mit anschließender Abfrage nebst Erfolgsbilanzierung. Die von der Software abgedeckten Lerngebiete erstrecken sich auf Lesen und Schreiben, Fremdsprachen, Mathematik, Technik, Musik, Geographie, Demografie, Tippschulen und Informatik. Zu den bekanntesten Herstellern zählen Atari, APX, Dorsett Educational Systems, Edufun, PDI und Spinnaker Software.\n\nDen mit Abstand größten Teil der sowohl kommerziellen als auch frei erhältlichen Atari-Software stellen die Spiele dar. Zu den frühen Shoot-’em-up-Spielen wie etwa \"Star Raiders\" oder der Brettspieleumsetzung \"3-D Tic-Tac-Toe\" kamen bereits ein Jahr später weitere Actionspiele, Adventures und Arcade-Umsetzungen hinzu. Sowohl professionelle Hersteller als auch Hobbyprogrammierer profitierten dabei von der Veröffentlichung technischer Dokumentationen seitens Atari, den Programmieranleitungen in den Computermagazinen und -büchern sowie von den mittlerweile aufgekommenen leistungsfähigen Entwicklungswerkzeugen. Unter den publizierten Titeln befanden sich jedoch auch viele schlechte Portierungen von beispielsweise Apple-II-Spielen ohne den unverwechselbaren „Atari-Look“, nämlich eine Mischung verschiedener „farbenprächtiger“ und weichverschobener Grafiken, ergänzt um die typische POKEY-Musik nebst Geräuscheffekten.\n\nUnter den für die Atari-Computer veröffentlichten Spielen befinden sich viele, die bereits in den frühen 1980er-Jahren als Videospieleklassiker galten: \"Star Raiders\" (vermutlich 1979), \"Asteroids\" (1981) und \"Pac-Man\" (1982). Insbesondere das 3D-Spiel \"Star Raiders\" galt vielen Spieledesignern der damaligen Zeit als prägendes Erlebnis und Grund, sich für einen Atari-Computer und nicht etwa einen Apple II oder Commodore PET zu entscheiden. In der Folge entstandene Werke wie \"Miner 2049er\" (Bill Hogue, Big Five Software, 1982), \"Eastern Front (1941)\" (Chris Crawford, APX, 1982), \"Capture the Flag\" (Paul Edelstein, Sirius Software, 1983), \"Archon\" (John Freemann, Electronic Arts, 1983) und \"M.U.L.E.\" (Daniel Bunten, Electronic Arts, 1983) zählen zu den herausragenden Titeln ihrer Zeit und ermöglichten Softwarehäusern wie beispielsweise Microprose und Electronic Arts den raschen Aufstieg zu Branchenriesen.\n\nZu den beliebtesten Spielen für die Atari-Computer gehören neben den Infocom-Abenteuern großteils Shoot-’em-up-Spiele wie \"Crossfire\" (Sierra On-Line, 1981) und \"Blue Max\" (Synapse Software, 1983), Rennspiele wie \"Pole Position\" (Atari, 1983), Kriegssimulationen wie \"Combat Leader\" (SSI, 1983), aber auch Grafik-Adventures wie \"Excalibur\" (APX, 1983) und \"Murder on the Zinderneuf\" (Electronic Arts, 1983).\nIn den 1980er-Jahren spielten neben den Fachbüchern die Computerzeitschriften für viele Heimcomputerbesitzer eine große Rolle. Die häufig monatlich erschienenen Ausgaben enthielten Testberichte zu Neuheiten, Programmieranleitungen und Software zum Abtippen. Sie dienten weiterhin als Werbe- und Informationsplattform sowie zur Kontaktaufnahme mit Gleichgesinnten.\n\nSpeziell mit den Atari-Heimcomputern befassten sich die englischsprachigen Magazine \"Antic\", \"Analog Computing\", \"Atari Connection\" und \"Atari Age\"; gelegentliche Berichte und Programme für die Atari-Rechner veröffentlichten unter anderem auch die auflagenstarken \"Byte Magazine\", \"Compute!\" und \"Creative Computing\". Während der Atari 400 in Deutschland verkauft wurde, waren Informationen und Programme unter anderen in den Zeitschriften \"Chip\", \"P.M. Computermagazin\", \"Computer Persönlich\" und \"Mein Home-Computer\" zu finden.\n\nNach dem Ende der Heimcomputerära Anfang der 1990er-Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Ende der 1990er-Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripheriegeräten entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reichte mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit u. a. ein verstärktes Transferieren von sonst möglicherweise verloren gegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird.\n\nAls leistungsfähigste Emulatoren für Windows und Linux-Systeme gelten \"Atari++\", \"Atari800Win Plus\", \"Mess32\" und \"Altirra\".\n\nDas Erscheinen des Atari 400 und 800 wurde durchweg positiv aufgenommen. Die auflagenstarke Zeitschrift \"Compute!\" schrieb von einer neuen Generation von Computern:\n\nVon denselben Rezensenten wird zudem ausgeführt, dass die Einordnung der neuen Geräte am ehesten mit der eines Hybriden zwischen Videospiel und Computer zu umschreiben sei. Sie enthielten das Beste beider Welten, was sie damit zu einem Personalcomputer und Heimgerät gleichermaßen mache. Diese Eigenschaften prädestinierten den Atari 400 geradezu für Lern- und Unterhaltungszwecke. Da die beste Hardware ohne entsprechende Software zu ihrem Gebrauch jedoch nutzlos sei, habe Atari aus den Fehlern der Konkurrenz gelernt und dem Benutzer mit der Programmiersprache Atari BASIC einen ausgesprochen leichten Zugang zu den farbenprächtigen Grafik- und Toneigenschaften seiner Geräte zur Seite gestellt. Diese Vermarktung von aufeinander abgestimmter Hard- und Software – auch beim direkt auf die Atari 8-Bit-Computer zugeschnittenen äußerst populären Spiel \"Star Raiders\" – stelle ein Novum dar.\n\nKritisch beurteilt wurde der Mangel an Erweiterbarkeit des Atari 400, insbesondere die ursprünglich ausgelieferten 8 KB RAM würden zur Programmierung des hochgelobten Atari BASIC bei weitem nicht ausreichen. Durch das modulare Konzept wären mehr Anschlusskabel als etwa beim kompakten Commodore PET vonnöten, was unter Umständen von Nachteil sein könne ebenso wie das nicht-validierende Abspeichern von Programmen auf Kassette. Ab Sommer 1980 wurden vor allem Lieferschwierigkeiten und das Ausbleiben von anwendungsorientierter Software bemängelt und den Rechnern von Adam Osborne keine große Zukunft vorausgesagt.\n\nAls sich die Atari-Computer entgegen den Voraussagen Osbornes dennoch etablieren konnten und sogar zum Marktführer aufgestiegen waren, wurden von der Fachpresse weiterhin Empfehlungen hauptsächlich für preisbewusste Haushalte ausgesprochen:\n\nÜbereinstimmend mit der Fachpresse sahen auch Spieleautoren wie David Fox (Programmierer bei Lucasfilm-Games) und Scott Adams (Gründer von Adventure International) in den Ataris die grafisch und tontechnisch leistungsfähigsten Geräte des gesamten Heimcomputermarktes:\n\nIm Laufe der Zeit geriet Ataris Vermarktungskonzept aber auch in die Kritik, da die Fähigkeiten als Anwendungscomputer nicht klar genug herausgestellt und unterstützt würden. Obwohl die Atari-Computer seit ihrer Einführung einen guten Ruf auch als leistungsfähige Personal Computer genossen hätten, sei spätestens mit der Produktionseinstellung des leistungsfähigen Diskettenlaufwerks Atari 815 der Einsatzschwerpunkt der Geräte auf den Heimbereich mit besonderem Augenmerk auf den Unterhaltungs- und Bildungssektor verschoben worden. Dazu kämen Fehler bei der Wahl der Vertriebswege. Die Verlagerung des Verkaufs durch große Ladenketten hätte kleinere Fachgeschäfte mit entsprechender Kompetenz und Serviceleistungen bewogen, mangels Konkurrenzfähigkeit die Atari-Rechner aus dem Angebot zu nehmen. Damit wäre ein weiteres wichtiges Standbein zur Versorgung der Rechner mit leistungsfähiger Anwendungssoftware entfallen, so dass der Atari 400 letztlich nur noch als reine Spielekonsole wahrgenommen und gekauft wurde. Hinzu sei gekommen, dass Atari selbst nichts unternommen habe, diesen Umstand zu ändern und beispielsweise den Atari 400 mit mehr als 16 KB RAM ab Werk anzubieten.\n\nKurz nach seinem Erscheinen in Deutschland wurde der Atari 400 vom damals auflagenstärksten Computermagazin \"Chip\" als einsteigerfreundliches Anfängergerät charakterisiert. Positiv hervorgehoben wurden zudem die stabile Geräteausführung, die grafischen Möglichkeiten, die Farbausgabe, eine ausführliche Dokumentation, die bereits vorhandene große Programmbibliothek nebst verschiedenen Programmiersprachen wie \"Atari PILOT\" und \"Atari Assembler\" und nicht zuletzt der günstige Preis. Kurz darauf wurde der Atari 400 durch dasselbe Computer-Magazin mit weitem Abstand vor dem Commodore VC 20 und Sinclair ZX81 zum „Computer des Jahres 1981“ gewählt:\n\nNeben der auch von anderen Rezensenten gelobten Einsteigerfreundlichkeit bildeten die mangelnde Erweiterbarkeit und die einfach gehaltene Tastatur die häufigsten Kritikpunkte:\n\nBereits kurz nach der Ablösung durch die technisch kaum veränderten Nachfolgemodelle 600XL und 800XL wird dem Atari 400 eine „exzellente Konstruktion“ bescheinigt, die einen neuen Standard auf dem Heimcomputermarkt gesetzt habe. Die „phantastische Grafik“ spiegele sich vor allem in den guten Spielen wider, einer der Stärken des Atari 400. Einer der wenigen Kritikpunkte bildete nach Meinung von Michael S. Tomczyk und Dietmar Eirich der bei Einführung zu hohe Preis:\n\nRückblickend verstand es Atari laut Bill Loguidice und Matt Barton erstmals, die Eigenschaften einer reinen Spielemaschine mit den Fähigkeiten damaliger Heimcomputer bei gleichzeitig leichter Bedienbarkeit zu kombinieren. Als einer der Hauptgründe für das Gelingen dieser anspruchsvollen Aufgabe gelten den beiden Autoren die in die Entwicklung einfließenden Erfahrungen der bereits am Bau der erfolgreichen VCS-2600-Spielekonsole beteiligten Atari-Ingenieure. Als Ergebnis waren erstmals in einem Heimcomputer elektronische Spezialbausteine zur Entlastung des Hauptprozessors zur Anwendung gekommen. Deren grafische Raffinessen in Form von beispielsweise der Player/Missile-Grafik seien wegweisend für spätere Geräte gewesen. Auch die Soundeigenschaften hätten durch Verwendung eines Spezialbausteins zur damals obersten Qualitätskategorie gehört und der Atari 400 habe den Apple II damit als besten Spiele-Computer abgelöst.\n\nAls entscheidenden Grund für die innerhalb kürzester Zeit ansteigende Popularität der Atari-Computer sehen die Autoren der Internetplattform Gamasutra die Veröffentlichung des Spiels \"Star Raiders\":\n\nFür den permanenten Mangel an leistungsfähiger Anwendungssoftware macht Tomczyk Ataris ursprüngliche und umstrittene Praktiken bezüglich der Veröffentlichung technischer Dokumentationen verantwortlich:\n\nEine spätere Änderung der restriktiven Informationenspolitik hätte den bereits entstandenen Rückstand nicht mehr aufholen helfen können. So seien mit fortschreitender Zeit hauptsächlich Spiele für die Atari-Heimcomputer erschienen, womit diese nun mehr und mehr als reine Spielemaschinen wahrgenommen wurden:\n\nDurch die damit von Atari selbstgeschaffene Konkurrenz zur hauseigenen Spielekonsole VCS 2600 und hauptsächlich infolge aufkommender Konkurrenz durch Texas Instruments und Commodore mit ihren umfangreichen Programmbibliotheken im Anwendungsbereich hätten die Verkaufserfolge nicht weitergeführt werden können. Entscheidende Marktanteile wären damit ab 1983 wieder dem Apple II und vor allem dem neu erschienenen Commodore 64 zugefallen.\n\nDer Atari 400 ist ständiges Ausstellungsstück im Computerspielemuseum Berlin.\n\n\n"}
