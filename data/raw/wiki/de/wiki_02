{"id": "32295", "url": "https://de.wikipedia.org/wiki?curid=32295", "title": "Atari 800XL", "text": "Atari 800XL\n\nDer Atari 800XL ist ein Heimcomputer des US-amerikanischen Herstellers Atari, Inc. Er basiert auf einer eigens für Atari gefertigten Variante des 6502-Mikroprozessors.\n\nDer Computer ist eine Weiterentwicklung des im März 1983 in den USA veröffentlichten Atari 1200XL. Die elektronischen Hauptbestandteile blieben weitestgehend erhalten, lediglich das optische Erscheinungsbild und technische Details zur Erweiterbarkeit und zur Produktionsvereinfachung wurden überarbeitet. Als direkten Konkurrenten zum Commodore 64 stattete Atari den Rechner mit 64 Kilobyte (KB) Arbeitsspeicher aus. Wie bei dem mit nur 16 KB Arbeitsspeicher ausgerüsteten Einsteigermodell Atari 600XL auch ist die Programmiersprache Atari BASIC im Rechner enthalten und steht nach dem Einschalten zur Verfügung.\n\nDas Gerät kam – begleitet von umfangreichen Werbekampagnen – Ende 1983 weltweit in den Handel. Im Weihnachtsgeschäft 1983 war der Computer wegen der verzögerten Produktionsaufnahme nicht in der nachgefragten Menge lieferbar, wodurch größere Marktanteile an die Konkurrenz insbesondere den Commodore 64 verlorengingen. Nach der Übernahme Ataris durch Jack Tramiel folgten bis zum Weihnachtsgeschäft 1984 weltweit schrittweise drastische Preissenkungen. Diese machten den Atari 800XL zum preisgünstigsten Computer seiner Leistungsklasse, vermochten jedoch nicht den Commodore 64 als Marktführer zu verdrängen.\n\nNach Markteintritt der Nachfolgebaureihe, der XE-Modelle, Anfang 1985 wurde parallel dazu die Produktion des Atari 800XL noch bis zum November 1985 weitergeführt. Mit dem Mitte 1986 beginnenden allmählichen Niedergang in Nordamerika und Westeuropa erlebte der Computer in den RGW-Staaten ein unerwartetes Aufleben, das – zusammen mit der XE-Baureihe – in der dortigen Marktführerschaft gipfelte. Die unerwartet große Nachfrage führte im Juli 1988 zu einer Wiederaufnahme der Produktion. Ende 1992 stellte Atari die Unterstützung und damit auch die Herstellung seiner 8-Bit-Computer endgültig ein.\n\nDie Fachpresse lobte kurz nach Erscheinen das ansprechende Äußere, die gute Verarbeitung, das eingebaute Atari BASIC und das große Angebot an Peripheriegeräten und Programmen.\n\nNachdem die seit 1979 produzierten Heimcomputer Atari 400 und Atari 800 nicht mehr dem Zeitgeschmack entsprachen und sich das Nachfolgemodell Atari 1200XL als Flop erwiesen hatte, sollten 1983 zeitgemäße und preisgünstige Geräte Atari aus der wirtschaftlichen Krise führen. Insbesondere im Segment der hochwertigen Heimcomputer erhoffte man sich, in Konkurrenz zu Commodore International und den Commodore 64 treten zu können.\n\nAtari sah einen zu seinen Vorgängermodellen kompatiblen Computer im Design des Atari 1200XL vor, allerdings ohne dessen Mängel und ergänzt um Erweiterungsmöglichkeiten. Angesichts der damaligen Marktsituation plante man dabei mit zwei verschiedenen technischen Konfigurationen. Das Einsteigermodell mit 16 KB Arbeitsspeicher – der spätere Atari 600XL – war als Konkurrenz zum Commodore VC 20 und Sinclair ZX Spectrum gedacht; die höherwertige Variante mit zeitgemäßen 64 KB Arbeitsspeicher zielte auf Marktanteile des erfolgreichen Commodore 64. Die firmenintern als „Surely“ und „Surely Plus“ geführten Entwicklungsprojekte stützten sich hauptsächlich auf verbliebene Kapazitäten und Personal des Projektes „Liz“, aus dem zuvor bereits der Atari 1200XL hervorgegangen war.\n\nDie Arbeiten an der höherwertigen Ausstattungsvariante der neuen XL-Computer begannen im März 1983. Als Grundlage diente die Systemarchitektur der Vorgängermodelle mit den Spezialbausteinen ANTIC, GTIA und POKEY sowie einer speziellen Variante des 6502-Mikroprozessors, die unter dem Namen „SALLY“ bereits im Atari 1200XL zum Einsatz kam. Darüber hinaus wurde ein neuartiger Speicherverwaltungsbaustein („FREDDIE“) konzipiert, der seinen Weg zunächst noch nicht in die Produktspezifikationen fand – mangels Entwicklungs- und Produktionskapazitäten schien eine fristgerechte Produktion in ausreichender Menge nicht möglich. Zu den technischen Neuerungen zählen unter anderem die Integration der Programmiersprache Atari BASIC in den Computer und die Ergänzung einer Anschlussmöglichkeit für Erweiterungen.\n\nDen fortan \"Atari 800XL\" genannten Computer präsentierte Atari zusammen mit neuer Peripherie erstmals auf der Consumer Electronics Show (CES), die Anfang Juni 1983 in Chicago stattfand. Von Atari eigens eingeladene Vertreter der größten nationalen Anwendergruppen (englisch \"User Groups\") erhielten bei der Vorstellung auf der CES die Möglichkeit zur Begutachtung der neuen Geräte. Durch die Einbeziehung der zukünftigen Benutzerschaft erhoffte sich Atari, eventuell vorhandene Schwachstellen noch vor Aufnahme der Produktion aufdecken zu können. Damit sollte ein Scheitern wie beim Vorgängermodell Atari 1200XL vermieden werden.\n\nWenig später erfolgte die Abnahme zur elektromagnetischen Verträglichkeit durch die US-amerikanische Federal Communications Commission (FCC) – eine maßgebliche Voraussetzung zur Verkaufbarkeit des Geräts in den USA. Zwischenzeitlich waren auch die Entwicklungsarbeiten zur Anpassung des Rechners an die verschiedenen Fernsehnormen abgeschlossen worden. Entsprechende PAL-Geräte beispielsweise wurden im Spätsommer 1983 auf der Internationalen Funkausstellung in Berlin vorgestellt. Die Überführung der Computer in die Produktion nach Hongkong erfolgte nach einem Wechsel in Ataris Führungsspitze mit einmonatiger Verzögerung, genauso wie die Herstellung der für den europäischen Markt bestimmten Geräte in Irland.\n\nDer Hersteller pries seinen Atari 800XL als leistungsstarkes und anwenderfreundliches Gerät hauptsächlich für Computereinsteiger („We made them smart enough to know you’re only human“, „Discover what you and Atari can do“) und Kleinanwender, für Weiterbildungszwecke und zum Spielen an („You’ll do more with Atari homecomputers“, „The 800XL: power enough for over 2.000 programs“). Dazu bediente man sich großformatiger Zeitungsanzeigen und Werbefilme für das Fernsehen. Eine Schlüsselrolle spielte dabei der bereits auf der CES im Juni 1983 als offizieller Werbeträger vorgestellte US-amerikanische Schauspieler Alan Alda. Dessen fünfjährigem und rund fünf Millionen US-Dollar teurem Engagement gingen umfangreiche Marktforschungsmaßnahmen seitens Atari voraus.\n\nDer Atari 800XL kam sowohl in Nordamerika als auch in Großbritannien Ende November 1983 in den Handel. Die unverbindliche Preisempfehlung lag in den USA bei 299 US-Dollar, die für Großbritannien bei 249 britischen Pfund. Durch die anfänglichen Produktionsverzögerungen konnten selbst mithilfe teurer Luftfrachteinfuhren bis Weihnachten in Nordamerika lediglich 60 Prozent der vorbestellten Geräte ausgeliefert werden. Die gesamte Jahresproduktion des Atari 600XL und 800XL – etwa 400.000 Geräte – war bis Ende des Jahres ausverkauft worden. Wohl aufgrund der allgemeinen Lieferengpässe kam der Atari 800XL in Frankreich und Westdeutschland erst im April 1984 in nennenswerten Stückzahlen in den Handel; der Preis dieser PAL-Ausführung lag bei 3200 Franc beziehungsweise bei rund 800 deutschen Mark (DM). Eine speziell für die französische SECAM-Fernsehnorm gefertigte Variante war ab Juni für 3500 Franc erhältlich. In Italien kam der Rechner vermutlich auch erst zu diesem Zeitpunkt und dann zum Preis von 707.000 Lira in den Handel.\n\nMit Beginn des Jahres 1984 erhöhte Atari in Nordamerika die Großhandelspreise der neuen XL-Computer um 40 US-Dollar. Dies geschah mit der Begründung fortan nur noch kostendeckend verkaufen und den ruinösen Preiskampf in der Heimcomputerbranche beenden zu wollen. Die Vermarktungsbemühungen richteten sich neben privaten Haushalten nun auch vermehrt auf Bildungseinrichtungen wie etwa Schulen. Im Sommer 1984 beteiligte sich Atari als Sponsor für Heimcomputer an den Olympischen Spielen in Los Angeles. Der Atari 800XL avancierte daraufhin zum „Official Home Computer of the 1984 Olympics“, womit den potentiellen Käufern ein gewisses Renommee der Geräte suggeriert werden sollte. Daneben schloss Atari Verträge über umfassende Fernsehwerbung, um weitere mögliche Interessenten erreichen zu können.\n\nKurz nach der für die gesamte Computerbranche völlig unerwarteten Übernahme von Atari durch Jack Tramiel im Juli 1984 herrschte zunächst Unklarheit über die Fortführung von Ataris XL-Produktlinie. Die kurz darauf veröffentlichten Zukunftspläne Tramiels sahen jedoch nur die Einstellung des ohnehin schon seit längerer Zeit als unprofitabel geltenden Atari 600XL vor. Übernommene Lagerbestände des Atari 800XL mit einem Umfang von etwa 100.000 Geräten und ab August 1984 vorgenommene Optimierungen im Herstellungsprozess – der monatliche Produktionsausstoß des Atari 800XL erreichte dadurch etwa 150.000 Geräte – führten kurz darauf zu den von der Fachpresse bereits erwarteten deutlichen Preisnachlässen.\n\nAb November 1984 folgten unter dem von Tramiel ausgegebenen Slogan „Power without the Price“ weitere Preissenkungen in Europa. Der sich kurz darauf anschließende vorweihnachtliche Preisrutsch auf 120 US-Dollar beziehungsweise 130 britische Pfund, den Verkaufspreis des Konkurrenzmodells Sinclair ZX Spectrum, gab zunächst Anlass zu Spekulationen über einen Ausverkauf zugunsten neuer Computermodelle. Genährt wurden die Gerüchte hauptsächlich durch bereits Ende September bekanntgewordene Andeutungen Ataris zu einem technisch und optisch aufgefrischten Nachfolgemodell des Atari 800XL. Die Firmenleitung dementierte umgehend und erklärte, dass die aggressive Preispolitik hauptsächlich aufgrund weiterer, zwischenzeitlich vorgenommener Optimierungen im Herstellungsprozess möglich geworden war. Sie bestätigte zudem ausdrücklich die Fortführung der Produktion. Der von Atari entfachte Preiskampf – auch in Westdeutschland fiel der Preis im Dezember 1984 von rund 650 auf 500 DM – zielte vor allem auf die Marktanteile des direkten Konkurrenten und Marktführers Commodore 64.\n\nIm Rahmen des von Jack Tramiel auch als „Marketing for the masses“ bezeichneten Vermarktungskonzeptes und seinen Kampfpreisen kamen zudem verstärkt Bündelangebote in den europäischen Handel. Die britische Warenhauskette Laskys beispielsweise bot den \"Starter Pak\" bestehend aus Computer, Datenrecorder Atari 1010, Joystick, Anleitungsmaterial und Software ab Dezember zu einem Preis von 170 britischen Pfund an. Damit zählte der Atari 800XL Ende 1984 zu den günstigsten Computern seiner Leistungsklasse; Konkurrenzmodelle wie etwa der Commodore 64 und die MSX-Computer waren deutlich teurer. 1984 gelang es Atari so, weltweit etwa 600.000 Exemplare des Atari 800XL abzusetzen und in den US-amerikanischen Schulen in die Riege der drei am meisten verbreiteten Schulcomputer aufzusteigen. Was jedoch nicht gelang, war die Verdrängung des Commodore 64: vom internationalen Marktführer fanden im selben Zeitraum etwa viermal soviel Exemplare ihre Käufer.\n\nZu Beginn des Jahres 1985 stellte Atari auf der CES in Las Vegas seine neueste Generation von Computern in Form der Atari-ST-Baureihe vor. Daneben hatte Atari auch seine 8-Bit-Heimcomputer, wie bereits im September des Vorjahres angekündigt, einer Verjüngungskur unterzogen. Die neuen Geräte verfügten über ein zeitgemäßes Gehäuse und verbesserte Technik – der ursprünglich bereits für den Atari 800XL gedachte Speicherverwaltungsbaustein \"FREDDY\" fand nun seinen Einsatz. Zunächst sollte lediglich der Atari 130XE mit seinen 128 KB Arbeitsspeicher in den Handel gelangen und zwar als Ergänzung zum Atari 800XL mit nur 64 KB Arbeitsspeicher.\n\nMit Erscheinen des Atari 130XE im Februar 1985 in den USA wurde der Preis des Atari 800XL – die reinen Herstellungskosten waren mittlerweile auf 80 US-Dollar gesenkt worden – weiter reduziert, in Großbritannien beispielsweise auf knapp unter 100 britische Pfund. Kurz darauf verstärkte Atari seine Bemühungen, insbesondere Neueinsteigern Diskettenlaufwerke durch günstige Komplettangebote schmackhaft zu machen, etwa in Form des \"Personal Computer Pack\" mit Computer, Diskettenlaufwerk Atari 1050 und Programme für etwa 250 britische Pfund. Für Westdeutschland sind keine derartigen Bündelangebote bekannt, dennoch wurden bis Mitte 1985 von den beiden Modellen Atari 600XL (Lagerbestände) und 800XL zusammen mindestens 100.000 Geräte verkauft.\n\nZur Ankurbelung der Verkäufe in Großbritannien gewährte Atari ab August 1985 Bildungseinrichtungen auf den unverbindlichen Verkaufspreis Rabatte von bis zu 25 Prozent. Eigens für Schulen wurde das Bündelangebot \"Atari LOGO System\" mit der einsteigerfreundlichen Programmiersprache Logo aufgelegt, um die Vormachtstellung von Acorns Computer BBC Micro in den staatlichen Bildungseinrichtungen zu brechen. Einen großen Erfolg erzielte Atari im September 1985 in den Niederlanden, wo man sich für den Atari 800XL als offiziellen Schulcomputer entschied. Neben den Anschaffungen der Bildungseinrichtungen selbst erhoffte sich Atari, dass Schüler und Studenten im Rahmen von späteren Privatkäufen auf das bereits aus der Schule Bekannte und Vertraute – einen Computer von Atari – zurückgreifen und damit allein in den Niederlanden Absätze in Höhe von etwa 100.000 Computern folgen würden.\n\nNachdem Atari im November 1985 die Einstellung der Produktion des Atari 800XL verkündet hatte, übernahm in Großbritannien die Warenhauskette Dixons die Vermarktung noch verbliebener Lagerbestände. Die von Dixons daraufhin rechtzeitig zum Weihnachtsgeschäft aufgelegten Bündelangebote markierten weitere Tiefstpreise im Heimcomputergeschäft: Ein Atari 800XL war nun zusammen mit dem Diskettenlaufwerk Atari 1050, einem Softwarepaket und Joystick für rund 170 britische Pfund erhältlich. Bis Weihnachten konnten so vermutlich 100.000 Computer allein in Großbritannien abgesetzt werden. In den USA war der Atari 800XL im Weihnachtsgeschäft zu einem ähnlich niedrigen Preis – unter 100 US-Dollar – erhältlich. In Westdeutschland führten Preise zwischen 200 und 250 DM ebenfalls zu gesteigerten Verkäufen, jedoch erreichte der Marktanteil des Atari 800XL für das Jahr 1985 nicht mehr als knapp sechs Prozent – der des unangefochtenen deutschen Marktführers Commodore 64 lag dagegen bei fast 40 Prozent; ebenfalls weit abgeschlagen folgten der Schneider CPC 464 mit ca. 15 Prozent und der Sinclair ZX Spectrum mit etwas über neun Prozent.\n\nLetzte größere Mengen des Atari 800XL waren in Großbritannien bis Februar 1986 veräußert worden. In Westdeutschland reichten die Lagerbestände dagegen bis zum zweiten Quartal 1987, obwohl im Jahr zuvor etwa 92.000 Computer ihre Abnehmer fanden. Als Nachfolger des Atari 800XL kam in Nordamerika und Großbritannien ab 1986 der zuvor schon in Kanada verkaufte Atari 65XE in die Läden. In Deutschland erschien der Nachfolger im Oktober in Form des zum Atari 65XE baugleichen Atari 800XE mit einem Preis von etwas unter 200 DM. Durch die Ausverkäufe war die Benutzerschaft für Ataris 8-Bit-Computer allein in Großbritannien bis Mitte 1986 auf etwa 300.000 Aktive angewachsen, in Nordamerika durch die Weihnachtsverkäufe im Jahr zuvor auf mehr als eine Million.\n\nMit Lockerung der Exportbeschränkungen für Hochtechnologiegüter Ende 1984 erfolgte die Ausfuhr des Atari 800XL ab 1985 auch in viele Länder des Ostblocks. Dort waren die Computer und Zubehör zwar bei staatlichen Handelsorganisationen, dafür aber nur gegen Devisen erhältlich. In der Deutschen Demokratischen Republik war das die Forum-Außenhandelsgesellschaft mit ihrem Intershop-Einzelhandelsnetz und dem Bezahlmittel der zur Deutschen Mark äquivalenten Forumschecks. In Polen erfolgte der Verkauf in Pewex-Filialen. Die ersten beiden 1985 von Pewex bestellten Chargen von insgesamt 5.500 Atari 800XL mit Zubehör fanden binnen weniger Tage ihre Abnehmer. Zum Schwarzmarktkurs in die Landeswährung Złoty umgerechnet, kostete ein Atari 800XL mit Datenrekorder Atari 1010 zunächst 150.000 Złoty und damit ungefähr das Jahresgehalt eines Universitätsdozenten in Polen. Nach der Freigabe des Handels durch die polnische Regierung und nach Ausweitung des Imports fiel der Preis auf umgerechnet 120.000–130.000 Złoty. Aufgrund dieses vergleichsweise geringen Preises verdrängten die Atari-Computer alsbald den zuvor häufig durch private Einfuhr beschafften Sinclair ZX Spectrum als den am weitesten verbreiteten Computer in Polen und stiegen so zum Marktführer auf. In der Tschechoslowakei boten Tuzex-Läden, ebenfalls für eine entsprechende Ersatzwährung, Atari-Computer zum Verkauf an.\n\nInfolge der rasch wachsenden Nachfrage gingen allein im Jahr 1987 etwa 100.000 Atari-8-Bit-Rechner in den Ostblock, davon 4.600 in die DDR – der Atari 800XL war dort bereits ab 1985 der erste offiziell importierte westliche Heimcomputer – und 10.500 in die ČSSR. In diesen beiden Ländern stiegen Ataris XL- und XE-Computer im Laufe des Jahres 1987 ebenfalls zum Marktführer auf. In den Folgejahren konnte die Marktführerschaft weiter gefestigt werden: allein in der Deutschen Demokratischen Republik waren bis zur Wende im November 1989 nur durch die landeseigenen Intershops etwa 100.000 Atari-Computer (verschiedener Baureihen) verkauft worden. Hinzu kamen weitere Geräte, die durch private Einfuhren oder durch Schenkungen von wohlmeinenden „West-Verwandten“ über Genex, einem Geschenkedienst der DDR, zu ihren Benutzern fanden.\n\nAuch in Westdeutschland waren die Verkaufszahlen inzwischen wieder stark gestiegen – bis Juli 1988 hatte Ataris eigenen Angaben zufolge dort seit Markteinführung etwa 500.000 Geräte verkauft. Zur Befriedigung der großen Nachfrage, die wohl nicht durch Ataris XE-Baureihe allein gedeckt werden konnte, nahm Atari eigenen Verlautbarungen nach im Juli 1988 die Produktion kurzerhand wieder auf. Die neu hergestellten Atari 800XL waren fortan zusammen mit dem Datenrekorder \"Atari XC12\" für knapp unter 200 DM erhältlich. \n\nDie überschaubare Architektur des Systems und umfangreiche Dokumentationen des Herstellers ermöglichen den miniaturisierten Nachbau der Elektronik des Atari 800XL und dazu kompatibler Modelle mit heutigen technischen Mitteln bei gleichzeitig überschaubarem Aufwand. Eine solche moderne Realisierung erfolgte erstmals 2014 – wie bei anderen Heimcomputersystemen auch – als Implementierung auf einem programmierbaren Logikschaltkreis (FPGA) nebst Einbettungssystem. Die Nachbildung mittels FPGA-Technologie war zunächst lediglich als technische Machbarkeitsstudie gedacht, stellte jedoch im Nachhinein auch ihren praktischen Nutzen unter Beweis: Durch die Miniaturisierung und die Möglichkeit des Batteriebetriebs ist sie eine leicht verstaubare, zuverlässig arbeitende und transportable Alternative zur originalen schonenswerten Technik.\n\nIm Gehäuse des Atari 800XL befindet sich eine einzelne Platine, die alle elektronischen und viele mechanische Komponenten wie Buchsen und Stecker des Computers enthält.\n\nDer Atari 800XL basiert auf einer Variante des 8-Bit-Mikroprozessors \"MOS 6502\", der häufig in zeitgenössischen Computern eingesetzt wurde. Im Unterschied zu den Vorgängermodellen Atari 400 und 800 wird beim Atari 800XL eine spezielle Variante des 6502 mit dem Namen \"Sally\" eingesetzt, die die Anzahl der elektronischen Bauelemente im Computer deutlich zu verringern half. Die CPU kann auf einen Adressraum von 65536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 Kilobytes (KB) festlegt. Der Systemtakt beträgt bei PAL-Geräten 1,77 MHz, für solche mit NTSC-Ausgabe dagegen 1,79 MHz.\n\nWesentlicher Bestandteil der Rechnerarchitektur sind die drei von Atari entwickelten Spezialbausteine \"Alphanumeric Television Interface Controller\" (\"ANTIC\"), \"Graphic Television Interface Adapter\" (\"GTIA\") und \"Potentiometer And Keyboard Integrated Circuit\" (\"POKEY\"). Sie sind funktionell derart konzipiert, dass sie innerhalb ihres Aufgabenbereiches flexibel einsetzbar sind und gleichzeitig die CPU entlasten.\nDie beiden Grafikbausteine ANTIC und CTIA/GTIA erzeugen das am Fernseher oder Monitor angezeigte Bild. Dazu sind zuvor vom Betriebssystem oder den Benutzer im Arbeitsspeicher entsprechende Daten in der Form der „Display List“ zu hinterlegen. Der CTIA/GTIA erlaubt unter anderem das Integrieren von maximal acht unabhängigen aber jeweils einfarbigen Grafikobjekten, den Sprites. Diese im Atari-Jargon auch „Player“ und „Missiles“ genannten Objekte werden gemäß benutzerdefinierbaren Überlappungsregeln in das vom ANTIC erzeugte Hintergrundbild kopiert und einer Kollisionsprüfung unterzogen. Dabei wird festgestellt, ob sich die Sprites untereinander oder bestimmte Teile des Hintergrundbildes („Playfield“) berühren. Diese Fähigkeiten wurden – wie sich bereits anhand der Namensgebung „Playfield“, „Player“ und „Missiles“ abzeichnet – zur vereinfachten Erstellung von Spielen mit interagierenden Grafikobjekten und schnellem Spielgeschehen entwickelt. Die Fähigkeiten der beiden Spezialbausteine ANTIC und CTIA/GTIA zusammengenommen, verleihen den Darstellungsmöglichkeiten der Atari-Rechner eine von anderen damaligen Heimcomputern unerreichte Flexibilität. Im dritten Spezialbaustein POKEY sind weitere elektronische Komponenten zusammengefasst. Diese betreffen im Wesentlichen die Tonerzeugung für jeden der vier Tonkanäle, die Tastaturabfrage und den Betrieb der seriellen Schnittstelle \"Serial Input Output\" \"(SIO)\" zur Kommunikation des Rechners mit entsprechenden Peripheriegeräten.\n\nDurch die hochintegrierte Ausführung (LSI) vereinen die Spezialbausteine viele elektronische Komponenten in sich und senken dadurch die Anzahl der im Rechner benötigten Bauteile, was wiederum eine nicht unerhebliche Kosten- und Platzersparnis mit sich bringt. Nicht zuletzt weil ihre Konstruktionspläne nie veröffentlicht wurden, waren sie mit damaliger Technik nicht wirtschaftlich zu kopieren, womit der in der Heimcomputerbranche durchaus übliche illegale Nachbau von Computern für den Atari 800XL ausgeschlossen werden konnte.\n\nDer von der CPU und ANTIC ansprechbare Adressraum segmentiert sich beim Atari 800XL in verschiedene Abschnitte unterschiedlicher Größe. Aus praktischen Gründen ist es üblich, für deren Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Ihr wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nDer Bereich von $0000 bis $BFFF ist hauptsächlich für Arbeitsspeicher vorgesehen. Dieser ist nicht vollständig durch den Benutzer verwendbar, denn fast im gesamten Bereich von $0000 bis $06FF hält das OS für den laufenden Betrieb benötigte Variablen vor. Wird der Selbsttest aktiviert, werden die zugehörigen Programmroutinen aus dem Festwertspeicher in den Adressblock von $5000 bis $57FF kopiert. Bei eingestecktem Modul mit 8 KB Festwertspeicher wird dessen Inhalt in den Bereich von $A000 bis $BFFF anstelle des dort sonst befindlichen BASIC eingeblendet. Verfügt das Steckmodul über 16 KB Festwertspeicher, reicht der Inhalt von $8000 bis $BFFF. Ab $C000 schließt sich das Betriebssystem an. Die Adressen der Spezialbausteine ANTIC, GTIA, POKEY und anderer Hardwarebestandteile befinden sich innerhalb eines von $D000 bis $D7FF reichenden \"Input/Output Block\" genannten Segmentes. Von $D800 bis zur oberen Speichergrenze $FFFF sind die restlichen Bestandteile des Betriebssystems und Treiber der über die parallele Schnittstelle angeschlossenen Geräte untergebracht. Durch Abschalten des Betriebssystems und BASIC können stattdessen Speicherbänke mit Arbeitsspeicher eingeblendet werden, so dass sich maximal 62 KB nutzen lassen.\n\nNach dem Einschalten des Rechners liest die CPU die Inhalte der ROM-Bausteine mit dem Betriebssystem aus und prüft zunächst den Modulschacht und startet gegebenenfalls das darauf befindliche Programm. Ist kein Modul vorhanden wird im nächsten Schritt der Status der Funktionstasten \"Option\" und \"Start\" abgefragt. Die gedrückte Option-Taste veranlasst das Betriebssystem das eingebaute BASIC des Computers zu deaktivieren und stattdessen beispielsweise ein ausführbares Programm von einem angeschlossenen Diskettenlaufwerk zu laden. Bei gleichzeitig gedrückter Start-Taste während des Einschaltens erfolgt das Laden eines ausführbaren Programms vom angeschlossenen Datenrekorder. Ist keine der beiden genannten Funktionstasten aktiv, startet der Computer das eingebaute BASIC und meldet sich mit blinkendem Cursor als bereit zur Befehlseingabe.\n\nAls Verbindungen zur Außenwelt dienen zwei Kontrollerbuchsen an der rechten Seite des Gehäuses, ein Schacht zur ausschließlichen Verwendung von ROM-Steckmodulen auf der Oberseite, ein koaxialer HF-Antennenanschluss für den Fernseher sowie eine Buchse der proprietären seriellen Schnittstelle (\"Serial Input Output\", kurz \"SIO\") auf der Rückseite. Letztere dient dem Betrieb von entsprechend ausgestatteten „intelligenten“ Peripheriegeräten, wobei ein von Atari speziell für diesen Zweck entwickeltes Übertragungsprotokoll und Steckersystem zum Einsatz kommen. Drucker, Diskettenlaufwerke und andere Geräte mit durchgeschleiften SIO-Buchsen können so mit nur einem einzigen Kabeltyp „verkettet“ angeschlossen werden. Daneben verfügt der Atari 800XL im Gegensatz zum Atari 1200XL über eine parallele Erweiterungsschnittstelle, deren Anschluss in der Rückseite des Gehäuses verbaut ist. Dieser herausgeführte Systembus erlaubt beispielsweise den Betrieb eines externen Bauelementeträgers wie der Erweiterungsbox \"Atari 1090\", die jedoch nie in den Handel gelangte.\n\nDer Atari 800XL funktioniert grundsätzlich mit allen von Atari für seine 8-Bit-Computer veröffentlichten Peripheriegeräten. Im Folgenden wird hauptsächlich auf die bis 1989 erhältlichen von Atari im XL-Design herausgebrachten kommerziellen Produkte eingegangen.\n\nIn Zusammenhang mit vor allem westlichen Heimcomputern der 1980er Jahre kamen zur Datensicherung hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die günstigste Variante der Datenaufzeichnung durch Kompaktkassetten hat im Allgemeinen den Nachteil niedriger Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren. Bei Veröffentlichung des Atari 800XL standen ihm Programmrekorder aber auch Diskettensysteme wie etwa die Floppy \"Atari 1050\" und wenig später auch Festplattensysteme als Massenspeicher zur Verfügung. Die noch zum Betrieb mit dem Atari 800 geeigneten Festplattensysteme der Firma Corvus sind aufgrund einiger beim 800XL nicht mehr vorhandener Anschlüsse nicht verwendbar.\n\nIm Gegensatz zu anderen zeitgenössischen Heimcomputern wie beispielsweise dem Tandy TRS-80 oder dem Sinclair ZX81 kann der Atari 800XL zum Speichern von Daten ab Werk nicht mit handelsüblichen Kassettenrekordern betrieben werden. Vielmehr benötigt er ein auf seine serielle Schnittstelle abgestimmtes Gerät – den Programmrekorder \"Atari 1010\". Die durchschnittliche Datenübertragungsrate beträgt dabei 600 Bit/s; auf einer 30-Minuten-Kassette finden 50 KB an Daten Platz. Das Gerät verfügt über einen Stereo-Tonkopf, wodurch parallel zum Lesevorgang das Abspielen von Musik oder gesprochenen Benutzungsanweisungen möglich ist. Aus Gründen der Kosten- und Platzersparnis wurde im Programmrekorder kein Lautsprecher eingebaut, die Audiosignale werden mithilfe des Spezialbausteins POKEY stattdessen am Fernsehgerät ausgegeben.\n\nZur Beschleunigung des Datentransfers kam Ende 1986 mit \"Rambit\" in Großbritannien eine kommerzielle Hardwarelösung nebst entsprechender Ansteuerungssoftware für knapp 20 britische Pfund in den Handel. Nach dem lötenden Einbau einer vom Hersteller bereits fertig bestückten Platine und dem Einsatz eines ebenfalls mitgelieferten Konverterprogrammes konnten mit dem modifizierten Atari-1010-Programmrekorder Daten mit Raten von 3300 bis 3600 Bit/s geladen werden.\n\nMit Einführung des Atari 800XL war auch ein gestalterisch darauf abgestimmtes Diskettenlaufwerk erhältlich, die Floppystation \"Atari 1050\". Mit dem Atari-1050-Diskettenlaufwerk können 5¼″-Disketten einseitig beschrieben werden, womit sich bis zu 127 KB Daten abspeichern lassen. Das ursprünglich mit dem Laufwerk ausgelieferte Diskettenbetriebssystem DOS 2.0s unterstützt lediglich einfache Schreibdichte (englisch \"Single Density\"). Damit lassen sich 88 KB an Daten auf einer Diskettenseite ablegen, wobei ein Sektor 128 Bytes enthält. Es wurde ab Mitte 1984 durch DOS 3.0 abgelöst, das die Atari-spezifische Betriebsart \"Enhanced Density\" unterstützt. Im Gegensatz zu doppelter Schreibdichte (englisch \"Double Density\") wird nicht die Byteanzahl pro Sektor, sondern die Anzahl der Sektoren pro Spur erhöht, was zu der proprietären Speicherkapazität von 127 KB führt. Hauptsächlich die Inkompatibilität von DOS 3.0 zu seiner Vorgängerversion DOS 2.0s führte schließlich 1985 zur Veröffentlichung des in vielerlei Hinsicht verbesserten DOS 2.5. Das Atari-1050-Diskettenlaufwerk kostete Mitte 1984 etwa 450 US-Dollar.\n\nZusammen mit Erscheinen des 800XL war eine Vielzahl von Atari-kompatiblen Diskettenlaufwerken diverser Dritthersteller erhältlich, die fast alle mit 5¼″-Disketten aber doppelter Schreibdichte arbeiteten. Dazu zählten Geräte von Percom, das \"Rana 1000\" für 400 US-Dollar und das Doppellaufwerk \"Astra 1620\" für 600 US-Dollar. Im Laufe des Jahres 1984 kamen weitere leistungsfähige Diskettenlaufwerke hinzu: das \"Percom AT-88\" für 420 US-Dollar, das \"Trak AT-D2\" für 500 US-Dollar, das für zweiseitiges Beschreiben von Disketten geeignete \"Trak AT-D4\" und das \"Indus GT\" für 500 Dollar. Eine Besonderheit bildeten das \"Amek AMDC I\" für 550 US-Dollar und das Doppellaufwerk \"Amek AMDC II\" für 760 US-Dollar, die beide auf damals selten gebrauchten 3″-Disketten basieren. Viele der Drittlaufwerke enthielten neben dem Anschluss- und Dokumentationsmaterial auch ein Diskettenbetriebssystem wie beispielsweise \"SmartDOS\" oder \"DOS XL\".\n\nIm Laufe des Jahres 1985 sanken die Anschaffungskosten für Diskettenlaufwerke, die Absätze stiegen und es wurden vermehrt Erweiterungen für das Laufwerk Atari 1050 auf den Markt gebracht. Damit war es möglich, die Zugriffszeiten von Ataris 1050 auf die Diskettendaten zu verkürzen und die Speicherkapazität mittels doppelter Schreibdichte pro Diskettenseite auf 180 KB zu erhöhen. Zu den bekanntesten dieser sogenannten Floppy-Speeder zählen \"US-Doubler\", \"Happy Enhancement 1050\", \"Super Archiver I\" & \"II\" mit oder ohne \"BitWriter\" und auch einige deutsche Produkte wie \"High Speed 1050\" vom Irata-Verlag, \"1050 Turbo\" von Gerhard Engl und diverse Ausführungen der \"Speedy 1050\" vom Compy Shop. Zum Lieferumfang dieser Modifikationen gehörte auch immer entsprechende Software wie beispielsweise \"SpartaDOS\", \"WarpDOS\" oder \"BiboDOS\". Mit Beginn der 1990er Jahre kamen mangels Versorgung mit Atari-Laufwerken diverse Eigenbauten in Kleinserie, wie beispielsweise die \"Floppy 2000 I\" und \"II\" von Klaus Peters Elektronik und polnische Fabrikate wie \"California Access CA-2001\" und \"TOMS 720\" hinzu.\n\nAb Anfang 1986 brachten Dritthersteller Festplattensysteme und dazu benötigte Software für den Atari 800XL auf den Markt. Zu den frühesten Anbietern zählt das Unternehmen Supra Corporation mit seinem \"Supra Drive\". Der Anschluss erfolgt über den herausgeführten Parallelbus, die Erweiterungsschnittstelle des Computers. Unter Zuhilfenahme mitgelieferter Systemprogramme wie etwa \"MyDOS\" lassen sich die 10 MB Speicherplatz des Laufwerkes vielfältig nutzen. Die im Vergleich zu normalen Diskettenlaufwerken kurzen Zugriffszeiten und die hohen Datentransferraten schlugen sich aber in einem relativ hohen Preis nieder: das Supra Drive war bei Erscheinen Anfang 1986 mit fast 800 US-Dollar etwa viermal so teuer wie das Diskettenlaufwerk Atari 1050. Das ab Ende 1986 hinzugekommene \"BTL Hard Disk System\" verfügt ebenfalls über eine Speicherkapazität von 10 MB, konnte vom Anwender jedoch bis auf 128 MB ausgebaut werden. Es wurde ebenfalls mit \"MyDOS\" ausgeliefert und kostete bei seiner Einführung etwa 600 US-Dollar. Ein weiteres Gerät, aber mit einer Kapazität von 20 MB und der Software \"SpartaDOS\" zu dessen Verwaltung, stellte das Unternehmen ICD Inc. im Jahr 1987 unter dem Namen \"FA-ST\" für rund 700 US-Dollar vor. 1989 kamen Festplattensysteme von Computer Software Services mit Kapazitäten von 5 bis 80 MB hinzu, die allesamt auf der Universalerweiterung \"The Black Box!\" basierten. Die Preise lagen zwischen 400 US-Dollar (10 MB) und knapp 900 US-Dollar (80 MB). \n\nAufgrund von Problemen bei der Übertragung von kopiergeschützten Programmen auf die Festplatten und in Anbetracht des hohen Preises fanden solche Systeme beim Großteil der Atari-Besitzer kaum Verwendung. Sie wurden hauptsächlich von Betreibern speicherplatzintensiver Mailboxen und bei professionellen Programmentwicklern eingesetzt.\n\nDie Bildausgabe des Atari 800XL kann via eingebautem HF-Modulator an einem handelsüblichen Fernsehgerät erfolgen. Eine bessere Bildqualität ermöglichen dagegen spezielle Monitore. Bereits im Mai 1985 standen für den Atari 800XL beispielsweise in Westdeutschland mehr als 15 verschiedene Monochrom-Monitore zur Verfügung, deren Preis zudem jeweils unterhalb von 500 DM lag. Von den teureren Farbmonitoren konnte der deutsche Benutzer zum gleichen Zeitpunkt aus acht verschiedenen Modellen mit Preisen von weniger als 1500 DM wählen. Ein von Atari eigens für die XL-Modellreihe produzierter Monitor existiert nicht.\n\nZur schriftlichen Fixierung von Text und Grafik dienen verschiedene Drucker, sowohl von Atari als auch von Drittherstellern. Mit Erscheinen des Atari 800XL bot Atari den Vierfarbplotter \"Atari 1020\" für 299 US-Dollar, den nadelbasierten Drucker \"Atari 1025\" und das mit Kugelkopf ausgestattete Schönschreibmodell \"Atari 1027\" an. Wollte der Benutzer dagegen einen der häufig leistungsfähigeren Drucker von Fremdherstellern anschließen, erforderte dies die Benutzung eines Zusatzgerätes, eines \"Printer Interfaces\". Angeschlossen an Ataris SIO-Buchse stellen diese Standardschnittstellen wie RS-232 oder Centronics nebst Anschlussbuchsen bereit. Erste Geräte erschienen 1984, womit Typenraddrucker wie der \"Transstar 120\", Tintenstrahldrucker wie der \"Hewlett-Packard Thinkjet\" und Nadeldrucker wie der \"Gemini 10X\" verwendet werden konnten. Neben den einfarbigen Druckern war ebenfalls der Einsatz von teureren farbfähigen Thermodruckern wie dem 1985 erschienenen \"Okimate 10\" und dem nadelbasierten Farbmodell \"Seikosha GP-700A\" möglich. Neben dem Printer Interface benötigen die Drucker zusätzlich spezielle Programme, die Gerätetreiber.\n\nIm September 1985 kam mit Ataris 1029 ein etwas leistungsfähigeres nadelbasiertes Modell hinzu, mit dem nun auch die Ausgabe von Grafik möglich wurde. Bereits Anfang 1986 hatten die Drucker von Fremdherstellern nach einer Umfrage der Computerzeitschrift \"Antic Magazine\" die mittlerweile in die Jahre gekommenen Modelle von Atari weitestgehend zugunsten etwa des \"Gemini 10X\", \"Star SG-10\" und diverser Modellen von Epson verdrängt. Die Auswahl der mit dem Atari 800XL benutzbaren Drucker hing auch in den Folgejahren in erster Linie von der Erhältlichkeit entsprechender Interfaces und zum Betrieb benötigter Gerätetreiber ab. Die Kompatibilität zu Epson-Druckern war dabei häufig Voraussetzung.\n\nVon Fremdherstellern existieren eine Fülle von Ausgabezusätzen wie die zur Sprachausgabe gedachte \"The Voicebox\" und \"The Voicebox II\" von The Alien Group sowie der 1986 hinzugekommene \"Voice Master\" von Covox aber beispielsweise auch eine selbstzubauende 3-D-Brille zum Betrachten von stereografischen Inhalten am Fernseher und ein programmierbarer Robotergreifarm.\n\nDie Schreibmaschinentastatur des Atari 800XL enthält insgesamt 56 Einzeltasten, eine Leer- und vier Funktionstasten. Die Bedienung des Computers kann alternativ über eine Maus erfolgen, wobei entsprechend darauf abgestimmte Programme wie beispielsweise Desktop-Publishing- oder Malprogramme Voraussetzung sind. Darüber hinaus stehen Trakbälle, Paddle-Controller und Joysticks verschiedenster Hersteller zur Verfügung, wobei Joysticks hauptsächlich zum Steuern von Spielen eingesetzt wurden.\n\nZur komfortableren Bedienung speziell von Malprogrammen etablierten sich rasch Grafiktabletts, die mithilfe einer berührungsempfindlichen Oberfläche die Position eines mitgelieferten Malstiftes (Stylus) bestimmen und durch entsprechende Software die gewünschten Aktionen am Bildschirm erzeugen. Im Gegensatz zu Zeichenprogrammen, die auf Joystickeingaben basieren, erlauben Grafiktabletts ein schnelleres und damit auch effizienteres Arbeiten insbesondere bei der Erstellung von Bildern. Zu den für die Atari-XL-Computer Ende 1984 erhältlichen zählten das \"Touch Tablet\" von Atari für etwa 90 US-Dollar, das \"Koala Touch Tablet\" von Koala Technologies für etwa 125 US-Dollar und das \"Power-Pad\" von Chalk Board. Die Benutzung von Grafiktabletts setzt dafür geeignete Grafikprogramme wie beispielsweise den \"Micro Illustrator\" voraus. Nachdem der Verkauf von \"Touch Tablet\" und \"Koala Touch Tablet\" eingestellt worden war, bot das Unternehmen Suncom mit der \"Animation Station\" ab 1987 eine Alternative zum Preis von rund 90 US-Dollar an.\n\nEine Alternative zur Eingabe von Grafikdaten per Joystick oder Grafiktablett stellt die Benutzung eines Lichtstiftes dar. Mithilfe dieses Geräts kann direkt auf dem Bildschirm gezeichnet beziehungsweise ein Programm bedient werden. Die Funktionsweise der Lichtstifte basiert auf der Positionsbestimmung des Elektronenstrahls eines Bildausgabegerätes und ist somit auf kathodenröhrenbasierte Geräte beschränkt. Ab Herbst 1984 waren Lichtstifte von vier verschiedenen Herstellern erhältlich: der \"Light Pen\" von Atari für knapp 100 US-Dollar, der \"Edumate Light Pen\" von Futurehouse für etwa 35 US-Dollar, der \"Tech Scetch Light Pen\" in verschiedenen Versionen ab 40 US-Dollar und der \"Mc Pen\" von Madison Computer für 49 US-Dollar. Im Lieferumfang enthalten war jeweils Software, wobei das von Atari auf Steckmodul gelieferte Malprogramm \"Atari Graphics\" als das leistungsfähigste eingestuft wurde.\n\nNeben wechselbaren Speichermedien wie Kassetten und Disketten existieren verschiedene weitere Möglichkeiten zum Datenaustausch zwischen Computern, auch verschiedener Bautypen. Dabei wird zwischen kabelgebundener und kabelloser Übertragung der Daten unterschieden. Entfällt aufgrund großer räumlicher Entfernung der Geräte eine direkte Verbindung via durchgehendem Kabel (z. B. die populäre Lösung \"SIO-2-PC\"), kann die Übertragung auch durch ein Telefon- oder Funknetz erfolgen. Allerdings eignete sich deren technischer Aufbau in den 1980er nicht zur direkten Versendung digitaler Daten, vielmehr mussten diese zuvor in übertragbare analoge Signale umkodiert und bei Empfang in digitale Daten rückkodiert werden. Diese Aufgaben des Modulierens und Demodulierens war speziellen Geräten, den Modems, vorbehalten. Hinzu kommt bei vielen Modems insbesondere bei denen von Drittherstellern zum Betrieb eine zusätzliche Schnittstelleneinheit, häufig \"Modem-Interface\" genannt.\n\nBei der Auswahl eines Modems spielten insbesondere in Westdeutschland rechtliche Gesichtspunkte wie die Zulassung durch die Deutsche Bundespost eine große Rolle. So war beispielsweise der Betrieb der in Nordamerika erschienenen Modems \"Atari 830\", \"Atari 835\" und \"Atari 1030\" wegen der fehlenden Postzulassung untersagt.\n\nZu den günstigsten und auch mit öffentlichen Fernsprechern betreibbaren Modems zählten bis Mitte der 1980er Jahre die Akustikkoppler. Sie waren jedoch langsam und wenig zuverlässig in der Datenübertragung, da die ausschließlich akustisch erfolgende Signalübertragung über den Telefonhörer durch Fremdgeräusche leicht gestört werden konnte. Für die nordamerikanischen Benutzer des Atari 800XL stand mit den Modem \"Atari 830\" ein solches Gerät mit einer Transferrate von 300 Baud zur Verfügung. Daneben boten eine große Anzahl von Drittherstellern ähnliche Geräte an. In Westdeutschland war Mitte 1985 beispielsweise das von Dynamics Marketing GmbH vertriebene und postzugelassene \"Ascom-Modem\" erhältlich.\n\nBei diesem Typ von Modem erfolgt das Einkoppeln der Daten auf elektrischem Wege direkt in die Telefonleitung, ohne den fehlerträchtigen Umweg über den Telefonhörer gehen zu müssen.\n\nDas von Atari für die XL-Serie bereitgestellte Modem \"Atari 1030\" überträgt Daten mit einer Rate von 300 Bit/s. Gegenüber den Modellen von Drittherstellern sind die Speicheranforderungen an den Computer gering, so dass es auch mit einem Atari 600XL mit nur 16 KB Arbeitsspeicher und ohne Diskettenlaufwerk eingesetzt wurde. Mit Anschaffungskosten von knapp 60 US-Dollar gehörte das Modem Atari 1030 Ende 1985 zu den günstigsten für die Atari-Computer erhältlichen. \"Hayes Smartmodem\", \"Signalman Express\" und Ataris \"SX-212\" waren ebenfalls leistungsfähige aber auch teurere Geräte, die Transferraten von bis zu 2400 Bit/s zuließen. Sinnvoll einsetzbar waren diese beispielsweise in Westdeutschland jedoch erst gegen Ende der 1980er Jahre, da Anfang 1986 kaum Mailboxen für auch nur 1200 Bit/s existierten.\n\nDiese im Amateurfunkbereich sehr beliebten Geräte stellen keine Verbindung mit einem konkreten Zielpunkt her: Vielmehr werden die Daten in ein Funkgerät eingespeist und anschließend als Radiowellen abgestrahlt, die jeder mit entsprechender Technik empfangen und verwerten kann. Sie eignen sich also insbesondere zum gleichzeitigen Verteilen von Daten an viele Empfänger. Die eingeschränkte Reichweite konnte durch technische Maßnahmen in vielen Fällen erheblich gesteigert werden und so regelrechte Netzwerke installiert werden. Die Übertragungsgeschwindigkeit war mit 300 Baud vergleichsweise gering, sie wurde jedoch vielfach durch die geringen Betriebskosten mehr als aufgewogen. Im Gegensatz zu telefonbasierten Übertragungsmethoden fielen nämlich kaum Gebühren an, die Mitte der 1980er Jahre insbesondere bei Ferngesprächen beträchtlich sein konnten. Zu den Ende 1985 für den Atari 800XL erhältlichen kommerziellen Geräten zählen Modems des US-amerikanischen Herstellers Kantronics wie \"Kantronics Interface II\" und Produkte von Macrotronics wie etwa \"RM 1000\".\n\nZur Steigerung des Leistungsfähigkeit standen dem Atari 800XL vielfältige Erweiterungen zur Verfügung, die im Wesentlichen in zwei Gruppen unterteilt werden können: Einbaulösungen – häufig verbunden mit Lötarbeiten an der Platine – und solche, deren Betrieb ausschließlich über die vom Computer bereitgestellten Schnittstellenbuchsen (Erweiterungs- und Modulschacht, Joystickanschlüsse, SIO) erfolgt. Der Vorteil der zweiten Gruppe bestand darin, dass der Computer nicht geöffnet werden musste und daher Garantieansprüche nicht erloschen. Im Folgenden werden ausschließlich kommerzielle Lösungen vorgestellt, die zudem Gegenstand der Begutachtung durch die zeitgenössischen Fachpresse waren, d. h. auch von der breiten Benutzerschaft wahrgenommen wurden.\n\nEinige für den Atari 800XL erhältlichen Speichererweiterungen erfordern zu ihrer Installation das Öffnen des Computergehäuses, andere wiederum werden über die Erweiterungsschnittstelle betrieben. Mit dem derart nachgerüsteten Zusatzspeicher und entsprechender Software wurden häufig virtuelle Diskettenlaufwerke (RAM-Disks) oder Datenpuffer für Drucker (englisch \"printer spooler\") realisiert. Damit die in einer RAM-Disk hinterlegten Daten beim Ausschalten des Computers nicht verlorengehen, verfügen einige der Erweiterungen über eine Batteriepufferung oder eine eigene Stromversorgung. Der an den Erweiterungsschacht des Atari 600XL anzuschließende Speicherzusatz \"Atari 1064\" kann konstruktionsbedingt nicht zur Aufrüstung des Atari 800XL verwendet werden.\n\nZu den bekanntesten Erweiterungen zählen \"Rambo XL\" mit 256 KB Arbeitsspeicher vom US-amerikanischen Hersteller ICD, \"Newell 256 KB\", \"Ramcharger\" von Magna Systems mit bis zu 1 MB RAM und speziell in Westdeutschland der 256-KB-Zusatz vom Compy Shop. Die ab Ende 1986 von ICD angebotene Multifunktionserweiterung \"ICD Multi I/O Board\" konnte wahlweise mit 256 KB oder 1 MB Arbeitsspeicher bestückt werden.\n\nDer Datenaustausch zwischen Atari 800XL und beispielsweise dem Diskettenlaufwerk Atari 1050 erfolgt mithilfe von Transfervorschriften, die nicht mit den damals üblichen Standards wie z. B. RS-232 verträglich sind. Sollen RS-232- oder auch Centronics-kompatible Geräte angesteuert werden, muss ein entsprechender Konverter zwischengeschaltet werden. Diese Schnittstelleneinheiten (englisch \"Interface Boxes\") bestehen häufig aus einer Kombination von Hard- und integrierter Software, in einigen Fällen sind sie mit zusätzlichem Arbeitsspeicher zum Zwischenspeichern von Druckerdaten ausgestattet.\n\nEnde 1984 waren zum Anschluss von Centronics-Druckern bereits verschiedene Konverter erhältlich, darunter z. B. \"MPP-1150 Printer Interface\" von Microbits Peripheral Products und \"Ape-Face\" von Digital Devices Corporation für knapp 100 US-Dollar. Andere wesentlich teurere Geräte verfügten zusätzlich über bis zu 512 KB Arbeitsspeicher, um auch umfangreichere Druckdaten zwischenspeichern zu können und damit den Computer zu entlasten. In Westdeutschland war ab 1985 mit der \"850XL Interface Box\" auch eine Variante mit zwei verschiedenen Schnittstellen, RS-232 und Centronics, erhältlich. Später kamen weitere Geräte mit verbesserten technischen Kenndaten hinzu, wozu vor allem Produkte von ICD wie das \"ICD Multi I/O Board\" und die \"P:R: Connection Box\" aber auch \"The Black Box!\" von Computer Software Services zu zählen sind.\n\nFür eine übersichtlichere und weniger ermüdende Anzeige der Bildinhalte dienen die für den Atari 800XL produzierten 80-Zeichen-Erweiterungen. Aufgrund der hohen horizontalen Auflösung von 560 Bildpunkten sind diese nicht zum Betrieb mit einem Fernseher geeignet, sondern erfordern entsprechende Computermonitore. Zu den bekannten Lösungen zählen \"ACE80XL\" von TNT-Computing und das von ICD entwickelte \"Multi I/O Board\" mit nachgerüsteter 80-Zeichen-Karte.\n\nEinige Erweiterungen zielen direkt auf einen Eingriff in die Systemarchitektur und dabei speziell auf die Funktion des Hauptprozessors. Entweder manipulieren sie diesen oder ersetzen ihn durch einen anderen Mikroprozessor. Zu den Geräten der ersten Gruppe sind die sogenannten \"Freezer\" zu zählen. Im laufenden Betrieb durch den Benutzer aktiviert, wird durch einen Freezer der weitere Programmablauf durch Anhalten des Hauptprozessors gestoppt und die Steuerung sämtlicher Systemfunktionen vom Freezer übernommen. Freezer sind dabei derart konstruiert, dass nach dem „Einfrieren“ Manipulationen am Systemzustand durch den Benutzer möglich sind. Dies reicht vom Ändern bestimmter Speicherbereiche bis hin zum Sichern des gesamten Systemzustands auf Diskette oder dessen Einladen von Diskette. Diese Funktionalitäten sind beispielsweise sinnvoll für die Fehleranalyse von Programmen, das Aushebeln von Kopierschutzmechanismen oder das Abspeichern eines anderweitig nicht sicherbaren Spielstandes. Die zweite Gruppe von Systemerweiterungen betrifft den Austausch des Hauptprozessors durch eine leistungsfähigere Variante oder einen anderen Prozessortypen, um beispielsweise auch Software von Fremdsystemen benutzen zu können.\n\nDer einzige als Hardwarelösung realisierte und kommerziell vertriebene Freezer für den Atari 800XL ist der \"Turbo Freezer XL\" von Bernhard Engl. Er war ab 1987 für rund 150 DM ausschließlich in Westdeutschland erhältlich. Der Anschluss erfolgt am herausgeführten Systembus, der Erweiterungsbuchse des Computers. Mit der ebenfalls am Erweiterungsport anzuschließenden Schnittstelleneinheit \"ATR-8000\" von SWP Microcomputer Products ist es möglich, mithilfe der darin verbauten Mikroprozessoren eine Vielzahl von Programmen für CP/M-Systeme und solche für IBM-kompatible Computer mit dem Atari 800XL als Terminal auszuführen. Das ab Ende 1988 beworbene \"Turbo-816\" enthält neben passender Ansteuerungselektronik den zum MOS 6502 abwärtskompatiblen 16-Bit-Mikroprozessor \"65816\" und ein daran angepasstes Betriebssystem. Um die Vorteile des alternativen Prozessors wie den größeren direkt benutzbaren Arbeitsspeicher voll ausschöpfen zu können, müssen jedoch vorhandene Programme modifiziert werden.\n\nSollen nach dem Einschalten Programme wie etwa die Systemsoftware unmittelbar zu Verfügung stehen, müssen sie in Festwertspeicher untergebracht sein. Dieser umfasste in den 1980er Jahren sowohl unveränderbare ROM-Bausteine als auch modifizierbare Varianten wie etwa EPROMs. Im Gegensatz zu den ROM-Bausteinen im Inneren des Atari 800XL oder in Steckmodulen, können Inhalte von EPROMs jederzeit wieder geändert werden. Neben einer Ultraviolett-Lampe zum Löschen des gesamten Inhalts bedarf es dazu eines sogenannten EPROM-Brenners nebst Software, eines speziellen externen Geräts häufig mit Nullkraftsockel und Elektronik zum Beschreiben („Brennen“) eines oder mehrerer EPROMs.\n\nDer via Steckmodulschacht zu betreibende \"ProBurner\" von Thompson Electronic galt Dezember 1985 als einer des besten für Ataris Heimcomputer und erlaubt die Benutzung vieler EPROM-Typen mit Speicherkapazitäten von 2 bis 16 KB. Speziell in Westdeutschland erhältlich war ab 1986 der modernere \"BiboBurner\" von Compy Shop, der EPROMs mit Speicherkapazitäten von bis zu 32 KB beschreiben kann. Ab 1990 kamen mit \"The Super E-Burner\" und dem noch später erschienenen \"The Gang Super E-Burner\" leistungsfähigere Varianten von Computer Software Services hinzu.\n\nZum Übertragen gedruckter oder als Videoaufnahme vorliegender Bilder in den Computer sind spezielle Konverter vonnöten, die Digitalisierer (englisch \"Digitizer\") und Scanner. Zum Einlesen von Videokamerabildern – was auch abgefilmte gedruckte Dokumente einschließt – diente der ab 1985 angebotene und 130 US-Dollar teure \"Computer Eyes Digitizer\" von Digital Vision. Zu dessen Lieferumfang zählte neben der Elektronik auch entsprechende Software. In Westdeutschland bot der Irata-Verlag mit seinem \"Digitizer\" ein ähnliches Gerät an. Zum direkten Einscannen von Papierdokumenten diente \"Easy Scan\" von Innovative Concepts. Das Gerät setzte zu seinem Betrieb jedoch einen Drucker voraus, auf dessen Druckkopf zuvor die Abtastoptik des Konverters vom Benutzer zu montieren war.\n\nZum Übertragen von analogen Tönen oder Sprache in eine computerverarbeitbare Form dient eine zweite Gruppe von Digitalisierern, die sogenannten \"Sound-Sampler\" und \"Midi-Interfaces\". Hierbei konnte der Atari-Benutzer auf verschiedene Geräte und Software zurückgreifen, die bekanntesten stammen von 2-Bit-Systems, Alpha Systems, Hybrid Arts und Wizztronics. In Westdeutschland war ab 1987 mit dem \"Sound ’n’ Sampler\" von Ralf David ebenfalls ein entsprechendes Gerät nebst Software erhältlich.\n\nDie Programmpalette für den Atari-800XL-Computer umfasste neben der von Atari und \"Atari Program Exchange\" (\"APX\") vertriebenen Auswahl kommerzieller Programme auch von Drittherstellern entwickelte und in Zeitschriften und Büchern publizierte Software (Listings) zum Abtippen. \n\nWie bei anderen Heimcomputern der 1980er Jahre auch erfolgte der Vertrieb kommerzieller Software auf verschiedenen Datenträgern. Die insbesondere bei Spieleherstellern beliebten preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes allerdings sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Zudem sind mit Datasetten bestimmte Betriebsarten wie die beispielsweise zum Betrieb von Datenbanken vorteilhafte relative Adressierung nicht möglich. Bei den in der Herstellung vielfach teureren Steckmodulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen von großem Vorteil war. Den besten Kompromiss zwischen Ladezeit, möglichen Betriebsarten, Verlässlichkeit und Speicherkapazität erzielten die Disketten. Deren Verwendung wurde bei Veröffentlichung des Atari 800XL durch die Diskettenlaufwerke von Atari und die von anderen Herstellern unterstützt. Durch die 1983 und 1984 noch sehr teuren Diskettenlaufwerke waren für die Atari-Computer Steckmodule und Kompaktkassetten bis dahin die am häufigsten verwendeten Datenträger. Diese Situation änderte sich erst, als Atari ab 1985 die Preise für das Diskettenlaufwerk 1050 merklich senkte.\n\nVon der in Umlauf befindlichen Software machten illegale Kopien („Raubkopien“) stets einen großen Teil aus – in den Ländern des Ostblocks war originale Software aus dem Westen bis zur Wende faktisch nie im Umlauf – und stellten damit kleinere Softwareentwickler häufig vor existentielle wirtschaftliche Schwierigkeiten. Daraufhin wurden zunehmend Kopierschutzsysteme insbesondere bei Spielen als der meistverkauften Software eingesetzt.\n\nDie Konfiguration und Initialisierung des Computers nach dem Einschalten beziehungsweise nach einem Reset fällt in den Aufgabenbereich des im Festwertspeicher untergebrachten Betriebssystems. Die Unterprogramme dieses 16 KB umfassenden \"Operating System (OS)\" steuern verschiedene Systemprozesse, die auch vom Benutzer angestoßen werden können. Dazu gehören die Durchführung von Ein- und Ausgabeoperationen wie etwa die Tastatur- und Joystickabfrage, Fließkommaberechnungen, die Abarbeitung von Systemprogrammen nach Unterbrechungen (Interrupts) und die Bereitstellung eines Unterprogramms zum Erzeugen verschiedener Grafikmodi. Gegenüber den Modellen Atari 400 und 800 verfügt das neue Betriebssystem über ein Diagnoseprogramm zum Selbsttest des Computers. Damit kann die Funktionsfähigkeit beispielsweise des Arbeitsspeichers oder der Tonerzeugung getestet werden. Da das auf dem Atari 1200XL basierende Betriebssystem des Atari 800XL nicht vollständig angepasst wurde, werden bei der Tastaturdiagnose Tasten angezeigt, die nur im Atari 1200XL vorhanden sind.\n\nDie Startadressen der einzelnen Unterprogramme sind an zentraler Stelle in Form einer Sprungtabelle zusammengefasst. Diese befindet sich bei allen Atari-Computern stets im selben Speicherbereich, womit die Kompatibilität mit früheren und späteren Betriebssystem-Revisionen gewahrt werden soll. Einige Programme benutzen jedoch entweder aus Unkenntnis ihrer Programmierer oder aus Kopierschutzgründen heraus diese Tabelle nicht, sondern rufen stattdessen die betreffenden Unterroutinen des Betriebssystems direkt auf. Da viele dieser Unterprogramme im Atari 800XL nun andere Speicherbereiche als noch beim Atari 400 und 800 belegen, führt deren Aufruf an der alten aber ungültigen Speicheradresse unweigerlich zu Programmabstürzen. Aus diesem Grunde werden einige Programme von Drittanbietern nicht korrekt auf den Atari-XL-Modellen ausgeführt. Atari veröffentlichte daraufhin mit der \"Translator Disk\" ein Programm, das die Inkompatibilitätsprobleme des Computers zumindest bis zum nächsten Warmstart behebt.\n\nBereits kurz nach Veröffentlichung der XL-Computer begannen sich alternative und erweiterte Betriebssysteme zu etablieren, häufig in Form von Zurüstplatinen. Dazu gehörte zum Beispiel \"Ramrod-XL\" mit dem auf EPROM befindlichen \"Omnimon XL\" wahlweise ergänzt um den \"Fastchip\" und \"Omniview XL\". Später kamen mit \"XOS/80\" von Computer Support, \"Boss II\" von Alien Macroware, \"OS Controller Board\" \"Expander\", \"6 System Switchbox\", \"Diamond OS\" und \"Ultra Speed Plus\" von Computer Software Services weitere alternative Betriebssysteme hinzu. Diese stellten dem Benutzer neben der gewünschten Kompatibilität zu den älteren Computermodellen Atari 400 und 800 auch erweiterte Funktionalitäten bereit. Dazu zählten beispielsweise optimierte Fließkomma- und Datentransferroutinen sowie leistungsfähige Werkzeuge zur Systemkontrolle und Fehlersuche. In Westdeutschland waren ab 1985 verschiedene Varianten des \"BiboMon\" auch in Verbindung mit dem \"Turbo-Freezer XL\" erhältlich.\n\nDiese Ergänzungen setzen auf das normale Betriebssystem in Verbindung mit dem Diskettenbetriebssystem (DOS) auf und erleichtern die Interaktion für den Benutzer. Sämtliche Aktionen, die andernfalls per Kommandozeile hätten ausgeführt werden müssen, finden nun menügesteuert in einer übersichtlichen fensterbasierten Umgebung statt. Die Bedienung der grafischen Elemente erfolgt dabei häufig über einen frei bewegbaren und meist pfeilförmigen Cursor. Zu den kommerziell erhältlichen Benutzeroberflächen zählten \"XL-TOS\", \"Diamond GOS\" und das zum Abtippen im deutschen Atari Magazin veröffentliche \"S.A.M.\" (\"Screen Aided Management\").\n\nWar die Bearbeitung einer Aufgabenstellung mit z. B. käuflich zu erwerbenden Programmen aus technischen oder wirtschaftlichen Gründen nicht möglich oder sollte beispielsweise neuartige Unterhaltungssoftware produziert werden, so musste dies mithilfe von entsprechenden Programmiersprachen in Eigenregie geschehen. Laut einer Ende 1988 von der auflagenstarken Zeitschrift \"Antic Magazine\" durchgeführten Umfrage war BASIC unter den Atari-Benutzern mit deutlichem Abstand die meistgenutzte aller Programmiersprachen.\n\nDie Erstellung von schnellen Actionspielen mit vielen bewegten Objekten auf dem Bildschirm erforderte Anfang der 1980er Jahre eine optimale Nutzung der Hardware insbesondere des Arbeitsspeichers. Im Heimcomputerbereich war dies ausschließlich durch die Verwendung von Assemblersprache mit entsprechenden Übersetzerprogrammen, den Assemblern, möglich. Die Auslieferung von Assemblern erfolgte in vielen Fällen mit einem zugehörigen Editor zur Eingabe der Programmanweisungen („Sourcecode“), häufig auch als Programmpaket mit Debugger und Disassembler zur Fehleranalyse.\n\nMit Einführung des Atari 800XL standen diesem ausgereifte und leistungsfähige Assembler, die zuvor für Atari 400 und 800 veröffentlicht worden waren, zur Verfügung. Einige dieser Assembler wie etwa der \"Synassembler\" von Synapse Software sind jedoch nur mit dem alten Betriebssystem oder entsprechenden Anpassungen lauffähig. Unter der Vielzahl der angebotenen Assembler galt der \"MAC/65\" von \"Optimized System Software\" als der mit Abstand beste und benutzerfreundlichste. Ergänzt durch den \"Ultra Disassembler\" von Adventure International zur Programmanalyse blieben für den ambitionierten Programmentwickler bereits Ende 1984 kaum Wünsche offen. Zu den bekanntesten Assemblern in Westdeutschland gehörten der 1985 erschienene \"Atmas II\" von Peter Finzel und der \"Bibo-Assembler\" vom Compy Shop.\n\nProgrammiereinsteiger zogen in vielen Fällen die übersichtlichen und einfach zu bedienenden, dafür aber weniger leistungsfähigen Programmier-Hochsprachen wie BASIC vor.\n\nDem von Atari veröffentlichten BASIC in den Revisionen \"B\" und \"C\" (ab Februar 1985) standen einige weitere zur Seite: Das den damaligen Quasi-Standard bildende Microsoft BASIC und ein zum Atari-BASIC abwärtskompatibles Produkt mit dem Namen \"BASIC XL\" von Optimized System Software. Insbesondere BASIC XL enthält erweiterte Editiermöglichkeiten, Vereinfachungen in der Befehlsstruktur und es ergänzt viele im Atari- und Microsoft-BASIC nicht implementierte Leistungsmerkmale. Dazu zählt beispielsweise eine bequeme Benutzung der Sprites („Player-Missiles-Grafik“) durch eigens dafür bereitgestellte Befehlswörter. Ende 1985 erschienen mit \"Advan BASIC\" und \"Turbo-BASIC XL\" zwei weitere leistungsfähige Programmiersprachen für den Atari 800XL.\n\nNeben der Programmiersprache BASIC in ihren verschiedenen Versionen waren mit Verkaufsstart des Atari 800XL auch für Schulungszwecke geeignete Sprachen wie \"Atari Logo\" und \"Atari PILOT\" erhältlich, die häufig in Bildungseinrichtungen eingesetzt wurden. Unterstützt durch Elemente wie die \"turtle graphics\" (Schildkrötengrafik) ist beispielsweise mit Logo eine kindgerechte und interaktive Einführung in die Grundlagen der Programmierung möglich. Mit \"QS-Forth\" von Quality Software, \"Extended fig-Forth\" von APX, \"English Software Forth\", \"Elcomp Forth\", \"Go-Forth\" von Red Rat Software und \"Inter-LISP/65\" von Datasoft reihen sich weitere Interpretersprachen in die Programmpalette für den Atari 800XL ein.\n\nNachteilig auf die Einsetzbarkeit von Interpreter-Programmen wirkten sich die in der Natur des Interpreters liegenden prinzipiellen Beschränkungen wie etwa die geringe Ausführungsgeschwindigkeit und der große Arbeitsspeicherbedarf aus. Diese Nachteile können durch spezielle Programme, die Compiler, abgemildert werden. Dabei werden ausführbare Maschinenprogramme erzeugt, die ohne Interpreter lauffähig sind und damit häufig eine schnellere Ausführung erlauben. Für das Atari BASIC stehen mit \"ABC BASIC Compiler\" von Monarch Systems, \"Datasoft BASIC Compiler\" von Datasoft und \"BASM\" von Computer Alliance verschiedene Compiler zur Verfügung. Ende 1984 erschien mit dem BASIC-Compiler von MMG der zu diesem Zeitpunkt leistungsfähigste für die XL-Computer. Ergänzt wurde die Programmpalette durch die Ende 1985 herausgebrachten Compiler für \"Advan BASIC\" und \"Turbo-BASIC XL\".\n\nVon den damals weitverbreiteten Compilersprachen \"C\" und \"Pascal\" existieren entsprechende Versionen auch für die XL-Computer. Dazu zählen \"Deep Blue C\" von Antic, \"C/65\" von Optimized Systems Software, \"Lightspeed C\" von Clearstar Softechnologies und \"DVC/65\" sowie \"Atari Pascal\" von APX, \"Draper Pascal\" in verschiedenen Versionen von Norman Draper und \"Kyan Pascal\" ebenfalls in verschiedenen Versionen von Kyan Software. Als leistungsfähigste aller Programmiersprachen galt das ausschließlich für Atari-Computer erhältliche \"Action!\" von Optimized System Software, das Elemente von C und Pascal sowie speziell auf Ataris Hardware abgestimmte Befehle in sich vereint. Eine Besonderheit unter den für die 8-Bit-Atari-Computer erhältlichen Compilersprachen ist das Anfang 1987 beim Verlag Rätz und Eberle erschienene \"MASIC\". Es dient ausschließlich zur Erstellung von unabhängigen Musik-Unterprogrammen zur Einbindung beispielsweise in Spiele oder Demonstrationen.\n\nDie Programmpalette für die Atari-8-Bit-Computer umfasste bis 1985 neben den Programmiersprachen zum Erstellen eigener Applikationen eine im Vergleich zum zeitgenössischen Konkurrenten Apple II lediglich kleine Auswahl an vorgefertigter kommerzieller Anwendungssoftware.\n\nZu den leistungsfähigsten Textverarbeitungsprogrammen bei Markteintritt des Atari 800XL zählten \"Atari Writer\" von Atari (\"Atari Schreiber\" in Westdeutschland), \"Bank Street Writer\" von Broderbund, \"Letter Perfect\" von LJK Enterprises und \"The Writer’s Tool\" von Optimized System Software. Für Kontierung und weitere betriebswirtschaftliche Aufgaben im häuslichen Bereich standen Ende 1984 \"VisiCalc\" von Visicorp, \"The Home Accountant\" von Continental Software, \"Data Perfect\" von LJK Enterprise, Synapses Programme \"Synfile+\", \"Syncalc\", \"Synstock\" und \"Syntrend\" sowie \"Complete Personal Accountant\" von Futurehouse zur Verfügung. Hinzu kamen zahlreiche Joystick-basierte Malprogramme wie \"Paint\" von Atari, \"Graphic Master\" und \"Micropainter\" von Datasoft, \"Moviemaker\" von Reston Software und \"Fun with Art\" von Epyx. Mit dem Sprachsyntheseprogramm \"S.A.M. – Software Automated Speech\" von Tronix und dem \"Advanced Musicsystem\" von APX waren zudem sehr gut bewertete Programme zur Steuerung der Tonausgabe erhältlich.\n\nIm Laufe des Jahres 1985 wurde die Programmauswahl um \"Print Shop\" von Broderbund, \"Paperclip\" von Batteries Included, \"Atariwriter+\" von Atari, \"Austrotext\" von Austro.com, \"Proofreader\" von Atari und \"StarTexter\" vom Sybex-Verlag weitere leistungsstarke Anwendungen im Layout- und Textverarbeitungsbereich erweitert. Datenbanken und Kleinanwendungen für das Rechnungswesen standen mit \"Austrobase\" von Austro.com, \"Business Inventory System\" von CodeWriter und \"Silent Butler\" von Atari zur Verfügung.\nDie Auswahl der Malprogramme bekam mit Antic’s \"RAMbrandt\" und den \"Micro Illustrator\" von Koala Technologies weitere Möglichkeiten, die der Musikprogramme mit \"MIDICom\" von Hybrid Arts, \"Music Studio\" von Activision und \"SoftSynth\" aus der deutschen Zeitschrift Happy Computer.\n\nDas Jahr 1986 brachte dem an Textverarbeitung und Desktop Publishing interessiertem Benutzer den \"First Xlent Word Processor\" von Xlent Software und \"AwardWare\" von Hi Tech Expressions. Daneben erschienen für das Rechnungswesen \"B/Graph\" von Ariola und \"Back to Basics Accounting System\" von Peachtree Software. Vervollständigt wurden die Neuerscheinungen durch die Grafikprogramme \"Blazing Paddles\" von Baudville, \"Design Master\" von Peter Finzel Productions, \"Envision\" von Antic Software und \"Technicolor Dream\" von Red Rat Software. Hybrid Arts ergänzte sein Midi-Portfolio um \"MIDI Music System\" und \"Oasis\". Mit \"Soundmachine\" erschien ein in Westdeutschland produziertes Musikprogramm.\n\nMit \"LuxGraph XL\", \"MiniOffice\", \"Newsroom\" von Springboard Software, \"Print Star\" vom AMC-Verlag, \"S.A.M. – Screen Aided Management\" vom Atari Magazin und \"SX Express!\" von Atari wurde die Vielfalt der Anwendungen 1987 und 1988 noch einmal erweitert.\n\nEntsprechend der Ausrichtung der Vorgängermodelle Atari 400 und 800 auch als Lerncomputer existiert eine Unmenge an Programmen, die dem computergestützten Vermitteln von Lehrinhalten und seiner anschließenden interaktiven Abfrage dienen. Das zu vermittelnde Wissen wird in spielerischer Form mit ständig steigendem Schwierigkeitsgrad präsentiert, um den Lernenden anhaltend zu motivieren. Dabei wird großer Wert auf eine altersgerechte Darbietung gelegt, die von Kleinkindern bis hin zu Studenten reicht. Bei den Jüngsten kommen häufig animierte Geschichten mit comicartigen Charakteren als begleitende Tutoren zum Einsatz, bei Jugendlichen werden abzufragende Lehrinhalte in Abenteuerspiele oder actionsreiche Weltraumabenteuer gekleidet, bei den höherstufigen Lehrinhalten für Studenten und Erwachsene überwiegt hingegen meist lexikalisch präsentiertes Wissen mit anschließender Abfrage nebst Erfolgsbilanzierung. Die von den Ende 1984 mit mehr als 100 Titeln abgedeckten Lerngebiete erstrecken sich auf Lesen und Schreiben, Fremdsprachen, Mathematik, Technik, Musik, Geographie, Demografie, Tippschulen und Informatik.\n\nZu den bekannten Herstellern zählen American Educational Computers, Atari, APX, Carousel Software, CBS Software, Walt Disney Productions, Dorsett Educational Systems, Edupro, Electronic Arts, The Learning Company, Maximus, Mindscape, PDI, Prentice Hall, Scholastic, Screenplay Computer Software, Sierra On-Line, Spinnaker Software, Sunburst Communications, Unicorn Software und Xerox-Weekly Reader.\n\nEin großer Teil der mit dem Atari 800XL verwendbaren Spiele stammt aus dem Zeitraum 1979 bis 1983 von den technisch weitgehend kompatiblen Vorgängermodellen Atari 400 und 800. Diese Versorgung mit hochwertigen Programmen ebbte mit der Übernahme Ataris durch Jack Tramiel im Juli 1984 und seinen zunächst unbekannten Zukunftsplänen merklich ab. Viele Softwareentwickler sahen sich mit wirtschaftlichen Unwägbarkeiten konfrontiert und wandten sich stattdessen vielversprechenderen Systemen wie etwa dem Commodore 64 zu. Dieser Trend setzte sich auch nach der weitestgehenden wirtschaftlichen Erholung Ataris Anfang 1985 fort, bevor ab Mitte 1985 bis 1987 wieder einige Titel – Konvertierungen zumeist – auch in den USA erschienen. Programme in nennenswerter Stückzahl kamen von Ende 1986 an lediglich in Europa auf den Markt, darunter insbesondere Spiele im Niedrigpreissegment („Low-Budget“), bevor die großflächige Softwareversorgung Ende 1989 auch dort zusammenbrach. Die Bezugsmöglichkeiten beschränkten sich fortan auf Zeitschriften und kleinere Versandhändler. Durch die umfangreichen Computerverkäufe in den Ostblock und die darauf gründende Softwarenachfrage bildete sich nach der Wende 1989 in Polen noch für einige Jahre eine eigene Herstellerlandschaft heraus: Neugründungen wie \"Laboratorium Komputerowe Avalon\", \"Mirage Software\" und \"A.S.F.\" produzierten und vertrieben mehr als 140 Spiele, Mirage Software sogar bis ins Jahr 1995 hinein.\n\nIn den 1980er Jahren spielten neben den Fachbüchern die Computerzeitschriften für viele Heimcomputerbesitzer eine große Rolle. Die häufig monatlich erschienenen Ausgaben enthielten Testberichte zu Neuheiten, Programmieranleitungen und Software zum Abtippen. Sie dienten weiterhin als Werbe- und Informationsplattform sowie zur Kontaktaufnahme mit Gleichgesinnten.\n\nSpeziell mit den Atari-Heimcomputern befassten sich im englischsprachigen Raum die Magazine \"Antic\", \"Analog Computing\", \"Atari Connection\", \"Atari Age\", \"Atari User\", \"Current Notes\" und \"Page 6\"; gelegentliche Berichte und Programme für die Atari-Rechner veröffentlichten unter anderem auch die auflagenstarken \"Byte Magazine\", \"Compute!\" und \"Creative Computing\". Im deutschsprachigen Raum erschienen regelmäßig Berichte in \"Aktueller Software Markt\", \"Chip\", \"Computer Kontakt\", \"Happy Computer\", \"Homecomputer\" und \"P.M. Computerheft\"; ausschließlich mit Atari-Themen befassten sich \"Atari Magazin\" und \"Zong\". Die Benutzer in Frankreich wurden von \"L’Atarien\", \"Pokey\" und \"Tilt\" mit Informationen und Programmlistings versorgt. In Polen enthielten \"Bajtek\" und \"Komputer\" häufig Beiträge zum Thema Atari.\n\nNach dem Ende der Heimcomputerära Anfang der 1990er Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Ende der 1990er Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripheriegeräten entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reichte mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit u. a. ein verstärktes Transferieren von sonst möglicherweise verloren gegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird.\n\nAls leistungsfähigste Emulatoren für Windows und Linux-Systeme gelten \"Atari++\", \"Atari800Win Plus\", \"Mess32\" und \"Altirra\".\n\nDie Fachpresse bescheinigte dem Atari 800XL kurz nach Erscheinen einstimmig eine gute Verarbeitung, wobei die Meinungen einzig bei der Qualität der Tastatur und der Güte des externen Netzteils auseinandergingen. Zu gefallen wusste zudem die große Auswahl an Programmen insbesondere der Spiele und die große Menge an Peripherie. Einigen Rezensenten entging jedoch nicht, dass das eigentlich fehlerbereinigt hätte sein sollende \"BASIC Revision B\" stattdessen mit neuen wenn auch weniger gravierenden Fehlern aufwartete. Weiterhin bemängelt wurde vor allem die nicht vollständige Abwärtskompatibilität zu den Modellen Atari 400 und 800, wobei man die von Atari wenig später bereitgestellte Lösung in Form der \"Translator Disk\" einhellig begrüßte. Auf Unverständnis stieß dagegen die fehlende Anschlussmöglichkeit für handelsübliche Kassettenrekorder, womit man Ataris eigene Geräte zu kaufen gezwungen war. Vor allem in Großbritannien wurden Stimmen laut, die die vergleichsweise hohen Preise für die Software kritisierten. In Westdeutschland bemängelte man zusätzlich das Fehlen einer deutschen Tastatur und das im internationalen Zeichensatz nicht enthaltene ‚ß‘. Insgesamt konnten die Leistungsdaten jedoch überzeugen, obgleich man die grundlegende Systemarchitektur als bereits in die Jahre gekommen ansah. Sie zähle aber dennoch zum Besten im Heimcomputerbereich:\n\nNach der Übernahme Ataris durch Jack Tramiel und den damit verbundenen starken Preissenkungen gegen Ende 1984 rückte der Atari 800XL erneut in den Fokus vieler Rezensenten. Das auflagenstärkste aller Computermagazine \"Byte\" beispielsweise folgte dabei im Wesentlichen den bereits zuvor veröffentlichten Rezensionen, verwies jedoch zusätzlich auf die mittlerweile gut organisierte Nutzerschaft und hob insbesondere das vorteilhafte Preis-Leistungs-Verhältnis hervor, das den Atari 800XL zu einem Schnäppchen mache. Die mit den Preissenkungen verbundene überaus positive Wahrnehmung des Atari 800XL kulminierte Mitte 1985 schließlich in der Auszeichnung \"Home Microcomputer Award 1985\":\n\nDie Systemarchitektur der Atari-Computer wurde rückblickend einstimmig als bahnbrechend und als Wegbereiter vieler späterer Systeme gesehen. Der Einschätzung mehrerer Autoren nach haben „Probleme in der Produktion“ und damit verbundene vorweihnachtliche Lieferschwierigkeiten des Jahres 1983 hervorgerufen durch „interne Veränderungen bei Atari“ die potentiell erreichbare Marktmacht des Atari 800XL nachhaltig geschmälert. Verpasste Marktanteile seien so hauptsächlich dem sich bereits etablierenden Commodore 64 zugefallen, wovon sich der zunächst auch relativ teure Atari 800XL nie habe erholen können. Hinzu kämen noch technische Vorteile des Commodore wie seine farbigen Sprites, die Voraussetzung für viele Spieleinnovationen gewesen seien und daher dem diesbezüglich unterlegenen Atari 800XL ab 1985 ein „Schattendasein“ zugewiesen hätten. Dennoch „verkaufte sich die 8-Bit-Reihe von Atari auf beiden Seiten des Atlantiks ganz ordentlich“, „aber nicht so gut, wie sie es verdient gehabt hätte.“\n\nDer Atari 800XL ist ständiges Ausstellungsstück unter anderem im Computermuseum Oldenburg.\n\n\n"}
{"id": "32521", "url": "https://de.wikipedia.org/wiki?curid=32521", "title": "Schachcomputer", "text": "Schachcomputer\n\nSchachcomputer (auch Schachrechner) sind spezialisierte Computer zum Spielen von Schach. Sie enthalten ein als Firmware eingebautes Schachprogramm und bilden die Gerätebasis für Computerschach.\n\nDer erste Automat, der angeblich Schach spielen konnte, war der „Schachtürke“ um 1770 von Wolfgang von Kempelen (1734–1804). Die Maschine bestand aus einer türkisch aussehenden lebensgroßen Figur, die an einem schreibtischgroßen Schrank auf einem Stuhl saß. Der Schrank enthielt eine komplizierte Mechanik, die vor jedem Spiel durch Öffnen der Türen in einer festgelegten Reihenfolge gezeigt wurde, um die Abwesenheit von Menschen zu beweisen. Der Apparat konnte aber nicht selbständig Schach spielen, denn die Maschine war „getürkt“ – wie es später zum geflügelten Wort werden sollte:\nEinige der besten Schachmeister dieser Zeit bedienten den Türken, der der Überlieferung zufolge sehr selten ein Spiel verlor. Zuletzt bediente ihn der Franzose Schlumberger, unter anderem auf einer mehrjährigen Tournee durch die Vereinigten Staaten. Sein Tod im Jahre 1838 beendete die aktive Laufbahn der Maschine, deren Geheimnis offiziell nie gelüftet wurde. Auch Berühmtheiten wie Maria Theresia, die die Auftraggeberin der Maschine war, und Napoleon spielten gegen den Schachtürken; Letzterer verlor drei Partien gegen den „Automaten“, der zu der Zeit von dem ersten bedeutenden deutschsprachigen Schachmeister Johann Baptist Allgaier gesteuert worden sein soll.\n\nDen ersten echt schachspielenden Automaten konstruierte der Spanier Leonardo Torres Quevedo. Als Spezialist für Steuerungssysteme baute er eine elektromechanische Maschine namens \"El Ajedrecista\" (), die das Endspiel König und Turm gegen König ausführen konnte. Erstmals vorgeführt wurde der Apparat 1914 in Paris, heute steht er in der Polytechnischen Universität von Madrid. Obwohl es sich dabei noch nicht um einen Computer handelte, war es doch eine elektrotechnische Meisterleistung, die Mechanik und Relaistechnik verbindet. Der Automat löste die gestellten Probleme allerdings nicht immer optimal in den wenigstmöglichen Zügen. Seit dieser Zeit lag die mechanische Schachforschung zunächst weitgehend brach, bis in den 1950er Jahren Digitalcomputer entwickelt wurden und die Elektronik die Mechanik hier ablösen sollte.\n\nDie Geschichte des Schachcomputers hängt ab jetzt sehr eng mit der Entwicklung von Schachprogrammen zusammen und lässt sich zumeist nicht mehr getrennt behandeln. Hier wird jetzt nur noch die Hardware beschrieben. Zur Entwicklung der Algorithmen siehe den Artikel „Schachprogramm“.\n\nBelle war eine festverdrahtete Maschine, die 1979 von Ken Thompson und Joe Condon in den Bell Laboratories in New Jersey entwickelt wurde. Sie konnte bis zu 180.000 Stellungen in der Sekunde erzeugen und erreichte eine Suchtiefe von bis zu neun Halbzügen. Belle dominierte die Computerschachszene bis 1983. Ihr wurde im gleichen Jahr von der US-Schachföderation der Titel eines Nationalen Meisters verliehen. Dies war die erste Auszeichnung dieser Art für einen Schachcomputer.\n\nSeit 1975 arbeitete Robert Hyatt, ein Professor aus Mississippi, an einem Programm namens \"Blitz\" und wandte sich im Jahr 1979 an die Forschungsabteilung der Firma Cray, die ihm einen Spitzenrechner (Cray-1) zur Verfügung stellte. In Zusammenarbeit mit Al Gower, einem Musikprofessor und Fernschachspieler, entwickelte Hyatt \"Cray Blitz\". Doch trotz der Rechenleistung eines solchen Supercomputers (Cray-1 war damals die schnellste Rechenanlage der Welt) war Belle nicht zu besiegen.\n\nErst mit Cray X-MP wurde Belle im Jahre 1983 von \"Cray Blitz\" geschlagen. \"Cray Blitz\" war in Fortran, C und Cray-Assembler programmiert. \"Cray X-MP\" hatte 16 Prozessoren und eine Leistung von 13.000 Mips. In dieser Konfiguration kostete der Rechner etwa 50 Millionen Dollar. Die Schachberechnungen wurden mit Hilfe eines speziellen Algorithmus auf die Prozessoren verteilt. \"Cray Blitz\" wurde 1983 und 1986 Computerschach-Weltmeister. \"Cray X-MP\" war kein Schachcomputer im engeren Sinne, sondern ein Universalrechner, auf dem in der Regel andere Programme liefen.\n\nDas Programm Crafty ist ein direkter Nachfahre von Cray Blitz und wird immer noch von Robert Hyatt weiterentwickelt.\n\nTrotz des Sieges der Universalmaschine Cray X-MP (in weiteren Spielen siegte auch \"Belle\" wieder) zeigte das Kosten-Nutzen-Verhältnis, dass in Spezialhardware noch ein enormes Potenzial lag: \"Belle\" kostete nur ein Tausendstel einer Cray-Maschine.\n\nAn der Carnegie-Mellon-Universität in Pittsburgh, USA, baute Hans Berliner einen Spezialrechner namens \"HiTech\". Obwohl es Berliner eher zur B-Strategie hinzog, baute er einen Brute-Force-Rechner mit 64-Prozessoren, der 120.000 Stellungen in der Sekunde untersuchte. Trotz einiger Erfolge blieb der große Durchbruch aus.\n\nDeep Thought war eine Vorentwicklung von Deep Blue, der Superschachrechner von IBM, dem letzten Spezialrechner seiner Art. Da PC-Schachprogramme heutzutage alle menschlichen Schachspieler mühelos schlagen, ist auch das Interesse an schachspielenden Groß- und Spezialrechnern zurückgegangen.\n\nDer Schachcomputer Hydra ist eine leistungsstarke Maschine. Sie ist eine Mischung aus Standard-Hardware (Linux-Computercluster von derzeit 32 Intel-Xeon-Prozessoren) und 32 FPGA-Karten für die Stellungsbewertung. Hydra versucht die Baumsuche zu parallelisieren. Er wird vom Österreicher „Chrilly“ Donninger, den Deutschen Ulf Lorenz und Christopher Lutz sowie dem Unternehmen PAL Computer Systems aus Abu Dhabi entwickelt.\n\nMit dem Fidelity Chess Challenger 1 erschien 1977 der erste kommerzielle Schachcomputer. Dies war der Startschuss zu einer stürmischen Entwicklung von immer neuen und leistungsfähigeren Schachcomputern in den 1980er Jahren. Bei frühen Schachcomputern musste man seinen Zug noch per Tastatur eingeben.\n\nDer Fidelity Elite A/S (Abbildung rechts) konnte jedoch bereits 1983 die Bewegung der Figuren selbst erkennen, wie bei heutigen Schachcomputern (z. B. durch kleine Magnete im Fuß der Figur), die meist die Form eines Schachbretts haben.\n\nEinen besonderen Weg ging die Schachcomputerentwicklung in der DDR, wo man den Schwerpunkt auf die Entwicklung von Schachcomputern für gehobene Ansprüche legte, insbesondere auch zwecks Deviseneinnahmen aus dem Export ins westliche Ausland.\n\nIm Jahr 1983 kam der MB Milton Bradley Milton auf den europäischen Markt. Dieser hatte im Gehäuseunterteil eine Robotereinheit, die mechanisch wie ein Plotter aufgebaut war. Diese Einheit konnte die Figuren per Elektromagnet von unten über das Schachbrett ziehen. Der MB Milton hieß auf dem amerikanischen Markt «MB Grandmaster». Beide waren in weiten Teilen baugleich mit weiteren Schachrobotern wie dem «Fidelity Phantom 6100» bzw. dem «Phantom 6126 Chesster» (Eyeball).\n\nEine andere Bauform vom gleichen Prinzip war im «Excalibur Mirage» eingesetzt worden. Andere Firmen bauten aufwendigere Schachroboter, indem die Bewegung der Figuren über einen oberhalb des Schachbrettes angebrachten Roboterarm vollzogen wurde. Beispiele hierfür sind der «Novag Robot Adversary» oder der nie in Serienproduktion gegangene «Excalibur Talking Robotic Chess 740».\n\nHeutzutage werden Mikro-Schachcomputer hauptsächlich im Niedrigpreissegment verkauft. Aktuelle Geräte aus der Massenproduktion verzichten auf mechanische Extras und legen den Schwerpunkt auf Trainingsfunktionen, zum Beispiel gespeicherte Übungen und Warnungen bei groben Fehlern. Die Marktnische für Schachcomputer mit besonders hochwertiger Ausstattung wird vorwiegend vom Gebrauchtmarkt bzw. innerhalb einer Sammlerszene bedient. Darüber hinaus gibt es Kleinserien neuer Geräte mit Spezialversionen von Software-Schachengines.\n\nIm Jahr 2016 stellte Digital Game Technology (DGT) den \"DGT Pi Chess Computer\", einen Schachcomputer auf Basis des Raspberry Pi, vor. Dieser ist in eine Schachuhr integriert und nutzt dessen Anzeige. Die Eingabe der Züge geschieht durch ein PC-Schachbrett mit Figurenerkennung welches mittels Bluetooth oder über den Universal Serial Bus (USB) angebunden wird und ebenfalls von DGT hergestellt wird. Als Schachengine kommt unter anderem Stockfish zum Einsatz. Erwähnenswert ist, dass die verwendete Firmware, PicoChess, frei verfügbar ist.\n\nDie weltweit erste große Computerschachmeisterschaft fand im September 1970 unter dem Namen \"1st ACM United States Computer Chess Championship\" in New York statt. Daraus entstand die \"North American Computer Chess Championship\" (NACCC, ), die bis 1994 zumeist jährlich stattfand. Darüber hinaus wurde von 1980 bis 2001 die Mikrocomputer-Schachweltmeisterschaft (kurz „Mikro-WM“) ausgerichtet. Häufig traten dabei die Teilnehmer mit speziell für das Turnier getunten Geräten an. Die erfolgreichsten WM-Schachcomputer enthielten Programme von Dan und Kathe Spracklen (Fidelity, CPU 6502, Ursprung Sargon) und Richard Lang (Mephisto, CPU 68000, Ursprung Psion).\n\nDas weltweit wichtigste und noch immer stattfindende Computerschachturnier, ist die \"World Computer Chess Championship (WCCC)\", , zu der sowohl auf Supercomputern laufende Schachprogramme als auch alle Arten von Schachcomputern zugelassen sind. Bei der 7. Computerschach-Weltmeisterschaft im Jahr 1992 gelang es erstmals einem „Mikro“, der ChessMachine Gideon 3.1 von Ed Schröder (auch als Mephisto RISC II Schachcomputer vermarktet), die Groß- und Spezialrechner zu distanzieren, und den Titel des Computerschachweltmeisters zu erringen.\n\nAb Anfang der 1990er-Jahre wurde für Schachprogramme verstärkt der Personal Computer eingesetzt, der ab diesem Zeitpunkt die leistungsfähigere und portablere Plattform darstellte. Der Markt für hochpreisige Mikro-Schachcomputer brach zusammen, fortan dominierten PC-Schachprogramme wie HIARCS, Rebel, Fritz, Genius oder MChess. Die Mikros nahmen ab 1994 nicht mehr an ihrer eigenen Weltmeisterschaft teil, die noch bis 2001 fortgeführt wurde.\n\nEinen Überblick zu den Anfängen der Mikrocomputer-Schachweltmeisterschaften veröffentlichte Alexander Canetti 1988, beginnend mit 1980 in London. Mephisto musste bis 1983 auf seinen ersten Einsatz warten. Im Jahre 1981 weitete sich die Nichtzulassung von Mephisto zu einem Skandal aus. Um die Spielstärke von Mikro-Schachcomputern und PC-Programmen zu vergleichen, erhalten diese aufgrund ihrer Spiele untereinander Elo-Zahlen, die im Rahmen der SSDF-Liste gepflegt werden.\n\nDeep Blue gelang es als erstem Computer der Welt 1996, einen amtierenden Schachweltmeister, Garri Kasparow, in einer Partie mit regulären Zeitkontrollen zu schlagen.\n\n\n"}
{"id": "32550", "url": "https://de.wikipedia.org/wiki?curid=32550", "title": "CinePaint", "text": "CinePaint\n\nCinePaint (ehemals \"Film Gimp\") ist eine professionelle Grafiksoftware für Filme. Da bei der Entwicklung von CinePaint Code aus unterschiedlichen Open Source Projekten eingeflossen ist, werden die Quelltexte entsprechend uneinheitlich unter verschiedenen freien Lizenzen veröffentlicht (GPL, LGPL, BSD, MPL).\n\nCinePaint erlaubt die Bearbeitung ganzer Bilderserien in einem Vorgang. Zudem unterstützt es Farbtiefen von 8, 16 und 32 Bit (HDR) pro Farbkanal – weit mehr, als auf einem normalen Monitor angezeigt werden können. Dadurch wird dieses Programm neben seinem eigentlichen Einsatzzweck in der Filmbranche auch für Fotografen interessant. Durch das integrierte Color Management können CMYK- und CIE*Lab-Bilder angezeigt und korrigiert werden. Ab Version 0.20 ist es möglich, HDR-Bilder aus normalen Belichtungsreihen berechnen zu lassen. Rohdatenformate von Digitalkameras werden mittels DCRaw eingelesen, alternativ gibt es ein UFRaw-Plugin.\n\nNeben Rhythm & Hues tragen die Studios von Sony Pictures Imageworks, ILM und DreamWorks Quelltext zu dem Projekt bei.\n\nDie Software wurde vom Quelltext des Bildbearbeitungsprogramms GIMP (Version 1.0.x, 1998) abgeleitet. Silicon Grail, die später von Apple aufgekauft wurden, und Rhythm & Hues übernahmen die Programmierung, um einen Ersatz für das gerade aufgegebene Adobe Photoshop für Silicon Graphics IRIX zu erhalten. Dieser Programmierbeitrag wurde aber nicht in GIMP aufgenommen. Daraufhin beschloss die Filmindustrie, eine Abspaltung mit Namen „Film Gimp“ weiterzuentwickeln. Später wurde das Projekt in „CinePaint“ umbenannt.\nVersion 2.0 ist zur Zeit in Entwicklung.\n\nVon folgenden Filmen ist bekannt, dass CinePaint im Produktionsprozess genutzt wurde:\n\n\"Harry Potter\", \"Cats & Dogs\", \"Dr. Dolittle 2\", \"Little Nicky – Satan Junior\", \"Wie der Grinch Weihnachten gestohlen hat\", \"The 6th Day\", \"Planet der Affen\", \"Scooby-Doo\", \"Stuart Little\", \"Showtime\", \"Blue Crush\", \"2 Fast 2 Furious\", \"Der Appartement Schreck\", \"Last Samurai\", \"Die Liga der außergewöhnlichen Gentlemen\", \"Buddy – Der Weihnachtself\", \"Looney Tunes\" und der fast nur mit freier Software produzierte Film \"Elephants Dream\".\n\n"}
{"id": "32558", "url": "https://de.wikipedia.org/wiki?curid=32558", "title": "Vektorgrafik", "text": "Vektorgrafik\n\nEine Vektorgrafik ist eine Computergrafik, die aus grafischen Primitiven wie Linien, Kreisen, Polygonen oder allgemeinen Kurven (Splines) zusammengesetzt ist. Meist sind mit Vektorgrafiken Darstellungen gemeint, deren Primitiven sich zweidimensional in der Ebene beschreiben lassen. Eine Bildbeschreibung, die sich auf dreidimensionalen Primitiven stützt, wird eher 3D-Modell oder Szene genannt und die Erzeugung zweidimensionaler Linien- und Flächengrafiken sowie fotorealistischer Bilder aus solchen 3D-Modellen wird Bildsynthese oder \"Rendern\" genannt.\n\nUm beispielsweise das Bild eines Kreises zu speichern, benötigt eine Vektorgrafik mindestens zwei Werte: die Lage des Kreismittelpunkts und den Kreisdurchmesser. Neben der Form und Position der Primitiven werden eventuell auch die Farbe, Strichstärke, diverse Füllmuster und weitere, das Aussehen bestimmende Daten angegeben.\n\nVektorgrafiken basieren, anders als Rastergrafiken, nicht auf einem Raster, in dem jedem Bildpunkt (Bildelement, englisch \",\" Pixel) ein Farbwert zugeordnet ist, sondern auf einer Bildbeschreibung, die die Objekte, aus denen das Bild aufgebaut ist, exakt definiert. So kann beispielsweise ein Kreis in einer Vektorgrafik über Lage des Mittelpunktes, Radius, Linienstärke und Farbe vollständig beschrieben werden; nur diese Parameter werden gespeichert. Im Vergleich zu Rastergrafiken lassen sich Vektorgrafiken daher oft mit deutlich geringerem Platzbedarf speichern. Eines der wesentlichen Merkmale und Vorteile gegenüber der Rastergrafik ist die stufenlose und verlustfreie Skalierbarkeit.\n\nDer Begriff „Vektorgrafik“ hängt damit zusammen, dass die bis in die 1980er Jahre verbreiteten Vektorbildschirme Linien („Vektoren“) mit einem Kathodenstrahl anzeigten. Vektorgrafiken im heutigen Sinn bestehen jedoch nicht nur aus Linien, sondern können auch andere Grundformen zulassen. Zur Anzeige auf heute üblichen Rasterbildschirmen müssen Vektorgrafiken gerastert werden.\n\nDie Erzeugung von Vektorgrafiken ist Gegenstand der geometrischen Modellierung und geschieht meist mittels eines Vektorgrafikprogramms oder direkt mit einer Auszeichnungssprache. Rastergrafiken können durch die sogenannte Vektorisierung mit gewissen Einschränkungen in Vektorgrafiken umgewandelt werden; manche Texterkennungsprogramme basieren auf einem Vektorisierungsalgorithmus. Mittlerweile bieten gängige Vektorgrafikprogramme Funktionen an, die es erlauben, Vektorgrafiken mit Farbverläufen und Transparenzstufen zu speichern und damit eine größere Zahl von Bildern zufriedenstellend zu beschreiben. Auch solche Vektorgrafiken lassen sich, im Gegensatz zu Rastergrafiken, bequem und verlustfrei verändern und transformieren.\n\nDie Stärke von Vektorgrafiken allgemein ist die Auflösungsunabhängigkeit, d. h., sie sind für eine Wiedergabe (Bildschirm, Drucken) in beliebiger Auflösung geeignet. Dies erfordert jedoch immer ein aufwändiges „Rendern“ des Vektorformats in ein Rasterformat. Ein Nachteil von Vektorgrafiken gegenüber Rastergrafiken ist ein unbekannter, inhaltsabhängiger Wiedergabeaufwand (in Rechenzeit und Arbeitsspeicher). Rastergrafiken haben i. A. den Vorteil, dass der Wiedergabeaufwand konstant und inhaltsunabhängig ist. Um diesen Nachteil von Vektorgrafiken zu minimieren, hält beispielsweise Wikipedia serverseitig vorgerenderte Rastergrafik-Vorschaubilder von SVG-Vektorgrafiken in mehreren Auflösungen vor.\n\nDie Stärke von Vektorgrafiken liegt bei Darstellungen, die als Zusammenstellung von grafischen Primitiven befriedigend beschrieben werden können, zum Beispiel Diagramme oder Firmenlogos. Sie sind nicht geeignet für gescannte Bilder und Digitalfotos, die naturgemäß als Rastergrafik erfasst werden und nicht verlustfrei umgewandelt werden können. Ebenfalls an die Grenzen stoßen Vektorformate bei komplexen gerenderten Bildern, die ebenfalls direkt als Rastergrafik berechnet werden. Allerdings spezialisieren sich immer mehr Firmen auf die Vektorisierung von Rastergrafiken. Dies ist vor allem von Interesse für großflächige Bildwerbung, Fahrzeugbeschriftung oder wenn die Vektorisierung als grafischer Effekt genutzt wird.\n\nVektorgrafiken können mithilfe von Plottern (Linienschreibern) oder Laserbeschriftern ohne vorherige Rasterung direkt auf verschiedene Materialien ausgegeben werden.\n\nZur Erstellung von Illustrationen, insbesondere für die Erstellung von Logos, können vektorbasierte Zeichenprogramme verwendet werden. Die von 3D-Modellierungswerkzeugen erzeugten 3D-Szenen können auch als Vektorgrafiken betrachtet werden.\n\nVektorgrafiken erlauben es, Dokumente unabhängig von der Auflösung des Ausgabegeräts zu beschreiben. Mit Hilfe einer vektorgrafikfähigen Seitenbeschreibungssprache wie PostScript oder dem daraus hervorgegangenen Portable Document Format (PDF) können Dokumente, im Gegensatz zu Rastergrafiken, mit der jeweils höchstmöglichen Auflösung auf Bildschirmen verlustfrei dargestellt oder gedruckt werden.\n\nIn Office-Paketen finden sich mehr oder weniger umfangreiche Zeichenfunktionen (\"Formen\"). Die so erstellten Grafiken sind Vektorgrafiken. Auch Diagramme sind Vektorgrafiken. Komplexere Füllungen werden für die Ausgabe gerastert. Für nicht fotorealistische Cliparts ist WMF unter Windows ein gebräuchliches Vektorgrafik-Dateiformat.\n\nAuf gängigen Computersystemen finden heute überwiegend sogenannte Outline-Schriften Verwendung, die die Umrisse jedes Zeichens als Vektorgrafik beschreiben. Wichtige Formate sind TrueType, PostScript und OpenType.\n\nFrühe Arcade-Spiele liefen mit Vektorgrafik. Das erste war Spacewar! aus dem Jahr 1962. Bekannt waren auch Lunar Lander (Atari, 1979), Battlezone (Atari, 1980) und Star Wars (Farb-Vektorgrafik, Atari, 1983). Es gab auch die Spielkonsole Vectrex.\n\nBei diesen Systemen wurde als technische Besonderheit die X/Y-Ablenkung der zur Darstellung verwendeten Bildröhre direkt angesteuert, statt zeilenweise einen Grafikspeicher auszugeben wie bei Punktrastergrafiken. Der Vorteil dieser Lösung liegt in der glatten, treppchenfreien Darstellung von Linien; zur Ausgabe allgemeiner, insbesondere Komplexgrafiken und Texte, ist diese Methode jedoch weniger geeignet.\n\nAuch auf Heimcomputern wie dem Amiga und Commodore 64 gab es vereinzelt Spiele, die auf Vektorgrafik basierten, beispielsweise Stunt Car Racer oder die Elite-Reihe. Hierbei wurden, anders als bei den Vektorgrafik-Systemen, jedoch die Vektorgrafiken nicht direkt ausgegeben, sondern berechnet und in eine Rastergrafik gewandelt. Aufgrund der hierfür benötigten – für damalige Verhältnisse – hohen Rechenleistung, waren diese Spiele jedoch im Allgemeinen vergleichsweise langsam bzw. detailarm.\n\nIm World Wide Web liegen Vektorgrafiken meist im offenen Format SVG oder als proprietäre SWF-Dateien (Adobe Flash) vor. Für 3D-Szenen gilt als Nachfolger der Virtual Reality Modeling Language (VRML) die Beschreibungssprache X3D. Wieder auferstanden ist die auf OpenGL basierende Web Graphics Library (WebGL) sowie die auf WebGL basierende JavaScript-Bibliothek Open 3D (o3d) von Google.\n\nBei Geoinformationssystemen (GIS) kann die Geometrie von Flurstücken und Landkarten in Form von Vektordaten gespeichert werden. Solche Vektorgrafiken lassen sich vergleichsweise einfach mit Sachdaten verknüpfen. Ein typisches GIS-Vektorformat ist das Shapefile.\n\nFür technische Zeichnungen finden CAD-Programme Verwendung. Hier wird die Geometrie vorab als Vektordaten gespeichert, was z. B die berechnete Bemaßung und das Erstellen von Massenauszügen und Stücklisten ermöglicht. Ein typisches CAD-Vektorformat ist das Drawing Interchange Format (DXF).\n\n\n"}
{"id": "33056", "url": "https://de.wikipedia.org/wiki?curid=33056", "title": "Gleitkommaeinheit", "text": "Gleitkommaeinheit\n\nGleitkommaeinheit, FPU (für \"Floating Point Unit\") oder NPU (für \"Numeric Processing Unit\") sind Begriffe aus der Computertechnik und bezeichnen einen speziellen Prozessor, der Operationen auf Gleitkommazahlen ausführt. Da insbesondere bei älteren, Mikrocode-basierten Systemen neben vergleichsweise einfachen Operationen wie Addition, Subtraktion, Multiplikation, Division oder Wurzelziehen auch transzendente Funktionen wie die Exponentialfunktion oder diverse trigonometrische Funktionen in Hardware ausgeführt waren, spricht man auch von mathematischen Koprozessoren.\n\nDie FPU kann auch als externer Chip in einem eigenen Gehäuse sitzen (z. B. Intel 80287) oder in einen bestimmten Bereich innerhalb der CPU integriert sein (z. B. Intel Pentium).\n\nFrühen CISC-Prozessoren fehlten meist Register und Befehle zur Behandlung von Gleitkommazahlen. Derlei Berechnungen und mathematische Funktionen wurden per Software-Bibliotheksaufrufe durch den auf Ganzzahlverarbeitung optimierten Hauptprozessor erledigt. Um die CPU von diesen rechenintensiven Aufgaben zu entlasten, gab es zunächst Arithmetik-Prozessoren wie den AMD AM9511, die als periphere Bausteine angesprochen wurden. Bei späteren CISC-CPUs wie den Intel x86-Prozessoren (bis zum 486er) oder den Motorola-68k-CPUs gab es die Möglichkeit, einen zusätzlichen Koprozessor auf dem Motherboard nachzurüsten.\n\nEingeläutet wurde das Zeitalter der integrierten FPUs durch mehrere Faktoren:\n\n\nDie Anwesenheit einer FPU ermöglicht eine erhebliche Leistungssteigerung für gleitkommaintensive Berechnungen. So boten Koprozessoren breitere Register: Schon bei 16- und 32-Bit-CPUs hatte die FPU häufig 64 Bit, 80 Bit oder auch 128 Bit breite Register. Dadurch konnten einfache Berechnungen mit höherer Genauigkeit durchgeführt werden, und es wurde ein größerer Wertebereich abgedeckt. Da auch die FPU im Inneren letztendlich eine digitale Recheneinheit darstellt, bedarf es weiterer, trickreicher Methoden, um eine echte Beschleunigung zu erhalten. Viele Modelle (z. B. der 8087) verfügen über hardwareseitig optimierte Rechenmethoden wie z. B. den CORDIC-Algorithmus für trigonometrische Funktionen, welcher nur durch Addition und Registerverschiebung, aber ohne langwierige Multiplikation auskommt. Oft wird eine große Beschleunigung auch über fest implementierte Lookup-Tabellen erreicht. Das heißt, die Werte werden nicht über mehrmalige Schleifendurchläufe ermittelt, sondern zuerst mit Hilfe von Tabellen näherungsweise und dann durch Interpolationsverfahren bis zu hinreichender Genauigkeit ermittelt (ein Fehler in einer solchen Tabelle war Ursache des sogenannten Pentium-Bug).\nWeiterhin kann eine FPU ihre Register oftmals als Matrix organisieren und so Vektorrechnungen beschleunigen.\n\nDie meisten FPUs stellen Operationen für die Grundrechenarten (mit höherer Genauigkeit als die CPU), Logarithmus-, Wurzel- und Potenzrechnung und trigonometrische Funktionen, sowie Funktionen für das Rechnen mit Matrizen zur Verfügung.\n\nDie Rechenleistung einer FPU wird meistens in SPECfp gemessen, im Gegensatz zu den SPECint einer CPU.\n\n\n"}
{"id": "33501", "url": "https://de.wikipedia.org/wiki?curid=33501", "title": "Calligra Suite", "text": "Calligra Suite\n\nDie Calligra Suite ist ein freies Grafik- und Office-Paket von KDE, das soweit möglich die OpenDocument-Formate verwendet. Die Komponenten decken das Anwendungsspektrum gängiger Office-Suiten ab wie LibreOffice, OpenOffice und Microsoft Office. Zusätzlich enthält das Paket Kreativ-Anwendungen zum Malen, Zeichnen und Schreiben von E-Books. Es ist 2010 aus KOffice hervorgegangen.\n\nSie besteht aus einer frame-basierten Textverarbeitung (Words), einer Tabellenkalkulation (Sheets), einem Präsentationsprogramm (Stage), einem Programm für Flussdiagramme (Flow), einem Vektorgrafikprogramm (Karbon), einer integrierten Datenbank (Kexi), einer Projektverwaltung (Plan), einem pixelbasierten Mal- und Bildbearbeitungsprogramm (Krita) und einbettbaren Objekten wie Graphen oder Diagrammen und Formeln.\n\nEine Automatisierung von wiederkehrenden Arbeitsschritten ist mit Hilfe von verschiedenen Skriptsprachen wie Python, Ruby und JavaScript oder über DBus möglich.\n\nDie Calligra Suite besteht aus folgenden Programmen:\n\n\n\n\n\n\nAls erste Calligra-Komponente wurde 1997 \"KPresenter\" als eigenständige Anwendung entwickelt. 1998 folge die Textverarbeitung \"KWord\". Später kamen weitere Anwendungen hinzu, die als \"KOffice\" gebündelt wurden.\n\nNach einem lang andauernden Streit mit KWord-Entwickler Thomas Zander gab das übrige Entwicklerteam am 6. Dezember 2010 bekannt, dass KOffice ab Version 2.4 in Calligra Suite abgespalten werden soll. Dies soll vor allem den Wert der Software als Technologie-Plattform widerspiegeln. Mit der Umbenennung des gesamten Pakets geht auch eine Umbenennung einzelner Komponenten einher. KSpread wird in \"Sheets\", KPresenter in \"Stage\", Kivio in \"Flow\" und KPlato in \"Plan\" umbenannt werden. KWord wurde nicht abgespalten, sondern stattdessen eine neue Textverarbeitung namens \"Words\" entwickelt.\n\nDie 2.3-Serie von KOffice wurde vom Calligra-Team gepflegt. Die Entwicklung für KOffice 2.4 wird seit der Abspaltung fast ausschließlich von Thomas Zander betrieben, der wiederum zusätzlich zur Weiterentwicklung von KWord Derivate von KSpread, KPresenter und Karbon14 unter neuen Namen in KOffice 2.4 aufgenommen hat. Alle anderen KOffice-2.3-Komponenten wurden ersatzlos entfernt.\n\nAm 18. Mai 2011 begann das Calligra-Team monatliche Schnappschüsse von Calligra Suite 2.4 zu veröffentlichen. Ursprünglich war geplant, die fertige 2.4-Version im Januar 2012 zu veröffentlichen, aber Probleme mit der \"Rückgängig/Wiederherstellen\"-Funktion sorgten für Verzögerungen und die fertige Version erschien schließlich am 11. April 2012.\n\n\n\n"}
{"id": "33709", "url": "https://de.wikipedia.org/wiki?curid=33709", "title": "WordNet", "text": "WordNet\n\nDas WordNet ist ein seit 1985 am \"Cognitive Science Laboratory\" der Princeton University entwickeltes lexikalisch-semantisches Netz der englischen Sprache.\n\nWordNet besteht aus einer Datenbank, die semantische und lexikalische Beziehungen zwischen den Wörtern enthält.\nDiese sind nach psycholinguistischen Erkenntnissen entworfen, da das WordNet ursprünglich entwickelt wurde, um natürlichsprachliche Texte für Computer verständlich zu machen.\nDie Datenbank, die frei durchsuchbar und mitsamt Software kostenlos verfügbar ist, wird auch für andere Zwecke eingesetzt.\n\nDas zum WordNet analoge deutschsprachige GermaNet ist im Gegensatz zu diesem nicht frei verfügbar. Daher bietet sich alternativ die Verwendung von WordNet-Übersetzungen an, die beispielsweise in der multilingualen Datenbank BabelNet vorhanden sind. OpenThesaurus ist ein ähnliches Projekt, das jedoch etwas bescheidener in der Zielsetzung ist (als Thesaurus für eine Textverarbeitung; nur \"istEin-Relation\"; Synonym-Gruppen werden ausgewiesen). Allerdings ist OpenThesaurus unter einer freien Lizenz verfügbar.\n\nMit der \"Open-de-WordNet initiative\" gibt es seit Anfang 2017 jedoch eine Initiative, die „Multilingual Open Wordnet Initiative“ um eine deutsche Komponente zu erweitern.\n\n\n\n"}
{"id": "33752", "url": "https://de.wikipedia.org/wiki?curid=33752", "title": "X Window System", "text": "X Window System\n\nDas X Window System (auch X Version 11, X11, X) ist ein Netzwerkprotokoll und eine Software, die Fenster auf Bitmap-Displays auf den meisten unixoiden Betriebssystemen und OpenVMS ermöglicht. X11 wurde auf allen gebräuchlichen Betriebssystemen implementiert.\n\nEs stellt einen Standardbaukasten und das Protokoll zum Bau einer grafischen Benutzeroberfläche zur Verfügung. Dazu gehören Zeichnen und Bewegen von Fenstern auf dem Bildschirm sowie das Behandeln der Benutzereingaben mit Maus und Tastatur. X schreibt keine spezielle Benutzerschnittstelle vor, sondern überlässt dies seinen Client-Programmen. Deshalb können X-basierte Umgebungen in ihrem Aussehen sehr variieren. Mithilfe von Desktop-Umgebungen – die bekanntesten, KDE und Gnome sind rechts abgebildet – wird dem entgegengewirkt. Endanwender kommen auf modernen Unix-Desktops mit den Kernprogrammen des X Window Systems kaum noch in Berührung.\n\nFrühere Anzeigeprotokolle konnten nur eingebaute oder direkt angeschlossene Anzeigen verwalten. In Erweiterung dazu wurde X spezifisch als Client-Server-System entworfen, was neben der lokalen auch eine netzwerktransparente Verwendung ermöglicht.\n\nDie X.Org Foundation betreut heute das Projekt und hat am 6. Juni 2012 die Version 7.7 der Referenzimplementierung als freie Software unter der MIT-Lizenz herausgegeben. Eine weitere, in den 1990ern weit verbreitete Implementierung ist XFree86.\n\nX wurde 1984 im Projekt Athena in Zusammenarbeit des MIT, DEC und IBM entwickelt. Die erste Version wurde am 19. Juni 1984 vorgestellt und 1986 wurde X 10.4 mit großem Erfolg gegen eine Schutzgebühr an alle Interessierten verteilt, womit X erstmals eine große Verbreitung auf den Unix-Rechnern jener Zeit erreichte. X10 war jedoch inkompatibel zum späteren X11. Im September 1987 schließlich folgte das erste Release von X11.\n\nAls 1988 die Popularität immer größer wurde, wurde das nicht-kommerzielle X-Konsortium gegründet, das die weitere Entwicklung übernahm. Es veröffentlichte verschiedene Versionen, die letzte große war X11R6 1994. Danach übernahm \"The Open Group\" die Entwicklung und Standardisierung. In der gleichen Zeit erreichte die freie X-Implementierung XFree86 einen immer größeren Bekanntheitsgrad und wurde zum De-facto-Standard der X-Implementierung. 2003 aber gab es projektinterne und lizenzrechtliche Probleme bei XFree86, die im Endeffekt zur Auflösung des Entwicklerteams führten. Unter dem Dach der daraufhin neu gegründeten X.Org Foundation wurde fortan eine Abspaltung des alten XFree86 weiterentwickelt, die unter dem Namen X.Org-Server größtenteils in dessen Fußstapfen trat. Seit etwa 2005 gilt er als der meistverbreitete X-Server und damit die meistverbreitete X-Implementierung.\n\nDer X-Server steuert die Ein- und Ausgabegeräte, wie unter anderem Zeigegeräte (z. B. Maus), Tastatur, Bildschirm und Grafikkarte. Dazu kommuniziert er in der Theorie mit dem Betriebssystem-Kernel, in der Praxis wird der Kernel aber auch umgangen. Das zugrundeliegende Konzept ist eine Erweiterung des Terminal-Konzeptes auf grafische Benutzeroberflächen: Ein Programm verlässt sich für die grafische Eingabe (Maus) und Ausgabe (Fenstertechnik) auf die Dienste eines anderen dafür spezialisierten Programms (X-Server), die Kommunikation läuft immer über das Netzwerk.\n\nDie Darstellung bei X ist die eines Rastergrafik-basierten Fenstermanagers, der verschiedene Funktionen bereitstellt. Dazu gehört das Zeichnen und Bewegen der Fenster, die ereignisorientierte Handhabung eines Zeigegeräts, z. B. einer Maus, die Interprozesskommunikation und teilweise auch die Verwaltungsfunktionen für Druck und Audio-Ausgabe. X ist also ein Minimalsystem, bei dem zum Zeichnen lediglich Primitive wie Linien, Muster etc. bereitgestellt werden.\n\nDas eigentliche Aussehen und Verhalten des Fenstersystems wird deshalb nicht von X bestimmt, sondern von dem Fenstermanager, der wie eine normale Anwendung einfach als Client des X-Servers läuft. Für das Aussehen der Programme selbst ist er aber nicht verantwortlich, das übernimmt meistens das sog. GUI-Toolkit, welches das Zeichnen und Verwalten der typischen Elemente einer grafischen Oberfläche wie z. B. Menüs und Buttons übernimmt. Siehe dazu auch Desktop-Umgebung.\n\nX baut auf einem Client-Server-Modell auf:\n\n\n\nDas in der Großrechner-Welt bekannte, textbasierte Terminal-System (dort meistens proprietäre wie IBM 5250 und 3270; unter Digitals VMS die bekannten seriellen Terminals wie z. B. VT100) erlebte seine grafische Weiterentwicklung zum X-Terminal; ein Rechner, auf dem nichts anderes als ein X-Server läuft. Diese X-Terminals werden häufig genutzt, um mit leistungsschwachen Rechnern auf rechenintensive Anwendungen zuzugreifen: Die Anwendung selbst läuft auf einem Server, das X-Terminal übernimmt nur die Anzeige der grafischen Oberfläche.\n\nMittlerweile hat es sich durchgesetzt, statt spezieller Hardware für X-Terminals auf Standard-Hardware von PCs zurückzugreifen. Mit dieser können die Funktionen von X-Terminals zum Beispiel in Form von Thin Client preiswert umgesetzt werden.\n\nUm schnelle 3D-Beschleunigung zu ermöglichen, stellt das X Window System die Schnittstelle Direct Rendering Infrastructure (DRI) bereit, über die Programme direkt auf die 3D-Hardware zugreifen können. Das geschieht bei freien Treibern durch die OpenGL-Implementierung Mesa 3D, im Fall von proprietären Treibern durch jeweils proprietäre Implementierungen von OpenGL.\n\nMit Hilfe von 3D-Erweiterungen wie AIGLX und xgl werden im X.Org-Server darüber hinaus 3D-Effekte auf dem Desktop selbst unterstützt.\n\nDas X-System wurde von Anfang an für den effizienten Netzwerkbetrieb konzipiert. Die Kommunikation zwischen Client und Server läuft über ein standardisiertes Protokoll ab, das nur relativ geringe Transferraten benötigt. Der Quellcode des X-Systems ist zwar objektorientiert, aber dennoch in C geschrieben, so dass eine Übersetzung auf nahezu jedem Unix-Rechner möglich ist. Das X-System ist im Gegensatz zu aktuellen MS-Windows-Versionen kein Bestandteil des Betriebssystems, weswegen ein Absturz von X in der Regel keinen Einfluss auf dessen Integrität hat: Stürzt der X-Server ab oder reagiert nicht mehr, können die Clients reagieren und z. B. auf einen Neustart des X-Servers warten oder sich (was meistens der Fall ist) beenden. Weiterhin hat der Fenstermanager (die „Bedienoberfläche“) im Prinzip denselben Status wie ein X-Client: Im laufenden Betrieb kann ein Fenstermanager durch einen anderen ersetzt werden. Bereits mit sehr kleinen Fenstermanagern (z. B. twm, rund 140 KB) kann komfortabel gearbeitet werden.\n\nDurch den durchdachten, hierarchischen Aufbau des Systems ist beispielsweise Folgendes möglich:\n\nDie Weiterentwicklung von X läuft im Vergleich zur Hardware auf großen Zeitskalen und mit nur geringen Veränderungen ab. Kompatibilitätsprobleme treten daher nicht auf, wenn nur Standardbibliotheken verwendet werden. Allerdings benutzt ein großer Teil der X-Clients heute Erweiterungen wie Xft, XVideo oder Xinerama.\n\nDie strikte Trennung des X Window Systems von der Benutzeroberfläche führt zu einem vergleichsweise flexiblen System, das aber auch sehr uneinheitlich erscheinen kann: Da das Design der Oberflächenelemente im Gegensatz zu Windows oder Mac OS nicht aus dem eingestellten Design des Haupt-GUI-Toolkits stammt, sondern es verschiedene Toolkits (unter anderem Qt und GTK+) mit verschiedenen Eigenschaften gibt, die verschiedene Designs verwenden, kann es durchaus vorkommen, dass jedes der genutzten Programme ein unterschiedliches Aussehen und Verhalten zeigt.\n\nMit codice_1 und codice_2 fungiert der X-Server als Keylogger, wobei selbst das sudo-Passwort ausgelesen werden kann. Voraussetzung für diese Art des Angriff ist aber, dass bereits ein X-Server mit codice_3-Rechten läuft. Viele Linux-Distributionen verhindern deshalb in der Standardkonfiguration die Anmeldung als codice_3. Seit mehreren Jahren wird an Wayland gearbeitet, welches den X-Server ablösen soll. Wayland soll deutlich weniger Sicherheitslücken beinhalten als X.\n\nEs wurden mehrere Ansätze entwickelt, um die Übertragungsgeschwindigkeit zu verbessern:\n\nMehrere Projekte versuchen, diese Techniken umzusetzen. Die XCB-Bibliothek ist eine Neufassung der Xlib-Bibliothek, die im X-Server den Datenverkehr zwischen Client und Server regelt. Bei der Entwicklung von XCB wird insbesondere Wert auf eine gesteigerte Effizienz gelegt.\n\nEinen hohen Bekanntheitsgrad hat ebenfalls das NX-Projekt der italienischen Firma NoMachine, das mit Hilfe eines Caches und Datenkompression X auch über Modem-Leitungen nutzbar macht.\n\nIn der Vergangenheit arbeitete auch das Low-Bandwidth-X-Projekt von Keith Packard in diesem Bereich. Es war gedacht, eine Erweiterung des X-Protokolls zu erstellen, um dessen Netzwerktransparenz auch für niedrige Bandbreiten nutzbar zu machen. Mit der Veröffentlichung der von X11R6.3 durch das X-Konsortium im Dezember 1996 wurde LBX eine volle Erweiterung des X-Protokolls. Da es aber nie großflächig genutzt wurde und auch nur für einen Teil der Probleme eine Lösung bieten konnte, wurde das Projekt im Laufe des Jahres 2000 von Keith Packard für tot erklärt.\n\nEine gesteigerte Einheitlichkeit wurde ebenfalls von mehreren Projekten verfolgt. Ansätze waren die Erstellung einer Desktopoberfläche mit einheitlichen Oberflächenbibliotheken oder aber das Ersetzen von X durch ein auf höherem Niveau angelegtes System, das von Beginn an eine einheitliche Behandlung von Oberflächenelementen beinhaltete. Das Fresco-Projekt und das Y Window System entwickelten jeweils Lösungen, um den zweiten Ansatz umzusetzen, die Projekte ruhen aber zurzeit.\n\nEin anderer, aktiv verfolgter Ansatz ist der Versuch, durch gemeinsame Richtlinien für Benutzerschnittstellen und Verhaltensweisen der Software Einheitlichkeit zwischen den verschiedenen Toolkits zu erlangen. Dieser Ansatz wird vor allen Dingen im Rahmen des freedesktop.org-Projekts verfolgt und führte dort z. B. zu einheitlichen Standards für Icons.\n\nIn den frühen 1990er-Jahren war die komplette X11-Distribution oft die größte Datei (rund 10 bis 50 MB) auf Unix-Rechnern. Da sie zudem frei war, wurde sie gerne als Mailbombe verwendet oder zumindest damit gedroht. „\"Soll ich Dir ein X11R3 schicken\"“ war daher im Usenet ein Running Gag, um Newbies auf Fehlverhalten hinzuweisen.\n\nDie X11-Farben sind heute Grundlage der Farbnamen in CSS.\n\n\n\n"}
{"id": "34200", "url": "https://de.wikipedia.org/wiki?curid=34200", "title": "Antz", "text": "Antz\n\nAntz ist ein computeranimierter Trickfilm. Er war neben \"Toy Story\" einer der ersten Filme, die komplett am Computer produziert wurden. Mit diesem Film stieg Dreamworks in die Produktion computeranimierter Filme ein und machte in der Folge der mit Disney kooperierenden Firma Pixar auf diesem Gebiet Konkurrenz.\n\nZ ist eine kleine neurotische Arbeiterameise, die mit ihrem Leben im Ameisenstaat unzufrieden ist. Eines Abends lernt er Prinzessin Bala in einer Bar kennen und verliebt sich auf der Stelle in sie. Deshalb tauscht er mit seinem Freund, dem Soldaten Weaver, den Platz, da die Prinzessin den Soldaten an diesem Tag ihre besten Wünsche mit auf den Weg gibt. Dass dieser Weg geradewegs in die Schlacht führt, empfindet Z als ein ungeheures Missverständnis, doch er wird von den anderen Soldaten mit in den Kampf gezogen. Es geht um einen Präventivschlag gegen die Termiten, der jedoch mit einem Debakel endet. Z kehrt als einziger Überlebender zurück und wird von der Königin als Kriegsheld persönlich empfangen. Dabei stellt sich heraus, dass er eigentlich nur ein Arbeiter ist und niemals auf dem Schlachtfeld hätte sein dürfen. In seiner letzten Not packt Z die Prinzessin als Geisel und verlässt die königliche Residenz zusammen mit Bala etwas unglimpflich durch den Müllschlucker.\n\nZ versucht zum sagenumwobenen \"Insektopia\" zu gelangen, einem Ort, wo Insekten glücklich und frei leben können, wobei sich ihm Bala nach einigen Vorbehalten letztendlich anschließt. Gleichzeitig versucht General Mandibel, ein Anhänger des totalitären Systems, den arbeitenden Pöbel zu vernichten, um mit einem von Grund auf „gereinigten“ Ameisenstaat neu zu beginnen. Da er dazu die Prinzessin benötigt, schickt er seinen Adjutanten Colonel Cutter los, um nach Z zu suchen.\n\nDerweil erreichen Z und die Prinzessin eine überdimensionale Picknickdecke mit eingeschweißten Sandwiches und allerlei Fressalien. Gierig stürzt sich Z auf die Brote, doch er beißt sich nur die Zähne aus. Zwei hilfsbereite Wespen haben schließlich Mitleid mit den beiden und wollen etwas zu fressen besorgen, als in dem Moment eine Fliegenklatsche auf das Wespenweibchen niederschlägt. Herr Wespe ist am Boden zerstört, und auch Bala muss um ihr Leben fürchten: Sie hat sich in einem Kaugummi an der Fußsohle des Menschen verklebt und fliegt nun mit dem Schuh durch die Luft. Nur mit Mühe kann Z mit aufspringen und gelangt so, nachdem der Kaugummi mit einer Münze abgehobelt wurde, in das wahre \"Insektopia\" – einen überfüllten Mülleimer.\n\nWährend es sich Bala und Z dort gutgehen lassen, graben die Ameisen in der Kolonie ihrem Unglück entgegen: Die Eröffnung des „Megatunnels“ steht kurz bevor, doch dieser führt zu einem See, und beim endgültigen Durchbruch soll der Bau überflutet und alle „niederen“ Ameisen ertränkt werden. Inzwischen wird Bala von Cutter ausfindig gemacht und zurückgebracht. Auch Z gelangt nun zurück in die Kolonie und kann die Prinzessin befreien, doch es ist zu spät, um Mandibels Plan aufzuhalten: Der Durchbruch zum See ist erfolgt. Riesige Wassermassen stürzen auf die Ameisen zu, die sich ängstlich auf einer kleinen Erhebung drängen. Doch unter Zs und Balas Führung reißen sich die Ameisen zusammen und bauen einen lebenden Turm zur Erdoberfläche. Als Mandibel sieht, dass die sogenannten „schwachen Elemente“ – und Z im Besonderen – stärker sind, als er angenommen hatte, und sich sogar Cutter gegen ihn stellt, stürzt er sich auf seinen früheren Offizier, reißt dabei aber Z mit in den überfluteten Bau und kommt dabei um; Z hingegen wird von Cutter gerettet. Zusammen feiern die Ameisen das Überleben der Kolonie und Zs Heldentum.\n\n\n\n"}
{"id": "34406", "url": "https://de.wikipedia.org/wiki?curid=34406", "title": "Taschenrechner", "text": "Taschenrechner\n\nEin Taschenrechner ist eine tragbare, handliche elektronische Rechenmaschine, mit deren Hilfe numerische Berechnungen ausgeführt werden können. Einige neuere technisch-wissenschaftliche Taschenrechner beherrschen auch symbolische Mathematik mittels eines Computeralgebrasystems (CAS), können also etwa Gleichungen umstellen oder lösen.\n\nPraktisch alle heutigen Taschenrechner verwenden elektronische Integrierte Schaltungen und LC-Displays als Anzeige und werden von einer Batterie oder Solarzelle mit Strom versorgt.\nBereits vor der Einführung der elektronischen Taschenrechner gab es einen Bedarf nach tragbaren Rechenhilfen. Dieser wurde mit mechanischen Taschenrechnern und Rechenschiebern befriedigt. Meist handelte es sich dabei um einfache Addiermaschinen. Auch Vier-Spezies-Maschinen – also Rechenmaschinen, die Addition, Subtraktion, Multiplikation und Division beherrschten – gab es in taschentauglicher Größe. Bekanntestes Beispiel ist die Curta.\n\nVorläufer der elektronischen Taschenrechner waren elektronische Tischrechner, bei denen der Integrationsgrad der Schaltungstechnik noch geringer war und die deshalb größere Dimensionen aufwiesen.\n\nDer erste elektronische, tatsächlich handflächengroße Taschenrechner wurde 1967 von Texas Instruments entwickelt. Ein 1,5 kg schwerer Prototyp dieses ersten Taschenrechners ist heute in der Smithsonian Institution ausgestellt. Auch dieser lief schon mit Batterien, frühere Rechner benötigten einen Stromanschluss.\nDie ersten kommerziell vertriebenen Taschenrechner wurden 1969 und 1970 von den japanischen Firmen Compucorp, Sanyo, Sharp und Canon hergestellt. Intel entwickelte für die japanische Firma Busicom einen der ersten Mikroprozessoren, den Intel 4004, der 1971 auf den Markt kam und in dem Modell Busicom 141-PF verwendet wurde. Als erster Taschenrechner, der mit einem Verkaufspreis von 10.000 Yen für die breite Masse erschwinglich war, gilt der 1972 veröffentlichte Casio Mini. 1972 brachte Texas Instruments den Taschenrechner SR 10 mit dem eigenen Mikroprozessor TMS1000 heraus. Diese Taschenrechner verfügten über wenig mehr als die vier Grundrechenarten. 1971 stellte Bowmar den ersten in den USA erhältlichen Taschenrechner her (Bowmar 901B/„Bowmar Brain“, Maße: 131 mm ×77 mm × 37 mm). Er hatte vier Funktionen und ein achtstelliges rotes LED-Display. Verkauft wurde er für 240 US$. Bowmar musste 1976 schließen.\n\n1972 erschien mit dem HP-35 von Hewlett-Packard der erste technisch-wissenschaftliche Taschenrechner mit trigonometrischen, logarithmischen und Exponentialrechnungs-Funktionen. Er wurde ein Verkaufserfolg und leitete das Ende der damals noch weit verbreiteten Rechenschieber ein. Einer seiner Entwickler war Steve Wozniak, der wenige Jahre später das Unternehmen Apple mitgründete und als Computer-Ingenieur die Entwicklung des Personal Computer maßgeblich beeinflusste.\n\nVor allem Hewlett Packard und Texas Instruments entwickelten ab 1974 auch programmierbare Taschenrechner. Seit Ende der 1980er Jahre kamen die ersten grafikfähigen Taschenrechner (GTR) auf den Markt.\n\nJe nach Art des Rechners ist für die Berechnung der gleichen Funktion eine unterschiedliche Eingabe erforderlich:\n\n\n\n\n\n\n\nDie meisten aktuellen Modelle enthalten mehrere der oben genannten Funktionsgruppen, vereinzelt sogar mit einer einfachen Tabellenkalkulation.\n\nAuch wenn heutige Taschenrechner im Regelfall kaum Programmfehler bei einfachen Berechnungen aufweisen, lassen sich zwischen verschiedenen Taschenrechnermodellen unterschiedliche Genauigkeiten und Auflösungen bei numerischen Berechnungen bestimmen. Die Gründe liegen in den numerischen Näherungsverfahren (beispielsweise Horner-Schema und CORDIC), mit denen beispielsweise transzendente Funktionen wie die Sinus-Funktion berechnet werden. Genauer gesagt kommt es auf die Anzahl der abgespeicherten Koeffizienten für die Funktionsapproximationen an: der dafür benötigte Speicherplatz war vor allem in der Anfangszeit ein extremer Engpass. Diese kleinen Unterschiede in den Verfahren und unterschiedliche Genauigkeiten lassen sich auch als Erkennungsmerkmal für eine bestimmte Firmware verwenden.\n\nBeispielsweise liefert die numerische Berechnung von sin (22) in Radiant auf verschiedenen Taschenrechnern folgende voneinander abweichende Ergebnisse:\n\n\nIn den Schulen haben sich diverse Abkürzungen für die jeweiligen Geräteklassen herausgebildet:\n\nMit Beschluss vom 18. Oktober 2012 hat die Kultusministerkonferenz (KMK) Bildungsstandards für die Allgemeine Hochschulreife in verschiedenen Fächern, darunter im Fach Mathematik, eingeführt und damit für diese Fächer die Einheitlichen Prüfungsanforderungen in der Abiturprüfung (EPA) abgelöst. Das Institut zur Qualitätsentwicklung im Bildungswesen (IQB) stellt im Auftrag der Kultusministerkonferenz einen Pool von Aufgaben zusammen, aus denen sich zukünftig Abiturprüfungen speisen sollen. In diesem Zusammenhang wurden Anforderungen zur Verwendung von digitalen Hilfsmitteln definiert. Als digitale Hilfsmittel zugelassen sind ein „einfacher wissenschaftlicher Taschenrechner“ oder ein Computeralgebrasystem (CAS). Für jedes der beiden digitalen Hilfsmittel wird vorausgesetzt, dass es bei seiner Verwendung einen Zugriff auf Netzwerke jeglicher Art nicht zulässt.\n\nDie Ausführungen zum „einfachen wissenschaftlichen Taschenrechner“ entsprechen den Vorgaben der Bundesländer Baden-Württemberg und Bayern. Nicht vorgesehen ist die Verwendung von programmierbaren Taschenrechnern. Ein Taschenrechner wird als programmierbar angesehen, wenn zusätzliche Routinen gespeichert werden können, die nicht zum ursprünglichen Funktionsumfang gehören. Derzeit erfüllen nur wenige Modelle diese Anforderungen, unter anderem TI-30X Plus MultiView von Texas Instruments. Abgesehen von Bayern und Baden-Württemberg sowie Berlin und Brandenburg erlauben die aktuellen Prüfungsbedingungen der übrigen Länder, sofern als digitales Hilfsmittel in der Abiturprüfung nicht GTR oder CAS vorgeschrieben sind, wissenschaftliche Taschenrechner, die in allen Punkten den Vorgaben des IQB widersprechen. \n\nBei Computeralgebrasystemen wird vorausgesetzt, dass das CAS über typische Funktionen wie das algebraische Lösen von Gleichungen und Gleichungssystemen, Differenzieren und Integrieren, Rechnen mit Vektoren und Matrizen und dergleichen verfügt. Außerdem wird vorausgesetzt, dass das CAS vor seiner Verwendung in der Prüfung in einen Zustand versetzt wird, in dem ein Zugriff auf Dateien und Programme, die nicht zum Lieferumfang oder einem Systemupdate gehören, unterbunden ist.\n\nDie nachfolgende Tabelle wurde anhand der Angaben der Kultusministerien der Bundesländer entwickelt. Soweit diese nicht auffindbar waren, wurden die Angaben der verschiedenen Taschenrechnerhersteller verwendet. Sie gibt die Gegebenheiten an Gymnasien hinsichtlich Zulassung in Prüfungen wieder, da der Einsatz im Unterricht aufgrund der pädagogischen Freiheit der Lehrkraft überall möglich ist. \n\nBis 2014 gab es keine bundesweit einheitlichen Bestimmungen zur Zulassung bestimmter Hilfsmittel zur Matura, da die Matura selbst dezentral, also von den Lehrkräften vor Ort, erstellt wurde. Die Entscheidung, ob ein bestimmtes Hilfsmittel zugelassen war oder nicht, oblag damit der jeweiligen Lehrkraft.\n\nAb dem Maturajahrgang wird es in Österreich eine bundesweit einheitliche Mathematik-Zentralmatura geben. Die Ausführungsbestimmungen werden auch die erlaubten Hilfsmittel festlegen.\n\nGegenwärtig gibt es weder schweizweite noch kantonsweite einheitliche Bestimmungen zur Zulassung bestimmter Hilfsmittel zur Maturitätsprüfung, da die Prüfung dezentral, sprich von den Lehrkräften selbst, erstellt wird. Die Entscheidung, ob ein bestimmtes Hilfsmittel zugelassen ist oder nicht, obliegt damit der jeweiligen Lehrkraft. Die üblichen Maturitätsprüfungen weisen zwei Teile auf, wobei der eine (mehrstündige, schriftliche) mit Taschenrechner, der andere (kurze, mündliche) Teil ohne Taschenrechner abgelegt wird. Die eidgenössische Maturitätsprüfung wird nur mündlich, ohne Taschenrechner, abgelegt.\n\nNachdem Taschenrechner im täglichen (Berufs-)Leben für viele Benutzer zu einem sehr oft gebrauchten Hilfsmittel geworden sind, wurde ihre Funktionalität in Softwareform auch auf PCs und Smartphones gebracht. Unter Windows findet man ihn beispielsweise unter der Kategorie „Zubehör“. Neben den meistens einfach gehaltenen Versionen (damit die Bedienung nicht zu kompliziert wird), wie sie vom Betriebssystemhersteller mitgeliefert werden, gibt es auch eine reichhaltige Auswahl an Fremdprodukten, oft als Gratisprogramme, die auch komplexere Funktionalitäten wie Programmierbarkeit oder Umrechnungen verschiedener physikalischer Größen ineinander bieten. Interpretersprachen, die den Direktmodus unterstützen, wie z. B. GW-BASIC oder das IBM-ROM-BASIC einiger PCs, können eingegebene Terme über den PRINT-Befehl berechnen, z. B. codice_1. Auch Tabellenkalkulationsprogramme sind zu Berechnungen in der Lage.\n\nEbenso haben Mobiltelefone zum allergrößten Teil einen Taschenrechner integriert. Aufgrund deren weiter Verbreitung steht heute eine Taschenrechnerfunktion quasi flächendeckend zur Verfügung. Für häufigere Nutzung, z. B. im Büro, werden jedoch weiterhin dedizierte Taschenrechner bevorzugt.\n\n\n"}
{"id": "35148", "url": "https://de.wikipedia.org/wiki?curid=35148", "title": "Chemometrik", "text": "Chemometrik\n\nUnter Chemometrik oder auch Chemometrie versteht man die chemische Teildisziplin, die sich mit der Anwendung mathematischer und statistischer Methoden beschäftigt, um zum einen in optimaler Weise chemische Verfahren und Experimente zu planen, zu entwickeln oder auszuwählen. Zum anderen kann mit chemometrischen Methoden ein Maximum an chemischen Informationen aus experimentellen Messdaten extrahiert werden. Beispielsweise sind Spektren der Nahinfrarotspektroskopie nur mittels der Chemometrie auswertbar.\n\nAls mathematische Werkzeuge stehen aus dem Bereich der multivariaten Datenanalyse unter anderem folgende Verfahren zur Verfügung\n\n\nAls weiteres Aufgabengebiet einzurechnen wäre das Thema LIMS (LaborInformationsManagementSystem), wobei es um die rechentechnische Unterstützung von laborinternen Abläufen geht, wie\n\n\nDie Chemometrik stellt damit auch eine Brücke zwischen der mathematischen Auswertung von analytischen Rohdaten einerseits und der wirtschaftlichen Auswertung andererseits dar.\n\n\n"}
{"id": "35157", "url": "https://de.wikipedia.org/wiki?curid=35157", "title": "Microsoft Editor", "text": "Microsoft Editor\n\nDer Microsoft Editor (kurz Editor oder zur Unterscheidung auch MS-Editor genannt, in der Eingabeaufforderung unter dem englischen Originalnamen Notepad startbar) ist ein einfacher Texteditor des Unternehmens Microsoft für das Betriebssystem Windows zum Erstellen unformatierter Texte im ANSI- oder Unicode-Format.\n\nNach dem zeilenorientierten Editor EDLIN, der noch unter MS-DOS und älteren NT-Windows-Versionen als Bestandteil des Betriebssystems ausgeliefert wurde, und dem darauf folgenden semigrafischen MS-DOS Editor ist der Editor bereits seit der ersten Windows-Version mit einer grafischen Benutzeroberfläche ausgestattet und als Standard-Texteditor für einfache Textdateien im Betriebssystem voreingestellt.\n\nAb der Version 4.0 (mit Windows NT) unterstützt das Programm – neben dem bereits zuvor verwendeten ANSI-Textformat (siehe auch Windows-1252) – auch Unicode mit den Zeichenkodierungen UTF-16 („Unicode“), UTF-16 Big Endian („Unicode Big Endian“) und UTF-8 („UTF-8“). ASCII (MS-DOS) wird auf NT nicht unterstützt. Aus den Programmen der Standardausstattung kann dies jedoch WordPad und auf 32-Bit-Versionen der alte MS-DOS Editor.\n\nAufgrund seiner Beschränkung auf Basisfunktionen eignet sich die Anwendung kaum zum Bearbeiten längerer Quelltexte oder zum Erstellen von ASCII-Art. Lediglich für kleine Bearbeitungen an Konfigurationsdateien, etwa im INI-Format, für die Windows keine grafische Schnittstelle bietet, ist das Programm geeignet. Ebenso können damit Batchdateien erstellt und verändert werden, indem man die entsprechende Datei mit der Endung codice_1 oder codice_2 speichert. Im Unterschied zu sogenannten Textverarbeitungsprogrammen unter Microsoft Windows gibt es im Editor wie auch in allen anderen Texteditoren nur die tatsächlich eingegebenen Zeichen und keinerlei sonstige Informationen wie Formatierungen. Beim Betrachten einer Datei mit Hilfe des Editors wird ihr Inhalt nicht interpretiert, sondern Zeichen für Zeichen dargestellt, wodurch auch Daten, die sonst dem Benutzer verborgen bleiben, sichtbar gemacht werden können. Einen Hex-Viewer oder Hex-Editor kann dies allerdings nicht ersetzen, zumal der Editor für große Dateien sehr viel Zeit zum Laden benötigt, da dieses die Datei zuerst komplett einlädt, bevor die Anzeige erfolgt. Auch kann das Programm bis Windows 10 1803 nur mit windowsspezifischen Zeilenendungen (codice_3) umgehen, was ihn in einem heterogenen Umfeld nutzlos macht; voraussichtlich mit der nächsten Windows-10-Ausgabe dürfte dieser Umstand behoben werden, der Editor kann dann mit codice_4 (Mac OS Classic) und codice_5 (Unix) umgehen.\n\nFrühere Versionen hatten zudem Probleme beim Öffnen großer Dateien. Unter den 16-Bit-Betriebssystemen bis einschließlich Windows 3.1 wurden Dateien größer als 45 KB im Nur-Lese-Modus geöffnet, Dateien größer als 54 KB konnten gar nicht geöffnet werden. Unter Windows 95, 98 und Me konnte der Editor keine Dateien größer als 64 KB öffnen.\n\nErst ab Windows 98 bzw. Windows NT 4.0 besteht die Möglichkeit, die Schriftart im Editor-Fenster zu ändern. Unter Windows 95 war die Schriftart noch auf die Systemschriftart festgelegt.\n\nNotepad (NT-basierte Windows-Versionen) speichert alle Unicode-Texte stets mit vorangestellter Byte Order Mark (BOM) und erkennt auch daran beim Laden das passende Format.\n\nDer Editor kann (beispielsweise im Gegensatz zu WordPad) auch Unicode-Dateien laden, denen das Byte Order Mark fehlt, wobei es die WinAPI-Funktion codice_6 benutzt. Diese enthält in Windows NT bis Windows XP einen Bug, der als vermeintliches Easter Egg des Editors namens “Bush hid the facts” („Bush hat die Tatsachen verheimlicht“) ausgelegt worden ist. Speichert man diesen Satz mit der Kodierung ANSI, wird er beim nächsten Öffnen nicht mehr korrekt dargestellt, da der Editor den Text als Unicode fehlinterpretiert.\n\nAllerdings tritt der Fehler nicht nur bei diesem Satz auf, sondern bei manchen Sätzen, deren erstes Wort aus einer geraden Anzahl an Buchstaben und alle anderen Wörter aus einer ungeraden Anzahl an Buchstaben bestehen.\n\nWenn in der ersten Zeile einer Datei codice_7 steht, hängt der MS-Editor automatisch nach jedem Öffnen der Datei Datum und Uhrzeit als letzte Zeile an.\n\nIm Unterschied zu diversen visuellen Programmierumgebungen hat der Benutzer bei einem ASCII-Texteditor die volle Kontrolle über das Geschehen, was besonders von puristischen Webseitenentwicklern und Programmierern genutzt wird, die einen WYSIWYG-Editor oder eine überladene integrierte Entwicklungsumgebung ablehnen, auf Windows-Plattformen arbeiten und den Quelltext ohne Hilfe schreiben wollen. Allerdings bevorzugt gerade diese Anwendergruppe Texteditoren mit farblicher Hervorhebung von Syntaxelementen und anderen Erweiterungen, da der Editor über eine nur begrenzte Anzahl an Rückgängig-Schritten verfügt, nicht universell erweiterbar ist und in keiner Weise die Konfigurierbarkeit eines umfangreicheren Editors wie z. B. Emacs oder Vim bieten können.\n\nFreie Vertreter dieser Gattung sind neben Emacs und Vim unter anderem JEdit, Notepad++, Notepad2. Proprietäre Alternativen sind unter anderem TextPad, PSPad und TED Notepad.\n"}
{"id": "35573", "url": "https://de.wikipedia.org/wiki?curid=35573", "title": "ARJ (Computerprogramm)", "text": "ARJ (Computerprogramm)\n\nARJ ist ein kommandozeilenorientiertes Anwendungsprogramm zur Archivierung und Kompression von Dateien. Gleichzeitig bezeichnet ARJ auch das proprietäre Dateiformat dieser Anwendung. Es wurde benannt nach seinem Erfinder Robert K. Jung (\"A\"rchivar \"R\"obert \"J\"ung). Ursprünglich für MS-DOS geschrieben gibt es mittlerweile mit ARJ32 eine Version für Windows.\n\nIm Jahr 2002 wurde eine freie Version der Software unter der GNU GPL veröffentlicht, die mittlerweile auf mehr Plattformen (Linux, Solaris und OS/2) lauffähig ist als die ursprünglichen Versionen der ARJ Software Inc.\n\nTeile des Verfahrens waren durch ein US-Patent geschützt.\n\nAnfang bis Mitte der 1990er Jahre war ARJ unter MS-DOS und Windows 3.11 das am weitesten verbreitete Packprogramm nach PKZIP, verdrängte das ZIP-Format aber nie von seinem Spitzenplatz. Gegenüber PKZIP und anderen Packprogrammen bot es den Vorteil, dass zum Packen und Entpacken das gleiche Programm verwendet wurde, wenngleich es auch einen schlanken separaten Entpacker gibt. ARJ zeichnete sich weiter dadurch aus, dass es eine Vielzahl von Kommandos und Schaltern besitzt, die nicht nur zum Einstellen der Kompressionsdichte und Packgeschwindigkeit dienen, sondern auch einen umfänglicheren Eingriff bieten in der Art, wie Dateien strukturiert, durchsucht und entpackt werden sollen. Fast alle Kommandos und Schalter lassen sich beliebig kombinieren, wobei die Reihenfolge mitunter relevant für das Ergebnis ist.\nDes Weiteren besitzt ARJ eine für damalige Verhältnisse robustere Handhabung mehrteiliger Archive (\"multi-volume archives\") und eroberte so in den Mailbox-Systemen einen Nischenbereich, welcher aber bereits Mitte der 1990er Jahre vom RAR-Format abgelöst wurde.\n\nBis zur Version 2.5 gab es einige Versionen, die in deutscher Sprache erschienen. Auch existieren mehrere Programme, die mit einem grafischen Aufsatz für Windows dem Programm zur fehlenden Menüführung verhalfen – etwa WinARJ.\nAls Kommandozeilen-Programm ist es nach der Einführung von Windows 95 aber nahezu komplett von ZIP und RAR verdrängt worden, welche eigene menügeführte Oberflächen brachten.\n\n\n"}
{"id": "36402", "url": "https://de.wikipedia.org/wiki?curid=36402", "title": "ICab", "text": "ICab\n\niCab ist ein Webbrowser für Mac OS, macOS und iOS.\n\nEin Ziel von iCab ist es, ein möglichst standardkonformer Browser zu sein. So zeigt iCab für Webseiten die Konformität zum HTML-Standard an, listet auf Wunsch auch die gefundenen Fehler und bietet im Kontextmenü die Option zur Überprüfung einer Webseite durch den W3C-Validator an. Ab Version 3.0 besteht iCab den Acid-Test, mit dem die Darstellungsfähigkeiten von Webbrowsern geprüft werden.\n\niCab hat seine Wurzeln in CAB, dem \"Crystal Atari Browser.\" Dessen Entwickler, der Darmstädter Alexander Clauss, ist auch heute noch die fast einzige Person, die an iCab schreibt, lediglich die JavaScript-Engine stammt von Thomas Much.\n\nAls iCab Anfang 1999 erschien, beeindruckte es durch seine Geschwindigkeit, geringe Größe und sparsamen Umgang mit dem zur Verfügung stehenden Speicher im Vergleich mit den damals zur Verfügung stehenden Alternativen Netscape Navigator und Internet Explorer. Dazu kamen umfangreiche Filterfunktionen für teilweise unerwünschte Web-Inhalte wie Werbebanner, Cookies, Pop-up-Fenster und mehr, die im Laufe der Zeit immer ausgefeilter wurden. iCab ist zurzeit der Browser mit den meisten Filteroptionen.\n\nObwohl iCab seit seinem Erscheinen ein vollständiger Webbrowser ist, wurden die bisher erschienenen Versionen lange Zeit als \"Preview\" (= Vorschau) bezeichnet, um anzuzeigen, dass noch nicht alle geplanten Funktionen realisiert worden sind. Insbesondere die CSS-Unterstützung war lange Zeit noch unvollständig. Eine „fertige“ Version war bereits für Anfang 2002 erwartet worden; die Fertigstellung hat sich jedoch verzögert. Eine erste Beta zu Version 3.0 ist im Dezember 2004 erschienen.\n\nDie Version 3 von iCab ist der einzige moderne Browser, der noch für Mac OS ab Version Mac OS 8.5 weiterentwickelt wird. Die Mozilla-Entwickler empfehlen allen Mac OS 9-Nutzern den Wechsel zu iCab.\n\nDie Version 4 von iCab, erschienen am 1. Januar 2008, wurde komplett neu geschrieben, basiert jetzt auf Cocoa anstatt Carbon, womit die Rendering Engine WebKit zum Einsatz kommt, und benötigt mindestens Mac OS X 10.3.9.\n\nFür ältere Systeme (einschließlich 68k-Macs) ist noch die Version 2.9.9 erhältlich, die jedoch nicht mehr weiter entwickelt wird.\n\nFür das iPhone, den iPod Touch und das iPad von Apple gibt es eine spezielle Version des Browser, genannt iCab Mobile, die seit dem 30. Mai 2018 in der Version 9.11 vorliegt.\n\nDie letzten Versionen des Browsers für Desktop- und Laptop-Rechner:\n\nVersionsgeschichte für iCab Mobile (nur die größeren Versionen und die aktuellste Version):\n\n\n"}
{"id": "36658", "url": "https://de.wikipedia.org/wiki?curid=36658", "title": "Linspire", "text": "Linspire\n\nLinspire (vormals \"LindowsOS\") war eine auf Debian basierende Linux-Distribution, deren Aussehen dem Betriebssystem Microsoft Windows ähnelt.\n\nDie Firma, die Linspire entwickelte und vertrieb, hieß von 2001 bis 2004 Lindows, ab 2004 dann Linspire, jeweils entsprechend dem Namen der Distribution. 2008 wurde sie in Digital Cornerstone umbenannt und am 1. Juli 2008 von der Firma Xandros gekauft, in der sie vollständig aufging. Seither ist keine weitere Version von Linspire mehr erschienen. Die letzte Version 6.0 wurde am 10. Oktober 2007 veröffentlicht.\n\nDas US-amerikanische Unternehmen Linspire, Inc. (vormals Lindows, Inc.) mit Sitz in San Diego, Kalifornien entwickelte das gleichnamige Betriebssystem auf Debian-Grundlage. Michael Robertson gründete vermutlich im August 2001 das Unternehmen aus MP3.com heraus. Am 20. Juli 2005 gab Robertson den Vorsitz als CEO an den Präsidenten und Gründer von Linspire, Kevin Carmony, ab. Er selbst blieb aber Vorstandsvorsitzender und widmete sich darüber hinaus anderen Projekten.\nAm 30. Juni 2008 gab Kevin Carmony in seinem Blog den Verkauf des Unternehmens an Xandros bekannt. Im Sommer 2008 kam es zur erwarteten Konsolidierung der Distributionen aus dem Hause Xandros. Im Ergebnis dessen wurde Linspire eingestellt.\n\nBereits während der Entwicklung von LindowsOS ging Microsoft gegen Michael Robertson wegen der angeblichen Verwechslungsgefahr und Namensähnlichkeit gerichtlich vor. In der Folge haben US-Gerichte festgestellt, dass der Begriff \"Windows\" nicht geschützt werden kann, da „Fenster“ ein allgemeiner Begriff ist. Der Richter lehnte 2002 die Klage unter anderem mit der Begründung ab, Microsoft selbst habe den Begriff Windows bereits verwendet, als das Betriebssystem Windows noch gar nicht auf dem Markt war, und die graphische Fenstertechnik (GUI) sei bereits von Xerox und von Apple früher als von Microsoft entwickelt und vermarktet worden.\n\nDas Verfahren lief von 2001 bis 2004, bisher hatte sich jedoch Microsoft immer bis zur nächsten Instanz berufen, der nächste Prozesstermin für die Namensklage gegen Lindows wurde auf 2003 verschoben. Das Gericht wollte von Microsoft zuerst mehr Beweise sehen. Für LindowsOS hat diese Verschiebung bisher keine Produktionsverzögerung bedeutet; die damit verbundene Publizität brachte dagegen dem neuen Betriebssystem sicher mehr Aufmerksamkeit, als dass es ihm geschadet hätte.\n\nIn Belgien, Finnland, Luxemburg, den Niederlanden und Schweden entschieden die Gerichte gegen Lindows und für Microsoft. Das Betriebssystem darf dort nur noch unter dem Namen „Lin---s“ (sprich: Lin-Dash) vertrieben werden, ebenso darf man die original Homepage „Lindows.com“ nicht mehr besuchen. In den genannten Ländern heißt diese Website „Lin---s.com“. Aus diesen Gründen nannte sich das Unternehmen und sein Produkt im April 2004 in Linspire um.\n\nIm Juli 2004 einigten sich Microsoft und Linspire außergerichtlich. Linspire erhielt von Microsoft 20 Millionen US-Dollar, im Gegenzug wurden alle Rechtsstreitigkeiten zwischen beiden Unternehmen eingestellt.\n\nLinspire sollte den Umstieg von Windows nach Linux erleichtern und hatte deshalb seinen Fokus auf die unkomplizierte Einsatzmöglichkeit in Schulen, Unternehmen und im Privatbereich gelegt. Linspire strebte eine Linux-Distribution an, die durch den durchschnittlichen Benutzer einfach zu bedienen ist.\n\nLinspire basiert auf Debian. Als besonderes Merkmal in der ersten Version von Linspire wurde die Möglichkeit angepriesen, Windows-Anwendungen durch Wine-Unterstützung installieren und verwenden zu können. Ab der Version 5 wurde dieser Schwerpunkt aufgegeben, gerade auch wegen der inzwischen fast grenzenlos erscheinenden Menge an Open-Source-Software.\n\nPositiv anzurechnen ist, dass Linspire eine einfache Installation von Programmen ermöglicht, die von ihnen „Click’n’Run“ (CNR, basierend auf Debians APT) genannt wurde.\n\nAPT ist zwar freie Software, seine Bedienung erschien den Entwicklern von Linspire jedoch für Linux-Neulinge als noch zu kompliziert. Daher wurde eine besonders einfach zu bedienende Benutzeroberfläche und ein leicht verändertes Paketsystem für eine jährliche Gebühr bereitgestellt. CNR basiert inzwischen nicht mehr auf APT. Die Installationsdateien mit der Endung codice_1 sind lediglich Pakete im .deb-Format (Debian-Paketdateien) mit einer DRM-ähnlichen Verschlüsselung. Linspire fragt vor der Installation, ob das Paket rechtmäßig erworben wurde. Nach dieser Abfrage kann es installiert werden.\n\nLinspires CNR Service (scherzhaft auch „Collect New Revenue“) erlaubte den Benutzern angeblich \"uneingeschränkten Zugriff auf Software, die im CNR-Warenhaus lagert\". Der Dienst erlaubt die Installation aller Anwendungen mit einem einzigen Klick. CNR enthält auch den CNB „Click’N’Buy“, der lediglich eine Menge zusätzlicher kommerzieller oder proprietärer Software enthält, und zwar ausschließlich für bezahlende Mitglieder. Zeitweise enthielt der CNR eine Palette von über 2500 unterschiedlichen Produkten – beginnend von sehr einfachen Anwendungen zu großen kommerziellen Hauptanwendungen wie Win4Lin und StarOffice. Die Programme waren mit größtenteils redaktionellen Anmerkungen und Bewertungen aus der Community versehen. Angeblich verursachte der Breitbanddownload dem Hersteller hohe Kosten. Linspire bot zwei verschiedene Preisklassen für seinen jährlichen CNR-Service an:\n\n\nLinspire war bei vielen Linux-Nutzern verpönt, da es – in der „CNR Gold Service“-Version – auch für die Installation von Programmen Geld verlangt, die sonst kostenlos im Internet verfügbar sind.\n\nEin Kritikpunkt an Linspire war, dass alle Benutzer zunächst einen root-Status erhielten, ohne konsequenterweise von der Distribution selbst über die damit verbundenen Gefahren informiert zu werden. Insbesondere erfahrene Nutzer wiesen vor der aktuellen Version darauf hin, dass dies zu einer Gefährdung der Systemsicherheit führe. Auf diese Weise kann nämlich jeder Nutzer (auch von außen) alle Dateien im System manipulieren. Dies erhöht die Gefahr durch Schadprogramme wie Viren oder Trojaner. Michael Robertson selbst argumentiert, dass es keinen nennenswerten Unterschied zwischen root und Benutzer gebe. Dies wird aber jetzt in der neu zum Download angebotenen Version (Freespire 2.0.6) vermieden, indem man in der Default-Einstellung nicht mehr „root“ ist. Man wird hier nunmehr ausdrücklich danach gefragt, ob man denn in den „root“-Status wechseln wolle, wenn man das Bedürfnis hat, grundlegende Sachen zu verändern. Dadurch wurde dieser Hauptkritikpunkt mit dem „root“-Problem behoben.\n\nWährend einige Linspire und Debian als einander sehr ähnliche Produkte betrachten, argumentieren andere, Linspire unterscheide sich von Debian und anderen Distributionen deutlich durch Dutzende proprietäre Programme auf seiner Installations-CD, wie zum Beispiel Unterstützung für MP3, DVD, Quick Time, Java, Flash, Real, Windows Media, Adobe PDF und proprietäre Treiber. Einige davon sind lizenziert, andere sind Produkte Dritter, die Gebühren von den Lizenznehmern fordern und auch von Linspire legal lizenziert wurden.\n\nDie Standard-Linspire-Installation enthält keine Network Services. Ebenso ist eine strenge Firewall installiert, die alles bis auf den Port 22 (SSH) abblockt.\n\nMit Linspire 6.0 begann auch Linspire den Weg einiger Linuxdistributoren zu folgen und bot seitdem von Microsoft lizenzierte Codecs an, die in das System integriert waren.\n\nLinspire sponserte das IRMA Project: Es soll Benutzern aus aller Welt ermöglichen, Linspire in verschiedene Sprachen zu übersetzen. IRMA unterstützt über 50 Fremdsprachen mit Hilfe von über 1.500 Übersetzern, wie etwa Englisch, Deutsch, Spanisch, Japanisch, Portugiesisch oder Italienisch.\n\n\"Linspire, Inc\" sponserte sehr viele Open-Source-Projekte, wie Pidgin, Kopete (Instant-Messaging-Clients), KDE-Apps.org und KDE-Look.org und das Nvu-Projekt, das eine Entwicklung eines WYSIWYG-Webseiten-Editors, basierend auf dem Mozilla-Composer-Quellcode vorsieht. In der Vergangenheit hat Linspire, Inc. dem Wine-Projekt über 500.000 US-Dollar gespendet und richtete ebenso mehrere Linux- und Open-Source-Veranstaltungen aus wie das jährliche \"Desktop Linux Summit\", die \"Debconf\" und die \"KDE Developers Conference\". Linspire war Mitglied in der DCC.\n\nLinspire bot folgende Editionen an:\n\n\nIm August 2005 erschien eine Live-CD namens \"Freespire\" im Internet. Freespire war eine auf dem Linspire-Quellcode basierende Linux-Distribution. Diese Distribution wurde von einem Linspire-Fan generiert und war von Linspire selbst weder produziert noch veröffentlicht worden. Freespire sorgte deshalb für einige Irritationen bei den Benutzern, die es für ein Produkt von Linspire hielten. Der Urheber wechselte freiwillig den Namen, um solche Verwechselungen zu vermeiden. Gleichzeitig mit dem Namenswechsel bekam das ehemalige Freespire-Projekt den Codenamen „Squiggle“ und die Überlegung, einen neuen Distributionsnamen zu suchen. Linspire reagierte sofort mit dem „free Linspire“-Angebot an die Benutzer bis zum 9. September 2005. So machten beide Projekte gegenseitig geschickt Werbung für sich.\n\nAb dem zweiten Quartal 2006 gab es allerdings ein neues, von Linspire initiiertes und gefördertes Projekt, das den Namen Freespire trug. Freespire war, wie der Name schon andeutet, eine Gratis-Variante von Linspire; sein Konzept ähnelte dem von Fedora.\n\n2003 strengte Linspire einen Gerichtsprozess gegen seinen einstigen Partner Xandros an. Linspire behauptete, dass Xandros eine Rückzahlung eines Kredits in Höhe von 750.000 US-Dollar verweigere und dass sowohl dieses Unternehmen als auch weitere Beklagte in Etikettenschwindel, Betrug und kriminelle Falschdarstellungen verwickelt seien. Während der Verhandlung von Linspires Investition in Xandros kam diese Information am 20. April 2004 ans Licht. Die beiden Unternehmen einigten sich im Juli 2005 außergerichtlich.\n\nMan vergleiche Linspire mit Xandros und Lycoris, die ein ähnliches Prinzip wie Linspire verfolgen.\n"}
{"id": "37831", "url": "https://de.wikipedia.org/wiki?curid=37831", "title": "Grid-Computing", "text": "Grid-Computing\n\nGrid-Computing ist eine Form des verteilten Rechnens, bei der ein virtueller Supercomputer aus einem Cluster lose gekoppelter Computer erzeugt wird. Es wurde entwickelt, um rechenintensive Probleme zu lösen. Heute wird Grid-Computing in vielen Bereichen, zum Teil auch kommerziell, eingesetzt, so zum Beispiel in der Pharmaforschung und den Wirtschaftswissenschaften, beim elektronischen Handel und bei Webdiensten. Es wird auch zum Risikomanagement in der Baudynamik und beim Finanzmanagement genutzt.\n\nVon typischen Computerclustern unterscheidet sich Grid-Computing in der wesentlich loseren Kopplung, der Heterogenität und der geographischen Zerstreuung der Computer. Des Weiteren ist ein Grid meistens bestimmt für eine spezielle Anwendung und nutzt häufig standardisierte Programmbibliotheken und Middleware.\n\nDie Grundlagen des Grid Computing stammen von Ian Foster, Carl Kesselman und Steven Tuecke (2001).\n\nDer erste Versuch einer Definition stammt von Ian Foster und Carl Kesselman in dem Buch „The Grid: Blueprint for a New Computing Infrastructure“:\n\nDa diese Definition vor der eigentlichen Entstehung von Grids verfasst wurde, wurde sie von Ian Foster in der zweiten Auflage des Buches noch einmal deutlich verfeinert:\n\nDer wesentliche Unterschied zur ursprünglichen Definition ist, dass die gemeinsame Nutzung von Ressourcen durch Virtuelle Organisationen bestimmt wird. Diese spielen auch in den heutigen Implementationen des Grids eine zentrale Rolle. Auch werden nun nicht mehr nur Hochleistungsrechner als Ressource bezeichnet, sondern allgemeine Ressourcen, wie beispielsweise physikalische Experimente.\n\nEs gibt noch weitere Versuche einer einheitlichen Definition in der Literatur (vgl.).\n\nFälschlicherweise werden Systeme wie Cluster Computing, Peer-to-Peer-Computing oder Meta computing als Grid-Systeme bezeichnet. Diese weisen zwar Aspekte des Grid-Computing auf, jedoch fehlen wesentliche Punkte um sie als Grid zu bezeichnen. Dieser Problematik hat sich Ian Foster in einer 3-Punkte-Checkliste angenommen. Die Eigenschaften eines Grid-Systems werden in kurzen Worten wie folgt definiert:\n\nVolunteer-Computing-Projekte (z. B. SETI@home) werden in der Regel nicht zu den Grid-Computing-Systemen gezählt, da die Rechenleistung dort, anders als beim Grid-Computing, nicht von Organisationen, sondern von anonymen und möglicherweise nicht vertrauenswürdigen Clients zur Verfügung gestellt wird.\n\nKonzepte für die Verteilung rechenintensiver Aufgaben hat es bereits in den 1960er Jahren gegeben. Die meisten der heutigen Forschungsarbeiten an Grid-Systemen haben ihren Ursprung jedoch in frühen Experimenten mit Hochgeschwindigkeitsnetzen. In diesem\nZusammenhang sind insbesondere die Projekte FAFNER und I-WAY zu nennen.\n\nDer Begriff Grid hat seinen Ursprung in dem Vergleich dieser Technologie zum Stromnetz (engl. Power Grid). Demnach soll das Grid einem Benutzer ebenso einfach Ressourcen wie z. B. Rechenleistung oder Speicherplatz über das Internet zur Verfügung stellen, wie es möglich ist Strom aus einer Steckdose zu beziehen. Der Benutzer übergibt seinen Auftrag über genormte Schnittstellen an das Grid, woraufhin die Ressourcenallokation automatisch erfolgt.\n\nDas I-WAY-Projekt (Information Wide Area Year) wurde im Jahr 1995 innerhalb der Gigabit-Testumgebung an der University of Illinois, in dem 17 Einrichtungen in den USA und Kanada zu einem Hochgeschwindigkeitsnetz zusammengeschlossen wurden, durchgeführt. Es hatte zur Aufgabe, verschiedene Supercomputer unter Verwendung existierender Netzwerke zu verbinden. I-WAY hat die Entwicklung des Globus-Projekts in hohem Maße unterstützt.\n\nDie motivierende Zielsetzung, die zu der Entwicklung der Grid-Technologie geführt hat, war die gemeinsame, koordinierte Nutzung von Ressourcen und die gemeinsame Lösung von Problemen innerhalb dynamischer, institutionsübergreifender, virtueller Organisationen (vgl.). Das bedeutet, nach der Festlegung von Abrechnungsdaten und Rechten soll ein direkter Zugang zu beispielsweise Rechenleistungen, Anwendungen, Daten oder Instrumenten gemeinschaftlich ermöglicht werden. Eine virtuelle Organisation (VO) ist in diesem Zusammenhang ein dynamischer Zusammenschluss von Individuen und/oder Institutionen, die gemeinsame Ziele bei der Nutzung des Grids verfolgen. Zwar liegt der Fokus vieler Arbeiten beim verteilten Rechnen, dennoch ist oberstes Ziel, analog zu der Entstehung des Internets, die Entwicklung eines einheitlichen, globalen Grids.\nGrob gesprochen lassen sich Grid-Computings unterteilen in Klassen, wie\n\nDie Klasse des \"Computing grid\" ist vergleichbar mit dem \"Power grid\", also dem Stromnetz: Dazu stellt der Verbraucher von Rechenleistung eine Verbindung zum Rechennetz her, ähnlich wie der Stromverbraucher zum Stromversorgungsnetz. Dort ist alles, was hinter der Steckdose passiert, für den Konsumenten verborgen; er nutzt einfach die angebotene Leistung.\n\nIn der Klasse des \"Data grid\" kooperieren nicht nur die (Hochleistungs-)Computersysteme der Beteiligten, um Rechenleistung zur Verfügung zu stellen, sondern auch Datenbestände werden verknüpft. Zugang zu solchen Grids bietet meist ein Grid-Portal.\n\nDaneben wird auch die Bereitstellung von Netzwerkressourcen „gridifiziert“, d. h. eine automatische Auswahl aus einem \"Pool\" von Ressourcen aufgrund bestimmter QoS-Parameter getroffen. Idealerweise sollte die Wahl der Ressourcen applikationsgetrieben, also abhängig von der Anwendung im Computing-Grid oder Data-Grid sein.\n\nZur Architektur eines Grid gibt es mehrere Konzepte. Jedem bekannten Konzept ist eigentümlich, dass es außer der nachfragenden Instanz und der von dort gestellten tatsächlichen Leistungsanforderung eine koordinierende Instanz für die Agglomeration von Rechenleistung und für die Zusammenführung der Teilleistungen geben muss. Außerdem ist eine strenge Hierarchie erforderlich, welche die Agglomeration von Rechenleistung nach objektiven Kriterien zulässt oder ausschließt. Jeder Computer in dem „Gitter“ ist eine den anderen Computern zunächst hierarchisch gleichgestellte Einheit (Peer-2-Peer).\n\nDie typischen Aufgaben, bei denen sich Grid-Computing als Strategie anbietet, sind solche, die die Leistung einzelner Computer überfordern. Dazu gehören beispielsweise die Integration, Auswertung und Darstellung von sehr großen Datenmengen aus der naturwissenschaftlichen und medizinischen Forschung. In der Routine werden die Techniken auch angewandt in der Meteorologie und rechenintensiven Simulationen in der Industrie. Insbesondere die Teilchenphysik mit Großexperimenten (z. B. der Large Hadron Collider) als naturwissenschaftliche Anwendung ist ein Vorreiter in der Weiterentwicklung und Etablierung von Grid-Technologien.\n\nDie typischen Probleme, die Grid-Computing mit sich bringt, sind der steigende Aufwand als Teil der verfügbaren Leistung für die Koordination. Daher steigt die Rechenleistung wegen des Koordinationsaufwandes nie linear mit der Zahl der beteiligten Rechner. Dieser Aspekt tritt bei komplexen numerischen Aufgaben in den Hintergrund.\n\nEine mögliche Softwarearchitektur für Grids ist die von Ian Foster mitentwickelte Open Grid Services Architecture (OGSA).\nDiese wird in Ansätzen bereits in ihrem Vorläufer, der Open Grid Services Infrastructure (OGSI), beschrieben. Deren Grundidee ist die Darstellung von beteiligten Komponenten (Rechner, Speicherplatz, Mikroskope, …) als Grid-Services in einer offenen Komponentenarchitektur.\n\nMit der Konvergenz der Web-Services des W3C und des OGSA Standards des Open Grid Forum (OGF) wurden Grid-Services auf einer technischen Ebene, so wie sie beispielsweise im Globus Toolkit 4 implementiert sind, zu Web-Services, welche die technischen Funktionalitäten der Grid-Middleware ermöglichen. OGSA schlägt in diesem Zusammenhang den Einsatz von WSRF (dem \"Web Services Resource Framework\") als grundlegenden Baustein für Service-Grids vor. So bekommen die Web-Services, deren Einsatz einheitliche Zugriffsverfahren auf die einzelnen Dienste eines Grids ermöglicht, zusätzlich noch einen Zustand: Sie werden mit statusbehafteten (engl. stateful) Ressourcen (Wie Dateien, Java-Objekten bzw. POJOs, Datensätzen in einer Datenbank) assoziiert. Dies ermöglicht es erst, Funktionalitäten auszuführen, die sich über mehrere Transaktionen erstrecken. Die Kombination eines Web-Services mit einer solchen statusbehafteten Ressource bildet eine sogenannte WS-Resource.\n\nEin zentrales und hardwareunabhängiges Konzept hinter der Grid-Philosophie ist das der virtuellen Organisationen (VO, siehe dort). Dabei werden Ressourcen (bzw. Services) dynamisch virtuellen Organisationen zugewiesen.\n\n\nPraktisch gesehen benötigt man an Hardware nichts weiter als einen Computer mit einer Netzwerkverbindung. Auf diesen Grid-Computern übernimmt eine Software das Lösen einer Teilaufgabe, welche beispielsweise mit Hilfe von Software, die eine große Aufgabe in eine Anzahl von Teilaufgaben für alle Knoten im Grid aufspalten kann und die Teilergebnisse wieder zusammenfasst, generiert wurde.\n\nDas im März 2006 zu Ende gegangene EGEE-Projekt (Enabling Grids for E-sciencE, früher Enabling Grids for E-science in Europe) ist das größte Grid-Projekt der Europäischen Union, mittlerweile mit weltweitem Einsatz. Unter dem Namen EGEE2 wird es seit\nApril 2006 fortgesetzt. Das Projekt wurde in der ersten Projektphase von der EU mit 32 Millionen Euro gefördert und stellt die weltweit größte Grid-Infrastruktur dar. 2010 wurde EGEE durch European Grid Infrastructure (EGI) abgelöst.\n\nBeteiligt sind unter anderen CERN (Schweiz), Karlsruher Institut für Technologie (KIT, Deutschland), Rutherford Appleton Laboratory (RAL, Vereinigtes Königreich), Istituto Nazionale di Fisica Nucleare (INFN, Italien) und Academia Sinica (ASCC, Taiwan). Siehe auch: Enabling Grids for E-sciencE.\n\nDas offene Grid Nordugrid auf der Basis der Grid-Middleware ARC ist aus einem Zusammenschluss von fünf skandinavischen Instituten hervorgegangen.\n\n\"Building and Promoting a Linux-Based Operating System to Support Virtual Organizations for Next Generation Grids\" ist ein Projekt, welches im 6. Rahmenprogramm von der Europäischen Union gefördert wird. Neben 17 europäischen Projektpartnern sind auch zwei aus China an XtreemOS beteiligt. Es ist im Juli 2006 gestartet und soll vier Jahre lang laufen.\n\nNeugrid ist ein Projekt, das von der Europäischen Union im 7. Rahmenprogramm finanziert wurde. Seine Infrastruktur ermöglicht der Wissenschaft die Erforschung von neurodegenerativen Erkrankungen.\n\nWie in diversen anderen Ländern (z. B. US: Cyberinfrastructure, UK: UK e-Science / OMII, NL: BIG-Grid) gibt es auch in Deutschland und Österreich Grid Initiativen zum Aufbau einer nationalen Grid-Infrastruktur.\n\n\n"}
{"id": "37866", "url": "https://de.wikipedia.org/wiki?curid=37866", "title": "Aurox Linux", "text": "Aurox Linux\n\nAurox Linux war die bekannteste Linux-Distribution aus Polen. Lizenziert ist Aurox unter GPL. Die Entwicklung von Aurox Linux wurde Ende Oktober 2006 eingestellt, eine Fortführung durch die Firma Coba Solutions, die die Distribution aufgebaut hat, kam nicht zu Stande. Als Nachfolger wurde aber von einigen Entwicklern die Distribution \"Jazz Linux\" gestartet.\n\nAurox Linux basierte ursprünglich auf Fedora, brachte jedoch viele eigene Komponenten und einen eigenen Kernel mit. Die Distribution wurde in Polen entwickelt und erschien quartalsweise als Zeitschrift des polnischen Linux+-Magazins mit der jeweils aktuellen Version und etlicher zusätzlicher Software auf 7 CDs, 1 DVD und ergänzenden Tipps, zudem stand die aktuelle auch zum kostenlosen Herunterladen mit vermindertem Umfang in Form von ISO-Abbildern zur Verfügung. Sie unterstützte die meisten europäischen Sprachen. Die letzte Version war Aurox 12 vom September 2006.\n\nZielsetzung des Projekts war es, eine Distribution zu schaffen, die speziell auf europäische Anforderungen zugeschnitten ist. Außerdem wurde sehr viel Wert auf Kompatibilität mit Notebooks gelegt, so dass alle wichtigen Technologien unterstützt wurden, wie beispielsweise ACPI oder W-LAN. Aurox Linux ermöglichte zudem als eine von wenigen Distributionen das Abspielen von CSS-Verschlüsselten Video-DVDs ohne Nachinstallation weiterer Programme oder Codecs.\n\n\n"}
{"id": "38154", "url": "https://de.wikipedia.org/wiki?curid=38154", "title": "AppleTalk", "text": "AppleTalk\n\nAppleTalk ist eine Gruppe von Netzwerkprotokollen und wurde von Apple Computer Ende 1983 entwickelt, um einen einfachen Zugang zu gemeinsamen Ressourcen wie Dateien oder Druckern im Netz zu ermöglichen. AppleTalk ist ein eingetragenes Warenzeichen von Apple, Inc.\n\nDurch die initiale exklusive Verknüpfung von AppleTalk mit der später zu LocalTalk umbenannten Netzwerkhardware, wird oft fälschlicherweise AppleTalk als Synonym für eine LocalTalk-Verkabelung benutzt.\n\nDie Entscheidung, eigene Netzwerkprotokolle, sowie LocalTalk als proprietäres Übertragungsmedium zu etablieren, ergab sich aus den 1983 vorherrschenden Gegebenheiten: Netzwerkhardware war selten standardisiert und sehr teuer; die Kosten erreichten oft die Anschaffungskosten für einen Personal Computer selbst.\n\nAngesichts der weiten Verbreitung von IP-basierten Netzwerken wurde AppleTalk ab Mac OS X 10.6 von Apple aufgegeben. Alle wesentlichen Dienste wurden entweder auf TCP/IP abgebildet (Beispiel: Dateizugriff über AFP, TCP Port 548) oder durch andere, bereits bestehende Protokolle ersetzt (Beispiel: Druckerzugriff über LPR oder JetDirect Ports oder IPP, Finden von Geräten und Diensten im Netzwerk mit Bonjour). Bonjour ist in seinen Fähigkeiten allerdings auf ein Netzwerksegment eingeschränkt, ebenso besteht keine Möglichkeit mehr, netzwerkfähige Geräte über \"Zonen\" logisch zu ordnen.\n\nEs gab für Nicht-Apple-PCs auch ISA-, MCA- sowie SBus-Karten mit LocalTalk-Schnittstelle, so dass solche PCs in ein AppleTalk-Netzwerk integriert werden und so Daten ausgetauscht und Drucker gemeinsam genutzt werden konnten.\n\nAppleTalk basiert zu großen Teilen auf dem (nicht patentierten) Cambridge Ring.\n\nAppleTalk beherrscht zwei Adressierungsmodi: \"Extended\" (Phase 2) und \"Nonextended\" (Phase 1).\n\nAppleTalk adressiert Geräte anhand einer dynamisch zugewiesenen Adresse. Diese Adresse setzt sich aus einer 16-Bit Netzwerkadresse (nur Phase 2) und einer 8-Bit Node-ID zusammen. Die Netzwerkadresse kann innerhalb eines zusammenhängenden Bereiches von einem Router vorgegeben werden. Die Netzadresse 0 steht dabei für das lokale Netzwerksegment, die Netzadressen 65280-65534 sind reserviert. Die Node-ID 0 ist ungültig, 1-127 sind für Benutzer gedacht, 128-254 für Server, 255 für Broadcast.\nDaran schließt sich eine 8-Bit Socket-Nummer zur Unterscheidung der einzelnen Dienste auf dem jeweiligen Gerät an. Die Sockets 0 und 255 sind ungültig, 1-127 sind für die statische und 128-254 sind für die dynamische Zuteilung gedacht.\n\nEine typische AppleTalk-Adresse wäre 1248.33:4.\n\nAppleTalk kennt das Konzept von \"Zonen\". Diese ermöglichen eine logische Gruppierung von Geräten, die nicht mit der physischen Struktur von Netzwerksegmenten übereinstimmen muss. Phase 2-Netzwerke unterstützen mehr als eine Zone pro Segment. Der Benutzer kann die Zonenzugehörigkeit dann wählen. Diese Zonenliste taucht im Benutzerprogramm \"Auswahl\" auf.\n\nDamit sich Benutzer nicht mit wenig eingängigen numerischen Adressen herumschlagen müssen, ist ein dynamischer Namensdienst (NBP) vorgesehen. Serverdienste registrieren sich mit dem jeweils lokalen NBP-Dienst. Netzwerkseitig basiert dieser auf Multicasts. Die \"Auswahl\" sendet Multicasts aus und trägt die individuellen Antworten zu einer Listenansicht zusammen.\n\nDie AppleTalk-Familie umfasst folgende Protokolle (gruppiert nach Netzschichten):\n\n\n\n\n\nEs werden mehrere Verbindungszugriffsprotokolle (Link-Access Protocols, LAP) unterstützt, die durch den sogenannten \"LAP-Manager\" verwaltet werden.\n\nDie Bitübertragungsschicht umfasst die Treiber für Netzschnittstellen.\n\nDie AppleTalk-Protokolle lassen sich in mehrere Schichten einteilen, die einen Protokollstapel (\"protocol stack\") bilden.\nDie Protokolle lassen sich wie folgt in das ISO-OSI-Referenzmodell einordnen:\n\nAppleTalk ist routingfähig, alle beteiligten Router müssen dabei AppleTalk unterstützen, gilt aber als „geschwätzig“, produziert also ständige Paketübertragungen, wobei einige AppleTalk-fähige Router (z. B. von Cisco oder Netopias von Farallon) in der Lage waren, diesen vor allem aus Keepalive-Paketen bestehenden Datenverkehr zu emulieren (AppleTalk-\"spoofing\"), so dass eine Dauerverbindung dadurch umgangen werden konnte. Verbindungen über X.25-Netzwerke waren ebenso möglich.<br>\nDie Verbindung zweier entfernter AppleTalk-Netzwerke z. B. über das TCP/IP-basierte Internet ist also nicht unmittelbar, sondern nur durch Verkapselung (z. B. durch Kinetics Internet Protocol) möglich, siehe Produktbeschreibung vom \"Apple Internet Router\".\n\nDurch die geringe Paketgröße des zugrundeliegenden Datagram Delivery Protocol (13 Byte Header, 587 Bytes Nutzdaten) skaliert AppleTalk auf schnelleren Übertragungsstrecken als 10 MBit/s Ethernet nur schlecht, der Datendurchsatz auf Fast Ethernet beträgt bei schnellen beteiligten Netzwerkkomponenten ungefähr ein Drittel des möglichen.\n\n"}
{"id": "38236", "url": "https://de.wikipedia.org/wiki?curid=38236", "title": "Apple II", "text": "Apple II\n\nDer Apple II (auch Apple ][ oder Apple //) des Unternehmens Apple Computer gehört zu den ersten 8-Bit-Mikrocomputern, die eine weite Verbreitung fanden. Zusammen mit dem Vorgängermodell, dem Apple I, ist er der bislang letzte in Serie hergestellte Computer, der von einer einzelnen Person, nämlich Steve Wozniak, entworfen wurde.\n\nDer Apple II hatte bei seiner Markteinführung acht freie Steckplätze (engl. Slots) des 8-Bit Apple-Bus-Systems, mit denen er individuell erweitert werden konnte. Die Apple-II-Baureihe war ein offenes System, das heißt, alle wesentlichen Konstruktionsdetails wurden veröffentlicht. Für das Gerät existierten zahlreiche Videospiele und Software für Privatanwender. Gleichzeitig war der Rechner auch für Büroanwendungen und weiteren professionellen Einsatz geeignet, unter anderem wegen seines modularen und erweiterbaren Aufbaus. Er wird daher in der Literatur zum Teil zu den Heimcomputern, aber auch zu den frühen Arbeitsplatzrechnern gezählt.\n\nDer Apple II ist der Nachfolger des Apple I und wurde von Steve Wozniak entwickelt und von Steve Jobs (beide Mitbegründer von Apple) ab April 1977 vermarktet. Bald erschienen diverse illegale Apple-Clones, auch von Heimwerkern selbst gebaute, da in den frühen Apple-II-Modellen nur leicht erhältliche Standard-Chips verwendet wurden. Wie bei anderen Rechnermodellen dieser Zeit wurden kommerzielle Clones hauptsächlich in Ostasien, Brasilien und dem damaligen kommunistischen Ostblock hergestellt, da dort die amerikanischen Originalrechner durch Export- oder Importbarrieren schwer erhältlich oder im Vergleich zum Durchschnittseinkommen zu teuer waren. Zugleich war eine rechtliche Verfolgung der Cloner in diesen Ländern damals schwer bis unmöglich. Legale Apple-Clones zu bauen war sehr aufwändig, da Apple die Firmware nicht lizenzierte (außer ITT 2020), und die Entwicklung einer kompatiblen, aber nicht identischen Firmware im Reinraum-Verfahren im Gegensatz zum späteren IBM-PC um einiges schwieriger war (weil noch keine saubere Einsprungtabelle existierte). Erst Mitte der 1980er Jahre kam mit dem Laser 128 ein weitgehend kompatibler und zugleich legaler Clone auf den Markt.\n\nDie Apple-II-Reihe wurde von 1977 bis 1993 gebaut, also etwa 16 Jahre. In dieser Zeit wurden über zwei Millionen Original Apple-II-Computer hergestellt. Die ungezählten Nachbauten dieser Baureihe sind in dieser Zahl nicht erfasst.\n\nDie Baureihe war vor allem deswegen so erfolgreich, weil der Rechner ein \"offenes System\" war. Bei den frühen Modellen waren sämtliche Schaltungen und Signale sowie die Firmware in für jedermann erhältlichen Publikationen dokumentiert, bei den späteren Modellen gab es zwar einige ASICs, deren genauer Inhalt nicht veröffentlicht wurde, ihre wesentlichen Funktionen waren aber ebenfalls kein Geheimnis. Man konnte für die Slots des Apple II die unterschiedlichsten Karten kaufen oder auch selbst bauen. Es gab Speichererweiterungskarten, diverse Schnittstellenkarten (beispielsweise für Drucker, Modems, Disketten- und Festplattenlaufwerke), Steuerungskarten (zum Beispiel für Fischertechnik-Baukästen, aber auch I/O-Karten für Industrie und Forschung), Grafikkarten, Soundkarten, Echtzeituhren, und sogar Prozessorkarten mit anderen Prozessoren, wie Z80 oder 68000.\n\nDie Software für den Apple war meist innovativer als bei der Konkurrenz. Das Programm Visicalc war die erste Tabellenkalkulation für Mikrocomputer überhaupt. Das Programm AppleWorks war das erste Programm, das eine Textverarbeitung, eine Tabellenkalkulation und eine Datenbank in einem Programm vereinigte.\n\nDer Apple II gehörte (zusammen mit den gleichzeitig vorgestellten Commodore PET 2001 und Tandy TRS 80) zu den ersten drei erfolgreichen Mikrocomputern, die nicht als Bausatz, sondern als fertige Geräte verkauft wurden. Üblich war es zur damaligen Zeit, dass sich der Käufer zumindest selbst um eine passende Tastatur und einen Monitor kümmern musste, teilweise wurden auch nur nackte Platinen ohne Netzteil und Gehäuse als Computer verkauft (so wie das erste Apple-Modell, der Apple I).\n\nIm Apple II kam eine 8-Bit 6502-CPU mit 1,020 MHz Taktfrequenz zum Einsatz. In der Grundausstattung verfügte er über 4 KB RAM, erweiterbar bis 64 KB, in den späteren Modellen auf bis zu 16 MB ausbaubar.\n\nEine Schreibmaschinentastatur, noch fast ohne Sondertasten, war eingebaut. Beim ersten Apple II konnte man damit nur Großbuchstaben eingeben – nach Auskunft des Apple-II-Entwicklers Steve Wozniak war die einzige Tastatur, die er sich zu dieser Zeit leisten konnte, eben eine mit Großbuchstaben.\n\nDer Ur-Apple-II beherrschte auch für die Ausgabe im Textmodus nur den ASCII-Zeichensatz ohne Kleinbuchstaben (64 druckbare Zeichen und 32 Steuerzeichen). Im Textmodus stellte der Apple 24 Zeilen zu je 40 Zeichen dar. Die Zeichen waren 5×7 Punkte groß und in 7×8 Punkte große Zeichenboxen eingeschrieben. Dabei war nur weiße Schrift auf schwarzem Grund möglich, die Farbfähigkeit des Apple kam nur im Grafikmodus zum Tragen. Jedes Zeichen konnte normal, negativ oder blinkend (schnell wechselnd zwischen normal und negativ) dargestellt werden. Der Zeichensatz war – anders als bei einigen anderen Heimcomputern – nicht über Software veränderbar. Kleinbuchstaben, Umlaute und grafische Zeichen waren daher nur über Hardware-Bastellösungen möglich (gepatchte EPROMs, die damals in Deutschland bei Apple-II-Benutzern weit verbreitet waren). Einige Clones, zum Beispiel der Basis 108, besaßen jedoch ab Werk Alternativzeichensätze. Erst der Apple IIe beherrschte Kleinbuchstaben und in seiner deutschen Version auch Umlaute – allerdings nur im Austausch gegen selten benutzte andere Zeichen des ASCII-Zeichensatzes, entsprechend der damals aktuellen Norm ISO 646. Es gab einen Umschalter an der Unterseite des Geräts, mit dem man zwischen ASCII und GSCII („German ASCII“) wählen konnte.\n\nKern des Apple-II-Designs war ein digitaler Grafik- und Zeichengenerator; dieser war so in das System eingearbeitet, dass er zugleich den Refresh des DRAM-Speichers übernahm. Die Grafik hatte einen speziellen Adressraum im Hauptspeicher. Neben dem bei jeder Speichergröße möglichen Textmodus und der groben „LoRes“-Farbgrafik (40×48 in 15 Farben) bot der Apple II ab einem Speicherausbau von 16 KB einen hochauflösenden „HiRes“-Grafikmodus mit 280×192 Pixeln; dabei war der Hintergrund (ungesetzte Pixel) immer schwarz, einzelne gesetzte Pixel erschienen auf einem Farbmonitor, je nachdem ob sie in gerad- oder ungeradzahligen Spalten standen, in zwei unterschiedlichen Farben, zwei oder mehr gesetzte Pixel nebeneinander dagegen immer weiß. Für je sieben Pixel gab es zudem die Möglichkeit, zwischen zwei Farbräumen hin- und herzuschalten. Damit konnte der Apple II hochauflösende Grafik in sechs Farben (schwarz, weiß, grün, violett, orange, türkisblau) darstellen, was 1977 revolutionär war. Zudem hatte das Umschalten in den zweiten Farbraum eine horizontale Verschiebung um die halbe Pixelbreite zur Folge, was bei geschickter Programmierung zur Erweiterung der horizontalen Auflösung genutzt werden konnte.\n\nAls Anzeige diente ein Video-Monitor oder mittels eines zwischengeschalteten HF-Modulators ein Fernseher (heutige Fernseher können über den Video- oder SCART-Eingang direkt angeschlossen werden, damalige Modelle hatten aber fast immer nur eine Antennenbuchse, die ein moduliertes HF-Signal erfordert). Die Farbausgabe funktionierte nur mit dem amerikanischen NTSC-Fernsehsystem, da sie sich dessen spezielle Eigenschaften zunutze machte, um überhaupt mit so wenigen Chips Farbgrafik zu ermöglichen. Ein wesentlicher Faktor für die Farberzeugung war die Verwendung von 14,318 MHz als primärem Takt im Apple II – das ist das Vierfache der NTSC-Farbträgerfrequenz. Das ermöglichte eine einfache Erzeugung der amplitudenmodulierten Farbartsignale.\n\nDa der gleiche Takt (geteilt durch 7 oder 14) auch als Bit-Takt für das Diskettenlaufwerk und für das gesamte Systemtiming diente, konnte er auch in den europäischen Apples nicht einfach durch ein Vielfaches der PAL-Farbträgerfrequenz ersetzt werden, sonst wären Disketten nicht mehr kompatibel zwischen den beiden Modellvarianten gewesen und Programme wären unterschiedlich schnell ausgeführt worden. In den europäischen Apples war daher eine PAL-Farbkarte vonnöten, die das Pseudo-NTSC-Signal der Hauptplatine empfing und nach PAL wandelte. Ab dem europäischen IIe war diese auf der Hauptplatine integriert. Das Pseudo-NTSC-Signal wird dabei von einem anderen Quarzoszillator erzeugt als der PAL-Farbträger. Da Quarzoszillatoren immer minimale Frequenzschwankungen aufweisen, entsteht in diesem Arrangement durch das unabhängige Schwanken der beiden Oszillatoren zwangsläufig eine unregelmäßige Frequenzüberlagerung (Schwebung). Dadurch ist das Farb-Videobild europäischer Apples unruhig; es neigt zu Bildzittern und zu auf- oder abwärts wandernden Moiré-Mustern. Der Textmodus ist von diesem Problem jedoch nicht betroffen.\n\nDas meistverbreitete Speichermedium waren 5¼-Zoll-Disketten. Steve Wozniak hatte die zu der Zeit verfügbaren Diskettenlaufwerke als zu teuer und ineffektiv angesehen und entwickelte eine eigene Steuerelektronik, die, indem sie einen Großteil der Codierungsarbeit und des Timings von der CPU des Computers erledigen ließ, tatsächlich preiswerter war und mehr Daten pro Diskette speichern konnte als Konkurrenzprodukte. Typische Diskettenlaufwerke hatten damals eine Kapazität von 80–90 KB, Wozniak schaffte mit der gleichen Laufwerksmechanik über 110 KB, in einer zweiten Version sogar 140 KB pro Diskettenseite. Das Design des Laufwerkscontrollers war so effektiv, dass Apple es später als Einchiplösung unter der Bezeichnung Integrated Woz Machine in Apple-II-Nachfolgern und auch in den Macintosh-Computern einsetzte.\n\nAls Betriebssystem diente bei Diskettengebrauch zusätzlich zum eingebauten Applesoft BASIC (bzw. vor dem II+ Integer BASIC) meist Apple DOS oder später ProDOS. Daneben gab es eine Vielzahl von schnelleren DOS-Betriebssystemen von Fremdherstellern, z. B. Diversi-DOS. Der Hüthig-Verlag gab eine Unix-Version namens KIX heraus. An Schulen und Universitäten war dagegen die UCSD-Pascal-Variante Apple Pascal als Betriebssystem weit verbreitet; darauf aufbauend gab es auch die Programmiersprache Modula-2. Speichern auf Tonbandkassetten war möglich, aber bei den Apples nur in der allerersten Zeit üblich, bevor 1978 ein Diskettenlaufwerk erhältlich wurde. Eine gewisse Bedeutung blieb der Kassette als Medium für Sicherheitskopien erhalten, da eine preiswerte C90-Kassette den Inhalt mehrerer Disketten speichern konnte und Disketten Anfang der 1980er Jahre sehr teuer waren (ca. 5 DM).\n\nAb 1983 gab es sogar Festplatten mit einer für damalige Verhältnisse enormen Kapazität von 10 und 20 MB. Das Unternehmen Frank & Britting GmbH, Forst (Baden) (nicht mehr existent) bot mit einer Mikrowinchester (3,5 Zoll) mit 10 MB Kapazität die ersten Festplatten des schottischen Herstellers Rodime mit ihren Controllern für den Apple an. Mit durchschnittlich 93 Millisekunden wurde auf die Daten zugegriffen. In vier Partitionen konnten auf der Festplatte DOS, CP/M, UCSD-Pascal und ProDOS gleichzeitig untergebracht werden. Nach dem Starten war für die Auswahl ein Menü zuständig.\n\nDas Unternehmen Compu-Shack (auch nicht mehr existierend) vertrieb diesen Einbausatz, der an die Stelle des Netzteils kam. Der Verkaufspreis für 20 MB betrug zum Verkaufsstart 4560 DM, für die 10 MB-Variante 3990 DM.\n\nDer Apple II verfügte über einen Erweiterungsbus mit acht Steckplätzen, die Steve Wozniak gegen den Willen von Steve Jobs durchgesetzt hatte. Da Wozniak diese Steckplätze wie das gesamte Design offen dokumentiert hatte, entstand eine Fülle von Zusatzkarten, beispielsweise die Erweiterungskarte \"Microsoft Softcard\" mit einem Z80-Prozessor, damit man den Rechner mit dem Betriebssystem Apple CP/M betreiben konnte und somit die dafür vorhandene Standard-Bürosoftware der damaligen Zeit, wie etwa WordStar und dBASE, nutzen konnte.\n\nWeitere beliebte Erweiterungen waren: eine Speichererweiterung um 16 KB namens „Apple II Language Card“, die den eingebauten ROM-Adressraum durch nachladbare Sprachen-Interpreter ersetzen konnte; 80-Zeichen-Karten, die die darstellbare Zeichenzahl pro Zeile verdoppelten (beliebt waren vor allem die „Videx VideoTerm“-Karte und ihre diversen Nachbauten; ab dem IIe war 80-Zeichen-Fähigkeit standardmäßig vorhanden, sobald eine Speichererweiterungskarte eingesteckt wurde), serielle und parallele Schnittstellenkarten für Drucker und externe Modems oder Akustikkoppler. Dazu kamen noch erste, simple Soundkarten wie das Mockingboard, die dem eingebauten Lautsprecher des Apple immerhin deutlich überlegen waren, aber nur von wenigen Programmen unterstützt wurden. Das Mockingboard enthielt u. a. den Soundchip AY-3-8910 von General Instrument, der fast identisch mit dem des Atari ST und der in Japan sehr erfolgreichen MSX-Computer war.\nDarüber hinaus existierten auch Erweiterungskarten, mit deren Hilfe es möglich war, Floppy Disk-Laufwerke oder sogar erste Festplatten an den Apple II anzuschließen. Es gab auch interne Modems.\n\nDas Konzept der Erweiterungskarten war so erfolgreich, dass es IBM später für den ersten PC übernahm. Allerdings stellte das Slotkonzept des IBM-PC einen deutlichen Rückschritt gegenüber dem Apple II dar. Erweiterungskarten für den Apple II konnten ihre Treibersoftware auf dem Board mitbringen und erhielten durch den Steckplatz eine feste (Speicher-)Adresse. Dadurch war es mit vielen Karten möglich, diese einfach in einen freien Steckplatz zu stecken und ohne weitere Installationsarbeiten zu benutzen. Dagegen mussten die Erweiterungskarten beim IBM-PC ihre Adresse mittels DIP-Schaltern eingestellt bekommen, was zu Problemen führte, wenn zwei Karten die gleiche Adresse bekamen. Zudem war es beim IBM-PC notwendig, die Treibersoftware zusätzlich zu installieren.\n\nSchon an den Ur-Apple II konnte man zwei Paddles oder einen analogen Joystick anschließen; ein primitiver Analog-Digital-Wandler war integriert (das Verfahren wurde vom Gameport des IBM PC kopiert und war lange im Einsatz, wurde aber von USB verdrängt). Der Joystick ließ sich auch relativ einfach in eigenen Programmen einbeziehen und zur Eingabe nutzen.\n\nDer Gameport hatte vier Eingänge für jeweils ein Potentiometer mit 150 kΩ, drei Eingänge für Tasten und fünf digitale Ausgänge, davon vier schaltbare, und einer, der nur kurze Pulse ausgeben konnte. Der vorgesehene Anschluss für den Gameport, und mehr noch die dort einzusetzenden Stecker, waren mechanisch nicht besonders stabil. Bei dem Anschluss handelte es sich um einen IC-Sockel im DIL16-Format im Inneren des Rechners; um dort etwas einzustecken, musste der Rechner geöffnet werden, das Kabel wurde dann durch eine Öffnung auf der Rückseite aus dem Gehäuse herausgeführt. Beim Apple IIe wurde dieser Anschluss um eine außen angebrachte 9-polige D-Sub-Buchse ergänzt, auf dieser wurden die digitalen Ausgänge nicht mehr herausgeführt. Die späteren Modelle hatten nur noch die D-Sub-Buchse, der interne Anschluss fiel weg.\n\nInteressant ist, dass die drei Eingänge für Tasten dadurch zustande kamen, dass der für die Tasten und Achsen verwendete Baustein acht Eingänge hatte, von denen einer für die Realisierung des Kassetten-Eingangs verwendet wurde. Die Achsen wurden durch vier Monoflops realisiert, deren Schaltzeit durch den eingestellten Widerstand der Potentiometer bestimmt wurde, somit blieben noch drei Eingänge für Tasten.\n\nDer Apple II verfügte von Haus aus nicht über Zeitgeber (Timer) und Unterbrechungen (Interrupts). Wozniak nannte als Grund dafür, dass er das Design möglichst einfach halten wollte. Außerdem hatte ein interruptgetriebener Tastaturtreiber versagt, so dass er für diesen auf ein „Polling“-Verfahren zurückgriff.\n\nWegen des fehlenden Timer-Interrupts war z. B. die zeitgenaue Wiedergabe von Musik über den eingebauten Lautsprecher recht aufwendig, es musste die Länge jedes einzelnen Maschinencode-Befehls errechnet werden, um den Lautsprecher in der richtigen Frequenz ein- und auszuschalten. Da einige 6502-Befehle je nach Registerinhalten unterschiedliche Zyklenzahlen benötigten, war das nicht trivial.\n\nAuch der Code für den Diskettenlaufwerks-Treiber war zyklengenau ausgestoppt; Wozniak konnte dadurch, dass er zeitkritische Funktionen in Software abwickelte, ein wegweisend einfaches und flexibles Floppy-Disk-Laufwerk bauen. Der Bootloader in einem PROM auf der Diskettenlaufwerk-Einsteckkarte, der autonom ein vollständiges DOS von der Diskette nachladen konnte, war inklusive der GCR-Codetabelle nur 252 Bytes groß.\n\nZusatzkarten wie die erwähnte „Mockingboard“-Soundkarte rüsteten die fehlenden Timer-Interrupts nach, zumeist auf Basis des inzwischen erschienen „Versatile Interface Adapter“-Chips 6522, der seinen Weg auch ins Apple-III-Design fand.\n\nDas Urmodell wurde 1979 zum \"Apple II+\" und \"Apple II europlus\" weiterentwickelt; letzterer war der erste in Europa im großen Stil verkaufte Mikrocomputer. Der II+ bot immer 48 KB Speicher (die kleineren Ausbauvarianten wurden nicht mehr unterstützt) und kam mit anderer Firmware: anders als der Apple II, in dessen ROM sich noch das von Steve Wozniak entwickelte Apple Integer Basic befand, bot er Applesoft BASIC, das von Microsoft stammte. Es war merklich langsamer, verbrauchte mehr Speicher und war weniger klar im Aufbau als Integer BASIC, hatte aber den großen Vorteil, dass es im Gegensatz zu diesem auch mit Gleitkommazahlen rechnen konnte. Eine Apple-eigene Erweiterung von Integer BASIC in diese Richtung war an Zeitmangel gescheitert, weshalb man sich zum Zukauf entschlossen hatte. Applesoft BASIC verfügt über Befehle zur Darstellung hochauflösender Farbgrafik, im Übrigen ist es weitgehend identisch mit dem Commodore BASIC z. B. des Commodore 64, denn auch dieses kam von Microsoft und entstammte der gleichen Codebasis. Der Europlus unterschied sich vom II+ nur in der Anpassung an die europäische Fernsehnorm mit ihren 50 Bildern pro Sekunde statt 60 wie in Amerika. Tastatur und Zeichensatz blieben beim Europlus amerikanisch, allerdings wurden von Bastlern und Wiederverkäufern hier oft Anpassungen vorgenommen.\n\n1980 mutierte der Apple II zum wenig erfolgreichen Apple III (dessen Scheitern am Markt IBM mit ihrem IBM-PC, der wie der Apple II eine offene Architektur besaß, eine große Marktlücke eröffnete), wurde dann 1982 zum IIe und 1984 dem portablen IIc fortentwickelt und letztlich 1986 durch den mit dem 65C816 Prozessor bestückten, teilweise mit 16 Bit arbeitenden II abgelöst, der aber von Apple kaum beworben wurde und neben den Apple-Macintosh-Rechnern, dem Atari ST und dem Commodore Amiga in Europa kaum Beachtung fand. Ein bereits 1984/1985 unter Mitarbeit von Steve Wozniak konzipierter Nachfolger mit den Codenamen \"Apple //x\" bzw. \"Golden Gate\" kam nicht über ein frühes Entwicklungsstadium heraus. Der auf der WD 65816-CPU basierende Rechner sollte u. a. mit einem 68000er-Prozessor erweiterbar sein, eine Kompatibilität zum Apple Macintosh oder eine grafische Benutzeroberfläche waren zu diesem Zeitpunkt jedoch nicht vorgesehen.\n\nDa die Apples in amerikanischen Schulen sehr beliebt und verbreitet waren, bestand die Serie noch einige Jahre fort. Nach zwei ROM-Revisionen wurde der Apple IIgs im November 1992 eingestellt. Eine dritte ROM-Revision mit dem Codenamen \"Mark Twain\" erreichte nur den Prototypen-Status; im Unterschied zum Seriengerät hätte dieser Rechner zusätzlich über eine integrierte Festplatte, ein integriertes 3,5-Zoll-Diskettenlaufwerk und eine SCSI-Schnittstelle verfügt.\n\nNach Produktionsende des Apple IIgs wurde der Apple IIe noch bis 1993 gefertigt. \n\nIn den USA bestimmte der Apple II auch die frühe Mailbox-Szene und es erschienen sehr viele Computerspiele für die Apples; in Westeuropa dominierten dagegen andere, später erschienene Systeme – wie zum Beispiel der Commodore C64 – den Spielemarkt. Mailboxen wurden in Deutschland wegen der höheren Telefongebühren und der extrem restriktiven Modem-Politik der damaligen Bundespost erst später populär. Die vier ersten Computerspiele (ROCKET PILOT, STAR WARS, SAUCER INVASION, SPACE MAZE) für den Apple II wurden von Bob Bishop entwickelt.\n\nEinige bekannte Klassiker erschienen zunächst auf dem Apple II. Viele davon wurden später auf andere gängige Heimcomputer wie den C64 oder den PC portiert, darunter Akalabeth, Ultima, Wizardry, Choplifter, Prince of Persia, Castle Wolfenstein und The Bard’s Tale. Exklusiv erschien insbesondere das erste Grafikadventure Mystery House. The Oregon Trail wurde vom Großrechner auch zuerst auf den Apple II portiert. Andererseits erschienen hier auch viele bekannte Arcade-Spiele, darunter Defender, Frogger, Dig Dug, Battlezone oder auch Spiele anderer Heimcomputer, wie California Games und The Last Ninja.\nIm Jahre 2013, 35 Jahre nachdem der Apple II auf den Markt gekommen war, wurde der dazugehörige Apple DOS-Quelltext vom Computer History Museum auf seiner Webseite veröffentlicht. Paul Laughton, der Programmierer, hatte ihn zur Verfügung gestellt.\n\n\n"}
{"id": "38450", "url": "https://de.wikipedia.org/wiki?curid=38450", "title": "HyperCard", "text": "HyperCard\n\nHyperCard ist eine Software für Hypertext mit eigenem Datenformat für eine alte Macintosh-Plattform.\n\nHyperCard wurde von Bill Atkinson für das Unternehmen Apple entwickelt und im August 1987 an die Öffentlichkeit gebracht. Neue Macintosh wurden dann serienmäßig mit dem Viewer von HyperCard ausgeliefert, der einzeln 50 US-Dollar kostete. Der Editor von HyperCard kostete 400 US-Dollar und wurde das erste für größere Anwenderkreise nutzbare Autorensystem für Hypermedia. Rechnernetze unterstützte HyperCard nicht.\n\nEnde der 1980er und Anfang der 1990er Jahre war HyperCard die bevorzugte Entwicklungsumgebung für Hypermedia-Lernsysteme.\n\nBill Atkinson entwickelte für Apple bereits 1985 eine Notizblock-Anwendung namens QuickFile, in der Texte wie in einer Kartei erfasst und dargestellt werden konnten. Zusätzlich waren jedoch auch noch Querverknüpfungen möglich. Die konsequente Weiterentwicklung dieses Systems führte zwei Jahre später zu HyperCard.\n\nDer Niedergang von HyperCard zeichnete sich im Jahr 1989 ab und war im Jahr 1994 besiegelt, als die Library of Congress von HyperCard auf HTML und das HTTP des World Wide Web umstellte. HyperCard war zum Scheitern verurteilt, weil es in der Kultur von Apple keine angemessene Aufgeschlossenheit für Netzwerktechnik gab.\n\nNach 1990 wurden verschiedene Anläufe unternommen, HyperCard weiterzuentwickeln. Unter Kevin Calhoun wurde HyperCard 3.0 bis zur Betaversion getrieben und 1996 an der jährlichen Worldwide Developers Conference (WWDC) in einer Auflage von 600 Stück verteilt. Neue Funktionen waren unter anderem vollständige Farbunterstützung und Internetkompatibilität.\n\nDie neue Version wurde jedoch nicht veröffentlicht. Calhoun, die treibende Kraft hinter dem Release, verließ 2001 enttäuscht Apple. 2004 wurde der Verkauf von Version 2.4 durch Apple eingestellt. Die jüngste Ausgabe von HyperCard stammt aus dem Jahr 1998.\n\nHyperCard wurde nicht mehr für macOS portiert, lief aber in der Classic-Umgebung.\n\nDie integrierte Skriptsprache \"HyperTalk\" ermöglicht auch Programmieranfängern mit geringem Lernaufwand Anwendungen wie z. B. eine Lernsoftware für rechnerunterstützten Unterricht zu erstellen. Auch das weltweit erfolgreiche Computerspiel Myst wurde mit Hilfe von HyperCard entwickelt.\n\nDas Einsatzspektrum von HyperCard reicht von einfach vernetzten Strukturen (Karteikarten mit Text) bis hin zu komplexen Datenbanken. Basisdokument ist jeweils der „Stapel“ (englisch \"Stack\").\n\nFolgende Nachfolgeprodukte zu HyperCard, die teilweise sogar noch alte HyperCard-Stapel einlesen können, und die HyperTalk-Skripte mit geringen oder keinen Änderungen ausführen können, gibt es:\n\n\nWeitere Produkte basieren auf Strukturellen Ideen von HyperCard:\n\n"}
{"id": "38874", "url": "https://de.wikipedia.org/wiki?curid=38874", "title": "PaintShop Pro", "text": "PaintShop Pro\n\nPaintShop Pro (auch Paint Shop Pro oder häufig abgekürzt PSP) ist ein Bildbearbeitungsprogramm des kanadischen Softwareherstellers Corel für Windows-Betriebssysteme. Es unterstützt Raster- und Vektorgrafik. Ursprünglich wurde Paint Shop Pro ab 1992 bei Jasc Software entwickelt. Corel übernahm das Unternehmen im Oktober 2004 und veröffentlicht seither jedes Jahr eine neue Version.\n\nIn den ersten Jahren stand das Programm unter einer Evaluierungs-Lizenz zur freien Verfügung und gewann in kurzer Zeit eine große Benutzergemeinde. Aus deren Anregung und konstruktiver Kritik entwickelte sich das zunächst nur als Bildbetrachter und Grafik-Konverter geplante Anwendungsprogramm zum Allroundwerkzeug für den (semi-)professionellen Bereich.\n\nDie erste Version der Software entwickelte Robert Voit im Jahr 1992, nachdem er im Vorjahr das Unternehmen \"Jasc Software\" gegründet hatte. JASC war ein Akronym, dem nachgesagt wird, eigentlich für „Just Another Software Company“ (zu Deutsch etwa: \"einfach ein weiteres Softwareunternehmen\") gestanden zu haben. Mit zunehmendem Erfolg von Paint Shop Pro und damit auch wachsender Bekanntheit des Unternehmens erklärte Robert Voit „Jets and Software Company“ (zu Deutsch: \"Flugzeuge und Softwareunternehmen\") zur offiziellen Bedeutung. Die Erwähnung von Flugzeugen bezieht sich wohl auf seine frühere berufliche Erfahrung. Vor der Firmengründung arbeitete er als Pilot bei Northwest Airlines.\n\nIm Herbst 2004 wurde das Unternehmen durch die Corel Corporation übernommen, die damit ihre Produktpalette um CorelDraw abrunden wollte. Im September 2005 brachte Corel mit \"Corel Paint Shop Pro X\" (als römische Zahl für Version 10) die erste eigene Paint-Shop-Pro-Version auf den Markt. Version 11 (XI) folgte im September 2006. Mit dieser Veröffentlichung ging eine Umbenennung in \"Corel Paint Shop Pro Photo\" einher. Im September 2007 wurde die Version 12 unter dem Namen \"Corel Paint Shop Pro Photo X2\" veröffentlicht, die unter anderem die Möglichkeit beinhaltete, das Aussehen des Programms mit Skins anzupassen.\n\nEin Jahr später, im September 2008, erschien eine erweiterte Version von Version 12 unter dem Namen Corel Paint Shop Pro Photo X2 Ultimate. Von der Basisversion der Software unterschied sie sich mit der Unterstützung von Rohdaten (RAW) und erweiterten Tools zur Bildoptimierung und Verfremdung. Zudem wurde der Background Remover verbessert, der das Freistellen von Objekten noch einfacher gestaltet. Neu hinzu zum Paket kam neben Corel Painter Photo Essentials 4 eine Software zur Datenrettung gelöschter Fotos.\n\nIm Januar 2010 brachte Corel die Version Paint Shop Photo Pro X3 auf den Markt. Neu hinzugekommen sind unter anderem ein Kamera-RAW-Editor, Automatisierung von Bearbeitungen zur Anwendung weiterer Bilder und neue Benutzeroberfläche des Fotoverwalters. Mit der Version X4, die im September 2011 veröffentlicht wurde, wurde unter anderem ein HDR-Modul zum Zusammenfügen unterschiedlich belichteter Aufnahmen eines Motivs und eine Funktion zur nachträglichen Vignettierung eines Fotos hinzugefügt. Und erstmals seit Übernahme durch Corel sind neue Anpassungsebenen implementiert worden. Die Software ist darüber hinaus als sogenannte Ultimate-Version erschienen, die zusätzlich lizenzfreie Bilder, einige digitale Filter des Herstellers Nik und das Kompressionsprogramm WinZip Pro umfasst. Im September 2012 erschien Version X5 mit Neuerungen wie z. B. Gesichtserkennung, Geotagging, Pseudo-HDR-Effekt aus den einzelnen Raw-Fotos und neue sofortige Effekte.\n\nIm September 2013 erschien Version X6, die erstmals nativ 64-Bit-Prozessorarchitekturen unterstützt. Nebenbei wurde die Benutzeroberfläche überarbeitet und die Geschwindigkeit verschiedener Funktionen verbessert, vor allem das Zusammenfügen von HDR-Bildern.\n\nDie Entwicklung des Programms von einem einsteigerfreundlichen Hilfswerkzeug zu einem Photoshop-ähnlichen professionellen Grafikpaket, wurde zwiespältig aufgenommen. Während viele Anwender in Paint Shop Pro eine auch für den privaten Einsatz kostengünstige Alternative zu den vergleichsweise teuren Konkurrenzprodukten sehen, werfen Kritiker Corel den Versuch vor, diese Produkte zu kopieren.\n\nSeit Mai 2008 erscheint ein (englischsprachiges) offizielles Magazin zu Paint Shop Pro Photo.\n\n"}
{"id": "41212", "url": "https://de.wikipedia.org/wiki?curid=41212", "title": "Notebook", "text": "Notebook\n\nEin Notebook [] ( ‚Notizbuch‘) oder Laptop [] (vom englischen \"‚‘\" wörtlich für „auf dem Schoß“, übertragen „Schoßrechner“), selten auch \"Klapprechner\", ist eine spezielle Bauform eines Personal Computers, die zu den Mobilgeräten zählt. Sie besitzt folgende grundlegende Eigenschaften:\n\nVon der Größe und Leistungsfähigkeit her liegen Notebooks nach heutigen Maßstäben zwischen den größeren Desktop-Computern und den kleineren Tablets. In den letzten Jahren sind Notebooks von der Leistung nahe an die klassischen Desktop-Computer herangekommen und haben ihnen Marktanteile abgenommen.\n\nEnde der 1980er-Jahre führte Toshiba die Bezeichnung \"Notebook\" ein, um besonders kompakte und leichte (wie ein Notizbuch) Geräte besser vermarkten zu können. Inzwischen werden die Bezeichnungen \"Notebook\" und \"Laptop\" im deutschen Sprachraum weitgehend synonym verwendet, wobei die Bezeichnung \"Notebook\" tendenziell für die mittelkleinen Ausführungen benutzt wird. \"Laptop\" (\"der\", auch \"das\") bezieht sich darauf, dass der mobile Computer auf dem Schoß Platz findet.\n\nIm englischen Sprachraum ist die Bezeichnung \"Laptop\" gängig; \"Notebook\" und auch \"Notebook Computer\" werden ebenfalls verwendet.\n\nAnfang der 2000er Jahre tauchte im deutschen Sprachraum auch der Begriff Klapprechner auf und wird auch in sprachwissenschaftlichen Texten als Beispiel verwendet. Sprachpflegerische Bemühungen, mit dieser Bezeichnung die Anglizismen „Notebook“ und „Laptop“ zu verdrängen, können mittlerweile als erfolglos gelten. Trotzdem vergab der Verein Deutsche Sprache e. V. seinen Negativpreis „Sprachpanscher des Jahres 2013“ an „den Duden“ mit ausdrücklichem Verweis in der Begründung darauf, dass das Wort „Klapprechner“ in dessen aktueller Ausgabe nicht aufgeführt sei.\n\nDer Begriff \"Netbook\" wurde seit 2007 für ein deutlich kleineres Gerät ohne optisches Laufwerk verwendet, dessen Tasten auch zu klein für die Verwendung im Zehnfingersystem sein können. Seit ungefähr 2011 wird der Begriff kaum noch verwendet, Tablets oder Convertibles lösten die Netbooks ab.\n\nDer Begriff \"Portable\" bezeichnet üblicherweise ein Gerät mit der Technik und den Ausmaßen eines Desktop-Computers, dessen zumeist kofferförmiges Gehäuse jedoch zum regelmäßigen Transport ausgelegt ist und in das ein Bildschirm zumeist unbeweglich integriert ist (speziell bei älteren Geräten mit Bildröhre). Im Gegensatz zu Notebooks sind solche Geräte in der Regel auf externe Stromversorgung angewiesen und haben nicht notwendig eine fest mit dem Gerät verbundene Tastatur. Aufgrund ihrer Größe können zumeist für Desktop-Computer bestimmte Erweiterungskarten der zum Produktionszeitpunkt gängigen Formate eingebaut werden. Computer dieser Art werden heute (2013) nur noch für Spezialanwendungen (z. B. militärisch) verwendet, wo Notebooks nicht eingesetzt werden können.\n\nDer Begriff \"Mobilrechner\" bezeichnet allgemein einen zum Einsatz an unterschiedlichen Standorten bestimmten tragbaren Rechner.\n\nIn die deutsche Umgangssprache fand zeitweise die verballhornende Bezeichnung \"Schlepptop\" Eingang. Diese entstand möglicherweise in der Anfangszeit der mobilen Portable-Computer, die kofferähnlich aussahen und etwa zehn Kilogramm wogen, wie 1981 der Osborne 1, 1985 der Portable 8810/25 der Nixdorf Computer AG (etwa 8000 DM teuer) oder der Kaypro II.\n\nDas Dynabook ist ein 1972 von Alan Kay am Xerox PARC entworfenes Konzept. Es zeigt ein flaches rechteckiges Gehäuse, in dessen Oberfläche sowohl Bildschirm als auch Tastatur in der gleichen Ebene integriert sind. Somit sind in ihm die Grundideen sowohl des Laptops (nur ohne die Klappbarkeit des Bildschirms) als auch des Tabletcomputers formuliert. Wegen der seinerzeit fehlenden technischen Möglichkeiten wurde das Konzept nicht umgesetzt.\n\nEiner der ersten als Laptop zu bezeichnenden Computer ist der GRiD Compass 1100, der vom britischen Industriedesigner Bill Moggridge 1979 entworfen, aber erst 1982 erstmals verkauft wurde. Dieser Laptop verfügte zwar über seinerzeit beachtliche 340 kB Hauptspeicher, hatte aber aufgrund der fehlenden IBM-Kompatibilität keinen kommerziellen Erfolg.\n\n1986, ein Jahr nach dem Toshiba T1100, erschien mit dem IBM PC Convertible das erste auch kommerziell erfolgreiche Notebook mit einer Taktfrequenz von 4,77 MHz, zwei 3,5-Zoll-Floppy-Laufwerken, 256 kB Speicher, einem LC-Bildschirm und Druckeranschlüssen. Noch im Jahr 1987 kostete der Toshiba \"T1100\" mit zwei 3,5-Zoll-Floppy-Laufwerken und LCD-Bildschirm in der Schweiz um die 3700 Franken, der \"T3100\" mit Plasmabildschirm, 640 kB Speicher und einer Harddisk von 10 Megabyte das Doppelte.\n\nDas erste Notebook mit heutigen Bedienelementen war das PowerBook 100 von Apple. Der Benutzer konnte erstmals ohne externe Eingabegeräte auskommen, da das PowerBook über einen Trackball sowie seitliche Handauflagen verfügte. Die Begriffe Notebook und Laptop sind mittlerweile ineinander übergegangen und werden gleichermaßen verwendet. Der erste Laptop, der auch offiziell als erster so bezeichnet wurde, war der Gavilan SC, der 1983 veröffentlicht wurde und stark einer Schreibmaschine ähnelte.\n\nFrüher hatte fast jedes Notebook ein Diskettenlaufwerk. Diese werden wegen der geringen Speicherkapazität von ca. 1,4 MB und dem Aufkommen von USB-Sticks jedoch heute nicht mehr verbaut.\n\nNotebooks wiegen in der Regel zwischen 700 g (Subnotebook) und 8 kg (Desknote) und sind nicht nur – wie der Name vielleicht vermuten ließe – als elektronisches Notizbuch, sondern als vollwertiger kompakter Arbeitsplatz-Rechner zu gebrauchen.\n\nDer größte Vorteil eines Notebooks im Vergleich zu den Desktop-PCs liegt in der Portabilität. Diese ermöglicht die Nutzung des Notebooks an den verschiedensten Orten – nicht nur daheim und im Büro, sondern auch bei Zugfahrten und Flügen. Ferner können Notebooks beispielsweise in Bibliotheken oder in den Geschäftsräumen von Kunden verwendet werden.\n\nDie Leistung typischer Notebooks ist den Desktop-PCs der gleichen Generation aus mehreren Gründen unterlegen. Bei Prozessoren hängt die nutzbare Rechenleistung stark von der Anzahl der Kerne (siehe auch Mehrkernprozessor) und deren Takt ab. Grafikchips profitieren von einer höheren Anzahl an Shader-Einheiten und eigenem dediziertem Grafikspeicher. All diese Faktoren bedingen eine hohe Energieaufnahme sowie die daraus resultierende hohe Wärmeabgabe. Während diese bei Desktop-PCs kein Problem darstellt, werden Notebooks dadurch limitiert. Sie bieten nur begrenzten Platz für Komponenten und Kühlsystem; außerdem ist deren Gewicht eine kritische Größe. Folglich werden – je nach Preiskategorie – meist langsamer getaktete oder Low-Voltage-Prozessoren verbaut und im Leistungsumfang beschnittene Grafikchips oder Onboardlösungen eingesetzt. Bei optischen Laufwerken und Festplatten bedingt die kleinere Bauform und die nötige Toleranz gegenüber Erschütterungen eine niedrigere Drehzahl, was niedrigere Transferraten bedeutet. Solid-State-Drives heben diesen Performance-Malus bei Festplatten jedoch auf.\n\nSeit den späten 2000er Jahren werden vermehrt auch großformatige Notebooks mit über 17 Zoll Bilddiagonale angeboten. Je nach Ausrichtung sind dabei mehrere Festplatten, zwei Grafikkarten und leistungsstarke Desktop-Prozessoren möglich. Unter dieser Ausstattung leidet allerdings die Akkulaufzeit, die im Leerlauf (idle) oft unter zwei Stunden liegt und sich unter Last noch einmal halbiert. Ebenso sind die Geräte sehr schwer – teils über 5 Kilogramm – und daher eher als Ersatz für einen Desktop-PC zu sehen.\n\nAuch Standard-Laptops sind heute durchaus leistungsstark genug, um bei alltäglichen Aufgaben einen Desktop zu ersetzen. Das macht sich auch in den Verkaufszahlen bemerkbar – inzwischen werden in Europa wesentlich mehr Notebooks als Desktop-PCs verkauft. Sie machten 2008 bereits 55 % aller verkauften PCs (bei Privatkunden sogar 70 %) aus. Bei Aufgaben wie CAD, 3D-Spielen und dem Rendering, die hohe Anforderungen an die Hardware stellen, sind Desktoprechner aber weiterhin die bessere Wahl.\n\nEin Nachteil gegenüber einem Desktop-PC ist die erschwerte Austauschbarkeit und Auswahl einzelner Systemkomponenten. Während der Käufer eines Desktop-PCs auf eine Vielzahl Austauschkomponenten zurückgreifen kann, ist er bei einem Notebook – je nach Hersteller mehr oder weniger – auf die Ersatzteile des Herstellers angewiesen. Probleme ergeben sich beim Austausch weniger durch die technischen Spezifikationen der einzelnen Komponenten, als vielmehr dadurch, dass nur Komponenten bestimmter Abmessungen und Anschlussgestalt im engen Notebookgehäuse Platz haben. Aus dieser Herstellerabhängigkeit ergeben sich im Reparaturfall meistens deutlich höhere Kosten und längere Reparaturzeiten als bei vergleichbaren Defekten an einem Desktop-PC.\n\nDer Hersteller Elitegroup hat 2002 eine neue Geräteklasse eingeführt, die sogenannten Desknotes (auch Desktop-Replacement-Computer oder DTR). Der Begriff ist eine Zusammenziehung aus \"Desktop\" und \"Notebook\". Diese Geräte werden als Desktop-Ersatz verkauft und sind für diesen Zweck speziell angepasst, indem auf stromsparende Technik für den Mobileinsatz und hochkapazitive Akkus verzichtet wird und stattdessen leistungsfähigere Komponenten aus dem Desktop-Bereich in das meist etwas größere Gehäuse eingebaut werden. Auch andere Hersteller wie Chaintech hatten solche Geräte im Angebot. Die resultierenden Produkte können zwar eingeschränkt mobil eingesetzt werden, man muss sich aber darüber im Klaren sein, dass sie dafür nicht gebaut wurden, da die Akkulaufzeit extrem kurz ist und viele im Akkubetrieb nur mit gesenktem Prozessortakt laufen. Bei Desknotes mit einer Bildschirmdiagonale größer 18,4 Zoll wird meist auf einen internen Akku verzichtet. Notebooks mit 17- und 18,4-Zoll-Formfaktor integrieren meist einen Lithium-Ionen-Akkumulator, die Laufzeit auf Batteriestrom ist gering und für ein DTR-Notebook zweitrangig. Selbst DTRs mit starken Akkus halten kaum länger als 2 Stunden durch; unabhängig ist man mit einem Desknote von einer Stromquelle (Steckdose) daher nicht. Desknotes haben oft eine höhere Leistungsfähigkeit als Notebooks mit kleinerem Formfaktor (< 17 Zoll), jedoch meist eine geringere Leistung als Desktop-Computer. Aufgrund des großvolumigen - jedoch auch klobigen und schweren - Gehäuses ist die Kühlung ebenfalls leichter zu bewerkstelligen als bei einem Notebook. Desknotes verfügen über eine erweiterte Notebooktastatur mit separatem Ziffernblock (Num-Block).\n\nEin Arbeitsplatzwechsel ist wesentlich einfacher als mit einem Standard-PC, und sie benötigen weniger Platz auf dem Schreibtisch. Es gibt vereinzelt sogar reine Desknotes, die ganz ohne Akkus auskommen und nur für den stationären Betrieb gedacht sind. Inzwischen sind diese Geräte weitgehend vom Markt verschwunden – einerseits, weil die Leistungsfähigkeit der Desktop-Prozessoren keinen so deutlichen Vorteil vor den Mobilkomponenten verspricht, denn gerade im Niedrigpreissegment sind die Anforderungen an die CPU-Geschwindigkeit gering, und andererseits, weil durch die stark steigenden Stückzahlen die Notebooks mit echter Mobiltechnik inzwischen billiger sind als die Desknote-Rechner. In der Frühzeit der Mikrocomputer gab es Kofferrechner, die zwar zum einfachen Transportieren gedacht waren, aber nicht zum Betrieb fern einer Steckdose.\n\nDesknotes stellen eine Hybridform aus Notebooks und Desktop-Systemen dar; ein Kompromiss aus den Vorteilen beider. Sie eignen sich für Anwender, die einen leistungsstarken Rechner benötigen und hin und wieder ihren Standort wechseln.\n\nFür die Anwendung in besonders rauer Umgebung oder unter klimatisch widrigen Bedingungen wurden die sogenannten \"Ruggedized\"-Notebooks konzipiert, die so robust ausgeführt sind, dass sie für Freiluft- und Freilandeinsatz geeignet sind. Es gibt \"Fully-ruggedized\"-Geräte, die vollständig gegen äußere Einflüsse geschützt sind, und \"Semi-ruggedized\"-Notebooks, die nur teilweise bestimmten Widrigkeiten widerstehen, so etwa Spritzwasser auf die Tastatur (Schutzart IP64) oder Stürzen aus einigen Dezimetern Höhe.\n\nDiese Geräte sind mit speziellen gehärteten Gehäusen versehen, die schlagfest sind und Spritzwasser sowie Hitze standhalten sollen. Anschlussstellen sind durch Gummi geschützt, um das Eindringen von Feuchtigkeit zu verhindern. Die Festplatte ist gel- oder gummigelagert, um sie vor Stößen zu schützen – insbesondere einen Sturz auf den Boden – und oft auch noch extra ummantelt, um das Eindringen von Feuchtigkeit zu verhindern, so dass im Ernstfall (Defekt des Computers) zumindest die Daten gerettet sind. Die Widerstandsfähigkeit dieser Geräte wird durch die DIN/VDE IP und den MIL-STD des US-Militärs angegeben. Sie kommen oft dann zum Einsatz, wenn sie außerhalb von Büros arbeiten müssen, beispielsweise bei der Polizei, beim Militär, bei Pannendiensten, Landvermessern oder ähnlichen Außendienstlern und in industrieller Umgebung, meistens zur Datenerfassung. Man kann sie damit auch als mobile Variante von Industrie-PCs ansehen. Aufgrund ihres vergleichsweise hohen Preises und der teilweise eingeschränkten Funktionalität (z. B. bei Schnittstellen oder Grafikleistung) wegen der speziellen Anpassung sind diese Notebooks für Privatanwender weniger interessant.\n\nEine Abgrenzung ist nicht immer klar möglich.\nAußerdem werden Notebooks nach der Bildschirmdiagonale klassifiziert, da die Größe des Geräts heute hauptsächlich von der Bildschirmdiagonale abhängt. Gängige Notebooks haben heute (2013) eine Bildschirmdiagonale von ca. 13 bis ca. 17 Zoll, d. h. über 33 bis etwa 44 cm. Netbooks haben meist ca. 7 Zoll bis 11,6 Zoll große Bildschirmdiagonalen.\n\nAls Convertible bezeichnet man einen Laptop der sich durch einen Klapp- oder Klickmechanismus auch als Tablet nutzen lässt. Die Eingabe kann also ganz normal über die Tastatur erfolgen oder auch, nach der Umwandlung, über einen Touchscreen.\n\nUltrabook ist ein eingetragenes Warenzeichen von Intel für besonders dünne und leichte Notebooks mit Intel-Prozessoren. Um den Namen tragen zu dürfen, müssen die Geräte eine Reihe von Anforderungen erfüllen. Dazu gehören eine hohe Akkulaufzeit, eine akzeptable Leistung und Tabletcomputer-ähnliche Eigenschaften wie schnelles Aufwachen aus dem Standby.\n\nDie Komponenten eines tragbaren Computers sind für den mobilen Einsatz optimiert.\n\nDer Einbau eines speziellen Notebookprozessors (Intel: Intel Core i, Intel Core Duo, Intel Core 2 Duo, Pentium Dual-Core, Pentium M, Celeron M, Atom; AMD: Athlon XP-M, Sempron, Turion 64, Turion 64 X2; Transmeta Efficeon; IBM/Motorola G4; VIA C7-M) reduziert die Leistungsaufnahme und verlängert so die Akkulaufzeit gegenüber kostengünstigeren Desktop-Prozessoren. In manchen Mobilrechnern werden jedoch aus Kosten- oder Leistungsgründen auch normale Desktop-Prozessoren verwendet.\n\nÜblicherweise werden in Notebooks heute TFT-Flachbildschirme in Größen zwischen 10,4 und 20 Zoll und in Bildauflösungen zwischen XGA (1024 × 768 Punkte) und WUXGA (1920 × 1200 Punkte) verbaut, inzwischen praktisch ausschließlich in den 16:10 und 16:9. Netbooks schließen jeweils am unteren Ende der Skala an und sind bis hinunter zu etwa 7 Zoll bei meist 1024 × 600 Pixeln verfügbar. Fast alle Bildschirme sind durch die Verwendung von günstigen TN-Panels auch Standardmonitoren für PCs hinsichtlich der maximalen Helligkeit, der darstellbaren Farben (6 Bit statt 8 pro Farbkanal), des Kontrasts und der Blickwinkel-abhängigen Farbstabilität unterlegen. Hochwertige Paneltypen (IPS, MVA/PVA) werden nur vereinzelt angeboten. Häufig werden Bildschirme mit spiegelnder Oberfläche verwendet, welche für den mobilen Einsatz allerdings nur schlecht geeignet sind. Vereinzelt gibt es jedoch noch oder wieder Geräte mit entspiegelten Bildschirmen, welche auch bei Tageslicht nutzbar sind. Transreflektiv ausgeführte Bildschirme sind nur in Nischenanwendungen zu finden. Üblicherweise ist es möglich, über VGA, DVI, HDMI oder (Mini-)DisplayPort einen externen Bildschirm anzuschließen und diesen zusätzlich oder anstatt des eingebauten Displays zu verwenden.\n\nDie derzeit am weitesten verbreiteten Zeigegeräte in Notebooks sind Touchpads. Eher selten (hauptsächlich bei höherwertigen Business-Geräten) und herstellergebunden werden auch Trackpoints angeboten, welche erstmals von IBM bei Geräten der ThinkPad-Serie (Heute von Lenovo hergestellt) verbaut wurden, wo auch bis heute ein roter Trackpoint verbaut ist.\n\nManche Notebooks sind mit berührungsempfindlichen Bildschirmen – sogenannten Touchscreens – ausgestattet, etwa um eine gute Bedienbarkeit mit Stiften oder Fingern zu ermöglichen. Bei Geräten, die auch als Tablet PCs verwendet werden können, sind in den Bildschirm integrierte Digitizer üblich. Bevor sich Touchpads und Trackpoints durchsetzten, waren Trackballs als Zeigegeräte verbreitet.\n\nDie in Laptops eingebauten Tastaturen lassen meist den üblichen Ziffernblock von Desktop-Tastaturen und manchmal auch die Pos1-Taste und Ende-Taste missen. Ein dedizierter Ziffernblock fehlt bei kompakten Geräten praktisch immer, ab einer Bildschirmdiagonale von 15 Zoll und größer ist genügend Platz dafür. Bei kleineren Geräten ist über die FN-Taste ein Teil der Tastatur als Ziffernblock umbelegbar, der allerdings dann nicht zusammen mit der Buchstabentastatur verwendet werden kann, da diese dann (zumindest teilweise) ausgeblendet ist. In vielen Subnotebooks kommen Tastaturen mit engerem Tastenlayout als den üblichen 19 mm × 19 mm zum Einsatz, was unter Umständen eine gewisse Eingewöhnungszeit benötigt. Laptoptasten lassen sich leichter drücken und sind um einiges leiser als die Tasten normaler Desktop-Tastaturen.\n\nEine weitere Notebook-Spezialität sind die manchmal noch vorhandenen PCMCIA-Steckplätze (auch PC-Card oder CardBus genannt) zum Einschub von Erweiterungskarten, die mittlerweile durch die modernere Variante ExpressCard ersetzt wird oder ganz weggefallen ist. Die Zahl der Schnittstellen ist gegenüber Desktop-PCs meist reduziert, so fehlen ältere Schnittstellen wie Parallel- oder RS232-Port und es stehen weniger USB-Buchsen zur Verfügung. Der FireWire-Anschluss ist, falls vorhanden, bei den meisten Notebooks nur vier- statt sechspolig ausgeführt, d. h. bietet keine Stromversorgung der angeschlossenen Geräte. Auch sind die Audio-Schnittstellen oft eingeschränkt.\n\nMit Stand Ende 2013 bieten die meisten aktuellen Notebook-Modelle nur zwei Speicher-Steckplätze anstelle der in Desktop-PCs üblichen zwei bis sechs. Gerade bei sehr preisgünstigen Geräten kann es zudem vorkommen, dass ein Speichermodul fest verlötet ist und somit nicht ohne Werkstatteingriff austauschbar ist. Notebooks nutzen zudem in der Regel kleinere Module (SO-DIMM) als Desktop-PCs, was die Bestückung zusätzlich einschränkt. Gerade hochkapazitive Module sind auf die Verwendung der neuesten Chipgeneration angewiesen und daher vergleichsweise teuer. Registered- und ECC-Speicher ist in der Regel nicht verfügbar.\n\nAuch die Festplatte ist auf üblicherweise 2,5 Zoll miniaturisiert; Subnotebooks sind eher mit 1,8 Zoll ausgestattet. Die 2,5-Zoll-Platten arbeiten etwa ein Drittel langsamer als Desktop-Platten (3,5-Zoll-Bauform) und bieten, je nach Baujahr und Bauhöhe, etwa 160 bis (Stand Anfang 2017) 2000 GB Speicherplatz. In 3,5-Zoll-Bauform sind dagegen (Stand Anfang 2017) bis zu 10.000 GB verfügbar. Die 1,8-Zoll-Platten sind demgegenüber nochmals deutlich eingeschränkt und werden derzeit besonders stark durch SSDs verdrängt, welche keinerlei Nachteile durch die Bauform haben. Für die früher verbauten ATA-Festplatten wurde eine eigene platzsparende Anschlussnorm (ATAPI-44) geschaffen, die neben Daten- und Steuerbus auch die Stromversorgung in die Steckerleiste integriert. Für den Betrieb einer Notebook-Festplatte an einem Desktop-PC (z. B. zur Datenrettung) ist ein entsprechender Adapter erforderlich. Die heutigen (Stand 2017) SATA-Festplatten im Notebookformat sind dagegen völlig pinkompatibel zu 3,5″-Laufwerken und benötigen für einen Einsatz in Desktoprechnern lediglich noch einen Einbaurahmen. Notebookplatten nutzen ausschließlich 5-V-Versorgungsspannung, was sie von Desktoplaufwerken unterscheidet. Die meisten Notebooks haben nur eine einzige Festplatte, wenige Geräte haben mehrere.\n\nIn der Regel haben Notebooks auch ein CD-, DVD- oder Blu-ray-Laufwerk, welches meist dem Slimline-Standard genügt. Die Brennmöglichkeit von CD und DVD ist Standard (sofern ein Laufwerk vorhanden), während Blu-ray-Brenner noch eher selten und teuer sind. Die Bauhöhe der Laufwerke beträgt üblicherweise 12,7 mm, obwohl auch 9,5 mm über den Superslimline-Standard möglich sind. Die kompaktere Bauform geht jedoch mit höheren Anforderungen an Elektronik und Mechanik einher, weswegen meist der größere Formfaktor gewählt wird. In den Spezifikationen zur SATA-6-Gb/s-Norm sind außerdem neue Konnektoren für eine nur 7 mm hohe Laufwerksgeneration genannt. Allen optischen Notebooklaufwerken ist gemein, dass sie zum Einbau in einen herstellerspezifischen Rahmen gesetzt werden müssen und zumeist auch eine dem Notebookdesign entsprechende Frontblende erhalten.\n\nFür den stationären Betrieb verfügen alle Notebooks über eine eingebaute oder externe Stromversorgung zum Betrieb des Gerätes und zum Laden des Akkus für den mobilen Betrieb. Im mobilen Einsatz verwenden fast alle modernen Notebooks Lithium-Ionen-Akkus zur Stromversorgung. Noch vor wenigen Jahren waren Akkus mit Nickel-Metall-Hydrid-Technik verbreitet, davor Akkus mit Nickel-Cadmium-Technik. Künftig könnten Lithium-Polymer-Zellen zum Standard werden, sind zurzeit aber noch eher selten zu finden.\nÜbliche Laufzeiten von Notebooks im Akkubetrieb liegen je nach Energieverbrauch und Akku-Kapazität bzw. Zellenanzahl zwischen einer halben und acht Stunden. Während in den Anfängen der mobilen Computer diese Akkus fest eingebaut waren, sind sie heute – von wenigen Ausnahmen wie bei den Apple MacBooks abgesehen – ohne Werkzeug austauschbar. Einige Modelle können optional mit einem zweiten Akku bestückt werden, um die Laufzeit zu erhöhen. Der Zusatzakku wird oft statt des optischen Laufwerks in dessen Schacht eingesetzt (drei Zellen), bei manchen Modellen kann er aber auch unterhalb des Hauptakkus angebracht werden und daher größer ausfallen (drei bis zwölf Zellen). Geladen werden die Akkus aus dem Niederspannungsnetz, dem Zigarettenanzünder von Kraftfahrzeugen oder der vergleichbaren sogenannten EmPower-Steckdose in Verkehrsflugzeugen (meist als Adapter auf den Zigarettenanzünder-Stecker).\n\n\nViele Prozessoren in Notebooks haben eine Abschaltfunktion, wenn die Temperatur im Gerät auf einen höheren Wert als vorgesehen ansteigt. Deshalb ist das einwandfreie Funktionieren der Kühlung über den Lüfter Voraussetzung für den laufenden Betrieb. Eine Heatpipe aus Kupfer im Notebook, welche die Wärme der Leistungs-Bauelemente aufnimmt, führt in dem gezeigten Beispiel rechts im Bild die Wärme an einen Kühlkörper. Durch Schlitze im Kühlkörper kann mit dem Lüfter Luft zum Abführen der Wärme aus dem Gehäuse geblasen werden. Verstopfen die Schlitze im Kühlkörper durch Staub und Schmutz, dann wird die Kühlung verschlechtert. Im Grenzfall bei einer totalen Verstopfung kann gar nicht mehr gekühlt werden, die Temperatur im Gerät steigt an, der Rechner schaltet sich nach einigen Minuten Betrieb ab. Ist der Kühlkörper von außen sichtbar, dann kann mit einem zugeschnittenen Stückchen dickeren Papiers die Lüftung Schlitz für Schlitz wieder gangbar gemacht werden. Ansonsten hilft nur eine Reinigung des Kühlkörpers durch Öffnen des Gerätes.\n\nAm Mobilrechnermarkt gibt es viele Marken, aber nur relativ wenige Hersteller, die für diese produzieren. Das liegt daran, dass namhafte Firmen (etwa Fujitsu Technology Solutions) bei Auftragsherstellern (Original Design Manufacturer (ODM)) die Notebooks einkaufen oder auch leicht modifiziert „persönlich“ produzieren lassen, um sie dann unter eigenem Namen zu verkaufen. Wie groß der eigene Anteil an den Geräteentwicklungen ist, lässt sich meist nur schwer abschätzen. Viele Notebookfertiger haben ihren Sitz und die Entwicklung in Taiwan, produziert zum großen Teil in der Volksrepublik China. Im Jahr 2011 betrug der Umsatz mit Notebooks in Deutschland 3,837 Milliarden Euro.\n\n„Echte“ Notebook-Hersteller sind unter anderem (in der Reihenfolge ihrer Produktionszahlen, soweit bekannt – die Zahlen sind mit Vorsicht zu genießen und eignen sich nur, um die Größenordnung abzuschätzen):\n\n\nViele namhafte Notebook-Anbieter geben bei diesen Produzenten Laptops aus hauptsächlich eigenem Design in Auftrag (Reihenfolge nach Verkaufszahlen im Jahre 2004, ebenfalls nicht ganz exakt):\n\nNur sehr wenige Notebook-Anbieter fertigen hauptsächlich selbst.\n\nDie Zusammenhänge der Produzenten, namhaften Hersteller und Eigenmarken-Anbieter ändern sich ständig. So versuchen Produzenten wie AsusTek, MSI oder Twinhead verstärkt, als Eigenmarke aufzutreten, während Elitegroup sich davon zurückzieht. Möglicherweise wird auch Lenovo nach dem Kauf der PC- und Notebooksparte von IBM eine ODM-Baureihe einführen.\n\nUnklar ist oftmals, wo die tatsächliche Endmontage stattfindet. Viele OEM-/ODM-Designs bekommen schon bei der Produktion in Asien ihre gesamte regionale Ausstattung (Tastatur, Handbücher). Andere werden als Barebones (ohne Speicher, CPU, Festplatte, manchmal auch ohne ODD oder ohne Display) angeliefert und dann in regionalen Fabriken (beispielsweise Fujitsu Technology Solutions in Augsburg für die Lifebooks, Toshiba in Regensburg, Dell in Irland) teils nach Kundenwünschen fertig bestückt.\n\nDie meisten Treibhausgasemissionen – gemessen in CO-Äquivalent (COe) – entstehen bei Notebooks während der Herstellung und nicht beim Gebrauch. Das entspräche der Nutzungsdauer von bis zu 88 Jahren, damit sich der Kauf eines Neugerätes „energetisch“ amortisiert. Hinsichtlich des Treibhausgases wurden drei verschiedene Datengrundlagen gewählt: EuP Lot 3, Ecoinvent 2.2 und Forschungsplan UBA UBA 2009: damit müsste ein Laptop mit zehn Prozent besserer Effektivität 33 (nach EuP) bis 88 (nach UBA) Jahre genutzt werden. Klaus Hieronymi, bei HP für Nachhaltigkeit zuständig, sagt: . Giftige Mittel werden möglichst ersetzt und teure Metalle eingeschränkter verwendet, so etwa Kupfer statt Silber. Allerdings werden die Erze ärmer und schlechter zugänglich: bei Kupfer muss bis zu 700-mal so viel Erz eingesetzt werden wie vor wenigen Jahrzehnten.\n"}
{"id": "41782", "url": "https://de.wikipedia.org/wiki?curid=41782", "title": "Drumcomputer", "text": "Drumcomputer\n\nEin Drumcomputer [], auch Drum Machine [] genannt, ist ein elektronisches Instrument zur Erzeugung perkussiver Klänge.\n\nDie Klangerzeugung erfolgt subtraktiv analog oder digital mit Hilfe von Samples. Angesteuert wird die Klangerzeugung dabei über\n\nIm letzten Fall besteht eine Überschneidung zum elektronischen Schlagzeug.\n\nDie ersten Drumcomputer waren einfache Automaten, die nur vorprogrammierte Rhythmen wie Mambo, Tango usw. abspielen konnten. Zielgruppe waren zumeist Alleinunterhalter. Beispiel für diese Art von Drumcomputern, die auch in Heimorgeln verwendet wurden, ist die „Rhythm Ace“-Serie des japanischen Unternehmens Ace Tone, die seit etwa 1967 hergestellt wurden.\n\nZu Beginn der 1980er Jahre kamen die ersten frei programmierbaren Drumcomputer auf den Markt. Die Klangerzeugung war analog, weshalb die Klänge nicht sehr natürlich klangen. Ein bekanntes Beispiel für diese Art der Drumcomputer ist der Roland TR-808. Rhythmen konnte man über den eingebauten Pattern-Sequenzer programmieren und das Abspielen per Schnittstelle mit anderen Geräten synchronisieren. Analoge Drumcomputer sind heute meist begehrte Sammlerstücke. Ihre Klänge, da natürlichen Klängen nicht sehr ähnlich, werden heute noch vielfältig in der Pop- und Dance-Musik angewendet. Aus diesem Grund findet man sie häufig in Form von Samples in moderneren Drumcomputern.\n\nMit dem Linn LM-1 setzte sich zunehmend eine Sample-basierte digitale Klangerzeugung durch. Der Klang wirkte zunehmend natürlicher, wobei die Realisierung der Wiedergabe von Beckenklängen aufgrund ihrer Länge und der damaligen Kosten für Speicherbausteine anfangs schwierig war. Einige Drumcomputer verwenden zur Ansteuerung die vom elektronischen Schlagzeug bekannten Drum-Pads. Dies sind druckempfindliche gummierte Flächen, die per Schlagzeug-Stick gespielt werden können. Ab Mitte der 1980er Jahre setzte sich ferner MIDI als Schnittstelle durch. Dies bedeutete, dass zunehmend nur noch die Klangerzeugung im Gerät stattfand und es über die MIDI-Schnittstelle extern angesteuert werden konnte bzw. musste.\n\nAb Mitte der 1990er Jahre ging die Verbreitung der Drumcomputer zurück, da sie zunehmend in Synthesizern (Workstations), wie der Korg M1 integriert, oder durch Sampler ersetzt wurden. Heute werden sie von Roland (unter dem Namen Boss), KORG, Elektron, Zoom und Alesis hergestellt.\n\nÜbliche Schlagzeugsamples sind Bassdrum, Snare, Hi-Hat, Cymbals, Tom und Percussion-Samples (zumeist Clap, Tambourin, Bongos, Rattle). Für die Samples sind meistens die wichtigsten Parameter wie Lautstärke, Dauer oder Geschwindigkeit/Tonhöhe (Pitch) einstellbar.\n\n\n"}
{"id": "41872", "url": "https://de.wikipedia.org/wiki?curid=41872", "title": "The Bat", "text": "The Bat\n\nThe Bat! ist ein proprietäres E-Mail-Programm für Windows. Entwickelt wird es seit 1997 von RITLabs, einer Softwarefirma mit Sitz in Chișinău, Moldawien.\n\nSeit Version 6.0.0, die nur als Vorabversion verfügbar war, ist The Bat! vollständig unicodetauglich. Dafür wurde ein großer Teil des Programms neu geschrieben.\n\nUnterstützt werden IMAP und POP3, deren SSL-Varianten mit sicherer Authentifizierung via APOP, CRAM-HMAC, NTLM und Compuserves RPA und SMTP inklusive sicherer Verbindung per StartTLS, SSL3 wird seit The Bat! 6.1 unterstützt. Via MAPI kann auch auf Mailfunktionen von Microsoft Exchange Server zugegriffen werden. GnuPG und PGP können eingebunden und aus The Bat! heraus angesprochen werden. S/MIME wird direkt unterstützt. Die Pro-Version verschlüsselt und sichert die Maildatenbanken bei Bedarf automatisch. HTML-Mails können auf Wunsch angezeigt werden, es werden jedoch keine aktiven Inhalte ausgeführt. The Bat! lädt keine Bilder, Stylesheets oder Skriptdateien nach. Beim Speichern/Öffnen von potentiell gefährlichen Dateien warnt das Programm den Anwender. Über Plugins kann es erweitert werden. Die Programmierschnittstelle steht auch externen Entwicklern offen.\n\nThe Bat! kann zu einem RSS-Reader erweitert und Virenscanner eingebunden werden.\n\nThe Bat! unterstützt beliebig viele Benutzerkonten, denen jeweils unterschiedliche Identitäten und Vorlagen zugewiesen werden können. Somit bekommt jedes Benutzerkonto eigene Ordner wie Posteingang und -ausgang. Diese werden nicht in einem zentralen Ordner abgelegt, können aber über die \"Virtuellen Ordner\" gemeinsam dargestellt werden.\n\nKomplexe verschachtelte Filter werden unterstützt. Sämtliche Eigenschaften einer Mail können als Kriterium angegeben werden. Trifft ein Filter, können Aktionen wie Verschieben, Markieren, Einfärben, Starten von externen Programmen, Exportieren, Weiterleiten, Adressbuchoperationen, Abspielen von Klängen und Weiteres ausgeführt werden. \"Selektive Filter\" lesen nur den Header einer E-Mail auf dem Server ein und löschen die Mail ggf., ohne sie vorher herunterzuladen. Als Spamfilter können verschiedene frei erhältliche Plug-ins verwendet werden. Virtuelle Ordner können mit zahlreichen Filterkriterien erstellt werden und zeigen Mails aus verschiedenen physischen Ordnern an.\n\nNeuen Nachrichten, Antworten und weiterzuleitenden Nachrichten können sehr flexible Vorlagen zugewiesen werden, etwa zur Erstellung automatisierter Anreden, Signaturen, Einleitungszeilen oder Cookies. Diese Cookies haben dabei aber nichts mit den HTTP-Cookies von Webseiten zu tun, sondern sind Vorlagen, mit deren Hilfe Texte aus externen Textdateien automatisch in Mails eingebunden werden können. Zudem gibt es zahlreiche vordefinierte Variablen. Vorlagen können pro Konto, Ordner, Adressbuchgruppe und Absender definiert werden. Zusammen mit den Filterfunktionen kann somit die Mailkommunikation zu einem großen Teil automatisiert werden. Mit der \"Serienbrief\"-Funktion können personalisierte Newsletter erstellt werden. \"Schnellvorlagen\" ermöglichen das schnelle Einfügen von Textbausteinen mit Shortcuts und die Wiederverwendung von Textbausteinen in verschiedenen Vorlagen.\n\nDie Rechtschreibprüfung prüft Texte während der Eingabe. Unbekannte Wörter kann man hinzufügen. Fremde Wörterbücher, beispielsweise von OpenOffice.org, können importiert werden.\n\nSymbolleisten können nach Wunsch angepasst werden, um zum Beispiel oft benutzte Funktionen als Symbole einzublenden oder die Oberfläche auf die notwendigen Symbole zu beschränken.\nEbenso können Shortcuts individuell angepasst werden.\n\nThe Bat! verfügt über einen Kalender mit Wiedervorlagefunktion und ein Notizbuch. \n\nÜber verschiedene Designs kann die Optik der Oberfläche angepasst werden. Farben und Muster der verschiedenen Oberflächenelemente werden damit beeinflusst.\n\nMehrere Adressbücher mit jeweils mehreren Adress-Gruppen können verwaltet werden. Eine Adresse kann in mehreren Gruppen Mitglied sein. Jede Adresse hat zahlreiche Felder; unter anderem mehrere E-Mail-Adressen, private und geschäftliche Postadresse, Geburtsdatum (mit optionaler Benachrichtigung), E-Mail-Vorlage, zu verwendende Codepage, Bild, OpenPGP-Keys und S/MIME-Zertifikate. Ab Version 7.0 (zurzeit in der Alphaphase) wird CardDAV unterstützt.\n\n\nDerzeit ermöglicht The Bat! den E-Mail-Import von folgenden Programmen:\n\n\nAdressbücher können in den folgenden Formaten importiert werden:\n\n\n"}
{"id": "42432", "url": "https://de.wikipedia.org/wiki?curid=42432", "title": "PET 2001", "text": "PET 2001\n\nDer Commodore PET 2001 (\"Personal Electronic Transactor\"; deutsch etwa \"persönlicher elektronischer Handlungsbeauftragter\") wurde im Januar 1977 vorgestellt und seit Juni 1977 für 795 US-Dollar vertrieben. Er ist damit der weltweit zweite für Privathaushalte erschwingliche und in Serie hergestellte \"persönliche Computer\" (PC). Er ist jedoch der weltweit erste PC in kompletter betriebsbereiter Ausführung, da er im Unterschied zum im April 1976 veröffentlichten ersten Personal Computer (dem Apple I) inklusive Gehäuse, Netzteil, Tastatur, Monitor und Massenspeicher (in Form einer Datasette) ausgeliefert wurde.\n\nDer PET 2001 wurde vor allem von Chuck Peddle entwickelt und Anfang 1977 auf der Consumer Electronics Show vorgestellt. Peddle hatte auch den Vorläufer KIM-1 entwickelt und war zuvor führend bei der Entwicklung des MOS-6502-Mikroprozessors gewesen, auf dem alle diese Geräte basieren.\n\nPeddle hatte mit dem KIM-1 schon einen direkten Vorläufer selbst konstruiert, auf dem er aufbauen konnte. Verglichen mit dem im April 1977 vorgestellten Apple II hatte der PET 2001 zwar keine Farbe, keine Einzelpunktgrafik und keine Erweiterungsslots, dafür konnte er aber Kleinbuchstaben darstellen, war (zumindest in Europa) nur ein Viertel so teuer und hatte in der Benutzeroberfläche einen intuitiv benutzbaren Screen-Editor, statt des eher gewöhnungsbedürftigen Zeileneditors beim Apple.\n\nZum Anschluss von Peripheriegeräten verfügte der PET über einen „parallelen IEC-Bus“ (IEEE-488-Anschluss). An diesem konnten neben Druckern und CBM-Diskettenlaufwerken auch Messgeräte und andere Steuerungsanlagen angeschlossen werden. Dies führte dazu, dass der PET (und etwas später vor allem seine direkten Nachfolgemodelle) vor allem an Universitäten und Forschungseinrichtungen, aber auch in Fabriken zur Produktionssteuerung eingesetzt wurde.\n\nDer PET 2001 war der erste Computer, der im deutschen Versandhandel (u. a. bei Quelle) erhältlich war. Der Preis betrug 2.999 DM, später rund 2.000 DM. Nach Entdeckung von Fehlern im BASIC – Arrays konnten eine maximale Größe von 256 Elementen erreichen – wurde der Preis noch einmal erheblich gesenkt.\n\nAuszug aus einem Prospekt des Unternehmens Vero (Vorläufer von Vobis) von 1979: \"Sehen Sie sich um: Überall noch Rechner, die zum 10-fachen Preis weniger leisten als er: Commodore PET 2001, der Computer, über den die Experten reden.\"\n\nDas englische Wort \"pet\" bedeutet auch „Haustier“ oder „Liebling“. Wie Chuck Peddle um 1990 in einem Interview erklärte, wurde der Name durch einen seinerzeit in den USA mit großem Erfolg verkauften Scherzartikel inspiriert: Pet Rock, ein Stein als Haustier. Das pflegeleichte Tierchen war in einer gepolsterten Schachtel erhältlich, inklusive einer Dressuranleitung.\n\n\n\nProgramme für den \"PET 2001\" werden primär in der Programmiersprache BASIC erstellt. Bereits wenige Sekunden nach dem Einschalten ist ein PET 2001 betriebsbereit und kann BASIC-Befehle entgegennehmen. Auch die Programmierung in Maschinensprache ist möglich, was entweder direkt über einen Assembler oder von BASIC aus mit Hilfe der BASIC-Befehle SYS (direkter Sprung zu einer Adresse), USR, PEEK und POKE (direktes Auslesen und Beschreiben einer Speicheradresse) geschieht.\n\nBei der Programmierung in BASIC dient der Bildschirm sowohl als Ausgabegerät als auch als Puffer für Benutzereingaben. Alle auf dem Bildschirm sichtbaren Ausgaben können jederzeit modifiziert und durch Betätigen der ENTER-Taste dem Rechner wieder als Benutzereingabe übergeben werden. Dieses Bedienungs- und Eingabekonzept wird als Bildschirm-Editor (engl. \"screen editor\") bezeichnet. Konkurrierende Rechner boten zu dieser Zeit lediglich wesentlich komplizierter zu bedienende \"Zeilen-Editoren\".\n\nDa der PET nur Zeichen darstellen kann, beschränken sich seine grafischen Fähigkeiten auf die Darstellung so genannter Blockgrafik. Hierzu enthält der Standard-Zeichensatz neben Großbuchstaben und Spielkartensymbolen alle Kombinationen von 2×2-Teilquadraten, mit denen eine Pseudo-Auflösung von 80×50 Punkten dargestellt werden kann. Ein alternativ verwendbarer Zeichensatz bietet Groß- und Kleinbuchstaben ohne Grafiksymbole.\n\nFertige Software gab es ausschließlich für den Spielbereich. Das Spiel Mondlandung, mit dem eine Mondlandung simuliert wurde und Spiele von Spielautomaten (Schiffe versenken) waren beliebte Spiele. Man behalf sich durch Selbstprogrammierung oder das Abtippen von Programmen aus der Funkschau, der ELO oder der Chip, wobei die beiden Letzteren 1979 neben den amerikanischen Zeitschriften Creative Computing und Byte die einzigen deutschen Zeitschriften für Computer waren. Zu dieser Zeit etablierte sich auch die erste Computersendung des deutschen Fernsehens: der WDR Computerclub mit Wolfgang Back und Wolfgang Rudolph. Dort wurden auch Programme in Form akustischer Signale gesendet, die – mit dem Mikrofon oder einem Verbindungskabel auf Musikkassette aufgenommen − später über eine Datasette in den PET eingelesen und ausgeführt werden konnten.\n\nBei der Programmierung des PET stieß man bereits bei den damaligen Anforderungen rasch an Grenzen. Nach dem Einschalten meldet ein PET 2001 mit einem Speicherausbau von 8 KB „7167 Bytes free“. Da Programm und Daten sich diesen Speicher teilen müssen, scheiterten oft schon − aus heutiger Sicht − kleine Vorhaben an den begrenzten Speicherkapazitäten des PET. Nicht selten wurden zwei Versionen eines Programmes hergestellt: eine übersichtliche und dokumentierte Version (ein Befehl je Zeile, viele Kommentare) und eine Arbeitsversion, in der durch mehrere Befehle pro Zeile sowie Verzicht auf Kommentare Bytes eingespart wurden, wo es nur ging.\n\nDie Betriebssoftware des PET 2001 unterteilt sich in den BASIC-Interpreter und den so genannten Kernal, der zur damaligen Zeit Commodore-intern noch \"Kernel\" genannt wurde und erst später (zu Zeiten des VC-20) den Namen \"Kernal\" bekam. Der Kernal stellt sozusagen das Betriebssystem des PET dar. Er beinhaltet die Software zur Systeminitialisierung und zur Hardware-nahen Ein- und Ausgabe. Darüber hinaus zeichnet der Kernal auch für den Bildschirm-Editor verantwortlich.\n\nDer BASIC-Interpreter des PET stammt ursprünglich von Microsoft und wurde von Commodore an den Kernal angepasst. Er belegt 8 KB des 14 KB großen ROMs und weist in seiner ersten Version noch einige ungewöhnliche Verhaltensweisen auf. So führt beispielsweise die Ausführung der Zeile\n\nzur Meldung \"Syntax Error\", da sie vom BASIC-Interpreter als\n\ninterpretiert wird. Umstritten ist, ob man bei diesem Verhalten von einem Fehler sprechen kann: Die einen sagen, die eingegebene Zeile sei syntaktisch korrekt und dürfe nicht zu einer Fehlermeldung führen, andere behaupten, dieses Verhalten sei wohldokumentiert und könne nicht als Fehler angesehen werden.\n\nDie Arbeitsgeschwindigkeit des BASIC-Interpreters des PET ist mit etwa einer Millisekunde pro BASIC-Befehl aus heutiger Sicht langsam, ebenso das Laden von Programmen mit zwei Minuten für 8 KB. So lief beispielsweise die in BASIC nachprogrammierte Version des Spiels Space Invaders sehr langsam. In Maschinensprache programmierte Anwendungen hingegen erlauben deutlich höhere Ausführungsgeschwindigkeiten. So gibt es beispielsweise ein Schachprogramm, das mit seiner Spielstärke für Laien mehr als ausreichend ist.\n\nUm die Kompatibilität von Maschinenprogrammen auf 8-Bit-Rechnern von Commodore über mehrere Kernal-Versionen und Rechner-Generationen zu ermöglichen, existiert am Ende des ROM-Bereichs (also unmittelbar vor der Adresse $FFFA) eine Sprungtabelle, über die die wichtigsten Kernel-Routinen aufgerufen werden können. Commodore hat diese Sprungtabelle bis zum C128 hin beibehalten. So gibt zum Beispiel der Maschinenbefehl codice_3 auf jedem 8-Bit-Rechner von Commodore den Inhalt des Akkumulators als ASCII-Zeichen auf den Bildschirm aus. Leider hat das die Kompatibilität von Anwendungssoftware nicht sehr gesteigert, weil viele Programmierer − vor allem aus Geschwindigkeitsgründen − diese kompatible Methode des Aufrufs schlichtweg ignoriert haben.\n\nDer PET besaß ebenso wie die spätere CBM-Baureihe oder der Commodore 64 lediglich Zeichensatz- und keine Pixelgrafik. Daher mussten grafische Elemente durch geschicktes Kombinieren von Sonderzeichen realisiert werden. Üblicherweise wurde hierfür die sogenannte \"Viertelpunktgrafik\" benutzt: Da zwölf spezielle Zeichen vorhanden waren, bei denen jeweils ein Viertel, die Hälfte bzw. drei Viertel der Fläche des Cursors (d. h. eines Zeichens) gefüllt waren, konnten einschließlich der Zeichen für einen leeren bzw. voll gefüllten Cursor Grafiken mit der im Vergleich zur Textdarstellung doppelten Zeilen- sowie Spaltenzahl erzeugt werden (d. h. mit der vierfachen Auflösung).\n\nZur Tonerzeugung wurde vielfach das Schieberegister des VIA 6522 benutzt. Dazu musste lediglich ein Verstärker an den Userport angeschlossen werden, um aus Rechtecksignalen bestehende Töne erzeugen zu können. Spätere CBM-Modelle hatten dazu schon einen Piezo-Lautsprecher auf der Platine angeschlossen. Ein ähnliches Verfahren nutzten die späteren PCs.\n\nObwohl der PET kein klassischer Spiel-Computer war, wurden einige Dutzend Spiele entwickelt und auf Kassette und teilweise Diskette ausgeliefert, hauptsächlich von Scott Adams und Avalon Hill.\n\nHerausragende in Assembler realisierte Projekte sind ein Schachprogramm oder eine Weltraumkampfsimulation in Pseudo-3D mit Star-Wars-Motiven.\n\nViele damalige Spielhallen-, Konsolen- sowie Brett- und Kartenspielklassiker wurden unter Abstrichen von Privatanwendern portiert, darunter zahlreiche Adventures, Programme aus der Unixwelt (Startrek, Space War), kleine Animationsfilme und Reaktionsspiele (Cowboy-Duell, Clown-Quest) sowie eine gelungene Umsetzung von Space Invaders (dessen BASIC-Version systembedingt sehr langsam war).\n\nMit Emulatoren wie VICE und M.E.S.S. kann der PET 2001 auf aktuellen Computern emuliert werden.\n\nAufgrund der guten Verkäufe in den USA und Kanada wurde die PET-Serie auch in Europa eingeführt. Allerdings kam es dort zu einem Konflikt mit Philips, da dieses Unternehmen sich bereits die Rechte am Namen „PET“ (dort Abkürzung für Programm-Entwicklungs-Terminal) gesichert hatte. So wurden die Modelle in „CBM“ umbenannt.\n\nAufwärtskompatibler Nachfolger des PET 2001 war der CBM 3001. \nMit Blick auf Büroanwendungen entwickelte Commodore den PET 2002 zur Büroserie CBM 4016/4032 und 8016/8032 weiter, an die Diskettenlaufwerke (5¼ Zoll, auch 8 Zoll) angeschlossen werden konnten und die über eine professionelle Schreibmaschinentastatur verfügten. Ab der 4000er-Serie gab es auch ein verbessertes Commodore BASIC mit diversen Befehlen zur Diskettenbenutzung.\n\nIm Jahr 1982 wurde die PET-Serie eingestellt. Als Nachfolger war die CBM-II-Serie vorgesehen, die aber wegen des beginnenden Sterbens nicht IBM-PC-kompatibler Businessrechner nur noch geringen Erfolg hatte. Bald darauf setzte dann auch Commodore im Business-Segment auf IBM-kompatible Modelle.\n\nMit Blick auf den Heimcomputermarkt, insbesondere die Fähigkeit, auf dem PET 2001 Computerspiele zu spielen, wurden ab Januar 1981 der VC20 und Dezember 1982 der C64 herausgebracht, die beide auf der Architektur des PET 2001 basierten, allerdings, außer für einfachste Programme, nicht mit dem PET (oder untereinander) kompatibel sind.\n\n\n"}
{"id": "42628", "url": "https://de.wikipedia.org/wiki?curid=42628", "title": "Linux User Group", "text": "Linux User Group\n\nEine Linux User Group, kurz LUG, ist eine Gruppe von Linux-interessierten Personen und -Anwendern, die sich mehr oder weniger regelmäßig treffen oder auf andere Weise sich miteinander vernetzen. Die meisten Treffen und Veranstaltungen von LUGs finden in größeren Städten statt. Einige nennen sich Linux-Stammtisch oder sind inzwischen als Verein eingetragen. Die LUGs organisieren beispielsweise „Installationspartys“. Jedoch sind die Themen meist nicht auf Linux beschränkt, sondern beschäftigen sich auch weitergehend mit freier Software. Ein reger Erfahrungsaustausch findet außerdem in Internetforen und eMailinglisten statt, wobei sich seit der Entwicklung der zahlreichen Linux-Derivate eine Vielzahl von teilweise auf bestimmte Distributionen spezialisierten Gruppen gebildet haben.\n\nAnalog zu LUGs gibt es auch Unix User Groups (UUG), die sich mit anderen Unix-Derivaten beschäftigen, beispielsweise mit den frei verfügbaren BSDs (FreeBSD, OpenBSD, NetBSD), aber auch kommerziellen Unix-Versionen. Auch andere Bezeichnungen die sich nicht direkt auf Linux, Unix oder BSD beziehen sind üblich. Viele haben aber meist den Begriff \"Frei\" im Nahmen, der sich auf freie Software bezieht.\n\nIn Österreich bestehen die \"Linux User Group Austria (LUGA)\" sowie einzelne LUGs in größeren Städten (Salzburg, Sankt Pölten, Linz, Graz) und in allen Bundesländern außer dem Burgenland (dort existiert aber eine Untergruppe des Internetclub Burgenland, die eine ähnliche Rolle spielt). Die österreichischen LUGs organisieren zusammen mit anderen Vereinen einmal jährlich die Linuxwochen. In der Schweiz existieren neben der \"LUGS (Linux User Group Switzerland)\" auch regionale User Groups in größeren Städten (Basel, Bern etc.) oder in größeren Regionen (Oberwallis, Kanton Aargau etc.). Einmal jährlich veranstaltet abwechselnd eine LUG das LUG-Camp, an dem sich regelmäßig die deutschsprachigen LUGs treffen.\n\nIn Italien gibt es LUGs in vielen Städten, sie sind in der ILS (\"Italian Linux Society\") zusammengefasst. Sie veranstalten jedes Jahr (seit 2006 Ende Oktober) einen landesweiten Linuxday. Seit ca. 2016 wird diese Veranstaltung auch mit dem Linux Presentation Day verbunden und dort z. B. auf linux-events.org gelistet.\n\n"}
{"id": "42652", "url": "https://de.wikipedia.org/wiki?curid=42652", "title": "Triumph Adler Alphatronic PC", "text": "Triumph Adler Alphatronic PC\n\nDer TA Alphatronic PC ist ein kompakter 8-Bit-Bürocomputer des deutschen Herstellers Triumph-Adler aus dem Jahr 1984. Er ist äußerlich leicht zu verwechseln mit dem 16-Bit-Rechner Alphatronic PC16 aus demselben Hause, unterscheidet sich jedoch durch das orange abgesetzte Gehäuse, das beim PC16 in Braun gehalten ist. Außerdem sind die Tastenkappen der Tastatur beim Alphatronic PC flach und die sechs Funktionstasten in der obersten Reihe ebenfalls in der Farbe Orange gehalten.\n\nUnter dem Namen \"Alphatronic\" hatte Triumph-Adler schon eine Zeitlang eine ganze Serie von Computern mit mehr oder minder gutem Erfolg auf den Markt gebracht. Der Alphatronic PC war der erste kompakte Tastaturrechner von TA. Wegen der unzureichenden Verkaufszahlen konzentrierte sich das Unternehmen jedoch bald wieder verstärkt auf das Kerngeschäft mit Schreibmaschinen. Hauptgrund für den schlechten Absatz waren vor allem die alten Vertriebswege: TA lieferte seinerzeit ausschließlich an lizenzierte Generalvertretungen mit Gebietsschutz. So mussten aus Büromaschinenmechanikern Computerexperten werden. Dies gelang nicht vielen, und nur wenige konnten so die Geräte überzeugend verkaufen. Die Computersparte von TA war eher auf Systeme der mittleren Datentechnik spezialisiert und mochte sich nur ungern mit den in dieser Branche nur mit einem müden Lächeln betrachteten „Homecomputern“ auseinandersetzen. Viele Geräte des Wettbewerbs konnte man sehr viel günstiger in jedem gut sortierten Kaufhaus oder im Versandhandel erwerben.\n\nDie äußerst rasante Entwicklung in diesem Markt sorgte dafür, dass viele Computer von TA technisch bereits überholt waren, bevor sie in Produktion gehen sollten. Einige Modelle verließen daher nicht einmal das Entwicklungsstadium. Auch ein späterer Versuch, mit PCs der Dario-Reihe in dem Markt wieder Fuß zu fassen, misslang. Der Vertrieb von OEM-Geräten des Mutterkonzerns Olivetti kam zu spät. Zudem waren die PCs von TA oft baugleich mit denen von Olivetti, allerdings wesentlich teurer.\n\nAndererseits lernten die Entwickler aus diesen Erfahrungen und brachten die Technik der TA-Computer in die Bildschirmschreibsysteme der sehr erfolgreichen VS- und BSM-Reihe ein. Der Alphatronic jedoch verschwand von der Bildfläche.\n\nTA brachte wenig später noch den Alphatronic PC16 mit Intel-8088-Prozessor und MS-DOS heraus (siehe Hauptartikel). Doch auch dieser war erneut zu teuer und technisch bereits überholt, als er auf den Markt kam.\n\nDer TA PC verwendete als CPU den Zilog Z80. Die Tastatur mit abgesetztem Ziffernblock und sechs Funktionstasten war in das Gehäuse integriert. Mit einem Preis von 1495 DM war er konkurrenzfähig und landete unter den ersten fünf der deutschen Verkaufsstatistik. Er wurde aber vorwiegend als kleiner Bürocomputer eingesetzt, da er keine Pixelgrafik, sondern nur eine Blockgrafik hatte. Anders als die Personalcomputer P1, P2, P3 und P4 wurde der TA PC nicht von Triumph-Adler selbst entwickelt und gebaut, sondern nach TA-Vorgaben in Japan entwickelt und dort zusammengebaut.\n\nStandardmäßig hatte der TA PC 64 kB RAM. Von den 32 kB ROM wurden 24 kB für einen BASIC-Interpreter von Microsoft (Microsoft ROM BASIC V5.11) benötigt.\n\n\nDer TA PC konnte mit unterschiedlichen Betriebssystemen arbeiten:\n\nDurch die Möglichkeit, das Betriebssystem CP/M einzusetzen, konnte auch Standardsoftware wie WordStar, dBASE, Turbo-Pascal oder Multiplan verwendet werden.\n\nEs gab zwei unterschiedliche Diskettenlaufwerke, die auf den 5¼-Zoll-Disketten 320 kB speichern konnten. Das erste Diskettenlaufwerk (F1) enthielt den Controller und wurde direkt mit einem 50-adrigen Kabel an den PC angeschlossen. Dieses Laufwerk kostete rund 1800 DM. Das zweite Laufwerk (F2) wurde an das erste Laufwerk angeschlossen und hatte keinen eigenen Controller. Wegen des fehlenden Controllers war das zweite Laufwerk auch wesentlich günstiger.\n\nGegen Ende der Produktionszeit wurde von einem Fremdanbieter eine für die damalige Zeit recht leistungsfähige Grafikkarte entwickelt, die zwischen Tastatur und Hauptplatine eingebaut und über einen reservierten Adressraum im oberen RAM-Bereich durch spezielle Befehle angesprochen wurde. Auf diese Weise konnte man die 64-kB-Grenze des Z80 umgehen und zusätzliche 32 kB Videospeicher nutzen.\n\n"}
{"id": "42756", "url": "https://de.wikipedia.org/wiki?curid=42756", "title": "LinuxTag", "text": "LinuxTag\n\nDer LinuxTag war die europaweit größte Messe zum Thema „freie Software“ mit dem Themenschwerpunkt Linux. Die bis 2014 jährlich stattfindende Veranstaltung gab einen umfassenden Überblick über den Markt um Linux und freie Software, und trug dazu bei, Kontakte zwischen Benutzern und Entwicklern herzustellen. Der LinuxTag war gemeinsam mit den weltweiten OpenSource World die für Anwender und Entwickler weltweit bedeutendste Veranstaltung dieser Art.\n\nDer LinuxTag hatte den Slogan „where .com meets .org“, also das Zusammentreffen von kommerziellen und nicht-kommerziellen Gruppen aus dem IT-Sektor. Zusätzlich gab es jedes Jahr ein weiteres Jahresmotto.\n\nDer LinuxTag sah sich als Teil der Free-Software-Bewegung und förderte diese Community in einem außergewöhnlichen Maße durch Unterstützung der zahlreichen Open-Source-Projekte. Diesen Projekten bot der LinuxTag mit eigenen Ständen, Foren und Vorträgen Möglichkeiten, ihre Software, ihre Konzepte und damit das ganze Projekt der Öffentlichkeit in angemessener Form vorzustellen - . Dabei konnten und sollten Projekte durch regen Austausch mit anderen Gruppen und Firmen konzeptionell und inhaltlich profitieren. Fortschritte und neue Technologien wurden ausgiebig diskutiert und neue Wege erschlossen.\n\nDie Konferenzmesse LinuxTag wurde 1996 von einer Handvoll Protagonisten der Unix-Arbeitsgruppe (Unix-AG) an der Universität Kaiserslautern ins Leben gerufen. Sie wollten über die damals neue Technologie Linux und Open Source informieren. Der LinuxTag bestand anfangs nur aus einer kleinen Zahl von Teilnehmern, sowohl Ausstellende als auch Vortragende. Um mit der schnell wachsenden Zahl von Teilnehmern und Ausstellern mithalten zu können, wurde der Standort mehrmals verlegt.\n\nDie ersten LinuxTage fanden an der Technischen Universität Kaiserslautern statt.\n\nDer erste LinuxTag war ein Themenabend über Linux. Bis 1998 hatte der LinuxTag bereits 3.000 Besucher, nachdem er zuvor zum ersten Mal überregional bekannt gemacht worden war. 1999 fand der LinuxTag mit etwa 7.000 Besuchern zum ersten Mal in einem eigenen Gebäude und zum bisher letzten Mal in Kaiserslautern statt. In der Zeit danach wurde der LinuxTag e. V. gebildet.\n\nIn den Jahren 2000 (15.000 Besucher) und 2001 wurde der LinuxTag in Stuttgart abgehalten.\n\nDer LinuxTag 2000 fand im Messezentrum Stuttgart vom 29. Juni 2000 bis zum 2. Juli 2000 statt. Es kamen bis zu 17.000 Besucher. Es fand erstmals ein Business-Kongress statt. Hier wurden Themen wie IT-Sicherheit, rechtliche Aspekte freier Software, kommerzielle Anwendungen unter Linux und Support zur Sprache kommen. IT-Entscheider konnten sich anhand von Fallstudien über die Einsatzbereiche freier Software informieren.\n\nDer LinuxTag 2001 fand im Messezentrum Stuttgart vom 5. Juli 2001 bis zum 8. Juli 2001 statt. Es kamen bis zu 14.870 Besucher. Er stand unter der Schirmherrschaft des Bundeswirtschaftsministeriums.\n\nVon 2002 bis 2005 fand der LinuxTag in Karlsruhe statt.\n\nDer LinuxTag fand im Jahr 2002 erstmals vom 6. Juni 2002 bis 9. Juni 2002 im Kongresszentrum in Karlsruhe statt. Es kamen etwa 13.000 Besucher. Er stand unter dem Motto „Open your mind, open your heart, open your source!“ (deutsch etwa: \"Öffne deinen Geist, öffne dein Herz, öffne deine Quell(text)e!\").\n\nDer LinuxTag 2003 stand unter dem Motto \"Open Horizons\" und fand vom 10. bis 13. Juli 2003 zum zweiten Mal in Karlsruhe statt. Zusammen mit der Eintrittskarte für 10 Euro erhielten die Besucher auch die auf dem LinuxTag zum ersten Mal erhältliche Knoppix-DVD und einen Tux-Pin. Geöffnet war jeweils von 9:00 bis 18:00 Uhr, außer sonntags (bis 16:00 Uhr). Mit 19.500 Besuchern stieg die Zahl der Besucher um 40 Prozent gegenüber dem Vorjahr.\n\nAls Aussteller waren sowohl Unternehmen als auch nicht-kommerzielle Gruppen vertreten. Apple zeigte Mac OS X in Verbindung mit Open Source. Rund 150 Aussteller waren 2003 dabei. 2002 waren es noch etwa 100 Aussteller gewesen.\n\nWeitere Highlights waren die Freigabe von OpenGroupware.org nach dem Modell von OpenOffice.org als Open Source und die kostenlose Umstellung mehrerer Dutzend Xboxen, teilweise mit Hardware-Modifikation durch zwei Lötpunkte, teilweise durch Einspielen des sogenannten MechInstallers auf Linux.\n\nDaneben gab es noch einen Programmierwettbewerb und am Sonntag fand zwischen 13 und 14 Uhr ein Weltrekordversuch statt: Auf einem Server sollten 100-Linux-Desktop-Sessions mit Gnome und KDE gleichzeitig zum Laufen gebracht werden. Dabei konnte über das Internet jeder mitmachen. Das Ergebnis wurde nicht bekanntgegeben.\n\nNeben der Ausstellung fanden auch Kongresse statt, bei denen renommierte Experten über einen Themenkreis referierten. Es gab zum Beispiel eine Debian-Konferenz und am Sonntag gab es einen Vortrag über TCPA mit anschließender Diskussion. Der Business- und Behördenkongress hat sich mit 400 Teilnehmern um etwa 60 % vergrößert. Das freie Vortragsprogramm wurde am Freitag vom Parlamentarischen Staatssekretär im Bundesministerium für Wirtschaft und Arbeit (BMWA), Rezzo Schlauch eröffnet.\n\nÜber Webcams konnte man auch einen virtuellen Besuch auf der Messe machen. Die Pingu-Cam (Tux der Pinguin ist das Linux-Maskottchen) zeigte Bilder aus dem Tiergarten von Karlsruhe, der direkt neben dem Messegelände liegt.\n\nDer LinuxTag 2004 fand vom 23. bis 26. Juni 2004 zum dritten Mal im Kongresszentrum in Karlsruhe statt. Wer sich auf der Homepage anmeldete, bekam freien Eintritt. Für 10 Euro Eintritt bekam man allerdings einen Tuxpin, eine Knoppix-DVD und eine DVD mit FreeBSD, NetBSD und OpenBSD.\n\nDer LinuxTag 2004 hatte das Motto \"„Free source – free world“\". Es wurden 16.175 Besucher gezählt. Bei der Rekordanzahl von etwa 170 Ausstellern waren neben vielen freien Projekten auch zahlreiche große und mittlere Unternehmen dabei. Hewlett-Packard war zum dritten Mal \"Offizieller Cornerstone Partner\". Weitere wichtige Firmen waren der C&L Verlag, Intel, Novell, Oracle, SAP und Sun Microsystems. Zum ersten Mal war auch Microsoft mit einem Stand vertreten.\n\nAuf dem eintägigen Business- und Behörden-Kongress am 24. Juni wurden Fallstudien und Erfolgsgeschichten über Einsatzmöglichkeiten von Open Source in Unternehmen und Behörden vorgestellt. Dabei kam unter anderem auch das Problem mit Viren und Würmern zur Sprache.\n\nFür den freien Kongress gab es eine Rekordbeteiligung mit etwa 350 Vorschlägen aus über 20 Ländern. 130 davon konnten im Programm untergebracht werden. Dabei war die Problematik der Softwarepatente ein wichtiges Thema.\n\nWettbewerbe auf diesem LinuxTag waren ein Coding-Marathon und der Hacking Contest 2004.\n\nDer LinuxTag 2005 fand in der Zeit vom 22. bis 25. Juni 2005 im Kongresszentrum Karlsruhe statt. Der LinuxTag 2005 war insgesamt der 11. LinuxTag und stand unter dem Motto „Linux everywhere“. Neben der Ausstellung verschiedener Firmen, die mit Linux mehr oder weniger zu tun haben, gab es auch 2005 wieder ein Vortragsprogramm. Auch der Business- und Behörden-Kongress fand am 22. Juni 2005 wieder statt. Vor dem offiziellen Start des LinuxTages gab es die Möglichkeit, sich vom 21. bis 23. Juni 2005 in verschiedenen Tutorials weiter zu bilden.\n\nJimmy Wales kündigte in seiner Eröffnungsrede die Zusammenarbeit zwischen Wikipedia und KDE an. Über eine Webservices-Schnittstelle soll jedes Programm direkt auf die Wikipedia zugreifen können. Der KDE-Medienplayer Amarok kann ab der Version 1.3 auf Wikipedia-Artikel von Künstlern zugreifen.\n\nDer Veranstalter sprach von 12.000 Besuchern, was hauptsächlich auf die neu gestalteten Eintrittspreise und die bis dato heißeste Woche des Jahres zurückgeführt wurde.\n\n2006 war Wiesbaden der Veranstaltungsort.\n\nDer LinuxTag 2006 fand vom 3. bis 6. Mai 2006 in den Rhein-Main-Hallen in Wiesbaden unter dem Thema \"„See, what’s ahead“\" statt. Laut Veranstalter besuchten über 9.000 Personen aus über 30 Nationen den LinuxTag 2006. Es gab viele, oft auch internationale Vorträge und verschiedenste Informationsstände; anwesend waren u. a. IBM, Avira und Sun Microsystems, wobei auch einige wie z. B. Hewlett-Packard oder Red Hat fehlten. Am \"Hacking-Contest\" nahmen drei Mannschaften teil.\n\nDer LinuxTag präsentierte sich noch stärker als in den Jahren zuvor als Open-Source-Veranstaltung und hatte erstmals einen BSD-Tag, mit Vorträgen der großen BSDs FreeBSD (und dessen Variante mit KDE-Oberfläche DesktopBSD), NetBSD und OpenBSD, die auch mit einer BSD-Area vertreten waren.\n\nDer am stärksten besuchte Vortrag des Jahres war die Keynote von Ubuntu-Gründer Mark Shuttleworth, der sich dabei selbst als den „Chefträumer von Ubuntu“ bezeichnete und über die gute Zusammenarbeit der Nutzer mit den Entwicklern sprach. Außerdem betonte er, dass Kubuntu und Ubuntu gleichwertig zu behandeln seien und dass es eine gute Zusammenarbeit der Entwickler gebe.\n\nDaneben gab es die Möglichkeit, einige der Vorträge über einen Videostream zu verfolgen, was von schätzungsweise 1.800 Personen genutzt wurde.\n\nSeit 2007 findet der LinuxTag in Berlin statt. Bis 2013 war der Veranstaltungsort die Messehallen unter dem Berliner Funkturm.\n\nDer LinuxTag 2007 fand vom 30. Mai bis zum 2. Juni 2007 mit dem Motto „Come in: We're open!“ statt. Er wurde von etwa 9600 Menschen besucht.\n\nDie Veranstaltung stand unter der Schirmherrschaft von Bundesinnenminister Wolfgang Schäuble. Dies löste Aufgrund der politischen Haltung des Bundesinnenministers eine rege Diskussionswelle aus, welche bis hin zu Boykottierungsaufrufen des LinuxTages ausuferte. Die Aufruhr in der Linux-Gemeinde war so groß, dass selbst ausländische Seiten darüber berichteten.\n\nDer LinuxTag 2008 fand vom 28. bis 31. Mai auf dem Berliner Messegelände mit 11.612 Besuchern statt. Der zweite LinuxTag in der Deutschen Hauptstadt steht unter der Schirmherrschaft von Bundesaußenminister und Vizekanzler Frank-Walter Steinmeier und findet im Rahmen einer sechstägigen „IT-Woche in der Hauptstadtregion“ statt, zu der auch die zum vierten Mal in Berlin stattfindende IT-Businessmesse IT Profits unter der Schirmherrschaft von Bundesverkehrsminister Wolfgang Tiefensee, zugleich Beauftragter der Bundesregierung für die Neuen Bundesländer, der 2. Deutsche Asterisk-Tag, der Anwender- und Entwicklerkonferenz zu Voice over IP, und der 8. @kit-Kongress zu juristischen Fragen der professionellen IT-Nutzung zählen. Wichtige Themen waren „Highlights für den Digital Lifestyle“ und die „Mobile + Embedded Area“.\n\nDer LinuxTag 2009 fand vom 24. bis 27. Juni auf dem Berliner Messegelände statt. Er hatte mehr als 10.000 Besucher. Er steht unter der Schirmherrschaft von Bundesaußenminister und Vizekanzler Frank-Walter Steinmeier. Der neue Präsident der Free Software Foundation Europa Karsten Gerloff besuchte den LinuxTag. Ein Schwerpunkt sind die „Abbildung von Businessprozessen mit Linux“ sowie „Open Source in den Farben der Trikolore“, wofür 14 Open-Source-Anbieter aus Frankreich ihr Produkt- und Dienstleistungsspektrum zeigen.\n\nDer 16. LinuxTag fand vom 9. bis 12. Juni 2010 auf dem Berliner Messegelände statt. Er wurde von etwa 11.600 Menschen besucht. Er steht unter der Schirmherrschaft von Cornelia Rogall-Grothe, Beauftragte der Bundesregierung für Informationstechnik. Keynotesprecher waren Microsofts General Manager James Utzschneider, der mit seinen offenen Umgang mit Open Source die Zuhörer verblüffte, der CEO von SugarCRM Larry Augustin unterstrich die wirtschaftliche Bedeutung von OSS und den Zusammenhang mit dem kommenden Trend Cloud-Computing, der Open-Source-Chef von Google, Chris DiBona, unterstrich das hohe Fachniveau des Kongresses, der Kernel-Entwickler Jonathan Corbet gab einen Ausblick auf den nächsten Linux-Kernel 2.6.35 und Ubuntu-Begründer Mark Shuttleworth steckte die Wegmarken für Ubuntus Desktops ab.\n\nDer 17. LinuxTag fand vom 11. bis 14. Mai 2011 auf dem Berliner Messegelände mit dem Motto „Zukunft gestalten, Renaissance einleiten“ statt. Er hatte 11.582 Besucher und stand unter der Schirmherrschaft von Cornelia Rogall-Grothe der Beauftragten der Bundesregierung für Informationstechnik. Keynotes wurden gehalten von Wim Coekaerts (Oracle), Bradley Kuhn (Software Freedom Conservancy) und Daniel Walsh (Red Hat).\n\nDer 18. LinuxTag fand vom 23. bis 26. Mai 2012 auf dem Berliner Messegelände mit dem Motto „Open minds create effective solutions!“ statt. Er steht unter der Schirmherrschaft von Cornelia Rogall-Grothe der Beauftragten der Bundesregierung für Informationstechnik. Auf dem LinuxTag fand die Premiere des „OpenMind ManagerMorning“ statt, auf dem Branchenexperten und Lehrende über die IT und Veränderungen in der Gesellschaft diskutierten und philosophierten.\n\nZudem fand auch die Premiere der neuen Vortragreihe „Open Minds Economy“, organisiert durch die Open Source Business Alliance und die Messe Berlin, statt, die das Erfolgsmodell von Open Source in den Bereichen Wirtschaft und Gesellschaft präsentiert.\n\nKeynotes wurden gehalten von Jimmy Schulz, Vorsitzender der Projektgruppe \"Interoperabilität, Standards und Open Source\" der Enquete-Kommission zu Internet und digitaler Gesellschaft im Deutschen Bundestag, Ulrich Drepper, Maintainer der GNU C Standard Library Glibc und Lars Knoll, Mitarbeiter bei Nokia und Chief Maintainer der QT-Bibliothek.\n\nDer 19. LinuxTag fand vom 22. bis 25. Mai 2013 auf dem Berliner Messegelände mit dem Motto „In der Wolke und „eingebettet“ – Siegeszug der freien Software hält an“ statt. Er steht unter der Schirmherrschaft von Cornelia Rogall-Grothe der Beauftragten der Bundesregierung für Informationstechnik.\n\nEs fand die Premiere des Open-IT Summit statt, der als parallele Konferenz zum LinuxTag von der Open Source Business Alliance (OSBA) und der Messe Berlin organisiert wurde und die Thematik Open Source im Businessumfeld stärker beleuchtet. Ebenfalls fand der OpenStack Day als erste größere Subkonferenz zu dem Thema OpenStack in Europa in Kooperation mit der OpenStack Foundation statt. Die Stiftung hat ihren Sitz in den USA und versteht sich als weltweit tätiges Sammelbecken für die gleichnamige, skalierbare Cloud-Management-Plattform.\n\nKeynotes wurden gehalten von Kernel-Entwickler Matthew Garrett zum Thema Unified Extensible Firmware Interface (UEFI) und Secure Boot und von Benjamin Mako Hill, Forscher am Massachusetts Institute of Technology, der dazu aufrief sogenannte Antifeatures nicht hinzunehmen, bei denen Hersteller Restriktionen in Geräte einbauen.\n\nDie Besucherzahlen des LinuxTags waren trotz immer mehr Benutzern von Open-Source-Programmen gesunken.\n\nUm sich auf die Veränderungen anzupassen, richtete sich der Linuxtag im Jahr 2014 durch Fokussierung und Diversifizierung auf das Kernthema der professionellen Nutzung von Open-Source-Software aus. Dazu ging der LinuxTag eine strategische Partnerschaft mit der droidcon ein und wendete sich verstärkt dem Vorstellen konkreter Techniken und Produkte zu, die im Umfeld von Open Source entstanden. Dazu gehörten unter anderem Cloud-Technik wie Infrastructure as a Service, IT-Service-Management, skalierbare Storage-Systeme, Enterprise-Web-Frameworks sowie Content-Management-Systeme und Embedded Linux.\n\nDer 20. LinuxTag fand vom 8. bis 10. Mai 2014 im STATION Berlin statt, in räumlicher und zeitlicher Nähe zur Media Convention Berlin (6. bis 7. Mai), der (6. bis 8. Mai) sowie der droidcon (8. bis 10. Mai 2014). Alle Events strebten eine enge Verknüpfung an, um über thematische Brücken eine Aufwertung für Besucher und Programm zu erzielen. Auf Kritik stieß der von 50 auf 150 Euro stark gestiegene Eintrittspreis.\n\nTrotz der Änderungen wurde die Veranstaltung anschließend eingestellt. Gründe dafür waren mangelnde Sponsoren und der hohe Arbeitsaufwand, der von den Vereinsmitgliedern nebenberuflich hätte erbracht werden müssen.\n\n\n"}
{"id": "42840", "url": "https://de.wikipedia.org/wiki?curid=42840", "title": "Computer Zeitung", "text": "Computer Zeitung\n\nDie Computer Zeitung, gegründet 1970 zur ersten CeBIT, war die erste westdeutsche Computerzeitung. In der DDR erschien bereits 1964 mit der rechentechnik/datenverarbeitung eine Zeitschrift, die sich schnell zu einem Fachmagazin entwickelte.\n\nHerausgeber war der Konradin Verlag Robert Kohlhammer in Leinfelden-Echterdingen, Chefredakteur war zuletzt Rainer Huttenloher.\n\nDie Computer Zeitung erschien wöchentlich am Montag im Berliner Zeitungsformat. Sie hatte eine verbreitete Auflage von ca. 59.800 Stück (gedruckte Auflage ca. 60.000), von der allein ca. 24.000 Exemplare an die Mitglieder der Gesellschaft für Informatik e. V. versendet wurden. Die Zeitschrift wurde im Zeitungsdruckverfahren hergestellt. Im 1. Quartal 2009 wurden 22.000 Exemplare verkauft.\n\nNach einer Mitteilung des Präsidenten der Gesellschaft für Informatik an ihre Mitglieder hat der Konradin Verlag die Produktion der Computer Zeitung zum 27. Juli 2009 eingestellt.\n\nDie Computer Zeitung wurde bis Oktober 2009 in einer Online-Version weitergeführt. Zum 17. Oktober wurde auch diese eingestellt. Die URL www.computerzeitung.de ist seitdem zum Auftritt der im selben Verlag erscheinenden Zeitschrift Bild der Wissenschaft umgeleitet.\n"}
{"id": "43009", "url": "https://de.wikipedia.org/wiki?curid=43009", "title": "Radiosity (Computergrafik)", "text": "Radiosity (Computergrafik)\n\nRadiosity bzw. Radiosität ist ein Verfahren zur Berechnung der Verteilung von Wärme- oder Lichtstrahlung innerhalb eines virtuellen Modells. In der Bildsynthese ist Radiosity neben auf Raytracing basierenden Algorithmen eines der beiden wichtigen Verfahren zur Berechnung des Lichteinfalls innerhalb einer Szene. Es beruht auf dem Energieerhaltungssatz: Alles Licht, das auf eine Fläche fällt und von dieser nicht absorbiert wird, wird von ihr zurückgeworfen. Außerdem kann eine Fläche auch selbstleuchtend sein.\n\nDas Radiosity-Verfahren basiert auf der Annahme, dass alle Oberflächen ideal diffuse Reflektoren bzw. alle Lichtquellen ideal diffuse Strahler sind. Ideal diffus bedeutet dabei, dass Licht in alle Richtungen gleichmäßig reflektiert bzw. abgestrahlt wird.\n\nIm Gegensatz zu Raytracing ist Radiosity nicht vom Blickpunkt abhängig; die Beleuchtung der Flächen wird also für die gesamte Szene unabhängig von der Position des Betrachters berechnet. Die blickpunktabhängige Verdeckungsberechnung hat in einem unabhängigen Schritt zu erfolgen.\n\n\n\nHistorisch gesehen war Radiosity interessant, da es die Simulation indirekter diffuser Beleuchtung auf einfache Weise erlaubte, was mit Raytracing lange Zeit nicht möglich war. Andererseits war Raytracing gut für spiegelnde und transparente Objekte geeignet, wozu wiederum Radiosity nicht fähig war. Es wurden daher anfangs Vorschläge zur Kombination von Radiosity mit Raytracing gemacht, die jedoch aufwendig waren und sich letztendlich nicht im großen Maße durchsetzen konnten.\n\nMit dem Aufkommen moderner globaler Beleuchtungsverfahren wie Path Tracing und Photon Mapping wurden jedoch die Möglichkeiten von Raytracing beträchtlich erweitert. Weil derartige Algorithmen alle von Radiosity unterstützten Effekte mit weniger Fehlern und auf elegantere Weise simulieren können, ist Radiosity im Bereich der hochwertigen realistischen Bildsynthese weitgehend aus der Mode gekommen. Kommerzielle Verwendung findet Radiosity vor allem beim Rendern von Architekturmodellen, bei denen eine zeitaufwendige Vorausberechnung vertretbar ist. Auch derartige Anwendungen sind jedoch mit Raytracing-basierten Verfahren (Particle Tracing) möglich.\n\nWeitere Anwendung finden Radiosity-Verfahren in den Bereichen der Klima- bzw. Wärmeforschung, da Wärmeverteilung eher diffus als gerichtet geschieht und Radiosity hier praktikabler als strahlenbasierte Ansätze ist.\n\nMit der eingangs formulierten Annahme kann die allgemeine Rendergleichung in die Radiosity-Gleichung überführt werden.\nmit\nDie gesuchte Radiosity im Punkt x ergibt sich aus einem Integral in geschlossener Form, was auf direktem Weg nicht berechnet werden kann. Abhilfe schafft die Diskretisierung der Oberfläche S: anstatt alle infinitesimal kleinen Teilflächen δA' zu betrachten, teilt man die Oberfläche S in zusammenhängende Teilflächen (\"Facetten\" oder \"Patches\" genannt) A auf (Finite-Elemente-Methode). Für diese Teilflächen gelten weitere Annahmen: jedes A ist planar; die jeweilige Radiosity B und der Reflexionsfaktor ρ sind über A konstant. Dies führt dann zur diskreten Radiosity-Gleichung.\nmit\n\nDie Radiosity formula_8 von Teilfläche formula_9 ist also gleich der Eigenstrahlung formula_10 der Teilfläche\nformula_9 plus der mit dem diffusen Reflexionsfaktor formula_12 gewichteten Summe der Radiosity formula_13 von allen anderen Teilflächen formula_14. Wobei hierbei der Formfaktor formula_15, welcher den Anteil an der von Fläche j abgegebenen Energie die auf Fläche i auftrifft festlegt, eingeht. Der Formfaktor berücksichtigt also die Ausrichtung und den Abstand der Teilflächen formula_16 zueinander. Da man dies für alle Teilflächen formula_9 berechnen muss, ergibt sich ein lineares Gleichungssystem mit so vielen Gleichungen und Unbekannten wie es Teilflächen gibt.\n\nIm ersten Schritt erfolgt die Festlegung der Primitiven: Wie und in welche Teilflächen soll eine gegebene kontinuierliche Oberfläche zerlegt werden? Üblich sind Dreieck und Quadrat. Bereits in dieser Phase wird zwischen Qualität und Effizienz entschieden. Je feinmaschiger das Netz, desto genauer sind die Ergebnisse, aber umso aufwendiger die Berechnungen.\n\nIn der Praxis benutzt man meist adaptive Verfahren. Ausgehend von einer z. B. triangulierten Oberfläche, werden nach dem hier angegebenen Schema die zugehörigen Radiosity-Werte aller Facetten bestimmt. Mit diesen Daten erfolgen dann weitere ortsabhängige Netzverfeinerungen. Ausschlaggebend dafür können sein: ein hoher Radiosity-Gradient benachbarter Facetten, Diskontinuitäten im Lichtverlauf (z. B. Lichtfleck) oder eine örtlich ungünstige Netzeinteilung (z. B. T-Knoten).\nDie diskrete Radiosity-Gleichung stellt eine Möglichkeit der Diskretisierung dar. Sie beruht auf der Annahme, dass die Radiosity über einer gegebenen Facette konstant ist, und verwendet daher konstante Basisfunktionen. Die Wahl von Basisfunktionen höheren Grads ist ebenfalls insbesondere mit dem Galerkin-Ansatz möglich.\n\nDer aufwendigste Schritt bei Radiosity ist, unabhängig von dem gewählten Algorithmus, die Berechnung der Formfaktoren. Ein Formfaktor gilt immer zwischen zwei Patches und beschreibt die Menge der ausgetauschten Strahlung, liegt also zwischen null (keine Strahlung wird ausgetauscht) und eins (alle Strahlung wird ausgetauscht).\n\nDer Formfaktor ist rein geometrischer Natur und wird durch die Stellung der Patches zueinander bestimmt. Außerdem spielt die Sichtbarkeit der Patches eine Rolle. Die Sichtbarkeitsberechnung braucht bei weitem die meiste Zeit in der Berechnung.\n\nDie Formel für einen Formfaktor lautet:\nmit\n\nDa das direkte Berechnen dieses Doppelintegrals sehr schwierig ist, werden im Allgemeinen Annäherungen verwendet.\n\nDas simpelste Verfahren ist nur für Flächen korrekt, die relativ klein und relativ weit entfernt sind und zwischen denen keine partielle Verdeckung besteht.\nMan berechnet dabei Winkel und Entfernung nur zwischen zwei repräsentativen Punkten, den Mittelpunkten der beiden Flächen.\nWobei formula_39 der Mittelpunkt von formula_22 und formula_41 der Mittelpunkt von formula_24 ist.\n\n\"(auch „Nusselts Analogon“)\"\n\nEs wird ein repräsentativer Punkt der Empfängerfläche, der Mittelpunkt, ausgewählt. Die sichtbaren Teile der Senderfläche werden auf die Einheitshalbkugel um diesen Punkt projiziert. Dadurch wird formula_43 beachtet. Dann wird die Projektion auf der Einheitskugel wiederum in die Fläche, in der formula_44 liegt, projiziert. Die entstandene Fläche wird durch formula_45 geteilt (Fläche des Einheitskreises). Durch diesen Schritt wird der Formfaktor\nformula_46\nermittelt.\n\nVon Cohen et al. stammt das sogenannte Hemi-Cube-Verfahren. Die Einheitshalbkugel nach Nusselt wird durch einen Einheitshalbwürfel approximiert, dessen Seitenflächen in ein diskretes Gitter unterteilt sind. Jeder Gitterfläche wird ein Gewichtungsfaktor, der \"Delta-Formfaktor\", zugeordnet, welcher von der Position der Gitterfläche abhängig ist. Ein Delta-Formfaktor ist damit der Formfaktor der Gitterfläche nach Nusselt. Die Summe der Delta-Formfaktoren ist 1.\n\nFür jede der fünf Halbwürfelflächen wird mit modifizierten Rasteralgorithmen (üblicherweise Z-Buffer) ein \"Item-Buffer\" berechnet. Dieser enthält für jede Gitterfläche die Identität der Objektfläche (\"Item\", rotes Dreieck im Bild), die darauf projiziert wurde. Für jedes \"Item\" wird die Summe der Delta-Formfaktoren der überdeckten Gitterflächen berechnet (rote Gitterflächen im Bild). Diese Summe wird als Formfaktor zwischen \"Item\" und betrachteter Fläche aufgefasst.\n\nStatt eines Halbwürfels wird nun nur noch eine einzige Fläche verwendet, welche zentriert über dem differentiell kleinen Flächenstück (dA) platziert wird. Diese Fläche wird ebenfalls in kleine, diskrete Bereiche aufgeteilt. Diesen werden dann genau wie beim Hemicube-Verfahren in Abhängigkeit von der Geometrie zu dA, den sogenannten Delta-Formfaktoren zugeordnet. Der Vorteil bei diesem Verfahren ist, dass ein Patch nur auf diese eine Fläche projiziert werden muss und nicht mehr auf fünf Flächen eines Würfels. Außerdem ist dieses Verfahren legitim, da Patches, die orthogonal auf der Fläche von dA stehen, keinen großen Beitrag an der Gesamthelligkeit haben. Diese Beobachtung kann man sich über den Cosinus zwischen den Normalen der beiden Flächenstücke klarmachen.\n\nDie diskrete Radiosity-Gleichung kann als lineares Gleichungssystem aufgefasst werden und lässt sich demzufolge nach einigen Umformschritten wie folgt in Matrixform darstellen.\n\nUm alle gesuchten Radiosity-Werte zu bestimmen, muss das Gleichungssystem gelöst werden. Naheliegend ist die Invertierung der Matrix (I-T) (Matrix-Inversion nach Gauß), was aber aufgrund des enormen Aufwands unpraktikabel ist.\n\nIm Allgemeinen existieren zwei unterschiedliche iterative Lösungsstrategien, die gegen die exakte Lösung konvergieren. Damit ist ein frei wählbarer Kompromiss zwischen Darstellungsqualität und Rechenzeit möglich.\n\n für jede Facette i\n\nDen letzten Schritt bildet das Rendern des fertigen Bildes. Die an den ausgesuchten Stellen berechneten Radiosity-Werte werden gemäß den gewählten Basisfunktionen kombiniert. Wurden Radiosity-Werte in den Netzknoten bestimmt ist z. B. auch Gouraud Shading möglich. Genügt das so erstellte Bild dem Anspruch nicht, so können weitere Iterationen aus (4) folgen. Treten ungewollte grafische Artefakte auf, so sollte mit entsprechenden Änderungen in der Netzstruktur der Algorithmus von vorn gestartet werden.\n\n1984 wurde das Verfahren erstmals von Goral et al. vorgestellt. Es stammt aus der Thermodynamik, wo es verwendet wurde, um den Austausch von Wärmestrahlung zu berechnen (Strahlungsrechnung). Zu dieser Zeit wurde die Full-Form-Factor-Matrix-Methode zum Lösen des Gleichungssystems verwendet. Hier wird das Gleichungssystem für alle Formfaktoren zwischen allen Patches (=Flächen) der Welt aufgestellt und dann mittels eines mathematischen Verfahrens (meist dem Gauß-Seidel-Verfahren) gelöst. Prinzipiell entspricht dieses Vorgehen dem Einsammeln von Radiosity. Für jedes Patch wird berechnet, wie viel Licht es von jedem anderen Patch erhält. Das hat den Nachteil, dass die Berechnung der gesamten Matrix extrem viel Zeit benötigt und viel Speicherplatz belegt, was das Verfahren für komplexe Szenen unbrauchbar macht.\n\n1988 wurde dann das Progressive-Refinement-Verfahren von Cohen et al. vorgestellt. Hier wird der Prozess umgedreht, und das Licht wird nicht mehr an jedem Patch eingesammelt, sondern von jedem Patch verschossen. So kann man zuerst einmal das Licht von den Patches mit dem größten Radiositywert versenden und sich dann denen mit wenig Radiosity zuwenden. Hier ist es nicht mehr nötig, die gesamte Matrix zu berechnen, sondern es werden in jedem Schritt nur noch die Formfaktoren von einem einzelnen Patch zu allen anderen benötigt. Dadurch sinkt der benötigte Speicherplatz enorm, und man erhält nach jedem Schritt ein brauchbares Bild. Je länger man wartet, desto besser wird das Bild, da immer mehr Indirektionen berechnet werden. Um zu konvergieren, braucht dieses Verfahren allerdings genauso lange wie die Full-Form-Factor-Matrix-Methode.\n\nIm Prinzip entspricht das Verfahren einer Reihenentwicklung der Matrix. Das Lineare Gleichungssystem, das sich aus der Strahlungsgleichung ergibt ist dann formula_48, wobei formula_49 die Einheitsmatrix, formula_50 die Matrix, bestehend aus den Formfaktoren und den Reflexionskoeffizienten und formula_21 die Eigenstrahlung beschreibt. Stellt man die Gleichung nun um zu formula_52, so lässt sich der rechte Teil formula_53 als Reihe der folgenden Form entwickeln, und man erhält eine inkrementelle Annäherung an die tatsächliche Strahlungsintensität:\n\n1991 wurde von Hanrahan et al. das Hierarchische Radiosity vorgestellt. Bei diesem Verfahren wird eine Patchhierarchie verwendet. Wenige große Patches bestehen aus vielen kleinen. Der Lichtaustausch wird jetzt auf unterschiedlichen Stufen vorgenommen. Wenn der Fehler gering ist, wird das Licht auf einer hohen Hierarchieebene ausgetauscht, wenn ein großer Fehler zu erwarten ist (zum Beispiel wenn viel Licht ausgetauscht wird oder die Flächen sehr nahe beieinander liegen), dann wird das Licht auf einer niedrigeren Ebene ausgetauscht. Dadurch reduziert sich die Anzahl der zu berechnenden Formfaktoren substantiell, und die Berechnung wird deutlich beschleunigt.\n\nZusätzlich zu diesen Verfahren wurden noch viele Erweiterungen ersonnen. Zum Beispiel gibt es die Methode des Clustering, welches eine Erweiterung des Hierarchischen Radiosity ist. Hier wird oberhalb der Patchhierarchie eine weitere Hierarchie erzeugt, die Cluster. Licht kann dann auch zwischen ganzen Clustergruppen ausgetauscht werden, abhängig vom zu erwartenden Fehler. Wieder spart man sich die Berechnung vieler Formfaktoren.\n\n"}
{"id": "43218", "url": "https://de.wikipedia.org/wiki?curid=43218", "title": "IBook", "text": "IBook\n\niBook ist eine Notebook-Serie des Unternehmens Apple. Die Computer waren von 1999 bis 2006 erhältlich und im Gegensatz zur damaligen PowerBook-Serie eher auf den Consumer-Markt zugeschnitten. Mit der Einführung der MacBooks wurden die iBooks eingestellt.\n\nAm 21. Juli 1999 stellte Steve Jobs das iBook während der Keynote-Präsentation auf der Messe Macworld Conference & Expo in New York vor. Das iBook wurde als preisgünstiges Einsteigernotebook für Schüler, Studenten und Privatanwender konzipiert. Mit der Farbgebung in Orange/Weiß (Tangerine) und Türkisblau/Weiß (Blueberry) und dem kurvigen Design orientierte sich das iBook am kommerziell erfolgreichen Consumer-Desktop-Modell iMac, das Apple aus den roten Zahlen in die Gewinnzone brachte. Mit dem Werbeslogan \"„iMac zum Mitnehmen“\" knüpfte man an diesen Erfolg an.\n\nDas iBook wurde in den USA für 1599 US-$ (in Deutschland 3880 Mark) von Apple auf den Markt gebracht und war damit rund 900 US-$ billiger als das Apple PowerBook für Profianwender, orientierte sich aber bei reduzierter Ausstattung an dessen Leistungsdaten. Aus Kostengründen wurde auf Stereolautsprecher, PC-Card Slot, Infrarotport, eingebautes Mikrofon, SCSI und Audio-in-Anschluss verzichtet. Bei den ersten iBook-Modellreihen fehlten im Vergleich zum PowerBook zudem FireWire-Port und Video-Out, die erst bei den im Jahr 2000 herausgegebenen \"Second-Edition\"-Modellen integriert wurden. Ein serienmäßiges DVD-Laufwerk erhielt nur das 466-MHz-Spitzenmodell dieser letzten Serie.\n\nIm iBook arbeitete wie im iMac ein PowerPC-G3-Chip, auf der linken Gehäuseseite befanden sich in der Standardausstattung USB-, Ethernet- und Modem-Ports sowie ein Lautsprecheranschluss, auf der gegenüberliegenden Seite ein CD-ROM-Laufwerk; auf ein Diskettenlaufwerk wurde verzichtet. Mit dem iBook führte Apple eine Reihe technischer Neuerungen ein: Durch die Unified-Motherboard-Architecture-Spezifikation konnten Motherboardkomponenten standardisiert und die Kosten gesenkt werden. Das iBook hat einen neuen 2×-AGP-Grafikchip des Unternehmens ATI und war das erste Apple-Notebook, in das die gleichzeitig vorgestellte neue AirPort-WLAN-Technologie für drahtlose Netzwerke optional integriert werden konnte. Eine Antenne war bereits im Display verbaut.\n\niBook-Designer Jonathan Ive entwarf das schwungvolle Polycarbonat-Gehäuse mit integriertem Tragegriff, das durch eine farbige Gummierung griffiger gemacht wurde. Das iBook konnte über eine stabile Mechanik dynamisch auf- und zugeklappt werden. Selbst das Netzgerät erhielt ein ausgefallenes rundes Jo-Jo-Design mit integrierter Kabelaufwicklung. Ein durchdachtes Energiemanagement sollten Akkulaufzeiten von bis zu sechs Stunden ermöglichen.\n\nDas iBook war im Vergleich zu den Marktkonkurrenten IBM und Compaq preisgünstiger, hatte eine bessere Grafikkarte und statt des im Consumernotebooksegment üblichen Passivmatrixbildschirms ein TFT-Display mit 800 × 600 Bildpunkten bei geringerem Gewicht.\n\nDie bonbonfarbene Farbgebung und das ungewöhnliche Design entfachten eine hitzige Debatte von Kritikern und Befürwortern. Das Clamshell-iBook erhielt wenig schmeichelhafte Spitznamen (Klodeckel, Barbiehandtasche, Puderdose). Die Ausstattung der ersten iBook-Generation wurde in verschiedenen Modellrevisionen kontinuierlich verbessert. Es gehört durch die robuste Technik und langlebige Komponenten zu den verlässlichsten Notebooks des Herstellers Apple. Das Clamshell-iBook befindet sich in den Sammlungen des Design Museum London und der Yale University Art Gallery.\n\n\n\n\n\nDie Produktion der ersten iBook-Generation wurde im Mai 2001 zugunsten des neuen \"Dual USB\"-iBooks eingestellt.\n\nDie einzigen von Apple vorgesehenen Erweiterungsmöglichkeiten sind die Vergrößerung des Arbeitsspeichers durch einen unter der Tastatur liegenden Steckplatz für 144-polige SODIMM-Module und die Installation einer Airportkarte nach dem 802.11b-Standard. Geräte anderer Anbieter ergänzten über den USB-Port fehlende Funktionen wie einen Mikrofon- und Soundeingang. Ein Wechsel der Festplatte erfordert die komplette Demontage des iBooks, die Controllerhardware lässt den Einbau von ATA-Festplatten bis 120 GB zu. Alle Clamshell-iBook-Modelle können mit entsprechender Arbeitsspeicherausstattung das Betriebssystem Mac OS X Panther nutzen. Auf den FireWire-Modellen kann man auch Mac OS X Tiger installieren.\n\nDie zweite Generation ist dagegen in schlichtem Weiß erschienen und etwa so groß wie ein DIN-A4-Blatt (Abmessungen ca. 28 cm × 23 cm bei 12″, 32,5 cm × 26 cm bei 14″). Die erste Version von 2001 zeichnete sich durch Gehäuseschalen aus farblosem Polycarbonat aus, die innen weiß lackiert wurden und einfallendes Licht zu den Schalenkanten weiterleiten. Damit wurde das gestalterische Thema der „Transluzenz“, das vom iMac G3 und dem iBook G3 „Clamshell“ eingeführt wurde, neu interpretiert. Die nachfolgenden Versionen haben einheitlich weiß durchgefärbte Gehäuseschalen. Der 12″- bzw. 14″-Bildschirm hat eine Auflösung von 1024 × 768 Pixeln. Die erste verwendete G3-CPU hat eine Taktfrequenz von 500 MHz; die letzte, bis September 2003 eingebaute Version ist mit 900 MHz getaktet.\n\nIm Oktober 2003 brachte Apple eine neue Serie von iBooks heraus, die iBook G4 genannt wurden. Der größte Unterschied zum vorherigen Modell bestand im Wechsel des Prozessors. Fortan wurden G4-Prozessoren an Stelle der veralteten G3-Prozessoren verwendet. Außerdem führte Apple auch bei den iBooks-Slot-In-Laufwerke ein. Optional war ab sofort auch integriertes Bluetooth verfügbar. Bis auf die Tastatur wurde am Design so gut wie nichts geändert.\n\nDie iBooks gab es auch weiterhin in zwei verschiedenen Displaygrößen: 12″ und 14″. Das 12″-Modell hatte anfangs einen 800-MHz-PowerPC-G4-Prozessor von Motorola eingebaut, sowie 30 GB Festplatte, 256 MB RAM und ein ComboDrive. Für drahtloses Netzwerk war die AirPort-Extreme-Karte optional und konnte auch nachträglich selbst eingebaut werden. Es gab zwei 14″-Modelle: Einmal mit 933 MHz G4, 40 GB HD und ComboDrive sowie mit 1 GHz G4, 60 GB HD, ComboDrive und mit eingebauter AirPort-Extreme-Karte.\n\nApple brachte im Februar 2004 eine überholte Fassung der iBooks heraus. Weiterhin in drei Modellen hatten das kleine 14″-Modell sowie das 12″-Modell einen 1-GHz-G4-Prozessor; das große 14″-Modell hatte einen 1,2-GHz-Prozessor und eine eingebaute AirPort-Extreme-Karte. Alles andere blieb gleich, bis auf zwei Kleinigkeiten im Inneren, die Apple veränderte: Man erhöhte die Bus-Geschwindigkeit von 100 auf 133 MHz und baute nun einen fest verlöteten 256-MB-RAM-Baustein ein. Für die Speichererweiterung hat das iBook einen weiteren Speicherslot. Vorher war immer ein 128-MB-Speichermodul eingelötet und ein weiteres 128-MB-Modul in den Speicherslot gesteckt, sodass man beim Erhöhen des RAMs einen Baustein wegwerfen musste. Außerdem konnten so nun durch Erweitern des fest eingelöteten 256-MB-Speichermoduls um ein 1-GB-Steckmodul 1280 MB RAM maximal genutzt werden.\n\nAb Oktober 2004 wurde das iBook von Apple in folgenden drei Versionen ausgeliefert: Das 12″-Modell gab es mit einem 1,2-GHz-Prozessor, einem DVD/CD-RW-Laufwerk und einer 30-GB-Festplatte. Das teurere 14″-iBook war mit einem 1,33-GHz-Prozessor ausgestattet und hatte eine 60-GB-Festplatte. Das 14″-Spitzenmodell war ebenfalls einen 1,33-GHz-Prozessor ausgerüstet, hatte aber im Gegensatz zu den anderen iBooks ein SuperDrive-Laufwerk, das auch DVDs beschreiben kann. Alle iBooks wurden mit 256 MB RAM ausgerüstet, die auf bis zu 1,25 GB nachgerüstet werden konnten; eine AirPort-Extreme-Karte wurde nun bei allen Modellen standardmäßig eingebaut.\n\nIm Juni 2005 wurde das iBook in einer überholten Variante ausgeliefert, am 26. Juli 2005 wurde sie im Apple Online-Store verfügbar gemacht. Der G4-Prozessor ist nun mit 1,33 GHz (12″) bzw. 1,42 GHz (14″) getaktet. Der eingebaute Speicher wurde auf 512 MB verdoppelt und in der Minimalkonfiguration eine 40-GB-Festplatte geliefert, optional waren 60 GB. Zudem wurden die Neuerungen aus dem PowerBook, wie etwa der Sudden Motion Sensor, Bluetooth 2.0 das neue Touchpad, eine neue Soundkarte sowie eine 32-MB-ATI-Radeon-9550-Grafikkarte mit Core-Image-Unterstützung eingebaut. Es ist die letzte Variante, die mit einem PowerPC-Prozessor bestückt ist.\n\nMit der Einführung des MacBooks 2006 wurde die iBook-Serie eingestellt. Die MacBooks wurden im Gegensatz zum iBook G4 mit i386-kompatiblen, ab November 2006 dann mit AMD64-kompatiblen Intel-Prozessoren ausgerüstet und sind nur noch mit 13″-TFT-Display erhältlich, und zwar sowohl in weiß als auch in schwarz.\n\nEine Besonderheit der Grafikhardware ist bei den jüngeren iBooks (ab 700 MHz Takt) zu beachten: Obwohl die verwendeten Grafikprozessoren einen Zwei-Bildschirm-Betrieb ermöglichen, wurde diese Funktion von Apple, wahrscheinlich zur vermarktungstechnischen Abgrenzung gegenüber der PowerBook- bzw. MacBook Pro-Serie, auf eine Spiegelung der Bildausgabe reduziert. Seit November 2002 existiert ein Patch (über das beim Starten ausgelesene NVRAM), der es ermöglicht, Auflösungen von bis zu 1920 px × 1440 px auf einem externen Monitor darzustellen. Mit entsprechenden Abstrichen seitens der Prozessorleistung kann das iBook dadurch auch für professionelle Grafikanwendungen genutzt werden.\n\nSchon Ende November 2003 berichteten iBook-Nutzer von einem Bildschirmproblem an ihrem Notebook. Ab einem bestimmten Zeitpunkt dachten einige Benutzer sogar daran, eine Sammelklage gegen Apple zu erheben. Als Antwort auf das Problem initiierte Apple im Januar 2004 das \"iBook Logic Board Repair Extension Program\", das die Reparaturkosten bei betroffenen iBooks für drei Jahre abdeckt, im Wesentlichen eine Garantieerweiterung für die betroffenen Produkte. Gemäß Anwenderberichten und Apple haben vor allem diejenigen Exemplare fehlerhafte Hauptplatinen (logic boards), welche zwischen April und Mai 2003 gefertigt wurden. Ein Austauschprogramm wie beim iBook wurde seitdem auch bei anderen, ähnlichen Apple-Hardwareproblemen angewandt, besonders bei Akkus der iBook-, PowerBook- und MacBook-Baureihen.\n\nAuch noch im August 2006 sind Nutzer von fehlerhaften Hauptplatinen betroffen. Einige von ihnen berichten, dass die Seriennummer ihrer iBooks nicht in dem von Apple genannten Bereich liegt. Andere sind nicht mehr von dem Austauschprogramm abgedeckt, weil der Zeitraum abgelaufen ist. Einige Anwender haben ihre Hauptplatinen (logic boards) mehrfach austauschen lassen, ohne dass der Fehler dauerhaft behoben werden konnte. Die staatliche dänische Verbraucherschutzagentur hat im Mai 2007 bei G4 iBooks einen Produktdesignfehler nachgewiesen, der viele Notebooks nach einiger Zeit unbrauchbar macht und erwartete von Apple, dass Schadensersatzzahlungen an betroffene Kunden geleistet werden. Dieses wurde von Apple am 18. September 2007 akzeptiert.\n\n"}
{"id": "43342", "url": "https://de.wikipedia.org/wiki?curid=43342", "title": "Amiga 1200", "text": "Amiga 1200\n\nBeim Amiga 1200 (kurz: A1200) handelt es sich um einen 32-Bit-Heimcomputer des amerikanischen Technologiekonzerns Commodore International aus West Chester, Pennsylvania, Vereinigte Staaten, der im Oktober 1992 zur Marktreife gelangte. Im unteren Marktsegment sollte der Rechner den erfolglosen Vorgänger Amiga 600 (kurz A600) ablösen.\n\nDer als Tastaturcomputer ausgeführte A1200 ist mit einem auf getakteten Hauptprozessor des Typs Motorola 68EC020, einem Arbeitsspeicher von (Chip-RAM) sowie einem Festspeicher von (ROM) ausgestattet, der das Betriebssystem nebst grafischer Benutzeroberfläche enthält. Das Modell ging aus dem für den Multimedia-Bereich konzipierten High-End-Rechner Amiga 4000 hervor und gehört damit zur dritten und letzten Generation der Amiga-Computer, die mit dem leistungsfähigen AGA-Chipsatz ausgestattet wurde.\n\nDer Markteinführungspreis des A1200 lag in den Vereinigten Staaten bei In Deutschland belief sich die unverbindliche Preisempfehlung des Herstellers zum gleichen Zeitpunkt auf Bis Ende 1993 wurden 95.500 Exemplare des Rechners allein auf dem deutschen Markt verkauft. Nach dem Konkurs des ursprünglichen Herstellers Commodore im April 1994 wurde die Produktion des A1200 zunächst für rund ein Jahr ausgesetzt. Ab Mai 1995 wurde der Rechner dann technisch nahezu unverändert von der deutschen Firma Amiga Technologies (einer Tochterfirma von Escom) neu aufgelegt. Im Zuge der Insolvenz von Escom wurde die Produktion des A1200 im Jahr 1996 schließlich endgültig eingestellt. Die genaue Anzahl der weltweit bis zu diesem Zeitpunkt abgesetzten Einheiten ist jedoch nicht bekannt.\n\nMit dem Ende des A1200 reduzierte sich die Zahl der auf dem Markt noch konkurrenzfähigen Systemplattformen auf zwei, nämlich die marktbeherrschenden IBM-PC- bzw. MS-DOS-Kompatiblen sowie die Rechner der Macintosh-Serie von Apple.\n\nIm Sommer 1991 übernahm der ehemalige IBM-Manager Bill Sydnes auf Betreiben des damaligen Geschäftsführers Mehdi Ali die betriebswirtschaftliche Leitung der Entwicklungsabteilung von Commodore International. Unter Sydnes’ Ägide geriet der im März 1992 als Nachfolger für das Erfolgsmodell Amiga 500 (kurz A500) bzw. den leicht verbesserten Amiga 500 Plus (kurz A500Plus) vorgestellte Amiga 600 zum Flop. Grund hierfür waren schwere Designfehler: So fehlte dem A600 ein numerischer Ziffernblock, die Tastatur war unergonomisch und aufgrund des winzigen Gehäuses sowie fehlender Schnittstellen war der Rechner kaum ausbaufähig. Obendrein lieferte der A600 kaum bessere Leistungsdaten als der A500Plus, da beide Modelle bereits mit dem ECS-Chipsatz ausgestattet waren.\n\nDa Sydnes mit übertriebener Eile bereits die Einstellung der Produktion des zwar veralteten, aber immer noch recht populären A500 sowie des A500Plus veranlasst hatte, um dem A600 keine hausinterne Konkurrenz zu bereiten, stand Commodore plötzlich ohne konkurrenzfähigen Amiga-Heimcomputer da. Im Mai 1992 beauftragte Sydnes Greg Berlin, der u. a. bereits an der Projektierung des Commodore 128 teilgenommen hatte, aber lediglich mit der Entwicklung eines neuen High-End-Rechners, aus dem der Amiga 4000 hervorging. Diese Entscheidung offenbarte Sydnes’ noch aus seiner Zeit bei IBM stammende Blindheit für das untere Marktsegment, aus dem Commodore jedoch traditionell seine Kundschaft rekrutierte. Als das neue High-End-Modell im September 1992 planungsgemäß die Serienreife erlangte, war Sydnes bereits entlassen worden und die Entwicklungsabteilung begann unverzüglich mit der Arbeit am Entwurf einer Heimcomputer-Version des Amiga 4000.\n\nIm Zentrum der Planungen für den neuen Rechner stand die Verwendung des AGA-Chipsatzes. Das Gehäuse mit integriertem 3½-Zoll-Diskettenlaufwerk war vom A500 inspiriert. Nach dem Vorbild des gescheiterten A600 sollte der neue Rechner ebenfalls über eine externe PCMCIA-Schnittstelle sowie einen AT-IDE-Pfostenstecker zum Anschluss einer internen 2½-Zoll-Festplatte verfügen. Außerdem entschied sich das Entwicklerteam für die Verwendung eines relativ leistungsfähigen, aber gleichzeitig kostengünstigen Hauptprozessors, den man im Motorola 68EC020 fand. Um die erweiterten Grafikfähigkeiten des AGA-Chipsatzes voll zur Geltung zu bringen, wurde die als intuitives Betriebssystem dienende grafische Benutzeroberfläche der Vorgängermodelle zum AmigaOS 3.0 weiterentwickelt.\n\nAuf Basis des Amiga 1200 entwickelte Commodore 1993 eine CD-ROM-Spielekonsole namens CD³², die allerdings letztlich erfolglos blieb.\n\nDie im A1200 verwendeten Hauptprozessoren wurden von Motorola produziert. Da Commodore aus einem Mangel an Liquidität die konzerneigene Halbleiterproduktion bei MOS Technology zu diesem Zeitpunkt bereits stark hatte zurückfahren müssen, wurde die Herstellung des AGA-Chipsatzes extern an Hewlett-Packard vergeben. Bis Weihnachten 1992 wurden aber nur ca. 100.000 AGA-Chipsätze ausgeliefert, obwohl rund 200.000 Vorbestellungen für den A1200 vorlagen, die vor allem aus Westeuropa kamen. Die von Commodore mit dem neuen Rechner erwirtschafteten Gewinne hätten also deutlich höher ausfallen können, sofern man es geschafft hätte, eine hinreichende Anzahl an Geräten zur Befriedigung der durchaus starken Nachfrage überhaupt herzustellen. Die langen Lieferzeiten verärgerten viele potenzielle Kunden. So wurde das Weihnachtsgeschäft des Jahres 1992 für Commodore zu einer Enttäuschung, da obendrein der bereits in großen Mengen produzierte A600 nach der Markteinführung der neuen Amiga-Modellreihe mit AGA-Chipsatz als technisch veraltet galt und sich zunehmend zum Ladenhüter entwickelte.\n\nMit der fortschrittlichen AGA-Architektur und dem guten Preis-Leistungs-Verhältnis gelang es dem A1200 jedoch, dem A500 allmählich den Rang abzulaufen. So überstiegen die Verkaufszahlen des neuen Low-End-Modells im Sommer 1993 erstmals die des einstigen Verkaufsschlagers. In Europa schaffte es der A1200 schließlich bis zur Insolvenz von Commodore immerhin, in puncto Marktanteile den Apple Macintosh vom zweiten Platz zu verdrängen, wenngleich es nicht gelang, die zu diesem Zeitpunkt bereits zementierte Marktdominanz der MS-DOS-Kompatiblen anzutasten oder gar zu brechen.\n\nUm den Verkauf des seit dem Konkurs von Commodore im Vorjahr nicht mehr produzierten Rechners anzukurbeln, wurde der A1200 ab Mai 1995 von Escom massiv in allen Filialen der US-amerikanischen Schnellrestaurant-Kette McDonald’s beworben. Escom verkaufte bis Dezember 1995 in Westeuropa allerdings lediglich 20.000 weitere Einheiten des Rechners und blieb damit weit hinter den eigenen Verkaufsprognosen zurück. Eine weitere Quelle berichtet allerdings von immerhin 40.000 bis zu diesem Zeitpunkt abgesetzten Geräten. Ab 1996 wurde der A1200 im Verbund mit zwei Software-Paketen angeboten. Das Amiga-Magic-Paket enthielt neben Anwendersoftware im Wert von mehr als auch zwei Spiele. Das Amiga-Surf-Paket umfasste einen mit einer 260-MB-Festplatte ausgestatteten A1200, ein 14,4-kBd-Modem sowie zusätzliche Software für einen Internetzugang bei einem günstigen Kaufpreis von .\n\nBei aller Fortschrittlichkeit des AGA-Chipsatzes war der Amiga Mitte der 1990er Jahre den damals üblichen MS-DOS-Rechnern mit dem Betriebssystem Windows 3.0, VGA-Grafikkarte und Soundkarte vor allem grafiktechnisch nicht mehr wirklich überlegen, sodass jetzt auch zunehmend anspruchsvolle Spiele auf der viel weiter verbreiteten PC-Plattform erschienen, während nur noch wenig neue Software für den vergleichsweise kleinen Amiga-Markt entwickelt wurde. Dadurch sank vor allem die Attraktivität des Heimcomputermodells A1200. Lediglich auf dem Gebiet der Videobearbeitung konnte sich das High-End-Modell Amiga 4000 noch eine Weile lang behaupten. Es gelang Escom aber nicht mehr, mit den Amiga-Modellen der dritten Generation einen größeren Marktanteil zu erobern.\n\nDie Tastatur des A1200 besitzt 96 Tasten und entspricht weitgehend dem damaligen Industriestandard. Sie ist in vier Bereiche unterteilt: die Haupttastatur, das numerische Tastenfeld, den Cursortastenblock sowie zehn Funktionstasten. Neben den üblichen Sondertasten besitzt der Rechner außerdem zwei Amiga-Tasten, die links bzw. rechts der Leertaste angeordnet sind und insbesondere der Menüauswahl dienen.\n\nDer A1200 besitzt ein rechteckiges Gehäuse aus hellbeigem Kunststoff, das 49 cm × 24,5 cm × 7 cm (Breite × Tiefe × Höhe) misst. Der Rechner ist ab Werk mit einem internen 3½-Zoll-Diskettenlaufwerk mit einer Speicherkapazität von ausgestattet, dessen Öffnung sich auf der rechten Seite befindet. Oberhalb des numerischen Zifferblocks befinden sich drei LED-Kontrollanzeigen, die über den Stromfluss, den Status des Diskettenlaufwerks sowie die Betriebsbereitschaft der optionalen internen Festplatte Auskunft geben. Auf der Oberseite sind außerdem zahlreiche Lüftungsschlitze ins Gehäuse eingelassen. Die Stromversorgung läuft über ein externes Netzteil, das auch den Netzschalter beherbergt.\n\nAuf der Rückseite verfügt der A1200 über zahlreiche Schnittstellen zum Anschluss von Peripheriegeräten. Dazu zählen zwei 9-polige Sub-D-Stecker für Joystick und Maus, ein 25-poliger Sub-D-Stecker als serielle Schnittstelle, eine 25-polige Sub-D-Buchse als parallele Schnittstelle, eine 23-polige Sub-D-Buchse für ein externes Diskettenlaufwerk, ein 23-poliger Sub-D-Stecker für das analoge RGB- bzw. das digitale RGBI-Videosignal, zwei Cinch-Buchsen für Composite-Farbvideo, eine Antennenbuchse am HF-Modulator zum Anschluss eines handelsüblichen Farbfernsehers sowie zwei Cinch-Buchsen für das Stereo-Audiosignal. Auf der linken Gehäuseseite befindet sich außerdem ein 68-poliger Standard-Steckplatz für eine 16-Bit-PCMCIA-Speicherkarte. Da die Standardisierung dieser Schnittstelle zum Zeitpunkt der Fertigungsaufnahme noch nicht abgeschlossen war, ist die verbaute Schnittstelle nicht vollständig kompatibel zum endgültigen PCMCIA-Standard.\n\nAuf der Unterseite besitzt der Rechner schließlich einen CPU-Erweiterungssteckplatz, die sog. \"trapdoor\" (dt. „Falltür“) mit zwei internen Schnittstellen, die durch eine Abdeckung vor Staub geschützt sind. Dabei handelt es sich um einen 44-poligen AT-IDE-Pfostenstecker zum Betrieb von internen IDE-Festplatten sowie um einen 150-poligen lokalen Prozessorbus zum Einstecken von Turbokarten mit zusätzlicher CPU.\n\nIm A1200 dient ein Motorola 68EC020 mit einer Taktfrequenz von als Hauptprozessor. Diese CPU basiert auf dem Motorola 68020, der 1984 die Serienreife erreichte und als erster echter 32-Bit-Prozessor der Motorola-68000er-Familie gilt. Genau wie dieser verfügt auch der Motorola 68EC020 über einen 32-Bit-Datenbus und einen entsprechenden Registersatz, unterscheidet sich aber durch seine Adressbusstrukturen vom ursprünglichen Motorola 68020. Der Adressbus der im A1200 verbauten Variante weist nämlich eine Wortbreite von lediglich 24 Bit auf, womit in der Vollausbaustufe immerhin theoretisch ein Adressraum von RAM ansteuerbar ist, die dem System per Speichererweiterungskarte hinzugefügt werden können. Von diesen RAM können bis zu als Chip-RAM und der Rest als Fast-RAM bzw. als I/O-Adressraum eingerichtet werden. Alternativ können über den CPU-Erweiterungssteckplatz Turbokarten mit vollwertigen 32-Bit-Hauptprozessoren nachgerüstet werden, um die Arbeitsgeschwindigkeit des A1200 unter Umgehung des Hauptprozessors auf das Niveau des Amiga 4000 (oder darüber hinaus) zu erhöhen. Besonders verbreitet waren Turbokarten wie die \"Blizzard III\", die mit einem auf getakteten Motorola 68030, Fast-RAM-Sockeln sowie einem parallelen SCSI-Controller ausgestattet sind.\n\nZwecks Entlastung des Hauptprozessors ist der A1200 mit einem System aus mehreren Koprozessoren ausgestattet, die für Videosignal, Tonausgabe, Ein- und Ausgabeoperationen sowie Speicherverwaltung verantwortlich sind. In ihrer Gesamtheit werden diese in ihrer Funktionsweise eng aufeinander abgestimmten Koprozessoren als Advanced Graphics Architecture (kurz: AGA-Chipsatz) bezeichnet, die ursprünglich für den Amiga 4000 entwickelt worden war. In Deutschland war die AGA-Architektur zur damaligen Zeit unter der Bezeichnung „AA-Chipsatz“ bekannt, um Verwechslungen mit der gleichnamigen AGA-Grafikkarte der MS-DOS-kompatiblen Commodore-PC-Reihe zu vermeiden, die genau wie der Amiga 1000 im Jahr 1985 eingeführt worden war.\n\nDie gesteigerte Leistungsfähigkeit der AGA-Architektur beruht auf verbesserten Custom-Chips sowie der durchgehenden Verwendung von 32-Bit-Datenbusstrukturen. Ihr Schwerpunkt besteht in der Verbesserung der Grafikfähigkeiten des A1200 im Vergleich zu den Vorgängermodellen aus der zweiten Generation der Amiga-Serie. Technikgeschichtlich geht die Designphilosophie der AGA-Architektur noch auf die Ende der 1970er Jahre unter der Führung von Jay Miner entwickelte Spielekonsole Atari 2600 sowie die 8-Bit-Atari-Heimcomputer zurück.\n\nDer immerhin seit sieben Jahren nur geringfügig veränderte Grafikprozessor Denise wurde durch den weiterentwickelten und wesentlich leistungsfähigeren Grafikchip Lisa ersetzt. So besitzt Lisa auf gleichem Raum 300.000 Transistoren mehr als ihre Vorgängerin. Prinzipiell blieb es aber bei der für die Amiga-Reihe typischen Verwendung von Bitplanes, von denen im neuen HAM8-Modus statt der bisherigen sechs nunmehr bis zu acht gleichzeitig bei der Erzeugung des RGB-Videosignals übereinander gelegt werden können. Mit ihrem 24-Bit-Farbraum ist Lisa in der Lage, 256 Farben gleichzeitig aus einer Palette von 16.777.216 Möglichkeiten in jeder Auflösung auf den Bildschirm zu bringen. Im Betrieb mit den in Europa üblichen PAL-Monitoren arbeitet Lisa mit sechs verschiedenen, vom Betriebssystem unterstützten Standard-Grafikmodi, die sich in puncto Auflösung und Bildwiederholfrequenz voneinander unterscheiden: \"SuperHiRes\" (1280 × 256), \"SuperHiRes Interlace\" (1280 × 512), \"HiRes\" (640 × 256), \"HiRes Interlace\" (640 × 512), \"LowRes\" (320 × 256) sowie \"LowRes Interlace\" (320 × 512). Theoretisch sind horizontale Auflösungen von mehr als 1.200 Pixeln möglich und im erwähnten HAM8-Modus können sogar fast so viele Farben gleichzeitig zur Darstellung gebracht werden, wie es Bildschirmpunkte gibt, was bei einer maximalen sichtbaren Auflösung von \"SuperHiRes Interlace Overscan\" (1504 × 576) einer Farbpalette von 866.304 Farben entspricht und damit praktisch Echtfarben sehr nahe kommt.\n\nDer ursprüngliche Custom-Chip Agnus und seine Nachfolger (unter ihnen etwa der Fat Agnus) wurden durch die deutlich verbesserte Alice ersetzt, die gezielt auf die neuen 32-Bit-CPUs der Typen 68020 und höher von Motorola zugeschnitten ist. Die offiziell als MOS Technology 8374 bezeichnete Alice koordiniert mit Hilfe eines eingebauten Adressgenerators mit integrierter DMA-Logik die nicht gleichzeitig möglichen Zugriffe von CPU, Grafikprozessor Lisa und Soundchip Paula auf den ab Werk eingebauten Arbeitsspeicher von Chip-RAM. Alice hilft außerdem mit zwei internen Koprozessoren (Blitter und Copper) beim Bildaufbau und ist überdies in der Lage, Speicherinhalte mit hoher Geschwindigkeit zu verschieben.\n\nUnverändert blieb dagegen der mit vier 8-Bit-Stereo-Kanälen ausgestattete digitale Soundchip Paula, dessen Design seit der Einführung der Amiga-Serie im Jahr 1985 keine nennenswerten Veränderungen erfahren hatte. Jeder Soundkanal verfügt über einen eigenen Digital-Analog-Umsetzer (DAC; engl. für \"Digital-to-Analog Converter\"). Um Töne oder Geräusche zu erzeugen, müssen die digitalen 8-Bit-Sounddateien erst von der CPU in den Arbeitsspeicher eingeschrieben werden, bevor sie von Alice Byte für Byte an Paula gesendet und über die vier DACs dann schließlich an die externen Analog-Lautsprecher übermittelt werden können.\n\nNeben der Klangerzeugung dient Paula auch als I/O-Baustein und ist für die Ansteuerung der Diskettenlaufwerke sowie der über die serielle Schnittstelle angeschlossenen Peripheriegeräte zuständig.\n\nDer A1200 besitzt einen Arbeitsspeicher von Chip-RAM, die auf vier dynamische 32-Bit-CMOS-RAM-Chips des Typs 424260-80 mit einer Speicherkapazität von jeweils verteilt sind. Der Hauptprozessor kann nur über den Koprozessor Alice auf das Chip-RAM zugreifen. Über Fast-RAM, das ausschließlich der CPU zur Verfügung steht und somit kürzere Zugriffszyklen erlaubt, verfügt der Rechner nicht von Haus aus. Es kann im Bedarfsfall aber nachgerüstet werden.\n\nDas Betriebssystem ist in zwei 16-Bit-ROM-Chips untergebracht, die jeweils ein Speichervolumen von besitzen und zusammen eine Wortbreite von 32 Bit erreichen. Insgesamt belegt das Betriebssystem damit ROM.\n\nDen Kern der Rechnerarchitektur des A1200 bildet der Systembus, der für den Austausch von Daten zwischen den Systemkomponenten genutzt wird. Der Systembus verfügt über einen durchgehenden 32-Bit-Datenbus sowie einen 24-Bit-Adressbus. Die Steuerung des Systembusses und die Koordination von Buszugriffen seitens des Hauptprozessors bzw. der Koprozessoren des AGA-Chipsatzes besorgen die beiden Spezialchips Gayle und Budgie.\n\nBei Gayle handelt es sich um ein multifunktionales Gate-Array, das aus verschiedenen Komponenten besteht, die über Glue Logic miteinander verbunden sind. Gayle übernimmt die Aufgabe der Adressdecodierung und unterstützt ausschließlich synchrone Operationen der CPU. Überdies fungiert sie als IDE-Controller für die interne Festplatte und ist für die Steuerung der PCMCIA-Schnittstelle verantwortlich.\n\nAuch bei Budgie handelt es sich um einen Mehrzweckbaustein mit einem breiten Spektrum an Bussteuerungs- und Logikfunktionen. Die Hauptaufgabe besteht jedoch im Busmanagement, weshalb sie meist als Buscontroller gehandelt wird. So verfügt Budgie über einen Puffer zum Zwischenspeichern von Bildinformationen und kontrolliert die Datenflussrichtungen auf dem Datenbus. Außerdem erfüllt Budgie noch eine Reihe von Logikfunktionen, wie etwa die der Taktgenerierung.\n\nDer A1200 war bis April 1994 in zwei Modellvarianten erhältlich. Neben der Grundversion brachte Commodore ein Modell heraus, das ab Werk neben dem ins Gehäuse integrierten 3½-Zoll-Diskettenlaufwerk mit Speicherkapazität bei doppelter Aufzeichnungsdichte (Double Density) zusätzlich mit einer internen 2½-Zoll-Festplatte mit einer Speicherkapazität von wahlweise 20 oder ausgestattet ist. Diese Modellvariante war unter der Bezeichnung „Amiga 1200HD“ (kurz A1200HD) bekannt, wobei das Anhängsel „HD“ für \"hard disk\" steht, also dem englischen Wort für „Festplatte“.\n\nDie von der Escom-Tochter Amiga Technologies ab Mai 1995 auf den Markt gebrachten Modelle behielten die schon von Commodore eingeführten Modellbezeichnungen. Technisch unterscheiden sie sich kaum von den ursprünglichen Modellvarianten, da eine technische Überarbeitung des Rechners aufgrund fehlender oder unvollständiger Dokumentation seitens Commodore praktisch unmöglich war. Beispielsweise musste der Soundchip Paula erst mühevoll Schicht für Schicht abgefräst werden, um seine Funktionsweise zu klären. Das Gehäuse besteht allerdings aus einem anderen Kunststoff.\n\nDas Typenlogo wurde an das Firmenlogo von Amiga Technologies angepasst und nach dem Vorbild des weiß-rot gemusterten Balles aus der berühmten Boing-Demo von der \"Winter Consumer Electronics Show\" des Jahres 1984 entsprechend neu gestaltet.\n\nDarüber hinaus musste das 3½-Zoll-Diskettenlaufwerk eines anderen Drittanbieters verbaut werden, da das ursprünglich verwendete Modell mittlerweile nicht mehr hergestellt wurde. Obwohl es recht teuer war, fiel die Wahl dabei auf ein an sich bereits veraltetes DD-Laufwerk von Mitsumi, da der Soundchip Paula nicht mit den neueren HD-Laufwerken synchronisiert werden konnte. Die Verwendung des Mitsumi-Modells führt zu Einschränkungen bei der Kompatibilität, insbesondere im Falle von Computerspielen, die noch für die Commodore-Version des A1200 geschrieben worden waren und unter Umgehung des Betriebssystems direkt auf die Hardware des internen Laufwerks zugreifen.\n\nDie interne Festplatte der Escom-Version des A1200HD besitzt außerdem eine gegenüber der Commodore-Variante deutlich erhöhte Speicherkapazität von .\n\nWie in den anderen Modellen der Amiga-Reihe kommt auch im A1200 das modular aufgebaute Betriebssystem AmigaOS zum Einsatz. Das AmigaOS ist von Haus aus multitaskingfähig und besteht aus der als grafische Benutzeroberfläche (GUI) dienenden Amiga Workbench sowie dem Kickstart, das diejenigen Komponenten des Betriebssystems enthält, die fest in die ROM-Speicherchips des Rechners eingebrannt sind. Dazu zählen der Betriebssystemkern \"Exec\", Teile des AmigaDOS, der Kommandozeileninterpreter \"Shell\" sowie die zentralen Systembibliotheken für das GUI. Zusätzlich müssen bei Inbetriebnahme weitere Systemdateien von einer Boot-Diskette oder einer Festplatte geladen werden.\n\nUm die Grafikfähigkeiten des AGA-Chipsatzes zur vollen Entfaltung zu bringen, wurde für die Amiga-Modelle der dritten Generation eine neue Version des Betriebssystems AmigaOS entwickelt, das AmigaOS 3.0. Es löste die Versionen AmigaOS 2.04 für den A500Plus sowie AmigaOS 2.1 für den A600 sowie den Amiga 3000 ab.\n\nZu den grafischen Verbesserungen zählen neue Mauspfeil-Bedienelemente, eine Menüleiste in 3D-Optik sowie Menüfenster in schwarz-weißer Farbgebung. Auch das Bootmenü wurde überarbeitet und enthält nun umfangreiche Möglichkeiten zur System-Fehlerdiagnose. Ein weiteres Charakteristikum des AmigaOS 3.0 besteht in der Einführung von \"Multiview\" mit einem eigenen Hypertextformat namens AmigaGuide, das vor allem für Hilfstexte zur Erleichterung der Bedienung gedacht war. Überdies gestattet das überarbeitete Betriebssystem erstmals die Wahl eigener Hintergrundbilder statt der bis dahin üblichen Hintergrundmuster.\n\n1994 erschien mit dem AmigaOS 3.1 eine geringfügig verbesserte Version des AmigaOS 3.0 für die Amiga-Modelle 4000 und 1200 sowie die Spielekonsole CD³², die neben einer erweiterten Programmbibliothek für Grafikkarten-Gerätetreiber namens \"ReTargetable Graphics\" (RTG) mit 24-Bit-Farbtiefe zusätzliche Dateitypen zur Verfügung stellt. Außerdem gestattet diese Version des Betriebssystem das Betreiben eines CD-ROM-Laufwerks. Das AmigaOS 3.1 stellt die letzte offizielle Version des Betriebssystems für klassische Amiga-Rechner dar. Später diente das AmigaOS 3.1 auch als Update für die genannten älteren Amiga-Modelle (A500, A600, A2000 und A3000).\n\nIn Europa wurde der A1200 mit Begeisterung aufgenommen. In den Testberichten der gängigen Computerzeitschriften fielen die Urteile vor allem im Hinblick auf die Leistungsfähigkeit der AGA-Architektur weitgehend positiv aus und die dritte Generation der Amiga-Reihe wurde im Gegensatz zur Vorgängergeneration als echte Innovation wahrgenommen. Lob erfuhren neben der durchgehenden 32-Bit-Architektur des neuen Rechners, die ihn in Kombination mit der neuen CPU drei- bis fünfmal schneller machten als vorherige Amiga-Modelle, vor allem die als herausragend empfundenen Grafikfähigkeiten des A1200. Kritik riefen etwa die Verwendung des dem Motorola 68030 unterlegenen Hauptprozessors Motorola 68EC020 sowie die stagnierenden Soundfähigkeiten des A1200 hervor.\n\nAuch aus technikgeschichtlicher Perspektive erfährt der A1200 eine positive Bewertung. So wird der Rechner zu den wenigen Modellen gezählt, bei denen der Hersteller Commodore International „fast alles richtig gemacht hatte.“ Der letzte Amiga-Heimcomputer habe der vom Konkurs bedrohten Herstellerfirma neue Impulse gegeben und sei aufgrund seiner hohen Leistungsfähigkeit bei vergleichsweise günstigen Kaufpreisen bei der Kundschaft „äußerst beliebt“ gewesen.\n\nUm den A1200 und andere Amiga-Modelle hat sich in den letzten Jahren eine lebendige Retrocomputing-Szene herausgebildet, die neben neuen Demos und Programmen auch neue Hardware-Erweiterungen für den Rechner produziert. Außerdem ist der Rechner auf technikhistorischen Webseiten sowie in Computermuseen regelmäßig als Exponat vertreten und besitzt somit einen festen Platz im kollektiven Gedächtnis. Überdies sind von Drittanbietern im Laufe der letzten knapp zwei Jahrzehnte Neuauflagen des Betriebssystems AmigaOS wie etwa die Versionen 3.5, 3.9, 4.0 (PowerPC) oder 4.1 (PowerPC) entstanden, die gegenüber den klassischen Versionen AmigaOS 3.0 und 3.1 über eine erheblich erweiterte Funktionalität verfügen. AmigaOS 3.5 und 3.9 laufen auf allen Amiga-Modellen, nicht nur dem A1200.\n\nFür Bastler und Retrocomputing-Begeisterte besonders interessant ist der interne Uhrenport des A1200. Ursprünglich für eine batteriebetriebene Echtzeituhr oder eine Speichererweiterung gedacht, schlummerte in dieser Schnittstelle lange ungeahntes Potenzial, welches heute etwa für diverse Controller, ISDN-Verbindungen, USB-Schnittstellen usw. genutzt wird. Aufgrund des fortgeschrittenen Alters des A1200 und der damit einhergehenden Materialermüdung ist mittlerweile ein Austausch der Elektrolytkondensatoren empfehlenswert, da diese aufgrund von Mängeln bei der Produktion eine Tendenz zum Auslaufen haben und der Hauptplatine sowie den empfindlichen elektronischen Bausteinen der noch im Betrieb befindlichen Modelle schwere Schäden zufügen können.\n\n\n\nAllgemeines\n\nSystemarchitektur\n"}
{"id": "43383", "url": "https://de.wikipedia.org/wiki?curid=43383", "title": "JavaStation", "text": "JavaStation\n\nDie JavaStation ist ein Network Computer (NC), der zwischen 1996 und 2000 von Sun Microsystems hergestellt wurde.\n\nDie Hardware basiert auf dem Design der Sun SPARCstation, einer sehr erfolgreichen Computer-Architektur von UNIX Workstations aus den 1980er Jahren. NC-typisch fehlen Computerkomponenten wie Disketten-, Festplatten- und CD-ROM-Laufwerke. Es sind vorrangig PC-Standardkomponenten eingesetzt: Anschlüsse für Tastatur und Maus (PS/2), Anschluss für Monitor mit VGA, Verwendung von SIMM/DIMM RAM.\n\nAls Betriebssystem wurde von Sun das eigene JavaOS geliefert, das im Wesentlichen die virtuelle Maschine (VM) für Suns Java zur Verfügung stellt. Darauf aufsetzend können Java-Programme laufen, zum Beispiel der HotJava-Browser. Es lässt sich aber auch Linux auf der JavaStation nutzen.\n\n\n"}
{"id": "43428", "url": "https://de.wikipedia.org/wiki?curid=43428", "title": "Computersimulation", "text": "Computersimulation\n\nUnter Computersimulation bzw. Rechnersimulation versteht man die Durchführung einer Simulation mit Hilfe eines Computers, genauer: eines Computerprogrammes. Dieses Programm beschreibt bzw. definiert das Simulationsmodell. \n\nZu den ersten Computersimulationen zählen eine Simulationen eines zweidimensionalen Harte-Kugel-Modells mittels des Metropolisalgorithmus und das Fermi-Pasta-Ulam-Experiment.\n\nIn der statischen Simulation spielt die Zeit keine Rolle. Das Modell ist statisch, d. h., es betrachtet nur einen Zeitpunkt, ist also quasi eine Momentaufnahme.\n\nFußt die Simulation auf Zufallszahlen und/oder Stochastik (Wahrscheinlichkeitsmathematik), so spricht man wegen der begrifflichen Nähe zum Glücksspiel von Monte-Carlo-Simulation. Diese Methode hat besonders in der Physik wichtige Anwendungen gefunden, und zwei Bücher des Physikers Kurt Binder gehören zu den meistzitierten Veröffentlichungen in dieser Wissenschaftssparte.\n\nFür die Modelle der dynamischen Simulation spielt die Zeit immer eine wesentliche Rolle. Die dynamische Simulation betrachtet Prozesse bzw. Abläufe.\n\nBei der kontinuierlichen Simulation werden stetige Prozesse abgebildet. Diese Art der Simulation nutzt Differentialgleichungen zur Darstellung physikalischer oder biologischer Gesetzmäßigkeiten, welche dem zu simulierenden Prozess zugrunde liegen.\n\nDie diskrete Simulation benutzt die Zeit, um nach statistisch oder zufällig bemessenen Zeitintervallen bestimmte Ereignisse hervorzurufen, welche ihrerseits den (nächsten) Systemzustand bestimmen.\n\nAuch als Ablaufsimulation oder ereignisgesteuerte Simulation bezeichnet, findet die diskrete Simulation im Produktions- und logistischen Bereich ihre hauptsächliche Anwendung. Der weit überwiegende Teil der Praxisprobleme liegt in diesem Bereich. Die Modelle dieser Simulation sind im Gegensatz zu den kontinuierlichen gut mit standardisierten Elementen (z. B. Zufallszahlen, Warteschlangen, Wahrscheinlichkeitsverteilungen usw.) darstellbar. Einen weiteren leistungsfähigen Ansatz zur Entwicklung diskreter, ereignisgesteuerter Modelle bietet die Petri-Netz-Theorie.\n\nDie Stärke der diskreten Simulation liegt darin, dass sie den \"Zufall\" bzw. die \"Wahrscheinlichkeit\" in das Modell mit einbezieht und bei genügend häufiger Durchrechnung eine Aussage über die zu erwartende Wahrscheinlichkeit der verschiedenen Systemzustände liefert. Das Anwendungsfeld für diese Art der Simulation ist daher entsprechend groß:\n\nVon \"hybrider Simulation\" spricht man dann, wenn das Modell sowohl Eigenschaften der kontinuierlichen als auch der diskreten Simulation aufweist. Derartige Modelle finden sich beispielsweise in medizinischen Simulationen – insbesondere zu Ausbildungszwecken – wieder, bei denen die zu simulierende Biologie nicht hinreichend bekannt ist, um ein ausreichend detailliertes, kontinuierliches Modell erstellen zu können.\n\nUnter Systemdynamik wird die Simulation\nSysteme verstanden. Im Wesentlichen werden unter solchen Simulatoren\nsubsumiert. Die Arbeitsweisen und Werkzeuge entsprechen nahezu zur Gänze denen der Regelungstechnik bzw. der Kybernetik.\n\nDie Multi-Agenten-Simulation, die als Spezialfall der diskreten Simulation gesehen werden kann, erlaubt, emergente Phänomene und dynamische Wechselwirkungen zu modellieren.\n\nObwohl ein Simulationsprogramm (Simulator) prinzipiell mit jeder allgemeinen Programmiersprache – in einfachen Fällen sogar mit Standardwerkzeugen wie z. B. einer Tabellenkalkulation – erstellt werden kann, wurden seit den 1960er-Jahren – nach der erstmaligen Verfügbarkeit hinreichend schneller Rechner – auch besondere Simulationssprachen entwickelt.\n\nZunächst beschränkten sich diese Sprachen noch auf die rein mathematische bzw. numerische Ermittlung und Darstellung der Simulationsverläufe und -ergebnisse. Mit dem Aufkommen immer leistungsfähiger PCs in den 1980er-Jahren trat jedoch mehr und mehr die graphische Repräsentation und in jüngerer Zeit auch die Animation hinzu.\n\nIn der diskreten Simulation gibt es derzeit Bestrebungen zur Implementierung optimierender Verfahren, wie z. B. Künstliche neuronale Netze, Genetische Algorithmen oder Fuzzy Logic. Diese Komponenten sollen den klassischen Simulatoren, welche an sich nicht optimierend wirken, die Eigenschaft der selbständigen Suche nach optimalen Lösungen hinzufügen.\n\nUnter dem Begriff „Digitale Fabrik“ versuchen große Unternehmen – besonders des Fahrzeug- und Flugzeugbaues – die (vorwiegend animierte) Ablaufsimulation mit Verfahren zur Kostenermittlung, zur automatisierten Erstellung technischer Dokumentation und Planungssystemen für Produktionsstätten und -anlagen zu koppeln, um so Entwicklungszeiten und -kosten sowie Qualitätsprüfungs- und Wartungsaufwendungen zu minimieren.\n\n\n"}
{"id": "43624", "url": "https://de.wikipedia.org/wiki?curid=43624", "title": "XFree86", "text": "XFree86\n\nXFree86 ist eine freie quelloffene Realisierung der Grafikschnittstelle X Window System (auch \"X11\" genannt). Sie stellt auf Unix und unixoiden Betriebssystemen (Linux, GNU Hurd, BSD und seinen Ableitungen) grundlegende Grafikfunktionen zur Verfügung. In der modularen Systemarchitektur eines grafischen Betriebssystems ist sie als „Dienstprogramm“ zwischen den gerätespezifischen Treibern und der grafischen Benutzeroberfläche (wie etwa KDE oder Gnome) angesiedelt und stellt dieser alle grundlegenden Grafikfunktionen wie etwa das Zeichnen von Rahmen und Fenstern zur Verfügung, die die Benutzeroberfläche oder die grafischen Anwendungsprogramme benötigen. Bis 2004 war dieser Dienst eine auf fast jedem Linux- oder BSD-System anzutreffende Software.\n\n\"XFree86\" ist nicht auf bestimmte Betriebssysteme beschränkt. Beispielsweise ist es auch Teil von Cygwin, einer Systemumgebung für NT-basierende Windowssysteme (NT bis Windows 10), die eine einfache Portierung von Linuxprogrammen auf Windows ermöglicht. Auch für OS/2 und macOS gibt es Portierungen, obwohl Apple bis 2012 eigene X11-Server zur Verfügung stellte (siehe XQuartz).\n\n\"XFree86\" wird seit 2008 nicht mehr weiter entwickelt. Hauptnachfolger ist der X.Org-Server.\n\nDer \"XFree86\"-Server kommuniziert mit dem Kernel (meistens ein Linux-, BSD- oder UNIX-Kernel), um Ein- und Ausgabegeräte anzusteuern, teilweise greift er aber auch selbst auf Tastaturen und Mäuse zu. Eine grundlegende Ausnahme bilden die Grafikkarten. Diese werden von \"XFree86\" direkt unter Umgehung des Kernels angesprochen. Für die weitaus meisten Grafikkarten der letzten 15 Jahre bringt \"XFree86\" eigene Treiber mit. Für viele verbreitete Karten gibt es Treiber des Herstellers. Es ist aber auch möglich, \"XFree86\" im „Linux-Framebuffer“ (einen Grafikbereich im Kernel) arbeiten zu lassen, um einen Gerätetreiber für Linux verwenden zu können.\n\nAuf einem typischen POSIX-System liegen die \"XFree86\"-Konfigurationsdateien im Verzeichnis \"/etc/X11\". Die grundlegende Konfiguration erfolgt in der Datei \"XF86Config\" oder \"XF86Config-4\", die unter anderem Einstellungen zum verwendeten Monitor, Tastatur, Maus und Grafikkarte enthält. Für weniger erfahrene Benutzer existieren Programme (auch grafische), die die Konfiguration erleichtern. Moderne Distributionen bieten auch eine halbautomatische Erkennung sinnvoller Einstellungen.\n\nDas Projekt wurde 1991 von David Wexelblat, Glenn Lai, David Dawes und Jim Tsillas begonnen, die gemeinsam darangingen, Fehler im Quellcode von X11 X386 (geschrieben von Thomas Röll) zu beheben. Diese Version hieß ursprünglich \"X386 1.2e\". Da neuere Versionen von X386 nur noch kommerziell verkauft wurden, entwickelte sich das Projekt fortan selbständig und wurde in XFree86 umbenannt, was ein Wortspiel ist (aus \"X-three-86\" wurde \"X-free-86\").\n\nMit Stand 1. Oktober 2001 unterstützte XFree86 die X11-Spezifikation R6.5.1 inklusive der GLX- und der X-Rendering-Erweiterungen.\n\nIm Jahr 2003 wurde Keith Packard, ein anerkannter X-Window-System-Entwickler, aus dem Kernteam ausgeschlossen. Ihm wurde eine Verschwörung vorgeworfen: Keith Packard habe versucht, das XFree86-Projekt zu spalten. Er arbeite weiter innerhalb des Projekts, versuche aber unter der Hand, andere Entwickler für ein neues, von ihm initiiertes X-Server-Projekt zu gewinnen. Packard bestritt dies; anhand einer E-Mail-Korrespondenz konnte jedoch nachgewiesen werden, dass er tatsächlich mit anderen Entwicklern über eine Spaltung diskutiert hatte. Packard selbst zog es vor, zu den Vorgängen nichts mehr zu sagen. Dies führte zur Schaffung von XWin, einem Forum zur Verbesserung von X und speziell XFree86, das später vollständig in freedesktop.org aufging. Keith Packard begann, basierend auf dem X-Window-System und in Kooperation mit freedesktop.org, ein ganz neues Entwicklungsprojekt mit Namen \"Xserver\". Xserver benutzt Kdrives Treiber-API-Modell. Die Autoren beschreiben das Projekt gerne als die nächste X-Server-Generation, die einer anderen Richtung als XFree86 folgt.\n\nSpäter stellte aber das XFree86-Kernteam fest, dass nur eine begrenzte Innovationskraft vorherrschte. Unter anderem wurde dies an der Struktur des Projekts festgemacht: Die Mitglieder des Kernteams wurden anhand ihrer Beiträge bewusst ausgewählt und blieben so ein enger, abgeschlossener Kreis. Wegen der kaum fortschreitenden Entwicklung von X beschloss das XFree86-Kernteam daraufhin am 30. Dezember 2003, sich am nächsten Tag selbst aufzulösen.\n\nNach der Auflösung des Kernteams schlossen sich viele Entwickler mit der alten X.Org Foundation zusammen, um diese in ein Open-Source-Projekt umzuwandeln, das zukünftig eine Implementierung eines freien X-Servers gewährleisten sollte. In Zusammenarbeit mit freedesktop.org stellt das neue Entwicklerteam sein Projekt unter dem Namen X.Org-Server zur Verfügung, der auf einer Weiterentwicklung von XFree86 4.4 RC2 beruht.\n\nMassive Unterstützung bekam das neue Projekt, als sich im Januar 2004 das XFree86-Projekt für eine neue Lizenz entschied, die eine besondere Werbe-Klausel mit sich brachte. Kritiker warfen ein, dass XFree86 damit zu nicht-freier Software werde. In jedem Fall war die Lizenzänderung problematisch. Viele Distributoren stuften diese Lizenz als inkompatibel zur GPL ein und unterstützten fortan die Abspaltung, die auf der XFree86-Version 4.4 RC2 basierte und in der alten Lizenz weiterentwickelt wurde.\n\nDie Abspaltung entwickelt seitdem unter dem Dach der X.Org Foundation ihren eigenen X-Server, den X.Org-Server.\n\nNeben diesem Projekt gab es zeitweise einen experimentellen Zweig des XFree86-Codes mit Namen \"Xouvert\", dessen Entwicklung jedoch bis auf weiteres eingestellt wurde.\n\nNachdem der Großteil der Entwickler zu X.Org wechselte, gab es nur noch geringen Fortschritt bei XFree86. Die letzte Version (4.8.0) erschien am 15. Dezember 2008, der letzte Codebeitrag wurde im Februar 2009 eingepflegt. Auf Nachfrage erklärte der letzte aktive Entwickler Marc Aurele La France, dass die Arbeit am Projekt mittlerweile eingestellt wurde.\n\n"}
{"id": "44014", "url": "https://de.wikipedia.org/wiki?curid=44014", "title": "ChessBase", "text": "ChessBase\n\nChessBase ist ein Unternehmen mit Sitz in Hamburg, das Schachsoftware entwickelt und vertreibt sowie eine Schachdatenbank und eine Schachnachrichtenseite betreibt. \n\nChessBase wurde 1987 von dem Physiker Matthias Wüllenweber und dem Wissenschaftsjournalisten Frederic Friedel als Gesellschaft bürgerlichen Rechts gegründet und später in eine GmbH umgewandelt. Dritter Gesellschafter neben Wüllenweber und Friedel wurde der Hamburger Stützpunkttrainer Gisbert Jacoby.\n\nDie Firma leitet sich ab vom gleichnamigen, von Matthias Wüllenweber entwickelten, Schachdatenbankprogramm ChessBase in Anlehnung an das zur damaligen Zeit populäre Datenbankprogramm \"dBASE\".\n\nAusgangspunkt des Unternehmens war der Vertrieb des Schachdatenbankprogramms \"ChessBase\", das in der Version 1.0 von Wüllenweber für den Atari ST entwickelt wurde. Ab Version 2.3 stand das \"ChessBase\"-Programm auch in einer von Mathias Feist entwickelten Version für das PC-Betriebssystem MS-DOS zur Verfügung. 1989 erschien parallel für den \"Atari\" und für PCs die Version \"ChessBase 3.0\". Die Version 4.0, bei der in der DOS-Version das Schachprogramm \"Fritz\" unter der gleichen Oberfläche zur Analyse mitrechnen konnte, war die letzte Version, die auch für den \"Atari\" erschien.\n\nDas Datenbankprogramm wurde konzipiert, um die Züge von Schachpartien auf Rechnern zu erfassen, gegebenenfalls mit Varianten zu kommentieren und zu speichern. Hierfür hatte Wüllenweber ein spezielles Speicherformat entwickelt, das alle Daten einer Partie (Spieler, Turnier, Turnierort, Datum, Notation, Variante, Bewertungszeichen etc.) in einer Datei ablegt („CBF“-Format). Zur Recherche erlaubt das \"ChessBase\"-Programm Zugriff auf die Partien in der Datenbank nach verschiedenen Kriterien wie Spielername, Turnier, Variante oder einem Eröffnungsindex.\n\nDie Entwicklung des \"ChessBase\"-Datenbankprogramms wurde zu Anfang auch von Garri Kasparow beeinflusst, der anlässlich eines Aufenthalts in Hamburg 1985 ChessBase erstmals besuchte und Vorschläge für die Weiterentwicklung machte.\n\nMit der Entwicklung der ersten \"Windows\"-Version ab 1994 wurde auch das Textformat „PGN“ unterstützt. Die Version \"Chessbase 6.0\" führte 1996 ein erweitertes programmeigenes Datenbankformat („CBH“-Format) ein. Neben der eigentlichen Datenbank wurde auch die Programmoberfläche erweitert und erlaubt zahlreiche parallele Anwendungen. So können neben dem Datenbankzugriff zur Analyse und Zugbestimmung unterschiedlichste Schach-Engines eingebunden werden, die nicht notwendigerweise aus dem eigenen Hause stammen.\n\n2007 war eine eingeschränkte Version unter dem Namen \"ChessBase Light 2007\" kostenfrei verfügbar. Sie konnte durch den Kauf eines Aktivierungsschlüssels zur Vollversion \"ChessBase 9\" freigeschaltet werden.\n\nDie Datenbanksoftware wurde kontinuierlich weiterentwickelt und an neue Windows-Betriebssysteme angepasst; so wurde beispielsweise bei der im Dezember 2012 veröffentlichten Version \"ChessBase 12\" das damals aktuelle Microsoft Windows 8 unterstützt.\n\nNeben dem Vertrieb des Schachdatenbankprogramms betätigte sich das Unternehmen ChessBase auch mit der Erfassung und dem Vertrieb von Schachpartien. Diese wurden zunächst im \"„ChessBase Magazin“\" auf Disketten verkauft, später folgte die Reihe \"„Turnierdiskette“\". 1990 begann ChessBase zusammen mit Autoren Trainingsdisketten in der Reihe \"„Eröffnungsdiskette“\" zu vermarkten.\n\n1991 entwickelte Mathias Feist eine Bedienoberfläche für Schach-Engines, die zusammen mit der von dem Niederländer Frans Morsch programmierten Schach-Engine \"Quest\" unter dem Namen \"Fritz\" vertrieben wurde.\n\nAußer dem Schachprogramm \"Fritz\" wurden weitere Schachprogramme, darunter u. a. \"Houdini\", \"HIARCS\", \"Junior\", \"Shredder\", \"Chess Tiger\", \"ZapChess\" und \"Rybka\" in verschiedenen Versionen, vertrieben.\n\nIm September 2001 ging ein eigener Schachserver online. Dieser löste bald nach Einführung den Marktführer \"Internet Chess Club\" in Bezug auf die Besucherzahlen als größten kommerziellen Schachserver ab. 2006 waren bei ChessBase über 200.000 Mitglieder angemeldet. Auf dem \"Fritz-Schachserver\" können Menschen gegen andere Schachspieler über das Internet Schach spielen. Es ist aber auch möglich, Computer gegeneinander spielen zu lassen. Der \"Fritz-Schachserver\" erlaubt außerdem interaktiven Unterricht und zahlreiche Spezialschachvarianten. Fast täglich werden dort zudem die Partien aus laufenden Großmeisterturnieren, Weltmeisterschaften oder Wettkämpfen live gezeigt und von den Besuchern im Chat diskutiert.\n\nFür den \"Fritz-Schachserver\" und die Trainingsreihe \"„Fritztrainer“\" entwickelte ChessBase das \"Chess Media System\". Mit diesem können Audio- oder Videoaufnahmen synchron zum Partieverlauf abgespielt werden. Damit wurden zahlreiche Lehrvideos produziert, darunter von Garri Kasparow, Wladimir Kramnik, Viswanathan Anand, Viktor Kortschnoi, Rustam Kasimjanov, Alexei Schirow, Adrian Mihalčišin, Daniel King, Andrew David Martin, Jacob Aagaard, Helmut Pfleger, Thomas Luther und Eva Moser.\n\nSeit 2003 wird jeden Freitag um 17 Uhr auf dem \"Fritz-Schachserver\" eine Live-Sendung verbreitet, in der Anfangszeit noch als \"Radio ChessBase\" bezeichnet, in der Nachrichten, Personen und Partien aus der internationalen Schach-Szene präsentiert werden. Viele der Sendungen sind über das ChessBase-Videoarchiv auch im Nachhinein kostenlos abrufbar (siehe auch Video-Archiv unter Weblinks).\n\nSeit 2003 erscheint das Schach-Lernprogramm \"„Fritz & Fertig“\" – bis Anfang 2009 in vier aufeinander aufbauende Folgen. Hiermit werden Kinder spielerisch in das Schachspiel eingeführt.\n\nIm Jahre 1994 hatte Matthias Wüllenweber ein Physiksimulationsprogramm entwickelt, das unter dem Namen \"„Albert Physik interaktiv“\" vom \"Springer-Verlag\" vertrieben wird.\n\nIm Oktober 2007 erschien mit Ludwig auch eine Musiksoftware, die für sich in Anspruch nimmt, selbständig komplexe Musiktitel verschiedener Stilrichtungen zu komponieren und arrangieren. Ludwig soll auch als virtueller Musiklehrer für diverse Instrumente und Chorgesang sowie als Begleitband dienen können. Zum Komponieren nutzt es ein Baumsuchverfahren, wie es auch in der Schachsoftware Fritz zum Einsatz kommt. Die Arrangements zu den von der Software komponierten Melodien werden anhand einer Skriptsprache generiert. Georg Mondwurf, Lehrbeauftragter für Musikdidaktik an der Universität Oldenburg, bemerkte: „\"Dass „Ludwig“ gute Möglichkeiten bietet, Gesang und Instrumentalspiel – ja sogar Improvisation zu trainieren, wurde bereits erwähnt. Dennoch: Während „Ludwig“ als Komponist schon heute restlos überzeugen kann, bleiben seine Fähigkeiten als Instrumentallehrer in dieser Version eher eine interessante Dreingabe, die einen seriösen Instrumentalunterricht kaum ersetzen werden.\"“\n\nDie Website von \"ChessBase\" wird täglich aktualisiert und bietet in drei Sprachen (deutsch, englisch, spanisch) aktuelle Schachnachrichten. Chefredakteur ist André Schulz. Außerdem steht eine kostenlose Online-Partiendatenbank mit über vier Millionen Schachpartien sowie ein Spielerlexikon mit Namen und Fotos von Tausenden von Schachspielern zur Verfügung.\n\n"}
{"id": "44980", "url": "https://de.wikipedia.org/wiki?curid=44980", "title": "Linuxtag", "text": "Linuxtag\n\nEin Linuxtag ist eine Veranstaltung mit dem Themenschwerpunkt Linux und freier Software, auf der unter anderem Vorträge, Messestände, Kurse etc. angeboten werden. Ihren Ursprung haben diese in den sogenannten „Installationspartys“ (engl. \"Installfest\"), bei denen Neueinsteigern bei der Installation von Linux geholfen wurde. Mit der immer einfacher werdenden Installation wurden diese weitgehend obsolet und die Verbreitung von Linux in den Betrieben brachte eine Verschiebung des Schwerpunkts auf die kommerzielle Nutzung.\n\nDer größte und bekannteste Linuxtag war bis 2014 der vom LinuxTag e. V. durchgeführte LinuxTag. Seit 2015 sind die Chemnitzer Linux-Tage der älteste und größte Linuxtag im deutschsprachigen Raum, daneben gibt es in verschiedenen Jahren Linuxtage und ähnliche Veranstaltungen steigender Anzahl, die in der Regel mit der Unterstützung von Universitäten\nabgehalten werden.\n\nUnter dem Titel Linuxwochen Österreich finden im Frühjahr (April – Juni) jedes Jahr in einigen Städten lokal organisierte Veranstaltungen zum Thema Linux in Österreich statt.\n\nIn den Jahren 2004 und 2005 fand der Event LOTS (\"Let’s Open The Source\") an der Universität Bern statt. Die Konferenz OpenExpo findet seit 2006 zweimal jährlich alternierend im Raum Zürich sowie in Bern parallel zur Messe Topsoft statt und widmet sich Open-Source-Themen. Verantwortlich für die Konferenz zeichnet sich die \"Swiss Open Systems User Group /ch/open\", welche auch den Event CH Open Source Awards veranstaltet. 2010 wurde erstmals der FrOSCamp an der ETH Zürich durchgeführt. Die Website ist seit 2012 nicht mehr erreichbar, kann aber noch über archive.org angesehen werden.\n\nIn der Schweiz finden seit 2012 keine regelmässigen Linux- oder FOSS-Tage statt.\nJährlich findet in Brüssel die FOSDEM (Free and Open source Software Developers’ European Meeting) statt, die eine europaweit wichtige Veranstaltung darstellt. Sie wird jedes Jahr gerne von bekannten Vertretern der Free- und Open-Source-Szene besucht. Sie findet üblicherweise Anfang Februar statt.\n\nIn Antwerpen finden außerdem jährlich die LOADays statt (Linux Open Admin Days).\n\nJährlich wird in Amiens etwas nördlich von Paris die Rencontres Mondiales du Logiciel Libre abgehalten – ein Event, in dem allerdings gut die Hälfte der Vorträge auf Englisch gehalten werden, was die Veranstaltung auch außerhalb Frankreichs interessant macht. Der Termin ist jedes Jahr im Juli.\n\nDie Unix-Anwendervereinigung Großbritanniens (UKUUG, \"United Kingdom Unix Users Group\") veranstaltet eine jährliche Linux-Konferenz mit einer technischen Ausrichtung. Sie findet üblicherweise im Spätsommer statt.\n\nDie italienischen LUGs und die \"Italian Linux Society\" organisieren jeden letzten Samstag im Oktober (seit 2006, vorher im November) einen nationalen Linuxtag.\n\nDie Linux User Group Bozen hält den Linuxtag seit 2004 im Rahmen der weiter ausgelegten SFSCon, plant 2017 aber sich wieder am nationalen Linuxday zu beteiligen.\n\nIn Stockholm fand 2007 ein Ableger der US-amerikanischen Linux World Expo statt.\n\nVon 2003 bis 2007 wurde in Slowenien die auf professionelle Linux-Anwender ausgerichtete IBLOC (International Business Linux and Open Source Conference) durchgeführt. Die Webseite ist offline und kann noch über WebArchive angesehen werden\n\nVon 2002 bis 2007 fand in Maribor das Kiblix Linux Festival statt.\n\nEs finden weltweit viele unterschiedliche Linux-Veranstaltungen statt. Neben diversen international oft „Install-Fests“ oder „LUG-Meetings“ genannten, eher regionalen Veranstaltungen hat eine Reihe von Linuxtagen auch internationale Bedeutung und Beteiligung.\n\nDie „linux.conf.au“ genannte Veranstaltung gilt als große und international bedeutsame Linux-Konferenz in Ozeanien. Sie findet üblicherweise im Frühjahr statt.\n\nVon 2001 bis 2012 fand in Bangalore, Indien, die FOSS.IN statt, die ein ähnliches Organisationsprinzip wie der LinuxTag hatte.\n\nIn Ottawa findet im Frühsommer traditionell das „Ottawa Linux Symposium“ (OLS) statt, auf dem sich insbesondere Entwickler des Linux-Kernels treffen und austauschen.\n\nÜberregionale Bedeutung besonders für Aussteller haben die beiden unter dem Namen „Linux World Expo“ veranstalteten Messen in San Francisco (Sommer) und in Boston (Winter, bis 2004: New York). Sie ziehen vorwiegend ein geschäftsorientiertes Publikum an. Einen technischeren Ansatz verfolgt die Open Source Conference in Portland. Bis 2001 gab es den \"Annual Linux Showcase\" (ALS, früherer Name: Atlanta Linux Showcase), die Veranstaltung wurde jedoch eingestellt.\n\n"}
{"id": "45020", "url": "https://de.wikipedia.org/wiki?curid=45020", "title": "Microsoft Pocket PC", "text": "Microsoft Pocket PC\n\nPocket PC (zu dt. etwa \"PC für die Westentasche\") ist ein von Microsoft seit der CeBIT 2000 in Deutschland geprägter Begriff. Er bezeichnet sowohl eine Reihe von PDAs, die mit dem Betriebssystem Windows Mobile (oder einem seiner Vorläufer) betrieben werden, als auch zum Teil das verwendete Betriebssystem selbst .\n\nDer Name Pocket PC wurde von Microsoft ursprünglich eher aus marketingtechnischen Gründen gewählt und diente dazu, sich von der Konkurrenz und dem bisherigen Marktführer Palm mit dem Betriebssystem Palm OS abzugrenzen und den Begriff \"Palm-Size PC\" (zu dt. etwa \"PC in Handflächengröße\") abzulösen. Seit Windows Mobile 2003 trennte sich Microsofts Marketingstrategie wieder von den Hardware-Begriffen Pocket PC und Smartphone und besann sich für das Betriebssystem auf den Markennamen Windows Mobile.\n\nIn Konkurrenz zu Windows Mobile stehen vor allem Apples iOS, Palm OS, Bada, Symbian, Blackberry und Linux bzw. Android.\n\nPocket PC basiert auf dem Betriebssystemkern Windows CE (WinCE) und definiert durch die über den Betriebssystemkern hinausgehende Oberfläche und die enthaltenen Anwendungen (Organizerfunktionalität, Webbrowser, E-Mail, Textverarbeitung, Tabellenkalkulation, …) das eigentlich für den Anwender Wesentliche des PDAs.\n\nWindows CE wurde speziell für die Verwendung in Klein- und Kleinstcomputern insbesondere für Industrie, Automotive und mobile Geräte entwickelt. Es stellte also „nur“ die Basis für das Betriebssystem Pocket PC dar, ist dem aber nicht gleichzusetzen.\n\nWindows CE kann auf unterschiedlichsten Plattformen mit verschiedensten Merkmalen laufen. Ein Entwickler nimmt hierzu den \"Microsoft Platform Builder\" und stellt sein individuelles Windows CE zusammen: mit oder ohne grafischer Oberfläche, Kommandozeile, mit Bluetooth-Unterstützung etc.\n\nDie Lizenzkosten pro ausgeliefertem Gerät mit Windows CE variieren in Abhängigkeit von den benutzten Funktionen und der benötigten Lizenzen. Für Pocket PC werden inoffiziell OEM-Lizenzkosten in Höhe von etwa 10–15 US-Dollar pro Gerät genannt.\n\nDie für die Entwicklung von Anwendungen für die verschiedenen Windows-CE- bzw. Windows-Mobile-Plattformen benötigten Werkzeuge („eMbedded Visual Tools“) und SDKs stellt Microsoft kostenlos zur Verfügung. Eine wesentlich umfassendere, kostenpflichtige Entwicklungsumgebung ist Microsoft Visual Studio.\n\nDie Bezeichnung Windows CE trat nach Einführung der für Endanwender als prägnanter geglaubten Plattformnamen \"Handheld PC\", \"Palm-size PC\", \"Pocket PC\" und \"Smartphone\" von Microsoft für die mobilen Plattformen in den Hintergrund. Nach der Einstellung der ersten PDA-Plattform \"Handheld PC\" und der Ablösung des \"Palm-size PC\" durch \"Pocket PC\" bleiben heute nur noch \"Pocket PC\" und \"Smartphone\". Diese beiden Plattformbezeichnungen sind heute nur noch Namenszusatz für die im Jahr 2003 eingeführte Dachmarke \"Windows Mobile\".\n\nDie Geräte der Typen \"Windows Mobile for Pocket PC\" und \"Windows Mobile for Smartphone\" benötigen nicht zuletzt wegen der unterschiedlich leistungsfähigen Hardware, Bildschirmauflösungen und Bedienkonzepte (ein Pocket PC besitzt einen Touchscreen, ein Smartphone hingegen nur eine normale Handytastatur und wenige Steuertasten zur Einhandbedienung) für die jeweilige Zielplattform entwickelte Software.\n\nHäufig wird nach Verfügbarkeit eines Updates (ROM-Update) oder einer neuen Betriebssystem-Version (Upgrade) gefragt. Microsoft stellt für Endkunden eine neue Version jedoch nicht selbst zur Verfügung. Dafür sind ausschließlich die Hersteller bzw. OEMs des jeweiligen Modells zuständig, wobei Updates häufiger angeboten werden, seltener jedoch Upgrades auf eine neue Betriebssystemversion (wie z. B. von Windows Mobile 2003 Second Edition auf Windows Mobile 5.0). Das Betriebssystem wird stark an die jeweilige Hardware angepasst, deshalb lohnt sich für Hersteller der Aufwand für echte Upgrades auch wegen der Vermarktung neuer Geräte nur selten.\n\n\"Windows Mobile for Pocket PC\", ehemals \"Microsoft Pocket PC\", erweitert die Funktionalität von \"Windows CE\" um typische Anwendungen für Taschencomputer wie Terminkalender oder Adressverwaltung. Die Benutzeroberfläche orientiert sich dabei an derjenigen von Windows, ist allerdings speziell für die Verwendung auf Taschencomputern angepasst worden.\n\n\"Windows Mobile for Pocket PC Phone Edition\" ist eine erweiterte Variante, die ein im PDA integriertes Telefonie-Modul (wie GSM oder UMTS) unterstützt. Es gibt also zusätzlich eine Empfangsanzeige, die Telefon-Anwendung, eine SMS-/MMS-Erweiterung für die E-Mail-Applikation (die bei \"normalen\" Pocket PCs via Bluetooth oder IrDA kommuniziert) und diverse zusätzliche Schnittstellen.\n\n\"Windows Mobile for Smartphones\", ehemals \"Windows Smartphone\", ist die Windows CE-Variante für Mobiltelefone. Im Gegensatz zu \"Windows Mobile for Pocket PC Phone Edition\" haben diese Geräte keinen Touchscreen, meist ein kleineres Display und eine Ziffern-Tastatur. Sie ähneln also mehr einem (damals) üblichen Mobiltelefon als einem PDA.\n\nObwohl der Name des Betriebssystems mittlerweile (fast) gleich ist, gibt es gewaltige Unterschiede zwischen \"Windows Mobile for Pocket PC (Phone Edition)\" und \"Windows Mobile for Smartphone\". Dadurch laufen die meisten Programme, die für Pocket PCs geschrieben wurden nicht auf Smartphones und umgekehrt.\nMicrosoft ist aber bestrebt, die Systeme wieder langsam zusammen wachsen zu lassen. So gibt es in Windows Mobile 5 beispielsweise die von den Smartphones bekannten Softkeys (zwei Tasten mit im Display angezeigter Funktion) und die Möglichkeit, nur \"signierte\" Programme ausführen zu lassen oder vor der Ausführung von unsignierten Anwendungen zu warnen.\n\nEinige PDA verfügen heute weiter über eine integrierte Mobiltelefon-Funktionalität, für diese bietet Microsoft eine eigene Betriebssystemvariante „Pocket PC Phone Edition“ an. PDA welche über eine solche Funktionalität verfügen werden über eine Vielzahl an Handelsbezeichnungen (MDA, Qtek/HTC, SPV, Xphone, VDA, PPC, Dopod, Ameo, TyTN, XDA, Xda) vermarktet. Eine Bezeichnung PDA orientiert sich an einer Bauart an der Displaygröße und am Fehlen einer Tastatur, sogenannte Smartphones sind in ihrer Bauform keine PDA daher eine eigene Geräte Kategorie.\n\nWindows CE unterschied noch deutlich zwischen Handheld-PC und einem normalen PDA (bis zum Jahr 2000 von Microsoft auch als \"Palm-Size PC\", später \"Pocket PC\" bezeichnet).\n\nHandheld-PCs als weitere Gerätegattung waren ein ganzes Stück größer und verfügen über ein deutlich größeres Display als die nur handflächengroßen PDAs. Hauptunterschied ist jedoch eine physisch vorhandene Tastatur, bei PDAs wird diese auf dem Bildschirm eingeblendet (nur wenige Pocket-PC-Modelle verfügen zusätzlich über eine integrierte Tastatur).\n\nBereits in Windows CE Version 1.0 (1996) war es möglich, rudimentäre Multimedia-Funktionen zu nutzen.\nMit Version 2.0 (1998) war es dann möglich, Farbdisplays mit bis zu 65.536 Farben anzusteuern.\nVon Beginn an setzte Microsoft darauf, diese Geräte auch in Netzwerke einzubinden und viele Erweiterungen zu ermöglichen.\n\nTraditionellerweise teilt sich der Speicher der Geräte in Datenspeicher und den Speicher zum Ausführen von Programmen auf. Waren in der ersten Generation noch 8 MB Gesamt-Speicher üblich, wuchs dieser bereits in der zweiten Generation auf bis zu 128 MB, der sich im laufenden Betrieb aufteilen ließ.\n\nMit der Einführung der Version 2002 kamen auch erste Geräte auf den Markt, die reale Auflösungen von 640×480 Pixeln (VGA) auf einem 3,5-Zoll-Display schafften.\n\nDie Pocket-PC-Plattform wird ab der Version 2002 massiv für den Massenmarkt optimiert. Dazu gehört es auch, viele für den Fachmann sinnvolle Funktionen, wie das Beenden von Anwendungen, Kontrolle über Netzwerk etc. entweder zu unterbinden, verbieten oder hinter „benutzerfreundlichen“ Schichten zu verstecken.\n\nEbenfalls mit der Version 2002 eingeführt wurde der „Connection Manager“, der die vollständige Kontrolle über jedwede Netzwerkverbindung (LAN, DFÜ etc.) übernimmt und vieles automatisieren soll.\n\nEinige der Automatismen arbeiten jedoch nicht im Sinne besonders professioneller Benutzer, können aber trotzdem nicht immer umgangen werden.\n\nEin Problem der gesamten Pocket-PC- und Windows-CE-Familie ist prinzipbedingt, dass sich die Prozessoren der einzelnen Geräte deutlich voneinander unterscheiden, so dass es nicht möglich ist, ein Programm, das für einen CPU-Typ geschrieben wurde, auf einem anderen ausführen zu können. Zwar ist ab der Version 2002 die Pocket-PC-Plattform nur noch als ARM-Variante verfügbar, aber viele alte Programme werden nicht mehr aktualisiert und stehen deshalb teilweise trotzdem nur für MIPS o. ä. zur Verfügung. Die Ausführung von Anwendungen anderer Windows-CE-Plattformen, wie Handheld-PC auf Pocket PCs, ist ebenfalls, meist aufgrund spezifischer Erweiterungen der jeweiligen Plattform, nicht möglich.\n\nNachdem Microsoft durch unterschiedliche Namen für ähnliche Windows-CE-Systeme für Verwirrung gesorgt hatte, erkannte das Unternehmen, dass eine Rückbesinnung auf das alte Namenssystem sinnvoll war. Windows CE 5.0 hatte, neben neuen Schnittstellen und integrierten WLAN-Funktionen, auch erstmals eine Unterstützung von 3D Grafikchips bekommen.\n\nEine der wichtigsten Änderungen, die Windows Mobile 5.0 mitbrachte, betraf den Speicher: Der interne Datenspeicher befindet sich nicht mehr im flüchtigen RAM, sondern als sogenannter \"Persistent Storage\" im Flash-ROM, was nun vor Datenverlust schützt, wenn die Akku-Energie aufgebraucht ist. Außerdem ließ das nunmehr modular aufgebaute Betriebssystem im ROM erstmals ein selektives Update zu und muss nicht mehr komplett geflasht (überschrieben) werden, wenn der Hersteller bzw. OEM Bugfixes, Verbesserungen oder Ergänzungen anbieten will.\n\nDie Version Windows Mobile 6.0 fügte einige kleine neue Funktionen wie besonders schnell durchsuchbare E-Mail-Titel, die Möglichkeit, auf einen Microsoft Exchange-Server zuzugreifen oder leichte Verbesserungen im Internet Explorer bzw. den Office-Programmen hinzu. Außerdem wartete Windows Mobile 6 mit einer an Windows Vista anlehnenden Optik auf und beinhaltet Stabilitäts- und Geschwindigkeits-Verbesserungen.\nDie meisten Veränderungen gegenüber Windows Mobile 5 liegen jedoch im Detail und sind nicht substantiell.\n\nPocket PC lief bis zur Version 2002 auf Geräten mit unterschiedlichen Architekturen, darunter Intel x86, MIPS, ARM, und Hitachi-SH-Prozessoren. Aufgrund des Aufwandes für die Hard- und Softwareentwickler durch die dadurch entstandene Gerätevielfalt, wird das Windows-Mobile-Betriebssystem von Microsoft ab Version 2003 nur noch für ARM-basierende bzw. kompatible Geräte angeboten. Die Hersteller der Geräte sind für die finale Anpassung des Betriebssystems an die Hardware zuständig und somit ist man nach dem Kauf eines Geräts an den Hersteller bzw. den OEM bezüglich eines Updates gebunden. Neue Modelle werden in einem sehr kurzen Zyklus von oft unter 1 Jahr auf den Markt gebracht, die älteren Modelle werden in Bezug auf Betriebssystempflege in der Regel sehr bald nicht weiter unterstützt.\n\nSoftware für Pocket PCs ist aus den genannten Gründen nicht lauffähig auf Windows PCs, oft auch nicht oder nicht voll kompatibel mit den entsprechenden Versionen für PC. Für eine ganze Reihe von Dateiformaten gibt es jedoch teilweise oder auch voll kompatible Betrachter und Bearbeitungssoftware für Pocket PCs. Das Synchronisationsprogramm Microsoft ActiveSync wird auf Desktop-PCs, welche mit Betriebssystemen vor Vista laufen, installiert, für PCs unter Windows Vista wird von Microsoft ein Nachfolgemodell von Active Sync angeboten; Windows Mobilitätscenter, damit stehen einem die Grundfunktionen von Active Sync. plus weitere zusätzliche Möglichkeiten zur Verfügung, welche vorher mit Zusatzsoftware nachinstalliert werden mussten. Diese beiden Programme stehen neben der Installation von Software über den Desktop auf den angeschlossenen Pocket PC insbesondere auch zur Synchronisation von Daten und Dateien zur Verfügung, wobei einige Dateiformate dabei konvertiert werden und bestimmte Formatierungen verloren gehen. Insbesondere die im ROM mitgelieferten Versionen der Microsoft Programme Word Mobile und Excel Mobile erfordern eine solche Konvertierung unter Verlust zahlreicher Informationen.\nFür sogenannte \"OTA\" (Over-the-Air) Installation, bei der man keinen Desktop-Computer benötigt, sondern eine eingebaute Luftschnittstelle (GSM/GPRS/UMTS, WLAN, Bluetooth) oder ein Mobiltelefon als Modem nutzt, stehen für Pocket PC Software oft komprimierte Setup-Dateien (.CAB) zur Verfügung.\n\nOptimiert für z. T. verschiedene mobile Betriebssysteme und kompatibel mit Desktop-Software gibt es einige wenige Programme und Dateiformate, von denen besonders Content Reader v. a. für E-Books hervorzuheben sind (Mobipocket, Microsoft Reader, eReader, Vade Mecum/Plucker, TomeRaider sowie Adobe Reader und RepliGo).\n\nPocket PCs werden neben den klassischen PDA-Funktionen wie PIM (Personal Information Management) und zusehends auch Telefonie-Funktionen (v. a. auch als mobile Internet-Terminals, für E-Mail und Messaging) zu einem erheblichen Teil auch als mobile Satellitennavigationssysteme genutzt. Beliebt sind auch Multimedia-Anwendungen und Spiele, für die sich die anspruchsvolle Plattform ebenfalls bestens eignet.\n\nDie Bandbreite der Anwendungssoftware für Pocket PC ist im Vergleich zum Desktop kaum eingeschränkt. Klassische bzw. am häufigsten auf Pocket PC eingesetzte Anwendungssoftware sind PIM (für Kontakte, Termine, Aufgaben und Notizen) und Navigationssoftware (für GPS-Satellitennavigation), darüber hinaus Spiele jeden Genres, Multimedia-Anwendungen (wie MP3-Player und Videosoftware, Bild-/Foto-Betrachtungs- und Bearbeitungssoftware), E-Books, Übersetzungssoftware, Office-Programme (Textverarbeitung, Tabellenkalkulation, Präsentationen und Datenbanken), insbesondere auf Phone-Edition-Geräten schließlich E-Mail, SMS, MMS, Messenger, Webbrowser, Feedreader (für Web-Feeds im RSS-Format) und verschiedenste \"Utilities\" wie z. B. Taschenrechner und Fahrtenbuch.\n\nDer Markt sorgt mit zigtausenden von Programmen dafür, dass für praktisch jede denkbare Anwendung eine große Bandbreite von Lösungen zur Verfügung steht. Es gibt auch ein sehr breites Angebot an Freeware.\n\nDie bekanntesten Hardware-Hersteller für Pocket-PC-Geräte sind u. a. HTC (zumeist für Mobiltelefon-Provider wie z. B. T-Mobile als OEMs sowie für Qtek und i-mate), Hewlett-Packard, Mitac (auch für verschiedene OEMs wie z. B. Yakumo), Dell, Fujitsu-Siemens, Asus, Acer, Eten (auch für den OEM Typhoon), Samsung, darüber hinaus noch mehrere für industrietaugliche Geräte.\nEin in der Vergangenheit sehr erfolgreicher Hersteller hat sich teilweise von Markt zurückgezogen: Casio bietet nur mehr industrietaugliche Pocket PCs an. Auch Toshiba hatte sich zeitweise vom europäischen Markt zurückgezogen, kehrte aber 2007 mit einer neuen Modellpalette zurück.\nEs folgen viele neue Hersteller nach, so dass die angebotene Modellvielfalt stetig zunimmt.\n\nAls Zubehör werden Erweiterungen der Hardware besonders für Funktionalität, die oft nicht durch integrierte Komponenten abgedeckt ist, angeboten: Speicherkarten (i. d. R. in den Formaten SD SecureDigital, MMC MultiMediaCard, CF CompactFlash, MicroDrive), GPS-Receiver (Empfänger für Satellitennavigationssignale, über Bluetooth oder kabelgebunden, z. T. auch mit TMC-Funktionalität für Verkehrsnachrichtensignale), WLAN (als Steckkarte für den SDIO- oder CF-Erweiterungssteckplatz), im professionellen bzw. Industrie-Einsatz z. B. auch Barcode-Scanner.\n\nWährend die Markteinführung reiner Pocket-PC-Varianten in den Jahren 2006/2007 eher zu einer Seltenheit wurde, entwickelte das Pocket PC Phone eine bemerkenswerte Funktionsvielfalt. Die teilweise auf unter 120×60×20 Millimeter geschrumpften Modelle verfügen über interne oder auf der Hauptplatine integrierte GPS-, WLAN- und Bluetooth-Module, ausziehbare QWERTZ-Tastatur, Webcam und Steckplätze für die SDHC-fähigen microSD-Speicherkarten, die Ende 2007 eine Kapazität von 8 Gigabyte erreichen.\n"}
{"id": "47623", "url": "https://de.wikipedia.org/wiki?curid=47623", "title": "Apple eMac", "text": "Apple eMac\n\nDer eMac ist ein von Apple und ursprünglich nur für den Bildungsmarkt konzipierter Computer. Er wurde im April 2002 der Öffentlichkeit vorgestellt.\n\nAufgrund hoher Kundennachfrage gab es ihn offiziell auch für Kunden zu kaufen, die nicht aus dem Bildungsbereich stammen. Der eMac kann als Nachfolger des Ur-iMac angesehen werden, denn er hat das gleiche Komplettrechner-Design mit Rechner und Monitor in einem Gehäuse wie dieser, allerdings einen größeren 17\"-Röhren (CRT)-Monitor und einen Wechsel auf die nächsthöhere Prozessorgeneration.\n\n\n\n\n\nNicht zuletzt im Zusammenhang mit dem Erscheinen der neuen iMac-Generation mit Intel-Prozessor im Januar 2006 stellte Apple den Vertrieb von eMacs Ende 2005 ein.\n\nAls Nachfolger wurde im Juli 2006 eine spezielle Version des 17-Zoll-iMacs eingeführt, die über eine weniger leistungsfähige Grafikhardware und ausschließlich ein ComboDrive verfügt.\nIm Gegensatz zum Standardmodell fehlt außerdem die Fernbedienung Apple Remote und das integrierte Bluetooth-Modul. Durch diese Einsparungen wurde der Preis für den Bildungs-iMac auf 938 Euro gesenkt.\nNachdem der neue iMac einige Tage im Bildungs-Store von Apple zu bestellen war, wurde er für einige Zeit aus dem Endkundenbereich entfernt und war seit der Einführung des 24\"-iMacs nun auch für Endkunden wieder bestellbar.\n\nNach Vorstellung des Aluminium-iMacs wurde zusammen mit dem 17\"-iMac auch der Bildungs-iMac aus dem Programm gestrichen.\n\n"}
{"id": "47665", "url": "https://de.wikipedia.org/wiki?curid=47665", "title": "Carry-Ripple-Addierer", "text": "Carry-Ripple-Addierer\n\nDer Carry-Ripple-Addierer (von engl. carry – \"Übertrag\", ripple – \"rieseln\"), auch \"Ripple-Carry-Addierer\" oder \"ripple-through carry\", ist ein Addiernetz, dient also der Addition mehrstelliger Binärzahlen.\n\nEin n-Bit-Carry-Ripple-Addierer kann zwei n-stellige Binärzahlen addieren, das Ergebnis hat n+1 Stellen. Das Schaltnetz hat damit 2n+1 (bzw. 2n ohne \"Carry in\") Eingänge und n+1 Ausgänge.\n\nEr setzt sich aus n Volladdierern (bzw. aus n−1 Volladdierern und 1 Halbaddierer ohne \"Carry in\") zusammen. Der Übertrags-Ausgang der Addierer wird jeweils an einen Eingang des nächsten Volladdierers angeschlossen. Der Übertrags-Ausgang des letzten Volladdierers bildet den (n+1)-ten Ausgang des Schaltnetzes.\n\nDa Volladdierer nicht unendlich schnell arbeiten, kann es zu Verzögerungen bei der Berechnung des Endergebnisses kommen, da der Volladdierer das korrekte Ergebnis erst dann ausgeben kann, wenn der vorhergehende Volladdierer das Übertragsbit geliefert hat. Im schlechtesten Fall führt die Addition a + b zu einem Übertrag, und für alle i > 0 gilt: a + b ≥ 1. Dann muss das Übertragsbit durch das gesamte Addiernetz wandern, bevor das richtige Ergebnis ausgegeben wird (\"Übertragspropagation\").\n\nUm diese langen Laufzeiten zu vermeiden, wurden beschleunigte Addiernetze entwickelt, etwa Carry-Skip, Carry-Look-Ahead ((Super)Ripple CLA und (Super)Block CLA), Conditional-Sum-Addition und Carry-Select-Addierer.\n"}
{"id": "48981", "url": "https://de.wikipedia.org/wiki?curid=48981", "title": "Messaging Application Programming Interface", "text": "Messaging Application Programming Interface\n\nMessaging Application Programming Interface (MAPI, dt.: \"Schnittstelle für die Programmierung von nachrichtenverarbeitenden Programmen\") ist eine von Microsoft entwickelte Windows-Programmierschnittstelle. MAPI ist die zentrale Schnittstelle von Microsoft Outlook und Microsoft Exchange Server und ermöglicht anderen Programmen den Zugriff auf E-Mail-Funktionen.\n\nZum Beispiel ist es mittels MAPI möglich, E-Mails über Outlook zu verschicken, ohne dass der Benutzer Outlook öffnet.\n\nMAPI basiert auf dem Component Object Model.\n\nMAPI existiert in den Versionen \"Simple MAPI\" und \"Extended MAPI\" (MAPI 1.0). Von einer Reihe anderer Hersteller wurde der Standard \"Vendor Independent Messaging\" (VIM) vorgeschlagen, der sich gegen MAPI jedoch nicht durchsetzen konnte.\n\nEinige freie Open-Source-Projekte bieten eine Unterstützung der MAPI-Schnittstelle. Beispielsweise gibt es das OpenMAPI-Projekt, das Zarafa MAPI4Linux (auch ein Teil von OpenMAPI) und die MAPI-Bibliothek libmapi als Teilprojekt von OpenChange. Diese Bibliothek wird auch in dem OpenChange Teilprojekt Evolution-MAPI verwendet. Evolution-MAPI stellt eine Verbindung von Evolution mit Exchange Servern dar. Des Weiteren stellt das Projekt DavMail einen dynamischen Übersetzer der MAPI-Schnittstelle auf die Protokolle IMAP und POP zur Verfügung, der die Anbindung von standardkompatiblen E-Mail-Programmen, wie z. B. Mozilla Thunderbird an einen Exchange-Server ermöglicht.\n\n"}
{"id": "49100", "url": "https://de.wikipedia.org/wiki?curid=49100", "title": "Einerkomplement", "text": "Einerkomplement\n\nDas Einerkomplement, auch (B-1)-Komplement, ist eine arithmetische Operation, die meist im Dualsystem angewendet wird. Dabei werden alle Ziffern bzw. Bits einer Dualzahl invertiert, das heißt: Aus codice_1 wird codice_2 und umgekehrt. Das hat zur Folge, dass jede Ziffer der Dualzahl und ihre korrespondierende Ziffer des Einerkomplements sich „zu codice_2 ergänzen“, was der Operation ihren Namen gibt. Die Operation wird auch als bitweise Negation bezeichnet und der Operator in verschiedenen Programmiersprachen als Tilde codice_4 notiert.\n\nPrinzipiell werden Zahlen im Register eines Computers als 0 und 1 dargestellt. Das Einerkomplement ist eine von mehreren bekannten Möglichkeiten, diesen gespeicherten Wert als Dezimalzahl zu interpretieren und in Rechenoperationen zu verarbeiten.\n\nEine Anwendung des Einerkomplements ist die gleichzeitige Manipulation einzelner Bits in einem Datenwort. Will man zum Beispiel im Wort codice_5 alle Bits löschen, die im Wort codice_6 gesetzt sind, so muss man codice_5 mit dem Einerkomplement von codice_6 bitweise und-verknüpfen, in C-Syntax codice_9\n\nEine andere Anwendung ist die Einerkomplementdarstellung, ein Verfahren zur binären Darstellung negativer Ganzzahlen. Zwar lässt sie sich leicht beschreiben – das Komplement der Darstellung einer negativen Zahl ist die normale Binärdarstellung ihres Betrags –, aber die Implementation einer Recheneinheit für so dargestellte Zahlen ist umständlich. Vorteile gegenüber der heute üblichen Zweierkomplement-Darstellung hat sie nur bei der ohnehin meist langsamen Division, bei der Multiplikation mit doppelt langem Ergebnis sowie bei der Bildung einfacher Prüfsummen.\n\nBinäre Kodierungen vorzeichenbehafteter Ganzzahlen haben meist folgende Eigenschaften:\nUnterschiede bestehen bei gesetztem höchstwertigen Bit. In diesem Fall ergibt sich bei der Einerkomplementdarstellung der Betrag durch Komplementbildung. Zum Beispiel erweist sich codice_12 durch die führende codice_2 als negativ und der Betrag ist codice_14, also codice_15 = 5. Durch diese Definition ergeben sich folgende weitere Eigenschaften der Einerkomplementdarstellung:\n\nDie Beispiele sind hier für eine Wortbreite von \"n\" = 4 bit angegeben. Für 8 und 16 bit ergeben sich die Maximalbeträge 127 bzw. 32767, allgemein formula_1\n\nDie in Einerkomplementdarstellung einfachste Rechenoperation ist die arithmetische Negation (unärer codice_19-Operator). Es ist lediglich das bitweise Komplement zu bilden. Dadurch lässt sich die Subtraktion (binärer codice_19-Operator) direkt auf die Addition zurückführen: 3 − 4 = 3 + (−4). Für die Durchführung dieser Addition ergibt ein für vorzeichenlose Zahlen konstruiertes Addierwerk das richtige Ergebnis:\n\nNachteil der Einerkomplementdarstellung ist die Behandlung des Falls, wenn bei einer Operation die Null durchschritten wird. Beispiel: Beim Berechnen von −4 + 6 = +2 erscheint nach einer einfachen Dualzahl-Addition der beiden Einerkomplementdarstellungen zunächst ein falsches Zwischenergebnis:\n\nDie codice_21 stünde für +1, nicht für +2. Damit ein korrektes Ergebnis erscheint, muss der am weitesten links stehende Übertrag ausgewertet werden (hier codice_2) und ggf. das Ergebnis um 1 erhöht werden. Mit anderen Worten muss der Übertrag noch zum Zwischenergebnis hinzuaddiert werden:\n\nBeim ersten Beispiel oben ist der Übertrag codice_1, daher entspricht das Zwischenergebnis dort schon dem Endergebnis.\n\nEin weiterer Nachteil ist das Entstehen einer Redundanz: Es existieren für die Null zwei Darstellungen: codice_16 (+0) und codice_17 (−0), siehe vorzeichenbehaftete Null. Zum einen wird bei einer begrenzten Anzahl von Bits nicht die maximale Ausdehnung des Betrags der darstellbaren Zahlen ausgenutzt. Der darstellbare Zahlenraum verringert sich um 1; da die Null zweimal vorhanden ist, fällt ein Datenwort für den Zahlenraum weg. Die Darstellung jeder anderen Zahl bleibt aber eindeutig. Im Beispiel hier mit 4 Bits werden mit den 2 = 16 verschiedenen Bitkombinationen nur 15 verschiedene Zahlen (von −7 bis 7) dargestellt.\n\nBeide geschilderten Probleme werden bei der Kodierung von Zahlen in der Zweierkomplementdarstellung vermieden.\n\nDas \"Hinzuaddieren des Übertrags\" verbessert die Empfindlichkeit einer einfachen Prüfsumme gegen mehrfache Bitfehler. So würde eine Prüfsumme mit der Überträge ignorierenden Modulo-Arithmetik mit einer Wahrscheinlichkeit von 50 % keinen Übertragungsfehler anzeigen, wenn das höchstwertige Bit oft falsch ist, z. B. konstant null. Das TCP verwendet eine Prüfsumme in Einerkomplement-Arithmetik, die diesen Mangel nicht aufweist und deren effiziente Berechnung auf Hardware ohne Einerkomplement-Rechenwerk in RFC 1071 beschrieben ist.\n\nDie nur im Binärsystem mögliche Invertierung entspricht der Rechenvorschrift formula_2 mit der Basis B des Stellenwertsystems. Im Dezimalsystem muss die Ziffer also von 9 abgezogen werden. Einige Autoren sprechen dann vom Neunerkomplement und allgemein vom (B-1)-Komplement\n\nBeispiel für das Dezimalsystem (dreistellig):\nUnd eine Beispielrechnung:\n\n"}
{"id": "49103", "url": "https://de.wikipedia.org/wiki?curid=49103", "title": "Zweierkomplement", "text": "Zweierkomplement\n\nDas Zweierkomplement (auch 2-Komplement – verallgemeinert b-Komplement (b Basis) –, Zweikomplement, B(inär)-Komplement, Basiskomplement, two’s complement) ist eine Möglichkeit, negative Integer-Zahlen im Dualsystem darzustellen, ohne zusätzliche Zeichen wie + und − zu benötigen. Dies ist vor allem für Computer bedeutend, welche mit Bits arbeiten, die als Werte nur 0 oder 1 annehmen. Das Zweierkomplement ist die vorherrschende Art, mit der positive und negative Zahlen im Computer dargestellt und für Rechenoperationen mit Hilfe des Rechenwerks erschlossen werden.\n\nÜblicherweise werden Zahlen im Computer im Binär repräsentiert und zum Beispiel mit den Ziffern 0 und 1 dargestellt. Das Zweierkomplement ist eine von mehreren bekannten Möglichkeiten, diesen gespeicherten Wert als Ganze Zahl zu interpretieren und in Rechenoperationen zu verarbeiten.\n\nBei binären Codierungen von negativen Zahlen, welche nicht im Zweierkomplementformat vorliegen, werden sowohl das Vorzeichen als auch der Betrag durch getrennte Bits dargestellt, daher ist es wichtig, zu wissen, welches Bit wofür verwendet wird. Üblicherweise wird das erreicht, indem sämtliche Zahlen eine konstante Stellenzahl haben und bei Bedarf mit führenden Nullen aufgefüllt werden, und einem davon getrennten Bit, welches das Vorzeichen codiert. Für die Verarbeitung sind dann entsprechende Steuerlogiken notwendig, welche die unterschiedlichen Bits und deren Bedeutung bewerten.\n\nBei der Codierung in der Zweierkomplementdarstellung ist dagegen die explizite Unterscheidung zwischen einem ausgezeichneten Vorzeichenbit und den Bits, die den Betrag beschreiben, nicht notwendig. Negative Zahlen sind daran zu erkennen, dass das höchstwertige Bit den Wert codice_1 hat. Bei codice_2 liegt eine positive Zahl oder der Wert 0 vor. Der Vorteil dieses Zahlenformates besteht darin, dass für Verarbeitung in digitalen Schaltungen keine zusätzlichen Steuerlogiken notwendig sind.\n\nDa in der Zweierkomplementdarstellung der Wert 0 den positiven Zahlen zugerechnet wird, umfasst der Wertebereich bei formula_1 binären Stellen allgemein den Bereich (falls nicht anders definiert):\n\nBeispiele:\n\nDie Zweierkomplementdarstellung benötigt, anders als die Einerkomplementdarstellung, keine Fallunterscheidung, ob mit negativen oder mit positiven Zahlen gerechnet wird. Das Problem der Einerkomplementdarstellung, zwei Darstellungen für die Null zu haben, tritt nicht auf. Positive Zahlen werden in der Zweierkomplementdarstellung mit einer führenden codice_2 (Vorzeichenbit) versehen und ansonsten nicht verändert.\n\nNegative Zahlen werden wie folgt aus einer positiven Zahl codiert: Sämtliche binären Stellen werden negiert und zu dem Ergebnis der Wert \"1\" addiert. (Mathematisch exaktes Verfahren siehe formale Umwandlung.)\n\nBeispielhafte Umwandlung der negativen Dezimalzahl −4 in die Zweierkomplementdarstellung unter Verwendung von 8 binären Stellen:\n\nTrick zur schnelleren Umwandlung (einer negativen in eine positive Binärzahl oder umgekehrt) von Hand: Von rechts angefangen, alle Nullen und die erste Eins abschreiben und alle nachfolgenden Stellen invertieren.\n\n\nAlternative Faustregel:\n\n\nDie Zweierkomplementdarstellung kann man sich auch so veranschaulichen: Alle Bits haben die gleiche Wertigkeit wie bei positiver Darstellung. Das MSB (most significant bit = höchstwertige bit) allerdings erhält die negative Wertigkeit. Durch Subtraktion dieses Bits lassen sich Zahlen sehr schnell umwandeln.\nBeispiel mit 8-Bit-Binärzahlen in Zweierkomplementdarstellung:\n\nIn einem nachfolgenden Abschnitt wird die Korrektheit dieses Verfahrens erläutert.\n\nAls weitere Methode kann man die Zahl, wenn sie negativ ist, einfach zu der Zahl direkt jenseits des Wertebereichs addieren. Beispielsweise überdecken vorzeichenlose 8-Bit-Zahlen den Wertebereich 0–255, die direkt folgende Zahl ist die 256. Eine −1 muss man nur zu 256 addieren und erhält den Wert 255 (= 1111 1111b), wie gewünscht. Analog führt eine –128 zum Wert 128.\n\nFür das Verständnis des Zweierkomplements ist es hilfreich, sich zu vergegenwärtigen, dass gängige Mikroprozessoren in Restklassenringen rechnen. Tatsächlich geht es gar nicht darum, positive und negative Zahlen unterschiedlich zu behandeln, vielmehr geht es um eine Vereinbarung, welche Repräsentanten gewählt werden, um eine bestimmte Restklasse zu beschreiben.\n\nDie vier Grundrechenarten im Dualsystem sind für positive und negative Zahlen völlig gleich.\n\nIm Kontext der vorherigen Beispiele kann man die Rechnung (−7) · (−3) auf einem Rechner mit 8 Bit Wortbreite betrachten. Bei 8 Bit Wortbreite ist der darstellbare Zahlbereich 0 bis 255, und selbst das ist schon eine Auswahl von Repräsentanten, denn es geht eigentlich um Klassen des Restklassenringes formula_3. Und an dieser Stelle kann man die Zahlen von 0 bis 255 als Repräsentanten wählen, oder aber, \"völlig äquivalent,\" die Zahlen −128 bis 127. Beim Rechnen im Zweierkomplement werden Zahlen, genau genommen, gar nicht „umgewandelt“, vielmehr werden zum Rechnen \"geeignete Repräsentanten der beteiligten Restklassen\" gewählt.\n\nIm Beispiel können für −3 und −7 die Repräsentanten 253 und 249 gewählt werden:\n\nMultipliziert ergibt sich:\n\nBei der „Umwandlung“ von positiven zu negativen Zahlen, es handelt sich tatsächlich nur um eine geschickte Auswahl von Repräsentanten der beteiligten Restklassen, fällt der besondere Charme des Zweierkomplements ins Auge: −1 ist nichts anderes als das Ergebnis der Rechnung 0 minus 1 gerechnet in den Grundrechenarten des Dualsystems. Es wird lediglich zu den 8 Bit eines Registers, wie sie in diesem Beispiel verwendet werden, ein Carrybit hinzugefügt.\n\nEs ergibt sich also die Zweierkomplementdarstellung von −1 und auch das Carry Bit ist korrekt gesetzt.\n\nMan kann nun die Umwandlung von Zahlen ins Zweierkomplement im Restklassenring deuten. Wenn z. B. die Darstellung von −3 gesucht wird, liegt −3 in derselben Kongruenzklasse wie 253. Es reicht also aus, zu −3 den Wert 256 zu addieren, um einen brauchbaren Repräsentanten zu erhalten.\n\nZählt man nun die Bitstellen der Bits 0 bis 7 zusammen, ist dies 255. Nimmt man also 3 als „positive“ Dezimalzahl und invertiert diese bitweise, ergibt sich der „Ergänzungswert“ zu 255, also 255 − 3. Und wenn das Ergebnis inkrementiert wird, ist dies (wegen Kommutativ- und Assoziativgesetz der Addition)\n\nDas ganze bitweise:\n\nAm Ende ist also das Carry Bit gesetzt, was eine negative Zahl anzeigt, und die Bitfolge 11111101, dies entspricht dezimal 253.\n\nWeiter oben im Text wurde die Deutung des Carry-Bit als −256 vorgeschlagen. Diese findet sich übrigens auch bei Tietze/Schenck. Tatsächlich darf man im Restklassenring formula_3 auf eine Zahl beliebig oft 256 addieren oder 256 subtrahieren. Die Kongruenzklasse wird nicht verlassen. Dies gilt ebenfalls für ganzzahlige Vielfache von 256.\nWenn also das Carry-Bit als −256 interpretiert wird, und 1 1 1 1 1 1 1 0 1 folglich als −256 + 253 = −3 hat man eigentlich nur die Formel 253 = −3 + 256 anders hingeschrieben.\n\nDie Interpretation im Restklassenring\ndes Zweierkomplements lässt auch folgende Darstellung eines Vorzeichenwechsels zu.\n\nIst formula_9 die Binärdarstellung einer formula_1-stelligen Zahl, so kann diese leicht von formula_11 abgezogen werden:\nformula_11 ist gerade formula_1 mal hintereinander die Ziffer formula_14. Bei einer ziffernweisen Subtraktion stehen dort ausschließlich die Differenzen formula_15 oder formula_16. Es werden keine Überträge nötig, jede Ziffer darf isoliert betrachtet werden.\n\nTatsächlich heißt das aber, dass die Differenzbildung formula_17 der bitweisen Komplementbildung von formula_18 entspricht: formula_19, dabei ist formula_20 das bitweise Komplement von formula_18.\n\nIm Restklassenring heißt das nichts anderes als:\nund damit\nund nach Addition von 1 auf beiden Seiten:\n\nGanz allgemein lässt sich also das Vorzeichen von formula_18 umkehren, indem im ersten Schritt formula_18 bitweise zu formula_20 invertiert und im zweiten Schritt 1 addiert wird.\n\nDies entspricht genau der „Alternativen Faustregel“ die im Abschnitt Umwandlung per Hand beschrieben wird. Und es funktioniert in beide Richtungen, egal ob eine Dezimalzahl erst ins Binärsystem umgewandelt werden soll um dann das Vorzeichen umzukehren, oder ob bei einer negativen Zahl erst das Vorzeichen umgekehrt wird, um dann die gewonnene positive Zahl ins Dezimalsystem umzuwandeln.\n\nAddition und Subtraktion benötigen keine Fallunterscheidung. Die Subtraktion wird auf eine Addition zurückgeführt.\n\nBeispiele an 8 Bit langen Zahlen ohne Vorzeichenerweiterung:\n\nDie vorderste Eins, in diesem Beispiel die 9. Stelle, wird verworfen.\n\nAuch hier wird das korrekte Ergebnis durch Weglassen der 9. Stelle, in diesen Fällen codice_1, gebildet.\n\nSolange der gültige n-stellige Zahlenbereich, bei 8 Bit breiten Zahlen der Wertebereich der Summe von −128 bis +127, nicht verlassen wird, funktioniert dieses Vorgehen ohne Vorzeichenerweiterung problemlos. Liegt dagegen der Wertebereich der Summe außerhalb des Intervalls, kommt es zu einem Überlauf, welcher in diesem Zusammenhang häufig und fälschlich mit dem Übertrag verwechselt wird. Die Abhilfe ist eine Vorzeichenerweiterung vor der Rechenoperation.\n\nKönnen die beiden Summanden beliebige Werte annehmen, ist für eine korrekte Addition in Zweierkomplementdarstellung eine \"Vorzeichenerweiterung\" nötig. Dabei wird von beiden Summanden zunächst die oberste Stelle dupliziert und somit die Stellenanzahl um eins vergrößert. In diesen Beispielen die 8. Stelle, welche auf die 9. Stelle kopiert wird. Anschließend wird die Addition wie oben, aber mit 9 Stellen, durchgeführt. Das Addierwerk muss dazu immer eine Stelle mehr umfassen.\n\nUnterscheiden sich in der berechneten Summe dann die höchstwertige und die Stelle darunter voneinander, ist das Ergebnis nicht mehr im Wertebereich der Summanden darstellbar – es ist ein Überlauf aufgetreten. Je nach Anwendungsfall wird dann mit dem um ein Bit breiteren und korrekten Ergebnis weitergerechnet oder ein Fehlerabbruch ist die Folge.\n\n\"Beispiel:\" Die Addition der beiden positiven Zahlen 50 und 80 ergibt 130 und überschreitet damit den Wertebereich. Die Zahl passt zwar noch in eine 8-Bit-Variable, aber das 8. Bit ist jetzt gesetzt, so dass die Zahl fälschlicherweise negativ erscheint. Manche Mikroprozessoren wie der 6502 melden ein solches Ereignis mit einem eigenen Statusbit, hier dem Overflow-Bit \"O\", das der Programmierer nach vorzeichenbehafteten Rechenoperationen abfragt und entsprechend reagieren kann.\n\nBeispiel für Vorzeichenerweiterung, die 9. Stelle der Vorzeichenerweiterung ist zur Verdeutlichung in Klammern geschrieben:\n\nIn beiden Fällen unterscheiden sich die 8. und 9. Stelle voneinander, eine Reduktion auf 8 Bit würde zu einem Fehler führen. Zur Verdeutlichung und Vergleich die obigen beiden Beispiele mit Vorzeichenerweiterung:\n\nin beiden Fällen unterscheiden sich die 8. und 9. Stelle der Summe nicht, die beiden Ergebnisse können somit korrekt wieder auf 8 Stellen reduziert werden. Generell kann die Stellenanzahl in der Zweierkomplementdarstellung, von oben beginnend, so lange und ohne Verfälschung des Wertes reduziert werden, bis sich die beiden obersten Stellen im Wert voneinander unterscheiden. Das verdeutlicht den Umstand, dass bei der Zweierkomplementdarstellung von Zahlen keine fixe Stelle für die Codierung des Vorzeichens existiert.\n\nAuch die Multiplikation ist in der Zweierkomplementdarstellung im Rahmen von Multiplizierwerken möglich und stellt insbesondere in der digitalen Signalverarbeitung eine Grundfunktion dar. Für die schaltungstechnische Realisierung von Multiplizierwerken gibt es verschiedene Möglichkeiten. Bei einem Parallelmultiplizierer wird das Produkt durch eine Vorzeichenerweiterung, Stellenverschiebung und anschließende Addition gebildet. Die einzelnen Faktoren müssen dabei immer auf die Produktlänge vorzeichenerweitert werden. Neben den Parallelmultiplizierern existieren auch effizientere Implementierungsvarianten der Multiplikation, welche auf dem Booth-Algorithmus oder dem Bit-Pair-Verfahren basieren.\n\nBei zwei Faktoren zu je 4 Bit Länge ist das Produkt 8 Bit lang. Oder allgemein: Für zwei \"n\" Bit bzw. \"m\" Bit breite Faktoren ist das Produkt \"n\"+\"m\" Bit lang und alle Faktoren müssen vor der Berechnung mittels Parallelmultiplizierer auf diese Länge vorzeichenrichtig erweitert werden. An der Operation −7 · −3 in Zweierkomplementdarstellung, deren Faktoren sich mit je 4 Bit darstellen lassen, soll das verdeutlicht werden:\n\nWegen der Vorzeichenerweiterung lässt sich die Anzahl der Summanden reduzieren. Der Grund liegt darin, dass die nach oben vorzeichenerweiterten Bits der einzelnen Faktoren im Zweierkomplement immer identische Werte aufweisen. In diesem Beispiel liefert die Summe über die letzten fünf Zeilen immer den negierten Wert der vierten Zeile, womit sich die Berechnung in diesem Fall auf die Summation von drei Zeilen und der Subtraktion der letzten Zeile und somit auf die Hälfte des obigen Aufwandes reduziert:\n\nDie Subtraktion in der letzten Zeile gilt unabhängig von den Vorzeichen der beiden Faktoren auch bei anderer Stellenanzahl und es muss keine Vorzeichenunterscheidung bei den Faktoren bzw. eine Vorzeichenkorrektur bei dem berechneten Produkt vorgenommen werden. Diese Subtraktion kann in schaltungstechnischen Realisierungen entweder durch Volladdierer, welche in den Subtraktionsmodus umgeschaltet werden können, erfolgen oder durch eine Invertierung der letzten Zeile und der zusätzlichen Addition von +1, analog wie bei der Bildung des Zweierkomplements.\n\nZur Verdeutlichung dieses optimierten Verfahrens eine Multiplikation mit unterschiedlichen Vorzeichen (−7) · 3 in Zweierkomplementdarstellung:\n\nWenn man eine Zahl von der Zweierkomplementdarstellung ins Dezimalsystem umcodieren will, muss man folgendermaßen (umgekehrt entsprechend der Umwandlung vom Dezimalsystem in die Zweierkomplementdarstellung) vorgehen:\n\n\nBeispiel:\n\nEine andere Vorgehensweise zur Umwandlung einer Zahl in Zweierkomplementdarstellung in das Dezimalsystem ist die folgende:\nHabe die Zahl in Zweierkomplementdarstellung formula_1 Stellen, gegeben sind also formula_1 Bits formula_30:\n\nIst formula_32 eine negative Zahl, so errechnet sich formula_32 in Zweierkomplementdarstellung (formula_18) mit formula_1 Stellen wie folgt:\nDementsprechend gilt auch\nwobei formula_38 der positiven Zahl entspricht und formula_39 bei der Rechnung als Übertrag in der formula_40-sten Stelle auftritt.\n\nFolgende Umformungen zeigen, dass diese alternative Wandlung korrekt ist. Ist formula_9 die Zweierkomplementdarstellung einer formula_1-stelligen negativen Zahl formula_32 und formula_44 das Suffix von formula_18, so errechnet sich der Wert formula_32 wie folgt:\nHierbei ist formula_48 die Interpretationsfunktion, die den Zahlenwert einer Zweierkomplementzahl ermittelt. Die zweite Zeile ergibt sich aus der Definition einer negativen Zweierkomplementzahl und die dritte aus der Konversion in die positive Darstellung der Zweierkomplementzahl, wobei formula_49 das Komplement von formula_50 sein soll. Die vierte Zeile folgt dann daraus, dass die Komplementbildung auch als Subtraktion von einem Eins-String der Länge formula_51 (formula_52) mit formula_50 dargestellt werden kann. Die letzte Zeile entspricht exakt der alternativen Wandlungsvorschrift und zeigt daher deren Korrektheit.\n\nDie Korrektheit dieses Verfahrens ist leicht einsichtig, wenn man in Betracht zieht, in welcher Reihenfolge die Zahlenwerte im Raum der Bitstrings bei einer formula_1-Bit-Zweierkomplementzahl verteilt sind:\n\nD. h. durch die Subtraktion der Zahl formula_56 von der Wertebereichsgrenze formula_57 erhält man bei Rechnung im Binärsystem die Codierung von formula_58 im Zweierkomplement.\n\nDie Zweierkomplementdarstellung kann auch bei Festkommazahlen angewandt werden, womit beispielsweise gebrochene Zahlen wie formula_59 binär dargestellt werden können. Festkommazahlen werden unter anderem im Bereich der digitalen Signalverarbeitung verwendet. Festkommazahlen werden allgemein durch ein Verschieben des Kommapunkts, der sich bei ganzen Zahlen immer rechts hinter der letzten Stelle befindet, gebildet. Dabei wird der Kommapunkt nicht in der Binärdarstellung gespeichert, sondern implizit seine Position als fix angenommen, wovon sich der Name der Festkommadarstellung ableitet.\n\nSomit bleiben die oben genannten Rechenregeln im Prinzip erhalten, lediglich die Werte verändern sich. Zur Bildung einer binären Zweierkomplementärdarstellung müssen sämtliche Binärstellen invertiert und anschließend der Wert einer Quantisierungsstufe 2 addiert werden. Dabei gibt k die Position der letzten darstellbaren binären Ziffer an. Bei obigen Ganzzahlen wäre das die Stelle k=0, womit bei der Bildung des Zweikomplements bei ganzen Zahlen nach der Invertierung der Wert 2=1 addiert werden muss. Ist der Kommapunkt beispielsweise um 2 Stellen nach links verschoben und umfasst das binäre Wort die beiden Stellen rechts vom Kommapunkt, wäre k=−2, und somit muss zur Bildung des Zweierkomplements 2=0,25 addiert werden. (Hinweis: Der Kommapunkt kann bei Festkommazahlen auch außerhalb des darstellbaren Wertebereiches liegen.)\n\nEin Beispiel soll das verdeutlichen: Eine binäre Zahl mit fünf Bit Wortlänge besitzt drei Vorkommastellen und zwei Nachkommastellen. Damit kann der Wertebereich −4 bis +3,75 in Schritten von 0,25 dargestellt werden. Die Zahl 2,25 entspricht der binären Zahl 010,01. Wird nun das Zweierkomplement davon gebildet, werden alle Stellen der binären Zahl invertiert und 2=0,25 addiert, was 101,11 = −2,25 ergibt.\n\nAuch in anderen Stellenwertsystemen kann man ganze Zahlen ohne Verwendung eines Minuszeichens darstellen. Man hat hier aber das Problem, dass die Unterscheidung von positiven und negativen Zahlen mehr oder weniger willkürlich vereinbart sein kann.\n\nBeschränkt man sich auf formula_1-stellige Zahlen zur Basis formula_61, dann kann man die natürlichen Zahlen von 0 bis formula_62 darstellen. Legt man eine Zahl in diesem Bereich als die größte positive Zahl fest, dann kann man jede größere Zahl formula_32 als Zweierkomplementdarstellung der negativen Zahl formula_64 auffassen.\n\nDie Rechenoperation der Negation wird analog durchgeführt wie zur Basis 2: Jede Ziffer formula_65 wird durch formula_66 ersetzt, und zur so entstehenden Zahl wird 1 addiert.\n\nFür die Basis formula_67 und die Stellenzahl formula_68 erhält man für −1 diese Darstellung:\nDamit wird −1 als 444 dargestellt. Die Addition 444 + 001 (zur Basis 5 und Stellenzahl 3) ergibt 000, da der letzte Übertrag wegfällt.\n\nLegen wir in diesem Beispiel die größte positive Zahl als 222 fest (zur Basis 5, dezimal hat diese Zahl den Wert +62), dann ist 223 = −222 die kleinste negative Zahl (dezimal −62). Der Zahlenbereich reicht also von dezimal −62 bis +62.\n\nZur Basis 10 und Stellenzahl 2 hat man 99 = −01 und 50 = −50, hier hat man also wie zur Basis 2 eine weitere Zahl neben der 0, die mit ihrer Zweierkomplementdarstellung übereinstimmt. Dieses Phänomen tritt mit jeder geraden Basis auf.\n\nVerallgemeinert man diese Schreibweise weiter, indem man unendlich viele Stellen zulässt, erhält man die Möglichkeit, p-adische ganze Zahlen darzustellen.\n\n\n"}
{"id": "49157", "url": "https://de.wikipedia.org/wiki?curid=49157", "title": "Gleitkommazahl", "text": "Gleitkommazahl\n\nEine Gleitkommazahl – häufig auch Fließkommazahl genannt (, wörtlich \"Zahl mit flottierendem Punkt\" oder auch [wohl weiter lehnübersetzt] \"Gleitpunktzahl\") – ist eine approximative Darstellung einer reellen Zahl.\n\nDie Menge der Gleitkommazahlen ist eine Teilmenge der rationalen Zahlen. Zusammen mit den auf ihnen definierten Operationen (Gleitkommaarithmetik) bilden die Gleitkommazahlen eine endliche Arithmetik, die vor allem im Hinblick auf numerische Berechnungen mit (binären) Rechnern entwickelt wurde.\n\nAlle (mechanischen oder elektronischen) Rechenhilfsmittel vom Abakus bis zum Computer verwenden als einfachste Form der Zahldarstellung Festkommazahlen. Dabei wird eine meistens begrenzte Ziffernfolge gespeichert und an festgelegter Stelle das Komma angenommen.\n\nBei größeren Rechnungen treten unweigerlich Überläufe auf, die eine Skalierung der Werte und eine erneute Durchrechnung erforderlich machen, um Endergebnis und alle Zwischenergebnisse in den erlaubten Wertebereich zu bringen. Diese Skalierung ist zeitraubend und muss automatisiert werden.\n\nEin naheliegender und direkt zu Gleitkommazahlen führender Gedanke ist dabei, bei jedem Wert die genaue Stelle des Kommas zusätzlich zu speichern. Das bedeutet mathematisch nichts anderes als die Darstellung der Zahl formula_1 mit zwei Werten, der Mantisse formula_2 und dem Exponenten formula_3: formula_4. Die Freiheit bei der Wahl des Exponenten kann genutzt werden, um die Mantisse in einen festgelegten Wertebereich, zum Beispiel formula_5 zu bringen. Dieser Schritt heißt Normalisierung der Mantisse.\n\nBeispiel: Der Wert der Lichtgeschwindigkeit beträgt \nNur die Mantisse der letzten Darstellung ist normalisiert.\n\nDiese Schreibweise wurde von Physikern und Mathematikern schon seit langem verwendet, um sehr große und sehr kleine Zahlen angeben zu können. Noch heute wird deshalb die Gleitkommaschreibweise auf Taschenrechnern speziell als \"wissenschaftliches Format (sci)\" bezeichnet.\n\nEine kompakte Variante der Exponentialschreibweise verwenden häufig Programmierer für die Ein- und Ausgabe von Zahlen in Textform, z. B. im Quelltext oder im Debugger: 2.99792458e8 (= 2,997.924.58 · 10) oder 3.141592653589793d0 (\"d\" wird bei Zahlen in doppelter Genauigkeit benutzt). Das \"e\" bzw. \"d\" ist also als Kurzform von „mal 10 hoch“ zu verstehen.\n\nBei Rechnungen mit Gleitkommazahlen wird jede Zahl und jedes Zwischenergebnis individuell skaliert (im Gegensatz zu einer globalen Skalierung). Die Skalierung (Berechnung des Exponenten) jedes Zwischenergebnisses erfordert zusätzlichen Rechenaufwand und wurde deshalb bis weit in die 1980er Jahre nach Möglichkeit vermieden. So hatten die damaligen PCs standardmäßig keinen Gleitkommaprozessor. Ein weiterer Faktor war der höhere Speicherbedarf von Gleitkommazahlen, der nur durch Verzicht auf höhere Genauigkeit eingeschränkt werden konnte. Dementsprechend hatten zunächst nur die Höchstleistungsrechner (number cruncher) eine Gleitkommaarithmetik oder wenigstens eine Hardwareunterstützung einer Software-Gleitkommaarithmetik.\n\nDie Wahl der Basis 10 ist willkürlich und nur dem üblichen Zehnersystem geschuldet. Gleitkommazahlen können mit beliebigen Basen dargestellt werden, im Allgemeinen gilt formula_7 mit einer beliebig gewählten Basis formula_8. Rechenanlagen verwenden formula_9 (vorherrschend), formula_10 (heute selten) oder formula_11 (z. B. für Finanzmathematik, siehe unten). Bei beliebiger Basis ist die Bedingung für normalisierte Zahlen formula_12.\n\nDie erste dokumentierte Verwendung der Gleitkommaschreibweise liegt etwa 2700 Jahre zurück: Im Zweistromland (Mesopotamien) wurden wissenschaftliche Rechnungen mit der Basis formula_13 durchgeführt und der Exponent (eine meistens kleine ganze Zahl) einfach im Kopf mitgeführt. Dasselbe Vorgehen war bis vor kurzer Zeit bei Berechnungen mit einem Rechenschieber üblich.\n\nIn Rechenautomaten wurde erstmals von Konrad Zuse eine eigene Gleitkommadarstellung für seine Computer Z1 und Z3 verwendet.\n\nIm vorigen Abschnitt wurden die grundlegenden Parameter einer Gleitkommazahl bereits vorgestellt. Es sind Basis formula_8, Anzahl der Mantissenstellen formula_15 und Anzahl der Exponentenstellen formula_16. Hinzu kommen weitere Parameter, die beim Rechnen die Rechenoperationen erleichtern sollen. In diesem Abschnitt werden Parameter und Bitfelder einer allgemeinen Gleitkommazahl kurz beschrieben.\n\nformula_17\n\nEin Parameter ist die gewählte Basis formula_18. Zahlen, die von Menschen direkt verarbeitet werden, verwenden entweder formula_11 oder formula_20. In diesem speziellen Fall verwendet man für den Exponenten die Vorsätze Kilo=1000, Mega=1000, Giga=1000, Tera=1000 und Milli=1000, Mikro=1000, Nano=1000, Piko=1000 des internationalen Einheitensystems.\n\nIm Computer haben sich das Dualsystem und seine Verwandten durchgesetzt und es sind die Basen formula_9, formula_22 und formula_10 üblich. Seit der Norm für Gleitkommazahlen IEEE 754 wird in modernen Computern fast ausschließlich die Basis formula_9 verwendet.\n\nformula_25\n\nDie Mantisse formula_26 enthält die Ziffern der Gleitkommazahl. Speichert man mehr Ziffern ab, so erhöht sich die Genauigkeit. Die Anzahl formula_15 der Mantissenziffern drückt also aus, wie exakt die Zahl approximiert wird. Dieser Gleitkommaparameter wird entweder direkt angegeben oder auch in Form der kleinsten Zahl formula_28 beschrieben, die zu 1 addiert werden kann und ein von 1 verschiedenes Ergebnis liefert (formula_29; formula_30 minimal!) (s. u. bei Eigenschaften).\n\nBeispiel: Bei IEEE-754-Zahlen vom Typ Single mit der Basis formula_9 ist die Mantisse formula_32 Stellen lang. Hier ist formula_33 1,19209289551e−0007.\n\nformula_34\n\nDer Exponent formula_35 speichert nach Normalisierung die genaue Stelle des Kommas und damit die Größenordnung der Zahl. Die Anzahl formula_16 der Exponentenziffern beschränkt die Variationsmöglichkeiten des Kommas und beschreibt damit den Wertebereich der dargestellten Gleitkommazahlen. Um ein System von Gleitkommazahlen zu beschreiben, gibt man den kleinsten und den größten möglichen Exponenten an oder auch die Anzahl der Exponenten und die Verschiebung zur 0 \"(Bias)\".\n\nBeispiel: Bei IEEE-754-Zahlen vom Typ Single mit der Basis formula_9 ist der kleinste mögliche Exponent −126 und der größte 127. Damit ist die größte in diesem System darstellbare Gleitkommazahl formula_38 und die kleinste normalisierte Gleitkommazahl formula_39. Diese Größen, formula_40 und formula_41, beschreiben den zulässigen Wertebereich.\n\nformula_42\n\nDie Darstellung einer Gleitkommazahl ist zunächst nicht eindeutig bestimmt. Die Zahl 2 kann als formula_43 oder auch formula_44 geschrieben werden.\n\nUm die Benutzung einer eindeutig bestimmten Darstellung zu erzwingen, werden daher oft \"normalisierte Gleitkommazahlen\" verwendet, bei denen die Mantisse in einen definierten Bereich gebracht wird. Zwei naheliegende Normalisierungsbedingungen sind formula_45 und formula_12. Die Zahl 2 würde man also nach der ersten Regel als formula_44 schreiben, die Darstellung formula_43 wäre dann nicht erlaubt. Das Rechnen mit normalisierten Zahlen ist einfacher, weshalb in der Vergangenheit manche Implementatoren einer Gleitkommaarithmetik nur normalisierte Zahlen zuließen. Die Zahl 0 kann allerdings nicht normalisiert dargestellt werden.\n\nDabei unterscheidet man – in Bezug zur üblichen Basis 10 im Zahlensystem:\n\n\nIn Gleitkommasystemen ist der Exponent eine Zahl mit Vorzeichen. Das macht die Implementierung einer zusätzlichen ganzzahligen Arithmetik mit Vorzeichen für Exponentenberechnungen erforderlich. Dieser zusätzliche Aufwand kann vermieden werden, wenn zum Exponenten formula_3 eine feste Zahl formula_53, der Biaswert oder Exzess, addiert wird und statt des Exponenten formula_3 die Summe formula_55 gespeichert wird. Diese Summe ist dann eine vorzeichenfreie positive Zahl. Meistens wird die Verwendung eines Bias formula_53 mit der Darstellung der 0 durch formula_57 kombiniert.\n\nEine heute selten anzutreffende Alternative ist die Darstellung des Exponenten im Zweierkomplement, im Einerkomplement oder als Betrags-Vorzeichenzahl.\n\nDer Vorteil der Biased-Darstellung besteht darin, dass auf diese Weise ein Größenvergleich zwischen zwei positiven Gleitkommazahlen erleichtert wird. Es genügt, die Ziffernfolgen \"em\", also jeweils Exponent \"e\" gefolgt von Mantisse \"m\", lexikografisch miteinander zu vergleichen.\nEine Gleitkomma-Subtraktion mit anschließendem Vergleich auf Null wäre weitaus aufwändiger. Der Nachteil der Biased-Darstellung gegenüber der Zweierkomplement-Darstellung besteht darin, dass nach einer Addition zweier Biased-Exponenten der Bias subtrahiert werden muss, um das richtige Ergebnis zu erhalten.\n\nIEEE 754 verwendet die Darstellung mit B=127 bei Single und B=1023 bei Double.\n\nDas Vorzeichen v einer Gleitkommazahl (+ oder −; auch +1 oder −1) kann immer in einem Bit kodiert werden. Meistens wird das Bit formula_58 für positive Zahlen (+) und das Bit formula_59 für negative Zahlen (−) verwendet. Mathematisch kann man schreiben formula_60\n\nIn den letzten Jahren hat sich die folgende Kurzdarstellung der wesentlichen Parameter formula_8, formula_15, formula_16 und formula_53 eines Gleitkommasystems durchgesetzt. Dabei schreibt man durch Punkte getrennt die Größen 1, formula_16, formula_15, formula_53 und formula_8 in genau dieser Reihenfolge auf. Die 1 ist dabei die Anzahl der Vorzeichenbits. Eine IEEE-754-Single-Zahl mit 1 Vorzeichenbit, 8 Exponentenbits und 23 Mantissenbits ist also eine 1.8.23.127.2 Gleitkommazahl. Geht die Basis formula_8 und der Bias formula_53 aus dem Zusammenhang hervor, kann beides weggelassen werden und man spricht von einer 1.8.23-Gleitkommazahl. \n\nEine zweite gängige Schreibweise lässt das Vorzeichenbit weg und gibt nur Mantissenlänge und Exponentenlänge an: s23e8.\n\nMit diesen Schreibweisen gilt für IEEE-754-Zahlen:\n\nBei der Darstellung normalisierter Mantissen im Binärsystem kann ein Bit eingespart werden. Da die erste Stelle einer normalisierten Zahl immer ungleich 0 ist, ist diese Stelle im Binärsystem immer gleich 1. Eine Ziffer mit dem festen Wert 1 muss nicht mehr explizit gespeichert werden, da sie implizit bekannt ist. Wird diese Möglichkeit genutzt, wird von einem \"\" (engl. wörtlich „verstecktes \"Bit\"“) gesprochen. Das erwähnte IEEE-Format für Gleitkommazahlen macht von dieser Einsparungsmöglichkeit Gebrauch, nicht jedoch das interne 80-Bit-Format der Intel-CPUs.\n\nSoll beispielsweise die Zahl 5,25 nach IEEE 754 in ein short real (single precision number) umgewandelt werden, wird nach dem Zwischenschritt der Binärwandlung zu 101,01 das Komma zweimal linksseitig verschoben, sodass eine normierte binäre Darstellung mit 1,0101e2 gegeben ist. Aufgrund des hidden bit wird lediglich die Zahlenfolge 0101 in die 23-stellige Mantisse übernommen. Die Verwendung eines hidden bit erzwingt allerdings eine gesonderte Darstellung der Null, da jede Mantisse aufgrund des hidden bit einen Wert größer 0 repräsentiert.\n\nGleitkommazahlen warten besonders für den mathematischen Laien mit einigen Überraschungen auf, die auch oft das Ergebnis von Taschenrechner- und Computerrechnungen beeinflussen. Am wichtigsten sind außer Kraft gesetzte geläufige mathematische Rechenregeln. Wer intensiv mit einem Rechenhilfsmittel arbeitet, muss diese Eigenschaften kennen. Sie gehen auf die begrenzte Genauigkeit zurück, mit der Mantisse und Exponent gespeichert werden. Die Konsequenz dieser Begrenzung wird klar, wenn man sich überlegt, dass die unendlich vielen reellen Zahlen durch endlich viele Ziffernkombinationen dargestellt werden sollen. Man kann sich die Gleitkommazahlen im Definitionsbereich eines Systems als lange Tabelle diskreter Werte vorstellen. Eine Gleitkommafunktion ordnet dann jedem Wert dieser Liste einen anderen Wert zu. Analoges gilt für zwei- und mehrstellige Operationen. Im Artikel Minifloats sind die entsprechenden Wertebereiche grafisch dargestellt.\n\nDaraus resultiert die leichte bis absolute Ungenauigkeit der Rechnungen und die außer Kraft gesetzte Gültigkeit geläufiger mathematischer Rechenregeln.\n\nUnter Auslöschung versteht man den Effekt, dass bei der Subtraktion fast gleich großer Zahlen das Ergebnis falsch wird.\n\nBeispiel:\n\nSubtrahiert man formula_71 und die Zahl 3,141 in einer vierstelligen Gleitkommaarithmetik (formula_11, formula_73), so erwartet der unbefangene Laie als korrekt gerundetes Ergebnis formula_74.\n\nTatsächlich erhält man als Ergebnis formula_75:\nDer vierstellige gerundete Wert von formula_71 ist formula_77, damit wird das Ergebnis der Rechnung zu formula_78. Zu diesem Ergebnis kommt es, da schon die Ausgangsgrößen, insbesondere formula_79 in der Gleitkommaarithmetik dargestellt sind und eben nicht exakt vorliegen.\n\nDie Addition bzw. Subtraktion einer betragsmäßig viel kleineren Zahl ändert die größere Zahl nicht.\n\nIm Beispiel der vierstelligen Dezimalarithmetik (formula_11, formula_73) ändert die Addition von 0,001 zu 100 am größeren Operanden nichts. Dasselbe gilt für die Subtraktion:\n\n(Die Ziffern hinter dem Strich | entfallen bei der Skalierung)\n\nDa es in der Gleitkommadarstellung eine kleinste positive Zahl gibt, unterhalb derer kein Wert mehr dargestellt werden kann, wird ein Ergebnis in diesem Bereich meistens durch 0 repräsentiert. In diesem Fall spricht man von einem Unterlauf (engl. underflow). Handelt es sich um ein Zwischenergebnis, so ist an diesem Punkt jede Information über das Ergebnis verloren gegangen. In manchen Fällen wird die Genauigkeit des Endergebnisses davon nicht berührt, aber in anderen Fällen kann das resultierende Endergebnis auch komplett falsch sein.\n\nDie Addition und die Multiplikation von Gleitkommazahlen ist nicht assoziativ, das heißt im Allgemeinen gilt:\n\nDie Addition und Multiplikation von Gleitkommazahlen ist auch nicht distributiv, das heißt im Allgemeinen gilt:\n\nIn einer Gleitkommaarithmetik haben manche normalerweise unlösbare Gleichungen eine Lösung. Dieser Effekt wird sogar ausgenutzt, um ein solches Gleitkommasystem zu beschreiben.\n\nBeispiel:\n\nIm Bereich der reellen Zahlen hat die Gleichung formula_87 für formula_88 keine Lösung.\n\nIn einer Gleitkommaarithmetik hat diese Gleichung viele Lösungen, nämlich alle Zahlen, die zu klein sind, um bei der Summe noch einen Effekt zu ergeben. Wieder mit dem Beispiel vierstelliger Dezimalbrüche (formula_11, formula_73) gilt (Der Strich | markiert die bei der Addition entfallenden Stellen):\n\n\nDie schon oben erwähnte kleinste Zahl formula_28, die zu 1 addiert werden kann und ein von 1 verschiedenes Ergebnis liefert (formula_92; formula_28 minimal!) nennt man Maschinengenauigkeit.\n\nWenn die Basis verschieden von 10 ist, müssen die Zahlen zwischen dem vorliegenden Gleitkommasystem und dem Dezimalsystem konvertiert werden, um eine menschenlesbare Darstellung zu erhalten. Das wird meist schnell (und oft ungenau) programmiert. Eine schon alte und wichtige Forderung an diese Konversion ist ihre bitgenaue Umkehrbarkeit. Ein im Dezimalsystem dargestelltes Ergebnis soll wieder eingelesen werden können und bitgenau dieselbe Darstellung im Gleitkommasystem reproduzieren.\n\nDiese Forderung wird häufig nicht beachtet. Eine Ausnahme ist hier Java, das den folgenden Satz beachtet:\n\nSatz: Man kann zeigen, dass es nicht ausreicht, die aufgrund der Mantissengenauigkeit berechnete Anzahl der Dezimalstellen aufzurunden und diese Dezimalstellen gerundet zu produzieren. Eine einzige weitere Stelle reicht jedoch aus (Theorem 15). Das ist der Grund, warum in der Darstellung reeller Zahlen, die von Java-Programmen produziert werden, immer eine zusätzliche und scheinbar überflüssige Stelle erscheint.\n\nSchon einfache Dezimalbrüche wie 0,1 können nicht exakt als binäre Gleitkommazahlen dargestellt werden, da jede rationale Zahl, deren gekürzter Nenner keine Zweierpotenz ist, im Binärsystem zu einer nicht abbrechenden, periodischen Darstellung führt. Von dieser werden nur die ersten formula_15 Ziffern gespeichert, wodurch Ungenauigkeit entsteht. Dezimal 0,1 ist binär 0,0001100110011… Allerdings wurde für binäre Gleitkommasysteme mit entsprechenden Rundungsregeln bewiesen, dass die Darstellung von 0,1 multipliziert mit 10 wieder exakt 1 ergibt. Allgemein gilt bei richtiger Rundung (m / 10) · 10 = m (Goldbergs Theorem 7 für den konkreten Fall n = 2 + 2 = 10).\n\nIn Disziplinen wie der Finanzmathematik werden oft Ergebnisse verlangt, die mit einer Handrechnung exakt übereinstimmen. Das geht nur mit einer dezimalen Gleitkommaarithmetik oder – mit einigen „Verrenkungen“ – mit einer Festkommaarithmetik.\n\nDie im Abschnitt Dezimalbrüche genannte Einschränkung, dass viele dieser Dezimalzahlen im Binärsystem eines Computers nicht genau dargestellt werden können, hat beim Programmieren Auswirkungen auf Vergleiche formula_95 zwischen Gleitkommazahlen. Ein Beispiel in der Sprache \"C\" soll dies verdeutlichen:\nint main(void) {\n\nObwohl die beiden Gleichungen formula_96 und formula_97 mathematisch korrekt sind, werden sie wegen der ungenauen Umrechnung ins Computer-Binärsystem falsch. Im Beispiel-Programm werden somit beide Ungleichungen als wahr angesehen.\n\nVergleiche müssen deshalb durch eine Abfrage ersetzt werden, ob die zu vergleichenden Werte im Rahmen einer erreichbaren Genauigkeit formula_98 (meist \"Toleranz\" genannt) als gleich angesehen werden können.\n\nToleriert man beim Vergleich einen absoluten Fehler, lautet eine mögliche Formulierung formula_99.\n\nToleriert man beim Vergleich einen relativen Fehler, lautet eine mögliche Formulierung formula_100. Der zweite Fall muss meist noch mit der Sonderfallabfrage formula_101 verbunden werden.\n\nAlternativ kann man alle Faktoren respektive Summanden inklusive des zu erwartenden Ergebnisses eben jener problematischen Vergleiche bei rationalen Gleitkommazahlen formula_102 auch mit formula_103 multiplizieren, wobei formula_2 den Index der letzten Nachkommastelle angibt. Allgemeiner gesagt: Alle Gleitkommazahlen müssen – sofern möglich – in Brüche umgewandelt werden. Diese können wiederum eindeutig und ohne Rundungen der Mantisse in das binäre Zahlensystem konvertiert werden. Compiler mancher Programmiersprachen (z. B. Java, Ruby, C++, Objective-C, Swift, Rust usw.) können direkt mit den Brüchen rechnen und vergleichen die Brüche in den oben aufgeführten bedingten Anweisungen (If-Anweisungen), welche dadurch nicht betreten werden. Andere Programmiersprachen (z. B. Object Pascal, PHP, JavaScript, Perl, Python usw.) wiederum wandeln den Bruch bzw. den Quotient als allerersten Schritt wieder in eine Binärzahl um und vergleichen dann erst beide Werte, wodurch in dem Fall beide Bedingungen wahr sind und die oben angegebenen Ausgaben getätigt werden.\n\nSogar Zahlen mit exakt denselben Bitmustern und somit eigentlich exakt identischen Werten werden vom Rechner mit formula_95 manchmal nicht als gleich angesehen. Das hat als Ursache die manchmal nicht identischen Formate im Speicher (Bsp.: Intel 64 Bit) und während einer Rechnung in der Gleitpunkteinheit (Bsp.: Intel 80 Bit). Wenn dieselben Bitmuster, die verglichen werden sollen, einmal aus dem Speicher und somit gerundet und einmal aus der FPU und somit mit der vollen Genauigkeit kommen, führt ein Vergleich zum falschen Ergebnis. Die Abhilfe ist dieselbe wie schon beschrieben. Bei größer/kleiner Vergleichen kann dieses Problem auch auftauchen, je nach verwendeter Sprache und Architektur sind spezielle Befehle und/oder ein Umweg über den Arbeitsspeicher zu nehmen um dies zu lösen.\n\nManche Rechenanlagen verwenden beim Rechnen mehrere verschiedene Formate. Bei Intel und verwandten Prozessoren wie AMD rechnet die FPU mit einem 80-Bit-Format. Gespeichert werden die Zahlen mit einem IEEE-754-kompatiblen 64-Bit- bzw. 32-Bit-Format. Bei Verwendung von mmx/sse-Extensions werden wieder andere Rechenformate verwendet. Das führt zu weiteren für Laien zunächst sehr undurchsichtigen Eigenschaften. Ein simpler Vergleich identischer Bitmuster auf Gleichheit kann zum Ergebnis führen, dass die scheinbar identischen Bitmuster doch verschieden sind. Das folgende Programm liefert manchmal paradoxe Ergebnisse, wenn man es mit dem gleichen Wert für x und y aufruft:\nvoid vergleiche (double x, double y){\ndouble z = 0.2; // der Wert ist nicht wichtig\nvergleiche (cos (z), z);\n\nDie Erklärung für dieses Verhalten ist, dass der Compiler zwei unabhängige cos-Berechnungen generiert, die eine vor dem Aufruf von vergleiche, die andere in vergleiche. Die Variable x empfängt den cos(z) mit 64 Bit. Die Berechnung des cos(y) kann bei Intel mit 80 Bit erfolgen; die beiden Ergebnisse sind unterschiedlich, wenn cos(y) nicht im Speicher, sondern direkt vom 80-bittigen Arbeitsregister mit der 64-bittigen Variablen x verglichen wird.\n\nOben erwähnte Beispiele sind durchwegs im Dezimalsystem angegeben, das heißt mit einer Basis \"b\" = 10. Computer verwenden stattdessen das Binärsystem mit einer Basis \"b\" = 2.\n\nGleitkommazahlen werden in Computern normalerweise als Folgen von 32 Bit \"(einfache Genauigkeit, )\" oder 64 Bit \"(doppelte Genauigkeit, )\" dargestellt.\n\nManche Prozessoren erlauben auch längere Gleitkommazahlen, so kennen die von der Intel-x86-Serie abgeleiteten Prozessoren (u. a. Intel Pentium und AMD Athlon) eine Gleitkommazahldarstellung mit 80 Bit für Zwischenergebnisse. Manche Systeme erlauben auch Gleitkommazahlen mit 128 Bit \"(Vierfache Genauigkeit)\". Einige ältere Systeme verwendeten auch noch andere Längen wie zum Beispiel 36 Bit (IBM 704, PDP-10, UNIVAC 1100/2200 series), 48 Bit (Burroughs) oder 60 Bit (CDC 6600),\n\nDaneben gibt es auch \"Minifloats\" genannte Systeme mit sehr wenigen Bits (etwa 8 oder 16), die in speicherarmen Systemen (Controllern) oder limitierten Datenströmen eingesetzt werden (zum Beispiel Grafikkarten).\n\nDas heute häufigste und bekannteste Gleitkommasystem wurde 1985 von IEEE konzipiert, in der \"IEEE 754\" niedergelegt, und ist in den meisten Computern als Hardware- oder Softwarearithmetik vorhanden. \"IEEE 854\" ist eine Norm für \"Gleitkomma-Dezimalzahlen\" oder Dezimalbrüche. Beide Normen werden in der Revision \"IEEE 754r\" zusammengeführt und erweitert.\n\nDie IEEE hat die Darstellung von Gleitkommazahlen in ihrem Standard IEEE 754 seit 1985 reglementiert; beinahe alle modernen Prozessoren folgen diesem Standard. Gegenbeispiele, die die Spezifikationen der Norm IEEE 754 nicht erfüllen, sind einige IBM-Großrechnersysteme (Hexfloat-Format), die VAX-Architektur und einige Supercomputer wie die von Cray. Die Sprache Java lehnt sich eng an IEEE 754 an, erfüllt die Norm aber nicht komplett.\n\nDie Definition des Hexfloat-Formates von IBM findet sich im Buch „Principles of Operation“ der z-Architektur.\n\nDer Power6 von IBM ist einer der ersten Prozessoren, welche dezimale Gleitkommaarithmetik hardwaremäßig implementiert haben; die Basis ist also 10. Im Folgenden wird aber nur die Basis 2 behandelt.\n\nStrenggenommen sind nur die normalisierten Zahlen aus IEEE 754 Gleitkommazahlen. Die denormalisierten Zahlen sind eigentlich Festkommazahlen; diese Sonderfälle wurden für spezielle numerische Zwecke geschaffen.\n\nDie tatsächliche Darstellung im Computer besteht also aus einem Vorzeichen-Bit, einigen Exponenten-Bits und einigen Mantissen-Bits. Wobei die Mantisse meistens normiert ist und Zahlen im Intervall <nowiki>[1; 2[</nowiki> darstellt. (Da in diesem Intervall das erste Bit mit der Wertigkeit \"Eins\" stets gesetzt ist, wird es meistens implizit angenommen und nicht gespeichert, siehe Hidden Bit). Der Exponent wird meistens im Biased-Format, oder auch im Zweierkomplement dargestellt. Des Weiteren werden zur Darstellung besonderer Werte (Null, Unendlich, Keine Zahl) meistens einige Exponentenwerte, zum Beispiel der größtmögliche und der kleinstmögliche Exponent, reserviert.\n\nEine Zahl \"f\" wird demzufolge als \"f\" = \"s\" · \"m\" · 2 dargestellt, wobei \"s\" 1 oder −1 ist.\n\nDurch die unterschiedliche binäre Darstellung der Zahlen kann es in beiden Systemen zu Artefakten kommen. Das heißt: Rationale Zahlen, die im Dezimalsystem „rund“ erscheinen, zum Beispiel formula_106, können im Binärsystem nicht exakt dargestellt werden (der Wert ist formula_107). Stattdessen wird ihre Binärdarstellung im Rahmen der jeweiligen Rechengenauigkeit gerundet, so dass man bei der Rückumwandlung ins Dezimalsystem z. B. den Wert 12,44999999900468785 erhält. Dieses kann in nachfolgenden Berechnungen zu unvorhergesehenen Ab- oder Aufrundungsfehlern führen.\n\nDie oben erwähnten Artefakte sind im Binärsystem unvermeidlich, da unendlich viele Zahlen, die im Dezimalsystem exakt dargestellt werden können, im Binärsystem periodische Zahlen mit unendlich vielen Nachkommastellen sind. Sie könnten nur durch die Verwendung von Codierungen mit der Basis 10 (oder anderer Basen der Form formula_108 mit beliebigem formula_109) vermieden werden, siehe auch BCD-Code. Binäre Gleitkommazahlen werden jedoch nach wie vor aus verschiedenen Gründen eingesetzt.\n\nAllgemein gibt es für jede Basis \"d\" unendlich viele rationale Zahlen, die zu einer anderen Basis eine endliche Darstellung (0-Periode) haben und zur Basis \"d\" eine unendliche Darstellung mit Periode. Das Dezimalsystem ist hier nur dadurch ausgezeichnet, dass Menschen daran gewöhnt sind, und daher für Eingabe- und Ausgabeformat der Rechnungen oft das Dezimalsystem bevorzugt wird.\n\nIn der Mathematik ist ein Gleitkommazahlensystem ein Tupel formula_110, wobei formula_111 die Basis, formula_112 den Bereich des Exponenten und formula_113 die Länge der Mantisse darstellt.\n\nDamit ist eine reelle Zahl \"x\" ≠ 0 darstellbar durch ein \"a\" und ein \"e\", so dass: formula_114 und formula_115 mit formula_116.\n\nHiermit ist eine mathematische Betrachtung des Rundungsfehlers möglich. Die obige Darstellung realisiert eine Projektion\n\nund damit ist der Rundungsfehler definiert als\n\nBei double-Werten entspricht formula_98 gerade formula_120 (ungefähr formula_121).\n\n 18,4 soll in eine Gleitkommazahl umgewandelt werden,\n\n1. Berechnung des Exzesses\n\n2. Umwandlung eines Dezimalbruches in eine duale Festkommazahl ohne Vorzeichen\n\n3. Normieren\n\n4. Berechnung des dualen Exponenten\n\n5. Vorzeichen-Bit bestimmen\n\n6. Die Gleitkommazahl bilden\n\nHier werden die genauen Rechenschritte vorgestellt, um einen Dezimalbruch in eine binäre Gleitkommazahl vom Typ \"Single\" nach IEEE 754 umzuwandeln. Dazu müssen nacheinander die drei Werte (Vorzeichen formula_122 (1 bit), Mantisse formula_2 und Exponent formula_3) berechnet werden, aus denen sich die Zahl formula_1 zusammensetzt:\n\nformula_126\n\n\nJe nachdem, ob die Zahl positiv oder negativ ist, ist formula_122 0 oder 1: formula_128 oder formula_129\n\nAlle weiteren Berechnungen erfolgen mit dem Betrag der Zahl.\n\n\nAls Nächstes wird der Exponent gespeichert. Beim IEEE \"single\"-Datentyp sind dafür 8 Bit vorgesehen.\nDer Exponent muss so gewählt werden, dass die Mantisse einen Wert zwischen 1 und 2 erhält:\n\nformula_130\n\nWenn hierbei ein Wert für den Exponenten herauskommt, der kleiner −126 oder größer 127 ist, kann die Zahl mit diesem Datentyp nicht gespeichert werden. Stattdessen wird die Zahl als 0 (Null) oder als „unendlich“ abgespeichert.\n\nDer Wert für den Exponenten wird jedoch nicht direkt gespeichert, sondern um einen Bias-Wert erhöht, um negative Werte zu vermeiden. Bei IEEE \"single\" ist der Bias-Wert 127. Somit werden die Exponentenwerte −126…+127 als sogenannte „Charakteristik“ zwischen 1…254 gespeichert. Die Werte 0 und 255 als Charakteristik sind reserviert für die speziellen Zahlenwerte „Null“, „Unendlich“ und „NaN“.\n\n\nDie Mantisse wird nun in den verbleibenden 23 Bit abgespeichert:\n\nformula_131\n\n\nZahl = +11,25\n\nVorzeichen = + → 0\n\nformula_132 → 3 + 127 = 130 → 10000010\n\nformula_133 → 01101000000000000000000\n\nDamit ergibt sich folgende Gleitkommazahl einfacher Genauigkeit:\n\n0 10000010 01101000000000000000000\n\n\nWill man aus einer Gleitkommazahl im Maschinenwort (32 Bit) eine Dezimalzahl errechnen, so kann man dies mit folgender Formel recht schnell erledigen:\n\nformula_134\n\n\nUm aus einer Gleitkommazahl im Maschinenwort (64 Bit) eine Dezimalzahl zu errechnen, kann man die folgende Formel verwenden:\n\nformula_135\n\nBeispiel:\n\nFolgende Binärzahl mit 64-Bit soll als Gleitkommazahl interpretiert werden:\n\n0 10001110100 0000101001000111101011101111111011000101001101001001\n\nBit 63 repräsentiert das Vorzeichen (1 Bit), also\n\nBit 62 bis 52 repräsentieren den Exponenten (11 Bit), also:\n\nBit 51 bis 0 repräsentieren die Mantisse (52 Bit), also:\n\nEingesetzt in die Formel ergibt sich als Ergebnis (Rundungswerte):\n\n\n"}
{"id": "49539", "url": "https://de.wikipedia.org/wiki?curid=49539", "title": "Paradox (Datenbank)", "text": "Paradox (Datenbank)\n\nParadox ist eine Datenbank-Entwicklungsumgebung bzw. ein dateibasiertes Datenbankformat der Firma Borland bzw. Corel, welches dBASE III, dBASE 3+ und dBASE IV sehr ähnlich und seit den 1980er Jahren auf dem Markt erhältlich ist.\n\nParadox ist für den Betrieb mit der Borland Database Engine (BDE) vorgesehen. Diese ist selbst nicht Teil von Paradox, sondern eine eigenständige Bibliothek und wird in vielen anderen Softwareprodukten eingesetzt.\n\nDie BDE stellt den Zugriff auf eine relationale Datenbank als Datenbankschnittstelle bereit. Im Falle von Paradox als Datenbankformat belegt dabei jede Tabelle (*.DB) und jeder Index eine (Primärindex *.PX) bzw. zwei (pro Sekundärindex jeweils *.XG? und *.YG?) eigene Datei(en). Enthält eine Tabelle Memo-Felder, Gültigkeitsprüfungen oder Formatierungsinformationen, werden diese ebenfalls in speziellen Files (*.MB, *.VAL, *.TV) abgelegt. Beim Mehrbenutzerbetrieb liegen diese Dateien auf einem Netzlaufwerk, während die BDE-Instanzen der teilnehmenden Rechner den gemeinsamen Zugriff miteinander durch eine Netzsteuerdatei („PDOXUSRS.NET“) und Sperrdateien (*.LCK) koordinieren. Paradox bzw. die DB-Engine beherrscht sogenanntes „Record-Locking“ (Sperren einzelner Datensätze) und ist daher – im Gegensatz zu Mitbewerbern wie etwa Microsoft Access – besser für Mehrbenutzerbetrieb geeignet.\n\nKernfunktionalität von Paradox sind die Formulare. Das sind Windows-Fenster, die man mit Hilfe der (kostenpflichtigen) Entwicklerversion durch Platzieren von vorgefertigten Eingabe- und Bedienelementen entwirft. Hierdurch entstehen Bildschirmmasken zum Betrachten und Eingeben von Daten.\n\nEine Programmierung im Sinne von Programmcode-Zeilen ist hierbei nicht erforderlich. Das Laden, Sperren und Speichern von Datensätzen besorgt die Paradox-Laufzeitumgebung automatisch.\n\nEine weitere Benutzerschnittstelle sind die sogenannten Reports. Sie werden in ähnlicher Weise konstruiert wie die Formulare, dienen aber nicht der Bildschirmdarstellung, sondern zum formatierten Ausdrucken.\n\nEin Formular belegt je eine Datei, ebenso ein Report.\n\nZu fast allen Ereignissen (etwa Eingaben, Blättern oder Mausklicks) kann Programmcode hinterlegt werden, der die Paradox-eigene Abarbeitung des Ereignisses in beliebiger Weise verändert. Beispielsweise kann man beim Ereignis „Speichern“ unstimmige Daten erkennen, eine Meldung ausgeben und die Speicherung abbrechen. In modernen Begriffen kann man dies als Scriptsprache bezeichnen.\n\nIn Paradox steht als Scriptsprache die Paradox Application Language (PAL) bzw. ObjectPAL (in den Windows-Versionen) zur Verfügung. ObjectPAL ist die objektbasierte Variante von PAL (es wird hier bei Paradox korrekt zwischen objektbasiert und objektorientiert unterschieden). Beide sind von Pascal abgeleitet.\n\nOft wird die Scriptsprache unscharf ebenfalls als „Paradox“ bezeichnet.\n\nParadox-Anwendungen werden in einen Zwischencode kompiliert. Zum Ablauf dieser Programme ist weiterhin ein Paradox-System oder eine (kostenfreie) Paradox-Runtime notwendig. Im Gegensatz zu etwa Access muss durch die Teilkompilierung der Quellcode nicht weitergegeben werden.\n\nParadox für Windows 1.0 erschien Anfang 1993. Aus Marketinggründen wurde die Version 1.1 im Jahre 1994 als Version 4.5 auf den Markt gebracht – da Paradox für DOS bei diesem Versionsstand war. Das Paradox-Tabellenformat wird daher bei Paradox für Windows 1.0 und 4.5 auch bis Level 4 unterstützt. Paradox für Windows 5.0 war dann die eigentliche zweite Generation (Paradox Table Level 5). Danach kam Paradox 7 in einer Version für Windows 16-Bit und Windows 32-Bit („Table Level 7“ – der letzte Formatstand). Das Produkt Paradox wurde nach Version 7 für Windows an Corel abgegeben (erste Version von Corel ist Corel Paradox 8). Corel integrierte das Programm in die \"Professional\"-Versionen seiner WordPerfect Office Suite. Es wurde in den Versionen 9, 10, 11, 12, X3 und X4 der Office-Suite veröffentlicht. Die letzte von Corel herausgegebene Paradox-Version ist 11.0.0.663, die im X4 Service Pack 1 ausgeliefert wurde. Die letzte deutsche Version ist Paradox 9.\n\nDer Inhalt von unverschlüsselten Paradox-Tabellen ist mit paradoxfremden Mitteln, auch mit einfachen Textverarbeitungsprogrammen, zu lesen. Das Paradox-Programm bietet die Möglichkeit, Tabellen zu verschlüsseln, sodass beim Zugriff über Paradox ein Passwort verlangt wird. Zur Entschlüsselung solcher Tabellen braucht man aber nicht unbedingt das Kennwort: Die Regeln der Verschlüsselung sind fest und der Schlüssel zum Lesen ist in der Datenbank mit gespeichert, sodass keine Brute-Force-Passwortsuche nötig ist. Verschiedene Hilfsprogramme wie \"decryptpdxtable\", \"pxunsec\" (bis Version 4) und \"pxdazz\" (bis Version 7) können zum Speichern einer unverschlüsselten Kopie bzw. zum Ermitteln des Passwortes einer Paradoxtabelle verwendet werden. Weiterhin existieren einige \"Universalpasswörter\", mit denen jede verschlüsselte Paradoxtabelle geöffnet werden kann.\n\n"}
{"id": "50259", "url": "https://de.wikipedia.org/wiki?curid=50259", "title": "Multiply-Accumulate", "text": "Multiply-Accumulate\n\nDiese Operation wird intensiv bei der Verarbeitung digitaler Signale genutzt. In modernen FPGAs sowie bei der Entwicklung von anwendungsspezifischen Schaltungen (\"ASIC\"s) wird diese Operation als Teil von DSP-Blöcken (Hardwareeinheiten) bereitgestellt; als Maschinenbefehl ist sie seit den 1980er Jahren in vielen Signalprozessoren wie auch seit Anfang der 2000er Jahre in konventionellen CPUs zu finden. Fused Multiply-Accumulate ist ein Multiply-Accumulate Befehl mit höherer Rechengenauigkeit.\n\nDurch die Erweiterung des Hardware-Multiplizierers können Prozessoren diese genauso schnell wie eine klassische Multiplikation ausführen – übliche Ausführungszeiten sind z. B. 2 Takte (40 ns) beim TMS320C40 von Texas Instruments mit 50 MHz Taktfrequenz und 5 Takten (2 ns) beim aktuellen Intel Haswell mit z. B. 2,5 GHz Taktfrequenz. \n\nEntgegen üblichen Darstellungen sind Multiply-Accumulate-Befehle auch für Berechnungen abseits der Haupteinsatzbereiche wie für die digitale Bildverarbeitung, Dekodierung von Videos, digitaler Filter und Regelungstechnik verwendbar.\n\nDie Argumente und das Ergebnis dieser Operation können je nach Prozessortyp und gewähltem Datentyp \n\nBei der MAC-Operation kann eine Verbesserung der Genauigkeit im finalen Ergebnis erzielt werden, indem die notwendige Rundung erst am Ende der MAC-Operation ausgeführt wird und die Zwischenergebnisse intern mit voller Auflösung ohne Rundungen durchgeführt werden. Diese Operation wird auch als , abgekürzt FMA oder FMAC, bezeichnet. Die FMAC-Operation bedingt, im Gegensatz zur MAC-Operation, breitere Datenpfade und damit verbunden einen erhöhten Hardwareaufwand.\n\nDer Geschwindigkeitszuwachs kann bis zu 100 % betragen. So dauert in vielen DSPs der Multiply-Accumulate-Befehl genauso lange wie eine einzelne Addition oder eine einzelne Multiplikation (Beispiel: Texas Instruments TMS320C40). Der Geschwindigkeitszuwachs beim Intel Haswell ist geringer. Ein Multiply-Accumulate-Befehl dauert 5 Takte, eine einzelne Multiplikation 5 Takte und eine einzelne Addition 3 Takte, was zusammen 8 Takte ergibt und bei optimalem Einsatz ein Gewinn von 60 % bringt.\n\nAuf der anderen Seite ist der Multiply-Accumulate-Befehl häufig der kritischste Befehl (kritischer Pfad) und begrenzt die Taktfrequenz nach oben. Ein weiteres Problem ist, dass man es in der Praxis sehr häufig mit Operationen zu tun hat, die die Form \n\nbenötigen würden.\n\nIm ersten Fall ist die MAC-Anweisung häufig nicht verwendbar, obwohl sie ein Exklusiv-Oder-Gatter von der benötigten Lösung entfernt ist. Im zweiten Fall bringt der MAC-Befehl einen deutlichen Nutzen, leider verbleibt eine Trivialoperation. Der dritte Fall wurde von AMD mit FMA4 adressiert, weiterhin lässt er sich durch das Registerumbenennung heutiger CPUs meist verstecken.\n\nBeispiele:\n"}
{"id": "50265", "url": "https://de.wikipedia.org/wiki?curid=50265", "title": "Arithmetisch-logische Einheit", "text": "Arithmetisch-logische Einheit\n\nEine arithmetisch-logische Einheit (, daher oft abgekürzt ALU) ist ein elektronisches Rechenwerk, welches in Prozessoren zum Einsatz kommt.\n\nDie ALU berechnet arithmetische und logische Funktionen. Sinnvollerweise kann sie mindestens folgende Minimaloperationen durchführen:\n\n\nTypischerweise liegen jedoch auch noch zusätzlich folgende Operationen vor, die aber auch mittels der oberen drei auf Kosten der Rechenzeit (in mehreren Takten) nachgebildet werden können:\n\n\n\nAlle ALUs verarbeiten Festkommazahlen. Gleitkommabefehle sind in vielen aktuellen CPUs mittlerweile Standard. Ausnahmen sind kleinere Mikrocontroller, die auf geringe Stromaufnahme oder auf geringe Herstellkosten getrimmt sind. Der Trend der letzten 15 Jahre ist es, ALUs auf Vektor-Verarbeitung umzustellen (Stand 2015).\n\nEine ALU kann meistens zwei Binärwerte mit gleicher Stellenzahl (n) miteinander verknüpfen. Man spricht von n-Bit-ALUs. Typische Werte für n sind 8, 16, 32 und 64. Begriffe wie 32-Bit- bzw. 64-Bit-CPU entstammen der Breite der Arbeitsregister der Prozessorarchitektur. Meist entspricht diese Breite auch der der ALU; mithin deuten diese Begriffe (32-Bit- bzw. 64-Bit-CPU) an, dass in entsprechenden Prozessoren auch ebensolche ALUs zum Einsatz kommen.\n\nDie n-Bit ALU ist meist aus einzelnen 1-Bit-ALUs zusammengesetzt, die jeweils an die höherwertige ALU ein Carry-Bit weiterreichen, mit dem ein Übertrag an der jeweiligen Stelle gekennzeichnet wird. (Zur Beschleunigung werden häufig andere Verfahren (z. B. Carry-Look-Ahead) eingesetzt; siehe dazu Carry-Ripple-Addierer.) Um die zusammengeschalteten 1-Bit-ALUs in die geforderte Funktionsart umzuschalten, hat jede 1-Bit ALU zusätzlich zu den Eingängen für die zu verknüpfenden Werte und das Carry-Bit noch einen Eingang für einen Steuervektor (Op.-Code); beim Einsatz in einem Prozessor werden diese Eingänge gemeinsam vom Steuerregister (Operationsregister, OR) des Prozessors versorgt.\n\nDie gesamte n-Bit ALU hat außer dem Ausgangsvektor für das Ergebnis noch einen Bedingungsvektor, um ihren Zustand zu signalisieren. Beim Einsatz in einem Prozessor wird dieser Bedingungsvektor in dessen Statusregister (auch \"Condition Code Register\") abgelegt.\n\nDieses Register enthält meistens vier Statusbits. Die einzelnen Werte des Statusregisters können von Maschinensprachebefehlen abgefragt werden und damit den weiteren Programm<nowiki></nowiki>verlauf beeinflussen (z. B. bedingte Sprünge):\n\n\nJe nach ALU-Typ gibt es weitere Flags, zum Beispiel:\n\n\nParallel Logic Units (PLUs) sind speziell auf Bitmanipulation zugeschnittene ALUs. Sie können parallel, also zeitgleich zu anderen Rechenwerken auf die Daten zugreifen und sind deshalb über einen eigenen Datenpfad mit dem Arbeitsspeicher verbunden. PLUs sind Bestandteil einiger Signalprozessor-Typen, die als Hochgeschwindigkeits-Controller eingesetzt werden.\n"}
{"id": "50832", "url": "https://de.wikipedia.org/wiki?curid=50832", "title": "DirectX", "text": "DirectX\n\nDirectX [ (AE) bzw. (BE)] ist eine Sammlung COM-basierter Programmierschnittstellen (englisch \"Application Programming Interface\", kurz API) für multimediaintensive Anwendungen (besonders Spiele) auf der Windows-Plattform und kommt auch auf der Spielkonsole Xbox zum Einsatz.\n\nDie DirectX-Sammlung von Software-Komponenten deckt nahezu den gesamten Multimediabereich ab. Vorrangig wird es bei der Darstellung komplexer 2D- und 3D-Grafik eingesetzt, bietet aber auch Unterstützung für Audio, diverse Eingabegeräte (zum Beispiel Maus, Joystick) und Netzwerkkommunikation.\n\nNachdem sich seit Ende der 1980er Jahre der IBM (kompatible) PC mit dem Betriebssystem MS-DOS als „Computer für Jedermann“ durchgesetzt hatte, begann ein paar Jahre später auch der Siegeszug der grafischen Benutzeroberfläche Microsoft Windows. In den 1990ern wurden die verbreiteten Anwendungen zunehmend für Microsoft Windows portiert, und viele neue Anwendungen wurden exklusiv für Windows mithilfe der WinAPI entwickelt. Einzige Ausnahme blieben Computerspiele, die anfangs zwar vorwiegend auf den mit besseren Multimediafähigkeiten bestückten Heimcomputern erschienen, aber auch mehr und mehr auf dem PC Einzug hielten.\n\nWindows bot zu diesem Zeitpunkt noch keine optimierten Programmier-Schnittstellen für schnelle Grafik- und Audio-Operationen, die aber für Spiele meist unabdingbar sind. Deshalb wurden Spiele meist nur für MS-DOS entwickelt. MS-DOS bot zwar auch keine speziell für Spiele entworfenen Schnittstellen, aber es erlaubte die völlige Kontrolle über den Prozessor und den ungehinderten Zugriff auf die gesamte angeschlossene Hardware. Vor allem konnte ein Programm unter MS-DOS sicher sein, nicht von anderen gleichzeitig auf demselben Rechner laufenden Programmen behindert zu werden, weil unter dem Single-Task-System MS-DOS immer nur ein einziges Programm laufen kann. Im Gegensatz dazu müssen Programme unter Multitasking-Systemen wie Windows auf andere Programme Rücksicht nehmen, die auf demselben Rechner simultan laufen. Microsoft schenkte der Entertainment-Branche und Unterhaltungsmedien wie Computerspielen zu diesem Zeitpunkt noch wenig Beachtung.\n\nNach dem einschlagenden Erfolg des Computerspiels \"Doom\" von der Firma id Software, die schon vorher mit Wolfenstein 3D gezeigt hatte, welches technische Potenzial in den eigentlich vorrangig für Büroarbeit verwendeten „IBM-kompatiblen PCs“ steckt, begann sich Microsoft für das Unterhaltungssegment zu interessieren. Dem System Windows 95 – das genau wie seine Vorgänger bei Erscheinen noch über keine Programmierschnittstelle für Spiele verfügte – wollte Microsoft nachträglich eine Schnittstelle hinzufügen, um den wachsenden Markt der PC-Spieler von MS-DOS wegzulocken und hin zu Windows 95. Diese erste Schnittstellen-Version nannte Microsoft „Game SDK“. Sie bestand lediglich aus einer Handvoll Funktionen, mit denen Grafiken direkt in den Grafikspeicher geblittet werden konnten. Diese erste Schnittstellen-Version wurde von der Spieleindustrie, die jahrelang für MS-DOS entwickelt und dort Erfahrung gesammelt hatte, nicht weiter beachtet.\n\nParallel dazu entwickelte Microsoft \"WinG\", eine Bibliothek mit gleichem Ansatz, jedoch bevorzugt für den 256-Farben-Modus. Diese Bibliothek kam zum Beispiel in „Lemmings for Windows“ zum Einsatz und war auch unter der Benutzeroberfläche \"Windows 3.11 (for Workgroups)\" verfügbar.\n\nEinen weiteren Anlauf unternahm Microsoft mit DirectX (Version 1.0). Microsoft konnte \"id software\" dazu bewegen, eine Portierung ihres erfolgreichen Spiels „Doom“ mit dieser API für Windows 95 zu entwickeln. Diese Spiel-Portierung wurde von Microsoft auf einer Messe präsentiert, um Entwickler aus aller Welt dazu zu bringen, mit DirectX direkt für den De-facto-Standard Windows 95 zu entwickeln statt für MS-DOS.\n\nAber auch dieser Versuch scheiterte zunächst, da Microsoft den Aufwand weit unterschätzte, der für eine brauchbare Programmbibliothek mit auf Spiele optimierten Grafik- und Sound-Funktionen nötig war. Noch Jahre später kamen Spiele auf den Markt, die eigentlich durchgängig Protected-Mode-DOS-Spiele waren und höchstens ein paar zusätzliche Hilfsprogramme (z. B. Editoren) für Windows mitbrachten oder nur die Autoplay-Funktionalität von Windows nutzten.\n\nSeit DirectX in Version 2.0 auch 3D-Funktionalität in Form der Komponente Direct3D mitbrachte, gelangte es mit Version 3.0 zu einer gewissen Reife und wurde allmählich auch von Entwicklern ernstgenommen. Mit ihm erschienen auch die ersten Spiele, die nur noch unter Windows mit DirectX und nicht mehr unter MS-DOS liefen, zum Beispiel Diablo.\n\nWährend der Entwicklung von DirectX 4.0 stellte sich heraus, dass viele Programmierer auf Funktionalitäten warteten, die erst für Version 5 vorgesehen waren. Also beschloss man, die wenigen Veränderungen von DirectX 4 nicht zu veröffentlichen, sondern direkt in die darauffolgende Version einzubauen.\nDirectX 6.0 unterstützte erstmals Multitexturing und Bumpmapping. Ab Version 7 gehören Transform and Lighting und Cubic Environment Mapping (\"CubeMaps\") zum Funktionsumfang. Das T&L-Paket wurde in der darauffolgenden Version deutlich ausgebaut und um Funktionen wie \"Triangle Tessellation\" erweitert. Dazu kamen außerdem frei programmierbare Vertex- und Pixel-Shader.\n\nDirectX 8 deckte viele Gebiete der Multimedia- und Spiele-Programmierung ab, wie 2D- und 3D-Grafik, Ton und Musik, Video/Capturing, Eingabe (und über Force Feedback auch „Ausgabe“ auf eigentlichen Eingabegeräten) und Netzwerk (siehe unten).\n\nDirectX 9.0 erschien im Dezember 2002. Es bot als große Neuerung die „High Level Shading Language“ (HLSL), womit Entwickler leichter 3D-Grafiken und Effekte erstellen konnten. HLSL bot für Entwickler eine flexible und leicht zu bedienende Entwicklungsumgebung und war dabei zu allen DirectX-fähigen Grafikkarten kompatibel, um die Anpassung auf die Hardware-Eigenheiten zu vereinfachen. Eine weitere Neuerung war eine Bibliothek, die Patch-Meshes und herkömmliche Polygon-Meshes sowie verbesserte Echtzeit-Animationen bot. Alle Direct3D-APIs enthielten erweiterte Programmfähigkeiten bei Low-Level-Grafiken mit programmierbaren Vertex- und Pixel-Shader-2.0-Modellen. Microsoft implementierte in DirectX 9.0 neue Assistenten zur Erzeugung von DirectX Media Objects (DMOs), für Audio-Effekte und für DirectMusic-Werkzeuge beim MIDI-Processing.\n\nDie Version DirectX 9.0c wurde im August 2004 als aktualisierte Version veröffentlicht. Nach Aussage von Microsoft ist die Unterstützung für DirectX 9 für die nächsten Jahre in jedem Falle gesichert, auch wenn Windows Vista und DirectX 10/10.1 in der Praxis zum Einsatz kommen. Im Juni 2010 veröffentlichte Microsoft die aktualisierte Version 9.29, welche laut \"dxdiag\" aber weiterhin DirectX 9.0c entspricht.\n\nDie Version DirectX 10.0 erschien Anfang 2007. Anders als die Vorgänger läuft diese nur unter dem Betriebssystemen Windows Vista und Windows 7. Zusätzlich muss im Computer eine für DirectX 10.0 taugliche Grafikkarte eingebaut sein.\n\nUnter den Systemen Windows 98, Me, 2000, XP oder Server 2003 steht weiterhin nur DirectX 9.0c zur Verfügung.\n\nIm April 2007 erschienen Meldungen, wonach die neuen Direct3D-10-Funktionen über eigene Bibliotheken auch auf anderen Windows-Versionen nutzbar seien. Das Unternehmen hinter diesem Projekt hat das Vorhaben inzwischen aber eingestellt und den Quelltext veröffentlicht.\n\nNeu an Version 10 ist nur Direct3D 10, das das neue Windows Display Driver Model und Shader Model 4 benutzt, kombiniert mit strengeren Regeln für die Hersteller von Grafikkarten, die die Direct3D-10-Kompatibilität für sich in Anspruch nehmen möchten.\n\nAb Direct3D 10 werden bis auf wenige Ausnahmen keine \"cap-bits\" mehr verwendet, anhand derer Programme feststellen können, welche Funktionen die Hardware zur Verfügung stellt. Daher muss Direct3D 10 und jede folgende Version neue Funktionen immer in Form von Mindestanforderungen einführen. Damit kommt Microsoft den Spieleentwicklern entgegen, da diese die Hardware bisher auf eine Vielzahl an möglichen Funktionskombinationen abfragen mussten. Dagegen werden Hardwarehersteller nun dazu gezwungen, bestimmte Funktionen zu implementieren, um kompatibel mit der jeweiligen Direct3D-Version zu sein. Dadurch wird die Fähigkeit der Hardwareproduzenten eingeschränkt, sich durch optionale Komponenten von Wettbewerbern abzuheben.\n\nDirect3D 10 unterscheidet sich von seinen Vorgängern weniger durch direkt sichtbare Eigenschaften als vielmehr durch Erweiterung, Modularisierung und Flexibilisierung der 3D-Funktionen. Es wird deshalb erst in Zukunft Anwendungen geben, die die neuen Möglichkeiten so ausschöpfen, dass der Anwender einen spürbaren Fortschritt sieht.\n\nDirectX 10.1 wurde zusammen mit Windows Vista SP1 ausgeliefert und enthält gegenüber DirectX 10 folgende Änderungen, die die Direct3D-API betreffen:\nFalls eine Grafikkarte eine dieser Funktionen nicht beherrscht, kann sie keine Direct3D-10.1-Schnittstelle anbieten, und die Applikation muss auf Direct3D 10 zurückfallen.\n\nZusätzlich wird XAudio 2 als Ersatz für DirectSound eingeführt.\n\nMicrosoft hatte bereits auf der Messe \"Gamefest\" (22. bis 23. Juli 2008) in Seattle neue Details zu DirectX 11 offiziell enthüllt.\n\nGrundsätzlich sollen alle Grafikkarten, die mit DirectX 10.x kompatibel sind, auch unter DirectX 11 eine vollständige Kompatibilität erreichen (was neue Funktionen von DirectX 11 auf älterer Hardware ausschließt).\n\nWesentliche Neuerungen sind:\n\nDirectX 11 ist als Teil von Windows 7 im vierten Quartal 2009 erschienen. Für Windows Vista startete Microsoft die Auslieferung von DirectX 11 am 28. Oktober 2009 zunächst per \"Windows Update\".\n\nDer technische Entwicklungsleiter für den Grafikbereich bei AMD teilte in einer Rede in Reykjavík mit, dass schon im ersten Quartal 2009 die voraussichtlich erste DirectX-11-Grafikkarte auf den Markt kommen sollte. Dies war allerdings erst mit dem Erscheinen der ATI-Radeon-HD-5000-Serie der Fall, die zu Anfang des vierten Quartals 2009 auf den Markt kam. Zudem wurden im DirectX SDK von November 2008 die ersten DirectX-11-Beispiele gezeigt.\n\nMicrosoft hat DirectX 12 auf der Game Developers Conference in San Francisco am 20. März 2014 vorgestellt. Ähnlich wie AMDs Mantle bietet DirectX 12 hardwarenähere Programmierung, so soll die CPU-Last (\"Overhead\") verringert werden. DirectX 12 soll unter anderem auf allen DirectX-11-fähigen Grafikkarten sowie Mobilgeräten und der Xbox One funktionieren. Die ersten Spiele mit DirectX-12-Unterstützung wurden für Ende 2015 erwartet.\n\nAnmerkungen:\n\n\nDirectX besteht aus folgenden Teilen:\n\n\"DirectX Graphics\" ist der am meisten genutzte Teil von DirectX. Er ermöglicht einen schnellen und direkten Zugriff auf die Grafikkarte, vorbei am Graphics Device Interface (GDI) und Display Device Interface (DDI), und unterstützt (mit „Direct2D“) 2D- und (mit „Direct3D“) 3D-Grafik.\n\nDirectX Graphics stellt sowohl eine Low-Level-API (Direct3D) als auch eine High-Level-API (Direct3DX) bereit. Die \"Low-Level\"-API Direct3D eignet sich für Anwendungen mit hoher Interaktionsrate oder Präsentationsfrequenz komplexer grafischer Szenen, wie beispielsweise bei 3D-Spielen. Bis zur Version 7 wurden bei der Low-Level-API zwischen 2D-Grafik (DirectDraw) und 3D-Grafik (Direct3D) unterschieden. Durch eine Überarbeitung des Grafikbereichs (in Version 8) wurden beide Teile unter einer einheitlichen API in Direct3D zusammengefasst. Die explizite, eigenständige Weiterentwicklung von DirectDraw wurde damit eingestellt.\n\nDie \"High-Level\"-API Direct3DX ermöglicht es hingegen, mit vertretbarem Aufwand 3D-Anwendungen zu realisieren. Direct3DX setzt auf Direct3D auf, das heißt, es nutzt dessen Basisfunktionalität.\n\nAußerdem emuliert die Funktionsgruppe nicht durch die Hardware unterstützte Funktionen. Dafür nutzt der \"Hardware Emulation Layer\" (HEL) die Möglichkeiten von MMX-Prozessoren zur Manipulation von Bildern und greift auf Funktionen des GDI zurück. Unterstützt werden unter anderem Seitenumschaltung (Flipping) (siehe Doppelpufferung), Blitting, Clipping, 3D-Z-Puffer, Overlays und direkte Steuerung des Datenflusses durch die Video-Port Hardware (Video-Port Manager).\n\nDirectSound wird zur Wiedergabe und Aufnahme von Toneffekten, z. B. Raumklangunterstützung (das heißt Positionierung der Klänge im 3D-Raum), verwendet. Die Daten aus mehreren Eingangspuffern (Secondary Sound Buffers) werden mit Effekten belegt und zusammen auf einen Ausgangpuffer (Primary Sound Buffer) gemischt. Eingangspuffer können in Software oder Hardware realisiert sein, statisch (zum Beispiel aus einer Datei) oder dynamisch (zum Beispiel Streaming von einem Mikrofoneingang) Daten liefern. Die Anzahl der Eingangspuffer, die gemischt werden können, ist ausschließlich durch die verfügbare Rechenleistung begrenzt. Es passt sich automatisch dem Leistungsspektrum der installierten Soundkarte an. Basiseffekte, wie \"Volume\", \"Frequency Control\", \"Panning\" bzw. Balance, zusätzliche Effekte, wie \"Reverb\" (Halleffekt), Chorus, Distortion, Equalization und 3D-Effekte, wie Rolloff, Amplitude Panning, Muffling, Arrival Offset und Doppler Shift Effekt stehen zur Verfügung. Da DirectSound durch XAudio 2 abgelöst wurde, sind keine Samples mehr für DirectSound in den DirectX-SDK-Releases nach November 2007 enthalten.\n\nFür die Wiedergabe von Musik (MIDI-Musik, allerdings keine komprimierte Musik wie MP3). DirectMusic liefert dazu einen Software-Synthesizer-Service. Verwendet wird ein DLS2-Synthesizer. Siehe DirectShow für die Wiedergabe von komprimierter Musik wie MP3 und Video (AVI, MPEG).\n\nXAudio 2 basiert auf der Xbox-360-Sound-API und hat DirectSound abgelöst. Die programmierbaren DSP-Programme ermöglichen EAX-ähnliche Effekte auf allen Tonausgabe-Geräten (ugs. mitunter als \"Sound-Karte\" bezeichnet, auch wenn es meist nur Bauteile auf der Hauptplatine sind); diese werden allerdings auf dem Hauptprozessor ausgeführt. Dies hat vor allem Kritik seitens des Herstellers Creative hervorgerufen, weil auf dessen Chips bzw. Karten solche DSP-Programme „in Hardware“ ausgeführt werden könnten, um Spielern somit einen Geschwindigkeitsvorteil zu bieten (durch Entlastung der CPU).\n\nDirect Input unterstützt Eingabegeräte, wie Tastatur, Maus, Joysticks usw. und ermöglicht Force-Feedback-Effekte (zum Beispiel das Vibrieren eines Gamepads oder Widerstand beim Bewegen eines Joysticks) und unterstützt den Zugriff auf analoge und digitale Eingabegeräte, welche ein absolutes Koordinatensystem verwenden. Ein Eingabegerät kann über bis zu sechs Bewegungsachsen und 32 Knöpfe verfügen. Der Zugriff über DirectInput umgeht das Windows Message System (das heißt Ereignis-, Melde- und Warteschlangen) und erfolgt direkt auf die Hardware; ein Geschwindigkeitsvorteil gegenüber der Win32-API. DirectInput erlaubt Anwendungen die Nutzung von möglichen Force-Feedback-Fähigkeiten der Eingabegeräte, um so Kraftrückkopplungseffekte zu erzeugen.\n\nXInput ist der neue Standard für Eingabegeräte unter Windows XP und späterer Betriebssysteme, sowie für die Xbox 360. XInput ignoriert alle älteren DirectInput-Geräte, wodurch ältere Joysticks und Joypads nur durch Simulation über Wrapper-Programme in den verschiedenen Spielen und Anwendungen funktionieren können. Bislang ist keine Lösung für dieses Problem von Microsoft bekannt, weshalb angenommen werden kann, dass das Unternehmen kein Interesse an einer weiteren Unterstützung solcher Geräte hat. Möchte man mit einem DirectInput-Controller XInput-Spiele betreiben, empfiehlt es sich, einen entsprechenden Emulator zu verwenden. Ob Microsoft in Zukunft reine DirectInput-Geräte mit XInput unterstützen wird, ist fraglich.\n\nDirectPlay dient zur Kommunikation von Multiplayerspielen untereinander, die auf mehreren Computern laufen (bei Netzwerkspielen oder Online-Spielen). Im Wesentlichen handelt es sich um ein Protokoll auf Anwendungsebene, und es ist somit unabhängig von konkret genutzten Protokollen der Transport- und Übertragungsebene (siehe dazu auch OSI-Modell). DirectPlay realisiert keine Mechanismen für das Zusammentreffen der Spieler (Matchmaking) oder das Abrechnen von Spielteilnahmen.\n\nDen Kern bildet die Spielesitzung (DirectPlay Session), welche von einem „Host“ genannten Rechner erzeugt und moderiert wird. Spieler sind logische Objekte, von denen es pro Rechner mehrere geben kann, daher wird zwischen lokalen und entfernten Spielern unterschieden. Das Konzept von Spielergruppen wird unterstützt, jeder Spieler kann dabei gleichzeitig mehreren Gruppen angehören. Die Spieler können an andere Spieler Nachrichten versenden (Chat).\n\n\"Hinweis:\" Für viele Spieleentwickler war die sehr hohe CPU-Last (\"Overhead\") von DirectPlay dafür entscheidend, es nicht zu verwenden und lieber einen eigenen effektiveren Netzwerk-Zugriff auf Basis von Winsock zu realisieren. Andere haben sich wegen der festen Bindung an Windows dagegen ausgesprochen, da zum Beispiel für speziell auf Windows-Systemen lauffähige First-Person-Shooter gerne auch eigene Linux-Server entwickelt werden.\n\nDirectShow oder ehemals ActiveMovie bzw. DirectX Media dient der Verarbeitung von Video- und Audio-Dateien, womit sich verschiedenste Arten von Video-Dateien (AVI, MPEG) und Ton-Dateien (z. B. MP3) wiedergeben oder erstellen lassen. Es wird auch Streaming unterstützt und ist durch DirectShow-Filter beliebig erweiterbar.\n\nDirectShow wurde inzwischen aus dem DirectX SDK entfernt und ist in das Windows Plattform-SDK aufgenommen worden. Somit gehört DirectShow streng genommen nicht mehr zu DirectX, sondern ist jetzt ein Bestandteil der Windows-Plattform.\n\nDirectSetup ermöglicht Programmierern, ihre Installationsroutine automatisch überprüfen zu lassen, ob die benötigte DirectX-Version bereits installiert ist, und diese andernfalls zu installieren.\n\nDirectX Media Objects bietet Möglichkeiten, Audio- und Video-Ströme zu verändern, und kann auch zusammen mit DirectSound und DirectShow verwendet werden.\n\nDirectX ermöglicht direkte Zugriffe auf die Hardware des Systems, ohne die Programme von der Hardware abhängig zu machen. So wird Spieleentwicklern eine Hardwareabstraktionsschicht (\"HAL\" von engl. \"hardware abstraction layer\") für die Spieleprogrammierung zur Verfügung gestellt, mit der langsame Schnittstellen (zum Beispiel Win GDI) umgangen werden. Funktionen, die nicht von der Hardware und damit nicht von der HAL bereitgestellt werden können, werden in der Hardware-Emulations-Schicht (\"HEL\" von engl. \"hardware emulation layer\") emuliert.\n\nNeben der in aktuellen Windows-Versionen enthaltene DirectX-API von Microsoft gibt es auch Varianten für andere Betriebssysteme. Diese Nachbildungen dienen dazu, Windows-Anwendungsprogramme ohne Windows-Betriebssystem zu nutzen. Aus rechtlichen Gründen sind meist nicht alle Funktionen der originalen API vorhanden, was die Kompatibilität mit Anwendungsprogrammen einschränken kann. Da die wenigsten Anwendungsprogramme alle Funktionen der DirectX-API benötigen, sind viele aber trotzdem ohne Einschränkungen nutzbar. Grafische Funktionen werden dabei meist mittels OpenGL bereitgestellt.\nBeispiele von DirectX-Implementierungen:\n\nNeben DirectX sind auch freie APIs verfügbar, die im Gegensatz zu DirectX nicht auf die Windows-Plattform beschränkt sind. Allerdings sind diese APIs nicht so umfassend, können aber große Teile von DirectX ersetzen und ermöglichen die Entwicklung plattformübergreifender Software. Einige dieser Alternativen wie etwa OpenGL und OpenAL bieten ebenso wie DirectX Hardwarebeschleunigung.\n\nWichtige APIs:\n\nFür die neue AMD Grafikkartengeneration \"Volcanic Islands\" kündigte AMD für Ende 2013 die neue 3D-Schnittstelle AMD Mantle als DirectX-Konkurrenz an.\n\nAnwendungen für \"DirectX\" werden mit Hilfe des DirectX SDK erstellt.\n\nEin informatives Testprogramm ist das \"DirectX-Diagnoseprogramm\". Es ist in Windows enthalten und kann über \"Start → Ausführen → dxdiag\" gestartet werden oder alternativ ab Windows Vista in das Suche-Fenster in der Startleiste eingegeben werden.\n\n\n"}
{"id": "50839", "url": "https://de.wikipedia.org/wiki?curid=50839", "title": "SCO gegen Linux", "text": "SCO gegen Linux\n\nUnter SCO gegen Linux werden zwischen 2003 und 2010 geführte Gerichtsprozesse der SCO Group gegen IBM und andere Unternehmen verstanden, bei denen es um den Vorwurf ging, dass Quellcode, an dem SCO angeblich die Rechte besitze, widerrechtlich in den Linux-Kernel kopiert worden sei. Die Gerichte stellten jedoch fest, dass SCO das Copyright an Unix nie – wie behauptet – erworben hatte und wiesen die Klagen letztlich als unbegründet ab, worauf SCO Einspruch erhob. Darüber hinaus konnten auch keine der behaupteten Unix-Plagiate im Linux-Quellcode gefunden werden. In der Folge ging die SCO Group in Insolvenz und wird zurzeit von einem Konkursverwalter (Stand September 2009) verwaltet.\n\nDas seit Anfang der 1990er populäre Betriebssystem Linux ist grundsätzlich freie Software und kann von jedem verändert und weitergegeben werden. Mit zunehmender Verbreitung wurde es von immer mehr Unternehmen unterstützt, was in einer Ankündigung IBMs gipfelte, rund eine Milliarde Dollar in Linux zu investieren.\n\nIBM, das ein eigenes Unix namens AIX im Portfolio hat, fuhr das Engagement für das eigene Betriebssystem zugunsten von Linux immer mehr zurück. Kurze Zeit später wurden die ersten Stimmen aus dem Lager von SCO laut, die in Frage stellten, ob Linux sich von alleine so entwickeln konnte, oder ob dort nicht urheberrechtlich geschützter Quelltext eingeflossen wäre, der hätte lizenziert werden müssen.\n\nFast alle auf Linux basierenden Betriebssysteme benutzen neben dem Kernel namens \"Linux\" auch die \"GNU Core Utilities\" aus dem \"GNU-Betriebssystem\". \"GNU\" steht für \"GNU's Not Unix\". Der Name wurde gewählt, weil das Betriebssystem zwar unixoid sein sollte, aber sich von Unix unterscheidet, indem es freie Software ist und keinen Quelltext von Unix enthält. Die \"Free Software Foundation\", die das \"GNU-Projekt\" verwaltet, waren denn auch zuversichtlich, das Urheberrecht an \"GNU\" nachweisen zu können. Nachdem SCO nach Urheberrechtsverletzungen in GNU und im Linux-Kernel recherchieren ließ, entschied sich die Unternehmensführung, nur den Linux-Kernel juristisch anzugreifen.\n\nIm Folgenden sind nun die daraus resultierenden Ereignisse aufgelistet.\n\n\n"}
{"id": "50967", "url": "https://de.wikipedia.org/wiki?curid=50967", "title": "Case Modding", "text": "Case Modding\n\nCase Modding (von engl. \"case\" = \"Gehäuse\", engl. \"modification\" = \"Veränderung\") ist primär das Verändern der äußeren Erscheinungsform des PCs zur optischen Aufwertung. Hierzu zählt hauptsächlich die optische Bearbeitung der im normalen Gehäuse nicht sichtbaren Komponenten. Ferner kann auch die technische Modifikation der Hardware-Komponenten als Modding bezeichnet werden.\n\nBeim Case Modding wird beispielsweise ein Fenster aus Plexiglas in das Seitenteil der Verkleidung eingesetzt und der PC-Innenraum mit Leuchtdioden und Leuchtstofflampen ausgeleuchtet oder eine Wasserkühlung eingebaut. Verwendet werden meist optisch auffällige Materialien. Es hat sich eine eigene \"Case-Modding-Gemeinde\" gebildet, die auch Wettbewerbe (z. B. DCMM – Deutsche Casemod Meisterschaft, GCCM – Games Convention Casemod Masters, IFA C3 – IFA CaseCon Championship) durchführt. Je nach Art und Umfang sind bei den Aktiven umfangreiche Kenntnisse und handwerkliches Geschick für die Umsetzung notwendig.\n\nBeim „Modden“ (von „modifizieren“ abgeleiteter Jargon-Begriff) wird großer Wert darauf gelegt, Gehäuse und Komponenten einzigartig zu gestalten. Dies führt zu unkonventionellen Lösungen, wie z. B. Bierkästen oder Mikrowellenherde (siehe Bild) als Gehäuse. Auch auffällige Lackierungen sind üblich.\n\nDas Innere des Computergehäuses kann zum Beispiel mit auffälligem Stoff ausgekleidet werden, der die Farbe der Kaltlichtkathode hat. Beliebt ist auch Carbon-Folie (also Dekorfolie, die das Muster von kohlenstofffaserverstärktem Kunststoff hat) als Verkleidung. Man kann natürlich auch den Innenraum in einer Kontrastfarbe lackieren, um ihn deutlich hervorzuheben.\n\nInzwischen zeichnet sich die Spitze der Case-Modding-Szene durch einen hohen Geld- und Zeitaufwand aus und versucht, die kreativen Gestaltungsmöglichkeiten stetig zu erweitern.\n\nBaut man seinen Computer in ein komplett selbst hergestelltes Gehäuse ein, redet man innerhalb der deutschen Moddingszene von einem \"CaseCon\" (von „Case Construction“ – Gehäuse-Konstruktion), während im englischsprachigen Raum die Bezeichnungen „Custom Computer Case“ bzw. „scratch built“ verwendet werden.\n\nInzwischen gibt es auch schon fertig „gemoddete“ Gehäuse zu kaufen, jedoch sind diese bei dem Kern der \"Case-Modding-Gemeinde\" nicht besonders angesehen, zumal der Begriff auch unpassend ist, da es sich um industriell gefertigte Serienprodukte und nicht mehr um individuelle Anfertigungen handelt. Hier wird der Begriff im Grunde erweitert und umfasst jegliche Computergehäuse, die mit einer oder mehrerer der üblichen Zutaten zu Case Mods (wie Fenster, Beleuchtung, LC-Displays) aufwarten. Geschichtlich ist anzumerken, dass schon Ende der 1980er Jahre die Firma Commodore speziell designte Varianten des Amiga 500 anbot, wofür die Fernsehmoderatorin Stefanie Tücking als Namensgeber gewonnen wurde.\nBesonders gern werden effektvolle Komponenten eingesetzt wie:\n\nNachteilig ist, dass solcherlei veränderte Gehäuse unter Umständen nicht mehr der EMV-Richtlinie entsprechen und somit zur Quelle weit reichender Hochfrequenzstrahlung werden können. Somit können sie im Umfeld elektronische Geräte, Funknetze und den Radio- bzw. Fernsehempfang stören. Jedermann, der einen PC verändert, wird rechtlich zum Hersteller und muss für die Einhaltung der Normen zur EMV und Sicherheit haften.\n\nDarüber hinaus können so umgebaute Computer auch selbst empfindlicher auf elektromagnetische Störstrahlungen von außen reagieren und dadurch eine erheblich reduzierte Störfestigkeit aufweisen. In der Praxis dürfte dies aber meist nur in der Nähe von starken Quellen elektromagnetischer Strahlen eintreten.\n\nAls weitere Spielart des Case Modding kann die Geräuschminderung angesehen werden. Hierbei geht es darum, durch Veränderungen am Gehäuse und den Komponenten einen möglichst leisen Computer zu bekommen, wobei eine Wasserkühlung zum Einsatz kommen kann.\n\nAls \"Casemodder\" werden Personen bezeichnet, die sogenannte Casemods bzw. Casecons erstellen. Sie erweitern ihre Computer um Funktionen wie z. B. eine Infrarotschnittstelle, über welche das komplette Fernsteuern des PCs ermöglicht wird, oder beleuchten ihn mit zahlreichen LEDs, Kaltlichtkathoden etc. bis hin zur kompletten Neugestaltung eines Gehäuses.\n\nIn den letzten Jahren hat sich ein Teil der professionellen Casemodder verstärkt dem Thema Kunst (\"Bildende Kunst\") und Design (\"Industriedesign\") zugewandt, was sich vor allem in der Gestaltung und Ausführung von Casemoddingarbeiten widerspiegelt. Zielsetzung ist dabei, dem künstlerischen Anspruch gerecht zu werden und sich von dem bloßen Verändern der äußeren Erscheinungsform des PCs zur optischen Aufwertung abzuheben und als eigenständige Kunstform im Bereich der bildenden Kunst zu gelten.\n\nZwar wird dies auch unter dem Begriff des Casemoddings geführt, jedoch wird der PC nun mehr durch eine künstlerisch-ästhetische Gestaltung zum puren Designobjekt, dessen Planung und Entwurf einen größeren zeitlichen Rahmen in Anspruch nimmt als die eigentliche Gestaltung am Objekt (dem PC) selbst.\n\nFederführend in dieser „neuen“ Casemodding-Bewegung sind vorrangig Personen, welche den Werdegang der deutschen Casemoddingszene in den letzten Jahren nachhaltig geprägt haben, darunter der mehrfache deutsche Casemoddingmeister und Autor Benjamin Franz.\n\nDCMM steht für Deutsche CaseMod Meisterschaft. Sie bietet Casemoddern jährlich die Möglichkeit, ihre sogenannten 'Mods' vorzustellen und sich mit der Kreativität und dem handwerklichen Geschick anderer Casemodder zu messen. Bewertet werden die Cases durch eine Jury aus Vertretern großer Casemodding-Communitys. Unabhängig davon wählen die Besucher der DCMM die spektakulärste Kreation. Die DCMM gilt mit ca. 60 bis 70 ausgestellten Exponaten je Veranstaltung als größtes Event der Casemodding-Szene weltweit.\nSeit 2008 gibt es neben den Bewertungskategorien Casemod und Casecon die Kategorie CE-Mod (Consumer Electronic Modified). In dieser Kategorie treten umgebaute elektronische Geräte aller Art an, die kein PC sind.\nSeit 2010 gibt es neben die oben genannten Kategorien auch Cases on the Move; dies sind Cases mit möglichst vielen, sich selbstständig bewegenden Elektro-Mechanische Komponenten (Unterkategorie der Casemods, Casecons, 24std Mods und CE-Mods) Die DCMM wird seit 2002 veranstaltet durch die planetlan GmbH aus Bochum.\n\nDeutsche Meister im Casemodding seit der ersten DCMM im Jahr 2002:\n\n\n"}
{"id": "51242", "url": "https://de.wikipedia.org/wiki?curid=51242", "title": "Microsoft Office", "text": "Microsoft Office\n\nMicrosoft Office bezeichnet ein Office-Paket von Microsoft für Windows, macOS, iOS, Android sowie Windows Phone. Für unterschiedliche Aufgabenstellungen werden verschiedene \"Editionen\" angeboten, die sich in enthaltenen Komponenten, Preis und Lizenzierung unterscheiden.\n\nDie erste Version von \"Microsoft Office\" erschien 1989 ausschließlich für den Apple Macintosh und umfasste die Textverarbeitung Word, die Tabellenkalkulation Excel und die Präsentationssoftware PowerPoint. Die letzten Versionen für klassisches Mac OS waren \"Microsoft Office 98\" und \"Microsoft Office 2001\". Das im Jahr 2001 erschienene \"Microsoft Office v.X\" war die erste Version für Mac OS X, das Nachfolgeprodukt \"Microsoft Office 2004\" erschien Juni 2004.\n\n\"Office 2008\" wurde mit Xcode komplett überarbeitet. Das Ziel war die Prozessorunabhängigkeit, sodass Office nativ auf Intel- wie auch auf PowerPCs läuft (Universal Binary). Mit dieser Version entfiel die rudimentär vorhandene VBA-Unterstützung zugunsten der AppleScript-Unterstützung. Microsoft hatte diesen Schritt damit begründet, dass der veraltete Programmcode mit modernen Compilern nicht mehr übersetzt werden könne und einer Portierung auf moderne GCC-Compiler unverhältnismäßige Schwierigkeiten entgegenstünden. Mit Office 2011 wurde die VBA-Unterstützung wieder eingeführt.\n\nDie OS-X-Version \"Office 2016\" ist seit dem 9. Juli 2015 als 32-Bit-Version verfügbar. Die 64-Bit-Version ist am 22. August 2016 zur Verfügung gestellt worden.\n\nDie aktuelle OS-X-Version \"Office 2019\" wurde am 24. September 2018 veröffentlicht.\n\nDie erste Office-Version für Windows erschien Ende September oder im Oktober 1990 (englische Version). Sie bündelte erstmals für Windows die zuvor schon erhältlichen Einzelprogramme Word for Windows 1.1, Excel for Windows 2.0 und PowerPoint for Windows 2.0 mittels eines gemeinsamen Installationsprogramms und bot bereits einen programmübergreifenden Datenaustausch per DDE. Diese Office-Version begnügte sich u. a. mit EGA-Grafik, 2 MB RAM und einem Prozessor der 80286-Klasse, erforderte aber bereits ein Zeigegerät und konnte – da Windows noch nicht sehr verbreitet war – unter DOS auch mit einer angepassten Windows-Runtime genutzt werden. 1992 erschien die Version Microsoft Office 3.0, die neben Word 2.0, Excel 4.0 und Powerpoint 2.0 auch Mail 3.0 umfasste. 1994 bildeten die drei Programme Word, Excel und Powerpoint in den Office 4.x-Versionen das Paket „Standard“, das in der „Professional“-Version jeweils um die Programme Access und Mail erweitert wurde (4.0 und 4.3). Sie arbeiteten z. B. über OLE 2.0 zusammen.\n\nIm Jahr 1995 erschien \"Microsoft Office 95\" (Version 7), in welchem sich zum ersten Mal alle Komponenten die Versionsnummer – 7.0 – teilten. Zudem war auch Access im Installationsprogramm integriert. Diese Version lief nur noch auf 32-Bit-Betriebssystemen (Windows 95 oder NT 3.51). Vorausgesetzt wurde ein 386er-PC. Ende 1996 wurde \"Microsoft Office 97\" (Version 8) veröffentlicht, das einen 486er-PC und Windows 95 bzw. NT 3.51 voraussetzte (Publisher 98 erforderte ggf. statt NT 3.51 die Version NT 4.0).\n\nBeim Mitte Juni 1999 erschienenen \"Microsoft Office 2000\" (Version 9) war ein Pentium-kompatibler Prozessor als Mindestvoraussetzung angegeben; der Support für Windows NT 3.51 entfiel ganz. Ende Mai 2001 kam \"Microsoft Office XP\" (Version 10). Bei Office XP verlangte die Windows-Version erstmals eine Produktaktivierung per Internet oder Telefon. Ohne diese Aktivierung stellte das Programm nach einigen Starts seine Funktion ein. Windows 95 wurde nicht mehr unterstützt. Office XP war das letzte Office-Paket, das auf MS-DOS-basierten Betriebssystemen lief (Windows 98/ME).\n\nAm 21. Oktober 2003 kam für Windows \"Microsoft Office 2003\" (Version 11) in den Handel. Dort wurde auch die Formular-Verarbeitung InfoPath eingeführt. Die 2003er-Version unterstützte nur noch Betriebssysteme auf NT-Basis, wobei mindestens Windows 2000 verwendet werden musste. Es entfiel die Unterstützung für Windows NT 4.0 und die Produktlinien Windows 98/ME. Office 2003 ist das älteste Office, das offiziell zu Windows 7 kompatibel ist.\n\nAb Version 2007 wurden in der Bedienoberfläche sogenannte Ribbons (kontextbezogene Funktions- bzw. Menüleisten) eingeführt.\n\nDie Office-Version 2016 für Windows ist seit dem 22. September 2015 verfügbar. Der Nachfolger, Office 2019, erschien am 24. September 2018.\n\nMicrosofts Office-Suiten nehmen in den USA und in Europa eine marktbeherrschende Stellung ein; die Komponenten \"Word, Excel, PowerPoint, Access\" und \"Outlook\" bilden in heutigen Büroumgebungen den De-facto-Standard in ihren jeweiligen Anwendungsfeldern und haben ehemals konkurrierende Produkte wie die Textverarbeitung \"WordPerfect\", die Tabellenkalkulationen \"Quattro Pro\" und \"Lotus 1-2-3\" bzw. \"Symphony\" sowie das Datenbankverwaltungssystem \"Paradox\" verdrängt. Die größten Konkurrenten für Microsofts Office-Suite stellen \"OpenOffice.org\" beziehungsweise \"LibreOffice\" dar, das je nach Studie einen Marktanteil zwischen 5 % und 19 % hält.\n\nDer HTML-Editor \"FrontPage\", welcher für Windows erfolgte, war die weltweit meistverkaufte Software in ihrem Marktsegment und in der Premium-Version von MS Office 2000, XP und 2003 enthalten. Trotzdem wurde die Entwicklung 2003 eingestellt. „Microsoft Office SharePoint Designer 2007“ übernahm in MS Office 2007 ähnliche Funktionen, setzt aber einen SharePoint-Server voraus. Im Herbst 2007 trat Microsoft Expression Web die Nachfolge von Microsoft \"FrontPage\" an.\n\nMicrosoft Office für Mac wird in verschiedenen Sprachen lokalisiert und herausgegeben.\n\nUnter Windows kann das englische Originalprodukt mit MUI-Sprachpaketen erweitert werden, sodass Benutzer aus einer Liste installierter Sprachen die für ihn passende jederzeit frei auswählen kann. Hinzu kommen noch Versionen für kleinere Sprachgruppen, die auf dem MUI-Interface aufsetzen und von interessierten Verbänden in Kooperation mit Microsoft realisiert werden können. Seit Sommer 2006 kann mithilfe der Lia Rumantscha für Office 2003 ein \"Language Pack\" inklusive Rechtschreib- und Grammatikprüfung für Rumantsch Grischun heruntergeladen werden; im Frühjahr 2007 wurde eine elsässische Version vom Amt für Sprache und Kultur im Elsass (Office pour la Langue et la Culture d'Alsace, OLCA) vorgestellt. Die Sprachpakete werden entweder mithilfe eines normalen Setup-Vorgangs nachinstalliert, oder durch ein Slipstreaming direkt in die Office-Installation eingebunden.\n\nMicrosoft Office Mobile ist eine speziell auf Smartphones ausgerichtete Variante von Microsoft Office. Offiziell unterstützt werden die Betriebssysteme Windows 10, iOS und Android. Frühere Versionen unterstützten auch Windows Mobile und Windows Phone.\n\nSeit Juni 2010 bietet Microsoft eine reduzierte Ausgabe der Anwendungen Word, Excel, PowerPoint und OneNote als Office Online (vormals Office Web Apps) an. Sie sind Bestandteil von Outlook.com und mit anderen Cloud-Angeboten verknüpft, unter anderem Microsoft OneDrive. Im Zuge der Vorstellung von Office 2013 und Windows 8 hat Microsoft auch eine neue Version der Office Online vorgestellt, die nun vollständig auf HTML5 basiert. Sie sind nicht mehr nur im Internet Explorer, sondern auch anderen Browsern und insbesondere auch im Safari unter Apple iOS nutzbar.\n\nFolgende Komponenten gehören zum derzeitigen Microsoft-Office-System \"(Office 2010 für Windows\", \"Office 2011 für Macintosh)\" und \"Office 2016 für Windows, Mac, iOS und Android\", die teilweise in verschiedenen Ausgaben erhältlich sind sowie zum Großteil auch, oder nur, als Einzelprogramme:\nAnmerkungen:\nKomponenten, welche nicht mehr in den Officepaketen (2010 bzw. 2011) enthalten sind:\nAnmerkungen:\n\nFolgende Server-Komponenten gehören zum derzeitigen Microsoft Office System\n\nDie verschiedenen Versionen von Office werden in unterschiedlichen Zusammenstellungen, sogenannten „Editionen“ angeboten. Diese unterscheiden sich in der Zielgruppe, der Zusammenstellung und dem Preis. Zudem ändert Microsoft die Komponenten dieser Zusammenstellungen von Version zu Version, d. h. die „Standard“-Edition der Version 2003 enthält andere Programme als in der Version 2010 usw. Auch unterscheiden sich die Pakete in der Anzahl der Installationslizenzen, was aber nicht immer aus dem Namen der angebotenen Edition erkennbar ist. \n\n\nAuflistung aller bisher ausgelieferten finalen Versionen, die für Windows, macOS bzw. Mac OS Classic erschienen sind:\nMit der Version 7.0, die nur für Windows herausgegeben wurde, erfolgte ein teilweise mehrfacher Versionssprung in Teilprogrammen, um:\nDie Mac- und Windows-Versionen erhielten ab der darauf folgenden Version 8 zwar jeweils dieselben internen Nummern, waren aber an der Bezeichnung (XP resp. v.X und den jeweils anderen Jahreszahlen) auseinanderzuhalten.\n\nDie untenstehende Tabelle enthält eine vereinfachte Darstellung der Interoperabilität von verbreiteten Dokumentenstandards mit Windows-basierten Office-Versionen.\n\nIn neueren Office-Versionen wird auch ein Export von Dokumenten ins PDF-Format ermöglicht. Ab Office 2013 steht mittels der Funktion \"PDF Reflow\" daneben eine Importmöglichkeit von Daten aus PDF-Dateien bereit. Laut Microsoft wird dabei allerdings nicht das Ziel verfolgt, Anzeige- oder Bearbeitungsprogramme für PDF-Dateien zu ersetzen.\n\nAnmerkungen:\n\nEine Untersuchung der niederländischen Regierung kam im Spätherbst 2018 zu dem Ergebnis, Microsoft Office 2016 und \"365\" sammele persönliche Nutzerdaten und verstößt damit gegen die DSGVO.\n\n\n"}
{"id": "51250", "url": "https://de.wikipedia.org/wiki?curid=51250", "title": "WordPerfect Office", "text": "WordPerfect Office\n\nWordPerfect Office ist eine Bürosoftware (Office-Paket) des kanadischen Software-Herstellers Corel. Es gibt vier verschiedene Paketvarianten: Standard, Professional, Home & Student und Corel Office, die sich in der Zusammenstellung unterscheiden. Die Version X4 unterstützt den Import des OpenDocument-Formats, allerdings noch nicht den Export, also das Speichern von WordPerfect-Dokumenten im OpenDocument-Format. Darüber hinaus besteht auch die Möglichkeit, Dateien aus der aktuellen Version von Microsoft Office zu importieren. Das Speichern in den proprietären Formaten von Microsoft Office ist allerdings nicht möglich. Aufgrund der geringen Nachfrage für den deutschsprachigen Raum gibt es seit X3 keine Version mehr auf Deutsch – lediglich eine englische und eine französische Version sind erhältlich.\n\nDie ersten Versionen entstanden bei der 1979 gegründeten WordPerfect Corporation für Data-General-Computer und in der Version 2.2 schließlich auch für den IBM-PC. Die 1989 erschienene Version WordPerfect 5.1 für DOS war die erfolgreichste überhaupt und verdrängte das bisher erfolgreiche WordStar als Marktführer unter den Textverarbeitungsprogrammen. 1991 erschien auch die erste Version für Microsoft Windows – WordPerfect 5.1.\n\nNach einer in den folgenden drei Jahren engen Zusammenarbeit mit Novell wurde WordPerfect im Juni 1994 von Novell übernommen, wobei der Produktname WordPerfect für das Office-Paket weiterhin Verwendung fand. Doch die Übernahme machte sich nicht bezahlt, und Novell verkaufte WordPerfect im Januar 1996 an Corel – mit einem Verlust von über 200 Millionen Dollar in gerade einmal 18 Monaten.\n\nIn den aktuellen Versionen der Programme WordPerfect, Quattro Pro und Presentations findet sich am linken Bildschirmrand standardmäßig eine vertikale, kontextsensitive Menüleiste. Diese von Corel als „Perfect Expert“ bezeichnete Leiste stellt dem Anwender den schnellen Zugriff auf die Basisaufgaben der jeweiligen Anwendung zur Verfügung.\n\n\n\n\nEine eingebaute PDF-Konvertierung ermöglicht es, Dokumente, Tabellen oder Präsentationen im Portable Document Format zu erstellen. In diesem Format gespeicherte Dokumente können in WordPerfect Office X3 und X4 geöffnet, bearbeitet und für andere Zwecke wiederverwendet werden.\n\nMit der Funktion zum Entfernen von Metadaten (ab WordPerfect Office X3) lässt sich der Zugriff auf vertrauliche Kommentare und Informationen in erstellten Dokumenten steuern.\n\n\nDie zuvor auch „Education Edition“ genannten Versionen von Corel WordPerfect Office X3 und X4 sind nur für Schüler, Studenten und Lehrkräfte vorgesehen und dürfen nur von akkreditierten Schulen und Universitäten, Lehrkräften, Schülern und Studenten, Erziehungsbehörden, Krankenhäusern, Bibliotheken, Museen sowie gemeinnützigen und religiösen Organisationen und Vereinen erworben werden. Die Education Edition unterliegt einem Endbenutzer-Lizenzabkommen mit beschränkten Rechten und darf nicht für kommerzielle Zwecke benutzt werden.\n\n\n\nEingebaute PDF-Konvertierung ermöglicht, Dokumente, Tabellen oder Präsentationen im Portable Document Format zu erstellen. In diesem Format gespeicherte Dokumente können in WordPerfect Office X3 und X4 geöffnet, bearbeitet und für andere Zwecke wiederverwendet werden.\n\nEine Funktion zum Entfernen von Metadaten (ab WordPerfect Office X3) lässt sich der Zugriff auf vertrauliche Kommentare und Informationen in erstellten Dokumenten steuern.\n\n\n\n"}
{"id": "51443", "url": "https://de.wikipedia.org/wiki?curid=51443", "title": "Sniffer", "text": "Sniffer\n\nEin Sniffer (von engl. \"sniff\" für schnüffeln) ist eine Software, die den Datenverkehr eines Netzwerks auf Auffälligkeiten überprüfen kann. Es handelt sich also um ein Werkzeug (Tool) der Netzwerkanalyse.\n\nSniffer ist das englische Wort für Schnüffler. Der Hersteller \"Network General\" bezeichnete damit eine Software zur Analyse von Netzwerken auf Auffälligkeiten im Datenverkehr. Da der Name dieses Produkts die Funktion der Software zutreffend beschrieb und eingängig war, hat er sich zum Begriff für Software dieser Art durchgesetzt.\n\nEin Sniffer kennt den so genannten \"non-promiscuous mode\" und den \"Promiscuous Mode\". Im \"non-promiscuous mode\" wird der ankommende und abgehende Datenverkehr des eigenen Computers „gesnifft“. Im \"Promiscuous Mode\" sammelt der Sniffer den gesamten Datenverkehr an die in diesen Modus geschaltete Netzwerkschnittstelle. Es werden also nicht nur die an ihn adressierten Frames empfangen, sondern auch die nicht an ihn adressierten. Der Adressat eines Frames wird in Ethernet-Netzwerken anhand der MAC-Adresse festgelegt.\n\nWeiterhin ist es von der Netzwerkstruktur abhängig, welche Daten ein Sniffer sehen kann. Werden die Computer mit Hubs verbunden, kann sämtlicher Traffic von den anderen Hosts mitgeschnitten werden. Wird ein Switch verwendet, ist nur wenig oder gar kein Datenverkehr zu sehen, der nicht für das sniffende System selbst bestimmt ist. Allerdings gibt es in diesem Fall mehrere Möglichkeiten wie z. B. ARP-Spoofing, ICMP-Redirects, DHCP-Spoofing oder MAC-Flooding, um trotzdem die Frames empfangen zu können.\n\nEs gibt mehrere Gründe, einen Sniffer zu benutzen:\n\n\"(Produkt-Übersicht: siehe unten)\"\n\nSeit Ende der 1980er Jahre gibt es LAN-Analyzer, allgemein als „Sniffer“ bekannt (benannt nach dem ältesten und lange am weitesten verbreiteten Produkt). Daher wird im Umfeld der LAN-Analyse oft allgemein vom „Sniffer-Einsatz“ gesprochen, ohne dass damit konkret das gleichnamige Produkt gemeint sein müsste, sondern eben lediglich \"irgendein\" Produkt dieser Gattung.\n\nAllgemein wird unterschieden zwischen:\n\nBis Ende der 1990er Jahre waren die Anwender praktisch vollständig auf kommerzielle Produkte angewiesen. Deren Mangel war weniger, dass sie Geld kosteten, sondern vielmehr, dass die Hersteller am Markt vorbei arbeiteten und wichtige Bedürfnisse nicht oder zu spät erkannten. Die Folge war, dass Anwender zur Selbsthilfe griffen (siehe Wireshark). Die Folge ist eine Krise vieler kommerzieller Hersteller.\n\nSeit ca. 2002 hat die Akzeptanz und Verbreitung des GPL-Analyzers \"Wireshark\" (früher Ethereal) immens zugenommen. Wesentliche Gründe sind, dass diese Software via Internet kostenfrei bezogen werden kann, ihre Mächtigkeit, ständige Aktualisierung und ihr Praxisbezug. Noch Ende der 1990er Jahre waren rund zehn bedeutende kommerzielle Hersteller von LAN-Analyzern weltweit am Markt tätig (kleinere nicht mitgerechnet); inzwischen ist die Zahl der nennenswerten Hersteller auf rund fünf gesunken.\n\nDer überaus großen Programmierergemeinde, die Wireshark inzwischen an sich binden konnte, kann auf der Seite der meisten kommerziellen Hersteller nichts mehr entgegengesetzt werden. Außerdem beteiligen sich inzwischen große Unternehmen, die eigene LAN-Protokolle verwenden, an der Entwicklung. Da Wireshark eine offene Plattform ist, hilft beispielsweise Siemens, die eigenen Protokolle der Maschinensteuerung oder der Medizintechnik zu analysieren.\n\nDie vier Hersteller, die LAN-Analyzer mit leistungsfähiger Hardware liefern können, sind (Stand 2007) NetScout durch die Übernahme von Network General (Sniffer), Network Instruments (Observer) und WildPackets (EtherPeek NX, OmniPeek) und die deutsche Firma Consistec mit ihren PA2010e-Lösungen als Nischenlösung für Telekom-Netzbetreiber, die sich eines modifizierten Ethereals/Wiresharks auf eigener Hardware bedienen. Weitgehend zurückgezogen bzw. auf dem Markt in Europa inzwischen eher bedeutungslos geworden sind die Hersteller Agilent (Hewlett-Packard), Acterna (Wavetek, Wandel & Goltermann) und Siemens (deren Hardware-Analyzer \"K1100\" in den 1990er Jahren eingestellt wurde; inzwischen engagiert sich Siemens im Wireshark-Umfeld).\n\nWichtige Produkte der LAN-Analyse in alphabetischer Reihenfolge:\n\nFreie Produkte:\nProprietäre Produkte:\n\n\n"}
{"id": "51908", "url": "https://de.wikipedia.org/wiki?curid=51908", "title": "Enlightenment", "text": "Enlightenment\n\nEnlightenment (meist kurz mit \"E\" bezeichnet), zu deutsch „Erleuchtung“, ist eine freie Desktop-Umgebung für X sowie Wayland, die 1997 von Carsten Haitzler ins Leben gerufen wurde. Die erste Version war ein Hack des bekannten Window Managers FVWM und hieß \"Fvwm-XPM\" (Rastermans FVWM). Im Laufe der Zeit ist die Zahl der Komponenten und der Funktionsumfang so weit angewachsen, dass nun nicht mehr ein bloßer Fenstermanager, sondern eine vollwertige Desktop-Umgebung vorliegt.\n\nEnlightenment wurde in der Folge jedoch als eigenständiges System entwickelt und erfreute sich lange Zeit großer Beliebtheit wegen seiner hohen Konfigurierbarkeit im Vergleich zu den damals gebräuchlichen Systemen wie TWM oder FVWM. Mit der im Oktober 1999 veröffentlichten Version 0.16 (auch als \"E16\" oder \"DR16\" bezeichnet) erreichte er seine größte Stabilität und Verbreitung (unter anderem als Unterbau für Gnome 1). Im Laufe der folgenden Jahre schien die Entwicklung einzuschlafen, und es wurden nur noch Bugfixes veröffentlicht. Während sich Haitzler auf eine Neuentwicklung konzentrierte, übernahm im Jahr 2003 mit der Veröffentlichung von DR16.6 Kim „\"kwo\"“ Woelders die Pflege des DR16-Zweiges.\n\nHaitzlers Designziel mit der Version 0.17 (auch \"E17\" oder \"DR17\") ist es, Enlightenment so konfigurierbar wie nur möglich in den Bereichen Aussehen und Bedienung zu gestalten. Enlightenment besitzt neben der Fensterverwaltung auch einen eingebauten Dateimanager und mehrere unter dem Namen \"Enlightenment Foundation Libraries (EFL)\" zusammengefasste Funktionsbibliotheken.\n\nLange Zeit war der in Entwicklung befindliche Nachfolger E17 nur in einer Alpha-Version im CVS zum freien Herunterladen freigegeben. Im November 2009 wurde bekannt, dass der Elektronikkonzern Samsung die Entwicklung von Enlightenment unterstützen will, um das Programm in eigenen Produkten einzusetzen. Im November 2010 stellte Electrolux (Frigidaire) in erstmaliger Kooperation mit \"ProFUSION\" einen Kühlschrank vor, dessen embedded System mit den EFL Libraries betrieben wird.\n\nDas erste E17-Release (ZERO) erschien am 21. Dezember 2012.\n\nEs ist in den offiziellen Paketquellen von Arch Linux, Debian, Fedora, Gentoo, OpenSUSE, PCLinuxOS, Sabayon Linux, Ubuntu und Void Linux vorhanden. Es war der Standard-Desktop in Bodhi Linux, welches jetzt Moksha verwendet, einen Fork von E17.\n\nMit der Version E20 wurde die Unterstützung für Wayland und FreeBSD ausgebaut. Auch wurden viele interne Widgets mit Elementary ersetzt. Der Desktop bekam eine neue Mixer-Infrastruktur sowie ein überarbeitetes Audio-Gadget. Eine weitere Änderung ist die neue Screen-Management-Infrastruktur.\nDie Enlightenment Foundation Libraries (Initialwort hieraus: EFL) sind ein Satz von Open-Source-Softwarebibliotheken für Grafik, die aus dem Enlightenment-Projekt entstanden sind. Sie werden von Enlightenment.org mit einiger Unterstützung von Terrasoft (mittlerweile von \"Fixstars Solutions\" aufgekauft) entwickelt. Das Ziel des Projektes ist es, mit den EFL einen flexiblen und dennoch mächtigen und einfach zu verwendenden Satz von Werkzeugen zu schaffen, der sowohl die Möglichkeiten von Enlightenment als auch von anderen Projekten, die auf den EFL basieren, verbessert. Dazu sollen die Bibliotheken portabel und optimiert gehalten werden, damit sie sogar auf Geräten wie PDAs funktionieren. Die Bibliotheken wurden für die Entwicklungsversion 0.17 des Fenstermanagers geschrieben.\n\n"}
{"id": "52380", "url": "https://de.wikipedia.org/wiki?curid=52380", "title": "StarOffice", "text": "StarOffice\n\nStarOffice war der Markenname eines Anwendungsprogramms der Klasse der Office-Pakete. Das Textprogramm StarWriter wurde 1984 von Marco Börries vertrieben und später zu einer vollständigen Office-Suite ausgebaut. Sein Unternehmen Star Division wurde später von Sun Microsystems übernommen, um ein Officepaket für mehrere Betriebssysteme – insbesondere Solaris aus eigenem Hause – anbieten zu können. Es bildete die Grundlage für die quellenoffenen Programme OpenOffice.org und des darauf basierenden LibreOffice. Die Entwickler waren an der Spezifizierung und Verbreitung des offenen Dokumentenformats OpenDocument beteiligt.\n\nDie Softwarefirma Star Division wurde 1984 von Marco Börries in Lüneburg gegründet. Er beauftragte den mit ihm befreundeten Programmierer Frank Aeffner mit der Entwicklung von StarWriter (damals noch in der Schreibweise „Star-Writer I“) für das Z80-Heimcomputersystem Schneider/Amstrad CPC unter CP/M. Die Veröffentlichung des Programms erfolgte 1985. Bereits ein Jahr später erschien die dritte Version, offenbar unter Zeitdruck, denn das Handbuch ging im Vorwort auf Funktionen ein, die noch nicht oder gerade noch rechtzeitig fertig wurden und es wurde im Verkaufskarton das ausführlichere Handbuch zur Vorversion zusätzlich beigelegt.\n\nMit dem Erscheinen der ersten i8086-Systeme Schneider PC1512 wurde es auf MS-DOS 3.2 portiert. Später folgte die Integration der anderen Einzelprogramme zu einer Office-Suite mit allen im Büro relevanten Programmmodulen für DOS und nachfolgend bis 1995 für Windows, die nun unter dem Namen „StarOffice“ firmierte. Sun Microsystems erwarb StarOffice 1999 und übernahm fortan Weiterentwicklung und Vertrieb. In der Zeit wurde auch der Quellcode offengelegt, das Paket unter dem Namen OpenOffice.org als Freie Software weiterentwickelt und von Sun mit einigen nicht freien Zusätzen und insbesondere professionellem Service ergänzt und vertrieben. Im Zuge dieser Entwicklung wurde das Dateiformat zunächst auf ein zip-komprimiertes Format (SXW) umgestellt, das dem späteren Format OpenDocument bereits ähnlich war, bevor auf diesen quelloffenen Standard umgestellt wurde (siehe Apache OpenOffice#Dateiformat).\n\nSun wurde am 27. Januar 2010 eine hundertprozentige Tochter der Oracle Corporation und das Programmpaket ab demselben Jahr unter dem Namen \"Oracle Open Office\" vertrieben. Gleichzeitig wurde die Versionsbezeichnung an das damalige OpenOffice.org angepasst, so dass aus „StarOffice Version 9“ „Oracle Open Office Version 3“ wurde, wobei beide der OpenOffice.org-Version 3 entsprachen.\n\nNachdem im September 2010 aus Unzufriedenheit mit Oracles Lizenz- und Entwicklerpolitik durch die von vielen ehemaligen OpenOffice.org-Entwicklern gegründete The Document Foundation die Abspaltung \"LibreOffice\" entstanden war, teilte Oracle im Juni 2011 mit, die Weiterentwicklung des kommerziellen Produkts einzustellen und die Verantwortung für die damalige Open-Source-Variante \"OpenOffice.org\" gänzlich auf die Entwicklergemeinschaft übertragen zu wollen. Oracle übertrug seine Rechte an OpenOffice auf die Apache Software Foundation. Seit April 2011 wird diese Variante unter dem Namen \"Apache OpenOffice\" weiterentwickelt.\n\nÄhnlich wie andere Office-Pakete umfasste StarOffice beziehungsweise Oracle Open Office Module für Textverarbeitung (Writer), Tabellenkalkulation (Calc), Präsentation (Impress), Zeichenprogramm (Draw), einen Formeleditor (Math) sowie eine Datenbankanbindung (Base).\n\nIn früheren Versionen (bis einschließlich StarOffice 5.2) waren auch ein E-Mail-Client (StarOffice Mail) und eine Terminverwaltung (StarOffice Schedule) integriert.\n\nStarOffice war ab der Version 3.0 (1995) so konzipiert, dass es auf verschiedenen Betriebssystemen lauffähig war. Damit war es das weltweit erste plattformübergreifende Officeprogramm. Zuletzt (2011) waren dies die Plattformen Microsoft Windows, macOS, Linux und Solaris (sowohl für die Sun SPARC als auch für die x86 Prozessorarchitektur).\n\nIn Asien wurde StarOffice unter dem Namen StarSuite vermarktet. Die Dateien beider Programme waren austauschbar und kompatibel. Der größte Funktionsunterschied bestand in der Unterstützung von chinesischen, japanischen und koreanischen Schriften (auf Unicode basierend) und Sprachen in der StarSuite.\n\nStarOffice bot einen Extension Manager, der freie und proprietäre Erweiterungen ins Officesystem einband. Die Schnittstelle wurde mit Programmupdate 1 der Version 9 von StarOffice nochmals überarbeitet und mit der des damaligen OpenOffice.org abgeglichen und vereinheitlicht. Sie unterstützte dann keine älteren Erweiterungen mehr. Der \"Duden Korrektor\" in Version 5 konnte so sowohl in OpenOffice.org ab Version 3.0.1 als auch in StarOffice 9 ab PU1 genutzt werden.\n\nStarOffice war eine kommerzielle Variante von OpenOffice.org, dem vollständig quellenoffenen Schwesterprojekt, das inzwischen unter dem Namen Apache OpenOffice weitergeführt wird. Einer der Hauptunterschiede zwischen dem damaligen OpenOffice.org und StarOffice war, dass Oracle und früher Sun für StarOffice Aktualisierungen („Update packs“) anbot, wohingegen die Anwender von OpenOffice.org das Gesamtpaket herunterladen mussten. Allerdings wuchsen die kumulativen Aktualisierungen mit fortschreitender Versionsnummer an und waren beispielsweise bei StarOffice 8 insgesamt größer als die entsprechenden OpenOffice.org-Vollversionen.\n\nWeiterhin integrierte Sun in StarOffice bzw. Oracle OpenOffice ein kommerzielles Rechtschreibprüfmodul, das der freien Variante in OpenOffice.org lange Zeit überlegen war. Außerdem gab es einen Thesaurus, das Datenbankmodul Adabas D, einige Vorlagen und Cliparts. Diese von Sun eingekauften kommerziellen Komponenten wurden aus den Lizenzgebühren von StarOffice finanziert; entsprechende freie Module, die für OpenOffice.org verfügbar waren, mussten gesondert nachinstalliert werden. Die Deluxe-Edition enthielt Internetprogramme, einen Personal Information Manager (PIM) und mit Lightning eine freie, vom Mozilla Kalenderprojekt entwickelte Kalenderanwendung.\n\nEin weiterer Unterschied zur Open-Source-Variante bestand lange Zeit darin, dass Sun zunächst nur für StarOffice und die StarSuite kommerzielle Kundenunterstützung anbot, was für viele Unternehmen bezüglich OpenOffice.org ein Ausschlusskriterium darstellte. Diese Politik wurde allerdings später geändert und Sun bot für beide Programme kommerzielle Dienstleistungen an. Die drei kostenlosen Anfragen für Einzelplatzlizenzen von Oracle Open Office wurden mit der Übernahme durch Oracle weitergeführt. Für Einzelbenutzer gab es telefonischen Support und ein Ticket-Helpdesk, über das lizenzierte User mittels Seriennummer ein Support-Ticket eröffnen konnten.\n\nStarOffice wurde nach mehrmaliger Änderung des Lizenzmodells am Ende wieder nach der Anzahl der Benutzer lizenziert. Erhältlich waren die \"Standard Edition\" in der Sprache und für das Computersystem des Downloads (wie bei OpenOffice.org) für einen Benutzer und eine Installation oder die \"Enterprise Edition\" für alle 17 unterstützten Sprachen und alle unterstützten Systeme ab 100 Benutzern. Früher wurde das Programm je Nutzer lizenziert und lizenzierte Benutzer konnten es bis zu fünfmal beispielsweise auf Notebook und Desktop, Linux und Windows installieren. Für Schüler, Studenten und akademische Einrichtungen konnte StarOffice lizenzkostenfrei heruntergeladen werden, ab der Version 9 wurde diese Gruppe aber auf das OpenOffice.org-Projekt verwiesen. Die Solaris-Variante blieb hingegen kostenlos.\n\nVon August 2007 bis November 2008 konnte die Version 8 von StarOffice für Privatnutzer kostenlos als Teil des Google Packs bezogen werden.\n\nDurch die freie Lizenzierung sind nach der Veröffentlichung von OpenOffice.org zahlreiche Derivate für unterschiedliche Plattformen entstanden, die die nachfolgende Grafik aufzeigt. In der nachfolgenden Tabelle sind v. a. die Abhängigkeiten zwischen StarOffice und OpenOffice.org gegenübergestellt, da es sich bei OpenOffice.org genaugenommen nicht um ein Derivat handelt, sondern lediglich um eine abweichende Lizenzierung ohne lizenzpflichtige Softwaremodule.\n\n „Product Update“, die Aktualisierungen der StarOffice-Vollversionen mit Fehlerbehebungen und neuen Funktionen\n\n"}
{"id": "52728", "url": "https://de.wikipedia.org/wiki?curid=52728", "title": "OpenSSH", "text": "OpenSSH\n\nOpenSSH ist ein Programmpaket zur Dateiübertragung. Dazu nutzt es Secure Shell (SSH) inklusive SSH File Transfer Protocol und beinhaltet dafür Clients, Dienstprogramme und einen Server.\n\nOpenSSH entstand 1999 als Abspaltung einer älteren Implementierung von Tatu Ylönen, die noch freie Software war. Programmierer von OpenBSD vereinfachten dieses Derivat von Björn Grönvall, brachten es auf den aktuellen Stand des Protokolls, und veröffentlichten ihre erste eigene Version als OpenSSH 1.2.2.\n\nBereits zur Premiere als Bestandteil von OpenBSD gab es OpenSSH auch für FreeBSD und Linux. Inzwischen arbeitet OpenSSH auf fast allem, was auch nur im entferntesten Unix ist, einschließlich Cygwin.\n\nUnternehmen wie IBM und Hewlett-Packard bieten OpenSSH für ihre Betriebssysteme wie AIX und HP-UX mit Erweiterungen an, und auch Microsoft bietet OpenSSH für Windows Server 2019 an.\n\nDie Lizenzierung von OpenSSH ist unübersichtlich, weil mehrere Lizenzen übernommen wurden, was letztlich der ursprünglichen BSD-Lizenz gleichkommt. Bei vielen Funktionen zur Verschlüsselung greift OpenSSH auf LibreSSL zurück.\n\nOpenSSH beinhaltet:\n\nNeben der Hauptaufgabe des Fernzugriffs bietet OpenSSH zusätzliche Funktionen wie Portweiterleitung an. Mit Portweiterleitung kann der Client einen lokalen Port öffnen und TCP-Verkehr über den Server an ein entferntes Ziel weiterleiten. Dies kann z. B. genutzt werden, um mit OpenSSH auf geschützte Ressourcen innerhalb eines Netzwerks zuzugreifen, ohne Aufbaus eines VPN. Die Portweiterleitung kann auch in umgekehrter Richtung erfolgen, wobei der Server Zugriff auf die Umgebung des Clients erhält.\n\nDie Entwicklung von OpenSSH ist in zwei Teams aufgeteilt. Das erste Team entwickelt nur für OpenBSD, um dessen Code möglichst einfach und sicher zu halten. Das zweite Team ergänzt OpenSSH zu einer Variante mit dem Buchstaben p in der Versionbezeichnung, die auf anderen Implementierungen von Unix und unixoiden Systemen einsetzbar ist.\n\n\n"}
{"id": "53129", "url": "https://de.wikipedia.org/wiki?curid=53129", "title": "VectorLinux", "text": "VectorLinux\n\nVectorLinux ist eine auf Slackware basierende Linux-Distribution aus Vancouver (Kanada). Die Distribution wird von Robert S. Lange, seiner Ehefrau Cara Leah Lange und Darrell Stavem entwickelt und läuft auf Intel-, AMD- und x86- kompatiblen PC-Systemen ab i586. Die Programmierer von VectorLinux haben sich zum Ziel gesetzt, die Distribution einfach und klein zu halten und den Benutzer entscheiden zu lassen, was das Betriebssystem machen soll. Das Betriebssystem ist in verschiedenen Varianten erhältlich und für kleinere Desktops bis zu Servern einsetzbar. \n\nEs werden vier Editionen von VectorLinux angeboten: SOHO, Standard, Light und Live.\n\nDie SOHO (Small Office / Home Office) Edition ist für moderne Computer konzipiert und basiert auf der KDE Software Compilation 4 Desktop-Umgebung. Zu den Anwendungen gehören LibreOffice, Java, GIMP, Xsane, CUPS, Xara Xtreme, Finanz-Anwendungen und weitere. Eine Deluxe-Edition von SOHO, die zusätzliche Anwendungen enthält, ist ebenfalls verfügbar.\n\nStandard Edition ist eine Ausgabe auf Basis von Xfce und speziell für den Einsatz auf älteren Computern mit langsameren Prozessoren und weniger RAM (ab 128 Megabyte) ausgelegt.\n\nDie kostenpflichtige „Deluxe“-Version enthält zusätzlich zur Standard-Edition viele Zusatzpakete, wie beispielsweise KDE oder das Office-Paket OpenOffice.org. Die CD enthält viele zusätzliche Anwendungen.\n\nDie Light Edition ist für ältere Computer mit begrenztem Festplattenplatz ausgelegt. Sie basiert auf den ressourcensparenden Fenstermanagern LXDE, JWM, und Fluxbox und läuft bereits ab 64 Megabyte RAM. Somit ist sie beispielsweise vergleichbar mit der Linux-Distribution Damn Small Linux.\nSie wird mit Opera als Browser / E-Mail / Chat-Client geliefert und enthält einige der Anwendungen aus der Standard Edition. Weitere Anwendungen stehen über den Paketmanager oder durch Kompilieren durch den Benutzer zur Verfügung. Die Light Edition ist auch einsetzbar auf Computern mit nur 64 MB RAM.\n\nDie Live-Editionen sind bootfähige CD-ROMs und ermöglichen dem Benutzer das Testen der Distribution, ohne sie auf der Festplatte installieren zu müssen. Man kann VectorLinux jedoch auch aus der Live-Edition-CD installieren. VectorLinux hat derzeit zwei Live-Editionen: Standard- und SOHO. Die neueste Ausgabe ist eine Vorschau auf die Deluxe Edition.\n\n"}
{"id": "53356", "url": "https://de.wikipedia.org/wiki?curid=53356", "title": "Yellow Dog Linux", "text": "Yellow Dog Linux\n\nYellow Dog Linux (Kurzform YDL) ist eine Linux-Distribution für Computer mit einem PowerPC- oder Power-Prozessor. Darunter fallen Arbeitsplatzrechner der Apple-Macintosh-Reihe (mit PowerPC-Prozessor; von 1994 bis 2006) und die originale PlayStation 3 (mit Cell-Prozessor) ebenso wie Hochleistungsrechner wie beispielsweise IBMs System p (mit Power5-Prozessoren).\n\n\"Yellow Dog Linux\" wurde erstmals 1999 von der in Colorado, USA ansässigen Firma \"Terra Soft Solutions\" veröffentlicht. Durch ein Lizenzabkommen mit Apple durfte \"Terra Soft Solutions\" Macs mit vorinstalliertem Linux verkaufen. \"Yellow Dog Linux\" wurde von Beginn an auf die PowerPC-Architektur optimiert und war bis zum Ende der PowerPC-basierten Macs 2006 die am weitesten verbreitete Linux-Distribution auf dieser Rechnerarchitektur.\n\nIm November 2008 wurde \"Terra Soft Solutions\" von der in Japan ansässigen Firma \"Fixstars Corporation\" aufgekauft.\n\nYellow Dog Linux (YDL) basierte ursprünglich auf Red Hat Linux und nutzt zur Paketverwaltung den Yellowdog Updater, Modified (YUM). Spätere Versionen von YDL basierten auf Fedora, der 2003 aus Red Hat Linux hervorgegangen freien Linux-Distribution von Red Hat. Version 7, die derzeit aktuelle Version, basiert auf CentOS. YUM selbst läuft daher auf allen kompatiblen Linux-Distributionen, also neben Fedora und CentOS auch auf RHEL.\n\nYDL verwendet das Paketverwaltungssystem RPM Package Manager und beinhaltet bekannte Programme wie Glibc, GCC, Xorg, KDE, Gnome und eine aktuelle Version des Linux-Kernels.\n\nYellow Dog Linux hat allgemein eine gute Hardwareunterstützung und galt auch auf alten Rechnern als recht flott. Manche Hardwarekomponenten, die in PowerPC-basierten Apple-Rechnern verbaut wurden, können jedoch unter Linux überhaupt nicht oder nur rudimentär genutzt werden, da die Hardwarehersteller zum einen keine eigenen Gerätetreiber für die Plattform Linux/PPC anbieten (z. B. gibt es die proprietären Linux-Treiber von ATI/AMD- und Nvidia nur für die IA-32-Architektur) und oft auch keine Spezifikationen der Hardware freigeben, so dass ein Entwickeln von Treibern seitens der Open-Source-Gemeinschaft nur sehr schwer möglich ist (z. B. Broadcom-WLAN-Chips, die als Grundlage für Airport Extreme dienten). Durch das Entwicklungsmodell von Linux profitiert jedoch auch die PowerPC-Architektur von Treibern, die eigentlich für IA32 entwickelt wurden: fast alle PCI-, PCIe- und USB-Komponenten, für die (quelloffene) Linux-Unterstützung existiert, funktioniert auch auf PowerPC-Rechnern.\n\nZu Beginn war der Fokus der Entwicklung auf Rechner gelegt, die auf dem Markt verfügbar waren. Dadurch sind fast alle Apple Macintosh-Rechner, die PowerPC-basiert sind, von Yellow Dog Linux unterstützt. Doch 2006 wurden sämtliche Apple-Computer auf Intel-Prozessoren umgestellt. PowerPC-Rechner waren plötzlich rar. Der Fokus der Entwicklung wurde daher zusehends in Richtung Server verschoben, zum Einsatz kommen meist Power-Prozessoren. Auch Cell-Prozessoren, die ein Abkömmling der PowerPC-Architektur sind, wurden von Yellow Dog Linux als erste Distribution unterstützt – bereits 2007 plante die United States Air Force in einem Forschungszentrum über 2.000 PlayStation 3 als Cluster zu einem Supercomputer zusammenzuschließen. Seit 2010 ist der Cluster mit dem Namen “Condor” in Betrieb und besteht aus 1.760 Konsolen, auf denen Yellow Dog Linux läuft. Er verarbeitet 500 Teraflops/s und lag 2010 an 12. Stelle auf der Liste der Top-500-Supercomputer der Welt.\n\nDie aktuelle Version ist seit Dezember 2012 YDL 7, die jedoch nur gemeinsam mit IBM PowerLinux 7R2 (mit Power7-Prozessoren) und dem Eiger EG-2S Embedded-Cell-Board verfügbar ist.\n\nVon dem für weitere System ebenfalls noch verfügbare YDL 6 werden folgende Systeme offiziell unterstützt:\n\nÄltere Computer sind nicht mehr offiziell unterstützt, funktionieren aber mit kleinen Einschränkungen. Diese sind z. B. G3-Macs, die unter YDL 5.x noch offiziell unterstützt wurden:\n\nSeit YDL Version 4 wird zur Installation ein DVD-Laufwerk vorausgesetzt und yaboot wird zum Start des Betriebssystems verwendet, was neuere Macs mit PowerPC G3-Prozessor (→) voraussetzt.\n\nBis YDL 3.x wurden auch ältere Macs () unterstützt, da bootx zum Betriebssystemstart verwendet wurde. Auch diverse Mac-Clones waren dadurch mit YDL startbar. Voraussetzung dafür war jedoch zumindest ein PowerPC-Prozessoren der 2. Generation (PowerPC 603).\n\nIm Oktober 2006 wurde von Sony verkündet, dass Yellow Dog Linux mit der neuen Version 5 die Mitte November 2006 erschienene PlayStation 3 (Kurzform: PS3) unterstützen wird. Danach wurde bekanntgegeben, dass für rund US-$ 50 in einer Offline-Variante ohne Support, US-$ 100 in einer mit Support und zwei Wochen nach dem Launch auch als kostenlose Online-Variante zur Verfügung stehen wird. Weiter soll in die Distribution auch Code vom Barcelona-Supercomputer-Center eingeflossen sein. Die Distribution lässt sich auf der in der PlayStation 3 integrierten Festplatte installieren und als vollwertiges Betriebssystem nutzen. Mitgeliefert wird auch eine Menge an Programmen wie Firefox, Thunderbird oder OpenOffice.org. Es wird die Desktop-Umgebung Enlightenment 17 genutzt. Durch ein Abkommen von \"Terra Soft Solutions\" mit \"Sony Computer Entertainment\" wird Yellow Dog Linux auf der PS3 offiziell unterstützt und lässt sich ohne Hürden installieren.\n\nDie neue PlayStation 3 Slim, die im September 2009 in den Handel kam, unterstützt die genannte Funktion nicht mehr, sodass kein Linux mehr installiert werden kann.\n\nAuch auf der originalen PlayStation 3 wurde mit der Firmwareaktualisierung 3.21 vom 1. April 2010 diese Funktion entfernt, was zu weltweiter Kritik und in den USA sogar zu einem Gerichtsverfahren gegen Sony führte. Die letzte offizielle Firmware, die Linux als anderes Betriebssystem () auf der PlayStation 3 ermöglicht, ist Version 3.15 vom 10. Dezember 2009.\n\n\"→ Entfernung der Unterstützung anderer Betriebssysteme im Artikel PlayStation 3\"\n\nSeit August 2012 gibt es Yellow Dog Linux 7, jedoch nur gemeinsam mit IBM PowerLinux 7R2 Server-Rack. Die \"Fixstars Corporation\" wirbt dafür unter anderem mit der Erfahrung aus dem Cluster-Project mit der United States Air Force. Als wesentliche Neuerung wird die automatische Nutzung von Vektorisierung (AltiVec) durch GCC 4.7 und die optionale Parallelisierungslösung angeführt. YDL 7 nutzt Linux-Kernel 2.6.32, GCC 4.4.6 und 4.7.0 und glibc 2.12.\n\nIm November 2007 wurde Version 6.0 von Yellow Dog Linux, noch von \"Terra Soft Solutions\", herausgegeben. Diese Version wurde spezifisch an die Hochleistungsrechner IBM BladeCenter QS21 angepasst, die mit Cell-Prozessoren betrieben werden, und war bis zum 3. März 2008 nur als sogenanntes \"Board Support Package\" käuflich zu erwerben. Auf Basis von CentOS 5 bringt YDL 6.0 den Linux-Kernel 2.6.22, die glibc 2.5, GCC 4.1.1, die grafische Oberfläche Enlightenment 17, OFED 1.2.5 für Infiniband-Unterstützung sowie Komponenten aus IBMs Software Development Kit für Multicore-Beschleunigung in der Version 3.0 mit.\nBei der aktuellen Version 6.2 vom Juni 2009, die bereits durch die \"Fixstars Corporation\" freigegeben wurde, sind – nach eigenen Angaben – folgende Verbesserungen enthalten:\nGenerell wurden alle vorhandenen Funktionen verbessert.\n\nPowerPC G3-Prozessoren wurden bis Version 5 offiziell unterstützt. PowerPC 603-Prozessoren (2. Generation) wurden bis YDL 3 unterstützt.\n\n"}
{"id": "54303", "url": "https://de.wikipedia.org/wiki?curid=54303", "title": "Linux-Distribution", "text": "Linux-Distribution\n\nEine Linux-Distribution ist ein Paket aufeinander abgestimmter Software um den Linux-Kernel. Manchmal wird der Begriff auf Zusammenstellungen begrenzt, die weitgehend linuxtypisch aufgebaut sind, was beispielsweise auf Android nicht zutrifft.\n\nDistributionen, in denen GNU-Programme eine essenzielle Rolle spielen, werden auch als „GNU/Linux-Distributionen“ bezeichnet. Die Namensgebung mit oder ohne GNU-Namenszusatz wird von den Distributoren je nach ihrer Position im GNU/Linux-Namensstreit unterschiedlich gehandhabt.\n\nFast jede Distribution ist um eine Paketverwaltung herum zusammengestellt, d. h., dass sämtliche Bestandteile der Installation als Pakete vorliegen und sich über den Paketmanager installieren, deinstallieren und updaten lassen. Die Pakete werden dazu online in sogenannten Repositories vorgehalten.\n\nZusammengestellt wird eine Linux-Distribution von seinem Distributor. Für gewöhnlich wählt dieser Programme aus, bei denen er die nötigen Rechte hat, passt sie mehr oder weniger an, paketiert sie in seiner Paketverwaltung und bietet das Ergebnis als Distribution an. Normalerweise werden lediglich wenige Programme vom Distributor selbst geschrieben, z. B. meist der Distributions-Installer. Der Distributor kann ein Unternehmen oder eine Gruppe von weltweit verteilten Freiwilligen sein. Er kann auch kommerziellen Support anbieten.\n\nSinn einer Distribution ist es, ein Paket aufeinander abgestimmter Software zu bilden. Den zentralen Teil bilden dabei der Linux-Kernel selbst sowie Systemprogramme und Bibliotheken. Je nach dem vorgesehenen Anwendungszweck der Distribution werden verschiedene Anwendungsprogramme (z. B. Webbrowser, Office-Anwendungen, Zeichenprogramme, Mediaplayer etc.) hinzugefügt.\n\nLinux-Distributionen halten in der Regel eine große Anzahl an Programmen in den Repositories zur Installation bereit. Dies steht im konzeptuellen Gegensatz zu anderen Betriebssystemen wie Windows und macOS, die neben dem Betriebssystem selbst nur wenige Anwendungen enthalten, dafür auf die Integration von Programmen von externen Anbietern, sogenannten ISVs, setzen.\n\nWeitere Aufgaben der Distributionen sind die Anpassung der Programme (durch Patchen), Hinzufügen eigener Programmentwicklungen (vor allem zur Installation und Konfiguration des Systems wie zum Beispiel apt, Synaptic, YaST) sowie (bis auf wenige Ausnahmen, z. B. Gentoo) Kompilierung und Paketierung (.deb, .rpm) der Programme. Die Bereitstellung von zusätzlichen Programmen und Updates erfolgt typischerweise zentral über ein Repository, welches über ein Paketverwaltungs-System mit dem Betriebssystem synchronisiert wird.\n\nAuch wenn bei Linux-Betriebssystemen Distributionen die bei weitem üblichste Variante sind, ist ein Betrieb von Linux auch ohne eine vorgefertigte Distribution möglich, zum Beispiel mithilfe von Linux From Scratch.\n\nNeben dem Linux-Kernel besteht eine Distribution meist aus der GNU-Software-Umgebung. Diese stellt große Teile des grundlegenden Basissystems mit den zahlreichen Systemdiensten (sogenannten Daemons) sowie diverse Anwendungen bereit, die bei einem unixoiden System erwartet werden. Distributionen, welche auch oder nur für Desktop-Systeme gedacht sind, verfügen normalerweise über ein Fenstersystem, derzeit meistens das X Window System. Ein solches ist für das Ausführen einer grafischen Benutzeroberfläche erforderlich. Darauf aufbauend steht meist eine Desktop-Umgebung, wie bspw. Gnome oder die KDE Software Compilation zur Verfügung, welche neben der reinen Benutzeroberfläche noch eine Auswahl an Anwendungsprogrammen mitbringt.\n\nErgänzend fügt ein Distributor normalerweise zahlreiche weitere Anwendungen bei. Dies sind beispielsweise Office-Pakete, Multimediasoftware, Editoren, E-Mail-Programme, Browser, aber auch Server-Dienste. Daneben finden sich meist Softwareentwicklungs-Werkzeuge wie Compiler bzw. Interpreter sowie Editoren.\n\nViele Softwarebestandteile von Linux-Distributionen, z. B. der Compiler GCC, stammen aus dem älteren GNU-Projekt. Dieses hatte sich schon vor der Entwicklung von Linux die Aufgabe gestellt, eine Alternative zu den kommerziellen Unix-Betriebssystemen zu entwickeln. Da der eigene Kernel des GNU-Projekts, GNU Hurd, noch in der Entwicklung ist, wird häufig als Ersatz der Linux-Kernel benutzt. Daher ist auch der Doppelname \"GNU/Linux\" für eine Distribution geläufig (z. B. bei Debian).\n\nEs gibt auch Linux-Distributionen, die auf die GNU-Softwareanteile oder ein X Window System komplett verzichten und stattdessen alternative Software nutzen. Diese Distributionen verhalten sich, wie beispielsweise FreeVMS oder Cosmoe, teilweise auch nicht annähernd wie ein Unix-System.\n\nWährend proprietäre Betriebssysteme häufig über den Einzelhandel vertrieben werden, ist dies bei Linux-Distributionen eher die Ausnahme. Die meisten Distributionen können heute kostenlos von der Website der Anbieter heruntergeladen werden. Diese finanzieren sich über Spenden, über kostenpflichtigen Support oder auch einfach nur über die Beteiligung von Freiwilligen. Nur vergleichsweise wenige Distributionen werden von gewinnorientierten Firmen entwickelt und sind teilweise über den Einzelhandel verfügbar. Zahlreiche Linux-Distributionen werden auch, von den Kunden unbemerkt, als Firmware auf einem Gerät oder sogar in größeren Maschinen oder Anlagen erworben. Dabei kann es sich z. B. um Werkzeugmaschinen, Fahrzeuge, Haushaltsgeräte, SPS, Messgeräte, Mobiltelefone, Modems, Digitalkameras, NAS oder Fernseher handeln.\n\nDa Linux nur ein Betriebssystem-Kernel ist, wird weitere Software benötigt, um ein benutzbares Betriebssystem zu erhalten. Aus diesem Grund kamen die ersten Linux-Distributionen schon kurz nach der GPL-Lizenzierung von Linux auf, als Anwender, die nicht zum direkten Entwicklerkreis gehörten, Linux zu nutzen begannen. Die ersten Distributionen hatten dabei das Ziel, das System beispielsweise mit der Software des GNU-Projekts zu einem arbeitsfähigen Betriebssystem zu bündeln. Zu ihnen gehörten \"MCC Interim Linux\", das auf den FTP-Servern der University of Manchester im Februar 1992 veröffentlicht wurde sowie \"TAMU\" und Softlanding Linux System (SLS), die etwas später herauskamen. Die erste kommerziell auf CD erhältliche Distribution war 1992 das von Adam J. Richters entwickelte Yggdrasil Linux. 1993 veröffentlichte Patrick Volkerding die Distribution Slackware, die auf SLS basiert. Sie ist die älteste heute noch aktive Linux-Distribution. Ebenfalls 1993, ungefähr einen Monat nach der Veröffentlichung von Slackware, wurde das Debian-Projekt ins Leben gerufen, das im Gegensatz zu Slackware gemeinschaftlich entwickelte. Die erste stabile Version kam 1996 heraus. Die heute wahrscheinlich am häufigsten verwendete PC-Distribution, Ubuntu, wurde 2004 von Canonical herausgebracht.\n\nDie ersten Nutzer kannten noch freie Software aus der Zeit vor den 1980er-Jahren und schätzten Linux, weil sie wieder die Verwertungsrechte an der von ihnen verwendeten Software besaßen. Spätere Nutzer waren Unix-Anwender, die Linux zunächst vor allem privat einsetzten und sich vor allem über den geringen Preis freuten. Waren die ersten Distributionen nur der Bequemlichkeit halber geschaffen worden, sind sie doch heute die übliche Art für Nutzer wie auch Entwickler, ein Linux-System zu installieren. Dabei werden die Linux-Distributionen heutzutage sowohl von Entwicklergruppen als auch von Firmen oder gemeinnützigen Projekten entwickelt und betrieben.\n\nDie Frage, welche Distributionen besonders beliebt sind, lässt sich nur schwer beantworten. Im deutschsprachigen Raum werden vor allem Ubuntu, Debian, openSUSE und Knoppix häufiger auch außerhalb der IT-Presse erwähnt. Darüber hinaus wäre Fedora zu nennen, das von dem börsennotierten US-Unternehmen Red Hat entwickelt wird.\n\nDa Distributionen praktisch eigene Produkte sind, konkurrieren diese am Markt miteinander und versuchen, sich einerseits voneinander abzugrenzen, andererseits aber auch anderen Distributionen keinen zu großen Vorteil zu überlassen. Daher unterscheiden sich zwar sämtliche Distributionen; es gibt aber kaum etwas, wofür sich nicht jede Distribution anpassen ließe. Hiervon ausgenommen sind nur Spezial-Systeme, etwa als Software im Embedded-Bereich.\n\nEinige Distributionen sind speziell auf einen Anwendungsfall optimiert. So gibt es etwa Systeme speziell für den Einsatz in Bildungseinrichtungen mit hierfür spezialisierter Software und zumeist einem Terminalserver-System, wodurch nur ein leistungsstarker Rechner benötigt wird und ansonsten auch ältere Hardware ausreicht. Beispiele sind hier Edubuntu oder DebianEdu. Ebenso gibt es Systeme speziell für veraltete Rechner, die einen geringeren Funktionsumfang haben und geringe Systemanforderungen stellen. Beispiele sind etwa Damn Small Linux oder Puppy Linux, die einen Umfang von nur 50 beziehungsweise 100 MB haben.\n\nFür Smartphones und Tablets gibt es speziell optimierte Linux-Distributionen. Sie bieten neben den Telefonie- und SMS-Funktionen diverse PIM-, Navigations- und Multimedia-Funktionen. Die Bedienung erfolgt typischerweise über Multi-Touch oder mit einem Stift. Linux-basierte Smartphone-Betriebssysteme werden meist von einem Unternehmenskonsortium oder einem einzelnen Unternehmen entwickelt und unterscheiden sich teilweise sehr stark von den sonst klassischen Desktop-, Embedded- und Server-Distributionen. Anders als im Embedded-Bereich sind Linux-basierte Smartphonesysteme aber nicht auf ein bestimmtes Gerät beschränkt. Vielmehr dienen sie als Betriebssystem für Geräte ganz unterschiedlicher Modellreihen und werden oft herstellerübergreifend eingesetzt.\n\nDie Architektur vieler Linux-basierter Smartphone- und Tablet-Betriebssysteme wie z. B. Android hat neben dem Linux-Kernel nur wenig Gemeinsamkeiten mit klassischen Linux-Distributionskonzepten. Ob Android als wichtigstes Linux-Kernel basierendes Smartphone-Betriebssystem auch als Linux-Distribution einzuordnen ist, wird kontrovers diskutiert. U.a. wird typischerweise auch nur ein kleiner Teil der sonst üblichen GNU-Software-Umgebung und -Tools genutzt. Da Android nicht vollständig freie Software ist und Googles Android Market die Verwendung unkontrollierter proprietärer Binär-Software ermöglicht, stehen Richard Stallman und die FSF Android sehr kritisch gegenüber und empfehlen die Verwendung von Alternativen. Die meist auf Linux genutzten UNIX-artigen Dienste und Tools werden teilweise durch eine Java-Laufzeitumgebung ersetzt. Dadurch entstehen neue Programmierschnittstellen, die sich auf beliebigen anderen Plattformen emulieren bzw. umsetzen lassen. Trotz großer Diskrepanzen wird Android jedoch von manchen über gemeinsame Eigenschaften mit Embedded-Linux-Distributionen bei den Linux-Distributionen eingeordnet. Andere Linux-basierende Smartphone-Betriebssysteme wie etwa Firefox OS, Ubuntu for phones, Maemo, Tizen, Mer, Sailfish OS und MeeGo nutzen größere Teile der klassischen GNU-Software-Umgebung, so dass diese teilweise einfacher mit klassischen Linux-Anwendungen ergänzt werden können und somit eher Linux-Distributionen im klassischen Sinne entsprechen.\n\nWährend die Marktanteile von bisher verbreiteten Mobil-Plattformen wie Apples iOS, Microsofts Windows Mobile und Nokias Symbian OS sanken, konnte Android Marktanteile hinzugewinnen. Seit Ende 2010 haben Linux-Systeme die Marktführerschaft auf dem schnell wachsenden Smartphone-Markt übernommen. Sie wiesen zusammen im Juli 2011 einen Marktanteil von mindestens 45 % auf. Aktuell ist Android die mit großem Abstand verbreitetste Linuxdistribution für Smartphones. Der Marktanteil lag im Mai 2016 bei 78 %.\n\nLinux ist ein beliebtes Betriebssystem in eingebetteten Systemen. Entsprechende Distributionen sind für gewöhnlich sehr abgespeckt, da sie nur auf bestimmte Aufgaben ausgelegt sind. Beispielsweise ist meistens keine oder nur eine spezialisierte grafische Oberfläche zu finden. Häufig handelt es sich um Echtzeitsysteme. Ähnlich Smartphonesystemen haben sie oft wenig Ähnlichkeit mit gewöhnlichen Distributionen.\n\nEine Besonderheit bilden Live-Systeme, die von CD, DVD, USB und anderen Medien gebootet werden. Handelte es sich hierbei zunächst nur um spezialisierte Distributionen, die den Funktionsumfang von Linux demonstrieren sollten, gehört es inzwischen zum guten Ton unter Linux-Distributionen, den Standard-Umfang in Form einer Live-CD oder -DVD bzw. einem Live-USB-Speicherstick anzubieten. Einige dieser Systeme lassen sich auch direkt von dem Medium aus installieren.\n\nLive-Systeme können als vollständiges Linux gestartet werden, ohne auf die Festplatte zu schreiben und ohne die bestehende Konfiguration eines Rechners zu verändern. So kann die entsprechende Linux-Distribution gefahrlos auf einem Computer getestet werden. Live-Systeme eignen sich auch hervorragend zur Datenrettung und Systemanalyse, da sie von der Konfiguration des bereits bestehenden Systems unabhängig sind und so auch von möglichen Infektionen durch Würmer und Viren nicht betroffen werden können.\n\nDie meisten Linux-Distributionen können auf derselben Hardware parallel zu anderen Betriebssystemen installiert werden. Als solche kommen bspw. eine weitere Linux-Distribution, ein anderes unixoides Betriebssystem wie macOS oder Solaris, oder aber auch ein Windows in Betracht. Prinzipiell sind zwei Vorgehensweisen zu unterscheiden:\n\nIn einer Multi-Boot-Konfiguration werden zwei oder mehr Betriebssysteme parallel auf verschiedene Festplatten-Partitionen installiert. Installationsprogramme moderner Linux-Distributionen können meist bereits installierte Betriebssysteme erkennen und eigenständig eine Multi-Boot-Konfiguration einrichten. Nach der Installation kann beim Bootvorgang über einen Bootloader oder Bootmanager gewählt werden, welches Betriebssystem starten soll.\n\nWerden die Betriebssysteme häufig gleichzeitig genutzt, bietet sich u. U. eher eine Virtualisierungs-Lösung an. Zu unterscheiden sind hierbei das Host- und Gast-System. Ersteres ist tatsächlich physisch auf der Hardware installiert. Innerhalb dessen kommt eine Virtualisierungssoftware wie bspw. VirtualBox oder KVM zum Einsatz. Diese emuliert für das Gast-System die gesamte erforderliche Hardware oder bietet durch ein Sicherheitssystem direkten Zugriff auf die tatsächlich vorhandene Hardware des Computers. Da diese in einer solchen Konfiguration für den gleichzeitigen Betrieb beider Systeme erforderlich ist, kann es zu Geschwindigkeitseinbußen kommen.\n\nAuch wenn man Spezial-Distributionen außer Acht lässt, unterscheiden sich auch gängige Linux-Distributionen in einigen Punkten.\n\nEinige Distributionen für Fortgeschrittene haben zum Beispiel keinen Installer, sondern nur eine Live-CD, die die nötigen Werkzeuge zur manuellen Installation bereitstellt (bspw. Arch und Gentoo). Die meisten bieten allerdings einen Installer in Form eines Assistenten an. Einige bieten zwar einen Assistenten an, erfordern aber Vorarbeiten, etwa das Partitionieren (bspw. Slackware). Die sonstige Art der Konfiguration entspricht normalerweise der Installationsmethode. Bei manchen Systemen muss man also die Konfigurationsdateien i. d. R. direkt bearbeiten, während andere für die wichtigsten Optionen Tools bereitstellen.\n\nEin wichtiger Punkt ist auch die kostenlose Verfügbarkeit. Einige wenige Distributionen kosten Geld (bspw. RHEL), während die meisten kostenlos sind.\n\nWeiter unterscheiden sich Distributionen in der Anzahl der unterstützten Architekturen (besonders vielfältig sind Gentoo und Debian). Auch spielen Art und Umfang der Dokumentation eine Rolle. So liegen einigen Produkten Handbücher bei (bspw. RHEL), während für die meisten nur Dokumentation auf Webseiten zur Verfügung steht. Manche Distributionen verzichten ganz auf eine offizielle Dokumentation und lassen diese lieber – bspw. als Wiki – von der Nutzerschaft pflegen. Kommerzielle Distributoren bieten darüber hinaus meist offiziellen Support an, welcher als Dienstleistung allerdings vergütet werden muss. Auch in der Lizenzpolitik gibt es Unterschiede. Einige Systeme haben ausschließlich freie Software in ihren Repositories (besonders konsequent bspw. Parabola), während andere auch unfreie aufnehmen. Als ein Kompromiss werden häufig Repositories mit proprietärer Software angeboten, die aber manuell zum Paketmanager hinzugefügt werden müssen (das machen bspw. Debian und Ubuntu) oder es wird eine Ausnahme für besonders wichtige Programme gemacht (bspw. auch Ubuntu). Kostenpflichtige Software wird fast nie aufgenommen. Zu unterscheiden sind weiter Community-Distributionen (bspw. Debian) von solchen, hinter denen Unternehmen stehen (bspw. Ubuntu). Auch die Updatezyklen spielen eine Rolle. Sie gehen von Rolling Releases (bspw. Arch, Gentoo und Debian Testing) bis hin zu vierjährigen Updatezyklen mit garantierter zehnjähriger Unterstützung einer Version (RHEL). Wichtig ist auch die Anzahl der Software in den Repositories. Entsprechend der Zielgruppe einer Distribution sind auch Größe und Fachkenntnis der Nutzerschaft verschieden.\n\nDie Unterschiede zwischen den Distributionen wirken sich oftmals auf deren Kompatibilität aus.\n\nSchon früh in der Geschichte der Distributionen entstanden Konzepte, die Installation weiterer Software zu vereinfachen. Meist sollte Software in Form kompilierter Pakete bereitgestellt und ein Mechanismus mitgeliefert werden, der funktionelle Abhängigkeiten zwischen installierten und nachgeladenen Paketen auflösen kann. Die entstandenen Paketmanagement-Systeme arbeiten mit je eigenen Paketformaten, zum Beispiel RPM oder dpkg. Viele Linux-Distributionen haben eine eigene Softwareverwaltung mit eigenen Binärpaketen, die zu anderen Distributionen teilweise inkompatibel sind.\n\nDie Kritik am Prinzip der Linux-Distributionen setzt unter anderem an diesem Punkt an. Da nicht jedes Software-Projekt und nicht jeder Software-Entwickler die Kenntnisse und Ressourcen hat, Software für jede einzelne Linux-Distribution bereitzustellen, wird oft nur der Quelltext veröffentlicht. Aus dem veröffentlichten Quelltext lauffähige Anwendungen zu erzeugen, ist jedoch potentiell ein komplizierter und fehlerträchtiger Prozess, der vielen Anwendern zu kompliziert sein kann. Diese bleiben dann oft auf die von der Distribution mitgelieferte Software angewiesen bzw. limitiert. Die Bereitstellung des Quellcodes als Softwareauslieferungsmethode ist jedoch für Anbieter kommerzieller Software, die Software binär ausliefern wollen, keine Option, weswegen diese die Menge von Distributionen und deren Paketformaten mit spezifischen Paketen bedienen müssen, was einen großen Mehraufwand bedeutet. Im Umfeld von Unternehmen hat deshalb nur eine begrenzte Auswahl an Distributionen eine Chance als allgemeine Arbeitsplattform.\n\nEin weitere wichtige Norm ist POSIX. Sie geht im Gegensatz zu LSB über Linux hinaus und soll einen Standard für alle unixoiden Betriebssysteme bilden. POSIX ist nicht kompatibel mit LSB. Linux-Distributionen halten sich für gewöhnlich an einen Großteil der Norm. Allerdings gibt es derzeit keine Distribution, die offiziell als POSIX-konform zertifiziert ist.\n\nDamit sich die Distributionen nicht weiter auseinanderentwickeln, wurde die Free Standards Group (heute Linux Foundation) mit dem Ziel gegründet, entsprechende Standards zwischen Distributionen zu fördern. Der Bekannteste ist die Linux Standard Base zur Förderung der binären Kompatibilität der Distributionen. Die LSB wird dabei von den verschiedenen Distributionen unterschiedlich strikt umgesetzt. Sie definiert übereinstimmende Binärschnittstellen („\"ABI\"“ genannt, für \"Application Binary Interface\"), einige Details zum inneren Aufbau und ein Paketsystem (hier RPM), das für die Installation von Software anderer Anbieter unterstützt werden muss.\n\nDie praktische Bedeutung dieser Regeln ist allerdings nur begrenzt. Die einseitige Festlegung auf des RPM-Paketformat wird teilweise angezweifelt, nachdem in den letzten Jahren durch Ubuntu oder Linux Mint das dpkg-Format eine große Verbreitung erlangt hat. Weil die meisten Distributionen, die dpkg nutzen, direkt auf Debian basieren, sind deren Pakete oft in anderen Distributionen, die ebenfalls auf Debian basieren, installierbar. Auf der anderen Seite setzen alle von Fedora (respektive Red Hat Linux), OpenSUSE und Mandriva abstammenden Distributionen auf RPM. Es ist mit einigen Einschränkungen durchaus möglich – z. B. mit Hilfe des \"OpenSuse Build Service\" – RPM-Pakete zu erstellen, die auf allen diesen Distributionen nutzbar sind.\n\nEine weitere Standardisierung stellt der Filesystem Hierarchy Standard dar, der eine gemeinsame Benennung einiger Datei- und Verzeichnisnamen und eine übereinstimmende Struktur der Basisverzeichnisse ermöglichen soll. Allerdings sind auch hier Details nicht geregelt, die bisher Inkompatibilitäten erzeugten. Andere Probleme ergeben sich erst durch die feste Integration von Anwendungen in den Systemverzeichnisbaum. Er wird von der \"Linux Standard Base\" vorausgesetzt.\n\nEs gibt einige Alternativansätze zu dem Modell der zentralen Softwareverbreitung über die Distributionen und deren Repositories. Projekte wie Autopackage, Zero Install oder der Klik-Nachfolger \"PortableLinuxApps\" versuchen eine einheitliche, aber dezentrale, distributionsunabhängige, binäre Softwareverbreitungsmöglichkeit zu schaffen, konnten aber bis jetzt faktisch keine relevante Verbreitung oder Unterstützung der Linux-Community erreichen.\n\nEin Schritt in diese Richtung war 2011 die Einführung eines \"Software Center\" in Ubuntu, nach dem Modell des App Stores von Apple, um die Anzahl der Applikationen signifikant erhöhen zu können, da das Distributionsmodell nur begrenzt skaliert.\n\n2012 betonte auch der Kernelentwickler Ingo Molnár die Notwendigkeit der Bereitstellung einer solchen dezentralen, skalierbaren und distributionsunabhängigen Softwareverbreitungsmethode; das Fehlen eines solchen Mechanismus sei eines der Kernprobleme des Linux-Desktops.\n\n\n\n"}
{"id": "54307", "url": "https://de.wikipedia.org/wiki?curid=54307", "title": "Linux (Kernel)", "text": "Linux (Kernel)\n\nLinux ( []) ist der Name eines Betriebssystem-Kernels, der im Jahr 1991 von Linus Torvalds ursprünglich für die x86-Architektur entwickelt und ab Version 0.12 unter der freien GNU General Public License (GPL) veröffentlicht wird. Er ist heute Teil einer Vielzahl von Betriebssystemen.\n\nDer Name \"Linux\" setzt sich zusammen aus dem Namen Linus und einem X für das als Vorbild dienende Unix. Er bezeichnet im weiteren Sinne mittlerweile nicht mehr nur den Kernel selbst, sondern übertragen davon ganze Linux-basierte Systeme und Distributionen. Dies führte zum GNU/Linux-Namensstreit.\n\nDer Kernel eines Betriebssystems bildet die hardwareabstrahierende Schicht, das heißt, er stellt der auf dieser Basis aufsetzenden Software eine einheitliche Schnittstelle (API) zur Verfügung, die unabhängig von der Rechnerarchitektur ist. Die Software kann so immer auf die Schnittstelle zugreifen und braucht die Hardware selbst, die sie nutzt, nicht genauer zu kennen. Linux ist dabei ein modularer monolithischer Kernel und zuständig für Speicherverwaltung, Prozessverwaltung, Multitasking, Lastverteilung, Sicherheitserzwingung und Eingabe/Ausgabe-Operationen auf verschiedenen Geräten.\n\nLinux ist fast ausschließlich in der Programmiersprache C geschrieben, wobei einige GNU-C-Erweiterungen benutzt werden. Eine Ausnahme bilden die architekturabhängigen Teile des Codes (im Verzeichnis \"arch\" innerhalb der Linux-Sourcen), wie zum Beispiel der Beginn des Systemstarts (Bootvorgang), der in Assemblersprache geschrieben ist.\n\nBei einem strikt monolithischen Kernel wird der gesamte Quellcode inklusive aller Treiber in das Kernel-Image (den ausführbaren Kernel) kompiliert. Im Gegensatz dazu kann Linux Module benutzen, die während des Betriebs geladen und wieder entfernt werden können. Damit wird die Flexibilität erreicht, um unterschiedlichste Hardware ansprechen zu können, ohne sämtliche (auch nicht benötigte) Treiber und andere Systemteile im Arbeitsspeicher halten zu müssen.\n\nSind Teile der Hardwarespezifikationen nicht genügend offengelegt, so stützt sich Linux notfalls über spezielle VM86-Modi auch auf das BIOS des Systems, u. a. auf die Erweiterungen gemäß den Standards APM, ACPI und VESA. Um unter diesen Voraussetzungen x86-kompatible Hardware z. B. auf der DEC-Alpha-Plattform zu betreiben, werden teilweise sogar Emulatoren zur Ausführung entsprechenden ROM-Codes verwendet. Linux selbst übernimmt das System beim Bootprozess typischerweise in dem Moment, in dem der BIOS-Bootloader erfolgreich war und alle Systeminitialisierungen des BIOS abgeschlossen sind.\n\nDer Kernel ist ein Betriebssystemkern und darf nicht als das eigentliche Betriebssystem verstanden werden. Dieses setzt sich aus dem Kernel und weiteren grundlegenden Bibliotheken und Programmen (die den Computer erst bedienbar machen) zusammen.\n\n\"Siehe auch:\" Gerätedatei, Network Block Device, Netfilter, Netzwerk-Scheduler, Prozess-Scheduler, Linux (Betriebssystem)\n\nMan kann zwischen vier Schnittstellen unterscheiden, die das Zusammenwirken von entweder kernelinternen Komponenten untereinander oder von Kernel und externer Software ermöglichen. Die Stabilität der externen Programmierschnittstelle wird garantiert, das heißt, dass Quellcode grundsätzlich ohne jegliche Veränderungen portierbar ist.\nDie Stabilität der internen Programmierschnittstellen wird nicht garantiert, diese können zehn Jahre oder wenige Monate stabil bleiben. Da der Linux-Kernel von einigen tausend Entwicklern vorangetrieben wird, ist der eventuell entstehende Aufwand zu verschmerzen.\n\nDie Binärschnittstelle des Kernels ist unerheblich, auf das komplette Betriebssystem kommt es an. Die Linux Standard Base (LSB) soll es ermöglichen, kommerzielle Programme unverändert zwischen Linux Betriebssystemen zu portieren.\nDie interne Binärschnittstelle ist nicht stabil, und es gibt keinerlei Bestrebungen dies zu ändern; dies hat zur Folge, dass ein internes Modul, welches z. B. für Linux 3.0 kompiliert worden ist, höchstwahrscheinlich nicht mit Linux-Kernel 3.1 zusammenarbeiten wird. Dies ist eine ganz bewusste Entscheidung.\n\nLinux ist ein monolithischer Kernel. Die Treiber im Kernel und die Kernel-Module laufen im privilegierten Modus (x86: Ring 0), haben also unbeschränkten Zugriff auf die Hardware. Einige wenige Module des Kernels laufen im eingeschränkten Benutzermodus (x86: Ring 3). Die \"Level\" 1 und 2 der x86-Architektur werden von Linux nicht genutzt, da sie auf vielen anderen Architekturen nicht existieren und der Kernel auf allen unterstützten Architekturen im Wesentlichen gleich funktionieren soll.\n\nNahezu jeder Treiber kann auch als Modul zur Verfügung stehen und vom System dann dynamisch nachgeladen werden. Ausgenommen davon sind Treiber, die für das Starten des Systems verantwortlich sind, bevor auf das Dateisystem zugegriffen werden kann. Man kann allerdings den Kernel so konfigurieren, dass ein CramFS- oder Initramfs-Dateisystem vor dem tatsächlichen Root-Dateisystem geladen wird, welches die weiteren für den Startprozess notwendigen Module enthält. Dadurch kann die Kernelgröße verringert und die Flexibilität drastisch erhöht werden.\n\nIm System laufende Programme bekommen wiederum vom Kernel Prozessorzeit zugewiesen. Jeder dieser Prozesse erhält einen eigenen, geschützten Speicherbereich und kann nur über Systemaufrufe auf die Gerätetreiber und das Betriebssystem zugreifen. Die Prozesse laufen dabei im Benutzermodus \"()\", während der Kernel im Kernel-Modus \"()\" arbeitet. Die Privilegien im Benutzermodus sind sehr eingeschränkt. Abstraktion und Speicherschutz sind nahezu vollkommen, ein direkter Zugriff wird nur sehr selten und unter genau kontrollierten Bedingungen gestattet. Dies hat den Vorteil, dass kein Programm z. B. durch einen Fehler das System zum Absturz bringen kann.\n\nLinux stellt wie sein Vorbild Unix eine vollständige Abstraktion und Virtualisierung für nahezu alle Betriebsmittel bereit (z. B. virtueller Speicher, Illusion eines eigenen Prozessors usw.).\n\nDie Tatsache, dass Linux nicht auf einem Microkernel basiert, war Thema eines berühmten Flame Wars zwischen Linus Torvalds und Andrew S. Tanenbaum. Anfang der 1990er Jahre, als Linux entwickelt wurde, galten monolithische Kernels als obsolet (Linux war zu diesem Zeitpunkt noch rein monolithisch). Die Diskussion und Zusammenfassungen sind im Artikel Geschichte von Linux näher beschrieben.\n\nDurch Erweiterungen wie FUSE und durch die zunehmende Verwendung von Kernel-Prozessen fließen mittlerweile auch zahlreiche Microkernel-Konzepte in Linux ein.\n\nObwohl Linus Torvalds eigentlich nicht beabsichtigt hatte, einen portierbaren Kernel zu schreiben, hat sich Linux dank des GNU Compilers GCC weitreichend in diese Richtung entwickelt. Es ist inzwischen eines der am häufigsten portierten Systeme (nur noch NetBSD läuft auf etwa gleich vielen Architekturen). Das Repertoire reicht dabei von eher selten anzutreffenden Betriebsumgebungen wie dem iPAQ-Handheld-Computer, Digitalkameras oder Großrechnern wie IBMs System z bis hin zu normalen PCs.\n\nObwohl die Portierung auf die S/390 ursprünglich ein vom IBM-Management nicht genehmigtes Unterfangen war \"(siehe auch: Skunk works)\", plant IBM auch die nächste IBM-Supercomputergeneration Blue Gene mit einem eigenen Linux-Port auszustatten.\n\nUrsprünglich hatte Torvalds eine ganz andere Art von Portierbarkeit für sein System angestrebt, nämlich die Möglichkeit, freie GPL- und andere quelloffene Software leicht unter Linux kompilieren zu können. Dieses Ziel wurde bereits sehr früh erreicht und macht sicherlich einen guten Teil des Erfolges von Linux aus, da es jedem eine einfache Möglichkeit bietet, auf einem freien System freie Software laufen zu lassen.\n\nDie ersten Architekturen, auf denen Linux lief, waren die von Linus Torvalds verwendeten Computer:\n\nDamit war Linux sehr früh 64-Bit-fähig (Linux 1.2 erschien 1995) und durch die Portierung auf Alpha war der Weg für weitere Portierungen frei. Zeitgleich arbeitete der Student Dave Miller ab 1993 an der Portierung auf SPARC von Sun Microsystems, einer damals weit verbreiteten Architektur. Doch lief Linux 2.0 von Mitte 1996 offiziell auf IA-32 und Alpha, konnte aber bereits SMP.\n\nMit Linux 2.2 vom Januar 1999 kamen folgende Ports hinzu:\n\nMit Linux 2.4 vom Januar 2001 schließlich kamen folgende Architekturen hinzu:\n\nTrotz der unterstützten Befehlssatzarchitekturen (, kurz ISA) ist für die Lauffähigkeit mehr nötig, sodass Linux gegenwärtig auf u. a. folgenden Plattformen und Architekturen läuft:\n\nLinux unterstützt zwei verschiedene Binärschnittstellen für ARM-Prozessoren. Die ältere Binärschnittstelle wird mit dem Akronym \"OABI (old application binary interface)\" bezeichnet und unterstützt die Prozessorarchitekturen bis einschließlich ARMv4, während die neuere Binärschnittstelle, die mit \"EABI (embedded application binary interface)\" bezeichnet wird, die Prozessorarchitekturen ab einschließlich ARMv4 unterstützt. Der bedeutendste Unterschied der Binärschnittstellen in Bezug auf Systemleistung ist die sehr viel bessere Unterstützung von Software-emulierten Gleitkommarechnungen durch EABI.\n\nEin besonderer Port ist das \"User Mode Linux\". Prinzipiell handelt es sich dabei um einen Port von Linux auf sein eigenes Systemcall-Interface. Dies ermöglicht es, einen Linux-Kernel als normalen Prozess auf einem laufenden Linux-System zu starten. Der User-Mode-Kernel greift dann nicht selbst auf die Hardware zu, sondern reicht entsprechende Anforderungen an den echten Kernel durch. Durch diese Konstellation werden „Sandkästen“ ähnlich den virtuellen Maschinen von Java oder den jails von FreeBSD möglich, in denen ein normaler Benutzer Root-Rechte haben kann, ohne dem tatsächlichen System schaden zu können.\n\nµClinux ist eine Linux-Variante für Computer ohne Memory Management Unit (MMU) und kommt vorwiegend auf Mikrocontrollern und eingebetteten Systemen zum Einsatz. Seit Linux-Version 2.6 ist µClinux Teil des Linux-Projektes.\n\nDie Entwicklung von Linux liegt durch die GNU General Public License und durch ein sehr offenes Entwicklungsmodell nicht in der Hand von Einzelpersonen, Konzernen oder Ländern, sondern in der Hand einer weltweiten Gemeinschaft vieler Programmierer, die sich hauptsächlich über das Internet austauschen.\nBei der Entwicklung kommunizieren die Entwickler fast ausschließlich über E-Mail, da Linus Torvalds behauptet, dass so die Meinungen nicht direkt aufeinander prallen.\nIn vielen Mailinglisten, aber auch in Foren und im Usenet besteht für jedermann die Möglichkeit, die Diskussionen über den Kernel zu verfolgen, sich daran zu beteiligen und auch aktive Beiträge zur Entwicklung zu leisten. Durch diese unkomplizierte Vorgehensweise ist eine schnelle und stetige Entwicklung gewährleistet, die auch die Möglichkeit mit sich bringt, dass jeder dem Kernel Fähigkeiten zukommen lassen kann, die er benötigt.\n\nEingegrenzt wird dies nur durch die Kontrolle von Linus Torvalds und einigen besonders verdienten Programmierern, die das letzte Wort über die Aufnahme von Verbesserungen und Patches in die offizielle Version haben. Manche Linux-Distributoren bauen auch eigene Funktionen in den Kernel ein, die im offiziellen Kernel (noch) nicht vorhanden sind.\n\nDer Entwicklungsprozess des Kernels ist wie der Kernel selbst ebenfalls immer weiterentwickelt worden. So führte der Rechtsprozess der SCO Group um angeblich illegal übertragenen Code in Linux zur Einführung eines „Linux Developer's Certificate of Origin“, das von Linus Torvalds und Andrew Morton bekanntgegeben wurde. Diese Änderung griff das Problem auf, dass nach dem bis dahin gültigen Modell des Linux-Entwicklungsprozesses die Herkunft einer Erweiterung oder Verbesserung des Kernels nicht nachvollzogen werden konnte.\n\nDie Versionskontrolle des Kernels unterliegt dem Programm Git. Dies wurde speziell für den Kernel entwickelt und auf dessen Bedürfnisse hin optimiert. Es wurde im April 2005 eingeführt, nachdem sich abgezeichnet hatte, dass das alte Versionskontrollsystem BitKeeper nicht mehr lange für die Kernelentwicklung würde genutzt werden können.\n\nAuf der Website \"kernel.org\" werden alle alten und neuen Kernel-Versionen archiviert. Die dort befindlichen Referenzkernel werden auch als \"Vanilla-Kernel\" bezeichnet (von umgangssprachlich engl. \"vanilla\" für \"Standard\" bzw. \"ohne Extras\" im Vergleich zu Distributionskernels). Auf diesem bauen die Distributionskernel auf, die von den einzelnen Linux-Distributionen um weitere Funktionen ergänzt werden. Die Kernel-Version des geladenen Betriebssystems kann mit dem Befehl uname -r abgefragt werden.\n\nDie frühen Kernelversionen (0.01 bis 0.99) hatten noch kein klares Nummerierungsschema. Version 1.0 sollte die erste „stabile“ Linux-Version werden. Beginnend mit Version 1.0 folgen die Versionsnummern von Linux einem bestimmten Schema:\n\nDie \"erste Ziffer\" wird nur bei grundlegenden Änderungen in der Systemarchitektur angehoben. Während der Entwicklung des 2.5er Kernels kam wegen der relativ grundlegenden Änderungen, verglichen mit dem 2.4er Kernel, die Diskussion unter den Kernel-Programmierern auf, den nächsten Produktionskernel als 3.0 zu deklarieren. Torvalds war aber aus verschiedenen Gründen dagegen, sodass der resultierende Kernel als 2.6 bezeichnet wurde.\n\nDie \"zweite Ziffer\" gibt das jeweilige „Majorrelease“ an. Bisher wurden stabile Versionen (Produktivkernel) von den Entwicklern stets durch gerade Ziffern wie 2.2, 2.4 und 2.6 gekennzeichnet, während die Testversionen (Entwicklerkernel) immer ungerade Ziffern trugen, wie zum Beispiel 2.3 und 2.5; diese Trennung ist aber seit Juli 2004 ausgesetzt, es gab keinen Entwicklerkernel mit der Nummer 2.7, stattdessen wurden die Änderungen laufend in die 2.6er-Serie eingearbeitet.\n\nZusätzlich bezeichnet eine \"dritte Zahl\" das „Minorrelease“, das die eigentliche Version kennzeichnet. Werden neue Funktionen hinzugefügt, steigt die dritte Zahl an. Der Kernel wird damit zum Beispiel mit einer Versionsnummer wie 2.6.7 bestimmt.\n\nUm die Korrektur eines schwerwiegenden NFS-Fehlers schneller verbreiten zu können, wurde mit der Version 2.6.8.1 erstmals eine \"vierte Ziffer\" eingeführt. Seit März 2005 (Kernel 2.6.11) wird diese Nummerierung offiziell verwendet. So ist es möglich, die Stabilität des Kernels trotz teilweise sehr kurzer Veröffentlichungszyklen zu gewährleisten und Korrekturen von kritischen Fehlern innerhalb weniger Stunden in den offiziellen Kernel zu übernehmen – wobei sich die vierte Ziffer erhöht (z. B. von 2.6.11.1 auf 2.6.11.2). Die Minorreleasenummer, also die dritte Ziffer, wird hingegen nur bei Einführung neuer Funktionen hochgezählt.\n\nIm Mai 2011 erklärte Linus Torvalds, die nach der Version 2.6.39 kommende Version nicht 2.6.40, sondern 3.0 zu benennen. Als Grund dafür führte er an, dass die Versionsnummern seiner Meinung nach zu hoch wurden. Die Versionsnummer '3' stehe gleichzeitig für das dritte Jahrzehnt, welches für den Linux-Kernel mit seinem 20. Geburtstag anfange. Bei neuen Versionen wird seitdem die zweite Ziffer erhöht und die dritte steht – anstelle der vierten – für Bugfixreleases.\n\nIm Februar 2015 erhöhte Torvalds auf Version 4.0 statt Version 3.20 nachdem er auf Google+ Meinungen hierzu eingeholt hatte. Seit März 2019 ist Linux 5.0 freigegeben. Dabei hat der Sprung von der letzten Versionsnummer 4.20 auf 5.0 keine tiefergehende Bedeutung. Die aktuelle Version soll eine modernere Speicherfunktion und mehr Geschwindigkeit liefern.\n\nNeue Funktionen finden sich im \"-mm\" Kernel des Kernelentwicklers Andrew Morton und werden anschließend in den Hauptzweig von Torvalds übernommen. Somit werden große Unterschiede zwischen Entwicklungs- und Produktionskernel und damit verbundene Portierungsprobleme zwischen den beiden Serien vermieden. Durch dieses Verfahren gibt es auch weniger Differenzen zwischen dem offiziellen Kernel und den Distributionskernel (früher wurden Features des Entwicklungszweiges von den Distributoren häufig in ihre eigenen Kernels rückintegriert). Allerdings litt 2004/2005 die Stabilität des 2.6er Kernels unter den häufig zu schnell übernommenen Änderungen. Ende Juli 2005 wurde deshalb ein neues Entwicklungsmodell beschlossen, das nach dem Erscheinen der Version 2.6.13 erstmals zur Anwendung kam: Neuerungen werden nur noch in den ersten zwei Wochen der Kernelentwicklung angenommen, wobei anschließend eine Qualitätssicherung bis zum endgültigen Erscheinen der neuen Version erfolgt.\n\nWährend Torvalds die neuesten Entwicklungsversionen veröffentlicht, wurde die Pflege der älteren „stabilen“ Versionen an andere Programmierer abgegeben. Gegenwärtig ist David Weinehall für die 2.0er Serie verantwortlich, Marc-Christian Petersen (zuvor Alan Cox) für den Kernel 2.2, Willy Tarreau (zuvor Marcelo Tosatti) für den Kernel 2.4, Greg Kroah-Hartman und Chris Wright für die aktuellen stabilen Kernel 3.x.y(-stable), Linus Torvalds für die aktuellen „normalen“ Kernel 4.x.y, und Andrew Morton für seinen experimentellen -mm-Zweig, basierend auf dem neuesten 4.x. Zusätzlich zu diesen offiziellen und über Kernel.org oder einen seiner Mirrors zu beziehenden Kernel-Quellcodes kann man auch alternative „Kernel-Trees“ aus anderen Quellen benutzen. Distributoren von Linux-basierten Betriebssystemen pflegen meistens ihre eigenen Versionen des Kernels und beschäftigen zu diesem Zwecke fest angestellte Kernel-Hacker, die ihre Änderungen meist auch in die offiziellen Kernels einfließen lassen.\n\nDistributions-Kernel sind häufig intensiv gepatcht, um auch Treiber zu enthalten, die noch nicht im offiziellen Kernel enthalten sind, von denen der Distributor aber glaubt, dass seine Kundschaft sie benötigen könnte und die notwendige Stabilität respektive Fehlerfreiheit dennoch gewährleistet ist.\n\nFolgende Versionen werden besonders lange mit Support (\"Long Term Support\") versorgt:\n\nDas folgende Schaubild stellt einzelne Versionen des Linux-Kernels anhand der Erscheinungsdaten auf einer Zeittafel angeordnet dar und soll dem Überblick dienen.\n\nBei Betrachtung der zuletzt erschienenen Versionen (siehe Tabelle) erfolgt die Entwicklung einer neuen Kernel-Version in durchschnittlich 82 Tagen. Der Kernel wird hierbei im Durchschnitt um 768 Dateien und 325.892 Quelltextzeilen (englisch \"Lines of Code\") erweitert. Das mit dem Datenkompressionsprogramm \"gzip\" komprimierte \"tar\"-Archiv (.tar.gz) wächst im Mittel um rund 2 Megabyte mit jeder veröffentlichten Hauptversion.\n\nAnmerkungen\n\nDie Kernel-Reihe 2.6 wurde ab Dezember 2001 auf Basis der damaligen 2.4er-Reihe entwickelt und wies umfangreiche Neuerungen auf. Für die Entwicklung war der neue Quelltext übersichtlicher und leichter zu pflegen, während Anwender durch die Überarbeitung des \"Prozess-Schedulers\" sowie des I/O-Bereiches und von geringeren Latenzzeiten profitierten. Dies wurde durch eine Reihe von Maßnahmen erreicht, die im Folgenden aufgezeigt werden:\n\nIn einem Multitasking-fähigen Betriebssystem muss es eine Instanz geben, die den Prozessen, die laufen wollen, Rechenzeit zuteilt. Diese Instanz bildet der Prozess-Scheduler. Seit dem Erscheinen von Linux 2.6 wurde mehrfach grundlegend am Scheduler gearbeitet.\n\nFür die ersten Kernel 2.6 war von Ingo Molnár ein gegenüber Linux 2.4 ganz neuer Scheduler konzipiert und implementiert worden, der \"O(1)-Scheduler\". Dieser erhielt seinen Namen, weil die relevanten Algorithmen, auf denen der Scheduler basierte, die Zeitkomplexität formula_1 haben. Dies bedeutet, dass die vom Scheduler für eigene Aufgaben benötigte Prozessorzeit unabhängig von der Anzahl der verwalteten Prozesse bzw. Threads ist. Insbesondere wurde etwa auf das Durchsuchen aller Prozesse nach dem momentan wichtigsten Prozess verzichtet.\n\nDer O(1)-Scheduler arbeitete auch bei sehr vielen Prozessen überaus effizient und benötigte selbst sehr wenig Rechenzeit. Er verwendete prinzipiell zwei verkettete Listen, in denen die Prozesse eingetragen waren, die noch laufen wollten, und diejenigen, die bereits gelaufen sind. Wenn alle Prozesse in der zweiten Liste standen, wurden die Datenfelder getauscht, und das Spiel begann von neuem. Der Scheduler war darüber hinaus so ausgelegt, dass Prozesse, die große Mengen Rechenzeit in Anspruch nehmen wollen, gegenüber interaktiven Prozessen benachteiligt werden, wenn beide zur gleichen Zeit laufen wollen.\n\nInteraktive Prozesse benötigen in der Regel nur sehr wenig Rechenzeit, sind dafür aber sehr zeitkritisch (so will der Benutzer beispielsweise nicht lange auf eine Reaktion der grafischen Oberfläche warten). Der O(1)-Scheduler besaß Heuristiken, um festzustellen, ob ein Prozess interaktiv ist oder die CPU eher lange belegt.\n\nDer interne „Takt“ des Kernels wurde ab dem Kernel 2.6 von 100 auf 1000 Hertz erhöht, das heißt, die kürzestmögliche Länge einer Zeitscheibe beträgt nun eine Millisekunde. Auch hiervon profitieren besonders die interaktiven Prozesse, da sie früher „wieder an der Reihe sind“. Da dies aber zu einer erhöhten CPU-Last und somit zu einem größeren Stromverbrauch führt, entschied man, den Takt ab dem Kernel 2.6.13 auf 250 Hertz voreinzustellen. Bei der Konfiguration des Kernels sind jedoch auch noch die Werte 100, 300 und 1000 Hertz wählbar.\n\nMit der Kernelversion 2.6.23 wurde im Oktober 2007 der O(1)-Scheduler durch einen \"Completely Fair Scheduler\" (kurz \"CFS\") ersetzt, der ebenfalls von Ingo Molnár entwickelt wurde. Der CFS als gegenwärtig einziger im Hauptentwicklungszweig verfügbarer Scheduler ist unter den Kernel-Entwicklern teilweise umstritten, da er seinen Schwerpunkt auf Skalierbarkeit auch bei Servern mit vielen Prozessorkernen legt. Entwickler wie Con Kolivas sind der Meinung, dass unter dieser Schwerpunktsetzung sowie einigen Designentscheidungen im CFS die Leistung auf typischen Desktop-Systemen leide.\n\nDer Kernel ist ab Version 2.6 in den meisten Funktionen präemptiv, d. h., selbst wenn das System gerade im Kernel-Modus Aufgaben ausführt, kann dieser Vorgang durch einen Prozess aus dem User-Modus unterbrochen werden. Der Kernel macht dann weiter, wenn der Usermodus-Prozess seine Zeitscheibe aufgebraucht hat oder selbst eine neue Zeitplanung (englisch ') anfordert, also dem Zeitplaner (englisch ') mitteilt, dass er einen anderen Task ausführen kann. Dies funktioniert, bis auf einige Kernel-Funktionen, die atomar (nicht unterbrechbar) ablaufen müssen, sehr gut und kommt ebenfalls der Interaktivität zugute.\n\nMit dem Kernel 2.6 werden für Linux erstmals Zugriffskontrolllisten (englisch \"\") nativ eingeführt. Diese sehr feinkörnige Rechteverwaltung ermöglicht es vor allem Systemadministratoren, die Rechte auf einem Dateisystem unabhängig vom Gruppen- und Nutzermodell zu gestalten und dabei faktisch beliebig viele spezielle Rechte pro Datei zu setzen. Die mangelnde Unterstützung von Zugriffskontrolllisten von Linux wurde vorher als massive Schwäche des Systems im Rahmen der Rechteverwaltung und der Möglichkeiten zur sicheren Konfiguration gesehen.\n\nDie Unterstützung von Zugriffskontrolllisten funktioniert dabei mit den Dateisystemen ext2, ext3, jfs und XFS nativ.\n\nMit dem Kernel 2.6.13 hielt erstmals eine \"Inotify\" genannte Funktion Einzug in den Kernel. Diese ermöglicht eine andauernde Überwachung von Dateien und Verzeichnissen – wird eines der überwachten Objekte geändert oder ein neues Objekt im Überwachungsraum erschaffen, gibt \"Inotify\" eine Meldung aus, die wiederum andere Programme zu definierten Tätigkeiten veranlassen kann. Dies ist insbesondere für Such- und Indexierungsfunktionen der Datenbestände von entscheidender Bedeutung, und ermöglicht erst den sinnvollen Einsatz von Desktop-Suchmaschinen wie Strigi oder Meta Tracker. Ohne eine solche Benachrichtigungsfunktion des Kernels müsste ein Prozess die zu überwachende Datei oder das zu überwachende Verzeichnis in bestimmten Zeitintervallen auf Änderungen überprüfen, was im Gegensatz zu Inotify zusätzliche Performance-Einbußen mit sich bringen würde.\n\nSoweit es möglich ist, wurde in Linux 2.6 die Maximalzahl für bestimmte Ressourcen angehoben. Die Anzahl von möglichen Benutzern und Gruppen erhöhte sich von 65.000 auf über 4 Milliarden, ebenso wie die Anzahl der Prozess-IDs (von 32.000 auf 1 Milliarde) und die Anzahl der Geräte (Major/Minor-Nummern). Weitere leistungssteigernde Maßnahmen betrafen die I/O-Scheduler, das Threading mit der neuen Native POSIX Thread Library und den Netzwerk-Stack, der nun ebenfalls in den meisten Tests O(1) skaliert ist. Außerdem wurde für die Verwaltung der I/O-Gerätedateien das früher genutzte devfs durch das neuere udev ersetzt, was viele Unzulänglichkeiten, wie zum Beispiel ein zu großes \"/dev/\"-Verzeichnis, beseitigt. Außerdem kann so eine einheitliche und konsistente Gerätebenennung erfolgen, die beständig bleibt, was vorher nicht der Fall war.\n\nDie heute von Linus Torvalds herausgegebene Fassung des Kernels enthält proprietäre Objekte in Maschinensprache (BLOBs) und ist daher nicht mehr ausschließlich Freie Software. Richard Stallman bezweifelt sogar, dass sie legal kopiert werden darf, da diese BLOBs im Widerspruch zur GPL stünden und die Rechte aus der GPL daher erlöschen würden. Resultierend daraus rät die Free Software Foundation deshalb dazu, nur BLOB-freie Versionen von Linux einzusetzen, bei denen diese Bestandteile entfernt wurden. Linux-Distributionen mit dem Kernel Linux-libre erfüllen diesen Anspruch.\n\nDie bei GPL-Software übliche Klausel, dass statt der Version 2 der GPL auch eine neuere Version verwendet werden kann, fehlt beim Linux-Kernel. Die Entscheidung, ob die im Juni 2007 erschienene Version 3 der Lizenz für Linux verwendet wird, ist damit prinzipiell nur mit Zustimmung aller Entwickler möglich. In einer Umfrage haben sich Torvalds und die meisten anderen Entwickler für die Beibehaltung der Version 2 der Lizenz ausgesprochen.\n\n\nEnglisch:\n\n\nDeutsch:\n\n"}
{"id": "54622", "url": "https://de.wikipedia.org/wiki?curid=54622", "title": "Outlook Express", "text": "Outlook Express\n\nOutlook Express (Abk. OE, in der Microsoft Support Knowledge Base auch OLEXP) ist ein E-Mail-Programm und Newsreader von Microsoft, der dem Betriebssystem Windows bis zum Erscheinen von Windows Vista beilag und deshalb auch von vielen Anwendern eingesetzt wurde. Trotz der Namensähnlichkeit ist das mit dem Internet Explorer zusammenhängende Programm ein von der Groupware Outlook unabhängiges Programm. Abgelöst wurde es mit dem Erscheinen von Windows Vista durch Windows Mail. In Windows 7 ist Windows Mail nicht mehr enthalten. Stattdessen bietet Microsoft den Nachfolger Windows Live Mail als Bestandteil von Windows Live Essentials zum Download an.\n\nDer Funktionsumfang von Outlook Express ist für den typischen privaten Einsatz gedacht. Es hat keine Groupware- und Unified-Messaging-Funktionen, verfügt dafür im Gegensatz zu Outlook über News-Funktionen. Es unterstützt die folgenden Protokolle: SMTP, POP3, IMAP4 und NNTP, sowie HTTPmail (eine auf WebDAV/HTTP basierende Variante zum Zugriff auf die Freemail-Konten von Lycos und Hotmail).\n\nOutlook Express erschien im Oktober 1997 in der Version 4 zusammen mit dem Internet Explorer 4 für Windows – zu einer Zeit also, in welcher der Browserkrieg heftig tobte. Outlook Express ging aus dem Programm \"Microsoft Internet Mail and News\" hervor (daher hat die Programmdatei von Outlook Express selbst in Version 6.0 noch den Namen \"msimn.exe\"). Enthalten waren sowohl Outlook Express als auch Internet Explorer in Windows 98.\n\nIm März 1999 wurde zusammen mit dem Internet Explorer 5 auch eine Version 5 von Outlook Express veröffentlicht, die neben einem geänderten Dateiformat (codice_1 statt codice_2, codice_3 und codice_4) ein paar Funktionserweiterungen, aber auch viele in den Folgejahren nicht oder sehr spät behobene Programmfehler mitbrachte. Diese Version wurde auch zusammen mit Windows 98 Second Edition ausgeliefert.\n\nJuli 2000 erschien die IE/OE-5.5-Linie (enthalten in Windows Millennium Edition). Diese Linie ist bis heute die letzte, die auf Windows 95 installiert werden kann.\n\nOutlook Express 6 erschien zusammen mit dem Internet Explorer 6 gleichzeitig mit Windows XP im August/September 2001, ohne dass sich Wesentliches an Funktionsumfang und Problemen geändert hätte.\n\nDie OE-5.5- und OE-6-Linien wurden zwischenzeitlich mit den jeweiligen Service Packs für den Internet Explorer und/oder Windows 2000 und Windows XP aktualisiert. Zusammen mit dem Service Pack 2 für Windows XP wurde insbesondere noch eine etwas verbesserte Version von Outlook Express 6 veröffentlicht, die hauptsächlich einige neue Sicherheitsfunktionen mitbrachte (wie z. B. die Möglichkeit, das Herunterladen von Bildern in HTML-E-Mails zu blockieren).\n\nFür Mac OS 9 gab es auch eine kostenlose Version von Outlook Express für den Macintosh, die allerdings mit der Windows-Version bis auf den Namen ansonsten recht wenig gemein hat, da sich diese Variante etwa wesentlich besser an E-Mail- und Usenet-Standards hält.\n\nOutlook Express ist immer wieder Angriffspunkt für Computerwürmer, da sich dem Hacker einerseits durch den Verbreitungsgrad eine relativ große Angriffsfläche bietet und Outlook Express insbesondere in früheren Versionen viele unsichere Standardeinstellungen aufweist. Diese ermöglichen das Ausnutzen von Sicherheitsmängeln, welche weitgehend auf die Verwendung des Internet Explorer zur Darstellung von Nachrichten zurückgehen.\n\nSeit IE/OE 5.5 SP2 (2001) wurden die Standardeinstellungen restriktiver gesetzt. So wird zur Darstellung von Nachrichten seither die „Zone für eingeschränkte Sites“ verwendet, welche standardmäßig das Ausführen von ActiveScripting und ActiveX unterbinden soll. Mit IE/OE 6 SP1 (2002) hielt eine weitere, empfehlenswerte Option Einzug, welche HTML-Nachrichten als Nur-Text-Nachrichten darstellt und somit jeden aktiven Inhalt bei der Anzeige einer Nachricht unterbindet („Extras – Optionen – Lesen – Alle Nachrichten als Nur-Text lesen“). Gleichwohl wird hierbei zur Darstellung der angezeigten Nachricht weiterhin auf den Internet Explorer zurückgegriffen. Bei der Darstellung von Links, Umlauten und Sonderzeichen kann es je nach verwendeter Kodierung (z. B. Abweichung von „UTF-8“) Probleme geben, ohne Angabe eines MIME-alternativen Textes in der E-Mail wird eine „leere“ Nachricht ohne Text angezeigt. Mit dem Erscheinen des Service Packs 2 für Windows XP (August 2004) verwendet Outlook Express bei aktivierter „Alle Nachrichten als Nur-Text lesen“-Option nicht mehr den Internet Explorer, sondern setzt auf das RichTextControl zur Darstellung dieser Nachrichten. An den Anzeigeproblemen in diesem Modus änderte sich allerdings nichts. Gleichzeitig wurden weitere Sicherheitsfunktionen eingeführt, die z. B. das Nachladen von Bildern in HTML-Nachrichten in der HTML-Ansicht unterbinden.\n\nOutlook Express zeigt eine leere E-Mail beim Empfang einer signierten PGP/MIME-Nachrichten an, der Klartext ist nur als Anhang vorhanden. Dies widerspricht dem E-Mail-Standard RFC 1847 aus dem Jahr 1995. Beim Antworten auf eine solche signierte Nachricht kommt bei Outlook-Express-Nutzern ohne digitale ID eine Fehlermeldung.\n\nViele OE-Anwender sind Neulinge im Usenet und kennen oft allgemeine Vereinbarungen und Standards nicht. Outlook Express unterstützt die Anwender dabei nicht. So muss der Anwender in OE zunächst via „Extras | Optionen | Senden | News | Nur-Text-Einstellungen“ die Option „MIME, Textkodierung: keine“ auswählen, damit 8-Bit-Zeichen wie z. B. Umlaute beim Leser als solche zu erkennen sind. Ferner neigt OE u. a. dazu, Kammquoting beim Antworten auf Postings zu produzieren, auch wenn mit dem SP2 für Windows XP „format=flowed“ Einzug hielt. Halbwegs komfortable Abhilfe dagegen schaffen Drittprogramme wie z. B. „OE-QuoteFix“, „Morver“ oder die „OE-PowerTools“.\n\nAus diesen Fehlern resultiert die im Usenet scherzhafte Bezeichnung „OjE“ für Outlook Express (der Ausruf „o je“ wird normalerweise bei einem leichteren Missgeschick verwendet). Positiv hervorzuheben sind bei Outlook Express die Unicode-Fähigkeiten und der Umgang mit Zeichensätzen (so denn MIME manuell aktiviert wurde).\n\n"}
{"id": "54738", "url": "https://de.wikipedia.org/wiki?curid=54738", "title": "Component Object Model", "text": "Component Object Model\n\nDas Component Object Model [] (abgekürzt COM) ist eine von Microsoft entwickelte Technik zur Interprozesskommunikation unter Windows. COM-Komponenten können sowohl in Form von Laufzeitmodulen (DLLs) als auch als ausführbare Programme umgesetzt sein. COM soll eine leichte Wiederverwendung von bereits geschriebenem Programmcode ermöglichen, zum Teil auch über Betriebssystemgrenzen hinweg. COM-Komponenten können unabhängig von der Programmiersprache eingesetzt werden.\n\nDas Component Object Model wurde von Microsoft 1992 mit der grafischen Benutzeroberfläche Windows 3.1 eingeführt.\n\nCOM basiert auf dem Client-Server-Modell. Ein COM-Client erzeugt eine COM-Komponente in einem so genannten COM-Server und nutzt die Funktionalität des Objektes über COM-Schnittstellen. Der Zugriff auf Objekte wird innerhalb eines Prozesses durch so genannte COM-Apartments synchronisiert.\n\nUnter einem COM-Server versteht man ein Laufzeitmodul (Dynamic Link Library) oder ein ausführbares Programm, das in einer COM-unterstützenden Programmiersprache erstellt wurde und COM-Komponenten anbietet und erstellen kann. Es gibt drei Typen von COM-Servern:\n\nIm Falle des \"In-process-Servers\" ist die COM-Komponente in einer DLL implementiert (sie tragen unter Windows oft die Dateiendung \"OCX\"). Diese DLLs müssen die Funktionen codice_1, codice_2, codice_3 und codice_4 exportieren.\nWird eine COM-Komponente eines In-process-Servers erzeugt, so wird der zugehörige Server (ein Server kann mehrere COM-Komponenten anbieten) in den Prozess des Clients geladen. In-process-Server sind besonders schnell, da der Zugriff auf die Funktionen der COM-Komponenten ohne Umwege erfolgt. Nachteilig ist, dass auf diese Weise jeder Prozess eigenen Speicherplatz mit den benutzten COM-Komponenten belegt und keine gemeinsame Speichernutzung möglich ist.\n\n\"Local Server\" sind unter Windows ausführbare Programme, die COM-Komponenten implementieren. Bei der Erzeugung einer COM-Komponente wird dieses Programm gestartet (sofern es nicht schon läuft) – dies bedeutet, dass ein ausführbares Programm vorliegen muss, eine DLL kann hier nicht aufgerufen werden. Zur Kommunikation zwischen Client und Server wird ein vereinfachtes RPC-Protokoll (Remote Procedure Call) benutzt. Local Server haben den Vorteil, dass sie nur einmal gestartet werden müssen und dann viele Clients bedienen können, was weniger Speicherplatz belegt. Zudem lassen sich so recht leicht Datenzugriffe auf einen gemeinsamen Datenbestand synchronisiert von mehreren laufenden Clients durchführen (wie zum Beispiel in Microsoft Outlook). Die Zugriffe über RPC sind allerdings langsamer.\n\nBefinden sich Server und Client in einem Rechnernetz, so kommt DCOM (\"Distributed COM\") zum Einsatz. Der Einsatz von DCOM ermöglicht grundsätzlich den Betrieb von Server und Client auf unterschiedlichen Betriebssystemen.\n\nDCOM benutzt im Gegensatz zum Local Server ein vollständig implementiertes RPC, was die Aufrufe jedoch (auch bei sehr geringer Netzwerkauslastung) deutlich verlangsamt. Die Implementierung vom DCOM unterscheidet sich von der von COM mit Local Server zusätzlich noch durch den vorgeschalteten Protokollstack.\n\nDie COM-Schnittstelle dient der Kommunikation zwischen Client und Server. Eine COM-Komponente kann dazu über allgemein definierte und vorgegebene Schnittstellen (zum Beispiel IUnknown, IDispatch) sowie über spezielle Schnittstellen angesprochen werden.\n\nJede Schnittstelle hat eine weltweit eindeutige Identifikationsnummer, die GUID (\"Globally Unique Identifier\"). Dadurch können auch mehrere Schnittstellen mit demselben Namen existieren (aber nicht mit derselben GUID).\n\nUm eine programmiersprachenübergreifende Client/Server-Kommunikation zu ermöglichen, findet an der Schnittstelle das so genannte \"Marshalling\" statt, das die auszutauschenden Daten in eine vordefinierte Binärrepräsentation wandelt.\n\nEine Schnittstelle erfüllt die Funktion einer abstrakten Klasse, die lediglich virtuelle Elementfunktionen enthält, die (wegen der Trennung von Deklaration und Implementierung) in der VTable alle auf 0 gesetzt werden. Die C-Version einer Schnittstelle ist entsprechend eine Struktur, die Funktionszeiger enthält. Die erzeugten COM-Objekte nutzt man dabei über Zeiger auf deren Schnittstellen.\n\nWenn ein COM-Objekt eine Schnittstelle implementiert, muss es alle Methoden der Schnittstelle überschreiben, also die VTable füllen. Dabei sind mindestens die drei Methoden von codice_5 zu implementieren, die für das Lebenszyklusmanagement zuständig sind und eventuell vorhandene, weitere implementierte Schnittstellen offenlegen.\n\nEine Schnittstelle sieht in der für COM-Komponenten nutzbaren IDL (\"Interface Definition Language\") wie folgt aus (als Beispiel dient das Interface codice_5):\n\n// Standardschnittstelle aller COM-Komponenten\ninterface IUnknown {\n\nJede Schnittstelle muss über eine Schnittstellen-Vererbung die Funktionen der hier gezeigten Schnittstelle codice_5 definieren, da dieses die grundlegenden Funktionen für COM implementiert. Eine weitere Vererbung der Schnittstellendefinitionen ist möglich.\n\nDa Programmiersprachen wie Visual Basic Script keine Typen kennen, hat Microsoft eine weitere Möglichkeit entwickelt, Funktionen aus COM-Schnittstellen aufzurufen. Für diese Möglichkeit muss die Schnittstelle die Funktionen der Schnittstelle codice_8 definieren. Dies ermöglicht es, eine COM-Komponente über codice_9 anzusprechen, ohne dass der COM-Client die Typbibliothek des Servers kennen muss. Da der Zugriff über das Dispatch-Interface sehr viel langsamer als der Zugriff über ein typisiertes Interface ist, wird oft beides implementiert (\"Dual Interface\"), so dass bei Programmiersprachen, die Zeiger beherrschen, beide Zugriffsmöglichkeiten zur Verfügung stehen.\n\nEine COM-Komponente bietet die aufrufbaren Funktionen über eine oder mehrere COM-Schnittstellen an. Die Erzeugung des Objektes erfolgt durch die Implementierung von codice_10 im COM-Server.\n\nDie Lebensdauer eines Objektes wird mittels Referenzzählung gesteuert. Eine COM-Komponente lebt nur so lange, wie die Differenz der Aufrufe von codice_11 (am Beginn der Verwendung einer Instanz) und codice_12 (Freigabe nach Verwendung der Instanz) nicht 0 ergibt.\n\nEine COM-Komponente kann mehrere Schnittstellen anbieten. Dies ist in bestimmten Situationen auch notwendig, um ein Programm erweitern zu können, ohne andere Programme neu kompilieren zu müssen, denn der Compiler kodiert die aus der VTable gelesenen Einsprungadressen der vom Client aufgerufenen Funktionen unter bestimmten Umständen fest. Wird die Schnittstelle einer Komponente später geändert, kann sich die Einsprungadresse ändern, was die Funktionstüchtigkeit des Clients beeinträchtigen würde. Zur Erweiterung der Serverfunktionalität wird also stattdessen eine weitere Schnittstelle implementiert.\n\nEine Vererbung von COM-Komponenten (\"Aggregation\") ist durch die Anforderungen der Binärkompatibilität nur in wenigen Programmiersprachen möglich. Dazu wird die zu vererbende Komponente über explizite Durchleitung der Schnittstellen über die erbende Komponente veröffentlicht.\n\nDer Client ist das Programm, das\nDer Client kennt die Funktionen, die von der COM-Komponente angeboten werden, da diese in den entsprechenden COM-Schnittstellen deklariert sind. Die Veröffentlichung von Schnittstellen erfolgt entweder über Typbibliotheken oder Beschreibungen in der IDL (\"Interface Definition Language\").\n\nCOM-Objekte werden bei der Erzeugung immer einem so genannten Apartment zugeordnet. Dabei handelt es sich um transparente Rahmen, die zur Synchronisierung von Methodenaufrufen mehrerer Objekte dienen, die mit unterschiedlichen Anforderungen an die Threadsicherheit arbeiten. Wird COM nicht mitgeteilt, dass eine entsprechende Komponente threadsicher ist, wird COM nur einen Aufruf gleichzeitig an ein Objekt erlauben. Threadsichere Komponenten können auf jedem Objekt beliebig viele Aufrufe gleichzeitig ausführen.\n\nGeschieht ein Aufruf im gleichen Apartment zwischen verschiedenen Objekten, ist kein Marshalling erforderlich. Wird jedoch eine Schnittstelle über Apartmentgrenzen hinweg benutzt, muss ein Marshalling erfolgen.\n\nJeder Thread, der COM verwenden möchte, muss sich vor der ersten Verwendung einer COM-Funktionalität einem Apartment zuordnen (MTA) oder ein neues Apartment erstellen (STA). Dies geschieht über die Funktion codice_13. Programmiersprachen mit integrierter COM-Unterstützung (zum Beispiel VB6 und die meisten .NET-Sprachen) führen diese Zuordnung oft automatisch durch.\n\nJede COM-Komponente wird bei Erzeugung einem Apartment zugeordnet. Falls die Apartment-Anforderungen der erzeugten Komponente zum Apartment des erzeugenden Threads passen, wird das Objekt dem gleichen Apartment zugeordnet. Bei Aufrufen über Prozessgrenzen hinweg liegen die beiden Objekte immer in verschiedenen Apartments. Die Zuordnung zu einem Apartment kann während der Lebensdauer des Objektes nicht geändert werden.\n\nEs gibt drei Arten von Apartments:\n\nDurch den Einsatz von COM gibt es die Möglichkeiten\nzu programmieren.\nViele der Funktionen des „Windows Platform SDKs“ sind über COM zugänglich. COM ist die Basis, auf der OLE-Automation und ActiveX aufbauen. Mit der Einführung des .NET-Frameworks verfolgte Microsoft allerdings die Strategie, COM unter Windows durch dieses Framework abzulösen. Im Folgenden werden die einzelnen Punkte der Aufzählung genauer erläutert.\n\nCOM-Komponenten sind unabhängig von der Programmiersprache. COM unterstützt den so genannten Binärstandard. Die erzeugte Binärdatei stellt einerseits die implementierten Funktionen zur Verfügung und andererseits eine Schnittstelle, die diese Funktionen aufzeigt. Mit Hilfe der Schnittstelle ist es möglich, von anderen Programmen aus die Funktionen zu verwenden. Dabei wird mit Konzepten aus dem Bereich Verteilte Systeme gearbeitet.\n\nEin weiterer Vorteil beim Einsatz von COM ist es, dass man die Verwaltung von neuen Softwarefeatures einfach in eine bestehende Anwendung integrieren kann. Oftmals kann es Probleme geben, wenn man herstellerneutrale oder herstellerübergreifende Softwarekomponenten mit weiteren Funktionen ausstattet. Dadurch kann zwar die eigene Software erweitert werden, jedoch besteht die Gefahr, dass andere Software, die ebenfalls die herstellerübergreifenden Komponenten verwendet, nicht mehr funktionsfähig bleibt.\n\nCOM bietet eine robuste Möglichkeit an, um eine Softwarekomponente mit neuen Funktionen zu erweitern. Dies wird dadurch ermöglicht, dass mehrere Schnittstellen in einer Header-Datei zusammengefasst werden können. Der folgende C++-Programmcode verdeutlicht dies:\n//\n// Interface mathematik.h\nclass IStandardMathFunctions : public IUnknown\npublic:\n\nclass IAdvancedMathFunctions : public IUnknown\npublic:\n\nDiese Header-Datei namens codice_14 enthält zwei Schnittstellen. Die erste Schnittstelle könnte beispielsweise die herstellerübergreifenden Funktionen anbieten, die von verschiedenen Programmen verwendet werden. Durch die zweite Schnittstelle codice_15 wird diese Softwarekomponente erweitert. Weitere Schnittstellen können jederzeit hinzugefügt werden. Die alten Schnittstellen und darin enthaltenen Funktionen gehen dabei nicht verloren. Das Hinzufügen neuer Schnittstellen statt des Veränderns derselben ist so die von Microsoft gedachte Form, Softwarekomponenten zu erweitern, da so keine Inkonsistenzen entstehen.\n\nx64-Applikationen können dank Marshalling auf 32bittige COM-Server zugreifen (und umgekehrt). Der COM-Server muss dann in einem eigenen Prozess laufen und seine Objekte können demnach nicht als In-process-Server instanziiert werden. COM-Applikationen sind jedoch fast ausschließlich auf die Windows-Betriebssystemfamilie und von dieser unterstützte Hardware angewiesen, eine Plattformunabhängigkeit war konzipiert, wurde aber wohl nur in wenigen Ausnahmefällen realisiert.\n\nBeim Einsatz von COM wird \"objektorientiert\" gearbeitet. Trotzdem können COM-Komponenten auch zum Beispiel in C erstellt und genutzt werden, da die Schnittstellen tatsächlich eine Sammlung von Funktionszeigern sind (abstrakte Klasse in C++, struct in C).\n\nCOM ist \"ortsunabhängig\", d. h. dass die einzelnen COM-Komponenten an einer zentralen Stelle \"(Registrierungsdatenbank)\" angemeldet werden und so der Zugriff auf die Komponenten unabhängig von ihrem eigentlichen Ort erfolgen kann.\n\nDas Steuern von Anwendungen über COM-Schnittstellen wird als Automatisierung bezeichnet. Von dieser Anwendungsmöglichkeit wird häufig im Rahmen von OLE (Object Linking and Embedding) Gebrauch gemacht.\n\nDurch eine Sicherheitslücke in der RPC-Implementierung von DCOM wurde die Angriffsweise des bekannten Wurms W32.Blaster möglich.\n\n\n\n"}
{"id": "54761", "url": "https://de.wikipedia.org/wiki?curid=54761", "title": "PowerBook", "text": "PowerBook\n\nDas PowerBook war ein tragbares Macintosh-Notebook von Apple.\n\nNachdem der erste Macintosh Portable von 1989 noch sehr schwer im Vergleich zu tragbaren DOS-Rechnern war, brachte Apple 1991 das erste Macintosh-PowerBook auf den Markt.\n\nIm Januar 2006 wurde das Nachfolgemodell, das MacBook Pro mit einem x86-Prozessor von Intel, eingeführt. Mittlerweile sind das MacBook, das MacBook Pro sowie das MacBook Air erhältlich. Diese lösen die 15″, 17″ und 12″ PowerBooks ab.\n\nDie nachfolgend erwähnten Codenamen sind Bezeichnungen, mit denen eine neue Geräte-Generation vor ihrem Erscheinen auf dem Markt intern benannt werden.\n\n\n\n\n\nTitanium\n\nHauptartikel: PowerBook G4 Titanium\n\nAluminium\n\n\n"}
{"id": "55617", "url": "https://de.wikipedia.org/wiki?curid=55617", "title": "Datasette", "text": "Datasette\n\nEine Datasette (Commodore: \"Datassette\") ist ein in den 1980er Jahren weitverbreitetes Bandlaufwerk, um Computerdaten auf herkömmlichen Kompaktkassetten (CC) zu speichern. Datasette ist ein Kofferwort aus (englisch für Daten) und \"Cassette\". Die Bezeichnung stammt ursprünglich von Commodore, wurde später gelegentlich auch für ähnliche Geräte anderer Heimcomputer, u. a. von Atari, Apple, Robotron, Tandy, Sinclair, Texas Instruments und Amstrad/Schneider verwendet.\n\nEs gab speziell als Datenbänder bezeichnete Kassetten (z. B. die Produkte Magna oder Computape), allerdings waren diese teuer und boten weniger Speicherplatz als herkömmliche Kassetten an, so dass meistens handelsübliche Audio-Kassetten als Daten-Kassette genutzt wurden. Auf einer herkömmlichen Kassette mit 30 Minuten können im Standardformat der Commodore-Rechner rund 100 kByte gespeichert werden. Durch die Verwendung von Ladebeschleunigern wie Turbo Tape können grob 1 MByte pro 30 Minuten Band gespeichert werden. Die Bitrate liegt im Bereich von rund 300 Bit/s bis zu knapp 5 kBit/s.\n\nHeute kommen Datasetten nicht mehr zum Einsatz, da sie den aktuellen Datenträgern in Bezug auf Kapazität und Geschwindigkeit um viele Größenordnungen unterlegen sind. Zudem sind fast alle Datasetten rein lineare Medien, bei dem Bandstellen vom Benutzer per Hand mittels langwierigem Spulen aufgesucht werden müssen; damit sind sie den Medien mit wahlfreiem Zugriff, wie etwa Disketten, Festplatten oder CD-ROMs, auch prinzipiell unterlegen.\n\nKassettenlaufwerke mit wahlfreiem Zugriff waren nur in der mittleren Datentechnik vor allem der 1970er-Jahre vertreten; als einzige Heimcomputer, die über ein Kassettenlaufwerk mit wahlfreiem Zugriff verfügten, gelten der Philips P2000M aus dem Jahr 1980 sowie der tragbare Epson HX-20 und PX-8. Beide verwendeten die vom Hersteller für Diktiergeräte entwickelte und auf den Start-/Stopp-Betrieb ausgelegte Minikassette. Dort brachten beide etwa 170 KB an Daten unter.\n\nDaneben gab es noch eine Reihe von Laufwerken, die spezielle Kassetten mit einem Endlosband verwendeten, etwa das MicroDrive der Firma Sinclair, oder das eher exotische Entrepo Quick Data Drive für den Commodore 64.\n\nEntfernte Verwandte der Datasette sind Laufwerke, die Daten auf VHS-Videokassetten oder auf Video-8-Bänder abspeichern. Diese wurden wegen ihres hohen Preises nicht bei Heimcomputern verwendet, kamen aber teilweise im Profibereich zur Datensicherung großer Archive zum Einsatz, da sie für damalige Verhältnisse eine extrem hohe Speicherkapazität boten (auf einem 240-Minuten-VHS-Band konnten schon Mitte der 1980er Jahre über 2 Gigabyte gespeichert werden, was damals dem Inhalt mehrerer Dutzend Festplatten entsprach). Bandlaufwerke dieser Art werden auch als „Streamer“ bezeichnet.\n\nAuch die ersten digitalen Synthesizer (sowie einige Analog-Synthesizer mit Speichermöglichkeit, wie der Korg Poly 61) verfügten oft über eine Buchse zum Anschluss eines Tonbandgerätes oder eines Kassettenrekorders, über die sich einzelne Presets auf Tonband oder Kassette speichern und wieder abrufen ließen. Da vom jeweiligen Instrument selber keine Steuerung des Laufwerks vorgenommen wurde, konnte prinzipiell jedes zur Aufzeichnung von Audio geeignete Medium verwendet werden. Die in den Tonstudios verwendeten Medien variierten mit der Zeit und so wurden auch digitale Formate wie DAT, DCC und Minidisc häufig genutzt. Liebhaber von klassischen Synthesizern speichern ihre Presets heute in der Regel über die Audioschnittstelle eines Personal Computers. \n\nDie Datasetten 1530 und 1531 von Commodore sind 0,7 kg schwer und 19,5 cm breit, 5 cm hoch und 15 cm tief. Sie unterschieden sich durch Gehäusefarbe und Steckertyp voneinander. Letztere hat einen 8-poligen Mini-DIN-Rundstecker nur für den Anschluss an die Heimcomputer der Commodore-264-Serie (C16, C116 und Plus/4), bei einigen 1531-Modellen lag ein 1530-kompatibler Adapterstecker bei.\n\nZum Bau einer Datasette werden das Laufwerk, der Vorverstärker und die Tonköpfe eines normalen Musik-Kassettenrekorders verwendet, auf Lautsprecher und Mikrofon wird verzichtet. Als zusätzliches Element besitzt sie einen Demodulator, welcher in diesem Fall ein Schmitt-Trigger ist und zur Ermittlung der Nulldurchgänge des empfangenen Signals dient.\n\nDie Daten werden meist (Ausnahme: Firma Atari) mit einer modifizierten Frequenzumtastung auf dem Band gespeichert, wie in nebenstehender Abbildung dargestellt. Bei dieser Aufzeichnung trägt die Amplitude keine Information. Beim Lesen und zum Dekodieren wurden nach dem Schmitt-Trigger die zeitlichen Abstände zwischen den positiven bzw. negativen Nulldurchgängen des Signals über Timer-Bausteine wie den MOS Technology CIA (6526) ausgemessen und aus dieser Zeitinformation die Dateninformation zurückgewonnen. In nebenstehender Skizze sind die für die Datenrekonstruktion wichtigen negativen Nulldurchgänge des Signals als schwarze Kreise auf der Mittellinie eingezeichnet. Je nach zeitlichem Abstand können so die beiden Werte logisch-\"1\" und logisch-\"0\" unterschieden werden.\n\nDie so einzeln empfangenen Bits werden dann in ein Schieberegister geschoben und am Anfang einer Datenübertragung fortlaufend mit speziellen Bitsequenzen zur Synchronisation verglichen. Das erste Byte einer solchen Sequenz zur Synchronisation des Blockanfangs wird als \"Lead-In-Byte\" bezeichnet, gefolgt von mehreren \"Sync-Bytes\", welche dazu dienen, mögliche Gleichlaufschwankungen des Bandes durch Abstimmung der Timer zu kompensieren. Beispielsweise verwendet der am Commodore 64 verwendete Schnelllader \"Turbo Tape 64\" als \"Lead-In-Byte\" 0x02, gefolgt von der Sync-Sequenz 0x08, 0x07, 0x06, 0x05, 0x03, 0x02, 0x01. Danach folgten die eigentlichen Nutzdaten.\n\nGeräte der Firma Atari verwendeten eine direkte Frequenzumtastung. Statt die Nulldurchgänge des Signals zu erkennen und deren Zeitabstand zu messen, wurden beim Abspielen zwei feststehende Frequenzen, die für die Bitwerte 0 und 1 standen, mittels Bandpassfiltern aus dem Audiosignal herausgefiltert und in ihrer Amplitude verglichen; das jeweils stärkere Signal bestimmte den von der Atari-Datasette ausgegebenen Logikpegel. Schnelllade-Programme ließen sich mit den Atari-Datasetten nicht nutzen, da die meisten Eigenschaften des Aufzeichnungsformats durch die Hardware der Geräte unveränderlich vorgegeben waren.\n\nFür manche Computermodelle (u. a. Apple II, die Sinclair-Modelle und die Homecomputer KC 85 aus DDR-Produktion) gab es keine passenden speziellen Datasetten vom selben Hersteller. Stattdessen konnte jeder handelsübliche Audio-Kassettenrekorder über die Ton-Ein- und Ausgänge angeschlossen werden, der Demodulator befand sich in diesem Fall im Computer selbst.\n\nAuch beim ursprünglichen IBM-PC sowie beim IBM PCjr war ein Datasettenport vorhanden, welcher wie der Tastaturport als 5-poliger weiblicher DIN-Rundsteckeranschluss ausgeführt war. Allerdings galten Kassetten als Speichermedien bereits zur Einführung des IBM-PC für das angepeilte Marktsegment als veraltet; außerdem konnte nur über das interne ROM-Basic des IBM-PC direkt auf die Datasette zugegriffen werden, nicht aber über das wesentlich mächtigere und meist eingesetzte DOS. Aus diesen Gründen wurde das Interface kaum genutzt und es entstand, anders als bei früheren Kleincomputern, kein Markt für vorbespielte Programmkassetten. Bereits beim unmittelbaren Nachfolger, dem IBM PC XT, wurde die Schnittstelle nicht mehr verwendet. Seitens des PC-BIOS war die Programmierschnittstelle für Anwendungsprogramme sehr einfach gehalten und bestand aus vier Funktionen des Software-Interrupts 15h. Diese ermöglichten, den Laufwerksmotor zu starten (Funktion 00h, AH=00h), zu stoppen (Funktion 01h, AH=01h) sowie das Lesen (Funktion 02h) und Schreiben (Funktion 03h) einer bestimmten Anzahl von Bytes auf das Band, welche in CX angegeben werden musste. In ES:BX war der Zeiger auf die Speicheradresse des Datenpuffers zu nennen. Da DOS – im Gegensatz zum Handling von Diskettenlaufwerken – keine weiteren Routinen für die Datasette bot, standen lediglich die Lowlevel-Zugriffsroutinen des BIOS für die Nutzung der Schnittstelle durch Programmierer und Anwender zur Verfügung, welche eigene Wege für die Verwaltung der Rohdaten auf den Kassetten finden mussten.\nNach dem Verschwinden der Datasetten-Schnittstelle wurde bei den PC-Nachfolgern der Interrupt 15h für andere Zwecke verwendet. Beim PC AT konnte ein versuchter Aufruf der Datasettenroutinen sogar zum Systemabsturz führen.\n\nZur Aufzeichnung und zum Auslesen von Daten ist es wichtig, dass der Tonkopf richtig justiert ist. Verwendet wurde typischerweise nur eine Monospur mit einer typischen Bandbreite von rund 10 kHz. Eine Ausnahme sind die Geräte von Atari, die auf einer Stereospur die Daten, auf der anderen eine Musikspur zur Untermalung des Ladevorgangs unterbrachten. Gleichlaufschwankungen, die vom Antrieb und der Kassette herrühren, wurden entweder durch eine entsprechend geringe und somit robustere Datenrate, oder bei einigen Schnellladern durch spezielle, laufend wiederholte Synchronisations-Sequenzen zur Laufzeit kompensiert, welche je nach Verfahren auch innerhalb von Datenblöcken wiederholt wurden.\n\nGespeichert werden kann, je nach Heimcomputer, in verschiedenen Dateiformaten. Selbstgeschriebene Programme wurden häufig als eine einzelne BASIC-Datei gespeichert. Kommerzielle Programme und Spiele bestanden, ebenso wie Diskettenprogramme, in der Regel aus mehreren Dateien (Titelgrafik, weitere Level), die dann nachgeladen wurden und oft in Maschinensprache gespeichert waren. Auf der Rückseite der Kassette befand sich häufig eine identische Kopie des Spiels oder weitere Level.\n\nEmulatoren verwenden fast nur Kassettenabbilder, wie .TAP und .T64, seltener auch echte Tondateien wie .WAV.\n\nIn der Fernsehsendung WDR Computerclub wurden Audiosignale als sogenannter \"Hard-Bit-Rock\" in BASICODE gesendet, die man aufnehmen und per Datasette einlesen konnte.\n\n"}
{"id": "55700", "url": "https://de.wikipedia.org/wiki?curid=55700", "title": "PC Magazin", "text": "PC Magazin\n\nPC Magazin, publiziert von der WEKA Media Publishing, ist eine monatlich erscheinende Computerzeitschrift, die sich vor allem mit den technischen Aspekten von Computern, Software, Hardware und weiteren IT-spezifischen Themen auseinandersetzt.\n\nDie Zeitschrift wurde 1987 als „DOS International“ (im damaligen Verlag DMV) gegründet. Die DOS International war die erste deutschsprachige Computerzeitschrift, die sich ausschließlich mit IBM-kompatiblem PCs beschäftigte. Weite Verbreitung fand sie vor allem unter Programmierern. Mitte 1992 konnte sich die „DOS“ mit einer verkauften Auflage von über 200.000 Heften – noch vor der Chip – sogar kurz an die Spitze der deutschsprachigen Computerzeitschriften setzen. \nAb Juli 1994 wurde die Zeitschrift mc nur noch als 45-seitige Beilage im Abonnement der DOS International vertrieben.\n\nMit der Übernahme des Magna Media Verlages fiel das Recht am Titel „PC Magazin“ an die WEKA Holding. Wegen der sinkenden Bedeutung des Betriebssystems „DOS“ entschied sich die Geschäftsleitung, den Titel des Flaggschiffs von „DOS“ auf „PC Magazin“ zu ändern. In einer Übergangsphase (1996/97) nannte sich die Publikation „PC-Magazin DOS“.\n\nDas ursprüngliche PC Magazin erschien von 1984 bis 1997 wöchentlich bei der PC Magazin Verlagsgesellschaft mbH, einem Tochterunternehmen der WEKA-Gruppe. Ab Juni 1997 wechselte die Redaktion dieser Zeitschrift zur \"InformationWeek\".\n\nSie erscheint je Auflage üblicherweise in zwei Ausgaben: als Super Premium-Ausgabe mit drei DVDs und als DVD-Ausgabe mit einer DVD. Die Inhalte bestehen hauptsächlich aus aktuellen News, Reports, Praxisbeiträgen und Tests.\n\nIm vierten Quartal 2012 lag die durchschnittliche verbreitete Auflage nach IVW bei 105.316 Exemplaren. Das sind 18.110 Exemplare pro Ausgabe weniger (–14,67 %) als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm innerhalb eines Jahres um 5.892 Abonnenten auf nun 49.035 pro Ausgabe ab (–10,73 %); damit bezogen rund 46,56 % der Leser die Zeitschrift im Abo.\n\n"}
{"id": "55995", "url": "https://de.wikipedia.org/wiki?curid=55995", "title": "Toy Story", "text": "Toy Story\n\nToy Story ist ein Computeranimationsfilm des Regisseurs John Lasseter aus dem Jahr 1995. Der von den Pixar Animation Studios im Auftrag der Walt Disney Company produzierte Film ist der erste vollständig am Computer erstellte Langfilm für das Kino. Er feierte am 19. November 1995 Premiere in Hollywood und kam am 21. März 1996 in die deutschen Kinos.\n\nIm Kinderzimmer von Andy hat die Hauptfigur, Cowboy-Sheriff Woody, das Sagen, denn er ist Andys Lieblingsspielzeug. Doch ein paar Tage vor dem Umzug der Familie erhält Andy zum Geburtstag den modernen Space-Ranger \"Buzz Lightyear\". Die Spielzeuge aus Andys Zimmer sind von ihm begeistert, schließlich ist er mit den modernsten Raffinessen ausgestattet, die ein Spielzeug nur haben kann. Plötzlich ist Woody nicht mehr das unangefochtene Spitzen-Spielzeug und will aus Eifersucht Buzz loswerden. Woody versucht Buzz und den anderen Spielzeugen immer wieder klarzumachen, dass Buzz nur ein Spielzeug sei, weil sich Buzz für den echten Buzz Lightyear hält, und spielt Buzz’ Fähigkeiten herunter. Schließlich verübt er ein Attentat auf ihn, indem er beabsichtigt, Buzz in eine dunkle Ecke des Kinderzimmers zu stoßen. Doch der Versuch schlägt fehl, Buzz wird versehentlich aus dem Fenster geschleudert und landet im Vorgarten. Die anderen Spielzeuge sind nicht in der Lage, ihm zu helfen, und ächten Woody, weil es für ein Spielzeug einem Todesurteil gleichkommt, sich in der Nähe des Nachbarjungen Sid zu befinden. Denn er probiert an wehrlosen Spielzeugen die grausamsten Foltermethoden aus.\n\nWährend sich die Spielzeuge noch streiten, nimmt Andy Woody zu einem Ausflug ins Erlebnislokal \"Pizza Planet\" mit. Buzz gelingt es, der beängstigenden Zukunft im Vorgarten zu entfliehen, indem er sich am Auto von Andys Mutter festhält, als es die Einfahrt verlässt. Als das Auto an einer Tankstelle hält, verlässt Andy es kurz. Kaum ist er außer Hörweite, geraten Woody und Buzz in einen handgreiflichen Streit, in dessen Verlauf beide aus dem Auto fallen und alleine bei der Tankstelle zurückbleiben. Doch sie haben Glück: Wenige Minuten später hält das Auto eines Pizza-Planet-Mitarbeiters neben ihnen. Woody begreift, dass eine heimliche Mitfahrt in diesem Auto die einzige Chance ist, zurück zu Andy zu finden. Doch er weiß, dass er Buzz mitnehmen muss, um das Missverständnis seines Unfalls aufklären zu können.\n\nIn der Pizza-Planet-Filiale verstecken sich die beiden in einem Spielautomaten. Kurz darauf entdeckt sie der Nachbarsjunge Sid, der sie mit nach Hause nimmt. In dessen Zimmer lebt eine große Anzahl entstellter und aus verschiedenen Bauteilen zusammengesetzter Spielzeuge. Sid fesselt Buzz an eine Rakete, um ihn am nächsten Tag in die Luft zu schießen. Buzz und Woody wollen fliehen, doch alle Versuche scheitern. An einem Werbespot im Fernsehen und der Funktionslosigkeit seiner Ausrüstung erkennt Buzz schließlich, dass er in Wahrheit nicht der echte Space-Ranger ist, sondern nur ein Spielzeug. Daraufhin verfällt er in eine Depression und wehrt sich nicht gegen den brutalen Sid. Zusammen mit den Spielzeugen aus Sids Zimmer kann Woody Buzz retten. Dabei erteilen sie Sid eine Lektion, so dass er aufhört, Spielzeuge zu quälen.\nWoody und Buzz gelingt die Flucht.\n\nDie beiden erreichen zwar Andys Wohnung, aber die Umzugsfuhre ist bereits gepackt und die Familie fährt ihnen vor der Nase davon. Andy ist unglücklich, seine beiden Lieblingsspielzeuge verloren zu haben und nicht mit in die neue Wohnung nehmen zu können. Nach dem gescheiterten Versuch, den Umzugswagen mit einem ferngesteuerten Spielzeugauto zu verfolgen, kommt Woody auf die Idee, Sids Rakete zu zünden, die Buzz immer noch auf dem Rücken gebunden trägt. Das Manöver glückt tatsächlich, und beide Spielzeuge fliegen durch das Schiebedach ins Auto. Andy findet sie sofort im Umzugskarton und ist überglücklich, beide wiederzuhaben. Die beiden Helden erkennen, dass es nicht nur darauf ankommt, das beste Spielzeug zu sein, sondern von einem Kind geliebt zu werden. Als Team haben sie es geschafft, Andys Herz zu erobern. Aus den ehemaligen Konkurrenten sind schließlich Freunde geworden.\n\nDie deutsche Synchronisation wurde von der FFS Film- & Fernseh-Synchron GmbH in München erstellt. Das Dialogbuch verfasste Pierre Peters-Arnolds, der auch für die Synchronregie zuständig war. Die im Original von Randy Newman komponierte und mit englischen Texten gesungene Filmmusik wird von Klaus Lage mit deutschen Texten gesungen.\n\nZum Rendern des Filmes wurde eine Renderfarm aus 117 Sun SparcStation 20 verwendet. Der fertige Film belegte 500 Gigabyte und wurde auf 1200 CDs zwischengespeichert. \n\nTrotz zahlreicher realer Spielzeuge im Film tritt keine Barbie-Puppe auf, da Hersteller Mattel dies nicht zulassen wollte. Nach dem großen Erfolg des ersten Teils erhielt man jedoch die Erlaubnis zur Verwendung von Barbies in \"Toy Story 2\".\n\nObwohl der Film im Breitbildformat 1,85:1 produziert wurde, enthält die deutsche DVD-Veröffentlichung (Special Edition) nur ein 1,78:1-Bild ().\n\n\nAm 2. Oktober 2009 wurde der Film in den USA in 3D wieder in die Kinos gebracht. Er war ausschließlich in einer Doppelvorführung zusammen mit Toy Story 2, welcher ebenfalls in 3D konvertiert wurde, zu sehen. Ursprünglich sollten die Vorführungen auf einen Zeitraum von zwei Wochen begrenzt werden. Wegen des großen Erfolgs wurde dies jedoch auf fünf Wochen ausgedehnt.\n\nFür die 3D-Konvertierung wurden die ursprünglichen Computerdaten aufbereitet und eine virtuelle zweite Kamera hinzugefügt, so dass ein stereoskopes Bildmaterial vorlag, was für die Tiefenwahrnehmung unerlässlich ist. Allein dieser Prozess nahm vier Monate in Anspruch. Anschließend brauchte es weitere sechs Monate, um die Filme mit adäquaten 3D-Effekten zu versehen. \n\nDie 3D-Doppelvorführung von Toy Story 1 und 2 hat innerhalb der fünfwöchigen Laufzeit 30.714.027 US-Dollar (23.686.658 €; Stand: 17. Juli 2010) eingespielt, davon alleine 12,5 Millionen US-Dollar (9,64 Millionen €; Stand 17. Juli 2010) am Eröffnungswochenende.\n\nDie Linuxdistribution Debian GNU/Linux verwendet die Namen der Charaktere aus \"Toy Story,\" um die verschiedenen Releases ihrer Software zu bezeichnen, „Buzz“ (1.1), „Rex“ (1.2), „Bo“ (1.3), „Hamm“ (2.0), „Potato“ (2.2), „Woody“ (3.0), „Sarge“ (3.1), „Etch“ (4.0), „Lenny“ (5.0), „Squeeze“ (6.0), „Wheezy“ (7.0), „Jessie“ (8.0), „Stretch“ (9.0), „Buster“ (10.0 testing) und „Sid“ (unstable).\n\nDer Besamungsbulle Toystory erhielt seinen Namen nach dem Film.\n\nDer Film erhielt überwiegend positive Kritiken: Bei Rotten Tomatoes fallen alle 98 Kritiken für den Film positiv aus – womit er eine Wertung von 100 % besitzt – und bei Metacritic konnte ein Metascore von 92, basierend auf 16 Kritiken, erzielt werden.\n\n2005 wurde \"Toy Story\" in das National Film Registry aufgenommen, in dem als besonders erhaltenswert geltende US-Filme verzeichnet sind. Der Film gehört zudem seit 2007 laut dem American Film Institute zu den 100 besten amerikanischen Filme aller Zeiten. \n\nDie Deutsche Film- und Medienbewertung (FBW) vergab das \"Prädikat wertvoll\".\n\n\n"}
{"id": "56152", "url": "https://de.wikipedia.org/wiki?curid=56152", "title": "MLDonkey", "text": "MLDonkey\n\nMLDonkey ist ein freies Filesharingprogramm, das auf verschiedene Netzwerke zugreifen kann.\n\nFolgende Netzwerke werden unterstützt:\n\nDie Unterstützung von Gnutella und Gnutella2 wurde ab Version 2.9.0 aufgegeben, da sie von keinem Maintainer gepflegt werden.\n\nAußerdem können auch HTTP-, FTP- und SFTP-/SCP-Downloads mit MLDonkey durchgeführt werden.\n\nMLDonkey wurde vor allem für Unix-basierte Systeme (GNU/Linux, diverse BSDs, macOS/Darwin) entwickelt, inzwischen existiert auch eine lauffähige Windows-Portierung.\n\nEiner der großen Unterschiede zu vergleichbaren Softwareanwendungen wie Kazaa oder Lopster ist die strikte Trennung in eine Kern-Applikation (\"Core\"), welche die eigentliche Funktionalität zur Verfügung stellt, und eine Oberfläche, die die Interaktion mit dem Benutzer zur Verfügung stellt. Hierzu stehen drei verschiedene Protokolle bereit:\n\nDiese Trennung ist insbesondere attraktiv, um den Kern von einem anderen Rechner aus zu steuern. Es muss nur eine Verbindung (mittels einer der drei genannten Varianten) aufgebaut werden um Kommandos abzusetzen. Zum Herunterladen selbst ist keine Verbindung von der Benutzerschnittstelle zum MLDonkey-Kern nötig.\n\nMLDonkey kann dieselbe Datei gleichzeitig aus dem eDonkey2000-Netzwerk und dem Overnet beziehen und dort anbieten, ist also ein Hybrid wie der offizielle eDonkey2000-Client.\n\n"}
{"id": "56162", "url": "https://de.wikipedia.org/wiki?curid=56162", "title": "ScummVM", "text": "ScummVM\n\nScummVM ist eine Software- und Skript-Sammlung, in der zahlreiche nachgebaute Spiel-Engines unter einer gemeinsamen grafischen Benutzeroberfläche zusammengefasst sind. Viele ältere Computer- und Konsolen-Spiele sind damit auf wesentlich moderneren oder auch ganz anderen Plattformen lauffähig, als deren ursprüngliche Entwickler vorhersehen oder bei der Programmierung berücksichtigen konnten.\n\nZunächst lediglich für Adventures auf Basis der Skriptsprache SCUMM des Anbieters LucasArts entwickelt, unterstützt \"ScummVM\" (Akronym für „Script creation utility for maniac mansion - Virtual Machine“) heute eine ganze Reihe Spiele anderer Hersteller wie beispielsweise Revolution Software, Adventure Soft oder Sierra Entertainment.\n\nScummVM ist selbst als freie Software unter der GNU General Public License veröffentlicht. Die einzelnen damit spielbaren Titel sind jedoch mit wenigen Ausnahmen weiterhin durch Rechte der jeweiligen Hersteller geschützt.\n\nScummVM ist weder ein Emulator noch eine Laufzeitumgebung, sondern Interpreter für verschiedene Skriptsprachen, wie sie für ältere Adventure-Spiele entwickelt wurden.\n\nFür ScummVM werden verschiedene Spiel-Interpreter per Reverse Engineering analysiert und anschließend neue Implementierungen entwickelt. Dabei wird auf weitestgehend kompatible und verhaltensgleiche Wirkung geachtet. So wird direkt auf die originalen Spiele-Ressourcen, wie z. B. Hintergründe, Bilder, Grafiken, Animationen, Figuren, Soundeffekte, Musik und Videos zugegriffen. Deren Verknüpfung mit den Benutzerinteraktionen als Spielablauf/Handlung werden jedoch nicht mehr vom Interpreter, sondern plattformunabhängig durch von den Spieldesignern geschriebene Skripte gesteuert — daher im Namen das „VM“ für Virtuelle Maschine.\n\nScummVM ermöglicht es, die Grafikdarstellung im Vergleich zum Original zu verbessern. Da viele alte Adventures standardmäßig nur mit einer Auflösung von 320×200 laufen, ist so ein angenehmes Spielen auch auf Monitoren mit einer deutlich höheren Auflösung möglich. ScummVM bietet viele verschiedene Grafikmodi, die unterschiedliche Techniken einsetzen, um die Qualität der Darstellung zu optimieren. Hierbei kommen vor allem Techniken wie Weichzeichnen, Kantenglättung und Filter zum Einsatz.\nAußerdem ist es mit ScummVM möglich, diverse Spiele mit Audio- und Musikausgabe zu spielen, auch wenn nicht die vom Hersteller vorgegebene Sound-Hardware verwendet wird, da die betriebssystemeigenen Soundfunktionen verwendet werden. Als Musikausgabe kann auch direkt \"FluidSynth\" verwendet werden, wenn das Betriebssystem keine MIDI-Ausgabe unterstützt.\n\nScummVM ist in C++ geschrieben und benutzt für Grafik- und Sound-Ausgaben die portablen Programmbibliotheken SDL, MAD, Vorbis oder Tremor, FLAC und libmpeg2. Neben dem Quellcode sind für die meisten unterstützten Betriebssysteme Binärdateien auf der Website des Projekts verfügbar. Viele Linux-Distributionen bieten kompilierte Programmpakete an, und auch in den Repositories der FreeBSD/NetBSD/OpenBSD-Distributionen ist es enthalten. Es gibt Implementierungen für die Plattformen Microsoft Windows, macOS, Unix- und Linux-Systeme, BSD, für iOS, Android, Windows CE, Symbian OS, bada, AmigaOS, Atari und die Konsolensysteme Dreamcast, GameCube, Nintendo DS, PlayStation, Wii und Xbox.\n\nAb Version 1.2.0 „FaSCInating release“ kann die zunächst rein englische Oberfläche auch in anderen Sprachen angezeigt werden.\n\nHinter ScummVM steht eine private Gruppe von Entwicklern. Etwa halbjährlich erscheinen neue Versionen, die neben Fehlerkorrekturen und Verbesserungen der Spielbarkeit auch immer Unterstützung für weitere Spiele mit sich bringen. Das Projekt ist seit 2007 regelmäßig beim Google Summer of Code vertreten.\n\nZunächst konzentrierte sich die Entwicklung ausschließlich auf das von LucasArts entwickelte Skriptsystem SCUMM und alle Spiele, die mit der zugehörigen Spiel-Engine \"SPUTM\" entwickelt wurden. Neben den LucasArts-Adventures wie den ersten drei Monkey-Island-Spielen nutzen diverse Spiele von Humongous Entertainment dieses System. Inzwischen wächst jedoch die Zahl der unterstützten Adventures zunehmend um Titel, die auf ganz anderen Engines entwickelt worden sind.\n\nDie Entwickler der Projekte Sarien und \"FreeSCI\" haben sich entschieden, das Ergebnis ihrer Arbeit in das ScummVM-Projekt einzubringen bzw. ScummVM gemeinsam weiter zu entwickeln. Bis dahin hatten sie sich erfolgreich, aber von ScummVM unabhängig, der Spielbarkeit älterer Adventures von Sierra On-Line (heute Sierra Entertainment) gewidmet.\nZahlreiche Adventures, die mit \"TrollVM\" (preAGI) und Adventure Game Interpreter (AGI) entwickelt wurden, werden schon seit einiger Zeit von ScummVM unterstützt. Ab Version 1.2.0 beinhaltet ScummVM die schon lange erwartete Ergänzung um den Sierra Creative Interpreter (SCI). Damit sind weitere Adventures aus der Reihe Leisure Suit Larry, den Quest-Serien King’s Quest, Police Quest, Space Quest und andere bekannte Klassiker, nicht zuletzt auch zahlreiche, ebenfalls in SCI entwickelte Fan Games über ScummVM spielbar.\n\nScummVM selbst enthält keine Spiele. Die Benutzer müssen sich also zunächst ein eigenes Exemplar eines der unterstützten Spiele kaufen. Durch ihr hohes Alter sind einige der Spiele jedoch in preiswerten Sammlungen, zum Beispiel den \"LucasArts Classics\", erhältlich.\n\nInzwischen gibt es auch einige Adventures, die von den Entwicklern freigegeben wurden:\n\nDeren Entwickler entschieden sich nicht nur, dem ScummVM-Team Einblick in den Quelltext zu gewähren, sondern die Spieldaten sowohl der CD- als auch der Disketten-Versionen als Freie Software zu veröffentlichen, wodurch sie auf der ScummVM-Website zum Herunterladen angeboten werden und darüber hinaus auch freien Linux-Distributionen beigefügt werden dürfen.\n\nWeitere Adventures sind \"Dragon History\" (\"NoSense\"), das für ScummVM neu überarbeitet und dessen Quelltext unter GPLv2 gestellt wurde, sowie \"TeenAgent\" (\"Metropolis Software House / CD Projekt\"), das auf Good Old Games als Freeware heruntergeladen kann.\n\nResidualVM (früher nur „Residual“) ist ein Nebenprojekt einiger ScummVM-Entwickler, welches zum Ziel hat, auch 3D-Grafik-Adventures ebenso auf verschiedenen Systemen lauffähig zu machen. Dabei konzentrierte man sich anfangs nur auf Grim Fandango und Flucht von Monkey Island, welche beide auf der Skriptsprache Lua, sowie der Spiel-Engine GrimE fußen. Mittlerweile wird auch unterstützt.\n\nAm 9. März 2014 gaben die Entwickler bekannt, dass ResidualVM unter der Schirmherrschaft von ScummVM als Projekt für das Stipendium Google Summer of Code akzeptiert wurde.\n\n\n"}
{"id": "56674", "url": "https://de.wikipedia.org/wiki?curid=56674", "title": "Computeranimation", "text": "Computeranimation\n\nComputeranimation bezeichnet die computergestützte Erzeugung von Animationen. Sie verwendet die Mittel der Computergrafik und ergänzt sie um zusätzliche Techniken.\n\nManchmal wird zwischen \"computergestützter\" und \"computergenerierter\" Animation unterschieden. Während erstere sich auf digitalisierte Zeichnungen stützt, zwischen denen die Animationssoftware interpoliert, um flüssige Bewegungsabläufe zu erhalten, arbeitet letztere mit einer dreidimensionalen Szene, aus der mit den Mitteln der Bildsynthese direkt Bilder erzeugt werden.\n\nDie Technik der Schlüsselbildanimation () stammt ursprünglich aus der Produktion von Zeichentrickfilmen. Dabei werden nicht alle Einzelbilder von Hand gezeichnet, sondern nur sogenannte Schlüsselbilder, die grob den Bewegungsablauf vorgeben. Die Einzelbilder zwischen den Schlüsselbildern werden durch Interpolationstechniken automatisch berechnet, dies nennt man auch \"Tweening.\"\n\nBei computergenerierten Animationen ist der Begriff „Schlüsselbild“ insofern irreführend, als die Interpolation nicht anhand der vollständigen Bilder erfolgt. Stattdessen werden verschiedene Parameter der Szene gesetzt, etwa die Positionen der Objektmittelpunkte, ihre Farben und Skalierung, die Kameraposition und -blickrichtung oder die Intensität der Lichtquellen. Für unterschiedliche Parameter können auch unterschiedliche Schlüsselbilder gewählt werden.\n\nUm plötzliche Geschwindigkeitsänderungen zu vermeiden, wird die Interpolationskurve der Schlüsselwerte meist so gewählt, dass ihre Ableitung stetig ist. Eine höhere stetige Differenzierbarkeit ist üblicherweise nicht nötig, da die zweite Ableitung (Beschleunigung) sich auch in der Natur oft abrupt ändert. Diese Eigenschaften machen Catmull-Rom-Splines zu einer guten Wahl für Animationskurven. Viele Animationssysteme erlauben dem Animator, durch gezielte Anpassung der Tangenten an den Schlüsselwerten die Animationskurve fein abzustimmen. Dies geschieht oft durch Kontrolle von \"Tension, Continuity\" und \"Bias\" (TCB). Dazu werden die Catmull-Rom- zu Kochanek-Bartels-Splines erweitert, mit denen diese drei Parameter für jeden Schlüsselwert eingestellt werden können.\n\nWenn ein Bewegungspfad durch Digitalisierung gewonnen wurde, so muss er vor seiner Verwendung erst geglättet werden, um Sprünge und Rauschen zu entfernen.\n\nDie bloße Beschreibung der Animationskurve ist im Allgemeinen nicht ausreichend, wenn nicht nur die Position, sondern auch die Geschwindigkeit eines Objekts entlang eines Pfades kontrolliert werden soll. Das liegt daran, dass durch die Anpassung einer Kurve an mehrere Punkte nichts über die Geschwindigkeit zwischen diesen Punkten ausgesagt wird. Die Parametrisierung der üblicherweise zur Animation verwendeten Kurventypen stimmt nicht mit der tatsächlichen Distanz entlang einer Kurve überein.\n\nEine Möglichkeit, die Geschwindigkeit eines Objektes zu kontrollieren, besteht darin, den Animator eine zusätzliche Distanz-Zeit-Funktion bestimmen zu lassen, die angibt, wie weit sich ein Objekt zu einem bestimmten Zeitpunkt entlang der Kurve bewegt haben soll. Ebenfalls möglich sind Geschwindigkeit-Zeit- oder sogar Beschleunigung-Zeit-Funktionen. In jedem Fall muss das Animationsprogramm intern die Kurvenlänge entlang einer Animationskurve in deren Parameterdarstellung umwandeln. Für die meisten Spline-Typen gibt es dazu keine analytische Formel, sodass Näherungsverfahren wie das Newton-Verfahren angewandt werden müssen.\n\nDas häufigste Geschwindigkeitsprofil in der Animation ist Dabei beschleunigt ein Objekt vom Startpunkt aus, erreicht eine Höchstgeschwindigkeit, und bremst schließlich bis zum Endpunkt wieder ab. Dieses Verhalten kann durch ein Segment der Sinusfunktion modelliert werden.\n\nDie Rotation ist neben der Verschiebung die einzige Transformation, die die Form eines Objektes beibehält; sie spielt daher bei der Animation starrer Körper eine große Rolle. Zur Interpolation von dreidimensionalen Rotationen werden besondere Methoden verwendet. Eine einfache Art, die Rotation eines Objektes anzugeben, ist mittels eulerscher Winkel. Wenn diese Winkel animiert werden, kann das Gimbal-Lock-Problem auftreten. Dieser Effekt entsteht, wenn eine der drei Rotationsachsen mit einer anderen zusammenfällt, wodurch ein Freiheitsgrad verloren geht.\n\nUm dieses Problem zu umgehen, werden in der Computeranimation Quaternionen zur Formulierung von Rotationen verwendet. Quaternionen bilden einen vierdimensionalen Raum, für den Operationen wie Addition und Multiplikation definiert sind. Um einen Punkt zu rotieren, wird er zunächst als Quaternion dargestellt, die Rotation im Quaternion-Raum angewandt, und wieder in die üblichen kartesischen Koordinaten umgewandelt. Aufeinanderfolgende Rotationen im Quaternion-Raum entsprechen Produkten von Quaternionen. Rotationen werden meistens durch Einheitsquaternionen ausgedrückt, die man sich als Punkte auf einer vierdimensionalen Einheitssphäre vorstellen kann. Die Interpolation auf der vierdimensionalen Einheitssphäre wird auch Slerp genannt. Da sie mathematisch sehr aufwändig ist, wird oft nur linear zwischen Quaternionen interpoliert. Die Zwischenschritte werden anschließend normalisiert, um sie wieder auf die vierdimensionale Einheitssphäre zu projizieren. Gleichmäßigere Resultate lassen sich erreichen, indem der De-Casteljau-Algorithmus angewandt wird, sodass mehrfache lineare Interpolationen ausgeführt werden.\n\nSoll ein Objekt einem Bewegungspfad folgen, so wird oft erwartet, dass es nicht nur verschoben wird, sondern auch, dass seine Orientation dem Pfad folgt. Die Beziehung zwischen der Orientation eines Objekts und den Kurveneigenschaften kann durch die frenetschen Formeln ausgedrückt werden. Dabei müssen einige Spezialfälle beachtet werden, in denen die Formeln nicht angewandt werden können, etwa bei Kurvenabschnitten ohne Krümmung. Falls die Kamera bewegt werden soll, so wird oft ein \"Center of interest\" angegeben, das stets im Zentrum des Bildes liegen soll. Das Animationsprogramm ändert dann die Orientation der Kamera entsprechend.\n\nObjekte sind oft hierarchisch modelliert, sodass die Randbedingungen bezüglich Verbundenheit oder relativer Platzierung in einer baumartigen Struktur organisiert sind. Ein Beispiel für ein hierarchisches Modell ist ein Planetensystem, in dem Monde um Planeten und diese wiederum um die Sonne rotieren. Maschinenteile oder Figuren sind aus zusammenhängenden Objekten aufgebaut, die untereinander mit diversen Gelenktypen wie Schub- und Drehgelenken verbunden sind. Derartige hierarchisch aufgebaute Objekte werden im Englischen \"Articulated Structures\" genannt. In der Computeranimation derartiger Modelle wird zum Teil auf die kinematischen Techniken der Robotik zurückgegriffen. Für die Character animation sind diese Techniken jedoch nicht ausreichend, da sich Figuren auf freiere, komplexere und geschicktere Weise bewegen als Roboter. Ein weiterer Grund ist, dass die Gelenke von Tieren selbst verformbar sind und somit nicht mit den üblichen Methoden modelliert werden können.\n\nEine direkte, vergleichsweise mühsame Methode zur Animation von kinematischen Ketten oder zusammenhängenden Strukturen nutzt die direkte Kinematik. Um eine bestimmte Bewegung zu definieren, beginnt der Animator an der Wurzel der Hierarchie und fährt mit den Unterknoten fort, wobei er jedes Mal das Animationsskript ändert. So wird bei der Animation eines Beines zunächst die zeitliche Änderung der Rotation am Hüftgelenk vorgegeben, was in einem plumpen Schritt resultiert. Anschließend wird das Kniegelenk und schließlich der Fußknöchel animiert. Selbst dieses einfache Beispiel ist mit Problemen behaftet, so etwa wird die vertikale Verschiebung der Hüfte während der Bewegung nicht berücksichtigt.\n\nDie inverse Kinematik arbeitet auf höherer Ebene. Hierbei wird nur die Bewegung der Endpunkte einer Struktur manuell definiert. Die Animationssoftware berechnet dann automatisch das Verhalten der übrigen Gelenke, um diese Bewegung auszuführen. Je komplizierter eine Struktur ist, desto schwieriger werden diese Berechnungen. Ein weiteres Problem der inversen Kinematik ist, dass sie dem Animator kaum Spielraum lässt.\n\nFür die Animation der Dinosaurier im Film \"Jurassic Park\" (1993) wurde direkte Kinematik verwendet, wobei jedoch die Bewegungsabläufe in Stop-Motion-Manier von echten Modellen mit Sensoren ermittelt wurden. Eine weitere Möglichkeit ist Motion Capture, bei der die Bewegungen eines Darstellers mittels Sensoren oder Marker auf dessen Körper erfasst werden.\nAußerdem gibt es noch rein synthetisch berechnete Modelle, die mit virtuellen Knochen und Muskeln realistische Bewegungen erzeugen.\n\nDie Kollisionserkennung ist ein unverzichtbares Hilfsmittel, wenn mehr als ein Objekt animiert werden soll. Dabei werden in einem ersten Schritt die Objekte ermittelt, die miteinander kollidieren, und in einem zweiten Schritt die genauen Kollisionspunkte berechnet. In ihrer naiven Form hat die Kollisionserkennung in Abhängigkeit von der Anzahl der berücksichtigten Objekte quadratische Laufzeit. Der Prozess kann erheblich beschleunigt werden, indem Objekte in Bounding Volumes gepackt werden, zwischen denen ein schnellerer Test auf Überlappung möglich ist. Wenn die Bounding Volumes einander nicht überlappen, so tun dies auch die enthaltenen Objekte nicht. Wenn ja, müssen die enthaltenen Objekte direkt gegeneinander getestet werden. Die Körper, die die Objekte umschließen, können auch hierarchisch strukturiert werden.\n\nEine weitere Möglichkeit zur Beschleunigung beruht auf der Erkenntnis, dass die Kollisionserkennung in gleichen Zeitabständen verschwenderisch ist. Stattdessen sollte sie zu Zeitpunkten durchgeführt werden, deren Häufigkeitsverteilung proportional zur Wahrscheinlichkeit des Auftretens von Kollisionen ist. Ein weiteres Problem ist, dass die Kollisionserkennung zu einem Zeitpunkt einsetzen kann, bei dem vor allem schnell bewegte Objekte bereits ein Stück ineinander eingedrungen sind. Dieser Effekt muss nachträglich korrigiert werden, etwa indem der Weg der Objekte entlang des Bewegungspfads zurückberechnet wird.\n\nDie Kollisionserkennung kann verschiedenen Anwendungen dienen, zum Beispiel um den Weg eines Objekts unter Vermeidung von Kollisionen zu berechnen. Physikalisch lässt sich eine Kollision als elastischer oder plastischer Stoß beschreiben und die weitere Bewegung der Objekte entsprechend berechnen.\n\nUm das Verhalten mehrerer Objekte mit den Mitteln der Gruppensimulation zu bestimmen, kann der Animator Techniken der künstlichen Intelligenz verwenden. Dadurch plant jedes der Objekte die Bewegungen, die einem festgelegten Plan entsprechen, und führt sie aus. Viele Computerspiele verwenden derartige \"autonome Objekte,\" um intelligente Gegner oder Verbündete des Spielers zu erzeugen. Es stellt sich heraus, dass eine einzelne Figur umso weniger Intelligenz aufweisen muss, je größer die Gruppe ist. Bei hunderten von Figuren entsteht ein Schwarmverhalten, bei dem die Bewegung jeder Figur – hier oft Boid genannt – auf einfache Weise von nur wenigen Nachbarn beeinflusst wird. Kollisionsvermeidung ist eines der Ergebnisse eines solchen Verhaltens.\n\nEine wesentlich einfachere, aber dennoch nützliche Technik zur Kontrolle von Objektgruppen sind Partikelsysteme. Die Anzahl der verwendeten, oft sehr kleinen Partikel ist üblicherweise erheblich größer als bei Schwarmanimationen und kann viele Tausend erreichen. Die genaue Anzahl der Partikel kann außerdem während der Animation schwanken, da bei jedem Schritt neue Partikel entstehen und alte gelöscht werden können. Dies ist einer der Gründe warum z. B. Flüssigkeitssimulationen wie RealFlow für jedes zu rendernde Bild ein neues 3D-Modell erzeugen. Die Bewegung einzelner Partikel folgt oft den Gesetzen der Mechanik. Zu den Anwendungen von Partikelsystemen zählen die Animation von Explosionen, spritzenden Flüssigkeiten, Rauch, Feuer oder Erscheinungen ohne feste Begrenzung. Um ein plausibleres Erscheinungsbild zu gewährleisten, können die Bewegung und andere Parameter der Partikel zufällig verändert werden. Das Verhalten eines Partikelsystems wird normalerweise bestimmt, indem Kräfte im Raum festgelegt werden. So kann zum Beispiel ein Partikel in eine neue Richtung „geblasen“ werden, sobald es einen bestimmten Ort erreicht, von einem „Gravitationszentrum“ angezogen oder die eigentlich fertiggestellte Simulation wie ein normales 3D-Modell verformt werden.\n\nMit physikalisch basierten Simulationen lassen sich meist wesentlich realistischere Animationen als mit Partikelsystemen erzeugen. Beispiele sind die Animation von Flüssigkeiten, Feuer, Rauch oder flexiblen Stofftüchern. Hierzu müssen die Differentialgleichungen, die diese Erscheinungen beschreiben, mit den Mitteln der numerischen Mathematik gelöst werden.\n\n\n\n"}
{"id": "56738", "url": "https://de.wikipedia.org/wiki?curid=56738", "title": "Macintosh Plus", "text": "Macintosh Plus\n\nDer Macintosh Plus war ein Rechnermodell der Firma Apple Computer. Er wurde im Jahre 1986 eingeführt und war bis 1990 im Programm.\n\nDer Macintosh Plus hatte als Prozessor einen Motorola 68000 mit 8 MHz Taktfrequenz und basierte auf dem Gehäuse der Macintosh 128K- und Macintosh 512K -Reihe mit eingebautem 9-Zoll-Bildschirm (512×342 Pixel, monochrom). Das Gehäuse war anfangs beigefarben, später platingrau.\nGegenüber den Vorgängermodellen wurde der Arbeitsspeicher auf 1 MB vergrößert und konnte durch simples Aufstecken von Speichermodulen auf 4 MB erweitert werden. Hinzukommen ein größeres ROM (128 KB statt 64 KB) und eine SCSI-Schnittstelle, an die insbesondere externe Fest- und Wechselplatten, aber auch SCSI-Ethernetadapter angeschlossen werden konnten.\nAuf der hinteren Innenseite des Gehäuses trugen die frühen Macintosh Plus die Signaturen der Entwickler. Dazu durfte jeder maßgeblich an der Entwicklung beteiligte Mitarbeiter eine Unterschrift auf einem Papier leisten, welche dann mittels Filmmatrize in die bereits vorhandene Gussform geätzt wurde. So entstand auf der Gehäuseinnenseite ein Reliefeffekt mit erhabenen Unterschriften. Diese erste Master-Gussform lebte jedoch nicht lange, und es wurden nur die ersten 70.000 Apples so gegossen. Danach kam eine zweite Form, bei der man den Aufwand einer teuren nachträglichen Ätzung nicht mehr betrieb.\n\nDer Macintosh Plus besaß keine Lüfter; er machte sich vielmehr den Kamineffekt zunutze. Die Lüftungsschlitze am Boden und auf der Gehäuseoberseite des Mac Plus durften daher nie abgedeckt werden, da sonst der Mac innerhalb kürzester Zeit überhitzte und Teile der Hardware sogar durchschmorten. Dies galt insbesondere für den Macintosh Plus, den Macintosh 512k(e) und den Macintosh 128k, die alle ohne Lüfter ausgeliefert wurden.\n\nObwohl der Macintosh einen passenden Anschluss für ein 5,25-Zoll-Laufwerk bot, durfte man diese Laufwerke nicht anschließen, da man möglicherweise den Controller des Mac beschädigen konnte. Es sollten daher nur die üblichen 3,5-Zoll-Laufwerke für 800 KB oder Apples SuperDrives (HD-Laufwerke mit 1440 KB) angeschlossen werden.\n\nNachfolger des Macintosh Plus war der Macintosh SE, der mit einem Erweiterungssteckplatz und ADB ausgestattet wurde.\n\nMit Mini vMac ist es möglich folgende Macs zu emulieren:\n\nEin ROM-Abbild eines dieser Macs wird für den Betrieb benötigt; es kann mittels Zusatzprogramm leicht von diesen ausgelesen werden. Mini vMac läuft unter Mac OS (X), Linux (x86), Solaris (SPARC und x86), Windows und anderen Betriebssystemen und erlaubt die Nutzung von System 0.8 bis 7.5.5.\n\n\n"}
{"id": "56899", "url": "https://de.wikipedia.org/wiki?curid=56899", "title": "On-Board-Unit (Mautsystem)", "text": "On-Board-Unit (Mautsystem)\n\nDie (OBU) ist ein Gerät, das in Lastkraftwagen eingebaut wird, um die automatische Abrechnung in einem Gebührenerhebungs- oder Mautsystem zu ermöglichen. Es gibt mittlerweile verschiedene OBUs am Markt. Bekannt sind in Mitteleuropa derzeit vor allem die Tripon-Geräte des Schweizer Unternehmens Fela Management AG (OBU in der Schweiz in Betrieb seit dem Jahr 2000), die GoBox der Firma Europpass im seit 2003 laufenden Mautsystem in Österreich sowie die von Grundig und Siemens VDO Automotive AG entwickelten OBUs für die von Toll Collect betriebene LKW-Maut in Deutschland. Seit dem Dezember 2012 werden von Toll Collect auch OBUs der Hersteller Grundig und Siemens ausgeliefert. Auch die französischen, italienischen, spanischen (und weitere Länder Europas) Autobahnbetreiber kennen OBUs für die Erfassung von Mautgebühren.\n\nDie OBUs der Firma Toll Collect verbleiben im Eigentum von Toll Collect und werden dem Benutzer nur zur Verfügung gestellt. Die Kosten für den Einbau (etwa 250 Euro), den Ausbau bei Vertragsende, und entsprechende Folgekosten (An- und Abfahrt, Standzeit) muss der Fahrzeughalter selbst tragen. Ursprünglich war geplant, mit Mautbeginn 300 Euro Pfand zu erheben, die der Nutzer als Mautguthaben abfahren kann. Im September 2004 wurde diese Verpflichtung aufgehoben. Der Einbau der ersten OBU-Geräte begann ab dem 1. Mai 2003.\n\nDie OBUs besitzen ungefähr die Größe eines Autoradios. Produziert werden sie von Grundig im Werk Braga (Portugal) und von der Continental AG (ehemals Siemens VDO Automotive AG) am Standort Villingen-Schwenningen. In Villingen werden von Conti rund zwei Drittel der Geräte hergestellt, der Rest kommt von Grundig. Die Conti/Siemens-OBU basiert auf dem ursprünglich vom Konkurrenz-Konsortium AGES entwickelten Prototyp.\n\nDie Herstellungskosten der OBU in Höhe von etwa 500 Euro je Stück wurden für die ersten 150.000 Stück von Toll Collect übernommen. 300.000 , die das Bundesministerium für Verkehr, Bau und Stadtentwicklung zusätzlich zur ursprünglich geplanten Stückzahl bestellt hat, wurden dem Ministerium von Toll Collect mit 150 Mio. Euro in Rechnung gestellt.\n\nDie Toll-Collect-OBUs besitzen einen Empfänger für das von den USA betriebene GPS-Satellitenortungssystem, mit dem die Position des Fahrzeugs jederzeit bestimmt werden kann. Eingebaut ist außerdem ein GSM-Modul und ein Speicher, in dem fahrzeugspezifische Angaben wie die Anzahl der Fahrzeugachsen, die Schadstoffklasse und das Kfz-Kennzeichen sowie die Positionsdaten aller mautpflichtigen deutschen Autobahnen enthalten sind. Aus diesen Daten, zusammen mit dem GPS-Signal errechnet die während der Fahrt die fällige Maut und speichert diese zu einem Datenpaket. In Form einer SMS wird dieses nach Erreichen einer bestimmten Datenmenge per GSM-Modul an den Zentralrechner von Toll Collect gesendet.\n\nDie Bauweise, das Konzept und die Bedienung der Geräte von Siemens und Grundig sind unterschiedlich. Die Siemens-OBU wird als vorprogrammierte Einheit hinter die Windschutzscheibe montiert und besitzt eine Zwei-Tasten-Bedienung. Das Grundig-Gerät ist aufgeteilt in einen kleinen Infrarot-Transponder hinter der Windschutzscheibe, und den eigentlichen Bordcomputer für das nach ISO 7736 genormte Einschubfach im Armaturenbrett. Es gibt viele Tasten, von denen jedoch nicht alle aktiviert sind.\n\nDie Software der ersten OBU-Geräte wurde von der EFKON mobility GmbH (Berlin) entwickelt. Aus Medienberichten ist bekannt, dass Teile davon auch von omp computer gmbh (Paderborn) stammen. (Im Aufsichtsrat dieses Unternehmens befand sich zum Zeitpunkt der Auftragsvergabe pikanterweise auch der damalige Toll-Collect-Geschäftsführer Michael Rummel.)\n\nIm Dezember 2003 wurde bekannt, dass die Software der neuen OBUs von den Firmen Siemens und IBM komplett neu entwickelt werden sollte.\n\nZunächst konnten von Toll Collect nicht genug geliefert und eingebaut werden, da Schwierigkeiten mit der komplexen Software der Geräte bestanden. Nachdem Medien und Spediteure schon seit Monaten über Probleme mit den ausgelieferten OBUs berichteten, räumte Toll Collect Ende September 2003 erstmals technische Fehler ein.\n\nDie \n\n20.000 Geräte (einige sprechen auch von 35.000) des Herstellers Grundig sollten im Rahmen einer Rückrufaktion des Systembetreibers Toll Collect repariert werden. Dabei sollte unter anderem ein Software-Update auf das Geräte-EEPROM gebrannt werden. Ein Grundig-Sprecher erklärte dazu, dass man sich als reiner Lieferant der Hardware betrachte, für die Software sei Toll Collect zuständig.\n\nSpäter wurde bekannt dass Michael Rummel, der damalige Geschäftsführer von Toll Collect, den Einbau der angeordnet hatte, obwohl er wusste, dass diese nicht funktionstüchtig waren. Eine Rolle bei dieser Entscheidung dürfte die oben angegebene Fähigkeit zum Softwareupdate per GSM-Funk gespielt haben, durch die die Software kurz vor \"Scharfschaltung\" des Systems noch hätte aktualisiert werden können. Ironischerweise war aber auch diese Funktion von den Softwareproblemen betroffen, wie sich später herausstellte.\n\nSeit Sommer 2004 wurde die neue, überarbeitete OBU von Toll Collect zum Einbau an die Vertragswerkstätten geliefert. Diese entspricht in ihrer Funktionalität der seit 1. Januar 2005 begonnenen ersten Stufe der Mauteinführung. Nach bisherigen Nachrichten funktioniert die OBU fehlerfrei und hat eine Zuverlässigkeit von über 99 %.\n\nZur Einführung der zweiten Stufe der Maut ab 1. Januar 2006 erhielt die OBU ihre volle Funktionalität, d. h. es sind über die „Luftschnittstelle“ Updates der Betriebsdaten möglich, welche unter anderem zum Aktualisieren der Kilometerpauschale dienen. Außerdem sollen so auch die mautpflichtigen Strecken bei Neubauten, Umwidmungen oder Fahrbahnverschwenkungen aktualisiert werden können.\n\nBis August 2005 waren etwa 450.000 OBUs in deutschen und ausländischen LKW eingebaut worden, davon waren 45.000 mit der OBU-Software 2.0 ausgerüstet. Bis April 2005 waren etwa 284.000 OBU in deutschen LKW eingebaut worden. Hinzu kommen die in ausländischen LKW verbauten OBU. So sind zum Beispiel etwa 33.100 in niederländischen, 20.500 in polnischen, 14.700 in österreichischen und 12.950 in tschechischen LKW eingebaut worden.\n\nIm Juli 2014 waren etwa 804.000 Fahrzeuge mit einer On-Board-Unit ausgestattet. Durch die für 2015 geplante Absenkung der Mautpflichtgrenze von 12 auf 7,5 t werden zusätzlich etwa 85.000 Fahrzeuge erwartet.\n\nIm Rahmen der Öffnung der nationalen Systeme für Drittanbieter werden Bordgeräte mit standardisierten Schnittstellen zu den Systemen der Mautbetreiber entwickelt. Dabei ist eine länderübergreifende Mauterhebung mit nur einem Gerät möglich.\n\n\n"}
{"id": "57063", "url": "https://de.wikipedia.org/wiki?curid=57063", "title": "Xetra", "text": "Xetra\n\nXetra (ETR, Marktidentifikationscode XETR) ist ein börslicher Handelsplatz der Frankfurter Wertpapierbörse (FWB). Mit einem Marktanteil von über 90 Prozent im Aktienhandel und bei börsengehandelten Fonds (ETF) ist er der bedeutendste Börsenhandelsplatz Deutschlands (Stand 2015). Der Name Xetra ist eine Abkürzung für („elektronischer Börsenhandel“).\n\nIm Jahr 2015 wurden 90 Prozent des gesamten Aktien­handels an deutschen Börsen über den Handelsplatz Xetra abgewickelt. In den DAX-Werten erreicht Xetra europaweit einen Marktanteil von 60 Prozent. Die Handelszeiten sind börsentäglich von 9.00 bis 17.30 Uhr. Die Preise auf Xetra sind Basis zur Berechnung des bekanntesten deutschen Aktienindex DAX.\n\nÜber die Zentralrechner in Frankfurt am Main sind über 200 Handelsteilnehmer aus 16 europäischen Ländern sowie aus Hongkong und den Vereinigten Arabischen Emiraten miteinander verbunden.\n\nDurch hohe Umsätze (Liquidität) wird eine Wertpapierorder am Handelsplatz Xetra schnell und zu marktgerechten Preisen ausgeführt. Dieses Prinzip wird zusätzlich durch Liquiditätsspender (sogenannte Designated Sponsors) unterstützt. Diese verpflichten sich, laufend verbindliche An- und Verkaufspreise (Quotes) zu stellen, und sorgen damit für zusätzliche Liquidität und faire Preise.\n\nDa der gesamte Börsenhandel elektronisch vollzogen wird, kann es am Handelsplatz Xetra zu Teilausführungen der Aufträge kommen. Banken und Online-Brokern entstehen durch taggleiche Teilausführungen jedoch keine zusätzlichen Transaktionskosten. Die meisten Anbieter stellen Teilausführungen bei ihren Kunden nicht zusätzlich in Rechnung.\n\nZur Verbesserung der Preiskontinuität und um Mistrades zu vermeiden, wurden am Handelsplatz Xetra Schutzmechanismen eingeführt. Dazu gehören die Volatilitätsunterbrechung (\"Volatility Interruption\"), die Market Order-Unterbrechung und die Liquiditätsunterbrechung. Der Handel am Handelsplatz Xetra erfolgt nach klaren Regeln, die für alle Handelsteilnehmer gleichermaßen gelten. Die unabhängige Marktaufsicht setzt sich zusammen aus der Handelsüberwachungsstelle (HÜSt), der Börsenaufsichtsbehörde des Hessischen Ministeriums für Wirtschaft, Verkehr und Landesentwicklung und der Bundesanstalt für Finanzdienstleistungsaufsicht (BaFin).\n\nDie Zulassung von Wertpapieren an der Frankfurter Wertpapierbörse und deren Handelsplätzen Xetra und Börse Frankfurt benötigt keine separate Zulassung. Neu gelistete Wertpapiere am Handelsplatz Xetra werden automatisch in den Handel an der Börse Frankfurt einbezogen.\n\nDie Trägergesellschaften der Frankfurter Wertpapierbörse sind die Deutsche Börse AG und die Börse Frankfurt Zertifikate AG.\n\nDas Handelssystem wurde von Beginn an als Markenprodukt konzipiert. Das gestaltete Logo wie auch die Wortmarke sind als Marke der Deutsche Börse AG geschützt.\n\nDie Handelstechnologie von Xetra löste am 28. November 1997 das IBIS ab. Befürworter führten marktgerechte Ausführungspreise, geringe Transaktionskosten, Gleichberechtigung, Standortunabhängigkeit und Anonymität der Handelspartner ins Feld. Als Herausforderungen galten vor allem Systemstabilität, Systemverfügbarkeit, Skalierbarkeit und Latenz sowie eine langfristig steigende Marktaktivität.\n\nDie Handelstechnologie von Xetra wurde schon in der Entwicklungsphase als Markenprodukt positioniert. Das Ziel war es, Xetra bei Handelsteilnehmern und in der Öffentlichkeit so besser positionieren und auch an andere Börsenbetreiber lizenzieren zu können. Die Konzeption und die informationstechnische Umsetzung der Handelstechnologie von Xetra basierten auf der Handelstechnologie von Eurex und wurden im Auftrag der Deutsche Börse AG von Andersen Consulting (heute Accenture) und Deutsche Börse Systems durchgeführt.\n\nDie Deutsche Börse AG betreibt heute am Standort Frankfurt am Main im Kassamarkt-Geschäftsbereich Deutsche Börse Cash Market zwei Handelsplätze basierend auf der Handelstechnologie von Xetra:\n\n\nDie Handelstechnologie von Xetra wird darüber hinaus seit November 1999 auch an der Wiener Börse verwendet. Im Dezember 2010 führte die Wiener Börse Xetra an der Ljubljana Stock Exchange ein; die Zagreber Börse folgte im Juli 2017. Eingesetzt wird die Handelstechnologie ebenfalls an der Prager Börse (seit November 2012), an der Budapester Börse (seit Dezember 2013), an der Irish Stock Exchange (seit 2000), an der Bulgarischen Börse (seit 2008), an der Malta Stock Exchange (seit 2012) und an der Cayman Islands Stock Exchange (seit 2013). Eurex Bonds, die Deutsche Börse Handelsplattform für Staatsanleihen, verwendet die Handelstechnologie von Xetra. Auf der Basis der Handelstechnologie von Xetra wurde auch das New Generation Trading System der Shanghai Stock Exchange entwickelt.\n\n\n"}
{"id": "57224", "url": "https://de.wikipedia.org/wiki?curid=57224", "title": "Sinclair ZX80", "text": "Sinclair ZX80\n\nDer Sinclair ZX80 ist ein Heimcomputer der britischen Firma Sinclair aus dem Jahr 1980. Er ist der Vorgänger des Sinclair ZX81. Die Hardware des in weit höheren Stückzahlen produzierten Nachfolgers ist ähnlich, jedoch höher integriert und dadurch kostengünstiger. Das Gehäuse des ZX80 ist weiß und etwas klobiger als das des schwarzen ZX81. Das Gehäuse des ZX80 wurde von dem Industriedesigner Rick Dickinson gestaltet. Die Z80-CPU ist mit 3,25 MHz getaktet, was dem damaligen Stand der Technik entspricht.\n\nBeim Sinclair ZX80 wollte man ein Minimalsystem schaffen, bei dem eine Besonderheit des Z80 Mikroprozessors genutzt wurde. Dieser erzeugt im Gegensatz zum Intel 8080, auf dem er basiert, während seiner internen Arbeitszyklen fortlaufende Adressen auf seinem Adressbus. Diese sind dazu gedacht dynamische Speicher zyklisch aufzufrischen. Sinclair nutzte diese Refresh-Adressen zweckentfremdet um Grafikdaten Pixel für Pixel fortlaufend aus dem Arbeitsspeicher abzurufen und auf dem Bildschirm darzustellen. Damit konnte ein Grafikbaustein eingespart werden. Der Prozessor konnte entweder Bilddaten erzeugen oder intern rechnen. So kippte das Bild bei jedem Tastendruck und während interner Programmabarbeitung weg und wurde dann bei Eingabeanforderungen oder Programmstopps wieder dargestellt. Dieser irritierende Effekt führte zum Slowmodus beim Nachfolgemodell ZX81. Der Prozessor konnte dann in den horizontalen und vertikalen Austastlücken, also an allen Bildschirmrändern, während keine Ansteuerung des Bildschirmsignals erfolgte, seine Rechenarbeiten erledigen, was die Arbeitsgeschwindigkeit deutlich einschränkte, aber ein stabiles Bild ermöglichte. Höhere Integration der Bausteine machte dies möglich. \n\nDer ZX80 wird über den eingebauten HF-Modulator mit einem Kabel an die Antennenbuchse eines Fernsehers angeschlossen. Dort wird Text mit 32 Spalten und 24 Zeilen angezeigt.\n\nDer Zeichensatz ist nicht ASCII-konform und enthält außer Buchstaben und Ziffern auch die Schlüsselwörter des BASIC. Die meisten Befehle des BASIC werden mit einem einzelnen Tastendruck eingegeben. Nur wenige Funktionen wie PEEK() und CHR$() müssen Buchstabe für Buchstabe eingegeben werden. Dadurch benötigen die Programme nur wenig Platz im relativ kleinen Arbeitsspeicher des ZX80.\n\nZwischen dem ZX80 und seinem Nachfolger ZX81 gibt es einige Unterschiede:\nAnsonsten sind der ZX80 und ZX81 technisch weitgehend gleich, so dass sich weitere Details zum ZX80 dem Artikel zum ZX81 entnehmen lassen. Der ZX80 ließ sich mit dem 8 KB großen ROM des ZX81 und einer dazu passenden neuen Tastaturfolie aufrüsten, wodurch die Unterschiede bezüglich Sprachumfang und Gleitkommazahlen verschwanden. Diese Aufrüstung war von Sinclair auch offiziell vorgesehen und das ZX81-ROM war gezielt so programmiert worden, dass es auch im ZX80 funktionierte. Allerdings kennt auch ein so aufgerüsteter ZX80 den SLOW-Modus nicht, der ein stabiles Bild (bei reduzierter Abarbeitungsgeschwindigkeit) darstellen kann. Für den versierten Bastler ist dieser jedoch mit geringem Aufwand und dem Einbau einiger weiterer Chips nachrüstbar, so dass sich der ZX80 zu einem vollwertigen ZX81 aufrüsten lässt. Allerdings hat ein Original-ZX80 wegen der Seltenheit heute einen deutlich höheren Sammlerwert als ein aufgerüsteter.\n\nBeide Rechner kann man rückwirkend eher als Forschungsprojekt ansehen, denn effektiv nutzen ließen sich beide nicht. Der Commodore PET war zu dieser Zeit unangefochten der leistungsstärkste Home-Computer, dem Sinclair keine Konkurrenz bot.\n\n"}
{"id": "58393", "url": "https://de.wikipedia.org/wiki?curid=58393", "title": "Produktionsplanung und -steuerung", "text": "Produktionsplanung und -steuerung\n\nDie Produktionsplanung und -steuerung (PPS) ist ein Teilgebiet der Produktionswirtschaft, das der Produktionstechnik und der Wirtschaftsinformatik besonders nahe steht. Die PPS beschäftigt sich mit der operativen, zeitlichen, mengenmäßigen und wenn nötig auch räumlichen Planung, Steuerung und Kontrolle, damit zusammenhängend auch der Verwaltung aller Vorgänge, die bei der Produktion von Waren und Gütern notwendig sind. Technische Rahmenbedingungen werden in der Arbeitsvorbereitung geplant. Diese unterteilt sich in die beiden Gebiete\n\nDie Produktionsplanung und -steuerung bildet heute nach wie vor den Kern eines jeden Industrieunternehmens. Im Vordergrund steht die Optimierung des gesamten Produktionssystems. Produktionssysteme beschreiben die ganzheitliche Produktionsorganisation und beinhalten die Darstellung aller Konzepte, Methoden und Werkzeuge, die in ihrem Zusammenwirken die Effektivität und Effizienz des gesamten Produktionsablaufes ausmachen.\n\nDie PPS teilt sich auf in die \"Produktionsplanung\", die die Vorgänge mittel- bis kurzfristig vorplant, und die \"Produktionssteuerung\", die anhand dieser Planung die Aufträge freigibt und steuert. Beide Bereiche greifen ineinander und sind insbesondere in kleinen bis mittelgroßen Betrieben meist auch in einem Verantwortungsbereich zusammengefasst.\n\nIn der Literatur herrscht keine Einigkeit über eine einheitliche Benennung und Aufteilung der einzelnen Schritte der PPS. Diese werden oftmals in unterschiedlicher Detaillierung aufgeführt und wurden von einem Autorenteam tabellarisch erfasst, systematisiert und gegenübergestellt.\n\nTeile der PPS sind die Produktionsprogrammplanung, die Materialbedarfsplanung, die Termin- und Kapazitätsplanung, die Auftragsfreigabe und die Auftragsüberwachung.\n\nIn der Regel werden die Prozesse der PPS durch PPS-Systeme unterstützt. Erste Ansätze integrierter Systeme wurden Anfang der 70er Jahre unter anderem von IBM mit COPICS entwickelt.\n\nTraditionelle PPS-Systeme basieren auf einem sukzessiven Planungskonzept. Die Aufgaben der Produktionsplanung und -steuerung werden in Teilprobleme zerlegt, die hintereinander gelöst werden. Jedoch sind die Übergänge zwischen den einzelnen Punkten oftmals fließend.\n\nDie massenhafte Verbreitung technisch komplexer Produkte und stetige Verkürzung der Produktlebenszyklen führen seit einigen Jahren zu einem ständig steigenden Entsorgungsbedarf, dieser führt zu steigender Relevanz der Demontageplanung und -steuerung (DPS). Die DPS ist weitestgehend analog zur PPS konzipiert.\n\nDie Produktionsplanung lässt sich nach Erich Gutenberg unterteilen in\n\n\nIm Rahmen der Produktionsprogrammplanung beschäftigt man sich mit der lang-, mittel- und kurzfristigen Planung, wobei Parameter wie Kapitalbindung, Fristigkeit und Korrigierbarkeit der Fehler entscheidend sind, ob sie einen strategischen oder operativen Charakter tragen. Bei \"langfristiger Planung\" (ab 3 Jahren) ist die Frage über Marktsegmente und entsprechende Produkte zu klären, die den Schwerpunkt unternehmerischer Aktivitäten bilden sollen. \"Mittelfristig\" (Quartals-, Jahresplanung) werden Produktgruppen geplant, so dass \"kurzfristig\" (unterjährig) lediglich über die Menge herzustellender Produkte zu entscheiden ist.\n\nDie Erstellung eines Produktionsprogramms ist bei variantenreichen und komplexen Erzeugnissen ein iterativer Planungsprozess, der sich über einen langen Zeitraum erstreckt. Daraus haben sich spezifische Methoden und Verfahren der Planung von Produktionsprogrammen entwickelt, wie dies bspw. im Automobilbau der Fall ist. Hier wird zunächst der langfristige Absatz je Land oder Absatzmarkt geschätzt, wobei unterschiedliche stochastische Prognoseverfahren eingesetzt werden. Aus dem prognostizierten Absatz wird der absatzorientierte Vertriebsplan erstellt, aus dem der fertigungsorientierte Produktionsplan abgeleitet wird. Vertriebs- und Produktionspläne enthalten noch keine konkreten Produkte, sondern nur aggregierte Produktionsmengen für die verschiedenen Produktarten (z. B. PKW, LKW, Omnibus) und deren Produktklassen bzw. Produktfamilien (z. B. VW Golf, VW Passat, VW Polo usw.). Erst relativ spät werden die Produktionsprogramme erstellt, in denen die echten Fahrzeugbestellungen der Kunden oder Händler stehen.\n\nDer Vertriebsplan ist marktbezogen, während der Produktionsplan werksbezogen ist und zugleich die Produktionskapazitäten beschreibt. Erst im Kurzfristzeitraum werden die Vertriebs- und Produktionsprogramme erstellt. Zuerst wird auf Basis der tatsächlichen Kundenbestellungen das marktorientierte Vertriebsprogramm erstellt, aus dem das Produktionsprogramm abgeleitet wird. Wegen der Variantenvielfalt und aufgrund von Restriktionen (z. B. Produktions- oder Lieferkapazitäten) wird dabei das Produktionsprogramm geglättet, wobei das Ziel der Produktion ist, alle Kundenbestellungen so rechtzeitig fertigzustellen, dass sie dem Vertrieb bzw. dem Kunden vereinbarungsgemäß übergeben werden können. Die meisten Unternehmen streben inzwischen an, möglichst nur noch Produkte nach Kundenauftrag (Build-to-Order) herzustellen und damit dem mass-customization nahezukommen.\nAuf Basis des verbindlichen Produktionsprogramms für Enderzeugnisse (Produkt) können Fertigungsprogramme (Fertigungsauftrag) für die benötigten Herstellteile und die Lieferprogramme (Lieferabruf) für erforderlichen Kaufteile abgeleitet werden. Dazu müssen die Produktionsprogramme über Stücklisten in ihre Komponenten (Teile und Baugruppen) aufgelöst werden.\n\nAusgehend vom Primärbedarf (verkaufsfähige und nachgefragte Enderzeugnisse) wird in der Materialbedarfsplanung ermittelt, welche Mengen an Rohstoff oder Rohteilen, Zwischenprodukten, Einzelteilen und Baugruppen (= \"Sekundärbedarf\"). zur Deckung des Primärbedarfs benötigt werden. Hierzu müssen die Komponenten, aus denen sich das Erzeugnis zusammensetzt, in Stücklisten oder Arbeitsplänen dokumentiert sein. Für alle Komponenten (Einzelteile und Baugruppen), die in der Stückliste vorkommen, sollte es auch eine technische Zeichnung geben, die den aktuellen Konstruktionsstand beschreibt, um die Konsistenz von kaufmännischen, produktionstechnischen und konstruktiven Daten und Prozessen sicherzustellen. Das Erzeugnis wird mit Hilfe dieser Stücklisten und Arbeitspläne \"sukzessiv\" in seine Komponenten zerlegt; dafür gibt es unterschiedliche Verfahren der Stücklistenauflösung, die sich an der Art der Stücklistendarstellung orientieren. Aus der Stücklistenauflösung erhält man zunächst den Bedarf an höheren Baugruppen und Einbauteilen. Mit Hilfe von Dispositionsparametern (z. B. Losgröße, Vorlauf-/Lieferzeit, …) können daraus die Fertigungs- und Lieferprogramme für die Baugruppen abgeleitet werden. Diese werden wiederum mit Hilfe von Stücklisten in kleinere Baugruppen und Einbauteile zerlegt, die wiederum die Grundlage für die nächste Stufe der Materialbedarfsplanung sind usw. usf. Die letzte Stufe der Stückliste sind immer die Einbauteile; diese sind entweder Kaufteile, die von anderen Herstellern beschafft werden müssen, oder Herstellteile, die vom Unternehmen selber hergestellt werden. Für die Fertigung dieser 'Hausteile' müssen die notwendigen Rohteile, Materialien und Rohstoffe eingekauft werden, die in einem Arbeitsplan dokumentiert werden (s. Eigenfertigung oder Fremdbezug).\n\nIn Industriezweigen mit technisch komplexen und variantenreichen Erzeugnissen, wie bspw. in der Automobilindustrie, ist die Ermittlung des Sekundärsbedarfs besonders aufwendig. Sie erfordert zuerst die Auflösung der Produktdefinition aus der individuellen Kundenbestellung mit Hilfe einer Varianten-Stückliste. Aufgrund der Variantenvielfalt wird das Produkt über Merkmale definiert, wobei ein Produktkonfigurator dem Kunden bei der Auswahl seiner gewünschten Merkmale hilft und ihn auf die zulässigen/unzulässigen Merkmalskombinationen hinweist. Anhand der Produktdefinition, die zwingender Bestandteil des Kundenauftrags ist, kann aus einer Komplex- bzw. Maximal-Stückliste eine auftragsbezogene Stückliste für ein einzelnes Produkt generiert werden, die alle benötigten Baugruppen und Teile enthält. Die Zusammenfassung aller so ermittelten singulären Sekundärbedarfe stellt den Ausgangspunkt für die Ermittlung des Nettobedarfs sowie der Auftragsbildung für Kauf- und Herstellteile dar. Dabei müssen verschiedene Restriktionen berücksichtigt werden, durch den der ermittelte Sekundärbedarf zeitlich und mengenmäßig an die vorhandenen Produktions- und Transportbedingungen angepasst wird: Produktions- und Transportkapazitäten, Losgrößen, Arbeitszeiten, Transportzeiten etc.\n\nDiese umfasst die\n\nDie Losgrößenplanung bestimmt wie viele Aufträge eines Produktes zu einem Los zusammengefasst werden können, so dass die Summe aus den entstehenden Produktions-, Lagerhaltungs-, Rüst- und Reinigungskosten minimiert wird. Die optimale Losgröße kann auf Grund von mangelnden Kapazitäten nicht immer realisiert werden und muss zu Lasten der Kosten gesplittet werden.\n\nSobald die zu produzierenden Mengen bekannt sind, wird mit der Terminplanung begonnen. Mittels der Durchlaufterminierung werden früheste und späteste Termine für die Durchführung einzelner Arbeitsschritte geplant.\nAnschließend muss die Frage geklärt werden, ob die erforderlichen Kapazitäten für das Produktionsprogramm vorhanden sind. Dies wird in der Kapazitätsterminierung grob geplant. Bei Kapazitätsengpässen müssen einzelne Arbeitsschritte in andere Zeiträume verschoben werden. Sobald dies geschehen ist, können grob terminierte Aufträge an die Produktionssteuerung weitergegeben werden.\n\nBei der Feinplanung wird festgelegt, welche Maschinen bestimmten Aufträgen zugeordnet werden. Kurzfristige Aufgaben der Produktionssteuerung sind vor allem in Zusammenhang mit kurzfristigen Änderungen in der Auftrags- oder Kapazitätsrealität zu sehen:\nDa die Zusammenhänge mehrdimensional sind, werden die Aufgaben der Produktionssteuerung vermehrt mit entsprechenden Softwaresystemen durchgeführt. Diese erlauben nicht nur, die genannten Aufgaben und Randbedingungen effizient und komfortabel auszuführen, sie ermöglichen zudem eine hohe Flexibilität des Planers und eine hohe Transparenz über den aktuellen Belegungs- und Terminzustand in der Produktion.\n\nWährend manche Systeme Methoden des Operations Research zur Optimierung der Ergebnisse verwenden, zeichnen sich praxisorientierte Systeme durch heuristische Arbeitsweisen unter Berücksichtigung arbeitsvorgangbezogener Prioritätsregeln aus, die dem Verständnis und der Anschauung des Produktionsplaners weitgehend entsprechen.\n\nDas Ergebnis sind Maschinenbelegungspläne und Betriebsmittelzuordnungen von Vorrichtungen, Werkzeugen, NC-Programmen und Zuordnungen von Mitarbeitern.\n\nBei der Planung mehrstufiger Produktionsverfahren (Batchbetrieb) werden nicht nur die Aufträge für Endprodukte (Halbfertigware für das neutrale Lager bzw. Abfüllaufträge) seriell auf unterschiedliche Produktionslinien angeordnet. Vielmehr müssen auch „Unteraufträge“ für die einzelnen Teilfertigungsstufen und deren Abhängigkeiten voneinander berücksichtigt und geplant werden. Speziell beim Batchbetrieb bedarf es der genauen Kenntnis über die Fertigungsanlagen und deren verfahrenstechnischen Möglichkeiten (rühren, heizen, kühlen, destillieren, evakuieren, etc.). So müssen aber auch Minimal- und Maximalmengen pro Charge – Behälterabhängig – berücksichtigt werden. Auch produktspezifische Parameter wie Chargentrennung bei Zwischenlagerungen, Verarbeitbarkeitszeiträume von Zwischen- bzw. Teilprodukten oder Unterbrechungsmöglichkeiten während der Produktion spielen in der Verfahrenstechnik eine große Rolle und gehen in die Reihenfolgeplanung ein. Dabei ist die Betrachtung von materialfolgeabhängigen Reinigungs- und Rüstzeiten aller Anlagenteile selbstverständlich.\n\nPlanungssysteme für komplexe Produktionsverfahren kombinieren aber auch Teil-Fertigungsstufen unterschiedlichen Typs. So liegt bei Produktionsstufen vom Typ „Batch-Charakter“ eine feste Belegungszeit vor wobei sich bei Typ „Konti-Charakter“ die Anlagenbelegungszeit aus einer Reaktions-, Durchlauf- bzw. Förderleistung [z. B.: kg/Std] errechnet.\nEine übersichtliche Darstellung der geplanten und laufenden Fertigung erfolgt in der dynamisierten Plantafel. Hier werden die Sollvorgaben aus dem Fertigungsplan mit dem Status aus der Fertigung verglichen und als Gantt-Diagramm dargestellt. Das integrierte Monitoring meldet dabei eventuelle Verzögerungen und errechnet neue Restlaufzeiten.\n\nDie Produktionssteuerung ist das Veranlassen, Überwachen und Sichern der Durchführung der freigegebenen Aufträge. Für den Bereich der Fertigung (und Montage) spricht man auch von \"Fertigungssteuerung\". Nachdem durch Feinterminierung die Maschinenbelegung festgelegt wurde, werden die Aufträge durch das Bereitstellen von Arbeitsbelegen für den Betrieb veranlasst. Die Überwachung erfolgt durch geeignete zeitnahe Rückmeldesysteme. Das Sichern ist das korrigierende Eingreifen bei Abweichungen, wie Menge, Termine und Qualität.\n\nDie von der Produktionsplanung eingehenden grob terminierten Aufträge werden hier feinterminiert. Einige Konzepte zur Auftragsfreigabe sind Kanban oder die belastungsorientierte Auftragsfreigabe (BoA-Prinzip).\n\nVoraussetzung einer Überwachung der Produktionsabläufe sind Rückmeldungen über den aktuellen Stand der Produktion, kurz eine Betriebsdatenerfassung. Die Rückmeldungen erfolgen entweder über direkte Eingaben an Bildschirmarbeitsplätzen oder über Betriebsdatenerfassungssysteme (BDE-Systeme). Diese Rückmeldedaten sind nicht nur für die Fertigungssteuerung von Bedeutung, sondern auch für die Bruttolohnabrechnung, die Materialbestandsfortschreibung, mitlaufende Kalkulation und Nachkalkulation, die Qualitätskontrolle und die Instandhaltung für die Wartungsplanung.\n\nZur Bewältigung der umfangreichen Aufgaben der operativen Produktionsplanung und -steuerung werden in der betrieblichen Praxis seit langem computergestützte Produktionsplanungs- und -steuerungssysteme (PPS-Systeme) eingesetzt, die nach dem Push-Prinzip arbeiten, da die Produktionsaufträge in den Produktionsprozess hineingedrückt werden. PPS-Systeme greifen regelmäßig auf eine Datenbank des Produktionsbereichs zurück, in der alle Daten über die Erzeugnisse, Produktionsprozesse und Ressourcen abgelegt sind.\n\nAuf der Basis aller Real- oder Planaufträge mit den jeweiligen Fertigstellungsterminen wird eine so genannte Durchlaufterminierung des gesamten Produktionsablaufs durchgeführt. Dies geschieht mit Hilfe festgelegter bzw. ermittelter durchschnittlicher Zeiten für einzelne Bearbeitungsschritte. Alle Aufträge werden somit in ihre Arbeitsschritte unterteilt und für diese Anfangs- und Endzeiten festgelegt sowie die sich daraus ergebenden Anfangs- und Endzeiten für die Aufträge errechnet.\n\nIm Anschluss an die Durchlaufterminierung wird für jede Ressource die resultierende Kapazitätsbelastung ermittelt und der Kapazitätsbedarf dem Kapazitätsangebot gegenübergestellt. Im Rahmen eines Kapazitätsbelastungsausgleichs wird versucht, Überbelastungen gegebenenfalls durch Terminverschiebungen nichtkritischer Aufträge sowie durch Einplanung von Überstunden zu beseitigen. Im Folgenden wird dann eine Auftragsreihenfolge mit genauem Start und Endtermin für jede Arbeitsgruppe oder Maschine festgelegt. Dieses Terminraster dient dann der Steuerung des Informations- und Materialflusses in der Produktion.\n\nBei der Produktion auf Abruf (Pull-Prinzip) beginnt die Produktionssteuerung – im Gegensatz zum Push-System – bei dem Produktionsprogramm für die Produkte, die von einem Kunden oder einem Händler bestellt worden sind. Es werden dann nur noch die benötigten Teile und Baugruppen aus den davorliegenden Fertigungsbereichen oder vom Lieferanten beschafft, die wiederum nur die erforderlichen Teile und Baugruppen beschaffen usw. usf. Das Pull-Prinzip erfordert eine entsprechende (Re)Organisation des gesamten Produktionsprozesses und der Lieferprozesse (s. a. SCM).\n\nGrundsätzliche Idee ist es, dass jede Stelle immer nur so viele Einheiten eines Erzeugnisses herstellt, wie tatsächlich von den nachfolgenden (verbrauchenden) Stellen benötigt werden (Produktion auf Abruf). Dieses Prinzip funktioniert am besten für Standardprodukte mit regelmäßigem Bedarfsverlauf, wenigen Varianten und einer materialflussorientierten Bedarfsmittelanordnung.\n\nEs können nacheinandergeschaltete selbststeuernde Regelkreise (Fertigungsbereiche) installiert werden und die kurzfristige Produktionssteuerung kann von den Mitarbeitern des jeweiligen Fertigungsbereiches übernommen werden.\nJeder Regelkreis besitzt eine Senke, in der Material zum nachgelagerten Regelkreis gebracht und dort verbraucht wird und eine Quelle, die vom vorgelagerten Regelkreis befüllt wird.\n\nEs können auch verkettete Fertigungs- und Belieferungsysteme installiert werden, die sich untereinander nach festen Regeln und Prinzipien versorgen. Folgende Anlieferungsvarianten können unterschieden werden:\n\nEs muss jedoch beachtet werden, dass, je nach Informationsgeschwindigkeit, ein Peitscheneffekt (bullwhip effect) vorkommen kann, da auch hier auf Sicherheit vorbestellt wird um eine Fehlmenge zu vermeiden. Der Bullwhip-Effekt kann durch das Konzept der Fortschrittszahl vermieden werden. Diese wird in der Großserienfertigung, zum Beispiel zwischen den Automobilherstellern und -zulieferern als Verfahren der Produktions- und Beschaffungssteuerung nach dem Pull-Prinzip eingesetzt.\n\n\n"}
{"id": "58756", "url": "https://de.wikipedia.org/wiki?curid=58756", "title": "MOS Technology 6502", "text": "MOS Technology 6502\n\nDer MOS Technology 6502 ist ein 8-Bit-Mikroprozessor von MOS Technology, Inc., der 1975 auf den Markt kam. Aufgrund seiner Unkompliziertheit und vor allem des im Vergleich zu den damals etablierten Intel- und Motorola-Prozessoren sehr niedrigen Preises bei großer Leistungsfähigkeit wurde er in vielen Heimcomputern (z. B. dem Commodore VC-20 und seinem Vorgänger PET 2001, dem Atari 800, Apple I, Apple II und BBC Micro), zahlreichen Schachcomputern (z. B. dem Mephisto Polgar mit 5 oder 10 MHz), im weltweit ersten Skatcomputer Skat Champion und vielen Peripheriegeräten verbaut. Der Prozessor wurde unter der Leitung von Chuck Peddle entwickelt.\n\nAuch andere Mitglieder dieser Prozessorfamilie waren sehr erfolgreich, so der 6510, ein Bestandteil des Commodore 64, und der 6507 in den Atari-Spielkonsolen. Hauptkonkurrent war damals der Zilog Z80, der z. B. in den Sinclair- und Schneider-Computern sowie in vielen CP/M-Rechnern zu finden war.\n\nDas Design des 6502 wurde an das des Motorola 6800 angelehnt (nicht zu verwechseln mit dem jüngeren Motorola 68000). Der Befehlssatz ist ähnlich, aber statt eines 16-Bit-Index-Registers werden zwei 8-Bit-Index-Register verwendet, deren Wert auf eine im Speicher (auch im Programmcode) vorgegebene 16-Bit-Adresse addiert wird. Deswegen haben 6502-Programme einen anderen Stil: Für Arrays, die größer als 256 Bytes sind, muss ein zusätzlicher Programmieraufwand betrieben werden. Andererseits geht die Manipulation von 8-Bit-Werten auf einem 8-Bit-Mikroprozessor erheblich schneller vonstatten. Anders als 16-Bit-Register des 6800 oder etwa Z80, welche eine absolute Adresse darstellen, stellen die 8-Bit-Register des 6502 lediglich einen Array-Index dar (im engen Wertebereich von 0 bis 255), und in manchen Fällen kann ein einziger 8-Bit-Increment- oder -Decrement-Befehl sowohl das Zählen der Adressen als auch das Setzen der Bedingung für den Schleifenabbruch bei index=0 leisten, was zu kompakten und schnellen Schleifen führt. Der einfacher gehaltene Befehlssatz führt allerdings dazu, dass 6502-Programme im Normalfall deutlich mehr Speicher benötigen als das Gleiche leistende 6800- oder Z80-Programme; zudem sind standardkonforme Compiler für höhere Programmiersprachen wie etwa Pascal oder C auf dem 6502 deutlich schwieriger implementierbar und erzeugen langsameren Code als auf anderen 8-Bit-Prozessoren. Die Ursache hierfür ist vor allem die auf 256 Bytes beschränkte Größe des Hardware-Stapelspeichers (s. u.) im 6502, so dass der für die meisten modernen Hochsprachen nötige große Stapelspeicher per Software nachgebildet werden muss. Um die Chipfläche klein zu halten, besitzt der 6502 des Weiteren nur einen Akkumulator im Gegensatz zu den zwei Akkumulatoren A und B im 6800.\n\nDer 64 KB große Adressraum des 6502 teilt sich in mehrere Bereiche auf, die sich vor allem an den \"Page-Grenzen\" orientieren, wo das \"High-Byte\" der 16-Bit-Adresse seinen Wert wechselt:\n\nTechnisch war der 6502 durchaus innovativ:\n\nVarianten des 6502-Prozessors werden heute (Stand 2008) unter anderen in folgenden Geräten eingesetzt:\nAls Cross-Compiler wird häufig der cc65 eingesetzt.\n\n\n\nBeispiele von Maschinenbefehlen, bzw. deren Opcode und \"Mnemonics\" in Verbindung mit verschiedenen Adressierungsarten.\n\nIn Assembler-Programmtexten des 6502 haben das Dollar- und Nummern-Zeichen sowie die Klammern folgende Bedeutung:\n\nEs gibt von verschiedenen Herstellern CMOS-Versionen des 6502, die einen erweiterten Befehlssatz bzw. weitere Adressierungsarten haben. Durch diese Änderungen dürfte es auch zu mitunter gravierenden Änderungen bei den im Folgenden behandelten undokumentierten Opcodes kommen.\n\nDer 6502 ist bekannt für eine ganze Reihe von Befehlen, die nicht in der offiziellen Dokumentation stehen, aber dennoch existieren und funktionieren. Umgangssprachlich nannte man solche Befehle \"„illegale Opcodes“.\" Von den prinzipiell 256 möglichen Opcodes sind nur 151 tatsächliche Befehle. Unter den verbleibenden 105 nicht dokumentierten Codes gibt es viele, die Funktionen haben, und zwar durchaus nützliche. Einige Assembler unterstützen solche Befehle, es gibt jedoch keine einheitliche Vorschrift ihrer Benennung mit Mnemonics. Ein Beispiel:\n\nMit vielen undokumentierten Opcodes lässt sich die Datenverarbeitung beschleunigen, da sie in wenigen Taktzyklen Funktionen erledigen, die sonst nur mit mehreren, aufeinanderfolgenden Befehlen möglich sind, was insgesamt erheblich mehr Taktzyklen verbraucht. Dies birgt allerdings das Risiko, dass solche Befehle nicht auf allen produzierten CPUs gleichermaßen funktionieren oder ein mögliches Nachfolgemodell diese Befehle gar nicht beherrscht. Dadurch würde das Programm seine Funktion nicht mehr korrekt oder gar nicht erfüllen.\n\nWeitere inoffizielle Funktionen des 6502 sind (Mnemonics nur inoffiziell):\n\n\n\n\n\n\n"}
{"id": "59001", "url": "https://de.wikipedia.org/wiki?curid=59001", "title": "Pixar", "text": "Pixar\n\nDie Pixar Animation Studios sind ein auf Computeranimationen und CGI spezialisiertes Unternehmen in Emeryville, Kalifornien. Pixar gehört seit 2006 zur Abteilung Walt Disney Motion Pictures Group der Walt Disney Company. Die offizielle Abkürzung seit 2006 lautet \"Disney·Pixar\", bis 2006 und umgangssprachlich noch immer nur \"Pixar\". Seinen Erfolg verdankt es seinen animierten Filmen wie \"Toy Story\" oder \"Findet Nemo\".\n\nPixar hat insgesamt zwölf Oscars erhalten. Der Name des Unternehmens „Pixar“ ist mutmaßlich ein Kunstwort aus der Computersprache und eine Kombination aus den Begriffen \"Pixer\" (ein frei erfundenes, spanisch-klingendes Wort des Co-Gründers Alvy Ray Smith) und \"Radar\" (ein technisch-assoziertes Wort).\n\nDie Ursprünge der \"Pixar Animation Studios\" gehen auf einen 1979 gegründeten Teil der Lucasfilm Graphics Groups zurück. 1984 stieß Animator John Lasseter zum Team, das bis dahin nur aus Computer-Spezialisten bestand. Im selben Jahr wurde auf der SIGGRAPH der erste Kurzfilm vorgestellt: \"André and Wally B.\"\n\nNach der Arbeit an Spezialeffekten für einige von Lucasfilm oder Industrial Light and Magic produzierte Filme wurde die Abteilung 1986 von Apple-Mitgründer Steve Jobs, kurz nachdem er sein Unternehmen (Apple) verlassen hatte, für 5 Millionen US-Dollar gekauft und in Pixar umbenannt. Jobs investierte weitere 5 Millionen Dollar in das jetzt selbstständige Unternehmen und übernahm die Position des CEO (Geschäftsführer), Vizepräsident wurde Edwin Catmull (der zuvor auch schon Vizepräsident der Abteilung innerhalb von Lucasfilm war). Im selben Jahr wurde an der SIGGRAPH der Kurzfilm \"Luxo Jr.\" als erste Pixar-Produktion vorgestellt. Ein Jahr später wurde der Film mit dem Prix Ars Electronica ausgezeichnet und erhielt eine Oscar-Nominierung.\n\nDas ursprüngliche Kerngeschäft von Pixar war der Pixar Image Computer, ein High-End-Rechner für Computer Designs. Genutzt wurde er vor allem von staatlichen Institutionen und der Medizinbranche. Die Kurzfilme wurden in erster Linie für Werbezwecke erstellt. Entsprechend hatte die Abteilung um John Lasseter eine Außenseiterrolle innerhalb des Unternehmens. Doch durch den eher mäßigen Erfolg der eigenen Hardware und wegen der guten Zukunftsaussichten für Computeranimationen wurde das Kerngeschäft aufgelöst und aus der Animations-Abteilung die \"Pixar Animation Studios\" gegründet.\n\nAb 1989 wurde die von Pixar entwickelte Rendering-Software RenderMan als Produkt angeboten. Heute gilt sie als Industriestandard in der 3D-Computergrafik. Im selben Jahr erstellte das Unternehmen zum ersten Mal einen Werbefilm.\n\nDas Unternehmenslogo ziert eine Schreibtischlampe, die ihren Achtungserfolg 1986 in einem der ersten prämierten Kurzfilme von Pixar hatte (\"Die kleine Lampe\") und seitdem im Pixar-Filmvorspann agiert.\n\nIn den folgenden Jahren produzierte Pixar viele Werbefilme für verschiedenste Unternehmen. Außerdem entwarfen sie Logos für Paramount und IBM. 1991 schloss Pixar Animation einen Vertrag mit den Walt Disney Studios über 26 Millionen US-Dollar ab, in dem die Produktion dreier animierter Spielfilme vereinbart wurde.\n\n1995 erschien mit \"Toy Story\" die erste Co-Produktion mit Disney. Der Film wurde ein durchschlagender Erfolg und spielte weltweit rund 360 Millionen US-Dollar ein. Eine Woche nach dem Start von Toy Story ging Pixar an die Börse. Die Aktie verdoppelte ihren Wert und Steve Jobs wurde zum Milliardär.\n\n1997 wurde der Vertrag mit Disney auf fünf Filme erweitert. Ein Jahr später landete Pixar mit \"Das große Krabbeln\" einen weiteren Erfolg, 1999 folgte \"Toy Story 2\". Die Qualität der Animation steigerte sich von Film zu Film, 2001 erschien \"Die Monster AG\" und 2003 \"Findet Nemo\", der Film war bis \"Toy Story 3\" der finanziell erfolgreichste Film von Pixar. Die ersten fünf Filme spielten zusammen 2,5 Milliarden US-Dollar ein. Aufgrund der ungleichen Rechteverteilung entstand jedoch ein Streit zwischen Pixar und Disney. Pixar war für Ideen und Produktion verantwortlich, während Disney Verleih und Marketing oblag. Die Kosten für Filme wurden dabei jeweils zu Hälfte aufgeteilt. Die Rechte an Geschichten und Fortsetzungen hielt jedoch Disney Pictures.\nMit \"Die Unglaublichen\" entstand 2004 Pixars sechster Film, doch bereits im Januar 2004 kündigte Pixar an, die Verträge mit Disney nicht zu verlängern. So sah es danach aus, dass mit Beendigung des letzten gemeinsam angekündigten Projekts \"Cars\" die Zusammenarbeit beendet sein sollte. Der Wechsel in der Führungsriege der Disney Company im Oktober 2005, bei dem Michael Eisner, der bisherige Leiter von Disney, von Robert A. Iger abgelöst wurde, veränderte die Situation allerdings.\n\nAm 24. Januar 2006 gab der Medien- und Entertainment-Konzern Walt Disney Company nach US-Börsenschluss bekannt, dass er die Pixar Inc. für 7,4 Milliarden US-Dollar übernehmen werde. Als Teil der Übernahme wurde Pixar-CEO Steve Jobs als Mitglied in den Verwaltungsrat von Disney aufgenommen. Zudem wurde Jobs größter Einzelaktionär bei Disney. Im Herbst des Jahres konnte \"Cars\" veröffentlicht werden.\n\nDer achte Film von Pixar ist \"Ratatouille\", der am 29. Juni 2007 in den USA und am 3. Oktober 2007 in Deutschland erschien. Regie führte Brad Bird, Regisseur des Pixar-Films \"Die Unglaublichen\", zusammen mit dem Newcomer Jan Pinkava. Am 27. Juni 2008 kam der neunte Pixar-Film \"WALL·E\" ins Kino. Im deutschsprachigen Raum wurde der Film, bei dem Andrew Stanton Regie führte, am 25. September 2008 veröffentlicht.\n\nAm 29. Mai 2009 brachte Pixar den Animationsfilm \"Oben\" in Amerika ins Kino, der von einem Rentner handelt, welcher mit einem kleinen Jungen Abenteuer erlebt. Die deutsche Veröffentlichung folgte am 17. September 2009.\n\nBis jetzt waren alle Pixar-Filme an den Kinokassen erfolgreich und kamen auch bei den Kritikern gut an. \"Toy Story 3\" ist der erfolgreichste Pixar-Film und wurde der weltweit finanziell erfolgreichste Animationsfilm aller Zeiten, bis \"Die Eiskönigin – Völlig unverfroren\" ihn im März 2014 überbot.\n\nDer dritte Teil von Toy Story erschien in Deutschland am 29. Juli 2010 in 3D, nachdem der Film in den USA am 18. Juni 2010 startete und das beste Startwochenende-Einspielergebnis aller Pixar-Filme erreichte. Vorher wurden die beiden bisherigen Toy-Story-Filme in einer neuen 3D-Version wieder aufgeführt. Ein weiteres Projekt des Animationsstudios ist die Fortsetzung von \"Cars\" namens \"Cars 2\". Der Film, der unter der Regie von John Lasseter entstand, kam am 28. Juni 2011 in die deutschen Kinos.\nAm 20. Juni 2013 ist \"Die Monster Uni\" \"(Monsters University)\" als Prequel zu \"Die Monster AG\" im deutschen Raum im Kino gestartet.\n\nIm November 2015 wurde der Film \"Arlo & Spot\" (Originaltitel \"The Good Dinosaur)\" veröffentlicht, in dem das Thema aufgegriffen wird, wie die Menschheit leben würde, wenn die Dinosaurier nie ausgestorben wären. Des Weiteren erschien im selben Jahr \"Alles steht Kopf\" (Originaltitel \"Inside Out\"). Hier wird der Zuschauer ins Innere des Menschen geführt. Mit \"Coco\" kam 2017 ein Animationsfilm in die Kinos, der sich mit dem Tag der Toten, einem der größten mexikanischen Feiertage, beschäftigt. \n\nAuf der Disney-Expo D23 wurden außerdem die Fortsetzungen \"Finding Dory\", \"\", \"\" und \"The Incredibles 2\" offiziell angekündigt. Erstere kam in Deutschland im September 2016 unter dem Titel \"Findet Dorie\" und letztere als \"Die Unglaublichen 2\" in die Kinos.\n\nZwischenzeitlich scheint sich der Optionsskandal um Apples CEO Steve Jobs' rückdatierte Aktien auch auf das Trickfilmstudio Pixar und den Creative Officer der Disney-Animationsstudios John Lasseter sowie den ehemaligen Präsidenten von Pixar Edwin Catmull auszuweiten. John Lasseter hatte zahlreiche Filme produziert und mitfinanziert und war nach der Übernahme durch Disney zum kreativen Chef der Studios ernannt worden. Er soll im Jahr 2001 im Rahmen eines Anstellungsvertrages ein Aktienoptionspaket von Pixar erhalten haben, welches mit dem niedrigsten Kurs des Vorjahres bepreist gewesen sein soll. Der Vertrag soll von Steve Jobs drei Monate später unterzeichnet worden sein. Lasseter soll eine Option über eine Million Pixar-Aktien im Wert zu 26,50 US-Dollar pro Stück erhalten haben. 2007 kam eine hausinterne Untersuchung bei Pixar zum Schluss, dass Aktienoptionen rückdatiert waren. Steve Jobs wurde durch das Gutachten entlastet.\n\n\n\n\nBei Pixar hat sich die Tradition entwickelt, Running Gags in ihre Produktionen einzubauen. In bisher jedem Animationsfilm taucht ein „Pizza-Planet“-Truck auf, der ursprünglich eine Rolle in \"Toy Story\" hatte. In darauf folgenden Filmen kann der Truck immer wieder im Hintergrund gesichtet werden. Einzige Ausnahme bislang: In \"Die Unglaublichen\" ist der Truck nicht enthalten. Auch die Nummer A113 kommt in jedem der Filme vor. Dies ist eine Anspielung auf die Nummer des ehemaligen CalArts-Klassenzimmers von Brad Bird und John Lasseter. Auch Pixars „Glücksbringer“ John Ratzenberger hat bisher noch immer eine kleine Nebenrolle bekommen. Manche Figuren aus Filmen spielen Nebenrollen in anderen, so ist etwa Bomb Voyage aus \"Die Unglaublichen\" in \"Ratatouille\" in einer kleinen Szene als Pantomime zu sehen. Ein weiteres wiederkehrendes Motiv ist ein Spielball mit einem roten Stern auf gelbem Untergrund, wie er im ersten Pixar-Film \"Die kleine Lampe\" zu sehen war. Bei \"Findet Nemo\" liegt eine Buzz-Lightyear-Spielfigur im Wartezimmer des Zahnarztes. Bei \"Monster AG\" liegt eine Nemo-Spielfigur in einem der Kinderzimmer. Im Kurzfilm \"Dein Freund, die Ratte\" fährt WALL·E einen Marsbus.\n\n\nFür ihre Animationsfilme ist Pixar mehrfach mit dem Oscar ausgezeichnet worden. Seit Einführung der Kategorie Bester animierter Spielfilm 2002 wurden alle ab 2001 erschienenen Pixar-Filme mit Ausnahme von \"Cars 2\", \"Die Monster Uni\", \"Arlo & Spot\" und \"Findet Dorie\" für den Preis nominiert und neun wurden ausgezeichnet (\"Die Monster AG\" verlor 2002 gegen \"Shrek – Der tollkühne Held, Cars\" 2006 gegen \"Happy Feet\" und \"Die Unglaublichen 2\" 2019 gegen \"\").\n\n1989: Tin Toy\n\n1996: Toy Story\n\n1998: Geri's Game\n\n2002: Die Monster AG\n\n2002: Der Vogelschreck\n\n2004: Findet Nemo\n\n2005: Die Unglaublichen\n\n2008: Ratatouille\n\n2009: WALL·E\n\n2010: Oben\n\n2011: Toy Story 3\n\n2013: Merida – Legende der Highlands\n\n2016: Alles steht Kopf\n\n2017: Piper\n\n2018: Coco\n\n\n2019: Bao\n\n\n1987: Luxo Jr.\n\n1996: Toy Story\n\n1999: Das große Krabbeln\n\n2000: Toy Story 2\n\n2002: Die Monster AG\n\n2003: Mike's neues Auto\n\n2004: Findet Nemo\n\n2004: Ein Schaf ist von der Wolle\n\n2005: Die Unglaublichen\n\n2006: Die Ein-Mann-Band\n\n2006: Lifted\n\n2007: Cars\n\n2008: Ratatouille\n\n2009: WALL·E\n\n2010: Oben\n\n2011: Toy Story 3\n\n2011: Day & Night\n\n2008: WALL·E – Der Letzte räumt die Erde auf\n\n\n"}
{"id": "59770", "url": "https://de.wikipedia.org/wiki?curid=59770", "title": "Newton (PDA)", "text": "Newton (PDA)\n\nDer Apple Newton war eine Produktreihe von PDAs der Firma Apple und anderen Firmen, die 1993 vorgestellt und deren Produktion 1998 bei der Neustrukturierung von Apple Computer unter Steve Jobs eingestellt wurde. Eigentlich ist Newton die Bezeichnung des Betriebssystems; die PDAs wurden größtenteils unter dem Namen \"MessagePad\" vermarktet.\n\nDer Newton zeichnete sich durch eine lernfähige Handschrifterkennung aus. Durch einen berührungsempfindlichen Bildschirm konnten direkt auf den Bildschirm geschriebene Zeichen und Worte erkannt werden. Die erste Version, \"Calligrapher\", wurde maßgeblich von der Moskauer Entwicklungsmannschaft Paragraph International entwickelt. Sie war für die Erkennung von Schreibschrift ausgerichtet. Aufgrund unzureichender Prozessorleistung funktionierte dies bei den ersten Modellen nur eingeschränkt. Dies besserte sich mit der neuen Version 2.0 des Betriebssystems. Hier kam zusätzlich die zweite Version der Handschrifterkennung, \"Rosetta\", zum Einsatz. Sie diente zusätzlich zur Erkennung von Druckschrift und wurde von Apple entwickelt. Das OS 2.0 wurde 1996 auf dem MessagePad 120 eingeführt.\n\nDer erste Newton, das OriginalMessagePad, besaß einen ARM 610 Prozessor mit 20 MHz, 4 MB ROM, 640 KB RAM sowie einen einfarbigen Flüssigkristallbildschirm mit einer Auflösung von 336 × 320 Punkten. Das MessagePad war etwas kleiner als ein DIN-A5-Blatt und wog 400 g. An dem vorletzten Modell, dem MessagePad 2000, wurde durch den StrongARM-110-Prozessor mit 162 MHz Taktfrequenz die Leistung stark angehoben. Der Bildschirm dieser Modelle besaß ein LCD mit 480 × 320 Punkten Auflösung und 16 Graustufen Farbtiefe. Das RAM wurde auf 4 MB, bzw. beim letzten Modell, dem MessagePad 2100, auf 8 MB erweitert. Für Erweiterungen standen zwei PCMCIA-Steckplätze zur Verfügung.\n\nDer Newton verwendete das Newton OS als Betriebssystem, das erste vollständig in C++ geschriebene Betriebssystem, das auf geringen Speicher- und Prozessorverbrauch optimiert war. Zur Programmierung des Newton wurde das auf der Programmiersprache Self basierende NewtonScript verwendet.\n\nDer Newton brachte bahnbrechende neue Funktionen in die Computerwelt, die langsam auch auf den Desktop übersprangen:\n\n\nEs gab folgende Modelle des Newton:\n\nDas erste Newton MessagePad, von den Anwendern kurz OMP (Original MessagePad) genannt, wurde erstmals im August 1993 auf der MAC World in Boston vorgestellt. Der OMP war ausschließlich mit einem englischen Betriebssystem erhältlich und wurde mit dem Batch 1.02 des NOS ausgeliefert. Das Betriebssystem hatte noch erhebliche Schwachstellen, teilweise funktionierte der OMP ganz einfach nicht und verweigerte den Dienst. Apple stellte innerhalb kurzer Zeit die Versionen 1.03, 1.04 und 1.05 zur Verfügung. Ab November 1993 wurde der OMP bereits mit den Versionen 1.10 und 1.11 ausgeliefert.\n\nDie Leistung des Prozessors mit 20 MHz konnte nicht überzeugen und war für die Schrifterkennung nicht ausreichend. Als Stromversorgung wurden für den OMP 4 AAA-Batterien gewählt und trotz der aufwendigen Trimmung des Prozessors auf Energiesparen, wurde die versprochene Arbeitszeit unterschritten.\n\nDas Newton MessagePad OMP hatte nicht viele Anschlüsse und Schnittstellen. Es besaß „nur“ einen Stromanschluss, um die Akkus zu laden, einen Anschluss, um das Newton MessagePad mit einem PC oder Mac zu verbinden und einen Kartenschacht, in den man u. a. Speicherkarten stecken konnte.\n\nDas Modell war das erste seiner Art und in mancher Hinsicht wegweisend. Jedoch war es bei der Einführung offenbar noch nicht marktreif; der Preis war zu hoch, und Apple verärgerte viele Kunden mit einem schlecht funktionierenden Gerät.\n\nDer Newton MessagePad 100 wurde bereits mit der NOS Version 1.3 ausgeliefert und war baugleich mit dem OMP. Bei der mit deutschem NOS ausgelieferten Version handelte es sich um Original-Message-Pads, welche durch ROM-Tausch auf NOS 1.3 gebracht wurden.\n\nApple hatte das MessagePad 110 schlanker gemacht, der Stift war nun im Gehäuse versenkbar. Die Breite wurde auf 106 Millimeter reduziert; dadurch lag der Newton ergonomischer in der Hand. Auch kamen nunmehr die größeren AA-Batterien zum Einsatz. Ausgeliefert mit der ROM-Version 1.2 und einem Softwarepatch auf 1.3 war er bereits mit 1 MByte RAM ausgestattet. Das NOS funktionierte ab dieser Version tadellos. Das Newton MessagePad 110 war der erste PDA, der mit einer vertikalen Schutzklappe für das berührungsempfindliche Display ausgeliefert wurde.\n\nWurde ab 1994 mit dem NOS 1.3 bzw. bereits mit NOS 2.0 ausgeliefert. In der Version mit NOS 2.0 wurde der Speicher auf 2 MB RAM aufgestockt.\n\nBis auf den Speicher und die zusätzliche Hintergrundbeleuchtung des Displays baugleich mit MessagePad 120. Ausgeliefert wurde das MessagePad 130 1996 mit 2,4 MB RAM und der Version 2.1 des NOS.\n\nDas Newton MessagePad 2000 wurde mit dem StrongARM 110-Prozessor mit 161,9 MHz ausgeliefert. Dies bedeutete eine Steigerung der Taktfrequenz um über 700 Prozent und stellte somit einen erheblichen Fortschritt gegenüber den Vorgängern dar. Damit erreichte der Newton bei der Taktfrequenz ein Niveau, das erst Jahre nach seinem Produktionsende in anderen PDAs üblich wurde. Das MessagePad 2000 war nur mit englischem NOS 2.1 erhältlich.\n\nDer Bildschirm hatte eine Diagonale von 5,9 Zoll bei einer Auflösung von 480 × 320 Punkten, 100 DPI und 16 Graustufen. Seine Maße waren: Höhe: 210,3 mm; Breite: 118,7 mm und Tiefe 27,5 mm. Er wog mit Batterien 640 Gramm.\n\nDer letzte Newton der Baureihe, der auch mit deutschem NOS 2.1 und mit 8 MB RAM und 2 PCMCIA-Slots Typ II ausgeliefert wurde.\n\nDer eMate unterschied sich von allen anderen Newton-Modellen durch die eingebaute Tastatur und entsprach von der Bauform her einem sehr kleinen Laptop bzw. Netbook.\n\nAls Prozessor wurde ein ARM 710a Prozessor mit einem Takt von 25 MHz verwendet. Die Bildschirmauflösung betrug wie bei den 2000er Newtons 480 × 320 Punkte mit 16 Graustufen. Ausgerüstet war er mit 3 MB RAM (1 MB DRAM und 2 MB Flash-Speicher); die Größe des ROM betrug 8 MB. Als weiteres Ausstattungsmerkmal besaß er einen PCMCIA-Slot vom Typ III. Seine Maße waren 305 × 290 × 53 Millimeter; er wog 1800 Gramm.\n\nNeben den Modellen von Apple wurde die Technologie auch an andere Hersteller lizenziert, die eigene Produkte auf den Markt brachten, so zum Beispiel Sharp mit dem ExpertPad und Siemens mit dem Notephone.\n\nNach der Berufung von Steve Jobs als CEO bei Apple wurde der Newton im Jahre 1998 eingestellt. Größere Bekanntheit erlangte Apples Newton 1995 durch seine Verwendung in dem Kinofilm mit Steven Seagal. Noch einige Jahre nach der Einstellung bestand eine Fangemeinde, die insbesondere die leistungsfähigeren späteren Modelle im Alltagseinsatz nutzte. Über Zusatzsoftware und Patches war es sogar möglich, einen Newton zusammen mit neuesten Mobiltelefonen über IrDA- oder Bluetooth-Verbindungen zu verwenden oder mit den Kalender- und Kontaktdaten von Mac OS X per Wireless LAN zu synchronisieren. Per Outlook oder Lotus Notes ist der Newton auch mit Windows synchronisierbar. Auch die Verwendung von neueren Speicherkarten mit ATA-Interface, etwa Compact-Flash-Speicherkarten wird so möglich.\n\nEine Wiederbelebung bei Apple erfuhr das PDA-Konzept durch die Präsentation des iPhone auf der Macworld Conference & Expo 2007 sowie des iPad auf einer speziellen Keynote 2010. Beide Geräte haben aber den Unterschied, dass diese nicht mit einem Stift, sondern nur mit den Fingern über einen Touchscreen zu bedienen sind. Erst mit dem iPad Pro im Jahre 2015 wurde optional die Bedienung per Stift möglich, mittels des separat zu erwerbenden Zubehörs Apple Pencil.\n\n"}
{"id": "60456", "url": "https://de.wikipedia.org/wiki?curid=60456", "title": "Adabas", "text": "Adabas\n\nAdabas (Adaptable Database System) ist ein Datenbankmanagementsystem der Darmstädter Software AG.\n\nEs wurde von Peter Schnell zunächst für die Großrechner von IBM – unter z/VSE und MVS – und Siemens – unter BS1000 und BS2000 – entwickelt. Die Erstinstallation fand im Jahre 1971 statt. Adabas wurde im Laufe der Jahre auch für andere Plattformen (wie VMS von DEC, verschiedene Unix-Systeme, Linux, Windows) verfügbar. Adabas wird häufig als Hochleistungsdatenbank bis weit in den Tera-Bereich eingesetzt; genauso aber auch als Abteilungs- oder sogar Desktop-Datenbank für Anwendungen der Software AG, insbesondere wenn es sich dabei um Anwendungen in Natural handelt. \n\nÜblicherweise wird Adabas nicht von Privatanwendern eingesetzt, da die zugrunde liegenden Konzepte und auch die Zugriffsmethoden einen nicht unerheblichen Lernaufwand bedingen.\n\nAdabas basiert auf dem \"NF²\"-Datenbankmodell. NF² steht dabei für NFNF = \"non first normal form\". Diese Bezeichnung bezieht sich darauf, dass Adabas die Daten nach dem Sprachgebrauch der relationalen Theorie nicht in der ersten Normalform abspeichert. Stattdessen werden die Daten in einem gepackten Format, das auch Periodengruppen und multiple Felder erlaubt, gespeichert. \n\nZusatzfeatures öffnen Adabas auch Anwendern, welche Datenbankinhalte über SQL (Adabas SQL Gateway) oder eine Serviceorientierte Architektur (Adabas SOA Gateway) nutzbar machen wollen.\n\nNeben Adabas existiert auch eine relationale Datenbank namens \"Adabas D\", die jedoch bis auf den Namen nichts mit Adabas gemeinsam hat. Das \"D\" steht hier für \"Department\". Zur besseren Unterscheidung wurde Adabas von Software AG zeitweise auch als \"Adabas C\" bezeichnet. Dabei stand das \"C\" für \"Central\". Diese Bezeichnung wurde aber in den letzten Jahren wieder aufgegeben. \n\nDie aktuelle Version auf dem Markt ist Adabas 2006 mit Datenzugriff in Echtzeit von jedem Standort aus und Unterstützung für Mainframe-, Linux-, Unix- und Windows-Plattformen.\n\n"}
{"id": "60468", "url": "https://de.wikipedia.org/wiki?curid=60468", "title": "Mission Control (Apple)", "text": "Mission Control (Apple)\n\nMission Control ist eine Funktion zur Übersicht von offenen Fenstern und Vollbild-Apps von OS X, die mit Version 10.3 „Panther“ eingeführt wurde und bis zur Version 10.6 „Exposé“ hieß.\n\nNach Betätigung einer Tastenkombination (standardmäßig F3, ehemals F9) werden entweder alle offenen Fenster aller aktiven Programme oder nur die Fenster des aktuellen Programms im Vordergrund verkleinert und wie Spielkarten auf dem Desktop dargestellt. Der Anwender kann dann das Fenster, mit dem er als Nächstes arbeiten will, auswählen und damit weiterarbeiten. Weiterhin können alle Fenster auf einmal aus dem sichtbaren Bereich verschoben werden, um einen ungehinderten Zugriff auf den Desktophintergrund zu ermöglichen. Mission Control wird außerdem benötigt für die Verwaltung der virtuellen Schreibtische in OS X, wodurch es dem Benutzer ermöglicht wird, die Schreibtische zu verschieben, zu löschen, oder neue zu erstellen.\n\nDes Weiteren kann man Mission Control auch vom Launchpad aus wie ein normales Programm starten, oder durch Bewegen des Mauszeigers in eine Ecke des Bildschirms (sogenannte „aktive Ecken“) aktivieren.\n\nAuf Rechnern, die Quartz Extreme unterstützen, wird die Darstellung der Fenster in Echtzeit aktualisiert. Mission Control funktioniert jedoch auch auf Rechnern ohne Quartz-Extreme-Unterstützung.\n\nMission Control wurde für andere Betriebssysteme nachempfunden. So gibt es für Windows XP iEx und für Unix Skippy (wird nicht weiterentwickelt), Compiz. Windows Vista enthält eine ähnliche Funktion, die die Fenster dreidimensional in einer Reihe hintereinander anordnet. Die Gnome Shell bietet mit dem sogenannten Aktivitäten-Modus eine vergleichbare Funktion.\n"}
{"id": "60544", "url": "https://de.wikipedia.org/wiki?curid=60544", "title": "Free Pascal", "text": "Free Pascal\n\nFree Pascal oder Free Pascal Compiler (abgekürzt FPC) ist ein freier Compiler für die Programmiersprachen Pascal und Object Pascal.\n\nDer Pascal-Compiler zeichnet sich besonders durch folgende Eigenschaften aus:\n\nDa sich Pascal durch eine sehr weitreichende Trennung zwischen Hardware/Betriebssystem und Programmcode auszeichnet, sind FPC-Programme in hohem Maße portierbar (in aller Regel genügt einfaches Kompilieren auf dem Zielsystem, um aus einem Windows-Programm ein Linux- oder DOS-Programm zu machen). Mit Hilfe von InstantFPC ist es sogar möglich, Pascal-Programme als bedarfsweise kompilierte Unix- und CGI-Skripte auszuführen.\n\nFür die Nutzung stehen verschiedene (ebenfalls frei verfügbare) integrierte Entwicklungsumgebungen (IDE) zur Verfügung:\n\nFree Pascal beherrscht den De-facto-Standard, die Borland-Pascal-Dialekte. In der Version 2.x ist Free Pascal nahezu Delphi-7-kompatibel. Begrenzt unterstützt werden außerdem die ANSI/ISO-Pascal-Dialekte und Apple Pascal. Außerdem existiert ein OBJFPC-Modus, der umfangreiche Objekt-Pascal-Erweiterungen und zahlreiche Schnittstellen z. B. zu Datenbanken aktiviert.\nDie verschiedenen Dialekte können sowohl über Kommandozeilenschalter als auch im Quelltext durch $MODE ausgewählt werden. Derzeit sind folgende Einstellungen möglich:\n\nAuf der FPC-Website befinden sich direkt installierbare Versionen für die meisten Betriebssysteme. Die Installationen z. B. unter Windows lassen sich jederzeit problemlos und vollständig wieder entfernen. Es existieren u. a. folgende Komponenten/Erweiterungen (ein aktueller Überblick findet sich auf der FPC-Website):\n\nFree Pascal entstand aus dem Wunsch heraus, 32-Bit-Pascalprogramme einfach und portierbar erstellen zu können. Damals konzentrierte Borland seine Entwicklung ganz auf Windows (das spätere Delphi) und lehnte die Weiterentwicklung des bis heute an Qualität und Geschwindigkeit immer noch vorbildlichen, aber fast komplett in Assembler programmierten Borland-Pascal-Compilers ab (selbst einfache Fehler wie der seit BP7 auftretende „Runtime Error 200“ – ein Überlauf in einer Schleife, welche die Geschwindigkeit der Hardware bestimmen sollte – wurden nicht mehr beseitigt). Der Student Florian Paul Klämpfl entwickelte daraufhin einen eigenen 32-Bit-Pascal-Compiler. Er nannte ihn zunächst FPK, nach den Initialen seines Namens.\n\nErste Arbeiten an Free Pascal begannen im Jahre 1993. Die erste Version war noch in Turbo Pascal ausschließlich für den DOS-Extender go32v1 geschrieben, was jedoch bereits ein enormer Fortschritt war, da jetzt bis zu 2 GB große Datenbereiche (z. B. Felder für Fast-Fourier-Analysen oder Digitale Filter) äußerst einfach verwaltet werden konnten. Bald konnte Free Pascal sich selbst kompilieren, so dass es zu einem 32-Bit-Programm wurde. Die Entwicklergemeinde erweiterte sich rasch und nach einiger Zeit portierte Michaël Van Canneyt Free Pascal auf Linux. Für OS/2 wurde die DOS-Portierung angepasst, sodass sie mit dem EMX-Extender zusammenarbeitet. Auch eine Win32-Portierung wurde durchgeführt.\n\nIm Allgemeinen sind veröffentlichte Versionen mit geraden Versionsnummern bezeichnet. Wie bei Lazarus stehen ungerade Versionsnummern für Test- und Entwicklungsversionen sowie Snapshots. Daher schreiten die Bezeichnungen der publizierten Versionen in Zweierschritten voran.\n\nVersion 1.0 erschien im Juli 2000. Diese Version hatte schon nahezu Turbo-Pascal‑7/​Delphi‑2-Kompatibilität. In der Folgezeit wurde Free Pascal noch auf zahlreiche andere Plattformen und Architekturen portiert, u. a. auf PowerPC, SPARC und ARM. Version 1.9.4 unterstützte erstmals auch Mac OS X.\n\nVersion 2.0.0 wurde nach langer Weiterentwicklung schließlich im Mai 2005 veröffentlicht.\n\nVersion 2.2 wurde im September 2007 veröffentlicht und unterstützt nun Windows CE, Game Boy Advance und Nintendo DS als Plattform.\n\nVersion 2.4 wurde am 1. Januar 2010 fertiggestellt und unterstützt nun 64-Bit-Mac-OS‑X, iPhone OS und Haiku.\n\nSeit August 2011 kann Free Pascal Byte-Code für die Java Virtual Machine erzeugen.\n\nIm Januar 2012 wurde die Version 2.6 veröffentlicht, die u. a. den Objective-Pascal-Dialekt implementiert. Objective Pascal ist eine Pascal-Schnittstelle zur unter OS X und iOS vorrangig eingesetzten Programmiersprache Objective-C.\n\nIn der Version 2.0.4 unterstützt Free Pascal folgende Architekturen:\n\nFolgende Betriebssysteme werden unterstützt:\n\nIn Entwicklung:\n\n\n"}
{"id": "61265", "url": "https://de.wikipedia.org/wiki?curid=61265", "title": "Microsoft Windows Server 2003", "text": "Microsoft Windows Server 2003\n\nMicrosoft Windows Server 2003 ist ein Betriebssystem und wurde im Jahr 2003 von Microsoft als Nachfolger für die Windows-2000-Serverprodukte veröffentlicht. Die interne Versionsnummer bei Microsoft lautet NT 5.2. Im Vergleich zu Windows 2000 Server enthält es keine grundlegenden Neuheiten, sondern viele kleine Neuerungen im Detail. Besonders im Bereich der Sicherheit wurden viele Verbesserungen vorgenommen, vor allem bei einigen besonders kritisierten Standardeinstellungen. Zum Beispiel werden spezielle Anwendungen wie der Internet Information Server nicht mehr standardmäßig installiert, so dass sich ein besser auf die Aufgabe bezogenes System aufbauen lässt.\n\nWindows Server 2003 war ursprünglich unter der Bezeichnung \"Windows 2002\" geplant. Später wurde das Produkt in \".NET Server\" umbenannt, um eine Verbindung zu .NET herzustellen. Man wandte sich jedoch später vom Vorhaben ab und nannte das Produkt schließlich in \"Windows Server 2003\" um.\n\nDas System unterstützt u. a.:\n\n\n\nSpeziell für Systeme mit x64-Prozessoren (AMDs Opteron, Athlon 64 und Turion 64 mit AMD64; Intels Pentium 4, Core 2 und Xeon mit Intel 64) gibt es folgende Varianten:\n\n\nSpeziell für Systeme mit Intel-Itanium-Prozessoren gibt es ebenfalls 64-Bit-Varianten (Enterprise- und Datacenter-Edition), welche dieselbe Hardwareunterstützung wie die x64-Editionen aufweisen.\n\nAußerdem gibt es speziell für kleine Betriebe die Varianten:\n\n\nSiehe auch:\n\n\nDas am 30. März 2005 veröffentlichte Service Pack 1 für Windows Server 2003 verbessert leicht die Performance, die Zuverlässigkeit und Sicherheit des Betriebssystem. Zu den Verbesserungen gehören vielfach die gleichen Updates wie die des Service Pack 2 für Windows XP-Nutzer. Folgende Funktionen wurden mit dem Service Pack 1 hinzugefügt:\nEine vollständige Liste der Updates finden Sie im Microsoft \"Knowledge Base\".\n\nDie Version Release 2 erschien im Februar 2006 und ist eine aktualisierte Fassung des Server 2003 (SP1) mit Erweiterungspaketen für alle Versionen (mit einer Einschränkung der Itanium-Unterstützung für die \"Enterprise Edition\").\n\nOffizielle Liste der Veränderungen im R2:\nFür eine detaillierte Aufstellung der Neuerungen hat Microsoft eine Produktseite für Windows Server 2003 R2 eingerichtet.\n\nDas Service Pack 2 für Microsoft Windows Server 2003 ist am 12. März 2007 erschienen. Die Sicherheit und Stabilität wurden verbessert und neue Funktionen sowie Aktualisierungen für vorhandene Komponenten hinzugefügt.\n\nDas Service Pack 2 kann auf folgenden Betriebssystemen installiert werden:\n\n\nAußerdem werden noch einige neue Funktionen bereitgestellt (\"siehe auch „Die Liste der Updates in Windows Server 2003 Service Pack 2“\"):\n\n\nDer Windows Storage Server 2003 (WSS2003) ist eine Version für Network Attached Storage. Diese Version besitzt eine eingeschränkte Funktionalität, ist aber preiswerter. Einsatzgebiete sind SQL-Server, Fileserver oder Mail- und Exchange-Server. Seit 2006 ist er nur noch in der R2-Variante verfügbar. Der Server wird auf Hardware vorinstalliert geliefert und kann per Webschnittstelle verwaltet werden.\n\n"}
{"id": "61724", "url": "https://de.wikipedia.org/wiki?curid=61724", "title": "Taligent", "text": "Taligent\n\nTaligent war eine von Apple und IBM im Jahr 1992 gemeinsam gegründete Firma, die die Aufgabe hatte, ein vollständig objektorientiertes und plattformunabhängiges Betriebssystem zu entwickeln. Durch diese Kooperation entstand auch die später von Apple genutzte PowerPC-Plattform.\n\nGemeinsam entwickelten Apple, IBM und HP die später als CommonPoint bekannte Laufzeitumgebung für unterschiedliche Betriebssysteme. Begonnen hatte die Entwicklung bei Apple bereits 1988 als Betriebssystemprojekt „Pink“, das später gemeinsam mit IBM als TalOS weiterentwickelt, aber schließlich eingestellt wurde. Nach dem Ausstieg von Apple wurde das Projekt als Laufzeitumgebung zuerst in TalAE, später in CommonPoint umbenannt und zwischenzeitlich gemeinsam mit HP weiterentwickelt.\n\nDie Entwicklung von CommonPoint wurde 1998 eingestellt.\n\nDas seit 1984 weiter entwickelte Betriebssystem Macintosh System Software wies einige konzeptionelle Mängel auf, die man nur durch eine Neuentwicklung beheben konnte. Das System unterstützte weder präemptives Multitasking, Mehrbenutzerbetrieb Speicherschutz noch dynamische Speicherverwaltung, weshalb es sehr anfällig für Instabilitäten war. Deshalb entschloss sich Apple, ein neues Betriebssystem von Grund auf zu entwickeln:\n\n1988 wurden während der Planung des Nachfolgers von System 6 alle Ideen auf rosa () Karteikarten festgehalten, die sich in einem von Grund auf neu entwickelten System verwirklichen ließen. Alle Ideen, die in einer neuen Version des bestehenden System 6 umsetzbar erschienen, wurden auf blaue () Karteikarten geschrieben. Aus den blauen Karten wurde 1991 System 7. (Das Entwicklerteam erhielt passend dazu den Namen „“ – eine Anspielung auf Figuren aus dem Film Yellow Submarine von den Beatles.)\n\n1988 wurde das Projekt „Pink“ in Angriff genommen. „Pink“ hatte ein vollständig objektorientiertes Betriebssystem zum Ziel, das sich für den Benutzer wie System 6 anfühlen sollte. Es wurde in C++ geschrieben und sollte bereits bestehende Anwendungen ausführen können. „Pink“ ermöglichte es, Dokumente sehr einfach zwischen Computern und Benutzern auszutauschen und gemeinsam daran zu arbeiten. Es führte ein komponentenbasiertes Dokument-Modell ein, das es sehr einfach machte, Inhalte von beliebigen Programmen in ein beliebiges Dokument einzufügen. Später entstand aus dieser Entwicklung OpenDoc.\n\nBald kam es innerhalb von Apple zu Rivalitäten zwischen dem „“- und dem „Pink“-Team, die damit endeten, dass alle Mitarbeiter zu einem einzigen Team zusammengefasst wurden.\n\nMitte 1991 gelang es Apple, die Firma IBM von seiner damaligen Entwicklung zu überzeugen, weshalb die gemeinsame Tochterfirma Taligent gegründet wurde, um „Pink“ unter dem neuen Namen TalOS gemeinsam zu vollenden. Bald merkte man aber, dass der Markt abseits von Apple kein neues Betriebssystem brauchte, weshalb das Projekt als Laufzeitumgebung () TalAE, für , weiterentwickelt wurde.\n\nAls Apple nach 1995 keinen Bedarf für TalAE sah, stieg es aus der Entwicklung aus und überließ IBM die alleinige Weiterentwicklung der Laufzeitumgebung. IBM nannte TalAE in CommonPoint um und integrierte es in die eigenen Produkte.\n\nFür Apple aber blieb damit das Problem des veralteten klassischen Mac OS weiterhin bestehen.\n\nAus TalAE wurde unter dem Namen CommonPoint eine Laufzeitumgebung, die auf AIX, HP-UX, OS/2, Windows NT und dem bei Apple in Copland entstehenden Betriebssystem lauffähig war. Ab Ende 1994 beteiligte sich auch die Firma Hewlett-Packard an der Entwicklung von CommonPoint. Ein Misserfolg war die kommerzielle Veröffentlichung von CommonPoint für AIX und OS/2 im Jahr 1995. Als im gleichen Jahr auch der CEO von Taligent, Dick Guarino, unerwartet verstarb und Apple das Projekt Copland höher priorisierte, kam die Entwicklung schnell nahezu zum Stillstand.\n\nApple und HP zogen sich im Jahr 1995 zurück, und IBM übernahm die Firma Taligent vollständig. Einige Teile von CommonPoint flossen in Lotus Notes ein und andere wurden an die Firmen Sun, Oracle und Netscape veräußert.\n\nIm Jahr 1996 veröffentlichte IBM den populären auf CommonPoint aufbauenden Model View Presenter (MVP).\n\nAls 1997 Bill Gates gefragt wurde, welche Windungen und Entwicklungen in der Software-Industrie ihn in den letzten 20 Jahren wirklich überraschten, war seine Antwort: „Kaleida and Taligent had less impact than we expected.“ („Kaleida und Taligent hatten weniger Einfluss als wir dachten.“)\n\nIm Januar 1998 wurde die Firma Taligent aufgelöst und die noch verbliebenen Angestellten wurden von IBM übernommen.\n\nDer Name ist ein Kofferwort, aus den englischen Begriffen „\"tal\"ent“ (deutsch: Begabung) und „intell\"igent\"“ (deutsch: intelligent) – wobei „e\"nt\"“ (eine Anspielung auf Windows \"NT\") und „\"intell\"“ (eine Anspielung auf den Chiphersteller \"Intel\") weggelassen wurden.\n"}
{"id": "62279", "url": "https://de.wikipedia.org/wiki?curid=62279", "title": "Distributed Interactive Simulation", "text": "Distributed Interactive Simulation\n\nDistributed Interactive Simulation (DIS) bezeichnet einen im IEEE 1278 definierten Datenaustauschstandard zur Steuerung von Simulationssystemen.\n\nDIS wird in professionellen zivilen und militärischen Simulationen zur Optimierung der Ausbildung genutzt (z. B. in Verkehrsleitzentralen, Radarzentren). Mittels DIS vernetzte Simulationssysteme können in Echtzeit simulierte Lageinformationen, z. B. zur synchronen Steuerung von Radarsimulatoren, austauschen und versetzen so die Nutzer der damit vernetzten Systeme in die Lage, in ihrem jeweiligen Simulator eine identische Umweltdarstellung wahrzunehmen.\n\nEin DIS-Simulationsobjekt wird als „Entity“ bezeichnet. Jede Entity muss einen eindeutigen (unique) Bezeichner tragen (Entity-Id).\n\nDie Kommunikation über DIS erfolgt Paket-orientiert mit UDP-Paketen (Multicast oder Broadcast) und TCP. Die eigentlichen Daten sind dabei binär codiert. Die Bytereihenfolge ist dabei in „network order“.\n\nDie über ein Wide Area Network (WAN) oder Local Area Network (LAN) verbundenen Simulationsteilnehmer können unter anderem folgende Informationsblöcke (Protocol Data Unit, PDU) austauschen:\n\nDer aktuell gültige IEEE Std 1278.1 von 2012 enthält die unterschiedlichsten PDUs die u. a. bis hin zur Simulation von Minenfeldern reichen.\n\nEin PDU enthält im Header immer die folgenden Bestandteile:\n\n\n\nMit einem ähnlichen Ansatz, aber objektorientiert, tritt die High Level Architecture (HLA) an, in der viele Experten den verbesserten Nachfolgestandard des DIS sehen. HLA unterstützt gerichtete Kommunikation über eine zentrale Verwaltungsstelle.\n"}
{"id": "62962", "url": "https://de.wikipedia.org/wiki?curid=62962", "title": "PowerPC G4", "text": "PowerPC G4\n\nPowerPC G4 ist die Bezeichnung für die RISC-Prozessoren PowerPC 74xx von Motorola.\n\nDer Name G4 spielt auf die \"vierte Generation\" (engl. ) von PowerPC-Chips an, die von Motorola gefertigt wurden. Das Design der Prozessoren wurde in enger Abstimmung zwischen Apple, IBM und Motorola (AIM-Allianz) entworfen, IBM blieb der Entwicklung jedoch fern. Mit ein Grund dafür waren unterschiedliche Ansichten, wie die SIMD-Einheit \"AltiVec\" in den Chip integriert werden sollte.\n\nDie G4-Prozessoren wurden, bevor die Produktion Ende 2005 eingestellt wurde, vor allem von dem amerikanischen Computerhersteller Apple unter anderem in den Modellreihen Power Mac und PowerBook verbaut. G4 ist deshalb auch die umgangssprachliche Bezeichnung für die verschiedenen \"Power Mac G4\"-Modelle. Motorola gliederte den Halbleiterbereich 2004 vollständig in die Tochterfirma Freescale Semiconductor aus.\n\nDer Nachfolger der G4-Reihe sind die im Gegenzug nur von IBM entwickelten und produzierten G5-Prozessoren. Freescale selbst hat die Reihe bisher noch nicht weiterentwickelt, legt allerdings den Schwerpunkt der Fortschritte in den Embedded-Bereich, wo beispielsweise weitere externe Schnittstellen auf den Chip verlegt werden, etwa mit dem MPC8641D. Gleichzeitig wurde der Name der Prozessorreihe in e600 geändert, die teilweise voll mit den von Apple verwendeten G4-Prozessoren kompatibel sind.\n\nG4-Prozessoren enthalten eine 128-Bit-Vektoreinheit (SIMD-Verfahren) mit dem Namen „AltiVec“. Diese Einheit ist vergleichbar mit der SSE-Technologie der IA32-Prozessoren. Damit können bis zu vier Gleitkommazahlen gleichzeitig verarbeitet werden, oder aber bis zu 16 Bytes. Es wurde auch die Mehrprozessorfähigkeit verbessert. Die Mitglieder der Vorgänger-Modellreihe G3 sind zwar fähig, mehrere Prozessoren zu koppeln, allerdings mit starken Leistungseinschränkungen. Für G4 wurde deshalb eine schnelle Chip-zu-Chip-Verbindung mit in die Architektur aufgenommen. Der Prozessor soll 33 Millionen Transistoren haben.\n\n\nDie bisher schnellsten Modelle sind der 7448 mit 1 MB L2-Cache bei max. 1,7 GHz (mehr bei geeigneter Kühlung) und der 7457 mit max. 2 MB L3-Cache und 1,33 GHz (der 7455 mit 1,42 GHz wurde nur für Apple hergestellt).\n\nDer \"Power Mac G4\" aus dem Jahr 1999 rechnete zu schnell, um nach China exportiert werden zu dürfen. Da seine Rechenleistung gerade noch in den damals definierten Bereich der Supercomputer fiel, unterlag er den amerikanischen Ausfuhrbeschränkungen.\n"}
{"id": "64477", "url": "https://de.wikipedia.org/wiki?curid=64477", "title": "Quartz (macOS)", "text": "Quartz (macOS)\n\nQuartz ist die Grafikschicht des Apple Betriebssystems macOS. Bei Quartz handelt es sich um eine Bibliothek für 2D- und 3D-Grafiken, die das grundlegende Darstellungsmodell für Mac OS X bildet. Aufbauend auf dem plattformübergreifenden PDF-Standard (Portable Document Format), kann Quartz 2D hochwertige Texte und Grafiken mit Kantenglättung anzeigen und ausdrucken und bietet Unterstützung für OpenType-, PostScript- und TrueType-Zeichensätze.\n\nQuartz besteht aus den folgenden Bausteinen:\n\nAb Mac OS X 10.2 „Jaguar“ wurde Quartz Compositor mit Quartz Extreme erweitert. Quartz Extreme benutzt OpenGL genauso wie eine normale Applikation und behandelt den Desktop wie eine 3D-Szene. Dadurch kann die Grafikkarte zur Berechnung der Effekte (wie Schatten oder Animationen) verwendet werden, was den Hauptprozessor entlastet.\n\nMicrosoft hat eine ähnliche Technik für Windows Vista entwickelt, die Windows Presentation Foundation.\n\nUnter Linux sind Xgl und AIGLX zusammen mit einem Composition-Manager wie Enlightenment e17 oder Compiz mit Quartz Extreme vergleichbar.\n\n\n"}
{"id": "65779", "url": "https://de.wikipedia.org/wiki?curid=65779", "title": "QuickTime VR", "text": "QuickTime VR\n\nQuickTime VR ist eine von Apple entwickelte Technik zur Darstellung von Panoramabildern auf mit QuickTime ausgestatteten Rechnern. Die Technik erlaubt die interaktive Navigation um die senkrechte und die waagrechte Achse, sowie die Kontrolle der Zoomstufe. So entsteht der Eindruck eines dreidimensionalen Raumes (das VR im Namen steht für \"virtual reality\", also virtuelle Realität).\n\nMan unterscheidet in diesem Zusammenhang zwischen den frühen zylindrischen und später auch möglichen kubischen Panoramen. Kubische (oder sphärische) Panoramen erlauben eine Darstellung von 180° in der Vertikalen, während dies bei zylindrischen Projektionen nicht möglich ist. Mehrere QuickTime-VR-Panoramen lassen sich zu virtuellen Rundgängen verknüpfen.\n\nNeben den QuickTime-VR-Panoramen gibt es auch \"QuickTime VR Object Movies\", mit denen sich der Betrachter frei um ein fotografiertes oder gerendertes Objekt herum bewegen und näher zoomen kann.\n\nQuickTime VR funktioniert auch auf Rechnern mit vergleichsweise geringer Rechenleistung, wurde aber mittlerweile durch auf Flash und später HTML5 basierenden Techniken zur Betrachtung von Panoramen weitgehend verdrängt.\n\n\n"}
{"id": "67451", "url": "https://de.wikipedia.org/wiki?curid=67451", "title": "Altair 8800", "text": "Altair 8800\n\nDer Altair 8800 war ein früher Heimcomputer, damals \"Microcomputer\" genannt zur Abgrenzung von den noch kühlschrankgroßen \"Minicomputern\". Mit seinen Kippschaltern zur Eingabe und Leuchtdioden zur Ausgabe (Maschinenkonsole) hatte dieses Gerät noch nicht den Bedienkomfort späterer Personal Computer, dennoch wurde es bei seiner Markteinführung vom Hersteller bereits so bezeichnet. Nach dem Start über die Maschinenkonsole lassen sich auch Betriebssystem, Programmierumgebungen – das Altair BASIC war die erste vertriebene Software von Microsoft – oder andere Programme von externen Datenspeichern laden; Ein- und Ausgabe können dann über ein angeschlossenes Terminal erfolgen. Der Altair 8800 diente innerhalb des Homebrew Computer Clubs als Kernstück für weitere Entwicklungen und hatte so einen wesentlichen Einfluss bei der Entwicklung der ersten persönlichen Computer.\n\n1974 wurde der Computer von Ed Roberts und seinem Unternehmen \"Micro Instrumentation and Telemetry Systems\" (\"MITS\") entwickelt und ab 1975 für 395 US-Dollar als Bausatz mittels Anzeigen in \"Popular Electronics\", \"Radio-Electronics\" und anderen Hobbyistenzeitschriften auf den Markt gebracht. Das Fertiggerät kostete 495 US-Dollar, nach heutiger Kaufkraft rund US-Dollar.\n\nWährend des Militärdienstes in den Waffenlabors der United States Air Force im Stützpunkt Kirtland (New Mexico) entschieden sich Ed Roberts und Forrest M. Mims III, ihr elektronisches Hintergrundwissen zu nutzen und kleine Raketenmodell-Bausätze für Bastler zu produzieren. Zu diesem Zweck gründeten sie zusammen mit Stab Nagle und Robert Zeller das Unternehmen \"Micro Instrumentation and Telemetrie Systems\", kurz \"MITS\", in Roberts Garage in Albuquerque und begannen, Funksender und Instrumente für Raketenmodelle zu verkaufen.\n\n1969 kaufte Roberts die Anteile der anderen und mietete ein größeres Büro, wo er Taschenrechner-Bausätze herstellte. Mims half ihm dabei, indem er Handbücher für einige der Bausätze schrieb, im Austausch dafür erhielt er Bausätze. 1972 aber entwickelte \"Texas Instruments\" einen eigenen Chip für Taschenrechner und begann komplette Taschenrechner für etwas mehr als die Hälfte des damals üblichen Marktpreises zu verkaufen. \"MITS\" und viele andere Unternehmen wurden durch diesen Umstand praktisch zerstört, und Roberts hatte nun Probleme, seine Schulden von einer halben Million US-Dollar zurückzuzahlen.\n\nMit der Vorstellung des Intel 8008 im Jahr 1972 und des verbesserten Nachfolgers 8080 im Jahr 1974 begannen einige Bastler, Mikrocomputer-Bausätze zu entwerfen. Im Juli 1974 wurde der Mark-8 in \"Radio-Electronics\" beworben. Der Bausatz wurde nur in Form von Bauplänen auf Papier verkauft, und obwohl ihm kein kommerzieller Erfolg beschieden war (oder gerade deshalb), bewog es die Redakteure von \"Popular Electronics\" dazu, als Erste einen kompletten Bausatz zu verkaufen. Zu diesem Zeitpunkt ist der weitere Verlauf der Geschichte unklar.\n\nRoberts suchte nach einem guten Geschäft für die CPU und brachte Intel schließlich dazu, ihm 8080-CPUs, die kosmetisch beschädigt waren, zu 75 US-Dollar statt des normalen (allerdings nur als symbolische Anspielung auf IBMs System/360-Mainframe zu verstehenden) Preises von 360 US-Dollar zu verkaufen.\n\nAls Nächstes musste ein Name für den noch namenlosen Bausatz gefunden werden; Roberts schlug zu Ehren von \"Popular Electronics\" den Namen \"PE-8\" vor. Les Solomon, der technische Leiter bei \"Popular Electronics\", suchte jedoch nach einem einprägsameren Namen. Wie er später erzählen sollte, habe er dabei seine Tochter gefragt (die sich gerade eine Folge von Raumschiff Enterprise ansah), wie der Computer auf der Enterprise heiße. Dieser hatte keinen Namen, aber die Crew um Captain Kirk flog gerade zum Altair.\n\nDas erste funktionierende Muster wurde umgehend nach New York zu \"Popular Electronics\" verschickt. Aufgrund eines Streiks bei dem Transportunternehmen kam dieses Exemplar aber nie an und ist seither verschollen. Jedoch hatte Solomon bereits Fotos vom Altair gemacht und schrieb den Artikel nun basierend auf den Fotos, während Roberts umgehend ein zweites Exemplar baute. Der Bausatz wurde am 19. Dezember 1974 offiziell verfügbar gemacht.\n\nDer Bausatz wurde im Januar 1975 in \"Popular Electronics\" vorgestellt. Da immer mehr Elektronik digital wurde, wandten sich die Hobbyelektroniker jener Zeit dem Computer zu, waren aber mit den Fähigkeiten und der Flexibilität der damals erhältlichen Computerbausätze unzufrieden. Der Altair hatte diese Nachteile nicht und hatte das Potential, wirklich von Nutzen zu sein. Außerdem war er auf Erweiterbarkeit ausgelegt und ermöglichte alle möglichen Experimente. Roberts musste 200 Bausätze im ersten Jahr verkaufen, um die Gewinnschwelle zu erreichen – stattdessen trafen bereits am ersten Tag 200 Bestellungen ein und vermehrten sich auf mehrere Tausend im ersten Monat.\n\nNach nur elf Monaten tauchte auf dem Markt die erste Konkurrenz in Form des IMSAI 8080 (Verkaufsstart 16. Dezember 1975) auf, der zusätzlich über eine Tastatur, Monitor und einen Floppy-Disk-Controller verfügte. Roberts war aufgebracht und verbrachte immer mehr Zeit damit, konkurrierende Unternehmen auszubooten, anstatt den Altair zu verbessern. 1976 waren schließlich eine Anzahl Bausätze erhältlich, die dem Altair voraus waren, und als Roberts von den aufkommenden Computerläden verlangte, nur noch den Altair zu verkaufen, wandten sich diese der Konkurrenz zu. So wurde MITS aus dem Markt geworfen, den die Firma selber geschaffen hatte\n\nAm 22. Mai 1977 kaufte die \"Pertec Computer Corporation\", ein sehr viel größerer Hersteller von Diskettenlaufwerken, \"MITS\" auf. Der Altair wurde daraufhin vom Markt genommen. Da \"MITS\" nicht die Rechte an Altair BASIC besaß und dieses nun unter dem Label von \"Micro-Soft\" weiter vermarktet wurde, gab es später einen Rechtsstreit zwischen der \"Pertec Computer Corporation\" und \"Microsoft\".\n\nDer verloren gegangene Prototyp bestand aus vier Karten, die übereinander gestapelt wurden. Die Bauteile für den kompletten Rechner passten nicht auf eine einzelne Hauptplatine. Ein weiteres Problem war, dass viele Teile, um den Altair überhaupt richtig nützlich zu machen, noch gar nicht existierten oder bis zur Vorstellung noch nicht fertig entwickelt sein würden.\n\nDeshalb entschied sich Roberts bei der Konstruktion des Bausatzes den Rechner auf austauschbaren Karten aufzubauen und die Hauptplatine so auf nicht mehr als eine Verbindung zwischen den einzelnen Karten zu reduzieren (Backplane). Das Basismodell bestand aus fünf Platinen, darunter eine für die CPU (2 MHz Intel 8080) und eine für den Arbeitsspeicher (256 Byte, erweiterbar auf 64 KB). Später gab es als zusätzliche Karten Massenspeicher, Ein/Ausgabe-Geräte (u. a. ein RS-232-Interface) und Speichererweiterungen.\n\nAnschließend musste noch eine günstige Steckverbindung gefunden werden – Roberts fand diesen im Randstecker mit 100 Pins. Das Ganze nannte er den S-100-Bus. Der S-100 wurde durch die professionelle Computergemeinde anerkannt und so zum ersten im Industriestandard unter der Nummer IEEE 696 genormten Bussystem. Er wurde außerdem zur Basis weiterer konkurrierender Mikrocomputer (z. B. IMSAI, SOL, Cromemco) und etwa bis 1985 verwendet.\n\nIm Grunde genommen war dieser Bus nichts anderes als die auf die Backplane geführten Pins der CPU. Der Aufbau war nicht durchdacht; so wurden verschiedene Stromleitungen mit unterschiedlichen Spannungen nebeneinander geführt, was Kurzschlüsse begünstigte. Eine weitere Kuriosität des Systems war, dass es über zwei unidirektionale 8 bit breite Datenbusse verfügte, aber nur einen bidirektionalen 16 bit breiten Adressbus. Aus Kostengründen wurde außerdem ein Netzteil verwendet, das elektrische Spannungen von +8 und +15 Volt lieferte, die auf den Karten auf die Standardspannungen von TTL (+5 V) und RS-232 (+12 V) heruntergeregelt werden mussten.\n\nWeiter vertriebene Varianten waren: \n\nDas Gerät selbst verfügte nicht über die heute übliche Peripherie, nicht einmal über eine Tastatur. Das Frontpanel verfügte über LEDs zur Anzeige von Adress- und Datenleitungen sowie über Kippschalter zur bitweisen Programmierung (siehe auch Maschinenkonsole). Zur Benutzung von Altair BASIC oder CP/M als Kommandozeilen-Betriebssystem musste ein Text-Terminal oder ein Fernschreiber über das RS-232-Interface angeschlossen werden.\n\n\n"}
{"id": "67929", "url": "https://de.wikipedia.org/wiki?curid=67929", "title": "DivX", "text": "DivX\n\nDivX (engl. []) ist ein MPEG-4-kompatibler Videocodec. Der Codec ist für seine Fähigkeit bekannt, große Videodateien bei guter Qualität vergleichsweise stark zu komprimieren.\n\nDivX 3.11 und frühere Versionen des Codecs entstanden, nachdem im Jahr 1998 Microsofts MPEG-4-Codec gehackt worden war. Aus einer Beta-Version des Windows Media Players heraus hatte ein französischer Hacker namens \"Jérôme Rota\" (Pseudonym: Gej, okzitanisch für „verrückt“) den Codec extrahiert. Das „Project Mayo“ war geboren, dem sich kurze Zeit später vier weitere Programmierer anschlossen. In den frühen Versionen des „DivX Player“ zierte ihn noch das „Project Mayo“-Emblem, das jedoch aufgrund vieler Rückfragen von Nutzern später entfernt wurde. Der Hack modifizierte den Microsoft-Codec, um das komprimierte Video nicht nur als ASF-Datei, sondern auch als AVI-Datei speichern zu können. Außerdem unterstützte der ursprüngliche Microsoft-Codec nur Bitraten von maximal 256 kbps; in der gehackten Version hingegen waren es bis zu 6.000. Die von Rota gegründete Firma \"DivXNetworks, Inc.\" entwickelte später eine völlig neue Version, um in den USA Patentverletzungen zu vermeiden. DivXNetworks hat in den USA ein Patent auf den neuen Codec angemeldet.\n\nIm Januar 2001 gründete DivXNetworks \"OpenDivX\" als Teil des Projektes \"Mayo\", welches Open-Source-Multimedia-Projekte beherbergen sollte. OpenDivX war ein Open-Source-MPEG-4-Videocodec, der von Grund auf neu geschrieben wurde; allerdings wurde der Code unter eine eingeschränkte Lizenz gestellt, und nur Mitglieder des \"DivX Advanced Research Centre\" (DARC) hatten Schreibzugriff zum CVS. Im Frühjahr 2001 schrieb DARC-Mitglied \"Sparky\" eine verbesserte Version des Encoderkerns, genannt \"encore2\", welcher dann ohne Vorwarnung vom CVS entfernt wurde. Die Erklärung von Sparky war: \"„Wir (unsere Vorgesetzten) entschieden, dass wir noch nicht bereit sind, es der Öffentlichkeit zu zeigen“\" (übersetzt).\n\nIm Juli 2001 fingen die Entwickler an, sich über einen Mangel an Aktivität des Projektes \"Mayo\" zu beschweren, da die letzte Quelltextveränderung schon Monate her war, Verbesserungen von Programmfehlern ignoriert wurden und die versprochene Dokumentation nicht erschienen war. Kurz danach veröffentlichte DARC eine Beta-Version ihres Closed Source und kommerziellen DivX-4-Codecs, welches auf \"encore2\" basierte, mit der Erklärung \"„Was die Community wirklich will, ist ein Winamp, nicht ein Linux“\" (übersetzt). Manche warfen DivXNetworks vor, OpenDivX nur gestartet zu haben, um anderer Leute Ideen zu sammeln und sie dann in ihrem DivX-4-Codec zu benutzen; manche waren enttäuscht, dass die Codeentwicklung stagniert hatte, wollten aber daran weiter arbeiten, während wieder andere wütend darüber waren, wie DivXNetworks ein sogenanntes Open-Source-Projekt handhabt. Danach wurde ein Fork von OpenDivX erstellt, der die letzte Version von \"encore2\" verwendet, den ein paar Leute sich heruntergeladen hatten, bevor er entfernt wurde. Seitdem wurde der gesamte OpenDivX-Code ersetzt und unter der GPL als Xvid-Codec veröffentlicht und weiterentwickelt.\n\nEin typischer 100 Minuten langer Spielfilm ist auf einer DVD sechs bis acht Gigabyte groß, mit der DivX-Videokompression lässt sich der Film auf einer CD-ROM (650 bis 700 MB) speichern. Die Qualität bleibt auch bei im Vergleich zu MPEG-2 niedrigen Bitraten von 650 bis 1.000 kBit/s relativ hoch, bei Szenen mit viel Bewegung können jedoch Kompressionsartefakte entstehen.\n\nMit der Weiterentwicklung des DivX-Codecs wurden wiederholt technische Verbesserungen erzielt. Vor allem die ab Version 4 unterstützte Multipass-Kodierung mit variabler Bitrate hat hierzu beigetragen, bei welcher in einem ersten Kodierungsdurchlauf die Ursprungsdatei hinsichtlich der Komplexität der aufeinanderfolgenden Einzelbilder analysiert wird. Erst in einem der darauffolgenden Durchläufe (meistens einem) wird die endgültige Videodatei erzeugt. Der Vorteil hierbei ist, dass bei gleichbleibendem Speicherplatzbedarf komplexe und schnell bewegte Szenen eine höhere Bitrate zugewiesen bekommen, während diese bei langsamen, ruhigen Bildfolgen wiederum verringert wird.\n\nNeuere Versionen unterstützen zudem verschiedene MPEG-Verfahren (sogenannte \"MPEG-Tools\") wie beispielsweise anamorphe Codierung und Global Motion Compensation, was bei annähernd gleicher Bildqualität wiederum zu einer Verringerung des Speicherplatzbedarfes führt.\n\nDer Erfolg von DivX im Heimbereich hat sich in den letzten Jahren so weit gesteigert, dass der Codec auch von DVD-Spielern unterstützt wird. Außerdem verfügen verschiedene Digitalkameras und Handymodelle über eine Video-Aufnahmefunktion im DivX-Format.\n\nUm sich weiter im Heimbereich zu festigen, wird die Breite an Abspielmöglichkeiten weiter gesteigert, indem der Codec auch für den Einsatz auf anderen Plattformen weiterentwickelt wird. So gibt es zusätzlich die Möglichkeit, Filme in ressourcen- und speicherschonenden Formaten für den Einsatz auf PDAs und Handhelds als auch für den anspruchsvollen Heimkinoeinsatz in hochauflösenden Formaten (High Definition Television) zu komprimieren.\n\nFilme mit hochauflösendem Bild verlangen sowohl mehr Leistung vom Prozessor als auch mehr Speicherplatz. Jedoch liegt der Speicherbedarf deutlich unter dem eines vergleichbaren MPEG-2-Filmes.\n\nDivX unterstützt ab Version 7.0 das H.264-Format sowie den Matroska-Container, jedoch ausschließlich über ein sehr einfach gestricktes Konvertierungsprogramm namens \"DivX Converter\". Außerdem bringt DivX 7 einen H.264-Decoder für DirectShow mit. Der eigentliche Codec wird weiterhin in Version 6.9.2 ausgeliefert, ohne H.264 und ohne Matroska-Unterstützung. Der DivX-H.264-Kommandozeilen-Encoder steht derzeit als Vorabversion auf der „DivX Labs“ Web-Seite zum Download bereit. Vor dem Download ist eine \"kostenlose\" Registrierung notwendig. Der Kommandozeilen-Encoder unterstützt AviSynth als Eingabe und gibt H.264-Rohdaten (ohne Container) aus.\n\n\n"}
{"id": "69966", "url": "https://de.wikipedia.org/wiki?curid=69966", "title": "Commodore-Maus", "text": "Commodore-Maus\n\nVon Commodore wurden zwei Modelle von Mäusen für den C64 und C128 angeboten. Da bei der Markteinführung des C64 im Jahr 1982 Computermäuse noch nicht üblich waren, mussten nachträglich Methoden entwickelt werden, diese dennoch benutzbar zu machen, indem die Mäuse sich elektrisch wie eines der vorgesehenen Eingabegeräte verhielten.\n\nTrotz des gleich aussehenden Steckers kann eine Commodore-Maus nicht am seriellen Anschluss eines PC betrieben werden, die höheren Spannungen (±12 V gegenüber 0 V/5 V) am PC-Anschluss würden sie wahrscheinlich sogar zerstören. Auch mit den Commodore-Amiga-Rechnern ist dieser Maustyp nicht kompatibel (die 1350 kann aber am Joystickanschluss eines Amiga angeschlossen werden und erfüllt dann die Funktionen eines Joysticks).\n\nDie abgebildete Gehäuseausführung ist von Commodore seit Einführung des Amiga lange weiterbenutzt worden, die ersten Maus-Gehäuseformen für Amigas sind äußerlich identisch. Unterscheiden lassen sich Mäuse für C64/C128 und Amiga einerseits am Bodenaufkleber. Dort findet sich in der C64-/C128-Version die Modellbezeichnung 1351. Weiterhin ist die Form des Steckers eine andere, wohingegen der Anschluss selbst identisch ist. Trotzdem ist die Amiga-Maus nicht kompatibel mit dem Modell für C64/C128.\n\n\"Siehe auch:\" Commodore-Produktübersicht\n\n\n"}
{"id": "70686", "url": "https://de.wikipedia.org/wiki?curid=70686", "title": "Big Brother Awards", "text": "Big Brother Awards\n\nDie Big Brother Awards (BBA) sind Negativpreise, die jährlich in mehreren Ländern an Behörden, Unternehmen, Organisationen und Personen vergeben werden. Die Preise werden, so die Stifter, an die verliehen, die in besonderer Weise und nachhaltig die Privatsphäre von Personen beeinträchtigen oder Dritten persönliche Daten zugänglich gemacht haben oder machen. Entgegengenommen wurden die Preise 2002 durch Microsoft, 2005 durch Blizzard Entertainment, 2007 durch die PTV Planung Transport Verkehr AG, 2008 durch die Deutsche Telekom, 2011 durch Gert G. Wagner, den Vorsitzenden der Zensuskommission der Bundesregierung, sowie 2016 durch Change.org. Die Organisation vergibt auch einen Positivpreis, um Personen und Organisationen zu ehren, die sich für Datenschutz eingesetzt haben. In Österreich ist dies der \"Defensor Libertatis\", erstmals vergeben 2005, in Deutschland der \"Julia-und-Winston-Award\", erstmals 2014 und benannt nach den Hauptpersonen des Romans \"1984\".\n\nDie Preise sollen auf die Datenschutzproblematik aufmerksam machen und negative Entwicklungen aufzeigen. Der Name dieser Auszeichnung ist eine Anspielung auf die Figur des Großen Bruders (Big Brother) in George Orwells Roman \"1984\". Ins Leben gerufen wurden die Big Brother Awards 1998 von Privacy International. Sie wurden erstmals 1998 in Großbritannien verliehen, 1999 zum ersten Mal in Österreich und seit dem Jahr 2000 auch in Deutschland und in der Schweiz. 2003 gab es bereits in 14 verschiedenen Ländern BBA-Preisverleihungen.\n\nIn Deutschland ist der Verein digitalcourage (vormals FoeBuD, Verein zur Förderung des öffentlichen bewegten und unbewegten Datenverkehrs) Stifter und Ausrichter des \"Negativpreises für Datenkraken\". Der Jury gehören neben digitalcourage sechs weitere unabhängige Organisationen an: Chaos Computer Club e. V. (CCC), Deutsche Vereinigung für Datenschutz e. V. (DVD), Förderverein Informationstechnik und Gesellschaft e. V. (FITUG), Forum InformatikerInnen für Frieden und gesellschaftliche Verantwortung e. V. (FIfF), Humanistische Union e. V. (HU) sowie Internationale Liga für Menschenrechte (ILMR).\n\nDie Verleihung fand am 20. Oktober 2006 in Bielefeld statt.\nDie Verleihung fand am 12. Oktober 2007 in Bielefeld statt.\nDie Verleihung fand am 24. Oktober 2008 in Bielefeld statt.\nDie Verleihung der 10. BigBrotherAwards fand am 16. Oktober 2009 in Bielefeld statt.\nDie Verleihung der deutschen Big Brother Awards 2011 fand am 1. April 2011 wieder in Bielefeld statt. Bei der Verleihung wurde damit erstmals das gesamte vergangene Kalenderjahr berücksichtigt.\n\nDie Verleihung der deutschen Big Brother Awards 2012 fand am 13. April 2012 in Bielefeld statt.\n\nDie Verleihung der deutschen Big Brother Awards 2013 fand am 12. April 2013 in Bielefeld statt.\n\nDie Verleihung der deutschen Big Brother Awards 2014 fand am 11. April 2014 in Bielefeld statt.\n\nDie Verleihung der fünfzehnten deutschen Big Brother Awards fand am 17. April 2015 in Bielefeld statt. Wie in den vorangegangenen Jahren wurden Vorschläge aus dem letzten Kalenderjahr berücksichtigt.\n\nDie Verleihung der sechzehnten deutschen Big Brother Awards fand am 22. April 2016 in Bielefeld statt.\nDie Verleihung der siebzehnten deutschen Big Brother Awards fand am 5. Mai 2017 in Bielefeld statt.\n\nGroße Resonanz in der Presseberichterstattung zur Preisvergabe erhielt vor allem der Award in der Kategorie \"Politik\" für die DİTİB, den Verband von Moscheegemeinden in Deutschland unter türkischer Regierungsführung. Bei der DİTİB tätige Imame, so die Laudatio, hätten Mitglieder und Besucher umfangreich überwacht, der Verband übernehme jedoch keine Verantwortung für diese Ausspionierung. Er erkläre die Vorgänge unzulässigerweise für intern, stelle sich nicht der öffentlichen Kritik und habe die Affäre vorschnell für erledigt erklärt. Um die Grundrechte der Gläubigen zu schützen, so die Laudatio weiter, müssten die zuständigen deutschen Behörden diese Spionagetätigkeiten umfassend aufklären und ohne diplomatische Rücksicht strafrechtlich verfolgen.\n\nIn einem Schreiben an Digitalcourage verwahrte sich die DİTİB schon im Vorfeld der Verleihung gegen die Kritik. Außerdem drohte er dem Verein mit rechtlichen Schritten wegen übler Nachrede.\n\nDie Verleihung der 18. deutschen Big Brother Awards fand am 20. April 2018 im Stadttheater Bielefeld statt.\n\nDie Verleihung fand am 25. Oktober 2005 im Wiener Rabenhof Theater statt.\nDie Verleihung fand am 25. Oktober 2006 im Wiener Rabenhof Theater statt.\nDie Verleihung fand am 25. Oktober 2007 im Rabenhof Theater in Wien statt.\n\nDie Verleihung fand am 25. Oktober 2008 im Rabenhof Theater in Wien statt.\n\nDie Verleihung fand am 25. Oktober 2009 in Wien statt.\nDie Verleihung fand am 25. Oktober 2010 im Rabenhof Theater in Wien statt.\nDie Verleihung fand am 25. Oktober 2011 im Rabenhof Theater in Wien statt.\n\nDie Verleihung fand am 25. Oktober 2012 im Rabenhof Theater in Wien statt.\nDie Verleihung fand am 25. Oktober 2013 im Rabenhof Theater in Wien statt. Das diesjährige Motto „Yes we scan“ lehnte sich pointiert an den populären Wahlkampfslogan von Barack Obama (Yes We Can) an.\nDie Verleihung fand am 25. Oktober 2014 im Rabenhof Theater in Wien statt. Das Motto lautete \"Keine Macht Spionen\"\nDie Verleihung fand am 25. Oktober 2015 im Rabenhof Theater in Wien statt.\nDie Verleihung fand unter dem Motto \"Das Schweigen der Lemminge\" am 25. Oktober 2016 im Rabenhof Theater in Wien statt. Durch die Gala führte eSeL Lorenz Seidler. Showeinlagen kamen von der Band Rammelhof.\nDie Verleihung fand am 25. Oktober 2017 im Rabenhof Theater in Wien statt.\nDie Verleihung fand am 25. Oktober 2018 im Rabenhof Theater in Wien statt.\n\nDie Verleihung fand am 26. Oktober 2000 in der Roten Fabrik in Zürich statt.\nDie Verleihung fand am 26. Oktober 2001 im Clubraum der Roten Fabrik in Zürich statt.\nDie Verleihung fand am 29. Oktober 2002 im Casinotheater Winterthur in Winterthur statt.\nDie Verleihung fand am 1. November 2003 im Dachstock der Reithalle in Bern statt.\nDie Verleihung fand am 16. Oktober 2004 in der Steeltec-Halle in Emmenbrücke statt.\nDie Verleihung fand am 29. Oktober 2005 in der Roten Fabrik in Zürich statt.\nDie Verleihung fand am 16. November 2006 im Kulturzentrum Sudhaus in Basel statt.\nDie Verleihung fand am 9. November 2007 im Palace in St. Gallen statt.\nDie Verleihung fand am 18. Oktober 2008 im Tojo Theater in Bern statt.\nDie Verleihung fand am 24. Oktober 2009 in der Roten Fabrik in Zürich statt.\nDer BBA wurde während 10 Jahren in der Schweiz mangels Ressourcen nicht mehr ausgerichtet.\n\nFür 2019 ist wieder eine Verleihung geplant.\n\n\n"}
{"id": "70978", "url": "https://de.wikipedia.org/wiki?curid=70978", "title": "Microsoft Tablet-PC", "text": "Microsoft Tablet-PC\n\nEin Microsoft Tablet-PC ( ‚Schreibtafel‘, US-engl. ‚Notizblock‘) ist ein tragbarer Tablet-PC mit Microsoft-Betriebssystem. \n\nEin Tablet-PC ist ein spezielles Notebook besonders leichter Ausführung ohne aufklappbare Teile.\nDer Begriff ist nicht markenrechtlich geschützt, es handelt sich also um eine Geräteklasse, die in der Hand gehalten und auch so bedient werden kann.\n\nModerne Designs verzichten auf eine Tastatur, die Eingabe von Daten im Dialog erfolgt durch Berühren der Displayfläche mit Stift oder Fingerspitze. Der berührungsempfindliche Bildschirm stellt entweder eine virtuelle Tastatur bereit oder zeigt Icons und Menüs zur Auswahl. Meist wird ein Tablet PC zusätzlich mit einer Software zur Handschrifterkennung ausgestattet.\n\nDie Idee des stiftbedienten Computers (\"penabled PC\" oder \"pen computer\") gab es schon lange vor den Tablet-PCs. Der Begriff \"Tablet PC\" selbst wurde 2001 von Microsoft eingeführt. Das erste Konzept zu einem tragbaren Computer mit intuitiver Benutzeroberfläche war in den 1960er und 1970er Jahren das geplante Dynabook. Bereits in den 1980er Jahren wurde die erste Software zur Handschrifterkennung von Charles Elbaum entwickelt. Einer der ersten Computer mit echter Stifteingabe war 1993 der Newton von Apple, der Vorläufer der heutigen PDAs.\n\n1991 wurde der Stift das erste Mal als ernsthafte Alternative zur Maus gesehen, worauf z. B. Microsoft die \"Pen Extensions\" für Windows 3.1 auf den Markt brachte. Zwischen 1992 und 1994 veröffentlichten mehrere Hersteller (beispielsweise Compaq, Fujitsu, IBM, NCR, Samsung und Toshiba) passende Hardware, die unter den zwei verfügbaren Betriebssystemen \"Windows for Pen Computers\" oder \"PenPoint\" (von GO Corporation) lief. Das DynaPad T100X von Toshiba war der erste richtige Vorläufer der Tablet-PCs. Das ThinkPad war, wie der Name erahnen lässt, ursprünglich als \"Slate\" geplant. Da diesen Konzepten keine großen Marktchancen eingeräumt wurden, verschwanden sie bei NCR und Samsung jedoch wieder in der Schublade. Pioniere für Stiftanwendungen und -hardware wie GRiD und die GO Corporation wurden aufgekauft oder liquidiert. Compaq, IBM, NEC und Toshiba stellten 1994/1995 alle stiftbasierten Produkte für Endverbraucher ein. Für die Industrie wurden jedoch weiterhin stiftbedienbare Computer hergestellt und verkauft. Ein Beispiel ist das SIMpad (2001).\n\nDa Microsoft weiterhin an einem Konzept von stiftbedienten Computern festhielt, wurden über Jahre bei ihren \"computing in the future\"-Präsentationen immer wieder stiftbasierte Eingabegeräte vorgestellt und letztendlich 2002 die \"Tablet PC\"-Erweiterung für das Betriebssystem Windows XP veröffentlicht. Gleichzeitig wurden verschiedene Gehäusebauformen für Tablet-PCs vorgestellt. Seit dem Start der \"Windows XP Tablet PC Edition\" sind eine Vielzahl von Endgeräten, unter anderem von Dell, Fujitsu-Siemens, Hewlett-Packard, Lenovo und Toshiba, erhältlich. Die ursprüngliche Idee des Tablet-PC nach dem Microsoft-Standard war, dass die Bedienung eines \"Tablet PC\" primär durch Schreiben mit dem Stift auf dem Bildschirm erfolgt. Dabei erfasst ein induktiver Digitizer die Stiftbewegung wie in einem Grafiktablett. Eine Bedienung mit Fingern war am Anfang jedoch nicht vorgesehen, weswegen Digitizer zur Stiftbedienung heute bei Tablet-PCs immer noch üblich sind.\n\nSeit 2005 dürfen nach den Spezifikationen von Microsoft auch Touchscreens eingesetzt werden, die eine Bedienung mit den Fingern ermöglichen (auch in Verbindung mit UMPCs). Hintergrund war, dass die meisten berührungssensitiven Displays anfangs nicht zwischen dem Auflegen der Hand auf dem Display und der Bedienung mit Stift oder Finger unterscheiden konnten, was zu Problemen beim Schreiben führte. Weitere Anforderungen waren, dass das Gerät einen Schnellstart aus dem Standby-Modus beherrschen und dem Legacy-Free-Design entsprechen muss (keine externen seriellen oder parallelen Schnittstellen). Außerdem kann der Bildschirminhalt über eine Taste für Hoch- und Querformat gedreht werden, damit wie auf Schreibblöcken üblich im Hochformat gearbeitet werden kann. Für die Verwendung in Unternehmensumgebungen ist ein Knopf für die Strg-Alt-Entf-Funktion zur Windows-Anmeldung und zum Aufruf des Taskmanagers vorhanden. Als Bauformen waren ursprünglich \"slate\", \"convertible\" oder \"hybrid\" vorgesehen (siehe Abschnitt \"Arten von Tablet-PCs\").\n\nDie Definition von Microsoft beschreibt jedoch nur, welche Anforderungen ein Tablet-PC erfüllen muss, dass dieser mit der \"Tablet PC Edition\" von Windows XP verkauft werden durfte. Die nachfolgenden Versionen von Windows stellten die Tablet-Funktionen für alle Geräte mit einem passenden Eingabegerät bereit.\n\nNeben den Geräten, die auf Microsofts \"Tablet PC\"-Standard basieren, existieren auch Geräte, die mit Linux oder macOS vertrieben werden. Aufgrund der Nutzung einer offenen Hardwarearchitektur sind die Betriebssysteme in der Regel untereinander austauschbar.\n\nNeben den Tablet-PCs hat sich mit den Tablets eine ähnliche Gerätekategorie etabliert, die aus fingerbedienbaren Embedded-Systemen in Slate-Bauform besteht. Diese Geräte basieren auf einer geschlossenen Hardwarearchitektur mit einem festen Betriebssystem, die im Gegensatz zu Tablet-PCs vorrangig zum Medienkonsum gedacht sind. Eingeführt wurde diese Gerätekategorie Anfang 2010 mit dem iPad durch Apple.\n\nTablet-PCs stellen eine Gerätekategorie zwischen Laptops einerseits und PDAs bzw. Smartphones andererseits dar. Tablet-PCs im klassischen Sinne besitzen den gleichen Funktionsumfang wie Notebooks und sind ähnlich aufgebaut. Die Unterschiede bestehen hauptsächlich in der Bauform des Gehäuses und in dem über Stift und Berührung bedienbaren Display. Die restlichen Komponenten wie Prozessor, Festplatte und Arbeitsspeicher entsprechen den üblichen PC-Standards, um die vollständige Kompatibilität zu Software für x86-Prozessoren zu ermöglichen. Neuere Entwicklungen, wie das \"iPad\" basieren hingegen auf der Technologie von Smartphones und übertragen diese auf die Geräteklasse der Tablet-Computer. Da Tablet-PCs vorwiegend für den mobilen Einsatz gedacht sind, entsprechen sie ungefähr der Größe von Subnotebooks oder Netbooks. Gegenüber PDAs ist der größte Unterschied die größere und schwerere Bauform. Außerdem kommt in Tablet-PCs häufig ein Digitizer statt des berührungsempfindlichen Displays in PDAs zum Einsatz.\n\nTablet-PCs können durch Schreiben mit dem Stift auf dem Bildschirm bedient werden, wobei ein induktiver Digitizer die Stiftbewegung erfasst. Im Gegensatz zum Touchscreen ist der Bildschirm dadurch berührungsunempfindlich, das heißt, der Handballen kann beim Schreiben auf dem Bildschirm liegen, ohne die Erkennung der Stiftposition zu stören. Allerdings ist die Bedienung damit im Gegensatz zu einem PDA ausschließlich mit einem passenden Spezialstift möglich. Diese Spezialstifte haben dafür aber Zusatzfunktionen, so lässt sich die Stärke des Drucks und die Stiftneigung beim Schreiben auswerten, und es gibt frei programmierbare Tasten.\n\nDie eingesetzte Technik im Display entspricht der von Grafiktabletts. Dies geht so weit, dass bei zahlreichen aktuellen Tablet-PCs die Erkennungstechnologie von Wacom, einem Spezialisten für Grafiktabletts, integriert ist. Aus diesem Grund können alle Anwendungen, die für den Einsatz mit Grafiktabletts vorgesehen sind, ebenfalls mit Tablet-PCs verwendet werden.\n\nEs kommen zwei Arten von Stiften zur Verwendung:\n\nSeit 2005 können nach den Spezifikationen von Microsoft auch Touchscreens eingesetzt werden, wobei das Display jedoch mehrere Eingaben gleichzeitig verarbeiten können muss. Dies ist nötig, um zu unterscheiden, ob der Benutzer mit dem Stift (oder dem Finger) etwas eingeben will oder nur die Hand ablegt. Für einen Touchscreen kommt ein Eingabestift ohne Elektronik zum Einsatz.\n\nDie drei ursprünglichen Konzepte von Tablet-PCs sind \"convertible\", \"hybrid\" und \"slate\". Seit 2006 existiert mit den UMPCs eine weitere Form tragbarer Computer, die nur mit den Fingern bedient werden.\n\nDie häufigste Bauform für Tablet-PCs ist der \"convertible\" (engl. für \"umwandelbar\"). Dabei hat der Benutzer im Prinzip die gleiche Funktionalität wie bei einem Notebook – insbesondere auch eine Tastatur – und kann zusätzlich das Display um 180 Grad drehen sowie anschließend mit dem Bildschirm nach oben über die Tastatur klappen, so dass der \"convertible\" mit dem Eingabestift wie ein Notizblock benutzt werden kann. Somit vereint diese Bauform die Vorteile von Tablet-PCs und konventionellen Notebooks. Nachteilig wirkt sich das höhere Gewicht und die deutlich größere Höhe des zusammengeklappten Gerätes gegenüber den anderen Bauformen aus, die eine längere Verwendung auf dem Arm erschweren. Dennoch sind \"convertibles\" verbreiteter, weil die eingebaute Tastatur den Benutzern die Sicherheit gibt, jederzeit mit dem gewohnten und bei Texteingabe schnelleren Eingabegerät arbeiten zu können. Der Stift kann aber auch im Notebook-Modus verwendet werden.\n\nAls \"Slate\" (engl. für \"Schiefertafel\") wird ein Tablet-PC bezeichnet, der auf die eigentliche Stiftbedienung beschränkt ist und keine Tastatur eingebaut hat. Damit besteht das Gehäuse im Gegensatz zum Notebook nur aus einem einzigen Block, statt in Tastatur- und Display-Block aufgeteilt zu sein. Durch die kompakte und gewichtsreduzierte Bauform dieser Geräte sind sie sehr mobil und ermöglichen ein längeres Arbeiten auf dem Arm. Das Fehlen der Tastatur ist unproblematisch, da sich alle Tablet-PCs auch ausschließlich durch den Stift mit Schrifterkennung sowie die am Gehäuse angebrachten Funktionstasten bedienen lassen. Bei Bedarf müssen Laufwerke und Tastatur als externe Geräte (üblicherweise über USB/Bluetooth oder eine Dockingstation) angeschlossen werden.\n\nEin Gerät, das äußerlich dieser Bauform entspricht, wurde 2010 von Apple mit dem \"iPad\" vorgestellt. Es unterscheidet sich aber in der fehlenden Stifteingabe und dem Fehlen von Anschlüssen für externe Eingabegeräte. Daneben ist durch die Verwendung eines abgespeckten Betriebssystems (Apple iOS) nur die Nutzung von freigegebenen Anwendungen möglich. Da andere Hersteller anschließend ähnliche Geräte auf den Markt brachten, entstand die neue Gerätekategorie der Tablet-Computer.\n\nEine seltene, aber häufiger werdende Bauform ist der \"Hybrid\" (engl. für \"Mischform\"), der die Eigenschaften von \"Convertible\" und \"Slate\" verbindet. Dabei kann die Tastatur genau wie beim \"Convertible\" hinter dem Bildschirm verstaut werden, ist aber zugleich ganz abnehmbar, was das Gerät in einen „reinen“ \"Slate\" verwandelt.\n\nEin \"Ultra-Mobile PC\" ist kein reiner Tablet-PC, sondern ein vollwertiger tragbarer Computer, der komplett mit den Fingern bedient werden kann und nur wenig größer als ein PDA ist. Auf einem UMPC kann eine Version von \"Windows XP Tablet PC Edition\" oder Windows Vista laufen, da auch ein Stift zur Eingabe verwendet werden kann. Damit ist ein UMPC ein \"Tablet PC\". UMPCs können für viele Aufgaben eingesetzt werden, beispielsweise als Multimediaabspielgerät oder Navigationsgerät mit allen Möglichkeiten einer PC-basierenden Plattform.\n\nZunächst ersetzt der Stift auf dem Tablet-PC die Maus, ähnlich wie bei der Eingabe über ein Grafiktablett an einem herkömmlichen PC. Einfaches Tippen auf dem Bildschirm entspricht einem Mausklick, doppeltes Tippen einem Doppelklick. Der rechte Mausklick wird durch \"tippen und halten\" ähnlich dem \"Pocket PC\" simuliert oder kann auf eine der meist vorhandenen Tasten am Stift gelegt werden. Ein Hauptmerkmal der Tablet-PCs ist die Möglichkeit der Texteingabe mit dem Stift, die jedoch nicht bei allen Tablet-PCs gegeben ist:\n\nDie Idee des (Windows XP) \"Tablet PC\" war, dem Benutzer mit dem Stift auf dem Bildschirm das Gefühl zu geben, er würde mit Stift und Papier arbeiten. Dafür errechnet das System aus der Stiftspur auf dem Bildschirm Bézierkurven, die das Geschriebene mathematisch repräsentieren. Je nach Stift und Fähigkeit des Displays können neben der Position des Stiftes ebenfalls Druck und Stiftneigung ausgewertet werden. Anschließend werden die Kurven gerastert auf dem Display angezeigt. Dadurch entsteht für den Benutzer das Gefühl, er würde auf dem Bildschirm schreiben oder zeichnen.\n\nViele Programme für Tablet-PCs sind in der Lage, diese Freihandeingaben in Text umzuwandeln. Je nach Ansatz geschieht dies automatisch im Hintergrund oder auf Benutzeranforderung hin. Auch die Freihandeingaben sind somit nach Stichworten durchsuchbar beziehungsweise können in anderen Programmen als normaler Text weiterverarbeitet werden. Allerdings eignet sich die Stifteingabe weniger für reine umfangreiche Texteingaben, da der Nutzer mit der Tastatur dabei in der Regel schneller ist. Für komplexe mathematische Formeln, Skizzen oder für das Kommentieren von Dokumenten ist diese Art der Eingabe jedoch von Vorteil.\n\nAls Betriebssystem kam ursprünglich eine erweiterte Version von Windows XP zum Einsatz, die \"Windows XP Tablet PC Edition\". Nach etwa zwei Jahren erschien eine weitere Version, die \"Windows XP Tablet Edition 2005\", die auf dem Windows XP Service Pack 2 basiert. Dabei wurden fast alle Komponenten wie der Tablet-Eingabebereich und die gesamte Handschrifterkennung überarbeitet. Bei dem Nachfolger, Windows Vista, gibt es keine spezielle Version für \"Tablet PC\" mehr, da die \"Tablet PC\"-Funktionen in allen Versionen außer Home Basic und Starter nativ enthalten sind.\nDurch die Standardhardware und die Kompatibilität zu Grafiktabletts kann der Stift auch unter anderen Betriebssystemen, wie beispielsweise Linux, verwendet werden. Die Firma Lycoris beispielsweise bot eine entsprechend angepasste Linux-Version mit den benötigten Treibern und Zusatzfunktionen für die Stifteingabe an. Mac OS X unterstützt die Stifteingabe und Handschrifterkennung nativ, jedoch sind passende Endgeräte bisher nur als modifizierte Versionen der Standard-Hardware von Drittanbietern erhältlich.\n\nUm ohne Tastatur Anwendungen nutzen zu können, die nicht speziell für Tablet-PCs angepasst sind, existiert in allen Betriebssystemen eine \"Freihandeingabeleiste\". Unter Windows erscheint diese, sobald mit dem Stift ein Textfeld ausgewählt wurde, und ermöglicht dem Benutzer die Eingabe von Text per Stift. Dabei kann die Handschrifterkennung oder die Bildschirmtastatur verwendet werden.\n\nAußerdem ist in der \"Tablet PC Edition\" von Windows XP das Programm Windows Journal enthalten, das Schreiben und Zeichnen wie auf einem normalen Block Papier ermöglicht. Dabei stehen alle gewohnten Möglichkeiten von Computerprogrammen zur Verfügung, also durchsuchen, rückgängig machen, kopieren, einfügen (von Bildern und Texten), Format ändern, verschieben, skalieren etc. Der Journal Viewer ermöglicht die Anzeige von Journal-Dateien auf Nicht-Tablet-PCs.\n\nMit Windows Vista wurde die Funktionalität für Tablet-PCs erweitert. Neu sind die Gestenerkennung sowie die Integration einer virtuellen Maus, die auch das Bedienen von sehr kleinen Bildschirmen ermöglicht, wie sie bei UMPCs verwendet werden. Die Handschrifterkennung ist in Windows Vista lernfähig und nicht mehr bloß über die Erweiterung des Wörterbuches anpassbar. Unter Windows Vista werden, im Gegensatz zu Windows XP \"Tablet PC Edition\", ausgeführte Klicks durch aufleuchtende Kreise visualisiert, um eine bessere Rückmeldung zu ermöglichen.\n\nIn Windows 7 hat Microsoft den Eingabebereich überarbeitet und dabei unter anderem die Tasten der virtuellen Tastatur vergrößert und Korrekturgesten eingeführt. Die Handschrifterkennung kann nun vom Anwender auch auf deutsch trainiert werden. Ein Mathematik-Eingabebereich ermöglicht das Schreiben mathematischer Formeln mit dem Stift.\n\nEs existieren Alternativen zu den Erweiterungen von Windows, so besteht mit \"riteScript\" eine Alternative zum Microsoft \"Tablet PC\"-Eingabebereich mit einer eigenen Handschrifterkennung. Mit diesem Programm ist die handschriftliche Texteingabe auf dem kompletten Bildschirm möglich.\n\nDie Handschrifterkennung bei der Eingabe über einen Stift direkt am PC unterscheidet sich deutlich von der Erkennung des Textes bei einem OCR-System. Bei der offline-Texterkennung in einem OCR-System muss erst eine Vorlage in ein digitales gerastertes Bild gewandelt werden, wohingegen die online-Handschrifterkennung in einem Tablet-PC direkt auf die mit einem Stift geschriebenen Kurven zugreifen kann. Damit sind Informationen wie die Geschwindigkeit der einzelnen Abschnitte der Kurvenzüge und die Reihenfolge der einzelnen Kurven verfügbar, die sich bei der offline-Texterkennung nicht mehr rekonstruieren lassen.\n\nDie Kurven werden dabei als Folge von Punkten gespeichert, die eine Reihe von Operationen zur Korrektur (Zeilenausrichtung, Normalisierung der Größe, Ausrichtung der Buchstaben) durchlaufen. Danach werden die Merkmale der Schrift, genauer die Trajektorien der Kurven, extrahiert. Anschließend werden Wahrscheinlichkeiten für verschiedene Wörter mittels Hidden-Markov-Modellen aus der Aneinanderreihung von einzelnen Buchstaben und Gruppen von Buchstaben (N-Gramm), die in ihrem Aussehen voneinander abhängig sind, berechnet. Abhängig von der Qualität der Modelle der Buchstaben(-gruppen) und des Wörterbuchumfangs können anschließend die erkannten Worte ausgegeben werden.\n\nDas Fehlen einer Tastatur bei manchen Modellen erschwert die Verwendung von Tastenkombinationen. Prinzipiell sind viele Tastenkombinationen über die Bildschirmtastatur erreichbar, jedoch gehen die Vorteile wie Schnelligkeit und einfache Erreichbarkeit der dahinterliegenden Funktionen damit verloren. Ähnlich wie Mausgesten, also das Zeichnen von Figuren mit einer Maus oder in diesem Fall dem Stift, kann der Benutzer mit definierten Figuren verschiedene Aktionen auslösen. Einige Programme unterstützen Gesten nativ, oft übernehmen jedoch spezialisierte Programme diese Funktion. Unter Windows Vista werden einige Gesten direkt vom Betriebssystem unterstützt, wie die Navigation in Webseiten (vorheriger/nächster) oder das Scrollen in Dokumenten.\n\nAlternativ bietet die englische Version von Windows XP \"Tablet Edition\" und alle Versionen von Windows Vista zusätzlich die Option der Spracheingabe über das in allen \"Tablet PC\" vorhandene Mikrofon. Im Gegensatz zur eingebauten Spracherkennung in Windows XP \"Tablet PC Edition\" können andere kommerzielle Anwendungen und die Spracherkennung in Windows Vista meist auch trainiert und erweitert werden.\n\nUm den Stift mit allen Vorteilen als Eingabegerät nutzen zu können, müssen die Anwendungen dafür vorbereitet sein.\nDie meisten vektorbasierenden Zeichenprogramme (z. B. CorelDraw oder Inkscape) unterstützen die Stifteingabe. Ebenso kann der Stift in vielen Bildbearbeitungsprogrammen (z. B. Adobe Photoshop) wie ein herkömmliches Grafiktablett verwendet werden.\n\nWeiterhin existieren von Microsoft mehrere Sammlungen von kleinen Softwaretools (\"Microsoft Experience Pack für Tablet PC\" und \"Microsoft Education Pack\") mit verschiedenen Anwendungen, die das Potential von Tablet-PCs zeigen sollen. Dabei sind beispielsweise Programme zum Erkennen von handgeschriebenen mathematischen Formeln, zum Schreiben von Noten, ein Kreuzworträtsel oder ein Programm für Klebezettel auf dem Desktop. Es werden auch spezielle Plug-Ins angeboten, beispielsweise für Microsoft Office, um Anwendungen mit Funktionen für Tablet-PCs zu erweitern und die Produktivität zu steigern. Beispiele hierfür sind \"Tablet Enhancements for Outlook TEO\" und \"InkGestures for Word\".\n\n\nDurch die abnehmende Größe der Geräte und die zunehmende Leistungsfähigkeit von PDAs verwischt die Grenze zwischen beiden Kategorien zunehmend.\n\nEine sogenannte Killerapplikation, die eine deutlich schnellere Verbreitung von Tablet-PCs bewirken würde, stand bis zum Jahr 2009 aber noch aus. Die Verbreitung wurde anfangs durch die sehr hohen Preise gebremst. Der Absatz im dritten Quartal 2003 betrug beispielsweise nur ungefähr 1 Prozent (92.000 Einheiten) aller verkauften Notebooks. Die Preise für die Endgeräte sinken jedoch immer weiter, und lagen 2008 ungefähr auf dem Niveau vergleichbarer Notebooks. Seit 2010 sind Geräte für einen Preis von weniger als 600 € verfügbar.\n\nTablet-PCs ermöglichen eine natürlichere Form der Eingabe – Zeichnen und Schreiben sind ohne Vorkenntnisse in Benutzung von Maus und Tastatur möglich. Die Verwendung von handgeschriebenen Notizen und Skizzen steigert die Produktivität, da alle Vorgänge wie auf Papier durchgeführt werden und digital archiviert werden können. Damit sind alle Dokumente immer verfügbar und gleichzeitig durchsuchbar ohne den Aufwand einer physischen Archivierung von Dokumenten.\n\nDie Gestenerkennung ermöglicht eine große Effizienzsteigerung, da mit einem Stift gegenüber Mausgesten eine genauere Benutzereingabe möglich ist. Die Verwendung des Stifts auf dem Bildschirm ermöglicht ein wesentlich präziseres Arbeiten als mit der Maus. Er ist daher ein gängiges Werkzeug für Künstler/Designer (digitales Malen).\n\n\"Slate\"-Tablet-PCs sind im Vergleich zu herkömmlichen Notebooks wesentlich kleiner und leichter und damit einfacher mitzuführen. Außerdem sind Tablet-PCs die einzigen wirklich \"mobilen Computer\", da sie im Stehen verwendet und mit einer Hand bedient werden können – auf der Unterseite ist bei einigen Modellen eine rutschfeste samtartige Schicht und teilweise ein Halteriemen angebracht, was das Halten vereinfacht. Da das Display für eine ständige Benutzung mit einem Stift und die Berührung mit der Hand ausgelegt ist, sind die Oberflächen meist kratzfest und unempfindlicher gegenüber Schweiß. Bei der Verwendung in Sitzungen oder bei Kundengesprächen unterbrechen sie nicht den Sichtkontakt zwischen Gesprächspartnern, da sie flach auf dem Tisch liegen können.\n\nSeit Ende 2015 gibt es einen neuen Microsoft Tablet-PC namens Microsoft Surface 4. Dieser verkörpert die Eigenschaften eines Notebooks in einem Tablet. Hierbei ist zusätzlich eine Tastatur vorhanden, welche sich ab- bzw. anmontieren lässt. Somit kann man entscheiden, ob man dieses als Tablet oder Notebook nutzen möchte.\n\n2003 waren durch die aufwändigere Konstruktion und den geringen Absatz Tablet-PCs gegenüber gleich ausgestatteten Notebooks noch bis zu 500 Euro teurer, 2010 war dieser Preisunterschied fast verschwunden.\n\nDie Bildschirmgrößen sind durch die Optimierung auf Mobilität beschränkt. Viele Modelle werden als Subnotebooks verkauft und sind nur mit 12\"- oder 13\"-Displays erhältlich, einige Ausnahmen besitzen 14,1\"-Displays. Die Bauart als Subnotebooks und das geringe Gewicht führen dazu, dass besonders die Slate-Modelle oft keine optischen Laufwerke besitzen und teilweise kleinere Standardakkus verwendet werden. Außerdem verbraucht die Rasterung der gezeichneten Kurven im Tablet-Betrieb wesentlich mehr Rechenzeit als das bloße Tippen von Text, was sich besonders bei älteren Modellen negativ auf die Akkulaufzeit auswirkt.\n\nBeim Schreiben mit dem Stift ist die Eingabegeschwindigkeit wesentlich geringer als die maximal mögliche Geschwindigkeit beim geübten Tastschreiben.\n\nConvertible (Computer)\n\n\n"}
{"id": "71254", "url": "https://de.wikipedia.org/wiki?curid=71254", "title": "AOL Instant Messenger", "text": "AOL Instant Messenger\n\nDer AOL Instant Messenger (AIM) war ein Instant-Messaging-Dienst des Unternehmens AOL. Der Dienst nutzte das OSCAR-Protokoll, das auch von ICQ benutzt wurde. Er wurde am 15. Dezember 2017 abgeschaltet.\n\nMit dem AIM war es auch Nicht-AOL-Kunden möglich, sowohl eine AOL-E-Mail-Adresse zu haben, als auch sich in offenen AOL-Chaträumen zu bewegen. Weite Teile der AOL-Chaträume bleiben jedoch exklusiv AOL-Mitgliedern vorbehalten. Weiter war es möglich mit ICQ-Usern zu kommunizieren. Mit der AIM-Userkennung war es auch möglich, aus großen Firmennetzen, in denen intern Lotus Sametime genutzt wird, mit AIM-Nutzern über Firewalls im Internet zu kommunizieren.\n\nAOL, das bis 2010 auch ICQ gehörte, wurde im September 2002 ein US-Software-Patent auf Instant Messaging zugesprochen. Im Oktober 2017 kündigte AOL an, den Instant Messenger am 15. Dezember 2017 einzustellen.\n\nAls die AOL-Software in Deutschland eingeführt wurde, gab es bereits die \"Instant-Message\"-Funktion, die in Deutschland den Namen \"Telegramm\" erhielt, und Chat-Räume in verschiedenen Größen. Bald wurde dies um die \"Buddy List\" erweitert, mit der man ständig aktuell sehen konnte, welche anderen AOL-Nutzer (aus dem Kreis der Buddys) online sind. Als Erweiterung kam schließlich die Funktion AIM mit einer eigenen kostenlos nutzbaren Software hinzu, die es erstmals erlaubte, die beschriebenen Funktionen sowohl mit AOL-Mitgliedern als auch mit AIM-Nutzern, die nicht AOL-Mitglieder sind, zu nutzen.\n\nAm 6. Oktober 2017 wurde bekannt gegeben, dass der AOL Instant Messenger am 15. Dezember 2017 nach über 20 Jahren seinen Dienst einstellt. Der Download von AIM ist bereits nicht mehr möglich.\n\nEine wichtige Funktion des AIM ist die Buddyliste. Mit der Buddyliste kann man sehen, ob Freunde online sind und sie anschreiben.\n\nAIM bietet die Möglichkeit, sich live mit einer Person zu unterhalten, in privaten und öffentlichen Räumen mit mehreren Personen zu chatten und Dateien austauschen (zum Beispiel Bilder, Töne und Videos). Der AIM-Assistent benachrichtigt den Nutzer sofort, wenn Freunde online gehen.\n\nZu den Funktionen gehören auch ein News- und ein Börsenticker.\n\nZusätzlich bekommt man sofort eine Benachrichtigung bei Eingang einer neuen E-Mail, sofern man Kunde bei AOL ist.\n\nEs sind teilweise auch Offline-Messages mit AIM möglich.\n\nIn den Nutzungsbestimmungen von AOL wird die Erstellung und Nutzung alternativer Clients zur Interaktion mit den bereitgestellten Diensten explizit verboten, findet aber trotzdem statt.\nNeben dem offiziellen Client gibt es kompatible Clients von Drittherstellern. Die meisten von ihnen sind Open-Source-Software und unterstützen neben dem AIM-Protokoll noch andere Protokolle wie z. B. ICQ, MSN, IRC und XMPP.\n\n"}
{"id": "71451", "url": "https://de.wikipedia.org/wiki?curid=71451", "title": "Slrn", "text": "Slrn\n\nslrn ist ein textbasierter Open-Source-Newsreader, der ursprünglich für unixoide Betriebssysteme und VMS entwickelt wurde. Mittlerweile steht er aber auch für andere Betriebssysteme, wie Microsoft Windows und OS/2 zur Verfügung. Er ist als freie Software unter der GNU General Public License (GPL) veröffentlicht.\n\nslrn wurde von John E. Davis geschrieben und 1994 veröffentlicht. Die Abkürzung steht für „S-Lang Read News“. S-Lang ist die Skriptsprache, auf der slrn basiert und mit deren Hilfe er vielfältig konfigurierbar ist.\n\n"}
{"id": "71804", "url": "https://de.wikipedia.org/wiki?curid=71804", "title": "Reiser File System", "text": "Reiser File System\n\nReiserFS ist ein Mehrzweck-Dateisystem, das von einer Entwicklergruppe um Hans Reiser in der ihm gehörenden Firma Namesys ab 2001 entwickelt und realisiert wurde. Das Reiser File System unterliegt der General Public License. Die Entwicklung der Version 3 wurde von MP3.com und der SuSE Linux GmbH unterstützt, die Version 4 vor allem von der DARPA und Linspire.\n\nReiserFS war das erste Journaling-Dateisystem, das im Linux-Kernel standardmäßig (ab Kernel-Version 2.4.1) enthalten war. Es wird im Wesentlichen für Logical Volumes oder RAID-Systeme eingesetzt.\n\nZurzeit wird ReiserFS in der Version 3 vom Linux-Kernel vollständig unterstützt. Für FreeBSD gibt es eine experimentelle Unterstützung, bisher nur für Leseoperationen. Kommerzielle Treiber gibt es auch für die Betriebssysteme von Microsoft.\n\nDas ReiserFS basiert auf der von Rudolf Bayer entwickelten Datenstruktur des B-Baums. Das gilt für die Versionen 1 bis 3.\n\nIn Version 3 wurde dem ReiserFS ein Journal hinzugefügt. Ursprünglich war ein Nachteil von ReiserFS gegenüber einigen anderen Journaling-Systemen, dass das Journaling nur für die Metainformationen, d. h. für die Verzeichnisse und Verwaltungssektoren, nicht jedoch für die Nutzdaten in den Dateien selbst angewendet wurde. Dies wurde im 2.6er Kernel behoben.\n\nReiser4 ist vollständig neu entwickelt und sollte nicht mit dem alten ReiserFS verwechselt werden. Daher wird es nicht als „ReiserFS 4“ vertrieben. Es wird eine Abwandlung der B*-Baum-Struktur verwendet, sogenannte \"Dancing Trees\". Der Hauptunterschied besteht darin, dass unzureichend gefüllte Knoten nicht bei jeder Modifikation des Baumes verschmolzen werden, sondern nur dann, wenn durch Speicherknappheit ein Zurückschreiben auf den Festspeicher gefordert wird oder eine Transaktion abgeschlossen wurde.\n\nEinen Geschwindigkeitsvorteil bietet Reiser4, weil es die Daten nicht mehr zweimal auf die Festplatte schreibt – zunächst in das Journal und anschließend in das Dateisystem. Stattdessen speichert es die Nutzdaten in einem wandernden Journal, das heißt sie werden direkt an die vorgesehene Stelle im Dateisystem geschrieben und das Journal bis zum Abschluss des Vorgangs darüber gelegt.\n\nDes Weiteren wurde eine flexible Plug-in-Struktur hinzugefügt, durch die besondere Metadaten-Typen, Verschlüsselung und Komprimierung realisiert werden können. Auf Reiser4 können Metadaten von Dateien, wie beispielsweise Titel und Künstler einer Musikdatei, im Dateisystem gespeichert werden, statt in Anwendungen. Der Unterschied zu beispielsweise ID3-Tags von MP3-Dateien und vergleichbaren Metadatensystemen besteht darin, dass hier ein Metadatensystem im Dateisystem und nicht im Containerformat der Datei integriert wird, das für alle Dateien einheitlich sein könnte. Falls dieser Ansatz jemals vollendet werden sollte, bräuchte man nicht mehr darauf zu achten, ob eine Anwendung alle Metadatentypen versteht, mit denen sie in Berührung kommen könnte. Sie könnte transparent über eine Funktion des Dateisystems auf die Metadaten zugreifen.\nWährend das die Kompatibilität von Anwendungen verbessern könnte, macht es die Kompatibilität von Dateisystemen schwieriger, da die Metadaten nicht auf ein anderes Dateisystem ohne dieses Metadatensystem, wie zum Beispiel ext4, XFS oder FAT32, kopiert werden könnten.\nAuch NTFS unterstützt solche Alternativen Datenströme.\n\nVorteile gegenüber anderen Dateisystemen bietet ReiserFS vor allem bei der Handhabung von vielen kleinen Dateien, da diese in den Verwaltungsknoten (wie bei NTFS in der MFT) gespeichert werden können. Das bedeutet, dass die Dateien im Dateisystem weniger Platz belegen und der Platz auf der Festplatte effizienter genutzt werden kann. Diese Funktionen des Dateisystems lassen sich über die Parameter beim Mounten festlegen. Die bekanntesten Parameter sind:\n\nFerner gibt es von den Entwicklern mitgelieferte Programme zur Verwaltung und Administration des Filesystems, die \"reiserfsprogs\":\n\n\n"}
{"id": "72445", "url": "https://de.wikipedia.org/wiki?curid=72445", "title": "Atari Falcon 030", "text": "Atari Falcon 030\n\nDer Atari Falcon 030 war ein ab 1992 verkaufter Heimcomputer der Atari Corporation.\n\nAls Nachfolger des 1040 STE gedacht, wurde der Falcon jedoch nur in geringen Stückzahlen verkauft.\n\nDer Falcon hatte eine mit 16 MHz getaktete CPU Motorola 68030. Zusätzlich bot er Platz für eine optionale FPU (Gleitkomma-Koprozessor) Motorola 68882.\n\nDer 16-MHz-Prozessor des Atari Falcon 030 war in allen gebauten Rechnern in Wahrheit eine 32-MHz-Version des 68030, da Motorola die langsamere Version nicht liefern konnte. Dies führte dazu, dass findige Bastler ihren Rechner schnell auf knapp das Doppelte beschleunigen konnten. Durch Austausch des Schwingquarzes erreichte man sogar 50 MHz bei Hauptprozessor und DSP bei gleichzeitiger Beschleunigung des Systembusses und dadurch höhere Grafikleistung.\n\nEine Besonderheit des Falcon 030 ist ein mit 32 MHz betriebener digitaler Signalprozessor vom Typ Motorola 56001. Der DSP kann völlig unabhängig von der CPU programmiert werden und machte den Falcon, etwa bei der Audiobearbeitung, sehr leistungsfähig.\n\nDie am häufigsten verkaufte Variante des Falcon wurde mit 4 MB RAM ausgeliefert. Weiterhin waren Modelle mit 1 und 14 MB RAM vorgesehen.\n\nIm Gegensatz zum Atari STE ist das RAM nicht über SIMM-Steckplätze erweiterbar, sondern ist auf einer eigenen Steckkarte aufgelötet. Später wurden Schaltpläne veröffentlicht, die eine Aufrüstung durch handelsübliche PS/2-Speicher zeigten.\n\nZusätzlich zu den vom Atari ST bekannten Bildschirmauflösungen ST-Low (320 × 200 Pixel bei 16 Farben), ST-Medium (640 × 200 bei 4 Farben) und ST-High (640 × 400 Pixel bei 2 Farben) bot er auch VGA (640 × 480 bei 16 oder 256 Farben) und Hi-Color-Auflösungen mit 65536 Farben, die wegen des verhältnismäßig langsamen Video-RAM-Zugriffs wahlweise nur mit eingeschränkter Auflösung oder als Interlace-Modus verfügbar waren. Alle Modi waren per PAL/NTSC-Videosignal auch auf einem Fernsehgerät darstellbar, die höheren Auflösungen aber nur über das Zeilensprungverfahren (\"Interlace\"), das prinzipbedingt mit flimmernder Darstellung verbunden ist. Die Grafikauflösungen des Falcon 030 konnte durch eine etwas später erschienene Softwarelösung noch erhöht werden und erreichte durch eine kleine Hardwaremodifikation sogar Auflösungen von 1280 × 960 Pixel, was Anfang der 1990er Jahre einen guten Wert für den Preis darstellte.\n\nBeliebt war der Rechner in Tonstudios, da er im Gegensatz zu den PCs eine integrierte MIDI-Schnittstelle hatte und einen DSP-Prozessor, mit dessen Hilfe Harddisk Recording in CD-Qualität möglich war. In dieser Rolle wurde er noch bis Ende der 90er Jahre eingesetzt.\n\n\nStandardmäßig wurde der Falcon mit einem 3,5-Zoll HD Floppylaufwerk und einer internen 2,5-Zoll-ATA-Festplatte ausgeliefert.\n\nAn den Joystickbuchsen konnten die Controller-Pads des Atari Jaguar angeschlossen werden, das Standard-Pad war für einige Zeit sogar in grau/blau verfügbar.\n\nSchon früh lieferten andere Anbieter Erweiterungen für den Falcon, die den Prozessortakt und den DSP-Takt erhöhten (z. B. den Skunk32), eine schnellere CPU (z. B. 68040) einsetzten, die Bildschirmauflösung erhöhten oder zum Digitalisieren von Videobildern verwendet werden konnten. Auch 2008 gab es mit der CT-63 eine in Handarbeit gebaute Aufrüstungskarte (engl. „upgrade“), die einen bis auf 100 MHz getakteten 68060-Prozessor und 512 MB RAM bot und damit die Leistung um den Faktor 25 steigerte.\n\nAls Betriebssystem wurde das TOS 4.01 (später 4.02 und 4.04) im ROM mitgeliefert, das eine Erweiterung der älteren TOS-Versionen um Funktionen zur Ansteuerung des DSP und der sonstigen neuen Komponenten ist. Auch die GEM-Oberfläche wurde mit einer 3D-Optik ausgestattet.\n\nDas neue Multitasking-Betriebssystem \"MultiTOS\" wurde von Atari nicht rechtzeitig fertiggestellt und daher separat auf Diskette mitgeliefert. Der Kernel von MultiTOS ist eine Weiterentwicklung von MiNT, einem ursprünglich von Eric R. Smith geschriebenen Unix-ähnlichen Kernel, der als Open-Source veröffentlicht wurde. Die Abkürzung stand für \"MiNT is Not TOS\". Dieser Kernel wurde dann von Atari lizenziert, und fortan stand die Abkürzung für \"MiNT is Now TOS\". Atari passte dann das \"AES\" (siehe: Graphical Environment Manager), also die Funktionalität der grafischen Oberfläche, an die Multitasking-Fähigkeit des neuen Kernels an. Das originale MultiTOS setzte sich jedoch wegen einiger Unzulänglichkeiten und langsamer Reaktionszeiten nicht durch, stattdessen fanden Konkurrenzprodukte anderer Hersteller (MagiC, N.AES) in Deutschland weite Verbreitung.\n\nEs wurden über 100 Spiele veröffentlicht, die auf dem Falcon liefen, davon die meisten als kompatible Atari-ST/STE/TT-Versionen. Die bekanntesten davon sind Bubble Bobble, Defender of the Crown, Dungeon Master, Elite, The Secret of Monkey Island, Ishar-Serie, Robinson’s Requiem und Turrican 2. Reine Falcon-Spiele sind etwa 25 bekannt; größtenteils unbekannte Spiele oder Prototypen, zuzüglich weiterer Fan- und Sharewarespiele. Die bekanntesten Spiele sind Pinball Dreams, die Rennsimulation Vroom, der erste Ego-Shooter auf dem Atari Running und das RPG Towers II.\n\nEine Variante des Falcon 030 sollte die sogenannte \"Microbox\" werden. Ursprünglich geplant war sie als der eigentliche Falcon 030. Diese Variante ist wie ein Mini-Tower konzipiert, wurde aber aufgrund der Verfügbarkeit der Bauteile und der höheren Kosten verworfen. Unterschiede sind der echte 32-Bit-Datenbus, ein Grafikchip mit 256 Farben und mehr Performance. Allerdings blieb es nur beim Prototyp.\n\nDie deutsche Musiksoftwarefirma C-LAB (siehe auch: Emagic) erwarb Anfang der neunziger Jahre die Lizenzrechte für den Bau und Vertrieb des Atari-Falcon, der dann unter dem Namen C-LAB-Falcon (Versionen MK-I, MK-II und MK-X) noch einige Zeit als Computer für MIDI- und Harddisk Recording inklusive technischer Verbesserungen und Zubehör-Hardware produziert wurde.\n\n"}
{"id": "75370", "url": "https://de.wikipedia.org/wiki?curid=75370", "title": "YaST", "text": "YaST\n\nYaST – Yet another Setup Tool (englisch für „noch ein weiteres Installationswerkzeug“) ist ein betriebssystemweites Installations- und Konfigurationswerkzeug, das in allen Versionen von SUSE Linux (openSUSE, SLES, SLED) zum Einsatz kommt. \"SLE\" steht für \"SUSE Linux Enterprise\" und bezeichnet alle \"Enterprise Grade\" Linux Versionen von SUSE, also alle Versionen, bei denen Maintenance und Support käuflich erworben werden können. openSUSE ist die Community Version, bei der es ausschließlich Support über Foren und Community gibt. openSUSE ist auch das Testfeld für die Entwicklung, die jeweils aktuelle stabile Version entspricht dem Stand der Entwicklung bei SLE. \n\nDarüber hinaus ist es Bestandteil von United Linux.\n\nYaST versteht sich als zentrales Werkzeug zur Installation, Konfiguration und Administration einer Linux-Distribution.\n\nDabei besteht YaST aus zwei Teilen:\n\nDas YaST-Kontrollzentrum ist modular aufgebaut. Nach einer Standardinstallation von openSUSE (Stand openSUSE 11.1) sind bereits eine Vielzahl von grundlegenden Modulen in das Kontrollzentrum eingebunden. Diese sind:\n\nDarüber hinaus kann der Benutzer aus einer Vielzahl von weiteren existierenden YaST-Modulen auswählen, und diese in das YaST-Kontrollzentrum einbinden, abhängig davon, welche Dienste er installiert hat und mit YaST verwalten möchte. Beispiele hierfür sind z. B. DHCP-Server, Samba-Server, Mail Server, etc.\n\nDas YaST-Kontrollzentrum besitzt vier verschiedene Benutzerschnittstellen:\n\nDas ncurses-basierte YaST und die grafischen Desktop-Interfaces YaST2 weisen den identischen Funktionsumfang auf. Damit eignet sich die ncurses-Version insbesondere für Server-Administratoren, die Server bequem mit YaST auf der Kommandozeile verwalten wollen, ohne jedoch für YaST2 extra einen grafischen Desktop installieren zu müssen.\n\nYaST wurde ursprünglich entwickelt, um eine deutsch lokalisierte Installation der zu Beginn noch Slackware-basierten SUSE Linux-Distribution zu ermöglichen. Spätestens ab S.u.S.E. Linux 4.2 übernahm jedoch YaST über die eigentliche Installation deutlich hinausgehende Aufgaben, wie Konfiguration von Netzwerk, Systemdiensten und Hardware. Ab der modul-basierten Installation von S.u.S.E. Linux (ab Version 5) spielte YaST eng mit dem Tool linuxrc zusammen.\n\nÄltere YaST-Versionen waren in C/C++ mit der Textoberflächen-Bibliothek curses geschrieben. Die ursprünglichen Autoren von YaST waren Thomas Fehr (einer der vier S.u.S.E.-Gründer) und Michael Andres.\n\nObwohl die bisherige YaST-Lizenz Veränderungen gestattete und somit den Anschein einer freien Lizenz erweckte, erlaubte sie die Weiterverbreitung nur in wenigen Sonderfällen, d. h. nur ohne Entgelt, nur über FTP-Server oder Mailboxen und nur zusammen mit SuSE Linux. YaST wurde daher üblicherweise nicht als freie Software angesehen und war in keiner anderen Linux-Distribution anzutreffen.\n\nNovell, der damalige Besitzer der SUSE Linux Products GmbH, hatte neue YaST-Versionen unter den Bestimmungen der GPL veröffentlicht, was YaST zu Freier Software macht.\n\nDer Lizenzwechsel von YaST hin zu Freier Software ermöglicht fortan die Portierung zu anderen Linux-Distributionen.\nInfolgedessen wurde das \"YaST4Debian\"-Projekt ins Leben gerufen, welches an einer Debian-Portierung arbeitet. Das Projekt, welches auch in Kontakt mit dem YaST development team von Novell/SuSE steht, konnte bereits einige wichtige Meilensteine erreichen, z. B. die Portierung der Module \"yast2-ncurses\" und \"yast2-qt\". In der Zwischenzeit ruht das Projekt jedoch aus Mangel an zeitlichen Ressourcen der fünf bislang am Projekt beteiligten freiwilligen Helfer. Diese rufen allerdings weiterhin zu aktiver Mithilfe weiterer Freiwilliger auf, um das Projekt erfolgreich fortsetzen zu können.\n\nDie Versionsnummern von YaST hatten vor der Version 1.0 immer Primzahlen hinter dem Komma, so beispielsweise 0.43, 0.47 o. Ä.\n\n\n"}
{"id": "75628", "url": "https://de.wikipedia.org/wiki?curid=75628", "title": "Demoszene", "text": "Demoszene\n\nDie Demoszene entwickelte sich unter Anhängern der Computerszene in den 1980er-Jahren während der Blütezeit der 8-Bit-Systeme. Ihre Mitglieder, die häufig \"Demoszener\" oder einfach \"Szener\" genannt werden, erzeugen mit Computerprogrammen auf Rechnern so genannte \"Demos\" – Digitale Kunst, meist in Form von musikalisch unterlegten Echtzeit-Animationen.\n\nMittlerweile ist die Demoszene zu einem etablierten Kreis innerhalb der Computerszene geworden; auf verschiedenen Veranstaltungen stellen sie ihre Werke auf Großleinwänden vor. Langsam aber sicher bahnt sich die Demoszene auch einen Weg in die Museen und auf die progressiven Film- und Videofestivals. Heute wird vorwiegend auf herkömmlichen PCs programmiert, aber auch Dutzende anderer Plattformen – von der Xbox bis zum Game Boy – halten für die Programmierkünste her. Im deutschsprachigen Raum werden für die Produkte dieser Kunstform \"die Demo\" und \"das Demo\" gleichermaßen verwendet.\n\nDie Demoszene ist im Wesentlichen auf Europa beschränkt; in den USA und in Asien existieren nur relativ kleine Ableger.\n\nAbgesehen von Crackintros der Crackerszene selber, wie z. B. heute noch die Gruppen SKIDROW und Razor 1911 diese mehrheitlich für höherwertige PC-Spiele produzieren, legt die heutige Demoszene eher Wert darauf, in keinem Zusammenhang mehr zur Cracker- oder Warez-Szene zu stehen. Noch zu Zeiten des C-64 und Amigas war dies nicht zu trennen, da sich aus den einfachen Intro-Gruß-Laufschriften der Cracker die ersten reinen Demos überhaupt erst entwickelt haben, insbesondere auch mit den Chiptunes die akustische Seite.\n\nWährend sich die Crackerszene jedoch vor allem mit der Verbreitung illegaler Kopien beschäftigt, widmet sich die Demoszene heute mehr dem Kunstaspekt der Computer. Nicht jeder heutige Demoszener war zwangsläufig ein ehemaliger Cracker oder auch nur Teil einer solchen Gruppe, aber viele Mitglieder früherer Cracker-Gruppen sind heute in der Demoszene aktiv. Das liegt unter anderem daran, dass zu einem guten Crack ein kunstvoll gestaltetes Intro gehört.\nDiese Werke ehemaliger Cracker-Gruppen stellen den wichtigsten künstlerischen Einfluss auf die Demoszene dar und begründen auch die Entstehung der heutigen Demoszene.\n\nFrühe Vorläufer, welche Ähnlichkeiten mit den späteren Demoeffekten und deren Motivation des „Spielens“ mit technischen Möglichkeiten aufweisen, sind sogenannte Display hacks, welche bis in die frühen 1950er zurückreichen. Direkte Vorläufer sind die einfachen, ausschließlich aus Text bestehenden Signaturen („Crack-Screens“), mit denen sich Cracker erstmals in den späten 1970er in Apple II-Software verewigten. Graduell wandelten diese sich zu immer aufwändigeren Werken, bestehend aus Graphik, Musik und Animation.\n\nAls die eigentliche Geburtsstunde der Demos wird üblicherweise die Hochkonjunkturphase der 8-Bit-Computer, insbesondere des Commodore 64, abgekürzt C64 (Markteinführung 1982), Atari 800 und Schneider/Amstrad CPC gesehen. In dieser Zeit erschienen massenhaft Computerspiele auf den Markt, die meist mit einem Kopierschutz auf Software-Ebene versehen waren, um eine illegale Vervielfältigung zu verhindern. Daraufhin entstanden sogenannte Cracker-Gruppen, die sich darauf spezialisierten, den Kopierschutz durch Manipulation am Programm zu entfernen. Diese Szene wuchs relativ schnell, und einzelne Gruppen begannen eine Art Wettbewerb darum, wer als erster bei den Spielen den Kopierschutz entfernte bzw. sie \"crackte\" und die kopierbare Fassung (\"cracked\") anschließend innerhalb der eigenen Szene verbreitete (\"released\").\n\nTraditionell wurden die \"gecrackten\" Spiele mit einem Intro versehen, das sich in der Szene auch \"Cracktro\" nannte. Anfänglich handelte es sich hierbei um ein kleines Programm, das vor dem eigentlichen Spiel automatisch startete und meist mit Computermusik und den Namen der Cracker-Gruppe mit Hilfe von Computeranimationen dem Nutzer vorführte. Zusätzlich wurden meist Scrolltexte verwendet, um befreundete Szene-Mitglieder und andere Gruppen zu grüßen (\"Greetings\") und auch Werbung für die eigene Gruppe durch die Veröffentlichung von Kontaktdaten in Form von BBS-Telefonnummern betrieb. Durch den Wettstreit zwischen den einzelnen Gruppen spielte neben der Qualität des Spiels auch die des Intros eine Rolle für das Ansehen der Gruppe. Es wurden immer aufwändigere Intros programmiert, die teilweise die Grenzen der zur Verfügung stehenden Hardware erreichten.\n\nZu diesem Zeitpunkt spezialisierten sich immer mehr Szener auf das eigentliche Programmieren von Intros – das Cracken und die Spiele an sich wurden für sie zur Nebensache. Zwar ersannen die Software-Firmen immer neue Kopierschutzverfahren, aber das Aushebeln dieser softwarebedingten Beschränkungen war relativ simpel im Vergleich zu der Programmierkunst, die für Intros aufgewendet wurde. Der Ruf (\"fame\") von renommierten Cracker-Gruppen gründete sich oft nur noch auf ihre spektakulären Intros. Es entstanden nun die ersten \"Demos\" (\"DemoDisks\") ohne gecracktes Spiel.\n\nSo wurden die Schwerpunkte auch innerhalb der Gruppen neu gesetzt. Mit dem Auftritt der 16-Bit-Systeme, vornehmlich dem Commodore Amiga und Atari ST, wuchsen auch die Möglichkeiten der Intro/Demo-Programmierer. Innerhalb der einzelnen Gruppierungen entstanden feste Aufgabenbereiche: Grafiker sorgen für die visuelle Präsentation, indem sie Bilder, Animationen und Schriftzüge (ASCII-Art etc.) erstellen. Musiker komponieren die Musik und akustischen Effekte, die oft mit Hilfe von Trackern erzeugt wurden. Programmierer (\"Coder\") leisten die hauptsächliche Programmierarbeit und fügen die einzelnen Elemente zu einer fertigen Demo zusammen. Diskmags – elektronische Zeitschriften, die auf Diskette weitergereicht wurden – förderten den Informationsaustausch innerhalb der Szene.\n\nEnde der 1980er und Anfang der 1990er Jahre wurden Schwarzkopien ein immer größeres Problem der Software-Industrie. Sie stellte wegen nicht ausreichender Gewinne teilweise die Entwicklung von Spielen für bestimmte Plattformen komplett ein. Einige beschuldigen die Cracker-Szene sogar, für den Untergang des Amigas maßgeblich mitverantwortlich gewesen zu sein, da dort das illegale Kopieren der Spiele ausuferte; Schwarzkopien vieler Spiele waren schon im Umlauf, bevor sie überhaupt auf dem Markt erschienen.\n\nMit der Dominanz der PC-Plattform und des Internets wurden Cracks problemlos erhältlich und aufgrund der Vielzahl an neuen Programmen und Spielen, Massenware. Anders als in den 1980er-Jahren mussten die Programme nicht mehr per Post versendet oder persönlich getauscht werden – man konnte sie einfach aus dem Internet herunterladen, einschließlich des Cracks. Diese hatten immer seltener eine Intro, meist waren die Informationen zum Crack und die \"Greetings\" nur noch in einer simplen Textdatei untergebracht. Auch die Behörden beschäftigten sich zunehmend mit dem Kampf gegen Schwarzkopien: Hausdurchsuchungen und Beschlagnahmungen der Hard- und Software und neben einzelnen Szenern wurden auch ganze Gruppen verhaftet. Neben anderen Interessen war das ein weiterer wichtiger Grund für die Abspaltung der Demoszene von den Cracker-Gruppen. Es entstanden verstärkt reine \"Demos\", also die Veröffentlichung von Computerkunst ohne Verbreitung eines (illegal kopierten) Spiels, und eine deutlichere Differenzierung/Emanzipation der Demoszene von der Crackszene.\n\nDie eigentliche Demoszene entwickelte sich weiter, vor allem in Nordeuropa und Skandinavien wuchs die Szene stark. Mitte der 1990er Jahre etablierte sie sich mit Veranstaltungen wie der finnischen Assembly, der norwegischen Gathering und der deutschen Mekka & Symposium, auf denen die besten Demos vorgeführt und gekürt wurden, mit jeweils vielen Tausend Besuchern. Es entstanden Online-Publikationen spezialisiert auf die Demoszene und deren Technologie, wie z. B. \"Hugi\" oder \"TraxWeekly\" ein Tracker-Magazin.\n\nMitte der 90er wurde die Demoszene zu einer der treibenden Kräfte für die Entwicklung der PC-Plattform in Richtung Multimedia, was auch von den Hardwareherstellen erkannt wurde und zur Unterstützung von Demo-Veranstaltungen (z. B. der \"Assembly\") durch Firmen wie Gravis oder AMD führte. Die Soundkarte von Gravis, die Ultrasound, wurde von der Demoszene entdeckt und dadurch erst breiter bekannt.\nDie Bedeutung zeigt sich beispielsweise 1999 in der vom Online-Magazin Slashdot initiierten retrospektiven Wahl des 1993 erschienenen Future Crew Demo \"Second Reality\" unter die \"„Top 10 Hacks of All Time“\".\n\nZu Ende 1990er und Anfang 2000er Jahre wurden viele neue Firmen von oder mit Demoszenern gegründet, die ihre Konzepte und Technologien einbrachten. Ein Beispiel ist die finnische Firma Remedy Entertainment, bekannt für die Max Payne Spiel-Serie, welche von Future Crew Mitgliedern gegründet wurde, und von der die meisten Angestellten ehemalige Demoszener sind.\n\nIn den 2000er Jahren entdeckte die Demoszene die mobilen Geräte mit begrenzter Rechenleistung als neues Spielfeld für Echtzeit-Demos. Parallel dazu wurden zunehmend selbstbegrenzende Kategorien für Demos auf dem PC eingeführt, da \"natürliche\" Hardwaregrenzen (z. B. Diskettengröße) mit wachsender Rechenleistung und Speicherplatz nicht mehr existierten. Eine populäre Kategorie hier sind die z. B. \"64k Demos\", Demos welche mit Klang und Grafik eine Speichergröße von 64kByte nicht überschreiten. Hierzu neuentwickelte Technologien sind die prozedurale Generierung, \".kkrieger\" als beispielhaftes, preisgekröntes Demo hierzu von \"Farbrausch\".\n\nAuch begann sich die Szene professioneller zu organisieren: In Deutschland fördert seit 2003 der eingetragene gemeinnützige Verein „Digitale Kultur“ die Demoszene, insbesondere durch Organisation der Evoke und Beschaffung von Sponsoren und Geldern. Ein eigens dafür entwickeltes CMS namens \"PartyMeister\" wird von der Demoszene für die Organisation von einigen Demopartys eingesetzt, beispielsweise die \"tUM\" oder die \"Buenzli\".\n\nSeit 2003 verleiht \"Scene.org\", eine weitere, weltweit agierende Organisation der Demoszene, die \"Scene.org Awards\", u. a. gesponsert durch Pixar. Die \"Scene.org Awards\" werden jährlich an die Ersteller der besten Intros und Demos des vorhergehenden Jahres vergeben. Die Gewinner werden (mit Ausnahme des Publikumspreises) von einer Jury aus angesehenen Demoszenern aus aller Welt ausgewählt. Die Preisvergabe fand bis 2010 auf der Breakpoint, der weltgrößten, reinen Demoparty, statt, seit 2011 auf der \"The Gathering\", zeitgleich mit der \"Revision\" in Saarbrücken.\n\nGrundlagen\n\nDie Anforderungen an eine gute Demo hängen maßgeblich von der zur Verfügung stehenden Hardware ab. Hier besteht ein wesentlicher Unterschied zwischen dem heute üblichen PC-System und älteren Plattformen wie dem Commodore 64, aber auch Konsolen wie der Sony Playstation oder dem Nintendo Game Boy.\n\nDie Leistung eines Systems ist vor allem von Hauptprozessor, Arbeitsspeicher sowie Grafik- und Soundprozessoren abhängig. Bei nicht-PC-Systemen sind diese Komponenten meist festgelegt und nicht austauschbar. Eine beispielsweise für den Commodore 64 programmierte Demo lief auf allen Rechnern gleich schnell. Bei einer Demo für den PC sieht das anders aus, viele aktuelle PC-Demos laufen nur auf High-End-Systemen.\n\nInsbesondere beim Arbeitsspeicher gibt es enorme Unterschiede zwischen älteren Systemen und heutigen PCs. Während ein Commodore 64 mit 65.535 Byte Arbeitsspeicher auskommen musste (zum Vergleich: dieser Wikipedia-Artikel inklusive aller Bilder würde schon nicht mehr in seinen Speicher passen), sind heutzutage Arbeitsspeicher von mehreren Gigabyte keine Seltenheit. Deshalb wurden für Demos für die PC-Architektur, aber auch andere Hardware-Plattformen (Amiga, C-64, Mobile Geräte…), Kategorien mit definierter maximaler Speichernutzung eingeführt, um die verschiedenen Demos vergleichbarer zu machen (→ Abschnitt Wettbewerbe in Partys).\n\nKritik am PC\n\nViele Szener aus der Frühzeit der Demoszene missbilligen heutige PC-Demos, insbesondere solche aus der „Unlimited“-Kategorie, also ohne eine Beschränkung des Hauptspeichers oder der Programmgröße. Ihr Vorwurf ist, dass man durch die fortschrittlichen technischen Rahmenbedingungen komplexe Grafik wie Texturen oder gerenderte 3D-Modelle komplett in den Arbeitsspeicher einlesen kann, während man früher aufgrund des sehr kleinen Speichers gezwungen war, solche Details während des Ablaufs (Laufzeit) der Demo in Echtzeit berechnen zu lassen.\n\nFür diese Kritiker besteht eine der wichtigsten Herausforderungen der Demo-Programmierung darin, durch Programmiertricks und ausgefeiltere Algorithmen immer mehr das Limit der Hardwarebeschränkungen auszureizen. Bei PC-Demos besteht aufgrund des Wegfalls dieser Beschränkungen darin keine Herausforderung mehr. Hierin liegt auch eine der Hauptursachen für die in den vergangenen Jahren stark zugenommene Aufmerksamkeit gegenüber Intros mit maximal 64 KB, 32 KB, 4 KB oder gar 256 Byte.\n\nZielsetzungen der PC-Demoszene\nDie PC-Demoszene sieht sich allerdings ganz anderen Herausforderungen gegenübergestellt, etwa der Optimierung neuer Techniken, beispielsweise des Pixel-Shadings. Durch die rasante Entwicklung der Technik hat sich insbesondere bei den jüngeren Mitgliedern der Demoszene eine Verlagerung vom Ausreizen der Technik („Pushing the Limits“) hin zur künstlerischen Gestaltung („Artcreation“) vollzogen. Während die Hardwarebegrenzungen der Vergangenheit dazu führten, dass die theoretisch formulierten Grenzen einer Hardware immer weiter hinausgeschoben werden (z. B. in Form von 3D-Umgebungen in Echtzeitberechnung auf dem C64), legte man auf dem PC den Schwerpunkt vielmehr auf die ästhetische Mediengestaltung und nutzt dabei die jeweils neu verfügbaren PC-Erweiterungen.\n\nFür die Anhänger älterer Plattformen bringt aber auch die Weiterentwicklung der Technik neue Herausforderungen. Längst haben sie mobile Endgeräte wie PDAs, MDAs und Mobiltelefone für ihre Zwecke entdeckt und führen auch darauf ihre Demos vor.\n\nProgrammier-Grundlagen\n\nEntsprechend der großen Hardwareunterschiede sind die Anforderungen an die Programmiertechnik für eine gute Demo grundsätzlich verschieden. Das Hauptproblem für Programmierer auf 8-Bit-Systemen war, mit dem begrenzten Arbeitsspeicher auszukommen. Bei einem Commodore 64 etwa standen 64 Kilobyte Hauptspeicher zur Verfügung. Um ein Programm möglichst klein zu halten, griff man auf solchen Rechnern auf Assembler als Programmiersprache zurück. Das hatte den zusätzlichen Vorteil, dass Routinen wie Grafikberechnung wesentlich schneller abliefen als beispielsweise mit dem im C64 eingebauten Basic-Interpreter.\n\nUm eine effektreiche und imposante Demo zu erstellen, besteht die Herausforderung vor allem darin, schnelle Algorithmen zu finden und sie platzsparend zu programmieren. Zudem entdecken findige Hacker immer wieder Tricks, um etwa vom System reservierten Speicher zu benutzen. Eine der ersten bahnbrechenden Methoden war der Rasterzeileninterrupt, der viele Effekte erst ermöglichte.\n\nEin aktuelleres Beispiel für eine Grenzleistung dieses „Pushing-the-Limits-Paradigma“ ist die Demo \"ShiZZle\" vom Team Pokémé. Diese für die kleine Handheld-Konsole \"Pokémon Mini\" programmierte Demo gewann die „Wild-Competition“ auf der Breakpoint 2005. Das Gerät hat nur zwei Kilobyte RAM, 768 Byte Videospeicher, ein 96 × 64 px großes Monochrom-Display und eine 8-bit-CPU. Die Demo wurde von den Besuchern trotzdem als eine der besten Demos überhaupt gefeiert.\n\nIm PC-Sektor besteht die Programmierkunst mehr in der Anwendung schon vorhandener Grafikbibliotheken wie der DirectX- oder OpenGL-Schnittstelle. Spezielle Routinen für Beleuchtungs-, Licht- und Schatteneffekte sowie spezielle Techniken wie Pixel-Shader ermöglichen es den Programmierern, immer eindrucksvollere Demos herzustellen.\n\nAufbau klassischer Demos\n\nUrsprüngliche Intros hatten zunächst einen relativ festen Aufbau: Musikalisch und mit Effekten hinterlegt wurden verschiedene visuelle Effekte gezeigt, etwa rotierende Sternenhimmel, allgemeine Spielereien mit Formen und Farben oder virtuelle Flüge durch imaginäre Landschaften. Im Vordergrund standen meist Textbotschaften, die oft durch Scrolltexte realisiert wurden. In diesen Texten wurden vor allem die Beteiligten der Intros aufgelistet („Credits“) und befreundete Szener und Gruppen gegrüßt („Greetings“).\n\nZusammenstellung zu Megademos\n\nFür die ersten unabhängigen Werke, die anfänglich noch als Intros ohne illegales Spiel dahinter gestaltet wurden, setzte sich schnell der Begriff \"Screen\" durch, auch zur Abgrenzung der \"Cracktros\" der illegalen Crackerszene. Da ohne Spiel eine weitaus größere Menge an Speicherplatz auf dem Datenträger zur Verfügung stand, wurden recht bald mehrere Screens zu einer \"Megademo\" verbunden.\n\nDie Auswahl der Bestandteile einer Megademo fand dabei für gewöhnlich über ein einfaches (seltener auch aufwändigeres) interaktives Auswahlmenü statt. Da das Hin- und Herschalten zwischen Menüs jedoch oft als mühsam und als unnötige Unterbrechung empfunden wurde, galten solche Menüs schnell als verpönt. Es setzte sich mit der Zeit eine Aufhebung der Trennung durch, und die Screens wurden zu einer festen Reihenfolge verbunden. Nach und nach bildete sich ein stimmiger Gesamtzusammenhang heraus, mit einem gemeinsamen Thema und einem schlüssigen Ablauf. Die ersten Demos in der heute üblichen Form entstanden.\n\nDie meisten Demo-Künstler organisieren sich in Gruppen Gleichgesinnter und arbeiten auf dieser Basis in Projektteams zusammen.\n\nWichtige Gruppen auf dem PC in den 1990er Jahren waren \"Future Crew\" (Second Reality) oder \"Triton\" (FastTracker), welche auch organisatorische Aufgaben für die gesamte Szene leisteten, wie die Organisation von Partys. Zu den bekanntesten in den 2000ern aktiven Gruppen aus Deutschland zählen beispielsweise \"Farbrausch\" oder \"Haujobb\". Innerhalb dieser Gruppierungen leisten die einzelnen Mitglieder ihren Teil in unterschiedlichen Aufgabenbereichen. Die Gruppenzugehörigkeit der an einem Projekt Beteiligten wird heute weniger wichtig genommen als früher; harte Konkurrenz, die einstmals bis in meist freundliche Auseinandersetzungen („Wars“) ausarten konnte, ist heute weitgehend einem Geist vielfacher Kooperation gewichen. Gruppennamen nehmen die Rollen von Produktmarken ein, die für bestimmte Stilrichtungen bzw. ein gewisses Qualitätsniveau stehen.\n\nEin wichtiger Bestandteil der Demoszene sind Demopartys – Veranstaltungen, auf denen die Szene für mehrere Tage zusammenkommt, die besten Demos der Gruppen auf Großleinwänden zeigt und prämiert. Im Unterschied zu einer LAN-Party steht bei einer Demoparty die Programmiertechnik und Computerkunst im Vordergrund. Auch wenn sich die Grenzen solcher Veranstaltungen teilweise mischen, wie bei der finnischen \"Assembly\" oder der norwegischen The Gathering, den größten derartigen Partys weltweit, wollen sich die Demoszener von den \"Gamern\" auf LAN-Partys grundsätzlich abheben. Gamer gelten auf vielen Veranstaltungen, beispielsweise den größten deutschen Veranstaltungen \"Revision\" und \"Evoke\", als verpönt. Wenngleich hier gelegentliches Spielen toleriert wird, legt man Wert darauf, dass die Erstellung und Vorführung der Demos im Vordergrund steht.\n\nNeben den bereits genannten Demopartys besitzen auch die \"Buenzli\" in der Schweiz und \"the Ultimate Meeting\" in Deutschland eine größere Bedeutung.\n\nEinladung\n\nEs bürgerte sich ein, dass eine der einladenden und organisierenden Demogruppen eine digitale Einladung in Form einer kleinen Demo, genannt \"Invitation\", mit den relevanten Informationen über Ablauf, Zeitplanung und Wettbewerben einer Party veröffentlichte.\n\nWettbewerbe\n\nMittelpunkt jeder Demoparty sind die Wettbewerbe und Vorführungen der Demos, so genannte \"Competitions\" oder kurz \"Compos\". Hier messen sich sowohl Gruppen als auch Einzelkünstler in verschiedenen Kategorien. Abgestimmt wird von den anwesenden Besuchern. Es ist durchaus üblich, Beiträge zu den \"Compos\" noch auf der Party fertigzustellen. Das \"Party Coding\" bis zum Stichtag wird trotz enormen Stresses als besondere Erfahrung angesehen, die für viele Szener fest zur Demoerstellung gehört.\n\nDamit ein Vergleich der eingereichten Demos möglich ist, werden die Wettbewerbe in verschiedene Kategorien unterteilt. Für besonders beliebte Plattformen wie dem Amiga oder Commodore 64 gibt es oft einen eigenen Wettbewerb. Für PC-Systeme unterteilt man traditionell in eine Speicherbegrenzung von 64, 16 oder 4 Kilobyte. Für alle anderen Plattformen – je exotischer, desto besser – gibt es die „Wild“-Kategorie. Ferner gibt es Unterkategorien wie \"reine Animation\", \"Rendering\", \"bestes Musikstück\" oder sogar \"beste ASCII-Art\".\n\nAuch Browser-Demos, Flash-Demos oder Java-Demos können in eigenen Wettbewerben antreten.\n\n\n\nInterviews und Berichte\n"}
{"id": "76021", "url": "https://de.wikipedia.org/wiki?curid=76021", "title": "Scancode", "text": "Scancode\n\nEin Scancode ist in der Computertechnik eine Nummer, die von der Tastatur eines Rechners an diesen gesendet wird, wenn eine Taste gedrückt oder losgelassen wird.\n\nIm Jahr 1981 hat IBM zusammen mit dem ersten IBM-PC eine Tastatur mit 83 Tasten eingeführt. Da diese auch beim 1983 erschienenen IBM PC/XT unverändert beibehalten wurde, ist sie heute als \"XT-Tastatur\" bekannt.\n\nFrühere Computertastaturen sendeten oft direkt ASCII oder Codes einer anderen Zeichensatznorm an den Rechner. Um aber beim für den Vertrieb in vielen Ländern geplanten IBM PC nicht in jeder länderspezifischen Tastaturvariante andere Codes generieren zu müssen, wurde jeder Taste eine Nummer zugeordnet, der sog. \"Scancode\". Dem Scancode wurde dann erst im Rechner vom Tastaturtreiber ein ASCII-Code zugeordnet. Für den Scancode wurden die Tasten einfach nach ihrer Lage auf der Tastatur reihenweise von links nach rechts durchnummeriert, im Gegensatz zum ASCII-Code, der der alphabetischen Anordnung folgt.\n\nEin Standard-Tastaturtreiber für die US-amerikanische Belegung war bei jedem PC im BIOS integriert, länderspezifische Zuordnungen erledigte ein Software-Tastaturtreiber, der als TSR-Programm beim Systemstart nachgeladen wurde. Unter MS-DOS heißt dieses Programm \"KEYB.COM\", andere Betriebssysteme verwendeten oft den gleichen Namen.\n\nSomit musste IBM bei den PC-Tastaturen für den Vertrieb in verschiedenen Ländern nur unterschiedlich bedruckte Tastenkappen einbauen, während die Elektronik der Tastatur gleich bleiben konnte und sich am Rechner selbst überhaupt nichts ändern musste.\n\nBei der XT-Tastatur gab es links einen Block von 10 Funktionstasten und rechts die alphanumerischen Tasten mit angegliedertem Zahlenblock. LEDs zur Signalisierung der aktuellen Zustände und einen separaten Pfeiltastenblock gab es noch nicht. Die Kommunikation geschah ausschließlich von der Tastatur zum PC, der PC konnte seinerseits also noch keine Befehle an die Tastatur schicken. Der prinzipielle Aufbau der seriellen Schnittstelle ist unter PS/2-Schnittstelle beschrieben; der typische Steckverbinder für PC- und AT-Tastaturen war allerdings über lange Jahre ein 5-poliger DIN-Steckverbinder, den man in Deutschland als Stecker für NF-Signale kannte („Überspielkabel“).\n\nDie logische Weiterentwicklung war die 1984 mit dem IBM PC/AT eingeführte \"AT-Tastatur\" mit 84 Tasten (abgesetzten Funktionstasten und 10er Block). Hier wurde die physikalische Schnittstelle beibehalten, jedoch wurde die Signalisierung komplett neu gestaltet. Die Kommunikation zwischen Rechner und Tastatur war nun bidirektional, erkennbar beispielsweise daran, dass der Rechner die Status-LEDs programmgesteuert schalten konnte. Die übertragenen Codes waren völlig andere als beim XT. Ein kleiner Ein-Chip-Rechner (Intel 8042) auf der Hauptplatine des Computers, etwas missverständlich „keyboard BIOS“ genannt, setzte die tatsächlich übertragenen Codes auf kompatible Werte um. Für die Software sah eine AT-Tastatur (mit ihrem vorgeschalteten „keyboard BIOS“) einer XT-Tastatur sehr ähnlich; so konnte die so wichtige Kompatibilität der Rechner erhalten bleiben. Die Umsetzung der Codes war abschaltbar, und IBM rechnete wohl damit, dass sie normalerweise abgeschaltet sein würde, sobald ein moderneres Betriebssystem verfügbar wurde; in der Praxis wurde aber weiterhin fast überall MS-DOS bzw. PC-DOS verwendet und die Umsetzung war praktisch immer eingeschaltet.\n\nDer Umbau eines XT auf AT erforderte meist den Austausch der Tastatur; es gab aber auch Tastaturen, die einen Umschalter auf der Unterseite hatten oder sich automatisch passend umschalten konnten.\n\nDer nächste Generationswechsel vollzog sich 1986, als IBM die 101 Tasten große MF2-Tastatur (\"multi-functional\") für den US-Markt herausbrachte. Analog wurde die länderspezifische 102 Tasten große MF2-Tastatur definiert. Letztlich basieren auf diesen beiden Tastaturbelegungen alle heute üblichen Erweiterungen, wie beispielsweise die 104/105-Tasten-Windows-Tastatur.\n\nDie MF2-Tastatur verwendete die gleiche Schnittstelle und Signalisierung wie die vorherige PC/AT-Tastatur. Ab 1987 wurde der Stecker für die IBM PS/2 Computer durch eine kleinere Variante ersetzt, die elektrischen Signale blieben aber die gleichen.\n\nDie Anordnung der Tasten wurde dahingehend geändert, dass nun die Funktionstasten, deren Anzahl zugleich um zwei erhöht wurde, oberhalb der alphanumerischen Tasten liegen. Der Zehnerblock und spezielle Tasten zur Cursorbewegung wurden optisch getrennt.\n\nGrundsätzlich wird beim Drücken einer Taste der entsprechende \"Scancode\" der Taste gesendet. Bei längerem Drücken wird der gleiche Scancode wiederholt gesendet und beim Loslassen wird ein sog. \"Break-Code $F0\" dem Scancode vorangesetzt.\n\nDas „keyboard BIOS“ setzt diese Werte allerdings weiterhin um. Nach dieser Umsetzung ist in der Regel beim \"make code\" das oberste Bit 0, beim \"break code\" aber 1.\n\nBeispiel: Die Leertaste hat nach der Umsetzung den \"make code\" $39 und den \"break code\" $B9.\n\nUm mit einer MF2-Tastatur kompatibel zur XT-Tastatur zu sein, bekamen die Tasten des neuen Cursorblocks (nach der Umsetzung) den gleichen Code mit einem speziellen Code \"$E0\" oder \"$E1\" zur Unterscheidung davorgesetzt. Alte Software ignorierte gewöhnlich den ihr unbekannten Code $E0 bzw. $E1 und verarbeitete nur den folgenden Code, der der gleiche war wie bei der jeweils entsprechenden Taste des alten kombinierten Ziffern/Cursorblocks. Neue Software konnte dagegen den Code $E0 bzw. $E1 bei Bedarf erkennen und wusste dann, dass eine Taste des neuen Cursorblocks und nicht eine des kombinierten Ziffern/Cursorblocks gedrückt worden war.\n\nDie amerikanische MF2-Tastatur hat 101 Tasten; die länderspezifische 102 Tasten. Die zusätzliche Taste trägt auf deutschen Tastaturen die drei Zeichen „<“, „>“ und „|“, die linke Umschalttaste ist dafür verkleinert.\n\n\nDie Codetabelle der PC-Tastatur ist komplex, innerhalb der MFM-Tastatur selbst existieren drei Tabellen mit Scancodes, welche durch das Senden von Befehlscodes an die Tastatur ausgewählt werden können.\n\n\nZusätzlich kann die Umwandlung von Tastaturcodes durch das im Rechnergehäuse befindliche „Keyboard BIOS“ ein- und ausgeschaltet werden (heutiger Standard: eingeschaltet); damit ergeben sich aus Sicht der Firmware und Software, die die Codes letztlich empfangen, insgesamt sechs mögliche Codevarianten.\n\nTastaturen anderer Hersteller als IBM haben teilweise die Scan Code Sets 1 und 3 nicht oder fehlerhaft umgesetzt. Ebenso funktioniert die Abschaltung der Scancodeumwandlung auf manchen Mainboards nicht korrekt. Auch die existierenden PC-Emulatoren implementieren diese Varianten nicht. Daher verwenden alle heutigen PC-Betriebssysteme ausschließlich das Scan Code Set 2 mit eingeschalteter Umwandlung. Der Controller im Keyboard überträgt die Codes zum \"Keyboard BIOS\" im PC mit Scancode Set 2, und dieses setzt sie dann um auf Scancode Set 1. Die Software bekommt daher auf allen PCs gewöhnlich nur die älteste Variante, Set 1, zu Gesicht – auf dem Ur-PC und auf XTs direkt, auf ATs und allen neueren PCs per Umsetzung.\n\nUSB-Tastaturen verwenden einen neuen Satz von Scancodes ohne Beziehung zu den verschiedenen älteren Systemen; die entsprechenden Treiber neuer Betriebssysteme können diese Codes verarbeiten. Für die Verwendung mit Bootloadern und älteren Betriebssystemen werden deren Scancodes vom BIOS des Rechners softwaremäßig auf die alten Scancodes abgebildet. Dabei wird ebenfalls nur das Scan Code Set 2 mit eingeschalteter Umwandlung emuliert. Manche BIOSe führen diese Umwandlung unvollständig durch, so wird z. B. teilweise die AltGr-Taste nicht von der Alt-Taste unterschieden, was für deutsche Tastaturen problematisch ist.\n\nDie Scancodes für USB-Tastaturen sind architekturübergreifend einheitlich. USB-Tastaturen für die Apple-Macintosh-Serie verwenden also die gleichen Scancodes wie PC-USB-Tastaturen.\n\n"}
{"id": "76306", "url": "https://de.wikipedia.org/wiki?curid=76306", "title": "Commodore Plus/4", "text": "Commodore Plus/4\n\nDer Commodore Plus/4 ist ein auf dem 7501- bzw. 8501-Mikroprozessor basierender Heimcomputer des US-amerikanischen Herstellers Commodore International.\n\nDas Gerät wurde ab Mitte 1983 unter der Bezeichnung \"Commodore 264\" zunächst als preiswerte Ergänzung zu den erfolgreichen Heimcomputern Commodore VC 20 und Commodore 64 konzipiert. Zudem sollten mit dem neuen Modell hauptsächlich dem Sinclair ZX Spectrum Marktanteile abgenommen werden. Die technischen und gestalterischen Daten des Computers hatten sich dabei insbesondere den von der Firmenleitung vorgegebenen maximalen Herstellungskosten von 50 US-Dollar unterzuordnen. Daraufhin wurde mithilfe des Spezialbausteins \"TED\" die Anzahl der elektronischen Schaltkreise deutlich reduziert und ein kleineres Gehäuse entworfen, ohne jedoch die Abwärtskompatibilität zu Commodore VC 20 und 64 zu wahren. Noch während der Entwicklung des TEDs beschloss Commodore eine Neuausrichtung des Produkts hin zu einem anwendungsorientierten Gerät für kleinere Büros. Die damit verbundene Aufrüstung auf 64 Kilobyte (KB) Arbeitsspeicher und die Implementierung der fest verbauten Anwendungssoftware \"3-plus-1\" führten schließlich neben einer beträchtlichen Verteuerung auch zur Umbenennung des Computers in \"Commodore Plus/4\".\n\nDas ab Herbst 1984 zunächst nur in Nordamerika ausgelieferte Gerät wurde als „Productivity Computer“ (Heimanwendercomputer) beworben und für 299 US-Dollar im Einzelhandel angeboten. Kurz darauf kamen weitere Absatzmärkte wie beispielsweise Europa hinzu. Aufgrund des schlechtgehenden Verkaufs folgten rasch teils drastische Preisnachlässe, die bereits nach Weihnachten 1985 in den vollständigen Ausverkauf durch Commodore mündeten. Bekanntheit erlangte der Rechner in Westdeutschland hauptsächlich durch den von Aldi ab 1986 im Abverkauf angebotenen \"Computer-Lernkurs\", ein Paket bestehend aus Commodore Plus/4 nebst Datenrekorder und zusätzlicher Einsteigerliteratur.\n\nDurch die geringe Hard- und Softwarekompatibilität zu den Vorgängermodellen, fehlende Grafikfähigkeiten für den Spiele-Bereich (Sprites) und nur eingeschränkte Tonerzeugung war dem Commodore Plus/4 trotz anderweitig guter technischer Kennzahlen und leistungsfähiger Peripheriegeräte kein großer kommerzieller Erfolg beschieden. Zum ursprünglichen Ziel, Sinclair die Marktführerschaft zu entreißen, konnte der Commodore Plus/4 mit lediglich rund 830.000 weltweit verkauften Geräten im Gegensatz zum Commodore 64 nur einen kleinen Teil beitragen. Die Produktion – auch der technisch abgerüsteten Einsteigervarianten Commodore 16 und Commodore 116 – wurde nach schrittweisen Drosselungen bereits 1985 eingestellt, da die Verkaufszahlen des älteren, aber besser unterstützten Commodore 64 unerwartete Höhen erreichten. Zudem standen mit firmeneigenen Modellen wie dem Commodore 128 und dem Amiga 1000, aber auch der ST-Reihe des direkten Konkurrenten Atari mittlerweile deutlich leistungsfähigere Nachfolger zur Verfügung.\n\nWährend der boomenden Heimcomputerära im Jahre 1983 war Commodores günstiges Einsteigermodell VC 20 von 1981 bereits veraltet und der Ende 1982 aufgelegte erfolgreiche Commodore 64 bediente lediglich das höhere Preissegment. Zur Erschließung des bislang nicht zugänglichen prosperierenden Niedrigpreisbereiches rund um den Sinclair Spectrum initiierte die Firmenleitung 1983 den Bau eines neuen Computers.\n\nDas auf der bewährten 8-Bit-Architektur basierende geplante neue Produkt hatte den Anweisungen der Firmenleitung folgend mit einem Minimum an Elektronik auszukommen. Damit sollte einerseits die Senkung der Produktionskosten auf 50 $ und andererseits die Verwendung eines kleineren Gehäuses ermöglicht werden – wichtige Kennzahlen, um mit einem anvisierten Verkaufspreis von 100 $ insbesondere in Hinblick auf den günstigen und kompakt bemessenen Sinclair Spectrum konkurrenzfähig sein zu können.\n\nDie mit der Entwicklung beauftragten Ingenieure von MOS Technology und Commodore wandten sich zunächst der Konstruktion eines neuen elektronischen Spezialbausteins zu. Dieser sollte sämtliche Funktionen zur Erzeugung von Grafik und Ton sowie von Ein- und Ausgabeoperationen in sich vereinen und Einsparungen sowohl von Produktionskosten als auch von Gehäuseplatz ermöglichen. Die entsprechenden Funktionsgruppen des später \"MOS 7360\" genannten integrierten Schaltkreises (Chip) wurden dabei von Grund auf neu entwickelt.\n\nDie Eigenschaften der im Commodore 64 und VC 20 verbauten Spezialbausteine für Grafik und Tonerzeugung flossen bei der Konstruktion nicht in die Spezifikationen ein. Zum einen hatten deren Entwickler mitsamt benötigtem Wissen Commodore bereits verlassen und zum anderen änderte die Vermarktungsabteilung zwischenzeitlich das potentielle Einsatzgebiet des neuen Systems, nachdem Commodore im Bereich hochwertiger Bürocomputer stetig Marktanteile verloren hatte. Das bislang unerschlossene Segment des professionellen Heimanwenderbereiches – kleinere Büros und Handwerker – rückte nun in den Fokus. Entsprechend sollten die Fähigkeiten des zu entwickelnden Systems auf diese neuen Anforderungen ausgerichtet werden. Dies betraf in erster Linie den Einsatz von Dienstprogrammen wie Textverarbeitung, Tabellenkalkulation, Datenbank und die vielseitige Darstellbarkeit der erzeugten Ergebnisse. Das neue Hauptaufgabengebiet der textorientierten Verwendung spiegelt sich zudem in der Benennung des Chips als \"Text Editing Device\" oder in akronymisierter Kurzform \"TED\" wider.\n\nDie im Ergebnis entstandenen guten Grafikfähigkeiten des MOS 7360 paarte man mit einem optimierten 6502-Mikroprozessor, dem \"MOS 7501\". Neuartige Komponenten auch für den Arbeitsspeicher in Form von 16-KB-DRAM-Chips halfen die Anzahl der Bauteile weiter zu reduzieren und die zu erweiternde eingebaute Programmiersprache BASIC versprach eine unkomplizierte Verwendung dieser neuen leistungsfähigen Hardware. Dieses speziell auf den neuen MOS 7360 und 7501 zugeschnittene Rechnersystem wurde fortan als \"TED-Projekt\" in den Entwicklungsunterlagen geführt. Erste Informationen, vermutlich von Commodore selbst im Sinne einer besseren Vermarktung auf einschlägigen Computermessen in Umlauf gebracht, sprachen dagegen entweder vom \"Ted-Computer\" oder dem Codenamen \"444\".\n\nAb Sommer 1983 wurde durch den Einfluss der Commodore-Vermarktungsabteilung das Entwicklungsprogramm des TED-Computers um verschiedene Ausbaustufen mit besonderen „Features“ erweitert. Schnell kristallisierten sich drei verschiedene Konfigurationen heraus, denen man besondere Aufmerksamkeit schenkte: Am unteren Ende der Modellreihe rangierte dabei der mit einer Radiergummitastatur und 16 KB Arbeitsspeicher ausgestattete \"Commodore 116\" als der ursprünglich geplante „Sinclair Killer“. Im mittleren Bereich siedelte man das ambitioniertere Modell Commodore 264 für den semiprofessionellen Gebrauch an. Das Konzept sah 64 KB Arbeitsspeicher, eine vollwertige Schreibmaschinentastatur und ein an Lotus 1-2-3 angelehntes, aber fest eingebautes Paket an Dienstprogrammen vor – ein Novum in der Mikrocomputergeschichte. Die Auswahl der einzelnen Anwendungsprogramme plante man dabei dem potentiellen Käufer zu überlassen, der sich seine maßgeschneiderte Programmsammlung damit auf einfache Art und Weise zusammenstellen können sollte. Das Premiummodell mit der Bezeichnung \"Commodore 364\" ergänzte den Commodore 264 um ein eingebautes Sprachsynthesemodul, ein vergrößertes Gehäuse mit Ziffernblock und entsprechende Software.\n\nIm Sinne einer komfortablen Bedienung des in die hochwertigen Modelle Commodore 264 und 364 zu integrierenden Softwarepakets begann Commodore diese mit einem den Macintosh-Rechnern von Apple nachempfundenen fensterbasierten Bediensystem auszustatten. Zudem gaben die Verantwortlichen vor, die vorgesehenen Applikationen wie Tabellenkalkulation, Textverarbeitung, Datenbank, Programmiersprachen und Grafikprogramme optimal aufeinander abzustimmen, um beispielsweise eine leichte Austauschbarkeit der Daten untereinander zu gewährleisten. Darüber hinaus gehende Aufgabenstellungen sollte der Benutzer durch selbst zu erstellende Programme bearbeiten können. Dazu sah man eine Erweiterung des Commodore Basic 2.0 zum unkomplizierten Erstellen von kaufmännisch orientierter Software und zum Benutzen der hochaufgelösten Farbgrafik des TED vor. Sämtliche Arbeiten zur Implementierung der Software wurden nach dem Umzug der Firmenzentrale im Spätsommer 1983 begonnen.\n\nEinem breiteren Publikum öffentlich vorgestellt wurden die Prototypen der Commodore-264-Modelle und des Commodore 364 mitsamt einem neuentwickelten 5¼-Zoll-Diskettenlaufwerk \"SFS 481\" erstmals auf der Fachmesse \"Winter CES\" in Las Vegas im Januar 1984. Infolge eines kurz darauf vollzogenen Personalwechsels in der Firmenleitung und den damit verbundenen Umstrukturierungswirren stellte Commodore die Weiterentwicklung des Commodore-364-Modells kurzerhand ein. Stattdessen wurde auf Geheiß der fortan stärker involvierten Vermarktungsabteilung eine auf der Technik des Commodore 116 basierende, in einem anthrazitgefärbten Gehäuse der Commodore-64-Reihe untergebrachte Version mit der Bezeichnung \"Commodore 16\" aus der Taufe gehoben. Im selben Zeitraum verließ eine Vielzahl von Entwicklungsingenieuren Commodore, wodurch die sich bereits in einem fortgeschrittenen Stadium befindlichen Arbeiten an der grafischen Benutzeroberfläche \"Magic Desk II\" des Commodore 264 mittlerweile brachlagen. Eilends engagierte man die externe Firma Tri-Micro, die bereits an einem ähnlichen Projekt für den Commodore 64 arbeitete. Den gegebenen Hardwarebeschränkungen von 64 KB RAM nebst fehlenden Datei-Auslagerungsmöglichkeiten und dem Zeitdruck geschuldet, mussten jedoch deutliche Zugeständnisse an den Umfang und die Qualität der zu konvertierenden Ersatzsoftware gemacht werden. Infolgedessen blieb das nun auf vier Anwendungsprogramme beschränkte Paket weit hinter den durch die Vorgänger und vormaligen Presseankündigungen geweckten Erwartungen als Konkurrent vom Marktführer Lotus 1-2-3 (mit mindestens 192 KB benötigtem Arbeitsspeicher) zurück.\n\nZwischenzeitlich stellte Commodore die Geräte auf weiteren internationalen Fachmessen vor und sondierte deren Vertriebsmöglichkeiten. In Deutschland beispielsweise waren der Commodore 264 und die abgerüsteten Varianten Commodore 16 und Commodore 116 erstmals auf der \"Hannover Messe\" im Frühjahr 1984 zu sehen.\n\nZur Vorbeugung gegen Verwechslungen mit dem Commodore 64 und um Missverständnisse bezüglich der Kapazität des verbauten Arbeitsspeichers auszuräumen, erhielt der Commodore 264 im Juni 1984 einen anderen Namen: Die neue Bezeichnung \"Commodore Plus/4\" betonte insbesondere das integrierte Softwarepaket \"3-plus-1\", ein Alleinstellungsmerkmal des Gerätes auf dem Heimcomputermarkt. Zur Ankurbelung der Verkäufe präsentierte Commodore den neu getauften Commodore Plus/4 zusammen mit dem als Lerncomputer deklarierten Einsteigermodell Commodore 16 im Juni 1984 auf der \"5th Commodore Show\" in London und wenig später auf der \"Summer CES\" in Chicago. Neben den Rechnern war dort ebenfalls ein Teil der neuen Peripheriegeräte in Form von zwei Druckern und der Datasette \"Commodore 1531\" ausgestellt. \n\nDie Herstellung der zwischenzeitlich zur Produktionsreife gebrachten Geräte übertrug Commodore verschiedenen Fabriken jeweils in der Nähe des künftigen Absatzgebietes: für die Versorgung des europäischen Marktes waren die Commodore-Werke im britischen Corby und im westdeutschen Braunschweig zuständig. Die von der zentralen Materialvergabestelle in Hongkong gelieferten vorgefertigten Komponenten wurden in den beiden Werken endmontiert, getestet und in den Vertrieb gebracht.\n\nZur Gewährleistung einer ausreichenden weltweiten Versorgung begann Commodore bereits nach der Summer CES die Geräte in großem Umfang vorzuproduzieren und einzulagern. Wegen einer zwischenzeitlich aufgekommenen weltweiten Knappheit an benötigten Bauelementen und Verzögerungen bei der Fertigstellung des integrierten Softwarepakets musste die Produktion gedrosselt werden. Erst im Herbst 1984 standen die geplanten Verkaufskontingente zur Verfügung.\n\nDer bereits während der Entwicklungsphase in der Presse als „Productivity Computer“ beworbene Commodore Plus/4 kam im Herbst 1984 zum Preis von 299 US$ in den Einzelhandel Nordamerikas. Das Erscheinen des Commodore Plus/4 wurde durch passende Peripheriegeräte in Form der Drucker \"DPS 1101\" und \"MPS 802\" sowie des Datenrekorders \"Commodore 1531\", ergänzt um spezielle nur mit dem Commodore Plus/4 verwendbare Joysticks begleitet. Diverse Spiele und weitere Software, wie etwa die beliebte Programmiersprache \"Logo\", rundeten das Premierenangebot ab. Der Verkauf lief jedoch nur schleppend, obwohl die Werbekampagne den Commodore Plus/4 frühzeitig und vollmundig als „professionellen Nachfolger“ für den Commodore 64 angekündigt und mit dem Slogan „The only computer with four leading software programs built in“ (deutsch: „Der einzige Computer mit vier integrierten branchenführenden Programmen“) große Erwartungen geweckt hatte.\n\nBis Spätherbst 1984 waren entsprechende Fertigungsstrecken und Verkaufskapazitäten auch in Europa und in kleineren Absatzmärkten wie etwa Neuseeland erschlossen worden, begleitet durch umfangreiche Werbekampagnen und Vorankündigungen in einschlägigen Computermagazinen. Der Verkauf in Europa startete schließlich Ende des Jahres zunächst in Großbritannien mit einem Preis von 249 £. Im Januar 1985 war der Commodore Plus/4 auch im westdeutschen Einzelhandel zum Preis von etwa 1300 DM erhältlich, einen Monat nachdem Commodore 116 und Commodore 16 für 448 DM beziehungsweise 498 DM premierten.\n\nBereits kurz nach Markteinführung zeichneten sich auch in Europa, allen voran in Großbritannien, große Vermarktungsschwierigkeiten für den Commodore Plus/4 ab. Als Reaktion darauf und in Hinblick auf die Konkurrenzmodelle reagierte die britische Niederlassung von Commodore im Frühjahr 1985 mit einer Preissenkung auf 149 £. Durch diese Entscheidung geriet auch der Preis des Commodore 64 unter Druck und musste ebenfalls gesenkt werden. Eine Entwicklung, die den Unmut vieler Händler nach sich zog – ließ sie doch die im hart umkämpften Heimcomputermarkt ohnehin geringen Gewinnmargen weiter schrumpfen. Daraufhin vorgenommene Veränderungen beim britischen Personal von Commodore führten zu einer Änderung der Verkaufspolitik hin zu Bündelangeboten für den Commodore Plus/4.\n\nAuf der \"6th Commodore Show\" im Juni 1985 wurde erstmals ein Bündelangebot, bestehend aus Commodore Plus/4, 1541-Diskettenlaufwerk, dem Nadeldrucker \"MPS 801\" und der Software \"Impex 3-2-1\", zum Verkaufspreis von 449 £ vorgestellt. Damit vollzog sich eine vollständige Kehrtwendung von der vormaligen Vermarktungsstrategie des „professionellen Heimanwendergeräts“ hin zu einer solchen des massenkompatiblen Einsteiger-Rundumpakets. Bei Commodore in Deutschland stattete man die Bündelangebote unterschiedlichen Umfangs zudem mit einem Programmierhandbuch und der Anleitungskassette \"BASIC-Kurs\" aus und deklarierte sie zum \"Computer-Lernkurs\" um. Diese Vermarktungsstrategie versuchte die mit Computern häufig völlig unerfahrene Kundschaft durch preiswerte Lockangebote an die Marke Commodore zu binden, um sich deren Kaufkraft für teurere Produkte wie den Commodore 64 oder Commodore 128 zu sichern.\n\nAls mit Sinclair QL, CPC 464 und Atari 800XL der Heimcomputermarkt zwischenzeitlich weitere ernstzunehmende Mitbewerber hervorbrachte, brach der ohnehin schlechte Absatz des Commodore Plus/4 vollends ein. Commodore reagierte Mitte 1985 mit der Einstellung der Produktion und weiteren Preissenkungen für bereits vorproduzierte Geräte. Die konkurrenzlos niedrigen Preise zogen indessen gutgehende Weihnachtsverkäufe nach sich, was wiederum für eine Belebung der Softwareversorgung durch Dritthersteller sorgte. Die daraufhin aufkeimenden Hoffnungen der Benutzer auf weitere Unterstützung durch den Hersteller währten jedoch nur kurz: Die Veröffentlichung des neuen Commodore 128 im Blick, ließ Commodore die Commodore-264-Produktlinie Anfang 1986 endgültig fallen. Die umfangreichen Lagerbestände wurden innerhalb kurzer Zeit in Chargen von bis zu 150.000 Stück an interessierte Großabnehmer veräußert.\n\nDie Aufkäufer ihrerseits begannen im Laufe des Jahres 1986 die Geräte weltweit zu Schleuderpreisen in den Handel zu bringen. Beispielsweise in Großbritannien stand das Komplettpaket \"Plus Pack\" mit Computer, Datasette, Joystick und zehn Kassettenspielen nun ab 99 £ zum Kauf, solche mit dem neuen Diskettenlaufwerk \"Commodore 1551\" und Drucker MPS 803 nebst Geschäftssoftware ab 299 £. Die übrigen Verkäufer zogen nach und senkten ihre Preise ebenfalls. In Deutschland begann insbesondere die Lebensmittelkette Aldi gegen Mai 1986 den stark verbilligten Computer-Lernkurs in seine Filialen zu bringen und machte so die Rechner Commodore Plus/4 und Commodore 16 einem größeren Kreis potentieller Interessenten zugänglich.\n\nDie ins Bodenlose gefallenen Preise zogen verstärktes Interesse auch außerhalb der westlichen Industrienationen nach sich. Viele Ostblock-Staaten, deren landeseigene Computerprodukte um ein Vielfaches teurer als der Commodore Plus/4 waren, nutzten die Gelegenheit zum Erwerb größerer Stückzahlen. Vor allem Ungarn deckte sich mit dem im Westen ungeliebten Rechner nebst Peripheriegeräten in größeren Mengen ein, wobei der Hauptteil der erworbenen Technik der Ausstattung staatlicher Bildungseinrichtungen diente.\n\nBis Ende 1987 waren den Angaben von Tri-Micro zufolge weltweit etwa 600.000 Rechner verkauft worden. Den größten Absatzmarkt stellte dabei Europa mit circa 450.000 verkauften Geräten dar, wovon etwa 100.000 Stück auf den Ostblock und dabei hauptsächlich Ungarn entfielen. Die nachfolgenden Jahre mit eingerechnet fanden nach Angaben eines ehemaligen Commodore-Mitarbeiters insgesamt etwa 830.000 Commodore-Plus/4-Computer weltweit ihre Abnehmer, wobei speziell auf Deutschland 286.500 Geräte entfallen. Der Computer verkaufte sich damit deutlich schlechter als der Commodore 64 (3 Millionen Stück allein in Deutschland) und VC 20 (2,5 Millionen Exemplare weltweit).\n\nDas Gehäuse des Commodore Plus/4 enthält eine einzelne Platine mit allen elektronischen Baugruppen, den Peripherieanschlüssen, dem nach außen geführten Systembus für Erweiterungen, der Bildschirmausgabe und der Spannungsregelung für das externe Netzteil. Die elektronischen Hauptbestandteile bilden die 7501- bzw. 8501-CPU (engl. \"central processing unit\"), das \"Text Editing Device\" (TED) und der Arbeits- (RAM) sowie Festwertspeicher (ROM). Zum Lieferumfang gehörten neben dem Computer ein Netzteil (5 Volt Gleichspannung, 9 Volt Wechselspannung), das Antennenkabel und die Bedienungsanleitungen für das Gerät und die eingebaute Software.\n\nDie Systemarchitektur basiert auf dem 8-Bit-Mikroprozessor \"MOS 7501\" oder einer moderneren Variante in Form des \"MOS 8501\". Es handelt sich dabei um abwärtskompatible Weiterentwicklungen der in Heimcomputern oft verbauten 6502- bzw. 6510-Mikroprozessoren. Die CPU kann auf einen Adressraum von 65536 Byte zugreifen, was auch die theoretisch mögliche Obergrenze des Arbeitsspeichers von 64 KB festlegt. Durch Bankumschaltung – die einen wesentlichen Teil der Rechnerarchitektur des Commodore Plus/4 darstellt – ist es möglich, auch mehr als 65536 verschiedene Bytes durch aufeinanderfolgendes Einblenden weiterer RAM- und ROM-Bausteine anzusprechen. Der Systemtakt beträgt bei PAL-Geräten 1,768 MHz, für solche mit NTSC-Ausgabe dagegen 1,788 MHz.\n\nNeben der CPU kann auch der Spezialbaustein TED direkt auf den Arbeitsspeicher und die Eingabe-/Ausgabegeräte zugreifen (engl. \"direct memory access\", DMA), beispielsweise um das anzuzeigende Bild aus Videodaten zu erzeugen. Dabei geht das System in den \"Shared-Bus\"-Modus über, in dem die Speicherzugriffe beider Bausteine in ständigem Wechsel erfolgen. Für die CPU sind dabei nur bei geradzahligen und für den TED nur bei ungeradzahligen Taktzahlen Operationen möglich. Dies entspricht effektiv einer Halbierung des CPU-Taktes auf 884 kHz bzw. 894 kHz. Hat der TED keine weiteren Bilddaten zu bearbeiten, d. h. während horizontaler und vertikaler Austastlücken sowie bei gelöschtem Bildschirm, werden – bis auf wenige Ausnahmen – wieder sämtliche Takte für die CPU freigegeben.\n\nDer 48-polige elektronische Spezialbaustein \"MOS 7360\" mit der Kurzbezeichnung TED (Akronym vom engl. \"Text Editing Device\") enthält die wesentlichen elektronischen Komponenten zur Erzeugung von Grafik, Ton sowie für Ein- und Ausgabeoperationen. Daneben gehören u. a. auch die Speicherverwaltung, das Auslesen der Tastatureingaben und die Takterzeugung zu seinen Aufgaben. Im Gegensatz zur CPU kann der TED nicht auf ROM-Inhalte zugreifen.\n\nDie verschiedenen Fernsehnormen (NTSC, PAL) werden durch entsprechende äußere Beschaltungen und Anpassungen des Betriebssystems („Kernal“) realisiert.\n\nDie für das Bildausgabegerät wie Monitor oder Fernseher bereitzustellenden Daten werden vom TED aus den im Arbeitsspeicher hinterlegten Bild- und Farbdaten erzeugt. Der im TED enthaltene Bildgenerator ermöglicht für 200 Fernsehzeilen jeweils die Ausgabe von 320 Bildpunkten. Die aus dem Video- und Farbspeicher gewonnenen Daten können je nach Arbeitsmodus des TED verschieden interpretiert und damit verschieden dargestellt werden. Unterstützt werden dabei Elemente mit einer Größe von 8 × 8, 2 × 1 und 1 × 1 Bildpunkten für Auflösungen von 40 × 25 Zeichen und Punktgrafik mit 160 × 200 sowie 320 × 200 Pixeln (Akronym vom engl. \"picture cell\", Grafikblock). Durch Rasterinterrupt-Programmierung können die verschiedenen grafischen Betriebsarten auf dem Bildschirm in vertikaler Abfolge gemischt werden.\n\nDas Aussehen der im \"Videospeicher\" hinterlegten Textzeichen wird durch einen maximal 256 Zeichen umfassenden Zeichensatz definiert. Die Zeichenfarbe wird über ein zugeordnetes Byte im \"Farbspeicher\", das \"Attribut\", festgelegt. Die zur Verfügung stehende Farbplatte umfasst neben Schwarz 15 weitere Farben in jeweils acht verschiedenen Helligkeitsstufen. Bei der Grafikstufe mit der höchsten Auflösung ist die Farbe der einzelnen Pixel – ähnlich dem Textmodus – innerhalb eines 8 × 8 Bildpunkte umfassenden Areals immer dieselbe, kann aber pro Areal frei aus den 121 möglichen gewählt werden.\n\nIm Mehrfarbmodus (engl. \"multi colour mode\") sowohl für Text als auch für Grafik werden die im Speicher hinterlegten Daten vom TED anders als im Normalmodus interpretiert. Damit sind mehr Farben in den 8 × 8 Bildpunkten umfassenden Arealen gleichzeitig darstellbar – die horizontale Auflösung der Pixel wird dabei jedoch halbiert und die zur Verfügung stehenden Farben auf insgesamt acht nebst jeweils entsprechenden Helligkeitsstufen reduziert. Für die Textdarstellung ist bei gleicher Auflösung wie beim Mehrfarbmodus eine weitere Betriebsart (engl. \"extended colour mode\") mit erhöhter Farbanzahl jedoch auf Kosten einer verringerten Auswahl von Zeichen (64 anstatt 256) möglich.\n\nVom Hintergrund unabhängige verschiebbare Grafikblöcke (Hardware-Sprites) sind, anders als bei vielen anderen zeitgenössischen Heimcomputern, im Commodore Plus/4 nicht integriert. Eine weiche Feinverschiebung (Scrolling) zum augenfreundlichen Bewegen des gesamten Bildschirminhaltes wird dagegen unterstützt.\n\nZur Tonerzeugung dienen die beiden separaten Generatoren „Stimme 1“ und „Stimme 2“, die jeweils eine Rechteckschwingung erzeugen können. Zugehörige Schwingungsparameter wie Amplitude (Lautstärke), Frequenz (Tonhöhe) und Tondauer sind frei einstell- und damit über die Zeit vom Benutzer variierbar. Eine Änderung der Schwingungsform („Klangfarbe“) in Sinus- oder Sägezahnform wie beim Soundchip des Commodore 64 ist nicht möglich; durch Kombinationen der Einzelkanäle können jedoch tontechnisch interessante Schwebungen generiert werden. Darüber hinaus kann einer der beiden Tonkanäle zum Erzeugen von weißem Rauschen („Stimme 3“), d. h. für bestimmte Toneffekte wie etwa Geräusche oder als Zufallszahlengenerator eingesetzt werden.\n\nDer von der CPU und TED ansprechbare Adressraum segmentiert sich beim Commodore Plus/4 in verschiedene Abschnitte unterschiedlicher Größe. Aus praktischen Gründen ist es üblich, für deren Adressen anstelle der dezimalen Notation die hexadezimale zu verwenden. Ihr wird zur besseren Unterscheidbarkeit üblicherweise ein $-Symbol vorangestellt. Den Adressen von 0 bis 65535 in dezimaler Notation entsprechen im hexadezimalen System die Adressen $0000 bis $FFFF.\n\nNach dem Einschalten des Computers besteht der Adressraum aus nahezu 32 KB RAM ($0002 bis $07FF für System- und BASIC-Variablen, $0800 bis $0BFF als Farbspeicher für den Textmodus, $0C00 bis $0FFF als Videospeicher für den Textmodus, von $1000 bis $7FFF frei) gefolgt von etwa 32 KB ROM ($8000 bis $BFFF für den BASIC-Interpreter, $C000 bis $FFFF für das Betriebssystem, $D000 bis $D7FF für den Zeichensatz, $D800 bis $FCFF und $FF40 bis $FFFF für das Betriebssystem). Die übrigen Bereiche sind für Ein- und Ausgabeoperationen ($0000, $0001, $FD00 bis $FEFF) und die Steuerung des TED ($FF00 bis $FF3F) reserviert. Die CPU liest zunächst die Inhalte der ROM-Bausteine mit dem Betriebssystem aus, womit der Commodore Plus/4 nebst angeschlossenen Peripheriegeräten initialisiert wird. Sind keine Steckmodule mit ausführbaren Inhalten am Expansionsport vorhanden, wird vom Betriebssystem das eingebaute BASIC gestartet und eine Einschaltmeldung mit blinkendem Cursor erscheint auf dem Bildschirm.\n\nWährend der Initialisierung kann der Anwender zu den ROMs mit den integrierten Anwendungsprogrammen umschalten. Wird ausschließlich RAM eingeblendet, stehen abzüglich der reservierten und der zur Systemerhaltung benötigten Bereiche 60.671 Bytes zur Verwendung mit BASIC oder dem 3-plus-1-Softwarepaket bereit. Der Wechsel zwischen den verschiedenen RAM bzw. ROM-Speicherbänken, die \"Bankumschaltung\", wird durch das Betriebssystem gesteuert.\n\nAls Verbindungen zur Außenwelt dienen ein Erweiterungssteckplatz (herausgeführter Systembus), zwei Joystickanschlüsse im Mini-DIN-Format, eine Monitorbuchse, ein koaxialer HF-Antennenanschluss für Fernseher, eine Buchse zum Verbinden mit der Datasette \"Commodore 1531\", eine serielle Commodore-Standardschnittstelle zum Gebrauch mit vielen Commodore-Peripheriegeräten wie dem 1541-Diskettenlaufwerk und eine RS-232-Schnittstelle in Form des \"User-Ports\". Dessen Ansteuerung übernimmt ein spezieller nur im Commodore Plus/4 verbauter Baustein, der \"MOS 6551\" beziehungsweise \"MOS 8551\" (ACIA).\n\nIn Zusammenhang mit vor allem westlichen Heimcomputern der 1980er Jahre kamen zur Datensicherung hauptsächlich Kassettenrekorder und Diskettenlaufwerke, im professionellen Umfeld bei den Personalcomputern zunehmend auch Fest- und Wechselplattenlaufwerke zum Einsatz. Die günstigste Variante der Datenaufzeichnung durch Kompaktkassetten hat im Allgemeinen den Nachteil niedriger Datenübertragungsraten und damit langer Ladezeiten, wohingegen die wesentlich schnelleren und verlässlicheren Disketten- und Plattenlaufwerke sehr viel teurer in der Anschaffung waren. Bei Veröffentlichung des Commodore Plus/4 standen ihm Kassetten- und Diskettensysteme als Massenspeicher zur Verfügung.\n\nDer Commodore Plus/4 verfügt ab Werk über eine Kassettenschnittstelle zum Datenaustausch mit der Datasette \"Commodore 1531\". Ein Betrieb mit den restlichen von Commodore produzierten Datasetten wie etwa dem Modell \"1530\" ist nur mit einem entsprechenden Adapter von Drittherstellern möglich. Als Speichermedien dienen Kompaktkassetten. Die durchschnittliche Datenübertragungsrate beträgt wie bei Commodore 64 und VC 20 auch etwa 300 Bit/s. Dieser im Vergleich zu anderen zeitgenössischem Heimcomputern (600 Bit/s bei Atari-Heimcomputern, 1500 Bit/s bei ZX Spectrum) geringe Wert ist dem aufwendigen Datenformat mit zweifach redundanten Blöcken und Prüfsummenvalidierung geschuldet. Durch Änderung dieses Aufzeichnungsformats mit speziellen Programmen, den \"Turbo Tapes\" wie beispielsweise \"CSJ Turbo Tape\" und \"Turbotape C16\", kann die Ladegeschwindigkeit bis um den Faktor 12 erhöht werden.\n\nÜber die serielle Standardschnittstelle können die Diskettenlaufwerke \"Commodore 1541\" (5¼ Zoll), \"Commodore 1570\" (5¼ Zoll), \"Commodore 1571\" (5¼ Zoll) und \"Commodore 1581\" (3½ Zoll) problemlos mit dem Commodore Plus/4 betrieben werden. Eine Benutzung des beschleunigten Datentransfers \"(Burst-Modus)\" der Nicht-1541-Modelle ist dabei nicht möglich. Die durchschnittliche Datenübertragungsrate beträgt bei der 1541-Diskettenstation in der älteren Version etwa 1200 Bit/s, ein im Vergleich zu anderen Heimcomputersystemen geringer Wert.\n\nDarüber hinaus produzierte Commodore ein eigens für den Commodore 16, 116 und Plus/4 entwickeltes Diskettenlaufwerk, dessen Datentransfer über eine parallele Schnittstelle, den Expansionsport erfolgt. Durch nicht näher benannte Probleme verzögerte sich die Veröffentlichung dieses zunächst \"SFS 481\" genannten 5¼-Zoll-Diskettenlaufwerks erheblich. Zwischenzeitlich in \"Commodore 1551\" umbenannt, war es erst ab Mitte 1986 erhältlich, häufig als Bestandteil eines der vielen Ausverkaufs-Bündelangebote. Das zum Einführungspreis von 400 DM auch einzeln erhältliche Gerät hat gegenüber dem älteren 1541-Modell einige wesentliche Vorteile: Die robustere Mechanik und eine neuartige Ansteuerungselektronik erhöhen die Verlässlichkeit deutlich, durch den Parallelanschluss beträgt die Datentransferrate etwa 6000 Bit/s. Der Befehlssatz beider Laufwerksbetriebssysteme (DOS) ist nahezu identisch, so dass 1541-Disketten mit der 1551-Diskettenstation und – bis auf wenige Spezialfälle – auch umgekehrt verwendet werden können.\n\nNeben den Commodore-eigenen Diskettenlaufwerken kann der Commodore Plus/4 auch mit Geräten von Drittherstellern wie beispielsweise dem \"Enhancer 2000\" von Comtel Group betrieben werden.\n\nZur Datenausgabe stehen dem Commodore Plus/4 verschiedene Möglichkeiten zur Verfügung: Über einen speziellen Monitorausgang kann der Rechner beispielsweise mit den Commodore-Farbmonitoren betrieben werden, wobei der \"Commodore 1703\" mit seinem dunkleren Gehäuse eigens für den Commodore 16 und Commodore Plus/4 entworfen wurde. Neben der Monitorausgabe ist via TV-Modulator auch die Anzeige an einem handelsüblichen Fernseher möglich, wobei die Bildqualität als deutlich schlechter einzustufen ist.\n\nZur schriftlichen Fixierung von Text oder Grafik dienen verschiedene Commodore-Drucker. Dazu zählen die nadelbasierten Modelle \"MPS 801\" bis \"803\", der Typenraddrucker \"DPS 1101\" mit einem Schriftbild in Schreibmaschinenqualität sowie der Vierfarbplotter \"Commodore 1520\". Daneben lassen sich auch viele Druckermodelle von Drittherstellern wie etwa Seikosha, Brother oder Star mit dem Commodore Plus/4 betreiben.\n\nDie im Commodore Plus/4 verbaute Tastatur verfügt über 67 Tasten in QWERTY-Anordnung mit vier abgesetzten Cursor-Tasten. Außerhalb des eigentlichen Tastaturfeldes befinden sich vier frei programmierbare und doppelt belegbare Funktionstasten.\n\nIm Gegensatz zu vielen anderen Heimcomputern seiner Zeit folgen die Joystickanschlüsse des Commodore Plus/4 keinem gängigen Standard. Vielmehr handelt es sich um eine technische Insellösung, die dem beschränkten Platz im kleinen Gehäuse geschuldet ist. Später herausgebrachte Adapter von Drittherstellern ermöglichen neben dem Betrieb der eigens für den Commodore Plus/4 konstruierten Joysticks auch den Anschluss der weitverbreiteten Atari-2600-kompatiblen Modelle (D-Sub: zweireihig 9-polig) wie etwa dem \"Competition-Pro-Joystick\" oder einem Vertreter der \"Quickshot\"-Reihe von Spectravideo.\n\nMithilfe der im Commodore Plus/4 verbauten UART-Schnittstelle in Form des \"MOS 6551\" können ohne weitere Softwareunterstützung RS-232-Hochgeschwindigkeit-Modems der damaligen Zeit betrieben werden. Allerdings verfügten nur die wenigsten Anwender im Jahre 1984 über Modems mit höheren Datenübertragungsraten, als durch Softwareemulation auch mit dem Commodore 64 möglich waren. Anfänglich kam ausschließlich das Telefonmodem \"Commodore 1660\" zum Einsatz.\n\nWie bei anderen Heimcomputern der 1980er Jahre auch erfolgte der Vertrieb kommerzieller Software auf verschiedenen Datenträgern. Die insbesondere bei Spieleherstellern beliebten preiswerten Kompaktkassetten waren durch die starke mechanische Beanspruchung des Magnetbandes allerdings sehr anfällig für Fehler und ihr Einsatz war oft mit langen Ladezeiten verbunden. Zudem sind mit Datasetten bestimmte Betriebsarten wie die beispielsweise zum Betrieb von Datenbanken vorteilhafte relative Adressierung nicht möglich. Bei den in der Herstellung vielfach teureren Steckmodulen dagegen standen die darin enthaltenen Programme sofort nach dem Einschalten des Computers zur Verfügung, was insbesondere bei Systemsoftware und oft genutzten Anwendungen von großem Vorteil war. Den besten Kompromiss zwischen Ladezeit, möglichen Betriebsarten, Verlässlichkeit und Speicherkapazität erzielten die Disketten, deren Verwendung bei Veröffentlichung des Commodore Plus/4 durch das 1541-Diskettenlaufwerk unterstützt wurde.\n\nDie Programmpalette für den Commodore-Plus/4-Computer umfasste neben der von Commodore vertriebenen Auswahl kommerzieller Programme auch von Drittherstellern entwickelte und in Zeitschriften und Büchern publizierte Software (Listings) zum Abtippen. Die meisten der kommerziellen Programme wurden auf Steckmodul und Diskette angeboten. Spiele, insbesondere solche von Drittherstellern, waren dagegen häufig nur auf Kompaktkassette erhältlich.\n\nVon der in Umlauf befindlichen Software machten illegale Kopien („Raubkopien“) stets einen großen Teil aus und stellten damit kleinere Softwareentwickler häufig vor existentielle wirtschaftliche Schwierigkeiten. Daraufhin wurden zunehmend Kopierschutzsysteme insbesondere bei Spielen als der meistverkauften Software eingesetzt.\n\nDie Konfiguration der Commodore-Plus/4-Hardware und des BASIC fällt in den Aufgabenbereich des Betriebssystems, für das sich im normalen Sprachgebrauch die Bezeichnung \"Kernal\" eingebürgert hat. Das Betriebssystem besteht aus insgesamt 39 Unterprogrammen; sie steuern die Ein-/Ausgabeoperationen, den Zugriff auf Systemvariablen, die Speicherverwaltung und den Betrieb des BASIC-Interpreters. Die Startadressen der einzelnen Subroutinen sind in einer durch den Benutzer zu verwendenden Sprungtabelle zusammengefasst, um die Softwarekompatibilität der erstellten Programme mit Commodore 16, Commodore 116 und zukünftigen Revisionen des Betriebssystems sicherzustellen.\n\nAufbauend auf der Systemsoftware kam dem benutzerspezifischen Einsatz des Commodore Plus/4 in unterschiedlichsten Anwendungsgebieten große Bedeutung zu. War dabei die Bearbeitung einer Aufgabenstellung mit z. B. käuflich zu erwerbenden Programmen aus technischen oder wirtschaftlichen Gründen nicht möglich oder sollte beispielsweise neuartige Unterhaltungssoftware produziert werden, so musste dies mithilfe von entsprechenden Programmiersprachen in Eigenregie geschehen.\n\nDie Erstellung zeitkritischer Actionspiele und Anwendungen in der Regelungstechnik erforderte Anfang der 1980er Jahre eine optimale Nutzung der Hardware insbesondere des Arbeitsspeichers. Im Heimcomputerbereich war dies ausschließlich durch die Verwendung von Assemblersprache mit entsprechenden Übersetzerprogrammen, den Assemblern, möglich. Die Auslieferung von Assemblern erfolgte in vielen Fällen mit einem zugehörigen Editor zur Eingabe der Programmanweisungen („Sourcecode“), häufig auch als Programmpaket mit Debugger und Disassembler zur Fehleranalyse. Im professionellen Entwicklerumfeld kamen vielfach Cross-Assembler zum Einsatz. Damit war es möglich, ausführbare Programme für Heimcomputer auf leistungsfähigeren und komfortabler zu bedienenden Fremdcomputerplattformen zu erzeugen. Beispielsweise erfolgte die Entwicklung der Commodore-Plus/4-Systemsoftware auf einem VAX-Computer von DEC.\n\nZum Programmieren in Maschinensprache steht dem Commodore-Plus/4-Benutzer der in den System-ROMs integrierte Monitor \"TEDMON\" zur Verfügung, der beispielsweise durch Eingabe eines BASIC-Kommandos gestartet werden kann. Neben dem Anzeigen, Editieren und Manipulieren von Speicher- und Registerinhalten sind damit auch eigene simple Assemblerprogramme beispielsweise zum Aufruf durch BASIC erstellbar. Aufgrund fehlender Funktionalitäten wie Haltepunkte, integriertes Textdatenformat, Sprungmarken oder Makro-Definitionen ist das Erstellen größerer Programme nebst Fehleranalyse schwierig und daher die Benutzung komfortablerer Alternativen wie etwa die des \"6502 Editor/Assembler Plus/4\" von York Electronic Research empfehlenswert.\n\nProgrammiereinsteiger zogen in vielen Fällen die übersichtlichen und einfach zu bedienenden, dafür aber weniger leistungsfähigen Programmier-Hochsprachen vor.\n\nDas zusammen mit dem Commodore Plus/4 ausgelieferte und leicht zu erlernende BASIC 3.5 ermöglicht durch seinen leistungsfähigen Satz von über 75 Befehlen und die verfügbaren 60.671 Bytes Arbeitsspeicher eine Umsetzung auch größerer Projekte nebst ansprechender Grafikausgabe. Nachteilig auf die Einsetzbarkeit von BASIC-Programmen wirkten sich die in der Natur des Interpreters liegenden prinzipiellen Beschränkungen wie etwa die geringe Ausführungsgeschwindigkeit und der große Arbeitsspeicherbedarf aus. Diese Nachteile können durch spezielle Programme, BASIC-Compiler, abgemildert werden. Dabei werden ausführbare Maschinenprogramme erzeugt, die ohne BASIC-Interpreter lauffähig sind und damit häufig eine schnellere Ausführung erlauben. Mit dem 1987 herausgebrachten Compiler \"Austrospeed +4\" werden deutliche Geschwindigkeitsvorteile insbesondere beim Lesen von Diskettendateien (bis zu 20-mal schneller) und bei bestimmten Sortieralgorithmen (bis zu viermal schneller) erreicht.\n\nNachdem viele Computer im Rahmen des 1986 begonnenen Ausverkaufs neue Besitzer insbesondere in Deutschland und Ungarn gefunden hatten, wurden wegen der verstärkten Nachfrage dort ab 1986 optimierte BASIC-Dialekte angeboten. Stellvertretend für deutsche Programme seien \"Turbobasic\" (1986), \"Markt & Technik Basic\" (1986) und \"Sprite-Basic\" (1988) angeführt; als ungarische Vertreter \"Lacisoft Basic\", \"Octasoft BASIC V7.0\" (1988) und \"Tool Basic 7.0\" (1989).\n\nNeben der Programmiersprache BASIC in ihren verschiedenen Dialekten existiert die Interpretersprache \"Logo\", die seit Verkaufsstart des Commodore Plus/4 erhältlich war. Unterstützt durch Elemente wie die \"turtle graphics\" (Schildkrötengrafik) ist eine kindgerechte und interaktive Einführung in die Grundlagen der Programmierung möglich. Ausgeliefert wurde Logo ausschließlich auf Disketten, womit zum Betrieb ein Diskettenlaufwerk vorausgesetzt wird.\n\nAls Mittelweg zwischen Interpreter-Hochsprache (langsam in der Ausführung, aber gut lesbare Sourcecodes und einfache Fehleranalyse) und Assemblersprache (schwer zu erlernen und umständlich zu handhaben, aber Anfang der 1980er Jahre alternativlos zur Erzeugung schneller und speichereffizienter Programme) etablierten sich auch im Heimcomputerbereich im Laufe der 1980er Jahre die Compiler-Hochsprachen. Die Ausführungsgeschwindigkeit der damit erzeugten Maschinenprogramme war im Vergleich zu interpretierten Programmen wie beim eingebauten BASIC sehr viel größer, reichte aber nicht ganz an die von Assemblern erzielte heran. Die Geschwindigkeitsnachteile gegenüber assemblierten Programmen wurden jedoch vielfach zugunsten eines leichter zu wartenden Quelltextes in Kauf genommen.\n\nIm Laufe der Zeit waren für die Commodore-Plus/4-Anwender Compilersprachen wie G-Pascal und verschiedene Versionen von Forth erhältlich.\n\nDie Programmpalette für den Commodore Plus/4 umfasst neben den Programmiersprachen zum Erstellen eigener Applikationen eine im Vergleich zum Commodore 64 lediglich kleine Auswahl an vorgefertigter kommerzieller Anwendungssoftware.\n\nDas integrierte 3-plus-1-Softwarepaket besteht aus vier eng miteinander verzahnten Anwendungsprogrammen, deren Daten mittels einer einfachen fensterbasierten Bedienoberfläche leicht untereinander ausgetauscht werden können. Die eingebaute Textverarbeitung ist mit ihrem beschränkten Funktionsumfang (Formatierungsbefehle, Suchen, Zeichenersetzung, Ausgabe von 80 Zeichen pro Zeile unterstützt) und der umständlichen Bedienung (Steuerzeichen nur beim Drucken wirksam) nicht für professionelle Zwecke geeignet, ebenso wenig wie die Tabellenkalkulation, deren Tabellen bedingt durch die maximale Bildschirmspaltenbreite von nur 40 Zeichen lediglich kleine Ausschnitte (3 × 12 von 17 × 50 Feldern) darzustellen vermag und damit sehr unübersichtlich ist. Zudem nutzt das mitgelieferte Grafikprogramm zum Darstellen der Tabellendaten bei weitem nicht die Möglichkeiten des TED aus, so dass die Zahlenreihen lediglich in Form von grob aufgelösten Säulendiagrammen visualisiert werden können. Die Dateiverwaltung genügt mit ihren maximal 17 Feldern à 38 Zeichen und damit insgesamt nur 999 speicherbaren Informationen ebenfalls nicht professionellen Ansprüchen wie etwa dem Einsatz in der Lagerhaltung.\n\nIm Laufe der Zeit ergänzte Commodore die Programmbibliothek um weitere leistungsfähigere Geschäftsprogramme wie \"Script/Plus\", \"Calc/Plus\" und \"Financial Advisor\" in Form von Steckmodulen. Daneben erschien eine Vielzahl von Anwendungen auf Diskette und Kassette, hauptsächlich in Europa und dabei insbesondere in Ungarn.\n\nDen mit Abstand größten Teil der sowohl kommerziellen als auch frei erhältlichen Commodore-Plus/4-Software stellen die Spiele dar. Am beliebtesten waren in erster Linie Umsetzungen von Arcade-Spielen. Neben Commodore, Scott Adams' Adventure International und Infocom veröffentlichten vor allem Low-Budget-Hersteller wie Anirog, Kingsoft (Anco), Mastertronic, Microdeal und Tynesoft für den Commodore Plus/4. Abgesehen von den von Commodore produzierten Steckmodultiteln wurden kommerzielle Commodore-Plus/4-Spiele meist auf Kompaktkassette mit Schnellladeprogrammen und häufig damit kombinierten Kopierschutzmechanismen ausgeliefert. Im Jahr 1987 waren in Deutschland über 150 kommerzielle Titel mit Preisen zwischen 10 und 40 DM erhältlich. Auch im damaligen Ostblock wurden Spiele entwickelt und vertrieben, wie z. B. Hungaroring.\n\nIn den 1980er Jahren spielten neben den Fachbüchern die Computerzeitschriften für viele Heimcomputerbesitzer eine große Rolle. Die häufig monatlich erschienenen Ausgaben enthielten Testberichte zu Neuheiten, Programmieranleitungen und Software zum Abtippen. Sie dienten weiterhin als Werbe- und Informationsplattform sowie zur Kontaktaufnahme mit Gleichgesinnten.\n\nFür die Commodore-Plus/4-Benutzer waren verschiedene auf ihre Bedürfnisse zugeschnittene Kiosk- und Abonnement-Publikationen erhältlich. In Westdeutschland waren dies häufig Sonderhefte populärer kommerzieller Computermagazine wie \"64’er\" oder \"Compute mit\".\n\nNach dem Ende der Heimcomputerära Anfang der 1990er Jahre und mit dem Aufkommen leistungsfähiger und erschwinglicher Rechentechnik Ende der 1990er Jahre wurden von engagierten Enthusiasten verstärkt Programme zum Emulieren von Heimcomputern und deren Peripheriegeräten entwickelt. Zum Spielen alter Klassiker verschiedenster Heimcomputersysteme reichte mithilfe der Emulatoren ein einzelnes modernes System mit Datenabbildern („Images“) der entsprechenden Heimcomputerprogramme. Das Aufkommen der Emulatoren setzte damit u. a. ein verstärktes Transferieren von sonst möglicherweise verloren gegangener Software auf moderne Speichermedien in Gang, womit ein wichtiger Beitrag zur Bewahrung digitaler Kultur geleistet wird.\n\nAls leistungsfähigste Emulatoren für Windows und Linux-Systeme gelten \"Versatile Commodore Emulator\" (VICE), \"Yet Another Plus/4 Emulator\" (YAPE) und \"plus4emu\".\n\nIn der Fachpresse fanden das kleine und kompakte Gehäuse, der große Arbeitsspeicher, das erweiterte BASIC 3.5 mit seinem umfangreichen Befehlssatz, die guten Grafikmöglichkeiten des TED („Farbwunder“), der eingebaute Assembler und die Unterstützung des vielerorts bereits vorhandenen 1541-Diskettenlaufwerks großen Anklang. Damit wurde die Leistungsfähigkeit des Geräts überwiegend als sich auf der Höhe der Zeit befindend eingeordnet. Einzig beim Preis schieden sich die Geister: die Meinungen teilten sich in „gerechtfertigt“ und „ungerechtfertigt“.\n\nDie in den Rezensionen beschriebenen Nachteile der Hardware beziehen sich hauptsächlich auf die Unterschiede zu den Vorgängermodellen Commodore 64 und VC 20. Den Hauptkritikpunkt bildete dabei die fehlende Abwärtskompatibilität bei gleichzeitig fehlendem Angebot an Software für den Commodore Plus/4. Daneben erregten die inkompatiblen Anschlüsse für Joystick und Datasette sowie die nicht standardisierte RS-232-Schnittstelle häufig das Missfallen der Tester. Neben der meist teuer erworbenen Software von Commodore 64 und VC 20 konnten so auch bestimmte vorhandene Peripheriegeräte wie etwa Standard-Joysticks nicht mit dem Commodore Plus/4 weitergenutzt werden, ein Umstand, der auf vollkommenes Unverständnis stieß und stetig Gerüchte über Commodores wahre Absicht – die reine Profitmaximierung durch erwartete Zubehörverkäufe – nährte.\n\nDas von Commodore beworbene Alleinstellungsmerkmal des Rechners, die integrierte Software, ließ nach ersten Messepräsentationen bereits eine nur eingeschränkte Verwendbarkeit erahnen. Nach dem Verkaufsstart und ausführlicheren Tests bestätigten sich die schlimmsten Befürchtungen, denn die eingebaute Software erwies sich vom Funktionsumfang und von der Handhabbarkeit her als für professionelle Zwecke vollkommen ungeeignet. Erschwerend kam hinzu, dass die Benutzung zwingend ein Diskettenlaufwerk und damit zusätzliche Anschaffungen voraussetzte.\n\nNachdem sich die Einschränkungen des Geräts und seiner eingebauten Software immer deutlicher herauskristallisiert hatten, stellte sich vermehrt die Frage nach dem Sinn und der Zielgruppe des Commodore Plus/4. Durch die weiteren versteckten Kosten für neue Software und Peripheriegeräte erachtete die Fachpresse selbst für Anwender als der von Commodore anvisierten Zielgruppe häufig den Erwerb eines Commodore-64-Systems mit zusätzlicher Software als die sinnvollere Option:\n\nKurze Zeit nach Markteinführung trat die Verkaufsmisere des Commodore Plus/4 begleitet von drastischen Worten wie etwa „die in den USA mehr oder weniger gescheiterten C16 und Plus/4“ (Commodore Horizons, Juni 1985) immer deutlicher zutage. Infolge des sich anschließenden Preisverfalles verbesserte sich nach Ansicht vieler Computermagazine das Preis-/Leistungsverhältnis zunehmend und der Rechner wurde in Europa und insbesondere in Westdeutschland fortan mit Wohlwollen betrachtet – die zeitgemäße Technik, das BASIC 3.5 und die zusätzlichen Programme seien bestens zur Heranführung von Neulingen an die Computertechnik geeignet:\n\nDer Commodore Plus/4 erfährt mittlerweile wieder verstärkte Wahrnehmung im Internet, in Zeitschriften und Büchern und wird rückblickend als ein Paradebeispiel für die Anhäufung unglücklicher Umstände, gepaart mit Konzeptlosigkeit und schlechter Vermarktung, gesehen.\n\nDer für die Realisierung des Commodore Plus/4 gewählte Zeitpunkt erwies sich nach Meinung von Ian Matthews als ungünstig, denn der übermächtige Commodore 64 hätte stets seine langen Schatten auf das Commodore-Plus/4-Projekt geworfen. Das Wohlergehen des Commodore 64 habe die Firmenpolitik beherrscht und den Großteil der Ressourcen und Fertigungskapazitäten beansprucht. Dies sei zu Lasten einer rechtzeitigen Fertigstellung des Commodore Plus/4 gegangen und habe laut Matthews damit zu einer Verschiebung des Verkaufsstarts mit entsprechend negativen vermarktungstechnischen Folgen geführt. Seine Sicht auf Commodores Vermarktungspolitik fasst Matthews in pointierter Kurzform folgendermaßen zusammen:\n\nNach Ansicht weiterer Autoren stellte sich die bei Commodore praktizierte Verkaufsstrategie, nämlich mit neuen Computern stets auch neue Software und Peripheriegeräte an den Käufer bringen zu wollen, im Falle des Commodore Plus/4 als verfehlt heraus – nur ein Bruchteil der potentiellen Kundschaft habe bereits teuer erworbene Soft- und Hardware für ein nur wenig leistungsfähigeres Gerät erneut kaufen und vormals erstellte Datenbestände umständlich konvertieren wollen.\n\nDie Verwendung des auf den ursprünglichen geplanten Sinclair-Konkurrenten zugeschnittenen „Billig-Chip TED“ (Stefan Egger, SCACOM 20/21) und damit fehlende Sprites und mangelnde Tonerzeugungsmöglichkeiten taten dem US-amerikanischen Computermagazin \"Commodore World\" und dem deutschen Autorenpaar Allner zufolge neben den Inkompatibilitäten ein Übriges, „den Markt“ abzuschrecken, der einen vollwertigen Nachfolger für den Commodore 64 erwartet hätte.\n\nVollmundig angekündigte Bestandteile wie die frei wählbare Software, die zugunsten einer fest installierten und dazu noch als sehr schlecht eingeschätzten Software („Tri-Micro's 3 Plus 1 software is best described as barely stable.“) fallengelassen wurden, hätten laut Matthews auch die letzte noch verbliebene Zielgruppe der Heimanwender vergrault.\n\n"}
{"id": "76936", "url": "https://de.wikipedia.org/wiki?curid=76936", "title": "Commodore 16", "text": "Commodore 16\n\nDer Commodore 16, kurz C16 ist ein Acht-Bit-Heimcomputer der Firma Commodore. Er ist ein Modell aus der Commodore-264-Serie und sollte den Commodore VC 20 als Einstiegscomputer ersetzen.\n\nDer C16 wurde konstruiert, um gegen andere Computer von Timex Corporation, Mattel und Texas Instruments im Preissegment unterhalb von 100 Dollar zu konkurrieren. Er war als Nachfolger des erfolgreichen VC 20 gedacht, denn dieser war mittlerweile technisch überholt. Der damals erfolgreiche C64 war mit mehr RAM sowie besseren Sound- und Grafikchips für das unterste Preissegment nicht geeignet. Die Computer von Sinclair (ZX80/ZX81/Spectrum 16k) waren zwar günstiger als der VC 20, besaßen jedoch nicht dessen große Schreibmaschinentastatur. Jack Tramiel, Gründer von Commodore International Ltd., befürchtete, dass einige japanische Unternehmen versuchen würden, mit sehr preisgünstigen Heimcomputern (MSX) auf dem amerikanischen Markt Fuß zu fassen. So wurde der C16 ins Leben gerufen, um Commodore im unteren Preissegment ein starkes Standbein zu verschaffen. Atari bot mit dem Atari 600XL auch einen Heimcomputer für den unteren Einstiegsmarkt an, der größere Bruder, der Atari 800XL, stand in Konkurrenz mit dem Commodore 64.\n\nDer C16 wurde im Juni 1984 angekündigt und ab 1985 verkauft. Zu diesem Zeitpunkt hatten sich Mattel und TI aus dem Heimcomputermarkt zurückgezogen, japanische Unternehmen brachten zwar dann mit den MSX-Computern auch Heimcomputer auf den Markt, aber auch diese standen eher in Konkurrenz zum Commodore 64. Im Jahr 1984 galten aber die Heimcomputer mit 64 KB RAM (C64, Atari 800XL, CPC 464, MSX) bereits als Einstiegsheimcomputer, so dass man schon vorher dem C16/C116 keinen großen Erfolg versprochen hatte.\n\nWie dem kompletten Trio der 264-Serie (C16, C116 und Plus/4) war auch dem C16 kein großer Erfolg beschieden. So gab es zunächst keine nennenswerte Menge an Spielen und „richtigen“ Anwendungen.\n\nEinzig durch die Position „erster Computer von Aldi“ erlangte er in Deutschland eine gewisse Berühmtheit und erlebte einen kleinen Boom. Es handelte sich dabei ursprünglich um eine reine Abverkaufsaktion von restlichen Lagerbeständen. Der Rechner wurde bei Aldi zusammen mit einem Datasettenlaufwerk \"1531\" und einer \"BASIC-Kurs\"-Software als „Lernpaket BASIC“ für nur 149 DM angeboten. Nachdem die komplette Charge innerhalb weniger Tage und dies nur in der Hälfte des (damaligen) Bundesgebiets ausverkauft worden war, mussten aus Ersatzteil-Restbeständen noch Exemplare nachproduziert werden, um die Nachfrage halbwegs zu befriedigen. Dieser kleine Boom machte sich auch am Software-Markt bemerkbar. Eine der deutschen Software-Firmen, die daraufhin einige für die damalige Zeit und das recht einfache System hochwertige Spiele und Anwendungen herausbrachte, war Kingsoft.\n\nSehr erfolgreich war der C16/116 auch in einigen Ostblockstaaten ohne eigene Heimcomputer-Industrie, vor allem in Ungarn. Dort konnten nur wenige sich die in der DDR und der Sowjetunion gebauten Computer leisten, und die meisten westlichen Modelle waren gar nicht erhältlich. Dorthin wurden, einfach um ohne Entsorgungskosten die Lager zu räumen, weitere Restbestände des C16 und C116 zu sehr günstigen Preisen exportiert, was einen eigenen kleinen Boom auslöste. Von dort stammen auch eine Reihe von inoffiziellen Konvertierungen beliebter C64-Spiele auf den C16.\n\nNachdem der C16 und die Varianten C116 und Plus 4 in mehreren europäischen Ländern Verbreitung gefunden hatte, wurden im Zeitraum von 1983 bis ca. 1992 etwas über 400 kommerzielle Spiele und ungefähr etwas über 100 kommerzielle Anwendungsprogramme produziert und vermarktet. Auch entwickelte sich relativ früh ein eigenes, parallel zu den vorherrschenden Commodore 64/128, sowie Schneider CPC Systemen, existierendes Systemsegment im Zeitschriftenmarkt. „Compute mit“ war zur damaligen Zeit eines der führenden Computermagazine für Commodore Computer, in der Ausgabe 43/84 (1984) wurden zahlreiche Varianten der Commodore 16-Familie erstmals vorgestellt.\n\nCommodore hat sich mit dem C16/116/PLUS4 wie auch später mit dem Amiga 600 Konkurrenz für die eigenen Produkte gemacht, ohne aber die Vorgänger an Leistung zu übertreffen.\n\nDer C16 enthält ein frühes Beispiel für ein Easter Egg: Bei Eingabe des Befehls SYS 52650 werden die Namen der Entwickler \"F. Bowen, J. Cooper, B. Herd, T. Ryan\" angezeigt. \"F. Bowen\" erscheint dabei invertiert, \"T. Ryan\" blinkend. Der leitende Elektronikentwickler Bil Herd war später auch für die Entwicklung des erfolgreicheren Commodore 128 verantwortlich.\n\nÄußerlich ähnelt der C16 dem VC 20 und dem C64 (=\"Brotkastengehäuse\"), jedoch wurden Gehäuse und Tastatur farblich verändert: Die ursprüngliche Planung sah ein anthrazitfarbenes Gehäuse mit dunkelgrauem Tastenfeld und hellgrauen Funktionstasten vor. Es wurden aus umgearbeiteten C64-Gehäusen Prototypen in dieser Farbgebung angefertigt und Bilder dieser Geräte wurden u. a. für Pressefotos und als Abbildung auf der Originalverpackung verwendet. Bei der Serienproduktion wurde das neue Farbschema wieder verworfen und das äußere Erscheinungsbild an das des Commodore 264 bzw. Plus/4 angepasst: Das Gehäuse wurde in Schwarz, das Tastenfeld in Hellgrau und die Funktionstasten in Dunkelgrau abgeändert. Bei späteren Geräten variieren die Gehäusefarben von anthrazitgrau matt bis schwarz glänzend.\n\nDie Tastaturbelegung des C16 war teilweise anders als bei dem VC 20 und C64, so gab es jetzt vier getrennte Cursor-Steuertasten (Pfeiltasten), während die Rücksetztaste RESTORE zugunsten eines seitlich angebrachten Reset-Knopfes entfiel.\n\nDer C16 besaß anders als der C64 keinen Userport. Der größte Anschluss ist der Expansions-Port, an diesem wurde auch die Floppy 1551 angeschlossen. An der rechten Gehäuseseite befinden sich\n\n\nAuf der Hinterseite des Gehäuses befinden sich:\n\n\n\n\nDie komplette 264-Serie besaß drei große Probleme: Sie hatten mit Ausnahme des Plus/4 keinen Userport, die Programme waren nicht kompatibel zum C64 und Spieleprogrammierer hatten das Problem, dass der TED keine Hardwareunterstützung für Sprites besaß wie der VIC-II des C64. Der im Gegensatz zum C64 verbesserte BASIC-Interpreter bot zwar auch Grafikfunktionen, unter anderem auch Shapes; für die Spieleprogrammierung, die üblicherweise in Assembler erfolgte, war dies jedoch unerheblich. Darüber hinaus konnte der C64 mit seinem SID einen deutlich besseren Klang produzieren als der TED.\n\nGerade der TED sorgte oft für Frust, denn der TED-Chip besaß keinen Kühlkörper, erhitzte sich aber durch einen ständigen Betrieb bei hoher Taktfrequenz zu stark, so dass er durchbrennen konnte. Auch Einstecken oder Herausziehen von Kabeln an den Joystickports bei laufendem Betrieb kann den TED leicht durchbrennen lassen, da er ungeschützt mit diesen verbunden ist.\n\nDie CPU im C16 kann, da sie ungeschützt mit dem seriellen Port und dem Datasettenanschluss verbunden ist, beim Einstecken oder Herausziehen von Kabeln an diesen Ports bei laufendem Betrieb ebenfalls durchbrennen. Gegen Ende der Produktion des C16 wurden in der Schaltung noch einige Schutzdioden nachgerüstet, die das Problem mindern.\n\nCommodore sah die Inkompatibilitäten als kein großes Problem an, denn schließlich waren auch viele Besitzer eines VC-20 auf den Nachfolger C64 gewechselt, obwohl dieser nicht kompatibel war. VC-20-Benutzer, die sich für den C16 hätten interessieren können, gab es aber 1985 nicht mehr viele, und C64-Besitzern gab der in den meisten technischen Eigenschaften schlechtere C16 keinen Anreiz zu wechseln, zumal beim Erscheinungstermin des C16 bereits eine riesige Menge an Software für den C64 verfügbar war und der C16 keinen Userport für Erweiterungen hatte wie der C64. So wurde der C16 und die komplette 264-Reihe ein Flop.\n\n"}
{"id": "76938", "url": "https://de.wikipedia.org/wiki?curid=76938", "title": "Commodore 116", "text": "Commodore 116\n\nDer C116 war ein als Einstiegscomputer gedachter Heimcomputer von Commodore. Er war ein Modell der Commodore-264-Serie und dem ursprünglichen Konzept, einen Computer in der Preisklasse der preisgünstigen Sinclair Research-Konkurrenzmodelle zu veröffentlichen, am ähnlichsten. Verkauft wurde er nur in Europa, obwohl es Prototypen mit der amerikanischen NTSC-Videonorm gab. Wie allen Computern der 264er-Reihe war ihm kein großer Erfolg beschieden. Der C116 wurde, wie die technisch ähnlichen C16 und Plus/4 nur kurze Zeit als BASIC-Lerncomputer im Set mit Datasette (Commodore 1531) und BASIC-Lernkassette angeboten.\n\nDer C116 erschien im Jahre 1984. Technisch war er mit dem C16 identisch, hatte aber ein kleineres Gehäuse mit einer Gummitastatur. Er verfügte über nur 16 KB RAM, davon 12 KB für BASIC verfügbar (2 KB im Grafikmodus). Er besaß keinen Userport, womit die Fertigung aufgrund eines eingesparten ICs gegenüber dem Plus/4 günstiger wurde. Laut Entwickler Bil Herd lag das ursprüngliche Preisziel bei nur 49 US-Dollar.\n\nEine Erweiterung des RAM auf 64 KB war nicht vorgesehen. Selbst mit extern angeschlossener RAM-Erweiterungen musste die Platine modifiziert werden. Besser und stabiler war die interne Erweiterung durch einen Ersatz der beiden RAM-ICs und Einlöten einiger Drähte. Mit einem so aufgerüsteten C116 konnte die überwiegende Mehrzahl der Plus/4-Programme verwendet werden.\n\n\n"}
{"id": "77275", "url": "https://de.wikipedia.org/wiki?curid=77275", "title": "Computeralgebrasystem", "text": "Computeralgebrasystem\n\nEin Computeralgebrasystem (CAS) ist ein Computerprogramm, das der Bearbeitung algebraischer Ausdrücke dient. Es löst nicht nur mathematische Aufgaben mit Zahlen (wie ein einfacher Taschenrechner), sondern auch solche mit symbolischen Ausdrücken (wie Variablen, Funktionen, Polynomen und Matrizen).\n\nDie im engeren Sinne algebraischen Aufgaben eines CAS umfassen:\n\nDarüber hinaus gehört zum Funktionsumfang vieler CAS:\n\nIm Gegensatz zu den „General-Purpose-Systemen“, die einen möglichst großen Teil der Mathematik abdecken, gibt es viele Spezialsysteme, beispielsweise zu Gruppentheorie, Gröbnerbasen, Algebraischer Zahlentheorie etc.\n\nEines der ersten Computeralgebrasysteme war Schoonschip, das 1963 von Martinus J. G. Veltman entwickelt wurde.\n\n\n\nComputeralgebrasysteme werden auch als Firmware in portablen Rechensystemen verwendet:\n\n\n"}
{"id": "77824", "url": "https://de.wikipedia.org/wiki?curid=77824", "title": "ThinkPad", "text": "ThinkPad\n\nThinkPad ist ein Markenname für tragbare Computer, unter dem IBM seit 1992 Notebooks vermarktete. 2005 verkaufte IBM seine PC-Sparte und die Marke \"ThinkPad\" an den chinesischen Computerkonzern Lenovo. Vertragsgemäß wäre Lenovo bis 2010 berechtigt gewesen, für ThinkPads das Logo von IBM zu nutzen, verzichtete jedoch bereits seit Ende 2007 darauf.\n\nGrund für den Verkauf an einen chinesischen Hersteller war unter anderem ein Einfuhrverbot in die Volksrepublik China. Lenovo konnte als chinesischer Hersteller den chinesischen Markt mit ThinkPads versorgen und seinen Umsatz erheblich steigern.\n\nDer Name ist auf ein kleines, ledernes Notizbuch zurückzuführen, das jeder IBM-Mitarbeiter bei der Einstellung ausgehändigt bekam. Dessen Aufdruck \"Think\" sowie die englische Bezeichnung für Notizbücher, Note\"pad\", wurden zu \"ThinkPad\" zusammengezogen.\n\nDas Design der ersten ThinkPads wurde im Wesentlichen vom Designer Richard Sapper beeinflusst und am IBM-Designcenter in Yamato in Japan erarbeitet. Das ThinkPad soll von den Proportionen her an eine traditionelle Zigarrenkiste erinnern, eine einfache, rechteckige Schachtel, die erst beim Öffnen ihren Inhalt enthüllt. Die schwarze Farbe der Thinkpads soll zu dem „mysteriösen“ Aussehen beitragen und dabei helfen, den Bildschirm heller erscheinen zu lassen. Dieses Aussehen hob ThinkPads deutlich von den damals grauen Alternativen ab. Einen gut erkennbaren farbigen Akzent setzt zudem der zwischen den Tasten G, H und B befindliche rote Trackpoint.\n\nDieses Design der ThinkPads hatte sich in den Jahren seit ihrem Erscheinen 1992 nur wenig verändert und konnte somit zu einem Markenzeichen einer konservativen und auf Wertbeständigkeit achtenden Firmenphilosophie werden. Mit Einführung des ThinkPad T431s wurde das Design jedoch deutlich geändert und modernisiert. So entfiel das klassische „Clamshell“-Design, das im Vergleich zu den ersten ThinkPads ohnehin nur noch in abgeschwächter Form vorhanden war, zudem wurde die Gehäusefarbe auf einen Grauton abgeändert.\n\nLenovo garantiert die Ersatzteillieferung für alle ThinkPad-Modelle für mindestens fünf Jahre.\nDa sich das Design der Geräte seit ihrer Einführung in den Grundelementen nur wenig verändert hat, besitzen sie einen hohen Bekanntheits- und Wiedererkennungsgrad. Ein besonderes Merkmal des Designs stellen die widerstandsfähigen, aus CFK, ABS sowie Magnesiumlegierungen gearbeiteten Notebookgehäuse – teils verstärkt durch den sogenannten „Structure Frame“ aus einer Magnesiumlegierung – dar, die zur allgemein hohen Robustheit beitragen.\n\nMit dem Modell 701 aus dem Jahr 1996 ist ein ThinkPad zu Ausstellungszwecken in die Sammlung des New Yorker Museum of Modern Art aufgenommen worden.\n\nDie ersten ThinkPads, die auf der Computerbörse Comdex im Jahre 1992 vorgestellt wurden, wurden in Anlehnung an die Modellbezeichnung der Fahrzeuge von BMW in die Serien 3xx, 5xx und 7xx eingeteilt, wobei 3xx die Einstiegsklasse, 5xx die Mittelklasse und 7xx die Luxusklasse bezeichnete. Mittlerweile haben Buchstaben die Modellbezeichnung übernommen, siehe unten.\n\nDas ThinkPad ist ein komplett in schwarz gehaltenes Notebook. Auf dem Deckel ist in der unteren rechten Ecke das ThinkPad-Logo im 45°-Winkel angebracht. Beim Aufklappen des Deckels steht das Logo bei älteren Modellen auf dem Kopf, bei neueren Modellen wurde das Logo hingegen um 180° gedreht. Die Scharniere des Deckels sind besonders robust dimensioniert. Am unteren Rand des Deckels sind LEDs eingelassen, die den Akkuladestand und den Powerstate des ThinkPads anzeigen. Dabei sind die LEDs so konzipiert, dass man sie sowohl bei aufgeklapptem und geschlossenem Deckel einsehen kann. Die überwiegend schwarz gehaltene Tastatur hat weiße Buchstaben und einen auffälligen roten Trackpoint zwischen den Tasten H, B und G. Der Einschaltknopf wurde von der linken Gehäuseseite (Ziehschalter) über die Tastatur gesetzt und ist seitdem von der Mitte an den rechten Rand angeordnet worden. Für die Lautstärkenkontrolle haben ThinkPads Lautstärketasten, bei ihrer Einführung fielen die Displayhelligkeitsschieber weg und wurden durch eine Fn-Tastenkombination ersetzt. Die Lautsprecher des ThinkPads waren zunächst neben der Tastatur platziert, bevor sie unter die Vorderseite kamen. Dazu war die Vorderseite abgeschrägt. Modernere ThinkPads haben die Lautsprecher jedoch wieder neben der Tastatur. Am unteren Deckelrand befindet sich bei vielen ThinkPads ein Mikrofon. Am oberen Deckelrand ist bei älteren ThinkPads eine kleine Leuchte (ThinkLight) integriert, um die Tastatur zu beleuchten. Diese fiel bei Einführung von hintergrundbeleuchteten Tastaturen weg. An der linken Gehäuseseite sind neben den Kühlrippen Standardschnittstellen wie USB und VGA vorhanden, auf der rechten Seite findet sich oftmals neben Serial ATA und FireWire auch ein PCI-Schacht.\n\nDas ThinkPad hat ein Grundgerüst aus einem Titan-Magnesiumwerkstoff, mit dem alle Komponenten verschraubt sind. In der linken oberen Ecke des Gehäuses sitzt ein Kühlkörper, in den ein Lüfter eingelassen ist. Der Kühlkörper ist mit einer Heatpipe mit dem Prozessor und gegebenenfalls der Grafikkarte verbunden, die in der oberen linken Ecke rechts des Kühlkörpers eingebaut sind. Bei älteren ThinkPads ist der Akku als Quader ausgeführt, der neben der Wartungsklappe auf der Unterseite des ThinkPads sitzt. Bei neueren Geräten ist der Akku als Stangenakku auf der Rückseite verbaut, und kann somit nach hinten aus dem ThinkPad herausragen. Diese Möglichkeit wird genutzt um größere Akkus verbauen zu können. Das Wechseln des Akkus ist ohne das Lösen einer Schraube oder Abdeckung möglich. Die Wartungsklappe auf der Unterseite bietet Zugang zum Arbeitsspeicher, der CMOS-Batterie und PCI-Anschlüssen. Der bei den meisten ThinkPads in der oberen rechten Ecke befindliche UltraBay-Schacht beherbergt meistens ein DVD-Laufwerk, kann aber auch mit anderen Laufwerken oder einem Zusatzakku ausgestattet werden. Dazu können UltraBay-Laufwerke mit einem mechanischen Verschluss ohne das Lösen von Schrauben aus dem ThinkPad herausgeschoben werden. An der linken unteren Seite des Gehäuses befindet sich ebenfalls ein Laufwerk, in der Regel ein Festplattenlaufwerk, bei älteren ThinkPads auch ein Diskettenlaufwerk (Siehe ThinkPad 345C, Bild rechts). Durch das Lösen einer Schraube ist auch dieses Laufwerk zugänglich. Laut Notebookcheck sollen ThinkPads besonders einfach zu warten sein.\n\nAls Schutz bei Stürzen haben alle ThinkPads mit herkömmlicher magnetischer Festplatte einen \"APS\" (\"Active Protection System\") genannten Bewegungssensor, der den Schreib-/Lesekopf der Festplatte in Parkposition fährt, wenn Beschleunigungen registriert werden, die zu einem Head-Crash führen könnten.\n\nViele ThinkPads verfügen über eine \"ESS\" (\"Embedded Security Subsystem\") genannte Sicherheitslösung, die auf einem \"TCG\"-Chip basiert, mit dem Daten in Echtzeit verschlüsselt werden können und nur noch in Notebooks mit ESS und dem erforderlichen Passwort entschlüsselt werden können. Der Security-Chip entspricht bei allen neueren Modellen der Trusted Platform Module-Spezifikation 1.2 und kann daher ab Windows Vista für die BitLocker-Laufwerksverschlüsselung genutzt werden. Der Chip ist im BIOS bzw. UEFI deaktivierbar, da er unter Umständen Rückschlüsse auf die Identität des ThinkPads erlaubt.\n\nUm das Power-On- bzw. Supervisor-Passwort zu löschen, reicht bei Computern üblicherweise oft das kurze Entfernen der CMOS-Batterie von der Hauptplatine. Weil diese Passwörter bei den ThinkPads in einem (nicht flüchtigen) EEPROM gesichert werden, reicht dies zum Zurücksetzen des Supervisor-Passworts nicht aus.\n\nBekannt sind die Notebooks der ThinkPad-Reihe unter anderem für die hohe Qualität ihrer Tastaturen, welche häufig als die besten in Notebooks verbauten Tastaturen angesehen werden. Die Tastatur ist spritzwassergeschützt, viele ThinkPads besitzen zudem Ablaufkanäle für verschüttete Flüssigkeiten. Viele ThinkPads mit Displaydiagonalen ab 35,56 cm (14 Zoll) haben einen am Rand gewölbten und verschließbaren Deckel, der das Eindringen von Fremdkörpern zwischen Tastatur und Bildschirm verhindern soll. Diese Konstruktion wird als \"Clamshell-Design\" bezeichnet. (Siehe Bild rechts.)\n\nEine Besonderheit ist die Anordnung der Hotkey-Kombination zum Einschalten des \"ThinkLights\" bzw. des Backlights der Tastatur. Die Tastenkombination dafür ist so angeordnet, dass sie auch im Dunkeln zu finden ist; die beiden Tasten hierfür befinden sich in der linken unteren (Fn) und rechten oberen Ecke.\nMit der Einführung der Backlight-Tastaturen wurde diese Tastenkombination geändert (Fn + Leertaste).\n\nEin weiteres Merkmal der Tastatur ist bei älteren Modellen die blaue \"ThinkVantage\"-Taste (früher \"Access IBM\"), mit der ein Programm aufgerufen wird, das verschiedene Hilfe- und Wartungsfunktionen anbietet. Beispielsweise kann das ThinkPad so wieder in den Auslieferungszustand versetzt werden, selbst wenn das installierte Betriebssystem defekt und nicht mehr funktionsfähig ist.\nMit der *30-Serie (Ivy Bridge) wurde die Tastenfarbe geändert (schwarz) und die Beschriftung „ThinkVantage“ entfernt, mit der x40-Serie (Haswell-Generation) entfällt die ThinkVantage-Taste vollständig.\n\nBis zur Übernahme von IBMs PC-Sparte durch Lenovo und dem Erscheinen der Z-Reihe im Jahr 2005 verzichteten alle ThinkPad-Modelle auf die drei Windowstasten. Durch diesen Verzicht stand insbesondere bei den damaligen 4:3-12\"-Subnotebooks mit ihrer (im Vergleich zu größeren ThinkPads) verkleinerten Tastatur mehr Platz für die angrenzenden Steuerungs- und Alt-Tasten zur Verfügung.\n\nCharakteristisch für die ThinkPad-Reihe ist der „TrackPoint“ genannte Piezo-Stift, der sich in der Mitte der Tastatur zwischen den Tasten G, H und B befindet und der Maussteuerung dient. Dieser Trackpoint sticht als roter Punkt auf der schwarzen Tastatur und dem schwarzen (bzw. grauen) Gehäuse hervor und wurde zu einem Erkennungsmerkmal für ThinkPads, das sich auch im i-Punkt im Logo der Produktreihe wiederfindet. Während die ersten ThinkPads ausschließlich TrackPoints als Mauseingabe verwendeten, wurde 2002 mit dem Modell T30 erstmals auch eine „UltraNav“ genannte Kombination aus Touchpad und TrackPoint angeboten.\n\nDie Trackpad-Tasten der meisten Thinkpads haben einen sehr tiefen Tastengang, vergleichbar mit denen der Tastatur, statt der bei vielen anderen Laptops üblichen Tasten mit einem klickenden Druckpunkt.\nMit Einführung des ThinkPad T431s wurden die Tasten für den TrackPoint in das Clickpad integriert, was später auch bei den ThinkPads der *40-Reihe (Haswell) übernommen wurde und kontrovers diskutiert wurde. Aufgrund des negativen Feedbacks hat Lenovo die dedizierten TrackPoint-Tasten in der Broadwell-Generation (*50-Reihe) wieder eingeführt.\n\nViele ThinkPads besitzen einen zusätzlichen Wechselschacht, der bei den meisten Modellen als \"UltraBay\" bezeichnet wird. Dieser ist vielseitig einsetzbar und kann beispielsweise für folgende Komponenten genutzt werden:\n\nDie Festplatte im UltraBay ist an denselben Festplatten-Controller wie die fest eingebaute Platte angeschlossen. Somit besteht keine besondere Limitierung hinsichtlich Bootfähigkeit, Kapazität, etc. Es handelt sich um eine handelsübliche 2,5-Zoll-Festplatte, die lediglich in einen speziellen UltraBay-Rahmen eingebaut wird, um in den UltraBay-Wechselschacht geschoben werden zu können. Komponenten im Wechselschacht können bei vielen Modellen während des laufenden Betriebes ausgetauscht werden, bei den Haswell-ThinkPads mit optischem Laufwerk beispielsweise ist dies jedoch nicht mehr möglich. Die neu angeschlossenen Geräte werden sogleich erkannt, das Notebook muss hierzu also nicht neu gestartet werden.\nAusnahme hiervon sind neben HDDs mit gesetztem Passwort unter anderem der Adapter für RS232 und Parallelport, da diese Schnittstellen nicht „Hot Plug“-fähig sind.\n\nDas ist eine leichte Reiseabdeckung, die anstelle eines UltraBay-Gerätes verwendet wird. Damit kann das Gesamtgewicht des Notebooks weiter verringert werden, falls unterwegs kein UltraBay-Gerät benötigt wird. Mit dem Adapter für serielle und parallele Anschlüsse (Ultra Slim-Bauform) ist es möglich, ThinkPads der T6x- und einige Modelle der R6x-Baureihe um eine serielle RS-232- und eine parallele IEEE 1284-Schnittstelle zu erweitern. Gerade bei Wartungstechnikern von Industrieanlagen ist diese Anschlussart sehr gefragt.\n\nDas Modell 750 wurde im Jahre 1993 an Bord des Space Shuttle Endeavour eingesetzt. Seitdem waren insgesamt 54 ThinkPads bei verschiedenen Space-Shuttle-Missionen mit an Bord der Raumfähren, meist Modelle der 755er und 760er Reihe. Auch auf der russischen Raumstation MIR wurden seit Februar 1999 ThinkPads benutzt, einschließlich der 750er und 770er Reihe. ThinkPads waren bis 2016 die einzigen Notebooks, die für Langzeiteinsätze an Bord der ISS (International Space Station) zertifiziert sind. Auf der ISS befanden sich mehr als 60 ThinkPads, die dort zwischen 1998 und 2016 eingesetzt wurden.\nDie seit dem Erwerb der ThinkPad-Marke durch Lenovo eingeführten Modellbezeichnungen setzen sich aus einem Großbuchstaben, der die Produktserie bezeichnet, und einer dreistelligen Ziffer zusammen, welche die Bildschirmgröße angibt. Eine Größe von 14,1 Zoll entspricht z. B. der Ziffer 400. Ausnahmen bilden die P-,X1-,Twist-, Yoga- und Helix-Serie. Leistungsfähige Modelle mit verhältnismäßig besserer Ausstattung haben zusätzlich ein \"p\" für \"Performance\" hinter der Modellbezeichnung. Von 2008 bis 2013 verzichtete Lenovo jedoch auf das \"p\". Besonders portable Modelle erhalten ein \"s\" für \"slim\".\n\nIBM verwendete hinter dem Buchstaben der Serie eine zweistellige Zahlenkombination, die jedoch nicht die Displaygröße, sondern die Generation angab. Die i-Serien hatten vierstellige Modellnummern. Die Modellbezeichnungen von einigen Notebooks der R-, A-, Z- und T-Serie endeten wie auch einige aktuelle Modelle mit einem „p“ (z. B. A30p), was auf eine CAD-zertifizierte Grafikkarte hinweist. Die Endung „e“ (z. B. R50e) weist auf eine günstigere Ausstattung hin. Das „e“ steht für „economy“.\n\nSeit 2014 lässt Lenovo ältere ThinkPad-Serien (z. B. G-Serie) wieder aufleben, diese firmieren jedoch nicht mehr unter dem Markennamen \"ThinkPad\", sondern schlichtweg unter dem Namen \"Lenovo\".\n\nVor der Jahrtausendwende verwendete IBM eine dreistellige Zahlenkombination als Modellbezeichnung. Die Hunderterstelle gab dabei die Serie an, so etwa entsprach die \"6\" der 600er-Serie für eine höherwertige Serie, dem Vorläufer der heutigen T-Serie. Die Buchstaben hinter den Zahlenkombinationen hatten nicht zwangsläufig eine Bedeutung; Das \"T\" beim ThinkPad 710T stand für \"Tablet-Computer\", das \"X\" beim ThinkPad 600X war jedoch nur als Abgrenzung zu den Vorgängern 600E und 600 gedacht und stand nicht für eine besondere Ausstattung.\n\nAktuelle Serien:\n\n\nZeitraum ab 2008 (eingestellt):\n\nZeitraum von 1999 bis 2008:\n\n\nZeitraum von 1992 bis 1999:\n\n"}
{"id": "78248", "url": "https://de.wikipedia.org/wiki?curid=78248", "title": "Dyne:bolic", "text": "Dyne:bolic\n\ndyne:bolic (auch einfach dynebolic) ist eine ausschließlich aus freier Software bestehende Live-Linux-Distribution mit dem Schwerpunkt Multimedia. Sie wurde speziell für Medienaktivisten und Künstler geschaffen und enthält daher eine Vielzahl an Programmen für die Bearbeitung, Konvertierung und Streaming von Ton- und Filmwerken.\n\ndyne:bolic ist darauf ausgerichtet, als sogenannte Live-CD direkt von CD gebootet und benutzt zu werden. Auf dem Rechner wird deshalb keinerlei installierte Software benötigt und somit auch keine Festplatte oder ähnliche Installationsmedien. Ähnlich wie andere Live-CDs erkennt und konfiguriert sie daher beim Systemstart die vorhandene Hardware samt Peripheriegeräten automatisch. Der verwendete Linux-Kernel ist auf geringe Latenz und hohe Geschwindigkeit optimiert, um sie speziell den Anforderungen für Ton- und Filmproduktion anzupassen.\n\ndyne:bolic ist jedoch keine modifizierte Version anderer bekannter Live-CDs wie Knoppix und nutzt ab der Version 2.4 Xfce als Standard-Desktop-Umgebung auf der Basis des X.Org-Servers. Zuvor wurde statt Xfce GNU Window Maker genutzt. Die minimalen Systemanforderungen sind mit einem PC der Pentium-MMX-Klasse (i586) und 64 MB Arbeitsspeicher relativ niedrig.\n\nDie aktuelle stabile Version von dyne:bolic ist die Version 2.5, welche auf der 2.6er-Reihe des Linuxkernels basiert (2.6.18). Die Version 1.4.1 wird mit einem 2.4er-Kernel zur Nutzung auf der Xbox-Spielekonsole angeboten.\n\nDie dauerhafte Installation von dyne:bolic besteht lediglich darin den „dyne“ genannten Ordner von der CD auf eine geeignet formatierte Festplattenpartition zu kopieren („Docking“). Dieses Dateisystem wird von der CD beim Systemstart automatisch erkannt und gebootet. Daneben ist es aber auch möglich, einen Bootloader zu installieren beziehungsweise den vorhandenen so zu modifizieren, dass dyne:bolic auch ohne die CD direkt gestartet werden kann. dyne:bolic kann daneben auch Benutzereinstellungen wie eine normale Linuxdistribution dauerhaft abspeichern („Nesting“). Ab Version 2.4 kann dieses „Nest“ zum Schutz der Privatsphäre auch verschlüsselt werden.\n\nDurch die Installation zusätzlicher Module, wie Entwicklungswerkzeuge oder Büroanwendungen kann das System erweitert werden. Diese Module werden dabei im „modules“-Ordner des Systems abgelegt und beim Systemstart automatisch integriert.\n\ndyne:bolic ist auf die Anforderungen von Medienaktivisten und Künstlern ausgerichtet, um ein nützliches Werkzeug zur Erstellung von multimedialen Inhalten zu sein. Mittels spezieller Aufnahme-, Bearbeitungs-, Konvertierungs- und Streamingprogrammen ermöglicht es die Erstellung und Verbreitung von Ton- und Filmwerken. So beinhaltet es XMMS zum Abspielen von Musik, das Trackerprogramm SoundTracker, den MPEG4 Audio- und Video-Streaming-Server Mp4Live, das VJ-Programm FreeJ, das virtuelle DJ-Turntable TerminatorX, den Tonspureditor Audacity, Pure Data, Cinelerra, die Videoschnittsoftware Jahshaka, Blender für 3D-Animation, das freie Grafikprogramm GIMP, das SVG-Vektorgrafikprogramm Inkscape und die Mehrspur-Aufnahmesoftware Ardour.\n\nDaneben bietet dyne:bolic auch Textverarbeitungen wie AbiWord, Mozilla als Webbrowser, Sylpheed für E-Mail, das Chatprogramm XChat, mit Bluefish einen HTML-Editor, Lopster für Filesharing und allgemein gebräuchliche Desktophilfsprogramme, wie beispielsweise die zsh als Standard-Shell.\n\nNeuere dyne:bolic-Versionen enthalten zahlreiche auf dem auf geringe Verzögerungszeit optimierten Audioframework JACK Audio Connection Kit basierende Programme zur Musikbearbeitung. Mit openMosix, welches nicht in der 2.x-Reihe in den Kernel integriert ist, wird automatisch zwischen mit dyne:bolic gebooteten Rechnern ein Computercluster eingerichtet. So ist es z. B. möglich, mit mehreren älteren Rechnern eine beachtliche Arbeitsgeschwindigkeit zu erreichen.\n\nAb Version 2.0 verfolgen die Entwickler den Ansatz, dyne:bolic in allen Aspekten zu einer „Livedistribution“ zu machen. Die meisten Live-CD-Distributionen sind nur zu Demonstrationszwecken gedacht; um neue Programmversionen auszuprobieren, muss man bei diesen auf eine neue CD-Version warten. dyne:bolic 2 besteht aus einem Basissystem, das durch zusätzliche Module einfach an die eigenen Bedürfnisse anpassbar sein soll.\n\nDie hauptsächlichen Verbesserungen gegenüber dem Version 1.x-Zweig sind:\n\nDer Hauptautor und Verantwortliche von dyne:bolic \"Denis „Jaromil“ Rojo\" hat einige in der Distribution enthaltene Werkzeuge selbst geschrieben, wie MusE, HasciiCam und FreeJ.\n\n\n"}
{"id": "79085", "url": "https://de.wikipedia.org/wiki?curid=79085", "title": "Porter-Stemmer-Algorithmus", "text": "Porter-Stemmer-Algorithmus\n\nDer Porter-Stemmer-Algorithmus ist ein verbreiteter Algorithmus der Computerlinguistik zum automatischen Zurückführen von Wörtern auf ihren Wortstamm (Stemming). Der Algorithmus basiert auf einer Menge von Verkürzungsregeln, die so lange auf ein Wort angewandt werden, bis dieses eine Minimalanzahl von Silben aufweist. Der ursprünglich für Wörter der englischen Sprache entwickelte Algorithmus kann relativ leicht für andere Sprachen portiert werden.\n\nMaßgeblich ist genaugenommen nicht die Anzahl der Silben, sondern die Anzahl der Vokal-Konsonant-Sequenzen. Jedes Wort lässt sich als eine Zeichenkette der Form [C](VC)[V] interpretieren, wobei C für eine Folge von einem oder mehreren Konsonanten und V für eine Folge von einem oder mehreren Vokalen steht. Gemessen wird die Anzahl m der Vokal-Konsonant-Sequenzen zwischen optional führenden Konsonanten und einer optionalen Folge von Vokalen am Ende.\n\n\"Beispiele\":\n\nDie Verkürzungsregeln bestehen aus Paaren von Bedingungen und Ableitungen für verschiedene Suffixe (Wortendungen). Die Regeln sind in Gruppen zusammengefasst, die nacheinander abgearbeitet werden. Aus jeder Gruppe darf nur eine Regel angewandt werden.\n\n\"Beispiel\": \nDie erste Gruppe beinhaltet die Suffix-Verkürzungsregeln \"sses\" → \"s\", \"ies\" → \"i\" und \"s\" → \"\", die beispielsweise zu den Ableitungen \"libraries\" → \"librari\" und \"Wikis\" → \"Wiki\" führen.\nEine später folgende Gruppe besteht aus der Regel \"y\" → \"i\", so dass beispielsweise das Wort \"library\" auf den gleichen Stamm (\"library\" → \"librari\") zurückgeführt wird.\n\nAuf der Webseite des Porter-Stemmer-Algorithmus finden sich Implementierungen in mehreren Programmiersprachen. Unter \"snowballstem.org\" befindet sich die von Martin Porter entwickelte Zeichenkettenverarbeitungssprache \"Snowball\", mit deren Hilfe Porter Stemmer beschrieben werden können. Dort findet man auch einen Porter Stemmer für die deutsche Sprache.\n\nDie aus einem Wort abgeleiteten Stämme entsprechen oft nicht den linguistisch korrekten Wortstämmen. Da das Ziel des Stemmings jedoch keine linguistische Analyse ist, sondern verwandte Worte auf ein und dieselbe Zeichenkette zurückgeführt werden sollen, spielt dies keine Rolle.\n\nWie praktisch alle Stemming-Algorithmen arbeitet auch der Porter-Stemmer nicht mit hundertprozentiger Genauigkeit, so dass es bei einigen Worten vorkommen kann, dass zu viel (\"Overstemming\") oder zu wenig (\"Understemming\") abgeschnitten wird. In der Praxis ist er jedoch ausreichend gut. (siehe auch weitere Hintergrundinformationen zum Thema im Artikel Stemming).\n\n\n"}
{"id": "79088", "url": "https://de.wikipedia.org/wiki?curid=79088", "title": "Stemming", "text": "Stemming\n\nAls Stemming (\"Stammformreduktion\", \"Normalformenreduktion\") bezeichnet man im Information Retrieval sowie in der linguistischen Informatik ein Verfahren, mit dem verschiedene morphologische Varianten eines Wortes auf ihren gemeinsamen Wortstamm zurückgeführt werden, z. B. die Deklination von \"Wortes\" oder \"Wörter\" zu \"Wort\" und Konjugation von \"gesehen\" oder \"sah\" zu \"seh\".\n\nIm Jahr 1968 veröffentlichte Julie Beth Lovins den ersten bekannten Stemming-Algorithmus. Dieser Algorithmus hatte einen großen Einfluss auf die weitere Entwicklung von Stemming-Algorithmen. Ein späterer Stemmer wurde 1980 von Martin Porter veröffentlicht. Dieser Stemmer wurde zum De-facto-Standard für das Stemming englischsprachiger Texte. Porter erhielt im Jahr 2000 den Tony Kent Strix Award für seine Arbeit auf dem Gebiet der Stemming-Algorithmen und des Information Retrievals.\n\nEs wurden viele Implementierungen des Porter-Stemmer-Algorithmus geschrieben und kostenlos verteilt, von denen viele jedoch kleine Fehler enthielten. Dies führte dazu, dass diese Stemmer niemals ihr volles Potenzial abschöpfen konnten. Um diese Fehlerquelle zu beseitigen, veröffentlichte Porter um das Jahr 2000 eine offizielle Implementierung des Algorithmus. In den folgenden Jahren erweiterte er seine Arbeit, indem er mit Snowball ein Framework zum Schreiben von Stemming-Algorithmen schuf. Des Weiteren schuf er einen verbesserten Stemmer für die englische Sprache zusammen mit Stemmern für andere Sprachen.\n\nZum Stemming gibt es verschiedene Algorithmen für verschiedene Sprachen. \nDie Entwicklung eines Stemmers ist eine experimentelle Wissenschaft, da\nAlgorithmen nicht verifiziert werden können, sondern erst an Textkorpora und in der Praxis getestet werden müssen.\n\nBeispiele:\n\nEine alternative, sehr viel einfachere und weniger genaue Möglichkeit ist die Suche nach Teil-Zeichenketten, z. B. mit dem Stern-Operator. Dies bezeichnet man auch als Trunkierung.\n\nIm Gegensatz zur Suche, beispielsweise mit regulären Ausdrücken, die für Suche in großen Datenbeständen – z. B. Suchmaschinen – zu langsam wäre, wird eine Menge von Texten einmalig indexiert, um später schnell durchsucht werden zu können.\n\nIn einigen Sprachen spielt auch die Wortzerlegung und Zusammensetzung (\"lief weg\" → \"weglaufen\") eine wichtige Rolle.\n\n"}
{"id": "80445", "url": "https://de.wikipedia.org/wiki?curid=80445", "title": "Colossus", "text": "Colossus\n\nColossus (Plural: \"Colossi\") war ein früher Röhrencomputer, der in England während des Zweiten Weltkriegs speziell zur Dechiffrierung von geheimen Nachrichten des deutschen Militärs gebaut wurde. Mit seiner Hilfe wurde ab 1943 in Bletchley Park die Entzifferung der deutschen Lorenz-Schlüsselmaschine möglich.\n\nDie Verschlüsselungsmethode – Erzeugung von Pseudozufallszahlen und exklusive Veroderung – wurde durch einen schwerwiegenden Fehler eines Wehrmachtssoldaten bereits bei einem der ersten Erprobungsversuche erkannt: Eine etwa 4000 Zeichen lange Nachricht wurde mit leicht unterschiedlichem Text, jedoch mit derselben Pseudozahlenfolge zweimal übertragen, was streng verboten war.\n\nDie Deutschen verwendeten einen täglich wechselnden Schlüssel, der unterschiedliche Zahlenfolgen erzeugte. Jede Nachricht musste daher getrennt entziffert werden. Eine manuelle Dechiffrierung einer Nachricht nahm mehrere Tage bis Wochen in Anspruch. Die enthaltene Information war dann in der Regel wertlos. Der Entwurf der Maschine stammte, aufbauend auf Ideen zu einer universellen Maschine von Alan Turing, von Max Newman, der erkannte, dass die Entzifferung mit Maschinenhilfe wesentlich beschleunigt werden kann. Gebaut wurde die Maschine von Tommy Flowers im Forschungszentrum der britischen Post in Dollis Hill.\n\nColossus bestand zunächst (1943) aus 1500 Röhren, später aus 2500. Bei einer Leistungsaufnahme von 4500 W konnte die Maschine erstaunliche 5000 Zeichen (à 5 Bit) pro Sekunde verarbeiten. Der Speicher bestand aus 5 Zeichen von je 5 Bit in Schieberegistern. Die Zeichen wurden photoelektrisch von einem Lochstreifen gelesen, die Lochreihe in der Streifenmitte erzeugte den Takt, bei 5000 Zeichen/sek also 200 µs. Innerhalb eines Taktes konnten etwa 100 Boolean-Operationen auf jeder der fünf Lochreihen und anschließend auf einer Zeichenmatrix parallel durchgeführt werden. Die Treffer wurden dann gezählt.\n\nZwischen 1943 und 1946 wurden insgesamt zehn Geräte gebaut. Colossus erlaubte die Entzifferung einer Nachricht innerhalb weniger Stunden. \nColossus gilt als der erste große Elektronenrechner (Röhrenrechner). Der Spezialrechner entstand vor dem riesigen amerikanischen Röhrenrechner ENIAC, aber nach dem kleinen elektronischen Atanasoff-Berry-Computer. Colossus kam nicht gegen die Enigma zum Einsatz.\n\nColossus war kein speicherprogrammierbarer Rechner, sondern konzipiert als Spezialrechner für den Bruch von „Tunny“-Nachrichten. Er wurde über Kabelverbindungen und Schalter programmiert. Seine Existenz wurde bis in die 1970er-Jahre geheimgehalten. Als Nachfolger können die ersten speicherprogrammierbaren Digitalrechner der Universitäten Manchester und Cambridge aufgefasst werden.\n\nErst 1970 wurde die Existenz von Colossus öffentlich bekannt. Ab 1990 baute Tony Sale, Ingenieur und ehemaliger Mitarbeiter des Nachrichtendienstes MI5, den Colossus für das Computermuseum in Bletchley Park nach. Am 6. Juni 1996 hatte er seinen ersten erfolgreichen Lauf. Zur offiziellen Einweihung des Gerätes am 16. November 2007 sandten deutsche Funkamateure einen mit einer originalen Lorenz-Maschine verschlüsselten Text. Colossus „knackte“ die Nachricht in drei Stunden und 35 Minuten, sie enthielt eine Einladung zu einer Ausstellung mit historischen Computern im Heinz Nixdorf MuseumsForum, Paderborn. Die Entzifferung verzögerte sich um rund 45 Minuten, da eine der 2400 Röhren des Rechners geplatzt war. Im parallel zur Übertragung veranstalteten Wettbewerb gelang es einem Funkamateur, das Signal mit der Antenne seines Clubs aufzufangen, akustisch zu bearbeiten und es mit einem Laptop unter FreeBSD schließlich in 46 Sekunden zu entziffern. \n\nDer Nachbau des Colossus ist im \"National Museum of Computing\" in Bletchley Park zu besichtigen.\n\n1970 entstand der Science-Fiction-Thriller „Colossus: The Forbin Project“ nach einem Roman von Dennis Feltham Jones. Er bezieht sich zwar nicht ausdrücklich auf seinen älteren Namensvetter, dennoch sind Parallelen erkennbar: Ein gigantischer Supercomputer, eingebettet in das Innere eines Berges, soll helfen, den Weltfrieden zu sichern. Professor Forbin (dargestellt von Eric Braeden), der geniale Entwickler des Computergiganten, welcher sogar einen Nuklearkrieg überstehen soll, rechnet nicht mit dem gefahrvollen Potential seiner Kreation.\n\n\n"}
{"id": "80990", "url": "https://de.wikipedia.org/wiki?curid=80990", "title": "Microsoft SQL Server", "text": "Microsoft SQL Server\n\nDer Microsoft SQL Server ist ein relationales Datenbankmanagementsystem von Microsoft.\n\nMicrosoft SQL Server entstand aus einer Zusammenarbeit der beiden Unternehmen Microsoft und Sybase Ende der 1980er Jahre. 1989 wurde die erste Version für das von Microsoft und IBM entwickelte Betriebssystem OS/2 veröffentlicht. Das Produkt entsprach prinzipiell dem Sybase SQL Server 4.0 für Unix und VMS. 1992 erschien der Microsoft SQL Server 4.2 für OS/2 1.3. Im Anschluss mit der Veröffentlichung von Windows NT im Jahr 1993 erschien schon bald Microsoft SQL Server 4.21, der anstatt auf OS/2 auf Windows NT als Betriebssystem setzte. In dieser Zeit lockerte sich außerdem die Kooperation zwischen Microsoft und Sybase. Im Jahr 1995 erschien mit Microsoft SQL Server 6.0 eine eigenständige Weiterentwicklung der Kooperation, dem 1996 die Version 6.5 folgte.\nMit der Version 7.0, die im Jahr 1999 erschien, verabschiedete sich Microsoft von der mit Sybase entwickelten Codebasis und brachte eine vollkommen neue Datenbank-Engine auf den Markt. Diese war auch Basis in den darauffolgenden Versionen ab SQL Server 2000.\n\nAb der Version SQL Server 2017 wurde der Support für folgende Linux-Systeme ergänzt: Red Hat Enterprise Linux, SUSE Linux Enterprise Server, Ubuntu und Docker.\n\nDer \"SQL Server\" ist ein relationales Datenbankmanagementsystem, das sich am Standard der aktuellen SQL-Version orientiert.\nDer Microsoft SQL Server liegt in verschiedenen Editionen vor, die ein vielfältiges Angebot abdecken. Die Editionen unterscheiden sich vor allem im Preis, ihren Funktionen und Hardwareeinschränkungen. Der MSSQL-Server kann auch als Data-Warehouse genutzt werden, indem es den Mitarbeitern in einem Unternehmen eine Sicht auf das Geschäft und dessen Daten ermöglicht. Durch seine Business-Intelligence-Plattform bietet er eine skalierbare Infrastruktur, die es der IT ermöglicht, die Nutzung von Business Intelligence im gesamten Unternehmen zu fördern und Business Intelligence dort bereitzustellen, wo Anwender es wünschen.\nDer SQL Server besteht aus vielen Services, wie z. B. Analysis Services, Reporting Services und Integration Services, und Tools, z. B. den SQL Server Data Tools (SSDT).\n\nMicrosoft SQL Server verwendet für Datenbankabfragen die SQL-Variante T-SQL (Transact-SQL). T-SQL fügt hauptsächlich zusätzliche Syntax zum Gebrauch in Stored Procedures und Transaktionen hinzu. Weiterhin unterstützt MSSQL OLE DB und ODBC (Open Database Connectivity).\n\nSeit SQL Server 2005 (Codename „Yukon“) werden unter anderem Programmiersprachen, welche auf der .NET CLR laufen, für das Erstellen von Stored Procedures unterstützt. Mit Visual Studio wird seit 2005 auch eine passende IDE mitgeliefert.\n\nIn einer Windows-Installation (sowohl auf Servern als auch auf Individualsystemen) können gleichzeitig mehrere (gleiche oder unterschiedliche) MSSQL-Server laufen, die als Instanzen bezeichnet werden. Jede Instanz kann wiederum mehrere Datenbanken enthalten. \n\nMicrosoft bietet eine Reihe von Techniken an, um Daten redundant zu speichern.\n\nEs ist möglich, gewisse Redundanztechnologien gleichzeitig parallel zu betreiben – zum Beispiel die Datenbankreplikation zusammen mit einer AlwaysOn-Availability-Group. Die „Datenbankreplikation“ teilt sich weiterhin in die Untervarianten „Snapshot Replication“, „Transactional Replication“, „Merge Replication“ und „Peer-To-Peer-Replication“.\n\nDer unter dem Codenamen \"Denali\" entwickelte SQL Server 2012 wurde am 6. März 2012 veröffentlicht. Bei den Editionen ließ Microsoft die erst beim Vorgänger eingeführte \"Datacenter Edition\" wieder wegfallen und führte stattdessen eine neue Edition \"Business Intelligence\" ein. Die Neuerungen oder Verbesserungen fanden in allen Bereichen des SQL Servers statt, darunter im Datenbankmodul, dem Analyse Service, dem Reporting Service und der Replikation.\n\nZu den wichtigsten Änderungen bzw. Neuerungen gehören unter anderem Performanceoptimierung für SharePoint, die Hochverfügbarkeitslösung „Always On“, die Cloud-Lösung „Azure“, die Data Quality Services „DQS“, Contained Database und Columnstore Index.\n\nAußerdem wurde die SQL-eigene Programmierschnittstelle T-SQL um weitere Funktionen ergänzt. Auf der anderen Seite wurden einige Funktionen der Vorgängerversionen in SQL Server 2012 entfernt und viele Funktionen als Deprecated gekennzeichnet, was bedeutet, dass diese Funktionen in zukünftigen Versionen wegfallen.\n\nDie Version 2014 (Codename \"Hekaton\", freigegeben am 1. April 2014) des SQL Servers wurde in Hinblick auf die performante Verarbeitung großer Datenmengen weiter optimiert, um die Echtzeit-Transaktionsverarbeitung zu verbessern. Hierzu gehören die Fähigkeit, Tabellen oder ganze Datenbanken komplett im Hauptspeicher zu halten (In-Memory-Datenbank) sowie eine verbesserte Handhabung spaltenbasierter Indexe.\n\nEs ist nun auch möglich, SQL-Server-Datendateien in Azure abzulegen und eine SQL-Server-Datenbank auf einem virtuellen Computer in Azure zu hosten.\n\nVerschlüsselung von Sicherungen während des Sicherungsvorganges mittels AES 128, AES 192, AES 256 und Triple DES wurde hinzugefügt.\n\nDie Unterstützung von Failoverclusterinstanzen wurde verbessert.\n\nDie aktuelle Version 2017 wurde am 2. Oktober 2017 freigegeben.\n\nAbhängig von der Version des Microsoft SQL Servers gibt es verschiedene Editionen des Produkts. Die Editionen unterscheiden sich entweder in ihrem Funktionsumfang oder der maximalen Hardwareunterstützung. So steht höherwertigen Editionen der Zugriff auf mehr Arbeitsspeicher oder mehr Prozessoren zur Verfügung, wodurch sie mehr Leistung bieten. Der jeweilige Name einer Edition deutet dabei auf seinen angedachten Einsatzort, respektive Einsatzzweck hin. So wird beispielsweise die unter SQL Server 2008 teuerste Version \"Datacenter-Edition\" für große Rechenzentren verwendet, während die \"SQL Server Web Edition\" speziell für Webhoster oder Websites gedacht ist.\n\nDie folgende Tabelle listet eine Übersicht verschiedener SQL-Server-Versionen und ihrer erhältlichen Editionen:\n\nNachfolgend sind die von Microsoft SQL unterstützten Datentypen aufgelistet. Jeder der Typen unterstützt neben seinem Wertebereich noch den Nullwert.\n\n\n"}
{"id": "81360", "url": "https://de.wikipedia.org/wiki?curid=81360", "title": "Rastergrafik", "text": "Rastergrafik\n\nEine Rastergrafik, auch Pixelgrafik (englisch \"raster graphics image\", \"digital image\", \"bitmap\" oder \"pixmap\"), ist eine Form der Beschreibung eines Bildes in Form von computerlesbaren Daten. Rastergrafiken bestehen aus einer rasterförmigen Anordnung von so genannten Pixeln (Bildpunkten), denen jeweils eine Farbe zugeordnet ist. Die Hauptmerkmale einer Rastergrafik sind daher die Bildgröße (Breite und Höhe gemessen in Pixeln, umgangssprachlich auch \"Bildauflösung\" genannt) sowie die Farbtiefe.\n\nDie Erzeugung und Bearbeitung von Rastergrafiken fällt in den Bereich der Computergrafik und Bildbearbeitung. Eine andere Art der Beschreibung von Bildern sind Vektorgrafiken.\n\nRastergrafiken eignen sich zur Darstellung komplexerer Bilder wie Fotos, die nicht mit Vektorgrafiken beschreibbar sind. Rastergrafiken können aus vorhandenem Material – etwa mit einem Scanner oder einer Digitalkamera – digitalisiert oder mit Bildbearbeitungssoftware erstellt werden.\n\nDie Pixel einer Rastergrafik können nur Informationen wie die Farbe enthalten. In letzteren Anwendungsgebieten haben Rasterdaten gegenüber Vektordaten den Nachteil, dass sie relativ ungenau sind und sich schlecht für topologische Analysen eignen.\n\nMinimalistische Pixelgrafiken sind inzwischen zu einer populären Kunstform geworden. Das Spektrum der so genannten Pixel-Art reicht dabei von Handylogos über Websites bis hin zu TV-Spots und Werbeplakaten. Von ihrer Beschaffenheit her ähneln Pixelgrafiken traditionellen Techniken wie dem Mosaik und der Kreuzstickerei.\n\nHeutige Computerbildschirme werden ausschließlich über eine Rastergrafik, die im Framebuffer abgelegt ist und den gesamten Bildschirminhalt enthält, angesteuert. Daher müssen Vektorgrafiken vor der Ausgabe gerastert werden. Bei der Ausgabe auf manchen Druckern übernimmt ein Raster Image Processor diesen Schritt. Alle Drucker mit linearem Papiervorschub eignen sich für die Wiedergabe von Pixelgrafiken, während Plotter mit linienorientierten Vektorgrafiken angesteuert werden sollten.\n\nDie bekanntesten Grafikformate für Rastergrafiken sind BMP, GIF, JPEG/JFIF, PNG und TIFF. Einige dieser Formate wenden verlustfreie oder verlustbehaftete Bildkompression an.\n\nZu den Nachteilen von Raster- gegenüber Vektorgrafiken gehört der meist relativ hohe Speicherverbrauch. Da Rastergrafiken nur aus einer begrenzten Anzahl von Pixeln bestehen, werden zweidimensionale geometrische Formen nur angenähert. Dabei tritt der Treppeneffekt oder gar der Alias-Effekt zutage, die mittels Antialiasing gedämpft werden können. Bei geometrischen Transformationen einer Rastergrafik, wie etwa der Skalierung, können Informationen verloren gehen oder Farbtöne erzeugt werden, die vorher nicht vorhanden waren.\n\nDie Umwandlung einer Vektorgrafik in eine Rastergrafik nennt sich Rasterung. Dieser Vorgang erfolgt jedes Mal, wenn eine Vektorgrafik auf dem Bildschirm dargestellt oder ausgedruckt wird.\n\nDer umgekehrte Weg, die sogenannte Vektorisierung von Rastergrafiken, ist wesentlich schwieriger. Sie kann manuell durch Nachzeichnen erfolgen, oder aber durch spezielle Funktionen eines Vektorgrafikprogramms wie zum Beispiel die Funktion Trace Bitmap von Inkscape, oder auch durch spezialisierte Programme wie Corel PowerTRACE oder Potrace. Dieser Weg ist zudem oft fehlerbehaftet, da grafische Primitive wie Geraden, Kreise oder Kurven in der Rastergrafik nur ungenau abgebildet sind und daher nicht exakt erkannt werden können. Besonders große Probleme entstehen durch Farb- oder Helligkeitsgradienten in Rasterbildern die beim Vektorisieren nur als diskrete Bänder (Banding) ungenügend approximiert werden.\n\n\n"}
{"id": "81373", "url": "https://de.wikipedia.org/wiki?curid=81373", "title": "Gammakorrektur", "text": "Gammakorrektur\n\nDie Gammakorrektur ist eine namentlich im Bereich der Bildverarbeitung häufig verwendete Korrekturfunktion zur Überführung einer physikalisch proportional (d. h. linear) wachsenden Größe in eine dem menschlichen Empfinden gemäß nicht linear wachsende Größe. Mathematisch gesehen handelt es sich bei dieser Funktion um eine Potenzfunktion mit einem oft nur kurz Gamma genannten Exponenten als einzigem Parameter.\n\nDie Gammakorrektur überführt eine Eingangsgröße I gemäß einer Abbildungsvorschrift in eine Ausgangsgröße I :\n\nDabei gilt:\n\nund\n\nJe nach Größe des Exponenten γ (gamma) unterscheidet man drei Fälle:\n\n\nIn der Grafik sind drei Beispielkurven eingezeichnet für γ=3, 0,3 und 1.\n\nManchmal wird auch der Kehrwert des Exponenten als Gamma bezeichnet: γ → 1/γ\n\nDer Normtext von DIN EN 61966-2 Anhang A (Farbmessung und Farbmanagement) verweist dabei auf die anfängliche Verwendung des Begriffs Gammakorrektur in der Fotografie durch Ferdinand Hurter und Vero Charles Driffield seit den 1890er Jahren. In der Fotografie wird er bisweilen synonym für Anstieg, Gradient und Kontrast benutzt. Definitionen für Bildschirmwiedergabe stammen von Irving Langmuir in den 1910er Jahren und von Oliver in den 1940er Jahren.\n\nZu unterscheiden sind auch „Elektronenkanonen“-Gammawerte von „Leuchtstoff“-Gammawerten. Die DIN EN-Norm 61966-A spricht deshalb von einer „Mehrdeutigkeit in der Definition des Begriffes 'Gamma'“ und empfiehlt, den Begriff in normativen Zusammenhängen nicht mehr zu verwenden.\n\nDie vom Menschen empfundene Helligkeit steigt in dunklen Bereichen steiler und in hellen weniger steil an. Die Stevenssche Potenzfunktion ordnet dem menschlichen Auge dabei ein Gamma von ca. 0,3 bis 0,5 zu.\nSoll das Helligkeitssignal eines Anzeigegerätes, beispielsweise eines Monitors, linear wahrgenommen werden, muss es daher mit dem reziproken des obigen Gammawerts (ca. 3,3 bis 2) vorverzerrt werden, damit sich beide Nichtlinearitäten für den Betrachter am Ende wieder aufheben. Ein typischer Wert für Bildschirme ist etwa ein Gamma von 2,2.\n\nDas mittlere Originalbild zeigt einen Graustufenkeil und drei Stufenkeile in den gesättigten Farben Rot, Grün und Blau, die jeweils 32 Felder mit linear zunehmender Helligkeit haben. Das linke Bild zeigt das Bild nach einer \"Gammakorrektur\" mit dem Exponenten formula_4 und das rechte Bild nach einer \"Gammakorrektur\" mit dem Exponenten formula_5. Die Helligkeiten der dunkelsten und hellsten Felder bleiben immer erhalten. Das jeweils 17. Feld von links hat im Originalbild (formula_6) eine Helligkeit von 50 %, im linken Bild (formula_4) eine Helligkeit von formula_8 und im rechten Bild eine Helligkeit von formula_9.\n\nDer Gamma-Wert eines durchschnittlichen mit Windows betriebenen Monitors liegt bei 2,2. Diese Einstellung empfiehlt sich, da fotografische Labore ebenfalls mit einem Gamma von 2,2 arbeiten, und so ein am Monitor für gut befundenes Bild auch dementsprechend belichtet wird. Über die Angabe eines Ausgabefarbprofils kann man das Gamma indirekt festsetzen. Hier bietet sich die Verwendung des sRGB-Profils für den „normalen“ Benutzer an, dem ein Gamma von näherungsweise 2,2 zu Grunde liegt.\n\nDerselbe Monitor an einem Mac wurde bis vor kurzem standardmäßig mit einem Gamma von 1,8 betrieben. Die Gründe entstammen noch der Zeit vor dem Color Management des ICC. Ein Gamma von 1,8 war für einen Workflow ohne Farbmanagement gedacht, damit die Monitordarstellung besser der Tonwertreproduktion von Schwarzweißdruckern entsprach. Heutzutage werden die Gammawerte durch Farbprofile (z. B. vom ICC) ergänzt; das Standard-Mac-Farbprofil enthält dabei ein Gamma von 1,8. Ab Mac OS X 10.6 (Snow Leopard) liegt der Standard-Gammawert bei 2,2.\n\nIm Gamma Testbild besteht ein einzelner farbiger Bereich zum Großteil aus einem karierten Muster mit 0 % und 100 % Helligkeit . Das Auge des Beobachters sieht dies als 50 % relative Leuchtdichte und integriert sie zu einer Helligkeit gemäß formula_10 . Dieser Zusammenhang ist als Weber-Fechner-Gesetz, genauer Stevenssche Potenzfunktion, bekannt und gilt für alle Sinnesorgane. Die einfarbigen Bereiche haben eine Helligkeit entsprechend dem eingetragenen Gamma-Wert. Ein Gamma von 1,0 hat die Helligkeit 50 %, Gamma 1,8 hat 68 % und Gamma 2,2 hat 73 %.\n\nDie Farben werden wegen der Farb-Empfindlichkeit des Auges in der Reihenfolge Grün, Rot und Blau abgeglichen. Der Abgleich erfolgt entweder über die Farbausgleich anpassen (color balance) Einstellung des Betriebssystems oder über die Primärfarben individuelle Helligkeits-Einstellung des Grafikkarten Konfigurationsprogramms. Ab MS-Windows 7 gibt es das Programm dccw.exe mit dem Gammakorrektur und Farbanpassung auf Betriebssystem-Ebene eingestellt wird . Andere Programme können auch ein ICC-Profil erstellen und laden . Siehe Monitorkalibrierung für eine Anleitung zur Einstellung für eine gute Wiedergabe von Monitor, Grafikkarte und Betriebssystem.\n\nHat man so den tatsächlichen Gamma-Wert seines Monitors herausgefunden, kann man mit Hilfe der Grafikeinstellungen seines Computers einen eigenen, von den Werksvorgaben unabhängigen Wert einstellen. Indem man in den Grafikeinstellungen den Kehrwert des gefundenen Wertes als Gamma einstellt, wird die Gamma-Korrektur des Monitors (theoretisch) neutralisiert; es ergibt sich ein tatsächlicher Gamma-Wert von 1. Will man einen tatsächlichen Gamma-Wert von z. B. 2,2 erhalten, so muss man den Kehrwert mit 2,2 multiplizieren und diesen Wert als Gamma-Wert einstellen. Hat man z. B. einen tatsächlichen Gamma-Wert von 1,3 festgestellt, so muss man in den Grafikeinstellungen den Wert 0,77 einstellen, um die Gamma-Korrektur zu neutralisieren, und 1,69, um den tatsächlichen Gamma-Wert von 2,2 zu erreichen.\n\nTonwertkorrekturen wie Änderungen der Helligkeit, des Kontrasts usw. überführen Farbwerte eines Bildes in andere Farbwerte desselben Farbraums. Ist die Korrekturfunktion dabei eine Potenzfunktion der Form A=E, spricht man von einer Gammakorrektur.\nIn grober Vereinfachung hebt eine solche Korrektur bei Verwendung von Gammawerten < 1 zu dunkle Bildpartien in ihrer Helligkeit an, während sie bei Verwendung von Gammawerten > 1 - umgekehrt - zu helle Bildpartien wieder in ihrer Helligkeit zurücknimmt. Bei einem Gammawert = 1 schließlich bleibt, da E = E ist, alles beim alten.\n\nManche Bildbearbeitungsprogramme (z. B. GIMP) geben bei der Gamma\"korrektur\" nicht den Gammawert selbst, sondern den Kehrwert 1/Gamma an, so dass eine Erhöhung des Wertes einer Erhöhung der Helligkeit entspricht.\n\nWird bei einem zu dunklen Bild der Helligkeitswert erhöht, so werden alle Farbtöne um den gleichen Wert aufgehellt. Die hellsten und die dunkelsten Farbtöne gehen verloren. Die sehr hellen Töne werden zu reinem Weiß. Schwarz und die sehr dunklen Töne werden zu einem Grauton. Insgesamt reduziert sich also das Helligkeitsspektrum des Bildes und es scheint sich ein heller Schleier über das Bild zu legen. Durch eine Erhöhung des Kontrasts kann dies teilweise ausgeglichen werden. Die dunklen Farbtöne werden so wieder hergestellt. Die Umwandlung der hellen Töne zu Weiß vergrößert sich jedoch noch.\n\nDie Aufhellung eines Bildes durch die Änderung des Gamma-Werts hat demgegenüber den Vorteil, dass die dunklen Töne stärker aufgehellt werden als die Hellen. Auch bleibt das Helligkeitsspektrum insgesamt erhalten. Das Spektrum (die Varianz) der hellen Töne wird \"gestaucht\", während das der dunklen Töne sich vergrößert.\n\nViele Bildbearbeitungsprogramme bieten die Möglichkeit, die in der Grafik abgebildeten Korrekturkurven individuell zu bearbeiten, um sie der gewünschten Helligkeitsverteilung bzw. der des Ursprungsbildes anzupassen.\n\nWird die Korrekturfunktion mit einem Koeffizienten formula_11 multipliziert (formula_12), ergibt sich daraus zusätzlich zu der eigentlichen Gammakorrektur noch eine Kontrasterhöhung (formula_13) bzw. -verminderung (formula_14), bei Hinzufügen einer additiven Konstante formula_15 dagegen (formula_16) über die Gammakorrektur hinaus eine Helligkeitserhöhung (formula_17) bzw. -verminderung (formula_18). In der Praxis führen so allein schon die beiden eben genannten zusätzlichen Optionen schnell zu einer verwirrenden Vielfalt von Korrekturmöglichkeiten - die obige Abb. rechts zeigt daher noch einmal separat das jeweils Charakteristische der Helligkeits-, Kontrast- und Gammawertkorrektur.\n\nBetrachtet wird zunächst ein abbildendes System mit \"ideal-linearem\" Verhalten:\n\n\nSoweit das einfachste Beispiel eines geschlossenen Systems, das genau aufeinander abgestimmt ist und alle informationstechnischen Anforderungen missachtet. In der Realität haben wir es aber mit offenen Systemen zu tun und wollen die Daten über die Kamera am Computer bearbeiten, diese auf den unterschiedlichen Ausgabemedien ausgeben und immer dasselbe Ergebnis sehen.\n\n\nDa es keine ideal-linearen Systeme gibt, müssen bei diesem Prozess noch zwei (bzw. sechs) Gammakorrekturen hinzugefügt werden. Zum einen hat der Chip ein nichtlineares Verhalten für die drei Farbkanäle unterschiedlicher Art. Diese müssen durch jeweils eine weitere Gammakorrektur ausgeglichen werden. Zum anderen verhalten sich die drei Leuchtstoffe eines Bildschirms nichtlinear. Die Farbkorrekturen sind bauteilbedingt und werden in der Regel schon in den Geräten selbst implementiert. Nur das Alter eines Geräts lässt das Resultat dieser Korrektur verschlechtern. Der Anwender bekommt von diesen Korrekturen in der Regel nichts mit. Ist die Korrektur veraltet, entsteht meist ein Farbkippen (ein Farbstich unterschiedlicher Färbung und Intensität über einem Graukeilverlauf).\n\nJedes abbildende System muss sich mit dem Problem der Helligkeitsempfindung auseinandersetzen. Dadurch ist eine Fülle von Gammakorrekturen entstanden. Die Farbfernsehsysteme PAL und NTSC, die Betriebssysteme Microsoft Windows und Mac OS Classic sowie unixoide Systeme, aber auch Druckerhersteller kennen das Problem (siehe auch Tonwertzuwachs).\n\nRGB-Monitore und TV-Geräte haben verschiedene Helligkeitsprofile und erfordern häufig eine Korrektur, um das Bild optimal darzustellen.\n\nIdealerweise würde ein Ausgabegerät den Helligkeitswert 0 als Schwarz und den Helligkeitswert 1 als Weiß abbilden und alle dazwischen liegenden Werte linear zwischen Schwarz und Weiß als unterschiedliche Grauwerte darstellen. Dies entspräche einem Gamma von 1.\n\nAufgrund produktionstechnisch bedingter Faktoren ist eine solche Linearität bei Aufnahmegeräten (z. B. Kameras) oder Ausgabegeräten (z. B. Bildröhren) nicht zu erreichen. Meist spielt die nichtlineare Eingangskennlinie eines Bildwandlers (z. B. eines LCDs) oder eines Kamera-CCD-Chips die entscheidende Rolle. Das heißt, dass bei einem Bild mit konstanter Helligkeitsänderung von Schwarz nach Weiß bei einem Gamma abweichend von 1 entweder die hellen und dunklen Stellen überproportional detailliert abgebildet werden oder aber die mittleren Graustufen.\n\nDamit im weiteren Produktionsweg keine Helligkeitsinformationen verloren gehen oder aber überbetont dargestellt werden, hat jedes Gerät, das eine nichtlineare Übertragungsfunktion besitzt, die Möglichkeit einer Gammakorrektur zur Linearisierung der Abbildungsleistung.\n\nBesitzt ein Gerät mehrere Bildwandler für unterschiedliche Farben, wie z. B. eine Dreiröhrenkamera, so kann dort aufgrund unterschiedlicher Empfindlichkeiten eine Gammakorrektur für jeden einzelnen Farbkanal notwendig sein.\n\nDie Gammakorrektur ist in der digitalen Bildverarbeitung auch als Potenztransformation bekannt.\n\nDer Zusammenhang zwischen den digitalen und den radiometrischen Daten wird in der Literatur als die Kathodenstrahlröhren-Übertragungsfunktion formula_24 \"(Gamma)\" bezeichnet. Der besseren Übersichtlichkeit halber werden wir diese Gesamtfunktion in zwei Teile, den digitalen Teil D (Grafikkarte) und den analogen Teil A (Monitoreingang, Bildschirm), aufspalten.\n\nDer Zusammenhang zwischen der Monitoreingangsspannung und der resultierenden Helligkeit (Leuchtdichte) des einzelnen Bildpunktes, also unsere Funktion A, folgt einer allgemeinen Potenzfunktion. Im einfachsten Modell ist:\n\nmit formula_26: auf 1 normierte Eingangsspannung und formula_19: auf 1 normiert Helligkeit.\n\nGrundlage dafür ist das Verhalten der beschleunigten Elektronen innerhalb der Bildröhre, wobei der Haupteffekt auf der abschirmenden Wirkung der Elektronenwolke in der Umgebung der Kathode beruht. Dieses einfache Modell wird durch die Hinzunahme von Konstanten erweitert, die diverse Monitorparameter abbilden. Das Optimum wäre ein linearer Zusammenhang mit Nullpunkt bei Null, maximales Ausgangssignal bei maximalem Eingangssignal sowie linearem Zusammenhang, d. h. formula_28.\n\nDie Leuchtdichteabhängigkeit der Farbphosphore bezüglich der Stromstärke ist ebenfalls mit einer Potenzfunktion beschreibbar, deren Exponent bei ca. 0,9 liegt. Daraus ergibt sich ein Gesamtexponent formula_24 zu 1,6 (Fernsehen) über 1,8 (Mac-Systeme) bis 2,2 (IBM-PC-kompatible Systeme) für Computermonitore.\n\nDie resultierende Abhängigkeit kann beschrieben werden durch:\n\nmit den Parametern\n\nDer Begriff des Gammas wurde erstmals in der Sensitometrie, also zur Beschreibung der Empfindlichkeit von fotografischem Material, eingeführt, wobei es neben dem Gamma-Wert auch verschiedene andere, fotografisches Material in dieser Hinsicht charakterisierende Parameter gibt.\n\nZur Bestimmung des Gamma-Werts eines fotografischen Materials, z. B. Schwarzweiß-Films oder -Fotopapiers, untersucht man mit Hilfe einer Dichte- oder Gradationskurve (siehe Beispielbild) die Steilheit seiner optischen Dichte, d. h. Schwärzung, in Abhängigkeit von der Belichtung und definiert dabei als Gamma-Wert (formula_24) die Steigung einer Tangente an den geradlinigen Teil dieser Kurve. Da die Schwärzung von Fotomaterial logarithmisch mit der Belichtung zunimmt, muss dabei in der mathematischen Formulierung dieses Zusammenhangs statt der Belichtung selbst ihr Logarithmus verwendet werden:\n\nformula_38.\n\nAnhand des so ermittelten Gamma-Werts kann man anschließend z. B. Fotopapiere nach ihrer Gradation in „harte“ und „weiche“ unterscheiden, d. h. solche, die auf eine \"Zunahme\" der Belichtung empfindlicher als „normal“ reagieren, oder auch weniger empfindlich. Fälschlicherweise wird der dabei beobachtete Gamma-Wert oft auch zur Beschreibung linearer Tonwertkorrekturen (s. o.) herangezogen, obwohl er in diesem Zusammenhang eher ein Maß für den Kontrast des Bildes ist.\n\n\n"}
{"id": "82205", "url": "https://de.wikipedia.org/wiki?curid=82205", "title": "MSX", "text": "MSX\n\nMSX ist ein 8-Bit-Heimcomputer-Standard seit 1982. Der Name leitet sich vom Namen des eingebauten Basic-Interpreters „MicroSoft eXtended BASIC“, abgekürzt MSX-BASIC, ab. MSX ist ein offener Standard, welcher von vielen japanischen und koreanischen Unternehmen wie Sony oder Sanyo implementiert wurde. In Europa entwickelte Philips MSX-Computer. Das Betriebssystem war ein Microsoft BASIC. In einigen Ländern wie zum Beispiel Japan oder den Niederlanden war MSX marktführend und nahm die Stellung ein, die in Deutschland und der Schweiz zur gleichen Zeit der C64 hatte. Für den Spielemarkt war auf diesem Computersystem Konami führend und entwickelte bereits die ersten Versionen der erfolgreichen Spieleserien wie Metal Gear oder die Gradius-Reihe. Auch für den Home-Office-Bereich gab es bereits Software mit Textverarbeitung und ein Karteikartensystem für den Druck von Etiketten. Der Standard wurde stets weiterentwickelt, und es gab die Architekturen \"MSX 1\", \"MSX 2\", \"MSX 2+\" und \"MSX turbo R\". Die Software war teilweise nicht kompatibel, da sich die Softwareentwickler der MSX-1-Computer nicht an die Vorgaben der Einsprungadressen ins Betriebssystem der Hersteller hielten, welches die Kompatibilität mit späteren Generationen der MSX-Computer garantiert hätte.\n\nMSX wurde 1982 unter der Federführung der von Kazuhiko Nishi gegründeten ASCII Corporation und von Microsoft Japan entwickelt. Vorläufer des MSX waren die Heimcomputer von Spectravideo. Vor allem der Spectravideo SVI-328 inspirierte Kazuhiko Nishi zu einem Standard. MSX wurde auf der Basis von SVI-328 entworfen. Eine Vielzahl japanischer und koreanischer Elektronikhersteller, Microsoft aus den USA und der europäische Hersteller Philips gingen eine Zweckgemeinschaft ein, um im explodierenden Markt der Heimcomputer einen Standard zu schaffen, der die Kompatibilität zwischen Computern verschiedener Unternehmen ermöglichen sollte, während andere Heimcomputer zueinander vollständig inkompatibel waren. Zur damaligen Zeit machten trotz des Wildwuchses an Systemen nur einige Hersteller das Rennen unter sich aus und dominierten den Markt; hierzu gehörten vor allem Commodore, Atari und Sinclair, während japanische Unternehmen nur im Heimatmarkt aktiv waren.\n\nDie einzige Möglichkeit, ein neues konkurrenzfähiges Modell zu einem vernünftigen Preis gegen die Vormachtstellung der Marktführer zu etablieren, war, einen Standard zu schaffen, um den größten Nachteil der bestehenden Systeme zu überwinden: die Inkompatibilität der verschiedenen Systeme.\n\nDennoch konnten die MSX-Rechner außer in Japan und auf den anderen Heimatmärkten der beteiligten Hersteller (Niederlande, Südamerika) keinen wirklichen Durchbruch für sich verbuchen. MSX kam einfach zu spät. Schon zu dieser Zeit war der klassische Heimcomputer ein Auslaufmodell, und mit den leistungsfähigen Nachfolgern auf 16-bit-Basis (Commodore Amiga, Atari ST, Sinclair QL) konnte die MSX-Architektur nicht mithalten.\n\nDie Hardware besteht zum großen Teil aus Standard-Komponenten ihrer Zeit: Ein Z80-Prozessor mit 3,58 MHz, der Grafikchip TMS9918 von Texas Instruments (den auch der Texas Instruments TI-99/4A verwendete) und ein Soundchip von General Instrument (AY-3-8910). Diese Komponenten waren damals nicht herausragend, aber konkurrenzfähig.\n\nEin aufgesetztes DOS namens MSX-DOS ist Datei-kompatibel mit MS-DOS und unterstützt MS-DOS-ähnliche Befehle. So konnte Microsoft den MSX für Heimcomputer bewerben und MS-DOS für Personal Computer.\n\nDer integrierte BASIC-Dialekt ist mit seinem umfangreichen Befehlssatz (auch für Sound, Grafiken und Sprites) dem vergleichbarer Computern (wie dem des C64) überlegen.\n\nEine Besonderheit der MSX-Heimcomputer ist es, dass die einzelnen Hersteller eigene Erweiterungen (Software und/oder Hardware) in ihre MSX-Heimcomputer einbauten. So ist zum Beispiel der CX5-M von Yamaha zusätzlich mit einem leistungsfähigeren Soundsystem ausgestattet (8-fach polyphoner DX-7-ähnlicher FM-Synthesizer mit 4 Operatoren), das zum Beispiel eine MIDI Schnittstelle und die Anschlussmöglichkeit für ein externes Keyboard mitbringt. Sony baute in seinen HitBit eine Datenbankanwendung ein.\n\nMSX war in Teilen der Welt – überwiegend den Heimatmärkten der Hersteller – sehr erfolgreich, vor allem in Japan, Brasilien, Südkorea und einigen arabischen und europäischen Staaten (zum Beispiel Niederlande). In den USA und in Deutschland waren MSX-Rechner nur mäßig erfolgreich. Heute genießt er unter Anhängern von Vintage-Computern einen Kultstatus vor allem in den Ländern, in denen sich der MSX gegenüber dem dominanten C64 einen größeren Marktanteil erobern konnte. Für den Standard-MSX wird immer noch Hardware produziert. Jüngste Entwicklung ist der 1-Chip-MSX, der wie eine Spielekonsole aussieht und die alten Cartridges unterstützt. Es gibt auch einen Hersteller \"Sunrise for MSX\" in der Schweiz und in den Niederlanden, der Hardware-Erweiterungen produziert, wie zum Beispiel IDE-Schnittstellen für Festplatten oder ein Soundsystem.\n\nEs gab vier Generationen des Standards: MSX 1 (1983), MSX 2 (1986), MSX 2+ (1988) und MSX turbo R (1990). Die ersten drei waren 8-Bit-Rechner auf Basis des Z80, MSX turbo R basierte auf dem Zilog Z800. Während MSX 1 von über einem Dutzend weltbekannter Unternehmen unterstützt wurde, war MSX turbo R nur noch der Versuch eines einzelnen Herstellers (Matsushita, mit seiner Marke Panasonic) den Standard weiterzuführen.\n\nIn Südkorea erschienen, entwickelt von Daewoo und angeboten unter deren Marke \"Zemmix\", MSX-basierte Spielkonsolen. MSX1: CPC 50 (Zemmix I), CPC 51 (Zemmix V). MSX2: CPC-61 (Zemmix Super V), CPG-120 (Zemmix Turbo).\n\nAnfangs verwendete MSX als Speichermedium Kassettenrekorder, im Computerbereich auch Datasette genannt. Es gab die BeeCard und einen entsprechenden Kartenleser, 2,8-Zoll- (QUICKDISK-DRIVE) und 5,25-Zoll-Diskettenlaufwerke. Später kamen einseitige (360 kB) und doppelseitige 3,5\"-Disketten (720 kB) hinzu, die in 80 Spuren zu 9 Sektoren mit je 512 Bytes formatiert sind. Doppelseitige Disketten haben das gleiche Datenformat wie MS-DOS (FAT12) und können zum Datenaustausch zwischen den beiden Systemen verwendet werden. Allerdings unterstützen nicht alle MSX-Computer doppelseitige Disketten, da einige Diskettenlaufwerke nur einen Schreib-/Lesekopf haben. Weiterhin kann auf Unterverzeichnisse erst ab MSX-DOS 2 zugegriffen werden. Einseitige Disketten werden unter MS-DOS nicht richtig erkannt, aber viele Emulatoren liefern Hilfsprogramme mit, die einen Zugriff ermöglichen.\n\nFür die MSX-Systeme existieren verschiedene Emulatoren.\n\nEiner davon ist \"openMSX\", der schon in einer sehr frühen Version in einem Emulator-Vergleich erfolgreich war und nach wie vor als sehr gut gilt. Es gibt ihn für verschiedene Plattformen, wie Windows, macOS und Android.\n\nEin weiterer MSX-Emulator für den PC mit dem Betriebssystem Windows ist \"RuMSX\" (Link siehe weiter unten), welcher unter der Leitung eines Österreichers aus Klagenfurt in Zusammenarbeit mit MSX-Fans aus vielen anderen Ländern (zum Beispiel Japan) laufend weiterentwickelt wird. Dieser Emulator kann auch mit Disketten-Images arbeiten, wodurch nicht nur die bremsenden Zugriffe auf das Diskettenlaufwerk entfallen, sondern der Emulator kann auch auf modernen PCs ohne Diskettenlaufwerk arbeiten. Ein Hilfsprogramm zur Erstellung und Verwaltung von Disketten-Images kann auf der RuMSX-Website heruntergeladen werden.\n\nDurch die Vielzahl der verschiedenen MSX-Hersteller ist manchmal die richtige Einstellung etwas schwierig, vor allem wenn nicht bekannt ist, für welche Generation das Spiel programmiert wurde.\n\nDie 2006 in Deutschland erschienene Spielkonsole Wii von Nintendo und der damit verbundene Spiele-Download Service \"Wii-Channel\" bieten einige MSX-Spiele zum Herunterladen für die Konsole an.\n\nSchon seit einiger Zeit ist es möglich, MSX-Spiele auf der Playstation Portable von Sony zu emulieren. Die Spiele sind jetzt schon gut spielbar. Der bekannteste Emulator ist wohl \"fMSX-PSP\". Auch auf dem Nintendo DS ist die MSX-Emulation über fMSX DS oder PenkoDS möglich. Zuvor erschien auch die DOS-Version von fMSX.\n\nSeit 2009 existiert ein MSX-Emulator namens \"BlueMSXWii\" für die Wii-Spiekonsole.\n\n\n\n\n\n\n\n\n\n"}
{"id": "82602", "url": "https://de.wikipedia.org/wiki?curid=82602", "title": "Commodore-264-Serie", "text": "Commodore-264-Serie\n\nDie 264er-Serie war eine Heimcomputer-Linie, die von Commodore als Nachfolger für den C64 gedacht war, aber in der geplanten Form nie verwirklicht wurde. Stattdessen kamen die Computer C16, C116 und Plus/4 auf den Markt. Da diese auf den ursprünglichen Modellen basierten, wurden sie unter dem Namen 264er zusammengefasst.\n\nIn den frühen 1980er Jahren kam es zu einem Preiskampf in der Heimcomputerbranche. Firmen wie Texas Instruments und Timex unterboten den Preis der Commodore PET-Linie. Der C64, der erste Computer, der 64 KiB RAM besaß, aber weniger als 600 US$ kostete, war aufgrund seiner vielen spezialisierten Chips aufwändig in der Herstellung. Commodores Geschäftsführer Jack Tramiel begann daher die Entwicklung einer Computerlinie, die mit viel weniger Chips auskommen und gleichzeitig C64- und VC-20-Besitzer zum Umstieg animieren sollte.\n\nUrsprünglich waren drei Modelle geplant: \"232\", \"264\" und \"364\". Der \"264\" stellte das Basis-Modell dar. Er sollte über 64 KiB RAM und 32 KiB ROM verfügen. Als Besonderheit war geplant, die Computer mit im ROM eingebauter Software auszustatten. Der Kunde sollte dabei aus vier verschiedenen Paketen wählen dürfen. Geblieben sind davon das „3 plus 1“-Paket des Plus/4. Der \"364\" sollte der „Große Bruder“ des \"264\" werden. Neben einer Tastatur mit Ziffernblock und größerem ROM (48 statt 32 KiB), sollte er vor allem über einen eingebauten Sprachsynthesizer mit 250 Wörtern verfügen (weitere nachladbar). Der \"232\" schließlich sollte als Sparversion des \"264\" nur 32 KiB RAM und keinerlei zusätzliche Software besitzen. Von beiden Rechnern wurden einige Prototypen angefertigt.\n\nNach dem Fortgang von Jack Tramiel wurde die Markteinführung der ursprünglichen Modelle verworfen. Stattdessen brachte man 1984 die Modelle \"C16\", \"C116\" und \"Plus/4\" auf den Markt. Sie waren zwar technisch ähnlich, aber ohne die speziellen Features wie Sprachausgabe. Außerdem verfügten die Varianten \"C16\" und \"C116\" nur über 16 KiB RAM. Alle drei Computer nutzten eine MOS-7501-CPU und einen MOS 7360 „TED“. Der „TED“ war ein All-In-One-Chip mit Video-, Sound- und I/O-Fähigkeiten. Das Design der Computer entsprach damit mehr dem VC20 als dem C64; die Chipanzahl und die Komplexität der Platine war allerdings wie geplant weit geringer als bei beiden Vorgängermodellen. \n\nExperimentell war bei der Entwicklung des C16 geplant, diesen bei der Produktion mit einer großen, lediglich einseitig konstruierten „Low-Cost“-Hauptplatine auszustatten, da das vom VC-20 bzw. C64 übernommene große „Brotkasten“-Gehäuse die entsprechende Platzmöglichkeit für eine solche Platine bot und man so die Chance sah, weitere Kosten in der Produktion einsparen zu können. Aufgrund technischer Probleme wurde dieser Plan jedoch kurzfristig wieder verworfen und stattdessen wurde in der Serienproduktion eine nachfolgend konstruierte, wieder ganz gewöhnliche doppelseitige Platine verwendet. Dies war der erste und einzige (gescheiterte) Versuch seitens Commodore, eine einseitige Platine zwecks Kostenersparnis zu entwickeln, und lediglich ein einziges Exemplar, verbaut in einem C16-Prototypen, ist derzeit bekannt bzw. erhalten geblieben.\n\n1984 ging der Trend im Computer-Markt weg von billigen hin zu leistungsfähigeren Computern; auch 16-Bit-Computer waren bereits im Kommen. Darüber hinaus war die gesamte Linie vollkommen inkompatibel zum C64. Commodore hatte das nicht für ein Problem gehalten, war doch auch der C64 vom VC20 komplett verschieden. Man hatte aber übersehen, dass 1984 bereits ein großes Softwareangebot für den C64 existierte und der C64 wesentlich leistungsfähiger als der VC20 war. Dagegen war selbst der Plus/4 dem C64 teilweise unterlegen. Deshalb wurde die gesamte Linie aufgrund der bescheidenen Hardware-Ausstattung, der Inkompatibilität sowie der mangelnden Software ein Flop.\n\nDer TED stellte eine damals beeindruckende Palette von 121 Farben dar. Die Auflösung von 320 × 200 Pixel entsprach dem C64 und war für den Anschluss an einen Fernseher geeignet. Er konnte aber im Gegensatz zum VIC II des C64 keine Sprites darstellen. Die Qualität des Tongenerators entsprach mehr dem VIC des VC-20 als dem SID des C64. Software für den C64 konnte aus diesen und anderen Gründen nur sehr schwer oder gar nicht konvertiert werden. Zudem war der TED berüchtigt dafür, sich durch Überhitzung selbst zu zerstören.\n\nDie Anschlüsse waren inkompatibel zum C64. So wurden für Datasette und Joysticks Mini-DIN-Buchsen verwendet. Die alten Peripheriegeräte konnten mittels Adaptern weiter genutzt werden, aber auf Kassette gespeicherte \"Daten\" konnten aufgrund unterschiedlicher Formate nicht mit dem C64 ausgetauscht werden. Die Belegung des Userports und des Expansionsports unterschied sich ebenfalls. Nur der serielle IEC-Bus (CBM-Bus) war gleich geblieben.\n\nDie Speicherverwaltung ermöglichte eine bessere Ausnutzung des RAMs als beim C64. Das Commodore BASIC 3.5 war stark verbessert und bot Befehle zur Strukturierung (Schleifenbefehle), für Soundausgabe sowie Grafik. Das Diskettenlaufwerk VC1551 war etwa viermal so schnell wie ein VC1541, aber zu diesem nur teilweise kompatibel.\n"}
{"id": "82706", "url": "https://de.wikipedia.org/wiki?curid=82706", "title": "Trillian", "text": "Trillian\n\nTrillian ist ein Instant Messenger des Unternehmens Cerulean Studios für verschiedene Protokolle. Das Programm ist für Windows, macOS, Android und iOS verfügbar. Die Linux-Version befindet sich noch in der Betaphase. Es existiert weiterhin eine clientunabhängige Webversion (Zugang via Webbrowser). Diese ermöglicht es, über verschiedene Protokolle auch mit Personen zu kommunizieren, die andere Instant Messenger oder soziale Netzwerke einsetzen.\n\nMit den aktuell erhältlichen Versionen kann man mit Nutzern der Instant Messaging-Netzwerke Astra, AIM, Google Talk, Facebook, ICQ, XMPP und IRC per Textchat kommunizieren. Im Astra-Netzwerk sind auch Audio- und Videochat sowie in Yahoo Webcam-Übertragungen möglich. Des Weiteren werden die sozialen Netzwerke wie Facebook, inklusive des Facebook Chat, MySpaceIM, Twitter, Foursquare, die VZ Netzwerke und LinkedIn unterstützt. Auch Bonjour kann mit dem mitgelieferten Plug-in verwendet werden. Außerdem existieren weitere Plug-ins für andere Protokolle, wie des Novell-GroupWise Messengers.\n\nBenannt wurde das Programm nach \"Tricia (Marie) McMillan\", genannt \"Trillian\", einer Hauptfigur aus dem Buch \"Per Anhalter durch die Galaxis\" von Douglas Adams. Das Programm ist in C und C++ programmiert.\n\nPlug-ins werden sowohl von Cerulean Studios selbst (wie z. B. eine integrierte Rechtschreibkorrektur (\"Spell Check\"), ein \"Minibrowser\" zum Betrachten von AIM-Profilen im HTML-Format und die meisten Netzwerk-Plug-ins (ICQ, AIM, Skype, WLM, Jabber, Yahoo, Bonjour)) als auch von unabhängigen Programmierern der Community erstellt. Beispiele für Ersteres sind unter anderem das Skype-Plug-in, für Letzteres ein Audio-Player (\"Trilly Tunes\") sowie diverse Plug-ins die von Cerulean Studios bereitgestellte, aber veraltete Plug-ins ersetzen (z. B. Prüfung von POP3- und IMAP-Postfächern auf neue Nachrichten (\"My Mail\"), einen Feedreader (\"Good News\"), einen Übersetzer und diverse Spiele). Auf der Cerulean Studios Website findet sich eine Liste mit vielen Plug-ins, weitere sind unter anderem im Plug-in-Bereich des offiziellen Forums zu finden.\n\nChatverläufe werden als xml- und txt-Dateien gespeichert sowie mit anderen gleichzeitig verbundenen Geräten synchronisiert. Kunden mit einer aktiven Pro-Lizenz können die Chronik auch online speichern und von überall aus abrufen.\n\nFür die Windows-Version sind außerdem verschiedene Skins verfügbar, mit denen sich das Aussehen der Benutzeroberfläche verändern lässt. Die meisten dieser Skins wurden ebenfalls von Mitgliedern der Trillian Community entwickelt. Viele Skins zum kostenfreien Download finden sich auf der offiziellen Website und im Skin-Bereich des Forums.\n\nPlug-ins\n\nWenn beide Gesprächspartner Trillian benutzen, ist es über ICQ und AIM möglich mit SecureIM verschlüsselt zu chatten. Dabei wird eine 128-Bit Blowfish-Verschlüsselung benutzt. Beim Chat über das Astra-Netzwerk wird generell eine verschlüsselte Übertragung verwendet. Eine Verschlüsselung mit Off-the-Record Messaging (OTR) ist nur per Drittanbieter-Plug-in möglich. Dieses Plug-in ist jedoch veraltet und funktioniert mit den aktuellen Versionen nicht mehr korrekt.\n\nAb Version 3.0 ist es in Trillian mit \"Instant Lookup\" (zu Deutsch etwa \"\"sofort nachschlagen\"\") möglich, direkt Wörter aus Chat-Gesprächen in Wikipedia nachzuschlagen. Ein- und ausgehende Texte werden automatisch mit Wikipedia-Einträgen verknüpft. Man erkennt solche Wörter daran, dass sie unterstrichen dargestellt werden. Wenn man dann mit der Maus über den unterstrichenen Text fährt, erscheint der Wikipedia-Text in einem kleinen Fenster. Leider werden nur vom englischen Wikipedia Informationen abgerufen und diese Funktion kann durch eine höhere CPU-Auslastung langsame Systeme ausbremsen.\nAlternativ kann man ein Wort mit der rechten Maustaste markieren und zwischen mehreren Suchmaschinen, wie auch Wikipedia, auswählen, in der das Wort anschließend im Browser gesucht wird.\n\nTrillian Pro unterstützt das Onlinespeichern aller Konversationen, in den kostenlosen Desktop-Versionen werden diese lokal als txt- und xml-Dateien abgelegt und bei Erwerb einer Pro-Lizenz auf Wunsch mit dem Astra-Server synchronisiert.\n\nAb Version 2.0 Pro gibt es einige, darunter auch animierte, Emoticons, die nicht in der normalen Liste sichtbar sind. Nach der Eingabe bestimmter Zeichenkombinationen (z. B. <:) und O:-)) erscheinen die Emoticons im Chatfenster.\n\nDie verschiedenen Trillian-Clients (z. B. Mobil und PC) können gleichzeitig betrieben werden. Über die Trillian-Server werden die Einstellungen (etwa Userliste) und die Nachrichten auf den verschiedenen Plattformen synchronisiert. So kann eine Kommunikation, die auf einem Gerät oder im Web begonnen wurde, auf einem anderen nahtlos fortgesetzt werden. Bei der Mobilversion wird die Verbindung zu den Netzwerken über die Server auch aufrechterhalten, wenn die Mobilfunkverbindung kurzzeitig unterbrochen ist. Über einen Pausen-Modus kann sie sogar mehrere Stunden bei ausgeschalteten Mobilfunkgerät gehalten werden.\n\n"}
{"id": "82893", "url": "https://de.wikipedia.org/wiki?curid=82893", "title": "Windows-1252", "text": "Windows-1252\n\nWindows-1252 auch \"CP 1252\" sowie \"Westeuropäisch (Western European)\" oder \"ANSI\". ist eine 8-Bit-Zeichenkodierung, die für das Betriebssystem Microsoft Windows entwickelt wurde. Der Zeichensatz basiert auf ISO 8859-1, weicht aber im Bereich 80 – 9F von dieser ab, statt der C1-Steuerzeichen beinhalten diese 32 Positionen hier 27 darstellbare Zeichen, u. a. die in ISO 8859-15 hinzugekommenen und einige für bessere Typografie notwendige Zeichen.\nManche Applikationen vermischen die Definition von ISO 8859-1 und Windows-1252. Da beispielsweise in HTML die zusätzlichen Steuerzeichen aus ISO 8859-1 keine Bedeutung haben, werden oft die druckbaren Zeichen aus Windows-1252 verwendet.\nAus diesem Grund schreibt der neue HTML5-Standard vor, dass als ISO 8859-1 markierte Texte als Windows-1252 zu interpretieren sind. Nichtsdestotrotz ist Windows-1252 ebenfalls bei der IANA registriert. Im Januar 2019 verwenden 3,5 % aller Websites die Zeichenkodierung implizit als ISO 8859-1, bei 0,6 % der Websites wird explizit Windows-1252 verwendet, bei jeweils fallender Tendenz. Latin-1 ist damit nach UTF-8 (93,0 %) die zweithäufigste Kodierung von Websites. Die Unterschiede zwischen all diesen Kodierungen sowie generell mangelnde Konsequenz bei der Unterstützung verschiedener Zeichensätze sind ein häufiges Interoperabilitätsproblem.\n\nDa Unicode auf der ISO 8859-1 Codierung aufbaut, sind die Unicode Codepoints der nicht farbig unterlegten Zeichen zu den Codewerten in Windows-1252 identisch. Für die farbig hinterlegten von ISO 8859-1 abweichenden Codepoints ergibt sich folgendes Mapping\n\nWindows-1252 enthält neben den Zeichen aus ISO 8859-1 auch diejenigen Zeichen, welche in ISO 8859-15 hinzugefügt wurden und dort einige weniger oft gebrauchten Zeichen aus ISO 8859-1 ersetzen. Allerdings unterscheidet sich bei diesen Zeichen die Position sowohl zwischen Windows-1252 und ISO 8859-15 als auch zu der Codierung in Unicode. Alle Zeichen die nicht in einer der beiden ISO-Codierungen auftauchen belegen dabei die folgenden Positionen.\n"}
{"id": "83173", "url": "https://de.wikipedia.org/wiki?curid=83173", "title": "Rootkit", "text": "Rootkit\n\nEin Rootkit (englisch etwa: „Administratorenbausatz“; root ist bei unixähnlichen Betriebssystemen der Benutzer mit Administratorrechten) ist eine Sammlung von Softwarewerkzeugen, die nach dem Einbruch in ein Softwaresystem auf dem kompromittierten System installiert wird, um zukünftige Anmeldevorgänge (Logins) des Eindringlings zu verbergen und Prozesse und Dateien zu verstecken.\n\nDer Begriff ist heute nicht mehr allein auf unixbasierte Betriebssysteme beschränkt, da es längst auch Rootkits für andere Systeme gibt. Antivirenprogramme versuchen, die Ursache der Kompromittierung zu entdecken. Zweck eines Rootkits ist es, Schadprogramme („malware“) vor den Antivirenprogrammen und dem Benutzer durch Tarnung zu verbergen.\n\nEine weitere Sammlung von Softwarewerkzeugen oder Bootloadern ist das „Bootkit“.\n\nDie ersten Sammlungen von Unix-Tools zu den oben genannten Zwecken bestanden aus modifizierten Versionen der Programme ps, passwd usw., die dann jede Spur des Angreifers, die sie normalerweise hinterlassen würden, verbergen und es dem Angreifer so ermöglichten, mit den Rechten des Systemadministrators root zu agieren, ohne dass der rechtmäßige Administrator dies bemerken konnte.\n\nEin Rootkit versteckt normalerweise Anmeldevorgänge, Prozesse und Logdateien und enthält oft Software, um Daten von Terminals, Netzwerkverbindungen und Tastaturanschläge und Mausklicks sowie Passwörter vom kompromittierten System abzugreifen. Hinzu können Backdoors (Hintertüren) kommen, die es dem Angreifer zukünftig vereinfachen, auf das kompromittierte System zuzugreifen, indem beispielsweise eine Shell gestartet wird, wenn an einen bestimmten Netzwerkport eine Verbindungsanfrage gestellt wurde. Die Grenze zwischen Rootkits und Trojanischen Pferden ist fließend, wobei ein Trojaner eine andere Vorgehensweise beim Infizieren eines Computersystems besitzt.\n\nDas Merkmal eines Rootkits ist es, dass es sich ohne Wissen des Administrators installiert und dem Angreifer so ermöglicht, die\nComputeranlage unerkannt für seine Zwecke zu nutzen. Dies sind u. a.:\nRootkits können neue Hintertüren („backdoors“) öffnen. Zudem versuchen Rootkits, den Weg ihres Einschleusens zu verschleiern, damit sie nicht von anderen entfernt werden.\n\nApplication-Rootkits bestehen lediglich aus modifizierten Systemprogrammen. Wegen der trivialen Möglichkeiten zur Erkennung dieser Art von Rootkits finden sie heute kaum noch Verwendung.\n\nHeutzutage finden sich fast ausschließlich Rootkits der folgenden drei Typen:\n\nKernel-Rootkits ersetzen Teile des Kernels durch eigenen Code, um sich selbst zu tarnen („stealth“) und dem Angreifer zusätzliche Funktionen zur Verfügung zu stellen („remote access“), die nur im Kontext des Kernels („ring-0“) ausgeführt werden können. Dies geschieht am häufigsten durch Nachladen von Kernel-Modulen. Man nennt diese Klasse von Rootkits daher auch \"LKM-Rootkits\" (LKM steht für engl. „loadable kernel module“). Einige Kernel-Rootkits kommen auch ohne LKM aus, da sie den Kernelspeicher direkt manipulieren. Unter Windows werden Kernel-Rootkits häufig durch die Einbindung neuer .sys-Treiber realisiert.\n\nEin solcher Treiber kann Funktionsaufrufe von Programmen abfangen, die beispielsweise Dateien auflisten oder laufende Prozesse anzeigen. Auf diese Weise versteckt das Rootkit seine eigene Anwesenheit auf einem Computer.\n\n„Userland-Rootkits“ sind vor allem unter Windows populär, da sie keinen Zugriff auf der Kernel-Ebene benötigen. Sie stellen jeweils eine DLL bereit, die sich anhand verschiedener API-Methoden (\"SetWindowsHookEx, ForceLibrary\") direkt in alle Prozesse einklinkt. Ist diese DLL einmal im System geladen, modifiziert sie ausgewählte API-Funktionen und leitet deren Ausführung auf sich selbst um („redirect“). Dadurch gelangt das Rootkit gezielt an Informationen, welche dann gefiltert oder manipuliert werden können.\n\nSpeicher-Rootkits existieren nur im Arbeitsspeicher des laufenden Systems. Nach dem Neustart („reboot“) des Systems sind diese Rootkits nicht mehr vorhanden.\n\nFast alle gängigen Server-, PC- und Laptop-Prozessoren besitzen heute Hardware-Funktionen, um Programmen einen virtuellen Prozessor vorzugaukeln. Dies wird häufig genutzt, um auf einer physikalischen Computeranlage mehrere auch unter Umständen verschiedene Betriebssysteme parallel betreiben zu können.\nVirtual Machine Based Rootkit (VMBR)s sind Rootkits, die ein vorhandenes Betriebssystem in eine virtuelle Umgebung verschieben. Dadurch ist das Betriebssystem in der virtuellen Umgebung gefangen. Die virtuelle Umgebung ist somit eine Software-Ebene unter dem Betriebssystem, was ein Erkennen des VMBR stark erschwert.\n\nMachbarkeitsnachweise für diese Technik lieferten Joanna Rutkowska mit dem Programm Bluepill sowie Microsoft Research mit dem Programm SubVirt. Bluepill kann, im Gegensatz zu SubVirt, ohne Neustart des zu infizierenden Computers installiert werden. Der Name Bluepill (englisch für „blaue Pille“) ist eine Analogie zum Film Matrix.\n\n\nDa eine hundertprozentige Erkennung von Rootkits unmöglich ist, ist die beste Methode zur Entfernung die vollständige Neuinstallation des Betriebssystems. Da sich bestimmte Rootkits im BIOS verstecken, bietet selbst diese Methode keine hundertprozentige Sicherheit über die Entfernung des Rootkits. Um eine Infizierung des BIOS im Voraus zu verhindern, sollte das BIOS hardwareseitig mit einem Schreibschutz versehen werden, z. B. durch einen Jumper auf der Hauptplatine.\n\nJedoch gibt es für viele Rootkits von offiziellen Herstellern bereits Programme zur Erkennung und Entfernung, z. B. das Sony Rootkit.\n\n\n"}
{"id": "84993", "url": "https://de.wikipedia.org/wiki?curid=84993", "title": "Kernel-Modul", "text": "Kernel-Modul\n\nEin Kernel-Modul (kurz „LKM“ für englisch: „“) ist ein spezielles Computerprogramm, das im laufenden Betrieb in den Kernel eines Betriebssystems geladen und wieder daraus entfernt werden kann.\n\nKernel-Module werden häufig für Gerätetreiber verwendet, da eine große Auswahl der Module für die unterschiedlichsten Hardware-Komponenten mit dem Betriebssystem mitgeliefert werden können, aber nur die wirklich benötigten Treiber in den Speicher geladen werden müssen.\n\nKernel-Module gibt es bei unterschiedlichen Betriebssystemen, wie zum Beispiel Linux, BSD oder Solaris.\n\nKernel-Module werden üblicherweise in der Programmiersprache C geschrieben und vor ihrem Laden in den Kernel und der späteren Ausführung dort, in die Maschinensprache für die jeweilige Ziel-Plattform übersetzt (kompiliert).\n\nDas Verfahren des dynamischen Hinzufügens von Kernel-Modulen wird z. B. beim Linux-Kernel dazu verwendet, um einen Standardkernel an die Hardware, auf der er ausgeführt wird, dynamisch anzupassen. So kann zum Beispiel der Treiber einer vorgefundenen Soundkarte zur Laufzeit des Systemkerns geladen werden, während die vorliegenden Treiber für nicht vorhandene Soundkarten ignoriert werden können und somit auch keinen Hauptspeicher belegen.\n\nEin weiterer Vorteil liegt darin, dass Erweiterungen für den Kernel integriert werden können, ohne dass das Betriebssystem neu gestartet werden muss. Denkbar wäre, dass man auf diese Weise den als Kernel-Modul realisierten Treiber einer Soundkarte entfernt und eine neuere Version dieses Treibers in das laufende System einbindet.\n"}
{"id": "85309", "url": "https://de.wikipedia.org/wiki?curid=85309", "title": "Mesa 3D", "text": "Mesa 3D\n\nMesa 3D ist eine freie Grafikbibliothek, die die OpenGL-Spezifikation umsetzt und auf vielen Betriebssystemen wie Linux, AmigaOS3, AmigaOS4, SkyOS, Haiku, ZETA und BSD genutzt wird, um OpenGL-Funktionalität zu implementieren. Hardwarebeschleunigte 3D-Grafik ist durch die Kombination mit der Direct Rendering Infrastructure möglich. Die Quellen der Mesa-Bibliothek stehen unter der MIT-Lizenz.\n\nIm August 1993 begann Brian Paul mit der Entwicklung einer Grafikbibliothek, die zu der damals neuen OpenGL-Programmierschnittstelle kompatibel sein sollte. Im November 1994 erhielt er die Genehmigung von SGI, Mesa zu veröffentlichen, und im Februar 1995 erschien Mesa 1.0. OpenGL war bis zu diesem Zeitpunkt noch nicht weitreichend verfügbar, und viele Entwickler kamen durch Mesa mit OpenGL in Kontakt. Im Oktober 1996 erschien Mesa 2.0, welches OpenGL 1.1 unterstützte. Die Version 2.2, die im März 1998 erschien, unterstützt die Hardwarebeschleunigung via 3dfx-Glide.\n\nDie am 22. Juni 2007 veröffentlichte Version 7.0 unterstützt erstmals OpenGL 2.1.\n\nMit der Veröffentlichung der Version 8.0 am 9. Februar 2012 wird die OpenGL-3.0-Spezifikation unterstützt. Mit der Version 9.0 am 8. Oktober 2012 kam die Unterstützung für OpenGL 3.1 hinzu. Ab der Version 10.0 aus dem Jahr 2013 wird OpenGL 3.3 unterstützt.\n\nMesa 11.x enthält alle Erweiterungen bis OpenGL 4.1 vollständig sowie den Großteil von OpenGL 4.2 bis 4.5.\n\nSeit Mesa 12 implementieren die Treiber für neuere AMD-, Nvidia und Intel-Grafikchips OpenGL 4.3 vollständig. Bis auf eine Erweiterung aus OpenGL 4.4 unterstützt der Intel-Treiber sogar bereits alles für OpenGL 4.5. Zusätzlich enthält Mesa 12 einen Vulkan-Treiber für Intel-Chips ab der Ivy-Bridge-Generation.\n\nDazu unterstützt offiziell ab Mesa 12 ein neuer Software-Treiber für CPU-Cluster mit Namen „OpenSWR“ (Open Software Rasterizer) OpenGL 3.1+. OpenSWR setzt auf LLVMpipe auf und skaliert dabei mit der Rechenleistung sehr gut und ist für große Datensätze gedacht wie sie in numerischen Simulationen vorkommen. Im Vergleich zu LLVMpipe wurden schon Beschleunigungen von Faktor 29 bis 51 erreicht mit Mesa 10.5.1 im Alpha-2-Status im Jahr 2015. In Mesa wird er mit „GALLIUM_DRIVER=swr“ eingeschaltet.\n\nMit Mesa 13 wird OpenGL 4.4 und 4.5 eigentlich voll unterstützt. Intel ab Broadwell, AMD GCN und Nvidia Fermi und Kepler (Maxwell nur 4.1) unterstützen alle Features, werden jedoch teilweise wegen fehlender Zertifizierung noch mit 4.3 oder 4.4 (13.0.1) angezeigt.\n\nIn Version 17.0 (neue Zählung) gibt es einige Verbesserungen (siehe Mesamatrix).\nFür Intel steht bei Haswell OpenGL 4.5 (bisher 3.3, mit nur 3 fehlenden Modulen in 4.0 und 4.1) und fast komplett OpenGL ES 3.2 zur Verfügung.\nDie Zertifizierung von OpenGL 4.4 für NVIDIA Fermi und Kepler, sowie 4.5 für AMD GCN steht nun auch hier zur Verfügung, nachdem dies in 13.0.x noch nicht geklappt hat. OpenGL 4.3 steht für NVIDIA Maxwell und Pascal (GM107+) am Start.\nJedoch nur Maxwell 1 (GeForce GTX 750 und andere mit GM1xx) profitiert von starken Gewinnen durch Erhöhung der Taktfrequenzen unter Last. Weiterhin leidet Maxwell 2 (GeForce 980 und andere mit GM2xx) unter dieser fehlenden Regelung so stark, dass nur 1/3 bis 1/2 der möglichen Leistung mit Mesa verfügbar ist.\n\nKhronos stellt seine CTS (test Suite) für OpenGL 4.4, 4.5 und OpenGL ES 3.0+ nun Open Source. Damit sind Tests nun kostenlos möglich auch für alle nicht kommerziellen Entwickler. Für Mesa 13 und 17 sind damit für einige Module wie \"Nouveau\" höhere OpenGL-Stufen möglich nach bestandenen Tests.\n\nVersion 17.1.0 ist seit 10. Mai 2017 verfügbar mit einigen Verbesserungen wie OpenGL 4.2 für Intel Ivybridge (für Gen 7 bisher 3.3) und weiterer Beschleunigung für Nvidia Maxwell und Pascal, sowie für Vulkan mit verbessertem RADV und ANV.\n\nIn Version 17.2 werden Intel und AMD zum Teil stark in 3D-Spielen beschleunigt. Dazu werden einige OpenGL 4.6-Module unterstützt. Für NVIDIA ist weiterhin \"Nouveau\" nur 2. Wahl mangels Informationen zur Beschleunigung, wie sie den originalen Treibern zur Verfügung stehen. Die Tests zu OpenGL 4.5 waren für Kepler zu 98,6 % erfolgreich.\n\nIn Version 17.3 wurden einige Module für OpenGL 4.6 fertig, so dass in der nächsten Version mit Spir-V-Unterstützung 4.6 komplett würde. Deutliche Fortschritte beim Vulkan-Treiber für Radeon-Grafikkarten und endlich Support für eine wichtige Kompressionstechnik sind Highlights. Darüber hinaus gibt es viele Optimierungen bei den OpenGL-Treibern für die GPUs von AMD und Intel. RADV als Vulkan-Treiber für AMD erfüllt nun voll den Khronos-Test.\n\nIn Version 18.0 wurden weitere Features für OpenGL fertig gestellt, um OpenGL 4.6 zu komplettieren. Es fehlt weiterhin Spir-V. Vulkan 1.0 wurde weiter ausgebaut und optimiert auf eine neue Stufe. Nvidia Nouveau unterstützt offiziell nur OpenGL 4.3 auch wegen einiger Fehler in Mesa, die zu Testfehlern in OpenGL 4.4 und 4.5 in der CTS führen.\nDie Intel und AMD-Treiber wie RadeonSI unterstützen OpenGL 4.5 mit den gleichen Fehlern. Der AMD R600g-Treiber für ältere AMD Terascale-Karten ist auf dem Weg OpenGL 4.5 zu unterstützen und übertrifft damit AMD mit maximal OpenGL 4.4 mit seinen offiziellen Treibern. Dabei bleiben R600 und Evergreen auf maximal OpenGL 4.3 limitiert wegen fehlender FP64-Einheiten.\nIntel Cannonlake wird nun auch unterstützt und damit auch die neueste Hardware von Intel.\nDazu wurde das neue Build-System Meson als neuer Standard freigeschaltet.\n\nIn Version 18.1 steht Vulkan 1.1 für Intel ab Skylake (ANV) und AMD GCN (RADV) zur Verfügung. Dabei muss natürlich die Hardware alle Module unterstützen. Für AMD GCN der 1. und 2. Generation (GCN 1.0 und 1.1) wurde dies bei AMD in den eigenen Windowstreibern und Conformance Tests bei Khronos verneint. Intel stellt ab Skylake Vulkan 1.1 zur Verfügung. AMD RX 550 und RX Vega 64 sind dort schon mit RADV für Vulkan 1.1 registriert.\n\nIn Version 18.2 stehen viele Verbesserungen zur Verfügung. OpenGL 4.6 vollständig zu unterstützen wurde noch nicht erreicht. Der neue Soft-Treiber VIRGL unterstützt nun OpenGL 4.3 und OpenGL ES 3.2. RadeonSI unterstützt nun ebenfalls OpenGL ES 3.2 und im Kompatibilitätsmodus nun OpenGL 4.4 nach 3.1 in Version 18.1. Der ASTC texture compression Support für RadeonSI wird als Vorbild in Zukunft in anderen Treibern dienen für dieses Feature. Dazu kommt für die neue Vega-20-Baureihe Support im RadeonSI-Treiber. Vulkan 1.1-Funktionen und weitere Funktionen werden im Intel ANV und AMD RADV unterstützt.\n\nIn Version 18.3 im Dezember 2018 stehen viele Verbesserungen zur Verfügung. AMD Raven 2, Picasso, Vega 20 support für RadeonSI und RADV ist nur ein Punkt von vielen. Intel Whiskey Lake und Amber Lake werden nun ebenfalls unterstützt. OpenGL 4.6 wurde mangels SPIR-V noch nicht erreicht.\n\nNach dem bisherigen Schema ist Version 19.0 für März 2019 vorgesehen.\n\nDie Zweige der jeweiligen Treiber unterstützen diese Stufen oft erst in späteren Versionen. Deshalb sollte möglichst eine aktuelle Version passend zum System und der Hardware installiert sein, um Limitierungen und Fehlern aus dem Weg zu gehen und um die Leistung zu maximieren. Beim Update von Mesa 12 auf 13 konnte die Leistung um bis zu 70 % gesteigert werden bei AMD Radeon RX 480 mit Ubuntu 16.10 und Linux 4.8.\n\nAktuelle Grafikkarten sind oft durch fehlende Informationen wie zum Beispiel zum Powermanagement in ihrer Leistung limitiert. Später stehen diese oft erst Jahre nach Verkaufsende der Baureihe zur Verfügung. So ist eine NVIDIA GTX 680 (Kepler) bis zu 10-mal schneller in Mesa 12.0 als neuere Karten der eigentlich schnelleren Maxwell-Architektur. Mit den Versionen 13 und 17 wurde Maxwell 1 stark beschleunigt. Nun sind Maxwell 2 und auch Pascal gering in ihrer Leistung gegenüber dem Nvidia-Treiber und den Vorgängern wegen fehlender Daten.\n\nMesa unterstützt ab Version 8 OpenGL ES 1.1 und 2.0 aufsetzend auf EGL 1.4. Ab Mesa 11 sind EGL 1.5 und OpenGL ES 3.0 möglich.\nMesa 12 unterstützt hier voll Version ES 3.0 und 3.1. Version 3.2 steht in Mesa 13.0 erstmals komplett zur Verfügung (siehe mesamatrix). Dazu werden optionale zusätzliche Funktionen für die einzelnen Stufen entwickelt aus dem Khronos-Baukasten neuer vorgeschlagener Module.\nIn Android und anderen Systemen sind EGL und OpenGL ES wichtige Bestandteile der Grafiksoftware.\n\nDas Testen mit Bugfixing und das Building wurden in den letzten Jahren stark verbessert. Der aktuelle Status der Intel-Entwicklung ist nun öffentlich einsehbar. Mit Iris ist ein Nachfolger von i965 in Arbeit für Hardware Skylake und neuer. Mit Spir-V wird das Mesa-Projekt für OpenGL stark modernisiert. Da starke Änderungen nötig sind, sind noch einige Fehler zu bereinigen.\n\nEine gemeinsame OpenCL-Plattform für alle wichtigen Plattformen mit GalliumCompute zu etablieren ist bisher gescheitert.\nDie Entwicklung ist hier langsam gegenüber anderen Modulen in Mesa (früher Clover: OpenCL 1.0 und 1.1 nahezu vollständig (CTS unvollständig), 1.2 teilweise für AMD GPU und unzureichend für NVIDIA GPU). Es findet zumindest die Wartung statt, damit der bisherige Code weiterhin läuft. 2017 kamen mit den Funktionen FP16 und INT64 atomics wichtige Teile für OpenCL 1.2 hinzu.\nMit einer Bridge kann nun ein AMDGPU-OpenCL2-Compiler mit Mesa 3D gekoppelt werden und erheblich mehr Programme laufen nun fehlerfrei. Neue Entwicklungen von Redhat für NIR und Spir-V ermöglicht auch Fortschritt für Clover.\nAls Nachfolger des hinkenden GalliumCompute von Mesa hat AMD ROCm (aktuell Version 2.2) begonnen. OpenCL 2.0 Syntax und OpenCL 1.2 Runtime sind ROCm-Basis für aktuelle Hardware der CPU von Intel und AMD mit PCIe 3.0 und GPU mit AMD GCN ab der 3. Generation mit Hawaii, Fiji, Polaris und Vega. Später soll dieses SDK dann als Open Source nutzbar sein. Ein Überblick des aktuellen Status von ROCm wurde präsentiert auf der XDC2018. Im Vergleich von aktueller AMD Hardware mit ROCm zu Nvidia mit neuer Turing and älteren Pascal Karten ist Nvidia in vielen Tests nun auch bei OpenCL im Vorteil. Ab Version 2.0 wird nun OpenCL 2.0 voll unterstützt.\n\nIntel China hat mit Beignet (aktuell Version 1.3.2) eine weitere Software für OpenCL (1.2, 2.0 ab Version 1.3.0 mit Skylake+) für Intel CPU mit eingebauter GPU ab Ivy Bridge abseits von Mesa implementiert. Unter dem Codenamen NEO wurde ein neuer Open-Source-Treiber für Hardware ab Gen. 8 mit Broadwell bis aktuell Cannonlake entwickelt mit aktuell OpenCL 2.1-Unterstützung. OpenCL 2.2 soll zeitnah folgen. Mit dieser neuen Compute Runtime NEO wird Beignet nicht mehr weiterentwickelt.\nFür NVIDIA steht nur der Closed-Source-Treiber mit OpenCL 1.1 für Tesla und Fermi, sowie OpenCL 1.2 für Kepler, Maxwell und Pascal zur Verfügung. OpenCL 2.0 ist hier in der Entwicklung.\nDas Clover final bis 1.2 entwickelt werden kann, zeigt das Projekt Shamrock als Port von Mesa Clover für ARM mit vollem Support von OpenCL 1.2 seit Juli 2016.\n\nIn Zukunft soll OpenCL als OpenCL-V ein Teil von Vulkan werden laut Khronos. Damit würden die Treiber von Vulkan ANV für Intel und RADV für AMD auch OpenCL ausführen. Weil Vulkan eine Weiterentwicklung von AMD Mantle und dieses ein OpenCL mit Graphik-Schicht ist, stehen die Chancen einer erfolgreichen Weiterentwicklung mit ähnlicher Performance wie dem jetzigen OpenCL gut.\n2017 waren die Fortschritte in Clover in Richtung OpenCL 1.2 und anderen Open-Source-Projekten in Richtung OpenCL 2.x gering.\n\nDas Mesa 3D-Projekt beinhaltet und pflegt auch Implementierungen von diversen Programmierschnittstellen für die Hardware-beschleunigte Rasterung:\n\n\nDas ebenfalls unter einer freien Lizenz entwickelte Wine-Projekt beinhaltet eine Implementierung von Microsofts Direct3D Version 9c. Diese kann entweder unter Zuhilfenahme einer Übersetzung von Direct3D-Calls auf OpenGL-Calls genutzt werden, oder aber seit der Veröffentlichung des Gallium3D-State-Trackers auch für Direct3D 9c direkt.\n\n\n"}
{"id": "85504", "url": "https://de.wikipedia.org/wiki?curid=85504", "title": "Geograf (CAD)", "text": "Geograf (CAD)\n\nGEOgraf ist ein CAD-Programm von HHK Datentechnik, das seit 1986 vor allem für die Vermessungstechnik entwickelt wird. Deshalb beschränken sich alle bisher entwickelten Module/Erweiterungen auf den Bereich Vermessung. Mit GEOgraf werden auch automatisierte Liegenschaftskartendaten und der Aufbau von GIS-Datenbeständen ermöglicht. Dazu stehen diverse Symbolbibliotheken zur Verfügung.\n\nSeit der Zusammenarbeit mit der Firma AKG Software Consulting GmbH gibt es für GEOgraf VESTRA-Werkzeuge. Somit ist das Programm auch für die Planung von kommunalen und klassifizierten Straßen geeignet. Als Schnittstelle für den Datenim- und export ist das OKSTRA-Format implementiert. GEOgraf wird auch von Auszubildenden zum Vermessungstechniker in der Berufsschule verwendet. Rechenprogramme wie KIVID oder KAVDI werden häufig zusammen mit GEOgraf benutzt.\n\nGEOgraf läuft auf PCs mit Microsoft Windows 2000, Windows XP, Windows Vista und Windows 7. Bei der Verwendung von Windows 9x kommt es gegebenenfalls zu Einschränkungen.\n\n"}
{"id": "85908", "url": "https://de.wikipedia.org/wiki?curid=85908", "title": "VariCAD", "text": "VariCAD\n\nVariCAD ist ein 2D/3D-CAD-Programm, das vorwiegend für den Maschinenbau konzipiert wurde. Mit dem Programm können zweidimensionale Zeichnungen und dreidimensionale Volumenmodelle, so genannte Solids, erstellt werden.\nVariCAD unterstützt Parameter, geometrische Zwangsbedingungen und Baugruppen-Konstruktionen, sowie Attribute mit automatischer Stücklisten-Generierung. Das System bietet darüber hinaus Konstruktions-Werkzeuge, wie z. B. für Schalenkörper, Rohrleitungen, Blech-Abwicklungen, Kollisionsanalysen usw.\n\nZum Programm gehört ein Normteilekatalog mit Schrauben, Muttern, Wälzlagern und vielem mehr. Dazu bietet es integrierte Berechnungsmodule zum Beispiel für Federn, Biegung und Torsion von Trägern, zudem Volumen, Masse und Schwerpunktsberechnungen.\n\nMit VariCAD können 2D-Daten von Fremdsystemen in den Formaten DXF und DWG-Format eingelesen und gespeichert werden, IGES-Daten können im 3D-Format exportiert werden. Die Unterstützung des Industrie-Standardformates für den Produkt-Datenaustausch STEP ermöglicht den Datenaustausch mit allen gängigen 3D-CAD Systemen. Die Ausgabe der 3D-Daten im STL-Format ermöglicht die direkte Erstellung von physischen Prototypen mit der Stereolithografie Technologie.\n\nVariCAD ist verfügbar für die Betriebssysteme Windows und Linux, unterstützt Unicode und daher zusätzlich auch Sprachen wie Japanisch, Russisch oder Chinesisch.\n\nMit dem kostenlosen und freien VariCAD Viewer existiert ein proprietärer CAD-Viewer, der auf demselben Geometrie-Kern wie VariCAD aufsetzt und daher neben dem VariCAD-eigenen 2D- und 3D-Format, die 2D-Formate DXF, DWG, sowie das 3D-Format \"STEP\" darstellen kann. Neben der grafischen Darstellung dieser Formate können mit dem Viewer Geometrien analysiert und abgemessen werden, 3D Elemente als hochauflösende Grafikdatei ausgegeben, 2D-Daten ausgedruckt und geplottet und in allen von VariCAD unterstützten Formaten gespeichert und damit konvertiert werden.\n\n"}
{"id": "85992", "url": "https://de.wikipedia.org/wiki?curid=85992", "title": "Roland TR-808", "text": "Roland TR-808\n\nDie TR-808 oder genauer: der „TR-808 Rhythm Composer“ ist eine analoge Drum Machine.\n\nDie TR-808 wurde 1981 von der Firma Roland auf den Markt gebracht und ist einer der beliebtesten analogen Drum-Synthesizer aller Zeiten. Ihr Klang, der echte Percussion-Instrumente imitieren sollte, ist elektrisch und eher weich. Er war u. a. prägend für House, den frühen Hip-Hop und Electro, wurde aber auch in der Popmusik vielfach eingesetzt. Einer der größten Hits, in denen die 808 exponiert eingesetzt wird, ist Marvin Gayes „Sexual Healing“. Darüber hinaus kann man sie auf vielen Phil-Collins- und Genesis-Aufnahmen hören. Auf Whitney Houstons „I Wanna Dance with Somebody (Who Loves Me)“ ist die Cowbell zu hören; der Song beginnt mit einem mehrfach wiederholten kurzen Pattern der 808. In Britney Spears’ Song Break the Ice wird mittels eines kurzen Breaks das Herzklopfen mit der Bassdrum der 808 verglichen. Die 808 genießt in der Geschichte der elektronischen Musik einen kaum vergleichbaren Kultstatus.\n\nDie TR-808 bietet folgende Klänge: Bassdrum, Snare, Low/Mid/Hi Toms, bzw. Congas, Rimshot, Claves, Hand Clap (Geräusch von klatschenden Händen) oder Maracas, Cowbell, Cymbal, Open und Closed Hi-Hat. Außerdem verfügt sie über einen ausgereiften Step-Sequenzer mit programmierbarem Accent (Anschlag). Die Klangerzeugung ist analog. Die Bassdrum kann mittels eines Filters (Tone) und in der Länge der Hüllkurve (Decay) verändert werden. Es handelt sich um wenig mehr als einen \"Bridget-T-network\"-Oszillator, einen Low-Filter und einen VCA. Der Output arbeitet kontinuierlich, wodurch die Überlagerung des gleichen Signals nach 32 Steps in einem mittleren Tempo möglich wird. Beim Attack handelt es sich um einen getriggerten CV-Pulsgenerator, der dem Audiosignal beigemischt wird. Die typische Abflachung des Decays direkt nach dem Attack ist dem Audiodesign des Oszillators zu verdanken und wirkt wie eine Kompression. Die Snare Drum besteht aus einem stimmbaren Grundton (Tone) und einem regelbaren Rauschanteil (Snappy). Die Low, Mid und Hi Toms bzw. Congas lassen sich jeweils in der Tonhöhe stimmen (Tuning). Das „Becken“ (Cymbal) kann spektral (Tone) und in der Länge (Decay) angepasst werden. Für die Hi-Hats wird die Summe sechs stark gegeneinander verstimmter Oszillatoren über einen Hochpass gefiltert. Die Open Hi-Hat lässt sich in der Länge (Decay) einstellen.\n\nFür die Instrumente stehen 13 Ausgänge zur Verfügung. An zwei als \"Master Out\" bezeichneten Ausgängen liegt die über die Level-Regler abmischbare Summe, einmal mit hohem, einmal mit niedrigem Pegel. Vom Master Out lassen sich jeweils die einzelnen Instrumente über ihre Buchsen abgreifen, d. h. ein Stecker in der Buchse nimmt das Instrument aus der Summe. Bassdrum, Snare, Cowbell, Cymbal, Closed und Open Hi-Hat haben jeweils einen eigenen Ausgang. Low, Mid und High Tom/Conga, Clap und Maracas sowie Rimshot und Claves liegen nur alternativ, d. h. über Schalter auf der Oberseite, an.\n\nEine Synchronisation ist über die von der Firma Roland entwickelte SYNC-Buchse in beide Richtungen möglich. Zum Betrieb in einer MIDI-Umgebung ist ein externer Midi-to-Sync-Konverter oder der Einbau einer MIDI-Schnittstelle nötig.\n\nCowbell, Clap und Accent stehen zusätzlich als programmierbare Trigger zur Verfügung. Damit lassen sich analoge Synthesizer, die nach Roland-Norm (also mit CV/Gate) arbeiten, aber auch Arpeggiatoren oder Sequenzer ansteuern.\n\nObwohl heute „gesampelte“ gute TR-808-Klänge leicht erhältlich sind, ist die originale TR-808 noch sehr gefragt und die Preise für Gebrauchtgeräte sind entsprechend hoch. Aufsehenerregend war die von der Firma Propellerhead 1996 auf den Markt gebrachte Software-Emulation „ReBirth RB-338“, bei der zwei Roland TB-303 mit einer 808 kombiniert wurden. Als günstigen Hardware-Nachbau gibt es die Novation Drumstation, die Sounds der beiden Klassiker TR-808/909 bietet, allerdings per vollständig digitaler Klangerzeugung, ohne Stepsequenzer und mit einem eher mäßigen Timing.\nDie Firma Acidlab bietet mit der „Miami“ einen Nachbau mit vollanaloger Klangerzeugung. Weitere Nachbau-Projekte mit Sequenzer und analoger Klangerzeuger sind die „MB-808“ von ucapps und der „Yocto“ von e-licktronic. Beide Projekte richten sich jedoch ausschließlich an die DIY-Gemeinde und werden nicht kommerziell als Fertiggerät an Endkunden vertrieben. Kenner beschreiben den Klang des „Yocto“ als authentischer als jenen der „Miami“, welche jedoch hinsichtlich der klanglichen Authentizität ebenfalls überwiegend positiv bewertet wird.\n2014 brachte die Firma Roland die TR-8 auf den Markt, eine Kombination aus 808 und 909 und mit der Klangerweiterung 7x7 noch zusätzlich 606, 707 und 727. Die ACB-Klangerzeugung (Analog Circuit Behaviour) ist im Unterschied zu den Originalmaschinen digital und emuliert die originalen Schaltungen der alten Geräte. 2018 kam die TR-8S auf den Markt, die neben zahlreichen Verbesserungen für die Performance nun auch noch um Sampling (nur Wiedergabe) erweitert wurde.\n\n1983 kam mit der TR-909 der Nachfolger der TR-808 auf den Markt, der einen wesentlich härteren und raueren Klang aufwies, wohingegen die Sounds der 808 ein überaus hohes Frequenzspektrum bieten, die Bassdrum entsprechend tiefgelegt werden kann und die Hi-Hats gewöhnlich sehr hoch liegen, zeichnet sich die 909 durch ihr sehr mittiges Sounddesign aus.\n\n\n"}
{"id": "85994", "url": "https://de.wikipedia.org/wiki?curid=85994", "title": "Roland TR-909", "text": "Roland TR-909\n\nDer Roland TR-909 ist ein sowohl auf analoger Synthese als auch auf Samples basierter Drumcomputer.\n\nDer TR-909 wurde 1983 von der Firma Roland auf den Markt gebracht. Er ist der Nachfolger der populären Roland TR-808 und mit ihm einer der beliebtesten analogen Drumsynthesizer aller Zeiten. Als er auf den Markt kam, hatten die ersten rein auf Sampling basierenden Drum Machines wie die 1982 erschienene Linn LM-1 von Linn Electronics begonnen, die analogen Varianten zu verdrängen. Der TR-909 vereinigte beide Techniken, indem er zum einen analoge Technik einsetzte, für HiHat und Cymbal aber 6-Bit Samples verwendete. Folgende Klänge standen zur Verfügung:\n\nNeu gegenüber dem TR-808 war auch die MIDI-Schnittstelle. Daten konnte man entweder auf Compact Cassette oder Flash-Steckmodulen speichern.\n\nDie Klänge des TR-909 sind sehr häufig zu hören, insbesondere in der House- und Techno-Musik. Seine Bass Drum und Hi-Hat sind bei der Produktion von Techno-Musik geläufige Klänge. Vor allem in den Anfangszeiten dieser Musikrichtung wurden sie in überdurchschnittlich vielen Produktionen eingesetzt und repräsentierten damit nahezu schon einen klanglichen Standard, der prägend auf das Genre wirkte. Die sehr durchschlagende Bass Drum ist beispielsweise auf \"Pump Up The Jam\" von Technotronic zu hören.\n\nOriginale TR-909 sind begehrte Instrumente, obwohl die Klänge inzwischen in Form von Samples und Softwaresynthesizern wie dem ReBirth RB-338 digital reproduzierbar sind.\n\nIm Herbst 2016 brachte Roland unter dem Namen TR-09 eine digitale Neukreation der TR-909 heraus. Die TR-09 lehnt sich in Funktion und Aussehen an das Original an, und ahmt mit digitaler Technik die analogen Sounds der TR-909 nach.\n\n"}
{"id": "86326", "url": "https://de.wikipedia.org/wiki?curid=86326", "title": "Software-Agent", "text": "Software-Agent\n\nAls Software-Agent (auch Agent oder Softbot) bezeichnet man ein Computerprogramm, das zu gewissem (wohl spezifiziertem) eigenständigem und eigendynamischem (autonomem) Verhalten fähig ist. Das bedeutet, dass abhängig von verschiedenen Zuständen (Status) ein bestimmter Verarbeitungsvorgang abläuft, ohne dass von außen ein weiteres Startsignal gegeben wird oder während des Vorgangs ein äußerer Steuerungseingriff erfolgt.\n\nLaut Michael Wooldridge gibt es keine allgemein anerkannte Definition eines Agenten. Es gibt zwar generelle Zustimmung, dass ein Agent selbstständig (autonom) sein muss, darüber hinaus gibt es aber wenig Einigung. Wooldridge versucht sich dennoch an einer Definition: „Ein Agent ist ein Computersystem, das sich in einer bestimmten Umgebung befindet und welches fähig ist, eigenständige Aktionen in dieser Umgebung durchzuführen, um seine (vorgegebenen) Ziele zu erreichen.“\nDie VDI-Richtlinie: VDI/VDE 2653 gibt folgende Definition: „Ein technischer Agent ist eine abgrenzbare (Hardware- oder/und Software-) Einheit mit definierten Zielen. Ein technischer Agent ist bestrebt, diese Ziele durch selbstständiges Verhalten zu erreichen und interagiert dabei mit seiner Umgebung und anderen Agenten.“\n\nDie Forschung über Künstliche Intelligenz definiert eine Software als Agenten, wenn sie folgende Eigenschaften besitzt, die den Grad der Autonomie des Programms beschreiben:\n\nautonom\nkognitiv\nkommunikativ\nmodal adaptiv\naktiv\nreaktiv\nrobust\nsozial\n\nDabei sind gleichzeitige Eingriffe von außen, die die Autonomie einschränken oder die Entscheidungskriterien verändern, nicht ausgeschlossen.\n\nKommt zu den genannten Eigenschaften die Fähigkeit hinzu, selbsttätig den Ausführungsort zu wechseln (zu \"migrieren\"), so spricht man von einem \"mobilen Agenten\". Dazu braucht er Fähigkeiten, die ihn zu einer gewissen Anpassung an andere Infrastruktur befähigen. Siehe hierzu Migration (Informationstechnik), das solche Mechanismen seitens eines Menschen beschreibt.\n\"Intelligente Agenten\" zeichnen sich durch Wissen, Lernfähigkeit, Schlussfolgerungen und die Möglichkeit zu Verhaltensänderungen aus.\n\nEin Netz aus einer Teilmenge von autonomen Agenten, die miteinander kommunizieren können, nennt man eine \"Population\". Diese Kommunikation wird durch die Dichte und die Verteilung der Agenten sowie deren Gruppierung und die zeitliche Varianz dieser Parameter beeinflusst.\n\nAgenten werden große Einsatzmöglichkeiten in den Bereichen E-Commerce, Informationsrecherche, Simulation, Erledigen von Routineaufgaben und in autonomen Systemen eingeräumt. Aber auch komplexe Aufgaben, beispielsweise in automatisierten Verhandlungen, können durch Softwareagenten übernommen werden Im Bereich Simulation gibt es dabei das Spezialgebiet der Multi-Agenten-Simulation bzw. Gruppensimulation mit eigenen Softwareprodukten. Letzteres wird gerne im Spielebereich eingesetzt.\nEs gibt zahlreiche Implementierungen von Agentenplattformen im wissenschaftlichen Umfeld. Diese haben meist einen speziellen Fokus, zum Beispiel intelligentes Verhalten, Sicherheit, effiziente Migration.\n\nEine ausführliche Übersicht über aktuelle Systeme, welches als Projekt (\"Co-ordination Action\") im Rahmen des sechsten Forschungsrahmenprogramms der Europäischen Kommission gefördert wird, ist AgentLink.org.\n\nUnter anderem existiert das umfangreiche, Java-basierte Agentenframework JADE.\n\nAgententypen unterscheiden sich in der Agentenarchitektur (nicht zu verwechseln mit der Architektur, auf der das Umgebungsprogramm läuft).\nUnter einer Agentenarchitektur versteht man die Art und Weise, wie die Definition\nund Verwaltung des Agentenverhaltens erfolgt. Prinzipiell herrscht dabei eine große Begriffsvielfalt, aber die Einteilung in drei weitgehend anerkannte Bereiche ist möglich:\n\n\"Reaktive\" (bzw. \"subkognitive\") \"Agenten\" verfügen prinzipiell nicht über eigenes Wissen, sondern agieren nur aufgrund ihrer Wahrnehmungen direkt und ohne Entscheidungsprozess.\n\nFolgende Agententypen treten in diesem Zusammenhang öfter auf:\n\nEinfach Reaktiver Agent\nBeobachtender Agent\n\n\"Adaptive Agenten\" verwalten ein Modell der eigenen Prozess- und Parameterstruktur. Diese können der eigenen Vorgeschichte und erkannten oder gemessenen äußeren Bedingungen angepasst werden. Dadurch wird eine adaptive Regelung und damit beispielsweise eine hinsichtlich der Ressourcen optimale Ausführung möglich.\n\n\"Kognitive Agenten\" verwalten ein Modell ihrer Umwelt in einer eigenen Datenstruktur. Dadurch wird Planung der Aktionen und schließlich auch zielgerichtetes Handeln möglich. Eine bekannte Unterklasse ist die Agentendefinition in den BDI Agenten durch Angabe der \"Beliefs\", \"Desires\" und \"Intentions\".\n\nFolgende Agententypen treten in diesem Zusammenhang öfter auf:\n\nZielbasierter Agent\nNutzenbasierter Agent\n\n\n\n"}
{"id": "87022", "url": "https://de.wikipedia.org/wiki?curid=87022", "title": "ME10", "text": "ME10\n\nME10, heute \"Creo Elements/Direct Drafting\" ist ein CAD-Programm ausschließlich für zweidimensionale Zeichnungen, das vor allem im Maschinenbau und in der Elektromechanik verbreitet ist.\n\nDas Programm wurde von Hewlett-Packard in Deutschland entwickelt. 1986 veröffentlichte HP die erste Version. Innerhalb von Hewlett-Packard entwickelte die Abteilung MDD (Mechanical Design Division) ME10 weiter. Diese Abteilung wurde 1996 ausgegliedert und als CoCreate Software GmbH als Tochtergesellschaft von Hewlett-Packard gegründet. Im Jahr 2000 wurde dann mit Hilfe von Investmentgesellschaften ein Management-Buy-out (MBO) initiiert. Damit ist CoCreate eine eigenständige Gesellschaft und mit weltweit 450 Mitarbeitern in 30 Ländern vertreten. \n\nDas Programm beinhaltet eine eigene Makrosprache. Zudem gibt es verschiedene Zusatzmodule, wie das Modul ‚Parametric‘ für die Variantenkonstruktion. ME10 wurde als offenes System konzipiert, d. h., es wurde Fremdfirmen überlassen die notwendigen, anwendungsspezifischen Lösungen anzubieten. Diese Lösungen gibt es z. B. als Normteilebibliotheken für den Maschinenbau. Eine Besonderheit von ME10 ist das Arbeiten mit einer Teilestruktur (Baumstruktur), die das Konstruieren im Maschinenbau besser abbildet als das Arbeiten mit Layern. Eine andere Besonderheit ist das Arbeiten mit Hilfsgeometrie. Auch das kommt dem Maschinenbau-Konstrukteur entgegen. Diese Technik wurde mittlerweile auch von anderen CAD-Programmen (AutoCAD) übernommen.\n\nME10 wurde ursprünglich für das Betriebssystem HP-UX entwickelt. Mit dem Erfolg von Microsoft Windows wurde auch eine Version für dieses Betriebssystem angeboten. Zwischenzeitlich wurden auch einige Versionen unter Linux entwickelt. Mittlerweile ist MS-Windows die Standard-Plattform für ME10.\n\nIm Jahre 2002 wurde unter dem neuen CEO William M. Gascoigne ME10 umbenannt und hieß danach \"OneSpace Designer Drafting\", inzwischen abgekürzt zu \"OneSpace Drafting\" (in Abgrenzung zum 3D-Produkt \"OneSpace Modeling\"). ME10 ist nach wie vor eines der am meisten genutzten 2D CAD-Programme im Maschinenbau im deutschsprachigen Raum.\n\n"}
{"id": "87077", "url": "https://de.wikipedia.org/wiki?curid=87077", "title": "Adobe Dreamweaver", "text": "Adobe Dreamweaver\n\nDreamweaver ist ein HTML-Editor des Unternehmens Adobe Systems (ursprünglich von Macromedia entwickelt), bestehend aus einer Kombination eines WYSIWYG-Editors mit paralleler Quelltextbearbeitung. Aufgrund seiner Komplexität, seines großen Funktionsumfangs und Preises wird das Programm eher von professionellen Anwendern verwendet. Dreamweaver läuft derzeit auf den Betriebssystemen macOS und Windows. Nach Herstellerangaben hat Dreamweaver über 3,2 Millionen Benutzer weltweit.\n\nDreamweaver ermöglicht die grafische Bearbeitung von Webseiten und bietet Funktionen wie Farbkennzeichnung und Autovervollständigen für HTML-Code und Skriptsprachen wie PHP und JavaScript.\n\nDie erste Dreamweaver-Version erschien 1997. In den Dreamweaver-Versionen 7 bis CS3 wurde für das Layout-Rendering die Opera-Engine \"Presto\" verwendet, ab der Version CS4 die WebKit-Engine. In Version 2.3 der Adobe Creative Suite ist neben GoLive auch Dreamweaver enthalten. In der Creative Suite 3.0 wurde GoLive vollständig durch Dreamweaver ersetzt.\n\nSeit dem Update auf Version 11.0.4 ist partielle Syntaxunterstützung für HTML5 und CSS3 implementiert; seit CS5.5 wird sie vollständig unterstützt. Im CS5.5 wurde zudem das OpenSource Projekt PhoneGap in Dreamweaver eingeführt. Hiermit soll eine Applikationprogrammierung für iOS und Android möglich sein. Mithilfe der Integrierten Entwicklungsumgebung LiveCode wird die App-Entwicklung unterstützt.\n\nAdobe bot bis Ende 2012 eine Reihe von Apps für Smartphones und Tablets, die Nutzer bei der Arbeit mit Dreamweaver unterstützen sollen. Dazu gehörte zwischen 2011 und der Einstellung beispielsweise \"Adobe Proto\" für Apple iOS, mit dessen Hilfe sich das grundlegende Layout einer Website auf dem mobilen Endgerät planen ließ. Das Programm unterstützte zahlreiche Gesten, mit denen sich Kopf-, Navigations- und Fußleiste planen sowie interaktive Elemente wie Bilder und Videos platzieren lassen. Sobald der Entwurf einer Website auf dem Tablet abgeschlossen ist, konnte der automatisch generierte Quelltext zur weiteren Bearbeitung als ZIP-Archiv exportiert und an einen regulären Computer transferiert werden.\n\n"}
{"id": "87684", "url": "https://de.wikipedia.org/wiki?curid=87684", "title": "Eurex", "text": "Eurex\n\nDie European Exchange, kurz Eurex, ist eine der weltweit größten Terminbörsen für Finanzderivate (Futures und Optionen), die 1998 aus dem Zusammenschluss der DTB (Deutsche Terminbörse) und der zur SWX Swiss Exchange gehörenden SOFFEX (Swiss Options and Financial Futures Exchange) hervorging. Die Marke \"Eurex\" ist geschützt.\n\nEnde 1996 unterzeichneten Deutsche Börse AG und SWX Swiss Exchange eine Absichtserklärung, um eine gemeinsame Handels- und Clearing-Plattform für ihre derivativen Produkte zu schaffen. \nAm 4. September 1997 wurde im schweizerischen Bürgenstock die Bildung einer solchen Plattform offiziell bekannt gegeben. Im Laufe des Jahres 1998 konnte die Fusion zwischen der Deutschen Terminbörse (DTB) und der Swiss Options and Financial Futures Exchange (SOFFEX) zur European Exchange (Eurex) technisch und organisatorisch umgesetzt werden. Seit September 1998 handeln die Mitglieder beider Börsen nun auf der gemeinsamen Handels- und Clearing-Plattform. Damit gehören die beiden Börsen zu den ersten Institutionen, die den Zugang zu Derivate-Märkten über elektronische Handelsplattformen ermöglicht haben.\n\nBis 2012 war die Muttergesellschaft der Eurex Frankfurt AG die Eurex Zürich AG, die 100 % der Anteile hielt und an der die Deutsche Börse AG und die SWX Swiss Exchange zu gleichen Teilen beteiligt war. Tochtergesellschaften der Eurex Frankfurt AG sind die U.S. Exchange Holdings, Inc., Eurex Clearing AG (\"ECAG\"), Eurex Repo GmbH und die Eurex Bonds GmbH. Die Eurex Deutschland AG ist der öffentlich-rechtliche Teil der Eurex. Die Eurex AG gehört seit 2012 zu 100 % der Deutsche Börse Group.\n\nZu den ersten Produkten, die neben den von der DTB und der SOFFEX übernommenen Produkten an Eurex neu eingeführt werden, gehören Optionen auf Euro-Schatz-Futures, Dreimonats-EURIBOR-Futures, Euro-Buxl-Futures sowie Futures auf den EURO STOXX 50 (Index-Future). Im Geschäftsjahr 1998 wurden bereits knapp über 248 Millionen Kontrakte durch insgesamt 313 registrierte Mitglieder gehandelt.\n\nSeit 1999 steht den Marktteilnehmern eine grafische Benutzeroberfläche für den Handel an der Eurex zur Verfügung; im Zuge der Kooperation zwischen Eurex und der Helsinki Exchange Group Ltd. (HEX) werden im September 1999 neue Produkte auf skandinavische Indizes und Einzelwerte eingeführt. Gegenüber dem Vorjahr kann Eurex im Jahr 1999 ein deutliches Wachstum an Mitgliedern (auf über 400) und im Volumen (insgesamt rund 380 Millionen gehandelte Kontrakte) verzeichnen.\n\nGestaffelt über das Jahr 2000 wurden immer mehr Optionen auf einzelne Werte des EURO STOXX 50 Index eingeführt. Ebenso wurde in der Folgezeit das Angebot an Futures und Optionen auf Branchenindizes des EURO STOXX und des STOXX Europe 600 erweitert, sowie auch auf Optionen auf die liquidesten amerikanischen Einzelwerte. Neben dem Geschäft mit Futures und Optionen baute Eurex ein weiteres Standbein im außerbörslichen Handel und Clearing auf: Gemeinsam mit institutionellen Marktteilnehmern betreibt Eurex ab Oktober 2000 die beiden Plattformen Eurex Bonds und Eurex Repo für den Handel mit Staatsanleihen beziehungsweise den Repohandel mit Staatsanleihen und Jumbo-Pfandbriefen.\n\nDie Vielzahl neuer Produkte und die stetige Verbesserung der Handels- und Clearingbedingungen führen dazu, dass im Jahr 2001 über 674 Millionen Kontrakte an Eurex gehandelt wurden.\n\nIm November 2002 wurde der Handel mit Futures und Optionen auf Börsengehandelte Fonds (ETF) eingeführt. Durch die erweiterte Sektorproduktpalette sowie Optionen auf immer mehr Einzelwerte aus Europa und den USA wurden im Jahr 2002 über 800 Millionen gehandelter Kontrakte registriert.\n\nAnfang des Jahres 2003 ergänzte Eurex ihre Produkte im Zinsbereich um Futures auf den Zinssatz EONIA (European Overnight Index Average). Der Vertrag zum gemeinsamen Betrieb der Eurex zwischen der Deutsche Börse AG und der SWX Swiss Exchange wurde vorzeitig bis zum Jahr 2014 verlängert. Ende des Jahres 2003 wurde die Grenze von einer Milliarde gehandelter Kontrakte, insgesamt 1,014 Milliarden, erreicht.\n\nIm Februar 2004 startete Eurex in Chicago die Terminbörse Eurex US und dehnte damit ihr Geschäftsmodell auch auf den Handel und das Clearing von USD-denominierten Produkten in den USA aus. 2003 wurden 1,014 Milliarden Kontrakte gehandelt.\n\nIm Frühjahr 2006 fand ein Wechsel auf dem Posten des Vorstandsvorsitzenden statt; Andreas Preuß übernahm die Geschäfte seines Vorgängers Rudolf Ferscha. In das Produktsortiment neu aufgenommen wurden so genannte Weekly Options, der Bereich der Aktienindexderivate wurde damit auch auf den kurzfristigen Laufzeitenbereich erweitert. Ebenso wurde das Angebot an Aktienderivaten weiter ergänzt – um Futures und Optionen auf Basiswerte aus den Mid Cap-Indizes MDAX und SMI MID sowie auf spanische und schwedische Basiswerte. Ebenso wurden Aktienindexoptionen auf MDAX und SMIM eingeführt. Weiter wurde die erste Eurex-Anbindung eines Mitglieds in Singapur vollzogen. Ab 1. Oktober 2006 übernahm die Man Group 70 % der Eurex US und führt diese unter dem Namen USFE (US Futures Exchange) weiter. Zum Jahresende 2006 lag das Handelsvolumen bei 1,53 Milliarden gehandelten Kontrakten.\n\n\nEurex bietet ein umfangreiches Angebot an Aktien-, Aktienindex-, Zins- und Volatilitätsderivaten sowie Derivaten auf Börsengehandelte Fonds an.\n\nIm Aktienoptionssegment können Investoren an Eurex knapp 200 Aktienoptionen auf amerikanische, deutsche, finnische, französische, italienische, niederländische, schweizerische, schwedische und spanische Basistitel handeln, darunter Aktienoptionen auf 49 der 50 Werte des EURO STOXX 50 Index.\nNach und nach wird von Eurex auch die Anzahl an Futures auf Aktien (Single Stock Futures) erweitert. Auf mittlerweile insgesamt rund 370 Einzelwerte können nunmehr auch Futures gehandelt werden.\n\nIm Bereich der Aktienindexderivate umfasst das Produktangebot Futures und Optionen auf die führenden internationalen Blue-Chip-Indizes, europäische Midcap-Segmente sowie verschiedene Branchenindizes. Im Einzelnen sind dies:\n\n\nDie neuesten Erweiterungen sind Volatilitäts-Futures auf die Volatilitäts-Indizes der Deutsche Börse AG (VDAX-NEW), SWX Swiss Exchange (VSMI) und STOXX Ltd. (VSTOXX). Damit können Marktteilnehmer zum ersten Mal Volatilitätsschwankungen auf den deutschen, schweizerischen und europäischen Aktienmärkten mit einer börsengehandelten Futures-Familie absichern.\n\n\nIm November 2002 wurde an der Eurex der Handel mit Futures und Optionen auf Börsengehandelte Fonds (ETF), eingeführt. Die entsprechenden Basiswerte sind:\n\n\nDie Zinsderivate decken die deutsche Zinskurve im Laufzeitenbereich von bis zu 35 Jahren sowie die Schweizer Zinskurve im Laufzeitenbereich von 8 bis 13 Jahren ab:\n\n\nAuf Euro-Schatz-, Euro-Bobl- sowie Euro-Bund-Futures können auch Optionen gehandelt werden.\n\nWeitere Produkte im Bereich der Zinsderivate sind die folgenden Geldmarktderivate:\n\n\nDie Zinsderivate machen rund die Hälfte des Handelsvolumens an Eurex aus. Der Euro-Bund-Futures stellt mit rund einer Million gehandelter Kontrakte pro Tag das umsatzstärkste Produkt an Eurex dar.\n\nSeit 5. Dezember 2007 bietet die Eurex in Kooperation mit der European Energy Exchange AG (EEX) den Handel mit Energiederivaten, im Bereich CO-Futures an. Ebenfalls kooperieren die beiden Clearinghäuser der Eurex und der EEX in dieser Hinsicht. Die Kontrakte werden physisch durch die European Commodity Clearing AG (ECC) beliefert.\n\nDas Geschäftsmodell von Eurex erstreckt sich nicht nur auf Termingeschäfte, sondern ebenso auf außerbörsliche Kassamarktgeschäfte.\n\nDie Eurex Bonds GmbH wurde im Jahr 2000 als gemeinsame Initiative der Eurex Frankfurt AG und führender Finanzinstitute ins Leben gerufen. Sie bietet eine elektronische Plattform für den außerbörslichen Handel und das Clearing von folgenden Finanzinstrumenten:\n\nDurch die Anbindung der Eurex-Bonds-Handelsplattform an das Clearingsystem der Eurex Clearing AG besteht eine unmittelbare Verbindung von Kassa- und Terminmarkt, durch die der elektronische Handel der Schuldverschreibungen der Bundesrepublik Deutschland und der Basistitel für alle in die Fixed-Income-Derivate lieferbaren Schuldverschreibungen in einem zentralen Orderbuch möglich wird.\n\nAn Eurex Repo können Marktteilnehmer eine breite Auswahl an Rückkaufvereinbarungen (Repos), unter anderem für deutsche und österreichische Staatsanleihen, Jumbo-Pfandbriefe, Länderanleihen, Schuldverschreibungen der KfW-Bankengruppe sowie der europäischen Investmentbankanleihen handeln. Repos kombinieren den Verkauf von Wertpapieren mit dem Rückkauf gleichartiger Papiere auf Termin.\n\nEurex Repo operiert auf zwei verschiedenen Märkten, dem Repo-Markt in Euro und dem Repo-Markt in Schweizer Franken. Die Marktteilnehmer haben so Zugang zu verlässlichen Handels-, Clearing- und Abwicklungssystemen für den europäischen und schweizerischen Repo-Markt.\n\nDer Repo-Markt in Euro ermöglicht europäischen Finanzdienstleistern den Zugang zur Refinanzierung mit europäischen Wertpapieren. Eurex Repo gibt den institutionellen Investoren die Möglichkeit, in verschiedenen standardisierten Besicherungs-Segmenten ihr Liquiditätsmanagement zu optimieren.\n\nAm Repo-Markt in Schweizer Franken verwalten über einhundert internationale Banken ihre Liquidität. Die Marktteilnehmer können ihr Funding- und Besicherungsmanagement direkt am Schweizer Interbankenmarkt sowie über eine Beteiligung an den meist täglich stattfindenden Auktionen der Schweizerischen Nationalbank (SNB) vornehmen.\n\nBei den Repo-Geschäften zwischen zwei institutionellen Investoren tritt die Eurex Clearing AG als zentraler Kontrahent auf und garantiert den Marktteilnehmern somit die tatsächliche Erfüllung der eingegangenen Leistungsverpflichtungen.\n\nGrundsätzlich sind Eurex Bonds und Eurex Repo für alle interessierten Kreditinstitute und Finanzdienstleister offen. Die Unternehmen müssen in ihrem jeweiligen Sitzstaat einer Finanzmarktaufsicht unterstehen. Für Privatanleger ist der Handel an Eurex Bonds und Eurex Repo derzeit nicht möglich, da es ein reiner Interbankenhandel ist und die Kontraktgrösse im mehrstelligen Millionenbereich liegt.\n\nAm 22. Februar 2016 wurde der Börsenhandel nach M1-Blow-Off in den Instrumenten FDAX, FDXM, FESX, BUND, SCHATZ, BOBL und BUXL durch Market Supervision für den europäischen Futureshandel im EUREX-System T7 von 11:09 bis 12:50 Uhr angehalten.\n\n"}
{"id": "88268", "url": "https://de.wikipedia.org/wiki?curid=88268", "title": "Mono (Software)", "text": "Mono (Software)\n\nMono (spanisch für „Affe“ oder auch vom griechischen \"monos\" für „allein“ oder „einzig“) ist eine alternative, quelloffene Implementierung von Microsofts .NET Framework. Sie ermöglicht die Entwicklung von plattformunabhängiger Software auf den Standards der Common Language Infrastructure und der Programmiersprache C#. Entstanden ist das Mono-Projekt 2001 unter Führung von Miguel de Icaza von der Firma Ximian, die 2003 von \"Novell\" aufgekauft wurde. Die Entwickler wurden 2011 in eine neue Firma \"Xamarin\" übernommen, die im Jahr 2016 eine Microsoft-Tochtergesellschaft wurde. In der Folge wurde Microsoft Hauptsponsor des Projektes.\n\nDie Existenzberechtigung von Mono als unabhängige Implementierung des .NET Frameworks begründet sich durch die hohe Anzahl unterstützter Plattformen und Architekturen. Durch Lizenzierung unter MIT-Lizenz gibt es für Entwickler und Nutzer praktisch keine Einschränkungen mehr. Mit Hilfe eines Einsatzes von Mono beispielsweise auf Linux-Backends können hohe Lizenzkosten für das Betriebssystem des Servers vermieden werden.\nHinzu kommt bei der Entwicklung von Smartphone-Apps der Wunsch, Code für verschiedene Plattformen (Android, iOS, Windows Phone) auf einer gemeinsamen Basis zu pflegen.\n\nMicrosoft dagegen bot seine Laufzeitumgebung \".NET\" lange Zeit ausschließlich für seine eigenen Windows-Betriebssysteme an. Ursprünglich gab es von Microsoft eine Open-Source-Variante von .NET namens Rotor, welche neben Windows auch für FreeBSD und macOS verfügbar war. Dennoch war das Unternehmen Xamarin einst der Ansicht, dass es mit einer eigenen Entwicklung in diesem Bereich Erfolg haben könnte, insbesondere weil ihm die von Microsoft angebotenen Lizenzbedingungen für viele Bereiche zu restriktiv erschienen. Seit dem 12. November 2014 begann Microsoft, Teilmengen des .Net-Frameworks via GitHub unter der MIT-Lizenz zu veröffentlichen. So befinden sich dort neben dem .NET Core und der offenen C#- und Visual-Basic-Compiler-Plattform Roslyn auch eine in der Entwicklung befindliche, plattformübergreifende Laufzeitumgebung, die .Net Core CLR. Mit diesen Projekten möchte Microsoft unter anderem die Entwicklung des Mono-Projekts unterstützen.\n\nMit Mono ist es möglich, Programme, welche für die \"Microsoft-.NET\"-Umgebung erstellt wurden, auch ohne Neuübersetzung unter unixähnlichen Betriebssystemen auszuführen. Der Vorteil einer solchen Crossplattform-Entwicklung besteht darin, den Komfort und Funktionsumfang der Microsoft Entwicklungsumgebung (Microsoft Visual Studio) zu nutzen. Auch SharpDevelop steht nur unter Windows zur Verfügung. Umgekehrt lassen sich mit MonoDevelop auf anderen Betriebssystemen Programme entwickeln, die auch mit \"Microsoft .NET\" lauffähig sind.\n\nMono bietet ab der Version 2.10 die Kompatibilität mit den nicht-Windows-spezifischen Bibliotheken von .NET 2.0. Anwendungen, die auf .NET 3.0 oder höher basieren, sind derzeit unter Mono unter Umständen nicht lauffähig, da bislang nur eine Teilmenge umgesetzt ist. Es fehlen Windows Presentation Foundation, Windows Workflow Foundation und teilweise Windows Communication Foundation. Des Weiteren ist der Zugriff auf windowsspezifische Funktionalitäten mittels P-Invoke oder COM Interop, d. h. die Verwendung von Bibliotheken, die nicht in IL-Code, sondern in normalem, Prozessor-spezifischem Assemblercode vorliegen, nicht gestattet. Zwar kann auch Mono auf Bibliotheken zugreifen, die in C oder C++ geschrieben sind, allerdings sind die meisten dieser Bibliotheken plattformabhängig.\n\nAktuell können Anwendungen auf Basis des .NET-4.7.2-Profils von Mono ausgeführt werden (Stand Januar 2019). Einschränkungen gibt es jedoch in verschiedenen Teilbereichen des Frameworks. Explizit ausgenommen wurde die Unterstützung der Windows Presentation Foundation, die auf absehbare Zeit nicht reimplementiert werden wird. XAML sollte im Rahmen von Moonlight unterstützt werden, dessen Entwicklung wurde allerdings eingestellt, später auch die des Original Silverlight. Im Rahmen von Xamarin Forms kam Xaml später aber trotzdem zum Einsatz.\n\nMono hat verglichen mit .NET auch eine kleine Menge von Zusatzfunktionen, die sich im Namespace Mono.* befinden, beispielsweise Schnittstellen für betriebssystemnahe Funktionen unter Unix (Mono.Unix.Native) oder die Mono.Cecil.dll, mit der Änderungen an bereits kompiliertem Code vorgenommen werden können.\n\nDes Weiteren ist die Generierung von nativem Code (aus der .exe oder .dll) mit Hilfe von mono --aot=full einfach und transparent möglich.\nUnter .NET dagegen werden Administrator-Rechte für den Aufruf von NGEN benötigt und das Ergebnis landet in einem Native Image Cache mit kryptischem Dateinamen.\n\nDas von Miguel de Icaza mitgegründete Unternehmen Ximian (am 4. August 2003 von Novell gekauft) setzte sich die Entwicklung einer Reihe von .NET-kompatiblen Entwicklungswerkzeugen zum Ziel, einschließlich eines C#-Compilers und einer \"Common Language Runtime\" für den Betrieb unter Windows, Linux, verschiedenen Unix-Derivaten sowie macOS.\n\nMiguel de Icaza interessierte sich für die .NET-Technologie, seit im Dezember 2000 die ersten .NET-Informationen verfügbar wurden. Im Februar 2001 begann er zu Übungszwecken einen C#-Compiler der Programmiersprache in C# zu schreiben. Im April 2001 konnte er auf einer GNOME-Konferenz eine erste Version präsentieren.\n\nBei Ximian gab es intern viele Diskussionen über die Entwicklung von Tools zur Produktivitätssteigerung, um mehr Anwendungen in kürzerer Zeit zu erstellen und die Entwicklungskosten reduzieren zu können. Nach einer Machbarkeitsstudie entstand das Mono-Team. Wegen der begrenzten Anzahl der Mitarbeiter war es Ximian allerdings nicht möglich, einen vollständigen .NET-Ersatz zu schreiben, also wurde das Mono-Open-Source-Projekt gegründet, welches auf der O’Reilly-Konferenz im Juli 2001 bekanntgegeben wurde.\n\nDrei Jahre später, am 30. Juni 2004, wurde Mono 1.0 veröffentlicht.\n\nMit der am 6. Oktober 2008 veröffentlichten Version 2.0 wurden die wichtigsten Eigenschaften von .NET 2.0 hinzugefügt und es gibt mit dem Projekt \"Olive\" auch eine Initiative, die neueren Technologien von .NET 3.0 und .NET 3.5 zu implementieren. Zudem wurde der C#-Compiler um die Sprach-Eigenschaften von C# 3.0 erweitert (hierbei handelt es sich in erster Linie um die Unterstützung von LINQ), sowie ein neuer XAML-Parser entwickelt. Allerdings weisen die Entwickler ausdrücklich darauf hin, dass die Windows Presentation Foundation wohl vorerst wegen der enormen Komplexität nicht implementiert wird. Ebenfalls mit der Version 2.0 ist nun auch ein Visual-Basic-8.0-Compiler mit an Bord.\n\nMit der am 9. Dezember 2009 veröffentlichten Version 2.4.3 wurde – neben einigen Fehlerkorrekturen – der freie C#-Compiler mit allen wesentlichen Funktionen von C# 4.0 ergänzt.\n\nAnfang 2011 stellte der neue Novell-Eigentümer Attachmate die Weiterentwicklung des Mono-Projektes ein und entließ am 2. Mai 2011 30 Mono-Entwickler. Nils Brauckmann (der neue Suse-Chef) nannte als Grund die geringe Nachfrage bei den Kunden.\nWenige Tage nach dem Stopp des Mono-Projekts bei Novell gründete Miguel de Icaza das Unternehmen Xamarin, das sich in Zukunft der Entwicklung von Mono widmen soll. Die meisten zuvor bei Suse angestellten Mono-Entwickler wechselten zu Xamarin.\n\nIn weiterer Folge gewährte SUSE Xamarin eine unbefristete Lizenz zur Nutzung und kommerziellen Verwertung von Mono, MonoTouch für iOS und Android sowie den Mono Tools for Visual Studio.\n\nMit der Übernahme durch Microsoft im Jahr 2016 wurden die Xamarin-Tools in Microsoft Visual Studio integriert und basierend auf der Xamarin-IDE \"Xamarin Studio\" wurde \"Visual Studio for Mac\" entwickelt. Mono wurde unter das Dach der .Net Foundation gestellt und unter der MIT-Lizenz neu herausgegeben.\n\nDurch die Bereitstellung der .NET Core Plattform als Open Source unter einer kompatiblen Lizenz ist die gegenseitige Übernahme von Code für beide Projekte wesentlich vereinfacht. Seit Mai 2017 enthalten die offiziellen Pakete von Mono auch eine Version des C#-Compiler (Roslyn) sowie von MSBuild. Dies ermöglicht die Übersetzung von Programmen, die C#-7.0 Konstrukte verwenden.\n\nIm Rahmen des noch in der Entwicklung befindlichen Blazor-Frameworks kommt derzeit eine Mono-Umgebung zum Einsatz, die als WebAssembly kompiliert wurde.\n\nSeit der Version 5.12 werden auch IBM i und IBM AIX unterstützt.\n\nTeile der Klassenbibliothek berühren möglicherweise Softwarepatente von Microsoft. Microsoft hat mit Novell ein gegenseitiges Patentabkommen geschlossen, welches Novell und seine Kunden vor Rechtsansprüchen Microsofts schützt. Dieses beinhaltet auch einen Patentschutz für Mono. Für (fast) alle anderen Nutzer besteht das Risiko jedoch weiterhin. Die Sicht der Entwickler auf das Problem brachte Linus Torvalds auf den Punkt, als er sich durch Patentansprüche von SCO mit dem Thema konfrontiert sah: „Ich achte prinzipiell nicht auf Patente, denn das wäre Zeitverschwendung.“ Nicht ohne Grund sind Softwarepatente höchst umstritten und in der EU formal nicht gestattet.\n\nDie grundlegenden Technologien sind teilweise durch Microsoft bei Ecma International und der ISO standardisiert worden.\nMicrosoft garantiert eine Lizenzierung der ECMA-Teile auf RAND-Basis.\nAndere Teile, wie zum Beispiel Windows Forms, ADO.NET und ASP.NET sind hiervon jedoch ausgeschlossen.\n\nDas Open Invention Network verteidigt \"Mono\" bei patentrechtlichen Auseinandersetzungen.\n\nAufgrund der Gefahr durch Patentklagen seitens Microsoft hat Richard Stallman, der ideologische Führer der Freie-Software-Bewegung, vor Mono gewarnt, nachdem einige Distributionen dazu übergegangen waren, Mono in die Standardinstallation aufzunehmen. Microsoft hat mittlerweile .NET und C# unwiderruflich unter die \"Community Promise\" Vereinbarung gestellt und will auf Patentklagen verzichten.\n\nMicrosoft hat 2013 seine Strategie bezüglich .NET grundlegend geändert und arbeitet daran, den Quellcode von .NET komplett als Open Source offenzulegen. Dazu wurde von Microsoft, Xamarin und anderen die Stiftung \".NET Foundation\" gegründet, welche die Rechte am .NET-Framework übertragen bekommen hat. Durch die Offenlegung der Quellcodes unter der MIT-Lizenz bzw. Apache-2.0-Lizenz ist der Quellcode des .NET-Frameworks nahezu beliebig – sprich auch in Closed-Source-Projekten – verwendbar. Lizenz- und patentrechtliche Auseinandersetzungen waren somit kaum noch möglich und somit auch nicht mehr zu befürchten. Microsoft und Xamarin arbeiteten seit 2015 gemeinsam daran, .NET auf unterschiedlichen Plattformen bereitzustellen. Dies mündete ein in die Übernahme von Xamarin durch Microsoft 2016, was Patentklagen gegen Xamarin (und Mono) natürlich erst recht sinnlos macht.\n\n\n\n"}
{"id": "89694", "url": "https://de.wikipedia.org/wiki?curid=89694", "title": "Bzip2", "text": "Bzip2\n\nbzip2 ist ein freies Komprimierungsprogramm zur verlustfreien Kompression von Dateien, entwickelt von Julian Seward. Es ist frei von jeglichen patentierten Algorithmen und wird unter einer BSD-ähnlichen Lizenz vertrieben.\n\nBzip2 komprimiert Daten in einem dreistufigen Verfahren: Zuerst werden die Eingangsdaten blockweise mit der umkehrbaren Burrows-Wheeler-Transformation sortiert. Das Ergebnis wird dann einer Move-to-Front-Transformation unterzogen. Deren Ergebnis wird dann schließlich einer Huffman-Kodierung unterzogen, die die eigentliche Datenkompression vornimmt.\n\nDie Kompression mit bzip2 ist oft effektiver, aber meist erheblich langsamer als die Kompression mit gzip oder rar. Seit 2003 existiert jedoch auch die Variante \"pbzip2\", die Multi-Threading beherrscht und auf aktuellen Mehrkernprozessoren erheblich schneller ist. \"pbzip\" zerlegt hierfür den Eingabedatenstrom in mehrere einzelne Ströme, welche separat komprimiert werden. Das Ergebnis ist eine Datei, welche die konkatenierten Bzip-Ströme enthält.\n\nMit bzip2 komprimierte Dateien werden durch die Dateiendung \".bz2\" gekennzeichnet. tar-Dateien, die mit bzip2 komprimiert wurden, haben üblicherweise die Erweiterung \".tar.bz2\" oder \".tbz2\". Ein Vorteil solcher mit bzip2 komprimierter tar-Dateien ist, dass sich bei Lesefehlern oder Beschädigungen alle noch lesbaren Blöcke mittels bzip2recover herauskopieren und anschließend entpacken lassen, während andere Kompressionsverfahren nach einem Lesefehler nicht weiterarbeiten können.\n\nbzip2 ist der Nachfolger von bzip, das ursprünglich arithmetisches Kodieren nach dem Blocksort benutzte; aus patentrechtlichen Gründen wurde bzip jedoch nicht mehr weiterentwickelt. \n\nDas Kommandozeilenprogramm bzip2 benutzt für die eigentliche Kompressions- und Dekompressionsarbeit eine Programmbibliothek namens \"libbzip2\", welche auch von anderen Programmen, die das bz2-Dateiformat lesen und schreiben können, verwendet wird.\n\nDiese Programmbibliothek bietet Funktionen, um beliebige Daten im Hauptspeicher zu komprimieren, und eine stdio-ähnliche Schnittstelle zum Lesen und Schreiben von bz2-komprimierten Dateien.\n\nEin codice_1 Datenstrom beginnt mit einer Signatur (4 Byte), gefolgt von Null oder mehr komprimierten Blöcken, direkt anschließend folgt ein End-of-Stream-Marker und ein CRC (32-Bit) für den Ursprungsinhalt der ganzen Datei. Die komprimierten Blöcke sind Bit-aligned (kein Padding).\n\n\n"}
{"id": "90874", "url": "https://de.wikipedia.org/wiki?curid=90874", "title": "Windows 9x", "text": "Windows 9x\n\nUnter den Bezeichnungen Windows 9x (abgekürzt auch Win9x) oder auch Windows-9x-Reihe werden die folgenden Betriebssysteme des Softwareunternehmens Microsoft für IA-32-Prozessoren (32-Bit, i386) zusammengefasst:\n\n\nUnter \"Windows 9x\" sind alle MS-DOS-basierten Windows-Betriebssysteme nach Windows 3.x zusammengefasst. Im Gegensatz zu Windows 3.x hat die Windows-9x-Reihe eine tief in den Kernel integrierte 32-Bit-Erweiterung, die auch die Win32-API bereitstellt. Windows 9x unterstützt für 32-Bit-Anwendungen präemptives Multitasking und für Kompatibilität mit Windows-3.x 16-Bit-Anwendungen kooperatives Multitasking. 32-Bit-Prozesse besitzen jeweils eigene virtuelle Adressräume (Speicherschutz), konsequenter Speicherschutz ist jedoch, um kompatibel mit Anwendungen zu bleiben, die direkt Hardware ansprechen, nicht gewährleistet.\n\nInsbesondere bei Aufzählungen von mehreren Betriebssystemen (z. B. auf einer Software-Verpackung oder im Internet) wird die Kurzschreibweise \"Windows 9x\" stellvertretend für die oben genannten Betriebssysteme verwendet. Der Begriff leitet sich aus den Namen her, die meist mit einer Neun beginnen. Windows Me schert aus diesem Schema aus, wird aber wegen seiner technischen Verwandtschaft zu den anderen Versionen auch unter \"Windows 9x\" zusammengefasst.\n\nIn allen früheren Versionen von Windows wurden die Versionsnummern auch als Produktnamen genutzt, seit Windows 9x trugen mehrere Windows-Betriebssysteme zwischenzeitlich eigene Namen. Diese lauten beispielsweise Windows 2000, Windows XP und Windows Vista. Von dieser Praxis wurde jedoch seit Windows 7 wieder abgerückt, wobei jedoch die Nummerierung des Produktnamens nicht den internen Versionsnummern folgt. Die lediglich intern vergebenen Versionsnummern in der 9x-Reihe sind 4.0, 4.1 und 4.9.\n\nEin weiterer Unterschied zu den Vorläufern ist, dass in jedem Windows 9x eine angepasste Version von MS-DOS enthalten ist. Microsoft wollte dem Benutzer beim Erscheinen von Windows 95 so suggerieren, MS-DOS sei ein Teil von Windows und existiere nicht mehr eigenständig. Allerdings sind die DOS-Versionen aus jedem Windows 9x auch ohne Windows vollständig lauffähig. Bei Windows Me kann man standardmäßig MS-DOS nur noch in einem Programmfenster ausführen. Mit einem Trick lässt sich jedoch auch der herkömmliche Real Mode aktivieren. Intern tragen diese DOS-Versionen die Nummern 7.00, 7.10 sowie 8.00.\n\nBeim Einsatz von USB-Massenspeicher ist häufig ein zusätzlicher nicht im Betriebssystem vorhandener Treiber erforderlich. Es existiert hierzu für Windows 98 auch ein generischer Treiber, der auch unterschiedliche Speichermedien unterstützt.\n"}
{"id": "90985", "url": "https://de.wikipedia.org/wiki?curid=90985", "title": "Sinclair QL", "text": "Sinclair QL\n\nDer Sinclair QL ist ein als Heim- und Personal Computer ausgelegter Computer des Herstellers Sinclair Research, der in der ursprünglichen, 1984 erstmals vermarkteten Variante über einen Motorola-68008-Prozessor, 128 kB RAM und zwei Microdrive-Bandlaufwerke verfügte. Das QL im Namen steht für Quantensprung (Englisch: quantum leap).\n\nTrotz seines teilweise äußerst fortschrittlichen Konzepts, etwa einer einfachen grafischen Benutzeroberfläche mit Fenstern, hochauflösender Grafik, einem sehr leistungsfähigen BASIC und dem damals fortschrittlichen Mikroprozessor wurde das Gerät kein kommerzieller Erfolg für Sinclair. Dies wurde unter anderem darauf zurückgeführt, dass der QL für professionelle Anwendungen wegen der langsamen Microdrive-Bandlaufwerke weniger geeignet, andererseits für einen reinen Spiele- und Hobby-orientierten Heimcomputer aber überdimensioniert und zu teuer war. Gleichzeitig verzögerte sich die Markteinführung immer wieder, und die ersten Geräte hatten technische Probleme, was einen Imageschaden verursachte. Durch die lange Verzögerung des Verkaufsstarts war der QL zudem nach kurzer Zeit mit Konkurrenzprodukten wie dem Atari ST und dem Commodore Amiga konfrontiert, die dem Gerät in vielen Aspekten überlegen waren.\n\nDer Sinclair QL ist in Form einer Konsole mit zwei integrierten Microdrive-Laufwerken aufgebaut. Microdrives sind Bandspeicher, die ein Miniatur-Endloskassette als Speichermedium verwenden und dank sektororientierter Datenspeicherung eine ähnliche Funktionalität wie Diskettenlaufwerke bieten. Die RAM-Grundausstattung von 128 KB war mittels ursprünglicher Erweiterungsmodule bis auf 896 kB aufrüstbar. Der Sinclair QL besitzt Anschlüsse sowohl für RGB- als auch damals geläufigere (F)BAS-Monitore, um seine Grafikauflösung von 512×256 Pixeln in vier Farben oder in vier Graustufen sichtbar machen zu können.\n\nEbenfalls ist ein HF-Modulator eingebaut, so dass der QL auch an einem Fernsehgerät angeschlossen werden kann. Besonders im, auch über den HF-Modulator verfügbaren, Grafikmodus von 512×256 Pixeln ist je nach Qualität der Analog-Elektronik des Fernsehgerätes mit Einschränkungen von Bildgröße und -qualität zu rechnen. Da in den 1980ern bei Heimcomputern üblicherweise Fernsehgeräte zur Bilddarstellung verwendet wurden, hatte Sinclair deswegen einen alternativen Darstellungsmodus von 256×256 Pixel in acht Farben mit vergrößertem Bildrand implementiert, der auch auf schlechten Fernsehgeräten noch eine vollständige und relativ deutliche Anzeige bietet.\n\nAuf seiner linken Seite bietet der QL einen Einschub für Erweiterungsmodule. Kleinere Exemplare, etwa Floppy-Disk-Controller, verschwinden komplett darin, andere ragen aus dem Gerät heraus. Die gängigsten Erweiterungsmodule waren Speichererweiterungen. Außerdem besitzt der QL einen Steckplatz an seiner Rückseite, in den ROM-Module gesteckt werden können.\n\nDas QDOS genannte Betriebssystem mit integrierter Shell und Basicinterpreter (in Abgrenzung zur ZX-Reihe „SuperBASIC“ genannt) befindet sich in einem 48 KB ROM. Es unterstützt serienmäßig – einmalig für die damalige Zeit – echtes präemptives Multitasking. Das BASIC des QL ist sehr umfangreich und leistungsfähig, es unterstützt komfortable Stringmanipulation und ähnlich wie Pascal geschachtelte Anweisungen, Funktionen mit mehreren Parametern, lokale Variable und rekursive Prozeduren.\n\nDie Benutzeroberfläche bietet einfache Fenstertechnik mit beliebig vielen und beliebig angeordneten Fenstern, die allerdings alle in derselben Ebene liegen – Ausgaben in sich überlappende Fenster können sich daher gegenseitig überschreiben (was von den Applikationen aufgefangen werden kann). Alle Fenster sind wie beim zeitgleich erschienenen Apple Macintosh „bitmapped“ und damit voll grafikfähig, d. h. jedes Pixel ist einzeln ansteuerbar. Das Betriebssystem unterscheidet Nur-Ausgabe-Fenster und Fenster, die sowohl zu beliebiger Ausgabe als auch zur Text-Eingabe dienen können. Da für Textausgabe jedem Fenster individuelle Zeichensätze zugeordnet werden können, sind auch Ausgaben in exotischen Schriftarten realisierbar.\n\nDer QL ist außerdem serienmäßig netzwerkfähig; das QLAN genannte Netzwerk läuft laut Herstellerangaben mit 100 kBit/s (tatsächlich aber 87 kBit/s) und kann bis zu 63 Sinclair QL (und auch Sinclair ZX Spectrum, sofern sie mit einem Interface 1 ausgestattet sind) miteinander verbinden. Es ist im ZX8302-Chip implementiert und mit einigen BASIC-Befehlen auch ansprechbar. QLAN ist ein Nachfolger des ZXNET, das im Interface 1 des Sinclair Spectrum enthalten ist. Der Funktionsumfang im Auslieferungszustand ist nach heutigen Maßstäben minimal, in der Werbung ist allerdings die gemeinsame Nutzung von Microdrive-Laufwerken und Druckern vermerkt, geredet wird auch von Dateiübertragung, von gemeinsamen Spielen und der Nutzung in Schulen. Mit Toolkit 2, einer ROM-Erweiterung des Drittherstellers QJump kann ein Fileserver implementiert werden, der über das QLAN Zugriff auf entfernte Speichermedien und Drucker anderer Netzwerkteilnehmer erlaubt. Eine Vernetzung war im Homecomputerbereich eher unüblich, entwickelte sich eher in Universitäten, im DFÜ und Businessbereich. Die spätere QL-Variante One-Per-Desk war, mit anderem Betriebssystem und abgeänderter Hardware, dann primär für den Telefonbetrieb ausgerichtet.\n\nDie beiden RS-232C-Anschlüsse können nur mit bis zu 9.600 Baud betrieben werden, im Nur-Sendemodus (z. B. an Drucker) auch mit 19.200 Baud. Für Spiele sind zwei Joystick-Anschlüsse vorhanden.\n\nAls Softwareausstattung wurden auch die für die damalige Zeit sehr leistungsfähigen Programme \"Quill\" (Textverarbeitung), \"Abacus\" (Tabellenkalkulation), \"Archive\" (Datenbank mit eigener Programmiersprache) und \"Easel\" (Geschäftsgrafik) mitgeliefert. Diese Programme stammten vom englischen Hersteller Psion, die zuvor bereits eine Reihe von Softwaretiteln für die Vorgänger des QL (ZX80/81, ZX Spectrum) produziert hatte. Dieses frühe Office-Paket wurde von der Presse ausgesprochen positiv beurteilt und galt lange als eines der wesentlichsten Argumente für den Kauf eines QL überhaupt. Einige Jahre später erschien das Programmpaket übrigens in leicht veränderter Form unter dem Titel \"XChange\" auch für IBM-kompatible Rechner (es kostete alleine mehr als ein Sinclair QL inklusive der Psion-Software).\n\nAbgesehen vom Sinclair-„Hauslieferanten“ Psion wurde der QL von Softwareherstellern jedoch wenig unterstützt. Insbesondere Spiele, die ihn für den typischen Heimanwender attraktiv gemacht hätten, gab es deutlich weniger als für den damaligen Hauptkonkurrenten C64. Für viele Büroanwender wäre das mitgelieferte Psion-Paket wegen seiner unbestrittenen Qualität ein ausreichender Kaufanreiz gewesen, wenn sie nicht durch die damals in vielen Zeitschriften erschienenen negativen Testberichte, die sich auf die verfrühten QL-Auslieferungen und die ungewohnten Microdrives bezogen, abgeschreckt worden wären.\n\nNach dem Produktionsende des QL setzten Insider die Entwicklung des QL und vor allem des Betriebssystems QDOS eigenmächtig fort. Unter anderem wurde QDOS auf die leistungsfähigere Hardware des Atari ST übertragen und angepasst.\n\nDer QL brachte einige größere Neuerungen mit sich: Er war der erste Homecomputer, dessen Betriebssystem über Multitasking verfügte. Der QL basierte zudem auf einer Motorola-CPU der 68k-Serie, also einem für seine Zeit sehr fortschrittlichen Mikroprozessor als Kern. Allerdings wurde mit dem Motorola 68008 die kleinste Variante ausgewählt; bei diesem Prozessor ist der Datenbus auf 8 Bit und der Adressbus auf 20 Bit reduziert, was den hardwaretechnischen Aufwand stark vereinfachte. Gegenüber einem gleich getakteten 68000 läuft der 68008 wegen der vermehrten Speicherzugriffszyklen etwa 15 % langsamer; der QL mit seiner mit 7,5 MHz getakteten 68008 CPU lief also etwa so schnell wie ein Computer mit einer mit 6,5 MHz getakteten 68000 CPU. Als zweiten Prozessor besaß der QL einen Intel 8049 für die Steuerung der Tastatur, Tonausgabe, Schnittstellen und der Microdrives; dadurch konnten Datenübertragungen bereits im Hintergrund stattfinden.\n\nIn anderer Hinsicht brachte der QL nur wenig Fortschritt: Wenn man kein teures Floppylaufwerk dazukaufte, erfolgte die Datenspeicherung auf den standardmäßig eingebauten eigenentwickelten Endloskassetten „Microdrives“ (aus der Entwicklung des Spectrum entlehnt), die nur langsame Datenzugriffe boten und ein ermüdungsanfälliges Endlosband benutzten. Zwar wurden an anderen Heimcomputern damals noch wesentlich langsamere Bandlaufwerke verwendet, aber bei den professionellen CP/M- und MSDOS-Computern waren die schnelleren, aber teuren 8\"- und 5¼\"-Diskettenlaufwerke Standard. Der etwa zur gleichen Zeit wie der QL erschienene Apple Macintosh enthielt sogar schon eines der brandneuen 3½\"-Laufwerke von Sony (mit 400 KB). Für den QL waren anfangs nur 5¼\"-Laufwerke erhältlich, im Herbst 1984 schließlich auch 3½\"-Laufwerke mit 720 kB Speicherkapazität. Anfang 1985 begannen die Preise für Diskettenlaufwerke stark zu fallen, tauchten somit zunehmend standardmäßig bei Homecomputern auf und ließen die Microdrives des QL umso exotischer erscheinen.\n\nFür einige Länder, so Deutschland und Frankreich, wurden lokalisierte QL produziert. Die deutsche Version wich in einigen Details von der englischen ab. Neben einer QWERTZ-Tastatur war auch das Betriebssystem eingedeutscht, etwa bei den Fehlermeldungen. Zudem hatte der „deutsche“ QL andere Joystick- und serielle Anschlussbuchsen. Außerdem war das Gehäuse durch eine Metallbedampfung und den Einbau einer Drossel besser funkentstört.\n\nEine Reihe von Faktoren trugen dazu bei, dass der QL im Vergleich zu seinen Vorgängern Sinclair ZX81 und ZX Spectrum ein kommerzieller Misserfolg wurde. Zunächst verging zwischen der Ankündigung und Auslieferung eine relativ lange Zeit; die britische Computerpresse interpretierte das Kürzel „QL“ daher auch als \"quite late\" („ziemlich spät“). Im Frühjahr 1984 wurden noch unfertige QL ausgeliefert. Da das Betriebssystem (auf ROM) nicht rechtzeitig fertiggestellt worden war, musste es bei diesen frühen Exemplaren in Form eines aus dem Gehäuse ragenden Dongles (auch „kludge“, dt.: „Behelfslösung“ genannt) ergänzt werden (wodurch die geplanten 32 KB ROM auf 48 erweitert wurden). Auch später wurden QL auffällig häufig als defekt reklamiert. Dadurch wurde das Image des Rechners frühzeitig beschädigt.\n\nAls er schließlich Ende 1984 in größeren Stückzahlen und funktionierenden Exemplaren ausgeliefert werden konnte (nun mit 48 KB eingebautem ROM), war der QL angesichts der raschen Entwicklung Mitte der 1980er-Jahre einem deutlich stärkeren Konkurrenzdruck ausgesetzt. Der QL unterstützte zwar Multitasking, farbige Rastergrafik und eine frühe Version der Fenstertechnik, wurde jedoch ausschließlich durch Tastaturkommandos bedient. Eine grafische Benutzeroberfläche mit Schreibtischmetapher, Icons und Mausbedienung, wie sie bei Xerox in den frühen 1970er-Jahren entwickelt und 1984 mit dem Erscheinen des Apple Macintosh Stand der Technik wurde, gab es nicht.\n\nSinclairs amerikanische Wettbewerber Atari und Commodore stellten 1985 mit dem Atari ST bzw. Amiga ebenfalls Modelle vor, die wie die Apple-Computer über eine grafische Benutzeroberfläche mit Mausbedienung verfügten und zudem leistungsfähiger waren da sie den Motorola-68000-Prozessor mit 16-Bit-Datenbus verwendeten. Somit war der QL als Bürocomputer nicht konkurrenzfähig.\n\nEin weiterer Aspekt, mit dem der QL ins Hintertreffen geriet, war die Verwendung des proprietären Microdrives als integrierter Massenspeicher. Microdrive-Cartridges waren gegenüber Disketten, die die im Heimanwenderbereich noch verbreiteten Kassetten als Speichermedien verdrängten, in mehrfacher Hinsicht im Nachteil: Sie wurden kurz nach Erscheinen des QL von den zunächst teureren Diskettenlaufwerken preislich unterboten, waren im Zugriff langsamer als jene, galten als störanfällig und fassten lediglich etwa 110 kB (in Hard- und Software nicht kompatibel mit den 85 KB beim Spectrum). Qualitätsprobleme bei Laufwerken und Medien sowie das Quasi-Monopol von Sinclair auf Datenträger und Produktion machte das System für Softwareproduzenten eher unattraktiv. Software für den Sinclair Spectrum und Sinclair ZX81 waren zuvor oft auf der weiter verbreiteten Kassette ausgeliefert worden.\n\nZwar konnten an den QL externe Diskettenlaufwerke angeschlossen werden, dafür musste aber zuerst ein Diskettencontroller von einem von mehreren Fremdanbietern erworben werden, während die späteren Konkurrenzmodelle von Atari (Atari ST) und noch später auch von Commodore (Amiga) mit fertig eingebauten 3½\"-Laufwerken aufwarteten.\n\nAlle Anwendergruppen des QL mussten jeweils teure Hardware dazukaufen. So ging das Konzept, für jeden etwas zu einem günstigen Preis zu bieten, erstmals seit Entwicklung des ZX80 nicht mehr auf.\n\nDie Sinclair-typischen Verspätungen und die teils fehlende Qualitätskontrolle passten nicht mehr in die Zeit, die von qualitativ guten Produkten wie dem Commodore C-64 und dem Amstrad CPC bestimmt waren. Die fehlende Maus und die fehleranfälligen Sinclair-Microdrive-Bandlaufwerke machten den Rechner dem Atari ST, dem Amiga, dem Archimedes und auch dem Apple Macintosh unterlegen. Die neue Rechnerarchitektur und die geänderte Datenträgerformatierung der Laufwerke machten ihn zum Spectrum und ZX81 inkompatibel. Zur CP/M- und IBM-Welt bestanden keine Schnittpunkte. Die günstige und modernisierte Hardware und das teils leistungsfähige Betriebssystem kamen, wie die angedachte und eingebaute Vernetzung, nicht mehr zum Tragen.\n\nDer QL konnte keine der Kernzielgruppen wirklich ansprechen. Die One-Per-Desk-Variante betraf einen anderen Markt.\n\nDiese Faktoren führten zu einem schleppenden Verkauf des QL, die Sinclair in finanzielle Schwierigkeiten brachte. Erschwerend kam hinzu, dass das Elektrofahrzeug Sinclair C5 sich 1985 als völliger Flop erwiesen hatte. In dieser Situation setzte man wieder auf den rasch veraltenden, aber populären ZX Spectrum, von dem 1986 eine erweiterte Version mit 128 KB RAM erschien. Dies war jedoch bereits zu spät: im April 1986 sah sich Sir Clive Sinclair schließlich gezwungen, das angeschlagene Unternehmen an den britischen Konkurrenten Amstrad zu verkaufen, der die QL-Produktion einstellte und die restlichen Rechner unter Buchwert absetzen musste.\n\n\n"}
{"id": "91221", "url": "https://de.wikipedia.org/wiki?curid=91221", "title": "EMule", "text": "EMule\n\neMule () ist ein weitverbreiteter und freier Filesharing-Client für das eDonkey2000- und das Kad-Netzwerk. Er wurde als Alternative zu dem ursprünglichen, mittlerweile aufgegebenen eDonkey-Client entwickelt und um zahlreiche Fähigkeiten erweitert. eMule ist mittlerweile der mit Abstand gebräuchlichste Client für das eDonkey2000-Netzwerk.\n\neMule ist frei und quelloffen, des Weiteren enthält es keine Werbung und Spionageprogramme. Es werden mehrere Protokolle (eD2K, Quellenaustausch, Kad) zum Auffinden anderer Clients verwendet. Ein internes Kredit-System sorgt für eine bevorzugte Behandlung von Clients, die große Mengen an Daten hochladen, so dass diese in der eigenen Warteschlange schneller vorankommen.\n\neMule verfügt über eine integrierte Datenkompression zwischen eMule- und kompatiblen Clienten.\nPeerCache wird unterstützt, so ist es Internetprovidern möglich, ihre von eMule verursachten Datenübertragungsrate durch Caching zu reduzieren.\n\nDie „intelligente Fehlerkorrektur“ (Intelligent Corruption Handling, kurz „I.C.H.“), die eMule dem original eDonkey-Client ebenfalls voraus hat, wurde inzwischen durch die bessere „Erweiterte Intelligente Fehlerkorrektur“ (Advanced Intelligent Corruption Handling, kurz A.I.C.H.) ersetzt. Diese verbesserte Variante kann nun defekte Dateisegmente schon in 180-kB-Größe ausmachen und unbeschädigte Teile retten. Beim originalen eDonkey-Client musste der gesamte \"Chunk\" (9,28 MB) erneut geladen werden. Weiterhin lassen sich die eDonkey-Downloadlinks mit einem zusätzlichen AICH-Hashwert ergänzen, was präventiv gegen defekte Daten schützen kann.\n\nMit der Vorschaufunktion können Videos und Archive angeschaut werden, bevor sie vollständig heruntergeladen wurden.\n\nEs ist möglich, die heruntergeladenen Dateien zu kategorisieren und in unterschiedliche Ordner zu verteilen.\nDateien können sowohl im Hochladen als auch im Herunterladen verschiedene Prioritäten zugewiesen werden. Die Verwendung von komplexen booleschen Suchanfragen wird unterstützt.\n\neMule hat einen integrierten IRC-Klienten. Es verwendet die freie \"SMIRC\"-Implementierung.\nMan kann eMule per Webschnittstelle von jedem beliebigen Rechner (oder Handheld) mit Zugang zum Netzwerk oder einem Internetanschluss aus fernsteuern.\n\nDurch das Erstellen von „Kollektionen“ ist es möglich, zusammenhängende Dateien im Netzwerk zu finden.\nDer Ordner mit nicht fertiggestellten (temporären) Dateien nimmt viel Platz in Anspruch, meist mehr als z. B. früher bei eDonkey, und es kann aufgrund der Arbeitsweise von eMule vorkommen, dass eine der Originalgröße entsprechende temporäre Datei angelegt wird, auch wenn erst ein paar Kilobytes heruntergeladen sind.\n\nMit der Version 0.47a sind folgende Funktionen hinzugekommen:\n\nDateien bis 256 GB (vorher 4 GB) werden unterstützt. Es gibt erweiterte Suchmöglichkeiten, verbesserte Proxyunterstützung und Unterstützung serverseitig gespeicherter Dateibewertungen. Clients, die Spam-Nachrichten verschicken, werden automatisch gebannt. eMule 0.47a ist bereits grundlegend Kad-2.0-fähig, eine vollständige Aktivierung erfolgte erst in späteren Versionen; das „alte“ Kad-1.0-Netz wird ab Version 0.50a nicht mehr unterstützt.\n\nAb der Version 0.47b ist „Obfuskation“ (Protokollverschleierung) im eDonkey2000-Netzwerk möglich. Eine Obfuskation ins Kademlia-Netzwerk kam ab der Version 0.49a vollständig hinzu. Das kann Providern und/oder Netzwerkadministratoren das Erkennen von eMule-Datenverkehr erschweren und so Gegenmaßnahmen eventuell verhindern. Entgegen weitläufigen Annahmen wird dadurch keine größere Anonymität erzielt. Unter Umständen äußert sich die Protokollverschlüsselung sogar in geringfügig höherer Nutzung der Übertragungsrate („Overhead“).\n\nDie Wurzeln von eMule lassen sich auf den 13. Mai 2002 zurückführen, als Hendrik Breitkreuz mit dem originalen eDonkey2000-Client unzufrieden war und deshalb entschied, einen besseren zu entwickeln. Die erste offizielle Version (0.02) wurde am 6. Juli 2002 als Quelltext auf SourceForge zum Download bereitgestellt und ist dort heute noch abrufbar. Mittlerweile hat sich Breitkreuz von der öffentlichen Entwicklung zurückgezogen, unterstützt das Projekt aber noch heute.\n\nDa eMule von Anfang an mit Quelltext veröffentlicht wurde, dauerte es nicht lange, bis die ersten Modifikationen (\"Mods\") auftauchten. Diese erweitern eMule um viele Features, die teilweise in die offiziellen eMule-Versionen übernommen wurden. Da mit rudimentären Programmierkenntnissen die typische eDonkey-Downloadbeschränkung ausgehebelt werden konnte, galt eMule in der Anfangszeit häufig als Leecher-Client. Um diesem entgegenzutreten, wurde mit der Version 0.19a vom 14. September 2002 ein Credit-System eingeführt. Credit-System bedeutet hier, dass man in der Warteschlange eines anderen Benutzers umso schneller vorrückt, je mehr Daten man ihm bereits gesendet hat. Eigene Kredite können nicht manipuliert werden, da diese bei den anderen Benutzern gespeichert sind.\n\neMule baut auf die serverbasierte eDonkey2000-Plattform auf, ist in Visual C++ programmiert und nutzt die Microsoft Foundation Classes. Der Client verfügt über zusätzliche Suchalgorithmen für Quellen wie den Quellenaustausch sowie über das serverlose Kad, das auf dem Kademlia-Algorithmus basiert.\n\nDer Großteil der eMule-Nutzer ist mit einem Server und somit dem eD2K-Netz (eD2K steht für eDonkey2000) verbunden. Der jeweilige Server indiziert meist alle von einem verbundenen Client freigegebenen Dateien. Fragt ein anderer Client nach einer bestimmten Datei, liefert der Server ihm bekannte (verbundene) Clients, die diese Datei anbieten. Die Kontaktaufnahme und der Download erfolgen dann direkt von Client zu Client ohne den Server (Ausnahme: LowID). Zudem fragt eMule alle Server in der Serverliste nach Quellen ab, nicht nur den verbundenen. Entgegen verbreiteter Auffassung ist es folglich auch irrelevant, mit welchem Server der Client verbunden ist, lediglich eine hohe Auslastung des Servers kann das Liefern der Quellen verzögern. Die Servermethode ist zwar die schnellste und benötigt eine geringe Übertragungsrate, liefert jedoch die wenigsten Quellen. Auch unterliegen alle Clients den Regeln des Servers, mit dem sie verbunden sind: Überwiegend werbefinanziert, beschränken Serverbetreiber oft die maximale Anzahl freigegebener Dateien, wodurch die Datenübertragungsrate für mehr angemeldete Benutzer statt für möglichst viele Dateien genutzt wird, was den Sinn des Tauschprogramms ad absurdum führt.\n\nDie Server dienen nur zum Finden anfänglicher Quellen, den Großteil findet eMule, indem es mit anderen Clients Quellen austauscht. Laden etwa Client „A“ und „B“ die gleichen Dateien herunter, kann „A“ bei „B“ anfragen, welche weiteren Clients er kennt, die ebenfalls diese Datei laden bzw. anbieten. Diese Methode produziert zwar mehr Overhead, als die Quellen von einem Server mitgeteilt zu bekommen, sie gilt aber inzwischen als unentbehrlich.\n\nNeben Server und Quellenaustausch gibt es eine dritte Methode, um an Quellen zu kommen. Da bei einem Ausfall vieler großer Server auch der Quellenaustausch nicht mehr richtig funktionieren würde (es werden erst ein paar Quellen gebraucht, um überhaupt untereinander tauschen zu können), wurde Kademlia als serverloses Netz hinzugefügt. Hier gibt es keine speziellen Server mehr, sondern jeder Client ist auch zugleich Server und arbeitet Suchanfragen anderer Clients ab. Der Vorteil ist, dass das Netz nicht mehr von einigen Serverbetreibern abhängig ist, jedoch kosten die zusätzlichen Aufgaben auf jedem Client etwas Datenübertragungsrate (meist ca. 1 kB/s jeweils für Up- und Download). Kademlia findet in der Regel auch Quellen, die per Server und Quellenaustausch unentdeckt bleiben.\n\nIm Normalfall kontaktiert eMule eine Quelle direkt und fragt nach einem Downloadplatz; das ist jedoch nicht möglich, wenn der Client eine LowID hat. Bestimmt wird die ID normalerweise vom Server, der nach dem Verbinden versucht, den Client direkt zu kontaktieren (im Kad-Netz übernimmt diese Aufgabe ein beliebiger Client). Schlägt das fehl, weil sich der Client zum Beispiel hinter einer Firewall oder einem Router befindet, der die Daten über den Port nicht direkt zu eMule durchlässt, erhält dieser eine LowID. In diesem Fall muss die Kontaktaufnahme über einen Vermittler erfolgen, dessen Rolle im eD2K-Netz der Server und im Kad-Netz ein kontaktierbarer Client, der sogenannte Buddy, übernimmt. eMule teilt dem Server/Buddy mit, dass es von einem LowID-Client herunterladen möchte, dieser übermittelt diese Informationen dem verbundenen LowID-Client, und dieser wiederum kontaktiert dann eMule zwecks der eigentlichen Datenübertragung.\n\nDa ein LowID-Client nicht von außen kontaktiert werden kann, können LowID-Clients nicht voneinander laden. Zusätzlich verursacht die Vermittlung über den Server/Buddy Last auf diesem, weshalb eMule wenn möglich von außen erreichbar sein sollte. LowID ist also keine Erfindung der Programmierer, um bestimmte Anwender zu bestrafen, sondern eine technische Bedingung, damit diese Anwender überhaupt am Netz teilnehmen können.\n\nEinige eMule-Modifikationen wie \"NeoMule\" nutzen ähnliche Techniken wie Skype, um die technische Einschränkung zu umgehen, dass LowID-Clients keine andere LowID-Clients kontaktieren können.\n\nLaut \"slyck.com\" zählte das eDonkey2000-Netzwerk im Mai 2006 etwas über 3 Millionen Nutzer. Andere Quellen gaben im Mai 2007 sogar über 4,5 Millionen an.\n\nSeit Veröffentlichung des eMule-Clients bei SourceForge wurde das Programm von dort über 650 Millionen Mal heruntergeladen (Stand: September 2012) und belegte damit lange den Spitzenplatz unter allen auf SourceForge gelisteten Open-Source-Programmen.\n\nDer Originalclient für Microsoft Windows ist \"eMule\". Dieser wird seit 2002 in unregelmäßigen Abständen aktualisiert.\n\nEs gilt zwischen eMule-Mods und eMule-Forks zu unterscheiden:\nEine Mod weicht vergleichsweise wenig vom Original ab, während ein Fork eine komplett eigenständige Entwicklung ist, die unabhängig vom eMule-Hauptcode entwickelt wird.\n\nSolche Mods gibt es, auch aufgrund der hohen Verbreitung von eMule, in inzwischen unüberschaubarer Anzahl. Einige von ihnen sind auf bestimmte Teilbereiche des eD2K-/KAD-Netzes optimiert; so ist die Modifikation \"ZZUL\" zum Beispiel auf das effiziente Verteilen einzelner Dateien spezialisiert.\n\nPlattformunabhängige Abwandlungen, die teilweise auf aktuellem eMule-Code basieren, sind zum Beispiel aMule und xMule. Beide sind nicht nur unter Windows, sondern auch unter anderen Betriebssystemen wie Linux oder macOS lauffähig.\n\nDes Weiteren existieren einige Forks, deren bekanntester Vertreter \"eMule Plus\" ist. Diese Version basiert auf dem veralteten Quellcode von eMule 0.30, wird aber regelmäßig aktualisiert. Eine Unterstützung für das serverlose Kademlia-Netzwerk bietet sie allerdings nicht.\n\nDie offene Entwicklung von eMule zog nicht nur Programmierer an, die bestrebt waren, Verbesserungen vorzunehmen, sondern auch solche, die dem Netzwerk schaden wollen bzw. es in Kauf nahmen, dass ihm geschadet wird, um den eigenen Download zu beschleunigen. Angefangen von Modifikationen, die sich unfair verhalten, indem sie den anderen Netzwerkteilnehmern Upload verweigern oder ihn unfair verteilen, über kommerzielle Abwandlungen, die den Benutzer zwingen, für den Gebrauch zu zahlen, bis hin zu Modifikationen, die gezielt das Netzwerk stören und beispielsweise defekte Daten verteilen, reicht die Palette. Es ist mittlerweile zur Gewohnheit geworden, bei solchen Abwandlungen von „Leechern“ zu sprechen, unabhängig davon, in welcher Weise sie das Netzwerk schädigen. Viele Programmierer „gutartiger“ Mods haben es sich zur Aufgabe gemacht, dagegen vorzugehen, indem Benutzer solcher „Leecher“-Mods erkannt und blockiert werden. Um das umzusetzen, gibt es mehrere unterschiedliche Ansätze, die auch unter den Modentwicklern selbst kontrovers diskutiert werden.\n\n"}
{"id": "91285", "url": "https://de.wikipedia.org/wiki?curid=91285", "title": "Xerox Star", "text": "Xerox Star\n\nDer Xerox Star, offizielle Bezeichnung: \"Xerox 8010 Information System\" war eine von der Xerox Systems Development Division (SDD) in El Segundo, Kalifornien, entwickelte Workstation mit grafischer Benutzeroberfläche (engl. \"graphical user interface\", kurz GUI).\n\nViele Innovationen stammten aus dem Forschungszentrum Xerox PARC, wobei einige Mitarbeiter des Instituts auch bei einer Zweigstelle von SDD in Palo Alto mitwirkten. Nach dem 1973 für die Forschung entwickelten Xerox Alto wurde in dem 1981 herausgebrachten Star die anwenderfreundliche grafische Benutzerschnittstelle erstmals in einem kommerziellen Computer angewandt. Es gab bereits einen per Maus bedienbaren Desktop mit Menüs und Fenstern. Neben einer Ethernet-Schnittstelle unterstützte das System, wie schon sein Vorgänger, die WYSIWYG-Darstellung. Der Star verwendete einen 16-bit-Zeichensatz, um europäische Schriften, Japanisch und später weitere Schriften zu unterstützen.\n\nHäufig wird dem Star nachgesagt, er sei Vorbild für die Entwicklung des Apple Macintosh. In der Tat war es jedoch der Xerox Alto, der Steve Jobs 1979 (und damit zwei Jahre vor Erscheinen des Star) bei einem Besuch von Xerox PARC zum Bau der Apple Lisa (1983) inspirierte. Der Apple Macintosh basierte 1984 auf der Apple Lisa.\n\nAm Xerox PARC wurde auch eine Lisp-Maschine mit dem Namen Xerox \"Dandelion\" auf Basis des 8010 entwickelt, der Name Star bezog sich auf das Betriebssystem mit GUI.\n\n"}
{"id": "91350", "url": "https://de.wikipedia.org/wiki?curid=91350", "title": "Kazaa", "text": "Kazaa\n\nKazaa war ein proprietäres Filesharingprogramm. Basierend auf einem Peer-to-Peer-System können Nutzer Musikdateien, Videos, Textdokumente und Bilder über das Internet tauschen.\n\nDas verwendete Protokoll, genannt FastTrack-Protokoll (nach der Firma FastTrack), ist nach Angaben der Hersteller dezentral und benötigt auch zum Verbinden und Durchsuchen des Netzes keine zentralen Server. Nach der Schließung von Napster sicherte das System dadurch und durch die Supernode-Architektur (\"schnelle Rechner verwalten die Suche\"), die es im Vergleich zu dem ebenfalls dezentralen Gnutella damals bevorteilte, eine breite Masse an Nutzern. Das Protokoll ist proprietär und aufgrund von Verschlüsselungsalgorithmen bisher noch nicht voll bekannt.\n\nEntwickelt wurde Kazaa vom Schweden Niklas Zennström und vom Dänen Janus Friis, die die Software im März 2001 veröffentlichten. Später, als die Legalität des Systems angezweifelt wurde, übernahmen „Sharman Networks“ das System. Da sowohl diese Firma als auch beteiligte Firmen eine verteilte und wenig transparente Struktur haben, ist es bis heute nicht zu einer Schließung gekommen.\n\nErstmals und einmalig wurde am 6. Mai 2004 ein Nutzer der Internettauschbörse Kazaa von dem Amtsgericht Cottbus wegen Urheberrechtsverletzung zu 400 Euro Geldstrafe verurteilt. Als Grundlage diente unter anderem der sichergestellte Rechner des Angeklagten.\n\nAm 11. Juni 2004 verlor die Firma „FastTrack Consumer Empowerment“ vor dem Europäischen Markenamt ein Widerspruchsverfahren gegen die Eintragung des Markennamens Kazaa. Im September 2005 verlor Sharman Networks ein australisches Gerichtsverfahren und wurde dazu verurteilt, die Software so zu modifizieren, dass australische Benutzer kein urheberrechtlich geschütztes Material mehr damit tauschen können. Die ARIA sollte eine Liste von blockierten Inhalten erstellen können, nach der sich der Kazaa-Client dann richtet.\nDieser Forderung kam der Hersteller zunächst nicht nach. Stattdessen erschien zum Ende der Frist ein Banner auf der offiziellen Seite, das erklärt, dass australische Nutzer die Software nun nicht mehr verwenden dürfen.\n\nIm Juli 2006 wurde schließlich im Rahmen eines Vergleichs mit der US-amerikanischen Recording Industry Association of America (RIAA) und dem Musikindustrieverband IFPI eine hohe Abfindungszahlung vereinbart und die Software mit Filterfunktionen für urheberrechtlich geschützte Werke versehen.\n\nSeit 2007 wird Kazaa Media Desktop ohne Spyware und Malware auf der offiziellen Webseite zum Download angeboten. Dennoch enthält sie weiterhin eine Toolbar und Werbe-Dateien, wie Links auf dem Desktop, Werbeeinblendungen in der Software.\n\nIm Februar 2009 wurde eine neue Version der Kazaa.com-Website online gestellt. Nun ist Kazaa gegen Bezahlung von 19,98 USD pro Monat nutzbar und gibt dem Kunden eine Flatrate zum Herunterladen von Songs. Diese sind jedoch mittels DRM geschützt und können auf maximal drei PCs gleichzeitig verwendet werden. Ein Abspielen auf tragbaren MP3-Player ist nicht möglich, zudem sperrt das DRM-Verfahren die heruntergeladenen Songs, falls der Nutzer seine Mitgliedschaft bei Kazaa kündigt.\n\nIm August 2012 wurde der Musikvertrieb eingestellt.\n\nDie Originalsoftware \"Kazaa Media Desktop\" enthält Spyware sowie Adware von Cydoor, die ein System reversibel stark verlangsamen oder sogar soweit schädigen kann, dass es in seiner Funktionalität beeinträchtigt wird.\n\nIm Gegensatz dazu sind die inoffiziellen Kazaa-Lite-Varianten benutzbar, ohne die Malware zu installieren oder auszuführen.\n\nKazaa/Sharman ist umstritten, weil es Geld mit Hilfe einer Software verdient, die angeblich nur für das Erhalten urheberrechtlich geschützter Werke dient, ohne die Urheber zu entlohnen.\nAuch wurde das Portal \"p2pnet.net\" juristisch dazu gezwungen, kritische Artikel über Sharman zu entfernen.\n\nWeiter wird die Software kritisiert, weil jeder teilnehmende Rechner seitens des Kazaa-Herstellers Sharman Networks auch für andere Zwecke genutzt werden kann.\n\n"}
{"id": "91482", "url": "https://de.wikipedia.org/wiki?curid=91482", "title": "Parser", "text": "Parser\n\nEin Parser [] (engl. \"to parse,\" „analysieren“, bzw. lateinisch \"pars,\" „Teil“; im Deutschen gelegentlich auch Zerteiler) ist ein Computerprogramm, das in der Informatik für die Zerlegung und Umwandlung einer Eingabe in ein für die Weiterverarbeitung geeigneteres Format zuständig ist. Häufig werden Parser eingesetzt, um im Anschluss an den Analysevorgang die Semantik der Eingabe zu erschließen und daraufhin Aktionen durchzuführen. \n\nIm Vergleich zu einem Recognizer, der die Eingabe analysiert und ausgibt, ob diese im Sinne der Vorgaben \"richtig\" oder \"falsch\" ist, gibt der Parser die Analyse einer Eingabe in einer gewünschten Form aus und erzeugt zusätzlich Strukturbeschreibungen.\n\nDie Syntaxanalyse \"(Parsing)\" findet auch außerhalb der Informatik Anwendung, z. B. bei der Untersuchung der Struktur von natürlichen Sprachen. In der Grammatik würde die Syntaxanalyse eines Satzes dem Zerlegen des Satzes in seine grammatikalischen Bestandteile (Syntax) entsprechen. Siehe dazu Linguistik.\n\nIm Allgemeinen wird ein Parser dazu verwendet, einen Text in eine neue Struktur zu übersetzen, z. B. in einen Syntaxbaum, welcher die Hierarchie zwischen den Elementen ausdrückt.\n\n\nZur Analyse des Texts verwenden Parser in der Regel einen separaten lexikalischen Scanner (auch \"Lexer\" genannt). Dieser zerlegt die (als simple Aneinanderreihung von Zeichen vorliegenden) Eingabedaten in Token (Eingabesymbole bzw. „Wörter“, die der Parser versteht); weil die Zerlegung in Token einer regulären Grammatik folgt, ist der Scanner meist ein endlicher Automat. Diese Token dienen als atomare Eingabezeichen des Parsers. Parser, die keinen separaten Scanner verwenden, werden \"Scannerless parsers\" genannt. \n\nDer eigentliche Parser als Implementierung eines abstrakten Automaten (meist realisiert als Kellerautomat) kümmert sich dagegen um die Grammatik der Eingabe, führt eine syntaktische Überprüfung der Eingangsdaten durch und erstellt in der Regel aus den Daten einen Ableitungsbaum (in Anlehnung an das Englische gelegentlich auch als \"Parse-Baum\" bezeichnet). Dieser wird danach zur Weiterverarbeitung der Daten verwendet; typische Anwendungen sind die semantische Analyse, Codegenerierung in einem Compiler oder Ausführung durch einen Interpreter.\n\nBei HTML würde ein lexikalischer Scanner die HTML-Datei in HTML-Tags und Fließtext zerlegen und diese Bestandteile an den Parser weiterreichen – d. h. den Scanner „interessiert“ nur das Aussehen der Syntaxelemente („wenn es in spitzen Klammern steht, ist es ein HTML-Tag“). Der Parser dagegen verarbeitet die syntaktischen Zusammenhänge, d. h. untersucht, welche Paare von Tags zusammengehören bzw. wie die Tags ineinander verschachtelt sind; die inhaltliche Bedeutung der Tags interessiert den Parser dagegen nicht, sondern wird erst von der darauf folgenden Weiterverarbeitung berücksichtigt.\n\nAnschaulich dargestellt ist ein Parser diejenige Software, welche die Anweisungen im Quelltext des Anwenders überprüft, weiterverarbeitet und weiterleitet.\n\nMan unterscheidet verschiedene Parse-Verfahren. Dabei wird nach genereller\nVorgehensweise, also der Unterscheidung nach der Reihenfolge, in der die\nKnoten des Ableitungsbaums erstellt werden\n(top-down, auch \"theoriegetriebenes Parsing\" oder\nbottom-up, auch \"eingabegetriebenes Parsing,\" sowie \"left corner\"), spezifischer Vorgehensweise (LL, LR, SLR, LALR, LC, …) und Implementierungstechnik (rekursiv absteigend, rekursiv aufsteigend oder tabellengesteuert) unterschieden. Weiter wird auch nach Grammatikart unterschieden.\n\nHier ein paar auf kontextfreien Grammatiken basierende Verfahren:\n\n\nDas Parsen wohldefinierter künstlicher Sprachen (siehe formale Sprachen, Programmiersprachen) ist weniger komplex als das Parsen frei gewachsener natürlicher Sprachen wie Englisch oder Deutsch, die durch eine Vielzahl von Mehrdeutigkeiten, Irregularitäten und Inkonsistenzen geprägt sind. Siehe hierzu auch Computerlinguistik.\n\n\"Hinweis:\" Der Begriff \"parsen\" sollte nicht mit dem Begriff \"kompilieren\" verwechselt werden. Letzteres erzeugt einen Zielcode aus einem Quellcode, dabei wird unter anderem auch geparst, darüber hinaus finden aber weitere Aktionen statt.\n\nParser werden häufig eingesetzt, um aus einer Aneinanderreihung von Symbolen eine Baumstruktur zu machen. Ein typisches Beispiel dafür sind mathematische Ausdrücke wie formula_1. Dieser Ausdruck, so wie er hier steht, besteht erstmal nur aus einer Reihe von Symbolen:\n\nDie Aufgabe des Parsers ist nun, die zugrundeliegende Struktur dieser Symbolfolge zu erkennen. Häufig geschieht das in Form eines Parsebaums, der in diesem Fall so aussehen kann:\n\nDies ist die Ausgabe eines einfachen Parsers. Diese Ausgabe kann nun durch weitere Programme analysiert werden.\n\n\n\n"}
{"id": "92075", "url": "https://de.wikipedia.org/wiki?curid=92075", "title": "Ext2", "text": "Ext2\n\nDas ext2 oder auch second extended filesystem war viele Jahre lang das Standarddateisystem des Linux-Betriebssystems, wurde allerdings mit der Zeit von seinen Nachfolgern ext3 und ext4 abgelöst. Es wurde ursprünglich 1993 von Rémy Card auf Basis des Extended Filesystem v1 entwickelt, das wiederum aus dem Minix-Dateisystem entstand. Die heutige Implementierung im Linux-Kernel stammt sowohl von ihm als auch Theodore Ts’o und Stephen Tweedie. Des Weiteren existieren Implementierungen für AmigaOS, FreeBSD, GNU Hurd, MiNT, MorphOS, NetBSD, OpenBSD, OS/2, RISC OS und Windows. Hauptnachteil von ext2 ist, dass es kein Journaling-Dateisystem ist. Es verliert daher zunehmend Benutzer an seinen abwärtskompatiblen Nachfolger ext3 und andere, neuere Dateisysteme.\n\next2 teilt viele seiner Eigenschaften mit traditionellen Unix-Filesystemen, etwa das Konzept der Blöcke, Inodes und Verzeichnisse. Wenn gewünscht, ist es um Eigenschaften wie Zugriffskontrolllisten, Fragmente, Wiederherstellung gelöschter Daten und Kompression erweiterbar. Die meisten der genannten Funktionen sind nicht serienmäßig implementiert, sondern existieren nur als Patches. Weiterhin gibt es einen Versionsmechanismus, der es erlaubt, neue Funktionen abwärtskompatibel hinzuzufügen (wie dies bei der Journaling-Erweiterung ext3 geschehen ist). Alle Informationen werden auf einem ext2-System im „Little Endian“-Format abgelegt, so dass ein Dateisystem auf verschiedenen Architekturen eingehängt werden kann, ohne dass es zu Inkompatibilitäten kommt.\n\nDer Platz auf einer mit ext2 formatierten Partition wird in Blöcke aufgeteilt. Diese haben eine feste Größe von 1 KiB, 2 KiB oder 4 KiB, auf Alpha-Prozessoren sind zudem Blockgrößen von 8 KiB möglich. Die Größe der Blöcke wird bei der Erstellung des Dateisystems festgelegt. Kleinere Blöcke führen zu weniger verschwendetem Platz pro Datei, benötigen jedoch mehr Zusatzaufwand bei der Verwaltung, und begrenzen indirekt die maximale Größe der Dateien und des ganzen Dateisystems.\n\nUm eine Fragmentierung von vornherein weitestgehend zu vermeiden, die den Zugriff auf große Mengen aufeinander folgender Blöcke bremsen würde, werden Blöcke in Blockgruppen zusammengefasst. Die Informationen über jede Blockgruppe werden in einer Deskriptortabelle abgelegt, die direkt hinter dem Superblock liegt. Zwei Blöcke in der Nähe des Anfangs der Blockgruppe sind für zwei Bitmaps reserviert, welche die Block- und Inode-Belegung in der Gruppe anzeigen. Da jede Bitmap nur einen Block belegen kann, ist die maximale Größe jeder Blockgruppe (in Blöcken) auf achtmal die Größe eines Blockes (in Bytes) begrenzt. Die auf die Bitmaps folgenden Blöcke enthalten die Inode-Tabelle für die Blockgruppe, und die übrigen sind als Datenblöcke nutzbar.\n\nDer Superblock enthält alle Informationen über die Konfiguration des Dateisystems. Der primäre Superblock liegt 1024 Byte hinter dem Anfang des Gerätes und ist wichtig, um das Dateisystem einbinden \"(mounten)\" zu können. Die Informationen im Superblock enthalten Felder, die zum Beispiel die Anzahl der Blöcke und Inodes im Dateisystem angeben, wie viele davon frei sind, wie viele Inodes und Blöcke in jeder Blockgruppe vorhanden sind, wann das Dateisystem eingebunden wurde, ob es beim letzten Mal korrekt ausgehängt wurde, wann es geändert wurde, welche Version vorliegt, und welches Betriebssystem es angelegt hat. Da bei einer Beschädigung des Superblocks das gesamte Dateisystem unbrauchbar wäre, werden vom Superblock mehrere Kopien, verteilt in mehreren Blockgruppen, gespeichert. Diese Superblock-Kopien erlauben im Fehlerfall eine Reparatur des Original-Superblocks.\n\nWenn das Dateisystem Revision 1 oder neuer ist, gibt es im Superblock weitere Felder, die den Namen des Datenträgers, eine eindeutige Identifikationsnummer und die Inode-Größe angeben, sowie Platz für die Konfigurationsinformationen optionaler Dateisystemfunktionen bieten.\n\nDer Inode (Indexknoten) ist ein fundamentales Konzept im ext2-Dateisystem. Jedes Objekt im Dateisystem wird durch einen Inode repräsentiert. Die Inode-Struktur enthält Zeiger (Verweise) auf die Blöcke, in denen die Daten des Objekts abgelegt sind, und außerdem alle Metadaten über ein Objekt mit Ausnahme seines Namens. Zu den Metadaten gehören Zugriffsrechte, Besitzer, Gruppe, Flags, Größe, die Anzahl der benutzten Blöcke, Zugriffszeitpunkt, Änderungszeitpunkt, Löschzeitpunkt, Anzahl der Verknüpfungen, Fragmente, Version (wird von NFS benötigt), erweiterte Attribute und eventuelle Zugriffskontrolllisten.\n\nEs gibt einige ungenutzte Felder und überladene Felder in der Inode-Struktur. Ein Feld ist für die Verzeichnis-Zugriffskontrollliste reserviert, wenn der Inode ein Verzeichnis ist, alternativ hält dieses Feld die oberen 32 Bit der Dateigröße, wenn der Inode eine reguläre Datei ist (dies erlaubt Dateigrößen über 2 GiB). Die meisten der übrigen Felder werden von Linux und GNU Hurd als vergrößerte Besitzer- und Gruppenfelder genutzt. GNU Hurd kennt außerdem zusätzliche Felder für erweiterte Rechteverwaltung und den Inode des Programms, das diesen Inode üblicherweise interpretiert.\n\nEs gibt im Inode Zeiger auf die ersten 12 Blöcke, welche die Daten der Datei enthalten. Außerdem gibt es einen Zeiger auf einen indirekten Block (der wiederum Zeiger auf den nächsten Satz von Blöcken der Datei enthält), einen Zeiger auf einen doppelt indirekten Block (der Zeiger auf weitere indirekte Blöcke enthält), und einen Zeiger auf einen dreifach indirekten Block (der Zeiger auf doppelt indirekte Blöcke enthält).\n\nDas \"Flags\"-Feld enthält einige ext2-spezifische \"Flags\", die nicht beispielsweise durch codice_1 beeinflusst werden können. Diese Flags können mit dem Programm codice_2 gelistet werden und mit codice_3 geändert werden. Diese Flags erlauben einer Datei besonderes Verhalten, welches über die POSIX-Dateiflags nicht darstellbar ist: Es gibt Flags für sicheres Löschen, Unlöschbarkeit, Kompression, synchrone Updates, Schreibschutz, indizierte Verzeichnisse, Journaling und einiges mehr. Die Attribute eines Verzeichnisses werden auf neu erzeugte darunterliegende Dateien vererbt.\nNicht alle Flags werden jedoch vom ext2-Treiber im Kernel umgesetzt: das Attribut „c“ (komprimieren) wird beispielsweise von der ext2-implementation des Linux-Kernels nicht unterstützt. Dafür gab es das inzwischen eingestellte extz-Projekt (= ext3 + Kompression + Verschlüsselung).\n\nEin Verzeichnis ist ein Dateisystemobjekt und hat wie eine normale Datei einen Inode. Prinzipiell ist es eine spezielle Datei, die jeden Dateinamen im Verzeichnis mit einer Inode-Nummer verknüpft. Neuere Versionen des Dateisystems legen zudem den Typ des Objektes (Datei, Verzeichnis, symbolische Verknüpfung, Gerät, FIFO, Socket) mit ab, um zu vermeiden, dass der Inode selbst auf diese Information geprüft werden muss (um dies nutzen zu können, ist eine neuere Version der glibc erforderlich).\n\nEin im Verzeichnis eingetragener Dateiname wird als Verknüpfung oder Harter Link bezeichnet, wenn die Abgrenzung gegenüber einer symbolischen Verknüpfung betont werden soll. Dahinter steckt eine „N zu 1“-Beziehung zwischen Verknüpfungen und Dateien. Die Datei, die aus den Nutzdaten und dem Inode besteht, wird erst über einen Dateipfad, also einen Verzeichniseintrag nutzbar. Da zu einer Datei beliebig viele Verzeichniseinträge angelegt werden können, ist es sinnvoll, diese nicht mit der Datei zu identifizieren, sondern als „Verweise“ auf die Datei zu begreifen. Mehrere Verknüpfungen zu einer Datei können im selben Verzeichnis stehen. Im Inode der Datei wird über die Anzahl der Verknüpfungen Buch geführt. Nach dem Löschen der letzten Verknüpfung einer Datei wird die Datei selbst, also der Inode und die Nutzdatenblöcke, freigegeben.\n\nBeim Erzeugen einer neuen Verzeichnisdatei werden gleich zwei Verknüpfungen dazu eingerichtet: Einer im übergeordneten Verzeichnis mit dem gewählten Verzeichnisnamen, einer mit dem Namen „.“ im neuen Verzeichnis selbst. Unterverzeichnisse haben jeweils noch eine Verknüpfung namens „..“ auf die übergeordnete Verzeichnisdatei. Für das Wurzelverzeichnis eines Dateisystems sind die beiden Verknüpfungen „.“ und „..“ identisch.\n\nSymbolische Verknüpfungen (Symlinks) sind ebenfalls Dateisystemobjekte mit Inodes. Wenn die Verknüpfung jedoch kürzer als 60 Bytes ist, werden ihre Daten direkt im Inode gespeichert. Dabei werden Felder benutzt, die normalerweise Zeiger auf Datenblöcke halten würden. Da die meisten Verknüpfungen weniger als 60 Zeichen lang sind, werden hierdurch die Inanspruchnahme eines Blocks für die symbolische Verknüpfung gespart. Symbolische Verknüpfungen können über Dateisystemgrenzen (also ebenfalls über mehrere Festplatten oder Partitionen) hinweg eingesetzt werden. Dabei kann es passieren, dass die Datei, auf die die symbolische Verknüpfung verweist, gelöscht wird, die Verknüpfung jedoch bestehen bleibt. Die Verknüpfung verweist damit auf eine nicht mehr vorhandene Datei und ist somit unbrauchbar geworden.\n\nZeichen- und blockorientierten Geräten sind nie Datenblöcke zugewiesen. Stattdessen wird die vom Kernel vergebene Gerätenummer im Inode abgelegt, wobei wiederum die Zeigerfelder auf Datenblöcke benutzt werden.\n\nInnerhalb des Dateisystems lässt sich eine bestimmte Anzahl von Blöcken für einen bestimmten Benutzer reservieren, normalerweise für den Systemadministrator \"root\". Dies erlaubt dem System, auch dann zu funktionieren, wenn nichtprivilegierte Benutzer den gesamten ihnen zur Verfügung stehenden Speicherplatz aufgefüllt haben. Der Mechanismus funktioniert unabhängig von \"Disk Quotas\". Weiterhin hilft er dabei, ein vollständiges Füllen des Dateisystems zu verhindern und so Fragmentierung zu bekämpfen.\n\nWährend der Startphase führen die meisten Systeme eine Konsistenzüberprüfung (\"e2fsck\") auf ihren Dateisystemen durch. Der Superblock des ext2-Systems enthält mehrere Felder, die anzeigen, ob codice_4 laufen sollte (da die Prüfung des Dateisystems lange dauern kann, wenn es sehr groß ist). codice_4 wird üblicherweise laufen, wenn das Dateisystem nicht sauber ausgehängt wurde oder eine einstellbare Maximalzeit zwischen zwei Routineüberprüfungen überschritten wurde.\n\next2 verfügt über einen ausgereiften Kompatibilitätsmechanismus, der es erlaubt, Dateisysteme unter Kernels zu verwenden, deren ext2fs-Treiber von einigen verwendeten Funktionen nichts weiß. Der Kompatibilitätsmechanismus steht seit ext2fs Revision 1 zur Verfügung. Es gibt dabei drei Felder von je 32 Bit Länge, eines für kompatible Eigenschaften (COMPAT), eines für nur lesekompatible Features (RO_COMPAT) und eines für inkompatible Eigenschaften (INCOMPAT).\n\nEin COMPAT-\"flag\" bedeutet, dass das Dateisystem eine Eigenschaft enthält, aber das Datenformat auf der Platte 100 % kompatibel mit älteren Formaten ist, so dass ein Kernel, der diese Funktion nicht kennt, im Dateisystem lesen und schreiben könnte, ohne es inkonsistent zu machen. Bestes Beispiel für ein COMPAT-\"flag\" ist die Funktion HAS_JOURNAL eines ext3-Dateisystems. Ein Kernel ohne ext3-Unterstützung kann ein solches Dateisystem problemlos als ext2fs einhängen und anschließend ohne Benutzung des Journals darauf schreiben, ohne irgendetwas zu beschädigen.\n\nEin RO_COMPAT-\"flag\" zeigt an, dass das Datenformat des Dateisystems beim Lesen 100 % kompatibel zu älteren Formaten ist. Ein Kernel ohne Kenntnis der in Frage stehenden Funktion könnte jedoch das Dateisystem korrumpieren, wenn er darauf schreibt, daher wird dies verhindert. Ein Beispiel für eine lesekompatible Eigenschaft ist SPARSE_SUPER, ein Dateisystemlayout, bei dem weniger Superblocksicherungen als normal üblich auf dem Datenträger abgelegt werden. Ein alter Kernel kann problemlos von einer solchen Festplatte lesen, wenn er jedoch einen Schreibversuch unternehmen würde, würden seine Schreibroutinen irreführende Fehlermeldungen produzieren und eventuell die Bitmaps inkonsistent werden.\n\nEin INCOMPAT-\"flag\" zeigt an, dass sich das Datenformat so geändert hat, dass Kernel ohne diese Eigenschaft weder schreiben noch lesen oder auch nur einhängen könnten. Als Beispiel für eine inkompatible Zusatzfunktion kann die optionale Kompression dienen; ein Kernel, der die Daten nicht dekomprimieren kann, würde nur „Müll“ vom Datenträger lesen. Auch ein inkonsistentes ext3-Dateisystem ist so lange inkompatibel, bis ein ext3-fähiger Kernel das Journal gelesen und die Inkonsistenzen beseitigt hat. Danach kann das ext3-System wieder als ext2 eingehängt werden.\n\nDas Hinzufügen neuer Eigenschaften zum ext2/3-Dateisystem erfordert immer eine Aktualisierung des zugehörigen \"toolkit e2fsprogs\", da die darin enthaltenen Prüfungswerkzeuge in der Lage sein müssen, alle Dateisystemeigenschaften zu kennen, um eine zuverlässige Feststellung und Behebung von Inkonsistenzen zu ermöglichen.\n\nDie Ursachen für gewisse Limits des ext2-Dateisystems können einerseits im Datenformat auf dem Datenträger begründet sein, andererseits durch den Kernel des zugrunde liegenden Betriebssystems. Die meisten werden einmalig bei der Erstellung des Dateisystems festgelegt und hängen von der gewählten Blockgröße und dem gewählten Verhältnis von Blöcken zu Inodes ab.\n\nBlockgrößen von 8 KiB sind standardmäßig nur auf Alpha-Architekturen möglich, sowie auf speziell konfigurierten und gepatchten anderen Architekturen. Unabhängig von den Fähigkeiten des Kernels können einige Userspace-Programme, denen Unterstützung für große Dateien fehlt, Dateien jenseits von 2 GiB nicht korrekt handhaben.\n\nDas Dateisystem begrenzt die Anzahl von Unterverzeichnissen in einem gegebenen Verzeichnis auf 32.000 Stück. Weiterhin wird angewarnt, wenn in einem Verzeichnis mehr als etwa 10.000 bis 15.000 Dateien liegen, dass Dateioperationen in solch großen Verzeichnissen lange dauern könnten. Die tatsächlich maximal mögliche Anzahl von Dateien ist akademischer Natur, da es bereits schwierig sein wird, genügend Dateinamen zu erzeugen, bevor das Limit von 130 Trillionen (10) Dateien pro Verzeichnis erreicht wird.\n\n\n\n\n"}
